{"cell_type":{"bf8d2190":"code","5d762780":"code","34ff7d0d":"code","039a52b4":"code","e0f901cf":"code","64466b31":"code","ca0c44d2":"code","c7e66aad":"code","5f30baa3":"code","647f8c07":"code","8edfa3b0":"code","36b71f96":"code","ec9afc2c":"code","02b3b76c":"code","33bf0402":"code","5ac08a23":"code","b1fb2fae":"code","ef12a991":"code","111d1294":"code","abef1954":"code","83633130":"code","688a2c32":"code","6a024c96":"code","9b93c6f6":"code","e8afb6cf":"code","95b340b9":"code","7d6b2a14":"code","20e18ba1":"code","2507a61c":"code","0321be4f":"code","5007e038":"code","b5a71672":"code","803524b5":"code","421eef92":"code","6364b0d7":"code","0205ebaa":"code","25bb7ff6":"code","3ade497b":"code","828ae9cf":"code","47d03f71":"code","fbad4ba6":"code","fd377e3d":"code","1a308281":"code","4a5b7f5d":"code","5e712475":"code","85b955b8":"code","43f83fbc":"code","6208abeb":"code","fb9b9e23":"code","7ccf4ea2":"code","56a83b4d":"code","5b1a4292":"code","bfe72ebf":"code","07393b5e":"markdown","494365fc":"markdown","e41ed303":"markdown","7fe51554":"markdown","e150f929":"markdown","bbd07765":"markdown","2e986a27":"markdown","5bcb6294":"markdown","858b9f80":"markdown","b5123020":"markdown","16947f36":"markdown","1d126dd4":"markdown","a71d435d":"markdown","2ef57a05":"markdown","27d1d8c5":"markdown","7776c280":"markdown","c6f3c5ce":"markdown","5d779468":"markdown","de7829a1":"markdown","fe52294b":"markdown","c6816897":"markdown","058cdc97":"markdown","622dd07e":"markdown","94090e8c":"markdown","48929616":"markdown","04626b25":"markdown","7ee6d662":"markdown","b82694d2":"markdown","6e8b572c":"markdown","58bfd57b":"markdown"},"source":{"bf8d2190":"import pandas as pd\nimport seaborn as sns\nimport numpy as np\nfrom scipy.stats import norm","5d762780":"data = pd.read_csv(\"..\/input\/hmeq.csv\")","34ff7d0d":"data.shape","039a52b4":"data.head()","e0f901cf":"data.isna().sum()","64466b31":"df = data.dropna()","ca0c44d2":"df.shape","c7e66aad":"df_con_bad  = df.drop(\"REASON\",axis = 1 )\ndf_con_bad = df_con_bad.drop(\"JOB\",axis = 1)","5f30baa3":"from pylab import rcParams\nrcParams['figure.figsize'] = 10, 10\ndf_con_bad_corr = df_con_bad.drop(\"BAD\",axis = 1)  \ncorr = df_con_bad_corr.corr()\nsns.heatmap(corr, \n        xticklabels=corr.columns,\n        yticklabels=corr.columns)","647f8c07":"dic = {\"LOAN\":df[\"LOAN\"],\"BAD\":df[\"BAD\"],\"MORTDUE\":df[\"MORTDUE\"],\"VALUE\":df[\"VALUE\"],\"YOJ\":df[\"YOJ\"]}\nrcParams['figure.figsize'] = 5, 5\ndf_pair = pd.DataFrame(dic)\nsns.pairplot(df_pair,vars=['LOAN', 'MORTDUE',\"VALUE\",\"YOJ\"],hue=\"BAD\")","8edfa3b0":"df[df[\"BAD\"]==0].shape","36b71f96":"df[df[\"BAD\"]==1].shape","ec9afc2c":"df_set_bad = df.set_index(\"BAD\")","02b3b76c":"import numpy as np\nimport matplotlib.pyplot as plt\nrcParams['figure.figsize'] = 8, 5\n# data to plot\nn_groups = 3\ndf_set_bad_mean_undefault = df_set_bad.loc[0].mean()\ndf_set_bad_mean_default = df_set_bad.loc[1].mean()\n\n# create plot\nfig, ax = plt.subplots()\nindex = np.arange(n_groups)\nbar_width = 0.35\nopacity = 0.8\n \nrects1 = plt.bar(index, df_set_bad_mean_undefault[0:3], bar_width,alpha=opacity,color='b',label='Undefault')\n \nrects2 = plt.bar(index + bar_width, df_set_bad_mean_default[0:3], bar_width,alpha=opacity,color='g',label='Default')\n \nplt.xlabel('Mean of each variables')\nplt.ylabel('$ value')\nplt.title('Comparison of mean of being undefault and default based on LOAN,MORTDUE and VALUE')\nplt.xticks(index + bar_width, (\"LOAN\",\"MORTDUE\",\"VALUE\",\"YOJ\",\"DEROG\",\"DELINQ\",\"CLAGE\",\"NINQ\",\"CLNO\",\"DEBTINC\"))\nplt.legend()\n \nplt.tight_layout()\nplt.show()","33bf0402":"df_set_bad.loc[0].mean()","5ac08a23":"df_set_bad.loc[1].mean()","b1fb2fae":"#pad=0.3, w_pad=4, h_pad=1.0\nimport matplotlib.pyplot as plt\nfrom pylab import rcParams\nrcParams['figure.figsize'] = 30, 15\nfig, axs = plt.subplots(3,3)\nplt.tight_layout()\nfig.subplots_adjust(top=0.88)\nsns.boxplot(x=\"BAD\", y=\"LOAN\", data=df,ax=axs[0,0])\naxs[0,0].set_title(\" Amount of the loan requesed for a group of being default and undefault\\n'\",fontsize=14)\nsns.boxplot(x=\"BAD\", y=\"MORTDUE\", data=df,ax=axs[0,1])\naxs[0,1].set_title(\" Amount due on existing mortgage for a group of being default and undefault\\n'\",fontsize=14)\nsns.boxplot(x=\"BAD\", y=\"VALUE\", data=df,ax=axs[0,2])\naxs[0,2].set_title(\" Value of current property for a group of being default and undefault\\n'\",fontsize=14)\n\nsns.boxplot(x=\"BAD\", y=\"YOJ\", data=df,ax=axs[1,0])\naxs[1,0].set_title(\"Years at present job for a group of being default and undefault\\n'\",fontsize=14)\n\nsns.boxplot(x=\"BAD\", y=\"DEROG\", data=df,ax=axs[1,1])\naxs[1,1].set_title(\" Number of major derogatory report for a group of being default and undefault\\n'\",fontsize=14)\n\nsns.boxplot(x=\"BAD\", y=\"DELINQ\", data=df,ax=axs[1,2])\naxs[1,2].set_title(\"     Number of delinquent credit lines  for a group of being default and undefault\\n'\",fontsize=14)\n\nsns.boxplot(x=\"BAD\", y=\"CLAGE\", data=df,ax=axs[2,0])\naxs[2,0].set_title(\" Age of oldest credit line in months for a group of being default and undefault\\n'\",fontsize=14)\n\n\nsns.boxplot(x=\"BAD\", y=\"CLNO\", data=df,ax=axs[2,1])\naxs[2,1].set_title(\" Number of credit lines for a group of being default and undefault\\n'\",fontsize=14)\n\n\nsns.boxplot(x=\"BAD\", y=\"DEBTINC\", data=df,ax=axs[2,2])\naxs[2,2].set_title(\"Debt-to-income ratio for a group of being default and undefault\\n'\",fontsize=14)\n\nplt.tight_layout()\nplt.show()","ef12a991":"df_set_job = df.set_index(\"JOB\")","111d1294":"df_set_job[\"BAD\"].value_counts()","abef1954":"import matplotlib.pyplot as plt\nfrom pylab import rcParams\nrcParams['figure.figsize'] = 20, 10\n\nfig, axs = plt.subplots(2,4)\nplt.tight_layout(pad=0.5, w_pad=4, h_pad=1.0)\n\nsns.boxplot(x=\"JOB\", y=\"LOAN\", data=df,ax=axs[0,0])\nsns.boxplot(x=\"JOB\", y=\"MORTDUE\", data=df,ax=axs[0,1])\nsns.boxplot(x=\"JOB\", y=\"VALUE\", data=df,ax=axs[0,2])\nsns.boxplot(x=\"JOB\", y=\"YOJ\", data=df,ax=axs[0,3])\nsns.boxplot(x=\"JOB\", y=\"CLAGE\", data=df,ax=axs[1,0])\nsns.boxplot(x=\"JOB\", y=\"NINQ\", data=df,ax=axs[1,1])\nsns.boxplot(x=\"JOB\", y=\"CLNO\", data=df,ax=axs[1,2])","83633130":"ct = pd.crosstab(df.BAD,df.JOB,margins=True) #making cross table  \nct","688a2c32":"from scipy.stats import chi2_contingency\nchi2, p, dof, ex = chi2_contingency(ct)\nprint(\"chi2 = \", chi2)\nprint(\"p-val = \", p)\nprint(\"degree of freedom = \",dof)\nprint(\"Expected:\")\npd.DataFrame(ex)\n","6a024c96":"import numpy as np\nimport pandas as pd\nimport scipy as sp\nimport sklearn as sk\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\nimport sklearn.ensemble as skens\nimport sklearn.metrics as skmetric\nimport sklearn.naive_bayes as sknb\nimport sklearn.tree as sktree\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nsns.set(style='white', color_codes=True, font_scale=1.3)\nimport sklearn.externals.six as sksix\nimport IPython.display as ipd\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn import metrics\nimport os","9b93c6f6":"import warnings\nwarnings.filterwarnings('ignore')","e8afb6cf":"df_train,df_test = train_test_split(df, test_size=0.3)\ndf_train_drop_cate = df_train.drop(\"REASON\",axis=1) #droping the categorical variables \ndf_test_drop_cate = df_test.drop(\"REASON\",axis=1)\ndf_train_drop_cate = df_train_drop_cate.drop(\"JOB\",axis=1)\ndf_test_drop_cate = df_test_drop_cate.drop(\"JOB\",axis=1)\ndf_train_drop_cate = df_train_drop_cate.drop(\"BAD\",axis=1)\ndf_test_drop_cate = df_test_drop_cate.drop(\"BAD\",axis=1)","95b340b9":"df_train.shape","7d6b2a14":"s = df_train_drop_cate","20e18ba1":"df_train_drop_cate_show = s\ndf_train_drop_cate_show[\"BAD\"] = df_train[\"BAD\"]","2507a61c":"df_train_drop_cate = df_train_drop_cate.drop(\"BAD\",axis=1)","0321be4f":"dt_model = sktree.DecisionTreeClassifier(max_depth=3,\n                                         criterion='entropy')\ndt_model.fit(df_train_drop_cate,df_train_drop_cate_show.BAD)","5007e038":"from sklearn.externals.six import StringIO  \nfrom IPython.display import Image  \nfrom sklearn.tree import export_graphviz\nimport graphviz\n\nexport_graphviz(dt_model, out_file=\"tree_dt_model.dot\",  \n                filled=True, rounded=True,\n                special_characters=True,feature_names=df_train_drop_cate.columns)\n#graph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \n#Image(filename = 'tree_dt_model.dot')","b5a71672":"!dot -Tpng tree_dt_model.dot -o tree_dt_model.png -Gdpi=600","803524b5":"from IPython.display import Image\nImage(filename = 'tree_dt_model.png')","421eef92":"predicted_labels = dt_model.predict(df_train_drop_cate)\ndf_train_drop_cate_show['predicted_dt_tree'] = predicted_labels","6364b0d7":"len(df_train_drop_cate_show[df_train_drop_cate_show['BAD'] == df_train_drop_cate_show[\"predicted_dt_tree\"]])\/len(df_train_drop_cate_show)","0205ebaa":"def comparePlot(input_frame,real_column,predicted_column):\n    df_a = input_frame.copy()\n    df_b = input_frame.copy()\n    df_a['label_source'] = 'BAD'\n    df_b['label_source'] = 'Classifier'\n    df_a['label'] = df_a[real_column]\n    df_b['label'] = df_b[predicted_column].apply(lambda x: 'Predict %s'%x)\n    df_c = pd.concat((df_a, df_b), axis=0, ignore_index=True)\n    sns.lmplot(x='DEBTINC', y='CLAGE', col='label_source',\n               hue='label', data=df_c, fit_reg=False, size=3);","25bb7ff6":"comparePlot(df_train_drop_cate_show,\"BAD\",\"predicted_dt_tree\")","3ade497b":"rf_model = skens.RandomForestClassifier(n_estimators=10,oob_score=True, criterion='entropy')\nrf_model.fit(df_train_drop_cate,df_train.BAD)","828ae9cf":"feat_importance = rf_model.feature_importances_\npd.DataFrame({'Feature Importance':feat_importance},\n            index=df_train_drop_cate.columns).plot(kind='barh')\nrcParams['figure.figsize'] = 10, 5","47d03f71":"predicted_labels = rf_model.predict(df_train_drop_cate)\ndf_train_drop_cate_show['predicted_rf_tree'] = predicted_labels\n","fbad4ba6":"comparePlot(df_train_drop_cate_show,\"BAD\",\"predicted_rf_tree\")","fd377e3d":"param_grid = {\n                 'n_estimators': [5, 10, 15, 20, 25],\n                 'max_depth': [2, 5, 7, 9],\n             }","1a308281":"from sklearn.model_selection import GridSearchCV","4a5b7f5d":"grid_clf = GridSearchCV(rf_model, param_grid, cv=10)\ngrid_clf.fit(df_train_drop_cate,df_train.BAD)","5e712475":"grid_clf.best_estimator_","85b955b8":"grid_clf.best_params_ ","43f83fbc":"#Turing the model into the one with max_depth': 9, 'n_estimators': 20\nrf_model2 = skens.RandomForestClassifier(n_estimators=20,oob_score=True,max_depth=9, criterion='entropy')\nrf_model2.fit(df_train_drop_cate,df_train.BAD)\npredicted_labels2 = rf_model.predict(df_train_drop_cate)\ndf_train_drop_cate_show['predicted_rf_tree2'] = predicted_labels2","6208abeb":"len(df_train_drop_cate_show[df_train_drop_cate_show['BAD'] == df_train_drop_cate_show[\"predicted_rf_tree2\"]])\/len(df_train_drop_cate_show)","fb9b9e23":"df[\"is_default\"] = np.where(df[\"BAD\"] == 1,\"default\",\"not_default\")\ndf_bay_train,df_bay_test = train_test_split(df, test_size=0.3)","7ccf4ea2":"\ngnb_model = sknb.GaussianNB()\n\ngnb_model.fit(df_bay_train[['DEBTINC']],df_bay_train['is_default'])","56a83b4d":"# test the model\ny_pred = gnb_model.predict(df_bay_test[['DEBTINC']])\ndf_bay_test['predicted_nb'] = y_pred","5b1a4292":"comparePlot(df_bay_test,\"is_default\",\"predicted_nb\")","bfe72ebf":"len(df_bay_test[df_bay_test['is_default'] == df_bay_test[\"predicted_nb\"]])\/len(df_bay_test)","07393b5e":"This is the desiton tree based on the data about the home equity loan. It shows how being default and undefault are clasified and what factors affect the clasification of being default. The first important factor is Debt to income ratio. When \"DEBTINC(bebt to income ratio) is less than or equal to 43.68\" for a given sample, that sample is assigned to the bottom left node. When it is false, the sample is assigned to the bottom right node. It keeps doing for each node of criterio to clasify samples.Besides, for this desion tree, the color of orange means that it is clasified as being undefault while that of the blue shows that the sample is done as being default. The pale color of blue and orange means entropy,which is the quantification of uncertainty in data. High entropy means higg uncertainy about predicting the result of being default or not.","494365fc":"By looking at the relation of each pairs, if one variable in pair has excessed value, it is likely to be default. For example, in the pair of LOAN and YOJ, high number of YOJ and low number of LOAN is likely to lead to being default. In the same way, in the pair of LOAN and YOJ, low number of YOJ and high number of LOAN is likely to lead to being default.","e41ed303":"# Question 3 #\n\n**The relation between job categories and the grops of being defaul and undefault**\n\nIn question 3, I would like to figure out the relation between job category and default. The main queston in this question is about \"Is the relationship between job category and default independent ?\" I use bar plots and chi square test to answer this question.\n\nFirstly, by setting JOB as index, I make df_set_job. Through this data frame, I show the rate of default based on each job. To know about the difference of value of variables in each job positon, bar plot is created. In addition,in order to do chi square test, I made a cross table and do hypothesis test about chi square.\n\n","7fe51554":"This is the number of NaN value and trying to drop the value with NaN**","e150f929":"## Conclusion ##\n\n**Question 1: What kind of relations exist between variables in this data set?**\n\nThere are not many high correlation between varianbles except for VALUE(vale of current property) & MORTDUE(amount due on existing mortgage),which is very normal because if there is high value of home, it is likely to be high amount due on exsiting morgage. In addition, low correlation betwen VALUE an LOAN(amount of the loan requested) shows the characteristic of the home equity loan in which the owners of high value of home does not need the home equity loan and people who suffer from money are likely to become obligoer of home equity loan. Besides, among pairs of variables such as LOAN,VALUE, YOJ, if one variable in pair has excessed value, the obligoers of the home equity loan are likely to be default.\n\n**Question 2 : How two group of being undefault and default differ ?**\n\nIn comparison of the two groups of being default and undefault based on the numerical variables, there are not big differnce between LOAN (amount of the loan requested),MORTDUE(amount due on existing mortgage) and VALUE(value of current property).However, there is a significant difference of DELOG(Number of major derogatory reports),DELINQ(Number of delinquent credit lines) and DEBTINC(Debt to income ratio)between two groups of being default and undefault. In fact, the mean value of DEROG for default group is about 6 times as high as the one for undefault group. In addition, the mean value of DELINQ for default group is about 4 times higher than the one for undefault group. Besides,the group of being default has higher debt to income ratio by 6% than the one of being undefault. This means that people who can not return the loan back are prone to have higher number of derogatory reports. In addition, The high number of deliquent credit lines tend to lead to the obligor being default. Based on this result, if there are high number of deliquent credit lines and derogatory reports, banks or some institution who are lending hoem equity loan should keep an eye on these obligor carefully and take some measurement in order to prevent them from losiing the money that they are ledning\n\n**Question 3 : Is the relationship between job category and default independent ?** \n\nThe relationship between job category and default independent based on the result of chi square test. This means that what job the obligoer have affect whether they are able to pay the money of home equity loan back or not. In addtion, people that mange their own company tend to have larger amount of loan request,amount of due on exsiting mortgage and value of current property than other jobs. Besides, by looking at bar plot of CLAGE and JOB, range of self is very small. This shows that people who mange their own company tend to able to pay the equity loan back very shortly\n\n**Question 4 :What factors affect being default greatly? and what are criterion to predict whether new clients who want to use equity loan could be default or not.** \n\nAccording the results from decition tree,random forest and Naive Bayes classifer, the highest accuracy rate is the one with random forest among three methods, which is 99% to predict about being default or undefault. From the feature importance of random forest, DEBTINC(Debt income ratio) and CLAGE(Age of oldest credit line in months) are important to classfy the samples. In addition, desition tree is also helpful to support the interpretation of clasification. Although the acuracty rate of clasification of desition tree is lower than random forest, the desition tree shows us criterions to clasify samples and future data. Based on the visualzied desition tree above, banks or some institutions issuing home equity loan could automate the process of issuing home equity loan and predict the futre possible clients could be default or not at 93% accuracy.","bbd07765":"This is the acuuracy rate of precition with Naive Bayes Classifier","2e986a27":"# Data explolation and classification about home equity loan by using ML with random forest and naive bayes classifier  #\n\nThis project is about exploring the data,hypothesis test and classification about home equity loan, which is a loan where the obligor uses the equity of his or her home as the underlying collateral.To do this, I use the data set in which there are 5960 raws and 13 columns. For this project, I would like to answer four questions below. \n\nWhat kind of relations exist between variables in this data set?\nHow two group of being undefault and default differ ?\nIs the relationship between job category and default independent ?\nWhat factors affect being default greatly? and what are criterion to predict whether new clients who want to use equity loan could be default or not.\nBy answering these four questions, it would be very helpful for a bank or some institutions that issue home equity loan to automate the process of issuing home equity loan and know whether obligers that can not pay the loan back and factors that affect being default. Because of this reason, it is worth of doing the project and it is very meanigful to answer these four questions above.\n\n**About the data set**\n\nThe data that I would like to use is about a home equity loan. The data can be downloaded in a form of CSV from this website https:\/\/www.kaggle.com\/ajay1735\/hmeq-data. The size of data is 5960 raws and 13 columns. For this data set, time period is not mentioned from kaggle and the orginal website that the data comes from \n\nThe data set has the following characteristics: \n\n\u25cf BAD: 1 = client defaulted on loan 0 = loan repaid \n\n\u25cf LOAN: Amount of the loan request \n\n\u25cf MORTDUE: Amount due on existing mortgage \n\n\u25cf VALUE: Value of current property \n\n\u25cf REASON: DebtCon = debt consolidation; HomeImp = home improvement \n\n\u25cf JOB: Occupational categories \n\n\u25cf YOJ: Years at present job \n\n\u25cf DEROG: Number of major derogatory reports \n\n\u25cf DELINQ: Number of delinquent credit lines \n\n\u25cf CLAGE: Age of oldest credit line in months \n\n\u25cf NINQ: Number of recent credit inquiries \n\n\u25cf CLNO: Number of credit lines \n\n\u25cf DEBTINC: Debt-to-income ratio \n\nAll of unit for money is $**\n\n\n**Cleaning the data set**\n\nThere are many raws having missing value. Therefore, I drop the data that have missing value. The original data set is 5960 raws. After droping the data with missing value, the data size is 3364. The method that I use to do it is dropna. The codes is shows below.","5bcb6294":"**Desition tree**\n\nObjective: predicting whether the obligor can pay home equity loan or nor based on the several features. \nPossible classes: BAD=0(the obligor can pay the home equity loan) or BAD = 1 (the obligor can not pay the home equity loan) \nFeatures: all features except of categorial varibales such as JOB,BAD and REASON","858b9f80":"If we compare the group of being default and undefault based on LOAN (amount of the loan requested),MORTDUE(amount due on existing mortgage) and VALUE(value of current property), there are not big differnce between them. Therefore, it is hard to predict whether the obligoer can pay the loan back based on the amount of the loan requested, amount of due on existing mortgage and value of current property.","b5123020":"The number of the data in case of undefault","16947f36":"This is the number of raw and columns after droping","1d126dd4":"## Question 2 ##\n\n**The relation between group of being default and default**\n\nIn question 2, I try to figure out the relation between group of being default and default. Main question in question 2 is \"How two group of being undefault and default differ ?\" To answer this question, boxplot, bar graphs, displaying summary of stats data can be used.\n\nAt first, to do make analysis in this question, I make df_set_bad by setting BAD as index. In addition, to make a bar graph, I create data frame df_set_bad_mean_undefault and df_set_bad_mean_default to extract the mean value of LOAN,MORTDUE and VALUE in two groups of being default and undefault. In addition, to creat a bar plot, I use data frame \"df\", which is the one that I created firstly.","a71d435d":"# Naive Bayes Classifier #\n\nTwo possible classes: \"default\" and \"undefault\". \nFeatures:CLAGE(age of oldest credit lines) and DEBTINC(Debt to income ratio)","2ef57a05":"This is the null hypothesis and alternative hypothesis to answer the question above.\n\nNull Hypothesis -> two group of being undefault or default and job position of peple who have borrow home equity loan are independent \n\nAlternative Hypothesis -> two group of being undefault or default and about job position are not independent \n\nBy answering the question, we could figure out whether job positon of the obliger affects the results of being default or not.","27d1d8c5":"According to the graph above, most of them are predcted by the random forest.\n\nThen, I would like to do the cross varidation to see the best model for random forest.","7776c280":"From this result, VALUE(value of home) has a strong correlation to MORTDUE(amount due on existing mortgage). This does make sence because if there is high value of home, it is likely to be high amount due on exsiting morgage. In additon, low correlation betwen Value an LOAN(amount of the loan requested) shows the characteristic of the home equity loan. Home equity loan is used as a collateral of value of home. If we have high value of home, the owners of the home could issue high amount of home equity loan. However, the amount of money requested has low correlation to Value of home,which means that the owners of high value of home does not need the home equity loan and people who suffer from money are likely to become obligoer of home equity loan.","c6f3c5ce":"This is the acuuracy rate of 99%, which is very high to predict about being default and undefault","5d779468":"This is the model for random forest","de7829a1":"This is the graphs of the plot of observed data and prediceted data. Most of plots on the right side and left side are matched wit each other.","fe52294b":"This is the summary of each variable for those who can not pay home equity loan back\n\nIf we compare the mean of DELOG(Number of major derogatory reports),DELINQ(Number of delinquent credit lines) and DEBTINC(Debt to income ratio)between two groups of being default and undefault, there is a significant difference of the value. The mean value of DEROG for default group is about 6 times as high as the one for undefault group. In addition, the mean value of DELINQ for default group is about 4 times higher than the one for undefault group. Besides, the group of being default has higher debt to income ratio by 6% than the one of being undefault.","c6816897":"This is the summary of each variable for those who can pay home equity loan back","058cdc97":"From this result, DEBTINC(ratio of debt to income) is the most important variable among them. It shows that if the ratio of debt to income is high, the obligoer can not pay home equity loan back. In addition, CLAGE is the second important variable. This means that having other loans longer tends to lead to being default.","622dd07e":"According to the graph, people that mange their own company tend to have larger amount of loan request,amount of due on exsiting mortgage and value of current property than other jobs. In addition, By looking at bar plot of CLAGE and JOB, range of self is very small. This shows that age of oldest credit line in months is likely to be small and people who mange their own company tend to able to pay the equity loan back very shortly\n\n**Hypothesis test: Two groups of being undefault or default and about job position are independent or not ?** \n\nIn order to test it, we use chi square test. In this hypothesis, I use Alpha level 5% and 2-tailed test. At first, I make cross table for this.","94090e8c":"Since p value is 0.00029481726266075394, we can reject the null hypothesis,which means that two group of being undefault or default and about job position are not independent. It implies that job positions affect whether the obligoer return its home equity loan or not.","48929616":"This is the accuracy rate based on the model of the desition tree above. This implies that based on the criterio of each node above the graph of detision tree, banks or some insitution are prone to predict whether the obligor can pay the home equity loan back to them at 93%.","04626b25":"## Question 1 ##\n\n**The relation between variables**\n\nIn question 1, we try to understand the relation between variables. The major question in question 1 is \"What kind of relations exist between variables in this data set?\" I would like to use heat map and pair plots to answer the first question.\n\nTo do this analysis, I drop the categorical variables such as REASON,JOB and BAD and make \"df_con_bad\" from the data frame \"df\",which is the one droping the raws with missing value above. In addition, to do anaylysis of pairplot, I create df_pair,which is composed of LOAN,BAD,MORTDUE,VALUE,YOJ from \"df\". This is because we can not create heat map and pairplots well witout making new data frame,which is challenge that I was faced with.","7ee6d662":"The number of the data in case of default","b82694d2":"If we compare two groups of being default and undefault, most of them has no big difference between them. However, the variables such as DEROG and DELINQ has difference betwee them. People who can not pay the loan back are likely to have higher number of derogatory reports.In addition, The high number of deliquent credit lines tend to lead to the obligor being default. Based on this result, if there are high number of deliquent credit lines and derogatory reports, banks or some institution who are lending hoem equity loan should keep an eye on these obligor carefully and take some measurement in order not to lose the money that they are lending.","6e8b572c":"**Random Forest**\n\nObjective \npredicting whether the obligor can pay home equity loan or nor based on the several features.\n\nPossible classes \nBAD=0(the obligor can pay the home equity loan) or BAD = 1 (the obligor can not pay the home equity loan)\n\nFeatures \nall features except of categorial varibales such as JOB,BAD and REASON","58bfd57b":"## Question 4 ##\n\n**Prediction through clasification methods such as decetion tree, random forest and Naive Bayes Classifier**\n\nIn question 4, I would like to predict being default or not based on clasification models such as decision tree, random forest and bayes classifiers. The main theme in this question is \"what factors affect default greatly? and what are criterion to predict whether new clients who want to use equity loan could be default or not.\n\n\nTo do classification, I split 70 % of the data into train data and 30% of the data into test data. In addition, new data frames df_train_drop_cate and df_test_drop_cate that drop categorical variables are created. By using these data frame, classification models such as Desition tree, random forest and Bayes Naives"}}