{"cell_type":{"bbbb39e7":"code","95d8b4f5":"code","5f229ecd":"code","74bce06e":"code","eaff77d3":"code","10d28397":"code","01624cee":"code","a3e89a35":"code","891e3ff0":"code","725088a5":"code","66536b57":"code","ecaf9ba6":"code","686ac545":"code","e6f20f57":"code","7c01af54":"code","c7ceb9ca":"code","b2a9198c":"code","fc7c158b":"code","3bffa4ed":"code","4b6011a9":"code","5ab4a62f":"code","160ee115":"code","87b75d21":"code","46b410c1":"code","236f59a0":"code","5ed2bd9b":"code","29e4917c":"code","3f134800":"markdown","3230ef42":"markdown","be5d2fd9":"markdown","803f470f":"markdown","9ac6b0f0":"markdown","521e5591":"markdown","8bdeaa07":"markdown","f334e340":"markdown","44dc6495":"markdown","c8eedc79":"markdown","674dc86d":"markdown","ab045eb9":"markdown","14910c1b":"markdown","7dce24f2":"markdown","cd6d40f1":"markdown","ed19f51e":"markdown","a7b2e388":"markdown","c393021a":"markdown","a62a20b5":"markdown","0da6072e":"markdown","96110e04":"markdown","b1ddc62b":"markdown","646eee4b":"markdown","d1ff52f4":"markdown","b90a33c2":"markdown","a0fcb1e9":"markdown","70d533e7":"markdown","22152448":"markdown","b3b026c6":"markdown","edcdfeb7":"markdown","2a1d2307":"markdown","7e16f039":"markdown","6153eaf0":"markdown","5af70e3b":"markdown","27b236a3":"markdown","8cd003b5":"markdown","ae1f12b2":"markdown","d8c5d64a":"markdown","63422725":"markdown","d07fe2e7":"markdown","3d513928":"markdown","e6459cb1":"markdown","49c16bda":"markdown","ef7d486d":"markdown","1e8899e6":"markdown","853b1620":"markdown","6330689c":"markdown","87f5a159":"markdown","b53e0ecc":"markdown","bc885a44":"markdown","d840ac83":"markdown"},"source":{"bbbb39e7":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","95d8b4f5":"import gensim\nfrom gensim import corpora\nfrom pprint import pprint","5f229ecd":"documents = [\"The Saudis are preparing a report that will acknowledge that\", \n             \"Saudi journalist Jamal Khashoggi's death was the result of an\", \n             \"interrogation that went wrong, one that was intended to lead\", \n             \"to his abduction from Turkey, according to two sources.\"]\n\ndocuments_2 = [\"One source says the report will likely conclude that\", \n                \"the operation was carried out without clearance and\", \n                \"transparency and that those involved will be held\", \n                \"responsible. One of the sources acknowledged that the\", \n                \"report is still being prepared and cautioned that\", \n                \"things could change.\"]","74bce06e":"# Tokenize(split) the sentences into words\ntexts=[[text for text in doc.split()] for doc in documents]\nprint(texts)\ntexts[0]\n","eaff77d3":"# Create dictionary\ndictionary=corpora.Dictionary(texts)\n\nprint(dictionary)","10d28397":"# Show the word to id map\nprint(dictionary.token2id)","01624cee":"#If you get new documents in the future, it is also possible to update an existing dictionary to include the new words.\n\ntexts=[[line for line in doc.split()] for doc in documents_2]  #documents_2 was made where documents was created\n\ndictionary.add_documents(texts)","a3e89a35":"from gensim.utils import simple_preprocess","891e3ff0":"#Repeating the above steps\n# Tokenize the docs\ntokenized_list = [simple_preprocess(doc) for doc in documents]\n\n# Create the Corpus\nmydict = corpora.Dictionary()\ncorpus=[mydict.doc2bow(doc , allow_update = True) for doc in tokenized_list ]\npprint(corpus)\n","725088a5":"# Show the Word Weights in Corpus\nfor doc in corpus:\n\n    print([[mydict[id], freq] for id, freq in doc])","66536b57":"from gensim import models\nimport numpy as np","ecaf9ba6":"# Create the TF-IDF model\n\ntfidf=models.TfidfModel(corpus , smartirs='ntc')","686ac545":"# Show the TF-IDF weights\nfor doc in tfidf[corpus]:\n    print([[mydict[id], np.around(freq, decimals=2)] for id, freq in doc])","e6f20f57":"from gensim.models import LdaModel , LdaMulticore\nfrom gensim.utils import simple_preprocess\nimport nltk \nfrom nltk import WordNetLemmatizer\nfrom nltk.corpus import stopwords\nimport re\nimport string\n","7c01af54":"#Creating a list of stopwords and punctuations. These are in-built functions of their respective libraries.\n#You can make your own as well\nstop_words = stopwords.words('english') +['the','a','an']\npunc = string.punctuation","c7ceb9ca":"documents = ['''The UEFA Champions League (abbreviated as UCL, also known as the European Cup) is an annual club football competition organised by the Union of European Football Associations (UEFA) and contested by top-division European clubs, deciding the competition winners through a group and knockout format. It is one of the most prestigious football tournaments in the world and the most prestigious club competition in European football, played by the national league champions (and, for some nations, one or more runners-up) of their national associations.\nIn its present format, the Champions League begins in late June with a preliminary round, three qualifying rounds and a play-off round, all played over two legs. The six surviving teams enter the group stage, joining 26 teams qualified in advance. The 32 teams are drawn into eight groups of four teams and play each other in a double round-robin system. The eight group winners and eight runners-up proceed to the knockout phase that culminates with the final match in late May or early June.[5] The competition has been won by 22 clubs, 12 of which have won it more than once.[8] Real Madrid is the most successful club in the tournament's history, having won it 13 times, including its first five seasons. Liverpool are the reigning champions, having beaten Tottenham Hotspur 2\u20130 in the 2019 final. Spanish clubs have the highest number of victories (18 wins), followed by England (13 wins) and Italy (12 wins). ''',\n             \n'''Machine learning involves computers discovering how they can perform tasks without being explicitly programmed to do so.\n    It involves computers learning from data provided so that they carry out certain tasks. For simple tasks assigned to computers,\n    it is possible to program algorithms telling the machine how to execute all steps required to solve the problem at hand; \non the computer's part, no learning is needed. For more advanced tasks, it can be challenging for a human to manually create the needed algorithms. In practice, it can turn out to be more effective to help the machine develop its own algorithm, rather than having human programmers specify every needed step.Types of supervised learning algorithms include Active learning, classification and regression.[30] Classification algorithms are used when the outputs are restricted to a limited set of values, and regression algorithms are used when the outputs may have any numerical value within a range. Manifold learning algorithms attempt to do so under the constraint that the learned representation is low-dimensional. Sparse coding algorithms attempt to do so under the constraint that the learned representation is sparse, meaning that the mathematical model has many zeros. \nMultilinear subspace learning algorithms aim to learn low-dimensional representations directly from tensor representations for multidimensional data, without reshaping them into higher-dimensional vectors.[42] Deep learning algorithms discover multiple levels of representation, or a hierarchy of features, with higher-level, more abstract features defined in terms of (or generating) lower-level features. It has been argued that an intelligent machine is one that learns a representation that disentangles the underlying factors of variation that explain the observed data.[43]''', \n             \n             \n             \n             '''India Coronavirus Cases: Pune has now overtaken Mumbai as the city with the maximum number of novel Coronavirus infections in Maharashtra. In fact, after Delhi, it has the highest number of cases in the entire country\nMost people infected with the COVID-19 virus will experience mild to moderate respiratory illness and recover without requiring special treatment.  Older people, and those with underlying medical problems like cardiovascular disease, diabetes, chronic respiratory disease, and cancer are more likely to develop serious illness.\nThe best way to prevent and slow down transmission is be well informed about the COVID-19 virus, the disease it causes and how it spreads. Protect yourself and others from infection by washing your hands or using an alcohol based rub frequently and not touching your face. \nWith 941 casualties in the past 24 hours, the toll in India topped 50,000 (50,921) on Monday. As many as 57,982 new cases were reported in the country, taking the tally to 26,47,664. Of the 26 lakh cases, at least 19 lakh have recovered, while over 6.7 lakh are still active. India has been reporting over 60,000 cases daily since August 7, barring August 11 when the country registered 53,601 new instances of the infection.\n       At this time, there are no specific vaccines or treatments for COVID-19. However, there are many ongoing clinical trials evaluating potential treatments. WHO will continue to provide updated information as soon as clinical findings become available.Coronavirus India News Live Updates: The Bihar government Monday extended the lockdown in its state till September 6 in view of the prevailing COVID-19 situation, According to a notification issued by the Home Department restrictions will remain in place in the district headquarters,\n       sub-divisional headquarters, block headquarters and all municipal areas.         \n             ''']\n","b2a9198c":"from nltk.corpus import wordnet\n\ndef get_wordnet_pos(word):\n    \n    \n    tag = nltk.pos_tag([word])[0][1][0].upper()\n    tag_dict = {\"J\": wordnet.ADJ,\n                \"N\": wordnet.NOUN,\n                \"V\": wordnet.VERB,\n                \"R\": wordnet.ADV}\n\n    return tag_dict.get(tag, wordnet.NOUN) ","fc7c158b":"data_processed=[]\nfor line in documents:\n    lemmatizer=WordNetLemmatizer()\n    doc_out=[]\n    for word in line.split() : \n        word=''.join([w.lower() for w in word if w not in ['[',']','(',')',',','.']]) #Removing [] from words like [2] etc as seen in wiki documents\n        if word not in stop_words and word not in punc:\n            lemmatized_wd= lemmatizer.lemmatize(word, get_wordnet_pos(word))#lemmatizing the words with its part of speech tag\n            if lemmatized_wd:\n                 doc_out.append(lemmatized_wd)\n        else : \n            continue\n    data_processed.append(doc_out)\nprint(data_processed)","3bffa4ed":"new_dict=corpora.Dictionary(data_processed)\ncorpus=[new_dict.doc2bow(line) for line in data_processed]\nprint(corpus)","4b6011a9":"lda_model= LdaMulticore(corpus=corpus,id2word=new_dict,random_state=100,num_topics=3,\n                         alpha='asymmetric',\n                         decay=0.5,\n                         iterations=200)\n\nlda_model.print_topics(-1)","5ab4a62f":"from gensim.models import LsiModel","160ee115":"lsi_model = LsiModel(corpus=corpus, id2word=new_dict, num_topics=3, decay=0.5)","87b75d21":"pprint(lsi_model.print_topics(-1))","46b410c1":"document_test=[\n    '''The top-ranked UEFA competition is the UEFA Champions League, which started in the 1992\/93 season and gathers the top 1\u20134 teams of each country's league (the number of teams depend on that country's ranking and can be upgraded or downgraded); this competition was re-structured from a previous one that only gathered the top team of each country (held from 1955 to 1992 and known as the European Champion Clubs' Cup or simply the European Cup).\n\nA second, lower-ranked competition is the UEFA Europa League. This competition, for national knockout cup winners and high-placed league teams, was launched by UEFA in 1971 as a successor of both the former UEFA Cup and the Inter-Cities Fairs Cup (also begun in 1955). A third competition, the UEFA Cup Winners' Cup, which had started in 1960, was absorbed into the UEFA Cup (now UEFA Europa League) in 1999.\n\nIn December 2018, UEFA announced the creation of a third club competition, with a working title of Europa League 2 (UEL2) (The name was later decided as UEFA Europa Conference League) . The competition would feature 32 teams directly in 8 groups of 4, with a knockout round between the second placed teams in UEFA Europa Conference League and the third placed teams in the Europa League, leading to a final 16 knockout stage featuring the eight group winners. UEFA announced that the first edition of the competition begins in 2021 [12].''',\n    \n    '''In March, after the lockdown was imposed, the United Nations (UN) and the World Health Organization (WHO) praised \n    India's response to the pandemic as 'comprehensive and robust,' terming the lockdown restrictions as 'aggressive but vital' \n    for containing the spread and building necessary healthcare infrastructure. At the same time, the Oxford COVID-19 Government Response\n    Tracker (OxCGRT) noted the government's swift and stringent actions, emergency policy-making, emergency investment in health care, \n    fiscal stimulus, investment in vaccine and drug R&D and gave India a score of 100 for the strict response.\n    Also in March, Michael Ryan, chief executive director of the WHO's health emergencies programme noted that India had tremendous \n    \\capacity to deal with the outbreak owing to its vast experience in eradicating smallpox and polio.[20][21][22] Other commentators\n    have raised concerns about the economic fallout arising as a result of the pandemic and preventive restrictions.[23][24] The lockdown was justified by the government and other agencies for being preemptive to prevent India from entering a higher stage which could make handling very difficult and cause even more losses thereafter.[25][26]'''\n]","236f59a0":"data_processed=[]\nfor line in document_test:\n    lemmatizer=WordNetLemmatizer()\n    doc_out=[]\n    for word in line.split() : \n        word=''.join([w.lower() for w in word if w not in ['[',']','(',')',',','.']])\n        if word not in stop_words and word not in punc:\n            lemmatized_wd= lemmatizer.lemmatize(word, get_wordnet_pos(word))\n            if lemmatized_wd:\n                 doc_out.append(lemmatized_wd)\n        else : \n            continue\n    data_processed.append(doc_out)\n\ncorpus_test=[new_dict.doc2bow(line) for line in data_processed]","5ed2bd9b":"top_topics = lda_model.get_document_topics(corpus_test, minimum_probability=0.0)","29e4917c":"for i in top_topics:\n    print(i)\n    max=-1\n    topic=1\n    for topic_no , predict in i:\n        if predict>max:\n            max=predict\n            topic=topic_no\n    print(topic,max)\n            ","3f134800":"In both cases you need to provide the number of topics as input. The model, in turn, will provide the topic keywords for each topic and the percentage contribution of topics in each document.\n\n> **The quality of topics is highly dependent on the quality of text processing and the number of topics you provide to the algorithm**","3230ef42":"If one wants to add words to the above dictioinary, it is possible as well.","be5d2fd9":"The above steps of converting the documents into list of lists can be done easily by simple_preprocess.","803f470f":"To use a gensim model on a set of documents, it is first required to convert the documents in suitable format known as the corpus. \n> Gensim requires the words to be converted into unique ids and these ids are matched from a dictionary which contains the word id pair.","9ac6b0f0":"> We'll see an example of topic modeling further in the notebook.","521e5591":"Above I have used the LDA model. \nThe outcome of the model is divided into **3 topics namely 0 ,1 ,2** . If you notice carefully , each topic has the words which were present in the respective documents.\n> The machine is smart enough to only distribute the words into topics with labels as numbers but not with a topic name. In our case , we can name the topics as follows:\n* 0 -> Machine Learning , \n* 1 -> Coronavirus , \n* 2 -> UCL Football.","8bdeaa07":"The Term Frequency \u2013 Inverse Document Frequency(TF-IDF) is also a bag-of-words model but unlike the regular corpus, TFIDF down weights tokens (words) that appears frequently across documents.","f334e340":"![maxresdefault.jpg](attachment:maxresdefault.jpg)","44dc6495":"Before creating our corpus , we need to to clean our data. This involves **removing**:\n> 1. stopwords(the , and ,is ,are etc)(we can add our own stopwords as well) \n> 2. punctuation marks\n> 3. Lemmatizing the words (here I have also provided the part of speech Tag for lemmatizing)","c8eedc79":"Gensim is billed as a Natural Language Processing package that does \u2018Topic Modeling for Humans\u2019. Topic modeling is basically a technique to extract the topics from large volumes of text and in turn identify these topics in some new data to classify the new data in the respective topics. ","674dc86d":"**Here you can see the words with their respective ids.**","ab045eb9":"**This notebook is about how one can create a Machine Learning Model to predict what a statement is about, that is to determine the topic of a given document using the Gensim library.**","14910c1b":"# Data Preprocessing (corpus creation) ","7dce24f2":"**Myself <a href='https:\/\/www.linkedin.com\/in\/aryan-parab-0b44991b2\/'>Aryan Parab<\/a> and my team member <a href='https:\/\/www.linkedin.com\/in\/aden-gomes-2b2a091b1\/'>Aden Gomes<\/a> created many such notebooks as part of the course work under \"Master in Data Science Programme\" at <a href='https:\/\/datascience.suvenconsultants.com\/'>Suven<\/a> , under mentor-ship of <a href='https:\/\/www.linkedin.com\/in\/rocky-jagtiani-3b390649\/'>Rocky Jagtiani<\/a> .**","cd6d40f1":"The Dictionary 'new_dict' created here will be used again on the new data.","ed19f51e":"Now lets use the above models to predict the topics for the following documents in the following lists.","a7b2e388":"# Implementing LDA","c393021a":"The doc2bow is very important. **It is converting the above list of words and dictionary into a bag of words. From this bag of words, it uses the unique ids assigned to each word and maps them to in their respective positions in the corpus.** \n> The item (0,1) means the word with unique id 0 and its frequency in that list(document) is 1. \n","a62a20b5":"For more details about the LDA parameters, check out :- https:\/\/radimrehurek.com\/gensim\/models\/ldamodel.html","0da6072e":"Firstly, breaking down each statement (document) in the list into a list of words.","96110e04":"Now lets see what the LSI model gives us as an output and after that understand what the output is.","b1ddc62b":"# Outcome","646eee4b":"From above , we have got our corpus in the required format. Its time for the models.","d1ff52f4":"Now its time to create the model. \n","b90a33c2":"First Step is to clean the Input and making a corpus.","a0fcb1e9":"The outcome of LDA and LSI provides us :\n>     1.The topic(s) that document belongs to along with percentage.\n>     2.The topic(s) each word in that document belongs to AND the phi values.\n>     \n","70d533e7":"Above is our processed data.","22152448":"> Here we are using corpora from the gensim library to get the required dictionary.","b3b026c6":"In the Lda model ,we are providing the corpus , the mapping dictionary and number of topics (=3).","edcdfeb7":"I would like to humbly and sincerely thank my mentor <a href= 'https:\/\/www.linkedin.com\/in\/rocky-jagtiani-3b390649\/'> Rocky Jagtiani<\/a>. He is more of a friend to me then mentor. The Python for Data Science taught by him and various assignments we did and are still doing is the best way to learn and skill in Data Science field. ","2a1d2307":"Compare the outcome with the original to understand what I did.","7e16f039":"**For our model training , I am using a list which consists of three documents on the topics of UEFA Champions League Football , info on Machine Learning and details about coronavirus.**","6153eaf0":"From the outcome we can see that:\n1. The first list has the maximum probability for topic 2 that is UCL Football topic and is predicted correctly.\n2. The second list gives us maximum probability for topic 1 which is the Coronavirus topic and it is also predicted correctly.","5af70e3b":"**Remember , more the data used to train the model more effective results will be generated for a vast variety of data.**","27b236a3":"> Just for your information , Each document has a word count of around 250 words.","8cd003b5":"Here are the necessary imports for the upcoming steps.","ae1f12b2":"# Creating the Corpus","d8c5d64a":"Notice the difference in weights of the words between the original corpus and the tfidf weighted corpus.\n\nThe words \u2018was\u2019 and \u2018the\u2019 occuring multiple times in the documents were weighted down. In simple terms, words that occur more frequently across the documents get smaller weights(as these might not carry much information about a particular topic).","63422725":"What is phi value?\n> **Phi value is the probability of the word belonging to that particular topic. And the sum of phi values for a given word adds up to the number of times that word occurred in that document.**","d07fe2e7":"Now using the above derieved list of lists, we will be creating a dictionary of unique words with their uniques ids.","3d513928":"I have used the LDA model here to predict and not the LSI. Make sure you are viewing the proper models while cross verifying the results.\n> You can also try using the LSI model. ","e6459cb1":"# Gensim","49c16bda":"This means that for the first document, the probabilty of it being a :\n1. Machine learning Topic -> 0.656%\n2. Coronavirus Topic      -> 0.504%\n3. UCL Football           -> 98.83%\n","ef7d486d":"Therefore, our model can now be used on new data.\n","1e8899e6":"Below is the word form of the above corpus . ","853b1620":"The final outcome after the doc2bow is called known as the 'corpus'. Before providing any documents to the our model(which will be derieved in a short while ) the documents are to be converted into a corpus. ","6330689c":"The objective of topic models is to extract the underlying topics from a given collection of text documents. Each document in the text is considered as a combination of topics and each topic is considered as a combination of related words.\n\n**Topic modeling can be done by algorithms like Latent Dirichlet Allocation (LDA) and Latent Semantic Indexing (LSI).**","87f5a159":"See https:\/\/datascience.suvenconsultants.com once for more","b53e0ecc":"Here the first test document is about UCL Football while the second is about Coronavirus.","bc885a44":"# The LDA Model","d840ac83":"Below lists are created to use as **an example to follow through the various steps of processing of the data** before topic modeling."}}