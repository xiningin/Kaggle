{"cell_type":{"b6d0be71":"code","e3703d58":"code","c8dda9c9":"code","a09f669f":"code","e41ece82":"code","ddba3271":"code","800ccfff":"code","43535a8a":"code","8d80aa59":"code","3b4aae06":"code","ea0a163e":"code","70bbe7ed":"code","3d5e09d0":"code","66587989":"code","7c3f78f2":"code","a238696c":"code","54ff1833":"code","fb3ed2d4":"code","dcd10f57":"code","1214bd8a":"code","4e5ec9d3":"code","3a1af174":"code","26a817ba":"code","1cb0d226":"code","6d6c9843":"code","063b5fd5":"code","03e832fd":"code","cc97a592":"code","76d24acc":"code","ef8f0a56":"code","51320114":"code","521d753e":"code","f9c40fbb":"code","0ded3a47":"markdown","c0287b52":"markdown","d47d5f24":"markdown","82ca283b":"markdown","f95483df":"markdown","b2dcbc65":"markdown","c84d36cc":"markdown","38445a68":"markdown","5d55822f":"markdown","670545f3":"markdown","69eea1b2":"markdown","d148b77d":"markdown","0743bc1d":"markdown","35ba5a83":"markdown","bd60af83":"markdown","f961352a":"markdown","ff086df5":"markdown"},"source":{"b6d0be71":"#########importing libraries \nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport csv\nimport pandas as pd#pandas to create small dataframes \nimport datetime #Convert to unix time\nimport time #Convert to unix time\n# if numpy is not installed already : pip3 install numpy\nimport numpy as np#Do aritmetic operations on arrays\n# matplotlib: used to plot graphs\nimport matplotlib\nimport matplotlib.pylab as plt\nimport seaborn as sns#Plots\nfrom matplotlib import rcParams#Size of plots  \nfrom sklearn.cluster import MiniBatchKMeans, KMeans#Clustering\nimport math\nimport pickle\nimport os\n# to install xgboost: pip3 install xgboost\nimport xgboost as xgb\n\nimport warnings\nimport networkx as nx  ###for graph technique \nimport pdb\nimport pickle","e3703d58":"\ntraincsv = pd.read_csv('..\/input\/social-network-friend-recomendation\/data\/train.csv')\nprint(traincsv[traincsv.isna().any(1)])\nprint(traincsv.info())\nprint(\"Number of duplicate entries: \",sum(traincsv.duplicated()))\ntraincsv.to_csv('..\/train_woheader.csv',header=False,index=False)\nprint(\"saved the graph into file\")\n#making graph \ng=nx.read_edgelist('..\/train_woheader.csv',delimiter=',',create_using=nx.DiGraph(),nodetype=int)\nprint(nx.info(g))","c8dda9c9":"sample=pd.read_csv('..\/input\/social-network-friend-recomendation\/data\/train.csv', nrows=50).to_csv('train_woheader_sample.csv',header=False,index=False)\n    \nsubgraph=nx.read_edgelist('train_woheader_sample.csv',delimiter=',',create_using=nx.DiGraph(),nodetype=int)\n\npos=nx.spring_layout(subgraph)\nnx.draw(subgraph,pos,node_color='#A0CBE2',edge_color='#00bb5e',width=1,edge_cmap=plt.cm.Blues,with_labels=True)\n#plt.savefig(\"graph_sample.pdf\")\nprint(nx.info(subgraph))","a09f669f":"print(\"Number of unique person\",len(g.nodes()))","e41ece82":"indegree_dist = list(dict(g.in_degree()).values())\nprint(len(g.in_degree()))\nprint(max(indegree_dist))\nindegree_dist.sort()\n\nplt.figure(figsize=(10,6))\nplt.plot(indegree_dist)\nplt.xlabel('Index No')\nplt.ylabel('No Of Followers')\nplt.show()","ddba3271":"plt.boxplot(indegree_dist)\nplt.title(\"no of followers\")\nplt.show()","800ccfff":"sns.distplot(indegree_dist, color='#16A085')","43535a8a":"outdegree_dist = list(dict(g.out_degree()).values())\nprint(len(g.out_degree()))\nprint(max(outdegree_dist))\noutdegree_dist.sort()\n\nplt.figure(figsize=(10,6))\nplt.plot(outdegree_dist)\nplt.xlabel('Index No')\nplt.ylabel('No Of Following')\nplt.show()","8d80aa59":"plt.boxplot(outdegree_dist)\nplt.title(\"no of following\")\nplt.show()","3b4aae06":"sns.distplot(outdegree_dist, color='#16A085')","ea0a163e":"from collections import Counter\ndict_in = list(dict(g.in_degree()).values())\ndict_out = list(dict(g.out_degree()).values())\n#print(len(dict_in))\n","70bbe7ed":"inoutdegree=list(dict_in+dict_out)\ninoutdegree.sort()\n\nplt.figure(figsize=(10,6))\nplt.plot(inoutdegree)\nplt.xlabel('Index No')\nplt.ylabel('No Of Following+followers')\nplt.show()","3d5e09d0":"plt.boxplot(inoutdegree)\nplt.title(\"no of following+followers\")\nplt.show()","66587989":"sns.distplot(inoutdegree,color='#16A085')","7c3f78f2":"import random\nr = csv.reader(open('..\/train_woheader.csv','r'))\nedges = dict()\nfor edge in r:\n        edges[(edge[0], edge[1])] = 1\n        \n        \nmissing_edges = set([])\nwhile (len(missing_edges)<94375):\n        a=random.randint(1, 1862220)\n        b=random.randint(1, 1862220)\n        tmp = edges.get((a,b),-1)\n        if tmp == -1 and a!=b:\n            try:\n                if nx.shortest_path_length(g,source=a,target=b) > 2: \n\n                    missing_edges.add((a,b))\n                else:\n                    continue  \n            except:  \n                    missing_edges.add((a,b))              \n        else:\n            continue\n\nprint(len(missing_edges))\n","a238696c":"from sklearn.model_selection import train_test_split\ndf_neg = pd.DataFrame(list(missing_edges), columns=['source_node', 'destination_node'])\ndf_pos = pd.read_csv('..\/input\/social-network-friend-recomendation\/data\/train.csv')\nprint(df_neg.head())\nprint(df_pos.head())\nprint(df_neg.shape)\nprint(df_pos.shape)\nX_train_pos, X_test_pos, y_train_pos, y_test_pos  = train_test_split(df_pos,np.ones(len(df_pos)),test_size=0.2, random_state=9)\nX_train_neg, X_test_neg, y_train_neg, y_test_neg  = train_test_split(df_neg,np.zeros(len(df_neg)),test_size=0.2, random_state=9)\n\nprint('='*60)\nprint(\"Number of nodes in the train data graph with edges\", X_train_pos.shape[0],\"=\",y_train_pos.shape[0])\nprint(\"Number of nodes in the train data graph without edges\", X_train_neg.shape[0],\"=\", y_train_neg.shape[0])\nprint('='*60)\nprint(\"Number of nodes in the test data graph with edges\", X_test_pos.shape[0],\"=\",y_test_pos.shape[0])\nprint(\"Number of nodes in the test data graph without edges\", X_test_neg.shape[0],\"=\",y_test_neg.shape[0])\n\n    #removing header and saving\nX_train_pos.to_csv('..\/train_pos_after_eda.csv',header=False, index=False)\nX_test_pos.to_csv('..\/test_pos_after_eda.csv',header=False, index=False)\nX_train_neg.to_csv('..\/train_neg_after_eda.csv',header=False, index=False)\nX_test_neg.to_csv('..\/test_neg_after_eda.csv',header=False, index=False)\n\n\nX_train_pos = pd.read_csv('..\/train_pos_after_eda.csv', names=['source_node', 'destination_node'])\nX_test_pos = pd.read_csv('..\/test_pos_after_eda.csv', names=['source_node', 'destination_node'])\nX_train_neg = pd.read_csv('..\/train_neg_after_eda.csv', names=['source_node', 'destination_node'])\nX_test_neg = pd.read_csv('..\/test_neg_after_eda.csv', names=['source_node', 'destination_node'])\n\n##concatinating x trian pos+ x trian neg ,  y_train pos+y_test_neg\nX_train = X_train_pos.append(X_train_neg,ignore_index=True)\ny_train = np.concatenate((y_train_pos,y_train_neg))\nX_test = X_test_pos.append(X_test_neg,ignore_index=True)\ny_test = np.concatenate((y_test_pos,y_test_neg)) \n\n\n#####\nX_train.to_csv('..\/train_after_eda.csv',header=False,index=False)\nX_test.to_csv('..\/test_after_eda.csv',header=False,index=False)\n#final dataset\npd.DataFrame(y_train.astype(int)).to_csv('..\/train_y.csv',header=False,index=False)\npd.DataFrame(y_test.astype(int)).to_csv('..\/test_y.csv',header=False,index=False)","54ff1833":"train_graph=nx.read_edgelist('..\/train_pos_after_eda.csv',delimiter=',',create_using=nx.DiGraph(),nodetype=int)\nprint(nx.info(train_graph))","fb3ed2d4":"def jaccard_for_followees(a,b):\n    try:\n        if len(set(train_graph.successors(a))) == 0  | len(set(train_graph.successors(b))) == 0:\n            return 0\n        sim = (len(set(train_graph.successors(a)).intersection(set(train_graph.successors(b)))))\/\\\n                                    (len(set(train_graph.successors(a)).union(set(train_graph.successors(b)))))\n    except:\n        return 0\n    return sim\ndef jaccard_for_followers(a,b):\n    try:\n        if len(set(train_graph.predecessors(a))) == 0  | len(set(g.predecessors(b))) == 0:\n            return 0\n        sim = (len(set(train_graph.predecessors(a)).intersection(set(train_graph.predecessors(b)))))\/\\\n                                 (len(set(train_graph.predecessors(a)).union(set(train_graph.predecessors(b)))))\n        return sim\n    except:\n        return 0","dcd10f57":"def cosine_for_followees(a,b):\n    try:\n        if len(set(train_graph.successors(a))) == 0  | len(set(train_graph.successors(b))) == 0:\n            return 0\n        sim = (len(set(train_graph.successors(a)).intersection(set(train_graph.successors(b)))))\/\\\n                                    (math.sqrt(len(set(train_graph.successors(a)))*len((set(train_graph.successors(b))))))\n        return sim\n    except:\n        return 0\ndef cosine_for_followers(a,b):\n    try:\n        \n        if len(set(train_graph.predecessors(a))) == 0  | len(set(train_graph.predecessors(b))) == 0:\n            return 0\n        sim = (len(set(train_graph.predecessors(a)).intersection(set(train_graph.predecessors(b)))))\/\\\n                                     (math.sqrt(len(set(train_graph.predecessors(a))))*(len(set(train_graph.predecessors(b)))))\n        return sim\n    except:\n        return 0","1214bd8a":"#if has direct edge then deleting that edge and calculating shortest path\ndef compute_shortest_path_length(a,b):\n    p=-1\n    try:\n        if train_graph.has_edge(a,b):\n            train_graph.remove_edge(a,b)\n            p= nx.shortest_path_length(train_graph,source=a,target=b)\n            train_graph.add_edge(a,b)\n        else:\n            p= nx.shortest_path_length(train_graph,source=a,target=b)\n        return p\n    except:\n        return -1","4e5ec9d3":"def calc_adar_in(a,b):\n    sum=0\n    try:\n        n=list(set(train_graph.successors(a)).intersection(set(train_graph.successors(b))))\n        if len(n)!=0:\n            for i in n:\n                sum=sum+(1\/np.log10(len(list(train_graph.predecessors(i)))))\n            return sum\n        else:\n            return 0\n    except:\n        return 0","3a1af174":"def follows_back(a,b):\n    if train_graph.has_edge(b,a):\n        return 1\n    else:\n        return 0","26a817ba":"df_final_train = pd.read_csv('..\/train_after_eda.csv', names=['source_node', 'destination_node'])\ndf_final_train['indicator_link'] = pd.read_csv('..\/train_y.csv', names=['indicator_link'])\nprint(\"Our train matrix size \",df_final_train.shape)\ndf_final_train.head(2)\ny_train = df_final_train['indicator_link']\n\n#test \ndf_final_test = pd.read_csv('..\/test_after_eda.csv', names=['source_node', 'destination_node'])\ndf_final_test['indicator_link'] = pd.read_csv('..\/test_y.csv', names=['indicator_link'])\nprint(\"Our test matrix size \",df_final_test.shape)\ndf_final_test.head(2)\ny_test=df_final_test['indicator_link']","1cb0d226":"df_final_train.head(2)","6d6c9843":"from tqdm import tqdm\nWeight_in = {}\nWeight_out = {}\nfor i in  tqdm(train_graph.nodes()):\n    s1=set(train_graph.predecessors(i))\n    w_in = 1.0\/(np.sqrt(1+len(s1)))\n    Weight_in[i]=w_in\n    \n    s2=set(train_graph.successors(i))\n    w_out = 1.0\/(np.sqrt(1+len(s2)))\n    Weight_out[i]=w_out\n    \n#for imputing with mean\nmean_weight_in = np.mean(list(Weight_in.values()))\nmean_weight_out = np.mean(list(Weight_out.values()))","063b5fd5":"print(df_final_test.shape)\nprint(df_final_train.shape)","03e832fd":"if not os.path.isfile('data\/fea_sample\/storage_sample_stage1.h5'):\n    #mapping jaccrd followers to train and test data\n    df_final_train['jaccard_followers'] = df_final_train[0:94375].apply(lambda row:\n                                            jaccard_for_followers(row['source_node'],row['destination_node']),axis=1)\n    df_final_test['jaccard_followers'] = df_final_test[0:24361].apply(lambda row:\n                                            jaccard_for_followers(row['source_node'],row['destination_node']),axis=1)\n\n    #mapping jaccrd followees to train and test data\n    df_final_train['jaccard_followees'] = df_final_train[:94375].apply(lambda row:\n                                            jaccard_for_followees(row['source_node'],row['destination_node']),axis=1)\n    df_final_test['jaccard_followees'] = df_final_test[0:24361].apply(lambda row:\n                                            jaccard_for_followees(row['source_node'],row['destination_node']),axis=1)\n    \n\n        #mapping jaccrd followers to train and test data\n    df_final_train['cosine_followers'] = df_final_train[:94375].apply(lambda row:\n                                            cosine_for_followers(row['source_node'],row['destination_node']),axis=1)\n    df_final_test['cosine_followers'] = df_final_test[0:24361].apply(lambda row:\n                                            cosine_for_followers(row['source_node'],row['destination_node']),axis=1)\n\n                                          \n    df_final_train['adar_index'] = df_final_train[:94375].apply(lambda row: calc_adar_in(row['source_node'],row['destination_node']),axis=1)\n    #mapping adar index on test\n    df_final_test['adar_index'] = df_final_test[0:24361].apply(lambda row: calc_adar_in(row['source_node'],row['destination_node']),axis=1)\n\n    #--------------------------------------------------------------------------------------------------------\n    #mapping followback or not on train\n    df_final_train['follows_back'] = df_final_train[:94375].apply(lambda row: follows_back(row['source_node'],row['destination_node']),axis=1)\n\n    #mapping followback or not on test\n    df_final_test['follows_back'] = df_final_test[0:24361].apply(lambda row: follows_back(row['source_node'],row['destination_node']),axis=1)\n    df_final_train['shortest_path'] = df_final_train[:94375].apply(lambda row: compute_shortest_path_length(row['source_node'],row['destination_node']),axis=1)\n    #mapping shortest path on test\n    df_final_test['shortest_path'] = df_final_test[0:24361].apply(lambda row: compute_shortest_path_length(row['source_node'],row['destination_node']),axis=1)\n    df_final_train['weight_in'] = df_final_train[:94375].destination_node.apply(lambda x: Weight_in.get(x,mean_weight_in))\n    df_final_train['weight_out'] = df_final_train[:94375].source_node.apply(lambda x: Weight_out.get(x,mean_weight_out))\n\n    #mapping to pandas test\n    df_final_test['weight_in'] = df_final_test[0:24361].destination_node.apply(lambda x: Weight_in.get(x,mean_weight_in))\n    df_final_test['weight_out'] = df_final_test[0:24361].source_node.apply(lambda x: Weight_out.get(x,mean_weight_out))\n\n\n    #some features engineerings on the in and out weights\n    df_final_train['weight_f1'] = df_final_train[:94375].weight_in + df_final_train.weight_out\n    df_final_train['weight_f2'] = df_final_train[:94375].weight_in * df_final_train.weight_out\n    df_final_train['weight_f3'] = (2*df_final_train[:94375].weight_in + 1*df_final_train.weight_out)\n    df_final_train['weight_f4'] = (1*df_final_train[:94375].weight_in + 2*df_final_train.weight_out)\n\n    #some features engineerings on the in and out weights\n    df_final_test['weight_f1'] = df_final_test[0:24361].weight_in + df_final_test.weight_out\n    df_final_test['weight_f2'] = df_final_test[0:24361].weight_in * df_final_test.weight_out\n    df_final_test['weight_f3'] = (2*df_final_test[0:24361].weight_in + 1*df_final_test.weight_out)\n    df_final_test['weight_f4'] = (1*df_final_test[0:24361].weight_in + 2*df_final_test.weight_out)","cc97a592":"\ndf_final_train.drop(['source_node', 'destination_node','indicator_link'],axis=1,inplace=True)\ndf_final_test.drop(['source_node', 'destination_node','indicator_link'],axis=1,inplace=True)\nprint(df_final_train.shape)","76d24acc":"df_final_train[df_final_train.isna().any(axis=1)]\ndf_final_train.fillna(0)\ndf_final_test.fillna(0)\ny_train.fillna(0)\ny_test.fillna(0)\ndf_final_train[df_final_train.isna().any(axis=1)]\n#print(df_final_train.isna())\n#print(df_final_test.isna().sum())","ef8f0a56":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.datasets import make_classification\n\nfrom sklearn.metrics import f1_score\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import f1_score\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom scipy.stats import randint as sp_randint\nfrom scipy.stats import uniform\nestimators = [10,50,100,250,450]\ntrain_scores = []\ntest_scores = []\nfor i in estimators:\n    clf = RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n            max_depth=5, max_features='auto', max_leaf_nodes=None,\n            min_impurity_decrease=0.0, min_impurity_split=None,\n            min_samples_leaf=52, min_samples_split=120,\n            min_weight_fraction_leaf=0.0, n_estimators=i, n_jobs=-1,random_state=25,verbose=0,warm_start=False)\n    clf.fit(df_final_train[:94375],y_train[:94375])\n    train_sc = f1_score(y_train[:94375],clf.predict(df_final_train[:94375]))\n    test_sc = f1_score(y_test[0:24361],clf.predict(df_final_test[0:24361]))\n    test_scores.append(test_sc)\n    train_scores.append(train_sc)\n    print('Estimators = ',i,'Train Score',train_sc,'test Score',test_sc)\nplt.plot(estimators,train_scores,label='Train Score')\nplt.plot(estimators,test_scores,label='Test Score')\nplt.xlabel('Estimators')\nplt.ylabel('Score')\nplt.title('Estimators vs score at depth of 5')","51320114":"param_dist = {\"n_estimators\":sp_randint(105,125),\n              \"max_depth\": sp_randint(10,15),\n              \"min_samples_split\": sp_randint(110,190),\n              \"min_samples_leaf\": sp_randint(25,65)}\n\nclf = RandomForestClassifier(random_state=25,n_jobs=-1)\n\nrf_random = RandomizedSearchCV(clf, param_distributions=param_dist,\n                                   n_iter=5,cv=10,scoring='f1',random_state=25)\n\nrf_random.fit(df_final_train[:94375],y_train[:94375])\n\nclf = RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n            max_depth=14, max_features='auto', max_leaf_nodes=None,\n            min_impurity_decrease=0.0, min_impurity_split=None,\n            min_samples_leaf=28, min_samples_split=111,\n            min_weight_fraction_leaf=0.0, n_estimators=121, n_jobs=-1,\n            oob_score=False, random_state=25, verbose=0, warm_start=False)\nclf.fit(df_final_train[:94375],y_train[:94375])\ny_test_pred=clf.predict_proba(df_final_test[0:24361])\n#from sklearn.metrics import f1_score\n#print('Train f1 score',f1_score(y_train[:94375],y_train_pred[:94375]))\nprint('Test f1 score',f1_score(y_test[0:24361],y_test_pred[0:24361]))","521d753e":"from sklearn.metrics import confusion_matrix\ndef plot_confusion_matrix(test_y, predict_y):\n    C = confusion_matrix(test_y, predict_y)\n    \n    A =(((C.T)\/(C.sum(axis=1))).T)\n    \n    B =(C\/C.sum(axis=0))\n    plt.figure(figsize=(20,4))\n    \n    labels = [1,2]\n    # representing A in heatmap format\n    cmap=sns.light_palette(\"blue\")\n    plt.subplot(1, 3, 1)\n    sns.heatmap(C, annot=True, cmap=cmap, fmt=\".3f\", xticklabels=labels, yticklabels=labels)\n    plt.xlabel('Predicted Class')\n    plt.ylabel('Original Class')\n    plt.title(\"Confusion matrix\")\n    \n    plt.subplot(1, 3, 2)\n    sns.heatmap(B, annot=True, cmap=cmap, fmt=\".3f\", xticklabels=labels, yticklabels=labels)\n    plt.xlabel('Predicted Class')\n    plt.ylabel('Original Class')\n    plt.title(\"Precision matrix\")\n    \n    plt.subplot(1, 3, 3)\n    # representing B in heatmap format\n    sns.heatmap(A, annot=True, cmap=cmap, fmt=\".3f\", xticklabels=labels, yticklabels=labels)\n    plt.xlabel('Predicted Class')\n    plt.ylabel('Original Class')\n    plt.title(\"Recall matrix\")\n    \n    plt.show()","f9c40fbb":"#plot_confusion_matrix(y_train,y_train_pred)\nprint('Test confusion_matrix')\nplot_confusion_matrix(y_test[0:24361],y_test_pred)","0ded3a47":"# Test and training splitting 80:20","c0287b52":"**NO of following + followers**","d47d5f24":"# **Jaccard INdex**\n\\begin{equation}\nj = \\frac{|X\\cap Y|}{|X \\cup Y|} \n\\end{equation}","82ca283b":"# Adding all features\n## 5.2 Adding a set of features\n\n__we will create these each of these features for both train and test data points__\n<ol>\n<li>jaccard_followers<\/li>\n<li>jaccard_followees<\/li>\n<li>cosine_followers<\/li>\n<li>cosine_followees<\/li>\n<li>num_followers_s<\/li>\n<li>num_followees_s<\/li>\n<li>num_followers_d<\/li>\n<li>num_followees_d<\/li>\n<li>inter_followers<\/li>\n<li>inter_followees<\/li>\n\n<li>adar index<\/li>\n<li>is following back<\/li>\n<li>belongs to same weakly connect components<\/li>\n<li>shortest path between source and destination<\/li>\n<li>Weight Features\n    <ul>\n        <li>weight of incoming edges<\/li>\n        <li>weight of outgoing edges<\/li>\n        <li>weight of incoming edges + weight of outgoing edges<\/li>\n        <li>weight of incoming edges * weight of outgoing edges<\/li>\n        <li>2*weight of incoming edges + weight of outgoing edges<\/li>\n        <li>weight of incoming edges + 2*weight of outgoing edges<\/li>\n    <\/ul>\n<\/li>\n<\/ol>\n\n","f95483df":"# **Shortest path**","b2dcbc65":"# Exploratory Analysis","c84d36cc":"**Creating dataset for nodes having no edges between them.**\nfor balanced data:9437519 x 2 size\n","38445a68":"# **IS Person follow Back**","5d55822f":"# Now it's time to applying a Model\n","670545f3":"# Random Forest","69eea1b2":"# Featurization start","d148b77d":"**On Following**","0743bc1d":"# **Adamic\/Adar Index**\n\nAdamic\/Adar measures is defined as inverted sum of degrees of common neighbours for given two vertices.\n$$A(x,y)=\\sum_{u \\in N(x) \\cap N(y)}\\frac{1}{log(|N(u)|)}$$","35ba5a83":"# **Cosine Distance**\n\\begin{equation}\nCosineDistance = \\frac{|X\\cap Y|}{|X|\\cdot|Y|} \n\\end{equation}","bd60af83":"# Social Network Graph Link Prediction\n\n### Problem statement: \nGiven a directed social graph, have to predict missing links to recommend users (Link Prediction in graph)\n\n### Data Overview\nTaken data from facebook's recruting challenge on kaggle \ndata contains two columns source and destination eac edge in graph \n    - Data columns (total 2 columns):  \n    - source_node         int64  \n    - destination_node    int64  \n  \n### Performance metric for supervised learning:  \n- Both precision and recall is important so F1 score is good choice\n- Confusion matrix","f961352a":"**ON followers**","ff086df5":"**Display the Graph**"}}