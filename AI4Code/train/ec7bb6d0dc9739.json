{"cell_type":{"c267bd0e":"code","cc0b9f71":"code","2071b4c5":"code","3b66b872":"code","4fad34bb":"code","936d0c29":"code","820137b3":"code","d84e279e":"code","9eb3039f":"code","718691bb":"code","462fa514":"code","07bfcdb7":"code","20b3ee23":"code","063cfdde":"code","16462186":"code","1442890e":"code","747d4261":"code","44827c42":"code","1f4e7c77":"code","5835950e":"code","423108ae":"code","b08b3db2":"code","6d43c4cd":"code","ece5e863":"code","a1269330":"code","8de408b0":"code","f5345913":"code","d628ca8f":"code","e4be05a2":"code","ce15c7c2":"code","c39f3de5":"code","84270d1b":"code","d49415f8":"code","d30009a5":"code","6f959344":"code","94139a8e":"code","20a0f5c8":"code","138f55e4":"code","d18e3d18":"code","1b169647":"code","41bf5da7":"code","97a3a071":"code","aaa174ed":"code","6a616b4d":"code","9a3621c9":"code","6e435218":"code","fb7f0d16":"code","4c213c90":"code","bd1c1260":"code","ac29bab6":"code","fa6c20ba":"markdown","0bed904f":"markdown","77d69a6c":"markdown","81588f08":"markdown","5fde3fd8":"markdown","e0a1ef04":"markdown","f434eb49":"markdown","b6cc42b1":"markdown","41e7c9e1":"markdown","e771f9e7":"markdown","53bbe8e9":"markdown","33246fbe":"markdown","9c400a0b":"markdown","549a2ad4":"markdown","a87cc466":"markdown","2ff0e43a":"markdown","5fbcae52":"markdown","1f13f57f":"markdown","c78e24f5":"markdown","8843132d":"markdown","01d14242":"markdown","b2d51912":"markdown"},"source":{"c267bd0e":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style(\"darkgrid\")\nimport plotly.express as px\nimport pandas_profiling as pp\nimport warnings\nwarnings.filterwarnings(\"ignore\")","cc0b9f71":"df = pd.read_csv(\"\/kaggle\/input\/breast-cancer-wisconsin-data\/data.csv\")\ndf","2071b4c5":"df.info()","3b66b872":"round(df.isna().sum() * 100 \/ len(df) , 2).sort_values(ascending = False)","4fad34bb":"df.drop([\"Unnamed: 32\" , \"id\"] , axis = 1 , inplace = True)","936d0c29":"from sklearn.preprocessing import LabelEncoder\nlb = LabelEncoder()\ndf[\"diagnosis\"] = lb.fit_transform(df[\"diagnosis\"])","820137b3":"plt.figure(figsize = (15 , 6))\ndf.dtypes.value_counts().plot.pie(explode=[0.3,0.3] , autopct='%1.2f%%' , shadow=True)","d84e279e":"sns.countplot(data = df , x = \"diagnosis\" )","9eb3039f":"plt.figure(figsize = (20 , 10))\nsns.heatmap(df.corr() , annot = True , cmap = \"coolwarm\")","718691bb":"df.drop([\"perimeter_mean\" , \"perimeter_worst\"] , axis = 1 , inplace = True)","462fa514":"df.hist(edgecolor = \"black\" , figsize = (15 , 15));","07bfcdb7":"for i in range(len(df.skew())):\n    if df.skew()[i] > 3 or df.skew()[i] < -3:\n        print(f\"{df.skew().index[i]} with skewness of {df.skew()[i] : >{20}}\")\n        print(\"\\n\")\n        plt.figure(figsize = (15 , 6))\n        sns.histplot(data = df , x = df.columns[i] , hue = \"diagnosis\" , kde = True)\n        plt.show()\n        print(\"\\n\\n\")","20b3ee23":"for i in range(len(df.kurtosis())):\n    if df.kurtosis()[i] > 10 or df.kurtosis()[i] < -10:\n        print(f\"{df.kurtosis().index[i]} with kurtosis of {df.kurtosis()[i] : >{20}}\")\n        print(\"\\n\")\n        plt.figure(figsize = (15 , 6))\n        sns.histplot(data = df , x = df.columns[i] , hue = \"diagnosis\" , kde = True)\n        plt.show()\n        print(\"\\n\\n\")","063cfdde":"df_temp = df.copy()","16462186":"df[\"diagnosis\"].value_counts()","1442890e":"# Percentile Cutoff method\n\nouts = [\"radius_se\" , \"perimeter_se\" , \"area_se\" , \"smoothness_se\" , \"concavity_se\" , \"fractal_dimension_se\"]\n\nfor i in outs:\n    df[i].loc[df[i] < np.percentile(df[i] , [1])[0] * 0.3] = np.percentile(df[i] , [1])[0]\n    df[i].loc[df[i] > np.percentile(df[i] , [99])[0] * 3] = np.percentile(df[i] , [99])[0]","747d4261":"# Exponential Smothening\n\nfor i in outs:\n    df[i] = np.log(df[i] + 1)","44827c42":"for i in range(len(df.skew())):\n    if df.skew()[i] > 3 or df.skew()[i] < -3:\n        print(f\"{df.skew().index[i]} with skewness of {df.skew()[i] : >{20}}\")\n        print(\"\\n\")\n        plt.figure(figsize = (15 , 6))\n        sns.histplot(data = df , x = df.columns[i] , hue = \"diagnosis\" , kde = True)\n        plt.show()\n        print(\"\\n\\n\")","1f4e7c77":"for i in range(len(df.kurtosis())):\n    if df.kurtosis()[i] > 10 or df.kurtosis()[i] < -10:\n        print(f\"{df.kurtosis().index[i]} with kurtosis of {df.kurtosis()[i] : >{20}}\")\n        print(\"\\n\")\n        plt.figure(figsize = (15 , 6))\n        sns.histplot(data = df , x = df.columns[i] , hue = \"diagnosis\" , kde = True)\n        plt.show()\n        print(\"\\n\\n\")","5835950e":"from sklearn.feature_selection import mutual_info_classif as mif\n\nmif_values = mif(df.drop([\"diagnosis\"] , axis = 1) , df[\"diagnosis\"])\n\npd.DataFrame(mif_values , index = df.drop([\"diagnosis\"] , axis = 1).columns).sort_values(by = 0 , ascending = False)","423108ae":"X = df.drop([\"diagnosis\"] , axis = 1)\ny = df[\"diagnosis\"]","b08b3db2":"y.value_counts()","6d43c4cd":"from sklearn.model_selection import train_test_split\n\nX_train , X_test , y_train , y_test = train_test_split(X , y , test_size = 0.2 , random_state = 42)\nlen(X_train) , len(X_test) , len(y_train) , len(y_test)","ece5e863":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\n\nints = X.columns\n\nX_train[ints] = scaler.fit_transform(X_train[ints])\nX_test[ints] = scaler.transform(X_test[ints])","a1269330":"from xgboost import XGBClassifier\nfrom catboost import CatBoostClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom lightgbm import LGBMClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\n\nfrom sklearn.metrics import confusion_matrix , roc_auc_score , f1_score , accuracy_score , classification_report , roc_curve , auc , plot_roc_curve\nfrom sklearn.model_selection import cross_val_score","8de408b0":"models = []\nmodels.append((\"XGBClassifier\", XGBClassifier(objective = 'binary:logistic' , random_state = 42 , eval_metric='mlogloss')))\nmodels.append((\"CatBoostClassifier\", CatBoostClassifier(random_state = 42 , verbose = 0)))\nmodels.append((\"RandomForest\", RandomForestClassifier(random_state = 42 , n_estimators = 200)))\nmodels.append((\"ExtraTreeRegressor\", ExtraTreesClassifier(random_state = 42 , n_estimators = 200)))\nmodels.append((\"Gradient Boosting Classifier\" , GradientBoostingClassifier(random_state = 42)))\nmodels.append((\"LightGBM\" , LGBMClassifier(random_state = 42 , n_estimators = 200)))\nmodels.append((\"Logistic Regression\", LogisticRegression(random_state = 42)))\nmodels.append((\"KNeigbors\", KNeighborsClassifier()))","f5345913":"def metrics(model , X_train , y_train , X_test , y_test , params = False):\n    \n    mod = model[1].fit(X_train , y_train)\n    preds = model[1].predict(X_test)\n    accuracies = cross_val_score(estimator = model[1], X = X_train , y = y_train, cv = 10)\n    cm = confusion_matrix(y_test , preds)\n    cf = classification_report(y_test , preds)\n    roc = roc_auc_score(y_test , model[1].predict_proba(X_test)[: , 1])\n    fpr, tpr, thresholds = roc_curve(y_test, preds)\n    ac = auc(fpr, tpr)\n    f1 = f1_score(y_test , preds)\n    \n    \n    print(\"\\n\")\n    print(model[0])\n    \n    print(\"\\n\")\n    if params:\n        print(f\"Best Parameters are : \\n\" , model[1].best_params_)\n        print(\"\\n\")\n        \n    print(f\"Confusion matrix : \\n\")\n    plt.figure(figsize = (8, 5))\n    sns.heatmap(cm, cmap = 'coolwarm', annot = True, annot_kws = {'fontsize': 20})\n    plt.show()\n    print(\"\\n\")\n    \n    print(f\"Training score : {model[1].score(X_train , y_train):.4f}\")\n    print(\"\\n\") \n    \n    print(f\"Test Score : {model[1].score(X_test , y_test):.4f}\")\n    print(\"\\n\")\n    \n    print(f\"K-fold accuracy : {np.mean(accuracies):.4f}\")\n    print(\"\\n\")\n    \n    print(f\"Standard Deviation of Accuracies in k-fold : {np.std(accuracies):.4f}\")\n    print(\"\\n\")\n    \n    print(f\"ROC AUC Score: {roc:.4f}\")\n    print('\\n')\n    \n    print(f\"F1 Score: {f1:.4f}\")\n    print(\"\\n\")\n    \n    print(f\"AUC : {ac:.4f}\")\n    print(\"\\n\")\n    \n    print(f\"Classification report : \\n\\n{cf}\")\n    print(\"\\n\")\n\n    plt.figure(figsize = (8, 5))\n    plot_roc_curve(model[1], X_test, y_test , color = '#FF4500')\n    plt.plot([0, 1], [0, 1], linestyle = '--', color = '#7CFC00')\n    plt.show()\n    print(\"\\n\")\n    print(\"*\"*100)\n    \n    print(\"\\n\\n\")\n    \n    sam = []\n    sam.append(model[0])\n    sam.append(model[1].score(X_train , y_train))\n    sam.append(model[1].score(X_test , y_test))\n    sam.append(np.mean(accuracies))\n    sam.append(np.std(accuracies))\n    sam.append(roc)\n    sam.append(f1)\n    sam.append(ac)\n    \n    return sam , mod","d628ca8f":"%%time\n\npre_final = []\n\nfor i in models:\n    sam = metrics(i , X_train , y_train , X_test , y_test)\n    pre_final.append(sam)","e4be05a2":"data_pre_final = [x[0] for x in pre_final]","ce15c7c2":"me = pd.DataFrame(data_pre_final , columns = [\"Model\" , \"Train Score\" , \"Test Score\" , \"K-fold Accuracy\" , \"K-fold Std\" , \"ROC_AUC_Score\" , \"F1 Score\" , \"AUC\"])\n\nme.sort_values(by = [ \"F1 Score\" , \"AUC\" , \"ROC_AUC_Score\" , \"K-fold Std\" , \"K-fold Accuracy\" , \"Test Score\" , \"Train Score\"] , inplace = True , ascending = [False , False , False , True , False , False , False])\nme = me.reset_index(drop = True)\nme","c39f3de5":"plt.figure(figsize = (10 , 6))\nsns.barplot(y = \"Model\" , x = \"F1 Score\" , data = me)\nplt.title(\"Model Comparision based on F1 Score\");","84270d1b":"plt.figure(figsize = (10 , 6))\nsns.barplot(y = \"Model\" , x = \"AUC\" , data = me)\nplt.title(\"Model Comparision based on AUC\");","d49415f8":"plt.figure(figsize = (10 , 6))\nsns.barplot(y = \"Model\" , x = \"ROC_AUC_Score\" , data = me)\nplt.title(\"Model Comparision based on ROC_AUC_Score\");","d30009a5":"plt.figure(figsize = (10 , 6))\nsns.barplot(y = \"Model\" , x = \"K-fold Accuracy\" , data = me)\nplt.title(\"Model Comparision based on K-fold Accuracy\");","6f959344":"from sklearn.ensemble import VotingClassifier\n\nvoting_models = models","94139a8e":"voting_soft = VotingClassifier(estimators = voting_models , voting = \"soft\")","20a0f5c8":"voting_soft.fit(X_train , y_train)","138f55e4":"def metrics_others(model , X_train , y_train , X_test , y_test , params = False):\n    \n    preds = model.predict(X_test)\n    cm = confusion_matrix(y_test , preds)\n    cf = classification_report(y_test , preds)\n    roc = roc_auc_score(y_test , model.predict_proba(X_test)[: , 1])\n    fpr, tpr, thresholds = roc_curve(y_test, preds)\n    ac = auc(fpr, tpr)\n    f1 = f1_score(y_test , preds)\n    \n    print(f\"Confusion matrix : \\n\")\n    plt.figure(figsize = (8, 5))\n    sns.heatmap(cm, cmap = 'coolwarm', annot = True, annot_kws = {'fontsize': 20})\n    plt.show()\n    print(\"\\n\")\n    \n    print(f\"Training score : {model.score(X_train , y_train):.4f}\")\n    print(\"\\n\") \n    \n    print(f\"Test Score : {model.score(X_test , y_test):.4f}\")\n    print(\"\\n\")\n    \n    print(f\"ROC AUC Score: {roc:.4f}\")\n    print('\\n')\n    \n    print(f\"F1 Score: {f1:.4f}\")\n    print(\"\\n\")\n    \n    print(f\"AUC : {ac:.4f}\")\n    print(\"\\n\")\n    \n    print(f\"Classification report : \\n\\n{cf}\")\n    print(\"\\n\")\n\n    plt.figure(figsize = (8, 5))\n    plot_roc_curve(model, X_test, y_test , color = '#FF4500')\n    plt.plot([0, 1], [0, 1], linestyle = '--', color = '#7CFC00')\n    plt.show()\n    print(\"\\n\")\n    print(\"*\"*100)\n    \n    print(\"\\n\\n\")\n    \n    sam = []\n    sam.append(model.score(X_train , y_train))\n    sam.append(model.score(X_test , y_test))\n    sam.append(roc)\n    sam.append(f1)\n    sam.append(ac)\n    \n    return sam","d18e3d18":"soft = metrics_others(voting_soft , X_train , y_train , X_test , y_test)","1b169647":"from catboost import CatBoostClassifier\ncat = CatBoostClassifier(loss_function = \"MultiClass\", \n                         eval_metric = \"TotalF1\",\n                         random_seed = 42 , \n                         classes_count = 2 ,\n                         depth = 10 ,\n                         iterations = 3500 , \n                         learning_rate = 0.1 ,\n                         leaf_estimation_iterations = 1 ,\n                         l2_leaf_reg = 1 ,\n                         bootstrap_type = \"Bayesian\" , \n                         bagging_temperature = 1 , \n                         random_strength = 1 ,\n                         od_type = \"Iter\", \n                         border_count = 100 ,\n                         od_wait = 50)","41bf5da7":"%%time\n\ncat.fit(X_train , y_train , use_best_model = True , eval_set=[(X_test , y_test)] , verbose = True)","97a3a071":"cat_preds = cat.predict(X_test)","aaa174ed":"f1_score(y_test , cat_preds)","6a616b4d":"final_cat = metrics_others(cat , X_train , y_train , X_test , y_test)","9a3621c9":"f1_score(y_test , cat.predict(X_test))","6e435218":"roc_auc_score(y_test , cat.predict_proba(X_test)[: , 1])","fb7f0d16":"linear = pre_final[5][1]","4c213c90":"f1_score(y_test , linear.predict(X_test))","bd1c1260":"roc_auc_score(y_test , linear.predict_proba(X_test)[: , 1])","ac29bab6":"f1_score(y_test , cat.predict(X_test))","fa6c20ba":"We Removed most of the outliers","0bed904f":"## Importing primary Libraries","77d69a6c":"### Linear Model","81588f08":"# Don't forget to upvote if you like the notebook . Thank You .  ","5fde3fd8":"## Model Evaluation with Voting Classifier","e0a1ef04":"### Treating Outliers","f434eb49":"# Breast Cancer Prediction Model","b6cc42b1":"## Basic Preprocessing and EDA","41e7c9e1":"### Checking for multicollinearity","e771f9e7":"### Rechecking for outliers after the Treatment","53bbe8e9":"## Since CatBoost Has More F1_Score , ROC_AUC_Score . We use Catboost","33246fbe":"Therefore , no issues of multicollinearity","9c400a0b":"## Feature Scaling","549a2ad4":"## Model Evaluation and Visualization","a87cc466":"### Dropping Highly correlated columns \/ Features ","2ff0e43a":"There are outliers in the above columns","5fbcae52":"## Model Evaluation with Catboost","1f13f57f":"## Model Fitting","c78e24f5":"## Train Test Split","8843132d":"## Final model can be Logistic Regression \/ CatBoost Classifier","01d14242":"### CatBoost","b2d51912":"## Outliers Treatment\n1. Skewness in the range of [-3 , 3]\n2. Kurtosis in the range of [-1 , 10]"}}