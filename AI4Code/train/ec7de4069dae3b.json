{"cell_type":{"94d6a8db":"code","2d9396f2":"code","d14abd12":"code","19b4ca14":"code","35a95cb5":"code","e6c53162":"code","ebf9d1f0":"code","829f11d8":"code","3a2f96fa":"code","0711ddef":"code","f33fd21c":"code","88e6d3d7":"markdown","a930acca":"markdown","0c0775b1":"markdown","f78f5135":"markdown","3f31e709":"markdown","60225da5":"markdown"},"source":{"94d6a8db":"import sys\n!pip install pytorch-tabnet\nsys.path.append('..\/input\/iterative-stratification\/iterative-stratification-master')\nfrom pytorch_tabnet.tab_model import TabNetRegressor","2d9396f2":"import optuna\nimport torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader, Dataset\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nfrom sklearn.model_selection import StratifiedKFold\nimport numpy as np\nimport pandas as pd \n\nimport os\nimport random\nimport sys\nos.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\nfrom tqdm import tqdm\nfrom sklearn.metrics import log_loss","d14abd12":"def seed_everything(seed_value):\n    random.seed(seed_value)\n    np.random.seed(seed_value)\n    torch.manual_seed(seed_value)\n    os.environ['PYTHONHASHSEED'] = str(seed_value)\n    \n    if torch.cuda.is_available(): \n        torch.cuda.manual_seed(seed_value)\n        torch.cuda.manual_seed_all(seed_value)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n        \nseed_everything(42)","19b4ca14":"data_path = \"..\/input\/lish-moa\/\"\ntrain = pd.read_csv(data_path+'train_features.csv')\ntrain.drop(columns=[\"sig_id\"], inplace=True)\n\ntrain_targets_scored = pd.read_csv(data_path+'train_targets_scored.csv')\ntrain_targets_scored.drop(columns=[\"sig_id\"], inplace=True)\n\ntest = pd.read_csv(data_path+'test_features.csv')\ntest.drop(columns=[\"sig_id\"], inplace=True)\n\nsubmission = pd.read_csv(data_path+'sample_submission.csv')\n\nremove_vehicle = False\n\nif remove_vehicle:\n    kept_index = train['cp_type']=='trt_cp'\n    train = train.loc[kept_index].reset_index(drop=True)\n    train_targets_scored = train_targets_scored.loc[kept_index].reset_index(drop=True)\n\ntrain[\"cp_type\"] = (train[\"cp_type\"]==\"trt_cp\") + 0\ntrain[\"cp_dose\"] = (train[\"cp_dose\"]==\"D1\") + 0\n\ntest[\"cp_type\"] = (test[\"cp_type\"]==\"trt_cp\") + 0\ntest[\"cp_dose\"] = (test[\"cp_dose\"]==\"D1\") + 0\n\nX_test = test.values","35a95cb5":"from iterstrat.ml_stratifiers import MultilabelStratifiedKFold, MultilabelStratifiedShuffleSplit\nfrom sklearn.metrics import roc_auc_score","e6c53162":"from sklearn.metrics import log_loss\nfrom pytorch_tabnet.metrics import Metric\nfrom sklearn.metrics import roc_auc_score, log_loss\n\nclass LogitsLogLoss(Metric):\n    \"\"\"\n    LogLoss with sigmoid applied\n    \"\"\"\n\n    def __init__(self):\n        self._name = \"logits_ll\"\n        self._maximize = False\n\n    def __call__(self, y_true, y_pred):\n        \"\"\"\n        Compute LogLoss of predictions.\n\n        Parameters\n        ----------\n        y_true: np.ndarray\n            Target matrix or vector\n        y_score: np.ndarray\n            Score matrix or vector\n\n        Returns\n        -------\n            float\n            LogLoss of predictions vs targets.\n        \"\"\"\n        logits = 1 \/ (1 + np.exp(-y_pred))\n        aux = (1-y_true)*np.log(1-logits+1e-15) + y_true*np.log(logits+1e-15)\n        return np.mean(-aux)","ebf9d1f0":"# Low number of epochs for Optuna\nMAX_EPOCH=15\n\ndef objective(trial):\n    \n    # all hyperparameters here\n#     mask_type = trial.suggest_categorical(\"mask_type\", [\"entmax\", \"sparsemax\"])\n    mask_type = \"entmax\"\n    n_da = trial.suggest_int(\"n_da\", 56, 64, step=4)\n#     n_steps = trial.suggest_int(\"n_steps\", 1, 3, step=1)\n    n_steps = 1\n    gamma = trial.suggest_float(\"gamma\", 1., 1.4., step=0.2)\n    lambda_sparse = trial.suggest_float(\"lambda_sparse\", 1e-6, 1e-3, log=True)\n#     n_shared = trial.suggest_int(\"n_shared\", 1, 3)\n    n_shared = 1\n    \n    tabnet_params = dict(n_d=n_da, n_a=n_da, n_steps=n_steps, gamma=gamma,\n                         lambda_sparse=lambda_sparse, optimizer_fn=torch.optim.Adam,\n                         optimizer_params=dict(lr=2e-2, weight_decay=1e-5),\n                         mask_type=mask_type, n_shared=n_shared,\n                         scheduler_params=dict(mode=\"min\",\n                                               patience=5,\n                                               min_lr=1e-5,\n                                               factor=0.5,),\n                         scheduler_fn=torch.optim.lr_scheduler.ReduceLROnPlateau,\n                         verbose=0,\n                         )\n\n    scores_auc_all= []\n    test_cv_preds = []\n\n    NB_SPLITS = 5\n    mskf = MultilabelStratifiedKFold(n_splits=NB_SPLITS, random_state=0, shuffle=True)\n    oof_preds = []\n    oof_targets = []\n    scores = []\n    for fold_nb, (train_idx, val_idx) in enumerate(mskf.split(train, train_targets_scored)):\n        print(\"FOLDS : \", fold_nb)\n\n        ## model\n        X_train, y_train = train.values[train_idx, :], train_targets_scored.values[train_idx, :]\n        X_val, y_val = train.values[val_idx, :], train_targets_scored.values[val_idx, :]\n        model = TabNetRegressor(**tabnet_params)\n\n        model.fit(X_train=X_train,\n                  y_train=y_train,\n                  eval_set=[(X_val, y_val)],\n                  eval_name = [\"val\"],\n                  eval_metric = [\"logits_ll\"],\n                  max_epochs=MAX_EPOCH,\n                  patience=20, batch_size=1024, virtual_batch_size=128,\n                  num_workers=1, drop_last=False,\n                  # use binary cross entropy as this is not a regression problem\n                  loss_fn=torch.nn.functional.binary_cross_entropy_with_logits)\n\n        preds_val = model.predict(X_val)\n        # Apply sigmoid to the predictions\n        preds =  1 \/ (1 + np.exp(-preds_val))\n        score = np.min(model.history[\"val_logits_ll\"])\n\n        ## save oof to compute the CV later\n        oof_preds.append(preds_val)\n        oof_targets.append(y_val)\n        scores.append(score)\n\n        # preds on test\n        preds_test = model.predict(X_test)\n        test_cv_preds.append(1 \/ (1 + np.exp(-preds_test)))\n\n    oof_preds_all = np.concatenate(oof_preds)\n    oof_targets_all = np.concatenate(oof_targets)\n    test_preds_all = np.stack(test_cv_preds)\n    \n    return np.mean(scores)","829f11d8":"pruner = optuna.pruners.MedianPruner() \n\nstudy = optuna.create_study(direction=\"minimize\", pruner=pruner)\nstudy.optimize(objective, n_jobs=-1, n_trials=50, gc_after_trial=True, timeout=None)\n\nprint(\"Number of finished trials: {}\".format(len(study.trials)))\n\nprint(\"Best trial:\")\ntrial = study.best_trial\n\nprint(\"  Value: {}\".format(trial.value))\n\nprint(\"  Params: \")\nfor key, value in trial.params.items():\n    print(\"    {}: {}\".format(key, value))","3a2f96fa":"plot_contour(study, params=['mask_type',\n                            'n_da',\n                            'n_steps',\n                            'gamma',\n                            'lambda_sparse',\n                            'n_shared'])","0711ddef":"plot_optimization_history(study)","f33fd21c":"plot_slice(study)","88e6d3d7":"#### You can run around 95 trials in background mode using GPU, without timing out","a930acca":"# Define custom metric for valdidation","0c0775b1":"# Data and minimal preprocessing","f78f5135":"### Visualize optimization trials","3f31e709":"## Optuna Objective function with hyperparameters","60225da5":"Original code by @optimo  from https:\/\/www.kaggle.com\/optimo\/tabnetregressor-2-0-train-infer"}}