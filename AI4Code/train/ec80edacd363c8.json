{"cell_type":{"1e824b94":"code","b1518578":"code","e75e5153":"code","a94c9da3":"code","a0ede8a0":"code","870554f0":"code","ddff91d7":"code","b2e6fef4":"code","ec449e2e":"code","e2405531":"code","5e5b2f6c":"code","a3ccdd9e":"code","12059bd6":"code","cfc568f0":"code","a5ba5be1":"code","307a2afb":"code","39b86a30":"code","b730756f":"code","0aea5a03":"markdown"},"source":{"1e824b94":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","b1518578":"import pandas as pd\nimport numpy as np\nimport nltk\nfrom collections import Counter\nimport seaborn as sns\nfrom nltk.util import ngrams\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import PorterStemmer\nfrom nltk.stem.wordnet import WordNetLemmatizer\nfrom gensim import models","e75e5153":"df = pd.read_csv('..\/input\/million-headlines\/abcnews-date-text.csv',nrows=10000)","a94c9da3":"df.head()","a0ede8a0":"df.shape","870554f0":"df.isnull().any().sum()","ddff91d7":"df.headline_text.duplicated().any()","b2e6fef4":"df = df.drop_duplicates('headline_text')\ndf.shape","ec449e2e":"df.headline_text.unique()","e2405531":"df['headline_text'].str.len().hist()","5e5b2f6c":"df['headline_text'].str.split().apply(lambda x : [len(i) for i in x]).map(lambda x: np.mean(x)).hist()","a3ccdd9e":"from nltk.corpus import stopwords\nnltk.download('stopwords')\nstop=set(stopwords.words('english'))","12059bd6":"corpus=[]\nnew= df['headline_text'].str.split()\nnew=new.values.tolist()\ncorpus=[word for i in new for word in i]\n\nfrom collections import defaultdict\ndic=defaultdict(int)\nfor word in corpus:\n    if word in stop:\n        dic[word]+=1","cfc568f0":"counter=Counter(corpus)\nmost=counter.most_common()\n\nx, y= [], []\nfor word,count in most[:40]:\n    if (word not in stop):\n        x.append(word)\n        y.append(count)\n        \nsns.barplot(x=y,y=x)","a5ba5be1":"def get_top_ngram(corpus, n=None):\n    vec = CountVectorizer(ngram_range=(n, n)).fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) \n                  for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:10]","307a2afb":"top_bi_grams=get_top_ngram(df['headline_text'],n=2)\nx,y=map(list,zip(*top_bi_grams))\nsns.barplot(x=y,y=x)","39b86a30":"top_tri_grams=get_top_ngram(df['headline_text'],n=3)\nx,y=map(list,zip(*top_tri_grams))\nsns.barplot(x=y,y=x)","b730756f":"import nltk\nnltk.download('punkt')\nnltk.download('wordnet')\n\ndef preprocess_news(dff):\n    corpus=[]\n    stem=PorterStemmer()\n    lem=WordNetLemmatizer()\n    for df in dff['headline_text']:\n        words=[w for w in word_tokenize(df) if (w not in stop)]\n        \n        words=[lem.lemmatize(w) for w in words if len(w)>2]\n        \n        corpus.append(words)\n    return corpus\n\ncorpus=preprocess_news(df)","0aea5a03":"###### Reference: https:\/\/neptune.ai\/blog\/exploratory-data-analysis-natural-language-processing-tools"}}