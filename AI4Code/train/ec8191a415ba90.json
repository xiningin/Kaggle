{"cell_type":{"ed7d292c":"code","f05c7fee":"code","3213d899":"code","af3b1321":"code","5bbc8dc8":"code","f0bbc2a2":"code","96ef69a5":"code","b99d59a1":"code","6a82ec4f":"code","4377f4a3":"code","b3a0e7a5":"code","8a593678":"code","3bd65ae3":"code","56f7ceb6":"code","b6baf22b":"code","7dd97ceb":"code","9ddef88d":"code","9eab9e4d":"code","ff49d6d9":"code","2d1d5f55":"code","6511a25b":"code","74ae22f8":"code","63a13f04":"code","bb981a2d":"code","d5b5e560":"code","59a45148":"code","5352a5ae":"code","4fedb0a7":"code","58e42e53":"code","0a7e3cd7":"code","e514eeef":"code","ad7543be":"code","65ec324c":"code","3c214fd4":"code","a2e801cd":"code","a2c355e9":"code","9bca54d6":"code","36ab2f8a":"code","6785231d":"code","44a3d135":"code","3b2700e9":"code","2613cc53":"code","13b3a9c6":"code","de55c37f":"code","19943186":"code","b7d7d2ac":"code","771772da":"code","1bb4e211":"code","10fe2fb4":"code","c5a50127":"code","76aec0a0":"code","34887bf2":"code","f1863088":"code","2310119f":"code","a19aa130":"code","f7644990":"code","3ddb3671":"code","ba09cde0":"code","e2bbde3f":"code","10dff642":"code","f86e7957":"code","c87bd1d5":"code","fcdcd619":"code","8590623a":"code","70db7761":"code","426acf44":"code","c6e782b3":"code","6e3ef7db":"code","75f7189d":"code","ce840694":"markdown","1d04776e":"markdown","36c45ad1":"markdown","948d8c46":"markdown","9be64ba2":"markdown","b9c6f63f":"markdown","70931879":"markdown","29f3c145":"markdown","b1df6fdf":"markdown","d24f4f92":"markdown","f574ab73":"markdown","e0cd04b4":"markdown","37551557":"markdown","8b2a2483":"markdown","97c01ea5":"markdown","c88a50d0":"markdown","37570d39":"markdown","6f8294f5":"markdown","e8f25877":"markdown","33bf464c":"markdown","9e34f41a":"markdown","51e3f885":"markdown","eedb5beb":"markdown","5644c4de":"markdown","783cc97d":"markdown","b2d63fb1":"markdown","14469895":"markdown","3a807e04":"markdown","66a277d2":"markdown","9f9a550a":"markdown","2c410643":"markdown","65b20965":"markdown","41cf18fd":"markdown","1c28e770":"markdown","14630498":"markdown","91508cad":"markdown","8fb5d3f9":"markdown","6d3bcebe":"markdown"},"source":{"ed7d292c":"# !pip install --upgrade pip -q\n# !pip install --upgrade setuptools -q\n# !pip install recommenders[examples] -q\n\n# import sys\n# import pandas as pd\n# import tensorflow as tf\n\n# from recommenders.utils.timer import Timer\n# from recommenders.models.ncf.ncf_singlenode import NCF\n# from recommenders.models.ncf.dataset import Dataset as NCFDataset\n# from recommenders.datasets.python_splitters import python_chrono_split, python_random_split\n# from recommenders.evaluation.python_evaluation import (rmse, mae, rsquared, exp_var, map_at_k, ndcg_at_k, precision_at_k, \n#                                                      recall_at_k, get_top_k_items)\n# from recommenders.utils.constants import SEED as DEFAULT_SEED\n# from recommenders.datasets.pandas_df_utils import filter_by\n\n# # # load sample dataset\n# from recommenders.datasets import movielens\n\n# print(\"System version: {}\".format(sys.version))\n# print(\"Pandas version: {}\".format(pd.__version__))\n# print(\"Tensorflow version: {}\".format(tf.__version__))","f05c7fee":"import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nimport matplotlib.pyplot as plt","3213d899":"import numpy as np\nimport pandas as pd\nimport os\npd.set_option('display.max_columns',None)\nimport seaborn as sns\nfrom collections import defaultdict\nfrom tqdm.notebook import tqdm\nimport gc\nimport itertools\n\ndef basic_summary(df):\n    \n    '''\n    Report the basic information about the input dataframe\n    \n    Args:\n    df -> pd.DataFrame\n    \n    Returns:\n    None\n    '''\n    \n    print(f\"Samples : {df.shape[0]:,} \\nColumns : {df.shape[1]} : {list(df.columns.values)}\")\n    print(\"\\nHeads\")\n    display(df.head(3))\n    print(\"\\nData types\")\n    display(pd.DataFrame(df.dtypes, columns=['dtypes']).transpose())\n    print(\"\\nNull values\")\n    display(pd.concat([df.isna().sum(),df.isna().mean() * 100],axis=1).rename({0:'count',1:'pct'},axis=1).transpose())\n    print(\"\\nBasic statistics\")\n    display(df.describe().transpose())\n      \n\nif __name__ == \"__main__\":\n    \n    BASE_PATH = '\/kaggle\/input\/anime-recommendations-database\/'\n    ANIME_DTYPES = {'anime_id': str, 'name': str, 'genre': str, 'type': str, 'episodes': str, 'rating': float, 'members': int}\n    RATING_DTYPES = {'user_id': str, 'anime_id': str, 'rating': int}\n    ANIME_PATH = os.path.join(BASE_PATH, 'anime.csv')\n    RATING_PATH = os.path.join(BASE_PATH, 'rating.csv')\n\n    anime = pd.read_csv(ANIME_PATH, dtype = ANIME_DTYPES)\n    rating = pd.read_csv(RATING_PATH,  dtype = RATING_DTYPES)","af3b1321":"basic_summary(anime)","5bbc8dc8":"basic_summary(rating)","f0bbc2a2":"def weighted_rating(v,m,R,C):\n    '''\n    Calculate the weighted rating\n    \n    Args:\n    v -> average rating for each anime (float)\n    m -> minimum votes required to be classified as popular (float)\n    R -> average rating for the anime (pd.Series)\n    C -> average rating for the whole dataset (pd.Series)\n    \n    Returns:\n    pd.Series\n    '''\n    return ( (v \/ (v + m)) * R) + ( (m \/ (v + m)) * C )\n\ndef assign_popular_based_score(rating):\n    '''\n    Assigned popular based score based on the IMDB weighted average.\n    \n    Args:\n    rating -> pd.DataFrame contains ['anime_id', 'rating'] for each user.\n    \n    Returns\n    popular_anime -> pd.DataFrame contains anime name and IMDB weighted score.\n    '''\n    \n    # pre processing\n    filter_rating = rating[rating['rating'] != -1]\n    vote_count = filter_rating.groupby('anime_id',as_index=False).agg({'user_id':'count', 'rating':'mean'})\n    vote_count.columns = ['anime_id','vote_count', 'avg_rating']\n    \n    # calcuate input parameters\n    C = np.mean(vote_count['avg_rating'])\n    m = np.percentile(vote_count['vote_count'], 70)\n    vote_count = vote_count[vote_count['vote_count'] >= m]\n    R = vote_count['avg_rating']\n    v = vote_count['vote_count']\n    vote_count['weighted_rating'] = weighted_rating(v,m,R,C)\n    \n    # post processing\n    vote_count = vote_count.merge(anime[['anime_id','name']],on=['anime_id'],how='left')\n    popular_anime = vote_count.loc[:,['anime_id', 'name', 'vote_count', 'avg_rating', 'weighted_rating']]\n    \n    return popular_anime","96ef69a5":"popular_anime = assign_popular_based_score(rating)","b99d59a1":"sns.barplot(data = popular_anime.sort_values('vote_count',ascending=False).head(10),\n            x = 'vote_count', y = 'name', palette='mako');\nsns.despine()","6a82ec4f":"sns.barplot(data = popular_anime.sort_values('weighted_rating',ascending=False).head(10),\n            x = 'weighted_rating', y = 'name', palette = 'mako');\nsns.despine()","4377f4a3":"eligible_rating = rating[rating['rating'] >= 0].copy()\ntrain, test = python_random_split(eligible_rating, ratio = .9, seed = 42)\ntrain.shape, test.shape","b3a0e7a5":"def evaluate_baseline(train, test, COL_PREDICTION, k):\n    \n    # Generate top-k recommendation based on the COL_PREDICTION\n    popular_anime = assign_popular_based_score(train)\n    item_counts = popular_anime.sort_values(COL_PREDICTION, ascending=False)\\\n    [['anime_id', COL_PREDICTION]].reset_index(drop=True).copy()\n    item_counts = item_counts.head(k)\n    \n    # prepare the prediction\n    user_item_col = ['user_id', 'anime_id']\n    test_users = test['user_id'].unique()\n    user_item_list = list(itertools.product(test_users, item_counts[\"anime_id\"]))\n    user_items = pd.DataFrame(user_item_list, columns = user_item_col)\n\n    print(\"Number of user-item pairs:\", len(user_items))\n    \n    # filter out the anime appeared in the test set\n    users_items_remove_seen = filter_by(user_items, train, user_item_col)\n    print(\"After remove seen items:\", len(users_items_remove_seen))\n\n    baseline_recommendations = pd.merge(item_counts, users_items_remove_seen, on=['anime_id'], how='inner')\n    \n    # evaluation the result\n    cols = {\n    'col_user' : 'user_id',\n    'col_item' : 'anime_id',\n    'col_rating' : 'rating'\n    }\n    cols['col_prediction'] = COL_PREDICTION\n\n    eval_map = map_at_k(test, baseline_recommendations, k=k, **cols)\n    eval_ndcg = ndcg_at_k(test, baseline_recommendations, k=k, **cols)\n    eval_precision = precision_at_k(test, baseline_recommendations, k=k, **cols)\n    eval_recall = recall_at_k(test, baseline_recommendations, k=k, **cols)\n\n    print(f'''\n    MAP: {eval_map:.4f}\\n\n    NDCG@{k}: {eval_ndcg:.4f}\\n\n    Precision@{k}: {eval_precision:.4f}\\n\n    Recall@{k}: {eval_recall:.4f}\n    ''')","8a593678":"evaluate_baseline(train, test, COL_PREDICTION = 'vote_count', k = 3)","3bd65ae3":"evaluate_baseline(train, test, COL_PREDICTION = 'weighted_rating', k = 3)","56f7ceb6":"def top_k_similar_anime(anime_id, top_k, corr_mat, map_name):\n    \n    # sort correlation value ascendingly and select top_k csr_anime_id\n    top_anime = corr_mat[anime_id,:].argsort()[-top_k:][::-1] \n    \n    # convert csr_anime_id to anime name\n    top_anime = [map_name[e] for e in top_anime] \n\n    return top_anime","b6baf22b":"rated_anime = anime.loc[anime['anime_id'].isin(rating['anime_id'])].copy()\n\n# extract the genre\ngenre = rated_anime['genre'].str.split(\",\", expand=True)\n\n# get all possible genre\nall_genre = set()\nfor c in genre.columns:\n    distinct_genre = genre[c].str.lower().str.strip().unique()\n    all_genre.update(distinct_genre)\n\nall_genre.remove(None)\nall_genre.remove(np.nan)\nprint(f\"The number of possible genre is : {len(all_genre)}\")\n\n# create item-genre matrix\nitem_genre_mat = rated_anime[['name','genre']].copy()\nitem_genre_mat['genre'] = item_genre_mat['genre'].str.lower().str.strip()\nfor genre in tqdm(all_genre):\n    item_genre_mat[genre] = np.where(item_genre_mat['genre'].str.contains(genre), 1, 0)\n    \nitem_genre_mat = item_genre_mat.drop(['genre'], axis=1)\nitem_genre_mat = item_genre_mat.set_index('name')\n\n# compute similarity matix\nfrom sklearn.metrics.pairwise import cosine_similarity\n\nind2name = {ind:name for ind,name in enumerate(item_genre_mat.index)}\nname2ind = {v:k for k,v in ind2name.items()}\n\ncosine_mat = cosine_similarity(item_genre_mat)","7dd97ceb":"similar_anime = top_k_similar_anime(name2ind['Naruto'],\n                                    top_k = 10,\n                                    corr_mat = cosine_mat,\n                                    map_name = ind2name)\n\nanime.loc[anime['name'].isin(similar_anime), ['name','genre']]","9ddef88d":"similar_anime = top_k_similar_anime(name2ind['Death Note'],\n                                    top_k = 10,\n                                    corr_mat = cosine_mat,\n                                    map_name = ind2name)\n\nanime.loc[anime['name'].isin(similar_anime), ['name','genre']]","9eab9e4d":"del cosine_mat\ngc.collect()","ff49d6d9":"# replace the rating -1 with 0 (convert from watched it but didn't rate the anime into they didn't like it)\ncollab_rating = rating[rating['anime_id'].isin(anime['anime_id'])].copy()\ncollab_rating['rating'] = collab_rating['rating'].replace(-1, 0)\n\nn_users = collab_rating['user_id'].nunique()\nn_animes = collab_rating['anime_id'].nunique()\nprint(f\"Unique users : {n_users:,} \\nUnique anime : {n_animes:,}\")","2d1d5f55":"# create ordered user_id, and anime_id\nmap_user_id = {v:int(i) for i,v in enumerate(sorted(collab_rating['user_id'].unique()))}\nmap_anime_id = {v:int(i) for i,v in enumerate(sorted(collab_rating['anime_id'].unique()))}\n\ncollab_rating['csr_user_id'] = collab_rating['user_id'].map(map_user_id)\ncollab_rating['csr_anime_id'] = collab_rating['anime_id'].map(map_anime_id)\ncollab_rating = collab_rating.merge(anime[['anime_id', 'name']], on='anime_id', how='left')\n\n# create another mapping from the anime name to the new defined indexes\nmap_csr_anime_id_to_name = {ind:name for ind, name in zip(collab_rating['csr_anime_id'], collab_rating['name'])}\nmap_name_to_csr_anime_id = {name:ind for ind, name in map_csr_anime_id_to_name.items()}","6511a25b":"from scipy.sparse import csr_matrix\nfrom tqdm.notebook import tqdm\n\nrow = collab_rating['csr_user_id']\ncol = collab_rating['csr_anime_id']\ndata = collab_rating['rating']\n\nmat = csr_matrix((data, (row, col)), shape=(n_users, n_animes))\nmat.eliminate_zeros()\n\nsparsity = float(len(mat.nonzero()[0]))\nsparsity \/= (mat.shape[0] * mat.shape[1])\nsparsity *= 100\n\nprint(f'Sparsity: {sparsity:4.2f}%. This means that {sparsity:4.2f}% of the user-item ratings have a value.')","74ae22f8":"def train_test_split(mat, test_size = 0.2):\n    \n    train = mat.copy()\n    \n    test_row = []\n    test_col = []\n    test_data = []\n    \n    for user in tqdm(range(mat.shape[0])):\n        \n        # extract the csr_anime_id that has a rating > 0\n        user_ratings = mat[user, :].nonzero()[1] \n        \n        # random test label based on each user_ratings size.\n        test_ratings = np.random.choice(user_ratings,\n                                        size = int(test_size * len(user_ratings)), \n                                        replace = False)\n        \n        # because the changing of the csr_matrix is expensive, we store the data and create new csr_matrix instead.\n        test_row.extend([user] * len(test_ratings))\n        test_col.extend(list(test_ratings))\n        test_data.extend(list(train[user, test_ratings].toarray()[0]))\n        \n        train[user, test_ratings] = 0\n    \n    test = csr_matrix((test_data, (test_row, test_col)), shape=(mat.shape[0], mat.shape[1]))\n    test.eliminate_zeros()\n    \n    return train, test","63a13f04":"train, test = train_test_split(mat)","bb981a2d":"# we need to reduce the size of user-item matrix so that we can compute the similarity based on the raw value.\n\ndef under_sampling(mat, ratio):\n    \n    sample_inds = np.random.choice(mat.shape[0],\n                    size = int(ratio * mat.shape[0]), \n                    replace = False)\n    \n    return train[sample_inds,:]","d5b5e560":"epsilon = 1e-9\nsmall_train = under_sampling(train, ratio = .1).toarray() + epsilon\nitem_corr_mat = np.corrcoef(small_train.T)","59a45148":"similar_anime = top_k_similar_anime(map_name_to_csr_anime_id['Naruto'],\n                                    top_k = 10,\n                                    corr_mat = item_corr_mat,\n                                    map_name = map_csr_anime_id_to_name)\n\nanime.loc[anime['name'].isin(similar_anime), ['name','genre']]","5352a5ae":"similar_anime = top_k_similar_anime(map_name_to_csr_anime_id['Death Note'],\n                                    top_k = 10,\n                                    corr_mat = item_corr_mat,\n                                    map_name = map_csr_anime_id_to_name)\n\nanime.loc[anime['name'].isin(similar_anime), ['name','genre']]","4fedb0a7":"del small_train, item_corr_mat\ngc.collect()","58e42e53":"from sklearn.decomposition import TruncatedSVD\n\nepsilon = 1e-9\nn_latent_factors = 10\n\nanime_svd = TruncatedSVD(n_components = n_latent_factors)\nanime_features = anime_svd.fit_transform(train.transpose()) + epsilon\n\nuser_svd = TruncatedSVD(n_components = n_latent_factors)\nuser_features = user_svd.fit_transform(train) + epsilon\n\nprint(f\"anime_features shape : {anime_features.shape}\\nuser_feature shape : {user_features.shape}\")","0a7e3cd7":"corr_mat = np.corrcoef(anime_features)","e514eeef":"similar_anime = top_k_similar_anime(map_name_to_csr_anime_id['Naruto'],\n                                    top_k = 10,\n                                    corr_mat = corr_mat,\n                                    map_name = map_csr_anime_id_to_name)\n\nanime.loc[anime['name'].isin(similar_anime), ['name','genre']]","ad7543be":"similar_anime = top_k_similar_anime(map_name_to_csr_anime_id['Death Note'],\n                                    top_k = 10,\n                                    corr_mat = corr_mat,\n                                    map_name = map_csr_anime_id_to_name)\n\nanime.loc[anime['name'].isin(similar_anime), ['name','genre']]","65ec324c":"from sklearn.metrics.pairwise import cosine_similarity\ncosine_mat = cosine_similarity(anime_features)","3c214fd4":"similar_anime = top_k_similar_anime(map_name_to_csr_anime_id['Naruto'],\n                                    top_k = 10,\n                                    corr_mat = cosine_mat,\n                                    map_name = map_csr_anime_id_to_name)\n\nanime.loc[anime['name'].isin(similar_anime), ['name','genre']]","a2e801cd":"similar_anime = top_k_similar_anime(map_name_to_csr_anime_id['Death Note'],\n                                    top_k = 10,\n                                    corr_mat = cosine_mat,\n                                    map_name = map_csr_anime_id_to_name)\n\nanime.loc[anime['name'].isin(similar_anime), ['name','genre']]","a2c355e9":"del user_features\ngc.collect()","9bca54d6":"from surprise import SVD, accuracy\nfrom surprise import Dataset, Reader\nfrom surprise.model_selection import cross_validate\nfrom surprise.model_selection.split import train_test_split\n\ndef pred2dict(predictions, top_k=None):\n    \n    rec_dict = defaultdict(list)\n    for user_id, anime_id, actual_rating, pred_rating, _ in tqdm(predictions):\n        rec_dict[user_id].append((anime_id, pred_rating))        \n        \n    return rec_dict\n\ndef get_top_k_recommendation(rec_dict, user_id, top_k, animeid2name):\n    \n    pred_ratings = rec_dict[user_id]\n    pred_ratings = sorted(pred_ratings, key=lambda x: x[1], reverse=True) # sort descendingly by pred_rating\n    pred_ratings = pred_ratings[:top_k]\n    recs = [animeid2name[e[0]] for e in pred_ratings]\n    \n    return recs\n\nreader = Reader(rating_scale=(1,10))\ndata = Dataset.load_from_df(collab_rating[['user_id','anime_id','rating']], reader)\ntrain, test = train_test_split(data, test_size=.2, random_state=42)\n\nalgo = SVD(random_state = 42)\nalgo.fit(train)\npred = algo.test(test)\naccuracy.rmse(pred)","36ab2f8a":"# predict known user and known item\nuser_ind = 0\nitem_ind = 0\n\nprint(f\"knows_users_{user_ind}? : {train.knows_user(user_ind)}, knows_item_{item_ind}? : {train.knows_item(item_ind)}\")\nalgo.predict(user_ind, item_ind)","6785231d":"# predict new user and new item\nuser_ind = 73515\nitem_ind = 11197\n\nprint(f\"knows_users_{user_ind}? : {train.knows_user(user_ind)}, knows_item_{item_ind}? : {train.knows_item(item_ind)}\")\nalgo.predict(user_ind, item_ind)","44a3d135":"animeid2name = {ind:name for ind,name in zip(collab_rating['anime_id'], collab_rating['name'])}\n\nrec_dict = pred2dict(pred)","3b2700e9":"corr_mat = cosine_similarity(algo.qi)","2613cc53":"similar_anime = top_k_similar_anime(map_name_to_csr_anime_id['Naruto'],\n                                    top_k = 10,\n                                    corr_mat = corr_mat,\n                                    map_name = map_csr_anime_id_to_name)\n\nanime.loc[anime['name'].isin(similar_anime), ['name','genre']]","13b3a9c6":"similar_anime = top_k_similar_anime(map_name_to_csr_anime_id['Death Note'],\n                                    top_k = 10,\n                                    corr_mat = corr_mat,\n                                    map_name = map_csr_anime_id_to_name)\n\nanime.loc[anime['name'].isin(similar_anime), ['name','genre']]","de55c37f":"user_id = '3'\n\n# Example rating of user#3\nprint(\"Showing recommendations for user: {}\".format(user_id))\nprint(\"Movies with high ratings from user\")\ndisplay(collab_rating[ (collab_rating['user_id'] == user_id) & (collab_rating['rating'] > 0)]\\\n.sort_values(['rating','name'],ascending=[False,True])\\\n.head(10)\\\n[['name','rating']])\n\n# Recommendation for user#3\nprint(\"Top 10 anime recommendations\")\nrecs = get_top_k_recommendation(rec_dict, user_id, 10, animeid2name)\nanime.loc[anime['name'].isin(recs), ['name','genre']]","19943186":"del corr_mat\ngc.collect()","b7d7d2ac":"# preprocessing \ndf = rating[rating['anime_id'].isin(anime['anime_id'])].copy()\ndf.loc[df['rating'] == -1, 'rating'] = 0\n\n# snippet code from Keras - CF for movie recommendations\nuser_ids = df[\"user_id\"].unique().tolist()\nuser2user_encoded = {x: i for i, x in enumerate(user_ids)}\nuserencoded2user = {i: x for i, x in enumerate(user_ids)}\n\nmovie_ids = df[\"anime_id\"].unique().tolist()\nmovie2movie_encoded = {x: i for i, x in enumerate(movie_ids)}\nmovie_encoded2movie = {i: x for i, x in enumerate(movie_ids)}\n\ndf[\"user\"] = df[\"user_id\"].map(user2user_encoded)\ndf[\"anime\"] = df[\"anime_id\"].map(movie2movie_encoded)\n\nnum_users = len(user2user_encoded)\nnum_movies = len(movie_encoded2movie)\ndf[\"rating\"] = df[\"rating\"].values.astype(np.float32)\n\n# min and max ratings will be used to normalize the ratings later\nmin_rating = min(df[\"rating\"])\nmax_rating = max(df[\"rating\"])\n\nprint(\n    \"Number of users: {}, Number of Movies: {}, Min rating: {}, Max rating: {}\".format(\n        num_users, num_movies, min_rating, max_rating\n    )\n)\n\n# Randomize the samples\ndf = df.sample(frac=1, random_state=42)\nx = df[[\"user\", \"anime\"]].values\n\n# Normalize the targets between 0 and 1. Makes it easy to train.\ny = df[\"rating\"].apply(lambda x: (x - min_rating) \/ (max_rating - min_rating)).values\n\n# Assuming training on 90% of the data and validating on 10%.\ntrain_indices = int(0.9 * df.shape[0])\nx_train, x_val, y_train, y_val = (\n    x[:train_indices],\n    x[train_indices:],\n    y[:train_indices],\n    y[train_indices:],\n)","771772da":"class RecommenderNet(keras.Model):\n    \n    def __init__(self, num_users, num_movies, embedding_size, **kwargs):\n        super(RecommenderNet, self).__init__(**kwargs)\n        self.num_users = num_users\n        self.num_movies = num_movies\n        self.embedding_size = embedding_size\n        self.user_embedding = layers.Embedding(\n            num_users,\n            embedding_size,\n            embeddings_initializer=\"he_normal\",\n            embeddings_regularizer=keras.regularizers.l2(1e-6),\n        )\n        self.user_bias = layers.Embedding(num_users, 1)\n        self.movie_embedding = layers.Embedding(\n            num_movies,\n            embedding_size,\n            embeddings_initializer=\"he_normal\",\n            embeddings_regularizer=keras.regularizers.l2(1e-6),\n        )\n        self.movie_bias = layers.Embedding(num_movies, 1)\n\n    def call(self, inputs):\n        \n        user_vector = self.user_embedding(inputs[:, 0])\n        user_bias = self.user_bias(inputs[:, 0])\n        \n        movie_vector = self.movie_embedding(inputs[:, 1])\n        movie_bias = self.movie_bias(inputs[:, 1])\n        \n        dot_user_movie = tf.tensordot(user_vector, movie_vector, 2)\n        \n        x = dot_user_movie + user_bias + movie_bias # Add all the components (including bias)\n        \n        return tf.nn.sigmoid(x) # The sigmoid activation forces the rating to between 0 and 1\n\nEMBEDDING_SIZE = 50\n\nmodel = RecommenderNet(num_users, num_movies, EMBEDDING_SIZE)\nmodel.compile(\n    loss=tf.keras.losses.BinaryCrossentropy(),\n    optimizer=keras.optimizers.Adam(lr=1e-5)\n)\n\nhistory = model.fit(\n    x = x_train,\n    y = y_train,\n    batch_size = 4096,\n    epochs = 5,\n    verbose = 1,\n    validation_data=(x_val, y_val),\n)","1bb4e211":"plt.plot(history.history[\"loss\"])\nplt.plot(history.history[\"val_loss\"])\nplt.title(\"model loss\")\nplt.ylabel(\"loss\")\nplt.xlabel(\"epoch\")\nplt.legend([\"train\", \"test\"], loc=\"upper left\")\nplt.show()","10fe2fb4":"user_input = layers.Input(shape=(1,))\nuser_bias = layers.Embedding(num_users, 1)(user_input)\nuser_embedding = layers.Embedding(\n    num_users, \n    EMBEDDING_SIZE,\n    embeddings_initializer=\"he_normal\",\n    embeddings_regularizer=keras.regularizers.l2(1e-6),\n)(user_input)\n\nitem_input = layers.Input(shape=(1,))\nitem_bias = layers.Embedding(num_movies, 1)(user_input)\nitem_embedding = layers.Embedding(\n    num_movies, \n    EMBEDDING_SIZE,\n    embeddings_initializer=\"he_normal\",\n    embeddings_regularizer=keras.regularizers.l2(1e-6),\n)(item_input)\n\ndot = layers.Dot(axes=1)([user_embedding, item_embedding])\nx = layers.Dense(64, activation='relu')(dot)\nx = layers.Dense(32, activation='relu')(x)\nx = layers.Dense(16, activation='relu')(x)\noutputs = layers.Dense(1, activation='sigmoid')(x)\n\nnew_model = keras.Model(inputs=[user_input, item_input], outputs=outputs, name=\"rec_net\")\nnew_model.summary()\n\nnew_model.compile(\n    loss=tf.keras.losses.BinaryCrossentropy(),\n    optimizer=keras.optimizers.Adam(lr=1e-5)\n)","c5a50127":"new_history = new_model.fit(\n    x = [x_train[:,0], x_train[:,1]],\n    y = y_train,\n    batch_size = 4096,\n    epochs = 5,\n    verbose = 1,\n    validation_data=([x_val[:,0], x_val[:,1]], y_val),\n)","76aec0a0":"plt.plot(new_history.history[\"loss\"])\nplt.plot(new_history.history[\"val_loss\"])\nplt.title(\"model loss\")\nplt.ylabel(\"loss\")\nplt.xlabel(\"epoch\")\nplt.legend([\"train\", \"test\"], loc=\"upper left\")\nplt.show()","34887bf2":"# embedding_2 is a item-latent matrix\nmodel.summary()\nitem_features = model.layers[2].get_weights()[0]\ncorr_mat = cosine_similarity(item_features)","f1863088":"similar_anime = top_k_similar_anime(map_name_to_csr_anime_id['Naruto'],\n                                    top_k = 10,\n                                    corr_mat = corr_mat,\n                                    map_name = map_csr_anime_id_to_name)\n\nanime.loc[anime['name'].isin(similar_anime), ['name','genre']]","2310119f":"similar_anime = top_k_similar_anime(map_name_to_csr_anime_id['Death Note'],\n                                    top_k = 10,\n                                    corr_mat = corr_mat,\n                                    map_name = map_csr_anime_id_to_name)\n\nanime.loc[anime['name'].isin(similar_anime), ['name','genre']]","a19aa130":"movie_df = anime.copy()\n\n# Let us get a user and see the top recommendations.\nuser_id = '3'\nmovies_watched_by_user = df[df.user_id == user_id]\nmovies_not_watched = movie_df[\n    ~movie_df[\"anime_id\"].isin(movies_watched_by_user.anime_id.values)\n][\"anime_id\"]\nmovies_not_watched = list(\n    set(movies_not_watched).intersection(set(movie2movie_encoded.keys()))\n)\nmovies_not_watched = [[movie2movie_encoded.get(x)] for x in movies_not_watched]\nuser_encoder = user2user_encoded.get(user_id)\nuser_movie_array = np.hstack(\n    ([[user_encoder]] * len(movies_not_watched), movies_not_watched)\n)\nratings = model.predict(user_movie_array).flatten()\ntop_ratings_indices = ratings.argsort()[-10:][::-1]\nrecommended_movie_ids = [\n    movie_encoded2movie.get(movies_not_watched[x][0]) for x in top_ratings_indices\n]\n\nprint(\"Showing recommendations for user: {}\".format(user_id))\nprint(\"Movies with high ratings from user\")\n\ntop_movies_user = (\n    movies_watched_by_user.merge(movie_df[['anime_id','name']], on='anime_id', how='left')\n    .sort_values(by=[\"rating\",\"name\"], ascending=[False,True])\n    .head(10)\n    .anime_id.values\n)\nmovie_df_rows = movie_df[movie_df[\"anime_id\"].isin(top_movies_user)].sort_values('name')\ndisplay(movie_df_rows[['name','genre']])\n\nprint(\"Top 10 anime recommendations\")\nrecommended_movies = movie_df[movie_df[\"anime_id\"].isin(recommended_movie_ids)]\ndisplay(recommended_movies[['name','genre']])","f7644990":"# create new input data\nnew_user_movie_array = np.array(user_movie_array)\n\n# find the max user index\nnew_user_ind = x_train[:,0].max() + 1\nnew_user_movie_array[:,0] = new_user_ind\n\n# try predict the user that doesn't appear in the training set\ntry:\n    new_ratings = model.predict(new_user_movie_array).flatten()\nexcept Exception as e:\n    print(\"When we try to predict the new user that doesn't appear in the training set before\")\n    print(\"We will get the following error\\n\")\n    print(e)","3ddb3671":"MOVIELENS_DATA_SIZE = '100k'\ndf = movielens.load_pandas_df(\n    size=MOVIELENS_DATA_SIZE,\n    header=[\"userID\", \"itemID\", \"rating\", \"timestamp\"]\n)\ntrain, test = python_chrono_split(df, 0.75)\nprint(train.shape, test.shape)","ba09cde0":"# Initial parameters\nTOP_K = 10\nEPOCHS = 50\nBATCH_SIZE = 1024\nSEED = DEFAULT_SEED\n\ndata = NCFDataset(train = train, test = test, seed=SEED)\n\nmodel = NCF (\n    n_users=data.n_users, \n    n_items=data.n_items,\n    model_type=\"NeuMF\",\n    n_factors=4,\n    layer_sizes=[16,8,4],\n    n_epochs=EPOCHS,\n    batch_size=BATCH_SIZE,\n    learning_rate=1e-3,\n    verbose=1,\n    seed=SEED\n)\n\nwith Timer() as train_time:\n    model.fit(data)\n\nprint(\"Took {} seconds for training.\".format(train_time.interval))","e2bbde3f":"# predict the data in the test set\npredictions = [[row.userID, row.itemID, model.predict(row.userID, row.itemID)]\n               for (_, row) in test.iterrows()]\n\n\npredictions = pd.DataFrame(predictions, columns=['userID', 'itemID', 'prediction'])","10dff642":"# try predict new user, and new item\nmodel.predict(943, 1681)\n\n# it throws an error if you try to look up for the new user, or new product.\n# thus this implementation is still a model-based approach\ntry:\n    model.predict(943, 1700)\nexcept KeyError as key:\n    print(\"When we try to predict the new user that doesn't appear in the training set before\")\n    print(\"We will get the following error\\n\")\n    print(f'IndexError: There is no {key} index to lookup from the model')\nexcept Exception as e:\n    print(e)\n    raise","f86e7957":"with Timer() as test_time:\n\n    users, items, preds = [], [], []\n    \n    # get the prediction for all user-item pairs in train set.\n    item = list(train.itemID.unique())\n    for user in train.userID.unique():\n        user = [user] * len(item) \n        users.extend(user)\n        items.extend(item)\n        preds.extend(list(model.predict(user, item, is_list=True)))\n    \n    all_predictions = pd.DataFrame(data={\"userID\": users, \"itemID\":items, \"prediction\":preds})\n\n    # remove seen item from the prediction.\n    merged = pd.merge(train, all_predictions, on=[\"userID\", \"itemID\"], how=\"outer\")\n    all_predictions = merged[merged.rating.isnull()].drop('rating', axis=1)\n\nprint(\"Took {} seconds for prediction.\".format(test_time.interval))\n\n# evaluate the top-k unseen recommendation and the test set.\neval_map = map_at_k(test, all_predictions, col_prediction='prediction', k=TOP_K)\neval_ndcg = ndcg_at_k(test, all_predictions, col_prediction='prediction', k=TOP_K)\neval_precision = precision_at_k(test, all_predictions, col_prediction='prediction', k=TOP_K)\neval_recall = recall_at_k(test, all_predictions, col_prediction='prediction', k=TOP_K)\n\nprint(\"MAP:\\t%f\" % eval_map,\n      \"NDCG:\\t%f\" % eval_ndcg,\n      \"Precision@K:\\t%f\" % eval_precision,\n      \"Recall@K:\\t%f\" % eval_recall, sep='\\n')","c87bd1d5":"del train, test\ngc.collect()","fcdcd619":"# sort item_genre_mat based on the csr_anime_id\nsorted_list = [k for k, v in sorted(map_name_to_csr_anime_id.items(), key=lambda item: item[1])]\nitem_genre_mat = item_genre_mat.loc[sorted_list,:]\n\n# creat the cosine similarity matrix from each technique\nct_cosine_mat = cosine_similarity(item_genre_mat) # content-based similarity from Genre\ncf_cosine_mat = cosine_similarity(anime_features) # CF-based latent features from truncatedSVD\n\n# averaging the prediction from content-based and CF technique\nensemble_mat = (ct_cosine_mat + cf_cosine_mat) \/ 2","8590623a":"similar_anime = top_k_similar_anime(map_name_to_csr_anime_id['Naruto'],\n                                    top_k = 10,\n                                    corr_mat = ensemble_mat,\n                                    map_name = map_csr_anime_id_to_name)\n\nanime.loc[anime['name'].isin(similar_anime), ['name','genre']]","70db7761":"similar_anime = top_k_similar_anime(map_name_to_csr_anime_id['Death Note'],\n                                    top_k = 10,\n                                    corr_mat = ensemble_mat,\n                                    map_name = map_csr_anime_id_to_name)\n\nanime.loc[anime['name'].isin(similar_anime), ['name','genre']]","426acf44":"del item_genre_mat, anime_features, ct_cosine_mat, cf_cosine_mat, ensemble_mat\ngc.collect()","c6e782b3":"# Sample data\nCOL_USER = \"UserId\"\nCOL_ITEM = \"MovieId\"\nCOL_RATING = \"Rating\"\nCOL_PREDICTION = \"Prediction\"\n\nHEADER = {\n    \"col_user\": COL_USER,\n    \"col_item\": COL_ITEM,\n    \"col_rating\": COL_RATING,\n    \"col_prediction\": COL_PREDICTION,\n}\n\ndf_true = pd.DataFrame(\n        {\n            COL_USER: [1, 1, 1, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3],\n            COL_ITEM: [1, 2, 3, 1, 4, 5, 6, 7, 2, 5, 6, 8, 9, 10, 11, 12, 13, 14],\n            COL_RATING: [5, 4, 3, 5, 5, 3, 3, 1, 5, 5, 5, 4, 4, 3, 3, 3, 2, 1],\n        }\n    )\ndf_pred = pd.DataFrame(\n    {\n        COL_USER: [1, 1, 1, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3],\n        COL_ITEM: [3, 10, 12, 10, 3, 5, 11, 13, 4, 10, 7, 13, 1, 3, 5, 2, 11, 14],\n        COL_PREDICTION: [14, 13, 12, 14, 13, 12, 11, 10, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5]\n    }\n)","6e3ef7db":"def calculate_hit_table(df_true, df_pred, COL_USER, COL_ITEM, COL_RATING, COL_PREDICTION, k=None):\n    \n    '''\n    filter the top-k product from the prediction dataframe\n    and assign the predicted `rank` column for the further calculation\n    the `rank` column will be used in calculating `ndcg` metric\n    `rank` is defined as the rank of `hit` item in the prediciton.\n    '''\n    \n    if k is None:\n        \n        top_k_item = df_pred.copy()\n\n    else:\n        \n        # from prediction data frame, groupby and filter the top 'k' prediction of each user.\n        top_k_item = df_pred.groupby(COL_USER, as_index=False)\\\n        .apply(lambda x: x.nlargest(k, COL_PREDICTION))\\\n        .reset_index(drop=True)\n\n    # assing predicted rank from 1 to k for each user\n    # we only care the rank of relavant item in the prediction.\n    # regardless of the rank in the ground truth, if the top-k prediction exists in the ground truth\n    # we calculate the dcg based on that predicted rank\n    top_k_item['rank'] = top_k_item.groupby(COL_USER, sort=False).cumcount() + 1\n\n    # match the prediction with the ground truth\n    # keep only the prediction appeared in ground truth\n    top_k_item = top_k_item.merge(\n        df_true,\n        on = [COL_USER, COL_ITEM]\n    )[[COL_USER, COL_ITEM, \"rank\"]]\n    \n    return top_k_item\n\ndef calculate_hit_count_table(df_hit, df_true, COL_USER, COL_ITEM, COL_RATING, COL_PREDICTION):\n    \n    '''\n    `hit` is defined as the number of item in prediction appeared in the ground truth\n    for each user, calculate `hit` and the `actual` total number of ground truth\n    this dataframe can be used to calculate the metrics `precision_at_k` and `recall_at_k`.\n    '''\n    \n    # count the actual ground truth number for each user\n    # this actual count will be used for calculating the 'idcg'\n    hit_count = pd.merge(\n        df_hit.groupby(COL_USER, as_index=False)[COL_USER].agg({\"hit\":\"count\"}),\n        df_true.groupby(COL_USER, as_index=False)[COL_USER].agg({\"actual\":\"count\"}),\n        on = COL_USER\n            )\n\n    # show the number of hit in the prediction, and the actual number of ground truth\n    return hit_count\n\ndef precision_at_k(df_hit_count, k):\n    \n    '''\n    calcualte the precision at k for each user and normalize by the number of users.\n    '''\n    \n    df = df_hit_count.copy()\n    \n    # calculate the precision at k for each user\n    df['precision'] = (\n        df['hit'] \/ k\n    )\n    \n    # calculate the average precision over all users\n    return df['precision'].mean()\n\ndef recall_at_k(df_hit_count, k):\n    \n    '''\n    calcualte the recall at k for each user and normalize by the number of users.\n    '''\n    \n    df = df_hit_count.copy()\n    \n    # calculate the precision at k for each user\n    df['recall'] = (\n        df['hit'] \/ df['actual']\n    )\n    \n    # calculate the average precision over all users\n    return df['recall'].mean()\n\ndef ndcg_at_k(df_hit, df_hit_count, COL_USER, k):\n    \n    '''\n    calculate the ndcg for each user and normalize by the number of users.\n    '''\n    \n    if df_hit.empty:\n        return 0.00\n\n    df_dcg = df_hit.copy()\n\n    # assign all relavants(i) value for the hit item equal to 1\n    # calcuate the dcg based on the predicted rank of each item\n    # np.log1p -> Calculates log(1 + x)\n    \n    # THINGS TO KNOW : here for the user_id 1 -> the movie_id 3 got the 1st rank in prediction.\n    # even if the ground truth rank is 3, it doesn't effect the dcg calcuation.\n    df_dcg['dcg'] = 1 \/ np.log1p(df_dcg['rank'])\n\n    # sum up discount cumalative gain for each user\n    df_dcg = df_dcg.groupby(COL_USER, as_index=False, sort=False).agg({'dcg':'sum'})\n\n    # merge number of actual ground truth to calculate the idcg\n    df_ndcg = pd.merge(df_dcg, df_hit_count, on = [COL_USER])\n\n    # calcuate the idcg by summing the relavant over the range of ground truth or top-k rank.\n    df_ndcg['idcg'] = df_ndcg['actual'].apply(\n        lambda x: sum(1 \/ np.log1p(range(1, min(x, k) + 1)))\n    )\n\n    # calcualte the ndcg from every user\n    df_ndcg['ndcg'] = (df_ndcg['dcg'] \/ df_ndcg['idcg'])\n    \n    return df_ndcg['ndcg'].mean()\n\ndef map_at_k(df_hit, df_hit_count, COL_USER):\n    \n    '''\n    calculate the average precision of each user and normalize by the number of users.\n    '''\n    \n    df_map = df_hit.copy()\n    \n    # assign the hit rank for each item in the prediction\n    df_map['hit_rank'] = df_map.groupby(COL_USER, as_index=False, sort=False)[COL_USER].cumcount() + 1\n    \n    # calculate the recipocal rank by dividing the hit_rank with the predicted rank\n    # if all ground truth appears in the prediction and all of it ranks correctly\n    # then we would get the recipocal_rank = 1 for every items\n    df_map['recipocal_rank'] = df_map['hit_rank'] \/ df_map['rank']\n    \n    # sum up the recipocal_rank value \n    df_map = df_map.groupby(COL_USER, as_index=False, sort=False).agg({'recipocal_rank':'sum'})\n    df_map = pd.merge(df_map, df_hit_count, on = [COL_USER])\n    \n    # average the recipocal rank by the number of actual item in ground truth\n    df_map['avg_precision'] = df_map['recipocal_rank'] \/ df_map['actual']\n    \n    # mean the avg_precision for all users\n    return df_map['avg_precision'].mean()","75f7189d":"k = 3\n\ndf_hit = calculate_hit_table(df_true, df_pred, COL_USER, COL_ITEM, COL_RATING, COL_PREDICTION, k)\nprint(\"Hit table - based on the prediction here is the item appeared in the ground truth with the predicted rank\")\ndisplay(df_hit)\n\ndf_hit_count = calculate_hit_count_table(df_hit, df_true, COL_USER, COL_ITEM, COL_RATING, COL_PREDICTION)\nprint(\"Hit count table - the number of hit item for the top k prediction and the total actual number of ground truth\")\ndisplay(df_hit_count)\n\nprint(f\"Precision@{k} : {precision_at_k(df_hit_count, k):.4f}\")\nprint(f\"Recall@{k} : {recall_at_k(df_hit_count, k):.4f}\")\nprint(f\"NDCG@{k} : {ndcg_at_k(df_hit, df_hit_count, COL_USER, k):.4f}\")\nprint(f\"MAP : {map_at_k(df_hit, df_hit_count, COL_USER):.4f}\")","ce840694":"**Intuition**\n\n","1d04776e":"# 3.2.2 Deep learning MF - Generalized Matrix Factorization (MF)\n\nUsually, in the MF method, we tried to learn the user-item interaction or tried to reconstruct the predicted rating with the inner product of the shared lated features.\n\nNow, for the deep learning approach, we use the different internal engine which is the DNNs to estimate the user and item laten features. We estimate the shared latent feature minimizing the target loss function (binary cross entropy).\n\nAfter that, we used the fitted model to predict the pair of anime that each customer have never watched to see what to recommend next.\n\n**References**\n- [Keras - Collaborative Filtering for Movie Recommendations](https:\/\/keras.io\/examples\/structured_data\/collaborative_filtering_movielens\/)\n- [Medium - Neural Collaborative Filtering](https:\/\/towardsdatascience.com\/neural-collaborative-filtering-96cef1009401)\n- [ACM - Neural Collaborative Filtering](https:\/\/dl.acm.org\/doi\/abs\/10.1145\/3038912.3052569)\n- [Github - Recommender Microsoft](https:\/\/github.com\/microsoft\/recommenders)","36c45ad1":"The surprise package has an ability to deal with the user and item that they have never seen before.","948d8c46":"# 1. Popular based recommendation\n\n![img](https:\/\/images.unsplash.com\/photo-1583258292688-d0213dc5a3a8?ixid=MnwxMjA3fDB8MHxzZWFyY2h8NXx8bWFya2V0fGVufDB8fDB8fA%3D%3D&ixlib=rb-1.2.1&auto=format&fit=crop&w=500&q=60)\n\n**Introduction**\n\nFor any machine learning problems, we need a **baseline model or method** to use as a reference whether our approach is good or not. \n\nOur machine learning prediction or sophsticated analysis should, at least, beat those baseline performance.\n\nFor recommendation system, we can make a simple baseline score with **popular item recommendation**\n\n**To define the popularity of the item**\n\nRegarding the IMDB syetem, there have a metrics called `weighted rating system` that is used to score the rating of each movie.\n\nHere is the formular \n```\n(WR) = (v \u00f7 (v+m)) \u00d7 R + (m \u00f7 (v+m)) \u00d7 C \n```\nwhere:\n- R = average rating for the movie. (rating)\n- v = number of votes for the movie. (members)\n- m = minimum votes required to be listed in the Top 250 (defined by > percentile 80 of total votes)\n- C = the average rating across the whole dataset.\n\n**Drawback**\n- It's not personalized. All the users will get the same exact list of popularity based recommendation.\n\n**Actions**\n\n- For new users, if we don't have any information about them we can provide the list based on ranking the `vote_count` or `weighted_rating` as a best guess.\n- In real world, this is the result when you see the section \"Popular on Netflix\"\n\n**Reference**\n- https:\/\/help.imdb.com\/article\/imdb\/track-movies-tv\/ratings-faq\/G67Y87TFYYP6TWAV?ref_=helpms_helpart_inline#","9be64ba2":"## Popularity based on the weighted score","b9c6f63f":"### Pearson correlation","70931879":"## Prediction - GMF","29f3c145":"## Prediction - Funk MF (SVD-like algorithm in surprise)","b1df6fdf":"**Summary**\n\n- We combine the similarity between both methods with the averaging. You can see the result is changed towards the more similar genre compared to the CF-based method and it's still shows other animes compared to the content based recommendation.\n\n- Here we expect somewhat better recommendation by leveraging the advantage of each technique.","d24f4f92":"## Compute similarity\n\n- When we try to compute the similarity with the whole matrix (75K x 11K) shape. We doesn't have enough memory to store the whole matrix.\n- So, for this purpose of illustration, we will undersampling the data set by 90%.","f574ab73":"### Cosine similarity","e0cd04b4":"### Item-based recommendation\n\nFor item-based recommendation, we can extract the latent factors matrix from the keras model. \n\nAfter that we can find the similarity between each item with respect to the latent factors.","37551557":"## 3.2 Model-based approach\n\n**Introduction**\n\n> Model-based CF uses machine learning algorithms to predict users\u2019 rating of unrated items. There are many model-based CF algorithms, the most commonly used are matrix factorization models such as to applying a SVD to reconstruct the rating matrix, latent Dirichlet allocation or Markov decision process based models. [Building a memory based collaborative filtering recommender](https:\/\/towardsdatascience.com\/how-does-collaborative-filtering-work-da56ea94e331)\n\n\n**Type**\n\n> 1. Matrix Factorization based\n    - TruncatedSVD\n    - Funk Matrix Factorization (SVD-like algorithm) (Surprise) <br>\n    >  Note that, in Funk MF **no singular value decomposition is applied**, it is a SVD-like machine learning model. [Wiki\/Matrix_factorization](https:\/\/en.wikipedia.org\/wiki\/Matrix_factorization_(recommender_systems))\n    - Probabilistic Matrix Factorization (fastai)\n    - Non negative Matrix Factorization (Surprise)\n2. Deep learning based\n    - Neural Collaborative filtering (keras)\n    \n**Implementation**\n\n**Item-based**\n\n- We can extract the `user_features` and `anime_features` based on the matrix factorization technique. The purpose is to get the underlying latent matrix generated the user-item interaction matrix.\n- After that, We select the `anime_features` for example. We calculate the `pearson correlation`, `cosine similarity`, or `KNN Neareast neighbour` between each item and check up for the `top_k` most similar item to recommend. \n- Just to note that the item is **not similar in term of the content** (like the content-based recommendation) and **not similar in term of the explicit user behavior** (like the memory-based CF recommendation), but in term of **latent factors** (underlying factors that we can't interpret explicitly) based on the matrix decomposition.\n\n**User-based**\n- We can extract the `user_features` and `anime_features` based on the matrix factorization technique. The purpose is to get the underlying latent matrix generated the user-item interaction matrix.\n- We find the group of similar users (the group size is arbitarily) based on the `pearson correlation`, `cosine similarity`, or `KNN Neareast neighbour`.\n- We average the rating of each item based on the group of similar users\n- Rank the item based on the averate rating descendently, and recommend the target user with the movie that they never rated it before ranking from the highest top-k average rating.\n\n**References**\n- [Building a memory based collaborative filtering recommender](https:\/\/towardsdatascience.com\/how-does-collaborative-filtering-work-da56ea94e331)\n- [Wiki\/Matrix_factorization](https:\/\/en.wikipedia.org\/wiki\/Matrix_factorization_(recommender_systems))","8b2a2483":"### Pearson correlation","97c01ea5":"**Summary**\n\n- You can see that with this size of data and the limitation of the resouce computation. This approach is hard to scale up in the real world.\n- It leads to the problem that we can't leverage the pattern inside our data. ","c88a50d0":"### Item-based recommendation","37570d39":"\n\n## 3.2.1.1 Matrix factorization - TruncatedSVD (sklearn)\n\n### TruncatedSVD\n\n![img](https:\/\/www.researchgate.net\/profile\/Jun-Xu-67\/publication\/321344494\/figure\/fig1\/AS:702109309751298@1544407312766\/Diagram-of-matrix-factorization.png)\n\n> Truncated SVD shares similarity with PCA while SVD is produced from the data matrix and the factorization of PCA is generated from the covariance matrix. Unlike regular SVDs, truncated SVD produces a factorization where the number of columns can be specified for a number of truncation. [recommender-system-singular-value-decomposition-svd-truncated-svd](https:\/\/towardsdatascience.com\/recommender-system-singular-value-decomposition-svd-truncated-svd-97096338f361)\n\n**Reference**\n- [recommender-system-singular-value-decomposition-svd-truncated-svd](https:\/\/towardsdatascience.com\/recommender-system-singular-value-decomposition-svd-truncated-svd-97096338f361)","6f8294f5":"### User-based recommendation","e8f25877":"# Evaluation\n\nThere are several useful metrics to evaluate the performance of the recommender system. It's important for anyone to know how each metric has been derived so that they can choose the right one suiting the objective of their project.\n\nWe can classify the overall metrics into 4 categories\n1. Rating - evaluate how accuracte the recommender is at predicting ratings.\n2. Ranking - evaluate how well the recommender is at ranking the relavant products.\n3. Classification - evaluate the performance interm of binanry labels.\n4. Non-accuracy - evaluate other aspects of the recommender system.\n\nTo select the right metric for your objective is crucial. Sometimes, we may need the precise predicted rating, but the other times we may need to care about the diversity or coverage of the recommendation as well.\n\nNow, let's compared each metrics for the aforementioned techniques in this notebook and let's see what is the best recommender so far.\n\n**References**\n- [Github - Recommender Microsoft Evaluation Examples](https:\/\/github.com\/microsoft\/recommenders\/tree\/main\/examples\/03_evaluate)\n- [Medium, Normalized Discounted Cumulative Gain](https:\/\/towardsdatascience.com\/normalized-discounted-cumulative-gain-37e6f75090e9)","33bf464c":"## Ranking \n\nThere are 2 kinds of use cases in the ranking metrics which are\n1. Hit - How much number of relavant items appeared in the top \"K\" recommendation.\n    - Precision, Recall\n2. Ranking - How well the recommender system ranks the relavant items prior than irrelavant items.\n    - MeanAveragePrecision (MAP), NormalizedDiscountedCumulativeGain (NDCG)","9e34f41a":"# 3.2.3 Deep learning MF - Neural Collaborative Filtering (NCF)\n\nDue to the resouce limitation, we decide to swith the dataset for this implementation to movielens@100K","51e3f885":"## 3.1 Memory based approache\n\n**Introduction**\n\n> The key difference of memory-based approach from the model-based techniques is that we are **not learning any parameter** using gradient descent (or any other optimization algorithm). The closest user or items are calculated only by using **Cosine similarity** or **Pearson correlation coefficients**, which are only based on arithmetic operations. [various-implementations-of-collaborative-filtering](https:\/\/towardsdatascience.com\/various-implementations-of-collaborative-filtering-100385c6dfe0) \n\n> Memory-based methods use user rating historical data to compute the similarity between users or items. The idea behind these methods is to define a similarity measure between users or items, and find the most similar to recommend unseen items. [Building a memory based collaborative filtering recommender](https:\/\/towardsdatascience.com\/how-does-collaborative-filtering-work-da56ea94e331)\n\n**Implementatiuon**\n- Due to the size of user-item matrix. It's impossible to compute the user_features with `n_users x n_users` shape or the anime_features with `n_animes x n_animes` shape with the current running instance.\n- Therefore, we drop the number of row for demostrating purpose.\n- Also, another way is to create the matrix with the lower dimension with the deterministic approach such as 'TruncatedSVD'\n\n**User-based**\n- We find the group of similar users (the group size is arbitarily) based on the `pearson correlation`, `cosine similarity`, or `KNN Neareast neighbour`.\n- We average the rating of each item based on the group of similar users\n- Rank the item based on the averate rating descendently, and recommend the target user with the movie that they never rated it before ranking from the highest `top_k` average rating.\n\n**Item-based**\n\n- We find the group of similar item based on the `pearson correlation`, `cosine similarity`, or `KNN Neareast neighbour`\n- Select up to the `top_k` most similar item to recommend. \n- Just to note that the item is **not similar in term of the content** (like the content-based recommendation) but it's **similar in term of the explicit rating from the user behavior** (the similarity between each item from the user-item matrix)\n\n**Drawback**\n- It's not scalable due to the sprasity of the data.\n- We needs to construct the similarity matrix everytime the new user comes. (Hard to maintain, and operationalize)\n\n**Actions**\n- The list result can be showed in the front-end application like \"Made for you\" -> Provide the list of recommended animes.\n- The list result can be showed in the front-end application like \"Because you like Naruto\" -> Provided the list of recommended animes.\n\n**Reference**\n- [various-implementations-of-collaborative-filtering](https:\/\/towardsdatascience.com\/various-implementations-of-collaborative-filtering-100385c6dfe0)\n- [Building a memory based collaborative filtering recommender](https:\/\/towardsdatascience.com\/how-does-collaborative-filtering-work-da56ea94e331)","eedb5beb":"### Calcualte ranking metrics step by step\n\n- These below code are from the recommender.evaluation.python_evalution.ndcg_at_k source code\n- I've comment it line by line to grapse a better understanding of how to calculate each metrics\n\n**Precision, Recall**\n\n![img](https:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/2\/26\/Precisionrecall.svg\/350px-Precisionrecall.svg.png)\n\nFor these metrics, we only need to know the number of `hit` item in the top-k prediction compared to the ground truth and the total `actual` number of ground truth.\n- Precision meands based on all the top-k prediction how much percentage of the item is in the ground truth\n- Recall means based on all the ground truth prediction how much percentage of the item is in the top-k prediction\n\nThe above figure shows you a better illustration of the calculation\n\n**NDCG**\n\nThere are 3 things we need to know in order to calculate the NDCG\n1. cumulative gain - how many number of `hit` from our prediction compared to the ground truth\n2. discount cumalative gain - adjust the cumalative gain based on the predicted rank\n3. ideal discount cumalative gain - calculate the best case DCG from the number of actual ground truth\n\n- The NDCG is the proportion of the DCG that we got from the model compared to the best possible sorted case from the IDCG.\n- To calcuate the single number represented the model performance, we can average the score by the number of user.\n\n**References**\n1. [Wiki - Discount cumulative gain](https:\/\/en.wikipedia.org\/wiki\/Discounted_cumulative_gain)\n2. [Recommender source code](https:\/\/microsoft-recommenders.readthedocs.io\/en\/latest\/_modules\/recommenders\/evaluation\/python_evaluation.html#ndcg_at_k)\n3. [Stanford Handout Reference](http:\/\/web.stanford.edu\/class\/cs276\/handouts\/EvaluationNew-handout-6-per.pdf)","5644c4de":"**Summary**\n\n- Here we use the Funk MF algorithms to create the latent factors matrix, and now we can build both the user-based, item-based recommendation.\n- We also randomly split out some users from the train set into the test set for the validation purpose.","783cc97d":"**Summary**\n- We use the deep learning architecture to learn the user-item interaction.\n- This way we have another kind of recommendation engine providing the different kind of recommendation.","b2d63fb1":"## 3.2.1.2 Matrix factorization - Funk MF (SVD-like algorithm in surprise)","14469895":"## Rating\n\n- The `rating` dataframe contains raw data of how each user rate each anime.\n- The rating score is in range [0,10]\n\n### Metadata\n\n- user_id - non identifiable randomly generated user id.\n- anime_id - the anime that this user has rated.\n- rating - rating out of 10 this user has assigned (-1 if the user watched it but didn't assign a rating).","3a807e04":"# Hands-on implementation for various recommender systems.\n\n![img](https:\/\/images.unsplash.com\/photo-1601944179066-29786cb9d32a?ixid=MnwxMjA3fDB8MHxzZWFyY2h8MTN8fG5ldGZsaXh8ZW58MHwwfDB8fA%3D%3D&ixlib=rb-1.2.1&auto=format&fit=crop&w=500&q=60)\n\nInspired by : https:\/\/github.com\/alanchn31\/recommender-system\n\n**What kind of data do we need to implement the recommendation system?**\n\nFor our target variable\n1. Explicit rating - A rating given by a user to an item on a scale (could be score 1 to 5, or 1 to 10).\n2. Implicit rating - A measurement to indicate the user preference indirectly. It could be a view, click, like, how long they read, how much they bought, etc.\n\nFor other features\n1. Content feature - Genre, Type, Number of subscriber, Age, Published channel, etc.\n\nHere we use `Anime-recommendations-database` as an input data for our tutorial.\n\n## Let's see what we have in our data source.","66a277d2":"# 2. Content-based recommendation\n\n**Introduction**\n\nFor example, if a person has liked the movie \u201cInception\u201d, then this algorithm will recommend movies that fall under the same genre.\n\nHere we create a better way of recommendation by introducing other features of the content into our engine. \n\nIt's an improvement compared to the popularity based recommendation we mentioned earlier. \n\nNow, the customer who read, watch, or like any kinds of specific products will get a recommendation based on the product they interacted in the past.\n\n![img](https:\/\/i.ibb.co\/S5GWr1r\/Content-recommendation.png)\n\n> Consider the example of Netflix. They save all the information related to each user in a vector form. This vector contains the past behavior of the user, i.e. the movies liked\/disliked by the user and the ratings given by them. This vector is known as the profile vector. All the information related to movies is stored in another vector called the item vector. Item vector contains the details of each movie, like genre, cast, director, etc. The content-based filtering algorithm finds **the cosine of the angle between the profile vector and item vector**, i.e. cosine similarity.\n\n\n\n**drawback**\n- A major drawback of this algorithm is that it is limited to recommending items that are of the same type. \n- It will **never recommend products which the user has not bought or liked** in the past. So if a user has watched or liked only action movies in the past, the system will recommend only action movies.\n\n**reference**\n- [Content based recommender system](https:\/\/towardsdatascience.com\/content-based-recommender-systems-28a1dbd858f5)","9f9a550a":"## Evaluation","2c410643":"## Popularity based on the number of votes count","65b20965":"### User-based recommendation","41cf18fd":"# 3. Collaborative filtering\n\n**Introduction**\n\n> The collaborative filtering algorithm uses **\u201cUser Behavior\u201d** for recommending items. This is one of the most commonly used algorithms in the industry as it is not dependent on any additional information. There are different types of collaborating filtering techniques. [comprehensive-guide-recommendation-engine-python](https:\/\/www.analyticsvidhya.com\/blog\/2018\/06\/comprehensive-guide-recommendation-engine-python\/)[1]\n\nThere are 2 types of memory based collaborative filtering \n\n![img](https:\/\/predictivehacks.com\/wp-content\/uploads\/2020\/06\/recommenders_systems.png) <br>\n1. User based - The user-similarity matrix will consist of some distance metric that measures the similarity between any two pairs of users.\n> This algorithm is useful when the number of users is less. Its **not effective when there are a large number of users** as it will take a lot of time to compute the similarity between all user pairs. This leads us to item-item collaborative filtering, which is effective when the number of users is more than the items being recommended. [1]\n2. Item based - Likewise, the item-similarity matrix will measure the similarity between any two pairs of items.\n\n**drawback**\n- What will happen if a new user or a new item is added in the dataset? It is called a **Cold Start**. There can be two types of cold start.\n    1. Visitor - Since there is no history of that user, the system does not know the preferences of that user\n        - These can be determined by what has been **popular recently overall or regionally**.\n    2. Product - More the interaction a product receives, the easier it is for our model to recommend that product to the right user.\n        - We can make use of **Content based filtering** to solve this problem. \n\n**References**\n- [comprehensive-guide-recommendation-engine-python](https:\/\/www.analyticsvidhya.com\/blog\/2018\/06\/comprehensive-guide-recommendation-engine-python\/)\n- [intro-to-collaborative-filtering](https:\/\/www.ethanrosenthal.com\/2015\/11\/02\/intro-to-collaborative-filtering\/)\n- [intro-to-recommender-system-collaborative-filtering](https:\/\/towardsdatascience.com\/intro-to-recommender-system-collaborative-filtering-64a238194a26)","1c28e770":"## Anime\n\n- The `anime` dataframe contains the data related to the anime. \n\n### Metadata\n\n- anime_id - myanimelist.net's unique id identifying an anime.\n- name - full name of anime.\n- genre - comma separated list of genres for this anime.\n- type - movie, TV, OVA, etc.\n- episodes - how many episodes in this show. (1 if movie).\n- rating - average rating out of 10 for this anime.\n- members - number of community members that are in this anime's \"group\".","14630498":"**Summary**\n\nWith this method, you can see that we can compute the similarity based on the specific number of latent factor.\n- Now, the recommendation would be based on some latent factors that we cannot explain directly.\n- But in mathematically speaking, it will be the top latent factor that minimize the loss between the Actual rating and Reconstructed rating.\n- You can see that now we reduce the size of matrix compared to the one in memory-based approach. however, it's still not good enough approach because eventually when the user, or item size growing with the time.<br> Soon it will reach the limitation of computation resouce as well.","91508cad":"# 3.2.4 Hybrid recommendation - Content based + Collaborative filtering\n\nLet's combine the both techniques between the content based and collaborative filtering based recoomendations together.\n\nWe can combine the result for those two techniques with the ensemble method.\n\nHere we use the item-based recommendation because we can dierectly combine the similarity matrix together.\n\n**Implemenation**\n\nThere are some anime that are missed between `anime` and `rating` dataframe.\n\nWe clean it by using only the anime that is existed on both dataframe.\n\nAfter that, we calcuate the cosine similarity matrix for Content-based and CF based\n\nThen combining it together with the averaging.\n\n**References**\n- [Creating a Hybrid Content-Collaborative Movie Recommender Using Deep Learning](https:\/\/towardsdatascience.com\/creating-a-hybrid-content-collaborative-movie-recommender-using-deep-learning-cc8b431618af)\n","8fb5d3f9":"## Item-based Collaborative filtering","6d3bcebe":"# Basic summary"}}