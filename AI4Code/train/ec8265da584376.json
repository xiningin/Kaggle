{"cell_type":{"f6b393a9":"code","3e908134":"code","56dd1488":"code","f7b96779":"code","18391782":"code","e52acfdb":"code","f3218ab6":"code","e7da06e3":"code","30c29052":"code","e52b1891":"code","f8e209f0":"code","9bff7107":"code","0908b8f8":"code","5c8d1dea":"code","a51d8d68":"code","3c2db12c":"code","d26828ab":"code","04be4655":"code","e536935b":"code","f36a4a40":"code","8f56cf92":"code","60f469ec":"code","ce407a89":"code","96801103":"code","4f8110ca":"code","9a78ec2b":"code","d148779e":"code","ba8f7a8c":"code","47ea330f":"code","08db3ba3":"code","79d9bd6d":"code","5caaa7b8":"code","590aa919":"code","79158c42":"code","12d031db":"code","ceae46e2":"code","89c5f7fe":"code","6ff31cc2":"code","64e0050e":"code","060872e0":"code","ac2ba253":"code","f881a48f":"code","98bd3286":"code","d6dc74f5":"markdown","1e35544f":"markdown","82122b32":"markdown","abc7e26f":"markdown","924cd867":"markdown","7595ce8c":"markdown","e1263536":"markdown","a3ab68dd":"markdown","fa002b5c":"markdown","100471ae":"markdown","f7b28539":"markdown","939e6516":"markdown","a1f0b52c":"markdown","57bf0936":"markdown","bb375f52":"markdown","3a3a7462":"markdown","bfe7ff34":"markdown","63b49033":"markdown","49076365":"markdown","ccbadb40":"markdown","d89b9449":"markdown","fbf6767a":"markdown","de1cfb57":"markdown","4c582efe":"markdown","15e50332":"markdown","f94e54ed":"markdown","faa681d0":"markdown","2facd78a":"markdown","aef42f81":"markdown","f36bd36e":"markdown","68eb70ee":"markdown","0d59dd49":"markdown","d45c6640":"markdown","ef02b24a":"markdown","66af5f1e":"markdown"},"source":{"f6b393a9":"# Import packages \nimport os\nimport gc\nimport pickle\nimport warnings\nimport random\nfrom random import sample\nfrom tqdm import tqdm\n\nimport pandas as pd \nimport numpy as np \nfrom sklearn.feature_selection import mutual_info_regression as mir\nfrom sklearn.metrics.cluster import normalized_mutual_info_score\nfrom sklearn.metrics.cluster import adjusted_mutual_info_score\nimport matplotlib.pyplot as plt \nimport seaborn as sns \nimport plotly.graph_objects as go \nimport plotly.express as px \n\n# Configuration \nwarnings.simplefilter('ignore')\npd.set_option('display.max_columns', 200)\nrandom.seed(2022)","3e908134":"# Variable definitions\nDATA_PATH_RAW = \"..\/input\/ubiquant-market-prediction\"\nDATA_PATH_RAW_CUSTOM = \"..\/input\/ubiquant-raw\"\nBASE_COLS = ['row_id', 'time_id', 'investment_id', 'target']\nFEAT_COLS = [f'f_{i}' for i in range(300)]","56dd1488":"# dtype_map = {\n#     'row_id': 'str',\n#     'time_id': 'uint16',\n#     'investment_id': 'uint16',\n#     'target': 'float32',\n# }\n# for col in FEAT_COLS:\n#     dtype_map[col] = 'float32'\n# df = pd.read_csv(os.path.join(DATA_PATH_RAW, 'train.csv'), dtype=dtype_map)\n# df.to_parquet(os.path.join(DATA_PATH_RAW, 'train_light.parquet'), index=False)","f7b96779":"df = pd.read_parquet(os.path.join(DATA_PATH_RAW_CUSTOM, 'train_light.parquet'))\ndf.head()","18391782":"df_test = pd.read_csv(os.path.join(DATA_PATH_RAW, 'example_test.csv'))\ndf_test.head()","e52acfdb":"df_sub = pd.read_csv(os.path.join(DATA_PATH_RAW, 'example_sample_submission.csv'))\ndf_sub.head()","f3218ab6":"del df_test, df_sub\ngc.collect()","e7da06e3":"print(f\"Shape of train.csv is {df.shape}\")","30c29052":"df.info()","e52b1891":"df.describe()","f8e209f0":"assert not df.isna().any().any()","9bff7107":"time_id_min = df['time_id'].min()\ntime_id_max = df['time_id'].max()\ntime_ids_range = [_ for _ in range(time_id_min, time_id_max)]\ntime_ids_skip = set(time_ids_range).difference(set(df['time_id'].unique()))\n\nprint(f\"There are {df['time_id'].nunique()} unique time_ids.\")\nprint(f\"The minimum time_id is {time_id_min}, \"\n      f\"and the maximum time_id is {time_id_max}.\")\nprint(f\"Skipped time_ids are {sorted(time_ids_skip)}\")","0908b8f8":"inv_id_min = df['investment_id'].min()\ninv_id_max = df['investment_id'].max()\ninv_ids_range = [_ for _ in range(inv_id_min, inv_id_max)]\ninv_ids_skip = set(inv_ids_range).difference(set(df['investment_id'].unique()))\n\nprint(f\"There are {df['investment_id'].nunique()} unique investment_ids.\")\nprint(f\"The minimum investment_id is {inv_id_min}, \"\n      f\"and the maximum investment_id is {inv_id_max}.\")\nprint(f\"Skipped investment_ids are {sorted(inv_ids_skip)} \"\n      f\"({len(inv_ids_skip)} in total)\")","5c8d1dea":"n_samples_time_id = df.groupby('time_id').agg('size')\nassert n_samples_time_id.equals(df.groupby('time_id')['investment_id'].nunique())\n\nfig, ax = plt.subplots(figsize=(14, 7))\nax.bar(n_samples_time_id.index, n_samples_time_id.values)\nax.set_title(\"#Samples in Each time_id\")\nax.set_xlabel(\"time_id\")\nax.set_ylabel(\"#Samples\")\nplt.show()","a51d8d68":"n_samples_max_t = n_samples_time_id.max()\nn_samples_max_time_id = n_samples_time_id.index[n_samples_time_id.argmax()]\nn_samples_min_t = n_samples_time_id.min()\nn_samples_min_time_id = n_samples_time_id.index[n_samples_time_id.argmin()]\n\nprint(f\"Maximum #samples in one time_id is {n_samples_max_t}, \"\n      f\"the corresponding time_id is {n_samples_max_time_id}.\")\nprint(f\"Minimum #samples in one time_id is {n_samples_min_t}, \"\n      f\"the corresponding time_id is {n_samples_min_time_id}.\")","3c2db12c":"n_samples_inv_id = df.groupby('investment_id').agg('size')\nassert n_samples_inv_id.equals(df.groupby('investment_id')['time_id'].nunique())\n\nfig, ax = plt.subplots(figsize=(14, 7))\nax.bar(n_samples_inv_id.index, n_samples_inv_id.values)\nax.set_title(\"#Samples of Each investment_id\")\nax.set_xlabel(\"investment_id\")\nax.set_ylabel(\"#Samples\")\nplt.show()","d26828ab":"n_samples_max_i = n_samples_inv_id.max()\nn_samples_max_inv_id = n_samples_inv_id.index[n_samples_inv_id.argmax()]\nn_samples_min_i = n_samples_inv_id.min()\nn_samples_min_inv_id = n_samples_inv_id.index[n_samples_inv_id.argmin()]\n\nprint(f\"Maximum #samples of one investment_id is {n_samples_max_i}, \"\n      f\"the corresponding investment_id is {n_samples_max_inv_id}.\")\nprint(f\"Minimum #samples of one investment_id is {n_samples_min_i}, \"\n      f\"the corresponding investment_id is {n_samples_min_inv_id}.\")","04be4655":"fig, ax = plt.subplots(figsize=(14, 7))\nax.hist(df['target'], bins=2000)\nax.set_title(\"Distribution of Predicting Target\")\nax.set_xlabel(\"Return Rate (Binned)\")\nax.set_ylabel(\"Value Count\")\nplt.show()","e536935b":"time_ids_rand = sample(list(df['time_id'].unique()), k=16)\n\nfig, axs = plt.subplots(nrows=4, ncols=4, figsize=(14, 14))\nfor i, time_id in enumerate(time_ids_rand):\n    target_ = df[df['time_id'] == time_id]['target']\n    mean, std = target_.mean(), target_.std()\n    axs[i\/\/4, i%4].hist(target_, bins=250)\n    axs[i\/\/4, i%4].set_title(f\"time_id={time_id}, mean={mean:.2f}, std={std:.2f}\")\n    axs[i\/\/4, i%4].set_xlabel(\"Return Rate (Binned)\")    \n    axs[i\/\/4, i%4].set_ylabel(\"Value Count\")\n    del target_\nplt.tight_layout()","f36a4a40":"inv_ids_rand = sample(list(df['investment_id'].unique()), k=16)\n\nfig, axs = plt.subplots(nrows=4, ncols=4, figsize=(14, 14))\nfor i, inv_id in enumerate(inv_ids_rand):\n    target_ = df[df['investment_id'] == inv_id]['target']\n    mean, std = target_.mean(), target_.std()\n    axs[i\/\/4, i%4].hist(target_, bins=250)\n    axs[i\/\/4, i%4].set_title(f\"inv_id={inv_id}, mean={mean:.2f}, std={std:.2f}\")\n    axs[i\/\/4, i%4].set_xlabel(\"Return Rate (Binned)\")    \n    axs[i\/\/4, i%4].set_ylabel(\"Value Count\")\n    del target_\nplt.tight_layout()","8f56cf92":"time_ids_complete = [t for t in range(df['time_id'].min(), df['time_id'].max())]\ninv_ids_rand = sample(list(df['investment_id'].unique()), k=3)\n\nfig = go.Figure()\nfor i, inv_id in tqdm(enumerate(inv_ids_rand)):\n    target_ = df[df['investment_id'] == inv_id].loc[:, ['time_id', 'target']]\n    target_.sort_values('time_id', inplace=True)\n    target_.set_index('time_id', drop=True, inplace=True)\n    target_ = target_.reindex(index=time_ids_complete)\n    \n    fig.add_trace(go.Scatter(x=time_ids_complete, y=target_['target'], \n                             mode='lines+markers', name=f'inv_{inv_id}'))\n    del target_\n    \nfig.update_layout(\n    title=\"Target Series of Different Investments\",\n    xaxis_title=\"time_id\",\n    yaxis_title=\"Return Rate\",\n    legend_title=\"investment_id\",\n)\nfig.show()","60f469ec":"target_map = df.pivot(index='investment_id', columns='time_id', values='target')\ntarget_map.head(2)","ce407a89":"inv_ids_rand = sample(list(target_map.index.unique()), 300)\n\nfig = go.Figure()\nfor i, inv_id in tqdm(enumerate(inv_ids_rand)):\n    target_vec = target_map[target_map.index == inv_id].values[0]\n    target_cumsum = np.nancumsum(target_vec)\n    \n    fig.add_trace(go.Scatter(x=target_map.columns, y=target_cumsum, \n                             mode='lines', name=f'inv_{inv_id}'))\n    del target_vec, target_cumsum\n    \nfig.update_layout(\n    title=\"Cumulative Return Rate of Different Investments\",\n    xaxis_title=\"time_id\",\n    yaxis_title=\"Cumulative Return Rate\",\n    legend_title=\"investment_id\",\n)\nfig.show()","96801103":"corrs_inv = target_map.T.corr()   # Derive corr of return rates of different invs\ncorrs_inv = abs(corrs_inv[corrs_inv != 1])   # Take off-diagonal corrs\ninv_ids_leg = list(n_samples_inv_id[n_samples_inv_id > 600].index)\ncorrs_inv = corrs_inv.loc[inv_ids_leg, inv_ids_leg]\n\ncorrs_max = {'inv_id1': [], 'inv_id2': [], 'corr': []}\nfor inv_id, corr_vec in corrs_inv.iterrows():\n    corrs_max['inv_id1'].append(inv_id)\n    corrs_max['inv_id2'].append(corr_vec.index[corr_vec.argmax()])\n    corrs_max['corr'].append(corr_vec.max())\ncorrs_max = pd.DataFrame.from_dict(corrs_max, orient='columns')\ncorrs_max.sort_values('corr', ascending=False, inplace=True)\ncorrs_max.head()","4f8110ca":"inv_ids_top = [194, 1144, 1121, 1929, 2406, 2669]\ntarget_map_ = target_map.loc[inv_ids_top, :]\nsns.pairplot(target_map_.T, corner=True)","9a78ec2b":"fig = go.Figure()\nfor i, inv_id in tqdm(enumerate(inv_ids_top)):\n    target_vec = target_map[target_map.index == inv_id].values[0]\n    target_cumsum = np.nancumsum(target_vec)\n    \n    fig.add_trace(go.Scatter(x=target_map.columns, y=target_cumsum, \n                             mode='lines', name=f'inv_{inv_id}'))\n    del target_vec, target_cumsum\n    \nfig.update_layout(\n    title=\"Cumulative Return Rate of Highly Correlated Investments\",\n    xaxis_title=\"time_id\",\n    yaxis_title=\"Cumulative Return Rate\",\n    legend_title=\"investment_id\",\n)\nfig.show()","d148779e":"feat_nums_rand = sample(range(0, 300), k=16)\n\nfig, axs = plt.subplots(nrows=4, ncols=4, figsize=(14, 14))\nfor i, feat_num in enumerate(feat_nums_rand):\n    feat = df[f'f_{feat_num}']\n    mean, std = feat.mean(), feat.std()\n    axs[i\/\/4, i%4].hist(feat, bins=250)\n    axs[i\/\/4, i%4].set_title(f\"f_{feat_num}, mean={mean:.2f}, std={std:.2f}\")\n    axs[i\/\/4, i%4].set_xlabel(\"Feature Value (Binned)\")    \n    axs[i\/\/4, i%4].set_ylabel(\"Value Count\")\n    del feat\nplt.tight_layout()","ba8f7a8c":"fig, axs = plt.subplots(nrows=60, ncols=5, figsize=(20, 240))\nfor i, feat_num in tqdm(enumerate(FEAT_COLS)):\n    feat = df[f'{feat_num}']\n    mean, std = feat.mean(), feat.std()\n    sns.boxplot(x=feat, color='green', saturation=0.4,\n                width=0.3, whis=3, ax=axs[i\/\/5, i%5])\n    axs[i\/\/5, i%5].axvline(mean, color= 'orange', linestyle='dotted',\n                           linewidth=2)\n    axs[i\/\/5, i%5].set_title(f\"{feat_num}, mean={mean:.2f}, std={std:.2f}\")\n    axs[i\/\/5, i%5].set_xlabel(\"Feature Value\")    \n    del feat\nplt.tight_layout()","47ea330f":"feats_low_var = [124, 170, 175, 182, 200, 272]\n\nfig, axs = plt.subplots(nrows=2, ncols=3, figsize=(14, 7))\nfor i, feat_num in enumerate(feats_low_var):\n    feat = df[f'f_{feat_num}']\n    mean, std = feat.mean(), feat.std()\n    axs[i\/\/3, i%3].hist(feat, bins=250)\n    axs[i\/\/3, i%3].set_title(f\"f_{feat_num}, mean={mean:.2f}, std={std:.2f}\")\n    axs[i\/\/3, i%3].set_xlabel(\"Feature Value (Binned)\")    \n    axs[i\/\/3, i%3].set_ylabel(\"Value Count\")\n    del feat\nplt.tight_layout()","08db3ba3":"fig, axs = plt.subplots(nrows=122, ncols=10, figsize=(40, 480))\nfeat_cols = [f'f_{i}' for i in feats_low_var]\n\nfor i, t in tqdm(enumerate(df['time_id'].unique())):\n    df_ = df[df['time_id'] == t]\n    for col in feat_cols:\n        axs[i\/\/10, i%10].plot(df_[col], label=col)\n    axs[i\/\/10, i%10].axes.xaxis.set_visible(False)\n    axs[i\/\/10, i%10].set_title(f\"time_id={t}, #inv={len(df_)}\")\n    if i == 0:\n        axs[i\/\/10, i%10].legend()\nfig.show()","79d9bd6d":"bins = [0, 50, 114, 306, 367, 586, 650, 665, 729, 818,\n        926, 956, 1013, 1051, 1141, 1145, 1209, 1220]\ndf['time_block'] = pd.cut(df['time_id'], bins=bins, right=True, \n                          include_lowest=True)\n\nfig, axs = plt.subplots(nrows=60, ncols=5, figsize=(24, 240))\nfor i, feat in tqdm(enumerate(FEAT_COLS)):\n    feat_std = df.groupby('time_block')[feat].std()\n    if i == 0:\n        xtick_lbs = feat_std.index\n        xticks = [_ for _ in range(len(xtick_lbs))]\n    axs[i\/\/5, i%5].plot(feat_std)\n    axs[i\/\/5, i%5].set_title(f\"Time-Grouped Std of {feat}\")\n    axs[i\/\/5, i%5].set_ylabel(\"Feature Std\")\n    axs[i\/\/5, i%5].set_xticks(xticks, xtick_lbs, rotation=90)\n    del feat_std\nplt.tight_layout()","5caaa7b8":"fig, ax = plt.subplots(figsize=(14, 7))\nfor i in [0, 1, 2, 5, 6, 8, 9, 296, 297, 298]:\n    feat_stds = df.groupby('time_block')[f'f_{i}'].std()\n    ax.plot(feat_stds, label=f'f_{i}')\n    \nax.set_title(\"Synchronous Time-Grouped Standard Deviation\")\nax.set_xlabel(\"time_id Interval\")\nax.set_ylabel(\"Standard Deviation\")\nax.legend()\nax.set_xticks(xticks, xtick_lbs, rotation=90)\nplt.show()","590aa919":"df_sub = df.sample(frac=0.2)\ncols = FEAT_COLS + ['target']","79158c42":"def plot_heatmap(df, depend):\n    '''Plot the heatmap based on correlation or mutual information.\n    \n    Parameters:\n        df: pd.DataFrame, measure of mutual dependence betweeb each \n            featue pair\n        depend: str, name of dependence, the choices are as follows:\n            {'Corr', 'NMI'}\n            \n    Return:\n        None\n    '''\n    fig = go.Figure()\n    fig.add_trace(go.Heatmap(x=df.index, \n                             y=df.columns, \n                             z=df.T, \n                             colorbar=dict(title=depend), \n                             hoverinfo='text',\n                             text=get_hover_text(df)))\n    fig.update_layout(\n        title=f\"{depend} of Feature Pairs\",\n        xaxis_title=\"Feature1 (Including target)\",\n        yaxis_title=\"Feature2 (Including target)\",\n        width=800, height=800\n    )\n    fig_widget = go.FigureWidget(fig)\n    fig_widget.show()\n\ndef get_hover_text(df):\n    '''Return text list storing hover information for interactive \n    heatmap.\n    \n    Parameters:\n        df: pd.DataFrame, measure of mutual dependence betweeb each \n            featue pair\n        \n    Return:\n        hover_txt: list, hover information displayed when hovering \n                   grids in the heatmap\n    '''\n    hover_txt = []\n    for yi, feat_y in enumerate(df.columns):\n        hover_txt.append([])\n        for xi, feat_x in enumerate(df.index):\n            hover_txt[-1].append(f\"Feature1: {feat_x}<br \/>Feature2: {feat_y} \"\n                                 f\"<br \/>Score: {df[feat_y][feat_x]}\")\n    return hover_txt","12d031db":"corrs = df_sub[cols].corr()\nplot_heatmap(corrs, depend='Corr')","ceae46e2":"corrs_with_target = corrs.loc[:, 'target']\ncorrs_with_target = corrs_with_target[corrs_with_target.index != 'target']\ncorrs_with_target = abs(corrs_with_target)\n\nfig, ax = plt.subplots(figsize=(14, 7))\nax.hist(corrs_with_target, bins=25)\nax.set_title(\"Absolute Correlation of Anonymized Features and Target\")\nax.set_xlabel(\"Absolute Correlation (Binned)\")\nax.set_ylabel(\"Value Count\")\nplt.show()\nprint(f\"Maximum absolute correlation with target is {corrs_with_target.max()}, \"\n      f\"and the corresponding feature is {corrs_with_target.argmax()}\")","89c5f7fe":"df_sub['time_block'] = pd.cut(df['time_id'], bins=25)\ntime_gps = df_sub.groupby('time_block')\n\nfig, axs = plt.subplots(nrows=5, ncols=5, figsize=(16, 16))\nfor i, (t, gp) in tqdm(enumerate(time_gps)):\n    corrs_gp = gp[cols].corr().loc[:, 'target']\n    corrs_gp = corrs_gp[corrs_gp.index != 'target']\n    corrs_gp = abs(corrs_gp)\n\n    axs[i\/\/5, i%5].hist(corrs_gp, bins=25)\n    axs[i\/\/5, i%5].set_title(f\"Time Block {t}\")\n    axs[i\/\/5, i%5].set_xlabel(\"Absolute Correlation (Binned)\")\n    axs[i\/\/5, i%5].set_ylabel(\"Value Count\")\n    del corrs_gp\nplt.tight_layout()","6ff31cc2":"fig, axs = plt.subplots(nrows=5, ncols=5, figsize=(16, 16))\nfor i, time_id in tqdm(enumerate(range(25))):\n    df_time_id = df[df['time_id'] == time_id]\n    corrs_time_id = df_time_id[cols].corr().loc[:, 'target']\n    corrs_time_id = corrs_time_id[corrs_time_id.index != 'target']\n    corrs_time_id = abs(corrs_time_id)\n\n    axs[i\/\/5, i%5].hist(corrs_time_id, bins=25)\n    axs[i\/\/5, i%5].set_title(f\"time_id={time_id}\")\n    axs[i\/\/5, i%5].set_xlabel(\"Absolute Correlation (Binned)\")\n    axs[i\/\/5, i%5].set_ylabel(\"Value Count\")\n    del df_time_id, corrs_time_id\nplt.tight_layout()","64e0050e":"inv_ids = sorted(df['investment_id'].unique())[:25]\n\nfig, axs = plt.subplots(nrows=5, ncols=5, figsize=(16, 16))\nfor i, inv_id in tqdm(enumerate(inv_ids)):\n    df_inv_id = df[df['investment_id'] == inv_id]\n    corrs_inv_id = df_inv_id[cols].corr().loc[:, 'target']\n    corrs_inv_id = corrs_inv_id[corrs_inv_id.index != 'target']\n    corrs_inv_id = abs(corrs_inv_id)\n\n    axs[i\/\/5, i%5].hist(corrs_inv_id, bins=25)\n    axs[i\/\/5, i%5].set_title(f\"inv_id={inv_id}\")\n    axs[i\/\/5, i%5].set_xlabel(\"Absolute Correlation (Binned)\")\n    axs[i\/\/5, i%5].set_ylabel(\"Value Count\")\n    del df_inv_id, corrs_inv_id\nplt.tight_layout()","060872e0":"corrs_t1205 = df[df['time_id'] == 1205][cols].corr()\nplot_heatmap(corrs_t1205, 'Corr (time_id=1205)')","ac2ba253":"corrs_t483 = df[df['time_id'] == 483][cols].corr()\nplot_heatmap(corrs_t483, 'Corr (time_id=483)')","f881a48f":"zero_ratios = {}\nn_samples = df.shape[0]\n\nfor col in cols:\n    zero_ratios[col] = (df[col] == 0).sum() \/ n_samples\nzero_ratios = pd.DataFrame.from_dict(zero_ratios, \n                                     orient='index', \n                                     columns=['zero_ratio'])\nzero_ratios.sort_values('zero_ratio', ascending=False, inplace=True)\nzero_ratios.T","98bd3286":"nmis = {}\nfor f in tqdm(FEAT_COLS):\n    nmis[f] = normalized_mutual_info_score(df_sub['target'], df_sub[f])\nnmis = pd.Series(nmis)\n\nfig, ax = plt.subplots(figsize=(14, 7))\nax.hist(nmis, bins=25)\nax.set_title(\"NMI between Anonymized Features and Target\")\nax.set_xlabel(\"Normalized Mutual Information (Binned)\")\nax.set_ylabel(\"Value Count\")\nplt.show()\nprint(f\"Maximum NMI with target is {nmis.max()}, \"\n      f\"and the corresponding feature is {nmis.argmax()}\")","d6dc74f5":"<a id=\"low_var_feat\"><\/a>\n### *Low-Variance Feature*\nObserving boxplots of features illustrated above, there are some features with **relatively low variance** compared with others. `f_124` is the one with the lowest variance and the highest zero ratio (about $47.54\\%$).\n#### Histogram","1e35544f":"<a id=\"return_rate_map\"><\/a>\n### *2D Predicting Target Map*\nReturn rate series can be converted to 2D target map using `time_id` and `investment_id` as two axes. Because not all investments have data in all `time_id`s, I simply leave `NaN`s for entris with no data recorded.","82122b32":"#### Correlation of Feature Pairs\nThe correlation is calculated using randomly selected subset of samples to reduce computational cost. From interactive heatmap, we can find that there are some **highly correlated** feature pairs (*e.g.*, (`f_148`, `f_205`)). ","abc7e26f":"<a id=\"primary_key\"><\/a>\n## ii. Primary Key\nThe primary key of this DataFrame is (`time_id`, `investment_id`) pair, which is presented as `row_id`. Because there's a supplementary statement related to `time_id` and `investment_id` in [data page](https:\/\/www.kaggle.com\/c\/ubiquant-market-prediction\/data), I decide to explore the primary key.\n\n<a id=\"time_id\"><\/a>\n### *`time_id`*\nIn addition to the inconsistency of time gaps between `time_id`s, there are also some skipped `time_id`s.","924cd867":"#### Clustering Property\nBased on the analysis of **low-variance** features, I try to find out if there's any **time-dependent property**. Through checking feature values of different investments in **every `time_id`**. It's evident that there's **clustering effect along time axis**. Though it's hard to conclude what's going on behind the scene, I think this may be some characteristic similar to [volatility clustering](https:\/\/en.wikipedia.org\/wiki\/Volatility_clustering).","7595ce8c":"#### Correlations with Predicting Target at `time_id` 483 and 1205\nBased on the analysis in this [section](#n_samples_time_id), `time_id` 1205 has the **most** number of investments and 483 the **least**. Comparing two heatmaps, we can easily observe the difference of correlations between two `time_id`s. Also, the occurrence of `NaN`s in the second heatmap inspires me to analyze **zero ratio** of each feature below. Finally, we can find out that a feature with higher **zero ratio** might have a higher chance to be a **zero vector** in some `time_id` like 483, which might contain valuable information to utilize.","e1263536":"#### Randomly Sampled `time_id`s\nThe **dispersions** of return rate distributions in different `time_id`s show diversity.","a3ab68dd":"#### Time-Grouped Dispersion \n**Dynamic dispersion** (measured by std) of feature values along time axis seems have some unknown **periodic fluctuation**. However, we can't get the real time gap between `time_id`s. Also, some of the features have **synchronous movement** of dispersion and following is just a manually selected example.","fa002b5c":"#### Descriptive Statistics\nObserving stats of DataFrame, we can find that most of the features have mean value close to **zero** and std close to **one**.","100471ae":"<a id=\"return_rate_series\"><\/a>\n### *Predicting Target Series*\nWith sequence nature shown by `time_id`s, I explore return rate from the perspective of **time-series**. We can see that some of the investments have **greater return rate fluctuation** than others, so **temporal pattern extraction** might be important. In addition, some **synchronous movement** in return rate series of different investments show the potential of modeling the **spatial dependency** among differnt investments.","f7b28539":"<a id=\"based_description_of_train\"><\/a>\n### *Basic Description of `train.csv`*\n#### Data Shape\nDetailed description of columns is as follows (, which can be found in [data page](https:\/\/www.kaggle.com\/c\/ubiquant-market-prediction\/data)):\n* `row_id` - A unique identifier for the row.\n* `time_id` - The ID code for the time the data was gathered. The time IDs are in order, but the real time between the time IDs is not constant and will likely be shorter for the final private test set than in the training set.\n* `investment_id` - The ID code for an investment. Not all investment have data in all time IDs.\n* `target` - The target.\n* `[f_0:f_299]` - Anonymized features generated from market data.","939e6516":"<a id=\"data_appearance\"><\/a>\n## i. Data Appearance\nIn this section, basic data appearance is introduced, helping readers understand how the data is structured and what information the data contains.\n\n<a id=\"large_dataset\"><\/a>\n### *Large Dataset*\nThe size of raw data is large (about 18.5GB); hence, there's a need to convert and dump the data for better storage and IO efficiency.","a1f0b52c":"#### Spatial Dependency among Investments\nTo observe whether there's any **spatial dependency** among investments, I derive correlations of return rate series of different `investment_id`s. Also, I only select those `investment_id`s with sufficient `time_id`s (*i.e.*, with threshold of 600 `time_id`s). Finally, some of highly correlated investments are found!","57bf0936":"<a id=\"target_exploration\"><\/a>\n## iii. Target Exploration\nThe predicting target of this competition is the **return rate** of investments, which is relevant for making trading decisions.\n\n<a id=\"return_rate_dist\"><\/a>\n### *Predicting Target Distribution*\n#### Complete Target Series\nThe predicting target seems to follow **normal distribution**, and there's no apparent outliers or tail existing. Hence, data conversion isn't necessary in this case.","bb375f52":"#### Randomly Sampled `investment_id`s\nThe **dispersions** of return rate distributions of different `investment_id`s show significant diversity. The reason behind the scene may be that each investment has its own characteristic (*e.g.*, larger trading volume, greater price fluctuation). Also, we should again notice that not all investments have data in all `time_id`s.","3a3a7462":"<a id=\"rel_btw_time_id_inv_id\"><\/a>\n### *Relationship between `time_id` and `investment_id`*\nAfter basic introduction of `time_id` and `investment_id`, following is the exploration of relationship between them.\n\n<a id=\"n_samples_time_id\"><\/a>\n#### Number of Samples in Each `time_id`\nBecause not all investments have data in all `time_id`s, number of samples (*i.e.*, number of investments) in each `time_id` is different from others. ","bfe7ff34":"#### Boxplot\nI use **3IQR whisker** to observe if there's any **extreme outliers** existing, instead of the default **1.5IQR whisker**. Also, **mean value** is marked to help observe if feature values are **strongly centered**. Then, it's obvious that most of features have **extreme outliers**. Moreover, some features have a single data point far away from others (*e.g.*, the minimum of `f_12`, the maximum of `f_104`). Some of features with relatively **low variance** (*e.g.*, `f_124`, `f_170`) are also observed. Finally, all these observations motivate me to dig deeper into [**outlier analysis**](#outliers) below.","63b49033":"<a id=\"primary_key\"><\/a>\n## ii. Primary Key\nThe primary key of this DataFrame is (`time_id`, `investment_id`) pair, which is presented as `row_id`. Because there's a supplementary statement related to `time_id` and `investment_id` in [data page](https:\/\/www.kaggle.com\/c\/ubiquant-market-prediction\/data), I decide to explore the primary key.\n\n<a id=\"time_id\"><\/a>\n### *`time_id`*\nIn addition to the inconsistency of time gaps between `time_id`s, there are also some skipped `time_id`s.","49076365":"#### `example_sample_submission.csv`\n`example_sample_submission.csv` is an example submission file provided so the publicly accessible copy of the API provides the correct data shape and format. And, we can see that there's no `investment_id` existing in the submission file.","ccbadb40":"#### Number of Samples of Each `investment_id`\nFrom the perspective of `investment_id`, we can again verify the fact that not all investments have data in all `time_id`s. Investment with `investment_id` 1415 only has data recorded in **2** `time_id`s (*i.e.*, 1216 and 1217), which will make it hard to capture the **temporal pattern**.","d89b9449":"<a id=\"outliers\"><\/a>\n### *Outliers*","fbf6767a":"#### Cumulative Return Rate\nCumulative return rates of randomly selected investments show siginificant diversity. Could I come to a conclusion that the return rate follows a **random walk**?","de1cfb57":"<a id=\"corrs_time_id\"><\/a>\n#### Correlations with Predicting Target in First 25 `time_id`s\nWith fine-grained time scale (*i.e.*, `time_id`), correlations of features with return rate become more diversified among different `time_id`s, showing that anonymized features **might** have dynamic effect on return rate in different time intervals.","4c582efe":"#### `example_test.csv`\n`example_test.csv` is the random data provided to demonstrate what shape and format of data the API will deliver to your notebook when you submit. That is, the file gives us a sense about what raw input data will be fed into the well-trained model in submission stage.","15e50332":"#### Concise Summary\nAfter conversion, raw data becomes more memory-efficient.","f94e54ed":"# Ubiquant Market Prediction - A Simple EDA\n\n## About this Competition \nIn this competition, the competitors are asked to build a model forecasting an investment's **return rate**, helping improve the ability of **quantitative** reseachers to make better predictions and decisions.\n> [Ubiquant](https:\/\/www.ubiquant.com\/website\/home)\uff1a\u300c\u91cf\u5316\u4ea4\u6613\u662f\u505a2\u500b$\\sigma$\u4ee5\u5167\u6700\u6b63\u78ba\u7684\u4e8b\uff0c\u540c\u6642\u63a7\u52362\u500b$\\sigma$\u4ee5\u5916\u7684\u98a8\u96aa\u3002\u4f46\u8981\u5f9e\u4e8b\u91cf\u5316\uff0c\u8981\u505a\u52302\u500b$\\sigma$\u4ee5\u5916\u7684\u512a\u79c0\u3002 \u300d\n\n## About this Notebook\nIn this kernel, I'll briefly illustrate what the dataset looks like. Also, some visualization will be implemented to facilitate the preliminary understanding of the data.\n\n## Acknowledgements\n* Data type conversion of raw DataFrame - [Speed Up Reading (csv-to-pickle)](https:\/\/www.kaggle.com\/columbia2131\/speed-up-reading-csv-to-pickle)\n* File IO using `.parquet` extension - [Fast Data Loading and Low Mem with Parquet Files](https:\/\/www.kaggle.com\/robikscube\/fast-data-loading-and-low-mem-with-parquet-files)\n\n## Table of Contents\n* [i. Data Appearance](#data_appearance)\n    * *[Large Dataset](#large_dataset)*\n    * *[Data File Description](#data_file_description)*\n    * *[Basic Description of `train.csv`](#based_description_of_train)*\n* [ii. Primary Key](#primary_key)\n    * *[`time_id`](#time_id)*\n    * *[`investment_id`](#inv_id)*\n    * *[Relationship between `time_id` and `investment_id`](#rel_btw_time_id_inv_id)*\n* [iii. Target Exploration](#target_exploration)\n    * *[Predicting Target Distribution](#return_rate_dist)*\n    * *[Predicting Target Series](#return_rate_series)*\n    * *[2D Predicting Target Map](#return_rate_map)*\n* [iv. Feature Exploration](#feature_exploration)\n    * *[Single Feature Distribution](#single_feat_dist)*\n    * *[Low-Variance Feature](#low_var_feat)*\n    * *[Outliers](#outliers)*\n    * *[Feature Interaction](#feat_interaction)*","faa681d0":"#### Correlation with Predicting Target\nAll anonymized features have low correlations with the predicting target. The maximum absolute correlation is approximately **0.06**.","2facd78a":"#### Mutual Information with Predicting Target\n[Mutual information](https:\/\/en.wikipedia.org\/wiki\/Mutual_information) can measure the **mutual dependence** of two variables, which isn't limited to **linear dependence** like **correlation**. We can see that most of the anonymized features have high NMI with return rate. But, I still have no idea about what's going on and I'll try to figure it out!\n<div class=\"alert alert-block alert-danger\">\n    <p>Another thing to mention is that the computational cost of calculating NMI is relatively high, so I don't implement <strong>pairwise<\/strong> NMI of all feature pairs.<\/p>\n<\/div>","aef42f81":"<a id=\"feat_interaction\"><\/a>\n### *Feature Interaction*\nAside from single feature distributions, feature dependency is illustrated to show how each feature interacts with other features and predicting target.","f36bd36e":"#### Correlations with Predicting Target of First 25 `investment_id`s\nIn addition to `time_id`, anonymized features of different investments also have different correlations with return rate, again showing that each investment has its own characteristic.","68eb70ee":"#### Correlations with Predicting Target in Different Time Blocks\nCorrelations of anonymized features with return rate are different in **discretized time blocks**, which motivates me to explore correlations in fine-grained time scale [below](#corrs_time_id).","0d59dd49":"<div class=\"alert alert-blocks alert-info\" style=\"text-align: center\">\n    <h3>Work in Progress...<\/h3>\n    <h3>Thanks for Your Attention!<\/h3>\n<\/div>","d45c6640":"#### Missing Values\nThere's no missing values in raw DataFrame.","ef02b24a":"<a id=\"feature_exploration\"><\/a>\n## iv. Feature Exploration\nAnonymized features can provide rich information boosting the quality of model learning. Though the actual meanings of features are unknown, it's still worth exploring underlying property and hidden pattern of them.\n\n<a id=\"single_feat_dist\"><\/a>\n### *Single Feature Distribution*\n#### Histogram\nThough their are some similar properties (*e.g.*, zero-mean) among randomly selected features, each feature still has its own characteristic (*e.g.*, [skewness](https:\/\/en.wikipedia.org\/wiki\/Skewness), [multimodal](https:\/\/en.wikipedia.org\/wiki\/Multimodal_distribution)).","66af5f1e":"<a id=\"data_file_description\"><\/a>\n### *Data File Description* \n#### `train.csv`\n`train.csv` contains information to build a model, including anonymized features derived from real market data, groundtruth (*i.e.*, obfuscated metric to predict) and corresponding primary key (*i.e.*, (`time_id`, `investment_id`) pair). Due to the large data size, `train.csv` has been converted and dumped in `.parquet` extension using code snippet above."}}