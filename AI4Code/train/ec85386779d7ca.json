{"cell_type":{"58723152":"code","6ae86bd9":"code","b6e1a607":"code","c08e0f5a":"code","05a0e96f":"code","e91ed9f6":"code","14e5ba0d":"code","e7ec7aa6":"code","a1c03cef":"code","df94dc49":"code","11a273c5":"code","20eb5a52":"code","8d523ead":"code","95587e4b":"code","a98db9c3":"code","12678a20":"code","39088eef":"code","11786f53":"code","60dba3d5":"code","a44e1112":"code","458d8134":"code","8564ccc0":"code","fb2a594e":"code","268f7715":"code","cad8d1e1":"code","94d53753":"code","da65d6fe":"code","0d017d3d":"code","4f76a5fa":"code","954c90fb":"code","74b72a12":"code","a31c79e9":"code","a5b75e49":"markdown","a45522df":"markdown","74a496b7":"markdown","737cc5bc":"markdown","4cb6e8a0":"markdown","8a5683a4":"markdown","a683ac4b":"markdown","e9c780d6":"markdown","1711697d":"markdown","900682c9":"markdown","59913834":"markdown","bb27765f":"markdown","d4eac17b":"markdown","0f8cc6c1":"markdown"},"source":{"58723152":"# let`s first import all the necessary libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns","6ae86bd9":"# let`s now load the titanic dataset\ndata=pd.read_csv(\"..\/input\/titanicdataset-traincsv\/train.csv\")","b6e1a607":"data.head() # look at the starting five rows of the dataset","c08e0f5a":"data.shape # rows and columns in the dataset","05a0e96f":"data.describe() #gives the statistical summary of the dataset","e91ed9f6":"data.info()","14e5ba0d":"data.isnull().sum() #look for the missing value within the dataset","e7ec7aa6":"# As the \"age,cabin and Embarked column have missing values\" so let`s fill the missing values","a1c03cef":"#let`s look at the cabin column\ndata[\"Cabin\"].isnull().sum()","df94dc49":"# so we have 687 missing values in cabin column so let`s drop this column\ndata.drop([\"Cabin\"],axis=1,inplace=True)","11a273c5":"data","20eb5a52":"# let`s now look at the `Age` column\ndata[\"Age\"].isnull().sum()","8d523ead":"# In Age column we have 177 missing value so let`s fill these values with mean or median , \n# I will use mean to fill it\nAge_mean=data[\"Age\"].mean()\ndata[\"Age\"]=data[\"Age\"].fillna(Age_mean)","95587e4b":"# Now let`s have look at Age column\ndata[\"Age\"].isnull().sum()","a98db9c3":"data=data.dropna() # Embarked has only 2 missing values so let`s drop the rows ","12678a20":"data.isnull().sum()","39088eef":"data.shape","11786f53":"# By looking at the data we can analyse that \"PassengerId,Name,SibSp,Parch,Ticket ,Fare,Embarked\" \n# can not effect on the survival of passenger\nfeatures=data[[\"Pclass\",\"Sex\",\"Age\",\"Fare\"]]\ntarget=data[\"Survived\"]","60dba3d5":"# As \"Sex\" is categorical data so let`s use LabelEncoder to covert it into Numerical data\nfrom sklearn.preprocessing import LabelEncoder","a44e1112":"le=LabelEncoder() #let`s create object of LabelEncoder\nfeatures[\"Sex_n\"]=le.fit_transform(features[\"Sex\"])","458d8134":"features\n# Sex column is converted into numerical column","8564ccc0":"# Now let`s drop the original \"Sex\" column\nfeatures.drop([\"Sex\"],axis=1,inplace=True)","fb2a594e":"features","268f7715":"# before building the model let`s split the dataset using train_test_split\nfrom sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test=train_test_split(features,target,test_size=0.2)","cad8d1e1":"len(x_train)","94d53753":"len(x_test)","da65d6fe":"from sklearn.linear_model import LogisticRegression\nmodel = LogisticRegression()\nmodel.fit(x_train,y_train) #fit method is used to train the model so here we are training our model\nmodel.score(x_test,y_test)  # score method will take \"x_test\" and will calculate survival rate\n# and will compare it with y_test and will give the accuracy of the model","0d017d3d":"# let`s now use decision tree algorithm for prediction\nfrom sklearn import tree\nmodel=tree.DecisionTreeClassifier()\nmodel.fit(x_train,y_train)\nmodel.score(x_test,y_test)","4f76a5fa":"from sklearn.ensemble import RandomForestClassifier\nmodel=RandomForestClassifier()\nmodel.fit(x_train,y_train)\nmodel.score(x_test,y_test)","954c90fb":"from sklearn.ensemble import RandomForestClassifier\nmodel=RandomForestClassifier(n_estimators=50,max_depth=10,random_state=42)\nmodel.fit(x_train,y_train)\nprint(\"The accuracy of model is: \",model.score(x_test,y_test)*100)","74b72a12":"y_predicted= model.predict(x_test)\ny_predicted","a31c79e9":"from sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_test,y_predicted)\nsns.heatmap(cm,annot=True)","a5b75e49":"## Logistic Regression","a45522df":"## Random Forest","74a496b7":"Lets now predict the survival of the pessengers using our Random Forest model.","737cc5bc":"1. Fill the missing values\n2. Convert the data type of age (float64) to int64","4cb6e8a0":"# Confusion Matric","8a5683a4":"# MODEL BUILDING","a683ac4b":"## Decision Tree","e9c780d6":"# Import Necessary Libraries","1711697d":"# Load and Read the dataset","900682c9":"So we can see that after fine tuning our accuracy is increased \nNow our accuracy is 86% which is pretty good. yayyyy :D","59913834":"# Fine Tuning","bb27765f":"# Exploratory Data Analysis","d4eac17b":"From the above models we can observe that without fine tuning any of the model, the Random Forest give us the best accuracy so lets fine tune the hyper parameters of the Random Forest.","0f8cc6c1":"# Data Preprocessing"}}