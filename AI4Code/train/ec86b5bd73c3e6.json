{"cell_type":{"2328bc13":"code","9a792f10":"code","2663c720":"code","a360275d":"code","ea116e4e":"code","eb1ea6d9":"code","a0d22244":"code","d5f0a6dc":"code","8b83a2d8":"code","96d44516":"code","988e6763":"code","3ee28626":"code","f5ea9e60":"code","8a6cd459":"code","4e6d8de6":"code","75dfef09":"code","98e70e30":"code","d964f77c":"code","a0c80e84":"code","1d711761":"code","fb51c370":"code","8393652f":"code","aaad34de":"code","32fd171e":"code","0d0c7db4":"code","f1c993d6":"code","1f6d4cdb":"code","5247ebe4":"code","0066241f":"code","2f52deba":"code","141a931f":"code","c0946ac5":"code","2df8ef95":"code","83bdb6e8":"code","f6473620":"code","78c5af97":"code","78ddb8d8":"code","18ad45ee":"code","3cfe9791":"code","da5dfa42":"code","4181e5de":"code","1da9461c":"code","b765c315":"code","26882a15":"code","ac4a0d47":"code","b0a4142c":"code","ef0e2675":"markdown","949db81a":"markdown","78c02e1f":"markdown","74fa5fcb":"markdown","719a9ac3":"markdown"},"source":{"2328bc13":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport torch","9a792f10":"import os\nos.listdir(\"..\/input\/virtual-hack\/car_data\/car_data\")","2663c720":"train_folder=\"..\/input\/virtual-hack\/car_data\/car_data\/train\/\"\ntest_folder =\"..\/input\/virtual-hack\/car_data\/car_data\/test\/\"","a360275d":"import os\nlen(os.listdir(train_folder))","ea116e4e":"traindata_per_class = list()\nfolders=list()\nfor folder in os.listdir(train_folder):\n  folders.append(folder)\n  length=len(os.listdir(train_folder+folder))\n  print(folder,length)\n  traindata_per_class.append(length)","eb1ea6d9":"plt.bar(folders,traindata_per_class)","a0d22244":"cat_df=pd.DataFrame({\"car_category\":folders,\"number\":traindata_per_class}).sort_values(\"car_category\")","d5f0a6dc":"cat_df[:50].set_index(\"car_category\")['number'].plot.bar(color=\"r\",figsize=(20,6))\n# the data is pretty evenly distributed","8b83a2d8":"folders","96d44516":"from torchvision import transforms\nimage_transforms={\n    \"train\":\n     transforms.Compose([transforms.RandomResizedCrop(size=256, scale=(0.8, 1.0)),\n        transforms.RandomRotation(degrees=15),\n        transforms.ColorJitter(),\n        transforms.RandomHorizontalFlip(),\n        transforms.CenterCrop(size=224),  # Image net standards\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406],\n                             [0.229, 0.224, 0.225])    \n        ]),\n     'test':\n    transforms.Compose([\n        transforms.Resize(size=256),\n        transforms.CenterCrop(size=224),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n    ])\n    \n}","988e6763":"# from torch.utils.data.sampler import SubsetRandomSampler\n\n# database_size = 192\n# batch_size=32\n# validation_pct=0.2\n# indices = list(range(database_size))\n# split = int(np.floor(validation_pct * dataset_size))\n# np.random.shuffle(indices)\n\n# train_indices,validation_indices = indices[split:],indices[:split]\n# train_sampler = SubsetRandomSampler(train_indices)\n# validation_sampler = SubsetRandomSampler(validation_indices)\n\n# train_loader = torch.utils.data.DataLoader()","3ee28626":"from torchvision import datasets\npath=\"..\/input\/virtual-hack\/car_data\/car_data\/\"\nimage_datasets={x:datasets.ImageFolder(os.path.join(path,x),image_transforms[x]) for x in ['train','test']}","f5ea9e60":"dataset_size=len(image_datasets['train'])\ntrain_size = int(0.8 * dataset_size)\nval_size = dataset_size - train_size\ntrain_dataset, val_dataset = torch.utils.data.random_split(image_datasets['train'], [train_size, val_size])","8a6cd459":"train_dataset.dataset","4e6d8de6":"val_dataset.dataset","75dfef09":"from torch.utils.data import DataLoader\ndataloader={\n    \"train\":DataLoader(train_dataset,batch_size=16,shuffle=True),\n    \"val\":DataLoader(val_dataset,batch_size=16,shuffle=True),\n    \"test\":DataLoader(image_datasets['test'],batch_size=1,shuffle=True)\n}","98e70e30":"trainiter = iter(dataloader['val'])\nfeatures,labels=next(trainiter)\nfeatures.shape,labels.shape","d964f77c":"len(trainiter.dataset)","a0c80e84":"from torchvision import models\nmodel = models.resnet34(pretrained=True)","1d711761":"for param in model.parameters():\n  param.requires_grad=False","fb51c370":"import torch.nn as nn\nmodel.fc= nn.Sequential(nn.Linear(512,256),\n                        nn.ReLU(),\n                        nn.Linear(256,196),\n                        nn.LogSoftmax(dim=1))","8393652f":"model = model.to(\"cuda\")\nmodel = nn.DataParallel(model)","aaad34de":"from torch import optim\ncriterion = nn.NLLLoss()\noptimizer = optim.Adam(model.parameters(),lr=1e-04)","32fd171e":"from torch import cuda,optim\ntrain_on_gpu = cuda.is_available()\nprint(f'Train on gpu: {train_on_gpu}')\n\n# Number of gpus\nif train_on_gpu:\n    gpu_count = cuda.device_count()\n    print(f'{gpu_count} gpus detected.')\n    if gpu_count > 1:\n        multi_gpu = True\n    else:\n        multi_gpu = False","0d0c7db4":"# from timeit import default_timer as timer\n# import torch\n# def train(model,criterion,optimizer,train_loader,valid_loader,save_file_name,max_epochs_stop=3,n_epochs=20\n#          ,print_every=2):\n    \n    \n#     epochs_no_improve=0\n#     valid_loss_min = np.Inf\n    \n#     valid_max_acc=0\n#     history=[]\n    \n#     try:\n#         print(f'Model has been trained for: {model.epochs} epochs \\n')\n#     except:\n#         model.epochs=0\n#         print(f'Training from scratch\\n')\n    \n#     overall_start=timer()\n    \n#     for epoch in range(n_epochs):\n#         train_loss = 0.0\n#         valid_loss = 0.0\n        \n#         train_acc = 0.0\n#         valid_acc = 0.0\n        \n#         model.train()\n#         start=timer()\n        \n#         #training loop\n#         for ii,(data,target) in enumerate(train_loader):\n            \n#             if train_on_gpu:\n#                 data,target=data.cuda(),target.cuda()\n                \n#             optimizer.zero_grad()\n            \n#             output = model(data)\n          \n#             loss = criterion(output,target)\n#             loss.backward()\n            \n#             optimizer.step()\n            \n#             train_loss+=loss.item() * data.size(0)\n            \n#             _,pred = torch.max(output,dim=1)\n#             correct_tensor = pred.eq(target.data.view_as(pred))\n            \n#             accuracy = torch.mean(correct_tensor.type(torch.FloatTensor))\n            \n#             train_acc+=accuracy.item() * data.size(0)\n#             print(\n#                 f'Epoch: {epoch}\\t{100 * (ii + 1) \/ len(train_loader):.2f}% complete. {timer() - start:.2f} seconds elapsed in epoch.',\n#                 end='\\r')\n        \n#         else:\n#             model.epochs+=1\n        \n#             with torch.no_grad():\n#                 model.eval()\n                \n#                 for data,target in valid_loader:\n#                     if train_on_gpu:\n#                         data,target=data.cuda(),target.cuda()\n                        \n#                         output = model(data)\n                       \n#                         loss = criterion(output,target)\n                        \n#                         valid_loss+=loss.item() * data.size(0)\n                        \n#                         _,pred = torch.max(output,dim=1)\n#                         correct_tensor = pred.eq(target.data.view_as(pred))\n#                         accuracy = torch.mean(correct_tensor.type(torch.FloatTensor))\n                        \n#                         valid_acc+=accuracy.item() * data.size(0)\n                        \n#                 train_loss = train_loss \/ len(train_loader.dataset)\n#                 valid_loss = valid_loss \/ len(valid_loader.dataset)\n                \n#                 train_acc = train_acc \/ len(train_loader.dataset)\n#                 valid_acc = valid_acc \/ len(valid_loader.dataset)\n                \n#                 history.append([train_loss,valid_loss,train_acc,valid_acc])\n                \n#                 if (epoch + 1) % print_every == 0:\n#                     print(\n#                         f'\\nEpoch: {epoch} \\tTraining Loss: {train_loss:.4f} \\tValidation Loss: {valid_loss:.4f}'\n#                     )\n#                     print(\n#                         f'\\t\\tTraining Accuracy: {100 * train_acc:.2f}%\\t Validation Accuracy: {100 * valid_acc:.2f}%'\n#                     )\n#                 if valid_loss < valid_loss_min:\n#                     # Save model\n#                     torch.save(model.state_dict(), save_file_name)\n#                     # Track improvement\n#                     epochs_no_improve = 0\n#                     valid_loss_min = valid_loss\n#                     valid_best_acc = valid_acc\n#                     best_epoch = epoch\n                    \n#                 else:\n#                     epochs_no_improve += 1\n#                     # Trigger early stopping\n#                     if epochs_no_improve >= max_epochs_stop:\n#                         print(\n#                             f'\\nEarly Stopping! Total epochs: {epoch}. Best epoch: {best_epoch} with loss: {valid_loss_min:.2f} and acc: {100 * valid_acc:.2f}%'\n#                         )\n#                         total_time = timer() - overall_start\n#                         print(\n#                             f'{total_time:.2f} total seconds elapsed. {total_time \/ (epoch+1):.2f} seconds per epoch.'\n#                         )\n\n#                         # Load the best state dict\n#                         model.load_state_dict(torch.load(save_file_name))\n#                         # Attach the optimizer\n#                         model.optimizer = optimizer\n\n#                         # Format history\n#                         history = pd.DataFrame(\n#                             history,\n#                             columns=[\n#                                 'train_loss', 'valid_loss', 'train_acc',\n#                                 'valid_acc'\n#                             ])\n#                         return model, history\n                    \n                    \n#     model.optimizer = optimizer\n#     total_time = timer() - overall_start\n#     print(\n#         f'\\nBest epoch: {best_epoch} with loss: {valid_loss_min:.2f} and acc: {100 * valid_acc:.2f}%'\n#     )\n#     print(\n#         f'{total_time:.2f} total seconds elapsed. {total_time \/ (epoch):.2f} seconds per epoch.'\n#     )\n#     # Format history\n#     history = pd.DataFrame(\n#         history,\n#         columns=['train_loss', 'valid_loss', 'train_acc', 'valid_acc'])\n#     return model, history\n","f1c993d6":"# save_file_name = 'resnet34-transfer-1.pth'\n# checkpoint_path = 'resnet34-transfer-1.pth'\n# optimizer = optim.Adam(model.parameters(),lr=1e-04)\n# model, history = train(\n#     model,\n#     criterion,\n#     optimizer,\n#     dataloader['train'],\n#     dataloader['val'],\n#     save_file_name=save_file_name,\n#     max_epochs_stop=5,\n#     n_epochs=40,\n#     print_every=1)","1f6d4cdb":"# plt.figure(figsize=(8, 6))\n# for c in ['train_loss', 'valid_loss']:\n#     plt.plot(\n#         history[c], label=c)\n# plt.legend()\n# plt.xlabel('Epoch')\n# plt.ylabel('Average Negative Log Likelihood')\n# plt.title('Training and Validation Losses')","5247ebe4":"# save_file_name = 'resnet34-transfer-1.pth'\n# model.load_state_dict(torch.load(save_file_name))","0066241f":"# for param in model.parameters():\n#   param.requires_grad=True","2f52deba":"# save_file_name = 'resnet34-transfer-2.pth'\n# checkpoint_path = 'resnet34-transfer-2.pth'\n# optimizer = optim.Adam(model.parameters(),lr=1e-03)\n# model, history = train(\n#     model,\n#     criterion,\n#     optimizer,\n#     dataloader['train'],\n#     dataloader['val'],\n#     save_file_name=save_file_name,\n#     max_epochs_stop=5,\n#     n_epochs=40,\n#     print_every=1)","141a931f":"# plt.figure(figsize=(8, 6))\n# for c in ['train_loss', 'valid_loss']:\n#     plt.plot(\n#         history[c], label=c)\n# plt.legend()\n# plt.xlabel('Epoch')\n# plt.ylabel('Average Negative Log Likelihood')\n# plt.title('Training and Validation Losses')","c0946ac5":"# plt.figure(figsize=(8, 6))\n# for c in ['train_acc', 'valid_acc']:\n#     plt.plot(\n#         100 * history[c], label=c)\n# plt.legend()\n# plt.xlabel('Epoch')\n# plt.ylabel('Average Accuracy')\n# plt.title('Training and Validation Accuracy')","2df8ef95":"import torch\ncheckpoint=torch.load(\"..\/input\/resnet2\/resnet34-transfer-2.pth\")\n\nmodel.load_state_dict(checkpoint)\nfor param in model.parameters():\n    param.requies_grad=False\n    \nmodel.eval()    \n","83bdb6e8":"data = {\n    'train':\n    datasets.ImageFolder(root=train_folder, transform=image_transforms['train']),\n    'test':\n    datasets.ImageFolder(root=test_folder, transform=image_transforms['test'])\n}","f6473620":"model.class_to_idx = data['train'].class_to_idx\nmodel.idx_to_class = {\n    idx: class_\n    for class_, idx in model.class_to_idx.items()\n}\n\nlist(model.idx_to_class.items())[:10]","78c5af97":"def predict(image_path, model, topk=5):\n    \"\"\"Make a prediction for an image using a trained model\n\n    Params\n    --------\n        image_path (str): filename of the image\n        model (PyTorch model): trained model for inference\n        topk (int): number of top predictions to return\n\n    Returns\n\n    \"\"\"\n    real_class = image_path.split('\/')[-2]\n\n    # Convert to pytorch tensor\n    img_tensor = process_image(image_path)\n\n    # Resize\n    if train_on_gpu:\n        img_tensor = img_tensor.view(1, 3, 224, 224).cuda()\n    else:\n        img_tensor = img_tensor.view(1, 3, 224, 224)\n\n    # Set to evaluation\n    \n    with torch.no_grad():\n        model.eval()\n        # Model outputs log probabilities\n        out = model(img_tensor)\n        ps = torch.exp(out)\n\n        # Find the topk predictions\n        topk, topclass = ps.topk(topk, dim=1)\n        \n        # Extract the actual classes and probabilities\n        top_classes = [\n            model.idx_to_class[class_] for class_ in topclass.cpu().numpy()[0]\n        ]\n        top_p = topk.cpu().numpy()[0]\n\n        return img_tensor.cpu().squeeze(),top_classes, top_p,real_class","78ddb8d8":"# import cv2\n# i=cv2.imread(test_folder+\"Chrysler 300 SRT-8 2010\"+\"\/02408.jpg\")\n# i.shape","18ad45ee":"# from PIL import Image\n# img, top_p, top_classes,_= predict(test_folder+\"Chrysler 300 SRT-8 2010\"+\"\/02408.jpg\", model)","3cfe9791":"\nimage_names=list()\n\nfor folder in os.listdir(test_folder):\n  for img in os.listdir(test_folder+\"\/\"+folder):\n    image_names.append(img.split(\".\")[0])\n     ","da5dfa42":"# image_names","4181e5de":"# results=list()\n# def test_per_class(model, test_loader, criterion, classes):\n    \n#     total_class = len(classes)\n\n#     test_loss = 0.0\n#     class_correct = list(0. for i in range(total_class))\n#     class_total = list(0. for i in range(total_class))\n\n#     device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    \n#     with torch.\n    \n#     model.eval()  # prep model for evaluation\n\n#     for data, target in test_loader:\n#         # Move input and label tensors to the default device\n#         data, target = data.to(device), target.to(device)\n#         # forward pass: compute predicted outputs by passing inputs to the model\n#         output = model(data)\n#         # calculate the loss\n#         loss = criterion(output, target)\n#         # update test loss\n#         test_loss += loss.item() * data.size(0)\n#         print(output.shape)\n#         # convert output probabilities to predicted class\n#         _, pred = torch.max(output, 1)\n        \n#         results.append(pred)\n#         # compare predictions to true label\n        \n#         # calculate test accuracy for each object class\n       \n","1da9461c":"results=list()\ndef test_per_class(model, test_loader, criterion, classes,topk=5):\n    \n    total_class = len(classes)\n\n    test_loss = 0.0\n    class_correct = list(0. for i in range(total_class))\n    class_total = list(0. for i in range(total_class))\n\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    \n    with torch.no_grad():\n      model.eval()  # prep model for evaluation\n\n      for data, target in test_loader:\n        # Move input and label tensors to the default device\n          data, target = data.to(device), target.to(device)\n        # forward pass: compute predicted outputs by passing inputs to the model\n          output = model(data)\n          \n          ps = torch.exp(output)\n          \n        # calculate the loss\n          loss = criterion(output, target)\n        # update test loss\n        # convert output probabilities to predicted class\n          topk, topclass = ps.topk(1, dim=1)\n           \n          \n        # Extract the actual classes and probabilities\n          top_classes = [\n            model.idx_to_class[class_] for class_ in topclass.cpu().numpy()[0]\n        ]\n          top_p = topk.cpu().numpy()[0]\n          \n          results.append(topclass[0][0]) \n\n        ","b765c315":"test_per_class(model,dataloader['test'],criterion,folders)","26882a15":"df = pd.DataFrame({'Id': image_names , 'Predicted': list(map(int,results))} , columns=['Id', 'Predicted'])","ac4a0d47":"df.to_csv(\"submission.csv\",index=False)","b0a4142c":"pd.read_csv(\"submission.csv\")","ef0e2675":"# test","949db81a":"# splitting the train data into train and validation","78c02e1f":"# Dataloaders","74fa5fcb":"# Load the model","719a9ac3":"# transforms"}}