{"cell_type":{"defbffc1":"code","1e8b40eb":"code","aa4d27a0":"code","fe9e0b3d":"code","a7994013":"code","3d207500":"code","f6ef29cf":"code","752cb620":"code","daa61eb0":"code","2a843b7c":"code","82ae6cbf":"code","d2454a3c":"code","d9cb7ee2":"code","bb9f1333":"code","d4761de5":"code","493312f0":"code","a9cbbbea":"code","289d4cbb":"code","005d5e43":"code","bd1868f3":"code","8e6b3404":"code","7d17e816":"code","161f57da":"markdown","3c31f6fe":"markdown","339dc919":"markdown","c382e1b9":"markdown","af410eb0":"markdown","bef15df3":"markdown","d4b4697b":"markdown","c58d91bc":"markdown","cd59907b":"markdown","cd4bd361":"markdown"},"source":{"defbffc1":"!pip install scikit-optimize --upgrade","1e8b40eb":"import os\nimport random\nimport re\nimport time\n\nimport numpy as np\nimport pandas as pd\nimport plotly.express as px\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer\n\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.linear_model import LogisticRegression, SGDClassifier, Perceptron, RidgeClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.svm import LinearSVC\nfrom sklearn.tree import DecisionTreeClassifier \n\nimport skopt\nfrom skopt import gp_minimize\nfrom skopt.space import Real, Integer\nfrom skopt.utils import use_named_args\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","aa4d27a0":"file_path = '\/kaggle\/input\/60k-stack-overflow-questions-with-quality-rate\/data.csv'\ndata = pd.read_csv(file_path)","fe9e0b3d":"data.head()","a7994013":"data['Body'] = data['Title'] + \" \" + data['Body']","3d207500":"data.head()","f6ef29cf":"# Clean the data\ndef clean_text(text):\n    text = text.lower()\n    text = re.sub(r'[^(a-zA-Z)\\s]','', text)\n    return text\n\ndata['Body'] = data['Body'].apply(clean_text)\ndata.head()","752cb620":"#def remove_stopword\nfrom nltk.corpus import stopwords \nfrom nltk.tokenize import word_tokenize \n  \nexample_sent = \"This is a sample sentence, showing off the stop words filtration.\"  \nstop_words = set(stopwords.words('english')) \n\ndef remove_stopword(words):\n    list_clean = [w for w in words.split(' ') if not w in stop_words]\n    return ' '.join(list_clean)\n\ndata['Body'] = data['Body'].apply(remove_stopword)\ndata.head()","daa61eb0":"data.shape","2a843b7c":"N = len(data)\nTRAIN_PERC = 0.8\nind_train = np.random.rand(N) < TRAIN_PERC\ntrain, test = data[ind_train], data[~ind_train]\nprint(f'len(train)={len(train)}; len(test)={len(test)}')","82ae6cbf":"count_vect = CountVectorizer()\nX_train_counts = count_vect.fit_transform(train.Body)\nX_train_counts.shape","d2454a3c":"X_test_counts = count_vect.transform(test.Body)\nX_test_counts.shape","d9cb7ee2":"tfidf_transformer = TfidfTransformer()\nX_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\nX_train_tfidf.shape","bb9f1333":"X_test_tfidf = tfidf_transformer.transform(X_test_counts)\nX_test_tfidf.shape","d4761de5":"# I tried to add the length of Title, length of Body and number of tags, but it seems to globally decrease the accuracy!\n\"\"\"\nfrom scipy.sparse import hstack\n\ndef add_len_feat(X_tfidf, train_or_test):\n    list_title_len = [[len(title)] for title in train_or_test.Title]\n    list_body_len = [[len(body)] for body in train_or_test.Body]\n    list_tag_len = [[len(tag)] for tag in train_or_test.tags_processed]\n    return hstack([X_tfidf, list_title_len, list_body_len, list_tag_len])\n\nX_train_tfidf = add_len_feat(X_train_tfidf, train)\nX_test_tfidf = add_len_feat(X_test_tfidf, test)\n\"\"\"","493312f0":"clf_dict = {\n    'LogisticRegression': LogisticRegression,\n    'MultinomialNB': MultinomialNB,\n    'DecisionTreeClassifier': DecisionTreeClassifier,\n    'SGDClassifier': SGDClassifier,\n    'Perceptron': Perceptron,\n    'RidgeClassifier': RidgeClassifier,\n    'LinearSVC': LinearSVC,\n    'RandomForestClassifier': RandomForestClassifier,\n    'GradientBoostingClassifier': GradientBoostingClassifier,\n    #'MLPClassifier': MLPClassifier,\n}","a9cbbbea":"def get_accuracy(clf, n_estimators=None, max_depth=None, learning_rate=None, max_iter=None):\n    start = time.time()\n    text_clf = clf(**params).fit(X_train_tfidf, train.Y)\n    predicted = text_clf.predict(X_test_tfidf)\n    print(f'Accuracy gets in {round(time.time()-start, 2)}s.')\n    return np.mean(predicted == test.Y)\n\nresult_dict = {}\n\nfor clf_str, clf_fn in clf_dict.items():\n    if clf_str == 'LogisticRegression':\n        params = {'max_iter': 200}\n    elif clf_str == 'RandomForestClassifier':\n        params = {'n_estimators': 50,\n                  'max_depth': 10}\n    elif clf_str == 'DecisionTreeClassifier':\n        params = {'max_depth': 10}\n    elif clf_str == 'GradientBoostingClassifier':\n        params = {'n_estimators': 50,\n                  'learning_rate': 0.1}\n    else:\n        params = {}\n    accuracy = get_accuracy(clf=clf_fn, **params)\n    result_dict[clf_str] = accuracy\n    print(f\"Clf={clf_str}; Accuracy={accuracy}\")","289d4cbb":"result_dict = {\n    k: v\n    for k, v in sorted(\n        result_dict.items(),\n        key=lambda x: x[1],\n        reverse=True\n    )\n}\n\nresult_dict","005d5e43":"DIM_Logistic = [\n    Integer(100, 400, name='max_iter')\n]\n\nDIM_SVC = [\n    Real(1e-5, 1, name='tol', prior='log-uniform'),\n    Real(0.1, 1.5, name='C', prior='log-uniform')\n]\n\nDIM_SGDC = [\n    Real(1e-5, 1e-2, name='alpha', prior='log-uniform')\n]\n\nDIM_RF = [\n    Integer(1, 100, name='n_estimators'),\n    Integer(5, 30, name='max_depth')\n]\n\nDIMS = {\n    'LogisticRegression': DIM_Logistic,\n    'LinearSVC': DIM_SVC,\n    'SGDClassifier': DIM_SGDC,\n    'RandomForestClassifier': DIM_RF\n}","bd1868f3":"def optimize(clf_str='LinearSVC'):\n    \n    dimensions = DIMS[clf_str]\n    print(dimensions)\n    \n    @use_named_args(dimensions=dimensions)\n    def fitness(**params):\n        clf = clf_dict[clf_str](**params)\n        text_clf = clf.fit(X_train_tfidf, train.Y)\n        predicted = text_clf.predict(X_test_tfidf)\n        accuracy = np.mean(predicted == test.Y)\n        print(f'accuracy={accuracy} with params={params}')\n        return -1.0 * accuracy\n    \n    res = gp_minimize(func=fitness,\n                      dimensions=dimensions,\n                      acq_func='EI', # Expected Improvement.\n                      n_calls=10,\n                      random_state=666)\n    print(f'best accuracy={-1.0 * res.fun} with {res.x}')\n    return res","8e6b3404":"res_dict = {}\nfor clf_str, clf_dim in DIMS.items():\n    print(f'start optimizaton for {clf_str}')\n    res = optimize(clf_str=clf_str)\n    res_dict[clf_str] = res","7d17e816":"for clf_str, res in res_dict.items():\n    hyperparameters_label = [hp.name for hp in DIMS[clf_str]]\n    best_hyperparameters = dict(zip(hyperparameters_label, res.x))\n    print(f'clf={clf_str}\\nbest accuracy={-res.fun}\\nbest hyperparameters={best_hyperparameters}\\n')","161f57da":"# please upvote if you liked it :D","3c31f6fe":"# Best accuracy per model and the respective hyperparameters","339dc919":"# Next steps","c382e1b9":"# **Libraries and Data importation**","af410eb0":"# **Some data preprocessing**","bef15df3":"# Train and test sets splitting","d4b4697b":"WIP\n\nNext steps:\n- continue hyperparameters optimization\n- include title, tags, date as features\n- try other ML models like stacking, Keras\/TF","c58d91bc":"# **Building the TFIDF features**","cd59907b":"# **Models benchmarking**","cd4bd361":"For the machine learning modelisation and optimization part, go directly to **\"Train and test sets splitting\"** section"}}