{"cell_type":{"f47e3b93":"code","bcec0a4d":"code","bce45f76":"code","3780591d":"code","748bcb92":"code","e26bf906":"code","eba7d328":"code","5f7e5cb1":"code","41d9c40d":"code","f90dabf3":"code","1c64a0cd":"code","1875e586":"code","dacc11e0":"code","c975e72e":"code","67d7ff6f":"code","35a6d696":"code","54d03c8b":"code","4446075d":"code","ddefcd4e":"code","81cde82d":"code","35ba6569":"code","56113efb":"code","4284ddba":"code","5f2c2274":"code","1c256318":"code","ed80f421":"code","d02a6ae4":"markdown","547aa3e9":"markdown","84418861":"markdown","49be0f99":"markdown","33fdfd27":"markdown","4e135e4a":"markdown","93ddc64d":"markdown","8020c3e4":"markdown","d0bcd7f4":"markdown","c27bfe60":"markdown","c809ae76":"markdown","2994aa5e":"markdown","415df96f":"markdown","bf03604f":"markdown","a8e03413":"markdown","7eb3e727":"markdown","da219a91":"markdown"},"source":{"f47e3b93":"''' This project is to predict whether a Student gets placed in the campus interview or not.'''","bcec0a4d":"import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nimport xgboost\nimport lightgbm\nfrom sklearn.metrics import confusion_matrix, accuracy_score\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n \nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","bce45f76":"df = pd.read_csv('\/kaggle\/input\/factors-affecting-campus-placement\/Placement_Data_Full_Class.csv')","3780591d":"### Run this to Profile data\n\nimport pandas_profiling as pp\n\n\nprofile = pp.ProfileReport(    df, title=\"Campus Recruitment Profile\", html={\"style\": {\"full_width\": True}}, sort=None)\nprofile","748bcb92":"#Getting all the categorical columns except the target\ncategorical_columns = df.select_dtypes(exclude = 'number').drop('status', axis = 1).columns\n\nprint(categorical_columns)","e26bf906":"# First considering only numerical values for feature selection\nX = df.iloc[:,[2,4,7,10,12,14]].values\nY = df.iloc[:,13].values","eba7d328":"print(X)","5f7e5cb1":"print(Y)","41d9c40d":"len(df)","f90dabf3":"df.isnull().sum()","1c64a0cd":"# So salary column contains null values","1875e586":"from sklearn.impute import SimpleImputer\nimputer = SimpleImputer(missing_values=np.nan, strategy='mean')\nimputer.fit(X[:,[5]])\nX[:,[5]] = imputer.transform(X[:,[5]])","dacc11e0":"print(X)","c975e72e":"plt.rcParams['figure.figsize']=15,6 \nsns.set_style(\"darkgrid\")\n\nx = df.iloc[:,[2,4,7,10,12,14]]\n\nfrom sklearn.ensemble import ExtraTreesClassifier\nimport matplotlib.pyplot as plt\nmodel = ExtraTreesClassifier()\nmodel.fit(X,Y)\nprint(model.feature_importances_) \nfeat_importances = pd.Series(model.feature_importances_, index=x.columns)\nfeat_importances.nlargest(12).plot(kind='barh')\nplt.show()","67d7ff6f":"# So we can conclude that 'Salary' and 'ssc_p' are two relavent features for predicting the status of placement for a student","35a6d696":"# Import the function\n#from scipy.stats import chi2_contingency\n#Testing the relationship\n#chi_res = chi2_contingency(pd.crosstab(df['status'], df['gender']))\n#print('Chi2 Statistic: {}, p-value: {}'.format(chi_res[0], chi_res[1]))","54d03c8b":"from scipy.stats import chi2_contingency\nchi2_check = []\nfor i in categorical_columns:\n    if chi2_contingency(pd.crosstab(df['status'], df[i]))[1] < 0.05:\n        chi2_check.append('Reject Null Hypothesis')\n    else:\n        chi2_check.append('Fail to Reject Null Hypothesis')\nres = pd.DataFrame(data = [categorical_columns, chi2_check] \n             ).T \nres.columns = ['Column', 'Hypothesis']\nprint(res)","4446075d":"# If we choose our p-value level to 0.05, if the p-value test result is more than 0.05 then we fail to reject the Null Hypothesis. \n# This means, there is no relationship between the Feature and Dependent Variable based on the Chi-Square test of independence.\n# And if the p-value test result is less than 0.05 then we reject the Null Hypothesis. \n# This means, there is a relationship between the Feature and Dependent Variable based on the Chi-Square test of independence.","ddefcd4e":"# So after feature selection of categorical and numerical features, X comes as,\nX = df.iloc[:,[2,9,11,14]].values\nimputer = SimpleImputer(missing_values=np.nan, strategy='mean')\nimputer.fit(X[:,[3]])\nX[:,[3]] = imputer.transform(X[:,[3]])","81cde82d":"print(df['workex'].unique())\nprint(df['specialisation'].unique())\nprint(df['status'].unique())","35ba6569":"from sklearn.preprocessing import LabelEncoder\nle1 = LabelEncoder()\nX[:,1] = le1.fit_transform(X[:, 1])\nle2 = LabelEncoder()\nX[:,2] = le2.fit_transform(X[:, 2])\nle3 = LabelEncoder()\nY = le3.fit_transform(Y)","56113efb":"print(X[0])","4284ddba":"print(Y)","5f2c2274":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, Y, test_size = 0.2, random_state=1)","1c256318":"from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train[:,[0,3]] = sc.fit_transform(X_train[:,[0,3]])\nX_test[:,[0,3]] = sc.transform(X_test[:,[0,3]])","ed80f421":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\nfrom catboost import CatBoostClassifier\nimport lightgbm\nimport xgboost\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\n\nnames = [\n    \"CatBoostClassifier\",\n    \"Logistic Regression\",\n    \"Support Vector Machine\",\n    \"Decision Tree\",\n    \"Neural Network\",\n    \"Random Forest\",\n    \"XGBoost\",\n    \"LGBMClassifier\",\n    \"XGBRFClassifier\",\n    \"GradientBoosting\",\n    \"GaussianNB\",\n    \"KNeighborsClassifier\"\n]\nmodels = [\n    CatBoostClassifier(verbose= False),\n    LogisticRegression(),\n    SVC(),\n    DecisionTreeClassifier(),\n    MLPClassifier(),\n    RandomForestClassifier(),\n    XGBClassifier(),\n    lightgbm.LGBMClassifier(max_depth=2, random_state=4),\n    xgboost.XGBRFClassifier(max_depth=3, random_state=1),\n    GradientBoostingClassifier(max_depth=2, random_state=1),\n    GaussianNB(),\n    KNeighborsClassifier(n_neighbors=5, p=2, metric='minkowski')\n]\n\naccuracy=[]\nfor model, name in zip(models,names):\n    model.fit(X_train, y_train)\n    \n    y_pred = model.predict(X_test)\n    print('Confusion matrix of ',name)\n    print(confusion_matrix(y_test, y_pred))\n    ac = accuracy_score(y_test, y_pred)\n    print('Accuracy score is ',ac)\n    accuracy.append(ac)\n    print('='*50)\n\nAccuracy_list = pd.DataFrame(list(zip(names, accuracy)),columns =['Model', 'Accuracy'])\nAccuracy_list= Accuracy_list.sort_values('Accuracy', axis=0, ascending=False, inplace=False, kind='quicksort', na_position='last', ignore_index=True, key=None)\n\nplt.rcParams['figure.figsize']=20,6 \nsns.set_style(\"darkgrid\")\nax = sns.barplot(x = 'Model',y = 'Accuracy',data = Accuracy_list , palette = \"rocket\", saturation =1.5)\nplt.xlabel(\"Model\", fontsize = 20 )\nplt.ylabel(\"Accuracy\", fontsize = 20)\nplt.title(\"Accuracy of different Models\", fontsize = 20)\nplt.xticks(fontsize = 11, horizontalalignment = 'center', rotation = 8)\nplt.yticks(fontsize = 13)\nfor p in ax.patches:\n    width, height = p.get_width(), p.get_height()\n    x, y = p.get_xy() \n    ax.annotate(f'{height:.2%}', (x + width\/2, y + height*1.02), ha='center', fontsize = 'x-large')\nplt.show()","d02a6ae4":"# Applying classification models on the Training set","547aa3e9":" # Please let me know in the comments what improvements can be done ","84418861":"So we conclude that 'workex' and 'specialisation' are two important features for predicting status.","49be0f99":"# Data Cleaning","33fdfd27":"# Feature Selection of Numerical Values - ExtraTreesClassifier","4e135e4a":"Observations : salary column contains null values","93ddc64d":"#### Finding the categories","8020c3e4":"## Mean imputation for null values","d0bcd7f4":"### Label encoding","c27bfe60":"# Splitting the df into training set and test set","c809ae76":"# Feature Selection of Categorical Data - chi2_contingency","2994aa5e":"# Profile data","415df96f":"# Encoding Categorical Values","bf03604f":"# **Import Libraries**","a8e03413":"# Importing the df","7eb3e727":"# About Data\n\n\nThis data set consists of Placement data of students in our campus. It includes secondary and higher secondary school percentage and specialization. It also includes degree specialization, type and Work experience and salary offers to the placed students\n\nSource: https:\/\/www.kaggle.com\/benroshan\/factors-affecting-campus-placement\n\n### Attribute Information:\n\nsl_no\nSerial Number\n\ngender\nGender- Male='M',Female='F'\n\nssc_p\nSecondary Education percentage- 10th Grade\n\nssc_b\nBoard of Education- Central\/ Others\n\nhsc_p\nHigher Secondary Education percentage- 12th Grade\n\nhsc_b\nBoard of Education- Central\/ Others\n\nhsc_s\nSpecialization in Higher Secondary Education\n\ndegree_p\nDegree Percentage\n\ndegree_t\nUnder Graduation(Degree type)- Field of degree education\n\nworkex\nWork Experience\n\netest_p\nEmployability test percentage ( conducted by college)\n\nspecialisation\nPost Graduation(MBA)- Specialization\n\nmba_p\nMBA percentage\n\nstatus\nStatus of placement- Placed\/Not placed\n\nsalary\nSalary offered by corporate to candidates","da219a91":"# Feature Scaling"}}