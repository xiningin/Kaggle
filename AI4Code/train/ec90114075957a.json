{"cell_type":{"36b4af5d":"code","86e1715d":"code","35ea8f89":"code","8c4b761e":"code","8948883b":"code","d3224135":"code","9ee0eb0a":"code","8750d620":"code","32089efe":"code","035927e4":"code","5a28e70f":"code","359716ec":"code","66f1818f":"code","c28ee1f3":"code","052dbcc5":"code","94f8197c":"code","9d0ac5b4":"code","19c52771":"code","8201f20c":"code","1e3b7784":"code","7a7aee5d":"code","c11f448a":"code","9df8c3cb":"code","ed6e3832":"markdown","489fbb33":"markdown","2a20b3cb":"markdown","c0cec7fc":"markdown","2876d3ae":"markdown","ee94264c":"markdown","2f1723d8":"markdown","e877d171":"markdown","573e7ebb":"markdown","b7f34008":"markdown","6444a91e":"markdown","fb140d9b":"markdown","9b44922f":"markdown","284c37ca":"markdown"},"source":{"36b4af5d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\n\"\"\"Import Fundamental Python libraries\"\"\"\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n\n\n## download spacy model specific for English language pipeline.\n\"\"\"uncomment this python code and run here in the Jupyter Cell like a python command\"\"\"\n# !python -m spacy download en_core_web_sm\n\n\"\"\"nlp is now the variable that has the pre-trained spacy model \"\"\"\nimport spacy\nnlp = spacy.load(\"en_core_web_sm\")\n\n\n# Gensim\nimport gensim\nimport gensim.corpora as corpora\nfrom gensim.utils import simple_preprocess\nfrom gensim.models import CoherenceModel\nimport matplotlib.pyplot as plt\n\n\n## NLTK Libraries\nimport nltk; \nnltk.download('stopwords')\n\n# Core Packages\nimport os, re, operator, warnings\nwarnings.filterwarnings('ignore')  # Let's not pay heed to them right now\n\n# Plotting tools\nimport pyLDAvis\nimport pyLDAvis.gensim_models # don't skip this\nimport matplotlib.pyplot as plt\n%matplotlib inline\n","86e1715d":"## Get a sampel test dataset from Gensim library itself, and load one of the text corpuss available\n\ndef clean(text):\n    return(''.join([i for i in text]))\n\ntest_data_dir = '{}'.format(os.sep).join([gensim.__path__[0], 'test', 'test_data'])\nprint(test_data_dir)\nlee_train_file = test_data_dir + os.sep + 'lee_background.cor'\ntext = open(lee_train_file).read()\ntext[:100]","35ea8f89":"## Stopwords from NTLK Libraries\n\n\nfrom nltk.corpus import stopwords\nstop_words = stopwords.words('english') \n\"\"\"This will import all the stopwords in English language , similarly you can do for any other language.\"\"\"\n\nstop_words.extend(['from', 'subject', 're', 'edu', 'use'])\n\"\"\"This attribute allows you to update your stop words list and make you pre-processing more aggressive and accurrate\"\"\"","8c4b761e":"# Importing a inbuilt dataset from SKlearn module which is famous for LDA implementation of NLP projects\n\nfrom sklearn.datasets import fetch_20newsgroups\ndataset =fetch_20newsgroups()\ndataset.keys()","8948883b":"\"\"\"We can create a Pandas dataframe using the key value from the above dataset object.\"\"\"\n\n# Initialise a pandas Dataframe\ndata = pd.DataFrame()\ndata['text'] = dataset.data\ndata['target'] = dataset.target\n\n\"\"\"Loop through each target value in dataset and get the respective label, this will be out topics.\"\"\"\nlabel=[]\nfor i in data['target']:\n    label.append(dataset.target_names[i])\ndata['label'] = label\ndata.head()","d3224135":"## All the Unique target names this dataset talks about.\n\ndata.label.unique()","9ee0eb0a":"data.text.head(5)","8750d620":"\"\"\"These are all he type of  labels present in the dataset, we can staright away clean the dataset \"\"\"\ndata.label.value_counts()","32089efe":"\"\"\"This fucntion will take a text string and spilt them at \"@\" sign and return first name and Domain of the email.\nThe Purpose to do this is to create two new feature m which might be useful\"\"\"\ndef fetch_email_domain(text):\n    name = re.split(\"@\" , text)[0]\n    domain = re.split(\"@\" , text)[1]\n    return name , domain\n\nfetch_email_domain(\"vjyadav193@gmail.com\")","035927e4":"### To Iterate over each news in every line, we will have to convert it into a list\n\ndata_list = data.text.tolist()\ndata_list[1]","5a28e70f":"\"\"\"WE see many things that needs to be clean\n1. From syntax, and erase Email from corpus.\n2. New line character \\n\n3. Punctuation and special characters\n\nmany More\"\"\"\n\ndata_list  = [re.sub('\\S*@\\S*\\s?', \"\" ,line) for line in data_list]\ndata_list\ndata_list  = [re.sub('\\s+', \" \" ,line) for line in data_list]\n\ndata_list  = [re.sub('\\'', \"\" ,line) for line in data_list]\ndata_list[1]","359716ec":"\"\"\"nlp is the model  that has the Spacy model pipleline built for English language \"\"\"\n\nimport spacy\nnlp = spacy.load(\"en_core_web_sm\")\n","66f1818f":"## Define the stop words using NLTK corpus data\n\nfrom nltk.corpus import stopwords\nstop_words = stopwords.words(\"english\")\nstop_words[1:5]\n\n","c28ee1f3":"# Gensim\u2019s simple_preprocess() is great for this. Additionally we have set deacc=True to remove the punctuations.\n\ndef sentence_to_word(sentences):\n    for sentence in sentences:\n        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n\ndata_words = list(sentence_to_word(data_list))\n\nprint(data_words[:1])","052dbcc5":"# Build the bigram and trigram models\n\nbigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\ntrigram = gensim.models.Phrases(bigram[data_words], threshold=100)  \n\n# Faster way to get a sentence clubbed as a trigram\/bigram\nbigram_mod = gensim.models.phrases.Phraser(bigram)\ntrigram_mod = gensim.models.phrases.Phraser(trigram)\n\n# See trigram example\nprint(trigram_mod[bigram_mod[data_words[0]]])","94f8197c":"# def remove_stopwords(text):\n#     for doc in text:\n#         if doc not in stop_words:\n#             return [words for words in simple_preprocess(str(doc))]\n                                    \n                    \ndef remove_stopwords(texts):\n    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n\n\n\ndef make_bigrams(texts):\n    return [bigram_mod[doc] for doc in texts]\n\n\n\ndef make_trigrams(texts):\n    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n\n\ndef lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n    \"\"\"https:\/\/spacy.io\/api\/annotation\"\"\"\n    texts_out = []\n    for sent in texts:\n        doc = nlp(\" \".join(sent)) \n        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n    return texts_out","9d0ac5b4":"# Remove Stop Words\ndata_words_nostops = remove_stopwords(data_words)\n\n# Form Bigrams\ndata_words_bigrams = make_bigrams(data_words_nostops)\n\n# Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n# python3 -m spacy download en_core_web_sm\nnlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n\n# Do lemmatization keeping only noun, adj, vb, adv\ndata_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n\nprint(data_lemmatized[:1])\n","19c52771":"# Create Dictionary\n\n\"\"\"This module implements the concept of the Dictionary in Pytons, a mapping between words and their integer ids.\"\"\"\nid2word = corpora.Dictionary(data_lemmatized)\n\n\n# Create Corpus\ntexts = data_lemmatized\n\n# Term Document Frequency\n\"\"\"doc2bow us Document to Bag of Words format, It outputs a tuple of token id and token words\"\"\"\ncorpus = [id2word.doc2bow(text) for text in texts]\n\n# View\n\nprint(corpus[:1])","8201f20c":"# Human readable format of corpus (term-frequency)\n\"\"\"This fucntion creates the reverse mapping of ID to original lemmatized word\"\"\"\n[[(id2word[id], freq) for id, freq in cp] for cp in corpus[:1]]","1e3b7784":"# Build LDA model\n\n\"\"\"Enable logging so we can see the progress of ducment convergence and it can help us tune our parameters for model\"\"\"\n\nimport logging\nlogging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n\n\n\"\"\"If you set passes = 10 you will see this line 10 times. \nMake sure that by the final passes, most of the documents have converged. \nSo you want to choose both passes and iterations to be high enough for this to happen.\"\"\"\n\n\nlda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n                                           id2word=id2word,\n                                           num_topics=20, \n                                           random_state=100,\n                                           update_every=1,\n                                           chunksize=100,\n                                           passes=10,\n                                           alpha='auto',\n                                            eval_every=1,                                        \n                                           per_word_topics=True)\n\n\n","7a7aee5d":"# Print the Keyword in the 10 topics\nprint(lda_model.print_topics())\ndoc_lda = lda_model[corpus]","c11f448a":"## Compute Model Perplexity and Coherence Score\n\n\n# Compute Perplexity\nprint('\\nPerplexity: ', lda_model.log_perplexity(corpus))  # a measure of how good the model is. lower the better.\n\n# Compute Coherence Score\ncoherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\ncoherence_lda = coherence_model_lda.get_coherence()\nprint('\\nCoherence Score: ', coherence_lda)","9df8c3cb":"# Visualize the topics\n\nimport pyLDAvis\npyLDAvis.enable_notebook()\nvis = pyLDAvis.gensim_models.prepare(lda_model, corpus, id2word)\nvis","ed6e3832":"## Tokenize the text to convert them into words format from the sentence format.","489fbb33":"In bag of Words as you might know, that the model creates a corpus with assigning unique ID to individual word, and then providing the frequency of the word. This helps to define how important that word is in that particular document.\n\nFor example, (0, 1) above implies, word id 0 occurs once in the first document. Likewise, word id 1 occurs twice and so on.\n\nThis is used as the input by the LDA model.\n","2a20b3cb":"## Building LDA Mallet Model","c0cec7fc":"## Approach by Understanding LDA\nLDA stands for Latent Dirichlet Allocation, and it is a type of topic modeling algorithm. The purpose of LDA is to learn the representation of a fixed number of topics, and given this number of topics learn the topic distribution that each document in a collection of documents has.\n\nMore explaination here: - http:\/\/littlesaiph.blogspot.com\/2012\/07\/laymans-explanation-of-online-lda.html","2876d3ae":"### Below is the utility functions used for basic preprocessing of Text data.","ee94264c":"###  Building the Topic Model","2f1723d8":"## Visualize the topics-keywords","e877d171":"Here, the listof index 0,1,3 are the index of topics, and the words shown are the top words with maximum weights responsible for distiunguishing that particular topic.","573e7ebb":"Automatically detect common phrases \u2013 aka multi-word expressions, word n-gram collocations \u2013 from a stream of sentences.\n","b7f34008":"### Data Transformation : - Dictionary and Corpus\n\nThe two main inputs to the LDA topic model are the dictionary(id2word) and the corpus. ","6444a91e":"## Bigrams and Trigrams creation using Gensim Phrase.","fb140d9b":"# Reference Articles\n\n1. All of the below links have been refered and used to build certain parts of this notebook. \n2. The purpose of this notebook is for learning and practicing, relevant credits goes to respective owners of below articles , books , websites.\n\nhttps:\/\/radimrehurek.com\/gensim\/models\/phrases.html\n\nhttps:\/\/www.machinelearningplus.com\/nlp\/topic-modeling-gensim-python\/#1introduction","9b44922f":"## Refer this Genesims document for detailed information of LDA model and its hyperparameter for tuning purposes.\n\nhttps:\/\/radimrehurek.com\/gensim\/auto_examples\/tutorials\/run_lda.html#sphx-glr-auto-examples-tutorials-run-lda-py\n\n","284c37ca":"####  Important Parameters however which you must know:\n\n1. Number of Topics;- It can be anything as per output expectations, or the data itself. For our data we will take K = 20, as we know the data has 20 news groups.\n\n2. chuncksize: - Controls how many documents are processed at a time in the training algorithm.\n\n3. passes: - controls how often we train the model on the entire corpus, same like \" epochs\"\n\n4. We set alpha = 'auto' and eta = 'auto'. Again this is somewhat technical, but essentially we are automatically learning two parameters in the model that we usually would have to specify explicitly."}}