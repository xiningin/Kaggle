{"cell_type":{"93ecbe28":"code","9c8b1f20":"code","602a9839":"code","1e99b47b":"code","5c445d44":"code","8b1c28bd":"code","6721ffa4":"code","359321da":"code","6951b6e8":"code","1fedf03d":"code","c0f3dc59":"code","8de9d128":"code","94f81be5":"code","ae9d13a4":"code","7fa39d5d":"code","a88e5276":"code","80b25d41":"code","ada3451e":"code","f10066d7":"code","695c1b1f":"code","dfa24126":"code","7a85b467":"code","8ede5a58":"code","834dfc42":"code","e28125c6":"markdown","67b59e8b":"markdown","d575f754":"markdown","088a58f3":"markdown","f5fee15c":"markdown","51276acf":"markdown","89ea6615":"markdown","cbefe320":"markdown","9c946a91":"markdown","f679013a":"markdown","bcd39301":"markdown","3bef3746":"markdown","c771711e":"markdown","c080b30c":"markdown","24f78dc2":"markdown","dd7a45c1":"markdown","71614ef8":"markdown","c351f2e9":"markdown","d4e52906":"markdown","1d7f586a":"markdown","40556a08":"markdown","11e5671a":"markdown","e19964f7":"markdown","91de1026":"markdown"},"source":{"93ecbe28":"import os\nimport shutil\nimport json\nimport yaml\nimport random\nimport pandas as pd","9c8b1f20":"!git clone https:\/\/github.com\/ultralytics\/yolov5\n%cd yolov5\n!pip install -qr requirements.txt\n%cd ..","602a9839":"# !pip install -q --upgrade wandb\n# key=YOUR_WANDB_TOKEN\n# import wandb\n# wandb.login(key=key)\n!wandb off","1e99b47b":"data = json.load(open('..\/input\/cowboyoutfits\/train.json', 'r'))\nann = data['annotations']\nrandom.seed(34)\nrandom.shuffle(ann)","5c445d44":"ci = [87, 1034, 131, 318, 588]  # category_id, \u5206\u522b\u5bf9\u5e94belt,sunglasses,boot,cowboy_hat,jacket\n\nprint('total:')\nfor i in ci:\n    count = 0\n    for j in ann:\n        if j['category_id'] == i:\n            count += 1\n    print(f'id: {i} counts: {count}')","8b1c28bd":"total_id = set(each['image_id'] for each in ann)\nval_id = set()\na, b, c, d, e = 0, 0, 0, 0, 0  # \u7528\u4e8e\u6bcf\u7c7b\u7684\u8ba1\u6570\nfor each in ann:\n    if (each['category_id'] == ci[0]) and (a < 2):\n        val_id.add(each['image_id'])\n        a += 1\n    elif (each['category_id'] == ci[1]) and (b < 20):\n        val_id.add(each['image_id'])\n        b += 1\n    elif (each['category_id'] == ci[2]) and (c < 4):\n        val_id.add(each['image_id'])\n        c += 1\n    elif (each['category_id'] == ci[3]) and (d < 7):\n        val_id.add(each['image_id'])\n        d += 1\n    elif (each['category_id'] == ci[4]) and (e < 17):\n        val_id.add(each['image_id'])\n        e += 1\n\nval_ann = []\nfor imid in val_id:\n    for each_ann in ann:\n        if each_ann['image_id'] == imid:\n            val_ann.append(each_ann)\n            \nlen(val_id),len(val_ann)","6721ffa4":"print('val set:')\nfor kind in ci:\n    num = 0\n    for i in val_ann:\n        if i['category_id'] == kind:\n            num += 1\n    print(f'id: {kind} counts: {num}')","359321da":"# The rest images are for training\ntrain_id = total_id - val_id\ntrain_ann = []\nfor each_ann in ann:\n    for tid in train_id:\n        if each_ann['image_id'] == tid:\n            train_ann.append(each_ann)\n            break\nlen(train_id), len(train_ann)","6951b6e8":"os.makedirs('.\/data\/images\/train', exist_ok=True)\nos.makedirs('.\/data\/images\/val', exist_ok=True)\n          \ntrain_img = []\n# Move train images\nfor j in data['images']:\n    for i in train_id:\n        if j['id'] == i:\n            shutil.copy('..\/input\/cowboyoutfits\/images\/'+j['file_name'], '.\/data\/images\/train')\n            train_img.append(j)\n            \nval_img = []\n# Move val images\nfor j in data['images']:\n    for i in val_id:\n        if j['id'] == i:\n            shutil.copy('..\/input\/cowboyoutfits\/images\/'+j['file_name'], '.\/data\/images\/val')\n            val_img.append(j)\n\nlen(val_img), len(train_img)","1fedf03d":"os.makedirs('.\/data\/labels\/train', exist_ok=True)\nos.makedirs('.\/data\/labels\/val', exist_ok=True)\n\ntrain_info = [(each['id'], each['file_name'].split('.')[0], each['width'], each['height']) for each in train_img]\nval_info = [(each['id'], each['file_name'].split('.')[0], each['width'], each['height']) for each in val_img]\n\ntrans = {f'{each}': f'{idx}' for (idx, each) in enumerate(ci)}  # Mapping the category_ids\n\n# Create *.txt files for training\nfor (imid, fn, w, h) in train_info:\n    with open('.\/data\/labels\/train\/' + fn + '.txt', 'w') as t_f:\n        for t_ann in train_ann:\n            if t_ann['image_id'] == imid:\n                # convert X_min,Y_min,w,h to X_center\/width,Y_center\/height,w\/width,h\/height\n                bbox = [str((t_ann['bbox'][0] + (t_ann['bbox'][2] \/ 2) - 1) \/ float(w)) + ' ',\n                        str((t_ann['bbox'][1] + (t_ann['bbox'][3] \/ 2) - 1) \/ float(h)) + ' ',\n                        str(t_ann['bbox'][2] \/ float(w)) + ' ', \n                        str(t_ann['bbox'][3] \/ float(h))]\n                t_f.write(trans[str(t_ann['category_id'])] + ' ' + str(bbox[0] + bbox[1] + bbox[2] + bbox[3]))\n                t_f.write('\\n')\n\n\n# Create *.txt files for evaluating\nfor (imid, fn, w, h) in val_info:\n    with open('.\/data\/labels\/val\/' + fn + '.txt', 'w') as v_f:\n        for v_ann in val_ann:\n            if v_ann['image_id'] == imid:\n                # convert X_min,Y_min,w,h to X_center\/width,Ycenter\/height,w\/width,h\/height\n                bbox = [str((v_ann['bbox'][0] + (v_ann['bbox'][2] \/ 2) - 1) \/ float(w)) + ' ',\n                        str((v_ann['bbox'][1] + (v_ann['bbox'][3] \/ 2) - 1) \/ float(h)) + ' ',\n                        str(v_ann['bbox'][2] \/ float(w)) + ' ',\n                        str(v_ann['bbox'][3] \/ float(h))]\n                v_f.write(trans[str(v_ann['category_id'])] + ' ' + str(bbox[0] + bbox[1] + bbox[2] + bbox[3]))\n                v_f.write('\\n')","c0f3dc59":"%cat .\/data\/labels\/val\/15eb34a12667056b.txt","8de9d128":"data_yaml = dict(\n    path = '.\/data',  # dataset root dir\n    train = 'images\/train',  # train images (relative to 'path')\n    val = 'images\/val',   # val images (relative to 'path')\n    test = 'images\/test',  # test images (relative to 'path') \u63a8\u7406\u7684\u65f6\u5019\u624d\u7528\u5f97\u5230\n    nc = 5,\n    names = ['belt', 'sunglasses', 'boot', 'cowboy_hat', 'jacket'],\n    download = 'None'\n    )\nwith open('.\/my_data_config.yaml', 'w') as f:\n    yaml.dump(data_yaml, f, default_flow_style=False)","94f81be5":"%cat .\/my_data_config.yaml","ae9d13a4":"hyp_yaml = dict(\n    lr0 = 0.01,  # initial learning rate (SGD=1E-2, Adam=1E-3)\n    lrf = 0.16,  # final OneCycleLR learning rate (lr0 * lrf)\n    momentum = 0.937,  # SGD momentum\/Adam beta1\n    weight_decay = 0.0005,  # optimizer weight decay 5e-4\n    warmup_epochs = 5.0,  # warmup epochs (fractions ok)\n    warmup_momentum = 0.8,  # warmup initial momentum\n    warmup_bias_lr = 0.1,  # warmup initial bias lr\n    box = 0.05,  # box loss gain\n    cls = 0.3,  # cls loss gain\n    cls_pw = 1.0,  # cls BCELoss positive_weight\n    obj = 0.7,  # obj loss gain (scale with pixels)\n    obj_pw = 1.0,  # obj BCELoss positive_weight\n    iou_t = 0.20,  # IoU training threshold\n    anchor_t = 4.0,  # anchor-multiple threshold\n    fl_gamma = 0.0,  # focal loss gamma (efficientDet default gamma=1.5)\n    hsv_h = 0.015,  # image HSV-Hue augmentation (fraction)\n    hsv_s = 0.7,  # image HSV-Saturation augmentation (fraction)\n    hsv_v = 0.4,  # image HSV-Value augmentation (fraction)\n    degrees = 0.0,  # image rotation (+\/- deg)\n    translate = 0.1,  # image translation (+\/- fraction)\n    scale = 0.25,  # image scale (+\/- gain)\n    shear = 0.0,  # image shear (+\/- deg)\n    perspective = 0.0,  # image perspective (+\/- fraction), range 0-0.001\n    flipud = 0.0,  # image flip up-down (probability)\n    fliplr = 0.5,  # image flip left-right (probability)\n    mosaic = 1.0,  # image mosaic (probability)\n    mixup = 0.0,  # image mixup (probability)\n    copy_paste = 0.0  # segment copy-paste (probability)\n    )\nwith open('.\/my_hyp_config.yaml', 'w') as f:\n    yaml.dump(hyp_yaml, f, default_flow_style=False)","7fa39d5d":"%cat .\/my_hyp_config.yaml","a88e5276":"model1_yaml = dict(\n    nc = 5,  # number of classes\n    depth_multiple = 1.33,  # model depth multiple\n    width_multiple = 1.25,  # layer channel multiple\n    anchors = 3,  # \u8fd9\u91cc\u628a\u9ed8\u8ba4\u7684anchors\u914d\u7f6e\u6539\u6210\u4e863\u4ee5\u542f\u7528autoanchor, \u83b7\u53d6\u9488\u5bf9\u81ea\u5df1\u8bad\u7ec3\u65f6\u7684img_size\u7684\u66f4\u4f18\u8d28\u7684anchor size\n    \n    # YOLOv5 backbone\n    backbone =\n      # [from, number, module, args]\n      [[-1, 1, 'Focus', [64, 3]],  # 0-P1\/2\n       [-1, 1, 'Conv', [128, 3, 2]],  # 1-P2\/4\n       [-1, 3, 'C3', [128]],\n       [-1, 1, 'Conv', [256, 3, 2]],  # 3-P3\/8\n       [-1, 9, 'C3', [256]],\n       [-1, 1, 'Conv', [512, 3, 2]],  # 5-P4\/16\n       [-1, 9, 'C3', [512]],\n       [-1, 1, 'Conv', [768, 3, 2]],  # 7-P5\/32\n       [-1, 3, 'C3', [768]],\n       [-1, 1, 'Conv', [1024, 3, 2]],  # 9-P6\/64\n       [-1, 1, 'SPP', [1024, [3, 5, 7]]],\n       [-1, 3, 'C3', [1024, 'False']],  # 11\n      ],\n\n    # YOLOv5 head\n    head =\n      [[-1, 1, 'Conv', [768, 1, 1]],\n       [-1, 1, 'nn.Upsample', ['None', 2, 'nearest']],\n       [[-1, 8], 1, 'Concat', [1]],  # cat backbone P5\n       [-1, 3, 'C3', [768, 'False']],  # 15\n\n       [-1, 1, 'Conv', [512, 1, 1]],\n       [-1, 1, 'nn.Upsample', ['None', 2, 'nearest']],\n       [[-1, 6], 1, 'Concat', [1]],  # cat backbone P4\n       [-1, 3, 'C3', [512, 'False']],  # 19\n\n       [-1, 1, 'Conv', [256, 1, 1]],\n       [-1, 1, 'nn.Upsample', ['None', 2, 'nearest']],\n       [[-1, 4], 1, 'Concat', [1]],  # cat backbone P3\n       [-1, 3, 'C3', [256, 'False']],  # 23 (P3\/8-small)\n\n       [-1, 1, 'Conv', [256, 3, 2]],\n       [[-1, 20], 1, 'Concat', [1]],  # cat head P4\n       [-1, 3, 'C3', [512, 'False']],  # 26 (P4\/16-medium)\n\n       [-1, 1, 'Conv', [512, 3, 2]],\n       [[-1, 16], 1, 'Concat', [1]],  # cat head P5\n       [-1, 3, 'C3', [768, 'False']],  # 29 (P5\/32-large)\n\n       [-1, 1, 'Conv', [768, 3, 2]],\n       [[-1, 12], 1, 'Concat', [1]],  # cat head P6\n       [-1, 3, 'C3', [1024, 'False']],  # 32 (P6\/64-xlarge)\n\n       [[23, 26, 29, 32], 1, 'Detect', ['nc', 'anchors']],  # Detect(P3, P4, P5, P6)\n      ]\n    )\nwith open('.\/yolov5x6.yaml', 'w') as f:\n    yaml.dump(model1_yaml, f, default_flow_style=True)","80b25d41":"model2_yaml = dict(\n    nc = 5,  # number of classes\n    depth_multiple = 1.33,  # model depth multiple\n    width_multiple = 1.25,  # layer channel multiple\n    anchors = 3,  # \u8fd9\u91cc\u628a\u9ed8\u8ba4\u7684anchors\u914d\u7f6e\u6539\u6210\u4e863\u4ee5\u542f\u7528autoanchor, \u83b7\u53d6\u9488\u5bf9\u81ea\u5df1\u8bad\u7ec3\u65f6\u7684img_size\u7684\u66f4\u4f18\u8d28\u7684anchor size\n    \n    # YOLOv5 backbone\n    backbone =\n      # [from, number, module, args]\n      [[-1, 1, 'Focus', [64, 3]],  # 0-P1\/2\n       [-1, 1, 'Conv', [128, 3, 2]],  # 1-P2\/4\n       [-1, 3, 'C3', [128]],\n       [-1, 1, 'Conv', [256, 3, 2]],  # 3-P3\/8\n       [-1, 9, 'C3', [256]],\n       [-1, 1, 'Conv', [512, 3, 2]],  # 5-P4\/16\n       [-1, 9, 'C3', [512]],\n       [-1, 1, 'Conv', [768, 3, 2]],  # 7-P5\/32\n       [-1, 3, 'C3', [768]],\n       [-1, 1, 'Conv', [1024, 3, 2]],  # 9-P6\/64\n       [-1, 1, 'SPP', [1024, [3, 5, 7]]],\n       [-1, 3, 'C3TR', [1024, 'False']],  # 11  <-------- C3TR() Transformer module\n      ],\n\n    # YOLOv5 head\n    head =\n      [[-1, 1, 'Conv', [768, 1, 1]],\n       [-1, 1, 'nn.Upsample', ['None', 2, 'nearest']],\n       [[-1, 8], 1, 'Concat', [1]],  # cat backbone P5\n       [-1, 3, 'C3', [768, 'False']],  # 15\n\n       [-1, 1, 'Conv', [512, 1, 1]],\n       [-1, 1, 'nn.Upsample', ['None', 2, 'nearest']],\n       [[-1, 6], 1, 'Concat', [1]],  # cat backbone P4\n       [-1, 3, 'C3', [512, 'False']],  # 19\n\n       [-1, 1, 'Conv', [256, 1, 1]],\n       [-1, 1, 'nn.Upsample', ['None', 2, 'nearest']],\n       [[-1, 4], 1, 'Concat', [1]],  # cat backbone P3\n       [-1, 3, 'C3', [256, 'False']],  # 23 (P3\/8-small)\n\n       [-1, 1, 'Conv', [256, 3, 2]],\n       [[-1, 20], 1, 'Concat', [1]],  # cat head P4\n       [-1, 3, 'C3', [512, 'False']],  # 26 (P4\/16-medium)\n\n       [-1, 1, 'Conv', [512, 3, 2]],\n       [[-1, 16], 1, 'Concat', [1]],  # cat head P5\n       [-1, 3, 'C3', [768, 'False']],  # 29 (P5\/32-large)\n\n       [-1, 1, 'Conv', [768, 3, 2]],\n       [[-1, 12], 1, 'Concat', [1]],  # cat head P6\n       [-1, 3, 'C3', [1024, 'False']],  # 32 (P6\/64-xlarge)\n\n       [[23, 26, 29, 32], 1, 'Detect', ['nc', 'anchors']],  # Detect(P3, P4, P5, P6)\n      ]\n    )\nwith open('.\/yolov5x6-transformer.yaml', 'w') as f:\n    yaml.dump(model2_yaml, f, default_flow_style=True)","ada3451e":"model3_yaml = dict(\n    nc = 5,  # number of classes\n    depth_multiple = 1.0,  # model depth multiple\n    width_multiple = 1.0,  # layer channel multiple\n    anchors = 3,  # \u8fd9\u91cc\u628a\u9ed8\u8ba4\u7684anchors\u914d\u7f6e\u6539\u6210\u4e863\u4ee5\u542f\u7528autoanchor, \u83b7\u53d6\u9488\u5bf9\u81ea\u5df1\u8bad\u7ec3\u65f6\u7684img_size\u7684\u66f4\u4f18\u8d28\u7684anchor size\n    \n    # YOLOv5 backbone\n    backbone =\n      # [from, number, module, args]\n      [[-1, 1, 'Focus', [64, 3]],  # 0-P1\/2\n       [-1, 1, 'Conv', [128, 3, 2]],  # 1-P2\/4\n       [-1, 3, 'C3', [128]],\n       [-1, 1, 'Conv', [256, 3, 2]],  # 3-P3\/8\n       [-1, 9, 'C3', [256]],\n       [-1, 1, 'Conv', [512, 3, 2]],  # 5-P4\/16\n       [-1, 9, 'C3TR', [512]],  #   <-------- C3TR() Transformer module\n       [-1, 1, 'Conv', [768, 3, 2]],  # 7-P5\/32\n       [-1, 3, 'C3', [768]],\n       [-1, 1, 'Conv', [1024, 3, 2]],  # 9-P6\/64\n       [-1, 1, 'SPP', [1024, [3, 5, 7]]],\n       [-1, 3, 'C3TR', [1024, 'False']],  # 11  <-------- C3TR() Transformer module\n      ],\n\n    # YOLOv5 head\n    head =\n      [[-1, 1, 'Conv', [768, 1, 1]],\n       [-1, 1, 'nn.Upsample', ['None', 2, 'nearest']],\n       [[-1, 8], 1, 'Concat', [1]],  # cat backbone P5\n       [-1, 3, 'C3', [768, 'False']],  # 15\n\n       [-1, 1, 'Conv', [512, 1, 1]],\n       [-1, 1, 'nn.Upsample', ['None', 2, 'nearest']],\n       [[-1, 6], 1, 'Concat', [1]],  # cat backbone P4\n       [-1, 3, 'C3', [512, 'False']],  # 19\n\n       [-1, 1, 'Conv', [256, 1, 1]],\n       [-1, 1, 'nn.Upsample', ['None', 2, 'nearest']],\n       [[-1, 4], 1, 'Concat', [1]],  # cat backbone P3\n       [-1, 3, 'C3', [256, 'False']],  # 23 (P3\/8-small)\n\n       [-1, 1, 'Conv', [256, 3, 2]],\n       [[-1, 20], 1, 'Concat', [1]],  # cat head P4\n       [-1, 3, 'C3', [512, 'False']],  # 26 (P4\/16-medium)\n\n       [-1, 1, 'Conv', [512, 3, 2]],\n       [[-1, 16], 1, 'Concat', [1]],  # cat head P5\n       [-1, 3, 'C3', [768, 'False']],  # 29 (P5\/32-large)\n\n       [-1, 1, 'Conv', [768, 3, 2]],\n       [[-1, 12], 1, 'Concat', [1]],  # cat head P6\n       [-1, 3, 'C3', [1024, 'False']],  # 32 (P6\/64-xlarge)\n\n       [[23, 26, 29, 32], 1, 'Detect', ['nc', 'anchors']],  # Detect(P3, P4, P5, P6)\n      ],\n    )\nwith open('.\/yolov5l6-transformer.yaml', 'w') as f:\n    yaml.dump(model3_yaml, f, default_flow_style=True)","f10066d7":"!python .\/yolov5\/train.py \\\n          --data .\/my_data_config.yaml \\\n          --cfg .\/yolov5x6.yaml \\\n          --hyp .\/my_hyp_config.yaml \\\n          --cache --exist-ok --multi-scale \\\n          --project .\/Cow_Boy_Outfits_Detection --name yolov5x6 \\\n          --img-size 1088 --batch-size 2 --epochs 1 --workers 2 --weights yolov5x6.pt  #  \u5b9e\u9645train\u4e86100\u4e2aepoch","695c1b1f":"!python .\/yolov5\/train.py \\\n          --data .\/my_data_config.yaml \\\n          --cfg .\/yolov5x6-transformer.yaml \\\n          --hyp .\/my_hyp_config.yaml \\\n          --cache --exist-ok --multi-scale \\\n          --project .\/Cow_Boy_Outfits_Detection --name yolov5x6-transformer \\\n          --img-size 832 --batch-size 4 --epochs 1 --workers 2 --weights yolov5x6.pt  #  \u5b9e\u9645train\u4e8680\u4e2aepoch","dfa24126":"!python .\/yolov5\/train.py \\\n          --data .\/my_data_config.yaml \\\n          --cfg .\/yolov5l6-transformer.yaml \\\n          --hyp .\/my_hyp_config.yaml \\\n          --cache --exist-ok --multi-scale \\\n          --project .\/Cow_Boy_Outfits_Detection --name yolov5l6-transformer \\\n          --img-size 640 --batch-size 2 --epochs 1 --workers 2 --weights yolov5l6.pt  #  \u5b9e\u9645train\u4e86100\u4e2aepoch","7a85b467":"os.makedirs('.\/data\/images\/test', exist_ok=True)\ntest_data = pd.read_csv('..\/input\/cowboyoutfits\/test.csv')\nfor each in test_data['file_name']:\n    shutil.copy(f'..\/input\/cowboyoutfits\/images\/{each}', '.\/data\/images\/test')","8ede5a58":"!python .\/yolov5\/val.py --weights \\\n         .\/Cow_Boy_Outfits_Detection\/yolov5x6\/weights\/best.pt \\\n         .\/Cow_Boy_Outfits_Detection\/yolov5x6-transformer\/weights\/best.pt \\\n         .\/Cow_Boy_Outfits_Detection\/yolov5l6-transformer\/weights\/best.pt \\\n         --data .\/my_data_config.yaml \\\n         --project .\/Cow_Boy_Outfits_Detection --name test_960_0.45_0.001 \\\n         --task test --augment --save-json --exist-ok \\\n         --imgsz 960 --batch-size 8 --iou-thres 0.45 --conf 0.001  # \u9009\u53d6\u6298\u4e2d\u4e00\u70b9\u7684imgsz","834dfc42":"import zipfile\nsubmission = json.load(open('.\/Cow_Boy_Outfits_Detection\/test_960_0.45_0.001\/best_predictions.json', 'r'))\ntrans_imid = {f\"{i.split('.')[0]}\": j for i, j in zip(test_data['file_name'], test_data['id'])}\ntrans_cid = {k: v for (v, k) in trans.items()}\nfor each in submission:\n    each['image_id'] = trans_imid[f\"{each['image_id']}\"]\n    each['category_id'] = trans_cid[f\"{each['category_id']}\"]\n\nwith open('.\/Cow_Boy_Outfits_Detection\/answer.json', 'w') as f:\n    json.dump(submission, f)\n    \n# Get answer.zip file\nzf = zipfile.ZipFile('.\/Cow_Boy_Outfits_Detection\/answer.zip', 'w')\nzf.write('.\/Cow_Boy_Outfits_Detection\/answer.json', 'answer.json')\nzf.close()","e28125c6":"![x6.png](https:\/\/i.loli.net\/2021\/08\/09\/bAse4onhl2U3WJH.png)","67b59e8b":"![val_batch2_pred.jpg](https:\/\/i.loli.net\/2021\/08\/09\/bNtZyCpEYBuiVGQ.jpg)","d575f754":"# Get submission","088a58f3":"> \u524d\u8a00\uff1a\u8bad\u7ec3\u73af\u5883\u662fcolab pro,\u6570\u636e\u5904\u7406\u90e8\u5206\u662f\u5728\u672c\u5730\u5b8c\u6210\u7684\u3002\u6700\u5f00\u59cb\u7684\u65f6\u5019\u6298\u817e\u4e86\u597d\u4e45mmdetection\uff0c\u5948\u4f55\u6c34\u5e73\u6709\u9650\uff0cdebug\u4e86\u8bb8\u4e45\u6d6a\u8d39\u4e86\u8bb8\u591a\u5b9d\u8d35\u7684\u65f6\u95f4\u4ecd\u4e0d\u80fd\u6b63\u5e38\u8bad\u7ec3\u8bc4\u4f30TnT\uff0c\u9042\u653e\u5f03\uff0c\u8f6c\u53bb\u7814\u7a76YOLOv5\u3002\u6700\u540e\u53ea\u5269\u4e00\u4e2a\u661f\u671f\u5de6\u53f3\u8c03\u6574\u3001\u8bad\u7ec3\u6a21\u578b\uff08final phase\u7684\u7ed3\u679c\u8fd8\u662f\u57287\u53f7\u4e0a\u5348\u624d\u8dd1\u51fa\u6765\u7684-\u3002-\uff09\u3002\u6240\u4ee5\u8d85\u53c2\u6570\u53ea\u662f\u7c97\u8c03\u4e86\u4e00\u4e0b\uff0c\u8bad\u7ec3\u7684epoch\u4e5f\u4e0d\u5145\u5206\uff0c\u4e0d\u8fc7\u8fd8\u662f\u5f97\u5230\u4e00\u4e2a\u8fd8\u4e0d\u9519\u7684\u7ed3\u679c\uff0c\u4f9b\u5927\u5bb6\u53c2\u8003\uff0c\u5e0c\u671b\u5404\u4f4d\u4e0d\u541d\u8d50\u6559\u3002\n","f5fee15c":"\n   MMdetection GitHub\u94fe\u63a5\uff1a[https:\/\/github.com\/open-mmlab\/mmdetection](http:\/\/)\n   \n   YOLOv5 GitHub\u94fe\u63a5\uff1a[https:\/\/github.com\/ultralytics\/yolov5](http:\/\/)","51276acf":"3. yolov5l6-transformer","89ea6615":"1. yolov5x6","cbefe320":"# Data preprocessing","9c946a91":"**Make dirs and generate yolo-type data structure**\n\nfor more information: [https:\/\/github.com\/ultralytics\/yolov5\/wiki\/Train-Custom-Data](http:\/\/)","f679013a":"# Inference","bcd39301":"**Thank you!! XD**","3bef3746":"**data config**","c771711e":"**Split train set&val set**\n\nPS: \u6838\u5fc3\u601d\u60f3\u662f\u5c06\u6807\u6ce8\u6846\u5206\u7c7b\u62bd\u6837\uff0c\u603b\u5171\u62bd50\u5f20\u56fe\u7247\uff0c\u4f46\u662f\u7531\u4e8e\u5404\u7c7b\u6570\u91cf\u5dee\u8ddd\u8fc7\u5927\uff0c\u6240\u4ee5\u5e76\u6ca1\u6709\u4e25\u683c\u6309\u7167\u67d0\u4e2a\u6bd4\u4f8b\uff0c\u8fd9\u91cc\u53ea\u9009\u4e86\u51e0\u4e2a\u5927\u6982\u7684\u6570\u5b57\uff0c**\u5206\u522b\u8868\u793aval set\u4e2d\u8be5\u7c7b\u7684\u6700\u5c11\u6570\u91cf**","c080b30c":"Install wandb to automatically track and visualize YOLOv5 \ud83d\ude80 runs (RECOMMENDED) ","24f78dc2":"Show the counts of each kind","dd7a45c1":"2. yolov5x6-transformer\n\nPS: \u6211\u53ea\u52a0\u4e86\u4e00\u4e2atransformer\u6a21\u5757, \u52a0\u591a\u4e86\u4f3c\u4e4eOOM\u4e86\uff0c\u914d\u7f6e\u597d\u7684\u670b\u53cb\u53ef\u4ee5\u8bd5\u8bd5\u591a\u52a0\u51e0\u4e2a","71614ef8":"# Code for training","c351f2e9":"# Training config\n* \u4e00\u5171\u8bad\u7ec3\u4e863\u4e2a\u6a21\u578b\uff0c\u6700\u540e\u63a8\u7406\u7684\u65f6\u5019\u4f5censemble\n* data config\u548chyp config\u90fd\u662f\u4e00\u6837\u7684\uff0c\u4e0d\u540c\u7684\u662f\u5177\u4f53\u7684model config","d4e52906":"Models:\n* Yolov5x6\n* Yolov5x6-transformer(1 transformer module)\n* Yolov5l6-transformer(2 transformer modules)","1d7f586a":"**hyp config**","40556a08":"**Model configs**","11e5671a":"![l6t.png](https:\/\/i.loli.net\/2021\/08\/09\/ueOJlQfmtnAiaT8.png)","e19964f7":"![x6t.png](https:\/\/i.loli.net\/2021\/08\/09\/bIafdFBKJmT9GCZ.png)","91de1026":"# Requirements"}}