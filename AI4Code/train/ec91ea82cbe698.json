{"cell_type":{"231db1ea":"code","accdc758":"code","5c1b2640":"code","fc13cfc8":"code","5baeaafd":"code","7cb745b8":"code","f92bfdf2":"code","e77db60e":"code","2a15b0d2":"code","0fe78584":"code","21b5d6b2":"code","0d93ded7":"code","21eda2b6":"code","ead72a42":"code","3209d032":"code","4450a33b":"markdown","bd3d1e67":"markdown"},"source":{"231db1ea":"# Import dependencies\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Magics\n%matplotlib inline\n%load_ext autoreload\n%autoreload 2\n%config InlineBackend.figure_format = 'retina'","accdc758":"# Read in data\ndata_path = '..\/input\/google-stock-data\/GOOG.csv'\nGOOG = pd.read_csv(data_path)","5c1b2640":"# Take a look at the data\nGOOG.head()","fc13cfc8":"# Take a look at a plot for the first few days\nGOOG[:10].plot(x='Date', y='Close')","5baeaafd":"# Drop our target column from the dataset\ndata = GOOG.drop(['Date'], axis=1)\ndata.head()\n\n# Drop other unwanted columns (too similar or otherwise)\ndata = data.drop(['Adj Close', 'Open'], axis=1)","7cb745b8":"# Standardize the variables so that training is easier (mean=0, std=1)\nq_features = ['High', 'Low', 'Close', 'Volume']\n\n# Create a dictionary to be able to convert back\nscaled_features = {}\nfor feat in q_features:\n    mean, std = data[feat].mean(), data[feat].std()\n    scaled_features[feat] = [mean, std]\n    data.loc[:, feat] = (data[feat] - mean)\/std\n    \ndata.head()","f92bfdf2":"# Create a test dataset for a year's worth of stock prices \ntest_data = data[-252:]\n\n# Take out the test dataset from the main dataset \ndata = data[:-252]\n\n# Separate the data into features and targets\ntarget_field = ['Close']\nfeatures, target = data.drop(target_field, axis=1), data[target_field]\ntest_features, test_target = test_data.drop(target_field, axis=1), test_data[target_field]\n\n# Create a validation and training set\ntrain_features, train_target = features[:-100], target[:-100] # Training dataset\nval_features, val_target = features[-100:], target[-100:] # Validation dataset","e77db60e":"# Take a look at the shapes for the datasets\nprint(data.shape)\nprint(test_data.shape)","2a15b0d2":"# Take a look at the basic structure of the network we will use\nfrom IPython.display import Image\nImage(filename=\"..\/input\/pictures\/BasicNN.png\", width=400, height=400)","0fe78584":"# Build the network\n\nclass Network(object):\n    def __init__(self, input_nodes, hidden_nodes, output_nodes, learning_rate):\n        \n        # Number of nodes within each layer\n        self.input_nodes = input_nodes\n        self.hidden_nodes = hidden_nodes\n        self.output_nodes = output_nodes\n        \n        # Initialize weights\n        self.weights_input_to_hidden = np.random.normal(0.0, self.input_nodes**-0.5, \n                                       (self.input_nodes, self.hidden_nodes))\n\n        self.weights_hidden_to_output = np.random.normal(0.0, self.hidden_nodes**-0.5, \n                                       (self.hidden_nodes, self.output_nodes))\n        self.lr = learning_rate\n        \n        # Implement sigmoid activation function\n        self.activation_function = lambda x : 1\/(1 + np.exp(-x))\n        \n    # Train network on features and target\n    def train(self, features, target):\n        \n        n_records = features.shape[0]\n        delta_weights_i_h = np.zeros(self.weights_input_to_hidden.shape)\n        delta_weights_h_o = np.zeros(self.weights_hidden_to_output.shape)\n        \n        for X, y in zip(features, target):\n            \n            # Implement forward pass function\n            final_outputs, hidden_outputs = self.forward_pass_train(X)\n            \n            # Implement backprop function\n            delta_weights_i_h, delta_weights_h_o = self.backprop(final_outputs, hidden_outputs, X, y,\n                                                                 delta_weights_i_h, delta_weights_h_o)\n            \n        self.update_weights(delta_weights_i_h, delta_weights_h_o, n_records)\n    \n    # Forward pass function\n    def forward_pass_train(self, X):\n        \n        # Hidden layer inputs and outputs\n        hidden_inputs = np.dot(X, self.weights_input_to_hidden) \n        hidden_outputs = self.activation_function(hidden_inputs)\n        \n        # Output layer inputs and outputs\n        final_inputs = np.dot(hidden_outputs, self.weights_hidden_to_output)\n        final_outputs = final_inputs\n        \n        return final_outputs, hidden_outputs\n    \n    # Backpropagation function\n    def backprop(self, final_outputs, hidden_outputs, X, y,\n                 delta_weights_i_h, delta_weights_h_o):\n        \n        # Output error\n        error = y - final_outputs\n        \n        # Hidden layer contribution to the output error\n        hidden_error = np.dot(error, self.weights_hidden_to_output.T)\n        \n        # Backprop error terms\n        output_error_term = error\n        hidden_error_term = hidden_error * hidden_outputs * (1 - hidden_outputs)\n        \n        # Input to hidden weight step\n        delta_weights_i_h += hidden_error_term * X[:,None]\n        \n        # Hidden to output weight step\n        delta_weights_h_o += output_error_term * hidden_outputs[:,None]\n        \n        return delta_weights_i_h, delta_weights_h_o\n    \n    # Update weights on grad descent step\n    def update_weights(self, delta_weights_i_h, delta_weights_h_o, n_records):\n        \n        # Update with grad descent step\n        self.weights_hidden_to_output += (self.lr * delta_weights_h_o)\/n_records\n        self.weights_input_to_hidden += (self.lr * delta_weights_i_h)\/n_records\n    \n    # Run forward pass through network\n    def run(self, features):\n        \n        # Hidden layer\n        hidden_inputs = np.dot(features, self.weights_input_to_hidden)\n        hidden_outputs = self.activation_function(hidden_inputs)\n        \n        # Output layer\n        final_inputs = np.dot(hidden_outputs, self.weights_hidden_to_output)\n        final_outputs = final_inputs\n        \n        return final_outputs","21b5d6b2":"# Set hyperparameters for training\n\niterations = 600\nlearning_rate = 0.4\nhidden_nodes = 12\noutput_nodes = 1","0d93ded7":"# Define mean squared error function\n\ndef MSE(y, Y):\n    return np.mean((y-Y)**2)","21eda2b6":"# Training the network\n\nimport sys\n\nN_i = train_features.shape[1]\nnetwork = Network(N_i, hidden_nodes, output_nodes, learning_rate)\n\nlosses = {'train':[], 'validation':[]}\nfor ii in range(iterations):\n    \n    # Go through a batch of records from the training dataset\n    batch = np.random.choice(train_features.index, size=128)\n    X, y = train_features.loc[batch].values, train_target.loc[batch]['Close']\n                             \n    network.train(X, y)\n    \n    # Print out training progress\n    train_loss = MSE(network.run(train_features).T, train_target['Close'].values)\n    val_loss = MSE(network.run(val_features).T, val_target['Close'].values)\n    sys.stdout.write(\"\\rProgress: {:2.1f}\".format(100 * ii\/float(iterations)) \\\n                     + \"% ... Training loss: \" + str(train_loss)[:7] \\\n                     + \" ... Validation loss: \" + str(val_loss)[:7])\n    sys.stdout.flush()\n    \n    losses['train'].append(train_loss)\n    losses['validation'].append(val_loss)","ead72a42":"# Plot the training and validation loss\n\nplt.plot(losses['train'], label='Training loss')\nplt.plot(losses['validation'], label='Validation loss')\nplt.legend()\n_ = plt.ylim()","3209d032":"# Plot the model predictions against the actual data\n\nfig, ax = plt.subplots(figsize=(22,10))\n\nmean, std = scaled_features['Close']\npredictions = network.run(test_features).T*std + mean\nax.plot(predictions[0], label='Prediction', color='r')\nax.plot((test_target['Close']*std + mean).values, label='Data', color='b')\nax.set_xlim(right=len(predictions))\nax.legend()\n\n\ndates = pd.to_datetime(GOOG.loc[test_data.index]['Date'])\ndates = dates.apply(lambda d: d.strftime('%b %d'))\nax.set_xticks(np.arange(len(dates)))\n_ = ax.set_xticklabels(dates, rotation=45, fontsize=4, ha='right')","4450a33b":"# Predicting stock prices\n\n### In this notebook, I am attempting to build a model that predicts a year of Google stock prices given some years of data. This model will not use any machine learning specific libraries, but rather a self-defined network. Feel free to look through; any and all feedback is welcome.","bd3d1e67":"### Just by knowing the data from the 'High', 'Low', and 'Volume' columns, the model does a decent job of predicting actual stock prices."}}