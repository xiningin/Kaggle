{"cell_type":{"13dd816d":"code","aeb7f0dc":"code","44c3bcea":"code","50e0e018":"code","1f495429":"code","fa9e0a79":"code","034501b4":"code","1fd9b38e":"code","39a4b177":"code","fde1a4dc":"code","c944ed58":"code","d7748746":"code","3f1b323e":"code","a0c15387":"code","3e199181":"code","dcf905b8":"code","0a5bde97":"code","fce0b821":"markdown","e67a25ce":"markdown"},"source":{"13dd816d":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport string\nimport re\nfrom nltk.stem.porter import PorterStemmer\nimport random\nfrom sklearn.model_selection import train_test_split\n\ndata = pd.read_csv(\"..\/input\/sms-spam-collection-dataset\/spam.csv\", encoding='latin-1')\ndata.head()","aeb7f0dc":"# Drop unwanted columns and rename remaining columns\nif len(data.columns) > 3: \n    data = data.drop(['Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4'], axis = 1)\nif data.columns[0] != 'label':\n    data = data.rename(columns={'v1': 'label', 'v2': 'text'})\ndata.head()","44c3bcea":"# Count observations in each label\ndata.label.value_counts()\n#data.info()","50e0e018":"# Convert label to numerical variable\ndata['label'] = data.label.map({'ham': 0, 'spam': 1})\ndata.head()","1f495429":"# Extract every text \ntexts = []\nfor index, row in data.iterrows():\n    texts.append((row['text'], row['label']))\ntexts[:5]","fa9e0a79":"# * * * PREPROCESSING * * * \n# Remove whitespace and punctutation \ntokenized = []\nfor t in texts:\n    m = t[0]\n    text = re.sub('[' + string.punctuation + ']', ' ', m)\n    text = re.sub('[\\n\\t\\r]', '', text)\n    words = text.split()\n    tokenized.append((words, t[1]))\ntokenized[0] # First element","034501b4":"# Remove stopwords\nstopwords = []\ntry:\n    f = open('..\/input\/stopword-lists-for-19-languages\/englishST.txt', 'r')\n    stopwords = f.read().split('\\n')\nexcept IOError:\n    print('Problem opening file')\nfinally:\n    f.close()\nprint('Sentence before stopwrods removed: \\n', tokenized[51])\nfiltered = []\nfor t in tokenized:\n    text = t[0]\n    f_text = []\n    for word in text:\n        if word not in stopwords and len(word) > 2:\n            f_text.append(word)\n    filtered.append((f_text, t[1]))\n\nprint('\\nSentence after stopwords removed: \\n', filtered[51])","1fd9b38e":"# Stem the words\nstemmer = PorterStemmer()\nstemmed = []\nfor t in filtered:\n    text = t[0]\n    stemmed_text = []\n    for word in text:\n        stemmed_word = stemmer.stem(word.lower())\n        stemmed_text.append(stemmed_word)\n    stemmed.append((stemmed_text, t[1]))\n\nstemmed[51]","39a4b177":"# Counting number of texts each word occurs\nword_count = {}\nfor t in stemmed:\n    text = t[0]\n    already_counted = []\n    for word in text:\n        if word not in word_count:\n            word_count[word] = 1\n        elif word not in already_counted:\n            word_count[word] += 1\n            already_counted.append(word)\n\n#  Removing the words that only occurs once\nfor i in range(len(stemmed)):\n    stemmed[i] = (list(filter(lambda x: word_count[x] > 4, stemmed[i][0])), stemmed[i][1])","fde1a4dc":"# Splitting data in trainingdata and testdata (80-20 ratio)\ntotaltexts = data.label.value_counts()\ntotal = totaltexts[0] + totaltexts[1] # Total number of texts\ntest_number = int(0.20 * total) # Number of testing mails\n# Picking randomly\ntest_set = []\ntaken = {}\nwhile len(test_set) < test_number:\n    #print(len(train_texts))\n    num = random.randint(0, test_number - 1)\n    if num not in taken.keys():\n        test_set.append(stemmed.pop(num))\n        taken[num] = 1\n\ntrain_set = stemmed # Trainset is the remaining texts\n        \n# Total number of hams and spams\nnumber_of_hams = data.label.value_counts()[0]\nnumber_of_spams = data.label.value_counts()[1]\n\nlen(train_set)\/total, len(test_set)\/total","c944ed58":"# * * * TRAINING THE MODEL * * * \n\n# meaning: Computing probabilities needed for P(Spam|Word)\n\n# Need to train these 4 possibilities:\n# 1) Probability that a word appears in spam messages\n# 2) Probability that a word appears in ham messages\n# 3) Overall probability that any given message is spam\n# 4) Overall probability that any given message is not spam (is ham)\n\ndef p_appears_in_spam(word):\n    count = 0\n    total_spams = 0\n    for t in train_set:\n        text = t[0]\n        if t[1] == 1:\n            total_spams += 1\n            if word in text:\n                count += 1\n    return count\/total_spams\n             \n\ndef p_appears_in_ham(word):\n    count = 0\n    total_hams = 0\n    for t in train_set:\n        text = t[0]\n        if t[1] == 0:\n            total_hams += 1\n            if word in text:\n                count += 1\n    return count\/total_hams\n\ndef total_spams_and_hams(tset):\n    spams = 0\n    hams = 0\n    for t in tset:\n        spams += 1 if t[1] == 1 else 0\n        hams += 1 if t[1] == 0 else 0\n    return spams, hams\n\n\np_spam = total_spams_and_hams(train_set)[0]\/len(train_set) # Probability that a message is spam\np_ham = total_spams_and_hams(train_set)[1]\/len(train_set) # Probability that a message is ham\n\n# Finally we can compute P(Spam | Word)\ndef p_is_spam_given_word(word):\n    return (p_appears_in_spam(word)*p_spam)\/((p_appears_in_spam(word)*p_spam + p_appears_in_ham(word)*p_ham))\n\nword = 'free'\nprint('Probability that a message is spam given the word \"{}\" is: {}'.format(word, p_is_spam_given_word(word)))","d7748746":"# Collecting the probabilities in a dictionary\nprobabilities = {}\nfor t in train_set:\n    text = t[0]\n    for word in text:\n        if word not in probabilities:\n            p = p_is_spam_given_word(word)\n            if p == 0:\n                probabilities[word] = 0.2 # To deal with the zero probability problem. Tweaking this value\n            elif p == 1:\n                probabilities[word] = 0.98 # Tweaking this value\n            else:\n                probabilities[word] = p","3f1b323e":"# * * * TESTING THE MODEL * * * \n# Training is done\n# This function will be used to classify new messages, using the trained probabilities \n\nfrom functools import reduce\ndef p_is_spam(words):\n    probs = []\n    for word in words:\n        if word in probabilities:\n            probs.append(probabilities[word])\n        # 'else' is for unseen word, a value to tweak\n        # Assumes it is somewhat higher probability that an unseen word belongs to a ham message than a spam message\n        # as \n        else:\n            probs.append(0.4) \n    probs_not = list(map(lambda prob: 1-prob, probs))\n    product = reduce(lambda x, y: x * y, probs, 1) \n    product_not = reduce(lambda x, y: x * y, probs_not, 1)\n    return product\/(product + product_not)","a0c15387":"total_correct = 0\ntrue_spam_as_spam = 0\ntrue_spam_as_ham = 0\ntrue_ham_as_ham = 0\ntrue_ham_as_spam = 0\n\n# Care most about minimizing false positives, that is: labeling non-spam messages as spam\nfalse_positives = []\n\nfor t in test_set:\n    guess = -1\n    words = t[0]\n    answer = t[1]\n    p_spam = p_is_spam(words)\n    # If p > 0.95, predict 'yes' (is spam)\n    guess = 1 if p_spam > 0.95 else 0\n    if guess == answer:\n        total_correct += 1\n        if answer == 0: # true negative\n            true_ham_as_ham += 1\n        else: # true positive\n            true_spam_as_spam += 1 \n    else:\n        if answer == 0: # false positive\n            true_ham_as_spam += 1\n            false_positives.append((words, p_spam))\n        else: # true negative\n            true_spam_as_ham += 1\n\n            \ntrue_spams = total_spams_and_hams(test_set)[0]\ntrue_hams = total_spams_and_hams(test_set)[1]\n\nprint('Total test texts: ', len(test_set))\nprint('Number of correct: ', total_correct)\nprint('Accuracy: ', total_correct*100\/(true_spams+true_hams))\nprint('-------------------------------')\nprint('Ham precision: ', true_ham_as_ham\/(true_ham_as_ham + true_spam_as_ham))\nprint('Ham recall: ', true_ham_as_ham\/(true_ham_as_ham + true_ham_as_spam))\nprint('Spam precision: ', true_spam_as_spam\/(true_spam_as_spam + true_ham_as_spam)) # Most important \nprint('Spam recall: ', true_spam_as_spam\/(true_spam_as_spam + true_spam_as_ham))\nprint('-------------------------------')\nprint('False Positives (hams that got labeled as spam):')\nfor i, (text, p) in enumerate(false_positives):\n    print('{}: Words in text: {} | Degree of certainty: {}'.format(i+1, text, p))","3e199181":"# * * * VISUALISATIONS * * * \nfrom wordcloud import WordCloud\n\nspam_words = \"\"\nham_words = \"\"\n\nall = train_set + test_set\n\nfor t in all:\n    text = t[0]\n    s = \"\"\n    for word in text:\n        s += word + ' '\n    if t[1] == 0:\n        ham_words += s\n    else:\n        spam_words += s + ' '\n\n# # Generate a word cloud image\nspam_wordcloud = WordCloud(width=600, height=400).generate(spam_words)\nham_wordcloud = WordCloud(width=600, height=400).generate(ham_words)\n","dcf905b8":"#Spam Word cloud\nplt.figure( figsize=(10,8), facecolor='k')\nplt.imshow(spam_wordcloud)\nplt.axis(\"off\")\nplt.tight_layout(pad=0)","0a5bde97":"# Ham Word cloud\nplt.figure( figsize=(10,8), facecolor='k')\nplt.imshow(ham_wordcloud)\nplt.axis(\"off\")\nplt.tight_layout(pad=0)","fce0b821":"### Combining Individual Probabilities\nDetermining whether a message is spam or ham based only on the presence of one word is error-prone, must try to consider all the words (or the most interesting) in the message\n###### Probability that a text is spam: $P(Spam) =  \\frac{p_1p_2...p_n}{p_1p_2...p_n + (1-p_1)(1-p_2)...(1-p_n)} $\n\n$p_1$: The probability $P(S|W_1)$, that it is spam knowing it contains a first word (for example \"free\")","e67a25ce":"###### Probability that a text containing a given word is spam (Bayes' theorem):\n$P(Spam|Word) =  \\frac{P(Word|Spam)P(Spam)}{P(Word|Spam)P(Spam) + P(Word|Ham)P(Ham)} $"}}