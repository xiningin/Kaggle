{"cell_type":{"31fcaf25":"code","64b0103c":"code","83329743":"code","4a53bd3b":"code","f25a54d8":"code","7babd410":"code","659b3556":"code","61c5721c":"code","d4ec136d":"code","254c50ed":"code","8c63bf04":"code","7f036100":"code","c4021b27":"markdown","278ee312":"markdown","dc77abd7":"markdown","14622740":"markdown","9027ef08":"markdown","ec1c10c6":"markdown","688b0814":"markdown","86c66f72":"markdown","cc104f4e":"markdown","238db476":"markdown","d803c829":"markdown","4b12a38c":"markdown","d76f54af":"markdown"},"source":{"31fcaf25":"import numpy as np\nimport pandas as pd\nimport optuna\nfrom optuna.integration import TFKerasPruningCallback\nfrom optuna.trial import TrialState\nfrom optuna.visualization import plot_intermediate_values\nfrom optuna.visualization import plot_optimization_history\nfrom optuna.visualization import plot_param_importances\nfrom optuna.visualization import plot_contour\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.layers import *\nfrom tensorflow.keras import *\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.callbacks import LearningRateScheduler\nfrom tensorflow.keras.optimizers.schedules import ExponentialDecay\nfrom sklearn.preprocessing import RobustScaler, normalize\nfrom sklearn.model_selection import train_test_split\nfrom pickle import load\n!cp ..\/input\/ventilator-feature-engineering\/VFE.py .","64b0103c":"from VFE import add_features\n\ntrain = np.load('..\/input\/ventilator-feature-engineering\/x_train.npy')\ntargets = np.load('..\/input\/ventilator-feature-engineering\/y_train.npy')\n\nBATCH_SIZE = 1024\n\n# test set\ntest_ori = pd.read_csv('..\/input\/ventilator-pressure-prediction\/test.csv')\ntest = add_features(test_ori)\ntest.drop(['id', 'breath_id'], axis=1, inplace=True)\n\nRS = load(open('..\/input\/ventilator-feature-engineering\/RS.pkl', 'rb'))\ntest = RS.transform(test)\ntest = test.reshape(-1, 80, test.shape[-1])","83329743":"X_train, X_test, y_train, y_test = train_test_split(train, targets, test_size=0.59284, random_state=21)\nX_test, X_valid, y_test, y_valid = train_test_split(X_test, y_test, test_size=0.79395, random_state=21)\nX_train.shape, X_test.shape, X_valid.shape","4a53bd3b":"# model creation\ndef create_lstm_model(trial):\n\n    x0 = tf.keras.layers.Input(shape=(train.shape[-2], train.shape[-1]))  \n\n    lstm_layers = 4\n    lstm_units = np.zeros(lstm_layers, dtype=np.int)\n    lstm_units[0] = trial.suggest_int(\"lstm_units_L1\", 768, 1536)\n    lstm = Bidirectional(keras.layers.LSTM(lstm_units[0], return_sequences=True))(x0)\n    for i in range(lstm_layers-1):\n        lstm_units[i+1] = trial.suggest_int(\"lstm_units_L{}\".format(i+2), lstm_units[i]\/\/2, lstm_units[i])\n        lstm = Bidirectional(keras.layers.LSTM(lstm_units[i+1], return_sequences=True))(lstm)    \n    dropout_rate = trial.suggest_float(\"lstm_dropout\", 0.0, 0.3)\n    lstm = Dropout(dropout_rate)(lstm)\n    dense_units = lstm_units[-1]\n    # try different activations\n    activation = trial.suggest_categorical(\"activation\", [\"relu\", \"selu\", \"elu\", \"swish\"])\n    lstm = Dense(dense_units, activation=activation)(lstm)\n    lstm = Dense(1)(lstm)\n\n    model = keras.Model(inputs=x0, outputs=lstm)\n    metrics = [\"mae\"]\n    model.compile(optimizer=\"adam\", loss=\"mae\", metrics=metrics)\n    \n    return model","f25a54d8":"# Function to get hardware strategy\ndef get_hardware_strategy():\n    try:\n        # TPU detection. No parameters necessary if TPU_NAME environment variable is\n        # set: this is always the case on Kaggle.\n        tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n        print('Running on TPU ', tpu.master())\n    except ValueError:\n        tpu = None\n\n    if tpu:\n        tf.config.experimental_connect_to_cluster(tpu)\n        tf.tpu.experimental.initialize_tpu_system(tpu)\n        strategy = tf.distribute.experimental.TPUStrategy(tpu)\n        tf.config.optimizer.set_jit(True)\n    else:\n        # Default distribution strategy in Tensorflow. Works on CPU and single GPU.\n        strategy = tf.distribute.get_strategy()\n\n    return tpu, strategy\n\ntpu, strategy = get_hardware_strategy()","7babd410":"EPOCHS = 30 # number of epocs per trial\n\ndef objective(trial):\n    \n    # Clear clutter from previous session graphs.\n    keras.backend.clear_session()\n    \n    with strategy.scope():\n        # Generate our trial model.\n        model = create_lstm_model(trial)\n\n        # learning rate scheduler\n        scheduler = ExponentialDecay(1e-3, 400*((len(train)*0.8)\/BATCH_SIZE), 1e-5)\n        lr = LearningRateScheduler(scheduler, verbose=0)\n    \n        # Fit the model on the training data.\n        # The TFKerasPruningCallback checks for pruning condition every epoch.\n        model.fit(\n            X_train,\n            y_train,\n            batch_size=BATCH_SIZE,\n            callbacks=[TFKerasPruningCallback(trial, \"val_loss\")],\n            epochs=EPOCHS,\n            validation_data=(X_test, y_test),\n            verbose=1,\n        )\n\n        # Evaluate the model accuracy on the validation set.\n        score = model.evaluate(X_valid, y_valid, verbose=0)\n        return score[1]","659b3556":"study = optuna.create_study(direction=\"minimize\", sampler=optuna.samplers.TPESampler(), pruner=optuna.pruners.HyperbandPruner())\nstudy.optimize(objective, n_trials=100)\npruned_trials = study.get_trials(deepcopy=False, states=[TrialState.PRUNED])\ncomplete_trials = study.get_trials(deepcopy=False, states=[TrialState.COMPLETE])","61c5721c":"plot_optimization_history(study)","d4ec136d":"plot_intermediate_values(study)","254c50ed":"plot_contour(study)","8c63bf04":"plot_param_importances(study)","7f036100":"print(\"Study statistics: \")\nprint(\"  Number of finished trials: \", len(study.trials))\nprint(\"  Number of pruned trials: \", len(pruned_trials))\nprint(\"  Number of complete trials: \", len(complete_trials))\n\nprint(\"Best trial:\")\ntrial = study.best_trial\n\nprint(\"  Value: \", trial.value)\n\nprint(\"  Params: \")\nfor key, value in trial.params.items():\n    print(\"    {}: {}\".format(key, value))","c4021b27":"Parameter contour plots - useful or confusing?","278ee312":"# Run optimization\nThere are different samplers and pruners to choose from, here we go for TPESampler and HyperbandPruner.","dc77abd7":"The parameter importance plot is really interesting:","14622740":"# Dataset creation\nTraining dataset is loaded from the [feature engineering notebook](https:\/\/www.kaggle.com\/mistag\/ventilator-feature-engineering).\nFeature engineering is based on [Ensemble Folds with MEDIAN](https:\/\/www.kaggle.com\/cdeotte\/ensemble-folds-with-median-0-153) by [Chris Deotte](https:\/\/www.kaggle.com\/cdeotte). The optimization is run on a smaller subset of the dataset.","9027ef08":"Finally list the optimized model parameters:","ec1c10c6":"# Result\nNow we can create a few interesting plots with the Optuna builtin visualization functions, starting with optimization history:","688b0814":"There are many hyperparameter optimization frameworks available, and in this notebook we will give [Optuna](https:\/\/optuna.org\/) a spin.","86c66f72":"## Objective function\nHere we define the Optuna objective function. The number of epochs per trial is a balance between execution time per trial and confidence in the result of each trial.","cc104f4e":"# Model building\nThe model below is from  [Ensemble Folds with MEDIAN](https:\/\/www.kaggle.com\/cdeotte\/ensemble-folds-with-median-0-153) by [Chris Deotte](https:\/\/www.kaggle.com\/cdeotte).. Hopefully Optuna will be able to figure out the optimal parameters in the model. All parameters that we want to explore are created with a trial.suggest_() function. ","238db476":"# Summary\nUsing Optuna we found a set of optimal model parameters. Next step is to [test the optimal model](https:\/\/www.kaggle.com\/mistag\/optuna-optimized-base-keras-model).","d803c829":"Finally we split the data into train and test sets. We keep a large holdout set for model evaluation.","4b12a38c":"![logo](https:\/\/optuna.org\/assets\/img\/bg.jpg)","d76f54af":"Visualize the loss curves of the trials:"}}