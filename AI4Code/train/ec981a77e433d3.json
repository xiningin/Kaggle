{"cell_type":{"8dafc234":"code","ebfda7ce":"code","ba64b54a":"code","f0961399":"code","afbefb3f":"code","e5c6ad49":"code","67f8e038":"code","a5e63626":"code","9f6a1d6c":"code","fd1b8974":"code","cada7ed2":"code","2b153725":"code","22f468bb":"code","72d8c0f6":"markdown","5a9ea9e4":"markdown","3344e450":"markdown","7a53ba4a":"markdown"},"source":{"8dafc234":"#Import needed libraries\nimport itertools\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import f1_score\nfrom sklearn.model_selection import train_test_split\nimport lightgbm as lgb\nimport textstat\nfrom scipy import stats\n%matplotlib inline","ebfda7ce":"#Import data\nprint('Importing data...')\ndf_train = pd.read_csv(\"..\/input\/train.csv\")","ba64b54a":"#meta features\ndf_train['length'] = df_train['question_text'].str.len()\n\ndf_train['numbers'] = df_train['question_text'].apply(lambda x: len([s for s in x if s.isdigit()]))\ndf_train['words'] = df_train['question_text'].apply(lambda x: len([s for s in x if s.isalpha()]))\ndf_train['spaces'] = df_train['question_text'].apply(lambda x: len([s for s in x if s.isspace()]))\ndf_train['other_chars'] = df_train['length'] - df_train['numbers'] - df_train['words'] - df_train['spaces']\n\ndf_train['numbers_ratio'] = df_train['numbers'] \/ df_train['length']\ndf_train['words_ratio'] = df_train['words'] \/ df_train['length']\ndf_train['spaces_ratio'] = df_train['spaces'] \/ df_train['length']\ndf_train['other_chars_ratio'] = df_train['other_chars'] \/ df_train['length']","f0961399":"#Sentence readability\nprint('Flesch index..')\ndf_train['flesch'] = df_train['question_text'].apply(lambda x: textstat.flesch_reading_ease(x))\nprint('gunning index..')\ndf_train['gunning'] = df_train['question_text'].apply(lambda x: textstat.gunning_fog(x))\nprint('smog index..')\ndf_train['smog'] = df_train['question_text'].apply(lambda x: textstat.smog_index(x))\nprint('auto index..')\ndf_train['auto'] = df_train['question_text'].apply(lambda x: textstat.automated_readability_index(x))\nprint('coleman index..')\ndf_train['coleman'] = df_train['question_text'].apply(lambda x: textstat.coleman_liau_index(x))\nprint('linear index..')\ndf_train['linsear'] = df_train['question_text'].apply(lambda x: textstat.linsear_write_formula(x))\nprint('dale index..')\ndf_train['dale'] = df_train['question_text'].apply(lambda x: textstat.dale_chall_readability_score(x))","afbefb3f":"#Separate in sincere and insicere dataframes \nsincere_df = df_train[df_train['target']==0]\ninsincere_df = df_train[df_train['target']==1]","e5c6ad49":"#Make sure we have enough samples for statistical significance\nprint('Sentences in sincere dataframe:', sincere_df.shape[0])\nprint('Sentences in insincere dataframe:', insincere_df.shape[0])","67f8e038":"#Perform students t-test\ncheck_columns = ['length', 'numbers', 'words', 'spaces', 'other_chars', 'numbers_ratio', \n                 'words_ratio', 'spaces_ratio','other_chars_ratio', 'flesch', 'gunning', \n                 'smog', 'auto', 'coleman', 'linsear', 'dale']\n\nfor col in check_columns:\n    t2, p2 = stats.ttest_ind(sincere_df[col],insincere_df[col])\n    print('t-stat:', t2, '. p-value:', p2)\n    if p2<0.05: \n        print('For feature', col, 'means are: DIFFERENT')\n    elif p2>=0.05: \n        print('For feature', col, 'means are: SAME')\n","a5e63626":"#Split in train and test\ntrain, valid = train_test_split(df_train, test_size=0.15)","9f6a1d6c":"train_x = train.drop(['qid', 'question_text', 'target', 'words', 'length', \n                      'words_ratio', 'spaces'], axis = 1)\ntrain_y = train['target']\n\nvalid_x = valid.drop(['qid', 'question_text', 'target', 'words', 'length', \n                      'words_ratio', 'spaces'], axis = 1)\nvalid_y = valid['target']","fd1b8974":"#LGB model\nlgb_train = lgb.Dataset(train_x, train_y)\nlgb_valid = lgb.Dataset(valid_x, valid_y)\n\n# Specify hyper-parameters as a dict\nparams = {\n    'task': 'train',\n    'boosting_type': 'gbdt',\n    'objective': 'binary',\n    'metric': 'auc',\n    'num_leaves': 18,\n    'max_depth': 4,\n    'learning_rate': 0.05,\n    #'feature_fraction': 0.95,\n    #'bagging_fraction': 0.8,\n    #'bagging_freq': 5,\n    #'reg_alpha': 0.1,\n    #'reg_lambda': 0.1,\n    'is_unbalance': True,\n    'num_class': 1,\n    #'scale_pos_weight': 3.2,\n    'verbose': 1,\n}\n\nnum_iter = 500\n\n# Train LightGBM model\nprint('Start training...')\ngbm = lgb.train(params,\n                lgb_train,\n                num_boost_round=num_iter,\n                valid_sets= lgb_valid,\n                early_stopping_rounds=40,\n                verbose_eval=20\n                )","cada7ed2":"# Plot Importances\nprint('Plot feature importances...')\nimportances = gbm.feature_importance(importance_type='gain')  # importance_type='split'\nmodel_columns = pd.DataFrame(train_x.columns, columns=['features'])\nfeat_imp = model_columns.copy()\nfeat_imp['importance'] = importances\nfeat_imp = feat_imp.sort_values(by='importance', ascending=False)\nfeat_imp.reset_index(inplace=True)\n\nplt.figure()\nplt.barh(np.arange(feat_imp.shape[0] - 1, -1, -1), feat_imp.importance)\nplt.yticks(np.arange(feat_imp.shape[0] - 1, -1, -1), (feat_imp.features))\nplt.title(\"Feature Importances\")\nplt.ylabel('Feature')\nplt.tight_layout()\nplt.show()","2b153725":"pred_lgb = gbm.predict(valid_x,\n                       num_iteration=40\n                       )","22f468bb":"#Sensitivity analysis\nsteps = np.arange(0.1,0.7, 0.01)\nvalidation_pred = []\nfor i in steps:\n    valid_pred_01_lstm = np.where(pred_lgb > i, 1, 0)\n    valid_pred_01_lstm = [int(item) for item in valid_pred_01_lstm]\n    f1_quora = f1_score(valid_y, valid_pred_01_lstm)\n    validation_pred.append(f1_quora)\n\nplt.figure()\nplt.plot(steps, validation_pred)\nplt.grid()\nplt.show()","72d8c0f6":"## Verdict: \nSeems like meta-features have little explanatory power on their own. Maybe if we put them in an ensemble they could capture some different signal and thus contribute positively to our model but I doubt even that...","5a9ea9e4":"Seems like in most case there is a statistical difference of the means between sincere and insincere questions. I wonder if that will provide any actual value in a Machine Learning model. Let's try. ","3344e450":"## Two sample T-Tests of mean similarity\n\nIn a student's t-test the null hypothesis is that the two means under consideration are statistically equal. If the p-value of the test is less than a significance threshold this null hypothesis is rejected. For example if p-value = 0.002 we can reject the null hypothesis at 5% significance level. \n\nCaution: That DOES not mean that we automatically accept the non-null hypothesis of mean inequality - we only reject the null hypothesis.","7a53ba4a":"## Lets try to cook the text meta-features to find any valuable information. \n\n**Recipe:** \n* First calculate some basic stats like text length, number of letters, numbers and other characters in text in both sincere in insincere questions.  \n* Then  'borrow' some ideas from this amazing kernel https:\/\/www.kaggle.com\/thebrownviking20\/analyzing-quora-for-the-insinceres# to calculate text quality and readability indices. \n* Afterwards, check mean similarites using student's T-Test \n* Finally put all incredients into a LightGBM and cook for a few iterations\n\n**Result: Food not eatable**"}}