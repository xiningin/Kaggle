{"cell_type":{"114ebeab":"code","1bd33573":"code","db7f3b18":"code","56503893":"code","c2d9826a":"code","11906ba6":"code","c896b177":"code","53b1a119":"code","d2955005":"code","9298ffd4":"code","aa279542":"code","74de9c53":"code","3d82c6b8":"code","69094728":"code","01be1d75":"code","d013ad31":"code","bd0f9505":"code","7d3f23ee":"code","fdd27f22":"code","b67c24b3":"code","75f1a68e":"code","32c8c186":"code","b5b86ce4":"code","4ce53fb1":"code","7eb1def8":"code","2aa10216":"code","26c65d8c":"markdown","04c60b10":"markdown","09ad5e70":"markdown","e780b9d4":"markdown","45931bd1":"markdown","7f0ddad2":"markdown","29326deb":"markdown","3162937a":"markdown","9e5034a7":"markdown","f9af0d20":"markdown","26d9a891":"markdown","bc523a8f":"markdown","0f17e32b":"markdown","53030fe0":"markdown","9de0a411":"markdown","ca76b1d3":"markdown","68deb80a":"markdown","8fb5d91c":"markdown","ad6dc088":"markdown","0c3648dd":"markdown","951fc97c":"markdown","64ccbbc5":"markdown"},"source":{"114ebeab":"import numpy as np \nimport pandas as pd \nimport random as rn\nimport re\nimport nltk\nimport os\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n\npd.set_option(\"display.max_rows\", 600)\npd.set_option(\"display.max_columns\", 500)\npd.set_option(\"max_colwidth\", 400)","1bd33573":"rn.seed(a=42)\np = 0.004  # to randomly select n% of the rows\n\ndf_reviews = pd.read_csv('\/kaggle\/input\/steam-reviews\/dataset.csv', \n                         skiprows=lambda i: i>0 and rn.random() > p)\n\n# size of dataframe\nprint(df_reviews.shape)\n# display the head of data\ndisplay(df_reviews.head())","db7f3b18":"# convert review text to string\ndf_reviews[\"review_text\"] = df_reviews[\"review_text\"].astype(str)\ndf_reviews[\"review_votes\"] = df_reviews[\"review_votes\"].astype(str)\ndf_reviews.review_text = df_reviews.review_text.apply(lambda s: s.strip())\n\n# drop the reviews with null score\ndf_reviews_2 = df_reviews[df_reviews[\"review_score\"].notnull()]\n\n# change the scores from 1, -1 to 1 and 0\ndf_reviews_2[\"review_score\"] = \\\nnp.where(df_reviews_2[\"review_score\"]==-1, 0, df_reviews_2[\"review_score\"])","56503893":"## Let's remove the \"Early Access Review\" comments. \n# These are the reviews with no comments writen by a human\/reviewer. \ndf_reviews_2 = df_reviews_2[df_reviews_2.review_text != \"Early Access Review\"]\ndf_reviews_2 = df_reviews_2[~df_reviews_2.review_text.isin(['nan'])]\nprint(df_reviews_2.shape)\n\n# Drop duplicates if there is any\ndf_reviews_2.drop_duplicates(['review_text', 'review_score'], inplace = True)\nprint(df_reviews_2.shape)","c2d9826a":"## Text Cleaning\ndef replace_hearts_with_PAD(text):\n    return re.sub(r\"[\u2665]+\", ' **** ' ,text)\ndf_reviews_2['review_text_clean'] = df_reviews_2.review_text.apply(replace_hearts_with_PAD)","11906ba6":"neg_reviews = df_reviews_2[df_reviews_2.review_score == 0]\nneg_reviews = neg_reviews.sample(n=2000, random_state = 1234)\nall_intents = neg_reviews.review_text_clean.tolist()\n\nprint(neg_reviews.shape)\ndisplay(neg_reviews.head())","c896b177":"all_sents = []\nfor intent in all_intents:\n    for sent in nltk.sent_tokenize(intent):\n        if len(sent.split()) > 4:\n            all_sents.append(sent)\nprint(len(all_sents))            \nall_intents = all_sents","53b1a119":"!pip install -U -q sentence-transformers","d2955005":"# for Sentence Transformer models\nfrom sentence_transformers import SentenceTransformer\nimport tensorflow as tf","9298ffd4":"# To limit allocating the whole GPU memory by Pytorch run this cell. \n# Source: https:\/\/www.tensorflow.org\/guide\/gpu\n\ngpus = tf.config.list_physical_devices('GPU')\nif gpus:\n    try:\n        # Currently, memory growth needs to be the same across GPUs\n        for gpu in gpus:\n            tf.config.experimental.set_memory_growth(gpu, True)\n        logical_gpus = tf.config.list_logical_devices('GPU')\n        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n    except RuntimeError as e:\n        # Memory growth must be set before GPUs have been initialized\n        print(e)","aa279542":"def embed(model, model_type, sentences):\n    if model_type == 'use':\n        embeddings = model(sentences)\n    elif model_type == 'sentence transformer':\n        embeddings = model.encode(sentences)\n    \n    return embeddings\n\nmodel_st1 = SentenceTransformer('all-mpnet-base-v2')\n# model_st2 = SentenceTransformer('all-MiniLM-L6-v2')\n# model_st3 = SentenceTransformer('all-distilroberta-v1')\n\nembeddings_st1 = embed(model_st1, 'sentence transformer', all_intents)\n# embeddings_st2 = embed(model_st2, 'sentence transformer', all_intents)\n# embeddings_st3 = embed(model_st3, 'sentence transformer', all_intents)","74de9c53":"! conda install -c conda-forge hdbscan -y -q","3d82c6b8":"import umap # dimensionality reduction\nimport hdbscan # clustering\nfrom functools import partial\n\n# To perform the Bayesian Optimization for searching the optimum hyperparameters, \n# we use hyperopt package:\nfrom hyperopt import hp\nfrom hyperopt import fmin, tpe, hp, STATUS_OK, space_eval, Trials","69094728":"def generate_clusters(message_embeddings,\n                      n_neighbors,\n                      n_components, \n                      min_cluster_size,\n                      min_samples = None,\n                      random_state = None):\n    \"\"\"\n    Returns HDBSCAN objects after first performing dimensionality reduction using UMAP\n    \n    Arguments:\n        message_embeddings: embeddings to use\n        n_neighbors: int, UMAP hyperparameter n_neighbors\n        n_components: int, UMAP hyperparameter n_components\n        min_cluster_size: int, HDBSCAN hyperparameter min_cluster_size\n        min_samples: int, HDBSCAN hyperparameter min_samples\n        random_state: int, random seed\n        \n    Returns:\n        clusters: HDBSCAN object of clusters\n    \"\"\"\n    \n    umap_embeddings = (umap.UMAP(n_neighbors = n_neighbors, \n                                n_components = n_components, \n                                metric = 'cosine', \n                                random_state=random_state)\n                            .fit_transform(message_embeddings))\n\n    clusters = hdbscan.HDBSCAN(min_cluster_size = min_cluster_size, \n                               min_samples = min_samples,\n                               metric='euclidean', \n                               gen_min_span_tree=True,\n                               cluster_selection_method='eom').fit(umap_embeddings)\n    \n    return clusters","01be1d75":"def score_clusters(clusters, prob_threshold = 0.05):\n    \"\"\"\n    Returns the label count and cost of a given clustering\n\n    Arguments:\n        clusters: HDBSCAN clustering object\n        prob_threshold: float, probability threshold to use for deciding\n                        what cluster labels are considered low confidence\n\n    Returns:\n        label_count: int, number of unique cluster labels, including noise\n        cost: float, fraction of data points whose cluster assignment has\n              a probability below cutoff threshold\n    \"\"\"\n    \n    cluster_labels = clusters.labels_\n    label_count = len(np.unique(cluster_labels))\n    total_num = len(clusters.labels_)\n    cost = (np.count_nonzero(clusters.probabilities_ < prob_threshold)\/total_num)\n    \n    return label_count, cost\n\ndef objective(params, embeddings, label_lower, label_upper):\n    \"\"\"\n    Objective function for hyperopt to minimize\n\n    Arguments:\n        params: dict, contains keys for 'n_neighbors', 'n_components',\n               'min_cluster_size', 'random_state' and\n               their values to use for evaluation\n        embeddings: embeddings to use\n        label_lower: int, lower end of range of number of expected clusters\n        label_upper: int, upper end of range of number of expected clusters\n\n    Returns:\n        loss: cost function result incorporating penalties for falling\n              outside desired range for number of clusters\n        label_count: int, number of unique cluster labels, including noise\n        status: string, hypoeropt status\n\n        \"\"\"\n    \n    clusters = generate_clusters(embeddings, \n                                 n_neighbors = params['n_neighbors'], \n                                 n_components = params['n_components'], \n                                 min_cluster_size = params['min_cluster_size'],\n                                 random_state = params['random_state'])\n    \n    label_count, cost = score_clusters(clusters, prob_threshold = 0.05) # 0.05\n    \n    #15% penalty on the cost function if outside the desired range of groups\n    if (label_count < label_lower) | (label_count > label_upper):\n        penalty = 1.0 #0.5 \n    else:\n        penalty = 0\n    \n    loss = cost + penalty\n    \n    return {'loss': loss, 'label_count': label_count, 'status': STATUS_OK}\n\ndef bayesian_search(embeddings, space, label_lower, label_upper, max_evals=100):\n    \"\"\"\n    Perform bayesian search on hyperparameter space using hyperopt\n\n    Arguments:\n        embeddings: embeddings to use\n        space: dict, contains keys for 'n_neighbors', 'n_components',\n               'min_cluster_size', and 'random_state' and\n               values that use built-in hyperopt functions to define\n               search spaces for each\n        label_lower: int, lower end of range of number of expected clusters\n        label_upper: int, upper end of range of number of expected clusters\n        max_evals: int, maximum number of parameter combinations to try\n\n    Saves the following to instance variables:\n        best_params: dict, contains keys for 'n_neighbors', 'n_components',\n               'min_cluster_size', 'min_samples', and 'random_state' and\n               values associated with lowest cost scenario tested\n        best_clusters: HDBSCAN object associated with lowest cost scenario\n                       tested\n        trials: hyperopt trials object for search\n\n        \"\"\"\n    \n    trials = Trials()\n    fmin_objective = partial(objective, \n                             embeddings=embeddings, \n                             label_lower=label_lower,\n                             label_upper=label_upper)\n    \n    best = fmin(fmin_objective, \n                space = space, \n                algo=tpe.suggest,\n                max_evals=max_evals, \n                trials=trials)\n\n    best_params = space_eval(space, best)\n    print ('best:')\n    print (best_params)\n    print (f\"label count: {trials.best_trial['result']['label_count']}\")\n    \n    best_clusters = generate_clusters(embeddings, \n                                      n_neighbors = best_params['n_neighbors'], \n                                      n_components = best_params['n_components'], \n                                      min_cluster_size = best_params['min_cluster_size'],\n                                      random_state = best_params['random_state'])\n    \n    return best_params, best_clusters, trials","d013ad31":"hspace = {\n    \"n_neighbors\": hp.choice('n_neighbors', range(3,32)),\n    \"n_components\": hp.choice('n_components', range(3,32)),\n    \"min_cluster_size\": hp.choice('min_cluster_size', range(2,32)),\n    \"random_state\": 42\n}\n\nlabel_lower = 10\nlabel_upper = 100\nmax_evals = 25 # change it to 50 or 100 for extra steps as you wish.","bd0f9505":"%%time\nbest_params_use, best_clusters_use, trials_use = bayesian_search(embeddings_st1, \n                                                                 space=hspace, \n                                                                 label_lower=label_lower, \n                                                                 label_upper=label_upper, \n                                                                 max_evals=max_evals)","7d3f23ee":"# C_ = 11\n# for index, clust in enumerate(best_clusters_use.labels_):\n#     if clust == C_:\n#         print(all_intents[index])\n#         print()","fdd27f22":"import collections\nimport spacy\nfrom spacy import displacy\nnlp = spacy.load(\"en_core_web_sm\")","b67c24b3":"data_clustered = pd.DataFrame(data = list(zip(all_intents,best_clusters_use.labels_)),\n                             columns = ['text', 'label_st1'])\ndata_clustered.head()","75f1a68e":"# example_category = data_clustered[data_clustered['label_st1']==2].reset_index(drop=True)\n# example_category.head() ","32c8c186":"# from sklearn.feature_extraction.text import TfidfVectorizer\nfrom collections import Counter\n\nsent_with_word_lemma = []\nfor intent in all_intents:\n    doc = nlp(intent)\n    sent_temp = \"\"\n    this_one = False\n    for token in doc:\n        if (token.pos_ in ['VERB', 'NOUN', 'ADJ']) or (token.dep_=='dobj'):\n            sent_temp += token.lemma_.lower() + \" \"\n    sent_with_word_lemma.append(sent_temp)","b5b86ce4":"def compute_IDF(documents):\n    word_count = Counter()\n    for doc in documents:\n        if 'drops(players' in doc:\n            print(doc)\n            print(doc.split())\n        words_set = set(doc.split())\n        word_count.update(words_set)\n    total = sum(word_count.values())\n    return {k: round((np.log2(total \/ v)))  for k, v in word_count.items()} # log2 is the best choice for our work (feel free)\n                                                                            # to try different functions.\n\nword_IDF = compute_IDF(sent_with_word_lemma)","4ce53fb1":"def get_group(df, category_col, category):\n    \"\"\"\n    Returns documents of a single category\n    \n    Arguments:\n        df: pandas dataframe of documents\n        category_col: str, column name corresponding to categories or clusters\n        category: int, cluster number to return\n    Returns:\n        single_category: pandas dataframe with documents from a single category\n    \"\"\"\n    \n    single_category = df[df[category_col]==category].reset_index(drop=True)\n\n    return single_category \n\ndef most_common(lst, n_words):\n    \"\"\"\n    Get most common words in a list of words\n    \n    Arguments:\n        lst: list, each element is a word\n        n_words: number of top common words to return\n    \n    Returns:\n        counter.most_common(n_words): counter object of n most common words\n    \"\"\"\n    counter=collections.Counter(lst)\n    \n    for k in list(counter): \n        if counter[k] ==1: # if appears only once, ignore it.\n            pass \n        else:\n            counter[k] *= word_IDF[k] # if the word appears more than once in the entire cluser, \n                                      # repeat that word \"IDF\" times in our bag. If a word is \n                                      # low-frequent word it has a high IDF values, so, with this\n                                      # technique we give more chance to this word to show up in \n                                      # the list of most common words\n        \n    return counter.most_common(n_words)\n\ndef extract_labels(category_docs, print_word_counts=False):\n    \"\"\"\n    Extract labels from documents in the same cluster by concatenating\n    most common verbs, ojects, and nouns\n\n    Argument:\n        category_docs: list of documents, all from the same category or\n                       clustering\n        print_word_counts: bool, True will print word counts of each type in this category\n\n    Returns:\n        label: str, group label derived from concatentating most common\n               verb, object, and two most common nouns\n\n    \"\"\"\n\n    verbs = []\n    dobjs = []\n    nouns = []\n    adjs = []\n    \n    verb = ''\n    dobj = ''\n    noun1 = ''\n    noun2 = ''\n\n    # for each document, append verbs, dobs, nouns, and adjectives to \n    # running lists for whole cluster\n    for i in range(len(category_docs)):\n        doc = nlp(category_docs[i])\n        for token in doc:\n            if (token.is_stop==False) and (len(str(token).strip()) > 0): \n                # ignore if it is a stop word or the length of stripped token is less than 1!\n                if token.pos_ == 'VERB':\n                    verbs.extend([token.lemma_.lower()]) \n\n                elif token.dep_=='dobj':\n                    dobjs.extend([token.lemma_.lower()]) \n\n                elif token.pos_=='NOUN':\n                    nouns.extend([token.lemma_.lower()]) \n                    \n                elif token.pos_=='ADJ':\n                    adjs.extend([token.lemma_.lower()])\n\n    # for printing out for inspection purposes\n    if print_word_counts:\n        for word_lst in [verbs, dobjs, nouns, adjs]:\n            counter=collections.Counter(word_lst)\n            print(counter)\n    \n    # take most common words of each form\n    if len(verbs) > 0:\n        verb = most_common(verbs, 1)[0][0]\n    \n    if len(dobjs) > 0:\n        dobj = most_common(dobjs, 1)[0][0]\n    \n    if len(nouns) > 0:\n        noun1 = most_common(nouns, 1)[0][0]\n    \n    if len(set(nouns)) > 1:\n        noun2 = most_common(nouns, 2)[1][0]\n    \n    # concatenate the most common verb-dobj-noun1-noun2 (if they exist)\n    label_words = [verb, dobj]\n    \n    for word in [noun1, noun2]:\n        if word not in label_words:\n            label_words.append(word)\n    \n    if '' in label_words:\n        label_words.remove('')\n    \n    label = '_'.join(label_words)\n    \n    return label\n\ndef apply_and_summarize_labels(df, category_col):\n    \"\"\"\n    Assign groups to original documents and provide group counts\n\n    Arguments:\n        df: pandas dataframe of original documents of interest to\n            cluster\n        category_col: str, column name corresponding to categories or clusters\n\n    Returns:\n        summary_df: pandas dataframe with model cluster assignment, number\n                    of documents in each cluster and derived labels\n    \"\"\"\n    \n    numerical_labels = df[category_col].unique()\n    \n    # create dictionary of the numerical category to the generated label\n    label_dict = {}\n    for label in numerical_labels:\n        current_category = list(get_group(df, category_col, label)['text'])\n        label_dict[label] = extract_labels(current_category)\n        \n    # create summary dataframe of numerical labels and counts\n    summary_df = (df.groupby(category_col)['text'].count()\n                    .reset_index()\n                    .rename(columns={'text':'count'})\n                    .sort_values('count', ascending=False))\n    \n    # apply generated labels\n    summary_df['label'] = summary_df.apply(lambda x: label_dict[x[category_col]], axis = 1)\n    \n    return summary_df","7eb1def8":"cluster_summary = apply_and_summarize_labels(data_clustered, 'label_st1')\ncluster_summary","2aa10216":"# C_ = 11\n# for index, clust in enumerate(best_clusters_use.labels_):\n#     if clust == C_:\n#         print(all_intents[index])\n#         print()","26c65d8c":"# Automatic cluster labeling","04c60b10":"# Use Bayesian Optimization with Hyperopt\n\nNote that in the \"objective\" function I changed the penalty from 0.15 (used [here](https:\/\/towardsdatascience.com\/clustering-sentence-embeddings-to-identify-intents-in-short-text-48d22d3bf02e)) to 1.0. You may need to update it as you change the settings. For instance, if you use positive comments or read more than or less than 2000 comments, you need to adjust this penalty value. Remember that this is the penalty (or cost) we add to the cost function if the number of clusters go above or beyond our boundaries (here 10 and 100). ","09ad5e70":"Note that the pipeline you see in this notebook is very similar to that of [BERTopic](https:\/\/maartengr.github.io\/BERTopic\/api\/bertopic.html). The difference is that with this pipeline you have extra control on the functions. You can easily modify them as wish (You will see many of those soon). The other key difference is the last step where BERTopic uses tf-IDF to build the cluster labels while, here, we use spaCy to find the word POS and, finally, concatenate the most common verb, direct object, and top two nouns from each cluster. This way the auto-generated topics will be more human readable (One of the most challenging steps of topic modeling). You will see the details at the end of this notebok.","e780b9d4":"## Split reviews into Sentences\n\nnltk sent_tokenize function is used to split the sentences. ","45931bd1":"# Read and Prep Data\nThe original dataset is quite large. I only read a subset of rows for faster run.","7f0ddad2":"# Introduction \/ Motivation\n\nThis idea came into my mind right after completing my [first notebook](https:\/\/www.kaggle.com\/dardodel\/steam-reviews-simpletransformers-classification) about this dataset, which was a classification model with SimpleTransformers. I thought that it would be very interesting to find the topics in the positive, negative or all comments. \n\nFor this study, I focused on only the negative comments which are more interesting to me. What is wrong with this game? Why do people not like it? What can we do to remove throublesome bugs? In simple words, how can we improve the user experience The other reason that I picked just negative (or we could pick just positive) reviews was that the topic clustering quality would be increased when we focus only on one sentiment, specially when NLP\/NLU models still struggle with negation or sarcasm complexity.  \n\nFor the topic modeling steps, I basically follow this fantastic [blog post](https:\/\/towardsdatascience.com\/clustering-sentence-embeddings-to-identify-intents-in-short-text-48d22d3bf02e) and don't talk about the very details as that blog post describes them very well. Thank you David for sharing your great work! I highly recommend to read through that post before dive into my notebook. However, I made small changes to the steps to make this approach more suitable for Steam dataset and improve the quality of topics. Please read the comments inside the code too.","29326deb":"It is recommended to reduce the dimension of sentence embeddings into a smaller dimensions before performing clustering. For more details see [this](https:\/\/towardsdatascience.com\/clustering-sentence-embeddings-to-identify-intents-in-short-text-48d22d3bf02e) or [this](https:\/\/bib.dbvis.de\/uploadedFiles\/155.pdf). For the dimensionality reduction we use [UMAP](https:\/\/umap-learn.readthedocs.io\/en\/latest\/) technique. \n\nHDBSCAN method is used here for clustering which has benefits to the topic modeling. It has some features (like the probability score) that help us to automate the optimum number of clusters which is a big trouble in an unsupervised topic modeling. \n\nLet's install hdbscan package but make sure to install it ONLY in the follwoing way (do NOT pip install it) to avoid any error message and thank me later ;)","3162937a":"## Find the IDF of every word","9e5034a7":"Now the first difference between my work and this [blog post](https:\/\/towardsdatascience.com\/clustering-sentence-embeddings-to-identify-intents-in-short-text-48d22d3bf02e) is here, splitting the comments\/reviews into sentences. Some of the Steam reviews are lengthy. It is not surprising to see multiple topics in one reviews. If we pass the whole lengthy comments through a sentence encoder, we will not see a nice and clean clustering outcome.\n\nIn this way we: \n\n1) Ignore noises and less-important sentences\/phrases in one comment \n\n2) We handle multi topics better in one comment. \n\nAlso, to remove noises, I include only the sentences with more than 4 words (5 and up). You can play with this threshold and explore the impacts. I would not recommend a threshold of 2 or smaller. \n","f9af0d20":"[This blog](https:\/\/towardsdatascience.com\/clustering-sentence-embeddings-to-identify-intents-in-short-text-48d22d3bf02e) found that among the follwoing transformer models, \"all-mpnet-base-v2\" results the best sentence embedding and consequently the best clustering outcome. In this notebook, I use the same model. It is an interesting exploration step to try other models (as well as Google Universal Sentence Encoder) and compare the results. ","26d9a891":"I found that all of the F words and similar words (such as shit, etc.) are replaced by multiple \"\u2665\"s in the original text. This is a source of confusion to the model that we are going to use to get the text embeddings. Heart is a symbol of love while they appear in the negative comments, therefore, it will impact our classifier. I replace them with '****' which seems a token close to the F words. In my [other notebook](https:\/\/www.kaggle.com\/dardodel\/steam-reviews-simpletransformers-classification), I found that this step would improve the embedding accuracy. ","bc523a8f":"In the following cell we define the space for hyperparameters search. Our reference [blog](https:\/\/towardsdatascience.com\/clustering-sentence-embeddings-to-identify-intents-in-short-text-48d22d3bf02e) used different ranges for search. I found that thie following ranges are better choices for Steam Reviews topic modeling. Feel free to experiment different ranges.","0f17e32b":"# Sentence Embedding\n\nInstall sentence-transformers package. We need it to get the sentence embedding vectors.","53030fe0":"word_IDF is a dictionary whose keys are the words values are the IDF scores. ","9de0a411":"USe the following cell to print the sentences of a given cluster (C_)","ca76b1d3":"For the automatic cluster labeling, we use spacy library to get the words POS (part of speech). From each cluster, we concatenate the most common verb, direct object, and top two nouns to build the cluster topic\/label. ","68deb80a":"Let's select 2000 negative samples randomly. Using more data will take much more time and it is out of the scope of this notebook. ","8fb5d91c":"The table above shows the cluster number, the number of samples in each and the suggested, auto-created labels (topics). I know, some of them are still vauge but I believe that, in general, it did a very good job! A domain expertise should step in and read a couple of samples from each cluster and modify the lable if needed. Unfortunately, I am not a gamer! and don't have much of knowledge about the games. Having such knowledge help us, specially, when the reviewer compares this game with other games or talks about the pros and cons of a game. However, I think I can clearly see the topics from this table. \n\nSome clusters talk about the resolution or the graphic of the game, or interestignly one cluster goes to the more details and covers the ftp (frame per second) aspect of it. We see some reviwers compare this game with others, or even recommend other games. Some believe that this is a boring game or even one cluster clearly shows the 4\/10 score. A couple of clusters hover around the pricing or recommending to buy the game on the sale (since these are negative comments and people think this game is only worth to play\/have with some discounts). I don't want to cover all topics here you can print the sentences of each cluster and compare the labels manually. If I want to add, I see few clusters clearly talks about the characters or the story of the game. Please go ahead and check out others yourself. ","ad6dc088":"## Helper Functions\n\nI used all the helper functions from our [reference blog](https:\/\/towardsdatascience.com\/clustering-sentence-embeddings-to-identify-intents-in-short-text-48d22d3bf02e) with slight changes to the \"most_common\" and \"extract_labels\" functions.","0c3648dd":"This is a new step I introduce here (not seen in our [reference blog](https:\/\/towardsdatascience.com\/clustering-sentence-embeddings-to-identify-intents-in-short-text-48d22d3bf02e)). As mentioned above, we pick the most common, verbs, nouns, etc. to build the cluster labels. But sometimes, the most common word\/verb in a cluster is not the best choice. For example, for Steam reviews, we will see the words \"play\" or \"game\" in most of the clusters or comments. To blanace that, I use IDF part of tf-IDF idea. If a word appears in all clusters or all comments it is probably not that important (think of for example stop words ... they are everywhere. Are they that important to be in the cluster lable? No.) In contrast, if a word is not everywhere (in the majority of comments) we pay more attention to it. Later, when we look for the most common words, we give more weight to low-frequent words (and less weight to high-frequent words, which have lower IDF score).\n\nTo get the IDF score we could use \"TfidfVectorizer\" function from sklearn package. But I prefer to define my own function because of two main reasons:\n\n1) TfidfVectorizer uses a special regex to find the words\/tokens. I want to capture everything between two blank spaces, including all symbols or punctuations. \n\n2) IDF is the log((number of documents)\/(number of documents the word appears in). I would like to try different functions other than log, or I would like to try differen log bases, such as 2, 10, or natural log, etc. \n\nSince later we will use the lemmetized and lowered version of words for cluster lebels, I re-generate the reviews with the lemmetized and lowered version of the words that we are interested in (verb, noun, adjectives, etc.). We use this list \"sent_with_word_lemma\" to find the IDF scores next. ","951fc97c":"# Dimensionality Reduction & Clustering","64ccbbc5":"#### Get the labels:"}}