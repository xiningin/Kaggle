{"cell_type":{"c3f5e0b8":"code","b4428c47":"code","212fe561":"code","1e161b53":"code","d280fbd0":"code","caa373c1":"code","b62bb082":"code","c9732fb8":"code","1837b3ab":"markdown","b1f51efa":"markdown","e1ad0f85":"markdown","7ebe5269":"markdown","eaec87b2":"markdown","2976719f":"markdown","84fca363":"markdown","de79276f":"markdown"},"source":{"c3f5e0b8":"import matplotlib.pyplot as plt\nimport numpy as np\n\n\ndef gradient_descent(gradient, start, learn_rate, n_iter=50, tolerance=1e-06):\n    x = np.linspace(-10,10,100)\n    y = x**2\n    fig = plt.figure()\n    ax = fig.add_subplot(1, 1, 1)\n    plt.plot(x,y, 'r')\n    vector = start\n    x1=[vector]\n    y1=[vector**2]\n    for _ in range(n_iter):\n        \n        diff = -learn_rate * gradient(vector)\n        if np.all(np.abs(diff) <= tolerance):\n            break\n        vector += diff\n        \n        x1.append(vector)\n        y1.append(vector**2)\n    \n    for i in range(len(x1)):\n        plt.plot(x1[i], y1[i], marker='o', markersize=3, color=\"blue\")\n    plt.plot(x1,y1)\n    plt.show()\n    return vector\ngradient_descent(  gradient=lambda v: 2 * v, start=20.0, learn_rate=0.9)","b4428c47":"import matplotlib.pyplot as plt\nimport numpy as np\n\n\ndef gradient_descent(gradient, start, learn_rate, n_iter=50, tolerance=1e-06):\n    x = np.linspace(-2.5,2.5,100)\n    y = x**4 - 5 * x**2 - 3 * x\n    fig = plt.figure()\n    ax = fig.add_subplot(1, 1, 1)\n    plt.plot(x,y, 'r')\n    vector = start\n    x1=[vector]\n    y1=[vector**4 - 5 * vector**2 - 3 * vector]\n    for _ in range(n_iter):\n        \n        diff = -learn_rate * gradient(vector)\n        if np.all(np.abs(diff) <= tolerance):\n            break\n        vector += diff\n        \n        x1.append(vector)\n        y1.append(vector**4 - 5 * vector**2 - 3 * vector)\n    \n    for i in range(len(x1)):\n        plt.plot(x1[i], y1[i], marker='o', markersize=3, color=\"blue\")\n    plt.plot(x1,y1)\n    plt.show()\n    return vector\ngradient_descent( gradient=lambda v: 4 * v**3 - 10 * v - 3, start=0, learn_rate=0.2)","212fe561":"def ssr_gradient(x, y, b):\n    res = b[0] + b[1] * x - y\n    return res.mean(), (res * x).mean()","1e161b53":"import numpy as np\ndef gradient_descent(\n    gradient, x, y, start, learn_rate=0.1, n_iter=50, tolerance=1e-06\n):\n    vector = start\n    for _ in range(n_iter):\n        diff = -learn_rate * np.array(gradient(x, y, vector))\n        if np.all(np.abs(diff) <= tolerance):\n            break\n        vector += diff\n    x1 = np.linspace(x[0],x[x.size-1],100)\n    y1 = vector[1]*x1 +vector[0]\n    plt.plot(x1,y1, 'r')\n    return vector\n\nx = np.array([5, 15, 25, 35, 45, 55])\ny = np.array([5, 15, 30, 32, 22, 38])\nfor i in range(x.size):\n    plt.plot(x[i], y[i], marker='o', markersize=3, color=\"blue\")\ngradient_descent(ssr_gradient, x, y, start=[1, 1], learn_rate=0.0007,n_iter=200_000)\n","d280fbd0":"import numpy as np\ndef gradient_descent(\n    gradient, x, y, start, learn_rate=0.1, n_iter=50, tolerance=1e-06,dtype=\"float64\"\n):\n    # Checking if the gradient is callable\n    if not callable(gradient):\n        raise TypeError(\"'gradient' must be callable\")\n\n    # Setting up the data type for NumPy arrays\n    dtype_ = np.dtype(dtype)\n\n    # Converting x and y to NumPy arrays\n    x, y = np.array(x, dtype=dtype_), np.array(y, dtype=dtype_)\n    if x.shape[0] != y.shape[0]:\n        raise ValueError(\"'x' and 'y' lengths do not match\")\n\n    # Initializing the values of the variables\n    vector = np.array(start, dtype=dtype_)\n\n    # Setting up and checking the learning rate\n    learn_rate = np.array(learn_rate, dtype=dtype_)\n    if np.any(learn_rate <= 0):\n        raise ValueError(\"'learn_rate' must be greater than zero\")\n\n    # Setting up and checking the maximal number of iterations\n    n_iter = int(n_iter)\n    if n_iter <= 0:\n        raise ValueError(\"'n_iter' must be greater than zero\")\n\n    # Setting up and checking the tolerance\n    tolerance = np.array(tolerance, dtype=dtype_)\n    if np.any(tolerance <= 0):\n        raise ValueError(\"'tolerance' must be greater than zero\")\n\n    # Performing the gradient descent loop\n    for _ in range(n_iter):\n        diff = -learn_rate * np.array(gradient(x, y, vector))\n        if np.all(np.abs(diff) <= tolerance):\n            break\n        vector += diff\n    x1 = np.linspace(x[0],x[x.size-1],100)\n    y1 = vector[1]*x1 +vector[0]\n    plt.plot(x1,y1, 'r')\n    return vector\n\nx = np.array([5, 15, 25, 35, 45, 55])\ny = np.array([5, 20, 14, 32, 22, 38])\nfor i in range(x.size):\n    plt.plot(x[i], y[i], marker='o', markersize=3, color=\"blue\")\ngradient_descent(ssr_gradient, x, y, start=[1, 1], learn_rate=0.0009,n_iter=200_000)\n","caa373c1":"import numpy as np\ndef sgd(\n    gradient, x, y, start, learn_rate=0.1, batch_size=1, n_iter=50,\n    tolerance=1e-06, dtype=\"float64\", random_state=None\n):\n    # Checking if the gradient is callable\n    if not callable(gradient):\n        raise TypeError(\"'gradient' must be callable\")\n\n    # Setting up the data type for NumPy arrays\n    dtype_ = np.dtype(dtype)\n\n    # Converting x and y to NumPy arrays\n    x, y = np.array(x, dtype=dtype_), np.array(y, dtype=dtype_)\n    n_obs = x.shape[0]\n    if n_obs != y.shape[0]:\n        raise ValueError(\"'x' and 'y' lengths do not match\")\n    xy = np.c_[x.reshape(n_obs, -1), y.reshape(n_obs, 1)]\n    \n    # Initializing the random number generator\n    seed = None if random_state is None else int(random_state)\n    rng = np.random.default_rng(seed=seed)\n\n    # Initializing the values of the variables\n    vector = np.array(start, dtype=dtype_)\n\n    # Setting up and checking the learning rate\n    learn_rate = np.array(learn_rate, dtype=dtype_)\n    if np.any(learn_rate <= 0):\n        raise ValueError(\"'learn_rate' must be greater than zero\")\n\n    # Setting up and checking the size of minibatches\n    batch_size = int(batch_size)\n    if not 0 < batch_size <= n_obs:\n        raise ValueError(\n            \"'batch_size' must be greater than zero and less than \"\n            \"or equal to the number of observations\"\n        )\n    \n    # Setting up and checking the maximal number of iterations\n    n_iter = int(n_iter)\n    if n_iter <= 0:\n        raise ValueError(\"'n_iter' must be greater than zero\")\n\n    # Setting up and checking the tolerance\n    tolerance = np.array(tolerance, dtype=dtype_)\n    if np.any(tolerance <= 0):\n        raise ValueError(\"'tolerance' must be greater than zero\")\n\n    # Performing the gradient descent loop\n    for _ in range(n_iter):\n        # Shuffle x and y\n        rng.shuffle(xy)\n        for start in range(0, n_obs, batch_size):\n            stop = start + batch_size\n            x_batch, y_batch = xy[start:stop, :-1], xy[start:stop, -1:]\n            \n            grad = np.array(gradient(x_batch, y_batch, vector), dtype_)\n            diff = -learn_rate * grad\n            if np.all(np.abs(diff) <= tolerance):\n                break\n        vector += diff\n    x1 = np.linspace(0,55,100)\n    y1 = vector[1]*x1 +vector[0]\n    plt.plot(x1,y1, 'r')\n    return vector\n\nx = np.array([5, 15, 25, 35, 45, 55])\ny = np.array([5, 20, 14, 32, 22, 38])\nfor i in range(x.size):\n    plt.plot(x[i], y[i], marker='o', markersize=3, color=\"blue\")\nsgd(ssr_gradient, x, y, start=[1, 1], learn_rate=0.0008,batch_size=3, n_iter=200_000, random_state=0)\n","b62bb082":"import numpy as np\n\ndef sgd(\n    gradient, x, y, start, learn_rate=0.1, decay_rate=0.0, batch_size=1,\n    n_iter=50, tolerance=1e-06, dtype=\"float64\", random_state=None\n):\n    # Checking if the gradient is callable\n    if not callable(gradient):\n        raise TypeError(\"'gradient' must be callable\")\n\n    # Setting up the data type for NumPy arrays\n    dtype_ = np.dtype(dtype)\n\n    # Converting x and y to NumPy arrays\n    x, y = np.array(x, dtype=dtype_), np.array(y, dtype=dtype_)\n    n_obs = x.shape[0]\n    if n_obs != y.shape[0]:\n        raise ValueError(\"'x' and 'y' lengths do not match\")\n    xy = np.c_[x.reshape(n_obs, -1), y.reshape(n_obs, 1)]\n\n    # Initializing the random number generator\n    seed = None if random_state is None else int(random_state)\n    rng = np.random.default_rng(seed=seed)\n\n    # Initializing the values of the variables\n    vector = np.array(start, dtype=dtype_)\n\n    # Setting up and checking the learning rate\n    learn_rate = np.array(learn_rate, dtype=dtype_)\n    if np.any(learn_rate <= 0):\n        raise ValueError(\"'learn_rate' must be greater than zero\")\n\n    # Setting up and checking the decay rate\n    decay_rate = np.array(decay_rate, dtype=dtype_)\n    if np.any(decay_rate < 0) or np.any(decay_rate > 1):\n        raise ValueError(\"'decay_rate' must be between zero and one\")\n\n    # Setting up and checking the size of minibatches\n    batch_size = int(batch_size)\n    if not 0 < batch_size <= n_obs:\n        raise ValueError(\n            \"'batch_size' must be greater than zero and less than \"\n            \"or equal to the number of observations\"\n        )\n\n    # Setting up and checking the maximal number of iterations\n    n_iter = int(n_iter)\n    if n_iter <= 0:\n        raise ValueError(\"'n_iter' must be greater than zero\")\n\n    # Setting up and checking the tolerance\n    tolerance = np.array(tolerance, dtype=dtype_)\n    if np.any(tolerance <= 0):\n        raise ValueError(\"'tolerance' must be greater than zero\")\n\n    # Setting the difference to zero for the first iteration\n    diff = 0\n\n    # Performing the gradient descent loop\n    for _ in range(n_iter):\n        # Shuffle x and y\n        rng.shuffle(xy)\n\n        # Performing minibatch moves\n        for start in range(0, n_obs, batch_size):\n            stop = start + batch_size\n            x_batch, y_batch = xy[start:stop, :-1], xy[start:stop, -1:]\n\n            # Recalculating the difference\n            grad = np.array(gradient(x_batch, y_batch, vector), dtype_)\n            diff = decay_rate * diff - learn_rate * grad\n\n            # Checking if the absolute difference is small enough\n            if np.all(np.abs(diff) <= tolerance):\n                break\n\n            # Updating the values of the variables\n            vector += diff\n    x1 = np.linspace(0,55,100)\n    y1 = vector[1]*x1 +vector[0]\n    plt.plot(x1,y1, 'r')\n    return vector if vector.shape else vector.item()\n\nx = np.array([5, 15, 25, 35, 45, 55])\ny = np.array([5, 20, 14, 32, 22, 38])\nfor i in range(x.size):\n    plt.plot(x[i], y[i], marker='o', markersize=3, color=\"blue\")\nsgd(ssr_gradient, x, y, start=[1, 1], learn_rate=0.0003,batch_size=3, n_iter=200_000, random_state=0)","c9732fb8":"import numpy as np\n\ndef sgd(\n    gradient, x, y, n_vars=None, start=None, learn_rate=0.1,\n    decay_rate=0.0, batch_size=1, n_iter=50, tolerance=1e-06,\n    dtype=\"float64\", random_state=None\n):\n    # Checking if the gradient is callable\n    if not callable(gradient):\n        raise TypeError(\"'gradient' must be callable\")\n\n    # Setting up the data type for NumPy arrays\n    dtype_ = np.dtype(dtype)\n\n    # Converting x and y to NumPy arrays\n    x, y = np.array(x, dtype=dtype_), np.array(y, dtype=dtype_)\n    n_obs = x.shape[0]\n    if n_obs != y.shape[0]:\n        raise ValueError(\"'x' and 'y' lengths do not match\")\n    xy = np.c_[x.reshape(n_obs, -1), y.reshape(n_obs, 1)]\n\n    # Initializing the random number generator\n    seed = None if random_state is None else int(random_state)\n    rng = np.random.default_rng(seed=seed)\n\n    # Initializing the values of the variables\n    vector = (\n        rng.normal(size=int(n_vars)).astype(dtype_)\n        if start is None else\n        np.array(start, dtype=dtype_)\n    )\n\n    # Setting up and checking the learning rate\n    learn_rate = np.array(learn_rate, dtype=dtype_)\n    if np.any(learn_rate <= 0):\n        raise ValueError(\"'learn_rate' must be greater than zero\")\n\n    # Setting up and checking the decay rate\n    decay_rate = np.array(decay_rate, dtype=dtype_)\n    if np.any(decay_rate < 0) or np.any(decay_rate > 1):\n        raise ValueError(\"'decay_rate' must be between zero and one\")\n\n    # Setting up and checking the size of minibatches\n    batch_size = int(batch_size)\n    if not 0 < batch_size <= n_obs:\n        raise ValueError(\n            \"'batch_size' must be greater than zero and less than \"\n            \"or equal to the number of observations\"\n        )\n\n    # Setting up and checking the maximal number of iterations\n    n_iter = int(n_iter)\n    if n_iter <= 0:\n        raise ValueError(\"'n_iter' must be greater than zero\")\n\n    # Setting up and checking the tolerance\n    tolerance = np.array(tolerance, dtype=dtype_)\n    if np.any(tolerance <= 0):\n        raise ValueError(\"'tolerance' must be greater than zero\")\n\n    # Setting the difference to zero for the first iteration\n    diff = 0\n\n    # Performing the gradient descent loop\n    for _ in range(n_iter):\n        # Shuffle x and y\n        rng.shuffle(xy)\n\n        # Performing minibatch moves\n        for start in range(0, n_obs, batch_size):\n            stop = start + batch_size\n            x_batch, y_batch = xy[start:stop, :-1], xy[start:stop, -1:]\n\n            # Recalculating the difference\n            grad = np.array(gradient(x_batch, y_batch, vector), dtype_)\n            diff = decay_rate * diff - learn_rate * grad\n\n            # Checking if the absolute difference is small enough\n            if np.all(np.abs(diff) <= tolerance):\n                break\n\n            # Updating the values of the variables\n            vector += diff\n    x1 = np.linspace(0,55,100)\n    y1 = vector[1]*x1 +vector[0]\n    plt.plot(x1,y1, 'r')\n    return vector if vector.shape else vector.item()\nx = np.array([5, 15, 25, 35, 45, 55])\ny = np.array([5, 20, 14, 32, 22, 38])\nfor i in range(x.size):\n    plt.plot(x[i], y[i], marker='o', markersize=3, color=\"blue\")\nsgd(ssr_gradient, x, y, n_vars=2, learn_rate=0.0001,decay_rate=0.02, batch_size=3, n_iter=100_000, random_state=0)","1837b3ab":"**\u1ee8NG D\u1ee4NG C\u1ee6A GRADIENT**\n1. T\u00ecm c\u1ef1c tr\u1ecb c\u1ee7a m\u1ed9t h\u00e0m s\u1ed1 (v\u00ed d\u1ee5 tr\u00ean)\n2. T\u00ecm h\u00e0m s\u1ed1 t\u1eeb c\u00e1c quan s\u00e1t x,y","b1f51efa":"**C\u1ea2I TI\u1ebeN CODE**","e1ad0f85":"**Learning rate \u1ea3nh h\u01b0\u1edfng r\u1ea5t l\u1edbn t\u1edbi k\u1ebft qu\u1ea3 c\u1ee7a c\u00e1c h\u00e0m s\u1ed1 kh\u00f4ng l\u1ed3i**","7ebe5269":"Momentum in Stochastic Gradient Descent","eaec87b2":"Gi\u00e1 tr\u1ecb b\u1eaft \u0111\u1ea7u ng\u1eabu nhi\u00ean thi\u1ebft l\u1eadp n\u1ebfu start khi g\u1ecdi h\u00e0m =none","2976719f":"Minibatches in Stochastic Gradient Descent","84fca363":"**GRADIENT C\u01a0 B\u1ea2N**","de79276f":"**Stochastic Gradient Descent Algorithms**"}}