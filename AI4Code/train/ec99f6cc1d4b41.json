{"cell_type":{"6657ecfc":"code","b0254510":"code","c705bd34":"code","a72d8734":"code","a167ceb7":"code","714d2adb":"code","d67a52df":"code","dfba063b":"code","8826216a":"code","6d1e4d25":"code","884bcc44":"code","f0adaa68":"code","3bd3a7ba":"code","66b295fc":"code","893ecd28":"code","ee7e64b1":"code","fbee2974":"code","b183176e":"code","6311b7c2":"code","b4da4a6d":"code","f09d789f":"code","88c56531":"code","de1474d9":"code","b7f0913d":"code","742e5b99":"code","2b87fbde":"code","3fd2606e":"code","d6871a0e":"code","583734e6":"markdown","9f8b4166":"markdown","d4c0a750":"markdown","90519b43":"markdown","215b8f5e":"markdown","49864cbf":"markdown","a20a6809":"markdown","1f570a28":"markdown","e4640ae1":"markdown","d258b305":"markdown","050c9cf9":"markdown","62df171f":"markdown","752dacf8":"markdown","3ec366fd":"markdown","f9b74381":"markdown","6affba45":"markdown","152240c8":"markdown","5f89e5c3":"markdown"},"source":{"6657ecfc":"# To ignore all warnings in the outputs.\nimport warnings\nwarnings.filterwarnings('ignore')","b0254510":"# To manipulate data directories\nimport os","c705bd34":"# Current Image Directory\nimage_dir = '..\/input\/pokemon-images-dataset\/pokemon_jpg\/pokemon_jpg'","a72d8734":"# To validate if augmentation is need or not \nprint(f' Total count of Images : { len( os.listdir( image_dir ) ) }')","a167ceb7":"# for image processing\nimport matplotlib.pyplot as plt\nimport cv2","714d2adb":"# function to load image\ndef load(image_path):\n  # read image from it's path\n  img = cv2.imread(image_path)\n  # convert to RGB from BGR\n  img = cv2.cvtColor(img , cv2.COLOR_BGR2RGB)\n  # resize\n  img = cv2.resize(img , (256,256))\n    \n  return img\n\n# visualize images\ndef display(im_dir):\n  # setting subplot structure\n  fig , ax = plt.subplots(4 , 5 , figsize=(20 , 9))\n  plt.suptitle('Pokemon Images' , size = 20)\n \n  for i in range(20):\n    # fetching image path\n    img_path = os.path.join(im_dir , os.listdir(im_dir)[i])\n    image = load(img_path)\n    plt.subplot(4 , 5 , i + 1)\n    plt.imshow(image)\n    plt.axis('off')\n\n  plt.show()","d67a52df":"# displaying pokemon image into 256x256 size\ndisplay( image_dir )","dfba063b":"import numpy as np\n# arbitary image path\narb_img_path = '..\/input\/pokemon-images-dataset\/pokemon_jpg\/pokemon_jpg\/110.jpg'\n# loading the image\narb_img = np.array(cv2.imread(arb_img_path))\nprint(f'Minimum pixel value : {min(arb_img.flatten())} ,  Maximum pixel value : {max(arb_img.flatten())}')\n","8826216a":"# pixel value distribution\n# code taken from https:\/\/stackoverflow.com\/questions\/12182891\/plot-image-color-histogram-using-matplotlib\ncolor = ('r','g','b')\nplt.figure()\nfor i,col in enumerate(color):\n    histr = cv2.calcHist( [arb_img] , [i] , None , [256] , [0 , 256])\n    plt.plot(histr , color = col , label = str(col))\n    plt.xlim([0,256])\nplt.legend()\nplt.show()","6d1e4d25":"# Loading Libraries for data preprocessing and Augmentation\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom keras.preprocessing.image import ImageDataGenerator as GEN","884bcc44":"# Only augmenting the train datas as more data need \n# to find the best fitted model and reduce bias.\n\n# train data generator\ntrain_augment = GEN(\n    rescale = 1. \/ 255 ,\n    horizontal_flip = True , \n    zoom_range = 0.1 ,\n    shear_range = 0.1 ,\n    validation_split = 0.2 , \n)\n\n# validation data generator\nval_augment = GEN(\n    rescale = 1. \/ 255 ,\n    validation_split = 0.2 , \n)","f0adaa68":"# Folder containing image folder.\n# As we need to fit the data into the generator \n# we need the pass the image directory , not the folder.\nim_folder = '..\/input\/pokemon-images-dataset\/pokemon_jpg'","3bd3a7ba":"# datagenartor hyperparameters\nIMG_SHAPE = (128 , 128)\nBATCH_SIZE = 10\nSEED = 42","66b295fc":"# creating train dataset\ntrain_data = train_augment.flow_from_directory(\n    im_folder , \n    target_size = IMG_SHAPE ,\n    batch_size = BATCH_SIZE , \n    subset = 'training' , \n    class_mode = None , \n    seed = SEED , \n)\n# creay=ting validation daatset\nval_data = val_augment.flow_from_directory(\n    im_folder , \n    target_size = IMG_SHAPE , \n    batch_size = BATCH_SIZE , \n    subset = 'validation' , \n    class_mode = None , \n    seed = SEED , \n)","893ecd28":"# function to check data generated after augmentation\ndef patch_visualize(data_patch , train = True):\n    \n    fig , ax = plt.subplots( 2 , 5 , figsize = (20 , 12))\n    plt.suptitle('Data Visualization' , size = 18)\n    for i in range(10):\n        \n        plt.subplot(2 , 5 , i + 1 )\n        plt.imshow(data_patch[i])\n        plt.axis('off')\n        \n    plt.show()","ee7e64b1":"# visualizing training data\npatch_visualize(train_data[0])","fbee2974":"# visualizing test data\npatch_visualize(val_data[0] , train = False)","b183176e":"# imporing libraries to create Convolutional AutoEncoder Network\n\nfrom tensorflow.keras.layers import (\n    Input , Conv2D , Conv2DTranspose , MaxPooling2D , UpSampling2D   # model layers\n)\nfrom tensorflow.keras.models import Model                            # functional model\nfrom tensorflow.keras.losses import MeanSquaredError                 # loss metric\nfrom tensorflow.keras.optimizers import Adam                         # optimizer","6311b7c2":"class CAE():\n    def __init__(self, input_shape):\n        self.input_shape = input_shape\n        self.c1 = Conv2D(16, (3, 3), activation='relu', padding='same')\n        self.c2 = Conv2D(8, (3, 3), activation='relu', padding='same')\n        self.c3 = Conv2D(8, (3, 3), activation='relu', padding='same')\n        self.pool = MaxPooling2D((2, 2))\n        self.ct1 = Conv2DTranspose(16, (3, 3), activation='relu', padding='same')\n        self.ct2 = Conv2DTranspose(8, (3, 3), activation='relu', padding='same')\n        self.ct3 = Conv2DTranspose(8, (3, 3), activation='relu', padding='same')\n        self.ups = UpSampling2D((2, 2))\n    \n    def network(self):\n        input_layer = Input(shape=self.input_shape)\n        x = self.c1(input_layer)\n        x = self.pool(x)\n        x = self.c2(x)\n        x = self.pool(x)\n        x = self.c3(x)\n\n        code_layer = self.pool(x)\n\n        x = self.ct3(code_layer)\n        x = self.ups(x)\n        x = self.ct2(x)\n        x = self.ups(x)\n        x = self.ct1(x)\n        x = self.ups(x)\n        output_layer = Conv2D(3, (3, 3), padding='same', name=\"Output_layer\")(x)\n        \n        model = Model( inputs = input_layer , outputs = output_layer , name = 'Poke_CAE')\n        return model","b4da4a6d":"# model class\ncae = CAE(IMG_SHAPE + (3,))","f09d789f":"# loading the model in backend\n\n# refershing the session\nkeras.backend.clear_session()\n\nmodel = cae.network()\n\n#visualizing the model architecture\nmodel.summary()","88c56531":"EPOCHS = 10\nloss_fn = MeanSquaredError()\noptimizer = Adam(1e-4)\nsteps_per_epoch = 1500\nloss_tracker_index = steps_per_epoch \/\/ 10","de1474d9":"import time\n\n# for storing the loss data\ntrain_loss_counter = []\nval_loss_counter = []\n\nfor epoch in range(EPOCHS) :\n    \n    # storing the time\n    start_time = time.time()\n    print(f'Epoch : {epoch + 1}')\n    print('Train Loss : [',end = \" \")\n    \n    for index, patch in enumerate(train_data):\n        \n        with tf.GradientTape() as tape:\n            # finding loss\n            outputs = model(patch , training = True)\n            patch_loss = loss_fn(patch , outputs)\n            \n        # applying gradients on model weights\n        grads = tape.gradient(patch_loss , model.trainable_weights)\n        optimizer.apply_gradients(zip(grads , model.trainable_weights))\n        # printing loss\n        if index % loss_tracker_index == 0:\n          print('%.5f'%(float(patch_loss)) , end = \" , \" if index != steps_per_epoch else \" ]\\n\")\n        if index == steps_per_epoch:\n          train_loss_counter.append(patch_loss)\n          break\n    # validation loss\n    tot_val_loss = 0\n    for index , val_patch in enumerate(val_data):\n        \n        val_output = model(val_patch , training = False)\n        tot_val_loss += loss_fn(val_patch , val_output)\n        \n        if index == steps_per_epoch \/\/ 10:\n            break\n    print('Validation Loss : %.4f'%(tot_val_loss \/ index))\n    \n    val_loss_counter.append(tot_val_loss \/ index)\n    \n    print('Time taken for epoch %d : %.2f seconds.'%(epoch + 1 , time.time() - start_time ))\n    print('...................................................\\n\\n')\nprint('Training Complete !!')","b7f0913d":"plt.figure( figsize = (10 , 6))\nplt.title('Model Loss')\nplt.plot(train_loss_counter , label = 'train')\nplt.plot(val_loss_counter , label = 'validation')\nplt.legend()\nplt.xlabel('EPOCH')\nplt.ylabel('MSE')\nplt.show()","742e5b99":"# function to visualize model performance\ndef visualize(data_patch , model , train = True):\n    \n    #predict over data\n    pred_patch = model.predict( data_patch )\n    fig , ax = plt.subplots( 4 , 4 , figsize = (20 , 8))\n    if train:\n        plt.suptitle('Model Evaluation on Train Data' , size = 18)\n    else:\n        plt.suptitle('Model Evaluation on Validation' , size = 18)\n    for i in range(8):\n        \n        plt.subplot(4 , 4 , i*2 + 1 )\n        plt.imshow(data_patch[i])\n        plt.title('Image')\n        plt.axis('off')\n        plt.subplot(4 , 4 , i*2 + 2 )\n        plt.imshow(pred_patch[i])\n        plt.title('Predicted')\n        plt.axis('off')\n        \n    plt.show()\n        ","2b87fbde":"# Visualizing on Train data\ntrain_data_patch = train_data[0]\n\nvisualize( train_data_patch , model )","3fd2606e":"# Visualizing on validation data\nvalidation_data_patch = val_data[0]\n\nvisualize( validation_data_patch , model , train = False)","d6871a0e":"# Encoder\n\nencoder = Model(inputs = model.input , outputs = model.get_layer('conv2d_transpose_2').output , name = 'Poke_Encoder')  \n# conv2d_transpose_2 layer represents the core\n\nencoder.summary()","583734e6":"Semi supervised learning has unfolded frontiers in deep learning.\nAutoEncoders, a technique or idea in semi supervised learning contributes to computer visison a lot. AutoEncoder is normally a generative network as it regenerates the identical version of corresponding data.\n\n\nIn this notebook **Convolutional AutoEncoder** has been explained and implemented on pokemon dataset.\n## Do UPVOTE if this helps you :)","9f8b4166":"# 5. Model Training :\n---","d4c0a750":"The encoder and decoder network are mirror-identical in this notebook.","90519b43":"# 1. Data Loading and Visualization :\n---\n At first we have to visualize to find out actual state of the images thus we can choose our data processing approach.","215b8f5e":"We can see the colours are distributed in 0 to 255 pixel values. For ease of model training , the images will be scaled into 0 to 1 range.","49864cbf":"The loss curve looks quite well.","a20a6809":"**NOTE** : In this trining we can see that the fetures were extraxted correctly , but the colour complexion and quality reduced due to short core lyer. If we store a larger core layer , this may diminish too. This was done on purpose to share this phenomena.","1f570a28":"# 8. Conclusion :\n---\nThe notebook correctly shows that too much redueed information will not generate much accurate data. So, we have to choose the correct architecture.\n\n\n\n# THANK YOU for visiting :)\n\n## Do visit my other works at [kaggle](https:\/\/kaggle.com\/sagnik1511\/code) and also at [github](https:\/\/github.com\/sagnik1511).\n\n![](https:\/\/ak.picdn.net\/shutterstock\/videos\/1017223462\/thumb\/2.jpg)","e4640ae1":"Every project should have a sequential approach.\nThe contents are as follows :\n              \n              1. Data Loading and Visualization\n              2. Image Preprocessing & Dataset Preparation\n              3. Model Building\n              4. Hyperparameter choosing\n              5. Model Training\n              6. Model Performnace Evaluation\n              7. Encoder Preparation\n              8. Conclusion","d258b305":"# 2. Image Preprocessing & Dataset Preparation :\n---\nNow we have seen the background and the pixel value distributions. Now we have to prepare the train and validation dataset to train and find best tuning model.","050c9cf9":"Now arbitarily checking image pixel value distribution","62df171f":"# 6. Model Performance Evaluation :\n---\nNow, the model has to be validated if it is overfitiing or underfitting.","752dacf8":"#### Loss Curve :","3ec366fd":"# 3. Model Building:\n---\n\nAs we have generated the train and validation data we can now move to model generation part.\nThe model is used in this notebook is a [Convolutional Autoencoder](https:\/\/towardsdatascience.com\/convolutional-autoencoders-for-image-noise-reduction-32fce9fc1763).\n","f9b74381":"#  7. Encoder Preparation :\n---\nNow from that we can take the encoder part and use it for encoding similar images.","6affba45":"# 4. Hyperparameters:\n---","152240c8":"We can see there are very less number of images. If we try to train the model with it , we may find high bias.\nSo, we need to augment the images.","5f89e5c3":"# Convolutional AutoEncoder on Pokemon Dataset :\n\n![](https:\/\/i2.wp.com\/sefiks.com\/wp-content\/uploads\/2018\/03\/convolutional-autoencoder.png?fit=1818%2C608&ssl=1)"}}