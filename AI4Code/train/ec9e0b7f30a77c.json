{"cell_type":{"c17f34b3":"code","ce817923":"code","74b7cf5a":"code","3b09560a":"code","296044d0":"code","c6da37db":"code","2b0633ea":"code","0b34c798":"code","c57caf44":"code","b480d636":"code","441804a2":"code","8cbeb5b8":"code","2751f0d0":"code","15dc54f6":"code","710aa8c6":"code","e5f259ff":"code","494588bd":"code","957d5902":"code","3f304eb9":"code","656a3ec7":"code","0e1ae3e0":"code","bdc6630b":"code","a7e53c20":"code","53261113":"markdown","463c76ae":"markdown","089b2111":"markdown","986ce1c4":"markdown","81452bb1":"markdown","db4abe51":"markdown","bb098e9f":"markdown","825f3fc7":"markdown","8afa1ce2":"markdown","add00daa":"markdown","dbc2c575":"markdown","1645742a":"markdown"},"source":{"c17f34b3":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn import preprocessing\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import StratifiedKFold\nfrom lightgbm import LGBMClassifier\nimport time\n\nseed=47\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","ce817923":"invoice_test = pd.read_csv('\/kaggle\/input\/fraud-detection-in-electricity-and-gas-consumption\/invoice_test.csv',low_memory=False)\ninvoice_train = pd.read_csv('\/kaggle\/input\/fraud-detection-in-electricity-and-gas-consumption\/invoice_train.csv',low_memory=False)\nclient_test = pd.read_csv('\/kaggle\/input\/fraud-detection-in-electricity-and-gas-consumption\/client_test.csv',low_memory=False)\nclient_train = pd.read_csv('\/kaggle\/input\/fraud-detection-in-electricity-and-gas-consumption\/client_train.csv',low_memory=False)\nsample_submission = pd.read_csv('\/kaggle\/input\/fraud-detection-in-electricity-and-gas-consumption\/SampleSubmission (2).csv',low_memory=False)","74b7cf5a":"ds = client_train.groupby(['target'])['client_id'].count()\nplt.bar(x=ds.index, height=ds.values, tick_label =[0,1])\nplt.title('target distribution')\nplt.show()","3b09560a":"for col in ['disrict','region','client_catg']:\n    ds = client_train.groupby([col])['client_id'].count()\n    plt.bar(x=ds.index, height=ds.values)\n    plt.title(col+' distribution')\n    plt.show()","296044d0":"print('Number of missing rows in invoice_train:',invoice_train.isna().sum().sum())\nprint('Number of missing rows in invoice_test:',invoice_test.isna().sum().sum(),'\\n')\nprint('Number of missing rows in client_train:',client_train.isna().sum().sum())\nprint('Number of missing rows in client_test:',client_test.isna().sum().sum())","c6da37db":"print('Number of unique values in invoice_train:')\nfor col in invoice_train.columns:\n    print(f\"{col} - {invoice_train[col].nunique()}\")","2b0633ea":"def feature_change(cl, inv):\n\n    cl['client_catg'] = cl['client_catg'].astype('category')\n    cl['disrict'] = cl['disrict'].astype('category')\n    cl['region'] = cl['region'].astype('category')\n    cl['region_group'] = cl['region'].apply(lambda x: 100 if x<100 else 300 if x>300 else 200)\n    cl['creation_date'] = pd.to_datetime(cl['creation_date'])\n    \n    cl['coop_time'] = (2019 - cl['creation_date'].dt.year)*12 - cl['creation_date'].dt.month\n\n    inv['counter_type'] = inv['counter_type'].map({\"ELEC\":1,\"GAZ\":0})\n    inv['counter_statue'] = inv['counter_statue'].map({0:0,1:1,2:2,3:3,4:4,5:5,769:5,'0':0,'5':5,'1':1,'4':4,'A':0,618:5,269375:5,46:5,420:5})\n    \n    inv['invoice_date'] = pd.to_datetime(inv['invoice_date'], dayfirst=True)\n    inv['invoice_month'] = inv['invoice_date'].dt.month\n    inv['invoice_year'] = inv['invoice_date'].dt.year\n    inv['is_weekday'] = ((pd.DatetimeIndex(inv.invoice_date).dayofweek) \/\/ 5 == 1).astype(float)\n    inv['delta_index'] = inv['new_index'] - inv['old_index']\n    \n    return cl, inv","0b34c798":"client_train1, invoice_train1 = feature_change(client_train, invoice_train)\nclient_test1, invoice_test1 = feature_change(client_test, invoice_test)","c57caf44":"def agg_feature(invoice, client_df, agg_stat):\n    \n    invoice['delta_time'] = invoice.sort_values(['client_id','invoice_date']).groupby('client_id')['invoice_date'].diff().dt.days.reset_index(drop=True)\n    agg_trans = invoice.groupby('client_id')[agg_stat+['delta_time']].agg(['mean','std','min','max'])\n    \n    agg_trans.columns = ['_'.join(col).strip() for col in agg_trans.columns.values]\n    agg_trans.reset_index(inplace=True)\n\n    df = invoice.groupby('client_id').size().reset_index(name='transactions_count')\n    agg_trans = pd.merge(df, agg_trans, on='client_id', how='left')\n    \n    weekday_avg = invoice.groupby('client_id')[['is_weekday']].agg(['mean'])\n    weekday_avg.columns = ['_'.join(col).strip() for col in weekday_avg.columns.values]\n    weekday_avg.reset_index(inplace=True)\n    client_df = pd.merge(client_df, weekday_avg, on='client_id', how='left')\n    \n    full_df = pd.merge(client_df, agg_trans, on='client_id', how='left')\n    \n    full_df['invoice_per_cooperation'] = full_df['transactions_count'] \/ full_df['coop_time']\n    \n    return full_df","b480d636":"agg_stat_columns = [\n 'tarif_type',\n 'counter_number',\n 'counter_statue',\n 'counter_code',\n 'reading_remarque',\n 'consommation_level_1',\n 'consommation_level_2',\n 'consommation_level_3',\n 'consommation_level_4',\n 'old_index',\n 'new_index',\n 'months_number',\n 'counter_type',\n 'invoice_month',\n 'invoice_year',\n 'delta_index'\n]\n\ntrain_df1 = agg_feature(invoice_train1, client_train1, agg_stat_columns)\ntest_df1 = agg_feature(invoice_test1, client_test1, agg_stat_columns)","441804a2":"def new_features(df):\n    \n    for col in agg_stat_columns:\n        df[col+'_range'] = df[col+'_max'] - df[col+'_min']\n        df[col+'_max_mean'] = df[col+'_max']\/df[col+'_mean']\n    \n    return df","8cbeb5b8":"train_df2 = new_features(train_df1)\ntest_df2 = new_features(test_df1)","2751f0d0":"print('Initial number of columns: ', len(client_train.columns)+len(invoice_train.columns))\nprint('Number of columns now: ', len(train_df2.columns))","15dc54f6":"def drop(df):\n\n    col_drop = ['client_id', 'creation_date']\n    for col in col_drop:\n        df.drop([col], axis=1, inplace=True)\n    return df","710aa8c6":"train_df = drop(train_df2)\ntest_df = drop(test_df2)","e5f259ff":"y = train_df['target']\nX = train_df.drop('target',axis=1)\n\nfeature_name = X.columns.tolist()","494588bd":"drop_col=['reading_remarque_max','counter_statue_min','counter_type_min','counter_type_max','counter_type_range',\n          'tarif_type_max', 'delta_index_min', 'consommation_level_4_mean']\n\nX = X.drop(drop_col, axis=1)\ntest_df = test_df.drop(drop_col, axis=1)","957d5902":"from optuna import Trial\nimport gc\nimport optuna\nfrom sklearn.model_selection import train_test_split\nimport lightgbm as lgb\n\ncategory_cols = ['disrict', 'client_catg', 'region']\n\ndef objective(trial:Trial):\n    \n    gc.collect()\n    models=[]\n    validScore=0\n   \n    model,log = fitLGBM(trial,X,y)\n    \n    models.append(model)\n    gc.collect()\n    validScore+=log\n    validScore\/=len(models)\n    \n    return validScore","3f304eb9":"def fitLGBM(trial,X, y):\n    \n    params={\n      'n_estimators':trial.suggest_int('n_estimators', 0, 1000), \n      'num_leaves':trial.suggest_int('num_leaves', 2, 512),\n      'max_depth':trial.suggest_int('max_depth', 2, 128),\n      'learning_rate': trial.suggest_loguniform('learning_rate', 0.001, 0.15),\n      'min_split_gain': trial.suggest_loguniform('min_split_gain', 0.001, 0.1),\n      'feature_fraction':trial.suggest_uniform('feature_fraction',0.1, 1.0),\n      'bagging_freq':trial.suggest_int('bagging_freq',0.1,10),\n      'verbosity': -1,\n      'random_state':seed\n            }\n    stkfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=seed)\n    model = LGBMClassifier(**params)\n    \n    res=[]\n    for i, (tdx, vdx) in enumerate(stkfold.split(X, y)):\n        X_train, X_valid, y_train, y_valid = X.iloc[tdx], X.iloc[vdx], y[tdx], y[vdx]\n        model.fit(X_train, y_train,\n                 eval_set=[(X_train, y_train), (X_valid, y_valid)],\n                 early_stopping_rounds=30, verbose=False)\n        preds = model.predict_proba(X_valid)\n        res.append(roc_auc_score(y_valid, preds[:,1]))\n    \n    err = np.mean(res)\n    \n    return model, err","656a3ec7":"#study = optuna.create_study(direction='maximize', pruner=optuna.pruners.MedianPruner(n_warmup_steps=5))\n#study.optimize(objective, , timeout=60*60*2)","0e1ae3e0":"model = LGBMClassifier(random_state=seed, n_estimators=830,num_leaves=454, max_depth=61,\n                       learning_rate=0.006910869038433314, min_split_gain=0.00667926424629105, \n                       feature_fraction=0.3764303138879782, bagging_freq=8)\n\nstkfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=seed)\n\ndef calc(X, y, model, cv):\n    res=[]\n    local_probs=pd.DataFrame()\n    probs = pd.DataFrame()\n\n    for i, (tdx, vdx) in enumerate(cv.split(X, y)):\n        X_train, X_valid, y_train, y_valid = X.iloc[tdx], X.iloc[vdx], y[tdx], y[vdx]\n        model.fit(X_train, y_train,\n                 eval_set=[(X_train, y_train), (X_valid, y_valid)],\n                 early_stopping_rounds=30, verbose=False)\n        \n        preds = model.predict_proba(X_valid)\n        oof_predict = model.predict_proba(test_df)\n        local_probs['fold_%i'%i] = oof_predict[:,1]\n        res.append(roc_auc_score(y_valid, preds[:,1]))\n\n    print('ROC AUC:', round(np.mean(res), 6))    \n    local_probs['res'] = local_probs.mean(axis=1)\n    probs['target'] = local_probs['res']\n    \n    return probs","bdc6630b":"%%time\nprobs = calc(X, y, model, stkfold)","a7e53c20":"submission = pd.DataFrame({\n        \"client_id\": sample_submission[\"client_id\"],\n        \"target\": probs['target']\n    })\nsubmission.to_csv('submission.csv', index=False)","53261113":"Now let's review how many features did we create:","463c76ae":"<h2>Importing libraries<\/h2>","089b2111":"* we created really a lot of features and sure, not all of them were usefull, so we dropped some unnessesary columns in next few cells\n* 'drop_col' array was made after using our own backward feature selection function","986ce1c4":"Also we created statistical 'max_mean' and 'range' features which noticeably improved score","81452bb1":"To sum up, at the time of publication of the notebook, we got 4th place in this competition!Thank you for watching, waiting your comments!\n\n<img src=\"https:\/\/raw.githubusercontent.com\/imgremlin\/Photos\/master\/lb.png\" width=\"700\"> ","db4abe51":"<h2>Modelling<\/h2>\n\n* we used [optuna](https:\/\/optuna.org\/) for hyperparameters tuning\n* it was performed with respect to StratifiedKFold cross validation on 5 folds\n* you can check parameters for tuning and their final values in cells below","bb098e9f":"<img src=\"https:\/\/raw.githubusercontent.com\/imgremlin\/Photos\/master\/electricity.jpg\" width=\"1000px\"> \n# Fraud Detection in Electricity and Gas Consumption Challenge\n**by team GORNYAKI (Tsepa Oleksii and Samoshin Andriy [Ukraine, KPI, IASA])**\n\nThanks to the organizers for this [challenge](https:\/\/zindi.africa\/competitions\/ai-hack-tunisia-4-predictive-analytics-challenge-1) and everyone for participating! In this notebook you will find:\n\n* importing libraries\n* basic EDA\n* feature engeneering\n* modelling\n* prediction \n* submission","825f3fc7":"<h2>Prediction and submission<\/h2>\n\nIn the next few cells you can see our local cross validation which almost match  LB score","8afa1ce2":"* created some aggregation features (min\/max\/mean\/std) over continious columns per every client\n* added 'delta_time' - amount of time between invoices for each user\n* created 'invoice_per_cooperation' - number of transactions per some amount of time","add00daa":"<h2>Feature engeneering<\/h2>\n\nIn this part we want to explain the most powerful decision in our notebook - feature creation","dbc2c575":"<h2>Basic EDA<\/h2>\n\nWe won't show full EDA, just want to attract your attention to tips which help us to reach good score.\n\nIn next two cells you will find value counts according each column in train and test set. This information we'll use in feature engeneering  ","1645742a":"* 'client_catg', 'district' and 'region' were assigned as categories to use them as categorical features in lgbm (as for me, lgbm for default threats with cat features slightly better than other encoders such as catboost\/target encoder)\n* 'region_group' created simply by dividing 'region' in 3 groups (we purposed that regions weren't randomly decoded)\n* 'coop_time' - amount of time since account creation in months\n* 'counter_type' was binary encoded \n* 'counter_statue' cleaned from mislabeled values\n* extracted month, year from 'invoice_date', also added binary feature - 'is_weekday'\n* not sure about any logical sense in 'delta_index', but it improved score"}}