{"cell_type":{"8e1f50df":"code","ef4ae8ba":"code","6d3e6bb7":"code","996d62b0":"code","fef7edf8":"code","d74f45b4":"code","4f13e1f0":"code","a7b7bc53":"code","e2a8086f":"code","8c7a8f2a":"code","4999d305":"code","25aca653":"code","3741d204":"code","6b56a290":"code","fef86607":"code","59b1136f":"code","a5b075c1":"code","6e7e8ff8":"code","c5d4679d":"code","8a00b288":"code","f7807584":"code","a949877e":"code","dff27e4c":"code","e3993f25":"code","d392fcd1":"code","d2b056ae":"code","b7d71566":"code","ee4ccf85":"code","d92f1684":"code","e7c51590":"code","3cba83d2":"code","3694db54":"code","1dd67238":"code","41088e36":"code","2ae4c152":"code","5c91ff90":"code","44c9396e":"code","7d44a4f3":"code","8a171cb6":"markdown","0127354c":"markdown","0e045eca":"markdown","1dae0c2c":"markdown","3318d5fb":"markdown","abe19fe0":"markdown","e2746c05":"markdown","2fa97e02":"markdown","9ee13b77":"markdown"},"source":{"8e1f50df":"import pandas as pd\nimport numpy as np\n\n# # read in \n# docs = pd.read_json('..\/input\/arxiv\/arxiv-metadata-oai-snapshot-2020-08-14.json', lines=True)\n\n# does not work. Out of memory","ef4ae8ba":"import dask.bag as db\nimport json\n\ndocs = db.read_text('..\/input\/arxiv\/arxiv-metadata-oai-snapshot-2020-08-14.json').map(json.loads)","6d3e6bb7":"docs.count().compute()\n# we have 1,747,307 documents","996d62b0":"# lets look at one document\ndocs.take(1)","fef7edf8":"# Top Submitters\ndocs.map(lambda x: x['submitter']).frequencies(sort = True).topk(10, key=1).compute()","d74f45b4":"# Top Authors\nparse_authors = trim = lambda x: [' '.join(a).strip() for a in x['authors_parsed']]\n\ndocs.map(parse_authors).flatten().frequencies(sort = True).topk(20, key=1).compute()","4f13e1f0":"docs.map(parse_authors).flatten().distinct().count().compute()","a7b7bc53":"# Submissions by datetime\nget_latest_version = lambda x: x['versions'][-1]['created']\n\ndates = (docs\n         .map(get_latest_version)\n         .frequencies(sort = True))\n\n# show top submissions datetime\n(dates.topk(10, key=1)\n      .compute())","e2a8086f":"# convert to dataframe \ndates = dates.to_dataframe(columns = ['submission_datetime','submissions']).compute()\ndates.head(4)","8c7a8f2a":"dates.count()\n# seems like around 9k papers dont have datetime associated with them","4999d305":"import altair as alt\n\n# enable altair to save chart data locally instead of embeding it in the notebook\nalt.data_transformers.enable('json')\n\n# convert string to date\ndate_format = '%a, %d %b %Y %H:%M:%S %Z'\ndates['submission_datetime'] = pd.to_datetime(dates.submission_datetime, format = date_format)\ndates['submission_date'] = dates.submission_datetime.dt.date\ndates2 = dates.groupby('submission_date').sum().reset_index()\n\n# set up chart\nalt.Chart(dates2).mark_bar().encode(\n    x='submission_date',\n    y=alt.Y('submissions', axis = alt.Axis(orient = 'right')),\n    tooltip = ['submission_date','submissions']\n).properties(\n    width = 600\n).configure_axis(\n    grid=False\n) .configure_view(strokeOpacity=0)\n\n# currently we are close to 1,000 submissions a day","25aca653":"# aggregate submissions by weekday and hour\ndates['weekday'] = dates.submission_datetime.dt.day_name()\ndates['hour'] = dates.submission_datetime.dt.hour\ndays = (dates\n .groupby(['submission_date','weekday','hour'])\n .submissions\n .sum()\n .groupby(['weekday','hour'])\n .mean()).reset_index()\n\norder = ['Monday','Tuesday','Wednesday','Thursday','Friday','Saturday','Sunday']\n\n# plot heatmap of average submissions by weekday and hour\n# set up base\nbase = alt.Chart(days).encode(\n    x=alt.X('hour:O', axis=alt.Axis(labelAngle=0, title=None)),\n    y=alt.Y('weekday', sort=order, \n            axis=alt.Axis(labelAngle=0, title=None))\n)\n# draw heatmap\nheatmap = base.mark_rect().encode(\n    color = alt.Color('submissions:Q', legend = None, scale=alt.Scale(scheme = 'blues')),\n    tooltip = ['weekday','hour','submissions']\n).properties(\n    title = 'Average Submissions by Day of Week and Hour',\n    width = 500,\n    height = 200\n)\n# apply labels\ntext = base.mark_text(baseline='middle').encode(\n    text=alt.Text('submissions:Q', format=\",.0f\"),\n    color = alt.condition(alt.datum.submissions < 8,\n                          alt.value('black'),\n                          alt.value('white'))\n    \n)\n\n(heatmap + text).configure_title(fontSize=16)","3741d204":"# calculate average submissions by month\ndates['month'] = dates.submission_datetime.dt.month_name().apply(lambda x: x[0:3])\ndates['year'] = dates.submission_datetime.dt.year\nmonths = (dates\n .groupby(['year','month'])\n .submissions\n .sum()\n .groupby(['month'])\n .mean()).reset_index()\n\norder = ['Jan','Feb', 'Mar', 'Apr', 'May','Jun','Jul','Aug','Sep', 'Oct', 'Nov','Dec']\n\n# plot heatmap of average submissions by month\n# set up base\nbase = alt.Chart(months).encode(\n    y=alt.Y('month', sort = order,\n            axis=alt.Axis(labelAngle=0, title=None))\n)\n# draw heatmap\nheatmap = base.mark_rect().encode(\n    color = alt.Color('submissions:Q', legend = None, scale=alt.Scale(scheme = 'blues')),\n    x = alt.X('submissions:Q', \n            axis=alt.Axis(labelAngle=0, title=None, labels = False)),\n    tooltip = ['month','submissions']\n).properties(\n    title = ['Avg Monthly Submissions'],\n    width = 50,\n    height = 200\n)\n# apply labels\ntext = base.mark_text(baseline='middle').encode(\n    text=alt.Text('submissions:Q', format=\",.0f\"),\n    color = alt.condition(alt.datum.submissions < 4550,\n                          alt.value('black'),\n                          alt.value('white'))\n    \n)\n\n(heatmap + text).configure_title(fontSize=14)","6b56a290":"# get only necessary fields\ntrim = lambda x: {'id': x['id'],\n                  'title': x['title'],\n                  'category':x['categories'].split(' '),\n                  'abstract':x['abstract']}\n# filter for papers published on or after 2019-01-01\ncolumns = ['id','category','abstract']\ndocs_df = (docs\n             .filter(lambda x: int(get_latest_version(x).split(' ')[3]) > 2018)\n             .map(trim)\n             .compute())\n\n# convert to pandas\ndocs_df = pd.DataFrame(docs_df)\n\n# add general category. we are going to use as our target variable\ndocs_df['general_category'] = docs_df.category.apply(lambda x:[a.split('.')[0] for a in x])","fef86607":"docs_df","59b1136f":"from sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.decomposition import PCA\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom nltk.corpus import stopwords\nstop_words = set(stopwords.words('english'))\nfrom sklearn.svm import LinearSVC\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import accuracy_score, roc_curve, auc, hamming_loss\nfrom sklearn.preprocessing import MultiLabelBinarizer\nfrom sklearn.calibration import CalibratedClassifierCV","a5b075c1":"# convert general category into label columns\nmlb = MultiLabelBinarizer()\nlabels = mlb.fit_transform(docs_df.general_category)\nlabels","6e7e8ff8":"# concatenate with the abstracts\ndf = pd.concat([docs_df[['abstract','title']], pd.DataFrame(labels)], axis=1)\ndf.columns = ['abstract','title'] + list(mlb.classes_)\ndf.head(4)","c5d4679d":"# sample and keep columns that have at least 1 positive example\nsample_df = df.sample(frac = 1, random_state = 4)\nkeep = sample_df.iloc[:,2:].apply(sum) > 1\nsample_df = pd.concat([sample_df.iloc[:,:2],sample_df.iloc[:,2:].iloc[:,keep.values]], axis = 1)\n\ncategories = sample_df.columns[2:]\n\n# removed categories\nprint('Removed following categories from training : {}'.format(str(keep[~keep].index.to_list())))","8a00b288":"# plot paper distribution by category\nsource = pd.DataFrame(sample_df.iloc[:,2:].apply(sum)).reset_index().rename(columns = {0:'count'})\nptotalText = alt.Text('PercentOfTotal:Q', format = '.2%')\n\nchart = alt.Chart(source).transform_joinaggregate(\n    TotalPapers='sum(count)',\n).transform_calculate(\n    PercentOfTotal=\"datum.count \/ datum.TotalPapers\"\n).mark_bar().encode(\n    x = 'index',\n    y = 'count',\n    tooltip = ['index','count', ptotalText]\n).properties(\n    title='arXiv Papers by Category, after 2019-01-01',\n    width = 800\n)\n\n# add percentage labels\nchart = chart + chart.mark_text(\n    align='center',\n    baseline='middle',\n    dx= 3,  # Nudges text to right so it doesn't appear on top of the bar,\n    dy = -5\n).encode(\n    text = ptotalText\n) \n\nchart","f7807584":"# plot abstracts using PCA\n# only grab abstracts with 1 category\nsample_pca = sample_df.loc[sample_df.iloc[:,2:].apply(sum, axis=1) == 1,:].sample(n = 15_000, random_state = 4)\nX = TfidfVectorizer(stop_words=stop_words).fit_transform(sample_pca.abstract).todense()\npca = PCA(n_components=2).fit(X)\ndata2D = pd.DataFrame(pca.transform(X), columns = ['PC1','PC2'])\n# get categories for abstracts\ncolor = sample_pca.iloc[:,2:].apply(lambda x: x.index[x>0][0], axis = 1)\ndata2D['category'] = color.to_list()\n\n# plot scatter plot \nalt.Chart(data2D).mark_circle(size=15).encode(\n    x=alt.X('PC1', axis = None),\n    y=alt.Y('PC2', axis = None),\n    color = 'category'\n).properties(\n    title='First 2 Principal Components of the arXiv TF-IDF Matrix by Category',\n    width = 600,\n    height = 600\n).configure_axis(\n    grid=False\n).configure_view(strokeOpacity=0)","a949877e":"# split into train and test\ntrain, test = train_test_split(sample_df, random_state=76, test_size=0.15, shuffle=True)\n\nX_train = train.abstract\nX_test = test.abstract\n\nprint(X_train.shape)\nprint(X_test.shape)","dff27e4c":"test.to_json('validation.json')","e3993f25":"def feature_importance(pipeline):\n    '''\n    Extract feature importances from pipeline. \n    Since I am using CalibratedClassifierCV I will average the coefficients over calibrated classifiers.\n    '''\n    # average coefficients over all calibrated classifiers\n    coef_avg = 0\n    classifiers = pipeline[1].estimators_[0].calibrated_classifiers_\n    for i in classifiers:\n        coef_avg = coef_avg + i.base_estimator.coef_\n    coef_avg  = (coef_avg\/len(classifiers)).tolist()[0]\n    # get feature names from tf-idf vectorizer\n    features = pipeline[0].get_feature_names()\n    # get 10 most important features\n    top_f = pd.DataFrame(list(zip(features,coef_avg)), columns = ['token','coef']) \\\n        .nlargest(10,'coef').to_dict(orient = 'records')\n    return top_f\n    ","d392fcd1":"%%time\n# define the pipeline\nclassifier = CalibratedClassifierCV(LinearSVC()) \n\n# for each category train the model and get accuracy, auc\nmodels = {}\nfeatures = {}\npreds = {}\nfor category in categories:\n    # give pipelines unique names. important!  \n    SVC_pipeline = Pipeline([\n                (f'tfidf_{category}', TfidfVectorizer(stop_words=stop_words)),\n                (f'clf_{category}', OneVsRestClassifier(classifier, n_jobs=1)),\n            ])\n    print('... Processing {}'.format(category))\n    # train the model using X_dtm & y\n    SVC_pipeline.fit(X_train, train[category])\n    models[category] = SVC_pipeline\n    # compute the testing accuracy\n    prediction = SVC_pipeline.predict(X_test)\n    preds[category] = prediction\n    accuracy = accuracy_score(test[category], prediction)\n    # compute auc\n    probas_ = SVC_pipeline.predict_proba(X_test)\n    fpr, tpr, thresholds = roc_curve(test[category], probas_[:, 1])\n    roc_auc = auc(fpr, tpr)\n    print(\"Accuracy : {} . Area under the ROC curve : {}\".format(round(accuracy,4), round(roc_auc,4)))\n    print()\n    # get most predictive features\n    features[category] = feature_importance(SVC_pipeline)\n\n    ","d2b056ae":"# 10 most important features by category\nfeatures_df = pd.DataFrame(features)\nfeatures_df.apply(lambda x: [d['token'] for d in x], axis=0)","b7d71566":"def predict_tags(X, labels = None):\n    '''\n    Predict tags for a given abstract.\n    \n    Args:\n      - X (list): an iterable with text.\n      - labels (pandas.Dataframe): label indicators for an abstract\n    '''\n    preds = []\n    if type(X) is str: # convert into iterable if string\n        X = [X]\n    \n    # get prediction from each model\n    for c in models.keys():\n        preds.append(models[c].predict(X))\n    \n    # print original labels if given\n    if labels is not None:\n        assert len(X) == 1, 'Only one extract at a time.'\n        predicted_tags = [k for k,v in zip(list(models.keys()),preds) if v[0] > 0]\n        original_tags = list(labels.index[labels.map(lambda x: x>0)])\n        print('Original Tags: {}'.format(str(original_tags)))\n        print(\"Predicted Tags: {}\".format(str(predicted_tags)))\n        \n    return preds","ee4ccf85":"# predict tags for 20 abstracts\nfor i in range(0,20):\n    print(test.title.iloc[i])\n    predict_tags(test['abstract'].iloc[i], labels = test.iloc[i,2:])\n    print()","d92f1684":"# get all predictions\ny_pred = np.array(predict_tags(test.abstract)).T\n\n# get true labels in the same order\ny_true = test[list(models.keys())].to_numpy()\ny_true","e7c51590":"y_pred","3cba83d2":"hamming_loss(y_true,y_pred)\n# fraction of labels assgined incorrectly.\n# the lower the better","3694db54":"accuracy_score(y_true, y_pred)","1dd67238":"def hamming_score(y_true, y_pred, normalize=True, sample_weight=None):\n    '''\n    Compute the Hamming score (a.k.a. label-based accuracy) for the multi-label case\n    http:\/\/stackoverflow.com\/q\/32239577\/395857\n    '''\n    acc_list = []\n    for i in range(y_true.shape[0]):\n        set_true = set( np.where(y_true[i])[0] )\n        set_pred = set( np.where(y_pred[i])[0] )\n        #print('\\nset_true: {0}'.format(set_true))\n        #print('set_pred: {0}'.format(set_pred))\n        tmp_a = None\n        if len(set_true) == 0 and len(set_pred) == 0:\n            tmp_a = 1\n        else:\n            tmp_a = len(set_true.intersection(set_pred))\/\\\n                    float( len(set_true.union(set_pred)) )\n        #print('tmp_a: {0}'.format(tmp_a))\n        acc_list.append(tmp_a)\n    return np.mean(acc_list)\n\n","41088e36":"hamming_score(y_true,y_pred)\n# label based accuracy.","2ae4c152":"# plot abstract distribution by true category vs predicted\nsource = pd.DataFrame(y_pred, columns = list(models.keys())).apply(sum).reset_index().rename(columns = {0:'count'})\nptotalText = alt.Text('PercentOfTotal:Q', format = '.2%')\n\n# set up bar chart\nchart2 = alt.Chart(source).transform_joinaggregate(\n    TotalPapers='sum(count)',\n).transform_calculate(\n    PercentOfTotal=\"datum.count \/ datum.TotalPapers\"\n).mark_bar().encode(\n    x = 'index',\n    y = 'count',\n    tooltip = ['index','count', ptotalText]\n).properties(\n    title='arXiv Papers by Predicted Category',\n    width = 800\n)\n\n# add percentage labels\nchart2 = chart2 + chart2.mark_text(\n    align='center',\n    baseline='middle',\n    dx= 3,  # Nudges text to right so it doesn't appear on top of the bar,\n    dy = -5\n).encode(\n    text = ptotalText\n) \n\n# concatenate charts\nalt.vconcat(chart, chart2)","5c91ff90":"import dask.bag as db\nimport json\n\nfrom sklearn.preprocessing import MultiLabelBinarizer\nfrom sklearn.model_selection import train_test_split\n\n# prepare data for mutli label classification\nSEED = 4\n\ndocs = db.read_text('..\/input\/arxiv\/arxiv-metadata-oai-snapshot-2020-08-14.json').map(json.loads)\n\n# get only necessary fields\ntrim = lambda x: {'id': x['id'],\n                  'title': x['title'],\n                  'category':x['categories'].split(' '),\n                  'abstract':x['abstract']}\n# filter for papers published on or after 2019-01-01\ncolumns = ['id','category','abstract']\ndocs_df = (docs\n             .filter(lambda x: int(get_latest_version(x).split(' ')[3]) > 2018)\n             .map(trim)\n             .compute())\n\n# convert to pandas\ndocs_df = pd.DataFrame(docs_df)\n\n# add general category. we are going to use as our target variable\ndocs_df['general_category'] = docs_df.category.apply(lambda x:[a.split('.')[0] for a in x])\n\n# convert general category into label columns\nmlb = MultiLabelBinarizer()\nlabels = mlb.fit_transform(docs_df.general_category)\n\n# concatenate with the abstracts\ndf = pd.concat([docs_df[['abstract','title']], pd.DataFrame(labels)], axis=1)\ndf.columns = ['abstract','title'] + list(mlb.classes_)\n\n# shuffle or sample and keep columns that have at least 2 positive examples\nsample_df = df.sample(frac = 1, random_state = SEED)\nkeep = sample_df.iloc[:,2:].apply(sum) > 1\nsample_df = pd.concat([sample_df.iloc[:,:2],sample_df.iloc[:,2:].iloc[:,keep.values]], axis = 1)\n\ncategories = sample_df.columns[2:]\n\n# removed categories\nprint('Removed following categories from training : {}'.format(str(keep[~keep].index.to_list())))\n\n# split into train and test\ntrain, test = train_test_split(sample_df, random_state=SEED, test_size=0.15, shuffle=True)\n\nX_train = train.abstract\nX_test = test.abstract\n\nprint(X_train.shape)\nprint(X_test.shape)\n\n# Now you can use the X_test and test dataframes to evaluate your model","44c9396e":"from sklearn.metrics import accuracy_score, hamming_loss\n\n# to evaluate the multi label classification model use below metrics\nprint('Hamming loss : {}'.format(hamming_loss(y_true,y_pred)))\nprint('Accuracy : {}'.format(accuracy_score(y_true,y_pred)))","7d44a4f3":"def hamming_score(y_true, y_pred, normalize=True, sample_weight=None):\n    '''\n    Compute the Hamming score (a.k.a. label-based accuracy) for the multi-label case\n    http:\/\/stackoverflow.com\/q\/32239577\/395857\n    '''\n    acc_list = []\n    for i in range(y_true.shape[0]):\n        set_true = set( np.where(y_true[i])[0] )\n        set_pred = set( np.where(y_pred[i])[0] )\n        #print('\\nset_true: {0}'.format(set_true))\n        #print('set_pred: {0}'.format(set_pred))\n        tmp_a = None\n        if len(set_true) == 0 and len(set_pred) == 0:\n            tmp_a = 1\n        else:\n            tmp_a = len(set_true.intersection(set_pred))\/\\\n                    float( len(set_true.union(set_pred)) )\n        #print('tmp_a: {0}'.format(tmp_a))\n        acc_list.append(tmp_a)\n    return np.mean(acc_list)\n\nprint('Hamming Score : {}'.format(hamming_score(y_true,y_pred)))","8a171cb6":"In this notebook, I will try to analyze the arXiv json file. Due to its huge size I am not able to just load it using kaggle notebooks with pandas `read_json` function. To solve for that I will be using **dask**. Dask allows us to use the distributed processing capabilities of our machines as well as lazy evaluation.\n\n![](https:\/\/tutorial.dask.org\/_images\/dask_horizontal.svg)\n\nWith the help of dask, pandas, sklearn and other modules I am planning to do \n* light exploratory analysis of the arXiv database.\n* multi-label classification for papers published after **2019-01-01**.\n\n# Reading data in","0127354c":"## Preprocess data","0e045eca":"# Competition: Prepare data for multi label classification \n\nIf you would like to compete in [Multi-label Text Classification task](https:\/\/www.kaggle.com\/Cornell-University\/arxiv\/tasks?taskId=1757) use below code to prepare your test data. Or just download the `validation.json` from this notebook.","1dae0c2c":"## Train Models for each Category","3318d5fb":"# Multi Label Classification\n\nFor multi label classification we will only need document id, categories and abstract. So let's exract those fields, discard everything else and convert to a pandas dataframe.","abe19fe0":"# Exploratory Analysis","e2746c05":"## Evaluate Model Performance","2fa97e02":"Once we compare the distribution of the predictions vs the actual categories we can see how far off our models are. And it looks like we did an alright job. Both distributions look very similar to each other. ","9ee13b77":"Now, that we have abstracts and all associated categories we can start to train models. We will train a model for each category."}}