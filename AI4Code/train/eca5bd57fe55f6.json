{"cell_type":{"59331084":"code","b2a8ded1":"code","1be19ef9":"code","72c6564c":"code","0b89b09c":"code","cdb8b21c":"code","a146297a":"code","07e7f734":"code","8149669b":"code","3436168c":"code","bc7d875b":"code","44a11944":"code","a760a1d5":"code","47d53a17":"code","43d7bb3b":"code","99d2d0eb":"code","827be49a":"code","9a78e54b":"code","aa837b86":"code","b723fa13":"code","fee4c925":"code","a436dadb":"code","0a8cae98":"code","fbd03236":"code","bdc3702e":"code","1af3f198":"code","69f1dcc2":"code","55c25091":"code","58922a17":"code","851b9917":"code","16c58446":"code","b52a1132":"code","5e89963f":"code","fb8a9490":"code","1c247234":"code","5b3ba521":"code","1b2b75fc":"code","4ffd8266":"code","486ea41d":"code","894a6f7b":"code","083e652a":"code","b8aca3c1":"code","61bdd997":"code","e0bb2b8a":"code","0d3486e4":"code","51dd6766":"code","e4ae0f32":"code","2fc522d5":"code","faebacd9":"code","4d18e00c":"code","1201926d":"code","6f83bab8":"markdown","6f49020e":"markdown","b525d351":"markdown","990a2cea":"markdown","afe41cbb":"markdown","ccf9a9a6":"markdown","1395868e":"markdown","0d4b1d82":"markdown","360077d4":"markdown","c1f0f57f":"markdown","c66334b5":"markdown","527a718c":"markdown","3a01e4c7":"markdown","ec715e24":"markdown","67450f77":"markdown","4a06d1c0":"markdown","1c421114":"markdown","c0af34a1":"markdown","54f8a434":"markdown"},"source":{"59331084":"## Min aim is to understand more about the data\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nfrom sklearn import preprocessing\nfrom xgboost import XGBRegressor\nimport sklearn.metrics as metrics\nimport math\nfrom scipy.stats import norm, skew\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\n## Display all the columns of the dataframe\n\npd.pandas.set_option('display.max_columns',None)","b2a8ded1":"dataset=pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')\n\n## print shape of dataset with rows and columns\nprint(dataset.shape)","1be19ef9":"## print the top5 records\ndataset.head()","72c6564c":"## 1 -step make the list of features which has missing values\nfeatures_with_na=[features for features in dataset.columns if dataset[features].isnull().sum()>1]\n## 2- step print the feature name and the percentage of missing values\n\nfor feature in features_with_na:\n    print(feature, np.round(dataset[feature].isnull().mean(), 4),  ' % missing values')","0b89b09c":"for feature in features_with_na:\n    data = dataset.copy()\n    \n    # let's make a variable that indicates 1 if the observation was missing or zero otherwise\n    data[feature] = np.where(data[feature].isnull(), 1, 0)\n    # let's calculate the mean SalePrice where the information is missing or present\n    data.groupby(feature)['SalePrice'].median().plot.bar()\n    plt.title(feature)\n    plt.show()\n","cdb8b21c":"print(\"Id of Houses {}\".format(len(dataset.Id)))\n","a146297a":"# list of numerical variables\nnumerical_features = [feature for feature in dataset.columns if dataset[feature].dtypes != 'O']\n\nprint('Number of numerical variables: ', len(numerical_features))\n\n# visualise the numerical variables\ndataset[numerical_features].head()","07e7f734":"# list of variables that contain year information\nyear_feature = [feature for feature in numerical_features if 'Yr' in feature or 'Year' in feature]\n\nyear_feature","8149669b":"# let's explore the content of these year variables\nfor feature in year_feature:\n    print(feature, dataset[feature].unique())\n","3436168c":"## Lets analyze the Temporal Datetime Variables\n## We will check whether there is a relation between year the house is sold and the sales price\n\ndataset.groupby('YrSold')['SalePrice'].median().plot()\nplt.xlabel('Year Sold')\nplt.ylabel('Median House Price')\nplt.title(\"House Price vs YearSold\")","bc7d875b":"year_feature","44a11944":"## Here we will compare the difference between All years feature with SalePrice\n\nfor feature in year_feature:\n    if feature!='YrSold':\n        data=dataset.copy()\n        ## We will capture the difference between year variable and year the house was sold for\n        data[feature]=data['YrSold']-data[feature]\n\n        plt.scatter(data[feature],data['SalePrice'])\n        plt.xlabel(feature)\n        plt.ylabel('SalePrice')\n        plt.show()\n","a760a1d5":"## Numerical variables are usually of 2 type\n## 1. Continous variable and Discrete Variables\n\ndiscrete_feature=[feature for feature in numerical_features if len(dataset[feature].unique())<25 and feature not in year_feature+['Id']]\nprint(\"Discrete Variables Count: {}\".format(len(discrete_feature)))","47d53a17":"discrete_feature","43d7bb3b":"dataset[discrete_feature].head()","99d2d0eb":"## Lets Find the realtionship between them and Sale PRice\nfor feature in discrete_feature:\n    data=dataset.copy()\n    data.groupby(feature)['SalePrice'].median().plot.bar()\n    plt.xlabel(feature)\n    plt.ylabel('SalePrice')\n    plt.title(feature)\n    plt.show()","827be49a":"## There is a relationship between variable number and SalePrice","9a78e54b":"continuous_feature=[feature for feature in numerical_features if feature not in discrete_feature+year_feature+['Id']]\nprint(\"Continuous feature Count {}\".format(len(continuous_feature)))\n","aa837b86":"## Lets analyse the continuous values by creating histograms to understand the distribution\n\nfor feature in continuous_feature:\n    data=dataset.copy()\n    data[feature].hist(bins=25)\n    plt.xlabel(feature)\n    plt.ylabel(\"Count\")\n    plt.title(feature)\n    plt.show()","b723fa13":"## We will be using logarithmic transformation\n\n\nfor feature in continuous_feature:\n    data=dataset.copy()\n    if 0 in data[feature].unique():\n        pass\n    else:\n        data[feature]=np.log(data[feature])\n        data['SalePrice']=np.log(data['SalePrice'])\n        plt.scatter(data[feature],data['SalePrice'])\n        plt.xlabel(feature)\n        plt.ylabel('SalesPrice')\n        plt.title(feature)\n        plt.show()","fee4c925":"for feature in continuous_feature:\n    data=dataset.copy()\n    if 0 in data[feature].unique():\n        pass\n    else:\n        data[feature]=np.log(data[feature])\n        data.boxplot(column=feature)\n        plt.ylabel(feature)\n        plt.title(feature)\n        plt.show()","a436dadb":"categorical_features=[feature for feature in dataset.columns if data[feature].dtypes=='O']\ncategorical_features\n","0a8cae98":"\ndataset[categorical_features].head()\n","fbd03236":"for feature in categorical_features:\n    print('The feature is {} and number of categories are {}'.format(feature,len(dataset[feature].unique())))","bdc3702e":"## Find out the relationship between categorical variable and dependent feature SalesPrice\nfor feature in categorical_features:\n    data=dataset.copy()\n    data.groupby(feature)['SalePrice'].median().plot.bar()\n    plt.xlabel(feature)\n    plt.ylabel('SalePrice')\n    plt.title(feature)\n    plt.show()","1af3f198":"y_train = dataset['SalePrice']\ntest_id = test['Id']\nall_data = pd.concat([dataset, test], axis=0, sort=False)\nall_data = all_data.drop(['Id', 'SalePrice'], axis=1)","69f1dcc2":"Total = all_data.isnull().sum().sort_values(ascending=False)\npercent = (all_data.isnull().sum() \/ all_data.isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([Total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_data.head(25)","55c25091":"all_data.drop((missing_data[missing_data['Total'] > 5]).index, axis=1, inplace=True)\nprint(all_data.isnull().sum().max())\n","58922a17":"total = all_data.isnull().sum().sort_values(ascending=False)\ntotal.head(19)","851b9917":"# filling the numeric data\nnumeric_missed = ['BsmtFinSF1',\n                  'BsmtFinSF2',\n                  'BsmtUnfSF',\n                  'TotalBsmtSF',\n                  'BsmtFullBath',\n                  'BsmtHalfBath',\n                  'GarageArea',\n                  'GarageCars']\n\nfor feature in numeric_missed:\n    all_data[feature] = all_data[feature].fillna(0)","16c58446":"#filling categorical data\ncategorical_missed = ['Exterior1st',\n                  'Exterior2nd',\n                  'SaleType',\n                  'MSZoning',\n                   'Electrical',\n                     'KitchenQual']\n\nfor feature in categorical_missed:\n    all_data[feature] = all_data[feature].fillna(all_data[feature].mode()[0])","b52a1132":"#Fill in the remaining missing values with the values that are most common for this feature.\n\nall_data['Functional'] = all_data['Functional'].fillna('Typ')\n","5e89963f":"all_data.drop(['Utilities'], axis=1, inplace=True)\n","fb8a9490":"all_data.isnull().sum().max() #just checking that there's no missing data missing...","1c247234":"numeric_feats = all_data.dtypes[all_data.dtypes != 'object'].index\nskewed_feats = all_data[numeric_feats].apply(lambda x: skew(x)).sort_values(ascending=False)\nhigh_skew = skewed_feats[abs(skewed_feats) > 0.5]\nhigh_skew","5b3ba521":"for feature in high_skew.index:\n    all_data[feature] = np.log1p(all_data[feature])","1b2b75fc":"#Add new feature\nall_data['TotalSF'] = all_data['TotalBsmtSF'] + all_data['1stFlrSF'] + all_data['2ndFlrSF']\n","4ffd8266":"#Converting the categorical to numerical\nall_data = pd.get_dummies(all_data)\nall_data.head()","486ea41d":"x_train =all_data[:len(y_train)]\nx_test = all_data[len(y_train):]","894a6f7b":"x_test.shape , x_train.shape\n","083e652a":"from xgboost import XGBRegressor\nxgbr = XGBRegressor()","b8aca3c1":"n_estimators = [1700, 1900, 2100, 2300, 2500,]\nmax_depth = [2, 3, 5, 10, 15]\nbooster = ['gbtree', 'gblinear']\nlearning_rate = [0.05, 0.10, 0.15, 0.20]\nmin_child_weight = [1, 2, 3, 4, 5]\nbase_score = [0.25, 0.50, 0.75, 1.0]\n\nhyperparameters_grid = {\n    'n_estimators' : n_estimators,\n    'max_depth' : max_depth,\n    'booster' : booster,\n    'learning_rate' : learning_rate,\n    'min_child_weight' : min_child_weight,\n    'base_score' : base_score\n}\n","61bdd997":"from sklearn.model_selection import RandomizedSearchCV\nrandom_cv = RandomizedSearchCV(estimator=xgbr,\n                               param_distributions= hyperparameters_grid,\n                              cv = 5, n_iter= 10, n_jobs= 4, verbose= 5,\n                              return_train_score= True, random_state= 42)","e0bb2b8a":"#random_cv.fit(x_train, y_train)","0d3486e4":"#random_cv.best_estimator_","51dd6766":"tuned_xgbr = XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n             importance_type='gain', interaction_constraints='',\n             learning_rate=0.1, max_delta_step=0, max_depth=2,\n             min_child_weight=2, missing=None, monotone_constraints='()',\n             n_estimators=1900, n_jobs=4, num_parallel_tree=1, random_state=0,\n             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n             tree_method='exact', validate_parameters=1, verbosity=None)","e4ae0f32":"tuned_xgbr.fit(x_train, y_train)","2fc522d5":"tuned_xgbr_prediction = tuned_xgbr.predict(all_data)","faebacd9":"tuned_xgbr_prediction","4d18e00c":"predictions = pd.DataFrame(tuned_xgbr_prediction)\nsubmission = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv')\npred_ds = pd.concat([submission['Id'], predictions], axis=1)\npred_ds.columns = ['Id', 'SalePrice']","1201926d":"submission.to_csv('SalePrice_N_submission.csv', index = False)","6f83bab8":"## Feature Engineering","6f49020e":"### Numerical Variables ","b525d351":"From the Dataset we have 4 year variables. We have extract information from the datetime variables like no of years or no of days. One example in this specific scenario can be difference in years between the year the house was built and the year the house was sold. We will be performing this analysis in the Feature Engineering ","990a2cea":"Here With the relation between the missing values and the dependent variable is clearly visible.So We need to replace these nan values with something meaningful which we will do in the Feature Engineering section\n\nFrom the above dataset some of the features like Id is not required","afe41cbb":"# Divide the problem into five steps:\n<ol>\n  <li>Data Analysis<\/li>\n  <li>Feature Engineering.<\/li>\n  <li>Feature Selection.<\/li>\n  <li>Model Building<\/li>\n  <li>Model Deployment<\/li>\n<\/ol>","ccf9a9a6":"### Outliers ","1395868e":"Fix The Skewness in the other features","0d4b1d82":"# In Data Analysis We will Analyze To Find out the below things \n\n<ol>\n  <li>Missing Values<\/li>\n  <li>All The Numerical Variables<\/li>\n  <li>Distribution of the Numerical Variables<\/li>\n  <li>Categorical Variables<\/li>\n  <li>Cardinality of Categorical Variables<\/li>\n  <li>Outliers<\/li>\n  <li>Relationship between independent and dependent feature(SalePrice)<\/li>\n  \n<\/ol>\n","360077d4":"We will be performing all the below steps in Feature Engineering\n<ol>\n    <li>Missing values<\/li>\n    <li>Temporal variables<\/li>\n    <li>Categorical variables: remove rare labels<\/li>\n    <li>Standarise the values of the variables to the same range<\/li>\n<\/ol>","c1f0f57f":"Firstly concatenate train and test datasets, preprocess, and then divide them again.","c66334b5":"### Temporal Variables(Eg: Datetime Variables)","527a718c":"### Categorical Variables ","3a01e4c7":"### Continuous Variable","ec715e24":"As you seen that the features have many missing values, we will note that they are not important features, none of them has (correlation > 0.5), so if we delete them we will not miss the data.","67450f77":"# House Prices: Advanced Regression","4a06d1c0":"### Since they are many missing values, we need to find the relationship between missing values and Sales Price\nLet's plot some diagram for this relationship\n\n","1c421114":"The main aim of the problem was to predict the sale price of a house based on a set of features such as the number of bedrooms, the neighbourhood within Ames, etc.","c0af34a1":"We cleaned the data very well, and now let's separate the data to its origin (train, test)","54f8a434":"## Model"}}