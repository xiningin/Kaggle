{"cell_type":{"9c60f68b":"code","96e888f6":"code","e54810cd":"code","a6d6702d":"code","7aa04724":"code","1306c902":"code","730c1f13":"code","244ddb00":"code","3f1bfc15":"code","f09be67e":"code","bb3f56c8":"code","cbe19f61":"code","abeae2fc":"code","985cfaa9":"code","24b61492":"code","77eb440f":"code","5c5de2a3":"code","4fb29cdd":"code","eee12c0b":"code","c939f169":"code","432aa686":"code","b38508a2":"code","eafb640c":"code","11815fc9":"code","32655931":"code","4c1f98fd":"code","5a3ac766":"code","cf9c78b4":"code","ba28bc58":"code","526bfbca":"code","c2887832":"code","efa28e7b":"code","3e78f6f3":"code","5a8dee12":"markdown","fc4a1088":"markdown","2547f54e":"markdown","17234b68":"markdown","e219d48a":"markdown","58214536":"markdown","00e64b65":"markdown","73eaab8a":"markdown","a66e0c7a":"markdown","e278341c":"markdown","bcdeae96":"markdown","5e73550d":"markdown","3cf47a47":"markdown"},"source":{"9c60f68b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","96e888f6":"os.chdir(\"..\/input\")\n#Importing the dataset\n\ndf_train = pd.read_csv('train.csv')\ndf_test= pd.read_csv('test.csv')\n","e54810cd":"# Checking the first few observation\ndf_train.head()","a6d6702d":"#Checking for duplicated values\ndf_train.duplicated().sum()","7aa04724":"#df_train.corr()","1306c902":"#Checking the count of target variable\ndf_train.label.value_counts()","730c1f13":"#Creating X_train, and y_train\n\ny_train = df_train['label']\nX_train = df_train.drop('label', axis = 1)","244ddb00":"#checking the statistics of y_train\nprint(y_train.value_counts().sort_index())\nprint(y_train.describe())","3f1bfc15":"#import the seaborn to check the distribution of target variable\n\nimport seaborn as sns\n\nsns.distplot(y_train)","f09be67e":"#Checking for null values in target variable\n\ny_train.isna().sum()\n","bb3f56c8":"X_train = X_train\/255\ndf_test = df_test\/255","cbe19f61":"from keras.utils import to_categorical\n\ny_train = to_categorical(y_train)","abeae2fc":"# Checking the uniue vectors formed \nunique_rows = np.unique(y_train, axis=0)\nunique_rows","985cfaa9":"def decode(datum):\n    return np.argmax(datum)","24b61492":"# Checking the mapped values\nfor i in range(unique_rows.shape[0]):\n    datum = unique_rows[i]\n    print('index: %d' % i)\n    print('encoded datum: %s' % datum)\n    decoded_datum = decode(unique_rows[i])\n    print('decoded datum: %s' % decoded_datum)\n    print()","77eb440f":"X_train = X_train.values.reshape(-1,28,28,1)\nX_train","5c5de2a3":"from sklearn.model_selection import train_test_split\n\nX_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size =0.1 , random_state = 42)","4fb29cdd":"df_test = df_test.values.reshape(-1,28,28,1)\ndf_test","eee12c0b":"g = plt.imshow(X_train[0][:,:,0])","c939f169":"from keras.models import Sequential\nfrom keras.layers import Conv2D, MaxPool2D, Dropout, Flatten, Dense\n","432aa686":"model = Sequential()\nmodel.add(Conv2D(filters = 64, padding= \"Same\", kernel_size = (2,2), activation = 'relu', input_shape = (28,28,1)))\nmodel.add(MaxPool2D(pool_size = (2,2)))\nmodel.add(Conv2D(filters= 32, kernel_size = (2,2), activation= \"relu\"))\nmodel.add(MaxPool2D(pool_size= (2,2)))\nmodel.add(Dropout(0.25))\nmodel.add(Conv2D(filters = 16, kernel_size = (2,2), activation = \"relu\", padding = \"Same\" ))\nmodel.add(MaxPool2D(pool_size= (2,2)))\nmodel.add(Flatten())\nmodel.add(Dense(256, activation = \"relu\"))\nmodel.add(Dropout(0.25))\nmodel.add(Dense(128, activation= \"relu\"))\nmodel.add(Dropout(0.33))\nmodel.add(Dense(10, activation = \"softmax\"))","b38508a2":"model.summary()","eafb640c":"from keras.utils import plot_model\n# plot graph\nplot_model(model, to_file='\/model_summary.png')","11815fc9":"#Defining optimizer\nfrom keras.optimizers import RMSprop\noptimizer = RMSprop(lr = 0.001, rho = 0.9, epsilon = 1e-08, decay =0.0 )","32655931":"#compile the model\nmodel.compile(optimizer = optimizer, loss = 'categorical_crossentropy', metrics = ['accuracy'])","4c1f98fd":"#Optimizing learning rate\nfrom keras.callbacks import ReduceLROnPlateau\n\nlearning_rate_reduction = ReduceLROnPlateau(monitor = 'val_acc',\n                                           patience = 3,\n                                           verbose = 1,\n                                           factor = 0.5,\n                                           min_lr = 0.00001)","5a3ac766":"#Agumenting the data \n\nfrom keras.preprocessing.image import ImageDataGenerator\n\ndatagen = ImageDataGenerator(featurewise_center = False,\n                            samplewise_center = False,\n                            featurewise_std_normalization= False,\n                            samplewise_std_normalization= False,\n                            zca_whitening= False,\n                            rotation_range = 10,\n                            zoom_range = 0.1,\n                            width_shift_range = 0.1,\n                            height_shift_range = 0.1,\n                            horizontal_flip= False,\n                            vertical_flip= False)\n\ndatagen.fit(X_train)","cf9c78b4":"batch_size = 75\nhistory = model.fit_generator(datagen.flow(X_train, y_train, batch_size = batch_size),\n                             epochs = 35, validation_data = (X_val, y_val),\n                             verbose = 1, steps_per_epoch = X_train.shape[0]\/\/batch_size,\n                             callbacks = [learning_rate_reduction])","ba28bc58":"#Plot the loss and accuracy curves for the training and validation set\n\nfig, ax  = plt.subplots(2,1)\nax[0].plot(history.history['loss'], color = 'b', label = 'Training loss')\nax[0].plot(history.history['val_loss'], color = 'r', label = 'Validation loss')\nlegend = ax[0].legend(loc ='best', shadow = True)\n\nax[1].plot(history.history['acc'], color = 'b', label = 'Training Accuracy')\nax[1].plot(history.history['val_acc'], color = 'r', label = 'Validation Accuracy')\nlegend = ax[1].legend(loc = 'best', shadow =True)","526bfbca":"### Plotting the confusion matrix\nfrom sklearn.metrics import confusion_matrix\ny_pred = model.predict(X_val)\ny_pred = np.argmax(y_pred, axis = 1)\ny_true = np.argmax(y_val, axis = 1)\ncm = confusion_matrix(y_true, y_pred)\n# plot the confusion matrix\nfig, ax = plt.subplots(figsize=(10,10))\nsns.heatmap(cm, cmap= \"YlGnBu\", annot=True, fmt='', ax=ax)\n","c2887832":"results = model.predict(df_test)\nresults = np.argmax(results, axis = 1)\nresults = pd.Series(results, name = 'Label')","efa28e7b":"os.chdir(\"..\/working\")","3e78f6f3":"submission = pd.concat([pd.Series(range(1,28001),name = \"ImageId\"),results],axis = 1)\n\nsubmission.to_csv(\"submission.csv\",index=False)","5a8dee12":"### Feature Scaling\n**We will be dividing all the observation by 255 as the range of pixel value lies between (0,255)**","fc4a1088":"**Below is the codes to change the target variable to categoriacal which is its desired form**","2547f54e":"**After checking the duplicated values we should also check whether there is column which is duplicated or not.**\n\n*Corr function of pandas will help us achieve the above question, if we have a correlation of 1 among two different columns we should definitely cross check the dataset*","17234b68":"### Predicting the Results","e219d48a":"### Plotting the Confusion Matrix\n\n**As we know accuracy is always not the correct parameter to judge the Performance of the model hence we will also be evaluating the confusion matrix for the above situation to validate our model**","58214536":"**The above plot help us vizualize that the values of target variable are equally distributed in the dataframe which is good per our objective as each category has equal amount of data approximately**","00e64b65":"## Data Preprocessing","73eaab8a":"### Training and Validation Curve","a66e0c7a":"### Reshaping the Data\n\n**Next we would reshape the data so that the data is in matrix form as we would be using CNN to fit the model and CNN take only matrix as input.**","e278341c":"**The above sum help us to understand that there is no null value in the target variable which is again a good sign for our objective.**","bcdeae96":"### Since we have the required data we will now split the data into two sets namely train and validate.\n**For this we will be using train_test_split method of Sklearn library**\n","5e73550d":"**Now as we have created the data in the required format, we now have to fit it in the input layer and then run our model**","3cf47a47":"### Evaluating the model\n**The Below curves will help us to validate the training and validation accuracy, which inturn will give us an overview weather the model is overfit or not.**"}}