{"cell_type":{"9362abd1":"code","99977b14":"code","b5ac3c11":"code","36d91dbf":"code","381d6ba1":"code","08b1cca2":"code","dc5110f3":"code","dcbf0cd0":"code","0588e52d":"code","0966e6dd":"code","39496d4c":"code","1c1bdd46":"code","5dd0c4fb":"code","ff47ee90":"code","132662fb":"code","9173e468":"code","5c3a3319":"code","f17d1b30":"code","17a5d1fa":"code","e0232dd0":"code","4f71b21c":"code","6cf7eaed":"code","bf38e201":"code","0f7b0df6":"code","6b85c604":"code","4a01d904":"code","587fba2f":"code","15928ab6":"code","f32c3676":"code","47b4d12a":"code","0d707cbb":"code","17cb43e8":"code","24bd088b":"markdown","0165f56f":"markdown","63ac436c":"markdown","18c6843a":"markdown","e9846194":"markdown","3b2c36aa":"markdown","2f467e8f":"markdown","738f4ef3":"markdown","f6275633":"markdown","49fec76f":"markdown","1622ef09":"markdown","257bf6fc":"markdown","80c8c8d2":"markdown","2a4a86ba":"markdown"},"source":{"9362abd1":"#import packages\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nimport math\nfrom sklearn.metrics import r2_score\nfrom sklearn.model_selection import cross_val_score","99977b14":"#Read data\ndf = pd.read_csv(\"..\/input\/vehicle-dataset-from-cardekho\/Car details v3.csv\")\nprint(df.head())\ndf.info()\ndf.shape","b5ac3c11":"#drop duplicate rows\ndf=df.drop_duplicates()\ndf.shape\ndf.info()","36d91dbf":"#check null values\ndf.isnull().any(axis=0)\ndf.loc[df.isnull().any(axis=1)]\ndf.isnull().sum(axis=0)\n","381d6ba1":"#Drop null values\ndf=df.dropna()\ndf.shape\ndf.duplicated()","08b1cca2":"years_driven=2021-df['year']\ndf['years_driven']=years_driven","dc5110f3":"df.head()","dcbf0cd0":"#drop 'name' , 'year','torque','brand','model'\ndel df['name']\ndel df['year']\ndel df['torque']\ndel df['seats']\ndf.head()","0588e52d":"#Clean mileage ,engine and max_power columns\na=df['mileage'].str.split(n=1,expand=True)\ndel df['mileage']\ndf['mileage(kmpl)']=a[0]\n\na=df['engine'].str.split(n=1,expand=True)\ndel df['engine']\ndf['engine(CC)']=a[0]\n\na=df['max_power'].str.split(n=1,expand=True)\ndel df['max_power']\ndf['max_power(bhp)']=a[0]","0966e6dd":"#change 3 columns mileage(kmpl),engine(CC),max_power(bhp) datatypes to numeric \ndf['mileage(kmpl)']=df['mileage(kmpl)'].astype(float)\ndf['engine(CC)']=df['engine(CC)'].astype(int)\ndf['max_power(bhp)']=df['max_power(bhp)'].astype(float)","39496d4c":"# get dummies\ndf= pd.get_dummies(data = df,drop_first=True) \ndf.head()","1c1bdd46":"# MINMAXSCALER\nfrom sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\nnum_vars = ['km_driven', 'years_driven','mileage(kmpl)','engine(CC)','max_power(bhp)']\ndf[num_vars] = scaler.fit_transform(df[num_vars])","5dd0c4fb":"#correlation to selling_price\ndf.corr()['selling_price']","ff47ee90":"#delete features with low correlation to selling_price\ndel df['fuel_LPG']\ndel df['seller_type_Trustmark Dealer']\ndel df['owner_Fourth & Above Owner']\ndf.head()","132662fb":"#Check dataset info after changing columns\ndf.info()\ndf.shape\ndf.describe()","9173e468":"#columns correlation\ndf.corr()","5c3a3319":"df.describe()","f17d1b30":"plt.figure(figsize=(15,8))\nsns.lineplot(data=df, x=\"km_driven\", y=\"selling_price\")","17a5d1fa":"sns.pairplot(df[['selling_price','km_driven','mileage(kmpl)','engine(CC)','years_driven']],diag_kind='kde')","e0232dd0":"#Split data and labels\nY=df['selling_price']\nX=df.drop('selling_price',1)","4f71b21c":"#Split data into train and test\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=5310)\nprint(\"x train: \",X_train.shape)\nprint(\"x test: \",X_test.shape)\nprint(\"y train: \",Y_train.shape)\nprint(\"y test: \",Y_test.shape)","6cf7eaed":"#Def model training function\nCV = []\nR2_train = []\nR2_test = []\ndef price_predict_model(model):\n    # Training model\n    model.fit(X_train,Y_train)\n            \n    # R2 score of train set\n    y_pred_train = model.predict(X_train)\n    R2_train_model = r2_score(Y_train,y_pred_train)\n    R2_train.append(round(R2_train_model,2))\n    \n    # R2 score of test set\n    y_pred_test = model.predict(X_test)\n    R2_test_model = r2_score(Y_test,y_pred_test)\n    R2_test.append(round(R2_test_model,2))\n    \n    # R2 mean of train set using Cross validation\n    cross_val = cross_val_score(model ,X_train ,Y_train ,cv=10)\n    cv_mean = cross_val.mean()\n    CV.append(round(cv_mean,2))\n    \n                        \n    # Printing results\n    print(\"Train R2-score :\",round(R2_train_model,2))\n    print(\"Test R2-score :\",round(R2_test_model,2))\n    print(\"Train CV scores :\",cross_val)\n    print(\"Train CV mean :\",round(cv_mean,2))\n    \n    # summary\n    import statsmodels.api as sm\n    from scipy import stats\n\n    X2 = sm.add_constant(X_train)\n    est = sm.OLS(Y_train, X2)\n    est2 = est.fit()\n    print(est2.summary())\n    \n    #Plot\n    plt.scatter(Y_test, y_pred_test,  color='blue')\n    plt.xlabel('Y_test')\n    plt.ylabel('Y-pred_test')\n\n    plt.xticks(())\n    plt.yticks(())\n\n    plt.show()\n    #plot2\n    print('\\nResidual plot for training data (blue) and test data (green):')\n    _ = plt.scatter(y_pred_train, Y_train-y_pred_train, c='blue', s=40, alpha=0.5, edgecolor='white')\n    _ = plt.scatter(y_pred_test, Y_test-y_pred_test, c='green', s=40, alpha=0.5, edgecolor='white')\n    _ = plt.plot([-10,10], [0,0], c='black')\n    _ = plt.ylabel('Residuals ($y - \\hat{y}$)')\n    _ = plt.xlabel('Predicted values ($\\hat{y}$)')\n    \n    plt.show()\n    \n    #hypothesis\n    from scipy import stats\n    import numpy as np\n\n    mwu_result = stats.mannwhitneyu(Y_test,y_pred_test)\n    print(mwu_result)\n    print('Can we reject H0:The prediction and real price is close:', 'Yes' if mwu_result.pvalue<0.05 else 'No')\n\n","bf38e201":"# Linear Regression\nlm = LinearRegression()\nlmfit = lm.fit(X_train, Y_train)","0f7b0df6":"print('Intercept:', lm.intercept_)\nprint('Coefficients:\\n', lm.coef_)\n\n\n# We use the score method to get r-squared\nprint('\\nR-squared:', lm.score(X_train, Y_train))\n\n\n# We can also calculate the standard error\nstderr = math.sqrt(np.mean((Y_train - lm.predict(X_train))**2))\nprint('\\nStandard error:', stderr)\n\nprice_predict_model(lm)","6b85c604":"from sklearn.linear_model import Ridge\nfrom sklearn.model_selection import RandomizedSearchCV\n\n# Creating Ridge model object\nrg = Ridge()\n# range of alpha \nalpha = np.logspace(-2,2)\n\n# Creating RandomizedSearchCV to find the best estimator of hyperparameter\nridge = RandomizedSearchCV(estimator = rg, param_distributions = dict(alpha=alpha))\n\nprice_predict_model(ridge)\n\n","4a01d904":"#best estimator for ridge\nprint(\"Best estimator:\",ridge.best_estimator_)","587fba2f":"from sklearn.linear_model import Lasso\nfrom sklearn.model_selection import RandomizedSearchCV\n\nls = Lasso()\nalpha = np.logspace(-2,2) # range for alpha\n\nlasso = RandomizedSearchCV(estimator = ls, param_distributions = dict(alpha=alpha))\nprice_predict_model(lasso)\n\n","15928ab6":"#best estimator for lasso\nprint(\"Best estimator:\",lasso.best_estimator_)","f32c3676":"# Random Forest before tuning\nfrom sklearn.ensemble import RandomForestRegressor\nrf=RandomForestRegressor(random_state = 5310)\nrf_fit=rf.fit(X_train, Y_train)\nprice_predict_model(rf_fit)","47b4d12a":"# Random Forest parameter tunning\nfrom sklearn.model_selection import GridSearchCV\nparam_grid = {\"n_estimators\":range(10,500,20)}\ngrid_search = GridSearchCV(RandomForestRegressor(),param_grid,cv = 3)\n\ngrid_search.fit(X_train,Y_train)\ngrid_search.best_params_, grid_search.best_score_","0d707cbb":"#random forest after tunning\nfrom sklearn.ensemble import RandomForestRegressor\nrf=RandomForestRegressor(random_state = 5310,n_estimators=50)\nrf_fit=rf.fit(X_train, Y_train)\nprice_predict_model(rf_fit)","17cb43e8":"Models = [\"LinearRegression\",\"Ridge Regression\",\"Lasso Regression\",\"Random Forest\",\"Random Forest(Tuned)\"]\nresults=pd.DataFrame({'Model': Models,'R Squared(Train)': R2_train,'R Squared(Test)': R2_test,'CV score mean(Train)': CV})\ndisplay(results)","24bd088b":"## Data Preprocessing","0165f56f":"# Conclusion","63ac436c":"# IDA","18c6843a":"## Load Data","e9846194":"### Feature Engineering(get dummies)","3b2c36aa":"## Train-Test Split","2f467e8f":"### Random Forest Regression","738f4ef3":"### Lasso Regression","f6275633":"#### References\nstackoverflow,\u2019Find p-value (significance) in scikit-learn LinearRegression\u2019,viewed 26 May 2021, https:\/\/stackoverflow.com\/questions\/27928275\/find-p-value-significance-in-scikit-learn-linearregression\n\nScikit learn, \u2018Linear Regression Example\u2019,viewed 28 May 2021, https:\/\/scikit-learn.org\/stable\/auto_examples\/linear_model\/plot_ols.html\n\nKaggle, \u2018Car price prediction\u2019, viewed 28 May 2021, https:\/\/www.kaggle.com\/mohaiminul101\/car-price-prediction\n\nStack abuse, \u2018Linear Regression in Python with Scikit-Learn\u2019, viewed 30 May 2021,\nhttps:\/\/stackabuse.com\/linear-regression-in-python-with-scikit-learn\/\n\nCSDN,\u2019\u3010\u673a\u5668\u5b66\u4e60\u5c0f\u8bba\u6587\u3011sklearn\u968f\u673a\u68ee\u6797RandomForestRegressor\u4ee3\u7801\u53ca\u8c03\u53c2\u2019, viewed 30 may 2021, https:\/\/blog.csdn.net\/xiaohutong1991\/article\/details\/108178143\n\ncnblogs,\u2019scikit-learn\u968f\u673a\u68ee\u6797\u8c03\u53c2\u5c0f\u7ed3\u2019, viewed 31 May 2021, https:\/\/www.cnblogs.com\/pinard\/p\/6160412.html\n\nNumpy, \u2018numpy.logspace\u2019, viewed 31 May 2021,\nhttps:\/\/numpy.org\/doc\/stable\/reference\/generated\/numpy.logspace.html","49fec76f":"## Training Models","1622ef09":"### Ridge Regression","257bf6fc":"After parameter tuning RF score raised to 0.98 train score and 0.91 test score.","80c8c8d2":"I used three regression model to predict car price, they all get very familiar result. The highest R^2 train and R^2 test are Linear Regression and Ridge Regression, the highest CV score mean Train is Ridge 0.62. In conclusion, Ridge Regression is the best model in this prediction.","2a4a86ba":"### Linear Regression"}}