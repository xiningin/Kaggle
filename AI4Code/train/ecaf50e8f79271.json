{"cell_type":{"068283fa":"code","895bb4df":"code","d87e1c04":"code","ea8999d0":"code","210ebb6a":"code","a3258c8b":"code","46773d26":"code","b8ffb777":"code","b827d994":"code","de72f353":"code","9b01157d":"code","d8846283":"code","4348afc9":"code","810e9414":"code","8cbcb456":"code","b7c652c4":"markdown"},"source":{"068283fa":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","895bb4df":"import tensorflow as tf\ntf.__version__","d87e1c04":"path_to_file = '..\/input\/original-json-schema\/json_schema.txt'","ea8999d0":"text = open(path_to_file, 'r',encoding='utf-8',\n                 errors='ignore').read()","210ebb6a":"print(text[:1000])","a3258c8b":"# The unique characters in the file\nvocab = sorted(set(text))\nprint(vocab)\nlen(vocab)","46773d26":"char_to_ind = {u:i for i, u in enumerate(vocab)}\nind_to_char = np.array(vocab)\nencoded_text = np.array([char_to_ind[c] for c in text])\nseq_len = 250\ntotal_num_seq = len(text)\/\/(seq_len+1)\ntotal_num_seq","b8ffb777":"# Create Training Sequences\nchar_dataset = tf.data.Dataset.from_tensor_slices(encoded_text)\n\nsequences = char_dataset.batch(seq_len+1, drop_remainder=True)\n\ndef create_seq_targets(seq):\n    input_txt = seq[:-1]\n    target_txt = seq[1:]\n    return input_txt, target_txt\n\ndataset = sequences.map(create_seq_targets)","b827d994":"# Batch size\nbatch_size = 128\n\n# Buffer size to shuffle the dataset so it doesn't attempt to shuffle\n# the entire sequence in memory. Instead, it maintains a buffer in which it shuffles elements\nbuffer_size = 10000\n\ndataset = dataset.shuffle(buffer_size).batch(batch_size, drop_remainder=True)\n\n# Length of the vocabulary in chars\nvocab_size = len(vocab)\n\n# The embedding dimension\nembed_dim = 64\n\n# Number of RNN units\nrnn_neurons = 2052","de72f353":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import LSTM,Dense,Embedding,Dropout,GRU\nfrom tensorflow.keras.losses import sparse_categorical_crossentropy","9b01157d":"def sparse_cat_loss(y_true,y_pred):\n    return sparse_categorical_crossentropy(y_true, y_pred, from_logits=True)","d8846283":"def create_model(vocab_size, embed_dim, rnn_neurons, batch_size):\n    model = Sequential()\n    model.add(Embedding(vocab_size, embed_dim,batch_input_shape=[batch_size, None]))\n    model.add(GRU(rnn_neurons,return_sequences=True,stateful=True,recurrent_initializer='glorot_uniform'))\n    # Final Dense Layer to Predict\n    model.add(Dense(vocab_size))\n    model.compile(optimizer='adam', loss=sparse_cat_loss) \n    return model","4348afc9":"model = create_model(\n  vocab_size = vocab_size,\n  embed_dim=embed_dim,\n  rnn_neurons=rnn_neurons,\n  batch_size=batch_size)","810e9414":"model.summary()","8cbcb456":"epochs = 3 ","b7c652c4":"Das War's Kaggle Notebook Runner: Mar\u00edlia Prata  @mpwolke"}}