{"cell_type":{"228363f8":"code","22f6246e":"code","fa9481cd":"code","58b267a2":"code","ef51d0d6":"code","e3c01159":"code","09192e90":"code","4017a958":"code","7f361a8b":"code","92e59905":"code","3a2bbab0":"code","dadfec16":"code","258d2c3f":"code","9609a52a":"code","d771855a":"code","b1a53d28":"code","f4a3a5b5":"code","d066a0c1":"code","8ad1d69b":"code","1da9ba82":"code","07cd0a71":"code","e31c7b8b":"code","52fe5808":"code","c045cf6a":"code","60970aa6":"code","a78471ec":"code","85fe70de":"code","d222ecef":"code","4b4e0a3e":"code","fe7f819d":"code","db222e04":"markdown","a70eb4c5":"markdown","d6095ed7":"markdown","44ae2952":"markdown","496f5a00":"markdown","f3f107fa":"markdown","e68662f6":"markdown","f5f9e5f3":"markdown","2749bd41":"markdown","552b968e":"markdown"},"source":{"228363f8":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session\n\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import LSTM","22f6246e":"covid_19_data = pd.read_csv('\/kaggle\/input\/novel-corona-virus-2019-dataset\/covid_19_data.csv', index_col = 'ObservationDate')\ncovid_19_data","fa9481cd":"covid_19_data['Country\/Region'].value_counts()","58b267a2":"\ndata_from_rus = covid_19_data[covid_19_data['Country\/Region'].isin(['Russia'])]\ndata_from_rus","ef51d0d6":"data = data_from_rus.groupby('ObservationDate').sum()['Confirmed']\ndata","e3c01159":"# xxx = pd.DataFrame([['05\/29\/2021',2,'moscow'],\n#                     ['05\/29\/2021',20, 'moscow'],\n#                     ['01\/31\/2020',4,'NaN'], \n#                     ['02\/02\/2020',5,'NaN']],\n#                    columns=['ObservationDate','Confirmed','Province\/State'])\n# xxx.groupby('ObservationDate').sum()['Confirmed']","09192e90":"data.sort_values()","4017a958":"df = np.array(data.sort_values())\ndf[:10]","7f361a8b":"data_from_rus['Confirmed'].plot( figsize = (16,5))\n\nplt.legend()\nplt.grid('On')\nplt.show()","92e59905":"scaler = StandardScaler()\ndataset = scaler.fit_transform(df.reshape(-1, 1))\ndataset[0][:10]","3a2bbab0":"train_size = int(len(dataset) * 0.67)\ntest_size = len(dataset) - train_size\ntrain, test = dataset[0:train_size,:], dataset[train_size:len(dataset),:]\nprint('train shape ', train.shape)\nprint('test shape ', test.shape)","dadfec16":"# \u0424\u0443\u043d\u043a\u0446\u0438\u044f \u0441\u043e\u0437\u0434\u0430\u043d\u0438\u044f \u0434\u0430\u0442\u0430\u0441\u0435\u0442\u0430\ndef create_dataset(dataset, look_back=1):\n\tdataX, dataY = [], []\n\tfor i in range(len(dataset)-look_back-1):\n\t\ta = dataset[i:(i+look_back), 0]\n\t\tdataX.append(a)\n\t\tdataY.append(dataset[i + look_back, 0])\n\treturn np.array(dataX), np.array(dataY)","258d2c3f":"# \u043a\u043e\u043b-\u0432\u043e \u0441\u043b\u0435\u0434\u0443\u044e\u0449\u0438\u0445 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0439\nlook_back = 40 \n\n# train test split\ntrainX, trainY = create_dataset(train, look_back)\ntestX, testY = create_dataset(test, look_back)\n\nprint('trainX ',trainX.shape)\nprint('testX ',testX.shape)","9609a52a":"# reshape input to be [samples, time steps, features]\ntrainX = np.reshape(trainX, (trainX.shape[0], trainX.shape[1], 1))\ntestX = np.reshape(testX, (testX.shape[0], testX.shape[1], 1))","d771855a":"trainX.shape","b1a53d28":"batch_size = 1\n\nmodel = Sequential()\nmodel.add(LSTM(128, batch_input_shape=(batch_size, look_back, 1), stateful=True))\nmodel.add(Dense(1))\n\nmodel.compile(loss='mean_squared_error', optimizer='sgd')\nfor i in range(60):\n\tmodel.fit(trainX, trainY, epochs=1, batch_size=batch_size, verbose=2, shuffle=False)\n\tmodel.reset_states()","f4a3a5b5":"trainPredict = model.predict(trainX, batch_size=batch_size)\nmodel.reset_states()\ntestPredict = model.predict(testX, batch_size=batch_size)","d066a0c1":"# trainPredict","8ad1d69b":"trainPredict = scaler.inverse_transform(trainPredict)\ntrainY = scaler.inverse_transform([trainY])\ntestPredict = scaler.inverse_transform(testPredict)\ntestY = scaler.inverse_transform([testY])","1da9ba82":"import math\ntrainScore = math.sqrt(mean_squared_error(trainY[0], trainPredict[:,0]))\nprint('Train Score: %.2f RMSE' % (trainScore))\ntestScore = math.sqrt(mean_squared_error(testY[0], testPredict[:,0]))\nprint('Test Score: %.2f RMSE' % (testScore))","07cd0a71":"testPredict[-9:-1]","e31c7b8b":"realdata = data.sort_values()\nrealdata[-9:-1]","52fe5808":"# realdata.index[-150:]\npredicted_data = pd.DataFrame(testPredict, index=realdata.index[-120:])\npredicted_data","c045cf6a":"plt.figure(figsize=(14,5))\n\n# plt.plot(realdata[-150:], )\nplt.plot(realdata, label='real data')\nplt.plot(predicted_data, label='predicted')\nplt.xticks([realdata.index[0], predicted_data.index[0] , realdata.index[-1]])\nplt.vlines(predicted_data.index[0], realdata.min(), realdata.max(), color = 'r', label='split')\n# plt.grid('on')\nplt.legend()\nplt.xlabel('\u0434\u043d\u0438')\nplt.show()","60970aa6":"testPredict.shape","a78471ec":"from datetime import datetime, timedelta\n# realdata.index[-1]\n# future_predicts = np.array([])\n# future_predicts = np.append(future_predicts, 1)\n# future_predicts = np.append(future_predicts, 2)\n# future_predicts\n# future_predicts = pd.DataFrame(columns = ['ObservationDate', 'Confirmed'])#, index= 'ObservationDate')\n# future_predicts.append({'ObservationDate':'25\/01\/2021','Confirmed':'150'}, ignore_index=True)\n\n# predicted_data = pd.DataFrame(testPredict, index=realdata.index[-151:-1])\n# predicted_data\n# dt = datetime.strptime(realdata.index[-1], '%m\/%d\/%Y')\n# first_date = np.array([dt + timedelta(days=1)])\n# first_date[0].strftime('%m\/%d\/%Y')\n# date_index = pd.DataFrame([realdata.index[-1]], columns=['ObservationDate'])\n# date_index\n","85fe70de":"def future_predict(model, last_look_back_days, future_days=50):\n    future_predicts = np.array([])\n    dt = datetime.strptime(realdata.index[-1], '%m\/%d\/%Y')\n    dates = np.array([])\n    \n    for day in range(future_days):\n        dataset = scaler.fit_transform(last_look_back_days.reshape(-1, 1))\n#         dataset = scaler.fit_transform(last_look_back_days)\n        testX, testY = create_dataset(dataset, look_back=len(last_look_back_days)-2)\n        testX = np.reshape(testX, (testX.shape[0], testX.shape[1], 1))\n        new_predict = model.predict(testX, batch_size=batch_size)\n        new_predict = scaler.inverse_transform(new_predict)\n        future_predicts = np.append(future_predicts, new_predict)\n        \n        dates = np.append(dates, [dt + timedelta(days=day)])\n        dates[day].strftime('%m\/%d\/%Y')\n        \n        model.reset_states()\n        last_look_back_days = np.delete(last_look_back_days, 0)\n        last_look_back_days = np.append(last_look_back_days, new_predict)\n#         new_day_confirmed = testX[-1]\n    predicts = pd.DataFrame(future_predicts, index=dates)\n    predicts\n    return predicts\n        ","d222ecef":"predicts = future_predict(model, predicted_data.values[-100:], future_days=100)\npredicts","4b4e0a3e":"# # np.append(realdata.index, str(predicts.index), )\n# # predicts.index\n# realdata.index\n# # datetime.strptime(realdata.index, '%m\/%d\/%Y')\n# for date in realdata.index:\n    ","fe7f819d":"plt.figure(figsize=(14,5))\n\n# plt.plot(realdata, label='real data')\nplt.plot(predicts, label='predicts')\n# plt.xticks([realdata.index[0], predicted_data.index[0] , realdata.index[-1]])\n# plt.vlines(predicted_data.index[0], realdata.min(), realdata.max(), color = 'r')\n# plt.grid('on')\nplt.legend()\nplt.xlabel('\u0434\u043d\u0438')\nplt.show()","db222e04":"train test split","a70eb4c5":"let's sort by date","d6095ed7":"First rows contains total info jf all Russia. Try to group by date","44ae2952":"\u041f\u043e\u043f\u0440\u043e\u0431\u0443\u044e \u0441\u0434\u0435\u043b\u0430\u0442\u044c \u043f\u0440\u043e\u0433\u043d\u043e\u0437 \u043d\u0430 \u0431\u0443\u0434\u0443\u0449\u0435\u0435","496f5a00":"morbidity chart","f3f107fa":"normalize data","e68662f6":"485 days observ","f5f9e5f3":"\u0414\u043b\u0438\u043d\u0430 \u0442\u0435\u0441\u0442\u0430 161, \u0430 \u043f\u0440\u0435\u0434\u0438\u043a\u0442 \u043c\u044b \u0432\u044b\u0434\u0430\u0435\u043c \u043d\u0430 151 (-10 \u0434\u043b\u044f look back)","2749bd41":"and make numpy 1D array","552b968e":"Data from many countries, make filter per Rus"}}