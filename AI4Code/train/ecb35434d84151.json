{"cell_type":{"515814b2":"code","5c63e5ea":"code","0380e39c":"code","cddf1f42":"code","b0c47a7f":"code","ff072de6":"code","835272df":"markdown","293ab62b":"markdown"},"source":{"515814b2":"import os\nimport numpy as np\nimport cv2\nimport scipy.io as sio\nimport time\n\nimport keras\nfrom keras.models import Sequential, Model\nfrom keras.layers import Input, concatenate, Conv2D, MaxPooling2D, Activation, UpSampling2D, BatchNormalization\nfrom keras.optimizers import Adam\nfrom keras import backend as K\nfrom keras.losses import binary_crossentropy\n\nimport pydicom\nfrom keras.preprocessing.image import load_img, img_to_array\nfrom time import time\nfrom sklearn.metrics import confusion_matrix","5c63e5ea":"def read_png_true(path):\n    img = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n    img_png = img.copy()\n    img_png = np.array(img_png.copy(), dtype=np.float64) \n    img_png \/= np.max(img_png)\n    img_png[img_png > 0] = 1\n    return img_png\n\ndef transform_to_hu(medical_image, image):\n    hu_image = image * medical_image.RescaleSlope + medical_image.RescaleIntercept\n    hu_image[hu_image < -1024] = -1024\n    return hu_image\n\ndef window_image(image, window_center, window_width):\n    window_image = image.copy()\n    img_min = window_center - (window_width \/ 2)\n    img_max = window_center + (window_width \/ 2)\n    window_image[window_image < img_min] = img_min\n    window_image[window_image > img_max] = img_max\n    return window_image\n\ndef resize_normalize(img):\n    img = np.array(img, dtype=np.float64)\n    img -= np.min(img)\n    img \/= np.max(img)\n    return img\n\ndef read_dicom(path, window_widht=400, window_level=60):\n    img_medical = pydicom.dcmread(path)\n    img_data = img_medical.pixel_array\n\n    img_hu = transform_to_hu(img_medical, img_data)\n    img_window = window_image(img_hu.copy(), window_level, window_widht)\n    img_window_norm = resize_normalize(img_window)\n\n    img_window_norm = np.expand_dims(img_window_norm, axis=2)   # (512, 512, 1)\n    img_ths = np.concatenate([img_window_norm, img_window_norm, img_window_norm], axis=2)   # (512, 512, 3)\n    return img_ths\n\ndef dice_coeff(y_true, y_pred):\n    smooth = 1.\n    y_true_f = K.flatten(y_true)\n    y_pred_f = K.flatten(y_pred)\n    intersection = K.sum(y_true_f * y_pred_f)\n    score = (2. * intersection + smooth) \/ (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n    return score\n\ndef dice_loss(y_true, y_pred):\n    loss = 1 - dice_coeff(y_true, y_pred)\n    return loss\n\ndef bce_dice_loss(y_true, y_pred):\n    loss = binary_crossentropy(y_true, y_pred) + dice_loss(y_true, y_pred)\n    return loss\n\ndef unet_bami(input_size, num_classes):\n    inputs = Input(input_size)\n    # 512\n    down0a = Conv2D(16, (3, 3), padding='same')(inputs)\n    down0a = BatchNormalization()(down0a)\n    down0a = Activation('relu')(down0a)\n    down0a = Conv2D(16, (3, 3), padding='same')(down0a)\n    down0a = BatchNormalization()(down0a)\n    down0a = Activation('relu')(down0a)\n    down0a_pool = MaxPooling2D((2, 2), strides=(2, 2))(down0a)\n    # 256\n    down0 = Conv2D(32, (3, 3), padding='same')(down0a_pool)\n    down0 = BatchNormalization()(down0)\n    down0 = Activation('relu')(down0)\n    down0 = Conv2D(32, (3, 3), padding='same')(down0)\n    down0 = BatchNormalization()(down0)\n    down0 = Activation('relu')(down0)\n    down0_pool = MaxPooling2D((2, 2), strides=(2, 2))(down0)\n    # 128\n    down1 = Conv2D(64, (3, 3), padding='same')(down0_pool)\n    down1 = BatchNormalization()(down1)\n    down1 = Activation('relu')(down1)\n    down1 = Conv2D(64, (3, 3), padding='same')(down1)\n    down1 = BatchNormalization()(down1)\n    down1 = Activation('relu')(down1)\n    down1_pool = MaxPooling2D((2, 2), strides=(2, 2))(down1)\n    # 64\n    down2 = Conv2D(128, (3, 3), padding='same')(down1_pool)\n    down2 = BatchNormalization()(down2)\n    down2 = Activation('relu')(down2)\n    down2 = Conv2D(128, (3, 3), padding='same')(down2)\n    down2 = BatchNormalization()(down2)\n    down2 = Activation('relu')(down2)\n    down2_pool = MaxPooling2D((2, 2), strides=(2, 2))(down2)\n    # 32\n    down3 = Conv2D(256, (3, 3), padding='same')(down2_pool)\n    down3 = BatchNormalization()(down3)\n    down3 = Activation('relu')(down3)\n    down3 = Conv2D(256, (3, 3), padding='same')(down3)\n    down3 = BatchNormalization()(down3)\n    down3 = Activation('relu')(down3)\n    down3_pool = MaxPooling2D((2, 2), strides=(2, 2))(down3)\n    # 16\n    down4 = Conv2D(512, (3, 3), padding='same')(down3_pool)\n    down4 = BatchNormalization()(down4)\n    down4 = Activation('relu')(down4)\n    down4 = Conv2D(512, (3, 3), padding='same')(down4)\n    down4 = BatchNormalization()(down4)\n    down4 = Activation('relu')(down4)\n    down4_pool = MaxPooling2D((2, 2), strides=(2, 2))(down4)\n    # 8\n    center = Conv2D(1024, (3, 3), padding='same')(down4_pool)\n    center = BatchNormalization()(center)\n    center = Activation('relu')(center)\n    center = Conv2D(1024, (3, 3), padding='same')(center)\n    center = BatchNormalization()(center)\n    center = Activation('relu')(center)\n    # center\n    up4 = UpSampling2D((2, 2))(center)\n    up4 = concatenate([down4, up4], axis=3)\n    up4 = Conv2D(512, (3, 3), padding='same')(up4)\n    up4 = BatchNormalization()(up4)\n    up4 = Activation('relu')(up4)\n    up4 = Conv2D(512, (3, 3), padding='same')(up4)\n    up4 = BatchNormalization()(up4)\n    up4 = Activation('relu')(up4)\n    up4 = Conv2D(512, (3, 3), padding='same')(up4)\n    up4 = BatchNormalization()(up4)\n    up4 = Activation('relu')(up4)\n    # 16\n    up3 = UpSampling2D((2, 2))(up4)\n    up3 = concatenate([down3, up3], axis=3)\n    up3 = Conv2D(256, (3, 3), padding='same')(up3)\n    up3 = BatchNormalization()(up3)\n    up3 = Activation('relu')(up3)\n    up3 = Conv2D(256, (3, 3), padding='same')(up3)\n    up3 = BatchNormalization()(up3)\n    up3 = Activation('relu')(up3)\n    up3 = Conv2D(256, (3, 3), padding='same')(up3)\n    up3 = BatchNormalization()(up3)\n    up3 = Activation('relu')(up3)\n    # 32\n    up2 = UpSampling2D((2, 2))(up3)\n    up2 = concatenate([down2, up2], axis=3)\n    up2 = Conv2D(128, (3, 3), padding='same')(up2)\n    up2 = BatchNormalization()(up2)\n    up2 = Activation('relu')(up2)\n    up2 = Conv2D(128, (3, 3), padding='same')(up2)\n    up2 = BatchNormalization()(up2)\n    up2 = Activation('relu')(up2)\n    up2 = Conv2D(128, (3, 3), padding='same')(up2)\n    up2 = BatchNormalization()(up2)\n    up2 = Activation('relu')(up2)\n    # 64\n    up1 = UpSampling2D((2, 2))(up2)\n    up1 = concatenate([down1, up1], axis=3)\n    up1 = Conv2D(64, (3, 3), padding='same')(up1)\n    up1 = BatchNormalization()(up1)\n    up1 = Activation('relu')(up1)\n    up1 = Conv2D(64, (3, 3), padding='same')(up1)\n    up1 = BatchNormalization()(up1)\n    up1 = Activation('relu')(up1)\n    up1 = Conv2D(64, (3, 3), padding='same')(up1)\n    up1 = BatchNormalization()(up1)\n    up1 = Activation('relu')(up1)\n    # 128\n    up0 = UpSampling2D((2, 2))(up1)\n    up0 = concatenate([down0, up0], axis=3)\n    up0 = Conv2D(32, (3, 3), padding='same')(up0)\n    up0 = BatchNormalization()(up0)\n    up0 = Activation('relu')(up0)\n    up0 = Conv2D(32, (3, 3), padding='same')(up0)\n    up0 = BatchNormalization()(up0)\n    up0 = Activation('relu')(up0)\n    up0 = Conv2D(32, (3, 3), padding='same')(up0)\n    up0 = BatchNormalization()(up0)\n    up0 = Activation('relu')(up0)\n    # 256\n    up0a = UpSampling2D((2, 2))(up0)\n    up0a = concatenate([down0a, up0a], axis=3)\n    up0a = Conv2D(16, (3, 3), padding='same')(up0a)\n    up0a = BatchNormalization()(up0a)\n    up0a = Activation('relu')(up0a)\n    up0a = Conv2D(16, (3, 3), padding='same')(up0a)\n    up0a = BatchNormalization()(up0a)\n    up0a = Activation('relu')(up0a)\n    up0a = Conv2D(16, (3, 3), padding='same')(up0a)\n    up0a = BatchNormalization()(up0a)\n    up0a = Activation('relu')(up0a)\n    # 512\n    classify = Conv2D(num_classes, (1, 1), activation='sigmoid')(up0a)\n\n    model = Model(inputs, classify)\n    model.compile(optimizer=Adam(lr=5e-4), loss=bce_dice_loss, metrics=[dice_coeff])\n    return model","0380e39c":"basic_dir = '..\/input\/urinary-stone-challenge\/'\nimages_dir_valid_dcm = basic_dir + 'Valid\/DCM\/'\nimages_dir_valid_mask = basic_dir + 'Valid\/Mask\/'\nimages_dir_test_dcm = basic_dir + 'Test\/DCM\/'\n\nimg_size = (512, 512, 3)\nimg_resize = (512, 512)\nvalid_idx = np.arange(601, 700+1)\ntest_idx = np.arange(701, 900+1)\n\nmodel = unet_bami(input_size=img_size, num_classes=1)\nmodel.load_weights('..\/input\/bami-model\/model_weight_30.h5')\nmodel.summary()","cddf1f42":"arr_IOU = []\nths_prob = 0.5\nfor idx in valid_idx:\n    ### True\n    input_true = read_png_true(images_dir_valid_mask + str(idx) + '.png')\n\n    ### Predict\n    img_dcm = read_dicom(images_dir_valid_dcm + str(idx) + '.dcm')\n    img = np.expand_dims(img_dcm, axis=0)\n    input_pred = model.predict(img)\n    input_pred = input_pred.squeeze()\n\n    input_pred[input_pred < ths_prob] = 0\n    input_pred[input_pred >= ths_prob] = 1\n\n    ### Confusions\n    y_true = np.asarray(input_true.flatten(), dtype=np.int32)\n    y_pred = np.asarray(input_pred.flatten(), dtype=np.int32)\n    cfs_mtx = confusion_matrix(y_true, y_pred, labels=[0, 1])\n    # print('TN: ', cfs_mtx[0][0], ', FP: ', cfs_mtx[0][1], ', FN: ', cfs_mtx[1][0], ', TP: ', cfs_mtx[1][1])\n\n    val_sum = cfs_mtx[0][1] + cfs_mtx[1][0] + cfs_mtx[1][1]\n    val_tp = cfs_mtx[1][1]\n    val_iou = np.asarray(val_tp, dtype=np.float64) \/ np.asarray(val_sum, dtype=np.float64)\n\n    arr_IOU.append(val_iou)\n    print('Num: ', idx, ', val_tp: ', val_tp, ', val_sum: ', val_sum, ', IOU: ', val_iou)\narr_IOU = np.asarray(arr_IOU)\nprint('Mean: ', np.mean(arr_IOU))","b0c47a7f":"### pip install openpyxl\nimport pandas as pd\ndf_excel = pd.DataFrame({'val_name': valid_idx, 'IOU': arr_IOU})\ndf_excel.to_excel('.\/Valid_IOU.xlsx')\n\nos.chdir('\/kaggle\/working')\nfrom IPython.display import FileLink\nFileLink('Valid_IOU.xlsx')","ff072de6":"Save_img_folder_mask ='.\/Test_Result_Mask\/'\nif not os.path.exists(Save_img_folder_mask):\n    os.makedirs(Save_img_folder_mask)\n    \nSave_img_folder_heatmap ='.\/Test_Result_Heatmap\/'\nif not os.path.exists(Save_img_folder_heatmap):\n    os.makedirs(Save_img_folder_heatmap)\n    \n    \nths_prob = 0.5\nfor idx in test_idx:\n    ### Predict\n    img_dcm = read_dicom(images_dir_test_dcm + str(idx) + '.dcm')\n    img = np.expand_dims(img_dcm, axis=0)\n    input_pred = model.predict(img)\n    input_pred = input_pred.squeeze()\n\n    input_pred[input_pred < ths_prob] = 0\n    input_pred[input_pred >= ths_prob] = 1\n\n    img_dcm = np.asarray(img_dcm.copy(), np.float64) * 255\n    img_pred = np.asarray(input_pred.copy(), np.float64) * 255\n    cv2.imwrite(Save_img_folder_mask + 'test_pred_' + str(idx) + '.png', img_pred)\n\n    mask_pred = img_dcm.copy()\n    for v_row in range(0, 512):\n        for v_col in range(0, 512):\n            if input_pred[v_row, v_col] != 0:\n                mask_pred[v_row, v_col, 0] = 0\n                mask_pred[v_row, v_col, 1] = 0\n    cv2.imwrite(Save_img_folder_heatmap + 'test_dcm_pred_' + str(idx) + '.png', mask_pred)\n    print('Num: ', idx, img_dcm.shape, img_pred.shape, mask_pred.shape)","835272df":"Validation IOU","293ab62b":"Saved Test Image "}}