{"cell_type":{"6c8a5a59":"code","54a3445c":"code","198ccc79":"code","3c5dc725":"code","f599518d":"code","3cbbafaa":"code","14a8afce":"code","73682d58":"code","7fd88803":"code","bb77e47e":"code","b8737086":"code","5b12ecdf":"code","fa718394":"code","7be286d3":"code","54396674":"code","cbad4934":"code","619deab9":"code","879c18d2":"code","9c9abd5d":"code","37b42bf7":"code","f68cb5a6":"code","fc588c6e":"code","2c1944b4":"code","5dafe346":"code","6b780754":"code","54375862":"code","82cbabc5":"code","0e9e1a0e":"code","a94eef86":"code","080c54ec":"code","e5562acd":"code","22a9a776":"code","0b0b53b4":"code","11d54062":"code","c22369d4":"code","2a75ac6c":"code","4f6d463c":"code","8c69649b":"code","5d3daea1":"code","e38cfe6c":"code","e0bd8442":"code","d02b7d4f":"code","23fce79d":"code","8f12f8ec":"code","11eb3805":"code","8ee871df":"code","d76ddf89":"code","88f812a0":"code","2228bf2e":"code","236863a9":"code","0acbe97d":"code","be211341":"code","1e08c0c9":"code","212c5391":"code","b79a9542":"code","219d8aee":"code","14096ec8":"code","f9537152":"code","27dde9ea":"code","6460d300":"code","188cebd1":"code","4637adbb":"code","83d3d015":"code","197d0f92":"code","1b245f6f":"code","93d67e8f":"code","cb8cb58d":"code","2bc4b3f2":"code","2b36be12":"code","895221cd":"code","1aa1fdff":"code","fdb399b6":"code","69e6dd52":"code","1f71d6b1":"code","29cb1c37":"code","596bc536":"code","b7b8b402":"code","6f680db4":"code","53354e79":"code","063f34a3":"code","946d0078":"code","969fe3a6":"code","1e016b08":"code","0336c93c":"code","ad15efa4":"code","b57a4a53":"code","75c1d6f4":"code","6a8d607c":"code","1915bef3":"code","8da74824":"code","20258bed":"code","877f1eec":"code","089a5a83":"code","c634f50f":"code","d987e086":"code","cb889537":"code","020f83f9":"code","d0eedaed":"code","bb5c759d":"code","2bb0b9aa":"code","7ce1369d":"code","2cfcd0f7":"code","9e040b06":"code","dc10b122":"code","e3112b87":"code","562dcf92":"code","9ddaeca7":"code","39432b99":"code","c2b9ae98":"code","1582955f":"code","2659b82a":"code","a125cec9":"code","223839b7":"code","ab48ca92":"code","33c8601c":"code","b4640a58":"code","512d0760":"code","5351dfe9":"code","d721c9bd":"code","7d73c20d":"code","dc913b9e":"code","b7a4453b":"code","c7a24cf5":"code","e0ba4956":"code","78530e8c":"code","693e5f7e":"code","029d693d":"code","f1c55421":"code","9df83934":"code","ec3b226e":"code","b6cece92":"code","f6ebbeb2":"code","032fe6bc":"code","35994ac4":"code","863a5cc8":"code","32a2a925":"code","6f920aba":"code","62a69ae5":"code","e8c8cda7":"code","27c2d7aa":"code","b69e83f9":"code","4a8082c5":"code","3c6f4f44":"code","3a249b1f":"code","def897e2":"code","00ffa8cf":"code","a72516ca":"code","44e2f123":"code","a5be4765":"code","8ab18fa7":"code","7867ea25":"code","fbb9b2e8":"code","1c601e27":"code","98d71b22":"code","0f079b04":"code","58f65ec7":"code","9a7f1fcc":"code","d617aaed":"code","5099e9d0":"code","4bb402df":"code","4473413b":"code","9fb866b8":"code","5400c1fa":"code","ccce6905":"code","9ae85433":"code","4b5ee64f":"code","a4b299c1":"code","95e6f7df":"code","3ada4f92":"code","4e6731a0":"code","6216ae10":"code","39ce3aff":"code","af583e69":"code","a9fcc338":"code","48d0f637":"code","f1857344":"code","aa760880":"code","ee3db59b":"code","ac938ec7":"code","1924420a":"code","2f8f8e30":"code","3a9fde92":"code","e1ae5b00":"code","b51076fc":"code","2348790d":"code","f020fcaa":"code","6188272b":"code","fb9ae21c":"code","bcaa9fff":"code","328184be":"code","5879ce25":"code","62876654":"code","3f48e1c9":"code","3b884f9a":"code","c352a857":"code","fac33bf9":"code","980fd1ff":"code","91ba990a":"code","2b8be4cc":"code","361512a3":"code","d52936e6":"code","12232077":"code","50cd4132":"code","d2f444ae":"code","e0d7b8f6":"markdown","117d6c8b":"markdown","3e29f752":"markdown","2263bde3":"markdown","bc248721":"markdown","fc39dc18":"markdown","ec91a666":"markdown","50117214":"markdown","7f0a07da":"markdown","dbcc2065":"markdown","1e15913c":"markdown","f7e27317":"markdown","c53c2fa5":"markdown","a5f1d09b":"markdown","c4c32b3b":"markdown","d4c17496":"markdown","228bfe9f":"markdown","c93e4f27":"markdown","0cc848ac":"markdown","48ea1bf7":"markdown","3b7526f2":"markdown","7c94648b":"markdown","396cfb25":"markdown","775456fc":"markdown","1bd768e9":"markdown","77c9fa96":"markdown","a7feaa68":"markdown","89a15c13":"markdown","6c669a19":"markdown","91623d82":"markdown","969cc5c7":"markdown","f9612f74":"markdown","de62fab2":"markdown","5ca08e95":"markdown","2170c30b":"markdown","7d94d8ce":"markdown","03bca4e3":"markdown","ac24b19d":"markdown","cf70cdde":"markdown","1afdd612":"markdown","bb0b44b0":"markdown","3649fa8c":"markdown","5c42bfe7":"markdown","e6de73d3":"markdown","f8a791c6":"markdown","c165c494":"markdown","060ea5fb":"markdown","5bd684e1":"markdown","cfae7298":"markdown","f5dbc768":"markdown","beedd247":"markdown","1c392fcc":"markdown","f5958da8":"markdown","2db2c9ee":"markdown","4513dac3":"markdown","7ba934a2":"markdown","8bf3066c":"markdown","dfa1f42e":"markdown","286faf73":"markdown","f9db1e52":"markdown","3234140c":"markdown","fc8071fb":"markdown","acb14993":"markdown"},"source":{"6c8a5a59":"import pandas as pd\nimport numpy as np","54a3445c":"data=pd.read_csv('..\/input\/bank-customer-churn-modeling\/Churn_Modelling.csv', sep=',')\ndata.head(10)","198ccc79":"data[data.Surname=='Hill']","3c5dc725":"len(pd.unique(data.Surname))","f599518d":"data=data.drop(['RowNumber','CustomerId', 'Surname'], axis='columns')","3cbbafaa":"data.describe()","14a8afce":"data.info()","73682d58":"def summary(data):\n    print('Shape: ' , data.shape)\n    return( pd.DataFrame({ \"Dtypes \":data.dtypes , \n                           \"NAs\":data.isnull().sum() ,\n                           \"uniques\":data.nunique() ,\n                            \"Levels\":[ data[i].unique() for i in data.columns]}))","7fd88803":"summary(data)","bb77e47e":"import matplotlib.pyplot as plt","b8737086":"fig = plt.figure(figsize=(15,10))\nax1=fig.add_subplot(221)\nax2=fig.add_subplot(222)\n\ng=ax1.hist(data['CreditScore'], bins=500, color='y', alpha=0.9)\ng=ax2.boxplot(data['CreditScore'])","5b12ecdf":"data=data.drop(data[data['CreditScore']<385].index)\nlen(data)","fa718394":"fig = plt.figure(figsize=(15,10))\nax1=fig.add_subplot(221)\nax2=fig.add_subplot(222)\n\ng=ax1.hist(data['CreditScore'], bins=500, color='y', alpha=0.9)\ng=ax2.boxplot(data['CreditScore'])","7be286d3":"fig = plt.figure(figsize=(15,10))\nax1=fig.add_subplot(221)\nax2=fig.add_subplot(222)\n\ng=ax1.hist(data['Age'], bins=500, color='y', alpha=0.9)\ng=ax2.boxplot(data['Age'])","54396674":"data=data.drop(data[data['Age']>60].index)\nlen(data)","cbad4934":"fig = plt.figure(figsize=(15,10))\nax2=fig.add_subplot(221)\nax3=fig.add_subplot(222)\n\ng2=ax2.hist(data['Age'], bins=500, color='y', alpha=0.9)\ng3=ax3.boxplot(data['Age'])","619deab9":"fig = plt.figure(figsize=(15,10))\nax1=fig.add_subplot(221)\nax2=fig.add_subplot(222)\n\ng=ax1.hist(data['Balance'], bins=500, color='y', alpha=0.9)\ng=ax2.boxplot(data['Balance'])","879c18d2":"len(data[data.Balance==0])","9c9abd5d":"fig = plt.figure(figsize=(15,10))\nax1=fig.add_subplot(221)\nax2=fig.add_subplot(222)\n\ng=ax1.hist(data['EstimatedSalary'], bins=1000, color='y', alpha=0.9)\ng=ax2.boxplot(data['EstimatedSalary'])","37b42bf7":"from scipy import stats","f68cb5a6":"W, p = stats.shapiro(data.CreditScore.iloc[:5000])\nprint(W, p)","fc588c6e":"W, p = stats.shapiro(data.Age.iloc[:5000])\nprint(W, p)","2c1944b4":"W, p = stats.shapiro(data.EstimatedSalary.iloc[:5000])\nprint(W, p)","5dafe346":"import seaborn as sns","6b780754":"corr = data.corr()\nax = sns.heatmap(\n    corr, \n    vmin=-1, vmax=1, center=0,\n    cmap=sns.diverging_palette(20, 220, n=200),\n    square=True\n)\nax.set_xticklabels(\n    ax.get_xticklabels(),\n    rotation=45,\n    horizontalalignment='right'\n);","54375862":"data.head(10)","82cbabc5":"from sklearn import preprocessing\n\nnorm = preprocessing.StandardScaler()\nnorm.fit(data[['CreditScore','Age','Balance','EstimatedSalary','Tenure','NumOfProducts']])\nN=norm.transform(data[['CreditScore','Age','Balance','EstimatedSalary','Tenure','NumOfProducts']])\nN","0e9e1a0e":"data[['CreditScore','Age','Balance','EstimatedSalary','Tenure','NumOfProducts']]=N","a94eef86":"data.head()","080c54ec":"data1 = pd.get_dummies(data, columns =['Gender', 'Geography'], drop_first=True)\ndata1.head()","e5562acd":"X = data1.iloc[:, 2:].drop(['Exited'], axis='columns')\n\nY = data1.iloc[:, 8]","22a9a776":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split","0b0b53b4":"X_train, X_test, Y_train, Y_test = train_test_split(X, Y, stratify=Y, train_size=(0.7), test_size=(0.3))","11d54062":"len(Y_test[Y_test==1])","c22369d4":"len(Y_test[Y_test==0])","2a75ac6c":"classifier=LogisticRegression()\nclassifier.fit(X_train, Y_train)","4f6d463c":"predicted_y = classifier.predict(X_test)\nprint('predicted_y:', predicted_y)\nprint('coef_:', classifier.coef_)\nprint('accuracy_score:',classifier.score(X_test, Y_test))","8c69649b":"len(predicted_y[np.where(predicted_y==0)])","5d3daea1":"len(predicted_y[np.where(predicted_y==1)])","e38cfe6c":"from sklearn.metrics import confusion_matrix\n\ncm = confusion_matrix(Y_test, predicted_y)\ntn, fp, fn, tp=cm.ravel()\nprint(cm)\nprint(tn, fp, fn, tp)","e0bd8442":"Re=tp\/(tp+fn)\nPr=tp\/(tp+fp)\nSp=tn\/(tn+fp)\nBac=(Re+Sp)\/2\nF=2*Re*Pr\/(Re+Pr)\nprint('Accuracy (log_reg):',classifier.score(X_test, Y_test))\nprint('Recall (log_reg):', Re)\nprint('Precision (log_reg):', Pr)\nprint('F-measure (log_reg):', F)\nb=2   # \u043f\u0440\u0438\u043e\u0440\u0438\u0442\u0435\u0442 \u0443 Recall # b>1(Recall), 0<b<1(Precision)\nF_b=(1+b**2)*Re*Pr\/(Re+Pr*b**2)\nprint('F_2-measure (log_reg):', F_b)\nprint('Balanced accuracy (log_reg):', Bac)\nprint('Specificity_ (log_reg):', Sp)","d02b7d4f":"from sklearn.model_selection import cross_val_score, GridSearchCV, StratifiedKFold, cross_validate","23fce79d":"clf_log = LogisticRegression()\n\ncv = StratifiedKFold(n_splits=10, shuffle=True, random_state=1)\nscores = cross_validate(clf_log, X, Y, cv=cv, n_jobs=-1, scoring=['accuracy','precision','recall'])\n\nprint(\"Accuracy_test (log_reg): {}\".format(scores['test_accuracy'].mean()), \n      \"Recall_test (log_reg): {}\".format(scores['test_recall'].mean()),\n      \"Precision_test (log_reg): {}\".format(scores['test_precision'].mean()), sep='\\n')\n\nb=2   # b>1(Recall), 0<b<1(Precision)\nF_b=(1+b**2)*scores['test_recall'].mean()*scores['test_precision'].mean()\/(scores['test_recall'].mean()+scores['test_precision'].mean()*b**2)\nprint('F_2-measure_test (log_reg):', F_b)","8f12f8ec":"from sklearn.ensemble import RandomForestClassifier","11eb3805":"clf_forest = RandomForestClassifier(random_state=1, n_estimators=500, min_samples_split=10, min_samples_leaf=2)\n\ncv = StratifiedKFold(n_splits=10, shuffle=True, random_state=1)\nscores = cross_validate(clf_forest, X, Y, cv=cv, n_jobs=-1, scoring=['accuracy','precision','recall'], return_train_score=True)\n\nprint(\"Accuracy_test (Forest): {}\".format(scores['test_accuracy'].mean()), \n      \"Recall_test (Forest): {}\".format(scores['test_recall'].mean()),\n      \"Precision_test (Forest): {}\".format(scores['test_precision'].mean()), sep='\\n')\n\nb=2   # b>1(Recall), 0<b<1(Precision)\nF_b=(1+b**2)*scores['test_recall'].mean()*scores['test_precision'].mean()\/(scores['test_recall'].mean()+scores['test_precision'].mean()*b**2)\nprint('F_2-measure_test (Forest):', F_b)","8ee871df":"clf_forest.fit(X_train, Y_train)\npredicted_y = clf_forest.predict(X_test)\npredicted_y","d76ddf89":"cm = confusion_matrix(Y_test, predicted_y)\ntn, fp, fn, tp=cm.ravel()\nprint(cm)","88f812a0":"# Hyperparameter optimization using RandomizedSearchCV\nfrom sklearn.model_selection import RandomizedSearchCV, GridSearchCV","2228bf2e":"#parameters\nparams = {\n    \"n_estimators\": [350, 400, 450],\n    \"min_samples_split\": [6, 8, 10],\n    \"min_samples_leaf\": [1, 2, 4]\n}","236863a9":"random_search=RandomizedSearchCV(clf_forest, param_distributions=params, n_iter=5, scoring='roc_auc',n_jobs=-1, cv=cv,verbose=3)","0acbe97d":"random_search.fit(X_train,Y_train)","be211341":"random_search.best_estimator_","1e08c0c9":"random_search.best_params_","212c5391":"random_forest = RandomForestClassifier(min_samples_leaf=4, min_samples_split=10,\n                       n_estimators=350, random_state=1)","b79a9542":"from sklearn.model_selection import cross_val_score\nscore = cross_val_score(random_forest,X,Y,cv=10)\nscore.mean()","219d8aee":"from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, fbeta_score","14096ec8":"random_forest.fit(X_train, Y_train)\nY_test_preds=random_forest.predict(X_test)","f9537152":"print('Accuracy (Forest): {0:.2f}'.format(accuracy_score(Y_test, Y_test_preds)))\nprint('Precision (Forest): {0:.2f}'.format(precision_score(Y_test, Y_test_preds)))\nprint('Recall (Forest): {0:.2f}'.format(recall_score(Y_test, Y_test_preds)))\nprint('F2 (Forest): {0:.2f}'.format(fbeta_score(Y_test, Y_test_preds, 2)))","27dde9ea":"from xgboost.sklearn import XGBClassifier","6460d300":"#parameters\nparams = {\n    \"learning_rate\"    :[0.05,0.10,0.15,0.20,0.25,0.30],\n    \"max_depth\"        :[ 3,4,5,6,8,10,12,15 ],\n    \"min_child_weight\" :[ 1,3,5,7 ],\n    \"gamma\"            :[ 0.0,0.1,0.2,0.3,0.4 ],\n    \"colsample_bytree\" :[ 0.3, 0.4, 0.5, 0.7 ]\n}","188cebd1":"classifier = XGBClassifier()","4637adbb":"random_search=RandomizedSearchCV(classifier, param_distributions=params, n_iter=5, scoring='roc_auc',n_jobs=-1, cv=cv,verbose=3)","83d3d015":"random_search.fit(X_train, Y_train)","197d0f92":"random_search.best_estimator_","1b245f6f":"random_search.best_params_","93d67e8f":"classifier = XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n              colsample_bynode=1, colsample_bytree=0.5, gamma=0.1,\n              learning_rate=0.25, max_delta_step=0, max_depth=4,\n              min_child_weight=5, missing=None, n_estimators=100, n_jobs=1,\n              nthread=None, objective='binary:logistic', random_state=0,\n              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n              silent=None, subsample=1, verbosity=1)","cb8cb58d":"score = cross_val_score(classifier,X,Y,cv=10)\nscore.mean()","2bc4b3f2":"classifier.fit(X_train, Y_train)\nY_test_preds=classifier.predict(X_test)","2b36be12":"print('Accuracy (XGboost): {0:.2f}'.format(accuracy_score(Y_test, Y_test_preds)))\nprint('Precision (XGboost): {0:.2f}'.format(precision_score(Y_test, Y_test_preds)))\nprint('Recall (XGboost): {0:.2f}'.format(recall_score(Y_test, Y_test_preds)))\nprint('F2 (XGboost): {0:.2f}'.format(fbeta_score(Y_test, Y_test_preds, 2)))","895221cd":"data.head()","1aa1fdff":"data.Exited[data.Exited==0].count()","fdb399b6":"data.Exited[data.Exited==1].count()","69e6dd52":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split","1f71d6b1":"X_train, X_test, Y_train, Y_test = train_test_split(X, Y, stratify=Y, train_size=(0.7), test_size=(0.3))","29cb1c37":"len(Y_test[Y_test==1])","596bc536":"len(Y_test[Y_test==0])","b7b8b402":"classifier=LogisticRegression(class_weight='balanced')\nclassifier.fit(X_train, Y_train)","6f680db4":"predicted_y = classifier.predict(X_test)\nprint('predicted_y:', predicted_y)\nprint('coef_:', classifier.coef_)\nprint('accuracy_score:',classifier.score(X_test, Y_test))","53354e79":"len(predicted_y[np.where(predicted_y==0)])","063f34a3":"len(predicted_y[np.where(predicted_y==1)])","946d0078":"from sklearn.metrics import confusion_matrix\n\ncm = confusion_matrix(Y_test, predicted_y)\ntn, fp, fn, tp=cm.ravel()\nprint(cm)\nprint(tn, fp, fn, tp)","969fe3a6":"Re=tp\/(tp+fn)\nPr=tp\/(tp+fp)\nSp=tn\/(tn+fp)\nBac=(Re+Sp)\/2\nF=2*Re*Pr\/(Re+Pr)\nprint('Accuracy (log_reg):',classifier.score(X_test, Y_test))\nprint('Recall (log_reg):', Re)\nprint('Precision (log_reg):', Pr)\nprint('F-measure (log_reg):', F)\nb=2   # \u043f\u0440\u0438\u043e\u0440\u0438\u0442\u0435\u0442 \u0443 Recall # b>1(Recall), 0<b<1(Precision)\nF_b=(1+b**2)*Re*Pr\/(Re+Pr*b**2)\nprint('F_2-measure (log_reg):', F_b)\nprint('Balanced accuracy (log_reg):', Bac)\nprint('Specificity_ (log_reg):', Sp)","1e016b08":"from sklearn.model_selection import cross_val_score, GridSearchCV, StratifiedKFold, cross_validate","0336c93c":"clf_log = LogisticRegression(class_weight='balanced')\n\ncv = StratifiedKFold(n_splits=10, shuffle=True, random_state=1)\nscores = cross_validate(clf_log, X, Y, cv=cv, n_jobs=-1, scoring=['accuracy','precision','recall'])\n\nprint(\"Accuracy_test (log_reg): {}\".format(scores['test_accuracy'].mean()), \n      \"Recall_test (log_reg): {}\".format(scores['test_recall'].mean()),\n      \"Precision_test (log_reg): {}\".format(scores['test_precision'].mean()), sep='\\n')\n\nb=2   # b>1(Recall), 0<b<1(Precision)\nF_b=(1+b**2)*scores['test_recall'].mean()*scores['test_precision'].mean()\/(scores['test_recall'].mean()+scores['test_precision'].mean()*b**2)\nprint('F_2-measure_test (log_reg):', F_b)","ad15efa4":"clf_forest = RandomForestClassifier(class_weight='balanced', random_state=1, n_estimators=500, min_samples_split=10, min_samples_leaf=2)\n\ncv = StratifiedKFold(n_splits=10, shuffle=True, random_state=1)\nscores = cross_validate(clf_forest, X, Y, cv=cv, n_jobs=-1, scoring=['accuracy','precision','recall'], return_train_score=True)\n\nprint(\"Accuracy_test (Forest): {}\".format(scores['test_accuracy'].mean()), \n      \"Recall_test (Forest): {}\".format(scores['test_recall'].mean()),\n      \"Precision_test (Forest): {}\".format(scores['test_precision'].mean()), sep='\\n')\n\nb=2   # b>1(Recall), 0<b<1(Precision)\nF_b=(1+b**2)*scores['test_recall'].mean()*scores['test_precision'].mean()\/(scores['test_recall'].mean()+scores['test_precision'].mean()*b**2)\nprint('F_2-measure_test (Forest):', F_b)","b57a4a53":"classifier = XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n              colsample_bynode=1, colsample_bytree=0.5, gamma=0.1,\n              learning_rate=0.25, max_delta_step=0, max_depth=4,\n              min_child_weight=5, missing=None, n_estimators=100, n_jobs=1,\n              nthread=None, objective='binary:logistic', random_state=0,\n              reg_alpha=0, reg_lambda=1, scale_pos_weight=5, seed=None,\n              silent=None, subsample=1, verbosity=1)","75c1d6f4":"score = cross_val_score(classifier,X,Y,cv=10)\nscore.mean()","6a8d607c":"from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, fbeta_score","1915bef3":"classifier.fit(X_train, Y_train)\nY_test_preds=classifier.predict(X_test)","8da74824":"print('Accuracy (XGBoost): {0:.2f}'.format(accuracy_score(Y_test, Y_test_preds)))\nprint('Precision (XGBoost): {0:.2f}'.format(precision_score(Y_test, Y_test_preds)))\nprint('Recall (XGBoost): {0:.2f}'.format(recall_score(Y_test, Y_test_preds)))\nprint('F2 (XGBoost): {0:.2f}'.format(fbeta_score(Y_test, Y_test_preds, 2)))","20258bed":"num_0 = len(data1[data1['Exited']==0])\nnum_1 = len(data1[data1['Exited']==1])\nprint(num_0,num_1)","877f1eec":"# oversampling\noversampled_data = pd.concat([ data1[data1['Exited']==0] , data1[data1['Exited']==1].sample(num_0, replace=True) ])\nprint(len(oversampled_data))","089a5a83":"# undersampling\nundersampled_data = pd.concat([data1[data1['Exited']==0].sample(num_1) , data1[data1['Exited']==1] ])\nprint(len(undersampled_data))","c634f50f":"X_o = oversampled_data.iloc[:, 2:].drop(['Exited'], axis='columns')\nY_o = oversampled_data.iloc[:, 8]","d987e086":"X_train_o, X_test_o, Y_train_o, Y_test_o = train_test_split(X_o, Y_o, stratify=Y_o, train_size=(0.7), test_size=(0.3))","cb889537":"classifier_o=LogisticRegression()\nclassifier_o.fit(X_train_o, Y_train_o)","020f83f9":"predicted_y_o = classifier_o.predict(X_test_o)\nprint('predicted_y:', predicted_y_o)\nprint('coef_:', classifier_o.coef_)\nprint('accuracy_score:',classifier_o.score(X_test_o, Y_test_o))","d0eedaed":"cm_o = confusion_matrix(Y_test_o, predicted_y_o)\ntn, fp, fn, tp=cm_o.ravel()\nprint(cm_o)","bb5c759d":"Re=tp\/(tp+fn)\nPr=tp\/(tp+fp)\nSp=tn\/(tn+fp)\nBac=(Re+Sp)\/2\nF=2*Re*Pr\/(Re+Pr)\nprint('Accuracy (log_reg):',classifier_o.score(X_test_o, Y_test_o))\nprint('Recall (log_reg):', Re)\nprint('Precision (log_reg):', Pr)\nprint('F-measure (log_reg):', F)\nb=2   # \u043f\u0440\u0438\u043e\u0440\u0438\u0442\u0435\u0442 \u0443 Recall # b>1(Recall), 0<b<1(Precision)\nF_b=(1+b**2)*Re*Pr\/(Re+Pr*b**2)\nprint('F_2-measure (log_reg):', F_b)\nprint('Balanced accuracy (log_reg):', Bac)\nprint('Specificity_ (log_reg):', Sp)","2bb0b9aa":"predicted_y_o = classifier_o.predict(X_test)\nprint('predicted_y:', predicted_y_o)\nprint('coef_:', classifier_o.coef_)\nprint('accuracy_score:',classifier_o.score(X_test, Y_test))","7ce1369d":"cm_o = confusion_matrix(Y_test, predicted_y_o)\ntn, fp, fn, tp=cm_o.ravel()\nprint(cm_o)","2cfcd0f7":"Re=tp\/(tp+fn)\nPr=tp\/(tp+fp)\nSp=tn\/(tn+fp)\nBac=(Re+Sp)\/2\nF=2*Re*Pr\/(Re+Pr)\nprint('Accuracy (log_reg):',classifier_o.score(X_test, Y_test))\nprint('Recall (log_reg):', Re)\nprint('Precision (log_reg):', Pr)\nprint('F-measure (log_reg):', F)\nb=2   # \u043f\u0440\u0438\u043e\u0440\u0438\u0442\u0435\u0442 \u0443 Recall # b>1(Recall), 0<b<1(Precision)\nF_b=(1+b**2)*Re*Pr\/(Re+Pr*b**2)\nprint('F_2-measure (log_reg):', F_b)\nprint('Balanced accuracy (log_reg):', Bac)\nprint('Specificity_ (log_reg):', Sp)","9e040b06":"X_u = undersampled_data.iloc[:, 2:].drop(['Exited'], axis='columns')\nY_u = undersampled_data.iloc[:, 8]","dc10b122":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split","e3112b87":"X_train_u, X_test_u, Y_train_u, Y_test_u = train_test_split(X_u, Y_u, stratify=Y_u, train_size=(0.7), test_size=(0.3))","562dcf92":"classifier_u=LogisticRegression()\nclassifier_u.fit(X_train_u, Y_train_u)","9ddaeca7":"predicted_y_u = classifier_u.predict(X_test_u)\nprint('predicted_y:', predicted_y_u)\nprint('coef_:', classifier_u.coef_)\nprint('accuracy_score:',classifier_u.score(X_test_u, Y_test_u))","39432b99":"cm_u = confusion_matrix(Y_test_u, predicted_y_u)\ntn, fp, fn, tp=cm_u.ravel()\nprint(cm_u)","c2b9ae98":"Re=tp\/(tp+fn)\nPr=tp\/(tp+fp)\nSp=tn\/(tn+fp)\nBac=(Re+Sp)\/2\nF=2*Re*Pr\/(Re+Pr)\nprint('Accuracy (log_reg):',classifier_u.score(X_test_u, Y_test_u))\nprint('Recall (log_reg):', Re)\nprint('Precision (log_reg):', Pr)\nprint('F-measure (log_reg):', F)\nb=2   # \u043f\u0440\u0438\u043e\u0440\u0438\u0442\u0435\u0442 \u0443 Recall # b>1(Recall), 0<b<1(Precision)\nF_b=(1+b**2)*Re*Pr\/(Re+Pr*b**2)\nprint('F_2-measure (log_reg):', F_b)\nprint('Balanced accuracy (log_reg):', Bac)\nprint('Specificity_ (log_reg):', Sp)","1582955f":"predicted_y_u = classifier_u.predict(X_test)\nprint('predicted_y:', predicted_y_u)\nprint('coef_:', classifier_u.coef_)\nprint('accuracy_score:',classifier_u.score(X_test, Y_test))","2659b82a":"cm_u = confusion_matrix(Y_test, predicted_y_u)\ntn, fp, fn, tp=cm_u.ravel()\nprint(cm_u)","a125cec9":"Re=tp\/(tp+fn)\nPr=tp\/(tp+fp)\nSp=tn\/(tn+fp)\nBac=(Re+Sp)\/2\nF=2*Re*Pr\/(Re+Pr)\nprint('Accuracy (log_reg):',classifier_u.score(X_test, Y_test))\nprint('Recall (log_reg):', Re)\nprint('Precision (log_reg):', Pr)\nprint('F-measure (log_reg):', F)\nb=2   # \u043f\u0440\u0438\u043e\u0440\u0438\u0442\u0435\u0442 \u0443 Recall # b>1(Recall), 0<b<1(Precision)\nF_b=(1+b**2)*Re*Pr\/(Re+Pr*b**2)\nprint('F_2-measure (log_reg):', F_b)\nprint('Balanced accuracy (log_reg):', Bac)\nprint('Specificity_ (log_reg):', Sp)","223839b7":"clf_forest_o = RandomForestClassifier(random_state=1, n_estimators=500, min_samples_split=10, min_samples_leaf=2)\n\ncv = StratifiedKFold(n_splits=10, shuffle=True, random_state=1)\nscores = cross_validate(clf_forest_o, X_o, Y_o, cv=cv, n_jobs=-1, scoring=['accuracy','precision','recall'], return_train_score=True)\n\nprint(\"Accuracy_test (Forest): {}\".format(scores['test_accuracy'].mean()), \n      \"Recall_test (Forest): {}\".format(scores['test_recall'].mean()),\n      \"Precision_test (Forest): {}\".format(scores['test_precision'].mean()), sep='\\n')\n\nb=2   # b>1(Recall), 0<b<1(Precision)\nF_b=(1+b**2)*scores['test_recall'].mean()*scores['test_precision'].mean()\/(scores['test_recall'].mean()+scores['test_precision'].mean()*b**2)\nprint('F_2-measure_test (Forest):', F_b)","ab48ca92":"clf_forest_o.fit(X_o, Y_o)","33c8601c":"predicted_y_o = clf_forest_o.predict(X_test)\nprint('predicted_y:', predicted_y_o)\nprint('accuracy_score:', clf_forest_o.score(X_test, Y_test))","b4640a58":"from sklearn.metrics import confusion_matrix\n\ncm_o = confusion_matrix(Y_test, predicted_y_o)\ntn, fp, fn, tp=cm_o.ravel()\nprint(cm_o)","512d0760":"Re=tp\/(tp+fn)\nPr=tp\/(tp+fp)\nSp=tn\/(tn+fp)\nBac=(Re+Sp)\/2\nF=2*Re*Pr\/(Re+Pr)\nprint('Accuracy (Forest):',clf_forest_o.score(X_test, Y_test))\nprint('Recall (Forest):', Re)\nprint('Precision (Forest):', Pr)\nprint('F-measure (Forest):', F)\nb=2   # \u043f\u0440\u0438\u043e\u0440\u0438\u0442\u0435\u0442 \u0443 Recall # b>1(Recall), 0<b<1(Precision)\nF_b=(1+b**2)*Re*Pr\/(Re+Pr*b**2)\nprint('F_2-measure (Forest):', F_b)\nprint('Balanced accuracy (Forest):', Bac)\nprint('Specificity_ (Forest):', Sp)","5351dfe9":"clf_forest_u = RandomForestClassifier(random_state=1, n_estimators=500, min_samples_split=10, min_samples_leaf=2)\n\ncv = StratifiedKFold(n_splits=10, shuffle=True, random_state=1)\nscores = cross_validate(clf_forest_u, X_u, Y_u, cv=cv, n_jobs=-1, scoring=['accuracy','precision','recall'], return_train_score=True)\n\nprint(\"Accuracy_test (Forest): {}\".format(scores['test_accuracy'].mean()), \n      \"Recall_test (Forest): {}\".format(scores['test_recall'].mean()),\n      \"Precision_test (Forest): {}\".format(scores['test_precision'].mean()), sep='\\n')\n\nb=2   # b>1(Recall), 0<b<1(Precision)\nF_b=(1+b**2)*scores['test_recall'].mean()*scores['test_precision'].mean()\/(scores['test_recall'].mean()+scores['test_precision'].mean()*b**2)\nprint('F_2-measure_test (Forest):', F_b)","d721c9bd":"clf_forest_u.fit(X_u, Y_u)","7d73c20d":"predicted_y_u = clf_forest_u.predict(X_test)\nprint('predicted_y:', predicted_y_u)\nprint('accuracy_score:', clf_forest_u.score(X_test, Y_test))","dc913b9e":"cm_u = confusion_matrix(Y_test, predicted_y_u)\ntn, fp, fn, tp=cm_u.ravel()\nprint(cm_u)","b7a4453b":"Re=tp\/(tp+fn)\nPr=tp\/(tp+fp)\nSp=tn\/(tn+fp)\nBac=(Re+Sp)\/2\nF=2*Re*Pr\/(Re+Pr)\nprint('Accuracy (Forest):',clf_forest_u.score(X_test, Y_test))\nprint('Recall (Forest):', Re)\nprint('Precision (Forest):', Pr)\nprint('F-measure (Forest):', F)\nb=2   # \u043f\u0440\u0438\u043e\u0440\u0438\u0442\u0435\u0442 \u0443 Recall # b>1(Recall), 0<b<1(Precision)\nF_b=(1+b**2)*Re*Pr\/(Re+Pr*b**2)\nprint('F_2-measure (Forest):', F_b)\nprint('Balanced accuracy (Forest):', Bac)\nprint('Specificity_ (Forest):', Sp)","c7a24cf5":"#parameters\nparams = {\n    \"learning_rate\"    :[0.05,0.10,0.15,0.20,0.25,0.30],\n    \"max_depth\"        :[ 3,4,5,6,8,10,12,15 ],\n    \"min_child_weight\" :[ 1,3,5,7 ],\n    \"gamma\"            :[ 0.0,0.1,0.2,0.3,0.4 ],\n    \"colsample_bytree\" :[ 0.3, 0.4, 0.5, 0.7 ]\n}","e0ba4956":"classifier = XGBClassifier()","78530e8c":"random_search=RandomizedSearchCV(classifier, param_distributions=params, n_iter=5, scoring='roc_auc',n_jobs=-1, cv=cv,verbose=3)","693e5f7e":"random_search.fit(X_o,Y_o)","029d693d":"random_search.best_estimator_","f1c55421":"random_search.best_params_","9df83934":"classifier = XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n              colsample_bynode=1, colsample_bytree=0.5, gamma=0.3,\n              learning_rate=0.15, max_delta_step=0, max_depth=10,\n              min_child_weight=5, missing=None, n_estimators=100, n_jobs=1,\n              nthread=None, objective='binary:logistic', random_state=0,\n              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n              silent=None, subsample=1, verbosity=1)","ec3b226e":"from sklearn.model_selection import cross_val_score\nscore = cross_val_score(classifier,X_o,Y_o,cv=10)\nscore.mean()","b6cece92":"from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, fbeta_score","f6ebbeb2":"classifier.fit(X_train_o, Y_train_o)\nY_test_preds=classifier.predict(X_test_o)","032fe6bc":"print('Accuracy (XCBoost): {0:.2f}'.format(accuracy_score(Y_test_o, Y_test_preds)))\nprint('Precision (XCBoost): {0:.2f}'.format(precision_score(Y_test_o, Y_test_preds)))\nprint('Recall (XCBoost): {0:.2f}'.format(recall_score(Y_test_o, Y_test_preds)))\nprint('F2 (XCBoost): {0:.2f}'.format(fbeta_score(Y_test_o, Y_test_preds, 2)))","35994ac4":"classifier.fit(X_train_o, Y_train_o)\nY_test_preds=classifier.predict(X_test)","863a5cc8":"print('Accuracy (XCBoost): {0:.2f}'.format(accuracy_score(Y_test, Y_test_preds)))\nprint('Precision (XCBoost): {0:.2f}'.format(precision_score(Y_test, Y_test_preds)))\nprint('Recall (XCBoost): {0:.2f}'.format(recall_score(Y_test, Y_test_preds)))\nprint('F2 (XCBoost): {0:.2f}'.format(fbeta_score(Y_test, Y_test_preds, 2)))","32a2a925":"#parameters\nparams = {\n    \"learning_rate\"    :[0.05,0.10,0.15,0.20,0.25,0.30],\n    \"max_depth\"        :[ 3,4,5,6,8,10,12,15 ],\n    \"min_child_weight\" :[ 1,3,5,7 ],\n    \"gamma\"            :[ 0.0,0.1,0.2,0.3,0.4 ],\n    \"colsample_bytree\" :[ 0.3, 0.4, 0.5, 0.7 ]\n}","6f920aba":"random_search.fit(X_u,Y_u)","62a69ae5":"random_search.best_estimator_","e8c8cda7":"random_search.best_params_","27c2d7aa":"classifier = XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n              colsample_bynode=1, colsample_bytree=0.7, gamma=0.3,\n              learning_rate=0.1, max_delta_step=0, max_depth=4,\n              min_child_weight=7, missing=None, n_estimators=100, n_jobs=1,\n              nthread=None, objective='binary:logistic', random_state=0,\n              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n              silent=None, subsample=1, verbosity=1)","b69e83f9":"score = cross_val_score(classifier,X_u,Y_u,cv=10)\nscore.mean()","4a8082c5":"classifier.fit(X_train_u, Y_train_u)\nY_test_preds=classifier.predict(X_test_u)","3c6f4f44":"print('Accuracy (XCBoost): {0:.2f}'.format(accuracy_score(Y_test_u, Y_test_preds)))\nprint('Precision (XCBoost): {0:.2f}'.format(precision_score(Y_test_u, Y_test_preds)))\nprint('Recall (XCBoost): {0:.2f}'.format(recall_score(Y_test_u, Y_test_preds)))\nprint('F2 (XCBoost): {0:.2f}'.format(fbeta_score(Y_test_u, Y_test_preds, 2)))","3a249b1f":"classifier.fit(X_train_u, Y_train_u)\nY_test_preds=classifier.predict(X_test)","def897e2":"print('Accuracy (XCBoost): {0:.2f}'.format(accuracy_score(Y_test, Y_test_preds)))\nprint('Precision (XCBoost): {0:.2f}'.format(precision_score(Y_test, Y_test_preds)))\nprint('Recall (XCBoost): {0:.2f}'.format(recall_score(Y_test, Y_test_preds)))\nprint('F2 (XCBoost): {0:.2f}'.format(fbeta_score(Y_test, Y_test_preds, 2)))","00ffa8cf":"from imblearn.over_sampling import SMOTE","a72516ca":"smote = SMOTE(sampling_strategy='minority')\nX_sm, Y_sm = smote.fit_sample(X, Y)","44e2f123":"len(Y_sm[Y_sm==0])","a5be4765":"len(X_sm)","8ab18fa7":"X_train_sm, X_test_sm, Y_train_sm, Y_test_sm = train_test_split(X_sm, Y_sm, random_state=0, stratify=Y_sm, train_size=(0.7), test_size=(0.3))","7867ea25":"len(Y_test_sm[Y_test_sm==1])","fbb9b2e8":"len(Y_test_sm[Y_test_sm==0])","1c601e27":"classifier=LogisticRegression()\nclassifier.fit(X_train_sm, Y_train_sm)","98d71b22":"predicted_y = classifier.predict(X_test_sm)\nprint('predicted_y:', predicted_y)\nprint('coef_:', classifier.coef_)\nprint('accuracy_score:',classifier.score(X_test_sm,Y_test_sm))","0f079b04":"cm = confusion_matrix(Y_test_sm, predicted_y)\nprint(cm)","58f65ec7":"tn, fp, fn, tp=cm.ravel()\nRe=tp\/(tp+fn)\nPr=tp\/(tp+fp)\nSp=tn\/(tn+fp)\nF=2*Re*Pr\/(Re+Pr)\nprint(tn, fp, fn, tp)\nprint('Accuracy_score (log_reg):',classifier.score(X_test_sm,Y_test_sm))\nprint('Recall (log_reg):', Re)\nprint('Precision (log_reg):', Pr)\nprint('F-measure (log_reg):', F)\nb=2   # \u043f\u0440\u0438\u043e\u0440\u0438\u0442\u0435\u0442 \u0443 Recall # b>1(Recall), 0<b<1(Precision)\nF_b=(1+b**2)*Re*Pr\/(Re+Pr*b**2)\nprint('F_2-measure (Forest):', F_b)\nprint('Balanced accuracy (Forest):', Bac)\nprint('Specificity_ (log_reg):', Sp)","9a7f1fcc":"from imblearn.under_sampling import TomekLinks","d617aaed":"tl = TomekLinks(sampling_strategy ='majority')\nX_tl, Y_tl = tl.fit_sample(X, Y)","5099e9d0":"len(Y_tl[Y_tl==0])","4bb402df":"len(Y_tl[Y_tl==1])","4473413b":"X_train_tl, X_test_tl, Y_train_tl, Y_test_tl = train_test_split(X_tl, Y_tl, random_state=0, stratify=Y_tl, train_size=(0.7), test_size=(0.3))","9fb866b8":"len(Y_test_tl[Y_test_tl==1])","5400c1fa":"len(Y_test_tl[Y_test_tl==0])","ccce6905":"classifier=LogisticRegression()\nclassifier.fit(X_train_tl, Y_train_tl)","9ae85433":"predicted_y = classifier.predict(X_test_tl)\nprint('predicted_y:', predicted_y)\nprint('coef_:', classifier.coef_)\nprint('accuracy_score:',classifier.score(X_test_tl,Y_test_tl))","4b5ee64f":"len(predicted_y[np.where(predicted_y==0)])","a4b299c1":"len(predicted_y[np.where(predicted_y==1)])","95e6f7df":"from sklearn.metrics import confusion_matrix\n\ncm = confusion_matrix(Y_test_tl, predicted_y)\ntn, fp, fn, tp=cm.ravel()\nprint(cm, tn, fp, fn, tp)","3ada4f92":"tn, fp, fn, tp=cm.ravel()\nRe=tp\/(tp+fn)\nPr=tp\/(tp+fp)\nSp=tn\/(tn+fp)\nF=2*Re*Pr\/(Re+Pr)\nprint(tn, fp, fn, tp)\nprint('accuracy_score (log_reg):',classifier.score(X_test_tl,Y_test_tl))\nprint('Recall (log_reg):', Re)\nprint('Precision (log_reg):', Pr)\nprint('F-measure (log_reg):', F)\nb=2   # \u043f\u0440\u0438\u043e\u0440\u0438\u0442\u0435\u0442 \u0443 Recall # b>1(Recall), 0<b<1(Precision)\nF_b=(1+b**2)*Re*Pr\/(Re+Pr*b**2)\nprint('F_2-measure (log_reg):', F_b)\nprint('Balanced accuracy (log_reg):', Bac)\nprint('Specificity_ (log_reg):', Sp)","4e6731a0":"clf_forest_sm = RandomForestClassifier(random_state=1, n_estimators=500, min_samples_split=10, min_samples_leaf=2)\n\ncv = StratifiedKFold(n_splits=10, shuffle=True, random_state=1)\nscores = cross_validate(clf_forest_sm, X_sm, Y_sm, cv=cv, n_jobs=-1, scoring=['accuracy','precision','recall'], return_train_score=True)\n\nprint(\"Accuracy_test (Forest): {}\".format(scores['test_accuracy'].mean()), \n      \"Recall_test (Forest): {}\".format(scores['test_recall'].mean()),\n      \"Precision_test (Forest): {}\".format(scores['test_precision'].mean()), sep='\\n')\n\nb=2   # b>1(Recall), 0<b<1(Precision)\nF_b=(1+b**2)*scores['test_recall'].mean()*scores['test_precision'].mean()\/(scores['test_recall'].mean()+scores['test_precision'].mean()*b**2)\nprint('F_2-measure_test (Forest):', F_b)","6216ae10":"clf_forest_sm.fit(X_sm, Y_sm)","39ce3aff":"predicted_y_sm = clf_forest_sm.predict(X_test)\nprint('predicted_y:', predicted_y_sm)\nprint('accuracy_score:', clf_forest_sm.score(X_test, Y_test))","af583e69":"cm_sm = confusion_matrix(Y_test, predicted_y_sm)\ntn, fp, fn, tp=cm_sm.ravel()\nprint(cm_u)","a9fcc338":"Re=tp\/(tp+fn)\nPr=tp\/(tp+fp)\nSp=tn\/(tn+fp)\nBac=(Re+Sp)\/2\nF=2*Re*Pr\/(Re+Pr)\nprint('Accuracy (Forest):',clf_forest_u.score(X_test, Y_test))\nprint('Recall (Forest):', Re)\nprint('Precision (Forest):', Pr)\nprint('F-measure (Forest):', F)\nb=2   # \u043f\u0440\u0438\u043e\u0440\u0438\u0442\u0435\u0442 \u0443 Recall # b>1(Recall), 0<b<1(Precision)\nF_b=(1+b**2)*Re*Pr\/(Re+Pr*b**2)\nprint('F_2-measure (Forest):', F_b)\nprint('Balanced accuracy (Forest):', Bac)\nprint('Specificity_ (Forest):', Sp)","48d0f637":"clf_forest_tl = RandomForestClassifier(random_state=1, n_estimators=500, min_samples_split=10, min_samples_leaf=2)\n\ncv = StratifiedKFold(n_splits=10, shuffle=True, random_state=1)\nscores = cross_validate(clf_forest_tl, X_tl, Y_tl, cv=cv, n_jobs=-1, scoring=['accuracy','precision','recall'], return_train_score=True)\n\nprint(\"Accuracy_test (Forest): {}\".format(scores['test_accuracy'].mean()), \n      \"Recall_test (Forest): {}\".format(scores['test_recall'].mean()),\n      \"Precision_test (Forest): {}\".format(scores['test_precision'].mean()), sep='\\n')\n\nb=2   # b>1(Recall), 0<b<1(Precision)\nF_b=(1+b**2)*scores['test_recall'].mean()*scores['test_precision'].mean()\/(scores['test_recall'].mean()+scores['test_precision'].mean()*b**2)\nprint('F_2-measure_test (Forest):', F_b)","f1857344":"clf_forest_tl.fit(X_tl, Y_tl)","aa760880":"predicted_y_tl = clf_forest_tl.predict(X_test)\nprint('predicted_y:', predicted_y_tl)\nprint('accuracy_score:', clf_forest_sm.score(X_test, Y_test))","ee3db59b":"cm_tl = confusion_matrix(Y_test, predicted_y_tl)\ntn, fp, fn, tp=cm_tl.ravel()\nprint(cm_u)","ac938ec7":"Re=tp\/(tp+fn)\nPr=tp\/(tp+fp)\nSp=tn\/(tn+fp)\nBac=(Re+Sp)\/2\nF=2*Re*Pr\/(Re+Pr)\nprint('Accuracy (Forest):',clf_forest_tl.score(X_test, Y_test))\nprint('Recall (Forest):', Re)\nprint('Precision (Forest):', Pr)\nprint('F-measure (Forest):', F)\nb=2   # \u043f\u0440\u0438\u043e\u0440\u0438\u0442\u0435\u0442 \u0443 Recall # b>1(Recall), 0<b<1(Precision)\nF_b=(1+b**2)*Re*Pr\/(Re+Pr*b**2)\nprint('F_2-measure (Forest):', F_b)\nprint('Balanced accuracy (Forest):', Bac)\nprint('Specificity_ (Forest):', Sp)","1924420a":"#parameters\nparams = {\n    \"learning_rate\"    :[0.05,0.10,0.15,0.20,0.25,0.30],\n    \"max_depth\"        :[ 3,4,5,6,8,10,12,15 ],\n    \"min_child_weight\" :[ 1,3,5,7 ],\n    \"gamma\"            :[ 0.0,0.1,0.2,0.3,0.4 ],\n    \"colsample_bytree\" :[ 0.3, 0.4, 0.5, 0.7 ]\n}","2f8f8e30":"classifier = XGBClassifier()","3a9fde92":"random_search=RandomizedSearchCV(classifier, param_distributions=params, n_iter=5, scoring='roc_auc',n_jobs=-1, cv=cv,verbose=3)","e1ae5b00":"random_search.fit(X_sm,Y_sm)","b51076fc":"random_search.best_estimator_","2348790d":"random_search.best_params_","f020fcaa":"classifier = XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n              colsample_bynode=1, colsample_bytree=0.5, gamma=0.3,\n              learning_rate=0.15, max_delta_step=0, max_depth=10,\n              min_child_weight=5, missing=None, n_estimators=100, n_jobs=1,\n              nthread=None, objective='binary:logistic', random_state=0,\n              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n              silent=None, subsample=1, verbosity=1)","6188272b":"score = cross_val_score(classifier,X_sm,Y_sm,cv=10)\nscore.mean()","fb9ae21c":"classifier.fit(X_train_sm, Y_train_sm)\nY_test_preds=classifier.predict(X_test_sm)","bcaa9fff":"print('Accuracy (XGBoost): {0:.2f}'.format(accuracy_score(Y_test_sm, Y_test_preds)))\nprint('Precision (XGBoost): {0:.2f}'.format(precision_score(Y_test_sm, Y_test_preds)))\nprint('Recall (XGBoost): {0:.2f}'.format(recall_score(Y_test_sm, Y_test_preds)))\nprint('F2 (XGBoost): {0:.2f}'.format(fbeta_score(Y_test_sm, Y_test_preds, 2)))","328184be":"classifier.fit(X_train_sm, Y_train_sm)\nY_test_preds=classifier.predict(X_test)","5879ce25":"print('Accuracy (XGBoost): {0:.2f}'.format(accuracy_score(Y_test, Y_test_preds)))\nprint('Precision (XGBoost): {0:.2f}'.format(precision_score(Y_test, Y_test_preds)))\nprint('Recall (XGBoost): {0:.2f}'.format(recall_score(Y_test, Y_test_preds)))\nprint('F2 (XGBoost): {0:.2f}'.format(fbeta_score(Y_test, Y_test_preds, 2)))","62876654":"#parameters\nparams = {\n    \"learning_rate\"    :[0.05,0.10,0.15,0.20,0.25,0.30],\n    \"max_depth\"        :[ 3,4,5,6,8,10,12,15 ],\n    \"min_child_weight\" :[ 1,3,5,7 ],\n    \"gamma\"            :[ 0.0,0.1,0.2,0.3,0.4 ],\n    \"colsample_bytree\" :[ 0.3, 0.4, 0.5, 0.7 ]\n}","3f48e1c9":"classifier = XGBClassifier()","3b884f9a":"random_search=RandomizedSearchCV(classifier, param_distributions=params, n_iter=5, scoring='roc_auc',n_jobs=-1, cv=cv,verbose=3)","c352a857":"random_search.fit(X_tl,Y_tl)","fac33bf9":"random_search.best_estimator_","980fd1ff":"random_search.best_params_","91ba990a":"classifier = XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n              colsample_bynode=1, colsample_bytree=0.5, gamma=0.3,\n              learning_rate=0.15, max_delta_step=0, max_depth=10,\n              min_child_weight=5, missing=None, n_estimators=100, n_jobs=1,\n              nthread=None, objective='binary:logistic', random_state=0,\n              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n              silent=None, subsample=1, verbosity=1)","2b8be4cc":"score = cross_val_score(classifier,X_tl,Y_tl,cv=10)\nscore.mean()","361512a3":"classifier.fit(X_train_tl, Y_train_tl)\nY_test_preds=classifier.predict(X_test_tl)","d52936e6":"print('Accuracy (XGBoost): {0:.2f}'.format(accuracy_score(Y_test_tl, Y_test_preds)))\nprint('Precision (XGBoost): {0:.2f}'.format(precision_score(Y_test_tl, Y_test_preds)))\nprint('Recall (XGBoost): {0:.2f}'.format(recall_score(Y_test_tl, Y_test_preds)))\nprint('F2 (XGBoost): {0:.2f}'.format(fbeta_score(Y_test_tl, Y_test_preds, 2)))","12232077":"classifier.fit(X_train_tl, Y_train_tl)\nY_test_preds=classifier.predict(X_test)","50cd4132":"print('Accuracy: {0:.2f}'.format(accuracy_score(Y_test, Y_test_preds)))\nprint('Precision: {0:.2f}'.format(precision_score(Y_test, Y_test_preds)))\nprint('Recall: {0:.2f}'.format(recall_score(Y_test, Y_test_preds)))\nprint('F2: {0:.2f}'.format(fbeta_score(Y_test, Y_test_preds, 2)))","d2f444ae":"tbl = {'Par.':[2, 3.1, 3.2, 3.2, 3.3, 3.3], \n         'Kind':['-', 'Balanced weight', 'Random oversampling', 'Random undersampling','Oversampling(SMOTE)','Undersampling(Tomek Links)'], \n         'Log_Reg':[0.02, 0.51, 0.50, 0.51, 0.63, 0.13], \n         'Forest':[0.39, 0.52, 0.92, 0.76, 0.83, 0.65], \n         'XGBoost':[0.39, 0.61, 0.79, 0.67, 0.67, 0.57]}\ntable=pd.DataFrame(tbl)\ntable","e0d7b8f6":"# 3. Improving the quality of the model, taking into account the imbalance of classes","117d6c8b":"##### Cross-validation did not improve model quality","3e29f752":"### In this work, the quality of the model was determined mainly by the metric F2. It is presented in the following table.","2263bde3":"#### Oversampling with SMOTE:\nIn SMOTE we create elements in close proximity to existing ones in a smaller set.","bc248721":"##### To overcome the deviation from the norm (outliers), you can use randomness in the models or random forests.","fc39dc18":"##### Checking for normality of distribution","ec91a666":"#### Testing on oversampled_data ","50117214":"---","7f0a07da":"#### As a result of oversampling and undersampling, the quality of the logistic regression model has not changed much.","dbcc2065":"#### Undersampling. Testing on data1","1e15913c":"#### Undersampling using Tomek Links. Testing on Tomek Links-data","f7e27317":"### 2.3 XGboost","c53c2fa5":"### 3.3.2 RandomForest considering the imbalance. Oversampling with SMOTE and Undersampling with Tomek Links","a5f1d09b":"### 2.2 RandomForest","c4c32b3b":"#### Oversampling. Testing on data1 ","d4c17496":"##### We got the F-measure close to 0, so the Recall is close to 0. And this, in turn, is due to the fact that the model makes many passes.\n- The F_2-measure in which completeness is preferred (b = 2) is very small.\n- Precision is not a good value, which means that the model is good at identifying \"good\" (0) clients.\n- The specificity (Specificity_) is high, because the model has a large percentage of loyal customers (the constant algorithm will show the same accuracy for this metric).\n- Analysis of these metrics tells us about the low quality of the model.","228bfe9f":"##### Optimize prediction","c93e4f27":"##### Let's try to reconfigure the model","0cc848ac":"#### Oversampling with SMOTE. Testing on data1","48ea1bf7":"##### Let's try to reconfigure the model","3b7526f2":"### 2.1. Logistic regression","7c94648b":"# 4. Conclusion","396cfb25":"##### Sampling methods: artificially duplicate observations from a rare class, or throw out some observations from a popular class.","775456fc":"#### Testing on undersampled_data","1bd768e9":"# 2. Exploring the balance of classes. Training models without accounting the imbalance","77c9fa96":"# 1. Loading and preparing data","a7feaa68":"##### Let's try to reconfigure the model","89a15c13":"#### Undersampling. Testing on undersampled_data1","6c669a19":"##### Let's try to reconfigure the model","91623d82":"##### Correlation","969cc5c7":"##### After weight balancing, all metrics are aligned. The completeness (Recall) has increased and, accordingly, the F_2-measure.","f9612f74":"### 3.2.2 RandomForest considering the imbalance. Random undersampling and oversampling","de62fab2":"#### Undersampling using Tomek Links. Testing on Tomek Links-data","5ca08e95":"### 3.2.2 XCBoost considering the imbalance. Random undersampling and oversampling","2170c30b":"##### Cross-validation regression","7d94d8ce":"#### Oversampling. Testing on data1 ","03bca4e3":"### 3.1.1 Logistic regression without cross-validation taking into account unbalance (balanced weight)","ac24b19d":"### 3.2.1 Logistic regression considering imbalance. Random undersampling and oversampling","cf70cdde":"#### One of the provided methods is called \"Tomek Links\". \"Links\" in this case are pairs of elements from different classes that are nearby. Using the algorithm, we will eventually remove the element of the pair from the larger set, which will allow the classifier to perform better.","1afdd612":"### 3.3.2 XGBoost considering the imbalance. Oversampling with SMOTE and Undersampling with Tomek Links","bb0b44b0":"### 3.1.2 Logistic regression on cross-validation taking into account the unbalance (balanced weight)","3649fa8c":"### 3.3.1 Logistic regression considering imbalance. Oversampling with SMOTE and Undersampling with Tomek Links","5c42bfe7":"#### Testing on data1","e6de73d3":"#### Undersampling using Tomek Links. Testing on data1-data","f8a791c6":"### The best quality model is Forest on Random oversampling with F_2=0.92.","c165c494":"#### Undersampling. Testing on undersampled_data1","060ea5fb":"#### Oversampling. Testing on oversampled_data ","5bd684e1":"### Oversampling","cfae7298":"#### Oversampling. Testing on data1 ","f5dbc768":"#### Oversampling. Testing on oversampled_data ","beedd247":"##### Precision and recall do not depend, in contrast to accuracy, on the ratio of classes and therefore are applicable in conditions of unbalanced samples.","1c392fcc":" \u0422\u0435\u0441\u0442\u043e\u0432\u043e\u0435 \u0437\u0430\u0434\u0430\u043d\u0438\u0435 \u00ab\u041e\u0442\u0442\u043e\u043a \u043a\u043b\u0438\u0435\u043d\u0442\u043e\u0432\u00bb\n \n\u041e\u043f\u0438\u0441\u0430\u043d\u0438\u0435\n\n\u0418\u0437 \u00ab\u0411\u0435\u0442\u0430-\u0411\u0430\u043d\u043a\u0430\u00bb \u0441\u0442\u0430\u043b\u0438 \u0443\u0445\u043e\u0434\u0438\u0442\u044c \u043a\u043b\u0438\u0435\u043d\u0442\u044b. \u041a\u0430\u0436\u0434\u044b\u0439 \u043c\u0435\u0441\u044f\u0446. \u041d\u0435\u043c\u043d\u043e\u0433\u043e, \u043d\u043e \u0437\u0430\u043c\u0435\u0442\u043d\u043e. \u0411\u0430\u043d\u043a\u043e\u0432\u0441\u043a\u0438\u0435 \u043c\u0430\u0440\u043a\u0435\u0442\u043e\u043b\u043e\u0433\u0438\n\u043f\u043e\u0441\u0447\u0438\u0442\u0430\u043b\u0438: \u0441\u043e\u0445\u0440\u0430\u043d\u044f\u0442\u044c \u0442\u0435\u043a\u0443\u0449\u0438\u0445 \u043a\u043b\u0438\u0435\u043d\u0442\u043e\u0432 \u0434\u0435\u0448\u0435\u0432\u043b\u0435, \u0447\u0435\u043c \u043f\u0440\u0438\u0432\u043b\u0435\u043a\u0430\u0442\u044c \u043d\u043e\u0432\u044b\u0445.\n\u041d\u0443\u0436\u043d\u043e \u0441\u043f\u0440\u043e\u0433\u043d\u043e\u0437\u0438\u0440\u043e\u0432\u0430\u0442\u044c, \u0443\u0439\u0434\u0451\u0442 \u043a\u043b\u0438\u0435\u043d\u0442 \u0438\u0437 \u0431\u0430\u043d\u043a\u0430 \u0432 \u0431\u043b\u0438\u0436\u0430\u0439\u0448\u0435\u0435 \u0432\u0440\u0435\u043c\u044f \u0438\u043b\u0438 \u043d\u0435\u0442. \u0412\u0430\u043c \u043f\u0440\u0435\u0434\u043e\u0441\u0442\u0430\u0432\u043b\u0435\u043d\u044b\n\u0438\u0441\u0442\u043e\u0440\u0438\u0447\u0435\u0441\u043a\u0438\u0435 \u0434\u0430\u043d\u043d\u044b\u0435 \u043e \u043f\u043e\u0432\u0435\u0434\u0435\u043d\u0438\u0438 \u043a\u043b\u0438\u0435\u043d\u0442\u043e\u0432 \u0438 \u0440\u0430\u0441\u0442\u043e\u0440\u0436\u0435\u043d\u0438\u0438 \u0434\u043e\u0433\u043e\u0432\u043e\u0440\u043e\u0432 \u0441 \u0431\u0430\u043d\u043a\u043e\u043c.\n\n\u0418\u0441\u0442\u043e\u0447\u043d\u0438\u043a \u0434\u0430\u043d\u043d\u044b\u0445: https:\/\/www.kaggle.com\/barelydedicated\/bank-customer-churn-modeling\n\n\u0418\u043d\u0441\u0442\u0440\u0443\u043a\u0446\u0438\u044f \u043f\u043e \u0432\u044b\u043f\u043e\u043b\u043d\u0435\u043d\u0438\u044e \u0437\u0430\u0434\u0430\u0447\u0438\n\n1. \u0417\u0430\u0433\u0440\u0443\u0437\u0438\u0442\u0435 \u0438 \u043f\u043e\u0434\u0433\u043e\u0442\u043e\u0432\u044c\u0442\u0435 \u0434\u0430\u043d\u043d\u044b\u0435\n2. \u0418\u0441\u0441\u043b\u0435\u0434\u0443\u0439\u0442\u0435 \u0431\u0430\u043b\u0430\u043d\u0441 \u043a\u043b\u0430\u0441\u0441\u043e\u0432, \u043e\u0431\u0443\u0447\u0438\u0442\u0435 \u043c\u043e\u0434\u0435\u043b\u044c \u0431\u0435\u0437 \u0443\u0447\u0435\u0442\u0430 \u0434\u0438\u0441\u0431\u0430\u043b\u0430\u043d\u0441\u0430\n3. \u0423\u043b\u0443\u0447\u0448\u0438\u0442\u0435 \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u043e \u043c\u043e\u0434\u0435\u043b\u0438, \u0443\u0447\u0438\u0442\u044b\u0432\u0430\u044f \u0434\u0438\u0441\u0431\u0430\u043b\u0430\u043d\u0441 \u043a\u043b\u0430\u0441\u0441\u043e\u0432\n4. \u041f\u0440\u043e\u0432\u0435\u0434\u0438\u0442\u0435 \u0444\u0438\u043d\u0430\u043b\u044c\u043d\u043e\u0435 \u0442\u0435\u0441\u0442\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u0435\n\n\u0412\u0441\u0435 \u043f\u0440\u0435\u043e\u0431\u0440\u0430\u0437\u043e\u0432\u0430\u043d\u0438\u044f \u0438 \u043f\u043e\u0441\u0442\u0440\u043e\u0435\u043d\u0438\u0435 \u0432\u044b\u043f\u043e\u043b\u043d\u044f\u0442\u044c \u0432 Python. \u0412 \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u0435 \u043f\u0440\u0435\u0434\u043e\u0441\u0442\u0430\u0432\u0438\u0442\u044c Notebook \u0441\n\u043a\u043e\u043c\u043c\u0435\u043d\u0442\u0430\u0440\u0438\u044f\u043c\u0438 \u043a \u0432\u044b\u043f\u043e\u043b\u043d\u044f\u0435\u043c\u044b\u043c \u0448\u0430\u0433\u0430\u043c \u0438 \u0432\u044b\u0432\u043e\u0434\u0430\u043c\u0438 \u043e \u043f\u0440\u043e\u0434\u0435\u043b\u0430\u043d\u043d\u043e\u0439 \u0440\u0430\u0431\u043e\u0442\u0435.\n\u041e\u043f\u0442\u0438\u043c\u0430\u043b\u044c\u043d\u0443\u044e \u043c\u043e\u0434\u0435\u043b\u044c \u0434\u043b\u044f \u0434\u0430\u043d\u043d\u043e\u0439 \u0437\u0430\u0434\u0430\u0447\u0438 \u0432\u044b\u0431\u0438\u0440\u0430\u0439\u0442\u0435 \u0441\u0430\u043c\u0438, \u0441\u0440\u0430\u0432\u043d\u0435\u043d\u0438\u0435 \u043d\u0435\u0441\u043a\u043e\u043b\u044c\u043a\u0438\u0445 \u043f\u043e\u0434\u0445\u043e\u0434\u043e\u0432\n\u043f\u0440\u0438\u0432\u0435\u0442\u0441\u0442\u0432\u0443\u0435\u0442\u0441\u044f. \u041b\u044e\u0431\u044b\u0435 \u0434\u043e\u043f\u043e\u043b\u043d\u0438\u0442\u0435\u043b\u044c\u043d\u044b\u0435 \u0434\u0435\u0439\u0441\u0442\u0432\u0438\u044f \u0434\u043b\u044f \u0443\u043b\u0443\u0447\u0448\u0435\u043d\u0438\u044f \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u0430 \u043c\u043e\u0434\u0435\u043b\u0438 \u0442\u0430\u043a\u0436\u0435\n\u043f\u0440\u0438\u0432\u0435\u0442\u0441\u0442\u0432\u0443\u044e\u0442\u0441\u044f.\n\n\u041e\u043f\u0438\u0441\u0430\u043d\u0438\u0435 \u0434\u0430\u043d\u043d\u044b\u0445.\n\u041f\u0440\u0438\u0437\u043d\u0430\u043a\u0438:\n\n- RowNumber \u2013 \u0438\u043d\u0434\u0435\u043a\u0441 \u0441\u0442\u0440\u043e\u043a\u0438 \u0432 \u0434\u0430\u043d\u043d\u044b\u0445\n- CustomerId \u2013 \u0443\u043d\u0438\u043a\u0430\u043b\u044c\u043d\u044b\u0439 \u0438\u0434\u0435\u043d\u0442\u0438\u0444\u0438\u043a\u0430\u0442\u043e\u0440 \u043a\u043b\u0438\u0435\u043d\u0442\u0430\n- Surname \u2013 \u0444\u0430\u043c\u0438\u043b\u0438\u044f\n- CreditScore \u2013 \u043a\u0440\u0435\u0434\u0438\u0442\u043d\u044b\u0439 \u0441\u043a\u043e\u0440\u0438\u043d\u0433\n- Geography \u2013 \u0441\u0442\u0440\u0430\u043d\u0430 \u043f\u0440\u043e\u0436\u0438\u0432\u0430\u043d\u0438\u044f\n- Gender \u2013 \u043f\u043e\u043b\n- Age \u2013 \u0432\u043e\u0437\u0440\u0430\u0441\u0442\n- Tenure \u2013 \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u043d\u0435\u0434\u0432\u0438\u0436\u0438\u043c\u043e\u0441\u0442\u0438 \u0443 \u043a\u043b\u0438\u0435\u043d\u0442\u0430\n- Balance \u2013 \u0431\u0430\u043b\u0430\u043d\u0441 \u043d\u0430 \u0441\u0447\u0435\u0442\u0435\n- NumOfProducts \u2013 \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u043f\u0440\u043e\u0434\u0443\u043a\u0442\u043e\u0432 \u0431\u0430\u043d\u043a\u0430, \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u043c\u044b\u0445 \u043a\u043b\u0438\u0435\u043d\u0442\u043e\u043c\n- HasCrCard \u2013 \u043d\u0430\u043b\u0438\u0447\u0438\u0435 \u043a\u0440\u0435\u0434\u0438\u0442\u043d\u043e\u0439 \u043a\u0430\u0440\u0442\u044b\n- IsActiveMember \u2013 \u0430\u043a\u0442\u0438\u0432\u043d\u043e\u0441\u0442\u044c \u043a\u043b\u0438\u0435\u043d\u0442\u0430\n- EstimatedSalary \u2013 \u043f\u0440\u0435\u0434\u043f\u043e\u043b\u0430\u0433\u0430\u0435\u043c\u0430\u044f \u0437\u0430\u0440\u043f\u043b\u0430\u0442\u0430\n\n\u0426\u0435\u043b\u0435\u0432\u043e\u0439 \u043f\u0440\u0438\u0437\u043d\u0430\u043a:\n- Exited \u2013 \u0444\u0430\u043a\u0442 \u0443\u0445\u043e\u0434\u0430 \u043a\u043b\u0438\u0435\u043d\u0442\u0430","f5958da8":"##### Let's try to reconfigure the model","2db2c9ee":"### Undersampling","4513dac3":"##### After weight balancing, all metrics are aligned. The completeness (Recall) has increased and, accordingly, the F_2-measure.","7ba934a2":"#### Undersampling. Testing on data1","8bf3066c":"##### The data is strongly unbalanced, this could lead to problems when predicting data.","dfa1f42e":"### 3.1.4 XGBoost on cross-validation taking into account the unbalance (balanced weight)","286faf73":"#### Testing on data1 ","f9db1e52":"### 3.1.3 RandomForest on cross-validation taking into account the unbalance (balanced weight)","3234140c":"#### Undersampling using Tomek Links:","fc8071fb":"##### normalization","acb14993":"#### Let's check if there are outliers in the data. Delete them"}}