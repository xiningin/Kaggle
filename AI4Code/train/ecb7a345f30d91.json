{"cell_type":{"bd8a0b4d":"code","7f8aae63":"code","eeae9b62":"code","ffe08b72":"code","ea13ae84":"code","41f1b762":"code","e238232c":"code","9b7cc1c7":"code","39f49c84":"code","10ea8cf4":"code","c89e6ba2":"code","b4fd1831":"code","26a19fb0":"code","c671ddf3":"code","10b5f04e":"code","d141f2c2":"code","3b331ed6":"code","7f4bf59f":"code","bcc203c1":"code","d90c35c8":"code","bf4d9796":"code","8573e4b3":"code","71d33a75":"code","667cbcf7":"code","67aa43da":"code","1b802256":"code","d9deacb7":"code","fcd5b19a":"code","6c0cb83a":"code","73814995":"code","5de39bf7":"code","e72075fa":"code","91793f5c":"code","4695da15":"code","826cc098":"code","1690a82f":"code","2131ea84":"code","75a0008c":"code","6fdc43e3":"code","0885c9fa":"code","7c78e721":"code","3679a452":"code","832188d5":"code","abdf8bb8":"code","cc599f12":"code","cb8c1037":"code","987a8b01":"markdown","b93d012e":"markdown","45bfbe0d":"markdown","ffe9f8f6":"markdown","3294573e":"markdown","b2282db5":"markdown","bd31ac31":"markdown","a78284fe":"markdown","1db871bb":"markdown","ae98d874":"markdown","56be0e57":"markdown","c1e4a58b":"markdown","5a822bcc":"markdown","af976096":"markdown","d63ead3c":"markdown","bbbfecce":"markdown","7af225ae":"markdown","de874419":"markdown","3df20fed":"markdown","ffb6c8e4":"markdown","813bac77":"markdown","1e37b1e4":"markdown","b9f2e150":"markdown","646b2fe6":"markdown","c3fb66c0":"markdown","ec35c63c":"markdown","3a6dcd52":"markdown","a1622561":"markdown","fdcf3245":"markdown","37b723da":"markdown","ce4be18a":"markdown","ffc060d6":"markdown","86085acd":"markdown","be68e7b7":"markdown","c1a8af1b":"markdown","720ea2cb":"markdown","b434c26c":"markdown","6b4b0a18":"markdown","98f10aa7":"markdown","ae95e63b":"markdown","59792c2c":"markdown","8afad12f":"markdown","e1d9ea4b":"markdown","8e771c4a":"markdown","4af1a6de":"markdown","dae6a26b":"markdown","37acd50f":"markdown"},"source":{"bd8a0b4d":"import warnings\nwarnings.filterwarnings('ignore')","7f8aae63":"!ls ..\/input\/","eeae9b62":"import pandas as pd\nht_dt = pd.read_csv(\"..\/input\/heart.csv\", header = 'infer')","ffe08b72":"print(\"The heart dataset has {0} rows and {1} columns\".format(ht_dt.shape[0], ht_dt.shape[1]))","ea13ae84":"ht_dt.head()","41f1b762":"import seaborn as sns\nsns.countplot(ht_dt['target'],label=\"Count\")","e238232c":"#Function to calculate missing value\ndef missing_values_table(df):\n        # Total missing values\n        mis_val = df.isnull().sum()\n        \n        # Percentage of missing values\n        mis_val_percent = 100 * df.isnull().sum() \/ len(df)\n        \n        # Make a table with the results\n        mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)\n        \n        # Rename the columns\n        mis_val_table_ren_columns = mis_val_table.rename(\n        columns = {0 : 'Missing Values', 1 : '% of Total Values'})\n        \n        # Sort the table by percentage of missing descending\n        mis_val_table_ren_columns = mis_val_table_ren_columns[\n            mis_val_table_ren_columns.iloc[:,1] != 0].sort_values(\n        '% of Total Values', ascending=False).round(1)\n        \n        # Print some summary information\n        print (\"Your selected dataframe has \" + str(df.shape[1]) + \" columns.\\n\"      \n            \"There are \" + str(mis_val_table_ren_columns.shape[0]) +\n              \" columns that have missing values.\")\n        \n        # Return the dataframe with missing information\n        return mis_val_table_ren_columns","9b7cc1c7":"missing_values_table(ht_dt)","39f49c84":"#Missing values with respect to each column in the dataset\nimport seaborn as sns\nsns.heatmap(ht_dt.isnull(), cbar=False)","10ea8cf4":"#correlation matrix\nimport matplotlib.pyplot as plt\n%matplotlib inline\nht_dt_ft = ht_dt.drop('target', axis=1)\nfig=plt.gcf()\nfig.set_size_inches(15,7)\nfig=sns.heatmap(ht_dt_ft.corr(),annot=True,cmap='cubehelix',linewidths=1,linecolor='k',\n                square=True,mask=False, vmin=-1, vmax=1,cbar_kws={\"orientation\": \"vertical\"},cbar=True)","c89e6ba2":"from scipy.stats import spearmanr\nimport numpy as np\nlabels = []\nvalues = []\nfor col in ht_dt.columns:\n    if col not in [\"target\"]:\n        labels.append(col)\n        values.append(spearmanr(ht_dt[col].values, ht_dt[\"target\"].values)[0])\ncorr_df = pd.DataFrame({'col_labels':labels, 'corr_values':values})\ncorr_df = corr_df.sort_values(by='corr_values')\n \nind = np.arange(corr_df.shape[0])\nwidth = 0.9\nfig, ax = plt.subplots(figsize=(12,30))\nrects = ax.barh(ind, np.array(corr_df.corr_values.values), color='g')\nax.set_yticks(ind)\nax.set_yticklabels(corr_df.col_labels.values, rotation='horizontal')\nax.set_xlabel(\"Correlation coefficient\")\nax.set_title(\"Correlation coefficient of the variables\")\nplt.show()","b4fd1831":"#scatterplot - set 1\nset1 = ['age', 'sex', 'cp', 'trestbps', 'chol', 'fbs', 'restecg', 'target']\nset1_dt = ht_dt[set1]\nsns.pairplot(set1_dt, hue=\"target\")","26a19fb0":"#scatterplot - set 2\nset2 = ['thalach','exang', 'oldpeak', 'slope', 'ca', 'thal', 'target']\nset2_dt = ht_dt[set2]\nsns.pairplot(set2_dt, hue=\"target\")","c671ddf3":"cm_surv = [\"darkgrey\" , \"lightgreen\"]\nage_uniq = ht_dt.age.nunique()\nprint(\"Number of unique values in age is {}\".format(age_uniq))\nsns.catplot(x=\"sex\", y=\"age\", hue=\"target\", inner=\"quart\", kind=\"violin\", palette=cm_surv, split=True, data=ht_dt)","10b5f04e":"trestbps_uniq = ht_dt.trestbps.nunique()\nprint(\"Number of unique values in trestbps is {}\".format(trestbps_uniq))\nsns.catplot(x=\"sex\", y=\"trestbps\", hue=\"target\", kind=\"violin\", inner=\"quart\", palette=cm_surv, split=True, data=ht_dt)","d141f2c2":"thalach_uniq = ht_dt.thalach.nunique()\nprint(\"Number of unique values in thalach is {}\".format(thalach_uniq))\nsns.catplot(x=\"sex\", y=\"thalach\", hue=\"target\", inner=\"quart\", kind=\"violin\", palette=cm_surv, split=True, data=ht_dt)","3b331ed6":"chol_uniq = ht_dt.chol.nunique()\nprint(\"Number of unique values in chol is {}\".format(chol_uniq))\nsns.catplot(x=\"sex\", y=\"chol\", hue=\"target\", inner=\"quart\", kind=\"violin\", palette=cm_surv, split=True, data=ht_dt)","7f4bf59f":"oldpeak_uniq = ht_dt.oldpeak.nunique()\nprint(\"Number of unique values in oldpeak is {}\".format(oldpeak_uniq))\nsns.catplot(x=\"sex\", y=\"oldpeak\", hue=\"target\", kind=\"violin\", inner=\"quart\", palette=cm_surv, split=True, data=ht_dt)","bcc203c1":"from pandasql import sqldf\npysqldf = lambda q: sqldf(q, globals())\n\nsex_q = \"\"\"\nselect sex, target, count(*) as cnt\nFrom ht_dt\nGROUP BY sex, target;\n\"\"\"\n\nsex_df = pysqldf(sex_q)\n\nsex_df_0 = sex_df[sex_df.target == 0]\nsex_df_1 = sex_df[sex_df.target == 1]\n\nimport plotly.plotly as py\nimport plotly.graph_objs as go\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\ninit_notebook_mode(connected=True)\n\nfig = {\n  \"data\": [\n    {\n      \"values\": sex_df_0.cnt,\n      \"labels\": sex_df_0.sex,\n      \"domain\": {\"x\": [0, .48]},\n      \"name\": \"No Heart Disease\",\n      \"hoverinfo\":\"label+percent+name\",\n      \"hole\": .4,\n      \"type\": \"pie\"\n    },     \n    {\n      \"values\": sex_df_1.cnt,\n      \"labels\": sex_df_1.sex,\n      \"domain\": {\"x\": [.52, 1]},\n      \"name\": \"With Heart Disease\",\n      \"hoverinfo\":\"label+percent+name\",\n      \"hole\": .4,\n      \"type\": \"pie\"\n    }],\n  \"layout\": {\n        \"title\":\"Sex Vs Target\",\n        \"annotations\": [\n            {\n                \"font\": {\n                    \"size\": 15\n                },\n                \"showarrow\": False,\n                \"text\": \"No Disease\",\n                \"x\": 0.16,\n                \"y\": 0.5\n            },\n            {\n                \"font\": {\n                    \"size\": 15\n                },\n                \"showarrow\": False,\n                \"text\": \"With Disease\",\n                \"x\": 0.85,\n                \"y\": 0.5\n            }\n        ]\n    }\n}\n\niplot(fig)","d90c35c8":"cp_q = \"\"\"\nselect cp, target, count(*) as cnt\nFrom ht_dt\nGROUP BY cp, target;\n\"\"\"\n\ncp_df = pysqldf(cp_q)\n\ncp_df_0 = cp_df[cp_df.target == 0]\ncp_df_1 = cp_df[cp_df.target == 1]\n\nfig = {\n  \"data\": [\n    {\n      \"values\": cp_df_0.cnt,\n      \"labels\": cp_df_0.cp,\n      \"domain\": {\"x\": [0, .48]},\n      \"name\": \"No Heart Disease\",\n      \"hoverinfo\":\"label+percent+name\",\n      \"hole\": .4,\n      \"type\": \"pie\"\n    },     \n    {\n      \"values\": cp_df_1.cnt,\n      \"labels\": cp_df_1.cp,\n      \"domain\": {\"x\": [.52, 1]},\n      \"name\": \"With Heart Disease\",\n      \"hoverinfo\":\"label+percent+name\",\n      \"hole\": .4,\n      \"type\": \"pie\"\n    }],\n  \"layout\": {\n        \"title\":\"cp Vs Target\",\n        \"annotations\": [\n            {\n                \"font\": {\n                    \"size\": 15\n                },\n                \"showarrow\": False,\n                \"text\": \"No Disease\",\n                \"x\": 0.17,\n                \"y\": 0.5\n            },\n            {\n                \"font\": {\n                    \"size\": 15\n                },\n                \"showarrow\": False,\n                \"text\": \"With Disease\",\n                \"x\": 0.85,\n                \"y\": 0.5\n            }\n        ]\n    }\n}\n\niplot(fig)","bf4d9796":"fbs_q = \"\"\"\nselect fbs, target, count(*) as cnt\nFrom ht_dt\nGROUP BY fbs, target;\n\"\"\"\n\nfbs_df = pysqldf(fbs_q)\n\nfbs_df_0 = fbs_df[fbs_df.target == 0]\nfbs_df_1 = fbs_df[fbs_df.target == 1]\n\nfig = {\n  \"data\": [\n    {\n      \"values\": fbs_df_0.cnt,\n      \"labels\": fbs_df_0.fbs,\n      \"domain\": {\"x\": [0, .48]},\n      \"name\": \"No Heart Disease\",\n      \"hoverinfo\":\"label+percent+name\",\n      \"hole\": .4,\n      \"type\": \"pie\"\n    },     \n    {\n      \"values\": fbs_df_1.cnt,\n      \"labels\": fbs_df_1.fbs,\n      \"domain\": {\"x\": [.52, 1]},\n      \"name\": \"With Heart Disease\",\n      \"hoverinfo\":\"label+percent+name\",\n      \"hole\": .4,\n      \"type\": \"pie\"\n    }],\n  \"layout\": {\n        \"title\":\"Fbs Vs Target\",\n        \"annotations\": [\n            {\n                \"font\": {\n                    \"size\": 15\n                },\n                \"showarrow\": False,\n                \"text\": \"No Disease\",\n                \"x\": 0.17,\n                \"y\": 0.5\n            },\n            {\n                \"font\": {\n                    \"size\": 15\n                },\n                \"showarrow\": False,\n                \"text\": \"With Disease\",\n                \"x\": 0.85,\n                \"y\": 0.5\n            }\n        ]\n    }\n}\n\niplot(fig)","8573e4b3":"restecg_q = \"\"\"\nselect restecg, target, count(*) as cnt\nFrom ht_dt\nGROUP BY restecg, target;\n\"\"\"\n\nrestecg_df = pysqldf(restecg_q)\n\nrestecg_df_0 = restecg_df[restecg_df.target == 0]\nrestecg_df_1 = restecg_df[restecg_df.target == 1]\n\nfig = {\n  \"data\": [\n    {\n      \"values\": restecg_df_0.cnt,\n      \"labels\": restecg_df_0.restecg,\n      \"domain\": {\"x\": [0, .48]},\n      \"name\": \"No Heart Disease\",\n      \"hoverinfo\":\"label+percent+name\",\n      \"hole\": .4,\n      \"type\": \"pie\"\n    },     \n    {\n      \"values\": restecg_df_1.cnt,\n      \"labels\": restecg_df_1.restecg,\n      \"domain\": {\"x\": [.52, 1]},\n      \"name\": \"With Heart Disease\",\n      \"hoverinfo\":\"label+percent+name\",\n      \"hole\": .4,\n      \"type\": \"pie\"\n    }],\n  \"layout\": {\n        \"title\":\"restecg Vs Target\",\n        \"annotations\": [\n            {\n                \"font\": {\n                    \"size\": 15\n                },\n                \"showarrow\": False,\n                \"text\": \"No Disease\",\n                \"x\": 0.17,\n                \"y\": 0.5\n            },\n            {\n                \"font\": {\n                    \"size\": 15\n                },\n                \"showarrow\": False,\n                \"text\": \"With Disease\",\n                \"x\": 0.85,\n                \"y\": 0.5\n            }\n        ]\n    }\n}\n\niplot(fig)","71d33a75":"exang_q = \"\"\"\nselect exang, target, count(*) as cnt\nFrom ht_dt\nGROUP BY exang, target;\n\"\"\"\n\nexang_df = pysqldf(exang_q)\n\nexang_df_0 = exang_df[exang_df.target == 0]\nexang_df_1 = exang_df[exang_df.target == 1]\n\nfig = {\n  \"data\": [\n    {\n      \"values\": exang_df_0.cnt,\n      \"labels\": exang_df_0.exang,\n      \"domain\": {\"x\": [0, .48]},\n      \"name\": \"No Heart Disease\",\n      \"hoverinfo\":\"label+percent+name\",\n      \"hole\": .4,\n      \"type\": \"pie\"\n    },     \n    {\n      \"values\": exang_df_1.cnt,\n      \"labels\": exang_df_1.exang,\n      \"domain\": {\"x\": [.52, 1]},\n      \"name\": \"With Heart Disease\",\n      \"hoverinfo\":\"label+percent+name\",\n      \"hole\": .4,\n      \"type\": \"pie\"\n    }],\n  \"layout\": {\n        \"title\":\"exang Vs Target\",\n        \"annotations\": [\n            {\n                \"font\": {\n                    \"size\": 15\n                },\n                \"showarrow\": False,\n                \"text\": \"No Disease\",\n                \"x\": 0.17,\n                \"y\": 0.5\n            },\n            {\n                \"font\": {\n                    \"size\": 15\n                },\n                \"showarrow\": False,\n                \"text\": \"With Disease\",\n                \"x\": 0.85,\n                \"y\": 0.5\n            }\n        ]\n    }\n}\n\niplot(fig)","667cbcf7":"sl_q = \"\"\"\nselect slope, target, count(*) as cnt\nFrom ht_dt\nGROUP BY slope, target;\n\"\"\"\n\nsl_df = pysqldf(sl_q)\n\nsl_df_0 = sl_df[sl_df.target == 0]\nsl_df_1 = sl_df[sl_df.target == 1]\n\nfig = {\n  \"data\": [\n    {\n      \"values\": sl_df_0.cnt,\n      \"labels\": sl_df_0.slope,\n      \"domain\": {\"x\": [0, .48]},\n      \"name\": \"No Heart Disease\",\n      \"hoverinfo\":\"label+percent+name\",\n      \"hole\": .4,\n      \"type\": \"pie\"\n    },     \n    {\n      \"values\": sl_df_1.cnt,\n      \"labels\": sl_df_1.slope,\n      \"domain\": {\"x\": [.52, 1]},\n      \"name\": \"With Heart Disease\",\n      \"hoverinfo\":\"label+percent+name\",\n      \"hole\": .4,\n      \"type\": \"pie\"\n    }],\n  \"layout\": {\n        \"title\":\"Slope Vs Target\",\n        \"annotations\": [\n            {\n                \"font\": {\n                    \"size\": 15\n                },\n                \"showarrow\": False,\n                \"text\": \"No Disease\",\n                \"x\": 0.17,\n                \"y\": 0.5\n            },\n            {\n                \"font\": {\n                    \"size\": 15\n                },\n                \"showarrow\": False,\n                \"text\": \"With Disease\",\n                \"x\": 0.85,\n                \"y\": 0.5\n            }\n        ]\n    }\n}\n\niplot(fig)","67aa43da":"ca_q = \"\"\"\nselect ca, target, count(*) as cnt\nFrom ht_dt\nGROUP BY ca, target;\n\"\"\"\n\nca_df = pysqldf(ca_q)\n\nca_df_0 = ca_df[ca_df.target == 0]\nca_df_1 = ca_df[ca_df.target == 1]\n\nfig = {\n  \"data\": [\n    {\n      \"values\": ca_df_0.cnt,\n      \"labels\": ca_df_0.ca,\n      \"domain\": {\"x\": [0, .48]},\n      \"name\": \"No Heart Disease\",\n      \"hoverinfo\":\"label+percent+name\",\n      \"hole\": .4,\n      \"type\": \"pie\"\n    },     \n    {\n      \"values\": ca_df_1.cnt,\n      \"labels\": ca_df_1.ca,\n      \"domain\": {\"x\": [.52, 1]},\n      \"name\": \"With Heart Disease\",\n      \"hoverinfo\":\"label+percent+name\",\n      \"hole\": .4,\n      \"type\": \"pie\"\n    }],\n  \"layout\": {\n        \"title\":\"Ca Vs Target\",\n        \"annotations\": [\n            {\n                \"font\": {\n                    \"size\": 15\n                },\n                \"showarrow\": False,\n                \"text\": \"No Disease\",\n                \"x\": 0.17,\n                \"y\": 0.5\n            },\n            {\n                \"font\": {\n                    \"size\": 15\n                },\n                \"showarrow\": False,\n                \"text\": \"With Disease\",\n                \"x\": 0.85,\n                \"y\": 0.5\n            }\n        ]\n    }\n}\n\niplot(fig)","1b802256":"thal_q = \"\"\"\nselect thal, target, count(*) as cnt\nFrom ht_dt\nGROUP BY thal, target;\n\"\"\"\n\nthal_df = pysqldf(thal_q)\n\nthal_df_0 = thal_df[thal_df.target == 0]\nthal_df_1 = thal_df[thal_df.target == 1]\n\nfig = {\n  \"data\": [\n    {\n      \"values\": thal_df_0.cnt,\n      \"labels\": thal_df_0.thal,\n      \"domain\": {\"x\": [0, .48]},\n      \"name\": \"No Heart Disease\",\n      \"hoverinfo\":\"label+percent+name\",\n      \"hole\": .4,\n      \"type\": \"pie\"\n    },     \n    {\n      \"values\": thal_df_1.cnt,\n      \"labels\": thal_df_1.thal,\n      \"domain\": {\"x\": [.52, 1]},\n      \"name\": \"With Heart Disease\",\n      \"hoverinfo\":\"label+percent+name\",\n      \"hole\": .4,\n      \"type\": \"pie\"\n    }],\n  \"layout\": {\n        \"title\":\"thal Vs Target\",\n        \"annotations\": [\n            {\n                \"font\": {\n                    \"size\": 15\n                },\n                \"showarrow\": False,\n                \"text\": \"No Disease\",\n                \"x\": 0.17,\n                \"y\": 0.5\n            },\n            {\n                \"font\": {\n                    \"size\": 15\n                },\n                \"showarrow\": False,\n                \"text\": \"With Disease\",\n                \"x\": 0.85,\n                \"y\": 0.5\n            }\n        ]\n    }\n}\n\niplot(fig)","d9deacb7":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(ht_dt.loc[:, ht_dt.columns != 'target'], \n                                                    ht_dt['target'], stratify=ht_dt['target'], \n                                                    random_state=66)\n\nprint(\"Training features have {0} records and Testing features have {1} records.\".\\\n      format(X_train.shape[0], X_test.shape[0]))","fcd5b19a":"from sklearn.linear_model import LogisticRegression\nlogreg = LogisticRegression().fit(X_train, y_train)\nprint(\"Training set score: {:.3f}\".format(logreg.score(X_train, y_train)))\nprint(\"Test set score: {:.3f}\".format(logreg.score(X_test, y_test)))","6c0cb83a":"from sklearn.tree import DecisionTreeClassifier\ntree = DecisionTreeClassifier(random_state=0)\ntree.fit(X_train, y_train)\nprint(\"Accuracy on training set: {:.3f}\".format(tree.score(X_train, y_train)))\nprint(\"Accuracy on test set: {:.3f}\".format(tree.score(X_test, y_test)))","73814995":"tree = DecisionTreeClassifier(max_depth=3, random_state=0)\ntree.fit(X_train, y_train)\nprint(\"Accuracy on training set: {:.3f}\".format(tree.score(X_train, y_train)))\nprint(\"Accuracy on test set: {:.3f}\".format(tree.score(X_test, y_test)))","5de39bf7":"print(\"Feature importances:\\n{}\".format(tree.feature_importances_))","e72075fa":"dis_ft = [x for i,x in enumerate(ht_dt.columns) if i!=8]\ndef plot_feature_importances_diabetes(model):\n    plt.figure(figsize=(8,6))\n    n_features = 13\n    plt.barh(range(n_features), model.feature_importances_, align='center')\n    plt.yticks(np.arange(n_features), dis_ft)\n    plt.xlabel(\"Feature importance\")\n    plt.ylabel(\"Feature\")\n    plt.ylim(-1, n_features)\nplot_feature_importances_diabetes(tree)\nplt.savefig('feature_importance')","91793f5c":"#Random forest with 100 trees\nfrom sklearn.ensemble import RandomForestClassifier\nrf = RandomForestClassifier(n_estimators=100, random_state=0)\nrf.fit(X_train, y_train)\nprint(\"Accuracy on training set: {:.3f}\".format(rf.score(X_train, y_train)))\nprint(\"Accuracy on test set: {:.3f}\".format(rf.score(X_test, y_test)))","4695da15":"rf1 = RandomForestClassifier(max_depth=3, n_estimators=100, random_state=0)\nrf1.fit(X_train, y_train)\nprint(\"Accuracy on training set: {:.3f}\".format(rf1.score(X_train, y_train)))\nprint(\"Accuracy on test set: {:.3f}\".format(rf1.score(X_test, y_test)))","826cc098":"plot_feature_importances_diabetes(rf1)","1690a82f":"from sklearn.ensemble import GradientBoostingClassifier\ngb = GradientBoostingClassifier(random_state=0)\ngb.fit(X_train, y_train)\nprint(\"Accuracy on training set: {:.3f}\".format(gb.score(X_train, y_train)))\nprint(\"Accuracy on test set: {:.3f}\".format(gb.score(X_test, y_test)))","2131ea84":"#GB after pruning\ngb1 = GradientBoostingClassifier(random_state=0, max_depth=1)\ngb1.fit(X_train, y_train)\nprint(\"****Gradient Boosting after Pruning using Max_depth****\")\nprint(\"Accuracy on training set: {:.3f}\".format(gb1.score(X_train, y_train)))\nprint(\"Accuracy on test set: {:.3f}\".format(gb1.score(X_test, y_test)))","75a0008c":"#GB after tuning learning rate\ngb2 = GradientBoostingClassifier(random_state=0, learning_rate=0.01)\ngb2.fit(X_train, y_train)\nprint(\"****Gradient Boosting after tuning Learning rate****\")\nprint(\"Accuracy on training set: {:.3f}\".format(gb2.score(X_train, y_train)))\nprint(\"Accuracy on test set: {:.3f}\".format(gb2.score(X_test, y_test)))","6fdc43e3":"plot_feature_importances_diabetes(gb2)","0885c9fa":"from sklearn.svm import SVC\nsvc = SVC()\nsvc.fit(X_train, y_train)\nprint(\"Accuracy on training set: {:.2f}\".format(svc.score(X_train, y_train)))\nprint(\"Accuracy on test set: {:.2f}\".format(svc.score(X_test, y_test)))","7c78e721":"from sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.fit_transform(X_test)\nsvc = SVC()\nsvc.fit(X_train_scaled, y_train)\nprint(\"****Results after scaling****\")\nprint(\"Accuracy on training set: {:.2f}\".format(svc.score(X_train_scaled, y_train)))\nprint(\"Accuracy on test set: {:.2f}\".format(svc.score(X_test_scaled, y_test)))","3679a452":"from sklearn.neural_network import MLPClassifier\nmlp = MLPClassifier(random_state=42)\nmlp.fit(X_train, y_train)\nprint(\"Accuracy on training set: {:.2f}\".format(mlp.score(X_train, y_train)))\nprint(\"Accuracy on test set: {:.2f}\".format(mlp.score(X_test, y_test)))","832188d5":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.fit_transform(X_test)\nmlp = MLPClassifier(random_state=0)\nmlp.fit(X_train_scaled, y_train)\nprint(\"****Results after scaling****\")\nprint(\"Accuracy on training set: {:.3f}\".format(mlp.score(X_train_scaled, y_train)))\nprint(\"Accuracy on test set: {:.3f}\".format(mlp.score(X_test_scaled, y_test)))","abdf8bb8":"#Tuning the iteration\nmlp = MLPClassifier(max_iter=1000, random_state=0)\nmlp.fit(X_train_scaled, y_train)\nprint(\"****Results after tuning iteration****\")\nprint(\"Accuracy on training set: {:.3f}\".format(mlp.score(X_train_scaled, y_train)))\nprint(\"Accuracy on test set: {:.3f}\".format(mlp.score(X_test_scaled, y_test)))","cc599f12":"mlp = MLPClassifier(max_iter=100, alpha=1, random_state=0)\nmlp.fit(X_train_scaled, y_train)\nprint(\"****Results after tuning alpha & regularizing the weights****\")\nprint(\"Accuracy on training set: {:.3f}\".format(mlp.score(X_train_scaled, y_train)))\nprint(\"Accuracy on test set: {:.3f}\".format(mlp.score(X_test_scaled, y_test)))","cb8c1037":"plt.figure(figsize=(20, 5))\nplt.imshow(mlp.coefs_[0], interpolation='none', cmap='viridis')\nplt.yticks(range(13), dis_ft)\nplt.xlabel(\"Columns in weight matrix\")\nplt.ylabel(\"Input feature\")\nplt.colorbar()","987a8b01":"**Plotting the first layer weights in a Neural Network**","b93d012e":"**Bi-variate analysis with respect to Target variable**","45bfbe0d":"**Decision Tree**","ffe9f8f6":"The model overfits quite substantially, with a perfect score on the training set and only 54% accuracy on the test set.\n\nSVM requires all the features to vary on a similar scale. We will need to re-scale our data that all the features are approximately on the same scale.","3294573e":"**Correaltion co-efficient Plot**","b2282db5":"**Viewing the shape of the dataset**","bd31ac31":"The dataset has no missing values in it. Hence, the plot is in one color.","a78284fe":"**Correlation between the Features**","1db871bb":"**Gradient Boosting**","ae98d874":"The Pearson correlation coefficient, r, can take a range of values from +1 to -1. A value of 0 indicates that there is no association between the two variables. A value greater than 0 indicates a positive association; that is, as the value of one variable increases, so does the value of the other variable.\n\nHere, we can see the maximum correlation value is approximately lies between -.5 and .5 which means none of the variables have strong correlation.","56be0e57":"**Note:**   \nKindly upvote the kernel if you find it useful. Suggestions are always welome. Let me know your thoughts in the comment if any.\n\nExperimenting with different models are the original work of Susan Li and I want to try her work on the new dataset and see how the results vary & learn things from it.\n\nLink of the original modelling work:    \n[Susan Li Work](https:\/\/towardsdatascience.com\/machine-learning-for-diabetes-562dd7df4d42)","c1e4a58b":"**Fbs Vs Target**","5a822bcc":"**Target Proportion in the Dataset**","af976096":"**Missing value details**","d63ead3c":"**Chol Vs Target**","bbbfecce":"**Context**  \nThis database contains 76 attributes, but all published experiments refer to using a subset of 14 of them. In particular, the Cleveland database is the only one that has been used by ML researchers to this date. The \"goal\" field refers to the presence of heart disease in the patient. It is integer valued from 0 (no presence) to 4.\n\n**Attribute Information:**   \n1. Age   \n2. Sex   \n3. Chest pain type (4 values)  \n4. Resting blood pressure  \n5. Serum cholestoral in mg\/dl  \n6. Fasting blood sugar > 120 mg\/dl \n7. Resting electrocardiographic results (values 0,1,2) \n8. Maximum heart rate achieved  \n9. Exercise induced angina  \n10. Oldpeak = ST depression induced by exercise relative to rest  \n11. The slope of the peak exercise ST segment  \n12. Number of major vessels (0-3) colored by flourosopy  \n13. Thal: 3 = normal; 6 = fixed defect; 7 = reversable defect   \n\nThe names and social security numbers of the patients were recently removed from the database, replaced with dummy values. One file has been \"processed\", that one containing the Cleveland database. All four unprocessed files also exist in this directory.\n\n**Acknowledgements - Creators: **  \n* Hungarian Institute of Cardiology. Budapest: Andras Janosi, M.D.   \n* University Hospital, Zurich, Switzerland: William Steinbrunn, M.D.   \n* University Hospital, Basel, Switzerland: Matthias Pfisterer, M.D.   \n* V.A. Medical Center, Long Beach and Cleveland Clinic Foundation: Robert Detrano, M.D., Ph.D.  \n\nDonor: David W. Aha (aha '@' ics.uci.edu) (714) 856-8779  \n\n**Inspiration**  \nExperiments with the Cleveland database have concentrated on simply attempting to distinguish presence (values 1,2,3,4) from absence (value 0).","7af225ae":"**exang Vs Target**","de874419":"**Random Forest**","3df20fed":"**Oldpeak Vs Target**","ffb6c8e4":"**Sex Vs Target**","813bac77":"[](https:\/\/www.kaggle.com\/ronitf\/heart-disease-uci)","1e37b1e4":"**Feature importance in Random Forest**","b9f2e150":"**Slope Vs Target**","646b2fe6":"**Support Vector Machine**","c3fb66c0":"**Trestbps Vs Target**","ec35c63c":"**Global Options**","3a6dcd52":"**Sample of the dataset**","a1622561":"**Logistic regression**","fdcf3245":"**Reading the Dataset**","37b723da":"**Feature Importance using GB**","ce4be18a":"**thal Vs Target**","ffc060d6":"![](https:\/\/resources.healthydirections.com\/resources\/web\/articles\/hd\/hd-women-and-heart-disease-sinatra-rollup-hd-cover.jpg)","86085acd":"Increasing the number of iterations only increased the training set performance, not the test set performance.\n\nLet\u2019s increase the alpha parameter and add stronger regularization of the weights.","be68e7b7":"**restecg Vs Target**","c1a8af1b":"**Visualizing the missing value**","720ea2cb":"**Deep Learning**","b434c26c":"The accuracy of the Multilayer perceptrons (MLP) is not as good as the other models at all, this is likely due to scaling of the data. deep learning algorithms also expect all input features to vary in a similar way, and ideally to have a mean of 0, and a variance of 1. We must re-scale our data so that it fulfills these requirements.","6b4b0a18":"**Age Vs Target**","98f10aa7":"**Thalach Vs Target**","ae95e63b":"**Listing the files**","59792c2c":"**Training & Testing Dataset**","8afad12f":"**ca Vs Target**","e1d9ea4b":"Random forest gives better accuracy with respect to Test accuracy when compared to Decision trees. Now, let us prune the depth of trees and check the accuracy.","8e771c4a":"The accuracy on the training set is 100%, while the test set accuracy is much worse. This is an indicative that the tree is overfitting and not generalizing well to new data. Therefore, we need to apply pre-pruning to the tree.\n\nWe set max_depth=3, limiting the depth of the tree decreases overfitting. This leads to a lower accuracy on the training set, but an improvement on the test set.","4af1a6de":"**Feature Importance in Decision Trees**","dae6a26b":"**Cp Vs Target**","37acd50f":"**Stay Tuned....**"}}