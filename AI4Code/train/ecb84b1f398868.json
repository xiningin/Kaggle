{"cell_type":{"8622843a":"code","cdcd1885":"code","e9d12bc5":"code","b7b77fab":"code","079b756b":"code","b431eb02":"code","a8e791dc":"code","985d666e":"code","9dc811da":"code","0361e1cf":"code","8e8b2e5f":"code","c8bf828d":"code","1dafaa3a":"code","f742ecb7":"markdown","f274b58c":"markdown","70fbdc64":"markdown","c5b34e2b":"markdown","cd6a7c09":"markdown","55f017a3":"markdown","b88e9a4a":"markdown","3d96d77d":"markdown","b50265e9":"markdown","245aac38":"markdown","e15567b5":"markdown"},"source":{"8622843a":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt","cdcd1885":"# Set up the X values.\n\n# Anscombe's Quartet - Set 1 - X Values\n\nX=np.array([10.0, 8.0, 13.0, 9.0, 11.0, 14.0, 6.0, 4.0 ,12.0 ,7.0, 5.0], dtype=\"float64\")\n\nX.flags.writeable = False   # Protecting the contents of X - make it immutable (ie. read only).","e9d12bc5":"# Set up the Y values.\n\n\n# Anscombe's Quartet - Set 1 - Y Values\n\nY=np.array([8.04, 6.95, 7.58, 8.81, 8.33, 9.96, 7.24, 4.26, 10.84, 4.82, 5.68], dtype=\"float64\")\n\nY.flags.writeable = False  # Protecting the contents of Y - make it immutable (ie. read only).\n","b7b77fab":"# Setting up some global values and also plotting the data to get a feel for it.\n\nXmin=np.min(X)\nXmax=np.max(X)\nYmin=np.min(Y)\nYmax=np.max(Y)\nprint(\"X min & max:\", Xmin, Xmax)\nprint(\"Y min & max:\", Ymin, Ymax)\n\n\n# plt.xlim(Xmin-1,Xmax+1)\nplt.xlim(0,Xmax+1)\nplt.ylim(0,Ymax+1)\n\nplt.scatter(X,Y, 10, color = 'blue')\nplt.show()","079b756b":"X_s=np.empty_like(X, dtype=\"float64\")     # X_s  X standardised\nY_s=np.empty_like(Y, dtype=\"float64\")     # Y_s  Y standardised\n\nX_s = (X - Xmin)\/(Xmax-Xmin)\nY_s = (Y - Ymin)\/(Ymax-Ymin)\n\n\nfor i in range(0, X.size):\n    print(\"i= \", i, \"\\t\\tx {:6.2f}\".format(X_s[i]),\"\\ty {:6.2f}\".format(Y_s[i]))\n\n\n\nplt.scatter(X_s,Y_s, 10, color = 'blue')\nplt.show()","b431eb02":"def CalculateNewY(X_orig, slope, intercept):\n    \n    Y_calc = np.empty_like(X_orig, dtype=\"float64\")\n\n    Y_calc = X_orig*slope+intercept\n        \n    return Y_calc","a8e791dc":"def CalculateSSE(original_Y, predicted_Y):\n    theSSE=0.0\n    \n    for i in range(0, original_Y.size):\n        theSSE += (original_Y[i]-predicted_Y[i])**2\n        \n    theSSE = theSSE\/2\n    \n    return theSSE","985d666e":"def SumPartialDerivativeOf_m (original_Y, calculated_Y , original_X):\n    \n    theSPD_m = 0.0\n    \n    for i in range(0, original_Y.size):\n        theSPD_m += original_X[i] *(calculated_Y[i]-original_Y[i])      \n\n    return theSPD_m","9dc811da":"def SumPartialDerivativeOf_c (original_Y, calculated_Y ):\n    \n    theSPD_c = 0.0\n    \n    for i in range(0, original_Y.size):\n        theSPD_c +=  calculated_Y[i] - original_Y[i]\n\n        \n    return theSPD_c","0361e1cf":"# Helper function\n\ndef DrawLineFromFormula(slope, intercept, color):\n    plt.xlim(-0.05, 1.05)\n    plt.ylim(-0.05, 1.05)\n    x = np.arange(-100, 100, 0.1)\n    plt.plot(x, slope*x+intercept, color)\n    return","8e8b2e5f":"# The Iteration\n\n# This is where we iterate until the optimistaion equation has stopped getting any better\n\ndef trials( m = 1, c = 0.75 , r= 0.01, acceptableDifference = 0.000001, maxNumOfTrials = 10000 ):\n\n    SSE_storage = []\n    \n    recordOfIterations = []\n    recordOfSlope = []\n    recordOfIntercept = []\n    recordOfSSE = []\n    \n\n    for i in range(0, maxNumOfTrials):    \n    \n        Y_hat = CalculateNewY(X_s, m, c)\n\n        ourSSE = CalculateSSE(Y_s,Y_hat)\n    \n        SSE_storage.append(ourSSE)    # This list is used to store the SSE errors - used for plotting later.\n        \n \n        if ( i > 0):\n            \n            if ( abs(oldSSE-ourSSE) < acceptableDifference):\n                print(\"\\nAfter \", i, \"iterations - we are done({:.10f})!\\n\\nOld SSE:\".format(acceptableDifference),oldSSE, \" new SSE: \", ourSSE, \"\\t Difference < {:12.10f}\".format(oldSSE-ourSSE),\"\\n\" )\n                \n                # Make sure to store the last value !\n        \n                recordOfIterations.append(i)\n                recordOfSlope.append(m)\n                recordOfIntercept.append(c)\n                recordOfSSE.append(ourSSE)        \n                break\n    \n            if( ourSSE > oldSSE):\n                print(\"Error adjustment process going the wrong way ...abort.\")\n                break\n\n\n        ourSPD_m = SumPartialDerivativeOf_m( Y_s, Y_hat, X_s)\n        ourSPD_c = SumPartialDerivativeOf_c( Y_s, Y_hat)\n\n        m = m - r*ourSPD_m\n        c = c - r*ourSPD_c\n\n        if (i%100 == 0):\n#       print(\"{:12}\".format(i),\"{:12.6f}\".format(m), \"{:12.6f}\".format(c), \"{:16.14f}\".format(ourSSE))\n            recordOfIterations.append(i)\n            recordOfSlope.append(m)\n            recordOfIntercept.append(c)\n            recordOfSSE.append(ourSSE)\n        \n        if((i%100 ==0)):\n            DrawLineFromFormula(m, c, 'g--')\n            \n        oldSSE = ourSSE\n        \n\n# Show the table of values\n\n    whatHappened = pd.DataFrame({\"Iterations\":recordOfIterations, \n                                \"Slope\":recordOfSlope,\n                                \"c\":recordOfIntercept,    \n                                \"SSE\":recordOfSSE\n                            })\n\n    pd.set_option('display.max_rows', None)\n    display(whatHappened)\n\n# Plot the original points and the final line.\n\n    plt.scatter(X_s, Y_s, 30, color = 'blue')\n\n    DrawLineFromFormula(m, c, 'black')\n    \n    plt.show()\n\n# Plot the SSE - it should show a nice decrease.\n# Plotting the error\n    \n    plt.title(\"SSE Plot\")\n    plt.xlabel('Number of iterations')\n    plt.ylabel('SSE')\n    plt.plot(SSE_storage)\n    plt.show()\n    \n\n    return m,c\n","c8bf828d":"m_slope = 1.1\nc_intercept = 2.0\nr_learning_rate = .01\n\nm,c = trials( m_slope, c_intercept, r_learning_rate, 0.0000000001, 3000)\n","1dafaa3a":"print(m, c)\nprint(\"Ymin:\", Ymin)\nprint(\"Y Range:\", Ymax - Ymin)\n\nc_final = (c * (Ymax - Ymin)) + Ymin\n\nprint(\"m {:6.4f}\".format(m),\"\\t final c {:6.4f}\".format(c_final))\n\n# plt.xlim(-1,Xmax+1)\n# plt.ylim(2,Ymax+1)\n\nx = np.arange(0, Xmax+1, 0.1, dtype=\"float64\")\ny = np.empty_like(x, dtype=\"float64\") \ny = m*x + c_final\n\n# plt.plot(x, y, 'black')\n# Plot the original points and the final line.\n\npoints = np.arange(Xmin-1, Xmax+1, 0.1)\nplt.plot(points, m*points+c_final, 'g--')\n\n\nplt.scatter(X,Y, 10, color = 'blue')\nplt.show()","f742ecb7":"For the Cost Function we are going to use the Sum of Squared Errors (SSE) function, which we define as follows:<br><br>\n$$\\frac{1}{2}\\sum_{1}^n (y-\\hat{y})^2  $$\n<br>\n$y$ is the sample value - the one that is given. <br>\n$\\hat{y}$ is the predicted value for the same x using the equation.<br><br>\nInitially the difference between these, $y-\\hat{y}$, will be large but by minimising the SSE equation we should be able to reduce this difference to be negligible.","f274b58c":"We now want to \"standardise\" the data. This will make the optimisation quicker. There are various ways to standardise the data but I am going to set the minimum value to 0 and the maximum value to 1. \n\nThe formula is $$ x_{new} = \\frac{ x_{orig} - x_{min}}{x_{max} - x_{min}} $$\n\n\n<b>Post Script<\/b>: The optimisation method that I use later, gradient descent using the Sum of Square Errors function(SSE), does not work unless the data is standardised. Go ahead and try it. If you do not standarise, the optimisation blows up! \n\n<b>To Try<\/b>: Implement the optimisation with another error function.","70fbdc64":"<br>We need to calculate the values of Y that are produced by our equation $y=mx+c$ equation.<br>","c5b34e2b":"Recall that we used the functions, $$ x_{new} = \\frac{ x_{orig} - x_{min}}{x_{max} - x_{min}} $$ and $$ y_{new} = \\frac{ y_{orig} - y_{min}}{y_{max} - y_{min}} $$<br>, to \"standardise\" our samples (the x,y pairs we were given at the start). \n\nOur new <b>m<\/b> is correct but our <b>c<\/b> needs to be mapped back to the range that our original y values were in.\n\n\nTo reverse this we need to use the following \n\n$$ y_{orig} = ({ y_{new} } * (y_{max} - y_{min})) + y_{min}  $$\n\nWe need to execute this once to map our <b>c<\/b> back to the orginal range.","cd6a7c09":"Not correct - the intercept 'c' is far too big. \n\n**What is the proper way to undo the standardisation of the sample data?**\n","55f017a3":"You need to next get the gradients of the Cost Function - these are the partial derivatives of the SSE wrt the slope and the intercept <br><br>\n$$ \\frac{\\partial SSE}{\\partial m} and \\frac{\\partial SSE}{\\partial c}$$\n\n\nEventually we are going to take the partial derivatives w.r.t. m and c of SE but we need to prepare SE first. That is, to get a form of it that makes taking the partial derivatives possible.<br>\n\n$$\n\\begin{eqnarray}\nSE = \\frac{1}{2}(y-\\hat{y})^2  \\\\\nSE = \\frac{1}{2}(y^2+\\hat{y}^2 -2y\\hat{y}) \n\\end{eqnarray}\n$$\n\n\nRecall that $\\hat{y}=mx+c$ gives us...\n$$\n\\begin{eqnarray}\nSE = \\frac{1}{2}(y^2+(mx+c)^2-2y(mx+c)) \\\\\nSE = \\frac{1}{2}(y^2+m^2x^2+c^2+2mxc -2ymx -2yc )\n\\end{eqnarray}\n$$\n<br><br> We now need to take partial derivatives w.r.t. m and c (\n$ \\frac{\\partial SE}{\\partial m} $ and $ \\frac{\\partial SE}{\\partial c} $) <br><br>\n\n$$\n\\begin{eqnarray}\n\\frac{\\partial SE}{\\partial m} = \\frac{1}{2} (2mx^2+2xc-2yx)    \\\\  \n\\\\\n\\frac{\\partial SE}{\\partial m} = mx^2+xc-yx \\\\\n\\\\\n\\frac{\\partial SE}{\\partial m} =x(mx+c-y)  \n\\end{eqnarray}\n$$\n\nRecall $\\hspace{1cm} \\hat{y}=mx+c $\n\n$$ \\frac{\\partial SE}{\\partial m} =x( \\hat{y}-y)$$\nThis can also be written as\n$$ \\frac{\\partial SE}{\\partial m} = -( y - \\hat{y})x$$\nOn to the partial derivative of SE w.r.t. <b>c<\/b>.\n$$\n\\begin{eqnarray}\n\\frac{\\partial SE}{\\partial c} = \\frac{1}{2} (2c+2mx-2y) \\\\\n\\frac{\\partial SE}{\\partial c} = c+mx-y  \\\\\n\\frac{\\partial SE}{\\partial c} = mx+c-y  \\\\\n\\end{eqnarray}\n$$\n\nRecall $\\hspace{1cm} \\hat{y}=mx+c        $\n$$\\frac{\\partial SE}{\\partial c} =  \\hat{y}-y   $$ \nThis can also be written as\n$$\\frac{\\partial SE}{\\partial c} = -(y- \\hat{y})   $$ ","b88e9a4a":"To get better and better values for m and c you re-run the following code:\n","3d96d77d":"Recall $$y=mx+c$$ We have to choose values for m and c. \n<br><br>\nThese can be anything but if they are \"reasonable\" you will need less iterations to converge on good values for m and c. \n\n","b50265e9":"<h1>Simple Linear Regression<\/h1>\nwith one independent variable <br><br>\nGoing to create a simple <b>linear regression<\/b> tool based on the equation of a line:<br><br>\n$$y=mx+c$$ <br>\n\n<br>** Everything is good until I try to undo the standardisation - see the very end.**\n\n","245aac38":"<br><br>\n$\\frac{\\partial SSE}{\\partial m} is just the \\sum \\frac{\\partial SE}{\\partial m}  $ <br><br>and<br><br> $\\frac{\\partial SSE}{\\partial c} is just the \\sum \\frac{\\partial SE}{\\partial c} $.","e15567b5":"We now need to update <b>m<\/b> and <b>c<\/b> so that they become better values. Recall that we start off with random values of <b>m<\/b> abd <b>c<\/b>. The update rules are:\n\n$$ m_{new}= m -r \\sum \\frac{\\partial SE}{\\partial m} $$\n<br>\n$$ c_{new}= c -r \\sum \\frac{\\partial SE}{\\partial c} $$\n<br>\n<br>\n<b>r<\/b> is the learning rate, the pace of adjustment of <b>m<\/b> and <b> c<\/b>."}}