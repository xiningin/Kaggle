{"cell_type":{"6916c878":"code","dd903146":"code","be717923":"code","2cb89c47":"code","a880a1f4":"code","d0050851":"code","f982318d":"code","5cd1054c":"code","0f57a8d0":"code","1397b52a":"code","106cf0f3":"code","4e36e41b":"markdown","d19f2eb5":"markdown","f820076f":"markdown","bbe8b83d":"markdown","175a541b":"markdown"},"source":{"6916c878":"import pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.metrics import mean_squared_error\nfrom xgboost import XGBRegressor\nimport optuna","dd903146":"df = pd.read_csv('..\/input\/train-stratkfolds-7\/train_stratkfolds.csv')\ndf_test = pd.read_csv('..\/input\/30-days-of-ml\/test.csv')\ndf_sample_submission = pd.read_csv('..\/input\/30-days-of-ml\/sample_submission.csv')","be717923":"useful_features = [col for col in df.columns if col not in ('id', 'target', 'kfold', 'cutbin', 'stratkfold')]\nobject_cols = [col for col in useful_features if col.startswith('cat')]\ndf_test = df_test[useful_features]","2cb89c47":"for col in object_cols:\n    temp_df = []\n    temp_test_feat = None\n    for fold in range(7):\n        xtrain = df[df['stratkfold'] != fold].reset_index(drop=True)\n        xvalid = df[df['stratkfold'] == fold].reset_index(drop=True)\n        feat = xtrain.groupby(col)['target'].agg('mean')\n        feat = feat.to_dict()\n        #print(feat)\n        xvalid.loc[:, f'tar_enc_{col}'] = xvalid[col].map(feat)\n        temp_df.append(xvalid)\n        if temp_test_feat is None:\n            temp_test_feat = df_test[col].map(feat)\n        else:\n            temp_test_feat += df_test[col].map(feat)\n        \n    temp_test_feat \/= 7    \n    #print(temp_test_feat)\n    df_test.loc[:, f'tar_enc_{col}'] = temp_test_feat\n    df = pd.concat(temp_df)\n","a880a1f4":"useful_features = [col for col in df.columns if col not in ('id', 'target', 'kfold', 'cutbin', 'stratkfold')]","d0050851":"# Only to discover the best stratkfold to tune\nfinal_predictions = []\nscores = []\nfor fold in range(7):\n    xtrain = df[df['stratkfold'] != fold].reset_index(drop=True)\n    xvalid = df[df['stratkfold'] == fold].reset_index(drop=True)\n    xtest = df_test.copy()\n    \n    ytrain = xtrain['target']\n    yvalid = xvalid['target']\n    \n    xtrain = xtrain[useful_features]\n    xvalid = xvalid [useful_features]\n    \n    ordinal_encoder = OrdinalEncoder()\n    \n    xtrain[object_cols] = ordinal_encoder.fit_transform(xtrain[object_cols])\n    xvalid[object_cols] = ordinal_encoder.transform(xvalid[object_cols])\n    xtest[object_cols] = ordinal_encoder.transform(xtest[object_cols])\n    \n    model = XGBRegressor(\n        \n        random_state=fold,\n        tree_method='gpu_hist',\n        gpu_id=0,\n        predictor = 'gpu_predictor',\n    )\n    model.fit(\n        xtrain,\n        ytrain,\n        early_stopping_rounds=100,\n        eval_set = [(xvalid, yvalid)],\n        verbose = False,\n        \n    )\n    preds_valid = model.predict(xvalid)\n    test_preds = model.predict(xtest)\n    final_predictions.append(test_preds)\n    rmse = mean_squared_error(yvalid, preds_valid, squared=False)\n    scores.append(rmse)\n    print(fold, rmse)\n    \nprint('Average rmse {}'.format(np.mean(scores)))","f982318d":"def run(trial):\n    final_predictions = []\n    fold = 0\n    learning_rate = trial.suggest_float('learning_rate', 0.01, 1)\n    gamma = trial.suggest_float('gamma', 0.01, 1)\n    min_child_weight = trial.suggest_float('min_child_weight', 0.1, 10)\n    reg_lambda = trial.suggest_float('reg_lambda', 0.01, 10)\n    reg_alpha = trial.suggest_float('reg_alpha', 0.01, 10)\n    subsample = trial.suggest_float('subsample', 0.01, 1.0)\n    colsample_bytree = trial.suggest_float('colsample_bytree', 0.1, 1)\n    max_depth = trial.suggest_int('max_depth', 1, 7)\n\n    xtrain = df[df['stratkfold'] != fold].reset_index(drop=True)\n    xvalid = df[df['stratkfold'] == fold].reset_index(drop=True)\n\n    ytrain = xtrain['target']\n    yvalid = xvalid['target']\n\n    xtrain = xtrain[useful_features]\n    xvalid = xvalid [useful_features]\n\n    ordinal_encoder = OrdinalEncoder()\n\n    xtrain[object_cols] = ordinal_encoder.fit_transform(xtrain[object_cols])\n    xvalid[object_cols] = ordinal_encoder.transform(xvalid[object_cols])\n\n    model = XGBRegressor(\n        random_state=42,\n        tree_method='gpu_hist',\n        gpu_id=0,\n        predictor = 'gpu_predictor',\n        n_estimators = 5000,\n        learning_rate = learning_rate,\n        gamma = gamma,\n        min_child_weight = min_child_weight,\n        reg_lambda = reg_lambda, \n        reg_alpha = reg_alpha, \n        colsample_bytree = colsample_bytree,\n        subsample = subsample,\n        max_depth = max_depth,\n    )\n    model.fit(\n        xtrain,\n        ytrain,\n        early_stopping_rounds=100,\n        eval_set = [(xvalid, yvalid)],\n        verbose = False,\n    )\n    preds_valid = model.predict(xvalid)\n    rmse =  mean_squared_error(yvalid, preds_valid, squared=False)\n    print(fold, rmse)\n    return rmse","5cd1054c":"study = optuna.create_study(direction='minimize')\nstudy.optimize(run, n_trials=10)","0f57a8d0":"print(study.best_params)\nprint(study.best_value)\n\nparams = study.best_params","1397b52a":"final_predictions = []\nscores = []\nfor fold in range(7):\n    xtrain = df[df['stratkfold'] != fold].reset_index(drop=True)\n    xvalid = df[df['stratkfold'] == fold].reset_index(drop=True)\n    xtest = df_test.copy()\n    \n    ytrain = xtrain['target']\n    yvalid = xvalid['target']\n    \n    xtrain = xtrain[useful_features]\n    xvalid = xvalid [useful_features]\n    \n    ordinal_encoder = OrdinalEncoder()\n    \n    xtrain[object_cols] = ordinal_encoder.fit_transform(xtrain[object_cols])\n    xvalid[object_cols] = ordinal_encoder.transform(xvalid[object_cols])\n    xtest[object_cols] = ordinal_encoder.transform(xtest[object_cols])\n    \n    model = XGBRegressor(\n        random_state=fold,\n        tree_method='gpu_hist',\n        gpu_id=0,\n        predictor = 'cpu_predictor',\n        n_estimators = 1000,\n        **params,\n    )\n    model.fit(\n        xtrain,\n        ytrain,\n        early_stopping_rounds=100,\n        eval_set = [(xvalid, yvalid)],\n        verbose = False,\n        \n    )\n    preds_valid = model.predict(xvalid)\n    test_preds = model.predict(xtest)\n    final_predictions.append(test_preds)\n    rmse = mean_squared_error(yvalid, preds_valid, squared=False)\n    scores.append(rmse)\n    print(fold, rmse)\n\nprint('Average rmse {}'.format(np.mean(scores)))","106cf0f3":"# preds = np.mean(np.column_stack(final_predictions), axis=1 )\n# print(preds)\n\n# df_sample_submission.head()\n\n# df_sample_submission['target'] = preds\n# df_sample_submission.head()\n\n# df_sample_submission.to_csv('my_submission.csv', index=False)","4e36e41b":"> When running the study I did it 100 times ant the best one was the **96th** run. For this notebook I will running only 10 run for my time sake!\n\n[I 2021-09-18 20:49:28,775] Trial 96 finished with value: 0.7166124671577337 and parameters: {'learning_rate': 0.0917516109299909, 'gamma': 0.7129456406330918, 'min_child_weight': 4.208288519604158, 'reg_lambda': 1.9462132944061952, 'reg_alpha': 1.773899502516016, 'subsample': 0.7708950167962303, 'colsample_bytree': 0.3345290316341074, 'max_depth': 2}. Best is trial 96 with value: 0.7166124671577337.","d19f2eb5":"___\n### OPTUNA","f820076f":"___\n\n### Target Encoding\nThis target encoding follow the method of @abhisek with the difference that I try seven (7) folds against 5. I choose the binning number following the sturge's rule = 1 + log(N).\nThe trainning file with those folds it's already uploaded so you can try it. I'll upload the notebook asap. I need to polish it a bit.","bbe8b83d":"### XGB with Optuna\nI consider myself a beginner and I'm thankful for many of you that share your knowledge, so I'll share every notebook that I think it's worthy and this definitely it's one of it.\n\nInfinite thanks to [@abhishek_thakur](https:\/\/www.kaggle.com\/abhishek) and their [youtube videos](https:\/\/www.youtube.com\/watch?v=_55G24aghPY&list=PL98nY_tJQXZnP-k3qCDd1hljVSciDV9_N), a great start for everyone, so follow and support him if you liked. In fact, many of the code in this notebook it's similar to those videos. \nThere are infinite great notebooks around here and many of them I find them by chance, but I want to mention this one just to mention to support another beginner, [A Beginners Approach to 30 Days of ML (top 15%)](https:\/\/www.kaggle.com\/evi125\/a-beginners-approach-to-30-days-of-ml-top-15), you can find there many tips of how to face the challenge.\n\nAnd if you liked my work please don't forget to vote, thank you!","175a541b":"Here, in my own laptop, I tried 10000 (10K) estimators, so I will keep it in 1K for the same reason, my time sake! You can try whatever you want and don't forget to turn on **GPU!!**"}}