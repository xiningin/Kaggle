{"cell_type":{"9e4531b5":"code","4d3d4954":"code","a0ed0dfa":"code","73f40e3c":"code","1037952c":"code","0b20a5d0":"code","f32e55ea":"code","8641662f":"code","805539bd":"code","cef53f00":"code","9b9b281d":"code","e1f7ec21":"code","d3190ad7":"code","e0569123":"code","26d89422":"code","e4401150":"code","804acb4f":"code","5f23f5df":"code","e4022b2d":"code","62df9724":"code","2deecff3":"code","b67f4acc":"code","b805b06f":"code","ebcdc17e":"code","2c4cf81d":"code","704ed313":"code","88b8e2f4":"code","a6a9f641":"code","42cf7e51":"code","c5296a10":"code","e9cfb6ee":"code","bfee832f":"code","e9a7b159":"code","0419bb64":"code","acbc8bf6":"code","35d9176b":"code","788648a5":"code","3c65e88a":"code","5a99632d":"code","ccc8951a":"code","3643199c":"code","25a39a42":"code","a181ec57":"code","5bfda86f":"code","852468de":"code","f57e6826":"code","0c71fbf1":"code","b98f74c5":"code","b30e15b0":"code","909eb7a0":"code","1ac3c053":"code","372d7285":"code","b18d7754":"code","8e4e5e9c":"code","a8eb57aa":"code","5a2deaab":"code","e766ac1f":"code","8f7f26d8":"code","60e8ea50":"markdown","a1049361":"markdown","4bf1d775":"markdown","78f61eb9":"markdown","9fe33590":"markdown","73df822a":"markdown","ebfcf59c":"markdown"},"source":{"9e4531b5":"# Import required libraries","4d3d4954":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport xgboost as xgb\nimport lightgbm as lgb\nimport datetime as dt\nimport calendar,warnings,itertools,matplotlib,keras,shutil\nimport tensorflow as tf\nimport statsmodels.api as sm\nfrom datetime import datetime\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split,cross_val_score, cross_val_predict\nfrom sklearn import svm,metrics,tree,preprocessing,linear_model\nfrom sklearn.preprocessing import MinMaxScaler,StandardScaler\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier, BaggingClassifier,ExtraTreesClassifier\nfrom sklearn.metrics import accuracy_score,mean_squared_error,recall_score,confusion_matrix,f1_score,roc_curve, auc\nfrom sklearn.datasets import load_iris,make_regression\n#from chefboost import Chefboost as chef\nfrom sklearn.metrics import roc_auc_score, accuracy_score, confusion_matrix, f1_score, precision_score, \\\nrecall_score, matthews_corrcoef, precision_recall_curve\nfrom sklearn.metrics import auc","a0ed0dfa":"import sklearn\n# Gradient Boosters\nimport xgboost as xgb # Accuracy\nfrom xgboost import XGBClassifier\n\nfrom sklearn.linear_model import LogisticRegression, SGDClassifier\nfrom sklearn.model_selection import train_test_split,cross_val_score, cross_val_predict\nfrom sklearn import svm,metrics,tree,preprocessing,linear_model\n# Decision Tree\nfrom sklearn.tree import DecisionTreeClassifier\n# Ensemble\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, VotingClassifier,ExtraTreesClassifier\n# Guassian\nfrom sklearn.gaussian_process import GaussianProcessClassifier\nfrom sklearn.gaussian_process.kernels import RBF\n# Bayesian\nfrom sklearn.naive_bayes import GaussianNB\n# Dimensionality Reduction\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n# Instance Based\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import StratifiedShuffleSplit\n# Nueral Network\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.metrics import accuracy_score,mean_squared_error,recall_score,confusion_matrix,f1_score,roc_curve, auc\nfrom sklearn.datasets import load_iris,make_regression\nfrom sklearn.svm import SVC\n\nfrom sklearn.metrics import roc_auc_score, accuracy_score, confusion_matrix, f1_score, precision_score,log_loss \nrecall_score\nfrom sklearn.metrics import precision_recall_curve\nfrom sklearn.metrics import auc\nfrom sklearn.model_selection import BaseCrossValidator, GridSearchCV, train_test_split,cross_val_score,cross_validate,cross_val_predict, KFold, StratifiedKFold, learning_curve\nfrom sklearn.metrics.cluster import fowlkes_mallows_score\nfrom sklearn.model_selection   import  RandomizedSearchCV\n# Standardization\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler","73f40e3c":"#Importing Dataset using pandas library\ndataset=pd.read_csv(\"..\/input\/predictive-main\/predictive_maintenance.csv\",header= 0,encoding= 'unicode_escape')\ndataset.head(10)# Checking top 10 rows in dataset","1037952c":"# Checking missing values\ndataset.apply(lambda x: sum(x.isnull())) ","0b20a5d0":"dataset.info()","f32e55ea":"dataset.nunique()","8641662f":"# Mode of Failure Types\ndataset['Failure Type'].unique()","805539bd":"dataset['Target'].unique()","cef53f00":"dataset['Target'].value_counts()","9b9b281d":"sns.countplot(x='Target', data=dataset)\nplt.title('Count of Target')","e1f7ec21":"g=sns.catplot(x='Failure Type', col='Target', kind='count', data=dataset)\ng.set_xticklabels(rotation=45)","d3190ad7":"g=sns.catplot(x='Type', col='Target', kind='count', data=dataset)\ng.set_xticklabels(rotation=0)","e0569123":"sns.pairplot(dataset.select_dtypes(exclude=object))","26d89422":"# dropping \ndataset = dataset.drop(['\u00ef\u00bb\u00bfUDI', 'Product ID'], axis=1)\ndataset","e4401150":"dataset.info()","804acb4f":"plt.figure(figsize=(12, 12))\nplt.subplot(3,3,1)\nsns.boxplot(x = 'Air temperature [K]', data=dataset)\nplt.subplot(3,3,2)\nsns.boxplot(x = 'Process temperature [K]', data=dataset)\nplt.subplot(3,3,3)\nsns.boxplot(x = 'Rotational speed [rpm]', data=dataset)\nplt.subplot(3,3,4)\nsns.boxplot(x = 'Torque [Nm]', data=dataset)\nplt.subplot(3,3,5)\nsns.boxplot(x = 'Tool wear [min]', data=dataset)\nplt.show","5f23f5df":"# Histogram - Air Temperature\nplt.hist(dataset['Air temperature [K]'])","e4022b2d":"# Histogram - Process Temperature\nplt.hist(dataset['Process temperature [K]'])","62df9724":"# Histogram - Rotational speed\nplt.hist(dataset['Rotational speed [rpm]'])","2deecff3":"# Histogram - Air Temperature\nplt.hist(dataset['Torque [Nm]'])","b67f4acc":"# Histogram - Air Temperature\nplt.hist(dataset['Tool wear [min]'])","b805b06f":"# Copy the dataset to new variable\ndata_copy1 = dataset.copy()","ebcdc17e":"data_copy1.info()","2c4cf81d":"# Treating outliers for numeric data (Torque and Rotational Speed)\n# Tranform the data for numerical columns using capping method\n# Treating outliers\n# Capping technique for treating the outliers\nQ1 = data_copy1.quantile(0.25)\nQ3 = data_copy1.quantile(0.75)\nIQR = Q3 - Q1\nprint(IQR)\n#'True' represent the presence of the outlier.\nprint(data_copy1 < (Q1 - 1.5 * IQR)) |(data_copy1 > (Q3 + 1.5 * IQR))","704ed313":"print(data_copy1['Air temperature [K]'].skew())\ndata_copy1['Air temperature [K]'].describe()\nprint(data_copy1['Process temperature [K]'].skew())\ndata_copy1['Process temperature [K]'].describe()\nprint(data_copy1['Rotational speed [rpm]'].skew())\ndata_copy1['Rotational speed [rpm]'].describe()\nprint(data_copy1['Torque [Nm]'].skew())\ndata_copy1['Torque [Nm]'].describe()\nprint(data_copy1['Tool wear [min]'].skew())\ndata_copy1['Tool wear [min]'].describe()","88b8e2f4":"# Rotational speed [rpm]\nprint(data_copy1['Rotational speed [rpm]'].quantile(0.10))\nprint(data_copy1['Rotational speed [rpm]'].quantile(0.90))","a6a9f641":"# Quantile-based Flooring and Capping\n# Treating outliers for Rotational speed [rpm]\ndata_copy1[\"Rotational speed [rpm]\"] = np.where(data_copy1[\"Rotational speed [rpm]\"] <1364.0, 1364.0,data_copy1['Rotational speed [rpm]'])\ndata_copy1[\"Rotational speed [rpm]\"] = np.where(data_copy1[\"Rotational speed [rpm]\"] >1746.0, 1746.0,data_copy1['Rotational speed [rpm]'])\nprint(data_copy1['Rotational speed [rpm]'].skew())\ndata_copy1.boxplot(['Rotational speed [rpm]'])","42cf7e51":"# Torque [Nm]\nprint(data_copy1['Torque [Nm]'].quantile(0.10))\nprint(data_copy1['Torque [Nm]'].quantile(0.90))","c5296a10":"# Quantile-based Flooring and Capping\n# Treating outliers for Torque [Nm]\ndata_copy1[\"Torque [Nm]\"] = np.where(data_copy1[\"Torque [Nm]\"] <27.2, 27.2,data_copy1['Torque [Nm]'])\ndata_copy1[\"Torque [Nm]\"] = np.where(data_copy1[\"Torque [Nm]\"] >52.6, 52.6,data_copy1['Torque [Nm]'])\nprint(data_copy1['Torque [Nm]'].skew())\ndata_copy1.boxplot(['Torque [Nm]'])","e9cfb6ee":"plt.figure(figsize=(12, 12))\nplt.subplot(3,3,1)\nsns.boxplot(x = 'Air temperature [K]', data=data_copy1)\nplt.subplot(3,3,2)\nsns.boxplot(x = 'Process temperature [K]', data=data_copy1)\nplt.subplot(3,3,3)\nsns.boxplot(x = 'Rotational speed [rpm]', data=data_copy1)\nplt.subplot(3,3,4)\nsns.boxplot(x = 'Torque [Nm]', data=data_copy1)\nplt.subplot(3,3,5)\nsns.boxplot(x = 'Tool wear [min]', data=data_copy1)\nplt.show","bfee832f":"data_copy1.info()","e9a7b159":"# Correlation plot\nplt.figure(figsize=(25,20))\nsns.heatmap(data_copy1.corr(),annot=True,annot_kws={\"size\": 15})\nsns.set(font_scale=1.8)\nplt.show()","0419bb64":"# Label encoding - Type and Failure Type\n# create the Labelencoder object\nle = preprocessing.LabelEncoder()\n#convert the categorical columns into numeric\ndata_copy1['Type']  = le.fit_transform(data_copy1['Type'])\ndata_copy1['Failure Type']  = le.fit_transform(data_copy1['Failure Type'])\n\n#display the initial records\ndata_copy1.head()","acbc8bf6":"data_copy1.tail()","35d9176b":"print(data_copy1['Type'].nunique())\nprint(data_copy1['Type'].value_counts())","788648a5":"print(data_copy1['Failure Type'].nunique())\nprint(data_copy1['Failure Type'].value_counts())","3c65e88a":"data_copy1['Target'].value_counts()","5a99632d":"x_train, x_test  = train_test_split(data_copy1, test_size = 0.3, random_state = 42) # 70\/30 split","ccc8951a":"from sklearn.utils import resample","3643199c":"# Separate majority and minority classes\ndf_majority = x_train[x_train.Target==0]\ndf_minority = x_train[x_train.Target==1]\n# Upsample minority class\ndf_minority_upsampled = resample(df_minority, \n                                 replace=True,     # sample with replacement\n                                 n_samples=9661,    # to match majority class\n                                 random_state=42) # reproducible results\n# Combine majority class with upsampled minority class\ndf_upsampled = pd.concat([df_majority, df_minority_upsampled])","25a39a42":"# Display new class counts\ndf_upsampled.Target.value_counts()","a181ec57":"yf_train = df_upsampled['Target']\nxf_train =df_upsampled.drop('Target',axis=1)","5bfda86f":"yf_test = x_test['Target']\nxf_test =x_test.drop('Target',axis=1)","852468de":"num_cols = ['Air temperature [K]','Process temperature [K]','Rotational speed [rpm]','Torque [Nm]','Tool wear [min]']","f57e6826":"from sklearn.preprocessing import MinMaxScaler\nscale1 = MinMaxScaler().fit(xf_train[num_cols])","0c71fbf1":"# transform the training data column\nxf_train[num_cols] = scale1.transform(xf_train[num_cols])\nxf_test[num_cols] =  scale1.transform(xf_test[num_cols])","b98f74c5":"print(xf_train.shape)\nprint(xf_test.shape)\nprint(yf_train.shape)\nprint(yf_test.shape)","b30e15b0":"xf_train.head(10)","909eb7a0":"xf_test.head(10)","1ac3c053":"yf_train.head()","372d7285":"yf_test.head()","b18d7754":"random_state=123\nclassifiers = { \"Decision Tree\" : DecisionTreeClassifier(criterion = 'entropy',random_state=random_state,class_weight='balanced'), \n                \"AdaBoost\" : AdaBoostClassifier(random_state=random_state),\n                \"Extra Trees\" : ExtraTreesClassifier(random_state=random_state),\n                \"LDA\" : LinearDiscriminantAnalysis(),           \n                \"Random Forest\" : RandomForestClassifier(class_weight='balanced', random_state=random_state),\n               \"LogisticRegression\":LogisticRegression(solver='lbfgs',random_state=random_state),\n               \"k-Nearest Neighbors\" : KNeighborsClassifier(n_neighbors=5),\n               \"Gaussian Naive Bayes\" : GaussianNB()\n              }\ndata_copy=[]\nAcc_Train = {}\nAcc_Test = {}\nTrain_Precision = {}\nTest_Precision = {}\nTrain_Recall = {}\nTest_Recall = {}\nTrain_F1_score = {}\nTest_F1_score = {}\nsns.set_style('whitegrid')\nplt.figure(figsize=(10,5))\nplt.title('Reciver Operating Characterstic Curve')\n    \nfor clf in classifiers:\n    print(clf)\n    classifiers[clf].fit(xf_train, yf_train) # Fitting train data for predection of Target\n    train_pred=classifiers[clf].predict(xf_train)   \n    yl_pred=classifiers[clf].predict(xf_test)\n    Acc_Train[clf]=accuracy_score(train_pred, yf_train)\n    Acc_Test[clf]=accuracy_score(yl_pred, yf_test) #Accuracy for predection of Target   \n    #recall_l=recall_score(yl_pred, y_ld1_test)# Recall score for predection of Target  \n    #Confusion_Matrix[clf]=confusion_matrix(y_test, yl_pred)#predection of Target\n    #f1_l=f1_score(y_ld1_test, yl_pred)#predection of Target\n    false_positive_rate, true_positive_rate ,threshold=roc_curve(yf_test,yl_pred)   \n    #data_copy.append(accuracy_l)\n    plt.plot(false_positive_rate,true_positive_rate,label=clf)\n    Train_Precision[clf] = precision_score(yf_train,train_pred)\n    Test_Precision[clf] = precision_score(yf_test,yl_pred)\n    Train_Recall[clf] = recall_score(yf_train,train_pred)  \n    Test_Recall[clf] = recall_score(yf_test,yl_pred) \n    Train_F1_score[clf] = f1_score(yf_train,train_pred)\n    Test_F1_score[clf] = f1_score(yf_test,yl_pred)\n    \n    \nAccuracy_train = pd.DataFrame([Acc_Train[vals]*100 for vals in Acc_Train],columns=['Accuracy Train'],index=[vals for vals in Acc_Train])\nAccuracy_pred = pd.DataFrame([Acc_Test[vals]*100 for vals in Acc_Test],columns=['Accuracy Test'],index=[vals for vals in Acc_Test])\nTrain_Prec = pd.DataFrame([Train_Precision[vals]*100 for vals in Train_Precision],columns=['Train_Precision'],index=[vals for vals in Train_Precision])\nTest_Prec = pd.DataFrame([Test_Precision[vals]*100 for vals in Test_Precision],columns=['Test_Precision'],index=[vals for vals in Test_Precision])\nTrain_Rec = pd.DataFrame([Train_Recall[vals]*100 for vals in Train_Recall],columns=['Train_Recall'],index=[vals for vals in Train_Recall])\nTest_Rec = pd.DataFrame([Test_Recall[vals]*100 for vals in Test_Recall],columns=['Test_Recall'],index=[vals for vals in Test_Recall])\nTrain_F1 =  pd.DataFrame([Train_F1_score[vals]*100 for vals in Train_F1_score],columns=['Train_F1_score'],index=[vals for vals in Train_F1_score])\nTest_F1 =  pd.DataFrame([Test_F1_score[vals]*100 for vals in Test_F1_score],columns=['Test_F1_score'],index=[vals for vals in Test_F1_score])\n\ntable = pd.concat([Accuracy_train,Accuracy_pred,Train_Prec,Test_Prec,Train_Rec,Test_Rec,Train_F1,Test_F1], axis=1)\nplt.plot([0,1],ls='--')\nplt.plot([0,0],[1,0],c='.5')\nplt.plot([1,1],c='.5')\nplt.ylabel('True positive rate')\nplt.xlabel('False positive rate')\nplt.legend()\nplt.show()\n\ntable","8e4e5e9c":"import re","a8eb57aa":"regex = re.compile(r\"\\[|\\]|<\", re.IGNORECASE)","5a2deaab":"xf_train1=xf_train.copy()\nxf_test1=xf_test.copy()\nyf_train1=yf_train.copy()\nyf_test1=yf_test.copy()","e766ac1f":"xf_train1.columns = [regex.sub(\"_\", col) if any(x in str(col) for x in set(('[', ']', '<'))) else col for col in xf_train1.columns.values]\nxf_test1.columns = [regex.sub(\"_\", col) if any(x in str(col) for x in set(('[', ']', '<'))) else col for col in xf_test1.columns.values]","8f7f26d8":"model_f1 = xgb.XGBClassifier(objective= 'reg:linear')\nmodel_f1=model_f1.fit(xf_train1,yf_train1) # Fitting train data for Target\n\ntrain_pred=model_f1.predict(xf_train1)   \nyl_pred=model_f1.predict(xf_test1)\nAcc_Train=accuracy_score(train_pred, yf_train1)\nAcc_Test=accuracy_score(yl_pred, yf_test1)\nTrain_Precision = precision_score(yf_train1,train_pred)\nTest_Precision = precision_score(yf_test1,yl_pred)\nTrain_Recall = recall_score(yf_train1,train_pred)  \nTest_Recall = recall_score(yf_test1,yl_pred) \nTrain_F1_score = f1_score(yf_train1,train_pred)\nTest_F1_score = f1_score(yf_test1,yl_pred)\nconf_f1=confusion_matrix(yf_test1, yl_pred)\n\nprint('Model paramters used are :',model_f1)\nprint('eXtreme Gradient Boosting classification - Accuracy Train of Target status is        :', (Acc_Train)*100,'%')\nprint('eXtreme Gradient Boosting classification - Accuracy Test of Target status is        :', (Acc_Test)*100,'%')\nprint('eXtreme Gradient Boosting classification - Recall Train score of Target status is        :', (Train_Recall)*100,'%')\nprint('eXtreme Gradient Boosting classification - Recall Test score of Target status is        :', (Test_Recall)*100,'%')\nprint('eXtreme Gradient Boosting classification - Conf Matrix of fraud status is        :\\n',  (conf_f1))\nprint('eXtreme Gradient Boosting classification - Train F1 score of fraud status is        :', (Train_F1_score)*100,'%')\nprint('eXtreme Gradient Boosting classification - Test F1 score of fraud status is        :', (Test_F1_score)*100,'%')","60e8ea50":"# Class Imbalance check","a1049361":"# Classification model with upsampled dataframe for Target","4bf1d775":"## exTreme Gradient Boosting","78f61eb9":"# Exploratory data analysis","9fe33590":"# Data import and Data Cleaning","73df822a":"## Up-sample Minority Class","ebfcf59c":"We can see that for Suspected Fraud, the class 0 has 9661 and class 1 has 339. Class 0 has 96.61% [(9661\/10000)x 100] and Class 1 has 3.39% [(339\/10000)x 100], We see that there is a class imbalance, where when models are measured with class imbalanced data set the predicted values would tend towards the majority class\n\n0 = Majority Class\n1 = Minority Class"}}