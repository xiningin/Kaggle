{"cell_type":{"96c7040c":"code","4842ad41":"code","62570b66":"code","fb986a31":"code","226f9142":"code","94ac4cdb":"code","fd3b7089":"code","5d54b391":"code","3dae245a":"code","e554239d":"code","9a74c876":"code","6eaf036f":"code","0a002c73":"code","fe039fa3":"code","6fa86602":"code","8857f51e":"code","4435d3f0":"code","f19c89c4":"code","8ecd0147":"code","e2f0b90c":"code","b35cf24a":"markdown","1a95f5b3":"markdown","08a51c2c":"markdown","16ad8cf2":"markdown","7c679acf":"markdown"},"source":{"96c7040c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom scipy import stats\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","4842ad41":"df = pd.read_csv('..\/input\/family-income-and-expenditure\/Family Income and Expenditure.csv')\ntarget = 'Total Household Income'\ndf.sample(5)\n","62570b66":"df.dtypes","fb986a31":"df.describe(include='all')","226f9142":"df = df.select_dtypes(include=['int64'])\ndf.sample(7)\n# print(df.dtypes)","94ac4cdb":"#Plot set up for uniformity\nsns.set_style('whitegrid')\nsns.set_context(\"paper\")\nfigsize = (30,14)\nsns.set()","fd3b7089":"print(df[target].describe())","5d54b391":"plt.hist(df[target], bins = 200)","3dae245a":"# Compute the correlation matrix\ncorr = df.corr()\n\n# Generate a mask for the upper triangle\nmask = np.triu(np.ones_like(corr, dtype=bool))\n\n# Set up the matplotlib figure\nf, ax = plt.subplots(figsize=(11, 9))\n\n# Generate a custom diverging colormap\ncmap = sns.diverging_palette(230, 20, as_cmap=True)\n\n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})","e554239d":"#Correlation with output variable\ncor_target = abs(corr[target])\n#Selecting highly correlated features\nrelevant_features = cor_target[cor_target>0.5]\nrelevant_features","9a74c876":"df = df[relevant_features.keys()]\ndf.sample(5)","6eaf036f":"x = df.drop(columns = [target])\ny = df[target]\n\nx.sample(5)","0a002c73":"for col in x.columns:\n    x[col] = (x[col]-x[col].mean())\/x[col].std()\nx.sample(5)","fe039fa3":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(x,y,test_size=0.1, random_state=214)","6fa86602":"from sklearn.metrics import mean_squared_error\nfrom sklearn.tree import DecisionTreeRegressor\ntree_reg = DecisionTreeRegressor(random_state = 214)\n\nfrom sklearn.ensemble import RandomForestRegressor\nforest_reg = RandomForestRegressor(random_state = 214)\n\ntree_reg.fit(X_train, y_train)\ny_pred = tree_reg.predict(X_test)\ntree_mse = mean_squared_error(y_test, y_pred)\ntree_rmse = np.sqrt(tree_mse)\ntree_rmse","8857f51e":"forest_reg.fit(X_train, y_train)\ny_pred = forest_reg.predict(X_test)\nforest_mse = mean_squared_error(y_test, y_pred)\nforest_rmse = np.sqrt(forest_mse)\nforest_rmse","4435d3f0":"from sklearn.model_selection import GridSearchCV\n\nparam_grid = [{'n_estimators': [3,10,30], 'max_features':[6,8,10,12]}]\ngrid_search = GridSearchCV(forest_reg, param_grid, cv = 5,\n                          scoring='neg_mean_squared_error',\n                          return_train_score = True)\ngrid_search.fit(X_train, y_train)","f19c89c4":"grid_search.best_estimator_\n","8ecd0147":"y_pred = grid_search.predict(X_test)\nforest_mse = mean_squared_error(y_test, y_pred)\nforest_rmse = np.sqrt(forest_mse)\nforest_rmse","e2f0b90c":"forest_reg.fit(X_train, y_train, n_estimators = 30, max_features=8)","b35cf24a":"**Exploratory Data Analysis**","1a95f5b3":"Plotting the histogram of Total Household Income, it can be seen that it is heavily concentrated on the lower end with few outliers with high incomes.","08a51c2c":"In this demo project, the performance of a single decision tree will be compared to a random forest in a regression problem. The dataset is the Family Income and Expenditure of Filipino Households. The models will predict the Total Household Income","16ad8cf2":"The dataset hase 41,544 samples and 59 variables. Most are integer values but some are strings (categorical). For the purposes of this project, only the numerical (int64) columns will be considered","7c679acf":"**Importing and Processing the data:**\nFirst the dataset will be imported from a CSV. Exploratory data analysis will be conducted to check the data types of the columns"}}