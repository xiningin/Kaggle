{"cell_type":{"a7339d2f":"code","8d814b3d":"code","797c2bb0":"code","cf9f8bfa":"code","c6a3eeca":"code","372b836a":"code","8c75247d":"code","6c4e8a79":"code","deacdd47":"code","2a22d3d4":"code","3fa5ab59":"code","2217e240":"code","2df28cf5":"code","687b033b":"code","7e12bcc8":"code","3ee7b295":"code","093a4997":"code","b257f097":"code","4bf4b536":"code","f994b56e":"code","e2bd45cd":"code","b36d8810":"code","fb20c9a9":"code","66693c8f":"code","2896b156":"code","b442937f":"code","cc33cf33":"code","d2603d4f":"code","e8240d3f":"code","c6d47929":"code","82e0681e":"code","0b74bb07":"code","ca780c1d":"code","ad241d57":"code","4527e62c":"code","8130cd7d":"code","abc625b1":"code","6e84eed7":"code","2bbe77e9":"code","718d75ff":"code","cd7af941":"code","7c084e58":"code","af3345af":"markdown","d2018636":"markdown","d1b06324":"markdown","7b402f39":"markdown"},"source":{"a7339d2f":"import torch\nimport torch.nn as nn\nfrom PIL import Image\nfrom pathlib import Path\nimport numpy as np\nfrom matplotlib import pyplot as plt\nimport os\nimport math\nimport pandas as pd","8d814b3d":"PATH = Path('..\/input\/kodluyoruz-mist\/data\/mnist')","797c2bb0":"img = np.array(Image.open(\"..\/input\/kodluyoruz-mist\/data\/mnist\/train\/0\/img_11161.jpg\"))","cf9f8bfa":"img.shape","c6a3eeca":"plt.imshow(img, cmap = \"gray\");","372b836a":"kernel = np.array([-1,1])","8c75247d":"out = np.zeros((28,27))","6c4e8a79":"img.shape","deacdd47":"def conv(img, kernel):\n    \n    out = np.zeros(img.shape)\n    img = np.pad(img,[(0, 0), (0, 1)],\"edge\") # This will do the padding for not to reduce size\n    \n    for i in range(img.shape[0]):\n    \n        for j in range(img.shape[1]-1):\n            out[i][j] = abs((img[i][j:j+2] * kernel).sum())\n    \n    return out","2a22d3d4":"plt.imshow(img, cmap = \"gray\");","3fa5ab59":"out = conv(img, kernel)","2217e240":"plt.imshow(out, cmap = \"gray\");","2df28cf5":"def _get_files(p, fs, extensions = None):\n    p = Path(p) # to support \/ notation\n    res = [p\/f for f in fs if not f.startswith(\".\") \n           and ((not extensions) or f'.{f.split(\".\")[-1].lower()}' in extensions)]\n    return res","687b033b":"def create_ds_from_file(src):\n    imgs, labels = [], []\n    \n    for label in range(10):\n        path = src\/str(label)\n        print(path)\n        t = [o.name for o in os.scandir(path)]\n        t = _get_files(path, t, extensions = [\".jpg\", \".png\"])\n        for e in t:\n            l = [np.array(Image.open(e)).reshape(28*28)]\n            imgs += l\n        labels += ([label] * len(t))\n    return torch.tensor(imgs,  dtype=torch.float32), torch.tensor(labels, dtype=torch.long).view(-1,1)","7e12bcc8":"trn_x, trn_y = create_ds_from_file(PATH\/\"train\")","3ee7b295":"val_x,val_y = create_ds_from_file(PATH\/\"validation\")","093a4997":"def normalization(array):\n    return (array - array.min()) \/ (array.max() - array.min())","b257f097":"img_norm = normalization(img)","4bf4b536":"plt.imshow(img_norm, cmap = \"gray\");","f994b56e":"trn_x.shape","e2bd45cd":"def multiple_normalization(train_or_valid_X):\n    for i in range(len(train_or_valid_X)):\n        train_or_valid_X[i] =  (train_or_valid_X[i] - torch.min(train_or_valid_X[i])) \/ (torch.max(train_or_valid_X[i]) - torch.min(train_or_valid_X[i]))\n    return train_or_valid_X","b36d8810":"train_norm = multiple_normalization(trn_x)","fb20c9a9":"train_norm[0].shape","66693c8f":"plt.imshow(train_norm[0].reshape(28,28), cmap = \"gray\");","2896b156":"val_x.shape","b442937f":"valid_norm = multiple_normalization(val_x)","cc33cf33":"valid_norm[0].shape","d2603d4f":"valid_norm[0].reshape(28,28)","e8240d3f":"plt.imshow(valid_norm[0].reshape(28,28), cmap = \"gray\");","c6d47929":"def create_ds_from_file(src):\n    imgs, labels = [], []\n    \n    for label in range(10):\n        path = src\/str(label)\n        print(path)\n        t = [o.name for o in os.scandir(path)]\n        t = _get_files(path, t, extensions = [\".jpg\", \".png\"])\n        for e in t:\n            img = np.array(Image.open(e))\n            l = [np.concatenate((conv(img, kernel).reshape(-1), img.reshape(-1)))]\n            imgs += l\n        labels += ([label] * len(t))\n    return torch.tensor(imgs,  dtype=torch.float32), torch.tensor(labels, dtype=torch.long).view(-1,1)","82e0681e":"trn_x, trn_y = create_ds_from_file(PATH\/\"train\")","0b74bb07":"val_x,val_y = create_ds_from_file(PATH\/\"validation\")","ca780c1d":"plt.imshow(trn_x[0].view(56,28), cmap = \"gray\")","ad241d57":"trn_x[0].shape","4527e62c":"plt.imshow(trn_x[0].reshape(56,28)[:28,:28], cmap = \"gray\")","8130cd7d":"plt.imshow(trn_x[0].reshape(56,28)[28:,:28], cmap = \"gray\")","abc625b1":"trn_x[0].shape[0]","6e84eed7":"def stack_normalization(array):\n    # Filtreli k\u0131s\u0131m icin\n    array_f = array[:784]\n    for i in range(784):\n        filtre_array = (array_f - array_f.min()) \/ (array_f.max() - array_f.min())\n        \n    # Filtresiz k\u0131s\u0131m icin\n    array_fs = array[784:]\n    for j in range(784,1568):\n        filtresiz_array = (array_fs - array_fs.min()) \/ (array_fs.max() - array_fs.min())\n    \n    arr = np.hstack((filtre_array,filtresiz_array))\n        \n    return np.array(arr)","2bbe77e9":"stack_normalization(trn_x[0])","718d75ff":"stack_normalization(trn_x[0]).shape","cd7af941":"plt.imshow(trn_x[0].view(56,28), cmap = \"gray\")","7c084e58":"plt.imshow(stack_normalization(trn_x[0]).reshape(56,28), cmap = \"gray\")","af3345af":"# Valid Normalization","d2018636":"# Train Normalization","d1b06324":"- ilk 784 = filtre uygulanm\u0131s olan\n- son 784 = filtre uygulanmam\u0131s olan","7b402f39":"# Normalization Code"}}