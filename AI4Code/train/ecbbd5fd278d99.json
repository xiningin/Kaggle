{"cell_type":{"39949a74":"code","1776cdb7":"code","dc890b27":"code","fbdb7409":"code","0c5c3f6b":"code","982bd122":"code","ffeae4ee":"code","df23c2db":"code","04ffc413":"code","0f8521b2":"code","f960be79":"code","db8c555b":"code","db1bfde7":"code","dd2140bb":"code","98926fe2":"code","382b7587":"markdown","59116f7a":"markdown","245c2fd6":"markdown","3db660ee":"markdown","6d2a820c":"markdown","43f1bc5e":"markdown","26c35ca7":"markdown","1f623e49":"markdown"},"source":{"39949a74":"!curl https:\/\/raw.githubusercontent.com\/pytorch\/xla\/master\/contrib\/scripts\/env-setup.py -o pytorch-xla-env-setup.py\n!python pytorch-xla-env-setup.py --version 20210331 --apt-packages libomp5 libopenblas-dev\n!rm -rf \/kaggle\/working\/*.whl\n!rm -rf \/kaggle\/working\/*.py","1776cdb7":"import os\nos.environ['XLA_USE_BF16'] = \"1\"\nos.environ['XLA_TENSOR_ALLOCATOR_MAXSIZE'] = '100000000'\n\nimport gc\nimport sys\nimport cv2\nimport math\nimport time\nimport tqdm\nimport random\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\nfrom joblib import Parallel, delayed\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import KFold,StratifiedKFold, GroupKFold\n\nimport torch\nimport torchvision\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.nn import Parameter\nimport torch.nn.functional as F\nfrom torch.optim import Adam, lr_scheduler\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.optim.optimizer import Optimizer\nfrom torch.optim.lr_scheduler import _LRScheduler\nfrom torch.optim.lr_scheduler import (CosineAnnealingWarmRestarts, CosineAnnealingLR, \n                                      ReduceLROnPlateau)\n\nimport torch_xla.core.xla_model as xm\nimport torch_xla.distributed.parallel_loader as pl\nimport torch_xla.distributed.xla_multiprocessing as xmp\n\nfrom transformers import (AutoModel, AutoTokenizer, \n                          AutoModelForSequenceClassification,\n                          get_constant_schedule_with_warmup,get_cosine_schedule_with_warmup)\n\nimport plotly.express as px\nimport plotly.graph_objs as go\nimport plotly.figure_factory as ff\n\n\nfrom colorama import Fore, Back, Style\ny_ = Fore.YELLOW\nr_ = Fore.RED\ng_ = Fore.GREEN\nb_ = Fore.BLUE\nm_ = Fore.MAGENTA\nc_ = Fore.CYAN\nsr_ = Style.RESET_ALL","dc890b27":"train_data = pd.read_csv('..\/input\/commonlitreadabilityprize\/train.csv')\ntest_data = pd.read_csv('..\/input\/commonlitreadabilityprize\/test.csv')\nsample = pd.read_csv('..\/input\/commonlitreadabilityprize\/sample_submission.csv')\n\n# train_data = train_data.loc[:,['excerpt','target']]\n    \n# df1 = train_data.sample(frac=1).reset_index(drop=True).rename(columns={'excerpt':'excerpt1','target':'target1'})\n# df2 = train_data.sample(frac=1).reset_index(drop=True).rename(columns={'excerpt':'excerpt2','target':'target2'})\n\n# df1['excerpt'] = df1['excerpt'].apply(lambda x:x[:int(len(x)\/2)])\n# df2['excerpt'] = df2['excerpt'].apply(lambda x:x[:int(len(x)\/2)])\n\n# df = pd.concat([df1,df2],axis=1)\n\n# df['excerpt'] = df['excerpt1'] + df['excerpt2']\n# df['target'] = (df['target1'] + df['target2'])\/2\n\n# df = df.loc[:,train_data.columns.tolist()]\n\n# train_data = pd.concat([train_data,df],axis=0).reset_index(drop=True)\n\n# for kfold  \nnum_bins = int(np.floor(1 + np.log2(len(train_data))))\ntrain_data.loc[:,'bins'] = pd.cut(train_data['target'],bins=num_bins,labels=False)\nbins = train_data.bins.to_numpy()\n\ntrain_data['is_positive'] = (train_data['target'] >=0)\n\ntrain_data['text_len']= train_data['excerpt'].apply(lambda x: len(x.split()))\n\ndef rmse_score(y_true,y_pred):\n    return np.sqrt(mean_squared_error(y_true,y_pred))","fbdb7409":"print(\"{0}Number of rows in train data: {1}{2}\\n{0}Number of columns in train data: {1}{3}\".format(y_,r_,train_data.shape[0],train_data.shape[1]))\nprint(\"{0}Number of rows in test data: {1}{2}\\n{0}Number of columns in test data: {1}{3}\".format(m_,r_,test_data.shape[0],test_data.shape[1]))\nprint(\"{0}Number of rows in sample : {1}{2}\\n{0}Number of columns in sample : {1}{3}\".format(c_,r_,sample.shape[0],sample.shape[1]))","0c5c3f6b":"train_data.head()","982bd122":"plt.style.use('fivethirtyeight')\ndef distribution1(feature,color1,color2,df=train_data):\n    plt.figure(figsize=(15,7))\n    \n    plt.subplot(121)\n    dist = sns.distplot(df[feature],color=color1)\n    a = dist.patches\n    xy = [(a[i].get_x() + a[i].get_width() \/ 2,a[i].get_height()) \\\n          for i in range(1,len(a)-1) if (a[i].get_height() > a[i-1].get_height() and a[i].get_height() > a[i+1].get_height())]\n    \n    for i,j in xy:\n        dist.annotate(\n            s=f\"{i:.3f}\",\n            xy=(i,j), \n            xycoords='data',\n            ha='center', \n            va='center', \n            fontsize=11, \n            color='black',\n            xytext=(0,7), \n            textcoords='offset points',\n        )\n    \n    qnt = df[feature].quantile([.25, .5, .75]).reset_index(level=0).to_numpy()\n    plt.subplot(122)\n    box = sns.boxplot(df[feature],color=color2)\n    for i,j in qnt:\n        box.annotate(str(j)[:4],xy= (j-.05,-0.01),horizontalalignment='center')\n        \n    print(\"{}Max value of {} is: {} {:.2f} \\n{}Min value of {} is: {} {:.2f}\\n{}Mean of {} is: {}{:.2f}\\n{}Standard Deviation of {} is:{}{:.2f}\"\\\n      .format(y_,feature,r_,df[feature].max(),g_,feature,r_,df[feature].min(),b_,feature,r_,df[feature].mean(),m_,feature,r_,df[feature].std()))","ffeae4ee":"distribution1('target','yellow','red')","df23c2db":"distribution1('text_len','red','blue')","04ffc413":"distribution1('standard_error','blue','green')","0f8521b2":"def scatterplot1(feature1,feature2,category,df=train_data):\n    fig = px.scatter(train_data, x=feature1, y=feature2, color=category, marginal_y=\"violin\",\n               marginal_x=\"box\", trendline=\"ols\", template=\"simple_white\")\n    fig.show()","f960be79":"scatterplot1('standard_error','target','is_positive')","db8c555b":"config = {\n    'lr': 5e-5,\n    'wd':1e-1,\n    'batch_size':64,\n    'max_len':256,\n    'epochs':15,\n    'nfolds':5,\n    'seed':42,\n}\n\ndef seed_everything(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONASSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True\n\nseed_everything(seed=config['seed'])","db1bfde7":"class CLRPDataset(nn.Module):\n    def __init__(self,df,tokenizer,max_len=128):\n        self.excerpt = df['excerpt'].to_numpy()\n        self.targets = df['target'].to_numpy()\n        self.max_len = max_len\n        self.tokenizer = tokenizer\n    \n    def __getitem__(self,idx):\n        encode = self.tokenizer(self.excerpt[idx],\n                                return_tensors='pt',\n                                max_length=self.max_len,\n                                padding='max_length',\n                                truncation=True)\n        \n        target = torch.tensor(self.targets[idx],dtype=torch.float)\n        \n        return encode, target\n    \n    def __len__(self):\n        return len(self.excerpt)","dd2140bb":"def run(plot_losses=True, verbose=True):\n        \n    def loss_fn(outputs,targets):\n        outputs = outputs.logits.squeeze(-1)\n        return torch.sqrt(nn.MSELoss()(outputs,targets))\n    \n    def train_loop(train_loader, model, loss_fn, device,optimizer,lr_scheduler=None):\n        model.train()\n        total_loss = 0\n        for i, (inputs,targets) in enumerate(train_loader):\n            optimizer.zero_grad()\n            inputs = {key:val.reshape(val.shape[0],-1).to(device) for key,val in inputs.items()}\n            targets = targets.to(device)\n            outputs = model(**inputs)\n            loss = loss_fn(outputs,targets)\n            loss.backward()\n            xm.optimizer_step(optimizer, barrier=True)\n            if lr_scheduler:\n                lr_scheduler.step()\n                \n            total_loss += loss.item()\n        total_loss \/= len(train_loader)\n        return total_loss\n        \n    def valid_loop(valid_loader, model, loss_fn, device):\n        model.eval()\n        total_loss = 0\n        valid_predictions = list()\n        with torch.no_grad():\n            for i, (inputs,targets) in enumerate(valid_loader):\n                inputs = {key:val.reshape(val.shape[0],-1).to(device) for key,val in inputs.items()}\n                targets = targets.to(device)\n                \n                outputs = model(**inputs)\n                loss = loss_fn(outputs,targets)\n                total_loss += loss.item()\n                outputs = outputs.logits.squeeze(-1).cpu().detach().numpy().tolist()\n                valid_predictions.extend(outputs)\n            total_loss \/= len(valid_loader)\n        return total_loss ,valid_predictions\n    \n    fold_train_losses = list()\n    fold_valid_losses = list()\n    fold_valid_predictions = list()\n    fold_valid_targets = list()\n\n    device = xm.xla_device(config['nfolds'] + 1)\n    print(f\"{device} is used\")\n    \n    train = train_data\n    kfold = StratifiedKFold(n_splits=config['nfolds'],shuffle=True,random_state=config['seed'])\n    for k , (train_idx,valid_idx) in enumerate(kfold.split(X=train,y=bins)):\n        x_train,x_valid = train.loc[train_idx],train.loc[valid_idx]\n        \n        MODEL_PATH = 'roberta-large'\n        model = AutoModelForSequenceClassification.from_pretrained(MODEL_PATH,num_labels=1)\n        model.to(device)\n        \n        tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n\n        train_ds = CLRPDataset(x_train,tokenizer,config['max_len'])\n        train_dl = DataLoader(train_ds,\n                              batch_size = config[\"batch_size\"],\n                              shuffle=True,\n                              num_workers = 4,\n                              pin_memory=True,\n                              drop_last=False\n                             )\n\n        valid_ds = CLRPDataset(x_valid,tokenizer,config['max_len'])\n        valid_dl = DataLoader(valid_ds,\n                              batch_size = config[\"batch_size\"],\n                              shuffle=False,\n                              num_workers = 4,\n                              pin_memory=True,\n                              drop_last=False,\n                             )\n        \n        optimizer = optim.AdamW(model.parameters(),lr=config['lr'],weight_decay=config['wd'])\n#         lr_scheduler = optim.lr_scheduler.OneCycleLR(optimizer,max_lr=1e-4,\n#                                                     steps_per_epoch=len(train_dl), epochs=config['epochs'])\n        \n        \n        lr_scheduler = None\n    \n        print(f\"Fold {k}\")\n        best_loss = 99999\n        \n        train_losses = list()\n        valid_losses = list()\n        best_valid_predictions = list()\n        start = time.time()\n        for i in range(config[\"epochs\"]):\n            train_loss = train_loop(train_dl,model,loss_fn,device,optimizer,lr_scheduler=lr_scheduler)\n            valid_loss,valid_predictions = valid_loop(valid_dl,model,loss_fn,device)\n\n            train_losses.append(train_loss)\n            valid_losses.append(valid_loss)\n            \n            end = time.time()\n            epoch_time = end - start\n            start = end\n            \n            valid_targets = x_valid['target'].to_list()\n                                                  \n            if verbose:\n                print(f\"epoch:{i} Training loss:{train_loss} | Validation loss:{valid_loss} |epoch time {epoch_time:.2f}s \")\n\n            if valid_loss <= best_loss:\n                if verbose:\n                    print(f\"{g_}Validation loss Decreased from {best_loss} to {valid_loss}{sr_}\")\n                    \n                best_loss = valid_loss\n                best_valid_predictions = valid_predictions\n#                 xm.save(model.state_dict(),f'.\/model{k}\/model{k}.bin')\n                model.save_pretrained(f'.\/model{k}')\n                tokenizer.save_pretrained(f'.\/model{k}')\n                \n        fold_train_losses.append(train_losses)\n        fold_valid_losses.append(valid_losses)\n        fold_valid_predictions.append(best_valid_predictions)\n        fold_valid_targets.append(x_valid['target'].tolist())\n        \n        if k == 0:\n            break\n        \n    if plot_losses == True:\n        plt.figure(figsize=(20,14))\n        for i, (t,v) in enumerate(zip(fold_train_losses,fold_valid_losses)):\n            plt.subplot(2,5,i+1)\n            plt.title(f\"Fold {i}\")\n            plt.plot(t,label=\"train_loss\")\n            plt.plot(v,label=\"valid_loss\")\n            plt.legend()\n        plt.show()\n        \n        plt.figure(figsize=(20,14))\n        for i, (p,t) in enumerate(zip(fold_valid_predictions,fold_valid_targets)):\n            plt.subplot(2,5,i+1)\n            plt.title(f\"Fold {i}\")\n            sns.distplot(p,label=\"predictions\")\n            sns.distplot(t,label=\"targets\")\n            plt.legend()\n        plt.show()","98926fe2":"run()","382b7587":"## Getting Data \ud83d\udcbe","59116f7a":"## Imports \ud83d\udcd7","245c2fd6":"### Distribution of target in train","3db660ee":"### Distribution of length of text","6d2a820c":"### Scatter plot standard error vs target","43f1bc5e":"## EDA","26c35ca7":"# ROBERTA moDEL","1f623e49":"### Distribution of standard error"}}