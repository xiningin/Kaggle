{"cell_type":{"9914e5bd":"code","943e65e8":"code","d0e87837":"code","d2b3e19c":"code","1d9c750d":"code","c877b024":"markdown","c6ac9fc4":"markdown","05ad1faf":"markdown","d40a22ac":"markdown"},"source":{"9914e5bd":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","943e65e8":"!pip install SpectralEmbeddings","d0e87837":"train_df=pd.read_csv('..\/input\/google-quest-challenge\/train.csv')\ntrain_df.head()","d2b3e19c":"!pip install pyvis","1d9c750d":"\nimport pandas as pd\nimport numpy as np\nimport SpectralEmbeddings.VanillaGCN as vgcn\nimport SpectralEmbeddings.ChebGCN as cgcn\nimport SpectralEmbeddings.SplineGCN as sgcn\nimport SpectralEmbeddings.GraphAutoencoder as graph_ae\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, iplot,plot\nimport plotly\nimport plotly.graph_objs as go\nimport networkx as nx\nfrom pyvis.network import Network\ninit_notebook_mode(connected=True)\n\n\ndef test_vanillagcn():\n    print(\"Testing for VanillaGCN embeddings having a source and target label\")\n    #train_df=pd.read_csv(\"E:\\\\train_graph\\\\train.csv\")\n    source_label='question_body'\n    target_label='category'\n    print(\"Input parameters are hidden units , number of layers,subset (values of entries to be considered for embeddings),epochs \")\n    hidden_units=32\n    num_layers=4\n    subset=200\n    epochs=10\n    v_emb,v_graph=vgcn.get_gcn_embeddings(hidden_units,train_df,source_label,target_label,epochs,num_layers,subset)\n    print(v_emb.shape)\n    return v_emb,v_graph\n    \ndef test_chebgcn():\n    print(\"Testing for ChebGCN embeddings having a source and target label\")\n    #train_df=pd.read_csv(\"E:\\\\train_graph\\\\train.csv\")\n    source_label='question_body'\n    target_label='category'\n    print(\"Input parameters are hidden units , number of layers,subset (values of entries to be considered for embeddings),epochs and k for Cheby polynomials\")\n    hidden_units=32\n    num_layers=4\n    subset=200\n    epochs=10\n    k=4\n    c_emb,c_graph=cgcn.get_chebgcn_embeddings(hidden_units,train_df,source_label,target_label,epochs,num_layers,subset,k)\n    print(c_emb.shape)\n    return c_emb,c_graph\n    \ndef test_sgcn():\n    print(\"Testing for SplineGCN embeddings having a source and target label\")\n    #train_df=pd.read_csv(\"E:\\\\train_graph\\\\train.csv\")\n    source_label='question_body'\n    target_label='category'\n    print(\"Input parameters are hidden units , number of layers,subset (values of entries to be considered for embeddings),epochs and k for Cheby polynomials\")\n    hidden_units=32\n    num_layers=4\n    subset=200\n    epochs=10\n    s_emb,s_graph=sgcn.get_splinegcn_embeddings(hidden_units,train_df,source_label,target_label,epochs,num_layers,subset)\n    print(s_emb.shape)\n    return s_emb,s_graph\n    \ndef test_graph_ae():\n    print(\"Testing for Graph Autoencoder embeddings having a source and target label\")    \n    #train_df=pd.read_csv(\"E:\\\\train_graph\\\\train.csv\")\n    source_label='question_body'\n    target_label='category'\n    print(\"Input parameters are hidden dimensions ,alpha,beta,epochs\")   \n    hidden_dims=[32,16]\n    alpha=1e-4\n    beta=1e-5\n    epochs=20\n    g_emb,graph_ae_pl=graph_ae.get_sdne_embeddings(train_df,source_label,target_label,hidden_dims,alpha,beta,epochs)\n    print(len(g_emb))\n    return g_emb,graph_ae_pl\n\ndef plotter(G,title):\n    pos = nx.spring_layout(G, k=0.5, iterations=50)\n    for n, p in pos.items():\n        G.nodes[n]['pos'] = p\n    edge_trace = go.Scatter(\n        x=[],\n        y=[],\n        line=dict(width=0.5,color='white'),\n        hoverinfo='none',\n        mode='lines')\n\n    for edge in G.edges():\n        x0, y0 = G.nodes[edge[0]]['pos']\n        x1, y1 = G.nodes[edge[1]]['pos']\n        edge_trace['x'] += tuple([x0, x1, None])\n        edge_trace['y'] += tuple([y0, y1, None])\n    node_trace = go.Scatter(\n        x=[],\n        y=[],\n        text=[],\n        mode='markers',\n        hoverinfo='text',\n        marker=dict(\n            showscale=True,\n            colorscale='ice',\n            reversescale=True,\n            color=[],\n            size=15,\n            colorbar=dict(\n                thickness=10,\n                title='Node Connections',\n                xanchor='left',\n                titleside='right'\n            ),\n            line=dict(width=0)))\n\n    for node in G.nodes():\n        x, y = G.nodes[node]['pos']\n        node_trace['x'] += tuple([x])\n        node_trace['y'] += tuple([y])\n    for node, adjacencies in enumerate(G.adjacency()):\n        node_trace['marker']['color']+=tuple([len(adjacencies[1])])\n        node_info = adjacencies[0] +' # of connections: '+str(len(adjacencies[1]))\n        node_trace['text']+=tuple([node_info])\n    fig = go.Figure(data=[edge_trace, node_trace],\n             layout=go.Layout(\n                title=title,\n                titlefont=dict(size=16),\n                showlegend=False,\n                hovermode='closest',\n                margin=dict(b=20,l=5,r=5,t=40),\n                plot_bgcolor='#222222',\n                annotations=[ dict(\n                    text=\"No. of connections\",\n                    showarrow=False,\n                    xref=\"paper\", yref=\"paper\") ],\n                xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),\n                yaxis=dict(showgrid=False, zeroline=False, showticklabels=False)))\n\n    plot(fig)    \n    \ndef plot_vgcn_embed(graph,node_num,emb,label):\n    \n    node,distances,questions=vgcn.node_level_embedding(graph,node_num,emb)\n    vg_df=pd.DataFrame()\n    vg_df['Premise']=[node]*len(distances)\n    vg_df['Hypothesis']=questions\n    vg_df['Chebyshev_Distance']=distances\n    vg_g=nx.from_pandas_edgelist(vg_df,source='Hypothesis',target='Premise',edge_attr='Chebyshev_Distance')\n    plotter(vg_g,label)\n    return vg_g\n\ndef plot_cgcn_embed(graph,node_num,emb,label):\n    \n    node,distances,questions=cgcn.node_level_embedding(graph,node_num,emb)\n    vg_df=pd.DataFrame()\n    vg_df['Premise']=[node]*len(distances)\n    vg_df['Hypothesis']=questions\n    vg_df['Chebyshev_Distance']=distances\n    vg_g=nx.from_pandas_edgelist(vg_df,source='Hypothesis',target='Premise',edge_attr='Chebyshev_Distance')\n    plotter(vg_g,label)\n    return vg_g\n\n\ndef plot_sgcn_embed(graph,node_num,emb,label):\n    \n    node,distances,questions=sgcn.node_level_embedding(graph,node_num,emb)\n    vg_df=pd.DataFrame()\n    vg_df['Premise']=[node]*len(distances)\n    vg_df['Hypothesis']=questions\n    vg_df['Chebyshev_Distance']=distances\n    vg_g=nx.from_pandas_edgelist(vg_df,source='Hypothesis',target='Premise',edge_attr='Chebyshev_Distance')\n    plotter(vg_g,label)\n    return vg_g\n\ndef plot_ae_embed(graph,node_num,emb,label):\n    \n    node,distances,questions=graph_ae.node_level_embedding(graph,node_num,emb)\n    vg_df=pd.DataFrame()\n    vg_df['Premise']=[node]*len(distances)\n    vg_df['Hypothesis']=questions\n    vg_df['Chebyshev_Distance']=distances\n    vg_g=nx.from_pandas_edgelist(vg_df,source='Hypothesis',target='Premise',edge_attr='Chebyshev_Distance')\n    plotter(vg_g,label)\n    return vg_g\n\ndef pyvis_plotter(graph,label):\n    network = Network(height='750px', width='100%', bgcolor='#222222', font_color='white')\n    network.from_nx(graph)\n    #network.enable_physics(True)\n    #network.show_buttons(filter_=['nodes'])\n    network.show('label.html')\n    \nif __name__=='__main__':\n    \"\"\"Embeddings generated from GCN variants are of the dimensions of (input subset size,set of labels)-> in this case if input\n       subset size is 20 and number of labels are 5 then the embedding dimension -> (20,5)\n       \n       Embedding generated from Graph Autoencoder are of the dimension (input subset size,dimension of autoencoder hidden units)\n    \n       \"\"\"\n    \n    print(\"======Vanilla GCN========\")    \n    \n    embed_wt,v_graph=test_vanillagcn()\n    node_num=12 #node number for plotting\n    if node_num>v_graph.number_of_nodes():\n        print('The node number should not be greater than number of nodes in graph')\n        node_num=v_graph.number_of_node()-1  \n    label=\"Vanilla GCN Chebshev similarity\"\n    v_g=plot_vgcn_embed(v_graph,node_num,embed_wt,label)\n    #pyvis_plotter(v_graph,'VanillaGCN')\n    \n    print(\"======Chebshev GCN========\")\n    \n    embed_wt_cheb,c_graph=test_chebgcn()\n    node_num=12 #node number for plotting\n    if node_num>c_graph.number_of_nodes():\n        print('The node number should not be greater than number of nodes in graph')\n        node_num=c_graph.number_of_node()-1 \n    label=\"Chebyshev GCN Chebshev similarity\"\n    plot_cgcn_embed(c_graph,node_num,embed_wt_cheb,label)\n    \n    print(\"======Spline GCN========\")\n    \n    embed_wt_spline,s_graph=test_sgcn()\n    node_num=12 #node number for plotting\n    if node_num>s_graph.number_of_nodes():\n        print('The node number should not be greater than number of nodes in graph')\n        node_num=s_graph.number_of_node()-1    \n    label=\"Spline GCN Chebshev similarity\"\n    c_g=plot_sgcn_embed(s_graph,node_num,embed_wt_spline,label)\n    #pyvis_plotter(c_g,'SplineGCN')\n    \n    \n    print(\"======Graph AutoEncoder========\")\n    \n    graph_ae_embed,ae_graph=test_graph_ae()\n    node_num=12 #node number for plotting\n    if node_num>ae_graph.number_of_nodes():\n        print('The node number should not be greater than number of nodes in graph')\n        node_num=ae_graph.number_of_node()-1    \n    label=\"Graph Autoencoder Chebschev similarity\"\n    plot_ae_embed(ae_graph,node_num,graph_ae_embed,label)\n    #pyvis_plotter(temp_g,'Graph_AE')\n    ","c877b024":"## Demonstration example and analysis\n\nFor this script, we can visualize the results from the plots provided in the explanation section. The main takeaways are that for GCN kernels, if we are restricting to a subset, the embeddings may change when more input nodes are added into that subset. Also the accuracy may be low for initial iterations and small graphs and should increase after sufficient nodes are added which represent the semantic information. The values captured here are in some cases of the order of e-06 and hence can be represented as NA (and hence the differences in the Chebyshev distances). This example contains all the GCN kernels and the AutoEncoder kernel together  run on the same dataset and are demarcated by certain print statements .Additionally kernel specifications and internal parameters are also printed to showcase the number of trainable node paramters and generated embedding space shape. (For demo purposes only 34 nodes have been taken to generate the embedding space.)","c6ac9fc4":"## Conclusion and Future work\n\nThis is a Graph Embedding library based entirely on node representations and spectral Graph theory. There are certain modifications and additions to be made from GCN perspective as well as in representing higher order network embeddings. \n\n<img src=\"http:\/\/tkipf.github.io\/graph-convolutional-networks\/images\/gcn_web.png\">","05ad1faf":"### Spectral Embeddings Demonstration \n\nThe following tanbs contain the steps as mentioned in the writeup. The first step is installation of the library from [Pypi](https:\/\/pypi.org\/project\/SpectralEmbeddings\/0.2) and then using the scripts as follows. As a disclaimer since GCN kernels are being fitted on categorical loss (crossentropy) in some cases the loss may be NA. and the embedding values may be written in the console as NA but there are very minute differences of e-06\/e-07 order. This value normalizes as the graph becomes larger and contain more node representations. In this case for our dataset, we will be creating a networkx graph from the pandas edgelist  by taking the 'question_body' and 'category' as the source and target labels. Once the graph gets created and we specify the subset values , we can visualize the embedding of each node (with the helper methods explained above) with the rest of the graph.","d40a22ac":"## SpectralEmbeddings\n\n<img src=\"https:\/\/github.com\/abhilash1910\/SpectralEmbeddings\/raw\/master\/Previews\/Graph_preview.PNG\">\n\n[Spectral Embeddings](https:\/\/github.com\/abhilash1910\/SpectralEmbeddings) is a [python package](https:\/\/pypi.org\/project\/SpectralEmbeddings\/0.2) which is used to generate embeddings from knowledge graphs with the help of deep graph convolution kernels and autoencoder networks. This library is used to generate 2 different kinds embeddings:\n\n- Graph AutoEncoder Embeddings:  This models the first and higher order similarity measures in a graph for each node in a neighborhood. The first and second order similarity measures are created through an Autoencoder circuit which preserves the proximity loss of similarity with reconstruction loss. This model has been implemented along the lines of [SDNE](https:\/\/paperswithcode.com\/method\/sdne) . These embeddings not only cover the first order dependencies but also are used to capture second order dependencies between node neighbors. The output of this AutoEncoder network has a dimension of (number of input entries,dimension of embedding space provided). The Graph Autoencoder also produces full embedding subspace over all the entries with the provided hidden dimensions and can be found in the example provided here.A preview of the generated embeddings are shown here:\n<img src=\"https:\/\/github.com\/abhilash1910\/SpectralEmbeddings\/raw\/master\/Previews\/Graph_AE_preview.PNG\">\n\n  The architecture for the Graph AutoEncoder is represented with the help of unsupervised local structure component (first order) and a supervised global structure component (second order) which are linked for each node in the graph.\n<img src=\"https:\/\/www.programmersought.com\/images\/979\/223a8a8bc9b82f9255018d248c355c8b.png\">\n\n  For using the library for the Graph AutoEncoder embeddings, we have the following steps:\n  - Install the library with pip\n    ```python\n    pip install SpectralEmbeddings==0.2\n    ```\n  - Create a function to read the input csv file. The input should contain atleast 2 columns - source and target(labels). And both should be in text format. These can include textual extracts and their corresponding labels. The graph is then created as a MultiDigraph from [networkx] with the target and source columns from the input csv file. While generating the embeddings, the extracts from the labels are also considered and can be used to determine which label is the closest to the provided source(input text). In the example below, the 'test_graph_ae' method shows this. The dataset chosen for this demonstration is [Google Quest QnA](https:\/\/www.kaggle.com\/c\/google-quest-challenge) and as such any dataset having a source and a label column(textual contents) can be used to generate the embeddings. The main function for creating the Graph AutoEncoder embedding is the 'get_sdne_embeddings' method. This method takes as parameters: hidden_dims (denotes the hidden embedding dimension of the neural network), alpha and beta are empirical constants for finetuning the embeddings, epochs (number of training iterations), the dataframe along with the source and target labels. The model outputs a embedding matrix (no of entries, no of hidden dims) and the corresponding graph. The graph can then be used for plotting the Chebyshev similarity between each node with the rest of the community neighborhood. The following preview shows the code for generating the Graph AutoEncoder Embeddings\n ```python\n   def test_graph_ae():\n      source_label='question_body'\n      target_label='category'\n      print(\"Input parameters are hidden dimensions ,alpha,beta,epochs\")   \n      hidden_dims=[32,16]\n      alpha=1e-4\n      beta=1e-5\n      epochs=20\n      g_emb,graph_ae_pl=graph_ae.get_sdne_embeddings(train_df,source_label,target_label,hidden_dims,alpha,beta,epochs)\n      print(g_emb)\n      return g_emb,graph_ae_pl\n  ```\n  For plotting(with plotly ) the node embedding of a particular node (represented through a number), the 'plot_ae_embed' method can be used, which takes as parameters the subgraph containing the input node with the rest of the nodes, the input node number and the embedding matrix (embedding weights). This is represented below as :\n  ```python\n  def plot_ae_embed(graph,node_num,emb,label):\n    \n      node,distances,questions=graph_ae.node_level_embedding(graph,node_num,emb)\n      vg_df=pd.DataFrame()\n      vg_df['Premise']=[node]*len(distances)\n      vg_df['Hypothesis']=questions\n      vg_df['Chebyshev_Distance']=distances\n      vg_g=nx.from_pandas_edgelist(vg_df,source='Hypothesis',target='Premise',edge_attr='Chebyshev_Distance')\n      plotter(vg_g,label)\n      return vg_g\n    ```\n  Alternately the 'pyvis_plotter' method can also be used which uses the [pyvis library](https:\/\/pyvis.readthedocs.io\/). Thus the only requirement for creating autoencoder based node representations is a dataframe containing source and target columns both of which should be in textual format.\n\n\n- Graph Convolution Kernel Embeddings: These embeddings are based on spectral graph convolution kernels which capture node representations through laplacian norm matrices. This part is based on the [Graph Convolution Network paper](http:\/\/arxiv.org\/abs\/1609.02907). The GCNs are based on deep neural networks which operate on the node features and the normalized laplacian of the adjacency matrix of input graph. The GCNs are mainly used for node\/subgraph classification tasks but here we are interested in capturing only the embeddings from the penultimate layer of the network. For this we create an Embedding based on Tensorflow as node features. We define that the nodes that don\u2019t have predecessors are in layer 0. The embeddings of these nodes are just their features. To calculate the embeddings of layer k we weight the average embeddings of layer k-1 and put it into an activation function. In this kernel there are 3 variations : Vanilla GCN, Chebyshev GCN and Spline GCN embeddings\n<img src=\"https:\/\/miro.medium.com\/max\/1072\/0*VO6JuDN7Ee1nefPL.png\">\n  \n  - VanillaGCN kernel: A Vanilla GCN\/GNN utilizes the graph laplacian (not normalized laplacian) along with a spectral filter and recursively augments the weights of the next layer based on the previous layer. Here the spectral filter weights are initialized using keras\/tf. The rest of the part involves multiplying the Laplacian tuple [node_features,adjacency matrix] with the spectral filter (kernel) and applying an activation over the result. Generally a softmax activation is applied for classifying the outputs according to the labels.Since we are not classifying the nodes, we can extract the final node weights from the penultimate layer . This allows the projection of the embedding subspace in the VGCN kernel. \n  The steps for using this(or any other variant of GCN kernel) is as follows:\n    - Install the library with pip\n    ```python\n    pip install SpectralEmbeddings==0.2\n    ```\n    - Create a function which is similar to 'test_vanillagcn' method. The important submethod is taken from the SpectralEmbeddings.VanillaGCN python script. The 'get_gcn_embeddings' method is of importance which is used to create the final embeddings after passing it through the kernel. The input parameters are the hidden units(number of hidden neurons for the intermediate GCN layers), number of layers (signifies the number of hidden layers), subset(this includes what part of the entire dataframe should be considered, for instance out of 2000 entries we would like to get the node embeddings of 25 such entries, so subset becomes 25), epochs, the dataframe (input) and the source,target labels (strings). The method returns the embedding matrix and the graph. The embedding matrix has a dimension of (size of subset entries, number of labels). For instance if the subset size is 20 and the set of labels is 6, then the final embedding dimension will be (20,6). Also since GCN uses a classification kernel the embeddings are projected on the basis of the number of unique labels in the input.(All graph kernels follow this rule for projection).\n  ```python\n    def test_vanillagcn():\n      print(\"Testing for VanillaGCN embeddings having a source and target label\")\n      #train_df=pd.read_csv(\"E:\\\\train_graph\\\\train.csv\")\n      source_label='question_body'\n      target_label='category'\n      print(\"Input parameters are hidden units , number of layers,subset (values of entries to be considered for embeddings),epochs \")\n      hidden_units=32\n      num_layers=4\n      subset=34\n      epochs=10\n      v_emb,v_graph=vgcn.get_gcn_embeddings(hidden_units,train_df,source_label,target_label,epochs,num_layers,subset)\n      print(v_emb.shape)\n      return v_emb,v_graph\n   ```\n   A preview of the generated embeddings from the dataset of 34 node entries is represented as :\n   <img src=\"https:\/\/github.com\/abhilash1910\/SpectralEmbeddings\/raw\/master\/Previews\/Vanilla_GCN_preview_1.PNG\">For plotting the embeddings of a node(similar to Graph AutoEncoder embeddings), we can use the 'plot_vgcn_embed' method as follows.\n   ```python\n    def plot_vgcn_embed(graph,node_num,emb,label):\n      node,distances,questions=vgcn.node_level_embedding(graph,node_num,emb)\n      vg_df=pd.DataFrame()\n      vg_df['Premise']=[node]*len(distances)\n      vg_df['Hypothesis']=questions\n      vg_df['Chebyshev_Distance']=distances\n      vg_g=nx.from_pandas_edgelist(vg_df,source='Hypothesis',target='Premise',edge_attr='Chebyshev_Distance')\n      plotter(vg_g,label)\n      return vg_g\n  ```\n  - ChebyshevGCN kernel: A Chebyshev GCN\/GNN  can be used for any arbitrary graph domain, but the limitation is that they are isotropic. Standard ConvNets produce anisotropic filters because Euclidean grids have direction, while Spectral GCNs compute isotropic filters since graphs have no notion of direction (up, down, left, right). CGCN are based on Chebyshev polynomials.The kernel used in a spectral convolution made of Chebyshev polynomials of the diagonal matrix of Laplacian eigenvalues. Chebyshev polynomials are a type of orthogonal polynomials with properties that make them very good at tasks like approximating functions.\n  The steps for using this(or any other variant of GCN kernel) is as follows:\n    - Install the library with pip\n    ```python\n    pip install SpectralEmbeddings==0.2\n    ```\n    - Create a function which is similar to 'test_chebgcn' method. The important submethod is taken from the SpectralEmbeddings.ChebGCN python script. The 'get_chebgcn_embeddings' method is of importance which is used to create the final embeddings after passing it through the kernel. The input parameters are the hidden units(number of hidden neurons for the intermediate GCN layers), number of layers (signifies the number of hidden layers), subset(this includes what part of the entire dataframe should be considered, for instance out of 2000 entries we would like to get the node embeddings of 25 such entries, so subset becomes 25), epochs,k (the order of Chebyshev polynomial to generate) the dataframe (input) and the source,target labels (strings). The method returns the embedding matrix and the graph. The embedding matrix has a dimension of (size of subset entries, number of labels). For instance if the subset size is 20 and the set of labels is 6, then the final embedding dimension will be (20,6). Also since GCN uses a classification kernel the embeddings are projected on the basis of the number of unique labels in the input.(All graph kernels follow this rule for projection).\n  ```python\n    def test_chebgcn():\n      print(\"Testing for ChebGCN embeddings having a source and target label\")\n      #train_df=pd.read_csv(\"E:\\\\train_graph\\\\train.csv\")\n      source_label='question_body'\n      target_label='category'\n      print(\"Input parameters are hidden units , number of layers,subset (values of entries to be considered for embeddings),epochs and k for Cheby polynomials\")\n      hidden_units=32\n      num_layers=4\n      subset=34\n      epochs=10\n      k=4\n      c_emb,c_graph=cgcn.get_chebgcn_embeddings(hidden_units,train_df,source_label,target_label,epochs,num_layers,subset,k)\n      print(c_emb.shape)\n      return c_emb,c_graph\n   ```\n   A preview of the generated embeddings from the dataset of 34 node entries is represented as :\n   <img src=\"https:\/\/github.com\/abhilash1910\/SpectralEmbeddings\/raw\/master\/Previews\/Chebyshev_GCN_preview.PNG\">For plotting the embeddings of a node(similar to Graph AutoEncoder embeddings), we can use the 'plot_cgcn_embed' method as follows.\n   ```python\n    def plot_cgcn_embed(graph,node_num,emb,label):\n      node,distances,questions=cgcn.node_level_embedding(graph,node_num,emb)\n      vg_df=pd.DataFrame()\n      vg_df['Premise']=[node]*len(distances)\n      vg_df['Hypothesis']=questions\n      vg_df['Chebyshev_Distance']=distances\n      vg_g=nx.from_pandas_edgelist(vg_df,source='Hypothesis',target='Premise',edge_attr='Chebyshev_Distance')\n      plotter(vg_g,label)\n      return vg_g\n  ```\n  - SplineGCN kernel: A Spline GCN\/GNN  involve computing smooth spectral filters to get localized spatial filters. The connection between smoothness in frequency domain and localization in space is based on Parseval\u2019s Identity (also Heisenberg uncertainty principle): smaller derivative of spectral filter (smoother function) ~ smaller variance of spatial filter (localization) In this case, we wrap the vanilla GCN with an additional spline functionality by decomposing the laplacian to its diagonals (1-spline) . This represents the eigenvectors which can be added independently instead of taking the entire laplacian at one time.\n  The steps for using this(or any other variant of GCN kernel) is as follows:\n    - Install the library with pip\n    ```python\n    pip install SpectralEmbeddings==0.2\n    ```\n    - Create a function which is similar to 'test_sgcn' method. The important submethod is taken from the SpectralEmbeddings.SplineGCN python script. The 'get_splinegcn_embeddings' method is of importance which is used to create the final embeddings after passing it through the kernel. The input parameters are the hidden units(number of hidden neurons for the intermediate GCN layers), number of layers (signifies the number of hidden layers), subset(this includes what part of the entire dataframe should be considered, for instance out of 2000 entries we would like to get the node embeddings of 25 such entries, so subset becomes 25), epochs, the dataframe (input) and the source,target labels (strings). The method returns the embedding matrix and the graph. The embedding matrix has a dimension of (size of subset entries, number of labels). For instance if the subset size is 20 and the set of labels is 6, then the final embedding dimension will be (20,6). Also since GCN uses a classification kernel the embeddings are projected on the basis of the number of unique labels in the input.(All graph kernels follow this rule for projection).\n  ```python\n    def test_sgcn():\n      print(\"Testing for SplineGCN embeddings having a source and target label\")\n      #train_df=pd.read_csv(\"E:\\\\train_graph\\\\train.csv\")\n      source_label='question_body'\n      target_label='category'\n      print(\"Input parameters are hidden units , number of layers,subset (values of entries to be considered for embeddings),epochs and k for Cheby polynomials\")\n      hidden_units=32\n      num_layers=4\n      subset=34\n      epochs=10\n      s_emb,s_graph=sgcn.get_splinegcn_embeddings(hidden_units,train_df,source_label,target_label,epochs,num_layers,subset)\n      print(s_emb.shape)\n      return s_emb,s_graph\n  ```\n   A preview of the generated embeddings from the dataset of 34 node entries is represented as :\n   <img src=\"https:\/\/github.com\/abhilash1910\/SpectralEmbeddings\/raw\/master\/Previews\/Spline_GCN_preview.PNG\">For plotting the embeddings of a node(similar to Graph AutoEncoder embeddings), we can use the 'plot_sgcn_embed' method as follows.\n   ```python\n    def plot_sgcn_embed(graph,node_num,emb,label):\n      node,distances,questions=sgcn.node_level_embedding(graph,node_num,emb)\n      vg_df=pd.DataFrame()\n      vg_df['Premise']=[node]*len(distances)\n      vg_df['Hypothesis']=questions\n      vg_df['Chebyshev_Distance']=distances\n      vg_g=nx.from_pandas_edgelist(vg_df,source='Hypothesis',target='Premise',edge_attr='Chebyshev_Distance')\n      plotter(vg_g,label)\n      return vg_g\n  ```\nAlternately all the 3 sub- GCN embeddings can also be plotted using the pyvis library. Also for importing the GCN and Graph AutoEncoder scripts the following has to be written at the start of the script\n\n```python\nimport SpectralEmbeddings.VanillaGCN as vgcn\nimport SpectralEmbeddings.ChebGCN as cgcn\nimport SpectralEmbeddings.SplineGCN as sgcn\nimport SpectralEmbeddings.GraphAutoencoder as graph_ae\n```\n\nIn the next cells, we see the example script for this library as mentioned in this writeup.\n\n<img src=\"https:\/\/camo.githubusercontent.com\/627c7b5447bd93dcc191ea19ade3b10d913e06d3ca4dd18259fb8fd79deb11d4\/68747470733a2f2f6d69726f2e6d656469756d2e636f6d2f6d61782f313833382f312a2d2d443174444d6a59577766316d76385a59526f37412e706e67\">"}}