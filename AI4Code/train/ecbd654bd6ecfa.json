{"cell_type":{"fa116e0e":"code","bc08de6b":"code","b7ac9028":"code","48d90d73":"code","391e1ef3":"code","fd981950":"code","9822fab1":"code","bcca05d8":"code","e23f98a9":"code","1dff5844":"code","d8e27d04":"code","82cb627b":"code","a8735684":"code","b24ab873":"code","ae23a719":"code","e3490d85":"code","8a81e203":"code","4be4a724":"code","c24f9bac":"code","2880ed83":"code","d2f31072":"code","5e775856":"code","c228fc5e":"code","77183fb1":"code","ef6118f4":"code","b64928ec":"code","5603b797":"code","20a1b2dd":"code","7a3d9880":"code","1d45050f":"code","c1543233":"code","8e23157c":"code","96b1901f":"code","5f130e30":"code","04e05048":"code","2c6066b6":"code","5a30bb18":"code","ce0249c7":"code","7e885fa9":"code","f44591ff":"code","371dca61":"code","d805f67e":"code","04e1b0a6":"code","eac6f875":"code","ba34e27b":"code","8596bab2":"code","d39c2c4c":"code","74f3ee9d":"code","74c66f2a":"code","96a473b3":"markdown","b76404b3":"markdown","65a89a6b":"markdown","c9b74c81":"markdown","0f4cd9e7":"markdown","ab047597":"markdown","ae90e063":"markdown","f875c072":"markdown","73d29389":"markdown","cd51aad3":"markdown","914e4e09":"markdown","ea2e2b88":"markdown","158bb436":"markdown","3e5661f1":"markdown","8e2bfc27":"markdown","12ff5f93":"markdown","5b5033fc":"markdown","a9209123":"markdown","18a9a443":"markdown","3b23c45a":"markdown","c477ae51":"markdown","4c078eb6":"markdown","4d284d48":"markdown","8323e0b8":"markdown","4eab2ed9":"markdown","a645d059":"markdown","6b1488ae":"markdown","5649dc3f":"markdown","8219e414":"markdown","0c54b729":"markdown","8aba8610":"markdown","f6fdbfa2":"markdown","5d43400c":"markdown"},"source":{"fa116e0e":"# importing libraries\nimport os\nimport torch\nimport torchvision\nimport tarfile\nfrom torchvision.datasets.utils import download_url\nfrom torch.utils.data import random_split\nfrom torchvision.datasets import ImageFolder\nfrom torchvision.transforms import ToTensor,ToPILImage\nimport matplotlib.pyplot as plt\nfrom torchvision.utils import make_grid\nfrom torch.utils.data.dataloader import DataLoader\nfrom torchvision.utils import make_grid\nimport torch.nn as nn\nimport torch.nn.functional as F","bc08de6b":"# Extract from tar archive file\nwith tarfile.open('..\/input\/cifar10\/cifar10.tgz', 'r:gz') as tar:\n    tar.extractall(path='.\/input')","b7ac9028":"print(os.listdir('.\/input\/cifar10'))\nclasses= os.listdir('.\/input\/cifar10\/train')\nprint(classes)","48d90d73":"cat_files= os.listdir('.\/input\/cifar10\/train\/cat')\nprint('No of training example for cats:',len(cat_files))\nprint(cat_files[:5])\n","391e1ef3":"automobile_files= os.listdir('.\/input\/cifar10\/train\/automobile')\nprint('No of training example for automobiles:',len(automobile_files))\nprint(automobile_files[:5])\n","fd981950":"cat_files= os.listdir('.\/input\/cifar10\/test\/cat')\nprint('No of test example for cats:',len(cat_files))\nprint(cat_files[:5])\n","9822fab1":"data_dir = '.\/input\/cifar10'\ndataset = ImageFolder(data_dir+'\/train', transform=ToTensor())\n","bcca05d8":"img, label= dataset[0]\nprint(img.shape,label)\nimg","e23f98a9":"dataset","1dff5844":"print(dataset.classes)","d8e27d04":"def show_image(img,label):\n    print('Label: ', dataset.classes[label],\"(\"+str(label)+\")\")\n    plt.imshow(img.permute(1,2,0))","82cb627b":"show_image(dataset[222][0],dataset[222][1])","a8735684":"show_image(*dataset[1099])","b24ab873":"random_seed=42\ntorch.manual_seed(random_seed);","ae23a719":"val_size = 5000\ntrain_size = len(dataset) - val_size\ntrain_ds, val_ds = random_split(dataset,[train_size, val_size],generator=torch.manual_seed(random_seed))\nlen(train_ds), len(val_ds)","e3490d85":"batch_size=128\ntrain_dl = DataLoader(train_ds,batch_size,shuffle=True,num_workers=4,pin_memory=True )\nval_dl = DataLoader(val_ds, batch_size, num_workers=4, pin_memory=True)","8a81e203":"train_dl # Data loader. Combines a dataset and a sampler, and provides an iterable over the given dataset.","4be4a724":"def show_images_batch(d1):\n    for images, labels in d1:\n        fig, ax= plt.subplots(figsize=(16,8))\n        ax.set_xticks([])\n        ax.set_yticks([])\n        ax.imshow(make_grid(images, nrow=16).permute(1,2,0))\n        break # to stop loop otherwise 4500 images in batch size of 128 will print and is computationally expensive","c24f9bac":"show_images_batch(train_dl) # training data single batch images","2880ed83":"show_images_batch(val_dl) # validation data single batch images","d2f31072":"def apply_kernel(image, kernel):\n    ri, ci = image.shape       # image dimensions\n    rk, ck = kernel.shape      # kernel dimensions\n    ro, co = ri-rk+1, ci-ck+1  # output dimensions, No padding and no striding\n    output = torch.zeros([ro, co])\n    for i in range(ro): \n        for j in range(co):\n            output[i,j] = torch.sum(image[i:i+rk,j:j+ck] * kernel) \n    return output","5e775856":"sample_image = torch.tensor([\n    [4, 4, 3, 1, 0], \n    [0, 0, 1, 4, 1], \n    [4, 1, 3, 3, 4], \n    [2, 0, 0, 3, 3], \n    [3, 0, 0, 0, 1]\n], dtype=torch.float32)\n\nsample_kernel = torch.tensor([\n    [1, 1, 1], \n    [1, 1, 1], \n    [1, 1, 1]\n], dtype=torch.float32) #blur filter\n\napply_kernel(sample_image, sample_kernel)","c228fc5e":"def accuracy(outputs,labels):\n    _,preds= torch.max(outputs, dim=1)\n    return torch.tensor(torch.sum(preds==labels).item()\/len(preds))\n\nclass ImageClassificationBase(nn.Module):\n    def training_step(self,batch):\n        images,labels= batch\n        out= self(images)  #Generate predictions\n        loss= F.cross_entropy(out,labels) #calculate loss\n        return loss\n    \n    def validation_step(self,batch):\n        images, labels = batch\n        out = self(images)  # Generate predictions\n        loss= F.cross_entropy(out,labels) # calculate loss\n        acc= accuracy(out,labels)\n        return {'val_loss':loss.detach(), 'val_acc':acc}\n    \n    def validation_epoch_end(self, outputs):\n        batch_losses = [x['val_loss'] for x in outputs]\n        epoch_loss= torch.stack(batch_losses).mean() # Stacking losses to combine losses and calculate average\n        batch_accs = [x['val_acc'] for x in outputs]\n        epoch_acc = torch.stack(batch_accs).mean() # Combine accuracies\n        return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}\n        \n    def epoch_end(self,epoch,result):\n        print(\"Epoch [{}], train_loss: {:.4f}, val_loss: {:.4f}, val_acc: {:.4f}\".format(\n            epoch, result['train_loss'], result['val_loss'], result['val_acc']))","77183fb1":"class Cifar10CnnModel(ImageClassificationBase):\n    def __init__(self):\n        super().__init__()\n        self.network = nn.Sequential(\n            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(2, 2), # output: 64 x 16 x 16\n\n            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n            nn.ReLU(),\n            nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(2, 2), # output: 128 x 8 x 8\n\n            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n            nn.ReLU(),\n            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(2, 2), # output: 256 x 4 x 4\n\n            nn.Flatten(), \n            nn.Linear(256*4*4, 1024),\n            nn.ReLU(),\n            nn.Linear(1024, 512),\n            nn.ReLU(),\n            nn.Linear(512, 10))\n        \n    def forward(self, xb):\n        return self.network(xb)","ef6118f4":"model= Cifar10CnnModel()\nmodel","b64928ec":"for images, labels in train_dl:\n    print('images.shape:', images.shape)\n    out = model(images)\n    print('out.shape:',out.shape)\n    print('out[0]',out[0])\n    break","5603b797":"out.shape","20a1b2dd":"def get_default_device():\n    \"\"\"Pick GPU if available, else CPU\"\"\"\n    if torch.cuda.is_available():\n        return torch.device('cuda')\n    else:\n        return torch.device('cpu')\n    \ndef to_device(data, device):\n    \"\"\"Move tensor(s) to chosen device\"\"\"\n    if isinstance(data, (list,tuple)):\n        return [to_device(x, device) for x in data]\n    return data.to(device, non_blocking=True)\n\nclass DeviceDataLoader():\n    \"\"\"Wrap a dataloader to move data to a device\"\"\"\n    def __init__(self, dl, device):\n        self.dl = dl\n        self.device = device\n        \n    def __iter__(self):\n        \"\"\"Yield a batch of data after moving it to device\"\"\"\n        for b in self.dl: \n            yield to_device(b, self.device)\n\n    def __len__(self):\n        \"\"\"Number of batches\"\"\"\n        return len(self.dl)","7a3d9880":"device = get_default_device()\ndevice","1d45050f":"train_dl = DeviceDataLoader(train_dl, device)\nval_dl = DeviceDataLoader(val_dl, device) # load data to  device (GPU if available)\nto_device(model, device) # move model to GPU if available","c1543233":"#In this mode, the result of every computation will have requires_grad=False, even when the inputs have requires_grad=True.\n@torch.no_grad() \ndef evaluate(model, val_loader):\n    model.eval() # Setting model to evaluation mode, the model can adjust its behavior regarding some operations, like Dropout.\n    outputs = [model.validation_step(batch) for batch in val_loader]\n    return model.validation_epoch_end(outputs)\n  \ndef fit(epochs, lr, model, train_loader, val_loader, opt_func= torch.optim.SGD):\n    history=[]\n    optimizer= opt_func(model.parameters(),lr) # model paramters w.r.t calculate derivative of loss\n    for epoch in range(epochs):\n        # Training phase\n        model.train() # Setting model to training mode\n        train_losses=[]\n        for batch in train_loader:\n            loss= model.training_step(batch)\n            train_losses.append(loss)\n            loss.backward() #compute  gradients\n            optimizer.step()\n            optimizer.zero_grad() # zero the gradients\n        #Validation phase\n        result= evaluate(model,val_loader)\n        result['train_loss'] = torch.stack(train_losses).mean().item()\n        model.epoch_end(epoch, result)\n        history.append(result)\n    return history\n            ","8e23157c":"model","96b1901f":"evaluate(model,val_dl)","5f130e30":"num_epochs = 15\nopt_func = torch.optim.Adam\nlr = 0.001","04e05048":"history = fit(num_epochs, lr, model, train_dl, val_dl, opt_func)","2c6066b6":"def plot_accuracies(history):\n    accuracies=[x['val_acc'] for x in history]\n    plt.plot(accuracies,'-x')\n    plt.xlabel('Epoch')\n    plt.ylabel('Accuracy')\n    plt.title('Accuracy vs No of epochs')","5a30bb18":"plt.figure(figsize=(10,6))\nplot_accuracies(history)","ce0249c7":"def plot_losses(history):\n    train_losses = [x.get('train_loss') for x in history]\n    val_losses = [x['val_loss'] for x in history]\n    plt.plot(train_losses, '-bx')\n    plt.plot(val_losses, '-rx')\n    plt.xlabel('epoch')\n    plt.ylabel('loss')\n    plt.legend(['Training', 'Validation'])\n    plt.title('Loss vs. No. of epochs');","7e885fa9":"plt.figure(figsize=(10,6))\nplot_losses(history)","f44591ff":"test_dataset = ImageFolder(data_dir+'\/test', transform=ToTensor())\ntest_dataset","371dca61":"def predict_image(img, model):\n    xb= to_device(img.unsqueeze(0),device) # unsqueeze turns an n-dimensionsal tensor into an n+1-dimensional one. But since it is ambiguous which axis the new dimension lies across, this needs to be specified.\n    # Get predictions from model\n    yb = model(xb)\n    # Pick index\/label with highest probability\n    _, preds= torch.max(yb, dim=1)\n    return dataset.classes[preds[0].item()]","d805f67e":"img,label= test_dataset[8]\nplt.imshow(img.permute(1,2,0))\nprint('Label:', dataset.classes[label], ', Predicted:', predict_image(img, model))","04e1b0a6":"img,label= test_dataset[1220]\nplt.imshow(img.permute(1,2,0))\nprint('Label:', dataset.classes[label], ', Predicted:', predict_image(img, model))","eac6f875":"img,label= test_dataset[345]\nplt.imshow(img.permute(1,2,0))\nprint('Label:', dataset.classes[label], ', Predicted:', predict_image(img, model))","ba34e27b":"img,label= test_dataset[6153]\nplt.imshow(img.permute(1,2,0))\nprint('Label:', dataset.classes[label], ', Predicted:', predict_image(img, model))","8596bab2":"img,label= test_dataset[456]\nplt.imshow(img.permute(1,2,0))\nprint('Label:', dataset.classes[label], ', Predicted:', predict_image(img, model))","d39c2c4c":"img,label= test_dataset[10]\nplt.imshow(img.permute(1,2,0))\nprint('Label:', dataset.classes[label], ', Predicted:', predict_image(img, model))","74f3ee9d":"img,label= test_dataset[1432]\nplt.imshow(img.permute(1,2,0))\nprint('Label:', dataset.classes[label], ', Predicted:', predict_image(img, model))","74c66f2a":"test_loader=DeviceDataLoader(DataLoader(test_dataset,batch_size),device)\ntest_result = evaluate(model, test_loader)\ntest_result","96a473b3":"<a id=\"4\"><\/a>\n## Training and Validation Datasets\n\nWhile building real world machine learning models, it is quite common to split the dataset into 3 parts:\n\n1. **Training set** - used to train the model i.e. compute the loss and adjust the weights of the model using gradient descent.\n2. **Validation set** - used to evaluate the model while training, adjust hyperparameters (learning rate etc.) and pick the best version of the model.\n3. **Test set** - used to compare different models, or different types of modeling approaches, and report the final accuracy of the model.\n\nSince there's no predefined validation set, we can set aside a small portion (5000 images) of the training set to be used as the validation set. We'll use the `random_split` helper method from PyTorch to do this. To ensure that we always create the same validation set, we'll also set a seed for the random number generator.","b76404b3":"Let's look inside a couple of folders, one from the training set and another from the test set. As an exercise, you can verify that that there are an equal number of images for each class, 5000 in the training set and 1000 in the test set.","65a89a6b":"Initialy, both the training and validation losses seem to decrease over time. However, if we train the model for long enough,training loss continues to decrease, while the validation loss stops decreasing, and even starts to increase after a certain point(overfitting).","c9b74c81":"We'll use `nn.Sequential` to chain the layers and activations functions into a single network architecture.","0f4cd9e7":"The dataset is extracted to the directory `input\/cifar10`. It contains 2 folders `train` and `test`, containing the training set (50000 images) and test set (10000 images) respectively. Each of them contains 10 folders, one for each class of images. Let's verify this using `os.listdir`.","ab047597":"## Defining the Model (Convolutional Neural Network)\n\n\n> The 2D convolution is a fairly simple operation at heart: you start with a kernel, which is simply a small matrix of weights. This kernel \u201cslides\u201d over the 2D input data, performing an elementwise multiplication with the part of the input it is currently on, and then summing up the results into a single output pixel.\n\n<img src=\"https:\/\/miro.medium.com\/max\/1070\/1*Zx-ZMLKab7VOCQTxdZ1OAw.gif\" style=\"max-width:400px;\">\n\n","ae90e063":"![cifar10](https:\/\/miro.medium.com\/max\/709\/1*LyV7_xga4jUHdx4_jHk1PQ.png)","f875c072":"The `Conv2d` layer transforms a 3-channel image to a 16-channel *feature map*, and the `MaxPool2d` layer halves the height and width. The feature map gets smaller as we add more layers, until we are finally left with a small feature map, which can be flattened into a vector. We can then add some fully connected layers at the end to get vector of size 10 for each image.\n\n<img src=\"https:\/\/i.imgur.com\/KKtPOKE.png\" style=\"max-width:540px\">\n\nLet's define the model by extending an `ImageClassificationBase` class which contains helper methods for training & validation.","73d29389":"Based on where you run this notebook, your default device could be a CPU (`torch.device('cpu')`) or a GPU (`torch.device('cuda')`)","cd51aad3":"Plot valdation set accuracies to study how the model improves over time.","914e4e09":"<a id=\"6\"><\/a>\n# Training the Model","ea2e2b88":"Before continuing, let's save our work using the `jovian` python library. ","158bb436":"<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:black; border:0; color:#ff6666' role=\"tab\" aria-controls=\"home\"><center>If you found this notebook helpful , some upvotes would be very much appreciated - That will keep me motivated \ud83d\ude0a<\/center><\/h2>\n","3e5661f1":"<a id=\"7\"><\/a>\n# Testing with individual images","8e2bfc27":"`Identifying where our model performs poorly can help us improve the model, by collecting more training data, \nincreasing\/decreasing the complexity of the model, and changing the hypeparameters.`","12ff5f93":"<a id=\"1\"><\/a>\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h1 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:black; border:0; color:#ff6666' role=\"tab\" aria-controls=\"home\"><center>About Dataset<\/center><\/h1>\n\n`CIFAR-10  is an established computer-vision dataset used for object recognition. It is a subset of the 80\n million tiny images dataset and consists of 60,000 32x32 color images containing one of 10 object classes, \n with 6000 images per class. It was collected by Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton.`\n\n","5b5033fc":"Our model reaches an accuracy of around 75%, and by looking at the graph, it seems unlikely that the model will achieve an accuracy higher than 80% even after training for a long time. This suggests that we might need to use a more powerful model to capture the relationship between the images and the labels more accurately.","a9209123":"The initial accuracy is around 10% in one epoch, which is expected model is not trained enough.","18a9a443":"<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h1 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:black; border:0; color:#ff6666' role=\"tab\" aria-controls=\"home\"><center>Table of Contents<\/center><\/h1>\n\n    \n    \n- [Problem Statement](#1)\n- [Import Libaries](#2)\n- [Exploring the Data](#3)     \n- [Training and Validation Datasets](#4)\n- [Convolutional Neural Network (CNN)](#5)\n- [Training the Model](#6)\n- [Testing with individual images)](#7)","3b23c45a":"<a id=\"5\"><\/a>\n# Convolutional Neural Network\n","c477ae51":"<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h1 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:black; border:0; color:#ff6666' role=\"tab\" aria-controls=\"home\"><center>Thank You \ud83d\ude4f <\/center><\/h1>\n","4c078eb6":"Let us implement a convolution operation on a 1 channel image with a 3x3 kernel.","4d284d48":"Let's look at a sample element from the training dataset. Each element is a tuple, containing a image tensor and a label. Since the data consists of 32x32 px color images with 3 channels (RGB), each image tensor has the shape `(3, 32, 32)`.","8323e0b8":"We can now wrap our training and validation data loaders using `DeviceDataLoader` for automatically transferring batches of data to the GPU (if available), and use `to_device` to move our model to the GPU (if available).","4eab2ed9":"m","a645d059":"<a id=\"3\"><\/a>\n# Exploring the Data","6b1488ae":"Tensor Image is a tensor with (C, H, W) shape, where C is a number of channels, H and W are image height and width. Batch of Tensor Images is a tensor of (B, C, H, W) shape, where B is a number of images in the batch.","5649dc3f":" We can use the `ImageFolder` class from `torchvision` to load the data as PyTorch tensors.","8219e414":"`detach()` detaches the output from the computationnal graph. So no gradient will be backproped along this variable.\n\n`torch.no_grad` says that no operation should build the graph.\n\n\nThe difference is that one refers to only a given variable on which it\u2019s called. The other affects all operations taking place within the with statement.tensor.detach() creates a tensor that shares storage with tensor that does not require grad. It detaches the output from the computational graph. So no gradient will be backpropagated along this variable.\n\nThe wrapper with torch.no_grad() temporarily set all the requires_grad flag to false. torch.no_grad says that no operation should build the graph.\n\nThe difference is that one refers to only a given variable on which it is called. The other affects all operations taking place within the with statement. Also, torch.no_grad will use less memory because it knows from the beginning that no gradients are needed so it doesn\u2019t need to keep intermediary results.","0c54b729":"<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h1 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:black; border:0; color:#ff6666' role=\"tab\" aria-controls=\"home\"><center>CIFAR-10 - Object Recognition in Images using CNN<\/center><\/h1>","8aba8610":"We can view the image using `matplotlib`, but we need to change the tensor dimensions to `(32,32,3)` instead of `(3,32,32)`. Let's create a helper function to display an image and its label.","f6fdbfa2":"The list of classes is stored in the `.classes` property of the dataset. The numeric label for each element corresponds to index of the element's label in the list of classes.","5d43400c":"<a id=\"2\"><\/a>\n# Import Libaries"}}