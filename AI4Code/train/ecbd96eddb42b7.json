{"cell_type":{"2565c2bb":"code","2779699f":"code","08dc07de":"code","eeeeae53":"code","c914176d":"code","8e08d0c0":"code","9b52fa10":"code","7d79fb04":"code","b1d68ac7":"code","aacb3e84":"code","4275212d":"code","c6970ec3":"code","e6ca6f99":"code","7cc0c6ba":"code","fa6f09e1":"code","b3acb040":"code","7c1f3726":"code","b57d56b5":"code","1bcf22ca":"code","a7764b50":"code","199a7ca2":"code","1a629246":"code","1a5fe08c":"code","31371528":"code","13a91769":"code","a727ddf5":"code","7b6858f5":"code","60585273":"code","5c8c2096":"code","a3c2d415":"code","4cf99eed":"code","39355618":"code","44248c4c":"code","21ac89fe":"code","d63291f9":"code","7484962c":"code","09b75e74":"code","3b0010a9":"code","cfcd39ef":"code","4c57de9c":"code","cb9be9ab":"code","b048ee3a":"code","6c009ba9":"code","93224f36":"code","350eff26":"code","5d3eb84d":"code","d230baa5":"code","2da9f923":"code","1c75d724":"code","a4a93fa9":"code","4209e1f3":"code","ce3822b4":"code","c59d1735":"code","c6e773ea":"code","345a604f":"code","acab9459":"code","72f60210":"code","59745932":"code","912b6984":"code","00af6ea4":"code","b11655be":"code","6a39b6b1":"code","e0514b9a":"code","94af349d":"code","c50016f4":"code","40b20524":"code","bf5ea565":"markdown","e157b856":"markdown","f51074a2":"markdown","d27e2126":"markdown","2b0f9f8e":"markdown","9775defd":"markdown","16833829":"markdown","e6954f54":"markdown","e94030be":"markdown","7c7ce29d":"markdown","9b518b89":"markdown","be6025f5":"markdown","a218b2e0":"markdown","0ec4b7e6":"markdown","356d076a":"markdown","a6b88632":"markdown","75eba45e":"markdown","a729a24a":"markdown","ce05c948":"markdown","c6fc7d91":"markdown","f38e47ac":"markdown","af437c83":"markdown","b500aeac":"markdown","7ff0371e":"markdown","f8d9cc01":"markdown","e3fd1588":"markdown","dc602083":"markdown","a66b5b49":"markdown","7f89f0e9":"markdown","e238cbe4":"markdown","8ec94187":"markdown","6f7acf5e":"markdown","9507a735":"markdown","96dcec31":"markdown","b12ebb86":"markdown","6307be4f":"markdown","ad6c1086":"markdown","ebaa5efd":"markdown","e840dac5":"markdown","68f2ca7c":"markdown","b436c07f":"markdown","62f32a5f":"markdown","be73ef83":"markdown"},"source":{"2565c2bb":"!pip install -qU pip","2779699f":"!pip install -qU xgboost\n!pip install -qU lightgbm","08dc07de":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom scipy import stats\nfrom scipy.stats import norm, skew\nimport os\n\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.linear_model import Lasso\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import r2_score\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\nfrom sklearn.preprocessing import RobustScaler\n\nplt.style.use(\"fivethirtyeight\")\npd.pandas.set_option('display.max_columns', None)\nsns.set_style('darkgrid')\n%matplotlib inline","eeeeae53":"train = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv')","c914176d":"print(train.shape)\nprint(test.shape)","8e08d0c0":"test['Id'].values","9b52fa10":"# dropping ID\n\ntrain.drop(['Id'], axis=1, inplace=True)\n\ntest_id = test['Id'].values # for submission\ntest.drop(['Id'], axis=1, inplace=True)","7d79fb04":"fig, ax = plt.subplots()\nax.scatter(x = train['GrLivArea'], y = train['SalePrice'])\nplt.ylabel('SalePrice', fontsize=13)\nplt.xlabel('GrLivArea', fontsize=13)\nplt.show()","b1d68ac7":"#Deleting outliers\n\ntrain = train.drop(train[(train['GrLivArea']>4000) & (train['SalePrice']<300000)].index)\nfig, ax = plt.subplots()\nax.scatter(train['GrLivArea'], train['SalePrice'])\nplt.ylabel('SalePrice', fontsize=13)\nplt.xlabel('GrLivArea', fontsize=13)\nplt.show()","aacb3e84":"sns.distplot(train['SalePrice'] , fit=norm);\n\n(mu, sigma) = norm.fit(train['SalePrice'])\nprint(f'mu = {mu:.2f} and sigma = {sigma:.2f}')\n\n#Now plot the distribution\nplt.legend([f'Normal dist. ($\\mu=$ {mu:.2f} and $\\sigma=$ {sigma:.2f} )'],\n            loc='best')\nplt.ylabel('Frequency')\nplt.title('SalePrice distribution')\n\n#Get also the QQ-plot\nfig = plt.figure()\nres = stats.probplot(train['SalePrice'], plot=plt)\nplt.show()","4275212d":"train[\"SalePrice\"] = np.log1p(train[\"SalePrice\"])\n\nsns.distplot(train['SalePrice'] , fit=norm);\n\n# Get the fitted parameters used by the function\n(mu, sigma) = norm.fit(train['SalePrice'])\nprint(f'mu = {mu:.2f} and sigma = {sigma:.2f}')\n\n#Now plot the distribution\nplt.legend([f'Normal dist. ($\\mu=$ {mu:.2f} and $\\sigma=$ {sigma:.2f} )'],\n            loc='best')\nplt.ylabel('Frequency')\nplt.title('SalePrice distribution')\n\n#Get also the QQ-plot\nfig = plt.figure()\nres = stats.probplot(train['SalePrice'], plot=plt)\nplt.show()","c6970ec3":"ntrain = train.shape[0]\nntest = test.shape[0]\ny_train = train.SalePrice.values\nall_data = pd.concat((train, test)).reset_index(drop=True)\nall_data.drop(['SalePrice'], axis=1, inplace=True)\nprint(f\"all_data size is : {all_data.shape}\")","e6ca6f99":"features_with_na = {feature: all_data[feature].isnull().sum() for feature in all_data.columns \n                    if all_data[feature].isnull().sum() > 0}\n\nsize = all_data.shape[0]\na = pd.DataFrame({\n    'features': list(features_with_na.keys()),\n    'Total': list(features_with_na.values()),\n    'Missing_PCT': [np.round((features_with_na[i] \/ size) * 100, 3) for i in features_with_na.keys()]\n}).sort_values(by='Missing_PCT', ascending=False).reset_index(drop=True)\na.style.background_gradient(cmap='Reds') ","7cc0c6ba":"print(f\"Total number of missing values: {all_data.isna().sum().sum()}\")","fa6f09e1":"num_with_nan = [feature for feature in features_with_na.keys() if train[feature].dtypes != 'O']\npd.DataFrame({\n    'feature': num_with_nan,\n    'Count': [all_data[i].isna().sum() for i in num_with_nan]\n})","b3acb040":"# LotFrontage\n\nall_data[\"LotFrontage\"] = all_data.groupby(\"Neighborhood\")[\"LotFrontage\"].apply(\n    lambda x: x.fillna(x.median()))\nall_data[\"LotFrontage\"].isna().sum()","7c1f3726":"# MasVnrArea\n\nall_data[\"MasVnrArea\"] = all_data[\"MasVnrArea\"].fillna(0)\nall_data[\"MasVnrArea\"].isna().sum()","b57d56b5":"# BsmtX\n\nfor col in ('BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF','TotalBsmtSF', \n            'BsmtFullBath', 'BsmtHalfBath'):\n    all_data[col] = all_data[col].fillna(0)\n\nall_data[['BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF','TotalBsmtSF', \n         'BsmtFullBath', 'BsmtHalfBath']].isna().sum()","1bcf22ca":"# GarageX\n\nfor col in ('GarageYrBlt', 'GarageArea', 'GarageCars'):\n    all_data[col] = all_data[col].fillna(0)\n\nall_data[['GarageYrBlt', 'GarageArea', 'GarageCars']].isna().sum()","a7764b50":"sns.heatmap(pd.DataFrame(\n    {\n        'BsmtFinSF1': train['BsmtFinSF1'],\n        'BsmtFinSF2': train['BsmtFinSF2'], \n        'BsmtUnfSF': train['BsmtUnfSF'],\n        'TotalBsmtSF': train['TotalBsmtSF'], \n        'BsmtFullBath': train['BsmtFullBath'], \n        'BsmtHalfBath': train['BsmtHalfBath'],\n        'SalePrice': train['SalePrice'],\n    }\n).corr(), cmap='coolwarm', annot=True) \nplt.title(\"Bsmt numerical features - train set\")\nplt.show()","199a7ca2":"sns.heatmap(pd.DataFrame(\n    {\n        'GarageYrBlt': train['GarageYrBlt'], \n        'GarageArea': train['GarageArea'],\n        'GarageCars': train['GarageCars'],\n        'SalePrice': train['SalePrice'],\n    }).corr(), annot=True, cmap='coolwarm'\n)\nplt.title(\"Garage numerical features - train set\")\nplt.show()","1a629246":"to_remove_ = ['BsmtFinSF2', 'BsmtUnfSF', 'BsmtFullBath', 'BsmtHalfBath', \n             'GarageYrBlt', 'GarageArea']","1a5fe08c":"all_data[num_with_nan].isna().sum()","31371528":"cat_nan = [feature for feature in features_with_na if all_data[feature].dtypes == \"O\"]\npd.DataFrame({\n    'feature': cat_nan,\n    'Count': [all_data[i].isna().sum() for i in cat_nan]\n}).sort_values(by=\"Count\", ascending = False).reset_index(drop=True)","13a91769":"to_remove_.extend(['PoolQC','MiscFeature','Alley','Fence', 'Utilities'])","a727ddf5":"plt.figure(figsize=(10, 8))\nbasement_variables = ['BsmtQual', 'BsmtCond', 'BsmtExposure', \n                      'BsmtFinType1', 'BsmtFinType2']\n\nfor i, feature in enumerate(basement_variables, 1):\n    plt.subplot(3, 2, i)\n    sns.boxenplot(data=train, x=feature, y=train['SalePrice'])\n\nplt.tight_layout(h_pad=1.2)\nplt.show()","7b6858f5":"plt.figure(figsize=(10, 8))\ngarage_variables = ['GarageType', 'GarageFinish', 'GarageQual', \n                    'GarageCond']\n\nfor i, feature in enumerate(garage_variables, 1):\n    plt.subplot(2, 2, i)\n    sns.boxenplot(data=train, x=feature, y=train['SalePrice'])\n\nplt.tight_layout(h_pad=1.2)\nplt.show()","60585273":"fill_none = [\"FireplaceQu\",  \"GarageType\", \"GarageFinish\", \"GarageQual\", \"GarageCond\", \n             \"BsmtQual\", \"BsmtCond\", \"BsmtExposure\", \"BsmtFinType1\", \"BsmtFinType2\", \"MasVnrType\"]\n\nfor feature in fill_none:\n    all_data[feature].fillna(\"None\", inplace=True)\n\nall_data[fill_none].isna().sum().sum()","5c8c2096":"fill_mode = [\"MSZoning\", \"Electrical\", \"KitchenQual\", \"Exterior1st\", \n             \"Exterior2nd\", \"SaleType\"]\n\nfor feature in fill_mode:\n    all_data[feature].fillna(all_data[feature].mode()[0], inplace=True)\n\nall_data[fill_mode].isna().sum().sum()","a3c2d415":"all_data[\"Functional\"] = all_data[\"Functional\"].fillna(\"Typ\")","4cf99eed":"print(f\"Number of missing values: {all_data.isna().sum().sum()}\")","39355618":"# dropping features\n\nall_data.drop(to_remove_, axis=1, inplace=True)\nall_data.shape","44248c4c":"all_data_cat = all_data.copy()\nall_data_cat.shape","21ac89fe":"all_data_free = all_data.copy()\nremove_cat_garage_bsmt = [\"GarageType\", \"GarageFinish\", \"GarageQual\", \"GarageCond\", \n             \"BsmtQual\", \"BsmtCond\", \"BsmtExposure\", \"BsmtFinType1\", \"BsmtFinType2\"]\nall_data_free.drop(remove_cat_garage_bsmt, axis=1, inplace=True)\nall_data_free.shape","d63291f9":"# MSSubClass=The building class\nall_data_cat['MSSubClass'] = all_data_cat['MSSubClass'].apply(str)\n\n\n# Changing OverallCond into a categorical variable\nall_data_cat['OverallCond'] = all_data_cat['OverallCond'].astype(str)\n\n\n# Year and month sold are transformed into categorical features.\nall_data_cat['YrSold'] = all_data_cat['YrSold'].astype(str)\nall_data_cat['MoSold'] = all_data_cat['MoSold'].astype(str)\n\n# same transformation to other dataframe\n# MSSubClass=The building class\nall_data_free['MSSubClass'] = all_data_free['MSSubClass'].apply(str)\n\n\n# Changing OverallCond into a categorical variable\nall_data_free['OverallCond'] = all_data_free['OverallCond'].astype(str)\n\n\n# Year and month sold are transformed into categorical features.\nall_data_free['YrSold'] = all_data_free['YrSold'].astype(str)\nall_data_free['MoSold'] = all_data_free['MoSold'].astype(str)","7484962c":"from sklearn.preprocessing import LabelEncoder\n\n\ncols = ['BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', 'BsmtQual', \n        'CentralAir', 'ExterCond', 'ExterQual', 'FireplaceQu', 'Functional', \n        'GarageCond', 'GarageFinish', 'GarageQual', 'HeatingQC', 'KitchenQual', \n        'LandSlope', 'LotShape', 'MSSubClass', 'MoSold', 'OverallCond', 'PavedDrive', \n         'Street', 'YrSold']\n\nfor feature in cols:\n    encoder = LabelEncoder()\n    encoder.fit(all_data_cat[feature].values)\n    all_data_cat[feature] = encoder.transform(all_data_cat[feature].values)\n\n\nprint(f'Shape all_data_cat: {all_data_cat.shape}')\n\n# Same for all_data_free\n\ncols = ['CentralAir', 'ExterCond', 'ExterQual', 'FireplaceQu', 'Functional', \n        'HeatingQC', 'KitchenQual', 'LandSlope', 'LotShape', 'MSSubClass', \n        'MoSold', 'OverallCond', 'PavedDrive', 'Street', 'YrSold']\n\nfor feature in cols:\n    encoder = LabelEncoder()\n    encoder.fit(all_data_free[feature].values)\n    all_data_free[feature] = encoder.transform(all_data_free[feature].values)\n\n\n\nprint(f'Shape all_data_free: {all_data_free.shape}')","09b75e74":"all_data_cat['TotalSF'] = all_data_cat['TotalBsmtSF'] + all_data_cat['1stFlrSF'] + all_data_cat['2ndFlrSF']\n\nall_data_free['TotalSF'] = all_data_free['TotalBsmtSF'] + all_data_free['1stFlrSF'] + all_data_free['2ndFlrSF']","3b0010a9":"# all_data_cat\n\nnumeric_feature_cat = all_data_cat.select_dtypes(\"number\").columns\n\n# Check the skew of all numerical features\nskewed_feats_cat = all_data_cat[numeric_feature_cat].apply(lambda x: skew(x.dropna())).sort_values(ascending=False)\nprint(\"\\nSkew in numerical features: \\n\")\nskewness_cat = pd.DataFrame({'Skew' :skewed_feats_cat})\nskewness_cat.head(10)","cfcd39ef":"# all_data_free\n\nnumeric_feature_free = all_data_free.select_dtypes(\"number\").columns\n\n# Check the skew of all numerical features\nskewed_feats_free = all_data_free[numeric_feature_free].apply(lambda x: skew(x.dropna())).sort_values(ascending=False)\nprint(\"\\nSkew in numerical features: \\n\")\nskewness_free = pd.DataFrame({'Skew' :skewed_feats_free})\nskewness_free.head(10)\n","4c57de9c":"# for all_data_cat\n\nfrom scipy.special import boxcox1p\n\nskewness_cat = skewness_cat[abs(skewness_cat) > 0.75]\nprint(\"There are {} skewed numerical features to Box Cox transform\".format(skewness_cat.shape[0]))\n\nskewed_features_cat = skewness_cat.index\nlam = 0.15\nfor feat in skewed_features_cat:\n    all_data_cat[feat] = boxcox1p(all_data_cat[feat], lam)","cb9be9ab":"# all_data_free\n\nskewness_free = skewness_free[abs(skewness_free) > 0.75]\nprint(\"There are {} skewed numerical features to Box Cox transform\".format(skewness_free.shape[0]))\n\nskewed_features_free = skewness_free.index\nlam = 0.15\nfor feat in skewed_features_free:\n    #all_data[feat] += 1\n    all_data_free[feat] = boxcox1p(all_data_free[feat], lam)\n ","b048ee3a":"print(f\"all_data_cat.shape: {all_data_cat.shape}\")\nprint(f\"all_data_free.shape: {all_data_free.shape}\")","6c009ba9":"all_data_free.head()","93224f36":"# apply min max scaler first","350eff26":"all_data_cat = pd.get_dummies(all_data_cat)\nprint(all_data_cat.shape)","5d3eb84d":"all_data_free = pd.get_dummies(all_data_free)\nprint(all_data_free.shape)","d230baa5":"all_data_free[numeric_feature_free].head()","2da9f923":"train_cat = all_data_cat[:ntrain]\ntest_cat = all_data_cat[ntrain:]","1c75d724":"train_free = all_data_free[:ntrain]\ntest_free = all_data_free[ntrain:]","a4a93fa9":"train_cat.shape, test_cat.shape","4209e1f3":"train_free.shape, test_free.shape","ce3822b4":"# Using MinMaxScaler","c59d1735":"X_cat = train_cat.copy()\ncols_cat = X_cat.columns\n\nscaler_cat = MinMaxScaler()\n\n# fitting MinaMaxScaler to training data\nscaler_cat.fit(X_cat)\n\n# transforming training and test data\nX_cat = scaler_cat.transform(X_cat)\ntest_cat = scaler_cat.transform(test_cat)\n\n\nX_cat = pd.DataFrame(X_cat, columns=[cols_cat])\ntest_cat = pd.DataFrame(test_cat, columns=[cols_cat])\nX_cat.shape, test_cat.shape","c6e773ea":"X_free = train_free.copy()\ncols_free = X_free.columns\n\nscaler_free = MinMaxScaler()\n\n# fitting MinaMaxScaler to training data\nscaler_free.fit(X_free)\n\n# transforming training and test data\nX_free = scaler_free.transform(X_free)\ntest_free = scaler_free.transform(test_free)\n\n\nX_free = pd.DataFrame(X_free, columns=[cols_free])\ntest_free = pd.DataFrame(test_free, columns=[cols_free])\nX_free.shape, test_free.shape","345a604f":"y = y_train","acab9459":"# Using SelectFromModel with lasso for selecting best features","72f60210":"# for X_cat\nlasso = Pipeline([\n    (\"scaler\", RobustScaler()), \n    (\"ls\", Lasso(alpha =0.0005, random_state=1))\n])\n\nfeature_sel_model = SelectFromModel(lasso).fit(X_cat, y)\n\ncoefficiets = feature_sel_model.estimator_['ls'].coef_\nX_cat_cols = []\n\nfor i, j in enumerate(coefficiets):\n    if j != 0:\n        X_cat_cols.append(X_cat.columns[i])\n\n\n# selected_feat_cat = X_cat.columns[(feature_sel_model.get_support())]\nprint(len(X_cat_cols))\n\nX_cat_lasso = X_cat[X_cat_cols].reset_index(drop=True)\ntest_cat_lasso = test_cat[X_cat_cols].reset_index(drop=True)\n\nprint(X_cat_lasso.shape, test_cat_lasso.shape)","59745932":"# for X_free\nlasso = Pipeline([\n    (\"scaler\", RobustScaler()), \n    (\"ls\", Lasso(alpha =0.0005, random_state=1))\n])\n\nfeature_sel_model = SelectFromModel(lasso).fit(X_free, y)\n\ncoefficiets = feature_sel_model.estimator_['ls'].coef_\nX_free_cols = []\n\nfor i, j in enumerate(coefficiets):\n    if j != 0:\n        X_free_cols.append(X_free.columns[i])\n\n\nprint(len(X_free_cols))\n\nX_free_lasso = X_free[X_free_cols].reset_index(drop=True)\ntest_free_lasso = test_free[X_free_cols].reset_index(drop=True)\n\nprint(X_free_lasso.shape, test_free_lasso.shape)","912b6984":"tree_models = {\n    \"Light_GBM\": LGBMRegressor(objective='regression',num_leaves=5,\n                              learning_rate=0.05, n_estimators=1200,\n                              max_bin = 55, bagging_fraction = 0.8,\n                              bagging_freq = 5, feature_fraction = 0.2319,\n                              feature_fraction_seed=9, bagging_seed=9,\n                              min_data_in_leaf=6, min_sum_hessian_in_leaf = 11),\n    \n    \"XGBoost\": XGBRegressor(colsample_bytree=0.4603, gamma=0.0468, learning_rate=0.05, \n                 max_depth=5, min_child_weight=1.7817, n_estimators=2200,\n                 reg_alpha=0.4640, reg_lambda=0.8571, subsample=0.5213,\n                 nthread = -1, objective=\"reg:squarederror\", random_state=42), \n    \n    \"Gradient_boosting\": GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05, \n                                                   max_depth=6, min_samples_split=17, \n                                                   max_features='sqrt', min_samples_leaf=13, \n                                                   loss='huber', random_state=42),\n               \n    \"Extra_trees\": ExtraTreesRegressor(n_estimators=2000, max_depth=9, min_samples_split= 13, \n                        max_leaf_nodes=11, min_weight_fraction_leaf=0.39, max_features='sqrt', \n                        n_jobs=-1, random_state=42),\n}","00af6ea4":"# for X_cat\n\nprint(\"For X_cat - Extra Trees\")\nselection_extra_cat = SelectFromModel(tree_models[\"Extra_trees\"]).fit(X_cat, y_train)\n\nselected_feat_extra_cat = X_cat.columns[(selection_extra_cat.get_support())]\nprint(\"Number of selected features:\", len(selected_feat_extra_cat))\n\nX_cat_extra = X_cat[selected_feat_extra_cat].reset_index(drop=True)\ntest_cat_extra = test_cat[selected_feat_extra_cat].reset_index(drop=True)\nprint(\"Transformed shape: \", X_cat_extra.shape, test_cat_extra.shape)\n\n# uncomment next line to print selected features\n# print(X_cat_extra.columns)\n# for X_free\n\nprint(\"\\nFor X_free - Extra Trees\")\n\nselection_extra_free = SelectFromModel(tree_models[\"Extra_trees\"]).fit(X_free, y_train)\n\nselected_feat_extra_free = X_free.columns[(selection_extra_free.get_support())]\nprint(\"Number of selected features:\", len(selected_feat_extra_free))\n\nX_free_extra = X_free[selected_feat_extra_free].reset_index(drop=True)\ntest_free_extra = test_free[selected_feat_extra_free].reset_index(drop=True)\nprint(\"Transformed shape: \", X_free_extra.shape, test_free_extra.shape)\n\n# uncomment next line to print selected features\n# print(X_cat_extra.columns)","b11655be":"def fit_model(model_name, model, X, y, n_folds=7, show=10):\n    X = X.copy()\n    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(X.values)\n    output = cross_validate(model, X.values, y, scoring=\"neg_mean_squared_error\", cv=kf, return_estimator=True)\n    \n    feat_scores = pd.DataFrame(index=X.columns)\n\n    for idx,estimator in enumerate(output['estimator']):\n        temp = pd.Series(estimator.feature_importances_, index=X.columns)\n        feat_scores[str(idx)] = temp\n\n    feat_scores.reset_index(inplace=True)\n\n    feat_scores['Importance'] = feat_scores.mean(axis=1)\n\n    feat_scores = feat_scores.sort_values(by=\"Importance\", ascending=False)[['level_0', \"Importance\"]].head(show)\n\n    plt.figure(figsize=(15, 10))\n    plt.gca()\n    sns.barplot(x=feat_scores['Importance'], y=feat_scores['level_0'])\n    plt.ylabel(\"Features\")\n    plt.title(f\"Feature Importance: {model_name}\")\n\n    print(f\"{model_name} score => RMSE: {np.round(np.mean(np.sqrt(-output['test_score'])), 4)}, std: {np.round(np.std(-output['test_score']), 4)}\")\n    \n    plt.show()\n    print(\"-------------------------------------------------------------------------------------------------\")","6a39b6b1":"# print(\"Running boosting tree on X_cat_lasso, X_cat_free, X_cat, X_free\")\n\n# training_data = {\n#     \"X_cat_lasso\": X_cat_lasso, \"X_free_lasso\": X_free_lasso,\n#     \"X_cat\": X_cat, \"X_free\": X_free, \n# }\n\n\n# for X_name, X in training_data.items():\n#     print(f\"X => {X_name}, shape: {X.shape}\\n\")\n#     for model_name, model in tree_models.items():\n#         fit_model(model_name, model, X, y, n_folds=7, show=35)\n#     print()","e0514b9a":"from sklearn.ensemble import StackingRegressor\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.svm import LinearSVR\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.base import clone\n\ndef rmsle(y, y_pred):\n    return np.sqrt(mean_squared_error(y, y_pred))\n\nlevel0 = list()\n# level0.extend(('LGB', tree_models[\"Light_GBM\"]))\nlevel0.append(('xgb', tree_models[\"XGBoost\"]))\nlevel0.append(('GBT', tree_models[\"Gradient_boosting\"]))\nlevel0.append(('xtra', tree_models[\"Extra_trees\"]))\n\ntraining_data = {\"X_cat_lasso\": X_cat_lasso, \"X_free_lasso\": X_free_lasso,\n                 \"X_cat\": X_cat, \"X_free\": X_free, \n                 }\n\n\nlevel1_models = {\n    \"Linear\": LinearRegression(), \n#     \"Linear_SVR\":  LinearSVR(epsilon=1.5),\n#     \"MLP\": MLPRegressor(hidden_layer_sizes=(2), activation='logistic', solver='sgd', \n#                         max_iter=1000, learning_rate='adaptive', random_state=42)\n}","94af349d":"# To see outputs, uncomment the next lines \n\n# for X_name, X in training_data.items():\n#     print(\"==========================================\")\n#     X = X.copy()\n#     print(f\"X => {X_name}, shape: {X.shape}\\n\")\n\n#     for name, level1 in level1_models.items():\n#         level1_ = clone(level1)\n#         model_ = StackingRegressor(estimators=level0, final_estimator=level1_, cv=7, n_jobs=-1)\n#         model_.fit(X, y)\n#         y_preds = model_.predict(X)\n#         print(f\"Model: {name}, R2: {r2_score(y_preds, y)}, RMSE: {rmsle(y_preds, y)}\")","c50016f4":"model_ = StackingRegressor(estimators=level0, final_estimator=level1_models['Linear'], cv=7, n_jobs=-1)\nmodel_.fit(X_cat_lasso, y)\ny_preds = model_.predict(X_cat_lasso)\nprint(f\"Model: Linear, R2: {r2_score(y_preds, y)}, RMSE: {rmsle(y_preds, y)}\")","40b20524":"test_predictions = model_.predict(test_cat_lasso)\n\n\nsub = pd.DataFrame()\n\n\nsub['Id'] = test_id\nsub['SalePrice'] = test_predictions\nsub.to_csv('submission.csv',index=False)","bf5ea565":"# Outlier removal\n\nThere are two outliers present as seen previously in the plot of `GrlivArea` and `SalePrice`","e157b856":"**Appropriate missing values for numerical variables**\n* `LotFrontage`: groupby `Neighborhood` and fill the **median** value for that neighborhood.\n* `MasVnrArea` - **median** or **0** as missing value suggests a no mason veneer present\n*  `BsmtFinSF1`, `BsmtFinSF2`, `BsmtUnfSF`, `TotalBsmtSF`, `BsmtFullBath`, `BsmtHalfBath` - No basement present - **0**\n* `GarageYrBlt`, `GarageCars`, `GarageArea` - no garage present - **0**\n\nFor **BsmX** variables we can drop all variables except `TotalBsmtSF`.\n\nFor **GarageX** variable we can only keep `GarageCars` as it will also give us information about the `area`.\n\n\n","f51074a2":"# Date and Time Engineering\n\n\nWhat we could do is express the 3 year variables in term of time intervals by subracting their value from `YrSold` as done during EDA.\nWe can then replace the 3 variable with the one that shows the highest correlation with the `SalePrice`.","d27e2126":"# Final Run and Creating Submission","2b0f9f8e":"<h1>Table of Contents<span class=\"tocSkip\"><\/span><\/h1>\n<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Combining-train-and-test-to-apply-transformations.\" data-toc-modified-id=\"Combining-train-and-test-to-apply-transformations.-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;<\/span>Combining train and test to apply transformations.<\/a><\/span><\/li><li><span><a href=\"#Feature-Engineering\" data-toc-modified-id=\"Feature-Engineering-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;<\/span>Feature Engineering<\/a><\/span><\/li><li><span><a href=\"#Outlier-removal\" data-toc-modified-id=\"Outlier-removal-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;<\/span>Outlier removal<\/a><\/span><\/li><li><span><a href=\"#Log-Transformation-of-target-variable\" data-toc-modified-id=\"Log-Transformation-of-target-variable-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;<\/span>Log-Transformation of target variable<\/a><\/span><\/li><li><span><a href=\"#Treating-missing-values\" data-toc-modified-id=\"Treating-missing-values-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;<\/span>Treating missing values<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Numerical-features\" data-toc-modified-id=\"Numerical-features-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;<\/span>Numerical features<\/a><\/span><\/li><li><span><a href=\"#Categorical-features\" data-toc-modified-id=\"Categorical-features-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;<\/span>Categorical features<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#Date-and-Time-Engineering\" data-toc-modified-id=\"Date-and-Time-Engineering-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;<\/span>Date and Time Engineering<\/a><\/span><\/li><li><span><a href=\"#Variable-Transformation\" data-toc-modified-id=\"Variable-Transformation-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;<\/span>Variable Transformation<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Transforming-some-numerical-variables-that-are-really-categorical\" data-toc-modified-id=\"Transforming-some-numerical-variables-that-are-really-categorical-7.1\"><span class=\"toc-item-num\">7.1&nbsp;&nbsp;<\/span>Transforming some numerical variables that are really categorical<\/a><\/span><\/li><li><span><a href=\"#Label-Encoding-some-categorical-features\" data-toc-modified-id=\"Label-Encoding-some-categorical-features-7.2\"><span class=\"toc-item-num\">7.2&nbsp;&nbsp;<\/span>Label Encoding some categorical features<\/a><\/span><\/li><li><span><a href=\"#Skewed-feature-transformation\" data-toc-modified-id=\"Skewed-feature-transformation-7.3\"><span class=\"toc-item-num\">7.3&nbsp;&nbsp;<\/span>Skewed feature transformation<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#Feature-Selection\" data-toc-modified-id=\"Feature-Selection-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;<\/span>Feature Selection<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#using-Lasso\" data-toc-modified-id=\"using-Lasso-8.1\"><span class=\"toc-item-num\">8.1&nbsp;&nbsp;<\/span>using Lasso<\/a><\/span><\/li><li><span><a href=\"#Using-tree-based-models\" data-toc-modified-id=\"Using-tree-based-models-8.2\"><span class=\"toc-item-num\">8.2&nbsp;&nbsp;<\/span>Using tree based models<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#Model-Building\" data-toc-modified-id=\"Model-Building-9\"><span class=\"toc-item-num\">9&nbsp;&nbsp;<\/span>Model Building<\/a><\/span><\/li><li><span><a href=\"#Stacking-regressor\" data-toc-modified-id=\"Stacking-regressor-10\"><span class=\"toc-item-num\">10&nbsp;&nbsp;<\/span>Stacking regressor<\/a><\/span><\/li><li><span><a href=\"#Final-Run-and-Creating-Submission\" data-toc-modified-id=\"Final-Run-and-Creating-Submission-11\"><span class=\"toc-item-num\">11&nbsp;&nbsp;<\/span>Final Run and Creating Submission<\/a><\/span><\/li><\/ul><\/div>","9775defd":"# Log-Transformation of target variable\n\nWe preivously saw that SalePrice was heavily skewed, we would need to transform this variable","16833829":"**Box Cox Transformation of (highly) skewed features**\n\nWe use the scipy function **boxcox1p** which computes the Box-Cox transformation of  $1+x$ .\n\nNote that setting  $\u03bb=0$  is equivalent to $log1p$ used above for the target variable.\n\nSee [this](http:\/\/onlinestatbook.com\/2\/transformations\/box-cox.html) page for more details on Box Cox Transformation as well as [the scipy function's page](https:\/\/docs.scipy.org\/doc\/scipy\/reference\/generated\/scipy.special.boxcox1p.html)\n\n","e6954f54":"As suspected, `GarageCars` is highly correlated with `GarageArea` and also has the highest correlation with `SalePrice`","e94030be":"## Label Encoding some categorical features","7c7ce29d":"## Categorical features","9b518b89":"The mean and median values of these features are different so it will be better if we fill these values with **median.**\n\nWhat we can also do is create a new feature with binary values to indicate if the value was missing at that point","be6025f5":"**Appropriate missing values for numerical variables**\n* `FireplaceQu`: missing values indicates _No fireplace present_ - **None.**\n* `GarageType`, `GarageFinish`, `GarageQual` & `GarageCond`: missing value indicates there's _no garage_ - **None.**\n* `BsmtQual`, `BsmtCond`, `BsmtExposure`, `BsmtFinType1` & `BsmtFinType2`: missing values indicate there's _no basement_ - **None.**\n* `MasVnrType`: missing value indicates there' _no masonry veneer_ - **None.**\n* `MSZoning`: missing value can be filled by the most frequent value _RL_ - **mode.**\n* `Functional`: from description _NA_ indicates - **Typical.**\n* `Utilities`: feature can be **dropped** is heavily dominated by _AllPub_. I was wrong in determining it to be important in the EDA part.\n* `Electrical`: 1 missing value, fill with most frequent value - **mode.**\n* `KitchenQual`: 1 missing value, fill with most frequent value - **mode.**\n* `Exterior1st` & `Exterior2nd`: 1 missing values in both, fill with most frequent value - **mode.**\n* `SaleType`: 1 missing value, fill with most frequent value - **mode.**\n\n\nAlthough, I can cannot provide the proof of it but `BsmtX` and  `GarageX` variables are mostly likely heavily correlated with each other, we can keep their numerical counterparts that convey the same information.\n\nWe can either drop these variables or keep them to see the result during feature selection.\n \nIn the future version, I'll update the notebook with methods to find the categorical coefficient between these variables and with the output to test my hypothesis above.","a218b2e0":"Running the next cell will for each **X_set** created perform a cross validation run using all tree based model, plot best 30 features that are most important.","0ec4b7e6":"## Using tree based models\n\n","356d076a":"We came down from 191 features to 77","a6b88632":"As suspected, `TotalBsmtSF` seems the only good choice to keep, also `BsmtFinSF1` can be kept if required. We can perform a feature selection on the features remaining at the end, to filter some more","75eba45e":"In this notebook we'll build on the insights we gattered from the first kernel.\nWe'll dive into:\n1. Outlier Engineering\n1. Missing Data Imputation\n2. Variable Transformation\n3. Date and Time Engineering\n4. Categorical Encoding\n5. Feature Creation (if necessary)","a729a24a":"We came down from 206 features to 83","ce05c948":"Demo: feature selection using **extra trees**","c6fc7d91":"## Transforming some numerical variables that are really categorical","f38e47ac":"Concatenate the train and test data in the same dataframe.","af437c83":"This is the second notebook on this project. This kernel will focus on **Feature Engineering**, **Feature Selection** and **Model Building**.\n\nFirst part of the Project: [Project 2 P1: EDA](https:\/\/www.kaggle.com\/veb101\/project-2-p1-eda)","b500aeac":"# Variable Transformation","7ff0371e":"# Feature Engineering\n\nFeature engineering is the process of using domain knowledge of the data to transform existing features or to create new variables from existing ones, for use in machine learning.\n\n[feature engineering](https:\/\/www.trainindata.com\/post\/feature-engineering-comprehensive-overview)","f8d9cc01":"_____________________________","e3fd1588":"**Private Run**\n\n\n1. X => X_cat_lasso, shape: (1458, 83)\n    * Model: Linear, R2: 0.9851682263983834, RMSE: 0.04847376103391416\n    * Model: Linear_SVR, R2: -6.633282458849109, RMSE: 0.7302971301321612\n    * Model: MLP, R2: -2983940.9730241755, RMSE: 0.39935765079237545\n    \n\n2. X => X_free_lasso, shape: (1458, 77)\n    * Model: Linear, R2: 0.981016025855836, RMSE: 0.05452379484010679\n    * Model: Linear_SVR, R2: -6.461431642367199, RMSE: 0.7167880791589558\n    * Model: MLP, R2: -2982300.610854054, RMSE: 0.39935750649014945\n\n\n3. X => X_cat, shape: (1458, 206)\n    * Model: Linear, R2: 0.9818263620627218, RMSE: 0.053600087870622565\n    * Model: Linear_SVR, R2: -6.812972415367375, RMSE: 0.7336647698772716\n    * Model: MLP, R2: -2964776.4375035004, RMSE: 0.3993568326589984\n\n\n4. X => X_free, shape: (1458, 191)\n    * Model: Linear, R2: 0.9825960824103169, RMSE: 0.05235665694025279\n    * Model: Linear_SVR, R2: -6.6057353381424395, RMSE: 0.7214971271278083\n    * Model: MLP, R2: -2920872.063956515, RMSE: 0.3993558505344274","dc602083":"# Feature Selection","a66b5b49":"# Stacking regressor","7f89f0e9":"* We can built a Pipeline of _RobustScaler_ , _tree_based_ selection and _Final model_ to see how they work.\n* If time permits I'll add this in future versions.\n* For now we'll directly use a tree based models to fit and predict with cross-validation.\n","e238cbe4":"## Numerical features","8ec94187":"large area but still small SalePrice","6f7acf5e":"# Model Building\n\nWe'll run a 7-Fold cross validation of **X_cat** and **X_free** to determine which if _BsmtX_ and _GarageX_ variables removed have any effect on the prediction. \n\nAlso we'll \n\nWe'll train our data on 4 Boosting trees algorithms:\n1. XGBoost\n2. Gradient Boosting trees\n3. Light GBM\n4. Extra trees\n","9507a735":"\n\n---\n\n","96dcec31":"# Combining train and test to apply transformations.\n","b12ebb86":"# Treating missing values","6307be4f":"## Skewed feature transformation\n\nDuring EDA we saw many continuous numeric features that were skewed. In this section we'll apply a Box-Cox transformation on them.","ad6c1086":"What we can do is keep two seperate dataframe \n1. Consisting of categorical `GarageX` and `BsmtX` variables.\n2. One Without.","ebaa5efd":"## using Lasso","e840dac5":"* We will drop features `PoolQC`, `MiscFeature`, `Alley` and `Fence`, `Utilities`.\n\n","68f2ca7c":"Candidate features to be dropped:\n* `BsmtFinSF2`\n* `BsmtUnfSF`\n* `BsmtFullBath`\n* `BsmtHalfBath`\n* `GarageYrBuilt`\n* `GarageArea`","b436c07f":"log transforming the feature to remove skewness","62f32a5f":"According to various top kernels total square footage might be an importatnt feature. Giving it a moments thought we can see why.","be73ef83":"Before starting I want to thank other kaggle users for their work on this problem. It helped me alot in understanding this problem.\n\nThis and others notebooks onn this project series relies heavily on other great kernels made on this dataset.\nNaming a few:\n1. [Comprehensive data exploration with Python](https:\/\/www.kaggle.com\/pmarcelino\/comprehensive-data-exploration-with-python)\n2. [A study on Regression applied to the Ames dataset](https:\/\/www.kaggle.com\/juliencs\/a-study-on-regression-applied-to-the-ames-dataset)\n3. [Eda and prediction of House Price](https:\/\/www.kaggle.com\/siddheshpujari\/eda-and-prediction-of-house-price)\n3. [Stacked Regressions to predict House Prices](https:\/\/www.kaggle.com\/serigne\/stacked-regressions-top-4-on-leaderboard)\n4. [Regularized Linear Models](https:\/\/www.kaggle.com\/apapiu\/regularized-linear-models)"}}