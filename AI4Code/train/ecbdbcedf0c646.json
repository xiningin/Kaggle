{"cell_type":{"9b153a6a":"code","a2a2d861":"code","1e1729c8":"code","0375f0da":"code","5c6d9141":"code","b300d891":"code","2ddd2242":"code","2708e7ae":"code","9a648044":"code","091fcd10":"code","43d7d482":"code","2c021272":"code","c17c53ae":"code","21c1dea4":"code","da2c2b41":"code","eaf107a9":"code","ac7b1d11":"code","b76617db":"code","12d23db3":"code","feba08c6":"code","08848cf9":"code","77834994":"code","9ce503b6":"code","4dd868f7":"code","2cabf6b7":"markdown","4fb0e280":"markdown","a47bf0dc":"markdown","11a89ccc":"markdown","5a50af9d":"markdown","7a2965bf":"markdown","ca7de7f4":"markdown","b48a4b4d":"markdown","282fd46a":"markdown","9d262a54":"markdown","b98c5284":"markdown","a7931494":"markdown","3b27f9c3":"markdown","45e93d2c":"markdown","0fe87cf3":"markdown","e0156b2c":"markdown","9828993a":"markdown","6754ed6b":"markdown"},"source":{"9b153a6a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.\n\nimport seaborn as sns\n%matplotlib inline","a2a2d861":"df_train = pd.read_csv('..\/input\/train.csv')\ndf_test = pd.read_csv('..\/input\/test.csv')","1e1729c8":"df_train['train'] = 1\ndf_test['train'] = 0\n\ndf = pd.concat([df_test, df_train])\n\n\nsurvived = df_train['Survived'].copy()\ndf = df.drop('Survived', axis = 1)","0375f0da":"df_train.head()","5c6d9141":"df_train.describe()","b300d891":"df['FamilySize'] = df['SibSp'] + df['Parch'] + 1\n\ndf['Name_length'] = df['Name'].apply(len)\n\ndf['IsAlone'] = 0\n\ndf.loc[df['FamilySize'] == 1, 'IsAlone'] = 1\n\ndf['Title'] = 0\n\ndf['Title'] = df.Name.str.extract('([A-Za-z]+)\\.') \n\ndf['Title'].replace(['Mlle','Mme','Ms','Dr','Major','Lady','Countess','Jonkheer','Col',\n                         'Rev','Capt','Sir','Don'], ['Miss','Miss','Miss','Mr','Mr','Mrs','Mrs','Other','Other','Other','Mr','Mr','Mr'], inplace = True)","2ddd2242":"df.loc[(df.Age.isnull())&(df.Title=='Mr'),'Age']= df.Age[df.Title==\"Mr\"].mean()\n\ndf.loc[(df.Age.isnull())&(df.Title=='Mrs'),'Age']= df.Age[df.Title==\"Mrs\"].mean()\n\ndf.loc[(df.Age.isnull())&(df.Title=='Master'),'Age']= df.Age[df.Title==\"Master\"].mean()\n\ndf.loc[(df.Age.isnull())&(df.Title=='Miss'),'Age']= df.Age[df.Title==\"Miss\"].mean()\n\ndf.loc[(df.Age.isnull())&(df.Title=='Other'),'Age']= df.Age[df.Title==\"Other\"].mean()\n\ndf = df.drop('Name', axis=1)\n\ndf['Embarked'] = df['Embarked'].fillna(df['Embarked'].mode().iloc[0])\n\ndf['Fare'] = df['Fare'].fillna(df['Fare'].mean())","2708e7ae":"df['Sex'] = df['Sex'].map( {'female': 1, 'male': 0} ).astype(int)\ndf['Embarked'] = df['Embarked'].map( {'Q': 0, 'S': 1, 'C': 2} ).astype(int)\ndf= df.drop(['Ticket', 'Cabin'], axis=1)\ndf['Title'] = pd.Categorical(df['Title'])\ndf['Title'] = df['Title'].cat.codes","9a648044":"df_train = df[df['train'] == 1]\ndf_test = df[df['train'] == 0]\ndf_train.drop('train', axis=1, inplace=True)\ndf_test.drop('train', axis=1, inplace=True)","091fcd10":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\ndf_train['Survived'] = survived\nX = df_train.drop(columns=[\"Survived\"]).values\ntrain_x = scaler.fit_transform(X)\ntrain_y = df_train[\"Survived\"].values\ntest = scaler.fit_transform(df_test.values)","43d7d482":"from sklearn.model_selection import RandomizedSearchCV\n\n# Number of trees in random forest\nn_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n# Number of features to consider at every split\nmax_features = ['auto', 'sqrt']\n# Maximum number of levels in tree\nmax_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\nmax_depth.append(None)\n# Minimum number of samples required to split a node\nmin_samples_split = [2, 5, 10]\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = [1, 2, 4]\n# Method of selecting samples for training each tree\nbootstrap = [True, False]\n\n# Create the grid\nrandom_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf,\n               'bootstrap': bootstrap}","2c021272":"from sklearn.ensemble import RandomForestClassifier\n\n# First create the base model to tune\nrf = RandomForestClassifier()\n\n# Random search of parameters, using 5 fold cross validation, \n# search across 100 different combinations, and use all available cores\nrf_clf = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100, cv = 5, \n                            random_state=42, n_jobs = -1)\n\nrf_clf.fit(train_x, train_y)","c17c53ae":"clf = rf_clf.best_estimator_","21c1dea4":"preds_rf = clf.predict(test)","da2c2b41":"rf_submit = pd.DataFrame({\"Survived\": preds_rf,\"PassengerId\": df_test.PassengerId})","eaf107a9":"rf_submit = rf_submit[[\"PassengerId\",\"Survived\"]]","ac7b1d11":"rf_submit.to_csv(\"submission_rf.csv\",index=False)","b76617db":"from xgboost import XGBClassifier\nfrom sklearn.model_selection import GridSearchCV\n\nxgb_model = XGBClassifier()\n\nparam_grid = {'n_estimators':[240,280,320],\n              'max_depth':[10,11,12],\n              'gamma':[0,1,2,3],\n              'max_delta_step':[0,1,2],\n              'min_child_weight':[1,2,3], \n              'colsample_bytree':[0.55,0.6,0.65],\n              'learning_rate':[0.1,0.2,0.3]\n            }\nxgb_clf = GridSearchCV(xgb_model, param_grid = param_grid, scoring = 'neg_log_loss', error_score = 0,  n_jobs = -1)\n\nxgb_clf.fit(train_x, train_y)","12d23db3":"preds_xgb = xgb_clf.predict(test)","feba08c6":"xgb_submit = pd.DataFrame({\"Survived\": preds_xgb,\"PassengerId\": df_test.PassengerId})\nxgb_submit = xgb_submit[[\"PassengerId\",\"Survived\"]]\nxgb_submit.to_csv(\"submission_xgb.csv\",index=False)","08848cf9":"from sklearn.ensemble import VotingClassifier\n\ndef get_voting(models):\n    ensemble = VotingClassifier(estimators=models, voting='soft')\n    return ensemble","77834994":"models = [('xgb', xgb_clf.best_estimator_),\n          ('rf', rf_clf.best_estimator_)]","9ce503b6":"voting_ens_model = get_voting(models)\nvoting_ens_model.fit(train_x, train_y)\nvoting_preds = voting_ens_model.predict(test)\nvoting_submit = pd.DataFrame({\"Survived\": voting_preds,\"PassengerId\": df_test.PassengerId})\nvoting_submit = voting_submit[[\"PassengerId\",\"Survived\"]]\nvoting_submit.to_csv(\"submission_ensembled.csv\",index=False)","4dd868f7":"df_test.shape","2cabf6b7":"**Features Engineering**\n\nBased on these excellent notebooks: \n\n* https:\/\/www.kaggle.com\/nicapotato\/titanic-feature-engineering\n* https:\/\/www.kaggle.com\/thedruid\/titanic-eda-fe-and-logistic-regression-top-20","4fb0e280":"Test accuracy is around 77.5% after submission","a47bf0dc":"Convert categoricals to numerics","11a89ccc":"To do so, let's set up a parameter grid to sample:","5a50af9d":"Areas of Improvement:\n\n* IsAlone (1 if passenger is alone and 0 if passenger is not) could be included in features engineering\n\n* Cross-Validation could have been performed across different models such as RandomForest, AdaBoost, Decision Tree\n\n* Ensembling could also be explored to boost accuracy score.","7a2965bf":"## Ensembling:\n---\n\nLet's do ensembling via voting ensemble.\n\nWhat is voting ensemble?\n* A voting ensemble involves making a prediction that is the average of multiple other classification models.\n\n* There are 2 types of voting ensemble, a hard voting ensemble and soft voting ensemble\n\n* Hard Voting - Predict the class with the largest sum of votes from models\n\n![image.png](attachment:image.png)","ca7de7f4":"Impute Missing Values","b48a4b4d":"Let's start our training with a random forest model!\n\nCredits to: https:\/\/towardsdatascience.com\/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74 for its guidance towards cross-validation for a random forest model.\n","282fd46a":"Let's try adjusting these hyperparameters:\n\n1) n_estimators = number of trees in the foreset\n\n2) max_features = max number of features considered for splitting a node\n\n3) max_depth = max number of levels in each decision tree\n\n4) min_samples_split = min number of data points placed in a node before the node is split\n\n5) min_samples_leaf = min number of data points allowed in a leaf node\n\n6) bootstrap = method for sampling data points (with or without replacement)","9d262a54":"XGBoost improved test accuracy to ~78.9%","b98c5284":"Due Credits to:\n\nTitanic Best Working Classfier, authored by Sina, for comprehensive features engineering \n\nCheck out that kernel here: https:\/\/www.kaggle.com\/sinakhorami\/titanic\/titanic-best-working-classifier\n","a7931494":"* Soft Voting - Predict the class with the largest summed probability from models.\n\n![image.png](attachment:image.png)\n","3b27f9c3":"The benefit of a random search is that we are not trying every combination, but selecting at random to sample a wide range of values.","45e93d2c":"Next model that can be train is an XGBoost Classifier\n\nTo save time, let's plug in best hyperparameters used by fellow competitors that performed CV:\n\n","0fe87cf3":"The most important settings for a random forest are the number of trees in the forest (n_estimators) and the number of features considered for splitting at each leaf node (max_features)","e0156b2c":"**Training our models:**","9828993a":"Concatenate train and test","6754ed6b":"We all get the idea of Titanic. It has been in books, movies, documentaries and etc. Ship hit iceberg, met its demise. But could there be more to the story? What about the survivors of Titanic? Are there properties that certain passengers had that helped in surviving the crash? This notebook would look deeper beyond the tip of the ice berg to investigate important attributes that could have boosted survival rates.\n\nThis notebook provides steps to perform feature engineering, cross-validation, model fitting and finally ensembling via vote ensemble.\n\nOf course, any further suggestions for improvement is always welcomed. Feel free to contact @: \nalanchn31@gmail.com"}}