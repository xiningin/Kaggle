{"cell_type":{"4ec0b765":"code","a011a1db":"code","e0c35c3b":"code","bfc3569a":"code","7c66d9a4":"code","fba07c64":"code","4b0ad889":"code","71faa2b3":"code","a07274a4":"code","9d426ce6":"code","1b9b12dd":"code","674dcdf3":"code","c04de083":"code","e6583093":"code","fa0c690f":"code","cef1ab9b":"code","78c0fbca":"code","cdcbe689":"code","d9da7cc2":"code","c7facd49":"code","6cdd74ae":"code","958fad78":"code","fd42f9df":"code","739b1f98":"code","1f246b0f":"code","6c84d7b5":"code","e2c01099":"markdown","80bb0d6f":"markdown","7da5d2fe":"markdown","b3c0d29d":"markdown","bfe6486d":"markdown","3c09abb0":"markdown","0f3b00be":"markdown","de400fcb":"markdown","f314bcad":"markdown","409a9c1d":"markdown","fab49886":"markdown","0a0a9f17":"markdown","2646a934":"markdown","eabaf7cb":"markdown","956ed432":"markdown","755cff26":"markdown","37310b5f":"markdown"},"source":{"4ec0b765":"import pandas as pd\nimport torch\nfrom sklearn.model_selection import train_test_split\nimport numpy as np\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","a011a1db":"df = pd.read_csv(\"..\/input\/weather-dataset-rattle-package\/weatherAUS.csv\")","e0c35c3b":"df = df[['Rainfall','Humidity3pm','RainToday','Pressure9am','RainTomorrow']]\ndf = df.dropna(how = 'any')\ndf.shape","bfc3569a":"df.isna().sum()","7c66d9a4":"df.RainToday[df.RainToday == 'Yes'] = 1 \ndf.RainToday[df.RainToday == 'No'] = 0\ndf.RainToday = pd.to_numeric(df.RainToday)\ndf.RainTomorrow[df.RainTomorrow == 'Yes'] = 1\ndf.RainTomorrow[df.RainTomorrow == 'No'] = 0\ndf.RainTomorrow = pd.to_numeric(df.RainTomorrow)\ndf.info()","fba07c64":"Y, X = df[['RainTomorrow']], df.drop('RainTomorrow', axis = 1, inplace = False)\n\nXtrain, Xtest, Ytrain, Ytest = train_test_split(X, Y, test_size=0.2, random_state=2)","4b0ad889":"# input must be numeric\nXtrain = torch.from_numpy(Xtrain.to_numpy()).float()\nXtest = torch.from_numpy(Xtest.to_numpy()).float()\nYtrain = torch.from_numpy(Ytrain.to_numpy()).float()\nYtest = torch.squeeze(torch.from_numpy(Ytest.to_numpy()).float())\n\nprint(Xtrain.shape, Xtest.shape)\nprint(Ytrain.shape, Ytest.shape)\n\nprint(\"Ytrain give one extra dimention ie \",Ytrain.shape)","71faa2b3":"print(Ytrain, Ytest)\n\nprint(\"So use squeeze funtion to remove the extra demention!!! \")\n\nYtrain = torch.squeeze(Ytrain)\nprint(Ytrain.shape)","a07274a4":"class Net(nn.Module):    # class Net will be the subclass of torch.nn.Module ie Class Net --EXTENDS--> Class nn.Module\n    def __init__(self, n_features):    # initialize the layers you want to use in this function\/method\n        super(Net, self).__init__()    # call to init method of superclass ie nn.Module\n        self.fc1 = nn.Linear(n_features, 5)    # Input Layer of n_features input nodes to 5 outputs\n        self.fc2 = nn.Linear(5, 3)             # 1st Hidden Layer of 5 nodes to 3 outputs\n        self.fc3 = nn.Linear(3, 1)             # 2st Hidden Layer of 3 nodes to 1 output\n    \n    def forward(self, x):              # Feed Forward\n        x = F.relu(self.fc1(x))        # torch.nn.functional.relu() ie a Activation Function \n        x = F.relu(self.fc2(x))\n        return torch.sigmoid(self.fc3(x))  # Output sigmoid activation function","9d426ce6":"net = Net(Xtrain.shape[1])\nnet","1b9b12dd":"criterion = nn.BCELoss()\nprint(net.parameters())","674dcdf3":"optimizer = torch.optim.Adam(net.parameters(), lr = 0.001)   # lr: learning rate for the model","c04de083":"device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\nprint(\"Device Selected: \", device)\n\nif device.type == 'cuda':\n    print(\"Device Detailes: \", torch.cuda.get_device_properties(0))","e6583093":"# Set all the tensors to gpu device if cuda is enable\nXtrain = Xtrain.to(device)\nXtest = Xtest.to(device)\nYtrain = Ytrain.to(device)\nYtest = Ytest.to(device)\n\n# Now set our Net model and criterion to gpu device\nnet = net.to(device)\ncriterion = criterion.to(device)","fa0c690f":"# Check if the abouve tensors are loaded on the GPU memory or not\nif 'cuda' in device.type:\n    print(\"While runing this notebook my gpu memory was 16280 MiB\")\n    print(\"and almost 700 to 900 Mib has been taken by above tensors ie Xtrain, Xtest, Ytrain and Ytest\\n\")\n    \n    !nvidia-smi\n\n    print(\"So this shows that our gpu works fine with torch!\")\n\nelse: print(\"This block wont run if you donot have gpu!!!\")","cef1ab9b":"def calculate_accuracy(y_true, y_pred):\n    predicted = y_pred.ge(.5).view(-1)\n    return (y_true == predicted).sum().float() \/ len(y_true)\n\ndef round_tensor(t, decimal_places = 6):\n    return round(t.item(), decimal_places)\n\nfor epoch in range(0, 1000):\n    y_pred = net(Xtrain)\n    \n    y_pred = torch.squeeze(y_pred)\n    train_loss = criterion(y_pred, Ytrain)\n    \n    if epoch % 100 == 0:\n        train_acc = calculate_accuracy(Ytrain, y_pred)\n\n        y_test_pred = net(Xtest)\n        y_test_pred = torch.squeeze(y_test_pred)\n\n        test_loss = criterion(y_test_pred, Ytest)\n\n        test_acc = calculate_accuracy(Ytest, y_test_pred)\n        print(f'''epoch {epoch}\n            Train set - loss: {round_tensor(train_loss)}, accuracy: {round_tensor(train_acc)}\n            Test  set - loss: {round_tensor(test_loss)}, accuracy: {round_tensor(test_acc)}''')\n        \n    optimizer.zero_grad()\n    train_loss.backward()\n    optimizer.step()","78c0fbca":"try:\n    5 \/ 0\nexcept Exception as e: print(e)","cdcbe689":"MODEL_PATH = 'model1.pth'\n\ntorch.save(net, MODEL_PATH)","d9da7cc2":"net = torch.load(MODEL_PATH)","c7facd49":"classes = ['No rain', 'Rain']\n\ny_pred = net(Xtest)\ny_pred","6cdd74ae":"# Convert the probabilities to binary classes ie (1 or 0) and (True or False) by the help of threshold values\ny_pred = y_pred.ge(.5).view(-1).cpu()\ny_pred","958fad78":"Ytest = Ytest.cpu()","fd42f9df":"from sklearn.metrics import classification_report\n\nprint(classification_report(Ytest, y_pred, target_names = classes))","739b1f98":"def will_it_rain(rainfall, humidity, rain_today, pressure):\n    t = torch.as_tensor([rainfall, humidity, rain_today, pressure]).float().to(device)\n    output = net(t)\n    return output.ge(.5).item()\n\nprint(\"A Function is defined to make single or bulk predictions which can be used for the real world productions too!\")","1f246b0f":"# Try 1\nwill_it_rain(10, 10, True, 2)","6c84d7b5":"# Try 2\nwill_it_rain(0, 1, False, 100)","e2c01099":"In PyTorch you define your Models as subclasses of **torch.nn.Module.**\n\nIn the **__init__** function, you are supposed to initialize the layers you want to use. Unlike keras, Pytorch goes more low level and you have to specify the sizes of your network so that everything matches.\n\nIn the forward method, you specify the connections of your layers. This means that you will use the layers you already initialized, in order to re-use the same layer for each forward pass of data you make.\n\ntorch.nn.Functional contains some useful functions like activation functions a convolution operations you can use. However, these are not full layers so if you want to specify a layer of any kind you should use torch.nn.Module.\n\nYou would use the torch.nn.Functional conv operations to define a custom layer for example with a convolution operation, but not to define a standard convolution layer.\n","80bb0d6f":"**Activation Functions:** \nIn neural networks, activation functions perform a transformation on a weighted sum of inputs plus biases to a neuron in order to compute its output.\nIn simple words Activation functions are mathematical equations that determine the output of a neural network. The function is attached to each neuron in the network, and determines whether it should be activated (\u201cfired\u201d) or not, based on whether each neuron's input is relevant for the model's prediction.\n\nA Detailed Separate Notebook will be provided by me on this topic!!!","7da5d2fe":"# Loss Functions\n\nMachines learn by means of a loss function. It\u2019s a method of evaluating how well specific algorithm models the given data. If predictions deviates too much from actual results, loss function would cough up a very large number. \n\ntorch.nn.BCELoss() Creates a criterion that measures the Binary Cross Entropy between the target and the output.\n\nSeparate Notebook will be provided on this topic soon!!!\n\nGradually, with the help of some optimization function, loss function learns to reduce the error in prediction.","b3c0d29d":"# Neural Network Model Building","bfe6486d":"# Quick data preprocessing","3c09abb0":"## References \nhttps:\/\/youtu.be\/O2ZNjOBPF4M\n\nhttps:\/\/pytorch.org\/docs\/stable\/nn.html#\n\nhttps:\/\/github.com\/curiousily\/Getting-Things-Done-with-Pytorch\/blob\/master\/04.first-neural-network.ipynb","0f3b00be":"# Our Neural Network with code reference\n\n![neural%20network.png](attachment:neural%20network.png)","de400fcb":"# Making Predictions in terms of production","f314bcad":"**Note:** The data used in this notebook is tabular data with shape -> (124689, 5) ","409a9c1d":"# Saving and Loading of Pytorch Model","fab49886":"# Optimizers\n\nRather than manually updating the weights of the model as we have been doing, we use the optim package to define an Optimizer that will update the weights for us. The optim package defines many optimization algorithms that are commonly used for deep learning, including SGD+momentum, RMSProp, Adam, etc.\n\nMore detailed notebook will be provided on this soon!!!","0a0a9f17":"# Dataset Formation (Train-Test) in Tensors","2646a934":"# Evaluation of the model","eabaf7cb":"# Forecasting \/ Predictions by our NN model","956ed432":"# Neural Networks in Pytorch","755cff26":"**Neural Network:** A neural network is a series of algorithms that endeavors to recognize underlying relationships in a set of data through a process that mimics the way the human brain operates.\n\nNeural networks are composed of various components like an input layer, hidden layers, an output layer and many nodes in each layer.\n\n![image.png](attachment:image.png)\n\n**Simple Neural Network:** We call a neural net Simple\/Shallow when there is only one Hidden layer between input and output layers.\n\n**Deep Neural Network(DNN):** As opposed to that we call a network deep only when the number of Hidden layers are 2 or more.\n\nThe main explanation is that the deep models are able to extract\/build better features than shallow models and to achieve this they make use of many intermediate hidden layers.\n\nFor more explaination on this click [here](https:\/\/www.quora.com\/How-does-deep-learning-work-and-how-is-it-different-from-normal-neural-networks-applied-with-SVM-How-does-one-go-about-starting-to-understand-them-papers-blogs-articles)","37310b5f":"# Things to run the model on **GPU** or Default CPU"}}