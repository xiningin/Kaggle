{"cell_type":{"40e1e042":"code","a640149c":"code","7e0ac73b":"code","f74699d4":"code","665ac7c3":"code","8af840ff":"code","a7d065ea":"code","06b47574":"code","bd7997fd":"code","463611e3":"code","d8eb8450":"code","586d8ea5":"code","7b0f6488":"code","8823c5a5":"code","2a0621d9":"code","d0c842f0":"code","b78f8ceb":"code","abcd9171":"code","6bd3cfae":"code","6866be61":"code","5a4b255b":"code","3163c8cb":"code","745f456b":"code","bf230396":"code","572e65dd":"code","dfe5215c":"code","e64b7847":"code","9ff647dc":"code","0a426ccf":"code","d7d6f23a":"code","d69f58be":"code","b2a857fa":"code","8ea27460":"code","2d7daec7":"code","876b1327":"code","3e7c3295":"code","f9267a6c":"code","1abb124e":"code","180b1554":"code","2b8c0553":"markdown","4e3c7e42":"markdown","80029d42":"markdown","04d4afbf":"markdown","1e576980":"markdown","b96b2c28":"markdown","650602cf":"markdown","f191e828":"markdown","f37beaf2":"markdown","6bc210bc":"markdown","369648af":"markdown","2fa61158":"markdown","2c565d82":"markdown"},"source":{"40e1e042":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.model_selection import train_test_split ,KFold, StratifiedKFold\nfrom sklearn.model_selection import cross_val_score, cross_validate, GridSearchCV\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score\nfrom sklearn.metrics import f1_score, confusion_matrix, precision_recall_curve, roc_curve\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder, Binarizer\nfrom sklearn.linear_model import LinearRegression, LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.tree import export_graphviz\nfrom sklearn.ensemble import RandomForestClassifier, RandomForestRegressor, VotingClassifier, GradientBoostingClassifier, AdaBoostClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\n\nfrom lightgbm import LGBMClassifier, plot_importance\n\nfrom imblearn.over_sampling import SMOTE\n\nimport graphviz","a640149c":"# Fetch data\ndata_train = pd.read_csv('..\/input\/loan-prediction-based-on-customer-behavior\/Training Data.csv')","7e0ac73b":"data_train.head()","f74699d4":"data_train.info()","665ac7c3":"data_train.isnull().sum()","8af840ff":"# Drop Unnecessary Columns\ndata_train.drop(['Id', 'CITY', 'STATE'], axis=1, inplace=True)\ndata_train","a7d065ea":"# Extract X for feature dataset, y for label dataset\nX = data_train.iloc[:, :-1]\ny = data_train.iloc[:, -1]","06b47574":"# # ML Algorithm cannot fit featues which contains characters\n# # Therefore, we should encode them into numbers\n# For applying various algorithms, it could be a good idea to select One-Hot Encoding\nX = pd.get_dummies(X)\nX","bd7997fd":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=11, stratify=y)","463611e3":"# Utility Function\ndef get_clf_eval(y_test, pred=None, pred_proba=None):\n    confusion = confusion_matrix(y_test, pred)\n    accuracy = accuracy_score(y_test, pred)\n    precision = precision_score(y_test, pred)\n    recall = recall_score(y_test, pred)\n    f1 = f1_score(y_test, pred)\n    roc_auc = roc_auc_score(y_test, pred_proba)\n    print('Confusion Matrix')\n    print(confusion)\n    print('Accuracy: {0:.4f}, Precision: {1:.4f}, Recall {2:.4f}, F1: {3:.4f}, AUC:{4:.4f}'.format(accuracy, precision, recall, f1, roc_auc))","d8eb8450":"# Process fitting, prediction and evalution by Logistic Regression\n# Create Estimator CLass\ndt_clf = DecisionTreeClassifier()\nlr_clf = LogisticRegression()\nrf_clf = RandomForestClassifier()\n\n# Fitting\ndt_clf.fit(X_train, y_train)\nlr_clf.fit(X_train, y_train)\nrf_clf.fit(X_train, y_train)\n\n# Prediction\ndt_pred = dt_clf.predict(X_test)\nlr_pred = lr_clf.predict(X_test)\nrf_pred = rf_clf.predict(X_test)\n\n# Pred_Proba\ndt_pred_proba = dt_clf.predict_proba(X_test)[:, 1]\nlr_pred_proba = lr_clf.predict_proba(X_test)[:, 1]\nrf_pred_proba = rf_clf.predict_proba(X_test)[:, 1]\n\n# Evaluation\nget_clf_eval(y_test, dt_pred, dt_pred_proba)\nget_clf_eval(y_test, lr_pred, lr_pred_proba)\nget_clf_eval(y_test, rf_pred, rf_pred_proba)","586d8ea5":"# Plot Function\ndef precision_recall_curve_plot(y_test, pred_proba_c1):\n    # Extarct ndarray of threshold and ndarray of precision, recall by itself\n    precisions, recalls, thresholds = precision_recall_curve(y_test, pred_proba_c1)\n\n    # Set X axis for threshold values, Y axis for precision, recall and create plot\n    plt.figure(figsize=(8, 6))\n    threshold_boundary = thresholds.shape[0]\n    plt.plot(thresholds, precisions[0:threshold_boundary], linestyle='--', label='precision')\n    plt.plot(thresholds, recalls[0:threshold_boundary], label='recall')\n\n    # Scaling threshold values of 0.1 units on X axis\n    start, end = plt.xlim()\n    plt.xticks(np.round(np.arange(start, end, 0.1), 2))\n\n    # Set labels of X axis, y axis, legend and grid\n    plt.xlabel('Threshold value'); plt.ylabel('Precision and Recall value')\n    plt.legend()\n    plt.grid()\n    plt.show()","7b0f6488":"# DecisionTreeClassifier\nprecision_recall_curve_plot(y_test, dt_pred_proba)\n# LogisitcRegression\nprecision_recall_curve_plot(y_test, lr_pred_proba)\n# RandomForestClassifier\nprecision_recall_curve_plot(y_test, rf_pred_proba)","8823c5a5":"# Plot Function\n\ndef roc_curve_plot(y_test, pred_proba_c1):\n    # Return values of FPR, TPR by thresholds\n    fprs, tprs, thresholds = roc_curve(y_test, pred_proba_c1)\n    # Plot ROC curve\n    plt.plot(fprs, tprs, label='ROC')\n    # Plot diagonal line\n    plt.plot([0, 1], [0, 1], 'k--', label='Random')\n\n    # Scaling threshold values of 0.1 units on X axis(FPR)\n    start, end = plt.xlim()\n    plt.xticks(np.round(np.arange(start, end, 0.1), 2))\n    plt.xlim(0, 1)\n    plt.ylim(0, 1)\n    # Set label of X, Y axis\n    plt.xlabel('FPR(1 - Sensitivity)')\n    plt.ylabel('TPR(Recall)')\n    plt.legend()\n\n    plt.show()\n\nroc_curve_plot(y_test, dt_pred_proba)\nroc_curve_plot(y_test, lr_pred_proba)\nroc_curve_plot(y_test, rf_pred_proba)\n","2a0621d9":"# Evaluation with custom threshold value\n# Set threshold value as 0.46\ncustom_threshold = 0.46\n\n# Extract 'Positive Class' in order to apply Binarizer\ncustom_pred_proba = rf_pred_proba.reshape(-1, 1)\n\nbinarizer = Binarizer(threshold=custom_threshold).fit(custom_pred_proba)\ncustom_predict = binarizer.transform(custom_pred_proba)\n\nget_clf_eval(y_test, custom_predict, custom_pred_proba)","d0c842f0":"# Plot\nprecision_recall_curve_plot(y_test, custom_pred_proba)\nroc_curve_plot(y_test, custom_pred_proba)","b78f8ceb":"# Re-check for the accuracy score by DecisionTreeClassifier\n# Extract hyperparmeters of DecisionTreeClassifier\nprint('Accuracy Score by DecisionTreeClassifier: {0:.4f}'.format(accuracy_score(y_test, dt_pred)))\nprint('Hyperparameters of DecisionTreeClassifier:\\n', dt_clf.get_params())","abcd9171":"# Tuning of hyperparameters by GridSearchCV\nparams = {\n    'max_depth' : range(10, 30, 5),\n    'min_samples_split' : range(20, 40, 10)\n}\n\ngrid_cv = GridSearchCV(dt_clf, param_grid=params, scoring='accuracy', cv=7, verbose=1, n_jobs=-1, refit=True)\ngrid_cv.fit(X_train, y_train)\nprint('The Best Average of Accuracy Scores by GridSearchCV: {0:.4f}'.format(grid_cv.best_score_))\nprint('The Best Parameters for Optimization: ', grid_cv.best_params_)","6bd3cfae":"# Create DataFarme with results of GridSearchCv\ncv_results_df = pd.DataFrame(grid_cv.cv_results_)\ncv_results_df[['rank_test_score', 'param_max_depth', 'param_min_samples_split', 'mean_test_score']]","6866be61":"best_dt_clf = grid_cv.best_estimator_\nbest_pred = best_dt_clf.predict(X_test)\nbest_accuracy = accuracy_score(y_test, best_pred)\nprint('Accuracy Score of DecisionTreeClassifier: {0:.4f}'.format(best_accuracy))","5a4b255b":"ftr_values = best_dt_clf.feature_importances_\nftr_values = pd.Series(ftr_values, index=X_train.columns)\nftr_top5 = ftr_values.sort_values(ascending=False)[:5]\n\nplt.figure(figsize=(10, 8))\nplt.title('Feature Importance Top 5')\nsns.barplot(x=ftr_top5, y=ftr_top5.index)\nplt.show()","3163c8cb":"# We have already created DeicisonTreeClassifier model above\nknn_clf = KNeighborsClassifier(n_neighbors=8)\n\n# Create VotingClassifier by soft voting\nvo_clf = VotingClassifier(estimators=[('DT', dt_clf), ('KNN', knn_clf)], voting='soft')\n\n# Fitting, Prediction and Evaluation of VotingClassifier\nvo_clf.fit(X_train, y_train)\nvo_pred = vo_clf.predict(X_test)\nprint('Accuracy Score of VotingClassifier: {0:.4f}'.format(accuracy_score(y_test, vo_pred)))","745f456b":"# Fitting, Prediction and Evalution by each model\nclassifiers = [dt_clf, knn_clf]\nfor classifier in classifiers:\n    classifier.fit(X_train, y_train)\n    pred = classifier.predict(X_test)\n    class_name = classifier.__class__.__name__\n    print('Accuracy Score of {0}: {1:.4f}'.format(class_name, accuracy_score(y_test, pred)))","bf230396":"# We have already created RandomForestClassifier\n\nparams = {\n    'n_estimators' : [100],\n    'max_depth' : [6, 8, 10, 12],\n    'min_samples_leaf' : [8, 12, 18],\n    'min_samples_split' : [8, 16, 20]\n}\n\ngrid_cv = GridSearchCV(rf_clf, param_grid=params, cv=5, n_jobs=-1)\ngrid_cv.fit(X_train, y_train)\n\nprint('The Best Average of Accuracy Scores by GridSearchCV: {0:.4f}'.format(grid_cv.best_score_))\nprint('The Best Parameters for Optimization: ', grid_cv.best_params_)","572e65dd":"best_dt_clf = grid_cv.best_estimator_\nbest_pred = best_dt_clf.predict(X_test)\nbest_accuracy = accuracy_score(y_test, best_pred)\nprint('Accuracy Score of DecisionTreeClassifier: {0:.4f}'.format(best_accuracy))","dfe5215c":"ftr_values = best_dt_clf.feature_importances_\nftr_values = pd.Series(ftr_values, index=X_train.columns)\nftr_top5 = ftr_values.sort_values(ascending=False)[:5]\n\nplt.figure(figsize=(10, 8))\nplt.title('Feature Importance Top 5')\nsns.barplot(x=ftr_top5, y=ftr_top5.index)\nplt.show()","e64b7847":"# Create estimator and process fitting, prediction and evaluation for model\ngb_clf = GradientBoostingClassifier(random_state=11)\ngb_clf.fit(X_train, y_train)\ngb_pred = gb_clf.predict(X_test)\ngb_accuracy = accuracy_score(y_test, gb_pred)\n\nprint('Accuracy Score of GradientBoostingClassifier: {0:.4f}'.format(gb_accuracy))","9ff647dc":"# Optimization by GridSearchCV\nparams = {\n    'n_estimators' : [100, 500],\n    'learning_rate' : [0.05, 1]\n}\n\ngrid_cv = GridSearchCV(gb_clf, param_grid=params, cv=2, n_jobs=-1 ,verbose=1)\ngrid_cv.fit(X_train, y_train)\n\nprint('The Best Average of Accuracy Scores by GridSearchCV: {0:.4f}'.format(grid_cv.best_score_))\nprint('The Best Parameters for Optimization: ', grid_cv.best_params_)","0a426ccf":"# Create estimator and process fitting, prediction and evaluation for model\nlgbm_wrapper = LGBMClassifier(n_estimators=400, num_leaves=64, n_jobs=-1, boost_from_average=False)\n\nevals = [(X_test, y_test)]\nlgbm_wrapper.fit(X_train, y_train, early_stopping_rounds=1200, eval_metric='logloss', eval_set=evals, verbose=True)\npreds = lgbm_wrapper.predict(X_test)\npred_proba = lgbm_wrapper.predict_proba(X_test)[:, 1]\nget_clf_eval(y_test, preds, pred_proba)","d7d6f23a":"# Plot Feature importance\nfig, ax = plt.subplots(figsize=(10, 12))\nplot_importance(lgbm_wrapper, ax=ax)","d69f58be":"# Create estimator and process fitting, prediction and evaluation for model\nsmote = SMOTE(random_state=11)\n\nX_train_over, y_train_over = smote.fit_resample(X_train, y_train)\nprint('Before applying SMOTE, each shape of Feature\/Label datasets: ', X_train.shape, y_train.shape)\nprint('After appling SMOTE, each shape of Feature\/Label datasets: ', X_train_over.shape, y_train_over.shape)","b2a857fa":"# Create estimator and process fitting, prediction and evaluation for model after applying SMOTE\nrf_clf.fit(X_train_over, y_train_over)\nrf_preds_over = rf_clf.predict(X_test)\nrf_pred_proba_over = rf_clf.predict_proba(X_test)[:, 1]\n\nget_clf_eval(y_test, rf_preds_over, rf_pred_proba_over)","8ea27460":"precision_recall_curve_plot(y_test, rf_pred_proba_over)","2d7daec7":"# Create estimator and process fitting, prediction and evaluation for model after applying SMOTE\nlgbm_clf = LGBMClassifier(n_estimators=1000, num_leaves=64, n_jobs=-1, boost_from_average=False)\n\nlgbm_clf.fit(X_train_over, y_train_over)\nlgbm_preds_over = lgbm_clf.predict(X_test)\nlgbm_pred_proba = lgbm_clf.predict_proba(X_test)[:, 1]\n\nget_clf_eval(y_test, lgbm_preds_over, lgbm_pred_proba)","876b1327":"# Create individual ML model\nknn_clf = KNeighborsClassifier(n_neighbors=4)\nrf_clf = RandomForestClassifier(n_estimators=100, random_state=11)\ndt_clf = DecisionTreeClassifier()\nada_clf = AdaBoostClassifier(n_estimators=100)\n\n# Create the model which will be fitted by dataset Stacking processed\nlr_final = LogisticRegression(C=10)","3e7c3295":"# Fitting each models\nknn_clf.fit(X_train, y_train)\nrf_clf.fit(X_train, y_train)\ndt_clf.fit(X_train, y_train)\nada_clf.fit(X_train, y_train)","f9267a6c":"# Predict each models and predict them\n\nknn_pred = knn_clf.predict(X_test)\nrf_pred = rf_clf.predict(X_test)\ndt_pred = dt_clf.predict(X_test)\nada_pred = ada_clf.predict(X_test)\n\nprint('Accuracy Score of KNN: {0:.4f}'.format(accuracy_score(y_test, knn_pred)))\nprint('Accuracy Score of RandomForestClassifier: {0:.4f}'.format(accuracy_score(y_test, rf_pred)))\nprint('Accuracy Score of DeicisionTreeClassifier: {0:.4f}'.format(accuracy_score(y_test, dt_pred)))\nprint('Accuracy Score of AdaBoostClassifier: {0:.4f}'.format(accuracy_score(y_test, ada_pred)))","1abb124e":"# Combine preds to one ndarray\npred = np.array([knn_pred, rf_pred, dt_pred, ada_pred])\nprint(pred.shape)\n\n# Transponse 'pred' in order to convert as Feature\npred = np.transpose(pred)\nprint(pred.shape)","180b1554":"# Fit, Predict, Evaluate for final model\nlr_final.fit(pred, y_test)\nfinal = lr_final.predict(pred)\n\nprint('Accuracy Score of Final Model: {0:.4f}'.format(accuracy_score(y_test, final)))","2b8c0553":"## Data Preprocessing","4e3c7e42":"### SMOTE: Over Sampling","80029d42":"### GBM(Gradient Boosting Machine)","04d4afbf":"## Evaluation","1e576980":"### Summary\n1. The performance with RandomForestClassifier was much better than other algorithms.\n2. As you can see the last plot, we could set threshold value as 0.3 for custom.","b96b2c28":"### DecisionTreeClassifier","650602cf":"## Import libraries and data","f191e828":"# Loan Prediction EDA","f37beaf2":"### Random Forest","6bc210bc":"### LightGBM","369648af":"### Ensemble Learning(Voting Classifier)","2fa61158":"### Stacking Ensemble","2c565d82":"## Classification"}}