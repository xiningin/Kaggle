{"cell_type":{"a58fd8f9":"code","6d953df1":"code","3d1cb1cb":"code","1d9b96e2":"code","cc733395":"code","8f417d48":"code","5a3c9518":"code","9dfefa25":"code","832c79f4":"code","bf58e39f":"code","47a39db3":"code","f69ac2fd":"code","bb5c017c":"code","c7e559e3":"markdown","8fc96696":"markdown","1b9f0457":"markdown","84bb778e":"markdown","06a80de0":"markdown"},"source":{"a58fd8f9":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\nimport seaborn as sns\n\nfrom sklearn.pipeline import Pipeline\n\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import FunctionTransformer\n\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import train_test_split\n\nimport sklearn.linear_model as lm\n\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import roc_curve, roc_auc_score, auc\nfrom sklearn.metrics import make_scorer\nfrom sklearn.metrics import cohen_kappa_score # https:\/\/www.kaggle.com\/aroraaman\/quadratic-kappa-metric-explained-in-5-simple-steps\n\nsns.set_style(\"darkgrid\")\nsns.set_context(\"paper\")\nmpl.style.use(\"seaborn\")","6d953df1":"SOURCE_DATA_FOLDER = \"\/kaggle\/input\/petfinder-adoption-prediction\/\"\nTRAIN_IMAGE_FOLDER = SOURCE_DATA_FOLDER + \"train_images\/\"\nTRAIN_METADATA_FOLDER = SOURCE_DATA_FOLDER + \"train_metadata\/\"\nTRAIN_SENTIMENT_FOLDER = SOURCE_DATA_FOLDER + \"train_sentiment\/\"\n\nBREED_LABELS = SOURCE_DATA_FOLDER + \"breed_labels.csv\"\nCOLOR_LABELS = SOURCE_DATA_FOLDER + \"color_labels.csv\"\nSTATE_LABELS = SOURCE_DATA_FOLDER + \"state_labels.csv\"\nTRAIN_TABULAR = SOURCE_DATA_FOLDER + \"train\/train.csv\"\nTEST_TABULAR = SOURCE_DATA_FOLDER + \"test\/test.csv\"\n\nquadratic_kappa_scorer = make_scorer(cohen_kappa_score, weights='quadratic')","3d1cb1cb":"train_tabular_df = pd.read_csv(TRAIN_TABULAR)\ntest_tabular_df = pd.read_csv(TEST_TABULAR)","1d9b96e2":"def getRescuerIDCountMap(data):\n    rescuerid_counts = data['RescuerID'].value_counts().to_dict()\n    return rescuerid_counts\n\ndef encodeRescuerIDCount(data, mapping):\n    dataset = data.copy()\n    dataset['RescuerID_Count'] = dataset['RescuerID'].map(mapping).fillna(0)\n    return dataset\n\ndef createRescuerIDCount(data):\n    # RescuerID Counts\n    dataset = data.copy()\n    rescuerid_counts = getRescuerIDCountMap(dataset)\n    dataset = encodeRescuerIDCount(dataset, rescuerid_counts)\n    return dataset, rescuerid_counts\n\ndef createDummyVariables(data):\n    dataset = data.copy()\n    # fee 0 dummy\n    dataset['Flag_Fee_0'] = np.nan\n    dataset['Flag_Fee_0'] = dataset['Flag_Fee_0'].mask(dataset['Fee'] == 0, 1)\n    dataset['Flag_Fee_0'] = dataset['Flag_Fee_0'].fillna(0)\n\n    # breed dummy variables\n    mixed_breed_id = 307\n\n    # pure breeds are where Breed1 == Breed2\n    pure_breed_cond = (dataset['Breed1'] != mixed_breed_id) & (dataset['Breed1'] == dataset['Breed2'])\n    dataset['Flag_Pure_Breed'] = np.nan\n    dataset['Flag_Pure_Breed'] = dataset['Flag_Pure_Breed'].mask(pure_breed_cond, 1)\n    dataset['Flag_Pure_Breed'] = dataset['Flag_Pure_Breed'].fillna(0)\n\n    # we define mixed breeds as Breed1 or Breed2 being Mixed-Breed, or Breed1 != Breed2 where Breed1 and Breed2 are not Mixed-Breed\n    mixed_breed_cond = ((dataset['Breed1'] == 307) | (dataset['Breed2'] == 307)) | ((dataset['Breed1'] != dataset['Breed2']) & \n                                                                                             (dataset['Breed2'] != 0) & \n                                                                                             (dataset['Breed2'] != 307)& \n                                                                                             (dataset['Breed1'] != 307))\n    dataset['Flag_Mixed_Breed'] = np.nan\n    dataset['Flag_Mixed_Breed'] = dataset['Flag_Mixed_Breed'].mask(mixed_breed_cond, 1)\n    dataset['Flag_Mixed_Breed'] = dataset['Flag_Mixed_Breed'].fillna(0)\n    \n    # age < 3 months\n    dataset['Flag_Age_le_3'] = np.nan\n    dataset['Flag_Age_le_3'] = dataset['Flag_Age_le_3'].mask(dataset['Age'] <= 3, 1)\n    dataset['Flag_Age_le_3'] = dataset['Flag_Age_le_3'].fillna(0)\n    \n    # Name exists\n    dataset['Flag_Name'] = np.nan\n    dataset['Flag_Name'] = dataset['Flag_Name'].mask(dataset['Name'].isnull(), 1)\n    dataset['Flag_Name'] = dataset['Flag_Name'].fillna(0)\n    return dataset\n\ndef standardiseVariables(data):\n#    std_cols = ['Age', 'Quantity', 'Fee', 'VideoAmt', 'PhotoAmt', 'RescuerID_Count']\n    std_cols = ['Age', 'Quantity', 'Fee', 'VideoAmt', 'PhotoAmt']\n    dataset = data.copy()\n    dataset[[c + \"_std\" for c in std_cols]] = StandardScaler().fit_transform(dataset[std_cols])\n    return dataset\n\ndef oneHotEncode(data):\n    dummy_cols = ['Type','Gender','Health','FurLength','State','Color1','Color2','Color3']\n    dataset = data.copy()\n    return pd.get_dummies(dataset, columns=dummy_cols)\n\ndef getTargetEncodingMap(data, col, response):\n    target_map = data.groupby([col])[response].mean().to_dict()\n    return target_map\n\ndef targetEncodeFeature(data, col, mapping):\n    # yes i know this is the same as above but im literally so lazy so ctrl c + v\n    dataset = data.copy()\n    dataset[col + '_target_encode'] = dataset[col].map(mapping).fillna(0)\n    return dataset\n\ndef targetEncodeFeatures(data, cols, response):\n    # yes i know this is the same as above but im literally so lazy so ctrl c + v\n    dataset = data.copy()\n    mappings = dict()\n    for col in cols:\n        mapping = getTargetEncodingMap(dataset, col, response)\n        mappings[col] = mapping\n        dataset = targetEncodeFeature(dataset, col, mapping)\n    return dataset, mappings\n\ndef dropNonFeatureColumns(data):\n    non_feature_cols = [\n        'PetID',\n        'Name',\n        'RescuerID',\n        'Age',\n        'Quantity',\n        'Fee',\n        'PhotoAmt',\n        'VideoAmt',\n        'Description',\n        'Breed1',\n        'Breed2'\n    ]\n    dataset = data.copy()\n    return dataset.drop(columns=non_feature_cols)\n\ndef preProcessTrain(data):\n    mappings = dict()\n    data, mappings['RescuerID'] = createRescuerIDCount(data)\n    data = createDummyVariables(data)\n    data = standardiseVariables(data)\n    data = oneHotEncode(data)\n    data, target_mappings = targetEncodeFeatures(data,\n                                                cols=['Breed1', 'Breed2'],\n                                                response='AdoptionSpeed')\n    mappings.update(target_mappings)\n    data = dropNonFeatureColumns(data)\n    return data, mappings\n\ndef preProcessTest(data, mappings, train_cols):\n    train_cols = list(train_cols)\n    train_cols.remove('AdoptionSpeed')\n    data = data.fillna(0)\n    data = encodeRescuerIDCount(data, mappings['RescuerID'])\n    data = createDummyVariables(data)\n    data = standardiseVariables(data)\n    data = oneHotEncode(data)\n    data = targetEncodeFeature(data, 'Breed1', mappings['Breed1'])\n    data = targetEncodeFeature(data, 'Breed2', mappings['Breed2'])\n    data = dropNonFeatureColumns(data)\n    data = data[train_cols]\n    return data","cc733395":"train, mappings = preProcessTrain(train_tabular_df)\ntest = preProcessTest(test_tabular_df, mappings, train.columns)","8f417d48":"train.shape","5a3c9518":"def splitDataXY(dataset, response_column):\n    temp = dataset.copy()\n    Y_train = temp[response_column]\n    X_train = temp.drop(columns=[response_column])\n    return X_train, Y_train","9dfefa25":"X_train_df, Y_train_df = splitDataXY(train, 'AdoptionSpeed')\nX_train = X_train_df.to_numpy()\nY_train = Y_train_df.to_numpy()\n\nlogreg = lm.LogisticRegression(solver='saga', \n                               penalty= 'l1', \n                               C=10, \n                               class_weight=None,\n                               max_iter=10000, \n                               n_jobs=-1)\n\nlogreg.fit(X_train, Y_train)\n\npredicted = logreg.predict(test)\n\nprint(\"Training score:\", cohen_kappa_score(logreg.predict(X_train), Y_train, weights='quadratic'))","832c79f4":"for i, c in enumerate(X_train_df.columns):\n    print(f'{i}. {c}')","bf58e39f":"print(\"Training score:\", cohen_kappa_score(logreg.predict(X_train), Y_train, weights='quadratic'))","47a39db3":"submission = pd.DataFrame.from_dict({'PetID': test_tabular_df['PetID'],\n                                     'AdoptionSpeed': predicted})\nprint(submission.shape)\nsubmission.head()","f69ac2fd":"submission.to_csv(\"\/kaggle\/working\/submission.csv\", index=False)","bb5c017c":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/working'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","c7e559e3":"```\n# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session\n```","8fc96696":"## 2. Preprocess data and transform features","1b9f0457":"## 3. Train and Evaluate\n\n1. Split datasets into features and responses\n2. Fit on training data\n3. Test on unseen test data\n4. Create submission file and submit.","84bb778e":"We use the values obtained from Gridsearch (code not run here)\n\n```\ngrid = {'C': [0.0001, 0.001, 0.01, 1, 10, 100],\n        'class_weight': [None, 'balanced']}\n\nlogreg = lm.LogisticRegression(solver='saga', max_iter=10000)\ngridsearch_model = GridSearchCV(logreg,\n                                 grid, \n#                                 cv=5, #\n                                 scoring=quadratic_kappa_scorer, \n                                 return_train_score=True,\n                                 verbose=3,\n                                 n_jobs=-1)\ngridsearch_model.fit(X_train, Y_train)\n```\n\n```\n$ Average time taken per fit: 61.51685015360514\n$ Best Parameters: {'C': 10, 'class_weight': None}\n$ Best Quadratic Kappa 0.3104591557225801\n```","06a80de0":"# Modeling PetFinder.my with Logistic Regression\n\n## 1. Load Data "}}