{"cell_type":{"59d85ef4":"code","b83c3c21":"code","b03f0ff1":"code","8b262b85":"code","2187da40":"code","b0a412b5":"code","0c5d5910":"code","39a5fa02":"code","f3e0b694":"code","3de96b40":"code","883f4d4b":"code","4d5b1e15":"code","c7f3d9e0":"code","38140965":"code","05b1758e":"code","028972c1":"code","d724ed51":"code","35289143":"code","5b49ccf3":"code","c072b78e":"code","63fef3fe":"code","2a558aa2":"markdown","a42bacf9":"markdown","b714d813":"markdown","d7ce96fd":"markdown","5b3e3b29":"markdown","2577560c":"markdown","5a5b52f8":"markdown","8f258de8":"markdown","3bc1c2c9":"markdown","d02405fa":"markdown","8e4c49b8":"markdown","c776d8fe":"markdown","9b0433f1":"markdown","213d9bbe":"markdown","71419930":"markdown","cc540594":"markdown","f6c1ad92":"markdown","7f0c89a8":"markdown","decdb742":"markdown","487fd6a7":"markdown","ec4494b3":"markdown","8ae571bb":"markdown","be908196":"markdown","33ebfe02":"markdown","a3424ca9":"markdown","9aca8272":"markdown"},"source":{"59d85ef4":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","b83c3c21":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')\n\ntrain = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\n\n#TEST\u306e\u30c7\u30fc\u30bf\u306f\u4eca\u56de\u4f7f\u7528\u3057\u306a\u3044\u3002\n#test = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\n","b03f0ff1":"train.isnull().sum()","8b262b85":"train.head(6)","2187da40":"print(train[train['Age'].isnull()])","b0a412b5":"train.describe()","0c5d5910":"train.hist(\"Age\")\nplt.tight_layout()\nplt.show()","39a5fa02":"train.boxplot(column=\"Age\")","f3e0b694":"train1 = train.dropna(subset=['Age'])\ntrain1.head(6)","3de96b40":"train2= train.fillna({'Age': 0 })\ntrain2.head(6)","883f4d4b":"train3=train.fillna(train.mean())\ntrain3.head(6)","4d5b1e15":"train4=train.fillna(train.median())\ntrain4.head(6)","c7f3d9e0":"train = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ntrain5 = train\n\n#\u5e73\u5747\u30fb\u6a19\u6e96\u504f\u5dee\u30fbnull\u6570\u3092\u53d6\u5f97\u3059\u308b\nAge_average = 29.69\nAge_std = 14.52\nAge_nullcount = 177\n\n# \u6b63\u898f\u5206\u5e03\u306b\u5f93\u3046\u3068\u3057\u3001\u6a19\u6e96\u504f\u5dee\u306e\u7bc4\u56f2\u5185\u3067\u30e9\u30f3\u30c0\u30e0\u306b\u6570\u5b57\u3092\u4f5c\u308b\nrand = np.random.randint(Age_average - Age_std, Age_average + Age_std , size = Age_nullcount)\n\n#Age\u306e\u6b20\u640d\u5024\u3092\u30e9\u30f3\u30c0\u30e0\u306a\u5024\u3067\u57cb\u3081\u308b\ntrain5[\"Age\"][np.isnan(train5[\"Age\"])] = rand\n\ntrain5.head(6)\n","38140965":"train6 = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\n\n#train\u30c7\u30fc\u30bf\ntrain6.loc[(train6['Name'].str.contains('Mr\\.')) & (train6['Age'].isnull()), 'Age'] = train[train['Name'].str.contains('Mr\\.')].Age.mean()\ntrain6.loc[(train6['Name'].str.contains('Mrs\\.')) & (train6['Age'].isnull()), 'Age'] = train[train['Name'].str.contains('Mrs\\.')].Age.mean()\ntrain6.loc[(train6['Name'].str.contains('Miss\\.')) & (train6['Age'].isnull()), 'Age'] = train[train['Name'].str.contains('Miss\\.')].Age.mean()\ntrain6.loc[(train6['Name'].str.contains('Master\\.')) & (train6['Age'].isnull()), 'Age'] = train[train['Name'].str.contains('Master\\.')].Age.mean()\ntrain6.loc[(train6['Name'].str.contains('Dr\\.')) & (train6['Age'].isnull()), 'Age'] = train[train['Name'].str.contains('Dr\\.')].Age.mean()\n\ntrain6.head(6)","05b1758e":"train[\"Fare\"]=train[\"Fare\"].fillna(train[\"Fare\"].median())\ntrain[\"Embarked\"]=train[\"Embarked\"].fillna(\"S\")\n\n\n# #test\u30c7\u30fc\u30bf\u306eAge\u306f\u4e2d\u592e\u5024\u3067\u6b20\u640d\u51e6\u7406\n# test[\"Fare\"]=test[\"Fare\"].fillna(test[\"Fare\"].median())\n# test[\"Age\"]=test[\"Age\"].fillna(test[\"Age\"].median())\n# test[\"Embarked\"]=test[\"Embarked\"].fillna(\"S\")\n\n# test.isnull().sum()\n","028972c1":"train = pd.get_dummies(train, columns=[\"Sex\",\"Pclass\",\"Embarked\"])\ntrain1 = pd.get_dummies(train2, columns=[\"Sex\",\"Pclass\",\"Embarked\"])\ntrain2 = pd.get_dummies(train2, columns=[\"Sex\",\"Pclass\",\"Embarked\"])\ntrain3 = pd.get_dummies(train3, columns=[\"Sex\",\"Pclass\",\"Embarked\"])\ntrain4 = pd.get_dummies(train4, columns=[\"Sex\",\"Pclass\",\"Embarked\"])\ntrain5 = pd.get_dummies(train5, columns=[\"Sex\",\"Pclass\",\"Embarked\"])\ntrain6 = pd.get_dummies(train6, columns=[\"Sex\",\"Pclass\",\"Embarked\"])\n\n# test = pd.get_dummies(test, columns=[\"Sex\",\"Pclass\",\"Embarked\"])\n","d724ed51":"train[\"FamilyNum\"] = train[\"SibSp\"] + train[\"Parch\"]\ntrain1[\"FamilyNum\"] = train1[\"SibSp\"] + train1[\"Parch\"]\ntrain2[\"FamilyNum\"] = train2[\"SibSp\"] + train2[\"Parch\"]\ntrain3[\"FamilyNum\"] = train3[\"SibSp\"] + train3[\"Parch\"]\ntrain4[\"FamilyNum\"] = train4[\"SibSp\"] + train4[\"Parch\"]\ntrain5[\"FamilyNum\"] = train5[\"SibSp\"] + train5[\"Parch\"]\ntrain6[\"FamilyNum\"] = train6[\"SibSp\"] + train6[\"Parch\"]\n\ntrain[\"hasFamily\"] = train[\"FamilyNum\"].apply(lambda x : 1 if x >= 1 else 0)\ntrain1[\"hasFamily\"] = train1[\"FamilyNum\"].apply(lambda x : 1 if x >= 1 else 0)\ntrain2[\"hasFamily\"] = train2[\"FamilyNum\"].apply(lambda x : 1 if x >= 1 else 0)\ntrain3[\"hasFamily\"] = train3[\"FamilyNum\"].apply(lambda x : 1 if x >= 1 else 0)\ntrain4[\"hasFamily\"] = train4[\"FamilyNum\"].apply(lambda x : 1 if x >= 1 else 0)\ntrain5[\"hasFamily\"] = train5[\"FamilyNum\"].apply(lambda x : 1 if x >= 1 else 0)\ntrain6[\"hasFamily\"] = train6[\"FamilyNum\"].apply(lambda x : 1 if x >= 1 else 0)\n\ntrain = train.drop(labels = [\"SibSp\"], axis = 1)\ntrain1 = train1.drop(labels = [\"SibSp\"], axis = 1)\ntrain2 = train2.drop(labels = [\"SibSp\"], axis = 1)\ntrain3 = train3.drop(labels = [\"SibSp\"], axis = 1)\ntrain4 = train4.drop(labels = [\"SibSp\"], axis = 1)\ntrain5 = train5.drop(labels = [\"SibSp\"], axis = 1)\ntrain6 = train6.drop(labels = [\"SibSp\"], axis = 1)\n\ntrain = train.drop(labels = [\"Parch\"], axis = 1)\ntrain1 = train1.drop(labels = [\"Parch\"], axis = 1)\ntrain2 = train2.drop(labels = [\"Parch\"], axis = 1)\ntrain3 = train3.drop(labels = [\"Parch\"], axis = 1)\ntrain4 = train4.drop(labels = [\"Parch\"], axis = 1)\ntrain5 = train5.drop(labels = [\"Parch\"], axis = 1)\ntrain6 = train6.drop(labels = [\"Parch\"], axis = 1)\n\n# test[\"FamilyNum\"] = test[\"SibSp\"] + test[\"Parch\"]\n# test[\"hasFamily\"] = test[\"FamilyNum\"].apply(lambda x : 1 if x >= 1 else 0)\n# test = test.drop(labels = [\"SibSp\"], axis = 1)\n# test = test.drop(labels = [\"Parch\"], axis = 1)\n ","35289143":"#\u4e0d\u8981\u30ab\u30e9\u30e0\u524a\u9664\ntrain = train.drop([\"PassengerId\", \"Name\", \"Ticket\", \"Cabin\"],axis=1)\ntrain1 = train1.drop([\"PassengerId\", \"Name\", \"Ticket\", \"Cabin\"],axis=1)\ntrain2 = train2.drop([\"PassengerId\", \"Name\", \"Ticket\", \"Cabin\"],axis=1)\ntrain3 = train3.drop([\"PassengerId\", \"Name\", \"Ticket\", \"Cabin\"],axis=1)\ntrain4 = train4.drop([\"PassengerId\", \"Name\", \"Ticket\", \"Cabin\"],axis=1)\ntrain5 = train5.drop([\"PassengerId\", \"Name\", \"Ticket\", \"Cabin\"],axis=1)\ntrain6 = train6.drop([\"PassengerId\", \"Name\", \"Ticket\", \"Cabin\"],axis=1)\n\n# test = test.drop([\"PassengerId\", \"Name\", \"Ticket\", \"Cabin\"],axis=1)\n\ntrain1.describe()\n","5b49ccf3":"from sklearn.model_selection import train_test_split\n\ntrain_X = train.drop('Survived',axis = 1)\ntrain_X1 = train1.drop('Survived',axis = 1)\ntrain_X2 = train2.drop('Survived',axis = 1)\ntrain_X3 = train3.drop('Survived',axis = 1)\ntrain_X4 = train4.drop('Survived',axis = 1)\ntrain_X5 = train5.drop('Survived',axis = 1)\ntrain_X6 = train6.drop('Survived',axis = 1)\n\ntrain_y = train.Survived\ntrain_y1 = train1.Survived\ntrain_y2 = train2.Survived\ntrain_y3 = train3.Survived\ntrain_y4 = train4.Survived\ntrain_y5 = train5.Survived\ntrain_y6 = train6.Survived\n\n\n(X_train, X_test, y_train, y_test) = train_test_split(train_X, train_y , test_size = 0.3 , random_state = 0)\n(X_train1, X_test1, y_train1, y_test1) = train_test_split(train_X1, train_y1 , test_size = 0.3 , random_state = 0)\n(X_train2, X_test2, y_train2, y_test2) = train_test_split(train_X2, train_y2 , test_size = 0.3 , random_state = 0)\n(X_train3, X_test3, y_train3, y_test3) = train_test_split(train_X3, train_y3 , test_size = 0.3 , random_state = 0)\n(X_train4, X_test4, y_train4, y_test4) = train_test_split(train_X4, train_y4 , test_size = 0.3 , random_state = 0)\n(X_train5, X_test5, y_train5, y_test5) = train_test_split(train_X5, train_y5 , test_size = 0.3 , random_state = 0)\n(X_train6, X_test6, y_train6, y_test6) = train_test_split(train_X6, train_y6 , test_size = 0.3 , random_state = 0)\n\n\nprint(\"X_train:\"+str(X_train1.shape))\n# print(\"X_test:\"+str(X_test1.shape))\nprint(\"y_train:\"+str(y_train1.shape))\n# print(\"y_test:\"+str(y_test1.shape))\n","c072b78e":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\n\n#\u30e2\u30c7\u30eb\u306e\u69cb\u7bc9\nrfc1 = RandomForestClassifier(random_state=0)\nrfc2 = RandomForestClassifier(random_state=0)\nrfc3 = RandomForestClassifier(random_state=0)\nrfc4 = RandomForestClassifier(random_state=0)\nrfc5 = RandomForestClassifier(random_state=0)\nrfc6 = RandomForestClassifier(random_state=0)\n\n#\u5b66\u7fd2\u30c7\u30fc\u30bf\u306b\u3066\u5b66\u7fd2\nrfc1.fit(X_train1, y_train1)\nrfc2.fit(X_train2, y_train2)\nrfc3.fit(X_train3, y_train3)\nrfc4.fit(X_train4, y_train4)\nrfc5.fit(X_train5, y_train5)\nrfc6.fit(X_train6, y_train6)\n\n# \u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\u306b\u3066\u4e88\u6e2c\ny_pred1 = rfc1.predict(X_test1)\ny_pred2 = rfc2.predict(X_test2)\ny_pred3 = rfc3.predict(X_test3)\ny_pred4 = rfc4.predict(X_test4)\ny_pred5 = rfc5.predict(X_test5)\ny_pred6 = rfc6.predict(X_test6)\n\n","63fef3fe":"#\u6b63\u89e3\u7387\nprint(\"\u6b63\u89e3\u7387\")\nprint(f'accuracy1:{accuracy_score(y_test1, y_pred1)}')\nprint(f'accuracy2:{accuracy_score(y_test2, y_pred2)}')\nprint(f'accuracy3:{accuracy_score(y_test3, y_pred3)}')\nprint(f'accuracy4:{accuracy_score(y_test4, y_pred4)}')\nprint(f'accuracy5:{accuracy_score(y_test5, y_pred5)}')\nprint(f'accuracy6:{accuracy_score(y_test6, y_pred6)}')\n\n","2a558aa2":"### \u6b20\u640d\u5024\u51e6\u74061:\u30ea\u30b9\u30c8\u30ef\u30a4\u30ba(\u884c\u524a\u9664)","a42bacf9":"### * \u30ab\u30c6\u30b4\u30ea\u5909\u6570\u3092One-hot encoding\u306b\u3066\u30c0\u30df\u30fc\u5909\u6570\u5316","b714d813":"#  **\u3010\u6f14\u7fd2\u3011\u6b20\u640d\u5024\u51e6\u7406**","d7ce96fd":"### * \u4e0d\u8981\u30ab\u30e9\u30e0\u3092\u524a\u9664","5b3e3b29":"### * \u5b66\u7fd2\u30c7\u30fc\u30bf\u306e\u30c7\u30fc\u30bf\u8868\u793a(\u6700\u521d\u306e6\u884c\u76ee)","2577560c":"# \u3010\u6f14\u7fd2\u3011\u30ab\u30c6\u30b4\u30ea\u30c7\u30fc\u30bf\u306e\u51e6\u7406","5a5b52f8":"###  \u5fc5\u8981\u306a\u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u30a4\u30f3\u30dd\u30fc\u30c8\n###  \u5b66\u7fd2\u30c7\u30fc\u30bf\u3068\u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\u3092\u30c7\u30fc\u30bf\u30d5\u30ec\u30fc\u30e0\u306b\u8aad\u307f\u8fbc\u307f","8f258de8":"### \u6b20\u640d\u5024\u51e6\u74065:\u5206\u6563\u306b\u57fa\u3065\u3044\u305f\u6b20\u640d\u5024\u88dc\u5b8c\n","3bc1c2c9":"### \u6b20\u640d\u5024\u51e6\u74066:\u4ed6\u306e\u30ab\u30e9\u30e0\u306e\u5024\u306b\u57fa\u3065\u3044\u305f\u6b20\u640d\u5024\u88dc\u5b8c","d02405fa":"### Age\u30ab\u30e9\u30e0\u610f\u5916\u306e\u6b20\u640d\u5024\u51e6\u7406","8e4c49b8":"### *  Age\u306e\u5206\u5e03\u78ba\u8a8d(\u7bb1\u3072\u3052\u56f3)","c776d8fe":"### \u691c\u8a3c\u3092\u5b9f\u65bd\u3059\u308b\u305f\u3081\u3001\u3053\u308c\u307e\u3067\u5404\u51e6\u7406\u3092\u884c\u3063\u305f\u5b66\u7fd2\u30c7\u30fc\u30bf\u3092\u3055\u3089\u306b\u3001\u5b66\u7fd2\u30c7\u30fc\u30bf\u3068\u691c\u8a3c\u30c7\u30fc\u30bf\u306b7:3\u306b\u5206\u5272  \n### \u5404\u6b20\u640d\u5024\u51e6\u7406\u306e\u30c7\u30fc\u30bf\u3092\u540c\u69d8\u306b\u30c7\u30fc\u30bf\u3092\u5206\u5272\u3059\u308b\u305f\u3081\u3001random_state\u306e\u5024\u30920\u3067\u56fa\u5b9a","9b0433f1":"###* \u6b63\u89e3\u7387\u3001\u7684\u4e2d\u7387","213d9bbe":"### \u6b20\u640d\u5024\u51e6\u74063:\u5e73\u5747\u5024\u57cb\u3081","71419930":"### *  Age\u306e\u5206\u5e03\u78ba\u8a8d(\u30d2\u30b9\u30c8\u30b0\u30e9\u30e0)","cc540594":"### * \u6b20\u640d\u5024\u306e\u78ba\u8a8d(\u5b66\u7fd2\u30c7\u30fc\u30bf)","f6c1ad92":"# \u3010\u6f14\u7fd2\u3011\u30d5\u30a9\u30fc\u30eb\u30c9\u30a2\u30a6\u30c8\u6cd5","7f0c89a8":"### \u6b20\u640d\u5024\u51e6\u74064:\u4e2d\u592e\u5024\u57cb\u3081","decdb742":"# \u3010\u6f14\u7fd2\u3011\u4e0d\u8981\u30ab\u30e9\u30e0\u524a\u9664","487fd6a7":"### * Age\u30ab\u30e9\u30e0\u306e\u8981\u7d04\u7d71\u8a08\u91cf\u3092\u78ba\u8a8d","ec4494b3":"###* \u30e9\u30f3\u30c0\u30e0\u30d5\u30a9\u30ec\u30b9\u30c8\u306b\u3066\u5b66\u7fd2\u3092\u884c\u3044\u3001\u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\u3067\u306e\u4e88\u6e2c\u7d50\u679c\u3092\u8868\u793a","8ae571bb":"### * \u65b0\u305f\u306a\u7279\u5fb4\u91cf\u3092\u4f5c\u6210","be908196":"### \u6b20\u640d\u5024\u51e6\u74062:0\u57cb\u3081","33ebfe02":"# \u3010\u6f14\u7fd2\u3011\u30e2\u30c7\u30eb\u5b66\u7fd2","a3424ca9":"# \u3010\u6f14\u7fd2\u3011\u65b0\u305f\u306a\u7279\u5fb4\u91cf\u306e\u4f5c\u6210","9aca8272":"# \u3010\u6f14\u7fd2\u3011\u30c7\u30fc\u30bf\u306e\u63a2\u7d22"}}