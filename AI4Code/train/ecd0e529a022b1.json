{"cell_type":{"4df888a0":"code","d174af72":"code","8198317c":"code","9487d0a0":"code","d06f2093":"code","615e8c3c":"code","e01aec7d":"code","6dc85989":"code","4481aa59":"code","ff66eda0":"code","3863eef9":"code","47e9490c":"code","42a20234":"code","f0793cb2":"code","828014a4":"code","4998a26a":"code","755ba34e":"code","f1623740":"code","6ea079bb":"code","fe262dc5":"markdown","26cc95a3":"markdown","8b453cae":"markdown","148524f2":"markdown"},"source":{"4df888a0":"import numpy\n# import gensim\nimport nltk\n# import seaborn\nimport pandas as pd\nimport re\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\nfrom nltk.tokenize import RegexpTokenizer\nfrom nltk.stem.wordnet import WordNetLemmatizer","d174af72":"summary = pd.read_csv('\/kaggle\/input\/news-summary\/news_summary.csv', encoding='iso-8859-1')","8198317c":"raw = pd.read_csv('..\/input\/indian-financial-news-articles-20032020\/IndianFinancialNews.csv', encoding='iso-8859-1')","9487d0a0":"raw.head() #how does raw DataFrame look like","d06f2093":"raw.iloc[0,0], raw.iloc[0,1] #viewing the contents inside raw's 0th element","615e8c3c":"#making a word_count column in the DataFrame\nraw['word_count'] = raw['Description'].apply(lambda x: len(str(x).split(\" \")))\nraw.head()","e01aec7d":"raw.word_count.describe() #describes what all data is there and other stats","6dc85989":"# freq = pd.Series(' '.join(raw['description']).split()).value_counts()[:20]\n# freq","4481aa59":"# Stopwords are all the commonly used english words which don't contribute to keywords such as 'as', 'are' etc\nstop_words = set(stopwords.words(\"english\"))\n# Creating a customized stopword list from data shown below after several iterations\nnew_words = [\"using\", \"show\", \"result\", \"large\", \"also\", \"iv\",\n             \"one\", \"two\", \"new\", \"previously\", \"shown\", \"year\", \"old\", \"said\", \"reportedly\",\n             \"added\", \"u\", \"day\", \"time\"]\nstop_words = stop_words.union(new_words) #customised stopwords added to previous stopword","ff66eda0":"# Creating a new list of texts called corpus where the following things are removed\ncorpus = []\nfor i in range(0, 5000):\n    # Remove punctuations\n    text = re.sub('[^a-zA-Z]', ' ', raw['Description'][i])\n    # Convert to lowercase\n    text = text.lower()\n    # Remove tags\n    text=re.sub(\"&lt;\/?.*?&gt;\",\" &lt;&gt; \",text)\n    # Remove special characters and digits\n    text=re.sub(\"(\\\\d|\\\\W)+\",\" \",text)\n    # Convert to list from string\n    text = text.split()\n    # Stemming\n    ps=PorterStemmer()\n    # Lemmatisation\n    lem = WordNetLemmatizer()\n    text = [lem.lemmatize(word) for word in text if not word in  stop_words] \n    text = \" \".join(text)\n    corpus.append(text)","3863eef9":"# After removing stopwords, punctions and normalizing to root words\ncorpus[0]","47e9490c":"# Tells us the max keywords used without including stopwords in the whole corpus and we add such words to new stop words\nfreq = pd.Series(' '.join(corpus).split()).value_counts()[:20]\nfreq ","42a20234":"# Corpus cell number chosen(arbritarily)\ncorpn = 120","f0793cb2":"# Tokenizes and builds a vocabulary\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport re\ncv=CountVectorizer(stop_words=stop_words, max_features=10, ngram_range=(1,3))","828014a4":"#this can be put inside a loop to get key words for all articles\ncorpi = [corpus[corpn]] #changing the number here will give us the key words for that specific article\nX=cv.fit_transform(corpi)\nlist(cv.vocabulary_.keys())[:10]","4998a26a":"corpi","755ba34e":"realtext = raw.iloc[corpn,1]\nrealtext","f1623740":"from sklearn.feature_extraction.text import TfidfTransformer\n \ntfidf_transformer=TfidfTransformer(smooth_idf=True,use_idf=True)\ntfidf_transformer.fit(X)\n# get feature names\nfeature_names=cv.get_feature_names()\n \n# fetch document for which keywords needs to be extracted\ndoc=corpus[corpn]\n \n#generate tf-idf for the given document\ntf_idf_vector=tfidf_transformer.transform(cv.transform([doc]))","6ea079bb":"#tf_idf sorting in descending order\nfrom scipy.sparse import coo_matrix\ndef sort_coo(coo_matrix):\n    tuples = zip(coo_matrix.col, coo_matrix.data)\n    return sorted(tuples, key=lambda x: (x[1], x[0]), reverse=True)\n \ndef extract_topn_from_vector(feature_names, sorted_items, topn=10):\n    \"\"\"get the feature names and tf-idf score of top n items\"\"\"\n    \n    #use only topn items from vector\n    sorted_items = sorted_items[:topn]\n \n    score_vals = []\n    feature_vals = []\n    \n    # word index and corresponding tf-idf score\n    for idx, score in sorted_items:\n        \n        #keep track of feature name and its corresponding score\n        score_vals.append(round(score, 3))\n        feature_vals.append(feature_names[idx])\n \n    #create a tuples of feature,score\n    #results = zip(feature_vals,score_vals)\n    results= {}\n    for idx in range(len(feature_vals)):\n        results[feature_vals[idx]]=score_vals[idx]\n    \n    return results\n#sort the tf-idf vectors by descending order of scores\nsorted_items=sort_coo(tf_idf_vector.tocoo())\n#extract only the top n; n here is 10\nkeywords=extract_topn_from_vector(feature_names,sorted_items,10)\n \n# now print the results\nprint(\"\\nAbstract:\")\nprint(doc)\nprint(\"\\nKeywords:\")\nfor k in keywords:\n    print(k,keywords[k])","fe262dc5":"Term Frequency: This summarizes how often a given word appears within a document.\nInverse Document Frequency: This downscales words that appear a lot across documents.","26cc95a3":"### Loading data into DataFrames","8b453cae":"## Only done for a single corpus text","148524f2":"### Frequency counted from all rows\/texts (so has all the various kinds of news)"}}