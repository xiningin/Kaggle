{"cell_type":{"c023d6fd":"code","cbc3041a":"code","9ec8babb":"code","071a997d":"code","e480cfaa":"code","4783149f":"code","4dc23689":"code","4a41c3f3":"code","2141e9d6":"code","e610ba87":"code","349cad21":"code","8e2314b5":"code","d2fc261d":"code","ac38dd7a":"code","ea5bd604":"code","663b4993":"code","f921a4c9":"code","fcaa089f":"code","b49bf956":"code","0500fa29":"code","db4a9bf8":"code","117a50e2":"code","55ef52bc":"code","b0712f5e":"code","e89484f6":"code","ab7dcb39":"code","b0c572c9":"code","5ad636da":"code","6deae296":"markdown","7d62255f":"markdown","6b48afff":"markdown","a0c404c5":"markdown","60a6df0c":"markdown","690255ae":"markdown","70a119c9":"markdown","4379325a":"markdown","b430e678":"markdown","d982360a":"markdown"},"source":{"c023d6fd":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","cbc3041a":"import pandas as pd # load and manipulate data and for One-Hot Encoding\nimport numpy as np # calculate the mean and standard deviation\nimport xgboost as xgb # XGBoost stuff\nfrom sklearn.model_selection import train_test_split # split  data into training and testing sets\nfrom sklearn.metrics import balanced_accuracy_score, roc_auc_score, make_scorer # for scoring during cross validation\nfrom sklearn.model_selection import GridSearchCV # cross validation\nfrom sklearn.metrics import confusion_matrix # creates a confusion matrix\nfrom sklearn.metrics import plot_confusion_matrix # draws a confusion matrix","9ec8babb":"train = pd.read_csv('\/kaggle\/input\/customer-churn-prediction-2020\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/customer-churn-prediction-2020\/test.csv')\ntrain.shape,test.shape","071a997d":"# look at the first five rows\ntrain.head()","e480cfaa":"# what kind of data is each column?\ntrain.dtypes","4783149f":"# check if any X_train columns contain any NAs\ntrain.columns[train.isna().any()].tolist()","4dc23689":"# check if any X_test columns contain any NAs\ntest.columns[test.isna().any()].tolist()","4a41c3f3":"X = train.drop('churn', axis=1).copy() \nX.head()","2141e9d6":"y = train['churn'].copy()\ny.head()","e610ba87":"X.dtypes","349cad21":"X['state'].unique()","8e2314b5":"X['area_code'].unique()","d2fc261d":"X['international_plan'].unique()","ac38dd7a":"X['voice_mail_plan'].unique()","ea5bd604":"# Create Binary Labels\nX['international_plan'] = np.where(X['international_plan'].str.contains('yes'), 1, 0) # Note that 'no' is 1 to keep consistent with the data dictionary\nX['voice_mail_plan'] = np.where(X['voice_mail_plan'].str.contains('yes'), 1, 0)\n\n# Do the same for our test set\ntest['international_plan'] = np.where(test['international_plan'].str.contains('yes'), 1, 0) # Note that 'no' is 1 to keep consistent with the data dictionary\ntest['voice_mail_plan'] = np.where(test['voice_mail_plan'].str.contains('yes'), 1, 0)\n\nX.head()","663b4993":"# Create Dummy Variables\nX = pd.get_dummies(X, columns=['state', 'area_code'])\n\n# Do the same for our test set\ntest = pd.get_dummies(test, columns=['state', 'area_code'])\n\nX.head()","f921a4c9":"# check to make sure y only has two values\ny.unique()","fcaa089f":"# change y to numeric \n# y = pd.Series(np.where(y == 'yes', 1, 0),y.index)","b49bf956":"# check if we need to stratify\n# sum(y)\/len(y)","0500fa29":"X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, stratify=y)","db4a9bf8":"# sum(y_train)\/len(y_train)","117a50e2":"# sum(y_test)\/len(y_test)","55ef52bc":"# ## NOTE: When data are imbalanced, the XGBoost manual says...\n# ## If you care only about the overall performance metric (AUC) of your prediction\n# ##     * Balance the positive and negative weights via scale_pos_weight\n# ##     * Use AUC for evaluation\n# ## ALSO NOTE: I ran GridSearchCV sequentially on subsets of parameter options, rather than all at once\n# ## in order to optimize parameters in a short period of time.\n\n## ROUND 1\nparam_grid = {\n     'max_depth': [3, 4, 5],\n     'learning_rate': [0.1, 0.01, 0.05],\n     'gamma': [0, 0.25, 1.0],\n     'reg_lambda': [0, 1.0, 10.0],\n     'scale_pos_weight': [1, 3, 5] # NOTE: XGBoost recommends sum(negative instances) \/ sum(positive instances)\n}\n# Output: {'gamma': 0.25, 'learning_rate': 0.1, 'max_depth': 4, 'reg_lambda': 10.0, 'scale_pos_weight': 1}\n# Because learning_rate and reg_lambda were at the ends of their range, we will continue to explore those...\n\n# ## ROUND 2\nparam_grid = {\n     'max_depth': [4],\n     'learning_rate': [0.1, 0.5, 1],\n     'gamma': [0.25],\n     'reg_lambda': [10.0, 20, 100],\n      'scale_pos_weight': [1]\n}\n# ## Output: {'gamma': 0.25, 'learning_rate': 0.1, 'max_depth': 4, 'reg_lambda': 10.0, 'scale_pos_weight': 1}\n\n## NOTE: To speed up cross validiation, and to further prevent overfitting.\n## We are only using a random subset of the data (90%) and are only\n## using a random subset of the features (columns) (50%) per tree.\noptimal_params = GridSearchCV(\n     estimator=xgb.XGBClassifier(objective='binary:logistic', \n                                 seed=42,\n                                 subsample=0.9,\n                                 colsample_bytree=0.5),\n     param_grid=param_grid,\n     scoring='roc_auc', ## see https:\/\/scikit-learn.org\/stable\/modules\/model_evaluation.html#scoring-parameter\n     verbose=0, # NOTE: If you want to see what Grid Search is doing, set verbose=2\n     n_jobs = 10,\n     cv = 3\n )\n\noptimal_params.fit(X_train, \n                    y_train, \n                    early_stopping_rounds=10,                \n                    eval_metric='auc',\n                    eval_set=[(X_test, y_test)],\n                    verbose=False)\nprint(optimal_params.best_params_)","b0712f5e":"# Evaluate Optimized Model\nclf_xgb = xgb.XGBClassifier(seed=42,\n                        objective='binary:logistic',\n                        gamma=0.25,\n                        learn_rate=0.01,\n                        max_depth=4,\n                        reg_lambda=10,\n                        scale_pos_weight=1,\n                        subsample=0.9,\n                        colsample_bytree=0.5)\nclf_xgb.fit(X_train, \n            y_train, \n            verbose=True, \n            early_stopping_rounds=10,\n            eval_metric='aucpr',\n            eval_set=[(X_test, y_test)])","e89484f6":"preds = clf_xgb.predict(X_test)\n\naccuracy = (preds == y_test).sum().astype(float) \/ len(preds)*100\n\nprint(\"XGBoost's prediction accuracy with optimal hyperparameters is: %3.2f\" % (accuracy))","ab7dcb39":"plot_confusion_matrix(clf_xgb, \n                      X_test, \n                      y_test,\n                      values_format='d',\n                      display_labels=[\"Did not leave\", \"Left\"])","b0c572c9":"predictor_cols = X_test.columns\n# Use the model to make predictions\npredicted = clf_xgb.predict(test[predictor_cols])\n# We will look at the predicted prices to ensure we have something sensible.\nprint(predicted)","5ad636da":"submission = pd.DataFrame({'id': test.id, 'churn': predicted})\n\n#Convert DataFrame to a csv file that can be uploaded\n#This is saved in the same directory as your notebook\nfilename = 'churn.csv'\n\nsubmission.to_csv(filename,index=False)\n\nprint('Saved file: ' + filename)","6deae296":"## Clean Data","7d62255f":"#### Part 3: Format Data - Binary and One Hot Encoding","6b48afff":"## Modeling","a0c404c5":"## Import Packages","60a6df0c":"#### Part 2: Format Data - Split into Dependent and Independent Variables","690255ae":"## Submit Results","70a119c9":"#### Part 4: Train, Test, Split","4379325a":"#### Part 1: Build and Evaluate Model","b430e678":"## Get Data","d982360a":"#### Part 1: Identify Missing Data"}}