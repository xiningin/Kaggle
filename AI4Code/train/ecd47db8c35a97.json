{"cell_type":{"361d9c33":"code","debe9d5a":"code","b8bb8f5a":"code","643ce1c1":"code","73998c97":"code","14fd256a":"code","49105622":"code","b3bd421c":"code","f2169bac":"code","919ffa21":"code","55568b14":"code","be46631f":"code","30586853":"code","aeeefdbb":"code","cc153bcb":"code","1c23792f":"code","1d0c388f":"code","c7739591":"code","95c2099a":"code","02a9cba1":"code","851da0c2":"code","731b0d61":"code","d303673e":"code","87bc8413":"code","c6a0a8c3":"markdown","36c06702":"markdown","01ea3a8b":"markdown","e0d3149f":"markdown","4146cd71":"markdown","27f49eff":"markdown","ab454022":"markdown","8e07996b":"markdown","2ac0f04f":"markdown","4772e8da":"markdown","c6946457":"markdown","0a7dfc91":"markdown","45feb8e6":"markdown","66e37526":"markdown","d04a6fe3":"markdown","bd57049a":"markdown","2740a823":"markdown","c43fd260":"markdown","a79e4a75":"markdown","d1c56b6e":"markdown","bcece0d4":"markdown","4490e09a":"markdown"},"source":{"361d9c33":"!pip install imutils","debe9d5a":"# DO NOT CHANGE ANYTHING IN THIS BLOCK\nimport numpy as np\nimport pandas as pd \nimport os\nimport matplotlib.pyplot as plt\nimport cv2 as cv\nimport tensorflow as tf\nimport tensorflow.keras\nimport time\nimport imutils\nimport hashlib\nimport pickle\nimport seaborn as sns\n\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.models import Sequential, load_model\nfrom tensorflow.keras.layers import BatchNormalization, Conv2D, MaxPooling2D, AveragePooling2D, Dense, Activation, Dropout, Flatten, Input\nfrom tensorflow.keras.metrics import categorical_accuracy\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\nfrom tensorflow.keras.optimizers import *\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.preprocessing.image import load_img, img_to_array\nfrom tensorflow.keras.preprocessing import image\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.applications.mobilenet_v2 import preprocess_input\nfrom tensorflow.keras.applications import MobileNetV2\nfrom typing import List\nfrom pathlib import Path\nfrom imutils.video import VideoStream\nfrom PIL import Image\n\nlabel_map = ['Anger', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']","b8bb8f5a":"# your code here\ndf=pd.read_csv('..\/input\/techx-facial-expression-recognition\/train.csv')\ndisplay(df)\n","643ce1c1":"# your code here\np=df['pixels']\nk=p.values\nprint(type(k))\nNum=[]\nfor k in p:\n    a=k.split( )\n    Num.append([len(a)])\n\n#     print(a)\ndisplay(Num)","73998c97":"'''\nimplement the function pixels_to_image\nit takes in one numpy array and convert it to a 48*48 2D array. \nIt will also perform the rescale part so that every element in the array is a number between 0 and 1\n(hint: we learned this in lab 6)\n'''\n\ndef pixels_to_image(pixels) -> np.ndarray:\n#   Your code here\n    x_str=pixels.split( )\n    x_int = list(map(int, x_str)) \n    A=np.array(x_int)\n    A=A\/255.0\n    X=A.reshape(48,48)\n    \n    return X\nframe=pixels_to_image(p[0])\nplt.imshow(frame, cmap=plt.cm.binary_r)","14fd256a":"# This is for sanity check\ntest_pixels = df.iloc[0, 1]\ntest_image = pixels_to_image(test_pixels)","49105622":"# your code here\nprint(frame.shape)\nprint(frame.ndim)","b3bd421c":"'''\nimplement the function pixels_to_image\nit takes in one numpy array and convert it to a 1*48*48*1 4D array. \n'''\n\ndef pixels_to_images(pixels) -> np.ndarray:\n    # your code here\n    x_str=pixels.split( )\n    x_int = list(map(int, x_str)) \n    A=np.array(x_int)\n    A=A\/255.0\n    X=A.reshape(1,48,48,1)\n    return X\n","f2169bac":"# sanity check\ntest_image = pixels_to_images(test_pixels)\ntest_image.shape","919ffa21":"# your code here\nX=[]\nfor i in p:\n    X.append(pixels_to_images(i))\n","55568b14":"# your code here\nprint(type(X))\n","be46631f":"# your code here\nprint(type(X[0]))","30586853":"# your code here\nX=np.concatenate(X)","aeeefdbb":"# your code here\nprint(np.shape(X))","cc153bcb":"# your code here\nfrom keras.utils import to_categorical\nY =(df['emotion'])\nprint(type(Y))\n# sanity check\nprint(Y[0])","1c23792f":"# your code here\nlabels=[]\nfor i in Y:\n    labels.append(label_map[i])\n# sanity check","1d0c388f":"# DO NOT CHANGE THIS\nnpy_data = 'numpy_images.npy'\nlabel_data = 'labels.pkl'\n\nif Path(npy_data).is_file() and Path(label_data).is_file():\n    print('Loading from previously processed training data X and labels Y')\n    with open(npy_data, 'rb') as f:\n        X = np.load(f)\n    with open(label_data, 'rb') as f:\n        Y = pickle.load(f)\nelse:\n    print('No training numpy array saved previously. Saving variable X to file.')\n    with open(npy_data, 'wb') as f:\n        np.save(f, X)\n    with open(label_data, 'wb') as f:\n        pickle.dump(Y, f)","c7739591":"# your code here\n\na=np.random.choice(len(Y),20)\nprint(a)\n\n\n#plt.imshow(frame, cmap=plt.cm.binary_r)\nplt.figure()\nfor i in range(len(a)):\n    frame=X[a[i], :, :, :].reshape(48,48)\n    plt.subplot(4,5,i+1)\n    plt.title(labels[a[i]])\n    plt.imshow(frame, cmap=plt.cm.binary_r)\nplt.show()\n\n","95c2099a":"# your code here\n\n#titanic = sns.load_dataset\nsns.set(style=\"darkgrid\")\nmydata={'emotion':labels}\nsns.countplot(x=\"emotion\", data=mydata)\n# for i in X:\n#     myx=i.reshape(48,48)\n#     a=myx.mean()\n#     print(a)\n\n# plt.show()","02a9cba1":"# your code here\nX_train,X_val,y_train,y_val=train_test_split(X, Y, test_size=0.1, random_state=42)","851da0c2":"# DO NOT CHANGE ANYTHING IN THIS CELL\ndef generate_id(digest_size=4):\n    hash = hashlib.blake2b(digest_size=digest_size)\n    hash.update(str(time.time()).encode('utf-8'))\n    return hash.hexdigest()","731b0d61":"# DO NOT CHANGE ANYTHING IN THIS CELL\ndef write_array_to_image_files(X, y, resolution, dataset_name):\n    arrays = []\n    for arr in X[:,:,:,0]:\n        arrays.append(np.array(Image.fromarray(arr).resize(size=(resolution, resolution))))\n    resized_array = np.array(arrays)\n    resized_array = np.expand_dims(resized_array, axis=-1)\n    resized_array = np.concatenate([resized_array, resized_array, resized_array], axis=-1)\n    \n    # First create the directories that corresponds to the target class name\n    for k in set(y):\n        dir_path = Path(f'dataset\/{dataset_name}\/{str(k)}')\n        dir_path.mkdir(parents=True, exist_ok=True)\n        \n    # Write each image into the target class folder\n    # Remember to multiple by 255 as our array data is normalized to 1\n    for arr, i in zip(resized_array, y):\n        im = Image.fromarray(np.uint8(arr*255))\n        im.save(f'dataset\/{dataset_name}\/{str(i)}\/{generate_id()}.jpg')","d303673e":"# Creates a graph.\n\nimport pickle\npath='labels.pkl'   #path='\/root\/\u2026\u2026\/aus_openface.pkl'   pkl\u6587\u4ef6\u6240\u5728\u8def\u5f84\n\nf=open(path,'rb')\ndata=pickle.load(f)\nprint(data)\n","87bc8413":"### We are done with data processing!! You can move on to the \"Training\" notebook for model training!","c6a0a8c3":"### STEP 2\n\nHow many values are in each \"pixels\" row? Write some code to figure this out!","36c06702":"What is the data type of the first element of X?","01ea3a8b":"## Part 1: Data Preprocessing\nIn this part, we will prepare the data to be ready for training (next part). There are 7 total steps. You will be performing a series of techniques which take in a csv file of pixels to jpg files for each image. We will also perform two parallel tasks. One is to transform all csv pixel data into 48x48 greyscale images that are ready for ConvNet training (part 2). The other is to transform all csv pixel data into 224x224 RGB images that are ready for transfer learning (part 4). \nYou will also perform the following techniques: \nPeek into data\nNumpy array manipulations\nWrite numpy array to image files with different resolution\n\n### Step 0: library import","e0d3149f":"What is the shape the resulting array?\n\nIs it two dimensional or three dimensional?","4146cd71":"### Training and validation split\n\nBefore we can train and evaluate our model, we need to first split our X, and Y variables into training and validation dataset.\n\nWrite some code below to do so.\n\nYou should end up with 4 variables, where the validation dataset contains 10% of the total data.\n\n*X_train*\n\n*X_val*\n\n*y_train*\n\n*y_val*\n","27f49eff":"Apply your function to the \"pixels\" data series to obtain a list of numpy image arrays \"X\"","ab454022":"### STEP 4","8e07996b":"### STEP 3\n\nWrite a function to convert each \"pixels\" string into a numpy array of the correct shape. ","2ac0f04f":"### STEP 7\n\nWrite your own data visualization or analysis code to further explore the data.\n\nYou can choose to answer some of these questions\n\n1. How is the data distributed across each of the emotions? Which emotion has the most images? Which emotion has the least?\n\n2. Are there particular emotion images that may be incorrect or ambiguous? Correcting them may give you better results when applying the model to the real world.\n\n3. What is the average pixel \"brightness\" in these images? Are there outliers that need to be adjusted?","4772e8da":"### STEP 5a\n\nJust having the images are not enough for training. We also need the emotion labels that correspond to each image.\n\nCreate a variabe *Y* which is an INTEGER version of the df dataframe column \"emotion\"","c6946457":"### STEP 1 \nLoad the raw dataset and take a look to understand what we are working with. The file name is `train.csv`","0a7dfc91":"### STEP 8\n\nBecause the training images for this dataset are small (48x48) and we do not have a lot of them, it is ok for now to load all the images into the computer memory while training (next up).\n\nHowever, if we start working with a large dataset of bigger images (for example 240x240), you may quickly run out of memory if you try to read them all into memory.\n\nFor this reason, it is better to save the images as files, and read them into memory as they are needed during training (using what's called a `generator` function, we highly encourage you to look up what this is).\n\nIn this step, you'll write the files to disk as grayscale images, with two different resolutsion (48x48) and (224x224).\n\nThe majority of this step is already provided to you. There are two functions that's already implemented: `generate_id` and `write_array_to_image_files`. You need to understand what these two functions do. After that you will use these two functions to accomplish the task listed above. \n","45feb8e6":"What is the data type of X?","66e37526":"### STEP 6\n\nSo far we've only processed the raw data, but have not even looked at how the images appear yet.\n\nLet's write some visualization code to see some *random* images and their emotion labels, and plot them on a grid of 4 x 5 table.\n\nYou can use the `np.random.choice` function to draw random images. ","d04a6fe3":"Verify that the shape of *X* is (32298, width, height, 1)\n\nThe first dimension is each of the images (32298 of them)\n\nThe second and third dimensions are the width and height\n\nThe last dimension if the number of color channels = 1","bd57049a":"Integer labels are useful for model training (as each number represents an output class), however, it is not as intuitive to quickly view the images with the corresponding expressions.\n\nCreate a variable called *labels* from the variable *Y*, which maps the integer class to a emotion name (given in *label_map*)","2740a823":"We actually want a four dimensional array (1, height, width, channels) where \"channels\" is the number of color channels.\n\nFor a grayscale image, the chanell will simply be 1. For RGB image, this will be 3.\n\nWe would like to have a four dimensional array because later we will be concatenating the arrays together from all images into one single numpy array. The concatenation will occur on the first dimension.\n\nCan you update the pixels_to_image function to reshape the image array into a (1, height, width, 1) array for grayscale?","c43fd260":"Finally convert this list of numpy arrays into a single numpy array, by using `np.concatenate`. \nYou can look up what `np.concatenate` does yourself. ","a79e4a75":"*emotion* - contains the values 0 - 6, and maps to the labels in *label_map* (in order)\n\n*pixels* - a STRING containing the raw grayscale value to s SQUARE image\n\n#### Question: what do the pixels column look like? What is the data type? ","d1c56b6e":"### STEP 5b\n\nProcessing the images above took a bit of time. It might be somewhat annoying to repeat this every time you rerun this notebook.\n\nTo save time, we will save our final numpy array in a file, and just load it in the next time we run the notebook, skipping everything above (except the imports)!\n\nIn the following code, if 'dataset\/numpy_images.npy' exists, then we load it in as variable *X*.\n\nIf it does not exists, we need to write *X* to the file (make sure you have ran the code above to get numpy array *X*)\n\nNote you don't have to do anything here. Try your best to understand these code. ","bcece0d4":"Check the shape of each of the above variables to make sure they are reasonable","4490e09a":"## Data Preprocessing\n\nThe training and testing images are provided to us as raw values in a csv file column called \"pixels\" and not as raw image files.\n\nWe need to preprocess this data such that the string raw values become a numpy array representing a grayscale image.\n\nEach row of the data file is one image, with corresponding label \"emotion\", and a \"usage\" of what the image is to be used for.\n\nIn the following section, we will work with the column \"pixels\" and turn it into numpy arrays that can be used for training the model."}}