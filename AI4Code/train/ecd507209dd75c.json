{"cell_type":{"c8fae082":"code","dba450b4":"code","7f89dab5":"code","aa4ff28a":"code","16a23555":"code","104472ae":"code","344a1877":"code","158e1a6d":"code","f2a0ae4a":"code","83b7bc55":"code","57a50b8f":"code","19e8943a":"code","42138fd6":"code","fcf45a6d":"code","709099ee":"code","f39dca07":"code","1e3a3bbf":"code","aa9db8a3":"code","256285d0":"code","7cedaf8d":"code","88050ea5":"code","306be901":"code","460bcc3e":"code","04a930bd":"code","f7af5c42":"code","4e818aec":"code","07434797":"code","2f81f622":"code","9467d491":"code","315b6d05":"code","d2123e86":"code","ea56a5b5":"code","13493431":"code","611a5d0a":"code","0bceaacc":"code","07ec6a0f":"code","0ee97d2c":"code","40eff860":"code","55a9ac19":"code","43554116":"code","2f5fe3a1":"code","cedc5676":"markdown","49ecaae3":"markdown","fa0b1b07":"markdown","bb9015fd":"markdown","6688c52c":"markdown","04ac244a":"markdown","25e0e70c":"markdown","63852af2":"markdown","428b20be":"markdown","e6b5670e":"markdown","bc2d0653":"markdown","ddd0ad9a":"markdown","ee4f795b":"markdown","ff5d832c":"markdown","402857f4":"markdown","4a9be6c1":"markdown","623ee83b":"markdown","1ba10580":"markdown","78fc618d":"markdown","cfb54351":"markdown","eb318acc":"markdown","458f274d":"markdown","59332826":"markdown","3fb6c9bc":"markdown","d62296f9":"markdown","8f3e8458":"markdown","65ab8bc9":"markdown","c6e83db4":"markdown"},"source":{"c8fae082":"#Model ID\nModelId='housing_price_FML_v1'\n\n#Setting the model target variable name\nvar_target = 'SalePrice'\n\n#process outputs such as MOJO model, images and performance of tested models\nOutputPath='\/kaggle\/working'\n\n#If you have a huge dataset, I should consider use a small sample for first execution\npct_sample_size = 1","dba450b4":"import glob\nimport functools\nimport datetime as dt\nimport pandas as pd\nimport numpy as np\nimport h2o\nimport matplotlib.pyplot as plt\nimport shap\nfrom pandas_profiling import ProfileReport\nfrom collections import defaultdict\nfrom pandas_profiling.model.base import get_var_type\nimport seaborn as sns\nimport os\nimport random\nfrom bayes_opt import BayesianOptimization\nfrom h2o.estimators import H2OXGBoostEstimator\nfrom h2o.estimators.gbm import H2OGradientBoostingEstimator","7f89dab5":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","aa4ff28a":"#Import bases with features for modeling\n#In this case we will use house price dataset available below\ndataprep_df_full = pd.read_csv('\/kaggle\/input\/home-data-for-ml-course\/train.csv')\n\n#The target variavle must be float 0 or 1\ndataprep_df_full['SalePrice'] = dataprep_df_full['SalePrice'].astype(float)\n\ndataprep_df_full.dtypes","16a23555":"random.seed(59354518745)\nfor i in range(len(dataprep_df_full)):\n    dataprep_df_full.loc[i, ('random')] = random.random()\ndataprep_df_full['dataset'] = ['train' if x <= 0.85 else 'test' for x in dataprep_df_full['random']]","104472ae":"#Work with a sample data if the pct_sample_size is less than 1\nif pct_sample_size == 1:\n    dataprep_df = dataprep_df_full\nelse:    \n    dataprep_df = dataprep_df_full.sample(frac=pct_sample_size, replace=False, random_state=1)","344a1877":"dataprep_df['OpenPorchSFX2'] = np.square(dataprep_df['OpenPorchSF'])\ndataprep_df['WoodDeckSFX2'] = np.square(dataprep_df['WoodDeckSF'])","158e1a6d":"#Generate report\n#If the database has many records or columns, the report can take a long time\n#If this is the case, disable the explorative, samples, correlations, missing_diagrams, duplicates and interactions options by commenting out\nprofile = ProfileReport(dataprep_df, title=f\"Pandas Profiling Report{ModelId}\"\n                        ,explorative=True\n                        ,samples=None\n                        ,correlations=None\n                        ,missing_diagrams=None\n                        ,duplicates=None\n                        ,interactions=None\n                       )\nprofile.to_file(\"profile.html\")\ndisplay(profile)","f2a0ae4a":"# Get all the types pandas_profiling offers\nlist_columns = dataprep_df.columns.drop('dataset').drop('SalePrice')\nd = {col: get_var_type(dataprep_df[col])['type'].value for col in list_columns}\nfd = defaultdict(list)\nfor k, v in d.items():\n    fd[v].append(k)\n     \ncols_by_base_type = dict(fd)\n# Group the types pandas_profiling offers to match typical needs\ncat_num_cols = defaultdict(list)\nfor k, v in cols_by_base_type.items():\n    # Treat boolean and unique columns as categorical\n    k = 'CAT' if k in ['BOOL', 'UNIQUE'] else k\n    cat_num_cols[k].extend(v)\ndict(cat_num_cols)","83b7bc55":"#It is necessary to define the types of variables (cageroric and numeric) to ensure that the type of data used in the modeling will be the most suitable.\n#For example, categorical variables need to be defined as a string because this prevents it from being treated as a numeric variable in H20 modeling\n#Another example is that the string variables will have a missing treatment by placing the missing category for all values found as 'null'\nCAT = ['MSZoning',\n  'Street',\n  'Alley',\n  'LotShape',\n  'LandContour',\n  'Utilities',\n  'LotConfig',\n  'LandSlope',\n  'Neighborhood',\n  'Condition1',\n  'Condition2',\n  'BldgType',\n  'HouseStyle',\n  'RoofStyle',\n  'RoofMatl',\n  'Exterior1st',\n  'Exterior2nd',\n  'MasVnrType',\n  'ExterQual',\n  'ExterCond',\n  'Foundation',\n  'BsmtQual',\n  'BsmtCond',\n  'BsmtExposure',\n  'BsmtFinType1',\n  'BsmtFinType2',\n  'Heating',\n  'HeatingQC',\n  'Electrical',\n  'BsmtFullBath',\n  'BsmtHalfBath',\n  'FullBath',\n  'HalfBath',\n  'KitchenAbvGr',\n  'KitchenQual',\n  'Functional',\n  'Fireplaces',\n  'FireplaceQu',\n  'GarageType',\n  'GarageFinish',\n  'GarageQual',\n  'GarageCond',\n  'PavedDrive',\n  'PoolQC',\n  'Fence',\n  'MiscFeature',\n  'SaleType',\n  'SaleCondition',\n  'CentralAir']\n#float\nNUM = ['MSSubClass',\n  'LotFrontage',\n  'LotArea',\n  'OverallQual',\n  'OverallCond',\n  'YearBuilt',\n  'YearRemodAdd',\n#   'MasVnrArea',\n#   'BsmtFinSF1',\n  'BsmtFinSF2',\n#   'BsmtUnfSF',\n  'TotalBsmtSF',\n  '1stFlrSF',\n  '2ndFlrSF',\n  'LowQualFinSF',\n#   'GrLivArea',\n  'BedroomAbvGr',\n#   'TotRmsAbvGrd',\n  'GarageYrBlt',\n  'GarageCars',\n#   'GarageArea',\n  'WoodDeckSF',\n#   'OpenPorchSF',\n#   'EnclosedPorch',\n  '3SsnPorch',\n  'ScreenPorch',\n#   'PoolArea',\n#   'MiscVal',\n'MoSold',\n'OpenPorchSFX2',\n'WoodDeckSFX2'\n      ]\nselected_features = CAT + NUM","57a50b8f":"#Numeric features must be float type\nfor col_name in NUM:\n    dataprep_df[col_name] = dataprep_df[col_name].astype(float)\n\n#Categorical features must be string type and null values will be filled with \"missing\"\nfor col_name in CAT:\n    dataprep_df[col_name] = dataprep_df[col_name].astype(str)\n    dataprep_df = dataprep_df.fillna(value={col_name: 'missing'})","19e8943a":"# Number of threads, nthreads = -1, means use all cores on your machine\n# max_mem_size is the maximum memory (in GB) to allocate to H2O\nh2o.init(nthreads = -1, max_mem_size = 8)","42138fd6":"#Import TRAINING base to the H20 context\ndata_hdf = h2o.H2OFrame(dataprep_df.query('dataset == \"train\"'))\n\n# Conversion of Target variables and categorical features to factor (enum)\n#no H2O it is necessary that the categorical variables are transformed into a factor\nfor col_name in CAT:\n    data_hdf[col_name] = data_hdf[col_name].asfactor()    \n    \n# Partition data into 90%, 10% chunks\n# Setting a seed will guarantee reproducibility\ntrain_hdf, valid_hdf = data_hdf.split_frame(ratios=[0.90], destination_frames=['train_hdf', 'valid_hdf'], seed=1)\n        \n#Notice that `split_frame()` uses approximate splitting not exact splitting (for efficiency), so these are not exactly 90%, 10% of the total rows.\nprint('Training: ' + str(train_hdf.nrow))\nprint('Validation: ' + str(valid_hdf.nrow))","fcf45a6d":"#Import TEST base to the H20 context\ntest_hdf = h2o.H2OFrame(dataprep_df.query('dataset == \"test\"'))\n\n# Conversion of Target variables and categorical features to factor (enum)\n#no H2O it is necessary that the categorical variables are transformed into a factor\nfor col_name in CAT:\n    test_hdf[col_name] = test_hdf[col_name].asfactor()    \n    \nprint('Training: ' + str(test_hdf.nrow))","709099ee":"#Bayesian Optimization for GBM\nbounds = {\n    'max_depth':(5,20),\n    'ntrees': (50,500),\n    'min_rows':(10,35),\n    'learn_rate':(0.001, 0.2),\n    'sample_rate':(0.5,0.9),\n    'col_sample_rate':(0.2,0.8)\n}\n\ndef train_model(max_depth, \n                ntrees,\n                min_rows, \n                learn_rate, \n                sample_rate, \n                col_sample_rate):\n    params = {\n        'max_depth': int(max_depth),\n        'ntrees': int(ntrees),\n        'min_rows': int(min_rows),\n        'learn_rate':learn_rate,\n        'sample_rate':sample_rate,\n        'col_sample_rate':col_sample_rate\n    }\n    \n    model = H2OGradientBoostingEstimator(seed=1, nfolds=5,**params)\n    model.train(x=selected_features, y=var_target, training_frame=train_hdf, validation_frame=valid_hdf)\n    return -model.rmse()\n\noptimizer = BayesianOptimization(\n    f=train_model,\n    pbounds=bounds,\n    random_state=1,\n)\noptimizer.maximize(init_points=10, n_iter=100)","f39dca07":"vModel='GBM_'\n\n#Execution time of the model\nstart = dt.datetime.now()\n\nparams = {\n    'max_depth': int(optimizer.max['params']['max_depth']),\n    'ntrees': int(optimizer.max['params']['ntrees']),\n    'min_rows': int(optimizer.max['params']['min_rows']),\n    'learn_rate':optimizer.max['params']['learn_rate'],\n    'sample_rate':optimizer.max['params']['sample_rate'],\n    'col_sample_rate':optimizer.max['params']['col_sample_rate']    \n    }\n\nGBM = H2OGradientBoostingEstimator(model_id='%s%s%s' % (vModel, ModelId, str(dt.datetime.now())[:19].replace('-',\"\").replace(':',\"\").replace(' ',\"_\")),\n                                   seed=1,\n                                   nfolds=5,\n                                   **params\n                                  )\n\n# The use of a validation_frame is recommended with using early stopping\nGBM.train(x=selected_features, y=var_target, training_frame=train_hdf, validation_frame=valid_hdf)\n\n#Execution time of the model\nstop = dt.datetime.now()\nexecution_time = stop-start\nprint(\"\\n\"+ \"Execution time: \" + str(execution_time) + \"\\n\")\nprint(GBM)","1e3a3bbf":"#Bayesian Optimization for GBM\nbounds = {\n    'max_depth':(5,20),\n    'ntrees': (50,500),\n    'min_rows':(10,35),\n    'learn_rate':(0.001, 0.2),\n    'sample_rate':(0.5,0.9),\n    'col_sample_rate':(0.2,0.8)\n}\n\ndef train_model(max_depth, \n                ntrees,\n                min_rows, \n                learn_rate, \n                sample_rate, \n                col_sample_rate):\n    params = {\n        'max_depth': int(max_depth),\n        'ntrees': int(ntrees),\n        'min_rows': int(min_rows),\n        'learn_rate':learn_rate,\n        'sample_rate':sample_rate,\n        'col_sample_rate':col_sample_rate\n    }\n    \n    model = H2OXGBoostEstimator(seed=1, nfolds=5,**params)\n    model.train(x=selected_features, y=var_target, training_frame=train_hdf, validation_frame=valid_hdf)\n    return -model.rmse()\n\noptimizer = BayesianOptimization(\n    f=train_model,\n    pbounds=bounds,\n    random_state=1,\n)\noptimizer.maximize(init_points=10, n_iter=100)","aa9db8a3":"vModel='XGBoost_cv_'\n\n#Execution time of the model\nstart = dt.datetime.now()\n\nparams = {\n    'max_depth': int(optimizer.max['params']['max_depth']),\n    'ntrees': int(optimizer.max['params']['ntrees']),\n    'min_rows': int(optimizer.max['params']['min_rows']),\n    'learn_rate':optimizer.max['params']['learn_rate'],\n    'sample_rate':optimizer.max['params']['sample_rate'],\n    'col_sample_rate':optimizer.max['params']['col_sample_rate']    \n    }\n\nXGBoost_cv = H2OXGBoostEstimator(model_id='%s%s%s' % (vModel, ModelId, str(dt.datetime.now())[:19].replace('-',\"\").replace(':',\"\").replace(' ',\"_\")),\n                                   seed=1,\n                                   nfolds=5,\n                                   **params\n                                  )\n\n# The use of a validation_frame is recommended with using early stopping\nXGBoost_cv.train(x=selected_features, y=var_target, training_frame=train_hdf, validation_frame=valid_hdf)\n\n#Execution time of the model\nstop = dt.datetime.now()\nexecution_time = stop-start\nprint(\"\\n\"+ \"Execution time: \" + str(execution_time) + \"\\n\")\nprint(XGBoost_cv)","256285d0":"#Create empty model list\nlist_models = []\n\n#Define the list of all models that have been executed and should be compared\ntry:\n    list_models.append(GBM)\nexcept NameError:\n    GBM = None\ntry:\n    list_models.append(XGBoost_cv)\nexcept NameError:\n    XGBoost_cv = None","7cedaf8d":"#Compare performance on the TEST dataset for all trained models\nplt.rcParams.update({'font.size': 12})\nfig = plt.figure(figsize=(10, 10))\nfor i in list_models:\n    #Save all models in H20 format\n    h2o.save_model(model=i, path='%s\/models\/todos\/' % OutputPath, force=True)\n    \n    #Ascertain the performance of all models on the test base\n    performance = i.model_performance(test_hdf)\n    \n    #Salve metrics\n    f=open(\"%s\/models\/todos\/performance_%s.csv\" % (OutputPath, i.model_id), 'w')\n    f.write(\n        str(i.model_id) + \";\"        \n        + str(performance.mae()) + ';'\n        + str(performance.rmse()) + ';'\n        + str(performance.r2()))\n    f.write('\\n')\n    f.close()\n    \n    if i.model_id==list_models[0].model_id:\n        df_plot = pd.DataFrame({'Model_id': i.model_id.split(\"_\")[0]+\"_\"+i.model_id.split(\"_\")[1]+\"_\"+i.model_id.split(\"_\")[2],\n                                    'MAE': int(performance.mae()*100)\/100,\n                                    'RMSE': int(performance.rmse()*100)\/100,\n                                    'R2': int(performance.r2()*100)\/100\n                                    }, index=[0])\n    else:\n        df_plot = df_plot.append(pd.DataFrame({'Model_id': i.model_id.split(\"_\")[0]+\"_\"+i.model_id.split(\"_\")[1]+\"_\"+i.model_id.split(\"_\")[2],\n                                    'MAE': int(performance.mae()*100)\/100,\n                                    'RMSE': int(performance.rmse()*100)\/100,\n                                    'R2': int(performance.r2()*100)\/100\n                                    }, index=[0]))\nax = df_plot.plot(kind='bar', x=\"Model_id\", title=\"MAE, RMSE e R2 for Model (Test dataset)\", grid=True, figsize=(10,5), legend=1)\nfor p in ax.patches:\n    ax.annotate(str(p.get_height()), (p.get_x() * 1.005, p.get_height() * 1.005))\nplt.legend(loc=3, prop={'size': 10})","88050ea5":"#Consider all models in the history .\/models\/todos\/performance_*.csv. To disregard any old version, set erase_modelos = \"S\":\napagar_modelos = 'N'\nif apagar_modelos == 'S':\n    os.system('rm %s\/models\/todos\/performance_*.csv' % OutputPath)","306be901":"sort_metric_best_model='MAE'\n#importar todos os modelos testados e imprmie na tela os 10 melhores erdedando per AUC\nmodelos_testados = pd.concat(map(functools.partial(pd.read_csv, sep=';', header=None), glob.glob('%s\/models\/todos\/performance_*.csv' % OutputPath)))\nmodelos_testados.columns = ('model_id', 'MAE', 'RMSE', 'R2')\nmodelos_testados = modelos_testados.sort_values(by=sort_metric_best_model, ascending=True)\nmodelos_testados = modelos_testados.drop_duplicates(subset=[\"model_id\"])\nprint('MBest Models. Sorted by : ' + str(sort_metric_best_model))\nmodelos_testados.reset_index(0).head(30)","460bcc3e":"#If you want to choose a model other than the first one on the list. Choose the position number:\nposicao_melhor_modelo=0\n\nmelhor_modelo = h2o.load_model('%s\/models\/todos\/%s' % (OutputPath, modelos_testados.iloc[posicao_melhor_modelo, 0]))\n(print(\"\\n\"+ \"BEST MODEL: \" + str(modelos_testados.iloc[posicao_melhor_modelo, 0]) + \"\\n\"))\n\nplt.rcParams.update({'font.size': 10})\ntry:\n    melhor_modelo.varimp_plot(50)\nexcept Exception as e:\n    print(\"Warning: This model doesn't have variable importances\")","04a930bd":"#Listar todas as vari\u00e1veis do modelo atual, ordenadas por variable importance\n#Para as variaveis definidas como fator (que possivelmente est\u00e3o como dummys), remover a categoria do nome e deixar apenas o nome orifinal da variavel\n\n#List all variables in the current model, ordered by variable importance\n#For variables defined as a factor (which possibly are like dummys), remove the category from the name and leave only the orifinal name of the variable\ntry:\n    df_features_sorted = melhor_modelo.varimp(True).variable.str.split('.', expand=True).drop_duplicates(subset = 0)[0].reset_index(drop=True)\nexcept Exception as e:\n    #As the model with ensemble in H20 does not show the importance of variables, we will include variables with higher IV first using result_formatado graph of step 5.1\n    df_features_sorted = result_formated_graph.Variable.reset_index(drop=True)","f7af5c42":"#Define the number of variables to be increased with each new model. Try to put 10% or 20% of the total, as it can take a long time\nqt_var=1\nqt_total_var = len(df_features_sorted)\n\ndict_model_tmp={}\ndict_performance={}\n\nfor i in range(qt_var, qt_total_var+qt_var, qt_var):    \n    df_features_sorted[0:i].values.tolist()    \n    \n    #If no model chosen is not an ensemble of models. Then use the same model for training with increment of variables\n    melhor_modelo_tmp = melhor_modelo\n    if melhor_modelo_tmp.model_id.lower().find(\"ensemble\") == -1:\n        dict_model_tmp[i] = melhor_modelo_tmp\n        dict_model_tmp[i].train(x = df_features_sorted[0:i].values.tolist(),\n                                y = var_target,\n                                training_frame=train_hdf, \n                                validation_frame=valid_hdf)\n    ##If it is not possible, for the home of an ensemble of models, use GradientBoostingEstimator to make the assessment\n    else:\n        dict_model_tmp[i] = H2OGradientBoostingEstimator(seed=1, model_id=str('model_tmp_%s' % i))\n        dict_model_tmp[i].train(x = df_features_sorted[0:i].values.tolist(),\n                                y = var_target,\n                                training_frame=train_hdf, \n                                validation_frame=valid_hdf)       \n\n\n    perform_oot = dict_model_tmp[i].model_performance(test_hdf)\n    dict_performance_tmp = {}\n    dict_performance_tmp['MAE'] = {'qt_var': i, 'medida': 'MAE', 'Validation_Dataset': dict_model_tmp[i].mae(valid=True), 'Test_Dataset': perform_oot.mae()}\n    dict_performance_tmp['RMSE'] = {'qt_var': i, 'medida': 'RMSE', 'Validation_Dataset': dict_model_tmp[i].rmse(valid=True), 'Test_Dataset': perform_oot.rmse()}\n    dict_performance_tmp['R2'] = {'qt_var': i, 'medida': 'R2', 'Validation_Dataset': dict_model_tmp[i].r2(valid=True), 'Test_Dataset': perform_oot.r2()}\n    dict_performance[i] = pd.DataFrame(dict_performance_tmp).transpose()","4e818aec":"##Plot graph comparing the increase in performance with the increase in variables\nfor i in dict_performance.keys():\n    if i == list(dict_performance.keys())[0]:\n        df_performance = dict_performance[i]\n    else:\n        df_performance = df_performance.append(dict_performance[i], ignore_index=True)\n\nlista_metricas_perf = df_performance['medida'].unique()\n\nfor i in range(len(lista_metricas_perf)):   \n    #selects only the metric to be analyzed\n    metrics_df_tmp = df_performance.query('medida == \"%s\"' % lista_metricas_perf[i])\n    metrics_df_tmp = metrics_df_tmp.set_index('qt_var')\n    del metrics_df_tmp['medida']\n    if lista_metricas_perf[i] == 'R2':\n        max_oot = metrics_df_tmp[metrics_df_tmp['Test_Dataset'] == metrics_df_tmp.Test_Dataset.max()].index.values\n    else:\n        max_oot = metrics_df_tmp[metrics_df_tmp['Test_Dataset'] == metrics_df_tmp.Test_Dataset.min()].index.values\n        \n    if lista_metricas_perf[i] == sort_metric_best_model:\n        max_oot_filtro = max_oot[0]        \n    \n    ax=metrics_df_tmp.plot(figsize=(15,5), linewidth=2, fontsize=10, marker='D', ms=5,\\\n                            title='Best %s with %s Variables' % (lista_metricas_perf[i].upper(), str(max_oot[0])))\n    plt.xlabel('Variables Number')\n    plt.ylabel('%s' % lista_metricas_perf[i].upper())\n    plt.grid(axis='y')\n    plt.legend(loc=0, prop={'size': 12})\n    #display(ax)","07434797":"print('Consider removing the following variables: '+ str(df_features_sorted[df_features_sorted.index > int(max_oot_filtro)].values.tolist()))","2f81f622":"#Save the H2O model in MOJO format and all the variables of the best model\nmelhor_modelo = h2o.load_model('%s\/models\/todos\/%s' % (OutputPath, modelos_testados.iloc[posicao_melhor_modelo, 0]))\ncaminho_modelo_mojo = melhor_modelo.download_mojo('%s\/models\/melhores\/' % OutputPath, get_genmodel_jar=True)\nprint(caminho_modelo_mojo)\ncaminho_modelo_h2o = h2o.save_model(model=melhor_modelo, path='%s\/models\/melhores\/' % OutputPath, force=True)","9467d491":"try:\n    features_names= melhor_modelo.varimp(True)\n    features_names.to_csv('%s\/models\/melhores\/features_names_%s.csv' % (OutputPath, melhor_modelo.model_id), sep=';')\nexcept Exception as e:\n    print(\"Warning: This model doesn't have variable importances\")","315b6d05":"class H2oProbWrapper:\n    def __init__(self, h2o_model, feature_names):\n        self.h2o_model = h2o_model\n        self.feature_names = feature_names\n    def predict_binary_prob(self, X):\n        if isinstance(X, pd.Series):\n            X = X.values.reshape(1,-1) \n        self.dataframe = pd.DataFrame(X, columns=self.feature_names)\n        \n        global NUM\n        #Variaveis explicativas continuas\n        for col_name in NUM:    \n            self.dataframe[col_name] = self.dataframe[col_name].astype(float)\n            \n        global CAT\n        for col_name in CAT:    \n            self.dataframe[col_name] = self.dataframe[col_name].astype(str)\n            self.dataframe = self.dataframe.fillna(value={col_name: 'missing'})\n        \n        self.h2oframe = h2o.H2OFrame(self.dataframe)\n        for col_name in CAT:\n            self.h2oframe[col_name] = self.h2oframe[col_name].asfactor()\n        \n        self.predictions = self.h2o_model.predict(self.h2oframe).as_data_frame().values\n        return self.predictions.astype('float64') [:,-1]","d2123e86":"#The calculation of the Shapley Value for H20 models takes a while. So it will only be done for 20 records. Increase the sample to deepen your analysis\nshap_sample = dataprep_df.query('dataset == \"test\"').loc[:,(selected_features)].sample(n=20, replace=False, random_state=1)\nshap_sample = shap_sample.fillna(0)","ea56a5b5":"h2o_wrapper = H2oProbWrapper(melhor_modelo, selected_features)\nh2o_explainer = shap.KernelExplainer(h2o_wrapper.predict_binary_prob, shap_sample)\nh2o_shap_values = h2o_explainer.shap_values(shap_sample, nsamples=\"auto\")","13493431":"fig = shap.summary_plot(h2o_shap_values, shap_sample, plot_type=\"bar\", show=True)\ndisplay(fig)","611a5d0a":"display(shap.summary_plot(h2o_shap_values, shap_sample, show=False))","0bceaacc":"#sort the features indexes by their importance in the model\n#(sum of SHAP value magnitudes over the validation dataset)\ntop_inds = np.argsort(-np.sum(np.abs(h2o_shap_values),0))\n\n#make SHAP plots of the three most important features\nfor i in range(9):\n    fig=shap.dependence_plot(top_inds[i], h2o_shap_values, shap_sample, show=False)\n#     display(fig)","07ec6a0f":"df_shap_values = pd.DataFrame(h2o_shap_values)\ndf_shap_values['sum_shap'] = df_shap_values.sum(axis=1)","0ee97d2c":"for i in df_shap_values.sort_values(by='sum_shap').iloc[0:3,:].index.values:\n    fig = shap.force_plot(h2o_explainer.expected_value, h2o_shap_values[i,:], shap_sample.iloc[i,:], matplotlib=True, show=True)\n    display(fig)","40eff860":"for i in df_shap_values.sort_values(by='sum_shap').iloc[0:3,:].index.values:\n    fig = shap.plots._waterfall.waterfall_legacy(h2o_explainer.expected_value, h2o_shap_values[i,:], shap_sample.iloc[i,:].to_numpy(), selected_features, show=True)\n    display(fig)","55a9ac19":"submission_df = pd.read_csv('\/kaggle\/input\/home-data-for-ml-course\/test.csv')\n\nsubmission_df['OpenPorchSFX2'] = np.square(submission_df['OpenPorchSF'])\nsubmission_df['WoodDeckSFX2'] = np.square(submission_df['WoodDeckSF'])\n\n#Numeric features must be float type\nfor col_name in NUM:    \n    submission_df[col_name] = submission_df[col_name].astype(float)    \n\n#Categorical features must be string type and null values will be filled with \"missing\"\nfor col_name in CAT:        \n    submission_df[col_name] = submission_df[col_name].astype(str)    \n    submission_df = submission_df.fillna(value={col_name: 'missing'}) ","43554116":"#Importar MOJO\ntry:\n    test_tmp = h2o.mojo_predict_pandas(submission_df, caminho_modelo_mojo)    \n    predict_df = submission_df.merge(test_tmp, left_index=True, right_index=True)\nexcept:    \n    submission_hdf = h2o.H2OFrame(submission_df)\n    for col_name in CAT:\n        submission_hdf[col_name] = submission_hdf[col_name].asfactor() \n    h2o_predict = melhor_modelo.predict(submission_hdf)\n    predict_df = h2o_predict.cbind(submission_hdf).as_data_frame()\n    \npredict_df.rename(columns={'predict':'SalePrice'}, inplace=True)\npredict_df = predict_df.reset_index(drop=True)\npredict_df.loc[:, ('Id', 'SalePrice')]","2f5fe3a1":"predict_df.loc[:, ('Id', 'SalePrice')].to_csv('\/kaggle\/working\/house_prices_submission.csv', index=False)","cedc5676":"# 6. Modeling","49ecaae3":"##### For more details on the pandas profiling library see https:\/\/github.com\/pandas-profiling\/pandas-profiling\n","fa0b1b07":"## 6.4 Choose the best model among all tested","bb9015fd":"## 3.1 Feature Engineering","6688c52c":"# This notebook will help you to do:\n* Import training and test data\n* Univariate Analysis\n* Bivariate Analysis\n* Run H2O XGBoost algorithm with Bayesian Optimization\n* Run H2O GBM algorithm with Bayesian Optimization\n* Check models performance in validation dataset\n* Interpret model output with Shapley Value\n* Generate csv submission file\n\nI\u2019m going to use H2O.ai and the python package bayesian-optimization developed by \nFernando Nogueira. The goal is to optimize the hyperparameters of a regression model using GBM and XGBoost as our machine learning algorithms.\nSee this link if you want to know more about [Bayesian Optimization](https:\/\/medium.com\/spikelab\/hyperparameter-optimization-using-bayesian-optimization-f1f393dcd36d)\n\nThis model is comparable to [H2O Auto ML](https:\/\/www.kaggle.com\/maxreis\/house-price-submission-with-h2o). Which one is better? H2O Auto ML or XGBoost\/GBM with Bayesian Optimization?\n\n## The house prices dataset will be used for this demonstration","04ac244a":"## Main SHAP Graphics","25e0e70c":"# 9. Save final dataset with predictions","63852af2":"## 6.3 Compare performance on the TEST dataset for all trained models","428b20be":"## END","e6b5670e":"# 3. Importing Data for Modeling","bc2d0653":"# 7. Calculate Shapley Values using SHAP KernelExplainer for H20 models","ddd0ad9a":"# 8. Predict test dataset using MOJO or H2O Model","ee4f795b":"## 6.4 Stepwise for Analysis of the importance of variables","ff5d832c":"## 4.1 Pandas Profiling","402857f4":"## 6.5 Exporting the best model to Deploy","4a9be6c1":"### It is necessary to create a variable to indicate the records used in training and testing. In this case we will use the random variable, but you can use a date variable for exemple if you have a base with a reference date to fix the training base as an out of time validation.","623ee83b":"# 1. Parameters","1ba10580":"## 6.1 Creating context and H2O and Importing data into the H2O context","78fc618d":"# 5. Classify the types of variables\n#### list all columns to select the ones to be used","cfb54351":"### Shap Force Plot","eb318acc":"#### The SHAP library calculates the Shaley Value of each variable used in the model and shows the individual impact of each variable on the predicted value for each record. To better understand how the SHAP library works, see the link https:\/\/github.com\/slundberg\/shap","458f274d":"### Shap Waterfall Plot","59332826":"### From the variables listed above you can select which  one will be tested in the model and confirm if the correct type is numeric(NUM) or categorical (CAT). Paste the correct information below:","3fb6c9bc":"# 4. Univariate Analysis","d62296f9":"## 6.2 Using H2O to performe many ML algorithms","8f3e8458":"## XGBoost","65ab8bc9":"# 2. Import Libraries","c6e83db4":"## GBM - Gradient Boosting Machine"}}