{"cell_type":{"135de8ea":"code","9a696d40":"code","fab61fe9":"code","f8226326":"code","e2e9a2b8":"code","cafc7a53":"code","7851e0ca":"code","b8211eb6":"code","4aa8e7a1":"code","b3f94da7":"code","0d70e4ac":"code","208d4c74":"code","ddb6a503":"code","2e989b2b":"code","c3d424f0":"code","4168f631":"code","234f2921":"code","19e512b7":"code","a0204b79":"code","fac6fece":"code","2fdeacec":"code","98d9ad88":"code","6574cccd":"code","3f88ad3a":"code","4e09d5c0":"code","82ecc598":"code","32d4a65e":"code","e4d480e8":"code","740103ba":"code","bdc7224a":"code","c7c700e8":"code","cc29d4e1":"code","51b83ceb":"code","e01c935e":"code","b2bf63bd":"code","2a387cd3":"code","be1f7284":"code","e35b32fe":"code","3203ada2":"code","599aacf0":"code","22c28ce1":"code","9219f3d6":"code","ff6e8619":"code","e4eefba8":"code","d1bc4a62":"code","ec6843eb":"code","423a5cc7":"code","c8fea5bb":"code","8ba6e1c0":"code","8aaaeaa8":"code","64ee26ac":"code","b2eb56b8":"code","c8ca9da7":"code","7221b260":"code","fb039cf2":"code","fd3c3800":"code","b4af2a71":"code","87175298":"markdown","b181e8c2":"markdown","2158791e":"markdown","18c611ef":"markdown","fa082c35":"markdown","093ee5b0":"markdown","6e71d00a":"markdown","e9659f45":"markdown","897caade":"markdown","be92ba6d":"markdown","39b1b5eb":"markdown","3f9b89ba":"markdown","4a2da595":"markdown","20e0dfdd":"markdown","3456d411":"markdown","94610831":"markdown","c92d4073":"markdown","5f82b0f6":"markdown","6214e0ba":"markdown","eba06f6a":"markdown","1c93802b":"markdown","4c1f34b0":"markdown","8ab1f6f4":"markdown","52ed93ef":"markdown","e5c2bfa1":"markdown","9589b506":"markdown","2adba88a":"markdown","e11bdded":"markdown","8330476a":"markdown","430a4463":"markdown","0f2d16f5":"markdown","149e3f8f":"markdown","076b391c":"markdown","297cb566":"markdown","87d37068":"markdown","4e039e57":"markdown","8b9633ad":"markdown","a903389a":"markdown","134bcc0d":"markdown","f04b864a":"markdown","edd3b95d":"markdown","f18d566e":"markdown","293dc1c2":"markdown","d4413724":"markdown","c41a19bb":"markdown","cbcb0128":"markdown","bf8c3307":"markdown","373686e1":"markdown","3b66b3d2":"markdown","0ec5f417":"markdown","942971c2":"markdown","2a35fa0c":"markdown"},"source":{"135de8ea":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n#import hvplot.pandas \n%matplotlib inline\n\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import RANSACRegressor\nfrom sklearn.linear_model import Ridge\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.linear_model import Lasso\nfrom sklearn.linear_model import ElasticNet\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import SGDRegressor\n\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\nfrom sklearn.svm import SVR\nimport xgboost as xgb\nimport lightgbm as lgb\n\n## ANN LIBRARY\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Input, Dense, Activation, Dropout\nfrom tensorflow.keras.optimizers import Adam\n\nfrom sklearn import metrics\nfrom sklearn.metrics import mean_squared_error, make_scorer","9a696d40":"train=pd.read_csv(r'\/kaggle\/input\/train-processedcsv\/train_Processed.csv')\ntest=pd.read_csv(r'\/kaggle\/input\/test-processed\/test_Processed.csv')\n\n# Local path\n#train=pd.read_csv(r'.\/Dataset\/train_Processed.csv')\n#test=pd.read_csv(r'.\/Dataset\/test_Processed.csv')\n","fab61fe9":"[features for features in train.columns if train[features].isnull().sum()>1]","f8226326":"train.tail()","e2e9a2b8":"test.tail()","cafc7a53":"## Capture the dependent feature\ntrainY=train[['SalePrice']]\n## drop dependent feature from dataset\ntrainX=train.drop(['SalePrice'],axis=1)","7851e0ca":"#TRAIN TEST SPLIT\n\nX_train, X_test, y_train, y_test = train_test_split(trainX, trainY, test_size=0.3, random_state=42)","b8211eb6":"# DEFINE EVALUATION MATRICS\n\n# Cross val score is whole data score, not the train test splitted data score with kfold=10\ndef cross_val(model):\n    #This is whole set, not the splitted\n    pred = cross_val_score(model, trainX, trainY, cv=10)\n    return format(round(pred.mean(),4),'.4f')\n\n# Here evaluation matrics for splitted test data\ndef print_evaluate(true, predicted):  \n    mae = metrics.mean_absolute_error(true, predicted)\n    mse = metrics.mean_squared_error(true, predicted)\n    rmse = np.sqrt(metrics.mean_squared_error(true, predicted))\n    rmsle = np.sqrt(metrics.mean_squared_log_error(true, predicted)) \n    # This rmsle is the matrics defined in kaggle for this casestudy\n    r2_square = metrics.r2_score(true, predicted)\n    print('MAE:', mae)\n    print('MSE:', mse)\n    print('RMSE:', rmse)\n    print('RMSLE:', rmsle)\n    print('R2 Square', r2_square)\n    print('__________________________________')\n    \n\n# Here evaluation matrics for splitted test data\ndef evaluate(true, predicted):\n    mae = round(metrics.mean_absolute_error(true, predicted),4)\n    mse = round(metrics.mean_squared_error(true, predicted),4)\n    rmse = round(np.sqrt(metrics.mean_squared_error(true, predicted)),4)\n    rmsle = round(np.sqrt(metrics.mean_squared_log_error(true, predicted)) ,4)\n    r2_square =metrics.r2_score(true, predicted)\n    return format(mae,'.4f'), format(mse,'.4f'), format(rmse,'.4f'), format(rmsle,'.4f') ,format(r2_square,'.4f')","4aa8e7a1":"lin_reg = LinearRegression(normalize=True)\nlin_reg.fit(X_train,y_train)","b3f94da7":"# print the intercept\nprint(lin_reg.intercept_)","0d70e4ac":"cf =  pd.DataFrame(lin_reg.coef_ ).T\ncol = pd.DataFrame(X_train.columns, columns=['columns'])\n\ncoeff_val = pd.concat([col,cf], axis=1 )\n\ncoeff_val.columns = ['columns','Coeffecient']\n\ncoeff_val.head()","208d4c74":"pred = lin_reg.predict(X_test)","ddb6a503":"plt.scatter(y_test,pred)\nplt.xlabel('True Values')\nplt.ylabel('Predicted Values')\nplt.show()","2e989b2b":"#Residual distplot\nsns.distplot((y_test-pred),bins=50)\n# here actual - pred will give residul plot in histogram of the residuals and make sure it looks normally distributed","c3d424f0":"test_pred = lin_reg.predict(X_test)\ntrain_pred = lin_reg.predict(X_train)\n\nprint('Test set evaluation:\\n_____________________________________')\nprint_evaluate(y_test, test_pred)\nprint('Train set evaluation:\\n_____________________________________')\nprint_evaluate(y_train, train_pred)","4168f631":"results_df = []\nresults_df = pd.DataFrame(data=[[\"Linear Regression\", *evaluate(y_test, test_pred) , cross_val(LinearRegression())]], \n                          columns=['Model', 'MAE', 'MSE', 'RMSE', 'RMSLE', 'R2 Square', \"Cross Validation\"])\nresults_df","234f2921":"#RANSACRegressor - Robust Regression\n\n#ValueError: Mean Squared Logarithmic Error cannot be used when targets contain negative values.\n\nmodel = make_pipeline(RobustScaler(), RANSACRegressor(base_estimator=LinearRegression(), max_trials=100))\n\n#model = RANSACRegressor(base_estimator=LinearRegression(), max_trials=100)\nmodel.fit(X_train, y_train)\n\ntest_pred = model.predict(X_test)\ntrain_pred = model.predict(X_train)\n\nprint('Test set evaluation:\\n_____________________________________')\nprint_evaluate(y_test, test_pred)\nprint('====================================')\nprint('Train set evaluation:\\n_____________________________________')\nprint_evaluate(y_train, train_pred)","19e512b7":"results_df_2 = pd.DataFrame(data=[[\"Robust Regression\", *evaluate(y_test, test_pred) , 0  ]], \n                            columns=['Model', 'MAE', 'MSE', 'RMSE','RMSLE', 'R2 Square', \"Cross Validation\"])\n\nresults_df = results_df.append(results_df_2, ignore_index=True)\nprint(results_df)\n\n# Here we commented the appned, because values are huge and making final table results_df very big","a0204b79":"# Ridge\n\nmodel = Ridge(alpha=100, solver='cholesky', tol=0.0001, random_state=42)\nmodel.fit(X_train, y_train)\npred = model.predict(X_test)\n\ntest_pred = model.predict(X_test)\ntrain_pred = model.predict(X_train)\n\nprint('Test set evaluation:\\n_____________________________________')\nprint_evaluate(y_test, test_pred)\nprint('====================================')\nprint('Train set evaluation:\\n_____________________________________')\nprint_evaluate(y_train, train_pred)","fac6fece":"results_df_2 = pd.DataFrame(data=[[\"Ridge Regression\", *evaluate(y_test, test_pred) , cross_val(Ridge())]], \n                            columns=['Model', 'MAE', 'MSE', 'RMSE','RMSLE', 'R2 Square', \"Cross Validation\"])\nresults_df = results_df.append(results_df_2, ignore_index=True)\nresults_df","2fdeacec":"\nKRR = KernelRidge(alpha=0.6, kernel='polynomial', degree=2, coef0=2.5)\n\nKRR.fit(X_train, y_train)\n\ntest_pred = KRR.predict(X_test)\ntrain_pred = KRR.predict(X_train)\n\n\nprint('Test set evaluation:\\n_____________________________________')\nprint_evaluate(y_test, test_pred)\nprint('====================================')\nprint('Train set evaluation:\\n_____________________________________')\nprint_evaluate(y_train, train_pred)","98d9ad88":"results_df_2 = pd.DataFrame(data=[[\"Kernel Regression\", *evaluate(y_test, test_pred) , cross_val(KernelRidge())]], \n                            columns=['Model', 'MAE', 'MSE', 'RMSE','RMSLE', 'R2 Square', \"Cross Validation\"])\nresults_df = results_df.append(results_df_2, ignore_index=True)\nresults_df","6574cccd":"# Lasso\n\n#In case scalarization is not done, we can use this pipeline method with RobustScaler() as 1st one before Laso regression\n\n#model = make_pipeline(RobustScaler(), Lasso(alpha =0.0005, random_state=1))\n\nmodel = Lasso(alpha=0.1, \n              precompute=True, \n#               warm_start=True, \n              positive=True, \n              selection='random',\n              random_state=42)\n\nmodel.fit(X_train, y_train)\n\ntest_pred = model.predict(X_test)\ntrain_pred = model.predict(X_train)\n\n\nprint('Test set evaluation:\\n_____________________________________')\nprint_evaluate(y_test, test_pred)\nprint('====================================')\nprint('Train set evaluation:\\n_____________________________________')\nprint_evaluate(y_train, train_pred)","3f88ad3a":"results_df_2 = pd.DataFrame(data=[[\"Lasso Regression\", *evaluate(y_test, test_pred) , cross_val(Lasso())]], \n                            columns=['Model', 'MAE', 'MSE', 'RMSE', 'RMSLE','R2 Square', \"Cross Validation\"])\nresults_df = results_df.append(results_df_2, ignore_index=True)\nresults_df","4e09d5c0":"# ElasticNet\n\n# To handle Outlier, we can use RobustScaler() in pipeline\nmodel = make_pipeline(RobustScaler(), \n                      ElasticNet(alpha=0.1, l1_ratio=0.9, selection='random', random_state=42)\n                     )\n                      \n                      \n#model = ElasticNet(alpha=0.1, l1_ratio=0.9, selection='random', random_state=42)\nmodel.fit(X_train, y_train)\n\ntest_pred = model.predict(X_test)\ntrain_pred = model.predict(X_train)\n\nprint('Test set evaluation:\\n_____________________________________')\nprint_evaluate(y_test, test_pred)\nprint('====================================')\nprint('Train set evaluation:\\n_____________________________________')\nprint_evaluate(y_train, train_pred)","82ecc598":"results_df_2 = pd.DataFrame(data=[[\"Elastic Net Regression\", *evaluate(y_test, test_pred) , cross_val(ElasticNet())]], \n                            columns=['Model', 'MAE', 'MSE', 'RMSE', 'RMSLE','R2 Square', \"Cross Validation\"])\nresults_df = results_df.append(results_df_2, ignore_index=True)\nresults_df","32d4a65e":"# PolynomialFeatures\n\npoly_reg = PolynomialFeatures(degree=2)\n\nX_train_2_d = poly_reg.fit_transform(X_train)\nX_test_2_d = poly_reg.transform(X_test)\n\nlin_reg = LinearRegression(normalize=True)\nlin_reg.fit(X_train_2_d,y_train)\n\ntest_pred = lin_reg.predict(X_test_2_d)\ntrain_pred = lin_reg.predict(X_train_2_d)\n\nprint('Test set evaluation:\\n_____________________________________')\nprint_evaluate(y_test, test_pred)\nprint('====================================')\nprint('Train set evaluation:\\n_____________________________________')\nprint_evaluate(y_train, train_pred)","e4d480e8":"results_df_2 = pd.DataFrame(data=[[\"Polynomail Regression\", *evaluate(y_test, test_pred), 0]], \n                            columns=['Model', 'MAE', 'MSE', 'RMSE', 'RMSLE', 'R2 Square', 'Cross Validation'])\nresults_df = results_df.append(results_df_2, ignore_index=True)\nresults_df","740103ba":"GBoost = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05,\n                                   max_depth=4, max_features='sqrt',\n                                   min_samples_leaf=15, min_samples_split=10, \n                                   loss='huber', random_state =5)\n\nGBoost.fit(X_train, y_train)\n\ntest_pred = GBoost.predict(X_test)\ntrain_pred = GBoost.predict(X_train)\n\nprint('Test set evaluation:\\n_____________________________________')\nprint_evaluate(y_test, test_pred)\nprint('====================================')\nprint('Train set evaluation:\\n_____________________________________')\nprint_evaluate(y_train, train_pred)","bdc7224a":"results_df_2 = pd.DataFrame(data=[[\"Gradient Boosting Regressor\", *evaluate(y_test, test_pred), 0]], \n                            columns=['Model', 'MAE', 'MSE', 'RMSE', 'RMSLE', 'R2 Square', 'Cross Validation'])\nresults_df = results_df.append(results_df_2, ignore_index=True)\nresults_df","c7c700e8":"# SGDRegressor\n\nsgd_reg = SGDRegressor(n_iter_no_change=250, penalty=None, eta0=0.0001, max_iter=100000)\nsgd_reg.fit(X_train, y_train)\n\ntest_pred = sgd_reg.predict(X_test)\ntrain_pred = sgd_reg.predict(X_train)\n\nprint('Test set evaluation:\\n_____________________________________')\nprint_evaluate(y_test, test_pred)\nprint('====================================')\nprint('Train set evaluation:\\n_____________________________________')\nprint_evaluate(y_train, train_pred)","cc29d4e1":"results_df_2 = pd.DataFrame(data=[[\"Stochastic Gradient Descent\", *evaluate(y_test, test_pred), 0]], \n                            columns=['Model', 'MAE', 'MSE', 'RMSE', 'RMSLE', 'R2 Square', 'Cross Validation'])\nresults_df = results_df.append(results_df_2, ignore_index=True)\nresults_df\n","51b83ceb":"# RandomForestRegressor\n\nrf_reg = RandomForestRegressor(n_estimators=1000)\nrf_reg.fit(X_train, y_train)\n\ntest_pred = rf_reg.predict(X_test)\ntrain_pred = rf_reg.predict(X_train)\n\nprint('Test set evaluation:\\n_____________________________________')\nprint_evaluate(y_test, test_pred)\n\nprint('Train set evaluation:\\n_____________________________________')\nprint_evaluate(y_train, train_pred)","e01c935e":"results_df_2 = pd.DataFrame(data=[[\"Random Forest Regressor\", *evaluate(y_test, test_pred), 0]], \n                            columns=['Model', 'MAE', 'MSE', 'RMSE', 'RMSLE', 'R2 Square', 'Cross Validation'])\nresults_df = results_df.append(results_df_2, ignore_index=True)\nresults_df","b2bf63bd":"# SVR\n\nsvm_reg = SVR(kernel='rbf', C=1000000, epsilon=0.001)\nsvm_reg.fit(X_train, y_train)\n\ntest_pred = svm_reg.predict(X_test)\ntrain_pred = svm_reg.predict(X_train)\n\nprint('Test set evaluation:\\n_____________________________________')\nprint_evaluate(y_test, test_pred)\n\nprint('Train set evaluation:\\n_____________________________________')\nprint_evaluate(y_train, train_pred)","2a387cd3":"results_df_2 = pd.DataFrame(data=[[\"SVM Regressor\", *evaluate(y_test, test_pred), 0]], \n                            columns=['Model', 'MAE', 'MSE', 'RMSE', 'RMSLE', 'R2 Square', 'Cross Validation'])\nresults_df = results_df.append(results_df_2, ignore_index=True)\nresults_df","be1f7284":"model_xgb = xgb.XGBRegressor(colsample_bytree=0.4603, gamma=0.0468, \n                             learning_rate=0.05, max_depth=3, \n                             min_child_weight=1.7817, n_estimators=2200,\n                             reg_alpha=0.4640, reg_lambda=0.8571,\n                             subsample=0.5213, silent=1,\n                             random_state =7, nthread = -1)\n\nmodel_xgb.fit(X_train, y_train)\n\ntest_pred = model_xgb.predict(X_test)\ntrain_pred = model_xgb.predict(X_train)\n\nprint('Test set evaluation:\\n_____________________________________')\nprint_evaluate(y_test, test_pred)\n\nprint('Train set evaluation:\\n_____________________________________')\nprint_evaluate(y_train, train_pred)","e35b32fe":"results_df_2 = pd.DataFrame(data=[[\"XGBoost\", *evaluate(y_test, test_pred), 0]], \n                            columns=['Model', 'MAE', 'MSE', 'RMSE','RMSLE', 'R2 Square', 'Cross Validation'])\nresults_df = results_df.append(results_df_2, ignore_index=True)\nresults_df","3203ada2":"model_lgb = lgb.LGBMRegressor(objective='regression',num_leaves=5,\n                              learning_rate=0.05, n_estimators=720,\n                              max_bin = 55, bagging_fraction = 0.8,\n                              bagging_freq = 5, feature_fraction = 0.2319,\n                              feature_fraction_seed=9, bagging_seed=9,\n                              min_data_in_leaf =6, min_sum_hessian_in_leaf = 11)\n\nmodel_lgb.fit(X_train, y_train)\n\ntest_pred = model_lgb.predict(X_test)\ntrain_pred = model_lgb.predict(X_train)\n\nprint('Test set evaluation:\\n_____________________________________')\nprint_evaluate(y_test, test_pred)\n\nprint('Train set evaluation:\\n_____________________________________')\nprint_evaluate(y_train, train_pred)","599aacf0":"results_df_2 = pd.DataFrame(data=[[\"LightGBM\", *evaluate(y_test, test_pred), 0]], \n                            columns=['Model', 'MAE', 'MSE', 'RMSE', 'RMSLE', 'R2 Square', 'Cross Validation'])\nresults_df = results_df.append(results_df_2, ignore_index=True)\nresults_df","22c28ce1":"# Sequential\n# optimizers - Adam\n\nX_train = np.array(X_train)\nX_test = np.array(X_test)\ny_train = np.array(y_train)\ny_test = np.array(y_test)\n\nmodel = Sequential()\n\nmodel.add(Dense(X_train.shape[1], activation='relu'))\nmodel.add(Dense(32, activation='relu'))\n# model.add(Dropout(0.2))\n\nmodel.add(Dense(64, activation='relu'))\n# model.add(Dropout(0.2))\n\nmodel.add(Dense(128, activation='relu'))\n# model.add(Dropout(0.2))\n\nmodel.add(Dense(512, activation='relu'))\nmodel.add(Dropout(0.1))\nmodel.add(Dense(1))\n\nmodel.compile(optimizer=Adam(0.00001), loss='mse')\n\nr = model.fit(X_train, y_train,\n              validation_data=(X_test,y_test),\n              batch_size=1,\n              epochs=100)","9219f3d6":"test_pred = model.predict(X_test)\ntrain_pred = model.predict(X_train)\n\nprint('Test set evaluation:\\n_____________________________________')\nprint_evaluate(y_test, test_pred)\n\nprint('Train set evaluation:\\n_____________________________________')\nprint_evaluate(y_train, train_pred)","ff6e8619":"results_df_2 = pd.DataFrame(data=[[\"Artficial Neural Network\", *evaluate(y_test, test_pred), 0]], \n                            columns=['Model', 'MAE', 'MSE', 'RMSE', 'RMSLE', 'R2 Square', 'Cross Validation'])\nresults_df = results_df.append(results_df_2, ignore_index=True)\nresults_df","e4eefba8":"pd.DataFrame(r.history)","d1bc4a62":"r_pd = pd.DataFrame(r.history)\n#pd.DataFrame(r.history).hvplot.line(y=['loss', 'val_loss'])\nr_pd.iloc[:,0]\nr_pd.iloc[:,1]\nplt.scatter(r_pd.iloc[:,0],r_pd.iloc[:,1])\nplt.xlabel('loss')\nplt.ylabel('val_loss')\nplt.show()","ec6843eb":"#pd.DataFrame({'True Values': y_test, 'Predicted Values': pred}).hvplot.scatter(x='True Values', y='Predicted Values')\n\nplt.scatter(y_test,pred)\nplt.xlabel('True Values')\nplt.ylabel('Predicted Values')\nplt.show()","423a5cc7":"results_df.set_index('Model', inplace=True)","c8fea5bb":"#Converted object to float, else bar plot will show error\nresults_df['R2 Square'] = results_df['R2 Square'].astype(float)\nresults_df.info()","8ba6e1c0":"#barh plot\n\nresults_df['R2 Square'].plot(kind='barh', figsize=(12, 8))","8aaaeaa8":"test.head()","64ee26ac":"# Remove id from the data\ntest_ID = test['Id']","b2eb56b8":"test_x = test.drop(\"Id\", axis = 1)\ntest_x.head()","c8ca9da7":"\"\"\"model_xgb = xgb.XGBRegressor(colsample_bytree=0.4603, gamma=0.0468, \n                             learning_rate=0.05, max_depth=3, \n                             min_child_weight=1.7817, n_estimators=2200,\n                             reg_alpha=0.4640, reg_lambda=0.8571,\n                             subsample=0.5213, silent=1,\n                             random_state =7, nthread = -1)\n\nmodel_xgb.fit(X_train, y_train)\n\n\"\"\"\n# These are builded earlier, so directly predicting here by XGBoost\n\ntest_pred = model_xgb.predict(test_x)\n","7221b260":"test_pred[:6]","fb039cf2":"test_pred = np.expm1(test_pred)","fd3c3800":"test_pred[:6]","b4af2a71":"my_submission = pd.DataFrame({'Id': test.Id, 'SalePrice': test_pred})\n# you could use any filename. We choose submission here\nmy_submission.to_csv(r'\/kaggle\/working\/submission.csv', index=False)","87175298":"<div class=\"alert alert-info\" style=\"background-color:#004492; color:white; padding:0px 10px; border-radius:2px;\"><h2 style='margin:10px 5px'>3.1.11.Support Vector Machine <\/h2>\n<\/div>\n","b181e8c2":"> This code not working here\n![image.png](attachment:image.png)","2158791e":"- This looks bad model, R2 is -ve","18c611ef":"> Split data to train test for evaluation later","fa082c35":"\n<div class=\"alert alert-info\" style=\"background-color:#004492; color:white; padding:0px 10px; border-radius:2px;\"><h2 style='margin:10px 5px'> 3.1.2. Robust Regression<\/h2>\n<\/div>\n\n\n- Robust regression is a form of regression analysis designed to overcome some limitations of traditional parametric and non-parametric methods. Robust regression methods are designed to be not overly affected by violations of assumptions by the underlying data-generating process.\n\n- One instance in which robust estimation should be considered is when there is a strong suspicion of heteroscedasticity.\n\n- A common situation in which robust estimation is used occurs when the data contain outliers. In the presence of outliers that do not come from the same data-generating process as the rest of the data, least squares estimation is inefficient and can be biased. Because the least squares predictions are dragged towards the outliers, and because the variance of the estimates is artificially inflated, the result is that outliers can be masked. (In many situations, including some areas of geostatistics and medical statistics, it is precisely the outliers that are of interest.)","093ee5b0":"> XGBoost and LightGBM having good R2 value. These 2 are the best models","6e71d00a":"> LR is better than PR, LR has less Errors(MSE,RMSE) and incrased R2","e9659f45":"<div class=\"alert alert-info\" style=\"background-color:#004492; color:white; padding:0px 10px; border-radius:2px;\"><h2 style='margin:10px 5px'> 3.1.6. Elastic Net<\/h2>\n<\/div>\n\n- A linear regression model trained with **L1 and L2 prior as regularizer**.\n\n- This combination allows for learning a sparse model where few of the weights are non-zero like Lasso, while still maintaining the regularization properties of Ridge.\n\n- Elastic-net is useful when there are multiple features which are correlated with one another. Lasso is likely to pick one of these at random, while elastic-net is likely to pick both.\n\n- A practical advantage of trading-off between Lasso and Ridge is it allows Elastic-Net to inherit some of Ridge\u2019s stability under rotation.\n\n- The objective function to minimize is in this case\n![image.png](attachment:image.png)","897caade":"## GOAL:\n\n**Apply different models and Compare all evaluation matrics( MSE,MAE, RMSE, RMSLE) and R2, Based on this finalize the model**","be92ba6d":"\n<div class=\"alert alert-info\" style=\"background-color:#008492; color:white; padding:0px 10px; border-radius:2px;\"><h2 style='margin:10px 5px'> 4. Models Comparison <\/h2>\n<\/div>\n","39b1b5eb":"<div class=\"alert alert-info\" style=\"background-color:#004492; color:white; padding:0px 10px; border-radius:2px;\"><h2 style='margin:10px 5px'> 3.1.5. LASSO Regression <\/h2>\n<\/div>\n\n- A linear model that estimates sparse coefficients.\n\n- Mathematically, it consists of a linear model trained with  \u21131  prior as regularizer. The objective function to minimize is:\n![image.png](attachment:image.png)\n\n- The lasso estimate thus solves the minimization of the least-squares penalty with  \u03b1\u2223\u2223w\u2223\u22231  added, where  \u03b1  is a constant and  \u2223\u2223w\u2223\u22231  is the  \u21131\u2212norm  of the parameter vector.","3f9b89ba":"> LR is better than RR, LR has less Errors(MSE,RMSE) and incrased R2","4a2da595":"> Here x and y value are same, which is diagnoal","20e0dfdd":"\n<div class=\"alert alert-info\" style=\"background-color:#008492; color:white; padding:0px 10px; border-radius:2px;\"><h2 style='margin:10px 5px'> 6. APPLY XGBOOST TO FINAL TEST DATA <\/h2>\n<\/div>","3456d411":"> GBR is better than SGD, GBR has less Errors(MSE,RMSE) and incrased R2","94610831":"> We are getting below error for RMSLE Calculation.\nSo we scaled the data by using **RobustScalar** in pipeline, so it avoided the error\n\n> Error:#ValueError: Mean Squared Logarithmic Error cannot be used when targets contain negative values.\n\n ","c92d4073":"<div class=\"alert alert-info\" style=\"background-color:#004492; color:white; padding:0px 10px; border-radius:2px;\"><h2 style='margin:10px 5px'> 3.1.10. Random Forest Regressor<\/h2>\n<\/div>","5f82b0f6":"> This looks bad model, R2 is -ve","6214e0ba":"> LGBM and XGboost are almost same and its betetr that ANN.  Both have less Errors(MSE,RMSE, RMSLE) and incrased R2 compared to complex ANN","eba06f6a":"![image.png](attachment:image.png)","1c93802b":"> XGBoost is better than GLR, GBR has less Errors(MSE,RMSE) and incrased R2","4c1f34b0":"<div class=\"alert alert-info\" style=\"background-color:#004492; color:white; padding:0px 10px; border-radius:2px;\"><h2 style='margin:10px 5px'> 3.1.4. KERNEL Ridge Regression <\/h2>\n<\/div>\n\n\n> It looks like Polynomial used kernel ='polynomial'","8ab1f6f4":"> Here we are getting log1p(x) applied scaled SalesPrice, (We converted SalesPrice data to log1p(x) applied data during preprocessing and used this in model training)  ","52ed93ef":"\n<div class=\"alert alert-info\" style=\"background-color:#004492; color:white; padding:0px 10px; border-radius:2px;\"><h2 style='margin:10px 5px'> 3.1.1. Linear Regression<\/h2>\n<\/div>\n\n#### 3.1.1 Base models - Linear Regression :","e5c2bfa1":"> LR is better than KR and RR, LR has less Errors(MSE,RMSE) and incrased R2 but KR almost same","9589b506":"<div class=\"alert alert-info\" style=\"background-color:#004492; color:white; padding:0px 10px; border-radius:2px;\"><h2 style='margin:10px 5px'>3.1.13 LightGBM : <\/h2>\n<\/div>","2adba88a":"> GBR is better than SVMR, GBR has less Errors(MSE,RMSE) and incrased R2","e11bdded":"### Make Submission\n\n- Hit the blue Publish button at the top of our notebook screen. It will take some time for our kernel to run. When it has finished our navigation bar at the top of the screen will have a tab for Output. \n- This only shows up if we have written an output file (like we did in the Prepare Submission File step).\n\n### Last Steps\n\n- Click on the Output button. This will bring us to a screen with an option to Submit to Competition. Hit that and we will see how our model performed.\n\n- If we want to go back to improve your model, click the Edit button, which re-opens the kernel. we'll need to re-run all the cells when you re-open the kernel.","8330476a":"> This looks good model","430a4463":"\n<div class=\"alert alert-info\" style=\"background-color:#008492; color:white; padding:0px 10px; border-radius:2px;\"><h2 style='margin:10px 5px'>END <\/h2>\n<\/div>\n","0f2d16f5":"<div class=\"alert alert-info\" style=\"background-color:#004492; color:white; padding:0px 10px; border-radius:2px;\"><h2 style='margin:10px 5px'> 3.1.8. Gradient Boosting Regressor<\/h2>\n<\/div>","149e3f8f":"<div class=\"alert alert-info\" style=\"background-color:#008492; color:white; padding:0px 10px; border-radius:2px;\"><h2 style='margin:10px 5px'> Advanced House Price Prediction - Model Selection <\/h2>\n<\/div>\n\n","076b391c":"<div class=\"alert alert-info\" style=\"background-color:#008492; color:white; padding:0px 10px; border-radius:2px;\"><h2 style='margin:10px 5px'> 3. Model Build <\/h2>\n<\/div>","297cb566":"> LR is better than ES, LR has less Errors(MSE,RMSE) and incrased R2","87d37068":"<div class=\"alert alert-info\" style=\"background-color:#004492; color:white; padding:0px 10px; border-radius:2px;\"><h2 style='margin:10px 5px'> 3.1.7. Polynomial Regression<\/h2>\n<\/div>\n\n- One common pattern within machine learning is to use **linear models trained on nonlinear functions of the data**. This approach maintains the generally fast performance of linear methods, while allowing them to fit a much wider range of data.\n\n- For example, a simple linear regression can be extended by constructing polynomial features from the coefficients. In the standard linear regression case, you might have a model that looks like this for two-dimensional data:\n\n![image.png](attachment:image.png)","4e039e57":"<div class=\"alert alert-info\" style=\"background-color:#008492; color:white; padding:0px 10px; border-radius:2px;\"><h2 style='margin:10px 5px'> 1. Import all Librariy  <\/h2>\n<\/div>","8b9633ad":"\n<div class=\"alert alert-info\" style=\"background-color:#008492; color:white; padding:0px 10px; border-radius:2px;\"><h2 style='margin:10px 5px'> 5. Summary <\/h2>\n<\/div>\n\n### Metric\n- Submissions are evaluated on Root-Mean-Squared-Error (RMSE) and RMSLE between the logarithm of the predicted value and the logarithm of the observed sales price.\n- Taking logs means that errors in predicting expensive houses and cheap houses will affect the result equally.\n\n\n## Conclusion: \n- In earlier NOTEBOOK, we did all Preprocessing, Scalarization etc\n- In this doc we tried most of the Regression algorithm, and found 2 model (XGBoost and LightGBM ) having less errors(MAE, MSE,RMSE,RMSLE) and high R2\n- So we are finalizing our model with XGBoost and applying same model for Kaggle's Test dataset\n\n","a903389a":"> GBR is better than LR, GBR has less Errors(MSE,RMSE) and incrased R2","134bcc0d":"- The main aim of this project is to predict the house price based on various features which we will discuss as we go ahead\n- In previous Notebook we preprocessed and applied all FE and created new train and test data.\n- Here we will apply different models to get good prediction\n\n- Linear Regression is the simplest algorithm in machine learning, it can be trained in different ways. In this notebook we will cover the following Regression algorithms:\n\n          1. Linear Regression\n          2. Robust Regression\n          3. Ridge Regression\n          4. KERNEL Ridge Regression\n          5. LASSO Regression\n          6. Elastic Net\n          7. Polynomial Regression\n          8. Gradient Boosting Regressor\n          9. Stochastic Gradient Descent (SGD)\n          10. Random Forest Regressor\n          11. Support Vector Machine Regression\n          12. XGBOOST\n          13. lightgbm\n          14. Artificial Neaural Networks\n        \n**Below few Req of  linear regression**     \n\n- **1.Linear Assumption**. Linear regression assumes that the relationship between your input and output is linear. It does not support anything else. This may be obvious, but it is good to remember when you have a lot of attributes. You may need to transform data to make the relationship linear (e.g. log transform for an exponential relationship).\n\n\n- **2.Remove Noise**. Linear regression assumes that your input and output variables are not noisy. Consider using data cleaning operations that let you better expose and clarify the signal in your data. This is most important for the output variable and you want to remove outliers in the output variable (y) if possible.\n\n\n- **3.Remove Collinearity**. Linear regression will over-fit your data when you have highly correlated input variables. Consider calculating pairwise correlations for your input data and removing the most correlated.\n\n\n- **4.Gaussian Distributions**. Linear regression will make more reliable predictions if your input and output variables have a Gaussian distribution. You may get some benefit using transforms (e.g. log or BoxCox) on you variables to make their distribution more Gaussian looking.\n\n\n- **5.Rescale Inputs**: Linear regression will often make more reliable predictions if you rescale input variables using standardization or normalization.\n\n#### Dataset to downloaded from the below link\nhttps:\/\/www.kaggle.com\/c\/house-prices-advanced-regression-techniques\/data","f04b864a":"> GBR is better than RF, GBR has less Errors(MSE,RMSE) and incrased R2","edd3b95d":"**Apllied different models and Compared all evaluation matrics( MSE,MAE, RMSE, RMSLE) and R2, Based on this finalized the XGBOOST model**","f18d566e":"### 6.1 REVERT BACK OUTPUT TO ORIGINAL SCALE\n\n- We will revert the log1p(x) applied data back to original scale by taking exp(Y)\n\n> Y = Log1p(x)\n\n> x = exp(Y)","293dc1c2":"<div class=\"alert alert-info\" style=\"background-color:#004492; color:white; padding:0px 10px; border-radius:2px;\"><h2 style='margin:10px 5px'> 3.1.9. Stochastic Gradient Descent<\/h2>\n<\/div>\n\n- Gradient Descent is a very generic optimization algorithm capable of finding optimal solutions to a wide range of problems. The general idea of Gradient Descent is to tweak parameters iteratively in order to minimize a cost function. Gradient Descent measures the local gradient of the error function with regards to the parameters vector, and it goes in the direction of descending gradient. Once the gradient is zero, you have reached a minimum","d4413724":"### 3.1.2 Linear Regression  - Predictions \n- Let's grab predictions off our test set and see how well it did!","c41a19bb":"## 3.1 Define a cross validation strategy\n\n- 1. We use the **cross_val_score** function of Sklearn. However this function has not a shuffle attribut, we add then one line of code, in order to shuffle the dataset prior to cross-validation\n\n- 2. **Evaluation** of regression model","cbcb0128":"\n<div class=\"alert alert-info\" style=\"background-color:#008492; color:white; padding:0px 10px; border-radius:2px;\"><h2 style='margin:10px 5px'> 7. Prepare Submission File <\/h2>\n<\/div>\n\n- We make submissions in CSV files. Our submissions usually have two columns: an ID column and a prediction column. The ID field comes from the test data (keeping whatever name the ID field had in that data, which for the housing data is the string 'Id'). The prediction column will use the name of the target field.\n\n- We will create a DataFrame with this data, and then use the dataframe's to_csv method to write our submission file. Explicitly include the argument index=False to prevent pandas from adding another column in our csv file.","bf8c3307":"<div class=\"alert alert-info\" style=\"background-color:#008492; color:white; padding:0px 10px; border-radius:2px;\"><h2 style='margin:10px 5px'> 2. Split train AS Input (X_train) and Output(y_train )<\/h2>\n<\/div>","373686e1":"> LGBM and XGboost are almost same, Both have less Errors(MSE,RMSE) and incrased R2","3b66b3d2":"<div class=\"alert alert-info\" style=\"background-color:#004492; color:white; padding:0px 10px; border-radius:2px;\"><h2 style='margin:10px 5px'>3.1.14. Artficial Neural Network <\/h2>\n<\/div>","0ec5f417":"> Here residual plot also Normal distributed. Its good","942971c2":"<div class=\"alert alert-info\" style=\"background-color:#004492; color:white; padding:0px 10px; border-radius:2px;\"><h2 style='margin:10px 5px'>3.1.12 XGBoost : <\/h2>\n<\/div>","2a35fa0c":"<div class=\"alert alert-info\" style=\"background-color:#004492; color:white; padding:0px 10px; border-radius:2px;\"><h2 style='margin:10px 5px'> 3.1.3. Ridge Regression<\/h2>\n<\/div>\n\n\n- Ridge regression addresses some of the problems of Ordinary Least Squares by imposing a penalty on the size of coefficients. The ridge coefficients minimize a penalized residual sum of squares,\n![image.png](attachment:image.png)\n \n- \u03b1>=0  is a complexity parameter that controls the amount of shrinkage: the larger the value of  \u03b1 , the greater the amount of shrinkage and thus the coefficients become more robust to collinearity.\n\n- Ridge regression is an L2 penalized model. Add the squared sum of the weights to the least-squares cost function."}}