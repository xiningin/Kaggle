{"cell_type":{"44898700":"code","75f9ef5f":"code","0b3765aa":"code","2042107e":"code","bbbd455d":"code","6fb2920e":"code","cc8cdb71":"code","cd5b5262":"code","5582e9d2":"code","e1706261":"code","ffbb05b0":"code","dc283a37":"code","c5b17244":"code","faa425d6":"code","b05d80d6":"code","67280290":"code","2db9a900":"code","4e768d1c":"code","550503c4":"code","eadf0976":"code","eb3aac29":"code","eab515b5":"code","8fdad194":"code","927d4b0c":"markdown","a6b51a66":"markdown","34a7eb18":"markdown","5515bb39":"markdown","d8a0f798":"markdown","d2ea755f":"markdown","548c4e76":"markdown","ef112dec":"markdown","0d83265e":"markdown","9c08742e":"markdown","29b65616":"markdown","bf3e84ec":"markdown","4273bca0":"markdown","9bd3405e":"markdown","f044d377":"markdown","fc17b3a6":"markdown","bd47550c":"markdown"},"source":{"44898700":"import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n%matplotlib inline\nimport warnings\nwarnings.filterwarnings('ignore')","75f9ef5f":"from sklearn.datasets import load_breast_cancer\ncancer=load_breast_cancer()\nX = pd.DataFrame(cancer['data'],columns=cancer['feature_names'])\nY = cancer['target']\nX.head(5)\n","0b3765aa":"X.shape","2042107e":"from sklearn.preprocessing import StandardScaler\nscaled_X = StandardScaler().fit_transform(X)\npd.DataFrame(scaled_X, columns=cancer['feature_names']).head()","bbbd455d":"features = scaled_X.T\ncovariance_matrix = np.cov(features)\nprint(covariance_matrix)","6fb2920e":"# Calculate the eigen vectors and eigen values of the covariance matrix using linalg.eig()\neig_vals, eig_vecs = np.linalg.eig(covariance_matrix)\n#Show the eigenvectors\nprint(\"Eigenvectors \\n%s\" %eig_vecs)","cc8cdb71":"#Show the eigenvalues\nprint(\"Eigenvalues \\n%s\" %eig_vals)","cd5b5262":"# The first principle component captures about 44% of the variance\neig_vals[0] \/ sum(eig_vals)","5582e9d2":"eig_vals[1] \/ sum(eig_vals)\n#About 19% of the variance is captured","e1706261":"# Let's choose the first two principle components\nV = eig_vecs[:, :2]","ffbb05b0":"# Examine our new transformed matrix with PC1 & PC2\nprojected_X = scaled_X.dot(V)\nprojected_X","dc283a37":"X_pca = pd.DataFrame(data = projected_X, columns = ['PC1', 'PC2'])\nX_pca.head()","c5b17244":"#Center the Data\nX_centered = scaled_X - scaled_X.mean()\n#Apply SVD\nU, s, V = np.linalg.svd(X_centered)\n#Choose Principal Components (Select Eigenvectors with Highest Eigenvalues)\nW = V.T[:, :2]\n#Project Original Matrix onto Eigenvectors\n#Compute dot product to project the new reduced dimensionality dataset\n\nprojected_X = X_centered.dot(W)","faa425d6":"X_pca = pd.DataFrame(data = projected_X, columns = ['PC1', 'PC2'])\nX_pca.head()","b05d80d6":"from sklearn.decomposition import PCA\npca = PCA(n_components=2)\npcomponents = pca.fit_transform(scaled_X)\nX_pca = pd.DataFrame(data = pcomponents, columns = ['PC1', 'PC2'])\nX_pca.shape","67280290":"X_pca.head()","2db9a900":"plt.figure(figsize=(8,6))\nplt.scatter(X_pca['PC1'], X_pca['PC2'], c=cancer['target'])\nplt.xlabel('First principle component')\nplt.ylabel('Second principle component')","4e768d1c":"# Get the first PC1 and divide by the total sum of eigenvalues\neig_vals[0] \/ sum(eig_vals)","550503c4":"pca.explained_variance_ratio_","eadf0976":"pd.DataFrame(pca.components_, columns=list(X.columns), index=('PC1','PC2'))","eb3aac29":"#Fitting the PCA algorithm with our Data\npca2 = PCA().fit(scaled_X)\n#Plotting the Cumulative Summation of the Explained Variance\nplt.figure()\nplt.plot(np.cumsum(pca2.explained_variance_ratio_))\nplt.xlabel('Number of Components')\nplt.ylabel('Variance (%)') #for each component\nplt.title('Pulsar Dataset Explained Variance')\nplt.show()","eab515b5":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\n\n# Create logistic regression object\nclf = LogisticRegression(random_state=0)\n# Split into training and test sets using ORIGINAL dataset\nX_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=1)\n# Train model\nmodel = clf.fit(X_train, y_train)\n# Perform 10-Fold Cross Validation\nresult = cross_val_score(clf, X_train, y_train, cv=10, scoring='f1')\nresult.mean()","8fdad194":"from sklearn.decomposition import PCA\npca = PCA(n_components=10)\npcomponents = pca.fit_transform(scaled_X)\nX_pca = pd.DataFrame(data = pcomponents)\n# Split into training and test sets using REDUCED dataset\nX_train, X_test, y_train, y_test = train_test_split(X_pca, Y, test_size=0.2, random_state=1)\n# Train model\nmodel = clf.fit(X_train, y_train)\n# Get predicted probabilities\ny_score = clf.predict_proba(X_test)[:,1]\n# Perform 10-Fold Cross Validation\nresult = cross_val_score(clf, X_train, y_train, cv=10, scoring='f1')\nresult.mean()","927d4b0c":"**Choose Principal Components (Select Eigenvectors with Highest Eigenvalues)**","a6b51a66":"# Principal Component Analysis using Scikit-Learn","34a7eb18":"**Eigen Value Decomposition of Covariance Matrix**","5515bb39":"# Implementing PCA\n\n* Eigen value decomposition of the covariance matrix}\n* Singular value decomposition of data matrix\n* Scikit-learn PCA","d8a0f798":"**Examine Principal Components of SVD**","d2ea755f":"# Visualization\n\nFinf if any patterns from our new dataset\n\nWe reduced the dimensions from 30 to 2 dimensions","548c4e76":"**Variance**","ef112dec":"# Singular Value Decomposition","0d83265e":"**Variance Ratio**","9c08742e":"**Matrix onto Eigenvectors**","29b65616":"**Calculate Eigen Vectors and Eigen Values from Covariance Matrix**\n\n* Eigen Values : Determines the magnitude of the vector (higher number captures more variance)\n* Eigin Vectors :  Determines the direction of the vector","bf3e84ec":"# Model Performance with PCA","4273bca0":"# Choosing the Right Number of Dimensions\n\nSelect the right number of dimensions that captures the most variance.","9bd3405e":"**Standardize the Dataset**","f044d377":"# Model Performance without PCA ","fc17b3a6":"**Calculate the covariance matrix**","bd47550c":"# Principle Component Analysis\n\n**Why Dimensionality Reduction?**\n* visualization\n* reduce noise\n* preserve useful info in low memory\n* less time complexity\n* less space complexity"}}