{"cell_type":{"c6953919":"code","1c801e71":"code","4972cd3f":"code","843fa2d6":"code","e68242f9":"code","34402cf0":"code","65799336":"code","dcf0ffb2":"code","6d30a9bb":"code","26de482f":"code","f9dad7bc":"code","bf7a41d7":"code","de16282f":"code","dc693292":"code","fe693b58":"code","12a2bab9":"markdown","ea36c448":"markdown","29b0277f":"markdown","2b3020c6":"markdown","b5583cf0":"markdown"},"source":{"c6953919":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","1c801e71":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\npd.set_option('display.max_columns',25)\n\nfrom sklearn.preprocessing import StandardScaler,MinMaxScaler\nfrom sklearn.model_selection import train_test_split,cross_val_score,learning_curve,GridSearchCV\nfrom sklearn.decomposition import PCA\nfrom collections import Counter\n\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import LogisticRegression,RidgeClassifier,SGDClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier,RandomForestClassifier,AdaBoostClassifier\nfrom xgboost import XGBClassifier\n\n\n","4972cd3f":"df = pd.read_csv('..\/input\/performance-prediction\/summary.csv')\ndf.head()","843fa2d6":"df1 = df.copy()","e68242f9":"na_cols = ['3PointPercent']\ndf1.loc[:,na_cols] = df1.loc[:,na_cols].fillna(np.mean(df1.loc[:,na_cols]))","34402cf0":"plt.figure(figsize=(8,8))\nhis = sns.histplot(data=df1,x='Target',hue='Target',bins=2)\nlocs = [(bar.get_width(),bar.get_height()) for bar in his.patches]\ncount = df['Target'].groupby(by=df['Target']).count()\nplt.text(locs[2][0]\/2,locs[2][1]+3,count[0],size=12)\nplt.text(locs[1][0]*1.5,locs[1][1]+3,count[1],size=12)\nplt.show()","65799336":"target = df1.loc[:,'Target']\ntrain = df1.drop(columns=['Target','Name'],axis=1)","dcf0ffb2":"from imblearn.combine import SMOTETomek\n\nsample = SMOTETomek(random_state=12)\nX_sample,y_sample = sample.fit_resample(train,target)","6d30a9bb":"Counter(y_sample)","26de482f":"X_train,X_test,y_train,y_test = train_test_split(X_sample,y_sample,test_size=0.33,random_state=42)","f9dad7bc":"scaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)","bf7a41d7":"models = {\n    'SVC': SVC(),\n    'LogisticRegression': LogisticRegression(),\n    'RidgeClassifier': RidgeClassifier(),\n    'SGDClassifier': SGDClassifier(),\n    'KNeighborsClassifier': KNeighborsClassifier(),\n    'GradientBoostingClassifier': GradientBoostingClassifier(),\n    'RandomForestClassifier': RandomForestClassifier(),\n    'AdaBoostClassifier': AdaBoostClassifier()\n}","de16282f":"for name,model in models.items():\n    print(f\"{name} Acc score: {np.mean(cross_val_score(model,X_train,y_train,cv=5))}\")","dc693292":"base_model1 = RandomForestClassifier()\nbase_model1.fit(X_train,y_train)\nprint(f\"Accuracy = {round(base_model1.score(X_test,y_test),2)*100}%\")","fe693b58":"train_sizes, train_scores, valid_scores = learning_curve(RandomForestClassifier(),\nX_train,y_train, train_sizes=[20,50, 80, 110,200,300,354,500,600,717], cv=5,random_state=42)\nax = sns.lineplot(x=train_sizes,y=np.mean(train_scores,axis=1),marker='o',)\nax.annotate('train scores',(300,0.76),color='g',size=11)\nsns.lineplot(x=train_sizes,y=np.mean(valid_scores,axis=1),marker='o')\nax.annotate('val scores',(300,0.66),color='r',size=11)\nplt.title('Learning Curve')\nplt.show()","12a2bab9":"# Prepare Data","ea36c448":"# Up Sampling","29b0277f":"# Scale Data","2b3020c6":"# Training","b5583cf0":"# Split Data"}}