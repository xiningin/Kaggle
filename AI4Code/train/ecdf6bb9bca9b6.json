{"cell_type":{"3cf6c09d":"code","3de8b9dd":"code","b9d4d163":"code","d6539a12":"code","81605517":"code","0cbf973b":"code","5bd44106":"code","18b28d70":"code","b1720844":"code","a5cfce8e":"code","3c674e1c":"code","dd61eb1d":"code","911f3a9c":"code","ac440c66":"code","cd3de016":"code","9f06cb17":"code","ec4880e3":"code","538fd4fc":"code","addb8a0b":"code","f3a8a6dc":"code","1a6438ee":"code","1cd97c30":"code","41ffc1bd":"code","584ff1df":"code","91853fa0":"code","bef113a8":"code","35309550":"code","de698226":"code","48fe2af3":"code","354ee5f8":"code","e796dfbd":"code","b7566646":"code","55034da9":"code","26cf5859":"code","efa18f85":"code","14c739d6":"code","60aa47c2":"code","7771df89":"code","8e0eed79":"code","65a36baf":"code","64b19879":"code","aefeaba4":"code","9a6837c3":"code","c42a0a55":"code","8056dd7d":"code","c8e0692f":"markdown","ec6cd248":"markdown","1de31afc":"markdown","2dbab0f1":"markdown","5f8b6192":"markdown","5d919979":"markdown","ae2b3b3d":"markdown","b2cd1aea":"markdown","611fd757":"markdown","747e28b9":"markdown","4e9af50b":"markdown","6f1863af":"markdown","75c5deb1":"markdown","7af4c31b":"markdown","351331e5":"markdown","59459342":"markdown","cf8f968a":"markdown","b6c74965":"markdown","f099c85b":"markdown","52dc4d2c":"markdown","d32bdf26":"markdown"},"source":{"3cf6c09d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","3de8b9dd":"import tensorflow as tf\nfrom tensorflow import keras\n\n\ndf=pd.read_csv(\"\/kaggle\/input\/bank-customer-churn-modeling\/Churn_Modelling.csv\")\ndf.head()","b9d4d163":"df.info()","d6539a12":"## rownumber and customer-id wont do any good to the data\n## lets drop them from the dataset\n\ndf.drop(['RowNumber','CustomerId'],axis=1,inplace=True)\ndf.columns","81605517":"## are there any null values in the dataset\n\ndf.isnull().sum()\n\n## no null values in the dataset","0cbf973b":"import matplotlib.pyplot as plt\n\nchurn_no=df[df['Exited']==0].EstimatedSalary\nchurn_yes=df[df['Exited']==1].EstimatedSalary\n\nplt.xlabel(\"Estimated_salary\")\nplt.ylabel(\"Number of customers\")\n\nplt.hist([churn_no,churn_yes],label=['exit_no','exit_yes'])\nplt.legend()","5bd44106":"\nchurn_no=df[df['Exited']==0].Tenure\nchurn_yes=df[df['Exited']==1].Tenure\n\nplt.xlabel(\"Tenure\")\nplt.ylabel(\"Number of customers\")\n\nplt.hist([churn_no,churn_yes],label=['churn_no','churn_yes'])\nplt.legend()","18b28d70":"## lets find out the unique values in the object columns\n## since some columns are of object datatypes\n\ndef values_in_columns(df):\n    for column in df:\n        if df[column].dtype=='object':\n            print(f'{column} : {df[column].unique()} : {df[column].nunique()}')\n\n","b1720844":"values_in_columns(df) \n\n## from the answer : only 3 columns are of object datatypes\n## and surname has 2932 unique entries\n## Geography has 3 unique entries\n## Gender has 2 unique entries","a5cfce8e":"print(df['Surname'].nunique())\n\nfrom sklearn.preprocessing import LabelEncoder\n\nlb = LabelEncoder()\n\ndf.Surname = lb.fit_transform(df.Surname)\n\n","3c674e1c":"\nprint(df['Geography'].unique())\n\n## geography column has just 3 unique entries\n## france, spain and Germany\n## lets create the dummy for these 3 entries\n\n\ndummy_dataset=pd.get_dummies(data=df,columns=[\"Geography\"])\n\n","dd61eb1d":"df['Gender'].unique()\n\n## lets replace \n## male --> 1\n## female --> 0\n\n\ndummy_dataset['Gender'] = dummy_dataset['Gender'].replace(['Male','Female'],[1,0])\n\ndummy_dataset['Gender'].value_counts()","911f3a9c":"dummy_dataset.head()","ac440c66":"## lets find out the unique values in the object columns\n## since some columns are of object datatypes\n\ndef values_in_columns(df):\n    for column in df:\n            print(f'{column} : {df[column].unique()} : {df[column].nunique()}')\n\nvalues_in_columns(dummy_dataset)","cd3de016":"\nscaling_cols=['CreditScore','Age','Tenure','Balance','NumOfProducts','EstimatedSalary']\n\nfrom sklearn.preprocessing import  MinMaxScaler\n\nscaler=MinMaxScaler()\n\ndummy_dataset[scaling_cols] = scaler.fit_transform(dummy_dataset[scaling_cols])\n\n\ndummy_dataset.drop(\"Surname\",axis=1,inplace=True)  ## many unique values","9f06cb17":"dummy_dataset['Exited'].value_counts()","ec4880e3":"from sklearn.model_selection import train_test_split\n\nx=dummy_dataset.drop(\"Exited\",axis=1)\ny=dummy_dataset['Exited']\n\n\nx_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.2,random_state=1)","538fd4fc":"x_train.shape, x_test.shape\n\n\n## because of 13 columns in the dataste\n## it gave us the idea that 13 neurons should be there in the final dataset","addb8a0b":"from tensorflow import keras\n\nmodel = keras.Sequential([\n    \n    ## reshaping the input entries\n    keras.layers.Dense(12, input_shape=(12,), activation='relu'),  \n    keras.layers.Dropout(0.50),    ## to avoid overfitting and underfiting\n\n    ## creating the hidden layer\n    keras.layers.Dense(10,activation='relu'),\n    keras.layers.Dropout(0.70),    ##  to avoid overfitting and underfiting\n    \n    keras.layers.Dense(150,activation='relu'),\n    keras.layers.Dropout(0.70),     ## to avoid overfitting and underfiting\n \n    \n    ## final neural layer\n    keras.layers.Dense(1,activation='sigmoid')\n    \n])\n\n\nmodel.compile(optimizer='SGD',\n             loss='binary_crossentropy',\n             metrics=['accuracy'])","f3a8a6dc":"model.fit(x_train,y_train,epochs=20)","1a6438ee":"model.evaluate(x_test,y_test)","1cd97c30":"y_pred=model.predict(x_test)\n\ny_predicted=[]\n\nfor i in y_pred:\n    if i>=0.5:\n        y_predicted.append(1)\n    else:\n        y_predicted.append(0)\n        \n        \nfrom sklearn.metrics import classification_report\n\nprint(classification_report(y_predicted,y_test))        ","41ffc1bd":"dummy_dataset['Exited'].value_counts()\n\n\n\n## clearly there is the imbalance in the dataset","584ff1df":"def ANN(xtrain,ytrain,xtest,ytest,loss,weights):\n    model=keras.Sequential([\n    keras.layers.Dense(12,input_shape=(12,),activation='relu'),\n    \n    keras.layers.Dense(15,activation='relu'), ## hidden layer\n    keras.layers.Dropout(0.5),    \n    \n    keras.layers.Dense(1,activation='sigmoid'),\n\n    ## use sigmoid since only we have to predict between 0 and 1\n    ## only 1 neuron in last step since again prediction between 0 and 1\n      \n    ])\n\n    model.compile( optimizer=\"adam\",\n    loss=loss, ## will be defined in the syntax of the function\n    metrics=['accuracy'])\n    \n    if weights==-1:\n        model.fit(xtrain,ytrain,epochs=100)\n    else:\n        model.fit(xtrain,ytrain,epochs=100,class_weight=weights)\n     \n    \n    print('#############################################################################')\n    print('########### ##################################################################')\n    print('Model Accuracy is :')\n    print(model.evaluate(xtest,ytest))\n    \n    print('#############################################################################')\n    print('#############################################################################')\n    \n    print(\"\\n\")\n    ypred3=model.predict(xtest)\n    ypred_ann = np.round(ypred3)\n    \n    print('#############################################################################')\n    print('#############################################################################')\n    \n    print('#############################################################################')\n    print(\"classification report is \\n\")\n    print(classification_report(ytest,ypred_ann))\n    print('#############################################################################')\n\n    \n    \n    return ypred_ann","91853fa0":"ypred_ann=ANN(x_train,y_train,x_test,y_test,'binary_crossentropy',-1)","bef113a8":"df_class_0, df_class_1 = df['Exited'].value_counts()\n\ndf_class_0, df_class_1\n\n## both store the frequency of 0th and 1th class","35309550":"dummy_class_0 = dummy_dataset[dummy_dataset['Exited']==0]\ndummy_class_1 = dummy_dataset[dummy_dataset['Exited']==1]\n\n\n## dummy_class_0 stores the entries with exited as 0\n## dummy_class_1 stores the entries with exited as 1\n","de698226":"## we will take 2037 samples from 0th row so as their are equal quantities of 0 and 1\n\ndummy_class_0_under=dummy_class_0.sample(df_class_1)\n\ndummy_class_0_under.shape","48fe2af3":"df_test_under=pd.concat([dummy_class_0_under,dummy_class_1],axis=0)\n\ndf_test_under['Exited'].value_counts()\n\n# so 0 and 1 have same number of columns","354ee5f8":"x=df_test_under.drop(\"Exited\",axis=1)\ny=df_test_under['Exited']\n\n## stratify will clubbed the samples in the same proportion as that of the inputs feed in to the sytax\n\nxtrain,xtest,ytrain,ytest=train_test_split(x,y,test_size=0.2,random_state=42,stratify=y)\n\n\nytrain.value_counts()","e796dfbd":"ypred=ANN(xtrain,ytrain,xtest,ytest,'binary_crossentropy',-1)","b7566646":"## scrool down in the epoch dialog box answer to see the confusion matrix\n\n\n## model accuracy : 79.38 %\n## F1 score  : 0_th class : 0.80\n## f1 score  : 1_th class : 0.79 ","55034da9":"df_class_0, df_class_1\n## class 0 has more sample","26cf5859":"## in this we are just oversampling the data\n## that is we are just duplicating the minority sample\n## so as to match with the size of majaority class","efa18f85":"dummy_class_1_over=dummy_class_1.sample(df_class_0,replace=True)\n\ndummy_class_1_over.shape\n\n\n\n## replace because original lenght of df-class-1 is 2037\n## so just to duplicate : we will use replace=True","14c739d6":"\ndf_test_over=pd.concat([dummy_class_0,dummy_class_1_over],axis=0)\n\ndf_test_over.shape","60aa47c2":"df_test_over['Exited'].value_counts()","7771df89":"\n## lets run the model\n\nx=df_test_over.drop(\"Exited\",axis=1)\ny=df_test_over['Exited']\n\n\nxtrain,xtest,ytrain,ytest=train_test_split(x,y,test_size=0.2,random_state=42,stratify=y)\n\nypred=ANN(xtrain,ytrain,xtest,ytest,'binary_crossentropy',-1)\n\n\n## stratify will keep the proportion of x and y as same as that of in the dataset ","8e0eed79":"## scrool down in the epoch dialog box answer to see the confusion matrix\n\n\n## model accuracy : 77.06 %\n## F1 score  : 0_th class : 0.77\n## f1 score  : 1_th class : 0.77 ","65a36baf":"x=dummy_dataset.drop('Exited',axis=1)\ny=dummy_dataset['Exited']","64b19879":"pip install imbalanced-learn","aefeaba4":"from imblearn.over_sampling import SMOTE\n\nsmote=SMOTE(sampling_strategy='minority')\nx_sm,y_sm=smote.fit_sample(x,y)    \n\n\n\ny_sm.value_counts()\n\n## equal number of samples  for 0 and 1","9a6837c3":"\nxtrain,xtest,ytrain,ytest=train_test_split(x_sm,y_sm,test_size=0.2,random_state=42,stratify=y_sm)\n\nytrain.value_counts()\n\n## ytrain also has equal nuber of samples\n\n\n## stratify will keep the proportion of x and y as same as that of in the dataset","c42a0a55":"## lets run the model\n\nypred=ANN(xtrain,ytrain,xtest,ytest,'binary_crossentropy',-1)","8056dd7d":"## scrool down in the epoch dialog box answer to see the confusion matrix\n\n\n## model accuracy : 80.04 %\n## F1 score  : 0_th class : 0.80\n## f1 score  : 1_th class : 0.80","c8e0692f":"Model Accuracy is good but the f1 score is not good for 1_th prediction class.\n\nit is coming to be zero and 88% for 0_th class","ec6cd248":"# METHOD 3 : SMOTE METHOD","1de31afc":"**from the classification report :\n0_th class has : 0.92 as f1 score\n1_th class has : 0.53 as f1 score**","2dbab0f1":"# SPLITTING THE DATA","5f8b6192":"# Building ANN Model","5d919979":"# **What is \"Unbalanced Dataset\"**\n# \nIn simple terms, an unbalanced dataset is one in which the target variable has more observations in one specific class than the others. . Besides, the problem is that models trained on unbalanced datasets often have poor results when they have to generalize (predict a class or classify unseen observations","ae2b3b3d":"# METHOD 1 : UNDERSAMPLING","b2cd1aea":"# Underssampling:\n\nUndersampling techniques remove examples from the training dataset that belong to the majority class in order to better balance the class distribution, such as reducing the skew from a 1:100 to a 1:10, 1:2, or even a 1:1 class distribution. This is different from oversampling that involves adding examples to the minority class in an effort to reduce the skew in the class distribution.\n\n\n# Oversampling:\n\nWhich consists in over-sizing the minority class by adding observations.Oversampling involves supplementing the training data with multiple copies of some of the minority classes. Oversampling can be done more than once (2x, 3x, 5x, 10x, etc.) This is one of the earliest proposed methods, that is also proven to be robust.Instead of duplicating every sample in the minority class, some of them may be randomly chosen with replacement.\n\n\n# SMOTE :\n\nit is an oversampling technique that generates synthetic samples from the minority class. It is used to obtain a synthetically class-balanced or nearly class-balanced training set, which is then used to train the classifier","611fd757":"**since we can see values in some columns are not normalized.\nif not normalized : it will create hinderance in the dataset\nlets normalize the dataset**","747e28b9":"# splitting the data","4e9af50b":" ****EXITED vs ESTIMATEDSALARY","6f1863af":"# Please upvote the kernel if you liked my work.","75c5deb1":"## lets convert the data_types of object data type to int or float","7af4c31b":"# So \"SMOTE\" method gives the good accuracy as well as the good f1 scores compared to other methods","351331e5":"# EXITED vs TENURE","59459342":"***lets create a common function of neural network which we will be using for prediction***","cf8f968a":"since the surname column have many unique entries.\nLets scale that colum so that it will be easier to carry out the neural network on scaled values","b6c74965":"# METHOD 2 : OVERSAMPLING","f099c85b":"# **scrool down in the epoch dialog box to see the classification report**","52dc4d2c":" Lets try to improve the f1 scores for boh the classes","d32bdf26":"# Methods to tackle Unbalanced Dataset: \n    1. Undersampling\n    2. Oversampling\n    3. SMOTE "}}