{"cell_type":{"adff2151":"code","8c42791a":"code","a592b825":"code","a044a648":"code","ee39bd65":"code","d67382f4":"code","5bd79bb8":"code","a20fde90":"code","1d26421d":"code","c20b067c":"code","b5115b16":"code","05d04a19":"code","8e1b1df4":"code","0be77107":"code","45ab932c":"code","b4a24cb2":"code","239454ff":"code","25c3c280":"code","f4634876":"code","805cb7ff":"code","b247c805":"code","0849bf10":"code","6249c4a9":"code","4dfc3262":"code","5b2c2ef8":"code","322fc81d":"code","9f028161":"code","0e0f65b1":"code","ae9039e2":"code","3175aa9a":"code","e4ea8f35":"code","df0471ea":"code","c3dc7800":"code","66b8e889":"code","709ec493":"code","82f57c7c":"code","d98c0051":"code","7f69ccbb":"code","f18c6898":"code","3ba19f36":"code","eebeb1d9":"code","841069e3":"code","7be35a60":"code","124de62f":"code","3035fc5f":"code","ef4bef09":"code","08ba9b04":"code","518ba044":"code","821b5400":"code","b8bc34b1":"code","816946ba":"code","585f79c9":"code","9c68bb3d":"code","c26dee47":"code","697b83e2":"code","1ad05f5f":"code","9d182d64":"code","4e2c59c5":"code","2da45bd2":"code","9c990641":"code","e3526eaf":"code","e5a3c1b9":"code","5bce4c11":"code","cc028383":"code","808af687":"code","22d1bf28":"code","6d40e28a":"code","6d892b71":"code","0e31a2c2":"code","51112174":"code","e2c79043":"code","6f7d2e24":"code","530b4038":"code","034b0f27":"code","0d93a8dc":"code","d6d1db77":"code","8b7a82d2":"code","d8c58ccf":"code","25b91586":"code","7b851a01":"markdown","e7f1148b":"markdown","9788b7cc":"markdown","2463cecd":"markdown","20d7c190":"markdown","a611dd41":"markdown","78d10b2c":"markdown","feca0d4a":"markdown","45fa7d12":"markdown","234f4718":"markdown","8c6735f1":"markdown","cd15a2d4":"markdown","e9ef413e":"markdown","b58610f4":"markdown","35044389":"markdown","9a4e8acc":"markdown","6d2673ca":"markdown","160d3322":"markdown"},"source":{"adff2151":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report,confusion_matrix, accuracy_score, f1_score\nfrom sklearn.linear_model import LogisticRegression\nsns.set() #make the graphs prettier","8c42791a":"train_data=pd.read_csv('..\/input\/titanic\/train.csv')\ntest_data=pd.read_csv('..\/input\/titanic\/test.csv')\nframes=[train_data,test_data]\nfull_data=pd.concat(frames, sort=True)\nfull_data.reindex()\n#full_data = full_data.loc[:,~full_data.columns.duplicated()]","a592b825":"full_data","a044a648":"train_data","ee39bd65":"test_data","d67382f4":"train_data.describe()","5bd79bb8":"train_data.isnull().sum()","a20fde90":"train_data.groupby('Embarked').count()","1d26421d":"#print(train_data['Sex'].unique()())\nprint(train_data[['Sex', 'Survived']].groupby('Sex').count())","c20b067c":"print(train_data[['Pclass', 'Survived']].groupby('Pclass').count())","b5115b16":"print(train_data[['Fare', 'Survived']].groupby('Fare').count())","05d04a19":"print(train_data[['Sex','Pclass', 'Survived']].groupby('Survived').count())","8e1b1df4":"train_data[['Fare','Pclass']]","0be77107":"sns.countplot(train_data['Survived'])","45ab932c":"corr=train_data.corr()\ncorr","b4a24cb2":"fig, ax = plt.subplots()\n#fig.set_size_inches(15, 10)\nsns.heatmap(corr,cmap='coolwarm',annot=True,linewidths=2)","239454ff":"sns.scatterplot(x=train_data['Pclass'], y=train_data['Survived'])","25c3c280":"pd.value_counts(train_data['Fare'])","f4634876":"train_title=[]\nfor i in train_data['Name'].str.split(','):\n    train_title.append(i[1].split('.')[0].lstrip())\ntest_title=[]\nfor i in test_data['Name'].str.split(','):\n    test_title.append(i[1].split('.')[0].lstrip())\nfull_title=[]\nfor i in full_data['Name'].str.split(','):\n    full_title.append(i[1].split('.')[0].lstrip())\n","805cb7ff":"train_data['Title']=train_title\ntest_data['Title']=test_title\nfull_data['Title']=full_title","b247c805":"new_index=[i for i in range(len(full_data))]\nfull_data.index= new_index","0849bf10":"#Age filli\ntrain_data[\"Age\"].fillna(full_data.groupby(\"Title\")[\"Age\"].transform(\"median\"), inplace=True)\ntest_data[\"Age\"].fillna(full_data.groupby(\"Title\")[\"Age\"].transform(\"median\"), inplace=True)\n","6249c4a9":"test_data[\"Fare\"].fillna(full_data.groupby(\"Pclass\")[\"Fare\"].transform(\"mean\"), inplace=True)","4dfc3262":"train_data.isnull().sum()","5b2c2ef8":"train_data[\"Embarked\"] = train_data[\"Embarked\"].astype('category')\ntrain_data[\"Embarked\"] = train_data[\"Embarked\"].cat.codes\n\n\ntest_data[\"Embarked\"] = test_data[\"Embarked\"].astype('category')\ntest_data[\"Embarked\"] = test_data[\"Embarked\"].cat.codes\n\nfull_data[\"Embarked\"] = full_data[\"Embarked\"].astype('category')\nfull_data[\"Embarked\"] = full_data[\"Embarked\"].cat.codes","322fc81d":"#Embarked Imputation\ntrain_data= train_data.dropna(axis=0, subset=['Embarked'])\ntest_data= test_data.dropna(axis=0, subset=['Embarked'])","9f028161":"cols_to_transform=['Pclass']\ntrain_data = pd.get_dummies( train_data, columns = cols_to_transform)\ntest_data = pd.get_dummies( test_data, columns = cols_to_transform)\n'''\ntrain_data[\"Pclass\"] = train_data[\"Pclass\"].astype('category')\ntrain_data[\"Pclass\"] = train_data[\"Pclass\"].cat.codes\n\n\ntest_data[\"Pclass\"] = test_data[\"Pclass\"].astype('category')\ntest_data[\"Pclass\"] = test_data[\"Pclass\"].cat.codes\n\nfull_data[\"Pclass\"] = full_data[\"Pclass\"].astype('category')\nfull_data[\"Pclass\"] = full_data[\"Pclass\"].cat.codes'''","0e0f65b1":"train_data","ae9039e2":"train_data[\"Sex\"] = train_data[\"Sex\"].astype('category')\ntrain_data[\"Sex\"] = train_data[\"Sex\"].cat.codes\n\ntest_data[\"Sex\"] = test_data[\"Sex\"].astype('category')\ntest_data[\"Sex\"] = test_data[\"Sex\"].cat.codes\n\n#data[\"Sex\"] = data[\"Sex\"].cat.codes","3175aa9a":"pd.value_counts(train_data['Sex'])","e4ea8f35":"train_data[\"Cabin\"] = train_data[\"Cabin\"].astype('category')\ntrain_data[\"Cabin\"] = train_data[\"Cabin\"].cat.codes\n\ntest_data[\"Cabin\"] = test_data[\"Cabin\"].astype('category')\ntest_data[\"Cabin\"] = test_data[\"Cabin\"].cat.codes\n","df0471ea":"train_data['Cabin'].unique()","c3dc7800":"train_data","66b8e889":"X = train_data[train_data.columns.difference(['Survived','Name','Ticket','Title','PassengerId'])]## Select all columns except 'Survived','Name','Ticket','Title'\nprint(X.shape)\ny = train_data['Survived']\nprint(y.shape)","709ec493":"y","82f57c7c":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)","d98c0051":"print('X_train',X_train.shape)\nprint('y_train',y_train.shape)\nprint('X_test',X_test.shape)\nprint('y_test',y_test.shape)","7f69ccbb":"X_train.head()","f18c6898":"X_train.to_csv('Xtrain.csv')\ny_train.to_csv('ytrain.csv')","3ba19f36":"from sklearn import tree\ndt1_gini = tree.DecisionTreeClassifier()   ## Instantiating DecisionTree-Classifier\ndt1_gini.fit(X_train, y_train)             ## Training Model","eebeb1d9":"print('Train Accuracy =',dt1_gini.score(X_train, y_train))\nprint('Test Accuracy =',dt1_gini.score(X_test, y_test))","841069e3":"from sklearn.metrics import confusion_matrix\n\n\nprint(\"Train - Confusion Matrix\")\nprint(confusion_matrix(y_train,dt1_gini.predict(X_train)))\n\nprint(\"Test - Confusion Matrix\")\nprint(confusion_matrix(y_test,dt1_gini.predict(X_test)))","7be35a60":"features = X_train.columns\nimportances = dt1_gini.feature_importances_\nindices = np.argsort(importances)[::-1]\npd.DataFrame([X_train.columns[indices],np.sort(importances)[::-1]])","124de62f":"importances","3035fc5f":"fig, ax = plt.subplots()\nplt.title('Feature Importances')\nplt.barh(range(len(indices)), importances[indices], color='black')\nplt.yticks(range(len(indices)), [features[i] for i in indices])\nplt.xlabel('Relative Importance')\nplt.show()","ef4bef09":"test_data1=test_data[test_data.columns.difference(['Name','Ticket','Title','PassengerId'])]","08ba9b04":"test_data1.isnull().sum()","518ba044":"predicted_PID=list(zip(test_data['PassengerId'].tolist(),dt1_gini.predict(test_data1)))","821b5400":"passenger=[]\nfor i in test_data['PassengerId']:\n    for j in predicted_PID:\n        if i==j[0]:\n            passenger.append(j[1])","b8bc34b1":"test_data['Survived']=passenger","816946ba":"test_data[['PassengerId','Survived']]","585f79c9":"dt2_entropy = tree.DecisionTreeClassifier(criterion='entropy',max_depth =5)\ndt2_entropy.fit(X_train, y_train)\nprint('Train Accuracy =',dt2_entropy.score(X_train, y_train))\nprint('Test Accuracy =',dt2_entropy.score(X_test, y_test))","9c68bb3d":"dt3_fraction15 = tree.DecisionTreeClassifier(criterion='entropy',max_leaf_nodes=15)\ndt3_fraction15.fit(X_train, y_train)\nprint('Train Accuracy =',dt3_fraction15.score(X_train, y_train))\nprint('Test Accuracy =',dt3_fraction15.score(X_test, y_test))","c26dee47":"from sklearn.model_selection import RandomizedSearchCV\nfrom sklearn import tree\n\ndt = tree.DecisionTreeClassifier() \n\nparam_grid = {'criterion':['gini','entropy'],\n             'max_leaf_nodes': np.arange(5,30,1),\n             'max_depth':np.arange(3,15,1),\n             }\n\n\nrsearch_acc = RandomizedSearchCV(estimator=dt, param_distributions=param_grid,n_iter=500)\nrsearch_acc.fit(X_train, y_train)\n\nprint(rsearch_acc.best_estimator_)\nprint('Train Accuracy =',rsearch_acc.best_score_)\nprint('Test Accuracy =',rsearch_acc.score(X_test, y_test))\n\nprint(\"Train - Confusion Matrix\")\nprint(confusion_matrix(y_train,rsearch_acc.predict(X_train)))\nprint(\"Test - Confusion Matrix\")\nprint(confusion_matrix(y_test,rsearch_acc.predict(X_test)))\n","697b83e2":"predicted_PID=list(zip(test_data['PassengerId'].tolist(),dt3_fraction15.predict(test_data1)))\n\npassenger=[]\nfor i in test_data['PassengerId']:\n    for j in predicted_PID:\n        if i==j[0]:\n            passenger.append(j[1])\ntest_data['Survived_entropy15']=passenger\n","1ad05f5f":"test_data[['PassengerId','Survived_entropy15']].to_csv('Predict_Top_Accuracy.csv')","9d182d64":"sns.countplot(test_data['Survived'])","4e2c59c5":"test_data.plot.hist(subplots=True,figsize=(15, 15), bins=20, by='Survived')","2da45bd2":"from sklearn.tree import export_graphviz\nexport_graphviz(dt3_fraction15, out_file='tree_limited.dot', feature_names = X_test.columns,\n                class_names = dt3_fraction15.predict(X_train).astype(str),\n                rounded = True, proportion = False, precision = 2, filled = True)\n","9c990641":"!dot -Tpng tree_limited.dot -o tree_limited.png -Gdpi=600","e3526eaf":"from IPython.display import Image\nImage(filename = 'tree_limited.png')","e5a3c1b9":"from sklearn.ensemble import RandomForestClassifier\n\nrfc = RandomForestClassifier()\n\nrfc.fit(X_train, y_train)","5bce4c11":"## Predict\ntrain_predictions = rfc.predict(X_train)\ntest_predictions = rfc.predict(X_test)\n\nfrom sklearn.metrics import accuracy_score,f1_score\n\n\nprint(\"TRAIN DATA ACCURACY\",accuracy_score(y_train,train_predictions))\nprint(\"\\nTrain data f1-score for class '1'\",f1_score(y_train,train_predictions,pos_label=0))\nprint(\"\\nTrain data f1-score for class '2'\",f1_score(y_train,train_predictions,pos_label=1))\n\n### Test data accuracy\nprint(\"\\n\\n--------------------------------------\\n\\n\")\nprint(\"TEST DATA ACCURACY\",accuracy_score(y_test,test_predictions))\nprint(\"\\nTest data f1-score for class '1'\",f1_score(y_test,test_predictions,pos_label=0))\nprint(\"\\nTest data f1-score for class '2'\",f1_score(y_test,test_predictions,pos_label=1))","cc028383":"\npredicted_PID=list(zip(test_data['PassengerId'].tolist(),rfc.predict(test_data1)))\n\npassenger=[]\nfor i in test_data['PassengerId']:\n    for j in predicted_PID:\n        if i==j[0]:\n            passenger.append(j[1])\ntest_data['Survived_random_forest']=passenger\n\ntest_data[['PassengerId','Survived_random_forest']].to_csv('Prediction_rf_81.csv',index=False)","808af687":"rfc.feature_importances_\nfeat_importances = pd.Series(rfc.feature_importances_, index = X_train.columns)\nfeat_importances.plot(kind='bar')","22d1bf28":"feat_importances_ordered = feat_importances.nlargest(10)\nfeat_importances_ordered.plot(kind='bar')","6d40e28a":"from sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.metrics import make_scorer\n\n## n_jobs = -1 uses all cores of processor\n## max_features is the maximum number of attributes to select for each tree\nrfc_grid = RandomForestClassifier(n_jobs=-1, max_features='sqrt', class_weight='balanced_subsample')\n \n# Use a grid over parameters of interest\n## n_estimators is the number of trees in the forest\n## max_depth is how deep each tree can be\n## min_sample_leaf is the minimum samples required in each leaf node for the root node to split\n## \"A node will only be split if in each of it's leaf nodes there should be min_sample_leaf\"\n\nparam_grid = {\"n_estimators\" : [10, 25, 50, 75, 100],\n           \"max_depth\" : [10, 12, 14, 16, 18, 20],\n           \"min_samples_leaf\" : [5, 10, 15, 20],\n           \"class_weight\" : ['balanced','balanced_subsample']}\n \nrfc_cv_grid = RandomizedSearchCV(estimator = rfc_grid, \n                                 param_distributions = param_grid, \n                                 cv = 3, n_iter=10)","6d892b71":"rfc_cv_grid.fit(X_train, y_train)","0e31a2c2":"rfc_cv_grid.best_params_\n#rfc_cv_grid.best_estimator_","51112174":"## Predict\ntrain_predictions = rfc_cv_grid.predict(X_train)\ntest_predictions = rfc_cv_grid.predict(X_test)\n\nprint(\"TRAIN DATA ACCURACY\",accuracy_score(y_train,train_predictions))\nprint(\"\\nTrain data f1-score for class '1'\",f1_score(y_train,train_predictions,pos_label=0))\nprint(\"\\nTrain data f1-score for class '2'\",f1_score(y_train,train_predictions,pos_label=1))\n\n### Test data accuracy\nprint(\"\\n\\n--------------------------------------\\n\\n\")\nprint(\"TEST DATA ACCURACY\",accuracy_score(y_test,test_predictions))\nprint(\"\\nTest data f1-score for class '1'\",f1_score(y_test,test_predictions,pos_label=0))\nprint(\"\\nTest data f1-score for class '2'\",f1_score(y_test,test_predictions,pos_label=1))","e2c79043":"# Feature Impotances\nrfc_cv_grid.best_estimator_.feature_importances_","6f7d2e24":"## Get important Features\nfeat_importances = pd.Series(rfc_cv_grid.best_estimator_.feature_importances_, index = X_train.columns)\nfeat_importances_ordered = feat_importances.nlargest(10)\nfeat_importances_ordered.plot(kind='bar')","530b4038":"\npredicted_PID=list(zip(test_data['PassengerId'].tolist(),rfc.predict(test_data1)))\n\npassenger=[]\nfor i in test_data['PassengerId']:\n    for j in predicted_PID:\n        if i==j[0]:\n            passenger.append(j[1])\ntest_data['Survived_random_forest_grid']=passenger\n\ntest_data[['PassengerId','Survived_random_forest_grid']].to_csv('Prediction_rfg_80.csv',index=False)","034b0f27":"from xgboost.sklearn import XGBClassifier\n\nxgb_grid = XGBClassifier()\n \n\nparams = {\n        'min_child_weight': [1, 5, 10],\n        'gamma': [0.5, 1, 1.5, 2, 5],\n        'subsample': [0.6, 0.8, 1.0],\n        'colsample_bytree': [0.6, 0.8, 1.0],\n        'max_depth': [3, 4, 5]\n        }\n \nxgb_cv_grid = RandomizedSearchCV(estimator = xgb_grid, \n                                 param_distributions = params, \n                                 cv = 4, n_iter=10)","0d93a8dc":"xgb_cv_grid.fit(X_train, y_train)","d6d1db77":"## Predict\ntrain_predictions = xgb_cv_grid.predict(X_train)\ntest_predictions = xgb_cv_grid.predict(X_test)\n\nprint(\"TRAIN DATA ACCURACY\",accuracy_score(y_train,train_predictions))\nprint(\"\\nTrain data f1-score for class '1'\",f1_score(y_train,train_predictions,pos_label=0))\nprint(\"\\nTrain data f1-score for class '2'\",f1_score(y_train,train_predictions,pos_label=1))\n\n### Test data accuracy\nprint(\"\\n\\n--------------------------------------\\n\\n\")\nprint(\"TEST DATA ACCURACY\",accuracy_score(y_test,test_predictions))\nprint(\"\\nTest data f1-score for class '1'\",f1_score(y_test,test_predictions,pos_label=0))\nprint(\"\\nTest data f1-score for class '2'\",f1_score(y_test,test_predictions,pos_label=1))","8b7a82d2":"\npredicted_PID=list(zip(test_data['PassengerId'].tolist(),xgb_cv_grid.predict(test_data1)))\n\npassenger=[]\nfor i in test_data['PassengerId']:\n    for j in predicted_PID:\n        if i==j[0]:\n            passenger.append(j[1])\ntest_data['Survived_random_xgb']=passenger\n\ntest_data[['PassengerId','Survived_random_xgb']].to_csv('Prediction_xgb_817.csv',index=False)","d8c58ccf":"# Feature Impotances\nxgb_cv_grid.best_estimator_.feature_importances_","25b91586":"## Get important Features\nfeat_importances = pd.Series(xgb_cv_grid.best_estimator_.feature_importances_, index = X_train.columns)\nfeat_importances_ordered = feat_importances.nlargest(10)\nfeat_importances_ordered.plot(kind='bar')","7b851a01":"# Correlation Matrix","e7f1148b":"# Confusion Matrix","9788b7cc":"# Feature Importances of Random Forest","2463cecd":"# Random Forest Tuning with RandomizedSearchCV","20d7c190":"# Importing the packages","a611dd41":"# Data Transfotmation","78d10b2c":"# Importing the data","feca0d4a":"# Entropy based tree\n# Max tree depth = 5","45fa7d12":"# Data Exploration","234f4718":"# Test Train Split","8c6735f1":"# Random Hyper Parameter Tuning\n","cd15a2d4":"# Basic Prediction","e9ef413e":"# Entropy Based\n# Max leaf nodes = 5","b58610f4":"# Randon Forest ","35044389":"# XG-BOOST","9a4e8acc":"# Feature Importances","6d2673ca":"I have solved the Titanic Survivall Prediction Problem using the following Assumptions\n--------------------------------------------------------------------------------------\n1. Age is important factor for survival.\n2. Sex has been the major factor in solving the problem as the women were given first priority during survival situations.\n3. Since famous personalities have died in the Titanic disaster, I go with an assumption that first class is most affected.\n4. First Class women were the first to be given the chances of survival.\n\nHow did I clean the data:\n-------------------------\n1. I had conbined the test and train data to full data.for cleaning purpose.\n2. The age was imputed using median of the full data.\n3. The embarked blank records were just 2 so I had removed the data from the training set.\n\nHow did I predict and why did I use those models:\n------------------------------------------------\nDecision Tree Classifier:\n-------------------------\n>  I had used Decision Tree Classifier for interpretability.\n>  I had used improvised the accuracy of the decision tree using randomized Grid search which then gave an accuracy of 81.5%\n\nRandom Forest Classifier:\n-------------------------\n> I had used this model as I was greedy for accuracy which gave an accuracy of 80.3%.\n\nXGBoost Classifier:\n-------------------\n> I had used this model purely for accuracy which gave an accuracy of 81.7%\n\n\nNote:\n----\nData Exploration proved essential to find the insights in the data hence my first advice before going to any prediction model blindly would be exploration.","160d3322":"# Visualizing the output tree with 80% accuracy"}}