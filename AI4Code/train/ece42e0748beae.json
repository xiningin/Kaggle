{"cell_type":{"1bba36b9":"code","8555e9df":"code","3d0c1881":"code","2a7494b0":"code","c13308c4":"code","877e405d":"code","4b5783de":"code","726a2dc1":"code","3a1da484":"code","28dcbdfa":"code","47048390":"code","324a75e7":"code","1e769973":"code","4b17717b":"code","b55ea608":"code","1ff31194":"code","eff6b74d":"code","b3590806":"code","766af13e":"markdown","d603e3cd":"markdown","81c9d192":"markdown","7da458f9":"markdown","e5590ba5":"markdown","5ae7122b":"markdown"},"source":{"1bba36b9":"import pandas as pd\nimport numpy as np\nimport time\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import log_loss\nfrom sklearn.model_selection import cross_val_score\n\nfrom sklearn.preprocessing import LabelEncoder\n\nfrom lightgbm import LGBMClassifier\n\nimport optuna\nfrom optuna.visualization import plot_optimization_history, plot_param_importances\nfrom optuna.integration import LightGBMPruningCallback\n\nimport warnings\nwarnings.filterwarnings(\"ignore\", message=\"categorical_column in param dict is overridden.\")\nwarnings.filterwarnings(\"ignore\", message='Overriding the parameters from Reference Dataset.')\n","8555e9df":"train=pd.read_csv('..\/input\/tabular-playground-series-jun-2021\/train.csv')\ntest=pd.read_csv('..\/input\/tabular-playground-series-jun-2021\/test.csv')\nsub=pd.read_csv('..\/input\/tabular-playground-series-jun-2021\/sample_submission.csv')","3d0c1881":"conditions = [\n    (train.target == \"Class_1\"), (train.target == \"Class_2\"), (train.target == \"Class_3\"),\n    (train.target == \"Class_4\"), (train.target == \"Class_5\"), (train.target == \"Class_6\"),\n    (train.target == \"Class_7\"), (train.target == \"Class_8\"), (train.target == \"Class_9\")\n]\nchoices = [0, 1, 2, 3, 4, 5, 6, 7, 8]\ntrain[\"target\"] = np.select(conditions, choices)\n\nX_test = test.drop(['id'], axis=1)\nX = train.drop(['id', 'target'], axis=1)\ny = train.target","2a7494b0":"for col in X.columns:\n    X[col] = X[col].astype('category')\n    \nfor col in X_test.columns:\n    X_test[col] = X_test[col].astype('category')","c13308c4":"K=5\nSEED=314\nESR=100\n\nfixed_params = {\n    'random_state': SEED,\n    'n_estimators': 100000, \n    #'boosting_type':'goss',\n    'learning_rate':0.01,\n    'metric':'multi_logloss'\n}\n\nkf = StratifiedKFold(n_splits=K, shuffle=True, random_state=SEED)","877e405d":"oof_lightautoml=pd.read_csv('..\/input\/tps-jun2021-lightautoml\/oof_lightautoml.csv')\nsub_lightautoml=pd.read_csv('..\/input\/tps-jun2021-lightautoml\/sub_lightautoml.csv')\n\noof_lightautoml = oof_lightautoml.drop('id', axis=1)\noof_lightautoml.columns = ['pred_lightautoml' + str(i) for i in range(1, 10)]\n\nsub_lightautoml = sub_lightautoml.drop('id', axis=1)\nsub_lightautoml.columns = ['pred_lightautoml' + str(i) for i in range(1, 10)]\n\nX = pd.concat([X, oof_lightautoml], axis=1)\nX_test = pd.concat([X_test, sub_lightautoml], axis=1)","4b5783de":"#lgb_oof = np.zeros((X.shape[0], 9))\n#lgb_pred = 0\n#\n#for fold, (train_idx, val_idx) in enumerate(kf.split(X=X, y=y)):\n#    print(f\"\u279c FOLD :{fold}\")\n#    X_train = X.iloc[train_idx]\n#    y_train = y.iloc[train_idx]\n#    X_val = X.iloc[val_idx]\n#    y_val = y.iloc[val_idx]\n#\n#    start = time.time()\n#    \n#    model = LGBMClassifier(**fixed_params)\n#    \n#    model.fit(X_train, y_train,\n#              eval_set=(X_val, y_val),\n#              early_stopping_rounds=ESR,\n#              verbose=0,\n#              eval_metric=\"multi_logloss\" \n#             )\n#    \n#    lgb_oof[val_idx,:] = model.predict_proba(X_val)\n#    lgb_pred += model.predict_proba(X_test) \/ K\n#    \n#    lgb_logloss = log_loss(y_val, lgb_oof[val_idx])\n#    print(f\"score: {lgb_logloss:.6f} \")\n#    print(f\"elapsed: {time.time()-start:.2f} sec\\n\")\n#    \n#    del model\n#\n#lgb_logloss = log_loss(y, lgb_oof)\n#print(f\"Final logloss score: {lgb_logloss} \u2714\ufe0f \")","726a2dc1":"def objective(trial):\n\n    max_depth = trial.suggest_int('max_depth', 3, 12)\n    max_num_leaves = (2 ** max_depth) - 1\n    \n    hyperparams = {\n        #'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1),\n        'max_depth': max_depth,\n        'num_leaves': trial.suggest_int('num_leaves', 7, max_num_leaves),\n        'min_split_gain' : trial.suggest_float('min_split_gain', 1e-8, 5, log=True), # gama\n        'reg_alpha': trial.suggest_float(\"reg_alpha\", 1e-8, 10.0, log=True), # l1\n        'reg_lambda': trial.suggest_float(\"reg_lambda\", 1e-8, 10.0, log=True), # l2\n        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.1, 0.8), # feature_fraction \n        'subsample': trial.suggest_float('subsample', 0.1, 0.8), # bagging_fraction \n        'subsample_freq': trial.suggest_int(\"subsample_freq\", 1, 7), # bagging_freq \n        'min_child_samples': trial.suggest_int(\"min_child_samples\", 5, 100), # min_data_in_leaf \n        'cat_smooth': trial.suggest_float('cat_smooth', 10, 50),\n        'cat_l2': trial.suggest_int('cat_l2', 1, 20)\n        #'extra_trees': trial.suggest_categorical(\"extra_trees\", [True, False])\n    }\n\n    params = dict(**fixed_params, **hyperparams)\n    lgb_oof = np.zeros((X.shape[0], 9))\n\n    for i, (train_idx, val_idx) in enumerate(kf.split(X, y) ):\n\n        X_train = X.iloc[train_idx]\n        y_train = y.iloc[train_idx]\n        X_val = X.iloc[val_idx]\n        y_val = y.iloc[val_idx]\n\n        model = LGBMClassifier(**params)\n\n        model.fit(X_train, y_train,\n                  eval_set=(X_val, y_val),\n                  early_stopping_rounds=ESR,\n                  verbose=0,\n                  eval_metric=\"multi_logloss\",\n                  callbacks=[LightGBMPruningCallback(trial, 'multi_logloss', valid_name=\"valid_0\")]\n                 )\n\n        lgb_oof[val_idx,:] = model.predict_proba(X_val)\n\n    return log_loss(y, lgb_oof)","3a1da484":"study = optuna.create_study(direction='minimize',\n                            pruner=optuna.pruners.HyperbandPruner(),\n                            #pruner=optuna.pruners.HyperbandPruner(min_resource=100,  reduction_factor=4),\n                            #sampler=optuna.samplers.TPESampler(n_startup_trials=50, multivariate=True, seed=123)\n                           )\n\nstudy.optimize(objective, \n               timeout=60*60*7.5, \n               #timeout=60*3, \n               n_trials=None, \n               gc_after_trial=False)","28dcbdfa":"study.best_value","47048390":"plot_optimization_history(study)","324a75e7":"optuna.visualization.plot_parallel_coordinate(study)","1e769973":"plot_param_importances(study)","4b17717b":"final_params = dict(**fixed_params, **study.best_params)\n\n# without catfeatures\n#final_params = {'random_state': 314,\n# 'n_estimators': 100000,\n# 'metric': 'multi_logloss',\n# 'max_depth': 7,\n# 'learning_rate': 0.03320472235360897,\n# 'num_leaves': 9,\n# 'min_split_gain': 0.0001542021952793842,\n# 'reg_alpha': 6.740022575250581,\n# 'reg_lambda': 1.2597688856185852e-08,\n# 'colsample_bytree': 0.4631454737475204,\n# 'subsample': 0.29923683243864324,\n# 'subsample_freq': 6,\n# 'min_child_samples': 77,\n# 'cat_smooth': 26.06212029378543,\n# 'cat_l2': 16,\n# 'extra_trees': False}\n\n# with catfeatures\n#final_params = {'random_state': 314,\n# 'n_estimators': 100000,\n# 'learning_rate': 0.01,\n# 'metric': 'multi_logloss',\n# 'max_depth': 12,\n# 'num_leaves': 463,\n# 'min_split_gain': 1.3140865283434602,\n# 'reg_alpha': 0.032315272094481116,\n# 'reg_lambda': 3.117239474534205e-05,\n# 'colsample_bytree': 0.5280756068150421,\n# 'subsample': 0.14886257378346607,\n# 'subsample_freq': 1,\n# 'min_child_samples': 12,\n# 'cat_smooth': 43.10632572020626,\n# 'cat_l2': 9}","b55ea608":"final_params","1ff31194":"lgb_oof = np.zeros((X.shape[0], 9))\nlgb_pred = 0\n\nfor fold, (train_idx, val_idx) in enumerate(kf.split(X=X, y=y)):\n    print(f\"\u279c FOLD :{fold}\")\n    X_train = X.iloc[train_idx]\n    y_train = y.iloc[train_idx]\n    X_val = X.iloc[val_idx]\n    y_val = y.iloc[val_idx]\n\n    start = time.time()\n    \n    model = LGBMClassifier(**final_params)\n    \n    model.fit(X_train, y_train,\n              eval_set=(X_val, y_val),\n              early_stopping_rounds=ESR,\n              verbose=0,\n              eval_metric=\"multi_logloss\" \n             )\n    \n    lgb_oof[val_idx,:] = model.predict_proba(X_val)\n    lgb_pred += model.predict_proba(X_test) \/ K\n    \n    lgb_logloss = log_loss(y_val, lgb_oof[val_idx])\n    print(f\"score: {lgb_logloss:.6f} \")\n    print(f\"elapsed: {time.time()-start:.2f} sec\\n\")\n    \n    del model\n\nlgb_logloss = log_loss(y, lgb_oof)\nprint(f\"Final logloss score: {lgb_logloss} \u2714\ufe0f \")","eff6b74d":"sub.iloc[:, 1:] = lgb_pred\nsub.to_csv(\"sub_lgb_optuned.csv\", index=False)","b3590806":"oof_lgb = pd.concat([train.id,\n                     pd.DataFrame(lgb_oof, \n                                  columns=[\"Class_1\", \"Class_2\", \"Class_3\",\n                                           \"Class_4\", \"Class_5\", \"Class_6\",\n                                           \"Class_7\", \"Class_8\", \"Class_9\"])],\n                    axis=1)\noof_lgb.to_csv(\"oof_lgb_optuned.csv\", index=False)","766af13e":"# Load Dependencies","d603e3cd":"# Optuna","81c9d192":"# Baseline","7da458f9":"# Sub","e5590ba5":"# Final Model","5ae7122b":"see: https:\/\/www.kaggle.com\/gomes555\/tps-jun2021-lightautoml"}}