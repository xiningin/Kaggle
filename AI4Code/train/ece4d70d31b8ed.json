{"cell_type":{"23c511ff":"code","3401b137":"code","90fd026f":"code","7246a993":"code","19dd9208":"code","d9fce5ed":"code","29d3de61":"code","a66495ea":"code","218ab47b":"code","1dc9e317":"code","9c84d591":"code","c999267d":"code","ec44826f":"code","62424820":"code","ef4b79f6":"code","6cb99b08":"code","5ff0f9d8":"code","801e5c91":"code","841acb52":"code","d734ae8c":"code","c8ebab11":"code","e8011f64":"code","98122269":"code","f2504aac":"code","16a143ae":"code","1f0ff217":"code","07cc3d72":"code","53298443":"code","90723278":"code","d88a55d5":"code","3da4103f":"code","8911d302":"code","1720297f":"code","e8cbbfb9":"code","6a934f04":"code","5a701875":"code","f5a885ea":"code","4e82b9e2":"code","89727fa3":"code","4e29a1a1":"code","7b85c693":"code","a48b66bb":"code","63055037":"code","a6db839b":"code","78c28691":"code","b7ca9627":"code","7b9f41e2":"code","6ed4fffc":"code","c9979fc2":"code","d805e979":"code","943cf9f8":"code","6ced6cee":"markdown","e57bef9d":"markdown","fe52b63f":"markdown","ab05b5d2":"markdown","83fe0c34":"markdown","ac8806ef":"markdown","a542fab1":"markdown","6bf128d6":"markdown","0db8a7e3":"markdown"},"source":{"23c511ff":"import numpy as np\nimport pandas as pd\nimport spacy\nfrom tqdm import tqdm\nimport re, gc, os\nimport time\nimport pickle\npd.set_option('display.max_colwidth',None)\nimport warnings\nwarnings.filterwarnings('ignore')","3401b137":"# Process the data sets\n\ndef load_data():\n    train = pd.read_csv('..\/input\/quora\/train.csv')\n    train.drop_duplicates(keep='first')\n    test  = pd.read_csv('..\/input\/quora\/test.csv')\n    submission  = pd.read_csv('..\/input\/quora\/sample_submission.csv')\n    return train, test, submission\n\ntrain, test,_= load_data()","90fd026f":"round(train['target'].value_counts(normalize=True)*100) # unbalanced data","7246a993":"train.head()","19dd9208":"# lowercase\ntrain['question_text'] = train['question_text'].apply(lambda x:x.lower())\ntest['question_text'] = test['question_text'].apply(lambda x:x.lower())","d9fce5ed":"def remove_qmark(s):\n    return re.sub(r\"[\/?.,']+\",'',s)\n\ntrain['question_text'] = train['question_text'].apply(lambda x:remove_qmark(x))\ntest['question_text']  = test['question_text'].apply(lambda x:remove_qmark(x))","29d3de61":"# remove whitespace\ntrain['question_text'] = train['question_text'].apply(lambda x:' '.join(x.split()))\ntest['question_text'] = test['question_text'].apply(lambda x: ' '.join(x.split()))","a66495ea":"train.rename(columns= {'qid':'idx','target':'label'},inplace =True)\ntest.rename(columns= {'qid':'idx'},inplace =True)","218ab47b":"train.head(1)","1dc9e317":"train.info()","9c84d591":"!pip install s3fs -q\n!pip install fsspec==0.8.7 -qq\n!pip install --no-index --find-links ..\/input\/hf-datasets\/wheels datasets -qq","c999267d":"import datasets\nfrom datasets import Dataset","ec44826f":"from sklearn.utils import shuffle\nindex = train[:128000].index\ntrain = shuffle(train[:128000])\ntrain.index = index\nlen(train)*.2","62424820":"train.info()","ef4b79f6":"df_train = train[:-25600].reset_index(drop=True) #156672 divisible by 64\ndf_valid = train[-25600:].reset_index(drop=True)","6cb99b08":"train_dataset = Dataset.from_pandas(df_train)\nvalid_dataset = Dataset.from_pandas(df_valid)\ndf_train.shape,df_valid.shape","5ff0f9d8":"train_dataset[0]","801e5c91":"def change_transformers_dataset_2_right_format(dataset, label_name): \n    return dataset.map(lambda example: {'label': example[label_name]}, remove_columns=[label_name])","841acb52":"!pip install transformers  -q","d734ae8c":"import transformers\nprint(transformers.__version__)","c8ebab11":"from transformers import AutoTokenizer\n\nmodel_checkpoint= \"..\/input\/bert-base-uncased\"    \ntokenizer = AutoTokenizer.from_pretrained(model_checkpoint,use_fast=True)","e8011f64":"task = \"sst2\"\nbatch_size = 64","98122269":"def tokenizer_function(examples):\n    return tokenizer(examples['question_text'],max_length =133,padding=True,truncation=True)","f2504aac":"encoded_train = train_dataset.map(tokenizer_function, batched=True)\nencoded_valid = valid_dataset.map(tokenizer_function, batched=True)","16a143ae":"print(encoded_train[0])","1f0ff217":"gc.collect()","07cc3d72":"encoded_train.set_format('torch',columns=['input_ids','attention_mask','label'])\nencoded_valid.set_format('torch',columns=['input_ids','attention_mask','label'])","53298443":"gc.collect()","90723278":"encoded_valid.set_format('torch',columns=['input_ids','attention_mask','label'])","d88a55d5":"from datasets import load_metric\nmetric = load_metric(\"accuracy\")","3da4103f":"metric","8911d302":"from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n\nmetric_name = \"accuracy\"\nmodel_name = model_checkpoint.split(\"\/\")[-1]\n\n\nargs = TrainingArguments(\n    output_dir ='\/results',\n    evaluation_strategy = \"epoch\",\n    learning_rate=2e-5,\n    per_device_train_batch_size=batch_size,\n    per_device_eval_batch_size=batch_size,\n    num_train_epochs=2,\n    weight_decay=0.01,\n    load_best_model_at_end=True,\n    metric_for_best_model=metric_name,\n    do_predict=True\n)","1720297f":"import sklearn\nfrom sklearn import metrics\nfrom sklearn.metrics import precision_recall_fscore_support,accuracy_score\n\ndef compute_metrics(pred):\n    labels = pred.label_ids\n    preds = pred.predictions.argmax(-1)\n    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='micro')\n    acc = accuracy_score(labels, preds)\n    return {\n        'accuracy': acc,\n        'f1': f1,\n        'precision': precision,\n        'recall': recall\n    }","e8cbbfb9":"model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=2)","6a934f04":"import torch\n# determine the device we will be using for training\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(\"[INFO] training using {}\".format(torch.cuda.get_device_name(0)))","5a701875":"torch.cuda.empty_cache()","f5a885ea":"trainer = Trainer(\n    model,\n    args,\n    train_dataset=encoded_train,\n    eval_dataset =encoded_valid,\n    tokenizer=tokenizer,\n    compute_metrics=compute_metrics\n)","4e82b9e2":"trainer.train()","89727fa3":"trainer.evaluate()","4e29a1a1":"#del train,encoded_train,encoded_valid\ngc.collect()","7b85c693":"test.info()","a48b66bb":"test_dataset = Dataset.from_pandas(test)\nencoded_test = test_dataset.map(tokenizer_function, batched=True)\nencoded_test.set_format('torch',columns=['input_ids','attention_mask'])\nencoded_test[0]","63055037":"#encoded_test_input_ids = encoded_test['input_ids']\n#encoded_test_attention_mask = encoded_test['attention_mask']","a6db839b":"gc.collect()","78c28691":"output = trainer.predict(encoded_test)","b7ca9627":"preds = output[0]#predictions","7b9f41e2":"labels = np.argmax(preds,axis=1)","6ed4fffc":"_,_,sub = load_data()\nsub.head()","c9979fc2":"sub['target'] = labels\nsub['target'].value_counts()","d805e979":"sub.info()","943cf9f8":"sub.head(10)","6ced6cee":"In this notebook, \n\n**I am fine-tuning the pre-trained BERT_Base_Uncased on the 'Quora Questions\" Dataset of IIITB,**\n\nto classify the sentences into sincere and insincere.\n\n<h1 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:green; border:9; color:white' role=\"tab\" aria-controls=\"home\"><center>Contents<\/center><\/h1>\n\n1. [Intuition into the BERT Architecture](#BERT_Architecture)\n2. [Classification Task](#Classification_Task)\n3. [Loading Pre-trained BERT Base Uncased](#Loading_Pre-trained_BERT_Base_Uncased)\n4. [Tokenization](#BERT_Tokenizer)\n5. [Fine Tuning](#Fine_Tuning)\n6. [Predictions](#Predictions)","e57bef9d":"## Tokenizing the input sentences","fe52b63f":"## Loading Pre-trained BERT Base Uncased","ab05b5d2":"![](https:\/\/images.prismic.io\/peltarionv2\/e2931afe-0ec2-4485-b26c-74b7b4fae208_BERT_token_encoder.svg)","83fe0c34":"## Intuition into the BERT Architecture","ac8806ef":"## Classification Task\n\nWe are extracting the cls_head embeddings from the final encoder layer of the BERT to train the BERT and Classifier to get the final model.","a542fab1":"## Fine Tuning the pre-trained BERT Base Uncased \n\n> Feature Extraction: gathering the embeddings from the last encoder and training only classifier\n\n> Training both BERT model and Classifier","6bf128d6":"## Preprocessing of the data","0db8a7e3":"### Submission"}}