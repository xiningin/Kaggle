{"cell_type":{"570d6d08":"code","9cb818d0":"code","cfdb12aa":"code","f340217a":"code","379a0cc1":"code","db794123":"code","9edf06df":"code","dc329e79":"code","d97dd09e":"code","4887edf9":"code","b08d279c":"code","4d929287":"code","0d4af6d8":"code","79e582f4":"code","8204fa2e":"code","95d647f1":"code","0ca9098b":"code","5d068eda":"code","30872725":"code","e8c726c0":"code","b7e1ab96":"code","0003d1d9":"code","6075236e":"code","5351a9f8":"code","e652ffa0":"code","0765224b":"code","ffcff6f5":"code","bcd96dcc":"code","bfcf2aa4":"code","7a5a97af":"code","45b7613a":"code","9ca8734f":"code","f51e12dc":"code","7428b10d":"code","5a3c0837":"code","fc56fd58":"code","c881ea1c":"code","3d0e79cf":"code","bc80ba29":"code","6a445085":"code","9df01086":"code","59947e6f":"code","2c58f766":"markdown","adbfd11c":"markdown"},"source":{"570d6d08":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","9cb818d0":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.feature_selection import VarianceThreshold\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.utils import shuffle\nfrom sklearn.ensemble import RandomForestClassifier\n\n\npd.set_option('display.max_columns', 100)","cfdb12aa":"train = pd.read_csv('\/kaggle\/input\/porto-seguro-safe-driver-prediction\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/porto-seguro-safe-driver-prediction\/test.csv')","f340217a":"train.head()","379a0cc1":"train.tail()","db794123":"train.describe()","9edf06df":"train.shape","dc329e79":"train.drop_duplicates()","d97dd09e":"train.shape","4887edf9":"train.drop_duplicates()\ntrain.shape","b08d279c":"test.shape","4d929287":"train.info()","0d4af6d8":"test.info()","79e582f4":"data = []\nfor f in train.columns:\n    if f == 'target':\n        role = 'target'\n    elif f == 'id':\n        role = 'id'\n    else:\n        role = 'input'\n    \n    if 'bin' in f or f == 'target':\n        level = 'binary'\n    elif 'cat' in f or f == 'id':\n        level = 'nominal'\n    elif train[f].dtype == float:\n        level = 'interval'\n    elif train[f].dtype == int:\n        level = 'ordinal'\n    \n    keep = True\n    if f == 'id':\n        keep = False\n        \n    dtype = train[f].dtype\n    \n    f_dict = {\n        'varname': f,\n        'role': role,\n        'level': level,\n        'keep': keep,\n        'dtype': dtype\n    }\n    data.append(f_dict)\n\nmeta = pd.DataFrame(data, columns=['varname', 'role', 'level', 'keep', 'dtype'])\nmeta.set_index('varname', inplace=True)\n        \n    ","8204fa2e":"meta","95d647f1":"meta[(meta.level =='nominal') & (meta.keep)].index","0ca9098b":"pd.DataFrame({'count': meta.groupby(['role', 'level'])['role'].size()}).reset_index()","5d068eda":"v = meta[(meta.level=='interval') & (meta.keep)].index\ntrain[v].describe()","30872725":"v = meta[(meta.level == 'ordinal') & (meta.keep)].index\ntrain[v].describe()","e8c726c0":"v = meta[(meta.level == 'binary') & (meta.keep)].index\ntrain[v].describe()","b7e1ab96":"desired_apriori=0.10\n\n# Get the indices per target value\nidx_0 = train[train.target == 0].index\nidx_1 = train[train.target == 1].index\n\n# Get original number of records per target value\nnb_0 = len(train.loc[idx_0])\nnb_1 = len(train.loc[idx_1])\n\n# Calculate the undersampling rate and resulting number of records with target=0\nundersampling_rate = ((1-desired_apriori)*nb_1)\/(nb_0*desired_apriori)\nundersampled_nb_0 = int(undersampling_rate*nb_0)\nprint('Rate to undersample records with target=0: {}'.format(undersampling_rate))\nprint('Number of records with target=0 after undersampling: {}'.format(undersampled_nb_0))\n\n# Randomly select records with target=0 to get at the desired a priori\nundersampled_idx = shuffle(idx_0, random_state=37, n_samples=undersampled_nb_0)\n\n# Construct list with remaining indices\nidx_list = list(undersampled_idx) + list(idx_1)\n\n# Return undersample data frame\ntrain = train.loc[idx_list].reset_index(drop=True)","0003d1d9":"vars_with_missing = []\n\nfor f in train.columns:\n    missings = train[train[f] == -1][f].count()\n    if missings > 0:\n        vars_with_missing.append(f)\n        missings_perc = missings\/train.shape[0]\n        \n        print('Variable {} has {} records ({:.2%}) with missing values'.format(f, missings, missings_perc))\n        \nprint('In total, there are {} variables with missing values'.format(len(vars_with_missing)))","6075236e":"vars_to_drop = ['ps_car_03_cat', 'ps_car_05_cat']\ntrain.drop(vars_to_drop, inplace=True, axis=1)\nmeta.loc[(vars_to_drop), 'keep'] = False","5351a9f8":"# Imputing with the mean or mode\nmean_imp = SimpleImputer(missing_values=-1, strategy='mean')\nmode_imp = SimpleImputer(missing_values=-1, strategy='most_frequent')\ntrain['ps_reg_03'] = mean_imp.fit_transform(train[['ps_reg_03']]).ravel()\ntrain['ps_car_12'] = mean_imp.fit_transform(train[['ps_car_12']]).ravel()\ntrain['ps_car_14'] = mean_imp.fit_transform(train[['ps_car_14']]).ravel()\ntrain['ps_car_11'] = mode_imp.fit_transform(train[['ps_car_11']]).ravel()","e652ffa0":"v = meta[(meta.level == 'nominal') & (meta.keep)].index\n\nfor f in v:\n    dist_values = train[f].value_counts().shape[0]\n    print('Variable {} has {} distinct values'.format(f, dist_values))","0765224b":"# Script by https:\/\/www.kaggle.com\/ogrellier\n# Code: https:\/\/www.kaggle.com\/ogrellier\/python-target-encoding-for-categorical-features\ndef add_noise(series, noise_level):\n    return series * (1 + noise_level * np.random.randn(len(series)))\n\ndef target_encode(trn_series=None, \n                  tst_series=None, \n                  target=None, \n                  min_samples_leaf=1, \n                  smoothing=1,\n                  noise_level=0):\n    \"\"\"\n    Smoothing is computed like in the following paper by Daniele Micci-Barreca\n    https:\/\/kaggle2.blob.core.windows.net\/forum-message-attachments\/225952\/7441\/high%20cardinality%20categoricals.pdf\n    trn_series : training categorical feature as a pd.Series\n    tst_series : test categorical feature as a pd.Series\n    target : target data as a pd.Series\n    min_samples_leaf (int) : minimum samples to take category average into account\n    smoothing (int) : smoothing effect to balance categorical average vs prior  \n    \"\"\" \n    assert len(trn_series) == len(target)\n    assert trn_series.name == tst_series.name\n    temp = pd.concat([trn_series, target], axis=1)\n    # Compute target mean \n    averages = temp.groupby(by=trn_series.name)[target.name].agg([\"mean\", \"count\"])\n    # Compute smoothing\n    smoothing = 1 \/ (1 + np.exp(-(averages[\"count\"] - min_samples_leaf) \/ smoothing))\n    # Apply average function to all target data\n    prior = target.mean()\n    # The bigger the count the less full_avg is taken into account\n    averages[target.name] = prior * (1 - smoothing) + averages[\"mean\"] * smoothing\n    averages.drop([\"mean\", \"count\"], axis=1, inplace=True)\n    # Apply averages to trn and tst series\n    ft_trn_series = pd.merge(\n        trn_series.to_frame(trn_series.name),\n        averages.reset_index().rename(columns={'index': target.name, target.name: 'average'}),\n        on=trn_series.name,\n        how='left')['average'].rename(trn_series.name + '_mean').fillna(prior)\n    # pd.merge does not keep the index so restore it\n    ft_trn_series.index = trn_series.index \n    ft_tst_series = pd.merge(\n        tst_series.to_frame(tst_series.name),\n        averages.reset_index().rename(columns={'index': target.name, target.name: 'average'}),\n        on=tst_series.name,\n        how='left')['average'].rename(trn_series.name + '_mean').fillna(prior)\n    # pd.merge does not keep the index so restore it\n    ft_tst_series.index = tst_series.index\n    return add_noise(ft_trn_series, noise_level), add_noise(ft_tst_series, noise_level)","ffcff6f5":"train_encoded, test_encoded = target_encode(train[\"ps_car_11_cat\"], \n                             test[\"ps_car_11_cat\"], \n                             target=train.target, \n                             min_samples_leaf=100,\n                             smoothing=10,\n                             noise_level=0.01)\n    \ntrain['ps_car_11_cat_te'] = train_encoded\ntrain.drop('ps_car_11_cat', axis=1, inplace=True)\nmeta.loc['ps_car_11_cat','keep'] = False  # Updating the meta\ntest['ps_car_11_cat_te'] = test_encoded\ntest.drop('ps_car_11_cat', axis=1, inplace=True)","bcd96dcc":"v = meta[(meta.level == 'nominal') & (meta.keep)].index\n\nfor f in v:\n    plt.figure()\n    fig, ax = plt.subplots(figsize=(20,10))\n    # Calculate the percentage of target=1 per category value\n    cat_perc = train[[f, 'target']].groupby([f],as_index=False).mean()\n    cat_perc.sort_values(by='target', ascending=False, inplace=True)\n    # Bar plot\n    # Order the bars descending on target mean\n    sns.barplot(ax=ax, x=f, y='target', data=cat_perc, order=cat_perc[f])\n    plt.ylabel('% target', fontsize=18)\n    plt.xlabel(f, fontsize=18)\n    plt.tick_params(axis='both', which='major', labelsize=18)\n    plt.show();","bfcf2aa4":"def corr_heatmap(v):\n    correlations = train[v].corr()\n\n    # Create color map ranging between two colors\n    cmap = sns.diverging_palette(220, 10, as_cmap=True)\n\n    fig, ax = plt.subplots(figsize=(10,10))\n    sns.heatmap(correlations, cmap=cmap, vmax=1.0, center=0, fmt='.2f',\n                square=True, linewidths=.5, annot=True, cbar_kws={\"shrink\": .75})\n    plt.show();\n    \nv = meta[(meta.level == 'interval') & (meta.keep)].index\ncorr_heatmap(v)","7a5a97af":"s = train.sample(frac=0.1)","45b7613a":"sns.lmplot(x='ps_reg_02', y='ps_reg_03', data=s, hue='target', palette='Set1', scatter_kws={'alpha':0.3})\nplt.show()","9ca8734f":"sns.lmplot(x='ps_car_12', y='ps_car_13', data=s, hue='target', palette='Set1', scatter_kws={'alpha':0.3})\nplt.show()","f51e12dc":"sns.lmplot(x='ps_car_12', y='ps_car_14', data=s, hue='target', palette='Set1', scatter_kws={'alpha':0.3})\nplt.show()","7428b10d":"sns.lmplot(x='ps_car_15', y='ps_car_13', data=s, hue='target', palette='Set1', scatter_kws={'alpha':0.3})\nplt.show()","5a3c0837":"v = meta[(meta.level == 'ordinal') & (meta.keep)].index\ncorr_heatmap(v)","fc56fd58":"v = meta[(meta.level == 'nominal') & (meta.keep)].index\nprint('Before dummification we have {} variables in train'.format(train.shape[1]))\ntrain = pd.get_dummies(train, columns=v, drop_first=True)\nprint('After dummification we have {} variables in train'.format(train.shape[1]))","c881ea1c":"v = meta[(meta.level == 'interval') & (meta.keep)].index\npoly = PolynomialFeatures(degree=2, interaction_only=False, include_bias=False)\ninteractions = pd.DataFrame(data=poly.fit_transform(train[v]), columns=poly.get_feature_names(v))\ninteractions.drop(v, axis=1, inplace=True)  # Remove the original columns\n# Concat the interaction variables to the train data\nprint('Before creating interactions we have {} variables in train'.format(train.shape[1]))\ntrain = pd.concat([train, interactions], axis=1)\nprint('After creating interactions we have {} variables in train'.format(train.shape[1]))","3d0e79cf":"selector = VarianceThreshold(threshold=.01)\nselector.fit(train.drop(['id', 'target'], axis=1)) # Fit to train without id and target variables\n\nf = np.vectorize(lambda x : not x) # Function to toggle boolean array elements\n\nv = train.drop(['id', 'target'], axis=1).columns[f(selector.get_support())]\nprint('{} variables have too low variance.'.format(len(v)))\nprint('These variables are {}'.format(list(v)))","bc80ba29":"X_train = train.drop(['id', 'target'], axis=1)\ny_train = train['target']\n\nfeat_labels = X_train.columns\n\nrf = RandomForestClassifier(n_estimators=1000, random_state=0, n_jobs=-1)\n\nrf.fit(X_train, y_train)\nimportances = rf.feature_importances_\n\nindices = np.argsort(rf.feature_importances_)[::-1]\n\nfor f in range(X_train.shape[1]):\n    print(\"%2d) %-*s %f\" % (f + 1, 30,feat_labels[indices[f]], importances[indices[f]]))","6a445085":"sfm = SelectFromModel(rf, threshold = 'median', prefit=True)\nprint('Number of features before selection: {}'.format(X_train.shape[1]))\nn_features = sfm.transform(X_train).shape[1]\nprint('Number of features after selection: {}'.format(n_features))\nselected_vars = list(feat_lables[sfm.get_support()])","9df01086":"train = train[selected_vars + ['target']]","59947e6f":"scaler = StandardScaler()\nscaler.fit_transform(train.drop(['target'], axis=1))","2c58f766":"    Exploratory Data Visualization","adbfd11c":"Feature Scaling"}}