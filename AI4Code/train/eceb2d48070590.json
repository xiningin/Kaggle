{"cell_type":{"93b5134b":"code","7babd899":"code","5cacda13":"code","721ee148":"code","eb8091fa":"code","a18734f9":"code","54770f2f":"code","92b528e3":"code","990bd2dd":"code","176d30e9":"code","940d72f1":"code","0857b2dc":"code","ee6de30e":"code","e42d7d6d":"code","7cf5015f":"code","f52170c6":"code","d2efb8fe":"code","4f7118d0":"code","30351006":"code","8cd9a717":"code","5e6a79c6":"code","c006748e":"code","f23b2a7b":"code","144e88dc":"code","698f145b":"code","fd8118b8":"markdown","ded6d549":"markdown","db337fa9":"markdown","fa9d20ad":"markdown"},"source":{"93b5134b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","7babd899":"# import libraries\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers","5cacda13":"train_data = pd.read_csv('..\/input\/nicht-mnist\/train.csv',header=None, index_col =0)\ntest_data = pd.read_csv('..\/input\/nicht-mnist\/test.csv',header=None , index_col = 0)","721ee148":"train_data.head()","eb8091fa":"y = train_data.pop(1)\nx = train_data","a18734f9":"x = x \/ 255.0\ntest_data = test_data \/ 255.0","54770f2f":"x = x.values.reshape(-1,28,28,1)\ntest_data = test_data.values.reshape(-1,28,28,1)","92b528e3":"y = y.values","990bd2dd":"from sklearn.preprocessing import LabelEncoder\nlabel_encoder=LabelEncoder()\ny = pd.DataFrame(label_encoder.fit_transform(y))","176d30e9":"from sklearn.model_selection import train_test_split\nx_train, x_val, y_train, y_val = train_test_split(x, y, test_size = 0.2, random_state=1)","940d72f1":"from keras.layers import Dense, Dropout","0857b2dc":"cnn = keras.models.Sequential()\ncnn.add(keras.layers.Conv2D(filters=32, kernel_size=3, padding='same', activation='relu', input_shape=(28,28,1))) \ncnn.add(keras.layers.MaxPool2D(pool_size = 2, strides=2))\ncnn.add(keras.layers.Flatten())\ncnn.add(keras.layers.Dense(128, activation='relu'))\ncnn.add(keras.layers.Dense(10,activation='softmax'))","ee6de30e":"cnn.compile(  optimizer= 'Adam', loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n              metrics=['accuracy'])","e42d7d6d":"history = cnn.fit(x_train, y_train,\n                  validation_data=(x_val, y_val), \n                  epochs=20)","7cf5015f":"test_loss, test_acc = cnn.evaluate(x_val,  y_val, verbose=2)\nprint('\\nTest accuracy:', test_acc)","f52170c6":"probability_model = tf.keras.Sequential([cnn, \n                                         tf.keras.layers.Softmax()])","d2efb8fe":"def plot_history(history, ep):\n    import matplotlib.pyplot as plt\n    acc = history.history['accuracy']\n    val_acc = history.history['val_accuracy']\n\n    loss = history.history['loss']\n    val_loss = history.history['val_loss']\n\n    epochs_range = range(ep)\n\n    plt.figure(figsize=(16, 8))\n    plt.subplot(1, 2, 1)\n    plt.plot(epochs_range, acc, label='Training Accuracy')\n    plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n    plt.legend(loc='lower right')\n    plt.title('Training and Validation Accuracy')\n\n    plt.subplot(1, 2, 2)\n    plt.plot(epochs_range, loss, label='Training Loss')\n    plt.plot(epochs_range, val_loss, label='Validation Loss')\n    plt.legend(loc='upper right')\n    plt.title('Training and Validation Loss')\n    plt.show()","4f7118d0":"plot_history(history, 20)","30351006":"predictions = probability_model.predict(test_data)","8cd9a717":"results = np.argmax(predictions,axis = 1)","5e6a79c6":"results = pd.Series(results,name=\"target\")","c006748e":"submission = pd.concat([pd.Series(range(0,9364),name = \"Id\"),results],axis = 1)","f23b2a7b":"submission['target'] = label_encoder.inverse_transform(submission['target'])","144e88dc":"submission.head()","698f145b":"submission.to_csv('sub_ensemble_10_cnn.csv', index=False)","fd8118b8":"# Part 2: Buiding a model","ded6d549":"# Part 1: Data preparation","db337fa9":"# Part 4: Making prediction","fa9d20ad":"# Part 3: Training the CNN"}}