{"cell_type":{"9e27647e":"code","130905f7":"code","06890f1a":"code","566a33d0":"code","307ccb62":"code","c685a15c":"code","5fc2e106":"code","f9fb0fc1":"code","61e0b0c2":"code","07880457":"code","48a97d88":"code","232877a2":"code","36dd716e":"code","4a35bb0f":"code","6964c385":"code","18662701":"code","cfcb6c82":"code","c6973c54":"code","4619ae0f":"code","b28720a6":"code","76168972":"code","0a41dbfc":"code","1e77cf53":"code","9c34bbce":"code","7c48211c":"code","096ffae0":"code","67e67416":"code","d711d54b":"code","801572a7":"code","ab4ef4f6":"code","f4db214f":"code","cc083f0b":"code","c8bc2694":"code","633cd132":"code","75318481":"code","270d0c2c":"code","9ffa11c7":"code","10b12f16":"code","b6eba5cc":"code","e1f03054":"code","34c17d28":"code","6596d4ec":"code","0815eaae":"code","1658b660":"code","1ed23c6f":"code","d4aaf088":"code","3b052927":"code","48521a23":"code","c648fb3a":"code","34000ddf":"code","a0b69347":"markdown","95d933d9":"markdown","57d4a377":"markdown","d274d5c6":"markdown","f5cef7bc":"markdown","285652f2":"markdown","2eda33b1":"markdown","137297b9":"markdown","cd05c7ce":"markdown","5a55740e":"markdown","f39a240a":"markdown","7e233056":"markdown","0d86ddd1":"markdown","e05614e9":"markdown","cfc95747":"markdown","eef2e21d":"markdown","c002b1a1":"markdown","ae675058":"markdown","6ba58ee9":"markdown","9f6d69ae":"markdown","70d797f2":"markdown","862c5311":"markdown"},"source":{"9e27647e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","130905f7":"train=pd.read_csv('\/kaggle\/input\/Train.csv')\ntest=pd.read_csv('\/kaggle\/input\/Test.csv')","06890f1a":"train.info()","566a33d0":"import seaborn as sns\n# to render the graphs\nimport matplotlib.pyplot as plt\n# import module to set some ploting parameters\nfrom matplotlib import rcParams\n\n\n# This function makes the plot directly on browser\n%matplotlib inline\n\n# Seting a universal figure size \nrcParams['figure.figsize'] = 10,8\n# let us find the missing values.represented as yellow lines\n\n","307ccb62":"cardinality_train ={}\nfor col in train.columns:\n    cardinality_train[col] = train[col].nunique()\n    \ncardinality_test ={}\nfor col in test.columns:\n    cardinality_test[col] = test[col].nunique()\n\n\ncardinality_train\n\n\n","c685a15c":"cardinality_test\n","5fc2e106":"data = pd.concat([train, test], axis=0, sort=False)","f9fb0fc1":"sns.pairplot(data)","61e0b0c2":"plt.figure(figsize=(12, 7))\nsns.boxplot(x='Ever_Married',y='Age',data=data,palette='winter')\nplt.show()\n\n","07880457":"\nsns.countplot(\"Ever_Married\",data=data,hue=\"Gender\", palette=\"hls\")\n","48a97d88":"sns.set(style=\"ticks\")\n\ng = sns.catplot(x=\"Work_Experience\", y=\"Age\", hue=\"Gender\",height=10, data=data)","232877a2":"plt.figure(figsize=(12, 7))\nsns.boxplot(x='Work_Experience',y='Age',data=train,palette='winter')\nplt.show()","36dd716e":"plt.subplot(2,1,1)\nsns.countplot(\"Family_Size\",data=data,hue=\"Spending_Score\", palette=\"hls\")\nplt.ylabel(\"count\", fontsize=18)\nplt.xlabel(\"size\", fontsize=18)\nplt.title(\"size  dist via spend\", fontsize=20)\nplt.show()","4a35bb0f":"plt.figure(figsize=(12, 7))\nsns.boxplot(x='Spending_Score',y='Family_Size',data=data,palette='winter')\nplt.show()","6964c385":"tab_info=pd.DataFrame(data.dtypes).T.rename(index={0:'column type'})\ntab_info=tab_info.append(pd.DataFrame(data.isnull().sum()).T.rename(index={0:'null values (nb)'}))\ntab_info=tab_info.append(pd.DataFrame(data.isnull().sum()\/data.shape[0]*100).T.\n                         rename(index={0:'null values (%)'}))","18662701":"tab_info","cfcb6c82":"sns.heatmap(data.isnull(),yticklabels=False,cbar=False,cmap='viridis')\n","c6973c54":"data.info()","4619ae0f":"# replacing the missing values of different columns\ndata['Ever_Married'].fillna('unknown',inplace=True)\ndata['Graduated'].fillna('unknown',inplace=True)\ndata['Profession'].fillna('unknown',inplace=True)\ndata['Work_Experience'].fillna(data['Work_Experience'].mode()[0], inplace=True)\ndata['Family_Size'].fillna(0,inplace=True)\ndata['Var_1'].fillna('Cat_0',inplace=True)\ndata.head(5)\n","b28720a6":"sns.heatmap(data.isnull(),yticklabels=False,cbar=False,cmap='viridis')\nprint( data.shape)\nprint(train.shape)\nprint(test.shape)","76168972":"from sklearn.preprocessing import StandardScaler, LabelEncoder\n#label Encoding\nle = LabelEncoder()\ncat_cols = ['Gender', 'Ever_Married', 'Graduated', 'Profession', 'Spending_Score', 'Var_1']\n\nfor col in cat_cols:\n    data[col] = data[col].astype(str)\n    LE = le.fit(data[col])\n    data[col] = LE.transform(data[col])\n    \n\n#Encoding Category Variables\n#def frequency_encoding(col):\n   # fe=combine_set.groupby(col).size()\/len(data)\n   # data[col]=data[col].apply(lambda x: fe[x])\n\n#for col in list(data.select_dtypes(include=['object']).columns):\n    #if col!='Segmentation':\n       # frequency_encoding(col)","0a41dbfc":"ForSubmission=data\ndata=data.drop(['ID'],axis=1)","1e77cf53":"ForSubmission","9c34bbce":"data","7c48211c":"# all the data cleansing and feature engineering is complete so lets seperate the test and train\n\ntrain_df=data[data['Segmentation'].isnull()==False]\ntest_df=data[data['Segmentation'].isnull()==True]\nForSubmission_test_df=ForSubmission[ForSubmission['Segmentation'].isnull()==True]","096ffae0":"# Creating Train and Test Data\nX=train_df.drop(['Segmentation'],axis=1)\nY=train_df.Segmentation","67e67416":"from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.pipeline import Pipeline\n\nfrom sklearn.model_selection import train_test_split, KFold, cross_validate\nfrom sklearn.metrics import accuracy_score\n#Models\nfrom sklearn.cluster import KMeans\nfrom sklearn.decomposition import PCA\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import RidgeClassifier, SGDClassifier, LogisticRegression\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, ExtraTreesClassifier, BaggingClassifier, VotingClassifier, RandomTreesEmbedding\n","d711d54b":"X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size = 0.25, random_state = 22)","801572a7":"clfs = []\nseed = 3\n\nclfs.append((\"LogReg\", \n             Pipeline([(\"Scaler\", StandardScaler()),\n                       (\"LogReg\", LogisticRegression())])))\n\nclfs.append((\"XGBClassifier\",\n             Pipeline([(\"Scaler\", StandardScaler()),\n                       (\"XGB\", XGBClassifier())]))) \nclfs.append((\"KNN\", \n             Pipeline([(\"Scaler\", StandardScaler()),\n                       (\"KNN\", KNeighborsClassifier())]))) \n\nclfs.append((\"DecisionTreeClassifier\", \n             Pipeline([(\"Scaler\", StandardScaler()),\n                       (\"DecisionTrees\", DecisionTreeClassifier())]))) \n\nclfs.append((\"RandomForestClassifier\", \n             Pipeline([(\"Scaler\", StandardScaler()),\n                       (\"RandomForest\", RandomForestClassifier())]))) \n\nclfs.append((\"GradientBoostingClassifier\", \n             Pipeline([(\"Scaler\", StandardScaler()),\n                       (\"GradientBoosting\", GradientBoostingClassifier(max_features=15, n_estimators=150))]))) \n\nclfs.append((\"RidgeClassifier\", \n             Pipeline([(\"Scaler\", StandardScaler()),\n                       (\"RidgeClassifier\", RidgeClassifier())])))\n\nclfs.append((\"BaggingRidgeClassifier\",\n             Pipeline([(\"Scaler\", StandardScaler()),\n                       (\"BaggingClassifier\", BaggingClassifier())])))\n\nclfs.append((\"ExtraTreesClassifier\",\n             Pipeline([(\"Scaler\", StandardScaler()),\n                       (\"ExtraTrees\", ExtraTreesClassifier())])))\n\n#'neg_mean_absolute_error', 'neg_mean_squared_error','r2'\nscoring = 'accuracy'\nn_folds = 10\n\nresults, names  = [], [] \n\nfor name, model  in clfs:\n    kfold = KFold(n_splits=n_folds, random_state=seed)\n    cv_results = cross_val_score(model, X_train, y_train, cv= 5, scoring=scoring, n_jobs=-1)*100    \n    names.append(name)\n    results.append(cv_results)    \n    #msg = \"%s: %f (+\/- %f)\" % (name, cv_results.mean(),  cv_results.std())\n    #print(msg)\n    \n# boxplot algorithm comparison\nfig = plt.figure(figsize=(15,6))\nfig.suptitle('Classifier Algorithm Comparison', fontsize=22)\nax = fig.add_subplot(111)\nsns.boxplot(x=names, y=results)\nax.set_xticklabels(names)\nax.set_xlabel(\"Algorithmn\", fontsize=20)\nax.set_ylabel(\"Accuracy of Models\", fontsize=18)\nax.set_xticklabels(ax.get_xticklabels(),rotation=45)\nplt.show()\n","ab4ef4f6":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import mean_squared_error, mean_squared_log_error\nfrom sklearn.preprocessing import *\nfrom sklearn.model_selection import *\nfrom sklearn.metrics import *\n#import gc\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import confusion_matrix, mean_absolute_error,accuracy_score, classification_report\nkfold = KFold(n_splits=10, random_state=7)\nfrom lightgbm import LGBMClassifier\nfrom sklearn import metrics\nfrom sklearn.metrics import mean_squared_log_error\nlgbc = LGBMClassifier(n_estimators=550,\n                     learning_rate=0.03,\n                     min_child_samples=40,\n                     random_state=1,\n                     colsample_bytree=0.5,\n                     reg_alpha=2,\n                     reg_lambda=2)\n\nresultsLGB = cross_val_score(lgbc,X_train, y_train,cv=kfold)\nprint(\"LightGBM\",resultsLGB.mean()*100)\n\n#model = LGBMClassifier()\n#cv = RepeatedStratifiedKFold(n_splits = 10, n_repeats = 3, random_state = 22)\n#n_scores = cross_val_score(model, X_train, y_train, scoring = 'accuracy', cv = cv, n_jobs = -1, error_score = 'raise')\n#print(\"LightGBM\",n_scores.mean()*100)","f4db214f":"LGB=lgbc.fit(X_train,y_train)","cc083f0b":"y_predict_LGBM = LGB.predict(X_test)\n#print(100*(np.sqrt(mean_squared_log_error(np.exp(y_test), np.exp(y_predict_LGBM)))))\nresultsLGB_test = cross_val_score(lgbc,X_test, y_test,cv=kfold)\nprint(\"LightGBM\",resultsLGB_test.mean()*100)\ny_predict_LGBM","c8bc2694":"sorted(zip(LGB.feature_importances_, X_train), reverse = True)","633cd132":"test_df\n","75318481":"test_df=test_df.drop(['Segmentation'],axis=1)","270d0c2c":"y_predict_LGBM_testdata = LGB.predict(test_df)\ny_predict_LGBM_testdata","9ffa11c7":"sorted(zip(LGB.feature_importances_, test_df), reverse = True)","10b12f16":"#lets sumbit the solution\npred = pd.DataFrame()\npred['ID'] = ForSubmission_test_df['ID']\n#pred['Segmentation'] = pd.Series((model.predict(test_data)).ravel())\npred['Segmentation'] = pd.Series(y_predict_LGBM_testdata)\ny_predict_LGBM_testdata\npred.to_csv('LGBM_Customer_Segementation.csv', index = None)","b6eba5cc":"from catboost import  CatBoostClassifier\nfrom sklearn import metrics\nfrom sklearn.metrics import mean_squared_log_error\nfrom sklearn.metrics import roc_auc_score\n\n#cb = CatBoostRegressor(\n    #n_estimators = 1000,\n    #learning_rate = 0.11,\n    #iterations=1000,\n    #loss_function = 'RMSE',\n    #eval_metric = 'RMSE',\n    #verbose=0)\n    \ncb= CatBoostClassifier(\n    iterations=100, \n    learning_rate=0.1, \n    #loss_function='CrossEntropy'\n)\n\n#rmsle = 0\n#for i in ratio:\n # x_train,y_train,x_val,y_val = train_test_split(i)\n\n#CAT=cb.fit(X_train,y_train)\n#resultsCAT = cross_val_score(cb,X_train, y_train,cv=kfold)\n#print(\"CAT\",resultsCAT.mean()*100)\n                        \ncb.fit(X_train, y_train, eval_set=[(X_test, y_test)], verbose=50,early_stopping_rounds = 100)","e1f03054":"kfold = KFold(n_splits=10, random_state=7)\nresultsCAT = cross_val_score(cb,X_train, y_train,cv=kfold)\nprint(\"CAT\",resultsCAT.mean()*100)","34c17d28":"y_predict_CAT = cb.predict(X_train)\n#print(100*(np.sqrt(mean_squared_log_error(np.exp(y_train), np.exp(y_predict_CAT)))))\nresultsCAT_train = cross_val_score(cb,X_train, y_train,cv=kfold)\nprint(\"CAT\",resultsCAT_train.mean()*100)","6596d4ec":"y_predict_CAT_testdata = cb.predict(test_df)\ny_predict_CAT_testdata","0815eaae":"sorted(zip(cb.feature_importances_, test_df), reverse = True)","1658b660":"#lets sumbit the solution\npred_CAT = pd.DataFrame()\npred_CAT['ID'] = ForSubmission_test_df['ID']\n#pred['Segmentation'] = pd.Series((model.predict(test_data)).ravel())\npred_CAT['Segmentation'] = pd.Series(y_predict_CAT_testdata.ravel())\npred_CAT.to_csv('CAT_Customer_Segementation.csv', index = None)","1ed23c6f":"xg=XGBClassifier(booster='gbtree',verbose=0,learning_rate=0.07,max_depth=8,objective='multi:softmax',\n                  n_estimators=1000,seed=294)\nxg.fit(X_train,y_train)","d4aaf088":"resultsXG_train = cross_val_score(xg,X_train, y_train,cv=kfold)\nprint(\"XGBOOST\",resultsXG_train.mean()*100)","3b052927":"y_predict_XGB_testdata = xg.predict(test_df)\ny_predict_XGB_testdata","48521a23":"#lets sumbit the solution\npred_XGB = pd.DataFrame()\npred_XGB['ID'] = ForSubmission_test_df['ID']\n#pred['Segmentation'] = pd.Series((model.predict(test_data)).ravel())\npred_XGB['Segmentation'] = pd.Series(y_predict_XGB_testdata)\npred_XGB.to_csv('XGB_Customer_Segementation.csv', index = None)","c648fb3a":"Final_Submission=pd.DataFrame()\nFinal_Submission=pd.concat([Final_Submission,pd.DataFrame(y_predict_LGBM_testdata),pd.DataFrame(y_predict_CAT_testdata.ravel()),pd.DataFrame(y_predict_XGB_testdata)],axis=1)\nFinal_Submission.columns=['LGBM','CAT','XGB']\n\nForSubmission=Final_Submission.mode(axis=1)[0]\nForSubmission.head(5)","34000ddf":"submission_dataframe=pd.DataFrame()\nsubmission_dataframe['ID']=ForSubmission_test_df['ID']\nsubmission_dataframe['Segmentation']=ForSubmission\nsubmission_dataframe.to_csv('Final Ensembelled_LGBM_CAT_XGB__Customer_Segementation.csv', index = None)","a0b69347":"*Many missing values*","95d933d9":"*Both test and train are having same column names. Only segementation column is missing in Test. We can combine both the train and test for easy manupulation of data cleansing and feature engineering*","57d4a377":"**Machine learning Model**","d274d5c6":"**CATBoost on sample train 53.88 accuracy. Similar to  LGBM accuracty Lets check with Xtest**","f5cef7bc":"**While looking at the number of null values in the dataframe, it is interesting to note that  \u223c 10% of the entries are missing","285652f2":"**Submitting the LGBM solution**","2eda33b1":"**The modelling can be improved by implementing Neural Network. Accuracy can also improved by doing more feature engineering. Using One Hot Encoding instead of Label Encoding because many factors are non Categorical **","137297b9":"**XG Boost**","cd05c7ce":"**XG Boost Accuracy is 48% But that of LGBM and CATBOOST are more than 50%**","5a55740e":"**Ensemble from LGBM, CATBOOST and XGBOOT**","f39a240a":"**Age and Prefession followed by Work Experience is the most significant factors impacting the Segements**","7e233056":"*For sample train and test data set the Accuracy is same 82% so there is no overfitting*","0d86ddd1":"**We have classification problem. Natrually lets apply the classifiers. Lets find which classifer is better fit for the train data**","e05614e9":"****XG Boost classifier is best fit among all. But the accuracy is 54%. May be this problem is multi classification ****\n**Lets try LGBM\u00b6\nLightGBM is a gradient boosting framework that uses tree based learning algorithms. It is designed to be distributed and efficient with the following advantages:\nFaster training speed and higher efficiency.Lower memory usage.Better accuracy.Support of parallel and GPU learning.Capable of handling large-scale data**","cfc95747":"**Sample test accuracy for CATBOOST is 52.88 So no overfitting***","eef2e21d":"**submit the CATBOOST CSV**","c002b1a1":"*Yellow colour represents the test data that does not have segementation column*","ae675058":"**As per CATBOOS Profession, Age followed by Spending are the significant factors **","6ba58ee9":"**Age and Prefession followed by Work Experience is the most significant factors impacting the Segements**","9f6d69ae":"**CATBOOST. CatBoost is a high-performance open source library for gradient boosting on decision trees**","70d797f2":"** Customer Segementation**\n\nProblem Statement: An automobile company has plans to enter new markets with their existing products (P1, P2, P3, P4 and P5). After intensive market research, they\u2019ve deduced that the behavior of new market is similar to their existing market.\n\nIn their existing market, the sales team has classified all customers into 4 segments (A, B, C, D ). Then, they performed segmented outreach and communication for different segment of customers. This strategy has work exceptionally well for them. They plan to use the same strategy on new markets and have identified 2627 new potential customers.\n\nYou are required to help the manager to predict the right group of the new customers.\n\nFurther dataset and Problem statement from [Analytics Vidya Platform](http:\/\/https:\/\/datahack.analyticsvidhya.com\/contest\/janatahack-customer-segmentation)","862c5311":"**Final Submission after ensembelling from XGBOOST, LGBM and CATBOOST**"}}