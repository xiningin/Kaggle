{"cell_type":{"b9f1db6d":"code","179498a4":"code","03ebb13d":"code","b68b56ed":"code","9ebec413":"code","6d486dc6":"code","7a6a8248":"code","7cb78a97":"code","90322a64":"code","692c5f07":"code","d05e7b28":"code","af1258b4":"code","ead2fcee":"code","877b3471":"code","889a78e5":"code","ca739b5d":"code","480f3a0f":"code","32f97e46":"code","8dc15350":"code","ba713fc6":"code","cb5c0041":"code","e71f1e2c":"code","962899ed":"code","d1b5638c":"code","01425082":"code","6e3bb68b":"code","eb62042c":"code","a0388e70":"code","abbce469":"code","83783a04":"code","e66a404f":"code","f03b2831":"code","7e2e6f3c":"code","e25d432d":"code","dd61c1c0":"code","1ab2ec8d":"code","65c38863":"code","3b898a38":"code","4e488fd3":"code","fa1d1d53":"code","28d652f2":"code","895b3299":"code","67f21c17":"code","cbd590d8":"code","c8f48795":"code","e0bb39b2":"code","ef0f9981":"code","ca08d852":"code","5056610d":"code","3ebe4ee7":"code","16150157":"code","6e467619":"code","e1935894":"code","d4151ef6":"code","57ae6f0a":"code","6e5b034e":"code","bf4cc226":"code","4cd84994":"code","d50e229d":"code","0ca67bfc":"code","46c22a5d":"code","07b9de66":"code","9900c9d1":"code","2bb840cf":"code","d4335768":"code","b05caf0e":"markdown","8171fe76":"markdown","86ad55ad":"markdown","b1bc8a9f":"markdown","06508fff":"markdown","28c2190d":"markdown"},"source":{"b9f1db6d":"# importing libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Displays all the columns of a dataframe\npd.set_option('display.max_columns',None)","179498a4":"# Importing training dataset\ndf = pd.read_csv('..\/input\/train.csv')\ndf.head(10)","03ebb13d":"# Total number of columns in the dataframe\nprint(len(df.columns))","b68b56ed":"print(df.count())","9ebec413":"# Droping columns with more than 6000 missing values\nmask = df.count() > 3000\ndf.drop(df.columns[~mask], axis = 1, inplace = True)","6d486dc6":"# Columns having object datatype\ndf.columns[df.dtypes == 'object']","7a6a8248":"# printing unique values of above columns\nprint(df.dependency.unique(),'\\n')\nprint(df.edjefe.unique(),'\\n')\nprint(df.edjefa.unique())","7cb78a97":"# Droping useless columns \ndf.drop(['Id', 'idhogar'], axis = 1, inplace = True)\n\n# Columns having both continuous and categorical data should be encoded using labelencoder or get_dummies\ndf.drop(['dependency', 'edjefe', 'edjefa'], axis = 1, inplace = True)","90322a64":"# Total number of rows in the dataframe\nprint(df.count().max())\n\n# Total number of null entries\nprint(df.isnull().sum().sum())\n\n# printing columns with missing rows\ncol_mask = df.columns[df.isnull().sum() > 0]\ndf[col_mask].head(10)","692c5f07":"# Droping rows with missing values\ndf.dropna(inplace = True)","d05e7b28":"# Total Number of rows left\nprint(df.count().max())\n\n# Total Number of columns left\nprint(len(df.columns))\n\n# Total Number of null entries\nprint(df.isnull().sum().sum())","af1258b4":"# Splitting data into dependent and independent variable\n# X is the independent variables matrix\nX = df.drop('Target', axis = 1)\n\n# y is the dependent variable vector\ny = df.Target","ead2fcee":"# Scaling Features\nfrom sklearn.preprocessing import StandardScaler\nss = StandardScaler()\n\nX = ss.fit_transform(X)","877b3471":"# Checking p-values of dataframe features\n\nimport statsmodels.formula.api as sm\nX1 = np.append(arr = np.ones((9552,1)).astype(int), values = X, axis = 1)\nX_opt = X1[:, range(0,135)]\nregressor_OLS = sm.OLS(endog = y, exog = X_opt).fit()\nregressor_OLS.summary()","889a78e5":"# Feature Selection using Backward Elimination technique\n\ndef backwardelimination(x, sl):\n    numvars = len(X[0])\n    for i in range(0, numvars):\n        regressor_OLS = sm.OLS(y,x).fit()\n        maxvar = max(regressor_OLS.pvalues)\n        if maxvar> sl:\n            for j in range(0, numvars-i):\n                if(regressor_OLS.pvalues[j].astype(float) == maxvar):\n                    x = np.delete(x,j,1)\n    regressor_OLS.summary()\n    return x\n\nsl = 0.05\nX_opt = X1[:, range(0,135)]\nX_modeled = backwardelimination(X_opt, sl)","ca739b5d":"X_modeled.shape","480f3a0f":"# Removing additional columns added for backward elimination\nX = X[:,1:]\nX.shape","32f97e46":"# Applying XGBoost Classifier\n\nfrom xgboost import XGBClassifier\nclc = XGBClassifier(n_estimators = 10)\nclc.fit(X, y)","8dc15350":"scores1 = []\nscores2 = []\nscores3 = []\nscores4 = []","ba713fc6":"# Applying 5-fold cross-validation to X_modeled matrix\n\nfrom sklearn.model_selection import cross_val_score\naccuracies = cross_val_score(estimator = clc, X = X_modeled, y = y, cv = 5)\nscores1.append(accuracies.mean())\naccuracies.mean()","cb5c0041":"# Applying 5-fold cross-validation to X matrix\n\naccuracies = cross_val_score(estimator = clc, X = X, y = y, cv = 5)\nscores1.append(accuracies.mean())\naccuracies.mean()","e71f1e2c":"# Feature Selection using PCA(principal component analysis)\n\nfrom sklearn.decomposition import PCA\npca = PCA(n_components = None)\nX1 = pca.fit_transform(X)\nev = pca.explained_variance_ratio_\nev","962899ed":"# Using 4-Component PCA\n\npca = PCA(n_components = 4)\nX1 = pca.fit_transform(X)\nev = pca.explained_variance_ratio_\nev","d1b5638c":"# Applying 5-fold cross-validation to X1 matrix\nclc1 = XGBClassifier()\nclc1.fit(X1, y)\n\naccuracies = cross_val_score(estimator = clc1, X = X1, y = y, cv = 5)\nscores1.append(accuracies.mean())\naccuracies.mean()","01425082":"##Visualizing accuracies of different models using barplot\nlog_cols = [\"Model\", \"Accuracy\"]\nlog = pd.DataFrame(columns=log_cols)\n\nimport seaborn as sns\n\nacc_dict = {'All-columns': scores1[0],'Backward Elimination': scores1[1], 'PCA': scores1[2]}\n\nfor clf in acc_dict:\n    log_entry = pd.DataFrame([[clf, acc_dict[clf]]], columns=log_cols)\n    log = log.append(log_entry)\n\nplt.xlabel('Accuracy')\nplt.title('Classifier Accuracy')\n\nsns.set_color_codes(\"muted\")\nsns.barplot(x='Accuracy', y='Model', data=log, color=\"g\")","6e3bb68b":"# Applying Random-Forest Classifier\n\nfrom sklearn.ensemble import RandomForestClassifier\nclc = RandomForestClassifier(n_estimators = 10)\nclc.fit(X, y)","eb62042c":"# Applying 5-fold cross-validation to X matrix\n\naccuracies = cross_val_score(estimator = clc, X = X, y = y, cv = 5)\nscores2.append(accuracies.mean())\naccuracies.mean()","a0388e70":"# Applying 5-fold cross-validation to X_modeled matrix\n\naccuracies = cross_val_score(estimator = clc, X = X_modeled, y = y, cv = 5)\nscores2.append(accuracies.mean())\naccuracies.mean()","abbce469":"# Using PCA components\nclc1 = RandomForestClassifier(n_estimators = 10)\nclc1.fit(X1, y)\n\naccuracies = cross_val_score(estimator = clc1, X = X1, y = y, cv = 5)\nscores2.append(accuracies.mean())\naccuracies.mean()","83783a04":"##Visualizing accuracies of different models using barplot\nlog_cols = [\"Model\", \"Accuracy\"]\nlog = pd.DataFrame(columns=log_cols)\n\nacc_dict = {'All-columns': scores2[0],'Backward Elimination': scores2[1], 'PCA': scores2[2]}\n\nfor clf in acc_dict:\n    log_entry = pd.DataFrame([[clf, acc_dict[clf]]], columns=log_cols)\n    log = log.append(log_entry)\n\nplt.xlabel('Accuracy')\nplt.title('Classifier Accuracy')\n\nsns.set_color_codes(\"muted\")\nsns.barplot(x='Accuracy', y='Model', data=log, color=\"r\")","e66a404f":"# Applying K-Neighbors Classifier\n\nfrom sklearn.neighbors import KNeighborsClassifier\nclc = KNeighborsClassifier(n_neighbors = 5)\nclc.fit(X, y)","f03b2831":"# Applying 5-fold cross-validation to X matrix\n\naccuracies = cross_val_score(estimator = clc, X = X, y = y, cv = 5)\nscores3.append(accuracies.mean())\naccuracies.mean()","7e2e6f3c":"# Applying 5-fold cross-validation to X_modeled matrix\n\naccuracies = cross_val_score(estimator = clc, X = X_modeled, y = y, cv = 5)\nscores3.append(accuracies.mean())\naccuracies.mean()","e25d432d":"# Using PCA components\nclc1 = KNeighborsClassifier(n_neighbors = 10)\nclc1.fit(X1, y)\n\naccuracies = cross_val_score(estimator = clc1, X = X1, y = y, cv = 5)\nscores3.append(accuracies.mean())\naccuracies.mean()","dd61c1c0":"##Visualizing accuracies of different models using barplot\nlog_cols = [\"Model\", \"Accuracy\"]\nlog = pd.DataFrame(columns=log_cols)\n\nacc_dict = {'All-columns': scores3[0],'Backward Elimination': scores3[1], 'PCA': scores3[2]}\n\nfor clf in acc_dict:\n    log_entry = pd.DataFrame([[clf, acc_dict[clf]]], columns=log_cols)\n    log = log.append(log_entry)\n\nplt.xlabel('Accuracy')\nplt.title('Classifier Accuracy')\n\nsns.set_color_codes(\"muted\")\nsns.barplot(x='Accuracy', y='Model', data=log, color=\"b\")","1ab2ec8d":"#Importing libraries\n\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.wrappers.scikit_learn import KerasClassifier","65c38863":"# Splitting X1 into training and testing data\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X1, y, test_size = 0.25, random_state = 42)","3b898a38":"# Splitting independent variable into different classes for neural networks\n\ny_train = pd.get_dummies(y_train)","4e488fd3":"# Applying Artificial Neural Networks\n\ndef func():    \n    \n    clc = None\n\n    #initializing ANN\n    clc = Sequential()\n\n    #Adding input layer and 1st hidden layer\n    clc.add(Dense(activation=\"relu\", units=300, kernel_initializer=\"uniform\", input_dim=4))\n\n    #Adding 2nd hidden layer\n    clc.add(Dense(activation=\"relu\", units=300, kernel_initializer=\"uniform\"))\n\n    #Adding output layer\n    clc.add(Dense(activation=\"softmax\", units=4, kernel_initializer=\"uniform\"))\n\n    #Compiling ANN\n    clc.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n    \n    return clc\n\nestimator = KerasClassifier(build_fn = func, epochs = 10, batch_size = 5)\n\n\n#fitting ANN\nestimator.fit(X_train, y_train)\n\n","fa1d1d53":"# Predicting test set results\n\npred = estimator.predict(X_test)\nprint(np.unique(pred))\n\npred1 = pd.DataFrame(pred)\npred1.columns = ['Target']\npred1.head()","28d652f2":"y_test = y_test.reset_index()\ny_test.drop('index', axis = 1, inplace = True)\ny_test.head()","895b3299":"# Calculating Accuracy\nacc = (pred1 == y_test).sum()\/len(y_test)\nscores4.append(acc[0])","67f21c17":"# Splitting X_modeled dataset\n\nX1_train, X1_test, y1_train, y1_test = train_test_split(X_modeled, y, test_size = 0.25, random_state = 42)","cbd590d8":"# Splitting independent variable into different classes for neural networks\n\ny1_train = pd.get_dummies(y1_train)","c8f48795":"# Applying Artificial Neural Networks\n\ndef func():    \n    \n    clc = None\n\n    #initializing ANN\n    clc = Sequential()\n\n    #Adding input layer and 1st hidden layer\n    clc.add(Dense(activation=\"relu\", units=100, kernel_initializer=\"uniform\", input_dim=X1_train.shape[1]))\n\n    #Adding 2nd hidden layer\n    clc.add(Dense(activation=\"relu\", units=100, kernel_initializer=\"uniform\"))\n\n    #Adding output layer\n    clc.add(Dense(activation=\"softmax\", units=4, kernel_initializer=\"uniform\"))\n\n    #Compiling ANN\n    clc.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n    \n    return clc\n\nestimator = KerasClassifier(build_fn = func, epochs = 10, batch_size = 5)\n\n\n#fitting ANN\nestimator.fit(X1_train, y1_train)\n\n","e0bb39b2":"# Predicting test set results\n\npred = estimator.predict(X1_test)\nprint(np.unique(pred))\n\npred1 = pd.DataFrame(pred)\npred1.columns = ['Target']\npred1.head()","ef0f9981":"# Re-indexing y_test\ny1_test = y1_test.reset_index()\ny1_test.drop('index', axis = 1, inplace = True)\ny1_test.head()","ca08d852":"# Calculating Accuracy\nacc = (pred1 == y1_test).sum()\/len(y1_test)\nscores4.append(acc[0])","5056610d":"max(scores3)","3ebe4ee7":"## To save score of each model used\nlog_cols = [\"Classifier\", \"Accuracy\"]\nlog = pd.DataFrame(columns=log_cols)","16150157":"import seaborn as sns\n\nacc_dict = {'KNeighborsClassifier':max(scores3),'RandomForestClassifier': max(scores2), 'XGBClassifier':max(scores1), 'Neural networks':max(scores4)}\n\nfor clf in acc_dict:\n    log_entry = pd.DataFrame([[clf, acc_dict[clf]]], columns=log_cols)\n    log = log.append(log_entry)\n\nplt.xlabel('Accuracy')\nplt.title('Classifier Accuracy')\n\nsns.set_color_codes(\"muted\")\nsns.barplot(x='Accuracy', y='Classifier', data=log, color=\"black\")","6e467619":"# Reading test file\n\ndf1 = pd.read_csv('..\/input\/test.csv')\ndf1.head()","e1935894":"print(df1.columns[df1.dtypes == 'object'])\ndf1.drop(df1.columns[df1.dtypes == 'object'], axis = 1, inplace = True)","d4151ef6":"print(df1.count())\nprint(df1.columns[df1.count() < 7000])\ndf1.drop(df1.columns[df1.count() < 7000], axis = 1, inplace = True)","57ae6f0a":"print(df1.isnull().sum().sum())","6e5b034e":"# Replacing Missing values with most frequent values of that columns\n\nfrom sklearn.preprocessing import Imputer\nimp = Imputer(missing_values = 'NaN', strategy = 'most_frequent')\ndf1 = imp.fit_transform(df1)","bf4cc226":"np.isnan(df1).sum()","4cd84994":"from sklearn.preprocessing import StandardScaler\nss = StandardScaler()\n\ndf1 = ss.fit_transform(df1)","d50e229d":"# Using 4-Component PCA\n\nfrom sklearn.decomposition import PCA\npca = PCA(n_components = 4)\ndf1 = pca.fit_transform(df1)\nev = pca.explained_variance_ratio_\nev","0ca67bfc":"y_pred = clc1.predict(df1)","46c22a5d":"# Importing sample_submission file\n\ndf2 = pd.read_csv('..\/input\/sample_submission.csv')\ndf2.head()","07b9de66":"df2.drop('Target', axis = 1, inplace = True)","9900c9d1":"df2.set_index('Id', inplace = True)","2bb840cf":"df2['Target'] = y_pred\ndf2.head()","d4335768":"# Writing to My_submission.csv\ndf2.to_csv('My_submission.csv')","b05caf0e":"### Data Preprocessing","8171fe76":"### Costa Rican Household Poverty Level Prediction\n\nHere's the backstory: Many social programs have a hard time making sure the right people are given enough aid. It\u2019s especially tricky when a program focuses on the poorest segment of the population. The world\u2019s poorest typically can\u2019t provide the necessary income and expense records to prove that they qualify.\n\nIn Latin America, one popular method uses an algorithm to verify income qualification. It\u2019s called the Proxy Means Test (or PMT). With PMT, agencies use a model that considers a family\u2019s observable household attributes like the material of their walls and ceiling, or the assets found in the home to classify them and predict their level of need.\n\nWhile this is an improvement, accuracy remains a problem as the region\u2019s population grows and poverty declines. So, here we are trying to identify which households have the highest need for social welfare assistance using different algorithms, the new algorithm could be implemented in other countries around the world.","86ad55ad":"#### XGBoost with 4-components PCA model is the most accurate out of the four classifiers on the household dataset.","b1bc8a9f":"### Applying Classifiers","06508fff":"### Feature Selection and Extraction","28c2190d":"### Applying Preprocessing steps to test dataset (for kaggle only)"}}