{"cell_type":{"a6e56c7c":"code","f8b63c9e":"code","4dac0a72":"code","cbbef572":"code","7ba4b4e9":"code","1c5d4c3d":"code","a7305573":"code","166d3430":"code","3bf478db":"code","33a48c6f":"code","154167be":"code","3b695002":"code","e9294486":"code","0afa0ca9":"code","0dbba9e8":"code","58f3f4ba":"code","5caf91e8":"code","ea5cf810":"code","9975418d":"code","17eb28dc":"code","de0aead2":"code","60d5c49b":"code","21c66cd9":"code","6fcf8ddd":"code","366effcb":"code","82f91888":"code","bc89d6e9":"code","f36ea885":"code","96fb3b8b":"code","9d8d0901":"code","02d581ff":"code","eb65ad2e":"code","77bbb291":"code","bef748ac":"code","671eff27":"code","722f855b":"code","3618d42f":"code","a669cec0":"code","bfa49928":"code","a7e837af":"code","5490d205":"code","8b2e1ddc":"code","e45835a0":"code","46bc7282":"code","4f076a1d":"code","94f97dc0":"code","7eb8d7d2":"markdown","379ba7cc":"markdown","983aa39b":"markdown","448b0d2c":"markdown","72cba0c4":"markdown","0b523f58":"markdown","14dbddb4":"markdown","178baadf":"markdown","5597ae98":"markdown","7547c9f4":"markdown","0161413a":"markdown","d317b65e":"markdown","f8062b54":"markdown","0751ce30":"markdown","7fed0521":"markdown","8e991444":"markdown","fe817561":"markdown","afa2bc8f":"markdown","aab836ab":"markdown","82b76d3f":"markdown","9bffc14c":"markdown","eb138574":"markdown","e8fb1a2a":"markdown","001d5f8d":"markdown","c6ff6bc3":"markdown","492bfe23":"markdown","d48ee4ef":"markdown","9c585821":"markdown","77af3ab8":"markdown","c7fc7f91":"markdown","16db39e3":"markdown","7eb91da4":"markdown","ff7da2f9":"markdown","7c09828c":"markdown","bac10261":"markdown","6c37c819":"markdown","3940c0d7":"markdown","6caaf21e":"markdown","b99788c6":"markdown","7f95685d":"markdown","8d348202":"markdown","611bdb1e":"markdown","8878fa77":"markdown"},"source":{"a6e56c7c":"# load libraries\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\npd.set_option('display.max_columns', None)\n%matplotlib inline\nsns.set_context('notebook')\nsns.set_style('whitegrid')\nsns.set_palette('Blues_r')\n\n# turn off warnings for final notebook\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# load dataset\ndf = pd.read_csv('..\/input\/marketing-data\/marketing_data.csv')\nprint(df.info())\ndf.head()","f8b63c9e":"# clean up column names that contain whitespace\ndf.columns = df.columns.str.replace(' ', '')\n\n# transform Income column to a numerical\ndf['Income'] = df['Income'].str.replace('$', '')\ndf['Income'] = df['Income'].str.replace(',', '').astype('float')","4dac0a72":"df.head()","cbbef572":"# null values\ndf.isnull().sum().sort_values(ascending=False)","7ba4b4e9":"plt.figure(figsize=(8,4))\nsns.distplot(df['Income'], kde=False, hist=True)\nplt.title('Income distribution', size=16)\nplt.ylabel('count');","1c5d4c3d":"df['Income'].plot(kind='box', figsize=(3,4), patch_artist=True)","a7305573":"df['Income'] = df['Income'].fillna(df['Income'].median())","166d3430":"# select columns to plot\ndf_to_plot = df.drop(columns=['ID', 'AcceptedCmp1', 'AcceptedCmp2', 'AcceptedCmp3', 'AcceptedCmp4', 'AcceptedCmp5', 'Response', 'Complain']).select_dtypes(include=np.number)\n\n# subplots\ndf_to_plot.plot(subplots=True, layout=(4,4), kind='box', figsize=(12,14), patch_artist=True)\nplt.subplots_adjust(wspace=0.5);","3bf478db":"df = df[df['Year_Birth'] > 1900].reset_index(drop=True)\n\nplt.figure(figsize=(3,4))\ndf['Year_Birth'].plot(kind='box', patch_artist=True);","33a48c6f":"df.info()","154167be":"df['Dt_Customer'] = pd.to_datetime(df['Dt_Customer'])","3b695002":"list(df.columns)","e9294486":"# Dependents\ndf['Dependents'] = df['Kidhome'] + df['Teenhome']\n\n# Year becoming a Customer\ndf['Year_Customer'] = pd.DatetimeIndex(df['Dt_Customer']).year\n\n# Total Amount Spent\nmnt_cols = [col for col in df.columns if 'Mnt' in col]\ndf['TotalMnt'] = df[mnt_cols].sum(axis=1)\n\n# Total Purchases\npurchases_cols = [col for col in df.columns if 'Purchases' in col]\ndf['TotalPurchases'] = df[purchases_cols].sum(axis=1)\n\n# Total Campaigns Accepted\ncampaigns_cols = [col for col in df.columns if 'Cmp' in col] + ['Response'] # 'Response' is for the latest campaign\ndf['TotalCampaignsAcc'] = df[campaigns_cols].sum(axis=1)\n\n# view new features, by customer ID\ndf[['ID', 'Dependents', 'Year_Customer', 'TotalMnt', 'TotalPurchases', 'TotalCampaignsAcc']].head()","0afa0ca9":"# calculate correlation matrix\n## using non-parametric test of correlation (kendall), since some features are binary\ncorrs = df.drop(columns='ID').select_dtypes(include=np.number).corr(method = 'kendall')\n\n# plot clustered heatmap of correlations\nsns.clustermap(corrs, cbar_pos=(-0.05, 0.8, 0.05, 0.18), cmap='coolwarm', center=0);","0dbba9e8":"sns.lmplot(x='Income', y='TotalMnt', data=df[df['Income'] < 200000]);","58f3f4ba":"plt.figure(figsize=(4,4))\nsns.boxplot(x='Dependents', y='TotalMnt', data=df);","5caf91e8":"plt.figure(figsize=(4,4))\nsns.boxplot(x='Dependents', y='NumDealsPurchases', data=df);","ea5cf810":"plt.figure(figsize=(5.5,4))\nsns.boxplot(x='TotalCampaignsAcc', y='Income', data=df[df['Income']<200000]);","9975418d":"plt.figure(figsize=(5.5,4))\nsns.boxplot(x='TotalCampaignsAcc', y='Dependents', data=df);","17eb28dc":"sns.lmplot(x='NumWebVisitsMonth', y='NumWebPurchases', data=df);","de0aead2":"sns.lmplot(x='NumWebVisitsMonth', y='NumDealsPurchases', data=df);","60d5c49b":"plt.figure(figsize=(8,3))\nsns.distplot(df['NumStorePurchases'], kde=False, hist=True, bins=12)\nplt.title('NumStorePurchases distribution', size=16)\nplt.ylabel('count');","21c66cd9":"# drop unique ID\ndf.drop(columns=['ID', 'Dt_Customer'], inplace=True)","6fcf8ddd":"# one-hot encoding of categorical features\nfrom sklearn.preprocessing import OneHotEncoder\n\n# get categorical features and review number of unique values\ncat = df.select_dtypes(exclude=np.number)\nprint(\"Number of unique values per categorical feature:\\n\", cat.nunique())\n\n# use one hot encoder\nenc = OneHotEncoder(sparse=False).fit(cat)\ncat_encoded = pd.DataFrame(enc.transform(cat))\ncat_encoded.columns = enc.get_feature_names(cat.columns)\n\n# merge with numeric data\nnum = df.drop(columns=cat.columns)\ndf2 = pd.concat([cat_encoded, num], axis=1)\ndf2.head()","366effcb":"from sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\n\n# isolate X and y variables, and perform train-test split\nX = df2.drop(columns='NumStorePurchases')\ny = df2['NumStorePurchases']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\n\n# linear regression model\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\n\n# predictions\npreds = model.predict(X_test)\n\n# evaluate model using RMSE\nprint(\"Linear regression model RMSE: \", np.sqrt(mean_squared_error(y_test, preds)))\nprint(\"Median value of target variable: \", y.median())","82f91888":"import eli5\nfrom eli5.sklearn import PermutationImportance\n\nperm = PermutationImportance(model, random_state=1).fit(X_test, y_test)\neli5.show_weights(perm, feature_names = X_test.columns.tolist(), top=5)","bc89d6e9":"import shap\n\n# calculate shap values \nex = shap.Explainer(model, X_train)\nshap_values = ex(X_test)\n\n# plot\nplt.title('SHAP summary for NumStorePurchases', size=16)\nshap.plots.beeswarm(shap_values, max_display=5);","f36ea885":"plt.figure(figsize=(5,4))\ndf.groupby('Country')['TotalPurchases'].sum().sort_values(ascending=False).plot(kind='bar')\nplt.title('Total Number of Purchases by Country', size=16)\nplt.ylabel('Number of Purchases');","96fb3b8b":"plt.figure(figsize=(5,4))\ndf.groupby('Country')['TotalMnt'].sum().sort_values(ascending=False).plot(kind='bar')\nplt.title('Total Amount Spent by Country', size=16)\nplt.ylabel('Amount Spent');","9d8d0901":"sns.lmplot(x='MntGoldProds', y='NumStorePurchases', data = df);","02d581ff":"from scipy.stats import kendalltau\n\nkendall_corr = kendalltau(x=df['MntGoldProds'], y=df['NumStorePurchases'])\n\n# print results\nprint('Kendall correlation (tau): ', kendall_corr.correlation)\nprint('Kendall p-value: ', kendall_corr.pvalue)","eb65ad2e":"# sum the marital status and phd dummy variables - the Married+PhD group will have value of 2\ndf2['Married_PhD'] = df2['Marital_Status_Married'] + df2['Education_PhD']\ndf2['Married_PhD'] = df2['Married_PhD'].replace({2:'Married-PhD', 1:'Other', 0:'Other'})\n\n# plot MntFishProducts between Married-PhD and others\nplt.figure(figsize=(2.5,4))\nsns.boxplot(x='Married_PhD', y='MntFishProducts', data=df2);","77bbb291":"# independent t-test p-value\nfrom scipy.stats import ttest_ind\npval = ttest_ind(df2[df2['Married_PhD'] == 'Married-PhD']['MntFishProducts'], df2[df2['Married_PhD'] == 'Other']['MntFishProducts']).pvalue\nprint(\"t-test p-value: \", round(pval, 3))","bef748ac":"# now drop the married-phD column created above, to include only the original variables in the analysis below\ndf2.drop(columns='Married_PhD', inplace=True)","671eff27":"plt.figure(figsize=(8,3))\nsns.distplot(df['MntFishProducts'], kde=False, hist=True, bins=12)\nplt.title('MntFishProducts distribution', size=16)\nplt.ylabel('count');","722f855b":"# isolate X and y variables, and perform train-test split\nX = df2.drop(columns='MntFishProducts')\ny = df2['MntFishProducts']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\n\n# linear regression model\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\n\n# predictions\npreds = model.predict(X_test)\n\n# evaluate model using RMSE\nprint(\"Linear regression model RMSE: \", np.sqrt(mean_squared_error(y_test, preds)))\nprint(\"Median value of target variable: \", y.median())","3618d42f":"perm = PermutationImportance(model, random_state=1).fit(X_test, y_test)\neli5.show_weights(perm, feature_names = X_test.columns.tolist(), top=7)","a669cec0":"import shap\n\n# calculate shap values \nex = shap.Explainer(model, X_train)\nshap_values = ex(X_test)\n\n# plot\nplt.title('SHAP summary for MntFishProducts', size=16)\nshap.plots.beeswarm(shap_values, max_display=7);","bfa49928":"# convert country codes to correct nomenclature for choropleth plot\n# the dataset doesn't provide information about country codes\n## ...so I'm taking my best guess about the largest nations that make sense given the codes provided\ndf['Country_code'] = df['Country'].replace({'SP': 'ESP', 'CA': 'CAN', 'US': 'USA', 'SA': 'ZAF', 'ME': 'MEX'})\n\n# success of campaigns by country code\ndf_cam = df[['Country_code', 'AcceptedCmp1', 'AcceptedCmp2', 'AcceptedCmp3', 'AcceptedCmp4', 'AcceptedCmp5', 'Response']].melt(\n    id_vars='Country_code', var_name='Campaign', value_name='Accepted (%)')\ndf_cam = pd.DataFrame(df_cam.groupby(['Country_code', 'Campaign'])['Accepted (%)'].mean()*100).reset_index(drop=False)\n\n# rename the campaign variables so they're easier to interpret\ndf_cam['Campaign'] = df_cam['Campaign'].replace({'AcceptedCmp1': '1',\n                                                'AcceptedCmp2': '2',\n                                                'AcceptedCmp3': '3',\n                                                'AcceptedCmp4': '4',\n                                                'AcceptedCmp5': '5',\n                                                 'Response': 'Most recent'\n                                                })\n\n# choropleth plot\nimport plotly.express as px\n\nfig = px.choropleth(df_cam, locationmode='ISO-3', color='Accepted (%)', facet_col='Campaign', facet_col_wrap=2,\n                    facet_row_spacing=0.05, facet_col_spacing=0.01, width=700,\n                    locations='Country_code', projection='natural earth', title='Advertising Campaign Success Rate by Country'\n                   )\nfig.show()","a7e837af":"# calculate logistic regression p-values for campaign acceptance ~ country using generalized linear model\nimport statsmodels.formula.api as smf\nimport statsmodels as sm\nfrom scipy import stats\n\n## get the data of interest for glm\ndf_cam_wide = df[['Country', 'AcceptedCmp1', 'AcceptedCmp2', 'AcceptedCmp3', 'AcceptedCmp4', 'AcceptedCmp5', 'Response']]\n\n## to store statistics results\nstat_results = []\n\n## perform glm\nfor col in df_cam_wide.drop(columns='Country').columns:\n    this_data = df_cam_wide[['Country', col]]\n    \n    # define formula\n    formula = col+'~Country'\n    \n    # logistic regression (family=binomial)\n    model = smf.glm(formula = formula, data=this_data, family=sm.genmod.families.Binomial())\n    result = model.fit()\n    \n    # get chisquare value for overall model (CampaignAccepted ~ Country) and calculate p-value\n    chisq = result.pearson_chi2\n    pval = stats.distributions.chi2.sf(chisq , 7) # Df Model = 7 degrees of freedom when you run result.summary()\n     \n    # append to stat_results\n    stat_results.append(pval)\n    \n    # print stat summary for entire model\n    print(result.summary())\n    \n## check results\nprint(\"\\nChisq p-values: \", stat_results)","5490d205":"# plotting\n## merge in the original country codes provided in the dataset\ncountries = df[['Country', 'Country_code']].drop_duplicates().reset_index(drop=True)\ndf_cam2 = df_cam.merge(countries, how='left', on='Country_code')\ndf_cam2.head()\n\n## bar graphs\ng = sns.FacetGrid(df_cam2, col='Campaign', col_wrap=3)\ng.map(sns.barplot, 'Country', 'Accepted (%)')\nfor ax, pval in zip(g.axes.flat, stat_results):\n    ax.text(0, 65, \"Chisq p-value: \"+str(pval), fontsize=9) #add text;","8b2e1ddc":"# calculate success rate (percent accepted)\ncam_success = pd.DataFrame(df[['AcceptedCmp1', 'AcceptedCmp2', 'AcceptedCmp3', 'AcceptedCmp4', 'AcceptedCmp5', 'Response']].mean()*100, \n                           columns=['Percent']).reset_index()\n\n# plot\nsns.barplot(x='Percent', y='index', data=cam_success.sort_values('Percent'), palette='Blues')\nplt.xlabel('Accepted (%)')\nplt.ylabel('Campaign')\nplt.title('Marketing campaign success rate', size=16);","e45835a0":"# list of cols with binary responses\nbinary_cols = [col for col in df.columns if 'Accepted' in col] + ['Response', 'Complain']\n\n# list of cols for spending \nmnt_cols = [col for col in df.columns if 'Mnt' in col]\n\n# list of cols for channels\nchannel_cols = [col for col in df.columns if 'Num' in col] + ['TotalPurchases', 'TotalCampaignsAcc']","46bc7282":"# average customer demographics\ndemographics = pd.DataFrame(round(df.drop(columns=binary_cols+mnt_cols+channel_cols).mean(), 1), columns=['Average']).reindex([\n    'Year_Birth', 'Year_Customer', 'Income', 'Dependents', 'Kidhome', 'Teenhome', 'Recency'])\n\ndemographics","4f076a1d":"spending = pd.DataFrame(round(df[mnt_cols].mean(), 1), columns=['Average']).sort_values(by='Average').reset_index()\n\n# plot\nax = sns.barplot(x='Average', y='index', data=spending, palette='Blues')\nplt.ylabel('Amount spent on...')\n\n## add text labels for each bar's value\nfor p,q in zip(ax.patches, spending['Average']):\n    ax.text(x=q+40,\n            y=p.get_y()+0.5,\n            s=q,\n            ha=\"center\") ;","94f97dc0":"channels = pd.DataFrame(round(df[channel_cols].mean(), 1), columns=['Average']).sort_values(by='Average').reset_index()\n\n# plot\nax = sns.barplot(x='Average', y='index', data=channels, palette='Blues')\nplt.ylabel('Number of...')\n\n## add text labels for each bar's value\nfor p,q in zip(ax.patches, channels['Average']):\n    ax.text(x=q+0.8,\n            y=p.get_y()+0.5,\n            s=q,\n            ha=\"center\") ;","7eb8d7d2":"# Conclusion","379ba7cc":"* Remove rows where `Year_Birth <= 1900`:","983aa39b":"* Perform feature engingeering as outlined in notes above:","448b0d2c":"* Investigate anomaly: \n    - Number of web visits in the last month is not positively correlated with number of web purchases\n    - Instead, it is positively correlated with the number of deals purchased, suggesting that deals are an effective way of stimulating purchases on the website","72cba0c4":"### Which products are performing best?\n\n* The average customer spent...\n    - \\$25-50 on Fruits, Sweets, Fish, or Gold products\n    - Over \\$160 on Meat products\n    - Over \\$300 on Wines\n    - Over \\$600 total\n* Products performing best:\n    - Wines\n    - Followed by meats","0b523f58":"* Explore the directionality of these effects, using SHAP values:\n    - Findings:\n        - The number of store purchases increases with higher number of total purchases ('TotalPurchases')\n        - The number of store purchases decreases with higher number of catalog, web, or deals purchases ('NumCatalogPurchases', 'NumWebPurchases', 'NumDealsPurchases')\n    - Interpretation:\n        - Customers who shop the most in stores are those who shop less via the catalog, website, or special deals","14dbddb4":"# Section 01: Exploratory Data Analysis\n\n### Are there any null values or outliers? How will you wrangle\/handle them?\n\n#### Null Values\n* Identify features containing null values:","178baadf":"### What does the average customer look like for this company?\n\n* Basic demographics: The average customer is...\n    - Born in 1969\n    - Became a customer in 2013\n    - Has an income of roughly \\$52,000 per year\n    - Has 1 dependent (roughly equally split between kids or teens)\n    - Made a purchase from our company in the last 49 days","5597ae98":"#### Outliers\n\n* Identify features containing outliers:\n    - Findings: Multiple features contain outliers (see boxplots below), but the only that likely indicate data entry errors are `Year_Birth <= 1900`","7547c9f4":"### Fish has Omega 3 fatty acids which are good for the brain. Accordingly, do \"Married PhD candidates\" have a significant relation with amount spent on fish? \n\n* We will compare `MntFishProducts` between Married PhD candidates and all other customers:\n    - Findings: Married PhD candidates spend significantly less on fish products compared to other customers.","0161413a":"* Fit linear regression model to training data (70% of dataset)\n* Evaluate predictions on test data (30% of dataset) using RMSE:\n    - Findings: The RMSE is exceedingly small compared to the median value of the target variable, indicating good model predictions","d317b65e":"* Transform `Dt_Customer` to datetime:","f8062b54":"### Your supervisor insists that people who buy gold are more conservative. Therefore, people who spent an above average amount on gold in the last 2 years would have more in store purchases. Justify or refute this statement using an appropriate statistical test\n\n* Plot relationship between amount spent on gold in the last 2 years (`MntGoldProds`) and number of in store purchases (`NumStorePurchases`):\n    - Findings: There is a positive relationship, but is it statistically significant?","0751ce30":"# Marketing Analytics Exploratory\/Statistical Analysis task\n\n_by Jennifer Crockett_","7fed0521":"## Introduction\n\nThis notebook will accomplish the following task:\n\n**Overall goal:**  \nYou're a marketing analyst and you've been told by the Chief Marketing Officer that recent marketing campaigns have not been as effective as they were expected to be. You need to analyze the data set to understand this problem and propose data-driven solutions.  \n\n**Section 01: Exploratory Data Analysis**  \n* Are there any null values or outliers? How will you wrangle\/handle them?\n* Are there any variables that warrant transformations?\n* Are there any useful variables that you can engineer with the given data?\n* Do you notice any patterns or anomalies in the data? Can you plot them?\n\n**Section 02: Statistical Analysis**  \nPlease run statistical tests in the form of regressions to answer these questions & propose data-driven action recommendations to your CMO. Make sure to interpret your results with non-statistical jargon so your CMO can understand your findings.  \n\n* What factors are significantly related to the number of store purchases?\n* Does US fare significantly better than the Rest of the World in terms of total purchases?\n* Your supervisor insists that people who buy gold are more conservative. Therefore, people who spent an above average amount on gold in the last 2 years would have more in store purchases. Justify or refute this statement using an appropriate statistical test\n* Fish has Omega 3 fatty acids which are good for the brain. Accordingly, do \"Married PhD candidates\" have a significant relation with amount spent on fish? What other factors are significantly related to amount spent on fish? (Hint: use your knowledge of interaction variables\/effects)\n* Is there a significant relationship between geographical regional and success of a campaign?\n\n**Section 03: Data Visualization**  \nPlease plot and visualize the answers to the below questions.  \n\n* Which marketing campaign is most successful?\n* What does the average customer look like for this company?\n* Which products are performing best?\n* Which channels are underperforming?","8e991444":"* Clean up column names\n* Transform selected columns to numeric format:\n    - `Income` to float","fe817561":"* Plot illustrating negative effect of having dependents (kids & teens) on spending:","afa2bc8f":"* Identify features that significantly affect the amount spent on fish, using permutation importance:\n    - Significant features:\n        - 'TotalMnt', 'MntWines', 'MntMeatProducts', 'MntGoldProds', 'MntSweetProducts', 'MntFruits'\n        - All other features are not significant","aab836ab":"* Impute null values in `Income`, using median value (to avoid skewing of the mean due to outliers):","82b76d3f":"* Identify features that significantly affect the number of store purchases, using permutation importance:\n    - Significant features:\n        - 'TotalPurchases', 'NumCatalogPurchases', 'NumWebPurchases', 'NumDealsPurchases'\n        - All other features are not significant","9bffc14c":"* Plot total amount spent by country: \n    - Findings: \n        - Spain (SP) has the highest total amount spent on purchases\n        - US is second to last, therefore the US does not fare better than the rest of the world in terms of the total amount spent on purchases","eb138574":"* The cleaned dataset:","e8fb1a2a":"### Are there any useful variables that you can engineer with the given data?\n\n* Review a list of the feature names below, from which we can engineer:\n    - The total number of dependents in the home ('Dependents') can be engineered from the sum of 'Kidhome' and 'Teenhome'\n    - The year of becoming a customer ('Year_Customer') can be engineered from 'Dt_Customer'\n    - The total amount spent ('TotalMnt') can be engineered from the sum of all features containing the keyword 'Mnt'\n    - The total purchases ('TotalPurchases') can be engineered from the sum of all features containing the keyword 'Purchases'\n    - The total number of campains accepted ('TotalCampaignsAcc') can be engineered from the sum of all features containing the keywords 'Cmp' and 'Response' (the latest campaign)","001d5f8d":"Please plot and visualize the answers to the below questions.\n\n### Which marketing campaign is most successful?\n\n* Plot marketing campaign overall acceptance rates:\n    - Findings: The most successful campaign is the most recent (column name: `Response`)","c6ff6bc3":"* Explore the directionality of these effects, using SHAP values:\n    - Findings:\n        - The amount spent on fish increases with higher total amount spent ('TotalMnt')\n        - The amount spent on fish decreases with higher amounts spent on wine, meat, gold, fruit, or sweets ('MntWines', 'MntMeatProducts', 'MntGoldProds', 'MntSweetProducts', 'MntFruits')\n    - Interpretation:\n        - Customers who spend the most on fish are those who spend less on other products (wine, meat, gold, fruit, and sweets)","492bfe23":"* Plot illustrating positive effect of having dependents (kids & teens) on number of deals purchased:","d48ee4ef":"### What other factors are significantly related to amount spent on fish?\n\n* Like with the analysis of `NumStorePurchases` above, we will use use a linear regression model with `MntFishProducts` as the target variable, and then use machine learning explainability techniques to get insights about which features predict the amount spent on fish\n* Begin by plotting the target variable:","9c585821":"* Plots illustrating the positive effect of income and negative effect of having kids & teens on advertising campaign acceptance:\n\nNote: For the purposes of the following plot, limiting income to < 200000 to remove outlier","77af3ab8":"# Section 03: Data Visualization","c7fc7f91":"* Fit linear regression model to training data (70% of dataset)\n* Evaluate predictions on test data (30% of dataset) using RMSE:\n    - Findings: The RMSE is exceedingly small compared to the median value of the target variable, indicating good model predictions","16db39e3":"## Are there any variables that warrant transformations?\n\n* View data types:\n    - Findings: The `Dt_Customer` column should be transformed to datetime format","7eb91da4":"* Plot illustrating the effect of high income on spending:\n\nNote: For the purposes of this plot, limiting income to < 200000 to remove outlier","ff7da2f9":"* Drop uninformative features\n    - `ID` is unique to each customer\n    - `Dt_Customer` will be dropped in favor of using engineered variable `Year_Customer`\n* Perform one-hot encoding of categorical features, encoded data shown below:","7c09828c":"* Perform Kendall correlation analysis (non-parametric test since `MntGoldProducts` is not normally distributed and contains outliers):\n    - Findings: There is significant positive correlation between `MntGoldProds` and `NumStorePurchases`","bac10261":"# Section 02: Statistical Analysis\n\nPlease run statistical tests in the form of regressions to answer these questions & propose data-driven action recommendations to your CMO. Make sure to interpret your results with non-statistical jargon so your CMO can understand your findings.  \n\n### What factors are significantly related to the number of store purchases?  \n\n* We will use use a linear regression model with `NumStorePurchases` as the target variable, and then use machine learning explainability techniques to get insights about which features predict the number of store purchases\n* Begin by plotting the target variable:","6c37c819":"### Which channels are underperforming?\n\n* Channels: The average customer...\n    - Accepted less than 1 advertising campaign\n    - Made 2 deals purchases, 2 catalog purchases, 4 web purchases, and 5 store purchases\n    - Averaged 14 total purchases\n    - Visited the website 5 times\n* Underperforming channels:\n    - Advertising campaigns\n    - Followed by deals, and catalog","3940c0d7":"### Is there a significant relationship between geographical regional and success of a campaign?\n\n* Plot success of campaigns by region:\n    - Findings:\n        - The campaign acceptance rates are low overall\n        - The campaign with the highest overall acceptance rate is the most recent campaign (column name: `Response`)\n        - The country with the highest acceptance rate in any campaign is Mexico\n    - Is the effect of region on campaign success statistically significant? See below.","6caaf21e":"## Dataset\n\nBefore beginning the analysis, we will load and view the dataset, and perform some initial cleaning.\n\n* View the dataset info:","b99788c6":"* The feature `Income` contains 24 null values\n* Plot this feature to identify best strategy for imputation\n    - Findings: \n        - Most incomes are distributed between \\\\$0-\\\\$100,000, with a few outliers\n        - Will impute null values with median value, to avoid effects of outliers on imputation value","7f95685d":"### Does US fare significantly better than the Rest of the World in terms of total purchases?\n\n* Plot total number of purchases by country:\n    - Findings: \n        - Spain (SP) has the highest number of purchases\n        - US is second to last, therefore the US does not fare better than the rest of the world in terms of the total number of purchases","8d348202":"### Do you notice any patterns or anomalies in the data? Can you plot them?\n\n* To identify patterns, we will first identify feature correlations. Positive correlations between features appear red, negative correlations appear blue, and no correlation appears grey in the clustered heatmap below.\n* From this heatmap we can observe the following clusters of correlated features:\n    - The **\"High Income\"** cluster:\n        - Amount spent ('TotalMnt' and other 'Mnt' features) and number of purchases ('TotalPurchases' and other 'Num...Purchases' features) are positively correlated with 'Income'\n        - Purchasing in store, on the web, or via the catalog ('NumStorePurchases', 'NumWebPurchases', 'NumCatalogPurchases') is positively correlated with 'Income'\n    - The **\"Have Kids & Teens\"** cluster:\n        - Amount spent ('TotalMnt' and other 'Mnt' features) and number of purchases ('TotalPurchases' and other 'Num...Purchases' features) are negatively correlated with 'Dependents' (with a stronger effect from kids *vs.* teens)\n        - Purchasing deals ('NumDealsPurchases') is positively correlated with 'Dependents' (kids and\/or teens) and negatively correlated with 'Income'\n    - The **\"Advertising Campaigns\"** cluster:\n        - Acceptance of the advertising campaigns ('AcceptedCmp' and 'Response') are strongly positively correlated with each other\n        - Weak positive correlation of the advertising campaigns is seen with the \"High Income\" cluster, and weak negative correlation is seen with the \"Have Kids & Teens\" cluster\n* Anomalies:\n    - Surprisingly, the number of website visits in the last month ('NumWebVisitsMonth') does not correlate with an increased number of web purchases ('NumWebPurchases')\n    - Instead, 'NumWebVisitsMonth' is positively correlated with the number of deals purchased ('NumDealsPurchases'), suggesting that  suggesting that deals are an effective way of stimulating purchases on the website","611bdb1e":"* Statistical summary of regional effects on campaign success:\n    - Methodology: Performed logistic regression for Campaign Accepted by Country, reporting Chisq p-value for overall model.\n    - Findings: The regional differences in advertising campaign success are statistically significant.","8878fa77":"\n**Recall the overall goal:**  \nYou're a marketing analyst and you've been told by the Chief Marketing Officer that recent marketing campaigns have not been as effective as they were expected to be. You need to analyze the data set to understand this problem and propose data-driven solutions...\n\n**Summary of actionable findings to improve advertising campaign success:**  \n* The most successful advertising campaign was the most recent campaign (column name: `Response`), and was particularly successful in Mexico (>60% acceptance rate!)\n    - Suggested action: Conduct future advertising campaigns using the same model recently implemented in Mexico. \n* Advertising campaign acceptance is positively correlated with income and negatively correlated with having kids\/teens\n    - Suggested action: Create two streams of targeted advertising campaigns, one aimed at high-income individuals without kids\/teens and another aimed at lower-income individuals with kids\/teens\n* The most successful products are wines and meats (*i.e.* the average customer spent the most on these items)\n    - Suggested action: Focus advertising campaigns on boosting sales of the less popular items\n* The underperforming channels are deals and catalog purchases (*i.e.* the average customer made the fewest purchases via these channels)\n* The best performing channels are web and store purchases (*i.e.* the average customer made the most purchases via these channels)\n    - Suggested action: Focus advertising campaigns on the more successful channels, to reach more customers"}}