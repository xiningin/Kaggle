{"cell_type":{"94583875":"code","4db19cf2":"code","94ae04a8":"code","8a0ef61b":"code","4f0f0099":"code","ab4003b5":"code","efe8625c":"code","c569608d":"code","f0d4b7e8":"code","f971b871":"code","21ccef20":"code","0389441d":"code","6bdd63a8":"code","0bae4277":"code","0581860c":"code","3ef4d22f":"code","1c8eb73c":"code","893079e5":"code","f18cf298":"code","7f9191b6":"code","b7f584f6":"code","54ab4cdc":"code","b0de9dd4":"code","9e403cf6":"code","6ea1e69f":"code","428dc052":"code","05858359":"code","85760dc3":"code","e65cb49c":"code","2c949218":"code","186a6ddb":"code","dc5ce4fc":"code","22706fac":"code","369ba777":"markdown","e4de64c9":"markdown","43784710":"markdown","8074fab7":"markdown","010a1dd6":"markdown","21ea30d4":"markdown","4a64fd31":"markdown","80bbed17":"markdown","1ab38707":"markdown","31766bed":"markdown","54c2f976":"markdown","8a847946":"markdown","fe6cc899":"markdown","bc538bd4":"markdown","e05405bd":"markdown","8292426f":"markdown","16ce47ed":"markdown","e858a1f4":"markdown","0d7a3d4e":"markdown","9db3b715":"markdown","b161c7e9":"markdown","bb38d3d3":"markdown","aed0fac5":"markdown","89d34201":"markdown"},"source":{"94583875":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import LabelEncoder","4db19cf2":"# read train and test set\ntrain = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\n\ntrain.head()","94ae04a8":"train.dtypes","8a0ef61b":"for col in train.drop('PassengerId', axis=1).columns:\n    if train[col].dtypes == 'float64': \n        sns.histplot(data=train, x=col, binwidth=5)\n        plt.show()\n    elif train[col].dtypes == 'int64' or col in ['Sex', 'Embarked']:\n        sns.countplot(data=train, x=col)\n        plt.show()","4f0f0099":"for col in train.drop('PassengerId', axis=1).columns:\n    if train[col].dtypes == 'float64': \n        sns.histplot(data=train, x=col, hue='Survived', binwidth=5)\n        plt.show()\n    elif train[col].dtypes == 'int64' or col in ['Sex', 'Embarked']:\n        sns.countplot(data=train, x=col, hue='Survived')\n        plt.show()","ab4003b5":"Train = train.copy()\nTrain.isnull().sum()","efe8625c":"Train.loc[Train.Embarked.isnull()]","c569608d":"pd.crosstab([Train.Pclass, Train.Sex], Train.Embarked, margins = True).style.background_gradient(cmap = 'Oranges')","f0d4b7e8":"Train['Embarked'] = Train.Embarked.fillna('C') # female, first class","f971b871":"Train['Title'] = Train.Name.str.extract(r'([A-Za-z]+)\\.')\npd.crosstab([Train.Pclass], [Train.Title], margins = True).style.background_gradient(cmap = 'Oranges')","21ccef20":"Train.loc[Train.Age.isnull()].Title.value_counts()","0389441d":"Train.loc[Train.Age.isnull() & (Train.Title == 'Dr')]","6bdd63a8":"fill_dr_age = Train.loc[(Train['Pclass'] == 1) & (Train['Sex'] == 'male')].Age.mean()\nTrain.loc[766,'Age'] = fill_dr_age # male, first class","0bae4277":"reduced_Train = Train.loc[Train.Title.isin(['Master', 'Mrs', 'Miss', 'Mr'])]\nmean_ages = reduced_Train.groupby(['Title','Pclass']).agg({'Age':'mean'})\nmean_ages","0581860c":"def age_imputer(cols):\n    Pclass, Title, Age = cols[0], cols[1], cols[2]\n    \n    if pd.isnull(Age):\n        return mean_ages.loc[Title, Pclass].mean()\n    \n    else:\n        return Age\n\n# apply the function to change the Age column\nTrain['Age'] = Train[['Pclass', 'Title', 'Age']].apply(age_imputer, axis = 1)","3ef4d22f":"sns.histplot(Train.Age, binwidth=5, color='Orange', label='Imputed ages')\nsns.histplot(train.Age, binwidth=5, label='Original data')\nplt.legend()","1c8eb73c":"Test = test.copy()\nTest.isnull().sum()","893079e5":"Test.loc[Test.Fare.isnull()]","f18cf298":"fill_fare = Test.loc[(Test['Pclass'] == 3) & (Test['Sex'] == 'male')].Fare.mean()\nTest['Fare'] = Test.Fare.fillna(fill_fare) # male, third class","7f9191b6":"Test['Title'] = Test.Name.str.extract(r'([A-Za-z]+)\\.')\nTest.loc[Test.Age.isnull()].Title.value_counts()","b7f584f6":"Test.loc[Test.Age.isnull() & (Test.Title == 'Ms')]","54ab4cdc":"fill_ms_age = Test.loc[(Test['Pclass'] == 3) & (Test['Sex'] == 'female')].Age.mean()\nTest.loc[88,'Age'] = fill_ms_age # female, third class","b0de9dd4":"reduced_Test = Test.loc[Test.Title.isin(['Master', 'Mrs', 'Miss', 'Mr'])]\nmean_ages = reduced_Test.groupby(['Title','Pclass']).agg({'Age':'mean'})\nmean_ages","9e403cf6":"Test['Age'] = Test[['Pclass', 'Title', 'Age']].apply(age_imputer, axis = 1)\nTest.isnull().sum()","6ea1e69f":"sns.histplot(Test.Age, binwidth=5, color='Orange', label='Imputed ages')\nsns.histplot(test.Age, binwidth=5, label='Original data')\nplt.legend()","428dc052":"# step 1\nfor df in [Train, Test]:\n    df['Age_range'] = 'None'\n    df.loc[(df['Age'] <= 16), 'Age_range'] = 'Infants, kids, children'\n    df.loc[(df['Age'] > 16) & (df['Age'] <= 32), 'Age_range'] = 'Teenage, Early Adulthood'\n    df.loc[(df['Age'] > 32) & (df['Age'] <= 48), 'Age_range'] = 'Mid Adulthood'\n    df.loc[(df['Age'] > 48) & (df['Age'] <= 64), 'Age_range'] = 'Late Adulthood'\n    df.loc[(df['Age'] > 64), 'Age_range'] = 'Aged'\n    \n    # step 2\n    df['Family'] = df['SibSp'] + df['Parch']\n    \n    # step 3\n    other_titles = df.Title.unique().tolist()\n    for title in ['Mr', 'Mrs', 'Miss', 'Master']:\n        other_titles.remove(title)\n    df.loc[df.Title.isin(other_titles), 'Title'] = 'Other title'\n    \n    # step 4\n    df.drop(['PassengerId', 'Name', 'Age', 'SibSp', 'Parch', 'Ticket', 'Cabin'], axis=1, inplace=True)\n    \n# step 5 and 6\nTrain_new = pd.get_dummies(Train, columns=['Pclass', 'Sex', 'Title', 'Age_range', 'Embarked'])\nTest_new = pd.get_dummies(Test, columns=['Pclass', 'Sex', 'Title', 'Age_range', 'Embarked'])\n\nle = LabelEncoder()\nTrain_new['Family'] = le.fit_transform(Train['Family'])\nTest_new['Family'] = le.fit_transform(Test['Family'])","05858359":"from sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import confusion_matrix, classification_report, accuracy_score\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import KFold\nfrom sklearn.linear_model import LogisticRegression","85760dc3":"X = Train_new.drop(\"Survived\", axis=1)\ny = Train_new['Survived']","e65cb49c":"sc = StandardScaler()\nX = sc.fit_transform(X)\nk_fold = KFold(n_splits=10, shuffle=True, random_state=0)","2c949218":"def mean_score(model):\n    return np.mean(cross_val_score(model, X, y, cv=k_fold, scoring=\"accuracy\"))","186a6ddb":"logreg = LogisticRegression()\n\nprint(\"Accuracy Scores: \" + format(cross_val_score(logreg, X, y, cv=k_fold, scoring=\"accuracy\")))\nprint(\" \")\nprint(\"Mean Accuracy Score: \" + str(mean_score(logreg)))\nprint(\" \")\nprint(\"Standard Deviation:\", cross_val_score(logreg, X, y, cv=k_fold, scoring=\"accuracy\").std())","dc5ce4fc":"logreg.fit(X,y)\nscaled_test = sc.fit_transform(Test_new)\nlogreg_prediction = logreg.predict(scaled_test)","22706fac":"output = pd.DataFrame({'PassengerId': test.PassengerId, 'Survived': logreg_prediction})\noutput.to_csv('my_submission.csv', index=False)","369ba777":"Then I fill the missing values for Embarked with the mode considering passenger class and sex. I find it more simple to work with a crosstab in this case.","e4de64c9":"Now, as it is shown below the Logistic Regression model scores an accuracy of 0.826 with a standard deviation of 0.026 which is not that bad in my opinion. Notice that cross_val_score is a function that evalates model's score by cross-validation requiring as input data and target but also model and cross validation method. Using KFold it returns an array of 10 values so by defining the function mean_score we retrieve the mean of the accuracy scores given the model.","43784710":"Here I work for Fare just as I did for Embarked in the training set by directly looking at the corresponding row in the dataframe.","8074fab7":"The train dataset has missing values in `Age`, `Cabin`, `Fare`, `Embarked`. Since `Cabin` has too many missing values I will assume that no information can be retrieved and simply drop it (not in this part though). On the other hand `Embarked` have such a small number of missing values that it is reasonable to look at them directly.","010a1dd6":"Names data contain a title referred to each passenger that could be used as a label. Also the first four titles are associated with most of the passengers.","21ea30d4":"Before using the model to predict remember that also the test set must be scaled. In order to do this we use the standard scaler fitted on the train set and do not fit it again on the test data, as we want to apply the scaling with the same mean and std.","4a64fd31":"To make the imputation easier I apply an imputer to the Age column which takes values from the groupby object already defined.","80bbed17":"The I repeat the same procedure to fill Age.","1ab38707":"# EDA #\n\nThe Exploratory Data Analisys follow this scheme:\n* First we give a quick look at the data.\n* Then we proceed to univariate analisys to retrieve useful information about the single independent variables, so that we can better understand what is the situation.\n* In the end we make a bivariate analisys to show the relations between the indipendent variables and our target: this way we can formulate an hypotesis about which variables may have predictive power.","31766bed":"The data consist of 10(+1) features (data inherent to the target variable appear only in the train set). Listing them one by one will result useful when it will come to Feature Engineering.\n* `Survived`: target (binary) variable, where 0 = No and 1 = Yes.\n* `PassengerId`, `Pclass`, `Name`, `Ticket`, `Cabin`, `Embarked`: nominal variables. Pclass refers to socio-economic status.\n* `Sex`: binary variable.\n* `SibSp`, `Parch`: ordinal variables, referring to the number of family members travelling with a certain passenger.\n* `Age`, `Fare`: continuous variables. Fare refers to the fare paid by a certain passenger.\n\n\nLet's take a closer look to these features by displaying histograms for the continuous ones and bar diagrams for the discrete ones.","54c2f976":"# Feature Engineering #\n\nLet me summarize here, so that the workflow of this section will be more straightforward to follow.\n\nWhat can be derived from the data analisys up to now:\n* The variable Title makes Name useless, as the first one contains the true relevant information while the second contains noise.\n* SibSp and Parch can be summed into a single variable that will refer to the totl number of family members travelling with the passenger and this variable can be used as an ordinal feature.\n* Pclass, Embarked have been treated up to now as nominal features.\n* Sulla base della EDA ha senso dividere Age per fasce di et\u00e0 e poi considerarla come una Nominal feature.\n\nWhat are the next steps:\n1. Age binning.\n2. Create a new column Family.\n3. Since Title has some categories referred to a little number of passengers, bin these categories into a new one.\n4. Drop the unessential features.\n5. Encode Title, Age, Pclass, Embarked with one-hot encoding using get_dummies.\n6. Encode Family with label encoding using LabelEncoder.\n\nFor step 1 I will create a binning of Age into 5 categories. Since the oldest passenger has 80 years every gap will be a multiple of 16: this way none of them will represent a larger class of individuals and the resulting variable can be considered as nominal.","8a847946":"# Generating Submission #","fe6cc899":"I will impute C and not S as with the same numbers of embarked first class females C as the highest ratio with respect to the total number of embarked people.","bc538bd4":"The survival rate is:\n* higher for females (75%) than for males (20%).\n* higher for the 1st (63%) and 2nd (47%) classes and drammatically decreases for 3rd class passengers (24%).\n* higher for the infant\/kids than for the adults.\n* higher for passengers who paid a higher fare, though this is probably related to the overpopulation in the third class.\n* lower for passengers embarked in Southampton, though because most passengers embarked there.\n\nTherefore, we may consider `Sex`, `Pclass`, `Age`, `Fare`, `Embarked` as good candidates for predicting the target variable.","e05405bd":"# Modelling #","8292426f":"Firstly I impute the age of the passenger titled as Dr by looking straight to the dataframe and filling with the mean with respect to sex and class.","16ce47ed":"Now let's deal with the four labels where more data are missing. The logic is the same but I will define a function to apply to the df to make it more simple to impute the values.","e858a1f4":"Key info we get from the plots:\n* Most passengers are male (around 65%).\n* The third class is the most populated (around 55%) and the other two classes are almost equally distributed.\n* As one can expect from the last remark, most passegners paid a low fare and there is only a little percentage of people who paid more.\n* Most passengers travelled alone.\n* The age distribution is skewed, most passengers have from 15 to 35 years, there are few old people but a considerable number of kids\/infants.\n* Most passengers embarked in Southampton (around 70%).\n\nNow let's look for surviving differences among the independent variables.","0d7a3d4e":"# Missing values: test set #","9db3b715":"# Missing values #\n\nLooking around on Kaggle I found out there's a range of people performing imputation on both train and test at once. This confused me a bit at the beginning but in the end this kernel https:\/\/www.kaggle.com\/gunesevitan\/advanced-feature-engineering-tutorial-with-titanic (which helped me a lot in the development of this project) made me understand what this could imply. Here I quote few lines that express the concept clearly.\n\n<br>\n\nThe division between training and test set is an attempt to replicate the situation where you have past information and are building a model which you will test on future as-yet unknown information: the training set takes the place of the past and the test set takes the place of the future, so you only get to test your trained model once.\nKeeping the past\/future analogy in mind, this means anything you do to pre-process or process your data, such as imputing missing values, you should do on the training set alone. You can then remember what you did to your training set if your test set also needs pre-processing or imputing, so that you do it the same way on both sets.\n\n<br>\nAlso, I will work on a copy of the train and test set so that i can look at the differences later.","b161c7e9":"# Missing values: Age #\n\nBy looking at the features available within the dataset no one except Name seems appropriate to me to try and guess the missing data from the Age column. Here is why: ","bb38d3d3":"The dataframe is finally clean. Let's take a look at how the Age distribution changed.","aed0fac5":"First let's split train features from the target variable.","89d34201":"Then we arrange things for cross validation using KFold with k=10. KFold devides the input dataset into K parts with same size and then shuffle them so that 1 of them acts like a test set and the other K-1 act like train set and this is repeated for K times and stops when each splitted part has played the role of test set. It will return then K values of accuracy  for the model and we can take the mean of these values as the real accuracy. This is usually the preffered method as it gives a better indication of how well your model will perform on unseen data. Of course it is useful to look also at the standard deviation of the accuracy vector as low std means high consistency in the value we get.\n\nIt also follows that while during train test split we scale train and test data separetely with KFold this is not possible, as we want to plug the entire train set into it."}}