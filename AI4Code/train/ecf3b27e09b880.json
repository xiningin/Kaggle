{"cell_type":{"97d089f8":"code","8c31937f":"code","0f8acc36":"code","f22a0d5b":"code","0598ba72":"code","5aede0c0":"code","e4678193":"code","9bdbee6f":"code","974e3f57":"code","524772cc":"code","15696529":"code","0d385cbf":"code","96ebe7c2":"code","de06357d":"code","b827f206":"code","fb11f190":"code","e2dd05f9":"code","9d07ce5f":"code","2e625138":"code","ec503cc3":"code","81fd347d":"code","e32b0ddd":"code","ac223627":"code","7ef95040":"code","f4d9f4c1":"code","db8ca985":"code","50feeb6a":"code","e43bea3e":"code","c6b63938":"code","eba7639d":"code","c1c24ef4":"code","3cbca517":"code","380e246d":"code","92549d98":"markdown","b65f4436":"markdown","b58c3936":"markdown","44889f6f":"markdown","05b30292":"markdown","e623a86c":"markdown","450c018f":"markdown","cde33bde":"markdown","d320ba71":"markdown","e2eee495":"markdown","8fa201b0":"markdown","a60e1957":"markdown","d0f4db98":"markdown","d7d7d7e4":"markdown","586f1471":"markdown","079b6f84":"markdown","f3c9ae9c":"markdown"},"source":{"97d089f8":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","8c31937f":"train = pd.read_csv('data\/train.csv')\ndata = pd.read_csv('data\/test.csv')","0f8acc36":"train.head(10)","f22a0d5b":"train.info()","0598ba72":"train.describe()","5aede0c0":"train.corr()['Survived'].sort_values()","e4678193":"train = train.drop(['PassengerId', 'Name', 'Cabin', 'Ticket'], axis=1)","9bdbee6f":"train.corr()['Survived'].sort_values()","974e3f57":"train['Sex'].replace(['male', 'female'], [0, 1], inplace=True)","524772cc":"train['Embarked'].replace(['S', 'C', 'Q'], [0, 1, 2], inplace=True)","15696529":"# check correlation between features\nax = sns.heatmap(train.corr())\nprint(train.corr())\nprint(ax)","0d385cbf":"# Normalize data\nnormal_train = (train - train.min()) \/ (train.max() - train.min())","96ebe7c2":"from sklearn.impute import KNNImputer\n\n# Handle missing values in \"Embarked\": fill in with median\ncol = normal_train.columns\nimputer = KNNImputer(missing_values=np.nan, n_neighbors=2)\nimputer.fit(normal_train)\nfilled_train = imputer.transform(normal_train)\nfilled_train = pd.DataFrame(data=filled_train, columns=col)\nfilled_train.info()","de06357d":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.gaussian_process import GaussianProcessClassifier\nfrom sklearn.gaussian_process.kernels import RBF\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, StackingClassifier\nfrom sklearn.experimental import enable_halving_search_cv\nfrom sklearn.model_selection import HalvingGridSearchCV","b827f206":"results = []\n    \n# Separate feature and target\nX = filled_train.drop('Survived', axis=1)\ny = filled_train['Survived']\nx_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=100)","fb11f190":"lr = LogisticRegression()\nlr.fit(x_train, y_train)\nscore = ['Logistic Regression', lr.score(x_train, y_train), lr.score(x_test, y_test), abs(lr.score(x_test, y_test) - lr.score(x_train, y_train))]\nresults.append(score)\nprint(pd.DataFrame(data = np.array([score]), columns = ['Model', 'Train Set Score', 'Test Set Score', 'Generalization Error']))","e2dd05f9":"# Parameter tuning\nknn_param = {'n_neighbors': range(1,11),\n              'weights': ['uniform', 'distance'],\n              'algorithm': ['auto', 'ball_tree', 'kd_tree'],\n              'leaf_size': [5, 10, 15, 20, 25]\n            }\nsh = HalvingGridSearchCV(KNeighborsClassifier(), knn_param, factor=3, random_state=100).fit(x_train, y_train)\nknn = sh.best_estimator_\nknn.fit(x_train, y_train)\nscore = ['KNN', knn.score(x_train, y_train), knn.score(x_test, y_test), abs(knn.score(x_test, y_test) - knn.score(x_train, y_train))]\nresults.append(score)\nprint('The best KNN estimator is:', knn, '\\n')\nprint(pd.DataFrame(data = np.array([score]), columns = ['Model', 'Train Set Score', 'Test Set Score', 'Generalization Error']))","9d07ce5f":"# Parameter tuning\nsvc_param = {'C': [5, 10, 15, 20, 30],\n              'kernel': ['rbf'],\n              'shrinking': [True, False],\n              'gamma': [0.5, 0.7, 0.8, 1],\n              'probability': [True, False]\n            }\nsh = HalvingGridSearchCV(SVC(), svc_param, factor=3, random_state=100).fit(x_train, y_train)\nsvc = sh.best_estimator_\nsvc.fit(x_train, y_train)\nscore = ['SVM', svc.score(x_train, y_train), svc.score(x_test, y_test), abs(\n    svc.score(x_test, y_test) - svc.score(x_train, y_train))]\nresults.append(score)\nprint('The best SVM estimator is:', svc, '\\n')\nprint(pd.DataFrame(data=np.array([score]), columns=['Model', 'Train Set Score', 'Test Set Score', 'Generalization Error']))","2e625138":"# Parameter tuning\ndectree_param = {'max_depth': [5, 7, 10, 12],\n                 'min_samples_split': [3, 5, 10, 15],\n                 'min_weight_fraction_leaf': [0, 1\/len(train), 2\/len(train), 4\/len(train)],\n                 'max_features': ['auto', 'sqrt', 'log2']\n                }\nsh = HalvingGridSearchCV(DecisionTreeClassifier(), dectree_param, factor=3, random_state=100).fit(X, y)\ndectree = sh.best_estimator_\ndectree.fit(x_train, y_train)\nscore = ['Decision Tree', dectree.score(x_train, y_train), dectree.score(x_test, y_test), abs(\n    dectree.score(x_test, y_test) - dectree.score(x_train, y_train))]\nresults.append(score)\nprint('The best Decision Tree estimator is:', dectree, '\\n')\nprint(pd.DataFrame(data=np.array([score]), columns=[\n      'Model', 'Train Set Score', 'Test Set Score', 'Generalization Error']))","ec503cc3":"# Parameter tuning\nranfor_param = {'max_depth': [8, 10, 12],\n                'min_samples_split': [5, 10, 50],\n                'max_features': ['auto', 'sqrt', 'log2'],\n                'warm_start': [True, False]\n               }\nsh = HalvingGridSearchCV(RandomForestClassifier(warm_start=True), ranfor_param, factor=3, random_state=100).fit(x_train, y_train)\nranfor = sh.best_estimator_\nranfor.fit(x_train, y_train)\nscore = ['Random Forest', ranfor.score(x_train, y_train), ranfor.score(x_test, y_test), abs(\n    ranfor.score(x_test, y_test) - ranfor.score(x_train, y_train))]\nresults.append(score)\nprint('The best Random Forest estimator is:', ranfor, '\\n')\nprint(pd.DataFrame(data=np.array([score]), columns=['Model', 'Train Set Score', 'Test Set Score', 'Generalization Error']))","81fd347d":"# Parameter tuning\nadaboost_param = {'base_estimator': [dectree, svc],\n                  'learning_rate': [1.0, 1.5, 2.0, 2.5]\n                 }\nsh = HalvingGridSearchCV(AdaBoostClassifier(), adaboost_param, factor=3, random_state=100).fit(x_train, y_train)\nadaboost = sh.best_estimator_\nadaboost.fit(x_train, y_train)\nscore = ['AdaBoost', adaboost.score(x_train, y_train), adaboost.score(x_test, y_test), abs(adaboost.score(x_test, y_test) - adaboost.score(x_train, y_train))]\nresults.append(score)\nprint('The best AdaBoost estimator is:', adaboost, '\\n')\nprint(pd.DataFrame(data=np.array([score]), columns=[\n      'Model', 'Train Set Score', 'Test Set Score', 'Generalization Error']))","e32b0ddd":"# Parameter tuning\nmodels = [\n    ('lr', lr), \n    ('knn', knn), \n    ('svm', svc), \n    ('dec', dectree), \n    ('ran', ranfor), \n    ('ada', adaboost)\n    ]\nstack = StackingClassifier(estimators = models, passthrough=True)\nstack.fit(x_train, y_train)\nscore = ['Stacking 1', stack.score(x_train, y_train), stack.score(x_test, y_test), abs(\n    stack.score(x_test, y_test) - stack.score(x_train, y_train))]\nresults.append(score)\nprint(pd.DataFrame(data=np.array([score]), columns=['Model', 'Train Set Score', 'Test Set Score', 'Generalization Error']))","ac223627":"# Parameter tuning\nmodels = [\n    ('knn', knn),\n    ('svm', svc),\n    ('dec', dectree),\n    ('ran', ranfor),\n    ]\nstack = StackingClassifier(estimators=models, passthrough=True)\nstack.fit(x_train, y_train)\nscore = ['Stacking 2', stack.score(x_train, y_train), stack.score(x_test, y_test), abs(stack.score(x_test, y_test) - stack.score(x_train, y_train))]\nresults.append(score)\nprint(pd.DataFrame(data=np.array([score]), columns=[\n      'Model', 'Train Set Score', 'Test Set Score', 'Generalization Error']))","7ef95040":"result_table = pd.DataFrame(data=results, columns=['Model', 'Train Set Score', 'Test Set Score', 'Generalization Error'])\nresult_table.iloc[:,1:] *= 100\nresult_table","f4d9f4c1":"x = np.arange(len(result_table['Model']))  # the label locations\nwidth = 0.2  # the width of the bars\n\nfig, ax = plt.subplots()\nrects1 = ax.bar(x - width\/2, result_table['Train Set Score'], width, label='Train Set Score')\nrects2 = ax.bar(x + width\/2, result_table['Test Set Score'], width, label='Test Set Score')\nax.set_ylabel('Score(%)')\nax.set_title('Accuracy of Predictions Across Models')\nax.set_ylim((60, 100))\nax.set_xticks(x, result_table['Model'])\nax.legend(loc='best')\n\nfig.autofmt_xdate()\n\nplt.show()","db8ca985":"import torch\nfrom torch import nn\nfrom torch import optim","50feeb6a":"# A simple model with 2 hidden layers\ninput_size = 7\nhidden_sizes = [1024, 1024]\noutput_size = 2\n\nmodel = nn.Sequential(nn.Linear(input_size, hidden_sizes[0]),\n                      nn.ReLU(),\n                      nn.Dropout(p=0.2),\n                      nn.Linear(hidden_sizes[0], hidden_sizes[1]),\n                      nn.ReLU(),\n                      nn.Dropout(p=0.2),                      \n                      nn.Linear(hidden_sizes[1], output_size),\n                      nn.Softmax(dim=1))\n\nprint(model)","e43bea3e":"criterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.SGD(model.parameters(), lr=0.01)\nepoch_no = 200\nbatch_size = 64\nbatch_no = x_train.shape[0] \/\/ batch_size\ntrain_loss = 0.0\n\nfor e in range(epoch_no):\n\n    # Mini-batch\n    for i in range(batch_no):\n        start = i * batch_size\n        end = start + batch_size\n        x_var = torch.FloatTensor(x_train[start:end].to_numpy())\n        y_var = torch.LongTensor(y_train[start:end].to_numpy())\n        \n        # Forward + Backward + Optimize\n        optimizer.zero_grad()\n        ypred_var = model(x_var)\n        loss = criterion(ypred_var, y_var)\n        loss.backward()\n        optimizer.step()\n        \n        train_loss += loss.item() * batch_size\n    \n    train_loss = train_loss \/ len(x_train)\n    if (e % 20 == 0):\n        print(f'Training loss at epoch {e}: {train_loss}')\n        \nprint(f'Training loss at epoch {epoch_no}: {train_loss}')           \n           \nwith torch.no_grad():\n    test_result = model(torch.FloatTensor(x_test.to_numpy()))\n    values, labels = torch.max(test_result, 1)\n    loss = criterion(test_result, torch.LongTensor(y_test.to_numpy()))\n    accuracy = np.sum(labels.data.numpy() == y_test) \/ len(y_test)\nprint(f'Validation loss: {loss.item()}')    \nprint(f'Accuracy: {accuracy}')    ","c6b63938":"data.info()","eba7639d":"# Use random forest\n# Repeat data manipulation procedures\nindex = pd.DataFrame(data['PassengerId'])\ndata = data.drop(['PassengerId', 'Name', 'Cabin', 'Ticket'], axis=1)\ndata['Sex'].replace(['male', 'female'], [0, 1], inplace=True)\ndata['Embarked'].replace(['S', 'C', 'Q'], [0, 1, 2], inplace=True)\nnormal_data = (data - data.min()) \/ (data.max() - data.min())\ncol = normal_data.columns\nimputer = KNNImputer(missing_values=np.nan, n_neighbors=2)\nimputer.fit(normal_data)\nfilled_data = imputer.transform(normal_data)\nfilled_data = pd.DataFrame(data=filled_data, columns=col)\nfilled_data.info()","c1c24ef4":"prediction = pd.DataFrame(ranfor.predict(filled_data).astype(int))\nprediction.columns = ['Survived']\nindex.columns = ['PassengerId']\nprint(index)\nprint(prediction)","3cbca517":"prediction = index.join(pd.DataFrame(prediction))\nprint(prediction)","380e246d":"prediction.to_csv(\"submission.csv\", index=False)","92549d98":"## Prediction & Submission","b65f4436":"Here's a summary of the models' performance.","b58c3936":"## Exploratory Data Analysis","44889f6f":"### Decision Tree","05b30292":"This notebook explores the performance of various models (parameteres are tuned).\nIt also has a simple design of a dense NN.\n\nSince Kaggle has compatibility problems with `sklearn.experimental`, please [click here](https:\/\/github.com\/fengyuli2002\/kaggle_with_yolanda\/blob\/main\/titanic\/titanic.ipynb) to view the notebook's output on GitHub.","e623a86c":"### AdaBoost","450c018f":"# Titanic - Machine Learning from Disaster","cde33bde":"### KNN","d320ba71":"### Support Vector Machine","e2eee495":"As shown in the figure, random forest seems to be the best model.","8fa201b0":"### Baseline: Logistic Regression","a60e1957":"### Extra Fun: Dense Neural Network","d0f4db98":"## Data Cleaning & Feature Engineering","d7d7d7e4":"### Stacking Classifier","586f1471":"### Random Forest","079b6f84":"## Modeling","f3c9ae9c":"It appears that the dataset is not large enough for a neural network to perform well."}}