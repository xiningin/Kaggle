{"cell_type":{"4d3203a5":"code","c606a8d9":"code","2f777593":"code","2c0b660a":"code","d324f688":"code","d86aba4a":"code","008a3982":"code","8c5d0c1a":"code","fd154f40":"code","3a002072":"code","18cbaf39":"code","327226ab":"code","b59a830f":"code","9e570826":"code","9a06c654":"code","e7042aa4":"code","4ad98d21":"code","3a791dba":"code","60ad8112":"code","6cb81197":"code","9cef0e46":"code","0f2c5433":"code","6436a9ae":"markdown","b9c28472":"markdown","82c7d26b":"markdown","d7e79598":"markdown","55ec90a9":"markdown","17f12bfe":"markdown","c178891b":"markdown","ba1f2f18":"markdown","2e9930ce":"markdown","15e51519":"markdown","f3426644":"markdown","7366032f":"markdown","c7d729f6":"markdown","09a22e5c":"markdown","bbae1406":"markdown","c717d049":"markdown","d3ae3087":"markdown","0256ff35":"markdown"},"source":{"4d3203a5":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","c606a8d9":"train_df = pd.read_csv('..\/input\/titanic\/train.csv')","2f777593":"train_df.info()","2c0b660a":"train_df.loc[0]","d324f688":"#Drop Name and Ticket\n\ntrain_df.drop(['Name', 'Ticket', 'PassengerId', 'Cabin'], axis=1, inplace=True)\ntrain_df.info()","d86aba4a":"X = train_df.drop('Survived', axis=1, inplace=False)\ny = train_df['Survived']\n\nprint(f'Input features shape: {X.shape}')\nprint(f'Labels shape: {y.shape}')","008a3982":"#Import visualization libraries\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns","8c5d0c1a":"sns.barplot(x = y.unique(), y = y.value_counts())\nplt.ylabel('Number of Passengers')","fd154f40":"frequencies = X.count()\n\nfreq_df = pd.DataFrame([frequencies], columns=X.columns)\nprint(freq_df.head())\n\nplt.figure(figsize=(10, 6))\nsns.barplot(x=X.columns, y = frequencies)\nplt.ylabel('Frequency')\nplt.xlabel('Feature')\nplt.show()\n\n","3a002072":"from sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\n\n'''\nThe following pipeline is only for numerical features:\n1. Fill in Missing Values with Median value of existing numbers\n2. Scale all the numbers to be in between 0 and 1\n'''\n\nnum_pipeline = Pipeline([\n    ('imputer', SimpleImputer(strategy=\"median\")), \n    ('std_scaler', StandardScaler())\n])\n\n'''\nThe following pipeline is only for categorical features:\n1. Fill in Missing Values with Most Frequent value from existing data\n2. One-hot encode (https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.preprocessing.OneHotEncoder.html)\n'''\n\ncat_pipeline = Pipeline([\n    ('cat_imputer', SimpleImputer(strategy='most_frequent')),\n    ('one_hot', OneHotEncoder())\n])\n","18cbaf39":"#Combine Pipelines\n\nfrom sklearn.compose import ColumnTransformer\n\nnum_attribs = ['Age', 'SibSp', 'Parch', 'Fare']\ncat_attribs = ['Pclass', 'Sex', 'Embarked']\n\nfull_pipeline = ColumnTransformer([(\"num\", num_pipeline, num_attribs), \n                                   (\"cat\", cat_pipeline, cat_attribs)])\nX = full_pipeline.fit_transform(X)\nX.shape","327226ab":"from sklearn.model_selection import train_test_split\n\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)","b59a830f":"#Import necessary libraries\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras import layers as L\n","9e570826":"#Step 1\nmodel = Sequential(name='titanic_model')\n\n#Step 2\nmodel.add(L.InputLayer(input_shape=(12,))) # necessary to use model.summary()\n\n#Step 3\nmodel.add(L.Dense(512, activation='relu'))\nmodel.add(L.Dense(1024, activation='relu'))\nmodel.add(L.Dropout(0.4)) #prevents overfitting by setting 40% of nuerons to 0\nmodel.add(L.Dense(512, activation='relu'))\nmodel.add(L.Dropout(0.4))\nmodel.add(L.Dense(128, activation='relu'))\nmodel.add(L.Dense(64, activation='relu'))\nmodel.add(L.Dense(64, activation='relu'))\nmodel.add(L.Dense(1, activation='sigmoid')) # output layer, use sigmoid for binary\n\nmodel.summary()","9a06c654":"#Step 4\nmodel.compile(loss='binary_crossentropy', optimizer=keras.optimizers.Adam(0.0001), metrics=['accuracy'])","e7042aa4":"'''\nThis custom callback stops training once the validation accuracy reaches 83%.\nThere are several callbacks already implemented by keras.\nhttps:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/callbacks\n\n'''\n\nclass myCallback(tf.keras.callbacks.Callback):\n    def on_epoch_end(self, epoch, logs={}):\n        if(logs.get('val_accuracy')>0.83):\n            print(\"\\nReached 83% accuracy so cancelling training!\")\n            self.model.stop_training = True\n\n\n#Step 6\nhistory = model.fit(X_train, y_train, \n                    validation_data=(X_val, y_val), \n                    batch_size=8, \n                    callbacks=[myCallback()],\n                    epochs=50)","4ad98d21":"plt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title(\"Model Loss\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Loss\")\nplt.legend(['train', 'val'], loc='upper right')\nplt.show()\n\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title(\"Model Accuracy\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Accuracy\")\nplt.legend(['train', 'val'], loc='upper right')\nplt.show()","3a791dba":"test_df = pd.read_csv('..\/input\/titanic\/test.csv')\ntest_df.drop(['PassengerId', 'Name', 'Ticket', 'Cabin'], axis=1, inplace=True)\ntest_df","60ad8112":"test_prepared = full_pipeline.transform(test_df)\ntest_prepared.shape","6cb81197":"preds = model.predict(test_prepared)\npreds[:5]","9cef0e46":"submission = pd.read_csv('..\/input\/titanic\/gender_submission.csv')\nsubmission['Survived'] = [0 if pred < 0.5 else 1 for pred in preds]\nsubmission.head()","0f2c5433":"from IPython.display import FileLink\n\n\nsubmission.to_csv('submission.csv',index=False)\nFileLink(r'submission.csv')","6436a9ae":"From the plot above, hopefully it is clear that the train loss goes down consistently while the validation loss starts increasing after the first few epochs. This is a good sign of overfitting. ","b9c28472":"First, we notice that the ```Survived``` category will not have any consequence if we drop the ```Name``` category. The ```Ticket``` class is also not going to be very useful. Keep in mind that the ```Ticket``` class would be categorical if we were to include it since not all of them are pure numbers. We will also drop ```PassengerId``` as it does not provide any useful information. For the purpose of this tutorial, we will not be using ```Cabin``` due to the large number of missing values.","82c7d26b":"# Prepare the Test Data\n\nAgain we have to drop the data as we did in the train data. Then we also have to transform it using the same pipeline as before. After that, we say the person survived if the prediction was greater then 0.5. Finally, we can make our submission.","d7e79598":"Before moving on, lets split ```train_df``` into ```X``` and ```y``` to prepare the data for training. ","55ec90a9":"## Data Analysis","17f12bfe":"Using ```.info``` is a quick way to investigate the data on a surface level. Now we can see each column name and how many values they have. It is clear that three columns, ```Age``` ```Cabin``` ```Embarked```, have some number of values missing. ","c178891b":"## Overview\n* How to Prepare Data for a Kaggle Competition\n* Using sklearn preprocessing features with pipelines\n* Intro to Using Tensorflow\n* Writing a Custom Callback\n* Making a Submission!","ba1f2f18":"# Reading the Data","2e9930ce":"Creating neural networks is pretty simple in tensorflow. Here are the steps:\n\n1. Define Sequential model ```model = Sequential()```\n2. Add Input Layer ```model.add(L.InputLayer(input_shape=...))``` --> Optional!\n3. Add fully connected layers ```model.add(L.Dense(...))```\n4. Compile Model\n5. Callbacks (if necessary)\n6. Train Model\n\nOf course, more complex models can also be created, but we can conquer this dataset by using basic tensorflow API. ","15e51519":"From the above figure, it is clear that the ```Age``` category has a significant number of missing values. However, from the values listed we ```Embarked``` is missing 2 values in addition.\n\nFortunatly, Sci-kit Learn provides a nice and easy method to impute these missing values with the simple imputer. ","f3426644":"Now that we have finished preparing the data, we are ready to split it into a train and validation set using ```train_test_split``` from sklearn.","7366032f":"Sometimes it is useful to look at one full row to see which columns will be most beneficial while training our data.","c7d729f6":"## Imputing and One-Hot Encoding","09a22e5c":"Fortunately, the imbalance isn't too bad for this dataset.\n\nNow, let's see which features have missing values. ","bbae1406":"Lets quickly look at the target variable to see evidence of **data imbalance**. ","c717d049":"# Neural Network\n\nNow that everything is ready, let's start building the model! We will see how we can use Tensorflow to create a simple neural network to solve this task. For now, we will use 2 hidden layers followed by the output layer with one nueron (binary problem). \n\nIf you would like to get an in-depth understanding of how neural networks learn through back-propogation, please visit this tutorial on [Neural Networks from Scratch](https:\/\/www.kaggle.com\/saumandas\/neural-networks-from-scratch-tutorial). You will learn how to create a neural network from scratch to model the California Housing Dataset. ","d3ae3087":"**Please Upvote if you found this helpful!**\n**Thanks**","0256ff35":"Imputing is the process of dealing with missing values. The most basic way to overcome this challenge would be to remove all the instances\/rows which contain missing values. However, if we do that, then we would only have 714 training instances which is much less than 891. Therefore, we use the ```SimpleImputer``` API. "}}