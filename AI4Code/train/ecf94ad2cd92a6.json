{"cell_type":{"846d5603":"code","c74af99a":"code","6cf55ad7":"code","37d444e6":"code","a6fd774a":"code","a658d27a":"code","2c0d6b2e":"code","5fa0fac0":"code","be22baa4":"code","68900e3e":"code","10f6ef0a":"code","6379cc1b":"code","1ae68b8b":"code","eb87ebfa":"code","e8507742":"code","2b80d94c":"code","69a400cd":"code","115cc55b":"code","d4b2ea1b":"code","6bc3432f":"code","b55984fc":"code","4d43d5fe":"code","fdc5c28f":"code","c06069d0":"code","60a78e60":"code","8acb8766":"code","498a9aa1":"code","8041a83f":"code","a86b50e4":"code","a8f4f216":"code","daaf62d2":"code","a53d1950":"code","df726431":"code","be2a63a3":"code","8ff3d224":"code","f014d077":"code","9b6e850a":"code","73933fad":"code","8d10d915":"code","e0a0edd0":"code","8beea9b7":"code","b40f03f3":"code","1fdd8487":"code","a908995a":"markdown","ebdd23c1":"markdown","66b19f44":"markdown","3d228991":"markdown","56dcebe3":"markdown","fcce38ff":"markdown","dfa5bf67":"markdown","da6ff4bd":"markdown","d0276622":"markdown","275cba4a":"markdown","5c97be80":"markdown","59374359":"markdown","3c1c9383":"markdown","95652f16":"markdown","757c0351":"markdown","cad276ae":"markdown","55827420":"markdown","f77a96aa":"markdown","c7998c17":"markdown","5f26fa9b":"markdown","113f1fc2":"markdown","67b95d2f":"markdown","21ebbadc":"markdown","a87e9f65":"markdown","a8c78c35":"markdown","32065800":"markdown","653dcc22":"markdown"},"source":{"846d5603":"import os\nimport pandas as pd\nimport numpy as np\nfrom tqdm.notebook import tqdm\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\n\nfrom collections import defaultdict\n\n\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms as tr","c74af99a":"# \u0417\u0430\u0433\u0440\u0443\u0436\u0430\u0435\u043c \u0434\u0430\u043d\u043d\u044b\u0435 \u0441 \u043f\u043e\u043c\u043e\u0448\u044c\u044e \u0431\u0438\u0431\u043b\u0438\u043e\u0442\u0435\u043a\u0438 pandas\ncaption_file_path = '..\/input\/flickr8k\/captions.txt'\ndf = pd.read_csv(caption_file_path)\ndf.head()","6cf55ad7":"# \u0441\u043e\u0445\u0440\u0430\u043d\u044f\u0435\u043c \u0432 dict, \u0442\u043e-\u0435\u0441\u0442\u044c data[i] = List[\u043d\u0430\u0434\u043f\u0438\u0441\u044c1, \u043d\u0430\u0434\u043f\u0438\u0441\u044c2, ..., \u043d\u0430\u0434\u043f\u0438\u0441\u044c5]\ndata = defaultdict(list)\nfor index, row in df.iterrows():\n    data[row['image']].append(row['caption'])","37d444e6":"test_size=0.2 # \u0431\u0435\u0440\u0451\u043c 20 \u043f\u0440\u043e\u0446\u0435\u043d\u0442\u043e\u0432 \u0434\u0430\u043d\u043d\u044b\u0445 \u0432 \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u0435 \u0442\u0435\u0441\u0442\u043e\u0432\u043e\u0433\u043e \u043c\u043d\u043e\u0436\u0435\u0441\u0442\u0432\u0430\nkeys = np.random.RandomState(seed=42).permutation(list(data.keys()))  # \u0434\u0435\u043b\u0430\u0435\u043c \u043f\u0435\u0440\u0435\u0441\u0442\u0430\u043d\u043e\u0432\u043a\u0443 \u043a\u043b\u044e\u0447\u0435\u0439 \u0432 \u043f\u0441\u0435\u0432\u0434\u043e-\u0441\u043b\u0443\u0447\u0430\u0439\u043d\u043e\u043c \u043f\u043e\u0440\u044f\u0434\u043a\u0435 (\u0442\u0430\u043a \u043a\u0430\u043a seed == 42) \ntest_keys, train_keys = keys[: int(test_size*len(keys))], keys[int(test_size*len(keys)): ]\ntrain_data = {k: v for k, v in data.items() if k in train_keys}\ntest_data = {k: v for k, v in data.items() if k in test_keys}","a6fd774a":"inds = list(range(10))\ndata_folder = '..\/input\/flickr8k\/Images'\n\nh, w = 2, 5\ntitle_width = 43\n\nassert h*w >= len(inds)\n\nfig, ax = plt.subplots(h, w, figsize=(20, 15))\n\nfor i, ind in enumerate(inds):\n    img_id = list(train_data.keys())[i]\n    captions = train_data[img_id]\n    \n    caption_adjasted = list(map(lambda el: '\\n'.join([(str(el[0]) + ': ' + el[1])[k:k+title_width] for k in range(0, 3 + len(el[1]), title_width)]), enumerate(captions)))\n    caption = '\\n'.join(caption_adjasted)\n    plt.subplot(h, w, i+1)\n    plt.title(caption)\n    plt.imshow(mpimg.imread(os.path.join(data_folder, img_id)))\n\nfig.tight_layout()\nplt.show()","a658d27a":"channel_mean = [0, 0, 0]\nchannel_std = [0, 0, 0]\nn = len(train_data)\nfor img_id in tqdm(train_data):\n    im = mpimg.imread(os.path.join(data_folder, img_id)) \/ 255\n    for c in range(3):\n        channel_mean[c] = channel_mean[c] + im[:,:,c].mean()\n        channel_std[c] = channel_std[c] + im[:,:,c].std()\n\nchannel_mean = np.array(channel_mean)\/n\nchannel_std = np.array(channel_std)\/n","2c0d6b2e":"print(f\"Channel mean: {channel_mean}\")\nprint(f\"Channel std: {channel_std}\")","5fa0fac0":"# \u0421\u043e\u0437\u0434\u0430\u0451\u043c \u0444\u0443\u043d\u043a\u0446\u0438\u044e \u0434\u043b\u044f \u043f\u0440\u0435\u043e\u0431\u0440\u0430\u0437\u043e\u0432\u0430\u043d\u0438\u044f \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u044f, \u0432\u0441\u0435 \u043d\u0435\u043e\u0431\u0445\u043e\u0434\u0438\u043c\u044b\u0435 \u0430\u0443\u0433\u043c\u0435\u043d\u0442\u0430\u0446\u0438\u0438 \u043d\u0443\u0436\u043d\u043e \u0432\u043a\u043b\u044e\u0447\u0438\u0442\u044c \u0437\u0434\u0435\u0441\u044c\nimage_prepare = tr.Compose([\n    tr.ToPILImage(),\n    tr.Resize((224, 224)),\n    tr.RandomHorizontalFlip(p=0.5),\n    tr.RandomApply([tr.RandomAffine(25, (0.05, 0.05), (0.7, 1.5))], p=0.5),\n    tr.ToTensor(),\n    tr.Normalize(channel_mean, channel_std)\n])\n\n\n# \u0410\u043d\u0430\u043b\u043e\u0433\u0438\u0447\u043d\u043e \u0434\u043b\u044f \u0432\u0430\u043b\u0438\u0434\u0430\u0446\u0438\u043e\u043d\u043d\u044b\u0445 \u0434\u0430\u043d\u043d\u044b\u0445\nimage_prepare_val = tr.Compose([\n    tr.ToPILImage(),\n    tr.Resize((224, 224)),\n    tr.ToTensor(),\n    tr.Normalize(channel_mean, channel_std)\n])","be22baa4":"from sklearn.preprocessing import minmax_scale\n\nimage = mpimg.imread(os.path.join(data_folder, list(train_data.keys())[0]))\n\nfig, ax = plt.subplots(2, 4, figsize=(20, 8))\nfor i in range(8):\n    plt.subplot(2, 4, i+1)\n    transformed_image = image_prepare(image).numpy().transpose(1, 2, 0) # \u0442\u0430\u043a \u043a\u0430\u043a \u043d\u0435\u043e\u0431\u0445\u043e\u0434\u0438\u043c\u043e \u043f\u043e\u043c\u0435\u043d\u044f\u0442\u044c \u043f\u043e\u0440\u044f\u0434\u043e\u043a [c, h, w] \u043d\u0430 [h, w, c]\n    plt.imshow(minmax_scale(transformed_image.reshape(3, -1), axis=1).reshape((224, 224, 3)))\nplt.show()","68900e3e":"import re # \u043f\u0430\u043a\u0435\u0442 \u0434\u043b\u044f \u0440\u0430\u0431\u043e\u0442\u044b \u0441 \u0440\u0435\u0433\u0443\u043b\u044f\u0440\u043d\u044b\u043c\u0438 \u0432\u044b\u0440\u0430\u0436\u0435\u043d\u0438\u044f\u043c\u0438\n\ndef tokenize(text):\n    \"\"\"Return list of strings.\n    :param text: string to be tokenized\n    \n    :return: list of tokens, each one is string\n    \"\"\"\n    text = text.lower()\n    text = re.sub(r\"[,.;@#?!&$]+\\ *\", \" \", text)\n    text = text.rstrip()\n    tokens = [elem for elem in text.split() if len(elem) > 0]\n    return ['<BOS>'] + tokens + ['<EOS>']","10f6ef0a":"sentence = \"Please, Do Not tokenize me!\"\ntokens = tokenize(sentence)\nprint(f\"Original: {sentence}\")\nprint(f\"Tokenized: {tokens}\")","6379cc1b":"from collections import Counter\nfrom tqdm.notebook import trange","1ae68b8b":"# \u041f\u043e\u0441\u0447\u0438\u0442\u0430\u0435\u043c \u0447\u0430\u0441\u0442\u043e\u0442\u044b \u0432\u0441\u0442\u0440\u0435\u0447\u0430\u0435\u043c\u043e\u0441\u0442\u0438 \u0442\u043e\u043a\u0435\u043d\u043e\u0432 \u0432 \u0442\u0440\u0435\u0439\u043d\u043e\u0432\u043e\u043c \u043c\u043d\u043e\u0436\u0435\u0441\u0442\u0432\u0435 \n# \u0438 \u0447\u0430\u0441\u0442\u043e\u0442\u044b \u0434\u043b\u0438\u043d \u0442\u043e\u043a\u0435\u043d\u0435\u0437\u0438\u0440\u043e\u0432\u0430\u043d\u043d\u044b\u0445 \u043f\u0440\u0435\u0434\u043b\u043e\u0436\u0435\u043d\u0438\u044f\n# vocab_freq['the'] == \u043e\u0431\u0449\u0435\u0435 \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0432\u0442\u043e \u0442\u043e\u043a\u0435\u043d\u043e\u0432 `the` \u0432 \u0442\u0440\u0435\u043d\u0438\u0440\u043e\u0432\u043e\u0447\u043d\u044b\u0445 \u043f\u043e\u0434\u043f\u0438\u0441\u044f\u043a \u043a \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u044f\u043c\n# sizes[5] == \u043e\u0431\u0437\u0435\u0435 \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0432\u043e \u043f\u043e\u0434\u043f\u0438\u0441\u0435\u0439, \u0434\u043b\u0438\u043d\u0430 \u0442\u043e\u043a\u0435\u043d\u0435\u0437\u0438\u0440\u043e\u0432\u0430\u043d\u043d\u043e\u0439 \u043f\u043e\u0441\u043b\u0435\u0434\u043e\u0432\u0430\u0442\u0435\u043b\u044c\u043d\u043e\u0441\u0442\u0438 \u043a\u043e\u0442\u043e\u0440\u044b\u0445 \u0440\u0430\u0432\u043d\u0430 5\nvocab_freq = Counter()\nsizes = Counter()\n\nfor img_id in train_data:\n    captions = train_data[img_id]\n    for cap in captions:\n        tokens = tokenize(cap)\n        vocab_freq.update(tokens)\n        sizes.update({len(tokens): 1})","eb87ebfa":"# \u041d\u0430\u043c \u043d\u0435 \u043d\u0443\u0436\u043d\u044b \u0442\u043e\u043a\u0435\u043d\u044b, \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u0432\u0441\u0442\u0440\u0435\u0447\u0430\u044e\u0442\u0441\u044f \u0440\u0435\u0434\u043a\u043e, \u043d\u0430\u043f\u0440\u0438\u043c\u0435\u0440 \u043c\u0435\u043d\u0435\u0435 5 \u0440\u0430\u0437,\n# \u0432\u0441\u0435 \u0442\u0430\u043a\u0438\u0435 \u0442\u043e\u043a\u0435\u043d\u044b \u0432 \u043d\u0430\u0448\u0435\u043c \u0441\u043b\u043e\u0432\u0430\u0440\u0435 \u0437\u0430\u043c\u0435\u043d\u0438\u043c \u043d\u0430 \u0441\u043f\u0435\u0446\u0438\u0430\u043b\u044c\u043d\u044b\u0439 \u0442\u043e\u043a\u0435\u043d <UNK> (Unknown)\n\nvocab_freq_new = Counter()\nfor word in vocab_freq:\n    if vocab_freq[word] <= 5:\n        vocab_freq_new.update({'<UNK>': vocab_freq[word]})\n    else:\n        vocab_freq_new.update({word: vocab_freq[word]})\n\nvocab_freq = vocab_freq_new\nmax_seq_len = np.max(list(sizes.keys()))\n","e8507742":"# \u041e\u0442\u0440\u0438\u0441\u0443\u0435\u043c 20 \u043d\u0430\u0438\u0431\u043e\u043b\u0435\u0435 \u0438 \u043d\u0430\u0438\u043c\u0435\u043d\u0435\u0435 \u0447\u0430\u0441\u0442\u043e\u0442\u043d\u044b\u0445 \u0441\u043b\u043e\u0432 (\u0442\u043e\u043a\u0435\u043d\u043e\u0432)\n# \u0430 \u0442\u0430\u043a \u0436\u0435 \u0440\u0430\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u0435 \u043f\u043e \u0447\u0438\u0441\u043b\u043e \u0442\u043e\u043a\u0435\u043d\u043e\u0432 \u0432 \u043f\u043e\u0434\u043f\u0438\u0441\u0438\nshow_ = 20\nfig, ax = plt.subplots(3, 1, figsize=(20, 12))\n\nplt.subplot(312)\nvocab_freq = {k: v for k, v in sorted(vocab_freq.items(), key=lambda item: item[1])}\nplt.title('least popular words')\nplt.bar(list(vocab_freq.keys())[:show_], list(vocab_freq.values())[:show_])\n\nplt.subplot(311)\nvocab_freq = {k: v for k, v in sorted(vocab_freq.items(), key=lambda item: item[1], reverse=True)}\nplt.title('most popular words')\nplt.bar(list(vocab_freq.keys())[:show_], list(vocab_freq.values())[:show_])\n\nplt.subplot(313)\nplt.title('sequence sizes')\nplt.bar(list(sizes.keys()), list(sizes.values()))\n\nfig.tight_layout()\nplt.show()","2b80d94c":"# \u041d\u0430\u043f\u0438\u0448\u0435\u043c \u0441\u043b\u043e\u0432\u0430\u0440\u0438, \u0434\u043b\u044f \u043c\u0430\u043f\u043f\u0438\u043d\u0433\u0430 \u0442\u043e\u043a\u0435\u043d\u0430 \u0432 \u0435\u0433\u043e \u0438\u043d\u0434\u0435\u043a\u0441 \u0438 \u043e\u0431\u0440\u0430\u0442\u043d\u043e\n# <UNK> - unknown - \u043e\u0437\u043d\u0430\u0447\u0430\u0435\u0442 \u043d\u0435\u0438\u0437\u0432\u0435\u0441\u0442\u043d\u044b\u0439 \u0442\u043e\u043a\u0435\u043d (\u043c\u044b \u043d\u0435 \u043d\u0430\u0431\u043b\u044e\u0434\u0430\u043b\u0438 \u0442\u0430\u043a\u043e\u0435 \u0441\u043b\u043e\u0432\u043e \u0432 \u043d\u0430\u0448\u0438\u0445 \u043f\u043e\u0434\u043f\u0438\u0441\u044f\u0445)\n# <BOS> - begin of sentence - \u043e\u0437\u043d\u0430\u0447\u0430\u0435\u0442 \u043d\u0430\u0447\u0430\u043b\u043e \u043f\u043e\u0441\u043b\u0435\u0434\u043e\u0432\u0430\u0442\u0435\u043b\u044c\u043d\u043e\u0441\u0442\u0438\n# <EOS> - end of sentence - \u043e\u0437\u043d\u0430\u0447\u0430\u0435\u0442 \u043a\u043e\u043d\u0435\u0446 \u043f\u043e\u0441\u043b\u0435\u0434\u043e\u0432\u0430\u0442\u0435\u043b\u044c\u043d\u043e\u0441\u0442\u0438\n# <PAD> - padding - \u0441\u043f\u0435\u0446\u0438\u0430\u043b\u044c\u043d\u044b\u0439 \u0442\u043e\u043a\u0435\u043d, \u0434\u043b\u044f \u0434\u043e\u0437\u0430\u043f\u043e\u043b\u043d\u0435\u043d\u0438\u044f \u043f\u043e\u0441\u043b\u0435\u0434\u043e\u0432\u0430\u0442\u0435\u043b\u044c\u043d\u043e\u0441\u0442\u0438 \u0434\u043e \u043d\u0435\u043e\u0431\u0445\u043e\u0434\u0438\u043c\u043e\u0433\u043e \u0447\u0438\u0441\u043b\u0430 \u0442\u043e\u043a\u0435\u043d\u043e\u0432\n\ntok_to_ind = {\n    '<UNK>': 0,\n    '<BOS>': 1,\n    '<EOS>': 2,\n    '<PAD>': 3,\n}\n\nind_to_tok = {\n    0: '<UNK>',\n    1: '<BOS>',\n    2: '<EOS>',\n    3: '<PAD>',\n}\n\nstart_ind = 4\nfor t in vocab_freq:\n    if not t in ['<UNK>', '<BOS>', '<EOS>', '<PAD>']:\n        tok_to_ind[t] = start_ind\n        ind_to_tok[start_ind] = t\n    else:\n        continue\n    start_ind += 1\n\nassert len(tok_to_ind) == len(ind_to_tok)\nvocab_size = len(tok_to_ind)\nprint('Vocab size: ', vocab_size)","69a400cd":"# \u0424\u0443\u043d\u043a\u0446\u0438\u044f, \u043a\u043e\u0442\u043e\u0440\u0430\u044f \u0442\u043e\u043a\u0435\u043d\u0435\u0437\u0438\u0440\u0443\u0435\u0442 \u0442\u0435\u043a\u0441\u0442 \u0438 \u0432\u043e\u0437\u0432\u0440\u0430\u0449\u0430\u0435\u0442 \u043f\u043e\u0441\u043b\u0435\u0434\u043e\u0432\u0430\u0442\u0435\u043b\u044c\u043d\u043e\u0441\u0442\u044c \u0438\u043d\u0434\u0435\u043a\u0441\u043e\u0432\ndef to_ids(text):\n    res = []\n    tokens = tokenize(text)\n    for t in tokens:\n        if t in tok_to_ind:\n            res.append(tok_to_ind[t])\n        else:\n            res.append(tok_to_ind['<UNK>'])\n    return res","115cc55b":"text = 'This is just simple example of a nonsense message'\nprint(tokenize(text))\nprint(to_ids(text))","d4b2ea1b":"import torch\n\nclass ImageCaptioningDataset(Dataset):\n    \"\"\"\n    imgs_path ~ \u043f\u0443\u0442\u044c \u043a \u043f\u0430\u043f\u043a\u0435 \u0441 \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u044f\u043c\u0438\n    captions_path ~ \u043f\u0443\u0442\u044c \u043a .tsv \u0444\u0430\u0439\u043b\u0443 \u0441 \u0437\u0430\u0433\u043e\u043b\u043e\u0432\u043a\u0430\u043c\u0438 \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u0439\n    \"\"\"\n    def __init__(self, data, is_train=True):\n        super(ImageCaptioningDataset).__init__()\n        # \u0427\u0438\u0442\u0430\u0435\u043c \u0438 \u0437\u0430\u043f\u0438\u0441\u044b\u0432\u0430\u0435\u043c \u0438\u0437 \u0444\u0430\u0439\u043b\u043e\u0432 \u0432 \u043f\u0430\u043c\u044f\u0442\u044c \u043a\u043b\u0430\u0441\u0441\u0430, \u0447\u0442\u043e\u0431\u044b \u0431\u044b\u0441\u0442\u0440\u043e \u043e\u0431\u0440\u0430\u0449\u0430\u0442\u044c\u0441\u044f \u0432\u043d\u0443\u0442\u0440\u0438 \u0434\u0430\u0442\u0430\u0441\u0435\u0442\u0430\n        # \u0415\u0441\u043b\u0438 \u043d\u0435 \u0445\u0432\u0430\u0442\u0430\u0435\u0442 \u043f\u0430\u043c\u044f\u0442\u0438 \u043d\u0430 \u0445\u0440\u0430\u043d\u0435\u043d\u0438\u0435 \u0432\u0441\u0435\u0445 \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u0439, \u0442\u043e \u043f\u043e\u0434\u0433\u0440\u0443\u0436\u0430\u0439\u0442\u0435 \u043f\u0440\u044f\u043c\u043e \u0432\u043e \u0432\u0440\u0435\u043c\u044f __getitem__, \u043d\u043e \u044d\u0442\u043e \u0437\u0430\u043c\u0435\u0434\u043b\u0438\u0442 \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u0435\n        # \u041f\u0440\u043e\u0432\u0435\u0434\u0438\u0442\u0435 \u0432\u0441\u044e \u043f\u0440\u0435\u0434\u043e\u0431\u0440\u0430\u0431\u043e\u0442\u043a\u0443, \u043a\u043e\u0442\u043e\u0440\u0443\u044e \u043c\u043e\u0436\u043d\u043e \u043f\u0440\u043e\u0432\u0435\u0441\u0442\u0438 \u0437\u0434\u0435\u0441\u044c \u0431\u0435\u0437 \u043f\u043e\u0442\u0435\u0440\u0438 \u0432\u0430\u0440\u0438\u0430\u0442\u0438\u0432\u043d\u043e\u0441\u0442\u0438 \u0434\u0430\u0442\u0430\u0441\u0435\u0442\u0430\n        self.data = data    \n        self.imgs_ids = list(self.data.keys())\n        self.is_train = is_train\n\n        self.in_segs_dict = {}\n        self.out_words_dict = {}\n        \n        for i in trange(len(self)):\n            img_id = self.imgs_ids[i]\n            self.in_segs_dict[i] = []\n            self.out_words_dict[i] = []\n\n            captions = self.data[img_id]\n            assert len(captions) == 5\n            for ind, caption in enumerate(captions):\n                in_seqs = tok_to_ind['<PAD>']*torch.ones(len(tokenize(caption)) - 1, max_seq_len, dtype=torch.int64)\n                out_words = torch.zeros(len(tokenize(caption)) - 1, dtype=torch.int64)\n\n                start = 0\n                indx = to_ids(caption)\n                for j in range(1, len(indx)):\n                    in_seqs[start, :j] = torch.Tensor(indx[:j])\n                    start += 1\n\n                start = 0\n                indx = to_ids(caption)[1:]\n                out_words[start: start + len(indx)] = torch.Tensor(indx)\n                start += len(indx)\n\n\n\n                self.in_segs_dict[i].append(in_seqs)\n                self.out_words_dict[i].append(out_words)\n\n    def __getitem__(self, index):\n\n        # \u041f\u043e\u043b\u0443\u0447\u0430\u0435\u043c \u043f\u0440\u0435\u0434\u043e\u0431\u0440\u0430\u0431\u043e\u0442\u0430\u043d\u043d\u043e\u0435 \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u0435\n        id_ = self.imgs_ids[index]\n        im_path = os.path.join(data_folder, id_)\n        im = mpimg.imread(im_path)\n        if self.is_train:\n            im = image_prepare(im)\n        else:\n            im = image_prepare_val(im)\n\n        cap_ind = np.random.choice(range(5))\n        return im, self.in_segs_dict[index][cap_ind], self.out_words_dict[index][cap_ind]\n\n    def __len__(self):\n        return len(self.imgs_ids)","6bc3432f":"ds_train = ImageCaptioningDataset(train_data)\nds_val = ImageCaptioningDataset(test_data, is_train=False)","b55984fc":"img_, in_seqs_, out_words_ = ds_train[0]\n\nassert isinstance(img_, torch.Tensor)\nassert isinstance(in_seqs_, torch.Tensor)\nassert in_seqs_.dtype in {torch.int64, torch.LongTensor}\nassert isinstance(out_words_, torch.Tensor)\nassert out_words_.dtype in {torch.int64, torch.LongTensor}\n\nassert img_.shape[0] == 3\nif len(in_seqs_.shape) == 2:\n    assert in_seqs_.shape == (out_words_.shape[0], max_seq_len)\nelse:\n    assert in_seqs_.shape[0] == max_seq_len\n    assert out_words_.shape[0] == 1\n\nassert ds_train.__len__() == len(train_data)\n\nprint(f\"\u0420\u0430\u0437\u043c\u0435\u0440 \u0442\u0435\u043d\u0437\u043e\u0440\u0430 \u043a\u0430\u0440\u0442\u0438\u043d\u043a\u0438: {img_.shape}\")\nprint(f\"\u0420\u0430\u0437\u043c\u0435\u0440 \u0442\u0435\u043d\u0437\u043e\u0440\u0430 \u0432\u0445\u043e\u0434\u043d\u044b\u0445 \u043f\u0440\u0435\u0434\u043b\u043e\u0436\u0435\u043d\u0438\u0439 (\u043f\u043e\u0434\u043f\u0438\u0441\u0435\u0439): {in_seqs_.shape}\")\nprint(f\"\u0420\u0430\u0437\u043c\u0435\u0440 \u0442\u0435\u043d\u0437\u043e\u0440\u0430 \u0432\u044b\u0445\u043e\u0434\u043d\u044b\u0445 \u0442\u043e\u043a\u0435\u043d\u043e\u0432: {out_words_.shape}\")","4d43d5fe":"# \u041f\u0440\u043e\u0432\u0435\u0440\u0438\u043c, \u0447\u0442\u043e \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u0435 \u0432\u0441\u0451 \u0435\u0449\u0451 \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u0435,\n# \u0434\u043b\u044f \u044d\u0442\u043e\u0433\u043e \u043f\u0435\u0440\u0435\u0432\u0435\u0434\u0451\u043c torch.tensor \u0432 numpy \u043c\u0430\u0441\u0441\u0438\u0432 \u0438 \u0432\u044b\u043f\u043e\u043b\u043d\u0438\u043c \u0435\u0449\u0451 \u0440\u044f\u0437 \u043f\u0440\u0435\u043e\u0431\u0440\u0430\u0437\u043e\u0432\u0430\u043d\u0438\u0439\nshape = img_.shape[1:]\nplt.imshow(minmax_scale(img_.numpy().transpose(1, 2, 0).reshape(3, -1), axis=1).reshape(*shape, 3))\nplt.show()","fdc5c28f":"k = 7\n# \u041f\u0440\u043e\u0432\u0435\u0440\u044c\u0442\u0435 \u0441\u0435\u0431\u044f, \u0432\u0438\u0437\u0443\u0430\u043b\u044c\u043d\u043e, \u0442\u0443\u0442 \u0434\u043e\u043b\u0436\u043d\u0430 \u0431\u044b\u0442\u044c \u043b\u0435\u0441\u0435\u043d\u043a\u0430 =) \nprint(in_seqs_[:k+1,:k+1].numpy())\nprint('\\n')\n# \u0422\u0443\u0442 \u0434\u043e\u043b\u0436\u043d\u044b \u0431\u044b\u0442\u044c \u0442\u043e\u043a\u0435\u043d\u044b, \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u043d\u0443\u0436\u043d\u043e \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u0442\u044c \u043d\u0430 \u043a\u0430\u0436\u0434\u0443\u044e \u0441\u0442\u0440\u043e\u0447\u043a\u0443 \u043c\u0430\u0442\u0440\u0438\u0446\u044b\nprint('    ', out_words_[:k].numpy())","c06069d0":"# \u0421\u043b\u043e\u0436\u0435\u043c \u0431\u0430\u0442\u0447 \u0432 list<tensor> \u0434\u043b\u044f \u0432\u0441\u0435\u0445 \u0434\u0430\u043d\u043d\u044b\u0445 \u0438\u0437 \u0431\u0430\u0442\u0447\u0430\ndef collate_fn(batch):\n    img_batch, in_seqs_batch, out_words_batch = [], [], []\n    for b in batch:\n        a, b, c = b\n        img_batch.append(a)\n        in_seqs_batch.append(b)\n        out_words_batch.append(c)\n    return torch.stack(img_batch), in_seqs_batch, out_words_batch","60a78e60":"# \u0421\u043e\u0437\u0434\u0430\u0451\u043c \u0442\u0440\u0435\u043d\u0438\u0440\u043e\u0432\u043e\u0447\u043d\u044b\u0439 \u0438 \u0432\u0430\u043b\u0438\u0434\u0430\u0446\u0438\u043e\u043d\u043d\u044b\u0439 \u0434\u0430\u0442\u0430\u043b\u043e\u0430\u043b\u0435\u0440\u044b, \u0441 \u0440\u0430\u0437\u043c\u0435\u0440\u043e\u043c \u0431\u0430\u0442\u0447\u0430 - 64\nbatch_size = 64\n\ndataloader_train = DataLoader(\n    dataset=ds_train,\n    batch_size=batch_size,\n    collate_fn=collate_fn,\n    shuffle=True,\n    drop_last=True,\n)\n\ndataloader_val = DataLoader(\n    dataset=ds_val,\n    batch_size=batch_size,\n    collate_fn=collate_fn,\n    shuffle=False,\n    drop_last=False,\n)","8acb8766":"img_batch, in_seqs_batch, out_words_batch = next(iter(dataloader_train))\n\nassert isinstance(img_batch, (list, torch.Tensor))\nassert isinstance(in_seqs_batch, (list, torch.Tensor))\nassert isinstance(out_words_batch, (list, torch.Tensor))\n\nassert len(img_batch) == batch_size\nassert len(in_seqs_batch) == batch_size\nassert len(out_words_batch) == batch_size\n\nassert isinstance(img_batch[0], torch.Tensor)\nassert isinstance(in_seqs_batch[0], torch.Tensor)\nassert in_seqs_batch[0].dtype in {torch.int64, torch.LongTensor}\nassert isinstance(out_words_batch[0], torch.Tensor)\nassert out_words_batch[0].dtype in {torch.int64, torch.LongTensor}\n\nassert all([img_batch[i].shape[0] == 3 for i in range(batch_size)])","498a9aa1":"from torchvision import models\nfrom torch import nn\nimport torch.nn.functional as F","8041a83f":"class img_fe_class(nn.Module):\n    def __init__(self):\n        super(img_fe_class, self).__init__()\n        self.img_fe = models.resnet18(pretrained=True)\n        self.img_fe = torch.nn.Sequential(*(list(self.img_fe.children())[:-1]))  # \u0423\u0434\u0430\u043b\u044f\u0435\u043c \u043f\u043e\u0441\u043b\u0435\u0434\u043d\u0438\u0439 \u043f\u043e\u043b\u043d\u043e\u0441\u0432\u044f\u0437\u043d\u044b\u0439 \u0441\u043b\u043e\u0439\n        self.feature_size = 512\n        \n    def forward(self, imgs):\n        x = self.img_fe(imgs)\n        return x[:, :, 0, 0]","a86b50e4":"img_fe = img_fe_class()\n    \nfeat_img = img_fe(img_[None, ...])\nassert len(feat_img.shape) == 2, \"\u041d\u0430 \u0432\u044b\u0445\u043e\u0434\u0435 \u0434\u043e\u043b\u0436\u043d\u0430 \u0431\u044b\u0442\u044c \u0434\u0432\u0443\u043c\u0435\u0440\u043d\u0430\u044f \u043c\u0430\u0442\u0440\u0438\u0446\u0430 [batch_size, feature_size]\"\nassert feat_img.shape[0] == 1, \"\u041f\u0435\u0440\u0432\u0430\u044f \u0440\u0430\u0437\u043c\u0435\u0440\u043d\u043e\u0441\u0442\u044c \u0434\u043e\u043b\u0436\u043d\u0430 \u0440\u0430\u0432\u043d\u044f\u0442\u044c\u0441\u044f 1, \u0442\u0430\u043a \u043a\u0430\u043a \u043c\u044b \u043f\u043e\u0434\u0430\u043b\u0438 \u043d\u0430 \u0432\u0445\u043e\u0434 \u0431\u0430\u0442\u0447 \u0438\u0437 \u043e\u0434\u043d\u043e\u0439 \u043a\u0430\u0440\u0442\u0438\u043d\u043a\u0438\"\n\nprint(f'outputs {feat_img.shape[1]} features from feature extractor')","a8f4f216":"! wget -O glove.zip https:\/\/huggingface.co\/stanfordnlp\/glove\/resolve\/main\/glove.840B.300d.zip\n# mirror https:\/\/nlp.stanford.edu\/data\/wordvecs\/glove.840B.300d.zip\n! unzip -q glove.zip","daaf62d2":"# \u041e\u0442\u043a\u0440\u044b\u0432\u0430\u0435\u043c glove\nnp.random.seed(19)\n\ndef load_glove_weights(File):\n    print(\"Loading Glove Weights\")\n    # \u0418\u043d\u0438\u0446\u0438\u0430\u043b\u0438\u0437\u0438\u0440\u0443\u0435\u043c \u0432\u0435\u0441\u0430 \u0434\u043b\u044f \u0432\u0441\u0435\u0445 \u0441\u043b\u043e\u0432 \u0441\u0442\u0430\u043d\u0434\u0430\u0440\u0442\u043d\u044b\u043c \u043d\u043e\u0440\u043c\u0430\u043b\u044c\u043d\u044b\u043c \u0440\u0430\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u0435\u043c\n    glove_weights = np.random.normal(0, 1, (vocab_size, 300))\n    mask_found = [False] * vocab_size\n    k = 0\n    with open(File,'r') as f:\n        for line in tqdm(f, total=2196018):\n            split_line = line.split()\n            try:\n                key = ' '.join(split_line[:-300])\n                # \u0431\u0435\u0440\u0451\u043c \u0432\u0441\u0435 \u0441\u043b\u043e\u0432\u0430 \u0438\u0437 key \u0432 \u0444\u043e\u0440\u043c\u0430\u0442\u0435 \u043d\u0430\u0448\u0438\u0445 \u0442\u043e\u043a\u0435\u043d\u043e\u0432 (\u043a\u0440\u043e\u043c\u0435 <BOS>, <EOS>)\n                words = tokenize(key)[1:-1]\n                # \u0414\u043e\u0431\u0430\u0432\u043b\u044f\u0435\u043c \u044d\u043c\u0431\u0435\u0434\u0434\u0438\u043d\u0433 \u0438\u0437 glove, \u0435\u0441\u043b\u0438 \u0437\u043d\u0430\u0435\u043c \u0441\u043b\u043e\u0432\u043e\n                for word in words:\n                    if word in tok_to_ind and not mask_found[tok_to_ind[word]]:\n                        embedding = np.array(list(map(lambda s: 0. if s == '.' else float(s), split_line[-300:])), dtype=np.double)\n                        glove_weights[tok_to_ind[word],:] = embedding\n                        mask_found[tok_to_ind[word]] = True\n                        k += 1\n            except:\n                continue\n    print(f\"{k} words from vocab of size {vocab_size} loaded!\")\n    glove_weights[tok_to_ind['<PAD>']] = np.zeros(300)\n    return glove_weights\n\nglove_weights = load_glove_weights('glove.840B.300d.txt')","a53d1950":"class text_fe_class(nn.Module):\n    def __init__(self, glove_weights=None):\n        super(text_fe_class, self).__init__()\n        if glove_weights is not None:\n            self.voc_size = glove_weights.shape[0]\n            self.h_emb_size = glove_weights.shape[1]\n            self.embed = nn.Embedding.from_pretrained(torch.from_numpy(glove_weights).float(),\n                                                      freeze=False,\n                                                      padding_idx=tok_to_ind['<PAD>'])\n        else:\n            self.voc_size = vocab_size\n            self.h_emb_size = 300\n            self.embed = nn.Embedding(num_embeddings=self.voc_size, embedding_dim=self.h_emb_size)\n\n        hid_size = self.h_emb_size\n        self.rnn = nn.LSTM(hid_size, hid_size, batch_first=True)\n        self.feature_size = hid_size\n\n    def compute_mask(self, in_seqs):\n        res = np.zeros_like(in_seqs.cpu().numpy())\n        for i in range(in_seqs.shape[0]):\n            for j in range(in_seqs.shape[1]):\n                if in_seqs[i, j] == tok_to_ind['<PAD>']:\n                    res[i, j] = j\n                    break\n        res = res.sum(1)\n        tmp = np.zeros((in_seqs.shape[0],in_seqs.shape[1], self.h_emb_size)) # 58, 38, 300\n        for i, ind in enumerate(res):\n            tmp[i, ind, :] = np.ones(self.h_emb_size)\n        return torch.tensor(tmp)\n        \n    def forward(self, in_seqs):\n        device = self.embed.weight.device\n        feat = self.embed(in_seqs)\n        lstm_out, (ht, ct) = self.rnn(feat)\n        mask = self.compute_mask(in_seqs).to(device)\n        return torch.mul(lstm_out, mask).sum(dim=1)","df726431":"# \u0415\u0441\u043b\u0438 in_seqs_ - \u043e\u0434\u0438\u043d \u0432\u0435\u043a\u0442\u043e\u0440, \u0442\u043e \u0432 \u044d\u0442\u043e\u043c \u0431\u043b\u043e\u043a\u0435 \u043d\u0443\u0436\u043d\u043e \u0437\u0430\u043f\u043e\u043b\u043d\u0438\u0442\u044c \u043a\u043e\u0434 \u0447\u0443\u0442\u044c \u0438\u043d\u0430\u0447\u0435\ntext_fe = text_fe_class(glove_weights)\nfeat_text = text_fe(in_seqs_)\n\nassert in_seqs_.shape[0] == feat_text.shape[0]\nassert len(feat_text.shape) == 2, '\u041f\u043e \u043a\u0430\u0436\u0434\u043e\u043c\u0443 \u0442\u0435\u043a\u0441\u0442\u0443 \u0432\u0430\u043c \u043d\u0443\u0436\u0435\u043d \u043e\u0434\u0438\u043d \u0432\u0435\u043a\u0442\u043e\u0440, \u0430 RNN \u0431\u043b\u043e\u043a \u0432\u043e\u0437\u0432\u0440\u0430\u0449\u0430\u0435\u0442 output \u0434\u043b\u044f \u043a\u0430\u0436\u0434\u043e\u0433\u043e \u0448\u0430\u0433\u0430 seq_len, \u043d\u0435 \u0437\u0430\u0431\u0443\u0434\u044c\u0442\u0435 \u043a\u0430\u043a-\u0442\u043e \u044d\u0442\u043e \u0438\u0441\u043f\u0440\u0430\u0432\u0438\u0442\u044c'\n    \nprint(in_seqs_.shape)\nprint(feat_text.shape)","be2a63a3":"device = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')\n\nclass image_captioning_model(nn.Module):\n    def __init__(self, img_fe, text_fe, device=device):\n        super(image_captioning_model, self).__init__()\n        self.img_fe = img_fe.to(device)\n        self.text_fe = text_fe.to(device)\n        self.device = device\n        self.lin = nn.Linear(self.text_fe.feature_size + self.img_fe.feature_size, vocab_size)\n        \n    def forward(self, img_batch, in_seqs_batch):\n        img_batch = img_batch.to(self.device)\n        img_features = self.img_fe(img_batch)\n        cap_features = []\n        for i, elem in enumerate(in_seqs_batch):\n            a = torch.cat([self.text_fe(elem.to(self.device)), img_features[i].repeat(elem.shape[0], 1).to(self.device)], dim=1)\n            cap_features.append(a)\n        final_features = torch.cat(cap_features, dim=0)\n        res = self.lin(final_features.float())\n        return res","8ff3d224":"img_fe = img_fe_class()\ntext_fe = text_fe_class(glove_weights)\nmodel = image_captioning_model(img_fe, text_fe).float().to(device)\n\nres = model(img_batch, in_seqs_batch)\n\nif isinstance(out_words_batch, list):\n    assert res.shape[0] == np.sum([el.shape[0] for el in out_words_batch])\nelse:\n    assert res.shape[0] == out_words_batch.shape[0]\nassert res.shape[1] == vocab_size","f014d077":"# \u0438\u043d\u0438\u0446\u0438\u0430\u043b\u0438\u0437\u0438\u0440\u0443\u0435\u043c \u043c\u043e\u0434\u0435\u043b\u044c, \u043e\u043f\u0442\u0438\u043c\u0438\u0437\u0430\u0442\u043e\u0440 \u0438 \u0444\u0443\u043d\u043a\u0446\u0438\u044e \u043f\u043e\u0442\u0435\u0440\u044c (loss)\nimg_fe = img_fe_class()\ntext_fe = text_fe_class(glove_weights)\nmodel = image_captioning_model(img_fe, text_fe).float().to(device)\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\ncriterion = F.cross_entropy","9b6e850a":"def compute_loss(pred, gt, criterion, device):\n    target = torch.cat([el for el in gt], axis=0)\n    target = target.to(device)\n    return criterion(pred, target)\n\n\ndef train(model, opt, loader, criterion, device):\n    model.train()\n    losses_tr = []\n    for img_batch, in_seqs_batch, out_words_batch in tqdm(loader):\n        optimizer.zero_grad()\n        pred = model(img_batch, in_seqs_batch)\n        loss = compute_loss(pred, out_words_batch, criterion, device)\n        \n        loss.backward()\n        optimizer.step()\n        losses_tr.append(loss.item()) \n    \n    return model, optimizer, np.mean(losses_tr)\n\ndef val(model, loader, criterion, device):\n    model.eval()\n    losses_val = []\n    with torch.no_grad():\n        for img_batch, in_seqs_batch, out_words_batch in tqdm(loader):\n            pred = model(img_batch, in_seqs_batch)\n            loss = compute_loss(pred, out_words_batch, criterion, device)\n\n            losses_val.append(loss.item())\n    \n    return np.mean(losses_val)","73933fad":"from IPython.display import clear_output\n\ndef get_lr(optimizer):\n    for param_group in optimizer.param_groups:\n        return param_group['lr']\n\n    \ndef set_lr(optimizer, lr):\n    for param_group in optimizer.param_groups:\n        param_group['lr'] = lr\n\nnum_starts = 0\n\ndef learning_loop(\n    model,\n    optimizer,\n    train_loader,\n    val_loader,\n    criterion,\n    scheduler=None,\n    min_lr=None,\n    epochs=10,\n    val_every=1,\n    draw_every=1,\n    separate_show=False,\n    model_name=f'model{num_starts}',\n    save_only_best=True,\n    device=device\n):\n    global num_starts\n    num_starts += 1\n    \n    losses = {'train': [], 'val': []}\n    best_val_loss = np.Inf\n    os.makedirs('.\/checkpoints', exist_ok=True)\n    os.makedirs(os.path.join('.\/checkpoints', model_name))\n\n    for epoch in range(1, epochs+1):\n        print(f'#{epoch}\/{epochs}, lr:{get_lr(optimizer)}:')\n        model, optimizer, loss = train(model, optimizer, train_loader, criterion, device)\n        losses['train'].append(loss)\n\n        if not (epoch % val_every):\n            loss = val(model, val_loader, criterion, device)\n            losses['val'].append(loss)\n            \n            # \u0421\u043e\u0445\u0440\u0430\u043d\u044f\u0435\u043c \u043b\u0443\u0447\u0448\u0443\u044e \u043f\u043e \u0432\u0430\u043b\u0438\u0434\u0430\u0446\u0438\u0438 \u043c\u043e\u0434\u0435\u043b\u044c\n            if ((not save_only_best) or (loss < best_val_loss)):\n                if scheduler:\n                    torch.save({\n                        'epoch': epoch,\n                        'model_state_dict': model.state_dict(),\n                        'optimizer_state_dict': optimizer.state_dict(),\n                        'scheduler_state_dict': scheduler.state_dict(),\n                        'losses': losses\n                    }, os.path.join('.\/checkpoints', model_name, f'{model_name}#{epoch}.pt'))\n                else:\n                    torch.save({\n                        'epoch': epoch,\n                        'model_state_dict': model.state_dict(),\n                        'optimizer_state_dict': optimizer.state_dict(),\n                        'losses': losses\n                    }, os.path.join('.\/checkpoints', model_name, f'{model_name}#{epoch}.pt'))\n                    \n                best_val_loss = loss\n            \n            if scheduler:\n                scheduler.step(loss)\n            \n            if not scheduler and ((epoch == 6) or (epoch == 8)):\n                cur_lr = get_lr(optimizer)\n                set_lr(optimizer, cur_lr \/ 10) # drop lr\n\n        if not (epoch % draw_every):\n            clear_output(True)\n            fig, ax = plt.subplots(1, 2 if separate_show else 1, figsize=(20, 10))\n            fig.suptitle(f'#{epoch}\/{epochs}:')\n\n            if separate_show:\n                plt.subplot(121)\n                plt.title('loss on train')\n            plt.plot(losses['train'], 'r.-', label='train')\n            plt.legend()\n\n            if separate_show:\n                plt.subplot(122)\n                plt.title('loss on validation')\n            else:\n                plt.title('losses')\n            plt.plot(losses['val'], 'g.-', label='val')\n            plt.legend()\n            \n            plt.show()\n        \n        if min_lr and get_lr(optimizer) <= min_lr:\n            print(f'Learning process ended with early stop after epoch {epoch}')\n            break\n    \n    return model, optimizer, losses","8d10d915":"model, optimizer, losses = learning_loop(\n    model = model,\n    optimizer = optimizer,\n    train_loader = dataloader_train,\n    val_loader = dataloader_val,\n    criterion = criterion,\n    scheduler = None,\n    epochs = 10,\n    min_lr = None,\n    save_only_best = False\n)","e0a0edd0":"def generate(model, image):\n    \"\"\"\n    :param model: image caption model, return logits\/scores for next token\n    :param image: numpy array of image  \n    :return: string - content on the image\n    \"\"\"\n    model.eval()\n    ################################################################\n    ############           YOUR CODE HERE!          ################\n    ################################################################\n    return \"generated string\"","8beea9b7":"inds = list(range(10))\n\nh, w = 2, 5\ntitle_width = 43\n\nassert h*w >= len(inds)\n\nfig, ax = plt.subplots(h, w, figsize=(20, 15))\n\nfor i, ind in enumerate(inds):\n    img_id = list(test_data.keys())[i]\n    img = mpimg.imread(os.path.join(data_folder, img_id))\n    \n    pred_caption = generate(model, img)\n    pred_caption = '\\n'.join([pred_caption[k:k+title_width] for k in range(0, len(pred_caption), title_width)])\n    \n    captions = test_data[img_id]\n    caption_adjasted = list(map(lambda el: '\\n'.join([(str(el[0]) + ': ' + el[1])[k:k+title_width] for k in range(0, 3 + len(el[1]), title_width)]), enumerate(captions)))\n    \n    caption = '\\n'.join(['pred:\\n' + pred_caption + '\\n\\n'] + caption_adjasted)\n    \n    plt.subplot(h, w, i+1)\n    plt.title(caption)\n    plt.imshow(img)\n\nfig.tight_layout()\nplt.show()","b40f03f3":"from torchtext.data.metrics import bleu_score\n\ndef get_bleu(model):\n    candidates = []\n    references = []\n\n    for i, ind in tqdm(enumerate(range(len(test_data))), total=len(test_data)):\n        img_id = list(test_data.keys())[ind]\n        img = mpimg.imread(os.path.join(data_folder, img_id))\n        pred_caption = generate(model, img)\n        pred_caption_tokens = list(pred_caption.split())\n\n        captions = test_data[img_id]\n        captions_tokens = [tokenize(cap)[1:-1] for cap in captions]\n\n        candidates.append(pred_caption_tokens)\n        references.append(captions_tokens)\n\n    return np.mean([bleu_score(candidates, references, weights=w) for w in np.eye(4)])","1fdd8487":"bleu_res = get_bleu(model)\nprint(bleu_res)","a908995a":"### \u0412\u0438\u0437\u0443\u0430\u043b\u0438\u0437\u0438\u0440\u0443\u0435\u043c \u043a\u0430\u0440\u0442\u0438\u043d\u043a\u0438 \u0438 \u043f\u043e\u0434\u043f\u0438\u0441\u0438","ebdd23c1":"\u0414\u0430\u043d\u043d\u044b\u0435 \u043f\u0440\u0435\u0434\u0441\u0442\u0432\u043b\u0435\u043d\u044b \u0432 \u0432\u0438\u0434\u0435 \u043f\u0430\u043f\u043a\u0438 `..\/input\/flickr8k\/Images` \u0441 \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u044f\u043c\u0438 \u043a\u043e\u043d\u0442\u0435\u043d\u0442\u0430, \u0432\u043c\u0435\u0441\u0442\u0435 \u0441 \u0444\u0430\u0439\u043b\u043e\u043c \u0430\u043d\u043d\u043e\u0442\u0430\u0446\u0438\u0439 - `..\/input\/flickr8k\/captions.txt`\n\n\u0414\u043b\u044f \u043a\u0430\u0436\u0434\u043e\u0439 \u043a\u0430\u0440\u0442\u0438\u043d\u043a\u0438 \u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u043b\u0435\u043d\u043e 5 \u0432\u043e\u0437\u043c\u043e\u0436\u043d\u044b\u0445 \u043f\u043e\u0434\u043f\u0438\u0441\u0435\u0439 \u043a\u043e\u043d\u0442\u0435\u043d\u0442\u0430 \u043d\u0430 \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u0438","66b19f44":"**\u0412 `glove_weights` \u0442\u0435\u043f\u0435\u0440\u044c \u043f\u043e \u0438\u043d\u0434\u0435\u043a\u0441\u0443 `i`  \u043b\u0435\u0436\u0438\u0442 \u044d\u043c\u0431\u0435\u0434\u0438\u043d\u0433, \u0441\u043e\u043e\u0442\u0432\u0435\u0442\u0441\u0442\u0432\u0443\u044e\u0449\u0438\u0439 \u0442\u043e\u043a\u0435\u043d\u0443 `ind_to_tok[i]`**\n\n\u041c\u043e\u0436\u043d\u043e \u0431\u044b\u043b\u043e \u0431\u044b \u0438 \u043d\u0435 \u0434\u0435\u043b\u0430\u0442\u044c \u044d\u0442\u043e\u0442 \u0448\u0430\u0433, \u043d\u043e \u0442\u043e\u0433\u0434\u0430 \u043f\u0440\u0438\u0448\u043b\u043e\u0441\u044c \u0431\u044b \u043e\u0431\u0443\u0447\u0430\u0442\u044c \u044d\u0442\u0438 \u044d\u043c\u0431\u0435\u0434\u0438\u043d\u0433\u0438 \u0441 \u043d\u0443\u043b\u044f, \u0442\u043e-\u0435\u0441\u0442\u044c \u0438\u043d\u0438\u0446\u0438\u0430\u043b\u0438\u0437\u0438\u0440\u043e\u0432\u0430\u0432 \u0438\u0445 \u0441\u043b\u0443\u0447\u0430\u0439\u043d\u044b\u043c \u0441\u043f\u043e\u0441\u043e\u0431\u043e\u043c. \u0422\u0443\u0442 \u044d\u0442\u043e \u0441\u0434\u0435\u043b\u0430\u043d\u043e \u043d\u0435 \u0442\u043e\u043b\u044c\u043a\u043e \u0434\u043b\u044f \u0443\u0441\u043a\u043e\u0440\u0435\u043d\u0438\u044f \u0442\u0440\u0435\u043d\u0438\u0440\u043e\u0432\u043a\u0438 \u0438 \u043f\u043e\u043b\u0443\u0447\u0435\u043d\u0438\u044f \u043b\u0443\u0447\u0448\u0438\u0445 \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u043e\u0432, \u043d\u043e \u0438 \u0441\u0442\u043e\u0439 \u0446\u0435\u043b\u044c\u044e, \u0447\u0442\u043e\u0431\u044b \u043f\u0440\u043e\u0441\u0442\u043e \u043f\u043e\u043a\u0430\u0437\u0430\u0442\u044c \u0438 \u043f\u043e\u043f\u0440\u043e\u0431\u043e\u0432\u0430\u0442\u044c \u044d\u0442\u0443 \u0442\u0435\u0445\u043d\u0438\u043a\u0443 \u043d\u0430 \u043f\u0440\u0430\u043a\u0442\u0438\u043a\u0435 =)","3d228991":"\u041d\u0430\u043c \u043d\u0435\u043e\u0431\u0445\u043e\u0434\u0438\u043c\u043e \u0443\u043c\u0435\u0442\u044c \u0440\u0430\u0437\u0431\u0438\u0432\u0430\u0442\u044c \u043f\u0440\u0435\u0434\u043b\u043e\u0436\u0435\u043d\u0438\u044f \u043d\u0430 \u043f\u043e\u0441\u043b\u0435\u0434\u043e\u0432\u0430\u0442\u0435\u043b\u044c\u043d\u043e\u0441\u0442\u044c \u0442\u043e\u043a\u0435\u043d\u043e\u0432, \u0447\u0442\u043e\u0431\u044b \u043e\u0431\u0440\u0430\u0431\u043e\u0442\u0430\u0442\u044c \u0442\u0435\u043a\u0441\u0442 \u0440\u0435\u043a\u0443\u0440\u0435\u043d\u0442\u043d\u043e\u0439 \u043d\u0435\u0439\u0440\u043e\u043d\u043d\u043e\u0439 \u0441\u0435\u0442\u044c\u044e (\u0432 \u0434\u0430\u043b\u044c\u043d\u0435\u0439\u0448\u0435\u043c \u0431\u0443\u0434\u0435\u0442 \u0431\u043e\u043b\u0435\u0435 \u043f\u043e\u043d\u044f\u0442\u043d\u043e)\n\n\u0412 \u0434\u0430\u043d\u043d\u043e\u043c \u0441\u043b\u0443\u0447\u0430\u0438, \u043d\u0430\u0448\u0430 \u0444\u0443\u043d\u043a\u0446\u0438\u044f \u0442\u043e\u043a\u0435\u043d\u0438\u0437\u0430\u0446\u0438\u0438 \u0431\u0443\u0434\u0435\u0442 \u0443\u0441\u0442\u0440\u043e\u0435\u043d\u0430 \u0441\u043b\u0435\u0434\u0443\u044e\u0449\u0438\u043c \u043e\u0431\u0440\u0430\u0437\u043e\u043c:\n\n1. \u041f\u0440\u0438\u0432\u043e\u0434\u0438\u043c \u0432\u0441\u0435 \u0441\u0438\u043c\u0432\u043e\u043b\u044b \u043a \u043d\u0438\u0436\u043d\u0435\u043c\u0443 \u0440\u0435\u0433\u0438\u0441\u0442\u0440\u0443\n2. \u0423\u0434\u0430\u043b\u044f\u0435\u043c \u0437\u043d\u0430\u043a\u0438 \u043f\u0440\u0435\u043f\u0438\u043d\u0430\u043d\u0438\u044f, \u043b\u0438\u0448\u043d\u044b\u0435 \u043f\u0440\u043e\u0431\u0435\u043b\u044b \u0438 \u0435\u0449\u0451 \u0440\u044f\u0434 \u043d\u0435 \u043d\u0443\u0436\u043d\u044b\u0445 \u0441\u0438\u043c\u0432\u043e\u043b\u043e\u0432\n3. \u0420\u0430\u0437\u0431\u0438\u0432\u0430\u0435\u043c \u043f\u0440\u0435\u0434\u043b\u043e\u0436\u0435\u043d\u0438\u044f \u043d\u0430 \u0441\u043b\u043e\u0432\u0430 (\u043f\u043e \u043f\u0440\u043e\u0431\u0435\u043b\u0430\u043c), \u044d\u0442\u043e \u0438 \u0431\u0443\u0434\u0443\u0442 \u043d\u0430\u0448\u0438 \u0442\u043e\u043a\u0435\u043d\u044b\n4. \u0414\u043e\u0431\u0430\u0432\u043b\u044f\u0435\u043c `Begin of sentence` (`<BOS>`) \u0438 `End of sentence` (`<EOS>`) \u0442\u043e\u043a\u0435\u043d\u044b \u0432 \u043d\u0430\u0447\u0430\u043b\u043e \u0438 \u0432 \u043a\u043e\u043d\u0435\u0446 \u043f\u043e\u0441\u043b\u0435\u0434\u043e\u0432\u0430\u0442\u0435\u043b\u044c\u043d\u043e\u0441\u0442\u0438 \u0441\u043e\u043e\u0442\u0432\u0435\u0442\u0441\u0442\u0432\u0435\u043d\u043d\u043e","56dcebe3":"### \u041d\u0435\u043e\u0431\u0445\u043e\u0434\u0438\u043c\u043e \u0440\u0430\u0437\u0431\u0438\u0442\u044c \u043d\u0430\u0448\u0438 \u0434\u0430\u043d\u043d\u044b\u0435 \u043d\u0430 \u0442\u0435\u0441\u0442\u043e\u0432\u0443\u044e \u0438 \u0442\u0440\u0435\u043d\u0438\u0440\u043e\u0432\u043e\u0447\u043d\u0443\u044e \u0432\u044b\u0431\u043e\u0440\u043a\u0438:","fcce38ff":"\u041f\u0440\u043e\u0432\u0435\u0440\u0438\u043c \u0440\u0435\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u044e:","dfa5bf67":"\u0412 \u043d\u0430\u0448\u0435\u043c \u0441\u043b\u0443\u0447\u0430\u0438, \u0442\u0435\u043a\u0441\u0442\u043e\u0432\u0430\u044f \u043c\u043e\u0434\u0435\u043b\u044c \u0431\u0443\u0434\u0435\u0442 \u043f\u0440\u0438\u043d\u0438\u043c\u0430\u0442\u044c \u043d\u0430 \u0432\u0445\u043e\u0434 \u0442\u0435\u043d\u0437\u043e\u0440 `in_segs` \u0440\u0430\u0437\u043c\u0435\u0440\u043d\u043e\u0441\u0442\u044c\u044e `[N, max_seq_len]`, \u0433\u0434\u0435 `N` - \u0447\u0438\u0441\u043b\u043e \u043f\u0440\u0435\u0434\u043b\u043e\u0436\u0435\u043d\u0438\u0439. \\\n`in_segs[0]` == \u043c\u0430\u0441\u0441\u0438\u0432 \u0438\u043d\u0434\u0435\u043a\u0441\u043e\u0432 \u0442\u043e\u043a\u0435\u043d\u043e\u0432 \u0434\u043b\u044f \u043f\u0435\u0440\u0432\u043e\u0439 \u0432\u0445\u043e\u0434\u043d\u043e\u0439 \u043f\u043e\u0441\u043b\u0435\u0434\u043e\u0432\u0430\u0442\u0435\u043b\u044c\u043d\u043e\u0441\u0442\u0438\\\n\u0414\u043b\u044f \u043a\u0430\u0436\u0434\u043e\u0433\u043e \u0438\u0437 `N` \u0432\u0445\u043e\u0434\u043d\u044b\u0445 \u043f\u043e\u0441\u043b\u0435\u0434\u043e\u0432\u0430\u0442\u0435\u043b\u044c\u043d\u043e\u0441\u0442\u0435\u0439, \u043c\u044b \u0445\u043e\u0442\u0438\u043c \u043f\u043e\u043b\u0443\u0447\u0438\u0442\u044c \u043e\u0434\u0438\u043d \u0432\u0435\u043a\u0442\u043e\u0440, \u043a\u043e\u0442\u043e\u0440\u044b\u0439 \u0431\u0443\u0434\u0435\u0442 \u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u043b\u044f\u0442\u044c \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0438 \u0438\u0445\u043e\u0434\u043d\u043e\u0433\u043e \u043f\u0440\u0435\u0434\u043b\u043e\u0436\u0435\u043d\u0438\u044f.\n\u041d\u0435\u043e\u0431\u0445\u043e\u0434\u0438\u043c\u043e \u0434\u043b\u044f \u043a\u0430\u0436\u0434\u043e\u0433\u043e \u0442\u043e\u043a\u0435\u043d\u0430 \u0434\u043e\u0441\u0442\u0430\u0442\u044c \u0441\u043e\u043e\u0442\u0432\u0435\u0442\u0441\u0442\u0432\u0443\u044e\u0449\u0438\u0439 \u0435\u043c\u0443 \u044d\u043c\u0431\u0435\u0434\u0438\u043d\u0433, \u044d\u0442\u043e \u0434\u0435\u043b\u0430\u0435\u0442\u0441\u044f \u0447\u0435\u0440\u0435\u0437 `torch.nn.Embedding` \u0441\u043b\u043e\u0439 (\u043d\u0430 \u043a\u0430\u0440\u0442\u0438\u043d\u043a\u0435 \u044d\u0442\u0438 \u044d\u043c\u0431\u0435\u0434\u0438\u043d\u0433\u0438 \u043e\u0431\u043e\u0437\u043d\u0430\u0447\u0435\u043d\u044b \u0447\u0435\u0440\u0435\u0437 $X_0$, $X_1$, $X_2$, ..., $X_n$),\\\n\u041d\u0430\u043f\u0440\u0438\u043c\u0435\u0440 \u0435\u0441\u043b\u0438 `features = embedding_layer(in_segs)`, \u0442\u043e `features.shape == [N, max_seq_len, embedding_dim]`, \u043d\u0430\u043f\u0440\u0438\u043c\u0435\u0440 \u0432 \u043d\u0430\u0448\u0435\u043c \u0441\u043b\u0443\u0447\u0430\u0438 `embedding_dim` = 300 \u0438 \u043f\u0440\u0438 \u044d\u0442\u043e\u043c\\\n`features[i][j]` - \u044d\u043c\u0431\u0435\u0434\u0438\u043d\u0433 \u0434\u043b\u044f \u0442\u043e\u043a\u0435\u043d\u0430 \u0432\u043e \u0432\u0445\u043e\u0434\u043d\u043e\u0439 \u043f\u043e\u0441\u043b\u0435\u0434\u043e\u0432\u0430\u0442\u0435\u043b\u044c\u043d\u043e\u0441\u0442\u0438 `j`, \u043a\u043e\u0442\u043e\u0440\u044b\u0439 \u0441\u0442\u043e\u044f\u043b \u043d\u0430 \u043c\u0435\u0441\u0442\u0435 `j`\n\n\u0422\u0435\u043f\u0435\u0440\u044c, \u0432\u0441\u0435 \u044d\u0442\u0438 `features` \u043d\u0443\u0436\u043d\u043e \u043f\u0440\u043e\u043f\u0443\u0441\u0442\u0438\u0442\u044c \u0447\u0435\u0440\u0435\u0437 \u0440\u0435\u043a\u0443\u0440\u0435\u043d\u0442\u043d\u0443\u044e \u043d\u0435\u0439\u0440\u043e\u043d\u043d\u0443\u044e \u0441\u0435\u0442\u044c, \u043c\u043e\u0436\u043d\u043e \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c [torch.nn.RNN](https:\/\/pytorch.org\/docs\/stable\/generated\/torch.nn.RNN.html) \u0438\u043b\u0438 [torch.nn.LSTM](https:\/\/pytorch.org\/docs\/stable\/generated\/torch.nn.LSTM.html) \u0441\u043b\u043e\u0439\\\n\u0422\u0430\u043a\u0438\u043c \u043e\u0431\u0440\u0430\u0437\u043e\u043c, \u043f\u043e\u0441\u043b\u0435 \u044d\u0442\u043e\u0433\u043e \u0448\u0430\u0433\u0430, \u043c\u044b \u043f\u043e\u043b\u0443\u0447\u0438\u043c `features_after_rnn` \u0442\u0435\u043d\u0437\u043e\u0440 (\u043d\u0430 \u043a\u0430\u0440\u0442\u0438\u043d\u043a\u0435 \u044d\u0442\u0438 \u0444\u0438\u0447\u0438 \u043e\u0431\u043e\u0437\u043d\u0430\u0447\u0435\u043d\u044b \u043a\u0430\u043a $Y_1$, $Y_2$, ... $Y_n$) , \u0442\u043e\u0439 \u0436\u0435 \u0440\u0430\u0437\u043c\u0435\u0440\u043d\u043e\u0441\u0442\u0438, \u0447\u0442\u043e \u0438 `features`, \u043d\u0430\u043f\u043e\u043c\u0438\u043d\u0430\u044e - `[N, max_seq_len, embedding_dim]`\n\u0422\u0435\u043f\u0435\u0440\u044c \u043d\u0435\u043e\u0431\u0445\u043e\u0434\u0438\u043c\u043e \u0434\u043b\u044f \u043a\u0430\u0436\u0434\u043e\u0439 \u0432\u0445\u043e\u0434\u043d\u043e\u0439 \u043f\u043e\u0441\u043b\u0435\u0434\u043e\u0432\u0430\u0442\u0435\u043b\u044c\u043d\u043e\u0441\u0442\u0438 (\u0438\u0445 `N` \u0448\u0442\u0443\u043a), \u043a\u043e\u0442\u043e\u0440\u0430\u044f \u0441\u043e\u0441\u0442\u043e\u0438\u0442 \u0438\u0437 `max_seq_len` \u0432\u0435\u043a\u0442\u043e\u0440\u043e\u0432, \u0441\u0444\u043e\u0440\u043c\u0438\u0440\u043e\u0432\u0430\u0442\u044c \u043e\u0434\u0438\u043d \u0432\u0435\u043a\u0442\u043e\u0440, \u043a\u043e\u0442\u043e\u0440\u044b\u0439 \u0431\u0443\u0434\u0435\u0442 \u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u043b\u044f\u0442\u044c \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0438 \u0432\u0441\u0435\u0433\u043e \u043f\u0440\u0435\u0434\u043b\u043e\u0436\u0435\u043d\u0438\u044f (\u0432\u0445\u043e\u0434\u043d\u043e\u0439 \u043f\u043e\u0441\u043b\u0435\u0434\u043e\u0432\u0430\u0442\u0435\u043b\u044c\u043d\u043e\u0441\u0442\u0438), \u0441\u0430\u043c\u044b\u043c \u043f\u0440\u043e\u0441\u0442\u044b \u0441\u043f\u043e\u0441\u043e\u0431\u043e\u043c, \u044d\u0442\u043e \u043c\u043e\u0436\u043d\u043e \u0441\u0434\u0435\u043b\u0430\u0442\u044c \u0443\u0441\u0440\u0435\u0434\u043d\u0438\u0432 \u044d\u0442\u0438 \u0432\u0435\u043a\u0442\u043e\u0440\u0430 \u0444\u0438\u0447\u0435\u0439. \u0422\u0430\u043a\u0438\u043c \u043e\u0431\u0440\u0430\u0437\u043e\u043c, \u0432 \u043a\u043e\u043d\u0446\u0435 \u043a\u043e\u043d\u0446\u043e\u0432 \u043c\u044b \u043f\u043e\u043b\u0443\u0447\u0438\u043c \u0442\u0435\u043d\u0437\u043e\u0440, \u0440\u0430\u0437\u043c\u0435\u0440\u043d\u043e\u0441\u0442\u044c\u044e `[N, embedding_dim]`","da6ff4bd":"### \u0421\u043c\u0435\u0448\u0430\u043d\u043d\u0430\u044f \u043c\u043e\u0434\u0435\u043b\u044c \u0434\u043b\u044f \u043f\u043e\u0434\u043f\u0438\u0441\u0438 \u043a\u043e\u043d\u0442\u0435\u043d\u0442\u0430 \u043d\u0430 \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u0438\n\u041d\u0430 \u044d\u0442\u043e\u043c \u0448\u0430\u0433\u0435 \u043f\u0440\u0435\u0434\u0441\u0442\u043e\u0438\u0442 \u0441\u043e\u0437\u0434\u0430\u0442\u044c \u043c\u043e\u0434\u0435\u043b\u044c, \u043a\u043e\u0442\u043e\u0440\u0430\u044f \u0441\u043e\u0441\u0442\u043e\u0438\u0442 \u0438\u0437 \u0434\u0432\u0443\u0445 \u043f\u0440\u0435\u0434\u044b\u0434\u0443\u0449\u0438\u0445 \u043c\u043e\u0434\u0435\u043b\u0435\u0439 (\u043c\u043e\u0434\u0435\u043b\u0438 \u0438\u0437\u0432\u043b\u0435\u043a\u0430\u0442\u0435\u043b\u044f \u043f\u0440\u0438\u0437\u043a\u0430\u043d\u043e\u0432 \u0441 \u043a\u0430\u0440\u0442\u0438\u043d\u043a\u0438, \u0438 \u043c\u043e\u0434\u0435\u043b\u0438 \u0438\u0437\u0432\u043b\u0435\u043a\u0430\u0442\u0435\u043b\u044f \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432 \u0441 \u0442\u0435\u043a\u0441\u0442\u0430).\\\n\u0412 \u0446\u0435\u043b\u043e\u043c, \u043d\u0430\u0448\u0430 \u043c\u043e\u0434\u0435\u043b\u044c \u0431\u0443\u0434\u0435\u0442 \u0438\u0437\u0432\u043b\u0435\u043a\u0430\u0442\u044c \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0438 \u0441 \u043a\u0430\u0440\u0442\u0438\u043d\u043a\u0438, \u0438\u0437\u0432\u043b\u0435\u043a\u0430\u0442\u044c \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0438 \u0438\u0437 \u0442\u0435\u043a\u0441\u0442\u0430, \u043f\u0440\u0430\u0432\u0438\u043b\u044c\u043d\u044b\u043c \u043e\u0431\u0440\u0430\u0437\u043e\u043c \u043a\u043e\u043d\u043a\u0430\u0442\u0435\u043d\u0438\u0440\u043e\u0432\u0430\u0442\u044c \u0438\u0442\u0438 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0438 \u0432 \u043e\u0434\u0438\u043d \u043e\u0431\u0449\u0438\u0439 \u0432\u0435\u043a\u0442\u043e\u0440 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432, \u043f\u043e \u043a\u043e\u0442\u043e\u0440\u043e\u043c\u0443 \u043c\u044b \u043d\u0430\u0443\u0447\u0438\u043c \u044d\u0442\u0443 \u0441\u0435\u0442\u044c \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u044b\u0432\u0430\u0442\u044c \u0441\u043b\u0435\u0434\u0443\u044e\u0449\u0435\u0435 \u0441\u043b\u043e\u0432\u043e.\n\u041f\u043e\u0434\u0440\u043e\u0431\u043d\u0435\u0435 \u043d\u0430 \u043a\u0430\u0440\u0442\u0438\u043d\u043a\u0435 \u043d\u0438\u0436e:\n![BigModel.PNG](attachment:0d2eab61-c28c-4297-ace9-1fbac25c6c2a.PNG)","d0276622":" - \u0427\u0442\u043e\u0431\u044b \u0431\u043e\u043b\u044c\u0448\u0435 \u0443\u0437\u043d\u0430\u0442\u044c \u043f\u0440\u043e \u0444\u0443\u043d\u043a\u0446\u0438\u044e `collate_fn`, \u043f\u0435\u0440\u0435\u0439\u0434\u0438\u0442\u0435 \u043f\u043e \u044d\u0442\u043e\u0439 [\u0441\u0441\u044b\u043b\u043a\u0435](https:\/\/pytorch.org\/docs\/stable\/data.html#dataloader-collate-fn)\n - \u0412 \u0434\u0430\u043d\u043d\u043e\u0439 \u0444\u0443\u043d\u043a\u0446\u0438\u0438, \u043d\u0430\u043c \u043d\u0430 \u0432\u0445\u043e\u0434 \u043f\u0440\u0438\u0434\u0451\u0442 \u0441\u043f\u0438\u0441\u043e\u043a \u043e\u0431\u044a\u0435\u043a\u0442\u043e\u0432, \u043a\u043e\u0442\u043e\u0440\u044b\u0439 \u0431\u044b\u043b \u043f\u043e\u043b\u0443\u0447\u0435\u043c \u0432\u044b\u0437\u043e\u0432\u043e\u043c __getitem__ \u0443 \u043d\u0430\u0448\u0435\u0433\u043e \u0434\u0430\u0442\u0430\u0441\u0435\u0442\u0430, \u0442\u043e-\u0435\u0441\u0442\u044c `batch = [our_dataset[i] for i in range(batch_size)]`\n - \u0422\u0430\u043a \u043a\u0430\u043a \u0443 \u043d\u0430\u0441 \u043a\u0430\u0440\u0442\u0438\u043d\u043a\u0438 \u043e\u0434\u043d\u043e\u0433\u043e \u0440\u0430\u0437\u043c\u0435\u0440\u0430, \u043c\u044b \u043c\u043e\u0436\u0435\u043c \u0441\u043b\u043e\u0436\u0438\u0442\u044c \u0438\u0445 \u0432 \u043e\u0434\u0438\u043d \u0431\u043e\u043b\u044c\u0448\u043e\u0439 \u0442\u0435\u043d\u0437\u043e\u0440, \u043f\u0443\u0442\u0451\u043c \u043a\u043e\u043d\u043a\u0430\u0442\u0435\u043d\u0430\u0446\u0438\u0438 \u043f\u043e \u043d\u0443\u043b\u0435\u0432\u043e\u0439 \u043e\u0441\u0438, \u0442\u0435\u043a\u0441\u0442\u043e\u0432\u044b\u0435 \u0434\u0430\u043d\u043d\u044b\u0435 \u043d\u0435 \u043f\u043e\u043b\u0443\u0447\u0438\u0442\u0441\u044f \u043e\u0431\u044a\u0435\u0434\u0438\u043d\u0438\u0442\u044c \u0432 \u043e\u0434\u0438\u043d \u0442\u0435\u043d\u0437\u043e\u0440, \u0442\u0430\u043a \u043a\u0430\u043a \u0443 \u043d\u0438\u0445 \u0440\u0430\u0437\u043d\u044b\u0439 \u0440\u0430\u0437\u043c\u0435\u0440 \u0432\u043d\u0443\u0442\u0440\u0438 \u0431\u0430\u0442\u0447\u0430.","275cba4a":"### Resnet18 \u0432 \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u0435 \u0438\u0437\u0432\u043b\u043a\u0430\u0442\u0435\u043b\u044f \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432 \u0434\u043b\u044f \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u044f\n![](data:image\/png;base64,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)\n\n\u041d\u0435\u043e\u0431\u0445\u043e\u0434\u0438\u043c\u043e \u0432\u0437\u044f\u0442\u044c \u0443\u0436\u0435 \u043f\u0440\u0435\u0434\u043e\u0431\u0443\u0447\u0435\u043d\u043d\u0443\u044e \u043d\u0435\u0439\u0440\u043e\u043d\u043d\u0443\u044e \u0441\u0435\u0442\u044c, \u0443\u0434\u0430\u043b\u0438\u0432 \u0438\u0437 \u043d\u0435\u0451 \u043f\u043e\u0441\u043b\u0435\u0434\u043d\u0438\u0439 \u043a\u043b\u0430\u0441\u0441\u0438\u0444\u0438\u0446\u0438\u0440\u0443\u044e\u0449\u0438\u0439 \u0441\u043b\u043e\u0439, \u0442\u0430\u043a \u043a\u0430\u043a \u043d\u0430\u043c \u043d\u0443\u0436\u043d\u044b \u0442\u043e\u043b\u044c\u043a\u043e \u0432\u044b\u0441\u043e\u043a\u043e\u0443\u0440\u043e\u0432\u043d\u0435\u0432\u044b\u0435 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0438 \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u044f\n- \u0412 \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u0435 baseline, \u043c\u043e\u0436\u043d\u043e \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c `resnet18` \u0438\u0437 `torchvision.models`, \u043f\u043e\u0434\u0440\u043e\u0431\u043d\u0435\u0435 \u0441 \u043c\u043e\u0434\u0435\u043b\u044c\u044e \u043c\u043e\u0436\u043d\u043e \u043e\u0437\u043d\u0430\u043a\u043e\u043c\u0438\u0442\u044c\u0441\u044f \u043f\u0440\u043e\u0447\u0438\u0442\u0430\u0432 \u043e\u0440\u0438\u0433\u0438\u043d\u0430\u043b\u044c\u043d\u0443\u044e [\u0441\u0442\u0430\u0442\u044c\u044e](https:\/\/arxiv.org\/abs\/1512.03385)\n- \u041f\u043e\u0434\u0440\u043e\u0431\u043d\u0435\u0435 \u043e \u0442\u043e\u043c \u043a\u0430\u043a \u0434\u043e\u043e\u0431\u0443\u0447\u0430\u0442\u044c \u043c\u043e\u0434\u0435\u043b\u0438 \u0438\u0437 `torchvision` \u043c\u043e\u0436\u043d\u043e \u043f\u0440\u043e\u0447\u0438\u0442\u0430\u0442\u044c [\u0437\u0434\u0435\u0441\u044c](https:\/\/pytorch.org\/tutorials\/beginner\/finetuning_torchvision_models_tutorial.html)","5c97be80":"- \u041f\u043e\u043b\u0443\u0447\u0430\u0435\u0442\u0441\u044f, \u0447\u0442\u043e \u043d\u0430\u0448\u0438 \u0434\u0430\u043d\u043d\u044b\u0435 \u0431\u0443\u0434\u0443\u0442 \u043f\u043e\u0434\u0430\u0432\u0430\u0442\u044c\u0441\u044f \u043f\u043e\u0440\u0446\u0438\u044f\u043c\u0438 (\u0431\u0430\u0442\u0447\u0430\u043c\u0438) \u0432 \u0441\u043b\u0435\u0434\u0443\u044e\u0449\u0435\u043c \u0432\u0438\u0434\u0435:\n- `img_batch` - \u0442\u0435\u043d\u0437\u043e\u0440 \u043a\u0430\u0440\u0442\u0438\u043d\u043e\u043a, \u0440\u0430\u0437\u043c\u0435\u0440\u043d\u043e\u0441\u0442\u044c\u044e `[batch_size, 3, h, w]`\n- `in_seqs_batch` - \u0441\u043f\u0438\u0441\u043e\u043a \u0438\u0437 `batch_size` \u0442\u0435\u043d\u0437\u043e\u0440\u043e\u0432, \u043a\u0430\u0436\u0434\u044b\u0439 \u0442\u0435\u043d\u0437\u043e\u0440 \u044d\u0442\u043e \u043c\u0430\u0442\u0440\u0438\u0446\u0430 \u0440\u0430\u0437\u043c\u0435\u0440\u043e\u043c`[N, max_seq_len]`\n- `out_words_batch` - \u0441\u043f\u0438\u0441\u043e\u043a \u0438\u0437`batch_size` \u0442\u0435\u043d\u0437\u043e\u0440\u043e\u0432, \u043a\u0430\u0436\u0434\u044b\u0439 \u0442\u0435\u043d\u0437\u043e\u0440 \u044d\u0442\u043e \u043f\u043e \u0441\u0443\u0442\u0438 \u043c\u0430\u0441\u0441\u0438\u0432 \u0440\u0430\u0437\u043c\u0435\u0440\u043e\u043c`[N]`","59374359":"### \u0412\u0438\u0437\u0443\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u044f \u0430\u0443\u0433\u043c\u0435\u043d\u0442\u0430\u0446\u0438\u0439 \u0434\u043b\u044f \u043e\u0434\u043d\u043e\u0439 \u043a\u0430\u0440\u0442\u0438\u043d\u043a\u0438 (\u0447\u0442\u043e\u0431\u044b \u043f\u0440\u043e\u0432\u0435\u0440\u0438\u0442\u044c \u0441\u0435\u0431\u044f)","3c1c9383":"### \u0414\u043e\u043c\u0430\u0448\u043d\u0435\u0435 \u0437\u0430\u0434\u0430\u043d\u0438\u0435\n\u0412 \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u0435 \u0434\u043e\u043c\u0430\u0448\u043d\u0435\u0433\u043e \u0437\u0430\u0434\u0430\u043d\u0438\u044f \u043f\u0440\u0435\u0434\u043b\u0430\u0433\u0430\u0435\u0442\u0441\u044f \u0440\u0435\u0430\u043b\u0438\u0437\u043e\u0432\u0430\u0442\u044c \u0444\u0443\u043d\u043a\u0446\u0438\u044e `generate`. \n\n\u0424\u0443\u043d\u043a\u0446\u0438\u044f \u0433\u0435\u043d\u0435\u0440\u0430\u0446\u0438\u0438 \u0440\u0430\u0431\u043e\u0442\u0430\u0435\u0442 \u0441\u043b\u0435\u0434\u0443\u044e\u0449\u0438\u043c \u043e\u0431\u0440\u0430\u0437\u043e\u043c:\n- \u041d\u0430 \u0432\u0445\u043e\u0434 \u043f\u043e\u0434\u0430\u0451\u0442\u0441\u044f \u043a\u0430\u0440\u0442\u0438\u043d\u043a\u0430 \u0438 \u043c\u043e\u0434\u0435\u043b\u044c\n- \u041a\u0430\u0440\u0442\u0438\u043d\u043a\u0430 \u043f\u0440\u0435\u043e\u0431\u0440\u0430\u0437\u0443\u0435\u0442\u0441\u044f \u0432 \u043f\u0440\u0438\u0432\u044b\u0447\u043d\u044b\u0439 \u0434\u043b\u044f \u043c\u043e\u0434\u0435\u043b\u0438 \u0432\u0438\u0434 (\u0447\u0435\u0440\u0435\u0437 `image_prepare_val`)\n- \u0412 \u0441\u0430\u043c\u043e\u043c \u043d\u0430\u0447\u0430\u043b\u0435, \u0443 \u043d\u0430\u0441 \u043d\u0435\u0442\u0443 \u0432\u0445\u043e\u0434\u043d\u043e\u0439 \u043f\u043e\u0441\u043b\u0435\u0434\u043e\u0432\u0430\u0442\u0435\u043b\u044c\u043d\u043e\u0441\u0442\u0438 \u0442\u043e\u043a\u0435\u043d\u043e\u0432, \u0437\u0430\u043f\u043e\u043b\u043d\u0438\u043c \u0435\u0451 \u0441\u043b\u0435\u0434\u0443\u044e\u0449\u0438\u043c \u043e\u0431\u0440\u0430\u0437\u043e\u043c: \u044d\u0442\u043e \u0431\u0443\u0434\u0435\u0442 \u0442\u0435\u043d\u0437\u043e\u0440, \u0441\u043e\u0441\u0442\u043e\u044f\u0449\u0438\u0439 \u0438\u0437 `tok_to_ind['<PAD>']` \u0440\u0430\u0437\u043c\u0435\u0440\u043e\u043c [`max_seq_len`], \u043d\u043e \u043d\u0430 \u043f\u0435\u0440\u0432\u043e\u043c \u043c\u0435\u0441\u0442\u0435 \u0431\u0443\u0434\u0435\u0442 \u0438\u043d\u0434\u0435\u043a\u0441 `<BOS>` \u0442\u043e\u043a\u0435\u043d\u0430\n- \u041f\u043e\u0434\u0430\u0434\u0438\u043c \u043d\u0430 \u0432\u0445\u043e\u0434 \u0431\u0430\u0442\u0447 \u0441\u043e\u0441\u0442\u043e\u044f\u0449\u0438\u0439 \u0438\u0437 \u043e\u0434\u043d\u043e\u0439 \u043a\u0430\u043d\u0442\u0438\u043d\u043a\u0438, \u0438 \u0431\u0430\u0442\u0447 \u0441\u043e\u0441\u0442\u043e\u044f\u0449\u0438\u0439 \u0438\u0437 \u043e\u0434\u043d\u043e\u0439 \u0432\u0445\u043e\u0434\u043d\u043e\u0439 \u043f\u043e\u0441\u043b\u0435\u0434\u043e\u0432\u0430\u0442\u0435\u043b\u044c\u043d\u043e\u0441\u0442\u0438 \u0442\u043e\u043a\u0435\u043d\u043e\u0432\n- \u041f\u043e\u043b\u0443\u0447\u0438\u043c \u0441\u043a\u043e\u0440\u044b \u0434\u043b\u044f \u0441\u043b\u0435\u0434\u0443\u044e\u0449\u0435\u0433\u043e \u0442\u043e\u043a\u0435\u043d\u0430, \u043f\u043e \u043d\u0438\u043c \u0432\u044b\u0431\u0435\u0440\u0435\u043c \u0441\u043b\u0435\u0434\u0443\u044e\u0449\u0438\u0439 \u0442\u043e\u043a\u0435\u043d (\u0442\u0443\u0442 \u043d\u0435\u043e\u0431\u0445\u043e\u0434\u0438\u043c\u043e \u043e\u043f\u0440\u0435\u0434\u0435\u043b\u0438\u0442\u044c\u0441\u044f \u0441\u043e \u0441\u0442\u0440\u0430\u0442\u0435\u0433\u0438\u0435\u0439 \u0438\u043b\u0438 \u0438\u0437\u043d\u0430\u0447\u0430\u043b\u044c\u043d\u043e\u0439 [\u0442\u0430\u043a\u0442\u0438\u043a\u043e\u0439](https:\/\/www.youtube.com\/watch?v=ty-ymKiKXxo), \u043a\u043e\u0442\u043e\u0440\u043e\u0439 \u0441\u0442\u043e\u0438\u0442 \u043f\u0440\u0438\u0434\u0435\u0440\u0436\u0438\u0432\u0430\u0442\u044c\u0441\u044f =) )\n- \u0414\u043e\u0431\u0430\u0432\u0438\u043c \u0438\u043d\u0434\u0435\u043a\u0441 \u044d\u0442\u043e\u0433\u043e \u0442\u043e\u043a\u0435\u043d\u0430 \u0432\u043e \u0432\u043e\u0445\u043e\u0434\u043d\u0443\u044e \u043f\u043e\u0441\u043b\u0435\u0434\u043e\u0432\u0430\u0442\u0435\u043b\u044c\u043d\u043e\u0441\u0442\u044c \u0438\u043d\u0434\u0435\u043a\u0441\u043e\u0432, \u0442\u0430\u043a\u0438\u043c \u043e\u0431\u0440\u0430\u0437\u043e\u043c, \u043d\u0430 \u044d\u0442\u043e\u043c \u0448\u0430\u0433\u0435, \u043d\u0430\u0448\u0430 \u043f\u043e\u0441\u043b\u0435\u0434\u043e\u0432\u0430\u0442\u0435\u043b\u044c\u043d\u043e\u0441\u0442\u044c \u0438\u043c\u0435\u0435\u0442 \u0441\u043b\u0435\u0434\u0443\u044e\u0449\u0438\u0439 \u0432\u0438\u0434 - `[tok_to_ind[<BOS>], tok_to_ind[next_token], tok_to_ind[<PAD>], tok_to_ind[<PAD>], ...., tok_to_ind[<PAD>] ]`\n- \u0411\u0443\u0434\u0435\u043c \u043f\u043e\u0432\u0442\u043e\u0440\u044f\u0442\u044c \u044d\u0442\u043e\u0442 \u043f\u0440\u043e\u0446\u0435\u0441\u0441 \u043f\u043e\u043a\u0430 `next_token` != `<EOS>` \u0438\u043b\u0438 \u043f\u043e\u043a\u0430 \u043c\u044b \u043d\u0435 \u043f\u0440\u0438\u0432\u044b\u0441\u0438\u043b\u0438 `max_seq_len` \u0448\u0430\u0433\u043e\u0432\n- \u0412\u0435\u0440\u043d\u0451\u043c \u0441\u0442\u0440\u043e\u043a\u0443 \u0438\u0437 \u0442\u043e\u043a\u0435\u043d\u043e\u0432 - \u043d\u0430\u0448 \u043e\u0442\u0432\u0435\u0442 \u0434\u043b\u044f \u0434\u0430\u043d\u043d\u043e\u0439 \u043a\u0430\u0440\u0442\u0438\u043d\u043a\u0438\n\n**NOTE 1:** \u0421\u0442\u0440\u0430\u0442\u0435\u0433\u0438\u044f, \u043f\u0440\u0438 \u043a\u043e\u0442\u043e\u0440\u043e\u0439 \u0432\u044b\u0431\u0438\u0440\u0430\u0435\u0442\u0441\u044f \u0441\u0430\u043c\u044b\u0439 \u0432\u0435\u0440\u043e\u044f\u0442\u043d\u044b\u0439 \u0441\u043b\u0435\u0434\u0443\u044e\u0449\u0438\u0439 \u0442\u043e\u043a\u0435\u043d \u043d\u0435 \u0433\u0430\u0440\u0430\u043d\u0442\u0438\u0440\u0443\u0435\u0442 \u0441\u0430\u043c\u044b\u0439 \u043b\u0443\u0447\u0448\u0438\u0439 \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442 \u0433\u0435\u043d\u0435\u0440\u0430\u0446\u0438\u0438. \u0412 \u0438\u0434\u0435\u0430\u043b\u0435, \u043d\u0435\u043e\u0431\u0445\u043e\u0434\u0438\u043c\u043e \u0440\u0430\u0441\u0441\u043c\u043e\u0442\u0440\u0435\u0442\u044c \u0432\u0441\u0435 \u0432\u043e\u0437\u043c\u043e\u0436\u043d\u044b\u0435 \u043f\u043e\u0441\u043b\u0435\u0434\u043e\u0432\u0430\u0442\u0435\u043b\u044c\u043d\u043e\u0441\u0442\u0438, \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u043c\u044b \u043c\u043e\u0436\u0435\u043c \u0441\u0433\u0435\u043d\u0435\u0440\u0438\u0440\u043e\u0432\u0430\u0442\u044c \u0438 \u0432\u044b\u0431\u0440\u0430\u0442\u044c \u0441\u0430\u043c\u0443\u044e \u0432\u0435\u0440\u043e\u044f\u0442\u043d\u0443\u044e (\u043f\u0440\u0430\u0432\u0434\u043e\u043f\u043e\u0434\u043e\u0431\u043d\u0443\u044e), \u043d\u043e \u0442\u0430\u043a\u0438\u0445 \u043f\u043e\u0441\u043b\u0435\u0434\u043e\u0432\u0430\u0442\u0435\u043b\u044c\u043d\u043e\u0441\u0442\u0435\u0439 \u0432\u0441\u0435\u0433\u043e - $max\\_seq\\_len^{vocab\\_size}$ \u0448\u0442\u0443\u043a. \u042d\u0442\u043e \u043e\u0447\u0435\u043d\u044c \u043c\u043d\u043e\u0433\u043e, \u043f\u043e\u044d\u0442\u043e\u043c\u0443 \u043c\u043e\u0436\u043d\u043e \u0432\u043e\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c\u0441\u044f \u0430\u043b\u0433\u043e\u0440\u0438\u0442\u043c\u043e\u043c `Beam search`, \u043f\u043e\u0434\u0440\u043e\u0431\u043d\u0435\u0435 \u043c\u043e\u0436\u043d\u043e \u043f\u0440\u043e\u0447\u0438\u0442\u0430\u0442\u044c, \u043d\u0430\u043f\u0440\u0438\u043c\u0435\u0440 \u043d\u0430 [wiki](https:\/\/en.wikipedia.org\/wiki\/Beam_search), \u0430 \u043f\u0440\u0438\u043c\u0438\u043d\u0438\u0442\u0435\u043b\u044c\u043d\u043e \u043a NLP - [\u0437\u0434\u0435\u0441\u044c](https:\/\/towardsdatascience.com\/foundations-of-nlp-explained-visually-beam-search-how-it-works-1586b9849a24)\n\n**NOTE 2:** \u0415\u0441\u043b\u0438 \u0443 \u0432\u0430\u0441 \u0447\u0442\u043e \u0442\u043e \u043d\u0435 \u043f\u043e\u043b\u0443\u0447\u0430\u0435\u0442\u0441\u044f, \u043d\u0435\u043e\u0431\u0445\u043e\u0434\u0438\u043c\u043e \u0438\u0437\u0443\u0447\u0438\u0442\u044c \u0432\u044b\u0448\u0435\u0441\u0442\u043e\u044f\u0449\u0438\u0439 \u043a\u043e\u0434 \u0435\u0449\u0451 \u0440\u0430\u0437, \u0431\u043e\u043b\u0435\u0435 \u043f\u043e\u0434\u0440\u043e\u0431\u043d\u043e =) \u0415\u0441\u043b\u0438 \u0441\u043e\u0432\u0441\u0435\u043c \u043d\u0438\u0447\u0435\u0433\u043e \u043d\u0435 [\u043f\u043e\u043b\u0443\u0447\u0430\u0435\u0442\u0441\u044f](https:\/\/www.youtube.com\/watch?v=dQw4w9WgXcQ).","95652f16":"### \u041e\u0446\u0435\u043d\u043a\u0430 \u0433\u043e\u0442\u043e\u0432\u043e\u0439 \u043c\u043e\u0434\u0435\u043b\u0438\n\n\u041e\u0446\u0435\u043d\u0438\u0442\u044c \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u043e \u043f\u043e\u043b\u0443\u0447\u0435\u043d\u043d\u043e\u0439 \u043c\u043e\u0434\u0435\u043b\u0438 \u043c\u043e\u0436\u043d\u043e \u0441 \u043f\u043e\u043c\u043e\u0448\u044c\u044e [BLUE](https:\/\/en.wikipedia.org\/wiki\/BLEU) \u043c\u0435\u0442\u0440\u0438\u043a\u0438, \u0447\u0435\u043c \u043e\u043d\u0430 \u0432\u044b\u0448\u0435, \u0442\u0435\u043c \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u0435\u043d\u043d\u0435 \u043c\u043e\u0434\u0435\u043b\u044c \u0441\u043f\u0440\u0430\u0432\u043b\u044f\u0435\u0442\u0441\u044f \u0441 \u043f\u043e\u0441\u0442\u0430\u0432\u043b\u0435\u043d\u043d\u043e\u0439 \u0437\u0430\u0434\u0430\u0447\u0435\u0439. ","757c0351":"### \u041f\u043e\u0434\u0441\u0447\u0451\u0442 \u0441\u0440\u0435\u0434\u043d\u0435\u0433\u043e \u0438 \u0434\u0438\u0441\u043f\u0435\u0440\u0441\u0438\u0438 \u043f\u043e \u043a\u0430\u0440\u0442\u0438\u043d\u043a\u0430\u043c (\u0434\u043b\u044f \u043a\u0430\u0436\u0434\u043e\u0433\u043e \u043a\u0430\u043d\u0430\u043b\u0430 \u043e\u0442\u0434\u0435\u043b\u044c\u043d\u043e)\n\u0414\u0430\u043d\u043d\u044b\u0435 \u043a\u043e\u043d\u0441\u0442\u0430\u043d\u0442\u044b \u0431\u0443\u0434\u0443\u0442 \u043d\u0443\u0436\u043d\u044b \u0432 \u0434\u043b\u044f \u0444\u0443\u043d\u043a\u0438\u0438, \u043a\u043e\u0442\u043e\u0440\u0430\u044f \u043d\u043e\u0440\u043c\u0438\u0440\u0443\u0435\u0442 \u043a\u0430\u0440\u0442\u0438\u043d\u043a\u0438 - https:\/\/pytorch.org\/vision\/stable\/transforms.html?id=torchvision.transforms.Normalize","cad276ae":"### \u0410\u0443\u0433\u043c\u0435\u043d\u0442\u0430\u0446\u0438\u044f \u0434\u0430\u043d\u043d\u044b\u0445\n\n\u041f\u043e \u0441\u043a\u043e\u043b\u044c\u043a\u0443 \u043c\u044b \u043d\u0435 \u0431\u0443\u0434\u0435\u043c \u0437\u0430\u043c\u043e\u0440\u0430\u0436\u0438\u0432\u0430\u0442\u044c \u0432\u0435\u0441\u0430 \u043d\u0430\u0448\u0435\u0439 \u043f\u0440\u0435\u0434-\u043e\u0431\u0443\u0447\u0435\u043d\u043d\u043e\u0439 \u043c\u043e\u0434\u0435\u043b\u0438, \u043a\u043e\u0442\u043e\u0440\u0430\u044f \u0438\u0437\u0432\u043b\u0435\u043a\u0430\u0435\u0442 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0438, \u0441\u0442\u043e\u0438\u0442 \u0432\u043e\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c\u0441\u044f \u0430\u0443\u0433\u043c\u0435\u043d\u0442\u0430\u0446\u0438\u0435\u0439 \u0434\u0430\u043d\u043d\u044b\u0445 \u043a\u0430\u0440\u0442\u0438\u043d\u043e\u043a\n\n\u041f\u0440\u043e \u0438\u0434\u0435\u044e \u0430\u0443\u0433\u043c\u0435\u043d\u0442\u0430\u0446\u0438\u0438 \u0434\u0430\u043d\u043d\u044b\u0445 \u043d\u0430 \u043f\u0440\u0438\u043c\u0435\u0440\u0435 \u043f\u0435\u0447\u0430\u0442\u043d\u044b\u0445 \u0441\u0438\u043c\u0432\u043e\u043b\u043e\u043c \u043c\u043e\u0436\u043d\u043e \u043f\u0440\u043e\u0447\u0438\u0442\u0430\u0442\u044c \u0437\u0434\u0435\u0441\u044c - https:\/\/habr.com\/ru\/company\/smartengines\/blog\/264677\/\n\n\u0422\u0440\u0430\u043d\u0441\u0444\u043e\u0440\u043c\u0430\u0446\u0438\u0438 \u0432\u044b \u043c\u043e\u0436\u0435\u0442\u0435 \u043d\u0430\u0439\u0442\u0438 \u0437\u0434\u0435\u0441\u044c - https:\/\/pytorch.org\/vision\/stable\/transforms.html\n\n\u041f\u0440\u043e `Compose` \u0442\u0430\u043a \u0436\u0435 \u043c\u043e\u0436\u043d\u043e \u043f\u0440\u043e\u0447\u0438\u0442\u0430\u0442\u044c \u043f\u043e \u044d\u0442\u043e\u0439 \u0436\u0435 \u0441\u0441\u044b\u043b\u043a\u0435.","55827420":"![RNN.PNG](attachment:5a107069-8260-4272-96e0-d7a153339c90.PNG)","f77a96aa":"# Image captioning notebook\n\u041d\u043e\u0443\u0442\u0431\u0443\u043a \u0441\u043e\u0434\u0435\u0440\u0436\u0438\u0442:\n- \u041d\u0435\u0431\u043e\u043b\u044c\u0448\u043e\u0439 \u043e\u0431\u0437\u043e\u0440 \u0434\u0430\u0442\u0430\u0441\u0435\u0442\u0430\n- \u041e\u0431\u0440\u0430\u0431\u043e\u0442\u043a\u0430 \u0434\u0430\u043d\u043d\u044b\u0445 (\u0442\u043e\u043a\u0435\u043d\u0438\u0437\u0430\u0446\u0438\u044f \u0438 \u0442\u0434)\n- \u041e\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u0435 \u043d\u0435\u0439\u0440\u043e\u043d\u043d\u044b\u0439 \u043c\u043e\u0434\u0435\u043b\u0435\u0439\n- \u0422\u0440\u0435\u043d\u0438\u0440\u043e\u0432\u043a\u0430 \u0441\u0435\u0442\u0438\n- \u0413\u0435\u043d\u0435\u0440\u0430\u0446\u0438\u044f \u043f\u043e\u0434\u043f\u0438\u0441\u0435\u0439 (\u0432\u043a\u043b\u044e\u0447\u0430\u044f \u0434\u043e\u043c\u0430\u0448\u043d\u0435\u0435 \u0437\u0430\u0434\u0430\u043d\u0438\u0435)\n- \u0412\u044b\u0447\u0438\u0441\u043b\u0435\u043d\u0438\u0435 \u043c\u0435\u0442\u0440\u0438\u043a\u0438 \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u0430 \u0433\u0435\u043d\u0435\u0440\u0430\u0446\u0438\u0438.","c7998c17":"## \u041e\u0431\u0437\u043e\u0440 \u0434\u0430\u0442\u0430\u0441\u0435\u0442\u0430","5f26fa9b":"### \u0420\u0430\u0431\u043e\u0442\u0430 \u0441 \u043f\u043e\u0434\u043f\u0438\u0441\u044f\u043c\u0438","113f1fc2":"### \u041f\u043e\u0441\u043c\u043e\u0442\u0440\u0438\u043c \u043d\u0430 \u0442\u043e, \u043a\u0430\u043a \u043d\u0430\u0448\u0430 \u043c\u043e\u0434\u0435\u043b\u044c \u0441\u043f\u0440\u0430\u0432\u043b\u044f\u0435\u0442\u0441\u044f \u0441 \u0437\u0430\u0434\u0430\u0447\u0435\u0439:","67b95d2f":"### \u0420\u0435\u043a\u0443\u0440\u0435\u043d\u0442\u043d\u0430\u044f \u043d\u0435\u0439\u0440\u043e\u043d\u043d\u0430\u044f \u0441\u0435\u0442\u044c \u0434\u043b\u044f \u043e\u0431\u0440\u0430\u0431\u043e\u0442\u043a\u0438 \u043f\u043e\u0441\u043b\u0435\u0434\u043e\u0432\u0430\u0442\u0435\u043b\u044c\u043d\u043e\u0441\u0442\u0435\u0439\n\n","21ebbadc":"#### \u0417\u0430\u043f\u0443\u0441\u043a\u0430\u0435\u043c \u0442\u0440\u0435\u043d\u0438\u0440\u043e\u0432\u043a\u0443","a87e9f65":"### \u0422\u0440\u0435\u043d\u0438\u0440\u043e\u0432\u043a\u0430 \u043c\u043e\u0434\u0435\u043b\u0438\n\u0422\u0435\u043f\u0435\u0440\u044c \u0443 \u043d\u0430\u0441 \u0432\u0441\u0451 \u0433\u043e\u0442\u043e\u0432\u043e \u043a \u0437\u0430\u043f\u0443\u0441\u043a\u0443 \u0442\u0440\u0435\u043d\u0438\u0440\u043e\u0432\u043a\u0438","a8c78c35":"- \u0421\u043a\u0430\u0447\u0430\u0435\u043c \u043f\u0440\u0435\u0434\u043e\u0431\u0443\u0447\u0435\u043d\u043d\u044b\u0435 \u044d\u043c\u0431\u0435\u0434\u0438\u043d\u0433\u0438 \u0434\u043b\u044f \u0441\u043b\u043e\u0432 \u0430\u043d\u0433\u043b\u0438\u0439\u0441\u043a\u043e\u0433\u043e \u044f\u0437\u044b\u043a\u0430","32065800":"### \u041f\u0440\u0435\u0434\u043e\u0431\u0443\u0447\u0435\u043d\u043d\u044b\u0435 \u044d\u043c\u0431\u0435\u0434\u0438\u043d\u0433\u0438 \u0441\u043b\u043e\u0432","653dcc22":"### \u041a\u043b\u0430\u0441\u0441 \u0434\u0430\u0442\u0430\u0441\u0435\u0442\u0430\n\n\u0412\u044b \u043c\u043e\u0436\u0435\u0442\u0435 \u043e\u0437\u043d\u0430\u043a\u043e\u043c\u0438\u0442\u044c\u0441\u044f \u0441 \u0442\u0435\u043c, \u0447\u0442\u043e \u043d\u0443\u0436\u043d\u043e \u0441\u0434\u0435\u043b\u0430\u0442\u044c, \u0447\u0442\u043e\u0431\u044b \u043e\u043f\u0438\u0441\u0430\u0442\u044c \u0434\u0430\u043d\u043d\u044b\u0439 \u043a\u043b\u0430\u0441\u0441 \u043f\u043e \u044d\u0442\u043e\u0439 \u0441\u0441\u043b\u044b\u043a\u0435 - https:\/\/pytorch.org\/tutorials\/beginner\/data_loading_tutorial.html\n\n\u041f\u043e \u0441\u0443\u0442\u0438, \u043d\u0435\u043e\u0431\u0445\u043e\u0434\u0438\u043c\u043e \u043d\u0430\u043f\u0438\u0441\u0430\u0442\u044c \u0442\u0440\u0438 \u043c\u0435\u0442\u043e\u0434\u0430 \u043a\u043b\u0430\u0441\u0441\u0430, \u0430 \u0438\u043c\u0435\u043d\u043d\u043e `__init__`, `__getitem__` \u0438 `__len__`, \u0443\u043d\u0430\u0441\u043b\u0435\u0434\u043e\u0432\u0430\u0432\u0448\u0438\u0441\u044c \u043e\u0442 `Dataset` \u043a\u043b\u0430\u0441\u0441\u0430, \u043a\u043e\u0442\u043e\u0440\u044b\u0439 \u043c\u044b \u0438\u043c\u043f\u043e\u0440\u0442\u0438\u0440\u043e\u0432\u0430\u043b\u0438 \u0440\u0430\u043d\u0435\u0435\n\n\u0421\u0430\u043c\u043e\u0435 \u0438\u043d\u0442\u0435\u0440\u0435\u0441\u043d\u043e\u0435 \u043f\u0440\u043e\u0438\u0441\u0445\u043e\u0434\u0438\u0442 \u0432 \u043c\u0435\u0442\u043e\u0434\u0435 `__getitem__`, \u0432 \u043a\u043e\u0442\u043e\u0440\u043e\u043c, \u043f\u043e \u0438\u043d\u0434\u0435\u043a\u0441\u0443 \u043d\u0435\u043e\u0431\u0445\u043e\u0434\u0438\u043c\u043e \u0432\u0435\u0440\u043d\u0443\u0442\u044c \u0442\u0440\u0438 \u043e\u0431\u044a\u0435\u043a\u0442\u0430:\n\n1. \u0422\u0440\u0430\u0441\u0444\u043e\u0440\u043c\u0438\u0440\u043e\u0432\u0430\u043d\u043d\u0443\u044e (\u0440\u0435\u0441\u0430\u0439\u0437 + \u0430\u0443\u0433\u043c\u0435\u043d\u0442\u0430\u0446\u0438\u044f + \u043f\u0435\u0440\u0435\u0432\u043e\u0434 \u0432 \u0442\u0435\u043d\u0437\u043e\u0440 + \u043d\u043e\u0440\u043c\u0438\u0440\u043e\u0432\u043a\u0430) **\u043a\u0430\u0440\u0442\u0438\u043d\u043a\u0443**,\n\n2. \u0414\u0430\u043b\u0435\u0435, \u0436\u0434\u044f \u044d\u0442\u043e\u0439 \u043a\u0430\u0440\u0442\u0438\u043d\u043a\u0438, \u0441\u043b\u0443\u0447\u0430\u0439\u043d\u043e, \u0432\u044b\u0431\u0438\u0440\u0430\u0435\u0442\u0441\u044f \u043e\u0434\u043d\u0430 \u0438\u0437 5 \u043f\u043e\u0434\u043f\u0438\u0441\u0435\u0439, \u0442\u043e\u043a\u0435\u043d\u0435\u0437\u0438\u0440\u0443\u0435\u0442\u0441\u044f, \u043f\u0435\u0440\u0435\u0432\u043e\u0434\u0438\u0442\u0441\u044f \u0432 \u0438\u043d\u0434\u0435\u043a\u0441\u044b, \u043d\u0430\u043f\u0440\u0438\u043c\u0435\u0440, \u043c\u044b \u043f\u043e\u043b\u0443\u0447\u0438\u043b\u0438 [  1,   4,  41,  20,   5,   4,  90, 163, 313,  65, 2] \\\n\u041a\u0430\u043a\u0438\u0435 \u0432\u043e\u0437\u043c\u043e\u0436\u043d\u044b\u0435 \u043f\u0440\u0435\u0444\u0438\u043a\u0441\u044b \u043f\u0440\u0435\u0434\u043b\u043e\u0436\u0435\u043d\u0438\u0439 \u043c\u043e\u0433\u0443\u0442 \u0431\u044b\u0442\u044c \u0441\u043e\u0441\u0442\u0430\u0432\u043b\u0435\u043d\u044b \u0438\u0437 \u044d\u0442\u043e\u0439 \u043f\u043e\u0441\u043b\u0435\u0434\u043e\u0432\u0430\u0442\u0435\u043b\u044c\u043d\u043e\u0441\u0442\u0438? \u041d\u0430\u043f\u0440\u0438\u043c\u0435\u0440:\n\n[1, 3, 3, 3, ....., 3]\\\n[1, 4, 3, 3, ....., 3]\\\n[1, 4, 41, 3, ....., 3]\\\n...\\\n[1, 4, 41, 163, ....., 3]\\\n[1, 4, 41, 20, 5, 4, 90, 163, 313, 65, 2, 3, ..., 3]\\\n\n\u0417\u0434\u0435\u0441\u044c \u0438\u043d\u0434\u0435\u043a\u0441 3 \u0441\u043e\u043e\u0442\u0432\u0435\u0442\u0441\u0442\u0432\u0443\u0435\u0442 \u043f\u0430\u0434\u0434\u0438\u043d\u0433\u0443, \u0434\u043b\u0438\u043d\u043d\u0430 \u043a\u0430\u0436\u0434\u043e\u0433\u043e \u0442\u0430\u043a\u043e\u0433\u043e \u043f\u0440\u0435\u0444\u0438\u043a\u0441\u0430 \u0434\u043e\u043b\u0436\u043d\u0430 \u0431\u044b\u0442\u044c \u0440\u0430\u0432\u043d\u0430 `max_seq_len`, \u043e\u043f\u0440\u0435\u0434\u0435\u043b\u0451\u043d\u043d\u043e\u0439 \u0440\u0430\u043d\u0435\u0435 \u0432 \u043d\u043e\u0443\u0442\u0431\u0443\u043a\u0435\n\n\u0418\u043c\u0435\u043d\u043d\u043e \u0442\u0430\u043a\u043e\u0439 **\u0442\u0435\u043d\u0437\u043e\u0440**, \u0440\u0430\u0437\u043c\u0435\u0440\u043d\u043e\u0441\u0442\u0438 `[len(tokenize(caption)) - 1, max_seq_len]` \u043c\u044b \u0438 \u0434\u043e\u043b\u0436\u043d\u044b \u0432\u0435\u0440\u043d\u0443\u0442\u044c\n\n3. \u041a\u0430\u0436\u0434\u043e\u043c\u0443 \u0442\u0430\u043a\u043e\u043c\u0443 \u043f\u0440\u0435\u0444\u0438\u043a\u0441\u0443 \u0441\u043e\u043e\u0442\u0432\u0435\u0442\u0441\u0432\u0443\u0435\u0442 \u043d\u0435\u043a\u043e\u0442\u043e\u0440\u044b\u0439 \u0438\u043d\u0434\u0435\u043a\u0441 \u0442\u043e\u043a\u0435\u043d\u0430, \u043a\u043e\u0442\u043e\u0440\u044b\u0439 \u043c\u044b \u0445\u043e\u0434\u0438\u043c \u043d\u0430\u0443\u0447\u0438\u0442\u044c \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u044b\u0432\u0430\u0442\u044c, \u0441 \u043f\u043e\u043c\u043e\u0448\u044c\u044e \u043d\u0435\u0439\u0440\u043e\u043d\u043d\u043e\u0439 \u0441\u0435\u0442\u0438. \u042d\u0442\u043e\u0442 \u0438\u043d\u0434\u0435\u043a\u0441 \u0441\u043e\u043e\u0442\u0432\u0435\u0442\u0441\u0442\u0432\u0443\u0435\u0442 \u0441\u043b\u0435\u0434\u0443\u044e\u0449\u0435\u043c\u0443 \u0442\u043e\u043a\u0435\u043d\u0443, \u043a\u043e\u0442\u043e\u0440\u044b\u0439 \u0441\u043a\u0440\u044b\u0432\u0430\u0435\u0442\u0441\u044f \u0437\u0430 \u043f\u0435\u0440\u0432\u044b\u043c \u043f\u0430\u0434\u0434\u0438\u043d\u0433\u043e\u043c \u0432 \u0434\u0430\u043d\u043d\u043e\u043c\\\n\u043f\u0440\u0435\u0444\u0438\u043a\u0441\u0435, \u043d\u0430\u043f\u0440\u0438\u043c\u0435\u0440 \u0434\u043b\u044f \u043f\u043e\u0441\u043b\u0435\u0434\u043e\u0432\u0430\u0442\u0435\u043b\u044c\u043d\u043e\u0441\u0442\u0438 [1, 3, 3, 3, ....., 3] \u0441\u043b\u0435\u0434\u0443\u044e\u0449\u0438\u0439 \u0442\u043e\u043a\u0435\u043d, \u043a\u043e\u0442\u043e\u0440\u044b\u0439 \u043d\u0443\u0436\u043d\u043e \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u0442\u044c \u044d\u0442\u043e `4` (\u0441\u043c \u043d\u0430 \u0441\u0442\u0440\u043e\u0447\u043a\u0443 \u043d\u0438\u0436\u0435)\\\n\u0430 \u0434\u043b\u044f \u043f\u043e\u0441\u043b\u0435\u0434\u043e\u0432\u0430\u0442\u0435\u043b\u044c\u043d\u043e\u0441\u0442\u0438 [1, 4, 3, 3, ....., 3] - `41` \u0438 \u0442\u0430\u043a \u0434\u0430\u043b\u0435\u0435. \u0421\u043b\u043e\u0436\u0438\u043c \u044d\u0442\u0438 \u0438\u043d\u0434\u0435\u043a\u0441\u044b \u0432 \u043e\u0434\u043d\u043e\u043c\u0435\u0440\u043d\u044b\u0439 \u0442\u0435\u043d\u0437\u043e\u0440 (\u043c\u0430\u0441\u0441\u0438\u0432), \u044d\u0442\u043e\u0442 **\u0442\u0435\u043d\u0437\u043e\u0440** \u043d\u0430\u043c \u0438 \u043d\u0435\u043e\u0431\u0445\u043e\u0434\u0438\u043c\u043e \u0432\u0435\u0440\u043d\u0443\u0442\u044c (\u0442\u0435\u043d\u0437\u043e\u0440 \u0431\u0443\u0434\u0435\u0442 \u0438\u043c\u0435\u0442\u044c \u0440\u0430\u0437\u043c\u0435\u0440\u043d\u043e\u0441\u0442\u044c `[len(tokenize(caption)) - 1]`)"}}