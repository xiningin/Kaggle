{"cell_type":{"a15cf9d6":"code","202e2489":"code","47cfeed0":"code","2a4fe8dd":"code","79589c76":"code","cd3b42cc":"code","818bd394":"code","dd0059f4":"code","2c641d4f":"code","e12d3fa4":"code","481f0552":"code","b2969aa4":"code","20747261":"code","9d313a4c":"code","a363c776":"code","98e255fd":"code","47a2cf7b":"code","7a5645ca":"code","db2e0749":"code","bea46aad":"code","fe81929a":"code","dc62c2f1":"code","fe8437a7":"code","adedfc9b":"code","1c91a004":"code","17184d4d":"code","13ef4c34":"code","8ef0906b":"code","19c2a173":"code","5559d029":"code","bfc2112d":"code","4d60bf67":"code","7b9ce003":"code","6ce36284":"code","fb9078cb":"code","247b2a8b":"code","d21ccc1f":"code","85e0f951":"code","592938f5":"code","97c88e81":"code","b9992958":"code","0816797a":"code","97cad732":"code","b9286f52":"code","4becb593":"code","39fd2e5c":"markdown","a1bdede0":"markdown","0641bcba":"markdown","fde60acb":"markdown","f3bae4bd":"markdown","b2577e9c":"markdown","8fdca4a7":"markdown","3580b4f4":"markdown","50629d82":"markdown","0b6138ab":"markdown","9cb53f66":"markdown","f66aec57":"markdown","da430d23":"markdown","23e7562c":"markdown","f21b4ae0":"markdown","a4e0edc3":"markdown","445ccf89":"markdown","52e1874d":"markdown"},"source":{"a15cf9d6":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns; sns.set_theme(color_codes=True)\n%matplotlib inline\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler, OrdinalEncoder, OneHotEncoder\nfrom sklearn.feature_selection import mutual_info_regression\nfrom sklearn.metrics import mean_squared_error,mean_absolute_error,explained_variance_score, r2_score\n\nfrom xgboost import XGBRegressor\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","202e2489":"df_train = pd.read_csv('..\/input\/tabular-playground-series-jan-2022\/train.csv')\ndf_test = pd.read_csv('..\/input\/tabular-playground-series-jan-2022\/test.csv')\ndf_holidays = pd.read_csv('..\/input\/public-and-unofficial-holidays-nor-fin-swe-201519\/holidays.csv') #HOLIDAYS CALENDAR AVAILABLE HERE https:\/\/www.kaggle.com\/vpallares\/public-and-unofficial-holidays-nor-fin-swe-201519\ndf_oecd = pd.read_csv('..\/input\/oecd-data-fin-nor-swe-20152019\/oecd_monthly_data.csv') #ECONOMICS DATASET AVAILABLE HERE https:\/\/www.kaggle.com\/siukeitin\/oecd-data-fin-nor-swe-20152019\ndf_gdp = pd.read_csv('..\/input\/consumer-price-index-20152019-nordic-countries\/Best_CPI.csv')  #GDP DATASET https:\/\/www.kaggle.com\/sardorabdirayimov\/consumer-price-index-20152019-nordic-countries\n","47cfeed0":"df_train.isnull().sum() #there are no null values, so we don't have to clean the df","2a4fe8dd":"def set_date_features(df):\n    df['date'] = pd.to_datetime(df['date'])          \n    df['day_of_week']=df['date'].dt.dayofweek       \n    df['day_of_month']=df['date'].dt.day            \n    df['weekend']=(df['day_of_week']\/\/5 == 1)       \n    df['weekend']=df['weekend'].astype('int64')       \n    df['week']=df['date'].dt.isocalendar().week     \n    df['week'][df['week']>52]=52                    \n    df['week']=df['week'].astype('int64')             \n    df['month']=df['date'].dt.month                 \n    df['quarter']=df['date'].dt.quarter             \n    df['year']=df['date'].dt.year    \n    return df","79589c76":"df_train = set_date_features(df_train)\ndf_test = set_date_features(df_test)","cd3b42cc":"df_holidays['date'] = pd.to_datetime(df_holidays['date'])   ","818bd394":"def holiday_fe(row):    \n    df = df_holidays[(df_holidays['date'] == row['date']) & (df_holidays['country'] == row['country'])]\n    if len(df) > 0:\n        retval = df.iloc[0]['event']\n    else:\n        retval = 'None'\n    \n    #THIS FIXES A PROBLEM WITH THE HOLIDAY DATASET, IN 2019 THE NEW YEAR'S EVE IS MISSING!\n    if (retval == 'None') & (row['month'] == 12) & (row['day_of_month'] == 31):\n        retval = \"New Year's Eve\"\n        \n    return retval ","dd0059f4":"df_train['Holiday'] = df_train.apply(lambda row: holiday_fe(row), axis = 1)\ndf_test['Holiday'] = df_test.apply(lambda row: holiday_fe(row), axis = 1)","2c641d4f":"df_train['Holiday_purchase'] = df_train['Holiday'].apply(lambda holiday: 0 if holiday == 'None' else 1)\ndf_test['Holiday_purchase'] = df_test['Holiday'].apply(lambda holiday: 0 if holiday == 'None' else 1)","e12d3fa4":"df_oecd['year'] = df_oecd['date'].apply(lambda date: int(date.split('-')[0]))\ndf_oecd['month'] = df_oecd['date'].apply(lambda date: int(date.split('-')[1]))","481f0552":"df_oecd.drop('CCI', axis = 1, inplace = True) #THIS VALUES IS MISSING FOR NORWAY, WE DROP IT","b2969aa4":"def oecd_fe(df):\n    df_tmp = pd.merge(left=df, right=df_oecd, how='left', on=['year','month','country'])\n    df_tmp = df_tmp.drop('date_y', axis=1)\n    df_tmp.rename(columns={'date_x':'date'}, inplace=True)\n    #if BCI > 100 means good market\"confidence\"\n    #useful info here https:\/\/data.oecd.org\/leadind\/business-confidence-index-bci.htm\n    df_tmp['Confidence'] = df_tmp['BCI'].apply(lambda bci: 1 if bci > 100 else 0)\n    df_tmp['Confidence'] = df_tmp['Confidence'].astype('int64')  \n    return df_tmp","20747261":"df_train = oecd_fe(df_train)\ndf_test = oecd_fe(df_test)","9d313a4c":"df_train = df_train.merge(df_gdp[['year','country','GDP']], how='left', on=['year','country'])\ndf_test = df_test.merge(df_gdp[['year','country','GDP']], how='left', on=['year','country'])","a363c776":"g = sns.FacetGrid(df_train, \n                  col_wrap=2,\n                  col=\"year\", \n                  hue='country',\n                  height=5,\n                  aspect=2,\n                  sharex=True, \n                  xlim=(0, 13))\ng.map_dataframe(sns.lineplot, 'month', 'num_sold')\ng.add_legend()","98e255fd":"plt.figure(figsize=(12,6))\n\ndf_no = df_train[df_train['country'] == 'Norway']\ndf_fi = df_train[df_train['country'] == 'Finland']\ndf_se = df_train[df_train['country'] == 'Sweden']\n\nsns.kdeplot(x=df_no['num_sold'], label = 'Norway')\nsns.kdeplot(x=df_fi['num_sold'], label = 'Finland')\nsns.kdeplot(x=df_se['num_sold'], label = 'Sweden')\n\nplt.title = 'Normal num_sold values'\nplt.legend()","47a2cf7b":"plt.figure(figsize=(12,6))\n\ndf_no = df_train[df_train['country'] == 'Norway']\ndf_fi = df_train[df_train['country'] == 'Finland']\ndf_se = df_train[df_train['country'] == 'Sweden']\n\nsns.kdeplot(x=np.log(df_no['num_sold']), label = 'Norway')\nsns.kdeplot(x=np.log(df_fi['num_sold']), label = 'Finland')\nsns.kdeplot(x=np.log(df_se['num_sold']), label = 'Sweden')\n\n\nplt.legend()","7a5645ca":"sns.pairplot(data=df_train[['BCI','CLI','CPI', 'GDP', 'country']], \n             x_vars=['BCI','CLI','CPI', 'GDP'], \n             hue='country')","db2e0749":"sns.regplot(data=df_no, x='CLI', y='BCI')\nsns.regplot(data=df_fi, x='CLI', y='BCI')\nsns.regplot(data=df_se, x='CLI', y='BCI')","bea46aad":"sns.regplot(data=df_no, x='CPI', y='BCI')\nsns.regplot(data=df_fi, x='CPI', y='BCI')\nsns.regplot(data=df_se, x='CPI', y='BCI')","fe81929a":"df_train['CPI_BCI_ratio'] = df_train['CPI'] \/ df_train['BCI']\ndf_train['CLI_BCI_ratio'] = df_train['CLI'] \/ df_train['BCI']\ndf_test['CPI_BCI_ratio'] = df_test['CPI'] \/ df_test['BCI']\ndf_test['CLI_BCI_ratio'] = df_test['CLI'] \/ df_test['BCI']","dc62c2f1":"X = df_train.drop(['date','row_id','num_sold'],axis=1)\ny = np.log(df_train['num_sold']) #AS WE SAID, IT WOULD BE EASIER TO PREDICT THE LOGARITHM THEN THE REAL VALUE, \n                                 #BECAUSE OF THE OUTLINERS\nX_test = df_test.drop(['date','row_id'], axis = 1)\n\nX_train, X_valid, y_train, y_valid = train_test_split(X,y,test_size=0.3,random_state=35,shuffle=True)","fe8437a7":"object_cols = [col for col in df_train.columns if df_train[col].dtype == \"object\"]\nobject_cols","adedfc9b":"high_cardinality_cols = [col for col in object_cols if df_train[col].nunique() > 10]\n\nordinal_encoder = OrdinalEncoder()#handle_unknown='use_encoded_value', unknown_value = 100\nX_train_encoded = X_train.copy()\nX_valid_encoded = X_valid.copy()\nX_test_encoded = X_test.copy()\n\nX_train_encoded[high_cardinality_cols] = ordinal_encoder.fit_transform(X_train[high_cardinality_cols])\nX_valid_encoded[high_cardinality_cols] = ordinal_encoder.transform(X_valid[high_cardinality_cols])\nX_test_encoded[high_cardinality_cols] = ordinal_encoder.transform(X_test[high_cardinality_cols])\n\nX_train_encoded[high_cardinality_cols] = X_train_encoded[high_cardinality_cols].astype('int64')\nX_valid_encoded[high_cardinality_cols] = X_valid_encoded[high_cardinality_cols].astype('int64')\nX_test_encoded[high_cardinality_cols] = X_test_encoded[high_cardinality_cols].astype('int64')","1c91a004":"low_cardinality_cols = [col for col in object_cols if X_train_encoded[col].nunique() < 10]","17184d4d":"def to_int(oh_cols):\n    for col in oh_cols.columns:\n        oh_cols[col] = oh_cols[col].astype('int64')\n    \n    return oh_cols","13ef4c34":"def encode_low_card(feature):\n    OH_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\n    OH_cols_train = pd.DataFrame(OH_encoder.fit_transform(X_train_encoded[[feature]]))\n    OH_cols_valid = pd.DataFrame(OH_encoder.transform(X_valid_encoded[[feature]]))\n    OH_cols_test = pd.DataFrame(OH_encoder.transform(X_test_encoded[[feature]]))\n    \n    #col_names = X_train_encoded[feature].sort_values().unique().tolist()\n        \n    OH_cols_train.columns = OH_encoder.get_feature_names([feature])\n    OH_cols_valid.columns = OH_encoder.get_feature_names([feature])\n    OH_cols_test.columns = OH_encoder.get_feature_names([feature])\n    \n    OH_cols_train = to_int(OH_cols_train)\n    OH_cols_valid = to_int(OH_cols_valid)\n    OH_cols_valid = to_int(OH_cols_valid)\n    \n    # One-hot encoding removed index; put it back\n    OH_cols_train.index = X_train_encoded.index\n    OH_cols_valid.index = X_valid_encoded.index\n    OH_cols_test.index = X_test_encoded.index\n    \n    # Remove categorical columns (will replace with one-hot encoding)\n    X_train_encoded.drop([feature], axis=1, inplace = True)\n    X_valid_encoded.drop([feature], axis=1, inplace = True)\n    X_test_encoded.drop([feature], axis=1, inplace = True)\n\n    return (pd.concat([X_train_encoded, OH_cols_train], axis=1), \n           pd.concat([X_valid_encoded, OH_cols_valid], axis=1), \n           pd.concat([X_test_encoded, OH_cols_test], axis=1))","8ef0906b":"for feature in low_cardinality_cols:\n    X_train_encoded, X_valid_encoded, X_test_encoded = encode_low_card(feature)\n","19c2a173":"X_train_encoded.info()","5559d029":"def make_mi_scores(X, y):\n    mi_scores = mutual_info_regression(X, y)\n    mi_scores = pd.Series(mi_scores, name=\"MI Scores\", index=X.columns)\n    mi_scores = mi_scores.sort_values(ascending=False)\n    return mi_scores","bfc2112d":"mi_scores = make_mi_scores(X_train_encoded, y_train)\nmi_scores","4d60bf67":"def plot_mi_scores(scores):\n    scores = scores.sort_values(ascending=True)\n    width = np.arange(len(scores))\n    ticks = list(scores.index)\n    plt.barh(width, scores)\n    plt.yticks(width, ticks)\n\n\nplt.figure(dpi=100, figsize=(8, 5))\nplot_mi_scores(mi_scores)","7b9ce003":"X_train = X_train_encoded\nX_valid = X_valid_encoded\nX_test = X_test_encoded","6ce36284":"X_train.shape","fb9078cb":"X_valid.shape","247b2a8b":"model = XGBRegressor(n_estimators=600, learning_rate=0.05)\nmodel.fit(X_train, y_train, \n          early_stopping_rounds=5, \n          eval_set=[(X_valid, y_valid)], \n          verbose=False)","d21ccc1f":"predictions = model.predict(X_valid)\npredictions = np.squeeze(predictions)","85e0f951":"plt.scatter(y_valid,predictions)","592938f5":"mean_absolute_error(y_valid,predictions)","97c88e81":"np.sqrt(mean_squared_error(y_valid,predictions))","b9992958":"explained_variance_score(y_valid,predictions)","0816797a":"r2_score(y_valid,predictions)","97cad732":"def smape(a, f):\n    return 1\/len(a) * np.sum(2 * np.abs(f-a) \/ (np.abs(a) + np.abs(f))*100)","b9286f52":"smape(y_valid,predictions)","4becb593":"predictions = np.exp(model.predict(X_test))\npredictions = np.squeeze(predictions)\noutput = pd.DataFrame({'row_id': df_test['row_id'],\n                       'num_sold': predictions})\n\noutput.to_csv('submission.csv', index=False)","39fd2e5c":"### Encoding Categorical Features","a1bdede0":"### Modelling","0641bcba":"### Preliminary Feature Engineering\n\nThe date field is not so useful for training the model. I'll break it up to year and month, just to have an idea about the time period.","fde60acb":"Holidays","f3bae4bd":"### EDA\n\nLet's plot out the num_sold based on the date","b2577e9c":"Seems like there's something between:\n- CLI -> BCI\n- CPI -> BCI","8fdca4a7":"### Splitting","3580b4f4":"### MUTUAL INFORMATION","50629d82":"### GOAL\n\nCreate a model that will predict the num_sold for the given new data\n\n\n### Overview\n\nDue to the nature of the dataset, we'll probably want to preprocessing the features and then do some EDA to find the correlation. After that I'll implement the model with a small Neural Network using Keras.","0b6138ab":"GDP","9cb53f66":"We'll try to create some \"ratio\" feature between CPI\/CLI and BCI","f66aec57":"# Tabular Playground Jan 2022","da430d23":"OECD","23e7562c":"### Model Evaluation","f21b4ae0":"Checking if there's some kind of correlation between the economics info","a4e0edc3":"Looking at the value that we're trying to predict, we can see that there's some big difference between the most usual values and the outloners. So, instead of dropping them, we'll try to predict the logarithm, and then convert it back to exponential before the submission. ","445ccf89":"We could also see that, logarithm-wise, the distribution of the values through the countries is quite homogeneous","52e1874d":"### Further Feature Engineering\n\nEconomics new features"}}