{"cell_type":{"6e60bb8e":"code","7c1beac7":"code","c5287774":"code","6d11a182":"code","e309dfe3":"code","48ebac8a":"code","edefed68":"code","609d16e2":"code","39240787":"code","9c37a238":"code","b9a98c54":"code","42475503":"code","60c42156":"code","10c14c10":"code","a3245229":"code","da0f763d":"code","155ee90d":"code","f9eb778c":"code","bd700dab":"code","c036603e":"code","822a3291":"code","737d6914":"code","4a30493f":"code","dd6fb878":"code","df78aff1":"code","32b923e0":"code","a9c002a5":"code","01860d09":"code","f033375f":"code","c97c6295":"code","95fcc548":"markdown","aef642d9":"markdown","5b862b9e":"markdown","4e554658":"markdown","ff59d0b1":"markdown","06ca0177":"markdown","24bd89d5":"markdown","3f1bb43b":"markdown","964c7db9":"markdown","d99265f7":"markdown","89301797":"markdown","bf0fb2f4":"markdown","7d922014":"markdown","12e0599f":"markdown","6ceaa984":"markdown","4a881042":"markdown","2de351f9":"markdown","ec71dbc0":"markdown","77dd4f85":"markdown","bc0f3707":"markdown","e72c5dd6":"markdown","b52b918e":"markdown"},"source":{"6e60bb8e":"import numpy as np\nimport pandas as pd\nimport sklearn\nfrom sklearn import *","7c1beac7":"raw = pd.read_csv('..\/input\/ufcdata\/raw_total_fight_data.csv', sep=';')\ndisplay(raw.shape)\ndisplay(raw.head())","c5287774":"fighter = pd.read_csv('..\/input\/ufcdata\/raw_fighter_details.csv')\nfighter","6d11a182":"data = raw \\\n    .merge(fighter.rename(lambda x: f'R_{x}', axis=1), left_on='R_fighter', right_on='R_fighter_name')\\\n    .merge(fighter.rename(lambda x: f'B_{x}', axis=1), left_on='B_fighter', right_on='B_fighter_name')\\\n    .drop(['R_fighter_name', 'B_fighter_name'], axis=1)\ndata","e309dfe3":"data.iloc[0]","48ebac8a":"from datetime import datetime, timedelta\nfrom dateutil.parser import parse\nimport re\n\ndata.drop([c for c in data.columns if c.endswith('_pct')], axis=1, inplace=True)\n\nfor c in ['date', 'R_DOB', 'B_DOB']:\n    data[c] = data[c].map(lambda x: parse(x) if type(x) == str else None)\n\ndata['R_age'] = (data['date'] - data['R_DOB']).map(lambda x: x.days \/ 365.25 if hasattr(x, 'days') else None)\ndata['B_age'] = (data['date'] - data['B_DOB']).map(lambda x: x.days \/ 365.25 if hasattr(x, 'days') else None)\n\ndata['R_Weight'] = data['R_Weight'].map(lambda x: int(re.search('\\d+', x)[0]) if type(x) == str else None)\ndata['B_Weight'] = data['B_Weight'].map(lambda x: int(re.search('\\d+', x)[0]) if type(x) == str else None)\n\ndef time2int(x):\n    m, s = x.split(':')\n    return 60*int(m) + int(s)\ndata['last_round_time'] = data['last_round_time'].map(time2int)\n\ndef winner2color(row):\n    return \\\n        'Blue' if row['Winner'] == row['B_fighter'] else \\\n        'Red'  if row['Winner'] == row['R_fighter'] else \\\n        'Draw'\ndata['Winner'] = data.apply(winner2color, axis=1)\n\ndef feet2inch(x):\n    if type(x) != str:\n        return None\n    feet = re.findall('(\\d+)\\'', x)\n    feet = int(feet[0]) if feet else 0\n    inch = re.findall('(\\d+)\\\"', x)\n    inch = int(inch[0]) if inch else 0\n    return feet*12 + inch\n\nfor c in data.columns:\n    if c[2:] in ['Height', 'Reach']:\n        data[c] = data[c].map(feet2inch)\n\nfor column in data.columns:\n    it = data[column].iloc[0]\n    if type(it) == str and 'of' in it:\n        data[column+'_landed'] = data[column].map(lambda x: int(x.split('of')[0]))\n        data[column+'_att']    = data[column].map(lambda x: int(x.split('of')[1]))\n        data.drop(column, axis=1, inplace=True)\n\ndata.drop(['R_fighter', 'B_fighter'], axis=1, inplace=True)\n\nfor (column, dtype) in data.dtypes.items():\n    if dtype == object:\n        data[column] = data[column].astype('category')\n        \nprint(data.shape)\ndata.head()","edefed68":"uniques = data.select_dtypes('category').nunique().sort_values()\nuniques","609d16e2":"from tqdm import tqdm_notebook as tqdm\nfrom joblib import Parallel, delayed\nfrom sklearn.base import BaseEstimator, ClassifierMixin\n\n\nclass EnsembleClassifier(BaseEstimator, ClassifierMixin):\n    def __init__(self, estimators):\n        self.estimators = estimators\n        \n    def fit(self, *_):\n        return self\n    \n    def predict(self, X):\n        votes = sum(\n            clf.predict_proba(X)\n            for clf in self.estimators\n        )\n        return np.array(self.estimators[0].classes_)[votes.argmax(axis=1)]\n    \n    \ndef best_model(models):\n    models = pd.DataFrame(models)\n    return models['estimator'][models['test_score'].idxmax()]\n\n\ndef permutation_importance(clf, X, y, n_repeats=10, scoring=None):\n    scoring = scoring or metrics.balanced_accuracy_score\n    y_base = scoring(y, clf.predict(X))\n    def aux(X, column):\n        X = X.copy()\n        acc = []\n        for _ in range(n_repeats):\n            it = X[column]\n            it[:] = it[np.random.permutation(it.index)]\n            acc.append(scoring(y, clf.predict(X)) - y_base)\n        return column, acc\n    results = Parallel(n_jobs=-1)(delayed(aux)(X, column) for column in tqdm(X.columns))\n    return pd.DataFrame(dict(results))\n\n# followings are not used.\ndef smoothed_lower_bound_binomial(c,C):\n    c += 0.5\n    C += 1\n    v = c*(C-c)\/C\n    return max(0, c - v**.5) \/ C\n\n\ndef smoothed_lower_bound_beta(c,C):\n    c += 0.5\n    C += 1\n    v = c*(C-c)\/C\/C\/(1+C)\n    return max(0, c - v**.5) \/ C","39240787":"def check(clf, X, y, scoring=None):\n    print(f'{metrics.classification_report(y, clf.predict(X))}')\n    importances = permutation_importance(clf, X, y, scoring=scoring)\n    return importances.abs().mean().sort_values().tail(10).plot.barh(title='Importance')\n\ndef X_y(data):\n    return data.dropna().drop(['Winner'], axis=1),  data.dropna()['Winner']","9c37a238":"from sklearn.base import BaseEstimator, ClassifierMixin\n\nclass TestClassifier(BaseEstimator, ClassifierMixin):\n    def predict(self, X):\n        return np.random.choice(['Red', 'Draw', 'Blue'], len(X))\n    \nX, y = X_y(data)\ncheck(TestClassifier(), X, y)","b9a98c54":"def cross_validate(\n    model, X, y,\n    fit=lambda model, X_train, y_train, X_valid, y_valid: model.fit(X_train, y_train),\n    predict=lambda model, X: model.predict(X),\n    scoring=lambda y_true, y_pred: metrics.balanced_accuracy_score(y_true, y_pred),\n    cv=5,\n):\n    cv = model_selection.check_cv(cv)\n        \n    result = {\n        'train_score': [],\n        'test_score': [],\n        'estimator': [],\n    }\n    \n    def aux(model, X, train_index, valid_index):\n        _model = sklearn.base.clone(model)\n        X_train, y_train = sklearn.utils.metaestimators._safe_split(_model, X, y, train_index)\n        X_valid, y_valid = sklearn.utils.metaestimators._safe_split(_model, X, y, valid_index, train_index)\n        fit(_model, X_train, y_train, X_valid, y_valid)\n        train_score = scoring(y_train, predict(_model, X_train))\n        test_score = scoring(y_valid, predict(_model, X_valid))\n        return _model, train_score, test_score\n\n    it = Parallel(n_jobs=-1)(\n        delayed(aux)(model, X, train_index, valid_index)\n        for train_index, valid_index in tqdm(cv.split(X, y))\n    )\n    for _model, train_score, test_score in it:\n        result['estimator'].append(_model)\n        result['train_score'].append(train_score)\n        result['test_score'].append(test_score)\n        \n    return result","42475503":"def cross_validate_catboost(X, y, **kwargs):\n    import catboost, utils\n    models = cross_validate(\n        catboost.CatBoostClassifier(\n            od_type='Iter',\n            od_wait=10,\n            eval_metric='AUC', # this is important to speed up the learning with overfitting detector.\n            **kwargs\n        ),\n        X, y,\n        fit=lambda m,Xt,yt,Xv,yv: m.fit(Xt, yt, eval_set=(Xv,yv),\n                                        cat_features=np.where(X.dtypes == 'category')[0],\n                                        verbose=0),\n    )\n    return pd.DataFrame(models)","60c42156":"X, y = X_y(data)\n\n%time models = cross_validate_catboost(X, y)\n\nmodel = EnsembleClassifier(models['estimator'])\n%time check(model, X, y)","10c14c10":"neutral_columns = [c for c in data.dtypes.index \n                   if not c.startswith('R_') \n                   and not c.startswith('B_')\n                   and not c == 'Winner']\nneutral_columns","a3245229":"%%time\ndef X_y_neutral(data):\n    return X_y(data.drop(neutral_columns, axis=1))\n\nX, y = X_y_neutral(data)\nmodels = cross_validate_catboost(X, y,)\nmodel = EnsembleClassifier(models['estimator'])\ncheck(model, X, y)","da0f763d":"def balancing_params(y):\n    counts = y.value_counts()\n    return { 'class_names': list(counts.index), 'class_weights': len(y) \/ counts.values }\nbalancing_params(y)","155ee90d":"def flip_concat(raw):\n    data = raw.copy()\n    def flip_color(c):\n        return \\\n            f'R_{c[2:]}' if c.startswith('B_') else \\\n            f'B_{c[2:]}' if c.startswith('R_') else \\\n            c\n    raw.columns = data.columns.map(flip_color)\n    data['Winner'] = data['Winner'].map({ 'Red': 'Blue', 'Blue': 'Red', 'Draw': 'Draw'})\n    it = pd.concat([raw, data], ignore_index=True, sort=False)\n    return it.astype({ c: 'category' if t == object else t for c, t in it.dtypes.items() })\nflipped = flip_concat(data)\nflipped.nunique().sort_values().tail(10)","f9eb778c":"X, y = X_y_neutral(data)\nmodels = cross_validate_catboost(X, y, **balancing_params(y))\nmodel = EnsembleClassifier(models['estimator'])\ncheck(model, X, y)","bd700dab":"def X_y_flip(data):\n    data = flip_concat(data)\n    return X_y_neutral(data)\n\nX, y = X_y_flip(data)\n\nmodels = cross_validate_catboost(X, y, **balancing_params(y))\nmodel = EnsembleClassifier(models['estimator'])\ncheck(model, X, y)","c036603e":"data_train, data_test = model_selection.train_test_split(data, test_size=1\/5)","822a3291":"X_train, y_train = X_y_flip(data_train)\nX_test , y_test  = X_y_flip(data_test)\n\nmodels = cross_validate_catboost(X_train, y_train, **balancing_params(y_train))\nmodel = EnsembleClassifier(models['estimator'])\n\nprint('Train')\ncheck(model, X_train, y_train)","737d6914":"print('Test')\ncheck(model, X_test, y_test)","4a30493f":"def difference_features(data):\n    data = data.copy()\n    dtypes = data.select_dtypes([np.int, np.float]).dtypes\n    for c in dtypes.index:\n        if c.startswith('R_'):\n            # exp(-|diff|) is 1 if |diff| is small and 0 if |diff| is large.\n            data[f'diff_{c[2:]}'] = np.exp(-(data[f'R_{c[2:]}'] - data[f'B_{c[2:]}']).abs())\n    return data\ndifference_features(data)","dd6fb878":"def X_y_draw(data):\n    X, y = X_y_flip(data)\n    return difference_features(X), y\n\nX_train, y_train = X_y_draw(data_train)\nX_test, y_test = X_y_draw(data_test)\n\nmodels = cross_validate_catboost(X_train, y_train, **balancing_params(y_train))\nmodel = EnsembleClassifier(models['estimator'])\n\nprint('Train')\ncheck(model, X_train, y_train)","df78aff1":"print('Test')\ncheck(model, X_test, y_test)","32b923e0":"def X_y_linear(data, X_y=X_y_flip):\n    X, y = X_y(data)\n    # we can drop these features safely, they were always low important.\n    # this simplify the preprocessing for linear model.\n    return X.select_dtypes(exclude=['category', 'datetime']), y\n\nlogreg = pipeline.Pipeline([\n    ('scale', preprocessing.StandardScaler()),\n    ('clf', linear_model.LogisticRegression(solver='lbfgs', multi_class='auto', class_weight='balanced')),\n])","a9c002a5":"X_train, y_train = X_y_linear(data_train)\nX_test , y_test  = X_y_linear(data_test)\n\nmodels = cross_validate(logreg, X_train, y_train)\nmodel = EnsembleClassifier(models['estimator'])\nprint('Train')\ncheck(model, X_train, y_train)","01860d09":"print('Test')\ncheck(model, X_test, y_test)","f033375f":"X_train, y_train = X_y_linear(data_train, X_y=X_y_draw)\nX_test , y_test  = X_y_linear(data_test, X_y=X_y_draw)\n\nmodels = cross_validate(logreg, X_train, y_train)\nmodel = EnsembleClassifier(models['estimator'])\nprint('Train')\ncheck(model, X_train, y_train)","c97c6295":"print('Test')\ncheck(model, X_test, y_test)","95fcc548":"## with the difference features","aef642d9":"### Flip the data too","5b862b9e":"## Deal with Imbalance\nNow we want to make this model be _symmetric_ about the fighter's color code. To this end, let's try the followings:\n- balancing classes by their weights, and\n- flipping the side of fighters.","4e554658":"Now the model says `*_KD`(# of knockdowns) are important to tell `Winner`. But Red's and Blue's `KD` have different importances. This might be caused by imbalance of labels.","ff59d0b1":"# Feature Engineering\nWe have good prediction scores except `Draw` class now. How to improve the score for `Draw` class?\n\n`Draw` score dropped to zero after discarding the \"color-independent\" features in the early stage of previous section.\nIt means we need some direct features in our data to indicate that \"There is no significant difference between two.\"\n\nLet's make such features by simple way.","06ca0177":"# Intro","24bd89d5":"It seems to be okay if we ignore the `Draw` scores. Balanced accuracy score(`macro avg recall`) is degraded by the low `Draw` class scores.","3f1bb43b":"### Balance the weights","964c7db9":"# Utilities","d99265f7":"As we expected, the disparity between the color codes is reduced.","89301797":"# Exploring data by Learning\nHere, we use `CatBoost` to analyse the data by observing how it fits to the data. In this way, we can compare categorical and numerical variables with a simple importance calculation.","bf0fb2f4":"# Conclusion\nWe've tried an incremental way to inspect the given data with CatBoost and made a predictor with 0.8~ accuracy score(`weighted avg recall`). Same thing can be done with other models like linear model, but you need to build up a preprocessing pipeline carefully.\n\nCatBoost is fast enough and versatile way to learn the data for analysis while letting us ignoring about which features are categorical, numerical or other dtype.","7d922014":"## without the difference features","12e0599f":"we use `cross_validate` function to make estimators for `EnsembleClassifier`.","6ceaa984":"We use this `check` function to inspect how the model is good or not.","4a881042":"It has greater influence on `Draw` scores with linear model than with CatBoost, but the recalls slightly worse than CatBoost.\n\nAnyway, it seems to be really hard to fight against the low scores of low proportional class.","2de351f9":"Balancing improved recall over `Draw` class.","ec71dbc0":"This slightly imporoves the `Draw` scores with CatBoost but it overfit on `Draw` class more significantly than before.","77dd4f85":"## Drop unsound features\n`win_by` looks like the most important variable here, but it's actually not related to the color code of fighters. It means that `Winner` is explained almost only by the variable which never reflects who fights in the match.\n\nDoes this make sense? Now let's drop those color-independent variables to prevent this.","bc0f3707":"# Preprocess","e72c5dd6":"## Check overfitting\nLet's make sure those ensembled models not to be overfitted to the data.","b52b918e":"# Compare with linear model\nLet's try some linear models to compare the above ways with CatBoost. Especially, we want to check whether the above feature engineering improves the `Draw` scores or not."}}