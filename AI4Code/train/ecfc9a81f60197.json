{"cell_type":{"7f120206":"code","3ef70117":"code","d56891ed":"code","7b0c9f23":"code","33faf914":"code","6125c5bb":"code","98cfa69c":"code","264cfdac":"code","2d31451b":"code","1bd69dff":"code","83c4659a":"code","6fba3e21":"code","457b023b":"code","4c9a0538":"code","ab4abb47":"code","482be488":"code","ff9fc976":"code","3a30d744":"code","16f3fb53":"code","b9f2fe16":"code","ae9adf2d":"code","3009a170":"code","f0dca749":"code","27a0ce46":"code","54a5d87c":"code","e625c765":"code","4f453578":"code","c1eab180":"code","e754d039":"code","3cae96b0":"code","4c41d0d5":"code","6afebf7d":"code","ce76fe9b":"code","e066b8ab":"code","2c27653c":"code","2926db1b":"code","1ab31b15":"markdown","0ce5d6b3":"markdown","7b4f29c8":"markdown","7042dea8":"markdown","e0488d6c":"markdown","2eddd949":"markdown","72cd9271":"markdown","1920617f":"markdown","7ca88a28":"markdown"},"source":{"7f120206":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport datatable as dt\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","3ef70117":"train_data = dt.fread('..\/input\/jane-street-market-prediction\/train.csv').to_pandas()","d56891ed":"type(train_data)","7b0c9f23":"train_data.head()","33faf914":"train_data.describe()","6125c5bb":"import seaborn as sns\nimport matplotlib.pyplot as plt","98cfa69c":"plt.figure(figsize=(10,10))\nsns.heatmap(train_data.corr(), annot=True)","264cfdac":"train_data.columns","2d31451b":"%%time\nfit, axes = plt.subplots(nrows=45, ncols=3, figsize=(25,250))\n\nfor i in range(2,137):\n    sns.distplot(train_data.iloc[:,i],ax=axes[(i-2)\/\/3,(i-2)%3])","1bd69dff":"from sklearn.model_selection import train_test_split\nfrom sklearn.feature_selection import mutual_info_regression, SelectPercentile, f_regression","83c4659a":"sample_data = train_data[train_data['date']==1]","6fba3e21":"sample_data=sample_data[sample_data['weight']!=0]","457b023b":"sample_data.dropna(axis=0,inplace=True)","4c9a0538":"resp_cols = [c for c in sample_data.columns if 'resp_' in c]","ab4abb47":"resp_cols","482be488":"features = [c for c in sample_data.columns if 'feature' in c]","ff9fc976":"X= sample_data.drop(labels=resp_cols, axis=1)","3a30d744":"X.drop(labels=['date','weight','resp'], axis=1, inplace=True)","16f3fb53":"y = sample_data.loc[:,resp_cols]","b9f2fe16":"X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.3,random_state=42)","ae9adf2d":"mi = SelectPercentile(mutual_info_regression, percentile=10)","3009a170":"for i in y:\n    #print(y[i])\n    mi.fit(X_train,y_train[i])","f0dca749":"X_train=X_train[X_train.columns[mi.get_support()]]","27a0ce46":"X_test=X_test[X_test.columns[mi.get_support()]]","54a5d87c":"X_train.head()","e625c765":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nX_train=scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)","4f453578":"len=X_train.shape[1]","c1eab180":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout","e754d039":"model = Sequential()\nmodel.add(Dense(len, activation='relu'))\nmodel.add(Dense(16,activation ='relu'))\n#model.add(Dropout(0.2))\nmodel.add(Dense(8,activation='relu'))\nmodel.add(Dense(16,activation ='relu'))\nmodel.add(Dense(32, activation='relu'))\nmodel.add(Dense(8,activation='relu'))\n#model.add(Dropout(0.2))\nmodel.add(Dense(4, activation='relu'))","3cae96b0":"model.compile(optimizer='adam', loss='mse',metrics=['accuracy'] )","4c41d0d5":"from tensorflow.keras.callbacks import EarlyStopping\ncall_back = EarlyStopping(monitor='val_loss',patience=20)","6afebf7d":"model.fit(X_train, y_train, verbose=2, callbacks=[call_back] ,validation_data=(X_test,y_test), epochs=300, batch_size=128)","ce76fe9b":"report = pd.DataFrame(model.history.history)","e066b8ab":"report['accuracy'].max()","2c27653c":"report[['loss','val_loss']].plot()","2926db1b":"report[['accuracy','val_accuracy']].plot()","1ab31b15":"# Explore Data","0ce5d6b3":"# Feature Selection process\nlets identify the feature before test & train split","7b4f29c8":"Taking sample date for date ==1","7042dea8":"# Selecting top 20 percentile features\nToo many features to find the coorelation and find the right features for predictions. let us use feature selection model from SKlearn to identify top 20 percentile features for prediction. ","e0488d6c":"# Read Data\nRead Data - with DataTable libraray, Data Table reads the big data in short time. we are converting the datatable to dataframe.","2eddd949":"# Sepreate independent & dependent variables\ncolumn name with \"resp_\" are the dependent variables and column name with \"feature\" are independent variables. in the below process, we are segregating the column names to split the data for X & y","72cd9271":"# conclusion:\n\nabove report shows the model is perfroming well with 51% accuracy. however, futher imporment on the feature selection whould help to predict better. ","1920617f":"# Plot data for more analysis\nImporting Ploting libraries to explore the data fruther","7ca88a28":"Generating model report"}}