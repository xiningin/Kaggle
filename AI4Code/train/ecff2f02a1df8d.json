{"cell_type":{"8daa3d8f":"code","851d196d":"code","e08a106d":"code","a8c1ddd1":"code","f74e27e6":"code","a59454fd":"code","480755b5":"code","78b806da":"code","49dd2996":"code","70dce55a":"markdown","7c6037b8":"markdown"},"source":{"8daa3d8f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","851d196d":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split","e08a106d":"df_full = pd.read_csv('..\/input\/digit-recognizer\/train.csv')\ndf_full.head(5)","a8c1ddd1":"# Dropping all pixels that are not 'utilised' in the training set.\n\ncolumns_list = df_full.columns.tolist()\ndropped_columns = []\n\nfor column in columns_list:\n    if len(df_full[column].unique()) == 1:\n        print('Deleting: ', column)\n        dropped_columns.append(column)\n        df_full.drop(column, axis=1, inplace=True)\nlen(dropped_columns)","f74e27e6":"X_data = df_full.drop('label', axis=1)\ny_data = df_full['label']\n\nX_train, X_test, y_train, y_test = train_test_split(X_data, y_data, random_state = 0)","a59454fd":"from sklearn.neighbors import KNeighborsClassifier\n\nkn = KNeighborsClassifier(n_neighbors = 5, n_jobs = -1)\n\nkn.fit(X_train, y_train)","480755b5":"# pass the 'multi_class = 'ovr' ' parameter to use One v Rest \n\nfrom sklearn.metrics import roc_auc_score\n\ny_pred = kn.predict_proba(X_test)\n\nauc_score = roc_auc_score(y_test, y_pred, multi_class = 'ovr')\nprint('AUC: ', auc_score)","78b806da":"df_test_full = pd.read_csv('..\/input\/digit-recognizer\/test.csv')\ndf_test = df_test_full.drop(dropped_columns, axis=1)\n\npredictions = kn.predict(df_test)\n","49dd2996":"ImageID = np.linspace(1, 28000, 28000).astype('int')\nsubmission = pd.DataFrame({'label':predictions, 'ImageID': ImageID}, index = ImageID)\nsubmission.set_index('ImageID', inplace=True)\nsubmission.to_csv('MNIST_Submission.csv')","70dce55a":"### The is a multi-class problem. \nThankfully, sklearn estimators are all prepared for multiclass classification via One-v-Rest, One-V-One etc.","7c6037b8":"# Digi-Recogniser from the MNIST Data Set via non-neural-network techniques\n\nAs I don't yet know nueral networks that well, nor the convulation involved. \n\n#### To Do: Kera, Tensorflow - learn these!\n\n### The MNIST Dataset\n We have two files, one with training data, the other with test data.\n There are no missing\/malformed entries.\n \n #### My bet is that we can optimise for non-neural network\n \n - Clean the data to exclude any features that present little to no bearing on the label\n - Identify a model that would be best suited to the large number of features (chosen pixels)\n - Train\n - Test\n - Predict\n - Clean and Save submission"}}