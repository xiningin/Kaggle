{"cell_type":{"d273f1ef":"code","a0ad6bd7":"code","d79f3eda":"code","443d19bf":"code","0fed0743":"code","e135e2b9":"code","93611023":"code","9edf26b0":"code","5acb0e93":"code","2173d203":"code","d5903d9d":"code","06ce6681":"code","c91e5e57":"code","30735bd5":"code","835decd2":"code","f8a4eaf9":"code","d0c2dca5":"code","ca33526d":"code","d5311d21":"code","f378857b":"code","a42fe88c":"code","6dfa0b13":"code","81d1cd3b":"code","a8489e01":"code","9610bacd":"code","96eabf52":"code","60401d43":"code","16b609a5":"code","ebc1ccde":"markdown","4f9f0976":"markdown","30e60a2c":"markdown","278a0f16":"markdown"},"source":{"d273f1ef":"import numpy as np\nimport pandas as pd\nimport pandas_profiling\nimport time\n\nfrom sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\nfrom sklearn.preprocessing import LabelEncoder, OrdinalEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.metrics import roc_auc_score, average_precision_score, log_loss\n\nfrom hyperopt import fmin, hp, tpe, Trials, space_eval\nfrom hyperopt.pyll import scope as ho_scope\nfrom hyperopt.pyll.stochastic import sample as ho_sample\nfrom functools import partial\n\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom catboost import CatBoostClassifier\n\nfrom imblearn.pipeline import Pipeline\n#from imblearn.over_sampling import ADASYN\nfrom imblearn.under_sampling import OneSidedSelection, NeighbourhoodCleaningRule, TomekLinks, RandomUnderSampler\n\nimport category_encoders as ce","a0ad6bd7":"def evalue_model(model, y_test, X_test, model_name):\n    \n    yhat_prob = [x[1] for x in model.predict_proba(X_test)]\n    \n    results = {'model': model_name,\n               'auc': roc_auc_score(y_true = y_test, y_score = yhat_prob),\n               'aucpr': average_precision_score(y_true = y_test, y_score = yhat_prob),\n               'logloss': log_loss(y_test, yhat_prob)}\n    \n    return results","d79f3eda":"submission = pd.read_csv('..\/input\/tabular-playground-series-mar-2021\/sample_submission.csv')\nnew_data = pd.read_csv('..\/input\/tabular-playground-series-mar-2021\/test.csv')\ndf = pd.read_csv('..\/input\/tabular-playground-series-mar-2021\/train.csv')","443d19bf":"#profile = df.profile_report(title=\"Profile train.csv\", explorative=True)\n#profile.to_file(output_file=\"profile_report.html\")","0fed0743":"df.drop(columns = \"id\", inplace = True)\nnew_data.drop(columns = \"id\", inplace = True)","e135e2b9":"for col in df.columns[df.dtypes == \"object\"].tolist():\n    df[col] = df[col].astype('category')","93611023":"X = df.drop('target', axis=1)\ny = df['target']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)","9edf26b0":"high_cardinality = [\"cat5\", \"cat7\", \"cat8\", \"cat10\"]\n\ncategorical_cols = X.columns[X.dtypes == \"category\"].tolist()\n\ncategorical_cols = list(set(categorical_cols) - set(high_cardinality))\n\ncat_columns_position = [X.columns.tolist().index(x) for x in categorical_cols + high_cardinality]","5acb0e93":"categorical_transformer = Pipeline(steps=[\n   # ('OrdinalEncoder', ce.OrdinalEncoder(cols=categorical_cols + high_cardinality))\n    ('OrdinalEncoder', OrdinalEncoder(handle_unknown = \"use_encoded_value\", unknown_value = 999))\n    #('TargetEncoder', ce.TargetEncoder(cols=high_cardinality))\n])\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('cat', categorical_transformer, categorical_cols + high_cardinality)\n])\n\nundersample = RandomUnderSampler(sampling_strategy='majority')","2173d203":"# Desicion Tree ----------------------------------------\nmodel_tree = DecisionTreeClassifier(random_state = 42)\n\npipe_tree = Pipeline([('preprocessor', preprocessor),\n                      ('clf', model_tree )])\n\npipe_tree_un = Pipeline([('preprocessor', preprocessor),\n                         ('undersample', undersample),\n                         ('clf', model_tree )])\n\n# Random Forest ----------------------------------------\nmodel_rf = RandomForestClassifier(random_state = 42, n_jobs=-1)\n\npipe_rf = Pipeline([('preprocessor', preprocessor),\n                    ('clf', model_rf )])\n\npipe_rf_un = Pipeline([('preprocessor', preprocessor),\n                       ('undersample', undersample),\n                       ('clf', model_rf )])\n\n# LightGBM ---------------------------------------------\nmodel_lgbm = LGBMClassifier(random_state = 42, \n                            device = \"gpu\", \n                            n_estimators = 500)\n\npipe_lgbm = Pipeline([('preprocessor', preprocessor),\n                      ('clf', model_lgbm )])\n\npipe_lgbm_un = Pipeline([('preprocessor', preprocessor),\n                         ('undersample', undersample),\n                         ('clf', model_lgbm )])\n\n# CatBoost ----------------------------------------------\nmodel_cat = CatBoostClassifier(random_state = 42, verbose = 0,\n                               task_type=\"GPU\", border_count = 32,\n                                n_estimators = 500,\n                                early_stopping_rounds = 100)\n\npipe_cat = Pipeline([('clf', model_cat )])\n\npipe_cat_un = Pipeline([('undersample', undersample),\n                        ('clf', model_cat )])\n\n# CatBoost (fast mode)----------------------------------\nmodel_cat_fast = CatBoostClassifier(random_seed=42, verbose = 0,\n                                    task_type=\"GPU\", border_count = 32, # turn on gpu\n                                    n_estimators = 500,\n                                     boosting_type = \"Plain\",\n                                     bootstrap_type = \"Bernoulli\",\n                                     max_ctr_complexity = 1,\n                                     subsample = 0.6,\n                                     #colsample_bylevel = 0.5, # ONLY CPU\n                                     #one_hot_max_size = 4,\n                                     #leaf_estimation_iterations = 5, #[1,10]\n                                     early_stopping_rounds = 100)\n\npipe_cat_fast = Pipeline([('clf', model_cat_fast )])\n\npipe_cat_fast_un = Pipeline([('undersample', undersample),\n                             ('clf', model_cat_fast )])","d5903d9d":"classifiers = {\n    \"DecisionTreeClassifier\": pipe_tree,\n    \"DecisionTreeClassifier_un\": pipe_tree_un,\n    \"RandomForestClassifier\": pipe_rf,\n    \"RandomForestClassifier_un\": pipe_rf_un,\n    \"LGBMClassifier\": pipe_lgbm,\n    \"LGBMClassifier_un\": pipe_lgbm_un,\n    \"CatBoostClassifier\": pipe_cat,\n    \"CatBoostClassifier_un\": pipe_cat_un,\n    \"CatBoostClassifier_fast\": pipe_cat_fast,\n    \"CatBoostClassifier_fast_un\": pipe_cat_fast_un\n}","06ce6681":"results = pd.DataFrame(columns= [\"model\", \"auc\", \"aucpr\", \"logloss\",  \"time\"])","c91e5e57":"%%time\n\nimport time\n\nfor key, classifier in classifiers.items():\n    print(\"Running\", key)\n    \n    start_time = time.time()\n    \n    if key.find(\"LGBMClassifier\") != -1:\n        classifiers[key] = classifier.fit(X_train, y_train, clf__categorical_feature = cat_columns_position)\n    elif key.find(\"CatBoostClassifier\") != -1:\n        classifiers[key] = classifier.fit(X_train, y_train, clf__cat_features = cat_columns_position)\n    else:\n        classifiers[key] = classifier.fit(X_train, y_train)\n\n    end_time = (time.time() - start_time)\n\n    results = results.append(\n        pd.concat([\n        pd.DataFrame(evalue_model(classifiers[key], y_test, X_test, key), index=[0]),\n        pd.DataFrame({\"time\" : end_time}, index = [0]) ],\n        axis=1)\n    )","30735bd5":"results.sort_values(by=['auc'], ascending = False)","835decd2":"from sklearn import metrics\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfor key, classifier in classifiers.items():\n    \n    yhat_prob = [x[1] for x in classifier.predict_proba(X_test)]\n\n    fpr, tpr, thresh = metrics.roc_curve(y_true = y_test, y_score = yhat_prob)\n    auc = metrics.roc_auc_score(y_test, yhat_prob)\n    plt.plot(fpr,tpr,label=key+\", auc=\"+str(round(auc, 4)))\n    \nplt.legend(loc=0)","f8a4eaf9":"# eval dataset to lgbm\n#X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42, stratify=y_train)","d0c2dca5":"hp_space = {\n    'undersample': hp.choice(label = 'undersample', options = [True, False]),\n    'clf': {\n        #'n_estimators': ho_scope.int(hp.quniform('n_estimators',200,600,100)), \n\n        'max_depth':  ho_scope.int(hp.quniform('max_depth',1,9,1)),\n        #'leaf_estimation_iterations':  ho_scope.int(hp.quniform('leaf_estimation_iterations',1,5,1)),\n        \n        'subsample': hp.uniform('colsample_bytree',0.2,0.7),\n        #'colsample_bylevel': hp.uniform('colsample_bylevel',0.2,0.6),\n        \n        'reg_lambda': hp.loguniform('reg_lambda',np.log(1e-4),np.log(7))\n        \n        # catboost \n       # 'border_count': hp.choice(label = 'border_count', options = [32, 64 ,128, 254]) # turn off on gpu\n        #'ctr_target_border_count': hp.choice(label = 'ctr_target_border_count', options = [1,5,10,20,50,100,200, 250])\n    }\n}\n\nho_sample(hp_space)","ca33526d":"iteracoes = Trials()","d5311d21":"def instancia_modelo(hiperparametros):\n        \n    clf = CatBoostClassifier(**hiperparametros['clf'],\n                             random_seed=42, verbose = 0,\n                             task_type=\"GPU\", border_count = 32, # turn on gpu\n                             boosting_type = \"Plain\",\n                             bootstrap_type = \"Bernoulli\",\n                             max_ctr_complexity = 0,\n                             n_estimators = 550,\n                             early_stopping_rounds = 100)\n\n    if hiperparametros['undersample'] == True:\n        undersample = RandomUnderSampler(sampling_strategy='majority')\n\n    else:\n        undersample = None\n        \n    #categorical_transformer = Pipeline(steps=[\n    #    ('OrdinalEncoder', ce.OrdinalEncoder(cols=categorical_cols)),\n    #    ('JamesSteinEncoder', ce.JamesSteinEncoder(cols=high_cardinality))\n    #])\n\n    #preprocessor = ColumnTransformer(\n    #    transformers=[\n    #        #('num', numerical_transformer, numerical_cols),\n    #        ('cat', categorical_transformer, high_cardinality + categorical_cols)\n    #])\n    \n    pipe = Pipeline([#('preprocessor', preprocessor),\n                       ('undersample', undersample),\n                       ('clf', clf) ])\n\n    return pipe","f378857b":"## criando uma fun\u00e7\u00e3o para realizar o treino do modelo\ndef funcao_para_minimizar(hiperparametros, features, target):\n    \n    ## criando uma instancia do modelo com a combina\u00e7\u00e3o definida de hiperparametros para usar dentro da fun\u00e7\u00e3o\n    pipe = instancia_modelo(hiperparametros)\n    \n    #pipe.named_steps.preprocessor.fit(X_train, y_train)\n    #X_val_interim = pipe.named_steps.preprocessor.transform(X_val)\n    # Usando dados de validacao\n    \n    #eval_set = [X_val, y_val]\n    \n    #fit_params={'clf__early_stopping_rounds': 100, \n    #            'clf__eval_metric': 'auc',\n    #            'clf__verbose': False,\n    #            'clf__categorical_feature': categorical_cols + high_cardinality,\n    #            'clf__eval_set': eval_set}\n    \n    fit_params={'clf__cat_features':cat_columns_position}\n    \n    cv = StratifiedKFold(n_splits=5)\n    \n    ## treinando o modelo com cross-validation\n    resultado = cross_val_score(estimator = pipe, \n                                X = features, \n                                y = target, \n                                scoring = \"roc_auc\",\n                                cv = cv, \n                                error_score = \"raise\",\n                                fit_params = fit_params,\n                                n_jobs = -1)\n    \n    ## retornando a metrica da performance do modelo\n    return -resultado.mean()","a42fe88c":"%%time\n\n## rodando a otimiza\u00e7\u00e3o\notimizacao = fmin(fn = partial(funcao_para_minimizar, features = X_train, target = y_train),\n                  space = hp_space, \n                  algo = tpe.suggest,\n                  trials = iteracoes,\n                  max_evals = int(120), \n                  rstate = np.random.RandomState(42))","6dfa0b13":"def extrai_space_eval(hp_space, trial):\n    ## desempacota o resultado\n    desempacota_trial = space_eval(space = hp_space, \n                                   hp_assignment = {k: v[0] for (k, v) in trial['misc']['vals'].items() if len(v) > 0})\n    \n    ## retornando o resultado\n    return desempacota_trial","81d1cd3b":"\ndef desempacota_dicionario(dicionario):\n    desempacotado = {}\n    for (chave, valor) in dicionario.items():\n        if isinstance(valor, dict):\n            desempacotado = {**desempacotado, **desempacota_dicionario(valor)}\n        else:\n            desempacotado[chave] = valor\n            \n    return desempacotado","a8489e01":"## colocando o historico em um dataframe\nhistorico = pd.DataFrame([desempacota_dicionario(extrai_space_eval(hp_space, x)) for x in iteracoes.trials])\n\n## colocando o AUC como uma das colunas\nhistorico['auc'] = [-x['loss'] for x in iteracoes.results]","9610bacd":"# hiperpar\u00e2metros selecionados pela otimiza\u00e7\u00e3o\nhiperparametros_selecionados = space_eval(space = hp_space, hp_assignment = otimizacao)\nprint('Hiperpar\u00e2metros selecionados:\\n%s' % hiperparametros_selecionados)","96eabf52":"import plotly.express as px\n\nhistorico.loc[:,'undersample'] = historico.loc[:,'undersample']*1\n\n\nfig = px.parallel_coordinates(historico, color=\"auc\")\nfig.show()","60401d43":"%%time\n\n# Local results\nmodelo_final = instancia_modelo(hiperparametros=hiperparametros_selecionados)\nmodelo_final = modelo_final.fit(X_train, y_train, clf__cat_features=cat_columns_position)\n\nresults = results.append(pd.DataFrame(evalue_model(modelo_final, y_test, X_test, \"final_model\"), index=[0]))\nresults.sort_values(by=['auc'], ascending = False)","16b609a5":"%%time\n\n# Submit results\nclf = CatBoostClassifier(**hiperparametros_selecionados['clf'],\n                         random_seed=42, verbose = 0, task_type=\"GPU\", border_count = 32,\n                         bootstrap_type = \"Bernoulli\",\n                         n_estimators = 3000,\n                         early_stopping_rounds = 100)\n    \npipe = Pipeline([#('preprocessor', preprocessor),\n                 ('undersample', undersample),\n                 ('clf', clf) ])\n\nfinal_fit = pipe.fit(X, y, clf__cat_features=cat_columns_position)\n\nsubmission.loc[:, 'target'] = final_fit.predict_proba(new_data)[:,1]\nsubmission.to_csv('submission.csv', index = False)","ebc1ccde":"# Baseline Models","4f9f0976":"# Bayes Tunning","30e60a2c":"# Import dependencies","278a0f16":"# Prepare Data"}}