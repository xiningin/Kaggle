{"cell_type":{"ec51196c":"code","b6e4e337":"code","1697c0e2":"code","4591e019":"code","9de554e9":"code","1012c57b":"code","2ef81562":"code","ed90a45a":"code","ece36956":"code","d8d76dc5":"code","9abd766b":"code","c495ab5c":"code","84e31b1f":"code","13e61a69":"code","d98d5c45":"code","a8d74145":"code","005e01e5":"code","5d60823d":"markdown","38973333":"markdown","b3b719d5":"markdown","4a7bca83":"markdown","987d334d":"markdown","2cb33c8c":"markdown","8b721d48":"markdown","ace0cbbe":"markdown","4bca879b":"markdown","fef0eeec":"markdown","7a9f85b2":"markdown","49d250d9":"markdown","e780055b":"markdown","3f10cc3e":"markdown","5bfaac6b":"markdown","803c0de7":"markdown"},"source":{"ec51196c":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport pickle\nimport time\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n\nimport os\nprint('-------------------------')\nprint('all files:')\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\nprint('-------------------------')","b6e4e337":"#teachers_folder = '..\/input\/bigger-is-better\/orthogonal_normlized_init\/orthogonal_normlized_init\/'\n#filename = 'FCN_LReLU_04__I-DxW-O__10-2x2-1__n_models_63__n_inits_20__n_iter_1200__seed_1111.pickle'\n\n#teachers_folder = '..\/input\/bigger-is-better\/with_weights\/with_weights\/'\n#filename = 'FCN_LReLU_05_tnorm_init__I-DxW-O__10-4x4-1__n_models_63__n_inits_20_fanin__n_iter_1200__seed_1234.pickle'\n\n#teachers_folder = '..\/input\/bigger-is-better\/with_weights_2\/with_weights_2\/'\n#filename = 'FCN_LReLU_05_tnorm_init__I-DxW-O__10-6x6-1__n_models_42__n_inits_20_fanin__n_iter_1200__seed_1234.pickle'\n\nteachers_folder = '..\/input\/bigger-is-better\/'\nfilename = 'FCN_LReLU_05_gauss_IO_norm_init__I-DxW-O__10-8x8-1__n_models_42__n_inits_30_fanin__n_iter_1200__seed_1234.pickle'\nfilename = 'FCN_LReLU_05_gauss_IO_norm_init__I-DxW-O__10-2x4-1__n_models_42__n_inits_30_fanin__n_iter_1200__seed_1234.pickle'\nfilename = 'FCN_LReLU_05_gauss_IO_norm_init__I-DxW-O__10-5x8-1__n_models_42__n_inits_30_fanin__n_iter_1200__seed_1234.pickle'\n\nteacher_filename = os.path.join(teachers_folder, filename)\n\nresults_filename = teacher_filename\nwith open(results_filename, \"rb\") as f:\n    training_results = pickle.load(f)\n    \nGT_model_name = results_filename.split('\/')[-1].split('__n_models')[0]\n\nprint('-----------------------------')\nprint('GT model name: \"%s\"' %(GT_model_name))\nprint('-----------------------------')\n\nprint('-----------------------------')\nall_model_names = list(training_results.keys())\nprint('all model names:')\nfor model_name in all_model_names:\n    print('  ' + model_name)\nprint('-----------------------------')\n\nsingle_model_results = training_results[all_model_names[8]]\nall_results_dict_keys = list(single_model_results.keys())\nprint('single model results dict keys = %s' %(all_results_dict_keys))\n\n\nprint('single model hyperparams = %s' %(single_model_results['model_hyperparams_dict']))\n\nprint('single model y_GT_stats = ' %(single_model_results['model_hyperparams_dict']['y_GT_stats_dict']))\n[print(x, single_model_results['model_hyperparams_dict']['y_GT_stats_dict'][x]) for x in single_model_results['model_hyperparams_dict']['y_GT_stats_dict'].keys()]\n\nprint('-----------------------------')\n\n# collect all learning curves for the same model in the same matrix\nlearning_curve_matrix = np.zeros((len(single_model_results['all learning curves']), single_model_results['all learning curves'][0]['valid_1_loss'].shape[0]))\nfor k, curr_learning_curves in enumerate(single_model_results['all learning curves']):\n    learning_curve_matrix[k, :] = curr_learning_curves['valid_1_loss']\n    \nnum_batches_vec = single_model_results['all learning curves'][0]['num_batches']\nnum_samples_vec = single_model_results['all learning curves'][0]['num_samples']\n\nprint('single model learning_curve_matrix.shape = %s' %(str(learning_curve_matrix.shape)))\nprint('max number of batches (training steps\/iterations) is %d' %(num_batches_vec[-1]))\nprint('max number training samples is %d' %(num_samples_vec[-1]))\n\nprint('-----------------------------')\nprint('single model all final losses = %s' %(single_model_results['all final losses']))\n\nprint('single model key outcomes = %s' %(single_model_results['key outcomes']))\nprint('-----------------------------')","1697c0e2":"plt.figure(figsize=(15,7))\nplt.subplots_adjust(left=0.06, bottom=0.06, right=0.94, top=0.94, hspace=0.1, wspace=0.1)\n\nplt.plot(num_batches_vec, learning_curve_matrix.T, color='b', alpha=0.6)\nplt.plot(num_batches_vec, learning_curve_matrix.mean(axis=0), color='k', label='average')\nplt.plot(num_batches_vec, learning_curve_matrix.max(axis=0), color='r', label='worst')\nplt.plot(num_batches_vec, learning_curve_matrix.min(axis=0), color='g', label='best')\nplt.xlabel('batch index', fontsize=20)\nplt.ylabel('MSE', fontsize=20)\nplt.legend(fontsize=20)","4591e019":"def get_learning_curve_matrix(model_name, valid_index=1):\n    single_model_results = training_results[model_name]\n\n    valid_loss = 'valid_%d_loss' %(valid_index)\n    \n    # collect all learning curves for the same model in the same matrix\n    learning_curve_matrix = np.zeros((len(single_model_results['all learning curves']), single_model_results['all learning curves'][0][valid_loss].shape[0]))\n    for k, curr_learning_curves in enumerate(single_model_results['all learning curves']):\n        learning_curve_matrix[k, :] = curr_learning_curves[valid_loss]\n\n    return learning_curve_matrix\n\n\ndef get_learning_curves(model_name, valid_index=1, percent_var_remaining=False):\n    single_model_results = training_results[model_name]\n\n    valid_loss = 'valid_%d_loss' %(valid_index)\n    valid_key = 'valid_%d' %(valid_index)\n    \n    # collect all learning curves for the same model in the same matrix\n    learning_curves = np.zeros((len(single_model_results['all learning curves']), single_model_results['all learning curves'][0][valid_loss].shape[0]))\n    for k, curr_learning_curves in enumerate(single_model_results['all learning curves']):\n        learning_curves[k, :] = curr_learning_curves[valid_loss]\n\n    training_steps = single_model_results['all learning curves'][0]['num_batches']\n    \n    if percent_var_remaining:\n        baseline_mse = single_model_results['model_hyperparams_dict']['y_GT_stats_dict'][valid_key]['mse_0']\n        learning_curves = 100 * learning_curves \/ baseline_mse\n        \n    return training_steps, learning_curves\n\n\ndef get_depth_x_width(model_name):\n    single_model_results = training_results[model_name]\n    \n    nn_depth = single_model_results['model_hyperparams_dict']['student_nn_depth']\n    nn_width = single_model_results['model_hyperparams_dict']['student_nn_width']\n    \n    return nn_depth, nn_width\n\n\ndef get_key_outcomes(model_name):\n    single_model_results = training_results[model_name]\n\n    return single_model_results['key outcomes']\n","9de554e9":"plt.figure(figsize=(16,12))\nplt.subplots_adjust(left=0.06, bottom=0.06, right=0.94, top=0.93, hspace=0.1, wspace=0.1)\nplt.suptitle('Average Learning Curves', fontsize=24)\n\nmax_depth_log2 = 5\nmax_width_log2 = 8\n\nall_model_names = list(training_results.keys())\nax0 = plt.subplot(2,1,1)\nax1 = plt.subplot(2,1,2)\n\nfor model_name in all_model_names:\n    model_label = model_name.split('_stu')[0]\n    learning_curve_matrix = get_learning_curve_matrix(model_name)\n    nn_depth, nn_width = get_depth_x_width(model_name)\n    curve_color = (np.log2(nn_depth) \/ max_depth_log2, 0.2, np.log2(nn_width) \/ max_width_log2)\n    ax0.plot(num_batches_vec, learning_curve_matrix.mean(axis=0), color=curve_color, label=model_label)\n    ax1.semilogy(num_batches_vec, learning_curve_matrix.mean(axis=0), color=curve_color, label=model_label)\n\nax0.set_ylabel('MSE', fontsize=20)\nax1.set_ylabel('MSE (log scale)', fontsize=20)\nax0.legend(fontsize=11, ncol=6)\nplt.xlabel('training steps', fontsize=20);","1012c57b":"plt.figure(figsize=(16,12))\nplt.subplots_adjust(left=0.06, bottom=0.06, right=0.94, top=0.93, hspace=0.1, wspace=0.1)\nplt.suptitle('Best Learning Curves', fontsize=24)\n\nall_model_names = list(training_results.keys())\nax0 = plt.subplot(2,1,1)\nax1 = plt.subplot(2,1,2)\n\nfor model_name in all_model_names:\n    model_label = model_name.split('_stu')[0]\n    learning_curve_matrix = get_learning_curve_matrix(model_name)\n    nn_depth, nn_width = get_depth_x_width(model_name)\n    curve_color = (np.log2(nn_depth) \/ max_depth_log2, 0.2, np.log2(nn_width) \/ max_width_log2)\n    ax0.plot(num_batches_vec, learning_curve_matrix.min(axis=0), color=curve_color, label=model_label)\n    ax1.semilogy(num_batches_vec, learning_curve_matrix.min(axis=0), color=curve_color, label=model_label)\n\nax0.set_ylabel('MSE', fontsize=20)\nax1.set_ylabel('MSE (log scale)', fontsize=20)\nax0.legend(fontsize=11, ncol=6)\nplt.xlabel('training steps', fontsize=20);","2ef81562":"requested_valid_index = 1\nvar_remaining_threshold = 5\n\nnot_reached_value = 1390\n\ndepth_lims = [1,32]\nwidth_lims = [1,256]\n\nall_model_names = list(training_results.keys())\nshort_model_names = [x.split('_stu')[0] for x in all_model_names]\n\ndepth_list = []\nwidth_list = []\n\nnum_iterations_to_reach = []\nfraction_reached = []\n\nall_short_model_names = []\n\nfor model_name in all_model_names:\n    nn_depth, nn_width = get_depth_x_width(model_name)\n    depth_OK = nn_depth >= depth_lims[0] and nn_depth <= depth_lims[1]\n    width_OK = nn_width >= width_lims[0] and nn_width <= width_lims[1]\n    \n    if depth_OK and width_OK:\n        training_steps, learning_curves = get_learning_curves(model_name, valid_index=requested_valid_index, percent_var_remaining=True)\n        \n        has_reached_matrix = learning_curves < var_remaining_threshold\n        \n        if has_reached_matrix.any():\n            fraction_reached_vec = has_reached_matrix.mean(axis=0)\n            first_ind = np.argmax(fraction_reached_vec > 0)\n            \n            num_iterations_to_reach.append(training_steps[first_ind])\n            fraction_reached.append(fraction_reached_vec[first_ind])\n        else:\n            num_iterations_to_reach.append(not_reached_value)\n            fraction_reached.append(0)\n\n        depth_list.append(nn_depth)\n        width_list.append(nn_width)\n        all_short_model_names.append(model_name.split('_stu')[0])\n\nnum_iterations_to_reach = np.array(num_iterations_to_reach)\nfraction_reached = np.array(fraction_reached)\n\nbar_plot_x_axis = range(len(all_short_model_names))\n\nplt.figure(figsize=(18,20));\nplt.subplots_adjust(left=0.06, bottom=0.06, right=0.94, top=0.96, hspace=0.33, wspace=0.1)\nplt.suptitle('learning speed for \"valid_%d\", GT model \"%s\"' %(requested_valid_index, GT_model_name), fontsize=24)\nplt.subplot(2,1,1);\nplt.bar(bar_plot_x_axis, num_iterations_to_reach)\nplt.xticks(bar_plot_x_axis, all_short_model_names, rotation=90, fontsize=18);\nplt.ylabel('num iterations to reach %d%s var remaining' %(var_remaining_threshold, '%'), fontsize=20)\nplt.ylim([0, 1.1 * num_iterations_to_reach.max()])\nplt.subplot(2,1,2);\nplt.bar(bar_plot_x_axis, num_iterations_to_reach)\nplt.xticks(bar_plot_x_axis, all_short_model_names, rotation=90, fontsize=18);\nplt.ylabel('num iterations to reach %d%s var remaining (log scale)' %(var_remaining_threshold, '%'), fontsize=20)\nplt.yscale('log');","ed90a45a":"requested_valid_index = 1\nrequested_train_steps = [30, 140, 1200]\ntrain_step_colors = ['red', 'blue', 'green']\ntrain_step_colors.reverse()\nshow_percent_remaining = True\ndepth_lims = [1,32]\nwidth_lims = [4,256]\n\ntraining_steps, learning_curves = get_learning_curves(model_name, valid_index=requested_valid_index, percent_var_remaining=show_percent_remaining)\n\n\nall_model_names = list(training_results.keys())\nshort_model_names = [x.split('_stu')[0] for x in all_model_names]\nbar_plot_x_axis = range(len(all_short_model_names))\n\n\n# extract all necessary results\nvalues_dict = {}\nfor requested_train_step in requested_train_steps:\n\n    training_steps, _ = get_learning_curves(model_name)\n    training_step_ind = np.argmin(np.abs(training_steps - requested_train_step))\n    requested_train_step_corrected = training_steps[training_step_ind]\n\n    depth_list = []\n    width_list = []\n\n    result_mean = []\n    result_std = []\n    result_best = []\n    result_90th_percentile = []\n\n    all_short_model_names = []\n\n    for model_name in all_model_names:\n        nn_depth, nn_width = get_depth_x_width(model_name)\n        depth_OK = nn_depth >= depth_lims[0] and nn_depth <= depth_lims[1]\n        width_OK = nn_width >= width_lims[0] and nn_width <= width_lims[1]\n\n        if depth_OK and width_OK:\n            _, learning_curves = get_learning_curves(model_name, valid_index=requested_valid_index, percent_var_remaining=show_percent_remaining)\n            result_vec = learning_curves[:,training_step_ind]\n\n            depth_list.append(nn_depth)\n            width_list.append(nn_width)\n            result_mean.append(result_vec.mean())\n            result_std.append(result_vec.std())\n            all_short_model_names.append(model_name.split('_stu')[0])\n\n    result_mean = np.array(result_mean)\n    result_std = np.array(result_std)\n\n    values_dict[requested_train_step_corrected] = {}\n    values_dict[requested_train_step_corrected]['result_mean'] = result_mean\n    values_dict[requested_train_step_corrected]['result_std'] = result_std    \n\n\n    \n# display the figure    \nsorted_train_steps = sorted(list(values_dict.keys()), reverse=True)\n\nplt.figure(figsize=(18,20));\nplt.subplots_adjust(left=0.06, bottom=0.06, right=0.94, top=0.96, hspace=0.33, wspace=0.1)\nplt.suptitle('\"valid_%d\" training dynamics for GT model \"%s\"' %(requested_valid_index, GT_model_name), fontsize=24)\n\n\nplt.subplot(2,1,1);\nfor k, train_step in enumerate(sorted_train_steps):\n    curr_result_mean = values_dict[train_step]['result_mean']    \n    curr_result_std = values_dict[train_step]['result_std']\n    curr_label = 'after_%d_iterations' %(train_step)\n    \n    if k == 0:\n        plt.bar(bar_plot_x_axis, curr_result_mean, yerr=curr_result_std, color=train_step_colors[-k], label=curr_label)\n    else:\n        prev_result_mean = values_dict[sorted_train_steps[k-1]]['result_mean']\n        result_mean_diff = curr_result_mean - prev_result_mean\n        plt.bar(bar_plot_x_axis, result_mean_diff, yerr=curr_result_std, bottom=prev_result_mean, color=train_step_colors[k], label=curr_label)\n        \nplt.xticks(bar_plot_x_axis, all_short_model_names, rotation=90, fontsize=18);\nif show_percent_remaining:\n    plt.ylabel('variance remaining (%)', fontsize=20)\nelse:\n    plt.ylabel('MSE', fontsize=20)\nplt.ylim([0, 1.3 * values_dict[sorted_train_steps[-1]]['result_mean'].max()])\nplt.legend(fontsize=20)\n\n\nplt.subplot(2,1,2);\nfor k, train_step in enumerate(sorted_train_steps):\n    curr_result_mean = values_dict[train_step]['result_mean']    \n    curr_result_std = values_dict[train_step]['result_std']\n    curr_label = 'after_%d_iterations' %(train_step)\n    \n    if k == 0:\n        plt.bar(bar_plot_x_axis, curr_result_mean, yerr=curr_result_std, color=train_step_colors[-k], label=curr_label)\n    else:\n        prev_result_mean = values_dict[sorted_train_steps[k-1]]['result_mean']\n        result_mean_diff = curr_result_mean - prev_result_mean\n        plt.bar(bar_plot_x_axis, result_mean_diff, yerr=curr_result_std, bottom=prev_result_mean, color=train_step_colors[k], label=curr_label)\n        \nplt.xticks(bar_plot_x_axis, all_short_model_names, rotation=90, fontsize=18);\nif show_percent_remaining:\n    plt.ylabel('variance remaining (%)', fontsize=20)\nelse:\n    plt.ylabel('MSE', fontsize=20)\n\nplt.xticks(bar_plot_x_axis, all_short_model_names, rotation=90, fontsize=18);\nif show_percent_remaining:\n    plt.ylabel('variance remaining (%) (log scale)', fontsize=20)\nelse:\n    plt.ylabel('MSE (log scale)', fontsize=20)\nplt.yscale('log')\nplt.legend(fontsize=20);","ece36956":"all_model_names = list(training_results.keys())\n\nfinal_mean = []\nfinal_std = []\ndepth = []\nwidth = []\n\nfor model_name in all_model_names:\n    key_outcomes = get_key_outcomes(model_name)\n    nn_depth, nn_width = get_depth_x_width(model_name)\n\n    final_mean.append(key_outcomes['mean final point'])\n    final_std.append(key_outcomes['std final point'])\n    \n    depth.append(nn_depth)\n    width.append(nn_width)\n\nshort_model_names = [x.split('_stu')[0] for x in all_model_names]\n\nfinal_mean = np.array(final_mean)\nfinal_std = np.array(final_std)\n    \nx_axis = range(len(short_model_names))\n\nmse_0 = single_model_results['model_hyperparams_dict']['y_GT_stats_dict']['valid_1']['mse_0']\nfinal_mean = 100 * final_mean \/ mse_0\nfinal_std  = 100 * final_std \/ mse_0\n\nplt.figure(figsize=(18,20));\nplt.subplots_adjust(left=0.06, bottom=0.06, right=0.94, top=0.93, hspace=0.33, wspace=0.1)\nplt.subplot(2,1,1);\nplt.bar(x_axis, final_mean, yerr=np.minimum(final_mean, final_std))\nplt.xticks(x_axis, short_model_names, rotation=90, fontsize=18);\nplt.ylabel('variance remaining (%)', fontsize=20)\nplt.subplot(2,1,2);\nplt.bar(x_axis, final_mean, yerr=np.minimum(final_mean, final_std))\nplt.xticks(x_axis, short_model_names, rotation=90, fontsize=18);\nplt.ylabel('variance remaining (%) (log scale)', fontsize=20)\nplt.yscale('log')","d8d76dc5":"# build a dataframe\ndataframe_cols = ['full_model_name', 'short_name', 'depth', 'width', 'final_MSE_mean', 'final_MSE_std']\nresults_summary_df = pd.DataFrame(index=range(len(all_model_names)), columns=dataframe_cols)\n\nresults_summary_df.loc[:,'full_model_name'] = all_model_names\nresults_summary_df.loc[:,'short_name'] = short_model_names\nresults_summary_df.loc[:,'depth'] = depth\nresults_summary_df.loc[:,'width'] = width\nresults_summary_df.loc[:,'final_MSE_mean'] = final_mean\nresults_summary_df.loc[:,'final_MSE_std'] = final_std\nresults_summary_df","9abd766b":"plt.figure(figsize=(15,10))\nplt.subplots_adjust(left=0.08, right=0.97, bottom=0.05, top=0.97, hspace=0.1)\n\nmax_depth = 32\nunique_depth = sorted(results_summary_df['depth'].unique())\nunique_depth = [x for x in unique_depth if x <= max_depth]\n\nplt.subplot(2,1,1);\nfor depth in unique_depth:\n    const_depth_curve_x   = results_summary_df.loc[results_summary_df.loc[:,'depth'] == depth, 'width']\n    const_depth_curve     = results_summary_df.loc[results_summary_df.loc[:,'depth'] == depth, 'final_MSE_mean']\n    const_depth_curve_std = results_summary_df.loc[results_summary_df.loc[:,'depth'] == depth, 'final_MSE_std']\n    const_depth_curve_std = np.minimum(const_depth_curve_std, const_depth_curve)\n    plt.errorbar(const_depth_curve_x, const_depth_curve, yerr=const_depth_curve_std, label='%d_layers' %(depth))\n\nplt.ylabel('MSE', fontsize=20)\nplt.legend(fontsize=16, ncol=1)\n\n\nplt.subplot(2,1,2);\nfor depth in unique_depth:\n    const_depth_curve_x   = results_summary_df.loc[results_summary_df.loc[:,'depth'] == depth, 'width']\n    const_depth_curve     = results_summary_df.loc[results_summary_df.loc[:,'depth'] == depth, 'final_MSE_mean']\n    const_depth_curve_std = results_summary_df.loc[results_summary_df.loc[:,'depth'] == depth, 'final_MSE_std']\n    const_depth_curve_std = np.minimum(const_depth_curve_std, const_depth_curve)\n    plt.errorbar(const_depth_curve_x, const_depth_curve, yerr=const_depth_curve_std, label='%d_layers' %(depth))\n    \nplt.xlabel('Width', fontsize=20)\nplt.ylabel('MSE (log scale)', fontsize=20)\nplt.yscale('log')","c495ab5c":"plt.figure(figsize=(15,10))\nplt.subplots_adjust(left=0.08, right=0.97, bottom=0.05, top=0.97, hspace=0.1)\n\nmax_depth = 32\nunique_depth = sorted(results_summary_df['depth'].unique())\nunique_depth = [x for x in unique_depth if x <= max_depth]\n\nplt.subplot(2,1,1);\nfor depth in unique_depth:\n    const_depth_curve_x   = results_summary_df.loc[results_summary_df.loc[:,'depth'] == depth, 'width']\n    const_depth_curve     = results_summary_df.loc[results_summary_df.loc[:,'depth'] == depth, 'final_MSE_mean']\n    const_depth_curve_std = results_summary_df.loc[results_summary_df.loc[:,'depth'] == depth, 'final_MSE_std']\n    const_depth_curve_std = np.minimum(const_depth_curve_std, const_depth_curve)\n    plt.errorbar(const_depth_curve_x, const_depth_curve, yerr=const_depth_curve_std, label='%d_layers' %(depth))\n\nplt.ylabel('MSE', fontsize=20)\nplt.legend(fontsize=16, ncol=1)\nplt.xscale('log')\n\nplt.subplot(2,1,2);\nfor depth in unique_depth:\n    const_depth_curve_x   = results_summary_df.loc[results_summary_df.loc[:,'depth'] == depth, 'width']\n    const_depth_curve     = results_summary_df.loc[results_summary_df.loc[:,'depth'] == depth, 'final_MSE_mean']\n    const_depth_curve_std = results_summary_df.loc[results_summary_df.loc[:,'depth'] == depth, 'final_MSE_std']\n    const_depth_curve_std = np.minimum(const_depth_curve_std, const_depth_curve)\n    plt.errorbar(const_depth_curve_x, const_depth_curve, yerr=const_depth_curve_std, label='%d_layers' %(depth))\n    \nplt.xlabel('Width (log scale)', fontsize=20)\nplt.ylabel('MSE (log scale)', fontsize=20)\nplt.yscale('log')\nplt.xscale('log');","84e31b1f":"plt.figure(figsize=(15,10))\nplt.subplots_adjust(left=0.08, right=0.97, bottom=0.05, top=0.97, hspace=0.1)\n\nmax_depth = 16\nunique_depth = sorted(results_summary_df['depth'].unique())\nunique_depth = [x for x in unique_depth if x <= max_depth]\n\nplt.subplot(2,1,1);\nfor depth in unique_depth:\n    const_depth_curve_x   = results_summary_df.loc[results_summary_df.loc[:,'depth'] == depth, 'width']\n    const_depth_curve     = results_summary_df.loc[results_summary_df.loc[:,'depth'] == depth, 'final_MSE_mean']\n    const_depth_curve_std = results_summary_df.loc[results_summary_df.loc[:,'depth'] == depth, 'final_MSE_std']\n    const_depth_curve_std = np.minimum(const_depth_curve_std, const_depth_curve)\n    plt.errorbar(const_depth_curve_x, const_depth_curve, label='%d_layers' %(depth))\n\nplt.ylabel('MSE', fontsize=20)\nplt.legend(fontsize=16, ncol=1)\nplt.xscale('log')\n\nplt.subplot(2,1,2);\nfor depth in unique_depth:\n    const_depth_curve_x   = results_summary_df.loc[results_summary_df.loc[:,'depth'] == depth, 'width']\n    const_depth_curve     = results_summary_df.loc[results_summary_df.loc[:,'depth'] == depth, 'final_MSE_mean']\n    const_depth_curve_std = results_summary_df.loc[results_summary_df.loc[:,'depth'] == depth, 'final_MSE_std']\n    const_depth_curve_std = np.minimum(const_depth_curve_std, const_depth_curve)\n    plt.errorbar(const_depth_curve_x, const_depth_curve, label='%d_layers' %(depth))\n    \nplt.xlabel('Width (log scale)', fontsize=20)\nplt.ylabel('MSE (log scale)', fontsize=20)\nplt.yscale('log')\nplt.xscale('log');","13e61a69":"plt.figure(figsize=(15,10))\nplt.subplots_adjust(left=0.08, right=0.97, bottom=0.05, top=0.97, hspace=0.1)\nmax_depth = 32\n\nunique_width = sorted(results_summary_df['width'].unique())\n\nplt.subplot(2,1,1);\nfor width in unique_width:\n    const_width_curve_x   = results_summary_df.loc[results_summary_df.loc[:,'width'] == width, 'depth']\n    const_width_curve     = results_summary_df.loc[results_summary_df.loc[:,'width'] == width, 'final_MSE_mean']\n    const_width_curve_std = results_summary_df.loc[results_summary_df.loc[:,'width'] == width, 'final_MSE_std']\n    const_width_curve_std = np.minimum(const_width_curve_std, const_width_curve)\n    \n    selected_inds = const_width_curve_x <= max_depth\n    const_width_curve_x   = const_width_curve_x[selected_inds]\n    const_width_curve     = const_width_curve[selected_inds]\n    const_width_curve_std = const_width_curve_std[selected_inds]\n    \n    plt.errorbar(const_width_curve_x, const_width_curve, yerr=const_width_curve_std, label='%d_units' %(width))\n\nplt.ylabel('MSE', fontsize=20)\nplt.legend(fontsize=16, ncol=4)\n\n\nplt.subplot(2,1,2);\nfor width in unique_width:\n    const_width_curve_x   = results_summary_df.loc[results_summary_df.loc[:,'width'] == width, 'depth']\n    const_width_curve     = results_summary_df.loc[results_summary_df.loc[:,'width'] == width, 'final_MSE_mean']\n    const_width_curve_std = results_summary_df.loc[results_summary_df.loc[:,'width'] == width, 'final_MSE_std']\n    const_width_curve_std = np.minimum(const_width_curve_std, const_width_curve)\n    \n    selected_inds = const_width_curve_x <= max_depth\n    const_width_curve_x   = const_width_curve_x[selected_inds]\n    const_width_curve     = const_width_curve[selected_inds]\n    const_width_curve_std = const_width_curve_std[selected_inds]\n\n    plt.errorbar(const_width_curve_x, const_width_curve, yerr=const_width_curve_std, label='%d_units' %(width))\n    \nplt.xlabel('Depth', fontsize=20)\nplt.ylabel('MSE (log scale)', fontsize=20)\nplt.yscale('log')","d98d5c45":"plt.figure(figsize=(15,10))\nplt.subplots_adjust(left=0.08, right=0.97, bottom=0.05, top=0.97, hspace=0.1)\nmax_depth = 32\n\nunique_width = sorted(results_summary_df['width'].unique())\n\nplt.subplot(2,1,1);\nfor width in unique_width:\n    const_width_curve_x   = results_summary_df.loc[results_summary_df.loc[:,'width'] == width, 'depth']\n    const_width_curve     = results_summary_df.loc[results_summary_df.loc[:,'width'] == width, 'final_MSE_mean']\n    const_width_curve_std = results_summary_df.loc[results_summary_df.loc[:,'width'] == width, 'final_MSE_std']\n    const_width_curve_std = np.minimum(const_width_curve_std, const_width_curve)\n    \n    selected_inds = const_width_curve_x <= max_depth\n    const_width_curve_x   = const_width_curve_x[selected_inds]\n    const_width_curve     = const_width_curve[selected_inds]\n    const_width_curve_std = const_width_curve_std[selected_inds]\n    \n    plt.errorbar(const_width_curve_x, const_width_curve, label='%d_units' %(width))\n\nplt.ylabel('MSE', fontsize=20)\nplt.legend(fontsize=16, ncol=4)\n\n\nplt.subplot(2,1,2);\nfor width in unique_width:\n    const_width_curve_x   = results_summary_df.loc[results_summary_df.loc[:,'width'] == width, 'depth']\n    const_width_curve     = results_summary_df.loc[results_summary_df.loc[:,'width'] == width, 'final_MSE_mean']\n    const_width_curve_std = results_summary_df.loc[results_summary_df.loc[:,'width'] == width, 'final_MSE_std']\n    const_width_curve_std = np.minimum(const_width_curve_std, const_width_curve)\n    \n    selected_inds = const_width_curve_x <= max_depth\n    const_width_curve_x   = const_width_curve_x[selected_inds]\n    const_width_curve     = const_width_curve[selected_inds]\n    const_width_curve_std = const_width_curve_std[selected_inds]\n\n    plt.errorbar(const_width_curve_x, const_width_curve, label='%d_units' %(width))\n    \nplt.xlabel('Depth', fontsize=20)\nplt.ylabel('MSE (log scale)', fontsize=20)\nplt.yscale('log')","a8d74145":"plt.figure(figsize=(15,10))\nplt.subplots_adjust(left=0.08, right=0.97, bottom=0.05, top=0.97, hspace=0.1)\nmax_depth = 32\n\nunique_width = sorted(results_summary_df['width'].unique())\n\nplt.subplot(2,1,1);\nfor width in unique_width:\n    const_width_curve_x   = results_summary_df.loc[results_summary_df.loc[:,'width'] == width, 'depth']\n    const_width_curve     = results_summary_df.loc[results_summary_df.loc[:,'width'] == width, 'final_MSE_mean']\n    const_width_curve_std = results_summary_df.loc[results_summary_df.loc[:,'width'] == width, 'final_MSE_std']\n    const_width_curve_std = np.minimum(const_width_curve_std, const_width_curve)\n    \n    selected_inds = const_width_curve_x <= max_depth\n    const_width_curve_x   = const_width_curve_x[selected_inds]\n    const_width_curve     = const_width_curve[selected_inds]\n    const_width_curve_std = const_width_curve_std[selected_inds]\n    \n    plt.errorbar(const_width_curve_x, const_width_curve, yerr=const_width_curve_std, label='%d_units' %(width))\n\nplt.ylabel('MSE', fontsize=20)\nplt.legend(fontsize=16, ncol=4)\nplt.xscale('log')\n\nplt.subplot(2,1,2);\nfor width in unique_width:\n    const_width_curve_x   = results_summary_df.loc[results_summary_df.loc[:,'width'] == width, 'depth']\n    const_width_curve     = results_summary_df.loc[results_summary_df.loc[:,'width'] == width, 'final_MSE_mean']\n    const_width_curve_std = results_summary_df.loc[results_summary_df.loc[:,'width'] == width, 'final_MSE_std']\n    const_width_curve_std = np.minimum(const_width_curve_std, const_width_curve)\n    \n    selected_inds = const_width_curve_x <= max_depth\n    const_width_curve_x   = const_width_curve_x[selected_inds]\n    const_width_curve     = const_width_curve[selected_inds]\n    const_width_curve_std = const_width_curve_std[selected_inds]\n\n    plt.errorbar(const_width_curve_x, const_width_curve, yerr=const_width_curve_std, label='%d_units' %(width))\n    \nplt.xlabel('Depth (log scale)', fontsize=20)\nplt.ylabel('MSE (log scale)', fontsize=20)\nplt.xscale('log')\nplt.yscale('log')","005e01e5":"plt.figure(figsize=(15,10))\nplt.subplots_adjust(left=0.08, right=0.97, bottom=0.05, top=0.97, hspace=0.1)\nmax_depth = 16\n\nunique_width = sorted(results_summary_df['width'].unique())\n\nplt.subplot(2,1,1);\nfor width in unique_width:\n    const_width_curve_x   = results_summary_df.loc[results_summary_df.loc[:,'width'] == width, 'depth']\n    const_width_curve     = results_summary_df.loc[results_summary_df.loc[:,'width'] == width, 'final_MSE_mean']\n    const_width_curve_std = results_summary_df.loc[results_summary_df.loc[:,'width'] == width, 'final_MSE_std']\n    const_width_curve_std = np.minimum(const_width_curve_std, const_width_curve)\n    \n    selected_inds = const_width_curve_x <= max_depth\n    const_width_curve_x   = const_width_curve_x[selected_inds]\n    const_width_curve     = const_width_curve[selected_inds]\n    const_width_curve_std = const_width_curve_std[selected_inds]\n    \n    plt.errorbar(const_width_curve_x, const_width_curve, label='%d_units' %(width))\n\nplt.ylabel('MSE', fontsize=20)\nplt.legend(fontsize=16, ncol=4)\nplt.xscale('log')\n\nplt.subplot(2,1,2);\nfor width in unique_width:\n    const_width_curve_x   = results_summary_df.loc[results_summary_df.loc[:,'width'] == width, 'depth']\n    const_width_curve     = results_summary_df.loc[results_summary_df.loc[:,'width'] == width, 'final_MSE_mean']\n    const_width_curve_std = results_summary_df.loc[results_summary_df.loc[:,'width'] == width, 'final_MSE_std']\n    const_width_curve_std = np.minimum(const_width_curve_std, const_width_curve)\n    \n    selected_inds = const_width_curve_x <= max_depth\n    const_width_curve_x   = const_width_curve_x[selected_inds]\n    const_width_curve     = const_width_curve[selected_inds]\n    const_width_curve_std = const_width_curve_std[selected_inds]\n\n    plt.errorbar(const_width_curve_x, const_width_curve, label='%d_units' %(width))\n    \nplt.xlabel('Depth (log scale)', fontsize=20)\nplt.ylabel('MSE (log scale)', fontsize=20)\nplt.xscale('log')\nplt.yscale('log')","5d60823d":"# Data access helper functions","38973333":"# without error bars","b3b719d5":"# Show number of iterations needed to get to 95% variance explained for all models","4a7bca83":"# Show without error bars","987d334d":"# Show model accuracy vs model complexity (x axis is depth in log scale)","2cb33c8c":"# Show \"learning dynamics\" by stacking bar plots","8b721d48":"# Show model accuracy vs model complexity (x axis is depth)","ace0cbbe":"# Collect results into a simple dataframe","4bca879b":"# Show a bar plot with all model's final accuracy\n","fef0eeec":"# Show without error bars","7a9f85b2":"# Load previously saved results and access the results dictionary","49d250d9":"# Show model accuracy vs model complexity (x axis is width in log scale)","e780055b":"# Display all learning curves of the first model","3f10cc3e":"# Display Best learning curves for all student models","5bfaac6b":"# Show model accuracy vs model complexity (x axis is width)","803c0de7":"# Display average learning curves of all student models"}}