{"cell_type":{"2d8c40ac":"code","9a46acdb":"code","c86d9955":"code","d2a61366":"code","88ecb605":"code","9a84efa4":"code","279ce323":"code","22425ad5":"code","ab7efab7":"code","4a331913":"code","f91b162f":"code","abb8b4e1":"code","4816ed39":"code","3774fe55":"code","238b490b":"code","35668301":"code","8cd1c284":"code","8168631a":"code","b444f78e":"code","d4a12c3d":"code","6260cbf2":"code","5ed3ac08":"code","d27ca352":"code","19c9ea00":"code","84749775":"code","8779bc9c":"code","5c326d83":"code","73f00a31":"code","52a58843":"code","c1ab3dc7":"code","8baec774":"code","8f45d6d9":"code","e1244fff":"code","03964667":"code","6d3b0094":"code","aee60cd4":"code","fbd361bb":"code","f9c852d9":"code","3f531ce3":"code","62a43084":"code","61108c60":"code","8b31b107":"code","63434bed":"markdown","a85587b8":"markdown","f88cbde9":"markdown","cfb013d1":"markdown","7740c28f":"markdown","cc892fe9":"markdown","a470beb7":"markdown","b52971fd":"markdown","8105669d":"markdown","bb42e8fc":"markdown","d4d44381":"markdown","c504ace1":"markdown","5fd599fd":"markdown"},"source":{"2d8c40ac":"import os\nimport seaborn as sn ;sn.set(font_scale=1.2)\nimport matplotlib.pyplot as plt             \nimport cv2                                 \nimport tensorflow as tf\nimport numpy as np \nimport pandas as pd \n\nfrom tensorflow import keras\nfrom tensorflow.keras.preprocessing import image\nfrom sklearn import metrics\nfrom sklearn.utils import shuffle\nfrom sklearn.metrics import confusion_matrix, classification_report, accuracy_score, recall_score\nfrom tqdm import tqdm","9a46acdb":"!nvidia-smi","c86d9955":"class_names = ['unsaturated','metastable','intermediate', 'labile']\nclass_names_label = {class_name:i \n                     for i, class_name in enumerate(class_names)\n                    }\n\nnb_classes = len(class_names)\n\nIMAGE_SIZE = (150, 150)","d2a61366":"def load_data():\n    \"\"\"\n        Load the data:\n            - 800 images to train the network.\n            - 200 images to evaluate how accurately the network learned to classify images.\n    \"\"\"\n    \n    datasets = ['..\/input\/sugar-crystal-v2\/sugar_dataset_kaggle_v2\/train', '..\/input\/sugar-crystal-v2\/sugar_dataset_kaggle_v2\/test']\n    output = []\n    \n    # Iterate through training and test sets\n    for dataset in datasets:\n        \n        images = []\n        labels = []\n        \n        print(\"Loading {}\".format(dataset))\n        \n        # Iterate through each folder corresponding to a category\n        for folder in os.listdir(dataset):\n            label = class_names_label[folder]\n            \n            # Iterate through each image in our folder\n            for file in tqdm(os.listdir(os.path.join(dataset, folder))):\n                \n                # Get the path name of the image\n                img_path = os.path.join(os.path.join(dataset, folder), file)\n                \n                # Open and resize the img\n                image = cv2.imread(img_path)\n                image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n                image = cv2.resize(image, IMAGE_SIZE) \n                \n                # Append the image and its corresponding label to the output\n                images.append(image)\n                labels.append(label)\n                \n        images = np.array(images, dtype = 'float32')\n        labels = np.array(labels, dtype = 'int32')   \n        \n        output.append((images, labels))\n\n    return output","88ecb605":"# Train_images = \u0e02\u0e49\u0e2d\u0e21\u0e39\u0e25\u0e2a\u0e33\u0e2b\u0e23\u0e31\u0e1a\u0e43\u0e0a\u0e49\u0e1c\u0e36\u0e01 \n# Train_labels = \u0e02\u0e49\u0e2d\u0e21\u0e39\u0e25\u0e2a\u0e33\u0e2b\u0e23\u0e31\u0e1a\u0e43\u0e0a\u0e49\u0e1c\u0e36\u0e01 \n\n# Test_images = \u0e02\u0e49\u0e2d\u0e21\u0e39\u0e25\u0e2a\u0e33\u0e2b\u0e23\u0e31\u0e1a\u0e43\u0e0a\u0e49\u0e1c\u0e36\u0e01 \n# Test_labels = \u0e02\u0e49\u0e2d\u0e21\u0e39\u0e25\u0e2a\u0e33\u0e2b\u0e23\u0e31\u0e1a\u0e43\u0e0a\u0e49\u0e1c\u0e36\u0e01\n\n(train_images, train_labels), (test_images, test_labels) = load_data()\n","9a84efa4":"# shuffle data\n\ntrain_images, train_labels = shuffle(train_images, train_labels, random_state=42)\n\nn_train = train_labels.shape[0]\nn_test = test_labels.shape[0]\n\nprint (\"Number of training examples: {}\".format(n_train))\nprint (\"Number of testing examples: {}\".format(n_test))\nprint (\"Each image is of size: {}\".format(IMAGE_SIZE))","279ce323":"_, train_counts = np.unique(train_labels, return_counts=True)\n_, test_counts = np.unique(test_labels, return_counts=True)\npd.DataFrame({'train': train_counts,\n                    'test': test_counts}, \n             index=class_names\n            ).plot.bar()\nplt.show()\n\nplt.pie(train_counts,\n        explode=(0, 0, 0, 0), \n        labels=class_names,\n        autopct='%1.1f%%')\n\nplt.axis('equal')\n# plt.title('\u0e2a\u0e31\u0e14\u0e2a\u0e48\u0e27\u0e19\u0e02\u0e2d\u0e07\u0e41\u0e15\u0e48\u0e25\u0e30 Class \u0e17\u0e35\u0e48\u0e19\u0e33\u0e21\u0e32\u0e43\u0e0a\u0e49\u0e1d\u0e36\u0e01')\nplt.show()","22425ad5":"class Data_augmentation:\n    def __init__(self, path, image_name):\n        '''\n        Import image\n        :param path: Path to the image\n        :param image_name: image name\n        '''\n        self.path = path\n        self.name = image_name\n        print(path+image_name)\n        self.image = cv2.imread(path+image_name)\n\n    def rotate(self, image, angle=90, scale=1.0):\n        '''\n        Rotate the image\n        :param image: image to be processed\n        :param angle: Rotation angle in degrees. \n        :Positive values mean counter-clockwise rotation \n            (the coordinate origin is assumed to be the top-left corner).\n        :param scale: Isotropic scale factor.\n        '''\n        w = image.shape[1]\n        h = image.shape[0]\n        #rotate matrix\n        M = cv2.getRotationMatrix2D((w\/2,h\/2), angle, scale)\n        #rotate\n        image = cv2.warpAffine(image,M,(w,h))\n        return image\n\n    def flip(self, image, vflip=False, hflip=False):\n        '''\n        Flip the image\n        :param image: image to be processed\n        :param vflip: whether to flip the image vertically\n        :param hflip: whether to flip the image horizontally\n        '''\n        if hflip or vflip:\n            if hflip and vflip:\n                c = -1\n            else:\n                c = 0 if vflip else 1\n            image = cv2.flip(image, flipCode=c)\n        return image \n    \n    \n    def image_augment(self, save_path): \n        '''\n        :Create the new image with imge augmentation\n        :param path: the path to store the new image\n        ''' \n        img = self.image.copy()\n        img_flip = self.flip(img, vflip=True, hflip=False)\n        img_rot = self.rotate(img)\n        img_gaussian = self.add_GaussianNoise(img)\n        \n        name_int = self.name[:len(self.name)-4]\n        cv2.imwrite(save_path+'%s' %str(name_int)+'_vflip.jpg', img_flip)\n        cv2.imwrite(save_path+'%s' %str(name_int)+'_rot.jpg', img_rot)\n        cv2.imwrite(save_path+'%s' %str(name_int)+'_GaussianNoise.jpg', img_gaussian)\n    \n    \n    def main(file_dir,output_path):\n        for root, _, files in os.walk(file_dir):\n            print(root)\n        for file in files:\n            raw_image = Data_augmentation(root,file)\n            raw_image.image_augment(output_path)","ab7efab7":"train_images = train_images \/ 255.0 \ntest_images = test_images \/ 255.0","4a331913":"def display_random_image(class_names, images, labels):\n    \"\"\"\n        Display a random image from the images array and its correspond label from the labels array.\n    \"\"\"\n    \n    index = np.random.randint(images.shape[0])\n    plt.figure()\n    plt.imshow(images[index])\n    plt.xticks([])\n    plt.yticks([])\n    plt.grid(True)\n    plt.title('Image #{} : '.format(index) + class_names[labels[index]])\n    plt.show()","f91b162f":"display_random_image(class_names, train_images, train_labels)","abb8b4e1":"def display_examples(class_names, images, labels):\n\n    fig = plt.figure(figsize=(10,10))\n    fig.suptitle(\"Some examples of Sugar Crystal in The dataset\", fontsize=20)\n    for i in range(16):\n        plt.subplot(4,4,i+1)\n        plt.xticks([])\n        plt.yticks([])\n        plt.grid(True)\n        plt.imshow(images[i], cmap=plt.cm.binary)\n        plt.xlabel(class_names[labels[i]])\n    plt.show()","4816ed39":"display_examples(class_names, train_images, train_labels)","3774fe55":"model = tf.keras.Sequential([\n    tf.keras.layers.Conv2D(16, (3, 3), activation = 'relu', input_shape = (150, 150, 3)), \n    tf.keras.layers.MaxPooling2D(2,2),\n    tf.keras.layers.Conv2D(32, (3, 3), activation = 'relu'),\n    tf.keras.layers.MaxPooling2D(2,2),\n    tf.keras.layers.Conv2D(64, (3, 3), activation = 'relu'),\n    tf.keras.layers.MaxPooling2D(2,2),\n    tf.keras.layers.Flatten(),\n    tf.keras.layers.Dense(128, activation=tf.nn.relu),\n    tf.keras.layers.Dense(4, activation=tf.nn.softmax)\n])\n\nmodel.summary()","238b490b":"# Model Diagram\nkeras.utils.plot_model(model, show_shapes=True)","35668301":"model.compile(optimizer = 'adam', loss = 'sparse_categorical_crossentropy', metrics=['accuracy'])\n\nhistory = model.fit(train_images, train_labels, batch_size=128, epochs=15, validation_split = 0.2)","8cd1c284":"def plot_accuracy_loss(history):\n    \"\"\"\n        Plot the accuracy and the loss during the training of the nn.\n    \"\"\"\n    fig = plt.figure(figsize=(24,16))\n\n    # Plot accuracy\n    plt.subplot(221)\n    plt.plot(history.history['accuracy'],'bo--', label = \"acc\")\n    plt.plot(history.history['val_accuracy'], 'ro--', label = \"val_acc\")\n    plt.title(\"train_acc vs val_acc\")\n    plt.ylabel(\"accuracy\")\n    plt.xlabel(\"epochs\")\n    plt.legend()\n\n    # Plot loss function\n    plt.subplot(222)\n    plt.plot(history.history['loss'],'bo--', label = \"loss\")\n    plt.plot(history.history['val_loss'], 'ro--', label = \"val_loss\")\n    plt.title(\"train_loss vs val_loss\")\n    plt.ylabel(\"loss\")\n    plt.xlabel(\"epochs\")\n\n    plt.legend()\n    plt.show()","8168631a":"plot_accuracy_loss(history)\n\n# Evaluate Model SimpleDCNNs\n\npredictions = model.predict(test_images)     # Vector of probabilities\npred_labels = np.argmax(predictions, axis = 1) # We take the highest probability\n\nprint(metrics.classification_report(test_labels, pred_labels,target_names=class_names))\nprint(\"Accuracy_score : {}\" .format(metrics.accuracy_score(test_labels, pred_labels)))\nprint(\"Precision_score : {}\".format(metrics.precision_score(test_labels, pred_labels, average ='weighted')))\nprint(\"recall : {}\" .format(metrics.recall_score(test_labels, pred_labels ,average ='weighted')))","b444f78e":"def print_mislabeled_images(class_names, test_images, test_labels, pred_labels):\n    \"\"\"\n        Print 16 examples of mislabeled images by the classifier, e.g when test_labels != pred_labels\n    \"\"\"\n    BOO = (test_labels == pred_labels)\n    mislabeled_indices = np.where(BOO == 0)\n    mislabeled_images = test_images[mislabeled_indices]\n    mislabeled_labels = pred_labels[mislabeled_indices]\n\n    title = \"Some examples of mislabeled images by the classifier:\"\n    display_examples(class_names,  mislabeled_images, mislabeled_labels)","d4a12c3d":"print_mislabeled_images(class_names, test_images, test_labels, pred_labels)","6260cbf2":"from keras.applications import InceptionV3\nfrom keras.preprocessing import image\nfrom keras.applications.vgg16 import preprocess_input\n\nmodel3 = InceptionV3(weights='imagenet', include_top=False)\n\ntrain_features = model3.predict(train_images)\ntest_features = model3.predict(test_images)\n\nprint (\"shape of train,test_image input :\",train_images.shape,test_images.shape)\nprint (\"shape of train,test_feature output :\",train_features.shape,test_features.shape)\n\nn_train, x, y, z = train_features.shape \nn_test, x, y, z = test_features.shape\nnumFeatures = x * y * z\n\nprint (\"overview of feature for custom model InceptionV3:\", numFeatures)","5ed3ac08":"from sklearn import decomposition\n\npca = decomposition.PCA(n_components = 2)\n\nX = train_features.reshape((n_train, x*y*z))\npca.fit(X)\n\nC = pca.transform(X) \nC1 = C[:,0]\nC2 = C[:,1]\n\n# Principal component analysis (PCA). InceptionV3\n\nplt.subplots(figsize=(10,10))\n\nfor i, class_name in enumerate(class_names):\n    plt.scatter(C1[train_labels == i][:], C2[train_labels == i][:], label = class_name, alpha=0.4)\nplt.legend()\nplt.title(\"PCA Projection InceptionV3 Before Transfer_Learning\")\nplt.show()","d27ca352":"model_InV3 = tf.keras.Sequential([\n    tf.keras.layers.Flatten(input_shape = (x, y, z)),\n    tf.keras.layers.Dense(50, activation=tf.nn.relu),\n    tf.keras.layers.Dense(4, activation=tf.nn.softmax)\n])\nmodel_InV3.summary()\n\nmodel_InV3.compile(optimizer = 'adam', loss = 'sparse_categorical_crossentropy', metrics=['accuracy'])\n\nhistory_InV3 = model_InV3.fit(train_features, train_labels, batch_size=128, epochs=15, validation_split = 0.2)","19c9ea00":"# plot Graph relationship between accuracy and interative of model IncetionV3 training\n\nplot_accuracy_loss(history_InV3)\n\n# Evaludate Model Transfer Learning InceptionV3 custom top\n\npredictions = model_InV3.predict(test_features)    \npred_labels = np.argmax(predictions, axis = 1)\n\nprint(metrics.classification_report(test_labels, pred_labels,target_names=class_names))\nprint(\"Accuracy_score : {}\" .format(metrics.accuracy_score(test_labels, pred_labels)))\nprint(\"Precision_score : {}\".format(metrics.precision_score(test_labels, pred_labels, average ='weighted')))\nprint(\"recall : {}\" .format(metrics.recall_score(test_labels, pred_labels ,average ='weighted')))","84749775":"from keras.applications import ResNet50V2\n\nmodel4 = ResNet50V2(weights='imagenet', include_top=False)\n\ntrain_features = model4.predict(train_images)\ntest_features = model4.predict(test_images)\n\nprint (\"shape of train,test_image input :\",train_images.shape,test_images.shape)\nprint (\"shape of train,test_feature output :\",train_features.shape,test_features.shape)\n\nn_train, x, y, z = train_features.shape \nn_test, x, y, z = test_features.shape\nnumFeatures = x * y * z\n\nprint (\"overview of feature for custom model ResNet50V2:\", numFeatures)","8779bc9c":"pca = decomposition.PCA(n_components = 2)\n\nX = train_features.reshape((n_train, x*y*z))\npca.fit(X)\n\nC = pca.transform(X) \nC1 = C[:,0]\nC2 = C[:,1]\n\n# Principal component analysis (PCA). InceptionV3\n\nplt.subplots(figsize=(10,10))\n\nfor i, class_name in enumerate(class_names):\n    plt.scatter(C1[train_labels == i][:], C2[train_labels == i][:], label = class_name, alpha=0.4)\nplt.legend()\nplt.title(\"PCA Projection ResNet50V2 Before Transfer_Learning\")\nplt.show()","5c326d83":"model_ResV2 = tf.keras.Sequential([\n    tf.keras.layers.Flatten(input_shape = (x, y, z)),\n    tf.keras.layers.Dense(50, activation=tf.nn.relu),\n    tf.keras.layers.Dense(4, activation=tf.nn.softmax)\n])\n\nmodel_ResV2.summary()\n\nmodel_ResV2.compile(optimizer = 'adam', loss = 'sparse_categorical_crossentropy', metrics=['accuracy'])\n\nhistory_ResV2 = model_ResV2.fit(train_features, train_labels, batch_size=128, epochs=15, validation_split = 0.2)","73f00a31":"# plot Graph  relationship between accuracy and interative of model ResNet50V2 training\n\nplot_accuracy_loss(history_ResV2)\n\n# Evaludate Model Transfer Learning ResNet50V2 custom top\n\npredictions = model_ResV2.predict(test_features)    \npred_labels = np.argmax(predictions, axis = 1)\n\nprint(metrics.classification_report(test_labels, pred_labels,target_names=class_names))\nprint(\"Accuracy_score : {}\" .format(metrics.accuracy_score(test_labels, pred_labels)))\nprint(\"Precision_score : {}\".format(metrics.precision_score(test_labels, pred_labels, average ='weighted')))\nprint(\"recall : {}\" .format(metrics.recall_score(test_labels, pred_labels ,average ='weighted')))","52a58843":"from keras.applications.vgg16 import VGG16\n\nmodel5 = VGG16(weights='imagenet', include_top=False)\n\ntrain_features = model5.predict(train_images)\ntest_features = model5.predict(test_images)\n\nprint (\"shape of train,test_image input :\",train_images.shape,test_images.shape)\nprint (\"shape of train,test_feature output :\",train_features.shape,test_features.shape)\n\nn_train, x, y, z = train_features.shape \nn_test, x, y, z = test_features.shape\nnumFeatures = x * y * z\n\nprint (\"overview of feature for custom model VGG16:\", numFeatures)","c1ab3dc7":"pca = decomposition.PCA(n_components = 2)\n\nX = train_features.reshape((n_train, x*y*z))\npca.fit(X)\n\nC = pca.transform(X) \nC1 = C[:,0]\nC2 = C[:,1]\n\n# Principal component analysis (PCA). VGG16\n\nplt.subplots(figsize=(10,10))\n\nfor i, class_name in enumerate(class_names):\n    plt.scatter(C1[train_labels == i][:], C2[train_labels == i][:], label = class_name, alpha=0.4)\nplt.legend()\nplt.title(\"PCA Projection VGG16 Before Transfer_Learning\")\nplt.show()","8baec774":"model_VGG16 = tf.keras.Sequential([\n    tf.keras.layers.Flatten(input_shape = (x, y, z)),\n    tf.keras.layers.Dense(50, activation=tf.nn.relu),\n    tf.keras.layers.Dense(4, activation=tf.nn.softmax)\n])\n\nmodel_VGG16.summary()\n\nmodel_VGG16.compile(optimizer = 'adam', loss = 'sparse_categorical_crossentropy', metrics=['accuracy'])\n\nhistory_VGG16 = model_VGG16.fit(train_features, train_labels, batch_size=128, epochs=15, validation_split = 0.2)","8f45d6d9":"# plot Graph  relationship between accuracy and interative of model VGG16 training\n\nplot_accuracy_loss(history_VGG16)\n\n# Evaludate Model Transfer Learning VGG16 custom top\n\npredictions = model_VGG16.predict(test_features)    \npred_labels = np.argmax(predictions, axis = 1)\n\nprint(metrics.classification_report(test_labels, pred_labels,target_names=class_names))\nprint(\"Accuracy_score : {}\" .format(metrics.accuracy_score(test_labels, pred_labels)))\nprint(\"Precision_score : {}\".format(metrics.precision_score(test_labels, pred_labels, average ='weighted')))\nprint(\"recall : {}\" .format(metrics.recall_score(test_labels, pred_labels ,average ='weighted')))","e1244fff":"np.random.seed(seed=200)\n\n# Number of estimators\nn_estimators = 10\n\n# Proporition of samples to use to train each training\nmax_samples = 0.8\n\nmax_samples *= n_train\nmax_samples = int(max_samples)\n\nprint( \"train set\", n_train, \"samples per epochs:\", max_samples) ","03964667":"models = list()\nrandom = np.random.randint(50, 100, size = n_estimators)\n\n\n# \u0e17\u0e33\u0e01\u0e32\u0e23\u0e2a\u0e38\u0e48\u0e21 iterate 1-10 \nfor i in range(n_estimators):\n    #from feature of VGG16 shape of train,test_feature output : (?, 4, 4, 512) (?, 4, 4, 512)\n    model = tf.keras.Sequential([ tf.keras.layers.Flatten(input_shape = (x, y, z)),                        \n                                # One layer with random size\n                                    tf.keras.layers.Dense(random[i],activation=tf.nn.relu),\n                                    tf.keras.layers.Dense(4, activation=tf.nn.softmax)\n                                ])\n    \n    model.compile(optimizer = 'adam', loss = 'sparse_categorical_crossentropy', metrics=['accuracy'])\n    \n    # Store model\n    models.append(model)\n\nmodel.summary()","6d3b0094":"histories = []\n\nfor i in range(n_estimators):\n    # Train each model on a bag of the training data\n    print (\"interate\",i+1)\n    train_idx = np.random.choice(len(train_features), size = max_samples)\n    histories.append(models[i].fit(train_features[train_idx], train_labels[train_idx], batch_size=128, epochs=10, validation_split = 0.1))\n    ","aee60cd4":"predictions = []\nfor i in range(n_estimators):\n    predictions.append(models[i].predict(test_features))\n    \npredictions = np.array(predictions)\npredictions = predictions.sum(axis = 0)\npred_labels = predictions.argmax(axis =1)\n\nprint(metrics.classification_report(test_labels, pred_labels,target_names=class_names))\nprint(\"Accuracy_score : {}\" .format(metrics.accuracy_score(test_labels, pred_labels)))\nprint(\"Precision_score : {}\".format(metrics.precision_score(test_labels, pred_labels, average ='weighted')))\nprint(\"recall : {}\" .format(metrics.recall_score(test_labels, pred_labels ,average ='weighted')))","fbd361bb":"from keras.models import Model\n\nmodel = VGG16(weights='imagenet', include_top=False)\nmodel = Model(inputs=model.inputs, outputs=model.layers[-5].output)\n\ntrain_features = model.predict(train_images)\ntest_features = model.predict(test_images)\n\nprint (\"shape of train,test_image input :\",train_images.shape,test_images.shape)\nprint (\"shape of train,test_feature output :\",train_features.shape,test_features.shape)","f9c852d9":"from keras.layers import Input, Dense, Conv2D, Activation , MaxPooling2D, Flatten\n\nmodel = VGG16(weights='imagenet', include_top=False)\n\ninput_shape = model.layers[-4].get_input_shape_at(0) # get the input shape of desired layer\nlayer_input = Input(shape = (9, 9, 512)) # a new input tensor to be able to feed the desired layer\n\n\nx = layer_input\nfor layer in model.layers[-4::1]:\n    x = layer(x)\n    \nx = Conv2D(64, (3, 3), activation='relu')(x)\nx = MaxPooling2D(pool_size=(2, 2))(x)\nx = Flatten()(x)\nx = Dense(100,activation='relu')(x)\nx = Dense(4,activation='softmax')(x)\n\n# create the model\nnew_model = Model(layer_input, x)\n\nnew_model.summary()","3f531ce3":"keras.utils.plot_model(new_model, show_shapes=True)","62a43084":"new_model.compile(optimizer = 'adam', loss = 'sparse_categorical_crossentropy', metrics=['accuracy'])\n\nhistory = new_model.fit(train_features, train_labels, batch_size=128, epochs=10, validation_split = 0.2)","61108c60":"plot_accuracy_loss(history)\n\npredictions = new_model.predict(test_features)    \npred_labels = np.argmax(predictions, axis = 1)\n\nprint(metrics.classification_report(test_labels, pred_labels,target_names=class_names))\nprint(\"Accuracy_score : {}\" .format(metrics.accuracy_score(test_labels, pred_labels)))\nprint(\"Precision_score : {}\".format(metrics.precision_score(test_labels, pred_labels, average ='weighted')))\nprint(\"recall : {}\" .format(metrics.recall_score(test_labels, pred_labels ,average ='weighted')))","8b31b107":"CM = confusion_matrix(test_labels, pred_labels)\nfig, ax = plt.subplots(figsize=(10, 8))\n\nsn.heatmap((4*CM\/np.sum(CM)), annot=True, fmt='.1%',\n           annot_kws={\"size\": 20}, cmap='Blues',\n           xticklabels=class_names, \n           yticklabels=class_names, ax = ax)\n\nax.set_title('Confusion matrix')\nplt.show()","63434bed":"# Augmentation\n\n* We pass the rescale option to the ImageDataGenerator object. The rescale=1.\/255 option is a very IMPORTANT parameter. It normalizes the image pixel values to have zero mean and standard deviation of 1. It helps your model to generally learn and update its parameters efficiently.\n* The second set of options is Image augmentation. They tell the ImageDataGenerator to randomly apply some transformation to the Image. This will help to augment our data-set and improve generalization.\n* Here we also create an ImageDataGenerator object for our validation set. Note: we don\u2019t do data augmentation here. We only perform rescale.","a85587b8":"# InceptionV3 \n","f88cbde9":"\n## Related theories\n* In boiling, syrup and crystallization, sugar is an important process that can be considered as the primary process for transforming sugar into a liquid syrup solution into crystalline sugar into solid crystalline form. The general principles of the crystallization process are Must extract the latent heat of the crystalline solid Out of the solution to be crystallized To cause a liquid to solid transition at the temperature of that crystallization Therefore, the crystallization process must Know the relationship of the mass distribution to be crystallized in the liquid and solid phases. Algorithm for the supersaturation of the concentrated solution. And the process of raising crystals to meet the standard requirements In this regard, both the boiling and chewing process must be controlled. Crystallization process The process of raising crystals And the process of crystallization To get the maximum amount of sugar crystals However, in the process of boiling, simmering and crystallization, this sugar is more difficult to control than normal conditions.\n\n![supersaturation cofficient](https:\/\/nzifst.org.nz\/resources\/unitoperations\/unopsassets\/fig9-8.gif)\n\n* This is because the liquid involved in this process is in the form of a very concentrated syrup. Due to its very high viscosity, it is difficult to control the flow or the flow through the piping system, and the heat transfer in the system must also be considered. In which each phase of this extremely saturated concentration Divided by the coefficient of supersaturated concentration is SC. From 1.00 - 1.20 is called the Metastable Zone. SC. Between 1.20 - 1.30 is called Intermediate Zone and from SC 1.30 - 1.40 is called Labile Zone. Each The range can be applied to control the sugar crystallization process depending on the technique of each plant. Each phase of this supersaturated concentration level has a characteristic of believed water.\n\n","cfb013d1":"# Data Collection\n\n* The total number of sugar crystal images used (Data Set) were 1,000 images, divided into 800 Training Sets and 200 Testing Data Sets by taking crystalline photographs. The crystal images were then separated by Experienced stewers To divide the crystals into 4 Classes by the expert of sugar simmer Classified by zones of crystallization conditions (Supersaturation Coefficient) is Unstaturated Zone, Metastable Zone, Intermediate Zone, Labile Zone, which the number of crystals that can be stored in each zone should be kept at least about 250 images per zone.\n\n* And confirming the validity of the recorded crystal images was done by the method of concluding the opinions of 5 expert technician and the verification process of the crystal photographs by making a test form for 32 images per One employee with the same number of random crystal images tested in this questionnaire\nThis test has an average accuracy of 95% and the value is between 92 - 98%.\n","7740c28f":"# Interative estimators VGG16","cc892fe9":"# Fine-Tuning Model VGG16","a470beb7":"# ResNet50V2","b52971fd":"# Transfer Learning\n   1. VGG16 https:\/\/keras.io\/api\/applications\/vgg\/#vgg16-function\n   2. InceptionV3 https:\/\/keras.io\/api\/applications\/inceptionv3\/\n   3. ResNetV2 https:\/\/keras.io\/api\/applications\/resnet\/#resnet50v2-function","8105669d":"# Confusion Metrix Multiclass","bb42e8fc":"# Evaluate Model","d4d44381":"# VGG16","c504ace1":"# ABSTRACT\n\n* Nowadays, artificial intelligent control is essential to replace experienced workers. The correct classification of sugar crystals during the production process is the basis for the control of the sugar crystallization process. Correct Classification of sugar crystals is the basis necessary for automatic control of process. This research uses the principles of deep learning using a neural network to identify the crystallization of sugar from the actual production process of sugar factories in Thailand. Performance was measured and compared with the Fine-tuning VGG16 model. It was accurate to identify sugar crystals between 82% and 90 %  of  four classes sugar crystal images classified by the crystallization conditions. The results of this study also show that this model is more accurate than other models. It can be used as a benchmark for monitoring the crystallization of sugar production processes. It is also the basis of an artificial intelligent control system based on transcribing human expertise.\n","5fd599fd":"# Model Simple DCNNs "}}