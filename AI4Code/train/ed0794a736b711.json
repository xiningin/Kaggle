{"cell_type":{"2c0249ac":"code","f737c232":"code","f7151177":"code","5268fc6c":"code","baf27099":"code","d8921d01":"code","528a22de":"code","a7f10a0e":"code","00df74f5":"code","cf288f76":"code","64431668":"code","44fc9202":"code","b78f91e2":"code","4526a728":"code","3ab070f9":"code","8e36be0a":"code","179427ec":"code","bc2dc73a":"markdown","3c185888":"markdown","5a69e513":"markdown","73bd483e":"markdown","85cac934":"markdown","963027d3":"markdown","36df474b":"markdown"},"source":{"2c0249ac":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","f737c232":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nimport zipfile\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns","f7151177":"# extract and read files\ndata_path = \"\/kaggle\/input\/instacart-market-basket-analysis\/\"\ndata_folder = os.listdir(data_path)\nmissing_value_formats = [\"n.a.\",\"?\",\"NA\",\"n\/a\", \"na\", \"--\",\"-\"]\n\nfor file in data_folder:\n    if file.endswith('.zip'):\n            with zipfile.ZipFile(data_path+file, 'r') as zip_ref:\n                zip_ref.extractall(\"kaggle\/working\/\")\n\ndata = {}\nfor file in os.listdir(\"kaggle\/working\/\"):\n    if file.endswith('.csv'):\n        data[file[:file.find('.')]] = pd.read_csv(\"kaggle\/working\/\"+file, na_values = missing_value_formats)\n        \ndata.keys()\n# dict_keys(['orders', 'order_products__prior', 'sample_submission', 'products', \n# 'departments', 'aisles', 'order_products__train'])","5268fc6c":"# have a look at all the files\n\ndef summarize_data(df):\n    print(\"\\nOverview\")\n    display(df.head())\n    print(\"\\nSummary\")\n    display(df.describe(include='all'))\n    print(\"\\nNull Values\")\n    display(df.isnull().sum()\/len(df))\n\nfor file in data.keys():\n    print(\"\\n\\n\",file)\n    summarize_data(data[file])","baf27099":"# Prior and Train order datasets have the same columns orders from different time frame - These need to be concatenated\n\nmaster_df = pd.concat([data['order_products__prior'], data['order_products__train']]).sort_values(by=['order_id'])\n\n# Merge the rest of the datasets\nmaster_df = pd.merge(left = master_df, right = data['products'],\n                             left_on='product_id', right_on='product_id').sort_values(by=['order_id']).reset_index(drop=True)\nmaster_df = pd.merge(left = master_df, right = data['aisles'],\n                             left_on='aisle_id', right_on='aisle_id').sort_values(by=['order_id']).reset_index(drop=True)\nmaster_df = pd.merge(left = master_df, right = data['departments'],\n                             left_on='department_id', right_on='department_id').sort_values(by=['order_id']).reset_index(drop=True)\nmaster_df = pd.merge(left = master_df, right = data['orders'],\n                             left_on='order_id', right_on='order_id').sort_values(by=['order_id']).reset_index(drop=True)\n\ncol_order = ['user_id','order_id','product_id','aisle_id','department_id','add_to_cart_order',\n 'reordered','product_name','aisle','department','eval_set','order_number','order_dow','order_hour_of_day',\n 'days_since_prior_order']\n\nmaster_df = master_df[col_order]\n\ndel data\n\nsummarize_data(master_df)","d8921d01":"# Identify Primary Key\nlen(master_df.groupby(['user_id','order_id','product_id'], as_index=False).count())","528a22de":"# Identify unique values in columns\nprint (\"\\nNumber of Rows : \", len(master_df))\nprint (\"Unique user_id: \", master_df['user_id'].nunique(),\", % :\", f\"{master_df['user_id'].nunique() \/ len(master_df): .2%}\")\nprint (\"Unique order_id: \", master_df['order_id'].nunique(),\", % :\", f\"{master_df['order_id'].nunique() \/ len(master_df): .2%}\")\nprint (\"Unique product_id: \", master_df['product_id'].nunique(),\", % :\", f\"{master_df['product_id'].nunique() \/ len(master_df): .2%}\")\nprint (\"Unique aisle_id: \", master_df['aisle_id'].nunique(),\", % :\", f\"{master_df['aisle_id'].nunique() \/ len(master_df): .2%}\")\nprint (\"Unique department_id: \", master_df['department_id'].nunique(),\", % :\", f\"{master_df['department_id'].nunique() \/ len(master_df): .2%}\")\nprint (\"Unique add_to_cart_order: \", master_df['add_to_cart_order'].nunique(),\", % :\", f\"{master_df['add_to_cart_order'].nunique() \/ len(master_df): .2%}\")\nprint (\"Unique reordered: \", master_df['reordered'].nunique(),\", % :\", f\"{master_df['reordered'].nunique() \/ len(master_df): .2%}\")\nprint (\"Unique product_name: \", master_df['product_name'].nunique(),\", % :\", f\"{master_df['product_name'].nunique() \/ len(master_df): .2%}\")\nprint (\"Unique aisle: \", master_df['aisle'].nunique(),\", % :\", f\"{master_df['aisle'].nunique() \/ len(master_df): .2%}\")\nprint (\"Unique department: \", master_df['department'].nunique(),\", % :\", f\"{master_df['department'].nunique() \/ len(master_df): .2%}\")\nprint (\"Unique eval_set: \", master_df['eval_set'].nunique(),\", % :\", f\"{master_df['eval_set'].nunique() \/ len(master_df): .2%}\")\nprint (\"Unique order_number: \", master_df['order_number'].nunique(),\", % :\", f\"{master_df['order_number'].nunique() \/ len(master_df): .2%}\")\nprint (\"Unique order_dow: \", master_df['order_dow'].nunique(),\", % :\", f\"{master_df['order_dow'].nunique() \/ len(master_df): .2%}\")\nprint (\"Unique order_hour_of_day: \", master_df['order_hour_of_day'].nunique(),\", % :\", f\"{master_df['order_hour_of_day'].nunique() \/ len(master_df): .2%}\")\nprint (\"Unique days_since_prior_order: \", master_df['days_since_prior_order'].nunique(),\", % :\", f\"{master_df['days_since_prior_order'].nunique() \/ len(master_df): .2%}\")","a7f10a0e":"# look at the heirarchy of products, aisles and departments\n\nmaster_df.groupby(['department','aisle','product_name'], as_index = False).size()","00df74f5":"############################################ Helper Function 3 ############################################################\n############################################ VISUALIZE DATA ##############################################################\n\ndef CreateCharts (ax, data, x, y, chart_type, legend = False, size = 5, hue = None, palette=None):\n    if chart_type == \"scatter\":\n        plot = sns.scatterplot(data=data, x=x, y=y, size=size, legend=legend, \n          hue=hue, sizes=(20, 200), palette = palette, ax=ax)\n    elif chart_type == \"bar\":\n        plot = sns.barplot(x=x, y=y, data=data, \n          hue=hue, palette = palette, ax=ax, ci=\"sd\")\n    elif chart_type == \"density\":\n        plot = sns.kdeplot(x=x, data=data, \n          shade=True, alpha=0.5, ax=ax)  \n    elif chart_type == \"swarm\":\n        plot = sns.swarmplot(x=x, y=y, hue=hue, data=data,\n                palette=palette, ax=ax)\n    elif chart_type =='hist':\n        plot = sns.histplot(data=data, x=x, kde=True, ax = ax, palette = palette )\n    return plot","cf288f76":"\nsns.set(style=\"darkgrid\")\nfig, ax = plt.subplots(4,3, figsize=(25,15))\n\n# Plot 1\ndata = master_df[['user_id','order_id']].drop_duplicates().groupby('user_id').size().reset_index(name='Number of Orders').sort_values(by='Number of Orders', ascending=False)\nax[0,0] = CreateCharts(ax[0,0], data, x = \"Number of Orders\", y=None, chart_type = \"hist\", palette=\"bright\")\nax[0,0].set_title('How many orders are made by a single user?', fontsize=10)\n\n\n# Plot 2\ndata = master_df.groupby('order_id').size().reset_index(name='Number of Products').sort_values(by='Number of Products', ascending=False)\nax[0,1] = CreateCharts(ax[0,1], data, x = \"Number of Products\", y=None, chart_type = \"hist\", palette=\"dark\")\nax[0,1].set_title('How many products are in a single order?', fontsize=10)\n\n\n# Plot 3\nimport matplotlib.patches as mpatches\ndata1 = master_df.groupby('days_since_prior_order', as_index=False).size()\nbar1 = sns.barplot(x=\"days_since_prior_order\",  y=\"size\", data=data1, color='darkblue', ax=ax[0,2])\ndata2 = master_df[master_df['reordered']==1].groupby('days_since_prior_order', as_index=False).size()\nbar2 = sns.barplot(x=\"days_since_prior_order\",  y=\"size\", data=data2,  color='lightblue', ax=ax[0,2])\ntop_bar = mpatches.Patch(color='darkblue', label='total orders')\nbottom_bar = mpatches.Patch(color='lightblue', label='reordered')\nax[0,2].legend(handles=[top_bar, bottom_bar])\nax[0,2].set_title('How many days have passed since the last order?', fontsize=10)\nax[0,2].set_ylabel('count')\n\n\n# Plot 4\n\ndata1 = master_df.groupby('order_dow', as_index=False).size()\nbar1 = sns.barplot(x=\"order_dow\",  y=\"size\", data=data1, color='maroon', ax=ax[1,0])\ndata2 = master_df[master_df['reordered']==1].groupby('order_dow', as_index=False).size()\nbar2 = sns.barplot(x=\"order_dow\",  y=\"size\", data=data2,  color='yellow', ax=ax[1,0])\ntop_bar = mpatches.Patch(color='maroon', label='total orders')\nbottom_bar = mpatches.Patch(color='yellow', label='reordered')\nax[1,0].legend(handles=[top_bar, bottom_bar])\nax[1,0].set_title('Which days have the most orders\/reorders?', fontsize=10)\nax[1,0].set_ylabel('count')\n\n# Plot 5\ndata1 = master_df.groupby('order_hour_of_day', as_index=False).size()\nbar1 = sns.barplot(x=\"order_hour_of_day\",  y=\"size\", data=data1, color='purple', ax=ax[1,1])\ndata2 = master_df[master_df['reordered']==1].groupby('order_hour_of_day', as_index=False).size()\nbar2 = sns.barplot(x=\"order_hour_of_day\",  y=\"size\", data=data2,  color='lightgreen', ax=ax[1,1])\ntop_bar = mpatches.Patch(color='purple', label='total orders')\nbottom_bar = mpatches.Patch(color='lightgreen', label='reordered')\nax[1,1].legend(handles=[top_bar, bottom_bar])\nax[1,1].set_title('Which hour of day has the most orders\/reorders?', fontsize=10)\nax[1,1].set_ylabel('count')\n\n# Plot 6\ndata = master_df.groupby('order_id', as_index = False)['add_to_cart_order'].max()\nax[1,2] = CreateCharts(ax[1,2], data, x = \"add_to_cart_order\", y=None, chart_type = \"hist\", palette=\"bright\")\nax[1,2].set_title('What is the cart size across orders?', fontsize=10)\n\n\n# Plot 7\ndata = master_df.groupby('product_name', as_index = False)['product_id'].count().sort_values(by='product_id', ascending=False)\nax[2,0] = CreateCharts(ax[2,0], data.head(25), \"product_name\", \"product_id\", \"bar\", palette = \"Set2\")\nax[2,0].set_xticklabels(data.head(25)['product_name'], rotation = 90)\nax[2,0].set_title('What are the most ordered products?', fontsize=10)\nax[2,0].set_xlabel('')\nax[2,0].set_ylabel('count')\n\n# Plot 8\ndata = master_df.groupby('aisle', as_index = False)['aisle_id'].count().sort_values(by='aisle_id', ascending=False)\nax[2,1] = CreateCharts(ax[2,1], data.head(25), \"aisle\", \"aisle_id\", \"bar\", palette = \"Set2\")\nax[2,1].set_xticklabels(data.head(25)['aisle'], rotation = 90)\nax[2,1].set_title('Which aisle do users order the most from?', fontsize=10)\nax[2,1].set_xlabel('')\nax[2,1].set_ylabel('count')\n\n# Plot 9\ndata = master_df.groupby('department', as_index = False)['department_id'].count().sort_values(by='department_id', ascending=False)\nax[2,2] = CreateCharts(ax[2,2], data.head(25), \"department\", \"department_id\", \"bar\", palette = \"Set2\")\nax[2,2].set_xticklabels(data.head(25)['department'], rotation = 90)\nax[2,2].set_title('Which department do users order the most from?', fontsize=10)\nax[2,2].set_xlabel('')\nax[2,2].set_ylabel('count')\n\n\n# Plot 10\ndata = master_df.groupby('product_name', as_index = False)['reordered'].sum().sort_values(by = 'reordered', ascending=False)\nax[3,0] = CreateCharts(ax[3,0], data.head(25),'product_name' , \"reordered\", \"bar\", palette = \"hls\")\nax[3,0].set_xticklabels(data.head(25)['product_name'], rotation = 90)\nax[3,0].set_title('What are the most reordered products?', fontsize=10)\nax[3,0].set_xlabel('')\n\n# Plot 11\ndata = master_df.groupby('aisle', as_index = False)['reordered'].sum().sort_values(by = 'reordered', ascending=False)\nax[3,1] = CreateCharts(ax[3,1], data.head(25),'aisle' , \"reordered\", \"bar\", palette = \"hls\")\nax[3,1].set_xticklabels(data.head(25)['aisle'], rotation = 90)\nax[3,1].set_title('Which aisle has the most reorders?', fontsize=10)\nax[3,1].set_xlabel('')\n\n# Plot 12\ndata = master_df.groupby('department', as_index = False)['reordered'].sum().sort_values(by = 'reordered', ascending=False)\nax[3,2] = CreateCharts(ax[3,2], data.head(25),'department' , \"reordered\", \"bar\", palette = \"hls\")\nax[3,2].set_xticklabels(data.head(25)['department'], rotation = 90)\nax[3,2].set_title('Which department has the most reorders?', fontsize=10)\nax[3,2].set_xlabel('')\n\n\nplt.subplots_adjust(left=0.125,\n                    bottom=0.1, \n                    right=0.9, \n                    top=0.9, \n                    wspace=0.2, \n                    hspace=1.5)\n\nplt.show()","64431668":"# Correlation\nplt.figure(figsize=(10,8))\nplt.title('Pearson Correlation of Features', size = 15)\ncolormap = sns.diverging_palette(10, 220, as_cmap = True)\nsns.heatmap(master_df.corr(),\n            cmap = colormap,\n            square = True,\n            annot = True,\n            linewidths=0.1,vmax=1.0, linecolor='white',\n            annot_kws={'fontsize':12 })\nplt.show()","44fc9202":"############################################ Helper Function 4 ############################################################\n################################################# RUN PCA #################################################################\n\ndef run_PCA(data, features, name):\n  pca = PCA(n_components=features)\n  scale_data = pd.DataFrame(scale(data), columns = data.columns, index = data.index)\n  pca_output = pca.fit_transform(data)\n  df_pca = pd.DataFrame(data = pca_output, columns = [name+str(i) for i in range(features)], index = data.index)\n  plot(pca.explained_variance_ratio_.cumsum(), linewidth=2)\n  print(\"\\nImportance of Components\")\n  print(pd.DataFrame(data = pca.components_, columns = data.columns, index = ['prod_'+str(i) for i in range(features)]))\n  return df_pca","b78f91e2":"# PCA to find product preference\nprod_pref = pd.pivot_table(master_df.groupby(['user_id','aisle'], as_index=False).size(), values='size', index='user_id',\n                    columns=['aisle'], aggfunc=np.sum, fill_value=0)\nprod_pref = run_PCA(prod_pref, 10, \"prod_pref\")\n\n# PCA to find day preference\n# day_pref = pd.pivot_table(master_df, values = 'order_dow', index = 'user_id',\n#                columns = ['department'], aggfunc=np.median, fill_value=-1)\n\n# day_pref = run_PCA(day_pref, 10, \"day_pref\")","4526a728":"dept_pref = pd.pivot_table(master_df.groupby(['user_id','department'], as_index=False).size(), values='size', index='user_id',\n                    columns=['department'], aggfunc=np.sum, fill_value=0)\ndept_pref['total'] = dept_pref.sum(axis = 1)\n\nfor dept in dept_pref.columns:\n  dept_pref[dept+'_perc'] = dept_pref[dept] \/ dept_pref['total']\n\ndept_pref","3ab070f9":"# Create features to demonstrate trend of products prefered by users\nmodel_input = prod_pref.merge(dept_pref, how='left', on='user_id')\n# Create features to demonstrate trends of orders made by users\nmodel_input = model_input.merge(master_df.groupby('user_id', as_index = False).agg(\n              {'order_id':'count',\n               'product_id':'count',\n               'days_since_prior_order':'mean',\n               'add_to_cart_order':'median',\n               }).rename(\n              columns={'order_id':'total_orders',\n                       'product_id':'total_products',\n                       'days_since_prior_order':'mean_days_since_prior_order',\n                       'add_to_cart_order':'median_cart_size'}),\n               how='left', on='user_id') \nmodel_input = model_input.set_index('user_id') \nmodel_input","8e36be0a":"## Make data suitable for chosen model\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# check for outliers \nfor col in model_input.columns:\n  cap_val =  (np.mean(model_input[col]) + 3*np.std(model_input[col]))\n  model_input[col+\"_capped\"] = 0\n  model_input[col+\"_capped\"][model_input[col] >= 0 ] = np.minimum(cap_val,model_input[col][model_input[col] >= 0 ])\n  model_input[col+\"_capped\"][model_input[col] < 0 ] = np.maximum((-1)*cap_val,model_input[col][model_input[col] < 0 ])\n  print(\"\\nOutliers\", col)\n  print (\"cap: \", cap_val)\n  print (\"capped positive:\", sum (model_input[col][model_input[col] >= 0 ] > cap_val))\n  print (\"capped negative:\", sum (model_input[col][model_input[col] < 0 ] < (-1)*cap_val))\n\n\n# normalize data\nscaler = StandardScaler()\nmodel_input_scaled = pd.DataFrame(scaler.fit_transform(model_input),columns = model_input.columns, index = model_input.index)","179427ec":"# final dataset\n\nX = model_input_scaled[['prod_pref0_capped',\n       'prod_pref1_capped', 'prod_pref2_capped', 'prod_pref3_capped',\n       'prod_pref4_capped', 'prod_pref5_capped', 'prod_pref6_capped', \n       'prod_pref7_capped', 'prod_pref8_capped', 'prod_pref9_capped',\n       'alcohol_perc', 'babies_perc', 'bakery_perc', 'beverages_perc',\n       'breakfast_perc', 'bulk_perc', 'canned goods_perc', 'dairy eggs_perc',\n       'deli_perc', 'dry goods pasta_perc', 'frozen_perc', 'household_perc',\n       'international_perc', 'meat seafood_perc', 'missing_perc', 'other_perc',\n       'pantry_perc', 'personal care_perc', 'pets_perc', 'produce_perc',\n       'snacks_perc', 'total_orders_capped',\n       'total_products_capped', 'mean_days_since_prior_order_capped',\n       'median_cart_size_capped']]\n\n# sns.pairplot(X)","bc2dc73a":"<br>\n\n**Overview of Tables**\n\n| Table Name | Column Names | Description | Hypothesis for Data Exploration  |\n| --- | --- | --- | --- | \n| aisles | aisle_id, aisle | 134 unique IDs, names for different aisles at Instacart | aisle which generates highest revenue, most frequently used aisle, distribution of<br> aisle usage based on demographics and other customer info, is there a trend of<br> aisle popularity with time of day, day of week, or any specific month of year? |\n| departments | department_id, department | 21 unique IDs, names for different departments including 'missing'<br> - looks like the rolled-up metric for aisles e.g. one department might <br> have multiple aisles | revenue by dept, frequency of use by dept, distribution of dept usage based<br> on demographics and other available customer info, dept popularity by time <br>of day, day of week, or any specific month of year  |\n| order-products-prior | order_id, product_id, add_to_cart_order, reordered | 32,434,489 rows at order-product level with 3,214,874 unique orders <br>for 49,677 unique products. 'add_to_cart_order' shows the order in which<br> they were added to the cart and 'reordered' | most ordered products, most frequent re-ordered products, products which <br>are only ordered once and not reordered, Number of products in one order,<br> are the products that are ordered together from the same aisle\/dept? |\n | order-products-train | order_id, product_id, add_to_cart_order, reordered | Similar to df_prior_orders but only has latest order information. 1,384,617 rows<br> with 131,209 unique order IDs and 39,123  | most ordered products, most frequent re-ordered products, products <br>which are only ordered once and not reordered, Number of products in <br>one order, are the products that are ordered together from the same aisle\/dept? |\n| orders | order_id, user_id, eval_set,<br> order_number, order_dow,<br> order_hour_of_day, days_since_prior_order | 3,421,083 orders showing information on order ID, user ID, which evaluation<br> dataset the order is in (prior, train, test), day of week, hour of day,<br> days since prior order | Most popular day & time for placing an order, trend of day & time by products |\n| products | product_id, product_name, aisle_id, department_id | 49,688 rows mapping products to aisles and departments |\tcovered in above metrics |\n\n<br>\n\n**Missing Values**\n\n* 6% values are missing for 'days since prior order' in 'orders' df --> Given these are only for Order Number 1, we can replace missing values with '0' days\n* No other missing values found in the data\n* FYI - We can use Imputers from sklearn in couldn't replace our missing data with 0\n\n","3c185888":"## 1. Problem Statement\n\n**Objective**: \"Use anonymized data on customer orders over time to predict which previously purchased products will be in a user\u2019s next order.\"","5a69e513":"## 4. Feature Engg.","73bd483e":"## 2. Load and View the Data","85cac934":"## 3. Master Dataset","963027d3":"# create a 100 features using FE tech","36df474b":"## 4. EDA"}}