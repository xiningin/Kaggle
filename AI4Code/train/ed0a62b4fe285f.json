{"cell_type":{"fdde1975":"code","f1ac0903":"code","a441ec46":"code","31728bf4":"code","0556803c":"code","46bba005":"code","a52a6b42":"code","6744efcb":"code","4f154692":"code","0141546d":"markdown","ae25f4d0":"markdown","fb6a35b6":"markdown"},"source":{"fdde1975":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","f1ac0903":"import pandas as pd\nimport numpy as np \nimport matplotlib.pyplot as plt","a441ec46":"data = pd.read_csv(\"..\/input\/\"+os.listdir(\"..\/input\")[0])","31728bf4":"data.head()\n","0556803c":"import matplotlib.pyplot as plt\ndata.target.value_counts()\ndata[['age','target']].hist()\n\nplt.show()\nplt.scatter(x=data.age[data.target==1], y=data.thalach[data.target==1], c='r')\nplt.scatter(x=data.age[data.target==0], y=data.thalach[data.target==0], c='g')\n\nplt.show()","46bba005":"#knn classifier\n\nX =np.array( data[['age','sex','cp','trestbps','chol','fbs','restecg','thalach','exang','oldpeak','slope','ca','thal']])\nY = np.array(data[['target']])\n\n#normalizing the data \n\nfor i in range(0,np.shape(X)[1]):\n    X[:,i]= (X[:,i]-np.average(X[:,i]))\/np.std(X[:,i])\n\n\nimport torch\nimport torchvision\nimport torch.nn as nn\nimport torchvision.transforms as transforms\nimport numpy as np\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42,shuffle= True)\n\nfrom sklearn.neighbors import KNeighborsClassifier\nneigh = KNeighborsClassifier(n_neighbors=5)\nneigh.fit(X_train, y_train) \naccuracy_kkn = neigh.score(X_test, y_test)\naccuracy_kkn_train = neigh.score(X_train, y_train)\n\nZ = neigh.predict(X_test)\n\nprint ('The train accuracy obtained is: {0:1.2f}'.format(accuracy_kkn_train))\nprint ('The test accuracy obtained is: {0:1.2f}'.format(accuracy_kkn))\n\nn=[]\nfor i in range(0,15):\n    neigh = KNeighborsClassifier(n_neighbors=i+1)\n    neigh.fit(X_train, y_train) \n    p = neigh.score(X_test, y_test)\n    n.append(p)\n    \n    \nplt.plot(range (0,np.size(n)),n)\nplt.xlabel('Number of nearest neighbours ')\nplt.ylabel('Accuracy in test data')\nplt.show()","a52a6b42":"#Logistic regression\n\n\n\nfrom sklearn.linear_model import LogisticRegression\nlogistic = LogisticRegression().fit(X_train, y_train)\ny_prediction = logistic.predict(X_test)\n\nprint (' The accuracy by logistic regression is: ')\naccuracy_logistic = logistic.score(X_test, y_test)\naccuracy_logistic_train = logistic.score(X_train, y_train)\n\nprint ('The train accuracy obtained is: {0:1.2f}'.format(accuracy_logistic_train))\nprint ('The test accuracy obtained is: {0:1.2f}'.format(accuracy_logistic))","6744efcb":"import torch\nimport torchvision\nimport torch.nn as nn\nimport torchvision.transforms as transforms\nimport numpy as np\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\n\n\n\n\ntrain_data = torch.FloatTensor(np.array(X_train))\ntrain_y = np.array(y_train)\n\ntrain_y =  torch.FloatTensor(train_y)\nx,y = Variable(train_data), Variable (train_y)\n\n\n#######Neural network\n\n\nn_in, n_h1,n_h2, n_out, batch_size = 13, 13,13, 1, 20\n\n\n\n# Creating a model with 2 hidden layer\nmodel = nn.Sequential(nn.Linear(n_in, n_h1, bias = True), nn.ReLU(),nn.Linear(n_h1, n_h2, bias =True),\n                     nn.ReLU(),nn.Linear(n_h2, n_out), nn.ReLU())\n\n\ncriterion = torch.nn.MSELoss()\n\n# Construct the optimizer (Stochastic Gradient Descent in this case)\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n\n# Gradient Descent\nfor epoch in range(10000):\n    # Forward pass: Compute predicted y by passing x to the model\n    y_pred = model(x)\n\n    # Compute and print loss\n    loss = criterion(y_pred, y)\n    if epoch%1000 == 0:\n        print('epoch: ', epoch,' loss: ', loss.item())\n\n    # Zero gradients, perform a backward pass, and update the weights.\n    optimizer.zero_grad()\n    \n    # perform a backward pass (backpropagation)\n    loss.backward()\n    \n    # Update the parameters\n    optimizer.step()\n","4f154692":"test_data = torch.FloatTensor(np.array(X_test))\n\ny_pred_test = model(test_data)\ny1_pred = y_pred_test.detach().numpy()\n\np1= (y1_pred>0.5)\n\n\nprint ('The test accuracy obtained is {0:1.2f}'.format(1-(np.sum(y_test-p1)\/np.size (p1))))\n","0141546d":"### Logistic Regression","ae25f4d0":"### KNN Classifier","fb6a35b6":"## Neural Net"}}