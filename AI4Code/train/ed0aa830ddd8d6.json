{"cell_type":{"7607ec2e":"code","c988d3df":"code","55ddd460":"code","6188204b":"code","3b50e877":"code","9992a506":"code","9dee3bcf":"code","dc978283":"code","3d446ea5":"code","03a7179c":"code","c9510ff1":"code","55e5642f":"code","ed149f05":"code","5085c11f":"code","2eda0ac4":"code","38ef70f8":"code","8ea1c331":"code","eca0a1d5":"code","85bf3051":"code","a8385cbf":"code","ee389597":"code","5279c0e2":"code","6fab50d9":"code","ed284048":"code","ad7fc1cf":"code","9dd8315d":"code","3f67bc74":"code","5576cf1b":"code","78729671":"code","9b41510b":"code","d4814e25":"markdown","083eaae0":"markdown","7bcde89e":"markdown","a9b29c4f":"markdown","67d19b3e":"markdown","f597419e":"markdown","bd420fa3":"markdown","ee40f2be":"markdown","abf03174":"markdown","110ca1b2":"markdown","53f8b12f":"markdown","a7b81812":"markdown","63decc30":"markdown","79763263":"markdown"},"source":{"7607ec2e":"import numpy as np\nimport pandas as pd\n\nimport warnings\nwarnings.simplefilter('ignore')\n\nimport gc, sys\ngc.enable()\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","c988d3df":"chunksize = 10**6","55ddd460":"train = None\n\nload_count = 0\n\nfor load_train in pd.read_csv('\/kaggle\/input\/riiid-test-answer-prediction\/train.csv', chunksize=chunksize, iterator=True):\n    \n    load_train = load_train[load_train['answered_correctly'] != -1]\n    \n    load_train['user_id__content_id'] = load_train['user_id'].astype(str) + '__' + load_train['content_id'].astype(str)\n    \n    load_train = load_train.drop_duplicates('user_id__content_id', keep = 'last')\n    \n    if train is None:\n        train = load_train\n    else:\n        train = pd.concat([train, load_train]).drop_duplicates('user_id__content_id', keep = 'last')\n    \n    load_count += chunksize\n    print('Rows processed:', load_count, 'Train set size:', train.shape[0])\n    \n    if load_count >= 15 * chunksize:\n        break","6188204b":"lectures = pd.read_csv(\"\/kaggle\/input\/riiid-test-answer-prediction\/lectures.csv\")","3b50e877":"questions = pd.read_csv(\"\/kaggle\/input\/riiid-test-answer-prediction\/questions.csv\")","9992a506":"question_columns = ['question_id', 'bundle_id', 'part']\n\n\ndef merge_question_columns(data):\n    return data.merge(questions[question_columns], right_on='question_id', left_on='content_id', how='left')","9dee3bcf":"train = merge_question_columns(train)","dc978283":"lectures['type_of'] = lectures['type_of'].replace('solving question', 'solving_question')\n\nlectures = pd.get_dummies(lectures, columns=['part', 'type_of'])\n\npart_lectures_columns = [column for column in lectures.columns if column.startswith('part')]\n\ntypes_of_lectures_columns = [column for column in lectures.columns if column.startswith('type_of_')]","3d446ea5":"lectures.head(10).T","03a7179c":"user_lecture_stats = None\n\n\ndef collect_user_lecture_stats(train):\n    \n    # merge lecture features to train dataset\n    train_lectures = train[train['content_type_id'] == 1].merge(lectures, right_on='lecture_id', left_on='content_id', how='left')\n    \n    # collect per user stats\n    user_lecture_stats_part = train_lectures.groupby('user_id')[part_lectures_columns + types_of_lectures_columns].sum()\n    \n    # add boolean features\n    for column in user_lecture_stats_part.columns:\n        bool_column = column + '_boolean'\n        user_lecture_stats_part[bool_column] = (user_lecture_stats_part[column] > 0).astype(int)\n    \n    return user_lecture_stats_part\n\ndef update_user_lecture_stats(user_lecture_stats_part):\n    global user_lecture_stats\n    if user_lecture_stats is None:\n        user_lecture_stats = user_lecture_stats_part\n    else:\n        user_lecture_stats = user_lecture_stats.add(user_lecture_stats_part, fill_value=0.)\n\ndef merge_user_lecture_stats(data):\n    return data.merge(user_lecture_stats, left_on='user_id', right_index=True, how='left').fillna(0)","c9510ff1":"from collections import Counter\n\n\nper_value_counts = {}\nglobal_counts = Counter()\n\n\ncolumns_target_encode = ['user_id', 'content_id', 'task_container_id',\n                         'bundle_id', 'part']\n\n\ndef add_or_update(column, value, count, answered_correctly):\n    # column\n    column_data = per_value_counts.get(column, {})\n    per_value_counts[column] = column_data\n    # value\n    value_data = column_data.get(value, Counter())\n    column_data[value] = value_data\n    # counters\n    value_data += Counter({'count': count, 'answered_correctly': answered_correctly})\n\ndef update_counts(data, column):\n    agg = data.groupby(column)['answered_correctly'].agg(['count', 'mean'])\n    agg['answered_correctly'] = agg['count'] * agg['mean']\n    for idx,row in agg.iterrows():\n        add_or_update(column, idx, row['count'], row['answered_correctly'])\n\ndef update_global_counts(data):\n    global global_counts\n    count = data.shape[0]\n    clicks = data[data['answered_correctly'] == 1].shape[0]\n    global_counts += Counter({'count': count, 'answered_correctly': clicks})\n\ndef update_all_counts(data, columns):\n    for column in columns:\n        update_counts(data, column)\n    update_global_counts(data)\n\ndef target_encode_value(column, value):\n    counts = per_value_counts.get(column, {}).get(value, Counter())\n    if 'answered_correctly' in counts:\n        return counts['answered_correctly'] \/ counts['count']\n    else:\n        return global_counts['answered_correctly'] \/ global_counts['count']\n\ndef target_encode(data, columns):\n    out = pd.DataFrame(index=data.index)\n    for column in columns:\n        out[column] = data[column].apply(lambda value: target_encode_value(column, value))\n    return out","55e5642f":"from sklearn.preprocessing import StandardScaler\n\n\ncolumns_std = ['prior_question_elapsed_time']\n\n\nscaler = StandardScaler()","ed149f05":"warmup_count = 0\n\nfor warmup_train in pd.read_csv('\/kaggle\/input\/riiid-test-answer-prediction\/train.csv', chunksize=chunksize, iterator=True):\n    \n    update_user_lecture_stats(collect_user_lecture_stats(warmup_train))\n    \n    warmup_train = warmup_train[warmup_train['answered_correctly'] != -1]\n    \n    warmup_train = merge_question_columns(warmup_train)\n    \n    update_all_counts(warmup_train, columns_target_encode)\n    \n    scaler.partial_fit(warmup_train[columns_std])\n    \n    warmup_count += chunksize\n    # print('Rows processed:', warmup_count)","5085c11f":"user_lecture_stats.head(10).T","2eda0ac4":"user_lecture_stats_boolean_columns = [column for column in user_lecture_stats.columns if column.endswith('_boolean')]\n\nuser_lecture_stats_boolean_columns","38ef70f8":"columns_copy = ['prior_question_had_explanation']\n\n\ndef make_x(data):\n    \n    # copy without changes\n    x = data[columns_copy + ['user_id']].fillna(0)\n    \n    # convert Bool to Int\n    x['prior_question_had_explanation'] = x['prior_question_had_explanation'].astype(int)\n    \n    # merge per user lecture stats\n    x_lecture_stats = merge_user_lecture_stats(x)[user_lecture_stats_boolean_columns]\n    \n    x = x.drop('user_id', axis=1)\n    \n    # target encode\n    x_target_encode = target_encode(data, columns_target_encode)\n    \n    # std\n    x_std = pd.DataFrame(scaler.transform(data[columns_std]), index=data.index, columns=columns_std)\n    for i,column in enumerate(x_std.columns):\n        x_std[column].fillna(scaler.mean_[i], inplace=True)\n    \n    return pd.concat([x, x_lecture_stats, x_target_encode, x_std], axis=1)","8ea1c331":"x_train = make_x(train)\n\ny_train = train['answered_correctly']\n\ndel train\ngc.collect()","eca0a1d5":"x_train.head(10).T","85bf3051":"import tensorflow as tf\nimport tensorflow_addons as tfa\n\nimport tensorflow.keras.backend as K","a8385cbf":"def make_layer(x, units, dropout_rate):\n    t = tfa.layers.WeightNormalization(tf.keras.layers.Dense(units))(x)\n    t = tf.keras.layers.BatchNormalization()(t)\n    t = tf.keras.layers.Activation('relu')(t)\n    t = tf.keras.layers.Dropout(dropout_rate)(t)\n    return t\n\n\ndef make_model(columns, units, dropout_rates):\n    \n    inputs = tf.keras.layers.Input(shape=(columns,))\n    x = tf.keras.layers.BatchNormalization()(inputs)\n\n    for i in range(len(units)):\n        u = units[i]\n        d = dropout_rates[i]\n        x = make_layer(x, u, d)\n       \n    y = tf.keras.layers.Dense(1, activation='sigmoid', name='dense_output')(x)\n    \n    model = tf.keras.Model(inputs=inputs, outputs=y)\n    model.compile(loss='binary_crossentropy',\n                  optimizer=tfa.optimizers.AdamW(learning_rate=1e-3, weight_decay=1e-5),\n                  metrics=['accuracy'])\n    return model","ee389597":"from sklearn.model_selection import KFold\n\n\ndef fit_validate(n_splits, x_train, y_train, units, dropout_rates, epochs, verbose, random_state):\n\n    estimators = []\n    histories = []\n    \n    scores = []\n\n    cv = KFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n    for train_idx, valid_idx in cv.split(x_train, y_train):\n\n        x_train_train = x_train.iloc[train_idx]\n        y_train_train = y_train.iloc[train_idx]\n        x_train_valid = x_train.iloc[valid_idx]\n        y_train_valid = y_train.iloc[valid_idx]\n\n        K.clear_session()\n        \n        es = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', min_delta=0.001, patience=10,\n                                              verbose=verbose, mode='max', restore_best_weights=True)\n\n        rl = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_accuracy', factor=0.1, patience=3, min_lr=1e-5,\n                                                  mode='max', verbose=verbose)\n\n        estimator = make_model(x_train.shape[1], units, dropout_rates)\n\n        history = estimator.fit(x_train_train, y_train_train,\n                                batch_size=128, epochs=epochs, callbacks=[es, rl],\n                                validation_data=(x_train_valid, y_train_valid),\n                                verbose=verbose)\n        \n        estimators.append(estimator)\n        histories.append(history)\n        \n        scores.append(history.history['val_accuracy'][-1])\n    \n    score = np.mean(scores)\n    \n    return estimators, histories, score","5279c0e2":"import optuna\n\nfrom logging import CRITICAL\noptuna.logging.set_verbosity(CRITICAL)\n\n\ndef objective(trial):\n    \n    n_layers = trial.suggest_int('n_layers', 1, 5)\n    \n    units = []\n    dropout_rates = []\n    for i in range(n_layers):\n        u = trial.suggest_categorical('units_{}'.format(i+1), [16, 32, 64, 128])\n        units.append(u)\n        r = trial.suggest_loguniform('dropout_rate_{}'.format(i+1), 0.1, 0.5)\n        dropout_rates.append(r)\n    \n    print('Units:', units, \"Dropout rates:\", dropout_rates)\n    \n    _, _, score = fit_validate(3, x_train, y_train, units, dropout_rates, 10, 0, 42)\n    print('Score:', score)\n    \n    return score\n\n\n# study = optuna.create_study(direction='maximize')\n# study.optimize(objective, n_trials=100)","6fab50d9":"# params = study.best_trial.params\n# params","ed284048":"params = {\n    'n_layers': 2,\n    'units_1': 128,\n    'dropout_rate_1': 0.1576269633262961,\n    'units_2': 64,\n    'dropout_rate_2': 0.26394371768001645\n}\nparams","ad7fc1cf":"K.clear_session()\n\n\nn_layers = params['n_layers']\nunits = []\ndropout_rates = []\nfor i in range(n_layers):\n    u = params['units_{}'.format(i+1)]\n    units.append(u)\n    d = params['dropout_rate_{}'.format(i+1)]\n    dropout_rates.append(d)\n\n\nestimators, histories, score = fit_validate(3, x_train, y_train, units, dropout_rates, 50, 2, 42)","9dd8315d":"print('Validation score:', score)","3f67bc74":"del x_train\ndel y_train\ngc.collect()","5576cf1b":"import matplotlib.pyplot as plt\n\n\nfig, axs = plt.subplots(2, 2, figsize=(18,18))\n\n# accuracy\nfor h in histories:\n    axs[0,0].plot(h.history['accuracy'], color='g')\naxs[0,0].set_title('Model accuracy - Train')\naxs[0,0].set_ylabel('Accuracy')\naxs[0,0].set_xlabel('Epoch')\n\nfor h in histories:\n    axs[0,1].plot(h.history['val_accuracy'], color='b')\naxs[0,1].set_title('Model accuracy - Test')\naxs[0,1].set_ylabel('Accuracy')\naxs[0,1].set_xlabel('Epoch')\n\n# loss\nfor h in histories:\n    axs[1,0].plot(h.history['loss'], color='g')\naxs[1,0].set_title('Model loss - Train')\naxs[1,0].set_ylabel('Loss')\naxs[1,0].set_xlabel('Epoch')\n\nfor h in histories:\n    axs[1,1].plot(h.history['val_loss'], color='b')\naxs[1,1].set_title('Model loss - Test')\naxs[1,1].set_ylabel('Loss')\naxs[1,1].set_xlabel('Epoch')\n\nfig.show()","78729671":"import riiideducation\n\nenv = riiideducation.make_env()\n\niter_test = env.iter_test()","9b41510b":"for (test, sample_prediction) in iter_test:\n    \n    test = merge_question_columns(test)\n    \n    x_test = make_x(test)\n    \n    y_preds = []\n    for estimator in estimators:\n        y_pred = estimator.predict(x_test)\n        y_preds.append(y_pred)\n    \n    test['answered_correctly'] = np.mean(y_preds, axis=0)\n    \n    env.predict(test.loc[test['content_type_id'] == 0, ['row_id', 'answered_correctly']])","d4814e25":"## Predict and submit","083eaae0":"## Load data","7bcde89e":"### Warmup using all train data","a9b29c4f":"# Keras\n\nFeatures:\n* Feature engineering: boolean flags that indicate that user watched at least one lecture of each type\n* Feature engineering: features from question dataset\n* Feature engineering: fill 'prior_question_elapsed_time' empty values with mean value\n* Target encoding using all train data\n* Standard scaling of 'prior_question_elapsed_time'\n* Keras\n* Optuna to find optimal NN parameters","67d19b3e":"### Per user lecture stats","f597419e":"Do OHE for some lectures features","bd420fa3":"Load more train data. Keep records where user last time answers a question.","ee40f2be":"## Feature engineering","abf03174":"## Show graphs","110ca1b2":"Collect per user lecture stats","53f8b12f":"### Standard scaling","a7b81812":"## Prepare x, y","63decc30":"## Keras","79763263":"### Target encoding"}}