{"cell_type":{"5df6a619":"code","a0172321":"code","172a4616":"code","cddba3b9":"code","b87c885a":"code","cc33e469":"code","379b0963":"code","854ba1bf":"code","21dbb76a":"code","dac1d8db":"code","93b99ee9":"code","99020abc":"code","16c56a36":"code","2d373ca1":"code","7a01e0f0":"code","21484f38":"code","bff69281":"code","85cd8cb5":"code","42767af3":"code","ffcada0a":"code","228feca6":"code","40528764":"code","3469dabe":"code","73bc7fdb":"code","7babda36":"code","7fff7ebf":"code","ad33ffb6":"code","6e55737c":"code","045eebb4":"code","c1a7cb16":"code","af4b536e":"code","5efeb948":"code","f467931d":"code","228403b2":"code","4b0bc4b7":"code","23297173":"code","62f19f4a":"code","1c00ad9f":"code","0b819d7b":"code","4a9ca882":"code","c1e77754":"code","faf1076e":"code","0d72aedf":"code","7cfc21ab":"code","c054250e":"code","7e45fdb1":"code","9d2c202f":"code","b8d6207f":"code","f0d812c0":"code","6bb8a9e1":"code","f53ce981":"code","2d05042a":"code","752b205c":"code","371af090":"code","3e20330d":"code","d47af35e":"code","64cb50a0":"code","91b55830":"code","be1d7fac":"code","e546254b":"code","c152ff04":"code","db492d56":"code","25edadbe":"code","d53a5eae":"code","67242a41":"code","f9e5f09f":"code","ce901771":"code","c316d551":"code","83e7950d":"code","cb9c128c":"code","1b469164":"code","f0f92de0":"code","bc41830a":"code","f681e0a7":"code","e15831ca":"code","33bfd3c7":"code","ef8352b1":"code","af9a31c1":"code","5b189c61":"code","3b65fabe":"code","cb0d3475":"code","5e1f2cd6":"code","e925f872":"code","f8c33a26":"markdown","0cb10568":"markdown","1d71ddfa":"markdown","201b0c2f":"markdown","339305de":"markdown","fc1724bc":"markdown","e173b477":"markdown","5e463867":"markdown","b6eccb7d":"markdown","c5067569":"markdown","b45bc361":"markdown","95009a7f":"markdown","47b4c305":"markdown","66a8bd58":"markdown","fe417c79":"markdown","707680f8":"markdown","d29b634d":"markdown","8e52f3f0":"markdown","5fd9cb66":"markdown","b7fe6e91":"markdown","a8720779":"markdown","166b0d88":"markdown","51523eb0":"markdown","0ac08edd":"markdown","291289c8":"markdown","252e58ca":"markdown","69973e93":"markdown","831466b7":"markdown","9935ef08":"markdown","040d68a2":"markdown","e38d8882":"markdown","cde1cea1":"markdown","e5cae3d7":"markdown","35262afd":"markdown","4e703048":"markdown","e72c80af":"markdown","969130ad":"markdown","cec503b5":"markdown","b56f262a":"markdown","8c410942":"markdown","ffadf92e":"markdown","00d85030":"markdown","912623c3":"markdown","81f7c7cc":"markdown","a8889c58":"markdown","0915f591":"markdown","9b946c1f":"markdown"},"source":{"5df6a619":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","a0172321":"base_url = '\/kaggle\/input\/web-traffic-time-series-forecasting\/'\n\nkey_1 = pd.read_csv(base_url+'key_1.csv')\ntrain_1 = pd.read_csv(base_url+'train_1.csv')\nsample_submission_1 = pd.read_csv(base_url+'sample_submission_1.csv')","172a4616":"print(train_1.shape, key_1.shape, sample_submission_1.shape)","cddba3b9":"train_1.head()","b87c885a":"key_1.head()","cc33e469":"print(key_1.Page[0])\nprint\nprint(key_1.Page[59])\nprint\nprint(key_1.Page[60])","379b0963":"sample_submission_1.head()","854ba1bf":"train_1.info()","21dbb76a":"train_1.head()","dac1d8db":"# Creating a list of wikipedia main sites \nsites = [\"wikipedia.org\", \"commons.wikimedia.org\", \"www.mediawiki.org\"]\n\n# Function to create a new column having the site part of the article page\ndef filter_by_site(page):\n    for site in sites:\n        if site in page:\n            return site\n\n# Creating a new column having the site part of the article page\ntrain_1['Site'] = train_1.Page.apply(filter_by_site)","93b99ee9":"train_1['Site'].value_counts(dropna=False)","99020abc":"import matplotlib.pyplot as plt\nimport seaborn as sns; sns.set()\n\nplt.figure(figsize=(12, 6))\nplt.title(\"Number of Wikipedia Articles by Sites\", fontsize=\"18\")\ntrain_1['Site'].value_counts().plot.bar(rot=0);","16c56a36":"# Checking which country codes exist in the article pages\ntrain_1.Page.str.split(pat=\".wikipedia.org\", expand=True).iloc[:,0].str[-3:].value_counts().index.to_list()","2d373ca1":"# Creating a list of country codes\ntrain_1.Page.str.split(pat=\".wikipedia.org\", expand=True).iloc[:,0].str[-2:].value_counts().index.to_list()[0:7]","7a01e0f0":"# Checking which agents + access exist in the article pages and creating a list with them\ntrain_1.Page.str.split(pat=\".wikipedia.org\", expand=True).iloc[:,1].str[1:].value_counts().index.to_list()","21484f38":"# Creating the list of country codes and agents\ncountries = train_1.Page.str.split(pat=\".wikipedia.org\", expand=True).iloc[:,0].str[-2:].value_counts().index.to_list()[0:7]\nagents = train_1.Page.str.split(pat=\".wikipedia.org\", expand=True).iloc[:,1].str[1:].value_counts().index.to_list()\n\n# Function to create a new column having the country code part of the article page\ndef filter_by_country(page):\n    for country in countries:\n        if \"_\"+country+\".\" in page:\n            return country\n\n# Creating a new column having the country code part of the article page\ntrain_1['Country'] = train_1.Page.apply(filter_by_country)\n\n# Function to create a new column having the agent + access part of the article page\ndef filter_by_agent(page):\n    for agent in agents:\n        if agent in page:\n            return agent\n\n# Creating a new column having the agent part of the article page\ntrain_1['Agent'] = train_1.Page.apply(filter_by_agent)","bff69281":"# Understanding what are the NaN values for the Country column\n# It seems that the URL page does not contain the country code for those cases\n\ntrain_1.Page[train_1['Country'].isna() == True]","85cd8cb5":"plt.figure(figsize=(12, 6))\nplt.title(\"Number of Wikipedia Articles by Country\", fontsize=\"18\")\ntrain_1['Country'].value_counts(dropna=False).plot.bar(rot=0);","42767af3":"train_1['Agent'].value_counts(dropna=False)","ffcada0a":"plt.figure(figsize=(12, 6))\nplt.title(\"Number of Wikipedia Articles by Agents\/Access\", fontsize=\"18\")\ntrain_1['Agent'].value_counts().plot.bar(rot=0);","228feca6":"# Creating a sample dataset from the Train dataset for analysis\ntrain_1_sample = train_1.drop(['Site','Country','Agent'], axis=1).sample(6, random_state=42)\ntrain_1_sample","40528764":"# Transposing the sample dataset to have Date Time at the index\ntrain_1_sampleT = train_1_sample.drop('Page', axis=1).T\ntrain_1_sampleT.columns = train_1_sample.Page.values\ntrain_1_sampleT.shape","3469dabe":"train_1_sampleT.head()","73bc7fdb":"# Plotting the Series from the sample dataset \nplt.figure(figsize=(16,8))\n\nfor k, v in enumerate(train_1_sampleT.columns):\n    plt.subplot(2, 3, k + 1)\n    plt.title( str(v.split(\".org\")[0])+\".org\"+\"\\n\"+str(v.split(\".org\")[1]) )\n    train_1_sampleT[v].plot()\n\nplt.tight_layout();","7babda36":"# Plotting the Series from the sample dataset at the same graph\nplt.figure(figsize=(15,8))\n\nfor v in train_1_sampleT.columns:\n    plt.plot(train_1_sampleT[v])\n    plt.legend(loc='upper center');","7fff7ebf":"# Plotting the histograms for the Series from the sample dataset\nplt.figure(figsize=(16,8))\n\nfor k, v in enumerate(train_1_sampleT.columns):\n    plt.subplot(2, 3, k + 1)\n    plt.title( str(v.split(\".org\")[0])+\".org\"+\"\\n\"+str(v.split(\".org\")[1]) )\n    sns.distplot(train_1_sampleT[v])\n\nplt.tight_layout();","ad33ffb6":"# Checking that the number of visits to the Wikipedia Articles have Gaussian Distribution (p-value=0)\nfrom scipy.stats import kstest, ks_2samp\n\npages = list(train_1_sampleT.columns)\n\nprint(\"Kolgomorov-Smirnov - Normality Test\")\nprint()\n\nfor p in pages:\n    print(p,':', kstest(train_1_sampleT[p], 'norm', alternative = 'less'))    ","6e55737c":"# List of the main Wikipedia Article sites\nsites","045eebb4":"# Creating sample datasets from the train dataset and filtering them by sites\ntrain_1_sample_site0 = train_1[train_1['Site'] == sites[0]].drop(['Site','Country','Agent'], axis=1).sample(6, random_state=42)\ntrain_1_sample_site1 = train_1[train_1['Site'] == sites[1]].drop(['Site','Country','Agent'], axis=1).sample(6, random_state=42)\ntrain_1_sample_site2 = train_1[train_1['Site'] == sites[2]].drop(['Site','Country','Agent'], axis=1).sample(6, random_state=42)\n\n# Transposing them to have the Date Time as index\ntrain_1_sampleT_site0 = train_1_sample_site0.drop('Page', axis=1).T\ntrain_1_sampleT_site0.columns = train_1_sample_site0.Page.values\ntrain_1_sampleT_site1 = train_1_sample_site1.drop('Page', axis=1).T\ntrain_1_sampleT_site1.columns = train_1_sample_site1.Page.values\ntrain_1_sampleT_site2 = train_1_sample_site2.drop('Page', axis=1).T\ntrain_1_sampleT_site2.columns = train_1_sample_site2.Page.values","c1a7cb16":"# Plotting the Series from the sample datasets\nplt.figure(figsize=(16,8))\n\nfor k, v in enumerate(train_1_sampleT_site0.columns):\n    plt.subplot(2, 3, k + 1)\n    plt.title( str(v.split(\".org\")[0])+\".org\"+\"\\n\"+str(v.split(\".org\")[1]) )\n    train_1_sampleT_site0[v].plot()\n\nplt.tight_layout();","af4b536e":"# Plotting the Series from the sample datasets at the same graph\nplt.figure(figsize=(15,8))\n\nfor v in train_1_sampleT_site0.columns:\n    plt.plot(train_1_sampleT_site0[v])\n    plt.legend(loc='upper center');","5efeb948":"# Plotting the Series from the sample datasets\nplt.figure(figsize=(16,8))\n\nfor k, v in enumerate(train_1_sampleT_site1.columns):\n    plt.subplot(2, 3, k + 1)\n    plt.title( str(v.split(\".org\")[0])+\".org\"+\"\\n\"+str(v.split(\".org\")[1]) )\n    train_1_sampleT_site1[v].plot()\n\nplt.tight_layout();","f467931d":"# Plotting the Series from the sample datasets at the same graph\nplt.figure(figsize=(15,8))\n\nfor v in train_1_sampleT_site1.columns:\n    plt.plot(train_1_sampleT_site1[v])\n    plt.legend(loc='upper center');","228403b2":"# Plotting the Series from the sample datasets\nplt.figure(figsize=(16,8))\n\nfor k, v in enumerate(train_1_sampleT_site2.columns):\n    plt.subplot(2, 3, k + 1)\n    plt.title( str(v.split(\".org\")[0])+\".org\"+\"\\n\"+str(v.split(\".org\")[1]) )\n    train_1_sampleT_site2[v].plot()\n\nplt.tight_layout();","4b0bc4b7":"# Plotting the Series from the sample datasets at the same graph\nplt.figure(figsize=(15,8))\n\nfor v in train_1_sampleT_site2.columns:\n    plt.plot(train_1_sampleT_site2[v])\n    plt.legend(loc='upper center');","23297173":"train_1_sampleT_site2.columns[4]","62f19f4a":"# List of the Wikipedia Article country codes\ncountries","1c00ad9f":"# Creating a sample dataset from the train dataset for countries having \"de\" code\ntrain_1_sample_de = train_1[train_1['Country'] == countries[2]].drop(['Site','Country','Agent'], axis=1).sample(6, random_state=42)\n\n# Transposing the sample dataset to have Date Time at the index\ntrain_1_sampleT_de = train_1_sample_de.drop('Page', axis=1).T\ntrain_1_sampleT_de.columns = train_1_sample_de.Page.values","0b819d7b":"# Plotting the Series from the sample dataset\nplt.figure(figsize=(16,8))\n\nfor k, v in enumerate(train_1_sampleT_de.columns):\n    plt.subplot(2, 3, k + 1)\n    plt.title( str(v.split(\".org\")[0])+\".org\"+\"\\n\"+str(v.split(\".org\")[1]) )\n    train_1_sampleT_de[v].plot()\n\nplt.tight_layout();","4a9ca882":"# Plotting the Series from the sample datasets at the same graph\nplt.figure(figsize=(15,8))\n\nfor v in train_1_sampleT_de.columns:\n    plt.plot(train_1_sampleT_de[v])\n    plt.legend(loc='upper center');","c1e77754":"# Import Prophet library\nfrom fbprophet import Prophet","faf1076e":"# Picked up one Time Series for the prophet modeling\ntrain_1_sampleT.columns[1]","0d72aedf":"# Creating a dataframe for the Time Series from the train_1 samples dataset\nds = pd.Series(train_1_sampleT.index)\ny = pd.Series(train_1_sampleT.iloc[:,1].values)\nframe = { 'ds': ds, 'y': y }\ndf = pd.DataFrame(frame)\ndf.head()","7cfc21ab":"df.plot();","c054250e":"# Instantiate and fit the Prophet model with no hyperparameters at all\nm = Prophet()\nm.fit(df);","7e45fdb1":"# Make dataframe for the future predictions to the next 60 days\n# By default it will also include the dates from the history\n# In summary it will have 550 + 60 days (610)\nfuture = m.make_future_dataframe(periods=60)\nfuture.tail()","9d2c202f":"# Predicting the values from the future dataframe\nforecast = m.predict(future)\nforecast[['ds', 'yhat', 'yhat_lower', 'yhat_upper']].tail()","b8d6207f":"forecast.shape","f0d812c0":"# The forecast object here is a new dataframe that includes a column yhat with the forecast, \n# as well as columns for components and uncertainty intervals\nforecast.head()","6bb8a9e1":"# Plotting the forecast by calling the Prophet.plot method and passing in the forecast dataframe\nfig1 = m.plot(forecast)","f53ce981":"# Plotting the forecast components by calling the Prophet.plot_components method\n# By default it includes the trend and seasonality of the time series\nfig2 = m.plot_components(forecast)","2d05042a":"# Plotting both the Actual values and Predict values at the same graph for comparison\nplt.figure(figsize=(15, 7))\nplt.plot(df.y)                  # Actual values in default blue color\nplt.plot(forecast.yhat, \"g\");   # Predicted values in green color","752b205c":"forecast['yhat'].tail()","371af090":"# Setting the floor value to 0 and the capacity to a lower value in the future\ndf['cap'] = 500\ndf['floor'] = 0.0\nfuture['cap'] = 500\nfuture['floor'] = 0.0\n\n# Instantiating prophet 'logistic' growth mode, then fitting and predicting future values\nm = Prophet(growth='logistic')\nforecast = m.fit(df).predict(future)\n\n# Plotting both the forecast predictions and components\nfig1 = m.plot(forecast)\nfig2 = m.plot_components(forecast)","3e20330d":"# Instantiate prophet with default seasonality parameters, fitting and predicting the future\n# Plotting both the forecast and its components\n# I will keep the default growth='linear' by now instead of 'logistic'\nm = Prophet(daily_seasonality=True, weekly_seasonality=True, yearly_seasonality=True)\nforecast = m.fit(df).predict(future)\nfig1 = m.plot(forecast)\nfig2 = m.plot_components(forecast)","d47af35e":"# Plotting both the Actual values and Predict values at the same graph for comparison\nplt.figure(figsize=(15, 7))\nplt.plot(df.y)                  # Actual values in default blue color\nplt.plot(forecast.yhat, \"g\");   # Predicted values in green color","64cb50a0":"# Checking the locations of the significant changepoints\nfrom fbprophet.plot import add_changepoints_to_plot\nfig = m.plot(forecast)\na = add_changepoints_to_plot(fig.gca(), m, forecast)","91b55830":"# Increasing the 'changepoint_range' parameter from default 80% to 90%\nm = Prophet(daily_seasonality=True, weekly_seasonality=True, yearly_seasonality=True,\n            changepoint_range=0.9)\nforecast = m.fit(df).predict(future)\nfig = m.plot(forecast)\na = add_changepoints_to_plot(fig.gca(), m, forecast)","be1d7fac":"deltas = m.params['delta'].mean(0)\nfig = plt.figure(facecolor='w', figsize=(10, 6))\nax = fig.add_subplot(111)\nax.bar(range(len(deltas)), deltas, facecolor='#0072B2', edgecolor='#0072B2')\nax.grid(True, which='major', c='gray', ls='-', lw=1, alpha=0.2)\nax.set_ylabel('Rate change')\nax.set_xlabel('Potential changepoint')\nfig.tight_layout()","e546254b":"# Changing the changepoint_range back to 80% since I don't want to make the trend more negative\n# Also increasing the changepoint_prior_scale from default 0.05 to 0.7\n# By default, changepoint_prior_scale parameter is set to 0.05, andi ncreasing it will make the trend more flexible\nm = Prophet(daily_seasonality=True, weekly_seasonality=True, yearly_seasonality=True,\n            changepoint_range=0.8, changepoint_prior_scale=0.7)\nforecast = m.fit(df).predict(future)\nfig = m.plot(forecast)\na = add_changepoints_to_plot(fig.gca(), m, forecast)","c152ff04":"deltas = m.params['delta'].mean(0)\nfig = plt.figure(facecolor='w', figsize=(10, 6))\nax = fig.add_subplot(111)\nax.bar(range(len(deltas)), deltas, facecolor='#0072B2', edgecolor='#0072B2')\nax.grid(True, which='major', c='gray', ls='-', lw=1, alpha=0.2)\nax.set_ylabel('Rate change')\nax.set_xlabel('Potential changepoint')\nfig.tight_layout()","db492d56":"# Plotting both the Actual values and Predict values at the same graph for comparison\nplt.figure(figsize=(15, 7))\nplt.plot(df.y)                  # Actual values in default blue color\nplt.plot(forecast.yhat, \"g\");   # Predicted values in green color","25edadbe":"train_1_sampleT.columns[1]","d53a5eae":"\"_es.\" in train_1_sampleT.columns[1]","67242a41":"from datetime import date\nimport holidays\n\n# Select country\nes_holidays = holidays.Spain(years = [2015,2016,2017])\nes_holidays = pd.DataFrame.from_dict(es_holidays, orient='index')\nes_holidays = pd.DataFrame({'holiday': 'Spain', 'ds': es_holidays.index})","f9e5f09f":"es_holidays.head()","ce901771":"# Instantiate prophet with seasonality, changepoints and holidays parameters\nm = Prophet(daily_seasonality=True, weekly_seasonality=True, yearly_seasonality=True,\n            changepoint_range=0.8, changepoint_prior_scale=0.7,\n            holidays=es_holidays)\nm.add_country_holidays(country_name='ES')\n# Fitting and predicting the future\nforecast = m.fit(df).predict(future)\n# Plotting both the forecast and its components\nfig1 = m.plot(forecast)\nfig2 = m.plot_components(forecast)","c316d551":"# Instantiate prophet with seasonality, changepoints and holidays parameters\nm = Prophet(daily_seasonality=True, weekly_seasonality=True, yearly_seasonality=True,\n            changepoint_range=0.8, changepoint_prior_scale=0.7,\n            holidays=es_holidays,\n            interval_width=0.95,\n            mcmc_samples=0)\nm.add_country_holidays(country_name='ES')\n# Fitting and predicting the future\nforecast = m.fit(df).predict(future)\n# Plotting both the forecast and its components\nfig1 = m.plot(forecast)\nfig2 = m.plot_components(forecast)","83e7950d":"deltas = m.params['delta'].mean(0)\nfig = plt.figure(facecolor='w', figsize=(10, 6))\nax = fig.add_subplot(111)\nax.bar(range(len(deltas)), deltas, facecolor='#0072B2', edgecolor='#0072B2')\nax.grid(True, which='major', c='gray', ls='-', lw=1, alpha=0.2)\nax.set_ylabel('Rate change')\nax.set_xlabel('Potential changepoint')\nfig.tight_layout()","cb9c128c":"plt.figure(figsize=(15, 7))\nplt.plot(df.y)\nplt.plot(forecast.yhat, \"g\");","1b469164":"from fbprophet.plot import plot_plotly\nimport plotly.offline as py\npy.init_notebook_mode()\n\nfig = plot_plotly(m, forecast)  # This returns a plotly Figure\npy.iplot(fig)","f0f92de0":"m.params","bc41830a":"def smape(y_true, y_pred):\n    denominator = (np.abs(y_true) + np.abs(y_pred))\n    diff = np.abs(y_true - y_pred) \/ denominator\n    diff[denominator == 0] = 0.0\n    return 200 * np.mean(diff)\n\n# Source: http:\/\/shortnotes.herokuapp.com\/how-to-implement-smape-function-in-python-149","f681e0a7":"smape_single_page = smape(df.y, forecast.yhat)\nsmape_single_page","e15831ca":"from fbprophet.diagnostics import cross_validation","33bfd3c7":"# horizon: forecast horizon\n# initial: size of the initial training period\n# period: spacing between cutoff dates\n#\n# Here we do cross-validation to assess prediction performance on a horizon of 60 days, \n# starting with 130 days of training data in the first cutoff and then making predictions every 60 days\n# On this 610 days time series, this corresponds to 8 total forecasts\n\ncv_results = cross_validation(m, initial='360 days', period='30 days', horizon='60 days')","ef8352b1":"smape_baseline = smape(cv_results.y, cv_results.yhat)\nsmape_baseline","af9a31c1":"train_1_all = train_1.drop(['Page','Site','Country','Agent'], axis=1).T\ntrain_1_all.columns = train_1.Page.values\ntrain_1_all.shape","5b189c61":"train_1_all.head()","3b65fabe":"# Filling up NaN values with 0 visits to avoid breaking the model fit\ntrain_1_all.fillna(0, inplace=True)\n\n# Selecting a few series to run the Prophet model against\nnum_series = 10\ntrain_1_sample = train_1_all.sample(num_series, axis=1, random_state=42)","cb0d3475":"# Plotting the Series from the sample datasets at the same graph\nplt.figure(figsize=(15,8))\n\nfor v in train_1_sample.columns:\n    plt.plot(train_1_sample[v])\n    plt.legend(loc='upper center');","5e1f2cd6":"%%time\n\nsmape_partial = 0\n\nfor k, v in enumerate(train_1_sample.columns):\n    ds = pd.Series(train_1_sample.index)\n    y = pd.Series(train_1_sample.iloc[:,k].values)\n    frame = { 'ds': ds, 'y': y }\n    df = pd.DataFrame(frame)\n    m_partial = Prophet(daily_seasonality=True, weekly_seasonality=True, yearly_seasonality=True)\n    forecast = m_partial.fit(df).predict(future)\n    smape_partial += smape(df.y, forecast.yhat)\n\nsmape_average = smape_partial \/ len(train_1_sample.columns)\nsmape_average","e925f872":"# train_1_sampleT.columns[1]+\"_\"+\"2017-01-01\"\n# train_1_sampleT.columns[1]+\"_\"+\"2017-01-01\" in list(key_1.Page.values)","f8c33a26":"### Prophet - Holidays","0cb10568":"**Time Series of \"WIKIPEDIA.ORG\" sites only**","1d71ddfa":"Now I will explore the use of Prophet changepoints to automatically detect these abrupt changes in the time series trajectories and see if it will allow the trend to adapt appropriately. ","201b0c2f":"Notes:\n\nFor all the sites samples, some series presented missing data (NaNs).\n\nFor one of the WWW.MEDIAWIKI.ORG Series sample, noticed there was no data at all.  \nFor this series, the URL contains the IP address instead of DNS name and it starts with \"User:\"","339305de":"### Prophet - Seasonality","fc1724bc":"## Exploratory Data Analisys (EDA)","e173b477":"## Multiple Time Series in parallel  \n\nAnother idea could be the use of Python multiprocessing package to forecast multiple Time Series in parallel.  \n\nSource:  \n\n<a href=\"https:\/\/medium.com\/spikelab\/forecasting-multiples-time-series-using-prophet-in-parallel-2515abd1a245\">Forecasting multiple time-series using Prophet in parallel<\/a>","5e463867":"In summary:\n\nWe need to predict the number of visits for the period between 2017-01-01 to 2017-03-1 (60 days) from training data (train_1) containing the visits to the 145063 pages in previous period given between 2015-07-01 to 2016-12-31 (550 days).","b6eccb7d":"## Evaluating the Model","c5067569":"The width of the uncertainty intervals (by default 80%) can be set using the parameter interval_width.  \nI will increase it to 95%.","b45bc361":"Facebook Prophet function is used do define a Prophet forecasting model in Python.  \n\nI will now use Prophet to model a specific Time Series got from samples of the training dataset. ","95009a7f":"### Prophet - Uncertainty interval","47b4c305":"By default changepoints are only inferred for the first 80% of the time series in order to have plenty of runway for projecting the trend forward and to avoid overfitting fluctuations at the end of the time series.\n\nSince I still see some changepoints after 80%, I will increase it to check for other ones.","66a8bd58":"## Modeling with Facebook Prophet","fe417c79":"SMAPE function\n\n$$ SMAPE = \\frac{100\\%}{n} \\sum_{t=1}^{n} \\frac{\\left|F_t - A_t\\right|}{(\\left|A_t\\right|+\\left|F_t\\right|)\/2} $$","707680f8":"#### Uncertainty in seasonality","d29b634d":"As per the above results, the time Series prediction shows a trend to the bottom, reaching negative values, which is not accepted in this case. There should be no negative visits to a Wikipedia Article...\n\nFor this reason, I tried to use the prophet logistic growth model handling a Saturating Minimum, setting the floor value to zero. However, in order to use a logistic growth trend with a saturating minimum, a maximum capacity must also be specified.","8e52f3f0":"**Time Series of \"COMMONS.WIKIMEDIA.ORG\" sites only**","5fd9cb66":"### Prophet - Cross Validation","b7fe6e91":"Let us look at a summary of some of the most important Prophet parameters for reference.\n\n**Trend parameters**\n\nParameter and Description\n\n- growth -> linear\u2019 or \u2018logistic\u2019 to specify a linear or logistic trend\n- changepoints -> List of dates at which to include potential changepoints (automatic if not specified)\n- n_changepoints -> If changepoints is not supplied, you may provide the number of changepoints to be automatically included\n- changepoint_prior_scale -> Parameter for changing flexibility of automatic changepoint selection\n\n**Seasonality & Holiday Parameters**\n\nParameter and Description\n\n- yearly_seasonality -> Fit yearly seasonality\n- weekly_seasonality -> Fit weekly seasonality\n- daily_seasonality -> Fit daily seasonality\n- holidays -> Feed dataframe containing holiday name and date\n- seasonality_prior_scale -> Parameter for changing strength of seasonality model\n- holidays_prior_scale -> Parameter for changing strength of holiday model\n\nSource: https:\/\/www.analyticsvidhya.com\/blog\/2018\/05\/generate-accurate-forecasts-facebook-prophet-python-r\/","a8720779":"**key_1.csv**\n\n- Contains 8.703.780 rows, each one representing the \"URL page\"_\"datetime\", where datetime varies from 2017-01-01 to 2017-03-01 (total of 60 days), which is the result of the total number of pages multiplied by 60 days (145063 x 60 = 8.703.780)\n- Contains 2 columns, first one is the \"URL page\"_\"datetime\", second one is the ID for that page","166b0d88":"Calculating the SMAPE for the time series prediction for the visits at a single URL page","51523eb0":"**Conclusion:** In this case, the fit was much better, which was expected since the seasonality capture the most relevant frequencies. Seasonalities are estimated using a partial Fourier sum. However, we could not capture the high picks.","0ac08edd":"## Multivariate Time Series models","291289c8":"## Collecting DATA","252e58ca":"### Exploring a Group of Time Series for a Specific Country - DE","69973e93":"**Conclusion:** in this case the prediction trend reached the capacity value defined (500). I will need to explore other prophet parameters to get better results. ","831466b7":"**sample_submission_1.csv**\n\n- Contains 8.703.780 rows, each one having the ID for the page and respective number of visits to the page at that datetime","9935ef08":"This parameter determines if the model uses Maximum a posteriori (MAP) estimation or a full Bayesian inference with the specified number of Markov Chain Monte Carlo (MCMC) samples to train and predict.\nSo if you make MCMC zero then it will do MAP estimation, otherwise you need to specify the number of samples to use with MCMC.\n\nSource: <a href=\"https:\/\/towardsdatascience.com\/implementing-facebook-prophet-efficiently-c241305405a3\">Implementing Facebook Prophet efficiently<\/a>\n\nSince we are using the SMAPE as the evaluation metric, I decided to keep mcmc_samples parameters to the default zero value.","040d68a2":"I could be using Multivariate Time Series (MTS) instead of the univariate models against all Time Series.  \nFollowing this approach, below are some ideas I could try in the future:\n\n- Vector Auto Regression (VAR)\n  - Johansen\u2019s test for checking the stationarity of any multivariate time series data  \n    (statsmodels.tsa.vector_ar.vecm import coint_johansen)\n  - Fit the model using VAR model from statsmodel library  \n    (from statsmodels.tsa.vector_ar.var_model import VAR)  \n- Random Forest  \n- Recurrent Neural Networs (RNN)  \n\nSources:  \n\n<a href=\"https:\/\/link.medium.com\/miaEiLC0c1\">A Multivariate Time Series Guide to Forecasting and Modeling (with Python codes)<\/a>)  \n<a href=\"https:\/\/towardsdatascience.com\/multivariate-time-series-forecasting-using-random-forest-2372f3ecbad1\">Multivariate Time Series Forecasting Using Random Forest<\/a>)  \n<a href=\"https:\/\/link.medium.com\/XFbTA4O0c1\">Interpreting recurrent neural networks on multivariate time series<\/a>","e38d8882":"## An interactive figure of the forecast created with Plotly","cde1cea1":"#### Uncertainty in the trend","e5cae3d7":"**train_1.csv**\n\n- Contains 145.063 rows representing different Wikipedia URL pages\n- Contains 551 columns, first column is the URL page and then each column represents a value in time from 2015-07-01 to 2016-12-31 (1.5 year, total of 550 days), where the value is the number of visits to the page in that day\n\nJul\/2015 - 31 days  \nAug\/2015 - 31 days  \nSep\/2015 - 30 days  \nOct\/2015 - 31 days  \nNov\/2015 - 30 days  \nDec\/2015 - 31 days  \n\nTotal: 184 days\n\n2016 - 366 days (leap year)\n\nTotal: 184 + 366 = 550 days","35262afd":"## Prophet - Running for Multiple Time Series","4e703048":"### Prophet - Changepoints","e72c80af":"### Prophet - Saturating forecasts","969130ad":"## Submitting to Kaggle","cec503b5":"I will include the default seasonality parameters to the Prophet model now.","b56f262a":"### Prophet - All parameters","8c410942":"**Conclusion:** The trend is going down faster when increasing the changepoint_range, making the prediction values more negative, which doesn't make sense. So I will keep changepoint_range to default 80%.","ffadf92e":"**Conclusion**: in this case it was possible to capture only the trend","00d85030":"### Exploring Groups of Time Series for Different Sites     ","912623c3":"# Web Traffic Time Series Forecasting\n\n**Forecast future traffic to Wikipedia pages**","81f7c7cc":"## Understanding the DATA","a8889c58":"Now I will include a dataframe for holidays. Since the wikipedia article time series I am analyzing has the country code \"es\", I will use the Spain holiday. I will also add years from 2015 to 2017 to the dataframe.","0915f591":"**Time Series of \"WWW.MEDIAWIKI.ORG\" sites only**","9b946c1f":"**Conclusion:** Now we got a pretty good model at this point."}}