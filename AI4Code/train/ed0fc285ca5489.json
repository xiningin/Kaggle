{"cell_type":{"0cfb9dd3":"code","eaa58bc2":"code","c7fecee8":"code","491e3f81":"code","6fd7c172":"code","5fa62ce3":"code","370a8234":"code","2bbe0868":"code","3b62e333":"code","b25f18ae":"code","d4c2ebe3":"code","4111fd61":"code","8d4da54d":"code","c4e12e44":"code","07dd68f4":"code","ece1e9f4":"code","c3b38144":"code","c2c4febe":"code","99e7e1fe":"code","47989da4":"code","0937af34":"markdown","dc16a07a":"markdown","f26d707b":"markdown","d9af4cf0":"markdown","4f91a3d9":"markdown"},"source":{"0cfb9dd3":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","eaa58bc2":"data=pd.read_csv(\"..\/input\/iris\/Iris.csv\")","c7fecee8":"data.head()","491e3f81":"data.info()","6fd7c172":"x=data.iloc[:,1:5].values # \u0130\u015flem g\u00f6recek de\u011ferler.\ny=data.iloc[:,5:].values # Tahmin edilecek de\u011ferler.\n","5fa62ce3":"from sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.33,random_state=0)\n#Verileri par\u00e7alad\u0131k.","370a8234":"from sklearn.preprocessing import StandardScaler\nss=StandardScaler()\nX_train=ss.fit_transform(x_train)\nX_test=ss.transform(x_test)\n#X de\u011ferlerimizi normalize ettik. ","2bbe0868":"from sklearn.linear_model import LogisticRegression\nlogr=LogisticRegression(random_state=0)\nlogr.fit(X_train,y_train) # Train de\u011ferlerine \u00f6\u011frenmesini istedik\ny_pred=logr.predict(X_test) # X_test de\u011ferine bak\u0131p y_test de\u011ferlerine tahmin ettirdik.","3b62e333":"from sklearn.metrics import confusion_matrix\ncm=confusion_matrix(y_test,y_pred) # Ne kadar de\u011feri do\u011fru tahmin ettik diye g\u00f6zlem yapmak i\u00e7in kulland\u0131k. (1,1)(2,2)(3,3) do\u011fru de\u011ferlerdir.\ncm\n","b25f18ae":"from sklearn.neighbors import KNeighborsClassifier\nknn=KNeighborsClassifier()\nknn.fit(X_train,y_train)\ny_pred=knn.predict(X_test)\n\ncm=confusion_matrix(y_test,y_pred)\ncm","d4c2ebe3":"from sklearn.svm import SVC\nsvc=SVC(kernel=\"linear\")\nsvc.fit(X_train,y_train)\ny_pred=svc.predict(X_test)\n\ncm=confusion_matrix(y_test,y_pred)\ncm","4111fd61":"from sklearn.naive_bayes import GaussianNB\ngnb=GaussianNB()\ngnb.fit(X_train,y_train)\ny_pred=gnb.predict(X_test)\n\ncm=confusion_matrix(y_test,y_pred)\ncm","8d4da54d":"from sklearn.tree import DecisionTreeClassifier\ndtc=DecisionTreeClassifier(criterion=\"gini\")\ndtc.fit(X_train,y_train)\ny_pred=dtc.predict(X_test)\n\ncm=confusion_matrix(y_test,y_pred)\ncm","c4e12e44":"x","07dd68f4":"#K-Means Algoritmas\u0131\nfrom sklearn.cluster import KMeans\nkm=KMeans(n_clusters=3,init=\"k-means++\")# 3 de\u011ferini kafandan vermemiz yanl\u0131\u015f sonuca g\u00f6t\u00fcrebilir. \nkm.fit(x) # ilk ba\u015fta belli de\u011ferler ile kontrol etmemiz laz\u0131m en iyi \u00e7al\u0131\u015ft\u0131\u011f\u0131 de\u011feri \u00f6\u011frenmek i\u00e7in.\nsonuclar=[]\nfor i in range(1,11):\n    km=KMeans(n_clusters=i,init=\"k-means++\",random_state=123)# random_state =111 dememizin sebebi ayn\u0131 de\u011ferlerden ba\u015fl\u0131yor olmas\u0131. Farkl\u0131 de\u011ferde verilebilir.\n    km.fit(x)\n    sonuclar.append(km.inertia_)\n# \u015eimdi en iyi se\u00e7ene\u011fi g\u00f6rmek i\u00e7in sonuclar\u0131 \u00e7izdirmemiz laz\u0131m\nimport matplotlib.pyplot as plt\nplt.plot(range(1,11),sonuclar)\nplt.show()","ece1e9f4":"# Grafi\u011fe bak\u0131ld\u0131\u011f\u0131nda en iyi noktam\u0131z 3 ile 4 olur ama 4'ten sonra azalma az oldu\u011fu i\u00e7in 4 en iyi nokta olacakt\u0131r.\n# Bu y\u00fczden 4 noktas\u0131n\u0131 se\u00e7ip tekrar g\u00f6relim\nkm=KMeans(n_clusters=4,init=\"k-means++\",random_state=123)\ntahmin=km.fit_predict(x)\n","c3b38144":"fig = data[data.Species=='Iris-setosa'].plot(kind='scatter',x='SepalLengthCm',y='SepalWidthCm',color='orange', label='Setosa')\ndata[data.Species=='Iris-versicolor'].plot(kind='scatter',x='SepalLengthCm',y='SepalWidthCm',color='blue', label='versicolor',ax=fig)\ndata[data.Species=='Iris-virginica'].plot(kind='scatter',x='SepalLengthCm',y='SepalWidthCm',color='green', label='virginica', ax=fig)\nfig.set_title(\"\u0130lk ba\u015ftaki veri da\u011f\u0131l\u0131m\u0131\")\nplt.show()","c2c4febe":"plt.scatter(x[tahmin==0,0],x[tahmin==0,1],color=\"r\",s=100)\nplt.scatter(x[tahmin==1,0],x[tahmin==1,1],color=\"g\",s=100)\nplt.scatter(x[tahmin==2,0],x[tahmin==2,1],color=\"b\",s=100)\nplt.scatter(x[tahmin==3,0],x[tahmin==3,1],color=\"y\",s=100)\nplt.title(\"K-Means uyguland\u0131ktan sonraki veri da\u011f\u0131l\u0131m\u0131\")\nplt.show()\n","99e7e1fe":"#Hierarchical Clustering ( Hiyerar\u015fik K\u00fcmeleme )\nfrom sklearn.cluster import AgglomerativeClustering\nac=AgglomerativeClustering(n_clusters=4,affinity=\"euclidean\",linkage=\"ward\") \ntahmin=ac.fit_predict(x)\n\nplt.scatter(x[tahmin==0,0],x[tahmin==0,1],color=\"r\",s=100)\nplt.scatter(x[tahmin==1,0],x[tahmin==1,1],color=\"g\",s=100)\nplt.scatter(x[tahmin==2,0],x[tahmin==2,1],color=\"b\",s=100)\nplt.scatter(x[tahmin==3,0],x[tahmin==3,1],color=\"y\",s=100)\nplt.title(\"AgglomerativeClustering\")\nplt.show()\n#Burada da dendrogram ile Ka\u00e7 k\u00fcme olu\u015faca\u011f\u0131n\u0131 tahmin edebiliriz.","47989da4":"import scipy.cluster.hierarchy as sch\nden=sch.dendrogram(sch.linkage(x,method=\"ward\"))\nplt.show()\n# Bu \u015fekilde dendrogram \u00e7izilebilir ve yorum yap\u0131labilir. \n# G\u00f6r\u00fcnt\u00fc b\u00fcy\u00fcd\u00fc\u011f\u00fc zaman daha iyi anla\u015f\u0131lacakt\u0131r. Hangi k\u00fcmelerin se\u00e7ildi\u011fi vs.","0937af34":"**Naive-Bayes**\n= Verimlidir, do\u011frusal olmayan problemler \u00fczerinde \u00e7al\u0131\u015f\u0131r,olas\u0131l\u0131ksal yakla\u015f\u0131md\u0131r.","dc16a07a":"**Decision Tree (Karar A\u011fa\u00e7lar\u0131)**\n= Do\u011frusal  ve do\u011frusal olmayan problemler \u00fczerinde de \u00e7al\u0131\u015f\u0131r. ","f26d707b":"**SVM**\n= \u0130ki s\u0131n\u0131fa ait verileri birbirinden en uygun \u015fekilde ay\u0131rmak i\u00e7in kullan\u0131l\u0131r. A\u015f\u0131r\u0131 \u00f6\u011frenmeye duyarl\u0131 de\u011fildir.\nDo\u011frusal olmayan problemler i\u00e7in ise uygun de\u011fildir.","d9af4cf0":"** K-NN **\n= K-en yak\u0131n kom\u015fuluk algoritmas\u0131 , uygulamas\u0131 kolay , anlamas\u0131 basit ve h\u0131zl\u0131 g\u00f6zetimli \u00f6\u011frenme algoritmas\u0131d\u0131r.\nDezavantajlar\u0131ndan en \u00f6nemlisi ise k say\u0131s\u0131n\u0131 do\u011fru se\u00e7ememektir.\n","4f91a3d9":"**Logistic Regression**\n = Logistic Regression benzer olarak d\u00fczlem \u00fczerindeki verileri yakalamaya \u00e7al\u0131\u015f\u0131r. Bunlar\u0131 do\u011frusal de\u011filde logaritmik e\u011fri \u00fczerinde yakalar.\nOlas\u0131l\u0131ksal yakla\u015f\u0131m, \u00f6zelliklerin istatiksel \u00f6nemi hakk\u0131nda bilgi verir.\nGenel form\u00fcl\u00fc = 1 \/ (1 + e^-x) 'dir."}}