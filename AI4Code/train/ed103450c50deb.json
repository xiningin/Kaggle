{"cell_type":{"f44bf4e2":"code","78b6c9b8":"code","514b5ebb":"code","f1e0f107":"code","f2668d71":"code","c0810be5":"code","328a1602":"code","d0d7164b":"code","6eabc5f2":"code","dbd04acb":"code","adcfdcab":"code","b166f85b":"code","9b93c494":"code","15eaccc3":"code","bf6798f8":"code","66fcd363":"code","f2ec770e":"markdown","814c6034":"markdown","3c58dc5c":"markdown","724c7843":"markdown","ff6bf60b":"markdown","824fc35a":"markdown","62b045f2":"markdown","a37ae231":"markdown","ef198266":"markdown","04b29765":"markdown","577a13d1":"markdown","7eca3dc4":"markdown","d9f632cc":"markdown","558529d2":"markdown","c6bed0ac":"markdown","b18e5a0d":"markdown","fd7d5656":"markdown","1b0a9570":"markdown","196bdbc8":"markdown","46146629":"markdown","cd0b20b2":"markdown","66d14977":"markdown"},"source":{"f44bf4e2":"import numpy as np\nimport pandas as pd\npd.options.display.float_format = '{:0.2f}'.format\n\nfrom bokeh.layouts import row, column\nfrom bokeh.plotting import figure, show\nfrom bokeh.colors import RGB\nfrom bokeh.models import Arrow, VeeHead, Label\nfrom bokeh.models.widgets import Panel, Tabs\nfrom bokeh.io import output_notebook\noutput_notebook()\n\nfrom IPython.display import display\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","78b6c9b8":"def plotSimple(title, xTitle, yTitle, x, y, y_range=(0, 100)):\n    p = figure(title = title, plot_width=300, plot_height=250, y_range=y_range)\n    p.circle(x, y, size=3, color=\"red\", alpha=1)\n    p.line(x, y, color=\"red\", alpha=1)\n    p.xaxis.axis_label = xTitle\n    p.yaxis.axis_label = yTitle\n    return p\n\ndef plotExperiment(xName, yName, experiment, df, model, blurcount, y_range=(0, 100)):\n    def blur():\n        lottery = np.random.random_sample(size=df.count()[0])\n        lottery = [0.00 <= r < 0.70 for r in lottery]\n        X = df[lottery][df.X]\n        y = df[lottery][df.y]\n        model.fit(X=X, y=y)\n        Y = model.predict(experiment)\n        p.line(experiment[xName], Y, color=\"lightblue\",line_width=6, line_alpha=1)    \n        p.line(experiment[xName], Y, color=\"blue\",line_width=0, line_alpha=0.5)    \n    p = figure(title = type(model).__name__, plot_width=300, plot_height=250, y_range=y_range)\n    [blur() for i in range(blurcount)]\n    Y = model.predict(X=experiment)\n    p.line(experiment[xName], Y, color=\"red\", line_width=3, line_alpha=1)    \n    p.xaxis.axis_label = xName\n    p.yaxis.axis_label = yName\n    return p    \n    \ndef plotRegression(df, model):  \n    def plotRegression1(selection, color, alpha=1):\n        X = df[selection][df.X]\n        y = df[selection][df.y]\n        Y = model.predict(X)\n        r2 = np.corrcoef(y, Y)[0,1]\n        se = np.std(y - Y)\n        p.circle(y.tolist(), Y, size=3, color=color, alpha=alpha)\n        p.line(y, y, color=\"red\")\n        return [r2,se]\n    model.fit(X=df[df.train][df.X], y=df[df.train][df.y])    \n    p = figure(title = type(model).__name__, plot_width=400, plot_height=400)\n    trainr2, trainse = plotRegression1(df.train, \"red\")\n    validr2, validse = plotRegression1(df.valid, \"blue\")\n    testr2,  testse  = plotRegression1(df.test,  \"green\")\n    r2text = \"R2: train={r1: 6.2f}, valid={r2: 6.2f}, test={r3: 6.2f}\".format(r1=trainr2,r2=validr2,r3=testr2)\n    setext = \"SE: train={r1: 6.2f}, valid={r2: 6.2f}, test={r3: 6.2f}\".format(r1=trainse,r2=validse,r3=testse)\n    p.text(x=[0.5], y = [0.2], text=[r2text], text_font_size=\"10pt\")\n    p.text(x=[0.5], y = [0.0], text=[setext], text_font_size=\"10pt\")    \n    p.xaxis.axis_label = \"measured\"\n    p.yaxis.axis_label = \"predicted\"\n    return p\n\ndef draw_PCAcomponents(df, ca, cb):\n    def draw_component(df, feature, ca, cb):\n        x_end=df[feature][ca]\n        y_end=df[feature][cb]\n        rl = (x_end**2+y_end**2)**0.5\n        xl = x_end\/rl * (1+rl)\n        yl = y_end\/rl * (1+rl)\n        xl = x_end * 2\n        yl = y_end * 2\n        angle = np.arctan2(y_end,x_end)\n        p.add_layout(Arrow(end=VeeHead(size=0), line_color=\"lightgrey\", x_end=xl, y_end=yl))\n        p.add_layout(Arrow(end=VeeHead(size=5), line_color=\"red\", x_end=x_end, y_end=y_end))\n        p.add_layout(Label(text = feature, x = xl, y = yl, text_font_size=\"8pt\", text_color=\"blue\", angle=angle))\n    size = 1.5\n    p = figure(title = \"title\", plot_width=400, plot_height=400, y_range=(-size,size), x_range=(-size,size))\n    p.xaxis.axis_label = \"C_\" + str(ca)\n    p.yaxis.axis_label = \"C_\" + str(cb)\n    a = np.array([a*2*np.pi\/36 for a in range(37)])\n    [p.line(r*np.cos(a), r*np.sin(a), color=\"grey\", alpha=r) for r in [0.2,0.4,0.6,0.8,1.0]]\n    [draw_component(df, feature, ca, cb) for feature in df.columns.tolist()]\n    return p\n\ndef myCorrelationPlot(df, pw=300, ph=300, tt=\"correlations\"):\n    colNames = df.columns.tolist()\n    rownames = df.index.tolist()\n    x = [c for r in rownames for c in colNames]\n    y = [r for r in rownames for c in colNames]\n    corarr = [df[c][r] for r in rownames for c in colNames]\n    colors = [RGB(255*(1-x)\/2,255*(1+x)\/2,0,0.7) for x in corarr]\n    p = figure(title=tt, x_range=colNames, y_range=rownames, plot_width=pw, plot_height=ph, toolbar_location=\"right\")\n    p.rect(x, y, color=colors, width=1, height=1)\n    p.xaxis.major_label_orientation = 3.14159\/2\n    c = myColorBar(75, ph)\n    return row(p,c)\n\ndef myColorBar(pw=75, ph=300, tt=\"colors\"):\n    from bokeh.models import CategoricalAxis\n    corarr = [-1.0, -0.8, -0.6, -0.4, -0.2, 0.0, 0.2, 0.4, 0.6, 0.8, 1.0]\n    colors = [RGB(255*(1-x)\/2,255*(1+x)\/2,0,0.7) for x in corarr]\n    colNames = [\"color\"]\n    rownames = [str(x) for x in corarr]\n    x = [c for r in rownames for c in colNames]\n    y = [r for r in rownames for c in colNames]\n    p = figure(title=tt, x_range=colNames, y_range=rownames, plot_width=pw, plot_height=ph, toolbar_location=None)\n    p.rect(x, y, color=colors, width=1, height=1)\n    p.xaxis.major_label_orientation = 3.14159\/2\n    return p\n\ndef panelTabs(ps):\n    ts = [Panel(child=p, title=p.title.text) for p in ps]\n    layout = Tabs(tabs=ts)\n    show(layout)","514b5ebb":"url = \"..\/input\/Concrete_Data_Yeh.csv\"\nbeton = pd.read_csv(url)\nbeton['w\/c'] = beton['water']\/beton['cement']\ndisplay(beton.describe())","f1e0f107":"show(myCorrelationPlot(beton.corr()))","f2668d71":"from sklearn.decomposition import PCA\nbetonNorm2 = (beton-beton.mean())\/beton.std()\npca = PCA().fit(betonNorm2)\n\nvar = pca.explained_variance_ratio_\nvarcum = [sum([w for w in var if w>=v]) for v in var]\n\np = plotSimple(\"cumulated variance\",\"component\",\"cum. var.\", range(9), varcum, (0.2,1.1))\nshow(p)\n\nnp.set_printoptions(formatter={'float': '{: 0.2f}'.format})\npd.options.display.float_format = '{:0.2f}'.format\n\npcadf = pd.DataFrame(pca.components_)\npcadf = pcadf.T\npcadf.columns = beton.columns\n\np1 = draw_PCAcomponents(pcadf, 0, 1)\np2 = draw_PCAcomponents(pcadf, 1, 2)\np3 = draw_PCAcomponents(pcadf, 2, 3)\np4 = draw_PCAcomponents(pcadf, 3, 4)\nshow(column(row(p1,p2),row(p3,p4)))","c0810be5":"betonNorm = beton\/beton.mean()\nbetonNorm.X = ['cement', 'slag', 'flyash', 'water', 'superplasticizer', 'coarseaggregate', 'fineaggregate', 'age']\nbetonNorm.y = 'csMPa'","328a1602":"lottery = np.random.random_sample(size=betonNorm.cement.count())\nbetonNorm.train = [0.00 <= r < 0.70 for r in lottery]\nbetonNorm.valid = [0.70 <= r < 0.85 for r in lottery]\nbetonNorm.test  = [0.85 <= r < 1.00 for r in lottery]","d0d7164b":"from sklearn import linear_model\nlinear = linear_model.LinearRegression()\np1 = plotRegression(betonNorm, linear)\nimportances = abs(linear.coef_)\/np.linalg.norm(linear.coef_)\nimportances = pd.DataFrame(data=importances, columns=['importance'], index = betonNorm.X)\nimportances.sort_values(by=\"importance\", ascending=False, inplace=True)\ndisplay(importances.T)\nshow(p1)","6eabc5f2":"from sklearn import linear_model\nransac = linear_model.RANSACRegressor(linear_model.LinearRegression())\np1 = plotRegression(betonNorm, ransac)\n\ninliers    = betonNorm[betonNorm.train]\ninliers.y  = inliers[betonNorm.y]\ninliers.X  = inliers[betonNorm.X]\noutliers   = betonNorm[betonNorm.train][np.logical_not(ransac.inlier_mask_)]\noutliers.y = outliers[betonNorm.y]\noutliers.X = outliers[betonNorm.X]\noutliers.p = ransac.predict(X=outliers.X)\n\nfrom sklearn import linear_model\nransacInliers = linear_model.LinearRegression()\np2 = plotRegression(betonNorm, ransacInliers)\np2.circle(outliers.y, outliers.p, size=5, color=\"red\", alpha=0.1)\n\nimportances = abs(ransac.estimator_.coef_)\/np.linalg.norm(ransac.estimator_.coef_)\nimportances = pd.DataFrame(data=importances, columns=['importance'], index = betonNorm.X)\nimportances.sort_values(by=\"importance\", ascending=False, inplace=True)\ndisplay(importances.T)\nshow(p2)","dbd04acb":"means = pd.DataFrame(inliers.mean(), columns=[\"inliers mean\"])\nmeans[\"outliers mean\"] = outliers.mean()\ndisplay(means.T)","adcfdcab":"from sklearn.tree import DecisionTreeRegressor\ntree = DecisionTreeRegressor(min_samples_split=10)\np1 = plotRegression(betonNorm, tree)\n\nimportances = pd.DataFrame(data=tree.feature_importances_, columns=['importance'], index = betonNorm.X)\nimportances.sort_values(by=\"importance\", ascending=False, inplace=True)\ndisplay(importances.T)\nshow(p1)","b166f85b":"from sklearn.ensemble import RandomForestRegressor\nforest = RandomForestRegressor(n_estimators=10, max_depth=None, min_samples_split=10, random_state=0)\np1 = plotRegression(betonNorm, forest)\n\nimportances = pd.DataFrame(data=forest.feature_importances_, columns=['importance'], index = betonNorm.X)\nimportances.sort_values(by=\"importance\", ascending=False, inplace=True)\ndisplay(importances.T)\nshow(p1)","9b93c494":"from sklearn import svm\nsvr = svm.SVR(kernel='rbf', C=1)\np1 = plotRegression(betonNorm, svr)\nshow(row(p1))","15eaccc3":"from sklearn.neural_network import MLPRegressor\nneural = MLPRegressor(solver='lbfgs', alpha=0.01, hidden_layer_sizes=(8), activation=\"logistic\", max_iter=10000)\np1 = plotRegression(betonNorm, neural)  \n\nimportances = [max(abs(neural.coefs_[0][i])) for i in range(8)]\nimportances = pd.DataFrame(data=importances, columns=['importance'], index = betonNorm.X)\nimportances.sort_values(by=\"importance\", ascending=False, inplace=True)\ndisplay(importances.T)\nshow(p1)","bf6798f8":"nsteps                = 50\nexperiment1                  = beton.head(nsteps).copy()\nexperiment1.cement           = beton.cement.mean()\nexperiment1.slag             = beton.slag.mean()\nexperiment1.flyash           = beton.flyash.mean()\nexperiment1.water            = beton.water.mean()\nexperiment1.superplasticizer = beton.superplasticizer.mean()\nexperiment1.coarseaggregate  = beton.coarseaggregate.mean()\nexperiment1.fineaggregate    = beton.fineaggregate.mean()\nexperiment1.age              = beton.age.mean()\nexperiment1.age              = 28\nxxx                          = np.linspace(-50,50,nsteps)\nexperiment1.water            = experiment1.water  + xxx\nexperiment1.cement           = experiment1.cement - 3.15 * xxx\nWCratio                      = experiment1.water \/ experiment1.cement\nexperiment1                  = experiment1\/beton.mean()\nexperiment1                  = experiment1[betonNorm.X]\n\np1 = plotExperiment(\"water\", \"csMPa\", experiment1, betonNorm, linear, 20, y_range=(0, 3))\np2 = plotExperiment(\"water\", \"csMPa\", experiment1, betonNorm, ransac, 20, y_range=(0, 3))\np3 = plotExperiment(\"water\", \"csMPa\", experiment1, betonNorm, tree, 20, y_range=(0, 3))\np4 = plotExperiment(\"water\", \"csMPa\", experiment1, betonNorm, forest, 20, y_range=(0, 3))\np5 = plotExperiment(\"water\", \"csMPa\", experiment1, betonNorm, svr, 20, y_range=(0, 3))\np6 = plotExperiment(\"water\", \"csMPa\", experiment1, betonNorm, neural, 20, y_range=(0, 3))\n\nshow(column(row(p1, p2, p3), row(p4, p5, p6)))\npanelTabs([p1,p2,p3,p4,p5,p6])","66fcd363":"nsteps                = 50\nexperiment2                  = beton.head(nsteps).copy()\nexperiment2.cement           = beton.cement.mean()\nexperiment2.slag             = beton.slag.mean()\nexperiment2.flyash           = beton.flyash.mean()\nexperiment2.water            = beton.water.mean()\nexperiment2.superplasticizer = beton.superplasticizer.mean()\nexperiment2.coarseaggregate  = beton.coarseaggregate.mean()\nexperiment2.fineaggregate    = beton.fineaggregate.mean()\nexperiment2.age              = np.linspace(1,100,nsteps)\nexperiment2                  = experiment2\/beton.mean()\nexperiment2                  = experiment2[betonNorm.X]\n\np1 = plotExperiment(\"age\", \"csMPa\", experiment2, betonNorm, linear, 20, y_range=(0, 3))\np2 = plotExperiment(\"age\", \"csMPa\", experiment2, betonNorm, ransac, 20, y_range=(0, 3))\np3 = plotExperiment(\"age\", \"csMPa\", experiment2, betonNorm, tree, 20, y_range=(0, 3))\np4 = plotExperiment(\"age\", \"csMPa\", experiment2, betonNorm, forest, 20, y_range=(0, 3))\np5 = plotExperiment(\"age\", \"csMPa\", experiment2, betonNorm, svr, 20, y_range=(0, 3))\np6 = plotExperiment(\"age\", \"csMPa\", experiment2, betonNorm, neural, 20, y_range=(0, 3))\n\nshow(column(row(p1, p2, p3), row(p4, p5, p6)))\npanelTabs([p1,p2,p3,p4,p5,p6])","f2ec770e":"## stuff","814c6034":"### age  effect  on strengths\nperforming  many  regressions with (50) different random training sets   \nshowing their predictions on  the  strength  versus  age  response   \nin red:  predictions based on the reference training set      \nin blue: predictions from random training sets   ","3c58dc5c":"## descriptive statistics","724c7843":"## References\nhttp:\/\/www.sciencedirect.com\/science\/article\/pii\/S0008884698001653   \nhttps:\/\/www.researchgate.net\/publication\/222447231_Modeling_of_Strength_of_High-Performance_Concrete_Using_Artificial_Neural_Networks_Cement_and_Concrete_research_2812_1797-1808\nhttps:\/\/archive.ics.uci.edu\/ml\/datasets\/Concrete+Compressive+Strength","ff6bf60b":"\n## What is illustrated here\nIn the last two sections you will see:   \n* how 6  different regressions  are sensitive to the random choice of the training set\n* with two examples:    \n    - the prediction of the **Concrete Strength**  as a function of its **age**    \n    - the prediction of the **Concrete Strength**  as a function of the **water\/cement ratio**\n    \nThe first sections deal with the description of the data, tha PCA, and the R\u00b2 of the correlations.\n\nThe 6 regressions tested here are: **linear,  ransac,  tree,  forest,  SVR,  neural** .","824fc35a":"# Revisiting a 20 year old Neural Network regression\n### Predictions sensitivity to the training set (1030 concrete samples, Yeh, 1998)","62b045f2":"### RANSAC Regressor (RANdom SAmple Consensus)\nhttp:\/\/scikit-learn.org\/stable\/modules\/linear_model.html#ransac-random-sample-consensus","a37ae231":"### the concrete data","ef198266":"### correlations","04b29765":"### neural_network MLPRegressor\nhttp:\/\/scikit-learn.org\/stable\/modules\/neural_networks_supervised.html#regression\nhttp:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.neural_network.MLPRegressor.html","577a13d1":"### W\/C effect  on 28d strengths\nperforming  many  regressions with (50) different random training sets   \nshowing their predictions on  the  strength  versus  water\/cement  ratio  response   \nin red:  predictions based on the reference training set    \nin blue: predictions from random training sets","7eca3dc4":"#### Inliers   and   Outliers \nObserve: outliers mainly have a larger mean age","d9f632cc":"### define training set, validation set, test set\nrandom splitting  implies that different runs may yield different results, allowing futher checks","558529d2":"## regressions, machine learning","c6bed0ac":"### Support Vector Machine Regression\nhttp:\/\/scikit-learn.org\/stable\/modules\/svm.html#svm-regression","b18e5a0d":"### RandomForestRegressor\nhttp:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.RandomForestRegressor.html","fd7d5656":"### PCA\nhttp:\/\/scikit-learn.org\/stable\/auto_examples\/decomposition\/plot_pca_vs_lda.html#sphx-glr-auto-examples-decomposition-plot-pca-vs-lda-py","1b0a9570":"#### Regression","196bdbc8":"### DecisionTreeRegressor\nhttp:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.tree.DecisionTreeRegressor.html","46146629":"### LinearRegression\nhttp:\/\/scikit-learn.org\/stable\/modules\/linear_model.html#ordinary-least-squares","cd0b20b2":"## Context\nThe compressive strength of concrete depends on its  age  and its composition :   \n\n* cement,\n* water,\t\n* slag,\n* flyash,\n* superplasticizer,\n* coarse aggregate,\t\n* fine aggregate\t  \n\nA correlation was performed by Yeh (1998) using neural networks.  ","66d14977":"### normalised data and variables"}}