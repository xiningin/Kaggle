{"cell_type":{"452c5221":"code","d955a354":"code","f7677f60":"code","25e7261b":"code","7e05826d":"code","fef16fe2":"code","f2d9e49d":"code","7bd0793f":"code","c9f527e0":"code","332ac766":"code","ddbfe32c":"code","bb5b2e8f":"code","d8248353":"code","28d7da1a":"code","68073bdb":"code","2311c322":"code","44c4cc79":"code","b5a1c1c2":"code","b77f8586":"code","d7c70a2c":"code","5e74eb08":"code","caa3c02d":"code","ca6ea2fe":"code","994c3510":"code","9bac3030":"code","cd2428a5":"code","2c843790":"code","d58fa820":"code","c7fe0488":"code","3e68df47":"code","ed3e57e8":"code","36239ca9":"code","5c05dcdd":"code","46222899":"code","92e6d5cc":"code","46350194":"code","a1fdcc32":"code","750fe693":"code","b0712211":"code","c319608e":"code","aa4bab78":"code","3be095d9":"code","b60228dd":"code","8a71e4a2":"code","a558255d":"code","7b936be5":"code","42a9c5d4":"code","2603bd8a":"code","8eeb9fe4":"code","f14caba3":"code","bd2cc813":"code","50925c6d":"code","1af1cf15":"code","62ffc033":"code","5c5f9113":"code","80d399b6":"code","910581a9":"code","bfc286d8":"code","91522023":"code","17f73851":"code","7cf40afd":"code","181f36d0":"code","2c165ea2":"code","8491177c":"code","f06fed9f":"code","c69950e5":"code","e13c15d9":"code","77320690":"code","47cee0a2":"code","4faa8f93":"code","2c383bff":"code","1ea1d7ed":"code","30d307a8":"code","ed935156":"code","bb3434cb":"code","65d77748":"code","169b9fc2":"markdown","dffd8d41":"markdown","89f8e379":"markdown","1abbe0a9":"markdown","b2256458":"markdown","27a955b6":"markdown","8e53eb17":"markdown","07728e91":"markdown","9689a401":"markdown","52d2a65a":"markdown","f17ba999":"markdown","bb5a5b93":"markdown","723fb4b3":"markdown","b5ec30e0":"markdown","7a109c5c":"markdown","ac606566":"markdown"},"source":{"452c5221":"import pandas as pd\nimport numpy as np\n%matplotlib inline\nimport matplotlib.pyplot as plt","d955a354":"x = np.linspace(0,50,501)\ny = np.sin(x)","f7677f60":"x","25e7261b":"y","7e05826d":"plt.plot(x,y)","fef16fe2":"df = pd.DataFrame(data=y,index=x,columns=['Sine'])","f2d9e49d":"df","7bd0793f":"len(df)","c9f527e0":"test_percent = 0.1","332ac766":"len(df)*test_percent","ddbfe32c":"test_point = np.round(len(df)*test_percent)","bb5b2e8f":"test_ind = int(len(df) - test_point)","d8248353":"test_ind","28d7da1a":"train = df.iloc[:test_ind]\ntest = df.iloc[test_ind:]","68073bdb":"train","2311c322":"test","44c4cc79":"from sklearn.preprocessing import MinMaxScaler","b5a1c1c2":"scaler = MinMaxScaler()","b77f8586":"# IGNORE WARNING ITS JUST CONVERTING TO FLOATS\n# WE ONLY FIT TO TRAININ DATA, OTHERWISE WE ARE CHEATING ASSUMING INFO ABOUT TEST SET\nscaler.fit(train)","d7c70a2c":"scaled_train = scaler.transform(train)\nscaled_test = scaler.transform(test)","5e74eb08":"from tensorflow.keras.preprocessing.sequence import TimeseriesGenerator","caa3c02d":"# scaled_train","ca6ea2fe":"# define generator\nlength = 2 # Length of the output sequences (in number of timesteps)\nbatch_size = 1 #Number of timeseries samples in each batch\ngenerator = TimeseriesGenerator(scaled_train, scaled_train, length=length, batch_size=batch_size)","994c3510":"len(scaled_train)","9bac3030":"len(generator) # n_input = 2","cd2428a5":"# scaled_train","2c843790":"# What does the first batch look like?\nX,y = generator[0]","d58fa820":"print(f'Given the Array: \\n{X.flatten()}')\nprint(f'Predict this y: \\n {y}')","c7fe0488":"# Let's redefine to get 10 steps back and then predict the next step out\nlength = 10 # Length of the output sequences (in number of timesteps)\ngenerator = TimeseriesGenerator(scaled_train, scaled_train, length=length, batch_size=1)","3e68df47":"# What does the first batch look like?\nX,y = generator[0]","ed3e57e8":"print(f'Given the Array: \\n{X.flatten()}')\nprint(f'Predict this y: \\n {y}')","36239ca9":"length = 50 # Length of the output sequences (in number of timesteps)\ngenerator = TimeseriesGenerator(scaled_train, scaled_train, length=length, batch_size=1)","5c05dcdd":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense,LSTM,SimpleRNN","46222899":"# We're only using one feature in our time series\nn_features = 1","92e6d5cc":"# define model\nmodel = Sequential()\n\n# Simple RNN layer\nmodel.add(SimpleRNN(50,input_shape=(length, n_features)))\n\n# Final Prediction\nmodel.add(Dense(1))\n\nmodel.compile(optimizer='adam', loss='mse')","46350194":"model.summary()","a1fdcc32":"# fit model\nmodel.fit_generator(generator,epochs=5)","750fe693":"model.history.history.keys()","b0712211":"losses = pd.DataFrame(model.history.history)\nlosses.plot()","c319608e":"first_eval_batch = scaled_train[-length:]","aa4bab78":"first_eval_batch","3be095d9":"first_eval_batch = first_eval_batch.reshape((1, length, n_features))","b60228dd":"model.predict(first_eval_batch)","8a71e4a2":"scaled_test[0]","a558255d":"test_predictions = []\n\nfirst_eval_batch = scaled_train[-length:]\ncurrent_batch = first_eval_batch.reshape((1, length, n_features))","7b936be5":"current_batch.shape","42a9c5d4":"current_batch","2603bd8a":"np.append(current_batch[:,1:,:],[[[99]]],axis=1)","8eeb9fe4":"test_predictions = []\n\nfirst_eval_batch = scaled_train[-length:]\ncurrent_batch = first_eval_batch.reshape((1, length, n_features))\n\nfor i in range(len(test)):\n    \n    # get prediction 1 time stamp ahead ([0] is for grabbing just the number instead of [array])\n    current_pred = model.predict(current_batch)[0]\n    \n    # store prediction\n    test_predictions.append(current_pred) \n    \n    # update batch to now include prediction and drop first value\n    current_batch = np.append(current_batch[:,1:,:],[[current_pred]],axis=1)","f14caba3":"test_predictions","bd2cc813":"scaled_test","50925c6d":"true_predictions = scaler.inverse_transform(test_predictions)","1af1cf15":"true_predictions","62ffc033":"test","5c5f9113":"# IGNORE WARNINGS\ntest['Predictions'] = true_predictions","80d399b6":"test","910581a9":"test.plot(figsize=(12,8))","bfc286d8":"from tensorflow.keras.callbacks import EarlyStopping","91522023":"early_stop = EarlyStopping(monitor='val_loss',patience=2)","17f73851":"length = 49\ngenerator = TimeseriesGenerator(scaled_train,scaled_train,\n                               length=length,batch_size=1)\n\n\nvalidation_generator = TimeseriesGenerator(scaled_test,scaled_test,\n                                          length=length,batch_size=1)","7cf40afd":"# define model\nmodel = Sequential()\n\n# Simple RNN layer\nmodel.add(LSTM(50,input_shape=(length, n_features)))\n\n# Final Prediction\nmodel.add(Dense(1))\n\nmodel.compile(optimizer='adam', loss='mse')","181f36d0":"model.fit_generator(generator,epochs=20,\n                   validation_data=validation_generator,\n                   callbacks=[early_stop])","2c165ea2":"test_predictions = []\n\nfirst_eval_batch = scaled_train[-length:]\ncurrent_batch = first_eval_batch.reshape((1, length, n_features))\n\nfor i in range(len(test)):\n    \n    # get prediction 1 time stamp ahead ([0] is for grabbing just the number instead of [array])\n    current_pred = model.predict(current_batch)[0]\n    \n    # store prediction\n    test_predictions.append(current_pred) \n    \n    # update batch to now include prediction and drop first value\n    current_batch = np.append(current_batch[:,1:,:],[[current_pred]],axis=1)","8491177c":"# IGNORE WARNINGS\ntrue_predictions = scaler.inverse_transform(test_predictions)\ntest['LSTM Predictions'] = true_predictions\ntest.plot(figsize=(12,8))","f06fed9f":"full_scaler = MinMaxScaler()\nscaled_full_data = full_scaler.fit_transform(df)","c69950e5":"length = 50 # Length of the output sequences (in number of timesteps)\ngenerator = TimeseriesGenerator(scaled_full_data, scaled_full_data, length=length, batch_size=1)","e13c15d9":"model = Sequential()\nmodel.add(LSTM(50, input_shape=(length, n_features)))\nmodel.add(Dense(1))\nmodel.compile(optimizer='adam', loss='mse')\nmodel.fit_generator(generator,epochs=6) #6 because it stopped early last time at 6","77320690":"forecast = []\n\nfirst_eval_batch = scaled_full_data[-length:]\ncurrent_batch = first_eval_batch.reshape((1, length, n_features))\n\nfor i in range(len(test)):\n    \n    # get prediction 1 time stamp ahead ([0] is for grabbing just the number instead of [array])\n    current_pred = model.predict(current_batch)[0]\n    \n    # store prediction\n    forecast.append(current_pred) \n    \n    # update batch to now include prediction and drop first value\n    current_batch = np.append(current_batch[:,1:,:],[[current_pred]],axis=1)","47cee0a2":"forecast = scaler.inverse_transform(forecast)","4faa8f93":"# forecast","2c383bff":"df","1ea1d7ed":"len(forecast)","30d307a8":"50*0.1","ed935156":"forecast_index = np.arange(50.1,55.1,step=0.1)","bb3434cb":"len(forecast_index)","65d77748":"plt.plot(df.index,df['Sine'])\nplt.plot(forecast_index,forecast)","169b9fc2":"# RNN and LSTM Example on Sine Wave","dffd8d41":"Let's turn this into a DataFrame","89f8e379":"**NOTE: PAY CLOSE ATTENTION HERE TO WHAT IS BEING OUTPUTED AND IN WHAT DIMENSIONS. ADD YOUR OWN PRINT() STATEMENTS TO SEE WHAT IS TRULY GOING ON!!**","1abbe0a9":"### Create the Model","b2256458":"## Scale Data","27a955b6":"## Evaluate on Test Data","8e53eb17":"## Train Test Split\n\nNote! This is very different from our usual test\/train split methodology!","07728e91":"# Please Upvote my kernel if you like it.","9689a401":"# Forecasting\n\nForecast into unknown range. We should first utilize all our data, since we are now forecasting!","52d2a65a":"## Adding in Early Stopping and Validation Generator","f17ba999":"# Time Series Generator\n\nThis class takes in a sequence of data-points gathered at\nequal intervals, along with time series parameters such as\nstride, length of history, etc., to produce batches for\ntraining\/validation.\n\n#### Arguments\n    data: Indexable generator (such as list or Numpy array)\n        containing consecutive data points (timesteps).\n        The data should be at 2D, and axis 0 is expected\n        to be the time dimension.\n    targets: Targets corresponding to timesteps in `data`.\n        It should have same length as `data`.\n    length: Length of the output sequences (in number of timesteps).\n    sampling_rate: Period between successive individual timesteps\n        within sequences. For rate `r`, timesteps\n        `data[i]`, `data[i-r]`, ... `data[i - length]`\n        are used for create a sample sequence.\n    stride: Period between successive output sequences.\n        For stride `s`, consecutive output samples would\n        be centered around `data[i]`, `data[i+s]`, `data[i+2*s]`, etc.\n    start_index: Data points earlier than `start_index` will not be used\n        in the output sequences. This is useful to reserve part of the\n        data for test or validation.\n    end_index: Data points later than `end_index` will not be used\n        in the output sequences. This is useful to reserve part of the\n        data for test or validation.\n    shuffle: Whether to shuffle output samples,\n        or instead draw them in chronological order.\n    reverse: Boolean: if `true`, timesteps in each output sample will be\n        in reverse chronological order.\n    batch_size: Number of timeseries samples in each batch\n        (except maybe the last one).","bb5a5b93":"## Data\n\nLet's use Numpy to create a simple sine wave.","723fb4b3":"Now you will be able to edit the length so that it makes sense for your time series!","b5ec30e0":"# LSTMS","7a109c5c":"Now let's put this logic in a for loop to predict into the future for the entire test range.\n\n----","ac606566":"## Inverse Transformations and Compare"}}