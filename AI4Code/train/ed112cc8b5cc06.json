{"cell_type":{"b8175cae":"code","0b0d5a7f":"code","9f137bb8":"code","b5c183b4":"code","4af136e4":"code","9daff060":"code","159300fc":"code","085fce20":"code","07118a73":"code","e2cfd922":"code","080cd773":"code","b96b2360":"code","9984b34a":"code","58149a71":"code","e015e94f":"code","93edd7cf":"code","7fd99e7d":"code","f1acf925":"code","41e5f631":"code","10285089":"code","5dd70ddc":"code","3329385b":"code","cf6bfec2":"code","0592c2b8":"code","d77b450e":"code","625ed33e":"code","2122d3df":"code","ccd8d0c7":"markdown","d9339111":"markdown","49afaffc":"markdown","94eefa6f":"markdown","653d3824":"markdown","c48f9322":"markdown","860fd82b":"markdown","c1f12f58":"markdown","f22640e7":"markdown","40b05af0":"markdown","0793e20a":"markdown","a0cfd346":"markdown","b9fb34d6":"markdown","7f1e335e":"markdown","5359c5ee":"markdown","09b2b004":"markdown","15ee319d":"markdown","6133b94a":"markdown","58da536d":"markdown","84aab496":"markdown","056f61c3":"markdown","4ef0708d":"markdown","87398c99":"markdown","623c7e2b":"markdown","79a60ac3":"markdown","8f124132":"markdown","1ad94021":"markdown","25ccce0c":"markdown","c8c234c6":"markdown","cce16e1b":"markdown","8a19ac06":"markdown","9d054233":"markdown","b7ee30fe":"markdown","3afe3972":"markdown","ea5c7d3e":"markdown","45cecfb4":"markdown"},"source":{"b8175cae":"#basic libraries\nimport numpy as np\nimport pandas as pd\n\n#seed the project\nnp.random.seed(64)\n\n#warning handle\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n\nprint(\"Set up completed\")","0b0d5a7f":"# Import train and test data\ndata = pd.read_csv('..\/input\/creditcardfraud\/creditcard.csv')","9f137bb8":"data.head()","b5c183b4":"print (f\"DF has {data.shape[0]} rows and {data.shape[1]} columns\")","4af136e4":"# Different data types in the dataset\ndata.dtypes","9daff060":"# Plot graphic of missing values\nimport missingno\nmissingno.matrix(data, figsize = (30,10));","159300fc":"data.isnull().sum().sort_values(ascending = False).head(5)","085fce20":"# Visualization \nimport matplotlib.pyplot as plt\nimport missingno\nimport seaborn as sns\nplt.style.use('seaborn-whitegrid')\nsns.set_palette(\"Set2\")","07118a73":"# Visualize the count of survivors\nsns.countplot('Class', data=data);","e2cfd922":"from collections import Counter\nCounter(data.Class)","080cd773":"print(\"Fraud to NonFraud Ratio of {:.3f}%\".format(492\/284315*100))","b96b2360":"sns.kdeplot(data.Amount[data.Class == 0], label = 'Fraud', shade=True)\nsns.kdeplot(data.Amount[data.Class == 1], label = 'NonFraud', shade=True)\nplt.xlabel('Amount');","9984b34a":"sns.kdeplot(data.Time[data.Class == 0], label = 'Fraud', shade=True)\nsns.kdeplot(data.Time[data.Class == 1], label = 'NonFraud', shade=True)\nplt.xlabel('Time');","58149a71":"data_corr = data.corr()","e015e94f":"plt.figure(figsize=(15,10))\nsns.heatmap(data_corr) # Displaying the Heatmap\n\nplt.title('Heatmap correlation')\nplt.show()","93edd7cf":"columns = data.columns.tolist()\n# Filter the columns to remove data we do not want \ncolumns = [c for c in columns if c not in [\"Class\"]]\n# Store the variable we are predicting \ntarget = \"Class\"\n# Define a random state \nstate = np.random.RandomState(64)\nX = data[columns]\nY = data[target]\n# Print the shapes of X & Y\nprint(X.shape)\nprint(Y.shape)","7fd99e7d":"from imblearn.under_sampling import NearMiss","f1acf925":"# Implementing Undersampling for Handling Imbalanced \nnm = NearMiss()\nX_res,y_res=nm.fit_sample(X,Y)","41e5f631":"X_res.shape,y_res.shape","10285089":"# let's double check our results\nfrom collections import Counter\nprint('Original dataset shape {}'.format(Counter(Y)))\nprint('Resampled dataset shape {}'.format(Counter(y_res)))","5dd70ddc":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X_res, y_res, test_size= 0.25, random_state= 0)\nprint(\"The split of the under_sampled data is as follows\")\nprint(\"X_train: \", len(X_train))\nprint(\"X_test: \", len(X_test))\nprint(\"y_train: \", len(y_train))\nprint(\"y_test: \", len(y_test))","3329385b":"# Scikit-learn library: For SVM\nfrom sklearn import preprocessing\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn import svm","cf6bfec2":"classifier = svm.SVC(kernel='linear') ","0592c2b8":"classifier.fit(X_res, y_res) # Then we train our model, with our balanced data train.","d77b450e":"#Predict the class using X_test\ny_pred = classifier.predict(X_test)","625ed33e":"# define the confusion matrix 1 which uses the undersampled dataset\nfrom sklearn.metrics import confusion_matrix\ncm1 = confusion_matrix(y_test, y_pred)","2122d3df":"from mlxtend.plotting import plot_confusion_matrix\ndef confusion_matrix_1(CM):\n    fig, ax = plot_confusion_matrix(conf_mat=CM)\n    plt.title(\"The Confusion Matrix 1 of Undersampled dataset\")\n    plt.ylabel(\"Actual\")\n    plt.xlabel(\"Predicted\")\n    plt.show()\n\n    print(\"The accuracy is \"+str((CM[1,1]+CM[0,0])\/(CM[0,0] + CM[0,1]+CM[1,0] + CM[1,1])*100) + \" %\")\n    print(\"The recall from the confusion matrix is \"+ str(CM[1,1]\/(CM[1,0] + CM[1,1])*100) +\" %\")\nconfusion_matrix_1(cm1)","ccd8d0c7":"Look at the results, we can see that our data is not imbalaced anymore! What we just did is to make both our target feature groups the same size.","d9339111":"### Splitting the undersampled data into training and test set","49afaffc":"### Prediction the class by fitting X_test to our classifier","94eefa6f":"### Dealing with Imbalanced Data\n#### Oversampling\n\nOne way to do oversampling is to replicate the under-represented class tuples until we attain a correct proportion between the class\n\nHowever our dataset is very large, and as we haven't infinite time nor the patience, we are going to run the classifier with the undersampled training data (for those using the undersampling principle if results are really bad just rerun the training dataset definition).\n\n#### Undersampling\n\nThis is the most useful method for this problem. If you want some more info, check out this excellent video from Krish Naik: https:\/\/www.youtube.com\/watch?v=YMPMZmlH5Bo. Just have in mind that we'll be using the collection NearMiss to make it more automatic and have less headaches.","653d3824":"Awesome, no missing values. Let's continue","c48f9322":"<a id=\"t2.\"><\/a>\n## 3. Exploration Data Analysis","860fd82b":"### Load the Data","c1f12f58":"We notice that the feature time doesn't seem to have an impact in the frequency of frauds.","f22640e7":"We have good accuracy and recall score so our model is giving good results when using the undersampled data. The real test will be when we apply the model to our whole dataset(skewed).","40b05af0":"<a id=\"t4.\"><\/a>\n## 4. Feature Engineering","0793e20a":"We can see that none of the features have a title or description, this was done to preserve privacy of the credit cards owners. We'll have to work around that.","a0cfd346":"### Missing Values\n\nMost ML algorithms have a hard time working with missing values, so let's see what our situation is. ","b9fb34d6":"### Create independent and Dependent Features","7f1e335e":"We can see that all our features are in fact numerical, floats, to be exact, great.","5359c5ee":"TOC:\n1. [Problem Framing](#t1.)\n2. [Assess Data Quality & Missing Values](#t2.)\n3. [Exploratory Data Analysis](#t3.)\n4. [Feature Engineering](#t4.)\n5. [Model Selection](#t5.)\n6. [Tessting the model](#t6.)\n7. [Presenting the Solution](#t7.)","09b2b004":"Just from this we can see that there is a very low occurance of fraud in comparison with non fraud which could cause some issues should we dive into any kind of predictions. \n\nThis dataset is unbalanced which means using the data as it is might result in unwanted behaviour from a supervised classifier. To make it easy to understand if a classifier were to train with this data set trying to achieve the best accuracy possible it would most likely label every transaction as a non-fraud.\n\nTo answer this problem we could use the oversampling principle or the undersampling principle The undersampling principle should be used only if we can be sure that the selected few tuples (in this case non-fraud) are representative of the whole non-fraud transactions of the dataset.","15ee319d":"### Data Types\n\nWe can see our dataset consist on different types of data.","6133b94a":"### Preparing the tools","58da536d":"### The Data\n\nThe datasets contains transactions made by credit cards in September 2013 by european cardholders. It contains only numerical input variables which are the result of a PCA transformation.\n\nThis dataset presents transactions that occurred in two days, where we have 492 frauds out of 284,807 transactions. The dataset is highly unbalanced, the positive class (frauds) account for 0.172% of all transactions.\n\nThe dataset has been collected and analysed during a research collaboration of Worldline and the Machine Learning Group (http:\/\/mlg.ulb.ac.be) of ULB (Universit\u00e9 Libre de Bruxelles) on big data mining and fraud detection.\n\nMore details on current and past projects on related topics are available on https:\/\/www.researchgate.net\/project\/Fraud-detection-5 and the page of the DefeatFraud project","84aab496":"As we can notice, most of the features are not correlated with each other. \n\nWhat can generally be done on a massive dataset is a dimension reduction. By picking th emost important dimensions, there is a possiblity of explaining most of the problem, thus gaining a considerable amount of time while preventing the accuracy to drop too much.\n\nHowever in this case given the fact that a PCA was previously performed, if the dimension reduction is effective then the PCA wasn't computed in the most effective way. Another way to put it is that no dimension reduction should be computed on a dataset on which a PCA was computed correctly.","056f61c3":"<a id=\"t2.\"><\/a>\n## 2. Data Quality & Missing Values Assessment","4ef0708d":"Welcome to my notebook! Let's see what SVM classification is all about. ","87398c99":"### Problem Definition\n\nIt is important that credit card companies are able to recognize fraudulent credit card transactions so that customers are not charged for items that they did not purchase.\n\nThis is a standard binary classification task. The datasets contains transactions made by credit cards in September 2013 by european cardholders. It contains only numerical input variables which are the result of a PCA transformation.\n\nThis dataset presents transactions that occurred in two days, where we have 492 frauds out of 284,807 transactions. The dataset is highly unbalanced, the positive class (frauds) account for 0.172% of all transactions.","623c7e2b":"<a id=\"t5.\"><\/a>\n## 5. Model Fitting and Testing","79a60ac3":"# Credit Card Fraud - SVM Classification","8f124132":"<a id=\"t7.\"><\/a>\n## 7. Presenting the Solution","1ad94021":"### Feature Exploration: Amount\n\nDescription: Transaction Amount","25ccce0c":"Class is a binary target with 492 instances of known fraud and 284,315 instances of non-fraud. Thats a ratio of:","c8c234c6":"So now, we'll use a SVM model classifier, with the scikit-learn library.","cce16e1b":"<a id=\"t1.\"><\/a>\n## 1. Problem Framing","8a19ac06":"### Correlation of features","9d054233":"### The Confusion Matrix\n\nThe Confusion matrix can be used to calculate the accuracy, recall, and precision from our model. Let us fit our confusion matrix class with y_pred(predicted results) and y_test( actual results)","b7ee30fe":"<a id=\"t6.\"><\/a>\n## 6. Testing the model","3afe3972":"Looks like there a lot more instances of small fraud amounts than really large ones.","ea5c7d3e":"### Feature Exploration: Time\n\nDescription: Number of seconds elapsed between this transaction and the first transaction in the dataset","45cecfb4":"### Target Feature: Class\n\nDescription: 1 for fraudulent transactions, 0 otherwise"}}