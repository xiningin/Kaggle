{"cell_type":{"552d39c4":"code","b230b76c":"code","acdb5334":"code","1381caf4":"code","5e0a9c01":"code","12d27958":"code","2f5e28a2":"code","5371d78c":"code","cdf6c8ec":"code","ade14c2f":"code","377f6907":"code","821d6764":"code","e1e0c859":"code","d5c56dce":"code","e2e3c4a0":"code","5fd194d8":"code","bfd9bf89":"code","23092f1e":"code","a7a2f912":"code","b24d1414":"code","cd03cea1":"code","0aee6605":"code","738c3c00":"code","db4bf14b":"code","ecd22df8":"code","6927096f":"code","fa61db11":"code","8cb291ab":"markdown","4841eda8":"markdown","ad284a8a":"markdown","bd8852ce":"markdown","a0731919":"markdown","ca3e1750":"markdown","71de7e4e":"markdown","d58deac9":"markdown","488be0d3":"markdown","56eeaf78":"markdown","79b14070":"markdown","0ff74332":"markdown","f5072245":"markdown","f1a9b726":"markdown","f6a2f113":"markdown","0d3d995c":"markdown","3463fd24":"markdown","9841e7c7":"markdown","ff5b938f":"markdown","d8d84edc":"markdown","04e05eef":"markdown","0433e417":"markdown","64ee4c80":"markdown","cfe52a10":"markdown","6ea5a408":"markdown","d92ccc0d":"markdown","c7c286e1":"markdown","95b47919":"markdown","6ba50968":"markdown","ab6e8dd2":"markdown","a8cb5fbe":"markdown","21c88813":"markdown","80ca2dc6":"markdown","7ca4f05b":"markdown","8cca41a6":"markdown","e4cee79a":"markdown","44ade942":"markdown","b9cf11b0":"markdown","683ee0ba":"markdown","3a8eda2a":"markdown","da9f7e5b":"markdown","24a5076c":"markdown","0fb4fa53":"markdown","d59b0389":"markdown","5eb1c0b5":"markdown","c12ab39e":"markdown","32a92b82":"markdown"},"source":{"552d39c4":"import numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nfrom sklearn.model_selection import train_test_split \nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix\n\n# for LIME\/SHAP\/permutation importance\nimport lime\nimport lime.lime_tabular\nimport shap \nimport eli5\nfrom eli5.sklearn import PermutationImportance\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","b230b76c":"df = pd.read_csv(\"..\/input\/biomechanical-features-of-orthopedic-patients\/column_2C_weka.csv\")\n# df2 = pd.read_csv(\"..\/input\/biomechanical-features-of-orthopedic-patients\/column_3C_weka.csv\")","acdb5334":"df.info()\n#df.head()","1381caf4":"df.describe()","5e0a9c01":"num_classes = len(df[\"class\"].unique())\nlabels = []\n\nprint(f'There are {num_classes} classes. Their distribution is:  ')\nfor i in range(num_classes):\n    # print(df[\"class\"].unique()[i])\n    labels.append(df[\"class\"].unique()[i])\n    \ndata = df['class'].value_counts()\ncolors = sns.color_palette('pastel')[5:7]\nexplode = (0.1, 0.0)\n\n# create pie chart\nplt.pie(df['class'].value_counts(), labels=labels, explode=explode, colors = colors, autopct='%.0f%%');","12d27958":"features = df.columns\n\nsns.set(style=\"darkgrid\")\nfig,ax = plt.subplots(2,3,figsize=(15,8))\n\n# enumerating 2D axes is a nice trick to plot subplots\nfor idx, item in enumerate(ax.reshape(-1)):  \n    sns.swarmplot(\"class\", features[idx], data=df, ax=item)","2f5e28a2":"sns.pairplot(df , hue=\"class\");","5371d78c":"# assign numerical values to the output classes\nclass_map = {'Normal':0, 'Abnormal':1}\ndf['class'] = df['class'].map(class_map)\n\n# isolate X and y from the dataset\ny = df['class'].values\nX = df.drop(['class'], axis=1)\n\n# stratefied train and test data set split\nX_train,X_test,y_train,y_test = train_test_split(X,y,stratify=y, test_size = 0.2,random_state = 2021)","cdf6c8ec":"# visualize data distribution after train test split\ndata_sets = [y_train, y_test]\nfig, axs = plt.subplots(1, 2, figsize=(10,5))\n\nfor i, data_set in enumerate(data_sets):\n    sns.countplot(data_set,  palette=\"pastel\", ax=axs[i])\n    axs[i].bar_label(axs[i].containers[0])\n    #axs[i].set_title(data_set)\nfig.suptitle('Train (left) and test (right) set label distribution')\nplt.show()","ade14c2f":"rfc = RandomForestClassifier(n_estimators=100, random_state=2021)\nrfc.fit(X_train,y_train)\n\nprint(f'baseline train score: {rfc.score(X_train,y_train): .3f}')\nprint(f'baseline test score: {rfc.score(X_test,y_test): .3f}')","377f6907":"rf = RandomForestClassifier(oob_score=True, random_state=2021, n_jobs=-1)\n\nparam_grid = { \"criterion\" : [\"gini\", \"entropy\"], \n              \"max_features\" : ['auto', 'log2'],\n              \"min_samples_leaf\" : [1, 5, 10], \n              \"min_samples_split\" : [2, 4, 10, 12], \n              \"n_estimators\": [50, 100, 400, 700]}\n\ngs = GridSearchCV(estimator=rf, param_grid=param_grid, scoring='accuracy', cv=3, n_jobs=-1)\ngs = gs.fit(X_train, y_train)\n\nprint(f'The best cv score is: {gs.best_score_: .3f}')\nprint(f'The best parameters are: \\n {gs.best_params_}')\n\n# df_cv_results = pd.DataFrame(gs.cv_results_)","821d6764":"rfc_2 = RandomForestClassifier(**gs.best_params_, random_state=2021)\n# rfc_2 = RandomForestClassifier(n_estimators=700, max_features='auto', min_samples_leaf=1, min_samples_split=12,  criterion='entropy', random_state=2021)\n\nrfc_2.fit(X_train, y_train)","e1e0c859":"pred = rfc_2.predict(X_test)\nprint(f'Accuracy for Random Forest on CV data: {accuracy_score(y_test,pred): .3f} \\n')\nprint(classification_report(y_test, pred))","d5c56dce":"cm = confusion_matrix(y_test, pred)\nconf_matrix=pd.DataFrame(data=cm,columns=['predicted:0','predicted:1'],index=['observed:0','observed:1'])\nplt.figure(figsize = (8,5))\nsns.heatmap(conf_matrix, annot=True,fmt='d',cmap='YlOrBr');","e2e3c4a0":"# getting the row index for the errors\nerrors = np.where(y_test != pred)\nprint(f'The row index for the errors: \\n {errors[0]}')","5fd194d8":"feature_imp = pd.Series(rfc_2.feature_importances_, index = features[0:6]).sort_values(ascending = False)\n\nsns.barplot(x=feature_imp, y=feature_imp.index, palette='pastel')\nplt.xlabel('Feature Importance Score')\nplt.ylabel('Features')\nplt.title(\"Visualizing Important Features\")\nplt.legend()\nplt.show()","bfd9bf89":"perm = PermutationImportance(rfc_2, random_state=2021).fit(X_test, y_test)\neli5.show_weights(perm, feature_names=X_train.columns.to_list())","23092f1e":"predict_fn_rf = lambda x: rfc_2.predict_proba(x).astype(float)\n\nlime_explainer = lime.lime_tabular.LimeTabularExplainer(X_train.values, \n                                                   mode=\"classification\",\n                                                   feature_names = X_train.columns,\n                                                   class_names = ['Normal', 'Abnormal'], # note class ordering 0,1\n                                                   kernel_width=5)\n\ndef lime_explain_instance(idx):\n    row = X_test.index[idx]\n    instance = X_test.loc[[row]].values[0]\n    exp = lime_explainer.explain_instance(instance, predict_fn_rf, num_features=len(X_train.columns))\n    exp.show_in_notebook(show_all=False)\n    \n    fig = exp.as_pyplot_figure()","a7a2f912":"idx = 50\nprint(f'For a correctly labeled instance where: ')\nprint(f'true label = {y_test[idx]}; predicted label = {pred[idx]}\\n')\nlime_explain_instance(idx)","b24d1414":"idx = 21\nprint(f'For a correctly labeled instance where: ')\nprint(f'true label = {y_test[idx]}; predicted label = {pred[idx]}\\n')\nlime_explain_instance(idx)","cd03cea1":"idx = errors[0][0]  \nprint(f'For an incorrectly labeled instance where: ')\nprint(f'true label = {y_test[idx]}; predicted label = {pred[idx]}\\n')\nlime_explain_instance(idx)","0aee6605":"shap_explainer = shap.TreeExplainer(rfc_2)\nshap.initjs()\n\ndef shap_explain_local(idx):\n    row = X_test.index[idx]\n    instance = X_test.loc[[row]]\n    shap_values = shap_explainer.shap_values(instance)\n    #shap.initjs()\n    fp = shap.force_plot(shap_explainer.expected_value[1], shap_values[1], instance)\n    return fp","738c3c00":"idx = 50\nprint(f'For a correctly labeled instance where: ')\nprint(f'true label = {y_test[idx]}; predicted label = {pred[idx]}\\n')\nshap_explain_local(idx)","db4bf14b":"idx = 21\nprint(f'For a correctly labeled instance where: ')\nprint(f'true label = {y_test[idx]}; predicted label = {pred[idx]}\\n')\nshap_explain_local(idx)","ecd22df8":"idx = errors[0][0]\nprint(f'For a correctly labeled instance where: ')\nprint(f'true label = {y_test[idx]}; predicted label = {pred[idx]}\\n')\nshap_explain_local(idx)","6927096f":"# calculate SHAP values for all of test data rather than a single instance\nshap_values = shap_explainer.shap_values(X_test)\n\n# for a variable importance\/bar plot\nclass_names = ['Normal', 'Abnormal']\nshap.summary_plot(shap_values, X_test, class_names = class_names)","fa61db11":"shap.summary_plot(shap_values[1], X_test)","8cb291ab":"## V. Discussion","4841eda8":"#### Local explainability  \nWe can visualize the contribution of each feature by means of a component chart called a force plot.","ad284a8a":"### Baseline model","bd8852ce":"#### Second data instance","a0731919":"### 1. Feature Importance\n\nThe feature importance, which is obtained as a by-product of the random forest algorithm, ranks each feature in order of its importance in the global model. Internally, for each individual tree, the algorithm computes the information gain each time a node is split on a feature. These values are averaged first for each tree and then across all the trees. The *mean decrease impurity* metric is a succint description of the process.  This is the approach implemented in `scikit-learn`.\n\nAlthough the feature importance does not involve additional computation, the disadvantage is that the results are biased towards high cardinality (numerous categories) and continuous numerical variables. In our case, however, all six features are similarly enumerated with continuous numerical variables. ","ca3e1750":"We preprocess the data for the `scikit-learn` implementation of the random forest algorithm.\nThese are some points to consider:\n\n**Feature scaling:**\nWe do not need to normalize or scale random forest data because we are only splitting on values and this partitioning does not depend on a distance formula.\n\n**Correlated data:**\nThe random forest algorithm uses bagging and feature sampling and so unlike say OLS, can handle correlated data to some extent.\n\n**Outliers:**\nThe random forest is robust to outliers.\n\n**Missing Values:**\nThis particular data set does not have missing values.\n\n**Imbalanced data:**\nThe dataset is imbalanced and this could bias the results. Since our focus is on interpretability, we will not address the imbalance in the data set.\n\n**Stratified train test split:**\nDue to the imbalance in the data set, we stratify the train-test split.\n\n**Assigning the target labels to numerical data:**\nWe convert the binary *Abnormal\/Normal* to 0\/1 because `scikit-learn` uses only numerical targets. (Note, this is a best practice since we do not need to explicitly do this conversion, the algorithm does this internally).","71de7e4e":"##### Third data instance","d58deac9":"The `f1-score` for the minority class is 0.65. This is much lower than 0.91 which is the `f1-score` for the majority class.","488be0d3":"We will look at four model interpretability\/explainability methods for the Random Forest classification model:\n\n1. Feature importance\n2. Permuation feature importance\n3. LIME\n4. SHAP ","56eeaf78":"By visual inspection, the abnormal and normal distributions are most different for the **degree_spondylolisthesis** and **pelvic_radius** features. ","79b14070":"Author: **Meena Mani**      \nProject: <span style=\"color:brown\">**Lower Back Pain: model interpretability with LIME, SHAP** <\/span>   \nDate: **November 2021**","0ff74332":"## II. Random Forest Classification Model","f5072245":"### 3. LIME (Local Interpretable Model-Agnostic Explanations)\n\nSimple models (e.g. a linear regression or shallow decision tree model) are interpretable; complex models on the other hand are in general less interpretable but have higher accuracy.    \n\nThe Random Forest model is a complex model. To make it and other complex models more explainable we can apply post hoc methods like LIME. The intuition behind LIME is to perturb the local area around a single data instance and apply a simple surrogate linear model to this neighborhood. This helps us interpret the outcome for an individual data point.\n\nWe will look at three data instances, two of which were predicted accurately by the model and one which was an error. ","f1a9b726":"The force plot above shows the breakdown where each feature either pushes the model output higher (pink) or lower (blue). The aggregate of these SHAP values is what moves an individual prediction from the base value (the average model output over the entire training dataset) to the resulting outcome. \n\nThe **degree_spondylolisthesis** feature is the biggest contributor to the final result. It lowers (blue) the final prediction from a base value of 0.6767. This is counter balanced by the second biggest contributor, the **pelvic_radius** feature. This increases (pink) the final prediction to give us a value of 0.51. Note, 0.51 is just above the classification threshold.\n\nThese results are  similar to the results from LIME (see above).","f6a2f113":"This is a variable importance plot that shows both the aggregate SHAP value for each predictor, as well as the  contribution of each variable to a particular class.  The variable at the top, **degree_spondylolisthesis**, has the highest predictive power. We also see that both the *Normal* and *Abnormal*  classes use the features to the same extent. If for instance one class used a feature exclusively, the classes would be easier to separate. ","0d3d995c":"Here the **degree_spondylolisthesis** is the most important feature followed by the  **pelvic_radius**. ","3463fd24":"### 2. Permutation feature importance\n\nThis approach directly measures feature importance by observing how a random permutation of each feature affects the out-of-bag performance. The intuition here is that randomly shuffling a feature column will while preserving the distribution of the variable, adversely affect the performance if the feature is important. For unimportant features, the performance will be relatively unaffected.   \nThis method is an alternative to the inbuilt random forest feature importance computation considered above. It gives a global interpretation that is not biased but is computationally more expensive.\n\nWe will use the `eli5` package to compute permutation importance.","9841e7c7":"### Grid search cross validation","ff5b938f":"### Data Preprocessing (for a Random Forest Classifier)","d8d84edc":"##### First data instance","04e05eef":"We use lumbar biomechanical features to classify the condition of the lumbar spine as normal or abnormal.\nWe use interpretable AI to understand the model and explainable AI to understand why specific data instances were classified as *Normal\/Abnormal* (Note: the terms interpretable and explainable are used variously; I use interpretable for global insights and explainable for local insights. For more on the topic see [this stackexchange discussion](https:\/\/datascience.stackexchange.com\/questions\/70164\/what-is-the-difference-between-explainable-and-interpretable-machine-learning)).   \nThe random forest classifier is a good vehicle for this analysis since in addition to model agnostic methods, many tree-based post hoc methods are available to explain outcomes. \n\n**Insights from EDA**  \nThe swarm plot in the pre-model EDA phase of this analysis showed that **degree_spondylolisthesis** had the largest class separation and this indicated that it would be the most important feature in the classification.\nThe **degree_spondylolisthesis** is a lumbar biomechanical feature that measures the degree to which a verterbra has slipped out of place; a displaced vertebra can pinch nerves and cause pain. \n\n**Global model-level interpretability**    \nFor global interpretability, we looked at feature importance, permutation feature importance, and the SHAP global summary charts. While the rank ordering with all three were similar, SHAP is the most accurate.\nThe **degree_spondylolisthesis** was the most significant feature followed by **pelvic_radius**.\nThe **pelvic_incidence** and **lumbar_lordosis_angle** were the least important features.  \n\n**Local instance-level explainability**  \nFor local explainability, we looked at LIME and SHAP. The feature most responsible for both correct and incorrect predictions was **degree_spondylolisthesis**. Since we are looking at individual outcomes, different features had different contributions. For instance **lumbar_lordosis_angle** was the third most important feature for the first data instance. There was a great deal of agreement between LIME and SHAP. Both gave similar results with similar rank ordering. This gives us a high level of confidence with the results.\n\n**Limitations**    \nAn important issue that we did not address was the need to balance the data which is skewed in a 2:1 ratio. This could potentially have affected the results.","0433e417":"#### First data instance \n","64ee4c80":"Once again the **degree_spondylolisthesis** feature is the biggest contributor. Here it lowers (blue) the final prediction from the base value of 0.6767. The **sacral_slope** along with **pelvic_radius** and **pelvic_tilt_numeric** are also responsible for lowering the final predication so that the subject is correctly classified as *Class 0*\/*Normal*.\n\nLIME gives similar results but the second, third and fourth features are ordered differently.","cfe52a10":"### Exploratory Data Analysis (EDA)","6ea5a408":"### 4. SHAP (SHapley Additive exPlanations) \n\nSHAP  is a game theoretic approach to explain the individual predictions of any machine learning model. The marginal contribution of each feature can be quantitatively assessed by computing the model prediction with and without each of the possible feature subsets containing that feature. Since the process (of computing all possible feature subsets) is computationally expensive, there exist approximate methods like Tree SHAP (which we will use here) which reduce the number of computations. ","d92ccc0d":"**Observations**    \nThere are 6 features (spine measurements).  \nThere are no columns with missing data.   \nAll the columns have continuous numerical data.","c7c286e1":"This is an imbalanced data set with an imbalance ratio is 2:1. This could bias the classifier. ","95b47919":"## III. Model Interpretability","6ba50968":"In this local instance, the **degree_spondylolisthesis** is the most important feature. It biases the classification towards *Normal*; this is counter balanced by the **pelvic_tilt_numeric** and **pelvic_radius** feature values. The net result is a classification that is borderline *Abnormal*.","ab6e8dd2":"#### Third data instance","a8cb5fbe":"This is a swarm plot so each dot in the visualization represents one prediction. If the actual feature value in the dataset is high, it is coded pink; if it is low it is coded blue. The x-axis represents the SHAP value, which is the impact on the model output. Note that this plot should be interpreted with respect to *Class 1*\/*Abnormal* results. \n\nTo interpret the plot for **degree_spondylolisthesis**, we see (for the *Abnormal* class) that high values (pink) give large positive predictions (SHAP values), low values (blue) give large negative predictions. Both these indicate that this feature is highly significant.\n\nTaking the **pelvic_redius** feature, we see that high values (pink) have negative predictions . This indicates that high values contribute to the opposite category i.e. *Class 0*\/*Normal*.","21c88813":"We will use these indices at a later stage to look at data points that were predicted incorrectly.","80ca2dc6":"By visual inspection, the **degree_spondylolisthesis** and **pelvic_radius** features are most uncorrelated. The **pelvic_incidence** feature is correlated with 3 out of 5 other features.\n\nThe random forest classifier is better at handling correlated data than OLS models.","7ca4f05b":"The subject is strongly classified as *Abnormal* when in fact they are *Normal*.\nThe **degree_spondylolisthesis** feature which appears to be the most relevant variable in the global model, is responsible for the false characterization in this case. A chart like this gives medical professionals insight to contributing factors for what would otherwise have been a black box result.","8cca41a6":"### Best model","e4cee79a":"### Model evaluation","44ade942":" The **degree_spondylolisthesis** feature is the biggest contributor; the subject is strongly characterized as *Abnormal* when in fact they are *Normal*. Seeing this component chart will allow medical professionals to understand the model result.","b9cf11b0":"Once again the **degree_spondylolisthesis** strongly influences the classification. The **pelvic_tilt_numeric**, **pelvic_radius** and other feature values all align in the same direction giving a result that is unambiguously *Normal*.","683ee0ba":"# Lower back pain:  Model interpretability with lumbar spine features\n\n**Keywords: LIME, SHAP, Feature Importance, Permutation Importance, Random Forest Classification**","3a8eda2a":"## I. Data","da9f7e5b":"#### Global Summary\n\nSHAP can also give a summary of an aggregate of data points, be it global data or a particular subgroup. The software presents the results either in a variable importance plot or a beeswarm plot format.","24a5076c":"The severity of lower back pain is not necessarily correlated to the degree of degeneration of nerves, bones, discs or tendons in the lumbar spine. In this exercise, we rely instead on spine measurements of the lower back (most likely obtained from medical images) to classify whether the condition of the spine is normal or abnormal. We use a random forest model for this.\n\nOur focus in this notebook will be to look at which features contribute most to the classification. The random forest algorithm computes an internal *variable importance*  measure that lists the important features of the model based on trends in the global dataset. An alternative computation for *variable importance* using permutation can also be done.\nIn addition to these two global methods, we will use LIME and SHAP for local interpretations to answer why individual patients (instances) were classified or misclassified as normal\/abnormal by the model.","0fb4fa53":"The important features are listed in descending order. We see that **degree_spondylolisthesis** is the most significant feature. The **pelvic_radius** is the second most important feature. This correlates with what we could see with the EDA charts.\n\nThe slightly negative values at the bottom indicate that those features are not important for the model. The value should be zero but by random chance the shuffled data gave a slightly better performance than the original feature ordering.","d59b0389":"##### Second data instance","5eb1c0b5":"##### Beeswarm plot","c12ab39e":"##### Variable importance plot","32a92b82":"We can see that the classification error is greater for the minority (*Normal* \/*Class 0*) class with 7\/20 data points incorrectly labeled. This indicates that we need to balance the data. "}}