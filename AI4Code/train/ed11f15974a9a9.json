{"cell_type":{"47394251":"code","b7dcb987":"code","cf082f03":"code","da4d616f":"code","73378076":"code","3c304434":"code","7d0a79e2":"code","77689885":"code","84f59702":"code","41edf00a":"code","b7636108":"code","686c06ad":"code","9b634985":"code","5e9af9b5":"code","afc0c5ee":"code","10308bad":"code","b36e25a6":"code","287dd556":"code","ab305eb2":"code","4ff7c265":"code","3e919b49":"code","bb8fbc1a":"code","0b788507":"code","07e0dbf1":"markdown","d760c11e":"markdown","1b2a4cb0":"markdown","a971e118":"markdown","692ea8af":"markdown"},"source":{"47394251":"import pandas as pd\nimport numpy as np\nfrom gensim.models import Word2Vec\nfrom sklearn.cluster import KMeans\nfrom sklearn.ensemble import RandomForestClassifier\n\nfrom bs4 import BeautifulSoup\nimport re\nimport time\n\nfrom nltk.corpus import stopwords\nimport nltk.data\nfrom nltk.stem.snowball import SnowballStemmer\nfrom multiprocessing import Pool\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n%matplotlib inline","b7dcb987":"df_train = pd.read_csv(\"..\/input\/nlp-dataset\/labeledTrainData.tsv\", header = 0,\n                      delimiter = \"\\t\", quoting = 3)\ndf_test = pd.read_csv(\"..\/input\/nlp-dataset\/testData.tsv\", header = 0,\n                      delimiter = \"\\t\", quoting = 3)\ndf_unlabeled = pd.read_csv(\"..\/input\/nlp-dataset\/unlabeledTrainData.tsv\", header = 0,\n                      delimiter = \"\\t\", quoting = 3)","cf082f03":"class KaggleWord2VecUtility(object):\n\n    @staticmethod\n    def review_to_wordlist(review, remove_stopwords=False):\n        # 1. HTML \uc81c\uac70\n        review_text = BeautifulSoup(review, \"html.parser\").get_text()\n        # 2. \ud2b9\uc218\ubb38\uc790\ub97c \uacf5\ubc31\uc73c\ub85c \ubc14\uafd4\uc90c\n        review_text = re.sub('[^a-zA-Z]', ' ', review_text)\n        # 3. \uc18c\ubb38\uc790\ub85c \ubcc0\ud658 \ud6c4 \ub098\ub208\ub2e4.\n        words = review_text.lower().split()\n        # 4. \ubd88\uc6a9\uc5b4 \uc81c\uac70\n        if remove_stopwords:\n            stops = set(stopwords.words('english'))\n            words = [w for w in words if not w in stops]\n        # 5. \uc5b4\uac04\ucd94\ucd9c\n        stemmer = SnowballStemmer('english')\n        words = [stemmer.stem(w) for w in words]\n        # 6. \ub9ac\uc2a4\ud2b8 \ud615\ud0dc\ub85c \ubc18\ud658\n        return(words)\n\n    @staticmethod\n    def review_to_join_words( review, remove_stopwords=False ):\n        words = KaggleWord2VecUtility.review_to_wordlist(\\\n            review, remove_stopwords=False)\n        join_words = ' '.join(words)\n        return join_words\n\n    @staticmethod\n    def review_to_sentences( review, remove_stopwords=False ):\n        # punkt tokenizer\ub97c \ub85c\ub4dc\ud55c\ub2e4.\n        \"\"\"\n        \uc774 \ub54c, pickle\uc744 \uc0ac\uc6a9\ud558\ub294\ub370\n        pickle\uc744 \ud1b5\ud574 \uac12\uc744 \uc800\uc7a5\ud558\uba74 \uc6d0\ub798 \ubcc0\uc218\uc5d0 \uc5f0\uacb0 \ub41c \ucc38\uc870\uac12 \uc5ed\uc2dc \uc800\uc7a5\ub41c\ub2e4.\n        \uc800\uc7a5\ub41c pickle\uc744 \ub2e4\uc2dc \uc77d\uc73c\uba74 \ubcc0\uc218\uc5d0 \uc5f0\uacb0\ub418\uc5c8\ub358\n        \ubaa8\ub4e0 \ub808\ud37c\ub7f0\uc2a4\uac00 \uacc4\uc18d \ucc38\uc870 \uc0c1\ud0dc\ub97c \uc720\uc9c0\ud55c\ub2e4.\n        \"\"\"\n        tokenizer = nltk.data.load('tokenizers\/punkt\/english.pickle')\n        # 1. nltk tokenizer\ub97c \uc0ac\uc6a9\ud574\uc11c \ub2e8\uc5b4\ub85c \ud1a0\ud070\ud654 \ud558\uace0 \uacf5\ubc31 \ub4f1\uc744 \uc81c\uac70\ud55c\ub2e4.\n        raw_sentences = tokenizer.tokenize(review.strip())\n        # 2. \uac01 \ubb38\uc7a5\uc744 \uc21c\ud68c\ud55c\ub2e4.\n        sentences = []\n        for raw_sentence in raw_sentences:\n            # \ube44\uc5b4\uc788\ub2e4\uba74 skip\n            if len(raw_sentence) > 0:\n                # \ud0dc\uadf8\uc81c\uac70, \uc54c\ud30c\ubcb3\ubb38\uc790\uac00 \uc544\ub2cc \uac83\uc740 \uacf5\ubc31\uc73c\ub85c \uce58\ud658, \ubd88\uc6a9\uc5b4\uc81c\uac70\n                sentences.append(\\\n                    KaggleWord2VecUtility.review_to_wordlist(\\\n                    raw_sentence, remove_stopwords))\n        return sentences\n\n\n    # \ucc38\uace0 : https:\/\/gist.github.com\/yong27\/7869662\n    # http:\/\/www.racketracer.com\/2016\/07\/06\/pandas-in-parallel\/\n    # \uc18d\ub3c4 \uac1c\uc120\uc744 \uc704\ud574 \uba40\ud2f0 \uc2a4\ub808\ub4dc\ub85c \uc791\uc5c5\ud558\ub3c4\ub85d\n    @staticmethod\n    def _apply_df(args):\n        df, func, kwargs = args\n        return df.apply(func, **kwargs)\n\n    @staticmethod\n    def apply_by_multiprocessing(df, func, **kwargs):\n        # \ud0a4\uc6cc\ub4dc \ud56d\ubaa9 \uc911 workers \ud30c\ub77c\uba54\ud130\ub97c \uaebc\ub0c4\n        workers = kwargs.pop('workers')\n        # \uc704\uc5d0\uc11c \uac00\uc838\uc628 workers \uc218\ub85c \ud504\ub85c\uc138\uc2a4 \ud480\uc744 \uc815\uc758\n        pool = Pool(processes=workers)\n        # \uc2e4\ud589\ud560 \ud568\uc218\uc640 \ub370\uc774\ud130\ud504\ub808\uc784\uc744 \uc6cc\ucee4\uc758 \uc218 \ub9cc\ud07c \ub098\ub220 \uc791\uc5c5\n        result = pool.map(KaggleWord2VecUtility._apply_df, [(d, func, kwargs)\n                for d in np.array_split(df, workers)])\n        pool.close()\n        # \uc791\uc5c5 \uacb0\uacfc\ub97c \ud569\uccd0\uc11c \ubc18\ud658\n        return pd.concat(result)\n    \n    \n# KaggleWord2VecUtility\ub97c class\ub85c \uc0dd\uc131\ud558\uc5ec \uc0ac\uc6a9 \n# \ucf54\ub4dc \ucd9c\ucc98: https:\/\/github.com\/corazzon\/KaggleStruggle\/blob\/master\/word2vec-nlp-tutorial\/KaggleWord2VecUtility.py","da4d616f":"sentences = []\nfor review in df_train[\"review\"]:\n    sentences += KaggleWord2VecUtility.review_to_sentences(\n    review, remove_stopwords = False)\n    \n# KaggleWord2VecUtility\uc744 \uc0ac\uc6a9\ud558\uc5ec train \ub370\uc774\ud130\ub97c \uc815\uc81c\ud574\uc900\ub2e4.","73378076":"for review in df_unlabeled[\"review\"]:\n    sentences += KaggleWord2VecUtility.review_to_sentences(\n    review, remove_stopwords = False)","3c304434":"num_features = 300 # \ubb38\uc790 \ubca1\ud130 \ucc28\uc6d0 \uc218 (size)\nmin_word_count = 40 # \ucd5c\uc18c \ubb38\uc790 \uc218 (min_count)\nnum_workers = 4 # \ubcd1\ub82c \ucc98\ub9ac \uc2a4\ub808\ub4dc \uc218 (workers)\ncontext = 10 # \ubb38\uc790\uc5f4 \ucc3d \ud06c\uae30 (window)\ndownsampling = 1e-3 # \ubb38\uc790 \ube48\ub3c4 \uc218 Downsample (sample)\n\nmodel = Word2Vec(sentences, workers = num_workers,\n                 size = num_features, min_count = min_word_count,\n                 window = context, sample = downsampling)\n\nmodel","7d0a79e2":"# \uc22b\uc790\ub85c \ub2e8\uc5b4\ub97c \ud45c\ud604\n# Word2Vec \ubaa8\ub378\uc740 \uc5b4\ud718\uc758 \uac01 \ub2e8\uc5b4\uc5d0 \ub300\ud55c feature \ubca1\ud130\ub85c \uad6c\uc131\ub418\uba70\n# 'syn0'\uc774\ub77c\ub294 \ub118\ud30c\uc774 \ubc30\uc5f4\ub85c \uc800\uc7a5\ub41c\ub2e4.\n# syn0\uc758 \ud589 \uc218\ub294 \ubaa8\ub378 \uc5b4\ud718\uc758 \ub2e8\uc5b4 \uc218\n# \uceec\ub7fc \uc218\ub294 part 2\uc5d0\uc11c \uc124\uc815\ud55c \ud53c\ucc98 \ubca1\ud130\uc758 \ud06c\uae30\ntype(model.wv.syn0)","77689885":"model.wv.syn0.shape","84f59702":"model.wv[\"flower\"].shape","41edf00a":"model.wv[\"flower\"][:10]","b7636108":"# \ub2e8\uc5b4 \ubca1\ud130\uc5d0\uc11c k-means\ub97c \uc2e4\ud589\ud558\uace0 \uc77c\ubd80 \ud074\ub7ec\uc2a4\ud130\ub97c \ucc0d\uc5b4\ubcf8\ub2e4.\nstart = time.time()\n\n# \ud074\ub7ec\uc2a4\ud130\uc758 \ud06c\uae30 \"k\"\ub97c \uc5b4\ud718 \ud06c\uae30\uc758 1\/5 \uc774\ub098 \ud3c9\uade0 5\ub2e8\uc5b4\ub85c \uc124\uc815\ud55c\ub2e4.\nword_vectors = model.wv.syn0\nnum_clusters = word_vectors.shape[0] \/ 5\nnum_clusters = int(num_clusters)\n\n# K-means\ub97c \uc815\uc758\ud558\uace0  \ud559\uc2b5\uc2dc\ud0a8\ub2e4.\nkmeans_clustering = KMeans(n_clusters = num_clusters)\nidx = kmeans_clustering.fit_predict(word_vectors)\n\n# \ub05d\ub09c\uc2dc\uac04\uc5d0\uc11c \uc2dc\uc791\uc2dc\uac04\uc744 \ube7c\uc11c \uac78\ub9b0 \uc2dc\uac04\uc744 \uad6c\ud55c\ub2e4.\nend = time.time()\nelapsed = end - start\n\nprint(\"Time taken for K Means clustering: \", elapsed, \"seconds.\")","686c06ad":"# \uac01 \uc5b4\ud718 \ub2e8\uc5b4\ub97c \ud074\ub7ec\uc2a4\ud130 \ubc88\ud638\uc5d0 \ub9e4\ud551\ub418\uac8c word\/index \uc0ac\uc804\uc744 \ub9cc\ub4e0\ub2e4.\nidx = list(idx)\nnames = model.wv.index2word\nword_centroid_map = {names[i]: idx[i] for i in range(len(names))}\n# word_cenetroid_map = dict(zip(model.wv.index2word, idx))\n\n# \uccab\ubc88\uc9f8 \ud074\ub7ec\uc2a4\ud130\uc758 \ucc98\uc74c 10\uac1c\ub97c \ucd9c\ub825\nfor cluster in range(0, 10):\n    # \ud074\ub7ec\uc2a4\ud130 \ubc88\ud638\ub97c \ucd9c\ub825\n    print(\"\\n Cluster {}\".format(cluster))\n    \n    # \ud074\ub7ec\uc2a4\ud130 \ubc88\ud638\uc640 \ud074\ub7ec\uc2a4\ud130\uc5d0 \uc788\ub294 \ub2e8\uc5b4\ub97c \ucc0d\ub294\ub2e4.\n    words = []\n    for i in range(0, len(list(word_centroid_map.values()))):\n        if(list(word_centroid_map.values())[i] == cluster):\n            words.append(list(word_centroid_map.keys())[i])\n    print(words)","9b634985":"\"\"\"\n\ud310\ub2e4\uc2a4\ub85c \ub370\uc774\ud130 \ud504\ub808\uc784 \ud615\ud0dc\uc758 \ub370\uc774\ud130\ub85c \uc77d\uc5b4\uc628\ub2e4.\n\n\uadf8\ub9ac\uace0 \uc774\uc804 \ud29c\ud1a0\ub9ac\uc5bc\uc5d0\uc11c \ud588\ub358 \uac83 \ucc98\ub7fc clean_train_review\uc640\nclean_test_review\ub85c \ud14d\uc2a4\ud2b8\ub97c \uc815\uc81c\ud55c\ub2e4.\n\"\"\"","5e9af9b5":"clean_train_reviews = []\nfor review in df_train[\"review\"]:\n    clean_train_reviews.append(\n        KaggleWord2VecUtility.review_to_wordlist(review,\n                                                remove_stopwords = True))","afc0c5ee":"clean_test_reviews = []\nfor review in df_test[\"review\"]:\n    clean_test_reviews.append(\n        KaggleWord2VecUtility.review_to_wordlist(review,\n                                                remove_stopwords = True))","10308bad":"# bag of centroids \uc0dd\uc131\n# \uc18d\ub3c4\ub97c \uc704\ud574 centroid \ud559\uc2b5 \uc138\ud2b8 bag\uc744 \ubbf8\ub9ac \ud560\ub2f9\ud55c\ub2e4.\ntrain_centroids = np.zeros((df_train[\"review\"].size, num_clusters),\n                          dtype = \"float32\")\n\ntrain_centroids[:5]","b36e25a6":"# centroid\ub294 \ub450 \ud074\ub7ec\uc2a4\ud130\uc758 \uc911\uc2ec\uc810\uc744 \uc815\uc758 \ud55c \ub2e4\uc74c \uc911\uc2ec\uc810\uc758 \uac70\ub9ac\ub97c \uce21\uc815\ud55c \uac83\ndef create_bag_of_centroids(wordlist, word_centroid_map):\n    \n    # \ud074\ub7ec\uc2a4\ud130\uc758 \uc218\ub294 word\/centroid map\uc5d0\uc11c \uac00\uc7a5 \ub192\uc740 \ud074\ub7ec\uc2a4\ud2b8 \uc778\ub371\uc2a4\uc640 \uac19\ub2e4.\n    num_centroids = max(word_centroid_map.values())+1\n    \n    # \uc18d\ub3c4\ub97c \uc704\ud574 bag of centroids vector\ub97c \ubbf8\ub9ac \ud560\ub2f9\ud55c\ub2e4.\n    bag_of_centroids = np.zeros(num_centroids, dtype = \"float32\")\n    \n    # \ub8e8\ud504\ub97c \ub3cc\uba70 \ub2e8\uc5b4\uac00 word_centroid_map\uc5d0 \uc788\ub2e4\uba74\n    # \ud574\ub2f9\ub418\ub294 \ud074\ub7ec\uc2a4\ud130\uc758 \uc218\ub97c \ud558\ub098\uc529 \uc99d\uac00\uc2dc\ucf1c \uc900\ub2e4.\n    for word in wordlist:\n        if word in word_centroid_map:\n            index = word_centroid_map[word]\n            bag_of_centroids[index] += 1\n            \n    # bag of centroids\ub97c \ubc18\ud658\ud55c\ub2e4.\n    return bag_of_centroids","287dd556":"# \ud559\uc2b5 \ub9ac\ubdf0\ub97c bags of centroids\ub85c \ubcc0\ud658\ud55c\ub2e4.\ncounter = 0\nfor review in clean_train_reviews:\n    train_centroids[counter] = create_bag_of_centroids(review,\n                                                      word_centroid_map)\n    counter += 1\n\n# \ud14c\uc2a4\ud2b8 \ub9ac\ubdf0\ub3c4 \uac19\uc740 \ubc29\ubc95\uc73c\ub85c \ubc18\ubcf5\ud574 \uc900\ub2e4.\ntest_centroids = np.zeros((df_test[\"review\"].size, num_clusters),\n                         dtype = \"float32\")\n\ncounter = 0\nfor review in clean_test_reviews:\n    test_centroids[counter] = create_bag_of_centroids(review,\n                                                      word_centroid_map)\n    counter += 1","ab305eb2":"# RandomForest\ub97c \uc0ac\uc6a9\ud558\uc5ec \ud559\uc2b5\uc2dc\ud0a4\uace0 \uc608\uce21\nfrom sklearn.ensemble import RandomForestClassifier\nrf = RandomForestClassifier(n_estimators = 100)\n\n# train \ub370\uc774\ud130\uc758 \ub808\uc774\ube14\uc744 \ud1b5\ud574 \ud559\uc2b5\uc2dc\ud0a4\uace0 \uc608\uce21\ud55c\ub2e4.\n%time rf = rf.fit(train_centroids, df_train[\"sentiment\"])","4ff7c265":"from sklearn.model_selection import cross_val_score\n%time score = np.mean(cross_val_score(rf, train_centroids,\\\n                                df_train[\"sentiment\"],\\\n                                      cv = 10,\\\n                                      scoring = \"roc_auc\"))","3e919b49":"%time result = rf.predict(test_centroids)","bb8fbc1a":"score","0b788507":"output = pd.DataFrame(data = {\"id\": df_test[\"id\"], \"sentiment\":result})\noutput.to_csv(\".\/submit_BagofCentroids_{:.3f}.csv\".format(score),index = False,quoting = 3)","07e0dbf1":"![](https:\/\/static.amazon.jobs\/teams\/53\/images\/IMDb_Header_Page.jpg?1501027252)","d760c11e":"## \uc65c \uc774 \ud29c\ud1a0\ub9ac\uc5bc\uc5d0\uc11c\ub294 BoW\uac00 \ub354 \uc88b\uc740 \uacb0\uacfc\ub97c \uac00\uc838\uc62c\uae4c?\n\n- \ubca1\ud130\ub97c \ud3c9\uade0\ud654\ud558\uace0 centroids\ub97c \uc0ac\uc6a9\ud558\uba74 \ub2e8\uc5b4 \uc21c\uc11c\uac00 \uc5b4\uae0b\ub098\uba70 BoW \uac1c\ub150\uacfc \ub9e4\uc6b0 \ube44\uc2b7\ud558\ub2e4. \uc131\ub2a5\uc774(\ud45c\uc900 \uc624\ucc28\uc758 \ubc94\uc704 \ub0b4\uc5d0\uc11c)\ube44\uc2b7\ud558\uae30 \ub54c\ubb38\uc5d0 \ud29c\ud1a0\ub9ac\uc5bc 1, 2, 3\uc774 \ub3d9\ub4f1\ud55c \uacb0\uacfc\ub97c \uac00\uc838\uc628\ub2e4.\n\n- 1) Word2Vec\uc744 \ub354 \ub9ce\uc740 \ud14d\uc2a4\ud2b8\ub85c \ud559\uc2b5\uc2dc\ud0a4\uba74 \uc131\ub2a5\uc774 \uc88b\uc544\uc9c4\ub2e4. Google\uc758 \uacb0\uacfc\ub294 10\uc5b5 \ub2e8\uc5b4\uac00 \ub118\ub294 corpus\uc5d0\uc11c \ubc30\uc6b4 \ub2e8\uc5b4 \ubca1\ud130\ub97c \uae30\ubc18\uc73c\ub85c \ud55c\ub2e4. \ud559\uc2b5\ub808\uc774\ube14\uc774 \uc788\uac70\ub098 \ub808\uc774\ube14\uc774 \uc5c6\ub294 \ud559\uc2b5 \uc138\ud2b8\ub294 \ub2e8\uc9c0 \ub300\ub825 \ucc9c\ud314\ubc31\ub9cc \ub2e8\uc5b4 \uc815\ub3c4\ub2e4. \ud3b8\uc758\uc0c1 Word2Vec\uc740 Google\uc758 \uc6d0\ub798 C\ub3c4\uad6c\uc5d0\uc11c \ucd9c\ub825\ub418\ub294 \uc0ac\uc804 \ud559\uc2b5\ub41c \ubaa8\ub378\uc744 \ub85c\ub4dc\ud558\ub294 \uae30\ub2a5\uc744 \uc81c\uacf5\ud558\uae30 \ub54c\ubb38\uc5d0 C\ub85c \ubaa8\ub378\uc744 \ud559\uc2b5\ud55c \ub2e4\uc74c Python\uc73c\ub85c \uac00\uc838\uc62c \uc218\ub3c4 \uc788\ub2e4.\n\n- 2) \ucd9c\ud310 \ub41c \uc790\ub8cc\ub4e4\uc5d0\uc11c \ubd84\uc0b0 \uc6cc\ub4dc \ubca1\ud130 \uae30\uc220\uc740 BoW \ubaa8\ub378\ubcf4\ub2e4 \uc6b0\uc218\ud55c \uac83\uc73c\ub85c \ub098\ud0c0\ub0ac\ub2e4. \uc774 \ub17c\ubb38\uc5d0\uc11c\ub294 IMDb \ub370\uc774\ud130 \uc9d1\ud569\uc5d0 \ub2e8\ub77d \ubca1\ud130(Paragraph Vector)\ub77c\ub294 \uc54c\uace0\ub9ac\uc998\uc744 \uc0ac\uc6a9\ud558\uc5ec \ud604\uc7ac\uae4c\uc9c0\uc758 \ucd5c\ucca8\ub2e8 \uacb0\uacfc \uc911 \uc77c\ubd80\ub97c \uc0dd\uc131\ud55c\ub2e4. \ub2e8\ub77d \ubca1\ud130\ub294 \ub2e8\uc5b4 \uc21c\uc11c \uc815\ubcf4\ub97c \ubcf4\uc874\ud558\ub294 \ubc18\uba74 \ubca1\ud130 \ud3c9\uade0\ud654 \ubc0f \ud074\ub7ec\uc2a4\ud130\ub9c1\uc740 \ub2e8\uc5b4 \uc21c\uc11c\ub97c \uc783\uc5b4 \ubc84\ub9ac\uae30 \ub54c\ubb38\uc5d0 \uc5ec\uae30\uc5d0\uc11c \uc2dc\ub3c4\ud558\ub294 \ubc29\uc2dd\ubcf4\ub2e4 \ubd80\ubd84\uc801\uc73c\ub85c \uc88b\ub2e4.","1b2a4cb0":"## \uccab \ubc88\uc9f8 \uc2dc\ub3c4 (average feature vectors)\n- \ud29c\ud1a0\ub9ac\uc5bc2\uc758 \ucf54\ub4dc\ub85c \ubca1\ud130\uc758 \ud3c9\uade0\uc744 \uad6c\ud55c\ub2e4.\n\n## \ub450 \ubc88\uc9f8 \uc2dc\ub3c4 (K-means)\n- Word2Vec\uc740 \uc758\ubbf8\uac00 \uad00\ub828\uc788\ub294 \ub2e8\uc5b4\ub4e4\uc758 \ud074\ub7ec\uc2a4\ud130\ub97c \uc0dd\uc131\ud558\uae30 \ub54c\ubb38\uc5d0 \ud074\ub7ec\uc2a4\ud130 \ub0b4\uc758 \ub2e8\uc5b4 \uc720\uc0ac\uc131\uc744 \uc774\uc6a9\ud558\ub294 \uac83\uc774\ub2e4.\n- \uc774\ub7f0\uc2dd\uc73c\ub85c \ubca1\ud130\ub97c \uadf8\ub8f9\ud654 \ud558\ub294 \uac83\uc744 'Vector Quantization'(\ubca1\ud130 \uc591\uc790\ud654)\ub77c\uace0 \ud55c\ub2e4.\n- \uc774\ub97c \uc704\ud574\uc11c\ub294 K-means\uc640 \uac19\uc740 \ud074\ub7ec\uc2a4\ud130\ub9c1 \uc54c\uace0\ub9ac\uc998\uc744 \uc0ac\uc6a9\ud558\uc5ec \ud074\ub7ec\uc2a4\ud130\ub77c\ub294 \ub2e8\uc5b4\uc758 \uc911\uc2ec\uc744 \ucc3e\uc544\uc57c \ud55c\ub2e4.\n- \ube44\uc9c0\ub3c4 \ud559\uc2b5\uc778 K-means\ub97c \ud1b5\ud574 \ud074\ub7ec\uc2a4\ud130\ub9c1\uc744 \ud558\uace0 \uc9c0\ub3c4\ud559\uc2b5\uc778 \ub79c\ub364\ud3ec\ub808\uc2a4\ud2b8\ub85c \ub9ac\ubdf0\uac00 \ucd94\ucc9c\uc778\uc9c0 \uc544\ub2cc\uc9c0\ub97c \uc608\uce21\ud55c\ub2e4.","a971e118":"# K-means \ud074\ub7ec\uc2a4\ud130\ub9c1\uc73c\ub85c \ub370\uc774\ud130 \ubb36\uae30\n- [k-\ud3c9\uade0 \uc54c\uace0\ub9ac\uc998 - \uc704\ud0a4\ubc31\uacfc, \uc6b0\ub9ac \ubaa8\ub450\uc758 \ubc31\uacfc\uc0ac\uc804](https:\/\/ko.wikipedia.org\/wiki\/K-%ED%8F%89%EA%B7%A0_%EC%95%8C%EA%B3%A0%EB%A6%AC%EC%A6%98)\n- \ud074\ub7ec\uc2a4\ud130\ub9c1\uc740 \ube44\uc9c0\ub3c4 \ud559\uc2b5 \uae30\ubc95\n- \ud074\ub7ec\uc2a4\ud130\ub9c1\uc740 \uc720\uc0ac\uc131 \ub4f1 \uac1c\ub150\uc5d0 \uae30\ucd08\ud574 \uba87\uba87 \uadf8\ub8f9\uc73c\ub85c \ubd84\ub958\ud558\ub294 \uae30\ubc95\n- \ud074\ub7ec\uc2a4\ud130\ub9c1\uc758 \ubaa9\uc801\uc740 \uc0d8\ud50c(\uc2e4\uc218\ub85c \uad6c\uc131\ub41c n\ucc28\uc6d0\uc758 \ubca1\ud130)\uc744 \ub0b4\ubd80\uc801\uc73c\ub85c\ub294 \ube44\uc2b7\ud558\uc9c0\ub9cc \uc678\ubd80\uc801\uc73c\ub85c \uacf5\ud1b5 \ubd84\ubaa8\uac00 \uc5c6\ub294 \uc5ec\ub7ec \uadf8\ub8f9\uc73c\ub85c \ubb36\ub294 \uac83 \n- \ud2b9\uc815 \ucc28\uc6d0\uc758 \ubc94\uc704\uac00 \ub2e4\ub978 \ucc28\uc6d0\uacfc \ucc28\uc774\uac00 \ud06c\uba74 \ud074\ub7ec\uc2a4\ud130\ub9c1 \ud558\uae30 \uc804\uc5d0 \uc2a4\ucf00\uc77c\uc744 \uc870\uc815\ud574\uc57c \ud55c\ub2e4.\n\n   1. \ucd5c\ucd08 \uc13c\ud2b8\ub85c\uc774\ub4dc(centroid)(\uc911\uc2ec\uc810)\ub85c K\uac1c\uc758 \ubca1\ud130\ub97c \ubb34\uc791\uc704\ub85c \uc120\uc815\ud55c\ub2e4.\n   2. \uac01 \uc0d8\ud50c\uc744 \uadf8 \uc704\uce58\uc5d0\uc11c \uac00\uc7a5 \uac00\uae4c\uc6b4 \uc13c\ud2b8\ub85c\uc774\ub4dc\uc5d0 \ud560\ub2f9\ud55c\ub2e4.\n   3. \uc13c\ud2b8\ub85c\uc774\ub4dc\uc758 \uc704\uce58\ub97c \uc7ac\uacc4\uc0b0\ud55c\ub2e4.\n   4. \uc13c\ud2b8\ub85c\uc774\ub4dc\uac00 \ub354 \uc774\uc0c1 \uc6c0\uc9c1\uc774\uc9c0 \uc54a\uc744 \ub54c\uae4c\uc9c0 2\uc640 3\uc744 \ubc18\ubcf5\ud55c\ub2e4.\n   \n\ucc38\uace0: [\ucc45]\ubaa8\ub450\uc758 \ub370\uc774\ud130 \uacfc\ud559(with \ud30c\uc774\uc36c)","692ea8af":"# Tutorial Part 3,4\n\n#### ***[\ucf54\ub4dc\ucd9c\ucc98: \uc624\ub298\ucf54\ub4dc(\ubc15\uc870\uc740 \ub2d8)](https:\/\/github.com\/corazzon)***"}}