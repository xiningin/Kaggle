{"cell_type":{"adf7f898":"code","ad52efb4":"code","1cff9f56":"code","a4eb5a30":"code","6d01bc3c":"code","79f85c8c":"code","14d7da80":"code","631fb055":"code","9cae899a":"code","703a4e0f":"code","8003abc6":"code","c31419f2":"code","2af76e49":"code","1f19b99b":"code","219c5a2b":"code","4d7e1ff9":"code","34c58dda":"code","9ada84f5":"code","7bd9e760":"code","2564d005":"code","08be68c5":"code","f920bdbd":"code","c926b620":"code","be6d5a95":"code","82d21fd2":"code","8da0eb2f":"code","38b0d7ee":"code","103ea8a6":"markdown","c1f16cf5":"markdown","b0b1b947":"markdown","0be1a887":"markdown","e56079fd":"markdown","70633ada":"markdown","b2b6f3eb":"markdown","554bd1b8":"markdown","c23fc564":"markdown","f21928b1":"markdown","e3cd08d6":"markdown"},"source":{"adf7f898":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","ad52efb4":"import pandas as pd \nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns","1cff9f56":"from path import Path\nfrom typing import List\n\ndef read_data(root: str = '..\/input\/hsemath2020flights',\n              target_values: List[str] = ['dep_delayed_15min']):\n    \"\"\"\n    It reads data and separate target values from features\n    \n    Args:\n    root - path to hsemath2020flights folder: str\n    target_values - list of columns that contain target values: List[str]\n    \n    Returns:\n    train - train dataset: pd.DataFrame\n    test - test dataset: pd.DataFrame\n    target_value - list of target columns: List[str]\n    features - list of feature columns: List[str]\n    \"\"\"\n    root = Path(root)\n    train, test = pd.read_csv(root \/ \"flights_train.csv\"), pd.read_csv(root \/ \"flights_test.csv\")\n    target_value = target_values\n    features = test.columns\n    \n    return train, test, target_value, features\n\nbase_train, base_test, TARGET_VALUE, FEATURES = read_data()\n\n\n","a4eb5a30":"len(base_train), len(base_test)","6d01bc3c":"# \u0420\u0430\u0437\u0434\u0435\u043b\u0438\u043c \u0434\u0430\u0442\u0443 \u043d\u0430 \u0441\u043e\u0441\u0442\u0430\u0432\u043b\u044f\u044e\u0449\u0438\u0435\nCATEGORICAL = ['AIRLINE', 'ORIGIN_AIRPORT', 'DESTINATION_AIRPORT']\n\ndef split_date_to_dmy(df: pd.DataFrame):\n    \"\"\"it splits column DATE into 3 columns YEAR, MONTH, DAY \n    and then removes DATE column\"\"\"\n    df['YEAR'] = df['DATE'].apply(lambda x: int(x.split('-')[2]))\n    df['MONTH'] = df['DATE'].apply(lambda x: int(x.split('-')[1]))\n    df['DAY'] = df['DATE'].apply(lambda x: int(x.split('-')[0]))\n    df.drop(['DATE'], axis=1, inplace=True)\n    return df\n\ndef split_dep_time_to_hm(df: pd.DataFrame):\n    \"\"\"it splits DEPARTURE_TIME into 2 columns: DEP_HOUR, DEP_MINUTE,\n    and then removes DEPARTURE_TIME column\"\"\"\n    df['DEPARTURE_TIME'] = df['DEPARTURE_TIME'].astype(int)\n    df['DEP_HOUR'] = df['DEPARTURE_TIME'].apply(lambda x: int((\"%04d\" % x)[:-2]))\n    df['DEP_HOUR'] = df['DEP_HOUR'].apply(lambda x: 0 if x == 24 else x)\n    df['DEP_MINUTE'] = df['DEPARTURE_TIME'].apply(lambda x: int((\"%04d\" % x)[-2:]))\n    df.drop(['DEPARTURE_TIME'], axis=1, inplace=True)\n    return df\n\ndef cat_as_str(df: pd.DataFrame, categorical_columns: List[str]):\n    \"\"\"each categorical value will be present as <*> for convenient\"\"\"\n    for col in categorical_columns:\n        df[col] = df[col].apply(lambda x: \"<\" + str(x) + \">\")\n    return df\n\ndef base_preparation(df: pd.DataFrame):\n    \"\"\"it combines previous fucntions\"\"\"\n    split_date_to_dmy(df)\n    split_dep_time_to_hm(df)\n    cat_as_str(df, CATEGORICAL)\n    return df\n    ","79f85c8c":"base_preparation(base_train)\nbase_preparation(base_test)\n\nbase_train.head()","14d7da80":"#\u043f\u043e\u0441\u043c\u043e\u0442\u0440\u0438\u043c \u0441\u043a\u043e\u043b\u044c\u043a\u043e \u043f\u0440\u043e\u043f\u0443\u0441\u043a\u043e\u0432 \u0432 \u0434\u0430\u043d\u043d\u044b\u0445\nbase_train.isnull().sum(axis=0)","631fb055":"# \u0441\u0442\u0430\u0442\u0438\u0441\u0442\u0438\u0447\u0435\u0441\u043a\u043e\u0435 \u043e\u043f\u0438\u0441\u0430\u043d\u0438\u0435 \u0434\u0430\u043d\u043d\u044b\u0445\nbase_train.describe()","9cae899a":"# \u0441\u043e\u043e\u0442\u043d\u043e\u0448\u0435\u043d\u0438\u0435 \u043a\u043b\u0430\u0441\u0441\u043e\u0432\nbase_train.groupby(TARGET_VALUE).count()","703a4e0f":"# \u0418\u0437\u043e\u0431\u0440\u0430\u0437\u0438\u043c \u0434\u0430\u043d\u043d\u044b\u0435 \u043a\u0430\u0436\u0434\u043e\u0433\u043e \u0441\u0442\u043e\u043b\u0431\u0446\u0430\n\ndef plot_hist(column: str, data: pd.DataFrame, ax=None):\n    if column in CATEGORICAL:\n        sns.countplot(x=column, data=data, ax=ax)        \n    else:\n        sns.distplot(data[col], ax=ax)\n\n        \ndef plot_conditional_hist(column: str, data: pd.DataFrame, figsize=(13, 7), cond=TARGET_VALUE[0]):\n    if column in CATEGORICAL:\n        g = sns.FacetGrid(data, col=cond)\n        g = (g.map(sns.countplot, column))\n        g.fig.set_size_inches(*figsize)\n                    \n    else:\n        _, bins = np.histogram(data[column])\n        g = sns.FacetGrid(data, col=cond)\n        g = (g.map(sns.distplot, column, bins=bins))\n        g.fig.set_size_inches(*figsize)","8003abc6":"fig, axes = plt.subplots(3, 3, figsize=(23, 19))\nfig.suptitle('Historgrams for columns', fontsize=16)\n\nfor i, col in enumerate(base_train.drop(TARGET_VALUE, axis=1).columns):\n    plot_hist(col, base_train, ax=axes[int(i \/ 3)][int(i % 3)])\n\nfor i in range(3):\n    plt.close(i + 2) # delete extra figures (seaborn troubles)\n\n\n","c31419f2":"# \u0420\u0430\u0441\u0441\u043c\u043e\u0442\u0440\u0438\u043c \u0443\u0441\u043b\u043e\u0432\u043d\u044b\u0435 \u0433\u0438\u0441\u0442\u043e\u0433\u0440\u0430\u043c\u043c\u044b (\u0437\u0430\u0432\u0438\u0441\u0438\u043c\u043e\u0441\u0442\u044c \u043e\u0442 \u0446\u0435\u043b\u0435\u0432\u043e\u0439 \u043f\u0435\u0440\u0435\u043c\u0435\u043d\u043d\u043e\u0439)\n\nfor i, col in enumerate(base_train.drop(TARGET_VALUE, axis=1).columns):\n    plot_conditional_hist(col, base_train, figsize=(21, 7))\n\n","2af76e49":"from sklearn.model_selection import train_test_split\n\n# \u0420\u0430\u0437\u0434\u0435\u043b\u0438\u043c base_train \u043d\u0430 train \u0438 test, \u0447\u0442\u043e\u0431\u044b \u0438\u043c\u0435\u0442\u044c \u0432\u043e\u0437\u043c\u043e\u0436\u043d\u043e\u0441\u0442\u044c \u043e\u0446\u0435\u043d\u0438\u0442\u044c \u043c\u043e\u0434\u0435\u043b\u044c \u0431\u0435\u0437 submission\ntrain, test = train_test_split(base_train, stratify=base_train[TARGET_VALUE], test_size=0.1)\nprint(len(test))\ntest.head()","1f19b99b":"def save_predictions(file_name, y_predict):\n    \"\"\"\n    It writes predictions in right format to '.\/file_name' file\n        \n    Args:\n    file_name - the name of the file where the predictions will be saved: str\n    y_predict - result of 'predict_proba' method, y_predict.shape = (n, 2): np.ndarray\n    \n    Returns:\n    None\n    \"\"\"\n    prediction = pd.DataFrame(y_predict[:, 1], columns=['dep_delayed_15min'])\n    prediction.to_csv(file_name, index_label='id')","219c5a2b":"def cut_data(data: pd.DataFrame):\n    \"\"\"It drops unnecessary features\"\"\"\n    new_data = data[['DISTANCE', 'MONTH']]\n    new_data['TIME'] = data.apply(lambda x: int(x['DEP_HOUR']) * 100 + int(x['DEP_MINUTE']), axis=1)\n    if TARGET_VALUE[0] in data.columns:\n        new_data[TARGET_VALUE] = data[TARGET_VALUE]\n    return new_data","4d7e1ff9":"train = cut_data(train)\ntest = cut_data(test)\nbase_test = cut_data(base_test)\nbase_test.head()","34c58dda":"CATEGORICAL = ['MONTH']\nFEATURES = base_test.columns","9ada84f5":"y_train = np.array(train[TARGET_VALUE].astype(int))\nX_train = np.array(train[FEATURES])\n\ny_test = np.array(test[TARGET_VALUE].astype(int))\nX_test = np.array(test[FEATURES].astype(int))\n\nX_base_test = np.array(base_test)\n\nindexes_of_categorical_features = [num for num, col in enumerate(FEATURES) if col in CATEGORICAL]\nindexes_of_categorical_features\nindexes_of_numerical_features = [num for num, col in enumerate(FEATURES) if col not in CATEGORICAL]\nindexes_of_numerical_features","7bd9e760":"from sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.compose import ColumnTransformer\n\nct = ColumnTransformer(\n    transformers=[('cat', OneHotEncoder(handle_unknown='ignore'), indexes_of_categorical_features),\n                  ('num', StandardScaler(), indexes_of_numerical_features)],    # The column numbers to be transformed (here is [0] but can be [0, 1, 3])\n    remainder='passthrough'                         # Leave the rest of the columns untouched\n)\n\nX_train_hat = ct.fit_transform(X_train).toarray()\nX_test_hat = ct.transform(X_test).toarray()\nX_base_test_hat = ct.transform(X_base_test).toarray()\n\n","2564d005":"X_train_hat.shape","08be68c5":"from sklearn.linear_model import LogisticRegression\n\nmodel = LogisticRegression()","f920bdbd":"from sklearn.model_selection import RandomizedSearchCV\n\nhyperparameter = {'penalty': ['l2'], 'C': np.linspace(0.1, 1, 100)}\nsearch = RandomizedSearchCV(model, hyperparameter, n_iter=9)\nsearch.fit(X_train_hat, y_train.ravel())\nl2 = search.best_params_\nl2","c926b620":"model = LogisticRegression(**l2)\nmodel.fit(X_train_hat, y_train.ravel())\n\ny_test_predictions = model.predict_proba(X_test_hat)\ny_test_predictions","be6d5a95":"from sklearn.metrics import roc_auc_score\n\n# \u041f\u043e\u0441\u043c\u043e\u0442\u0440\u0438\u043c \u043a\u0430\u043a\u043e\u0439 \u0441\u043a\u043e\u0440 \u0434\u0430\u0441\u0442 \u043c\u043e\u0434\u0435\u043b\u044c \u043d\u0430 \u043d\u0430\u0448\u0435\u043c \u0432\u044b\u0434\u0435\u043b\u0435\u043d\u043d\u043e\u043c \u043a\u0443\u0441\u043a\u0435 \u043e\u0431\u0443\u0447\u0430\u044e\u0449\u0435\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0438\nroc_auc_score(y_test.ravel(), y_test_predictions[:, 1])","82d21fd2":"y_base_test_predictions = model.predict_proba(X_base_test_hat)\nsave_predictions('.\/logreg_v0.0_tunnedl2_0.67.csv', y_base_test_predictions)","8da0eb2f":"pd.read_csv('.\/logreg_v0.0_tunnedl2_0.67.csv').head()","38b0d7ee":"# \u043c\u043e\u0434\u0435\u043b\u044c \u0432\u044b\u0434\u0430\u0435\u0442 0.67706 \u0441\u043a\u043e\u0440.","103ea8a6":"## \u041f\u0440\u043e\u0432\u0435\u0440\u043a\u0430 \u0440\u0430\u0431\u043e\u0442\u044b \u043c\u043e\u0434\u0435\u043b\u0438","c1f16cf5":"## \u0412\u0438\u0437\u0443\u0430\u043b\u044c\u043d\u044b\u0439 \u0430\u043d\u0430\u043b\u0438\u0437","b0b1b947":"## \u0411\u0430\u0437\u043e\u0432\u0430\u044f \u043f\u0440\u0435\u0434\u043e\u0431\u0440\u0430\u0431\u043e\u0442\u043a\u0430","0be1a887":"## \u041a\u043e\u043d\u0435\u0447\u043d\u044b\u0439 \u043f\u0440\u0435\u0434\u0438\u043a\u0442","e56079fd":"## \u0417\u0430\u043f\u0438\u0441\u044c \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u0439 \u0432 \u0444\u0430\u0439\u043b","70633ada":"\u041f\u043e \u0443\u0441\u043b\u043e\u0432\u0438\u044e \u0432 \u0431\u0430\u0437\u043e\u0432\u0443\u044e \u043c\u043e\u0434\u0435\u043b\u044c \u043d\u0435\u043e\u0431\u0445\u043e\u0434\u0438\u043c\u043e \u0438\u0437 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432 \u043f\u043e\u0434\u0430\u0432\u0430\u0442\u044c \u0442\u043e\u043b\u044c\u043a\u043e 'DISTANCE', 'MONTH', 'TIME'","b2b6f3eb":"## \u041f\u043e\u0434\u0431\u043e\u0440 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u043e\u0432","554bd1b8":"## \u0421\u0447\u0438\u0442\u044b\u0432\u0430\u043d\u0438\u0435 \u0434\u0430\u043d\u043d\u044b\u0445","c23fc564":"> ## \u0420\u0430\u0437\u0434\u0435\u043b\u0435\u043d\u0438\u0435 \u043d\u0430 \u0442\u0435\u0441\u0442 \u0438 \u043e\u0431\u0443\u0447\u0430\u044e\u0449\u0443\u044e","f21928b1":"## \u041a\u043e\u0434\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u0435 \u043a\u0430\u0442\u0435\u0433\u043e\u0440\u0438\u0430\u043b\u044c\u043d\u044b\u0445 \u043f\u0435\u0440\u0435\u043c\u0435\u043d\u043d\u044b\u0445","e3cd08d6":"## \u0411\u0430\u0437\u043e\u0432\u0430\u044f \u043c\u043e\u0434\u0435\u043b\u044c -- \u043b\u043e\u0433\u0438\u0441\u0442\u0438\u0447\u0435\u0441\u043a\u0430\u044f \u0440\u0435\u0433\u0440\u0435\u0441\u0441\u0438\u044f"}}