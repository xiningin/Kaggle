{"cell_type":{"732b3f06":"code","34285c59":"code","97adcc5d":"code","35846218":"code","32b07e64":"code","b2dc37c5":"code","cc9364d6":"code","993a8eea":"code","5d39854b":"code","2388811e":"code","461d68f7":"code","62bcab61":"code","cfb6c84b":"code","3f3f1925":"code","c663bd32":"code","b9a7d5a1":"code","f3e96efe":"code","1799d24f":"code","22f08b88":"markdown","e1770a63":"markdown","3071bc26":"markdown","4c6c8c95":"markdown","1248db50":"markdown","45decbdd":"markdown","fa62ae3c":"markdown","fab9cad4":"markdown","be0a30be":"markdown","eb098fdc":"markdown","06070431":"markdown","fb1ec4af":"markdown","061e34ca":"markdown","e2ea3ab2":"markdown","afb543e6":"markdown","9f16f5cc":"markdown","34f8eaa1":"markdown","6e9aeace":"markdown","27809ade":"markdown","08a1cbd0":"markdown","35435ea2":"markdown","86fcd86c":"markdown","c5d37d82":"markdown","47a526a1":"markdown","34bed1f3":"markdown","d0dadd77":"markdown"},"source":{"732b3f06":"import keras\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport os\n\nfrom sklearn import preprocessing\nfrom sklearn.metrics import confusion_matrix, recall_score, precision_score\nfrom keras.models import Sequential,load_model\nfrom keras.layers import Dense, Dropout, LSTM\n\n# define path to save model\nmodel_path = 'binary_model.h5'","34285c59":"train_df = pd.read_csv('..\/input\/pm-dataset\/PM_train.txt', sep=\" \", header=None)\ntrain_df.drop(train_df.columns[[26, 27]], axis=1, inplace=True)\ntrain_df.columns = ['id', 'cycle', 'setting1', 'setting2', 'setting3', 's1', 's2', 's3',\n                     's4', 's5', 's6', 's7', 's8', 's9', 's10', 's11', 's12', 's13', 's14',\n                     's15', 's16', 's17', 's18', 's19', 's20', 's21']\n\ntrain_df = train_df.sort_values(['id','cycle'])\ntrain_df.tail()","97adcc5d":"test_df = pd.read_csv('..\/input\/pm-dataset\/PM_test.txt', sep=\" \", header=None)\ntest_df.drop(test_df.columns[[26, 27]], axis=1, inplace=True)\ntest_df.columns = ['id', 'cycle', 'setting1', 'setting2', 'setting3', 's1', 's2', 's3',\n                     's4', 's5', 's6', 's7', 's8', 's9', 's10', 's11', 's12', 's13', 's14',\n                     's15', 's16', 's17', 's18', 's19', 's20', 's21']\ntest_df.head()","35846218":"truth_df = pd.read_csv('..\/input\/pm-dataset\/PM_truth.txt', sep=\" \", header=None)\ntruth_df.drop(truth_df.columns[[1]], axis=1, inplace=True)\ntruth_df.head()","32b07e64":"rul = pd.DataFrame(train_df.groupby('id')['cycle'].max()).reset_index()\nrul.columns = ['id', 'max']\ntrain_df = train_df.merge(rul, on=['id'], how='left')\ntrain_df['RUL'] = train_df['max'] - train_df['cycle']\ntrain_df.drop('max', axis=1, inplace=True)\n\n# Generate label columns for training data\n# we will only make use of \"label1\" for binary classification, while trying to answer the question: is a specific engine going to fail within w1 cycles?\nw1 = 30\nw0 = 15\ntrain_df['label1'] = np.where(train_df['RUL'] <= w1, 1, 0 )\ntrain_df['label2'] = train_df['label1']\ntrain_df.loc[train_df['RUL'] <= w0, 'label2'] = 2\n\n# MinMax normalization (from 0 to 1)\ntrain_df['cycle_norm'] = train_df['cycle']\ncols_normalize = train_df.columns.difference(['id','cycle','RUL','label1','label2'])\nmin_max_scaler = preprocessing.MinMaxScaler()\nnorm_train_df = pd.DataFrame(min_max_scaler.fit_transform(train_df[cols_normalize]), \n                             columns=cols_normalize, \n                             index=train_df.index)\njoin_df = train_df[train_df.columns.difference(cols_normalize)].join(norm_train_df)\ntrain_df = join_df.reindex(columns = train_df.columns)\ntrain_df.head()","b2dc37c5":"# MinMax normalization (from 0 to 1)\ntest_df['cycle_norm'] = test_df['cycle']\nnorm_test_df = pd.DataFrame(min_max_scaler.transform(test_df[cols_normalize]), \n                            columns=cols_normalize, \n                            index=test_df.index)\ntest_join_df = test_df[test_df.columns.difference(cols_normalize)].join(norm_test_df)\ntest_df = test_join_df.reindex(columns = test_df.columns)\ntest_df = test_df.reset_index(drop=True)\n\n\n# We use the ground truth dataset to generate labels for the test data.\n# generate column max for test data\nrul = pd.DataFrame(test_df.groupby('id')['cycle'].max()).reset_index()\nrul.columns = ['id', 'max']\ntruth_df.columns = ['more']\ntruth_df['id'] = truth_df.index + 1\ntruth_df['max'] = rul['max'] + truth_df['more']\ntruth_df.drop('more', axis=1, inplace=True)\n\n# generate RUL for test data\ntest_df = test_df.merge(truth_df, on=['id'], how='left')\ntest_df['RUL'] = test_df['max'] - test_df['cycle']\ntest_df.drop('max', axis=1, inplace=True)\n\n# generate label columns w0 and w1 for test data\ntest_df['label1'] = np.where(test_df['RUL'] <= w1, 1, 0 )\ntest_df['label2'] = test_df['label1']\ntest_df.loc[test_df['RUL'] <= w0, 'label2'] = 2\ntest_df.head()","cc9364d6":"# pick a large window size of 50 cycles\nsequence_length = 50\n\ndef gen_sequence(id_df, seq_length, seq_cols):\n    \"\"\" Only sequences that meet the window-length are considered, no padding is used. This means for testing\n    we need to drop those which are below the window-length.\"\"\"\n    # for one id we put all the rows in a single matrix\n    data_matrix = id_df[seq_cols].values\n    num_elements = data_matrix.shape[0]\n    # Iterate over two lists in parallel.\n    # For example id1 have 192 rows and sequence_length is equal to 50\n    # so zip iterate over two following list of numbers (0,112),(50,192)\n    for start, stop in zip(range(0, num_elements-seq_length), range(seq_length, num_elements)):\n        yield data_matrix[start:stop, :]\n        \n# pick the feature columns \nsensor_cols = ['s' + str(i) for i in range(1,22)]\nsequence_cols = ['setting1', 'setting2', 'setting3', 'cycle_norm']\nsequence_cols.extend(sensor_cols)\n\n# generator for the sequences\nseq_gen = (list(gen_sequence(train_df[train_df['id']==id], sequence_length, sequence_cols)) \n           for id in train_df['id'].unique())\n\n# generate sequences and convert to numpy array\nseq_array = np.concatenate(list(seq_gen)).astype(np.float32)\nprint(\"seq_array shape =\",seq_array.shape)\nseq_array","993a8eea":"def gen_labels(id_df, seq_length, label):\n    \n    # For one id we put all the labels in a single matrix.\n    \n    data_matrix = id_df[label].values\n    num_elements = data_matrix.shape[0]\n\n    return data_matrix[seq_length:num_elements, :]\n\n# generate labels\nlabel_gen = [gen_labels(train_df[train_df['id']==id], sequence_length, ['label1']) \n             for id in train_df['id'].unique()]\nlabel_array = np.concatenate(label_gen).astype(np.float32)\nprint(\"label_array shape =\",label_array.shape)\nlabel_array","5d39854b":"# build the network\nnb_features = seq_array.shape[2]\nnb_out = label_array.shape[1]\n\nmodel = Sequential()\n\nmodel.add(LSTM(\n         input_shape=(sequence_length, nb_features),\n         units=100,\n         return_sequences=True))\nmodel.add(Dropout(0.2))\n\nmodel.add(LSTM(\n          units=50,\n          return_sequences=False))\nmodel.add(Dropout(0.2))\n\nmodel.add(Dense(units=nb_out, activation='sigmoid'))\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n\nprint(model.summary())\n\n# fit the network\nhistory = model.fit(seq_array, label_array, epochs=100, batch_size=200, validation_split=0.05, verbose=2,\n          callbacks = [keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=10, verbose=0, mode='min'),\n                       keras.callbacks.ModelCheckpoint(model_path,monitor='val_loss', save_best_only=True, mode='min', verbose=0)]\n          )\n\n# list all data in history\nprint(history.history.keys())","2388811e":"# summarize history for Accuracy\nfig_acc = plt.figure(figsize=(10, 10))\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('model accuracy (Higher is better)')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()\nfig_acc.savefig(\"model_accuracy.png\")\n\n# summarize history for Loss\nfig_acc = plt.figure(figsize=(10, 10))\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss (Lower is better)')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()\nfig_acc.savefig(\"model_loss.png\")\n\n# training metrics\nscores = model.evaluate(seq_array, label_array, verbose=1, batch_size=200)\nprint('Accurracy: {}'.format(scores[1]))\n\n# make predictions and compute confusion matrix\ny_pred = model.predict_classes(seq_array,verbose=1, batch_size=200)\ny_true = label_array\nprint(y_pred.shape)\nprint(\"\\nPrediction on Validation Set is: \", y_pred)\n\ntest_set = pd.DataFrame(y_pred)\ntest_set.to_csv('binary_submit_train.csv', index = None)\n\nprint('Confusion matrix\\n- x-axis is true labels.\\n- y-axis is predicted labels')\ncm = confusion_matrix(y_true, y_pred)\nprint(cm)\n\n# compute precision and recall\nprecision = precision_score(y_true, y_pred)\nrecall = recall_score(y_true, y_pred)\nprint( 'precision = ', precision, '\\n', 'recall = ', recall)","461d68f7":"# We pick the last sequence for each id in the test data\n\nseq_array_test_last = [test_df[test_df['id']==id][sequence_cols].values[-sequence_length:] \n                       for id in test_df['id'].unique() if len(test_df[test_df['id']==id]) >= sequence_length]\n\nseq_array_test_last = np.asarray(seq_array_test_last).astype(np.float32)\n#print(\"seq_array_test_last\")\n#print(seq_array_test_last)\n#print(seq_array_test_last.shape)\n\n# Similarly, we pick the labels\n\n# it is used to take only the labels of the sequences that are at least 50 long\ny_mask = [len(test_df[test_df['id']==id]) >= sequence_length for id in test_df['id'].unique()]\nprint(\"y_mask\")\nprint(len(y_mask))\nlabel_array_test_last = test_df.groupby('id')['label1'].nth(-1)[y_mask].values\nlabel_array_test_last = label_array_test_last.reshape(label_array_test_last.shape[0],1).astype(np.float32)\nprint(label_array_test_last.shape)\n#print(\"label_array_test_last\")\n#print(label_array_test_last)\n\n# if best iteration's model was saved then load and use it\nif os.path.isfile(model_path):\n    estimator = load_model(model_path)\n\n# test metrics\nscores_test = estimator.evaluate(seq_array_test_last, label_array_test_last, verbose=2)\nprint('Accurracy: {}'.format(scores_test[1]))\n\n# make predictions and compute confusion matrix\ny_pred_test = estimator.predict_classes(seq_array_test_last)\ny_true_test = label_array_test_last\nprint(y_pred.shape)\n\ntest_set = pd.DataFrame(y_pred_test)\ntest_set.to_csv('binary_submit_test.csv', index = None)\n\nprint('Confusion matrix\\n- x-axis is true labels.\\n- y-axis is predicted labels')\ncm = confusion_matrix(y_true_test, y_pred_test)\nprint(cm)\n\n# compute precision and recall\nprecision_test = precision_score(y_true_test, y_pred_test)\nrecall_test = recall_score(y_true_test, y_pred_test)\nf1_test = 2 * (precision_test * recall_test) \/ (precision_test + recall_test)\nprint( 'Precision: ', precision_test, '\\n', 'Recall: ', recall_test,'\\n', 'F1-score:', f1_test )\n\n# Plot in blue color the predicted data and in green color the\n# actual data to verify visually the accuracy of the model.\nfig_verify = plt.figure(figsize=(10, 5))\nplt.plot(y_pred_test, color=\"blue\")\nplt.plot(y_true_test, color=\"green\")\nplt.title('prediction')\nplt.ylabel('value')\nplt.xlabel('row')\nplt.legend(['predicted', 'actual data'], loc='upper left')\nplt.show()\nfig_verify.savefig(\"model_verify.png\")","62bcab61":"import keras\nimport keras.backend as K\nfrom keras.layers.core import Activation\nfrom keras.models import Sequential,load_model\nfrom keras.layers import Dense, Dropout, LSTM\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport os\nfrom sklearn import preprocessing\n\n\n# define path to save model\nmodel_path = 'regression_model.h5'","cfb6c84b":"# read training data - It is the aircraft engine run-to-failure data.\ntrain_df = pd.read_csv('..\/input\/pm-dataset\/PM_train.txt', sep=\" \", header=None)\ntrain_df.drop(train_df.columns[[26, 27]], axis=1, inplace=True)\ntrain_df.columns = ['id', 'cycle', 'setting1', 'setting2', 'setting3', 's1', 's2', 's3',\n                     's4', 's5', 's6', 's7', 's8', 's9', 's10', 's11', 's12', 's13', 's14',\n                     's15', 's16', 's17', 's18', 's19', 's20', 's21']\n\ntrain_df = train_df.sort_values(['id','cycle'])\n\n# read test data - It is the aircraft engine operating data without failure events recorded.\ntest_df = pd.read_csv('..\/input\/pm-dataset\/PM_test.txt', sep=\" \", header=None)\ntest_df.drop(test_df.columns[[26, 27]], axis=1, inplace=True)\ntest_df.columns = ['id', 'cycle', 'setting1', 'setting2', 'setting3', 's1', 's2', 's3',\n                     's4', 's5', 's6', 's7', 's8', 's9', 's10', 's11', 's12', 's13', 's14',\n                     's15', 's16', 's17', 's18', 's19', 's20', 's21']\n\n# read ground truth data - It contains the information of true remaining cycles for each engine in the testing data.\ntruth_df = pd.read_csv('..\/input\/pm-dataset\/PM_truth.txt', sep=\" \", header=None)\ntruth_df.drop(truth_df.columns[[1]], axis=1, inplace=True)","3f3f1925":"# Data Labeling - generate column RUL(Remaining Usefull Life or Time to Failure) for training data\nrul = pd.DataFrame(train_df.groupby('id')['cycle'].max()).reset_index()\nrul.columns = ['id', 'max']\ntrain_df = train_df.merge(rul, on=['id'], how='left')\ntrain_df['RUL'] = train_df['max'] - train_df['cycle']\ntrain_df.drop('max', axis=1, inplace=True)\n\n# generate label columns for training data\n# we will only make use of \"label1\" for binary classification, \n# while trying to answer the question: is a specific engine going to fail within w1 cycles?\nw1 = 30\nw0 = 15\ntrain_df['label1'] = np.where(train_df['RUL'] <= w1, 1, 0 )\ntrain_df['label2'] = train_df['label1']\ntrain_df.loc[train_df['RUL'] <= w0, 'label2'] = 2\n\n# MinMax normalization (from 0 to 1)\ntrain_df['cycle_norm'] = train_df['cycle']\ncols_normalize = train_df.columns.difference(['id','cycle','RUL','label1','label2'])\nmin_max_scaler = preprocessing.MinMaxScaler()\nnorm_train_df = pd.DataFrame(min_max_scaler.fit_transform(train_df[cols_normalize]), \n                             columns=cols_normalize, \n                             index=train_df.index)\njoin_df = train_df[train_df.columns.difference(cols_normalize)].join(norm_train_df)\ntrain_df = join_df.reindex(columns = train_df.columns)\n\n\n# Next to testing data\n# MinMax normalization (from 0 to 1)\ntest_df['cycle_norm'] = test_df['cycle']\nnorm_test_df = pd.DataFrame(min_max_scaler.transform(test_df[cols_normalize]), \n                            columns=cols_normalize, \n                            index=test_df.index)\ntest_join_df = test_df[test_df.columns.difference(cols_normalize)].join(norm_test_df)\ntest_df = test_join_df.reindex(columns = test_df.columns)\ntest_df = test_df.reset_index(drop=True)\nprint(test_df.head())\n\n# We use the ground truth dataset to generate labels for the test data.\n# generate column max for test data\nrul = pd.DataFrame(test_df.groupby('id')['cycle'].max()).reset_index()\nrul.columns = ['id', 'max']\ntruth_df.columns = ['more']\ntruth_df['id'] = truth_df.index + 1\ntruth_df['max'] = rul['max'] + truth_df['more']\ntruth_df.drop('more', axis=1, inplace=True)\n\n# generate RUL for test data\ntest_df = test_df.merge(truth_df, on=['id'], how='left')\ntest_df['RUL'] = test_df['max'] - test_df['cycle']\ntest_df.drop('max', axis=1, inplace=True)\n\n# generate label columns w0 and w1 for test data\ntest_df['label1'] = np.where(test_df['RUL'] <= w1, 1, 0 )\ntest_df['label2'] = test_df['label1']\ntest_df.loc[test_df['RUL'] <= w0, 'label2'] = 2\n\n\n","c663bd32":"# pick a large window size of 50 cycles\nsequence_length = 50\n\n# function to reshape features into (samples, time steps, features) \ndef gen_sequence(id_df, seq_length, seq_cols):\n    \"\"\" Only sequences that meet the window-length are considered, no padding is used. This means for testing\n    we need to drop those which are below the window-length.\"\"\"\n    # for one id I put all the rows in a single matrix\n    data_matrix = id_df[seq_cols].values\n    num_elements = data_matrix.shape[0]\n    # Iterate over two lists in parallel.\n    # For example id1 have 192 rows and sequence_length is equal to 50\n    # so zip iterate over two following list of numbers (0,142),(50,192)\n\n    for start, stop in zip(range(0, num_elements-seq_length), range(seq_length, num_elements)):\n        yield data_matrix[start:stop, :]\n        \n# pick the feature columns \nsensor_cols = ['s' + str(i) for i in range(1,22)]\nsequence_cols = ['setting1', 'setting2', 'setting3', 'cycle_norm']\nsequence_cols.extend(sensor_cols)\n\n\n# val is a list of 192 - 50 = 142 bi-dimensional array (50 rows x 25 columns)\nval=list(gen_sequence(train_df[train_df['id']==1], sequence_length, sequence_cols))\nprint(len(val))\n\n# generator for the sequences\n# transform each id of the train dataset in a sequence\nseq_gen = (list(gen_sequence(train_df[train_df['id']==id], sequence_length, sequence_cols)) \n           for id in train_df['id'].unique())\n\n# generate sequences and convert to numpy array\nseq_array = np.concatenate(list(seq_gen)).astype(np.float32)\nprint(seq_array.shape)\n\n# function to generate labels\ndef gen_labels(id_df, seq_length, label):\n\n    data_matrix = id_df[label].values\n    num_elements = data_matrix.shape[0]\n\n    return data_matrix[seq_length:num_elements, :]\n\n# generate labels\nlabel_gen = [gen_labels(train_df[train_df['id']==id], sequence_length, ['RUL']) \n             for id in train_df['id'].unique()]\n\nlabel_array = np.concatenate(label_gen).astype(np.float32)\nlabel_array.shape","b9a7d5a1":"def r2_keras(y_true, y_pred):\n    \"\"\"Coefficient of Determination \n    \"\"\"\n    SS_res =  K.sum(K.square( y_true - y_pred ))\n    SS_tot = K.sum(K.square( y_true - K.mean(y_true) ) )\n    return ( 1 - SS_res\/(SS_tot + K.epsilon()) )\n\n# Next, we build a deep network. \n# The first layer is an LSTM layer with 100 units followed by another LSTM layer with 50 units. \n# Dropout is also applied after each LSTM layer to control overfitting. \n# Final layer is a Dense output layer with single unit and linear activation since this is a regression problem.\nnb_features = seq_array.shape[2]\nnb_out = label_array.shape[1]\n\nmodel = Sequential()\nmodel.add(LSTM(\n         input_shape=(sequence_length, nb_features),\n         units=100,\n         return_sequences=True))\nmodel.add(Dropout(0.2))\nmodel.add(LSTM(\n          units=50,\n          return_sequences=False))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(units=nb_out))\nmodel.add(Activation(\"linear\"))\nmodel.compile(loss='mean_squared_error', optimizer='rmsprop',metrics=['mae',r2_keras])\n\nprint(model.summary())\n\n# fit the network\nhistory = model.fit(seq_array, label_array, epochs=100, batch_size=200, validation_split=0.05, verbose=2,\n          callbacks = [keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=10, verbose=0, mode='min'),\n                       keras.callbacks.ModelCheckpoint(model_path,monitor='val_loss', save_best_only=True, mode='min', verbose=0)]\n          )\n\n# list all data in history\nprint(history.history.keys())","f3e96efe":"# summarize history for R^2\nfig_acc = plt.figure(figsize=(10, 10))\nplt.plot(history.history['r2_keras'])\nplt.plot(history.history['val_r2_keras'])\nplt.title('model r^2')\nplt.ylabel('R^2')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()\nfig_acc.savefig(\"model_r2.png\")\n\n# summarize history for MAE\nfig_acc = plt.figure(figsize=(10, 10))\nplt.plot(history.history['mae'])\nplt.plot(history.history['val_mae'])\nplt.title('model MAE')\nplt.ylabel('MAE')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()\nfig_acc.savefig(\"model_mae.png\")\n\n# summarize history for Loss\nfig_acc = plt.figure(figsize=(10, 10))\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()\nfig_acc.savefig(\"model_regression_loss.png\")\n\n# training metrics\nscores = model.evaluate(seq_array, label_array, verbose=1, batch_size=200)\nprint('\\nMAE: {}'.format(scores[1]))\nprint('\\nR^2: {}'.format(scores[2]))\n\ny_pred = model.predict(seq_array,verbose=1, batch_size=200)\ny_true = label_array\n\ntest_set = pd.DataFrame(y_pred)\ntest_set.to_csv('submit_train.csv', index = None)","1799d24f":"# We pick the last sequence for each id in the test data\nseq_array_test_last = [test_df[test_df['id']==id][sequence_cols].values[-sequence_length:] \n                       for id in test_df['id'].unique() if len(test_df[test_df['id']==id]) >= sequence_length]\n\nseq_array_test_last = np.asarray(seq_array_test_last).astype(np.float32)\n#print(\"seq_array_test_last\")\n#print(seq_array_test_last)\n#print(seq_array_test_last.shape)\n\n# Similarly, we pick the labels\n#print(\"y_mask\")\ny_mask = [len(test_df[test_df['id']==id]) >= sequence_length for id in test_df['id'].unique()]\nlabel_array_test_last = test_df.groupby('id')['RUL'].nth(-1)[y_mask].values\nlabel_array_test_last = label_array_test_last.reshape(label_array_test_last.shape[0],1).astype(np.float32)\n#print(label_array_test_last.shape)\n#print(\"label_array_test_last\")\n#print(label_array_test_last)\n\n# if best iteration's model was saved then load and use it\nif os.path.isfile(model_path):\n    estimator = load_model(model_path,custom_objects={'r2_keras': r2_keras})\n\n    # test metrics\n    scores_test = estimator.evaluate(seq_array_test_last, label_array_test_last, verbose=2)\n    print('\\nMAE: {}'.format(scores_test[1]))\n    print('\\nR^2: {}'.format(scores_test[2]))\n\n    y_pred_test = estimator.predict(seq_array_test_last)\n    y_true_test = label_array_test_last\n\n    test_set = pd.DataFrame(y_pred_test)\n    test_set.to_csv('submit_test.csv', index = None)\n    test_set= test_set.apply(np.ceil)\n\n    # Plot in blue color the predicted data and in green color the\n    # actual data to verify visually the accuracy of the model.\n    fig_verify = plt.figure(figsize=(10, 5))\n    plt.plot(y_pred_test, color=\"blue\")\n    plt.plot(y_true_test, color=\"green\")\n    plt.title('prediction')\n    plt.ylabel('value')\n    plt.xlabel('row')\n    plt.legend(['predicted', 'actual data'], loc='upper left')\n    plt.show()\n    fig_verify.savefig(\"model_regression_verify.png\")","22f08b88":"## Model Evaluation on Validation set","e1770a63":"**Read test data - It is the aircraft engine operating data without failure events recorded**","3071bc26":"## Data Ingestion\nIn the following section, we ingest the training, test and ground truth datasets from azure storage. The training data consists of multiple multivariate time series with \"cycle\" as the time unit, together with 21 sensor readings for each cycle. Each time series can be assumed as being generated from a different engine of the same type. The testing data has the same data schema as the training data. The only difference is that the data does not indicate when the failure occurs. Finally, the ground truth data provides the number of remaining working cycles for the engines in the testing data.","4c6c8c95":"## Data Preprocessing","1248db50":"<img src=\"https:\/\/img2.cgtrader.com\/items\/2153687\/4fb4f455c0\/pw-gtf-geared-turbofan-engine-3d-model-max-obj-mtl-3ds-c4d-lwo-lw-lws-ma-mb.jpg\" width=\"900px\">","45decbdd":"**Function to generate labels**","fa62ae3c":"## Evaluate on Test set","fab9cad4":"# Regression\nHow many more cycles an in-service engine will last before it fails?","be0a30be":"# Predictive Maintenance using LSTM\n\nDeep learning has proven to show superior performance in certain domains such as object recognition and image classification. It has also gained popularity in domains such as finance where time-series data plays an important role. Predictive Maintenance is also a domain where data is collected over time to monitor the state of an asset with the goal of finding patterns to predict failures which can also benefit from certain deep learning algorithms. Among the deep learning methods, Long Short Term Memory (LSTM) networks are especially appealing to the predictive maintenance domain due to the fact that they are very good at learning from sequences. This fact lends itself to their applications using time series data by making it possible to look back for longer periods of time to detect failure patterns. In this notebook, we build an LSTM network for the data set and scenerio described at Predictive Maintenance Template to predict remaining useful life of aircraft engines. In summary, the template uses simulated aircraft sensor values to predict when an aircraft engine will fail in the future so that maintenance can be planned in advance.\n","eb098fdc":"## Model Evaluation on Test set","06070431":"**Importing libraries**","fb1ec4af":"# Binary classification\nPredict if an asset will fail within certain time frame (e.g. cycles)","061e34ca":"![](https:\/\/storage.googleapis.com\/kagglesdsdata\/datasets%2F675628%2F1187995%2Ffeatures.PNG?GoogleAccessId=databundle-worker-v2@kaggle-161607.iam.gserviceaccount.com&Expires=1590650972&Signature=UTbUJ0kbk4z48slf6OnG3W5M0tbhrs3OhQDd%2BAZeWliS9B7QQFaRfDb3FB5NqiV0CQysvoJLXeBLuq4Ho5Ox1kXswON2duOlyQDHq9%2Feoz3aHDK3ykkOupydexrY3yc749keKgv47o6TM9CXT7pERyQ14KcuXGTMHM3XDGdzLSM480J8bPuU7U36X4zmOFELW1ctaMXqFJlgXZ5pttIV1aFiWKNLeDjaxokI46kzvF%2BMD%2FTvPRSrojaZ45sVfVYM5nvyT4qMpH2vWBe%2B5f6sUn1JKdrnW0gaI1zIAD5L0Vso%2FOMuXqJL5InY6MK6exyog7go62n%2B7zOMbZQyZ3uUSg%3D%3D)","e2ea3ab2":"## Model Evaluation on Validation set","afb543e6":"## LSTM\n\nThe coefficient of determination R2 can describe how \u201cgood\u201d a model is at making predictions: it represents the proportion of the variance in the dependent variable that is predictable from the independent\n\nmean absolute error (MAE) is a measure of errors between paired observations expressing the same phenomenon\n\nLoss function Mean squared error (MSE) is the most commonly used loss function for regression. The loss is the mean overseen data of the squared differences between true and predicted values.\n\nRmsProp is an optimizer that utilizes the magnitude of recent gradients to normalize the gradients. We always keep a moving average over the root mean squared (hence Rms) gradients, by which we divide the current gradient. \n\n","9f16f5cc":" **Read training data - It is the aircraft engine run-to-failure data**","34f8eaa1":"## Data Ingestion","6e9aeace":"# References\n\n\n- [1] Deep Learning for Predictive Maintenance https:\/\/github.com\/Azure\/lstms_for_predictive_maintenance\/blob\/master\/Deep%20Learning%20Basics%20for%20Predictive%20Maintenance.ipynb\n- [2] Predictive Maintenance: Step 2A of 3, train and evaluate regression models https:\/\/gallery.cortanaintelligence.com\/Experiment\/Predictive-Maintenance-Step-2A-of-3-train-and-evaluate-regression-models-2\n- [3] A. Saxena and K. Goebel (2008). \"Turbofan Engine Degradation Simulation Data Set\", NASA Ames Prognostics Data Repository (https:\/\/ti.arc.nasa.gov\/tech\/dash\/groups\/pcoe\/prognostic-data-repository\/#turbofan), NASA Ames Research Center, Moffett Field, CA \n- [4] Understanding LSTM Networks http:\/\/colah.github.io\/posts\/2015-08-Understanding-LSTMs\/\n\n","27809ade":"**Read ground truth data - It contains the information of true remaining cycles for each engine in the testing data**","08a1cbd0":"**Data Labeling - generate column RUL(Remaining Usefull Life or Time to Failure) for training data**","35435ea2":"**Next for testing data**","86fcd86c":"## Describing the dataset\nThe dataset consists of sensor readings from a fleet of simulated aircraft gas turbine engines operating conditions as a multiple multivariate time series. The dataset consists of separate training and test sets. The testset is similar to the training set, except that each engine\u2019s measurements are truncated some (unknown) amount of time before it fails. The data is provided as a ZIP-compressed text file with 26 columns of numbers. Each row represents a snapshot of data taken during a single operational cycle and each column represents a different variable.","c5d37d82":"**Function to reshape features into (samples, time steps, features)**","47a526a1":"**Next, we build a deep network.**\n\n**The first layer is an LSTM layer with 100 units followed by another LSTM layer with 50 units.** \n\n**Dropout is also applied after each LSTM layer to control overfitting.** \n\n**Final layer is a Dense output layer with single unit and sigmoid activation since this is a binary classification problem.**\n\nStacking LSTM hidden layers makes the model deeper, more accurately earning the description as a deep learning technique. \nThe additional hidden layers are understood to recombine the learned representation from prior layers and create new representations at high levels of abstraction. \n\nAdam is a replacement optimization algorithm for stochastic gradient descent for training deep learning models. Adam combines the best properties of the AdaGrad and RMSProp algorithms to provide an optimization algorithm that can handle sparse gradients on noisy problems.\n\nThe loss function binary crossentropy is used on yes\/no decisions, e.g., multi-label classification. The loss tells you how wrong your model\u2019s predictions are.\n","34bed1f3":"## LSTM","d0dadd77":"## Data Preprocessing"}}