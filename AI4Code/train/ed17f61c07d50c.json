{"cell_type":{"6bf7997d":"code","9da9426c":"code","620f13e5":"code","ac2a41a8":"code","10b05a6f":"code","2098371b":"code","526d807e":"code","fc524014":"code","6bfa9e1f":"code","9c266224":"code","9e5b6bc4":"code","7416b543":"code","efe55449":"code","d2f57d97":"code","31e39d18":"code","d9469a22":"code","86ed0087":"code","b140a621":"code","fdb05bff":"code","2d98ec39":"code","0bb5a91b":"code","e8c5ab29":"code","49923186":"code","d00a2f4c":"code","1afe9f05":"code","983dd443":"code","cfc8fa4d":"code","439287ea":"code","5d5eb9d7":"code","11ea2097":"code","0b854215":"code","12e60f98":"code","83eb3907":"code","ce0bced0":"code","19395139":"code","bf5f3805":"markdown","5380657b":"markdown","ce9d4823":"markdown"},"source":{"6bf7997d":"import re\nimport nltk\nimport string\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","9da9426c":"test = pd.read_csv('\/kaggle\/input\/test.csv')\ntrain = pd.read_csv('\/kaggle\/input\/train.csv')\ntest1 = pd.read_csv('\/kaggle\/input\/test.csv')","620f13e5":"train.head()","ac2a41a8":"train = train[['original_text', 'sentiment_class']]\ntest = test[['original_text']]","10b05a6f":"train['sentiment_class'].value_counts()","2098371b":"str_len_test = test[test['original_text'].str.len() < 700]['original_text'].str.len()\nstr_len_train = train[train['original_text'].str.len() < 700]['original_text'].str.len()\n\nplt.hist(str_len_train, bins=100, label=\"Train Text\")\nplt.hist(str_len_test, bins=100, label=\"Test Text\")\nplt.legend()\nplt.show()","526d807e":"def clean_summary(text):\n    text = re.sub(\"\\'\", \"\", text)\n    text = re.sub(\"[^a-zA-Z#]\",\" \",text)\n    text = re.sub(\"http\\S+|www.\\S+\",\"\", text)\n    text = re.sub(\"igshid\",\"\", text)\n    text = re.sub(\"instagram\",\"\", text)\n    text = re.sub(\"twitter\",\"\", text)\n    text = re.sub(\"mothersday\",\"\", text)\n    text = re.sub(\"happymothersday\",\"\", text)\n    text = re.sub(\"motheringsubday\",\"\", text)\n    text = re.sub(\"happy\",\"\", text)\n    text = ' '.join(text.split())\n    text = text.lower()\n    text = ' '.join([w for w in text.split() if len(w)>3])\n    words = text.split()\n    text = \" \".join(sorted(set(words), key=words.index))\n    return text","fc524014":"train['clean_text'] = train['original_text'].apply(lambda x: clean_summary(x))\ntrain.head(2)","6bfa9e1f":"from nltk.corpus import stopwords\nstop_words = set(stopwords.words('english'))\ndef remove_stopwords(text):\n    no_stopword_text = [w for w in text.split() if not w in stop_words]\n    return ' '.join(no_stopword_text)\n\nnltk.download('words')\nwords = set(nltk.corpus.words.words())\n\ndef remove_nonenglish(text):\n    return \" \".join(w for w in nltk.wordpunct_tokenize(text) if w.lower() in words or not w.isalpha())\n    \ntrain['clean_text'] = train['clean_text'].apply(lambda x: remove_stopwords(x))\n\ntrain.head()","9c266224":"test['clean_text'] = test['original_text'].apply(lambda x: clean_summary(x))\ntest['clean_text'] = test['clean_text'].apply(lambda x: remove_stopwords(x))\ntest.head()","9e5b6bc4":"combine = train.append(test)","7416b543":"from wordcloud import WordCloud","efe55449":"all_words = ' '.join([text for text in combine['clean_text']])\nwordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(all_words)\nplt.figure(figsize=(10, 7))\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis('off')\nplt.show()","d2f57d97":"not_sarcastic =' '.join([text for text in combine['clean_text'][combine['sentiment_class'] == 0]])\nwordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(not_sarcastic)\nplt.figure(figsize=(10, 7))\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis('off')\nplt.show()","31e39d18":"not_sarcastic =' '.join([text for text in combine['clean_text'][combine['sentiment_class'] == 1]])\nwordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(not_sarcastic)\nplt.figure(figsize=(10, 7))\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis('off')\nplt.show()","d9469a22":"not_sarcastic =' '.join([text for text in combine['clean_text'][combine['sentiment_class'] == -1]])\nwordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(not_sarcastic)\nplt.figure(figsize=(10, 7))\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis('off')\nplt.show()","86ed0087":"def hashtag_extract(x):\n    hashtags = []\n    for i in x:\n        ht = re.findall(r\"#(\\w+)\", i)\n        hashtags.append(ht)\n    return hashtags","b140a621":"HT_positive = hashtag_extract(combine['clean_text'][combine['sentiment_class'] == 0])\nHT_negative = hashtag_extract(combine['clean_text'][combine['sentiment_class'] == 1])\nHT_neutral = hashtag_extract(combine['clean_text'][combine['sentiment_class'] == -1])","fdb05bff":"HT_positive = sum(HT_positive,[])\nHT_negative = sum(HT_negative,[])\nHT_neutral = sum(HT_neutral,[])","2d98ec39":"a = nltk.FreqDist(HT_positive)\nd = pd.DataFrame({'Hashtag': list(a.keys()),'Count': list(a.values())})\nd = d.nlargest(columns='Count', n=10)\nplt.figure(figsize=(16,5))\nax = sns.barplot(data=d, x= \"Hashtag\", y = \"Count\")\nax.set(ylabel = 'Count')\nplt.show()","0bb5a91b":"a = nltk.FreqDist(HT_negative)\nd = pd.DataFrame({'Hashtag': list(a.keys()),'Count': list(a.values())})\nd = d.nlargest(columns='Count', n=10)\nplt.figure(figsize=(16,5))\nax = sns.barplot(data=d, x= \"Hashtag\", y = \"Count\")\nax.set(ylabel = 'Count')\nplt.show()","e8c5ab29":"a = nltk.FreqDist(HT_neutral)\nd = pd.DataFrame({'Hashtag': list(a.keys()),'Count': list(a.values())})\nd = d.nlargest(columns='Count', n=10)\nplt.figure(figsize=(16,5))\nax = sns.barplot(data=d, x= \"Hashtag\", y = \"Count\")\nax.set(ylabel = 'Count')\nplt.show()","49923186":"def clean_summary_modelling(text):\n    text = re.sub(\"\\'\", \"\", text)\n    text = re.sub(\"[^a-zA-Z]\",\" \",text)\n    text = re.sub(\"http\\S+|www.\\S+\",\"\", text)\n    text = re.sub(\"igshid\",\"\", text)\n    text = re.sub(\"instagram\",\"\", text)\n    text = re.sub(\"twitter\",\"\", text)\n    text = re.sub(\"mothersday\",\"\", text)\n    text = re.sub(\"happymothersday\",\"\", text)\n    text = re.sub(\"motheringsubday\",\"\", text)\n    text = ' '.join(text.split())\n    text = text.lower()\n    text = ' '.join([w for w in text.split() if len(w)>3])\n    words = text.split()\n    text = \" \".join(sorted(set(words), key=words.index))\n    return text","d00a2f4c":"train['clean_text'] = train['original_text'].apply(lambda x: clean_summary_modelling(x))\ntrain['clean_text'] = train['clean_text'].apply(lambda x: remove_stopwords(x))\ntrain['clean_text'] = train['clean_text'].apply(lambda x: remove_nonenglish(x))\n\ntrain.head()","1afe9f05":"test['clean_text'] = test['original_text'].apply(lambda x: clean_summary_modelling(x))\ntest['clean_text'] = test['clean_text'].apply(lambda x: remove_stopwords(x))\ntest['clean_text'] = test['clean_text'].apply(lambda x: remove_nonenglish(x))\ntest.head()","983dd443":"X = train.drop(['sentiment_class','original_text'], axis=1)\ny = train['sentiment_class']\n\nX_test = test.drop(['original_text'], axis=1)\nX.shape, y.shape, X_test.shape","cfc8fa4d":"from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier","439287ea":"xtrain, xval, ytrain, yval = train_test_split(X, y, test_size=0.2, random_state=10)","5d5eb9d7":"tfidf_vectorizer = TfidfVectorizer(max_df=0.46, min_df=1, max_features=3000)\nx_train = tfidf_vectorizer.fit_transform(xtrain['clean_text'])\nx_val = tfidf_vectorizer.transform(xval['clean_text'])","11ea2097":"classifier = LogisticRegression()\n\nclassifier.fit(x_train, ytrain)","0b854215":"from sklearn.metrics import f1_score, accuracy_score, confusion_matrix\npred = classifier.predict(x_val)\n100 * f1_score(yval, pred, average='weighted'), accuracy_score(yval, pred)","12e60f98":"confusion_matrix(yval, pred)","83eb3907":"test_x = tfidf_vectorizer.transform(X_test['clean_text'])\npred_test = classifier.predict(test_x)","ce0bced0":"submission = pd.DataFrame()\nsubmission['id'] = test1['id']\nsubmission['sentiment_class'] = pred_test\nsubmission['sentiment_class'].value_counts()","19395139":"submission.to_csv('Submission.csv', index=False)","bf5f3805":"You work in an event management company. On Mother's Day, your company has organized an event where they want to cast positive Mother's Day related tweets in a presentation. Data engineers have already collected the data related to Mother's Day that must be categorized into positive, negative, and neutral tweets.\n\nYou are appointed as a Machine Learning Engineer for this project. Your task is to build a model that helps the company classify these sentiments of the tweets into positive, negative, and neutral.","5380657b":"# Problem Statement","ce9d4823":"Github Repo: https:\/\/github.com\/bilalProgTech\/online-data-science-ml-challenges\/tree\/master\/HackerEarth-Mothers-Day-ML-Challenge"}}