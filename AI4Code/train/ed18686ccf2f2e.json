{"cell_type":{"19b24417":"code","fab833b4":"code","0b73b09b":"code","0b036185":"code","5b54d0dd":"code","9efc0ea7":"code","0280e8b4":"code","702e3278":"code","455bd4db":"code","73862624":"code","e4ed1a51":"markdown","758720f4":"markdown","3d8bf9c6":"markdown","6928824f":"markdown","1aa5934b":"markdown","e7248621":"markdown","a6d379a1":"markdown","f65e9413":"markdown","546a077e":"markdown"},"source":{"19b24417":"import numpy as np","fab833b4":"# Generating our data\nnp.random.seed(1)\nX = np.random.rand(3, 10)","0b73b09b":"# The data has 3 records each having 10 dimensions\nprint(f\"Data:\\n {X} \\n Shape: {X.shape}\")","0b036185":"# mean centering the data\n# i.e., removing mean, and centering all data around the mean\nX_demeaned = X - np.mean(X, 0)\n\nprint(f\"Mean Normalized:\\n {X_demeaned} \\n Shape: {X_demeaned.shape}\")","5b54d0dd":"# Computing covariance matrix of the demeaned data\ncov = np.cov(X_demeaned, rowvar=False)\n\nprint(f\"Covariance matrix:\\n {cov} \\n Shape: {cov.shape}\")","9efc0ea7":"# Computing eigenvalues and eigenvectors\neigenvals, eigenvecs = np.linalg.eigh(cov)\n\nprint(f\"Eigenvals: {eigenvals} \\n\\n Eigenvecs:\\n {eigenvecs} \\n\\n Eigenvecs Shape: {eigenvecs.shape}\")","0280e8b4":"# sort eigenvalue in increasing order (get the indices from the sort)\nidx_sorted = np.argsort(eigenvals)\n\n# sorting indices in decreasing order\nidx_sorted_decreasing = idx_sorted[::-1]\n\n# Sorting eigenvectors and eigenvalues\neigenvals_sorted = eigenvals[idx_sorted_decreasing]\n\neigenvecs_sorted = eigenvecs[:,idx_sorted_decreasing]\n\n# Selecting only k number of features (first k number of eigenvecs_sorted)\n# Here, taking k=2\nn_components = 2\nfeature_vector = eigenvecs_sorted[:,0:n_components]\n\nprint(f\"Feature vector:\\n {feature_vector} \\n Feature vector's shape: {feature_vector.shape}\")","702e3278":"# Printing the percentage of variance explained by the first two components\n\nprint(f\"Variance explained by 1st Principal Component: {eigenvals_sorted[0]\/eigenvals_sorted.sum()}\")\nprint(f\"Variance explained by 2nd Principal Component: {eigenvals_sorted[1]\/eigenvals_sorted.sum()}\")","455bd4db":"X_reduced = np.dot(feature_vector.T, X_demeaned.T).T\n\nprint(X_reduced)","73862624":"print(f\"\"\"Shape of original data: {X.shape}\nShape or reduced data: {X_reduced.shape}\"\"\")","e4ed1a51":"From above, we can see that our data has 3 records and 10 dimensions. In this notebook, we will use PCA to reduce the dimensions of our data from 10 dimensions to 3 dimensions.","758720f4":"### Understanding data","3d8bf9c6":"Thus, we can observe that the dimensions of the 3 data points have been reduced from 10 dimensions to 2 dimensions.","6928824f":"### 3. Eigenvals and eigenvecs","1aa5934b":"### 2. Covariance matrix","e7248621":"### 1. Mean Normalizing","a6d379a1":"<center><font color='red' size=\"8\">Thankyou!<\/font><\/center>","f65e9413":"### 4. Constructing Feature Vectors","546a077e":"### 5. Project mean normalized data to principal components\n\n\\begin{align}\nX_{reduced} =  (FeatureVector^T \\cdot X_{demeaned}^T)^T\n\\end{align}"}}