{"cell_type":{"573531fc":"code","51ae9bf8":"code","a67005f7":"code","2aee7bb9":"code","f7832f77":"code","c8004d26":"code","7d4a6e2e":"code","66302591":"code","aeb1cf50":"code","8230aaf1":"code","1ad9a2a0":"code","2c2082be":"code","216c89ec":"code","2de129eb":"code","ffa89e5a":"code","cc0d2432":"code","baad5a34":"code","ea01d401":"code","27a1d919":"code","127a5c1a":"code","99332217":"code","ee2f84e5":"code","7dbdccd7":"code","b8f6d3b7":"markdown","b46d4e67":"markdown","28fe0bd8":"markdown","a585a381":"markdown","d3776440":"markdown","c1a09173":"markdown","48189536":"markdown","b443ac7c":"markdown","b0489712":"markdown","792985f7":"markdown","588a6664":"markdown","dc4c5785":"markdown","cbd21c4e":"markdown","3d643de0":"markdown"},"source":{"573531fc":"## Loading the packages \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import GridSearchCV","51ae9bf8":"## Import and Explore the data \ndf = pd.read_csv(\"..\/input\/breast-cancer.csv\")","a67005f7":"df = df.drop(['id'], axis=1)","2aee7bb9":"# A list comprehension to code the positive label as 1 and the negative label as 0\ndf['diagnosis'] = [1 if df['diagnosis'][i] =='M' else 0 for i in np.arange(0,len(df['diagnosis']),1)]","f7832f77":"# Columns names list\ndf.columns","c8004d26":"# DataFrame shape\ndf.shape","7d4a6e2e":"# Statistical Descriptive of df DataFrame\ndf.describe()","66302591":"# df's variable type\ndf.dtypes","aeb1cf50":"# Detect missing values\nprint(df.isnull().sum())","8230aaf1":"# Convert the 'diagnosis' target variable to categorical data \ndf['diagnosis'] = df.diagnosis.astype('category')\nassert df['diagnosis'].dtype=='category' ## if the 'diagnosis' variable wasn't converted to categorical, the assert function return an error","1ad9a2a0":"# Split the data into train and test\nX = df[df.columns[1:]] ## Features\nY = df[df.columns[0]]  ## target\nX_train, X_test, Y_train, Y_test=train_test_split(X, Y, test_size=0.3, random_state=42, stratify=Y)","2c2082be":"#-----------------------------------------\n# Creat a KNN model and predictions Model1\n#-----------------------------------------\nmodel_knn = KNeighborsClassifier(n_neighbors=4)\nmodel_knn.fit(X_train, Y_train)\n\n# predict using the knn model\nY_pred = model_knn.predict(X_test)\n\n# Print the classification metrics\nprint(confusion_matrix(Y_test, Y_pred))\nprint(classification_report(Y_test, Y_pred))","216c89ec":"# Plot the ROC curve \nY_pred_prob = model_knn.predict_proba(X_test)[:,1]\nfpr, tpr, thresholds = roc_curve(Y_test, Y_pred_prob)\nplt.plot([0,1], [0,1], 'r--')\nplt.plot(fpr, tpr, label='KNN ROC curve model1')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('K Nearest Neighbors for knn')\nplt.show()\n\n# calculate the AUC and accuracy \nacc_model1 = model_knn.score(X_test, Y_test)\nauc_model1 = roc_auc_score(Y_test, Y_pred_prob)\nprint('model 1: the AUC is equal to :{}'.format(auc_model1))\nprint('model 1: the ACCURACY is equal to : {}'.format(acc_model1))","2de129eb":"#--------------------------------------------------------\n# Overfitting and underfitting in function of k : Model 2\n#--------------------------------------------------------\nneighbor = np.arange(4,15) # range of value of K we test\ntrain_accuracy = np.empty(len(neighbor)) # A list of multiple train's accuracies\ntest_accuracy = np.empty(len(neighbor)) # A list of multiple test's accuracies\nfor ind, k in enumerate(neighbor):\n    mod_knn = KNeighborsClassifier(n_neighbors=k)\n    mod_knn.fit(X_train, Y_train)\n    train_accuracy[ind] = mod_knn.score(X_train, Y_train)\n    test_accuracy[ind] = mod_knn.score(X_test, Y_test)\n    \n# Compare the accuracy in function of multiple value of k\nplt.figure()\nplt.plot(neighbor, train_accuracy, label='training accuracy')\nplt.plot(neighbor, test_accuracy, label='testing accuracy')\nplt.legend()\nplt.xlabel('Number of Neighbors')\nplt.ylabel('Accuracy')\nplt.show()","ffa89e5a":"# Get the best model and it evaluation\nk0 = np.argmax(test_accuracy) + 4\nb_knn = KNeighborsClassifier(n_neighbors=k0)\nb_knn.fit(X_train, Y_train)\n# predict using the b_knn model\nbY_pred = b_knn.predict(X_test)\n# Print the classification metrics\nprint(confusion_matrix(Y_test, bY_pred))\nprint(classification_report(Y_test, bY_pred))","cc0d2432":"# ROC curve for the b_knn\nbY_pred_prob = b_knn.predict_proba(X_test)[:,1]\nfpr, tpr, threshold = roc_curve(Y_test, bY_pred_prob, pos_label=1) \nplt.plot([0,1], [0,1], 'k--')\nplt.plot(fpr, tpr, label = 'KNN-ROC Curve model 2')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('K Nearest Neighbors for b_knn')\nplt.show()\n\n# calculate the AUC and accuracy \nacc_model2 = b_knn.score(X_test, Y_test)\nauc_model2 = roc_auc_score(Y_test, bY_pred_prob)\nprint('model 2 : the AUC is equal to: {}'.format(auc_model2))\nprint('model 2: the ACCURACY is equal to: {}'.format(acc_model2))","baad5a34":"# ---------------------------------------\n# Tune the parameter K of the KNN Model 3\n# ---------------------------------------\nparam_grid = {'n_neighbors': np.arange(1,30)}\nknn = KNeighborsClassifier()\nGrid_knn = GridSearchCV(knn, param_grid, scoring='roc_auc', cv=5)\nGrid_knn.fit(X_train, Y_train)\n\n# predict using the tunde model\ntY_pred = Grid_knn.predict(X_test)\n\n# Print the classification metrics\nprint(confusion_matrix(Y_test, tY_pred))\nprint(classification_report(Y_test, tY_pred))","ea01d401":"# ROC curve for the b_knn\ntY_pred_prob = Grid_knn.predict_proba(X_test)[:,1]\nfpr, tpr, threshold = roc_curve(Y_test, tY_pred_prob)\nplt.plot([0,1], [0,1], 'k--')\nplt.plot(fpr, tpr, label = 'KNN-ROC Curve model 3')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('K Nearest Neighbors for Grid_knn')\nplt.show()\n\n# calculate the AUC and accuracy \nacc_model3 = Grid_knn.score(X_test, Y_test)\nauc_model3 = roc_auc_score(Y_test, tY_pred_prob)\nprint('model 3 : the AUC is equal to: {}'.format(auc_model3))\nprint('model 3: the ACCURACY is equal to: {}'.format(acc_model3))\n\n# Get the best parameters for KNN model:\nprint(Grid_knn.best_params_)\nprint(Grid_knn.best_score_)","27a1d919":"# -----------------------------------------------------------------------------------\n# Let's tune our KNN model using a grid search to find best parameters to model with: model4\n# -----------------------------------------------------------------------------------\nfrom sklearn.model_selection import RandomizedSearchCV\nparams = {'n_neighbors': np.arange(4,30,1),\n          'weights':['uniform', 'distance'],\n          'algorithm':['ball_tree', 'kd_tree'],\n          'metric':['euclidean']}\nknn = KNeighborsClassifier()\nRando_knn = RandomizedSearchCV(knn, params, scoring='roc_auc', cv = 5)\nRando_knn.fit(X_train, Y_train)\n\n# Predict using the tuned model\nrY_pred = Rando_knn.predict(X_test)\n\n# Print the classification metrics\nprint(confusion_matrix(Y_test, rY_pred))\nprint(classification_report(Y_test, rY_pred))","127a5c1a":"# Plot the ROC curve and accuracy\nrY_pred_prob = Rando_knn.predict_proba(X_test)[:,1]\nfpr, tpr, threshold = roc_curve(Y_test, rY_pred_prob)\nplt.plot([0,1], [0,1], 'b--')\nplt.plot(fpr, tpr, label='KNN-ROC Curve model 4')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('K Nearest Neighbors for Rando_knn')\nplt.show()","99332217":"# calculate the AUC and accuracy \nacc_model4 = Rando_knn.score(X_test, Y_test)\nauc_model4 = roc_auc_score(Y_test, rY_pred_prob)\nprint('model 4 : the AUC is equal to: {}'.format(auc_model4))\nprint('model 4: the ACCURACY is equal to: {}'.format(acc_model4))\n\n# Get the best parameters for KNN model:\nprint(Rando_knn.best_params_)\nprint(Rando_knn.best_score_)","ee2f84e5":"# --------------------------------------------------------------------------------------------\n# Cross validation for Knn : Relation between variation of accuracy and AUC in Function of value of K\n# --------------------------------------------------------------------------------------------\nfrom sklearn.model_selection import cross_val_score\nk_range = range(1,30)\nk_scores_auc=[]\nk_scores_acc=[]\nfor k in k_range:\n    #1.Rune the KNN\n    knn = KNeighborsClassifier(n_neighbors=k)\n    #2.obtain cross_val_score for KNeighborsClassifier with k neighbours\n    scores_auc = cross_val_score(knn, X, Y, cv=5, scoring='roc_auc')\n    scores_acc = cross_val_score(knn, X, Y, cv=5, scoring='accuracy')\n    k_scores_auc.append(scores_auc.mean())\n    k_scores_acc.append(scores_acc.mean())\n\nplt.plot(k_range, k_scores_auc, label = 'AUC')\nplt.plot(k_range, k_scores_acc, label = 'ACCURACY')\nplt.legend()\nplt.xlabel('k for Knn')\nplt.ylabel('Evaluation in %')\nplt.show()","7dbdccd7":"#---------\n## SUM UP\n#---------\nsumup = pd.DataFrame({'model':[1,2,3,4], 'accuracy':[acc_model1, acc_model2, acc_model3, acc_model4],\n                      'AUC':[auc_model1, auc_model2, auc_model3,auc_model4]})\nprint(sumup)","b8f6d3b7":"**Let's begin modeling using KNN with 4 nearest neighbors.**\n**We begin with model1.**","b46d4e67":"**Let's fit multiple models that depend to K, and choose the best one. **\n**We call it model2.**","28fe0bd8":"We plot the ROC Curve of model1's predictions, and calculate its accuracy and AUC.","a585a381":"***We want to fit a KNN model classifier for the Breast Cancer Data.***\n\nWe preform here 3 main models: model1, model2, model3, and model4\n\nOur purpose is to chose the model that perform the best accuracy on test data and give an acceptable estimation of AUC.\n\nModel 1 is a K Nearest Neighbors with K=4 as number of neighbors.\n\nModel 2 tries to fit multiple KNN models based on different values of K and then choose K_optimal that gives the best results.\n\nModel 3 exploits the subclass \"GridSearch\" of sklearn, and uses a grid of value for K and explore the best model.\n\nModel4 exploits the subclass RandomizedSearchCV of sklearn, it uses a grid of multiple parameters that can affect the results of the KNN model, we talk here about whether to affcet equal weights to observation or favor the nearest points of a query observation and give them more influence on that query point. We also try using that subclass to know which algorithm works better to define the space of nearest neighbors (KD-tree, Ball-tree), and we used the euclidean distance since all the data is numeric. \n\nIn the last part, we compare the evolution of accuracy and AUC in function of different values of k","d3776440":"Let's explore the shape of data, the type and, descriptive statistics of each variable, and the name of each one.","c1a09173":"After exploring the data, we split it to features and target variable.","48189536":"It seems that modeling with multiple values of K and choose the best one, give us a model with better accuracy and AUC than fitting a KNN model with 4 neighbors.","b443ac7c":"After importing the data, we drop 'id' column, and use a list comprehension to recode the 'diagnosis' variable as 0 if diagnosis == 'B' and 1 if diagnosis == 'M'.","b0489712":"it seems that the RandomizedSearchCV gives the same number of neighbors:25, the best algorithm was 'ball-tree', and it offers an acceptable accuracy of 98.37%","792985f7":"**Let's tune our KNN model using the sub class GridSearche of sklearn.**\n\n**We call it model3.**","588a6664":"So the GridSearch subclass found that using 25 as number of nearest neighbors gives better accuracy and AUC.","dc4c5785":"Using a 5 Folds Cross Validation for multiple values of nearest neihgbors, we notice that in a range between 12 to 17 nearest neeighbors we got the best accuracy but not for the AUC, since it reachs its maximum for a range value of K between 25 and 30. \nBoth of these results confirm what we get in three models.\nSince AUC give best evaluation of a classifier model, we can conclude that model2 is the suitable one for our Breast Cancer Data.","cbd21c4e":"We convert the 'diagnosis' variable from object type to categorical.","3d643de0":"While we implement the value of K each time, we notice that the training accuracy trends to decrease, and the testig accuracy look to increase. So we take the K_optimal=12 that is the best number of neighbors we can have, for better Accuracy on test data. However, we need to take into consideration the AUC too."}}