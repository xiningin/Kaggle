{"cell_type":{"8869bdfe":"code","6c0acfd7":"code","4b3e14c2":"code","8f001351":"code","1b94a960":"code","3d36f220":"code","23e667b5":"code","21c2cabb":"code","ed700d2d":"code","45e81f8e":"code","063e432a":"code","67b88917":"code","c6556372":"code","4eddfb24":"code","36046e99":"code","69ea11a1":"code","d9c8d75b":"code","1e286e36":"code","bbead8e1":"code","2bd96085":"code","89d46397":"code","9344b19b":"code","744b78d4":"code","7869973e":"code","308cb940":"code","7eb1cb77":"code","7143b2fb":"code","83dc5828":"code","aed0c66a":"code","d28c1726":"code","8a4f3cd5":"code","52412922":"code","6725f8bc":"code","5e916bbc":"code","a4eb39de":"code","a0ca3f4b":"code","d093d7e2":"code","188d2bd5":"code","9f59fb55":"code","24a41af9":"code","5c1e3579":"code","a18484de":"code","d3ffdb4e":"code","7841e585":"code","8e148309":"code","bc5c01be":"code","b32d69ee":"code","49b869ca":"code","3d911e42":"code","f59241dc":"code","c0041c61":"code","b27dc5d5":"code","b15cb31c":"code","f6d38834":"code","4f611fe6":"code","c1d93866":"code","d4ca8848":"code","7923ea4c":"code","6785051e":"code","d5b38442":"code","124bc7e1":"code","f873ecdc":"code","01ab62ab":"code","3eafc26f":"code","637a71b7":"markdown","c3d86628":"markdown","d5ca924c":"markdown","4698f35d":"markdown","bee44997":"markdown","1c2d008f":"markdown","a7f50fe6":"markdown","022cac15":"markdown","83dafc98":"markdown","52e93717":"markdown","53542aef":"markdown","e2e53ea7":"markdown","73d98515":"markdown","4bc23534":"markdown","6e11c0c7":"markdown","5ecc9441":"markdown","40d9c9a3":"markdown","232bb1c0":"markdown","92f347e5":"markdown","28f6dd73":"markdown"},"source":{"8869bdfe":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.feature_selection import SelectFromModel\n%matplotlib inline\n# Classification\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier, ExtraTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, BaggingClassifier, ExtraTreesClassifier, GradientBoostingClassifier\n\nimport xgboost as xgb\nimport lightgbm as lgb\nimport catboost as cat\n\n# Preprocessing\nfrom sklearn.metrics import log_loss\nfrom sklearn.preprocessing import LabelEncoder, MinMaxScaler\nfrom sklearn.model_selection import train_test_split, cross_val_score","6c0acfd7":"test = pd.read_csv('..\/input\/insurance-churn-prediction\/Test.csv')\ntrain = pd.read_csv('..\/input\/insurance-churn-prediction\/Train.csv')","4b3e14c2":"train.head()","8f001351":"sns.heatmap(train.isnull())","1b94a960":"df=train.append(test,ignore_index=True)","3d36f220":"# #pd.plotting.scatter_matrix(df, alpha=0.2, figsize=(10, 10))\n# import pandas_profiling\n# prof = pandas_profiling.ProfileReport(df)\n# prof.to_file(output_file='output.html')","23e667b5":"original_cols = test.columns\noriginal_cols","21c2cabb":"df.info()","ed700d2d":"df['labels'].value_counts()","45e81f8e":"#df['feature_3'].head()\n#plt.hist(df['feature_3'])\nsns.distplot(df['feature_3'])","063e432a":"x3=df['feature_3']\ndiff = np.diff(np.sort(x3))\ndiff\nnp.unique(diff)","67b88917":"x3_shift = ((x3\/0.00388311868) - 0.8369211 +260).round()\nx3_shift.value_counts()\nx3_shift_log = np.log(x3_shift)\ndf['feature_3_shift_log'] = x3_shift_log\nsns.distplot(x3_shift_log)","c6556372":"x2=df['feature_2']\nnp.diff(np.sort(x2.unique()))\n","4eddfb24":"np.diff(np.sort(x2.unique())\/0.12015788)\nx2_shift = ((df['feature_2']\/0.12015788) -0.193581 + 15)\nx2_shift = x2_shift.round()\nnp.sort(x2_shift.unique())","36046e99":"(x2_shift).value_counts()\nsns.distplot(x2_shift)\nx2_shift_bin = x2_shift.apply(lambda x : 1 if x>15 else 0)\ndf['feature_2_shift'] =x2_shift\ndf['feature_2_shift_bin'] =x2_shift_bin","69ea11a1":"x2_shift.value_counts()","d9c8d75b":"x0 = df['feature_0']\nsns.boxplot(x0)","1e286e36":"diff = np.diff(np.sort(x0))\nnp.unique(diff)","bbead8e1":"x0_shift = (x0\/0.09417398 -  0.063788 + 54).round()\nsns.distplot(np.log(x0_shift))\n#sns.distplot(x0_shift)\n#sns.boxplot(x0_shift)\nx0_shift.describe()\ndf['feature_0_shift_log']  = np.log(x0_shift)","2bd96085":"x1 = df['feature_1']\nsns.boxplot(x1)","89d46397":"diff = np.diff(np.sort(x1))\nnp.unique(diff)","9344b19b":"x1_shift = (x1\/0.000328436115 - 0.727934 + 10000).round()\nx1_shift_log = np.log(x1_shift)\nsns.distplot(x1_shift_log)\nx1_shift.value_counts()\n#sns.boxplot(x1_shift_log)\ndf['feature1_shift_log']  = x1_shift_log","744b78d4":"x1_shift.value_counts()","7869973e":"x14 = df['feature_14']\n#sns.boxplot(x14)\nsns.distplot(x14)","308cb940":"x14.value_counts()","7eb1cb77":"x4 = df['feature_4']","7143b2fb":"sns.distplot(x4)","83dc5828":"diff = np.diff(np.sort(x4))\nnp.unique(diff)","aed0c66a":"x4_shift = (x4\/0.64558058 - 0.11808 + 2).round().value_counts()","d28c1726":"sns.distplot(np.log(x4_shift))\ndf['x4_shift_log'] = np.log(x4_shift)","8a4f3cd5":"x5 = df['feature_5']\nx5.value_counts()","52412922":"diff = np.diff(np.sort(x5))\nnp.unique(diff)","6725f8bc":"x5_shift = (x5\/0.00998725 - 0.802206 +43).round()\nx5_shift.value_counts()","5e916bbc":"(np.log(x5_shift)).value_counts()\ndf['x5_shift_log'] = np.log(x5_shift)\ndf['x5_cat'] = df['x5_shift_log'].apply(lambda x : 1 if x==0 else 0)","a4eb39de":"x6 = df['feature_6']\nx6.value_counts()","a0ca3f4b":"diff = np.diff(np.sort(x6))\nnp.unique(diff)","d093d7e2":"x6_shift = (x6\/0.4341379 - 0.419677 + 2).round()\nx6_shift.value_counts()\n(np.log(x6_shift)).value_counts()\ndf['x6_shift'] = x6_shift\ndf['x6_shift_log'] = np.log(x6_shift)\ndf['x6_cat'] = df['x6_shift_log'].apply(lambda x : 1 if x==0 else 0)\n","188d2bd5":"x7 = df['feature_7']\nx7.value_counts()","9f59fb55":"for i in [0,1,2,3,4,5,6,14] :\n    col = 'feature_'+str(i)\n    print(col)\n    df.drop(columns = [col],inplace = True)\n","24a41af9":"\nnew_col = ['feature_3_shift_log',\n       'feature_2_shift', 'feature_2_shift_bin', 'feature_0_shift_log',\n       'feature1_shift_log', 'x4_shift_log', 'x5_shift_log', 'x5_cat',\n       'x6_shift', 'x6_shift_log', 'x6_cat']\nnew_col","5c1e3579":"# for i in range(len(new_col)):\n#     if(new_col[i]=='labels') :\n        \n#         continue\n    \n#     else :\n        \n#         for j in range(len(new_col)) :\n            \n#             if(new_col[j]=='labels') :\n#                 continue\n#             elif i<j :\n# #                print(new_col[i],new_col[j])\n#                 colm = new_col[i]+\"_mul_\"+new_col[j]\n#                 cols = new_col[i]+\"_sum_\"+new_col[j]\n#                 cold = new_col[i]+\"_diff_\"+new_col[j]\n#                 coldi = new_col[i]+\"_div_\"+new_col[j]\n#                 #print(col)\n#                 df[colm] = df[new_col[i]]*df[new_col[j]]\n#                 df[cols] = df[new_col[i]]+df[new_col[j]]\n#                 #df[cold] = df[new_col[i]]-df[new_col[j]]\n#                 #df[coldi] = df[new_col[i]]\/df[new_col[j]]\n#             else :\n#                 continue","a18484de":"labels = df['labels']\ndf = df.dropna(axis=1)","d3ffdb4e":"df['labels']= labels","7841e585":"sns.heatmap(df.isnull())","8e148309":"df = df.replace([np.inf, -np.inf], 0)","bc5c01be":"feat = df.columns\nfeat = feat.drop('labels')","b32d69ee":"feat","49b869ca":"target = 'labels'","3d911e42":"(train[target].value_counts() \/ train.shape[0])*100","f59241dc":"df_train=df[df['labels'].isnull()==False].copy()","c0041c61":"# from imblearn.over_sampling import SMOTE\n# sm = SMOTE(random_state=42, sampling_strategy='all')\n# X_train_ovr, y_train_ovr = sm.fit_sample(df_train[feat], df_train[target])\n\n# print(\"After Oversampling : {} --> {}\".format(X_train_ovr.shape, y_train_ovr.shape))","b27dc5d5":"# train_ovr = pd.DataFrame(X_train_ovr, columns=df_train.columns.tolist())\n# train_ovr[target] = y_train_ovr\n\n# train_ovr.shape","b15cb31c":"# (train_ovr[target].value_counts() \/ train_ovr.shape[0])*100","f6d38834":"def baseliner(X, y, cv=3, metric='f1_macro'):\n    print(\"Baseliner Models\\n\")\n    eval_dict = {}\n    models = [lgb.LGBMClassifier(), xgb.XGBClassifier(),\n              #GradientBoostingClassifier(),\n                  LogisticRegression(), GaussianNB(), RandomForestClassifier(), DecisionTreeClassifier(),\n                  ExtraTreeClassifier(), AdaBoostClassifier(), BaggingClassifier(),\n              #ExtraTreesClassifier(),\n              #SVC(probability=True), KNeighborsClassifier() \n                 ]\n    print(\"Model Name \\t |   f1\")\n    print(\"--\" * 50)\n\n    for index, model in enumerate(models, 0):\n        model_name = str(model).split(\"(\")[0]\n        eval_dict[model_name] = {}\n\n        results = cross_val_score(model, X, y, cv=cv, scoring=metric)\n        eval_dict[model_name]['cv'] = results.mean()\n\n        print(\"%s \\t | %.4f \\t\" % (\n            model_name[:12], eval_dict[model_name]['cv']))","4f611fe6":"df_train=df[df['labels'].isnull()==False].copy()\ndf_test=df[df['labels'].isnull()==True].copy()\ndf_test.drop(columns=['labels'],axis=1, inplace=True)\n\nprint(df_train.shape,df_test.shape)","c1d93866":"x = df_train.drop('labels',axis=1)\ny = df_train['labels']","d4ca8848":"baseliner(x, y)","7923ea4c":"from sklearn.metrics import f1_score\n\ndef lgb_f1_score(y_hat, data):\n    y_true = data.get_label()\n    y_hat = np.round(y_hat) # scikits f1 doesn't like probabilities\n    return 'f1', f1_score(y_true, y_hat), True","6785051e":"def lgb_model(train, features, target, ts=False, plot=True):\n    evals_result = {}\n    trainX, validX, trainY, validY = train_test_split(train[features], train[target], shuffle=False, test_size=0.2, random_state=13)\n    print(\"LGB Model\")\n    lgb_train_set = lgb.Dataset(trainX, label=trainY)\n    lgb_valid_set = lgb.Dataset(validX, label=validY)\n\n    MAX_ROUNDS = 2000\n    lgb_params = {\n        \"boosting\": 'gbdt',\n        \"learning_rate\": 0.1,\n        \"nthread\": -1,\n        \"seed\": 13,\n        \"num_boost_round\": MAX_ROUNDS,\n        \"objective\": \"binary\",\n    }\n\n    lgb_model = lgb.train(\n        lgb_params,\n        train_set=lgb_train_set,\n        valid_sets=[lgb_train_set, lgb_valid_set],\n        early_stopping_rounds=250,\n        verbose_eval=100,\n        evals_result=evals_result,\n        feval=lgb_f1_score # New metric to be optimised\n    )\n    if plot:\n        lgb.plot_importance(lgb_model, figsize=(24, 24))\n        lgb.plot_metric(evals_result, metric='f1')\n\n    return lgb_model, lgb_model.best_score","d5b38442":"lgbM, score = lgb_model(df_train, feat, target, True, True)","124bc7e1":"y_preds = lgbM.predict(df_test[feat])\ny_preds","f873ecdc":"df_lgb = pd.DataFrame({'labels':y_preds})\ndf_lgb['labels'] = df_lgb['labels'].apply(lambda x : 1 if x>0.5 else 0)","01ab62ab":"df_lgb['labels'].value_counts()","3eafc26f":"import time\ntimes = time.strftime(\"%Y%m%d-%H%M%S\")\n\ndf_lgb.to_excel('submission-lgb_'+times+'.xlsx',index=False)","637a71b7":"Variable 6","c3d86628":"# Import Libraries","d5ca924c":"Feature 0","4698f35d":"Feature 5","bee44997":"Feature 14","1c2d008f":"Checking a baseline score with all models then fianlised using LGBM for submission","a7f50fe6":"# Reading Data","022cac15":"For anonimysed data, I tried to look into the the transformations for each variable. First I try to find the shift and co-efficient used for every variable and then check if there was any transformation such as exponenet or log function applied.","83dafc98":"Also checked for imbalance, but didnt play a very big role","52e93717":"Feature 7","53542aef":"Feature 3","e2e53ea7":"\nInsurance companies around the world operate in a very competitive environment. With various aspects of data collected from millions of customers, it is painstakingly hard to analyze and understand the reason for a customer\u2019s decision to switch to a different insurance provider.\n\nFor an industry where customer acquisition and retention are equally important, and the former being a more expensive process, insurance companies rely on data to understand customer behavior to prevent retention. Thus knowing whether a customer is possibly going to switch beforehand gives Insurance companies an opportunity to come up with strategies to prevent it from actually happening.\n\nGiven are 16 distinguishing factors that can help in understanding the customer churn, your objective as a data scientist is to build a Machine Learning model that can predict whether the insurance company will lose a customer or not using these factors.\n\nYou are provided with 16 anonymized factors (feature_0 to feature 15) that influence the churn of customers in the insurance industry\n\nhttps:\/\/www.machinehack.com\/course\/insurance-churn-prediction-weekend-hackathon-2\/","73d98515":"Feature 4","4bc23534":"# Feature Engineering","6e11c0c7":"I tried making new features with simple mathematical fucntions with these features but didnt turn out too useful","5ecc9441":"# MachineHack Insurance Churn Prediction - Problem Statement","40d9c9a3":"Feature 2","232bb1c0":"# Modelling","92f347e5":"I generally use pandas profling to ge an overall look at the data before deep-diving! ","28f6dd73":"Feature 1"}}