{"cell_type":{"13933b5b":"code","6feb2e11":"code","0d35d131":"code","5156c0b8":"code","e09cfe88":"code","9e547792":"code","f58e4cc0":"code","2667d6a6":"code","21e25238":"code","fb680a49":"code","ef8e384f":"code","498cf1d9":"code","ea849224":"code","fc036521":"code","23f8e722":"code","b0bcb037":"code","c81afb3e":"code","16bc1a9d":"code","59173da3":"code","25f6396c":"code","23e9a5de":"code","f2d61a0d":"code","9de5f48b":"code","8034799c":"code","3b3da0e0":"code","776a533b":"code","17929695":"code","5631811b":"code","68f38e68":"code","60321e83":"code","cc075a01":"code","341b4231":"code","40593a80":"code","15a0f10d":"code","6bad2fb9":"code","75be81c9":"code","8b83e180":"code","a3fd9431":"code","20dd20f0":"code","890464f3":"code","10826e4f":"code","5cf4c4c6":"code","8fad717e":"code","117ebacb":"code","52f7a8c4":"code","ceed0831":"code","a0285adb":"code","d692a6d4":"code","5dca1806":"code","03604805":"code","3b203a4a":"code","36a25630":"code","1add0507":"code","ff80836d":"code","7b1ba535":"code","cd4e229e":"code","3b8dc4ab":"code","df97ee96":"code","dc977af6":"code","b7675235":"code","af5a0381":"code","8b1a117b":"code","e959f40f":"code","9dd0f323":"code","b79c8b75":"code","fe5c5f28":"code","44bf789a":"code","54fa1d3d":"code","5becf506":"code","d6cd7a50":"code","24b64430":"code","fc507443":"code","57d33c44":"code","a7c894de":"code","aac942ee":"code","d070f238":"code","abaa7058":"code","7b9d3bb7":"code","d16ef843":"code","c1449ff2":"code","1b4bfb38":"code","7379da09":"code","0d497a47":"code","64288b0a":"code","e87bd630":"code","e4c08276":"code","e71980ed":"code","e586f33b":"code","6abeb596":"code","f5491454":"code","f4d66062":"code","38a55f50":"code","a7b2b950":"code","765d5bf9":"code","01cc15f8":"code","69272c6f":"code","7cd2ec66":"code","6ec06992":"code","3d3507e5":"code","fedd1f67":"code","cbaa99d5":"code","cdda8fe7":"code","b5290fb9":"code","10642abb":"code","addb9fea":"code","52914c1c":"code","86829728":"code","7a9060f6":"code","92dcb1b8":"code","386a424b":"code","3b7f8a5f":"code","100c58ba":"code","818c4898":"code","00e4c33f":"code","d5d8c922":"code","ea6df033":"code","dabc2be3":"code","c51be802":"code","92ef58cc":"code","01d25aa8":"code","7557e0e2":"code","5525873b":"code","6579d6d5":"code","1631723c":"code","567d1586":"code","40075d00":"code","be3f00a7":"code","0b53f96c":"code","e6c89b37":"code","06f7d791":"code","0476569e":"code","b440def7":"code","dc57d0a5":"code","f1793d60":"code","aad6ee5d":"code","798d6b0c":"code","8221c5e6":"code","b731bc44":"code","bae77066":"code","ec66975b":"code","d04ddaed":"code","4ecfb67a":"code","1924d13f":"code","02b105ef":"code","69b63e90":"code","4ed87e4c":"code","83e69337":"code","e89df754":"code","bb1e6066":"code","a7e54a1a":"code","a173b387":"code","516e6401":"code","d5613623":"code","59d5812d":"code","24c12053":"code","b2cfd61f":"code","10548af7":"code","2f7afa28":"code","e673492a":"code","ed45b98a":"code","71d5e590":"code","c33eccd9":"code","8cf89a61":"code","45ed87ab":"code","8f07fc37":"code","3ffb0760":"code","c68e3e2b":"code","b0e111f2":"code","73eaef0f":"code","bb791b61":"code","a4cf5d2d":"code","32ad48f6":"code","081cc061":"code","a025e651":"code","9948f725":"code","29fbd271":"code","a9dfe90c":"code","335335c0":"code","117009cc":"code","bd1c8753":"code","bb226b5b":"code","76d75aee":"code","04c85d21":"code","d461be5f":"code","a0af9b9d":"code","8952b805":"code","f1ceb820":"code","4049905a":"code","d8344b89":"code","755d6644":"code","17c14922":"code","cc43ed73":"code","8adec459":"code","b22bc4e2":"code","40ff1790":"code","f6dcd662":"code","3d3da092":"code","f8085189":"code","2cdea2b8":"code","5edafd6e":"code","4cc31921":"code","044c626f":"code","6240248c":"code","e481b6ef":"code","9ddbbef5":"code","70c10993":"code","ceb04242":"code","720ad773":"code","fc71312a":"code","69a0d77b":"code","5e228dc7":"code","adbf40f8":"code","64c894be":"code","3ebb7dfa":"code","62349d15":"code","1bc5d6dc":"code","0cf4739a":"code","5ff989ba":"code","320c5d61":"code","6bd24936":"code","7839baeb":"code","c0088b08":"code","0a4c57be":"code","3b701e93":"code","7b72db8a":"code","542cc725":"code","7416361a":"code","77d6529a":"code","53681021":"code","01abfba4":"code","db5afb5c":"code","84d6894a":"code","2ba83a35":"code","63fbdc37":"code","d8648bfe":"code","8bf2674c":"code","30ec3bd9":"code","cab67852":"code","9890a774":"code","125b68be":"code","9650df95":"code","69c0234b":"code","e03289da":"code","39b4c929":"code","47bb4a5d":"code","321e7779":"code","f67d5dd2":"markdown","6c0e042b":"markdown","9a71be44":"markdown","57b6813b":"markdown","0f95e4ff":"markdown","54c0e8d1":"markdown","85d79ab1":"markdown","fce96708":"markdown","4039fd40":"markdown","49d26592":"markdown","bcb4f883":"markdown","a8f5c7f8":"markdown","92fb44e7":"markdown","3bbd0685":"markdown","6cb489db":"markdown","87ed4f16":"markdown","76636425":"markdown","df2feb48":"markdown","e07a2595":"markdown","7a4f4cca":"markdown","8cb2c59a":"markdown","7462fd48":"markdown","620f5d6a":"markdown","d1ee9237":"markdown","12e54946":"markdown","3313dbdc":"markdown","23253a83":"markdown","eaf5abd2":"markdown","259c238a":"markdown","e01dbcb4":"markdown","7875c3b6":"markdown","3fdedc92":"markdown","f5a58d8c":"markdown","04947f1c":"markdown","403df7a0":"markdown","489ca9d5":"markdown","2c4f8967":"markdown","68ad7718":"markdown","f2ad5803":"markdown","b377cce1":"markdown","c5e866d7":"markdown","209de9d8":"markdown","77c06b06":"markdown","409426cf":"markdown","f6fd3645":"markdown","60c869b3":"markdown","523bd380":"markdown","84162422":"markdown","6ceb764d":"markdown","25b34235":"markdown","fd11950e":"markdown","f6d89be5":"markdown","6ebf1fdf":"markdown","ddf73275":"markdown","6a6e73c3":"markdown","85ad8fd4":"markdown","71174a12":"markdown","39f1556d":"markdown","d722af86":"markdown","451ecee6":"markdown","65c960b1":"markdown","a36366ba":"markdown","11f14506":"markdown","12a5fd97":"markdown","3fb82336":"markdown","f712404d":"markdown","605ccdf7":"markdown","2f9ae44b":"markdown","dc9d170d":"markdown","c5a46b2e":"markdown","0bc5bc82":"markdown","d0e765e6":"markdown","ab52a6dd":"markdown","38e41750":"markdown","dcae80f5":"markdown","5df5841e":"markdown","9dfa8a7e":"markdown","3eb2c855":"markdown","3b82744f":"markdown","c1f6d3ad":"markdown","f5e132f1":"markdown","e9263ca7":"markdown","a7bcd499":"markdown","07bbd5bc":"markdown","35b5a628":"markdown","a84e21fd":"markdown","4ad1d687":"markdown","e66cfadd":"markdown","317a94c9":"markdown","a4051153":"markdown","0e8a1eee":"markdown","e36945f8":"markdown","0b21658f":"markdown","616c9278":"markdown","baad2adc":"markdown","35b4cfc8":"markdown","42b3c305":"markdown","13f6890a":"markdown","0f27c3c6":"markdown","baefc3c6":"markdown","7065c320":"markdown","0208346e":"markdown","ebe3747c":"markdown","992f747b":"markdown","72c1b0de":"markdown","90074c18":"markdown","bb31b9c4":"markdown","f20f0374":"markdown","a254279b":"markdown","534440b1":"markdown","9cb44208":"markdown","97365d27":"markdown","b6737d68":"markdown","761358c5":"markdown","3e5b2923":"markdown","12890b0e":"markdown","f4992466":"markdown","8f60d420":"markdown","872b692a":"markdown","df14e5cc":"markdown","1d4ba372":"markdown","f0ea60ec":"markdown","18eb84a7":"markdown","2db744a4":"markdown","a8ffc8a1":"markdown","db1fda42":"markdown","24870352":"markdown","df556598":"markdown","2d9583a9":"markdown","5417e63b":"markdown","0196bf55":"markdown","b5a0269f":"markdown","c25b3131":"markdown","428bc4a9":"markdown","0e9cdb81":"markdown","3fd6b4e5":"markdown","794ff7f1":"markdown","69d6b80f":"markdown","82712c56":"markdown","273af123":"markdown","2903d222":"markdown","6c6334a5":"markdown","ef9b1901":"markdown","621e3ce5":"markdown","1f911f97":"markdown","aaaf58df":"markdown","815f0be3":"markdown","3d02baec":"markdown","e1423b58":"markdown","1669d26b":"markdown","21045877":"markdown","dbf6bf54":"markdown","8c2059f5":"markdown","4ec9ab66":"markdown","668398df":"markdown","0e71e628":"markdown","98cd0e2e":"markdown","9ac520c2":"markdown","5c968d28":"markdown","35bb8067":"markdown","f79031f2":"markdown","f03dfd4f":"markdown","5990cdc0":"markdown","a3c9ddf2":"markdown","e5db9dd9":"markdown","c9f2cfc0":"markdown","f135aae6":"markdown","3535685c":"markdown","0418753c":"markdown","bc9d03ab":"markdown","87c9a769":"markdown","331d50db":"markdown","80a4feeb":"markdown","f89c9f8e":"markdown","09f7e7e1":"markdown","2d3c3b5d":"markdown","d8bd79a5":"markdown","cbb4df94":"markdown"},"source":{"13933b5b":"#import some necessary libraries\nimport datetime\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n%matplotlib inline\nimport matplotlib.pyplot as plt  # Matlab-style plotting\nimport seaborn as sns\ncolor = sns.color_palette()\nsns.set_style('darkgrid')\n#import warnings\n#def ignore_warn(*args, **kwargs):\n#    pass\n#warnings.warn = ignore_warn #ignore annoying warning (from sklearn and seaborn)\nimport time\n\nfrom scipy import stats\nfrom scipy.stats import norm, skew, kurtosis, boxcox #for some statistics\nfrom scipy.special import boxcox1p, inv_boxcox, inv_boxcox1p\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.pipeline import Pipeline\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import PolynomialFeatures, MinMaxScaler, LabelEncoder, RobustScaler, StandardScaler\nfrom sklearn.decomposition import PCA, NMF\nfrom sklearn.feature_selection import SelectKBest, chi2\n\npd.set_option('display.float_format', lambda x: '{:.3f}'.format(x)) #Limiting floats output to 3 decimal points\n\nfrom subprocess import check_output\n\nStartTime = datetime.datetime.now()","6feb2e11":"class MyTimer():\n    # usage:\n    #with MyTimer():                            \n    #    rf.fit(X_train, y_train)\n    \n    def __init__(self):\n        self.start = time.time()\n    def __enter__(self):\n        return self\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        end = time.time()\n        runtime = end - self.start\n        msg = 'The function took {time} seconds to complete'\n        print(msg.format(time=runtime))","0d35d131":"# default competition\ncompetition = 'SR' # StackedRegression\n\ntry:\n    a = check_output([\"ls\", \"..\/input\"]).decode(\"utf8\") # new Kaggle competition\nexcept:\n    a=''\nfinally:\n    print('')\ntry:\n    b = check_output([\"ls\", \"-rlt\", \"..\/StackedRegression\"]).decode(\"utf8\")\nexcept:\n    b=''\nfinally:\n    print('')    \n#if (competition == 'SRP_2'): # Stacked Regressions Part 2\nif (len(a) > 0): # new competition\n    competition = 'SR'\n    train = pd.read_csv('..\/input\/home-data-for-ml-course\/train.csv')#,index_col='Id')\n    test = pd.read_csv('..\/input\/home-data-for-ml-course\/test.csv')#,index_col='Id')\nelif (len(b)): # run locally\n    competition = 'SR'\n    train = pd.read_csv('input\/train.csv')\n    test = pd.read_csv('input\/test.csv')\nelse: # old competition\n    competition = 'SRP_2'\n    train = pd.read_csv('..\/input\/train.csv')\n    test = pd.read_csv('..\/input\/test.csv')","5156c0b8":"##display the first five rows of the train dataset.\ntrain.head(5)","e09cfe88":"##display the first five rows of the test dataset.\ntest.head(5)\n","9e547792":"fig, ax = plt.subplots()\n#ax.scatter(x = train['GrLivArea'], y = train['SalePrice']\nax.scatter(x = train['GrLivArea'], y = np.log(train['SalePrice']))\nplt.ylabel('SalePrice', fontsize=13)\nplt.xlabel('GrLivArea', fontsize=13)\n\n#m, b = np.polyfit(train['GrLivArea'], train['SalePrice'], 1)\nm, b = np.polyfit(train['GrLivArea'], np.log(train['SalePrice']), 1)\n#m = slope, b=intercept\nplt.plot(train['GrLivArea'], m*train['GrLivArea'] + b)\n\nplt.show()\n","f58e4cc0":"train.shape[1]\n#a = int(np.sqrt(train.shape[1]))\na = 4\nb = int(train.shape[1]\/4)\nr = int(train.shape[1]\/a)\nc = int(train.shape[1]\/b)\ni = 0\nfig, ax = plt.subplots(nrows=r, ncols=c, figsize=(15, 60))\nfor row in ax:\n    for col in row:\n        try:\n            col.scatter(x = train[train.columns[i]], y = np.log(train['SalePrice']))\n            col.title.set_text(train.columns[i])\n        except:\n            temp=1\n        #except Exception as e:\n        #    print(e.message, e.args)\n        finally:\n            temp=1\n        i = i + 1\n        \nplt.show()","2667d6a6":"#Deleting outliers - none of these deletions help improve the score\n\n#train = train.drop(train[(train['LotFrontage']>300) & (train['SalePrice']<400000)].index)\n#train = train.drop(train[(train['LotArea']>100000) & (train['SalePrice']<400000)].index)\n#train = train.drop(train[(train['BsmtFinSF1']>4000) & (train['SalePrice']<250000)].index)\n#train = train.drop(train[(train['GrLivArea']>4000) & (train['SalePrice']<250000)].index)\n\n# only these 2 deletions help improve the score\ntrain = train.drop(train[(train['OverallQual']>9) & (train['SalePrice']<220000)].index)\ntrain = train.drop(train[(train['GrLivArea']>4000) & (train['SalePrice']<300000)].index)\n","21e25238":"test","fb680a49":"#check the numbers of samples and features\nprint(\"The train data size before dropping Id feature is : {} \".format(train.shape))\nprint(\"The test data size before dropping Id feature is : {} \".format(test.shape))\n\n#Save the 'Id' column\ntrain_ID = train['Id']\ntest_ID = test['Id']\n\n\n#Now drop the  'Id' colum since it's unnecessary for  the prediction process.\ntrain.drop(\"Id\", axis = 1, inplace = True)\ntest.drop(\"Id\", axis = 1, inplace = True)\n\n#check again the data size after dropping the 'Id' variable\nprint(\"\\nThe train data size after dropping Id feature is : {} \".format(train.shape)) \nprint(\"The test data size after dropping Id feature is : {} \".format(test.shape))","ef8e384f":"#Check the graphic again\nfig, ax = plt.subplots()\nax.scatter(train['GrLivArea'], np.log(train['SalePrice']))\nplt.ylabel('SalePrice', fontsize=13)\nplt.xlabel('GrLivArea', fontsize=13)\n#m, b = np.polyfit(train['GrLivArea'], train['SalePrice'], 1)\nm, b = np.polyfit(train['GrLivArea'], np.log(train['SalePrice']), 1)\n#m = slope, b=intercept\nplt.plot(train['GrLivArea'], m*train['GrLivArea'] + b)\nplt.show()","498cf1d9":"# linear \nx_data = train['GrLivArea']\ny_data = train['SalePrice']\nlog_y_data = np.log(train['SalePrice'])\n\ncurve_fit = np.polyfit(x_data, log_y_data, 1)\nprint(curve_fit)\ny = np.exp(1.11618915e+01) * np.exp(5.70762509e-04*x_data)\nplt.plot(x_data, y_data, \"o\")\nplt.scatter(x_data, y,c='red')","ea849224":"# linear with log y\ny = np.exp(1.11618915e+01) * np.exp(5.70762509e-04*x_data)\nplt.plot(x_data, np.log(y_data), \"o\")\nplt.scatter(x_data, np.log(y),c='red')","fc036521":"# polynomial\nx_data = train['GrLivArea']\ny_data = train['SalePrice']\nlog_y_data = np.log(train['SalePrice'])\ncurve_fit_gla = np.polyfit(x_data, y_data, 2)\ny = curve_fit_gla[2] + curve_fit_gla[1]*x_data + curve_fit_gla[0]*x_data**2","23f8e722":"plt.plot(x_data, y_data, \"o\")\nplt.scatter(x_data, y,c='red')","b0bcb037":"# polynomial with log y\nplt.plot(x_data, np.log(y_data), \"o\")\nplt.scatter(x_data, np.log(y),c='red')","c81afb3e":"y_data #HERE","16bc1a9d":"# polynomial with log y\nplt.plot(x_data, np.log(y_data), \"o\")\nplt.scatter(x_data, np.log(y),c='red')","59173da3":"sns.distplot(train['SalePrice'] , fit=norm)\n\n_skew = skew(train['SalePrice'])\n_kurtosis = kurtosis(train['SalePrice'])\n\n# Get the fitted parameters used by the function\n(mu, sigma) = norm.fit(train['SalePrice'])\nprint( '\\n mu = {:.2f} and sigma = {:.2f}, skew = {:.2f} kurtosis = {:.2f}\\n'.format(mu, sigma, _skew, _kurtosis))\n\n#Now plot the distribution\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n            loc='best')\nplt.ylabel('Frequency')\nplt.title('SalePrice distribution')\n\n#Get also the QQ-plot\nfig = plt.figure()\nres = stats.probplot(train['SalePrice'], plot=plt)\nplt.show()\n","25f6396c":"#We try the numpy function log1p which  applies log(1+x) to all elements of the column\n\n# option 1 - original\n#train[\"SalePrice\"] = np.log1p(train[\"SalePrice\"])\n\n# Option 2: use box-cox transform - this performs better than the log(1+x)\n# try different alpha values  between 0 and 1\nlam_l = 0.35 # optimized value\ntrain[\"SalePrice\"] = boxcox1p(train[\"SalePrice\"], lam_l) \n\n# Option 3: boxcox letting the algorithm select lmbda based on least-likelihood calculation\n#train[\"SalePrice\"], lam_l = boxcox1p(x=train[\"SalePrice\"], lmbda=None)\n\n# option 4 - use log, compare to log1p => score is same\n#train[\"SalePrice\"] = np.log(train[\"SalePrice\"])","23e9a5de":"x = np.linspace(0, 20)\ny1 = np.log(x)\ny2 = np.log1p(x)\ny3 = boxcox1p(x, 0.35)\ny4 = boxcox1p(x, 0.10)\ny5 = boxcox1p(x, 0.50)\nplt.plot(x,y1,label='log') \nplt.plot(x,y2,label='log1p') \nplt.plot(x,y3,label='boxcox .35') \nplt.plot(x,y4,label='boxcox .10') \nplt.plot(x,y5,label='boxcox .50') \nplt.legend()\nplt.show()\n\nx = np.linspace(0, 100000)\ny1 = np.log(x)\ny2 = np.log1p(x)\ny3 = boxcox1p(x, 0.35)\ny4 = boxcox1p(x, 0.10)\ny5 = boxcox1p(x, 0.50)\nplt.plot(x,y1,label='log') \nplt.plot(x,y2,label='log1p') \nplt.plot(x,y3,label='boxcox .35') \nplt.plot(x,y4,label='boxcox .10') \nplt.plot(x,y5,label='boxcox .50') \nplt.legend()\nplt.show()","f2d61a0d":"# to revert back use y = inv_boxcox1p(x, lam_l) => train[\"SalePrice\"] = inv_boxcox1p(train[\"SalePrice\"], lam_l)\n# think the underlying formula is this: # pred_y = np.power((y_box * lambda_) + 1, 1 \/ lambda_) - 1\n\n#Check the new distribution \nsns.distplot(train['SalePrice'] , fit=norm);\n\n_skew = skew(train['SalePrice'])\n_kurtosis = kurtosis(train['SalePrice'])\n\n# Get the fitted parameters used by the function\n(mu, sigma) = norm.fit(train['SalePrice'])\nprint( '\\n mu = {:.2f} and sigma = {:.2f}, skew = {:.2f} kurtosis = {:.2f}\\n'.format(mu, sigma, _skew, _kurtosis))\n\n#Now plot the distribution\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n            loc='best')\nplt.ylabel('Frequency')\nplt.title('SalePrice distribution')\n\n#Get also the QQ-plot\nfig = plt.figure()\nres = stats.probplot(train['SalePrice'], plot=plt)\nplt.show()\n","9de5f48b":"ntrain = train.shape[0]\nntest = test.shape[0]\ny_train = train.SalePrice.values\nall_data = pd.concat((train, test)).reset_index(drop=True)\nall_data.drop(['SalePrice'], axis=1, inplace=True)\nprint(\"all_data size is : {}\".format(all_data.shape))","8034799c":"def add_gla(row, p):\n    return (p[2] + p[1]*row.GrLivArea + p[0]*(row.GrLivArea**2))\n\n#all_data['GrLivAreaPoly'] = all_data.apply(lambda row: add_gla(row,curve_fit_gla), axis=1)","3b3da0e0":"all_data.GrLivArea[:ntrain]","776a533b":"correlation_matrix = np.corrcoef(all_data.GrLivArea[:ntrain], y_train)\ncorrelation_xy = correlation_matrix[0,1]\nr_squared = correlation_xy**2\n\nprint(r_squared)","17929695":"for i in range(1,11,1):\n    j = i\/10\n    correlation_matrix = np.corrcoef(all_data.GrLivArea[:ntrain]**j, y_train)\n    correlation_xy = correlation_matrix[0,1]\n    r_squared = correlation_xy**2\n\n    print(j, r_squared)","5631811b":"def add_gla2(row, p):\n    return (row.GrLivArea**p)\n\n#all_data['GrLivAreaRoot'] = all_data.apply(lambda row: add_gla2(row,0.3), axis=1)","68f38e68":"all_data.head()","60321e83":"all_data_na = (all_data.isnull().sum() \/ len(all_data)) * 100\nall_data_na = all_data_na.drop(all_data_na[all_data_na == 0].index).sort_values(ascending=False)[:30]\nmissing_data = pd.DataFrame({'Missing Ratio' :all_data_na})\nmissing_data.head(30)","cc075a01":"all_numerical = all_data.select_dtypes(include=np.number).columns.tolist()\nmissing_data.index.values.tolist()\nmissing_df = all_data[missing_data.index.values.tolist()]\nmissing_numerical = missing_df.select_dtypes(include=np.number).columns.tolist()","341b4231":"f, ax = plt.subplots(figsize=(15, 12))\nplt.xticks(rotation='90')\nsns.barplot(x=all_data_na.index, y=all_data_na)\nplt.xlabel('Features', fontsize=15)\nplt.ylabel('Percent of missing values', fontsize=15)\nplt.title('Percent missing data by feature', fontsize=15)","40593a80":"#Correlation map to see how features are correlated with each other and with SalePrice\ncorrmat = train.corr(method='kendall')\nplt.subplots(figsize=(12,9))\nsns.heatmap(corrmat, vmax=0.9, square=True)","15a0f10d":"print (corrmat['SalePrice'].sort_values(ascending=False)[:5], '\\n')\nprint (corrmat['SalePrice'].sort_values(ascending=False)[-5:])","6bad2fb9":"all_data.shape","75be81c9":"ImputeToNone = [\"Alley\", \"BsmtQual\", \"BsmtCond\", \"BsmtExposure\", \"BsmtFinType1\", \"BsmtFinType2\", \"FireplaceQu\", \"GarageType\", \"GarageFinish\", \"GarageQual\", \"GarageCond\", \"PoolQC\", \"Fence\", \"MiscFeature\"]\nfor col in ImputeToNone:  \n    all_data[col].fillna(\"None\", inplace=True)","8b83e180":"#Group by neighborhood and fill in missing value by the median LotFrontage of all the neighborhood\n#all_data[\"LotFrontage\"] = all_data.groupby(\"Neighborhood\")[\"LotFrontage\"].transform(\n#    lambda x: x.fillna(x.median()))","a3fd9431":"# selecting another column\nall_data[missing_numerical]","20dd20f0":"corrmat_all = all_data[all_numerical].corr(method='kendall')\ncorrmat_all","890464f3":"corrmat2 = all_data[missing_numerical].corr(method='kendall')\ncorrmat2\n#print (corrmat2['LotFrontage'].sort_values(ascending=False), '\\n')","10826e4f":"all_data","5cf4c4c6":"def ImputeData(all_data, numerical_input, col_to_impute):\n    from sklearn.impute import KNNImputer\n    \n    Missing = all_data[numerical_input]\n    imputer = KNNImputer(n_neighbors=5, weights='uniform', metric='nan_euclidean')\n    imputer.fit(Missing)\n    Xtrans = imputer.transform(Missing)\n    df_miss = pd.DataFrame(Xtrans,columns = Missing.columns)\n    all_data[col_to_impute] = df_miss[col_to_impute]\n    return (all_data)\n    ","8fad717e":"all_data = ImputeData(all_data, all_numerical, 'LotFrontage')","117ebacb":"all_data","52f7a8c4":"for col in ('GarageYrBlt', 'GarageArea', 'GarageCars', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF','TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath'):\n    all_data[col] = all_data[col].fillna(0)","ceed0831":"all_data[\"MasVnrType\"] = all_data[\"MasVnrType\"].fillna(\"None\")\nall_data[\"MasVnrArea\"] = all_data[\"MasVnrArea\"].fillna(0)","a0285adb":"all_data['MSZoning'].value_counts()","d692a6d4":"# this one may be a bit dangerous, maybe try to get zone from neighborhood most common value, similar to LotFrontage previously\nall_data['MSZoning'] = all_data['MSZoning'].fillna(all_data['MSZoning'].mode()[0])\n\n# NEW, slightly worse score\n#all_data[\"MSZoning\"] = all_data.groupby(\"Neighborhood\")[\"MSZoning\"].transform(\n#    lambda x: x.fillna(x.mode()))","5dca1806":"all_data['Utilities'].value_counts()","03604805":"all_data = all_data.drop(['Utilities'], axis=1)","3b203a4a":"all_data[\"Functional\"] = all_data[\"Functional\"].fillna(\"Typ\")","36a25630":"all_data['Electrical'] = all_data['Electrical'].fillna(all_data['Electrical'].mode()[0])","1add0507":"all_data['KitchenQual'] = all_data['KitchenQual'].fillna(all_data['KitchenQual'].mode()[0])","ff80836d":"all_data['Exterior1st'] = all_data['Exterior1st'].fillna(all_data['Exterior1st'].mode()[0])\nall_data['Exterior2nd'] = all_data['Exterior2nd'].fillna(all_data['Exterior2nd'].mode()[0])","7b1ba535":"all_data['SaleType'] = all_data['SaleType'].fillna(all_data['SaleType'].mode()[0])","cd4e229e":"\nall_data['MSSubClass'] = all_data['MSSubClass'].fillna(\"None\")\n\n","3b8dc4ab":"#Check remaining missing values if any \nall_data_na = (all_data.isnull().sum() \/ len(all_data)) * 100\nall_data_na = all_data_na.drop(all_data_na[all_data_na == 0].index).sort_values(ascending=False)\nmissing_data = pd.DataFrame({'Missing Ratio' :all_data_na})\nmissing_data.head()","df97ee96":"all_data['MSSubClass'].value_counts()\n","dc977af6":"all_data['OverallCond'].value_counts()","b7675235":"all_data.shape","af5a0381":"import datetime\nYr = all_data['YrSold'].min()\nMo = all_data['MoSold'].min()\nt = datetime.datetime(Yr, Mo, 1, 0, 0)\n\ndef calculateYrMo (row):   \n    return int((datetime.datetime(row.YrSold,row.MoSold,1) - t).total_seconds())","8b1a117b":"# either way will work\n#all_data['YrMoSold'] = all_data.apply(lambda row: int((datetime.datetime(row.YrSold,row.MoSold,1) - t).total_seconds()), axis=1)\n\nall_data['YrMoSold'] = all_data.apply(lambda row: calculateYrMo(row), axis=1)","e959f40f":"#MSSubClass=The building class\nall_data['MSSubClass'] = all_data['MSSubClass'].apply(str)\n\n#Changing OverallCond into a categorical variable\nall_data['OverallCond'] = all_data['OverallCond'].astype(str)\n\n#Year and month sold are transformed into categorical features.\nall_data['YrSold'] = all_data['YrSold'].astype(str)\nall_data['MoSold'] = all_data['MoSold'].astype(str)","9dd0f323":"# Edit: Dropping PoolQC for missing values => makes the model worse, reverting\n#all_data = all_data.drop(['PoolQC'], axis=1)\n\ncols = ('FireplaceQu', 'BsmtQual', 'BsmtCond', 'GarageQual', 'GarageCond', \n        'ExterQual', 'ExterCond','HeatingQC', 'PoolQC', 'KitchenQual', 'BsmtFinType1', \n        'BsmtFinType2', 'Functional', 'Fence', 'BsmtExposure', 'GarageFinish', 'LandSlope',\n        'LotShape', 'PavedDrive', 'Street', 'Alley', 'CentralAir', 'MSSubClass', 'OverallCond', \n        'YrSold', 'MoSold', 'YrMoSold')\n# process columns, apply LabelEncoder to categorical features\nfor c in cols:\n    lbl = LabelEncoder() \n    lbl.fit(list(all_data[c].values)) \n    all_data[c] = lbl.transform(list(all_data[c].values))\n\n# shape\nprint('Shape all_data: {}'.format(all_data.shape))\n","b79c8b75":"# feature engineering add new features \nall_data['TotalSF'] = all_data['TotalBsmtSF'] + all_data['1stFlrSF'] + all_data['2ndFlrSF']\n\nall_data['YrBltAndRemod'] = all_data['YearBuilt'] + all_data['YearRemodAdd'] # A-\nall_data['Total_sqr_footage'] = (all_data['BsmtFinSF1'] + all_data['BsmtFinSF2'] + all_data['1stFlrSF'] + all_data['2ndFlrSF']) # B-\n#all_data['Total_Bathrooms'] = (all_data['FullBath'] + (0.5 * all_data['HalfBath']) + all_data['BsmtFullBath'] + (0.5 * all_data['BsmtHalfBath'])) # C-\n#all_data['Total_porch_sf'] = (all_data['OpenPorchSF'] + all_data['3SsnPorch'] + all_data['EnclosedPorch'] + all_data['ScreenPorch'] + all_data['WoodDeckSF']) # D-\n#all_data['haspool'] = all_data['PoolArea'].apply(lambda x: 1 if x > 0 else 0) # E-\n#all_data['has2ndfloor'] = all_data['2ndFlrSF'].apply(lambda x: 1 if x > 0 else 0) # F-\n#all_data['hasgarage'] = all_data['GarageArea'].apply(lambda x: 1 if x > 0 else 0) # G-\n#all_data['hasbsmt'] = all_data['TotalBsmtSF'].apply(lambda x: 1 if x > 0 else 0) # H-\n#all_data['hasfireplace'] = all_data['Fireplaces'].apply(lambda x: 1 if x > 0 else 0) # I-\n","fe5c5f28":"numeric_feats = all_data.dtypes[all_data.dtypes != \"object\"].index\n\n# Check the skew of all numerical features\nskewed_feats = all_data[numeric_feats].apply(lambda x: skew(x.dropna())).sort_values(ascending=False)\nprint(\"\\nSkew in numerical features: \\n\")\nskewness = pd.DataFrame({'Skew' :skewed_feats})\nskewness.head(10)\n","44bf789a":"skewness = skewness[abs(skewness) > 0.75]\nprint(\"There are {} skewed numerical features to Box Cox transform\".format(skewness.shape[0]))\n\nfrom scipy.special import boxcox1p\nskewed_features = skewness.index\n\nlam_f = 0.15\nfor feat in skewed_features:\n    all_data[feat] = boxcox1p(all_data[feat], lam_f)\n    #all_data[skewed_features] = np.log1p(all_data[skewed_features])","54fa1d3d":"all_data = pd.get_dummies(all_data)\nprint(all_data.shape)","5becf506":"correlations = corrmat['SalePrice'].sort_values(ascending=False)\ndf_corr = correlations.to_frame()\nprint(df_corr.query(\"abs(SalePrice) < 0.011\"))\nlow_corr = df_corr.query(\"abs(SalePrice) < 0.011\").index.values.tolist()\n#print('dropping these columns for low correlation', low_corr)\n#for i in low_corr: \n#    all_data = all_data.drop([i], axis=1)","d6cd7a50":"# to choose number of components, look at this chart. Reference: https:\/\/jakevdp.github.io\/PythonDataScienceHandbook\/05.09-principal-component-analysis.html\n\npca = PCA().fit(all_data)\nplt.plot(np.cumsum(pca.explained_variance_ratio_))\nplt.xlabel('number of components')\nplt.ylabel('cumulative explained variance')","24b64430":"#do PCA\/StandardScaler+clip here or before the skew boxcox1p transform\n\nn_components = 50\npca = PCA(n_components=n_components)\nall_data_pca = pca.fit_transform(all_data)","fc507443":"print(all_data.shape)\nprint(all_data_pca.shape)","57d33c44":"weights = np.round(pca.components_, 3) \nev = np.round(pca.explained_variance_ratio_, 3)\nprint('explained variance ratio',ev)\npca_wt = pd.DataFrame(weights)#, columns=all_data.columns)\npca_wt.head()","a7c894de":"pca_wt.shape ","aac942ee":"#Correlation map to see how features are correlated with each other and with SalePrice\ncorrmat = pd.DataFrame(all_data_pca).corr(method='kendall')\nplt.subplots(figsize=(12,9))\nplt.title(\"Kendall's Correlation Matrix PCA applied\", fontsize=16)\nsns.heatmap(corrmat, vmax=0.9, square=True)\n\n\n#Correlation map to see how features are correlated with each other and with SalePrice\ncorrmat = train.corr(method='kendall')\nplt.subplots(figsize=(12,9))\nplt.title(\"Kendall's Correlation Matrix Initial Train Set\", fontsize=16)\nsns.heatmap(corrmat, vmax=0.9, square=True);\n","d070f238":"train_orig = train.copy()\ntrain_orig['SalePrice'] = y_train\ncorrmat = train_orig.corr(method='kendall')\nprint (corrmat['SalePrice'].sort_values(ascending=False)[:5], '\\n')\nprint (corrmat['SalePrice'].sort_values(ascending=False)[-5:])","abaa7058":"correlations = corrmat[\"SalePrice\"].sort_values(ascending=False)\nfeatures = correlations.index[0:10]\nsns.pairplot(train[features], height = 2.5)\nplt.show();","7b9d3bb7":"print(type(all_data))\nprint(type(pd.DataFrame(all_data_pca)))","d16ef843":"sc = StandardScaler()\nrc = RobustScaler()\n\nuse_pca = 0 # using PCA currently hurts the score\nuse_normalization = 0 # using StandardScaler doesn't work, try RobustScaler now\n\nif (use_pca == 1):\n    all_data_pca = pd.DataFrame(all_data_pca)\n    train = all_data_pca[:ntrain]\n    test = all_data_pca[ntrain:]\n    all_data_pca.head()\nelif (use_normalization == 1):\n    train = all_data[:ntrain]\n    test = all_data[ntrain:]\n    scaled_features = sc.fit_transform(train)\n    train = pd.DataFrame(scaled_features, index=train.index, columns=train.columns)\n    scaled_features = sc.transform(test)\n    test = pd.DataFrame(scaled_features, index=test.index, columns=test.columns)   \nelif (use_normalization == 2):\n    train = all_data[:ntrain]\n    test = all_data[ntrain:]\n    scaled_features = rc.fit_transform(train)\n    train = pd.DataFrame(scaled_features, index=train.index, columns=train.columns)\n    scaled_features = rc.transform(test)\n    test = pd.DataFrame(scaled_features, index=test.index, columns=test.columns)  \nelse:\n    # back to original splits (from train.csv and test.csv)\n    train = all_data[:ntrain]\n    test = all_data[ntrain:]\n","c1449ff2":"save_data = 0\nif (save_data == 1):\n    df1 = train.copy()\n    df1['SalePrice'] = inv_boxcox1p(y_train, lam_l)\n    df1.insert(0, 'Id', list(train_ID), allow_duplicates=False)\n    df1.to_csv('HousePricesTrain.csv', index=False)  \n    df2 = test.copy()\n    df2.insert(0, 'Id', list(test_ID), allow_duplicates=False)\n    df2.to_csv('HousePricesTest.csv', index=False) ","1b4bfb38":"#Correlation map to see how features are correlated with each other and with SalePrice\ncorrmat = train.corr(method='kendall')\nplt.subplots(figsize=(24,18))\nsns.heatmap(corrmat, vmax=0.9, square=True)","7379da09":"#Correlation map to see how features are correlated with each other and with SalePrice\ncorrmat = test.corr(method='kendall')\nplt.subplots(figsize=(24,18))\nsns.heatmap(corrmat, vmax=0.9, square=True)","0d497a47":"train.hist(bins=20, figsize=(30,20))\nplt.show()","64288b0a":"test.hist(bins=20, figsize=(30,20))\nplt.show()","e87bd630":"train.describe()","e4c08276":"test.describe()","e71980ed":"a='''\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split\nX = train\ny = y_train\ntrain, val, y_train, y_val = train_test_split(X, y, test_size=0.20, random_state=201)#, stratify=y)'''","e586f33b":"print(\"Modelling: \", datetime.datetime.now())","6abeb596":"train","f5491454":"test","f4d66062":"y_train","38a55f50":"# need train index and test index\nuse_feature_selection = 0\nif (use_feature_selection == 1):\n    import pickle\n\n    with open('X_train_sfs_50.pkl', 'rb') as fid:\n        train = pickle.load(fid)\n    with open('X_test_sfs_50.pkl', 'rb') as fid:\n        test = pickle.load(fid)\n    train = pd.DataFrame(train, index=train_ID)\n    test = pd.DataFrame(test, index=test_ID)","a7b2b950":"train","765d5bf9":"norm = MinMaxScaler().fit(train)\ntrain_norm_arr = norm.transform(train)\ntest_norm_arr = norm.transform(test)\ntrain_norm = pd.DataFrame(norm.transform(train), index=train.index, columns=train.columns)\ntest_norm = pd.DataFrame(norm.transform(test), index=test.index, columns=test.columns)","01cc15f8":"train_norm","69272c6f":"# make copy of datasets\ntrain_stand = train.copy()\ntest_stand = test.copy()\n\n# apply standardization on numerical features\nfor i in all_numerical:\n    # fit on training data column\n    scale = StandardScaler().fit(train_stand[[i]])\n    # transform the training data column\n    train_stand[i] = scale.transform(train_stand[[i]])\n    # transform the testing data column\n    test_stand[i] = scale.transform(test_stand[[i]])","7cd2ec66":"train_stand","6ec06992":"test","3d3507e5":"from sklearn.linear_model import ElasticNet, Lasso, BayesianRidge, LassoLarsIC, LinearRegression, Ridge\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, ExtraTreesRegressor, AdaBoostRegressor, BaggingRegressor\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split, cross_validate\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\nfrom sklearn.svm import SVR\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\n\n#from sklearn.metrics import mean_squared_log_error\n# to run locally: conda install -c anaconda py-xgboost\nimport xgboost as xgb\nimport lightgbm as lgb\nfrom catboost import Pool, CatBoostRegressor","fedd1f67":"import keras\nimport math\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import Dropout\nfrom keras.layers import LSTM\nfrom keras.layers import Embedding\n#from keras.callbacks import ModelCheckpoint, TensorBoard\nfrom keras.callbacks import LearningRateScheduler, EarlyStopping, History, LambdaCallback\nfrom keras import regularizers\nfrom keras import backend as K\nfrom keras.layers import Conv1D\nfrom keras.layers import BatchNormalization\nfrom keras.layers import MaxPool1D\nfrom keras.layers import Flatten\nfrom keras.backend import sigmoid\nfrom keras.utils.generic_utils import get_custom_objects\nfrom keras.layers import Activation\nfrom keras.wrappers.scikit_learn import KerasRegressor\nfrom keras.optimizers import Adam   #for adam optimizer","cbaa99d5":"#method = 'ensemble'\nmethod = 'stacked'","cdda8fe7":"# v 75\n\nimport random as rn\nrn.seed(1) # random\nfrom numpy.random import seed\nseed(7) # or 7\nimport tensorflow as tf\ntf.random.set_seed(0) # tf\n\na='''\nfrom numpy.random import seed\nseed(1)\nseed = 7 # optimized\nnp.random.seed(seed)\nimport tensorflow as tf\ntf.random.set_seed(0) # tf\n'''","b5290fb9":"# define base model\nfrom keras.optimizers import Adam, SGD, RMSprop   #for adam optimizer\ndef baseline_model(dim=223, opt_sel=\"adam\", learning_rate = 0.001, neurons = 1, beta_1=0.9, beta_2=0.999, epsilon=1e-07, amsgrad=False, decay = 0.0002, momentum=0.9):\n    def bm():\n        # create model\n        model = Sequential()\n        #model.add(Dense(neurons, input_dim=223, kernel_initializer='normal', activation='relu'))\n        model.add(Dense(neurons, input_dim=dim, kernel_initializer='normal', activation='relu'))\n        model.add(Dense(1, kernel_initializer='normal'))\n        #model.add(Dense(1, kernel_initializer='normal')) # added to v86\n        # Compile model\n        if (opt_sel == \"adam\"):\n            #opt = Adam(lr=learning_rate, beta_1=beta_1, beta_2=beta_2, epsilon=epsilon, amsgrad=amsgrad) # added to v86\n            opt = Adam(lr=learning_rate)\n        elif(opt_sel == \"sgd\"):\n            opt = SGD(learning_rate=learning_rate, momentum=momentum, decay=decay, nesterov=True)\n        model.compile(loss='mean_squared_error', optimizer=opt)\n        return model\n    return bm","10642abb":"train_inputs =  preprocessing.scale(train)\nn_cols = train_inputs.shape[1]\ninput_shape = (n_cols, )\n# Creates a model given an activation and learning rate\n# Create the model object with default arguments\ndef create_model(learning_rate = 0.001, activation='relu'):\n  \n    # Set Adam optimizer with the given learning rate\n    opt = Adam(lr = learning_rate)\n  \n    # Create your binary classification model  \n    model = Sequential()\n    model.add(Dense(128,\n                    activation = activation,\n                    input_shape = input_shape,\n                    activity_regularizer = regularizers.l2(1e-5)))\n    model.add(Dropout(0.50))\n    model.add(Dense(128,\n                    activation = activation, \n                    activity_regularizer = regularizers.l2(1e-5)))\n    model.add(Dropout(0.50))\n    model.add(Dense(1, activation = activation))\n    # Compile the model\n    model.compile(optimizer = opt,\n                  #loss = \"mean_absolute_error\",\n                  loss = \"mean_squared_error\",\n                  metrics = ['mse', \"mape\"])\n    return model","addb9fea":"from sklearn.model_selection import RandomizedSearchCV\nfrom keras.optimizers import Adam   #for adam optimizer\noptimize_nn = 0\nif (optimize_nn == 1):\n    # Create a KerasClassifier object\n    model = KerasRegressor(build_fn = create_model,\n                           verbose = 0)\n    # Define the hyperparameter space\n    params = {'activation': [\"relu\"],#, \"tanh\"],\n              'batch_size': [1, 4],#, 2, 4], \n              'epochs': [100, 150, 200],\n              'neurons':[8, 16, 32],\n              'learning_rate': [0.01, 0.005, 0.001]}\n    # Create a randomize search cv object \n    random_search = RandomizedSearchCV(model,\n                                       param_distributions = params,\n                                       cv = KFold(10))\n    random_search_results = random_search.fit(train_inputs, y_train)\n    print(\"Best Score: \",\n          random_search_results.best_score_,\n          \"and Best Params: \",\n          random_search_results.best_params_)","52914c1c":"if (optimize_nn == 1):\n    model = KerasRegressor(build_fn = create_model,\n                           epochs = 100, \n                           batch_size = 16,\n                           verbose = 0)\n    # Calculate the accuracy score for each fold\n    kfolds = cross_val_score(model,\n                             train_inputs,\n                             train_targets,\n                             cv = 10)\n    # Print the mean accuracy\n    print('The mean accuracy was:', kfolds.mean())\n    # Print the accuracy standard deviation\n    print('With a standard deviation of:', kfolds.std())","86829728":"# define variable learning rate function\ndef step_decay(epoch, lr):\n    drop = 0.995 # was .999\n    epochs_drop = 175.0 # was 175, sgd likes 200+, adam likes 100\n    lrate = lr * math.pow(drop, math.floor((1+epoch)\/epochs_drop))\n    print(\"epoch=\" + str(epoch) + \" lr=\" + str(lr) + \" lrate=\" + str(lrate))\n    return lrate","7a9060f6":"# Print the batch number at the beginning of every batch.\n\nclass CustomCallback(keras.callbacks.Callback):\n    def __init__(self, train=None, validation=None):\n        super(CustomCallback, self).__init__()\n        self.validation = validation\n        self.train = train\n        \n    def on_train_begin(self, logs={}):\n        #val_loss_hist = []\n        #train_loss_hist = []\n        #lr_hist = []\n        self.val_loss_hist   = []\n        self.train_loss_hist = []\n        self.lr_hist         = []\n    \n    def on_epoch_end(self, epoch, logs=None):\n        keys = list(logs.keys())\n        #val_loss_hist.append([logs['val_loss']])\n        #train_loss_hist.append([logs['loss']])\n        #lr_hist.append([logs['lr']])\n        self.val_loss_hist.append([logs['val_loss']])\n        self.train_loss_hist.append([logs['loss']])\n        self.lr_hist.append([logs['lr']])\n        #print(\"End epoch {} of training; got log keys: {}\".format(epoch, keys))\n        \nlogging_callback = LambdaCallback(\n    on_epoch_end=lambda epoch, logs: print('val_loss:', logs['val_loss'])\n)","92dcb1b8":"# evaluate model # .0005 -> 54, .001 -> 53, .005 -> 48, .01 -> 55\nlrate = LearningRateScheduler(step_decay)\nearly_stopping = EarlyStopping(monitor='val_loss', patience=50, mode='auto', restore_best_weights = True)\ndnn_history = CustomCallback()\ncallbacks_list = [lrate, early_stopping, dnn_history] \n# num_epochs = 1000 # added in v86\nnum_epochs = 100\nkeras_optimizer = \"adam\"\n\nif (keras_optimizer == \"adam\"): # train loss 47, val loss 70\n    # v86 had learning_rate = 0.001, batch size 2\n    dnn = KerasRegressor(build_fn=baseline_model(dim=223, opt_sel=keras_optimizer, learning_rate = 0.005, neurons = 8, beta_1=0.9, beta_2=0.999, epsilon=1e-07, amsgrad=False), epochs=num_epochs, batch_size=1, verbose=1)\n    #dnn_meta = KerasRegressor(build_fn=baseline_model(dim=5, opt_sel=keras_optimizer, learning_rate = 0.001, neurons = 8, beta_1=0.9, beta_2=0.999, epsilon=1e-07, amsgrad=False), epochs=num_epochs, batch_size=2, verbose=1)\n\nelif (keras_optimizer == \"sgd\"): # loss 27, val loss 69\n    dnn = KerasRegressor(build_fn=baseline_model(dim=223, opt_sel=keras_optimizer, learning_rate=0.000005, neurons=32, decay=0.000001, momentum=0.9), epochs=num_epochs, batch_size=8, verbose=1)\n    # can't get sgd to give decent results, only adam works as a metamodel\n    #dnn_meta = KerasRegressor(build_fn=baseline_model(dim=5, opt_sel=keras_optimizer, learning_rate=0.000005, neurons=32, decay=0.000001, momentum=0.9), epochs=num_epochs, batch_size=8, verbose=1)\n\ndnn_meta = KerasRegressor(build_fn=baseline_model(dim=5, opt_sel=\"adam\", learning_rate = 0.001, neurons = 8, beta_1=0.9, beta_2=0.999, epsilon=1e-07, amsgrad=False), epochs=num_epochs, batch_size=2, verbose=1)\n    \nif (optimize_nn == 1):\n    kfold = KFold(n_splits=10)\n    results = cross_val_score(dnn, train, y_train, cv=kfold)\n    print(\"Baseline: %.2f (%.2f) MSE\" % (results.mean(), results.std()))","386a424b":"#dnn_pipe = Pipeline([('norm', MinMaxScaler()), ('dnn', dnn)])\n#dnn_pipe.fit(train_norm, y_train, dnn__shuffle=True, dnn__validation_split=0.3, dnn__callbacks=callbacks_list)\n#dnn_train_pred = inv_boxcox1p(dnn_pipe.predict(train_norm), lam_l)\n#dnn_pred = inv_boxcox1p(dnn_pipe.predict(test_norm), lam_l)","3b7f8a5f":"#dnn.fit(train, y_train, shuffle=True, validation_split=0.3, callbacks=callbacks_list) # added to v86\ndnn.fit(train, y_train)\ndnn_train_pred = inv_boxcox1p(dnn.predict(train), lam_l)\ndnn_pred = inv_boxcox1p(dnn.predict(test), lam_l)","100c58ba":"dnn_pred","818c4898":"dnn.get_params()","00e4c33f":"a='''\nnum_epochs = len(dnn_history.val_loss_hist)\nxc         = range(num_epochs)\nplt.figure()\nplt.plot(xc, dnn_history.train_loss_hist, label='train')\nplt.plot(xc, dnn_history.val_loss_hist, label='val')\nplt.plot(xc, dnn_history.lr_hist, label='lr')\nplt.show()'''","d5d8c922":"dnn_pred","ea6df033":"#Validation function\n# train and y_train are both log scaled so just need to take the square of the delta between them to calculate the error, then take the sqrt to get rmsle\n# but for now y_train is boxcox1p(), not log(). Use this to convert back: inv_boxcox1p(y_train, lam_l)\nn_folds=5 # was 5 => better score but twice as slow now\n\ndef rmsle_cv(model):\n    print(\"running rmsle_cv code\")\n    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(train) # was 42\n    rmse= np.sqrt(-cross_val_score(model, train, y_train, scoring=\"neg_mean_squared_error\", cv = kf)) # also r2\n    print(\"raw rmse scores for each fold:\", rmse)\n    return(rmse)\n\ndef r2_cv(model):\n    print(\"running r2_cv code\")\n    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(train) # was 42\n    r2= cross_val_score(model, train, y_train, scoring=\"r2\", cv = kf) # also r2\n    print(\"raw r2 scores for each fold:\", r2)\n    return(r2)\n\n# used for another competition\ndef mae_cv(model):\n    print(\"running mae_cv code\")\n    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(train) # was 42\n    mae = -cross_val_score(model, train, y_train, scoring=\"neg_mean_absolute_error\", cv = kf) # also r2\n    print(\"raw mae scores for each fold:\", mae)\n    return(mae)\n\ndef all_cv(model, n_folds, cv):\n    print(\"running cross_validate\")\n    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(train) # was 42\n    # other scores: https:\/\/scikit-learn.org\/stable\/modules\/model_evaluation.html#scoring-parameter\n    scorers = {\n        'r2': 'r2',\n        'nmsle': 'neg_mean_squared_log_error', \n        'nmse': 'neg_mean_squared_error',\n        'mae': 'neg_mean_absolute_error'\n    }\n    scores = cross_validate(model, train, y_train, scoring=scorers,\n                           cv=kf, return_train_score=True)\n    return(scores)","dabc2be3":"print(y_train.mean())\nprint(inv_boxcox1p(y_train, lam_l).mean())","c51be802":"def runGSCV(num_trials, features, y_values):\n    non_nested_scores = np.zeros(num_trials) # INCREASES BIAS\n    nested_scores = np.zeros(num_trials)\n    # Loop for each trial\n    for i in range(num_trials):\n        print(\"Running GridSearchCV:\")\n        with MyTimer():    \n            #grid_result = gsc.fit(train, y_train)  \n            grid_result = gsc.fit(features, y_values)  \n        non_nested_scores[i] = grid_result.best_score_\n        if (competition == 'SR'):\n            print(\"Best mae %f using %s\" % ( -grid_result.best_score_, grid_result.best_params_))\n        else:\n            print(\"Best rmse %f using %s\" % ( np.sqrt(-grid_result.best_score_), grid_result.best_params_))\n        \n        # nested\/non-nested cross validation: https:\/\/scikit-learn.org\/stable\/auto_examples\/model_selection\/plot_nested_cross_validation_iris.html\n        with MyTimer():    \n            #nested_score = cross_val_score(gsc, X=train, y=y_train, cv=outer_cv, verbose=0).mean() \n            nested_score = cross_val_score(gsc, X=features, y=y_values, cv=outer_cv, verbose=0).mean() \n            # source code for cross_val_score is here: https:\/\/github.com\/scikit-learn\/scikit-learn\/blob\/master\/sklearn\/model_selection\/_validation.py#L137\n        if (competition == 'SR'):\n            print(\"nested mae score from KFold %0.3f\" % -nested_score)\n        else:\n            print(\"nested rmse score from KFold %0.3f\" % np.sqrt(-nested_score))\n        \n        nested_scores[i] = nested_score\n        print('grid_result',grid_result)\n        print(\"mean scores: r2(%0.3f) mae(%0.3f) nmse(%0.3f) nmsle(%0.3f)\" % (grid_result.cv_results_['mean_test_r2'].mean(), -grid_result.cv_results_['mean_test_mae'].mean(),  np.sqrt(-grid_result.cv_results_['mean_test_nmse'].mean()), grid_result.cv_results_['mean_test_nmsle'].mean() ))\n        #print(\"mean scores: r2(%0.3f) nmse(%0.3f) mae(%0.3f)\" % (grid_result.cv_results_['mean_test_r2'].mean(), np.sqrt(-grid_result.cv_results_['mean_test_nmse'].mean()), grid_result.cv_results_['mean_test_mae'].mean()))\n    return grid_result\n","92ef58cc":"def calc_all_scores(model, n_folds=5, cv=5):\n    scores = all_cv(model, n_folds, cv)\n    #scores['train_<scorer1_name>'']\n    #scores['test_<scorer1_name>'']\n    print(\"\\n mae_cv score: {:.4f} ({:.4f})\\n\".format( (-scores['test_mae']).mean(), scores['test_mae'].std() ))\n    print(\"\\n rmsle_cv score: {:.4f} ({:.4f})\\n\".format( (np.sqrt(-scores['test_nmse'])).mean(), scores['test_nmse'].std() ))\n    print(\"\\n r2_cv score: {:.4f} ({:.4f})\\n\".format( scores['test_r2'].mean(), scores['test_r2'].std() ))\n    return (scores)\n\n# useful when you can't decide on parameter setting from best_params_\n# result_details(grid_result,'mean_test_nmse',100)\ndef result_details(grid_result,sorting='mean_test_nmse',cols=100):\n    param_df = pd.DataFrame.from_records(grid_result.cv_results_['params'])\n    param_df['mean_test_nmse'] = np.sqrt(-grid_result.cv_results_['mean_test_nmse'])\n    param_df['std_test_nmse'] = np.sqrt(grid_result.cv_results_['std_test_nmse'])\n    param_df['mean_test_mae'] = -grid_result.cv_results_['mean_test_mae']\n    param_df['std_test_mae'] = -grid_result.cv_results_['std_test_mae']\n    param_df['mean_test_r2'] = -grid_result.cv_results_['mean_test_r2']\n    param_df['std_test_r2'] = -grid_result.cv_results_['std_test_r2']\n    return param_df.sort_values(by=[sorting]).tail(cols)\n\ndef rmsle(y, y_pred):\n    return np.sqrt(mean_squared_error(y, y_pred))\n\ndef mae(y, y_pred):\n    return mean_absolute_error(y,y_pred)","01d25aa8":"# initialize the algorithm for the GridSearchCV function\nlasso = Lasso()\ntuningLasso = 1 # takes 2 minutes to complete\n\nif (tuningLasso == 1):\n    # use this when tuning\n    param_grid={\n        'alpha':[0.01,], # done, lower keeps getting better, but don't want to go too low and begin overfitting (alpha is related to L1 reg)\n        'fit_intercept':[True], # done, big difference\n        'normalize':[False], # done, big difference\n        'precompute':[False], # done, no difference\n        'copy_X':[True], # done, no difference\n        'max_iter':[200], # done\n        'tol':[0.05], # done, not much difference # was 0.005 but that would cause error: ConvergenceWarning: Objective did not converge\n        'warm_start':[False], # done, no difference\n        'positive':[False], # done, big difference\n        'random_state':[1],\n        'selection':['cyclic'] # done both are same, cyclic is default\n    }\n\nelse:\n    # use this when not tuning\n    param_grid={\n        'alpha':[0.2], \n        'fit_intercept':[True],\n        'normalize':[False],\n        'precompute':[False],\n        'copy_X':[True],\n        'max_iter':[200],\n        'tol':[0.0001],\n        'warm_start':[False],\n        'positive':[False],\n        'random_state':[None],\n        'selection':['cyclic']\n    }\n\nscorers = {\n    'r2': 'r2',\n    'nmsle': 'neg_mean_squared_log_error', \n    'nmse': 'neg_mean_squared_error',\n    'mae': 'neg_mean_absolute_error'\n}\n# To be used within GridSearch \ninner_cv = KFold(n_splits=4, shuffle=True, random_state=None)\n\n# To be used in outer CV \nouter_cv = KFold(n_splits=4, shuffle=True, random_state=None)    \n\n#inner loop KFold example:\ngsc = GridSearchCV(\n    estimator=lasso,\n    param_grid=param_grid,\n    #scoring='neg_mean_squared_error', # 'roc_auc', # or 'r2', etc\n    scoring=scorers, # 'roc_auc', # or 'r2', etc -> can output several scores, but only refit to one. Others are just calculated\n    #scoring='neg_mean_squared_error', # or look here for other choices \n    # https:\/\/scikit-learn.org\/stable\/modules\/model_evaluation.html#scoring-parameter\n    #cv=5,\n    cv=inner_cv, # this will use KFold splitting to change train\/test\/validation datasets randomly\n    verbose=0,\n    return_train_score=True, # keep the other scores\n    refit='nmse' # use this one for optimizing\n)\n\ngrid_result = runGSCV(2, train, y_train)\n\nrd = result_details(grid_result,'random_state',100)\nrd[['random_state','alpha','mean_test_nmse','std_test_nmse','mean_test_mae','std_test_mae','mean_test_r2','std_test_r2']].sort_values(by=['random_state','alpha'])\n","7557e0e2":"tuning_lasso = 1\nlasso = make_pipeline(RobustScaler(), Lasso(alpha =0.0005, tol=0.05, random_state=1)) # was 1 \nlasso_new = make_pipeline(RobustScaler(), Lasso(**grid_result.best_params_))\n#l = \"{'alpha': 0.2, 'copy_X': True, 'fit_intercept': True, 'max_iter': 200, 'normalize': False, 'positive': False, 'precompute': False, 'random_state': 1, 'selection': 'cyclic', 'tol': 0.005, 'warm_start': False}\"\n#Lasso_new = make_pipeline(RobustScaler(), Lasso(**l))\n#lasso_ss = make_pipeline(StandardScaler(), Lasso(alpha =0.0005, random_state=1)) # was 1 => worse score","5525873b":"if (tuning_lasso == 1):\n    #TEMP\n    model_results = [] # model flow, mae, rmsle\n    models = [lasso, lasso_new]\n\n    for model in models:\n        #print(model)\n        with MyTimer(): \n            scores = calc_all_scores(model,5,5)\n        #print(\"------------------------------------------\")\n        model_results.append([model, (-scores['test_mae']).mean(), (np.sqrt(-scores['test_nmse'])).mean(), scores['test_r2'].mean()])\n\n    df_mr = pd.DataFrame(model_results, columns = ['model','mae','rmsle','r2'])\n    df_mr.sort_values(by=['rmsle'])","6579d6d5":"lasso = make_pipeline(RobustScaler(), Lasso(alpha =0.0005, tol=0.05, random_state=1)) # was 1 \n\nif (tuning_lasso == 1):\n    for i in [2,5,20,42,99]:\n        from sklearn.linear_model import Lasso\n        print('random_state =',i)\n\n        l = {'alpha': 0.01, 'copy_X': True, 'fit_intercept': True, 'max_iter': 200, 'normalize': False, 'positive': False, 'precompute': False, 'selection': 'cyclic', 'tol': 0.05, 'warm_start': False}\n        lasso_new = make_pipeline(RobustScaler(), Lasso(**l, random_state=i))\n        #lasso_new = Lasso(**l, random_state=i)\n\n        model_results = [] # model flow, mae, rmsle\n        models = [lasso, lasso_new]\n\n        for model in models:\n            #print(model)\n            with MyTimer(): \n                scores = calc_all_scores(model,5,5)\n            #print(\"------------------------------------------\")\n            model_results.append([model, (-scores['test_mae']).mean(), (np.sqrt(-scores['test_nmse'])).mean(), scores['test_r2'].mean()])\n\n        df_mr = pd.DataFrame(model_results, columns = ['model','mae','rmsle','r2'])\n        print(df_mr.sort_values(by=['rmsle']))\nelse:\n    lasso_new = make_pipeline(RobustScaler(), Lasso(**grid_result.best_params_, random_state=i))","1631723c":"model_results","567d1586":"method","40075d00":"# initialize the algorithm for the GridSearchCV function\nif (method == \"ensemble\"):\n    enet_tol = 0.01 # or try 0.01 - default is 0.0001\n    ENet = ElasticNet(tol=enet_tol) # added tol=0.05 to avoid errors\n    tuningENet = 0 # takes 2 minutes to complete\n\n    if (tuningENet == 1):\n        # use this when tuning\n        param_grid={\n            'alpha':[0.01],\n            'l1_ratio':[0.75,0.8,0.85,0.9],\n            'fit_intercept':[True], # ,False\n            'normalize':[False], # True,\n            'max_iter':range(350,450,50),\n            'selection':['random'], # 'cyclic',\n            'random_state':[3],\n            'tol':[enet_tol]\n        }\n\n    else:\n        # use this when not tuning\n        param_grid={\n            'alpha':[0.01],\n            'l1_ratio':[.9],\n            'fit_intercept':[True],\n            'normalize':[False],\n            'max_iter':[350], # default 1000\n            'selection':['random'],\n            'random_state':[3],\n            'tol':[enet_tol]\n        }\n\nelse: # (method == \"stacked\")\n    enet_tol = 0.0001 # or try 0.01 \n    ENet = ElasticNet() # added tol=0.05 to avoid errors\n    tuningENet = 0 # takes 2 minutes to complete\n\n    if (tuningENet == 1):\n        # use this when tuning\n        param_grid={\n            'alpha':[0.01,0.05],\n            'l1_ratio':[0.8,0.85,0.9],\n            'fit_intercept':[True], # ,False\n            'normalize':[False], # True,\n            'max_iter':range(350,450,50),\n            'selection':['random'], # 'cyclic',\n            'random_state':[3],\n            'tol':[enet_tol]\n        }\n\n    else:\n        # use this when not tuning\n        param_grid={\n            'alpha':[0.05],\n            'l1_ratio':[.85],\n            'fit_intercept':[True],\n            'normalize':[False],\n            'max_iter':[500], # default 1000\n            'selection':['random'],\n            'random_state':[3],\n            'tol':[enet_tol]\n        }\n\nscorers = {\n    'r2': 'r2',\n    'nmsle': 'neg_mean_squared_log_error',\n    'nmse': 'neg_mean_squared_error',\n    'mae': 'neg_mean_absolute_error'\n}\n# To be used within GridSearch \ninner_cv = KFold(n_splits=4, shuffle=True, random_state=None)\n\n# To be used in outer CV \nouter_cv = KFold(n_splits=4, shuffle=True, random_state=None)    \n\n#inner loop KFold example:\ngsc = GridSearchCV(\n    estimator=ENet,\n    param_grid=param_grid,\n    scoring=scorers, # 'roc_auc', # or 'r2', etc -> can output several scores, but only refit to one. Others are just calculated\n    #scoring='neg_mean_squared_error', # or look here for other choices \n    # https:\/\/scikit-learn.org\/stable\/modules\/model_evaluation.html#scoring-parameter\n    #cv=5,\n    cv=inner_cv, # this will use KFold splitting to change train\/test\/validation datasets randomly\n    verbose=0,\n    return_train_score=True, # keep the other scores\n    refit='nmse' # use this one for optimizing\n)\n\ngrid_result = runGSCV(5, train, y_train)\n\nrd = result_details(grid_result,'mean_test_nmse',100)\nrd[['max_iter','l1_ratio','mean_test_nmse','std_test_nmse','mean_test_mae','std_test_mae','mean_test_r2','std_test_r2']]#.sort_values(by=['n_estimators','mean_test_nmse'])\n","be3f00a7":"grid_result.best_params_","0b53f96c":"rd = result_details(grid_result,'mean_test_nmse',100)\nrd[['max_iter','l1_ratio','mean_test_nmse','std_test_nmse','mean_test_mae','std_test_mae','mean_test_r2','std_test_r2']].sort_values(by=['max_iter','l1_ratio'])\n","e6c89b37":"#ENet_orig = make_pipeline(StandardScaler(), ElasticNet(alpha=0.0005, l1_ratio=.9, random_state=3))\nENet = make_pipeline(StandardScaler(), ElasticNet(**grid_result.best_params_))\nENet_new = make_pipeline(RobustScaler(), ElasticNet(**grid_result.best_params_))\n\npolynomial_features = PolynomialFeatures(degree=2, include_bias=False)\nENet_poly = Pipeline([(\"polynomial_features\", polynomial_features), (\"ENet\", ENet)])","06f7d791":"# initialize the algorithm for the GridSearchCV function\nlr1 = LinearRegression()\ntuningLR = 0 # takes 2 minutes to complete\n\nif (tuningLR == 1):\n    # use this when tuning\n    param_grid={\n        'fit_intercept':[True,False], \n        'normalize':[True,False]\n    }\n\nelse:\n    # use this when not tuning\n    param_grid={\n        'fit_intercept':[False], \n        'normalize':[False]\n    }\n\nscorers = {\n    'r2': 'r2',\n    #'nmsle': 'neg_mean_squared_log_error',\n    'nmse': 'neg_mean_squared_error',\n    'mae': 'neg_mean_absolute_error'\n}\n# To be used within GridSearch \ninner_cv = KFold(n_splits=4, shuffle=True, random_state=None)\n\n# To be used in outer CV \nouter_cv = KFold(n_splits=4, shuffle=True, random_state=None)    \n\n#inner loop KFold example:\ngsc = GridSearchCV(\n    estimator=lr1,\n    param_grid=param_grid,\n    scoring=scorers, # 'roc_auc', # or 'r2', etc -> can output several scores, but only refit to one. Others are just calculated\n    #scoring='neg_mean_squared_error', # or look here for other choices \n    # https:\/\/scikit-learn.org\/stable\/modules\/model_evaluation.html#scoring-parameter\n    #cv=5,\n    cv=inner_cv, # this will use KFold splitting to change train\/test\/validation datasets randomly\n    verbose=0,\n    return_train_score=True, # keep the other scores\n    refit='nmse' # use this one for optimizing\n)\n\ngrid_result = gsc.fit(train, y_train) \n#grid_result = runGSCV(2, train, y_train)\n\nrd = result_details(grid_result,'mean_test_nmse',100)\nrd[['fit_intercept','normalize','mean_test_nmse','std_test_nmse','mean_test_mae','std_test_mae','mean_test_r2','std_test_r2']]#.sort_values(by=['n_estimators','mean_test_nmse'])\n","0476569e":"polynomial_features = PolynomialFeatures(degree=2, include_bias=False)\nlr1 = LinearRegression(fit_intercept=True,normalize=False) # defaults fit_intercept=True, normalize=False\nfrom sklearn.feature_selection import f_regression, f_classif\n#lr_poly = Pipeline([(\"polynomial_features\", polynomial_features), (\"linear_regression\", LinearRegression(fit_intercept=True,normalize=False))])\n\n# using PCA\nlr_poly = Pipeline([(\"polynomial_features\", polynomial_features), ('reduce_dim', PCA(n_components=360)), (\"linear_regression\", LinearRegression())])\n# using NMF\n#lr_poly = Pipeline([(\"polynomial_features\", polynomial_features), ('reduce_dim', NMF(n_components=360, alpha=0.8, l1_ratio=0.8,random_state=0)), (\"linear_regression\", LinearRegression())])\n# using SelectKBest (score_func: also chi2, f_regression) - doesn't work, gives blank predictions and some very high and low predictions\n#lr_poly = Pipeline([(\"polynomial_features\", polynomial_features), ('reduce_dim', SelectKBest(score_func=f_regression,k=1000)), (\"linear_regression\", LinearRegression(normalize=True))])\n","b440def7":"trans = PolynomialFeatures(degree=2)\ndata = trans.fit_transform(train)\nprint(data)\nprint(data.shape)","dc57d0a5":"tune_kr = 1\nif (tune_kr == 1):\n    # initialize the algorithm for the GridSearchCV function\n    KRR = KernelRidge()\n    tuningKRR = 0 # this took 40 mins, 20 per iteration\n\n    if (tuningKRR == 1):\n        # use this when tuning\n        param_grid={\n            'alpha':[2.2,2.4], \n            'kernel':['polynomial'], #for entire list see: https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.metrics.pairwise.kernel_metrics.html#sklearn.metrics.pairwise.kernel_metrics\n            'gamma':[0.0001,0.001,0.01,0.1],\n            'degree':[1,2,3,4,5,6], \n            'coef0':[0.1,0.3,0.5,1.0,2.0]\n        }\n\n    else:\n        # use this when not tuning\n        # nmse: Best mae 583416973.611280 using {'alpha': 2.2, 'coef0': 0.5, 'degree': 5, 'gamma': 0.001, 'kernel': 'polynomial'}\n        # mae: Best mae 15805.764347 using {'alpha': 2.0, 'coef0': 0.1, 'degree': 5, 'gamma': 0.001, 'kernel': 'polynomial'}\n        param_grid={\n            'alpha':[2.2], \n            'kernel':['polynomial'], # 'linear', 'rbf'\n            'gamma':[0.001],\n            'degree':[4], \n            'coef0':[1.0]\n        }\n    scorers = {\n        'r2': 'r2',\n        'nmsle': 'neg_mean_squared_log_error',\n        'nmse': 'neg_mean_squared_error',\n        'mae': 'neg_mean_absolute_error'\n    }\n    # To be used within GridSearch \n    inner_cv = KFold(n_splits=4, shuffle=True, random_state=None)\n    # To be used in outer CV \n    outer_cv = KFold(n_splits=4, shuffle=True, random_state=None)    \n\n    #inner loop KFold example:\n    gsc = GridSearchCV(\n        estimator=KRR,\n        param_grid=param_grid,\n        scoring=scorers, # 'roc_auc', # or 'r2', etc -> can output several scores, but only refit to one. Others are just calculated\n        #scoring='neg_mean_squared_error', # or look here for other choices \n        # https:\/\/scikit-learn.org\/stable\/modules\/model_evaluation.html#scoring-parameter\n        #cv=5,\n        cv=inner_cv, # this will use KFold splitting to change train\/test\/validation datasets randomly\n        verbose=0,\n        return_train_score=True, # keep the other scores\n        refit='nmse' # use this one for optimizing\n    )\n\n    grid_result = runGSCV(2, train, y_train)\n\n    rd = result_details(grid_result,'mean_test_nmse',100)\n    rd[['alpha','degree','mean_test_nmse','std_test_nmse','mean_test_mae','std_test_mae','mean_test_r2','std_test_r2']]#.sort_values(by=['n_estimators','mean_test_nmse'])\n","f1793d60":"#KRR_orig = KernelRidge(alpha=0.6, kernel='polynomial', degree=2, coef0=2.5)\n\n#KRR = KernelRidge(**grid_result.best_params_)\nkrr = {'alpha': 2.2, 'coef0': 0.5, 'degree': 5, 'gamma': 0.001, 'kernel': 'polynomial'}\nKRR = KernelRidge(**krr)\n#KRR = KernelRidge(alpha=2.2, coef0=0.5, degree=5, gamma=0.001, kernel='polynomial')\n\nif (tune_kr == 1):\n    KRR_new = KernelRidge(**grid_result.best_params_)\nelse:\n    krr_new = {'alpha': 2.3, 'coef0': 1.0, 'degree': 4, 'gamma': 0.001, 'kernel': 'polynomial'}\n    KRR_new = KernelRidge(**krr)","aad6ee5d":"# initialize the algorithm for the GridSearchCV function\nrf = RandomForestRegressor()\ntuningRF = 0 # this took 2 hours last time, 1 hour per iteration\n\nif (tuningRF == 1):\n    # use this when tuning\n    param_grid={\n        'max_depth':[3,4,5],\n        'max_features':[None,'sqrt','log2'], \n        # 'max_features': range(50,401,50),\n        # 'max_features': [50,100], # can be list or range or other\n        'n_estimators':range(25,100,25), \n        #'class_weight':[None,'balanced'],  \n        'min_samples_leaf':range(5,15,5), \n        'min_samples_split':range(10,30,10), \n        'criterion':['mse', 'mae'] \n    }\n\nelse:\n    # use this when not tuning\n    param_grid={\n        'max_depth':[5],\n        'max_features':[None], # max_features is None is default and works here, removing 'sqrt','log2'\n        # 'max_features': range(50,401,50),\n        # 'max_features': [50,100], # can be list or range or other\n        'n_estimators': [50], # number of trees selecting 100, removing range(50,126,25)\n        #'class_weight':[None], # None was selected, removing 'balanced'\n        'min_samples_leaf': [5], #selecting 10, removing range 10,40,10)\n        'min_samples_split': [10], # selecting 20, removing range(20,80,10),\n        'criterion':['mse'] # remove gini as it is never selected\n    }\n\nscorers = {\n    'r2': 'r2',\n    'nmsle': 'neg_mean_squared_log_error',\n    'nmse': 'neg_mean_squared_error',\n    'mae': 'neg_mean_absolute_error'\n}\n# To be used within GridSearch \ninner_cv = KFold(n_splits=4, shuffle=True, random_state=None)\n\n# To be used in outer CV \nouter_cv = KFold(n_splits=4, shuffle=True, random_state=None)    \n\n#inner loop KFold example:\ngsc = GridSearchCV(\n    estimator=rf,\n    param_grid=param_grid,\n    scoring=scorers, # 'roc_auc', # or 'r2', etc -> can output several scores, but only refit to one. Others are just calculated\n    #scoring='neg_mean_squared_error', # or look here for other choices \n    # https:\/\/scikit-learn.org\/stable\/modules\/model_evaluation.html#scoring-parameter\n    #cv=5,\n    cv=inner_cv, # this will use KFold splitting to change train\/test\/validation datasets randomly\n    verbose=0,\n    return_train_score=True, # keep the other scores\n    refit='nmse' # use this one for optimizing\n)\n\ngrid_result = runGSCV(2, train, y_train)\n\nrd = result_details(grid_result,'mean_test_nmse',100)\nrd[['n_estimators','mean_test_nmse','std_test_nmse','mean_test_mae','std_test_mae','mean_test_r2','std_test_r2']]#.sort_values(by=['n_estimators','mean_test_nmse'])\n","798d6b0c":"#RF_orig = make_pipeline(StandardScaler(), RandomForestRegressor(max_depth=3,n_estimators=500))\nRF = make_pipeline(StandardScaler(), RandomForestRegressor(**grid_result.best_params_)) # better than default, but still not good\nRF_new = make_pipeline(RobustScaler(), RandomForestRegressor(**grid_result.best_params_)) # better than default, but still not good","8221c5e6":"print(\"Optimize GBoost: \", datetime.datetime.now())","b731bc44":"GBoost = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05,\n                                   max_depth=4, max_features='sqrt',\n                                   min_samples_leaf=15, min_samples_split=10, \n                                   loss='huber', random_state=5) # was 5\n# learning_ratefloat, default=0.1\n# learning rate shrinks the contribution of each tree by learning_rate. There is a trade-off between learning_rate and n_estimators.\ntuning_gb = 0\nif (tuning_gb == 1):\n    # initialize the algorithm for the GridSearchCV function\n    GBoost_new = GradientBoostingRegressor()\n    tuningGB = 1\n    if (tuningGB == 1):\n        # use this when tuning\n        param_grid={\n            #'loss':['ls','lad','huber','quantile'],\n            'loss':['huber'], # done\n            'learning_rate':[0.05],\n            'n_estimators':[3000], # done\n            'subsample':[1.0],\n            #'criterion':['friedman_mse','mse','mae'],\n            'criterion':['friedman_mse'], # done\n            'min_samples_split':[10],\n            'min_samples_leaf':[15],\n            'min_weight_fraction_leaf':[0.0],\n            'max_depth':[2,3,4], # done\n            'min_impurity_decrease':[0.0],\n            'min_impurity_split':[None],\n            'init':[None],\n            'random_state':[None],\n            #'max_features':[None,'auto','sqrt','log2'],\n            'max_features':['sqrt'], # done\n            'alpha':[0.60], # done\n            'verbose':[0],\n            'max_leaf_nodes':[None],\n            'warm_start':[False],\n            'presort':['deprecated'],\n            'validation_fraction':[0.1],\n            'n_iter_no_change':[None],\n            'tol':[0.0001],\n            'ccp_alpha':[0.0],\n            'random_state':[5,20,42]\n        }\n    else:\n        # use this when not tuning\n        param_grid={\n            'loss':['huber'], \n            'learning_rate':[0.05],\n            'n_estimators':[3000], \n            'subsample':[1.0],\n            'criterion':['friedman_mse'], \n            'min_samples_split':[10],\n            'min_samples_leaf':[15],\n            'min_weight_fraction_leaf':[0.0],\n            'max_depth':[2], \n            'min_impurity_decrease':[0.0],\n            'min_impurity_split':[None],\n            'init':[None],\n            'random_state':[None],\n            'max_features':['sqrt'], \n            'alpha':[0.60], \n            'verbose':[0],\n            'max_leaf_nodes':[None],\n            'warm_start':[False],\n            'presort':['deprecated'],\n            'validation_fraction':[0.1],\n            'n_iter_no_change':[None],\n            'tol':[0.0001],\n            'ccp_alpha':[0.0],\n            'random_state':[5]\n        }\n    scorers = {\n        'r2': 'r2',\n        'nmsle': 'neg_mean_squared_log_error',\n        'nmse': 'neg_mean_squared_error',\n        'mae': 'neg_mean_absolute_error'\n    }\n    # To be used within GridSearch \n    inner_cv = KFold(n_splits=4, shuffle=True, random_state=None)\n    # To be used in outer CV \n    outer_cv = KFold(n_splits=4, shuffle=True, random_state=None)    \n\n    #inner loop KFold example:\n    gsc = GridSearchCV(\n        estimator=GBoost_new,\n        param_grid=param_grid,\n        scoring=scorers, # 'roc_auc', # or 'r2', etc -> can output several scores, but only refit to one. Others are just calculated\n        cv=inner_cv, # this will use KFold splitting to change train\/test\/validation datasets randomly\n        verbose=0,\n        return_train_score=True, # keep the other scores\n        refit='nmse' # use this one for optimizing\n    )\n\n    grid_result = runGSCV(2, subtrain, y_subtrain)\n\nrd = result_details(grid_result,'mean_test_nmse',100)\nrd[['criterion','max_depth','mean_test_nmse','std_test_nmse','mean_test_mae','std_test_mae','mean_test_r2','std_test_r2']]#.sort_values(by=['n_estimators','mean_test_nmse'])\n","bae77066":"rd[['criterion','max_depth','mean_test_nmse','std_test_nmse','mean_test_mae','std_test_mae','mean_test_r2','std_test_r2']].sort_values(by=['criterion','max_depth'])\n","ec66975b":"if (tuning_gb == 1):\n    GBoost_new = GradientBoostingRegressor(**grid_result.best_params_)\nelse:\n    gbr  = {'alpha': 0.6, 'ccp_alpha': 0.0, 'criterion': 'friedman_mse', 'init': None, 'learning_rate': 0.05, 'loss': 'huber', 'max_depth': 3, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_impurity_split': None, 'min_samples_leaf': 15, 'min_samples_split': 10, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 3000, 'n_iter_no_change': None, 'presort': 'deprecated', 'random_state': 5, 'subsample': 1.0, 'tol': 0.0001, 'validation_fraction': 0.1, 'verbose': 0, 'warm_start': False}\n    GBoost_new = GradientBoostingRegressor(**gbr)","d04ddaed":"if (tuning_gb == 1):\n    #TEMP\n    model_results = [] # model flow, mae, rmsle\n    models = [GBoost, GBoost_new]\n\n    for model in models:\n        #print(model)\n        with MyTimer(): \n            scores = calc_all_scores(model,5,5)\n        #print(\"------------------------------------------\")\n        model_results.append([model, (-scores['test_mae']).mean(), (np.sqrt(-scores['test_nmse'])).mean(), scores['test_r2'].mean()])\n\n    df_mr = pd.DataFrame(model_results, columns = ['model','mae','rmsle','r2'])\n    df_mr.sort_values(by=['rmsle'])","4ecfb67a":"if (tuning_gb == 1):\n    GBoost.fit(subtrain.values, y_subtrain)\n    gboost_train_pred = inv_boxcox1p(GBoost.predict(subtrain.values), lam_l)\n    gboost_val_pred = inv_boxcox1p(GBoost.predict(val.values), lam_l)\n    print('GBoost')\n    print('train results')\n    print(mae(y_subtrain, gboost_train_pred))\n    print(rmsle(y_subtrain, gboost_train_pred))\n    print('test results')\n    print(mae(y_val, gboost_val_pred))\n    print(rmsle(y_val, gboost_val_pred))\n\n    GBoost_new.fit(subtrain.values, y_subtrain)\n    gboost_train_pred = inv_boxcox1p(GBoost_new.predict(subtrain.values), lam_l)\n    gboost_val_pred = inv_boxcox1p(GBoost_new.predict(val.values), lam_l)\n    print('GBoost_new')\n    print('train results')\n    print(mae(y_subtrain, gboost_train_pred))\n    print(rmsle(y_subtrain, gboost_train_pred))\n    print('test results')\n    print(mae(y_val, gboost_val_pred))\n    print(rmsle(y_val, gboost_val_pred))","1924d13f":"if (tuning_gb == 1):\n    GBoost_new.fit(train, y_train)\n    gboost_pred = inv_boxcox1p(GBoost_new.predict(test), lam_l)\n    gboost_pred\n    sub = pd.DataFrame()\n    #sub['Id'] = test['Id']\n    sub['Id'] = test_ID\n    sub['SalePrice'] = gboost_pred\n    sub.to_csv('submission.csv',index=False)","02b105ef":"if (tuning_gb == 1):\n    GBoost.fit(train, y_train)\n    gboost_pred = inv_boxcox1p(GBoost.predict(test), lam_l)\n    gboost_pred\n    sub = pd.DataFrame()\n    #sub['Id'] = test['Id']\n    sub['Id'] = test_ID\n    sub['SalePrice'] = gboost_pred\n    sub.to_csv('submission.csv',index=False)","69b63e90":"if (tuning_gb == 1):\n\n    model_results = [] # model flow, mae, rmsle\n    models = [GBoost, GBoost_new]\n\n    for model in models:\n        #print(model)\n        with MyTimer(): \n            scores = calc_all_scores(model,10,10)\n        #print(\"------------------------------------------\")\n        model_results.append([model, (-scores['test_mae']).mean(), (np.sqrt(-scores['test_nmse'])).mean(), scores['test_r2'].mean()])\n\n    df_mr = pd.DataFrame(model_results, columns = ['model','mae','rmsle','r2'])\n    df_mr.sort_values(by=['rmsle'])","4ed87e4c":"print(\"Optimize XGB: \", datetime.datetime.now())","83e69337":"tuning_xgb = 0\nif (tuning_xgb == 1):\n    # initialize the algorithm for the GridSearchCV function# initialize the algorithm for the GridSearchCV function\n    xgb1 = xgb.XGBRegressor()\n    tuningXGB = 1 # this took 2 hours last time, 1 hour per iteration\n\n    if (tuningXGB == 1):\n        # use this when tuning\n        param_grid={\n            'colsample_bytree':[0.4603],\n            'gamma':[0.0468], # done - all values almost identical results\n            'colsample_bylevel':[0.3], # done - all give same result\n            'objective':['reg:squarederror'], # done - Default:'reg:squarederror', None, reg:pseudohubererror, reg:squaredlogerror, reg:gamma\n            'booster':['gbtree'], # done - Default: 'gbtree', 'gblinear' or 'dart'\n            'learning_rate':[0.04], # done\n            'max_depth':[3], # - done\n            'importance_type':['gain'], # done - all give same value, Default:'gain', 'weight', 'cover', 'total_gain' or 'total_cover'\n            'min_child_weight':[1.7817], # done - no difference with several values\n            'n_estimators':[1000], # done\n            'reg_alpha':[0.4], # done\n            'reg_lambda':[0.8571], # done\n            'subsample':[0.5], # done\n            'silent':[1],\n            'random_state':[35],\n            'scale_pos_weight':[1],\n            'eval_metric':['rmse'], # done - all options have same results  Default:rmse for regression rmse, mae, rmsle, logloss, cox-nloglik\n            #'nthread ':[-1],\n            'verbosity':[0]\n        }\n\n    else:\n        # use this when not tuning\n        param_grid={\n            'colsample_bytree':[0.4603],\n            'gamma':[0.0468],\n            'colsample_bylevel':[0.3],\n            'objective':['reg:squarederror'], # 'binary:logistic', 'reg:squarederror', 'rank:pairwise', None\n            'booster':['gbtree'], # 'gbtree', 'gblinear' or 'dart'\n            'learning_rate':[0.04],\n            'max_depth':[3],\n            'importance_type':['gain'], # 'gain', 'weight', 'cover', 'total_gain' or 'total_cover'\n            'min_child_weight':[1.7817],\n            'n_estimators':[1000],\n            'reg_alpha':[0.4],\n            'reg_lambda':[0.8571],\n            'subsample':[0.5],\n            'silent':[1],\n            'random_state':[35],\n            'scale_pos_weight':[1],\n            'eval_metric':['rmse'],\n            #'nthread ':[-1],\n            'nthread ':[-1],\n            'verbosity':[0]\n        }\n\n    scorers = {\n        'r2': 'r2',\n        'nmsle': 'neg_mean_squared_log_error',\n        'nmse': 'neg_mean_squared_error',\n        'mae': 'neg_mean_absolute_error'\n    }\n    # To be used within GridSearch \n    inner_cv = KFold(n_splits=4, shuffle=True, random_state=None)\n\n    # To be used in outer CV \n    outer_cv = KFold(n_splits=4, shuffle=True, random_state=None)    \n\n    #inner loop KFold example:\n    gsc = GridSearchCV(\n        estimator=xgb1,\n        param_grid=param_grid,\n        scoring=scorers, # 'roc_auc', # or 'r2', etc -> can output several scores, but only refit to one. Others are just calculated\n        #scoring='neg_mean_squared_error', # or look here for other choices \n        # https:\/\/scikit-learn.org\/stable\/modules\/model_evaluation.html#scoring-parameter\n        #cv=5,\n        cv=inner_cv, # this will use KFold splitting to change train\/test\/validation datasets randomly\n        verbose=0,\n        return_train_score=True, # keep the other scores\n        refit='nmse' # use this one for optimizing\n    )\n\n    grid_result = runGSCV(2, train, y_train)\n","e89df754":"if (tuning_xgb == 1):\n    rd = result_details(grid_result,'mean_test_nmse',100)\n\n    rd[['random_state','eval_metric','mean_test_nmse','std_test_nmse','mean_test_mae','std_test_mae','mean_test_r2','std_test_r2']].sort_values(by=['random_state','mean_test_nmse'])\n","bb1e6066":"model_xgb = xgb.XGBRegressor(booster='gbtree',colsample_bytree=0.4603, gamma=0.0468, # colsample_bylevel, objective, booster (gbtree, gblinear or dart.) # Default: 'gbtree'\n                             learning_rate=0.05, max_depth=3, # importance_type (\u201cgain\u201d, \u201cweight\u201d, \u201ccover\u201d, \u201ctotal_gain\u201d or \u201ctotal_cover\u201d.)\n                             min_child_weight=1.7817, n_estimators=2200,\n                             reg_alpha=0.4640, reg_lambda=0.8571,\n                             subsample=0.5213, silent=1,\n                             random_state=35) # was random_state=7, cannot set to None \n\nif (tuning_xgb == 1):\n    import xgboost as xgb\n    for i in [2,5,20,42,99]:\n        print('random_state =',i)\n\n        model_xgb_new = xgb.XGBRegressor(booster='gbtree',colsample_bytree=0.4603, gamma=0.0468, # colsample_bylevel, objective, booster (gbtree, gblinear or dart.) # Default: 'gbtree'\n                                 learning_rate=0.04, max_depth=3, # importance_type (\u201cgain\u201d, \u201cweight\u201d, \u201ccover\u201d, \u201ctotal_gain\u201d or \u201ctotal_cover\u201d.)\n                                 min_child_weight=1.7817, n_estimators=1000,\n                                 reg_alpha=0.4, reg_lambda=0.8571,\n                                 subsample=0.45, silent=1,\n                                 random_state=i) # was random_state=7, cannot set to None \n\n        model_xgb_new2 = xgb.XGBRegressor(booster='gbtree',colsample_bytree=0.4603, gamma=0.0468, # colsample_bylevel, objective, booster (gbtree, gblinear or dart.) # Default: 'gbtree'\n                                 learning_rate=0.04, max_depth=3, # importance_type (\u201cgain\u201d, \u201cweight\u201d, \u201ccover\u201d, \u201ctotal_gain\u201d or \u201ctotal_cover\u201d.)\n                                 min_child_weight=1.7817, n_estimators=1000,\n                                 reg_alpha=0.4, reg_lambda=0.8571,\n                                 subsample=0.5, silent=1,\n                                 random_state=i) # was random_state=7, cannot set to None \n\n        model_xgb_new3 = xgb.XGBRegressor(booster='gbtree',colsample_bytree=0.4603, gamma=0.0468, # colsample_bylevel, objective, booster (gbtree, gblinear or dart.) # Default: 'gbtree'\n                                 learning_rate=0.04, max_depth=3, # importance_type (\u201cgain\u201d, \u201cweight\u201d, \u201ccover\u201d, \u201ctotal_gain\u201d or \u201ctotal_cover\u201d.)\n                                 min_child_weight=1.7817, n_estimators=1000,\n                                 reg_alpha=0.4, reg_lambda=0.8571,\n                                 subsample=0.5213, silent=1,\n                                 random_state=i) # was random_state=7, cannot set to None\n\n        model_results = [] # model flow, mae, rmsle\n        models = [model_xgb, model_xgb_new, model_xgb_new2, model_xgb_new3]\n\n        for model in models:\n            #print(model)\n            with MyTimer(): \n                scores = calc_all_scores(model,5,5)\n            #print(\"------------------------------------------\")\n            model_results.append([model, (-scores['test_mae']).mean(), (np.sqrt(-scores['test_nmse'])).mean(), scores['test_r2'].mean()])\n\n        df_mr = pd.DataFrame(model_results, columns = ['model','mae','rmsle','r2'])\n        print(df_mr.sort_values(by=['rmsle']))\nelse:\n    model_xgb_new = xgb.XGBRegressor(**grid_result.best_params_)","a7e54a1a":"show_metrics = 0\nif (show_metrics == 1):\n    import graphviz\n    model_xgb.fit(train, y_train,  verbose=False) #  eval_set=[(X_test, y_test)]\n    xgb.plot_importance(model_xgb)\n    xgb.to_graphviz(model_xgb, num_trees=20)","a173b387":"print(\"Optimize LightGBM: \", datetime.datetime.now())","516e6401":"tuning_lgb = 0\nif (tuning_lgb == 1):\n    lgb1 = lgb.LGBMRegressor()\n    tuningLGB = 0\n\n    if (tuningLGB == 1):\n        # use this when tuning\n        param_grid={\n            'objective':['regression'], # - only one option for regression\n            'boosting_type':['gbdt'], # - done gbdt dart goss rf\n            'num_leaves':[5,6], # - done\n            'learning_rate':[0.05], # - done\n            'n_estimators':[650,750], # - done\n            'max_bin':[45,55], # - done\n            'bagging_fraction':[0.85], # - done\n            'bagging_freq':[5], # - done\n            'feature_fraction':[0.2319], # - done\n            'feature_fraction_seed':[9], \n            'bagging_seed':[9],\n            'min_data_in_leaf':[9], # - done\n            'min_sum_hessian_in_leaf':[11], # - done\n            'max_depth':[-1], # - -1 means no limit\n            'subsample_for_bin':[500,1000], # - done\n            'class_weight':[None],\n            'min_split_gain':[0.0],\n            'min_child_weight':[0.001],\n            'min_child_samples':[5], # - done\n            'subsample':[1.0],\n            'subsample_freq':[0],\n            'colsample_bytree':[1.0],\n            'reg_alpha':[0.0], # - l1 regularization done\n            'reg_lambda':[0.0], # - L2 regularization done\n            'random_state':[1],\n            'importance_type':['split'] # - done\n        }\n    else:\n        # use this when not tuning\n        param_grid={\n            'objective':['regression'], # - only one option for regression\n            'boosting_type':['gbdt'], # - done gbdt dart goss rf\n            'num_leaves':[5], # - done, maybe 5 is okay too\n            'learning_rate':[0.05], # - done\n            'n_estimators':[650], # - done\n            'max_bin':[55], # - done\n            'bagging_fraction':[0.85], # - done\n            'bagging_freq':[5], # - done\n            'feature_fraction':[0.2319], # - done\n            'feature_fraction_seed':[9], \n            'bagging_seed':[9],\n            'min_data_in_leaf':[9], # - done\n            'min_sum_hessian_in_leaf':[11], # - done\n            'max_depth':[-1], # - -1 means no limit\n            'subsample_for_bin':[1000], # - done\n            'class_weight':[None],\n            'min_split_gain':[0.0],\n            'min_child_weight':[0.001],\n            'min_child_samples':[5], # - done\n            'subsample':[1.0],\n            'subsample_freq':[0],\n            'colsample_bytree':[1.0],\n            'reg_alpha':[0.0], # - l1 regularization done\n            'reg_lambda':[0.0], # - L2 regularization done\n            'random_state':[1],\n            'importance_type':['split'] # - done\n        }\n\n    scorers = {\n        'r2': 'r2',\n        'nmsle': 'neg_mean_squared_log_error',\n        'nmse': 'neg_mean_squared_error',\n        'mae': 'neg_mean_absolute_error'\n    }\n    # To be used within GridSearch \n    inner_cv = KFold(n_splits=4, shuffle=True, random_state=None)\n\n    # To be used in outer CV \n    outer_cv = KFold(n_splits=4, shuffle=True, random_state=None)    \n\n    #inner loop KFold example:\n    gsc = GridSearchCV(\n        estimator=lgb1,\n        param_grid=param_grid,\n        scoring=scorers, # 'roc_auc', # or 'r2', etc -> can output several scores, but only refit to one. Others are just calculated\n        #scoring='neg_mean_squared_error', # or look here for other choices \n        # https:\/\/scikit-learn.org\/stable\/modules\/model_evaluation.html#scoring-parameter\n        #cv=5,\n        cv=inner_cv, # this will use KFold splitting to change train\/test\/validation datasets randomly\n        verbose=0,\n        return_train_score=True, # keep the other scores\n        refit='nmse' # use this one for optimizing\n    )\n\n    grid_result = runGSCV(2, train, y_train)\n","d5613623":"model_lgb = lgb.LGBMRegressor(objective='regression',num_leaves=5,\n                              learning_rate=0.05, n_estimators=720,\n                              max_bin = 55, bagging_fraction = 0.8,\n                              bagging_freq = 5, feature_fraction = 0.2319,\n                              feature_fraction_seed=9, bagging_seed=9, random_state=10,\n                              min_data_in_leaf =6, min_sum_hessian_in_leaf = 11)\nif (tuning_lgb == 1):\n    model_lgb_new = lgb.LGBMRegressor(**grid_result.best_params_)\nelse:\n    lgbm = {'bagging_fraction': 0.85, 'bagging_freq': 5, 'bagging_seed': 9, 'boosting_type': 'gbdt', 'class_weight': None, 'colsample_bytree': 1.0, 'feature_fraction': 0.2319, 'feature_fraction_seed': 9, 'importance_type': 'split', 'learning_rate': 0.05, 'max_bin': 55, 'max_depth': -1, 'min_child_samples': 5, 'min_child_weight': 0.001, 'min_data_in_leaf': 9, 'min_split_gain': 0.0, 'min_sum_hessian_in_leaf': 11, 'n_estimators': 650, 'num_leaves': 5, 'objective': 'regression', 'random_state': None, 'reg_alpha': 0.0, 'reg_lambda': 0.0, 'subsample': 1.0, 'subsample_for_bin': 1000, 'subsample_freq': 0}\n    model_lgb_new = lgb.LGBMRegressor(**lgbm)","59d5812d":"# initialize the algorithm for the GridSearchCV function\nbr = BayesianRidge()\ntuningBR = 1 # this took 2 hours last time, 1 hour per iteration\n\nif (tuningBR == 1):\n    # use this when tuning\n    param_grid={\n        'n_iter':[50],\n        'tol':[0.001],\n        'alpha_1':[1e-06],\n        'alpha_2':[1e-05],\n        'lambda_1':[1e-05],\n        'lambda_2':[1e-06],\n        'alpha_init':[None],\n        'lambda_init':[None],\n        'compute_score':[True,False],\n        'fit_intercept':[True,False],\n        'normalize':[False,True],\n        'copy_X':[True],\n        'verbose':[False]\n    }\n\nelse:\n    # use this when not tuning\n    param_grid={\n        'n_iter':[50],\n        'tol':[0.001],\n        'alpha_1':[1e-06],\n        'alpha_2':[1e-05],\n        'lambda_1':[1e-05],\n        'lambda_2':[1e-06],\n        'alpha_init':[None],\n        'lambda_init':[None],\n        'compute_score':[True,False],\n        'fit_intercept':[True,False],\n        'normalize':[False,True],\n        'copy_X':[True],\n        'verbose':[False]\n    }\n\nscorers = {\n    'r2': 'r2',\n    'nmsle': 'neg_mean_squared_log_error',\n    'nmse': 'neg_mean_squared_error',\n    'mae': 'neg_mean_absolute_error'\n}\n# To be used within GridSearch \ninner_cv = KFold(n_splits=4, shuffle=True, random_state=None)\n\n# To be used in outer CV \nouter_cv = KFold(n_splits=4, shuffle=True, random_state=None)    \n\n#inner loop KFold example:\ngsc = GridSearchCV(\n    estimator=br,\n    param_grid=param_grid,\n    scoring=scorers, # 'roc_auc', # or 'r2', etc -> can output several scores, but only refit to one. Others are just calculated\n    #scoring='neg_mean_squared_error', # or look here for other choices \n    # https:\/\/scikit-learn.org\/stable\/modules\/model_evaluation.html#scoring-parameter\n    #cv=5,\n    cv=inner_cv, # this will use KFold splitting to change train\/test\/validation datasets randomly\n    verbose=0,\n    return_train_score=True, # keep the other scores\n    refit='nmse' # use this one for optimizing\n)\n\ngrid_result = runGSCV(2, train, y_train)","24c12053":"tuning_br = 0\nBR = BayesianRidge()\nif (tuning_br == 1):\n    BR_new = BayesianRidge(**grid_result.best_params_)","b2cfd61f":"rd = result_details(grid_result,'alpha_1',100)\nrd[['alpha_1','alpha_2','mean_test_nmse','std_test_nmse','mean_test_mae','std_test_mae','mean_test_r2','std_test_r2']]","10548af7":"if (tuning_br == 1):\n    model_results = [] # model flow, mae, rmsle\n    models = [BR, BR_new]\n\n    for model in models:\n        #print(model)\n        with MyTimer(): \n            scores = calc_all_scores(model,10,10)\n        #print(\"------------------------------------------\")\n        model_results.append([model, (-scores['test_mae']).mean(), (np.sqrt(-scores['test_nmse'])).mean(), scores['test_r2'].mean()])\n\n    df_mr = pd.DataFrame(model_results, columns = ['model','mae','rmsle','r2'])\n    df_mr.sort_values(by=['rmsle'])","2f7afa28":"# initialize the algorithm for the GridSearchCV function\n# defaults are best\nET = ExtraTreesRegressor()\ntuningET = 0 # this took 2 hours last time, 1 hour per iteration\n\nif (tuningET == 1):\n    # use this when tuning\n    param_grid={\n        'n_estimators':[100], # revisit at possibly 2000, but this algorithm becomes really slow for large values\n        'criterion':['mse'], # done Default: mse, mae\n        'max_depth':[None], # done - above 30 result converges to None\n        'min_samples_split':[2], # done - inconsistently better\n        'min_samples_leaf':[1],\n        'min_weight_fraction_leaf':[0.0],\n        'max_features':['auto'], # done - Default:\u201cauto\u201d, \u201csqrt\u201d, \u201clog2\u201d, None\n        'max_leaf_nodes':[None],\n        'min_impurity_decrease':[0.0],\n        'min_impurity_split':[None],\n        'bootstrap':[False],\n        'oob_score':[False], # done - True doesn't work, results in a nan value\n        'n_jobs':[None],\n        'random_state':[1,5,42,55,98],\n        'verbose':[0],\n        'warm_start':[False],\n        'ccp_alpha':[0.0],\n        'max_samples':[None] # done - no difference\n    }\n\nelse:\n    # use this when not tuning\n    param_grid={\n        'n_estimators':[100],\n        'criterion':['mse'],\n        'max_depth':[None],\n        'min_samples_split':[2],\n        'min_samples_leaf':[1],\n        'min_weight_fraction_leaf':[0.0],\n        'max_features':['auto'],\n        'max_leaf_nodes':[None],\n        'min_impurity_decrease':[0.0],\n        'min_impurity_split':[None],\n        'bootstrap':[False],\n        'oob_score':[False],\n        'n_jobs':[None],\n        'random_state':[None],\n        'verbose':[0],\n        'warm_start':[False],\n        'ccp_alpha':[0.0],\n        'max_samples':[None]\n    }\n\nscorers = {\n    'r2': 'r2',\n    'nmsle': 'neg_mean_squared_log_error',\n    'nmse': 'neg_mean_squared_error',\n    'mae': 'neg_mean_absolute_error'\n}\n# To be used within GridSearch \ninner_cv = KFold(n_splits=4, shuffle=True, random_state=None)\n\n# To be used in outer CV \nouter_cv = KFold(n_splits=4, shuffle=True, random_state=None)    \n\n#inner loop KFold example:\ngsc = GridSearchCV(\n    estimator=ET,\n    param_grid=param_grid,\n    scoring=scorers, # 'roc_auc', # or 'r2', etc -> can output several scores, but only refit to one. Others are just calculated\n    #scoring='neg_mean_squared_error', # or look here for other choices \n    # https:\/\/scikit-learn.org\/stable\/modules\/model_evaluation.html#scoring-parameter\n    #cv=5,\n    cv=inner_cv, # this will use KFold splitting to change train\/test\/validation datasets randomly\n    verbose=0,\n    return_train_score=True, # keep the other scores\n    refit='nmse' # use this one for optimizing\n)\n\ngrid_result = runGSCV(2, train, y_train)","e673492a":"rd = result_details(grid_result,'mean_test_nmse',100)\nrd[['random_state','n_estimators','mean_test_nmse','std_test_nmse','mean_test_mae','std_test_mae','mean_test_r2','std_test_r2']]","ed45b98a":"#ET = make_pipeline(RobustScaler(), ExtraTreesRegressor()) # was 1 Tree algorithms don't need scaling\ntuning_et = 0\nif (tuning_et == 1):\n    for i in [2,5,20,42,99]:\n        print('random_state =',i)\n        ET = ExtraTreesRegressor(random_state=i)\n        #ET2 = make_pipeline(StandardScaler(), ExtraTreesRegressor(random_state=i))\n\n        e = {'bootstrap': False, 'ccp_alpha': 0.0, 'criterion': 'mse', 'max_depth': None, 'max_features': 'auto', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_impurity_split': None, 'min_samples_leaf': 1, 'min_samples_split': 6, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 50, 'n_jobs': None, 'oob_score': False, 'verbose': 0, 'warm_start': False}\n        ET_new = ExtraTreesRegressor(**e, random_state=i)\n        #ET_new2 = make_pipeline(StandardScaler(), ExtraTreesRegressor(**e, random_state=i))\n\n        model_results = [] # model flow, mae, rmsle\n        models = [ET, ET_new]\n\n        for model in models:\n            #print(model)\n            with MyTimer(): \n                scores = calc_all_scores(model,5,5)\n            #print(\"------------------------------------------\")\n            model_results.append([model, (-scores['test_mae']).mean(), (np.sqrt(-scores['test_nmse'])).mean(), scores['test_r2'].mean()])\n\n        df_mr = pd.DataFrame(model_results, columns = ['model','mae','rmsle','r2'])\n        print(df_mr.sort_values(by=['rmsle']))\nelse:\n    ET_new = make_pipeline(RobustScaler(), ExtraTreesRegressor(**grid_result.best_params_))","71d5e590":"# initialize the algorithm for the GridSearchCV function\nR = Ridge(alpha=1.0)\ntuningR = 1 # this took 2 hours last time, 1 hour per iteration\n\nif (tuningR == 1):\n    # use this when tuning\n    param_grid={\n        'alpha':[8], # done\n        'fit_intercept':[True], # done\n        'normalize':[False], # done\n        'copy_X':[True],\n        'max_iter':[None], # done - no difference\n        'tol':[0.001],\n        'solver':['auto'], # done - Default:\u2018auto\u2019, \u2018svd\u2019, \u2018cholesky\u2019, \u2018lsqr\u2019, \u2018sparse_cg\u2019, \u2018sag\u2019, \u2018saga\u2019\n        'random_state':[1,10,42,99,127]\n    }\n\nelse:\n    # use this when not tuning\n    param_grid={\n        'alpha':[1.0],\n        'fit_intercept':[True],\n        'normalize':[False],\n        'copy_X':[True],\n        'max_iter':[None],\n        'tol':[0.001],\n        'solver':['auto'],\n        'random_state':[None]\n    }\n\nscorers = {\n    'r2': 'r2',\n    'nmsle': 'neg_mean_squared_log_error',\n    'nmse': 'neg_mean_squared_error',\n    'mae': 'neg_mean_absolute_error'\n}\n# To be used within GridSearch \ninner_cv = KFold(n_splits=4, shuffle=True, random_state=None)\n\n# To be used in outer CV \nouter_cv = KFold(n_splits=4, shuffle=True, random_state=None)    \n\n#inner loop KFold example:\ngsc = GridSearchCV(\n    estimator=R,\n    param_grid=param_grid,\n    scoring=scorers, # 'roc_auc', # or 'r2', etc -> can output several scores, but only refit to one. Others are just calculated\n    #scoring='neg_mean_squared_error', # or look here for other choices \n    # https:\/\/scikit-learn.org\/stable\/modules\/model_evaluation.html#scoring-parameter\n    #cv=5,\n    cv=inner_cv, # this will use KFold splitting to change train\/test\/validation datasets randomly\n    verbose=0,\n    return_train_score=True, # keep the other scores\n    refit='nmse' # use this one for optimizing\n)\n\ngrid_result = runGSCV(5, train, y_train)","c33eccd9":"rd = result_details(grid_result,'mean_test_nmse',100)\nsummary = rd[['alpha','random_state','mean_test_nmse','std_test_nmse','mean_test_mae','std_test_mae','mean_test_r2','std_test_r2']].sort_values(by=['random_state','mean_test_nmse'])\n#summary.groupby(['fit_intercept'], as_index=False).agg({'mean_test_nmse': 'mean', 'mean_test_nmse': 'std', 'mean_test_mae': 'mean', 'mean_test_r2': 'mean'}).sort_values(by=['mean_test_mae'])\nsummary.groupby(['alpha'], as_index=False).agg({'mean_test_nmse': 'mean', 'mean_test_mae': 'mean', 'mean_test_r2': 'mean'}).sort_values(by=['mean_test_mae'])\n","8cf89a61":"#R = make_pipeline(RobustScaler(), Ridge(alpha =0.0005, random_state=1)) # was 1\ntuning_r = 1\nif (tuning_r == 1):\n    for i in [2,5,20,42,99]:\n        print('random_state =',i)\n\n        r= {'alpha': 8, 'copy_X': True, 'fit_intercept': True, 'max_iter': None, 'normalize': False, 'solver': 'auto', 'tol': 0.001}\n        #R_new = make_pipeline(RobustScaler(), Ridge(**r, random_state=i))\n        R_new = Ridge(**r, random_state=i)\n\n        model_results = [] # model flow, mae, rmsle\n        models = [R, R_new]\n\n        for model in models:\n            #print(model)\n            with MyTimer(): \n                scores = calc_all_scores(model,5,5)\n            #print(\"------------------------------------------\")\n            model_results.append([model, (-scores['test_mae']).mean(), (np.sqrt(-scores['test_nmse'])).mean(), scores['test_r2'].mean()])\n\n        df_mr = pd.DataFrame(model_results, columns = ['model','mae','rmsle','r2'])\n        print(df_mr.sort_values(by=['rmsle']))\nelse:\n    #R_new = make_pipeline(RobustScaler(), Ridge(**grid_result.best_params_))\n    R_new = Ridge(**grid_result.best_params_)","45ed87ab":"#CBoost = CatBoostRegressor(logging_level='Silent',random_seed=0)\nCBoost = CatBoostRegressor(logging_level='Silent', random_state=0, depth=6, n_estimators=1000,eval_metric='RMSE',bagging_temperature=1,grow_policy='SymmetricTree',bootstrap_type='MVS') # l2_leaf_reg, learning_rate unknown","8f07fc37":"tuning_cb = 0\nif (tuning_cb == 1):\n    # initialize the algorithm for the GridSearchCV function\n    cb = CatBoostRegressor()\n    if (tuningCB == 1):\n        # use this when tuning\n        param_grid={\n            'nan_mode':['Min'],\n            'eval_metric':['RMSE'],\n            'iterations':[1000],\n            'sampling_frequency':['PerTree'],\n            'leaf_estimation_method':['Newton'],\n            'grow_policy':['SymmetricTree'],\n            'penalties_coefficient':[1],\n            'boosting_type':['Plain'],\n            'model_shrink_mode':['Constant'],\n            'feature_border_type':['GreedyLogSum'],\n            #'bayesian_matrix_reg':[0.10000000149011612],\n            'l2_leaf_reg':[3],\n            'random_strength':[1],\n            'rsm':[1],\n            'boost_from_average':[True],\n            'model_size_reg':[0.5],\n            'subsample':[0.800000011920929],\n            'use_best_model':[False],\n            'random_seed':[0,2,15],\n            'depth':[6], # done\n            'border_count':[254],\n            #'classes_count':[0],\n            #'auto_class_weights':['None'],\n            'sparse_features_conflict_fraction':[0],\n            'leaf_estimation_backtracking':['AnyImprovement'],\n            'best_model_min_trees':[1],\n            'model_shrink_rate':[0],\n            'min_data_in_leaf':[1],\n            'loss_function':['RMSE'],\n            'learning_rate':[0.04174000024795532],\n            'score_function':['Cosine'],\n            'task_type':['CPU'],\n            'leaf_estimation_iterations':[1],\n            'bootstrap_type':['MVS'],\n            'max_leaves':[31],\n            'logging_level':['Silent']\n        }\n\n    else:\n        # use this when not tuning\n        param_grid={\n            'nan_mode':['Min'],\n            'eval_metric':['RMSE'],\n            'iterations':[4000],\n            'sampling_frequency':['PerTree'],\n            'leaf_estimation_method':['Newton'],\n            'grow_policy':['SymmetricTree'],\n            'penalties_coefficient':[1],\n            'boosting_type':['Plain'],\n            'model_shrink_mode':['Constant'],\n            'feature_border_type':['GreedyLogSum'],\n            #'bayesian_matrix_reg':[0.10000000149011612],\n            'l2_leaf_reg':[3],\n            'random_strength':[1],\n            'rsm':[1],\n            'boost_from_average':[True],\n            'model_size_reg':[0.5],\n            'subsample':[0.800000011920929],\n            'use_best_model':[False],\n            'random_seed':[15],\n            'depth':[6],\n            'border_count':[254],\n            #'classes_count':[0],\n            #'auto_class_weights':['None'],\n            'sparse_features_conflict_fraction':[0],\n            'leaf_estimation_backtracking':['AnyImprovement'],\n            'best_model_min_trees':[1],\n            'model_shrink_rate':[0],\n            'min_data_in_leaf':[1],\n            'loss_function':['RMSE'],\n            'learning_rate':[0.04174000024795532],\n            'score_function':['Cosine'],\n            'task_type':['CPU'],\n            'leaf_estimation_iterations':[1],\n            'bootstrap_type':['MVS'],\n            'max_leaves':[31],\n            'logging_level':['Silent']\n        }\n\n    scorers = {\n        'r2': 'r2',\n        'nmsle': 'neg_mean_squared_log_error',\n        'nmse': 'neg_mean_squared_error',\n        'mae': 'neg_mean_absolute_error'\n    }\n    # To be used within GridSearch \n    inner_cv = KFold(n_splits=4, shuffle=True, random_state=None)\n\n    # To be used in outer CV \n    outer_cv = KFold(n_splits=4, shuffle=True, random_state=None)    \n\n    #inner loop KFold example:\n    gsc = GridSearchCV(\n        estimator=cb,\n        param_grid=param_grid,\n        scoring=scorers, # 'roc_auc', # or 'r2', etc -> can output several scores, but only refit to one. Others are just calculated\n        #scoring='neg_mean_squared_error', # or look here for other choices \n        # https:\/\/scikit-learn.org\/stable\/modules\/model_evaluation.html#scoring-parameter\n        #cv=5,\n        cv=inner_cv, # this will use KFold splitting to change train\/test\/validation datasets randomly\n        verbose=0,\n        return_train_score=True, # keep the other scores\n        refit='nmse'#, # use this one for optimizing\n        #plot=True\n    )\n    grid_result = runGSCV(2, train, y_train)\n    rd = result_details(grid_result,'depth',100)\n    rd[['random_state','depth','mean_test_nmse','std_test_nmse','mean_test_mae','std_test_mae','mean_test_r2','std_test_r2']]","3ffb0760":"#R = make_pipeline(RobustScaler(), Ridge(alpha =0.0005, random_state=1)) # was 1\nif (tuning_cb == 1):\n    for i in [2,15,20,42,99]:\n        print('random_state =', i)\n        #CBoost     = CatBoostRegressor(logging_level='Silent', random_state=i) # don't touch this variable\n        #CBoost_def = CatBoostRegressor(logging_level='Silent', random_state=i, depth=6, n_estimators=1000,eval_metric='RMSE',bagging_temperature=1,grow_policy='SymmetricTree',bootstrap_type='MVS') # l2_leaf_reg, learning_rate unknown    \n        # 'max_leaves': 64, creates nan values, default of 31 is fine, or remove entirely...\n        CBoost_new = CatBoostRegressor(**grid_result.best_params_) \n\n        model_results = [] # model flow, mae, rmsle\n        models = [CBoost, CBoost_new]\n\n        for model in models:\n            #print(model)\n            with MyTimer(): \n                scores = calc_all_scores(model,5,5)\n            model_results.append([model.get_params(), (-scores['test_mae']).mean(), (np.sqrt(-scores['test_nmse'])).mean(), scores['test_r2'].mean()])\n\n        df_mr = pd.DataFrame(model_results, columns = ['model','mae','rmsle','r2'])\n        print(df_mr.sort_values(by=['rmsle']))\nelse:\n    # was n_estimators=3500\n    CBoost_new = CatBoostRegressor(logging_level='Silent', random_state=15, depth=5, l2_leaf_reg=1.0, n_estimators=1700,eval_metric='RMSE',learning_rate=0.025,random_strength=3.7,bagging_temperature=1.0,grow_policy='SymmetricTree',bootstrap_type='Bayesian')#,bayesian_matrix_reg=0.10000000149011612)\n","c68e3e2b":"from copy import deepcopy\nimport pickle\nCBoost_new2 = deepcopy(CBoost_new)\nCBoost_new2.fit(train,y_train)\nwith open('CBoost_new.pkl', 'wb') as fid:\n    pickle.dump(CBoost_new2, fid)    \n# load it again\nwith open('CBoost_new.pkl', 'rb') as fid:\n    CBoost_new2 = pickle.load(fid)\nCBoost_new2.predict(train[0:1])","b0e111f2":"# adding polynomial only\nCBoost_poly = Pipeline([(\"polynomial_features\", polynomial_features), (\"cboost_regression\", CBoost_new)])\n\n# using PCA\n#CBoost_poly = Pipeline([(\"polynomial_features\", polynomial_features), ('reduce_dim', PCA(random_state=0)), (\"cboost_regression\", CBoost_new)])\n# using NMF\n#CBoost_poly = Pipeline([(\"polynomial_features\", polynomial_features), ('reduce_dim', NMF(n_components=360, alpha=0.8, l1_ratio=0.8,random_state=0)), (\"cboost_regression\", CBoost_new)])\n# using SelectKBest (score_func: also chi2, f_regression) - doesn't work, gives blank predictions and some very high and low predictions\n#CBoost_poly = Pipeline([(\"polynomial_features\", polynomial_features), ('reduce_dim', SelectKBest(score_func=f_regression,k=1500)), (\"cboost_regression\", CBoost_new)])\n","73eaef0f":"AB = AdaBoostRegressor()\nfrom sklearn.svm import SVR\nSVR = SVR()\nDT = DecisionTreeRegressor()\nKN = KNeighborsRegressor()\nB = BaggingRegressor()","bb791b61":"if (tuning_gb == 1):\n    model_results = [] # model flow, mae, rmsle\n    models = [GBoost, GBoost_orig]#, GBoost] # model_lgb_op, lasso_ns, \n\n    for model in models:\n        #print(model)\n        with MyTimer(): \n            scores = calc_all_scores(model)\n        #print(\"------------------------------------------\")\n        model_results.append([model, (-scores['test_mae']).mean(), (np.sqrt(-scores['test_nmse'])).mean(), scores['test_r2'].mean()])\n\n    df_mr = pd.DataFrame(model_results, columns = ['model','mae','rmsle','r2'])\n    df_mr.sort_values(by=['rmsle'])","a4cf5d2d":"print(\"Calculate Metrics: \", datetime.datetime.now())","32ad48f6":"compare_models = 0\nif (compare_models == 1):\n    model_results = [] # model flow, mae, rmsle\n    # GBoost_new is better than GBoost, but has lower final score, think this may be  overfitting\n    # BR_new has same results, here, but better final score\n    models = [lasso, lasso_new, model_lgb, ENet, ENet_new, KRR, GBoost, GBoost_new, model_xgb, BR, ET, ET_new, RF, RF_new, AB, SVR, DT, KN, B, R, R_new, CBoost, CBoost_new] # worse or same: BR_new, model_lgb_new, lasso_ns, model_xgb_new,\n\n    for model in models:\n        #print(model)\n        with MyTimer(): \n            scores = calc_all_scores(model)\n        #print(\"------------------------------------------\")\n        try:\n            print(model)\n            label = model\n        except KeyError as err:\n            print(\"KeyError error: {0}\".format(err))\n            label = model.__class__()\n        except Exception as e:\n            print(e.message, e.args)\n            label = model.__class__()\n        finally:\n            print(\"Continue\") \n        print('label', label)    \n        #model_results.append([model, (-scores['test_mae']).mean(), (np.sqrt(-scores['test_nmse'])).mean(), scores['test_r2'].mean()])\n        model_results.append([label, (-scores['test_mae']).mean(), (np.sqrt(-scores['test_nmse'])).mean(), scores['test_r2'].mean()])\n\n    df_mr = pd.DataFrame(model_results, columns = ['model','mae','rmsle','r2'])\n    df_mr.sort_values(by=['rmsle'])","081cc061":"df_mr.sort_values(by=['rmsle'])","a025e651":"print(\"Stacking and Ensembling: \", datetime.datetime.now())","9948f725":"# Variant A\nclass AveragingModels(BaseEstimator, RegressorMixin, TransformerMixin):\n    def __init__(self, models):\n        self.models = models\n        \n    # we define clones of the original models to fit the data in\n    def fit(self, X, y):\n        self.models_ = [clone(x) for x in self.models]\n        \n        # Train cloned base models\n        for model in self.models_:\n            model_name = model.__class__.__name__\n            model_details = str(model)\n            print('model_name:', model_name)\n            print('model_details:', model_details)\n            #model.fit(X, y)\n            if (\"KerasRegressor\" in model_name):\n                model.fit(X, y, shuffle=True, validation_split=0.3, callbacks=callbacks_list) # fit the model for this fold\n            if (\"keras\" in model_details):\n                #model.fit(X, y, shuffle=True, validation_split=0.3, callbacks=callbacks_list) # fit the model for this fold\n                model.fit(X, y, dnn__shuffle=True, dnn__validation_split=0.3, dnn__callbacks=callbacks_list)\n            else:\n                model.fit(X, y) # fit the model for this fold\n\n        return self\n    \n    #Now we do the predictions for cloned models and average them\n    def predict(self, X):\n        predictions = np.column_stack([\n            model.predict(X) for model in self.models_\n        ])\n        # IDEA: return weighted means\n        return np.mean(predictions, axis=1)\n","29fbd271":"model","a9dfe90c":"#use_voting_regressor = 0 # AveragingModels Averages\nuse_voting_regressor = 1 # VotingRegressor Averages \n#CBoost_new_10 = CatBoostRegressor(logging_level='Silent', random_state=10, depth=5, l2_leaf_reg=1.0, n_estimators=1700,eval_metric='RMSE',learning_rate=0.025,random_strength=3.7,bagging_temperature=1.0,grow_policy='SymmetricTree',bootstrap_type='Bayesian')\n\nwith MyTimer():\n    if (use_voting_regressor == 1):\n        print(\"running VotingRegressor\")\n        from sklearn.ensemble import VotingRegressor\n        estimator_list = [('CBoost', CBoost_new),('xgb',model_xgb),('ENet',ENet),('gboost',GBoost),('krr',KRR),('br',BR)]\n        weight_list = [4,2,2,2,2,2]\n        #estimator_list = [('dnn', dnn)]\n        averaged_models = VotingRegressor(estimators=estimator_list, weights=weight_list) \n        averaged_models.fit(train, y_train)\n        averaged_train_pred = averaged_models.predict(train)\n        averaged_pred = inv_boxcox1p(averaged_models.predict(test), lam_l)\n    else:\n        print(\"running AveragingModels\")\n        #AveragingModels will fit and predict each model and predict using the mean of the individual predictions\n        averaged_models = AveragingModels(models = (CBoost_new,CBoost_new,model_xgb,ENet,GBoost,KRR,BR))# ENet, model_xgb,dnn_pipe)) # Adding ENet and RF is worse, model_xgb_new is worse        \n        averaged_models.fit(train, y_train)\n        averaged_train_pred = averaged_models.predict(train)\n        averaged_pred = inv_boxcox1p(averaged_models.predict(test), lam_l)\n\nshow_metrics = 0\nif (show_metrics == 1):\n    score = mae_cv(averaged_models)\n    print(\" Averaged base models score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\n    score = rmsle_cv(averaged_models)\n    print(\" Averaged base models score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))        \n\n    print(mae(y_train, averaged_train_pred))\n    print(rmsle(y_train, averaged_train_pred))\nprint(averaged_pred)","335335c0":"a='''\nnum_epochs = len(dnn_history.val_loss_hist)\nxc         = range(num_epochs)\nplt.figure()\nplt.plot(xc, dnn_history.train_loss_hist, label='train')\nplt.plot(xc, dnn_history.val_loss_hist, label='val')\nplt.plot(xc, dnn_history.lr_hist, label='lr')\nplt.show()'''","117009cc":"ap=pd.DataFrame(averaged_pred)\nap.to_csv('ap2.csv',index=False)","bd1c8753":"CBoost_new.fit(train,y_train)\nprint(CBoost_new.predict(test))\nmodel_xgb.fit(train,y_train)\nprint(model_xgb.predict(test))\nENet.fit(train,y_train)\nprint(ENet.predict(test))\nGBoost.fit(train,y_train)\nprint(GBoost.predict(test))\nKRR.fit(train,y_train)\nprint(KRR.predict(test))\nBR.fit(train,y_train)\nprint(BR.predict(test))","bb226b5b":"averaged_models.get_params()","76d75aee":"if (use_voting_regressor == 1):\n    model_xgb.fit(train, y_train)\n    model_lgb.fit(train, y_train)\n    GBoost.fit(train, y_train)\n    CBoost.fit(train, y_train)\n    KRR.fit(train, y_train)\n    BR.fit(train, y_train)\n    averaged_models_temp = VotingRegressor(estimators=estimator_list, weights=weight_list) \n    averaged_models_temp.fit(train, y_train)\n\n    xt = test[:20]\n\n    pred1 = model_xgb.predict(xt)\n    pred2 = model_lgb.predict(xt)\n    pred3 = GBoost.predict(xt)\n    pred4 = CBoost.predict(xt)\n    pred5 = KRR.predict(xt)\n    pred6 = BR.predict(xt)\n    pred7 = averaged_models_temp.predict(xt)\n    plt.figure(figsize=(12,12))\n    plt.plot(pred1, 'gd', label='XGBRegressor')\n    plt.plot(pred2, 'bd', label='LGBRegressor')\n    plt.plot(pred3, 'yd', label='GradientBoostingRegressor')\n    plt.plot(pred4, 'rd', label='CatBoostRegressor')\n    plt.plot(pred5, 'gs', label='KernelRidge')\n    plt.plot(pred6, 'bs', label='BayesianRidge')\n    plt.plot(pred7, 'r*', ms=10, label='VotingRegressor')\n\n    plt.tick_params(axis='x', which='both', bottom=False, top=False,\n                    labelbottom=False)\n    plt.ylabel('predicted')\n    plt.xlabel('training samples')\n    plt.legend(loc=\"best\")\n    plt.title('Regressor predictions and their average')\n\n    plt.show()\n","04c85d21":"# Variant B\nclass StackingAveragedModels(BaseEstimator, RegressorMixin, TransformerMixin):\n    def __init__(self, base_models, meta_model, n_folds=10, shuffle=True): # increasing the n_folds value should give a more accurate prediction, averaged over n_fold iterations\n        self.base_models = base_models\n        self.meta_model = meta_model\n        self.n_folds = n_folds\n        self.shuffle = shuffle\n\n    # Fit the data on clones of the original models\n    def fit(self, X, y):\n        self.base_models_ = [list() for x in self.base_models]\n        self.meta_model_ = clone(self.meta_model)\n        kfold = KFold(n_splits=self.n_folds, shuffle=True, random_state=156) # was 156\n        print(\"shuffle=\" + str(self.shuffle))\n        \n        # Train cloned base models then create out-of-fold predictions\n        # that are needed to train the cloned meta-model\n        out_of_fold_predictions = np.zeros((X.shape[0], len(self.base_models)))\n        for i, model in enumerate(self.base_models): # for each model passed in\n            with MyTimer():\n                model_name = model.__class__.__name__\n                model_details = str(model)\n                print('model_name:', model_name)\n                print('model_details:', model_details)\n                for train_index, holdout_index in kfold.split(X, y): # create train,holdout splits for the number of folds\n                    instance = clone(model)\n                    self.base_models_[i].append(instance)\n                    #if (\"KerasRegressor\" in model_name):\n                    if (\"KerasRegressor\" in model_name):\n                        hist = instance.fit(X[train_index], y[train_index], shuffle=self.shuffle, validation_split=0.3, callbacks=callbacks_list) # fit the model for this fold\n                    elif (\"keras\" in model_details):\n                        instance.fit(X[train_index], y[train_index], dnn__shuffle=True, dnn__validation_split=0.3, dnn__callbacks=callbacks_list)\n                    else:\n                        instance.fit(X[train_index], y[train_index]) # fit the model for this fold\n                    y_pred = instance.predict(X[holdout_index]) # predict values for this fold\n                    out_of_fold_predictions[holdout_index, i] = y_pred # add predictions for this model and fold (random rows)\n                    #print('out_of_fold_predictions', out_of_fold_predictions)\n        # Now train the cloned  meta-model using the out-of-fold predictions as new and only feature\n        print(\"out_of_fold_predictions\", out_of_fold_predictions)\n        \n        meta_model_name = self.meta_model_.__class__.__name__\n        print(\"meta_model_name:\", meta_model_name)\n        if (\"KerasRegressor\" in meta_model_name):\n            self.meta_model_.fit(out_of_fold_predictions, y, shuffle=self.shuffle, validation_split=0.3, callbacks=callbacks_list) # need to see out_of_fold_predictions feature set\n        elif (\"keras\" in str(self.meta_model_)):\n            self.meta_model_.fit(out_of_fold_predictions, y, dnn__shuffle=True, dnn__validation_split=0.3, dnn__callbacks=callbacks_list) # need to see out_of_fold_predictions feature set\n        else:\n            self.meta_model_.fit(out_of_fold_predictions, y) # need to see out_of_fold_predictions feature set\n\n        #self.meta_model_.fit(out_of_fold_predictions, y) # need to see out_of_fold_predictions feature set\n        return self \n    \n    # Calculate the predictions of all base models on the test data and use the averaged predictions as \n    # meta-features for the final prediction which is calculated by the meta-model\n    \n    # add MinMax\n    def predict(self, X):\n        # column_stack() function is used to stack 1-D arrays as columns into a 2-D array.\n        meta_features = np.column_stack([\n            np.column_stack([model.predict(X) for model in base_models]).mean(axis=1)\n            for base_models in self.base_models_ ])\n        return self.meta_model_.predict(meta_features)","d461be5f":"# can't get sgd to give decent results, only adam works as a metamodel\n# dnn_meta = KerasRegressor(build_fn=baseline_model(dim=5, opt_sel=\"sgd\", learning_rate=0.000005, neurons=32, decay=0.000001, momentum=0.9), epochs=num_epochs, batch_size=8, verbose=1)\n# dnn_meta = KerasRegressor(build_fn=baseline_model(dim=5, opt_sel='adam', learning_rate = 0.001, neurons = 8, beta_1=0.9, beta_2=0.999, epsilon=1e-07, amsgrad=False), epochs=1000, batch_size=2, verbose=1)\n","a0af9b9d":"# ~ 10 minutes to run\n# verify: this class uses out of fold predictions in the stacking method, so rows in dataset are split up betwen models and each row in dataset is only used once\nstacked_averaged_models = StackingAveragedModels(base_models = (ENet, KRR, GBoost, model_xgb, lr_poly),#, dnn),\n                                                 meta_model = R_new, shuffle=True) \n\nif (compare_models == 1):\n    with MyTimer():\n        if (competition == 'SR'):\n            score = mae_cv(stacked_averaged_models)\n            print(\"Stacking Averaged models score mean and std: {:.4f} ({:.4f})\".format(score.mean(), score.std()))\n        else:\n            score = rmsle_cv(stacked_averaged_models)\n            print(\"Stacking Averaged models score mean and std: {:.4f} ({:.4f})\".format(score.mean(), score.std()))","8952b805":"#stacked_averaged_models.fit(train.values, y_train)\n#stacked_averaged_models # this doesn't work when using CBoost on my desktop","f1ceb820":"use_Regressor = 0 # default - best score\n#use_Regressor = 1 # StackingRegressor - worst score\n#use_Regressor = 2 # StackingCVRegressor - middle score\n\nif (use_Regressor == 1):\n    from sklearn.ensemble import StackingRegressor\n    from sklearn.linear_model import RidgeCV, LassoCV\n    from sklearn.linear_model import ElasticNetCV\n\n    use_cv = 1\n    k = {'alpha': 2.2, 'coef0': 0.5, 'degree': 5, 'gamma': 0.001, 'kernel': 'polynomial'}\n    if (use_cv == 1):\n        e = {'fit_intercept': True, 'l1_ratio': 0.85, 'max_iter': 100, 'normalize': False, 'selection': 'random', 'cv': 10} # 'alpha': 0.05,\n        r = {'fit_intercept': True, 'normalize': False, 'cv': None, 'gcv_mode': 'auto'} # cv value has no effect\n        estimators = [('enet', make_pipeline(StandardScaler(), ElasticNetCV(**e, random_state=3))),\n                      ('gboost', GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05, max_depth=4, max_features='sqrt', min_samples_leaf=15, min_samples_split=10,  loss='huber', random_state=5)),\n                      ('krr', KernelRidge(**k)),\n                      ('br', BayesianRidge())]\n        reg = StackingRegressor(\n            estimators=estimators,\n            final_estimator=RidgeCV(**r))\n    else:\n        e = {'alpha': 0.05, 'fit_intercept': True, 'l1_ratio': 0.85, 'max_iter': 100, 'normalize': False, 'selection': 'random', 'tol': 0.05}\n        r = {'alpha': 8, 'copy_X': True, 'fit_intercept': True, 'max_iter': None, 'normalize': False, 'solver': 'auto', 'tol': 0.05, 'cv': None, 'gcv_mode': 'auto', 'random_state': 99} #  \n        estimators = [('enet', make_pipeline(StandardScaler(), ElasticNet(**e, random_state=3))),\n                      ('gboost', GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05, max_depth=4, max_features='sqrt', min_samples_leaf=15, min_samples_split=10,  loss='huber', random_state=5)),\n                      ('krr', KernelRidge(**k)),\n                      ('br', BayesianRidge())]\n        reg = StackingRegressor(\n            estimators=estimators,\n            final_estimator=Ridge(**r))\n\n    reg.fit(train, y_train)\n    stacked_pred = inv_boxcox1p(reg.predict(test), lam_l)\n    #reg.transform(inv_boxcox1p(stacked_averaged_models_temp, lam_l))\n    print(stacked_pred)\n    print(reg)","4049905a":"a='''\n# Note Multiple stacking layers can be achieved by assigning final_estimator to a StackingClassifier or StackingRegressor:\n\nfinal_layer = StackingRegressor(\n    estimators=[('rf', RandomForestRegressor(random_state=42)),\n                ('gbrt', GradientBoostingRegressor(random_state=42))],\n    final_estimator=RidgeCV()\n    )\nmulti_layer_regressor = StackingRegressor(\n    estimators=[('ridge', RidgeCV()),\n                ('lasso', LassoCV(random_state=42)),\n                ('svr', SVR(C=1, gamma=1e-6, kernel='rbf'))],\n    final_estimator=final_layer\n)\nmulti_layer_regressor.fit(X_train, y_train)'''","d8344b89":"if (use_Regressor == 2):\n    from mlxtend.regressor import StackingCVRegressor\n    from sklearn.linear_model import RidgeCV, LassoCV\n    from sklearn.linear_model import ElasticNetCV\n\n    use_cv = 0\n    k = {'alpha': 2.2, 'coef0': 0.5, 'degree': 5, 'gamma': 0.001, 'kernel': 'polynomial'}\n    if (use_cv == 1):\n        e = {'fit_intercept': True, 'l1_ratio': 0.85, 'max_iter': 100, 'normalize': False, 'selection': 'random', 'cv': 10} # 'alpha': 0.05,\n        r = {'fit_intercept': True, 'normalize': False, 'cv': None, 'gcv_mode': 'auto'} # cv value has no effect\n        enet = make_pipeline(StandardScaler(), ElasticNetCV(**e, random_state=3))\n        gboost = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05, max_depth=4, max_features='sqrt', min_samples_leaf=15, min_samples_split=10,  loss='huber', random_state=5)\n        krr = KernelRidge(**k)\n        br = BayesianRidge()\n        r = RidgeCV(**r)\n        reg = StackingCVRegressor(\n            regressors=(enet, gboost, krr, br),\n            meta_regressor=r)\n    else:\n        e = {'alpha': 0.05, 'fit_intercept': True, 'l1_ratio': 0.85, 'max_iter': 100, 'normalize': False, 'selection': 'random'}\n        r = {'alpha': 8, 'copy_X': True, 'fit_intercept': True, 'max_iter': None, 'normalize': False, 'solver': 'auto', 'tol': 0.001, 'random_state': 99} #  \n        enet = make_pipeline(StandardScaler(), ElasticNet(**e, random_state=3))\n        gboost = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05, max_depth=4, max_features='sqrt', min_samples_leaf=15, min_samples_split=10,  loss='huber', random_state=5)\n        krr = KernelRidge(**k)\n        br = BayesianRidge()\n        r = Ridge(**r)\n        reg = StackingCVRegressor(\n            regressors=(enet, gboost, krr, br),\n            meta_regressor=r)\n\n    reg.fit(train, y_train)\n    stacked_pred = inv_boxcox1p(reg.predict(test), lam_l)\n    print(stacked_pred)\n    print(reg)\n    \n    print('5-fold cross validation scores:\\n')\n    for clf, label in zip([enet, gboost, krr, br], ['enet', 'gboost', \n                                                'krr', 'br',\n                                                'StackingCVRegressor']):\n        scores = cross_val_score(clf, train, y_train, cv=5, scoring='neg_mean_squared_error')\n        print(\"Neg. MSE Score: %0.2f (+\/- %0.2f) [%s]\" % (scores.mean(), scores.std(), label))\n        \n        #scores = cross_val_score(clf, train, y_train, cv=5)\n        #print(\"R^2 Score: %0.2f (+\/- %0.2f) [%s]\" % (scores.mean(), scores.std(), label))","755d6644":"averaged_models.fit(train, y_train)\naveraged_train_pred = averaged_models.predict(train)\nif (use_voting_regressor == 0):\n    averaged_pred = inv_boxcox1p(averaged_models.predict(test), lam_l)\n\nif (competition == 'SR'):\n    print(mae(y_train, averaged_train_pred))\nelse:\n    print(rmsle(y_train, averaged_train_pred))","17c14922":"#pre adjustment\naveraged_pred","cc43ed73":"plt.figure(figsize=(15,15))\nplt.scatter(averaged_train_pred, y_train, alpha=.7,\n            color='b') #alpha helps to show overlapping data\nplt.xlabel('Predicted Price')\nplt.ylabel('Actual Price')\nplt.title('Averaged Model')\nplt.show()","8adec459":"averaged_models.predict(test)","b22bc4e2":"averaged_train_pred","40ff1790":"#post adjustment\naveraged_pred","f6dcd662":"y_train.shape","3d3da092":"if (method == \"stacked\"):\n    with MyTimer():\n        # StackingAveragedModels runs 10 kfolds iterations\n        stacked_averaged_models.fit(train.values, y_train)\n    with MyTimer():\n        stacked_train_pred = stacked_averaged_models.predict(train.values)\n    if (use_Regressor == 0):\n        stacked_pred = inv_boxcox1p(stacked_averaged_models.predict(test.values), lam_l)\n\n    print(mae(y_train, stacked_train_pred))\n    print(rmsle(y_train, stacked_train_pred))\n\n    print(stacked_train_pred)\n    print(stacked_pred)","f8085189":"stacked_averaged_models.meta_model_","2cdea2b8":"a='''\nnum_epochs = len(dnn_history.val_loss_hist)\nxc         = range(num_epochs)\nplt.figure()\nplt.plot(xc, dnn_history.train_loss_hist, label='train')\nplt.plot(xc, dnn_history.val_loss_hist, label='val')\nplt.plot(xc, dnn_history.lr_hist, label='lr')\nplt.show()'''","5edafd6e":"def fit_pred(train, y_train, test, model):\n    #model.fit(train, y_train)\n    \n    model_name = model.__class__.__name__\n    print('model_name:', model_name)\n    if (\"XGBRegressor\" in model_name):\n        model.fit(train, y_train) # fit the model for this fold\n    elif (\"KerasRegressor\" in model_name):\n        model.fit(train, y_train, shuffle=False, validation_split=0.3, callbacks=callbacks_list) # fit the model for this fold\n    elif (\"keras\" in str(model)):\n        model.fit(train, y_train, dnn__shuffle=True, dnn__validation_split=0.3, dnn__callbacks=callbacks_list) # fit the model for this fold\n    else:\n        print(\"train\", train)\n        print(\"y_train\", y_train)\n        model.fit(train, y_train) # fit the model for this fold\n\n    model_train_pred = model.predict(train)\n    model_pred = inv_boxcox1p(model.predict(test), lam_l)\n    return(model_train_pred, model_pred)\n    \n#models      = [dnn, dnn_pipe, CBoost, lasso, lasso_new, ENet, KRR, GBoost, GBoost_new, model_xgb, model_xgb_new, BR, ET, ET_new, RF, RF_new, AB, SVR, DT, KN, B] # model_lgb,\n#model_names = ['dnn','dnn_pipe','CBoost', 'lasso', 'lasso_new', 'ENet', 'KRR', 'GBoost', 'GBoost_new', 'model_xgb', 'model_xgb_new', 'BR', 'ET', 'ET_new', 'RF', 'RF_new', 'AB', 'SVR', 'DT', 'KN', 'B']\nmodels      = [ CBoost, lasso, lasso_new, ENet, KRR, GBoost, GBoost_new, model_xgb, BR, ET, ET_new, RF, RF_new, AB, KN, B] # model_lgb,\nmodel_names = ['CBoost', 'lasso', 'lasso_new', 'ENet', 'KRR', 'GBoost', 'GBoost_new', 'model_xgb', 'BR', 'ET', 'ET_new', 'RF', 'RF_new', 'AB', 'KN', 'B']\ndf_train_pred = pd.DataFrame()\ndf_test_pred = pd.DataFrame()\nwith MyTimer():\n    for i in range(0,len(models)):\n        #print(\"models[i]\", models[i])\n        #print(\"model_names[i]:\", model_names[i])\n        mn = model_names[i]+\"_pred\"\n        train_pred, test_pred = fit_pred(train, y_train, test, models[i])\n        df_train_pred[mn] = train_pred\n        df_test_pred[mn] = test_pred\n        #print(mn, test_pred)    \n  ","4cc31921":"xt = df_train_pred[:20]\n\npred1 = xt['lasso_pred']\npred2 = xt['lasso_new_pred']\npred3 = xt['ENet_pred']\npred4 = xt['RF_pred']\npred5 = xt['CBoost_pred']\npred6 = xt['GBoost_pred']\n#pred7 = xt['dnn_pred']\n\nplt.figure(figsize=(12,12))\nplt.plot(pred1, 'gd', label='lasso')\nplt.plot(pred2, 'bd', label='lasso_new')\nplt.plot(pred3, 'yd', label='ENet')\nplt.plot(pred4, 'gs', label='RF')\nplt.plot(pred5, 'bs', label='CBoost')\nplt.plot(pred6, 'r*', ms=10, label='GBoost')\n#plt.plot(pred7, 'b*', ms=10, label='DNN')\n\nplt.tick_params(axis='x', which='both', bottom=False, top=False,\n                labelbottom=False)\nplt.ylabel('predicted')\nplt.xlabel('training samples')\nplt.legend(loc=\"best\")\nplt.title('Regressor predictions and their average')\n\nplt.show()","044c626f":"# values are not normalized\n#train[train.columns].mean().head()","6240248c":"# are train and test normalized? between -1 and 1\nreplace_xgb = 0 # new optimized model is worse, was overfit\nif (replace_xgb == 1):\n    model_xgb_new.fit(train, y_train)\n    xgb_train_pred = model_xgb_new.predict(train)\n    xgb_pred = inv_boxcox1p(model_xgb_new.predict(test), lam_l)\nelse:\n    model_xgb.fit(train, y_train)\n    xgb_train_pred = model_xgb.predict(train)\n    xgb_pred = inv_boxcox1p(model_xgb.predict(test), lam_l)\n\nif (competition == 'SR'):\n    print(mae(y_train, xgb_train_pred))\nelse:\n    print(rmsle(y_train, xgb_train_pred))","e481b6ef":"model_lgb.fit(train, y_train)\nlgb_train_pred = model_lgb.predict(train)\nlgb_pred = inv_boxcox1p(model_lgb.predict(test), lam_l)\n\nif (competition == 'SR'):\n    print(mae(y_train, lgb_train_pred))\nelse:\n    print(rmsle(y_train, lgb_train_pred))","9ddbbef5":"if (tuning_lgb == 1):\n    model_lgb_op.fit(train, y_train)\n    lgb_train_pred_op = model_lgb_op.predict(train)\n    lgb_pred_op = inv_boxcox1p(model_lgb_op.predict(test), lam_l)\n\n    if (competition == 'SR'):\n        print(mae(y_train, lgb_train_pred_op))\n    else:\n        print(rmsle(y_train, lgb_train_pred_op))","70c10993":"# compare values with optimization\nprint(lgb_train_pred)\nif (tuning_lgb == 1):\n    print(lgb_train_pred_op)\nprint(lgb_pred)\nif (tuning_lgb == 1):\n    print(lgb_pred_op)","ceb04242":"with open('CBoost_new.pkl', 'rb') as fid:\n    CBoost_new = pickle.load(fid)\n    \nCBoost_new.fit\ncb_pred = inv_boxcox1p(CBoost_new.predict(test), lam_l)\ncb_train_pred = inv_boxcox1p(CBoost_new.predict(train), lam_l)\nprint(cb_pred)","720ad773":"ENet.fit(train,y_train)\nKRR.fit(train,y_train)\nGBoost.fit(train,y_train)\nlr_poly.fit(train,y_train)\nmodel_xgb.fit(train,y_train)\nenet_pred = inv_boxcox1p(ENet.predict(test), lam_l)\nkrr_pred = inv_boxcox1p(KRR.predict(test), lam_l)\ngboost_pred = inv_boxcox1p(GBoost.predict(test), lam_l)\nlr_pred = inv_boxcox1p(lr_poly.predict(test), lam_l)\n\nbr_pred = inv_boxcox1p(BR.predict(test), lam_l)\nbr_train_pred = inv_boxcox1p(BR.predict(train), lam_l)\n# all models being used in final model\n#print(cb_pred)\n#print(xgb_pred)\n#print(lgb_pred)\nprint(br_pred)\nprint(enet_pred) \nprint(krr_pred)\nprint(gboost_pred)\nprint(lr_pred) # changes each time\n#print(dnn_pred)","fc71312a":"if (method == 'stacked'):\n    testing_ratio = 0\n    if (testing_ratio == 1):\n        stkr = 1.00\n        xgbr = 0.00\n        lgbr = 0.00\n        cbr  = 0.00\n        brr  = 0.00\n    else:\n        stkr = 0.70\n        xgbr = 0.10 # .10\n        lgbr = 0.00\n        cbr  = 0.20 # .20\n        brr  = 0.00 # 0\n\n    '''RMSE on the entire Train data when averaging'''\n\n    print('RMSLE score on train data:')\n    print(mae(y_train,stacked_train_pred*stkr+ xgb_train_pred*xgbr + lgb_train_pred*lgbr + cb_train_pred*cbr + br_train_pred*brr ))\n    print(rmsle(y_train,stacked_train_pred*stkr + xgb_train_pred*xgbr + lgb_train_pred*lgbr + cb_train_pred*cbr + br_train_pred*brr ))","69a0d77b":"print(method)\nif (method == 'stacked'):\n    ensemble = stacked_pred*stkr + xgb_pred*xgbr + lgb_pred*lgbr + cb_pred*cbr + br_pred*brr  # if using averaged_pred, need to add averaged_pred here\nelse:\n    ensemble = averaged_pred\n","5e228dc7":"print(xgb_pred)\nprint(lgb_pred)\nprint(cb_pred)\nprint(br_pred)\nprint(averaged_pred)\nprint(ensemble)","adbf40f8":"if (method == 'stacked'):\n    print(y_train,stacked_train_pred * stkr + xgb_train_pred * xgbr + lgb_train_pred * lgbr) # if using averaged_pred, need to add averaged_pred here\n    print(y_train,stacked_train_pred)","64c894be":"sub_train = pd.DataFrame()\nsub_train['Id'] = train_ID\nif (method == 'stacked'):\n    sub_train['SalePrice'] = inv_boxcox1p(stacked_train_pred, lam_l)\nelse:\n    sub_train['SalePrice'] = inv_boxcox1p(averaged_train_pred, lam_l)","3ebb7dfa":"Predicted = sub_train['SalePrice']\nActual = inv_boxcox1p(y_train, lam_l)\nplt.figure(figsize=(15,15))\nplt.scatter(sub_train['SalePrice'], Actual, alpha=.7,\n            color='b') #alpha helps to show overlapping data\nplt.xlabel('Predicted Price')\nplt.ylabel('Actual Price')\nplt.title('Averaged Model')\n\nm, b = np.polyfit(Predicted, Actual, 1)\n#m = slope, b=intercept\nplt.plot(Predicted, m*Predicted+b,c='red')\nplt.show()","62349d15":"# Pre-adjustment score\nprint(\"mae for boxcox(SalePrice)\",mae(y_train, sub_train['SalePrice']))\nprint(\"mse for boxcox(SalePrice)\",rmsle(y_train, sub_train['SalePrice']))\nprint(\"mae for SalePrice\",mae(Actual, Predicted))\nprint(\"mse for SalePrice\",rmsle(Actual, Predicted))","1bc5d6dc":"def AdjustHigh(sub_train, y_train):\n    AdjustedScores = []\n    for i in np.arange(.994, 1.000, 0.01):\n        for j in np.arange(1.00, 1.10, .01):\n\n            q1 = sub_train['SalePrice'].quantile(0.0025)\n            q2 = sub_train['SalePrice'].quantile(0.0045)\n            q3 = sub_train['SalePrice'].quantile(i)\n\n            #Verify the cutoffs for the adjustment\n            # adjust at low end\n            #sub_train['SalePrice2'] = sub_train['SalePrice'].apply(lambda x: x if x > q1 else x*0.79)\n            #sub_train['SalePrice2'] = sub_train['SalePrice'].apply(lambda x: x if x > q2 else x*0.89)\n\n            # adjust at high end\n            sub_train['SalePrice2'] = sub_train['SalePrice'].apply(lambda x: x if x < q3 else x*j)\n\n            Predicted = sub_train['SalePrice2']\n            Actual = inv_boxcox1p(y_train, lam_l)\n\n            # Pre-adjustment score\n            #print(\"mae for boxcox(SalePrice)\",mae(y_train, sub_train['SalePrice2']))\n            #print(\"mse for boxcox(SalePrice)\",rmsle(y_train, sub_train['SalePrice2']))\n            #print(\"mae for SalePrice\",mae(Actual, Predicted))\n            #print(\"mse for SalePrice\",rmsle(Actual, Predicted))\n\n            AdjustedScores.append([i, j, mae(y_train, boxcox1p(sub_train['SalePrice2'], lam_l)), rmsle(y_train, boxcox1p(sub_train['SalePrice2'], lam_l)), mae(Actual, Predicted), rmsle(Actual, Predicted)])\n    print(q1,q2,q3)\n    df_adj = pd.DataFrame(AdjustedScores, columns=[\"QUANT\",\"COEF\",\"MAE_BC\",\"RMSE_BC\",\"MAE_SP\",\"RMSE_SP\"])\n    print('quantiles vs coefficients')\n    df_adj.sort_values(by=['RMSE_BC'])\n    print(df_adj)\n    df2 = df_adj.sort_values(by=['RMSE_BC']).reset_index()\n    coef_hi = df2.COEF[0]\n    quant_hi = df2.QUANT[0]\n    return (coef_hi, quant_hi)\n\nch, qh = AdjustHigh(sub_train, y_train)","0cf4739a":"print('ch',ch)\nprint('qh',qh)","5ff989ba":"q1 = sub_train['SalePrice'].quantile(0.0015)\nq2 = sub_train['SalePrice'].quantile(0.01)\nq3 = sub_train['SalePrice'].quantile(qh)\n\n#Verify the cutoffs for the adjustment\nprint(q1,q2,q3)\n# adjust at low end\n#sub_train['SalePrice2'] = sub_train['SalePrice'].apply(lambda x: x if x > q1 else x*0.79)\n#sub_train['SalePrice2'] = sub_train['SalePrice'].apply(lambda x: x if x > q2 else x*0.89)\n\n# adjust at high end\nsub_train['SalePrice2'] = sub_train['SalePrice'].apply(lambda x: x if x < q3 else x*ch)","320c5d61":"plt.figure(figsize=(15,15))\nplt.scatter(sub_train['SalePrice'], sub_train['SalePrice2'], alpha=.7,\n            color='b') #alpha helps to show overlapping data\nplt.xlabel('Predicted Price')\nplt.ylabel('Adjusted Predicted Price')\nplt.title('Averaged Model')\nplt.show()","6bd24936":"sub_train.query(\"SalePrice != SalePrice2\")","7839baeb":"Predicted = sub_train['SalePrice2']\nActual = inv_boxcox1p(y_train, lam_l)\nplt.figure(figsize=(15,15))\nplt.scatter(sub_train['SalePrice2'], Actual, alpha=.7,\n            color='b') #alpha helps to show overlapping data\nplt.xlabel('Adj Predicted Price')\nplt.ylabel('Actual Price')\nplt.title('Averaged Model')\n\nm, b = np.polyfit(Predicted, Actual, 1)\n#m = slope, b=intercept\nplt.plot(Predicted, m*Predicted+b,c='red')\nplt.show()","c0088b08":"# Post adjustment for high score\nprint(\"mae for boxcox(SalePrice2)\",mae(y_train, sub_train['SalePrice2']))\nprint(\"mse for boxcox(SalePrice2)\",rmsle(y_train, sub_train['SalePrice2']))\nprint(\"mae for SalePrice2\",mae(Actual, Predicted))\nprint(\"mse for SalePrice2\",rmsle(Actual, Predicted))","0a4c57be":"def AdjustLow(sub_train, y_train):\n    AdjustedScores = []\n    for i in np.arange(.00, .02, 0.001):\n        for j in np.arange(.90, 1.10, 0.01):\n\n            q1 = sub_train['SalePrice'].quantile(i)\n            q2 = sub_train['SalePrice'].quantile(0.1)\n            q3 = sub_train['SalePrice'].quantile(.995)\n\n            #Verify the cutoffs for the adjustment\n            #print(q1,q2,q3)\n            # adjust at low end\n            sub_train['SalePrice2'] = sub_train['SalePrice'].apply(lambda x: x if x > q1 else x*j)\n            #sub_train['SalePrice2'] = sub_train['SalePrice'].apply(lambda x: x if x > q2 else x*0.89)\n\n            # adjust at high end\n            #sub_train['SalePrice2'] = sub_train['SalePrice'].apply(lambda x: x if x < q3 else x*j)\n\n            Predicted = sub_train['SalePrice2']\n            Actual = inv_boxcox1p(y_train, lam_l)\n\n            # Pre-adjustment score\n            #print(\"mae for boxcox(SalePrice)\",mae(y_train, sub_train['SalePrice2']))\n            #print(\"mse for boxcox(SalePrice)\",rmsle(y_train, sub_train['SalePrice2']))\n            #print(\"mae for SalePrice\",mae(Actual, Predicted))\n            #print(\"mse for SalePrice\",rmsle(Actual, Predicted))\n\n            AdjustedScores.append([i, j, mae(y_train, boxcox1p(sub_train['SalePrice2'], lam_l)), rmsle(y_train, boxcox1p(sub_train['SalePrice2'], lam_l)), mae(Actual, Predicted), rmsle(Actual, Predicted)])\n  \n    df_adj = pd.DataFrame(AdjustedScores, columns=[\"QUANT\",\"COEF\",\"MAE_BC\",\"RMSE_BC\",\"MAE_SP\",\"RMSE_SP\"])\n    print('quantiles vs coefficients')\n    df_adj.sort_values(by=['RMSE_BC'])\n    print(df_adj)\n    df2 = df_adj.sort_values(by=['RMSE_BC']).reset_index()\n    coef_lo = df2.COEF[0]\n    quant_lo = df2.QUANT[0]\n    return (coef_lo, quant_lo)\n\ncl, ql = AdjustLow(sub_train, y_train)","3b701e93":"print('cl',cl)\nprint('ql',ql)","7b72db8a":"q1 = sub_train['SalePrice'].quantile(ql)\nq2 = sub_train['SalePrice'].quantile(0.1)\nq3 = sub_train['SalePrice'].quantile(qh)\n\n#Verify the cutoffs for the adjustment\nprint(q1,q2,q3)\n# adjust at low end\n\n#sub_train['SalePrice2'] = sub_train['SalePrice'].apply(lambda x: x if (x > q1) else x*cl) \n#sub_train['SalePrice2'] = sub_train['SalePrice'].apply(lambda x: x if x > q2 else x*0.89)\n\n# adjust at high end\n\n#sub_train['SalePrice2'] = sub_train['SalePrice'].apply(lambda x: x if (x < q3) else x*ch)\n\nsub_train['SalePrice2'] = sub_train['SalePrice'].apply(lambda x: x*cl if x < q1 else ( x*ch if x > q3 else x))\nsub_train.query(\"SalePrice != SalePrice2\")","542cc725":"#sub_train['SalePrice2'] = sub_train['SalePrice'].apply(lambda x: x if (x > q1 and x < q3) else x*cl) \nsub_train['SalePrice2'] = sub_train['SalePrice'].apply(lambda x: x*cl if x < q1 else ( x*ch if x > q3 else x))\n\n#                                                       lambda x: x*10 if x<2 else (x**2 if x<4 else x+10)\n\nsub_train.query(\"SalePrice != SalePrice2\")","7416361a":"plt.figure(figsize=(15,15))\nplt.scatter(sub_train['SalePrice'], sub_train['SalePrice2'], alpha=.7,\n            color='b') #alpha helps to show overlapping data\nplt.xlabel('Predicted Price')\nplt.ylabel('Adjusted Predicted Price')\nplt.title('Averaged Model')\nplt.show()","77d6529a":"Predicted = sub_train['SalePrice2']\nActual = inv_boxcox1p(y_train, lam_l)\nplt.figure(figsize=(15,15))\nplt.scatter(sub_train['SalePrice2'], Actual, alpha=.7,\n            color='b') #alpha helps to show overlapping data\nplt.xlabel('Adj Predicted Price')\nplt.ylabel('Actual Price')\nplt.title('Averaged Model')\n\nm, b = np.polyfit(Predicted, Actual, 1)\n#m = slope, b=intercept\nplt.plot(Predicted, m*Predicted+b,c='red')\n\nplt.show()","53681021":"sub_train.query(\"SalePrice != SalePrice2\")","01abfba4":"# Post adjustment for low and high score\nprint(\"mae for boxcox(SalePrice2)\",mae(y_train, sub_train['SalePrice2']))\nprint(\"mse for boxcox(SalePrice2)\",rmsle(y_train, sub_train['SalePrice2']))\nprint(\"mae for SalePrice2\",mae(Actual, Predicted))\nprint(\"mse for SalePrice2\",rmsle(Actual, Predicted))","db5afb5c":"from yellowbrick.regressor import ResidualsPlot\n\nmodels = [lasso, lasso_new, model_lgb, ENet, ENet_new, KRR, GBoost, GBoost_new, model_xgb, BR, ET, ET_new, RF, RF_new, AB, SVR, DT, KN, B, R, R_new]#, CBoost, CBoost_new] # Catboost does not work\nfor model in models:\n    visualizer = ResidualsPlot(model)\n    visualizer.fit(train, y_train)  # Fit the training data to the visualizer\n    visualizer.score(train, y_train)  # Evaluate the model on the test data\n    visualizer.show()                 # Finalize and render the figure","84d6894a":"import seaborn\nseaborn.residplot(Actual, cb_train_pred)","2ba83a35":"#seaborn.residplot(Actual, dnn_train_pred)","63fbdc37":"from sklearn.svm import SVR\nsvr_new = SVR()\nsvr_new.fit(train,y_train)\nsvr_train_pred = svr_new.predict(train)\n\nseaborn.residplot(Actual, inv_boxcox1p(svr_train_pred, lam_l))\nplt.savefig(\"out.png\")","d8648bfe":"seaborn.residplot(Actual, Predicted)\nplt.savefig(\"out.png\")","8bf2674c":"#View the QQ-plot\nfig = plt.figure()\nres = stats.probplot(svr_train_pred, plot=plt)\nplt.show()","30ec3bd9":"sub = pd.DataFrame()\nsub['Id'] = test_ID\nsub['SalePrice'] = ensemble\n#sub.to_csv('submission.csv',index=False)","cab67852":"adjust_both = 0\nadjust_low = 0\nadjust_high = 1\n\nq1 = sub['SalePrice'].quantile(ql)\nq2 = sub['SalePrice'].quantile(0.1)\nq3 = sub['SalePrice'].quantile(qh)\n\nif (adjust_both == 1):\n    sub['SalePrice'] = sub['SalePrice'].apply(lambda x: x*cl if (x < q1) else ( x*ch if (x > q3) else x))\nelif (adjust_low == 1):\n    sub['SalePrice'] = sub['SalePrice'].apply(lambda x: x if (x > q1) else x*cl) \nelif (adjust_high == 1):\n    sub['SalePrice'] = sub['SalePrice'].apply(lambda x: x if (x < q3) else x*ch)\nelse:\n    print(\"no adjustments made\")","9890a774":"sub.to_csv('submission.csv',index=False)","125b68be":"sub.head()","9650df95":"print(\"Start: \", StartTime)\nprint(\"End: \", datetime.datetime.now())","69c0234b":"from mlens.ensemble import SuperLearner\nensemble2 = SuperLearner(random_state=1, verbose=2)","e03289da":"# http:\/\/rasbt.github.io\/mlxtend\/user_guide\/feature_selection\/SequentialFeatureSelector\/\n# accuracy levels off at k_features=25, this code takes several hours to run!!\nfeature_select = 0\nif (feature_select == 1):\n    from mlxtend.feature_selection import SequentialFeatureSelector as SFS\n\n    sfs1 = SFS(CBoost_new, \n               k_features=50, \n               forward=True, \n               floating=False, \n               verbose=2,\n               scoring='neg_mean_squared_error',\n               cv=2)\n\n    sfs1 = sfs1.fit(train, y_train)\n\n    from mlxtend.plotting import plot_sequential_feature_selection as plot_sfs\n    import matplotlib.pyplot as plt\n\n    fig = plot_sfs(sfs1.get_metric_dict(), kind='std_err')\n\n    plt.title('Sequential Forward Selection (w. StdErr)')\n    plt.grid()\n    plt.show()","39b4c929":"if (feature_select == 1):\n    # Generate the new subsets based on the selected features\n    # Note that the transform call is equivalent to\n    # X_train[:, sfs1.k_feature_idx_]\n\n    X_train_sfs = sfs1.transform(train)\n    X_test_sfs = sfs1.transform(test)\n\n    # Fit the estimator using the new feature subset\n    # and make a prediction on the test data\n    CBoost_new.fit(X_train_sfs, y_train)\n    y_pred = CBoost_new.predict(X_train_sfs)\n\n    # Compute the accuracy of the prediction\n    #classification\n    #acc = float((y_train == y_pred).sum()) \/ y_pred.shape[0]\n    #print('Test set accuracy: %.2f %%' % (acc * 100))\n    #regression\n    print(\"mae\",mae(y_train, y_pred))\n    print(\"rmsle\",rmsle(y_train, y_pred))","47bb4a5d":"print(\"End: \", datetime.datetime.now())","321e7779":"print(stacked_averaged_models)\nprint(stacked_averaged_models.meta_model.get_params())","f67d5dd2":"We just average five models here **XGBoost, GBoost, BayesianRidge, KRR and LightGBM**.  Of course we could easily add more models in the mix. ","6c0e042b":"The shape values are the number of columns in the PCA x the number of original columns","9a71be44":"- **CatBoost**\n\nReference: https:\/\/catboost.ai\/docs\/concepts\/python-reference_parameters-list.html","57b6813b":"Show both adjustments","0f95e4ff":"Visualize the loss over each epoch","54c0e8d1":"This chart does not look linear, or at least the line is not matching the data across the entire x axis. Looks like a drop off for High GrLivArea, seems home buyers are not willing to pay a corresponding amount extra for the large living area, looking for a \"volume discount\" maybe...\nLets look at this closer, first fit a line, next try a polynomial fit to compare","85d79ab1":"Edit: pca.components_ is the matrix you can use to calculate the inverse of the PCA analysis, i.e. go back to the original dataset\nreference: https:\/\/stats.stackexchange.com\/a\/143949","fce96708":"Cross Correlation chart to view collinearity within the features. Kendall's seems appropriate as the Output Variable is numerical and much of the input is categorical. Here is a chart to guide you to which method to use based on your dataset.\n\n![image.png](attachment:image.png)","4039fd40":"We impute them  by proceeding sequentially  through features with missing values \n\n- **PoolQC** : data description says NA means \"No  Pool\". That make sense, given the huge ratio of missing value (+99%) and majority of houses have no Pool at all in general. \n- **MiscFeature** : data description says NA means \"no misc feature\"\n- **Alley** : data description says NA means \"no alley access\"\n- **Fence** : data description says NA means \"no fence\"\n- **FireplaceQu** : data description says NA means \"no fireplace\"\n- **GarageType, GarageFinish, GarageQual and GarageCond** : Replacing missing data with None\n- **BsmtQual, BsmtCond, BsmtExposure, BsmtFinType1 and BsmtFinType2** : For all these categorical basement-related features, NaN means that there is no  basement.","49d26592":"Look for other correlations, maybe all the basement olumns will correlate like BsmtFullBath and BsmtFinSF1 and Fin vs Unf have negative correlation. Both make sense...","bcb4f883":"We can see at the bottom right two with extremely large GrLivArea that are of a low price. These values are huge oultliers.\nTherefore, we can safely delete them.\n\nEdit: Think this deletion is safe to do as there are only 2 points and they seem to be abnormal, possibly data error","a8f5c7f8":"We use the scipy  function boxcox1p which computes the Box-Cox transformation of **\\\\(1 + x\\\\)**. \n\nNote that setting \\\\( \\lambda = 0 \\\\) is equivalent to log1p used above for the target variable.  \n\nSee [this page][1] for more details on Box Cox Transformation as well as [the scipy function's page][2]\n[1]: http:\/\/onlinestatbook.com\/2\/transformations\/box-cox.html\n[2]: https:\/\/docs.scipy.org\/doc\/scipy-0.19.0\/reference\/generated\/scipy.special.boxcox1p.html","92fb44e7":"In this approach, we add a meta-model on averaged base models and use the out-of-folds predictions of these base models to train our meta-model. \n\nThe procedure, for the training part, may be described as follows:\n\n\n1. Split the total training set into two disjoint sets (here **train** and .**holdout** )\n\n2. Train several base models on the first part (**train**)\n\n3. Test these base models on the second part (**holdout**)\n\n4. Use the predictions from 3)  (called  out-of-folds predictions) as the inputs, and the correct responses (target variable) as the outputs  to train a higher level learner called **meta-model**.\n\nThe first three steps are done iteratively . If we take for example a 5-fold stacking , we first split the training data into 5 folds. Then we will do 5 iterations. In each iteration,  we train every base model on 4 folds and predict on the remaining fold (holdout fold). \n\nSo, we will be sure, after 5 iterations , that the entire data is used to get out-of-folds predictions that we will then use as \nnew feature to train our meta-model in the step 4.\n\nFor the prediction part , We average the predictions of  all base models on the test data  and used them as **meta-features**  on which, the final prediction is done with the meta-model.\n","3bbd0685":"**Adding one more important feature**","6cb489db":"add the previous averaged models here","87ed4f16":"Compare different predictions to each other:\n\nhttps:\/\/scikit-learn.org\/stable\/auto_examples\/ensemble\/plot_voting_regressor.html","76636425":"- **Electrical** : It has one NA value. Since this feature has mostly 'SBrkr', we can set that for the missing value.\n","df2feb48":"Optimize","e07a2595":"try different pipelines, scaler, etc\nreference: https:\/\/scikit-learn.org\/stable\/auto_examples\/inspection\/plot_permutation_importance.html#sphx-glr-auto-examples-inspection-plot-permutation-importance-py\n\n![image.png](attachment:image.png)\n\nTry polynomial features:\nreference: https:\/\/scikit-learn.org\/stable\/auto_examples\/model_selection\/plot_underfitting_overfitting.html#sphx-glr-auto-examples-model-selection-plot-underfitting-overfitting-py\nfrom sklearn.preprocessing import PolynomialFeatures\n\npolynomial_features = PolynomialFeatures(degree=degrees[i],\n                                         include_bias=False)\nlinear_regression = LinearRegression()\npipeline = Pipeline([(\"polynomial_features\", polynomial_features),\n                     (\"linear_regression\", linear_regression)])\n\nscores = cross_val_score(pipeline, X[:, np.newaxis], y,\n                         scoring=\"neg_mean_squared_error\", cv=10)       \n                             \n                             \nestimators = []\nestimators.append(('standardize', StandardScaler()))\nestimators.append(('lda', LinearDiscriminantAnalysis()))\nmodel = Pipeline(estimators)    \n\n\nfeatures = []\nfeatures.append(('pca', PCA(n_components=3)))\nfeatures.append(('select_best', SelectKBest(k=6)))\nfeature_union = FeatureUnion(features)\nestimators = []\nestimators.append(('feature_union', feature_union))\nestimators.append(('logistic', LogisticRegression()))\nmodel = Pipeline(estimators)","7a4f4cca":"Is there any remaining missing value ? ","8cb2c59a":"**Transformation of the target variable**\n \nDefault is to use log1p as this is included in the evaluation metric, rmsle \nEdit: also trying box-cox transform.\nThe Box-Cox transform is given by:\n<pre>\ny = (x**lmbda - 1) \/ lmbda,  for lmbda > 0  \n    log(x),                  for lmbda = 0\n    \ny = ((1+x)**lmbda - 1) \/ lmbda  if lmbda != 0\n    log(1+x)                    if lmbda == 0","7462fd48":"Now we look at a polynomial fit","620f5d6a":"**Create File for Submission to Kaggle**","d1ee9237":"our column count went from 216 to n_component value","12e54946":"- **More Models** :","3313dbdc":"![image.png](attachment:image.png)","23253a83":"No missing values\n","eaf5abd2":"Create predictions for all models for testing","259c238a":"try another method, imputation","e01dbcb4":"The target variable is right skewed.  As (linear) models love normally distributed data , we need to transform this variable and make it more normally distributed.\n","7875c3b6":"**Averaged base models score**","3fdedc92":"##Stacking  models","f5a58d8c":"- **BayesianRidge** :","04947f1c":"Get Pre-adjustment score for comparison","403df7a0":"Show new predictions","489ca9d5":"- **MSSubClass** : Na most likely means No building class. We can replace missing values with None\n","2c4f8967":"Objective: Predict prices for test (test.csv) dataset based on model build from train (train.csv) dataset  \n\nEvaluation metric: \"The RMSE between log of Saleprice and log of prediction\". Need to convert salesprice to log value first. However seems that BoxCox does a better job here. For my testing I will include boxcox1p()\n\nOriginal competition (explains the evaluation metric): https:\/\/www.kaggle.com\/c\/home-data-for-ml-course\/overview\/evaluation. The hard work is paying off, my submission on that site is #3 out of 38000, top .0001%\n\nOriginal notebook source: https:\/\/www.kaggle.com\/serigne\/stacked-regressions-top-4-on-leaderboard  \n\nReference for good ideas: https:\/\/towardsdatascience.com\/tricks-i-used-to-succeed-on-a-famous-kaggle-competition-adaa16c09f22\n\nOriginal default score was .11543, new best score is 0.11353\n\nStacked and Ensembled Regressions to predict House Prices - improves score by ~ 5% \nHow to Kaggle: https:\/\/www.youtube.com\/watch?v=GJBOMWpLpTQ\n\nReferences for stacking and ensembling:\nhttps:\/\/www.kaggle.com\/getting-started\/18153\nhttps:\/\/developer.ibm.com\/technologies\/artificial-intelligence\/articles\/stack-machine-learning-models-get-better-results\n\nAuthor: Donald S  \nOriginal: July 2020  \n\nNeed to submit every 2 months as the leaderboard will rollover after this 2 month period","68ad7718":"![Faron](http:\/\/i.imgur.com\/QBuDOjs.jpg)\n\n(Image source [Faron](https:\/\/www.kaggle.com\/getting-started\/18153#post103381))\n\nThe only difference between M6 and M1-M5 is, that M6 is trained on the entire original training data, whereas M1-M5 are trained only on 4 out of 5 folds.\n\nWith M1-M5 you can build valid out-of-fold predictions for the training set (the orange ones) to form a single \"new feature\" for the 2nd layer (not possible with M6). You can also predict the test set with M1-M5 to get 5 sets of test set predictions .. but you only need one set of test set predictions for the corresponding feature to the orange out-of-fold train set predictions.\n\nHence, you reduce those 5 sets to 1 by averaging. That's the first variant, A. Alternatively, you train M6 and use its test set predictions as the feature for the 2nd layer (instead of the average of the test set predictions from M1-M5).","f2ad5803":"Want to add ordinal or int column for Year and Month, this is the function to perform that task","b377cce1":"- **LinearRegression** :","c5e866d7":"We add **XGBoost and LightGBM** to the **StackedRegressor** defined previously. ","209de9d8":"##Base models","77c06b06":"now we have 100x the original (~ 200) number of features (~ 20000)","409426cf":"use different function to calculate ensembles","f6fd3645":"Here is the difference between BoxCox and Log values, the difference is substantial at the value of a house","60c869b3":"Some imputation methods result in biased parameter estimates, such as means, correlations, and regression coefficients, unless the data are Missing Completely at Random (MCAR). The bias is often worse than with listwise deletion, the default in most software.  \n\nThe extent of the bias depends on many factors, including the imputation method, the missing data mechanism, the proportion of the data that is missing, and the information available in the data set.  \n\nMoreover, all single imputation methods underestimate standard errors.\n\nReference: https:\/\/www.theanalysisfactor.com\/mar-and-mcar-missing-data\/\n\nMultiple imputation can overcome most of these shortcomings, but at the expense of time. They take more time to implement and run\n\nReference: https:\/\/www.theanalysisfactor.com\/missing-data-two-recommended-solutions\/\n\nMethods: (can use in pipeline - estimator = make_pipeline(imputer, regressor) ) reference: https:\/\/scikit-learn.org\/stable\/auto_examples\/impute\/plot_missing_values.html#sphx-glr-auto-examples-impute-plot-missing-values-py  \n\nSimple Regression:\n\nfrom sklearn.impute import SimpleImputer\nimp = SimpleImputer(missing_values=np.nan, strategy='mean') for numerical\nimp = SimpleImputer(strategy=\"most_frequent\") for categorical\n\nMultivariate Regression:\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer\nimp = IterativeImputer(max_iter=10, random_state=0)\n\nkNN:\nfrom sklearn.impute import KNNImputer\nimputer = KNNImputer(n_neighbors=2, weights=\"uniform\")\n\nOther:\nStochastic regression, Bayesian Linear Regression, Bayesian Binary Logistic Regression\nhttps:\/\/pypi.org\/project\/autoimpute\/","523bd380":"split train into train and validate 1458 records -> 70\/30 => 1000 Train\/400 Val","84162422":"**Getting dummy categorical features**","6ceb764d":"**Transforming some numerical variables that are really categorical**\n\nEdit: I will add a new column year and date as ordinal, since there is some order to the values it may be useful","25b34235":"###Missing Data","fd11950e":"**Stacking averaged Models Class**","f6d89be5":"**LightGBM:**","6ebf1fdf":"Find best cutoff and adjustment at low end","ddf73275":"Dropping even a single column gives a worse score\nConclusion: there is information in every column and the models are able to extract that information effectively!","6a6e73c3":"choose method to use, ensemble(average) or stacked","85ad8fd4":"**Box Cox Transformation of (highly) skewed features**","71174a12":"- **XGBoost** :","39f1556d":"- **Functional** : data description says NA means typical","d722af86":"Alternate Method to calculate missing Zoning values, neighborhood should be zoned the same most of the time","451ecee6":"As you can see, the PCA analysis did its job, the features show little correlation now","65c960b1":"Another way to combine multiple models:\n\nThe function cross_val_predict is appropriate for:\nVisualization of predictions obtained from different models.\n\nModel blending: When predictions of one supervised estimator are used to train another estimator in ensemble methods.","a36366ba":"<pre>\n1)   11781.87529     Original\n2)   11711.80408     move averaged_model code from Ensemble_73 notebook\n3)   11755.28295     move to stacked model\n4)   11888.77687     add CBoost_new to base_models:  (ENet, GBoost, KRR, BR, CBoost_new)\n5)   11888.77687     retry\n6)   11904.73136     remove GBoost (ENet, KRR, BR, CBoost_new)\n7)   11709.32360     revert to (ENet, GBoost, KRR, BR) use CBoost_new in the averaged part (xgb, lgb, cb) (.10,.10,.10)\n8)   11686.46788     try different ratio (.10,.05,.15)\n9)   11672.31686     try another ratio (.10,.01,.19) \n10)  11669.76682     another ratio (.05,.01,.24)\n11)  11677.88360     another ratio (.00,.00,.30)\n12)  11668.44304     remove lgb only (.05,.00,.25)\n13)  11669.04168     now try to increase the xgb ratio (.10,.00,.20)\n14)  11625.18894     model_xgb random_state=2 (was 7)\n15)  11613.88646     model_xgb random_state=35\n16)  11616.30327     check ensemble\/averaged score (score to beat is 11655)\n17)  13803.54887     use saved file from FeatureSelection=50 for all datasets\n18)  13351.98762     try with stacking\n19)  11613.88646     revert to #15, full dataset\n20)                  lr only, averaged_models -> 3 bad predictions (id=1594,2121,2905)\n21)  28232.03296     lr_poly only\n22)  14739.49921     ENet_poly\n23)  14197.84154     lr optimized (fit_intercept=False)\n24)  28963.03702     lr_poly optimized\n25)  28232.03296     lr_poly defaults\n26)  14197.84154     lr optimized #23\n27)  14201.67170     change high adjustment range for quantile: .994 to .980 and multiply by: 1.10 to 1.20\n28)  13924.57237     lr_poly using PCA to reduce the number of features (20k to 200), revert to #27 for adjustments\n29)  13789.09102     PCA(n_components=360)\n30)  15948.10915     NMF - Non-Negative Matrix Factorization: (n_components=360,defaults alpha = l1_ratio = 0)\n31)  13972.13677     optimized alpha and l1_ratio\n32) use SelectKBest: k = 360, f_regression => too many missing, very high and very low, values\n33)  13908.96621     CBoost_poly, Try SelectKBest with CBoost_new => works okay but score is worse. At least SelectKBest can produce valid scores for all rows with this algorithm\n34)  12945.71600     CBoost_poly: Try poly with CBoost_new, no feature selection\n35)  19349.45291     CBoost_poly: add PCA to poly, n_components=360\n36)  20887.29175     n_components=1000\n37)  24090.89712     PCA but use all features, add random_state=0\n38)  12838.23056     CBoost_new pipeline, with poly, n_estimators=3500, remove PCA as it does not perform well at any parameter settings\n39)  13397.69326     CBoost_new pipeline, with poly, optimized settings: k=1500, n_estimators=2000\n40)  11610.46960     use everything in the stacked part, ENet, GBoost, KRR, BR, xgb, lgb and cb still with .70 weighting\n41)  12456.65219     all stacked, CBoost only\n42)  12066.47424     all stacked, CBoost and KRR\n43)  11911.33866     all stacked, base_models = (CBoost_new, KRR, model_xgb)\n44)  11826.13805     all stacked, base_models = (CBoost_new, KRR, model_xgb, BR)\n45)  11839.80086     base_models = (CBoost_new, KRR, model_xgb, BR, GBoost)\n46)  11797.25832     base_models = (CBoost_new, KRR, model_xgb, BR, ENet), remove GBoost as it makes the score worse, add ENet\n47)  11799.19252     base_models = (CBoost_new, KRR, model_xgb, BR, ENet, model_lgb)\n48)  11613.88646     revert to #19 base_models = (ENet, KRR, BR, GBoost) + R_new\n49)  11726.83000     everything in the (.70) stacked part: base_models = (CBoost_new, KRR, model_xgb, BR, ENet, GBoost, model_lgb)\n50)  11799.28313     too many, use these only: base_models = (ENet, KRR, BR, CBoost_new)\n51)  13715.43547     try ENet only, stacking only\n52)  12712.76481     base_models = (ENet, KRR)\n53)  12751.55440     base_models = (ENet, KRR, BR)\n54)  11826.60122     base_models = (ENet, KRR, GBoost)\n55)  11879.33777     base_models = (ENet, KRR, GBoost, CBoost_new) n_estimators=1700\n56)  11696.61863     base_models = (ENet, KRR, GBoost, model_xgb)\n57)  11722.37128     base_models = (ENet, KRR, GBoost, model_xgb, model_lgb)\n58)  11767.30210     base_models = (ENet, KRR, GBoost, model_xgb, CBoost_new) n_estimators=3500\n59)  11694.95810     base_models = (ENet, KRR, GBoost, model_xgb, lr_poly), PCA(n_components=360)\n60)  11617.31540     reduce stacking to .70, add xgb .10, cb .20 back to model\n61)  11627.68558     add BR to averaged  xgbr .07, cbr .16, BR .07\n62)  11654.13031     replace BR with lgb: xgbr .07, cbr .16, lgbr .07\n63)  11616.89215     revert to #60\n64)  11613.71535     rerun same settings, lr_poly and stacked_average change each time\n65)  11692.93377     StackingAveragedModels n_folds=20\n66)  11616.85524     revert to #63, n_folds back to 10\n67)  11614.61863     look at errors for normal distribution, etc\n68)  14615.79289     use dnn model only in averaged_models calc\n69)  11722.85135     revert to averaged_models, using (CBoost_new,model_xgb,ENet,GBoost,KRR,BR)\n70)  11616.30327     use VotingRegressor with weights (CBoost_new,model_xgb,ENet,GBoost,KRR,BR)\n71)  11616.30327     fix lasso convergence errors\n72)  11609.72514**   try to fix ENet non-convergence, use tol = 0.01 and find best for other parameter settings\n73)  11615.05001     verify stacked method still working\n73e) 11609.72514     verify ensemble also\n74)  11608.09702     add dnn to stacked\/base model list\n74e) 11650.46685     verify ensemble, but using averaged not using VotingRegressor, use CBoost_new twice\n75)  11595.02554*    remove dnn from stacked, but leave it in the ensemble method, add seed = 7 for dnn\n76)  11603.44785     using dnn with SGD as optimizer, shuffle=False\n77)  11623.09195     SGD with shuffle=True\n78)  11600.44132     revert to Adam as optimizer\n79)  11645.80657     use LearningRateScheduler and SGD for dnn\n80)  11639.52100     use LearningRateScheduler with Adam for dnn\n81)  11604.18732     EarlyStopping on val_loss\n82)  11770.93048     normalize train and test data (to work better with dnn)\n82e) 13807.38879     test on averaged class, with normalized data\n83)  11636.50751     remove MinMax\n83e) 11704.90957     test ensemble\/averaged\n84)  11651.95404     use MinMax on dnn model only\n84e) 11655.72651     verify ensemble method\n85)  11644.03728     use dnn\/adam as metamodel only\n86)  11603.45534     not using dnn, revert to R_new as metamodel, upt enet_tol = 0.001 back in, max_iter=500\n\n\n\ncompare nmse for train for new vs old models, test is worse for new, is train also worse (underfit), or better (overfit)?\n\n* best score stacking and averaging\n** best score for averaging only\n\n\nfind and try other ridge regression and lasso regression algorithms and techniques\n\ndone:   \n    try pca on features => no improvement in score. will drop this for now\n    change ratios of models to find which is best predictor (gives best score), then work on improving that model first\n    change scale factor of 3 models => score does not correlate to rmsle, try to find a better metric\n    try StandardScaler and np.clip on features, currently see RobustScaler() being used but i already screened fliers so StandardScaler should be adequate\n    more filtering of fliers\n    use PCA or other for lr_poly pipeline as there may be too many features now\n    use GridsearchCV to find optimal parameters, however, need to find the best way to evaluate the scores. RMSLE doesn't seem to match well...\n\nto do:\n    plot the error (pred-act) and squared_error\n    add lgb to stacked, try RF,ET as meta-model even CBoost_new or lgb\n    need to investigate why using BoxCox1p is (slightly) better than the evaluation metric further...  \n    try different pipelines, scaler, etc\n    reference: https:\/\/scikit-learn.org\/stable\/auto_examples\/inspection\/plot_permutation_importance.html#sphx-glr-auto-examples-inspection-plot-permutation-importance-py\n    \n    ideas from other kernels:\n\n    add new features for non-linear features (OverallQual, etc) add a new column with log or square of feature\n       \n    tune min_samples_leaf and min_samples_split for GradientBoostingRegressor!\n    \n    add linear regression from here: https:\/\/github.com\/chouhbik\/Kaggle-House-Prices\/blob\/master\/Kaggle-house-prices-KamalChouhbi.ipynb\n    \nhttps:\/\/machinelearningmastery.com\/stacking-ensemble-machine-learning-with-python\/    \n    \nLook at this later for deep learning stacking: https:\/\/machinelearningmastery.com\/stacking-ensemble-for-deep-learning-neural-networks\/\n","11f14506":"Some ML algorithms need scaled data, such as those based on gradient boosting (linear regression, neural network) \n\n![image.png](attachment:df1e6ca7-96e3-482b-8f21-15ebe7bec3ab.png)\n\nand distance (KNN, SVM). Scaling the data in these cases will give each feature an equal chance to contribute to the result.\nMinMaxScaler equation:\n\n![image.png](attachment:d287d671-3a1d-49a0-b59a-f011fccd52a3.png)\n\nAlso some datasets need this normalization when the distributions are non-gaussian","12a5fd97":"On this gif, the base models are algorithms 0, 1, 2 and the meta-model is algorithm 3. The entire training dataset is \nA+B (target variable y known) that we can split into train part (A) and holdout part (B). And the test dataset is C. \n\nB1 (which is the prediction from the holdout part)  is the new feature used to train the meta-model 3 and C1 (which\nis the prediction  from the test dataset) is the meta-feature on which the final prediction is done. ","3fb82336":"Now try to improve upon this","f712404d":"Individual Model result comparision:\n\n![image.png](attachment:ee8be97e-2a74-4b3a-951f-8b7041cf8da7.png)","605ccdf7":"- **MasVnrArea and MasVnrType** : NA most likely means no masonry veneer for these houses. We can fill 0 for the area and None for the type. \n","2f9ae44b":"\nList of possible scoring values:  \nRegression  \n\n\u2018explained_variance\u2019 metrics.explained_variance_score  \n\u2018max_error\u2019 metrics.max_error  \n\u2018neg_mean_absolute_error\u2019 metrics.mean_absolute_error  \n\u2018neg_mean_squared_error\u2019 metrics.mean_squared_error  \n\u2018neg_root_mean_squared_error\u2019 metrics.mean_squared_error  \n\u2018neg_mean_squared_log_error\u2019 metrics.mean_squared_log_error  \n\u2018neg_median_absolute_error\u2019 metrics.median_absolute_error  \n\u2018r2\u2019 metrics.r2_score  \n\u2018neg_mean_poisson_deviance\u2019 metrics.mean_poisson_deviance  \n\u2018neg_mean_gamma_deviance\u2019 metrics.mean_gamma_deviance  ","dc9d170d":"Seems that RMSLE does not correlate to a good score\n\n![image.png](attachment:image.png)","c5a46b2e":"Making the same adjustment on the test data for our submission","0bc5bc82":"- **Random Forest Regressor** :","d0e765e6":"Compare Train to Test data, verify the distributions look similar, maybe add probablity plots per feature with train and test on same chart","ab52a6dd":"###Final Training and Prediction","38e41750":"#Data Processing","dcae80f5":"**Import libraries**","5df5841e":"The skew seems now corrected and the data appears more normally distributed. \n\nEdit: Both distributions have a positive kurtosis, which means there is a steep dropoff in the curve from the center, or the tails have few points. Skewness is close to 0 now, so that metric is closer to a normal distribution. BoxCox1p() has worse numbers than log() but still performs better in the predictions, will keep BoxCox1p","9dfa8a7e":"We will be using Stacking and Ensembling methods in this notebook. I will only create one dataset for all models to use, but you can create multiple datasets, each to be used by a different set of models, depending on your use case.\n\n![image.png](attachment:image.png)","3eb2c855":"xgb reference: https:\/\/xgboost.readthedocs.io\/en\/latest\/parameter.html","3b82744f":"Compare the r-squared values","c1f6d3ad":"I am getting conflicting results for  best params, sometimes huber or lad and sometimes friedman_mse or mae, so will look at more detailed output. This style output is much more useful for deciding between parameter values, adding the different random states shows the consistency, or lack of, for each setting","f5e132f1":"**Ensemble prediction:**\n\nwhen deciding which models to include in an ensemble:  \n    fewer are better  \n    more diverse are better  ","e9263ca7":"I recommend plotting the cumulative sum of eigenvalues (assuming they are in descending order). If you divide each value by the total sum of eigenvalues prior to plotting, then your plot will show the fraction of total variance retained vs. number of eigenvalues. The plot will then provide a good indication of when you hit the point of diminishing returns (i.e., little variance is gained by retaining additional eigenvalues).","a7bcd499":"- **Gradient Boosting Regression** :\n\nWith **huber**  loss that makes it robust to outliers\n    ","07bbd5bc":"This model needs improvement, will run cross validate on it","35b5a628":"Visualize the results","a84e21fd":"Need to look at the y_log relationship since that is what we will be predicting in the model (convert back later)","4ad1d687":"Residuals Plot: Actual vs Predicted for Train data - test for normality of residual errors\ncan't plot the stacked model, get an error so plot each model individually","e66cfadd":"![image.png](attachment:image.png)","317a94c9":"Another look at the feature to output correlations","a4051153":"- **Additional testing**\n\nCompare any random state","0e8a1eee":"###Note : \n Outliers removal is note always safe.  We decided to delete these two as they are very huge and  really  bad ( extremely large areas for very low  prices). \n\nThere are probably others outliers in the training data.   However, removing all them  may affect badly our models if ever there were also  outliers  in the test data. That's why , instead of removing them all, we will just manage to make some of our  models robust on them. You can refer to  the modelling part of this notebook for that. \n\nEdit: todo - look for other outliers in all columns. Maybe better to use StandardScaler() and np.clip","e36945f8":"- **Utilities** : For this categorical feature all records are \"AllPub\", except for one \"NoSeWa\"  and 2 NA . Since the house with 'NoSewa' is in the training set, **this feature won't help in predictive modelling**. We can then safely  remove it.\n","0b21658f":"let's first  concatenate the train and test data in the same dataframe","616c9278":"Calculate Metrics for model list","baad2adc":"##Features engineering","35b4cfc8":"We get again a better score by adding a meta learner","42b3c305":"separate into lists for each data type","13f6890a":"maybe do a groupby to make this table more manageable and easier to read","0f27c3c6":"**Stacking Averaged models Score**","baefc3c6":"###Base models scores","7065c320":"**Averaged base models class**\n\nIn averaging methods, the driving principle is to build several estimators independently and then to average their predictions. On average, the combined estimator is usually better than any of the single base estimator because its variance is reduced.","0208346e":"Then, add this column from previous investigation to the dataset y = curve_fit_gla[2] + curve_fit_gla[1]x_data + curve_fit_gla[0]x_data**2","ebe3747c":"Show new predictions","992f747b":"Let's see how these base models perform on the data by evaluating the  cross-validation rmsle error","72c1b0de":"Kfold is useful for thorough testing of a model, will give a more accurate score based on remove some data test on the remaining and change the data removed each time. See image below for details:","90074c18":"**SalePrice** is the variable we need to predict. So let's do some analysis on this variable first.","bb31b9c4":"- **Elastic Net Regression** :\n\nagain made robust to outliers\n\ncombines Lasso L1 Linear regularization and Ridge L2 Quadratic\/Squared regularization penalties together into one algorithm","f20f0374":"Save Cleansed Data to disk","a254279b":"This fit looks like it may be better, will add a column to the dataset using this equation after it is merged (all_data)\n\ny = curve_fit_gla[2] + curve_fit_gla[1]*x_data + curve_fit_gla[0]*x_data**2\n\nx_data = train['GrLivArea']\n","534440b1":"correlation looks much better at the high end","9cb44208":"Since area related features are very important to determine house prices, we add one more feature which is the total area of basement, first and second floor areas of each house","97365d27":"- **LotFrontage** : Since the area of each street connected to the house property most likely have a similar area to other houses in its neighborhood , we can **fill in missing values by the median LotFrontage of the neighborhood**.","b6737d68":"**Data Correlation**\n","761358c5":"- **Additional testing**\n\nCompare any random state","3e5b2923":"Edit: Look for fliers in other columns","12890b0e":"[Documentation][1] for the Ames Housing Data indicates that there are outliers present in the training data\n[1]: http:\/\/ww2.amstat.org\/publications\/jse\/v19n3\/Decock\/DataDocumentation.txt","f4992466":"###Less simple Stacking : Adding a Meta-model","8f60d420":"###Imputing missing values ","872b692a":"Final stacked model","df14e5cc":"distribution of residual errors looks normal except for ET() and SVR()","1d4ba372":"use the 0.3 value as the improvement below this value is minimal","f0ea60ec":"Safe to drop the Id column now as we are finished deleting points (otherwise our Train_ID list will be smaller than y_train","18eb84a7":"Now lets use a log y scale","2db744a4":"##Target Variable","a8ffc8a1":"- **GarageYrBlt, GarageArea and GarageCars** : Replacing missing data with 0 (Since No garage = no cars in such garage.)\n- **BsmtFinSF1, BsmtFinSF2, BsmtUnfSF, TotalBsmtSF, BsmtFullBath and BsmtHalfBath** : missing values are likely zero for having no basement\n","db1fda42":"Plot prediction vs actual for train for one last verification of the model","24870352":"More filtering of data to try","df556598":" We choose number of eigenvalues to calculate based on previous chart, 20 looks like a good number, the chart starts to roll off around 15 and almost hits max a 20. We can also try 30,40 and 50 to squeeze a little more variance out...","2d9583a9":"Testing the new PCA dataset for analysis - RMSE looks works after PCA is applied, need to look at the Kaggle score later and see if it correlates, could be a mistake to use PCA on categorical dummy data. However, XGB is better with PCA n=50 option. Maybe use a heavier weight for that portion, or use all_data_pca only on that model...","5417e63b":"- **Exterior1st and Exterior2nd** : Again Both Exterior 1 & 2 have only one missing value. We will just substitute in the most common string\n","0196bf55":"**Define a cross validation strategy**","b5a0269f":"Keras earning rate and early stopping during training\n\nhttps:\/\/machinelearningmastery.com\/how-to-stop-training-deep-neural-networks-at-the-right-time-using-early-stopping\/\n\nhttps:\/\/www.tensorflow.org\/guide\/keras\/custom_callback","c25b3131":"- **MSZoning (The general zoning classification)** :  'RL' is by far  the most common value.  So we can fill in missing values with 'RL'\n","428bc4a9":"Do some PCA for the dataset, to remove some of the collinearity. Not sure if this will have any effect as collinearity is usually not detrimental to many or most algorithms","0e9cdb81":"My equation has a worse correlation... will need to investigate this later. For now, i will simplify an just use a direct function...","3fd6b4e5":"We begin with this simple approach of averaging base models.  We build a new **class**  to extend scikit-learn BaseEstimator, RegressorMixin, TransformerMixin classes with our model and also to leverage encapsulation and code reuse ([inheritance][1]) \n\n\n  [1]: https:\/\/en.wikipedia.org\/wiki\/Inheritance_(object-oriented_programming)","794ff7f1":"Show adjustments","69d6b80f":"<pre>\nTypical flow of model building: Use GridSearch for determining Best parameters => **best_estimator_ \n\ngood basic resource for models: \nhttps:\/\/scikit-learn.org\/stable\/modules\/cross_validation.html\nhttps:\/\/scikit-learn.org\/stable\/modules\/grid_search.html\n\nA search consists of:\nan estimator (regressor or classifier such as sklearn.svm.SVC());  \na parameter space;  \na method for searching or sampling candidates;  \na cross-validation scheme; and  \na score function.  ****\n","82712c56":"Compare the r-squared values for different functions","273af123":"-  **LASSO  Regression**  : \n\nThis model may be very sensitive to outliers. So we need to made it more robust on them. For that we use the sklearn's  **Robustscaler()**  method on pipeline, also want to compare to StandardScaler() => RobustScaler() is slightly better","2903d222":"Use another Stacking function, from mlxtend","6c6334a5":"Check predictions, are they on same scale as SalePrice in Train dataset?","ef9b1901":"Let's explore these outliers\n","621e3ce5":"- **Modelling**","1f911f97":"Diagram of out of fold cross validated stacked models\n\n\n![image.png](attachment:image.png)\n\nReference: http:\/\/rasbt.github.io\/mlxtend\/user_guide\/regressor\/StackingCVRegressor\/","aaaf58df":"multilayer stacking","815f0be3":"**XGBoost:**","3d02baec":"Add CBoost model from Ensemble notebook","e1423b58":"###Simplest Stacking approach : Averaging base models","1669d26b":"We use the **cross_val_score** function of Sklearn. However this function has no shuffle attribute, so we add one line of code,  in order to shuffle the dataset  prior to cross-validation","21045877":"- **SaleType** : Fill in again with most frequent which is \"WD\"","dbf6bf54":"Look at some correlation values in a list format","8c2059f5":"* save this inv function for later, may need it:\n<pre>\nimport cmath\nd = (b**2) - (4*a*c)\nprint(-b-cmath.sqrt(d))\/(2*a)\nprint(-b+cmath.sqrt(d))\/(2*a)\ndef add_gla3(row, p):\n    return (p[2] + p[1]*row.GrLivArea + p[0]*(row.GrLivArea**2)) <-- change this function\nall_data['GrLivAreaPoly2'] = all_data.apply(lambda row: add_gla3(row,curve_fit_gla), axis=1)","4ec9ab66":"Now for r squared calculations we need to limit the comparison to only the train data since the test data does not have a Sales Price to compare to","668398df":"Try Bayesian Optimization\nreference: https:\/\/blog.floydhub.com\/guide-to-hyperparameters-search-for-deep-learning-models\/\n\nTune other parameters\nreference: https:\/\/machinelearningmastery.com\/grid-search-hyperparameters-deep-learning-models-python-keras\/","0e71e628":"Example plot\n\n![image.png](attachment:image.png)","98cd0e2e":"Other ML Algorithms need StandardScaler, such as SVR\n","9ac520c2":"##Outliers","5c968d28":"Look for any good correlations to use for imputation","35bb8067":"* check and compare the new columns","f79031f2":"**StackedRegressor:**","f03dfd4f":"- **Kernel Ridge Regression** :","5990cdc0":"replace cross_val_score() with cross_validate()\n# reference: https:\/\/scikit-learn.org\/stable\/modules\/cross_validation.html\n\n    from sklearn.metrics import make_scorer\n    scoring = {'prec_macro': 'precision_macro',\n               'rec_macro': make_scorer(recall_score, average='macro')}\n    scores = cross_validate(clf, X, y, scoring=scoring,\n                            cv=5, return_train_score=True)\n    sorted(scores.keys())\n    ['fit_time', 'score_time', 'test_prec_macro', 'test_rec_macro',\n     'train_prec_macro', 'train_rec_macro']\n    scores['train_rec_macro']\n    array([0.97..., 0.97..., 0.99..., 0.98..., 0.98...])","a3c9ddf2":"try votingRegressor, it allows different weighting for each model","e5db9dd9":"**Label Encoding some categorical variables that may contain information in their ordering set** ","c9f2cfc0":"Find best cutoff and adjustment at high end","f135aae6":"## Ensembling StackedRegressor, XGBoost and LightGBM","3535685c":"**Skewed features**","0418753c":"Try dropping the lowest correlating columns","bc9d03ab":"- **KitchenQual**: Only one NA value, and same as Electrical, we set 'TA' (which is the most frequent)  for the missing value in KitchenQual.\n","87c9a769":"We have bumped up the predictions and they look correct so far, now to verify on the previous chart","331d50db":"At the high end, SP > 520000, the model predicts too low for 7\/8 points","80a4feeb":"plot the poly fit data","f89c9f8e":"Polynomial pipeline\n\nreference: https:\/\/scikit-learn.org\/stable\/auto_examples\/model_selection\/plot_underfitting_overfitting.html#sphx-glr-auto-examples-model-selection-plot-underfitting-overfitting-py\n","09f7e7e1":"- **LightGBM** :","2d3c3b5d":"alternate data source using top n features from Sequential Feature Selector (FeatureSelection.ipynb)\n\nReference: http:\/\/rasbt.github.io\/mlxtend\/user_guide\/feature_selection\/SequentialFeatureSelector\/\n\n![image.png](attachment:00dcc0bf-31c4-441c-a124-3259de47d99c.png)","d8bd79a5":"Getting the new train and test sets. ","cbb4df94":"looks like we have 2, MasVnrArea and BsmtFinSF1 so use those for this test"}}