{"cell_type":{"7dec05d1":"code","3d7e8a8e":"code","588c2a6e":"code","73c0c870":"code","5fc3dc98":"code","4e4083ea":"code","1fb9579b":"code","32bfb86c":"code","efc795a9":"code","3399f313":"code","32a6f4fe":"code","8733f81b":"code","9734d0bc":"code","8672e991":"code","7c4a6e56":"code","3b944298":"code","358664d9":"code","5a87cba5":"code","85d150d6":"code","91b7455c":"code","dac4ee07":"markdown","012473ea":"markdown","f62acd75":"markdown","3f28b5ae":"markdown","55a147b7":"markdown","87b01c87":"markdown","a6ba0d4f":"markdown"},"source":{"7dec05d1":"import numpy as np \nimport pandas as pd\n\n# https:\/\/www.kaggle.com\/friedchips\/clean-removal-of-data-drift\/output\nimport os\nimport random as rn\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n#!pip install optuna\n#import optuna","3d7e8a8e":"# imports\n%matplotlib inline\n\nimport gc\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy import signal\nfrom tqdm import tqdm\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import confusion_matrix, f1_score, plot_confusion_matrix\nfrom keras.models import Model\nfrom keras.optimizers import Adagrad\nimport keras.layers as L\nimport lightgbm as lgb\nimport xgboost as xgb\nimport pickle\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout\nfrom keras.layers.convolutional import Conv1D, UpSampling1D\nfrom keras.layers.pooling import MaxPool1D\nfrom keras.layers.core import Dense, Activation, Dropout, Flatten\nfrom keras.layers.pooling import MaxPooling1D\nfrom sklearn.preprocessing import OneHotEncoder","588c2a6e":"os.environ['PYTHONHASHSEED'] = '0'\nnp.random.seed(7)\nrn.seed(7)","73c0c870":"MLP_EPOCH_NUM = 1#30\nLGBM_BOOST_NUM = 1#3000\nFOLD_NUM = 4","5fc3dc98":"# read data\ndata = pd.read_csv('..\/input\/data-without-drift\/train_clean.csv')\ndata.iloc[478587:478588, [1]] = -2\ndata.iloc[478609:478610, [1]] = -2\ndata_ = data[3500000:3642922].append(data[3822754:4000000])\ndata = data[:3500000].append(data[4000000:]).reset_index().append(data_, ignore_index=True)\ndata.head()\ndata[[\"signal\", \"open_channels\"]].plot(figsize=(19,5), alpha=0.7)","4e4083ea":"def calc_gradients(s, n_grads=3):\n    '''\n    Calculate gradients for a pandas series. Returns the same number of samples\n    '''\n    grads = pd.DataFrame()\n    \n    g = s.values\n    for i in range(n_grads):\n        g = np.gradient(g)\n        grads['grad_' + str(i+1)] = g\n        \n    return grads","1fb9579b":"def calc_low_pass(s, n_filts=10):\n    '''\n    Applies low pass filters to the signal. Left delayed and no delayed\n    '''\n    wns = np.logspace(-2, -0.9, n_filts)\n    \n    low_pass = pd.DataFrame()\n    x = s.values\n    for wn in wns:\n        b, a = signal.butter(1, Wn=wn, btype='low')\n        zi = signal.lfilter_zi(b, a)\n        low_pass['lowpass_lf_' + str('%.4f' %wn)] = signal.lfilter(b, a, x, zi=zi*x[0])[0]\n        low_pass['lowpass_ff_' + str('%.4f' %wn)] = signal.filtfilt(b, a, x)\n        \n    return low_pass","32bfb86c":"def calc_high_pass(s, n_filts=10):\n    '''\n    Applies high pass filters to the signal. Left delayed and no delayed\n    '''\n    wns = np.logspace(-2, -0.9, n_filts)\n    \n    high_pass = pd.DataFrame()\n    x = s.values\n    for wn in wns:\n        b, a = signal.butter(1, Wn=wn, btype='high')\n        zi = signal.lfilter_zi(b, a)\n        high_pass['highpass_lf_' + str('%.4f' %wn)] = signal.lfilter(b, a, x, zi=zi*x[0])[0]\n        high_pass['highpass_ff_' + str('%.4f' %wn)] = signal.filtfilt(b, a, x)\n        \n    return high_pass","efc795a9":"def calc_roll_stats(s, windows=[3, 10, 50, 100, 500]):\n    '''\n    Calculates rolling stats like mean, std, min, max...\n    '''\n    roll_stats = pd.DataFrame()\n    for w in windows:\n        #roll_stats['roll_mean_2_' + str(w)] = s.rolling(window=2*w, min_periods=1).mean().shift(-w)\n        #roll_stats['roll_std_2_' + str(w)] = s.rolling(window=2*w, min_periods=1).std().shift(-w)\n        #roll_stats['roll_min_2_' + str(w)] = s.rolling(window=2*w, min_periods=1).min().shift(-w)\n        #roll_stats['roll_max_2_' + str(w)] = s.rolling(window=2*w, min_periods=1).max().shift(-w)\n        #roll_stats['roll_range_2_' + str(w)] = roll_stats['roll_max_2_' + str(w)] - roll_stats['roll_min_2_' + str(w)].shift(-w)\n        roll_stats['roll_mean_' + str(w)] = s.rolling(window=w, min_periods=1).mean()\n        roll_stats['roll_std_' + str(w)] = s.rolling(window=w, min_periods=1).std()\n        roll_stats['roll_min_' + str(w)] = s.rolling(window=w, min_periods=1).min()\n        roll_stats['roll_max_' + str(w)] = s.rolling(window=w, min_periods=1).max()\n        roll_stats['roll_range_' + str(w)] = roll_stats['roll_max_' + str(w)] - roll_stats['roll_min_' + str(w)]\n        roll_stats['roll_mean_s_' + str(w)] = s.rolling(window=w, min_periods=1).mean().shift(-w)\n        roll_stats['roll_std_s_' + str(w)] = s.rolling(window=w, min_periods=1).std().shift(-w)\n        roll_stats['roll_min_s_' + str(w)] = s.rolling(window=w, min_periods=1).min().shift(-w)\n        roll_stats['roll_max_s_' + str(w)] = s.rolling(window=w, min_periods=1).max().shift(-w)\n        roll_stats['roll_range_s_' + str(w)] = roll_stats['roll_max_s_' + str(w)] - roll_stats['roll_min_s_' + str(w)]\n        roll_stats['roll_min_abs_' + str(w)] = s.rolling(window=2*w, min_periods=1).min().abs().shift(-w)\n        roll_stats['roll_range_sbs_' + str(w)] = roll_stats['roll_max_' + str(w)] - roll_stats['roll_min_abs_' + str(w)].shift(-w)\n        roll_stats['roll_q10_' + str(w)] = s.rolling(window=2*w, min_periods=1).quantile(0.10).shift(-w)\n        roll_stats['roll_q25_' + str(w)] = s.rolling(window=2*w, min_periods=1).quantile(0.25).shift(-w)\n        roll_stats['roll_q50_' + str(w)] = s.rolling(window=2*w, min_periods=1).quantile(0.50).shift(-w)\n        roll_stats['roll_q75_' + str(w)] = s.rolling(window=2*w, min_periods=1).quantile(0.75).shift(-w)\n        roll_stats['roll_q90_' + str(w)] = s.rolling(window=2*w, min_periods=1).quantile(0.90).shift(-w)\n        roll_stats['mean_abs_chg' + str(w)] = roll_stats.apply(lambda x: np.mean(np.abs(np.diff(x))))\n    \n    # add zeros when na values (std)\n    roll_stats = roll_stats.fillna(value=0)\n             \n    return roll_stats","3399f313":"def calc_ewm(s, windows=[10, 50, 100, 1000]):\n    '''\n    Calculates exponential weighted functions\n    '''\n    ewm = pd.DataFrame()\n    for w in windows:\n        ewm['ewm_mean_' + str(w)] = s.ewm(span=w, min_periods=1).mean()\n        ewm['ewm_std_' + str(w)] = s.ewm(span=w, min_periods=1).std()\n        \n    # add zeros when na values (std)\n    ewm = ewm.fillna(value=0)\n        \n    return ewm","32a6f4fe":"def add_features(s):\n    '''\n    All calculations together\n    '''\n    gradients = calc_gradients(s)\n    low_pass = calc_low_pass(s)\n    high_pass = calc_high_pass(s)\n    roll_stats = calc_roll_stats(s)\n    ewm = calc_ewm(s)\n    \n    return pd.concat([s, gradients, low_pass, high_pass, roll_stats, ewm], axis=1)\n\n\ndef divide_and_add_features(s, signal_size=500000):\n    '''\n    Divide the signal in bags of \"signal_size\".\n    Normalize the data dividing it by 15.0\n    '''\n    # normalize\n    s = s\/15.0\n    \n    ls = []\n    for i in tqdm(range(int(s.shape[0]\/signal_size))):\n        sig = s[i*signal_size:(i+1)*signal_size].copy().reset_index(drop=True)\n        sig_featured = add_features(sig)\n        ls.append(sig_featured)\n    if len(s) > 4000000:\n        sig = s[(i+1)*signal_size:4820168].copy().reset_index(drop=True)\n        sig_featured = add_features(sig)\n        ls.append(sig_featured)\n    \n    df = pd.concat(ls, axis=0)\n    df['signal_shift_+1'] = [0,] + list(df['signal'].values[:-1])\n    df['signal_shift_-1'] = list(df['signal'].values[1:]) + [0]\n    df['signal_shift_+2'] = [0,] + [1,] + list(df['signal'].values[:-2])\n    df['signal_shift_-2'] = list(df['signal'].values[2:]) + [0] + [1]\n    df['signal_shift_+3'] = [0,] + [1,] + [1,] + list(df['signal'].values[:-3])\n    df['signal_shift_-3'] = list(df['signal'].values[3:]) + [0] + [1] + [2]\n    return df","8733f81b":"# apply every feature to data\ndf = divide_and_add_features(data['signal'])\ndf.head()","9734d0bc":"def reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() \/ 1024**2\n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)\n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n\n    end_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) \/ start_mem))\n\n    return df\ndf = reduce_mem_usage(df)","8672e991":"# read m_data\nm_data = pd.read_csv('..\/input\/viterbi\/Viterbi_train.csv', names=[\"gb\", \"signal\"])\nm_data_ = m_data[3500000:3642922].append(m_data[3822754:4000000])\nm_data = m_data[:3500000].append(m_data[4000000:]).reset_index().append(m_data_, ignore_index=True)\nm_data.head()\nm_data[[\"signal\"]].plot(figsize=(19,5), alpha=0.7)\ndf[\"m_signal\"] = m_data[\"signal\"][:-1].values\/15\ndf = reduce_mem_usage(df)\ndf_columns = df.columns","7c4a6e56":"def get_class_weight(classes, exp=1):\n    '''\n    Weight of the class is inversely proportional to the population of the class.\n    There is an exponent for adding more weight.\n    '''\n    hist, _ = np.histogram(classes, bins=np.arange(12)-0.5)\n    class_weight = hist.sum()\/np.power(hist, exp)\n    \n    return class_weight","3b944298":"def create_mpl(shape):\n    '''\n    Returns a keras model\n    '''\n    \n    model = Sequential()\n    model.add(Conv1D(128,3,input_shape=shape))\n    model.add(Activation('relu'))\n    model.add(Conv1D(128,3))\n    model.add(Activation('relu'))\n    model.add(MaxPool1D(pool_size=2))\n\n    model.add(Conv1D(252,3))\n    model.add(Activation('relu'))\n    model.add(MaxPool1D(pool_size=2))\n\n    model.add(Flatten())\n    model.add(Dense(1024))\n    model.add(Activation('relu'))\n    model.add(Dropout(1.0))\n\n    model.add(L.Dense(11, activation='softmax'))\n    \n    return model","358664d9":"def lgb_Metric(preds, dtrain):\n    labels = dtrain.get_label()\n    num_labels = 11\n    preds = preds.reshape(num_labels, len(preds)\/\/num_labels)\n    preds = np.argmax(preds, axis=0)\n    score = f1_score(labels, preds, average=\"macro\")\n    return ('KaggleMetric', score, True)\nlgb_r_params = {\n    'objective': 'multiclass',\n    'num_class': 11,\n    'metric': 'multi_logloss',\n    'learning_rate': 0.00987173774816051,\n    'lambda_l1': 0.00031963798315506463,\n    'lambda_l2': 0.18977456778807847,\n    'num_leaves': 171, \n    'feature_fraction': 0.58733782457345, \n    'bagging_fraction': 0.7057826081907392, \n    'bagging_freq': 4}","5a87cba5":"import pandas as pd\ntest_data = pd.read_csv('..\/input\/data-without-drift\/test_clean.csv')\n\ntest_df = divide_and_add_features(test_data['signal'])\ntest_df = reduce_mem_usage(test_df)\n# read m_data\nm_data = pd.read_csv('..\/input\/viterbi\/Viterbi_test.csv').rename(columns={\"open_channels\": \"signal\"})\nm_data_ = m_data[3500000:3642922].append(m_data[3822754:4000000])\nm_data = m_data[:3500000].append(m_data[4000000:]).reset_index().append(m_data_, ignore_index=True)\nm_data.head()\nm_data[[\"signal\"]].plot(figsize=(19,5), alpha=0.7)\ntest_df[\"m_signal\"] = m_data[\"signal\"].values\/15\ntest_df = reduce_mem_usage(test_df)\ntest_df.shape","85d150d6":"kf = KFold(n_splits=FOLD_NUM, shuffle=True, random_state=42)\n\npreds = np.zeros(2000000*11).reshape((2000000, 11))\n\nX = df#.values\ny = data['open_channels']\ny_values = data['open_channels'].values\n\nfor i, (tdx, vdx) in enumerate(kf.split(X, y)):\n    print(f'Fold : {i}')\n    X_train, X_valid, y_train, y_valid = X.iloc[tdx], X.iloc[vdx], y_values[tdx], y_values[vdx]\n    #X_train, X_valid, y_train, y_valid = X[tdx], X[vdx], y_values[tdx], y_values[vdx]\n    print(f\"sep: {X_train.shape}, {X_valid.shape}, {y_train.shape}, {y_valid.shape}\")\n    \n    #MLP\n    mlp = create_mpl((X_train.values.shape[1], 1))\n    #mlp = create_mpl(X_train[0].shape)\n    mlp.compile(optimizer=\"adam\", loss='sparse_categorical_crossentropy', metrics=['sparse_categorical_accuracy'])\n    class_weight = get_class_weight(y_train)\n\n    mlp.fit(x=np.reshape(X_train.values, (-1, X_train.shape[1], 1)), y=y_train, epochs=MLP_EPOCH_NUM, batch_size=1024, \n            class_weight=class_weight,\n            validation_data=(np.reshape(X_valid.values, (-1, X_valid.shape[1], 1)), y_valid))\n    #mlp.fit(x=X_train, y=y_train, epochs=MLP_EPOCH_NUM, batch_size=1024, class_weight=class_weight,\n    #       validation_data=(X_valid, y_valid), verbose=0)\n    mlp_pred = mlp.predict(np.reshape(X_valid.values, (-1, X_valid.shape[1], 1)))\n    #mlp_pred = mlp.predict(X_valid)\n    f1_mlp = f1_score(y_valid, np.argmax(mlp_pred, axis=-1), average='macro')\n    print(f\"f1 score is :{f1_mlp}\")\n    plt.figure(1)\n    plt.plot(mlp.history.history['loss'], 'b', label='loss')\n    plt.plot(mlp.history.history['val_loss'], 'r', label='loss')\n    plt.xlabel('epochs')\n    plt.legend()\n    plt.figure(2)\n    plt.plot(mlp.history.history['sparse_categorical_accuracy'], 'g', label='sparse_categorical_accuracy')\n    plt.plot(mlp.history.history['val_sparse_categorical_accuracy'], 'r', label='val_sparse_categorical_accuracy')\n    plt.xlabel('epochs')\n    plt.legend()\n    plt.show()\n    \n    # GBC\n    #lgb_dataset = lgb.Dataset(X_train, label=y_train, weight=class_weight[y_train])\n    lgb_dataset = lgb.Dataset(X_train.values, label=y_train, weight=class_weight[y_train])\n    #lgb_valid_dataset = lgb.Dataset(X_valid, label=y_valid, weight=class_weight[y_valid])\n    lgb_valid_dataset = lgb.Dataset(X_valid.values, label=y_valid, weight=class_weight[y_valid])\n    print('Training LGBM...')\n    gbc = lgb.train(lgb_r_params, lgb_dataset, LGBM_BOOST_NUM, valid_names=[\"train\", \"valid\"], \n                    valid_sets=[lgb_dataset, lgb_valid_dataset], verbose_eval=-1, \n                    feval=lgb_Metric, early_stopping_rounds=10)\n    print('LGBM trained!')\n    # predict on test\n    gbc_pred = gbc.predict(X_valid.values, num_iteration=gbc.best_iteration)\n    attr2 = {k: v for k, v in zip(df_columns, gbc.feature_importance()) if 200 > v and v>0}\n    print(\"weak fe##############\")\n    print(attr2)\n    print(\"##############\")\n    #gbc_pred = gbc.predict(X_valid, num_iteration=gbc.best_iteration)\n    print(f1_score(y_valid, np.argmax(gbc_pred, axis=1), average='macro'))\n    \n    # lists for keep results\n    f1s = []\n    alphas = []\n\n    # loop for every alpha\n    for alpha in tqdm(np.linspace(0,1,101)):\n        #y_pred = alpha*mlp_pred + (1 - alpha)*np.round(np.clip(gbc_pred, 0, 10)).astype(int)\n        y_pred = alpha*mlp_pred + (1 - alpha)*gbc_pred\n        f1 = f1_score(y_valid, np.argmax(y_pred, axis=1), average='macro')\n        f1s.append(f1)\n        alphas.append(alpha)\n\n    # convert to numpy arrays\n    f1s = np.array(f1s)\n    alphas = np.array(alphas)\n\n    # get best_alpha\n    best_alpha = alphas[np.argmax(f1s)]\n\n    print('best_f1=', f1s.max())\n    print('best_alpha=', best_alpha)\n    plt.plot(alphas, f1s)\n    plt.title('f1_score for ensemble')\n    plt.xlabel('alpha')\n    plt.ylabel('f1_score')\n    plt.show()\n\n    mlp_pred = mlp.predict(np.reshape(test_df.values, (-1, test_df.shape[1], 1)))\n    #mlp_pred = mlp.predict(test_df.values)\n    gbc_pred = gbc.predict(test_df, num_iteration=gbc.best_iteration)\n    #gbc_pred = gbc.predict(test_df.values, num_iteration=gbc.best_iteration)\n    pred = best_alpha*mlp_pred + (1 - best_alpha)*gbc_pred\n    preds += pred\n    \n    \n    print(f\"f1_mlp is {f1_mlp}\")\n    gc.collect()\n","91b7455c":"pred = np.argmax(preds, axis=1)\n\nprint('Writing submission...')\nsubmission = pd.DataFrame()\nsubmission['time'] = test_data['time']\n\nsubmission['open_channels'] = pred\nsubmission.to_csv('mlp_gbc_submission.csv', index=False, float_format='%.4f')\n\nprint('Submission finished!')","dac4ee07":"Let's plot the signals to see how they look like.","012473ea":"# reffrence  \nhttps:\/\/www.kaggle.com\/martxelo\/fe-and-ensemble-mlp-and-lgbm","f62acd75":"# Load data","3f28b5ae":"# Classes weights","55a147b7":"# Submit result","87b01c87":"# Build a MLP model","a6ba0d4f":"# Feature engineering\nAdd to signal several other signals: gradients, rolling mean, std, low\/high pass filters...\n\nFE is the same as this notebook https:\/\/www.kaggle.com\/martxelo\/fe-and-simple-mlp with corrections in filters."}}