{"cell_type":{"8594f7ea":"code","7ed9a44e":"code","649b8871":"code","32754311":"code","47d99cbf":"code","948774a0":"code","b21716fb":"code","cad8298f":"code","40ec147e":"markdown","1f13dcb0":"markdown","6e6ba8b2":"markdown","a64ebe0e":"markdown","9fdf7b76":"markdown","ce5f9c73":"markdown","d0d0063c":"markdown"},"source":{"8594f7ea":"import numpy as np\nimport pandas as pd \nfrom sklearn.model_selection import KFold\n\nimport numpy as np\nimport pandas as pd \n\nimport os\nimport gc\nimport psutil\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler, Normalizer,MinMaxScaler\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import roc_auc_score\n\nfrom xgboost import XGBClassifier, XGBRegressor\nfrom sklearn.preprocessing import PowerTransformer\nfrom sklearn.preprocessing import PolynomialFeatures\n\nfrom optuna.integration import LightGBMPruningCallback\n\n# get skewed features to impute median instead of mean\nfrom scipy.stats import skew\nfrom sklearn.ensemble import AdaBoostClassifier\n\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn import linear_model\nfrom sklearn.linear_model import Ridge,Lasso\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.decomposition import PCA\n\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.metrics import roc_auc_score\nfrom xgboost import XGBRegressor, XGBRFRegressor\n\nimport itertools\nimport optuna\nfrom lightgbm import LGBMClassifier,LGBMRegressor\nimport lightgbm as lgb\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom catboost import CatBoostRegressor, CatBoostClassifier\n\nfrom sklearn.experimental import enable_hist_gradient_boosting\nfrom sklearn.ensemble import HistGradientBoostingClassifier\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","7ed9a44e":"train_data = pd.read_csv('..\/input\/titanic-create-folds\/TITANIC_Folds.csv')\ntest_data = pd.read_csv(\"..\/input\/titanic\/test.csv\")","649b8871":"train_data.drop('Cabin',axis= 1,inplace= True)\ntest_data.drop('Cabin',axis= 1,inplace= True)\ntrain_data['Age'] = train_data['Age'].fillna(int(train_data['Age'].mean()))\ntest_data['Age'] = test_data['Age'].fillna(int(test_data['Age'].mean()))\ntrain_data['Embarked'] = train_data['Embarked'].fillna(train_data['Embarked'].mode()[0])\ntest_data['Fare'] = test_data['Fare'].fillna(test_data['Fare'].median())\n\ntrain_data.drop('Ticket',axis= 1,inplace= True)\ntest_data.drop('Ticket',axis= 1,inplace= True)\ntrain_data['Family_size']=train_data.SibSp + train_data.Parch + 1\ntest_data['Family_size']=test_data.SibSp + test_data.Parch + 1\ntrain_data['IsAlone'] = 1\ntrain_data[['IsAlone']][train_data.Family_size >1] = 0\ntest_data['IsAlone'] = 1\ntest_data[['IsAlone']][test_data.Family_size >1] = 0\n#-----------------------------------------------\ntrain_data['isTrain'] = 1\ntest_data['isTrain'] = 0\ntt = pd.concat([train_data,test_data])\ntt['Title']= tt.Name.apply(lambda x: x.split(',')[1].split('.')[0])\nstat_min = 10\ntitle_names = (tt['Title'].value_counts() >= stat_min)\nt=title_names.reset_index()# most common titles\nmost_freq_titles = list(t[t.Title == True]['index'])\ntt['Title']= tt.Title.apply(lambda x: x if x in most_freq_titles else 'other')\ntt= pd.get_dummies(data= tt,columns=['Title'],drop_first= True)\ntt['Fare_bins']=pd.qcut(tt.Fare, 4)\ntt['Age_bins']= pd.cut(tt.Age.astype(int), 5)\n\nlabel = LabelEncoder()\nfor rows in ['Age_bins','Fare_bins']:\n    tt[rows]=label.fit_transform(tt[rows])\n\ntt.drop(['Name','Fare','Age'],axis=1,inplace=True) # we have created 'Title' , 'Fare_bins', 'Age_bins' \ntt = pd.get_dummies(tt,columns= ['Sex','Embarked'],drop_first = True)\ntrain_data = tt[tt.isTrain == 1]\ntest_data = tt[tt.isTrain == 0]\ntest_data.drop(['Survived','isTrain','fold'],axis=1,inplace = True)\ntrain_data.drop(['isTrain'],axis=1,inplace= True)\n\n# Note don't drop PassengerId column of train_data and test_data as it is used later. Instead drop it only for test and my_folds\nuseful_features = test_data.drop('PassengerId',axis=1).columns.tolist() #########################################\ntest = test_data[useful_features]\nmy_folds = train_data.copy()","32754311":"test.shape, my_folds.shape, useful_features","47d99cbf":"def obj(trial,xtrain,ytrain,xvalid,yvalid):\n    \n    params = {\n        'iterations':trial.suggest_int(\"iterations\", 300, 1200),\n        'objective': trial.suggest_categorical('objective', ['Logloss', 'CrossEntropy']),\n        'bootstrap_type': trial.suggest_categorical('bootstrap_type', ['Bayesian', 'Bernoulli', 'MVS']),\n        'od_wait':trial.suggest_int('od_wait', 500, 2000),\n        'learning_rate' : trial.suggest_uniform('learning_rate',0.02,1),\n        'reg_lambda': trial.suggest_uniform('reg_lambda',1e-5,100),\n        'random_strength': trial.suggest_uniform('random_strength',10,50),\n        'depth': trial.suggest_int('depth',1,15),\n        'min_data_in_leaf': trial.suggest_int('min_data_in_leaf',1,30),\n        'leaf_estimation_iterations': trial.suggest_int('leaf_estimation_iterations',1,15),\n        'verbose': False,\n        'task_type' : 'GPU',\n        'devices' : '0'\n    } \n    if params['bootstrap_type'] == 'Bayesian':\n        params['bagging_temperature'] = trial.suggest_float('bagging_temperature', 0, 10)\n    elif params['bootstrap_type'] == 'Bernoulli':\n        params['subsample'] = trial.suggest_float('subsample', 0.1, 1)\n        \n        \n    model = CatBoostClassifier(**params, random_state=141)\n\n    model.fit(xtrain, ytrain)\n\n    valid_preds = model.predict_proba(xvalid)[:,1]\n    #test_preds = model.predict(xtest)\n    score = roc_auc_score(yvalid, valid_preds)  # since it is a classification problem so we will use roc auc score\n\n\n    return score\n\n# create trial function\ndef run(my_folds1):   \n \n    my_folds1 = my_folds.copy()\n    #test1  = test.copy()\n\n    fold=0\n    xtrain = my_folds[my_folds1.fold != fold].reset_index(drop=True)\n    xvalid = my_folds[my_folds1.fold == fold].reset_index(drop=True)\n    #xtest = test1.copy()\n\n    ytrain = xtrain.Survived\n    yvalid = xvalid.Survived\n\n    xtrain = xtrain[useful_features]\n    xvalid = xvalid[useful_features]\n\n    ## preprocess\n    si = SimpleImputer(strategy='median')\n    xtrain = si.fit_transform(xtrain)\n    xvalid = si.transform(xvalid)\n    #xtest = si.transform(xtest)\n\n    # scale\n    ss = RobustScaler()\n    xtrain = ss.fit_transform(xtrain)\n    xvalid = ss.transform(xvalid)\n    #xtest = ss.transform(xtest)\n\n    xtrain = pd.DataFrame(xtrain, columns=useful_features)\n    xvalid = pd.DataFrame(xvalid, columns=useful_features)\n    #xtest = pd.DataFrame(xtest, columns=useful_features)\n    \n#     for col in useful_features:\n#         xtrain[col] = np.log1p(xtrain[col])\n#         xvalid[col] = np.log1p(xvalid[col])\n#         #xtest[col] = np.log1p(xtest[col])\n        \n    #create optuna study\n    study = optuna.create_study(\n        direction='maximize',\n        study_name='CATboost_c'\n    )\n\n    study.optimize( lambda trial: obj(trial,xtrain,ytrain,xvalid,yvalid),n_trials= 50 ) # it tries 50 different values to find optimal hyperparameter\n\n    print(f\"Best Params: {study.best_trial.params}\")\n    print(f\"Best Trial: {study.best_trial.value}\")\n    \n    return study.best_trial.params, study.best_trial.value","948774a0":"bp,bv=run(my_folds)","b21716fb":"bp ","cad8298f":"bv","40ec147e":"<a id=\"3\"><\/a>\n# <p style=\"background-color:#B721FF;font-family:newtimeroman;color:#444160;font-size:150%;text-align:center;border-radius:20px 60px;\">PREPROCESSING<\/p>\n\n<p style=\"font-family:newtimeroman;font-size:120%;color:#444160;\">In PREPROCESSING notebook we have already explained all the steps [Make sure you have checked it]. Here we will quickly use same code. <\/p> ","1f13dcb0":"<a id=\"5\"><\/a>\n<p style=\"background-color:#B721FF;font-family:newtimeroman;color:#444160;font-size:150%;text-align:center;border-radius:20px 60px;\">CONCLUSION<\/p>\n<p style=\"font-family:newtimeroman;font-size:120%;color:#444160;\">Now we will use this optimized model to make predictions. In the next notebook we will find optimal hyperparameter for another model we will keep doing this for other models keeping the same PREPROCESSING\/feature enginnering part. Then we will move to ROUND2 where we will do some other feature engineering and probably use some other scaler and repeat the same process. \n<br> That is all for now, If you have any doubt feel free to ask me in the comment. If you like my effort please do <b>UPVOTE<\/b>, it really keeps me motivated. <\/p>\n\n**<span style=\"color:#444160;\"> Thanks!<\/span>**\n<a id=\"6\"><\/a>\n<p style=\"background-color:#B721FF;font-family:newtimeroman;color:#444160;font-size:150%;text-align:center;border-radius:20px 60px;\">END<\/p>\n    <a href=\"#top\" role=\"button\" aria-pressed=\"true\" >\u2b06\ufe0fBack to Table of Contents \u2b06\ufe0f<\/a>","6e6ba8b2":"<a id=\"2\"><\/a>\n# <p style=\"background-color:#B721FF;font-family:newtimeroman;color:#444160;font-size:150%;text-align:center;border-radius:20px 60px;\">READ DATASETS<\/p>\n<p style=\"font-family:newtimeroman;font-size:120%;color:#444160;\">\nEither you can add the dataset whoose link I have given above or if you have TITANIC_Create_Folds notebook you can add it's output from Add data option. Both contains TITANIC_Folds.csv (modified train set).\nNow read it as train_data.\n<\/p>","a64ebe0e":"<p style=\"font-family:newtimeroman;font-size:120%;color:#444160;\"><i>bp<\/i> returns best parameters <\/p> ","9fdf7b76":"<a id=\"4\"><\/a>\n# <p style=\"background-color:#B721FF;font-family:newtimeroman;color:#444160;font-size:150%;text-align:center;border-radius:20px 60px;\">HYPERPARAMETER OPTIMIZATION<\/p>\n<p style=\"font-family:newtimeroman;font-size:120%;color:#444160;\">This step is very crucial, here we are using OPTUNA to find optimal hyperparameter.\n    <br>We have created two functions <b>obj()<\/b> and <b>run()<\/b>\n    <br><b>run()<\/b> takes <b>my_folds<\/b> as input which we have created then it creates a OPTUNA study and feeds this data. \n    <br> This <b>obj()<\/b> method is created by OPTUNA which takes xtrain, ytrain, xvalid, yvalid as input and trains Catboost Classifier model then make predictions on xvalid and then return roc_auc_score. To know more about OPTUNA you can look at working of OPTUNA from their official doccument.\n    <br>[If you do not understand anything you can ask me in the comment]<\/p> ","ce5f9c73":"\n<a id=\"0\"><\/a>\n# <p style=\"background-color:#FFCC70;font-family:newtimeroman;color:#444160;font-size:150%;text-align:center;border-radius:5px 5px;\">LEVEL1 ROUND2 CATBOOST<br><p style=\"background-color:#B721FF;font-family:newtimeroman;color:#444160;font-size:150%;text-align:center;border-radius:20px 60px;\">INTRODUCTION<\/p><\/p>\n<p style=\"font-family:newtimeroman;font-size:120%;color:#444160;\">This is a part of the notebook series <i>\"My_Complete_Pipeline_for_any_ML_Competition\"<\/i> where we are building complete pipeline.<br><br> \n\ud83d\udcccLink of first notebook of the series <a href=\"https:\/\/www.kaggle.com\/raj401\/my-complete-pipeline-for-any-ml-competition\">https:\/\/www.kaggle.com\/raj401\/my-complete-pipeline-for-any-ml-competition<\/a><br>\n\ud83d\udcccLink of notebook where we have created folds <a href=\"https:\/\/www.kaggle.com\/raj401\/titanic-create-folds\">https:\/\/www.kaggle.com\/raj401\/titanic-create-folds<\/a><br>\n<p style=\"font-family:newtimeroman;font-size:120%;color:#444160;\">\n    If you like my effort please do <b><span style=\"color:crimson; font-size:20px\">UPVOTE\ud83d\udc4d<\/span><\/b>, it really keeps me motivated. <\/p>\n\n<p style=\"background-color:#B721FF;font-family:newtimeroman;color:#444160;font-size:50%;text-align:center;border-radius:20px 60px;\">\"\"<\/p> \n<p style=\"font-family:newtimeroman;font-size:120%;color:#444160;\">In the previous notebook we have modified our training set by adding new column named 'fold' and then saved it as <i>TITANIC_folds.csv<\/i>. In this notebook we will use this modified training set instead of original training set and do hyperparameter tuning of Catboost Classifer using OPTUNA. I am providing the link of <i>TITANIC_folds.csv<\/i> you can just add it to your notebook and you are good to go.<b>\n<br>[Make sure you have added it before moving further. If you have TITANIC_Create_Folds notebook you can also add that notebook instead.]<br><\/b>\n\ud83d\udcccLink of Dataset containing <i>TITANIC_folds.csv<\/i> <a href=\"https:\/\/www.kaggle.com\/raj401\/titanic-all-datasets\">https:\/\/www.kaggle.com\/raj401\/titanic-all-datasets<\/a><br><\/p> \n\n\n<a id='top'><\/a>\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<p style=\"background-color:#B721FF;font-family:newtimeroman;color:#444160;font-size:150%;text-align:center;border-radius:20px 60px;\">TABLE OF CONTENTS<\/p>   \n    \n* [1. IMPORTING LIBRARIES](#1)\n    \n* [2. READ DATASETS](#2)\n\n* [3. PREPROCESSING](#3)\n    \n    \n* [4. HYPERPARAMETER OPTIMIZATION](#4)\n    \n* [5. CONCLUSION](#5)\n    \n* [6. END](#6)\n\n<a id=\"1\"><\/a>\n# <p style=\"background-color:#B721FF;font-family:newtimeroman;color:#444160;font-size:150%;text-align:center;border-radius:20px 60px;\">IMPORTING LIBRARIES<\/p>","d0d0063c":"<p style=\"font-family:newtimeroman;font-size:120%;color:#444160;\"><i>bv<\/i> returns best roc_auc_score <\/p> "}}