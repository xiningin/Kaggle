{"cell_type":{"be1389f8":"code","de44f7ba":"code","813786f9":"code","943f5bc2":"code","23044682":"code","389ea0a9":"code","6a8fbb23":"markdown","7a696f7d":"markdown","a9f88e02":"markdown","716c76e4":"markdown","562ad2d4":"markdown","9e8943bc":"markdown","d4d64f56":"markdown","772dc4f3":"markdown","acf1cd46":"markdown"},"source":{"be1389f8":"# import necessary tools\nfrom numpy import mean\nfrom numpy import std\nfrom numpy.random import randn\nfrom numpy.random import seed\nfrom numpy import cov\nfrom scipy.stats import pearsonr\nfrom scipy.stats import spearmanr\nimport matplotlib\nfrom matplotlib import pyplot\n# seed random number generator\nseed(1)","de44f7ba":"# generate related variables\n\n# prepare data\n# adding 100 makes the mean 100 and multiplying by 20 makes the std 20.\ndata1 = 20 * randn(1000) + 100 \n# add \"noise\" to the first data. The noise has a mean of 50 and std of 10.\ndata2 = data1 + (10 * randn(1000) + 50)\n# summarize\nprint('data1: mean=%.3f stdv=%.3f' % (mean(data1), std(data1)))\nprint('data2: mean=%.3f stdv=%.3f' % (mean(data2), std(data2)))","813786f9":"# plot\nmatplotlib.rcParams['figure.figsize'] = [12.0, 8.0]\npyplot.scatter(data1, data2)\npyplot.show()","943f5bc2":"# calculate covariance matrix\ncovariance = cov(data1, data2)\nprint(covariance)","23044682":"# calculate the Pearson's correlation between two variables\ncorr, _ = pearsonr(data1, data2)\nprint('Pearsons correlation: %.3f' % corr)","389ea0a9":"# calculate the spearmans's correlation between two variables\ncorr, _ = spearmanr(data1, data2)\nprint('Spearmans correlation: %.3f' % corr)","6a8fbb23":"The following shows a scatter plot of the two variables. Because we contrived the dataset, we know there is a relationship between the two variables. This is clear when we review the generated scatter plot where we can see an increasing trend.","7a696f7d":"For a sample covariance, the formula is slightly adjusted:\n![image.png](attachment:image.png)\nWhere:\n\n- Xi \u2013 the values of the X-variable\n- Yj \u2013 the values of the Y-variable\n- X\u0304 \u2013 the mean (average) of the X-variable\n- \u0232 \u2013 the mean (average) of the Y-variable\n- n \u2013 the number of the data points","a9f88e02":"### Test Dataset\nBefore we look at correlation methods, let\u2019s define a dataset we can use to test the methods.\n\nWe will generate 1,000 samples of two variables with a strong positive correlation. The first variable will be random numbers drawn from a Gaussian distribution with a mean of 100 and a standard deviation of 20. The second variable will be values from the first variable with Gaussian noise added with a mean of a 50 and a standard deviation of 10.\n\nWe will use the randn() function to generate random Gaussian values with a mean of 0 and a standard deviation of 1, then multiply the results by our own standard deviation and add the mean to shift the values into the preferred range.\n\nThe pseudorandom number generator is seeded (at the top of this notebook) to ensure that we get the same sample of numbers each time the code is run.","716c76e4":"Running the example calculates and prints the Spearman\u2019s correlation coefficient.\n\nWe know that the data is Gaussian and that the relationship between the variables is linear. Nevertheless, the nonparametric rank-based approach shows a strong correlation between the variables of 0.8.\n```\nSpearmans correlation: 0.872\n```\nAs with the Pearson\u2019s correlation coefficient, the coefficient can be calculated pair-wise for each variable in a dataset to give a correlation matrix for review.","562ad2d4":"# Covariance vs. Correlation\nCovariance and correlation both primarily assess the relationship between variables. The closest analogy to the relationship between them is the relationship between the variance and standard deviation.\n\nCovariance measures the total variation of two random variables from their expected values. Using covariance, we can only gauge the direction of the relationship (whether the variables tend to move in tandem or show an inverse relationship). However, it does not indicate the strength of the relationship, nor the dependency between the variables.\n\nOn the other hand, *correlation* measures the strength of the relationship between variables. Correlation is the scaled measure of covariance. It is dimensionless. In other words, the correlation coefficient is always a pure value and not measured in any units.\n\nThe relationship between the two concepts can be expressed using the formula below:\n![image.png](attachment:image.png)\nWhere:\n\n- \u03c1(X,Y) \u2013 the correlation between the variables X and Y\n- Cov(X,Y) \u2013 the covariance between the variables X and Y\n- \u03c3X \u2013 the standard deviation of the X-variable\n- \u03c3Y \u2013 the standard deviation of the Y-variable","9e8943bc":"Formula for Covariance\nThe covariance formula is similar to the formula for correlation and deals with the calculation of data points from the average value in a dataset.\n\nFor example, the covariance between two random variables X and Y can be calculated using the following formula (for population):\n![image.png](attachment:image.png)","d4d64f56":"Running the example calculates and prints the Pearson\u2019s correlation coefficient.\n\nWe can see that the two variables are positively correlated and that the correlation is 0.8. This suggests a high level of correlation, e.g. a value above 0.5 and close to 1.0.\n```\nPearsons correlation: 0.888\n```\nThe Pearson\u2019s correlation coefficient can be used to evaluate the relationship between more than two variables.\n\nThis can be done by calculating a matrix of the relationships between each pair of variables in the dataset. The result is a symmetric matrix called a correlation matrix with a value of 1.0 along the diagonal as each column always perfectly correlates with itself.\n\n### Spearman\u2019s Correlation\nTwo variables may be related by a nonlinear relationship, such that the relationship is stronger or weaker across the distribution of the variables.\n\nFurther, the two variables being considered may have a non-Gaussian distribution.\n\nIn this case, the Spearman\u2019s correlation coefficient (named for Charles Spearman) can be used to summarize the strength between the two data samples. This test of relationship can also be used if there is a linear relationship between the variables, but will have slightly less power (e.g. may result in lower coefficient scores).\n\nAs with the Pearson correlation coefficient, the scores are between -1 and 1 for perfectly negatively correlated variables and perfectly positively correlated respectively.\n\nInstead of calculating the coefficient using covariance and standard deviations on the samples themselves, these statistics are calculated from the relative rank of values on each sample. This is a common approach used in non-parametric statistics, e.g. statistical methods where we do not assume a distribution of the data such as Gaussian.\n```\nSpearman's correlation coefficient = covariance(rank(X), rank(Y)) \/ (stdv(rank(X)) * stdv(rank(Y)))\n```\nA linear relationship between the variables is not assumed, although a monotonic relationship is assumed. This is a mathematical name for an increasing or decreasing relationship between the two variables.\n\nThis site has an excellent explanation of a Spearman's correlation and of a monotonic relationship: https:\/\/statistics.laerd.com\/statistical-guides\/spearmans-rank-order-correlation-statistical-guide.php\n\nIf you are unsure of the distribution and possible relationships between two variables, Spearman correlation coefficient is a good tool to use.\n\nThe spearmanr() SciPy function can be used to calculate the Spearman\u2019s correlation coefficient between two data samples with the same length.\n\nWe can calculate the correlation between the two variables in our test problem.\n\nThe complete example is listed below.","772dc4f3":"The covariance and covariance matrix are used widely within statistics and multivariate analysis to characterize the relationships between two or more variables.\n\nRunning the example calculates and prints the covariance matrix.\n\nBecause the dataset was contrived with each variable drawn from a Gaussian distribution and the variables linearly correlated, covariance is a reasonable method for describing the relationship.\n\nThe covariance between the two variables is 389.75. We can see that it is positive, suggesting the variables change in the same direction as we expect.\n\nA problem with covariance as a statistical tool alone is that it is challenging to interpret. This leads us to the Pearson\u2019s correlation coefficient next.\n\n### Pearson\u2019s Correlation\nThe Pearson correlation coefficient (named for Karl Pearson) can be used to summarize the strength of the linear relationship between two data samples.\n\nThe Pearson\u2019s correlation coefficient is calculated as the covariance of the two variables divided by the product of the standard deviation of each data sample. It is the normalization of the covariance between the two variables to give an interpretable score.\n```\nPearson's correlation coefficient = covariance(X, Y) \/ (stdv(X) * stdv(Y))\n```\nThe use of mean and standard deviation in the calculation suggests the need for the two data samples to have a Gaussian or Gaussian-like distribution.\n\nThe result of the calculation, the correlation coefficient can be interpreted to understand the relationship.\n\nThe coefficient returns a value between -1 and 1 that represents the limits of correlation from a full negative correlation to a full positive correlation. A value of 0 means no correlation. The value must be interpreted, where often a value below -0.5 or above 0.5 indicates a notable correlation, and values below those values suggests a less notable correlation.\n\nThe pearsonr() SciPy function can be used to calculate the Pearson\u2019s correlation coefficient between two data samples with the same length.\n\nWe can calculate the correlation between the two variables in our test problem.\n\nThe complete example is listed below.","acf1cd46":"Before we look at calculating some correlation scores, we must first look at an important statistical building block, called covariance.\n\n### Covariance\nVariables can be related by a linear relationship. This is a relationship that is consistently additive across the two data samples.\n\nThis relationship can be summarized between two variables, called the covariance. It is calculated as the average of the product between the values from each sample, where the values haven been centered (had their mean subtracted).\n\nThe calculation of the sample covariance shown previously, but in pseudocode:\n```\ncov(X, Y) = sum( (x - mean(X)) * (y - mean(Y)) ) \/ (n-1)\n```\n\nThe use of the mean in the calculation suggests the need for each data sample to have a Gaussian or Gaussian-like distribution.\n\nThe sign of the covariance can be interpreted as whether the two variables change in the same direction (positive) or change in different directions (negative). The magnitude of the covariance is not easily interpreted. A covariance value of zero indicates that both variables are completely independent.\n\nThe cov() NumPy function can be used to calculate a covariance matrix between two or more variables.\n\n```\ncovariance = cov(data1, data2)\n```\nThe diagonal of the matrix contains the covariance between each variable and itself. The other values in the matrix represent the covariance between the two variables; in this case, the remaining two values are the same given that we are calculating the covariance for only two variables.\n\nWe can calculate the covariance matrix for the two variables in our test problem."}}