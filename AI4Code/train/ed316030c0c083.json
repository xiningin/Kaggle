{"cell_type":{"9ec082d7":"code","c5c980f5":"code","16442a5e":"code","f4f5131b":"code","fa309fc1":"code","65166f99":"code","0f176325":"code","39153d6c":"code","fd7dc713":"code","b20c9a8c":"code","36496cac":"code","2a40d4e5":"code","a6ee62fd":"code","0927da91":"code","c8a651bf":"code","64007be8":"code","f6e46c85":"code","61c6fd6f":"code","36f3bccc":"code","625e95f4":"code","6f909f90":"code","8103be02":"code","50886beb":"code","a47d2c5c":"code","ed28332b":"code","50d2dfd6":"code","715bb1bb":"code","ed178c81":"code","1c08d81b":"code","2360423d":"code","3aeb50da":"code","89e5cbe9":"code","c4ec3f36":"code","88de9454":"code","0db36d8a":"code","039d906c":"markdown","5ca11f7d":"markdown","790cebf5":"markdown","ab3a0262":"markdown","0720a611":"markdown","e510ff04":"markdown","f2889a61":"markdown","5287233b":"markdown","7e1fa8dc":"markdown","47204b21":"markdown","57044f9a":"markdown","3af4363f":"markdown","ffc1a860":"markdown","be6f9c34":"markdown","4f12cba2":"markdown","6827d4c9":"markdown","24a6e914":"markdown","d4dd4ce7":"markdown","f497459c":"markdown","676618b6":"markdown","0abb8259":"markdown","cb14c5cf":"markdown","afa95586":"markdown","beb9d37a":"markdown","b4ebe279":"markdown","7b6a3fe3":"markdown","99fca288":"markdown","ee6c9ef4":"markdown"},"source":{"9ec082d7":"import numpy as np\nimport pandas as pd\nimport os\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn import datasets, metrics\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom tabulate import tabulate","c5c980f5":"#Import file creditcard.csv\ncreditdf = pd.read_csv(\"..\/input\/creditcard\/creditcard.csv\")","16442a5e":"#Determine number of rows & columns\ncreditdf.shape","f4f5131b":"#Explore general information on data\ncreditdf.info()","fa309fc1":"#Exploring the first data points in detail\ncreditdf.head()","65166f99":"#Number of fraudulent and legitimate entries\nfraudNumber = creditdf.Time[creditdf.Class == 1].count()\nlegitNumber = creditdf.Time[creditdf.Class == 0].count()\nprint(f\"Number of fraudulent cases: {fraudNumber}\")\nprint(f\"Number of legit cases: {legitNumber}\")","0f176325":"#Describe the 'Amount' feature with basic statistics\ncreditdf.Amount.describe()","39153d6c":"#Exploring correlation (Pearson) of features\nplt.figure(figsize=(15, 15))\nplt.title(\"Creditcard Fraud Detection - Correlation of Features (Pearson)\")\ncorr = creditdf.corr()\nsns.heatmap(\n    corr,\n    xticklabels=corr.columns,\n    yticklabels=corr.columns,\n    linewidths=1,\n    cmap=\"Greens\",\n)\nplt.show()","fd7dc713":"#Calculation of total amount of fraudulent transactions\nfraudAmount = creditdf.Amount[creditdf.Class == 1].sum()\nprint(f\"The total amount of fraudulent transactions is: {fraudAmount}\")","b20c9a8c":"f, (ax1, ax2) = plt.subplots(2, 1, sharex=True, figsize=(15,8))\n\nax1.hist(creditdf.Amount[creditdf.Class == 1], bins = 35)\nax1.set_title('Fraudulent Transactions')\n\nax2.hist(creditdf.Amount[creditdf.Class == 0], bins = 35)\nax2.set_title('Legitimate Transactions')\n\nplt.xlabel('Transaction Amount in $')\nplt.ylabel('Number of Transactions')\nplt.yscale('log')\nplt.show()","36496cac":"#Scaling 'Amount' feature to have zero mean and standard deviation of 1\nfrom scipy import stats\ncreditdf['ScaledAmount'] = stats.zscore(creditdf[\"Amount\"])","2a40d4e5":"creditdf.ScaledAmount.describe()","a6ee62fd":"#output\ny= creditdf.Class\n \n#input\nx=creditdf.drop(['Time','Class'],axis=1) #According to the project description the prediction should be made without the feature 'Time'\n\n#splitting of data\nx_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2, random_state=420) # A random_state number is used for reproducable results\nx_testOrigAmount = x_test # Saving the original 'Amount' feature for subsequent calculations\nx_train = x_train.drop(['Amount'],axis=1)\nx_test = x_test.drop(['Amount'],axis=1)\n\n#printing shapes of testing and training sets :\nprint(\"Shape of original dataset (incl. added column 'ScaledAmount'): \", creditdf.shape)\nprint(\"Shape of input - training set: \", x_train.shape)\nprint(\"Shape of output - training set: \", y_train.shape)\nprint(\"Shape of input - testing set: \", x_test.shape)\nprint(\"Shape of output - testing set: \", y_test.shape)","0927da91":"#Fitting the model on the training data\nlogreg = LogisticRegression()\nlogregModel = logreg.fit(x_train,y_train)\ny_predLog = logreg.predict(x_test)","c8a651bf":"print(metrics.classification_report(y_test,y_predLog))","64007be8":"accuracyLog = metrics.accuracy_score(y_test, y_predLog)\nprecisionLog = metrics.precision_score(y_test, y_predLog)\nrecallLog = metrics.recall_score(y_test, y_predLog)\n\nprint(\"Accuracy:\",accuracyLog)\nprint(\"Precision:\",precisionLog)\nprint(\"Recall:\",recallLog)","f6e46c85":"def cf_visualization(pred, color):\n    cm=metrics.confusion_matrix(y_test,pred)\n    cf_matrix=pd.DataFrame(data=cm,columns=['Predicted: 0 (Legit)','Predicted: 1 (Fraud)'],index=['Actual: 0 (Legit)','Actual: 1 (Fraud)'])\n    plt.figure(figsize = (8,5))\n    sns.heatmap(cf_matrix, annot=True,fmt='d',cmap=color);","61c6fd6f":"cf_visualization(pred = y_predLog, color = \"Greens\")","36f3bccc":"#Number of transactions falsely reported as fraudulent (false positives) or legitimate (false negatives)\ndef fpcount(pred):\n    return x_test.ScaledAmount[(pred==1) & (y_test==0)].count(); #function to count false positives\ndef fncount(pred):\n    return x_test.ScaledAmount[(pred==0) & (y_test==1)].count(); #function to count false negatives\n\nprint(\"The number of false positives is: \", fpcount(y_predLog))\nprint(\"The number of false negatives is: \", fncount(y_predLog))","625e95f4":"#Calculating total amount of money in missed fraudulent transactions (false negatives) of test data\ndef fnmoney(pred):\n    return x_testOrigAmount.Amount[(pred==0) & (y_test==1)].sum()\nprint(\"The total amount of money in missed fraudulent transactions (false negatives) amounts to: \", fnmoney(pred = y_predLog))","6f909f90":"#Fitting the model on the training data\ntrees = DecisionTreeClassifier(random_state=420)\ntreesModel = trees.fit(x_train,y_train)\ny_predTrees = trees.predict(x_test)","8103be02":"print(metrics.classification_report(y_test,y_predTrees))","50886beb":"accuracyTrees = metrics.accuracy_score(y_test, y_predTrees)\nprecisionTrees = metrics.precision_score(y_test, y_predTrees)\nrecallTrees = metrics.recall_score(y_test, y_predTrees)\n\nprint(\"Accuracy:\",accuracyTrees)\nprint(\"Precision:\",precisionTrees)\nprint(\"Recall:\",recallTrees)","a47d2c5c":"cf_visualization(pred = y_predTrees, color = \"Blues\")","ed28332b":"#Number of transactions falsely reported as fraudulent (false positives) or legitimate (false negatives)\nprint(\"The number of false positives is: \", fpcount(y_predTrees))\nprint(\"The number of false negatives is: \", fncount(y_predTrees))","50d2dfd6":"#Calculating total amount of money in missed fraudulent transactions (false negatives) of test data\nprint(\"The total amount of money in missed fraudulent transactions (false negatives) amounts to: \", fnmoney(pred = y_predTrees))","715bb1bb":"#Fitting the model on the training data\nrf = RandomForestClassifier(random_state=420)\nrfModel = rf.fit(x_train,y_train)\ny_predRf = rf.predict(x_test)","ed178c81":"print(metrics.classification_report(y_test,y_predRf))","1c08d81b":"accuracyRf = metrics.accuracy_score(y_test, y_predRf)\nprecisionRf = metrics.precision_score(y_test, y_predRf)\nrecallRf = metrics.recall_score(y_test, y_predRf)\n\nprint(\"Accuracy:\",accuracyRf)\nprint(\"Precision:\",precisionRf)\nprint(\"Recall:\",recallRf)","2360423d":"cf_visualization(pred = y_predRf, color = \"Oranges\")","3aeb50da":"#Number of transactions falsely reported as fraudulent (false positives) or legitimate (false negatives)\nprint(\"The number of false positives is: \", fpcount(y_predRf))\nprint(\"The number of false negatives is: \", fncount(y_predRf))","89e5cbe9":"#Calculating total amount of money in missed fraudulent transactions (false negatives) of test data\nprint(\"The total amount of money in missed fraudulent transactions (false negatives) amounts to: \", fnmoney(pred = y_predRf))","c4ec3f36":"evaluationTable = [(\"Logistic Regression\",accuracyLog,precisionLog,recallLog),(\"Decision Trees\",accuracyTrees,precisionTrees,recallTrees),(\"Random Forest\",accuracyRf,precisionRf,recallRf)]\nheaders = [\"Algorithm\",\"Accuracy\",\"Precision\",\"Recall\"]\nprint(tabulate(evaluationTable, headers=headers))","88de9454":"cf_visualization(pred = y_predLog, color = \"Greens\")\ncf_visualization(pred = y_predTrees, color = \"Blues\")\ncf_visualization(pred = y_predRf, color = \"Oranges\")","0db36d8a":"lostMoneyTable = [(\"Logistic Regression\",fnmoney(y_predLog)),(\"Decision Trees\",fnmoney(y_predTrees)),(\"Random Forest\",fnmoney(y_predRf),)]\nheaders = [\"Damage due to missed fraud ($)\"]\nprint(tabulate(lostMoneyTable, headers=headers))\n\nprint(\"The total amount of money in all transactions amounts to: \", x_testOrigAmount.Amount.sum())","039d906c":"# 1. Introduction\n### About the project\nThe aim of this project is to analyze (anonymized) credit card transactions and predict which of these transactions are fraudulent. The data set has already been cleaned, thus there are no records with missing features.\n\n### Goal of the project\nThe goal of this project is to compare different machine learning models in terms of performance. In an executive summary the results will be shown at a glance.","5ca11f7d":"### Evaluation of Model","790cebf5":"### General Description of Random Forest\n#### Advantages\n* Random Forest is suitable for highly correlated data\n* Since it is an ensemble of decision trees errors can be reduced in comparison\n* Can handle vast amounts of data and performs well on imbalanced data\n* The risk of overfitting is generally rather low\n\n#### Disadvantages\n* Random Forest requires features with a certain amount of predictive power to work\n* It is rather difficult to understand and explain the model to non-professionals\n\n#### Typical Use Cases\n* Random Forest is often used in recommendation systems for example in ecommerce but it can also be used for classification problems like fraud, disease identification etc.\n\n[Source](https:\/\/towardsdatascience.com\/pros-and-cons-of-various-classification-ml-algorithms-3b5bfb3c87d6)","ab3a0262":"### General Description of Decision Trees\n#### Advantages\n* The concept of decision trees is easy to understand even for non-professionals\n* Decision trees are good in handling missing values\n* Irrelevant features in the data do not affect the performance of decision trees\n\n#### Disadvantages\n* There is a certain risk of overfitting\n* Decision trees can be very sensitive to changes in data with strongly varying outcomes\n\n#### Typical Use Cases\n* Decision Trees can be used for optimization problems like maximizing profit or minimizing cost but also for example in marketing to identify potential buyers or factors in attracting and retaining customers.\n\n[Source](https:\/\/towardsdatascience.com\/pros-and-cons-of-various-classification-ml-algorithms-3b5bfb3c87d6)","0720a611":"Our goal was to develop a fraud detection system for creditcard transactions. For this purpose we have trained and evaluated three different machine learning algorithms in order to determine the most favorable model for our case. All three models have a high accuracy of more than 99%. This means out of all transactions more than 99% are classified correctly. However, the value of accuracy can be misleading as this metric alone is not sufficient to evaluate whether a model is well suited for our purpose.\nThis can easily be illustrated with an example. Imagine we would have 10,000 transactions out of which 100 were fraudulent. Our model detects 9,900 legitimate transactions correctly. But at the same time it identifies only 1 out of 100 fraudulent transactions as fraud and falsely classifies 99 as legitimate. We would probably all agree that this model is not very good since 99 fraudulent transactions would have been missed. However, in terms of accuracy we would still see a score of 99%. To resolve this issue we need to look at precision and recall. These metrics tell us more about the practical performance of the three models.\n\nLet's have a look at the above-mentioned metrics for each model.","e510ff04":"## 4.3 Decision Trees","f2889a61":"Import required libraries:","5287233b":"## 4.4 Random Forest","7e1fa8dc":"The confusion matrix above can be used to visualize precision and recall of our model. It shows true negatives (top left), false positives (top right), false negatives (bottom left) and true positives (bottom right).\nOur logistic regression model has a very high accuracy in terms of classification. However, more important in our case is precision and recall of the model. Our precision value shows the ratio of transactions that are predicted as fraud which are actually fraudulent. The most important metric in our case is recall because it tells us how many cases of false negatives there are. Fraud detection is one of the cases where false negatives are particularly critical. The recall value tells us how many cases of fraud are actually detected and how many are missed and thus shown as legitimate. A high number of false negatives could cause high damages for the bank and its customers.","47204b21":"# 4. Data Analysis & Machine Learning Algorithms","57044f9a":"### Evaluation of Model","3af4363f":"# 3. Data Description, Exploration & Manipulation","ffc1a860":"The dataset contains 31 rows and 284,807 columns.","be6f9c34":"In total there are 492 fraudulent cases as stated above. If we assumed the average transaction amount of 88.35 for all of these transactions this would amount to 43,468.20. This could indicate that the fraudulent cases are not necessarily distinguishable by the transaction amount alone. To support this claim we can have a closer look at the 'Amount' feature.","4f12cba2":"It appears that the vast majority of fraudulent transactions have very small transaction amounts and there seem to be no cases of fraud with an amount above 2,500$.","6827d4c9":"# 2. Data Preparation\n","24a6e914":"The average amount (mean) of the creditcard transaction is approximately 88.35. The maximum transaction amount appears to be very high in comparison. This could be a first indication of fraud. This could be analyzed further by calculating the total amount involved in fraudulent transactions.","d4dd4ce7":"# Contents\n1. Introduction\n2. Data Preparation\n3. Data Description, Exploration & Manipulation\n4. Data Analysis\n5. Executive Summary","f497459c":"### General Description of Logistic Regression\n#### Advantages\n* The algorithm is simple and does not need much processing power\n* Features do not need to be scaled\n\n#### Disadvantages\n* The algorithm is not well suited for non-linear data and does perform very poorly there\n* Performance deteriorates if the data contains many irrelevant or strongly correlated features\n* Performance in general is rather limited compared to other algorithms\n\n#### Typical Use Cases\n* Logistic Regression is mostly used for binary classification problems. Our fraud detection problem would be such a case as there are two possible outcomes to predict: fraud or legitimate.\n\n[Source](https:\/\/towardsdatascience.com\/pros-and-cons-of-various-classification-ml-algorithms-3b5bfb3c87d6)","676618b6":"Again the Random Forest algorithm performs best in terms of damages caused due to missed fraudulent transactions. From a total transaction amount of 4,944,262.41\\\\$ in our test set the Random Forest algorithm has missed 14 cases of fraud worth 2,386.88\\\\$. This is a ratio of 0.05%. While this might not yet be acceptable in practice, this is not a bad result considering that our training data set only contained 227,845 entries.\n\nMy recommendation would therefore be to train the Random Forest algorithm further with more data in order to utilize it in the future. Especially in combination with other fraud detection mechanisms, I am convinced that we will be able to obtain a detection of fraudulent transactions near 100%.","0abb8259":"The new feature 'ScaledAmount' now has a mean close to 0 and a standard deviation close to 1.","cb14c5cf":"There appear to be no considerable correlations among the features.","afa95586":"### Evaluation of Model","beb9d37a":"The Random Forest algorithm outperforms both Logistic Regression and Decision Trees in all three metrics we have measured. Let's have a closer look what the numbers of precision and recall above actually mean at the example of the Random Forest Algorithm. Precision gives an indication of the ratio of true positives to false positives. True positives in our case are fraudulent transactions which have been correctly classified as fraud. False positives on the other hand are legitimate transactions which have been falsely classified as fraud. The higher the precision value the lower the number of false positives. The metric of precision is therefore important for customer satisfaction because customers will not be happy if many of their legitimate transactions are blocked because of a suspicion of fraud. A precision value of 0.977 as stated above would mean that out of 1,000 transactions which the algorithm predicts as fraudulent, 23 would actually be legitimate and be falsely classified.\n\nThe metric of recall gives an indication how many false negatives the algorithm predicts. False negatives in our case are fraudulent transactions which the algorithm classifies as legitimate. This is particularly critical as cases of fraud are missed by the algorithm which could potentially cause severe damages for the bank and customers. Our Random Forest algorithm has a recall value of 0.857, i.e. out of 1,000 actual fraud cases 143 would be missed. In our test data the Random Forest Algorithm missed 14 out of 98 fraud cases.\n\nLooking at the confusion matrices for all three algorithms we can compare the number of false negatives by looking at the bottom left quadrant in order to get a better idea of the recall values above.","b4ebe279":"# 5. Executive Summary","7b6a3fe3":"## 4.2 Logistic Regression Model","99fca288":"## 4.1 Splitting the data in training and test data (80\/20)","ee6c9ef4":"As stated above the Random Forest Algorithm outperforms the other algorithms and accordingly has the lowest number of false negatives and false positives. The precision of our algorithm might be sufficient as the little amount of false positives could be double-checked and rectified. In terms of recall however, one could argue that the algorithm does not yet perform well enough and should be trained further. Out of 98 fraud cases 14 were missed by the algorithm. To get an idea of the caused damage we can have a look at the total amount of money that would have been lost in our test case."}}