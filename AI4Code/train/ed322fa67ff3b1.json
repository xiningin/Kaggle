{"cell_type":{"77aded7f":"code","80483b2a":"code","a5ba2dbe":"code","cb9ae5b8":"code","1264a8f4":"code","4109f783":"code","d17c03f1":"code","0d58b1cd":"code","06f7d28a":"code","5cec2e07":"code","6f2df49a":"code","434dfe90":"code","86613dd2":"code","b023a8f0":"code","9e53f34a":"code","f735ac02":"code","5b104399":"code","355fe954":"code","dac46fe5":"code","f0c24c78":"code","579a592b":"code","26ae322a":"code","26826253":"code","463f49e0":"code","5bd56b47":"code","9ebb688d":"code","671a05ff":"code","23be06d1":"code","38f49ec8":"code","6fd009b3":"code","01263d91":"code","ce49553e":"code","3f44578a":"markdown","174817b1":"markdown","d6e5b3ea":"markdown","6adbe7f8":"markdown","467f4246":"markdown","74b3b657":"markdown","7141f73e":"markdown","69d297d6":"markdown","a2555633":"markdown","ea05185c":"markdown"},"source":{"77aded7f":"import seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression, RidgeClassifier\nfrom sklearn.model_selection import train_test_split\nimport warnings\nimport plotly.express as px\nwarnings.filterwarnings(\"ignore\")\nfrom sklearn.metrics import accuracy_score\nimport numpy as np\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom scipy.stats import randint\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import accuracy_score, mean_squared_error, mean_absolute_error","80483b2a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","a5ba2dbe":"#data = pd.read_csv('\/kaggle\/input\/heart-attack-analysis-prediction-dataset\/o2Saturation.csv')\n#data\n","cb9ae5b8":"data = pd.read_csv('\/kaggle\/input\/heart-attack-analysis-prediction-dataset\/heart.csv')\ndata","1264a8f4":"data.var()","4109f783":"data.describe(include='all')","d17c03f1":"data.isnull().sum()","0d58b1cd":"data.info()","06f7d28a":"correlation= data.corr()\ncorrelation","5cec2e07":"fig = plt.figure(figsize = (15,8))\nsns.heatmap(correlation, annot = True)","6f2df49a":"\nfig = plt.figure(figsize = (15,8))\ndata.boxplot()","434dfe90":"df1 = data.select_dtypes([np.int, np.float])\nfor i, col in enumerate(df1.columns):\n    plt.figure(i)\n    sns.distplot(df1[col])","86613dd2":"# # Log transformation\n# data[\"log_age\"]= np.log(data['age'])\n# data[\"log_trtbps\"]= np.log(data['trtbps'])\n# data[\"log_chol\"]= np.log(data['chol'])\n# data[\"log_thalachh\"]= np.log(data['thalachh'])\n","b023a8f0":"# data1 = data.copy()","9e53f34a":"# data = data.drop(['age', 'trtbps', 'chol', 'thalachh'], axis = 1)","f735ac02":"for i in data.columns:\n    print(f'{i}{data[i].unique()}')","5b104399":"X = data.drop(['output'], axis =1)\ny = data['output']\nX","355fe954":"x_scaled = StandardScaler()\nx_scaled.fit(X)\nx_scale = x_scaled.transform(X)\nx_scale\nXdata_scaled = pd.DataFrame(x_scale, columns=X.columns)\nXdata_scaled","dac46fe5":"plt.figure(figsize=(12,8))\nXdata_scaled.boxplot()","f0c24c78":"def test_models(models, X,y, iterations = 100):\n    results = {}\n    for i in models:\n        r2_train = []\n        r2_test = []\n        for j in range(iterations):\n            x_train, x_test, y_train, y_test = train_test_split(X,y,test_size= 0.2)\n            models[i].fit(x_train,y_train).predict(x_train)\n            r2_train.append(models[i].fit(x_train,y_train).score(x_train, y_train))\n            models[i].fit(x_train,y_train).predict(x_test)\n            r2_test.append(models[i].fit(x_train,y_train).score(x_test, y_test))\n            \n            results[i] = [np.mean(r2_train), np.mean(r2_test)]\n           \n    return pd.DataFrame(results)","579a592b":"models = {'LG': LogisticRegression(),'Ridgeclassifier': RidgeClassifier()}\n\ntest_models(models, Xdata_scaled, y)","26ae322a":"x_train, x_test, y_train, y_test = train_test_split(Xdata_scaled,y,test_size= 0.25, random_state=5)","26826253":"from sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.model_selection import GridSearchCV\nmodel = LogisticRegression()\nsolvers = ['newton-cg', 'lbfgs', 'liblinear']\npenalty = ['l2','10', '15']\nc_values = [100, 10, 1.0, 0.1, 0.01]\ngrid = dict(solver=solvers,penalty=penalty,C=c_values)\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\ngrid_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, cv=cv, scoring='accuracy',error_score=0)\ngrid_result = grid_search.fit(x_train, y_train)\nprint(grid_result.best_params_)\nprint(grid_result.score(x_test, y_test))","463f49e0":"model = RidgeClassifier()\nalpha = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n# define grid search\ngrid = dict(alpha=alpha)\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\ngrid_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, cv=cv, scoring='accuracy',error_score=0)\ngrid_result = grid_search.fit(x_train, y_train)\n\nprint(grid_result.best_params_)\nprint(grid_result.score(x_test, y_test))","5bd56b47":"parameters = {\n    \"n_estimators\":[5,50,250,500],\n    \"max_depth\":[1,3,5,7,9],\n    \"learning_rate\":[0.01,0.1,1,10,100]}\n\ngb = GradientBoostingClassifier()\n\ngb_search = GridSearchCV(estimator = gb, param_grid = parameters, \n\n                          cv = 10, n_jobs = -1, return_train_score=True)\n\ngb_search.fit(x_train, y_train)\n\nprint(gb_search.best_params_)\nprint(gb_search.score(x_test, y_test))","9ebb688d":"param_grid = {\"max_depth\": [3, None],\n              \"max_features\": randint(1, 9),\n              \"min_samples_leaf\": randint(1, 9),\n              \"criterion\": [\"gini\", \"entropy\"]}\n\ntree = DecisionTreeClassifier()\n\n# Instantiate the RandomizedSearchCV object: tree_cv\ntree_cv = RandomizedSearchCV(tree, param_grid, cv=5)\n\n# Fit it to the data\ntree_cv.fit(x_train, y_train)\n\nprint(tree_cv.best_params_)\nprint(tree_cv.score(x_test, y_test))","671a05ff":"from sklearn.ensemble import RandomForestClassifier\n\nparam_grid = {  'max_depth': np.arange(10, 100, 10),\n              'max_features': ['auto',' log2'], 'n_estimators': [25,30, 35, 50, 100]}\n\nrfc = RandomForestClassifier()\n\ng_search = GridSearchCV(estimator = rfc, param_grid = param_grid, \n\n                          cv = 5, n_jobs = -1, return_train_score=True)\n\ng_search.fit(x_train, y_train)\n\nprint(g_search.best_params_)\nprint(g_search.score(x_test, y_test))","23be06d1":"model = SVC()\nkernel = ['poly', 'rbf', 'sigmoid']\nC = [50, 10, 1.0, 0.1, 0.01]\ngamma = ['scale']\n# define grid search\ngrid = dict(kernel=kernel,C=C,gamma=gamma)\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\ngrid_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, cv=cv, scoring='accuracy',error_score=0)\ngrid_result = grid_search.fit(x_train, y_train)\nprint(grid_result.best_params_)\nprint(grid_result.score(x_test, y_test))","38f49ec8":"from sklearn.neighbors import KNeighborsClassifier\n#List Hyperparameters that we want to tune.\n#leaf_size = list(range(1,50))\nn_neighbors = list(range(1,30))\n#p=[1,2]\n#Convert to dictionary\nhyperparameters = dict(n_neighbors=n_neighbors)\n#Create new KNN object\nknn_2 = KNeighborsClassifier()\n#Use GridSearch\nclf = GridSearchCV(knn_2, hyperparameters, cv=3)\n#Fit the model\ngrid_result = clf.fit(x_train,y_train)\n\nprint(grid_result.best_params_)\nprint(grid_result.score(x_test, y_test))","6fd009b3":"lgc = LogisticRegression()\nlgc.fit(x_train, y_train)\ny_test_pred = lgc.predict(x_test)\naccuracy_score(y_test,y_test_pred)","01263d91":"gbc = GradientBoostingClassifier(learning_rate = 0.01, max_depth = 1, n_estimators = 500)","ce49553e":"gbc.fit(x_train, y_train)\ny_test_pred = gbc.predict(x_test)\naccuracy_score(y_test, y_test_pred)\n","3f44578a":"## Hyperparameter turning for Ridgeclassifier","174817b1":"## Gradient boosting classifier","d6e5b3ea":"##  Hyperparameter tuning for RandomForestClassification","6adbe7f8":"## Hyperparameter turning for LogisticRegression","467f4246":"##  Hyperparameter tuning for SVM","74b3b657":"##  Hyperparameter tuning for KNeighborsClassifier","7141f73e":"## hyperparameter turning for Decision Tree Classification","69d297d6":"## **During the hyperparameter turning I found a best parameters to get best scores for several models. Out of all models I found best score for LogisticRegression, Ridgeclassifier, GradientBoostingClassifier and Randomforestclassifier have acuuracy score of 0.91.**","a2555633":"## Hyperparameter turning for GradientBoostingClassifier","ea05185c":"## Logistic Regression"}}