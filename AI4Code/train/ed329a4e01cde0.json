{"cell_type":{"6a540a3d":"code","851c951f":"code","68e69e50":"code","82ce57d0":"code","977279c4":"code","75b8baaf":"code","c1daa125":"code","c585ca34":"code","db3f8581":"code","9b1036db":"code","231ba9ed":"code","c76317e1":"code","31b1b560":"code","41c57702":"code","49a3fa37":"code","b1c34b01":"code","b2145cbd":"code","b090bcb7":"code","60fadb40":"code","4c3bea84":"code","6455fff4":"code","beb9bc7e":"code","6cfeffc2":"code","caff8568":"code","e70f50b4":"code","792cc34a":"code","97f81aa8":"code","5ffe329f":"code","81f5e87c":"code","25647211":"code","ae123513":"code","18b1a85c":"code","b383b1d3":"code","15036695":"code","c6c89218":"code","976aaf58":"code","aabb4890":"code","e16160d2":"code","6acec3d7":"code","4016f949":"code","651feabe":"code","3616fc2f":"code","a950aaab":"code","4b094166":"code","38f52580":"code","61bbe30c":"code","7dabeff0":"code","6b045359":"code","9b9e8f93":"code","8d1e0c44":"code","d92e7e03":"code","f3b1f244":"code","7dd902a5":"code","c96d840e":"code","132aa736":"code","4483754e":"code","90e5e3b1":"code","2b35c847":"code","38e9be11":"code","2b4ee24d":"code","5dea474e":"code","1c54285b":"code","f70cfe94":"code","fa3e0b6d":"code","5a2d89a1":"code","07997be9":"code","8f6901e1":"code","f3e12671":"code","7e546f3b":"code","42a666fd":"code","a729e836":"code","e0014c30":"code","7c920098":"code","53aa0584":"code","a38d04bc":"code","9de46a8a":"code","0d5582c2":"code","77f48bc0":"code","5733358c":"code","156a61a4":"code","d3fe758a":"code","cf9b2468":"code","2a1a1738":"code","4d0deead":"code","1134873f":"code","0900b600":"code","53206bcd":"code","0c5d6416":"code","9419bf23":"code","3f2f1a5c":"code","1bdb2386":"code","3cd75d7f":"code","4a4814d8":"code","80750e5e":"code","57096db8":"code","126bb685":"code","9ee118b2":"code","0ad6c0f9":"code","62d250d7":"code","4ab4f94a":"code","d6eb35cf":"code","f0a231ed":"code","eb293e33":"code","d2165838":"code","9a3d8327":"code","524e634d":"code","46825d50":"code","0da7ecc9":"code","10d94f22":"code","bf769480":"code","00168096":"code","31140947":"code","b59ebc36":"code","acedf8fc":"code","e22b2f81":"code","60adff13":"code","65001b9e":"code","b3b918b7":"code","716f5273":"code","0b9260ca":"code","eda663ee":"code","6221418c":"code","5fb08f26":"code","7053f76d":"code","53680198":"code","12bb169b":"code","72c13e8a":"code","fe2b439f":"code","af4711c5":"code","73f8573a":"code","5fdba71e":"code","a8024dc4":"code","dbe0f0b1":"code","7884ec7b":"code","c6f7e8e5":"code","7d09c034":"code","659b22b8":"code","6a5859a7":"code","15283a68":"code","1f2d9c67":"code","1204c294":"code","a076d7fe":"code","cdfc5114":"code","33baa2d4":"code","db995a86":"code","64f18b13":"code","15dd0406":"code","d1aa443a":"markdown","5438e414":"markdown","cbb8c745":"markdown","dd90d419":"markdown","71e21478":"markdown","a46b9472":"markdown","15a9b33c":"markdown","b616ee07":"markdown","cdc2ed4f":"markdown","ba61d34b":"markdown","cb2b4ff2":"markdown","7c1663b2":"markdown","d69695e3":"markdown","6c7fb9d0":"markdown","2f8b4ee9":"markdown","17023f74":"markdown","d9d7f64e":"markdown","26f0d0dc":"markdown","eb576b48":"markdown","4950d1e6":"markdown","ab81d401":"markdown","0f7912e9":"markdown","7978e7bb":"markdown","89355b0b":"markdown","183466ea":"markdown","49f40ffe":"markdown","419a0056":"markdown","801e906d":"markdown","bcc6945f":"markdown","1f13fa4b":"markdown","30e1e227":"markdown","c42cc307":"markdown","40ea8f32":"markdown","0e55a019":"markdown","f898f4a4":"markdown","4492d5b8":"markdown","e9dabeb3":"markdown","8c1bbb93":"markdown","fec799ea":"markdown","46437a53":"markdown","5c389c03":"markdown","9355b307":"markdown","c7f252cb":"markdown","5849a3f6":"markdown","c7ab3ac5":"markdown","24a6bedb":"markdown","1572a4b9":"markdown","52ceb8e0":"markdown","d658d2e1":"markdown","783c96b2":"markdown","b9fa89ee":"markdown","68dc6cbc":"markdown","00a43be5":"markdown","8702a7f9":"markdown","95e621d1":"markdown","233b12bd":"markdown","7b9f8244":"markdown","cb02e787":"markdown","dc951652":"markdown","180f3bbf":"markdown"},"source":{"6a540a3d":"!pip install cord-19-tools\n!pip install spacy-langdetect\n!pip install https:\/\/s3-us-west-2.amazonaws.com\/ai2-s2-scispacy\/releases\/v0.2.4\/en_core_sci_lg-0.2.4.tar.gz","851c951f":"import cotools as co\nimport gc\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom IPython.core.display import display, HTML\nfrom collections import defaultdict\nimport functools\nimport spacy\nfrom spacy.matcher import PhraseMatcher\nfrom spacy_langdetect import LanguageDetector\nimport en_core_sci_lg\nimport os\nimport re\nimport sys\nimport glob\n  \nfrom sklearn import cluster\nfrom sklearn import metrics\nfrom sklearn.manifold import TSNE\n\n#from bert_serving.client import BertClient  # if using bert\n\nfrom gensim.models import Word2Vec\nfrom gensim.summarization.summarizer import summarize \nfrom gensim.summarization import keywords \n\nfrom tqdm.notebook import tqdm\n\nfrom nltk.corpus import stopwords\nfrom string import punctuation\nfrom nltk.stem.lancaster import LancasterStemmer\nfrom nltk.tokenize import sent_tokenize,word_tokenize\nfrom nltk.probability import FreqDist\nfrom nltk.cluster import KMeansClusterer\nimport nltk\nfrom heapq import nlargest\n\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D","68e69e50":"pd.options.mode.chained_assignment = None","82ce57d0":"# Use the ipwidgets package to display a progress bar to give the user an indication\n# of the progress of code execution for certain portions of the notebook.\n# This function is intended to be used in the definition of a for loop to indicate\n# how far execution has gotten through the object being iterated over in the for loop.\n#\n# Inputs: (sequence) - contains the for loop iteration (e.g., list or iterator)\n#         (every)    - number of steps to display\n#\n# Outputs: displays the progress bar in the notebook\n#          (yield record) - returns the current iteration object back to the calling for loop\n#\ndef log_progress(sequence, every=None, size=None, name='Items'):\n    \n    '''\n    Tracks the progress of for loop iteration\n    \n    Inputs: (sequence) - contains the for loop iteration\n            (every) - the number of steps to display\n    Outputs: (display) - a tracking bar that shows the progress of for loop iteration\n    '''\n    \n    from ipywidgets import IntProgress, HTML, VBox\n    from IPython.display import display\n\n    is_iterator = False\n    if size is None:\n        try:\n            size = len(sequence)\n        except TypeError:\n            is_iterator = True\n    if size is not None:\n        if every is None:\n            if size <= 200:\n                every = 1\n            else:\n                every = int(size \/ 200)     # every 0.5%\n    else:\n        assert every is not None, 'sequence is iterator, set every'\n    \n    # Instantiate and display the progress bar        \n    if is_iterator:\n        progress = IntProgress(min=0, max=1, value=1)\n        progress.bar_style = 'info'\n    else:\n        progress = IntProgress(min=0, max=size, value=0)\n    label = HTML()\n    box = VBox(children=[label, progress])\n    display(box)\n    \n    # Update the progress bar state at each iteration of the for loop using this function\n    index = 0\n    try:\n        for index, record in enumerate(sequence, 1):\n            if index == 1 or index % every == 0:\n                if is_iterator:\n                    label.value = '{name}: {index} \/ ?'.format(\n                        name=name,\n                        index=index\n                    )\n                else:\n                    progress.value = index\n                    label.value = u'{name}: {index} \/ {size}'.format(\n                        name=name,\n                        index=index,\n                        size=size\n                    )\n                    \n            # return the current iteration object, preserving the state of the function\n            yield record\n    except:\n        progress.bar_style = 'danger'\n        raise\n    else:\n        progress.bar_style = 'success'\n        progress.value = index\n        label.value = \"{name}: {index}\".format(\n            name=name,\n            index=str(index or '?')\n        )\n        \n# Use the spaCy library natural language processing capabilities to clean an input text, \n# in string format, for punctuation, stop words, and lemmatization.\n#\n# Inputs (text) - a string to clean and lemmatize\n#\n# Outputs - a modified version of the input string that has been cleaned by removing punctuation, \n#           stop words, and pronouns, and has had the remaining words converted into corresponding lemmas\n#         \ndef process_text(text):\n    \n    '''\n    Cleans an input text in string format for punctuation, stopwords and lemmatization\n    \n    Inputs: (string) - input text\n    Outputs: (string) - a cleaned output that removes punctuation, stopwords and converts words to lemma\n    '''\n    \n    # Create a spaCy \"Doc\" object from the input text string.\n    doc = nlp(text.lower())\n    result = [] # list that will contain the lemmas for each word in the input string\n    \n    for token in doc:\n        \n        if token.text in nlp.Defaults.stop_words: #screen out stop words\n            continue\n        if token.is_punct:                        #screen out punctuations\n            continue\n        if token.lemma_ == '-PRON-':              #screen out pronouns\n            continue\n        \n        result.append(token.lemma_)\n    \n    # Return the lemmatized version of the cleaned input text string\n    return \" \".join(result)","977279c4":"# Set the path where the raw data is\ndata_dir = '\/kaggle\/input\/CORD-19-research-challenge'\n# Set the current working directory path to where the raw data is\nos.chdir(data_dir)\n\n# Set the path where the formatted data will be stored\noutput_dir = '\/kaggle\/working\/'\n\n# Read in the metadata.csv file as a pandas DataFrame\nmetadata_information = pd.read_csv('metadata.csv')","75b8baaf":"metadata_information.shape","c1daa125":"# Checks if string input can be interpreted as a date\n#    \n# Inputs: (string) - string to check whether it is a valid date\n# Outputs: (bool) - True if string is a valid date; False otherwise\n#\ndef is_date(string, fuzzy=False):\n    \n    '''\n    Checks if string input can be interpreted as a date\n    \n    Inputs: (string) - to check if date interpretation is possible\n    Outputs: (bool) - True if possible else False\n    '''\n    from dateutil.parser import parse\n    \n    try: \n        parse(string, fuzzy=fuzzy)\n        return True\n\n    except ValueError:\n        return False","c585ca34":"#print('Please input in the earliest date to filter the research paper (yyyy-mm-dd)!')\n#filter_date = str(input())\n\n# Modify this date per user requirements, or enter a non-valid date string to disable publication date filtering\nfilter_date = '2019-12-01'\n\n# paper_id_list is a list of the IDs for all papers published after the specified date\n# (or all papers if the date filtering is disabled).\n\nif is_date(filter_date) == True:\n    paper_id_list = metadata_information[metadata_information['publish_time'] >= filter_date].dropna(subset=['sha'])['sha'].tolist()\n    \nelse:\n    paper_id_list = metadata_information['sha'].tolist()","db3f8581":"def create_library(list_of_folders, list_of_papers = paper_id_list):\n    \n    import json\n    #Internal Library\n    internal_library = []\n\n    for i in log_progress(list_of_folders, every = 1):\n\n        try:\n\n            pdf_file_path = data_dir + '\/' + i + '\/' + i + '\/pdf_json'\n            pdf_file_list = [i for i in os.listdir(pdf_file_path) if i.split('.')[0] in list_of_papers]\n            print('There are {a} papers in the {c} group after {b}.'.format(a = len(pdf_file_list), b = filter_date, c = str(i + str('_pdf'))))\n\n            for each_file in pdf_file_list:\n                file_path = data_dir + '\/' + i + '\/' + i + '\/pdf_json\/' + each_file\n\n                with open(file_path) as f:\n                    data = json.load(f)\n\n                internal_library.append(data)\n\n        except:\n            continue\n\n        try:\n\n            pmc_file_path = data_dir + '\/' + i + '\/' + i + '\/pmc_json'\n            pmc_file_list = [i for i in os.listdir(pmc_file_path) if i.split('.')[0] in list_of_papers]\n            print('There are {a} papers in the {c} group after {b}.'.format(a = len(pmc_file_list), b = filter_date, c = str(i + str('_pmc'))))\n\n            for each_file in pmc_file_list:\n                file_path = data_dir + '\/' + i + '\/' + i + '\/pmc_json\/' + each_file\n\n                with open(file_path) as f:\n                    data = json.load(f)\n\n                internal_library.append(data)\n\n        except:\n            continue\n            \n    return internal_library\n\ndef data_creation(list_of_folders, metadata, date = filter_date, list_of_papers = paper_id_list):\n    \n    '''\n    Converts JSON files into CSV based on various criteria\n    \n    Inputs: (list) - List_of_folders ; the names of the sub-directories in the library\n            (DataFrame) - metadata ; metadata information provided in the library\n            (string) - date ; filter criteria on publishing date information available in metadata\n            (list) - list_of_papers ; containing the index information of papers published after date\n    Outputs: (DataFrame) - dataframe containing on relevant papers\n    '''    \n    internal_library = create_library(list_of_folders = selected_folders, list_of_papers = paper_id_list)\n\n    title_list = []          # list of paper titles\n    abstract_list = []       # list of paper abstracts\n    text_list = []           # list of paper full texts\n\n    # Extracting title, abstract and text information for each paper\n    # each_dataset is a list of dictionaries, where each dictionary corresponds to one paper\n    for i in list(range(0, len(internal_library))):\n\n        title_list.append(internal_library[i].get('metadata').get('title'))\n\n        try:\n            abstract_list.append(co.abstract(internal_library[i]))\n        except:\n            abstract_list.append('No Abstract')\n\n        text_list.append(co.text(internal_library[i]))\n\n    #Extracting Paper ID Information\n    paper_id = [i.get('paper_id') for i in internal_library]   # list of the ID for each paper\n\n    #Extracting the location and country that published the research paper\n    primary_location_list = []      # list of the primary locations for the authors of each paper\n    primary_country_list = []       # list of the primary countries for the authors of each paper\n\n    # Extracting the primary location, and country for the authors of each paper\n    # each_dataset is a list of dictionaries, where each dictionary corresponds to one paper\n\n    # Extract list of metadata dictionaries for each paper\n    internal_metadata = [i['metadata'] for i in internal_library]\n\n    # individual_paper_metadata is the 'metadata' dictionary for one paper\n    for individual_paper_metadata in internal_metadata:\n\n        # Extract the list of author dictionaries for the current paper\n        authors_information = individual_paper_metadata.get('authors')\n\n        if len(authors_information) == 0:\n            primary_location_list.append('None')\n            primary_country_list.append('None')\n\n        else:\n            location = None\n            country = None\n            i = 1\n\n            # Find the first author of the paper with valid data for \"institution\",\n            # location, and country, extract this information, and add to\n            # the respective lists for all the papers\n            while location == None and i <= len(authors_information):\n\n                if bool(authors_information[i-1].get('affiliation')) == True:\n\n                    location = authors_information[i-1].get('affiliation').get('location').get('settlement')\n                    country = authors_information[i-1].get('affiliation').get('location').get('country')\n\n                i += 1\n\n            primary_location_list.append(location)\n            primary_country_list.append(country)\n                \n    #Loading all the extracted information into a DataFrame for merger\n    index_df = pd.DataFrame(paper_id, columns =  ['paper_id'])\n\n    geographical_df = pd.DataFrame(primary_location_list, columns = ['Location'])\n    geographical_df['Country'] = primary_country_list\n\n    paper_info_df = pd.DataFrame(title_list, columns = ['Title'])\n    paper_info_df['Abstract'] = abstract_list\n    paper_info_df['Text'] = text_list\n    \n    #This dataframe contains all the information extracted from the JSON files and converted into CSV.\n    combined_df = pd.concat([index_df, geographical_df, paper_info_df], axis = 1)\n    \n    #Creating the merger between the metadata (45000+) and the research papers (33000+)\n    part_1 = metadata[['sha', 'abstract', 'url', 'publish_time']]\n\n    test_df = combined_df.merge(part_1, left_on = ['paper_id'], right_on = ['sha'], how = 'left')\n    test_df.drop(['sha'], axis = 1,inplace = True)\n    test_df = test_df[['paper_id', 'url', 'publish_time', 'Location', 'Country', 'Title', 'Abstract', 'abstract', 'Text']]\n    \n    #In the event where the JSON's abstract is null but there is an abstract in the metadata, it will be used as a substitute.\n    test_df['Abstract'] = np.where(test_df['Abstract'] == '', test_df['abstract'], test_df['Abstract'])\n    test_df.drop(['abstract'], axis = 1, inplace = True)\n    \n    gc.collect()\n    \n    return test_df","9b1036db":"# Create a list of the datasets corresponding to each subdirectory over which we can iterate\n\n# Define as a list the names of all the subdirectories in the 'Raw Data'\n# directory where the dataset files are stored\nselected_folders = ['comm_use_subset', 'noncomm_use_subset', 'custom_license', 'biorxiv_medrxiv']\ntest_df = data_creation(list_of_folders = selected_folders, metadata = metadata_information)","231ba9ed":"test_df.columns","c76317e1":"test_df.to_csv(output_dir + 'Checkpoint_1.csv', index = False)","31b1b560":"#Cleaning up after each section to save space\ndel paper_id_list\ndel metadata_information\ndel selected_folders\n\nimport gc\ngc.collect()","41c57702":"def cleaning_dataset(dataset, columns_to_clean):\n    \n    # each_column is on of the defined text section columns from the DataFrame\n    # Use the log_progress() helper function defined above to indicate the progress of the execution\n    for each_column in log_progress(columns_to_clean, every = 1):\n\n        # Fill in any null text items with \"No Information\"\n        dataset[each_column] = dataset[each_column].fillna('No Information')\n\n        # Remove square-bracketed references (i.e., [1])\n        dataset[each_column] = dataset[each_column].apply(lambda x: re.sub(r'\\[.*?]', r'', x))\n\n        # Remove parenthesis references (i.e., (1))\n        dataset[each_column] = dataset[each_column].apply(lambda x: re.sub(r'\\((.*?)\\)', r'', x))\n\n        # Remove garbage characters\n        dataset[each_column] = dataset[each_column].apply(lambda x: re.sub(r'[^a-zA-z0-9.%\\s-]', r'', x))\n\n        # Remove unnecessary white space\n        dataset[each_column] = dataset[each_column].apply(lambda x: re.sub(r' +', r' ', x))\n\n        # Remove unnecessary white space at the end of the text section\n        dataset[each_column] = dataset[each_column].apply(lambda x: x.rstrip())\n\n        # Remove white space before punctuation marks\n        dataset[each_column] = dataset[each_column].apply(lambda x: re.sub(r'\\s([?.!\"](?:\\s|$))', r'\\1', x))\n\n    cleaned_abstract = []     # list of cleaned abstracts for all the papers\n    abstract_count = []       # list of the word counts for each paper abstract\n\n    # Clean up abstracts as abstracts may contain unnecessary starting words like 'background' or 'abstract'\n    # Count the words in each cleaned abstract and add the list of abstract word counts for each paper to\n    # the test_df Data Frame\n    #\n    # i is the abstract text (string) for one paper\n    for i in dataset['Abstract']:\n\n        if i.split(' ')[0].lower() == 'background' or i.split(' ')[0].lower() == 'abstract':\n            cleaned_abstract.append(' '.join(i.split(' ')[1:]))\n            abstract_count.append(len(i.split(' ')[1:]))\n\n        else:\n            cleaned_abstract.append(i)\n            abstract_count.append(len(i.split()))\n\n    dataset['Abstract'] = cleaned_abstract\n    dataset['Abstract Word Count'] = abstract_count\n\n    # Removing the words figure X.X from the passages because it contributes no meaning\n    fig_exp = re.compile(r\"Fig(?:ure|.|-)\\s+(?:\\d*[a-zA-Z]*|[a-zA-Z]*\\d*|\\d*)\", flags=re.IGNORECASE) \n    dataset['Text'] = [(re.sub(fig_exp, '', i)) for i in test_df['Text']]\n\n    # Remove other instances of poor references and annotations\n    poor_annotation_exp_1 = re.compile(r'(\\d)\\s+(\\d]*)', flags = re.IGNORECASE)\n    dataset['Text'] = [(re.sub(poor_annotation_exp_1, '', i)) for i in test_df['Text']]\n\n    poor_annotation_exp_2 = re.compile(r'(\\d])*', flags = re.IGNORECASE)\n    dataset['Text'] = [(re.sub(poor_annotation_exp_2, '', i)) for i in test_df['Text']]\n    \n    gc.collect()\n    \n    return dataset","49a3fa37":"## Cleaning up Dataset in the selected text columns\ntext_columns = ['Title', 'Abstract', 'Text']\ntest_df = cleaning_dataset(dataset = test_df, columns_to_clean = text_columns)","b1c34b01":"test_df['Abstract'].describe(include='all')","b2145cbd":"test_df.drop_duplicates(['Abstract', 'Text'], inplace = True)","b090bcb7":"test_df['Text'].describe(include = 'all')","60fadb40":"test_df.to_csv(output_dir + 'Checkpoint_2.csv', index = False)","4c3bea84":"test_df.columns","6455fff4":"test_df.dropna(subset = ['Text'], inplace = True)","beb9bc7e":"def dimension_reduction(dataset, n = 3, n_components = 3, use_hashing_vectorizer = False):\n\n    dataset = dataset.reset_index().drop(['index'], axis = 1)\n    \n    #Extracting Trigrams vectors for all 3885 documents\n    if use_hashing_vectorizer == False:\n    \n        vectorizer=TfidfVectorizer(ngram_range=(n,n))\n        vectorized_vectors=vectorizer.fit_transform(dataset['Text'].tolist())\n        \n    else:\n        \n        vectorizer=HashingVectorizer(ngram_range=(n,n))\n        vectorized_vectors=vectorizer.fit_transform(dataset['Text'].tolist())\n\n    #Dimensionality Reduction\n    tsne_reduction = TSNE(n_components = 3, perplexity = 10, learning_rate = 100, random_state = 777)\n    tsne_data = tsne_reduction.fit_transform(vectorized_vectors)\n\n    #Converting components of T-SNE into dataframe\n    tsne_df = pd.DataFrame(tsne_data, columns = [i for i in range(0, tsne_data.shape[1])])\n    gc.collect()\n    return tsne_df\n\ndef visualizing_dimensions(dataset):\n\n    fig = plt.figure(1, figsize=(7, 5))\n    ax = Axes3D(fig, rect=[0, 0, .95, 1], elev=48, azim=134)\n\n    ax.scatter(dataset[0], dataset[1], dataset[2], c=dataset[2], cmap='viridis', linewidth=0.5)\n\n    ax.set_xlabel('Component A')\n    ax.set_ylabel('Component B')\n    ax.set_zlabel('Component C')\n\n    plt.show()\n    gc.collect()\n    \ndef outlier_removals(dim_reduced_dataset, dataset, n_components = 3, number_std_dev = 2.5, verbose = 1):\n    \n    outlier_papers = []\n    print('{a} standard deviation is being used to clean the dataset.'.format(a = number_std_dev))\n    print()\n    for i in range(0, n_components):\n        \n        upper = dim_reduced_dataset[i].mean() + number_std_dev*dim_reduced_dataset[i].std()\n        lower = dim_reduced_dataset[i].mean() - number_std_dev*dim_reduced_dataset[i].std()\n\n        outlier_df = dim_reduced_dataset[(dim_reduced_dataset[i] >= upper) | (dim_reduced_dataset[i] <= lower)]\n        outlier_list = outlier_df.reset_index()['index'].tolist()\n        \n        outlier_papers += outlier_list\n        \n    outlier_papers = list(set(outlier_papers))\n    \n    if verbose == 1:\n        print('There are {a} outlier papers identified.'.format(a = len(outlier_papers)))\n        print()\n        \n    outlier_papers_df = dataset.iloc[outlier_papers,:]\n    \n    if verbose == 1:\n        print('These are the texts that are determined as abnormal.')\n        print()\n        for i in outlier_papers_df['Text']:\n            print(i)\n            print()\n    \n    #remove outliers\n    cleaned_df = dataset.drop(outlier_papers, axis = 0)\n    cleaned_df.reset_index().drop(columns = ['index'], axis = 1)\n    gc.collect()\n    return cleaned_df\n\ndef full_cleaning_process(dataset, n = 3, n_components = 3, use_hashing_vectorizer = False, std_dev = 3, verbose = 1):\n    \n    starting_datashape = dataset.shape[0]\n    dim_reduced_dataset = dimension_reduction(dataset, n = n, n_components = n_components, use_hashing_vectorizer = use_hashing_vectorizer)\n    print('Before Cleaning Up -')\n    visualizing_dimensions(dim_reduced_dataset)\n    output_df = outlier_removals(dim_reduced_dataset, dataset, n_components = n_components, number_std_dev = std_dev, verbose = verbose)\n    ending_datashape = output_df.shape[0]\n    print('{a} rows were dropped in this cleaning process.'.format(a = starting_datashape - ending_datashape))\n    print()\n    print('After Cleaning Up -')\n    visualizing_dimensions(dimension_reduction(output_df, n = 3, n_components = 3, use_hashing_vectorizer = False))\n    gc.collect()\n    return output_df","6cfeffc2":"test_df = full_cleaning_process(test_df, std_dev = 2.5)","caff8568":"minimum_word_count = 150\n\ntest_df = test_df.reset_index().drop(['index'], axis = 1)\ntest_df['Text Word Count'] = [len(i.split()) for i in test_df['Text']]\n\ndirty_list = []\n\nfor index, value in test_df.iterrows():\n    \n    if (value['Text Word Count'] <= minimum_word_count):\n        dirty_list.append(index)\n        \nweird_papers_df = test_df.iloc[dirty_list,:]\n\nfor index, value in weird_papers_df.iterrows():\n    print(value['Text Word Count'], value['Text'])\n    print()","e70f50b4":"test_df = test_df.drop(dirty_list, axis = 0)\ntest_df = test_df.reset_index().drop(['index'], axis = 1)","792cc34a":"test_df.to_csv(output_dir + 'Checkpoint_3.csv', index = False)","97f81aa8":"#Cleaning up after each section to save space\ngc.collect()","5ffe329f":"#Scientific NLP library has been loaded to find articles that may or may not be in english. This acts as a final data\n#clean-up, ensuring that the subsequent ML techniques are performed on as clean a dataset as possible.\nnlp = en_core_sci_lg.load()\nnlp.add_pipe(LanguageDetector(), name=\"language_detector\", last=True)","81f5e87c":"#There exists a possibility that in this library of research papers, there exists non-English papers.\n\n#Since some texts exceed the maximum length and may cause memory allocation, we will cut-off the text at the maximum length\n#instead of changing it - to control computational resources\n\nlanguage_list = []\nfor i in log_progress(test_df['Text'], every = 1):\n    \n    if len(i) <= 1000000:\n    \n        doc = nlp(i)\n        language_list.append(doc._.language)\n        \n    else:\n        \n        cut_off_index = i[:1000000].rfind('.')\n        focus_i = i[:cut_off_index + 1]\n        \n        doc = nlp(focus_i)\n        language_list.append(doc._.language)","25647211":"#Storing information on the language detected of the paper. This score provides an indication\n#of how much of the paper is in that particular language detected - helping us deal with papers with a combination of languages.\nfiltered_language_list = [i['language'].upper() for i in language_list]\ntest_df['Language'] = filtered_language_list\n\n#Filtering out only research papers in English to perform topic modelling.\nenglish_df = test_df[test_df['Language'] == 'EN']\nprint('There are {a} research papers in English out of {b} research papers.'.format(a = english_df.shape[0], b = test_df.shape[0]))","ae123513":"# drop off Language column, as all articles are English\nenglish_df.drop(columns='Language', inplace=True)","18b1a85c":"english_df.shape","b383b1d3":"english_df.to_csv(output_dir + 'Checkpoint_4.csv', index = False)","15036695":"#Cleaning up after each section to save space\ngc.collect()","c6c89218":"# Load libraries \nimport os \nimport numpy as np \nimport pandas as pd \nimport glob\nimport gc\n\nfrom tqdm.notebook import tqdm\n\n# Load word cloud function\nfrom wordcloud import WordCloud, STOPWORDS \nimport matplotlib.pyplot as plt \n\nimport spacy\nfrom spacy.matcher import PhraseMatcher #import PhraseMatcher class\nnlp = spacy.load('en_core_web_lg') # Language class with the English model 'en_core_web_lg' is loaded","976aaf58":"def wordcloud_draw(text, color = 'white'):\n    \"\"\"\n    Plots wordcloud of string text after removing stopwords\n    \"\"\"\n    cleaned_word = \" \".join([word for word in text.split()])\n    wordcloud = WordCloud(stopwords=STOPWORDS,\n                      background_color=color,\n                      width=1000,\n                      height=1000\n                     ).generate(cleaned_word)\n    plt.figure(1,figsize=(15, 15))\n    plt.imshow(wordcloud)\n    plt.axis('off')\n    display(plt.show())","aabb4890":"# Load checkpoint #4\n\ndf=pd.read_csv(output_dir + 'Checkpoint_4.csv')\ndf.shape","e16160d2":"# clean up\ndel test_df\ndel english_df\n\ngc.collect()","6acec3d7":"# Set list data directory\nlists_data_dir = '\/kaggle\/input\/task10lists\/master'\nos.chdir(lists_data_dir)\nos.getcwd()","4016f949":"# Load list of therapeutics:\ndf_therapeutics = pd.read_csv('therapeutics.csv')\n#df_therapeutics.shape\n#df_therapeutics.head()\ntherapeutics_list = df_therapeutics.iloc[:, 0].tolist()\nprint(therapeutics_list)","651feabe":"# Load list of vaccines:\ndf_vaccine = pd.read_csv('vaccines.csv')\n#df_vaccine.shape\n#df_vaccine.head()\nvaccine_list = df_vaccine.iloc[:, 0].tolist()\nprint(vaccine_list)","3616fc2f":"# Load list of animals:\ndf_animals = pd.read_csv('animals.csv')\n#df_animals.shape\n#df_animals.head()\nanimals_list = df_animals.iloc[:, 0].tolist()\nprint(animals_list)","a950aaab":"# Load list of covid19:\ndf_covid19 = pd.read_csv('covid-19.csv')\n#df_covid19.shape\n#df_covid19.head()\ncovid19_list = df_covid19.iloc[:, 0].tolist()\nprint(covid19_list)","4b094166":"# Load list of drugs:\ndf_drugs = pd.read_csv('drugs.csv')\n#df_drugs.shape\n#df_drugs.head()\ndrugs_list = df_drugs.iloc[:, 0].tolist()\nprint(drugs_list)","38f52580":"# effectivenes\ndf_effectivenes = pd.read_csv('effectivenes.csv')\n#df_effectivenes.shape\n#df_effectivenes.head()\neffectivenes_list = df_effectivenes.iloc[:, 0].tolist()\nprint(effectivenes_list)","61bbe30c":"# symptom\ndf_symptom = pd.read_csv('symptom.csv')\n#df_symptom.shape\n#df_symptom.head()\nsymptom_list = df_symptom.iloc[:, 0].tolist()\nprint(symptom_list)","7dabeff0":"# human\ndf_human = pd.read_csv('human.csv')\n#df_human.shape\n#df_human.head()\nhuman_list = df_human.iloc[:, 0].tolist()\nprint(human_list)","6b045359":"# model\ndf_model = pd.read_csv('model.csv')\n#df_model.shape\n#df_model.head()\nmodel_list = df_model.iloc[:, 0].tolist()\nprint(model_list)","9b9e8f93":"# recipient\ndf_recipient = pd.read_csv('recipient.csv')\n#df_recipient.shape\n#df_recipient.head()\nrecipient_list = df_recipient.iloc[:, 0].tolist()\nprint(recipient_list)","8d1e0c44":"# antiviral \ndf_antiviral_agent  = pd.read_csv('antiviral.csv')\n#df_antiviral_agent.shape\n#df_antiviral_agent.head()\nantiviral_agent_list = df_antiviral_agent.iloc[:, 0].tolist()\nprint(antiviral_agent_list)","d92e7e03":"# challenge\ndf_challenge = pd.read_csv('challenge.csv')\n#df_challenge.shape\n#df_challenge.head()\nchallenge_list = df_challenge.iloc[:, 0].tolist()\nprint(challenge_list)","f3b1f244":"# universal\ndf_universal = pd.read_csv('universal.csv')\n#df_universal.shape\n#df_universal.head()\nuniversal_list = df_universal.iloc[:, 0].tolist()\nprint(universal_list)","7dd902a5":"# prioritize\ndf_prioritize = pd.read_csv('prioritize.csv')\n#df_prioritize.shape\n#df_prioritize.head()\nprioritize_list = df_prioritize.iloc[:, 0].tolist()\nprint(prioritize_list)","c96d840e":"# scarce\ndf_scarce = pd.read_csv('scarce.csv')\n#df_scarce.shape\n#df_scarce.head()\nscarce_list = df_scarce.iloc[:, 0].tolist()\nprint(scarce_list)","132aa736":"# healthcare\ndf_healthcare = pd.read_csv('healthcare.csv')\n#df_healthcare.shape\n#df_healthcare.head()\nhealthcare_list = df_healthcare.iloc[:, 0].tolist()\nprint(healthcare_list)","4483754e":"# ppe\ndf_ppe = pd.read_csv('ppe.csv')\n#df_ppe.shape\n#df_ppe.head()\nppe_list = df_ppe.iloc[:, 0].tolist()\nprint(ppe_list)","90e5e3b1":"# risk\ndf_risk = pd.read_csv('risk.csv')\n#df_risk.shape\n#df_risk.head()\nrisk_list = df_risk.iloc[:, 0].tolist()\nprint(risk_list)","2b35c847":"# ADE\ndf_ADE = pd.read_csv('antibody.csv')\n#df_ADE.shape\n#df_ADE.head()\nADE_list = df_ADE.iloc[:, 0].tolist()\nprint(ADE_list)","38e9be11":"# Use LOWER case\nmatcher = PhraseMatcher(nlp.vocab, attr='LOWER')","2b4ee24d":"# Load list into NLP \n\npatterns = [nlp.make_doc(text) for text in therapeutics_list] \nmatcher.add(\"1\", None, *patterns)\n\npatterns = [nlp.make_doc(text) for text in vaccine_list] \nmatcher.add(\"2\", None, *patterns)\n\npatterns = [nlp.make_doc(text) for text in animals_list] \nmatcher.add(\"3\", None, *patterns)\n\npatterns = [nlp.make_doc(text) for text in covid19_list] \nmatcher.add(\"4\", None, *patterns)\n\npatterns = [nlp.make_doc(text) for text in drugs_list] \nmatcher.add(\"5\", None, *patterns)\n\npatterns = [nlp.make_doc(text) for text in effectivenes_list]\nmatcher.add('6', None, *patterns)\n\npatterns = [nlp.make_doc(text) for text in symptom_list]\nmatcher.add('7', None, *patterns)\n\npatterns = [nlp.make_doc(text) for text in human_list]\nmatcher.add('8', None, *patterns)\n\npatterns = [nlp.make_doc(text) for text in model_list]\nmatcher.add('9', None, *patterns)\n\npatterns = [nlp.make_doc(text) for text in recipient_list]\nmatcher.add('10', None, *patterns)\n\npatterns = [nlp.make_doc(text) for text in antiviral_agent_list]\nmatcher.add('11', None, *patterns)\n\npatterns = [nlp.make_doc(text) for text in challenge_list]\nmatcher.add('12', None, *patterns)\n\npatterns = [nlp.make_doc(text) for text in universal_list]\nmatcher.add('13', None, *patterns)\n\npatterns = [nlp.make_doc(text) for text in prioritize_list]\nmatcher.add('14', None, *patterns)\n\npatterns = [nlp.make_doc(text) for text in scarce_list]\nmatcher.add('15', None, *patterns)\n\npatterns = [nlp.make_doc(text) for text in healthcare_list]\nmatcher.add('16', None, *patterns)\n\npatterns = [nlp.make_doc(text) for text in ppe_list]\nmatcher.add('17', None, *patterns)\n\npatterns = [nlp.make_doc(text) for text in risk_list]\nmatcher.add('18', None, *patterns)\n\npatterns = [nlp.make_doc(text) for text in ADE_list]\nmatcher.add('19', None, *patterns)","5dea474e":"df.describe()","1c54285b":"import gc\ngc.collect()","f70cfe94":"# add column to data to prepare ranking for given paper\ndf = df.assign(p_1=0,p_2=0,p_3=0,p_4=0,p_5=0,p_6=0,p_7=0,p_8=0,p_9=0,p_10=0,p_11=0,p_12=0,p_13=0,p_14=0,p_15=0,p_16=0,p_17=0,p_18=0,p_19=0)","fa3e0b6d":"df.head()","5a2d89a1":"matching_rows = []\nmatching_paper_id = []\n\nnlp.max_length = 206000000\n\npbar = tqdm()\npbar.reset(total=len(df)) \n\nfor i, row in df[:].iterrows(): \n    pbar.update()\n    if pd.isnull(row[\"Text\"]):\n        continue\n    doc = nlp(row[\"Text\"])\n    matches = matcher(doc)\n    if len(matches) > 0:\n        matching_rows.append(i)\n        matching_paper_id.append(row[\"paper_id\"])\n    for match_id, start, end in matches:\n        # Get the string representation \n        string_id = nlp.vocab.strings[match_id]  #string_id shows matching location \n        span = doc[start:end]  \n        df.loc[i, \"p_\" + string_id] = 1  ","07997be9":"df.describe()","8f6901e1":"# Prepare ranking: drugs + convid 19 + effectiveness\ndf = df.assign(rank=df[\"p_5\"] + df[\"p_4\"] + df[\"p_6\"])","f3e12671":"# This result should be added to the narrative, it's going to show the % per list matching\ndf.describe(include='all')","7e546f3b":"# Show total number of articles with matching list \nprint(len(df[df[\"rank\"] == 1]))\nprint(len(df[df[\"rank\"] == 2]))\nprint(len(df[df[\"rank\"] == 3]))","42a666fd":"#print number of articles with all matching\ndf[df[\"rank\"] == 3]","a729e836":"##word cloud matching drugs + effectiveness + sympton + covid 19 \ntext_world_cloud=\"\"\nfor i, row in df[df[\"rank\"] == 2].iterrows(): \n    text_world_cloud = text_world_cloud +\" \" + str (row[\"Title\"])\n#Visualization rank == 2\nwordcloud_draw(text_world_cloud.lower())","e0014c30":"# Prepare ranking: vacciness + ADE + covid 19  \ndf = df.assign(rank=df[\"p_2\"] + df[\"p_19\"] + df[\"p_4\"])","7c920098":"# This result should be added to the narrative, it's going to show the % per list matching\ndf.describe()","53aa0584":"# Show total number of articles with matching list \nprint(len(df[df[\"rank\"] == 1]))\nprint(len(df[df[\"rank\"] == 2]))\nprint(len(df[df[\"rank\"] == 3]))","a38d04bc":"#print number of articles with all matching\ndf[df[\"rank\"] == 3]","9de46a8a":"##word cloud matching vacciness + receipts + ade   \ntext_world_cloud=\"\"\nfor i, row in df[df[\"rank\"] == 3].iterrows(): \n    text_world_cloud = text_world_cloud +\" \" + str (row[\"Title\"])\n#Visualization rank == 3\nwordcloud_draw(text_world_cloud.lower())","0d5582c2":"# Prepare ranking: vacciness + animals + human + model + covid 19 \ndf = df.assign(rank=df[\"p_2\"] + df[\"p_3\"] + df[\"p_8\"] + df[\"p_9\"] + df[\"p_4\"])","77f48bc0":"# This result should be added to the narrative, it's going to show the % per list matching\ndf.describe()","5733358c":"# Show total number of articles with matching list \nprint(len(df[df[\"rank\"] == 1]))\nprint(len(df[df[\"rank\"] == 2]))\nprint(len(df[df[\"rank\"] == 3]))\nprint(len(df[df[\"rank\"] == 4]))\nprint(len(df[df[\"rank\"] == 5]))","156a61a4":"#print number of articles with all matching\ndf[df[\"rank\"] == 5]","d3fe758a":"##word cloud matching vacciness + animals + human + model  \ntext_world_cloud=\"\"\nfor i, row in df[df[\"rank\"] == 4].iterrows(): \n    text_world_cloud = text_world_cloud +\" \" + str (row[\"Title\"])\n#Visualization rank == 5\nwordcloud_draw(text_world_cloud.lower())","cf9b2468":"# Prepare ranking: therapeutics + symptons + antiviral agent + covid 19   \ndf = df.assign(rank=df[\"p_1\"] + df[\"p_7\"] + df[\"p_11\"] + df[\"p_4\"])","2a1a1738":"# This result should be added to the narrative, it's going to show the % per list matching\ndf.describe()","4d0deead":"# Show total number of articles with matching list \nprint(len(df[df[\"rank\"] == 1]))\nprint(len(df[df[\"rank\"] == 2]))\nprint(len(df[df[\"rank\"] == 3]))\nprint(len(df[df[\"rank\"] == 4]))","1134873f":"#print number of articles with all matching\ndf[df[\"rank\"] == 4]","0900b600":"##word cloud matching therapeutics + symptons + antiviral agent \ntext_world_cloud=\"\"\nfor i, row in df[df[\"rank\"] == 4].iterrows(): \n    text_world_cloud = text_world_cloud +\" \" + str (row[\"Title\"])\n#Visualization rank == 4\nwordcloud_draw(text_world_cloud.lower())","53206bcd":"# Prepare ranking: therapeutics + prioritize + covid 19  \ndf = df.assign(rank=df[\"p_1\"] + df[\"p_14\"] + df[\"p_4\"])","0c5d6416":"# This result should be added to the narrative, it's going to show the % per list matching\ndf.describe()","9419bf23":"# Show total number of articles with matching list \nprint(len(df[df[\"rank\"] == 1]))\nprint(len(df[df[\"rank\"] == 2]))\nprint(len(df[df[\"rank\"] == 3]))","3f2f1a5c":"#print number of articles with all matching\ndf[df[\"rank\"] == 3]","1bdb2386":"##word cloud matching therapeutics + prioritize + scarce \ntext_world_cloud=\"\"\nfor i, row in df[df[\"rank\"] == 3].iterrows(): \n    text_world_cloud = text_world_cloud +\" \" + str (row[\"Title\"])\n#Visualization rank == 4\nwordcloud_draw(text_world_cloud.lower())","3cd75d7f":"# Prepare ranking: vaccines + universal + covid 19 \ndf = df.assign(rank=df[\"p_13\"] + df[\"p_4\"])","4a4814d8":"# This result should be added to the narrative, it's going to show the % per list matching\ndf.describe()","80750e5e":"# Show total number of articles with matching list \nprint(len(df[df[\"rank\"] == 1]))\nprint(len(df[df[\"rank\"] == 2]))","57096db8":"#print number of articles with all matching\ndf[df[\"rank\"] == 2]","126bb685":"##word cloud matching vaccines + universal \ntext_world_cloud=\"\"\nfor i, row in df[df[\"rank\"] == 2].iterrows(): \n    text_world_cloud = text_world_cloud +\" \" + str (row[\"Title\"])\n#Visualization rank == 2\nwordcloud_draw(text_world_cloud.lower())","9ee118b2":"# Prepare ranking: animals + model + challenge + Covid 19 \ndf = df.assign(rank=df[\"p_3\"] + df[\"p_9\"] + df[\"p_12\"] + df[\"p_4\"])","0ad6c0f9":"# This result should be added to the narrative, it's going to show the % per list matching\ndf.describe()","62d250d7":"# Show total number of articles with matching list \nprint(len(df[df[\"rank\"] == 1]))\nprint(len(df[df[\"rank\"] == 2]))\nprint(len(df[df[\"rank\"] == 3]))\nprint(len(df[df[\"rank\"] == 4]))","4ab4f94a":"#print number of articles with all matching\ndf[df[\"rank\"] == 4]","d6eb35cf":"##word cloud matching animals + model + challenge  \ntext_world_cloud=\"\"\nfor i, row in df[df[\"rank\"] == 4].iterrows(): \n    text_world_cloud = text_world_cloud +\" \" + str (row[\"Title\"])\n#Visualization rank == 4\nwordcloud_draw(text_world_cloud.lower()) ","f0a231ed":"# Prepare ranking: healthcare + ppe + covid 19 \ndf = df.assign(rank=df[\"p_16\"] + df[\"p_17\"] + df[\"p_4\"])","eb293e33":"# This result should be added to the narrative, it's going to show the % per list matching\ndf.describe()","d2165838":"# Show total number of articles with matching list \nprint(len(df[df[\"rank\"] == 1]))\nprint(len(df[df[\"rank\"] == 2]))\nprint(len(df[df[\"rank\"] == 3]))","9a3d8327":"#print number of articles with all matching\ndf[df[\"rank\"] == 3]","524e634d":"##word cloud matching healthcare + ppe \ntext_world_cloud=\"\"\nfor i, row in df[df[\"rank\"] == 3].iterrows(): \n    text_world_cloud = text_world_cloud +\" \" + str (row[\"Title\"])\n#Visualization rank == 3\nwordcloud_draw(text_world_cloud.lower())","46825d50":"# Prepare ranking: animals + model + challenge+ covid 19\ndf = df.assign(rank=df[\"p_3\"] + df[\"p_9\"] + df[\"p_12\"] + df[\"p_4\"])","0da7ecc9":"# This result should be added to the narrative, it's going to show the % per list matching\ndf.describe()","10d94f22":"# Show total number of articles with matching list \nprint(len(df[df[\"rank\"] == 1]))\nprint(len(df[df[\"rank\"] == 2]))\nprint(len(df[df[\"rank\"] == 3]))\nprint(len(df[df[\"rank\"] == 4]))","bf769480":"#print number of articles with all matching\ndf[df[\"rank\"] == 4]","00168096":"##word cloud matching animals + model + challenge \ntext_world_cloud=\"\"\nfor i, row in df[df[\"rank\"] == 4].iterrows(): \n    text_world_cloud = text_world_cloud +\" \" + str (row[\"Title\"])\n#Visualization rank == 4\nwordcloud_draw(text_world_cloud.lower())","31140947":"# Prepare ranking: therapeutics + animals + model + covid 19\ndf = df.assign(rank=df[\"p_1\"] + df[\"p_2\"] + df[\"p_3\"] + df[\"p_4\"])","b59ebc36":"# This result should be added to the narrative, it's going to show the % per list matching\ndf.describe()","acedf8fc":"# Show total number of articles with matching list \nprint(len(df[df[\"rank\"] == 1]))\nprint(len(df[df[\"rank\"] == 2]))\nprint(len(df[df[\"rank\"] == 3]))\nprint(len(df[df[\"rank\"] == 4]))","e22b2f81":"#print number of articles with all matching\ndf[df[\"rank\"] == 4]\n","60adff13":"##word cloud matching animals + model + challenge \ntext_world_cloud=\"\"\nfor i, row in df[df[\"rank\"] == 4].iterrows(): \n    text_world_cloud = text_world_cloud +\" \" + str (row[\"Title\"])\n#Visualization rank == 4\nwordcloud_draw(text_world_cloud.lower())","65001b9e":"#Cleaning up after each section to save space\ngc.collect()","b3b918b7":"questions=['Effectiveness of drugs being developed and tried to treat COVID-19 patients.',\n          'Methods evaluating potential complication of Antibody-Dependent Enhancement (ADE) in vaccine recipients.',\n          'Exploration of use of best animal models and their predictive value for a human vaccine.',\n          'Capabilities to discover a therapeutic (not vaccine) for the disease, and clinical effectiveness studies to discover therapeutics, to include antiviral agents.',\n          'Alternative models to aid decision makers in determining how to prioritize and distribute scarce, newly proven therapeutics as production ramps up.',\n          'Efforts targeted at a universal coronavirus vaccine.',\n          'Efforts to develop animal models and standardize challenge studies.',\n          'Efforts to develop prophylaxis clinical studies and prioritize in healthcare workers.',\n          'Approaches to evaluate risk for enhanced disease after vaccination.',\n          'Assays to evaluate vaccine immune response and process development for vaccines, alongside suitable animal models.']","716f5273":"def list_entity(path):  # read the list of entites\n    df = pd.read_csv(path)\n    lists=[]\n    for row in df.iterrows():\n        lists.append(row[1].values[0])\n    return lists  \n\n# search the dataframe for covid_19 the keywords\ndf_covid=df[functools.reduce(lambda a, b: a|b, (df['Text'].str.contains(s) for s in list_entity('..\/covid-19.csv')))]\n\n \nlen(df_covid)\n","0b9260ca":"main_ent={'main_1':'drugs','main_2':'antibody','main_3':'animals','main_4':'therapeutics'\n          ,'main_5':'therapeutics','main_6':'vaccines','main_7':'animals','main_8': 'healthcare','main_9':'risk', 'main_10': 'vaccines'}\n\ndef entity_list(task,main_ent):\n    path=f\"..\/subtask_{task}\"\n    labels=defaultdict(list)\n    paths_list=defaultdict(list)\n    try:\n        file_name=[]\n        for file in os.listdir(path):\n            if file.endswith(\".csv\"):\n                file_name.append(file)\n        main_name=f\"main_{task}\"\n        labels['main'] = [os.path.splitext(name)[0] for name in file_name if os.path.splitext(name)[0]==main_ent[main_name]]\n        labels['sides'] = [os.path.splitext(name)[0] for name in file_name if os.path.splitext(name)[0]!=main_ent[main_name]]\n        for name in file_name:\n            paths_list[os.path.splitext(name)[0]] = os.path.join(path,name)\n    except (OSError, IOError) as e:\n        print(\"The folder does not exist\")\n        \n    return labels,paths_list\n    \n\ndef list_entity(path):  # read the list of entites\n    df = pd.read_csv(path)\n    lists=[]\n    for row in df.iterrows():\n        lists.append(row[1].values[0])\n    return lists  \n\ndef add_entities(labels,paths):  # add the list of entites\n    for key in labels.keys():\n        for val in labels[key]:\n            patterns = [nlp(text) for text in list_entity(paths[val])] \n            matcher.add(val, None, *patterns) \n\ndef remove_entities(labels):    # remove the list of entites\n    for key in labels.keys():\n        for val in labels[key]:\n            matcher.remove(val) \n    \ndef check_existance(par,where_ind,doc):  # check if any entity exist on the par, output: give the dict with key equll to entity and value equll to 1 if it exist \n    dict_list=defaultdict(list)\n    st=LancasterStemmer()\n    for key in where_ind:\n        for val in where_ind[key]:\n            stem_par=[st.stem(word) for word in word_tokenize(par)]\n            if st.stem(str(doc[val[0]:val[1]])) in stem_par:\n                dict_list[key]=1\n    return dict_list  \n\ndef prefrom_or(dict_list,labels):  \n    exist=0\n    for val in labels['sides']:\n            if dict_list[val]==1:\n                exist=1\n    return exist  \n\n\n\n\ndef print_title_summary(titel_main,all_sent,publish_time,nlp_question):\n    \n    unique_titles = list(set(titel_main))\n    scores=[]\n    all_titles=[]\n    all_text=[]\n    out_put=pd.DataFrame(columns=['title','publish_time','text','scores'])\n    \n    for title in unique_titles:\n        indices = [i for i, x in enumerate(titel_main) if x == title]\n        text = []\n        time=[]\n        if indices: \n            for ind in indices:\n                text.append(all_sent[ind])\n                combined_text = ' '.join(text)\n                time.append(publish_time[ind])\n            \n            score = nlp_question.similarity(nlp(combined_text))\n            out_put=out_put.append({'title':title,'publish_time':time,'text':combined_text,'scores':score}, ignore_index=True)\n\n \n    out_put=out_put.sort_values(by=['scores'],ascending=False)\n    #for row in out_put.iterrows():\n    #    display(HTML('<b>'+row[1]['title']+'<\/b> : <i>'+row[1]['text']+'<\/i>, ')) \n        \n    return out_put\n    #display(HTML('<b>'+title+'<\/b> : <i>'+combined_text+'<\/i>, '))     \n            ","eda663ee":"\ndef sent_vectorizer(sent, model):\n    sent_vec =[]\n    numw = 0\n    for w in sent:\n        try:\n            if numw == 0:\n                sent_vec = model[w]\n            else:\n                sent_vec = np.add(sent_vec, model[w])\n            numw += 1\n        except:\n            pass\n     \n    return np.asarray(sent_vec) \/ numw\n\n\ndef sent2words(all_sent):\n    sent_as_words = []\n    #if all_sent is list():\n    for s in all_sent:\n        sent_as_words.append(s.split())\n            \n    #else:\n    #    sent_as_words=all_sent.split()\n    \n    return sent_as_words\n\n\ndef sent_embedding(solution,all_sent):\n    if solution== 'bert':\n#            all_sent_list=  sent_tokenize(all_sent)      \n        client = BertClient()\n        embadded_vec = client.encode(all_sent) \n    else:\n\n        sent_as_words = sent2words(all_sent)    \n        model = Word2Vec(sent_as_words, min_count=1)\n        embadded_vec=[]\n        for sentence in sent_as_words:\n            embadded_vec.append(sent_vectorizer(sentence, model))   \n\n    return embadded_vec\n\ndef cluster_alg(NUM_CLUSTERS,embadded_vec):\n    \n    kclusterer = KMeansClusterer(NUM_CLUSTERS, distance=nltk.cluster.util.cosine_distance, repeats=25)\n    assigned_clusters = kclusterer.cluster(embadded_vec, assign_clusters=True)\n    \n    return assigned_clusters\n\ndef post_processing_bert(all_sent,ratio):\n    NUM_CLUSTERS = 3\n    embadded_vec = sent_embedding('bert',all_sent)  # embedding the sent based on solution\n    assigned_clusters = cluster_alg(NUM_CLUSTERS,embadded_vec)\n    #display(HTML('<b>'+'Highlights'+'<\/b>'))\n    summary=cluster_summry(sent2words(all_sent),NUM_CLUSTERS,assigned_clusters,ratio)\n    return summary","6221418c":"#spacy.load('en_core_web_sm')\ndef cluster_summry(sent_as_words,NUM_CLUSTERS,assigned_clusters,ratio):\n    st=LancasterStemmer()\n\n    summary_dataframe=pd.DataFrame(columns=['keyword','summary'])\n    summary_par = []\n    keys_max=[]\n    for c in range(NUM_CLUSTERS):\n        sent_in_cluster = []\n        for j in range(len(sent_as_words)):\n            if (assigned_clusters[j] == c):\n          \n                sent_in_cluster.append(' '.join(sent_as_words[j]))\n        if len(' '.join(sent_in_cluster)) > ratio :    \n            summary_par = summarize(' '.join(sent_in_cluster), word_count = ratio)\n            \n        else: \n            summary_par = ' '.join(sent_in_cluster)\n\n\n        j=0\n        keyword_intia = keywords(' '.join(sent_in_cluster)).split('\\n')[0]\n        while st.stem(keyword_intia)  in keys_max:\n            j+=1 \n            keyword_intia=keywords(' '.join(sent_in_cluster)).split('\\n')[j]\n            \n        keys_max.append(st.stem(keyword_intia))\n        \n        \n        \n        summary_dataframe=summary_dataframe.append({'keyword':keyword_intia,'summary':summary_par}, ignore_index=True)\n    \n    return summary_dataframe\n                  \n","5fb08f26":"import sys\n\nif not sys.warnoptions:\n    import warnings\n    warnings.simplefilter(\"ignore\")\n    \nimport random    \n\n\ndef search_task(task, df_covid, main_ent,questions,q):\n\n    all_sent = []\n    titles = []\n    publish_time=[]\n    labels,paths = entity_list(task, main_ent)\n    if labels:\n        df_reduced=df_covid[functools.reduce(lambda a, b: b|a, (df_covid['Text'].str.contains(s) for s in list_entity(paths[main_ent[f\"main_{task}\"]])))]\n        # to reduce runtime, randomly sample half of the data if rows > 2000\n        #if len(df_reduced) > 200:\n        #    df_reduced = df_reduced.sample(frac=0.1, replace=True, random_state=1)\n        #print(df_reduced.shape)\n\n        pbar = tqdm()\n        pbar.reset(total=len(df_reduced)) \n        add_entities(labels,paths)   # add the entity to the exiting model\n    \n        for row in df_reduced.iterrows():  # go through body of each paper in dataframe\n            pbar.update()\n            doc = nlp(row[1]['Text'])         \n            matches = matcher(doc)    # tag the predefined entities\n            rule_id = []\n            where_ind = defaultdict(list)\n        \n            for match_id, start, end in matches:\n                rule = nlp.vocab.strings[match_id]\n                nlp.max_length = 206000000\n                rule_id.append(rule)  # get the unicode ID, i.e. 'COLOR'\n                where_ind[rule].append((start,end))\n            exist=0    \n            for st in labels['sides']:\n                if st in rule_id:\n                    exist=1\n                \n            if labels['main'][0] in rule_id and exist:    # check the paper talk about at the first main topic\n                for par in doc.sents:\n                    dict_list = check_existance(par.text,where_ind,doc)\n                \n                    if dict_list[labels['main'][0]] == 1: \n                    \n                        if prefrom_or(dict_list,labels)==1:      # check if the par has the combination of entities\n                            all_sent.append(par.text)  # all senteces\n                            titles.append(row[1]['Title'])\n                            publish_time.append(row[1]['publish_time'])\n                         \n                            #display(HTML('<b>'+row[1]['title']+'<\/b> : <i>'+par.text+'<\/i>, '))  # print the related part of paper \n                \n        #display(HTML('<b>'+questions[task-1]+'<\/b>' ))                 \n        \n        if all_sent:\n            nlp_question = nlp(questions[task-1])\n            score_papers=print_title_summary(titles,all_sent,publish_time,nlp_question)\n            #print('csv out',task)\n            score_papers.to_csv(output_dir + f\"papers_subtask_{task}.csv\")\n            \n            #summary=post_processing_bert(all_sent,100)\n            #summary.to_csv(output_dir + f\"summary_subtask_{task}.csv\")\n        else:\n            #print('no all sent - task',task)\n            score_papers=[]\n            #remove_entities(labels)     # remove the existing entities\n        \n    #q.put((score_papers,task))\n    #return(score_papers,task)\n     \n    res = 'Process worker ' + str(q)\n    print(\"Worker finish job\",q)\n    q.put(res)\n    return res","7053f76d":"import multiprocessing as mp\nimport time\n\nmatcher = PhraseMatcher(nlp.vocab)  \n\ndef listener(q):\n    \"\"\"listens for messages on the q, writes to file. \"\"\"\n    print(\"start listener\")\n    while 1:\n        m = q.get()\n        print(\"listener get message: {}\".format(m))\n        if m == None:\n            print(\"listener get kill message\")\n            break\n\ndef main():\n    #must use Manager queue here, or will not work\n    manager = mp.Manager()\n    q = manager.Queue()    \n    pool = mp.Pool(mp.cpu_count()+2)\n    #put listener to work first\n    watcher = pool.apply_async(listener, (q,))\n    \n    pbar = tqdm()\n    pbar.reset(total=len(range(1,11))) \n    #fire off workers\n    jobs = []\n\n    for task in range(1,11):\n        print('processing task', task)\n        pbar.update()\n        job=pool.apply_async(search_task,(task,df_covid, main_ent,questions,q) )\n        jobs.append(job)\n\n    # collect results from the workers through the pool result queue\n    for job in jobs:\n        #print('Get job -',job)\n        job.get()\n        \n    #now we are done, kill the listener\n    q.put(None)\n    #q.task_done\n    pool.close()\n    pool.join()\n    \nif __name__ == \"__main__\":\n   main()","53680198":"#nlp = spacy.load(\"en\")\n#matcher = PhraseMatcher(nlp.vocab)  \n#for task in range(1,11):\n#    print('processing task', task)\n#    search_task(task,df_covid, main_ent,questions)","12bb169b":"\ndef print_output(path):\n    papers=pd.read_csv(path)\n    df=papers.drop_duplicates()   \n    df=df.dropna()\n    df=df.drop(['Unnamed: 0'], axis=1)\n\n\n    time=[]\n    for i in range(len(df)):\n        if df.iloc[i]['publish_time'][1:4]=='nan':\n            time.append('nan')\n        else:    \n            time.append(df.loc[i]['publish_time'][2:12])\n\n    df['publish time']=time\n    df=df.drop(['publish_time'], axis=1)\n    display(HTML(df.to_html()))","72c13e8a":"from PIL import Image","fe2b439f":"Image.open('\/kaggle\/input\/ericsson-task10-img\/T10_Task1.png')","af4711c5":"if os.path.exists(output_dir + 'papers_subtask_1.csv'):\n    print_output(output_dir + \"papers_subtask_1.csv\")\nelse: \n    display(HTML('<b>'+\"No related article is found\"+'<\/b>' )) ","73f8573a":"Image.open('\/kaggle\/input\/ericsson-task10-img\/T10_Task2.png')","5fdba71e":"if os.path.exists(output_dir + 'papers_subtask_2.csv'):\n    print_output(output_dir + \"papers_subtask_2.csv\")\nelse: \n    display(HTML('<b>'+\"No related article is found\"+'<\/b>' )) ","a8024dc4":"Image.open('\/kaggle\/input\/ericsson-task10-img\/T10_Task3.png')","dbe0f0b1":"if os.path.exists(output_dir + 'papers_subtask_3.csv'):\n    print_output(output_dir + \"papers_subtask_3.csv\")\nelse: \n    display(HTML('<b>'+\"No related article is found\"+'<\/b>' )) ","7884ec7b":"Image.open('\/kaggle\/input\/ericsson-task10-img\/T10_Task4.png')","c6f7e8e5":"if os.path.exists(output_dir + 'papers_subtask_4.csv'):\n    print_output(output_dir + \"papers_subtask_4.csv\")\nelse: \n    display(HTML('<b>'+\"No related article is found\"+'<\/b>' )) ","7d09c034":"Image.open('\/kaggle\/input\/ericsson-task10-img\/T10_Task5.png')","659b22b8":"if os.path.exists(output_dir + 'papers_subtask_5.csv'):\n    print_output(output_dir + \"papers_subtask_5.csv\")\nelse: \n    display(HTML('<b>'+\"No related article is found\"+'<\/b>' )) ","6a5859a7":"Image.open('\/kaggle\/input\/ericsson-task10-img\/T10_Task6.png')","15283a68":"if os.path.exists(output_dir + 'papers_subtask_6.csv'):\n    print_output(output_dir + \"papers_subtask_6.csv\")\nelse: \n    display(HTML('<b>'+\"No related article is found\"+'<\/b>' )) ","1f2d9c67":"Image.open('\/kaggle\/input\/ericsson-task10-img\/T10_Task7.png')","1204c294":"if os.path.exists(output_dir + 'papers_subtask_7.csv'):\n    print_output(output_dir + \"papers_subtask_7.csv\")\nelse: \n    display(HTML('<b>'+\"No related article is found\"+'<\/b>' )) ","a076d7fe":"Image.open('\/kaggle\/input\/ericsson-task10-img\/T10_Task8.png')","cdfc5114":"if os.path.exists(output_dir + 'papers_subtask_8.csv'):\n    print_output(output_dir + \"papers_subtask_8.csv\")\nelse: \n    display(HTML('<b>'+\"No related article is found\"+'<\/b>' )) ","33baa2d4":"Image.open('\/kaggle\/input\/ericsson-task10-img\/T10_Task9.png')","db995a86":"if os.path.exists(output_dir + 'papers_subtask_9.csv'):\n    print_output(output_dir + \"papers_subtask_9.csv\")\nelse: \n    display(HTML('<b>'+\"No related article is found\"+'<\/b>' )) ","64f18b13":"Image.open('\/kaggle\/input\/ericsson-task10-img\/T10_Task10.png')","15dd0406":"if os.path.exists(output_dir + 'papers_subtask_10.csv'):\n    print_output(output_dir + \"papers_subtask_10.csv\")\nelse: \n    display(HTML('<b>'+\"No related article is found\"+'<\/b>' )) ","d1aa443a":"### 4.2.2 Extracting and Formatting the Data for all Papers in the Dataset\n\nWe can now load, extract, and reformat the data for the papers into a format suitable for further analysis. The raw data for the CORD-19 Research Challenge is accessible on the Kaggle site\nat the following path:  \"\/kaggle\/input\/CORD-19-research-challenge\". \n\nIn addition, for storing the formatted data, we will use \"\/kaggle\/working\/\".\n\n#### 4.2.2.1 Loading the Metadata\n\nThe *metada.csv* file, located in the **Input** directory, contains useful summary information about each article in the dataset. The information provided for each paper includes (but is not limited to) the following:\n\n* title\n* license under which the paper is published\n* abstract text\n* publication date\n* authors\n* publishing journal\n* whether the data includes the full text of the paper\n\nWe use metadata to extract some additional information not found in the JSON files for each paper. The follow code reads in the *metadata.csv* file and converts it into a *pandas* DataFrame for further processing.","5438e414":"### 4.1.2 Defining Useful Helper Functions\n\nThe following two helper functions are used as part of the code.\n\n```log_progress()``` uses the *ipwidgets* Python package to create and display a progress bar which gives the user running the notebook an indication of the progress of code execution for certain portions of the notebook where this function is called.\n\n\n```process_text()``` uses the *spaCy* package natural language processing capabilities to clean and lemmatize an input text string.  The function creates a spaCy \"Doc\" object from the input string, which is a list of tokens that break down the string into its constituent parts; e.g., individual words, spaces, punctuation marks, and lemmas corresponding to each word. It then runs through the tokens, screens out punctuation, stop words, and pronouns, and returns a list of the lemmas corresponding to each remaining word.","cbb8c745":"At this point, we want to save our combined DataFrame as a .csv file. We also want to delete the variables we no longer need to reclaim memory space before continuing code execution.","dd90d419":"## <font color=black>Sub Task 10.9: Approaches to evaluate risk for enhanced disease after vaccination\n.<\/font>\n***","71e21478":"### 4.3.4 Identifying and Removing Papers whose Primary Language is not English\n\nIt is possible that the dataset at this point contains papers that are not in English. As a final cleaning step, we want to remove from the dataset papers whose primary language is not English. This ensures that the subsequent machine learning techniques we employ to help answer the questions for Task 1 are performed on a dataset that is as clean as possible.","a46b9472":"## 4.2 Acquiring and Preprocessing the Dataset\n\nIn this step, downloading the CORD-19 dataset and extracting and formatting the data are explained.\n\n### 4.2.1 Downloading the CORD-19 Dataset from Kaggle\n\n**ATTENTION**\nThe need to download the CORD-19 dataset from Kaggle is applicable only if this notebook is downloaded and run outside of Kaggle. If this is the case, follow the instructions on the Kaggle site. \n\nIf wanting to write code for downloading the dataset, the following steps can be used as guidance:\n1. Import the Kaggle API\n2. Set the local directory where you want to download the dataset\n3. Set the Kaggle account credentials, and establish an authenticated Kaggle API instance.\n4. Download the CORD-19 Research Challenge dataset using the Kaggle API method","15a9b33c":"### <font color=blue><b>Support Functions<\/b>\n***","b616ee07":"#### 4.3.3.1 Identifying and Removing Papers with No Body Text\n\nThe first step is straightforward - we simply use the *pandas* ```.dropna()``` method to drop any papers that have a null value in the \"Text\" column (paper full text) of our DataFrame. While there are no research paper without any text in this current library, there is a possibility that it will exist in a larger library. We will drop them as they yield limited information.","cdc2ed4f":"#### 4.3.3.2 Removing Papers that Do Not Appear to be Relevant to the Corpus as a Whole\n\nTo identify papers do not appear to be relevant to the corpus as a whole - i.e., research on COVID-19 and related coronaviruses - we perform TF-IDF analysis, using trigrams, across all the paper text. Once we have the corpus vocabulary (features) and TF-IDF scores, we use t-SNE dimensionality reduction to compare the TF-IDF paper scores and highlight papers with scores that significantly differ from those of the main body of papers. A significant difference in TF-IDF score indicates that the content of the paper is not that closely related to that of the main body of papers.\n\nThe code below uses the *scikit-learn* ```TfidfVectorizer``` class to extract trigrams (features) and and compute TF-IDF scores for the text of all the papers in the dataset. Since the number of features across 50,000+ papers becomes quite large (63+ million), the code then uses t-SNE dimensionality reduction to project the paper TD-IDF scores into a three dimensional space that can be visualized and in which outliers can be identified per standard criteria.","ba61d34b":"#### 4.3.2.1 Checkpoint 2","cb2b4ff2":"![KeywordLists.png](attachment:KeywordLists.png)","7c1663b2":"### 5.7 Subtask 10.7: Efforts to develop animal models and standardize challenge studies\n","d69695e3":"### 5.10 Subtask 10.10:  Assays to evaluate vaccine immune response and process development for vaccines, alongside suitable animal models (in conjunction with therapeutics)\n","6c7fb9d0":"\n## 3.2 Modeling\n\n\nUsing relevant articles for each question, we are using spaCy similarity to find out words close semantically. This is accomplished by finding similarity between word vector in the vector space. spaCy is one of the fastest natural language processing libraries used widely.\n\n\nSpaCy phrase matching, name entity recognition (NER) search categories specified entities in a text. The phrase matching finds phrases that match entities, this engine not only let you find words and phrases, it also gives the correlate document.\u00a0\n\n\n## 3.3 Post-procesing\n\n\nIn order to extract the relevant paragraph or sentence, we are using BERT word embedding and NLKT clustering. The cluster is going to start with an arbitrary k until allocate the close vector.\u00a0Gensim includes streamed parallelized implementations of fastText, non-negative matrix factorization (NMF), latent Dirichlet allocation (LDA), word2vec and doc2vec algorithms.\u00a0\n\n\nSpaCy phrase matching, name entity recognition (NER) search categories specified entities in a text. The phrase matching finds phrases that match entities, this engine not only let you find words and phrases, it also gives the correlate document.\u00a0\n\n\n## 3.4 Pros\n\n- Easy, the same program can be used for all subtask. \n- Accurancy, creation of specific key words helps to naildown pertinent papers.\u202f \n- Faster, independent researchers showed that spaCy offered the fastest syntactic parser in the world and that its accuracy is within 1%. \n\n## 3.5 Cons\u202f\n\n- Text summarization helps to allocate the exact parapragh. This helps us to identify that additonal improvement can be done. ","2f8b4ee9":"\n### 5.6 Subtask 10.6:  Efforts targeted at a universal coronavirus vaccine.\n","17023f74":"## Finding the list of papers related to Covid-19 \n","d9d7f64e":"#### 4.2.2.3 Extracting Key Data from each Paper\n\nNow, we want to extract relevant data from each of the papers. The next code block accomplishes this as outlined in the descriptive subsections below.\n\n##### Extract Title, Abstract, Text, and Paper ID Information for Each Paper\n\nNext, we extract the title, abstract, full text, and paper ID for each paper, across all 7 sub-directories. We take all the paper titles, abstracts, etc., and put them in separate lists. The result of this code is that we have separate lists (most of which are lists of strings) which contain:\n\n- the title of each paper \n- the abstract of each paper\n- the full text of each paper\n- the ID of each paper","26f0d0dc":"#  4. The Code\n\nThe major preprocessing we have performed includes:\n\n1. Preparation\n2. Acquiring and preprocessing the dataset\n3. Cleaning the dataset \n\n## 4.1 Preparation\n\nFor the preparation task, required software packages are imported and installed, and helper functions are defined to make the code more readable and efficient.\n\n### 4.1.1 Installing and Importing the Necessary Python Packages\n\nThe following code blocks install the necessary Python packages on the system, and import them into the Python environment.","eb576b48":"### 4.2.3 Checkpoint 1","4950d1e6":"## 4.3 Cleaning the Dataset\n\nNow that we have extracted the relevant raw data for all the papers and reformatted it into a single *pandas* DataFrame, we need to do some additional cleaning of the dataset. This includes:\n\n- removing unecessary or unhelpful characters and words\n- removing duplicate papers\n- making the country names uniform english names\n\n### 4.3.1 Cleaning the Paper Text Sections\n\nThe text extracted for the paper JSON files is quite dirty. The code below cleans the various text sections that we extracted from the raw data for each paper. Specifically, it:\n\n- fills holes (i.e., null values) in the DataFrame with the string, \"No Information\"\n- removes unnecessary garbage characters and white space\n- removes references and annotations (i.e., [1], (1), etc.)\n- removes \"figure X.X\" references \n\nNote that we are using the ```log_progress()``` helper function defined above to provide an indication of the progress of the execution.\n\nAdditionally for the abstract text of each paper, it removes unnecessary starting words, such as \"background\" or \"abstract\", and also counts the number of words in each abstract and adds a column to the \"test_df\" DataFrame which contains the number of words in the abstract for each paper.","ab81d401":"## <font color=black>Sub Task 10.4: Capabilities to discover a therapeutic (not vaccine) for the disease, and clinical effectiveness studies to discover therapeutics, to include antiviral agents.<\/font>\n***","0f7912e9":"## <font color=black>Sub Task 10.7:Efforts to develop animal models and standardize challenge studies.<\/font>\n***","7978e7bb":"### 5.2 Subtask 10.2:  Methods evaluating potential complication of Antibody-Dependent Enhancement (ADE) in vaccine recipients.\n","89355b0b":"\n### 5.9 Subtask 10.9:  Approaches to evaluate risk for enhanced disease after vaccination\n","183466ea":"# COVID-19 U.S. White House Challenge\n\n\n# 1. Introduction\n\nAt **Ericsson**, we are part of an extraordinary family. Not only do we have some of the best and brightest tech minds in the industry, but our teams are also committed to giving back and dedicated to making the world a better place. Therefore, in response to the call to action issued by the White House to the tech community in March 2019 to confront the COVID-19 pandemic, **Ericsson** has decided to partner with the National Institutes of Health, Georgetown University and the White House Office of Science and Technology Policy on their open-research dataset challenge.\n\nThe U.S. White House and a coalition of leading research groups have prepared the COVID-19 Open Research Dataset (CORD-19). The CORD-19 dataset represents the most extensive machine-readable coronavirus literature collection of over 47,000 scholarly articles about COVID-19, SARS-CoV-2, and related coronaviruses. However, the rapid increase in the volume and type of coronavirus literature makes it difficult for the medical community to keep up. Using data mining tools, we can help the medical community in developing answers to high priority scientific questions related to vaccines and therapeutics.\n\n## 1.1 Ericsson Team-10\n#### Our team was in charge of researching about the following COVID-19 related questions:\nWhat do we know about vaccines and therapeutics? \n\nWhat has been published concerning research and development and evaluation efforts of vaccines and therapeutics?\n\n## 1.2 Our Ericsson Family Contributors\n\nThe following **Team-10** members from the Ericsson family contributed to the development of this COVID-19 Challenge Jupyter Notebook:  \n\n    Bishoy Alphonse      Karanj Rupareliya      Oana-Gabriela Borcoci      Yamini Saragadam\n    Danlin Jin           Kiran Krishna Guda     Pramit Mehrotra            Yue Xin\n    Disha Goel           Madhava Bhamy          Robin Von\t \n    Jieneng Yang\t     Nanda Taliyakula       Rohit Rajput\t \n\nSpecial recognition and appreciation to the following **Team-10** Ericsson Family members whose diligence, self-motivation, and dedication to always go the extra mile to achieve the best possible results for this COVID-19 Challenge is truly remarkable:\n\n    Amanda Perez         Diego Martos           Jim Reno                    Ricardo Omana\n    Debasis Maity        Dimple Thomas          Jing Hu                     Serveh Shalmashi\n    Deidre Marshall      Emmet Moore            Melissa Hatfield            Sneha Wadhwa\n    Denis Shleifman      Forough Yaghoubi       Mukunda Prasad Jena         Venkata Snehith Reddy\n    Derrick Hernandez    Hernan Peniche         Omar Nushaiwat              Wilfredo Velez\n\n# 2. Notebook\u2019s Goal \n\nThe goal of this notebook is to help the medical community in finding answers to the questions below as it relates to COVID-19.   Using data science as well as data mining techniques, and tools, this notebook intends to assist the medical community in quickly finding the most relevant scholarly articles that could help answer these questions.\n* Effectiveness of drugs being developed and tried to treat COVID-19 patients.\n* Methods evaluating potential complications of Antibody-Dependent Enhancement (ADE) in vaccine recipients.\n* Exploration of the use of best animal models and their predictive value for a human vaccine.\n* Capabilities to discover a therapeutic (not vaccine) for the disease, and clinical effectiveness studies to discover therapeutics, to include antiviral agents.\n* Alternative models to aid decision makers in determining how to prioritize and distribute scarce, newly proven therapeutics as production ramps up. This could include identifying approaches for expanding production capacity to ensure equitable and timely distribution to populations in need.\n* Efforts targeted at a universal coronavirus vaccine.\n* Efforts to develop animal models and standardize challenge studies\n* Efforts to develop prophylaxis clinical studies and prioritize in healthcare workers\n* Approaches to evaluate risk for enhanced disease after vaccination\n* Assays to evaluate vaccine immune response and process development for vaccines, alongside suitable animal models (in conjunction with therapeutics)","49f40ffe":"# 5. Notebook Results \n\nThe following graphs shows the lists used per question and the number of articles that match the criteria. From that list, a clusterization of keywords that are interrelated produced the Text Examples for each Subtask.\n\n### 5.1 Subtask 10.1:  Effectiveness of drugs being developed and tried to treat COVID-19 patients: Clinical and bench trials to investigate less common viral inhibitors against COVID-19 such as naproxen, clarithromycin, and minocyclinethat that may exert effects on viral replication.\n","419a0056":"![T10_Methodology.png](attachment:T10_Methodology.png)","801e906d":"##### Creating a List of Datasets that Correspond to Raw Data Subdirectories\n\nEach article in the dataset is located in one of four subdirectories under the \"Raw Data\" directory, depending on how the article is licensed:\n- \"comm_use_subset\"\n- \"noncomm_use_subset\"\n- \"custom_license\"\n- \"biorxiv_medrxiv\"\n\nThe following code block reads in the papers from each subdirectory using the ``Paperset`` method from the COVID-19 Data Tools package. The detailed information for each paper is formatted into a Python dictionary, and the result of the ``Paperset`` method is a list of dictionaries, where each dictionary describes one paper in the subdirectory. In total then we create four lists of dictionaries - one list for the papers in each subdirectory [comm_use_subset; noncomm_use_subset; biorxiv_medrxiv; pmc_custom_license].","bcc6945f":"## <font color=black>Sub Task 10.8: Efforts to develop prophylaxis clinical studies and prioritize in healthcare workers<\/font>\n***","1f13fa4b":"## Text summarization","30e1e227":"### 4.3.3 Identifying and Removing Papers that are Outliers\n\nTo further clean the dataset, we want to remove any papers that would be classified as outliers. \nAn outlier would be a paper that:\n\n- has no text\n- has text that does not appear to be relevant to the corpus as a whole, based on TF-IDF scoring\n- has less than 150 words of text\n\nThe following code identifies and removes outlier papers based on these criteria.\n\nThe first step is easy - we simply use the *pandas* ```.dropna()``` method to drop any papers that have a null value in the \"Text\" column (paper full text) of our DataFrame.","c42cc307":"#### 4.3.3.3 Removing Papers with Less Than 150 Words \n\nThe last step in elimination outliers from the dataset is to identify papers containing less than 150 words. The minimum of 150 words has been derived using numerous rounds of experimentation. It also supports the minimum input requirements to generate a smart summary for a paper as part of our results.\n\nThe code below creates a word count for each paper and adds the list of word counts as a new column in the DataFrame. It then identifies those papers that contain less than 150 words of text.","40ea8f32":"# 6. References\n\n(1) https:\/\/spacy.io\/\n\n(2) https:\/\/www.kaggle.com\/allen-institute-for-ai\/CORD-19-research-challenge\n\n(3) https:\/\/www.nltk.org\/\n\n(4) https:\/\/scikit-learn.org\/\n\n(5) https:\/\/pypi.org\/project\/gensim\/\n\n(6) https:\/\/github.com\/tqdm\/tqdm\n\n(7) https:\/\/matplotlib.org\/\n\n(8) https:\/\/catalog.data.gov\/dataset\n\n(9) https:\/\/meshb.nlm.nih.gov\/","0e55a019":"### 4.3.2 Removing Duplicate Papers\n\nIf we examine some characteristics of the \"Abstract\" column of the \"test_df\" DataFrame, we notice that there appear to be a significant number (~700+) of duplicate abstracts in the data set. One possible reason for this is that a given paper may have been published in more than one journal. We obviously do not want such duplicates. The following code looks for duplicated papers, indicated either by duplicate abstract text or duplicate full text, and removes the corresponding rows from the DataFrame. In the output below we see that dropping the apparent duplicates has reduced the number of non-unique text entries in the DataFrame to less than a hundred.","f898f4a4":"### 5.5 Subtask 10.5:  Alternative models to aid decision makers in determining how to prioritize and distribute scarce, newly proven therapeutics as production ramps up. This could include identifying approaches for expanding production capacity to ensure equitable and timely distribution to populations in need.\n","4492d5b8":"## <font color=black>Sub Task 10.5:Alternative models to aid decision makers in determining how to prioritize and distribute scarce, newly proven therapeutics as production ramps up. This could include identifying approaches for expanding production capacity to ensure equitable and timely distribution to populations in need.<\/font>\n***","e9dabeb3":"Output of the following function is a dict where each key is the predefined entities and its value showing if that entity appears on par(paragraph or sentecnes)                   \n\n","8c1bbb93":"\n### 5.4 Subtask 10.4:  Capabilities to discover a therapeutic (not vaccine) for the disease, and clinical effectiveness studies to discover therapeutics, to include antiviral agents.\n","fec799ea":"#### Extracting Relevant Papers based on Date\n\nIn order to save memory, we include the option here to filter for papers published after a specified date. The function below, ```is_date()```, takes a string as input and uses the *dateutil* package to see if the string can be interpreted as a date. The function returns \"True\" if it can; \"False\" otherwise.\n\nThe code block following the ```is_date()``` function then takes a date as a string input (our default value is \"2019-12-01\"), and then identifies the IDs of from the metadata of those papers that were published after this date. To set a specific date, simply modify the \"filter_date\" variable appropriately, or enter a non-valid date string to disable this filtering altogether. The result of this code block, \"paper_id_list\", is the list of IDs for papers published after the specified date (or all papers if the filtering is disabled).","46437a53":"## <font color=black>Sub Task 10.3:Exploration of use of best animal models and their predictive value for a human vaccine.<\/font>\n***","5c389c03":"## <font color=black>Sub Task 10.2: Methods evaluating potential complication of Antibody-Dependent Enhancement (ADE) in vaccine recipients.<\/font>\n***","9355b307":"## Creating the name entities based phrase matching","c7f252cb":"Taking a look at the text of the papers with less than 150 words, we can see two things. Firstly, these papers contain little to no information that is useful to provide context or insights to medical researchers. Secondly, some of these papers have non-English content. We remove these papers from the data set, creating a new DataFrame, \"super_cleaned_df\".","5849a3f6":"### 5.8 Subtask 10.8:  Efforts to develop prophylaxis clinical studies and prioritize in healthcare workers\n","c7ab3ac5":"## <font color=black>Sub Task 10.6:Efforts targeted at a universal coronavirus vaccine.<\/font>\n***","24a6bedb":"The three-dimensional graph above shows a representation of the TD-IDF paper scores after t-SNE dimensionality reduction. We see that most of the papers are clustered together, with handful of papers exhibiting scores significantly outside the main cluster. This handful of papers are outliers that we want to remove from the dataset.\n\nTo more quantitatively identify the outlier papers, we use a standard 95% confidence interval criterion. Specifically, we select those papers with scores more than 2.5 standard deviations (this value is selected based on experimentation) away from the mean of the main cluster in any of the three axes.\n\nFrom the printout, we are able to see the number of outlier papers and its contents.","1572a4b9":"![T10_Banner.png](attachment:T10_Banner.png)","52ceb8e0":"### 4.3.4.1 Checkpoint 4\n\nBefore continuing, we again save our current version of the \"english_df\" DataFrame as \"\/kaggle\/working\/Checkpoint_4.csv\", and perform a memory cleanup.","d658d2e1":"From the output below, we see that the metadata file has information on 51078 papers, but the raw data in the subdirectories includes JSON files for only 3890 papers. Thus, not all the papers in the metadata file are present in the raw data. For our analysis, we will use the papers that have JSON files, as this should provide more contextual understanding and answers to the researchers.\n\nIterating through all the seven available datasets and extracting JSON information into list format in preparation for conversion into DataFrame.","783c96b2":"### <font color=blue><b>Load clean dataset<\/b>\n***","b9fa89ee":"# 7. Acknowledgments\n\n\nAlice Omana, Regulatory Affairs Pharmacist<br>\nhttps:\/\/www.linkedin.com\/in\/alice-oma%C3%B1a-28659938\n\n\nEric Murillo, PhD, Full Professor at School of Medicine in Mexico<br>\nhttps:\/\/www.linkedin.com\/in\/eric-murillo-rodriguez-phd-a7358445","68dc6cbc":"## <font color=black>Sub Task 10.1: Effectiveness of drugs being developed and tried to treat COVID-19 patients: Clinical and bench trials to investigate less common viral inhibitors against COVID-19 such as naproxen, clarithromycin, and minocyclinethat that may exert effects on viral replication.<\/font>\n***","00a43be5":"## <font color=black>Sub Task 10.10: Assays to evaluate vaccine immune response and process development for vaccines, alongside suitable animal models in conjunction with therapeutics<\/font>\n***","8702a7f9":"## Creating the name entities based on the question\n\n\nThe model is based on the Name Entity Recognition (NER), the details can be found in: http:\/\/urszulaczerwinska.github.io\/works\/egg_ner.\n\nThe tools that has been used in based on phrase matching in Spacy: https:\/\/spacy.io\/usage\/rule-based-matching.\n   \nFor each question the list of entity is gathered from csv file and create the label and path to that label as dictionary.","95e621d1":"#### 4.3.3.4 Checkpoint 3","233b12bd":"#### 4.2.2.2 Creating the Preprocessed Dataset \n\nFrom the output of the metadata_information.shape method shown below, we see that the metadata file has information on 51078 papers, but the raw data in the subdirectories includes JSON files for 59,311 papers. Thus, not all the papers in raw data are present in the metadata file. For our analysis, we will use the papers that have JSON files, as this should provide more contextual understanding and answers for researchers.\n**Note**: 51,078 indicates the number of rows in the resulting output file while 18 represents the number of columns. Each row represents a paper and each column represents the metadata associated with that paper.","7b9f8244":"\n### 5.3 Subtask 10.3:  Exploration of use of best animal models and their predictive value for a human vaccine.\n","cb02e787":"##### Load predefined reference lists","dc951652":"# 3. Methodology \n\nThe workflow in the figure above illustrates the approach taken pre-process, model, and post-process the cord-19 data set.\n\n\nThe first step is the pre-processing.  It involves basic data cleansing across the dataset, applying a proper selection of the paper\u2019s language, availability of full research document, and other basic elements.\nAt the core of the modeling step, a Natural Language Processing (NLP) library is found (1).  It provides the possibility to execute different tasks like clustering, summarization, phrase matching, a ranking of the given research papers.\n\n\nFinally, the selection of the most relevant documents is conducted considering the representation of presented words and sentences using a vector form.  This functionality provides a way to measure the semantic similarity of a given search sub-task.\n\n## 3.1 Pre-processing\n### 3.1.1 Dataset description\nEach paper in the dataset is represented by a JSON file, and is located in one of four subdirectories under the \"\/kaggle\/input\/CORD-19-research-challenge\" directory, depending on how the article is licensed:\n\n* \"comm_use_subset\"\n* \"noncomm_use_subset\"\n* \"custom_license\"\n* \"biorxiv_medrxiv\"\n\nFor each paper, we want to extract the following data:\n\n* paper ID\n* publication date\n* title\n* abstract text\n* body text\n* primary location and country for the author(s)\n\n### 3.1.2 Data cleaning \nThe following steps were considered for dataset cleaning:\n\n* Remove unnecessary or unhelpful characters and words from the paper text\n* Remove duplicate papers\n* Remove papers which are not in English\n* Eliminate null values \n* Remove blank space\n* Removes references and annotations\n\n### 3.1.3 Entity creation  \n\nA set of lists were prepared with keywords to reduce the dataset that will be used to identity the relevenat articles. \n\nThe following table shows the Keyword lists used with the number of keywords under each list:","180f3bbf":"## Post processing"}}