{"cell_type":{"4fe6fbbf":"code","9bb57583":"code","b8b74eb1":"code","346f55d6":"code","7b34bac5":"code","d0930d7b":"code","641a0676":"code","0289c61e":"code","e24232e6":"code","23c2aa3c":"code","a7617d22":"code","548f66e2":"code","423720f5":"code","eb8fd7ee":"code","c643c407":"code","042c89b1":"code","1f99582b":"code","145f3f9e":"code","60dd68d5":"markdown","25ad3a19":"markdown","c44ec4de":"markdown","6c06ab78":"markdown","7658dfa1":"markdown","3f55422e":"markdown"},"source":{"4fe6fbbf":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.preprocessing.image import load_img\nimport cv2\nimport random\n\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","9bb57583":"os.listdir(\"\/kaggle\/input\/face-mask-12k-images-dataset\/Face Mask Dataset\/Train\/\")","b8b74eb1":"print(\"Images in Train Dataset:\\n\")\nprint(\"Number of Images for with Mask category:{}\".format(len(os.listdir(\"\/kaggle\/input\/face-mask-12k-images-dataset\/Face Mask Dataset\/Train\/WithMask\"))))\nprint(\"Number of Images for with WithoutMask category:{}\".format(len(os.listdir(\"\/kaggle\/input\/face-mask-12k-images-dataset\/Face Mask Dataset\/Train\/WithoutMask\/\"))))\n","346f55d6":"train_dir = \"\/kaggle\/input\/face-mask-12k-images-dataset\/Face Mask Dataset\/Train\/\"\ntest_dir = \"\/kaggle\/input\/face-mask-12k-images-dataset\/Face Mask Dataset\/Test\/\"","7b34bac5":"#with Mask\nplt.figure(figsize=(12,7))\nfor i in range(5):\n    sample = random.choice(os.listdir(train_dir+\"WithMask\/\"))\n    plt.subplot(1,5,i+1)\n    img = load_img(train_dir+\"WithMask\/\"+sample)\n    plt.subplots_adjust(hspace=0.001)\n    plt.xlabel(\"With Mask\")\n    plt.imshow(img)\nplt.show()\n\n#without Mask\nplt.figure(figsize=(12,7))\nfor i in range(5):\n    sample = random.choice(os.listdir(train_dir+\"WithoutMask\/\"))\n    plt.subplot(1,5,i+1)\n    img = load_img(train_dir+\"WithoutMask\/\"+sample)\n    plt.subplots_adjust(hspace=0.001)\n    plt.xlabel(\"Without Mask\")\n    plt.imshow(img)\nplt.show()","d0930d7b":"height = 150\nwidth=150\ntrain_datagen = ImageDataGenerator(rescale=1.0\/255,validation_split=0.2,shear_range = 0.2,zoom_range=0.2,horizontal_flip=True)\ntrain = train_datagen.flow_from_directory(directory=train_dir,target_size=(height,width),\n                                          class_mode=\"categorical\",batch_size=32,subset = \"training\")\n\nvalid_datagen = ImageDataGenerator(rescale=1.0\/255)\n\nvalid = train_datagen.flow_from_directory(directory=train_dir,target_size=(height,width),\n                                          class_mode=\"categorical\",batch_size=32,subset=\"validation\")","641a0676":"from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2\n\nmobilenet = MobileNetV2(weights = \"imagenet\",include_top = False,input_shape=(150,150,3))","0289c61e":"for layer in mobilenet.layers:\n    layer.trainable = False","e24232e6":"from tensorflow.keras import Sequential\nfrom tensorflow.keras.layers import Flatten,Dense\nmodel = Sequential()\nmodel.add(mobilenet)\nmodel.add(Flatten())\nmodel.add(Dense(2,activation=\"sigmoid\"))","23c2aa3c":"model.summary()","a7617d22":"model.compile(optimizer=\"adam\",loss=\"categorical_crossentropy\",metrics =\"accuracy\")","548f66e2":"from tensorflow.keras.callbacks import ModelCheckpoint,EarlyStopping\ncheckpoint = ModelCheckpoint(\"moblenet_facemask.h5\",monitor=\"val_accuracy\",save_best_only=True,verbose=1)\nearlystop = EarlyStopping(monitor=\"val_acc\",patience=5,verbose=1)","423720f5":"history = model.fit_generator(generator=train,steps_per_epoch=len(train)\/\/ 32,validation_data=valid,\n                             validation_steps = len(valid)\/\/32,callbacks=[checkpoint,earlystop],epochs=15)","eb8fd7ee":"model.evaluate_generator(valid)","c643c407":"model.save(\"face_mask.h5\")\npred = model.predict_classes(valid)\npred[:15]","042c89b1":"mask = \"..\/input\/with-and-without-mask\/\"\nplt.figure(figsize=(8,7))\nlabel = {0:\"With Mask\",1:\"Without Mask\"}\ncolor_label = {0: (0,255,0),1 : (0,0,255)}\ncascade = cv2.CascadeClassifier(\"..\/input\/frontalface\/haarcascade_frontalface_default.xml\")\ncount = 0\ni = \"..\/input\/with-and-without-mask\/mask9.jpg\"\n\nframe =cv2.imread(i)\ngray = cv2.cvtColor(frame,cv2.COLOR_BGR2GRAY)\nfaces = cascade.detectMultiScale(gray,1.1,4)\nfor x,y,w,h in faces:\n    face_image = frame[y:y+h,x:x+w]\n    resize_img  = cv2.resize(face_image,(150,150))\n    normalized = resize_img\/255.0\n    reshape = np.reshape(normalized,(1,150,150,3))\n    reshape = np.vstack([reshape])\n    result = model.predict_classes(reshape)\n    \n    if result == 0:\n        cv2.rectangle(frame,(x,y),(x+w,y+h),color_label[0],3)\n        cv2.rectangle(frame,(x,y-50),(x+w,y),color_label[0],-1)\n        cv2.putText(frame,label[0],(x,y-10),cv2.FONT_HERSHEY_SIMPLEX,1,(255,255,255),2)\n        frame = cv2.cvtColor(frame,cv2.COLOR_BGR2RGB)\n        plt.imshow(frame)\n    elif result == 1:\n        cv2.rectangle(frame,(x,y),(x+w,y+h),color_label[1],3)\n        cv2.rectangle(frame,(x,y-50),(x+w,y),color_label[1],-1)\n        cv2.putText(frame,label[1],(x,y-10),cv2.FONT_HERSHEY_SIMPLEX,1,(255,255,255),2)\n        frame = cv2.cvtColor(frame,cv2.COLOR_BGR2RGB)\n        plt.imshow(frame)\n    #plt.imshow(frame)\nplt.show()\ncv2.destroyAllWindows()\n        ","1f99582b":"mask = \"..\/input\/with-and-without-mask\/\"\nplt.figure(figsize=(8,7))\nlabel = {0:\"With Mask\",1:\"Without Mask\"}\ncolor_label = {0: (0,255,0),1 : (0,0,255)}\ncascade = cv2.CascadeClassifier(\"..\/input\/frontalface\/haarcascade_frontalface_default.xml\")\ncount = 0\ni = \"..\/input\/with-and-without-mask\/mask1.jpg\"\n\nframe =cv2.imread(i)\ngray = cv2.cvtColor(frame,cv2.COLOR_BGR2GRAY)\nfaces = cascade.detectMultiScale(gray,1.1,4)\nfor x,y,w,h in faces:\n    face_image = frame[y:y+h,x:x+w]\n    resize_img  = cv2.resize(face_image,(150,150))\n    normalized = resize_img\/255.0\n    reshape = np.reshape(normalized,(1,150,150,3))\n    reshape = np.vstack([reshape])\n    result = model.predict_classes(reshape)\n    \n    if result == 0:\n        cv2.rectangle(frame,(x,y),(x+w,y+h),color_label[0],3)\n        cv2.rectangle(frame,(x,y-50),(x+w,y),color_label[0],-1)\n        cv2.putText(frame,label[0],(x,y-10),cv2.FONT_HERSHEY_SIMPLEX,1,(255,255,255),2)\n        frame = cv2.cvtColor(frame,cv2.COLOR_BGR2RGB)\n        plt.imshow(frame)\n    elif result == 1:\n        cv2.rectangle(frame,(x,y),(x+w,y+h),color_label[0],3)\n        cv2.rectangle(frame,(x,y-50),(x+w,y),color_label[0],-1)\n        cv2.putText(frame,label[1],(x,y-10),cv2.FONT_HERSHEY_SIMPLEX,1,(255,255,255),2)\n        frame = cv2.cvtColor(frame,cv2.COLOR_BGR2RGB)\n        plt.imshow(frame)\n    #plt.imshow(frame)\nplt.show()\ncv2.destroyAllWindows()\n        ","145f3f9e":"mask = \"..\/input\/with-and-without-mask\/\"\nplt.figure(figsize=(8,7))\nlabel = {0:\"With Mask\",1:\"Without Mask\"}\ncolor_label = {0: (0,255,0),1 : (0,0,255)}\ncascade = cv2.CascadeClassifier(\"..\/input\/frontalface\/haarcascade_frontalface_default.xml\")\ncount = 0\ni = \"..\/input\/with-and-without-mask\/mask8.jpg\"\n\nframe =cv2.imread(i)\ngray = cv2.cvtColor(frame,cv2.COLOR_BGR2GRAY)\nfaces = cascade.detectMultiScale(gray,1.1,4)\nfor x,y,w,h in faces:\n    face_image = frame[y:y+h,x:x+w]\n    resize_img  = cv2.resize(face_image,(150,150))\n    normalized = resize_img\/255.0\n    reshape = np.reshape(normalized,(1,150,150,3))\n    reshape = np.vstack([reshape])\n    result = model.predict_classes(reshape)\n    \n    if result == 0:\n        cv2.rectangle(frame,(x,y),(x+w,y+h),color_label[0],3)\n        cv2.rectangle(frame,(x,y-50),(x+w,y),color_label[0],-1)\n        cv2.putText(frame,label[0],(x,y-10),cv2.FONT_HERSHEY_SIMPLEX,1,(255,255,255),2)\n        frame = cv2.cvtColor(frame,cv2.COLOR_BGR2RGB)\n        plt.imshow(frame)\n    elif result == 1:\n        cv2.rectangle(frame,(x,y),(x+w,y+h),color_label[1],3)\n        cv2.rectangle(frame,(x,y-50),(x+w,y),color_label[1],-1)\n        cv2.putText(frame,label[1],(x,y-10),cv2.FONT_HERSHEY_SIMPLEX,1,(255,255,255),2)\n        frame = cv2.cvtColor(frame,cv2.COLOR_BGR2RGB)\n        plt.imshow(frame)\nplt.show()\ncv2.destroyAllWindows()\n        ","60dd68d5":"><h3>Model Building:<\/h3>","25ad3a19":"---\n\n<h1 style=\"text-align: center;font-size: 40px;\">Face Mask Detection using MobileNet & OpenCV<\/h1>\n\n---","c44ec4de":"><h3>Images:<\/h3>","6c06ab78":"<h3>Data Augmentation:<\/h3>","7658dfa1":"><h3>Checking Model's Performance:<\/h3>","3f55422e":"---\n\n<h1 style=\"text-align: center;font-size: 20px;\">Thanks for Reading!!<\/h1>\n\n---"}}