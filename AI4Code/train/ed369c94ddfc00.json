{"cell_type":{"39538379":"code","ce58e1f3":"code","6bd7c8ed":"code","4d2b475b":"code","717f7712":"code","00f759f3":"code","bf91b741":"code","32d8df66":"code","d6b9a74a":"code","27b32c59":"code","de958f24":"code","2c3f5c22":"code","b8e9df2a":"code","8eb9db0b":"code","e12d5b21":"code","b181d13a":"code","898f8bfe":"code","0c80cd8f":"code","a634bc50":"code","9ca15dd7":"code","786748fe":"code","d23f1797":"markdown","10f32d70":"markdown"},"source":{"39538379":"# This Python 3\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nfrom sklearn import preprocessing, metrics \nfrom sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\nfrom sklearn.feature_selection import RFECV\nfrom sklearn.ensemble import RandomForestClassifier\nimport xgboost as xgb\nfrom xgboost.sklearn import XGBClassifier","ce58e1f3":"train = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')","6bd7c8ed":"# Create full data set for convenient transformation\n# (drop Survived and PassengerId to reduce possible bias)\nfull = pd.concat([train.drop('Survived', axis=1), test], axis=0)\nfull.drop('PassengerId', axis=1, inplace=True)","4d2b475b":"# There are null entries!\nfull.isnull().sum()","717f7712":"# Impute Embarked and Fare with the mode and mean\nfull['Embarked'].fillna(full['Embarked'].mode(), inplace=True)\nfull['Fare'].fillna(full['Fare'].mean(), inplace = True)","00f759f3":"# Number of nulls correlates to survivial\n# Instead of imputing we can use this\ndef null_count(df):\n    return df[[\"Cabin\", \"Age\"]].apply(lambda x: x.isnull().astype(int)).sum(axis=1)\ntrain[\"nnull\"] = null_count(train)\nprint(train.groupby(\"nnull\").agg(({'PassengerId':'size', 'Survived':'mean'})))\nfull[\"nnull\"] = null_count(full) # Apply to full dataset","bf91b741":"# Cabin type (first letter in cabin) also correlates to survival\ndef cabin_type(df):\n    cab = df['Cabin'].astype(str).str[0] # this captures the letter\n    return cab.map(\n        {k: i for i, k in enumerate(cab.unique())})\ntrain[\"Cabin_type\"] = cabin_type(train)\n# this transforms the letters into numbers\nprint(train.groupby(\"Cabin_type\").agg(({'PassengerId':'size', 'Survived':'mean'})))\nfull[\"Cabin_type\"] = cabin_type(train)","32d8df66":"# We can drop no longer used columns\nfull.drop([\"Cabin\", \"Age\"], inplace=True, axis=1) # Drop replaced column\n# Now there are no more null\nfull.isnull().sum()","d6b9a74a":"# Titles are correlated to survival, but there are many types so we collapse titles to fewer categories\ndef extract_titles(df):\n    titles = {\n        \"Mr\" :         \"Mr\",\n        \"Mme\":         \"Mrs\",\n        \"Ms\":          \"Mrs\",\n        \"Mrs\" :        \"Mrs\",\n        \"Master\" :     \"Master\",\n        \"Mlle\":        \"Miss\",\n        \"Miss\" :       \"Miss\",\n        \"Capt\":        \"Officer\",\n        \"Col\":         \"Officer\",\n        \"Major\":       \"Officer\",\n        \"Dr\":          \"Officer\",\n        \"Rev\":         \"Officer\",\n        \"Jonkheer\":    \"Royalty\",\n        \"Don\":         \"Royalty\",\n        \"Sir\" :        \"Royalty\",\n        \"Countess\":    \"Royalty\",\n        \"Dona\":        \"Royalty\",\n        \"Lady\" :       \"Royalty\"\n    }\n    return df[\"Name\"].str.extract(' ([A-Za-z]+)\\.',expand=False).map(titles)\ntrain[\"title\"] = extract_titles(train)\n# this transforms the letters into numbers\nprint(train.groupby(\"title\")[[\"Survived\"]].mean())\nfull[\"title\"] = extract_titles(full)","27b32c59":"# Make a famliy size from parch and sibsp\nfull[\"Family_size\"] = full[[\"Parch\", \"SibSp\"]].sum(axis=1) + 1\nfull.drop([\"Parch\", \"SibSp\", 'Name', 'Ticket'], inplace=True, axis=1) # Drop useless columns","de958f24":"# Encode sex as 0 or 1\nlable_encoder = preprocessing.LabelEncoder()\nlable_encoder.fit(full[\"Sex\"])\nfull[\"Sex\"] = lable_encoder.transform(full[\"Sex\"])","2c3f5c22":"# Expand categoricals to dummy booleans\ndummies = pd.get_dummies(full, columns = [\"title\", 'nnull', 'Cabin_type', 'Embarked'])","b8e9df2a":"display(dummies.head())","8eb9db0b":"X = dummies[:len(train)]\nnew_X = dummies[len(train):]\ny = train.Survived\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size = .3, random_state = 1, stratify = y)","e12d5b21":"def grid_search(clf, grid, X, y, cv=10):\n    gs = GridSearchCV(\n        clf,\n        grid,\n        scoring='roc_auc',\n        iid=False,\n        verbose=1,\n        cv=cv)\n    gs.fit(X, y)\n    print(\"Params\", gs.best_params_)\n    print(\"Score\", gs.best_score_)\n    return gs","b181d13a":"xgbclf = XGBClassifier(\n    learning_rate =0.1,\n    n_estimators=1000,\n    max_depth=5,\n    min_child_weight=1,\n    gamma=0,\n    subsample=0.8,\n    colsample_bytree=0.8,\n    objective= 'binary:logistic',\n    scale_pos_weight=1,\n    seed=1)\n# Find max_depth and min_child_weight\ngs_1 = grid_search(\n    xgbclf,\n    {\n        'max_depth':range(3,10,1),\n        'min_child_weight':range(1,6,1)\n    },\n    X_train, y_train)","898f8bfe":"# Now find best gamma\ngs_2 = grid_search(\n    gs_1.best_estimator_,\n    {'gamma':[i*0.1 for i in range(0,5)]},\n    X_train, y_train)","0c80cd8f":"# Find subsample and colsample\ngs_3 = grid_search(\n    gs_2.best_estimator_,\n    {\n         'subsample':[i*0.1 for i in range(6,10)],\n         'colsample_bytree':[i*0.1 for i in range(6,10)]\n    },\n    X_train, y_train)","a634bc50":"# Find regularization parameter\ngs_4 = grid_search(\n    gs_3.best_estimator_,\n    {'reg_alpha':[1e-5, 1e-2, 0.1, 1, 100]},\n    X_train, y_train)","9ca15dd7":"params = gs_4.best_params_\nparams[\"learning_rate\"] = 0.01\nxgbclf = XGBClassifier(**params)\nxgbclf.fit(X_train, y_train)","786748fe":"xgb_pred = xgbclf.predict(new_X)\nsubmission = pd.concat([test.PassengerId, pd.DataFrame(xgb_pred)], axis = 'columns')\nsubmission.columns = [\"PassengerId\", \"Survived\"]\nsubmission.to_csv('titanic_submission.csv', header = True, index = False)","d23f1797":"# Data wrangling and feature engineering","10f32d70":"# Machine learning part"}}