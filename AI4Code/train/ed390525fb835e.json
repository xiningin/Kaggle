{"cell_type":{"bb151dd5":"code","c85a6d55":"code","6964d5e3":"code","a8257f90":"code","082c4d07":"code","7a51a088":"code","f2de39e7":"code","4439223a":"code","2a4e9d79":"code","fbd5a126":"code","45901bc8":"code","bd4575e1":"code","9bb676af":"code","b224ffbf":"code","2b75818a":"code","f18452a4":"code","286c630d":"code","8e7cdea7":"code","98284caf":"code","8548b23e":"code","6e5aaec2":"code","ccc35d68":"code","2db85cde":"code","84f24ccc":"code","2c045efd":"code","b86fb070":"code","42f36ba6":"code","033083d5":"code","30c07136":"code","f08cb53d":"code","1e0caada":"code","e126b831":"code","b4aee583":"code","099d2b60":"code","33c91f7f":"code","773ca708":"code","2ba63dd8":"code","22e8c6de":"code","938842cd":"code","bd0265a9":"code","3cc49ced":"code","dce123e3":"code","937f32ba":"code","f9ed596e":"code","b520b6e5":"markdown","cd202fb9":"markdown","bbd5a578":"markdown","19e572ec":"markdown","54f17aa8":"markdown","88a2195b":"markdown","6f7714d2":"markdown","a7b6ec92":"markdown","51e8bd50":"markdown","8f35eb3a":"markdown","033d487f":"markdown","13be1866":"markdown","099b2191":"markdown","dfe25654":"markdown","828210fd":"markdown","6a99f988":"markdown"},"source":{"bb151dd5":"%env SM_FRAMEWORK=tf.keras\n!pip install -q segmentation-models==\"1.0.1\"","c85a6d55":"%matplotlib inline\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport os, json, re, math\n\nfrom tqdm import tqdm\nfrom glob import glob\nimport gc\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils import class_weight\nfrom sklearn.metrics import classification_report\n\nimport cv2\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom IPython.display import display\n\nimport tifffile as tiff\n\nplt.rcParams[\"figure.figsize\"] = (12, 8)\nplt.rcParams['axes.titlesize'] = 16\n\nfrom kaggle_datasets import KaggleDatasets\n\nimport tensorflow as tf\nimport tensorflow.keras.layers as L\n\nfrom tensorflow.data import Dataset\nfrom tensorflow.keras.utils import get_custom_objects\n\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint, EarlyStopping\n\nfrom tensorflow import keras\ntf.keras.backend.set_image_data_format('channels_last')\n\nimport segmentation_models as sm\n\nprint(os.listdir('\/kaggle\/input\/'))\nprint(os.listdir('\/kaggle\/input\/hubmap-kidney-segmentation\/'))\n\nprint(\"Tensorflow version \" + tf.__version__)\n\nfrom time import time, strftime, gmtime\nstart = time()\nimport datetime\nprint(str(datetime.datetime.now()))","6964d5e3":"base_dir = '\/kaggle\/input\/hubmap-kidney-segmentation\/'\nBACKBONE = 'efficientnetb7'\n","a8257f90":"os.listdir(base_dir + 'train\/')","082c4d07":"print('Number of train images: ', len(os.listdir(base_dir + 'train\/')))","7a51a088":"train = pd.read_csv(base_dir + 'train.csv')\ntrain","f2de39e7":"meta = pd.read_csv(base_dir + 'HuBMAP-20-dataset_information.csv')\nmeta","4439223a":"meta.info()","2a4e9d79":"train.isnull().sum()","fbd5a126":"meta.isnull().sum()","45901bc8":"meta.fillna(meta.mean(), inplace = True)","bd4575e1":"with open(os.path.join(base_dir, 'train\/0486052bb-anatomical-structure.json')) as file:\n    ana = json.loads(file.read())\n    \nprint(json.dumps(ana))","9bb676af":"fig, ax = plt.subplots(1, 2)\n\nsns.distplot(meta['width_pixels'], ax = ax[0], kde = True, rug = True)\nax[0].axvline(np.mean(meta['width_pixels']), color = 'g', linestyle = '--')\nax[0].axvline(np.median(meta['width_pixels']), color = 'b', linestyle = '-')\nax[0].legend({'Median', 'Mean'})\n\nsns.distplot(meta['height_pixels'], ax = ax[1], kde = True, rug = True)\nax[1].axvline(np.mean(meta['height_pixels']), color = 'g', linestyle = '--')\nax[1].axvline(np.median(meta['height_pixels']), color = 'b', linestyle = '-')\nax[1].legend({'Median', 'Mean'})\n\nplt.suptitle('Distribution Plot of Pixel Width and Height')","b224ffbf":"fig, ax = plt.subplots(1, 2)\n\nsns.distplot(meta['bmi_kg\/m^2'], ax = ax[0], kde = True, rug = True)\nax[0].axvline(np.mean(meta['bmi_kg\/m^2']), color = 'g', linestyle = '--')\nax[0].axvline(np.median(meta['bmi_kg\/m^2']), color = 'b', linestyle = '-')\nax[0].legend({'Median', 'Mean'})\n\nsns.distplot(meta['age'], ax = ax[1], kde = True, rug = True)\nax[1].axvline(np.mean(meta['age']), color = 'g', linestyle = '--')\nax[1].axvline(np.median(meta['age']), color = 'b', linestyle = '-')\nax[1].legend({'Median', 'Mean'})\n\nplt.suptitle('Distribution Plot of BMI and Age')","2b75818a":"fig, ax = plt.subplots(1, 2)\n\nsns.distplot(meta['weight_kilograms'], ax = ax[0], kde = True, rug = True)\nax[0].axvline(np.mean(meta['weight_kilograms']), color = 'g', linestyle = '--')\nax[0].axvline(np.median(meta['weight_kilograms']), color = 'b', linestyle = '-')\nax[0].legend({'Median', 'Mean'})\n\nsns.distplot(meta['height_centimeters'], ax = ax[1], kde = True, rug = True)\nax[1].axvline(np.mean(meta['height_centimeters']), color = 'g', linestyle = '--')\nax[1].axvline(np.median(meta['height_centimeters']), color = 'b', linestyle = '-')\nax[1].legend({'Median', 'Mean'})\n\nplt.suptitle('Distribution Plot of Weight and Height')","f18452a4":"fig, ax = plt.subplots(1, 2)\n\nsns.countplot(meta['sex'], ax = ax[0])\nsns.countplot(meta['race'], ax = ax[1])\nplt.suptitle('Count Plot of Sex and Race')","286c630d":"fig, ax = plt.subplots(1, 2)\n\nsns.countplot(meta['ethnicity'], ax = ax[0])\nsns.countplot(meta['race'], ax = ax[1])\nplt.suptitle('Count Plot of Ethnicity and Race')","8e7cdea7":"fig, ax = plt.subplots(1, 2)\n\nsns.countplot(meta['sex'], ax = ax[0])\nsns.countplot(meta['laterality'], ax = ax[1])\nplt.suptitle('Count Plot of Sex and Laterality')","98284caf":"# https:\/\/www.kaggle.com\/paulorzp\/rle-functions-run-lenght-encode-decode\n\ndef mask2rle(img):\n    \n    '''\n    img: numpy array, 1 - mask, 0 - background\n    Returns run length as string formated\n    '''\n    pixels= img.T.flatten()\n    pixels = np.concatenate([[0], pixels, [0]])\n    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n    runs[1::2] -= runs[::2]\n    return ' '.join(str(x) for x in runs)\n \ndef rle2mask(mask_rle, shape):\n    \n    '''\n    mask_rle: run-length as string formated (start length)\n    shape: (width,height) of array to return \n    Returns numpy array, 1 - mask, 0 - background\n\n    '''\n    s = mask_rle.split()\n    starts, lengths = [np.asarray(x, dtype=int) for x in (s[0:][::2], s[1:][::2])]\n    starts -= 1\n    ends = starts + lengths\n    img = np.zeros(shape[0]*shape[1], dtype=np.uint8)\n    for lo, hi in zip(starts, ends):\n        img[lo:hi] = 1\n    return img.reshape(shape).T\n\ndef read_tiff(image, encoding_index, resize=None):\n    \n    '''\n    read tiff images and mask.\n    ----------------------------\n    \n    Arguments:\n    image -- tiff image\n    encoding_index -- corresponding tiff file encoding index.\n    \n    Returns:\n    tiff_image -- tiff image\n    tiff_mask -- segmentation mask\n    '''\n    \n    tiff_image = tiff.imread(os.path.join(base_dir, f'train\/{image}.tiff'))\n    \n    if len(tiff_image.shape) == 5:\n        tiff_image = np.transpose(tiff_image.squeeze(), (1,2,0))\n        \n    tiff_mask = rle2mask(train['encoding'][encoding_index],\n                         (tiff_image.shape[1], tiff_image.shape[0]))\n    \n    print(f'Image Shape: {tiff_image.shape}')\n    print(f'Mask Shape: {tiff_mask.shape}')\n    \n    if resize:\n        rescaled = (tiff_image.shape[1] \/\/ resize, tiff_image.shape[0] \/\/ resize)\n        tiff_image = cv2.resize(tiff_image, rescaled)\n        tiff_mask = cv2.resize(tiff_mask, rescaled)\n\n    return tiff_image, tiff_mask\n\ndef read_test_tiff(image, resize=None):\n    \n    '''\n    read tiff images.\n    ----------------------------\n    \n    Arguments:\n    image -- tiff image\n    \n    Returns:\n    tiff_image -- tiff image\n    tiff_mask -- segmentation mask\n    '''\n    \n    tiff_image = tiff.imread(os.path.join(base_dir, f'test\/{image}.tiff'))\n    \n    if len(tiff_image.shape) == 5:\n        tiff_image = np.transpose(tiff_image.squeeze(), (1,2,0))\n    \n    if resize:\n        rescaled = (tiff_image.shape[1] \/\/ resize, tiff_image.shape[0] \/\/ resize)\n        tiff_image = cv2.resize(tiff_image, rescaled)\n\n    return tiff_image\n\ndef plot(image, mask):\n    \n    '''\n    plot image and mask\n    ---------------------\n    \n    Arguments:\n    image -- tiff image \n    mask -- segmentation mask\n    \n    Returns:\n    matplotlib plot\n    '''\n    plt.figure(figsize = (15, 15))\n\n    # Image\n    plt.subplot(1, 3, 1)\n    plt.imshow(image)\n    plt.title(\"Image\", fontsize = 16)\n\n    # Mask\n    plt.subplot(1, 3, 2)\n    plt.imshow(mask)\n    plt.title(\"Image Mask\", fontsize = 16)\n\n    # Image + Mask\n    plt.subplot(1, 3, 3)\n    plt.imshow(image)\n    plt.imshow(mask, alpha=0.5)\n    plt.title(\"Image + Mask\", fontsize = 16);\n    \ndef plot_subset(image, mask, start_rh, end_rh, start_cw, end_cw):\n    \n    '''\n    plot image and mask\n    ---------------------\n    \n    Arguments:\n    image -- tiff image \n    mask -- segmentation mask\n    start_rh -- height start\n    end_rh -- height end\n    start_cw -- width start \n    end_cw -- width end\n    \n    Returns:\n    matplotlib plot\n    '''\n\n    # Figure size\n    plt.figure(figsize=(15, 15))\n\n    # subset image and mask\n    subset_image = image[start_rh:end_rh, start_cw:end_cw, :]\n    subset_mask = mask[start_rh:end_rh, start_cw:end_cw]\n\n    # Image\n    plt.subplot(1, 3, 1)\n    plt.imshow(subset_image)\n    plt.title(\"Zoomed Image\", fontsize=16)\n\n    # Mask\n    plt.subplot(1, 3, 2)\n    plt.imshow(subset_mask)\n    plt.title(\"Zoomed Image Mask\", fontsize=16)\n\n    # Image + Mask\n    plt.subplot(1, 3, 3)\n    plt.imshow(subset_image)\n    plt.imshow(subset_mask, alpha=0.5)\n    plt.title(\"Zoomed Image + Mask\", fontsize=16);","8548b23e":"glob(base_dir + 'train\/*.tiff'), glob(base_dir + 'test\/*.tiff') ","6e5aaec2":"img, msk = read_tiff('2f6ecfcdf', 0, resize = None)\ngc.collect()","ccc35d68":"#plot(img, msk)\ngc.collect()","2db85cde":"plot_subset(img, msk, 5000, 10000, 10000, 15000)\ngc.collect()","84f24ccc":"plot_subset(img, msk, 4000, 11000, 8000, 12000)\ngc.collect()","2c045efd":"try:\n    # detect and initialize tpu\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print('Connecting to tpu...')\n    print('device running at:', tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    print('Initializing TPU...')\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    # instantiate a distribution strategy\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n    print(\"TPU initialized\")\nelse:\n    print('Using deafualt strategy...')\n    strategy = tf.distribute.get_strategy()\n\nREPLICAS = strategy.num_replicas_in_sync\nprint(f\"REPLICAS:  {REPLICAS}\")","b86fb070":"AUTO = tf.data.experimental.AUTOTUNE\n\nSEED = 42\nBATCH_SIZE = 8 * REPLICAS # ideal batch size is 128 per core. (128*8=1024)\nIMAGE_DIM = [512, 512]\nEPOCHS = 60\nLR = 5e-4\n\nGCS_PATH = KaggleDatasets().get_gcs_path('hubmap-512x512')\nprint(GCS_PATH)\n\n#Append train and mask to GCS PATH\nTIFF = tf.io.gfile.glob(str(GCS_PATH + '\/train\/*'))\nMASK = tf.io.gfile.glob(str(GCS_PATH + '\/masks\/*'))","42f36ba6":"TRAIN_TIFF = Dataset.from_tensor_slices(TIFF)\nTRAIN_MASK = Dataset.from_tensor_slices(MASK)\n\nTIFF_COUNT = tf.data.experimental.cardinality(TRAIN_TIFF).numpy()\nMASK_COUNT = tf.data.experimental.cardinality(TRAIN_MASK).numpy()\n\nprint('Training Data')\nprint(f'Total Tiff Images: {TIFF_COUNT}')\nprint(f'Total Masks: {MASK_COUNT}')","033083d5":"for files in TRAIN_TIFF.take(5):\n    print(files.numpy())\nprint('\\n')\nfor files in TRAIN_MASK.take(5):\n    print(files.numpy())","30c07136":"def decode_image_and_mask(image, mask, augment = True):\n    \n    '''\n    decode image and mask in order to\n    feed data to TPU.\n    --------------------------------\n    \n    Arguments:\n    image -- patches of huge tiff file in png format.\n    mask -- patches of mask in png format.\n    augment -- apply augmentations on images and masks.\n    \n    Return:\n    image \n    mask\n    '''\n    \n    # load raw data as string\n    image = tf.io.read_file(image)\n    mask = tf.io.read_file(mask)\n    \n    image = tf.io.decode_png(image, channels = 3)  # convert compressed string to 3D uint8 tensor\n    mask = tf.io.decode_png(mask)  # convert compressed string to uinst8 tensor\n    \n    if augment:\n        \n        if tf.random.uniform(()) > 0.5:\n            image = tf.image.flip_left_right(image)\n            mask = tf.image.flip_left_right(mask)\n            \n        if tf.random.uniform(()) > 0.4:\n            image = tf.image.flip_up_down(image)\n            mask = tf.image.flip_up_down(mask)\n            \n        if tf.random.uniform(()) > 0.5:\n            image = tf.image.rot90(image, k=1)\n            mask = tf.image.rot90(mask, k=1)\n            \n        if tf.random.uniform(()) > 0.45:\n            image = tf.image.random_saturation(image, 0.7, 1.3)\n            \n        if tf.random.uniform(()) > 0.45:\n            image = tf.image.random_contrast(image, 0.8, 1.2)\n    \n    image = tf.image.convert_image_dtype(image, tf.float32) # convert to floats in the [0,1] range\n    mask = tf.cast(mask, tf.float32)  # convert to floats 1. and 0.\n    \n    image = tf.image.resize(image, [*IMAGE_DIM])\n    image = tf.reshape(image, [*IMAGE_DIM, 3])  # reshaping image tensor\n    \n    mask = tf.image.resize(mask, [*IMAGE_DIM])\n    mask = tf.reshape(mask, [*IMAGE_DIM]) # reshaping mask tensor\n    \n    return image, mask\n\ndef generate_dataset(tiff, masks, batch_size = 16, shuffle = True):\n    \n    '''\n    generate batches of tf.Dataset\n    object\n    --------------------------------\n    \n    Arguments:\n    tiff -- tf.data.Dataset object (tf.Tensor)\n    mask -- tf.data.Dataset object (tf.Tensor)\n    batch_size -- batches of image, mask pair\n    shuffle -- generate train if True or validation data if False\n    \n    Return:\n    dataset - tf.data.Dataset dataset \n    '''\n    \n    \n    dataset = Dataset.zip((tiff, masks)) # create dataset by zipping (image, mask) into pair\n    dataset = dataset.map(decode_image_and_mask, num_parallel_calls = AUTO) # decode raw data coming from GCS bucket to valid image, mask pair \n    dataset = dataset.cache() # cache dataset preprocessing work that doesn't fit in memory\n    dataset = dataset.repeat() \n    \n    # shuffle while training else set to False\n    if shuffle:\n        dataset = dataset.shuffle(buffer_size = 1000)\n        \n    dataset = dataset.batch(batch_size, drop_remainder = True) # generate batches of data\n    dataset = dataset.prefetch(buffer_size = AUTO) # fetch dataset while model is training\n    return dataset","f08cb53d":"train_dataset = generate_dataset(TRAIN_TIFF, TRAIN_MASK, batch_size = BATCH_SIZE)","1e0caada":"for img, msk in train_dataset.take(1):\n    image_batch, mask_batch = img, msk\n    print(\"Image shape: \", image_batch.numpy().shape)\n    print(\"Mask shape: \", mask_batch.numpy().shape)","e126b831":"plot(image_batch[0], mask_batch[0])","b4aee583":"plt.figure(figsize = (16,16))\nfor i,(img, mask) in enumerate(zip(image_batch[:64], mask_batch[:64])):\n    plt.subplot(8, 8, i + 1)\n    plt.imshow(img, vmin = 0, vmax = 255)\n    plt.imshow(mask, alpha = 0.4)\n    plt.axis('off')\n    plt.subplots_adjust(wspace = None, hspace = None)","099d2b60":"train_img, valid_img, train_msk, valid_msk = train_test_split(TIFF, MASK, test_size = 0.2, random_state = 42)\nprint(len(train_img), len(train_msk))\nprint(len(valid_img), len(valid_msk))","33c91f7f":"TRAIN_TIFF = Dataset.from_tensor_slices(train_img)\nTRAIN_MASK = Dataset.from_tensor_slices(train_msk)\n\nVALID_TIFF = Dataset.from_tensor_slices(valid_img)\nVALID_MASK = Dataset.from_tensor_slices(valid_msk)\n\nTRAIN_TIFF_CNT = tf.data.experimental.cardinality(TRAIN_TIFF).numpy()\nTRAIN_MASK_CNT = tf.data.experimental.cardinality(TRAIN_MASK).numpy()\n\nVALID_TIFF_CNT = tf.data.experimental.cardinality(VALID_TIFF).numpy()\nVALID_MASK_CNT = tf.data.experimental.cardinality(VALID_MASK).numpy()\n\nprint('Training Data Count')\nprint(f'Total Train Tiff Images: {TRAIN_TIFF_CNT}')\nprint(f'Total Train Masks: {TRAIN_MASK_CNT}')\n\nprint('Validation Data Count')\nprint(f'Total Valid Tiff Images: {VALID_TIFF_CNT}')\nprint(f'Total Valid Masks: {VALID_MASK_CNT}')","773ca708":"steps_per_epoch = tf.data.experimental.cardinality(TRAIN_TIFF).numpy() \/\/ BATCH_SIZE\nvalid_steps = tf.data.experimental.cardinality(VALID_TIFF).numpy() \/\/ BATCH_SIZE","2ba63dd8":"train_dataset = generate_dataset(TRAIN_TIFF, TRAIN_MASK, batch_size = BATCH_SIZE)\nvalid_dataset = generate_dataset(VALID_TIFF, VALID_MASK, batch_size = BATCH_SIZE)\ntrain_dataset, valid_dataset","22e8c6de":"with strategy.scope():\n    model = sm.Unet(BACKBONE)\n\noptimizer = 'adam'\nmodel.compile(optimizer = optimizer,\n              loss = tf.keras.losses.BinaryCrossentropy(),    \n              metrics = [sm.metrics.iou_score, 'accuracy'])\n\nearly = tf.keras.callbacks.EarlyStopping(monitor = 'val_iou_score', patience = 10, mode = 'max')\ncheck = tf.keras.callbacks.ModelCheckpoint(filepath = 'sm_unet.h5', monitor = 'val_iou_score', save_weights_only = True, \n                                       save_best_only = True, mode = 'max')\nreduce = tf.keras.callbacks.ReduceLROnPlateau(monitor = 'val_loss', factor = 0.1, patience = 10, \n                                              min_lr = 0.00001)\n#model.summary()","938842cd":"history = model.fit(train_dataset, \n                   epochs = EPOCHS, \n                   steps_per_epoch = steps_per_epoch, \n                   callbacks = [check, reduce], \n                   validation_data = valid_dataset, \n                   validation_steps = valid_steps, \n                   verbose = 1)","bd0265a9":"pd.DataFrame(history.history).plot(y = ['accuracy', 'val_accuracy'], logy = False)\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Accuracy\")","3cc49ced":"pd.DataFrame(history.history).plot(y = ['loss', 'val_loss'], logy = False)\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Loss\")","dce123e3":"pd.DataFrame(history.history).plot(y = ['iou_score', 'val_iou_score'], logy = False)\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"IOU_Score\")","937f32ba":"model.load_weights('sm_unet.h5')\nmodel.save('hubmap_sm.h5')","f9ed596e":"finish = time()\nprint(strftime(\"%H:%M:%S\", gmtime(finish - start)))","b520b6e5":"__Check image and mask size in the dataset__","cd202fb9":"- CPU Notebook <= 9 hours run-time\n- GPU Notebook <= 9 hours run-time\n- TPUs will not be available for making submissions, however TPUs can be used for model training\n- Internet access disabled - Can be used for Training by not for Inference\n- Submission file must be named \"submission.csv\"","bbd5a578":"__Competition Challenge__","19e572ec":"__Helper Functions__","54f17aa8":"__Any Missing values in train and meta data?__","88a2195b":"__Competition Rules__","6f7714d2":"The competition is evaluated on the mean Dice coefficient.","a7b6ec92":"- Taking care of Missing values","51e8bd50":"__Visualization__","8f35eb3a":"__Split dataset into train and validation sets__","033d487f":"__Model using Segemntation Models by Qubvel__","13be1866":"__Visualize images and masks__","099b2191":"__Checking one json file__","dfe25654":"__TPU Config__","828210fd":"The challenge is to detect functional tissue units (FTUs) across different tissue preparation pipelines. An FTU is defined as a \u201cthree-dimensional block of cells centered around a capillary, such that each cell in this block is within diffusion distance from any other cell in the same block\u201d (de Bono, 2013). The goal of this competition is the __implementation of a successful and robust glomeruli FTU detector.__","6a99f988":"__Competition Metric__"}}