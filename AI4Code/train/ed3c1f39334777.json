{"cell_type":{"7a3121a0":"code","4c4e5481":"code","268c7087":"code","1f764cf6":"code","f9498c9d":"code","8be33276":"code","cb176009":"markdown","bf14feb6":"markdown","c306079e":"markdown","4f86d13f":"markdown","25e1f40e":"markdown"},"source":{"7a3121a0":"from re import T\nimport pandas as pd\nfrom sklearn import model_selection\n\nif __name__ == \"__main__\":\n\n    # read the training data\n    df = pd.read_csv(\"..\/input\/imdb-dataset-of-50k-movie-reviews\/IMDB Dataset.csv\")\n\n    # map positive to 1 anf negative to 0\n    df.sentiment = df.sentiment.apply(\n        lambda x: 1 if x == \"positive\" else 0\n    )\n\n    # we create a new column called kfold and fill it with -1\n    df[\"kfold\"] = -1\n\n    # the next step is to randomize the rows of the data\n    df = df.sample(frac=1).reset_index(drop=True)\n\n    # fetch labels\n    y = df.sentiment.values\n\n    # init the kfold class from model_selection module\n    kf = model_selection.StratifiedKFold(n_splits=5)\n\n    # fill the new kfold column\n    for f, (t_, v_) in enumerate(kf.split(X=df, y=y)):\n        df.loc[v_, 'kfold'] = f\n\n    # save the new csv file with kfold column\n    df.to_csv(\"train_folds.csv\", index=False)\n","4c4e5481":"import torch\n\nclass IMDBdataset:\n\n    def __init__(self, reviews, targets):\n        \"\"\"\n        :param reviews: this is a numpy array\n        :param targets: a vector, numpy array\n        \"\"\"\n\n        self.reviews = reviews\n        self.targets = targets\n\n    def __len__(self):\n        # returns length of the dataset\n        return len(self.reviews)\n\n    def __getitem__(self, item):\n\n        # for any given item, which is an int,\n        # return review and targets as torch tensor\n        # item is the index of the item in concern\n\n        review = self.reviews[item, :]\n        target = self.targets[item]\n\n        return {\n            \"review\": torch.tensor(review, dtype=torch.long),\n            \"target\": torch.tensor(target, dtype=torch.float)\n        }","268c7087":"import torch\nimport torch.nn as nn\n\nclass LSTM(nn.Module):\n\n    def __init__(self, embedding_matrix):\n        \"\"\"\n        :param embedding_matrix: numpy array with vectors for all words\n        \"\"\"\n\n        super(LSTM, self).__init__()\n\n        # number of words = number of rows in embedding matrix\n        num_words = embedding_matrix.shape[0]\n\n        # dimension of embedding is number of columns in the matrix\n        embed_dim = embedding_matrix.shape[1]\n\n        # we define an input embedding layer\n        self.embedding = nn.Embedding(\n            num_embeddings=num_words,\n            embedding_dim=embed_dim\n        )\n\n        # embedding matrix is used as weights of the emnedding layer\n        self.embedding.weights = nn.Parameter(\n            torch.tensor(\n                embedding_matrix,\n                dtype=torch.float32\n            )\n        )\n\n        # we dont want to train the pretrained embeddings\n        self.embedding.weights.requires_grad = False\n\n        # a simple bidirectonal LSTM with hidden size of 128\n        self.lstm = nn.LSTM(\n            input_size = embed_dim,\n            hidden_size = 128,\n            bidirectional=True,\n            batch_first=True\n        )\n\n        # output layer which is a linear layer\n        # we have only one output\n        # input(512) = 128 + 128 for mean and same for max pooling\n        self.linear = nn.Linear(512, 1)\n\n\n    def forward(self, x):\n\n        # JUST A NOTE: IN BIDIRECTIONAL LSTM\n        # h0 and c0 will be of the shape => (2*L X N X M)\n        # 2*L due to bidirectional nature!\n\n\n        # pass the data through the embedding layer\n        # the input is just the tokens\n        # batch of sentences\n        # N X T --> N X T X D ( because batch_first )\n        x = self.embedding(x)\n\n        # move embedding output to lstm\n        # x : hidden states for the final layer for each time step,\n        # the RNN\/LSTM output\n        # _ : hidden states over all hidden layes, but only for the final time stamp\n        x, _ = self.lstm(x)\n        # N X T X D --> N X T X M (x)\n        # M is the hidden size\n\n        # apply mean and max pooling on lstm\n        # dimension 1 is about the time axis\n        avg_pool = torch.mean(x, 1)\n        max_pool, _ = torch.max(x, 1)\n\n        # concatenate mean and max pooling\n        # this is why size is 512\n        # 128 for each direction = 256 ( as the LSTM is bidirectional )\n        # avg_pool = 256 and max_pool = 256\n        out = torch.cat((avg_pool, max_pool), 1)\n\n        # pass through the linear layer and return the output\n        out = self.linear(out)\n\n        # return the linear output\n        return out","1f764cf6":"import torch\nimport torch.nn as nn\n\ndef train(data_loader, model, optimizer, device):\n    \"\"\"\n    This is the main training function that trains model for one epoch\n    :param data_loader: this is the torch DataLoader\n    :param model: model (lstm model)\n    :param optimizer: torch optimizer, e.g. adam, sgd, etc.\n    :param device: this can be \"cuda\" or \"cpu\"\n    \"\"\"\n\n    # set the model to training mode\n    model.train()\n\n    # go through the batches of data in data\n    for data in data_loader:\n\n        # fetch review and target from the dict\n        reviews = data[\"review\"]\n        targets = data[\"target\"]\n\n        # move the data to device that we want to use\n        reviews = reviews.to(device, dtype=torch.long)\n        targets = targets.to(device, dtype=torch.float)\n\n        # clear the gradients\n        optimizer.zero_grad()\n\n        # make predictions from the model\n        predictions = model(reviews)\n\n        # calculate the loss\n        loss = nn.BCEWithLogitsLoss()(predictions, targets.view(-1, 1))\n\n        # compute the gradient of loss w.r.t.\n        # all parameters of the model that are trainable\n        loss.backward()\n\n        # single optimization step\n        optimizer.step()\n\n    \ndef evaluate(data_loader, model, device):\n\n    # init empty lists to store predictions and targets\n    final_predictions = []\n    final_targets = []\n\n    # put the model in eval mode\n    model.eval()\n\n    # disable the gradient calculation\n    with torch.no_grad():\n        for data in data_loader:\n            reviews = data[\"review\"]\n            # targets = data[\"target\"]\n\n            reviews = reviews.to(device, dtype=torch.long)\n            # targets = targets.to(device, dtype=torch.float)\n\n            # make the predictions\n            predictions = model(reviews)\n\n            # move predictions and targets to list\n            # we need to move predictions and targets to cpu too\n            predictions = predictions.cpu().numpy().tolist()\n            targets = data[\"target\"].cpu().numpy().tolist()\n\n            final_predictions.extend(predictions)\n            final_targets.extend(targets)\n\n    # return final predictions and final targets\n    return final_predictions, final_targets","f9498c9d":"# We define all the configurations here\nMAX_LEN = 128\nTRAIN_BATCH_SIZE = 16\nVALID_BATCH_SIZE = 8\nEPOCHS = 10","8be33276":"import io\nimport torch\n\nimport numpy as np\nimport pandas as pd\n\nimport tensorflow as tf\n\nfrom sklearn import metrics\n\ndef load_vectors(fname):\n\n    f = open(fname,'r')\n    gloveModel = {}\n    for line in f:\n        splitLines = line.split()\n        word = splitLines[0]\n        wordEmbedding = np.array([float(value) for value in splitLines[1:]])\n        gloveModel[word] = wordEmbedding\n    print(len(gloveModel),\" words loaded!\")\n    f.close()\n    return gloveModel\n\n\ndef create_embedding_matrix(word_index, embedding_dict):\n    \"\"\"\n    This function creates the word embedding matrix.\n    :param word_index: a dictionary with word:index_value\n    :param embedding_dict: a dictionary with word:embedding_vector\n    :return: a numpy array with embedding vectors for all known words\n    \"\"\"\n\n    # init matrix with zeros\n    # 100 for glove embeddings, change it accordingly\n    embedding_matrix = np.zeros((len(word_index) + 1, 100))\n\n    # loop over all the words\n    for word, i in word_index.items():\n\n        # if word is found in pre-trained embeddings,\n        # update the matrix.\n        # if the word is not found,\n        # the vector is zeros\n        if word in embedding_dict:\n            embedding_matrix[i] = embedding_dict[word]\n\n    # return embedding matrix\n    return embedding_matrix\n\n\ndef run(df, fold):\n    \"\"\"\n    Run training and validation for a given fold and dataset\n    :param df: pandas DataFrame with kfold column\n    :param fold: current fold, int\n    \"\"\"\n\n    # fetch the training dataframe\n    train_df = df[df.kfold != fold].reset_index(drop=True)\n\n    # fetch validation dataframe\n    valid_df = df[df.kfold == fold].reset_index(drop=True)\n\n    print(\"Fitting tokenizer\")\n\n    # we use tf.keras for tokenization\n    tokenizer = tf.keras.preprocessing.text.Tokenizer()\n    tokenizer.fit_on_texts(df.review.values.tolist())\n\n    # convert training data to sequences\n    # each sentence is a bunch of number sequences\n    # which are just indices\n    xtrain = tokenizer.texts_to_sequences(train_df.review.values)\n    xtest = tokenizer.texts_to_sequences(valid_df.review.values)\n\n    # zero pad the training sequences given the maximum length\n    # this padding is done on left hand side\n    # if sequnece is > MAX_LEN, it is truncated on left hand side too\n    xtrain = tf.keras.preprocessing.sequence.pad_sequences(\n        xtrain, maxlen=MAX_LEN\n    )\n\n    # zero pad the validation sequences\n    xtest = tf.keras.preprocessing.sequence.pad_sequences(\n        xtest, maxlen=MAX_LEN\n    )\n\n    # init dataset class for training\n    train_dataset = IMDBdataset(\n        reviews=xtrain,\n        targets=train_df.sentiment.values\n    )\n\n    # create torch dataloader for training\n    # torch dataloader loads the data using dataset\n    # class in batches specified by batch_size\n    train_data_loader = torch.utils.data.DataLoader(\n        train_dataset,\n        batch_size=TRAIN_BATCH_SIZE,\n        num_workers=2\n    )\n\n    # init dataset class for validation\n    valid_dataset = IMDBdataset(\n        reviews=xtest,\n        targets=valid_df.sentiment.values\n    )\n\n    # create torch dataloader for validation\n    valid_data_loader = torch.utils.data.DataLoader(\n        valid_dataset,\n        batch_size=VALID_BATCH_SIZE,\n        num_workers=1\n    )\n\n    print(\"Loading Embeddings\")\n\n    # load the embeddings \n    embedding_dict = load_vectors(\"..\/input\/glove-embeddings\/glove.6B.100d.txt\")\n    embedding_matrix = create_embedding_matrix(\n        tokenizer.word_index, embedding_dict\n    )\n\n    # create torch device, since we use gpu, we are using cuda\n    device = torch.device(\"cuda\")\n\n    # fetch out LSTM model\n    model = LSTM(embedding_matrix)\n\n    # send model to device\n    model.to(device)\n\n    # init the Adam optimizer\n    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n\n    print(\"Training Model\")\n\n    # set best accuarcy to zero\n    best_accuracy = 0.0\n    \n    # set early stoping counter to zero\n    early_stopping_counter = 0\n\n    # train and validate for all epochs\n    for epoch in range(EPOCHS):\n\n        # train one epoch\n        train(train_data_loader, model, optimizer, device)\n\n        # validate\n        output, targets = evaluate(valid_data_loader, model, device)\n\n        # use threshold of 0.5\n        # NOTE: we are using linear layer and no sigmoid\n        # we should do this 0.5 threshold after sigmoid\n\n        # calculate accuracy\n        outputs = np.array(output) >= 0.5\n        \n        # calculate accuracy\n        accuracy = metrics.accuracy_score(targets, outputs)\n\n        print(f\"Fold: {fold}, Epoch: {epoch}, Accuracy Score = {accuracy}\")\n        \n        # simple early stopping\n        if accuracy > best_accuracy:\n            best_accuracy = accuracy\n\n        else:\n            early_stopping_counter += 1\n\n        if early_stopping_counter > 2:\n            break\n\n\nif __name__ == '__main__':\n\n    # load data\n    df = pd.read_csv(\".\/train_folds.csv\")\n\n    # train for all folds\n\n    for f_ in range(5):\n        run(df, f_)\n        print(\"\")\n        print(\"--------------------------\")\n        print(\"\")","cb176009":"## Step 1 : Create Folds","bf14feb6":"## Step 5 : Training Loop, Embeddings and Inference","c306079e":"## Step 3 : LSTM Model","4f86d13f":"## Step 2 : Make dataset class","25e1f40e":"## Step 4 : Training and evaluation functions"}}