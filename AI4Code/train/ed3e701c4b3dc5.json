{"cell_type":{"5a3555ea":"code","9ab268f4":"code","e13e2474":"code","7500bd92":"code","f70a369e":"code","ead9e72b":"code","14491627":"code","acc3601e":"code","1b02a396":"code","9b7c818c":"code","00c5d4a0":"code","2dca78eb":"code","7593d4a8":"code","efb63b2b":"code","708c9a10":"code","f7e10ec2":"code","8eea083e":"code","294ab54c":"code","8cefa8a9":"code","de303281":"code","119e8c17":"code","f891a7d9":"code","dde12ad4":"code","4aba7d51":"code","1fdd1ae0":"code","8e37cc6d":"code","65d57be4":"code","abdab45a":"code","0a0cfb66":"code","31e8c743":"code","5f9aef6c":"code","7836a09c":"code","7d139d82":"code","dedbc7e8":"code","6f06e973":"code","54ca21fb":"code","78807aaf":"code","977849ee":"code","85eeed98":"code","831b4135":"code","b7f95d5f":"code","941f9875":"code","b0cce4e4":"code","93e149ce":"code","8903dbfe":"code","55abbdcf":"code","e67b9ec4":"code","ba9b0d22":"code","6391c9e6":"code","9cc4164a":"code","48ec2692":"code","d1e72b58":"code","a1c3651c":"code","48e5946b":"code","5d4f3c80":"code","bd5553ce":"code","7508b31b":"code","f9d8655d":"code","fdc4027d":"code","3b189dc7":"code","b3b60052":"code","957070b2":"code","0b8c9eca":"code","feb01eba":"code","49c222dd":"code","b185c5c5":"code","92eea8f2":"code","00a50099":"code","e56dd729":"markdown","4b41bd38":"markdown","f6199b06":"markdown","dc8169a8":"markdown","f00173ac":"markdown","1cba803b":"markdown","49d90b7a":"markdown","016e6189":"markdown","1f53af51":"markdown","199c5fcc":"markdown","47dc49bf":"markdown","84d56507":"markdown","c8aa36d5":"markdown","a357f293":"markdown","736e80f2":"markdown","d4451189":"markdown","25882b1d":"markdown"},"source":{"5a3555ea":"\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns","9ab268f4":"\ndf=pd.read_csv(\"..\/input\/loan-dataset\/loans.csv\")\ndf","e13e2474":"\ndf[\"Credit History\"]=df[\"Credit History\"].map({\"bad\":\"bad\",\"unknown\":\"good\",\"good\":\"good\"})\n#Here we use map() function in order to assign \"good\" as the most common value of the column in exchange for unknown values\ndf[\"Credit History\"]","7500bd92":"df[\"Income\"]=df[\"Income\"].map({\"< NOK 15K\":1,\"NOK 15K \u2013 NOK 35K\":2,\"> NOK 35K\":3})\ndf[\"Income\"]\n#Here we use map() function in order to assign reasonable values the machine can understand rather than stringd","f70a369e":"#we can also use other methods to transform into numbers\ndf_latest=pd.get_dummies(data=df,columns=[\"Credit History\"],drop_first=True)\ndf_latest\n# Here we use pd.get_dummies() function or order to transform \"Credit History\" into dummy numbers","ead9e72b":"# Here we use pd.get_dummies() function or order to transform \"\"Debt\",\"Collatoral\"\" columns into dummy numbers\ndf_latest=pd.get_dummies(data=df_latest,columns=[\"Debt\",\"Collatoral\"],drop_first=True)\ndf_latest","14491627":"\ndf_latest.drop(\"No\",axis=1,inplace=True)","acc3601e":"df_latest","1b02a396":"\nfrom sklearn.tree import DecisionTreeClassifier","9b7c818c":"dtree=DecisionTreeClassifier()\n#Here we create an instance of the algorithm in order to use the algorithm ","00c5d4a0":"#Here we separate the features of the data from the entire data and assign to X variable\nX=df_latest.drop(\"Risk\",axis=1)\nX","2dca78eb":"#Here we separate the target column of the data from the entire data and assign to y variable\ny=df[\"Risk\"]\ny","7593d4a8":"dtree.fit(X,y)\n#Here we make algorithm to fit the data ","efb63b2b":"from sklearn import tree","708c9a10":"print(tree.export_text(dtree))\n#Here we get an report of how the algorith split nodes from the root node to the last node","f7e10ec2":"X #This is the latest version of the features after data engineering","8eea083e":"df #here is the original version of the data","294ab54c":"plt.figure(figsize=(15,10))\nplt.imshow(plt.imread(\"..\/input\/entropy\/entropy.png\"))\n# Here we get a picture of the formula of the information gain\n# We will use these mathematical formulas to calculate them by code","8cefa8a9":"plt.figure(figsize=(15,10))\nplt.imshow(plt.imread(\"..\/input\/information-gain\/information_gain.jpg\"))\n# Here we get a picture of the formula of the information gain","de303281":"plt.figure(figsize=(15,10))\nplt.imshow(plt.imread(\"..\/input\/entropy2\/entropy2.png\"))\n#Here is the formula of how we can calculate entropy\n#below we will calculate the entropy and information gain based on these formulas","119e8c17":"counts=np.bincount(df_latest[\"Income\"])\ncounts\n#This code will return the number of each unique value in a column\n","f891a7d9":"len(df_latest[\"Income\"]) \n#Here we get the length of the column","dde12ad4":"counts\/(len(df_latest[\"Income\"]))\n#here we calculate the probability of each value vy dividing the length of the entire column\n","4aba7d51":"import math","1fdd1ae0":"#Now we will combine the codes above and create a function that calculates entrpy of each column\ndef calc_entropy(column):\n    counts=np.bincount(column) #This code will return the number of each unique value in a column\n    probability=counts\/(len(column))#here we calculate the probability of each value vy dividing the length of the entire column\n    entropy=0#we start 0 as the intial value of entropy\n    for prob in probability: # here we a for loop to go throuh each probability of each unique value in the column\n        if prob >0:\n            entropy += prob * math.log(prob, 2) # here calculate entropy of each value and add them to find the total emtropy\n    return -entropy # we should return - * entropy due to the formula\n    ","8e37cc6d":"# Now we calculate the entropy of each column by using the function we have created\nprint(calc_entropy(df_latest[\"Income\"]))\nprint(calc_entropy(df_latest[\"Credit History_good\"]))\nprint(calc_entropy(df_latest[\"Debt_low\"]))\nprint(calc_entropy(df_latest[\"Collatoral_no\"]))\nprint(calc_entropy(df_latest[\"Collatoral_yes\"]))","65d57be4":"#Now we will calculate information gain after calculating total entropy of each column\n#Therefore we have to transform our target column into numerical values in order to make mathematical calculation\ndf_latest[\"Risk\"]=df_latest[\"Risk\"].map({\"low\":1,\"medium\":2,\"high\":3})\ndf_latest[\"Risk\"]","abdab45a":"#Here is our function to calculate information gain in order to show us which column provides the highest information gain \ndef information_gain(data, split,target):\n    original_entropy=calc_entropy(data[target])\n    values=data[split].unique()\n    left_split=data[data[split]==values[0]]\n    right_split=data[data[split]==values[1]]\n    subract=0\n    for subset in [left_split,right_split]:\n        prob=(subset.shape[0])\/data.shape[0]\n        subract += prob * calc_entropy(subset[target])\n    return  original_entropy - subract\n    ","0a0cfb66":"#Here we print information gain of  each column if it is selected as the splitting node\nprint(information_gain(df_latest,\"Income\",\"Risk\"))\nprint(information_gain(df_latest,\"Credit History_good\",\"Risk\"))\nprint(information_gain(df_latest,\"Debt_low\",\"Risk\"))\nprint(information_gain(df_latest,\"Collatoral_no\",\"Risk\"))\nprint(information_gain(df_latest,\"Collatoral_yes\",\"Risk\"))\n","31e8c743":"print(tree.export_text(dtree))","5f9aef6c":"fig=plt.figure(figsize=(20,10))\n_=tree.plot_tree(dtree,feature_names=df_latest[\"Income\"],class_names=y,filled=True)","7836a09c":"df2=pd.read_csv(\"..\/input\/mushroom-dataset\/Mushroom.csv\")\ndf2\n#First I import my data","7d139d82":"sample=df2.sample(25) # here we select 25 units as our sample via df.sample() method\nsample","dedbc7e8":"df2.info()\n# Here we overall information about the data in the sample","6f06e973":"df2.describe() # here we get overall statistical information about the data in the sample","54ca21fb":"df2.columns # here we list the names of the all columns in the sample","78807aaf":"sample.isnull().sum() # here we check whether our sample has missing data or not\n#Our sample has not any missing data as we can see below:","977849ee":"y=sample[\"p\"] # Here we assign the \"p\" column as our target data because this columns shows whether a mushroom is edible or not\ny","85eeed98":"X=sample.drop(\"p\",axis=1) # here we assign all the columns as our features apart from the target column and assign to variable X\nX","831b4135":"from sklearn.preprocessing import LabelEncoder\n#In order to transform all these letters into meaningful numerical values I import LabelEncoder() function","b7f95d5f":"Encoder_X=LabelEncoder()\nfor col in X.columns:\n    X[col]=Encoder_X.fit_transform(X[col])\n#Here we transform all the feature columns into numerical by using a for loop and LabelEncoder() function","941f9875":"X","b0cce4e4":"Encoder_y=LabelEncoder()\ny=Encoder_y.fit_transform(y)\ny\n#Here we transform all the values in the target column into numerical value by using LabelEncoder() function","93e149ce":"y.shape #This is the shape of the target","8903dbfe":"X.shape #This is the shape of the features of the mushrooms","55abbdcf":"from sklearn.model_selection import train_test_split\n#I import train_test_split() to split my data into train and test subsets","e67b9ec4":" X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n#here split both features and target column into test and train subsets and we assign test size %20 via test_size=0.2 parameter","ba9b0d22":" X_train #This is our train set of the features","6391c9e6":" X_test #This is our test set of the features","9cc4164a":"y_train #This is our train set of the target ","48ec2692":"y_test #This is our test set of the target ","d1e72b58":"from sklearn.tree import DecisionTreeClassifier\n#Now our data is ready to machine learning and I call the DecisionTreeClassifier algorithm \n#because we have a classification problem","a1c3651c":"dtree2=DecisionTreeClassifier()","48e5946b":"dtree2.fit(X_train,y_train) # Now the algorithm fits the train set","5d4f3c80":"predictions=dtree2.predict(X_test) #Here we make the peridiction based on the X_test features ","bd5553ce":"y_test #This includes original target values","7508b31b":"predictions #This is the prodictions of the algorithm\n#As we can easily see that the algorithm failed only one of the 5 predictions \n#and mislabeled the second one as edible though it was poisonous","f9d8655d":"from sklearn.metrics import classification_report, confusion_matrix","fdc4027d":"print(classification_report(y_test,predictions)) \n# These are the results according to classification report\n#Precision is %80 when it comes to predicti edible mushrooms but %0 when it comes to predict poisonous mushrooms","3b189dc7":"plt.figure(figsize=(15,10))\nplt.imshow(plt.imread(\"..\/input\/accuracy\/accuracy.gif\"))","b3b60052":"print(confusion_matrix(y_test,predictions))\n#These are the results of performance from confusion matrix","957070b2":"plt.figure(figsize=(15,10))\nplt.imshow(plt.imread(\"..\/input\/confusion\/confusion.png\"))\n#This figure below helps us to evaluate the results form the confusion matrix","0b8c9eca":"from statsmodels.stats.proportion import proportion_confint","feb01eba":"def confidence_interval(lower,upper):\n    lower,upper=proportion_confint(4,5,0.1)\n    return f\"lower={lower}, upper={upper}\"","49c222dd":"print(confidence_interval(4,5)) ","b185c5c5":"predictions #These are the predictions","92eea8f2":"y_test #These are the real values or labels","00a50099":"#Good luck","e56dd729":"<font color=\"blue\" >\nInformation gain is the main key that is used by Decision Tree Algorithms to construct a Decision Tree.\n    \nDecision Trees algorithm will always tries to maximize Information gain.\n    \nAn attribute with highest Information gain will tested\/split first.","4b41bd38":"<font color=\"blue\" >\nThe next step is to create functions to measure entropy and information gain of the several splitting points of the data in order to show the calculations the algorithm is performing and to understand why the algorith choose which column or feature as splitting nodes from the beginning to the end.","f6199b06":"<font color=\"blue\" >\nNow we will measure the performance of the algorithm ","dc8169a8":"<font color=\"blue\" >\nThese are the results according to classification report:\n    \nPrecision is %80 when it comes to predicti edible mushrooms but %0 when it comes to predict poisonous mushrooms\n\nThe accuracy is also %80.\n\nWe can visualize how you should evaluate the accuracy versus precision scores\n ","f00173ac":"<font color=\"blue\" >\nThe fourth step is to drop unnecessary columns as No column we gives only index of the row and it meaningless for the algorithm","1cba803b":"<font color=\"blue\" >\nHere we can visualize the splitting nodes of the algorithm based on the infromation as we calculates with our own code","49d90b7a":"# Problem 1:","016e6189":"<font color=\"blue\" >\nSecondly, we import the data we aim to use","1f53af51":"<font color=\"blue\" >\nEntropy is a measure of disorder or uncertainty and the goal of machine learning models and Data Scientists in general is to reduce uncertainty.","199c5fcc":"<font color=\"blue\" >\nFirst we import libraries we plan to use","47dc49bf":"<font color=\"blue\" >\nThirdly we need to deal with the missing data","84d56507":"<font color=\"blue\" >\nThe next step is to amalyze how the algorith split data and why it chooses certain splitting points","c8aa36d5":"# Problem 2: ","a357f293":"<font color=\"blue\" >\n\nAs we can see the code above the \"Income\" column gives the highest information gain which complies with the exact result of the choices of algorithm as the code below:","736e80f2":"<font color=\"blue\" >\nBased on confusion matrix, we have 4 true negative predictions(edible mushrooms are represnted as negative) and one false negative prediction(This means that our algothm labeled as negative(edible) but it is false, it should be positive(poisonous)","d4451189":"<font color=\"blue\" >\nThe using a function above we report the average performance with the 90% confidence interval in the code below.\n\nThis means that we can trust the predictions of the algorithm  at least %50 and %100 as the highest trust with the 90% confidence interval \n","25882b1d":"<font color=\"blue\" >\nThe next step is to implement the algorithm you want to use and split data into two as features and the target"}}