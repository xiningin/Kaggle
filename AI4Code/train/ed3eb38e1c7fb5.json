{"cell_type":{"49d00a76":"code","acc6a04a":"code","53c4ae85":"code","7454965e":"code","a6ef4c83":"code","6c6b34aa":"code","17492bf9":"code","789f6a0e":"code","56fe50c8":"code","3bf54a65":"code","880e196d":"code","b238d256":"code","0748fb21":"code","20ad84b4":"code","fad42e26":"code","787ee381":"code","c0c672b0":"code","04c896aa":"code","d67f4d1c":"code","e3d43406":"code","c0b0d9e1":"code","fc4f468f":"code","24533244":"code","c7198ea5":"code","46e9ef9a":"code","e6e74417":"code","8380668e":"code","98c3b450":"code","e1f9a5a3":"code","f8bd0646":"markdown","38f62401":"markdown","bce0e435":"markdown","ed292fe3":"markdown","0d67112f":"markdown","dea1b367":"markdown","6b8de7c7":"markdown","33455584":"markdown","08e56c22":"markdown","ece87d0f":"markdown","247c39c0":"markdown","d1d446cc":"markdown","936876c7":"markdown","3c26eb36":"markdown","1d0c0937":"markdown","2d8df495":"markdown","e9b24ad1":"markdown","47830b9a":"markdown","9d572a6d":"markdown","350df4c0":"markdown","fc797f60":"markdown","ae454e72":"markdown","bebb39d8":"markdown","386f8e10":"markdown","617a8ac3":"markdown","e54eb088":"markdown"},"source":{"49d00a76":"!pip install datasets","acc6a04a":"# visualization libraries\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# pytorch libraries\nimport torch # the main pytorch library\nimport torch.nn as nn # the sub-library containing Softmax, Module and other useful functions\nimport torch.optim as optim # the sub-library containing the common optimizers (SGD, Adam, etc.)\n\n# huggingface's transformers library\nfrom transformers import RobertaForTokenClassification, RobertaTokenizer\n\n# huggingface's datasets library\nfrom datasets import load_dataset\n\n# the tqdm library used to show the iteration progress\nimport tqdm\ntqdmn = tqdm.notebook.tqdm","53c4ae85":"roberta_version = 'roberta-base'\ntokenizer = RobertaTokenizer.from_pretrained(roberta_version)","7454965e":"dataset = load_dataset(\"conll2003\")","a6ef4c83":"dataset","6c6b34aa":"dataset.shape","17492bf9":"dataset['train'][0]","789f6a0e":"num_labels = dataset['train'].features['ner_tags'].feature.num_classes","56fe50c8":"print(dataset['train'].description)","3bf54a65":"print(dataset['train'].citation)","880e196d":"print(dataset['train'].homepage)","b238d256":"print(dataset['train'].license)","0748fb21":"def add_encodings(example):\n    \"\"\"Processing the example\n    \n    Args:\n        example (dict): The dataset example.\n    \n    Returns:\n        dict: The dictionary containing the following updates:\n            - input_ids: The list of input ids of the tokens.\n            - attention_mask: The attention mask list.\n            - ner_tags: The updated ner_tags.\n    \n    \"\"\"\n    # get the encodings of the tokens. The tokens are already split, that is why we must add is_split_into_words=True\n    encodings = tokenizer(example['tokens'], truncation=True, padding='max_length', is_split_into_words=True)\n    # extend the ner_tags so that it matches the max_length of the input_ids\n    labels = example['ner_tags'] + [0] * (tokenizer.model_max_length - len(example['ner_tags']))\n    # return the encodings and the extended ner_tags\n    return { **encodings, 'labels': labels }","20ad84b4":"# modify\/format all datasets so that they include the 'input_ids', 'attention_mask' \n# and 'labels' used to train and evaluate the model\ndataset = dataset.map(add_encodings)","fad42e26":"# format the datasets so that we return only 'input_ids', 'attention_mask' and 'labels' \n# making it easier to train and validate the model\ndataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])","787ee381":"# get the NER labels and create two dictionaries for accessing their ids\nlabels = dataset['train'].features['ner_tags'].feature\nlabel2id = { k: labels.str2int(k) for k in labels.names }\nid2label = { v: k for k, v in label2id.items() }","c0c672b0":"id2label","04c896aa":"label2id","d67f4d1c":"# initialize the model and provide the 'num_labels' used to create the classification layer\nmodel = RobertaForTokenClassification.from_pretrained(roberta_version, num_labels=num_labels)\n# assign the 'id2label' and 'label2id' model configs\nmodel.config.id2label = id2label\nmodel.config.label2id = label2id","e3d43406":"model","c0b0d9e1":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","fc4f468f":"# set the model in 'train' mode and send it to the device\nmodel.train().to(device)\n# initialize the Adam optimizer (used for training\/updating the model)\noptimizer = optim.AdamW(params=model.parameters(), lr=1e-5)","24533244":"# set the number of epochs \nn_epochs = 3\n# batch the train data so that each batch contains 4 examples (using 'batch_size')\ntrain_data = torch.utils.data.DataLoader(dataset['train'], batch_size=4)","c7198ea5":"train_loss = []\n# iterate through the data 'n_epochs' times\nfor epoch in tqdmn(range(n_epochs)):\n    current_loss = 0\n    # iterate through each batch of the train data\n    for i, batch in enumerate(tqdmn(train_data)):\n        # move the batch tensors to the same device as the \n        batch = { k: v.to(device) for k, v in batch.items() }\n        # send 'input_ids', 'attention_mask' and 'labels' to the model\n        outputs = model(**batch)\n        # the outputs are of shape (loss, logits)\n        loss = outputs[0]\n        # with the .backward method it calculates all \n        # of  the gradients used for autograd\n        loss.backward()\n        # NOTE: if we append `loss` (a tensor) we will force the GPU to save\n        # the loss into its memory, potentially filling it up. To avoid this\n        # we rather store its float value, which can be accessed through the\n        # `.item` method\n        current_loss += loss.item()\n        if i % 8 == 0 and i > 0:\n            # update the model using the optimizer\n            optimizer.step()\n            # once we update the model we set the gradients to zero\n            optimizer.zero_grad()\n            # store the loss value for visualization\n            train_loss.append(current_loss \/ 32)\n            current_loss = 0\n    # update the model one last time for this epoch\n    optimizer.step()\n    optimizer.zero_grad()","46e9ef9a":"fig, ax = plt.subplots(figsize=(10, 4))\n# visualize the loss values\nax.plot(train_loss)\n# set the labels\nax.set_ylabel('Loss')\nax.set_xlabel('Iterations (32 examples)')\nfig.tight_layout()\nplt.show()","e6e74417":"model = model.eval()\n# batch the train data so that each batch contains 4 examples (using 'batch_size')\ntest_data = torch.utils.data.DataLoader(dataset['test'], batch_size=4)","8380668e":"# create the confusion matrix\nconfusion = torch.zeros(num_labels, num_labels)\n\n# iterate through each batch of the train data\nfor i, batch in enumerate(tqdmn(test_data)):\n    # do not calculate the gradients\n    with torch.no_grad():\n        # move the batch tensors to the same device as the model\n        batch = { k: v.to(device) for k, v in batch.items() }\n        # send 'input_ids', 'attention_mask' and 'labels' to the model\n        outputs = model(**batch)\n            \n    # get the sentence lengths\n    s_lengths = batch['attention_mask'].sum(dim=1)\n    # iterate through the examples\n    for idx, length in enumerate(s_lengths):\n        # get the true values\n        true_values = batch['labels'][idx][:length]\n        # get the predicted values\n        pred_values = torch.argmax(outputs[1], dim=2)[idx][:length]\n        # go through all true and predicted values and store them in the confusion matrix\n        for true, pred in zip(true_values, pred_values):\n            confusion[true.item()][pred.item()] += 1","98c3b450":"# Normalize by dividing every row by its sum\nfor i in range(num_labels):\n    confusion[i] = confusion[i] \/ confusion[i].sum()","e1f9a5a3":"fig, ax = plt.subplots(figsize=(10, 10))\n# visualize the loss values\nax.matshow(confusion.numpy())\n\n# get the labels\nlabels = list(label2id.keys())\nids = np.arange(len(labels))\n\nax.set_ylabel('True Labels', fontsize='x-large')\nax.set_xlabel('Pred Labels', fontsize='x-large')\n\n# set the x ticks\nax.set_xticks(ids)\nax.set_xticklabels(labels)\n\n# set the y ticks\nax.set_yticks(ids)\nax.set_yticklabels(labels)\n\n# plot figure\nfig.tight_layout()\nplt.show()","f8bd0646":"To get the shape (number of examples and features) one can use the `.shape` attribute.","38f62401":"## Visualize the Training Loss\n\nJust visualize the training loss through the iterations.","bce0e435":"# Import RoBERTa Tokenizer\n\nNow we will import a pretrained RoBERTa model from Hugging Face. There are a lot of different models that were trained with different hyperparameters and tasks. A full list of official models is available [here][pretrained], but there are also a lot of models that are uploaded by the [community][community].\n\nIn this notebook we will use the `roberta-base` model. To use it we will need to import both the `tokenizer` and the `model`: The `tokenizer` will enable us to transform strings into tensors that can be then sent to the `model`, which in turn will give us the embeddings.\n\nAt this point we will only import the tokenizer as we will require it when we manipulate with data. The model will be defined later.\n\n[pretrained]: https:\/\/huggingface.co\/transformers\/pretrained_models.html\n[community]: https:\/\/huggingface.co\/models","ed292fe3":"# RoBERTa Model for Token Classification\n\nSince Named Entity Recognition can be seen as token classification we will use the RobertaForTokenClassification model provided by huggingface.\n\nWe will import the pretrained `RoBERTa` model and provide it the number of labels we will have. In addition, we will change the model configurations so that it will be easy for us to assign the labels and\/or label ids.","0d67112f":"# Training the Model\n\nIn this notebook, we will fine-tune the model to correctly identify named entities. During training we will update both the `RoBERTa` model weights as well as the `classifier`.","dea1b367":"With `pytorch` we are able to move the python calculations to the GPU. To do this we define the `device` on which we wish to run the calculations. Depending if `cuda` (the GPU drivers that enable running calculations on the graphic card) is enabled on the machine, we define the device as follows:","6b8de7c7":"Outputing the model will show us its architecture. As we can see, the first part is the architecture of the `RoBERTa` model, with an added `dropout` and `classifier` layers at the end. When we load a pretrained model, the `RoBERTa` weights are already set, but it requires fine-tuning for specific tasks.","33455584":"# Dataset Metadata\n\nIn additional to the actual data, the imported datasets can also have various metadata attached:\n\n- `description`. The description of the dataset.\n- `citation`. The citation used if one is submitting the results in a paper.\n- `homepage`. The homepage of the dataset.\n- `license`. The license under which the dataset can be used.","08e56c22":"Visualize the confusion matrix.","ece87d0f":"# Import Dataset\n\nIn this section we will use the `datasets` library to import one the NER benchmark datasets called [CoNLL-2003][conll2003]. This dataset contains the tokenized sentences and its corresponding named entities, part-of-speech tags and the syntactic chunk tags. In this example, we will train a NER model using only the named entities.\n\nBut first we need to import the dataset. This is done with the `load_dataset` function which allows us to download and prepare a dataset from the huggingface's [dataset index][datasets]. \n\n\n[conll2003]: https:\/\/www.aclweb.org\/anthology\/W03-0419\/\n[datasets]: https:\/\/huggingface.co\/datasets","247c39c0":"Another thing to define for training a model is the number of epochs `n_epochs`. This determines the number of times we will go through the whole dataset during training.\n\nTo train the dataset we will use the `dataset['train']` data. We also want to batch the examples, so that we can process multiple examples at once. Our batch size will be 4, meaning that there will be 4 examples processed in parallel.\n\n**NOTE:** Generally, when training a model it is advised to use a bigger batch. Here is why:\n\n- **Averaging gradients.** Training a model means calculating the gradient and changing the model parameters in the direction of that gradient, i.e. $params = params - learning_rate * gradient$. For each example we calculate the gradient and update the model, but for some examples the gradient can be directed in an odd direction (e.g. away from the optimal point). When using batches, the model will first get the gradient of every example and then average them. The average is then used to update the model. With this we can minimize the effect of the odd gradients that we might get during training.\n\n- **Faster training.** Training the model on the GPU allows us to process data faster. This is also due to the parallel processing capabilities of the GPU, meaning that it can process multiple examples at once. Increasing the batch size will make the training time shorter.\n\nAlthough there are benefits to using bigger batches, there are also some drawbacks:\n\n- **Bigger batches take more space.** When training on the GPU, one of the main problems is running out of GPU memory. That is usually because of two reasons: 1) the model is to big and cannot be trained on the GPU, or 2) the batch size is too big. The solution to the first reason is to get more GPU power. The solution to the second reason is to make the batch smaller. If there are still problems with GPU memory, try the [solutions provided here][cuda].\n\n[cuda]: https:\/\/pytorch.org\/docs\/stable\/notes\/faq.html\n","d1d446cc":"The imported dataset is a `datasets.DatasetDict` - a dictionary that contains three key values: \n\n- `train`. Used to train the model.\n- `validation`. Used to evaluate the model during training.\n- `test`. Used for evaluating the trained model.\n\nEach subset is a `datasets.Dataset` object which has a variety of attributes and methods. \n\n**NOTE:** Not all imported datasets are a `datasets.DatasetDict`, some can be already a `datasets.Dataset`. Depends on the dataset that one wants to upload.\n","936876c7":"Since we will train the model we will set it to `train`. In addition, we will run the model on the `device` defined above (if the `cuda` is available we will train the model on the GPU, otherwise on the CPU).\n\nWe will also define an `optimizer`. An optimizer is an object that will help us update the model. There is a whole [list of optimizers][optim], but in this example we will use the [AdamW optimizer][adam] which is currently one of the most popular optimizers used in deep learning. \n\nEvery optimizer accepts the model parameters (`params`) which can be acquired with `model.parameters()`. In addition, the optimizer accepts other inputs such as the learning rate `lr` used to specify for how much we want to move the model parameters in the direction of the gradient.\n\n[optim]: https:\/\/pytorch.org\/docs\/stable\/optim.html\n[adam]: https:\/\/pytorch.org\/docs\/stable\/optim.html#torch.optim.AdamW","3c26eb36":"# Evaluate the Model\n\nTo evaluate the model we must first put it in the `eval` mode. In addition, we must prepare the test dataset in a similar fasion as we did for training. ","1d0c0937":"The model will be evaluated by creating the confusion matrix. The confusion matrix will show the missmatch between the true labels and the predicted ones. \n\nFirst we create a tensor of size (`num_labels`, `num_labels`) in which we will store the results. Then, for each batch in the test dataset we will compare the true label with the predicted one and store it in the confusion matrix.","2d8df495":"Now we will normalize the confusion matrix by its rows so that we will see how did the model perform. ","e9b24ad1":"# Import the Libraries","47830b9a":"Also, we want to have an easy way to map the NER tags to indices and vice-versa. We will do this with the following lines.","9d572a6d":"# Named Entity Recognition using RoBERTa\n\nIn this notebook we will train a Named Entity Recognition (NER) model using RoBERTa.\n\nThe python libraries that we will use in this notebook are:\n\n- [transformers][transformers]. The huggingface library containing the general-purpose architectures for NLP.\n- [datasets][datasets]. The huggingface library containing datasets and evaluation metrics for NLP.\n- [torch][torch]. An open source machine learning framework used to create and train models on the GPU.\n- [matplotlib][matplotlib]. A comprehensive library for creating static, animated and interactive visualizations in Python.\n\n[transformers]: https:\/\/huggingface.co\/transformers\/index.html\n[datasets]: https:\/\/huggingface.co\/docs\/datasets\/\n[torch]: https:\/\/pytorch.org\/\n[matplotlib]: https:\/\/matplotlib.org\/\n\n**NOTE:** The notebook uses `transformers-3.5.1` and `torch-1.7.0` versions.","350df4c0":"Simply run the `.map` method with the `add_encodings` to format the dataset.","fc797f60":"In addition to adding new values to the dataset examples, we also want to modify what values are returned by the dataset by default and in which format. To do this, we will use the `.set_format` method and provide the following inputs:\n\n- `columns`. This will specify which attributes in the dataset we want to return by default.\n- `type`. This will transform the attributes specified in the `columns` attribute to the appropriate type. In our example we will transform into the `torch` tensor.","ae454e72":"Since the `datasets` library is not available on Kaggle by default we will need to install it using `pip install`.","bebb39d8":"# Dataset Format\n\nThe dataset is going to be used to train and evaluate the NER model. In order to do so, we must first modify and\/or add the examples so that they can be sent to the model. With `datasets` this can be done using the `.map` method. The map method accepts a `dict` and returns a `dict`. \n\nIf the returned dictionary contains new key-values, these are then added to input dictionary. In addition, if the returned dictionary contains keys that are also in the input dictionary, it overrides them.\n\nWith the `add_encodings` method we want to add the following values to our dataset examples:\n\n- `input_ids`. The input ids that will be sent to the model. This is generated by the `tokenizer`.\n- `attention_mask`. The attention_mask that will be sent to the model. This is generated by the `tokenizer`.\n- `labels`. The labels that will be sent to the model to calculate the loss value. This will be created from the `ner_tags` attribute of the examples.\n","386f8e10":"Each model has two states:\n\n- `train`. If the model is in this state it is configured for training Components such as Dropout and other that have been found to improve the models performance are active in this mode.\n- `eval`. If the model is in this state it is configured for production. Components such as Dropout are deactivated, since they are used only when training the model.\n\nIf the model is only going to be used in production, then set the model to eval. If the model is first going to be fine-tuned, set the model to train. After training, set the model to eval. **NOTE:** It is good practice to specify the models state before using it.","617a8ac3":"To access an example in a subset one can 'pretend' that the dataset is a list and access it as such.","e54eb088":"Since the dataset contains the named entity tags, we want to know the number of such tags. This number is found in the `ClassLabel` object and can be accessed in the following way."}}