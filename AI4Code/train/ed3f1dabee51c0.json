{"cell_type":{"ae827c4b":"code","238fac16":"code","fbe1bcf0":"code","2256f7c4":"code","fb7dadb5":"code","a8893f5e":"code","5b65282a":"code","2dd269d7":"code","9f30e507":"code","ec9f6276":"markdown","dab7d629":"markdown","e5b57145":"markdown","0860ddc7":"markdown","9aafbe8e":"markdown","0ccb1cec":"markdown","4daee84e":"markdown","90d0764a":"markdown","48c28a85":"markdown","6f5aaed1":"markdown"},"source":{"ae827c4b":"# ConnectX environment was defined in v0.1.6\n!pip install 'kaggle-environments>=0.1.6'","238fac16":"from kaggle_environments import evaluate, make, utils\n\nenv = make(\"connectx\", debug=True)\nenv.render()","fbe1bcf0":"def my_agent(obs, config):\n    import numpy as np\n    import random\n    \n    # Gets board at next step if agent drops piece in selected column\n    def drop_piece(grid, col, piece, config):\n        next_grid = grid.copy()\n        for row in range(config.rows-1, -1, -1):\n            if next_grid[row][col] == 0:\n                break\n        next_grid[row][col] = piece\n        return next_grid\n\n    # Returns True if dropping piece in column results in game win\n    def check_winning_move(obs, config, col, piece):\n        # Convert the board to a 2D grid\n        grid = np.asarray(obs.board).reshape(config.rows, config.columns)\n        next_grid = drop_piece(grid, col, piece, config)\n        # horizontal\n        for row in range(config.rows):\n            for col in range(config.columns-(config.inarow-1)):\n                window = list(next_grid[row,col:col+config.inarow])\n                if window.count(piece) == config.inarow:\n                    return True\n        # vertical\n        for row in range(config.rows-(config.inarow-1)):\n            for col in range(config.columns):\n                window = list(next_grid[row:row+config.inarow,col])\n                if window.count(piece) == config.inarow:\n                    return True\n        # positive diagonal\n        for row in range(config.rows-(config.inarow-1)):\n            for col in range(config.columns-(config.inarow-1)):\n                window = list(next_grid[range(row, row+config.inarow), range(col, col+config.inarow)])\n                if window.count(piece) == config.inarow:\n                    return True\n        # negative diagonal\n        for row in range(config.inarow-1, config.rows):\n            for col in range(config.columns-(config.inarow-1)):\n                window = list(next_grid[range(row, row-config.inarow, -1), range(col, col+config.inarow)])\n                if window.count(piece) == config.inarow:\n                    return True\n        return False\n    \n    valid_moves = [col for col in range(config.columns) if obs.board[col] == 0]\n    for col in valid_moves:\n        if check_winning_move(obs, config, col, obs.mark):\n            return col\n    for col in valid_moves:\n        if check_winning_move(obs, config, col, obs.mark%2+1):\n            return col\n    return random.choice(valid_moves)\n\n# This agent random chooses a non-empty column.\n#def my_agent(observation, configuration):\n    #from random import choice\n    #return choice([c for c in range(configuration.columns) if observation.board[c] == 0])","2256f7c4":"env.reset()\n# Play as the first agent against default \"random\" agent.\nenv.run([my_agent, \"random\"])\nenv.render(mode=\"ipython\", width=500, height=450)","fb7dadb5":"# Play as first position against random agent.\ntrainer = env.train([None, \"random\"])\n\nobservation = trainer.reset()\n\nwhile not env.done:\n    my_action = my_agent(observation, env.configuration)\n    print(\"My Action\", my_action)\n    observation, reward, done, info = trainer.step(my_action)\n    # env.render(mode=\"ipython\", width=100, height=90, header=False, controls=False)\nenv.render()","a8893f5e":"def mean_reward(rewards):\n    return sum(r[0] for r in rewards) \/ float(len(rewards))\n\n# Run multiple episodes to estimate its performance.\nprint(\"My Agent vs Random Agent:\", mean_reward(evaluate(\"connectx\", [my_agent, \"random\"], num_episodes=10)))\nprint(\"My Agent vs Negamax Agent:\", mean_reward(evaluate(\"connectx\", [my_agent, \"negamax\"], num_episodes=10)))","5b65282a":"# \"None\" represents which agent you'll manually play as (first or second player).\nenv.play([None, \"negamax\"], width=500, height=450)","2dd269d7":"import inspect\nimport os\n\ndef write_agent_to_file(function, file):\n    with open(file, \"a\" if os.path.exists(file) else \"w\") as f:\n        f.write(inspect.getsource(function))\n        print(function, \"written to\", file)\n\nwrite_agent_to_file(my_agent, \"submission.py\")","9f30e507":"# Note: Stdout replacement is a temporary workaround.\nimport sys\nout = sys.stdout\nsubmission = utils.read_file(\"\/kaggle\/working\/submission.py\")\nagent = utils.get_last_callable(submission)\nsys.stdout = out\n\nenv = make(\"connectx\", debug=True)\nenv.run([agent, agent])\nprint(\"Success!\" if env.state[0].status == env.state[1].status == \"DONE\" else \"Failed...\")","ec9f6276":"# Create ConnectX Environment","dab7d629":"# References\n\n* https:\/\/www.kaggle.com\/ajeffries\/connectx-getting-started","e5b57145":"# Create an Agent\n\nTo create the submission, an agent function should be fully encapsulated (no external dependencies).  \n\nWhen your agent is being evaluated against others, it will not have access to the Kaggle docker image.  Only the following can be imported: Python Standard Library Modules, gym, numpy, scipy, pytorch (1.3.1, cpu only), and more may be added later.\n\n","0860ddc7":"# Validate Submission\nPlay your submission against itself.  This is the first episode the competition will run to weed out erroneous agents.\n\nWhy validate? This roughly verifies that your submission is fully encapsulated and can be run remotely.","9aafbe8e":"# Write Submission File","0ccb1cec":"# Debug\/Train your Agent","4daee84e":"# Evaluate your Agent","90d0764a":"# Play your Agent\nClick on any column to place a checker there (\"manually select action\").","48c28a85":"# Install kaggle-environments","6f5aaed1":"# Test your Agent"}}