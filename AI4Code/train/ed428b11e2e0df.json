{"cell_type":{"22c9d99a":"code","bd5b3ffa":"code","ccb2212a":"code","eb7a6940":"code","d47ef456":"code","acc23066":"code","970b41d2":"code","2f0ce12b":"code","bf58de3d":"code","2004bce0":"code","b15854f3":"code","adcba982":"code","bde6b6d3":"code","2f383da6":"code","cdaf8482":"code","2e626b2c":"code","bd657759":"code","b0cd4680":"code","6cb4defa":"code","ddb8e9d3":"code","5774710f":"code","9e27c284":"code","6d89be68":"code","dc4fbfec":"code","f6b12bdb":"code","9fe5b2d9":"code","01bc480e":"code","f35976e2":"code","1bf9c514":"code","b6307061":"code","a01437c8":"code","7ece1ff0":"code","a6e25c1f":"code","567862a3":"code","b547ef99":"code","bf4be723":"code","a8cf1ae9":"code","afa8f9c6":"code","22b4e423":"code","ae68e2ec":"code","23f7e098":"code","f729b163":"code","c6fcc380":"code","13038dd3":"code","17731c9a":"code","44992581":"code","ebbd3c12":"code","bd7afcc2":"code","e9f9a36d":"code","a5f14c55":"code","de53358a":"code","b1f118b0":"markdown","66e10274":"markdown","192640c4":"markdown","c59ed171":"markdown","e0273f31":"markdown","6ff30608":"markdown","7a12832c":"markdown","9c925364":"markdown","10decd18":"markdown","970a01be":"markdown","b9b1772b":"markdown","e5853630":"markdown","d4fb819a":"markdown","8d71574b":"markdown","0ac2834e":"markdown","e10fefc7":"markdown","ab7504f9":"markdown","70cafc15":"markdown","5bea5b90":"markdown","d52b47d3":"markdown","6b2b6128":"markdown","6c3f753e":"markdown","729b69d5":"markdown","c5d698e5":"markdown","cc09a249":"markdown","f6309243":"markdown","1e89ca94":"markdown","894bf25e":"markdown","4d1a75c9":"markdown","eca547ca":"markdown","2098b755":"markdown","2f4f62bb":"markdown","fb3a2947":"markdown","fe99d8db":"markdown","5c385127":"markdown"},"source":{"22c9d99a":"import numpy as np \nimport pandas as pd\nimport os\n\nimport matplotlib.pyplot as plt\nfrom matplotlib.pylab import rcParams\nimport seaborn as sns\n\nrcParams['figure.figsize'] = (8, 5)\n\ntrain = pd.read_csv('..\/input\/train\/train.csv')\ntest  = pd.read_csv('..\/input\/test\/test.csv')\ntrain.columns = train.columns.str.lower()\ntest.columns = test.columns.str.lower()\n\ncat_vars = ['type', 'breed1', 'breed2', 'gender', 'color1', 'color2',\n            'color3', 'vaccinated', 'dewormed', 'sterilized', 'health', 'state']\n\n# no missing values in categorical variables\nassert not train[cat_vars].isnull().any().any()\nassert not test[cat_vars].isnull().any().any()\n\ntrain[cat_vars].nunique()","bd5b3ffa":"def encode_onehot(train, test, categ_variables):\n    df_onehot = pd.get_dummies(pd.concat([train[cat_vars], test[cat_vars]]).astype(str))\n    return df_onehot[:len(train)], df_onehot[len(train):]\n\ntrain_onehot, test_onehot = encode_onehot(train, test, cat_vars)\ntrain_onehot.shape","ccb2212a":"encod_type = train.groupby('type')['adoptionspeed'].mean()\nprint(encod_type)\ntrain.loc[:, 'type_mean_enc'] = train['type'].map(encod_type)\ntrain[['type','type_mean_enc']].head()","eb7a6940":"(train.groupby('breed1').size() \/ len(train)).nlargest(10)","d47ef456":"def encode_target_smooth(data, target, categ_variables, smooth):\n    \"\"\"    \n    Apply target encoding with smoothing.\n    \n    Parameters\n    ----------\n    data: pd.DataFrame\n    target: str, dependent variable\n    categ_variables: list of str, variables to encode\n    smooth: int, number of observations to weigh global average with\n    \n    Returns\n    --------\n    encoded_dataset: pd.DataFrame\n    code_map: dict, mapping to be used on validation\/test datasets \n    defaul_map: dict, mapping to replace previously unseen values with\n    \"\"\"\n    train_target = data.copy()\n    code_map = dict()    # stores mapping between original and encoded values\n    default_map = dict() # stores global average of each variable\n    \n    for v in categ_variables:\n        prior = data[target].mean()\n        n = data.groupby(v).size()\n        mu = data.groupby(v)[target].mean()\n        mu_smoothed = (n * mu + smooth * prior) \/ (n + smooth)\n        \n        train_target.loc[:, v] = train_target[v].map(mu_smoothed)        \n        code_map[v] = mu_smoothed\n        default_map[v] = prior        \n    return train_target, code_map, default_map","acc23066":"train_target_smooth, target_map, default_map = encode_target_smooth(train, 'adoptionspeed', cat_vars, 500)\ntest_target_smooth = test.copy()\nfor v in cat_vars:\n    test_target_smooth.loc[:, v] = test_target_smooth[v].map(target_map[v])","970b41d2":"train_target_smooth[cat_vars].head()","2f0ce12b":"def impact_coding_leak(data, feature, target, n_folds=20, n_inner_folds=10):\n    from sklearn.model_selection import StratifiedKFold\n    '''\n    ! Using oof_default_mean for encoding inner folds introduces leak.\n    \n    Source: https:\/\/www.kaggle.com\/tnarik\/likelihood-encoding-of-categorical-features\n    \n    Changelog:    \n    a) Replaced KFold with StratifiedFold due to class imbalance\n    b) Rewrote .apply() with .map() for readability\n    c) Removed redundant apply in the inner loop\n    '''\n    impact_coded = pd.Series()\n    \n    oof_default_mean = data[target].mean() # Gobal mean to use by default (you could further tune this)\n    kf = StratifiedKFold(n_splits=n_folds, shuffle=True) # KFold in the original\n    oof_mean_cv = pd.DataFrame()\n    split = 0\n    for infold, oof in kf.split(data[feature], data[target]):\n\n        kf_inner = StratifiedKFold(n_splits=n_inner_folds, shuffle=True)\n        inner_split = 0\n        inner_oof_mean_cv = pd.DataFrame()\n        oof_default_inner_mean = data.iloc[infold][target].mean()\n        \n        for infold_inner, oof_inner in kf_inner.split(data.iloc[infold], data.loc[infold, target]):\n            # The mean to apply to the inner oof split (a 1\/n_folds % based on the rest)\n            oof_mean = data.iloc[infold_inner].groupby(by=feature)[target].mean()\n\n            # Also populate mapping (this has all group -> mean for all inner CV folds)\n            inner_oof_mean_cv = inner_oof_mean_cv.join(pd.DataFrame(oof_mean), rsuffix=inner_split, how='outer')\n            inner_oof_mean_cv.fillna(value=oof_default_inner_mean, inplace=True)\n            inner_split += 1\n\n        # compute mean for each value of categorical value across oof iterations\n        inner_oof_mean_cv_map = inner_oof_mean_cv.mean(axis=1)\n\n        # Also populate mapping\n        oof_mean_cv = oof_mean_cv.join(pd.DataFrame(inner_oof_mean_cv), rsuffix=split, how='outer')\n        oof_mean_cv.fillna(value=oof_default_mean, inplace=True)\n        split += 1\n\n        feature_mean = data.loc[oof, feature].map(inner_oof_mean_cv_map).fillna(oof_default_mean)\n        impact_coded = impact_coded.append(feature_mean)\n            \n    return impact_coded, oof_mean_cv.mean(axis=1), oof_default_mean\n\n\ndef impact_coding(data, feature, target, n_folds=20, n_inner_folds=10):\n    from sklearn.model_selection import StratifiedKFold\n    '''\n    ! Using oof_default_mean for encoding inner folds introduces leak.\n    \n    Source: https:\/\/www.kaggle.com\/tnarik\/likelihood-encoding-of-categorical-features\n    \n    Changelog:    \n    a) Replaced KFold with StratifiedFold due to class imbalance\n    b) Rewrote .apply() with .map() for readability\n    c) Removed redundant apply in the inner loop\n    d) Removed global average; use local mean to fill NaN values in out-of-fold set\n    '''\n    impact_coded = pd.Series()\n        \n    kf = StratifiedKFold(n_splits=n_folds, shuffle=True) # KFold in the original\n    oof_mean_cv = pd.DataFrame()\n    split = 0\n    for infold, oof in kf.split(data[feature], data[target]):\n\n        kf_inner = StratifiedKFold(n_splits=n_inner_folds, shuffle=True)\n        inner_split = 0\n        inner_oof_mean_cv = pd.DataFrame()\n        oof_default_inner_mean = data.iloc[infold][target].mean()\n        \n        for infold_inner, oof_inner in kf_inner.split(data.iloc[infold], data.loc[infold, target]):\n                    \n            # The mean to apply to the inner oof split (a 1\/n_folds % based on the rest)\n            oof_mean = data.iloc[infold_inner].groupby(by=feature)[target].mean()\n            \n            # Also populate mapping (this has all group -> mean for all inner CV folds)\n            inner_oof_mean_cv = inner_oof_mean_cv.join(pd.DataFrame(oof_mean), rsuffix=inner_split, how='outer')\n            inner_oof_mean_cv.fillna(value=oof_default_inner_mean, inplace=True)\n            inner_split += 1\n\n        # compute mean for each value of categorical value across oof iterations\n        inner_oof_mean_cv_map = inner_oof_mean_cv.mean(axis=1)\n\n        # Also populate mapping\n        oof_mean_cv = oof_mean_cv.join(pd.DataFrame(inner_oof_mean_cv), rsuffix=split, how='outer')\n        oof_mean_cv.fillna(value=oof_default_inner_mean, inplace=True) # <- local mean as default\n        split += 1\n\n        feature_mean = data.loc[oof, feature].map(inner_oof_mean_cv_map).fillna(oof_default_inner_mean)\n        impact_coded = impact_coded.append(feature_mean)\n    \n    oof_default_mean = data[target].mean() # Gobal mean to use by default (you could further tune this)\n    return impact_coded, oof_mean_cv.mean(axis=1), oof_default_mean\n\ndef encode_target_cv(data, target, categ_variables, impact_coder=impact_coding):\n    \"\"\"Apply original function for each <categ_variables> in  <data>\n    Reduced number of validation folds\n    \"\"\"\n    train_target = data.copy() \n    \n    code_map = dict()\n    default_map = dict()\n    for f in categ_variables:\n        train_target.loc[:, f], code_map[f], default_map[f] = impact_coder(train_target, f, target)\n        \n    return train_target, code_map, default_map","bf58de3d":"train_target_cv, code_map, default_map = encode_target_cv(train, 'adoptionspeed', cat_vars, impact_coder=impact_coding)","2004bce0":"train_target_cv[cat_vars].head()","b15854f3":"corr_map = dict()\nfor v in cat_vars:\n    corr_map[v] = np.corrcoef(train_target_cv[v], train_target_smooth[v])[0, 1]    \nreg_correl = pd.Series(corr_map)\n\nnum_categories = train[cat_vars].nunique()","adcba982":"reg_correl.plot(kind='barh', color='green', alpha=0.3)\n_ = plt.title('Correlation between mean-encoded-variables\\n using smoothing and CV', fontsize=16)","bde6b6d3":"fig = plt.figure(figsize=(10, 5))\n_ = sns.kdeplot(train_target_smooth['breed1'], label='simple smoothing')\n_ = sns.kdeplot(train_target_cv['breed1'], label='cross-validation')\n_ = plt.title('Cross-validation regularisation introduced more variation than simple smoothing')","2f383da6":"train[cat_vars].nunique().plot(kind='barh')\n_ = plt.title('Number of unique categories')","cdaf8482":"fig, ax = plt.subplots()\n_ = ax.scatter(num_categories, reg_correl)\n_ = ax.set_xlabel('Number of unique categories in a variable', fontsize=14)\n_ = ax.set_ylabel('Correlation between 2 regularisations', fontsize=14)\nfor i, txt in enumerate(num_categories.index):\n    ax.annotate(txt, (num_categories[i], reg_correl[i]))","2e626b2c":"train.groupby('health').size()","bd657759":"def get_categor_spread(data, categ_variables):\n    spread = dict()\n    for v in categ_variables:\n        dist = data.groupby(v).size()\n        spread[v] = dist.max() \/ dist.min() \/ len(data)\n    return spread","b0cd4680":"spread = pd.Series(get_categor_spread(train, cat_vars))\nspread.plot(kind='barh')\n_ = plt.title('Larger spread indicates bigger difference between value\\n with highest and lowest number of observations')","6cb4defa":"fig, ax = plt.subplots()\n_ = ax.scatter(spread, reg_correl)\n_ = ax.set_xlabel('Number of unique categories in a variable', fontsize=14)\n_ = ax.set_ylabel('Correlation between 2 regularisations', fontsize=14)\nfor i, txt in enumerate(num_categories.index):\n    ax.annotate(txt, (num_categories[i], reg_correl[i]))","ddb8e9d3":"from sklearn.ensemble import GradientBoostingClassifier\n\nfrom sklearn.model_selection import StratifiedKFold, cross_val_score\nfrom sklearn.metrics import cohen_kappa_score, make_scorer\n\ngbc = GradientBoostingClassifier(n_estimators=10, random_state=20190301)\n\nskf = StratifiedKFold(n_splits=10, random_state=20190301)\n\nkappa_scorer = make_scorer(cohen_kappa_score, weights='quadratic')","5774710f":"def cross_validate_encoder(X, target, categorical_vars, encoder,\n                           model, n_splits=10, **kwargs):\n    \"\"\"Evaluate perfomance of encoding categorical varaibles with <encoder> by fitting \n    <model> and measuring average kappa on <n_samples> cross validation.\n    \n    Make sure to apply mean encoding only to infold set.\n            \n    Parameters\n    ----------\n    X: pd.DataFrame, train data\n    target: str, response variable\n    categorical_vars: list of str, categorical variables to encode\n    encoder: custom function to apply\n    model: sklearn model to fit\n    n_splits: number of cross-validation folds\n    **kwargs: key-word arguments to encoder\n    \n    Returns\n    ----------\n    metric_cvs: np.array of float, metrics computed on the held-out fold\n    \"\"\"     \n    skf = StratifiedKFold(n_splits=n_splits, random_state=20190301)\n    metric_cvs = list()\n    \n    for fold_idx, val_idx in skf.split(X=X, y=X[target]):\n        train_fold, valid_fold = X.loc[fold_idx].reset_index(drop=True), \\\n                                 X.loc[val_idx].reset_index(drop=True)\n        \n        # apply encoding to k-th fold and validation set \n        train_fold, code_map, default_map = encoder(train_fold, target, categorical_vars, **kwargs)\n        for v in categorical_vars:\n            valid_fold.loc[:, v] = valid_fold[v].map(code_map[v]).fillna(default_map[v])\n        \n        # fit model on training fold\n        model.fit(train_fold[categorical_vars], train_fold[target])\n        # predict out-of-fold\n        oof_pred = model.predict(valid_fold[categorical_vars])\n        metric_cvs.append(cohen_kappa_score(valid_fold[target], oof_pred, weights='quadratic'))        \n    return np.array(metric_cvs)","9e27c284":"print('Evaluating one-hot... ')\nscore_onehot_gbc = cross_val_score(estimator=gbc, X=train_onehot, y=train.adoptionspeed,\n                                   cv=skf, scoring=kappa_scorer)\n\nprint('Evaluating mean encoding with smoothing... ')\nscore_target_smooth_gbc = cross_validate_encoder(train, 'adoptionspeed', cat_vars, \n                                                 encode_target_smooth, gbc, smooth=500)\n\nprint('Evaluating mean encoding with cross-validation...')\nscore_target_cv_gbc = cross_validate_encoder(train, 'adoptionspeed', cat_vars, \n                                             encode_target_cv, gbc)","6d89be68":"summary_gb = pd.DataFrame({'kappa_cv10_mean': [score_onehot_gbc.mean(), \n                                               score_target_smooth_gbc.mean(), \n                                               score_target_cv_gbc.mean()],\n                           'kappa_cv10_std': [score_onehot_gbc.std(), \n                                              score_target_smooth_gbc.std(),\n                                              score_target_cv_gbc.std()]},\n                          index=['GradientBoosting - one hot', \n                                 'GradientBoosting - mean (smoothing)',\n                                 'GradientBoosting - mean (cross-validation)']\n                         )\n\nsummary_gb","dc4fbfec":"summary_gb['kappa_cv10_mean'].plot(kind='barh', color='lightblue', xerr=summary_gb.kappa_cv10_std, ecolor='red')\n_ = plt.xlabel('10-CV kappa average', fontsize=14)\n_ = plt.title('Comparison of encoding schemes', fontsize=16, y=1.01)","f6b12bdb":"from sklearn.metrics import roc_auc_score","9fe5b2d9":"def generate_data(n_obs=10000, n_lev=500, variance=1):\n    np.random.seed(20190325)\n    n_vars = 4\n    df = np.empty([n_obs, n_vars])\n    for i in range(n_vars):\n        df[:, i] = np.random.choice(np.arange(n_lev), size=n_obs, replace=True)\n    df = pd.DataFrame(df).astype(int)\n    cat_cols = ['x_bad1', 'x_bad2', 'x_good1', 'x_good2']\n    df.columns = cat_cols\n    \n    # y depends only x_good1 and x_good2\n    y = (0.2 * np.random.normal(size=n_obs) \n         + 0.5 * np.where(df.x_good1 > n_lev \/ 2, 1, -1) \n         + 0.3 * np.where(df.x_good2 > n_lev \/ 2, 1, -1)\n         + np.random.normal(scale=variance, size=n_obs)\n        )\n    df.loc[:, 'y'] = y > 0\n    df.loc[:, 'split_group'] = np.random.choice(('cal','train','test'), \n                                                size=n_obs, \n                                                replace=True, \n                                                p=(0.6, 0.2, 0.2))\n    \n    df.loc[:, cat_cols] = df[cat_cols].astype(str) + '_level'\n    return df","01bc480e":"df = generate_data()\ndf.head()","f35976e2":"df_train = df.loc[df.split_group!='test'].reset_index(drop=True)\ndf_test = df.loc[df.split_group=='test'].reset_index(drop=True)\ncat_cols = ['x_bad1', 'x_bad2', 'x_good1', 'x_good2']","1bf9c514":"def encode_onehot(train, test, categ_variables):\n    df_onehot = pd.get_dummies(pd.concat([train[categ_variables], test[categ_variables]]).astype(str))\n    return df_onehot[:len(train)], df_onehot[len(train):]\n\ntrain_onehot, test_onehot = encode_onehot(df_train, df_test, cat_cols)\ntrain_onehot.shape","b6307061":"gbc = GradientBoostingClassifier(n_estimators=10, random_state=20190325)\ngbc.fit(train_onehot, df_train['y'])\nprint(gbc.classes_)\n\n# train\nprint(roc_auc_score(df_train['y'], gbc.predict_proba(train_onehot)[:, 1])) # taking True class\n# test\nprint(roc_auc_score(df_test['y'], gbc.predict_proba(test_onehot)[:, 1]))\n\n# import features\npd.Series(index=train_onehot.columns, data=gbc.feature_importances_).nlargest(10)","a01437c8":"def encode_target_naive(train, test):\n    df_train_naive = train.copy()\n    df_test_naive = test.copy()\n    \n    default = df_train['y'].mean()\n    for v in cat_cols:    \n        encod_map = df_train.groupby(v)['y'].mean()\n        df_train_naive.loc[:, v] = df_train_naive[v].map(encod_map).fillna(default)\n        df_test_naive.loc[:, v] = df_test_naive[v].map(encod_map).fillna(default)\n    return df_train_naive, df_test_naive","7ece1ff0":"df_train_naive, df_test_naive = encode_target_naive(df_train, df_test)","a6e25c1f":"gbc = GradientBoostingClassifier(random_state=20190325, n_estimators=10)\ngbc.fit(df_train_naive[cat_cols], df_train_naive['y'])\npd.Series(gbc.feature_importances_, index=cat_cols)","567862a3":"pd.Series(gbc.feature_importances_, index=cat_cols)","b547ef99":"# train\nroc_auc_score(y_true=df_train_naive['y'], \n              y_score = gbc.predict_proba(df_train_naive[cat_cols])[:, 1])","bf4be723":"# test\nroc_auc_score(y_true=df_test_naive['y'], \n              y_score = gbc.predict_proba(df_test_naive[cat_cols])[:, 1])","a8cf1ae9":"gbc = GradientBoostingClassifier(random_state=20190325)\ngbc.fit(df_train_naive[cat_cols], df_train_naive['y'])\npd.Series(gbc.feature_importances_, index=cat_cols)","afa8f9c6":"# train\nroc_auc_score(y_true=df_train_naive['y'], \n              y_score = gbc.predict_proba(df_train_naive[cat_cols])[:, 1])","22b4e423":"# test\nroc_auc_score(y_true=df_test_naive['y'], \n              y_score = gbc.predict_proba(df_test_naive[cat_cols])[:, 1])","ae68e2ec":"from sklearn.metrics import roc_auc_score\ndef cross_validate_encoder(X, target, categorical_vars, encoder,\n                           model, n_splits=10, **kwargs):\n    \"\"\"Evaluate perfomance of encoding categorical varaibles with <encoder> by fitting \n    <model> and measuring average kappa on <n_samples> cross validation.\n    \n    Make sure to apply mean encoding only to infold set.\n            \n    Parameters\n    ----------\n    X: pd.DataFrame, train data\n    target: str, response variable\n    categorical_vars: list of str, categorical variables to encode\n    encoder: custom function to apply\n    model: sklearn model to fit\n    n_splits: number of cross-validation folds\n    **kwargs: key-word arguments to encoder\n    \n    Returns\n    ----------\n    metric_cvs: np.array of float, metrics computed on the held-out fold\n    \"\"\"     \n    skf = StratifiedKFold(n_splits=n_splits, random_state=20190301)\n    metric_cvs = list()\n    \n    for fold_idx, val_idx in skf.split(X=X, y=X[target]):\n        train_fold, valid_fold = X.loc[fold_idx].reset_index(drop=True), \\\n                                 X.loc[val_idx].reset_index(drop=True)\n        \n        # apply encoding to k-th fold and validation set \n        train_fold, code_map, default_map = encoder(train_fold, target, categorical_vars, **kwargs)\n        for v in categorical_vars:\n            valid_fold.loc[:, v] = valid_fold[v].map(code_map[v]).fillna(default_map[v])\n        \n        # fit model on training fold\n        model.fit(train_fold[categorical_vars], train_fold[target])\n        # predict out-of-fold\n        oof_pred = model.predict_proba(valid_fold[categorical_vars])[:, 1]\n        metric_cvs.append(roc_auc_score(valid_fold[target], oof_pred))\n    return np.array(metric_cvs)","23f7e098":"gbc = GradientBoostingClassifier(n_estimators=10, random_state=20190325)\n\nprint('Evaluating mean encoding with smoothing... ')\nscore_target_smooth_gbc = cross_validate_encoder(df_train, 'y', cat_cols, encode_target_smooth, gbc, smooth=500)\n\nprint('Evaluating mean encoding with cross-validation...')\nscore_target_cv_gbc = cross_validate_encoder(df_train, 'y', cat_cols, encode_target_cv, gbc)","f729b163":"score_target_smooth_gbc.mean(), score_target_cv_gbc.mean()","c6fcc380":"score_target_smooth_gbc.mean(), score_target_cv_gbc.mean()","13038dd3":"gbc = GradientBoostingClassifier(random_state=20190325, n_estimators=10)\ndf_train_smooth, code_map, default_map = encode_target_smooth(df_train, 'y', cat_cols, smooth=500)\n\ndf_test_smooth = df_test.copy()\nfor v in cat_cols:\n    df_test_smooth.loc[:, v] = df_test_smooth[v].map(code_map[v]).fillna(default_map[v])\n    \ngbc.fit(df_train_smooth[cat_cols], df_train_smooth['y'])","17731c9a":"pd.Series(gbc.feature_importances_, index=cat_cols)","44992581":"# train\nroc_auc_score(y_true=df_train_smooth['y'], \n              y_score = gbc.predict_proba(df_train_smooth[cat_cols])[:, 1])","ebbd3c12":"# test\nroc_auc_score(y_true=df_test_smooth['y'], y_score = gbc.predict_proba(df_test_smooth[cat_cols])[:, 1])","bd7afcc2":"gbc = GradientBoostingClassifier(random_state=20190325, n_estimators=10)\ndf_train_cv, code_map, default_map = encode_target_cv(df_train, 'y', cat_cols)\n\ndf_test_cv = df_test.copy()\nfor v in cat_cols:\n    df_test_cv.loc[:, v] = df_test_cv[v].map(code_map[v]).fillna(default_map[v])\n\ngbc.fit(df_train_cv[cat_cols], df_train_cv['y'])    ","e9f9a36d":"pd.Series(gbc.feature_importances_, index=cat_cols)","a5f14c55":"roc_auc_score(y_true=df_train_cv['y'], y_score = gbc.predict_proba(df_train_cv[cat_cols])[:, 1])","de53358a":"roc_auc_score(y_true=df_test_cv['y'], y_score = gbc.predict_proba(df_test_cv[cat_cols])[:, 1])","b1f118b0":"## With regularisations","66e10274":"If value of categorical variable has *n* observations that is lower than *smooth*, then the weighted average will be dominated by the global average.\n\nThe most popular Kernel that I've found implements a more elaborate version: https:\/\/www.kaggle.com\/ogrellier\/python-target-encoding-for-categorical-features\n\nIt refers to  this  paper: https:\/\/dl.acm.org\/citation.cfm?id=507538 Direct link: http:\/\/helios.mm.di.uoa.gr\/~rouvas\/ssi\/sigkdd\/sigkdd.vol3.1\/barreca.ps\n","192640c4":"The model correctly picked important feature, however, AUC is quite poor.","c59ed171":"## 2) Target encoding","e0273f31":"Suprisingly, mean encoding with smoothing performs very close to cross-validated one.  ","6ff30608":"### 3.1 Additive smoothing\n**Main idea**: if there are few observations  for an  unique value of categorical variable that we want to encode, rely more on global average for target variable.\n\nSimplest version is implemented below and follows  https:\/\/maxhalford.github.io\/blog\/target-encoding-done-the-right-way\/","7a12832c":"On second thought, it makes sense: **what matters is not only how many unique categories are in the feature, but how evenly the data is distributed across them. **","9c925364":"Let's see how correated are the results of additive smoothinga and cross-validation","10decd18":"## 5. Do they actually improve predictions?\n* Compare to one-hot encoding\n* Use GBM\n","970a01be":"It starts to overfit with more estimators.","b9b1772b":"# What is it about?\nIt's a small workbook to try out mean encoding  on the competition's dataset and understand  its regularisation techniques. \n\n**Main questions**:\n\n* What is mean encoding?\n* What are the techniques to avloid overfitting?\n* Is cross-validation scheme worth the additional workload compared to smoothing?\n* What uplift does mean encoding offer compared to one-hot encoding?\n\nI publish it to save time for somebody who may pose similar questions.\n\n\n# UPDATE 25.03.19\nExamine encoding schemes on a synthetic set following p.14 of [vtreat: a data.frame Processor for Predictive Modeling](https:\/\/arxiv.org\/abs\/1611.09477)\n\n\n# UPDATE 17.03.19\n\nThank you, Aditya Soni, for pointing out the data leakage in impact_coding() function. This pushed me to read more about it and I found this paper that explains the issue more\n\n[vtreat: a data.frame Processor for Predictive Modeling](https:\/\/arxiv.org\/abs\/1611.09477)\n\n\n\nNow I see that the main purpose of cross-validation loops in impact_coding() function was to compute mean for the given chunk of data based on the rest of data, thus ensuring that we don't simply memorise the data (aka leaking target variable into encoding categorical variable for given data fold).\n\nEncoding categorical variable as it's done in encode_target_smooth()  - on the whole dataset - and fitting the model causes the data leakage that cross-validation technique aimed to prevent.\n","e5853630":"Generate 4 categorial variables with 500 unique labels each; target variable depends only two of them.","d4fb819a":"**Quesion: should it be  applied on disjoint sets of data to avoid target leaking?****","8d71574b":"# I Motivation\nMaybe it's this time of year, but I've suddenly felt an urge to come to Kaggle for glory and fame and sat down to explore  present competition's dataset, only to find that a few categorical variables have pretty high cardinality.","0ac2834e":"### 3.2 Cross-validation\n**Main idea**: introduce variability into encoding estimates by averaging mean over several folds.\n\nSource: https:\/\/www.kaggle.com\/tnarik\/likelihood-encoding-of-categorical-features, which seems to have been motivated by this discussion: https:\/\/www.kaggle.com\/c\/mercedes-benz-greener-manufacturing\/discussion\/36136#201638\n\n The scheme below does the folloiwing:\n* splits data set into 20 folds, \n* for each held-out fold,  estimate the mean encoding based on the further 10-fold splits of the training folds.","e10fefc7":" One-hot encoding has been my solution before but this time around I stopped and pondered: there must be a better way to handle it.\n\n","ab7504f9":"* It correcly picks x_good1 feature\n* Accuracy is much higher than with one-hot encoding","70cafc15":"### Naive","5bea5b90":"**Observations**\n* the model trained on target encoding  has much higher accuracy than one-hot\n* hold-out accuracy of cross-validation and smoothed encodings  are very similar, although smoothed encoding was evaluated on the same dataset that the model was trained, which poses higher risks of overfit","d52b47d3":"## 3. What regularisations are out there?\n\nThere seem to be two schools of thoughts on how to incorporate regularisation into mean encoding\n* additive smoothing\n* cross-validation\n\n","6b2b6128":"Surprisingly, tecniques deviated even for **health** and **state*** that have relatively low number of unique values.","6c3f753e":"## Conclusion\n(At least for GBM)\n\n* Mean encoding indeed seems to work better than one-hot encoding,\n* CV regularisation seems to out-perform smoothing","729b69d5":"I've read a few articles on common encoding techniques, one of which caught my attention: **mean encoding (aka target, aka likelihood, aka impact)**, whereby each distinct value of categorical value is replaced with average value of target variable we're trying to predict.\n\nAfter some googling and reading through amazing Kaggle kernels and forums, I've realized that: \n\n*  mean encoding as it is has high risk of overfitting, so some kind of regularisation is required\n* there are several ways of doing this regularisation\n\nDissecting their code and aplplying on single dataset led to this kernel.","c5d698e5":"For majority of categorical variables, the results are pretty close. They differ most for high-cardinality like **breed1** and **breed2**.","cc09a249":"# II Main part\n\n## 1. What is mean encoding?\n\nTreating **adoptionspeed** as continuous variable for simplicity, each unique value of **type**, **breed1** etc. variables  will be replaced with average of **adoptionspeed**. For example","f6309243":"# 6. Verify encodings on synthetic dataset\n[Reference, p.14](https:\/\/arxiv.org\/abs\/1611.09477)","1e89ca94":"# IV References\n## Additive Smoothing\n* https:\/\/www.kaggle.com\/ogrellier\/python-target-encoding-for-categorical-features\n* https:\/\/maxhalford.github.io\/blog\/target-encoding-done-the-right-way\/\n\n    \n## Cross Validation\n* https:\/\/www.kaggle.com\/tnarik\/likelihood-encoding-of-categorical-features\n* https:\/\/www.kaggle.com\/c\/mercedes-benz-greener-manufacturing\/discussion\/36136#201638    ","894bf25e":"## 3) Verify on whole train\/test sets + examine feature importance","4d1a75c9":"### Check default number of trees","eca547ca":"# III Further reading","2098b755":"## 4. How close are the results of the two regularisation methods?","2f4f62bb":"~~E.g. more than half of pets in **train** were of either 307-th or 266-th breed, which means  that for majority of **breed1** values we'll have only a handful of observations to estimate the mean.  Memorising the training dataset in such a way  is the definition of  **overfitting**. To make use of our mean encoding of **breed1** nonetheless, we can employ **regularisation*~~","fb3a2947":"## 1) One-hot","fe99d8db":"## 2. What causes overfitting?\n~~For many unique values of such  high-cardinality variable as **breed1**, we have only a handful available observations~~\n\nLearning mean encoding on the same data as we use to train the model. This is similar to tuning model parameters on the whole data.\n\nAs the  authors of  **vtreat** package poin out:\n\n*Care must be taken when impact coding variables \u2013 or when using nested models in general, for example in model stacking or superlearning (van der Laan, Polley, and Hubbard (2007)): **the data used to do the impact coding should not be the same as the data used to fit the overall model**. This is because the impact coding (or the base models in superlearning) are relatively complex, high-degree-of-freedom models masquerading as low-degree-of-freedom single variables. As such, they may not be handled appropriately by downstream machine learning algorithms. In the case of impact-coded, high-cardinality categorical variables, the resulting impact coding may memorize patterns in the training data, making the variable appear more statistically significant than it really is to downstream modeling algorithms.* [p.13](https:\/\/arxiv.org\/abs\/1611.09477)","5c385127":"0. More on comparing Laplace smoothing and cross-validation schemes: http:\/\/www.win-vector.com\/blog\/2016\/11\/laplace-noising-versus-simulated-out-of-sample-methods-cross-frames\/?relatedposts_hit=1&relatedposts_origin=5231&relatedposts_position=1\n1. Incorporating mean encoding into grand cross-validation: https:\/\/www.kaggle.com\/c\/porto-seguro-safe-driver-prediction\/discussion\/44987\n2. Comprehensive study of mean encoding: https:\/\/www.kaggle.com\/vprokopev\/mean-likelihood-encodings-a-comprehensive-study"}}