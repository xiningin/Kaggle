{"cell_type":{"6db0b0b8":"code","3a97c963":"code","e372d13e":"code","84940a96":"code","800e66cc":"code","49f88efa":"code","556eae36":"code","dd56bf66":"code","9a3e3f2e":"code","014b62bf":"code","bb06f4d5":"code","8f9c8a6c":"code","1180b381":"code","ecf7f15a":"code","420c69d5":"code","18a423a4":"code","469530f1":"code","c8da2be8":"code","93812d8b":"code","77e76f0d":"code","6e488ff9":"code","4537011f":"code","0c4a7ade":"code","94d785d5":"code","21428549":"code","15259c62":"code","568965f8":"code","95fb1cd0":"code","53bf2356":"code","b5dca968":"code","1a8b8cad":"code","10272802":"code","c252a264":"code","41988ea2":"code","4c395934":"code","1a227dd5":"code","b0d96143":"code","15157460":"code","745a7816":"code","2f0e081c":"code","3f69e482":"code","25212019":"code","1aa17f7b":"code","49cc804f":"code","44e7a559":"code","463e0b71":"code","0a5c1b8c":"code","95d70f6f":"code","816d482b":"code","57850f5b":"code","cfdfb146":"code","3890b07a":"code","817d518f":"markdown","ab23294d":"markdown","3d86bb4b":"markdown","7d1d9d29":"markdown","7aea8c7a":"markdown","9e933d3e":"markdown","3f1575f6":"markdown","f9f84f6a":"markdown","47060d1b":"markdown","44ec8ed7":"markdown","640db7d0":"markdown","d881fa3a":"markdown","38969d0b":"markdown","52cee029":"markdown","f2224b3f":"markdown","525f3409":"markdown","21f0d4c9":"markdown","10c22e9c":"markdown","2d84a48f":"markdown","e243a9cb":"markdown","d637bde0":"markdown","0147e4f8":"markdown","90ffcac0":"markdown","eaf52932":"markdown","35f575d0":"markdown"},"source":{"6db0b0b8":"from plotly.offline import init_notebook_mode, iplot_mpl, download_plotlyjs, plot, iplot\nimport plotly_express as px\nimport plotly.figure_factory as ff\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nimport warnings\nwarnings.filterwarnings('ignore')\ninit_notebook_mode(connected=True)\nimport pandas_profiling\n\nfrom sklearn.preprocessing import scale\nfrom sklearn.preprocessing import StandardScaler\nimport pandas as pd\n\nfrom sklearn.model_selection import GridSearchCV, cross_val_score\nfrom pandas import DataFrame\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.model_selection import cross_val_predict\n\nfrom sklearn.metrics import accuracy_score\n\nimport catboost\nfrom catboost import CatBoostRegressor, Pool, cv\nimport shap\n\n\nplt.rcParams['figure.dpi'] = 300\n%config InlineBackend.figure_format = 'svg'","3a97c963":"pip install feature_engine\n","e372d13e":"pip install ppscore","84940a96":"pip install missingpy","800e66cc":"from feature_engine.imputation import CategoricalImputer\nfrom feature_engine.encoding import OrdinalEncoder\nimport ppscore as pps\nfrom missingpy import MissForest","49f88efa":"#We load the train & test DataSets\ndf=pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest=pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')","556eae36":"#We separate our feature matrix from our target vector\ny=df.SalePrice","dd56bf66":"df.head()","9a3e3f2e":"df.info()","014b62bf":"#pandas_profiling.ProfileReport(df)","bb06f4d5":"#We drop these variables\ndrop_features=['Id','MiscFeature','MiscVal','MoSold','YrSold','SaleType','SaleCondition','Condition2',\n               'Utilities','3SsnPorch','LowQualFinSF','PoolArea','PoolQC','Street','Alley']\ndf.drop(drop_features,axis=1,inplace=True)\ntest.drop(drop_features,axis=1,inplace=True)","8f9c8a6c":"p=df.copy()\npredictors=pps.predictors(p, \"SalePrice\")\npredictors.head(10)","1180b381":"p=df.copy()\npredictors=pps.predictors(p, \"SalePrice\")\n\nfig = px.bar(predictors, x=\"x\", y='ppscore',\n              title=\"Bar Plot - Predictive power score atributes with target\",\n              color_discrete_sequence=px.colors.qualitative.Pastel2).update_layout( \n    paper_bgcolor='rgb(243, 243, 243)',\n    plot_bgcolor='rgb(243, 243, 243)')\n\nfig.show()","ecf7f15a":"#y=df.SalePrice\nX=df.copy()","420c69d5":"X.drop('SalePrice',axis=1,inplace=True)","18a423a4":"# Separate the data between numeric and categorical\nX_n=X.select_dtypes(exclude=[object])\nX_c=X.select_dtypes(include=[object])\n\n#We do the same for the test data\ntest_n=test.select_dtypes(exclude=[object])\ntest_c=test.select_dtypes(include=[object])","469530f1":"# set up the categorical imputer\nc_imputer = CategoricalImputer(imputation_method='missing',fill_value= 'Missing')\n\n# fit the imputer\nc_imputer.fit(X_c)","c8da2be8":"# transform the data\nX_c= c_imputer.transform(X_c)","93812d8b":"# set up the categorical imputer\nc_imputer = CategoricalImputer(imputation_method='missing',fill_value= 'Missing')\n\n# fit the imputer\nc_imputer.fit(test_c)\n# transform the data","77e76f0d":"test_c= c_imputer.transform(test_c)","6e488ff9":"test_c.fillna('Missing',inplace=True)","4537011f":"# missForest imputation\nimputer = MissForest(random_state=20)\n\nimputed_X = imputer.fit_transform(X_n)","0c4a7ade":"# missForest imputation for test data\nimputer = MissForest(random_state=20)\n\nimputed_test = imputer.fit_transform(test_n)","94d785d5":"#We set up the numeric data Frames\nX_n = pd.DataFrame(data=imputed_X, columns=X_n.columns.tolist())\ntest_n=pd.DataFrame(data=imputed_test, columns=test_n.columns.tolist())","21428549":"#We unite the processed DataFrames\nX=pd.concat([X_n, X_c], axis=1)\ntest=pd.concat([test_n, test_c], axis=1)","15259c62":"ordinal_features=['ExterQual','ExterCond','BsmtQual','BsmtCond','HeatingQC','KitchenQual','FireplaceQu','GarageQual','GarageCond']","568965f8":"encoder = OrdinalEncoder(encoding_method='ordered',variables=ordinal_features)\n# fit the encoder\nencoder.fit(X,y)\n\n# transform the data\nX=encoder.transform(X)\ntest=encoder.transform(test)","95fb1cd0":"test.fillna(0,inplace=True)","53bf2356":"X[ordinal_features].head()","b5dca968":"#We add a predictor for the Total Bathrooms\nX['Total_Bathrooms']= X['FullBath'] + 0.5 * X['HalfBath']\ntest['Total_Bathrooms']= test['FullBath'] + 0.5 * test['HalfBath']","1a8b8cad":"#We prepare the data so that the algorithm can process categorical attributes\ncategorical_features_indices = np.where(X.dtypes == 'object') [0]","10272802":"categorical_features_indices","c252a264":"X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)","41988ea2":"train_pool = Pool(X_train, y_train,cat_features=categorical_features_indices)\nvalidation_pool = Pool(X_val, y_val,cat_features=categorical_features_indices)","4c395934":"model = CatBoostRegressor(iterations=5000,  random_seed=42, early_stopping_rounds=50)\n\nmodel.fit(train_pool, eval_set=validation_pool, verbose=100, plot=True)","1a227dd5":"model.score(X_val,y_val)","b0d96143":"ax=model.get_feature_importance(type='PredictionValuesChange',prettified=True)\n\nfig = px.bar(ax, x='Feature Id',y=['Importances'],\n              title=\"Cat Boost predictors importance - Prediction value change\",\n              color_discrete_sequence=px.colors.qualitative.Prism).update_layout( \n    paper_bgcolor='rgb(243, 243, 243)',\n    plot_bgcolor='rgb(243, 243, 243)')\n\nfig.show()","15157460":"ax=model.get_feature_importance(train_pool, type='LossFunctionChange',prettified=True)\n\nfig = px.bar(ax, x='Feature Id',y=['Importances'],\n              title=\"Cat Boost predictors importance - Loss Function Change\",\n              color_discrete_sequence=px.colors.qualitative.Vivid).update_layout( \n    paper_bgcolor='rgb(243, 243, 243)',\n    plot_bgcolor='rgb(243, 243, 243)')\n\nfig.show()","745a7816":"fi = model.get_feature_importance(train_pool,type=\"Interaction\", prettified=True)\n\ncols=X.columns.tolist()\nfor i in fi['First Feature Index'].tolist():\n    fi['First Feature Index'].replace({i:cols[i]},inplace=True)\n\nfor i in fi['Second Feature Index'].tolist():\n    fi['Second Feature Index'].replace({i:cols[i]},inplace=True)\n    \nfi['feature-interaction']=fi['First Feature Index'].astype(str) + '-' + fi['Second Feature Index'].astype(str)\n\nfig = px.bar(fi.head(15), x='feature-interaction',y=['Interaction'],\n              title=\"Cat Boost predictors importance- Feature interaction\",\n   color_discrete_sequence=px.colors.qualitative.Dark24).update_layout( \n    paper_bgcolor='rgb(243, 243, 243)',\n    plot_bgcolor='rgb(243, 243, 243)')\n\nfig.show()","2f0e081c":"indices, scores = model.get_object_importance(\n    train_pool,\n    validation_pool,\n    importance_values_sign='Positive', # Positive values means that the optimized metric\n    update_method='AllPoints'         # value is increased because of given train objects.                                     # So here we get the indices of bad train objects.\n)","3f69e482":"object_importance=pd.DataFrame(scores)\nobject_importance['indices']=indices\nobject_importance.columns=['scores','indices']","25212019":"object_importance.round(2)","1aa17f7b":"explainer = shap.TreeExplainer(model)\nshap_values = explainer.shap_values(X_train)\nshap.summary_plot(shap_values,X_train,alpha=0.5)","49cc804f":"X_full=pd.concat([X_train,X_val])\ny_full=pd.concat([y_train,y_val])","44e7a559":"full_pool = Pool(X_full, y_full,cat_features=categorical_features_indices)","463e0b71":"model = CatBoostRegressor(iterations=1087,  random_seed=42, early_stopping_rounds=75)\n\nmodel.fit(full_pool, verbose=100, plot=True)","0a5c1b8c":"# Use the model to make predictions\npredicted_prices = model.predict(test)","95d70f6f":"model.predict(test)","816d482b":"test.id=test.index","57850f5b":"test['Id']=test.index","cfdfb146":"my_submission = pd.DataFrame({'Id':test.Id, 'SalePrice': predicted_prices})\n# you could use any filename. We choose submission here","3890b07a":"my_submission","817d518f":"## 2.4 Power Predictive Score","ab23294d":"## 4.2 Loss Function Change importance","3d86bb4b":"## 3.1 Filling missing values","7d1d9d29":"## 2.3 Pandas Profiling","7aea8c7a":"# House Prices \n\nIn this notebook we'll build a regression model to predict housing sales prices sale price using different data preprocessing techniques and CatBoost Regressor.","9e933d3e":"## 4.1 Model Score","3f1575f6":"## 4.6 Model with full training data","f9f84f6a":"We will fill up the numeric missing values with the miss forest algorithm & the categorical missing values with the categorical missing imputer.","47060d1b":"We can check the features with the higher PPS:","44ec8ed7":"## 3.2 Ordinal encoding of the qualitative categorical features\n\nThe OrdinalCategoricalEncoder() replaces categories by ordinal numbers(0, 1, 2, 3, etc). The numbers are ordered based on the mean of the target per category. \n\nWe use this method for all the qualitative features.","640db7d0":"## 4.2  Prediction Values Change importance","d881fa3a":"## 4.3 Feature interaction importance","38969d0b":"# 5. Prediction","52cee029":"OverallQual seems to be the feature with the highest PPS.","f2224b3f":"# 1 . Data Loading ","525f3409":"# 3. Data Preprocessing","21f0d4c9":"All the previously category missing values have been filled up with the category \"Missing\". \nWe can apply the missForest imputation for missing numeric data.","10c22e9c":"After a first examination we can conclude that many ariables don't offer any predictive power use for model-building. \n\n- Some have almost all values 0\/missing\/ unique single values. \n\n- Others don't offer any intrinsic description of the houses like MoSol or YerSold.","2d84a48f":"## 4.4 Object importance ","e243a9cb":"# 4. BaseLine CatBoost Regressor","d637bde0":"## 2.1 Variable Information","0147e4f8":"## 4.5 Shap Summary Plot","90ffcac0":"# 2 . Data Preprocessing & Data Exploration","eaf52932":"With Pandas Profiling we can examine the basic properties of the Data. \n\n- We have several features (both numerical and categorical) with lot of missing values.\n- We can examine some of the interactions. \n- The target distribution has a long tail to the right.","35f575d0":"We have 79 features with many missing values and a target colum to predict the price. "}}