{"cell_type":{"f11974d3":"code","5a0db0ba":"code","65f08388":"code","ff78cfd8":"code","ff9cbe45":"code","a9dc9d7a":"code","c002c7d6":"code","f9d07ef3":"code","3607823b":"code","4d1ecadd":"code","3fb4a695":"code","6c75b96a":"code","f7c5735a":"code","a756cdbd":"code","b6e9995f":"code","69122ec8":"code","a46c27cb":"code","2055fa00":"code","1a3aef79":"code","d878fa99":"code","8bc6b874":"code","bfbcdf4d":"code","6324b9d0":"code","87f619fd":"code","1035a739":"code","e84efef0":"code","bda33f02":"code","657f1d4c":"code","75b7ee2b":"code","1c9e8ae3":"code","a95614c2":"code","db9ed2d5":"code","ef912fee":"code","311d87f9":"code","928777cf":"code","fda8e095":"code","755e2286":"code","dbaf5bbb":"code","12649c22":"code","d3c4c7cf":"code","bbbeb297":"code","d4369502":"code","aad09ac7":"code","60229c40":"code","6f4f48fa":"code","69643ce1":"code","23cf055e":"code","db5621fb":"code","1bf39149":"markdown","7dadab4a":"markdown","56eb0d3e":"markdown","ab7ca7ea":"markdown","584a0ad5":"markdown","7d8a8507":"markdown","a9410ef1":"markdown","835f8321":"markdown","2cb87459":"markdown","fa753baa":"markdown","fe030b52":"markdown","8a888d54":"markdown","21811985":"markdown","c60ff823":"markdown","d8d92187":"markdown","b60852fe":"markdown","5c6735df":"markdown","c8c637ab":"markdown","7fa4d50c":"markdown","4b7f834f":"markdown","46edc3bb":"markdown","961a0e6d":"markdown","48d7de00":"markdown","e5a58fa5":"markdown"},"source":{"f11974d3":"import numpy as np\nimport pandas as pd\n\nfrom keras.models import Sequential, load_model\nfrom keras.layers import Dense, LSTM\nfrom keras.callbacks import ModelCheckpoint, TensorBoard\n\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import r2_score as R2_score\nfrom sklearn.preprocessing import MinMaxScaler\n\nimport matplotlib.pyplot as plt\nfrom matplotlib.ticker import MaxNLocator\n\nfrom IPython.display import display_html\n\nimport os\n","5a0db0ba":"data_train = pd.read_csv('..\/input\/daily-climate-time-series-data\/DailyDelhiClimateTrain.csv',sep=',')\ndata_train.head()","65f08388":"data_train.shape","ff78cfd8":"data_test = pd.read_csv('..\/input\/daily-climate-time-series-data\/DailyDelhiClimateTest.csv',sep=',')\ndata_test.head()","ff9cbe45":"data_test.shape","a9dc9d7a":"data = pd.concat([data_train,data_test])\ndata.head()","c002c7d6":"data.shape","f9d07ef3":"data.isnull().sum()","3607823b":"data.describe()","4d1ecadd":"data['date'] = pd.to_datetime(data['date'])","3fb4a695":"data = data.rename(columns={\"meantemp\":\"temp\",\"wind_speed\":\"wind\",\"meanpressure\":\"pressure\"})","6c75b96a":"data.head()","f7c5735a":"print(\"Starting date of time series: \", data.date.min())\nprint(\"Final date of time series:    \", data.date.max())","a756cdbd":"dates = data['date'].values\ntemp  = data['temp'].values\nhumidity = data['humidity'].values\nwind = data['wind'].values\npressure = data['pressure'].values","b6e9995f":"plt.figure(figsize=(15,5))\nplt.plot(dates, temp)\nplt.title('Temperature average',\n          fontsize=20);","69122ec8":"plt.figure(figsize=(15,5))\nplt.plot(dates, temp, 'o-')\nplt.title('Temperature average (3 hours interval)', fontsize=20)\nplt.axis([dates[-150],dates[-1],0,50]);","a46c27cb":"plt.figure(figsize=(15,5))\nplt.subplot(1,2,1)\nplt.hist(temp, bins=30)\nplt.xlabel('Temperature', fontsize=20)\nplt.subplot(1,2,2)\naux = np.log( temp[1:] \/ temp[0:-1]  )\nplt.hist(aux, bins=30)\nplt.xlabel('Logarithmic temperature increment', fontsize=20)\nplt.show()\nprint(\"Temperature average                      :\", temp.mean())\nprint(\"Logarithmic temperature increment average:\", aux.mean())","2055fa00":"plt.figure(figsize=(15,5))\nplt.plot(dates[1:], aux)\nplt.title('Temperature (Logarithmic)',\n          fontsize=20);","1a3aef79":"eps = 1e-11\n\nNAN = np.NAN\n\n# Logarithmic transformation\n\ndef transform_logratios(serie):\n    aux = np.log((serie[1:]+eps) \/ (serie[0:-1]+eps))\n    return np.hstack( ([NAN], aux))\ndef inverse_transform_logratios(log_ratio, temp_prev):\n    return np.multiply(temp_prev, np.exp(log_ratio))","d878fa99":"transform = transform_logratios\ninverse_transform = inverse_transform_logratios","8bc6b874":"scaler = MinMaxScaler()\ntransformed = scaler.fit_transform(data.loc[:, [\"humidity\",\"wind\",\"pressure\"]])","bfbcdf4d":"transformed = pd.DataFrame(transformed, columns = [\"humidity_s\",\"wind_s\",\"pressure_s\"])\ntransformed.head()","6324b9d0":"humidity_s = transformed['humidity_s'].values\nwind_s = transformed['wind_s'].values\npressure_s = transformed['pressure_s'].values","87f619fd":"def winnowing(series, target, prev_known,\n               W_in=1, W_out=1):\n    n = len(series[0])\n    dataX = NAN*np.ones((n,W_in,len(series)))\n    if np.sometrue([s.dtype == object for s in series]):\n        dataX = dataX.astype(object)\n    if W_out==1:\n        dataY = series[target].copy()\n    else:\n        dataY = NAN*np.ones((n,W_out))\n        if series[target].dtype == object:\n            dataY = dataY.astype(object)\n        dataY[:,0] = series[target].copy()\n        for i in range(1,W_out):\n            dataY[:-i,i] = dataY[i:,0].copy()\n    \n    for i in range(n):\n        for j,s in enumerate(prev_known):\n            int_s = int(s) \n            ini_X = max([0,W_in-i-int_s])\n            dataX[i, ini_X:,j] = \\\n            series[j][max([0,i-W_in+int_s]):min([n,i+int_s])]\n    \n    return dataX, dataY\n","1035a739":"def my_dfs_display(dfs,names):\n    df_styler = []\n    for df,n in zip(dfs,names):\n        df_styler.append(df.style.set_table_attributes(\"style='display:inline'\").\\\n                         set_caption(n))\n    display_html(df_styler[0]._repr_html_()+\"__\"+df_styler[1]._repr_html_(),\n                 raw=True)","e84efef0":"def info_winnowing(X,Y,names_series,name_target,times=None):\n    c0  = '\\033[1m'  \n    c1  = '\\033[0m'  \n    W_in = X.shape[1]\n    if len(Y.shape)==1:\n        W_out = 1\n    else:\n        W_out = Y.shape[1]\n    print(len(X), \"windows created \\n\")\n    print(\"X.shape={}\".format(X.shape),\" Y.shape={}\".format(Y.shape),\"\\n\")\n    for t in range(len(X)):\n        print(c0,\"Window %d:\"%t, c1)\n        if times is None:\n            names_ts = [\"t=\"+str(t+i-W_in) for i in range(W_in)]\n            names_ts_pred = [\"t=\"+str(t+i) for i in range(W_out)]\n        else:\n            times = list(times)\n            if (t-W_in)<0:\n                names_ts = [\"?\"+str(i) for i in range(W_in-t)] + times[:t]\n            else:\n                names_ts = times[(t-W_in):t]\n            if (t+W_out-1)>=len(times):\n                names_ts_pred = times[t:] + [\"?\"+str(i) for i in range(W_out-(len(times)-t))]\n            else:\n                names_ts_pred = times[t:(t+W_out)]\n        aux1 = pd.DataFrame(X[t].T,columns=names_ts,index=names_series)\n        aux2 = pd.DataFrame([Y[t]],columns=names_ts_pred,\n                            index=[name_target])\n        if W_out==1:\n            my_dfs_display((aux1,aux2),\n                           (\"X[{}].shape={}\".format(t,X[t].shape),\n                            \"Y[{}]={}\".format(t,Y[t])))\n        else:\n            my_dfs_display((aux1,aux2),\n                           (\"X[{}].shape={}\".format(t,X[t].shape),\n                            \"Y[{}].shape={}\".format(t,Y[t].shape)))\n","bda33f02":"logratio_temp = transform(temp)\n\nseries = [logratio_temp, humidity_s, wind_s, pressure_s]\nprev_known = [False, False, False, False]","657f1d4c":"print(np.shape(series))\nprint(np.shape(prev_known))","75b7ee2b":"lookback = 6  # Window_in\n\nX, y = winnowing (series, target=0, prev_known=prev_known,\n                  W_in=lookback)\n\nprint(X.shape, np.shape(y))","1c9e8ae3":"info_winnowing(X[:20],y[:20],\n                 names_series=[\"logratio_temp\",\n                                 \"humidity_s\", \"wind_s\",\n                                 \"pressure_s\"],\n                 name_target=\"logratio_temp\",\n                 times=dates)\n","a95614c2":"print(X.shape)\nprint(np.shape(temp))","db9ed2d5":"X_train = X[(lookback+1):len(data_train)]\ny_train = y[(lookback+1):len(data_train)]\ntemp_train = temp[(lookback+1):len(data_train)]\ntemp_test  = temp[len(data_train):]\nX_test  = X[len(data_train):]\ny_test  = y[len(data_train):]\n\nprint(np.shape(temp_train))\nprint(np.shape(temp_test))","ef912fee":"temp_prev_train =  np.hstack(( [NAN], temp_train[:-1]))\ntemp_prev_test  =  np.hstack(( temp_train[-1:],\n                                      temp_test[:-1]))\ndates_train     = dates[(lookback+1):len(data_train)]\ndates_test      = dates[len(data_train):]","311d87f9":"print(X_train.shape, y_train.shape)","928777cf":"model = Sequential()\nmodel.add(LSTM(10, input_shape=(lookback, X_train.shape[2]),\n#              kernel_regularizer='l1'\n              )\n         )\nmodel.add(Dense(1,\n#                kernel_regularizer='l1'\n               )\n         )\nmodel.compile(loss='mean_squared_error', optimizer='adam', metrics=['mse']) # 'RMSprop'\n# mean_absolute_error\n\nimport keras.backend as K\nprint(K.get_value(model.optimizer.lr))","fda8e095":"model.optimizer.lr","755e2286":"model.summary()","dbaf5bbb":"def training_graphic(tr_mse, val_mse):\n    ax=plt.figure(figsize=(10,4)).gca()\n    plt.plot(1+np.arange(len(tr_mse)), tr_mse)\n    plt.plot(1+np.arange(len(val_mse)), val_mse)\n    plt.title('mse', fontsize=18)\n    plt.xlabel('time', fontsize=18)\n    plt.ylabel('mse', fontsize=18)\n    plt.legend(['Training', 'Validation'], loc='upper left')\n    ax.xaxis.set_major_locator(MaxNLocator(integer=True))\n    plt.show()","12649c22":"epochs = 200\nbatch_size = 64\nNval = 200\ncontrol_val = True\nsave_training_tensorboard = False\n\n\nif not control_val:\n    history = model.fit(X_train, y_train, epochs=epochs,\n                        batch_size=batch_size, verbose=2)\n    \nelse:    \n    acum_tr_mse = []\n    acum_val_mse = []\n    filepath=\"best_model.h5\"\n    checkpoint = ModelCheckpoint(filepath, monitor='val_mse', verbose=2,\n                                 save_best_only=True,\n                                 mode='min') \n\n    if save_training_tensorboard:\n        callbacks_list = callbacks + [checkpoint]\n    else:\n        callbacks_list = [checkpoint]\n    \n    for e in range(epochs):\n        history = model.fit(X_train[:-Nval], y_train[:-Nval],\n                            batch_size=batch_size,\n                            epochs=1,\n                            callbacks=callbacks_list,\n                            verbose=0,\n                            validation_data=(X_train[-Nval:], y_train[-Nval:]))\n        \n        acum_tr_mse  += history.history['mse']\n        acum_val_mse += history.history['val_mse']\n        \n        if (e+1)%50 == 0:\n            training_graphic(acum_tr_mse, acum_val_mse)","d3c4c7cf":"model = load_model('best_model.h5') ","bbbeb297":"y_train_prediction = model.predict(X_train).flatten()\ny_test_prediction = model.predict(X_test).flatten()","d4369502":"temp_train_pred = inverse_transform(y_train_prediction,\n                                          temp_prev_train)\ntemp_test_pred  = inverse_transform(y_test_prediction,\n                                          temp_prev_test)","aad09ac7":"temp_train_pred","60229c40":"plt.figure(figsize=(15,7))\nplt.plot(dates_train, temp_train, '--', c='royalblue',\n         label=\"Training\")\nplt.plot(dates_train, temp_train_pred,  c='darkorange',\n         label=\"Training daily predictions\")\n\nplt.xticks(fontsize=12)\nplt.yticks(fontsize=14)\nplt.axis([dates_train[4],dates_train[-1],0,75])\nplt.legend(fontsize=14);","6f4f48fa":"plt.figure(figsize=(15,5))\nplt.plot(dates_train, temp_train, '--', c='royalblue',\n         label='Training')\nplt.plot(dates_train, temp_train_pred,  c='darkorange',\n         label='Training predictions')\nplt.plot(dates_test, temp_test, '--',   c='green',\n         label='Test')\nplt.plot(dates_test, temp_test_pred,    c='red',\n         label='Test predictions')\nplt.xticks(fontsize=12)\nplt.yticks(fontsize=14)\nplt.title('Daily predictions', fontsize=16)\nplt.legend(fontsize=14);","69643ce1":"plt.figure(figsize=(15,5))\nplt.plot(dates_train, temp_train, '--', c='royalblue',\n         label='Training')\nplt.plot(dates_train, temp_train_pred,  c='darkorange',\n         label='Training predictions')\nplt.plot(dates_test, temp_test, '--',   c='green',\n         label='Test')\nplt.plot(dates_test, temp_test_pred,    c='red',\n         label='Test predictions')\nplt.title('Daily predictions (zoom)', fontsize=16)\nplt.legend(fontsize=14)\nplt.xticks(fontsize=12)\nplt.yticks(fontsize=14)\nplt.axis([dates_train[-200],dates_test[-100],0,50]);","23cf055e":"# R2 scores\nprint(\"R2 - Training      : \",\n      R2_score(temp_train[1:], temp_train_pred[1:]))\nprint(\"R2 - Test          : \",\n      R2_score(temp_test, temp_test_pred))\nprint(\"r2 - Interval 1 day     : \",\n      R2_score(temp_test[1:], temp_test[:-1]))\nprint(\"R2 - Interval 1 week : \",\n      R2_score(temp_test[7:], temp_test[:-7]))\nprint(\"R2 - Interval 4 weeks: \",\n      R2_score(temp_test[28:], temp_test[:-28]))\nprint(\"R2 - Interval 1 year: \",\n      R2_score(temp_train[7*52:], temp_train[:-7*52]))","db5621fb":"# RMSEs\nsqrt = np.sqrt\nprint(\"RMSE - Training      : \",\n      sqrt(mean_squared_error(temp_train[1:],\n                              temp_train_pred[1:])))\nprint(\"RMSE - Test          : \",\n      sqrt(mean_squared_error(temp_test,\n                              temp_test_pred)))\nprint(\"RMSE - Interval 1 day    : \",\n      sqrt(mean_squared_error(temp_test[1:],\n                              temp_test[:-1])))\nprint(\"RMSE - Interval 1 week : \",\n      sqrt(mean_squared_error(temp_test[7:],\n                              temp_test[:-7])))\nprint(\"RMSE - Interval 4 weeks: \",\n      sqrt(mean_squared_error(temp_test[28:],\n                              temp_test[:-28])))","1bf39149":"In order to work with Keras, the data needs to be reshaped and treated. The variables needs to be converted to dummies (not in our case since the dataset we are working with doens\u00b4t need any conversion) and also, very important, needs to be winnowed. \n\nIt means, Keras will process the data in different windows and we have to define this windows. With that purpose, I will use the following functions where: \n\n* series: all variables involved in our model, including the one to be predicted. \n* target: variable to predict.\n* prev_known: variable that can be used in advance to improve our model. For example, we could add information related to weekends or holidays which could shed some information to the purpose of a model. If we would like to know the sales of a supermarket, it is not the same during the week or weekend. It is important to make sure we can use these variable in advance also in a supposed production model (none of these variable in this kernel since weather variables cannot be known previously).\n* W_in: it is the window or frecuency we will split our dataset.\n* W_out: it is the exit window, which is one since we just want to know the next day prediction. ","7dadab4a":"## <font color=\"#fcc200\"> 2. Import libraries","56eb0d3e":"Let\u00b4s plot the temperature during the two years time frame. \nWe can see the cyclical behavior each year, obviously the temperature gets higher in summer and goes down in winter. ","ab7ca7ea":"We just need to apply our model to get the predictions undoing the logarithmic transformation and also the prediction visualizations and the different errors depending on the future time frames. ","584a0ad5":"## <font color=\"#fcc200\"> 1. Introduction","7d8a8507":"# Times series with LSTM ","a9410ef1":"Finally, the network needs to be created. It is a really simple network where the window size needs to be again specify. ","835f8321":"I will convert the date column to datetime and rename some of the columns to simplify the following process. ","2cb87459":"Zooming the graphic above we see how the temperature changes from one day to another but it has a tendency depending on the period of the year. ","fa753baa":"The goal of this kernel is to work with times series using Keras. I will develop an easy example using a dataset with information related to the weather in Dehli from 2013 till 2017. \nThe objective will be to predict the temperature having some daily information as the wind speed, pressure and humidity. \nWith this purpose, I will preprocess the data, transforming variables and reshaping the data in order to work with Keras. Finally, I will create a LSTM neural network which will be trained and saved. It can be used for futures predictions and result visualizations.","fe030b52":"## <font color=\"#fcc200\"> 9. Predictions","8a888d54":"I will create the function used to transform the original variable (temperature) and also the one needed to undo the transformation. This second function will be used  to obtain the final predictions.","21811985":"## <font color=\"#fcc200\"> 6. Data winnowing","c60ff823":"## <font color=\"fcc200\"> 5. Variable transformation","d8d92187":"## <font color=\"#fcc200\"> 3. Load data","b60852fe":"## <font color=\"#fcc200\"> 8. Model with Keras","5c6735df":"I have used a window of 6 days. After trying different windows, 6 days is the one which gives me a better prediction.\n\nWe can see below the final structure created and how it will be passed to the neural network. We see how we have a 6 days window with 4 variables for each day.","c8c637ab":"The idea is to transform the variable to be predicted. I will use a logarithmic transformation but first let\u00b4s plot the histogram of the original variable and the transformed one. \n\nThe purpose of this transformation is to standarizated the original variable and also supressed possible outliers. ","7fa4d50c":"## <font color=\"#fcc200\"> 7. Training and test sets","4b7f834f":"With the purpose of training and saving the final network, the next functions will be implemented to choose the one that gives the better result (lowest mean square error).","46edc3bb":"## <font color=\"#fcc200\"> 4. Initial preprocessing and data visualization","961a0e6d":"We can load the model already trained and just makes predictions with it.","48d7de00":"Next, I will split the dataset in training and test.","e5a58fa5":"There are two different datasets: training and test. Both have the same structure, containing the data collected in Dehli from  January 2013 to April 2017.\n\nThe columns of both datasets are teh same. See description below: \n\n* date: Date of format YYYY-MM-DD.\n* meantemp: Mean temperature averaged out from multiple 3 hour intervals in a day.\n* humidity: Humidity value for the day (units are grams of water vapor per cubic meter volume of air).\n* wind_speed: Wind speed measured in kmph.\n* meanpressure: Pressure reading of weather (measure in atm)\n    \nTo make it easier, I will concatenate both sets and just work with a single dataframe."}}