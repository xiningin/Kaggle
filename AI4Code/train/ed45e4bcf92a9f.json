{"cell_type":{"c5663091":"code","df24e21d":"code","19b72c9a":"code","50b72584":"code","a9789e62":"code","b3f38f67":"code","b718d5ea":"code","a60445a9":"code","6998793c":"code","22585dd2":"code","6b4dbbe9":"code","d1205c8d":"code","995f3037":"code","2dad2d1f":"code","03370568":"code","37aaf1be":"code","da7456da":"code","09dff2ee":"code","f9e2bd0b":"code","0ed23fe7":"code","be6dcf08":"code","1762dcc6":"code","a0b95b09":"code","7fa33d11":"code","aac01a87":"code","ad2baa8e":"code","27cc06ef":"code","189f3648":"code","861bb4bd":"code","482ebcdb":"code","143386e1":"code","36dff6a2":"code","a49ac209":"code","31567d31":"code","1f5db97c":"code","5931ceb0":"code","ffb6247e":"code","f24b0c44":"code","46114747":"code","b03af86e":"code","16c3b5ea":"code","9d079dc5":"code","904821b3":"code","992440d1":"code","37e85093":"code","0359496f":"code","413eb979":"code","207ffbbe":"code","5bef2c76":"code","a713dcb8":"code","4be2f788":"code","35a213ec":"code","0a629268":"code","74b5d0a1":"code","2d1acd4d":"code","97f70171":"code","cea3e07d":"code","febe0a99":"code","cd82a564":"code","2c2f407d":"code","5895297b":"code","78c28f3b":"code","44a5e45d":"code","96a37132":"code","999b6c32":"code","35e79a74":"code","6ab5d31f":"code","f688e7af":"code","22f0ab74":"code","4fd57d7c":"code","b851af13":"code","ed6d59c7":"code","dbd3bff5":"code","47f122c6":"code","b286da0b":"code","4339418d":"code","bd44cf8c":"code","0617c351":"code","21aea5fa":"code","75bb0968":"code","3d4797b7":"code","f8df187f":"code","f16339eb":"code","2625b744":"code","ba5c42f0":"code","378a18d0":"code","d468b5e8":"code","05174e81":"code","04f51446":"code","1c0d2fcc":"code","a21b5da9":"code","65ba2168":"code","c94a442a":"code","442aa9f9":"code","4c44b1cd":"code","67b072b8":"code","659fe4f8":"code","1c3c1e70":"code","b25c5ce1":"code","15e43dd0":"code","6d738bdb":"code","3d3e307f":"code","e0d4d131":"code","ba94e5a6":"code","109c612d":"code","3ad78076":"code","38e56ac5":"code","c5198aea":"code","416fe713":"code","40f1db4f":"code","50056cc3":"code","c489eb95":"code","452ad35a":"code","6a3bdb58":"code","04bdf517":"code","0329335b":"code","6e1e4d58":"code","d5e87a6c":"code","e4a5f15a":"code","1e4f17d4":"code","522c320f":"code","76f1b34e":"code","32fb0498":"code","0de27fff":"code","d74e62f6":"code","d597c1d1":"code","53c6c006":"code","f5e6cd9f":"code","adf1ca53":"code","b45d2b3b":"code","c0ee3d4f":"code","fa4c5220":"code","62bad787":"code","6e2aecd8":"code","1b5a4f0b":"code","44335c4f":"code","b7c49fc9":"markdown","9bd9975c":"markdown","ef514fc8":"markdown","e816570c":"markdown","c93a5d40":"markdown","cb3a73c3":"markdown","f05cf5c4":"markdown","a8a3eed7":"markdown","f0089e22":"markdown","d3de0e80":"markdown","2e63fa73":"markdown","13899d76":"markdown","25739707":"markdown","a883579e":"markdown","b6454832":"markdown","57df7438":"markdown","7a105ddf":"markdown","bcb491ed":"markdown","05b26d6d":"markdown","69260703":"markdown","da7b61c7":"markdown","35a60e87":"markdown","7741e1b8":"markdown","bfe61329":"markdown","76d4c6a3":"markdown","522f9915":"markdown","0baf3917":"markdown","aa225ad2":"markdown","c1ed049e":"markdown","45934814":"markdown","25446293":"markdown","02344942":"markdown","ed10fcfb":"markdown","28644ffe":"markdown","25cbb64b":"markdown","38fbd5a3":"markdown","ded6b9ed":"markdown","565fab47":"markdown","88910166":"markdown","9870c50e":"markdown","0602b10f":"markdown","de2565e0":"markdown","320eb10b":"markdown","2c7ddf3f":"markdown","f3280b4c":"markdown","ff449f7a":"markdown","43d654f4":"markdown","f3912cdc":"markdown","31e2e06e":"markdown","06bdb25e":"markdown","38b99b71":"markdown","cfc3b77c":"markdown","a2787890":"markdown","035cd036":"markdown","c4081954":"markdown","2233a3ab":"markdown","c6b13443":"markdown","bf7a4e7f":"markdown","6ea44cad":"markdown","278d4b32":"markdown","230fd3a8":"markdown","25363b0e":"markdown","36d724ce":"markdown","4be15246":"markdown"},"source":{"c5663091":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\n\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import OrdinalEncoder\n\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nfrom scipy.stats import skew\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import r2_score\n\n\n\n\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.svm import SVR as SupportVectorRegression\nfrom sklearn.linear_model import ARDRegression\nfrom sklearn.linear_model import BayesianRidge\nfrom sklearn.linear_model import Ridge, RidgeCV\nfrom lightgbm import LGBMRegressor\nfrom xgboost import XGBRegressor\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor, BaggingRegressor\nfrom sklearn.ensemble import VotingRegressor\n\nimport itertools\nfrom math import sqrt\nimport copy","df24e21d":"import warnings\nwarnings.filterwarnings('ignore')","19b72c9a":"#reading csv file of train and test data\ntrain_df = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest_df = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')","50b72584":"#saving the test id for submission\ntest_ID = test_df['Id']","a9789e62":"print(train_df.head())","b3f38f67":"print(train_df.describe)","b718d5ea":"train_df.info()","a60445a9":"#SalePrice histogram\nfig = px.histogram(train_df, x=\"SalePrice\", marginal=\"violin\", hover_data=train_df.columns, width=800, height=500, template=\"plotly_dark\")\nfig.show()","6998793c":"#skewness of target\nprint(train_df['SalePrice'].skew())","22585dd2":"#OverallQual vs SalePrice\nfig = px.scatter(train_df, x=\"OverallQual\", y='SalePrice',hover_data=train_df.columns, width=800, height=500, template=\"plotly_dark\")\nfig.show()","6b4dbbe9":"#OverallQual histogram\nfig = px.histogram(train_df, x=\"OverallQual\", nbins=15,marginal=\"violin\", hover_data=train_df.columns, width=800, height=500, template=\"plotly_dark\")\nfig.show()","d1205c8d":"#Overall skewness\nprint(train_df['OverallCond'].skew())","995f3037":"#GrLivArea vs SalePrice\nfig = px.scatter(train_df, x=\"GrLivArea\", y='SalePrice', hover_data=train_df.columns, width=800, height=500, template=\"plotly_dark\")\nfig.show()","2dad2d1f":"#GrLivArea histogram\nfig = px.histogram(train_df, x=\"GrLivArea\", nbins=15,marginal=\"violin\", hover_data=train_df.columns, width=800, height=500, template=\"plotly_dark\")\nfig.show()","03370568":"#GrLivArea skewness\nprint(train_df['GrLivArea'].skew())","37aaf1be":"#pairplotting for other features\ncols = ['TotalBsmtSF', '1stFlrSF', 'MasVnrArea', 'TotRmsAbvGrd', 'YearBuilt', 'SalePrice']\nfig = px.scatter_matrix(train_df[cols])\nfig.update_layout(height=600, width=1290, template=\"plotly_dark\")\nfig.show()","da7456da":"fig = px.histogram(train_df, x=\"SalePrice\", marginal=\"violin\", hover_data=train_df.columns, width=800, height=500, template=\"plotly_dark\")\nfig.show()","09dff2ee":"print(train_df['SalePrice'].skew())","f9e2bd0b":"train_df['SalePrice'] = np.log1p(train_df['SalePrice'])\n\n#train_df['SalePrice'] = boxcox1p(train_df['SalePrice'], 0) ","0ed23fe7":"fig = px.histogram(train_df, x=\"SalePrice\", marginal=\"violin\", hover_data=train_df.columns, width=800, height=500, template=\"plotly_dark\")\nfig.show()","be6dcf08":"print(train_df['SalePrice'].skew())","1762dcc6":"#combining train and test data into the same dataframe\n\n#concatting train and test dataframes\ntrain_rows = train_df.shape[0]\ny_train = train_df['SalePrice']\nall_data = pd.concat((train_df.drop(['SalePrice'], axis=1), test_df)).reset_index(drop=True)","a0b95b09":"#checking all data info\nall_data.info()","7fa33d11":"all_data[['MSZoning', 'LotFrontage', 'Alley', 'Utilities', 'Exterior1st', 'Exterior2nd', 'MasVnrType', 'MasVnrArea', 'BsmtQual',\n'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinSF1', 'BsmtFinType2', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF',\n'Electrical', 'BsmtFullBath', 'BsmtHalfBath', 'KitchenQual', 'Functional', 'FireplaceQu', 'GarageType', 'GarageYrBlt', 'GarageFinish',\n'GarageCars', 'GarageArea', 'GarageQual', 'GarageCond', 'PoolQC', 'Fence', 'MiscFeature', 'SaleType']].info()","aac01a87":"LotFrontge_train = all_data[['LotFrontage', 'Neighborhood']][:train_rows]\nLotFrontge_test = all_data[['LotFrontage', 'Neighborhood']][train_rows:]\n\nLotFrontge_train['LotFrontage'] = LotFrontge_train.groupby('Neighborhood')['LotFrontage'].transform(lambda x: x.fillna(x.median()))\nLotFrontge_test['LotFrontage'] = LotFrontge_test.groupby('Neighborhood')['LotFrontage'].transform(lambda x: x.fillna(x.median()))\n\nLotFrontge_train.drop('Neighborhood', axis=1, inplace=True)\nLotFrontge_test.drop('Neighborhood', axis=1, inplace=True)\n\nLotFrontage = pd.concat((LotFrontge_train, LotFrontge_test)).reset_index(drop=True)\n\nall_data['LotFrontage'] = LotFrontage","ad2baa8e":"all_data[\"MasVnrArea\"] = all_data[\"MasVnrArea\"].fillna(0)","27cc06ef":"all_data[\"BsmtFinSF1\"] = all_data[\"BsmtFinSF1\"].fillna(0)\nall_data[\"BsmtFinSF2\"] = all_data[\"BsmtFinSF2\"].fillna(0)","189f3648":"all_data[\"BsmtUnfSF\"] = all_data[\"BsmtUnfSF\"].fillna(0)\nall_data[\"TotalBsmtSF\"] = all_data[\"TotalBsmtSF\"].fillna(0)","861bb4bd":"all_data[\"BsmtFullBath\"] = all_data[\"BsmtFullBath\"].fillna(0)\nall_data[\"BsmtHalfBath\"] = all_data[\"BsmtHalfBath\"].fillna(0)","482ebcdb":"all_data[\"GarageYrBlt\"] = all_data[\"GarageYrBlt\"].fillna(0)\nall_data[\"GarageCars\"] = all_data[\"GarageCars\"].fillna(0)\nall_data[\"GarageArea\"] = all_data[\"GarageArea\"].fillna(0)","143386e1":"all_data['MSZoning'] = all_data['MSZoning'].fillna(all_data['MSZoning'].mode()[0])","36dff6a2":"all_data.drop('Utilities', axis=1, inplace=True)","a49ac209":"all_data['Exterior1st'] = all_data['Exterior1st'].fillna(all_data['Exterior1st'].mode()[0])\nall_data['Exterior2nd'] = all_data['Exterior2nd'].fillna(all_data['Exterior2nd'].mode()[0])","31567d31":"all_data['Electrical'] = all_data['Electrical'].fillna(all_data['Electrical'].mode()[0])\nall_data['KitchenQual'] = all_data['KitchenQual'].fillna(all_data['KitchenQual'].mode()[0])","1f5db97c":"all_data[\"Functional\"] = all_data[\"Functional\"].fillna(\"Typ\")","5931ceb0":"all_data['SaleType'] = all_data['SaleType'].fillna(all_data['SaleType'].mode()[0])","ffb6247e":"replace_na_none = ['Alley', 'MasVnrType', 'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', 'FireplaceQu', 'GarageType', \n                   'GarageFinish', 'GarageQual', 'GarageCond', 'PoolQC', 'Fence', 'MiscFeature']\n\nfor feature in replace_na_none:\n  all_data[feature] = all_data[feature].fillna(\"None\")","f24b0c44":"#printing all categorical features\nall_data.select_dtypes('object').columns","46114747":"non_ordinal_features = ['MSZoning', 'Street', 'LotShape', 'LandContour', 'LotConfig', 'LandSlope', 'Neighborhood', 'Condition1', 'Condition2', 'BldgType',\n'HouseStyle', 'RoofStyle', 'RoofMatl', 'Exterior1st', 'Exterior2nd', 'Foundation', 'Heating', 'CentralAir', 'Electrical',\n'Functional', 'PavedDrive', 'SaleType', 'SaleCondition']\n\nfor feature in non_ordinal_features:\n  le = LabelEncoder()\n  le.fit(all_data[feature].values)\n  all_data[feature] = le.transform(all_data[feature].values)","b03af86e":"for col in all_data.columns:\n  if col in ['FireplaceQu', 'BsmtQual', 'BsmtCond', 'GarageQual', 'GarageCond']:\n    oe = OrdinalEncoder(categories=[['None','Po','Fa','TA','Gd','Ex']])\n    oe.fit(all_data.loc[:,[col]])\n    all_data[col] = oe.transform(all_data.loc[:,[col]])\n  \n  elif col in ['BsmtFinType1', 'BsmtFinType2']:\n    oe = OrdinalEncoder(categories=[['None','Unf','LwQ','Rec','BLQ','ALQ','GLQ']])\n    oe.fit(all_data.loc[:,[col]])\n    all_data[col] = oe.transform(all_data.loc[:,[col]])\n\n  elif col in ['HeatingQC', 'ExterCond', 'ExterQual', 'KitchenQual']:\n    oe = OrdinalEncoder(categories=[['Po','Fa','TA','Gd','Ex']])\n    oe.fit(all_data.loc[:,[col]])\n    all_data[col] = oe.transform(all_data.loc[:,[col]])\n  \n  elif col == 'Alley':\n    oe = OrdinalEncoder(categories=[['None','Pave','Grvl']])\n    oe.fit(all_data.loc[:,[col]])\n    all_data[col] = oe.transform(all_data.loc[:,[col]])\n\n  elif col == 'BsmtExposure':\n    oe = OrdinalEncoder(categories=[['None','No','Mn','Av','Gd']])\n    oe.fit(all_data.loc[:,[col]])\n    all_data[col] = oe.transform(all_data.loc[:,[col]])  \n\n  elif col == 'GarageType':\n    oe = OrdinalEncoder(categories=[['None','Detchd','CarPort','BuiltIn','Basment','Attchd','2Types']])\n    oe.fit(all_data.loc[:,[col]])\n    all_data[col] = oe.transform(all_data.loc[:,[col]])\n\n  elif col == 'GarageFinish':\n    oe = OrdinalEncoder(categories=[['None','Unf','RFn','Fin']])\n    oe.fit(all_data.loc[:,[col]])\n    all_data[col] = oe.transform(all_data.loc[:,[col]]) \n\n  elif col == 'Fence':\n    oe = OrdinalEncoder(categories=[['None','MnWw','GdWo','MnPrv','GdPrv']])\n    oe.fit(all_data.loc[:,[col]])\n    all_data[col] = oe.transform(all_data.loc[:,[col]]) \n\n  elif col == 'PoolQC':\n    oe = OrdinalEncoder(categories=[['None','Fa','TA','Gd','Ex']])\n    oe.fit(all_data.loc[:,[col]])\n    all_data[col] = oe.transform(all_data.loc[:,[col]])\n\n  elif col == 'MiscFeature':\n    oe = OrdinalEncoder(categories=[['None','TenC','Shed','Othr','Gar2','Elev']])\n    oe.fit(all_data.loc[:,[col]])\n    all_data[col] = oe.transform(all_data.loc[:,[col]])\n\n  elif col == 'MasVnrType':\n    oe = OrdinalEncoder(categories=[['None','Stone','CBlock','BrkFace','BrkCmn']])\n    oe.fit(all_data.loc[:,[col]])\n    all_data[col] = oe.transform(all_data.loc[:,[col]])","16c3b5ea":"#making sure all data is now numeric\nall_data.dtypes.unique()","9d079dc5":"all_data['TotalSF'] = all_data['TotalBsmtSF'] + all_data['1stFlrSF'] + all_data['2ndFlrSF']","904821b3":"train_df = all_data[:train_rows].join(y_train)\ntest_df = all_data[train_rows:]","992440d1":"#correlation matrix for the target and the features\ncorr_matrix=train_df.corr()\ntarget_corr_list = corr_matrix[\"SalePrice\"].sort_values(ascending=False)","37e85093":"#printing correlation matrix in descinding order\nwith pd.option_context('display.max_rows', None, 'display.max_columns', None):\n    print(target_corr_list)","0359496f":"#obtaining features that have the lowest correlation\nlowest_corr_matrix = round(abs(train_df.corr()), 2)\nzero_target_corr_list = lowest_corr_matrix[\"SalePrice\"].sort_values()","413eb979":"zero_target_corr_list.head(8)","207ffbbe":"#removing columns with low correlation\ntrain_df.drop(['Condition2', 'BsmtFinSF2', 'BsmtHalfBath', 'MiscVal','Id'], axis=1, inplace=True)\ntest_df.drop(['Condition2', 'BsmtFinSF2', 'BsmtHalfBath', 'MiscVal','Id'], axis=1, inplace=True)","5bef2c76":"#remove 'TotalBsmtSF', '1stFlrSF' and '2ndFlrSF' since we added the 'TotalSF' feature to represent them\ntrain_df.drop(['TotalBsmtSF', '1stFlrSF','2ndFlrSF'], axis=1,inplace=True)\ntest_df.drop(['TotalBsmtSF', '1stFlrSF','2ndFlrSF'], axis=1,inplace=True)","a713dcb8":"#removing 'PoolQC', 'PoolArea', 'Street' and '3SsnPorch' from the data since there is no much variance in them, so the models wouldn't be able to observe a pattern from them.\ntrain_df.drop(['PoolQC', 'PoolArea', 'Street', '3SsnPorch'], axis=1,inplace=True)\ntest_df.drop(['PoolQC', 'PoolArea', 'Street', '3SsnPorch'], axis=1,inplace=True)","4be2f788":"#creating the heatmap\nmask = np.zeros_like(train_df.corr())\ntriangle_indices = np.triu_indices_from(mask)\nmask[triangle_indices] = True\n\n\nplt.figure(figsize=(100,100))\nsns.heatmap(train_df.corr(), mask=mask, annot=True, annot_kws={\"size\": 14})\nsns.set_style('white')\nplt.xticks(fontsize=14)\nplt.yticks(fontsize=14)\nplt.show()","35a213ec":"#function to calculate VIF \ndef calc_vif(X):\n    vif = pd.DataFrame()\n    vif[\"variables\"] = X.columns\n    vif[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n\n    return vif","0a629268":"VIF = calc_vif(train_df.iloc[:, :-1])","74b5d0a1":"with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n    print(VIF)","2d1acd4d":"train_df.drop(['GarageArea', 'GrLivArea', 'GarageYrBlt', 'Fireplaces', 'ExterQual','MSSubClass'], axis=1, inplace=True)\ntest_df.drop(['GarageArea', 'GrLivArea', 'GarageYrBlt', 'Fireplaces', 'ExterQual','MSSubClass'], axis=1, inplace=True)","97f70171":"VIF = calc_vif(train_df.drop(['SalePrice'], axis=1))","cea3e07d":"with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n    print(VIF)","febe0a99":"y_train = train_df['SalePrice']\nall_data = pd.concat((train_df.drop(['SalePrice'], axis=1), test_df)).reset_index(drop=True)","cd82a564":"#obtaining skewness for all features\n\nskewed_features = all_data[all_data.columns].apply(lambda x: skew(x.dropna())).sort_values(ascending=False)\nskewness = pd.DataFrame({'Skew' :skewed_features})\nskewness.head(10)","2c2f407d":"skewness = skewness[abs(skewness) > 0.5]\nprint(f'Number of features that have absloute skewness > 0.5: {skewness.shape[0]} features')","5895297b":"fig = px.histogram(all_data, x=\"LotArea\", marginal=\"violin\", hover_data=all_data.columns, width=800, height=500, template=\"plotly_dark\")\nfig.show()","78c28f3b":"skewed_features = skewness.index","44a5e45d":"for index in skewed_features:\n    all_data[index] = np.log1p(all_data[index])","96a37132":"fig = px.histogram(all_data, x=\"LotArea\", marginal=\"violin\", hover_data=all_data.columns, width=800, height=500, template=\"plotly_dark\")\nfig.show()","999b6c32":"train_df = all_data[:train_rows].join(y_train)\nX = train_df.drop('SalePrice', axis=1)\ny = train_df['SalePrice']","35e79a74":"test_df = all_data[train_rows:]","6ab5d31f":"df1 = pd.DataFrame(columns=('LR', 'DTR', 'SVR', 'ARDR', 'BR', 'RR', 'RR_CV', 'LGBM', 'XGB', 'RFR', 'GBR', 'Ada', 'BaggingR'))\n\nfor i in range(0,20):\n  X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2, random_state=i)\n  LR = LinearRegression()\n  DTR = DecisionTreeRegressor()\n  SVR = SupportVectorRegression()\n  ARDR = ARDRegression()\n  BR = BayesianRidge()\n  RR = Ridge()\n  RR_CV = RidgeCV()\n  LGBM = LGBMRegressor()\n  XGB = XGBRegressor(verbosity = 0)\n  RFR = RandomForestRegressor()\n  GBR = GradientBoostingRegressor()\n  Ada = AdaBoostRegressor()\n  BaggingR = BaggingRegressor()\n\n  df1.loc[i] = [LR.fit(X_train, y_train).score(X_test, y_test), DTR.fit(X_train, y_train).score(X_test, y_test), SVR.fit(X_train, y_train).score(X_test, y_test),\n                ARDR.fit(X_train, y_train).score(X_test, y_test), BR.fit(X_train, y_train).score(X_test, y_test), RR.fit(X_train, y_train).score(X_test, y_test),\n                RR_CV.fit(X_train, y_train).score(X_test, y_test), LGBM.fit(X_train, y_train).score(X_test, y_test), XGB.fit(X_train, y_train).score(X_test, y_test),\n                RFR.fit(X_train, y_train).score(X_test, y_test), GBR.fit(X_train, y_train).score(X_test, y_test), Ada.fit(X_train, y_train).score(X_test, y_test),\n                BaggingR.fit(X_train, y_train).score(X_test, y_test)]              ","f688e7af":"df1.sort_values(['LR', 'DTR', 'SVR', 'ARDR', 'BR', 'RR', 'RR_CV', 'LGBM', 'XGB', 'RFR', 'GBR', 'Ada', 'BaggingR'], ascending=False)","22f0ab74":"X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2, random_state=18)","4fd57d7c":"class GridSearch:\n\n\n  def __init__(self, model, parameter_grid):\n    self.model = model\n    self.parameter_grid = parameter_grid\n    self.best_model_ = None\n    self.best_params_ = dict()\n  \n  def generate_permutations(self):\n    keys = self.parameter_grid.keys()\n    vals = self.parameter_grid.values()\n    for instance in itertools.product(*vals):\n        yield dict(zip(keys, instance))\n\n  def fit_test(self, X_train, y_train, X_test, y_test):\n    dummy = list(self.generate_permutations())\n    keys = list(dummy[0].keys())\n    best_score = 0.0\n    scores = []\n    for para in dummy:\n      self.model.set_params(**para)\n      self.model.fit(X_train, y_train)\n      score = self.model.score(X_test, y_test)\n      scores.append(score)\n      if score >= best_score:\n        best_score = score\n        self.best_params_ = self.model.get_params(deep=False)\n        self.best_model_ = copy.deepcopy(self.model)\n    self.plot_curve(scores)\n\n  def plot_curve(self,scores):\n    fig = px.line(\n        pd.DataFrame({\n            'Iterations': range(len(scores)),\n            'Scores': scores\n        }),\n        x='Iterations',\n        y='Scores',\n        template=\"plotly_dark\",\n        width=800, height=500)\n    fig.update_layout(title={\n        'text': f'{type(self.model).__name__} Model Accuracy Graph',\n        'y':0.95,\n        'x':0.5,\n        'xanchor': 'center',\n        'yanchor': 'top'})\n    fig.show()","b851af13":"def reg_metrics(y_true, y_predict):\n  acc = r2_score(y_true, y_predict)\n  mse = mean_squared_error(y_true, y_predict)\n  print(f'The accuracy score: {acc}')\n  print(f'The root mean squred error: {sqrt(mse)}')","ed6d59c7":"LR = LinearRegression()\nLR.fit(X_train, y_train)","dbd3bff5":"reg_metrics(y_test, LR.predict(X_test))","47f122c6":"decision_tree_grid = {\n        'max_depth': range(1,20),\n        'min_samples_leaf': range(1,20)\n    }\ndecision_tree_grid_search = GridSearch(DecisionTreeRegressor(), decision_tree_grid)","b286da0b":"decision_tree_grid_search.fit_test(X_train, y_train, X_test, y_test)","4339418d":"DTR = decision_tree_grid_search.best_model_","bd44cf8c":"DTR.get_params","0617c351":"reg_metrics(y_test, DTR.predict(X_test))","21aea5fa":"SVR_parameter_grid = {\n         'C': [0.1,1, 10, 100], \n         'gamma': [1,0.1,0.01,0.001]\n         }\nSVR_grid_search = GridSearch(SupportVectorRegression(), SVR_parameter_grid)","75bb0968":"SVR_grid_search.fit_test(X_train, y_train, X_test, y_test)","3d4797b7":"SVR = SVR_grid_search.best_model_","f8df187f":"SVR.get_params","f16339eb":"reg_metrics(y_test, SVR.predict(X_test))","2625b744":"ARDR = ARDRegression()\nARDR.fit(X_train, y_train)","ba5c42f0":"reg_metrics(y_test, ARDR.predict(X_test))","378a18d0":"BR = BayesianRidge()\nBR.fit(X_train, y_train)","d468b5e8":"reg_metrics(y_test, BR.predict(X_test))","05174e81":"RR_parameter_grid = {\n    'alpha': [0.01, 0.1, 1, 10],\n    'solver': ['auto', 'svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag', 'saga']\n}\n\nRR_grid_search = GridSearch(Ridge(), RR_parameter_grid)","04f51446":"RR_grid_search.fit_test(X_train, y_train, X_test, y_test)","1c0d2fcc":"RR = RR_grid_search.best_model_","a21b5da9":"RR.get_params","65ba2168":"reg_metrics(y_test, RR.predict(X_test))","c94a442a":"RR_CV = RidgeCV()\nRR_CV.fit(X_train, y_train)","442aa9f9":"reg_metrics(y_test, RR_CV.predict(X_test))","4c44b1cd":"LGBM_parameter_grid = {\n    'num_leaves': [7, 14, 21, 28, 31, 50],\n    'learning_rate': [0.1, 0.03, 0.003],\n    'max_depth': [-1, 3, 5],\n    'n_estimators': [50, 100, 200, 500],\n}\n\nLGBM_grid_search = GridSearch(LGBMRegressor(), LGBM_parameter_grid)","67b072b8":"LGBM_grid_search.fit_test(X_train, y_train, X_test, y_test)","659fe4f8":"LGBM = LGBM_grid_search.best_model_","1c3c1e70":"LGBM.get_params","b25c5ce1":"reg_metrics(y_test, LGBM.predict(X_test))","15e43dd0":"XGB_parameter_grid = {\n \"learning_rate\": [0.05, 0.10, 0.15, 0.20, 0.25, 0.30],\n \"max_depth\": [3, 4, 5, 6, 8, 10, 12, 15],\n \"gamma\": [0.0, 0.1, 0.2 , 0.3, 0.4]\n}\n\nXGB_grid_search = GridSearch(XGBRegressor(verbosity=0), XGB_parameter_grid)","6d738bdb":"XGB_grid_search.fit_test(X_train, y_train, X_test, y_test)","3d3e307f":"XGB = XGB_grid_search.best_model_","e0d4d131":"XGB.get_params","ba94e5a6":"reg_metrics(y_test, XGB.predict(X_test))","109c612d":"RFR_paramter_grid = {\n 'max_depth': [10, 20, 30, 40, 50],\n 'min_samples_leaf': [1, 2, 4],\n 'n_estimators': [200, 400, 600, 800, 1000]\n}\nRFR_grid_search = GridSearch(RandomForestRegressor(), RFR_paramter_grid)","3ad78076":"RFR_grid_search.fit_test(X_train, y_train, X_test, y_test)","38e56ac5":"RFR = RFR_grid_search.best_model_","c5198aea":"RFR.get_params","416fe713":"reg_metrics(y_test, RFR.predict(X_test))","40f1db4f":"GB_paramter_grid = {\n    'n_estimators': [100,500,1000, 1500],\n    'max_depth': [4,6,8,10]\n}\nGB_grid_search = GridSearch(GradientBoostingRegressor(), GB_paramter_grid)","50056cc3":"GB_grid_search.fit_test(X_train, y_train, X_test, y_test)","c489eb95":"GB = GB_grid_search.best_model_","452ad35a":"GB.get_params","6a3bdb58":"reg_metrics(y_test, GB.predict(X_test))","04bdf517":"Ada_paramter_grid = {\n 'n_estimators': [50, 100],\n 'learning_rate' : [0.01,0.05,0.1,0.3,1],\n 'loss' : ['linear', 'square', 'exponential']\n}\nAda_grid_search = GridSearch(AdaBoostRegressor(), Ada_paramter_grid)","0329335b":"Ada_grid_search.fit_test(X_train, y_train, X_test, y_test)","6e1e4d58":"Ada = Ada_grid_search.best_model_","d5e87a6c":"Ada.get_params","e4a5f15a":"reg_metrics(y_test, Ada.predict(X_test))","1e4f17d4":"bagging_paramter_grid = {\n  \"max_samples\": [0.5, 1.0],\n  \"max_features\": [0.5, 1.0],\n  'n_estimators': np.arange(10,150,10)\n}\nbagging_grid_search = GridSearch(BaggingRegressor(), bagging_paramter_grid)","522c320f":"bagging_grid_search.fit_test(X_train, y_train, X_test, y_test)","76f1b34e":"BaggingR = bagging_grid_search.best_model_","32fb0498":"BaggingR.get_params","0de27fff":"reg_metrics(y_test, BaggingR.predict(X_test))","d74e62f6":"#creating the objects to that will go into the ensemble, using the parameters obtained from grid search\nSVR = SupportVectorRegression(C=10, cache_size=200, coef0=0.0, degree=3, epsilon=0.1, gamma=0.001,\n    kernel='rbf', max_iter=-1, shrinking=True, tol=0.001, verbose=False)\n \nRR = Ridge(alpha=0.01, copy_X=True, fit_intercept=True, max_iter=None,\n      normalize=False, random_state=None, solver='svd', tol=0.001)\n \nLGBM = LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n              importance_type='split', learning_rate=0.03, max_depth=-1,\n              min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,\n              n_estimators=200, n_jobs=-1, num_leaves=28, objective=None,\n              random_state=None, reg_alpha=0.0, reg_lambda=0.0, silent=True,\n              subsample=1.0, subsample_for_bin=200000, subsample_freq=0)\n \nXGB = XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n             colsample_bynode=1, colsample_bytree=1, gamma=0.0,\n             importance_type='gain', learning_rate=0.15, max_delta_step=0,\n             max_depth=4, min_child_weight=1, n_estimators=100,\n             n_jobs=1, nthread=None, objective='reg:linear', random_state=0,\n             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n             silent=None, subsample=1, verbosity=0)\n \nGB = GradientBoostingRegressor(alpha=0.9, ccp_alpha=0.0, criterion='friedman_mse',\n                          init=None, learning_rate=0.1, loss='ls', max_depth=4,\n                          max_features=None, max_leaf_nodes=None,\n                          min_impurity_decrease=0.0, min_impurity_split=None,\n                          min_samples_leaf=1, min_samples_split=2,\n                          min_weight_fraction_leaf=0.0, n_estimators=1000,\n                          n_iter_no_change=None, presort='deprecated',\n                          random_state=None, subsample=1.0, tol=0.0001,\n                          validation_fraction=0.1, verbose=0, warm_start=False)\n \nBaggingR = BaggingRegressor(base_estimator=None, bootstrap=True, bootstrap_features=False,\n                 max_features=0.5, max_samples=1.0, n_estimators=40,\n                 n_jobs=None, oob_score=False, random_state=None, verbose=0,\n                 warm_start=False)","d597c1d1":"ensemble_estimators = [('SVR', SVR), ('RR', RR), ('LGBM', LGBM), ('XGB', XGB), ('GB', GB), ('BaggingR', BaggingR)]\n#creating the ensemble with arbitrary weights, which will be optimized later\nensemble = VotingRegressor(estimators=ensemble_estimators, weights=[2,1,2,3,3,3])","53c6c006":"ensemble.fit(X_train, y_train)","f5e6cd9f":"ensemble.score(X_test, y_test)","adf1ca53":"df2 = pd.DataFrame(columns=('w1', 'w2','w3', 'w4','score'))\n\ni = 0\nfor w1 in range(1,5):\n    for w2 in range(1,5):\n      for w3 in range(1,5):\n        for w4 in range(1,5):\n          if len(set((w1,w2,w3,w4))) == 1:\n          # skip if all weights are equal\n            continue\n          ensemble = VotingRegressor(estimators=ensemble_estimators, weights=[w2, w1, w2, w4, w2, w3])\n          ensemble.fit(X_train, y_train)\n\n          df2.loc[i] = [w1, w2, w3, w4, ensemble.score(X_test, y_test)]\n          i += 1","b45d2b3b":"df2.sort_values('score', ascending=False)","c0ee3d4f":"ensemble = VotingRegressor(estimators=ensemble_estimators, weights=[1,1,1,2,1,2])","fa4c5220":"#X represents the train features, y is the target\nensemble.fit(X, y)","62bad787":"#making predictions\npred = ensemble.predict(test_df)","6e2aecd8":"#taking the inverse of log1p transformation\npred = np.expm1(pred)","1b5a4f0b":"sub = pd.DataFrame()\nsub['Id'] = test_ID\nsub['SalePrice'] = pred","44335c4f":"sub.to_csv('submission.csv', index=False)","b7c49fc9":"## Encoding Categorical Features","9bd9975c":"9 - XGB","ef514fc8":"After looking at the data alongside data description we divided encoding into two sections.","e816570c":"5 - Bayesian Ridge","c93a5d40":"'MasVnrArea' we will replace NaN values with 0, since it is most likely to mean that the house doesn't have masnory veneer.","cb3a73c3":"# EDA","f05cf5c4":"so from the previous list:\n\n1 - 'GarageCars' and 'GarageArea'. We will remove 'GarageArea'\n\n2 - 'TotalSF' with 'GrLivArea'. We will remove 'GrLivArea'\n\n3 - 'GarageYrBlt' with 'YrBlt'. We will remove  'GarageYrBlt'\n\n4 - 'FirePlaceQual' with 'FirePlaces'. We will remove 'FirePlaces'\n\n5 - 'ExterQual' with 'OverallQual'. We will remove 'ExterQual'\n\n6 - 'BldgType' with 'MSSubClass'. We will remove 'MsSubClass'","a8a3eed7":"from looking at the above data frame, we will use seed 18","f0089e22":"10 - Random Forest Regressor ","d3de0e80":"In order to be able to evaluate our models and tune their hyper parameters, we need a test data of some sort. To do this, we will split the training data into a train and test sets, just in order to be able to see what we're actually working with. After obtaining the best models and the best hyper paramter, and for the actual submitted predictions we will re-train the models on the entire train data set.  ","2e63fa73":"In this section we will combine some of the best performing models from the previous section, to form one regressor that depends on all models.","13899d76":"1 - Linear Regression","25739707":"1 - Encoding non-ordinal features ","a883579e":"# Feature Engineering","b6454832":"We will create the ensemble using the best weights obtained. This ensemble will be trained on the entire train data set, and then we will make predictions and submit them.  ","57df7438":"## Feature Selection","7a105ddf":"11 - Gradient Boosting Regressor","bcb491ed":"## Regression Ensemble","05b26d6d":"optimizing the weights of the ensemble","69260703":"'Exterior1st' and 'Exterior2nd' NaN values will be replaced with the mode. ","da7b61c7":"6 - Ridge Regression","35a60e87":"we have a lot of features and most of the features are skewed. Since we don't want the models to be trined on skewed data, we will deal with them in this section. ","7741e1b8":"'BsmtFullBath' and 'BsmtHalfBath' NaN values will be replaced with 0, for having no basement. ","bfe61329":"'Utilities' doesn't have any variance to it. So we will drop it.","76d4c6a3":"in this section we will start dealing with data. From data description we decided how we are going to deal with features.","522f9915":"12 - Ada Boost Regressor","0baf3917":"8 - LGBM","aa225ad2":"## Adding Feature To Represent House Area","c1ed049e":"after knowing the highly correlated features we want to deal with them. This is done through looking at the VIF for the features then removing the correlated featuers and checking whether removing the features will help reduce the VIF. ","45934814":"2 - Encoding ordinal features:\n\nThese features have an order to them, also most of these features had NaN values in them, and as explained in the previous section of the notebook, we replaced the NaN values in these features with 'None' to be able to encode them correctly.","25446293":"'LotFrontage' has a lot of missing values, and since its numerical data we will replace NaN values with the median of the neighborhood which the house is located in. Because the data is combined, we will preform the median replacement on the train rows with the median of the train data, and median repalcement on the test rows with the median of the test data.","02344942":"'Electrical', 'KitchenQual' have one NaN value so we will be replace that with the mode.","ed10fcfb":"'Functional' will be replace with Typical according to the data description.","28644ffe":"'GarageYrBlt', 'GarageCars' and 'GarageArea' NaN valeus will be replaced by 0, for having no garage.","25cbb64b":"### Colinearity\nIn this section we will look for features that have high correlation with each other and start to deal with them.\n\n1 - 'GarageCars' and 'Garage Area'.\n\n2 - 'TotalSF' with 'GrLivArea'\n\n3 - 'GarageYrBlt' with 'YrBlt'\n\n4 - 'FirePlaceQl' with 'FirePlaces'\n\n5 - 'TotRmsAbvGrd' with 'GrLivArea'\n\n6 - 'ExterQual' with 'OverallQual'\n\n7 - 'BldgType' with 'MSSubClass'","38fbd5a3":"### Numerical Features with NaN Values","ded6b9ed":"# Creating The Used Ensemble & Making Predictions","565fab47":"if VIF > 10, then the features correpsonding to such result are highly correlated with other features in the dataset. We are aiming to be able to reduce the VIF values but still maintain data. ","88910166":"Displaying information of columns which only have NaN values","9870c50e":"## Dealing with NaN Values\n","0602b10f":"From carefully reading the data description and looking at the data description in the EDA section, we looked at each one of the features which contin NaN values and we decided on some important steps that are going to be taken to deal with such data.\n\n\n","de2565e0":"'BsmtFinSF1' and ''BsmtFinSF2' will be replaced with 0, it most likely mean that there is no basement. ","320eb10b":"## Best Seed","2c7ddf3f":"In this section we will look for the best seed to split the data by.","f3280b4c":"7 - Ridge CV","ff449f7a":"'SaleType' only has one record with NaN so we will replace it with the mode.","43d654f4":"4 - ARD Regression","f3912cdc":"### Categorical Features with NaN values","31e2e06e":"We will write a class that we're going to use in grid search to tune the hyper paramteres of our models. This class works by training the model on the training set and evaluating it on the test set.","06bdb25e":"we will transform these features by using boxcox1p transformation. The reason for this is that some columns have 0 in them, so using regular boxcox would result in an error as it only accepts posetive values.","38b99b71":"we will plot the histogram of 'LotArea' to show data before correcting skewness","cfc3b77c":"13 - Bagging Regressor","a2787890":"We will remove the 6 lowest correlation columns as they wouldn't make much of a difference when training our model. The 'ID' column will also be removed as it doesn't have any meaning to any ML model.","035cd036":"dealing with the skewness of the target by applying log(1+x) transofrmation","c4081954":"2 - Decision tree Regressor","2233a3ab":"'MSZoning' NaN values will be replaced by the mode.","c6b13443":"3 - Support Vector Regressor\n","bf7a4e7f":"## Target Skewness","6ea44cad":"The following list of features are feature which have NaN values that will be replaced with None. as None has a meaning for them. For example 'Alley' NaN values mean that there is no access to an alley. The same logic could be applied to the other features in the list.  ","278d4b32":"'BsmtUnfSF' and 'TotalBsmtUnf' NaN values will be replaced by 0, as it also indicates there's no basement. ","230fd3a8":"Submission","25363b0e":"# Data Modeling","36d724ce":"## Feature Skewness","4be15246":"As we can see fro the graph data is positively skewed, and from the skewness coefficient which should be as close to zero as possible. A way to deal with this is using feature transformation. We will use log(1+x) transofrmation, which is equivalent to using boxcox1p with lambda set to equal zero. "}}