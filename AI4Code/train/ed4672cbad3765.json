{"cell_type":{"c648078f":"code","8ae4c20d":"code","daa673a0":"code","5d8bbeab":"code","6016e6a9":"code","7e700416":"code","db9d1519":"code","0820329c":"code","6c92178e":"code","ac4abdc5":"code","a50dc79d":"code","3863eec2":"code","a8eabbcc":"code","7580dec8":"code","0331b827":"code","80a19732":"code","ea7cfca0":"code","f4859021":"code","9e310b29":"markdown","bf3d0f36":"markdown","6a1318f5":"markdown","037d0032":"markdown","e689c40f":"markdown","abdc1be2":"markdown","b98f52ca":"markdown","9c7c3f49":"markdown","8a109857":"markdown","f1835396":"markdown","0a6bd5d5":"markdown","307e40eb":"markdown","be55e24a":"markdown","a1354430":"markdown","72d9a1d7":"markdown","94f907e9":"markdown","62c10f39":"markdown","49ef2d51":"markdown"},"source":{"c648078f":"import os\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","8ae4c20d":"df = pd.read_csv(\"\/kaggle\/input\/telco-customer-churn\/WA_Fn-UseC_-Telco-Customer-Churn.csv\")\ndf[\"TotalCharges\"] = df[\"TotalCharges\"].apply(lambda x: float(\"0\"+x.strip()))\ndf.head()","daa673a0":"from sklearn.preprocessing import LabelEncoder\ncat_cols = [\"gender\", \"Partner\", \"Dependents\", \"PhoneService\", \"MultipleLines\", \n            \"InternetService\", \"OnlineSecurity\", \"OnlineBackup\", \"DeviceProtection\", \"TechSupport\", \n            \"StreamingTV\", \"StreamingMovies\", \"Contract\", \"PaperlessBilling\", \"PaymentMethod\"]\nlbl_enc = LabelEncoder()\nfor col in cat_cols:\n    df[col] = lbl_enc.fit_transform(df[col])\ndf.head()","5d8bbeab":"churn_map_dict = {\"Yes\":1, \"No\":0}\ndf[\"Churn\"] = df[\"Churn\"].map(churn_map_dict)\ndf[\"Churn\"].value_counts()\n\ncols_to_use = [col for col in df.columns if col not in [\"customerID\", \"Churn\"]]\nX = df[cols_to_use].values\ny = df[\"Churn\"].values","6016e6a9":"from sklearn.model_selection import train_test_split\n\ntrain_X, test_X, train_y, test_y = train_test_split(X, y, test_size=0.2, random_state=2020)","7e700416":"from bayes_opt import BayesianOptimization\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.ensemble import RandomForestClassifier as RFC\n\n# function for cross validation\ndef rfc_cv(n_estimators, min_samples_split, max_features, data, targets):\n    estimator = RFC(\n        n_estimators=n_estimators,\n        min_samples_split=min_samples_split,\n        max_features=max_features,\n        random_state=2\n    )\n    cval = cross_val_score(estimator, data, targets,\n                           scoring='neg_log_loss', cv=4)\n    return cval.mean()\n\n# optimization function\ndef optimize_rfc(data, targets):\n    def rfc_crossval(n_estimators, min_samples_split, max_features):\n        return rfc_cv(\n            n_estimators=int(n_estimators),\n            min_samples_split=int(min_samples_split),\n            max_features=max(min(max_features, 0.999), 1e-3),\n            data=data,\n            targets=targets,\n        )\n\n    # evaluation space\n    optimizer = BayesianOptimization(\n        f=rfc_crossval,\n        pbounds={\n            \"n_estimators\": (10, 250),\n            \"min_samples_split\": (2, 25),\n            \"max_features\": (0.1, 0.999),\n        },\n        random_state=1234,\n        verbose=2\n    )\n    optimizer.maximize(n_iter=10)\n\n    print(\"Final result:\", optimizer.max)\n    \n    return optimizer\n    \nresult = optimize_rfc(train_X, train_y)","db9d1519":"print(result.max)","0820329c":"from hyperopt import hp, tpe, STATUS_OK, Trials\nfrom hyperopt.fmin import fmin\n\nspace ={\n    'n_estimators': hp.choice('n_estimators', np.arange(10, 250, dtype=int)),\n    'min_samples_split' : hp.choice('min_samples_split', np.arange(2, 25, dtype=int)),\n    'max_features': hp.quniform ('max_features', 0.1, 0.99, 0.02)\n    }\n\ndef rfc_cv(space, data=train_X, targets=train_y):\n    estimator = RFC(\n        n_estimators=space[\"n_estimators\"],\n        min_samples_split=space[\"min_samples_split\"],\n        max_features=space[\"max_features\"],\n        random_state=2\n    )\n    cval = cross_val_score(estimator, data, targets,\n                           scoring='neg_log_loss', cv=4)\n    return {'loss':-cval.mean(), 'status': STATUS_OK }\n\ntrials = Trials()\nbest = fmin(fn=rfc_cv,\n            space=space,\n            algo=tpe.suggest,\n            max_evals=5, # change\n            trials=trials)","6c92178e":"print(best)","ac4abdc5":"import h2o\nh2o.init()","a50dc79d":"h2o_train = h2o.H2OFrame(train_X).cbind(h2o.H2OFrame(train_y))\nh2o_test = h2o.H2OFrame(test_X).cbind(h2o.H2OFrame(test_y))","3863eec2":"from h2o.estimators.gbm import H2OGradientBoostingEstimator\n\npredictors = h2o_train.columns[:-1]\nresponse = \"C110\"\nh2o_train[response] = h2o_train[response].asfactor()\n\ngbm = H2OGradientBoostingEstimator()\ngbm.train(x=predictors, y=response, training_frame=h2o_train)","a8eabbcc":"from h2o.automl import H2OAutoML\n\naml = H2OAutoML(max_models = 10, \n                max_runtime_secs=100, \n                seed = 1,\n                stopping_metric = \"logloss\",\n                nfolds=4\n               )\naml.train(x=predictors, y=response, training_frame=h2o_train)","7580dec8":"aml.leaderboard","0331b827":"aml.leader","80a19732":"import autosklearn.classification\n\nautoml = autosklearn.classification.AutoSklearnClassifier(\n    time_left_for_this_task=100,\n    per_run_time_limit=30,\n    resampling_strategy='cv',\n    resampling_strategy_arguments={'folds': 4}\n)\nautoml.fit(train_X.copy(), train_y.copy())","ea7cfca0":"from tpot import TPOTClassifier\n\nautoml = TPOTClassifier(max_time_mins=1,\n                        max_eval_time_mins=0.5, \n                        scoring='neg_log_loss',\n                        cv=4,\n                        random_state=2020,\n                        verbosity=1\n                       )\nautoml.fit(train_X, train_y)","f4859021":"automl.fitted_pipeline_","9e310b29":"### Auto Sklearn","bf3d0f36":"### TPOT","6a1318f5":"Github - https:\/\/github.com\/hyperopt\/hyperopt\n\nLet us use the hyperopt package to get the best set of parameters now. ","037d0032":"We can check the metrics for the best model in both train and test datasets.","e689c40f":"Split the dataset to get train and test sets","abdc1be2":"Let us now call the AutoML function in H2O to get the best model for this dataset.","b98f52ca":"Link - https:\/\/epistasislab.github.io\/tpot\/\n\nTPOT is a Python Automated Machine Learning tool that optimizes machine learning pipelines using genetic programming. Let us use TPOT now to get the best pipeline. ","9c7c3f49":"### HyperOpt","8a109857":"Github - https:\/\/automl.github.io\/auto-sklearn\/master\/\n\nHad some trouble installing this package in kaggle kernels but below is simple code example to run the automl in local machine.","f1835396":"### Bayesian Optimization","0a6bd5d5":"Code flow to build a GBM model in H2O","307e40eb":"**Work in progress. Please stay tuned**","be55e24a":"### Objective of the notebook:\n\nTo look at the different open source hyperparameter tuning and automl packages available and to get a quick start with them.\n\n### Introduction\n\nWe will take the telecom customer churn dataset to explore. Let us import the datasets and do some preprocessing. The preprocessing steps are\n1. Converting the `TotalCharges` variable to float from string\n2. Converting the categorical variables to numerical ones through label encoding\n3. Converting the target column `Churn` to 0 and 1. \n","a1354430":"### H2O AutoML\n\nDoc link: http:\/\/docs.h2o.ai\/h2o\/latest-stable\/h2o-docs\/automl.html\n\nNow let us look at some of the AutoML packages where both the models and the best hyperparameters are chosen automatically.","72d9a1d7":"Let us check the leaderboard output from the AutoML function. We can see that in addition to individual models, it also builds stacking models. Infact, these stacked models gave good results for this dataset. ","94f907e9":"Best pipeline that comes out of TPOT can be seen using the following command.","62c10f39":"Create a H2OFrame for both train and test datasets","49ef2d51":"Github - https:\/\/github.com\/fmfn\/BayesianOptimization\n\nPure Python implementation of bayesian global optimization with gaussian processes.\n\nLet us use the dataset prepared above to build a Random Forest classifier model. Bayesian Optimization is used to find the best parameters for the Random Forest model."}}