{"cell_type":{"0e3511f9":"code","7abc617f":"code","42e0d24d":"code","12ffad57":"code","8772c0e2":"code","07dda3a4":"code","95c96622":"code","0ff96997":"code","01c343f7":"code","8716b53e":"code","9fbb2148":"code","dffd9a15":"code","c7a0e305":"code","da600a2c":"code","932d9c07":"code","8d610023":"code","99c8eb57":"code","12cecea3":"markdown"},"source":{"0e3511f9":"import pandas as pd\nimport numpy as np\nfrom sklearn.impute import *\n\ntrain = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv')\ny = train.SalePrice\n\ntrain.drop(['Id', 'SalePrice'], axis=1, inplace=True)\ntest.drop(['Id'], axis=1, inplace=True)\n\ntrain.head()","7abc617f":"data = pd.concat([train, test])","42e0d24d":"data.info()","12ffad57":"# list(filter(lambda column: (data[column].count() != data.shape[0]), data.keys()))\ncolumns_with_nan = {column: data[column].count() for column in data.keys() if data[column].count() != data.shape[0]}\ncolumns_with_nan","8772c0e2":"#Sorting the vector from the smallest amount of NaN to the largest \nordered_columns_by_nan_count = [key for key, value in sorted(columns_with_nan.items(), key=lambda item: item[1])][::-1]\nordered_columns_by_nan_count","07dda3a4":"from sklearn.linear_model import LinearRegression\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.metrics import mean_squared_error, accuracy_score\nfrom sklearn.preprocessing import *\nfrom xgboost import XGBClassifier\n\ndef doMethodImpute(data, target_column, model=DecisionTreeRegressor):\n    class doMethodImputeIntern():\n        def __init__(self, target_column, not_append, to_append, onehotencoder, model):\n            self.target_column = target_column\n            self.not_append = not_append\n            self.to_append = to_append\n            self.onehotencoder = onehotencoder\n            self.model = model\n\n        def transform(self, new_data):\n            Xt = self.onehotencoder.transform(new_data[self.to_append]).toarray()\n            Xt = np.concatenate((Xt, new_data[self.not_append]), axis=1)\n            ypt = self.model.predict(Xt)\n\n            news_data[self.target_column].fillna(pd.Series(ypt), inplace=True)\n\n            return self\n\n    columns_to_fit = []\n    for key in data.keys():\n        if key == target_column: continue\n        if data[target_column].notnull().sum() == data[data[target_column].notnull()][key].notnull().sum():\n            columns_to_fit.append(key)\n\n    not_append, to_append = [], []\n    for column in columns_to_fit:\n        if data[column].dtype == np.float64 or data[column].nunique() > 5:\n            if data[column].dtype == object:\n                continue\n            not_append.append(column)\n        else:\n            to_append.append(column)\n\n    onehotencoder = OneHotEncoder(handle_unknown='ignore')\n    categorical = data[data[target_column].notnull()][to_append]\n    X = onehotencoder.fit_transform(categorical).toarray()\n    X = np.concatenate((X, data[data[target_column].notnull()][not_append]), axis=1)\n\n    model = model()\n    model.fit(X, data[data[target_column].notnull()][target_column])\n    yp = model.predict(X)\n\n    Xt = onehotencoder.transform(data[to_append].fillna(0)).toarray()\n    Xt = np.concatenate((Xt, data[not_append]), axis=1)\n    np.nan_to_num(Xt, copy=False)\n    ypt = model.predict(Xt)\n\n    data[target_column].fillna(pd.Series(ypt), inplace=True)\n\n    return doMethodImputeIntern(target_column, not_append, to_append, onehotencoder, model)","95c96622":"from pandas.api.types import is_integer_dtype\n\n#impute nan with regression if at least 50% of the feature is not Nan \nfor col in ordered_columns_by_nan_count:\n    print(col)\n    if columns_with_nan[col] > 0.5*data.shape[0]:\n        if data[col].dtype == np.float64 or (is_integer_dtype(data[col].dtype) and data[col].nunique() > 5):\n            model=DecisionTreeRegressor\n            doMethodImpute(data, col, model=model)\n        elif data[col].nunique() <= 5:\n            model=XGBClassifier\n            doMethodImpute(data, col, model=model)\n        else:\n            data.fillna({col: 'notEvaluated'}, inplace=True)\n    else:\n        if data[col].dtype == np.float64 or is_integer_dtype(data[col].dtype):\n            imputer = SimpleImputer(missing_values=np.nan, strategy = 'median')\n            imputer = imputer.fit(data[[col]])\n            data[[col]] = imputer.transform(data[[col]])\n        else:\n            data.fillna({col: 'notEvaluated'}, inplace=True)\n","0ff96997":"data.info()","01c343f7":"data['MiscVal'].skew(), data['GarageArea'].skew()","8716b53e":"data['GarageArea'].hist()","9fbb2148":"data['MiscVal'].hist()","dffd9a15":"for col in data.keys():\n    if data[col].dtype == np.float64 or is_integer_dtype(data[col].dtype):\n        print(f\"{col}:\\t{data[col].skew()}:\\t{min(data[col])}\")","c7a0e305":"from sklearn.preprocessing import StandardScaler\n\nfor col in data.keys():\n    if data[col].dtype == np.float64 or (is_integer_dtype(data[col].dtype) and data[col].nunique() > 5):\n        if min(data[col]) > 0 and data[col].skew() > 1:\n            data[col] = np.log(data[col])\n\n        enc = StandardScaler()\n        enc = enc.fit(data[col].to_numpy().reshape(-1, 1))\n        data[col] = enc.transform(data[col].to_numpy().reshape(-1, 1))\n\n        print(f\"{col}:\\t{data[col].skew()}:\\t{min(data[col])}\")\n","da600a2c":"from sklearn.preprocessing import *\n\ncategorical_features = []\nno_categorical_features = []\nfor col in data.keys():\n    if data[col].dtype == object or (is_integer_dtype(data[col].dtype) and data[col].nunique() <= 5):\n        categorical_features.append(col)\n    else:\n        no_categorical_features.append(col)\n\ncat = data[categorical_features]\nnot_cat = data[no_categorical_features]\nonehotencoder = OneHotEncoder()\nX = onehotencoder.fit_transform(cat).toarray()\n\nX = np.concatenate([X, not_cat], axis=1)","932d9c07":"df_X = pd.DataFrame(X)\ntrain_imputed = df_X.head(train.shape[0]).copy()\ntest_imputed = df_X.tail(test.shape[0]).copy()","8d610023":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(train_imputed, y, test_size=0.10, random_state=42)\n\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.linear_model import LinearRegression\n\nfrom sklearn.metrics import mean_squared_error, mean_squared_log_error\n\nmodel = DecisionTreeRegressor()\nmodel.fit(X_train, y_train)\n\nyp = model.predict(X_test)\n\nprint(np.sqrt(mean_squared_log_error(y_test, yp)))","99c8eb57":"from xgboost import XGBRegressor\nmodel = XGBRegressor()\nmodel.fit(train_imputed, y)\nyp = model.predict(test_imputed)\n\nwith open('pred.csv', 'w') as out, open('\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv') as inp:\n    out.write('Id,SalePrice\\n')\n    inp.readline()\n    for v in yp:\n        line = inp.readline().split(\",\")[0]\n        out.write(line + \",\" + str(int(v)) + \"\\n\")","12cecea3":"https:\/\/www.kaggle.com\/c\/house-prices-advanced-regression-techniques"}}