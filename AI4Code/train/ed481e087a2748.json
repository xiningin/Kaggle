{"cell_type":{"6852700e":"code","350a03ec":"code","a97b8c37":"code","9b0f4343":"code","f426bf22":"code","f179c078":"code","fec1896f":"code","6cd1ddb3":"code","10f33623":"code","351939a0":"code","beeffcbb":"code","d90613aa":"code","7fc2e3a3":"code","049e1438":"code","95abd4ca":"code","0d4dddb3":"code","bf062096":"code","a20646b4":"code","583e9774":"code","326f9706":"code","eb4ad652":"code","f73834cd":"code","2a6fd938":"code","44780ba9":"code","7b2807d1":"code","1ac76629":"code","ac345de5":"code","3727bfd3":"code","d75f2366":"code","5e0d97be":"code","a3ba8508":"code","d70ea8a8":"code","773138c2":"code","ae14f1b1":"code","9f8ec666":"code","60eb8be3":"code","6b26b226":"code","7334db5d":"code","3da821e7":"code","44a209fa":"code","304abca1":"code","099b8fff":"code","5f6e359c":"code","7dca2a91":"code","2abee1b6":"code","9efb568d":"code","2b38ea23":"code","1baef9bc":"code","2560bbf8":"code","213bf74e":"code","19b01d1b":"code","cff3e2f0":"code","72b28d29":"code","b4177964":"code","02ed9022":"code","a8fac3ac":"code","74cbe574":"code","6cb71cea":"code","c74771d3":"code","5aa41c6e":"code","f49baca8":"code","5feb4b32":"code","8aacaefe":"code","ade80462":"code","70917656":"code","fa6058f1":"code","76a4a8e4":"code","be286821":"code","f2f38a8d":"code","2f8d0a6e":"code","e596f460":"code","13550536":"code","6af927a6":"code","88217c19":"code","8d98c35f":"code","eb007a48":"code","c35ae6cd":"code","33947208":"code","05b57546":"code","734f1fcf":"markdown","7342887b":"markdown","152ca656":"markdown","cba58136":"markdown","7deae2b1":"markdown","463f9c0f":"markdown","3492363f":"markdown","aa6e25f7":"markdown","440f7690":"markdown","84db1868":"markdown","49f56ff7":"markdown","6dfb5205":"markdown","c4788b93":"markdown","8885e44c":"markdown","21fa7168":"markdown","263b229a":"markdown","51d9c070":"markdown","e3e6b12c":"markdown","60cb00a1":"markdown","62a7faae":"markdown","4fb953a4":"markdown","e071f1fd":"markdown","0ac93965":"markdown","eccd82f2":"markdown"},"source":{"6852700e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","350a03ec":"import warnings\nfrom sklearn.exceptions import ConvergenceWarning\nwarnings.simplefilter(action='ignore', category=FutureWarning)\nwarnings.simplefilter(\"ignore\", category=ConvergenceWarning)\n\ndiabetes_df = pd.read_csv('\/kaggle\/input\/pima-indians-diabetes-database\/diabetes.csv')\ndiabetes_df.head()","a97b8c37":"diabetes_df.info()","9b0f4343":"diabetes_df.isna().sum()","f426bf22":"diabetes_df.describe()","f179c078":"diabetes_df.duplicated().sum()","fec1896f":"replace_cols = [col for col in diabetes_df.columns if not(col == 'Outcome' or col == 'Pregnancies')]\n\ndiabetes_df[replace_cols] = diabetes_df[replace_cols].replace({0: np.nan})\n\ndiabetes_df.isna().sum()","6cd1ddb3":"# create a version of the df with no NaN for testing use (**not to be used for real training**)\n# just used to test pipeline functions on some non NaN data\ntesting_df = diabetes_df.dropna()\ntesting_y = testing_df.pop('Outcome')\ntesting_df.isna().sum()\ntesting_y","10f33623":"from sklearn.model_selection import train_test_split\ny_outcome = diabetes_df.pop('Outcome')\ndf_train, df_test, y_train, y_test = train_test_split(diabetes_df, y_outcome, test_size=0.2, stratify=y_outcome, random_state=8)","351939a0":"y_train.hist()","beeffcbb":"df_train.hist(figsize=(10, 10))","d90613aa":"import seaborn as sns\nfrom typing import Tuple\ndef kde_with_log(x: str, df: pd.DataFrame = df_train) -> Tuple[sns.kdeplot]:\n    \"\"\"Make a kde graph with the logged version of the graph beside\n    \"\"\"\n    fig, axs = plt.subplots(1, 2, figsize=(8, 4))\n    ax1 = sns.kdeplot(data=df_train, x=x, ax=axs[0])\n    ax1.set_title(f'{x} Frequency')\n\n    if df_train[x].min() != 0:\n        # apply normal log if no values are less than 0\n        log_x = df_train[x].apply(np.log)\n    else:\n        # apply shifted log if some values equal zero\n        log_x = df_train[x].apply(np.log1p)\n    ax2 = sns.kdeplot(x=log_x, ax=axs[1])\n    ax2.set_title(f'Logged {x} Frequency')\n    \n    return (ax1, ax2)","7fc2e3a3":"import seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline","049e1438":"# check all logged relationships to see which right skewed distributions may be more informative with a log transformation\nfor col in df_train.columns:\n    ax1, ax2 = kde_with_log(col)","95abd4ca":"def numerical_v_outcome(x, y, ax, graph_type='violin', title=None, df=df_train):\n    \"\"\"Graph all features vs the outcome. If outcome (x) is None, simply graph features\n    \"\"\"\n    if title == None:\n        title = f'{y} vs Outcome'\n    if type(y) == str:\n        sns_y = df[y]\n    else:\n        sns_y = y\n        \n    if graph_type == \"violin\":\n        ax = sns.violinplot(x=x, y=sns_y, ax=ax)\n    else:\n        ax = sns.boxplot(x=x, y=sns_y, ax=ax)\n    ax.set_title(title)\n    return ax","0d4dddb3":"# plot various factors vs the outcome with a violin plot to see the frequency of distribution\nfig, axes = plt.subplots(3, 3, figsize=(15, 15))\nindex_1 = 0\nindex_2 = 0\nfig.suptitle('Violin plots: Outcome vs Features')\n\nfor i in range(len(df_train.columns)):\n#     if diabetes_df.columns[i] != \"Outcome\":\n    numerical_v_outcome(y_train, df_train.columns[i], axes[index_1, index_2])\n#     else:\n#         fig.delaxes(axes[index_1, index_2])\n    index_1 += 1\n    if index_1 == 3:\n        index_1 = 0\n        index_2 += 1\nfig.delaxes(axes[index_1, index_2])","bf062096":"# plot various factors vs the outcome showing the points to spot outliers\nfig, axes = plt.subplots(3, 3, figsize=(15, 15))\nindex_1 = 0\nindex_2 = 0\nfig.suptitle('Boxplots: Outcome vs Features')\n\nfor i in range(len(df_train.columns)):\n#     if diabetes_df.columns[i] != \"Outcome\":\n    numerical_v_outcome(y_train, df_train.columns[i], axes[index_1, index_2], graph_type='boxplot')\n#     else:\n#         fig.delaxes(axes[index_1, index_2])\n    index_1 += 1\n    if index_1 == 3:\n        index_1 = 0\n        index_2 += 1\nfig.delaxes(axes[index_1, index_2])","a20646b4":"# plot various logged factors vs the outcome to show how normalizinf the data works\n# note, some of these graphs are repeated from the large graphs before\nfig, axes = plt.subplots(3, 3, figsize=(15, 15))\nindex_1 = 0\nindex_2 = 0\nfig.suptitle('Boxplots: Outcome vs Features')\n\nfor i in range(len(df_train.columns)):\n    if df_train[df_train.columns[i]].min() > 0:\n        numerical_v_outcome(y_train, df_train[df_train.columns[i]].apply(np.log), axes[index_1, index_2], graph_type='boxplot', title=f'Log {df_train.columns[i]} vs Outcome Boxplot')\n    else:\n        numerical_v_outcome(y_train, df_train[df_train.columns[i]].apply(np.log1p), axes[index_1, index_2], graph_type='boxplot', title=f'Log {df_train.columns[i]} vs Outcome Boxplot')\n    index_1 += 1\n    if index_1 == 3:\n        index_1 = 0\n        index_2 += 1\nfig.delaxes(axes[index_1, index_2])","583e9774":"# explore insulin vs bmi\nax = sns.scatterplot(x=df_train['Insulin'], y=df_train['BMI'])","326f9706":"# plot various factors vs the outcome showing the points to spot outliers\nfig, axes = plt.subplots(3, 3, figsize=(15, 15))\nindex_1 = 0\nindex_2 = 0\nfig.suptitle('Boxplots: Outcome vs Features')\n\nfor i in range(len(df_train.columns)):\n#     if diabetes_df.columns[i] != \"Outcome\":\n    numerical_v_outcome(None, df_train.columns[i], axes[index_1, index_2], graph_type='boxplot', title=f'{df_train.columns[i]}')\n#     else:\n#         fig.delaxes(axes[index_1, index_2])\n    index_1 += 1\n    if index_1 == 3:\n        index_1 = 0\n        index_2 += 1\nfig.delaxes(axes[index_1, index_2])","eb4ad652":"from sklearn.base import TransformerMixin, BaseEstimator\n\nclass Debug(BaseEstimator, TransformerMixin):\n\n    def transform(self, X):\n        # store the attribute X_ to see scaled data in this step\n        self.X_ = X\n        return X\n        \n    def fit(self, X, y=None, **fit_params):\n        return self","f73834cd":"from sklearn.pipeline import Pipeline\nimport xgboost as xgb\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.tree import DecisionTreeClassifier\n\nbasic_pipeline = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='median')),\n    ('clean_data', Debug()),\n    ('model', DecisionTreeClassifier(random_state=0))\n])\nbasic_pipeline","2a6fd938":"def get_median_imputed_data(df):\n    median_imputer = SimpleImputer(strategy='median')\n    imputed_df = pd.DataFrame(median_imputer.fit_transform(df), columns=df.columns)\n    return imputed_df","44780ba9":"from sklearn.metrics import classification_report, roc_auc_score\nfrom sklearn.model_selection import cross_val_score, cross_validate\nfrom typing import Dict\n\n\ndef score_pipeline_model(X: pd.DataFrame, y: pd.Series, pipeline: Pipeline, tuning: bool = False) -> Dict[str, int]:\n    \"\"\"\n    Score a model that is in a pipeline. Only look at F1 score if hyperparameter tuning is occuring\n    \"\"\"\n    if not tuning:\n        # add multiple socring methods to avoid running this process multiple times\n        scoring = {\n            'accuracy': 'accuracy',\n            'precision': 'precision',\n            'recall': 'recall',\n            'f1': 'f1',\n            'roc_auc': 'roc_auc'\n        }\n\n        scores = cross_validate(pipeline, X, y, scoring=scoring, cv=5)\n\n        scoring_dict = {}\n        for score in scores.keys():\n            if score.startswith('test_'):\n    #             print(scores[score])\n                scoring_dict[score] = scores[score].mean()\n        return scoring_dict\n    #     return scores.mean()\n    else:\n        scores = cross_val_score(pipeline, X, y, scoring='f1', cv=5)\n        return scores.mean()","7b2807d1":"from sklearn.metrics import confusion_matrix\n\ndef get_preds(X: pd.DataFrame, y: pd.Series, pipeline: Pipeline,\n                            X_test: pd.DataFrame = None, y_test: pd.DataFrame = None) -> Tuple[pd.Series, np.ndarray]:\n    \"\"\"Fit a pipeline and create predictions \n    \n    Args:\n        X: Either the entire dataframe or the training dataframe (if X_test is also passed)\n        y: Either the entire predictions or the training predictions (if y_test is also passed)\n        pipeline: The pipeline to fit the data on\n        X_test: Df containing test data\n        y_test: Series containing test outcomes\n    \n    Returns:\n        A tuple containing the true outcomes and the predictions\n    \"\"\"\n    if X_test == None and y_test == None:\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=88)\n    else:\n        X_train, y_train = X, y\n    \n    pipeline.fit(X_train, y_train)\n    \n    preds = pipeline.predict(X_test)\n    \n    return (y_test, preds)","1ac76629":"basic_pipeline_scoring = score_pipeline_model(df_train, y_train, basic_pipeline)\nprint(basic_pipeline_scoring)","ac345de5":"outcome, basic_predictions = get_preds(df_train, y_train, basic_pipeline)\nprint(classification_report(outcome, basic_predictions))","3727bfd3":"from sklearn.metrics import confusion_matrix\ncm = confusion_matrix(outcome, basic_predictions)\nax = sns.heatmap(cm, annot=True)","d75f2366":"from typing import List\ndef _set_up_kmeans(df: pd.DataFrame, features: List[int]) -> pd.DataFrame:\n    \"\"\"Private function to normalize data for kmeans\n    \n    Args:\n        df: Current dataframe being used\n        features: List of features for kmeans to be applied on\n    \n    Returns:\n        Dataframe for specific features with scaled data\n    \"\"\"\n    df_copy = df.copy()\n    df_selected = df_copy.loc[:, features]\n    df_scaled = (df_selected - df_selected.mean(axis=0)) \/ df_selected.std(axis=0)\n    return df_scaled\n    ","5e0d97be":"from sklearn.cluster import KMeans\n\ndef kmeans_cluster(df: pd.DataFrame, n_clusters: int) -> pd.DataFrame:\n    \"\"\"Creates cluster features using Kmeans on certain features\n    \n    Args:\n        df: Current dataframe being used\n        n_clusters: The number of clusters to create using Kmeans\n        \n    Returns:\n        The same dataframe with an additional clusters feature\n    \"\"\"\n    features = ['BMI', 'Insulin', 'Glucose', 'Age', 'Pregnancies']\n    X_scaled = _set_up_kmeans(df, features)\n  \n    k_means = KMeans(n_clusters, n_init=50, max_iter=1000, random_state=42)\n    \n    # make a new column in the dataframe for each cluster\n    df.loc[:, 'Cluster'] = k_means.fit_predict(X_scaled)\n    return df  ","a3ba8508":"def kmeans_cluster_dist(df: pd.DataFrame, n_clusters: int) -> pd.DataFrame:\n    \"\"\"Creates cluster distance features for kmeans on certain df features\n    \n    Args:\n        df: Current dataframe being used\n        n_clusters: The number of clusters to create using Kmeans\n        \n    Returns:\n        The same dataframe with an additional cluster distances features\n    \"\"\"\n    features = ['BMI', 'Insulin', 'Glucose', 'Age', 'Pregnancies']\n    X_scaled = _set_up_kmeans(df, features)\n    \n    k_means = KMeans(n_clusters, n_init=50, max_iter=1000, random_state=41)\n    cluster_distances = k_means.fit_transform(X_scaled)\n    \n    # make a new df for the cluster distances and join it to the old df\n    cluster_dist_df = pd.DataFrame(cluster_distances, columns=[f'centeroid_dist_{i}' for i in range(cluster_distances.shape[1])], index=df.index)\n#     print(cluster_dist_df)\n    df = df.join(cluster_dist_df)\n    return df","d70ea8a8":"from sklearn.feature_selection import mutual_info_regression\ndef get_mi_scores(df, y):\n    df = df.copy()\n    # label encode categorical data\n    for col in df.select_dtypes(['object', 'category']):\n        df_copy[col], _ = df[col].factorize()\n    discrete = [pd.api.types.is_integer_dtype(col_type) for col_type in df.dtypes]\n    mi_scores = pd.Series(mutual_info_regression(df, y, discrete_features=discrete, random_state=42), index=df.columns)\n    mi_scores = mi_scores.sort_values(ascending=False)\n    return mi_scores","773138c2":"# test mutual index with the non zero features\nprint(get_mi_scores(testing_df, testing_y))","ae14f1b1":"from sklearn.decomposition import PCA\nfrom typing import Tuple\ndef _apply_pca(df: pd.DataFrame, features: List[str]) -> Tuple[np.ndarray, PCA, List[str]]:\n    \"\"\"Private funciton to apply pca on certain features in a df\n    \n    Args:\n        df: Current dataframe being used\n        features: List of features for pca to be applied on\n    \n    Returns:\n        Numpy array containing the principal components for specific features, trained PCA model, and column names for the PCs\n    \"\"\"\n    df = df[features].copy()\n    df = (df - df.mean(axis=0)) \/ df.std(axis=0)\n    # create architecture\n    pca = PCA(random_state=42)\n    pc = pca.fit_transform(df)\n    \n    # get column names\n    new_cols = [f'pc_{i+1}' for i in range(len(features))]\n    return (pc, pca, new_cols)","9f8ec666":"def create_pc_pca(df: pd.DataFrame, feature_eng: bool = True) -> pd.DataFrame:\n    \"\"\"Creates principal components after PCA is applied\n    \n    Args:\n        df: The current dataframe being used\n        feature_eng: If this is the feature engineering iteration (false would mean training in pipeline)\n    \n    Returns:\n        A new dataframe that contains the relevant principal components as features\n    \"\"\"\n    \n#     if feature_eng:\n#         pc_y = testing_y\n#     else:\n#         pc_y = y_train\n    df = df.copy()\n    features = ['BMI', 'Insulin', 'Glucose', 'Age', 'Pregnancies']\n    pc, pca, new_cols = _apply_pca(df, features)\n    pc_df = pd.DataFrame(pc, columns=new_cols, index=df.index)\n    \n    # only take good PC \n#     scores_array = get_mi_scores(pc_df, pc_y)\n    evr = pca.explained_variance_ratio_\n    \n    good_pc = []\n    MI_CUTOFF = 0.06\n    EVR_CUTOFF = 1 \/ len(features)\n    for i in range(len(features)):\n#         print(scores_array[i], evr[i])\n#         if scores_array[i] > MI_CUTOFF and evr[i] > EVR_CUTOFF:\n#         if evr[i] > EVR_CUTOFF:\n        good_pc.append(f'pc_{i+1}')\n    \n    pc_df = pc_df[good_pc]\n    df = df.join(pc_df)\n    return df\n     ","60eb8be3":"# get an imputed version of the training data to check findings of PCA loadings\nimputed_train_df = get_median_imputed_data(df_train)\nimputed_train_df.isna().sum()","6b26b226":"def get_pca_loadings(df: pd.DataFrame, features: List[str]) -> pd.DataFrame:\n    df = df.copy()\n    _, pca, col_names = _apply_pca(df, features)\n    # get the loadings for the principal components\n    loadings = pd.DataFrame(pca.components_.T, columns=col_names, index=features)\n    return loadings","7334db5d":"features = ['BMI', 'Insulin', 'Glucose', 'Age', 'Pregnancies']\n# get the loadings for the principal components\nloadings = get_pca_loadings(imputed_train_df, features)\nloadings","3da821e7":"# repeat the same process using only 4 features to see if there are any different results\nfeatures_2 = ['BMI', 'Insulin', 'Glucose', 'Age']\nloadings_2 = get_pca_loadings(imputed_train_df, features_2)\nloadings_2","44a209fa":"# repeat one last time, this time without BMI\nfeatures_3 = ['Pregnancies', 'Insulin', 'Glucose', 'Age']\nloadings_3 = get_pca_loadings(imputed_train_df, features_3)\nloadings_3","304abca1":"def pca_loading_features(df: pd.DataFrame, ignore_features: List[str] = []) -> pd.DataFrame:\n    \"\"\"Create new features from the above analysis of PCA loadings\n    \n    Args:\n        df: The current DataFrame being used\n        ignore_feature: List of features to ignore from the function\n        \n    Returns:\n        A DataFrame with the new pca features\n    \"\"\"\n    df = df.copy()\n    if 'glucose_and_age' not in ignore_features:\n        df['glucose_and_age'] = df['Glucose'] * df['Age']\n    \n    if 'insulin_to_bmi' not in ignore_features:\n        df['insulin_to_bmi'] = df['Insulin'] \/ df['BMI']\n    \n    if 'age_to_bmi' not in ignore_features:\n        df['age_to_bmi'] = df['Age'] \/ df['BMI']\n    \n    if 'insulin_glucose_effect' not in ignore_features:\n        df['insulin_glucose_effect'] = df['Insulin'] \/ df['Glucose']\n    \n    return df","099b8fff":"def unskew_with_log(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"Apply log transformations to normalize data discovered in EDA\n    \n    Args:\n        df: The current dataframe\n    \n    Returns:\n        A new df containing logged versions of certain features\n    \"\"\"\n    df=df.copy()\n    \n    features=['DiabetesPedigreeFunction', 'Insulin', 'Age', 'Pregnancies']\n    \n    # loop through list of features and apply log transformations\n    for feature in features:\n        if df[feature].min() > 0:\n            df[f'logged_{feature}'] = df[feature].apply(np.log)\n#             df[feature] = df[feature].apply(np.log)\n        else:\n            df[f'logged_{feature}'] = df[feature].apply(np.log1p)\n#             df[feature] = df[feature].apply(np.log1p)\n    return df","5f6e359c":"def compare_median(row: pd.Series, medians: pd.Series) -> pd.Series:\n    \"\"\"Adds counts to above or below medians\n    \n    Args:\n        row: A row in a dataframe (one person)\n        medians: The medians for the current dataframe\n    \n    Returns:\n        A new series containing the new count features\n    \"\"\"\n    # get counts for features above the median\n    row['greater_than_median'] = row.gt(medians).sum()\n    # get counts for features equal to or less than - remove to not have repetitive features\n#     row['less_than_median'] = row.le(medians).sum()\n    return row","7dca2a91":"def row_median_counts(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"Apply counts above and below the median for each feature in a row\n    \n    Each of the initial features are higher in diabetics (seen in eda), so doing a median count on per feature will reveal more at risk people\n    \n    Args:\n        df: The current dataframe\n    \n    Returns:\n        A new df containing a median count feature\n    \"\"\"\n    initial_cols = ['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin',\n       'BMI', 'DiabetesPedigreeFunction', 'Age']\n    \n    df = df.copy()\n    medians = df[initial_cols].apply(lambda col: col.median())\n    df = df.apply(compare_median, axis=1, args=(medians,))\n    return df","2abee1b6":"def remove_outliers(df: pd.DataFrame, features: List[str] = ['SkinThickness', 'Glucose', 'Pregnancies']) -> pd.DataFrame:\n    \"\"\"Remove outliers from certain features to avoid over fitting\n    \n    Upon examination of the data, 3 features in particular appear to have outliers. SkinThinkness and Pregnancies have\n    clear outliers in the box and whisker plots; these should be removed. Glucose has outliers in the non diabetic grouping\n    which should be moved into back down. This comes from my knowledge that glucose can be affected by what the person has \n    eaten recently - eating an ice cream recently will cause the person to have a glucose spike and cause outliers in the data.\n    \n    Args:\n        df: The current dataframe\n        features: A list of features to move inside of the IQR\n    \n    Returns:\n        A dataframe with the outliers removed\n    \"\"\"\n    df = df.copy()\n    for feature in features:\n        # calculate the interquartile range (IQR) for a certain feature; where the bulk of the data is\n        quart_1 = df[feature].quantile(0.25)\n        quart_3 = df[feature].quantile(0.75)\n        iqr = quart_3 - quart_1\n        \n        # calculte the upper limits of the 'whiskers' in the box and whisker plots\n        upper_lim = quart_3 + 1.5*iqr\n        lower_lim = quart_1 - 1.5*iqr\n        \n        # replace outlier data\n        df.loc[(df[feature] > upper_lim), feature] = upper_lim\n        df.loc[(df[feature] < lower_lim), feature] = lower_lim\n        \n    return df","9efb568d":"def bin_data(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\" Put the data into bins to generalize observations\n    \n    SkinThickness data from: http:\/\/apjcn.nhri.org.tw\/server\/courses\/obesity\/anthro.doc#:~:text=For%20adults%2C%20the%20standard%20normal,either%20borderline%2C%20or%20fat%20depleted\n    Glucose Data: https:\/\/www.ncbi.nlm.nih.gov\/books\/NBK541081\/\n    Blood Pressure: https:\/\/www.webmd.com\/hypertension-high-blood-pressure\/guide\/diastolic-and-systolic-blood-pressure-know-your-numbers#1\n    BMI: https:\/\/www.nhlbi.nih.gov\/health\/educational\/lose_wt\/BMI\/bmi_tbl.pdf\n    \n    Label encode certain features based on research\n    \n    \"\"\"\n    df.loc[(df['Pregnancies'] == 0), 'No Preg v Preg'] = 0\n    df.loc[(df['Pregnancies'] > 0), 'No Preg v Preg'] = 1\n    \n    df.loc[df['SkinThickness'] <= 9, 'Skin Fat'] = 0\n    df.loc[df['SkinThickness'] > 30, 'Skin Fat'] = 2\n    df.loc[(df['SkinThickness'] > 9) & (df['SkinThickness'] <= 30), 'Skin Fat'] = 1\n    \n    df.loc[df['Glucose'] >= 140, 'Glucose Group'] = 1\n    df.loc[df['Glucose'] < 140, 'Glucose Group'] = 0\n    \n    # over 45 significantly higher risk of T2D, less than 30 is common for T1D diagnosis\n    df.loc[df['Age'] >= 45, 'Age Group'] = 2\n    df.loc[(df['Age'] >= 30) & (df['Age'] < 45), 'Age Group'] = 1\n    df.loc[df['Age'] < 30, 'Age Group'] = 0\n    \n    df.loc[df['BMI'] < 18.5, 'BMI Group'] = -1\n    df.loc[(df['BMI'] >= 18.5) & (df['BMI'] < 25), 'BMI Group'] = 0\n    df.loc[(df['BMI'] >= 25) & (df['BMI'] < 30), 'BMI Group'] = 1\n    df.loc[df['BMI'] >= 30, 'BMI Group'] = 2\n    \n    df.loc[df['BloodPressure'] < 80, 'BloodPressure Group'] = 0\n    df.loc[(df['BloodPressure'] >= 80) & (df['BloodPressure'] < 90), 'BloodPressure Group'] = 1\n    df.loc[(df['BloodPressure'] >= 90) & (df['BloodPressure'] <= 120), 'BloodPressure Group'] = 2\n    df.loc[df['BloodPressure'] > 120, 'BloodPressure Group'] = 3\n    \n    df.head()\n    \n    return df","2b38ea23":"type(df_train.columns)","1baef9bc":"def ndarray_to_df(ndarray: np.ndarray, cols: pd.Index = df_train.columns) -> pd.DataFrame:\n    \"\"\"Convert an ndarray to a dataframe with certain columns\n    \n    Args:\n        ndarray: The initial numpy array\n        cols: The columns of the new dataframe\n    \n    Returns:\n        A new dataframe with the column names included\n    \"\"\"\n    new_df = pd.DataFrame(ndarray, columns=cols)\n    return new_df","2560bbf8":"from sklearn.preprocessing import FunctionTransformer, RobustScaler, StandardScaler\n\n\ntest_feature_pipeline = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='mean')),\n    ('pandarizer', FunctionTransformer(ndarray_to_df)),\n#     ('log_transform', FunctionTransformer(unskew_with_log)),\n#     ('kmeans_cluster', FunctionTransformer(kmeans_cluster, kw_args={'n_clusters': 2})),\n#     ('kmeans_dist', FunctionTransformer(kmeans_cluster_dist, kw_args={'n_clusters': 2})),\n#     ('pc_pca', FunctionTransformer(create_pc_pca, kw_args={'feature_eng': False})),\n    # from testing, this is a very good transformation function\n    ('pc_loading_features', FunctionTransformer(pca_loading_features, kw_args={'ignore_features': []})),\n#     ('log_transform', FunctionTransformer(unskew_with_log)),\n#     ('above_median_counts', FunctionTransformer(row_median_counts)),\n    ('remove_outliers', FunctionTransformer(remove_outliers, kw_args={'features':  ['SkinThickness', 'Insulin',\n       'DiabetesPedigreeFunction', 'Age']})),\n#     , kw_args={'features': ['Age', 'SkinThickness']}\n    ('bin_data', FunctionTransformer(bin_data)),\n#     ('log_transform', FunctionTransformer(unskew_with_log)),\n#     ('view_data', Debug()),\n#     ('scaler', StandardScaler()),\n    ('model', xgb.XGBClassifier(verbosity=0, random_state=0))\n])\n\ntest_feature_pipeline","213bf74e":"feature_scoring = score_pipeline_model(df_train, y_train, test_feature_pipeline)\nprint(feature_scoring)","19b01d1b":"outcome, feature_preds = get_preds(df_train, y_train, test_feature_pipeline)\nprint(classification_report(outcome, feature_preds))","cff3e2f0":"cm = confusion_matrix(outcome, feature_preds)\nax = sns.heatmap(cm, annot=True)","72b28d29":"from sklearn.ensemble import RandomForestClassifier\n\nrf_pipeline = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='median')),\n    ('pandarizer', FunctionTransformer(ndarray_to_df)),\n#     ('log_transform', FunctionTransformer(unskew_with_log)),\n#     ('kmeans_cluster', FunctionTransformer(kmeans_cluster, kw_args={'n_clusters': 2})),\n    ('kmeans_dist', FunctionTransformer(kmeans_cluster_dist, kw_args={'n_clusters': 2})),\n#     ('pc_pca', FunctionTransformer(create_pc_pca, kw_args={'feature_eng': False})),\n    # from testing, this is a very good transformation function\n    ('pc_loading_features', FunctionTransformer(pca_loading_features, kw_args={'ignore_features': []})),\n#     ('above_median_counts', FunctionTransformer(row_median_counts)),\n     ('remove_outliers', FunctionTransformer(remove_outliers, kw_args={'features':  [ 'SkinThickness', 'Insulin',\n       'DiabetesPedigreeFunction', 'Age']})),\n    ('bin_data', FunctionTransformer(bin_data)),\n    ('view_data', Debug()),\n    ('model', RandomForestClassifier(random_state=0))\n])\n\nrf_pipeline","b4177964":"rf_scoring = score_pipeline_model(df_train, y_train, rf_pipeline)\nprint(rf_scoring)","02ed9022":"outcome, feature_preds = get_preds(df_train, y_train, rf_pipeline)\nprint(classification_report(outcome, feature_preds))","a8fac3ac":"cm = confusion_matrix(outcome, feature_preds)\nax = sns.heatmap(cm, annot=True)","74cbe574":"from sklearn.svm import SVC\n\ntest_feature_pipeline = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='mean')),\n    ('pandarizer', FunctionTransformer(ndarray_to_df)),\n#     ('log_transform', FunctionTransformer(unskew_with_log)),\n    ('kmeans_cluster', FunctionTransformer(kmeans_cluster, kw_args={'n_clusters': 2})),\n#     ('kmeans_dist', FunctionTransformer(kmeans_cluster_dist, kw_args={'n_clusters': 2})),\n#     ('pc_pca', FunctionTransformer(create_pc_pca, kw_args={'feature_eng': False})),\n    # from testing, this is a very good transformation function\n    ('pc_loading_features', FunctionTransformer(pca_loading_features, kw_args={'ignore_features': []})),\n#     ('log_transform', FunctionTransformer(unskew_with_log)),\n#     ('above_median_counts', FunctionTransformer(row_median_counts)),\n#     ('remove_outliers', FunctionTransformer(remove_outliers, kw_args={'features':  ['SkinThickness', 'Insulin','Age']})),\n#     , kw_args={'features': ['Age', 'SkinThickness']}\n    ('bin_data', FunctionTransformer(bin_data)),\n    ('log_transform', FunctionTransformer(unskew_with_log)),\n#     ('view_data', Debug()),\n    ('scaler', StandardScaler()),\n    ('model', SVC(random_state=0, kernel='linear'))\n])\n\ntest_feature_pipeline","6cb71cea":"feature_scoring = score_pipeline_model(df_train, y_train, test_feature_pipeline)\nprint(feature_scoring)","c74771d3":"outcome, feature_preds = get_preds(df_train, y_train, test_feature_pipeline)\nprint(classification_report(outcome, feature_preds))","5aa41c6e":"cm = confusion_matrix(outcome, feature_preds)\nax = sns.heatmap(cm, annot=True)","f49baca8":"from tensorflow import keras\nfrom tensorflow.keras import layers\nimport tensorflow as tf\n\nX_train_neural, X_test_neural, y_train_neural, y_test_neural = train_test_split(df_train, y_train, test_size=0.2, stratify=y_train, random_state=42)\nX_train_nt, X_valid_nt, y_train_nt, y_valid_nt = train_test_split(X_train_neural, y_train_neural, test_size=0.3, stratify=y_train_neural, random_state=42)\n\ndata_cleaning_pipeline = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='median')),\n    ('pandarizer', FunctionTransformer(ndarray_to_df)),\n    ('log_transform', FunctionTransformer(unskew_with_log)),\n    ('kmeans_cluster', FunctionTransformer(kmeans_cluster, kw_args={'n_clusters': 2})),\n    ('kmeans_dist', FunctionTransformer(kmeans_cluster_dist, kw_args={'n_clusters': 2})),\n    ('pc_pca', FunctionTransformer(create_pc_pca, kw_args={'feature_eng': False})),\n    # from testing, this is a very good transformation function\n    ('pc_loading_features', FunctionTransformer(pca_loading_features, kw_args={'ignore_features': []})),\n    ('above_median_counts', FunctionTransformer(row_median_counts)),\n     ('remove_outliers', FunctionTransformer(remove_outliers, kw_args={'features':  [ 'SkinThickness', 'Insulin',\n       'DiabetesPedigreeFunction', 'Age']})),\n    ('bin_data', FunctionTransformer(bin_data)),\n    ('scaler', StandardScaler())\n])\n\nX_train_nt","5feb4b32":"X_train_nt = data_cleaning_pipeline.fit_transform(X_train_nt)\nX_valid_nt = data_cleaning_pipeline.transform(X_valid_nt)\nX_valid_nt","8aacaefe":"from tensorflow.keras.callbacks import EarlyStopping\n\nmodel = keras.Sequential([\n    # normalize input data again in the inout layer\n    layers.BatchNormalization(input_shape=[X_train_nt.shape[1]]),\n    \n    # hidden dense layer 1\n    layers.Dense(8),\n    layers.BatchNormalization(),\n    layers.Activation('relu'),\n    layers.Dropout(0.25),\n    \n    # hidden dense layer 2\n    layers.Dense(8),\n#     layers.BatchNormalization(),\n    layers.Activation('relu'),\n    layers.Dropout(0.25),\n    \n    # output layer\n    layers.Dense(1, activation='sigmoid')\n])\n\n# compile the model using adam and binary_crossentropy\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=[tf.keras.metrics.BinaryAccuracy(name=\"binary_accuracy\", dtype=None, threshold=0.4)])\n\n# set early stopping to prevent over fitting\nearly_stopping = EarlyStopping(patience=10, min_delta=0.001, restore_best_weights=True)\n\n# fit the model and check it's performance\nhistory = model.fit(\n    X_train_nt, y_train_nt,\n    validation_data=(X_valid_nt, y_valid_nt),\n    batch_size=140,\n    epochs=1000,\n    callbacks=[early_stopping],\n    verbose=0, \n)","ade80462":"history_df = pd.DataFrame(history.history)\nhistory_df.loc[:, ['loss', 'val_loss']].plot()\nhistory_df.loc[:, ['binary_accuracy', 'val_binary_accuracy']].plot()\n\nprint((f\"Best Validation Loss: {round(history_df['val_loss'].min(), 4)}\" + f\"\\nBest Validation Accuracy: {round(history_df['val_binary_accuracy'].max(), 4)}\"))","70917656":"X_test_neural = data_cleaning_pipeline.transform(X_test_neural)\nnn_preds = model.predict(X_test_neural)\nnn_preds","fa6058f1":"binary_nn_preds = []\n\nfor pred in nn_preds:\n    # 0.36 cutoff from the softmax function to predict a diabetic\n    if pred >= 0.4:\n        binary_nn_preds.append(1)\n    else:\n        binary_nn_preds.append(0)\n\nprint(classification_report(y_test_neural, binary_nn_preds))","76a4a8e4":"cm = confusion_matrix(y_test_neural, binary_nn_preds)\nax = sns.heatmap(cm, annot=True)","be286821":"def clean_data_for_nn(df, pipeline):\n    \n    clean_df = pipeline.transform(df)\n    return clean_df\n    ","f2f38a8d":"df_test_nn = clean_data_for_nn(df_test, data_cleaning_pipeline)\nfinal_nn_preds = model.predict(df_test_nn)\n\nfinal_binary_nn_preds = []\n\nfor pred in final_nn_preds:\n    # 0.4 cutoff from the softmax function to predict a diabetic\n    if pred >= 0.4:\n        final_binary_nn_preds.append(1)\n    else:\n        final_binary_nn_preds.append(0)\n\nprint(classification_report(y_test, final_binary_nn_preds))\ncm = confusion_matrix(y_test, final_binary_nn_preds)\nax = sns.heatmap(cm, annot=True)","2f8d0a6e":"# use optuna for hyperparameter tuning\nimport optuna\n\ndef objective(trial):\n    params = {\n        'max_depth': trial.suggest_int('max_depth', 3, 10),\n        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1), \n        'n_estimators': trial.suggest_int('n_estimators', 1000, 3000),\n        'min_child_weight': trial.suggest_int(\"min_child_weight\", 1, 10),\n        'colsample_bytree': trial.suggest_float(\"colsample_bytree\", 0.5, 1.0),\n        'subsample': trial.suggest_float(\"subsample\", 0.5, 1.0),  \n        'gamma': trial.suggest_float('gamma', 0.5, 2.5)\n    }\n    \n    xgb_pipeline = Pipeline(steps=[\n        ('imputer', SimpleImputer(strategy='mean')),\n        ('pandarizer', FunctionTransformer(ndarray_to_df)),\n        # from testing, this is a very good transformation function\n        ('pc_loading_features', FunctionTransformer(pca_loading_features, kw_args={'ignore_features': []})),\n        ('remove_outliers', FunctionTransformer(remove_outliers, kw_args={'features':  ['SkinThickness', 'Insulin',\n           'DiabetesPedigreeFunction', 'Age']})),\n        ('bin_data', FunctionTransformer(bin_data)),\n        ('model', xgb.XGBClassifier(**params))\n    ])\n    \n    return score_pipeline_model(df_train, y_train, xgb_pipeline, tuning=True)\n\nstudy = optuna.create_study(direction='maximize')\nstudy.optimize(objective, n_trials=45)\nxgb_best_params = study.best_params","e596f460":"print(xgb_best_params)","13550536":"# use optuna for rf hyperparameter tuning\ndef rf_objective(trial):\n    params = {\n        'max_depth': trial.suggest_int('max_depth', 10, 100, 10),\n        'n_estimators': trial.suggest_int('n_estimators', 200, 3000, 100),\n        # after feature engineering, there are around 30 features\n        'max_features': trial.suggest_int(\"max_features\", 3, 10),\n        'min_samples_leaf': trial.suggest_int(\"min_samples_leaf\", 1, 15),\n        'min_samples_split': trial.suggest_int(\"min_samples_split\", 2, 20),  \n    }\n    \n    rf_pipeline = Pipeline(steps=[\n        ('imputer', SimpleImputer(strategy='median')),\n        ('pandarizer', FunctionTransformer(ndarray_to_df)),\n        ('kmeans_dist', FunctionTransformer(kmeans_cluster_dist, kw_args={'n_clusters': 2})),\n        ('pc_loading_features', FunctionTransformer(pca_loading_features, kw_args={'ignore_features': []})),\n        ('remove_outliers', FunctionTransformer(remove_outliers, kw_args={'features':  [ 'SkinThickness', 'Insulin', 'DiabetesPedigreeFunction', 'Age']})),\n        ('bin_data', FunctionTransformer(bin_data)),\n        ('view_data', Debug()),\n        ('model', RandomForestClassifier(**params))\n    ])\n    \n    return score_pipeline_model(df_train, y_train, rf_pipeline, tuning=True)\n\nstudy_2 = optuna.create_study(direction='maximize')\nstudy_2.optimize(rf_objective, n_trials=30)\nrf_best_params = study_2.best_params","6af927a6":"final_xgb_pipeline = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='mean')),\n    ('pandarizer', FunctionTransformer(ndarray_to_df)),\n    ('pc_loading_features', FunctionTransformer(pca_loading_features, kw_args={'ignore_features': []})),\n    ('remove_outliers', FunctionTransformer(remove_outliers, kw_args={'features':  ['SkinThickness', 'Insulin',\n       'DiabetesPedigreeFunction', 'Age']})),\n    ('bin_data', FunctionTransformer(bin_data)),\n    ('view_data', Debug()),\n    ('model', xgb.XGBClassifier(**xgb_best_params))\n])\n\nfinal_xgb_pipeline.fit(df_train, y_train)\n# print(final_xgb_pipeline['view_data'].X_)\nfinal_predictions = final_xgb_pipeline.predict(df_test)\n\nprint(classification_report(y_test, final_predictions))","88217c19":"cm = confusion_matrix(y_test, final_predictions)\nax = sns.heatmap(cm, annot=True)","8d98c35f":"final_rf_pipeline = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='mean')),\n    ('pandarizer', FunctionTransformer(ndarray_to_df)),\n#     ('kmeans_dist', FunctionTransformer(kmeans_cluster_dist, kw_args={'n_clusters': 2})),\n    ('pc_loading_features', FunctionTransformer(pca_loading_features, kw_args={'ignore_features': []})),\n#     ('remove_outliers', FunctionTransformer(remove_outliers, kw_args={'features':  [ 'SkinThickness', 'Insulin', 'DiabetesPedigreeFunction', 'Age']})),\n    ('bin_data', FunctionTransformer(bin_data)),\n    ('view_data', Debug()),\n    ('model', RandomForestClassifier(**rf_best_params))\n])\n\nfinal_rf_pipeline.fit(df_train, y_train)\nfinal_rf_preds = final_rf_pipeline.predict(df_test)\n\nprint(classification_report(y_test, final_rf_preds))","eb007a48":"cm = confusion_matrix(y_test, final_rf_preds)\nax = sns.heatmap(cm, annot=True)","c35ae6cd":"# use optuna for hyperparameter tuning\nimport optuna\n\ndef objective(trial):\n    params = {\n        'C': trial.suggest_int('C', 1, 3),\n        'gamma': trial.suggest_float('gamma', 0.01, 0.1)\n    }\n    \n    svm_pipeline = Pipeline(steps=[\n        ('imputer', SimpleImputer(strategy='mean')),\n        ('pandarizer', FunctionTransformer(ndarray_to_df)),\n        ('kmeans_cluster', FunctionTransformer(kmeans_cluster, kw_args={'n_clusters': 2})),\n        ('pc_loading_features', FunctionTransformer(pca_loading_features, kw_args={'ignore_features': []})),\n        ('bin_data', FunctionTransformer(bin_data)),\n        ('log_transform', FunctionTransformer(unskew_with_log)),\n        ('scaler', StandardScaler()),\n        ('model', SVC(**params))\n    ])\n    \n    return score_pipeline_model(df_train, y_train, svm_pipeline, tuning=True)\n\nstudy = optuna.create_study(direction='maximize')\nstudy.optimize(objective, n_trials=35)\nsvm_best_params = study.best_params","33947208":"final_svm_pipeline = Pipeline(steps=[\n        ('imputer', SimpleImputer(strategy='mean')),\n        ('pandarizer', FunctionTransformer(ndarray_to_df)),\n        ('kmeans_cluster', FunctionTransformer(kmeans_cluster, kw_args={'n_clusters': 2})),\n        ('pc_loading_features', FunctionTransformer(pca_loading_features, kw_args={'ignore_features': []})),\n        ('bin_data', FunctionTransformer(bin_data)),\n        ('log_transform', FunctionTransformer(unskew_with_log)),\n        ('scaler', StandardScaler()),\n        ('model', SVC(**svm_best_params))\n])\n\nfinal_svm_pipeline.fit(df_train, y_train)\n# print(final_xgb_pipeline['view_data'].X_)\nfinal_svm_predictions = final_svm_pipeline.predict(df_test)\n\nprint(classification_report(y_test, final_svm_predictions))","05b57546":"cm = confusion_matrix(y_test, final_svm_predictions)\nax = sns.heatmap(cm, annot=True)","734f1fcf":"- There are many columns that contain various 0 values; these are likely Na values and should be replaced\n- Pregnancies and Outcome with values of zero should be kept","7342887b":"- PC 1 shows that there are people who have high insulin, glucose, age, and pregnancies. These can be added to make a new 'vulnerable' category\n- PC 3 shows a potential grouping of low bmi and high insulin (ratio can be made)","152ca656":"- By comparing the graphs, it is evident that blood glucose, age, and BMI mark visible differences\n- The diabetes pedigree function is surprisingly not as useful as I would have expected it to be\n- 3 features seemed to be normalized better using a log transformation: consider replacing the features by the logged versions. The 3 features were: DiabetesPedigreeFunction, Insulin, and Age. The pregnency feature is also well normalized with a log transformation. \n- Many of the graphs also have outliers in the upper ranges so it would be good to scale those values into more normal ranges to not overfit the model","cba58136":"## 4. Baseline Scoring\n- With data now being clean, make a function which can score a model based on particular data\n- Call this function with the initial train data and basic model to get a baseline score","7deae2b1":"## 7. Create a model using deep learning\n- See if a neural network can have better results on the data","463f9c0f":"## 5. Feature Engineering\n- Apply feature engineering (to just training data for the initial testing but whole data set on full run)\n- Apply Kmeans for grouping in clusters as well as distance to a certain cluster\n- Apply PCA to explore loadings of Principal Components (PC)\n- Given loadings of PC, apply mathematical transformations and interactions between data (ratios, sums, diff, etc)\n- Consider applying log transformations to certain skewed features\n- Make counts (XGB is a tree based model which cannot aggregate well across multiple columns). Do counts for above and below the respective medians\n- Flag or move outliers into the interquartile range (IQR). Some data outside of this range my not be outliers - do not just remove extreme data.\n- Create bins for numerical columns to avoid over fitting\n- Make sure all transformations are in functions (so that function transformers can be applied to the pipeline)\n- Make a pipeline containing the transformations and try out different combinations to see what gives the best results","3492363f":"- Factors that are commonly known to be associated with diabetes occur in adults who have type 2 diabetes\n- Type 1 diabetes may occur in kids and the causes are still unknown\n- Adults with a certain combinations of factors (like obesity or family history) are more likely to develop T2D\n- Family history also affects T1D\n- The diabetes pedigree function takes family history into account","aa6e25f7":"- Split up the data in order to avoid leakage\n- Only use df_train and y_train for training the model\n- The testing data should not be looked at so it will be left aside until the final model is created (to test the final model)","440f7690":"## 3. Basic model\n- The data has NaN values and has not been split up\n- Create a basic pipeline which can deal with imputing and applies an XGBClassifier\n- Use cross validation on the classifier to ensure accurate scores; call a scoring function with the initial train data and basic model to get a baseline score","84db1868":"- The featues between glucose and BMI seem to have the most effect. This is seen in the EDA and in the mi scores for the data\n- Use these 5 features for PCA and Kmeans","49f56ff7":"**Final Model**\n- Create a final model using the best hyperparametrs and evalute it on the test set","6dfb5205":"### Feature Engineering pipeline results\n- Each model has similar results on the dataset\n- Take each of these models and apply hyperparameter tuning to make the final predictions\n- **Note:** The data was also trained on a **decision tree classifier** and **logistic regression** model but training results were not as good so these models have been omited from the notebook","c4788b93":"## 1. Read in the data","8885e44c":"## 2. Explore and vizualize the data","21fa7168":"- Evident from the graph, the training data contains more non diabetics than diabetics","263b229a":"- Feature engineering pipeline has been decided - test it's performance against a random forest model","51d9c070":"## ","e3e6b12c":"## 8. Hyperparameter tuning and Final models\n- Use the best features to create a final pipeline\n- Split the data into train\/test data \n- Apply hyperparameters on the train data\n- Create a final model using the training data and best hyperparameters\n- Use the validation data to test the final model; use classification report to see how the model did","60cb00a1":"## 6. Machine learning\n- Using a pipeline, find the combination of 'feature engineering functions' which will provide the best results for a basic model\n- Use both and XGBoostClassifier and a RandomForrestClassifier\n- Create an ANN and evaluate it's performance (scale data for ANN)","62a7faae":"**About Scoring Method of Choice**\n- calculate the cross validation score using F1 as the main guide for performance (to take into account precision and recall\/sensitivity)\n- remember: precision is ratio of true diabetics to all people predicted to be diabetic (TP\/TP+FP)\n- recall\/sensitivity is the ratio of true diabetics to all TRUE diabetics (TP\/TP+FN)\n- we care less about specificity since we would rather flag someone as diabetic and have them checked by a doctor to determine that they are not diabetic. Therefore, F1 scores is a good measure to see how the model performs in the cases that we most care about; the cases that are diabetic\n- change some of the functions in the feature pipeline to get a pipeline that creates the best features on the basic XGB Classifier. This will provide us with the best features to train our final model \n","4fb953a4":"- PC 2 in the above loadings show a contrast between age and bmi: low bmi and high age (may be expressed as a ratio)\n- PC 4 in the above loadings and PC 3 in the below loadings show a contrast between glucose and insulin: low glucose and high insulin","e071f1fd":"## 9. Final Thoughts\n- Other than the neural network, each of the tuned final models performed relatively similarly on the final test set\n- Hope you learned from the feature engineering techniques!","0ac93965":"- Make a function to calculate MI scores\n- Test on some data and use it on PCA to check usefulness of principal components (PCs)","eccd82f2":"- Initial model has accuracy of about 71%\n- Use feature engineering to improve the performance"}}