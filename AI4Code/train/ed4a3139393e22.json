{"cell_type":{"2ebc4035":"code","3feee5bd":"code","97486a37":"code","c9a41907":"code","459c8731":"code","6425c11d":"code","f560110f":"code","14ee88c2":"code","8b7b1c19":"code","26fffe0d":"code","9f957221":"code","efb8b0e6":"code","864f9a74":"code","9716c4ff":"code","6634e358":"code","10803d5a":"code","5937bb40":"code","44b1ad13":"code","0ef0d2d3":"code","6d6bbbc5":"code","46999aab":"code","df60152b":"code","ff57bcf2":"code","50b6c2f6":"code","e58ba5d4":"code","bb764ac2":"code","3652ee6a":"code","13e7dce2":"code","3dfb6089":"code","7fdaf059":"code","63ab1143":"code","52809afb":"code","bff0de22":"code","605b7158":"code","f5162bfc":"code","398c9cf8":"code","d7e6f92c":"code","5f36796c":"code","e1df2f18":"code","67bd64d8":"code","39abf08b":"code","bde71827":"markdown","fc23f648":"markdown","b1a68c31":"markdown","53a41c5b":"markdown","0d54292b":"markdown","7ae86331":"markdown","a7b07383":"markdown","b1a3d340":"markdown","b9067bc4":"markdown","544a8a15":"markdown","e35499f5":"markdown","cd84fc66":"markdown","7e0a6232":"markdown","163d2668":"markdown"},"source":{"2ebc4035":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.ensemble import VotingClassifier \nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier, ExtraTreesClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.feature_selection import RFECV\nfrom sklearn import metrics\n\n%matplotlib inline\nplt.rcParams['figure.figsize'] = [10,7]\n\nfrom IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = \"all\"\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","3feee5bd":"train_df = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ntest_df = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\ndata = train_df.append(test_df)","97486a37":"train_df","c9a41907":"test_df","459c8731":"train_df.info()","6425c11d":"train_df.isnull().sum()","f560110f":"test_df.isnull().sum()","14ee88c2":"train_df.corr()","8b7b1c19":"g = sns.heatmap(train_df.drop('PassengerId',axis=1).corr(), annot=True, cmap='coolwarm')","26fffe0d":"#Explore Pclass vs Survived\ng = sns.catplot(data=train_df, y='Survived', x='Pclass', kind='bar')","9f957221":"#Explore Age vs Survived\ng = sns.catplot(data=train_df, x='Survived', y='Age', kind='bar', height=5)","efb8b0e6":"#Explore SibSp vs Survived\ng = sns.catplot(data=train_df, x='SibSp', y='Survived', kind='bar', height=5)","864f9a74":"#Explore Parch vs Survived\ng = sns.catplot(data=train_df, x='Parch', y='Survived', kind='bar', height=5)","9716c4ff":"#Explore Fare vs Survived\ng = sns.catplot(data=train_df, y='Fare', x='Survived', kind='bar', height=5)","6634e358":"data['Title'] = data['Name']\nfor name_string in data['Name']:\n    data['Title'] = data['Name'].str.extract('([A-Za-z]+)\\.', expand=True)\ndata['Title'].value_counts()\n","10803d5a":"title_changes = {'Mlle': 'Miss', 'Major': 'Mr', 'Col': 'Mr', 'Sir': 'Mr', 'Don': 'Mr', 'Mme': 'Miss',\n          'Jonkheer': 'Mr', 'Lady': 'Mrs', 'Capt': 'Mr', 'Countess': 'Mrs', 'Ms': 'Miss', 'Dona': 'Mrs'}\ndata.replace({'Title': title_changes}, inplace=True)\ntitles = ['Dr', 'Master', 'Miss', 'Mr', 'Mrs', 'Rev']\nfor title in titles:\n    age_to_impute = data.groupby('Title')['Age'].median()[titles.index(title)]\n    data.loc[(data['Age'].isnull()) & (data['Title'] == title), 'Age'] = age_to_impute\ntrain_df['Age'] = data['Age'][:891]\ntest_df['Age'] = data['Age'][891:]\ndata.drop('Title', axis = 1, inplace = True)","5937bb40":"data['Family_Size'] = data['Parch'] + data['SibSp'] + 1\ntrain_df['Family_Size'] = data['Family_Size'][:891]\ntest_df['Family_Size'] = data['Family_Size'][891:]","44b1ad13":"data['Last_Name'] = data['Name'].apply(lambda x: str.split(x, \",\")[0])\ndata['Fare'].fillna(data['Fare'].mean(), inplace=True)\nDEFAULT_SURVIVAL_VALUE = 0.5\ndata['Family_Survival'] = DEFAULT_SURVIVAL_VALUE","0ef0d2d3":"for grp, grp_df in data[['Survived','Name', 'Last_Name', 'Fare', 'Ticket', 'PassengerId',\n                           'SibSp', 'Parch', 'Age', 'Cabin']].groupby(['Last_Name', 'Fare']):   \n    if (len(grp_df) != 1):\n        for ind, row in grp_df.iterrows():\n            smax = grp_df.drop(ind)['Survived'].max()\n            smin = grp_df.drop(ind)['Survived'].min()\n            passID = row['PassengerId']\n            if (smax == 1.0):\n                data.loc[data['PassengerId'] == passID, 'Family_Survival'] = 1\n            elif (smin==0.0):\n                data.loc[data['PassengerId'] == passID, 'Family_Survival'] = 0\nprint(\"Number of passengers with family survival information:\", \n      data.loc[data['Family_Survival']!=0.5].shape[0])","6d6bbbc5":"for _, grp_df in data.groupby('Ticket'):\n    if (len(grp_df) != 1):\n        for ind, row in grp_df.iterrows():\n            if (row['Family_Survival'] == 0) | (row['Family_Survival']== 0.5):\n                smax = grp_df.drop(ind)['Survived'].max()\n                smin = grp_df.drop(ind)['Survived'].min()\n                passID = row['PassengerId']\n                if (smax == 1.0):\n                    data.loc[data['PassengerId'] == passID, 'Family_Survival'] = 1\n                elif (smin==0.0):\n                    data.loc[data['PassengerId'] == passID, 'Family_Survival'] = 0                      \nprint(\"Number of passenger with family\/group survival information: \" \n      +str(data[data['Family_Survival']!=0.5].shape[0]))\ntrain_df['Family_Survival'] = data['Family_Survival'][:891]\ntest_df['Family_Survival'] = data['Family_Survival'][891:]","46999aab":"data['Fare'].fillna(data['Fare'].median(), inplace = True)\ndata['FareBin'] = pd.qcut(data['Fare'], 5)\nlabel = LabelEncoder()\ndata['FareBin_Code'] = label.fit_transform(data['FareBin'])\ntrain_df['FareBin_Code'] = data['FareBin_Code'][:891]\ntest_df['FareBin_Code'] = data['FareBin_Code'][891:]\ntrain_df.drop(['Fare'], 1, inplace=True)\ntest_df.drop(['Fare'], 1, inplace=True)","df60152b":"data['AgeBin'] = pd.qcut(data['Age'], 4)\nlabel = LabelEncoder()\ndata['AgeBin_Code'] = label.fit_transform(data['AgeBin'])\ntrain_df['AgeBin_Code'] = data['AgeBin_Code'][:891]\ntest_df['AgeBin_Code'] = data['AgeBin_Code'][891:]\ntrain_df.drop(['Age'], 1, inplace=True)\ntest_df.drop(['Age'], 1, inplace=True)","ff57bcf2":"train_df['Sex'].replace(['male','female'],[0,1],inplace=True)\ntest_df['Sex'].replace(['male','female'],[0,1],inplace=True)\ntrain_df.drop(['Name', 'PassengerId', 'SibSp', 'Parch', 'Ticket', 'Cabin',\n               'Embarked'], axis = 1, inplace = True)\ntest_df.drop(['Name','PassengerId', 'SibSp', 'Parch', 'Ticket', 'Cabin',\n              'Embarked'], axis = 1, inplace = True)","50b6c2f6":"train_df.head(10)","e58ba5d4":"test_df.head(10)","bb764ac2":"x = train_df.drop('Survived', axis=1)\ny = train_df['Survived']\n\nx_test = test_df.copy()","3652ee6a":"from sklearn.model_selection import KFold\n#Stratified k fold cross validation\nkfold = KFold(n_splits=10)","13e7dce2":"std_scaler = StandardScaler()\nx = std_scaler.fit_transform(x)\nx_test = std_scaler.transform(x_test)","3dfb6089":"# Store scores of all models\ngs_scores = {}","7fdaf059":"# Adaboost\nDTC = DecisionTreeClassifier()\n\nadaDTC = AdaBoostClassifier(DTC, random_state=7)\n\nada_param_grid = {\"base_estimator__criterion\" : [\"gini\", \"entropy\"],\n              \"base_estimator__splitter\" :   [\"best\", \"random\"],\n              \"algorithm\" : [\"SAMME\",\"SAMME.R\"],\n              \"n_estimators\" :[1,2],\n              \"learning_rate\":  [0.0001, 0.001, 0.01, 0.1, 0.2, 0.3,1.5]}\n\ngsadaDTC = GridSearchCV(adaDTC,param_grid = ada_param_grid, cv=kfold, scoring=\"accuracy\", n_jobs= -1, verbose = 1)\n\ngsadaDTC.fit(x,y)\n\nada_best = gsadaDTC.best_estimator_\n\n#Best score \ngs_scores['Ada'] = gsadaDTC.best_score_","63ab1143":"# ExtraTrees \nExtC = ExtraTreesClassifier()\n\n\n## Search grid for optimal parameters\nex_param_grid = {\"max_depth\": [None],\n              \"max_features\": [None, 'auto'],\n              \"min_samples_split\": [2, 3, 10],\n              \"min_samples_leaf\": [1, 3, 10],\n              \"bootstrap\": [False],\n              \"n_estimators\" :[100,300],\n              \"criterion\": [\"gini\"]}\n\n\ngsExtC = GridSearchCV(ExtC,param_grid = ex_param_grid, cv=kfold, scoring=\"accuracy\", n_jobs= -1, verbose = 1)\n\ngsExtC.fit(x,y)\n\nExtC_best = gsExtC.best_estimator_\n\n# Best score\ngs_scores['ET'] = gsExtC.best_score_","52809afb":"# RFC Parameters tunning \nRFC = RandomForestClassifier()\n\n\n## Search grid for optimal parameters\nrf_param_grid = {\"max_depth\": [None],\n              \"max_features\": [None, 'auto'],\n              \"min_samples_split\": [2, 3, 10],\n              \"min_samples_leaf\": [1, 3, 10],\n              \"bootstrap\": [False],\n              \"n_estimators\" :[100,300],\n              \"criterion\": [\"gini\"]}\n\n\ngsRFC = GridSearchCV(RFC,param_grid = rf_param_grid, cv=kfold, scoring=\"accuracy\", n_jobs= -1, verbose = 1)\n\ngsRFC.fit(x,y)\n\nRFC_best = gsRFC.best_estimator_\n\n# Best score\ngs_scores['RFC'] = gsRFC.best_score_","bff0de22":"# Gradient boosting tunning\n\nGBC = GradientBoostingClassifier()\ngb_param_grid = {'loss' : [\"deviance\"],\n              'n_estimators' : [100,200,300],\n              'learning_rate': [0.1, 0.05, 0.01],\n              'max_depth': [4, 8],\n              'min_samples_leaf': [100,150],\n              'max_features': [None, 'auto'] \n              }\n\ngsGBC = GridSearchCV(GBC,param_grid = gb_param_grid, cv=kfold, scoring=\"accuracy\", n_jobs= -1, verbose = 1)\n\ngsGBC.fit(x,y)\n\nGBC_best = gsGBC.best_estimator_\n\n# Best score\ngs_scores['GBC'] = gsGBC.best_score_","605b7158":"# SVM\n\nSVMC = SVC(probability=True)\nsvc_param_grid = {'kernel':['rbf'],\n                  'gamma':[0.001,0.01,0.1,1],\n                  'C':[0.1,1,10,50,100,200,300,]}\ngsSVMC = GridSearchCV(SVMC, param_grid=svc_param_grid, cv=kfold, scoring='accuracy', n_jobs = -1, verbose = 1)\n\ngsSVMC.fit(x,y)\n\nSVMC_best = gsSVMC.best_estimator_\n\n# Best score\ngs_scores['SVC'] = gsSVMC.best_score_","f5162bfc":"# KNN\n\nknn = KNeighborsClassifier()\nn_neighbors = [6,7,8,9,10,11,12,14,16,18,20,22]\nalgorithm = ['auto']\nweights = ['uniform', 'distance']\nleaf_size = list(range(1,50,5))\nhyperparams = {'algorithm': algorithm,\n               'weights': weights,\n               'leaf_size': leaf_size,\n               'n_neighbors': n_neighbors}\n\ngsKNN=GridSearchCV(estimator = knn, param_grid = hyperparams, verbose=1, cv=10, scoring = \"roc_auc\", n_jobs=-1)\n\ngsKNN.fit(x, y)\n\nknn_best = gsKNN.best_estimator_\n\n# Best score\ngs_scores['KNN'] = gsKNN.best_score_","398c9cf8":"gs_scores_df = pd.DataFrame(gs_scores, index=[0])\ngs_scores_df","d7e6f92c":"g = sns.catplot(data=gs_scores_df,kind='bar')","5f36796c":"# def get_reduced_features(models):\n#     model_num=1\n#     for model in models:\n#         print('Model', model_num)\n#         rfecv = RFECV(estimator=model, step=1, cv=kfold, scoring='accuracy')\n#         rfecv.fit(x,y)\n#         print('Optimal no. of features: ', rfecv.n_features_)\n#         feat_support = rfecv.support_\n#         feat_grid_score = rfecv.grid_scores_\n#         print('Selected columns are: ', *x.columns[feat_support])\n#         print('Not selected columns are: ', *x.columns[feat_support!=True])\n#         print('Cross val score: ', feat_grid_score.mean(), end='\\n\\n')\n#         model_num+=1\n    \n\n# get_reduced_features([RFC_best,ExtC_best,ada_best,GBC_best])","e1df2f18":"# votingC = VotingClassifier(estimators=[('rfc', RFC_best), ('extc', ExtC_best),\n# ('svc', SVMC_best), ('adac',ada_best),('gbc',GBC_best), ('knn',knn_best)], voting='soft', n_jobs=-1)\n\n# votingC = votingC.fit(x,y)\n\n# from sklearn.model_selection import cross_val_score\n# print(cross_val_score(votingC, x, y, cv=10).mean())","67bd64d8":"#Get predictions for test data\n# y_pred = votingC.predict(x_test)\nknn = KNeighborsClassifier(algorithm='auto', leaf_size=26, metric='minkowski', \n                           metric_params=None, n_jobs=1, n_neighbors=6, p=2, \n                           weights='uniform')\nknn.fit(x, y)\ny_pred = knn.predict(x_test)","39abf08b":"temp = pd.DataFrame(pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")['PassengerId'])\ntemp['Survived'] = y_pred\ntemp.to_csv(\"..\/working\/submission.csv\", index = False)\nprint(\"Predictions submitted\")","bde71827":"# Titanic predictions using KNN","fc23f648":"The increased survival of passengers travelling with siblings\/parents\/children hints at the requirement of features \nthat indicate whether a passenger was travelling with family or not.\n\nFamilies had a higher survival rate than individuals","b1a68c31":"In order to identify the family\/group that each passenger belonged to, there are two approaches:\n\n1. Check for common last name and fare\n2. Check for same ticket initials\n\n","53a41c5b":"Organise Fare and Age into bins alongwith encoding for various features","0d54292b":"Fill the missing age values by the medians of titles for each row","7ae86331":"# Model training and predictions","a7b07383":"# Feature Engineering","b1a3d340":"## Use Names to create a Title column\n\nThere are various titles across all passengers; let's simplify that into 'Dr', 'Master', 'Miss', 'Mr', 'Mrs', 'Rev'(Reverend) ","b9067bc4":"Make a new feature that helps identify whether a passenger was alone or not ","544a8a15":"As expected, the economy class of passengers affects their survival","e35499f5":"Upon initial inspection, Age, SibSp and Parch have very weak relationships with Survived as compared to Pclass and Age\n\nHowever, there are transitive relationships such as Age, Pclass and Survived; these will need to be further inspected\n\nThe missing values will also affect correlation, which might change once those values are filled appropriately\n\nThe features Sex, Cabin, Ticket, Embarked need to be converted to integral values ","cd84fc66":"As observed, there are many missing values in Age and Cabin which will need to be filled\n\nFirst, let's try to see how the given features are related to Survived","7e0a6232":"# Import datasets","163d2668":"As previously observed, the (pseudo) economic status affected survival \n\nThe remaining features can also help in aiding and generating features "}}