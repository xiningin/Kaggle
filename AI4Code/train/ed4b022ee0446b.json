{"cell_type":{"be15ebf3":"code","dd8b4ccf":"code","f7ed3171":"code","2a02b96d":"code","95a5e607":"code","535f2cb7":"code","38034705":"code","bc945059":"code","bab95270":"code","76fd7d5f":"code","890a065e":"code","b3f7c74f":"code","d5d2d68e":"code","fcad02d7":"code","efd7e366":"code","8b33fcb8":"code","63a8ce20":"code","f666c609":"code","ed098eb9":"code","aefbdbc4":"code","c4f93a4c":"code","0b814110":"code","fc36ee6e":"code","9cf0b66d":"code","53ca73cd":"code","134699bc":"code","4549989c":"code","777192fb":"code","fc5a6042":"code","8267d598":"code","0ae0f119":"code","cd57ded2":"code","002fd0e0":"code","bb92235b":"code","c143d5b9":"code","2bbb0a96":"code","9456d770":"code","a58ac8b8":"code","d8459879":"code","9d75461a":"code","c2d8eff1":"code","95edab12":"code","4ff02dbb":"code","93167497":"code","8e5ce72f":"code","ba4aec03":"code","583ca150":"code","e8e704a0":"code","ff9af2e0":"code","e01ce682":"markdown","089db307":"markdown","8179169a":"markdown","96415ed5":"markdown","0ce4d896":"markdown","1cabb01c":"markdown","02f2508d":"markdown","6d5ebe70":"markdown","4464b989":"markdown","5a785d1c":"markdown","6ca5ae60":"markdown","5cad54cc":"markdown","b4bcc804":"markdown"},"source":{"be15ebf3":"# Import packages\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.metrics import make_scorer, accuracy_score, classification_report, confusion_matrix\n\nimport warnings\nwarnings.filterwarnings('ignore')\npd.set_option(\"display.max_columns\", None)","dd8b4ccf":"# Make scorer: accuracy\naccuracy = make_scorer(accuracy_score)","f7ed3171":"# Load dataset\ntrainSet = pd.read_csv('..\/input\/costa-rican-household-poverty-prediction\/train.csv')\ntestSet = pd.read_csv('..\/input\/costa-rican-household-poverty-prediction\/test.csv')\nsubmitSet = pd.read_csv('..\/input\/costa-rican-household-poverty-prediction\/sample_submission.csv')\n\ntrainSet.head()","2a02b96d":"# Drop columns with lacking data\ntrain = trainSet.drop(columns=['Id','idhogar','rez_esc', 'v18q1', 'v2a1', 'dependency', 'edjefe', 'edjefa'])\n\n# Drop rows with missing values\ntrain = train.dropna(axis=0)\n\nprint(train.shape)\ntrain.head()","95a5e607":"# Select features with high importance\nselected = ['tipovivi5', 'hogar_mayor', 'abastaguano', 'epared2', 'area1',\n       'tipovivi1', 'elimbasu2', 'refrig', 'mobilephone', 'energcocinar2',\n       'pisocemento', 'pareddes', 'elimbasu1', 'etecho2', 'lugar4',\n       'paredmad', 'paredzinc', 'lugar1', 'tamviv', 'lugar3',\n       'television', 'rooms', 'epared1', 'tipovivi3', 'etecho1',\n       'SQBedjefe', 'epared3', 'parentesco9', 'bedrooms', 'r4m2',\n       'overcrowding', 'sanitario5', 'paredzocalo', 'eviv1', 'paredpreb',\n       'etecho3', 'abastaguadentro', 'r4m3', 'techozinc', 'pisomadera',\n       'sanitario3', 'eviv2', 'tamhog', 'v14a', 'r4t2', 'public',\n       'lugar5', 'elimbasu3', 'r4m1', 'hacdor', 'r4t3', 'energcocinar4',\n       'r4h1', 'sanitario2', 'hogar_adul', 'r4h2', 'cielorazo',\n       'qmobilephone', 'tipovivi4', 'pisomoscer', 'meaneduc', 'tipovivi2',\n       'paredblolad', 'computer', 'r4t1', 'pisonotiene', 'SQBdependency',\n       'eviv3', 'hacapo', 'hogar_nin', 'v18q']","535f2cb7":"# train validation split\nX_train, X_val, y_train, y_val = train_test_split(train[selected], train['Target'],\n                                                  test_size=0.2, random_state=123,\n                                                  stratify=train['Target'])","38034705":"!apt install -y build-essential swig curl\n!curl https:\/\/raw.githubusercontent.com\/automl\/auto-sklearn\/master\/requirements.txt | xargs -n 1 -L 1 pip install\n!pip install auto-sklearn","bc945059":"from autosklearn.classification import AutoSklearnClassifier","bab95270":"# Create the model\nsklearn = AutoSklearnClassifier(time_left_for_this_task=3*60, per_run_time_limit=15, n_jobs=-1)\n\n# Fit the training data\nsklearn.fit(X_train, y_train)\n\n# Sprint Statistics\nprint(sklearn.sprint_statistics())\n\n# Predict the validation data\npred_sklearn = sklearn.predict(X_val)\n\n# Compute the accuracy\nprint('Accuracy: ' + str(accuracy_score(y_val, pred_sklearn)))","76fd7d5f":"# Prediction results\nprint('Confusion Matrix')\nprint(pd.DataFrame(confusion_matrix(y_val, pred_sklearn), index=[1,2,3,4], columns=[1,2,3,4]))\nprint('')\nprint('Classification Report')\nprint(classification_report(y_val, pred_sklearn))","890a065e":"# Show the models\nprint(sklearn.show_models())","b3f7c74f":"from tpot import TPOTClassifier","d5d2d68e":"# TPOT that are stopped earlier. It still gives temporary best pipeline.\n# Create model\ncv = RepeatedStratifiedKFold(n_splits=3, n_repeats=3, random_state=123) \ntpot = TPOTClassifier(generations=8, population_size=50, cv=cv, scoring='accuracy', verbosity=2, random_state=123, n_jobs=-1)\n\n# Fir the training data\ntpot.fit(X_train, y_train)\n\n# Export the result\ntpot.export('tpot_model.py')","fcad02d7":"# Create the model\ncv = RepeatedStratifiedKFold(n_splits=3, n_repeats=3, random_state=123) \ntpot = TPOTClassifier(generations=5, population_size=5, cv=cv, scoring='accuracy', verbosity=2, random_state=123, n_jobs=-1)\n\n# Fit the training data\ntpot.fit(X_train, y_train)\n\n# Export the result\ntpot.export('tpot_model.py')","efd7e366":"import numpy as np\nimport pandas as pd\nfrom sklearn.ensemble import ExtraTreesClassifier, RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import make_pipeline, make_union\nfrom tpot.builtins import StackingEstimator\nfrom tpot.export_utils import set_param_recursive\n\n# NOTE: Make sure that the outcome column is labeled 'target' in the data file\n#tpot_data = pd.read_csv('PATH\/TO\/DATA\/FILE', sep='COLUMN_SEPARATOR', dtype=np.float64)\n#features = tpot_data.drop('target', axis=1)\n#training_features, testing_features, training_target, testing_target = \\\n#            train_test_split(features, tpot_data['target'], random_state=123)\n\ntraining_features = X_train\ntesting_features = X_val\ntraining_target= y_train\ntesting_target = y_val\n\n# Average CV score on the training set was: 0.8822579941543428\nexported_pipeline = make_pipeline(\n    StackingEstimator(estimator=ExtraTreesClassifier(bootstrap=False, criterion=\"gini\", max_features=0.5, min_samples_leaf=13, min_samples_split=13, n_estimators=100)),\n    RandomForestClassifier(bootstrap=True, criterion=\"gini\", max_features=0.8500000000000001, min_samples_leaf=4, min_samples_split=12, n_estimators=100)\n)\n# Fix random state for all the steps in exported pipeline\nset_param_recursive(exported_pipeline.steps, 'random_state', 123)\n\nexported_pipeline.fit(training_features, training_target)\nresults = exported_pipeline.predict(testing_features)","8b33fcb8":"import numpy as np\nimport pandas as pd\nfrom sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import make_pipeline, make_union\nfrom tpot.builtins import StackingEstimator\nfrom tpot.export_utils import set_param_recursive\n\n# NOTE: Make sure that the outcome column is labeled 'target' in the data file\n#tpot_data = pd.read_csv('PATH\/TO\/DATA\/FILE', sep='COLUMN_SEPARATOR', dtype=np.float64)\n#features = tpot_data.drop('target', axis=1)\n#training_features, testing_features, training_target, testing_target = \\\n#            train_test_split(features, tpot_data['target'], random_state=123)\n\ntraining_features = X_train\ntesting_features = X_val\ntraining_target= y_train\ntesting_target = y_val\n\n# Average CV score on the training set was: 0.859616978580465\nexported_pipeline = make_pipeline(\n    StackingEstimator(estimator=GradientBoostingClassifier(learning_rate=0.001, max_depth=2, max_features=0.7000000000000001, min_samples_leaf=1, min_samples_split=19, n_estimators=100, subsample=0.15000000000000002)),\n    RandomForestClassifier(bootstrap=True, criterion=\"gini\", max_features=0.8500000000000001, min_samples_leaf=4, min_samples_split=12, n_estimators=100)\n)\n# Fix random state for all the steps in exported pipeline\nset_param_recursive(exported_pipeline.steps, 'random_state', 123)\n\nexported_pipeline.fit(training_features, training_target)\nresults = exported_pipeline.predict(testing_features)\n","63a8ce20":"pred_tpot = results\n\n# Compute the accuracy\nprint('Accuracy: ' + str(accuracy_score(y_val, pred_tpot)))\nprint('')\n\n# Prediction results\nprint('Confusion Matrix')\nprint(pd.DataFrame(confusion_matrix(y_val, pred_tpot), index=[1,2,3,4], columns=[1,2,3,4]))\nprint('')\nprint('Classification Report')\nprint(classification_report(y_val, pred_tpot))","f666c609":"!pip install git+https:\/\/github.com\/hyperopt\/hyperopt-sklearn.git","ed098eb9":"from hpsklearn import HyperoptEstimator\nfrom hpsklearn import any_classifier\nfrom hpsklearn import any_preprocessing\nfrom hyperopt import tpe","aefbdbc4":"# Convert data into array\nX_train_ar = np.array(X_train)\nX_val_ar = np.array(X_val)\ny_train_ar = np.array(y_train)\ny_val_ar = np.array(y_val)","c4f93a4c":"# Create the model\nhyperopt = HyperoptEstimator(classifier=any_classifier('cla'), preprocessing=any_preprocessing('pre'),\n                             algo=tpe.suggest, max_evals=50, trial_timeout=30)\n\n# Fit the training data\nhyperopt.fit(X_train_ar, y_train_ar)","0b814110":"# Predict the validation data\npred_hyperopt = hyperopt.predict(X_val)\n\n# Compute the accuracy\nprint('Accuracy: ' + str(accuracy_score(y_val, pred_hyperopt)))\nprint('')\n\n# Prediction results\nprint('Confusion Matrix')\nprint(pd.DataFrame(confusion_matrix(y_val, pred_hyperopt), index=[1,2,3,4], columns=[1,2,3,4]))\nprint('')\nprint('Classification Report')\nprint(classification_report(y_val, pred_hyperopt))","fc36ee6e":"# Show the models\nprint(hyperopt.best_model())","9cf0b66d":"!pip install autokeras","53ca73cd":"import autokeras","134699bc":"# Create the model\nkeras = autokeras.StructuredDataClassifier(max_trials=8)\n\n# Fit the training dataset\nkeras.fit(X_train, y_train, epochs=100)\n\n# Predict the validation data\npred_keras = keras.predict(X_val)\n\n# Compute the accuracy\nprint('Accuracy: ' + str(accuracy_score(y_val, pred_keras)))","4549989c":"# Convert predicted result into pandas series with numeric type\npred_keras_ = pd.DataFrame(pred_keras)\npred_keras_ = pred_keras_[0]\npred_keras_ = pd.to_numeric(pred_keras_)\n\n# Compute the accuracy\nprint('Accuracy: ' + str(accuracy_score(y_val, pred_keras_)))\nprint('')\n\n# Prediction results\nprint('Confusion Matrix')\nprint(pd.DataFrame(confusion_matrix(y_val, pred_keras_), index=[1,2,3,4], columns=[1,2,3,4]))\nprint('')\nprint('Classification Report')\nprint(classification_report(y_val, pred_keras_))","777192fb":"# Show the built models\nkeras_export = keras.export_model()\nkeras_export.summary() # Scroll to the end of the warnings to find the neural network summary","fc5a6042":"!pip install -q -U git+https:\/\/github.com\/mljar\/mljar-supervised.git@master","8267d598":"from supervised.automl import AutoML","0ae0f119":"# Create the model\nmljar = AutoML(mode=\"Compete\",  eval_metric=\"accuracy\", total_time_limit=300,\n               features_selection=True)\n\n# Fit the training data\nmljar.fit(X_train, y_train)","cd57ded2":"# Predict the validation data\npred_mljar = sklearn.predict(X_val)\n\n# Compute the accuracy\nprint('Accuracy: ' + str(accuracy_score(y_val, pred_mljar)))","002fd0e0":"# Prediction results\nprint('Confusion Matrix')\nprint(pd.DataFrame(confusion_matrix(y_val, pred_mljar), index=[1,2,3,4], columns=[1,2,3,4]))\nprint('')\nprint('Classification Report')\nprint(classification_report(y_val, pred_mljar))","bb92235b":"# Show the model results\nmljar.report()","c143d5b9":"!pip install -U pip\n!pip install -U setuptools wheel\n!pip install -U \"mxnet<2.0.0\"\n!pip install autogluon  ","2bbb0a96":"from autogluon.tabular import TabularDataset, TabularPredictor","9456d770":"# Prepare the data\nXy_train = X_train.reset_index(drop=True)\nXy_train['Target'] = y_train\n\nXy_val = X_val.reset_index(drop=True)\nXy_val['Target'] = y_val\n\nX_train_gluon = TabularDataset(Xy_train)\nX_val_gluon = TabularDataset(Xy_val)\n\n# Fit the training data\ngluon = TabularPredictor(label='Target').fit(X_train_gluon, time_limit=120)","a58ac8b8":"# Predict the training data\ngluon_pred = gluon.predict(X_val)\n\n# Compute the accuracy\nprint('Accuracy: ' + str(accuracy_score(y_val, gluon_pred)))","d8459879":"# Prediction results\nprint('Confusion Matrix')\nprint(pd.DataFrame(confusion_matrix(y_val, gluon_pred), index=[1,2,3,4], columns=[1,2,3,4]))\nprint('')\nprint('Classification Report')\nprint(classification_report(y_val, gluon_pred))","9d75461a":"# Show the models\nleaderboard = gluon.leaderboard(X_train_gluon)","c2d8eff1":"leaderboard","95edab12":"import h2o\nfrom h2o.automl import H2OAutoML","4ff02dbb":"h2o.init()","93167497":"# Convert H2O Frame\nXy_train_h2o = h2o.H2OFrame(Xy_train)\nX_val_h2o = h2o.H2OFrame(X_val)","8e5ce72f":"Xy_train_h2o['Target'] = Xy_train_h2o['Target'].asfactor()","ba4aec03":"# Create the model\nh2o_model = H2OAutoML(max_runtime_secs=120, seed=123)\n\n# Fit the model\nh2o_model.train(x=Xy_train_h2o.columns, y='Target', training_frame=Xy_train_h2o)","583ca150":"# Predict the training data\nh2o_pred = h2o_model.predict(X_val_h2o)\nh2o_pred","e8e704a0":"# Convert back H2ODataFrame to Pandas DataFrame\nh2o_pred_ = h2o.as_list(h2o_pred['predict'])\nh2o_pred_ = h2o_pred_['predict']\n\n# Compute the accuracy\nprint('Accuracy: ' + str(accuracy_score(y_val, h2o_pred_)))\nprint('')\n\n# Prediction results\nprint('Confusion Matrix')\nprint(pd.DataFrame(confusion_matrix(y_val, h2o_pred_), index=[1,2,3,4], columns=[1,2,3,4]))\nprint('')\nprint('Classification Report')\nprint(classification_report(y_val, h2o_pred_))","ff9af2e0":"# Show the model results\nleaderboard_h2o = h2o.automl.get_leaderboard(h2o_model, extra_columns = 'ALL')\nleaderboard_h2o","e01ce682":"# 3. Hyperopt","089db307":"#  4. AutoKeras","8179169a":"# 5. MLJAR","96415ed5":" # \ud83e\udd16 Automated Machine Learning-Classification\n  \nThis notebook provides Automated Machine Learning (AutoML) algorithms for a multi-class classification task. Data preparation is just simply performed as the pre-processing will be automatically done, followed by building Machine Learning algorithms and tuning the hyperparameters. The objective of this notebook is to serve as a cheat sheet.","0ce4d896":"The task is to predict which poverty class each household is in. There are 4 classes of poverty level: 1 = extreme poverty, 2 = moderate poverty, 3 = vulnerable households, and 4 = non vulnerable households.","1cabb01c":"To find the process of feature selection, please visit this notebook https:\/\/www.kaggle.com\/rendyk\/multi-classclassification-accuracy-povertylevel\n\nThat notebook demonstrates regression using conventional Machine Learning algorithms for learning the same dataset.","02f2508d":"The above cell takes too long time to finish. Thus, it is stopped earlier and it shows the temporary result.","6d5ebe70":"# 2. Tree-based Pipeline Optimization Tool (TPOT)","4464b989":"# 1. Auto-Sklearn","5a785d1c":"# 6. AutoGluon","6ca5ae60":"# 7. H2O","5cad54cc":"Below is the tpot_model.py with a little adjustment.","b4bcc804":"Hyperopr autoML stopped earlier. I have checked with online discussions and found that many people encountered the same problem. But, we can still retrieve the (intermediate) result although it is not the final result."}}