{"cell_type":{"9b3df11c":"code","f1b52cd8":"code","36503060":"code","3381b1c7":"code","f781b85c":"code","a74c8ff6":"code","58e47c22":"code","b256e6d7":"code","0a3f55e4":"code","f13688ae":"code","eb7f2935":"code","b63c31d1":"code","91cbb89e":"code","259ec3ba":"code","d8014283":"code","4c0a682c":"code","98e2ec55":"code","8223b82e":"code","5c40d6ef":"code","86b6c8d3":"code","a1fa2d33":"code","d706288d":"code","0eda5961":"code","e0a619df":"code","f0b89d1f":"code","d67fc730":"code","132425f1":"code","c6c74499":"code","6baf948b":"code","1eb0e515":"code","51eff0ac":"code","97099e70":"code","bec4dc4e":"code","e7fe7e06":"code","b2c1487d":"code","9cac36b7":"code","e89e718c":"code","5d2cd4d8":"code","271fd6ab":"code","c9df7efa":"code","f98b3390":"markdown","5dc9b0fc":"markdown","041f3b2a":"markdown","7ca71497":"markdown"},"source":{"9b3df11c":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline","f1b52cd8":"df=pd.read_csv('..\/input\/fbstock\/FB.csv')\ndf.head()","36503060":"df.tail()","3381b1c7":"df.shape","f781b85c":"df.isnull().sum()","a74c8ff6":"df_1=df.reset_index()['Close']\ndf_1","58e47c22":"df_1.shape","b256e6d7":"plt.plot(df_1)","0a3f55e4":"np.array(df_1)","f13688ae":"# LSTM are sensitive to the scaled data. So applying minmax scaler(normalization)\n\nfrom sklearn.preprocessing import MinMaxScaler","eb7f2935":"scaler=MinMaxScaler(feature_range=(0,1))\n# for reshaping purpose np.array() is used bcos df_1 dataframe doesnot have reshape argument.\ndf_1=scaler.fit_transform(np.array(df_1).reshape(-1,1))    \nprint(df_1.shape)\ndf_1","b63c31d1":"# Spliting the data---> for training, 65% of data and remaining for testing.\n\ntrain_size=int(len(df_1)*0.65)\nprint(train_size)\ntest_size=len(df_1)-train_size\nprint(test_size)","91cbb89e":"#Training data\n\nprint(len(df_1[:train_size,:]))     \nprint('\\n',df_1[:train_size,:])","259ec3ba":"#Testing data\n\nprint(len(df_1[train_size:,:]))     \n#print(df_1[train_size:,:])","d8014283":"#Assigning variable for train and test data\n\ntrain_df,test_df=df_1[:train_size,:],df_1[train_size:,:]","4c0a682c":"def create_dataset(dataset,timestep):\n    dataX, dataY=[],[]\n    for i in range(len(dataset)-timestep-1):   #(range(0, 944))\n        a=dataset[i:(i+timestep),0]            #If timestep=100, in 1st row 0 to 99 values is appended...2nd row 1 to 100 is appended...           \n        dataX.append(a)\n        dataY.append(dataset[(i+timestep),0])  #(output) In 1st row 100th value is appended...2nd row 101 value is appended...\n    return np.array(dataX),np.array(dataY)","98e2ec55":"X_train, y_train=create_dataset(train_df,100)\nX_test, y_test=create_dataset(test_df,100)","8223b82e":"print(X_train.shape), print(y_train.shape)","5c40d6ef":"print(X_test.shape), print(y_test.shape)","86b6c8d3":"#reshape input as (samples,timesteps,features) which is required for LSTM\n\nX_train=X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\nX_test=X_test.reshape(X_test.shape[0], X_test.shape[1], 1)","a1fa2d33":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, LSTM","d706288d":"model=Sequential()\nmodel.add(LSTM(50,return_sequences=True,input_shape=(100,1)))\nmodel.add(LSTM(50,return_sequences=True))\nmodel.add(LSTM(50))\nmodel.add(Dense(1))\nmodel.compile(loss='mean_squared_error',optimizer='adam')","0eda5961":"model.summary()","e0a619df":"model.fit(X_train,y_train,validation_data=(X_test,y_test),epochs=30,verbose=2)","f0b89d1f":"train_pred=model.predict(X_train)\ntest_pred=model.predict(X_test)","d67fc730":"# Transforming back to original form\n\ntrain_pred=scaler.inverse_transform(train_pred)\ntest_pred=scaler.inverse_transform(test_pred)","132425f1":"train_pred","c6c74499":"# RMSE performance metrics\n\nimport math\nfrom sklearn.metrics import mean_squared_error\n\nmath.sqrt(mean_squared_error(y_train,train_pred))","6baf948b":"math.sqrt(mean_squared_error(y_test,test_pred))","1eb0e515":"\n### Plotting \n# shift train predictions for plotting\nlook_back=100\ntrainPredictPlot = np.empty_like(df_1)\ntrainPredictPlot[:, :] = np.nan\ntrainPredictPlot[look_back:len(train_pred)+look_back, :] = train_pred\n# shift test predictions for plotting\ntestPredictPlot = np.empty_like(df_1)\ntestPredictPlot[:, :] = np.nan\ntestPredictPlot[len(train_pred)+(look_back*2)+1:len(df_1)-1, :] = test_pred\n# plot baseline and predictions\nplt.plot(scaler.inverse_transform(df_1))\nplt.plot(trainPredictPlot)\nplt.plot(testPredictPlot)\nplt.show()","51eff0ac":"len(test_df)","97099e70":"x_input=test_df[463:].reshape(1,-1)\nx_input.shape","bec4dc4e":"x_input","e7fe7e06":"temp_input=list(x_input)\ntemp_input=temp_input[0].tolist()","b2c1487d":"temp_input","9cac36b7":"\n# demonstrate prediction for next 10 days\nfrom numpy import array\n\nlst_output=[]\nn_steps=100\ni=0\nwhile(i<100):\n    \n    if(len(temp_input)>100):\n        #print(temp_input)\n        x_input=np.array(temp_input[1:])\n        print(\"{} day input {}\".format(i,x_input))\n        x_input=x_input.reshape(1,-1)\n        x_input = x_input.reshape((1, n_steps, 1))\n        #print(x_input)\n        yhat = model.predict(x_input, verbose=0)\n        print(\"{} day output {}\".format(i,yhat))\n        temp_input.extend(yhat[0].tolist())\n        temp_input=temp_input[1:]\n        #print(temp_input)\n        lst_output.extend(yhat.tolist())\n        i=i+1\n    else:\n        x_input = x_input.reshape((1, n_steps,1))\n        yhat = model.predict(x_input, verbose=0)\n        print(yhat[0])\n        temp_input.extend(yhat[0].tolist())\n        print(len(temp_input))\n        lst_output.extend(yhat.tolist())\n        i=i+1\n    \n\nprint(lst_output)","e89e718c":"day_new=np.arange(1,101)\nday_pred=np.arange(101,201)","5d2cd4d8":"len(df_1)","271fd6ab":"plt.plot(day_new,scaler.inverse_transform(df_1[1508:]))\nplt.plot(day_pred,scaler.inverse_transform(lst_output))","c9df7efa":"df3=df_1.tolist()\ndf3.extend(lst_output)\nplt.plot(df3[1200:])","f98b3390":"### Creating a stacked LSTM model","5dc9b0fc":"## Preprocess the data\n\n#### Preprocess the data------>mostly we use cross validation, random seed methods(for linear problem it is best). But in time series there will be always a relation between the previous data. \n(eg --> Take a 4 days of stock prices. In that day2 stock price have related to day1. It have some connection. And day3 stock price have sone connection with day2 and day1...like this it goes on. so while data split, for training we have to take data with respective to continious date and for testing take remaining data)","041f3b2a":"eg: If the stock cost is [45,78,98,65,32,15,100,15,57,45]\n\n    Every cost value has some dependency of previous value. Here 98 has some dependency of 45 & 78.... \n    \n    1. our train data is [45,78,98,65,32,15]   &   test data is [100,15,57,45]\n    \n    2. Now we are going to split X_train, y_train, X_test, y_test. For this we are spliting our above train data to X_train,y_train and test data to X_tesy, y_test.\n    \n    3. Timestep method is used here------->which means, in train data if timestep=2 then, it will take 1st 2 values as X_train and y_train will be 3rd value(assumes as predicted value). And in 2nd row it will take 2nd and 3rd values of train data as input and 4th data as output...\n               \n               X_train      y_train\n                [45,78]       [98]\n                [78,98]       [65]\n                [98,65]       [32]   like this it goes on.... same for test data also\n                \n  FOR CREATING A DATASET OF ABOVE METHOD FUNCTION IS USED BELOW","7ca71497":"## Predict stock for next 100 days"}}