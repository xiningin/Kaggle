{"cell_type":{"ac3752fe":"code","c30bad0b":"code","3a56505d":"markdown","fce26ea7":"markdown","2f98e23f":"markdown","2cf46bb3":"markdown","304ed8e4":"markdown","0a9127a7":"markdown","3cb90250":"markdown"},"source":{"ac3752fe":"from IPython.display import Image","c30bad0b":"display(Image(\"\/kaggle\/input\/sample-images\/shallow-dnn.png\"))","3a56505d":"## Architecture of a Shallow DNN\n**6 Inputs, 9 Hidden Nodes, 4 Outputs**","fce26ea7":"## Forward Propagation\nFrom the inputs and weights, biases of the hidden layer, \n$$z_1 = W_1x + b_1 \\tag{1}$$\n$$h = ReLU(z_1) \\tag{2}$$\nFrom hidden layer ($h$) and weights, biases to the output layer\n$$z_2 = W_2h + b_2 \\tag{3}$$\n$$\\hat y = softmax(z_2) \\tag{4}$$\n\n### Dimension Analysis\nHow to select the weights, let us do dimensionality reduction\nLet us examine the dimensions of all elements in the equation 1\n$$z_1 = W_1h + b_1$$\n$$dimensions$$\n$$N \\times 1 = [?, ?] \\times [I, 1] + [?, ?]$$\n$$naturally$$\n$$N \\times 1 = [N, I] \\times [I, 1] + [N, 1]$$\n$$hence$$\n$$9 \\times 1 = [9, 6] \\times [6, 1] + [9, 1]$$\nHence $N \\times I $ is the dimension of the weight matrix $W_1$\n\n\nLet us examine the dimensions of all elements in the equation 2\n$$z_2 = W_2h + b_2$$\n$$dimensions$$\n$$O \\times 1 = [?, ?] \\times [N, 1] + [?, ?]$$\n$$then$$\n$$O \\times 1 = [O, N] \\times [N, 1] + [O, 1]$$\n$$hence$$\n$$4 \\times 1 = [4, 9] \\times [9, 1] + [4, 1]$$\nHence $O \\times N $ is the dimension of the weight matrix $W_2$","2f98e23f":"# Forward and Backpropagation Algorithm\n\nAn associated source code will be published shortly.\n\nIn this post, we shall explore a shallow Dense Neural Network(DNN) with \n- an Input Layer\n- a Single Hidden Layer and\n- an Output Layer\n\nThe typical architecture of a feed forward network has the above units. \nWhere \n- $x$ is a vector that represents the input values, \n- $\\hat y$ is a vector that represents the predictions and \n- $h$ is vector that represents the hidden layer, $h$ is a hyperparameter selected based on the context of the problem\n\nFurther to the above 3 parameters, connection weights $[W_1, W_2]$ and connection biases $[b_1, b_2]$ are the other matrices and vectors used in a DNN","2cf46bb3":"\nFollowing are the processes we shall cover in the quest of understanding a simple Neural Network architecture,  \n- Forward Propagation\n- Cross Entropy Loss (covered in a separate post)\n- Activation Functions\n    - Softmax\n    - ReLU\n- Backpropagation\n- Gradient Descent\n- Extraction of features of the hidden layer\n\nFrom the above Diagram...\n- There are 6 Inputs of dimension $(I \\times 1)$ ie (6 X 1)\n- There are 9 nodes in the Hidden Layer of dimension $(N \\times 1)$ ie (9 X 1)\n- There are 4 nodes in the Output Layer of dimension $(O \\times 1)$ ie (4 X 1)","304ed8e4":"## Cross Entropy Loss\nOur goal is to minimize the loss $J$\n$$J = - \\sum\\limits_{k=1}^V y_k \\log\\hat y_k \\tag{5}$$ \n[Cost(Loss) Function for Binary Classification - A Deep Dive](https:\/\/www.linkedin.com\/pulse\/costloss-function-binary-classification-gowri-shankar\/)","0a9127a7":"## Backpropagation - Pull your sleeves and do some partial derivatives\nWe calculated the $\\hat y$ during forward propagation and now our goal is to optimize the weights by minimizing the loss. This is done by calculating the partial derivatives wrt $[W_2, b_2]$ and then $[W_1, b_1]$\n$$\\frac{\\partial J}{\\partial W_1} = ReLU\\left( W^T_2(\\hat y - y)\\right)x^T \\tag{6}$$\n$$\\frac{\\partial J}{\\partial W_2} = (\\hat y - y)h^T \\tag{7}$$\n$$\\frac{\\partial J}{\\partial b_1} = ReLU\\left(W^T_2(\\hat y - y)\\right) \\tag{8}$$\n$$\\frac{\\partial J}{\\partial b_2} = \\hat y - y \\tag{9}$$","3cb90250":"## Gradient Descent\nGradient descent is the process during which the weights and biases are updated by subtracting $\\alpha$ times the learning rate of calculated gradients from the original matrices and biases\n\n$$W_1 := W_1 - \\alpha\\frac{\\partial J}{\\partial {W_{1}}} \\tag{10}$$\n$$W_2 := W_2 - \\alpha\\frac{\\partial J}{\\partial W_2} \\tag{11}$$\n$$b_1 := b_1 - \\alpha\\frac{\\partial J}{\\partial b_1} \\tag{12}$$\n$$b_2 := b_2 - \\alpha\\frac{\\partial J}{\\partial b_2} \\tag{13}$$\n\n[Demystifying Gradient Descent - A Deep Dive](https:\/\/www.linkedin.com\/pulse\/demystifying-gradient-descent-classification-problem-gowri-shankar\/)"}}