{"cell_type":{"ab8dffaa":"code","0dae6ab0":"code","a05cdb4d":"code","03c29458":"code","3b85a5de":"markdown","5cd762f5":"markdown"},"source":{"ab8dffaa":"!pip install transformers","0dae6ab0":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n\ndata_path = \"\/kaggle\/input\/CORD-19-research-challenge\/2020-03-13\/\"\n\nsources = pd.read_csv(data_path + \"all_sources_metadata_2020-03-13.csv\")\nsources = sources[[\"title\", \"abstract\", \"Microsoft Academic Paper ID\"]].dropna(subset=['title', 'abstract'])\n\n\n\n\n\n\n\n\n","a05cdb4d":"from transformers import AutoModelForQuestionAnswering, AutoTokenizer\nimport torch\n\ntokenizer = AutoTokenizer.from_pretrained(\"bert-large-cased-whole-word-masking-finetuned-squad\")\nmodel = AutoModelForQuestionAnswering.from_pretrained(\"bert-large-cased-whole-word-masking-finetuned-squad\")\n\n","03c29458":"# todo: Probably a better way \nsources = sources[sources['abstract'].str.contains(\"ADE\")]\n\n# Just a test\n# Put all the titles together and try answer extraction\n\n\ndef chunkTitles(titles, nb):\n    total = len(titles)\n    delta = int(total \/ nb)\n    \n    chunks = []\n    for i in range(0, total, nb):\n        chunks.append(titles[i: i + nb])\n        \n    return chunks\n\n\nabstracts = sources['abstract'].astype(str)\n\nquestion = \"What are the methods for evaluating complication of Antibody-Dependent Enhancement?\"\n\n\nfor abstracts_chunked in chunkTitles(abstracts, 1):\n    abstracts_together = \" \".join((\"\".join(abstracts_chunked)).split(\" \")[:200])\n    \n    encoded_question = tokenizer.encode_plus(question, abstracts_together, add_special_tokens=True, return_tensors=\"pt\")\n    input_ids = encoded_question[\"input_ids\"]\n    answer_start_scores, answer_end_scores = model(input_ids)\n\n    answer_start = torch.argmax(answer_start_scores)\n    answer_end = torch.argmax(answer_end_scores)\n\n    answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(input_ids[0][answer_start.data:answer_end.data].tolist()))\n    \n    \n    if len(answer) > 0:\n        print(answer)\n        print()\n        \n        \n#todo: Maybe take all the responses and make a summary?\n","3b85a5de":"Initialize the model and tokenizer","5cd762f5":"# Question: \n# What do we know about vaccines and therapeutics?"}}