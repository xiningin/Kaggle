{"cell_type":{"1b94d5b9":"code","db2ac889":"code","11a5088c":"code","68d327d7":"code","db933130":"code","eb675ffb":"code","ee98e36d":"code","94f425d9":"code","38e4674f":"code","ab387d35":"code","19a51cd7":"code","982ddaba":"code","09868e07":"code","a4801f5d":"code","12753dba":"markdown","41c2767e":"markdown","ed887fe6":"markdown","c7db72df":"markdown","c5d774b2":"markdown","7d346de4":"markdown","a4ca4f1f":"markdown","9a645aaf":"markdown","157039f9":"markdown","a4fc31f8":"markdown","0c88b554":"markdown","2e9e10b0":"markdown","5b10f20a":"markdown","ff35f31b":"markdown","414b9f2a":"markdown","e5a148e7":"markdown","ae72151a":"markdown","98fd7a8c":"markdown","26e1cfbe":"markdown"},"source":{"1b94d5b9":"import numpy as np \nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport matplotlib.pyplot as plt\nimport matplotlib.image as implt\nimport seaborn as sns\n\nfrom PIL import Image\nimport cv2\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport os\ncwd = os.getcwd()\nos.chdir(cwd)\nprint(os.listdir(\"..\/input\"))\n","db2ac889":"# Load at the data\ndata_test ='..\/input\/cat-and-dog\/test_set\/test_set\/'\ndata_train ='..\/input\/cat-and-dog\/training_set\/training_set'\n\ntrain_dogs = sorted(os.listdir(data_train +'\/dogs'))\ntrain_cats =  sorted(os.listdir(data_train +'\/cats'))\n\ntest_dogs = sorted(os.listdir(data_test +'\/dogs'))\ntest_cats =  sorted(os.listdir(data_test +'\/cats'))","11a5088c":"img_1 = implt.imread(data_train +'\/dogs\/dog.3851.jpg')\nimg_2 = implt.imread(data_train +'\/cats\/cat.3041.jpg')\n\nplt.subplot(1, 2, 1)\nplt.title('Dog')\nplt.imshow(img_1)       \nplt.subplot(1, 2, 2)\nplt.title('Cat')\nplt.imshow(img_2) ","68d327d7":"img_size = 64\ndogs_images = []\ncats_images = [] \nlabel = []\n\nfor i in train_dogs:\n    if '.jpg' in i:\n        if os.path.isfile(data_train +'\/dogs\/'+ i):\n            images = Image.open(data_train +'\/dogs\/'+ i).convert('L') #converting grayscale         \n            images = images.resize((img_size,img_size), Image.ANTIALIAS) #resizing to 64,64\n            images = np.asarray(images)\/255.0 #normalizing images\n            dogs_images.append(images)  \n            label.append(1) #label 1 for dogs\n                    \nfor i in train_cats:\n    if '.jpg' in i:\n        if os.path.isfile(data_train+'\/cats\/'+ i):\n            images = Image.open(data_train+'\/cats\/'+ i).convert('L')\n            images = images.resize((img_size,img_size), Image.ANTIALIAS)\n            images = np.asarray(images)\/255.0 #normalizing images\n            cats_images.append(images)  \n            label.append(0) #label 0 for cats          \n           \nx_train = np.concatenate((dogs_images,cats_images),axis=0) # training dataset\nx_train_label = np.asarray(label)# label array containing 0 and 1\nx_train_label = x_train_label.reshape(x_train_label.shape[0],1)\n\nprint(\"dogs_images:\",np.shape(dogs_images) , \"cats_images:\",np.shape(cats_images))\nprint(\"train_dataset:\",np.shape(x_train), \"train_values:\",np.shape(x_train_label))","db933130":"img_size = 64\ndogs_images = []\ncats_images = [] \nlabel = []\n\nfor i in test_dogs:\n    if '.jpg' in i:\n        if os.path.isfile(data_test +'\/dogs\/'+ i):\n            images = Image.open(data_test +'\/dogs\/'+ i).convert('L') #converting grayscale            \n            images = images.resize((img_size,img_size), Image.ANTIALIAS) #resizing to 64,64\n            images = np.asarray(images)\/255.0 #normalizing images\n            dogs_images.append(images)  \n            label.append(1) #label 1 for dogs\n \nfor i in test_cats:\n    if '.jpg' in i:\n        if os.path.isfile(data_test +'\/cats\/'+ i):\n            images = Image.open(data_test +'\/cats\/'+ i).convert('L')\n            images = images.resize((img_size,img_size), Image.ANTIALIAS)\n            images = np.asarray(images)\/255.0 #normalizing images\n            cats_images.append(images)  \n            label.append(0) #label 0 for cats       \n            \nx_test = np.concatenate((dogs_images,cats_images),axis=0) # test dataset\nx_test_label = np.asarray(label) # corresponding labels\nx_test_label = x_test_label.reshape(x_test_label.shape[0],1)\n\nprint(\"dogs_images:\",np.shape(dogs_images), \"cats_images:\",np.shape(cats_images))\nprint(\"test_dataset:\",np.shape(x_test), \"test_values:\",np.shape(x_test_label))","eb675ffb":"x = np.concatenate((x_train,x_test),axis=0) #train_data\ny = np.concatenate((x_train_label,x_test_label),axis=0) #test data\nx = x.reshape(x.shape[0],x.shape[1]*x.shape[2]) #flatten 3D image array to 2D\nprint(\"images:\",x.shape, \"labels:\", y.shape)","ee98e36d":"from sklearn.model_selection import train_test_split\nX_train, X_test, Y_train, Y_test = train_test_split(x, y, test_size=0.2, random_state=0)\n\nx_train = X_train.T\nx_test = X_test.T\ny_train = Y_train.T\ny_test = Y_test.T\nprint(\"x train: \",x_train.shape)\nprint(\"x test: \",x_test.shape)\nprint(\"y train: \",y_train.shape)\nprint(\"y test: \",y_test.shape)","94f425d9":"def initialize_weights_and_bias(dimension):\n    w=np.full((dimension,1),0.01)   \n    b=0.0\n    return w,b\ndef sigmoid(z):\n    y_head=1\/(1+np.exp(-z))\n    return y_head\ndef forward_backward_propagation(w,b,x_train,y_train):\n    #forward propagation\n    z=np.dot(w.T,x_train)+b\n    y_head=sigmoid(z)\n    loss= -y_train*np.log(y_head)- (1-y_train)*np.log(1-y_head)\n    cost=(np.sum(loss))\/x_train.shape[1]   #x_train.shape[1] is for scaling\n    \n    #backward propagation\n    #x_train.shape[1] is for scaling\n    derivative_weight=(np.dot(x_train,((y_head-y_train).T)))\/x_train.shape[1] \n    derivative_bias=np.sum(y_head-y_train)\/x_train.shape[1]\n    gradients={'derivative_weight':derivative_weight,'derivative_bias':derivative_bias}\n    \n    return cost,gradients\ndef update(w,b,x_train,y_train,learnig_rate,number_of_iteration):\n    cost_list=[]\n    cost_list2=[]\n    index=[]\n    \n    #updating(learning) parametres is number_of_iteration times\n    for i in range(number_of_iteration):\n        #make fordward and backward propagation and find cost and gradients\n        cost,gradients=forward_backward_propagation(w,b,x_train,y_train)\n        cost_list.append(cost)\n        #update\n        w=w-learnig_rate*gradients['derivative_weight']\n        b=b-learnig_rate*gradients['derivative_bias']\n        \n        if i%50 == 0:\n            cost_list2.append(cost)\n            index.append(i)\n            print('cost after iteration %i: %f:' %(i,cost))\n    \n    #we update(learn) parametres weights and bias\n    parameters={'weight':w,'bias':b}\n    plt.figure(figsize=(4,4))\n    plt.plot(index,cost_list2)\n    plt.xticks(index,rotation='vertical')\n    plt.xlabel('Number of iteration')\n    plt.ylabel('Cost')\n    plt.show()\n    \n    return parameters,gradients,cost_list\ndef predict(w,b,x_test):\n    z=sigmoid(np.dot(w.T,x_test)+b)\n    Y_prediction=np.zeros((1,x_test.shape[1]))\n    #We're making an estimate based on our condition.\n    for i in range(z.shape[1]):\n        if z[0,i]<=0.5:\n            Y_prediction[0,i]=0\n        else:\n            Y_prediction[0,i]=1\n    return Y_prediction","38e4674f":"def logistic_regression(x_train, y_train, x_test, y_test, learning_rate ,  num_iterations):\n    # initialize\n    dimension =  x_train.shape[0]  # 4096\n    w,b = initialize_weights_and_bias(dimension)\n    parameters, gradients, cost_list = update(w, b, x_train, y_train, learning_rate,num_iterations)\n    \n    y_prediction_test = predict(parameters[\"weight\"],parameters[\"bias\"],x_test)\n    y_prediction_train = predict(parameters[\"weight\"],parameters[\"bias\"],x_train)\n\n    train_acc_lr = round((100 - np.mean(np.abs(y_prediction_train - y_train)) * 100),2)\n    test_acc_lr = round((100 - np.mean(np.abs(y_prediction_test - y_test)) * 100),2)\n\n    from sklearn.metrics import confusion_matrix\n    cm_test = confusion_matrix(y_test.T, y_prediction_test.T)\n    cm_train = confusion_matrix(y_train.T, y_prediction_train.T)\n    \n    fig = plt.figure(figsize=(15,15))\n    ax1 = fig.add_subplot(3, 3, 1) # row, column, position\n    ax1.set_title('Confusion Matrix for Train Data')\n\n    ax2 = fig.add_subplot(3, 3, 2) # row, column, position\n    ax2.set_title('Confusion Matrix for Test Data')\n    \n    sns.heatmap(data=cm_train,annot=True, linewidth=0.7, linecolor='cyan', fmt='.0f', ax=ax1, cmap='BuPu')\n    sns.heatmap(data=cm_test,annot=True, linewidth=0.7, linecolor='cyan', fmt='.0f', ax=ax2, cmap='BuPu')  \n    \n    \n    # Print train\/test Errors\n    print(\"train accuracy: %\", train_acc_lr)\n    print(\"test accuracy: %\", test_acc_lr)\n    return train_acc_lr, test_acc_lr\n    \n\ntrain_acc_lr, test_acc_lr = logistic_regression(x_train, y_train, x_test, y_test,learning_rate = 0.003, num_iterations = 500)","ab387d35":"#Now we use sklearn libray \nfrom sklearn.linear_model import LogisticRegression\nlr=LogisticRegression(random_state = 0)\nlr.fit(x_train.T,y_train.T)\n\ny_pred_test=lr.predict(x_test.T)\ny_pred_train=lr.predict(x_train.T)\n\nfrom sklearn.metrics import confusion_matrix\ncm_test = confusion_matrix(y_test.T, y_pred_test)\ncm_train = confusion_matrix(y_train.T, y_pred_train)\n\nfig = plt.figure(figsize=(15,15))\nax1 = fig.add_subplot(3, 3, 1) # row, column, position\nax1.set_title('Confusion Matrix for Train Data')\n\nax2 = fig.add_subplot(3, 3, 2) # row, column, position\nax2.set_title('Confusion Matrix for Test Data')\n\nsns.heatmap(data=cm_train,annot=True, linewidth=0.7, linecolor='cyan', fmt='.0f', ax=ax1, cmap='BuPu')\nsns.heatmap(data=cm_test,annot=True, linewidth=0.7, linecolor='cyan', fmt='.0f', ax=ax2, cmap='BuPu')  \n\nprint('train accuracy: {}'.format(lr.score(x_train.T,y_train.T)))\nprint('test accuracy: {}'.format(lr.score(x_test.T,y_test.T)))","19a51cd7":"# intialize parameters and layer sizes\ndef initialize_parameters_and_layer_sizes_NN(x_train, y_train):\n    parameters = {\"weight1\": np.random.randn(3,x_train.shape[0]) * 0.1,\n                  \"bias1\": np.zeros((3,1)),\n                  \"weight2\": np.random.randn(y_train.shape[0],3) * 0.1,\n                  \"bias2\": np.zeros((y_train.shape[0],1))}\n    return parameters\n\ndef forward_propagation_NN(x_train, parameters):\n\n    Z1 = np.dot(parameters[\"weight1\"],x_train) +parameters[\"bias1\"]\n    A1 = np.tanh(Z1)\n    Z2 = np.dot(parameters[\"weight2\"],A1) + parameters[\"bias2\"]\n    A2 = sigmoid(Z2)\n\n    cache = {\"Z1\": Z1,\n             \"A1\": A1,\n             \"Z2\": Z2,\n             \"A2\": A2}\n    \n    return A2, cache\n\n# Compute cost\ndef compute_cost_NN(A2, Y, parameters):\n    logprobs = np.multiply(np.log(A2),Y)\n    cost = -np.sum(logprobs)\/Y.shape[1]\n    return cost\n\n# Backward Propagation\ndef backward_propagation_NN(parameters, cache, X, Y):\n\n    dZ2 = cache[\"A2\"]-Y\n    dW2 = np.dot(dZ2,cache[\"A1\"].T)\/X.shape[1]\n    db2 = np.sum(dZ2,axis =1,keepdims=True)\/X.shape[1]\n    dZ1 = np.dot(parameters[\"weight2\"].T,dZ2)*(1 - np.power(cache[\"A1\"], 2))\n    dW1 = np.dot(dZ1,X.T)\/X.shape[1]\n    db1 = np.sum(dZ1,axis =1,keepdims=True)\/X.shape[1]\n    grads = {\"dweight1\": dW1,\n             \"dbias1\": db1,\n             \"dweight2\": dW2,\n             \"dbias2\": db2}\n    return grads\n\n# update parameters\ndef update_parameters_NN(parameters, grads, learning_rate = 0.01):\n    parameters = {\"weight1\": parameters[\"weight1\"]-learning_rate*grads[\"dweight1\"],\n                  \"bias1\": parameters[\"bias1\"]-learning_rate*grads[\"dbias1\"],\n                  \"weight2\": parameters[\"weight2\"]-learning_rate*grads[\"dweight2\"],\n                  \"bias2\": parameters[\"bias2\"]-learning_rate*grads[\"dbias2\"]}\n    \n    return parameters\n\n# prediction\ndef predict_NN(parameters,x_test):\n    # x_test is a input for forward propagation\n    A2, cache = forward_propagation_NN(x_test,parameters)\n    Y_prediction = np.zeros((1,x_test.shape[1]))\n    # if z is bigger than 0.5, our prediction is sign one (y_head=1),\n    # if z is smaller than 0.5, our prediction is sign zero (y_head=0),\n    for i in range(A2.shape[1]):\n        if A2[0,i]<= 0.5:\n            Y_prediction[0,i] = 0\n        else:\n            Y_prediction[0,i] = 1\n\n    return Y_prediction","982ddaba":"# 2 - Layer neural network\ndef two_layer_neural_network(x_train, y_train,x_test,y_test, num_iterations):\n    cost_list = []\n    index_list = []\n    #initialize parameters and layer sizes\n    parameters = initialize_parameters_and_layer_sizes_NN(x_train, y_train)\n\n    for i in range(0, num_iterations):\n         # forward propagation\n        A2, cache = forward_propagation_NN(x_train,parameters)\n        # compute cost\n        cost = compute_cost_NN(A2, y_train, parameters)\n         # backward propagation\n        grads = backward_propagation_NN(parameters, cache, x_train, y_train)\n         # update parameters\n        parameters = update_parameters_NN(parameters, grads)\n        \n        if i % 100 == 0:\n            cost_list.append(cost)\n            index_list.append(i)\n            print (\"Cost after iteration %i: %f\" %(i, cost))\n    plt.plot(index_list,cost_list)\n    plt.xticks(index_list,rotation='vertical')\n    plt.xlabel(\"Number of Iterarion\")\n    plt.ylabel(\"Cost\")\n    plt.show()\n    \n    # predict\n    y_prediction_test = predict_NN(parameters,x_test)\n    y_prediction_train = predict_NN(parameters,x_train)\n    \n    from sklearn.metrics import confusion_matrix\n    cm_test = confusion_matrix(y_test.T, y_prediction_test.T)\n    cm_train = confusion_matrix(y_train.T, y_prediction_train.T)\n    \n    fig = plt.figure(figsize=(15,15))\n    ax1 = fig.add_subplot(3, 3, 1) # row, column, position\n    ax1.set_title('Confusion Matrix for Train Data')\n\n    ax2 = fig.add_subplot(3, 3, 2) # row, column, position\n    ax2.set_title('Confusion Matrix for Test Data')\n    \n    sns.heatmap(data=cm_train,annot=True, linewidth=0.7, linecolor='cyan', fmt='.0f', ax=ax1, cmap='BuPu')\n    sns.heatmap(data=cm_test,annot=True, linewidth=0.7, linecolor='cyan', fmt='.0f', ax=ax2, cmap='BuPu')  \n\n    # Print train\/test Errors\n    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(y_prediction_train - y_train)) * 100))\n    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(y_prediction_test - y_test)) * 100))\n    return parameters\n\nparameters = two_layer_neural_network(x_train, y_train,x_test,y_test, num_iterations=500)","09868e07":"# reshaping\nx_train, x_test, y_train, y_test = x_train.T, x_test.T, y_train.T, y_test.T","a4801f5d":"# Evaluating the ANN\nfrom keras.wrappers.scikit_learn import KerasClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom keras.models import Sequential # initialize neural network library\nfrom keras.layers import Dense # build our layers library\ndef build_classifier():\n    classifier = Sequential() # initialize neural network\n    classifier.add(Dense(units =32 , kernel_initializer = 'uniform', activation = 'relu', input_dim = x_train.shape[1]))\n    classifier.add(Dense(units = 32, kernel_initializer = 'uniform', activation = 'relu')) \n    classifier.add(Dense(units = 64, kernel_initializer = 'uniform', activation = 'relu')) \n    classifier.add(Dense(units = 32, kernel_initializer = 'uniform', activation = 'relu')) \n    classifier.add(Dense(units = 64, kernel_initializer = 'uniform', activation = 'relu'))\n    classifier.add(Dense(units = 1, kernel_initializer = 'uniform', activation = 'sigmoid')) \n    classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n    return classifier\nclassifier = KerasClassifier(build_fn = build_classifier, epochs = 50)\naccuracies = cross_val_score(estimator = classifier, X = x_train, y = y_train, cv = 3)\nmean = accuracies.mean()\nvariance = accuracies.std()\nprint(\"Accuracy mean: \"+ str(mean))\nprint(\"Accuracy variance: \"+ str(variance))","12753dba":"![](https:\/\/i.stack.imgur.com\/9jzpy.jpg)","41c2767e":"**<h1>INTRODUCTION<\/h1>**\nWith this kernel,cats and dogs will be classified using dataset containing images of cats and dogs.\n\n\n**<h4>Content<\/h4>**\n* [Exploring the Dataset](#1)\n* [Processing Dataset](#2)\n* [Logistic Regression](#3)\n* [Logistic Regression with sklearn](#4)\n* [Artificial Neural Network (ANN)](#5)\n","ed887fe6":"<a id=1><\/a>\n**<h1>Exploring the Dataset<\/h1>**\nLet's importing dataset, train and test folders.","c7db72df":"* We're concatenating train, test arrays and labels, and flattening 'x'","c5d774b2":"* Scaling down and grayscale the test set images ","7d346de4":"**<h4>Reference<\/h4>**\n * https:\/\/www.kaggle.com\/kanncaa1","a4ca4f1f":"* You can adjust learning_rate and num_iteration to check how the result is affected.","9a645aaf":"**<h3>L-Layer Neural Network<\/h3>**","157039f9":"* Scaling down and grayscale the train set images","a4fc31f8":"**<h3>2-Layer Neural Network<\/h3>**\n- Size of layers and initializing parameters weights and bias\n- Forward propagation\n- Loss function and Cost function\n- Backward propagation\n- Update Parameters\n- Prediction with learnt parameters weight and bias\n- Create Model","0c88b554":"* We split our data as training and test data. 20% of our data will be test data.","2e9e10b0":"![](http:\/\/media5.datahacker.rs\/2018\/06\/img_29_new.png)","5b10f20a":"\nLet's check the sample from the data","ff35f31b":"* Let's run our logistic regression function","414b9f2a":"<a id=4><\/a>\n**<h1>Logistic Regression with sklearn<\/h1>**","e5a148e7":"Logistic Regression Summary\n* **initialize_weights_and_bias :** has initial values of weights and bias\n* **sigmoid :** activation function that limit the output to a range between 0 and 1\n* **forward_backward propogation :** is used to calculate cost function(error) and gradient descent(to learn proper weights and bias values that minimize the error)\n* **update :** updating learning parameters 'w' and 'b' to find best values of them for better training\n* **predict :** uses x_test as input for forward propogation","ae72151a":"<a id=5><\/a>\n**<h1>Artificial Neural Network (ANN)<\/h1>**","98fd7a8c":"<a id=2><\/a>\n**<h1> Logistic Regression without sklearn<\/h1>**","26e1cfbe":"<a id=2><\/a>\n**<h1>Processing Dataset<\/h1>**\nThe dataset contains RGB color images. We will convert these images to grayscale. In this way, we will be able to process with one dimension."}}