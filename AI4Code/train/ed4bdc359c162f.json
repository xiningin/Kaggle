{"cell_type":{"3c9e551b":"code","2ce82716":"code","35808ff3":"code","26afaf8a":"code","d9d00b28":"code","aec6eb2b":"code","7287d82b":"code","90181c90":"code","25176e0d":"code","1f69641b":"code","093ff06d":"code","5e202eca":"code","458ec450":"code","e4454494":"code","d461dd48":"code","f1dced54":"code","87b1e89c":"code","f290bb6f":"code","af691c7e":"code","514e1ec9":"code","93a60998":"code","56b513d9":"code","7fda61a2":"code","d38e4564":"code","d8a29a8b":"code","3802b0a5":"code","c88266f4":"code","8baf0c0f":"code","abb9188d":"code","ea870ea4":"code","b94a25a1":"code","09955ba8":"code","e14ce515":"code","18ede4ca":"markdown","0c84d484":"markdown","8f054391":"markdown","5e318969":"markdown","fee2ac84":"markdown","1a923848":"markdown","aba9c490":"markdown","e55c3cbd":"markdown","8255f0ba":"markdown","00e02e42":"markdown","7c46f661":"markdown","8860553c":"markdown"},"source":{"3c9e551b":"# importing necessary libraries \nimport pandas as pd\nimport tensorflow as tf\nimport os\nimport re\nimport numpy as np\nfrom string import punctuation\nfrom zipfile import ZipFile\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt","2ce82716":"# importing neural network libraries\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout, Embedding, GRU, LSTM, RNN, SpatialDropout1D","35808ff3":"train = pd.read_csv('..\/input\/fake-news\/train.csv')\ntest = pd.read_csv('..\/input\/fake-news\/test.csv')\ntrain_data = train.copy()\ntest_data = test.copy()","26afaf8a":"train_data = train_data.set_index('id', drop = True)","d9d00b28":"print(train_data.shape)\ntrain_data.head()","aec6eb2b":"print(test_data.shape)\ntest_data.head()","7287d82b":"# checking for missing values\ntrain_data.isnull().sum()","90181c90":"# dropping missing values from text columns alone. \ntrain_data[['title', 'author']] = train_data[['title', 'author']].fillna(value = 'Missing')\ntrain_data = train_data.dropna()\ntrain_data.isnull().sum()","25176e0d":"length = []\n[length.append(len(str(text))) for text in train_data['text']]\ntrain_data['length'] = length\ntrain_data.head()","1f69641b":"min(train_data['length']), max(train_data['length']), round(sum(train_data['length'])\/len(train_data['length']))","093ff06d":"len(train_data[train_data['length'] < 50])","5e202eca":"train_data['text'][train_data['length'] < 50]","458ec450":"# dropping the outliers\ntrain_data = train_data.drop(train_data['text'][train_data['length'] < 50].index, axis = 0)","e4454494":"min(train_data['length']), max(train_data['length']), round(sum(train_data['length'])\/len(train_data['length']))","d461dd48":"max_features = 4500","f1dced54":"# Tokenizing the text - converting the words, letters into counts or numbers. \n# We dont need to explicitly remove the punctuations. we have an inbuilt option in Tokenizer for this purpose\ntokenizer = Tokenizer(num_words = max_features, filters='!\"#$%&()*+,-.\/:;<=>?@[\\\\]^_`{|}~\\t\\n', lower = True, split = ' ')\ntokenizer.fit_on_texts(texts = train_data['text'])\nX = tokenizer.texts_to_sequences(texts = train_data['text'])","87b1e89c":"# now applying padding to make them even shaped.\nX = pad_sequences(sequences = X, maxlen = max_features, padding = 'pre')","f290bb6f":"print(X.shape)\ny = train_data['label'].values\nprint(y.shape)","af691c7e":"# splitting the data training data for training and validation.\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 101)","514e1ec9":"# LSTM Neural Network\nlstm_model = Sequential(name = 'lstm_nn_model')\nlstm_model.add(layer = Embedding(input_dim = max_features, output_dim = 120, name = '1st_layer'))\nlstm_model.add(layer = LSTM(units = 120, dropout = 0.2, recurrent_dropout = 0.2, name = '2nd_layer'))\nlstm_model.add(layer = Dropout(rate = 0.5, name = '3rd_layer'))\nlstm_model.add(layer = Dense(units = 120,  activation = 'relu', name = '4th_layer'))\nlstm_model.add(layer = Dropout(rate = 0.5, name = '5th_layer'))\nlstm_model.add(layer = Dense(units = len(set(y)),  activation = 'sigmoid', name = 'output_layer'))\n# compiling the model\nlstm_model.compile(optimizer = 'adam', loss = 'sparse_categorical_crossentropy', metrics = ['accuracy'])","93a60998":"lstm_model_fit = lstm_model.fit(X_train, y_train, epochs = 1)","56b513d9":"# GRU neural Network\ngru_model = Sequential(name = 'gru_nn_model')\ngru_model.add(layer = Embedding(input_dim = max_features, output_dim = 120, name = '1st_layer'))\ngru_model.add(layer = GRU(units = 120, dropout = 0.2, \n                          recurrent_dropout = 0.2, recurrent_activation = 'relu', \n                          activation = 'relu', name = '2nd_layer'))\ngru_model.add(layer = Dropout(rate = 0.4, name = '3rd_layer'))\ngru_model.add(layer = Dense(units = 120, activation = 'relu', name = '4th_layer'))\ngru_model.add(layer = Dropout(rate = 0.2, name = '5th_layer'))\ngru_model.add(layer = Dense(units = len(set(y)), activation = 'softmax', name = 'output_layer'))\n# compiling the model\ngru_model.compile(optimizer = 'adam', loss = 'sparse_categorical_crossentropy', metrics = ['accuracy'])","7fda61a2":"gru_model.summary()","d38e4564":"gru_model_fit = gru_model.fit(X_train, y_train, epochs = 1)","d8a29a8b":"print(test.shape)\ntest_data = test.copy()\nprint(test_data.shape)","3802b0a5":"test_data = test_data.set_index('id', drop = True)\ntest_data.shape","c88266f4":"test_data = test_data.fillna(' ')\nprint(test_data.shape)\ntest_data.isnull().sum()","8baf0c0f":"tokenizer.fit_on_texts(texts = test_data['text'])\ntest_text = tokenizer.texts_to_sequences(texts = test_data['text'])","abb9188d":"test_text = pad_sequences(sequences = test_text, maxlen = max_features, padding = 'pre')","ea870ea4":"lstm_prediction = lstm_model.predict_classes(test_text)","b94a25a1":"submission = pd.DataFrame({'id':test_data.index, 'label':lstm_prediction})\nsubmission.shape","09955ba8":"submission.head()","e14ce515":"submission.to_csv('submission.csv', index = False)","18ede4ca":"The LSTM predictions have more accuracy.","0c84d484":"Preprocessing the Text before feeding it into the neural networks","8f054391":"We got our training data preprocessed and ready for training the neural network. \n\nWe have to create a neural network now","5e318969":"we can keep 4500 as max features for training the neural network.\n\n**minimum length is 1 ?? Looks like there are some outliers.**","fee2ac84":"*Mostly empty texts. They can be removed since they will surely guide the neural network in the wrong way*","1a923848":"out of 20,000 training samples, around 40 samples (bothering only the text column) have missing values. so we can drop them at once","aba9c490":"Constructing GRU Neural Network","e55c3cbd":"Application of LSTM and GRU Recurrent Neural Networks in Fake NEWS detection","8255f0ba":"**There are 107 outliers in this dataset. Outliers can be removed. It is a good practice to check the outliers before removing them**","00e02e42":"**Filling the Missing values**","7c46f661":"Prediction:","8860553c":"Now preparing the test dataset"}}