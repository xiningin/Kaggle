{"cell_type":{"c7857c16":"code","b220db63":"code","a3dc9d1e":"code","8a2e1f84":"code","6dd9873c":"code","24cacf8e":"code","9ab64523":"code","a74bb226":"code","a28f0846":"code","bbafafa1":"code","ab170f28":"code","b6344667":"code","e5da64fa":"code","5db550dc":"code","9cfc531a":"code","2e4ce1d6":"code","35bcc034":"code","dd3732e3":"code","b0f372a4":"code","7e362948":"code","fdbbe729":"markdown","42b2dac5":"markdown","a499a6be":"markdown","1d1098f0":"markdown","c965ebf1":"markdown","b149d307":"markdown","7a4ac3e7":"markdown","a79a7db7":"markdown","5ffab9f7":"markdown","6cb56160":"markdown","03ba5d05":"markdown","959c6ea3":"markdown","318f27a1":"markdown","897b3a91":"markdown","5da14f63":"markdown","2aea6b0d":"markdown"},"source":{"c7857c16":"# load in the data \n\nimport pandas as pd \nimport numpy as np\n\ndf = pd.read_csv('\/kaggle\/input\/spam-text-message-classification\/SPAM text message 20170820 - Data.csv')","b220db63":"df['Category'].value_counts(normalize = True)","a3dc9d1e":"from sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n# convert all text to lowercase \ndf['Message'] = df['Message'].str.lower()\n\n# perform train test split \nX_train, X_test, y_train, y_test = train_test_split(df['Message'], df['Category'], random_state=11)\n\n# vectorize text using TFIDF\ntfidf = TfidfVectorizer()\nX_train_tfidf = tfidf.fit_transform(X_train)\nX_test_tfidf = tfidf.transform(X_test)","8a2e1f84":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report\n\n\nrf = RandomForestClassifier(random_state = 11)\nrf.fit(X_train_tfidf, y_train)\nprint(classification_report(y_test, rf.predict(X_test_tfidf)))","6dd9873c":"# check distribution before applying over sampling \n\ndf['Category'].value_counts()","24cacf8e":"from imblearn.over_sampling import RandomOverSampler\n\nros = RandomOverSampler()\nX_ros, y_ros = ros.fit_resample(X_train_tfidf, y_train)\n\n# check distribution after applying over sampling \ny_ros.value_counts()","9ab64523":"rf = RandomForestClassifier()\nrf.fit(X_ros, y_ros)\nprint(classification_report(y_test, rf.predict(X_test_tfidf)))","a74bb226":"from imblearn.under_sampling import RandomUnderSampler\n\nrus = RandomUnderSampler()\nX_rus, y_rus = rus.fit_resample(X_train_tfidf, y_train)","a28f0846":"# check distribution after random under sampling \ny_rus.value_counts()","bbafafa1":"rf = RandomForestClassifier()\nrf.fit(X_rus, y_rus)\nprint(classification_report(y_test, rf.predict(X_test_tfidf)))","ab170f28":"rf = RandomForestClassifier(class_weight = 'balanced')\nrf.fit(X_train_tfidf, y_train)\nprint(classification_report(y_test, rf.predict(X_test_tfidf)))","b6344667":"!pip install googletrans==3.1.0a0","e5da64fa":"from googletrans import Translator\n\ntranslator = Translator()\n\n# translate to French\nfrench = translator.translate(df.loc[2, 'Message'], dest = 'fr')\n# translate back to English\ntranslator.translate(french.text, dest = 'en').text","5db550dc":"# original message \ndf.loc[2, 'Message']","9cfc531a":"df_train = pd.concat([X_train, y_train], axis = 1)\ndf_train.head(2)","2e4ce1d6":"import time \ntranslated_text = []\n\nfor message in df_train[df_train['Category'] == 'spam']['Message']:\n    language = np.random.choice(['fr', 'es', 'de'])\n    translated_message = translator.translate(message, dest = language)\n    translated_text.append(translator.translate(translated_message.text, dest = 'en').text)\n    time.sleep(1)","35bcc034":"translations_df = pd.DataFrame({'Message': translated_text,'Category': 'spam'})\ndf_train_translations = pd.concat([df_train, translations_df])","dd3732e3":"df_train_translations['Category'].value_counts()","b0f372a4":"df_train_translations['Message'] = df_train_translations['Message'].str.lower()\n\n# perform TFIDF \nX_train_trans_tfidf = tfidf.transform(df_train_translations['Message'])","7e362948":"rf.fit(X_train_trans_tfidf, df_train_translations['Category'])\nprint(classification_report(y_test, rf.predict(X_test_tfidf)))","fdbbe729":"### Data Augmentation \n\nNow we will try translating the Spam Messages to another language and then translate them back to English.  The idea is that we will add a little noise by performing a translation.\n\nAn example of this can be seen below ","42b2dac5":"Use the randomforest classifier on the translated data ","a499a6be":"# Imblanced Target Variable with Text Data\n\nIn this notebook I will show 4 different techniques for handling imblanced target variable \n1. Oversampling the minority class using imblearn\n2. Undersampling the majority class using imblearn\n3. Using the `class_weight` parameter in a sklearn model\n4. Data Augmentation - by translating the text into another language and then translating it back ","1d1098f0":"Now lets see an example of this for a single message.  I am going to take a message, translate it to French, and then translate it back to English ","c965ebf1":"87% of my data is of class ham and 13% is of class spam \n\nFor this notebook, I am going to be focusing on different techniques for handling imbalanced classes.  For this reason I am going to be using TF-IDF and a Random Forest Classifier for all of the different techniques. ","b149d307":"If you remember from earlier, we originally had 570 spam messages and we now have 1,140 spam messages after the data augmentation. \n\nPerform same TF-IDF that I did earlier ","7a4ac3e7":"This time we see that the precision score went down a bit for the minority class, but the recall increased.  F1-Score increased from 0.90 (baseline) to 0.92.\n\n### Class Weight ","a79a7db7":"We see that I get a F1 score on the minority class `spam` of 0.90.\n\n## Conclusions \nWe have tried 3 different techniques for handling the unbalanced class.  Next steps, trying out more data augmentation because even after doubling the number of `spam` messages there were still a lot less `spam` messages than `ham` messages with the data augmentation technique.  ","5ffab9f7":"I am going to take each Spam message and then randomly translate that message to either French, Spanish, or German, then will translate that back to English","6cb56160":"Applying the same model with the over sampled data ","03ba5d05":"F1-Score for the minority class went down to 0.88","959c6ea3":"Combine the translated and non-translated messages to one dataframe ","318f27a1":"To begin I am starting with a random forest model where I do not do anything to the classes even though they are imbalanced\n\n\n### Baseline Classifier","897b3a91":"I see that I get a relatively low recall on the minority class `spam` of 0.85\n\n### Random Over Sampling\n\nNext I am going to try random over sampling ","5da14f63":"We see that the orginal message is slightly different than the translated message.  This allows me to add new data to the dataset that is slighly different than the original messages. \n\nNow I'm going to do this for all of the Spam messages in the training set ","2aea6b0d":"We get very similar results as the baseline classifier \n\n### Random Under Sampling"}}