{"cell_type":{"58fe0f52":"code","e54a9dfe":"code","712ae5f3":"code","1fb3ea90":"code","c5b6c7e2":"code","1e5205c5":"code","f0e09e74":"code","7efb200c":"code","c7afc30a":"code","37cf027c":"code","ef2a5dce":"markdown","e34e4a40":"markdown","8cf8d896":"markdown","d80a1cdb":"markdown","91954682":"markdown"},"source":{"58fe0f52":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import StandardScaler\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","e54a9dfe":"#importing the dataset \nHAD = pd.read_csv(r\"\/kaggle\/input\/heart-attack-analysis-prediction-dataset\/heart.csv\")\nHAD.head(5)\n","712ae5f3":"sns.set()\nlst = [\"Age\",\"Sex\",\"Chest pain\",\"Resting blood pressure\",\"Cholestrol\",\"Fasting blood suger >120? \",\"resting electrocardiographic result\",\"Max heart rate\",\"Excercise induced angina\",\"old Peak\",\"slp\",\"caa\",\"thall\",\"Whether high risk?\"]\nlst1 = list(HAD.columns)\nplt.figure(figsize=(17,22))\nfor i in range (0,len(lst)):\n    plt.subplot(5,3,i+1)\n    plt.hist(HAD[lst1[i]],bins=15,color=\"green\")\n    plt.plot()\n    plt.xlabel(lst[i])\n    plt.ylabel(\"Counts\")\n\n","1fb3ea90":"plt.figure(figsize=(15,15))\nsns.heatmap(HAD.corr(),annot=True,cmap=\"Blues_r\")","c5b6c7e2":"#Exploring and removing data with 0,1 values as they wont be of much help in making the clusters\nlst = [\"sex\",\"fbs\",\"exng\",\"output\"]\nfor i in range(0, len(lst)):\n    print(HAD[lst[i]].value_counts())\n","1e5205c5":"#dropping the above explored columns\nHADE = HAD.drop(lst,axis=1)\nHADE.head(5)\n","f0e09e74":"#standardizing the dataset and applying K-means clustering on the dataset\nSS = StandardScaler(with_mean=0,with_std=1)\nHADES = SS.fit_transform(HADE)\nfrom sklearn.cluster import KMeans\nKMC = KMeans(n_clusters=2)\nKMC.fit(HADES)\ncenter_age = [KMC.cluster_centers_[0][0],KMC.cluster_centers_[1][0]]\ncenter_thalachh = [KMC.cluster_centers_[0][5],KMC.cluster_centers_[1][5]]","7efb200c":"#to have a visualisation of the prediction with n = 2 lets plot graph between age and max_heart_rate as these two variables have wide range of value\n\nplt.figure(figsize=(20,10))\nplt.subplot(1,2,1)\nsns.scatterplot(x=\"age\",y=\"thalachh\",data=HADE,hue = KMC.labels_,s=40)\nplt.title(\"Predicted outcome based on clustering\")\nplt.xlabel(\"Age\")\nplt.ylabel(\"Max heart Rate\")\nplt.subplot(1,2,2)\nsns.scatterplot(x=\"age\",y=\"thalachh\",data=HADE,hue = HAD[\"output\"],s=40)\nplt.title(\"Actual outcome\")\nplt.xlabel(\"Age\")\nplt.ylabel(\"Max heart Rate\")","c7afc30a":"def accr(actual_labels,predicted_labels):\n    count=0\n    for i in range(0,len(actual_labels)):\n        if(actual_labels[i]==predicted_labels[i]):\n            count = count+1\n    accuracy = count\/len(actual_labels)\n    return(1-accuracy)\nprint(accr(list(HAD[\"output\"]),list(KMC.labels_)))       ","37cf027c":"metric = []\ni=[]\nfor k in range (2,10):\n    KMC = KMeans(n_clusters=k)\n    KMC.fit(HADES)\n    metric.append(KMC.inertia_)\n    i.append(k)\nplt.figure(figsize=(10,10))    \nplt.plot(i,metric,'bx-')\nplt.xlabel('No of centroids')\nplt.ylabel('sum of square of distance between nearest centroid')","ef2a5dce":"> Data description :-\n> We have dataset with 303 rows and 14 columns,\n> each column represents,\n> 1. \"age\" = age of the patient.\n> 2. \"sex\" = whether male(1) or female(0), \n> 3. \"cp\" = intensity of chest pain (whether mild,moderate or severe)\n> 4. \"trtbps\" = resting blood pressure (in mm Hg)\n> 5. \"chol\" = cholestrol in mg\/dl\n> 6. \"fbs\" = fasting blood sugar>120, 1 for \"True\" and 0 for \"False\"\n> 7. \"restecg\" = resting electrocardiographic results\n> 8. \"thalachh\" = maximum heart rate achieved\n> 9. \"exng\" = excercise induced angina\n> 10. \"old peak\" = previous peak","e34e4a40":"Clustering is a great technique to divide the dataset into different similar segments, the purpose of this notebook is to apply different clustering techniques, namely :-\n1. K-means clustering\n2. heirarchial clustering\n3. Density based clustering (DBSCAN)\n\nWe will be applying visualisations and analyse the clusters using these methods and report the pros and cons of using them as we explore while we proceed with the implementation\n\nI have chosen The heart-attack-prediction data to perform clustering.","8cf8d896":"![](https:\/\/www.ucsf.edu\/sites\/default\/files\/legacy_files\/futuristicheart-600-04092012.jpg)","d80a1cdb":"> The clusters predicted by KMeans has accuracy of around 80%, which is good enough","91954682":"Lets analyze the pros and cons of using the KMeans clustering :-\n\nPros would be :-\n1. Can be used on large dataset\n2. Easy to use\n\nCons would be :-\n1. it uses spherical shaped clusters only, hence fails when we have different segment based on other shapes"}}