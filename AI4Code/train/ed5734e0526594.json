{"cell_type":{"be9ab3c4":"code","7be65aea":"code","70e7bc4f":"code","01e6bc55":"code","2c42da27":"code","e653ad94":"markdown","77c332d7":"markdown","89301db8":"markdown","426a04f5":"markdown","b54291e9":"markdown","0cde3fae":"markdown"},"source":{"be9ab3c4":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nimport lightgbm as lgb\nimport random\nimport os\nimport numpy as np\n\n# Function to fix some seeds\ndef seed_everything(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n\nSEED = 42\nseed_everything(SEED)\n\n# Read train data\ndf_train = pd.read_csv(\"\/kaggle\/input\/tabular-playground-series-dec-2021\/train.csv\")\ndf_train.drop([\"Id\"], axis=1, inplace=True)\n\n# Only sample a smaller fraction of the train data for speed purposes\ndf_train = df_train.sample(frac=0.25, random_state=SEED)","7be65aea":"y = df_train[\"Cover_Type\"]\nX = df_train.drop([\"Cover_Type\"], axis=1)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=SEED)","70e7bc4f":"params = {\n    \"objective\": \"multiclass\",\n    \"num_classes\": 7,\n    \"random_state\": SEED,\n}\n\nfor _ in range(3):\n    model = lgb.LGBMClassifier(**params)\n    model.fit(X_train, y_train)\n    y_test_pred = model.predict(X_test)\n    print(accuracy_score(y_test, y_test_pred))","01e6bc55":"y = (df_train[\"Cover_Type\"] > 3).astype(int)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=SEED)","2c42da27":"params = {\n    \"objective\": \"binary\",\n    \"num_classes\": 1,\n    \"random_state\": SEED,\n}\n\nfor _ in range(3):\n    model = lgb.LGBMClassifier(**params)\n    model.fit(X_train, y_train)\n    y_test_pred = model.predict(X_test)\n    print(accuracy_score(y_test, y_test_pred))","e653ad94":"For binary classification the accuracy scores are the same. What happens with the multiclass problem? Is it doing something under the hood that I'm not aware of?","77c332d7":"Now we can try to repeat the same process for binary classification. First, convert the target to binary:","89301db8":"Ok, now let's fit the model, predict on the test set, and calculate the accuracy score. Let's do it 3 times and check whether the scores are different:","426a04f5":"You (probably) see that the values are not the same, and the gap between those is significant (I said probably because sometimes some of them are the same!). ","b54291e9":"I used a simple train-test split because it is only to show the no reproducible results:","0cde3fae":"##### Fixing the seeds during your experimentation stage is important in order to be sure that your improvements are real and not due to randomness. \n\n##### In this problem I just wanted to give a try to the LightGBM model and I bumped into the following reproducibility issue (that drove me crazy):"}}