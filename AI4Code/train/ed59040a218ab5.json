{"cell_type":{"8bd3bf56":"code","ab5ff89f":"code","44b66fd2":"code","81709d61":"code","8a6655f0":"code","e6b56d65":"code","8ec022c4":"code","9e97eba1":"code","865c6593":"code","b0c532cd":"code","d010711a":"code","e9dccbe1":"code","f6932a8a":"code","9ad3d0db":"code","a02176a8":"code","493f8124":"code","a076ee4a":"code","e8346cf6":"code","ba8903b6":"code","42797bd4":"markdown","0f71989d":"markdown","1716e036":"markdown","23136cb9":"markdown","bf2d517c":"markdown","2026ff55":"markdown","2700b1e3":"markdown","c2eaf8ba":"markdown","8f3d4088":"markdown","1c47f5a2":"markdown","8d2a220d":"markdown","1fce194e":"markdown","105640fa":"markdown","fcfd9fde":"markdown","338aa59a":"markdown","1e4b5704":"markdown","4a393359":"markdown","bb044dc8":"markdown","f7ba285e":"markdown","64b8338b":"markdown","bee503af":"markdown","5886cc2b":"markdown","5d900754":"markdown","70cccb29":"markdown"},"source":{"8bd3bf56":"import string\nimport re\nimport operator\nimport pickle\nfrom collections import Counter\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom tqdm._tqdm_notebook import tqdm_notebook as tqdm; tqdm.pandas()\n\nfrom nltk.tokenize.treebank import TreebankWordTokenizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score, accuracy_score, confusion_matrix\nfrom torchtext.data import Example, Field, Dataset, LabelField, Iterator, BucketIterator\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\n\n# dataframe options to display the whole comments\npd.set_option('display.max_colwidth', -1)\n\nINPUT = \"..\/input\/\"\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'","ab5ff89f":"# load only necessary fields to reduce memory footprint\nfields = ['id', 'comment_text', 'toxicity_annotator_count', 'insult', 'target']\n# load training set\ntrain_df = pd.read_csv(f\"{INPUT}jigsaw-unintended-bias-in-toxicity-classification\/train.csv\", usecols=fields).set_index('id')\nX_train = train_df['comment_text']\ny_train = train_df['target'] >= 0.5  # create binary target column\ntrain_df['is_toxic'] = y_train\ntrain_df['is_insult'] = train_df['insult'] >= 0.5\n# split training set to train\/dev\/test set with stratified strategy\nX_train, X_dev_test, y_train, y_dev_test = train_test_split(X_train, y_train, stratify=y_train, test_size=0.2, random_state=1337)\nX_dev, X_test, y_dev, y_test = train_test_split(X_dev_test, y_dev_test, stratify=y_dev_test, test_size=0.5, random_state=1338)\n# load submission set\ntest_df = pd.read_csv(f\"{INPUT}jigsaw-unintended-bias-in-toxicity-classification\/test.csv\").set_index('id')\nX_submission = test_df['comment_text']","44b66fd2":"print(\"Length train: {:,}; dev: {:,}; test: {:,}; submission: {:,}\".format(\n    len(X_train), len(X_dev), len(X_test), len(X_submission)\n))","81709d61":"def load_embeddings(path):\n    with open(path,'rb') as f:\n        emb_arr = pickle.load(f)\n    return emb_arr\n\nEMBEDDING_PATH = f\"{INPUT}pickled-glove840b300d-for-10sec-loading\/glove.840B.300d.pkl\"\nembeddings = load_embeddings(EMBEDDING_PATH)","8a6655f0":"# take all comments with toxicity annotator count >= 10 ** 1.5\n# train_trust_df = train_df[(train_df.index.isin(X_train.index)) & (train_df['toxicity_annotator_count'] >= 10 ** 1.5)]\n# filter comments with toxicity annotator count < 10 ** 1.5 based on the target value\n# train_chosen_df = train_df[train_df.index.isin(X_train.index) & (train_df['toxicity_annotator_count'] < 10 ** 1.5)]\ntrain_chosen_df = train_df[train_df.index.isin(X_train.index)]\n# for safe comments, take only those with both target and insult value < 0.2 (\"very safe\")\ntrain_chosen_very_safe_df = train_chosen_df[(train_chosen_df['target'] < 0.2) & (train_chosen_df['insult'] < 0.2)]\n# for toxic comments, ignore those between 0.4 and 0.5\ntrain_chosen_toxic_df = train_chosen_df[train_chosen_df['target'] >= 0.6]\ntrain_chosen_others_df = train_chosen_df[(train_chosen_df['target'] >= 0.2) & (train_chosen_df['target'] < 0.4)]\n# take maximum 1mil to fit processing within 2-hour time limit for submission and the given memory size\n# n_very_safe = min(len(train_chosen_very_safe_df), \n#                   1100032 - len(train_trust_df) - len(train_chosen_others_df) - len(train_chosen_toxic_df))\nn_very_safe = min(len(train_chosen_very_safe_df), \n                  1100032 - len(train_chosen_others_df) - len(train_chosen_toxic_df))\ntrain_chosen_very_safe_sample_df = train_chosen_very_safe_df.sample(n=n_very_safe, random_state=1337)\n# get all valid comment indices\ntrain_indices = [\n    #*train_trust_df.index.values,\n    *train_chosen_very_safe_sample_df.index.values, \n    *train_chosen_others_df.index.values, \n    *train_chosen_toxic_df.index.values\n]\nX_train = X_train[X_train.index.isin(train_indices)]\nX_dev = X_dev.sample(frac=1, random_state=1337)\nX_test = X_test.sample(frac=1, random_state=1337)","e6b56d65":"print(\"Length train: {:,}; dev: {:,}; test: {:,}; submission: {:,}\".format(\n    len(X_train), len(X_dev), len(X_test), len(X_submission)\n))","8ec022c4":"def build_character_set(sentences, verbose=True):\n    characters_in_dataset = {}\n    for sentence in tqdm(sentences, disable=(not verbose)):\n        for character in sentence:\n            try:\n                characters_in_dataset[character] += 1\n            except KeyError:\n                characters_in_dataset[character] = 1\n    return characters_in_dataset\n\ntrain_charset = build_character_set(X_train)\n# build all accepted characters\nlatin_similar = \" \u2019'\u2018\u00c6\u00d0\u018e\u018f\u0190\u0194\u0132\u014a\u0152\u1e9e\u00de\u01f7\u021c\u00e6\u00f0\u01dd\u0259\u025b\u0263\u0133\u014b\u0153\u0138\u017f\u00df\u00fe\u01bf\u021d\u0104\u0181\u00c7\u0110\u018a\u0118\u0126\u012e\u0198\u0141\u00d8\u01a0\u015e\u0218\u0162\u021a\u0166\u0172\u01afY\u0328\u01b3\u0105\u0253\u00e7\u0111\u0257\u0119\u0127\u012f\u0199\u0142\u00f8\u01a1\u015f\u0219\u0163\u021b\u0167\u0173\u01b0y\u0328\u01b4\u00c1\u00c0\u00c2\u00c4\u01cd\u0102\u0100\u00c3\u00c5\u01fa\u0104\u00c6\u01fc\u01e2\u0181\u0106\u010a\u0108\u010c\u00c7\u010e\u1e0c\u0110\u018a\u00d0\u00c9\u00c8\u0116\u00ca\u00cb\u011a\u0114\u0112\u0118\u1eb8\u018e\u018f\u0190\u0120\u011c\u01e6\u011e\u0122\u0194\u00e1\u00e0\u00e2\u00e4\u01ce\u0103\u0101\u00e3\u00e5\u01fb\u0105\u00e6\u01fd\u01e3\u0253\u0107\u010b\u0109\u010d\u00e7\u010f\u1e0d\u0111\u0257\u00f0\u00e9\u00e8\u0117\u00ea\u00eb\u011b\u0115\u0113\u0119\u1eb9\u01dd\u0259\u025b\u0121\u011d\u01e7\u011f\u0123\u0263\u0124\u1e24\u0126I\u00cd\u00cc\u0130\u00ce\u00cf\u01cf\u012c\u012a\u0128\u012e\u1eca\u0132\u0134\u0136\u0198\u0139\u013b\u0141\u013d\u013f\u02bcN\u0143N\u0308\u0147\u00d1\u0145\u014a\u00d3\u00d2\u00d4\u00d6\u01d1\u014e\u014c\u00d5\u0150\u1ecc\u00d8\u01fe\u01a0\u0152\u0125\u1e25\u0127\u0131\u00ed\u00eci\u00ee\u00ef\u01d0\u012d\u012b\u0129\u012f\u1ecb\u0133\u0135\u0137\u0199\u0138\u013a\u013c\u0142\u013e\u0140\u0149\u0144n\u0308\u0148\u00f1\u0146\u014b\u00f3\u00f2\u00f4\u00f6\u01d2\u014f\u014d\u00f5\u0151\u1ecd\u00f8\u01ff\u01a1\u0153\u0154\u0158\u0156\u015a\u015c\u0160\u015e\u0218\u1e62\u1e9e\u0164\u0162\u1e6c\u0166\u00de\u00da\u00d9\u00db\u00dc\u01d3\u016c\u016a\u0168\u0170\u016e\u0172\u1ee4\u01af\u1e82\u1e80\u0174\u1e84\u01f7\u00dd\u1ef2\u0176\u0178\u0232\u1ef8\u01b3\u0179\u017b\u017d\u1e92\u0155\u0159\u0157\u017f\u015b\u015d\u0161\u015f\u0219\u1e63\u00df\u0165\u0163\u1e6d\u0167\u00fe\u00fa\u00f9\u00fb\u00fc\u01d4\u016d\u016b\u0169\u0171\u016f\u0173\u1ee5\u01b0\u1e83\u1e81\u0175\u1e85\u01bf\u00fd\u1ef3\u0177\u00ff\u0233\u1ef9\u01b4\u017a\u017c\u017e\u1e93\"\nwhite_list = string.ascii_letters + string.digits + latin_similar\n# check all symbols that have embeddings to keep it\nglove_symbols = [symbol for symbol in embeddings if len(symbol) == 1 and symbol not in white_list]\nchar_translate_map = {}\n# isolate symbols that have embeddings vector in glove\nfor symbol in glove_symbols:\n    char_translate_map[ord(symbol)] = ' ' + symbol + ' '\n# remove all unknown symbols\nunknown_symbols = [c for c in train_charset if c not in white_list and c not in glove_symbols]\nfor symbol in unknown_symbols:\n    char_translate_map[ord(symbol)] = ''\n# replace newline characters with normal separator (blank space)\nchar_translate_map[ord('\\n')] = ' ' ","9e97eba1":"def find_in_embedding(word, embedding):\n    '''\n    find the word's vector in the embedding. \n    If there is no exact word found, try also with lower case and title case.\n    In case there is no entry found in the embedding, replace with the unknown token (<unk>)\n    '''\n    if word in embedding:\n        return word, embedding[word]\n    elif word.lower() in embedding:\n        return word.lower(), embedding[word.lower()]\n    elif word.title() in embedding:\n        return word.title(), embedding[word.title()]\n    else:\n        return '<unk>', embedding['<unk>']\n\ndef preprocess(comment, translate_map, tokenizer, emb):\n    # translate symbols\n    preprocessed_comment = comment.translate(translate_map)\n    # handle quote at the beginning\n    # because we accepted quotes as characters, sometimes we encounter token such as \"'we\".\n    # with this step, we transform \"'we\" into \"' we\" and such that it can be tokenized as 2 tokens.\n    preprocessed_comment = [(\"' \" + token[1:]) if len(token) > 0 and token[0] == \"'\" else token for token in preprocessed_comment.split(' ')]\n    # tokenize with Treebank tokenizer\n    tokenized_comment = tokenizer.tokenize(' '.join(preprocessed_comment))\n    # check and find each token in embeddings\n    tokenized_comment = [find_in_embedding(token, emb)[0] for token in tokenized_comment]\n    return ' '.join(tokenized_comment)\n\ntokenizer = TreebankWordTokenizer()\nX_train = X_train.progress_apply(lambda comment: preprocess(comment, char_translate_map, tokenizer, embeddings))\nX_dev = X_dev.progress_apply(lambda comment: preprocess(comment, char_translate_map, tokenizer, embeddings))\nX_test = X_test.progress_apply(lambda comment: preprocess(comment, char_translate_map, tokenizer, embeddings))\nX_submission = X_submission.progress_apply(lambda comment: preprocess(comment, char_translate_map, tokenizer, embeddings))","865c6593":"X_train.head()","b0c532cd":"def prepare_torch_dataset(comments, target, fields):\n    '''\n    transform a series `comments` to torchtext `Dataset`\n    '''\n    df = pd.DataFrame(comments).join(target[['is_toxic', 'is_insult']], how='left')\n    train_examples = [Example.fromlist(i, fields) for i in tqdm(df.values.tolist())]\n    return Dataset(train_examples, fields)\n\n# define the Fields\nID = Field(sequential=False, use_vocab=False, dtype=torch.long)\nTEXT = Field(sequential=True, tokenize='spacy', pad_first=True)\nLABEL = LabelField(sequential=False, use_vocab=False, dtype=torch.float32)\nfields = [(\"comment_text\", TEXT), (\"target\", LABEL), (\"insult\", LABEL)]\n# prepare train\/dev\/test datasets as torch `Dataset`\ntrain_dataset = prepare_torch_dataset(X_train, train_df, fields)\ndev_dataset = prepare_torch_dataset(X_dev, train_df, fields)\ntest_dataset = prepare_torch_dataset(X_test, train_df, fields)\n# prepare submission dataset as torch `Dataset`\ntest_df['comment_text'] = X_submission\nsubm_fields = [(\"id\", ID), (\"comment_text\", TEXT)]  # notice that there is no label field for the submission dataset\nsubm_examples = [Example.fromlist(i, subm_fields) for i in tqdm(test_df.reset_index().values.tolist())]\nsubm_dataset = Dataset(subm_examples, subm_fields)","d010711a":"def build_matrix(vocab_itos, emb):\n    embedding_matrix = np.zeros((len(vocab_itos), 300))\n    for i, word in enumerate(vocab_itos):\n        embedding_matrix[i] = find_in_embedding(word, emb)[1]\n    return embedding_matrix\n\nTEXT.build_vocab(train_dataset, dev_dataset, max_size=60000)\nembedding_matrix = build_matrix(TEXT.vocab.itos, embeddings)","e9dccbe1":"for i in range(20):\n    word = TEXT.vocab.itos[i]\n    print(\"id={}, token='{}', freq={}\".format(i, word, TEXT.vocab.freqs[word]))","f6932a8a":"train_iter, dev_iter, test_iter, subm_iter = BucketIterator.splits(\n    (train_dataset, dev_dataset, test_dataset, subm_dataset),\n    batch_sizes=(64, 64, 64, 64),\n    device=0,\n    sort_key=lambda comment: len(comment.comment_text),\n    sort_within_batch=False,\n    repeat=False  # do not repeat any inputs\n)\n\nclass BatchWrapper:\n    '''\n    helper wrapper to iterate over `BucketIterator` during training and return (x, y).\n    from: https:\/\/mlexplained.com\/2018\/02\/08\/a-comprehensive-tutorial-to-torchtext\/\n    '''\n    \n    def __init__(self, dl, x_var, y_vars):\n        self.dl, self.x_var, self.y_vars = dl, x_var, y_vars\n\n    def __iter__(self):\n        for batch in self.dl:\n            # return x_var attribute of the data loader (dl) as x\n            x = getattr(batch, self.x_var)\n            # return label attributes of the data loader (dl) as y\n            y = torch.cat([getattr(batch, feat).unsqueeze(1) for feat in self.y_vars], dim=1)\n            yield (x, y)\n\n    def __len__(self):\n        return len(self.dl)\n\ntrain_dl = BatchWrapper(train_iter, \"comment_text\", [\"target\", \"insult\"])\ndev_dl = BatchWrapper(dev_iter, \"comment_text\", [\"target\", \"insult\"])\ntest_dl = BatchWrapper(test_iter, \"comment_text\", [\"target\", \"insult\"])","9ad3d0db":"class SpatialDropout(nn.Dropout2d):\n    def forward(self, x):\n        x = x.unsqueeze(2)    # (N, T, 1, K)\n        x = x.permute(0, 3, 2, 1)  # (N, K, 1, T)\n        x = super(SpatialDropout, self).forward(x)  # (N, K, 1, T), some features are masked\n        x = x.permute(0, 3, 2, 1)  # (N, T, 1, K)\n        x = x.squeeze(2)  # (N, T, K)\n        return x\n\nclass LSTM(nn.Module):\n    def __init__(self, emb_matrix, lstm_units=150):\n        super().__init__()\n        # LAYER 1: EMBEDDING, non-trainable\n        self.embedding = nn.Embedding(*emb_matrix.shape)\n        # use glove embeddings\n        self.embedding.weight = nn.Parameter(torch.tensor(emb_matrix, dtype=torch.float32), requires_grad=False)\n        # LAYER 2: SPATIAL DROPOUT\n        self.embedding_dropout = SpatialDropout(0.3)\n        # LAYER 3: LSTM, bi-directional, output = 2 * lstm_units\n        self.lstm_units = lstm_units\n        self.lstm1 = nn.LSTM(embedding_matrix.shape[1], lstm_units, bidirectional=True)\n        # LAYER 4: LSTM, bi-directional, output = 2 * lstm_units\n        self.lstm2 = nn.LSTM(2 * lstm_units, lstm_units, bidirectional=True)\n        # LAYER 5: CONCAT, no object created\n        # LAYER 6: FC1\n        self.linear1 = nn.Linear(3 * 2 * lstm_units, 3 * 2 * lstm_units)\n        # LAYER 7: FC2\n        self.linear2 = nn.Linear(3 * 2 * lstm_units, 3 * 2 * lstm_units)\n        # LAYER 8: ADDITION of layer outputs, no object created\n        # LAYER 9: DROPOUT\n        self.dropout = nn.Dropout(0.1)\n        # LAYER 10: OUTPUT LAYER for target\n        self.target_out = nn.Linear(3 * 2 * lstm_units, 1)\n        # LAYER 11: OUTPUT LAYER for insult\n        self.aux_out = nn.Linear(3 * 2 * lstm_units, 1)\n\n    def forward(self, seq):\n        # get embedding vector of every word\n        h_emb = self.embedding_dropout(self.embedding(seq))\n        # walk through the first bi-directional LSTM\n        h_lstm1, _ = self.lstm1(h_emb)\n        # use output of previous LSTM as input for the second bi-directional LSTM\n        h_lstm2, _ = self.lstm2(h_lstm1)\n        # get the latest state of LSTM 2\n        # because it is bi-directional, get the first half output of the last state (LSTM direction forward)\n        # and the second half output of the first state (LSTM direction backward)\n        h_lstm_last = h_lstm2[-1, :, :self.lstm_units]\n        h_lstm_first = h_lstm2[0, :, self.lstm_units:]\n        # concat both half\n        h_bi_lstm = torch.cat((h_lstm_first, h_lstm_last), -1)\n        # get average and max pool of all hidden state from LSTM 2\n        avg_pool = torch.mean(h_lstm2, 0)\n        max_pool, _ = torch.max(h_lstm2, 0)\n        # concat last state with pooling\n        h_conc = torch.cat((h_bi_lstm, max_pool, avg_pool), 1)\n        # fully-connected layer with last state and the pooling\n        h_lin1 = F.relu(self.linear1(h_conc))\n        h_lin2 = F.relu(self.linear2(h_conc))\n        # add concat with output of fully-connected layer\n        h_conc_add = h_conc + h_lin1 + h_lin2\n        # another dropout to force model check last state, average pool, and max pool values\n        h_conc_do = self.dropout(h_conc_add)\n        # get linear prediction of target and insult\n        # we do not use sigmoid here because pyTorch provides a better loss calculation\n        # by combining sigmoid with cross-entropy\n        target_out = self.target_out(h_conc_do)\n        aux_out = self.aux_out(h_conc_do)\n        out = torch.cat([target_out, aux_out], 1)\n        return out\n    \nmodel = LSTM(embedding_matrix).to(device)\nmodel","a02176a8":"def sigmoid(x):\n    return 1 \/ (1 + np.exp(-x))\n\ndef evaluate(model, dl, loss_func):\n    '''\n    evaluation steps for dev and test set\n    '''\n    running_loss = 0.0\n    model.eval()  # turn on evaluation mode\n    all_preds = {'y_pred' : [], 'y_true' : []}\n    for x, y in dl: # iterate over mini-batches\n        x, y = x.to(device), y.to(device)  # transfer x and y to GPU when it is available\n        preds = model(x)  # get the prediction from the model\n        loss = loss_func(preds, y)  # calculate loss values\n        # add all predictions and y_true to a dictionary for calculating the evaluation metrics\n        all_preds['y_pred'] = [*all_preds['y_pred'], *sigmoid(preds.detach().cpu().numpy()[:,0].ravel())]\n        all_preds['y_true'] = [*all_preds['y_true'], *y.detach().cpu().numpy()[:,0].ravel()]\n        # add current mini-batch loss to the total loss\n        running_loss += loss.item() \/ len(dl)\n    # create a binary prediction to calculate accuracy\n    all_preds['y_pred_bin'] = [1 if pred >= 0.5 else 0 for pred in all_preds['y_pred']]\n    print('\\tEval Loss: {:.4f}, Eval AUC: {:.4f}, Eval Accuracy: {:.4f}'.format(\n        loss,\n        roc_auc_score(all_preds['y_true'], all_preds['y_pred']),\n        accuracy_score(all_preds['y_true'], all_preds['y_pred_bin'])\n    ))\n    # confusion analysis, especially helpful for imbalanced dataset\n    tn, fp, fn, tp = confusion_matrix(all_preds['y_true'], all_preds['y_pred_bin']).ravel()\n    print('\\tEval CM: tn={:,}({:.2f}%), tp={:,}({:.2f}%), fn={:,}({:.2f}%), fp={:,}({:.2f}%)'.format(\n        tn, tn * 100 \/ len(all_preds['y_pred_bin']),\n        tp, tp * 100 \/ len(all_preds['y_pred_bin']),\n        fn, fn * 100 \/ len(all_preds['y_pred_bin']),\n        fp, fp * 100 \/ len(all_preds['y_pred_bin'])\n    ))\n    \n    \ndef train(model, train, dev, n_epochs, loss_func, lr):\n    '''\n    model training steps for the training set. Output dev metrics at each epoch.\n    PyTorch requires implementation of training procedure.\n    '''\n    opt = optim.Adam(model.parameters(), lr=lr)  # optimizer\n    # degrade learning rate at every epoch with lr scheduler\n    scheduler = optim.lr_scheduler.StepLR(opt, step_size=1, gamma=0.3)\n    for epoch in range(1, n_epochs + 1):\n        print('Epoch: {}'.format(epoch))\n        model.train()  # during training, dropout is activated\n        running_loss = 0.0\n        for x, y in tqdm(train):\n            opt.zero_grad()  # necessary at the beginning of each mini-batch for pytorch\n            x, y = x.to(device), y.to(device)  # transfer to GPU when available\n            preds = model(x)  # get prediction\n            loss = loss_func(preds, y)  # calculate loss\n            loss.backward()  # backpropagation\n            opt.step()  # calculate optimizer with adam\n            running_loss += loss.item() \/ len(train_dl)  # add current mini-batch loss to total loss\n        print('\\tTraining Loss: {:.4f}'.format(running_loss))\n        evaluate(model, dev, loss_func)  # check dev scores\n        scheduler.step()  # reduce learning rate\n\n# Binary Cross-Entropy With Logits Loss: optimized loss function for binary classification\n# when using this loss function, do not use sigmoid activation function in the output layer of the model.\nbce_loss = nn.BCEWithLogitsLoss(\n    pos_weight=torch.FloatTensor([1]).cuda(),   # I tried to penalize mis-classified positive instance more\n                                                # but it did not give better LB score\n    reduction='mean'  # because we are predicting 2 outputs, take the mean as loss value\n)\ntrain(model, train_dl, dev_dl, 2, bce_loss, lr=8e-4)  # run training","493f8124":"evaluate(model, test_dl, bce_loss)  # evaluate on test set","a076ee4a":"# fine-tuning embedding layer\nmodel.embedding.weight.requires_grad = True\ntrain(model, train_dl, dev_dl, 1, bce_loss, lr=8e-5)","e8346cf6":"evaluate(model, test_dl, bce_loss)  # evaluate on test set","ba8903b6":"model.eval()  # turn on evaluation mode to ignore dropouts\nsubm_id = []\nsubm_preds = []\nfor example in tqdm(subm_iter):\n    x_id = example.id\n    x = example.comment_text.to(device)\n    preds = model(x)\n    subm_id = [*subm_id, *x_id.detach().cpu().numpy().ravel()]\n    subm_preds = [*subm_preds, *sigmoid(preds.detach().cpu().numpy()[:,0].ravel())]\nsubmission = pd.DataFrame.from_dict({\n    'id': subm_id,\n    'prediction': subm_preds\n})\nsubmission.to_csv('submission.csv', index=False)","42797bd4":"## 2.1. Filtering","0f71989d":"## 4.1. Network Architecture","1716e036":"Because I use embeddings, there are some symbols that do not have the respective embedding vectors. In this section, I check all characters used in the training set. The character set is compared to glove embeddings symbols afterward. Instead of using slow `replace()` function, we will use `translate()`. Basically, `translate()` take a dictionary of character mapping as input. The key of the dictionary is the ordinal of the characters.","23136cb9":"# 1. Data Loading","bf2d517c":"Based on the EDA, I choose to ignore comments with target value between 0.4 and 0.6. Also, because the training set consists of an abundant number of comments with target value = 0, I decided only to sample comments that have both really low value for target and insult.","2026ff55":"## 3.1. Create PyTorch Datasets","2700b1e3":"# 2. Data Preprocessing","c2eaf8ba":"In this kernel, I implement LSTM for toxicity prediction using only PyTorch (and torchtext) whenever possible. I have been a keras user since forever, but I notice more and more new architectures are built through PyTorch. Thus, the main purpose of building this kernel for me is to learn PyTorch. I leave some comments to explain what each section is trying to do, hopefully it can help you too.\n\nI incorporate many techniques that I have learned from other excellent kernels, shoutout to:\n- @bminixhofer: https:\/\/www.kaggle.com\/bminixhofer\/simple-lstm-pytorch-version\n- @christofhenkel: https:\/\/www.kaggle.com\/christofhenkel\/how-to-preprocessing-for-glove-part1-eda\n\nHaving been inspired by those kernels, there are some modifications:\n- based on my EDA kernel (https:\/\/www.kaggle.com\/yosefardhito\/eda-annotations-lexical-and-bag-of-words), I ignore comments with both low toxicity annotator count and toxic target value between 0.4 and 0.6 because I noticed how comments with target between that range has a questionable label even for me, let alone the model. For auxiliary attributes, I use only 'insult' because it is the only attributes significantly correlated with the target column. \n- I add a simple train\/dev\/test split process so I can experiment with the training data alone.\n- Paddings are added at the beginning instead of after the comment.\n- Slight modification of the Simple LSTM model on the concatenation.\n\nWhy LSTM? I know from the stories of the top leaderboards that BERT and XLNet provide the best result. However, since the goal of this kernel is to learn, I prefer training a model with simpler and modifiable architecture. I want to emphasize on learning how to do it in PyTorch instead. Surely, pre-trained BERT and XLNet are next to be mastered!","8f3d4088":"I only try glove embeddings during this experiment. Pickled version is used to reduce time for loading.","1c47f5a2":"# Toxic Comment Detection with PyTorch LSTM","8d2a220d":"## 1.2. Load Embedding","1fce194e":"The evaluation metrics used in this loop is basic AUC ROC, whereas the competition impose sub-groups AUC score in the calculation.","105640fa":"Before we can train a pytorch model, we have to transform the input to the form that is expected by the model. One option is to use keras text preprocessing, but I'd rather stick with pytorch family, i.e. torchtext. First, we want to transform the input into a torchtext `Dataset`. A `Dataset` is a collection of `Example`, which consists of `Field`. More info: https:\/\/torchtext.readthedocs.io\/en\/latest\/data.html\n\nA `Field` has many useful options for NLP, among many of them are:\n- `pad_first=True` to set padding tokens added at the beginning instead of after the comment\n- `fix_length=None`, a default, to have varying length\n- `sequential=True` means the field is to be treated a sequential data. For text, set it to `True`, set otherwise for labels.\n- `use_vocab=True` whether to build vocabulary based on this field. Set to `True` for the text fields.\n\nThe downside of this approach is the slowness due to the iteration to create the `Example` and `Dataset`. There should be a better way to convert the input to torchtext `Dataset`. There is also a `TabularDataset` class which can transform the whole dataset at one, but I am not aware how to include the preprocessing required without exporting the preprocessed dataset beforehand. ","fcfd9fde":"In this dataset, the comment length ranges from 1 word to hundreds. If we take the maximum length of a comment and pad everything according to the maximum length, it won't be effective for training. Sequence bucketing can help reduce the time required to train. We reorder the comments based on their length, such that comments with similar length is more likely to belong on the same batch. At each batch, pad every comments to the maximum comment length on that batch. Therefore, we do not need to pad 1-word comment to the max length, and training can be much quicker.\n\nI avoid truncating any of the comment because it is misleading to decide either truncate from beginning or from end. A comment can be toxic just because 1 or 2 words it contains and those words can be at the beginning or at the end. Consequently, some outlier lengthy comments will affect the padding of much shorter ones.\n\nIn torchtext, we can do sequence bucketing with `BucketIterator` class. The important parameters are `sort_key` and `sort_within_batch`. Basically, `sort_key=True` does what we expect from sequence bucketing and it is no longer necessary to also set `sort_within_batch=True` because the comments in one batch will be padded to the maximum length of that batch anyway.","338aa59a":"## 2.2. Prepare Accepted Characters","1e4b5704":"# 4. Model Training","4a393359":"## 2.3. Text Preprocessing","bb044dc8":"# 3. Data Preparation for PyTorch","f7ba285e":"## 1.1. Load Dataset","64b8338b":"The architecture of the network follows Simple LSTM architecture (https:\/\/www.kaggle.com\/thousandvoices\/simple-lstm) with slight modification. These are the layers:\n1. Embedding layer (1x60000) &rarr; (60000x300)\n2. Spatial dropout of embedding with dropout rate 0.3\n3. BiLSTM 1: 300 &rarr; 150 * 2 (bi-directional)\n4. BiLSTM 2: 300 &rarr; 300\n5. Concatenation of last state of BiLSTM (both direction) with average and max pooling of all hidden state: 300 + 300 + 300 = 900\n6. Fully-connected 1: 900 of concat result &rarr; 900\n7. Fully-connected 2: 900 of concat result &rarr; 900\n8. Add result of layer 5, 6, and 7: 900\n9. Dropout rate of 0.1 from layer 8\n10. Output layer for target: 768 &rarr; 1\n11. Output layer for insult: 768 &rarr; 1","bee503af":"# 5. Submission","5886cc2b":"## 3.2. Sequence Bucketing","5d900754":"Quite handily, torchtext also provides us with `build_vocab()` capability. This function helps us with the mapping between token and integer, useful for embeddings. Additionally, it contains the frequency of each word as well. `max_size` parameter determines maximum number of words registered in the vocabulary, truncated based on the frequency (top `max_size` words with the highest frequency will be used).","70cccb29":"## 4.2. Training"}}