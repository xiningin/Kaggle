{"cell_type":{"1ebc1952":"code","f07274f1":"code","0c629e16":"code","98b1098d":"code","8f2efcff":"code","d7d381c0":"code","8569e36b":"code","593ca8ff":"code","9f926e3b":"code","e2cc8ab7":"code","abebe852":"code","4f4f24f1":"code","4d51b983":"code","75d54f71":"code","f3eaa007":"code","b6d7b4b3":"code","7c9d314d":"code","9f064cce":"code","6aced4e6":"markdown","29b410b0":"markdown","64adeaed":"markdown","e0b22a70":"markdown","61f17a66":"markdown"},"source":{"1ebc1952":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n!pip install tensorflow==2.0.0-alpha0\n!pip install tensorflow_datasets\nimport tensorflow as tf\nimport tensorflow_probability as tfp\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom sklearn.datasets import make_regression\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","f07274f1":"data_train = pd.read_csv('\/kaggle\/input\/digit-recognizer\/train.csv')\ndata_train.head()\ndata_test = pd.read_csv('\/kaggle\/input\/digit-recognizer\/test.csv')","0c629e16":"x_train = data_train.drop(['label'], axis = 1).values \/ 255\ny_train = data_train['label'].values\nx_test = data_test.values \/ 255","98b1098d":"model = tf.keras.models.Sequential([\n    tf.keras.layers.Dense(392, activation = 'relu'),\n    tf.keras.layers.Dense(196, activation = 'relu'),\n    tf.keras.layers.Dense(392, activation = 'relu'),\n    tf.keras.layers.Dense(784, activation = 'sigmoid')\n])\nmodel.compile(optimizer = 'adam', loss = 'mse')","8f2efcff":"model.fit(x_train, x_train, epochs = 5, verbose = 0)","d7d381c0":"ix = np.random.randint(0, len(x_test), 2)\ntest = x_test[ix]\ntest_outcome = model.predict(test)\nf, ax = plt.subplots(2, 2, figsize = (5, 5))\nax[0, 0].imshow(test[0].reshape(28, 28))\nax[0, 0].set_title('original')\nax[1, 0].imshow(test[1].reshape(28, 28))\nax[0, 1].imshow(test_outcome[0].reshape(28, 28))\nax[0, 1].set_title('reconstructed')\nax[1, 1].imshow(test_outcome[1].reshape(28, 28))","8569e36b":"encoded_size = 16\nprior = tfp.distributions.Independent(tfp.distributions.Normal(loc=tf.zeros(encoded_size), scale=1),\n                        reinterpreted_batch_ndims=1)\nencoder = tf.keras.Sequential([\n    tf.keras.layers.InputLayer(input_shape = (784, )),\n    tf.keras.layers.Dense(392, activation = 'relu'),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.Dense(196, activation = 'relu'),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.Dense(tfp.layers.MultivariateNormalTriL.params_size(encoded_size),\n               activation=None),\n    tfp.layers.MultivariateNormalTriL(\n        encoded_size,\n        activity_regularizer=tfp.layers.KLDivergenceRegularizer(prior)),\n])\nencoder.summary()","593ca8ff":"decoder = tf.keras.Sequential([\n    tf.keras.layers.InputLayer(input_shape = [encoded_size]),\n    tf.keras.layers.Dense(392, activation = 'relu'),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.Dense(784, activation = 'relu'),\n    tf.keras.layers.BatchNormalization(),\n    tfp.layers.IndependentBernoulli((784, ), tfp.distributions.Bernoulli.logits),\n])\ndecoder.summary()","9f926e3b":"vae = tf.keras.Model(inputs = encoder.inputs, outputs = decoder(encoder.outputs[0]))\nnegloglik = lambda x, rv_x: -rv_x.log_prob(x)\nvae.compile(optimizer = tf.optimizers.Adam(learning_rate=1e-4),\n            loss = negloglik)\nvae.fit(x_train, x_train, epochs = 20, verbose = 0)\nvae.summary()","e2cc8ab7":"ix = np.random.randint(0, len(x_test), 2)\ntest = x_test[ix]\ntest_outcome = vae(test)\nf, ax = plt.subplots(2, 3, figsize = (10, 5))\nax[0, 0].imshow(test[0].reshape(28, 28))\nax[0, 0].set_title('original')\nax[1, 0].imshow(test[1].reshape(28, 28))\n\nax[0, 1].imshow(np.array(test_outcome.mode()[0]).reshape(28, 28))\nax[0, 1].set_title('reconstructed mode')\nax[1, 1].imshow(np.array(test_outcome.mode()[1]).reshape(28, 28))\n\nax[0, 2].imshow(np.array(test_outcome.mean()[0]).reshape(28, 28))\nax[0, 2].set_title('reconstructed mean')\nax[1, 2].imshow(np.array(test_outcome.mean()[1]).reshape(28, 28))","abebe852":"z = prior.sample(10)\nxtilde = decoder(z)\nf, ax = plt.subplots(1, 10, figsize = (20, 10))\nfor i in range(10):\n    ax[i].imshow(np.array(xtilde.sample()[i]).reshape(28, 28))\n    ax[i].axis('off')\nplt.show()\n\nf, ax = plt.subplots(1, 10, figsize = (20, 10))\nfor i in range(10):\n    ax[i].imshow(np.array(xtilde.mode()[i]).reshape(28, 28))\n    ax[i].axis('off')\nplt.show()\n\nf, ax = plt.subplots(1, 10, figsize = (20, 10))\nfor i in range(10):\n    ax[i].imshow(np.array(xtilde.mean()[i]).reshape(28, 28))\n    ax[i].axis('off')\nplt.show()","4f4f24f1":"x = np.arange(0, 5, 0.01)\ny = 1 - np.exp(- x) + np.random.rand(x.shape[0]) * np.sqrt(x)\nplt.scatter(x, y)","4d51b983":"r_model = tf.keras.models.Sequential([\n    tf.keras.layers.Dense(16, activation = 'relu'),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.Dense(32, activation = 'relu'),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.Dense(1, activation = None)\n])\nr_model.compile(optimizer = 'adam', loss = 'mse')","75d54f71":"r_model.fit(x.reshape(-1, 1), y.reshape(-1, 1), epochs = 100, verbose = 0)","f3eaa007":"y_pred = r_model.predict(x.reshape(-1, 1))\nplt.scatter(x, y)\nplt.scatter(x, y_pred.reshape(-1))","b6d7b4b3":"r_model = tf.keras.models.Sequential([\n    tf.keras.layers.Dense(16, activation = 'relu'),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.Dense(32, activation = 'relu'),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.Dense(2,),\n    tfp.layers.DistributionLambda(\n      lambda t: tfp.distributions.Normal(loc=t[..., :1],\n                           scale=1e-3 + tf.math.softplus(0.05 * t[..., 1:])))\n])","7c9d314d":"negloglik = lambda x, rv_x: -rv_x.log_prob(x)\nr_model.compile(optimizer = tf.optimizers.Adam(learning_rate=0.05), loss = negloglik)\nr_model.fit(x.reshape(-1, 1), y.reshape(-1, 1), epochs = 100, verbose = 0)\nr_model.summary()","9f064cce":"yhat = r_model(x.reshape(-1, 1))\nmean = yhat.mean()\nstddev = yhat.stddev()\nmean_plus_stddev = mean - 1.96 * stddev\nmean_minus_stddev = mean + 1.96 * stddev\n\nplt.scatter(x, y)\nplt.plot(x, np.array(mean).reshape(-1))\nplt.plot(x, np.array(mean_plus_stddev).reshape(-1))\nplt.plot(x, np.array(mean_minus_stddev).reshape(-1))","6aced4e6":"Now, we use priors to generate never-seen-before digits:","29b410b0":"Bravo!  \nSince we now know how tensorflow probability works, let's jump into a toy dataset and see it predicting 95% confidence interval in a regression problem.","64adeaed":"Now we implement an autoencoder. It is a model for feature extraction. The latent space contains the compressed representation of the original data.","e0b22a70":"We now have an autoencoder that knows how to reconstruct hand-written digits, even though it have not seen the digit before. However, the reconstructed image will always look the same as the original, making it useless as a generative model.  \nNext, we will try to understand the core concept of variational autoencoder by writing one.","61f17a66":"This notebook helps you to get started with Tensorflow Probability, a module for probabilistic reasoning within the DL framework.  \nWhy do we want to think probabilistically?  The simplest answer is that we want to have the ability to make prediction with uncertainty estimation. This would help us understand the data better and to make decision based on probabilistic reasoning.  \nAnother application for Tensorflow Probability is in generative model. In models such as GANs or Variational Autoencoder, we need the probability module to have random initialization.  \nIn this notebook, we will start with an autoencoder. Next, we will build a variational autoencoder that take advantages of Tensorflow Probability model. Lastly, we will make a probablistic regression model."}}