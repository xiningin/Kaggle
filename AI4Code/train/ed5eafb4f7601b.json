{"cell_type":{"7c4a8e45":"code","57b677b8":"code","a40ea665":"code","bdf12e72":"code","d428bc82":"code","2ed25a17":"code","e9383055":"code","07fecf2a":"code","0620b151":"code","0056f7ad":"code","b90f55c1":"code","20ba558c":"code","59f6dea3":"markdown","6e95074a":"markdown","dd4c546e":"markdown"},"source":{"7c4a8e45":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nSTART_TAG = \"<START>\"\nSTOP_TAG = \"<STOP>\"\nEMBEDDING_DIM = 5\nHIDDEN_DIM = 4","57b677b8":"def prepare_sequence(seq, to_ix):\n    idxs = [to_ix[w] for w in seq]\n    \n    return torch.tensor(idxs, dtype=torch.long)","a40ea665":"class BiLSTM_CRF(nn.Module):\n\n    def __init__(self, vocab_size, tag_to_ix, embedding_dim, hidden_dim):\n        super(BiLSTM_CRF, self).__init__()\n        self.embedding_dim = embedding_dim\n        self.hidden_dim = hidden_dim\n        self.vocab_size = vocab_size\n        self.tag_to_ix = tag_to_ix\n        self.tagset_size = len(tag_to_ix)\n\n        self.word_embeds = nn.Embedding(vocab_size, embedding_dim)\n        self.lstm = nn.LSTM(embedding_dim, hidden_dim \/\/ 2,\n                            num_layers=1, bidirectional=True)\n\n        # \u5c06LSTM\u7684\u8f93\u51fa\u6620\u5c04\u5230\u6807\u8bb0\u7a7a\u95f4\n        self.hidden2tag = nn.Linear(hidden_dim, self.tagset_size)\n\n        # \u8f6c\u79fb\u77e9\u9635\n        self.transitions = nn.Parameter(\n            torch.randn(self.tagset_size, self.tagset_size))\n  \n        self.transitions.data[tag_to_ix[START_TAG], :] = -10000\n        self.transitions.data[:, tag_to_ix[STOP_TAG]] = -10000\n        \n        # \u521d\u59cb\u5316\u9690\u85cf\u5c42\n        self.hidden = self.init_hidden()\n\n    def init_hidden(self):\n        # \u521d\u59cb\u5316\u9690\u85cf\u5c42\u4e2d\u7684\u53c2\u6570\n        return (torch.randn(2, 1, self.hidden_dim \/\/ 2),\n                torch.randn(2, 1, self.hidden_dim \/\/ 2))\n\n    def _forward_alg(self, feats):\n        # \u524d\u5411\u8ba1\u7b97\n        init_alphas = torch.full([self.tagset_size], -10000.)\n        \n        # \u521d\u59cb\u5316\n        init_alphas[self.tag_to_ix[START_TAG]] = 0.\n\n        forward_var_list=[]\n        forward_var_list.append(init_alphas)\n        for feat_index in range(feats.shape[0]):\n            gamar_r_l = torch.stack([forward_var_list[feat_index]] * feats.shape[1])\n            t_r1_k = torch.unsqueeze(feats[feat_index],0).transpose(0,1)\n            aa = gamar_r_l + t_r1_k + self.transitions\n            forward_var_list.append(torch.logsumexp(aa,dim=1))\n       \n        terminal_var = forward_var_list[-1] + self.transitions[self.tag_to_ix[STOP_TAG]]\n        terminal_var = torch.unsqueeze(terminal_var,0)\n        alpha = torch.logsumexp(terminal_var, dim=1)[0]\n        \n        return alpha\n\n    def _get_lstm_features(self, sentence):\n        # \u5f97\u5230lstm\u9884\u6d4b\u5f97\u5230\u7684\u6807\u7b7e\n        \n        self.hidden = self.init_hidden()\n        embeds = self.word_embeds(sentence).view(len(sentence), 1, -1)\n        \n        lstm_out, self.hidden = self.lstm(embeds, self.hidden)\n        lstm_out = lstm_out.view(len(sentence), self.hidden_dim)\n        lstm_feats = self.hidden2tag(lstm_out)\n        \n        # print('LSTM feature', lstm_feats)\n        return lstm_feats\n\n    def _score_sentence(self, feats, tags):\n        # \u5bf9\u7ed9\u5b9atag\u5e8f\u5217\u6253\u5206\n        score = torch.zeros(1)\n        tags = torch.cat([torch.tensor([self.tag_to_ix[START_TAG]], dtype=torch.long), tags])\n        \n        for i, feat in enumerate(feats):\n            score = score + \\\n                self.transitions[tags[i + 1], tags[i]] + feat[tags[i + 1]]\n        score = score + self.transitions[self.tag_to_ix[STOP_TAG], tags[-1]]\n        \n        return score\n\n    def _viterbi_decode(self, feats):\n        # \u5f97\u5230\u6700\u4f73\u9884\u6d4b\u5e8f\u5217\n        backpointers = []\n        \n        # \u521d\u59cb\u5316viterbi\u53d8\u91cf\n        init_vvars = torch.full((1, self.tagset_size), -10000.)\n        init_vvars[0][self.tag_to_ix[START_TAG]] = 0\n\n        forward_var_list = []\n        forward_var_list.append(init_vvars)\n\n        for feat_index in range(feats.shape[0]):\n            gamar_r_l = torch.stack([forward_var_list[feat_index]] * feats.shape[1])\n            gamar_r_l = torch.squeeze(gamar_r_l)\n            next_tag_var = gamar_r_l + self.transitions\n            viterbivars_t,bptrs_t = torch.max(next_tag_var,dim=1)\n\n            t_r1_k = torch.unsqueeze(feats[feat_index], 0)\n            forward_var_new = torch.unsqueeze(viterbivars_t,0) + t_r1_k\n\n            forward_var_list.append(forward_var_new)\n            backpointers.append(bptrs_t.tolist())\n\n        # \u8ba1\u7b97\u77e5\u9053stop_tag\n        terminal_var = forward_var_list[-1] + self.transitions[self.tag_to_ix[STOP_TAG]]\n        best_tag_id = torch.argmax(terminal_var).tolist()\n        path_score = terminal_var[0][best_tag_id]\n\n        # \u56de\u6eaf\u5f97\u5230\u6700\u4f73\u8def\u5f84\n        best_path = [best_tag_id]\n        for bptrs_t in reversed(backpointers):\n            best_tag_id = bptrs_t[best_tag_id]\n            best_path.append(best_tag_id)\n            \n        # \u5f39\u51fa\u5f00\u59cb\u6807\u8bb0\n        start = best_path.pop()\n        assert start == self.tag_to_ix[START_TAG]  \n        best_path.reverse()\n        \n        return path_score, best_path\n\n\n    def neg_log_likelihood(self, sentence, tags):\n        feats = self._get_lstm_features(sentence)\n        forward_score = self._forward_alg(feats)\n        gold_score = self._score_sentence(feats, tags)\n        return forward_score - gold_score\n\n    def forward(self, sentence):  \n        # \u771f\u6b63\u7684forward\u51fd\u6570\n        \n        # \u5f97\u5230LSTM\u7684\u8f93\u51fa\n        lstm_feats = self._get_lstm_features(sentence)\n\n        # \u5f97\u5230\u8f93\u51fa\u5e8f\u5217\n        score, tag_seq = self._viterbi_decode(lstm_feats)\n        return score, tag_seq","bdf12e72":"START_TAG = \"<START>\"\nSTOP_TAG = \"<STOP>\"\nEMBEDDING_DIM = 300\nHIDDEN_DIM = 256","d428bc82":"# \u8bad\u7ec3\u6570\u636e\ntraining_data = [(\n    \"the wall street journal reported today that apple corporation made money\".split(),\n    \"B I I I O O O B I O O\".split()\n), (\n    \"georgia tech is a university in georgia\".split(),\n    \"B I O O O O B\".split()\n)]","2ed25a17":"# \u7f16\u7801\nword_to_ix = {}\nfor sentence, tags in training_data:\n    for word in sentence:\n        if word not in word_to_ix:\n            word_to_ix[word] = len(word_to_ix)\n\ntag_to_ix = {\"B\": 0, \"I\": 1, \"O\": 2, START_TAG: 3, STOP_TAG: 4}\n\nprint(word_to_ix)\nprint(tag_to_ix)","e9383055":"# \u5b9a\u4e49\u6a21\u578b\u548c\u4f18\u5316\u51fd\u6570\nmodel = BiLSTM_CRF(len(word_to_ix), tag_to_ix, EMBEDDING_DIM, HIDDEN_DIM)\noptimizer = optim.SGD(model.parameters(), lr=0.01, weight_decay=1e-4)\n\nprint(model)","07fecf2a":"# \u6a21\u578b\u8bad\u7ec3\u4e4b\u524d\u7684\u9884\u6d4b\n# with torch.no_grad():\nprecheck_sent = prepare_sequence(training_data[0][0], word_to_ix)\nprecheck_tags = torch.tensor([tag_to_ix[t] for t in training_data[0][1]], dtype=torch.long)\n    \nprint('prediction is : ', model(precheck_sent))\nprint('true is : ', precheck_tags)","0620b151":"num_epochs = 300\nlosses = []\n# \u8bad\u7ec3\nfor epoch in range(num_epochs):\n    for sentence, tags in training_data:\n        # \u6e05\u7a7a\u68af\u5ea6\n        model.zero_grad()\n\n        # \u7f16\u7801\n        sentence_in = prepare_sequence(sentence, word_to_ix)\n        targets = torch.tensor([tag_to_ix[t] for t in tags], dtype=torch.long)\n\n        # \u8ba1\u7b97loss\n        loss = model.neg_log_likelihood(sentence_in, targets)\n        losses.append(loss)\n        \n        # \u53cd\u5411\u4f20\u64ad\n        loss.backward()\n        # \u53c2\u6570\u4f18\u5316\n        optimizer.step()   ","0056f7ad":"train_loss = []\ni = 0\nwhile True:\n    temp = 0\n    for j in range(len(training_data)):\n        if i + j < len(losses):\n            temp += losses[i + j]\n    train_loss.append(temp \/ len(training_data))\n    \n    i += len(training_data)\n    if i > len(losses)-len(training_data):\n        break\nlen(train_loss)","b90f55c1":"import matplotlib.pyplot as plt\n%matplotlib inline\n\n# plt.figure((10, 8))\nplt.xlabel('epoch')\nplt.ylabel('loss')\nplt.plot(range(num_epochs), train_loss)","20ba558c":"# Check predictions after training\nwith torch.no_grad():\n    precheck_sent = prepare_sequence(training_data[0][0], word_to_ix)\n\n    print('predictions is : ', model(precheck_sent))\n    print('true is : ', precheck_tags)  ","59f6dea3":"### \u6a21\u578b\u8bad\u7ec3\u4e4b\u540e\u7684\u9884\u6d4b","6e95074a":"### \u6f14\u793a\n- \u4f7f\u7528\u521d\u59cb\u5316\u4e4b\u540e\u7684\u6a21\u578b\u8fdb\u884c\u4e00\u6b21\u9884\u6d4b\n- \u4f7f\u7528\u7ecf\u8fc7\u8bad\u7ec3\u4e4b\u540e\u7684\u6a21\u578b\u8fdb\u884c\u4e00\u6b21\u9884\u6d4b","dd4c546e":"### \u8bad\u7ec3\u4e4b\u524d\u7684\u9884\u6d4b"}}