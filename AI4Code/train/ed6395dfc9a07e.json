{"cell_type":{"2abe2212":"code","02a00be2":"code","623cc6a4":"code","2e25c95b":"code","16152822":"code","1b1c5b06":"code","c25529b9":"code","7e6d8d1e":"code","ff64ba2d":"code","9f1fafbe":"code","dba3b91d":"code","393a77cd":"code","2bdfea7e":"code","4cb7f8ad":"code","c8e981e5":"code","3798504b":"code","682d8e6f":"code","10e6b11a":"code","2f45fd6f":"code","4fc2700a":"code","02d8ee3a":"code","18b3f279":"code","9cde306c":"code","17497fca":"code","425c345f":"code","ae762263":"code","c2c5c99f":"code","0ca26a35":"code","e7efb68c":"code","50a60989":"code","3a67aadd":"code","669d3510":"code","3adf0e36":"code","8477c997":"code","4dd31d5e":"code","1bbea962":"code","0411c307":"code","078596ec":"code","12b5e5ae":"code","dc2818e5":"code","f2c3e5a7":"code","b06b2bca":"code","18cabd96":"code","9085e5ec":"code","21b3d895":"code","1b3aae68":"code","78a2c202":"code","8e0b90bf":"code","124b5481":"markdown","aa789da2":"markdown","95a8923f":"markdown","dad574bc":"markdown","22ec508f":"markdown","7f70b483":"markdown","6d21e906":"markdown"},"source":{"2abe2212":"!pip install --quiet sparkmagic\n!pip install --quiet pyspark","02a00be2":"!pyspark --version\n","623cc6a4":"#  Increase the width of notebook to display all columns of data\n\nfrom IPython.core.display import display, HTML\ndisplay(HTML(\"<style>.container { width:100% !important; }<\/style>\"))\n","2e25c95b":"# Show multiple outputs of a single cell\n\nfrom IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = \"all\"","16152822":"from pyspark import SparkConf, SparkContext\nfrom pyspark.sql import SparkSession, SQLContext\n\nfrom pyspark.sql.types import *\nimport pyspark.sql.functions as F\nfrom pyspark.sql.functions import udf, col","1b1c5b06":"spark = SparkSession \\\n            .builder.master('local[*]')\\\n            .appName('allstate_claims')\\\n            .getOrCreate()     \nspark\n","c25529b9":"sc = spark.sparkContext\nsc","7e6d8d1e":"sqlContext = SQLContext(spark.sparkContext)\nsqlContext","ff64ba2d":"#  Read, transform and understand the data\n#    pyspark creates a spark-session variable: spark\n\ndf = spark.read.csv(\n                   path = \"..\/input\/allstate-claims-severity\/train.csv\",   \n                   header = True,\n                   inferSchema= True,           # Infer datatypes automatically\n                   sep=\",\"\n                   )","9f1fafbe":"df.take(2)","dba3b91d":"df.show(2)","393a77cd":"df.dtypes\n","2bdfea7e":"# Data shape\ndf.count()     #How many rows?      \ncols = df.columns\nlen(cols)            \nprint(cols)","4cb7f8ad":"df.printSchema()","c8e981e5":"# What is the nature of df:\ntype(df)                     # pyspark.sql.dataframe.DataFrame","3798504b":"#  We also cache the data so that we only read it from disk once.\ndf.cache()\ndf.is_cached            # Checks if df is cached","682d8e6f":"# Show database in parts:\ndf.select(cols[:15]).show(3)\ndf.select(cols[15:25]).show(3)\ndf.select(cols[25:35]).show(3)\ndf.select(cols[35:45]).show(3)\ndf.select(cols[45:]).show(3)","10e6b11a":"df.tail(2)","2f45fd6f":"(df.describe().select(\n                    \"summary\",\n                    F.round(\"cont1\", 4).alias(\"cont1\"),\n                    F.round(\"cont2\", 4).alias(\"cont2\"),\n                    F.round(\"cont3\", 4).alias(\"cont3\"),\n                    F.round(\"cont4\", 4).alias(\"cont4\"),\n                    F.round(\"cont5\", 4).alias(\"cont5\"),\n                    F.round(\"cont6\", 4).alias(\"cont6\"),\n                    F.round(\"cont7\", 4).alias(\"cont7\"),\n                    F.round(\"cont13\", 4).alias(\"cont13\"),\n                    F.round(\"cont14\", 4).alias(\"cont14\"),\n                    F.round(\"loss\", 4).alias(\"loss\"))\n                    .show())","4fc2700a":"# Adjust the values of `medianHouseValue`\ndf = df.withColumn(\"loss\", col(\"loss\")\/1000)","02d8ee3a":"df.show(2)","18b3f279":"#  Which columns to drop?\n\ncolumns_to_drop = ['id']\ndf= df.drop(*columns_to_drop)","9cde306c":"df.dtypes","17497fca":"from pyspark.sql.functions import col\n\ndf.select(col(\"loss\")).show(5)\ndf.select(\"loss\").show(5)","425c345f":"\ndf = df.withColumnRenamed('loss', 'label')\nprint(df.columns)","ae762263":"# setting random seed for notebook reproducability\nimport pandas as pd\nimport numpy as np\n\nrnd_seed=23\nnp.random.seed=rnd_seed\nnp.random.set_state=rnd_seed","c2c5c99f":"# Data splitting  #\n\n# Split the dataset randomly into 70% for training and 30% for testing.\ntrain, validation = df.randomSplit([0.7, 0.3],seed=rnd_seed)\n\n\nprint(train.count()\/df.count())\nprint(validation.count()\/df.count())\n# Split the dataset randomly into 70% for training and 30% for testing.\n\n#splits = df.randomSplit([0.7, 0.3])\n#train = splits[0]\n#test = splits[1].withColumnRenamed(\"loss\", \"Label\")\n#train_rows = train.count()\n#test_rows = test.count()\n#print(\"Training Rows:\", train_rows, \" Testing Rows:\", test_rows)","0ca26a35":"train.count()","e7efb68c":"train.explain(extended=True)\n","50a60989":"#train.checkpoint()\ntrain = spark.createDataFrame(train.rdd, schema=train.schema)\n","3a67aadd":"# Now, check the size of your DAG\n\n# Displays the  length of physical plan\ntrain.explain(extended=True)\n","669d3510":"validation.explain(extended=True)","3adf0e36":"#validation.checkpoint()\nvalidation = spark.createDataFrame(validation.rdd, schema=validation.schema)","8477c997":"validation.explain(extended=True)\n","4dd31d5e":"#  Encode 'string' column to index-column. \n#     Indexing begins from 0.\nfrom pyspark.ml.feature import StringIndexer\n\n# List all categorical columns and create objects to StringIndex all these categorical columns\n\ncat_columns = [ c[0] for c in df.dtypes if c[1] == \"string\"]\n\n\nstringindexer_stages = [ StringIndexer(inputCol=c, outputCol='stringindexed_' + c) for c in cat_columns]\nstringindexer_stages","1bbea962":"len(stringindexer_stages)","0411c307":"# Prepare (one) object to OneHotEncode categorical columns (received from above)\n#  OHE an indexed column after StringIndexing and create one another column\nfrom pyspark.ml.feature import OneHotEncoder\n\nin_cols = ['stringindexed_' + c for c in cat_columns]\nohe_cols = ['onehotencoded_' + c  for c in cat_columns]\nonehotencoder_stages = [OneHotEncoder(inputCols=in_cols, outputCols=ohe_cols)]","078596ec":"# iii)  Prepare a (one) list of all numerical and OneHotEncoded columns. Exclude 'loss' column from this list.\n\n# Unlike in other languages, in spark\n#       type-classes are to be separateky imported\n#       They are not part of core classes or modules\nfrom pyspark.sql.types import DoubleType\n\ndouble_cols =   [  i[0] for i in df.dtypes if i[1] == 'double' ] \n\ndouble_cols.remove('label')  \n\ndouble_cols\n\n","12b5e5ae":"#  Create a combined list of double + ohe_cols\n\nfeaturesCols = double_cols + ohe_cols\nprint(featuresCols)\nlen(featuresCols)\n","dc2818e5":"# Create a VectorAssembler object to assemble all the columns as above\nfrom pyspark.ml.feature import VectorAssembler\n#   Create an instance of VectorAssembler class.\n#          This object will be used to assemble all featureCols\n#          (a list of columns) into one column with name\n#           'rawFeatures'\n\nvectorassembler = VectorAssembler(\n                                  inputCols=featuresCols,\n                                  outputCol=\"rawFeatures\"\n                                 )","f2c3e5a7":"from pyspark.ml import Pipeline\nfrom pyspark.ml.regression import GBTRegressor\nfrom pyspark.ml.feature import VectorIndexer\nfrom pyspark.ml.evaluation import RegressionEvaluator","b06b2bca":"# Create an object to perform modeling using GBTRegressor\n\ngbt = GBTRegressor(labelCol=\"label\",featuresCol=\"rawFeatures\",predictionCol='predlabel', maxIter=10)","18cabd96":"# 9.2 Create pipeline model\npipeline = Pipeline(stages=[                        \\\n                             *stringindexer_stages, \\\n                             *onehotencoder_stages, \\\n                             vectorassembler,       \\\n                             gbt                    \\\n                           ]                        \\\n                   )","9085e5ec":"#  Run the pipeline\nimport os, time\n\nstart = time.time()\npipelineModel = pipeline.fit(train)\nend = time.time()\n(end - start)\/60           \n","21b3d895":"# Make predictions on validation data.\n#      Note it is NOT pipelineModel.predict()\n\nprediction = pipelineModel.transform(validation)\npredicted = prediction.select(\"predlabel\", \"label\")\n#predicted.show(100, truncate=False)","1b3aae68":"#  Show 10 columns including predicted column\npredicted.show(10, truncate=False)\n","78a2c202":"predicted","8e0b90bf":"\n# 10.3 Evaluate results\n# Create evaluator object.  class is, as:\n\nfrom pyspark.ml.evaluation import RegressionEvaluator\nfrom pyspark.mllib.evaluation import RegressionMetrics\n\n\nevaluator = RegressionEvaluator(predictionCol='predlabel', labelCol='label', metricName='rmse')\n\nprint(\"RMSE: {0}\".format(evaluator.evaluate(predicted)))","124b5481":"Summary Statistics:\nSpark DataFrames include some built-in functions for statistical processing. The describe() function performs summary statistics calculations on all numeric columns and returns them as a DataFrame.","aa789da2":"## Creating transformation objects","95a8923f":"Look at the minimum and maximum values of all the (numerical) attributes. We see that multiple attributes have a wide range of values: we will need to normalize our dataset.","dad574bc":"As number of processes of our trainingData is too large. \nAs a result, the DAG size (Directed Acyclical Graph - a logical flow of operations constructed by spark) for our data becomes too large to handle and we may end up getting the following error -\nPy4JJavaError: An error occurred while calling o903.fit.\n\nTherefore, By converting our train data dataFrame into RDD (Resilient Distributed Dataset) and then back to DataFrame again will also shrink the DAG considerably.","22ec508f":"#Preprocessing The Target Values:\n\nFirst, let's start with the loss column, our dependent variable. To facilitate our working with the target values, we will express the house values in units of 1000. That means that a target such as 3037.3377 should become 3.037:","7f70b483":"\n> ## Below are the steps to transfer files from local drive to Hadoop Ecosystem:\n    \ncd ~\n\nhdfs dfs -rm -r hdfs:\/\/localhost:9000\/user\/ashok\/data_files\/prabhash_assignment_allstate_claims\n\nhdfs dfs -mkdir -p hdfs:\/\/localhost:9000\/user\/ashok\/data_files\/prabhash_assignment_allstate_claims\n\nTransfer files from local file system\n\ncd ~\n\nhdfs dfs -put \/cdata\/prabhash_assignment_allstate_claims\/train.csv \n\nhdfs:\/\/localhost:9000\/user\/ashok\/data_files\/prabhash_assignment_allstate_claims\n\nhdfs dfs -ls -h hdfs:\/\/localhost:9000\/user\/ashok\/data_files\/prabhash_assignment_allstate_claims","6d21e906":" ## Experiment with SPARK ML (PYSPARK)\n"}}