{"cell_type":{"696152fc":"code","7d77a72c":"code","5fe0123f":"code","cba2e970":"code","49b1513d":"code","df4c5c12":"code","6f39af56":"code","e1c61c5d":"code","1570f8a4":"code","b585889b":"code","65a767b1":"code","d32f8f19":"code","bdb3a21c":"code","8e2089dd":"code","ec80a433":"code","c266590c":"code","e6c2498e":"code","8bc4bb66":"code","f2badf3f":"code","c42b95d8":"code","5bd9cc69":"code","bdf71d07":"code","2f8d4a5e":"code","cf6611c4":"code","52523fa7":"code","fb6805e4":"code","0a80a428":"code","1bb8a708":"code","19ef8fbb":"code","5b3acac1":"code","5fbdb2e6":"code","57d468ce":"code","838398ec":"code","d585575c":"code","e3b9118d":"code","52540a6c":"code","62803199":"code","ff91c848":"code","3ac86be4":"code","aa43a856":"code","9770b421":"code","37b148ad":"code","840ee6cb":"markdown","af0ca31a":"markdown","e79bd231":"markdown","df92650b":"markdown","7cdb358a":"markdown","2f3d1961":"markdown","9dce5e5a":"markdown","7f4f44e3":"markdown","b5da1d1e":"markdown","3959bad9":"markdown","9ba9d20c":"markdown"},"source":{"696152fc":"# Regular imports\nimport numpy as np\nimport pandas as pd\nimport sklearn as sk\nimport lightgbm as lhgbm\n\n#For Visualizations\nimport pandas_profiling as pp\nimport matplotlib as mpl\nimport seaborn as sns\n\n#Ensemble Models\nimport xgboost as xgb\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\nsns.set()\nplt.style.use('classic')\npd.set_option('max_columns', None)\n\nfrom sklearn.linear_model import *\n\n\n#To Split Data \nfrom sklearn.model_selection import train_test_split, StratifiedShuffleSplit, GridSearchCV, cross_val_score, KFold\n\n#To Preprocess Data \nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler, OrdinalEncoder, FunctionTransformer, OneHotEncoder, Normalizer\nfrom sklearn.impute import SimpleImputer\nfrom imblearn import FunctionSampler\n\n\n#To Pipeline the process \nfrom sklearn.compose import ColumnTransformer, TransformedTargetRegressor\n#from lineartree import  LinearTreeRegressor, LinearBoostRegressor\nfrom sklearn.pipeline import Pipeline\n\n# used in Utilities\/Functions section\nfrom scipy import stats\n\n## Import Models\nfrom lightgbm import LGBMRegressor\nfrom xgboost import XGBRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom mlxtend.regressor import StackingCVRegressor\n\n\n# Import metrics\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, accuracy_score, r2_score\nfrom xgboost import plot_importance\n\n\n# variables\ntarget_feature = 'target' \nrandom_seed    = 42\nmax_rows_per_class  = 100 ## use high number (300000) to get all data \nfull_run ='Y'  ","7d77a72c":"#Python libraries and their versions used for this problem\nprint('SciKit Learn:',sk.__version__)\nprint('Pandas:',pd.__version__)\nprint('Numpy:',np.__version__)\nprint('Seaborn:',sns.__version__)\nprint('MatPlot Library:', mpl.__version__)\nprint('XG Boost:',xgb.__version__)\nprint('Pandas Profiling:', pp.__version__)\nprint('LightGBM:', lhgbm.__version__)\n","5fe0123f":"#Define Utilities\/Functions\n\n# Function to show essential info about Dataset\ndef ShowEssentialInfo(df):\n    # Check No of rows & columns\n    print(\"\\n\",'*** Shape:',df.shape)\n\n    # Check data\n    print(\"\\n\",'*** Data:',df.head(10))\n    \n    # Check Info about Object types of data\n    print(\"\\n\",'*** Info:')\n    df.info()\n\n    #Count missing values \n    print(\"\\n\",'*** Missing values:')\n    print(df.isnull().sum())\n\n    #Data Statistics\n    print(\"\\n\",'*** Data Statistics:')\n    print(df.describe(include='all'))\n    \n    #Data Skew\n    #print(\"\\n\",'*** Skewness:')\n    #print('#** skewness is a degree of asymmetry observed in a probability distribution that deviates from the symmetrical normal distribution (bell curve) in a given set of data')\n    #print('#** If the skewness is between -0.5 & 0.5, the data are nearly symmetrical.')\n    #print('#** If the skewness is between -1 & -0.5 (negative skewed) or between 0.5 & 1(positive skewed), the data are slightly skewed.')\n    #print('#** If the skewness is lower than -1 (negative skewed) or greater than 1 (positive skewed), the data are extremely skewed.')\n    #print(df.skew(axis=1))\n \n    #Data Kurtosis\n    #print(\"\\n\",'*** Kurtosis:')\n    #print('#** Kurtosis quantifies shape of the distribution and the degree of presence of outliers in the distribution.')\n    #print('# High kurtosis in a data set is an indicator that data has heavy outliers.')\n    #print('# Low kurtosis in a data set is an indicator that data has lack of outliers.')\n    #print('# If kurtosis value + means pointy and \u2014 means flat.')    \n    #print(df.kurt(axis=1))\n \n    \n\ndef treatoutliers(df=None, columns=None, factor=1.5, method='IQR', treatment='cap'):\n\n    for column in columns:\n        if method == 'STD':\n            permissable_std = factor * df[column].std()\n            col_mean = df[column].mean()\n            floor, ceil = col_mean - permissable_std, col_mean + permissable_std\n        elif method == 'IQR':\n            Q1 = df[column].quantile(0.25)\n            Q3 = df[column].quantile(0.75)\n            IQR = Q3 - Q1\n            floor, ceil = Q1 - factor * IQR, Q3 + factor * IQR\n#         print(floor, ceil)\n        if treatment == 'remove':\n            print(treatment, column)\n            df = df[(df[column] >= floor) & (df[column] <= ceil)]\n        elif treatment == 'cap':\n            print(treatment, column)\n            df[column] = df[column].clip(floor, ceil)\n\n    return df\n    \ndef get_sample_dataset(df, categorical_features, max_rows_per_class=1000):\n    rows = []\n    df_sub = pd.DataFrame()\n    for x in categorical_features:\n        for idx,name in enumerate(df[x].value_counts().index.tolist()):\n            nrows = df[x].value_counts()[idx]\n            nsample = min(nrows, max_rows_per_class)\n            data = df.loc[df[x] == name].sample(n=nsample, random_state=random_seed)\n            #print(data.info())\n            df_sub = df_sub.append(data)\n    return df_sub\n\n\ndef log_transform(x):\n    return np.log(x + 1)\n\ndef exp_transform(x):\n    return np.exp(x)\n\n","cba2e970":"# Load the training data\nX_full = pd.read_csv(\"..\/input\/30-days-of-ml\/train.csv\", index_col=0)\nX_test_full = pd.read_csv(\"..\/input\/30-days-of-ml\/test.csv\", index_col=0)\n\nX_all = pd.concat([X_full,X_test_full]) \n","49b1513d":"## Check for Data types & Missing data\nShowEssentialInfo(X_full)","df4c5c12":"ShowEssentialInfo(X_test_full)","6f39af56":"# Identify Numeric \/ Categorical features this would be used to transform features later \n\n# \"Cardinality\" means the number of unique values in a column\n# Select categorical columns with relatively low cardinality (convenient but arbitrary)\ncategorical_features  = [cname for cname in X_full.columns if\n                    X_full[cname].dtype in [\"object\"]]\n\n# Select numerical columns\nnumeric_features  = [cname for cname in X_full.columns if \n                X_full[cname].dtype in ['int64', 'float64']\n                 ]\n\n# Keep selected columns only\nmy_features = categorical_features + numeric_features\n\nprint('categorical_features({}):'.format(len(categorical_features)),categorical_features)\nprint('numeric_features({}):'.format(len(numeric_features)), numeric_features)\nprint('my_features:({})'.format(len(my_features)), my_features)\n","e1c61c5d":"# Identify Numeric \/ Categorical features this would be used to transform features later \n\n# \"Cardinality\" means the number of unique values in a column\n# Select categorical columns with relatively low cardinality (convenient but arbitrary)\ncategorical_features  = [cname for cname in X_full.columns if\n                    X_full[cname].dtype == \"object\"]\n\n# Select numerical columns\nnumeric_features  = [cname for cname in X_full.columns if \n                X_full[cname].dtype in ['int64', 'float64']\n                 ]\n\n# Keep selected columns only\nmy_features = categorical_features + numeric_features\n\nprint('categorical_features({}):'.format(len(categorical_features)),categorical_features)\nprint('numeric_features({}):'.format(len(numeric_features)), numeric_features)\nprint('my_features:({})'.format(len(my_features)), my_features)","1570f8a4":"from statsmodels.stats.outliers_influence import variance_inflation_factor\n\n#print(X_random_subset['cat9'].value_counts(()).index.tolist())\n\n## Feature Selection using variance_inflation_factor\ndef cal_vif(X, thresh=6):\n    output = pd.DataFrame()\n    k =X_vif.shape[1]\n    vif = [variance_inflation_factor(X_vif.values, i) for i in range(k)]\n    for i in range(1,k):\n        print('Iteration No ', i)\n        print(vif)\n        a = int(np.argmax(vif))\n        if(vif[a]<=thresh):\n            print('break')\n            break\n        if(i==1):\n            print('i=1', i)\n            output=X_vif.drop(X_vif.columns[a], axis=1)  \n        elif(i>1):\n            print('i>1', i)\n            output=output.drop(output.columns[a], axis=1)\n        l = len(output.columns)    \n        vif=[variance_inflation_factor(output.values, j) for j in range(l)]\n    return(output)     \n\n\n# creating dummies for categorical_features\n\n# the independent variables set\nX_vif=X_all.copy()\ny_vif=X_vif[target_feature]\nX_vif=X_vif.drop(target_feature,1)\n\n#{'A':0, 'B':1, 'C':2, 'D':3, 'E':5, 'F':6,'G':7,'H':8,'I':9,'J':10,'K':11,'L':12,'M':13,'N':14,'O':15}\nX_vif[categorical_features] = X_vif[categorical_features] = X_vif[categorical_features].applymap(lambda x: ord(x)-65)\n\n#selected_features = pd.DataFrame()\nselected_features = cal_vif(X_vif)\n#selected_features = X_full.head(5)\nselected_features.head()","b585889b":"selected_features['target'] = y_vif","65a767b1":"exclude_columns = set(list(X_full.columns)) - set(list(selected_features.columns))\nprint(exclude_columns)\n","d32f8f19":"ShowEssentialInfo(selected_features)","bdb3a21c":"## Correlations\ncorrelations = X_full.corr()\nf, ax = plt.subplots(figsize=(12, 12))\nsns.heatmap(correlations, square=True, cbar=True, annot=True, vmax=.9);","8e2089dd":"#Correlation with output variable\ncor_target = abs(correlations[target_feature])\n#Selecting highly correlated features\nrelevant_features = cor_target[cor_target>0]\nrelevant_features","ec80a433":"## Data Distribution of numeric features \nX_full[numeric_features].hist(bins=100, figsize=(24,12))","c266590c":"## Verify distribution with log transform \nX_full[numeric_features].hist(bins=100, figsize=(24,12), log = True)","e6c2498e":"## Box Plot for Outliers\nfig = plt.figure(figsize=(18,6))\nsns.boxplot(data=X_full[numeric_features], orient=\"h\", palette=\"Set2\");\nplt.xticks(fontsize= 14)\nplt.title('Box plot of numerical columns', fontsize=16);","8bc4bb66":"# Deal with duplicate rows\nX_full[my_features].drop_duplicates()","f2badf3f":"# Deal with Outliers\n\n#remove outliers from target column \n#for colName in [['target']]:\n    #X_full = treatoutliers(df=X_full,columns=colName, treatment='remove')         \n    \n#Quantile-based Flooring and Capping\nfor colName in [['target','cont0','cont6','cont8']]:\n    X_full = treatoutliers(df=X_full,columns=colName, treatment='cap')      \n    \nShowEssentialInfo(X_full)","c42b95d8":"## Box Plot for Outliers\nfig = plt.figure(figsize=(18,6))\nsns.boxplot(data=X_full[numeric_features], orient=\"h\", palette=\"Set2\");\nplt.xticks(fontsize= 14)\nplt.title('Box plot of numerical columns after handling Outliers', fontsize=16);","5bd9cc69":"# Deal with missing data\n## No Missing data in this dataset :)","bdf71d07":"if full_run == 'N' :\n    X_random_subset = get_sample_dataset(X_full, categorical_features, max_rows_per_class)\nelse:\n    X_random_subset = X_full.copy()\n    \nShowEssentialInfo(X_random_subset)","2f8d4a5e":"# Remove rows with missing target, separate target from predictors\nX_random_subset.dropna(axis=0, subset=[target_feature], inplace=True)\ny = X_random_subset.pop(target_feature)\n\n\n#Prieview features\nShowEssentialInfo(X_random_subset)\n","cf6611c4":"# A scatter plot matrix is a grid (or matrix) of scatter plots used to visualize bivariate relationships between combinations of variables. \n# Each scatter plot in the matrix visualizes the relationship between a pair of variables, allowing many relationships to be explored in one chart.\n\n#pd.plotting.scatter_matrix(X_random_subset[:1000], figsize=(30, 20), alpha=0.2)\n\n","52523fa7":"# Break off validation set from training data\nX_train_full, X_valid_full, y_train, y_valid = train_test_split(X_random_subset, y, \n                                                                train_size=0.9, test_size=0.1,\n                                                                random_state=0)","fb6805e4":"# Identify Numeric \/ Categorical features this would be used to transform features later \n\n# \"Cardinality\" means the number of unique values in a column\n# Select categorical columns with relatively low cardinality (convenient but arbitrary)\ncategorical_features  = [cname for cname in X_full.columns if\n                    X_full[cname].dtype in [\"object\"] and cname not in exclude_columns\n                        ]\n\n# Select numerical columns\nnumeric_features  = [cname for cname in X_full.columns if \n                X_full[cname].dtype in ['int64', 'float64'] and\n                     cname not in exclude_columns\n                 ]\n\n#remove target column from Numeric \/ categorical features\nif numeric_features.count(target_feature) == 1:\n    numeric_features.remove(target_feature)\nelif  categorical_features.count(target_feature) == 1:\n    categorical_features.remove(target_feature)\n\n# Keep selected columns only\nmy_features = categorical_features + numeric_features\n\nprint('categorical_features({}):'.format(len(categorical_features)),categorical_features)\nprint('numeric_features({}):'.format(len(numeric_features)), numeric_features)\nprint('my_features:({})'.format(len(my_features)), my_features)","0a80a428":"X_train = X_train_full[my_features]\nX_valid = X_valid_full[my_features]\nX_test = X_test_full[my_features]","1bb8a708":"ShowEssentialInfo(X_train)","19ef8fbb":"ShowEssentialInfo(X_valid)","5b3acac1":"ShowEssentialInfo(X_test)","5fbdb2e6":"# Define the model Parameters, can be optimized using either Optuna or Grid Search CV\n\n#CPU parameters\n#lgbm_params = {'n_estimators' : 10000,  'max_depth' : 2, 'learning_rate' : 0.1, 'subsample' : 0.95, 'colsample_bytree' : 0.85, 'reg_alpha' : 30.0, 'reg_lambda' : 25.0 , 'num_leaves' : 4, 'max_bin' : 512, 'random_state' : random_seed}\n#xgb_params = {'n_estimators': 10000, 'max_depth': 3, 'learning_rate': 0.03628302216953097, 'gamma': 0, 'min_child_weight': 1, 'subsample': 0.7875490025178415, 'colsample_bytree': 0.11807135201147481, 'reg_alpha': 23.13181079976304, 'reg_lambda': 0.0008746338866473539, 'random_state':random_seed}\n\n#GPU parameters\nlgbm_params = {'n_estimators' : 10000, 'max_depth' : 2, 'learning_rate' : 0.1, 'subsample' : 0.95, 'colsample_bytree' : 0.85, 'reg_alpha' : 30.0, 'reg_lambda' : 25.0 , 'num_leaves' : 4, 'random_state' : random_seed, 'device':'gpu'}\nxgb_params = {'n_estimators': 10000, 'max_depth': 3, 'learning_rate': 0.036, 'gamma': 0, 'min_child_weight': 1, 'subsample': 0.79, 'colsample_bytree': 0.112, 'reg_alpha': 23.132, 'reg_lambda': 0.0009, 'random_state':random_seed, 'tree_method':'gpu_hist', 'predictor':'gpu_predictor'}\n\nmodel = XGBRegressor(**xgb_params) \nlgbm_model = LGBMRegressor(**lgbm_params)\n\n#model = Ridge(alpha=0.05, normalize=True) #  RMSE: 0.7410128061594929\n#model = Lasso(alpha=0.5, normalize=True)  # RMSE: 0.747933770739031\n#model = LinearRegression(normalize=True)  # RMSE: 0.7410032228563359\n#model = DecisionTreeRegressor(max_depth=3) #RMSE: 0.7429294772568634\n#model = RandomForestRegressor(n_estimators=50, random_state=rans, max_depth=3) #RMSE: 0.7421769885950228\n#model = XGBRegressor(n_estimators=500, learning_rate=0.35, n_jobs=-1, random_state=rans, eval_metric ='rmse', objective ='reg:squarederror', booster='gblinear') \n#RMSE: 0.748752632103789\n#model = BaggingRegressor(RandomForestRegressor(n_estimators=50, random_state=rans, max_depth=3), n_estimators=2, random_state=rans, max_samples=0.8, max_features=0.7, bootstrap=True, bootstrap_features=True, n_jobs=-1)  \n#RMSE: 0.742235730626387\n#model = MLPRegressor(activation='tanh', hidden_layer_sizes= (100,3) ,learning_rate='adaptive', solver='adam', max_iter=1000)\n#RMSE: 0.7342802036691075\n\n\n","57d468ce":"%%time\n\n#transformer = FunctionTransformer(log_transform)\n\n# Preprocessing for numerical data\nnumerical_transformer = Pipeline(steps=[\n       ('imputer', SimpleImputer(strategy='mean'))\n       #,('transformer', transformer)\n       ,('RobustScaler', RobustScaler(with_centering=True, with_scaling=True, quantile_range=(25.0, 75.0), copy=True))  \n       ,('scaler', StandardScaler())\n      # ,('scaler', MinMaxScaler())\n      #,('normalizer',  Normalizer())\n])\n\n# Preprocessing for categorical data\ncategorical_transformer = Pipeline(steps=[\n    #('imputer', SimpleImputer(strategy='constant'))\n    ('imputer', SimpleImputer(strategy='most_frequent')) \n    #,('onehot', OneHotEncoder(handle_unknown='ignore'))\n    ,('scaler', OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=-1))\n])\n\n\n# Bundle preprocessing for numerical and categorical data\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numerical_transformer, numeric_features),\n        ('cat', categorical_transformer, categorical_features)\n    ],\n    remainder=\"passthrough\"\n  )\n","838398ec":"#pca = PCA(n_components=50)\n#Bundle preprocessing and modeling code in a pipeline\nxgb_bundle = Pipeline(steps=[('preprocessor', preprocessor),\n                      #('pca',pca),\n                      ('model', model)\n                     ])\n\n\nlgbm_bundle = Pipeline(steps=[('preprocessor', preprocessor),\n                      #('pca',pca),\n                      ('model', lgbm_model)\n                     ])\n\n## TransformedTargetRegressor pipeline Scales Target column during training and inverts the transformations during prediction\nxgbclf = TransformedTargetRegressor(regressor=xgb_bundle, transformer=StandardScaler())\nlgbmclf = TransformedTargetRegressor(regressor=lgbm_bundle, transformer=StandardScaler())\n\n#xgbclf = xgb_bundle\n#lgbmclf = lgbm_bundle","d585575c":"## XGB Regressor\nfrom sklearn import set_config\nset_config(display='diagram')\nxgbclf\n","e3b9118d":"## LGBM Regressor,\nlgbmclf","52540a6c":"nSplits = 10\nkf = KFold(n_splits=nSplits, shuffle=True, random_state=random_seed)","62803199":"%%time\n\n\n#final_model = clf.fit(X_train, y_train)    \n#preds_valid = final_model.predict(X_valid)\n#print MAE, RMSE\n#print('MAE:',mean_absolute_error(y_valid, preds_valid))\n#print('RMSE:',mean_squared_error(y_valid, preds_valid, squared=False))\n    \n\navg_valid_rmse = 0 # initialise variable for the average RMSE \npreds_valid = 0 # initialise variable for the Validation set predicitons \npredictions = 0 # initialise variable for the Test set predicitons\nncount =1\n\n\n#for train_idx, test_idx in kf.split(X_random_subset.iloc[rand_idx]):\nfor train_idx, test_idx in kf.split(X_random_subset[my_features]):    \n    \n    X_train_split, X_valid_split = X_random_subset[my_features].iloc[train_idx], X_random_subset[my_features].iloc[test_idx]\n    y_train_split, y_valid_split = y.iloc[train_idx], y.iloc[test_idx]\n    \n    #if ncount%2==0:\n    if ncount%2==0:\n        print('XGB')\n        final_model = xgbclf.fit(X_train_split, y_train_split)\n    else :\n        print('XGB')\n        final_model = xgbclf.fit(X_train_split, y_train_split)\n        \n    \n    #Predict current Validation fold using the fold model\n    preds_valid_split = final_model.predict(X_valid_split)\n    \n    #Calculate the rmse error for current Validation fold\n    rmse = mean_squared_error(preds_valid_split, y_valid_split, squared=False)\n    print(\"rmse:{}\".format(rmse))\n    \n    #Average of the rmse errors\n    avg_valid_rmse += rmse \/ nSplits\n    \n    #Predict Total Validation & Test sets using the fold Model \n    preds_valid_all = final_model.predict(X_valid)\n    preds_test_all = final_model.predict(X_test)\n\n    #Average of the predictions\n    preds_valid += preds_valid_all \/ nSplits\n    predictions += preds_test_all \/ nSplits\n    ncount += 1\n\nprint(\"Average Validation rmse: {}\".format(avg_valid_rmse))   \n\n","ff91c848":"result_df=pd.DataFrame({'Actual':y_valid, 'Predicted':preds_valid, 'Diff':preds_valid-y_valid})  \nresult_df['Diff'].round().value_counts()","3ac86be4":"## setting plot style\nplt.style.use('fivethirtyeight')\n  \n## plotting residual errors in training data\nplt.scatter(final_model.predict(X_train), final_model.predict(X_train) - y_train,\n            color = \"green\", s = 10, label = 'Train data')\n  \n## plotting residual errors in Validation data\nplt.scatter(preds_valid, preds_valid-y_valid,\n            color = \"blue\", s = 10, label = 'Validation data')\n  \n## plotting line for zero residual error\nplt.hlines(y = 0, xmin = 0, xmax = 50, linewidth = 2)\n  \n## plotting legend\nplt.legend(loc = 'upper right')\n  \n## plot title\nplt.title(\"Residual errors\")\n  \n## method call for showing the plot\nplt.show()","aa43a856":"final_model.get_params","9770b421":"#Compare results\nplt.plot(y_valid.values, label='Actual')\nplt.plot(preds_valid, label='Predicted')\nplt.ylabel('Target')\n\nplt.legend()\nplt.show()","37b148ad":"# Use the model to generate predictions\n#predictions = final_model.predict(X_test)\n\n# Save the predictions to a CSV file\noutput = pd.DataFrame({'Id': X_test.index,\n                       'target': predictions})\noutput.to_csv('submission.csv', index=False)","840ee6cb":"No Null values in Data","af0ca31a":"# Step 2: Load the data\n\nNext, we'll load the training and test data.  \n\nWe set `index_col=0` in the code cell below to use the `id` column to index the DataFrame.","e79bd231":"# Step 3: Prepare the data\n\n","df92650b":"Looks like few outliers in Cont0, Cont6, Cont8, target columns.\nLets check the ouliers in  target column now.","7cdb358a":"The next code cell separates the target (which we assign to `y`) from the training features.","2f3d1961":"# Step 4: Train a model\n\nNow that the data is prepared, the next step is to train a model.  \n\nLets fit a XG Boost Regression model to the data.","9dce5e5a":"# Step 1: Import helpful libraries","7f4f44e3":"\n# Step 5: Submit to the competition\n\nWe'll begin by using the trained model to generate predictions, which we'll save to a CSV file.","b5da1d1e":"No Duplicate Rows in the data set...","3959bad9":"All features are weakly correlated to target feature","9ba9d20c":"The above distribution looks good after log transformation"}}