{"cell_type":{"2bfb0595":"code","6bcdb44f":"code","9fb1ddde":"code","dbfc4715":"code","feef8dac":"code","2dc19ab2":"code","f5e8cce9":"code","3b8e8c32":"code","7209b1c7":"code","431afc8e":"code","56663684":"code","ffd1f297":"code","8fc1fcf0":"code","7eb6e303":"code","78958201":"code","f446e181":"code","74a4a814":"code","8d2d004d":"code","0582f3e8":"code","6ce549fe":"code","2866f2fb":"code","036bcb21":"code","a4068c07":"code","f56abfd2":"code","a13d4f95":"code","bfebebfd":"code","f472f49e":"code","29c83b22":"code","b68df413":"code","18dcc21b":"code","2d932685":"code","a529b82a":"code","b6fe1fea":"code","9d1e254c":"code","985ce78b":"code","fea530b0":"code","677c0d21":"code","ffd3ea7b":"code","9e19b971":"code","a89045ff":"code","04dbe207":"code","29e35687":"code","7e667279":"code","4824859f":"code","eec73567":"code","7dad0662":"code","bc1c2281":"code","c252027c":"code","dd4ebf1e":"code","e18b8523":"code","8d934cf8":"markdown","3449be0b":"markdown","47dd37ce":"markdown","d9862ce7":"markdown","20e83dba":"markdown","4fa2d0e4":"markdown","7c6abc98":"markdown","7a30589f":"markdown","03575852":"markdown","36fe1265":"markdown","64796e16":"markdown","8eb8fd7b":"markdown","ff5e013d":"markdown","1ea89663":"markdown","f6bedbdb":"markdown","3dfd479d":"markdown","547bbf47":"markdown","510a28d7":"markdown","c474195c":"markdown","b6ca1806":"markdown","5010e272":"markdown","b704cbaf":"markdown","33e72cf3":"markdown","3feff821":"markdown"},"source":{"2bfb0595":"#!pip install --no-cache-dir --upgrade comet_ml\n#from comet_ml import Experiment\n#experiment = Experiment(api_key='', project_name='jhb-ss2-classification', workspace='carynpialat')","6bcdb44f":"!pip install emot","9fb1ddde":"# Packages\nimport numpy as np\nimport pandas as pd\nimport re\nimport spacy\nimport nltk\nfrom emot.emo_unicode import UNICODE_EMO, EMOTICONS\nfrom io import StringIO\nfrom html.parser import HTMLParser\nimport string\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.tokenize import TweetTokenizer\nfrom nltk.corpus import stopwords\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\nfrom sklearn.utils import resample\n\n# Visualisations\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom wordcloud import WordCloud\n\n# Vectorisers\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n# Training\nfrom sklearn.model_selection import train_test_split, KFold, cross_val_score, GridSearchCV\n\n# Classification models\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.naive_bayes import MultinomialNB, ComplementNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n\n# Model evaluation\nfrom sklearn.metrics import confusion_matrix, classification_report, f1_score, make_scorer\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score\n\n# Save models\nimport pickle","dbfc4715":"nlp = spacy.load('en_core_web_sm')","feef8dac":"nltk.download('vader_lexicon')","2dc19ab2":"# Training data\ndf = pd.read_csv('..\/input\/climate-change-belief-analysis\/train.csv')\ndf.set_index('tweetid', inplace=True)\n\n#Test data\ndf_test = pd.read_csv('..\/input\/climate-change-belief-analysis\/test.csv')\ndf_test.set_index('tweetid', inplace=True)","f5e8cce9":"# View the training data\ndf.head()","3b8e8c32":"# View the training data\ndf_test.head()","7209b1c7":"# More details on the rows, columns and content of the datasets\nprint('Training data' + ('\\n'))\nprint(df.info())\nprint('\\n' + 'Shape of the training data: {}' .format(df.shape))\nprint('Number of unique tweets in the training data: {}'.format(len(set(df['message']))))\nprint('Number of missing values in the training data:' + '\\n' + '{}' .format(df.isnull().sum()))\nprint('\\n\\n' + 'Test data' + '\\n')\nprint(df_test.info())\nprint('\\n' + 'Shape of the test data: {}' .format(df_test.shape))\nprint('Number of unique tweets in the test data: {}'.format(len(set(df_test['message']))))\nprint('Number of missing values in the test data:' + '\\n' + '{}' .format(df_test.isnull().sum()))","431afc8e":"# Remove html tags from messages\nfrom io import StringIO\nfrom html.parser import HTMLParser\n\nclass MLStripper(HTMLParser):\n    def __init__(self):\n        super().__init__()\n        self.reset()\n        self.strict = False\n        self.convert_charrefs= True\n        self.text = StringIO()\n    def handle_data(self, d):\n        self.text.write(d)\n    def get_data(self):\n        return self.text.getvalue()\n\ndef strip_tags(html):\n    s = MLStripper()\n    s.feed(html)\n    return s.get_data()\n\ndf['msg_clean'] = df['message'].apply(strip_tags)\ndf_test['msg_clean'] = df_test['message'].apply(strip_tags)","56663684":"# Remove urls, new lines and hashtags from training data\nre_pattern = [r'((http|https)\\:\\\/\\\/)?[a-zA-Z0-9\\.\\\/\\?\\:@\\-_=#]+\\.([a-zA-Z]){2,6}([a-zA-Z0-9\\.\\&\\\/\\?\\:@\\-_=#])*']\ndf['msg_clean'] = df['msg_clean'].replace(to_replace = re_pattern, \n                                          value = r'website', regex = True)\ndf['msg_clean'] = df['msg_clean'].replace(to_replace = r'\\n', \n                                          value = r'', regex = True)\ndf['msg_clean'] = df['msg_clean'].replace(to_replace = r'@', \n                                          value = r'twitterhandle ', regex = True)\n\n# Remove urls, new lines and hashtags from test data\ndf_test['msg_clean'] = df_test['msg_clean'].replace(to_replace = re_pattern, \n                                                    value = r'website', regex = True)\ndf_test['msg_clean'] = df_test['msg_clean'].replace(to_replace = r'\\n', \n                                                    value = r'', regex = True)\ndf_test['msg_clean'] = df_test['msg_clean'].replace(to_replace = r'@', \n                                                    value = r'twitterhandle ', regex = True)","ffd1f297":"# Converting the emoticons into text \n!pip install emot --upgrade\nfrom emot.emo_unicode import UNICODE_EMO, EMOTICONS\ndef convert_emojis(text):\n    for emot in UNICODE_EMO:\n        text = text.replace(emot, \"_\".join(UNICODE_EMO[emot].replace(\",\",\"\").\n                                           replace(\":\",\"\").split()))\n        return text\n# Function which will change the emoji to a string\ndef emoji_remove(string):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"  # \n                           u\"\\U000024C2-\\U0001F251\"  # \n                           \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'', string)\n\ndf['msg_clean'] = df['msg_clean'].apply(convert_emojis)\ndf['msg_clean'] = df['msg_clean'].apply(emoji_remove)\n\ndf_test['msg_clean'] = df_test['msg_clean'].apply(convert_emojis)\ndf_test['msg_clean'] = df_test['msg_clean'].apply(emoji_remove)","8fc1fcf0":"# Tokenise tweets\ntokenizer = TweetTokenizer()\ndf['msg_clean'] = df['msg_clean'].apply(tokenizer.tokenize)\n\ndf_test['msg_clean'] = df_test['msg_clean'].apply(tokenizer.tokenize)","7eb6e303":"# Replace contractions with full form of the words\ncontractions = {\n\"ain't\": \"am not \/ are not\",\n\"aren't\": \"are not\",\n\"can't\": \"cannot\",\n\"can't've\": \"cannot have\",\n\"'cause\": \"because\",\n\"could've\": \"could have\",\n\"couldn't\": \"could not\",\n\"couldn't've\": \"could not have\",\n\"didn't\": \"did not\",\n\"doesn't\": \"does not\",\n\"don't\": \"do not\",\n\"hadn't\": \"had not\",\n\"hadn't've\": \"had not have\",\n\"hasn't\": \"has not\",\n\"haven't\": \"have not\",\n\"he'd\": \"he had \/ he would\",\n\"he'd've\": \"he would have\",\n\"he'll\": \"he will\",\n\"he'll've\": \"he will have\",\n\"he's\": \"he has \/ he is\",\n\"how'd\": \"how did\",\n\"how'd'y\": \"how do you\",\n\"how'll\": \"how will\",\n\"how's\": \"how has \/ how is\",\n\"i'd\": \"I had \/ I would\",\n\"i'd've\": \"I would have\",\n\"i'll\": \"I will\",\n\"i'll've\": \"I will have\",\n\"i'm\": \"I am\",\n\"i've\": \"I have\",\n\"isn't\": \"is not\",\n\"it'd\": \"it had \/ it would\",\n\"it'd've\": \"it would have\",\n\"it'll\": \"it will\",\n\"it'll've\": \"it will have\",\n\"it's\": \"it has \/ it is\",\n\"let's\": \"let us\",\n\"ma'am\": \"madam\",\n\"mayn't\": \"may not\",\n\"might've\": \"might have\",\n\"mightn't\": \"might not\",\n\"mightn't've\": \"might not have\",\n\"must've\": \"must have\",\n\"mustn't\": \"must not\",\n\"mustn't've\": \"must not have\",\n\"needn't\": \"need not\",\n\"needn't've\": \"need not have\",\n\"o'clock\": \"of the clock\",\n\"oughtn't\": \"ought not\",\n\"oughtn't've\": \"ought not have\",\n\"shan't\": \"shall not\",\n\"sha'n't\": \"shall not\",\n\"shan't've\": \"shall not have\",\n\"she'd\": \"she had \/ she would\",\n\"she'd've\": \"she would have\",\n\"she'll\": \"she will\",\n\"she'll've\": \"she will have\",\n\"she's\": \"she has \/ she is\",\n\"should've\": \"should have\",\n\"shouldn't\": \"should not\",\n\"shouldn't've\": \"should not have\",\n\"so've\": \"so have\",\n\"so's\": \"so has\",\n\"that'd\": \"that would \/ that had\",\n\"that'd've\": \"that would have\",\n\"that's\": \"that has \/ that is\",\n\"there'd\": \"there had \/ there would\",\n\"there'd've\": \"there would have\",\n\"there's\": \"there has \/ there is\",\n\"they'd\": \"they had \/ they would\",\n\"they'd've\": \"they would have\",\n\"they'll\": \"they will\",\n\"they'll've\": \"they will have\",\n\"they're\": \"they are\",\n\"they've\": \"they have\",\n\"to've\": \"to have\",\n\"wasn't\": \"was not\",\n\"we'd\": \"we had \/ we would\",\n\"we'd've\": \"we would have\",\n\"we'll\": \"we will\",\n\"we'll've\": \"we will have\",\n\"we're\": \"we are\",\n\"we've\": \"we have\",\n\"weren't\": \"were not\",\n\"what'll\": \"what will\",\n\"what'll've\": \"what will have\",\n\"what're\": \"what are\",\n\"what's\": \"what has \/ what is\",\n\"what've\": \"what have\",\n\"when's\": \"when has \/ when is\",\n\"when've\": \"when have\",\n\"where'd\": \"where did\",\n\"where's\": \"where has \/ where is\",\n\"where've\": \"where have\",\n\"who'll\": \"who will\",\n\"who'll've\": \"who will have\",\n\"who's\": \"who has \/ who is\",\n\"who've\": \"who have\",\n\"why's\": \"why has \/ why is\",\n\"why've\": \"why have\",\n\"will've\": \"will have\",\n\"won't\": \"will not\",\n\"won't've\": \"will not have\",\n\"would've\": \"would have\",\n\"wouldn't\": \"would not\",\n\"wouldn't've\": \"would not have\",\n\"y'all\": \"you all\",\n\"y'all'd\": \"you all would\",\n\"y'all'd've\": \"you all would have\",\n\"y'all're\": \"you all are\",\n\"y'all've\": \"you all have\",\n\"you'd\": \"you had \/ you would\",\n\"you'd've\": \"you would have\",\n\"you'll\": \"you will\",\n\"you'll've\": \"you will have\",\n\"you're\": \"you are\",\n\"you've\": \"you have\"\n}\n\n# Replace contractions with full length words\ndf['msg_clean'] = df['msg_clean'].apply(lambda x: [word.replace(word, contractions[word.lower()]) if word.lower() in contractions else word for word in x])\n\n# Remove collections words\n# Collection words are the words that you used to query your data from Twitter.\n# Thus, you can expect that these terms will be found in each tweet. This could skew your word frequency analysis.\ncollection_words = ['climatechange', 'climate', 'change']\ndf['msg_clean'] = [[w for w in word if not w in collection_words] for word in df['msg_clean']]\n\n# Remove stop words\nstop_words = stopwords.words('english')\ndf['msg_clean'] = df['msg_clean'].apply(lambda row: [word for word in row if word not in stop_words])\n\n# Perform the same steps on the testing data\ndf_test['msg_clean'] = df_test['msg_clean'].apply(lambda x: [word.replace(word, contractions[word.lower()]) if word.lower() in contractions else word for word in x])\ndf_test['msg_clean'] = [[w for w in word if not w in collection_words] for word in df_test['msg_clean']]\ndf_test['msg_clean'] = df_test['msg_clean'].apply(lambda row: [word for word in row if word not in stop_words])\n\n# Transform the list of tokens into a single string\ndf['msg_clean'] = df['msg_clean'].apply(lambda x: ' '.join([i for i in x]))\ndf_test['msg_clean'] = df_test['msg_clean'].apply(lambda x: ' '.join([i for i in x]))","78958201":"# Remove punctutation and numbers from tweets\ndef remove_punctuation_numbers(post):\n    punc_numbers = string.punctuation + '0123456789'\n    return ''.join([l for l in post if l not in punc_numbers])\n\ndf['msg_clean'] = df['msg_clean'].apply(remove_punctuation_numbers)\ndf_test['msg_clean'] = df_test['msg_clean'].apply(remove_punctuation_numbers)","f446e181":"df['msg_clean'] = df['msg_clean'].str.lower()\ndf_test['msg_clean'] = df_test['msg_clean'].str.lower()","74a4a814":"# Lemmatise tweets\nlemmatizer = WordNetLemmatizer()\ndf['msg_clean'] = df['msg_clean'].apply(lemmatizer.lemmatize)\ndf_test['msg_clean'] = df_test['msg_clean'].apply(lemmatizer.lemmatize)","8d2d004d":"# Bar plot displaying the count of each sentiment for the dataframe\nsns.countplot(df['sentiment'])\nplt.title('Number of tweets per sentiment group')\nplt.show()","0582f3e8":"#Wordcloud of tweets for news\nwordcloud = WordCloud(background_color='white', width=800, height=400).generate(' '.join(df[df['sentiment'] == 2]\n                                          ['msg_clean']))\nplt.figure( figsize=(12,6))\nplt.imshow(wordcloud)\nplt.axis(\"off\")\nplt.show()","6ce549fe":"#Wordcloud of tweets for pro climate change\nwordcloud = WordCloud(background_color='white', width=800, height=400).generate(' '.join(df[df['sentiment'] == 1]\n                                          ['msg_clean']))\nplt.figure( figsize=(12,6))\nplt.imshow(wordcloud)\nplt.axis(\"off\")\nplt.show()","2866f2fb":"#Wordcloud of tweets for neutral \nwordcloud = WordCloud(background_color='white', width=800, height=400).generate(' '.join(df[df['sentiment'] == 0]\n                                          ['msg_clean']))\nplt.figure( figsize=(12,6))\nplt.imshow(wordcloud)\nplt.axis(\"off\")\nplt.show()","036bcb21":"#Wordcloud of tweets for anti climate change\nwordcloud = WordCloud(background_color='white', width=800, height=400).generate(' '.join(df[df['sentiment'] == -1]\n                                          ['msg_clean']))\nplt.figure( figsize=(12,6))\nplt.imshow(wordcloud)\nplt.axis(\"off\")\nplt.show()","a4068c07":"# Creating a variable for the sentiment analyser as it is too long\nsid = SentimentIntensityAnalyzer()\n\n# Add SlangSD dictionary to vader lexicon \n# (Wu et al. (2016): http:\/\/arxiv.org\/abs\/1608.05129)\n\nslang = pd.read_csv('..\/input\/slangsdtxt\/SlangSD.txt', sep='\\t', names=['word', 'score'])\nslang_dict = dict(zip(slang['word'], slang['score']))\nsid.lexicon.update(slang_dict)\n\n#Sentiment analysis of cleaned tweets\ndf['pos']  = df['message'].apply(lambda review: sid.polarity_scores(review)).apply(lambda score_dict: score_dict['pos'])\ndf['neg']  = df['message'].apply(lambda review: sid.polarity_scores(review)).apply(lambda score_dict: score_dict['neg'])\ndf['neu']  = df['message'].apply(lambda review: sid.polarity_scores(review)).apply(lambda score_dict: score_dict['neu'])\ndf['compound']  = df['message'].apply(lambda review: sid.polarity_scores(review)).apply(lambda score_dict: score_dict['compound'])","f56abfd2":"plt.figure( figsize=(12,3) )\nsns.kdeplot(df['pos'][df['sentiment'] == 2], shade = True, label = 'News')\nsns.kdeplot(df['pos'][df['sentiment'] == 1], shade = True, label = 'Pro')\nsns.kdeplot(df['pos'][df['sentiment'] == 0], shade = True, label = 'Neutral')\nsns.kdeplot(df['pos'][df['sentiment'] == -1], shade = True, label = 'Con')\nplt.xlabel('')\nplt.title('Positive sentiment score')\nplt.show()\n\nplt.figure( figsize=(12,3) )\nsns.kdeplot(df['neu'][df['sentiment'] == 2], shade = True, label = 'News')\nsns.kdeplot(df['neu'][df['sentiment'] == 1], shade = True, label = 'Pro')\nsns.kdeplot(df['neu'][df['sentiment'] == 0], shade = True, label = 'Neutral')\nsns.kdeplot(df['neu'][df['sentiment'] == -1], shade = True, label = 'Con')\nplt.xlabel('')\nplt.title('Neutral sentiment score')\nplt.show()\n\nplt.figure( figsize=(12,3) )\nsns.kdeplot(df['neg'][df['sentiment'] == 2], shade = True, label = 'News')\nsns.kdeplot(df['neg'][df['sentiment'] == 1], shade = True, label = 'Pro')\nsns.kdeplot(df['neg'][df['sentiment'] == 0], shade = True, label = 'Neutral')\nsns.kdeplot(df['neg'][df['sentiment'] == -1], shade = True, label = 'Con')\nplt.xlabel('')\nplt.title('Negative sentiment score')\nplt.show()\n\nplt.figure( figsize=(12,3) )\nsns.kdeplot(df['compound'][df['sentiment'] == 2], shade = True, label = 'News')\nsns.kdeplot(df['compound'][df['sentiment'] == 1], shade = True, label = 'Pro')\nsns.kdeplot(df['compound'][df['sentiment'] == 0], shade = True, label = 'Neutral')\nsns.kdeplot(df['compound'][df['sentiment'] == -1], shade = True, label = 'Con')\nplt.xlabel('')\nplt.title('Compound sentiment score')\nplt.show()","a13d4f95":"#add word count\ndf['word_count'] = df['message'].apply(lambda x: len(x.split()))\n\n#add unique word count\ndf['unique_words'] = df['message'].apply(lambda x: len(set(x.split())))\n\n#add stopword count\nstop_words = list(stopwords.words(\"english\"))\ndf['number_stopwords'] = df['message'].apply(lambda x: len([i for i in x.lower().split() if i in stop_words]))\n\n#add punctuation count\ndf['punctuation'] = df['message'].apply(lambda x: len([i for i in str(x) if i in string.punctuation]))\n\n#add url count\ndf['number_urls'] = df['message'].apply(lambda x: len([i for i in x.lower().split() if 'http' in i or 'https' in i]))\n\n#add mention count\ndf['mentions'] = df['message'].apply(lambda x: len([i for i in str(x) if i == '@']))\n\n#add hashtag count\ndf['hashtags'] = df['message'].apply(lambda x: len([i for i in str(x) if i == '#']))","bfebebfd":"plt.subplot(1,4,1)\nsns.boxplot(y='word_count', x='sentiment', data=df)\nplt.title('Number of words')\nplt.ylabel('Count')\nplt.xlabel('')\nfig = plt.gcf()\nfig.set_size_inches( 23, 5)\n\nplt.subplot(1,4,2)\nsns.boxplot(y='unique_words', x='sentiment', data=df)\nplt.title('Number of unique words')\nplt.ylabel('')\nplt.xlabel('')\nfig = plt.gcf()\nfig.set_size_inches( 23, 5)\n\nplt.subplot(1,4,3)\nsns.boxplot(y='number_stopwords', x='sentiment', data=df)\nplt.title('Number of stop words')\nplt.ylabel('')\nplt.xlabel('')\nfig = plt.gcf()\nfig.set_size_inches( 23, 5)\n\nplt.subplot(1,4,4)\nsns.boxplot(y='punctuation', x='sentiment', data=df)\nplt.title('Number of punctuation symbols')\nplt.ylabel('')\nplt.xlabel('')\nfig = plt.gcf()\nfig.set_size_inches( 23, 4)\nplt.show()","f472f49e":"plt.subplot(1,3,1)\nsns.stripplot(y='number_urls', x='sentiment', data=df, jitter=True)\nplt.title('Number of urls')\nplt.ylabel('Count')\nplt.xlabel('')\nfig = plt.gcf()\nfig.set_size_inches( 23, 5)\n\nplt.subplot(1,3,2)\nsns.stripplot(y='mentions', x='sentiment', data=df, jitter=True)\nplt.title('Number of mentions')\nplt.ylabel('')\nplt.xlabel('')\nfig = plt.gcf()\nfig.set_size_inches( 23, 5)\n\nplt.subplot(1,3,3)\nsns.stripplot(y='hashtags', x='sentiment', data=df, jitter=True)\nplt.title('Number of hashtags')\nplt.ylabel('')\nplt.xlabel('')\nfig = plt.gcf()\nfig.set_size_inches( 23, 5)\n\nplt.show()","29c83b22":"df['length'] = df['message'].apply(len)\n\nplt.figure( figsize=(12,6) )\nsns.kdeplot(df['length'][df['sentiment'] == 2], shade = True, label = 'News')\nsns.kdeplot(df['length'][df['sentiment'] == 1], shade = True, label = 'Pro')\nsns.kdeplot(df['length'][df['sentiment'] == 0], shade = True, label = 'Neutral')\nsns.kdeplot(df['length'][df['sentiment'] == -1], shade = True, label = 'Con')\nplt.xlabel('Length of words')\nplt.title('Distribution of length of tweets')\nplt.show()\n\nprint('\\n' + 'Average length of news tweets:\\t\\t{}'.format(round(df['length'][df['sentiment'] == 2].mean(), 3)))\nprint('\\n' + 'Average length of pro tweets:\\t\\t{}'.format(round(df['length'][df['sentiment'] == 1].mean(), 3)))\nprint('\\n' + 'Average length of neutral tweets:\\t{}'.format(round(df['length'][df['sentiment'] == 0].mean(), 3)))\nprint('\\n' + 'Average length of con tweets:\\t\\t{}'.format(round(df['length'][df['sentiment'] == -1].mean(), 3)))","b68df413":"def count_pos(df, sentiment):\n    pos_dict = {}\n    df_pos = df[df['sentiment'] == sentiment]\n    for i in range(len(df_pos)):\n        text = nlp(df.iloc[i, 1])\n        for j in range(len(text)):\n            part_of_speech = text[j].pos_\n            if part_of_speech in pos_dict.keys():\n                pos_dict[part_of_speech] += 1\n            else:\n                pos_dict[part_of_speech] = 1\n    return pos_dict","18dcc21b":"grp_2_pos = pd.DataFrame.from_dict(count_pos(df, 2), orient='index', columns=['news'])\ngrp_1_pos = pd.DataFrame.from_dict(count_pos(df, 1), orient='index', columns=['pro'])\ngrp_0_pos = pd.DataFrame.from_dict(count_pos(df, 0), orient='index', columns=['neutral'])\ngrp_neg1_pos = pd.DataFrame.from_dict(count_pos(df, -1), orient='index', columns=['con'])\n\ngrp_2_pos['news'] = grp_2_pos['news']\/(len(df[df['sentiment'] == 2]))\ngrp_1_pos['pro'] = grp_1_pos['pro']\/(len(df[df['sentiment'] == 1]))\ngrp_0_pos['neutral'] = grp_0_pos['neutral']\/(len(df[df['sentiment'] == 0]))\ngrp_neg1_pos['con'] = grp_neg1_pos['con']\/(len(df[df['sentiment'] == -1]))\n\ndf_pos = pd.merge(grp_2_pos, grp_1_pos, how='outer', left_index=True, right_index=True)\ndf_pos = pd.merge(df_pos, grp_0_pos, how='outer', left_index=True, right_index=True)\ndf_pos = pd.merge(df_pos, grp_neg1_pos, how='outer', left_index=True, right_index=True)\n\ndf_pos['type'] = df_pos.index\ndf_pos = pd.melt(df_pos, id_vars='type', var_name=\"sentiment\", value_name=\"count\")","2d932685":"sns.factorplot(x='type', y='count', hue='sentiment', data=df_pos, kind='bar')\nplt.xlabel('Part of Speech', size=20)\nplt.ylabel('Count', size=20)\nplt.xticks(size = 17)\nplt.yticks(size = 17)\nplt.legend(prop={'size':20})\nfig = plt.gcf()\nfig.set_size_inches(25, 12)\nplt.show()","a529b82a":"def count_ent(df, sentiment):\n    ent_dict = {}\n    name_dict = {}\n    df_pos = df[df['sentiment'] == sentiment]\n    for i in range(len(df_pos)):\n        text = nlp(df.iloc[i, 1])\n        if text.ents:\n            for ent in text.ents:\n                if ent.label_ in ent_dict.keys():\n                    ent_dict[ent.label_] += 1\n                else:\n                    ent_dict[ent.label_] = 1\n    return ent_dict","b6fe1fea":"grp_2_ent = pd.DataFrame.from_dict(count_ent(df, 2), orient='index', columns=['news'])\ngrp_1_ent = pd.DataFrame.from_dict(count_ent(df, 1), orient='index', columns=['pro'])\ngrp_0_ent = pd.DataFrame.from_dict(count_ent(df, 0), orient='index', columns=['neutral'])\ngrp_neg1_ent = pd.DataFrame.from_dict(count_ent(df, -1), orient='index', columns=['con'])\n\ngrp_2_ent['news'] = grp_2_ent['news']\/(len(df[df['sentiment'] == 2]))\ngrp_1_ent['pro'] = grp_1_ent['pro']\/(len(df[df['sentiment'] == 1]))\ngrp_0_ent['neutral'] = grp_0_ent['neutral']\/(len(df[df['sentiment'] == 0]))\ngrp_neg1_ent['con'] = grp_neg1_ent['con']\/(len(df[df['sentiment'] == -1]))\n\ndf_ent = pd.merge(grp_2_ent, grp_1_ent, how='outer', left_index=True, right_index=True)\ndf_ent = pd.merge(df_ent, grp_0_ent, how='outer', left_index=True, right_index=True)\ndf_ent = pd.merge(df_ent, grp_neg1_ent, how='outer', left_index=True, right_index=True)\n\ndf_ent['type'] = df_ent.index\ndf_ent = pd.melt(df_ent, id_vars='type', var_name=\"sentiment\", value_name=\"count\")","9d1e254c":"sns.factorplot(x='type', y='count', hue='sentiment', data=df_ent, kind='bar')\nplt.xlabel('Named Entities', size=20)\nplt.ylabel('Average number of words', size=20)\nplt.xticks(size = 17)\nplt.yticks(size = 17)\nplt.legend(prop={'size':20})\nfig = plt.gcf()\nfig.set_size_inches(30, 12)\nplt.show()","985ce78b":"vectorizer = TfidfVectorizer(sublinear_tf=True, \n                             smooth_idf = True, \n                             max_df = 0.3, \n                             strip_accents = 'ascii', \n                             ngram_range = (1, 2))","fea530b0":"# Splitting the data into variable and response \nX = vectorizer.fit_transform(df['msg_clean'].values)\ny = df['sentiment'].values\n\nprint('Shape of the vectorised training data: {}'.format(X.shape))","677c0d21":"# Splitting the training data into training (80%) and validation (20%) sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","ffd3ea7b":"# Evaluate the F1 score of different classifier models by cross-validation\nrandom_state = 42\nkf = KFold(n_splits=10, random_state=random_state, shuffle=True)\n\nclf = [LogisticRegression(max_iter = 4000), \n       LinearSVC(random_state=random_state),  \n       ComplementNB()]\n\nscores = []\nfor i in range(len(clf)):\n    scores.append(cross_val_score(clf[i], X_train, y_train, \n                                  scoring=make_scorer(f1_score, average='macro'), \n                                  cv=kf).mean())\n\nresult = pd.DataFrame({'Algorithm': ['LR', 'LSVC', 'CNB'], 'F1_macro': scores})\nresult = result.sort_values('F1_macro', ascending=False)\n\nprint(result)","9e19b971":"# Compare results of best performing versus worst performing model\n## Best performing model: LinearSVC()\nclf = LinearSVC(random_state=random_state)\nclf.fit(X_train, y_train)\ny_pred = clf.predict(X_test)\n\nprint('Best model performance' + '\\n')\nprint('Accuracy: {}'.format(accuracy_score(y_test, y_pred)))\nprint('Precision: {}'.format(precision_score(y_test, y_pred, average='macro')))\nprint('Recall: {}'.format(recall_score(y_test, y_pred, average='macro')))\nprint('F1: {}'.format(f1_score(y_test, y_pred, average='macro')))\nprint('\\n' + classification_report(y_test, y_pred))","a89045ff":"# Compare results of best performing versus worst performing model\n## Worst performing model: LinearSVC()\nclf = KNeighborsClassifier(3)\nclf.fit(X_train, y_train)\ny_pred = clf.predict(X_test)\n\nprint('Worst model performance' + '\\n')\nprint('Accuracy: {}'.format(accuracy_score(y_test, y_pred)))\nprint('Precision: {}'.format(precision_score(y_test, y_pred, average='macro')))\nprint('Recall: {}'.format(recall_score(y_test, y_pred, average='macro')))\nprint('F1: {}'.format(f1_score(y_test, y_pred, average='macro')))\nprint('\\n' + classification_report(y_test, y_pred))","04dbe207":"# Specify the range of 'C' parameters for LinearSVC\nparams = {'C': [0.1, 0.5, 1, 5, 10]}\n\nclf = GridSearchCV(LinearSVC(max_iter=4000, multi_class='ovr'), \n                   param_grid=params, cv=kf, \n                   scoring=make_scorer(f1_score, average='macro')).fit(X_train, y_train)\n\nprint('Best score: {}'.format(clf.best_score_))\nprint('Best parameters: {}'.format(clf.best_params_))","29e35687":"svc = LinearSVC(random_state=random_state, C=clf.best_params_['C'], multi_class='ovr')\nsvc.fit(X_train, y_train)\ny_pred = svc.predict(X_test)\n\nsvc_tuned = LinearSVC(random_state=random_state)\nsvc_tuned.fit(X_train, y_train)\ny_pred_tuned = svc_tuned.predict(X_test)\n\nprint('LinearSVC model performance' + '\\n')\nprint('Accuracy: {}'.format(accuracy_score(y_test, y_pred)) + \n      '  >>>  {}'.format(accuracy_score(y_test, y_pred_tuned)))\nprint('Precision: {}'.format(precision_score(y_test, y_pred, average='macro')) + \n      '  >>>  {}'.format(precision_score(y_test, y_pred_tuned, average='macro')))\nprint('Recall: {}'.format(recall_score(y_test, y_pred, average='macro')) + \n      '  >>>  {}'.format(recall_score(y_test, y_pred_tuned, average='macro')))\nprint('F1: {}'.format(f1_score(y_test, y_pred, average='macro')) + \n      '  >>>  {}'.format(f1_score(y_test, y_pred_tuned, average='macro')))\n#print('\\n' + classification_report(y_test, y_pred) + \n#      '  >>>  {}'.format(classification_report(y_test, y_pred_tuned)))\nprint('\\n' + classification_report(y_test, y_pred))\nprint('\\n' + classification_report(y_test, y_pred_tuned) + '\\n')","7e667279":"#Specify the range of alpha parameters for ComplementNB\nparams = {'alpha': [0.1, 0.5, 1], \n          'norm': [True, False]}\n\nclf2 = GridSearchCV(ComplementNB(), \n                   param_grid=params, \n                   cv=kf, scoring=make_scorer(f1_score, average='macro')).fit(X_train, y_train)\n\nprint('Best score: {}'.format(clf2.best_score_))\nprint('Best parameters: {}'.format(clf2.best_params_))","4824859f":"cnb = ComplementNB(alpha=clf2.best_params_['alpha'], norm=clf2.best_params_['norm'])\ncnb.fit(X_train, y_train)\ny_pred = cnb.predict(X_test)\n\ncnb_tuned = ComplementNB()\ncnb_tuned.fit(X_train, y_train)\ny_pred_tuned = cnb_tuned.predict(X_test)\n\nprint('ComplementNB model performance' + '\\n')\nprint('Accuracy: {}'.format(accuracy_score(y_test, y_pred)) + \n      '  >>>  {}'.format(accuracy_score(y_test, y_pred_tuned)))\nprint('Precision: {}'.format(precision_score(y_test, y_pred, average='macro')) + \n      '  >>>  {}'.format(precision_score(y_test, y_pred_tuned, average='macro')))\nprint('Recall: {}'.format(recall_score(y_test, y_pred, average='macro')) + \n      '  >>>  {}'.format(recall_score(y_test, y_pred_tuned, average='macro')))\nprint('F1: {}'.format(f1_score(y_test, y_pred, average='macro')) + \n      '  >>>  {}'.format(f1_score(y_test, y_pred_tuned, average='macro')))\n#print('\\n' + classification_report(y_test, y_pred) + \n#      '  >>>  {}'.format(classification_report(y_test, y_pred_tuned)))\nprint('\\n' + classification_report(y_test, y_pred))\nprint('\\n' + classification_report(y_test, y_pred_tuned) + '\\n')","eec73567":"X_test = vectorizer.transform(df_test['msg_clean'].values)","7dad0662":"# Running a prediction on the test data\nclassifier = LinearSVC(max_iter=4000)\nlinearsvc = classifier.fit(X, y)\ny_pred = classifier.predict(X_test)","bc1c2281":"# Storing the predictions in a CSV\npredictions = pd.DataFrame({\"tweetid\":df_test.index, \"sentiment\": y_pred})\npredictions.to_csv('submission_28.csv', index=False)\npredictions.head()","c252027c":"#params = {'alpha': 0.5, 'clf': 'ComplementNB'}\n#metrics = {'kaggle_score': 0.755}\n\n#Log parameters and results\n#experiment.log_parameters(params)\n#experiment.log_metrics(metrics)","dd4ebf1e":"#experiment.end()","e18b8523":"#experiment.display()","8d934cf8":"## Data preprocessing","3449be0b":" These boxplots give a summary of how the respective groups formulate their tweets. There does not appear to be any significant difference between the groups, however there are still some patterns apparent in the data. For example, news tweets seem to be shorter and use fewer unique words than the other groups. This might be expected as professional journalists would tend towards brevity.","47dd37ce":"## Modelling","d9862ce7":"In this figure, we examined how many named entities are used by each group. Organisations are the largest group for all sentiment classes. News and pro classes tend to use more ORG and PERSON features. Con and neutral tend to use more GPE (geopolitical entities) features. It would be interesting to explore precisely which entities are named in the different classes although due to time constraints it was not possible at this time.","20e83dba":"Now that the tweets have been cleaned and normalised, we apply vectorisation to the data which is the process of converting text into numerical features. We selected TfidfVectorizer for this process as it provides a means of prioritising less frequent tokens and penalising those that appear in almost all of the tweets. This is important because ubiquitous tokens carrying little meaningful information about the contents of the corpus and potentially shadow the frequencies of rarer terms (https:\/\/scikit-learn.org\/stable\/modules\/feature_extraction.html#text-feature-extraction). Furthermore, we investigated the contibution of phrases to the analysis in the form of bi- and tri-grams (two and three contiguous tokens such as 'global warming'). We observed higher prediction capabilities using bi-grams and the vectorizer parameters were adjusted accordingly. The vectorizer was fit on the training data and used to transform the training and test sets.","4fa2d0e4":"## Conclusion\n\n","7c6abc98":"## Introduction","7a30589f":"Ultimately, we selected a LinearSVC with default parameters as our final model. Despite identifying a set of optimised parameters based on the hyperparameter tuning, the results when submitted to the competition were not as good.","03575852":"In this project, we have built a classification model that can predict the sentiment of a person regarding climate change based on their tweets on the topic. Our best performing model produced an F1 score of 0.76, additional analysis and\/or data will be necessary to improve upon this score. Based on the findings, the best performing model was the Linear SVC. This model can be used by interested companies to evaluate their clients sentiments towards global warming.","36fe1265":"Climate change or Global warming is a phenomenon where the average global temperature increases. This is a result of \"carbon dioxide (CO2) and other air pollutants and greenhouse gases collecting in the atmosphere and absorb sunlight and solar radiation that have bounced off the earth\u2019s surface.\" As stated by the Natural Resources Defense Council. https:\/\/www.nrdc.org\/stories\/global-warming-101#warming\n\nMany companies are built around lessening one\u2019s environmental impact or carbon footprint, especially now when the world is changing rapidly. Organisations would like to have insights as to how the general public perceive climate change because this would add to their marketing research which would aid their future strategies.\n\nThe purpose of this report is to create a machine learning classifier model which will seek to classify whether or not someone believes in climate change based on their tweet. \n\nBuilding a successful classification model requires a thorough investigation of how different text such as slang, punctuation and capital words affects a particular sentiment. Also which words are mostly associated with the topic of climate change and each sentiment.\n\nTwitter, one of the biggest social media platforms will be used as the data that will assist in confirming whether people believe climate change is a real threat or not.","64796e16":"## Prediction","8eb8fd7b":"## Load packages","ff5e013d":"## Hyperparameter tuning for the two best performing classifiers","1ea89663":"Now we move onto evaluation of different classifiers. Initially we use cross validation with the number of splits set as 10 to help find the best fit model based on the data. We selected nine models to test including (A) LogisticRegression (linear), (B) SVC and LinearSVC (support vector machines), (C) Multinomial and Complement Naive Bayes, (D) KNeighborsClassifier (K-nearest neighbours), (E) DecisionTreeClassifier and (F) RandomForestClassifier and AdaBoostClassifier (ensemble). Furthermore, we selected the F1 score to score the models because this was the same as given on the Kaggle competition.","f6bedbdb":"Examining the length of the tweets in each group shows they tend to be similar in length (111-129 words) although pro and con tweets are longer than the others.","3dfd479d":"## Model evaluation","547bbf47":"In this figure, we explored the make up of each tweet with regards to the parts of speech that are used by the different groups. It seems that there are no significant differences between them, however further analysis might reveal more informative differences.","510a28d7":"## EDA","c474195c":"By examining the wordclouds produced for the different sentiment scores, we can get an idea of what the different groups refer to in the tweets. The size of the word indicates its prevalence in the tweets. For example, it appears that news-related tweets tend to include a number of links to various websites as well as handles. We can also observe that it seems that news and pro tweets seem to use a wider range of words than neutral or con tweets, judging by the fact that the former groups seem to have more smaller words and less bigger words relative to the latter groups. Furthermore, the pro and con groups include a number of words that might be expected in each group. For example, the pro tweets include words suggesting a call to action and environmeal concerns whilst the con group mentions words such as alarmist, scam and hoax.","b6ca1806":"Investigation of the sentiment scores of the different groups shows some interesting results. None of the groups showed particularly positive sentiments in their tweets, although news and pro tweets generally showed the lowest values. The groups had similar neutral scores. In general, all of the groups showed fairly high negative sentiments regarding climate change. This might be expected because people who fall into the pro category tend to discuss topics such as the terrible, long-lasting environmental effects whilst people in the con group tend to discuss the topic in the context of conspiracy theories and scams. The compound score summarises the observations seen in the first three graph and concludes that the general sentiment of people tweeting about climate change tends to be negative.","5010e272":"### JHB Team 8 2020\n### Mpho Marufu, Siyabonga Mtshemla, Thabo Ntsekhe, Caryn Pialat, Makhosazane Seroka and Dorcas Vena","b704cbaf":"We then performed hyperparameter tuning on the two best performing models identified from the cross-validation step which in this case was LinearSVC and ComplementNB, both of which are known to perform well with unbalanced classes which is a feature of the current dataset. This step provides a means for identifying the set of optimal hyperparameters for a learning algorithm. Here, we evaluated the C and multi_class parameter for the LinearSVC and alpha for the ComplementNaiveBayes. C represents the regularization parameter that controls the trade off between the achieving a low training error and a low testing error. Alpha is the smoothing or regularization parameter in naive Bayes.","33e72cf3":"## Load data","3feff821":"# Climate Change Belief Analysis"}}