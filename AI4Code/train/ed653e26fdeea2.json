{"cell_type":{"270e4143":"code","c0bdb8a7":"code","c65fdaa6":"code","689d3f03":"code","61852843":"code","735ca30f":"code","ac9ed525":"code","28fe0f68":"code","6be7c2d7":"code","953b78e4":"code","42169191":"code","2496af8b":"markdown"},"source":{"270e4143":"!pip install ..\/input\/sacremoses\/sacremoses-master\/ > \/dev\/null","c0bdb8a7":"import os\nimport sys\nimport glob\nimport torch\n\nsys.path.insert(0, \"..\/input\/transformers\/transformers-master\/\")\nimport transformers","c65fdaa6":"import numpy as np\nimport pandas as pd\nimport math\nimport catboost as ctb\nfrom tqdm import tqdm_notebook","689d3f03":"def sigmoid(x):\n    return 1 \/ (1 + math.exp(-x))","61852843":"def chunks(l, n):\n    \"\"\"Yield successive n-sized chunks from l.\"\"\"\n    for i in range(0, len(l), n):\n        yield l[i:i + n]\n","735ca30f":"def fetch_vectors(string_list, batch_size=64):\n    # inspired by https:\/\/jalammar.github.io\/a-visual-guide-to-using-bert-for-the-first-time\/\n    DEVICE = torch.device(\"cuda\")\n    tokenizer = transformers.DistilBertTokenizer.from_pretrained(\"..\/input\/distilbertbaseuncased\/\")\n    model = transformers.DistilBertModel.from_pretrained(\"..\/input\/distilbertbaseuncased\/\")\n    model.to(DEVICE)\n\n    fin_features = []\n    for data in tqdm_notebook(chunks(string_list, batch_size)):\n        tokenized = []\n        for x in data:\n            x = \" \".join(x.strip().split()[:300])\n            tok = tokenizer.encode(x, add_special_tokens=True)\n            tokenized.append(tok[:512])\n\n        max_len = 512\n        padded = np.array([i + [0] * (max_len - len(i)) for i in tokenized])\n        attention_mask = np.where(padded != 0, 1, 0)\n        input_ids = torch.tensor(padded).to(DEVICE)\n        attention_mask = torch.tensor(attention_mask).to(DEVICE)\n\n        with torch.no_grad():\n            last_hidden_states = model(input_ids, attention_mask=attention_mask)\n\n        features = last_hidden_states[0][:, 0, :].cpu().numpy()\n        fin_features.append(features)\n\n    fin_features = np.vstack(fin_features)\n    return fin_features","ac9ed525":"df_train = pd.read_csv(\"..\/input\/google-quest-challenge\/train.csv\").fillna(\"none\")\ndf_test = pd.read_csv(\"..\/input\/google-quest-challenge\/test.csv\").fillna(\"none\")\n\nsample = pd.read_csv(\"..\/input\/google-quest-challenge\/sample_submission.csv\")\ntarget_cols = list(sample.drop(\"qa_id\", axis=1).columns)\n\ntrain_question_body_dense = fetch_vectors(df_train.question_body.values)\ntrain_answer_dense = fetch_vectors(df_train.answer.values)\n\ntest_question_body_dense = fetch_vectors(df_test.question_body.values)\ntest_answer_dense = fetch_vectors(df_test.answer.values)","28fe0f68":"xtrain = np.hstack((train_question_body_dense, train_answer_dense))\nxtest = np.hstack((test_question_body_dense, test_answer_dense))","6be7c2d7":"for tc in target_cols:\n    print(tc)\n    clf = ctb.CatBoostRegressor(task_type=\"GPU\")\n    clf.fit(xtrain, df_train[tc].values)\n    preds = [sigmoid(x) for x in clf.predict(xtest)]\n    sample[tc] = preds","953b78e4":"sample.to_csv(\"submission.csv\", index=False)","42169191":"sample.head()","2496af8b":"## If you like the kernel, consider upvoting it and the associated datasets:\n- https:\/\/www.kaggle.com\/abhishek\/transformers\n- https:\/\/www.kaggle.com\/abhishek\/sacremoses\n- https:\/\/www.kaggle.com\/abhishek\/distilbertbaseuncased\n\nin case of any questions, feel free to ask.\n**P.S. combining this with bilstm nnet will give you 0.33+**"}}