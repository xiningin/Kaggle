{"cell_type":{"d80e8d0f":"code","e0f60ddb":"code","2edabca0":"code","9285af39":"code","b397c0e5":"code","1d29b19d":"code","653ab3ce":"code","8a17ad26":"code","c852b22c":"code","072f6f6e":"code","825f77fa":"code","7d53c99f":"code","722aab2e":"code","4e988dd2":"code","897e0d81":"code","1015d0fe":"code","5de75152":"code","c97e5f1c":"code","2c169b5f":"code","3d3d9cb8":"code","636f54f5":"code","d1eaae2d":"code","12482fcd":"code","5088d436":"code","4ae61dd3":"code","af30d2ff":"markdown","a4e42aff":"markdown","97c0f1da":"markdown","35b7339d":"markdown","a49c0346":"markdown","13dea03d":"markdown","f0e150a2":"markdown","e781290e":"markdown","7877387f":"markdown","578d0952":"markdown","3e9f7d0c":"markdown"},"source":{"d80e8d0f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","e0f60ddb":"import matplotlib.pyplot as plt\n%matplotlib inline\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\ndf = pd.read_csv('\/kaggle\/input\/creditcardfraud\/creditcard.csv')\ndf.head(3)","2edabca0":"df.info()","9285af39":"df.describe()","b397c0e5":"# \uc804\ucc98\ub9ac \ud568\uc218 \uc0dd\uc131\nfrom sklearn.model_selection import train_test_split\n\ndef get_preprocessed_df(df=None):\n    df_copy = df.copy()\n    df_copy.drop('Time', axis=1, inplace=True)\n    return df_copy","1d29b19d":"# \ub370\uc774\ud130 \uac00\uacf5 \ud6c4 \ud559\uc2b5\/\ub370\uc774\ud130 \uc138\ud2b8 \ubc18\ud658 \ud568\uc218 \uc0dd\uc131\ndef get_train_test_dataset(df=None):\n    # \uc804\ucc98\ub9ac \uc644\ub8cc\n    df_copy = get_preprocessed_df(df)\n    x_features = df_copy.iloc[:, :-1]\n    y_target = df_copy.iloc[:, -1]\n    # target \ubcc0\uc218 \uae30\uc900\uc73c\ub85c stratify \ubd84\ub9ac\n    x_train, x_test, y_train, y_test = train_test_split(x_features, y_target,\n                                                       random_state = 1995,\n                                                       test_size = .3,\n                                                       stratify = y_target)\n    # \ud559\uc2b5\/\ud14c\uc2a4\ud2b8 \ub370\uc774\ud130 \uc138\ud2b8 \ubc18\ud658\n    return x_train, x_test, y_train, y_test\n\nx_train, x_test, y_train, y_test = get_train_test_dataset(df)","653ab3ce":"print('\ud559\uc2b5 \ub370\uc774\ud130 \ub808\uc774\ube14 \uac12 \ube44\uc728')\nprint(y_train.value_counts() \/ y_train.shape[0] * 100)\nprint('\ud14c\uc2a4\ud2b8 \ub370\uc774\ud130 \ub808\uc774\ube14 \uac12 \ube44\uc728')\nprint(y_test.value_counts() \/ y_test.shape[0] * 100)","8a17ad26":"# \uc608\uce21 \uc131\ub2a5 \ud3c9\uac00 \ud568\uc218 \ub9cc\ub4e4\uae30\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score, f1_score, confusion_matrix, precision_recall_curve, roc_curve\n\ndef get_clf_eval(y_test, pred=None, pred_proba=None):\n    confusion = confusion_matrix(y_test, pred)\n    accuracy = accuracy_score(y_test, pred)\n    precision = precision_score(y_test, pred)\n    recall = recall_score(y_test, pred)\n    f1 = f1_score(y_test, pred)\n    \n    roc_auc = roc_auc_score(y_test, pred_proba)\n    print('\uc624\ucc28 \ud589\ub82c confusion matrix')\n    print(confusion)\n    print('\uc815\ud655\ub3c4 accuracy: {0:.4f}, \uc815\ubc00\ub3c4 precision: {1:.4f}, \uc7ac\ud604\uc728 recall: {2:.4f}, \\F1: {3:.4f}, AUC: {4:.4f}'.format(accuracy, precision, recall, f1, roc_auc))","c852b22c":"# \ubaa8\ub378 \ub9cc\ub4e4\uae30 - \ub85c\uc9c0\uc2a4\ud2f1 \ud68c\uadc0\nfrom sklearn.linear_model import LogisticRegression\n\nlr = LogisticRegression()\nlr.fit(x_train, y_train)\nlr_pred = lr.predict(x_test)\nlr_pred_proba = lr.predict_proba(x_test)[:, 1]\n\nget_clf_eval(y_test, lr_pred, lr_pred_proba)","072f6f6e":"# \ubc18\ubcf5\uc801\uc73c\ub85c \ubaa8\ub378\uc744 \ubcc0\uacbd\ud574 \ud559\uc2b5\/\uc608\uce21\/\ud3c9\uac00\ub97c \uc9c4\ud589\ud558\ub294 \ud568\uc218 \uc0dd\uc131\ndef get_model_train_eval(model, ftr_train=None, ftr_test=None, tgt_train=None, tgt_test=None):\n    model.fit(ftr_train, tgt_train)\n    pred = model.predict(ftr_test)\n    pred_proba = model.predict_proba(ftr_test)[:, 1]\n    get_clf_eval(tgt_test, pred, pred_proba)","825f77fa":"# \ubaa8\ub378 \uc0dd\uc131 - LightGBM\n# \ubcf8 \ub370\uc774\ud130 \uc138\ud2b8\ub97c \uadf9\ub3c4\ub85c \ubd88\uade0\ud615\ud55c \ub808\uc774\ube14 \uac12 \ubd84\ud3ec\ub97c \uac00\uc9c0\uace0 \uc788\uae30 \ub54c\ubb38\uc5d0 boost_from_average=False\ub85c \uc9c0\uc815\ud574\uc57c \ud569\ub2c8\ub2e4\n# used only in regression, binary, multiclassova and cross-entropy applications\n# adjusts initial score to the mean of labels for faster convergence\n# \ud3c9\uade0\uce58 \ubd80\uc2a4\ud305\uc73c\ub85c \ud558\uba74 \uac12\uc774 \ub9de\uc9c0 \uc54a\uae30 \ub54c\ubb38\uc5d0?\n\nfrom lightgbm import LGBMClassifier\n\nlgbm = LGBMClassifier(n_estimators=1000,\n                     num_leaves=64,\n                     n_jobs=-1,\n                     boost_from_average=False,\n                     random_state=1995)\nget_model_train_eval(lgbm,\n                    ftr_train = x_train,\n                    ftr_test = x_test,\n                    tgt_train = y_train,\n                    tgt_test = y_test)","7d53c99f":"# \ub370\uc774\ud130 \ubd84\ud3ec \ubcc0\ud658 \ud6c4 \ubaa8\ub378 \ud559\uc2b5\/\uc608\uce21\/\ud3c9\uac00\n# `Amount` feature \ubd84\ud3ec\ub3c4 \ud655\uc778\ud558\uae30\n\nimport seaborn as sns\nplt.figure(figsize = (8,4))\nplt.xticks(range(0, 30000, 1000), rotation=60)\nsns.distplot(df['Amount'])","722aab2e":"# \uc815\uaddc \ubd84\ud3ec \uc2a4\ucf00\uc77c\ub9c1 \ud568\uc218 \uc0dd\uc131\nfrom sklearn.preprocessing import StandardScaler\n\ndef get_preprocessed_df(df=None):\n    df_copy = df.copy()\n    scaler = StandardScaler()\n    amount_n = scaler.fit_transform(df_copy['Amount'].values.reshape(-1, 1))\n    # \ubcc0\ud658\ub41c Amount\ub97c Amount_scaled\ub85c \ud53c\ucc98\uba85 \ubcc0\uacbd \ud6c4\uc5d0 dataframe \ub9e8 \uc55e column\uc5d0 \uc785\ub825\n    df_copy.insert(0, 'Amount_scaled', amount_n)\n    # \uae30\uc874 time, amount \ubcc0\uc218 \uc0ad\uc81c\n    df_copy.drop(columns = ['Time', 'Amount'], inplace=True)\n    return df_copy","4e988dd2":"# amount \uc2a4\ucf00\uc77c\ub9c1 \uc774\ud6c4\uc5d0 \ub85c\uc9c0\uc2a4\ud2f1 \ud68c\uadc0 \ubc0f lightGBM \uc218\ud589\nx_train, x_test, y_train, y_test = get_train_test_dataset(df)\n\nprint('### \ub85c\uc9c0\uc2a4\ud2f1 \ud68c\uadc0 \uc608\uce21 \uc131\ub2a5 ###')\nlr = LogisticRegression()\nget_model_train_eval(lr,\n                     ftr_train = x_train,\n                     ftr_test = x_test,\n                     tgt_train = y_train,\n                     tgt_test = y_test)\nprint('### LightGBM \uc608\uce21 \uc131\ub2a5 ###')\nlgbm = LGBMClassifier(n_estimators = 1000,\n                     num_leaves = 64,\n                     n_jobs = -1,\n                     random_state = 1995,\n                     boost_from_average=False)\nget_model_train_eval(lgbm,\n                    ftr_train = x_train,\n                    ftr_test = x_test,\n                    tgt_train = y_train,\n                    tgt_test = y_test)","897e0d81":"# \ub85c\uadf8 \ubcc0\ud658 \ud6c4 \ubaa8\ub378 \uc608\uce21\/\ud3c9\uac00\ndef get_preprocessed_df(df=None):\n    df_copy = df.copy()\n    # \ub85c\uadf8 \ubcc0\ud658\n    amount_n = np.log1p(df_copy['Amount'])\n    df_copy.insert(0, 'Amount_scaled', amount_n)\n    df_copy.drop(columns = ['Time', 'Amount'], inplace=True)\n    return df_copy","1015d0fe":"x_train, x_test, y_train, y_test = get_train_test_dataset(df)\n\nprint('### \ub85c\uc9c0\uc2a4\ud2f1 \ud68c\uadc0 \uc608\uce21 \uc131\ub2a5 ###')\nget_model_train_eval(lr,\n                     ftr_train = x_train,\n                     ftr_test = x_test,\n                     tgt_train = y_train,\n                     tgt_test = y_test)\nprint('### LightGBM \uc608\uce21 \uc131\ub2a5 ###')\nget_model_train_eval(lgbm,\n                    ftr_train = x_train,\n                    ftr_test = x_test,\n                    tgt_train = y_train,\n                    tgt_test = y_test)","5de75152":"# \uc774\uc0c1\uce58 \uc81c\uac70 \ud6c4 \ubaa8\ub378 \ud559\uc2b5\/\ud3c9\uac00\n# dataframe\uc758 correlation \uad6c\ud55c \ub4a4 heatmap\uc73c\ub85c \ub098\ud0c0\ub0b4\uae30\nplt.figure(figsize=(9, 9))\ncorr = df.corr()\nsns.heatmap(corr, cmap='RdBu')","c97e5f1c":"# \uc774 \uc911 V14\uc5d0 \ub300\ud574\uc11c \uc774\uc0c1\uce58\ub97c \ucc3e\uc544\uc11c \uc81c\uac70\ud558\uae30 \ndef get_outlier(df=None,\n               column=None,\n               weight=1.5):\n    # fraud\uc5d0 \ud574\ub2f9\ud558\ub294 column \ub370\uc774\ud130\ub9cc \ucd94\ucd9c, 1\uc0ac\ubd84\uc704\uc640 3\uc0ac\ubd84\uc704 \uc9c0\uc810\uc744 \uad6c\ud568\n    fraud = df[df['Class'] == 1][column]\n    quantile_25 = np.percentile(fraud.values, 25)\n    quantile_75 = np.percentile(fraud.values, 75)\n    # IQR\uc744 \uad6c\ud558\uace0 IQR\uc5d0 1.5\ub97c \uacf1\ud574 \ucd5c\ub313\uac12\uacfc \ucd5c\uc19f\uac12 \uc9c0\uc810 \uad6c\ud568\n    iqr = quantile_75 - quantile_25\n    iqr_weight = iqr * weight\n    lowest_val = quantile_25 - iqr_weight\n    highest_val = quantile_75 + iqr_weight\n    # \uc774\uc0c1\uce58 index \ubc18\ud658\n    outlier_index = fraud[(fraud < lowest_val) | (fraud > highest_val)].index\n    return outlier_index","2c169b5f":"outlier_index = get_outlier(df=df, column='V14', weight=1.5)\nprint('\uc774\uc0c1\uce58 \ub370\uc774\ud130 \uc778\ub371\uc2a4: ', outlier_index)","3d3d9cb8":"# \ucd94\ucd9c\ub41c \uc774\uc0c1\uce58 \uc81c\uac70 \ud6c4 \ubaa8\ub378 \uc801\uc6a9\ndef get_preprocessed_df(df=None):\n    df_copy = df.copy()\n    scaler = StandardScaler()\n    amount_n = scaler.fit_transform(df_copy['Amount'].values.reshape(-1, 1))\n    df_copy.insert(0, 'Amount_scaled', amount_n)\n    df_copy.drop(columns = ['Time', 'Amount'], axis=1, inplace=True)\n    # \uc774\uc0c1\uce58 \uc0ad\uc81c \n    outlier_index = get_outlier(df=df_copy, column = 'V14', weight=1.5)\n    df_copy.drop(outlier_index, axis=0, inplace=True)\n    return df_copy\n\nx_train, x_test, y_train, y_test = get_train_test_dataset(df)\n\nprint('### \ub85c\uc9c0\uc2a4\ud2f1 \ud68c\uadc0 \uc608\uce21 \uc131\ub2a5 ###')\nget_model_train_eval(lr,\n                     ftr_train = x_train,\n                     ftr_test = x_test,\n                     tgt_train = y_train,\n                     tgt_test = y_test)\nprint('### LightGBM \uc608\uce21 \uc131\ub2a5 ###')\nget_model_train_eval(lgbm,\n                    ftr_train = x_train,\n                    ftr_test = x_test,\n                    tgt_train = y_train,\n                    tgt_test = y_test)","636f54f5":"# SMOTE \uc624\ubc84 \uc0d8\ud50c\ub9c1 \uc801\uc6a9 \ud6c4 \ubaa8\ub378 \ud559\uc2b5\/\ud3c9\uac00\nfrom imblearn.over_sampling import SMOTE\n\nsmote = SMOTE(random_state=1995)\nx_train_over, y_train_over = smote.fit_sample(x_train, y_train)\nprint('SMOTE \uc801\uc6a9 \uc804 \ud559\uc2b5\uc6a9 \ud53c\ucc98\/\ub808\uc774\ube14 \ub370\uc774\ud130 \uc138\ud2b8: ', x_train.shape, y_train.shape)\nprint('SMOTE \uc801\uc6a9 \ud6c4 \ud559\uc2b5\uc6a9 \ud53c\ucc98\/\ub808\uc774\ube14 \ub370\uc774\ud130 \uc138\ud2b8: ', x_train_over.shape, y_train_over.shape)\nprint('SMOTE \uc801\uc6a9 \ud6c4 \ub808\uc774\ube14 \uac12 \ubd84\ud3ec: \\n', pd.Series(y_train_over).value_counts())","d1eaae2d":"# SMOTE \ub370\uc774\ud130\ub85c \ub85c\uc9c0\uc2a4\ud2f1 \ud68c\uadc0 \ubaa8\ub378 \uc801\uc6a9\nlr = LogisticRegression()\nget_model_train_eval(lr,\n                    ftr_train = x_train_over, ftr_test = x_test,\n                    tgt_train = y_train_over, tgt_test = y_test)","12482fcd":"# \uc784\uacd7\uac12 \ud655\uc778\ud558\uae30\n# \uc784\uacc4\uac12 \uc124\uc815\nthresholds = [.4, .45, .5, .55, .6]\n\ndef precision_recall_curve_plot(y_test, pred_proba_c1):\n    # threshold ndarray\uc640 \uc774 threshold\uc5d0 \ub530\ub978 \uc815\ubc00\ub3c4, \uc7ac\ud604\uc728 ndarray \ucd94\ucd9c\ud558\uae30\n    precisions, recalls, thresholds = precision_recall_curve(y_test, pred_proba_c1)\n    \n    # x\ucd95\uc744 threshold\uac12\uc73c\ub85c, y\ucd95\uc740 \uc815\ubc00\ub3c4, \uc7ac\ud604\uc728 \uac12\uc73c\ub85c \uac01\uac01 plot \uc218\ud589. \uc815\ubc00\ub3c4\ub294 \uc810\uc120\uc73c\ub85c \ud45c\uc2dc\n    plt.figure(figsize=(8, 6))\n    threshold_boundary = thresholds.shape[0]\n    plt.plot(thresholds, precisions[0:threshold_boundary], linestyle='--', label='precision')\n    plt.plot(thresholds, recalls[0:threshold_boundary], label='recall')\n    \n    # threshold \uac12 x\ucd95\uc758 scale\uc744 0, 1 \ub2e8\uc704\ub85c \ubcc0\uacbd\n    start, end = plt.xlim()\n    plt.xticks(np.round(np.arange(start, end, 0.1), 2))\n    \n    # x, y\ucd95 label\uacfc legend, grid \uc124\uc815\n    plt.xlabel('Threshold value'); plt.ylabel('Precision and Recall value')\n    plt.legend(); plt.grid()\n    plt.show()","5088d436":"precision_recall_curve_plot(y_test, lr.predict_proba(x_test)[:, 1])","4ae61dd3":"# SMOTE \ub370\uc774\ud130\ub85c LGBM \uc801\uc6a9\nlgbm = LGBMClassifier(random_state = 1995,\n                     n_estimators = 1000,\n                     num_leaves = 64,\n                     n_jobs = -1,\n                     boost_from_average = False)\nget_model_train_eval(lgbm,\n                    ftr_train = x_train_over, ftr_test = x_test,\n                    tgt_train = y_train_over, tgt_test = y_test)","af30d2ff":"`Class` feature\uc640 \uc74c\uc758 \uc0c1\uad00\uad00\uacc4\uac00 \uac00\uc7a5 \ub192\uc740 feature\ub294 `V14`, `V17`","a4e42aff":"![img](https:\/\/github.com\/ameliachoi\/tutorial-python-machine-learning\/blob\/master\/Screen%20Shot%202020-09-21%20at%203.49.56%20PM.png?raw=true)\n\n\ub85c\uc9c0\uc2a4\ud2f1 \ud68c\uadc0\uc758 \uacbd\uc6b0, `Amount` feature log scaling\uc774, LGBM\uc758 \uacbd\uc6b0, `Amount` feature standard scaling\uc774 \uc131\ub2a5\uc774 \uac00\uc7a5 \uc88b\uc558\uc2b5\ub2c8\ub2e4.","97c0f1da":"*\ub85c\uc9c0\uc2a4\ud2f1 \ud68c\uadc0\uc758 \uacbd\uc6b0, \uc624\ubc84 \uc0d8\ud50c\ub9c1\uc73c\ub85c \ud559\uc2b5\ud560 \uacbd\uc6b0 \uc7ac\ud604\uc728\uc774 \ub9e4\uc6b0 \ud070 \ud3ed\uc73c\ub85c \uc99d\uac00\ud558\uc9c0\ub9cc \uc815\ubc00\ub3c4\uac00 5%\ub300\ub85c \uae09\uac10\ud569\ub2c8\ub2e4. \uc7ac\ud604\uc728\uc774 \ub192\ub354\ub77c\ub3c4 \uc774 \uc815\ub3c4\ub85c \uc800\uc870\ud55c \uc815\ubc00\ub3c4\ub85c\ub294 \ud604\uc2e4 \uc5c5\ubb34\uc5d0 \uc801\uc6a9\ud560 \uc218\uac00 \uc5c6\uc2b5\ub2c8\ub2e4.*","35b7339d":"---","a49c0346":"*\ub85c\uc9c0\uc2a4\ud2f1 \ud68c\uadc0\uc640 LGBM \ubaa8\ub450 \uc2a4\ucf00\uc77c\ub9c1 \uc774\uc804\uacfc \ube44\uad50\ud560 \ub54c \uc7ac\ud604\uc728\uacfc ROC-AUC score\uac00 \ubcc0\ud654\ud558\uc600\ub2e4.*","13dea03d":"*LGBM score : recall\uc774 83.11%, ROC-AUC\uac00 98.81%\ub85c logistic regression\ubcf4\ub2e4 \uc131\ub2a5 \ud5a5\uc0c1*","f0e150a2":"`Amount` feature (\uce74\ub4dc \uc0ac\uc6a9\uae08\uc561)\uc758 \uacbd\uc6b0, 1000\ubd88 \uc774\ud558\uc778 \ub370\uc774\ud130\uac00 \ub300\ubd80\ubd84\uc785\ub2c8\ub2e4.\n\n\uc774 \ubcc0\uc218\ub97c **\ud45c\uc900 \uc815\uaddc\ubd84\ud3ec** \ud615\ud0dc\ub3c4 \ubcc0\ud658\ud55c \ub4a4\uc5d0 \ub85c\uc9c0\uc2a4\ud2f1 \ud68c\uadc0\uc758 \uc608\uce21 \uc131\ub2a5\uc744 \uce21\uc815\ud574 \ubcf4\uaca0\uc2b5\ub2c8\ub2e4.","e781290e":"*Logistic Regression score : \uc7ac\ud604\uc728 recall\uc774 60.81%, ROC-AUC\uac00 95.49%*","7877387f":"## [Tutorial] Credit Card Fraud Detection \uc2e0\uc6a9\uce74\ub4dc \uc0ac\uae30 \uac80\ucd9c\n### \ucc45 <\ud30c\uc774\uc36c \uba38\uc2e0\ub7ec\ub2dd \uc644\ubcbd \ub9c8\uc2a4\ud130> \ud544\uc0ac \ucf54\ub4dc\uc785\ub2c8\ub2e4.","578d0952":"`Time` feature\uc758 \uacbd\uc6b0, \ud070 \uc758\ubbf8\uac00 \uc5c6\uae30 \ub54c\ubb38\uc5d0 \uc81c\uac70\ud569\ub2c8\ub2e4.\n\n`Amount` feature\ub294 \uc2e0\uc6a9\uce74\ub4dc \ud2b8\ub79c\uc81d\uc158 \uae08\uc561\uc744 \uc758\ubbf8\ud558\uba70, `Class`\ub294 label\ub85c\uc11c 0\uc758 \uacbd\uc6b0 \uc815\uc0c1, 1\uc758 \uacbd\uc6b0 \uc0ac\uae30 \ud2b8\ub79c\uc81d\uc158\uc785\ub2c8\ub2e4.","3e9f7d0c":"\uc784\uacd7\uac12\uc774 0.99 \uc774\ud558\uc5d0\uc11c\ub294 \uc7ac\ud604\uc728\uc774 \ub9e4\uc6b0 \uc88b\uace0 \uc815\ubc00\ub3c4\uac00 \uadf9\ub2e8\uc801\uc73c\ub85c \ub0ae\ub2e4\uac00, 0.99 \uc774\uc0c1\ubd80\ud130 \ubc18\ub300\ub85c \uc7ac\ud604\uc728\uc774 \ub300\ud3ed \ub5a8\uc5b4\uc9c0\uace0 \uc815\ubc00\ub3c4\uac00 \ub192\uc544\uc9d1\ub2c8\ub2e4.\n\n**\uc704\uc758 \uacbd\uc6b0\uc5d0\ub294 \uc784\uacd7\uac12\uc744 \uc870\uc815\ud574\ub3c4 \uc88b\uc740 \ud3c9\uac00 \uc810\uc218\ub97c \ubc1b\uc744 \uc218 \uc5c6\uc73c\ubbc0\ub85c, \ub2e4\ub978 \ubaa8\ub378\uc744 \uc774\uc6a9\ud569\ub2c8\ub2e4.**"}}