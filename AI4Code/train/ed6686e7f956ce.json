{"cell_type":{"a4613eb4":"code","6fcc7e97":"code","14c03006":"code","72563b09":"code","df7c0f43":"code","f3bb0bec":"code","25aa93ce":"code","d6b62005":"code","e0ee27e4":"code","9c9e160a":"code","ff11059c":"code","8ff44232":"code","0c0a7de9":"code","23b63cdd":"code","57ed1f87":"code","3ff614b3":"code","42b34388":"code","aa3d6f98":"code","8525184b":"code","000fe5e2":"code","83e286c5":"code","d2052661":"code","c384073c":"code","2fcedf74":"code","28401d1a":"code","3fa54d61":"code","05fb14d2":"code","d5d5078a":"code","ff54dd43":"code","af90baa4":"code","b85e99ff":"code","02b95fbd":"code","7431ea3e":"code","fcfc38f1":"code","cc08dc4f":"code","da60012c":"code","2538f193":"code","61babc55":"code","f0e207cc":"code","7047fb13":"code","1b196bf9":"code","8f2f8b63":"code","1dd68e79":"code","82a2b13e":"code","c1e80c96":"code","924744ee":"code","f0a1c695":"markdown","fcde603c":"markdown","e1d2a7b3":"markdown","a4dc386d":"markdown","d8be0de9":"markdown","827b5e90":"markdown","90207d02":"markdown","5ac3a108":"markdown","22cd0d7f":"markdown","30c809af":"markdown","c9786559":"markdown"},"source":{"a4613eb4":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","6fcc7e97":"import pandas as pd","14c03006":"whole_dataset = pd.read_csv(\"\/kaggle\/input\/twitter-user-gender-classification\/gender-classifier-DFE-791531.csv\", encoding = \"latin1\")","72563b09":"whole_dataset","df7c0f43":"new_dataset = pd.concat([whole_dataset.gender,whole_dataset.description], axis = 1)","f3bb0bec":"new_dataset","25aa93ce":"new_dataset.head(20)","d6b62005":"new_dataset = new_dataset.dropna(axis = 0)","e0ee27e4":"new_dataset.head(20)","9c9e160a":"new_dataset.gender = [1 if i == \"female\" else 0 for i in new_dataset.gender]","ff11059c":"new_dataset.gender","8ff44232":"new_dataset.head(10)","0c0a7de9":"import re","23b63cdd":"sentence1 = new_dataset.description[4]\nprint(sentence1)","57ed1f87":"sentence1_after_regularexpression = re.sub(\"[^a-zA-Z]\", \" \", sentence1)","3ff614b3":"sentence1_after_regularexpression","42b34388":"sentence1_after_regularexpression = sentence1_after_regularexpression.lower()","aa3d6f98":"sentence1_after_regularexpression","8525184b":"import nltk","000fe5e2":"nltk.download(\"stopwords\")","83e286c5":"from nltk.corpus import stopwords  ","d2052661":"sentence1_after_regularexpression = nltk.word_tokenize(sentence1_after_regularexpression)","c384073c":"sentence1_after_regularexpression","2fcedf74":"example_string1 = \"shouldn't go away\"","28401d1a":"after_splitting = example_string1.split(\" \")\nafter_splitting","3fa54d61":"after_nltk_word_tokenize = nltk.word_tokenize(example_string1)\nafter_nltk_word_tokenize","05fb14d2":"sentence1_after_regularexpression ","d5d5078a":"sentence1_after_regularexpression_andstopwords = [i for i in sentence1_after_regularexpression if not i in set(stopwords.words(\"english\"))]","ff54dd43":"sentence1_after_regularexpression_andstopwords","af90baa4":"import nltk as nlp","b85e99ff":"lemma = nlp.WordNetLemmatizer()\nsentence1_after_regularexpression_andstopwords_andlemmatization = [lemma.lemmatize(i) for i in sentence1_after_regularexpression_andstopwords]\n\nsentence1_after_regularexpression_andstopwords_andlemmatization","02b95fbd":"last_sent = \" \".join(sentence1_after_regularexpression_andstopwords_andlemmatization)","7431ea3e":"last_sent","fcfc38f1":"import re\nimport nltk\nfrom nltk.corpus import stopwords  \nimport nltk as nlp","cc08dc4f":"description_list = []\n\nfor each_sentence in new_dataset.description:\n    each_sentence = re.sub(\"[^a-zA-Z]\", \" \", each_sentence)   # regular expression\n    each_sentence = each_sentence.lower()                     # Lower case\n    each_sentence = nltk.word_tokenize(each_sentence)         # Splitting\n    each_sentence =  [i for i in each_sentence if not i in set(stopwords.words(\"english\"))]  # Irrelevant words\n    lemma = nlp.WordNetLemmatizer() \n    each_sentence = [lemma.lemmatize(i) for i in each_sentence]    # Finding root of the words\n    each_sentence = \" \".join(each_sentence)                        # Converting list to string\n    \n    description_list.append(each_sentence)\n    ","da60012c":"from sklearn.feature_extraction.text import CountVectorizer\ncount_vectorizer = CountVectorizer(max_features=4000)  # Most 4000 repeated words.","2538f193":"sparce_matrix = count_vectorizer.fit_transform(description_list)\nsparce_matrix = sparce_matrix.toarray()\nsparce_matrix","61babc55":"sparce_matrix.shape","f0e207cc":"count_vectorizer.get_feature_names()","7047fb13":"x = sparce_matrix\ny = new_dataset.gender.values.reshape(-1,1)","1b196bf9":"print(x.shape)\nprint(y.shape)","8f2f8b63":"from sklearn.model_selection import train_test_split","1dd68e79":"xtrain, xtest, ytrain, ytest = train_test_split(x, y, test_size = 0.1, random_state = 42)","82a2b13e":"from sklearn.naive_bayes import GaussianNB","c1e80c96":"nb = GaussianNB()\nnb.fit(xtrain, ytrain)","924744ee":"predictions = nb.predict(xtest)\nprint(\"Accuracy:\", nb.score(predictions.reshape(-1,1), ytest))","f0a1c695":"![](https:\/\/miro.medium.com\/max\/880\/1*hLvya7MXjsSc3NS2SoLMEg.png)","fcde603c":"## Implementin each path for the whole dataset","e1d2a7b3":"Before reading the dataset, with the following code, we say that \"this dataset has latin letters and according to this knowledge, encode this dataset\".","a4dc386d":"Using nltk library and tokenizer method split our words better.","d8be0de9":"### Machine Learning Model:","827b5e90":"I am going to use only \"gender\" and \"text\" columns, that's why I need to concat these two columns. In the following parts I will do a classification for predicting the text writer's gender.","90207d02":"Most repetitive (800) words:","5ac3a108":"### Bag of Words:","22cd0d7f":"If a row contains NaN value, delete this row:","30c809af":"### Cleaning data:","c9786559":"Stopwords were downloaded as corpus file and we import them after that process."}}