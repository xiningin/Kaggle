{"cell_type":{"93b9dfc0":"code","aef70a44":"code","7d93f1a1":"code","5d5eb11c":"code","67246cba":"code","0e162578":"code","3c627347":"code","6d7e7048":"code","112cda99":"code","3cbb8df8":"code","5debb01c":"code","e3af8260":"code","371d8ad4":"code","701cf023":"code","6e55a51e":"code","513c8bd0":"code","83417255":"code","89015da0":"code","8dd05f49":"code","4f95737e":"code","fada539e":"code","78fba814":"code","502f3a78":"code","cbe98741":"code","b136e4dc":"code","806d196b":"code","7b8f203f":"code","e3d7e545":"markdown","8950921a":"markdown","f602adcf":"markdown","0ef20dbf":"markdown","b09e2b5d":"markdown","e74eb63f":"markdown","4ba69fea":"markdown","81fa0b07":"markdown","af6ecf7e":"markdown","3441bb5f":"markdown","c8327a5f":"markdown","bd9be811":"markdown","899b0a60":"markdown","b9f6ef84":"markdown","169a3517":"markdown","15336b1b":"markdown","2fc6c326":"markdown","970019e6":"markdown","11982626":"markdown"},"source":{"93b9dfc0":"!ls ..\/input\/skmlengineer2104week4","aef70a44":"import pandas as pd\nimport numpy as np\n\n\ntrain_df = pd.read_csv('..\/input\/skmlengineer2104week4\/train.csv')\ntest_df = pd.read_csv('..\/input\/skmlengineer2104week4\/test.csv')\nsubmission_df = pd.read_csv('..\/input\/skmlengineer2104week4\/sample_submission.csv') ","7d93f1a1":"train_df.head()","5d5eb11c":"test_df.head()","67246cba":"submission_df.head()","0e162578":"train_df.info(verbose=True, null_counts=True)","3c627347":"train_df.describe()","6d7e7048":"for colname in train_df.columns:\n  nunique = train_df[colname].nunique()\n  print(\"col {} | {} unique values\".format(colname, nunique))","112cda99":"for colname in test_df.columns:\n  nunique = test_df[colname].nunique()\n  print(\"col {} | {} unique values\".format(colname, nunique))","3cbb8df8":"to_drop = ['No.', 'LOT_ID']\ntrain_df.drop(to_drop, axis=1, inplace=True)\ntest_df.drop(to_drop, axis=1, inplace=True)","5debb01c":"# display constant columns\ntrain_df.loc[:, train_df.apply(pd.Series.nunique, axis=0) == 1]","e3af8260":"# remove constant columns\nmask_non_constant_col = train_df.apply(pd.Series.nunique) != 1\ntrain_df = train_df.loc[:, mask_non_constant_col]\ntest_df = test_df.loc[:, mask_non_constant_col]","371d8ad4":"import numpy as np\n\n# Exclude label column (We MUST NOT remove cols highly correlated with labels!)\ntrain_df_wo_label = train_df.drop(['PKT_Y'], axis=1, inplace=False)\n\n# Create correlation matrix\ncorr_matrix = train_df_wo_label.corr().abs()\n\n# Select upper triangle of correlation matrix\nupper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n\n# Find features with correlation greater than 0.9\nto_drop = [column for column in upper.columns if any(upper[column] > 0.95)]\n\nto_drop","701cf023":"# Drop features \ntrain_df.drop(to_drop, axis=1, inplace=True)\ntest_df.drop(to_drop, axis=1, inplace=True)","6e55a51e":"train_df.dropna(inplace=True)","513c8bd0":"train_df['GRADE_CD'].unique()","83417255":"test_df['GRADE_CD'].unique()","89015da0":"train_df = pd.get_dummies(train_df, columns = ['GRADE_CD'])\ntest_df = pd.get_dummies(test_df, columns = ['GRADE_CD'])","8dd05f49":"train_df.columns","4f95737e":"test_df.columns","fada539e":"feature_columns = [column for column in train_df.columns if column != 'PKT_Y']\nX = train_df.loc[:, feature_columns].to_numpy()\ny = train_df['PKT_Y'].to_numpy()\nX_test = test_df.to_numpy()\nprint(\"shape of features in training data\", X.shape)\nprint(\"shape of labels in training data\", y.shape)\nprint(\"shape of features in test data\", X_test.shape)","78fba814":"from sklearn.model_selection import train_test_split\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.1, random_state=7)\nprint(\"shape of features in training split\", X_train.shape)\nprint(\"shape of labels in training split\", y_train.shape)\nprint(\"shape of features in validation split\", X_valid.shape)\nprint(\"shape of labels in validation split\", y_valid.shape)","502f3a78":"from xgboost import XGBClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import fbeta_score, make_scorer\n\nftwo_scorer = make_scorer(fbeta_score, beta=2) # callable ftn to compute F2-score \n\nsubsample_ratio = [0.2*(i+1) for i in range(5)]\n\n# grid search\nmodel = XGBClassifier(tree_method=\"gpu_hist\")\nparam_grid = dict(subsample=subsample_ratio)\nkfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=7)\n# grid search with all processors (n_jobs=-1)\ngrid_search = GridSearchCV(model, param_grid, \n                           scoring=ftwo_scorer, # F2-score\n                           n_jobs=-1, cv=kfold, verbose=2)\ngrid_result = grid_search.fit(X, y)\n# summarize results\nprint(\"Best: {:.3f} using {}\".format(grid_result.best_score_, grid_result.best_params_))\nmeans = grid_result.cv_results_['mean_test_score']\nstds = grid_result.cv_results_['std_test_score']\nparams = grid_result.cv_results_['params']\nfor mean, stdev, param in zip(means, stds, params):\n\tprint(\"{:.6f} ({:.6f}) with: {}\".format(mean, stdev, param))\n","cbe98741":"# get the best estimator\nprint(grid_result)\nbest_model = grid_result.estimator","b136e4dc":"# train best model\nbest_model.fit(X, y)\n\n# predict on test set\ny_pred = best_model.predict(X_test)","806d196b":"# dump results\nsubmission_df['PKT_Y'] = pd.Series(y_pred.astype(int))\nsubmission_df.head()","7b8f203f":"# save to csv file\nsubmission_df.to_csv('.\/ta_submission.csv', index=False)","e3d7e545":"\n## 1. Explore the data\nFirst, we have to know the shape of data.","8950921a":"# Evaluation\n\nThe last thing you have to do is save results into a csv file.\n\nBefore then, you first fill the dataframe for submission.\n\n","f602adcf":"**Further improvement**: adjust tree parameters, large-scale ensemble, trying models other than trees, and\/or tackling data imbalance","0ef20dbf":"### B. Remove redundant columns\n\nWe also have to remove redundant columns for efficient computation.\n\nFor this, we can use `pandas.DataFrame.corr()`.\nIt computes the correlation matrix between columns.\n\nOther things are straightforward: we leave only one among highly correlated columns.\n\n**Tips**  \n*   `np.triu(array, k)`: Return a copy of an array with the elements below the \n`k`-th diagonal zeroed. For example,\n    ```\n    np.triu(np.ones((5,5)), k=1).astype(np.bool)\n    ```   \n    **Output**:\n    ```\n    array([\n       [False,  True,  True,  True,  True],\n       [False, False,  True,  True,  True],\n       [False, False, False,  True,  True],\n       [False, False, False, False,  True],\n       [False, False, False, False, False]\n    ])\n    ```\n*   `any(boolean_list)`: Return `True` if there is any `True`.\n    ```\n    print(any([True, False]))\n    print(any([False, False]))\n    ```   \n    **Output**:\n    ```\n    True\n    False\n    ```","b09e2b5d":"# **Challenge with Naive TA**\n","e74eb63f":"Let's prepare dataset first.","4ba69fea":"## 2. Remove unnecessary columns\n\nFirst you remove the columns that seem useless even at a glance.","81fa0b07":"`pd.Series.nuique` is a function which returns counts of mutually exclusive elements.","af6ecf7e":"### B. One-hot encoding\n\nYou should encode nominal variables in one-hot vectors.","3441bb5f":"## 6. (Optional) Split validation data \n\nThe next step is getting the validation split.\n\nIf you are planning to use K-fold cross validation, you can skip this step.","c8327a5f":"You are also able to try to use cool EDA libraries!","bd9be811":"### A. Label encoding\n\nFor ordinal variables and the target label, you have to encode them to numbers.\n\nIn this situation, you don't have any ordinal variable.\n","899b0a60":"## 4. Handle categorical columns","b9f6ef84":"## 7. Standardization & Normalization\n\nFor obtaining better results, you could standardize and normalize continuous columns.\n\nHowever, your naive TA just decided to use tree-based classifiers.\nIn case, standardization & normalization do not affect the model. ([Why?](https:\/\/datascience.stackexchange.com\/questions\/5277\/do-you-have-to-normalize-data-when-building-decision-trees-using-r))\n\nIf you want to do them, you can refer to previous notebook files!","169a3517":"## 3. Handle missing values\n\nYou should also handle missing values.\n\nYou could try various imputation techniques learned so far.\n\nHowever, your naive TA just removes all rows with missing values!","15336b1b":"# EDA & Preprocessing\n","2fc6c326":"## 5. Extract numpy arrays\n\nNow your data seems to be well-ordered with numeric values.\nThen you can extract numpy arrays, which works well on any python ML libraries.","970019e6":"# Obtaining a model\n\nGiven well-formatted training and validaiton splits, you can directly train a model and tune hyperparameters. \n\nWe already learned various supervised learning methods!","11982626":"### A. Remove constant columns\n\n`df.apply()` applies a function across the given axis.\n\nWe can remove constant columns using `df.apply()` and `pd.Series.nunique()`."}}