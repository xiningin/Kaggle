{"cell_type":{"93e6495e":"code","750f7303":"code","8ba7bc36":"code","d6b1aa0b":"code","69072dbb":"code","cd9564e6":"code","e0867d3b":"code","3a63f518":"code","cc36bd08":"code","c4798d16":"code","319684b7":"code","f0e66740":"code","b26b7a99":"code","0a5f410a":"code","ab6c9c4b":"code","74c19990":"code","7364df33":"code","7a2045d0":"code","cbbb8deb":"code","5b0c7408":"code","729165a0":"code","85fe6903":"code","c9156db9":"code","73ba9cc7":"code","4ce2d39f":"code","45262756":"code","3c8b2d40":"code","8ed11b8d":"code","c73a5bdc":"code","2d4785ef":"code","fe8f66d3":"code","950a7176":"code","75b11674":"code","54e0223f":"code","138240cf":"code","8756052a":"code","6a00cefc":"code","811ea7b5":"markdown","a0c4ec3a":"markdown"},"source":{"93e6495e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","750f7303":"\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nimport glob\nimport scipy\nimport cv2\n\nimport keras","8ba7bc36":"\nimport random","d6b1aa0b":"\ntrain_data = pd.read_csv('..\/input\/train.csv')","69072dbb":"\ntrain_data.shape","cd9564e6":"\ntrain_data.head()","e0867d3b":"\ntrain_data.has_cactus.unique()","3a63f518":"\ntrain_data.has_cactus.hist()","cc36bd08":"\ntrain_data.has_cactus.value_counts()","c4798d16":"\ntrain_data.has_cactus.plot()","319684b7":"\ndef image_generator2(batch_size = 16, all_data=True, shuffle=True, train=True, indexes=None):\n    while True:\n        if indexes is None:\n            if train:\n                if all_data:\n                    indexes = np.arange(train_data.shape[0])\n                else:\n                    indexes = np.arange(train_data[:15000].shape[0])\n                if shuffle:\n                    np.random.shuffle(indexes)\n            else:\n                indexes = np.arange(train_data[15000:].shape[0])\n            \n        N = int(len(indexes) \/ batch_size)\n       \n\n        # Read in each input, perform preprocessing and get labels\n        for i in range(N):\n            current_indexes = indexes[i*batch_size: (i+1)*batch_size]\n            batch_input = []\n            batch_output = [] \n            for index in current_indexes:\n                img = mpimg.imread('..\/input\/train\/train\/' + train_data.id[index])\n                batch_input += [img]\n                batch_input += [img[::-1, :, :]]\n                batch_input += [img[:, ::-1, :]]\n                batch_input += [np.rot90(img)]\n                \n                temp_img = np.zeros_like(img)\n                temp_img[:28, :, :] = img[4:, :, :]\n                batch_input += [temp_img]\n                \n                temp_img = np.zeros_like(img)\n                temp_img[:, :28, :] = img[:, 4:, :]\n                batch_input += [temp_img]\n                \n                temp_img = np.zeros_like(img)\n                temp_img[4:, :, :] = img[:28, :, :]\n                batch_input += [temp_img]\n                \n                temp_img = np.zeros_like(img)\n                temp_img[:, 4:, :] = img[:, :28, :]\n                batch_input += [temp_img]\n                \n                batch_input += [cv2.resize(img[2:30, 2:30, :], (32, 32))]\n                \n                batch_input += [scipy.ndimage.interpolation.rotate(img, 10, reshape=False)]\n                \n                batch_input += [scipy.ndimage.interpolation.rotate(img, 5, reshape=False)]\n                \n                \n                for _ in range(11):\n                    batch_output += [train_data.has_cactus[index]]\n                \n            batch_input = np.array( batch_input )\n            batch_output = np.array( batch_output )\n        \n            yield( batch_input, batch_output.reshape(-1, 1) )","f0e66740":"positive_examples = train_data[train_data.has_cactus==1]\nnegative_examples = train_data[train_data.has_cactus==0]","b26b7a99":"def augment_img(img):\n    batch_input = []\n    batch_input += [img]\n    batch_input += [img[::-1, :, :]]\n    batch_input += [img[:, ::-1, :]]\n    batch_input += [np.rot90(img)]\n                \n    temp_img = np.zeros_like(img)\n    temp_img[:28, :, :] = img[4:, :, :]\n    batch_input += [temp_img]\n                \n    temp_img = np.zeros_like(img)\n    temp_img[:, :28, :] = img[:, 4:, :]\n    batch_input += [temp_img]\n                \n    temp_img = np.zeros_like(img)\n    temp_img[4:, :, :] = img[:28, :, :]\n    batch_input += [temp_img]\n                \n    temp_img = np.zeros_like(img)\n    temp_img[:, 4:, :] = img[:, :28, :]\n    batch_input += [temp_img]\n                \n    batch_input += [cv2.resize(img[2:30, 2:30, :], (32, 32))]\n                \n    batch_input += [scipy.ndimage.interpolation.rotate(img, 10, reshape=False)]\n                \n    batch_input += [scipy.ndimage.interpolation.rotate(img, 5, reshape=False)]\n    \n    return batch_input","0a5f410a":"\ndef image_generator(batch_size = 8, all_data=True, shuffle=True, train=True, indexes=None):\n    while True:\n        if indexes is None:\n            if train:\n                indexes = positive_examples.index.tolist()\n                neg_indexes = negative_examples.index.tolist()\n                if shuffle:\n                    np.random.shuffle(indexes)\n                    np.random.shuffle(neg_indexes)\n            \n        N = int(len(indexes) \/ (batch_size\/2))\n        neg_N = int(len(neg_indexes) \/ (batch_size\/2))\n       \n        j = 0\n\n        # Read in each input, perform preprocessing and get labels\n        for i in range(N):\n            current_indexes = indexes[i*(batch_size\/\/2): (i+1)*(batch_size\/\/2)]\n            current_neg_indexes = neg_indexes[j*(batch_size\/\/2): (j+1)*(batch_size\/\/2)]\n            j = (j + 1) % neg_N\n            batch_input = []\n            batch_output = [] \n            for ind in range(len(current_indexes)):\n                index = current_indexes[ind]\n                neg_index = current_neg_indexes[ind]\n                \n                img = mpimg.imread('..\/input\/train\/train\/' + train_data.id[index])\n                batch_input.extend(augment_img(img))\n                for _ in range(11):\n                    batch_output += [train_data.has_cactus[index]]\n                \n                neg_img = mpimg.imread('..\/input\/train\/train\/' + train_data.id[neg_index])\n                batch_input.extend(augment_img(neg_img))\n                for _ in range(11):\n                    batch_output += [train_data.has_cactus[neg_index]]\n                \n#                 factor = 0.05\n#                 new_img = factor*neg_img + (1-factor)*img\n#                 batch_input.append(new_img)\n#                 batch_output += [factor*train_data.has_cactus[neg_index]+(1-factor)*train_data.has_cactus[index]]\n                \n#                 factor = 0.95\n#                 new_img = factor*neg_img + (1-factor)*img\n#                 batch_input.append(new_img)\n#                 batch_output += [factor*train_data.has_cactus[neg_index]+(1-factor)*train_data.has_cactus[index]]\n            \n                \n                \n            batch_input = np.array( batch_input )\n            batch_output = np.array( batch_output )\n        \n            yield( batch_input, batch_output.reshape(-1, 1) )","ab6c9c4b":"model = keras.models.Sequential()\nmodel.add(keras.layers.Conv2D(64, (5, 5), input_shape=(32, 32, 3)))\nmodel.add(keras.layers.BatchNormalization())\nmodel.add(keras.layers.LeakyReLU(alpha=0.3))\n\nmodel.add(keras.layers.Conv2D(64, (5, 5)))\nmodel.add(keras.layers.BatchNormalization())\nmodel.add(keras.layers.LeakyReLU(alpha=0.3))\n\nmodel.add(keras.layers.Conv2D(128, (5, 5)))\nmodel.add(keras.layers.BatchNormalization())\nmodel.add(keras.layers.LeakyReLU(alpha=0.3))\n\nmodel.add(keras.layers.Conv2D(128, (5, 5)))\nmodel.add(keras.layers.BatchNormalization())\nmodel.add(keras.layers.LeakyReLU(alpha=0.3))\n\nmodel.add(keras.layers.Conv2D(256, (3, 3)))\nmodel.add(keras.layers.BatchNormalization())\nmodel.add(keras.layers.LeakyReLU(alpha=0.3))\n\nmodel.add(keras.layers.Conv2D(256, (3, 3)))\nmodel.add(keras.layers.BatchNormalization())\nmodel.add(keras.layers.LeakyReLU(alpha=0.3))\n\nmodel.add(keras.layers.Conv2D(512, (3, 3)))\nmodel.add(keras.layers.BatchNormalization())\nmodel.add(keras.layers.LeakyReLU(alpha=0.3))\n\nmodel.add(keras.layers.Flatten())\n\n\nmodel.add(keras.layers.Dense(100))\nmodel.add(keras.layers.BatchNormalization())\nmodel.add(keras.layers.LeakyReLU(alpha=0.3))\n\nmodel.add(keras.layers.Dense(1, activation='sigmoid'))","74c19990":"\nmodel.summary()","7364df33":"\nopt = keras.optimizers.SGD(lr=0.0001, momentum=0.9, nesterov=True)\nmodel.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy'])","7a2045d0":"def step_decay_schedule(initial_lr=1e-3, decay_factor=0.75, step_size=10):\n    '''\n    Wrapper function to create a LearningRateScheduler with step decay schedule.\n    '''\n    def schedule(epoch):\n        return initial_lr * (decay_factor ** np.floor(epoch\/step_size))\n    \n    return keras.callbacks.LearningRateScheduler(schedule)\n\nlr_sched = step_decay_schedule(initial_lr=1e-3, decay_factor=0.75, step_size=2)\nearly_stop = keras.callbacks.EarlyStopping(monitor='loss', patience=3)\n\nmodel.fit_generator(image_generator(), steps_per_epoch= train_data.shape[0] \/ 8, epochs=30, callbacks=[lr_sched, early_stop])","cbbb8deb":"# def step_decay_schedule(initial_lr=1e-3, decay_factor=0.75, step_size=10):\n#     '''\n#     Wrapper function to create a LearningRateScheduler with step decay schedule.\n#     '''\n#     def schedule(epoch):\n#         return initial_lr * (decay_factor ** np.floor(epoch\/step_size))\n    \n#     return keras.callbacks.LearningRateScheduler(schedule)\n\n# lr_sched = step_decay_schedule(initial_lr=1e-3, decay_factor=0.75, step_size=2)\n\n# model.fit_generator(image_generator(), steps_per_epoch= train_data.shape[0] \/ 8, epochs=20, callbacks=[lr_sched])","5b0c7408":"model.evaluate_generator(image_generator2(), steps=train_data.shape[0]\/\/16)","729165a0":"# model.evaluate_generator(image_generator(), steps=train_data.shape[0]\/\/8)","85fe6903":"\n# keras.backend.eval(model.optimizer.lr.assign(0.00001))","c9156db9":"\n# model.fit_generator(image_generator(), steps_per_epoch= train_data.shape[0] \/ 16, epochs=15)","73ba9cc7":"\nindexes = np.arange(train_data.shape[0])\nN = int(len(indexes) \/ 64)   \nbatch_size = 64\n\nwrong_ind = []\nfor i in range(N):\n            current_indexes = indexes[i*64: (i+1)*64]\n            batch_input = []\n            batch_output = [] \n            for index in current_indexes:\n                img = mpimg.imread('..\/input\/train\/train\/' + train_data.id[index])\n                batch_input += [img]\n                batch_output.append(train_data.has_cactus[index])\n            \n            batch_input = np.array( batch_input )\n#             batch_output = np.array( batch_output )\n\n            model_pred = model.predict_classes(batch_input)\n            for j in range(len(batch_output)):\n                if model_pred[j] != batch_output[j]:\n                    wrong_ind.append(i*batch_size+j)","4ce2d39f":"\nlen(wrong_ind)","45262756":"\nindexes = np.arange(train_data.shape[0])\nN = int(len(indexes) \/ 64)   \nbatch_size = 64\n\nwrong_ind = []\nfor i in range(N):\n            current_indexes = indexes[i*64: (i+1)*64]\n            batch_input = []\n            batch_output = [] \n            for index in current_indexes:\n                img = mpimg.imread('..\/input\/train\/train\/' + train_data.id[index])\n                batch_input += [img[::-1, :, :]]\n                batch_output.append(train_data.has_cactus[index])\n            \n            batch_input = np.array( batch_input )\n\n            model_pred = model.predict_classes(batch_input)\n            for j in range(len(batch_output)):\n                if model_pred[j] != batch_output[j]:\n                    wrong_ind.append(i*batch_size+j)","3c8b2d40":"\nlen(wrong_ind)","8ed11b8d":"\nindexes = np.arange(train_data.shape[0])\nN = int(len(indexes) \/ 64)   \nbatch_size = 64\n\nwrong_ind = []\nfor i in range(N):\n            current_indexes = indexes[i*64: (i+1)*64]\n            batch_input = []\n            batch_output = [] \n            for index in current_indexes:\n                img = mpimg.imread('..\/input\/train\/train\/' + train_data.id[index])\n                batch_input += [img[:, ::-1, :]]\n                batch_output.append(train_data.has_cactus[index])\n            \n            batch_input = np.array( batch_input )\n\n            model_pred = model.predict_classes(batch_input)\n            for j in range(len(batch_output)):\n                if model_pred[j] != batch_output[j]:\n                    wrong_ind.append(i*batch_size+j)","c73a5bdc":"\nlen(wrong_ind)","2d4785ef":"\n!ls ..\/input\/test\/test\/* | wc -l","fe8f66d3":"\ntest_files = os.listdir('..\/input\/test\/test\/')","950a7176":"\nlen(test_files)","75b11674":"\nbatch = 40\nall_out = []\nfor i in range(int(4000\/batch)):\n    images = []\n    for j in range(batch):\n        img = mpimg.imread('..\/input\/test\/test\/'+test_files[i*batch + j])\n        images += [img]\n    out = model.predict(np.array(images))\n    all_out += [out]","54e0223f":"\nall_out = np.array(all_out).reshape((-1, 1))\n","138240cf":"\nall_out.shape","8756052a":"\nsub_file = pd.DataFrame(data = {'id': test_files, 'has_cactus': all_out.reshape(-1).tolist()})","6a00cefc":"\nsub_file.to_csv('sample_submission.csv', index=False)","811ea7b5":"**Model**","a0c4ec3a":"**Exploration**\n"}}