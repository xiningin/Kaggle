{"cell_type":{"9f9ffc4b":"code","85ff0c15":"code","79df53bf":"code","839d5859":"code","4fd55eac":"code","2124c6a5":"code","73f1df3b":"code","3dba85f3":"markdown","b150a0c8":"markdown","0d8282f8":"markdown","d203542b":"markdown","220bb74a":"markdown"},"source":{"9f9ffc4b":"import pandas as pd\nimport numpy as np\n\nimport tensorflow as tf\nimport tensorflow.keras.backend as K\nimport transformers\nfrom transformers import *\nfrom sklearn import metrics\nfrom sklearn.model_selection import KFold\n\n\nprint('Transformers version: ', transformers.__version__)\nprint('Tensorflow version: ', tf.__version__)","85ff0c15":"data_dir = '\/kaggle\/input\/nlp-getting-started\/'\ntrain_df = pd.read_csv(data_dir+'train.csv')\ntest_df = pd.read_csv(data_dir+'test.csv')\ntrain_df = train_df.sample(n=len(train_df), random_state=42)\nsample_submission = pd.read_csv(data_dir+'sample_submission.csv')\nprint(train_df['target'].value_counts())\ntrain_df.head(2)","79df53bf":"from nltk.tokenize.treebank import TreebankWordTokenizer\ntree_tokenizer = TreebankWordTokenizer()\ndef get_tree_tokens(x):\n    x = tree_tokenizer.tokenize(x)\n    x = ' '.join(x)\n    return x\ntrain_df.text = train_df.text.apply(get_tree_tokens)\ntest_df.text = test_df.text.apply(get_tree_tokens)","839d5859":"# from: https:\/\/www.kaggle.com\/utsavnandi\/roberta-using-huggingface-tf-implementation\ndef to_tokens(input_text, tokenizer):\n    output = tokenizer.encode_plus(input_text, max_length=90, pad_to_max_length=True)\n    return output\n\ndef select_field(features, field):\n    return [feature[field] for feature in features]\n\nimport re\ndef clean_tweet(tweet):\n    # Removing the @\n    #tweet = re.sub(r\"@[A-Za-z0-9]+\", ' ', tweet)\n    # Removing the URL links\n    #tweet = re.sub(r\"https?:\/\/[A-Za-z0-9.\/]+\", ' ', tweet)\n    # Keeping only letters\n    #tweet = re.sub(r\"[^a-zA-Z.!?']\", ' ', tweet)\n    # Removing additional whitespaces\n    tweet = re.sub(r\" +\", ' ', tweet)\n    return tweet\n\ndef preprocess_data(tokenizer, train_df, test_df):\n    train_text = train_df['text'].apply(clean_tweet)\n    test_text = test_df['text'].apply(clean_tweet)\n    train_encoded = train_text.apply(lambda x: to_tokens(x, tokenizer))\n    test_encoded = test_text.apply(lambda x: to_tokens(x, tokenizer))\n\n    #create attention masks\n    input_ids_train = np.array(select_field(train_encoded, 'input_ids'))\n    attention_masks_train = np.array(select_field(train_encoded, 'attention_mask'))\n\n    input_ids_test = np.array(select_field(test_encoded, 'input_ids'))\n    attention_masks_test = np.array(select_field(test_encoded, 'attention_mask'))\n\n    # concatonate masks\n    train_X = [input_ids_train, attention_masks_train]\n    test_X = [input_ids_test, attention_masks_test]\n    #OHE target\n    train_y = tf.keras.utils.to_categorical(train_df['target'].values.reshape(-1, 1))\n\n    return train_X, train_y, test_X","4fd55eac":"# code from https:\/\/github.com\/huggingface\/transformers\n# Transformers has a unified API\n# for 10 transformer architectures and 30 pretrained weights.\n#          Model          | Tokenizer          | Pretrained weights shortcut\ndef load_pretrained_model(model_class='bert', model_name='bert-base-cased', task='binary', learning_rate=3e-5, epsilon=1e-8, lower_case=False):\n  MODEL_CLASSES = {\n    \"bert\": (BertConfig, TFBertForSequenceClassification, BertTokenizer),\n    \"xlnet\": (XLNetConfig, TFXLNetForSequenceClassification, XLNetTokenizer),\n    \"xlm\": (XLMConfig, TFXLMForSequenceClassification, XLMTokenizer),\n    \"roberta\": (RobertaConfig, TFRobertaForSequenceClassification, RobertaTokenizer),\n    \"distilbert\": (DistilBertConfig, TFDistilBertForSequenceClassification, DistilBertTokenizer),\n    \"albert\": (AlbertConfig, TFAlbertForSequenceClassification, AlbertTokenizer),\n    #\"xlmroberta\": (XLMRobertaConfig, XLMRobertaForSequenceClassification, XLMRobertaTokenizer), No tensorflow version yet\n  }\n  model_metrics = [\n        tf.keras.metrics.TruePositives(name='tp'),\n        tf.keras.metrics.FalsePositives(name='fp'),\n        tf.keras.metrics.TrueNegatives(name='tn'),\n        tf.keras.metrics.FalseNegatives(name='fn'), \n        tf.keras.metrics.BinaryAccuracy(name='accuracy'),\n        tf.keras.metrics.Precision(name='precision'),\n        tf.keras.metrics.Recall(name='recall'),\n        tf.keras.metrics.AUC(name='auc'),\n  ]\n\n  \n  config_class, model_class, tokenizer_class = MODEL_CLASSES[model_class]\n\n  config = config_class.from_pretrained(model_name, num_labels=2, finetuning_task=task)\n\n\n  model = model_class.from_pretrained(model_name)\n  optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate, epsilon=epsilon, clipnorm=1.0)\n  loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n  metric = tf.keras.metrics.BinaryAccuracy('accuracy')\n  model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n  #model.summary()\n\n  tokenizer = tokenizer_class.from_pretrained(model_name, lower_case = lower_case)\n\n  return config, model, tokenizer","2124c6a5":"# load model, process data for model\n_, _, tokenizer = load_pretrained_model(model_class='roberta', model_name='roberta-base', learning_rate=2e-5, lower_case=False)\ntrain_X, train_y, test_X = preprocess_data(tokenizer=tokenizer, train_df=train_df, test_df=test_df)\n\n\nkf = KFold(n_splits=6)\ntest_preds = []\ni = 0\nfor train_idx, test_idx in kf.split(train_X[0]):\n    i+=1\n    if i not in [1, 5]: #only do 2 folds to save time\n        continue\n    train_split_X = [train_X[i][train_idx] for i in range(len(train_X))]\n    test_split_X = [train_X[i][test_idx] for i in range(len(train_X))]\n\n    train_split_y = train_y[train_idx]\n    test_split_y = train_y[test_idx]\n    #create class weights to account for inbalance\n    positive = train_df.iloc[train_idx, :].target.value_counts()[0]\n    negative = train_df.iloc[train_idx, :].target.value_counts()[1]\n    pos_weight = positive \/ (positive + negative)\n    neg_weight = negative \/ (positive + negative)\n\n    class_weight = [{0:pos_weight, 1:neg_weight}, {0:neg_weight, 1:pos_weight}]\n\n    K.clear_session()\n    config, model, tokenizer = load_pretrained_model(model_class='roberta', model_name='roberta-base', learning_rate=2e-5, lower_case=False)\n\n    # fit, test model\n    model.fit(train_split_X, train_split_y, batch_size=64, epochs=3, class_weight=class_weight, validation_data=(test_split_X, test_split_y))\n\n    val_preds = model.predict(test_split_X, batch_size=32, verbose=1)\n    val_preds = np.argmax(val_preds, axis=1).flatten()\n    print(metrics.accuracy_score(train_df.iloc[test_idx, :].target.values, val_preds))\n\n    preds1 = model.predict(test_X, batch_size=32, verbose=1)\n    test_preds.append(preds1)","73f1df3b":"test_preds2 = np.average(test_preds, axis=0)\ntest_preds3 = np.argmax(test_preds2, axis=1).flatten()\nsample_submission['target'] = test_preds3\nsample_submission['target'].value_counts()\nsample_submission.to_csv('new_submission.csv', index=False)","3dba85f3":"# Output Predictions","b150a0c8":"# Function to load models","0d8282f8":"# Data Prep Functions","d203542b":"# Import Data","220bb74a":"# Train Model"}}