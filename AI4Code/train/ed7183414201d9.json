{"cell_type":{"cb172a09":"code","bbe0e614":"code","a524a409":"code","46c02209":"code","7dd31bd2":"code","57a6572c":"code","9a278444":"code","7617dad9":"code","3e8d71ff":"code","e9a2ea35":"code","e9ee12e1":"code","bea3bfc3":"code","6b7bb615":"code","5232f286":"code","020407e5":"code","e7dd383e":"code","a4e60964":"code","24c5a511":"code","301cf81a":"code","06edb45e":"code","f9406833":"code","cd9acc2c":"code","bf66d550":"code","3b4c94c5":"code","db31d6c8":"code","201f8951":"code","e6af3b78":"code","3cf94973":"code","faf8d8cd":"code","3b845641":"code","a938b186":"code","3aa7a584":"code","090facd1":"code","25d87e5a":"code","a65bc85d":"code","0ccfa42c":"code","0843ab9f":"code","089a0ab1":"code","d39d5f81":"code","7d8298ce":"code","96795cfe":"code","4a6b9e88":"code","b4a2baf1":"code","d4a95090":"code","d8b25dc6":"code","b6fe6a24":"code","4cf565f4":"code","68a31393":"code","c82666eb":"code","7cc1d1f7":"code","fde4e6ef":"code","a90fd04c":"markdown","aa9658da":"markdown","99e8fcb3":"markdown","b6599e01":"markdown","b7cd7a91":"markdown","7547b34c":"markdown","727de8fc":"markdown","bc1c0f43":"markdown","9c1fbce9":"markdown","e7ecc823":"markdown","3f9ecc08":"markdown","989a6ba7":"markdown","e3b2608d":"markdown","8be66840":"markdown","3ce737e4":"markdown","a6bd42d4":"markdown","82933dd6":"markdown","12b58fbf":"markdown","0a5ac557":"markdown","517fca69":"markdown","c3aa6e20":"markdown","3a085600":"markdown","613351a0":"markdown","15a8c3e6":"markdown","0f4ece7f":"markdown","abe022ed":"markdown","8c8eec28":"markdown","ca04ad72":"markdown","d7e88cf8":"markdown","b03e706b":"markdown","03e05e0c":"markdown","580c7881":"markdown","2d71294f":"markdown","988605fa":"markdown","d75016c4":"markdown","4e012524":"markdown","06ccea1c":"markdown","f0252978":"markdown","17aa2c92":"markdown","e37554df":"markdown","70e4c73e":"markdown","8602bd72":"markdown","3484a0a4":"markdown","3daa2921":"markdown","af279723":"markdown","7e6621d8":"markdown","87b4f3f3":"markdown","e9694bf4":"markdown","15c3202a":"markdown","76c72fed":"markdown","5e1a58ff":"markdown"},"source":{"cb172a09":"# Data Manipulation\nimport pandas as pd\npd.options.mode.chained_assignment = None\nimport numpy as np\nimport missingno as msno\n\n# Visualization\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\n\n# Machine Learning\nfrom sklearn.model_selection import train_test_split, KFold, cross_val_score\nfrom sklearn import model_selection, metrics\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler, RobustScaler, MaxAbsScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.linear_model import LinearRegression, Ridge, Lasso\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor, AdaBoostRegressor\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\n\n# Other packages\nfrom scipy.stats import norm\nfrom scipy import stats\n\n# Import data\ndf_salary = pd.read_csv('..\/input\/nba-player-salaries-as-at-2020\/NBA Players Salaries 1920.csv')\ndf_stats1718 = pd.read_csv('..\/input\/nba-players-stats-2016-2017\/NBA Players Stats 201718.csv')\ndf_stats1819 = pd.read_csv('..\/input\/nba-players-stats-2016-2017\/NBA Players Stats 201819.csv')\ndf_list = [df_stats1718, df_stats1819, df_salary]","bbe0e614":"# Salary\ndf_salary.head()","a524a409":"# Season Stats 17\/18\ndf_stats1718.head()","46c02209":"# Season Stats 18\/19\ndf_stats1819.head()","7dd31bd2":"# Correct player names\nfor df in df_list:\n    df[['Player', 'Del']] = df.Player.str.split(\"\\\\\", expand = True)\n\ndf_stats1718 = df_stats1718.drop(['Del'], axis = 1)\ndf_stats1819 = df_stats1819.drop(['Del'], axis = 1)\ndf_salary = df_salary.drop(['Del'], axis = 1)","57a6572c":"# Delete $ signs and turn column into float\ndf_salary['2019-20'] = df_salary['2019-20'].str[:-2].astype(float)\n\n# Rename salary column\ndf_salary = df_salary.rename(columns = {'2019-20': 'Salary 19\/20'})\n\n# Transform salary to 1000\ndf_salary['Salary 19\/20'] = df_salary['Salary 19\/20']\/1000","9a278444":"# As total stats always is in the top row we can simply use the drop_duplicates function\ndf_stats1718 = df_stats1718.drop_duplicates(['Player'])\ndf_stats1819 = df_stats1819.drop_duplicates(['Player'])","7617dad9":"# Add season year to corresponding columns\ncolumns_renamed = [s + ' 17\/18' for s in list(df_stats1718.columns)]\ndf_stats1718.columns = list(df_stats1718.columns)[:3] + columns_renamed[3:]\n\ncolumns_renamed = [s + ' 18\/19' for s in list(df_stats1819.columns)]\ndf_stats1819.columns = list(df_stats1819.columns)[:3] + columns_renamed[3:]\n\n# Delete Pos column from 17\/18 df; we need it only once\ndf_stats1718 = df_stats1718.drop('Pos', axis = 1)","3e8d71ff":"# Merge datasets\ndf_stats = df_stats1718.merge(df_stats1819, how = 'outer',left_on = ['Player'],right_on = ['Player'])\ndf = df_stats.merge(df_salary, how = 'outer', left_on = ['Player'],right_on = ['Player'])\n\ndf.head()","e9a2ea35":"len(df)","e9ee12e1":"df.dtypes","bea3bfc3":"# Columns of dataset\ndf.columns","6b7bb615":"# Drop unnecessary columns\ndf = df.drop(['Rk_x', 'Rk_y', 'Rk', 'Tm', '2020-21', '2021-22', '2022-23', '2023-24', '2024-25',\n              'Signed Using', 'Guaranteed'], axis = 1)","5232f286":"# Number of missing values for each column\ndf.isnull().sum()","020407e5":"# Drop rows with NaN\n# Create Dataframe without stats for season 17\/18\ndf1 = df.dropna(subset = ['Salary 19\/20', 'PTS 18\/19', 'eFG% 18\/19'])\ndf1 = df1.reset_index()\ncolumns = list(df1.columns)\nfor i in columns:\n    if '17\/18' in i:\n        df1 = df1.drop([i], axis = 1)\ndf1 = df1.reset_index()\n\n# Create Dataframe with stats for season 17\/18\ndf2 = df.dropna(subset = ['Salary 19\/20', 'PTS 18\/19', 'eFG% 18\/19', 'PTS 17\/18', 'eFG% 17\/18'])\ndf2 = df2.reset_index()","e7dd383e":"# Get unique positions\nprint(df1.Pos.unique())\nprint(df2.Pos.unique())","a4e60964":"# Replace duplicate positions with first position.\ndf1 = df1.replace({'SF-SG': 'SF', 'PF-SF': 'PF', 'SG-PF': 'SG', 'C-PF': 'C', 'SG-SF': 'SG'})\ndf2 = df2.replace({'SF-SG': 'SF', 'PF-SF': 'PF', 'SG-PF': 'SG', 'C-PF': 'C', 'SG-SF': 'SG'})","24c5a511":"# Get absolute growth\n# List of stats of which we want the growth\nlist_growth = ['eFG%','TRB', 'AST', 'STL', 'BLK', 'TOV', 'PF', 'PTS']\n\n# Add absolute growth columns\nfor i in list_growth:\n    df2[i + ' +-'] = df2[i + ' 18\/19'] - df2[i + ' 17\/18']\n    \n# Drop 17\/18 columns\ncolumns = list(df2.columns)\nfor i in columns:\n    if '17\/18' in i:\n        df2 = df2.drop([i], axis = 1)","301cf81a":"print(df1.shape)\nprint(df2.shape)","06edb45e":"# Setup dataframe\ndf_sal = df1[['Player', 'Salary 19\/20']]\ndf_sal.sort_values(by = 'Salary 19\/20', ascending = False, inplace = True)\n\n# Create barchart\nsns.catplot(x = 'Player', y = 'Salary 19\/20', kind = 'bar', data = df_sal.head()).set(xlabel = None)\nplt.title('Players with highest salary (in 1000)')\nplt.ylim([35000, 40000])\nplt.xticks(rotation = 90)","f9406833":"# Statistics summary\ndf1['Salary 19\/20'].describe()","cd9acc2c":"# Histogram\nsns.distplot(df1['Salary 19\/20'])","bf66d550":"# Setup dataframes\ndf_pts = df1[['Player', 'PTS 18\/19']]\ndf_pts.sort_values(by = 'PTS 18\/19', ascending = False, inplace = True)\ndf_ast = df1[['Player', 'AST 18\/19']]\ndf_ast.sort_values(by = 'AST 18\/19', ascending = False, inplace = True)\ndf_stl = df1[['Player', 'STL 18\/19']]\ndf_stl.sort_values(by = 'STL 18\/19', ascending = False, inplace = True)\ndf_trb = df1[['Player', 'TRB 18\/19']]\ndf_trb.sort_values(by = 'TRB 18\/19', ascending = False, inplace = True)\n\n# Set up figure\nf, axes = plt.subplots(2, 2, figsize=(20, 15))\nsns.despine(left=True)\n\n# Create barcharts\nsns.barplot(x = 'PTS 18\/19', y = 'Player', data = df_pts.head(), color = \"b\", ax = axes[0, 0]).set(ylabel = None)\nsns.barplot(x = 'AST 18\/19', y = 'Player', data = df_ast.head(), color = \"r\", ax = axes[0, 1]).set(ylabel = None)\nsns.barplot(x = 'STL 18\/19', y = 'Player', data = df_stl.head(), color = \"g\", ax = axes[1, 0]).set(ylabel = None)\nsns.barplot(x = 'TRB 18\/19', y = 'Player', data = df_trb.head(), color = \"m\", ax = axes[1, 1]).set(ylabel = None)","3b4c94c5":"# Set up figure\nf, axes = plt.subplots(2, 2, figsize=(20, 15))\nsns.despine(left=True)\n\n# Histograms\nsns.distplot(df1['PTS 18\/19'], color = \"b\", ax = axes[0, 0])\nsns.distplot(df1['AST 18\/19'], color = \"r\", ax = axes[0, 1])\nsns.distplot(df1['STL 18\/19'], color = \"g\", ax = axes[1, 0])\nsns.distplot(df1['TRB 18\/19'], color = \"m\", ax = axes[1, 1])","db31d6c8":"# Set up figure\nf, axes = plt.subplots(2, 2, figsize=(20, 15))\n\n# Regressionplot\nsns.regplot(x = df1['PTS 18\/19'], y = df1['Salary 19\/20'], color=\"b\", ax=axes[0, 0])\nsns.regplot(x = df1['AST 18\/19'], y = df1['Salary 19\/20'], color=\"r\", ax=axes[0, 1])\nsns.regplot(x = df1['STL 18\/19'], y = df1['Salary 19\/20'], color=\"g\", ax=axes[1, 0])\nsns.regplot(x = df1['TRB 18\/19'], y = df1['Salary 19\/20'], color=\"m\", ax=axes[1, 1])","201f8951":"# Relationship with effecitve field goal percentage\nsns.regplot(x = df1['eFG% 18\/19'], y = df1['Salary 19\/20'])","e6af3b78":"# Relationship with minutes played per game\nsns.regplot(x = df1['MP 18\/19'], y = df1['Salary 19\/20'])","3cf94973":"# Relationship with age\nsns.regplot(x = df1['Age 18\/19'], y = df1['Salary 19\/20'])","faf8d8cd":"# Relationship with Position\nsns.boxplot(x = 'Pos', y = 'Salary 19\/20', data = df1, order = ['PG', 'SG', 'SF', 'PF', 'C'])","3b845641":"sns.set(style = \"white\")\ncor_matrix = df1.loc[:, 'Age 18\/19': 'Salary 19\/20'].corr()\n\n# Generate a mask for the upper triangle\nmask = np.triu(np.ones_like(cor_matrix, dtype = np.bool))\n\nplt.figure(figsize = (15, 12))\n\n# Generate a custom diverging colormap\ncmap = sns.diverging_palette(220, 10, as_cmap = True)\n\nsns.heatmap(cor_matrix, mask = mask, cmap = cmap, center = 0,\n            square = True, linewidths = .5, cbar_kws = {\"shrink\": .5})","a938b186":"cor_matrix = df2.loc[:, ['eFG% +-','TRB +-', 'AST +-', 'STL +-', 'BLK +-', 'TOV +-', 'PF +-', 'PTS +-', \n                        'Salary 19\/20']].corr()\n\n# Generate a mask for the upper triangle\nmask = np.triu(np.ones_like(cor_matrix, dtype = np.bool))\n\nplt.figure(figsize = (10, 8))\n\n# Generate a custom diverging colormap\ncmap = sns.diverging_palette(220, 10, as_cmap = True)\n\nsns.heatmap(cor_matrix, mask = mask, cmap = cmap, center = 0,\n            square = True, linewidths = .5, cbar_kws = {\"shrink\": .5})","3aa7a584":"y = df1.loc[:, 'Salary 19\/20']\n\nx = df1.loc[:, ['Pos', 'Age 18\/19', 'G 18\/19', 'GS 18\/19', 'MP 18\/19', 'FG 18\/19', 'FGA 18\/19',\n                'FG% 18\/19', '3P 18\/19', '3PA 18\/19', '2P 18\/19', '2PA 18\/19', '2P% 18\/19', \n                'eFG% 18\/19', 'FT 18\/19', 'FTA 18\/19', 'ORB 18\/19', 'DRB 18\/19', 'TRB 18\/19', \n                'AST 18\/19', 'STL 18\/19', 'BLK 18\/19', 'TOV 18\/19', 'PF 18\/19', 'PTS 18\/19']] \n\nprint(x.shape)\nprint(y.shape)","090facd1":"# Instantiate OneHotEncoder\nohe = OneHotEncoder(categories = [['PG', 'SG', 'SF', 'PF', 'C']])\n\n# Apply one-hot encoder\nx_ohe = pd.DataFrame(ohe.fit_transform(x['Pos'].to_frame()).toarray())\n\n# Get feature names\nx_ohe.columns = ohe.get_feature_names(['Pos'])\n\n# One-hot encoding removed index; put it back\nx_ohe.index = x.index\n\n# Add one-hot encoded columns to numerical features and remove categorical column\nx = pd.concat([x, x_ohe], axis=1).drop(['Pos'], axis=1)\n\n# How does it look like?\nx.head()","25d87e5a":"# Split data using train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 0)\n\nprint(x_train.shape)\nprint(y_train.shape)\nprint(x_test.shape)\nprint(y_test.shape)","a65bc85d":"#Apply cube-root transformation\ny_train = pd.DataFrame(np.cbrt([y_train])).T\ny_test = pd.DataFrame(np.cbrt([y_test])).T\ny = pd.DataFrame(np.cbrt([y])).T\n\n#transformed histogram and normal probability plot\nf, axes = plt.subplots(1, 2, figsize = (10, 5), sharex = True)\nsns.distplot(y_train, color = \"skyblue\", fit = norm, ax = axes[0], axlabel = \"y_train\")\nsns.distplot(y_test, color = \"olive\",fit = norm, ax = axes[1], axlabel = \"y_test\")\n#sns.distplot(y, color = \"olive\",fit = norm, axlabel = \"y\")","0ccfa42c":"# Use Robustscaler\n\n#scaler = RobustScaler()\n#x_train_scaled = pd.DataFrame(scaler.fit_transform(x_train), index = x_train.index, columns = x_train.columns)\n#x_test_scaled = pd.DataFrame(scaler.transform(x_test), index = x_test.index, columns = x_test.columns)\n\n#x_train_scaled.head()","0843ab9f":"# Function which uses an algorithm as input and returns the desired accuracy metrics and some predictions\n\ndef alg_fit(alg, x_train, y_train, x_test, name, y_true, df, mse, r2):\n    \n    # Model selection\n    mod = alg.fit(x_train, y_train)\n    \n    # Prediction\n    y_pred = mod.predict(x_test)\n    \n    # Accuracy\n    acc1 = round(mse(y_test, y_pred), 4)\n    acc2 = round(r2(y_test, y_pred), 4)\n    \n    # Accuracy table\n    x_test['y_pred'] = mod.predict(x_test)\n    df_acc = pd.merge(df, x_test, how = 'right')\n    x_test.drop(['y_pred'], axis = 1, inplace = True)\n    df_acc = df_acc[[name, y_true, 'y_pred']]\n    df_acc.sort_values(by = y_true, ascending = False, inplace = True)\n    df_acc['y_pred'] = df_acc['y_pred']**3\n    \n    return y_pred, acc1, acc2, df_acc","089a0ab1":"# Linear Regression\ny_pred_lin, mse_lin, r2_lin, df_acc_lin = alg_fit(LinearRegression(), x_train, y_train, x_test, 'Player', 'Salary 19\/20', \n                                                  df1, metrics.mean_squared_error, metrics.r2_score)\n\nprint(\"Root Mean Squared Error: %s\" % round(np.sqrt(mse_lin), 4))\nprint(\"R-squared: %s\" % r2_lin)\ndf_acc_lin.head(10)","d39d5f81":"# Ridge Regression\ny_pred_rid, mse_rid, r2_rid, df_acc_rid = alg_fit(Ridge(alpha = 1), x_train, y_train, x_test, 'Player', 'Salary 19\/20',\n                                                  df1, metrics.mean_squared_error, metrics.r2_score)\n\nprint(\"Root Mean Squared Error: %s\" % round(np.sqrt(mse_rid), 4))\nprint(\"R-squared: %s\" % r2_rid)\ndf_acc_rid.head(10)","7d8298ce":"# Lasso Regression\ny_pred_las, mse_las, r2_las, df_acc_las = alg_fit(Lasso(alpha = 0.001), x_train, y_train, x_test, 'Player', 'Salary 19\/20',\n                                                  df1, metrics.mean_squared_error, metrics.r2_score)\n\nprint(\"Root Mean Squared Error: %s\" % round(np.sqrt(mse_las), 4))\nprint(\"R-squared: %s\" % r2_las)\ndf_acc_las.head(10)","96795cfe":"def alg_fit_cv(alg, x, y, mse, r2):\n    \n    # Cross validation\n    cv = KFold(shuffle = True, random_state = 0, n_splits = 5)\n    \n    # Accuracy\n    scores1 = cross_val_score(alg, x, y, cv = cv, scoring = mse)\n    scores2 = cross_val_score(alg, x, y, cv = cv, scoring = r2)\n    acc1_cv = round(scores1.mean(), 4)\n    acc2_cv = round(scores2.mean(), 4)\n    \n    return acc1_cv, acc2_cv","4a6b9e88":"# Linear Regression\n\nmse_cv_lin, r2_cv_lin = alg_fit_cv(LinearRegression(), x, y, 'neg_mean_squared_error', 'r2')\n\nprint(\"Root Mean Squared Error: %s\" % round(np.sqrt(mse_cv_lin*-1), 4))\nprint(\"R-squared: %s\" % r2_cv_lin)","b4a2baf1":"# Ridge Regression\nmse_cv_rid, r2_cv_rid = alg_fit_cv(Ridge(alpha = 23), x, y, 'neg_mean_squared_error', 'r2')\n\nprint(\"Root Mean Squared Error: %s\" % round(np.sqrt(mse_cv_rid*-1), 4))\nprint(\"R-squared: %s\" % r2_cv_rid)","d4a95090":"# Lasso Regression\nmse_cv_las, r2_cv_las = alg_fit_cv(Ridge(alpha = 23), x, y, 'neg_mean_squared_error', 'r2')\n\nprint(\"Root Mean Squared Error: %s\" % round(np.sqrt(mse_cv_las*-1), 4))\nprint(\"R-squared: %s\" % r2_cv_las)","d8b25dc6":"# LightGBM Regressor (after some parameter tuning)\nlgbm = LGBMRegressor(objective = 'regression',\n                     num_leaves = 20,\n                     learning_rate = 0.03,\n                     n_estimators = 200,\n                     max_bin = 50,\n                     bagging_fraction = 0.85,\n                     bagging_freq = 4,\n                     bagging_seed = 6,\n                     feature_fraction = 0.2,\n                     feature_fraction_seed = 7,\n                     verbose = -1)\n\nmse_cv_lgbm, r2_cv_lgbm = alg_fit_cv(lgbm, x, y, 'neg_mean_squared_error', 'r2')\n\nprint(\"Root Mean Squared Error: %s\" % round(np.sqrt(mse_cv_lgbm*-1), 4))\nprint(\"R-squared: %s\" % r2_cv_lgbm)","b6fe6a24":"# XGB-Regressor (after some parameter tuning)\nxgb = XGBRegressor(n_estimators = 300,\n                   max_depth = 2,\n                   min_child_weight = 0,\n                   gamma = 8,\n                   subsample = 0.6,\n                   colsample_bytree = 0.9,\n                   objective = 'reg:squarederror',\n                   nthread = -1,\n                   scale_pos_weight = 1,\n                   seed = 27,\n                   learning_rate = 0.02,\n                   reg_alpha = 0.006)\n\nmse_cv_xgb, r2_cv_xgb = alg_fit_cv(xgb, x, y, 'neg_mean_squared_error', 'r2')\n\nprint(\"Root Mean Squared Error: %s\" % round(np.sqrt(mse_cv_xgb*-1), 4))\nprint(\"R-squared: %s\" % r2_cv_xgb)","4cf565f4":"# Merge y and x back together\n#df_new = pd.concat([y, x], axis=1)\n\n# Compute Z-score for the dataframe\n#z = np.abs(stats.zscore(df_new))\n\n# Delete rows with outliers\n#df_new = df_new[(z < 4).all(axis = 1)].reset_index()\n\n# Split into y and x again\n#y_new = df_new.loc[:, 0]\n#x_new = df_new.loc[:, 'PTS 18\/19':] \n#print(x_new.shape)\n#print(y_new.shape)","68a31393":"# Model\nmod = xgb.fit(x, y)\n\n# Feature importance\ndf_feature_importance = pd.DataFrame(xgb.feature_importances_, index = x.columns, \n                                     columns = ['feature importance']).sort_values('feature importance', \n                                                                                   ascending = False)\ndf_feature_importance","c82666eb":"# Drop out features with low importance or which are redundant\nx_new = x.loc[:, ['PTS 18\/19', 'Pos_PG', 'Pos_SG', 'Pos_SF', 'Pos_PF', 'Pos_C', 'Age 18\/19', 'STL 18\/19', \n                  'G 18\/19', 'TRB 18\/19', 'AST 18\/19', 'PF 18\/19', 'MP 18\/19']]","7cc1d1f7":"# XGB-Regressor (after some parameter tuning)\nxgb_new = XGBRegressor(n_estimators = 270,\n                       max_depth = 2,\n                       min_child_weight = 0,\n                       gamma = 18,\n                       subsample = 0.7,\n                       colsample_bytree = 0.9,\n                       objective = 'reg:squarederror',\n                       nthread = -1,\n                       scale_pos_weight = 1,\n                       seed = 27,\n                       learning_rate = 0.023,\n                       reg_alpha = 0.02)\n\nmse_cv_xgb, r2_cv_xgb = alg_fit_cv(xgb_new, x_new, y, 'neg_mean_squared_error', 'r2')\n\nprint(\"Root Mean Squared Error: %s\" % round(np.sqrt(mse_cv_xgb*-1), 4))\nprint(\"R-squared: %s\" % r2_cv_xgb)","fde4e6ef":"# Split data now with x_new\nx_train, x_test, y_train, y_test = train_test_split(x_new, y, test_size = 0.2, random_state = 0)\n\n# Use function to fit algorithm\ny_pred_xgb, mse_xgb, r2_xgb, df_acc_xgb = alg_fit(xgb_new, x_train, y_train, x_test, 'Player', 'Salary 19\/20', \n                                                  df1, metrics.mean_squared_error, metrics.r2_score)\n\nprint(\"Root Mean Squared Error: %s\" % round(np.sqrt(mse_xgb), 4))\nprint(\"R-squared: %s\" % r2_xgb)\ndf_acc_xgb.head(10)","a90fd04c":"**What are the leaders in each category?**","aa9658da":"**Scaling features**\n\nFor some of our following algorithms it is important that our features are scaled.","99e8fcb3":"How does the distribution look like?","b6599e01":"**We do not need the currency for Salary and we need to rename the salary column**\n\nIn this analysis we will only need the Salary of the season 19\/20. That is why we will only change the mentioned aspects for salary 19\/20.","b7cd7a91":"As one would expect there is a positive linear relationship between all these features and the salary.","7547b34c":"**We only want the player names in the column \"Player\"**","727de8fc":"# 3. Data Preparation\n\n**Define target variable and features**","bc1c0f43":"**New features**\n\nWe will use the features with high importance. Some features can't be used as they are redundant or close to another feature. For example field goals per game (FG 18\/19) is not necessary as we already have pionts per game (PTS 18\/19).","9c1fbce9":"**What is the length of this dataset?**","e7ecc823":"# 4. Building Machine Learning Models\n\n## Basic machine learning algorithms\n\nWe will set up a function to prevent wirting similar code repeated times. We will evaluate these models with the root-mean-squared-error and the R-squared.","3f9ecc08":"**Delete duplicate rows for same player in season stats datasets**\n\nAs for our analysis it is not important where the player played in a season we can delete all duplicate rows of players who switched teams during the season and keep the row with total season stats. ","989a6ba7":"There is also a positive relationship with minutes played per game. It is possibly an exponential relationship. But we need to be careful when analysing this relationship. One can assume that good stats cause a higher salary in the following season and at the same time more minutes per game. Therefore the reason for this positive relationship are most probably the stats.  ","e3b2608d":"**Normalise y**\n\nAs we saw in our analysis the dependent variable does not approximately follow a normal distribution, but it is necessary for most of our models. That is why we need to normalise y for the next steps.","8be66840":"# What is this project about?\n\nBesides playing professional Basketball I like watching and analysing NBA players and games. That is why I would like my first project to be about NBA analytics. \nIn this work i would like to build up a model which predicts the salaries of NBA Players using multiple Stats (like points\/rebounds\/assists per game) of the previous two NBA Seasons as input variables.","3ce737e4":"**Drop unnecessary columns**\n\nFor our further analysis some columns will no longer be necessary. For example salaries for future seasons will not be of interest. Let us have a look at the columns first.","a6bd42d4":"A quick summary of the main statistical measures:","82933dd6":"**What about the relationship between the absolute changes and salary?**\n\nIt would be interesting to know if this also plays a role. A player who was performing better in season 18\/19 than 17\/18 and therefore is expected to get even better might have a higher salary. Let us have a look.","12b58fbf":"# 1. Data Cleaning\n\n## How does our data look like?\n\nLet us look at the first five rows of the datasets.","0a5ac557":"## Final Model ","517fca69":"**Correlation matrix**\n\nThe analysis until now was based on our intuition and what we think is important for explaining salary. Let us do a more objective analysis and get a perfect overview of all relationships of our variables. Therefore we will use a heatmap.","c3aa6e20":"**Relationship with possible features**\n\nLet us see how the relationship between some of the most important statistics in basketball and salary looks like. For now we will only focus on the statistics of the season 2018\/19. Again we will look at the relationship with points, assists, steals and rebounds per game.","3a085600":"We can say that there is no linear relationship between age and salary.","613351a0":"**How do the distributions look like?**","15a8c3e6":"**Again: What are the insights?**\n\n1. Multicollinearity:\n\nThe first thing we can observe in this triangular heatmap is that there are a lot of red coloured squares. This is an important information for our prediction model. These variables with such a high correlation give almost the same information to our prediction model and would only raise the variance of our estimation. We have to keep that in mind when building up the model.\n\n\n\n2. Correlations with Salary 2019\/20:\n\nAs we already analysed there is a linear relationship between salary and the four main stats per game. But there are also other variables that should be taken into account. For example fouls per game (PF 18\/19) play a big role for explaining the salary. \n\nWe have to be careful when analysing the correlation of salary and minutes played (MP) and games started (GS). We can not simply say that players who play more automatically earn more. As the offensive and defensive stats determine how long the coach lets the player in the game and also determine the salary of next season the relationship is not causal. What we can use in our model is total games played (G). This statistic can for example measure the relationship of a players vulnerability and his salary.\n\nWe can also not say that more Turnovers (TOV) lead to a higher salary because of the high correlation. This is of course not the case. A player with good offensive and defensive stats gets more playing time and also a higher salary. It is therefore more reasonable to say that good players with high salary have more turnovers on average because they get more playing time. ","0f4ece7f":"We only want five positions: PG, SG, SF, PF, C.","abe022ed":"# 2. Exploring our dataset\n\n## Analysis of target variable Salaries (in 1000)\n\nWhich players have the highest salary?","8c8eec28":"**Feature importance**\n\nWe will use the XGB-Regressor to detect the most important features as it had the best accuracy score so far.","ca04ad72":"**Linear Regression**\n\nWe will start and test a simple linear regression model.","d7e88cf8":"**What are the datatypes of the different columns?**","b03e706b":"### Cross Validation\n\nThis will give us a more precise accuracy measure.","03e05e0c":"**Lasso Regression**\n\nThe lasso regression is quite similar conceptually to ridge regression. It additionally adds a penalty for non-zero coeffecients. Unlike ridge regression it limits the absolute values of the coefficients rather than the sum of squared coefficients.","580c7881":"**Ridge Regression**\n\nAs we have a problem of multicollinearity in our linear regression model, we need to find a fitting solution. With high multicollinearity our estimation is most probably imprecise and has large standard errors. \nRidge regression mitigates this problem by providing improved efficiency in our estimation in exchange for an amount of bias. ","2d71294f":"## Advanced models\n\nLet us use more advanced approaches.","988605fa":"Therefore we can say that it is not possible to identify any order or correlation. This is important for Encoding this variable later on.","d75016c4":"## Absolute growth of main stats from 17\/18 to 18\/19\n\nIn our analysis we will use the stats from the season 17\/18 for our second dataframe (df2) to get the absolute growth of the main stats. ","4e012524":"So for our further analysis we have 358 rows (Players) with stats for the two seasons and salaries for 19\/20. If we leave out the stats of the season 17\/18 we have 73 additional rows.","06ccea1c":"There seem to be weak positive to no linear relationships between the absolute changes and salary. That is why we will leave out the absolute changes and therefore the stats of season 17\/18 in our prediction model.","f0252978":"**Split Data into train and test**","17aa2c92":"**Which positions do we have?**","e37554df":"**Drop rows with no Salary, no Stats for every season and NaNs for eFG%**\n\nSome players did not play in the season 17\/18 or in the season 18\/19. Some players do not have a contract for the season 19\/20. Therefore the missing values are reasonable. There are some additional missing values for percentages. For now we will drop the rows for missing effective Field Goal Percentage (eFG%). For our analysis the other Percentages will probably not even be needed.","70e4c73e":"**Let's see how it performs on some test data**","8602bd72":"## Missing values\n\nHow many missing values do we have in each column?","3484a0a4":"The most important statistics in basketball are also not normally but right-skewed distributed.","3daa2921":"We chose effective field goal percentage instead of normal field goal percentage because it accounts for the fact that three-point field goals count for three points while field goals only count for two points. Here we can hardly observe a positive linear relationship. There are players with a high eFG% but with low salaries. But there are no players with a low eFG% and a high salary.","af279723":"**One-Hot encoding for feature position**\n\nWe will use the One-Hot encoder to deal with the categorical variable 'Pos_x'. We can not use the normal Label encoder because an ordering would tell our algorithm that certain positions are 'better' than others. ","7e6621d8":"## Optimizing our data and features\n\n**Outliers**","87b4f3f3":"**What are the insights?**\n\n1. Large standard deviation which means that the salary is spread out\n2. Salary is not normally distributed\n3. Right-skewed distribution\n\nFor most of the prediction models it is important that the data is normally distributed. Let us keep that in mind when building up the model.","e9694bf4":"## Analysis of most important player stats\n\nNow let us look at the variables that most probably explain a large part of the salary of NBA players. We are going to focus on points, assists, steals and rebounds per game of the season 18\/19.","15c3202a":"## Import packages and data\n\nThe data I use are the NBA-Player salaries of the season 2019\/20 and their stats of the seasons 2017\/18 and 2018\/19. ","76c72fed":"Now we merge","5e1a58ff":"## Merge Datasets\n\nFirst we need to assign the corresponding year to every column of each dataset of season stats. "}}