{"cell_type":{"d712256d":"code","73921016":"code","aca10359":"code","b33f3748":"code","c1bc6ac8":"code","f112bde7":"code","0a89a5e7":"code","b9697db4":"code","4644417f":"code","d508c902":"code","6ff60916":"code","7c9aefc1":"code","5f54b0ce":"code","734f63f7":"code","191cd445":"code","74b77fb1":"code","955cb911":"code","beffdcc6":"markdown","718e8518":"markdown","5fbb0163":"markdown","1d9fe80c":"markdown","327260a8":"markdown","b80d6360":"markdown"},"source":{"d712256d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","73921016":"import cv2  \nimport numpy as np\nimport matplotlib.pyplot as plt\nimport os \nfrom tqdm import tqdm","aca10359":"def getOpticalFlow(video):\n    \"\"\"Calculate dense optical flow of input video\n    Args:\n        video: the input video with shape of [frames,height,width,channel]. dtype=np.array\n    Returns:\n        flows_x: the optical flow at x-axis, with the shape of [frames,height,width,channel]\n        flows_y: the optical flow at y-axis, with the shape of [frames,height,width,channel]\n    \"\"\"\n    # initialize the list of optical flows\n    gray_video = []\n    for i in range(len(video)):\n        img = cv2.cvtColor(video[i], cv2.COLOR_RGB2GRAY)\n        gray_video.append(np.reshape(img,(224,224,1)))\n\n    flows = []\n    for i in range(0,len(video)-1):\n        # calculate optical flow between each pair of frames\n        flow = cv2.calcOpticalFlowFarneback(gray_video[i], gray_video[i+1], None, 0.5, 3, 15, 3, 5, 1.2, cv2.OPTFLOW_FARNEBACK_GAUSSIAN)\n        # subtract the mean in order to eliminate the movement of camera\n        flow[..., 0] -= np.mean(flow[..., 0])\n        flow[..., 1] -= np.mean(flow[..., 1])\n        # normalize each component in optical flow\n        flow[..., 0] = cv2.normalize(flow[..., 0],None,0,255,cv2.NORM_MINMAX)\n        flow[..., 1] = cv2.normalize(flow[..., 1],None,0,255,cv2.NORM_MINMAX)\n        # Add into list \n        flows.append(flow)\n        \n    # Padding the last frame as empty array\n    flows.append(np.zeros((224,224,2)))\n      \n    return np.array(flows, dtype=np.float32)","b33f3748":"def Video2Npy(file_path, resize=(224,224)):\n    \"\"\"Load video and tansfer it into .npy format\n    Args:\n        file_path: the path of video file\n        resize: the target resolution of output video\n    Returns:\n        frames: gray-scale video\n        flows: magnitude video of optical flows \n    \"\"\"\n    # Load video\n    cap = cv2.VideoCapture(file_path)\n    # Get number of frames\n    len_frames = int(cap.get(7))\n    # Extract frames from video\n    try:\n        frames = []\n        for i in range(len_frames-1):\n            _, frame = cap.read()\n            frame = cv2.resize(frame,resize, interpolation=cv2.INTER_AREA)\n            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n            frame = np.reshape(frame, (224,224,3))\n            frames.append(frame)   \n    except:\n        print(\"Error: \", file_path, len_frames,i)\n    finally:\n        frames = np.array(frames)\n        cap.release()\n            \n    # Get the optical flow of video\n    flows = getOpticalFlow(frames)\n    \n    result = np.zeros((len(flows),224,224,5))\n    result[...,:3] = frames\n    result[...,3:] = flows\n    \n    return result","c1bc6ac8":"def Save2Npy(file_dir, save_dir):\n    \"\"\"Transfer all the videos and save them into specified directory\n    Args:\n        file_dir: source folder of target videos\n        save_dir: destination folder of output .npy files\n    \"\"\"\n    if not os.path.exists(save_dir):\n        os.makedirs(save_dir)\n    # List the files\n    videos = os.listdir(file_dir)\n    for v in tqdm(videos):\n        # Split video name\n        video_name = v.split('.')[0]\n        # Get src \n        video_path = os.path.join(file_dir, v)\n        # Get dest \n        save_path = os.path.join(save_dir, video_name+'.npy') \n        # Load and preprocess video\n        data = Video2Npy(file_path=video_path, resize=(224,224))\n        data = np.uint8(data)\n        # Save as .npy file\n        np.save(save_path, data)\n    \n    return None","f112bde7":"source_path = '..\/input\/crimeucfdataset\/Anomaly_Dataset\/Anomaly_Videos\/Anomaly-Videos-Part-2\/Burglary\/'\ntarget_path = '.\/'\n\n\"\"\"for f1 in ['train', 'val']:\n    for f2 in ['Fight', 'NonFight']:\n        path1 = os.path.join(source_path, f1, f2)\n        path2 = os.path.join(target_path, f1, f2)\n        Save2Npy(file_dir=path1, save_dir=path2)\"\"\"\nSave2Npy(file_dir=source_path, save_dir=target_path)","0a89a5e7":"\n\ndef warn(*args, **kwargs):\n    pass\nimport warnings\nwarnings.warn = warn\n\n","b9697db4":"\n\nimport numpy as np\nimport os\nfrom time import time\nimport cv2\n\n","4644417f":"from keras.utils import Sequence\nfrom keras.utils import np_utils\n\nclass DataGenerator(Sequence):\n    \"\"\"Data Generator inherited from keras.utils.Sequence\n    Args: \n        directory: the path of data set, and each sub-folder will be assigned to one class\n        batch_size: the number of data points in each batch\n        shuffle: whether to shuffle the data per epoch\n    Note:\n        If you want to load file with other data format, please fix the method of \"load_data\" as you want\n    \"\"\"\n    def __init__(self, directory, batch_size=1, shuffle=True, data_augmentation=True):\n        # Initialize the params\n        self.batch_size = batch_size\n        self.directory = directory\n        self.shuffle = shuffle\n        self.data_aug = data_augmentation\n        # Load all the save_path of files, and create a dictionary that save the pair of \"data:label\"\n        self.X_path, self.Y_dict = self.search_data() \n        # Print basic statistics information\n        self.print_stats()\n        return None\n        \n    def search_data(self):\n        X_path = []\n        Y_dict = {}\n        # list all kinds of sub-folders\n        self.dirs = sorted(os.listdir(self.directory))\n        one_hots = np_utils.to_categorical(range(len(self.dirs)))\n        for i,folder in enumerate(self.dirs):\n            folder_path = os.path.join(self.directory,folder)\n            for file in os.listdir(folder_path):\n                file_path = os.path.join(folder_path,file)\n                # append the each file path, and keep its label  \n                X_path.append(file_path)\n                Y_dict[file_path] = one_hots[i]\n        return X_path, Y_dict\n    \n    def print_stats(self):\n        # calculate basic information\n        self.n_files = len(self.X_path)\n        self.n_classes = len(self.dirs)\n        self.indexes = np.arange(len(self.X_path))\n        np.random.shuffle(self.indexes)\n        # Output states\n        print(\"Found {} files belonging to {} classes.\".format(self.n_files,self.n_classes))\n        for i,label in enumerate(self.dirs):\n            print('%10s : '%(label),i)\n        return None\n    \n    def __len__(self):\n        # calculate the iterations of each epoch\n        steps_per_epoch = np.ceil(len(self.X_path) \/ float(self.batch_size))\n        return int(steps_per_epoch)\n\n    def __getitem__(self, index):\n        \"\"\"Get the data of each batch\n        \"\"\"\n        # get the indexs of each batch\n        batch_indexs = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n        # using batch_indexs to get path of current batch\n        batch_path = [self.X_path[k] for k in batch_indexs]\n        # get batch data\n        batch_x, batch_y = self.data_generation(batch_path)\n        return batch_x, batch_y\n\n    def on_epoch_end(self):\n        # shuffle the data at each end of epoch\n        if self.shuffle == True:\n            np.random.shuffle(self.indexes)\n\n    def data_generation(self, batch_path):\n        # load data into memory, you can change the np.load to any method you want\n        batch_x = [self.load_data(x) for x in batch_path]\n        batch_y = [self.Y_dict[x] for x in batch_path]\n        # transfer the data format and take one-hot coding for labels\n        batch_x = np.array(batch_x)\n        batch_y = np.array(batch_y)\n        return batch_x, batch_y\n      \n    def normalize(self, data):\n        mean = np.mean(data)\n        std = np.std(data)\n        return (data-mean) \/ std\n    \n    def random_flip(self, video, prob):\n        s = np.random.rand()\n        if s < prob:\n            video = np.flip(m=video, axis=2)\n        return video    \n    \n    def uniform_sampling(self, video, target_frames=64):\n        # get total frames of input video and calculate sampling interval \n        len_frames = int(len(video))\n        interval = int(np.ceil(len_frames\/target_frames))\n        # init empty list for sampled video and \n        sampled_video = []\n        for i in range(0,len_frames,interval):\n            sampled_video.append(video[i])     \n        # calculate numer of padded frames and fix it \n        num_pad = target_frames - len(sampled_video)\n        if num_pad>0:\n            padding = [video[i] for i in range(-num_pad,0)]\n            sampled_video += padding     \n        # get sampled video\n        return np.array(sampled_video, dtype=np.float32)\n    \n    def dynamic_crop(self, video):\n        # extract layer of optical flow from video\n        opt_flows = video[...,3]\n        # sum of optical flow magnitude of individual frame\n        magnitude = np.sum(opt_flows, axis=0)\n        # filter slight noise by threshold \n        thresh = np.mean(magnitude)\n        magnitude[magnitude<thresh] = 0\n        # calculate center of gravity of magnitude map and adding 0.001 to avoid empty value\n        x_pdf = np.sum(magnitude, axis=1) + 0.001\n        y_pdf = np.sum(magnitude, axis=0) + 0.001\n        # normalize PDF of x and y so that the sum of probs = 1\n        x_pdf \/= np.sum(x_pdf)\n        y_pdf \/= np.sum(y_pdf)\n        # randomly choose some candidates for x and y \n        x_points = np.random.choice(a=np.arange(224), size=10, replace=True, p=x_pdf)\n        y_points = np.random.choice(a=np.arange(224), size=10, replace=True, p=y_pdf)\n        # get the mean of x and y coordinates for better robustness\n        x = int(np.mean(x_points))\n        y = int(np.mean(y_points))\n        # avoid to beyond boundaries of array\n        x = max(56,min(x,167))\n        y = max(56,min(y,167))\n        # get cropped video \n        return video[:,x-56:x+56,y-56:y+56,:]  \n    \n    def color_jitter(self,video):\n        # range of s-component: 0-1\n        # range of v component: 0-255\n        s_jitter = np.random.uniform(-0.2,0.2)\n        v_jitter = np.random.uniform(-30,30)\n        for i in range(len(video)):\n            hsv = cv2.cvtColor(video[i], cv2.COLOR_RGB2HSV)\n            s = hsv[...,1] + s_jitter\n            v = hsv[...,2] + v_jitter\n            s[s<0] = 0\n            s[s>1] = 1\n            v[v<0] = 0\n            v[v>255] = 255\n            hsv[...,1] = s\n            hsv[...,2] = v\n            video[i] = cv2.cvtColor(hsv, cv2.COLOR_HSV2RGB)\n        return video\n        \n    def load_data(self, path):\n        data = np.load(path, mmap_mode='r')[...,:3]\n        data = np.float32(data)\n        # sampling 64 frames uniformly from the entire video\n        data = self.uniform_sampling(video=data, target_frames=64)\n        # whether to utilize the data augmentation\n        if  self.data_aug:\n            data = self.color_jitter(data)\n            data = self.random_flip(data, prob=0.5)\n        # normalize\n        data = self.normalize(data)\n        return data","d508c902":"from keras.models import Sequential, Input, Model\nfrom keras.layers import Dense, Flatten, Conv3D, MaxPooling3D, Dropout, BatchNormalization, Activation, LeakyReLU, Add, Multiply\nfrom keras.regularizers import l2\nfrom keras.layers.core import Lambda\nfrom keras.layers.core import Lambda","6ff60916":"inputs = Input(shape=(64,224,224,3))\n\n#####################################################\nrgb = inputs\nrgb = Conv3D(\n    16, kernel_size=(1,3,3), strides=(1,1,1), kernel_initializer='he_normal', activation='relu', padding='same')(rgb)\nrgb = Conv3D(\n    16, kernel_size=(3,1,1), strides=(1,1,1), kernel_initializer='he_normal', activation='relu', padding='same')(rgb)\nrgb = MaxPooling3D(pool_size=(1,2,2))(rgb)\n\nrgb = Conv3D(\n    16, kernel_size=(1,3,3), strides=(1,1,1), kernel_initializer='he_normal', activation='relu', padding='same')(rgb)\nrgb = Conv3D(\n    16, kernel_size=(3,1,1), strides=(1,1,1), kernel_initializer='he_normal', activation='relu', padding='same')(rgb)\nrgb = MaxPooling3D(pool_size=(1,2,2))(rgb)\n\nrgb = Conv3D(\n    32, kernel_size=(1,3,3), strides=(1,1,1), kernel_initializer='he_normal', activation='relu', padding='same')(rgb)\nrgb = Conv3D(\n    32, kernel_size=(3,1,1), strides=(1,1,1), kernel_initializer='he_normal', activation='relu', padding='same')(rgb)\nrgb = MaxPooling3D(pool_size=(1,2,2))(rgb)\n\nrgb = Conv3D(\n    32, kernel_size=(1,3,3), strides=(1,1,1), kernel_initializer='he_normal', activation='relu', padding='same')(rgb)\nrgb = Conv3D(\n    32, kernel_size=(3,1,1), strides=(1,1,1), kernel_initializer='he_normal', activation='relu', padding='same')(rgb)\nrgb = MaxPooling3D(pool_size=(1,2,2))(rgb)\n\n#####################################################\nx = MaxPooling3D(pool_size=(8,1,1))(rgb)\n\n#####################################################\nx = Conv3D(\n    64, kernel_size=(1,3,3), strides=(1,1,1), kernel_initializer='he_normal', activation='relu', padding='same')(x)\nx = Conv3D(\n    64, kernel_size=(3,1,1), strides=(1,1,1), kernel_initializer='he_normal', activation='relu', padding='same')(x)\nx = MaxPooling3D(pool_size=(2,2,2))(x)\n\nx = Conv3D(\n    64, kernel_size=(1,3,3), strides=(1,1,1), kernel_initializer='he_normal', activation='relu', padding='same')(x)\nx = Conv3D(\n    64, kernel_size=(3,1,1), strides=(1,1,1), kernel_initializer='he_normal', activation='relu', padding='same')(x)\nx = MaxPooling3D(pool_size=(2,2,2))(x)\n\nx = Conv3D(\n    128, kernel_size=(1,3,3), strides=(1,1,1), kernel_initializer='he_normal', activation='relu', padding='same')(x)\nx = Conv3D(\n    128, kernel_size=(3,1,1), strides=(1,1,1), kernel_initializer='he_normal', activation='relu', padding='same')(x)\nx = MaxPooling3D(pool_size=(2,3,3))(x)\n\n#####################################################\nx = Flatten()(x)\nx = Dense(128,activation='relu')(x)\nx = Dropout(0.2)(x)\nx = Dense(32, activation='relu')(x)\npred = Dense(2, activation='softmax')(x)\nmodel = Model(inputs=inputs, outputs=pred)\n\nmodel.summary()","7c9aefc1":"import keras.backend as K\nfrom keras.callbacks import LearningRateScheduler\n\ndef scheduler(epoch):\n    # Every 10 epochs, the learning rate is reduced to 1\/10 of the original\n    if epoch % 10 == 0 and epoch != 0:\n        lr = K.get_value(parallel_model.optimizer.lr)\n        K.set_value(parallel_model.optimizer.lr, lr * 0.5)\n    return K.get_value(parallel_model.optimizer.lr)\n\nreduce_lr = LearningRateScheduler(scheduler)\ncallbacks_list = [reduce_lr]","5f54b0ce":"os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1,2,3\"\n\nfrom keras.utils import multi_gpu_model   \nparallel_model = multi_gpu_model(model, gpus=4)","734f63f7":"from keras.optimizers import Adam, SGD\n\nadam = Adam(lr=0.01, beta_1=0.9, beta_2=0.999, epsilon=1e-8)\nsgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n\nparallel_model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy'])","191cd445":"num_epochs  = 30\nnum_workers = 16\nbatch_size  = 16","74b77fb1":"dataset = 'RWF2000-opt'\n\ntrain_generator = DataGenerator(directory='..\/Datasets\/{}\/train'.format(dataset), \n                                batch_size=batch_size, \n                                data_augmentation=True)\n\nval_generator = DataGenerator(directory='..\/Datasets\/{}\/val'.format(dataset),\n                              batch_size=batch_size, \n                              data_augmentation=False)","955cb911":"hist = parallel_model.fit_generator(\n    generator=train_generator, \n    validation_data=val_generator,\n    callbacks=callbacks_list,\n    verbose=1, \n    epochs=num_epochs,\n    workers=num_workers ,\n    max_queue_size=8,\n    steps_per_epoch=len(train_generator),\n    validation_steps=len(val_generator))","beffdcc6":"Set the GPUs and make it parallel","718e8518":"## Preprocessing","5fbb0163":"### Build MOdel","1d9fe80c":"Model Compiling","327260a8":"## Training","b80d6360":"\n## Model Training\n\n    set essential params\n\n"}}