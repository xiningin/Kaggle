{"cell_type":{"7b97e859":"code","9e337f1c":"code","c3f08974":"code","63281488":"code","5fa1300a":"code","24008d88":"code","b3a1d7f4":"code","7f348541":"code","b613b059":"code","d98c383d":"code","6cd412ed":"code","527ca7d3":"code","a2d96ba0":"code","b4a12a26":"code","8e64e733":"code","2bc719ea":"code","d710bc55":"code","f4e5dc38":"code","a34591dc":"code","23921ae5":"code","da108ca0":"code","9bf46602":"code","039e50f5":"code","aa60d3b9":"code","9e4a7759":"code","c8f75839":"code","f57daaac":"code","8d2ebcc5":"code","830c3743":"code","0ff33ba4":"code","be3c485c":"code","77c63548":"code","3ee50301":"code","73193914":"code","6d149f48":"code","36a08bd8":"code","5434c378":"code","46981930":"code","5b9b17e5":"code","c6a3b36f":"code","59be5831":"code","59a2d8d5":"code","b05f243d":"code","54a7a0a3":"code","5d592e53":"code","471429be":"code","e723456e":"code","abeda45c":"code","94f7bb6e":"code","9d282ea7":"code","a1d4f9ea":"code","fc4f6e2d":"code","e02a4f18":"code","dde0e567":"code","eb5764a6":"code","3213db6c":"code","6ee1941e":"code","f768c2eb":"code","ac540045":"code","432da18e":"code","a13f5c1b":"code","48b78be0":"code","b56fc31e":"code","8c56ef4c":"code","437fb870":"code","222c197f":"code","b9f90b0b":"code","ea5e68a8":"code","034e8af0":"code","48ecccfd":"code","6191cb6d":"code","36df0331":"code","15e8d028":"code","282eab63":"code","76919d23":"code","b847fb55":"code","3abece5d":"markdown","319f7bd1":"markdown","4a4e43f9":"markdown","ea8cb99d":"markdown","8188a961":"markdown","f49ac4db":"markdown","4a70a735":"markdown","05cf72d4":"markdown","e5eac50c":"markdown","b2043ea6":"markdown","d7353214":"markdown","5c17ace7":"markdown","044a5679":"markdown","ec15c801":"markdown","85abe2ec":"markdown","72a8c2a3":"markdown","1e86d1ac":"markdown","376b4aed":"markdown","642b4e3a":"markdown","45447df2":"markdown","edf37c70":"markdown","737dc609":"markdown","268206d4":"markdown","839191a9":"markdown","7ac925a0":"markdown","5e981c46":"markdown","1f3fa796":"markdown","033133f6":"markdown","7b14bdd3":"markdown","63dcedf5":"markdown","b131b5f2":"markdown","7262d4b9":"markdown","f0267267":"markdown","82d459ba":"markdown","5530ede5":"markdown","511ce889":"markdown","9e01ef53":"markdown","8be597d6":"markdown","8b311420":"markdown","4224eba0":"markdown","338de643":"markdown","8dbeba4f":"markdown","a734c6e7":"markdown","86aa09a3":"markdown","dcb2125a":"markdown","923df8c7":"markdown","6beac98f":"markdown","9ffd91a3":"markdown","481ebf90":"markdown","44ccee4d":"markdown","fb990104":"markdown","c9f71cc2":"markdown","d1589d05":"markdown","24e5d33f":"markdown","8d70d51c":"markdown","e977c94e":"markdown","f5d4e751":"markdown","e50ef343":"markdown","c6939b4f":"markdown"},"source":{"7b97e859":"import numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom math import sqrt\n#sklearn\nfrom sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, KFold\nfrom sklearn.linear_model import LinearRegression, Ridge, RidgeCV, Lasso, LassoCV, ElasticNet, ElasticNetCV\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.impute import SimpleImputer, KNNImputer\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.feature_selection import SelectFromModel\nimport xgboost as xgb\nimport lightgbm as lgb","9e337f1c":"# display all the dateframe\nfrom IPython.display import display\npd.options.display.max_columns = None\npd.options.display.max_rows = None","c3f08974":"train_data = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv', index_col='Id')\ntest_data = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv', index_col='Id')\nsubmission = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv', index_col='Id')","63281488":"# combine train and test data \ndf_all = pd.concat([train_data.drop('SalePrice', axis=1), test_data], sort=True)  #df without the target","5fa1300a":"df_all.head()","24008d88":"df_all.shape","b3a1d7f4":"df_all.info()","7f348541":"numeric_data = df_all.select_dtypes(include=['int64','float64']) # by default pandas read data in 64 bit\ncategorical_data = df_all.select_dtypes(include=['object'])","b613b059":"print('Numeric Features:',len(list(numeric_data.columns)))\nprint('Categorical Features:',len(list(categorical_data.columns)))","d98c383d":"fig, ax = plt.subplots(nrows = 1, ncols = 2, figsize = (18, 6))\nsns.heatmap(train_data.isnull(), yticklabels=False, ax = ax[0], cbar=False, cmap='viridis')\nax[0].set_title('Trian data')\n\nsns.heatmap(test_data.isnull(), yticklabels=False, ax = ax[1], cbar=False, cmap='viridis')\nax[1].set_title('Test data');","6cd412ed":"missing = df_all.isnull().sum().sort_values(ascending=False)\npercentage = (df_all.isnull().sum() \/ df_all.isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([missing, percentage], axis=1, keys=['Missing', '%']) \nmissing_data = pd.concat([missing_data, df_all.dtypes], axis=1, join='inner').rename(columns={0:'type'})\nmissing_data = missing_data[missing_data != 0].dropna()\nmissing_data","527ca7d3":"missing_data[missing_data['type'] != 'object']","a2d96ba0":"missing_data[missing_data['type'] == 'object']","b4a12a26":"# adding the target to our df\ndf_all = pd.concat([df_all, train_data['SalePrice']], axis=1)\nnormal_sp = df_all['SalePrice'].dropna().map(lambda i: np.log(i) if i > 0 else 0)\nprint(df_all['SalePrice'].skew())\nprint(normal_sp.skew())\n\nfig, ax = plt.subplots(ncols=2, figsize=(12,6))\ndf_all.hist('SalePrice', ax=ax[0])\nnormal_sp.hist(ax=ax[1])\nplt.show();","8e64e733":"df_all['SalePrice'].describe()","2bc719ea":"corr = df_all.corr()\nplt.figure(figsize=(12, 12))\nsns.heatmap(corr, vmax=1, square=True, cmap=sns.diverging_palette(180, 10, as_cmap = True));","d710bc55":"# correlation with the target\ncorr_matrix = df_all.corr()\ncorr_matrix[\"SalePrice\"].sort_values(ascending=False)","f4e5dc38":"fig, ax = plt.subplots(figsize=(14, 8))\ncorrmat = abs(df_all.corr())\n# cols = corrmat.nlargest(k, 'SalePrice')['SalePrice'].index\ncols = corrmat[corrmat['SalePrice'] >= 0.5].index    # use this line or the line above\ncm = abs(np.corrcoef(df_all[cols].dropna().values.T))\n\nsns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10},\n            yticklabels=cols.values, xticklabels=cols.values, ax=ax, cmap=\"YlGnBu\")\nplt.title('The highest correlated numeric features with Sale Price')\nplt.show();","a34591dc":"fig, axes = plt.subplots(ncols=4, nrows=4, \n                         figsize=(5 * 5, 5 * 5), sharey=True)\naxes = np.ravel(axes)\ncols = ['OverallQual','OverallCond','ExterQual','ExterCond','BsmtQual',\n        'BsmtCond','GarageQual','GarageCond', 'MSSubClass','MSZoning',\n        'Neighborhood','BldgType','HouseStyle','Heating','Electrical','SaleType']\n\nfor i, c in zip(np.arange(len(axes)), cols):\n    ax = sns.boxplot(x=c, y='SalePrice', data=df_all, ax=axes[i], palette=\"Set2\")\n    ax.set_title(c)\n    ax.set_xlabel(\"\")","23921ae5":"object_features = df_all.loc[:, df_all.dtypes == np.object] \nfor col in object_features.columns:\n    print(df_all[col].value_counts(), '\\n\\n\\n\\n')","da108ca0":"df_all['GarageYrBlt'].fillna(df_all['YearBuilt'], inplace = True)","9bf46602":"# for feat in ['YearBuilt','YearRemodAdd', 'GarageYrBlt']:\n#     df_all[feat] = df_all['YrSold'] - df_all[feat]","039e50f5":"# new features to indicate the null values\n# for feat in missing_data[missing_data['type'] != 'object'].index:   # index here contains columns names\n#     df_all[feat+'_NaN'] = np.where(df_all[feat].isnull(), 1, 0)","aa60d3b9":"# imputer = KNNImputer(n_neighbors=60)\n# df_all.loc[:, df_all.dtypes != np.object] = imputer.fit_transform(numeric_data)","9e4a7759":"# imp = SimpleImputer(missing_values=np.nan, strategy='median')\n# df_all.loc[:, df_all.dtypes != np.object] = imp.fit_transform(numeric_data)","c8f75839":"df_all[numeric_data.columns] = df_all[numeric_data.columns].interpolate(method='linear')","f57daaac":"df_all[numeric_data.columns].isnull().sum().sum()","8d2ebcc5":"edit_values = ['Alley','BsmtQual','BsmtCond','BsmtExposure','BsmtFinType1','BsmtFinType2','FireplaceQu','GarageType',\n              'GarageQual','PoolQC','Fence','MiscFeature','MasVnrType', 'GarageCond', 'GarageFinish']\n\ndf_all[edit_values] = df_all[edit_values].fillna('NA')","830c3743":"catg_nulls = df_all[categorical_data.columns].isnull().sum()[df_all.isnull().sum() > 0]\nprint(f'Total Missing values: {catg_nulls.sum()} \\n\\n{catg_nulls}')","0ff33ba4":"# _ = [df_all[col].fillna(df_all[col].mode()[0], inplace=True) for col in catg_nulls.index]","be3c485c":"df_all= pd.get_dummies(df_all, drop_first=True)","77c63548":"fig, ax = plt.subplots(figsize=(12, 8))\ncorrmat = abs(df_all.corr())\ncols = corrmat.nlargest(20, 'SalePrice')['SalePrice'].index\n# cols = corrmat[corrmat['SalePrice'] > 0.5].index    # use this line or the line above\ncm = abs(np.corrcoef(df_all[cols].dropna().values.T))\n\nsns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10},\n            yticklabels=cols.values, xticklabels=cols.values, ax=ax, cmap=\"YlGnBu\")\nax.set_title('The highest correlated features with Sale Price')\nplt.show();","3ee50301":"# correlation with the target\ntop_corr = abs(df_all.corr())\ntop_features = top_corr.sort_values(by=\"SalePrice\", ascending=False).head(20)","73193914":"top_features['SalePrice'].sort_values( ascending=False)","6d149f48":"df_all_xgb = df_all.copy()","36a08bd8":"# skewed_feat = ['LotFrontage', 'LotArea', '1stFlrSF', 'GrLivArea', 'SalePrice']\nskewed_feat = numeric_data.columns.to_list()\nskewed_feat.append('SalePrice')\nfor feat in skewed_feat:\n    df_all[feat] = np.log1p(df_all[feat])","5434c378":"# Train Data\nnewtraining = df_all.loc[:1460]\n# Test Data\nnewtesting = df_all.loc[1461:].drop('SalePrice', axis=1)","46981930":"newtraining.shape, newtesting.shape","5b9b17e5":"newtraining_xgb = df_all_xgb.loc[:1460]\nnewtesting_xgb = df_all_xgb.loc[1461:].drop('SalePrice', axis=1)","c6a3b36f":"# features_corr = abs(newtraining.corr()).sort_values(by='SalePrice', ascending=False).drop('SalePrice', axis=1).drop('SalePrice')\n\n# # np.where will return 2 arrays the first one(the good one) contains row indexes,\n# # and the second one contains columns indexes(to drop some)\n# keep_feat, del_feat = np.where((features_corr > .90) & (features_corr < 1))","59be5831":"# # to get keep_feat and del_feat names \n# keep_feat = features_corr.iloc[keep_feat, :].index # use rows and .index to get columns names\n# keep_feat = keep_feat.drop_duplicates()\n# del_feat = features_corr.iloc[:, del_feat].columns # use columns and .columns to get columns names\n# del_feat = del_feat.drop_duplicates()","59a2d8d5":"# keep_and_del = list(zip(keep_feat, del_feat)) # Correlated columns\n# keep_and_del","b05f243d":"# feat_to_drop = []\n# for col in keep_and_del:\n#     if col[0] in del_feat:\n#         if np.where(del_feat == col[0]) < np.where(keep_feat == col[0]):\n#             feat_to_drop.append(col[0])\n            \n# feat_to_drop","54a7a0a3":"# newtraining.drop(feat_to_drop, axis=1, inplace=True)\n# newtesting.drop(feat_to_drop, axis=1, inplace=True)\n# no_outliars_df.drop(feat_to_drop, axis=1, inplace=True)","5d592e53":"y = newtraining['SalePrice']\nX = newtraining.drop('SalePrice', axis=1)","471429be":"y_xgb = newtraining_xgb['SalePrice']\nX_xgb = newtraining_xgb.drop('SalePrice', axis=1)","e723456e":"print('Number of features before features selection: ', X.shape[1])","abeda45c":"feat_sel = SelectFromModel(Lasso(alpha=0.001, random_state=42))\nfeat_sel.fit(X, y);","94f7bb6e":"X = feat_sel.transform(X)\nnewtesting = feat_sel.transform(newtesting)","9d282ea7":"print('Number of features after features selection: ', X.shape[1])","a1d4f9ea":"# from sklearn.preprocessing import PowerTransformer\n# pt = PowerTransformer(method='box-cox')\n# Xpt = pt.fit_transform(X)\n# tpt = pt.transform(newtesting)","fc4f6e2d":"ss = StandardScaler()\nXs =ss.fit_transform(X)\ntesting = ss.transform(newtesting)","e02a4f18":"X_train, X_test, y_train, y_test = train_test_split(\n    Xs, y, test_size=0.10, random_state=42)","dde0e567":"X_train_xgb, X_test_xgb, y_train_xgb, y_test_xgb = train_test_split(\n    X_xgb, y_xgb, test_size=1, random_state=42)","eb5764a6":"def modeling(model, X_train, y_train, test_data, X_test=None, y_test=None, prefit=False):\n    '''Takes model and data then print model results with some linear metrics then return predictions'''\n    \n    start = \"\\033[1m\" # to create BOLD print\n    end = \"\\033[0;0m\" # to create BOLD print\n    \n    # Print bold model name \n    model_name = str(model).split('(')[0]\n    print(''.join(['\\n', start, model_name, end]))\n    \n    #Fit model\n    if not prefit:\n        model.fit(X_train, y_train)\n    \n    #Accuarcy score    \n    print('Train Score', model.score(X_train, y_train))\n    try:\n        print('Test Score :', model.score(X_test, y_test))\n    except: pass\n    \n    #cross val score\n    cv=KFold(n_splits=5, shuffle=True, random_state=42)\n    mse = -(cross_val_score(model, X_train, y_train, cv=cv, scoring = 'neg_mean_squared_error'))\n    print('CV RMSE: ', np.sqrt(mse))\n    print('CV RMSE mean: ', np.sqrt(mse).mean())\n    \n    #predictions\n    y_pred = np.expm1(model.predict(test_data))\n    print('\\nFirst 5 Predictions: \\n', y_pred[:5])  \n\n    return y_pred","3213db6c":"# Models\nlr = LinearRegression()\nlasso = Lasso(alpha=0.001)\nridge = Ridge(7)\nelastic = ElasticNet(0.002335721469090121)\ntree = DecisionTreeRegressor(max_depth=10, random_state=42,)\nrandomF = RandomForestRegressor(max_depth=18, random_state=42)\nknn_reg = KNeighborsRegressor(n_neighbors=7)\nlasso_cv = LassoCV(alphas= np.logspace(-5, 0, num=20), cv=5)\nridge_cv = RidgeCV(alphas= np.logspace(-5, 20, num=100), cv=5)\nelastic_cv = ElasticNetCV(alphas= np.logspace(-5, 0, num=20), cv=5)\n\n\n\nmodels = [(lr,'lr'), (lasso,'lasso'), (ridge,'ridge'), (elastic,'elastic'), (tree,'tree'), (randomF,'randomF'),\n         (knn_reg,'knn_reg'), (lasso_cv,'lasso_cv'), (ridge_cv,'ridge_cv'), (elastic_cv,'elastic_cv')]\n\npreds = {}    # empty dict to save all models predictions\nfor model, name in models:\n    preds[name] = modeling(model, X_train, y_train, testing, X_test, y_test)","6ee1941e":"def g_search(model, param, X_train, y_train, test_data, X_test=None, y_test=None):\n    '''Simple grid search with kfold'''\n    cv=KFold(n_splits=5, shuffle=True, random_state=42)\n    gs = GridSearchCV(model,\n                  param,\n                  scoring='neg_mean_squared_error',\n                  cv=cv,\n                  n_jobs=-1,\n                  verbose=0)\n    gs.fit(X_train, y_train)\n    \n    # Results\n    y_pred = modeling(gs.best_estimator_, X_train, y_train, test_data, X_test, y_test, prefit=True) # print results and return predictions\n    \n    print('Best parameters: ', gs.best_params_)\n    \n    return y_pred","f768c2eb":"# grid search using all the data (Xs, y)\ngrid_lasso_pred = g_search(Lasso(), {'alpha': np.logspace(-5, 0, num=20)}, Xs, y, testing)\ngrid_ridge_pred = g_search(Ridge(), {'alpha': np.arange(0, 400, 1)}, Xs, y, testing)\ngrid_elastic_pred = g_search(ElasticNet(), {'alpha': np.logspace(-5, 0, num=20)}, Xs, y, testing)","ac540045":"xgb_model = xgb.XGBRegressor(\n                                 colsample_bytree=0.2,\n                                 gamma=0.0,\n                                 learning_rate=0.01,\n                                 max_depth=4,\n                                 min_child_weight=1.5,\n                                 n_estimators=3800,                                                                  \n                                 reg_alpha=0.9,\n                                 reg_lambda=0.6,\n                                 subsample=0.2,\n                                 seed=42,\n                                 tree_method='approx'\n                                 )\n\nxgb_hist = xgb_model.fit(X_train_xgb,y_train_xgb)\n#                          eval_set=[(X_train_xgb,y_train_xgb),(X_test_xgb,y_test_xgb)],\n#                          eval_metric='rmse',\n#                          early_stopping_rounds=3000\n#                         )\n# before combining train and predicted fit with : n_estimators=3800\n# after combining train and predicted fit with : n_estimators=4500","432da18e":"xgb_predictions = xgb_model.predict(newtesting_xgb)\nxgb_predictions[:5]","a13f5c1b":"cv=KFold(n_splits=5, shuffle=True, random_state=42)\nmse = -(cross_val_score(xgb_model, X_xgb, y_xgb, cv=cv, scoring='neg_mean_squared_error'))","48b78be0":"np.sqrt(mse).mean()","b56fc31e":"train_and_pred = pd.concat([X_xgb, newtesting_xgb])","8c56ef4c":"new_y = y_xgb.append(pd.Series(xgb_predictions)).reset_index(drop=True)\nnew_y.index += 1\nnew_y.index.name = 'Id'","437fb870":"X_train_xgb, X_test_xgb, y_train_xgb, y_test_xgb = train_test_split(\n    train_and_pred, new_y, test_size=1, random_state=42)","222c197f":"xgb_model.n_estimators = 4500  # change n_estimator from 3800 to 4500\nxgb_model.fit(X_train_xgb,y_train_xgb)\n\nxgb_predictions = xgb_model.predict(newtesting_xgb)\nxgb_predictions[:5]","b9f90b0b":"lgb_reg=lgb.LGBMRegressor(objective='regression', num_leaves=5, learning_rate=0.035,\n                                n_estimators=3500, max_bin=50, bagging_fraction=0.65,bagging_freq=5, bagging_seed=7, \n                                feature_fraction=0.201, feature_fraction_seed=7,n_jobs=-1)\nlgb_reg.fit(X_train, y_train)\nprint('Train: ',lgb_reg.score(X_train,y_train))\nprint('Test: ',lgb_reg.score(X_test,y_test))","ea5e68a8":"cv=KFold(n_splits=5, shuffle=True, random_state=42)\nmse = -(cross_val_score(lgb_reg, Xs, y, cv=cv, scoring='neg_mean_squared_error'))\nnp.sqrt(mse).mean()","034e8af0":"lgb_reg_predictions = np.expm1(lgb_reg.predict(testing))\nlgb_reg_predictions[:5]","48ecccfd":"new_pred = (xgb_predictions + grid_elastic_pred) \/ 2\nnew_pred[:5]","6191cb6d":"the_submission = submission.copy()\nthe_submission['SalePrice'] = new_pred\nthe_submission['SalePrice'].head()","36df0331":"the_submission.to_csv('the_submission.csv')","15e8d028":"# Final Result: !!!!!!! KAGGLE SCORE: 0.11696 !!!!!!!\n\n# This is the best result I got\n# This result from (elastic_predictions + xgb_predictions) \/ 2\n\n# And each model trained with different features and elastic with log1p and features selection but not xgboost\n# ========================================== And finally ====================================\n\n# new_pred = (elastic_predictions + xgb_predictions) \/ 2\n\n\n# In the next two cells each model parameters ","282eab63":"# !!!! This resualt with adding the predicted values to the train again and fit with xgboost again\n#                                        (the first fit: n_estimators=3800, the second fit: n_estimators=4500)\n# xgb_model = xgb.XGBRegressor(\n#                                  colsample_bytree=0.2,\n#                                  gamma=0.0,\n#                                  learning_rate=0.01,\n#                                  max_depth=4,\n#                                  min_child_weight=1.5,\n#                                  n_estimators=3800 and 4500,                                                                  \n#                                  reg_alpha=0.9,\n#                                  reg_lambda=0.6,\n#                                  subsample=0.2,\n#                                  seed=42,\n#                                  tree_method='approx'\n#                                  )\n\n# xgb_hist = xgb_model.fit(X_train,y_train,\n#                          eval_set=[(X_train,y_train),(X_test,y_test)],\n#                          eval_metric='rmse',\n#                         )\n\n\n# X_train, X_test, y_train, y_test = train_test_split(\n#     X, y, test_size=1, random_state=42)    # test size only 1 because I want to train on almost all the data\n# Kaggle score : 0.12047","76919d23":"# This result with np.log1p for all numeric features\n\n# elastic=ElasticNet(0.001)\n# elastic.fit(X, y)\n\n# skewed_feat = numeric_data.columns.to_list()\n# skewed_feat.append('SalePrice')\n# for feat in skewed_feat:\n#     df_all[feat] = np.log1p(df_all[feat])\n\n# feat_sel = SelectFromModel(Lasso(alpha=0.001, random_state=42))\n# feat_sel.fit(Xs, y)  # Standardized\n\n# Kaggle score: 0.12339","b847fb55":"# ============================================================ XGBRegressor ====================================\n# xgb_model = xgb.XGBRegressor(\n#                                  colsample_bytree=0.2,\n#                                  gamma=0.0,\n#                                  learning_rate=0.01,\n#                                  max_depth=4,\n#                                  min_child_weight=1.5,\n#                                  n_estimators=3800,                                                                  \n#                                  reg_alpha=0.9,\n#                                  reg_lambda=0.6,\n#                                  subsample=0.2,\n#                                  seed=42,\n#                                  tree_method='approx'\n#                                  )\n\n# xgb_hist = xgb_model.fit(X_train,y_train,\n#                          eval_set=[(X_train,y_train),(X_test,y_test)],\n#                          eval_metric='rmse',\n#                         )\n\n# X_train, X_test, y_train, y_test = train_test_split(\n#     Xs, y, test_size=1, random_state=42)    # test size 1% only because we want to traim in almost all the data\n# Kaggle score : 0.12102\n\n\n# ============================================================ ElasticNet ====================================\n# This result with np.log1p for all numeric features\n\n# elastic=ElasticNet(0.001)\n# elastic.fit(X, y)\n\n# skewed_feat = numeric_data.columns.to_list()\n# skewed_feat.append('SalePrice')\n# for feat in skewed_feat:\n#     df_all[feat] = np.log1p(df_all[feat])\n\n# feat_sel = SelectFromModel(Lasso(alpha=0.001, random_state=42))\n# feat_sel.fit(Xs, y)  # Standardized\n\n# Kaggle score: 0.12339\n\n# ============================================================ Lasso ====================================\n# This result with np.log1p for all numeric features\n\n# lasso=Lasso(0.0001)\n# lasso.fit(X, y)\n\n# skewed_feat = numeric_data.columns.to_list()\n# skewed_feat.append('SalePrice')\n# for feat in skewed_feat:\n#     df_all[feat] = np.log1p(df_all[feat])\n\n\n# feat_sel = SelectFromModel(Lasso(alpha=0.001, random_state=42))\n# feat_sel.fit(X, y)\n\n\n# score: 0.124475\n# Kaggle score: 0.12352\n\n# ============================================================ Ridge ====================================\n# This result with np.log1p for all numeric features\n\n# ridge = Ridge(2)\n# ridge.fit(X, y)\n\n# skewed_feat = numeric_data.columns.to_list()\n# skewed_feat.append('SalePrice')\n# for feat in skewed_feat:\n#     df_all[feat] = np.log1p(df_all[feat])\n\n\n# feat_sel = SelectFromModel(Lasso(alpha=0.001, random_state=42))\n# feat_sel.fit(X, y)\n\n# Kaggle score: 0.12390\n\n# ============================================================ LGBMRegressor ====================================\n\n# This resualt without filling numeric missing values (without the imputer)\n# lgb_reg_o=lgb.LGBMRegressor(objective='regression', num_leaves=5, learning_rate=0.035,\n#                                 n_estimators=2500, max_bin=50, bagging_fraction=0.65,bagging_freq=5, bagging_seed=7, \n#                                 feature_fraction=0.201, feature_fraction_seed=7,n_jobs=-1)\n# lgb_reg_o.fit(Xo_train, yo_train)\n\n# Xo_train, Xo_test, yo_train, yo_test = train_test_split(\n#     Xso, yo, test_size=1, random_state=42)    # test size 1% only because we want to traim in almost all the data\n\n# Xo_train, Xo_test, yo_train, yo_test = train_test_split(\n#     Xso, yo, test_size=1, random_state=42)    # test size 1% only because we want to traim in almost all the data\n# Kaggle score : 0.12595","3abece5d":"# Features Selection","319f7bd1":"### Build XGBOOST  Regressor","4a4e43f9":"**Filling the rest of null categorical features with mode** (leaving the cell below commented will lead \"get_dummies\" later to ignore these null values, and when we tested that we got a better results)","ea8cb99d":"## Best model parameters (XGBRegressor) ","8188a961":"## Normal distribution","f49ac4db":"## Separating Train and Test data","4a70a735":"![housesbanner.png](attachment:housesbanner.png)","05cf72d4":"**KNNImputer** (good results but not the best)<br>\nIf you have Sklearn 0.22.2 or above you can use this imputer","e5eac50c":"## Correlation Matrix with SalePrice","b2043ea6":"### Finding the Best Alpha for (Lasso, ElasticNet, Ridge)","d7353214":"## Getting dummies for all categorical columns","5c17ace7":"Categorical features with meaningful null values, For specific meaning look in the date description in kaggle","044a5679":"***Finding correlated columns 0.90 or higher, Then drop the one which have less correlation with SalePrice***","ec15c801":"Saving copy of 'df_all' before correcting skewness, because after testing xgboost the results better without skew correction.","85abe2ec":"  # <center>  House Prices <\/center>\n  ","72a8c2a3":"###  Results","1e86d1ac":"Let's take a look at the most important variable <b>SalePrice<\/b>","376b4aed":"**for all models except XGBoost**","642b4e3a":"Object features with null values","45447df2":"## Best second model parameters (ElasticNet) ","edf37c70":"Number of numeric null values after imputing","737dc609":"the data contains a lot of outliars which make the data distribution skewed, So we will take the log of all the numeric features to be less skewed ","268206d4":"Numeric features with null values","839191a9":"## Outliars","7ac925a0":"## Best models without refitting the predictions and without blending","5e981c46":"For missing values in 'GarageYrBlt' we can take the year from 'YearBuilt'","1f3fa796":"## Correlation matrix with SalePrice after getting dummies","033133f6":"## Exploring object features","7b14bdd3":"## Importing packages","63dcedf5":"### These datasets contain 79 explanatory variables:\n\n*   Train data have SalePrice (dependent variable) and other predictors variables.\n*   Test data contains the same variables that in train data, but  without SalePrice (dependent variable) because this data will be submitted to kaggle.\n\n ","b131b5f2":"**Linear interpolate** (THE BEST RESULTS)","7262d4b9":"##   Fill missing values","f0267267":"Note: we will not be doing features selection for 'X_xgb' because the results also better without features selection for xgboost ","82d459ba":"## Missing Values","5530ede5":"### Submission","511ce889":"## Drop features correlated with each other\n**(Doesn't improve the results but less features)**<br>\n(this manual method for experiment)","9e01ef53":"## Features Selection using (Lasso)\n**(Much better results with linear models,  but slightly worse in tree based models)**","8be597d6":"# Features Engineering","8b311420":"**Out of 81 features, 34 features have missing values.**","4224eba0":"## Introduction\n","338de643":"**SimpleImputer** (good results but also not the best)","8dbeba4f":"**for Xgboost**","a734c6e7":"# Exploring the Data","86aa09a3":"Here is another correlation matrix, but this one for features 50% or higher with SalePrice\n<br><br>\nAs we see in the correlation matrix, the features that related to quality and the size affect the sale price, which might affect our results. In addition, OverallQual feature has a significant impact on sale price more than other features.\n<br><br>\nNote: these correlations only for the numeric features, We will do another correlation matrix after features engineering","dcb2125a":"### Build LGBMRegressor","923df8c7":"###   Filling object Missing values","6beac98f":"## Distribution of SalePrice","9ffd91a3":"## Apply modeling","481ebf90":"These datasets include information about residential homes that were sold from 2006 to 2010 in Ames, Iowa. The purpose will be to predict the final price of each home.","44ccee4d":"It seems there is a long tail to the right which means high sale prices, which will make the mean to be much higher than the median.","fb990104":"### Impute numeric missing values","c9f71cc2":"# Modeling","d1589d05":"## Loading the Datasets","24e5d33f":"### Best different models parameters","8d70d51c":"**Adding predictions to the train data then fit again**","e977c94e":"Now let's take a look at the most important features, which would have a strong linear releationship with <b>SalePrice<\/b>\n<br><br>\nNote: these correlations only for the numeric features, We will do another correlation matrix after features engineering","f5d4e751":"The 2 cells below are some ideas but after modeling got slightly worse results, So they are commented ","e50ef343":"## Data Types","c6939b4f":"# Best Result (elastic_predictions + xgb_predictions) \/ 2"}}