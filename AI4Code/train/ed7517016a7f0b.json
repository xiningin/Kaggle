{"cell_type":{"9799a909":"code","cc9e2afc":"code","3587d20b":"code","cddf163a":"code","7358c73d":"code","5c437927":"code","93188386":"code","ee45eddd":"code","00cb60d2":"code","00c7523e":"code","040ffa30":"code","c2bf6be6":"code","c534405b":"code","37b8d1c8":"code","c526a88c":"code","c1466234":"code","06d7bcd5":"code","5151593a":"code","002ec128":"code","d8832bba":"code","54b8baf7":"code","dcb7b360":"code","2264e1ac":"code","6f4b7c1e":"markdown","4183738f":"markdown","1d2b136e":"markdown","ac17fa25":"markdown","0832920e":"markdown","eaa01ead":"markdown","2ccdbab1":"markdown","ae8c01cf":"markdown","38853718":"markdown","811aa988":"markdown","84388bd7":"markdown","4200b0e5":"markdown","b915da8b":"markdown","19e89a23":"markdown","f1e3955a":"markdown","f4387069":"markdown","72a18b2e":"markdown","d2207f3d":"markdown","5397552e":"markdown","d8f2a962":"markdown","251f646f":"markdown"},"source":{"9799a909":"from IPython.display import Image\n\nImage(\"..\/input\/twitterimg\/images.png\", width = \"400px\")","cc9e2afc":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport nltk\nnltk.download('stopwords')\nnltk.download('wordnet')\nimport re\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import classification_report","3587d20b":"df=pd.read_csv(\"..\/input\/nlp-getting-started\/train.csv\")\n\nprint(df.head())","cddf163a":"df.shape","7358c73d":"print(df.isnull().sum())","5c437927":"sns.countplot('target',data=df)","93188386":"df[\"keyword\"].value_counts()","ee45eddd":"data=df.drop(['location','keyword'],axis=1)\ndata.head()","00cb60d2":"# Cleaning the reviews\n\ncorpus = []\nfor i in range(0,7613):\n\n  # Cleaning special character from the tweets\n  review = re.sub(pattern='[^a-zA-Z]',repl=' ', string=data['text'][i])#remove everything apart from capital A to Z and small a to z\n  \n\n  # Converting the entire tweets into lower case\n  tweets = review.lower()\n\n  # Tokenizing the tweetsby words\n  tweets_words = tweets.split()\n \n  # Removing the stop words\n  tweets_words = [word for word in tweets_words if not word in set(stopwords.words('english'))]\n  \n  # lemmitizing  the words\n  lemmatizer = WordNetLemmatizer()\n  tweets= [lemmatizer.lemmatize(word) for word in tweets_words]\n\n  # Joining the lemmitized words\n  tweets = ' '.join(tweets)\n  \n  # Creating a corpus\n  corpus.append(tweets)","00c7523e":"corpus[:5]","040ffa30":"cv = CountVectorizer()\nX = cv.fit_transform(corpus).toarray()\ny = data['target']\nprint(X.shape)","c2bf6be6":"print(X)","c534405b":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)","37b8d1c8":"# Fitting Naive Bayes to the Training set\nfrom sklearn.naive_bayes import MultinomialNB\nclassifier = MultinomialNB()\nclassifier.fit(X_train, y_train)\n\n\n# Predicting the Test set results\ny_pred = classifier.predict(X_test)\nprint(y_pred)","c526a88c":"accuracy=confusion_matrix(y_test,y_pred )\nprint(\"confusion_matrix:\",accuracy)\n","c1466234":"accuracy=accuracy_score(y_test,y_pred )\nprint(\"accuracy_score:\",accuracy)\n","06d7bcd5":"\nprint(classification_report(y_test,y_pred ))","5151593a":"data_test=pd.read_csv(\"..\/input\/nlp-getting-started\/test.csv\")\nprint(data_test.head())","002ec128":"data_test.isnull().sum()\n","d8832bba":"data_tes=data_test.drop(['keyword','location'],axis=1)\ndata_tes.shape\n","54b8baf7":"\ncorpus1 = []\nfor i in range(0,3263):\n\n\n  # Cleaning special character from the tweets\n  review = re.sub(pattern='[^a-zA-Z]',repl=' ', string=data_tes['text'][i])\n  \n\n  # Converting the entire tweets into lower case\n  tweets = review.lower()\n\n  # Tokenizing the tweets by words\n  tweets_words = review.split()\n \n  # Removing the stop words\n  tweets_words = [word for word in tweets_words if not word in set(stopwords.words('english'))]\n  \n  # lemmitizing the words\n  lemmatizer = WordNetLemmatizer()\n  tweets = [lemmatizer.lemmatize(word) for word in tweets_words]\n\n  # Joining the lemmitized words\n  tweets = ' '.join(tweets)\n\n  y_pred=cv.transform([review]).toarray()\n  pre=classifier.predict(y_pred)\n  corpus1.append(pre)\n\nprint(len(corpus1))\n","dcb7b360":"\n# Create a submisison dataframe and append the relevant columns\nsubmission = pd.DataFrame()\n\nsubmission['id'] = data_tes['id']\nsubmission['target'] = corpus1","2264e1ac":"# Let's convert our submission dataframe 'Survived' column to ints\nsubmission['target'] = submission['target'].astype(int)\nprint('Converted Survived column to integers.')\n\nprint(submission.head())\n\n\n\n\n# Are our test and submission dataframes the same length?\nif len(submission) == len(data_tes):\n    print(\"Submission dataframe is the same length as test ({} rows).\".format(len(submission)))\nelse:\n    print(\"Dataframes mismatched, won't be able to submit to Kaggle.\")\n\n# Convert submisison dataframe to csv for submission to csv \n# for Kaggle submisison\nsubmission.to_csv('..\/submission_nlp1.csv', index=False)\nprint('Submission CSV is ready!')","6f4b7c1e":"# Checking Accuracy","4183738f":"# Table of content\n* Introduction\n* Importing Necessary Library\n* Loading The Dataset\n* Cleaning The Text Data\n* Convert Text To Machine Readable Form\n* Model Creation\n* Checking Accuracy\n* Output Prediction\n* Submission File","1d2b136e":"### Lemmatization:\n### Let\u2019s start with Why we need lemmatization ?\n### As textual data is non linear and there might be some noise present, so in order to remove the noise(unwanted stuff) we have to perform some tasks on the textual data. This process of removing noise is what we call normalization.\n### Lemmatization is the one of the text normalization techniques. In lemmatization, the words are replaced by the root words or the words with similar context.\n### E.g.- Walking will be replaced by Walk(walk is the root word of walking)","ac17fa25":"# Convert Text To Machine Readable Form","0832920e":"### Splitting data into training part and testing part","eaa01ead":"### Cleaning text in testing dataset","2ccdbab1":"# Submission File","ae8c01cf":"![](https:\/\/cdn.analyticsvidhya.com\/wp-content\/uploads\/2020\/05\/Screenshot-from-2020-05-21-12-46-42.png)","38853718":"# Introduction","811aa988":"\n\n### Twitter has become an important communication channel in times of emergency.\n### The ubiquitousness of smartphones enables people to announce an emergency they\u2019re observing in real-time. Because of this, more agencies are interested in programatically monitoring Twitter (i.e. disaster relief organizations and news agencies).\n### Our task in this project is to predict the twitter tweets is Disaster Tweets or Not, I will explain everything in a simple way.","84388bd7":"# Model Creation","4200b0e5":"### There are more missing values in the keyword and location so we can drop it.","b915da8b":"# Loading The Dataset","19e89a23":"### Every text will be converted into machnie read able form","f1e3955a":"# Cleaning The Text Data","f4387069":"### We have to do samething for the test dataset","72a18b2e":"# If  this Notebook is useful for you please upvote it !!!!","d2207f3d":"### Our cleaned text data:","5397552e":"### Every text will be converted like this:","d8f2a962":"# Importing Necessary Library","251f646f":"### when ever we have a text data we have to clean the data for remove some unnecessary symboles,and uncesessary stopwords."}}