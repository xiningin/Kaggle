{"cell_type":{"a72abff0":"code","fe001f7f":"code","26c4a2e5":"code","417ab5de":"code","9ac71bac":"code","d0958abf":"code","a67e65a5":"code","41c4f098":"code","ac2699da":"code","94763f85":"code","048ebc32":"code","00b53bb4":"code","97ba0387":"code","f0c68d47":"code","0e7ddc5e":"code","fa0ecde0":"code","de434a29":"code","953340bd":"code","8590a967":"code","2fb403c3":"code","2f804aec":"code","05e62ef7":"code","bd04b39d":"code","8869776b":"code","6a1dc92c":"code","f2f55dad":"code","9d2489eb":"code","69676ae3":"code","5cbf9216":"code","6a7c1db1":"code","27970506":"code","a1b5b075":"code","dc3d6898":"code","cb1b42c7":"code","650a0b59":"code","847f24f1":"code","e196543d":"code","87a23ba8":"code","067b908d":"code","b60ff4c4":"code","41a81119":"code","7af16713":"code","ca41c5e7":"code","9c6a234c":"code","09580a4b":"code","402bcfb8":"code","e023b74f":"code","c69005e9":"code","253b1202":"code","e1be8863":"code","f762fe21":"code","a665b269":"code","543dab88":"code","50d65bf5":"code","c8bd8756":"code","a8e30ad6":"code","1ccab2eb":"code","2ff073bd":"markdown","cef2a4ee":"markdown","596696cc":"markdown","6e9582ce":"markdown","502743cf":"markdown","02fa3a91":"markdown","0f59872d":"markdown","6c44501a":"markdown","b8b53569":"markdown","cffb2695":"markdown","53f40b7d":"markdown","2c4e51e2":"markdown","37c65c3e":"markdown","048911f6":"markdown","cb15c944":"markdown","909db3d7":"markdown","497e720c":"markdown","1090d77a":"markdown","70eba22a":"markdown","f591d0a4":"markdown","80722c7a":"markdown","be3e82ee":"markdown","8fae0044":"markdown","e030f251":"markdown","ea8f72e1":"markdown","08d6739e":"markdown","2392af07":"markdown","852d7ad6":"markdown","4135659e":"markdown","19843457":"markdown","f1f69469":"markdown","4a9c4da3":"markdown"},"source":{"a72abff0":"import numpy as np # linear algebra\nimport pandas as pd # data processing\nimport seaborn as sns #data visualization\nfrom scipy import stats\nfrom scipy.stats import norm\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom sklearn import preprocessing as prep\nimport warnings\nwarnings.filterwarnings(\"ignore\")","fe001f7f":"train = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv')\n\nprint('train data shape: ', train.shape, '\\ntest data shape: ', test.shape)\nprint('Reading done!')","26c4a2e5":"train.drop('Id', axis = 1, inplace = True)\ntest.drop('Id', axis = 1, inplace = True)\nprint(\"Dropped redundant column 'Id' from both train and test data\")\n\ntarget = 'SalePrice'\nprint('Target variable saved in a variable for further use!')","417ab5de":"def getnumcatfeat(df):\n    \n    \"\"\"Returns two lists of numeric and categorical features\"\"\"\n    \n    numfeat, catfeat = list(df.select_dtypes(include=np.number)), list(df.select_dtypes(exclude=np.number))\n    return numfeat, catfeat\n\nnumfeat, catfeat = getnumcatfeat(train)\nnumfeat.remove(target)\n\nprint('Categorical & Numeric features seperated in two lists!')","9ac71bac":"fig, a = plt.subplots(nrows=1, ncols=2, figsize = (20,7))\n\n\nsns.distplot(train[target], fit = norm, ax=a[0])\n(mu, sig) = norm.fit(train[target]) \na[0].legend(['$\\mu=$ {:.2f}, $\\sigma=$ {:.2f}, Skew = {:.4f}, Kurtosis = {:.4f}'.format(mu,sig,train[target].skew(),train[target].kurt())])\na[0].set_ylabel('Frequency')\na[0].axvline(train[target].mean(), color = 'Red') \na[0].axvline(train[target].median(), color = 'Green') \na[0].set_title(target + ' distribution')\n\ntemp=np.log1p(train[target])\n\nsns.distplot(temp, fit = norm, ax=a[1])\n(mu, sig) = norm.fit(temp) \na[1].legend(['$\\mu=$ {:.2f}, $\\sigma=$ {:.2f}, Skew = {:.4f}, Kurtosis = {:.4f}'.format(mu,sig,temp.skew(),temp.kurt())])\na[1].set_ylabel('Frequency')\na[1].axvline(temp.mean(), color = 'Red')\na[1].axvline(temp.median(), color = 'Green') \na[1].set_title('Transformed '+ target + ' distribution')\n\ntrain[target] = np.log1p(train[target])\n\nplt.show()","d0958abf":"temp = 'OverallQual'\nf,a = plt.subplots(figsize=(8,6))\nsns.boxplot(x= temp, y = target, data = train)\nplt.show()","a67e65a5":"train.drop(train[((train[temp]==3) | (train[temp]==4)) & (train[target]<10.75)].index, inplace=True)","41c4f098":"temp = 'GrLivArea'\nf,a = plt.subplots(figsize=(8,6))\nsns.scatterplot(x=temp, y=target, data=train)\ncorr, _ = stats.pearsonr(train[temp], train[target])\nplt.title('Pearsons correlation: %.3f' % corr)\nplt.show()","ac2699da":"train.drop(train[(train[temp]>4000) & (train[target]<12.5)].index, inplace=True)","94763f85":"temp = 'GarageCars'\nf,a = plt.subplots(figsize=(8,6))\nsns.boxplot(x=temp,y=target,data=train)\nplt.show()","048ebc32":"temp = 'TotalBsmtSF'\nf,a = plt.subplots(figsize=(8,6))\nsns.scatterplot(x=temp,y=target,data=train)\ncorr, _ = stats.pearsonr(train[temp], train[target])\nplt.title('Pearsons correlation: %.3f' % corr)\nplt.show()","00b53bb4":"cor = train.corr()\nf,a = plt.subplots(figsize=(15,10))\nsns.heatmap(cor)\nplt.show()","97ba0387":"topn = 20\nprint('Top ', topn, ' correlated features to target features')\ncor[target].sort_values(ascending=False)[1:(topn+1)]","f0c68d47":"s = cor.unstack().sort_values(ascending = False)[len(cor):]\n\ntopn = 20\nprint('Top', int(topn\/2), 'correlated features\\n')\ns[:topn:2]\n","0e7ddc5e":"train_labels = train[target].reset_index(drop=True)\ntrain_features = train.drop(target, axis=1)\ntest_features = test\n\ndf = pd.concat([train_features, test_features]).reset_index(drop=True)\n\n## dropping the columns with multicollinearity\ndf.drop(['GarageArea', 'GarageYrBlt', 'TotRmsAbvGrd', '1stFlrSF'], axis=1, inplace=True)\n\ndf.shape","fa0ecde0":"# topn = 20\n# # print(cor[target].sort_values()[:topn])\n# temp = list(cor[target].sort_values()[:topn].index)\n# df.drop(temp, axis=1, inplace=True)\n\n# print('Removed least ', topn, ' correlated features')","de434a29":"numfeat, catfeat = getnumcatfeat(df)","953340bd":"temp = 0","8590a967":"assert(temp<len(catfeat))\nprint('Feature name: ', catfeat[temp], '\\nUnique values: ', df[catfeat[temp]].unique(), \n      '\\nData type: ', df[catfeat[temp]].dtype, '\\nValue ', temp, ' out of ', len(catfeat))\ntemp +=1","2fb403c3":"temp = 0","2f804aec":"assert(temp<len(numfeat))\nprint('Feature name: ', numfeat[temp], '\\nUnique values: ', df[numfeat[temp]].unique(), \n      '\\nData type: ', df[numfeat[temp]].dtype, '\\nIndex ', temp, ' out of ', len(numfeat)-1)\ntemp +=1","05e62ef7":"df['YearBuilt'] = df['YearBuilt'].astype('category')\ndf['YearRemodAdd'] = df['YearRemodAdd'].astype('category')\ndf['MoSold'] = df['MoSold'].astype('category')\ndf['YrSold'] = df['YrSold'].astype('category')","bd04b39d":"numfeat, catfeat = getnumcatfeat(df)","8869776b":"temp = df.isnull().sum().sort_values(ascending=False)\/df.shape[0]*100\ntemp = temp[temp>0]\ntemp = pd.DataFrame(temp, columns = ['MissPercent'])\n\nf,a = plt.subplots(figsize=(12,10))\n\nsub = sns.barplot(x='MissPercent', y = temp.index, data=temp, orient='h')\nplt.title('Percent of missing values, size = '+ str(temp.shape[0]))\n## Annotating the bar chart\nfor p,t in zip(sub.patches, temp['MissPercent']):\n    plt.text(2.3+p.get_width(), p.get_y()+p.get_height()\/2, '{:.2f}'.format(t), ha='center', va = 'center')\n\nsns.despine(top=True, right=True)\n\nplt.show()","6a1dc92c":"def findit(df, strin):\n    \"\"\"\n    CONVENIENCE FUNCTION FOR ANOTHER FUNCTION\n    \"\"\"\n\n    temp = []\n    for col in df.columns:\n        if col[:len(strin)]==strin:\n            temp.append(col)\n    if len(temp)==0:\n        return 0\n    return temp\n\ndef fillit(df, strin):\n    \"\"\"\n    \n    CONVENIENCE FUNCTION\n    \n    Finds features beginning with 'strin' in its beginning.\n    Then fills null values of categorical and numeric features\n    with str('None') and int(0) values respectively.\n    \n    \"\"\"\n    temp = findit(df,strin)\n    for col in temp:\n        if df[col].dtype == object:\n            df[col].fillna('None', inplace=True)\n        else:\n            df[col].fillna(0, inplace=True)\n    return None","f2f55dad":"df['PoolQC'].fillna('None', inplace=True)\ndf['MiscFeature'].fillna('None', inplace=True)\ndf['Alley'].fillna(df['Alley'].mode()[0], inplace=True)\ndf['Fence'].fillna('None', inplace=True)\ndf['FireplaceQu'].fillna(\"None\", inplace=True)\ndf['LotFrontage'] = df.groupby('Neighborhood')['LotFrontage'].transform(lambda x: x.fillna(x.median()))\nfillit(df,'Garage')\nfillit(df,'Bsmt')\nfillit(df,'Mas')\ndf['MSZoning'] = df.groupby('MSSubClass')['MSZoning'].transform(lambda x: x.fillna(x.mode()[0]))\n# df['MSZoning'].fillna(df['MSZoning'].mode()[0], inplace=True)\ndf['Functional'].fillna('Typ', inplace=True)\n# Replace the missing values in each of the columns below with their mode\ndf['Electrical'].fillna(df['Electrical'].mode()[0],inplace=True)\ndf['KitchenQual'].fillna(df['KitchenQual'].mode()[0],inplace=True)\ndf['Exterior1st'].fillna(df['Exterior1st'].mode()[0],inplace=True)\ndf['Exterior2nd'].fillna(df['Exterior2nd'].mode()[0],inplace=True)\ndf['SaleType'].fillna(df['SaleType'].mode()[0],inplace=True)\ndf['Utilities'].fillna(df['Utilities'].mode()[0], inplace=True)\ndf['TotalBsmtSF'].fillna(df['TotalBsmtSF'].mode()[0],inplace=True)\n","9d2489eb":"df.isnull().sum().sort_values(ascending=False)","69676ae3":"temp = list(df[df['PoolQC']=='None']['PoolArea'].unique())\ndf['PoolArea'] = df['PoolArea'].replace(temp,0)","5cbf9216":"def minfun(lamb):\n    return round(pd.Series(stats.boxcox(1+df[temp],lmbda=lamb)).skew(), 2)\n\ndef retlamb(df, numfeat, tol, n_iter=100):\n    \"\"\"\n    \n    CONVENIENCE FUNCTION\n    \n    Returns optimized values of lambda to be used in\n    boxcox transformation for each feature so that \n    skewness is minimized\n    \n    \"\"\"\n    valLambda = {}\n    lim1, lim2 = 0, 4\n    idx=0\n    for temp in numfeat:\n        lim1, lim2 = 0, 2\n        for i in range(n_iter):\n            lamb1=0.5*(lim1+lim2)-tol\n            cal1 = round(pd.Series(stats.boxcox(1+df[temp],lmbda=lamb1)).skew(), 4)\n            lamb2=0.5*(lim1+lim2)+tol\n            cal2 = round(pd.Series(stats.boxcox(1+df[temp],lmbda=lamb2)).skew(), 4)\n            if abs(cal1)<abs(cal2):\n                lim2=lamb2\n            elif abs(cal1)>=abs(cal2) :\n                lim1=lamb1\n        valLambda[idx] = 0.5*(lim1+lim2)\n        idx+=1\n    return valLambda","6a7c1db1":"valLambda = retlamb(df,numfeat, tol=0.0001, n_iter=1000)\nvalLambda","27970506":"temp = 2\nlamb = valLambda[temp]\n# temp, lamb = 0, 1\nf,a = plt.subplots(nrows=1, ncols=2, figsize=(16,6))\n\nsns.distplot(df[numfeat[temp]], ax=a[0], kde=False)\na[0].legend(['$\\mu=$ {:.2f}, $\\sigma=$ {:.2f}, Skew = {:.4f}, Kurtosis = {:.4f}'.format(mu,sig,df[numfeat[temp]].skew(),df[numfeat[temp]].kurt())])\na[0].set_title('Distribution of '+ numfeat[temp])\n\ntempdf = pd.Series(stats.boxcox(1+df[numfeat[temp]],lmbda=lamb))\n\nsns.distplot(tempdf, ax=a[1], kde=False)\na[1].legend(['$\\mu=$ {:.2f}, $\\sigma=$ {:.2f}, Skew = {:.4f}, Kurtosis = {:.4f}'.format(mu,sig,tempdf.skew(),tempdf.kurt())])\na[1].set_title('Transformed distribution of '+ numfeat[temp])\n\nvalLambda[temp] = lamb\n\nplt.show()","a1b5b075":"for temp, lamb in valLambda.items():\n    df[numfeat[temp]] = stats.boxcox(1+df[numfeat[temp]],lmbda=lamb)\n    \n# ## SIMPLE SIMPLE FIX FOR ALL DRAMA DONE IS ALL THE ABOVE 3 CELLS but my approach gave better results so I happily stuck with it\n# for temp in range(len(numfeat)):\n#     df[numfeat[temp]] = stats.boxcox(1+df[numfeat[temp]], stats.boxcox_normmax(df[numfeat[temp]] + 1))","dc3d6898":"df[numfeat].skew()","cb1b42c7":"df = pd.get_dummies(df).reset_index(drop=True)\ndf.shape","650a0b59":"minmaxscalar = prep.MinMaxScaler()\ndf1 = pd.DataFrame(minmaxscalar.fit_transform(df), columns = df.columns)\ndf1.head()","847f24f1":"X = df1.iloc[:len(train_labels),:]\nX_test = df1.iloc[len(train_labels):, :]\ny = train_labels\n\nX.shape, X_test.shape, y.shape","e196543d":"from sklearn.model_selection import cross_val_score, KFold\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.ensemble import RandomForestRegressor\nfrom lightgbm import LGBMRegressor\nfrom xgboost import XGBRegressor\nfrom sklearn.model_selection import RandomizedSearchCV \nfrom sklearn.linear_model import Lasso\n\n","87a23ba8":"# X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 0)\n\nkf = KFold(n_splits = 7, random_state=0, shuffle=True)\n\nscores = {}","067b908d":"def cv_rmse(model, X=X):\n    rmse = np.sqrt(-cross_val_score(model, X, y, scoring=\"neg_mean_squared_error\", cv=kf))\n    return (rmse)","b60ff4c4":"lightgbm = LGBMRegressor(objective='regression', \n                       num_leaves=6,\n                       learning_rate=0.009, \n                       n_estimators=7000,\n                       max_bin=200, \n                       bagging_fraction=0.8,\n                       bagging_freq=4, \n                       bagging_seed=8,\n                       feature_fraction=0.2,\n                       feature_fraction_seed=8,\n                       min_sum_hessian_in_leaf = 11,\n                       verbose=-1,\n                       random_state=42)","41a81119":"score = cv_rmse(lightgbm)\nprint(\"lightgbm: {:.4f} ({:.4f})\".format(score.mean(), score.std()))\nscores['lgb'] = (score.mean(), score.std())","7af16713":"xgboost = XGBRegressor(learning_rate=0.01,\n                       n_estimators=6000,\n                       max_depth=4,\n                       min_child_weight=0,\n                       gamma=0.6,\n                       subsample=0.7,\n                       colsample_bytree=0.7,\n                       objective='reg:squarederror',\n                       nthread=-1,\n                       scale_pos_weight=1,\n                       seed=27,\n                       reg_alpha=0.00006,\n                       random_state=42)","ca41c5e7":"score = cv_rmse(xgboost)\nprint(\"xgboost: {:.4f} ({:.4f})\".format(score.mean(), score.std()))\nscores['xgb'] = (score.mean(), score.std())","9c6a234c":"# Number of trees in random forest\nn_estimators = [int(x) for x in np.linspace(start = 100, stop = 1200, num = 50)]\n\n# Number of features to consider at every split\nmax_features = ['auto', 'sqrt']\n\n# Maximum number of levels in tree\nmax_depth = [int(x) for x in np.linspace(5, 300, 100)]\n\n# Minimum number of samples required to split a node\nmin_samples_split = [int(x) for x in np.linspace(2,500,100)]\n\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = [int(x) for x in np.linspace(2,500,100)]\n\n# Create the random grid\nrforestgrid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf}","09580a4b":"rforest = RandomForestRegressor()","402bcfb8":"# clf = RandomizedSearchCV(rforest, rforestgrid, cv=5, n_iter=50, n_jobs=1)\n# # search = clf.fit(X,y)","e023b74f":"# search.best_params_","c69005e9":"rforest_tuned = RandomForestRegressor(n_estimators = 1000,\n                                       min_samples_split = 2,\n                                       min_samples_leaf = 1,\n                                       max_features = 'auto',\n                                       max_depth = 100\n                                      )","253b1202":"score = cv_rmse(rforest_tuned)\nprint(\"rforest: {:4f} ({:.4f})\".format(score.mean(), score.std()))\nscores['rforest'] = (score.mean(), score.std())","e1be8863":"lasso = Lasso(alpha=0.000328)","f762fe21":"score = cv_rmse(lasso)\nprint(\"lasso: {:4f} ({:.4f})\".format(score.mean(), score.std()))\nscores['lasso'] = (score.mean(), score.std())","a665b269":"model = lasso.fit(X, y)","543dab88":"submission = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv\")\nsubmission.shape","50d65bf5":"model.predict(X_test).shape","c8bd8756":"submission.iloc[:,1] = np.floor(np.expm1(model.predict(X_test)))","a8e30ad6":"submission.to_csv(\"submission_try4.csv\", index=False)","1ccab2eb":"## Downloading the submission file\n\nfrom IPython.display import FileLink\nFileLink('submission_try4.csv')","2ff073bd":"# MODEL FITTING","cef2a4ee":"This is a parameter I tuned myself and it gave pretty good results. :P","596696cc":"# DATA PREPROCESSING","6e9582ce":"Dealing with missing values","502743cf":"Removed 2 outliers.","02fa3a91":"## Light GB model","0f59872d":"None of the categorical features seem to be misclassified. Checking for numeric features.","6c44501a":"Log transform gives the best result as data has almost exponential tendency.","b8b53569":"## XG Boost model","cffb2695":"## Data Cleaning","53f40b7d":"## Lasso Regr Model","2c4e51e2":"# IMPORTS & GETTING READY","37c65c3e":"3 outliers removed.","048911f6":"# Submission ","cb15c944":"# A FRIENDLY MESSAGE","909db3d7":"Completing seperation of features wrt their type, the first thing I want to see is how my data looks.\n### Starting off with the target variable.","497e720c":"Data has linear form with a few outliers. ","1090d77a":"Feeling good about the data. Moving towards the combined data preprocessing.","70eba22a":"## Random Forest model","f591d0a4":"This is my first ever kernel in Kaggle. As nervous as I was, I landed up at top 18% in my first submission. Learned a lot in the process and looking forward to keep going on with other submissions.\n\nI know it is a very common problem set for anyone to submit and upload a kernel but I thought of sharing it anyway to share my approach hoping to get feedbacks from the community on how I can improve. \n\nI have shared my mistakes (commented out) along with my code to showcase my different approaches and the results I got from them. If you are a beginner it makes you see the thought process of other beginners while approaching a problem and become more comfortable with your doubts.\n\nI have taken a very basic approach and I hope you find it useful. If you do, please upvote, it'll just motivate me more to keep trying more and more problems. \n\nHOPING TO HEAR FROM YOU ALL. THANK YOU IN ADVANCE :)","80722c7a":"I feel like dropping a few columns which show multicolinearity.","be3e82ee":"### To check if any feature type is misclassified","8fae0044":"Shows a very good correlation shown. So nothing requires removal.","e030f251":"Looks linear but two outliers very evident.","ea8f72e1":"I tried to remove a few least correlated features from the training set. The model turned out to give bad results so I stuck with not dropping them","08d6739e":"A few wrong values exist for pool area where PoolQC is None. ","2392af07":"I feel nothing requires removal for garagecars","852d7ad6":"Numerical data will help better decide which rows to drop.","4135659e":"The target variable is very close to a Normal Distribution now. \n### Moving ahead to another variables.","19843457":"Showing one scenario of how I tuned the parameters of my model.","f1f69469":"No more null values. Looking forward to outlier removal and data transformation.","4a9c4da3":"Changed variable type of category as misclassified in data. Chose category instead of object\/string as models work faster on them."}}