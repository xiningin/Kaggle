{"cell_type":{"fd0fc433":"code","c96adb7e":"code","a300fc0e":"code","9f187d29":"code","e0aebcd8":"code","c7aa6e62":"code","d2c44b7c":"code","5f2b772e":"code","445faad5":"code","5c7fd0e5":"code","d661168c":"code","071b884b":"code","614789c6":"code","794460eb":"code","76acf3b7":"code","47ceb1f1":"code","51475c83":"markdown","50ebcca4":"markdown"},"source":{"fd0fc433":"import os\nimport imageio\nimport tensorflow as tf\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","c96adb7e":"# load the train_csv file with the image properties\nbase_dir = '..\/input\/landmark-recognition-2020\/'\ntrain_csv = pd.read_csv('..\/input\/landmark-recognition-multiprocessing-image-size\/train_featured.csv')","a300fc0e":"# This swaps the values of xsize with ysize columns as they were assigned wrongly in a previous kernel.\nys_temp = train_csv.xsize.copy()\ntrain_csv.xsize = train_csv.ysize\ntrain_csv.ysize = ys_temp\ndel ys_temp","9f187d29":"train_csv.head()","e0aebcd8":"g = sns.jointplot(x=\"xsize\", y=\"ysize\", data=train_csv)","c7aa6e62":"print('Range of xsize and ysize:')\nprint('---'*8)\nprint(train_csv.xsize.min(), '<= xsize <=', train_csv.xsize.max())\nprint(train_csv.ysize.min(), '<= ysize <=', train_csv.ysize.max())","d2c44b7c":"print('The most commonly observed 3 values of xsize and ysize:')\nxcount = train_csv.xsize.value_counts()\nycount = train_csv.ysize.value_counts()\nprint(xcount[:3])\nprint(ycount[:3])","5f2b772e":"train_csv[(train_csv.ysize < 100) & (train_csv.xsize > 600)]","445faad5":"def load_image(idx):\n    impath = base_dir + 'train\/' + '\/'.join(list(idx[:3])) + '\/' + idx + '.jpg'\n    return imageio.imread(impath)","5c7fd0e5":"plt.imshow(load_image(train_csv.loc[43027, 'id']))\nplt.show()","d661168c":"plt.imshow(load_image(train_csv.loc[491551, 'id']))\nplt.show()","071b884b":"plt.imshow(load_image(train_csv.loc[1289336, 'id']))\nplt.show()","614789c6":"train_csv[(train_csv.xsize < 200) & (train_csv.ysize > 600)]","794460eb":"plt.imshow(load_image(train_csv.loc[338829, 'id']))\nplt.show()","76acf3b7":"plt.imshow(load_image(train_csv.loc[546929, 'id']))\nplt.show()","47ceb1f1":"plt.imshow(load_image(train_csv.loc[965744, 'id']))\nplt.show()","51475c83":"## Xsize vs Ysize\n---","50ebcca4":"Google Landmark Recognition - Image Shape Distribution\n---\nThe train dataset contains ~1.5 million images in varying shapes. While the majority of the images have an aspect ratio of ~1, some images are in the form of horizontal or vertical strips. This notebook presents some examples of such images. \n\nTraditionally, convolutional neural networks (CNNs), used in the state of the art recognition solutions, are trained on square images. Therefore, the images have to be resized during preprocessing step. The methods (cropping, zeropadding, or a combination of both) used to resize these vertical (or horizontal) strips may affect the performance of CNNs."}}