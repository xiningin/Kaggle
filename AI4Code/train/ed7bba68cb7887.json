{"cell_type":{"f05adf09":"code","dc321691":"code","cb544359":"code","3ea4ed30":"code","f11c345b":"code","0b68d307":"code","cc272552":"code","d7747cbb":"code","cea5732f":"code","b688287f":"code","e6b902c8":"code","2e0ff258":"code","6d680bbd":"code","bd00fdf9":"code","2c0e18ca":"code","613cfac3":"code","10c793e6":"code","91fafdc2":"markdown","337600c2":"markdown","611fe2c7":"markdown","af8f5bc7":"markdown","c36a15ca":"markdown","e2df977d":"markdown","11559ea7":"markdown","d6721445":"markdown","9e08061f":"markdown","c5479da3":"markdown"},"source":{"f05adf09":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport keras\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras import layers\nfrom keras.models import Sequential\nimport keras.utils as ku\nfrom keras.callbacks import EarlyStopping","dc321691":"# Loading the dataset\ndata = pd.read_json('\/kaggle\/input\/quotes-dataset\/quotes.json')\nprint(data.shape)\ndata.head()","cb544359":"# Dropping duplicates and creating a list containing all the quotes\nquotes = data['Quote'].drop_duplicates()\nprint(f\"Total Unique Quotes: {quotes.shape}\")\n\n# Considering only top 3000 quotes\nquotes_filt = quotes.sample(3000)\nprint(f\"Filtered Quotes: {quotes_filt.shape}\")\nall_quotes = list(quotes_filt)\nall_quotes[:2]","3ea4ed30":"# Tokeinization\ntokenizer = Tokenizer()\n\n# Function to create the sequences\ndef generate_sequences(corpus):\n    tokenizer.fit_on_texts(corpus)\n    total_words = len(tokenizer.word_index) + 1\n    print(f\"Total unique words in the text corpus: {total_words}\")\n    input_sequences = []\n    for line in corpus:\n        seq = tokenizer.texts_to_sequences([line])[0]\n        for i in range(1, len(seq)):\n            ngram_seq = seq[:i+1]\n            input_sequences.append(ngram_seq)\n            \n    return input_sequences, total_words\n\n# Generating sequences\ninput_sequences, total_words = generate_sequences(all_quotes)\ninput_sequences[:5]","f11c345b":"# Generating predictors and labels from the padded sequences\ndef generate_input_sequence(input_sequences):\n    maxlen = max([len(x) for x in input_sequences])\n    input_sequences = pad_sequences(input_sequences, maxlen=maxlen)\n    predictors, label = input_sequences[:, :-1], input_sequences[:, -1]\n    label = ku.to_categorical(label, num_classes=total_words)\n    return predictors, label, maxlen\n\npredictors, label, maxlen = generate_input_sequence(input_sequences)\npredictors[:1], label[:1]","0b68d307":"# Building the model\nembedding_dim = 64\n\ndef create_model(maxlen, embedding_dim, total_words):\n    model = Sequential()\n    model.add(layers.Embedding(total_words, embedding_dim, input_length = maxlen))\n    model.add(layers.LSTM(128, dropout=0.2))\n    model.add(layers.Dense(total_words, activation='softmax'))\n    \n    # compiling the model\n    model.compile(loss='categorical_crossentropy', optimizer='adam')\n    return model\n\nmodel = create_model(maxlen, embedding_dim, total_words)\nmodel.summary()","cc272552":"predictors.shape , label.shape, maxlen","d7747cbb":"# Training the model\n# model.fit(predictors, label, epochs=50, batch_size=64)","cea5732f":"# Save the model for later use\n# model.save(\"Quotes_generator.h5\")","b688287f":"# Loading the model\nfrom keras.models import load_model\n\nQuotes_gen = load_model(\"..\/input\/quote-generator-trained-model\/Quotes_generator.h5\")","e6b902c8":"Quotes_gen.summary()","2e0ff258":"# Text generating function\ndef generate_quote(seed_text, num_words, model, maxlen):\n    \n    for _ in range(num_words):\n        tokens = tokenizer.texts_to_sequences([seed_text])[0]\n        tokens = pad_sequences([tokens], maxlen=maxlen, padding='pre')\n        \n        predicted = model.predict_classes(tokens)\n        \n        output_word = ''\n        \n        for word, index in tokenizer.word_index.items():\n            if index == predicted:\n                output_word = word\n                break\n        seed_text = seed_text + \" \" + output_word\n    \n    return seed_text","6d680bbd":"# Let's try to generate some quotes\nprint(generate_quote(\"Passion\", num_words = 10, model= Quotes_gen, maxlen=maxlen))","bd00fdf9":"print(generate_quote(\"Love\", num_words = 20, model= Quotes_gen, maxlen=maxlen))","2c0e18ca":"print(generate_quote(\"legend\", num_words = 15, model= Quotes_gen, maxlen=maxlen))","613cfac3":"print(generate_quote(\"consistency matters\", num_words = 15, model= Quotes_gen, maxlen=maxlen))","10c793e6":"print(generate_quote(\"Follow your passion\", num_words = 20, model= Quotes_gen, maxlen=maxlen))","91fafdc2":"In this kernel, I will walk you through the process of generating text using LSTM. For purpose of this tutorial, we will use Quotes dataset and train our model to create our custom quotes generator.\n\nLet's start by importing necessary libraries and loading in the dataset.","337600c2":"Finally, we are done with the preprocessing part of task. Now, we will start building our LSTM model for text generation. You can think of this model as a multiclass text classification task- given the previous words, the model will predict the next word which has high probability.\n\n**Model Architecture:**\n* Embedding layer with the embedding dimension of 64\n* LSTM Layer with 128 units with dropout\n* A dense layer with number of units equal to the total words in the vocabulary with **softmax** activation since it is a mulitclass classification task.\n* The optimizer we use here is **Adam**, loss is **categorical_crossentropy**, and an epoch of 50.","611fe2c7":"# **Generating Quotes using LSTM**","af8f5bc7":"**Reference:** https:\/\/medium.com\/@shivambansal36\/language-modelling-text-generation-using-lstms-deep-learning-for-nlp-ed36b224b275 ","c36a15ca":"The generated quotes looks okayish but still can be improved a lot. Due to my system specs, I had train it using only 3000 quotes and 50 epochs with only one LSTM layer. You can tune these parameters and train for more epochs to get higher quality results.\n\nThis ends our task of generating text using LSTM. Here, I have used word level text generation but this works only if you have huge amount of data. Even, if you have huge amount of data, you will face huge dimensionality for the label, since you will be using total words in the dictionary for prediction and this leads to system crash, if you are training in a relatively low power system like mine.\n\nIn such cases, you can use character level text generation since total characters in english is only 26 and adding up some punctuations would take this to max 30-40 characters. In, the follow up versions of this notebook, I will train a character level model and compare its generation quality with the word level model.\n\nTill then, Happy Learning!!","e2df977d":"Now that we have our trained model, we will create a function to generate text.\n\nThe function takes in the trained model, the input words (also called seed text), how many words to genereate and maximum squence length. The function then tokenize the text, padds it and predict using our trained model.\n\nThe model predicts one word at a time. So after every prediction, we will get the word for the predicted label and append it to the seed_text. This process continues for the specified number of words you want to genereate. And once it is done, the text will then be returned.\n","11559ea7":"Now that we have the data in required format, but each sequences are of different length. So, before feeding into the model, we will first pad the sequences to same length.\n\nAlso, we need to create predictor and label from the prepared sequences by taking all the tokens except the last one as predictors and the last token as label (For example, think of it like the data in the above table: \"Don't cry\" as predictors and \"because\" as label).","d6721445":"The dataset has features such as:\n* Quote\n* Author\n* Tags\n* Popularity\n* Category\n\nBut for our task at hand, we are interested in only the Quote feature of the dataset. If you look at the Quote column, a single quote is attributed to multiple categories such as life, happiness, etc. So, we will drop the duplicate quotes and consider only unique quotes.","9e08061f":"The model has been trained for almost two hours for only 50 epochs. So, will save the model to avoid training every time we want to generate a pice of text.","c5479da3":"Next step is to preprocess and prepare the data for a Text Generation model. \n\nFirst, we will tokenize the text usign Keras **Tokenizer** class to create a vocabulary and convert the text into sequence of token indexes.\n\nThere are two levels at which you can generate text:\n1. Character level\n2. Word level\n\nHere, first we will focus on word level text generation. Suppose, let's consider the quote **\"Don't cry because it's over, smile because it happened\"**, we have to prepare our data in the format below:\n![image.png](attachment:image.png)\nThe reason why we need to prepare our data in such a way is very intuitive because, even when we write any piece of text, we will form sentences word by word i.e., the next word we write depends upon the previous words we have used. So, when we give the model sequences in this format, it will also try to learn the sequence and predict the next possible word exactly how we do."}}