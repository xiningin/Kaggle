{"cell_type":{"69f3bedb":"code","d27c6dff":"code","bbb28efc":"code","784bd34b":"code","094bfa95":"code","67af2866":"code","80afa84b":"code","15f50608":"code","c3703e56":"code","a8da6d30":"code","f5df3fe5":"code","8ecc5052":"code","c9f07978":"code","eb4837db":"code","2c885e84":"code","f4864d0c":"code","35c62624":"code","cc824bb8":"code","5ea8694d":"code","38d3f950":"code","f2cd59a8":"code","654f5b58":"code","9abb7273":"code","6bf42a6d":"code","039ccc05":"code","adefa4c1":"code","d35c17dd":"code","6969d78b":"markdown","321df871":"markdown","d3ca9b34":"markdown","50ccdbb2":"markdown","4dd17078":"markdown","5ab4c115":"markdown","346ac3a3":"markdown","d5bd77c0":"markdown","81b5614c":"markdown","4b1d05bc":"markdown","6908196b":"markdown"},"source":{"69f3bedb":"# importing the libraries\nimport pandas as pd\nimport tensorflow as tf\nimport numpy as np","d27c6dff":"# importing the Deep Learning Libraries\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Embedding, LSTM, GRU, Dense, Dropout","bbb28efc":"# loading the training data\ntraining_data = pd.read_csv('\/kaggle\/input\/quora-insincere-questions-classification\/train.csv')","784bd34b":"training_data.head()","094bfa95":"# dropping the qid\ntraining_data = training_data.drop(['qid'], axis = 1)","67af2866":"# creating a feature length that contains the total length of the question\ntraining_data['length'] = training_data['question_text'].apply(lambda s: len(s))\n# I used a basic way of utilizing a lambda function.","80afa84b":"# now checking the mean length of the text for tokenizing the data.\nmin(training_data['length']), max(training_data['length']), round(sum(training_data['length'])\/len(training_data['length']))","15f50608":"training_data[training_data['length'] <= 9]","c3703e56":"training_data = training_data.drop(training_data[training_data['length'] <= 9].index, axis = 0)\nmin(training_data['length']), max(training_data['length']), round(sum(training_data['length'])\/len(training_data['length']))","a8da6d30":"training_data.isnull().sum()","f5df3fe5":"# Tokenizing the text - Converting each word, even letters into numbers. \nmax_length = round(sum(training_data['length'])\/len(training_data['length']))\ntokenizer = Tokenizer(num_words = max_length, \n                      filters = '!\"#$%&()*+,-.\/:;<=>?@[\\\\]^_`{|}~\\t\\n',\n                     lower = True,\n                     split = ' ')","8ecc5052":"tokenizer.fit_on_texts(training_data['question_text'])","c9f07978":"# Actual Conversion takes place here.\nX = tokenizer.texts_to_sequences(training_data['question_text'])","eb4837db":"print(len(X), len(X[0]), len(X[1]), len(X[2]))","2c885e84":"X = pad_sequences(sequences = X, padding = 'pre', maxlen = max_length)\nprint(len(X), len(X[0]), len(X[1]), len(X[2]))","f4864d0c":"y = training_data['target'].values\ny.shape","35c62624":"# LSTM Neural Network\nlstm = Sequential()\nlstm.add(Embedding(input_dim = max_length, output_dim = 120))\nlstm.add(LSTM(units = 120, recurrent_dropout = 0.2))\nlstm.add(Dropout(rate = 0.2))\nlstm.add(Dense(units = 120, activation = 'relu'))\nlstm.add(Dropout(rate = 0.1))\nlstm.add(Dense(units = 2, activation = 'softmax'))\n\nlstm.compile(optimizer = 'adam', loss = 'sparse_categorical_crossentropy', metrics = ['accuracy'])","cc824bb8":"lstm_fitted = lstm.fit(X, y, epochs = 1)","5ea8694d":"# importing the testing data\ntesting_data = pd.read_csv('\/kaggle\/input\/quora-insincere-questions-classification\/test.csv')","38d3f950":"testing_data.head()","f2cd59a8":"# converting the data into tokens\nX_test = tokenizer.texts_to_sequences(testing_data['question_text'])","654f5b58":"print(len(X_test), len(X_test[0]), len(X_test[1]), len(X_test[2]))","9abb7273":"# paddding the sequences\nX_test = pad_sequences(X_test, maxlen = max_length, padding = 'pre')\nprint(len(X_test), len(X_test[0]), len(X_test[1]), len(X_test[2]))","6bf42a6d":"# predicting the test set\nlstm_prediction = lstm.predict_classes(X_test)","039ccc05":"# creating a dataframe for submitting\nsubmission = pd.DataFrame(({'qid':testing_data['qid'], 'prediction':lstm_prediction}))","adefa4c1":"submission.head()","d35c17dd":"submission.to_csv('submission.csv', index = False)","6969d78b":"No Missing Values !\n\nLet us start the Deep Learning part now !","321df871":"Thank you for viewing my kernel. Please comment if you have any creative ideas of doing traditional methods.\n\n*Let's Learn ! Let's Learn !* ","d3ca9b34":"Note: you can play with the hyperparameters to get the expected accuracy.","50ccdbb2":"minimum length = 1 ?? looks like outliers, How can a question contain just a single word ? Let us do some preprocessing. ","4dd17078":"Hi, if you are a beginner in Tensorflow and would like to catch up the essence of Natural Language Processing (Like me), please upvote <3","5ab4c115":"We are good to go !!","346ac3a3":"Now the data is ready to be fed into the neural network. Now constructing the neural network NLP. \n\nI will create a neural network with minimum layers so that beginners like me can understand without complexity.","d5bd77c0":"1. We dont need the qid to train the model.\n2. The question_text is the text input that has to be fitted in the model along with the target\n3. The target has 2 classes.","81b5614c":"Looks sensible. Now lets check for missing values (if any)","4b1d05bc":"As you can see the lengths are not same. So Pad sequences are used. Pad sequences adds a specific value, usually 0, before or after the text sequence to make them equal in length","6908196b":"oops...**Are these even complete questions ???**"}}