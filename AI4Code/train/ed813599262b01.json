{"cell_type":{"b723e09f":"code","9707dfcc":"code","915509da":"code","acb37653":"code","bb3d1e90":"code","2ff9b8b5":"code","90450618":"code","f31052d0":"code","e0d4580d":"code","c153adbe":"code","48ab415d":"code","9055f2a5":"code","603dc8be":"code","8d6da071":"code","2849c441":"code","b9c054e3":"code","8b279466":"code","0ea5a05f":"code","2338841a":"code","1b20a73a":"code","f8bab9a2":"code","7f5f322f":"code","ff6ef274":"code","8d171a40":"code","830285fc":"code","f2d3a84c":"code","f3754957":"code","7a5a716b":"code","cf13921b":"code","8f29a550":"code","49900fef":"code","8b1b8738":"code","93e49fd7":"code","fef37c1e":"code","5f3ff91a":"code","fdaab76e":"code","e5d23216":"code","6a5b1973":"markdown","237c345c":"markdown","bc646443":"markdown","26c77af1":"markdown","2c33d2f0":"markdown","a8beb450":"markdown","2c202fc9":"markdown","0e2e6f93":"markdown","b76dd05e":"markdown","027cbc82":"markdown","a902da03":"markdown","0ed9c2b9":"markdown","a7517ecf":"markdown","3adc9a37":"markdown","5e74ebf1":"markdown","106c8a7e":"markdown","a751ed29":"markdown","ea2ca0eb":"markdown","ffe61bd5":"markdown","35dda86d":"markdown","974eebd0":"markdown","6f28c953":"markdown","cdce899a":"markdown","7cbb3470":"markdown","f3145028":"markdown","db4d737f":"markdown"},"source":{"b723e09f":"####Loading useful packages\n\n#For data manipulation\nimport numpy as np\nimport pandas as pd\n\n#For plotting\nimport matplotlib.pyplot as pp\nimport seaborn as sns\n\n#This just ensures that our plots appear\n%matplotlib inline                   \n\n\n#For surpressing warnings\nimport warnings\nwarnings.filterwarnings('ignore')","9707dfcc":"#loading in the data\ntrain_data = pd.read_csv(\"..\/input\/test23\/train.csv\")\ntest_data = pd.read_csv(\"..\/input\/test22\/test.csv\")\n\n#Let's take a look at our training data\ntrain_data.head()","915509da":"#Let's take a look at our testing data\ntest_data.head()","acb37653":"#What's the shape of the data like ?\nprint(\"Training data dimenstions: \",train_data.shape)\nprint(\"Testing data dimenstions: \",test_data.shape)","bb3d1e90":"#Having a closer look at one of the customers.....\ntrain_data.iloc[1,]","2ff9b8b5":"#Let's look at the distribution of the target variable\npp.hist(train_data.loc[:,'target'])\npp.xlabel(\"target\")\npp.ylabel(\"frequency\")\npp.title(\"Distribution of target variable\")","90450618":"#Let's check the data type of each of the variables\ntrain_data.info()","f31052d0":"#Making's lists on variables which belong to each group\n\ncategorical_variables = [train_data.columns[i] for i in range(len(train_data.columns)) if 'cat' in train_data.columns[i]]\n\nbinary_variables = [train_data.columns[i] for i in range(len(train_data.columns)) if 'bin' in train_data.columns[i]]\n\ninterval_variables = [train_data.columns[i] for i in range(len(train_data.columns)) if (train_data.loc[:,train_data.columns[i]].dtype==float and 'cat' not in train_data.columns[i] and 'bin' not in train_data.columns[i])]\n\nordinal_variables = [train_data.columns[i] for i in range(len(train_data.columns)) if (train_data.loc[:,train_data.columns[i]].dtype == 'int64' and 'cat' not in train_data.columns[i] and 'bin' not in train_data.columns[i])][2:]\n\n","e0d4580d":"pd.Series(train_data.isnull().sum())","c153adbe":"#Categorical Variables\nfor i in categorical_variables:\n    train_data.loc[:,i].value_counts(dropna=False).plot.bar()\n    pp.xlabel(i)\n    pp.show()","48ab415d":"#Binary variables\nfor i in binary_variables:\n    train_data.loc[:,i].value_counts(dropna=False).plot.bar()\n    pp.xlabel(i)\n    pp.show()","9055f2a5":"#Interval variables\nfor i in interval_variables:\n    train_data.loc[:,i].value_counts(dropna=False).plot.hist()\n    pp.xlabel(i)\n    pp.show()\n    ","603dc8be":"#Ordinal variables\nfor i in ordinal_variables:\n    train_data.loc[:,i].value_counts(dropna=False).plot.bar()\n    pp.xlabel(i)\n    pp.show()","8d6da071":"#Categorical variables\ntrain_data.loc[:,categorical_variables].describe()","2849c441":"#Binary Variables\ntrain_data.loc[:,binary_variables].describe()","b9c054e3":"#Interval variables\ntrain_data.loc[:,interval_variables].describe()","8b279466":"#Ordinal variables\ntrain_data.loc[:,ordinal_variables].describe()","0ea5a05f":"#Categorial Variables\ntest_data.loc[:,categorical_variables].describe()","2338841a":"#Binary variables\ntest_data.loc[:,binary_variables].describe()","1b20a73a":"#Interval variables\ntest_data.loc[:,interval_variables].describe()","f8bab9a2":"#Ordinal Variables\ntest_data.loc[:,ordinal_variables].describe()","7f5f322f":"#Interval Variables\nfor i in interval_variables:\n    sns.boxplot(train_data.loc[:,i],showfliers=True)\n    pp.xlabel(i)\n    pp.show()","ff6ef274":"#Ordinal variables\nfor i in ordinal_variables:\n    sns.boxplot(train_data.loc[:,i],showfliers=True)\n    pp.xlabel(i)\n    pp.show()","8d171a40":"#Finding the interquartile range\nQ1 = train_data.quantile(0.25)\nQ3 = train_data.quantile(0.75)\nIQR = Q3 - Q1","830285fc":"int_var = ['ps_reg_02','ps_reg_03','ps_car_12','ps_car_13','ps_car_14','ps_car_15']\nod_var = ['ps_ind_14','ps_calc_04','ps_calc_06','ps_calc_07','ps_calc_08','ps_calc_10','ps_calc_11','ps_calc_12','ps_calc_13','ps_calc_14']","f2d3a84c":"#Separating the outliers from the non-outliers\noutliers = train_data[(train_data > (Q3 + 1.5 * IQR))|(train_data < (Q1 - 1.5 * IQR))].drop(labels='target',axis=1)\noutliers['target'] = train_data['target']\nnon_outliers = train_data[(train_data <= (Q3 + 1.5 * IQR))&(train_data >= (Q1 - 1.5 * IQR))].drop(labels='target',axis=1)\nnon_outliers['target'] = train_data['target']\n","f3754957":"#for ordinal variables\nfor i in od_var:\n    print(\"{} outliers have \".format(i),outliers[['target',i]].dropna(axis=0).groupby('target').count().iloc[1,0]\/outliers[['target',i]].dropna(axis=0).groupby('target').count().sum(axis=0)[0],\"ones\")\n    print(\"{} outliers have \".format(i),outliers[['target',i]].dropna(axis=0).groupby('target').count().iloc[0,0]\/outliers[['target',i]].dropna(axis=0).groupby('target').count().sum(axis=0)[0],\"zeroes\")\n    print(\"{} non-outliers have \".format(i),non_outliers[['target',i]].dropna(axis=0).groupby('target').count().iloc[1,0]\/non_outliers[['target',i]].dropna(axis=0).groupby('target').count().sum(axis=0)[0],\"ones\")\n    print(\"{} non-outliers have \".format(i),non_outliers[['target',i]].dropna(axis=0).groupby('target').count().iloc[0,0]\/non_outliers[['target',i]].dropna(axis=0).groupby('target').count().sum(axis=0)[0],\"zeroes\")\n    print(\"\")\n    print(\"\")","7a5a716b":"#For interval variables\nfor i in int_var:\n    print(\"{} outliers have \".format(i),outliers[['target',i]].dropna(axis=0).groupby('target').count().iloc[1,0]\/outliers[['target',i]].dropna(axis=0).groupby('target').count().sum(axis=0)[0],\"ones\")\n    print(\"{} outliers have \".format(i),outliers[['target',i]].dropna(axis=0).groupby('target').count().iloc[0,0]\/outliers[['target',i]].dropna(axis=0).groupby('target').count().sum(axis=0)[0],\"zeroes\")\n    print(\"{} non-outliers have \".format(i),non_outliers[['target',i]].dropna(axis=0).groupby('target').count().iloc[1,0]\/non_outliers[['target',i]].dropna(axis=0).groupby('target').count().sum(axis=0)[0],\"ones\")\n    print(\"{} non-outliers have \".format(i),non_outliers[['target',i]].dropna(axis=0).groupby('target').count().iloc[0,0]\/non_outliers[['target',i]].dropna(axis=0).groupby('target').count().sum(axis=0)[0],\"zeroes\")\n    print(\"\")\n    print(\"\")","cf13921b":"#Let's look at the correlation of the features with the response\ntarget_correlations = train_data.corr().iloc[:,1]\ntarget_correlations = target_correlations.iloc[2:]\ntarget_correlations.abs().sort_values(ascending=False)","8f29a550":"#Checking for collinearity\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor  \n\nvif_train_data = train_data.drop(['target','id'],axis = 1)\ncolnames = train_data.columns.drop(['target','id'])\nvif_table = pd.DataFrame()\n\nfor i in colnames:\n    a = vif_train_data.columns.get_loc(i)\n    vif = variance_inflation_factor(np.array(vif_train_data),exog_idx=a)\n    vif_table.loc[0,i] = vif","49900fef":"vif_table.loc[:,vif_table.loc[0,:] > 5]","8b1b8738":"target_correlations.abs().sort_values(ascending=False).head(5)","93e49fd7":"#Let's put the top 5 \nft_top5 = train_data.loc[:,['target','ps_car_13','ps_car_12','ps_ind_17_bin','ps_car_07_cat','ps_reg_02']]","fef37c1e":"#Let's examine the relationship between target and our top variables\nfor i in range(5):\n#Kernal density plot of claims that did happen\n    sns.kdeplot(train_data.loc[train_data.loc[:,'target'] == 0,target_correlations.abs().sort_values(ascending=False).index[i]], label = 'target ==0')\n    sns.kdeplot(train_data.loc[train_data.loc[:,'target'] == 1,target_correlations.abs().sort_values(ascending=False).index[i]], label = 'target ==1')\n    pp.xlabel('ps_car_13'); pp.ylabel('Density');pp.title('Distribution of {}'.format(target_correlations.abs().sort_values(ascending=False).index[i]))\n    pp.show()","5f3ff91a":"#Make a new dataframe for polynomial feature\npoly_features = ft_top5\npoly_features = poly_features.loc[:,['ps_car_13','ps_car_12','ps_ind_17_bin','ps_car_07_cat','ps_reg_02']]\npoly_features_test = ft_top5.loc[:,['ps_car_13','ps_car_12','ps_ind_17_bin','ps_car_07_cat','ps_reg_02']]\n\nfrom sklearn.preprocessing import PolynomialFeatures\n#train the features\npoly_transformer = PolynomialFeatures(degree =3)\npoly_transformer.fit(poly_features)\n\n#Transform the features\npoly_features = poly_transformer.transform(poly_features)\npoly_features_test = poly_transformer.transform(poly_features_test)","fdaab76e":"poly_features = pd.DataFrame(poly_features, columns = poly_transformer.get_feature_names(input_features=['ps_car_13','ps_car_12','ps_ind_17_bin','ps_car_07_cat','ps_reg_02']))\npoly_features['target'] = train_data.loc[:,'target']\npoly_corrs = poly_features.corr()['target'].sort_values()","e5d23216":"poly_corrs.abs().sort_values(ascending = False)","6a5b1973":"There doesn't seem to any significant differences in density for when target= 0 and for when target=1.","237c345c":"All of our variables are numerical, fantastic! This means no encoding is needed. Let's put our variables into the groups that are specified:categorical, binary, interval and ordinal variables...","bc646443":"Okay, that confirms that there are no missing values. Let's now look at the summary statistics for each group of variables...\n","26c77af1":"As we'd expect, the training data has one more column than the testing data. Let's take a look at one of the observations....","2c33d2f0":"Let's examine the relationship between the top 5 features and the target variable","a8beb450":"Next, let's make two list of interval and ordinal variables that had a significant number of outliers, based on the graphs.","2c202fc9":"Great! The outliers seem not to have any effect on claming. Let's examine the correlations next....","0e2e6f93":"Let's start by loading in some useful packages...","b76dd05e":"I will dive into feature engineering in a future kernel :)","027cbc82":"Next, let's load in the data and take at the training data...","a902da03":"Next,let's separate the outlier data from the non-outlier data","0ed9c2b9":"Let's extract the data for the top 5 features.....","a7517ecf":"Let's check for collinearity.....","3adc9a37":"First, we are going find the interquartile-range(IQR)......","5e74ebf1":"Everything seems okay. Let's dive deeper into interval and continous variables using box and whisker plots...","106c8a7e":"Great,nothing seems out of place there! Let's view how our target variable, whether a driver claims or not, is distributed....","a751ed29":"Let's try making new features using the top 5 features , polynomial method up to degree three and check  if the correlation improves....","ea2ca0eb":"Before we proceed to examine each category of variables, let's check if we have any missing values....","ffe61bd5":"There doesn't seem to be any different combinations of variables different from our top 5 that seem to have a stronger correlation.","35dda86d":"There seems to be quite a few variables with outliers! Let's examine these outliers a little bit closer.....","974eebd0":"Look's like we have an imbalanced class problem, we'll deal with that later. Let's examine the data types of our training data...\n","6f28c953":"Now, let's look at the testing data","cdce899a":"Ahhh, our top variable,ps_car_13, has a VIF over 5. Although I want to drop variables which show high collinearity, but because ps_car_13 has the strongest relationship  with target and also because it's only marginally over 5,  I am going to keep ps_car_13 in my top 5 variables and use those 5 variables going forward'","7cbb3470":"Let's check for factors with a VIF > 5 ....","f3145028":"# **Porto Seguro's Safe Driver Prediction**\n\nHello, this kernel will do some data exploration on the Porto Seguro Safe Driver Prediction dataset. Any helpful feedback would be appreciated :)","db4d737f":"Hmmm, this looks suspicious. Let's try visualize this and see if we get the same outcome..."}}