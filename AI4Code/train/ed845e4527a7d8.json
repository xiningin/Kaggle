{"cell_type":{"f02105b0":"code","dc169957":"code","5388f70b":"code","7cddfa97":"code","2f107a6b":"code","5c36a40a":"code","db0ac38d":"code","46a219dd":"code","2b153e60":"code","7a54614f":"code","e79cd373":"code","541fae8c":"code","b0c6dc19":"code","b20a05dd":"code","6daec9db":"code","7e8e452a":"code","3c139bd6":"code","d090ddd8":"code","fd0bc2c5":"code","53cd2a1d":"code","b08dfc68":"code","5f3b4d07":"code","12c75d8d":"code","27fe2da2":"code","8c16e5f1":"code","149d44ea":"code","71c8f915":"code","e23dcc66":"code","330a7ad4":"markdown","29c7361c":"markdown","7b43f9af":"markdown","fa2498af":"markdown","9168eb47":"markdown","981cc625":"markdown","499f679c":"markdown","37aa0edf":"markdown","f5936a59":"markdown","6dd75264":"markdown","b46dd5e6":"markdown","8f3888a7":"markdown","678553c3":"markdown","257c602d":"markdown","c1c22d14":"markdown","a8abe932":"markdown","a191d8b3":"markdown","35fa4cf7":"markdown","a2a0f4f4":"markdown"},"source":{"f02105b0":"\"\"\"\nData Loading and other libraries\n\"\"\"\nimport warnings\nimport pandas as pd\nimport numpy as np\nfrom tqdm import tqdm\n\n\"\"\"\nTransformer libraries useful to using the pretrained model and data preprocessing\n\"\"\"\nimport torch\nfrom keras.preprocessing.sequence import pad_sequences\nfrom transformers import BertTokenizer,  AutoModelForSequenceClassification\n\n\"\"\"\nSimilarity search section: cosine similarity search and facebook AI research library\n\"\"\"\nfrom sklearn.metrics.pairwise import cosine_similarity\n!pip install faiss-gpu # please uncomment this line when you're running the notebook for the first time\nimport faiss","dc169957":"warnings.filterwarnings(\"ignore\")","5388f70b":"data = pd.read_csv(\"..\/input\/cord19createdataframe\/cord19_df.csv\") \nprint(\"Data Shape: {}\".format(data.shape))","7cddfa97":"# Percentage of missing column values\npercent_missing = data.isnull().sum() * 100 \/ len(data)\npercent_missing","2f107a6b":"# remove articles with missing abstract\ndata = data.dropna(subset = ['abstract'])\ndata = data.reset_index(drop = True)\npercent_missing = data.isnull().sum() * 100 \/ len(data)\npercent_missing","5c36a40a":"# Show first N (default value is 100) words of each of the #total_number random articles\ndef show_random_articles(total_number, df, n=100):\n    \n    # Get the random number of articles\n    n_reviews = df.sample(total_number)\n    \n    # Print each one of the articles\n    for val in list(n_reviews.index):\n        print(\"Article #{}\".format(val))\n        print(\" --> Title: {}\".format(df.iloc[val][\"title\"]))\n        print(\" --> Abstract: {} ...\".format(\" \".join(df.iloc[val][\"abstract\"].split()[:n])))\n        print(\"\\n\")\n        \n# Show 3 random headlines\nshow_random_articles(3, data)","db0ac38d":"# Get the SciBERT pretrained model path from Allen AI repo\npretrained_model = 'allenai\/scibert_scivocab_uncased'\n\n# Get the tokenizer from the previous path\nsciBERT_tokenizer = BertTokenizer.from_pretrained(pretrained_model, \n                                          do_lower_case=True)\n\n# Get the model\nmodel = AutoModelForSequenceClassification.from_pretrained(pretrained_model,\n                                                          output_attentions=False,\n                                                          output_hidden_states=True)","46a219dd":"def convert_single_abstract_to_embedding(tokenizer, model, in_text, MAX_LEN = 510):\n    \n    input_ids = tokenizer.encode(\n                        in_text, \n                        add_special_tokens = True, \n                        max_length = MAX_LEN,                           \n                   )    \n\n    results = pad_sequences([input_ids], maxlen=MAX_LEN, dtype=\"long\", \n                              truncating=\"post\", padding=\"post\")\n    \n    # Remove the outer list.\n    input_ids = results[0]\n\n    # Create attention masks    \n    attention_mask = [int(i>0) for i in input_ids]\n    \n    # Convert to tensors.\n    input_ids = torch.tensor(input_ids)\n    attention_mask = torch.tensor(attention_mask)\n\n    # Add an extra dimension for the \"batch\" (even though there is only one \n    # input in this batch.)\n    input_ids = input_ids.unsqueeze(0)\n    attention_mask = attention_mask.unsqueeze(0)\n    \n    # Put the model in \"evaluation\" mode, meaning feed-forward operation.\n    model.eval()\n\n    #input_ids = input_ids.to(device)\n    #attention_mask = attention_mask.to(device)\n    \n    # Run the text through BERT, and collect all of the hidden states produced\n    # from all 12 layers. \n    with torch.no_grad():        \n        logits, encoded_layers = model(\n                                    input_ids = input_ids, \n                                    token_type_ids = None, \n                                    attention_mask = attention_mask,\n                                    return_dict=False)\n\n    layer_i = 12 # The last BERT layer before the classifier.\n    batch_i = 0 # Only one input in the batch.\n    token_i = 0 # The first token, corresponding to [CLS]\n        \n    # Extract the embedding.\n    embedding = encoded_layers[layer_i][batch_i][token_i]\n\n    # Move to the CPU and convert to numpy ndarray.\n    embedding = embedding.detach().cpu().numpy()\n\n    return(embedding)","2b153e60":"input_abstract = data.abstract.iloc[30]\n\n# Use the model and tokenizer to generate an embedding for the input_abstract\nabstract_embedding = convert_single_abstract_to_embedding(sciBERT_tokenizer, model, input_abstract)\n\nprint('Embedding shape: {}'.format(abstract_embedding.shape))","7a54614f":"def get_min_viable_data(df, sample_size=2000):\n    \n    # Select only the columns we need for the analysis\n    useless_cols = ['methods', 'results', 'source', 'doi',\n           'body_text', 'publish_time', 'authors', 'journal', 'arxiv_id',\n           'publish_year', 'is_covid19', 'study_design']\n\n    df.drop(useless_cols, axis=1, inplace=True)\n\n    \"\"\"\n    It was taking too much time to run the analysis on the overall dataset, so I decided to take \n    a subset (2000 observations) of the original dataset in order to speed the processing.\n    \"\"\"\n\n    df = df.sample(sample_size)\n    \n    return df","e79cd373":"def convert_overall_text_to_embedding(df):\n    \n    # The list of all the embeddings\n    embeddings = []\n    \n    # Get overall text data\n    overall_text_data = data.abstract.values\n    \n    # Loop over all the comment and get the embeddings\n    for abstract in tqdm(overall_text_data):\n        \n        # Get the embedding \n        embedding = convert_single_abstract_to_embedding(sciBERT_tokenizer, model, abstract)\n        \n        #add it to the list\n        embeddings.append(embedding)\n        \n    print(\"Conversion Done!\")\n    \n    return embeddings","541fae8c":"\"\"\"\n# This task can take a lot of time depending on the sample_size value \nin the \"get_min_viable_data\" function\n\"\"\"\ndata = get_min_viable_data(data)\nembeddings = convert_overall_text_to_embedding(data)","b0c6dc19":"# Create a new column that will contain embedding of each body text\ndef create_final_embeddings(df, embeddings):\n    \n    df[\"embeddings\"] = embeddings\n    df[\"embeddings\"] = df[\"embeddings\"].apply(lambda emb: np.array(emb))\n    df[\"embeddings\"] = df[\"embeddings\"].apply(lambda emb: emb.reshape(1, -1))\n    \n    return df","b20a05dd":"data = create_final_embeddings(data, embeddings)\ndata.head(3)","6daec9db":"def process_query(query_text):\n    \"\"\"\n    # Create a vector for given query and adjust it for cosine similarity search\n    \"\"\"\n\n    query_vect = convert_single_abstract_to_embedding(sciBERT_tokenizer, model, query_text)\n    query_vect = np.array(query_vect)\n    query_vect = query_vect.reshape(1, -1)\n    return query_vect\n\n\ndef get_top_N_articles_cosine(query_text, data, top_N=5):\n    \"\"\"\n    Retrieve top_N (5 is default value) articles similar to the query\n    \"\"\"\n    query_vect = process_query(query_text)\n    revevant_cols = [\"title\", \"abstract\", \"url\", \"cos_sim\"]\n    \n    # Run similarity Search\n    data[\"cos_sim\"] = data[\"embeddings\"].apply(lambda x: cosine_similarity(query_vect, x))\n    data[\"cos_sim\"] = data[\"cos_sim\"].apply(lambda x: x[0][0])\n    \n    \"\"\"\n    Sort Cosine Similarity Column in Descending Order \n    Here we start at 1 to remove similarity with itself because it is always 1\n    \"\"\"\n    moost_similar_articles = data.sort_values(by='cos_sim', ascending=False)[1:top_N+1]\n    \n    return moost_similar_articles[revevant_cols]","7e8e452a":"query_text_test = data.iloc[0].abstract\n\ntop_articles = get_top_N_articles_cosine(query_text_test, data)","3c139bd6":"top_articles","d090ddd8":"top_articles.iloc[0].abstract","fd0bc2c5":"top_articles.iloc[1].abstract","53cd2a1d":"top_articles.iloc[2].abstract","b08dfc68":"top_articles.iloc[3].abstract","5f3b4d07":"embedding_dimension = len(embeddings[0])","12c75d8d":"indexFlatL2 = faiss.IndexFlatL2(embedding_dimension)\n\n# Convert the embeddings list of vectors into a 2D array.\nvectors = np.stack(embeddings)\n\nindexFlatL2.add(vectors)","27fe2da2":"print(\"Total Added Number of Vectors: {}\".format(indexFlatL2.ntotal))","8c16e5f1":"# Get query vector\nquery_text = data.iloc[0].abstract \nquery_vector = process_query(query_text)\n\nK = 5\n\n# Run the search\nD, I = indexFlatL2.search(query_vector, K)","149d44ea":"I # this contains the index of all the similar articles","71c8f915":"D # this contains the L2 distance values of all the similar articles","e23dcc66":"for i in range(I.shape[1]):\n    \n    article_index = I[0, i]\n    \n    abstract = data.iloc[article_index].abstract\n    print(\"** Article #{} **\".format(article_index))\n    print(\"** --> Abstract : \\n{}**\".format(abstract))\n    print(\"** --> L2 Distance: %.2f**\" % D[0, i])\n    print(\"\\n\")","330a7ad4":"## Utility functions","29c7361c":"# Find Similar Documents From Scientific Corpus Using Deep Learning With SciBERT       \nThis kernel is a comprehensive overview of performing semantic similarity of documents with KNN and Cosine Similarity.","7b43f9af":"There are 47110 articles overall, each one having 16 columns. \n\n**Note**: We will focuse our analysis on the **abstract** column for simplicity sake, also it is the one with 0% missing data. But you could use other textual columns such as **body_text**; it is up to you. \nOn the other hand, we will use 2000 observation in order to speed the processing. ","fa2498af":"# Useful Libraries","9168eb47":"**Note**:  \nI decided to breakdown all the steps on purpose in order to make sure you understand properly. But you can put everything together into a single function.  ","981cc625":"# About the data   \n- This CORD-19 data set, a resource of over 59,000 scholarly articles, including over 48,000 with full text, about COVID-19, SARS-CoV-2, and related coronaviruses. \n- It is downloadable from Kaggle. \n- Further details about the dataset can be found on this [page](https:\/\/www.kaggle.com\/danielwolffram\/discovid-ai-a-search-and-recommendation-engine\/data). ","499f679c":"# Data Processing & Vectorization     \nThe data processing aims to vectorize the articles' body text so that we can perform the similarity analysis. Since we are dealing with scientific document, we will use the SciBERT model and tokenizer to generate an embedding for each of the articles using their text data.  \nSciBERT is a pretrained language model for Scientific text data. You can find more information about it on the [Semantic Scholar](https:\/\/www.semanticscholar.org\/paper\/SciBERT%3A-A-Pretrained-Language-Model-for-Scientific-Beltagy-Lo\/5e98fe2163640da8ab9695b9ee9c433bb30f5353)   \nHere is how we proceed:  \n\n## Load model artifacts   \nLoad the pretrained model & tokenizer. When loading the pretrained model, we need to set the output_hidden_states to True so that we can extract the embeddings.  ","37aa0edf":"#### Data loading","f5936a59":"# Introduction  \nWhen reading an interesting article, you might want to find similar articles from the a large number of candidate publications. Manual processing is obviously not the strategy to go for. Why not take advantage of the power of Artificial Intelligence to solve such problem? \nFrom this article, you will be able to use SciBERT and cosine similarity in order to find articles that are most similar in meaning to your specific query.  \n\n# Approach    \nHere are the different steps performed \n* Data extraction and cleaning   \n* Data Processing \n    * Load the pretrained model  \n    * Vectorize documents\n     \n* Semantic Similarity search \n    * Cosine Similarity   \n    * k-NN with Faiss","6dd75264":"### Similarity Search Using KNN with Faiss   ","b46dd5e6":"### Test on a single text data  \nHere we test the function on the \"abstract\" field of the 30th article. You can choose whatever number you want, as long as it exists in the data.","8f3888a7":"## Transform text data to embeddings   \nThis function *convert_single_abstract_to_embedding* is mostly inspired of the BERT Word [Embeddings Tutorial](https:\/\/mccormickml.com\/2019\/05\/14\/BERT-word-embeddings-tutorial\/#3-extracting-embeddings) of Chris McCormick \n\nIt aims to create an embedding for a given text data using SciBERT pre-trained model. ","678553c3":"# Similarity Search   \nEach of the body text data has a corresponding embedding. Now, we can perform the similarity analysis between a given ***query*** vector and all the embeddings vectors. The scope of this article is limited to:  \n- Cosine similarity which ...     \n- k-Nearest Neighbor (KNN) search ","257c602d":"**(768,)** means that the embedding is composed of 768 values. Now that the convert from text to embedding works, we can finally apply it to all the our text data. But, before that, we are going to remove some columns from the data, in order to have less columns in the result of the final query search. Also, I selected only 2000 articles to perform the analysis, so that the overall processing does not become time-consuming.","c1c22d14":"Made with \u2665\ufe0f by Zoumana   \nDid you like it?  Git it an upvote and [let's connet](https:\/\/medium.com\/@zoumanakeita)","a8abe932":"### Similarity Search with Cosine","a191d8b3":"### Perform Query    \nWe will use the same query as previously. Change it to another one if you want.  ","35fa4cf7":"**Observation**   \n- The lower the distance is, the most similar the article is to the query.   \n- The first document has L2 = 0, which means 100% similarity. This is obvious, because the query was compared with itself. \n- We can simply remove it to the analysis.","a2a0f4f4":"Faiss is a library developed by [Facebook AI Research](https:\/\/research.facebook.com\/research-areas\/facebook-ai-research-fair\/). According to their [wikipage](https:\/\/github.com\/facebookresearch\/faiss\/wiki), \n> Faiss is a library for efficient similarity search and clustering of dense vectors. It contains algorithms that search in sets of vectors of any size, up to ones that possibly do not fit in RAM\n\nHere are the steps to build the search engine using the previously built embeddings  \n- create the flat index: This is used to flat the vectors. The index uses the L2 (Euclidean) distance metrics to mesure the similarity betweeen the query vector and all the vectors (embeddings). \n- add all the vectors to the index \n- define the number **K** of similar document we want \n- run the similarity search  "}}