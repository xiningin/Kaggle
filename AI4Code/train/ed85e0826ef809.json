{"cell_type":{"e5834e89":"code","44628a0c":"code","84980c06":"code","6162df61":"code","737c07b0":"code","bb63ad6e":"code","26f468a2":"code","be056742":"code","4599aebc":"code","ab472220":"code","bde8ce08":"code","7f5dea29":"code","bd4614da":"code","e7eda9b7":"code","1ed8ca05":"code","cd4a2d19":"code","d196d6a6":"code","f2e5e0c2":"code","58202266":"code","9cf265b8":"code","4ba4c1ec":"code","86c83b3f":"code","059b1817":"code","48a423c3":"code","1b0ff9a8":"code","c76f781e":"code","d00ec6f5":"code","f1bb7f26":"code","11123ebf":"code","c64d6999":"code","68d938b9":"code","aeffa0f5":"code","f40d9403":"markdown","ec3841c2":"markdown","0438f17f":"markdown","d6562cae":"markdown","aa4e4eea":"markdown","cd789097":"markdown","370f9afd":"markdown","5743a1de":"markdown","a6d74e6c":"markdown","bbfc26ca":"markdown","0ed9a7dd":"markdown","e021c6a4":"markdown","ec7114e2":"markdown"},"source":{"e5834e89":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)","44628a0c":"def load_metadata(metadata_file):\n    df = pd.read_csv(metadata_file,\n                 dtype={'Microsoft Academic Paper ID': str,\n                        'pubmed_id': str, \n                        \"title\":str,\n                        \"absract\":str,\n                        \"WHO #Covidence\": str})\n    print(f'Loaded metadata with {len(df)} records')\n    return df","84980c06":"METADATA_FILE = '\/kaggle\/input\/CORD-19-research-challenge\/metadata.csv'\n\ndf = load_metadata(METADATA_FILE)","6162df61":"df[[\"title\",\"abstract\"]].head()","737c07b0":"def isNaN(string):\n    return string != string\nprint('hello', isNaN('hello'))\nprint(np.nan,isNaN(np.nan))","bb63ad6e":"df['has_title'] = df.title.apply(lambda x: not isNaN(x) and x != 'TOC')\ndf['has_abstract'] = df.abstract.apply(lambda x: not isNaN(x) and x != 'TOC')","26f468a2":"df['has_title'].value_counts()","be056742":"df['has_abstract'].value_counts()","4599aebc":"n_records = df.shape[0]\npct_has_title = df['has_title'].sum()\/n_records * 100\npct_has_abstract = df['has_abstract'].sum()\/n_records * 100\npct_has_title_and_abstract = df[df['has_title'] & df['has_abstract']].shape[0]\/n_records * 100\npct_has_title_or_abstract = df[df['has_title'] | df['has_abstract']].shape[0]\/n_records * 100\n\nprint(f\"Number of records: {n_records}\")\nprint(f\"Records with title: {pct_has_title:.2f}%\")\nprint(f\"Records with abstract: {pct_has_abstract:.2f}%\")\nprint(f\"Records with both: {pct_has_title_and_abstract:.2f}%\")\nprint(f\"Records with text: {pct_has_title_or_abstract:.2f}%\")","ab472220":"df['title_len'] = df[\"title\"].apply(lambda x : len(str(x)))\ndf['abstract_len'] = df[\"abstract\"].apply(lambda x : len(str(x)))","bde8ce08":"df[df.has_title].title_len.hist(bins=30)","7f5dea29":"df[df.has_abstract].abstract_len.hist()","bd4614da":"df[df.title_len < 15][[\"title_len\",\"title\",\"abstract\"]]","e7eda9b7":"df[~df.has_abstract][[\"title_len\",\"abstract_len\",\"title\",\"abstract\"]]","1ed8ca05":"df[(df.abstract_len > 5) & (df.abstract_len < 30) ][[\"title_len\",\"abstract_len\",\"title\",\"abstract\"]]","cd4a2d19":"import langdetect as ld\n\nSEED= 53 \n\nfrom langdetect import DetectorFactory\nDetectorFactory.seed = SEED","d196d6a6":"def lang_detect(title, abstract):\n    try:\n        str_abstract = '' if (isNaN(abstract)) else abstract\n        str_title = '' if (isNaN(title)) else title\n        return ld.detect((str_title + ' ' + str_abstract).strip())\n    except: \n        return '--'\n\nprint(lang_detect(df.iloc[0].title, 'Hola'))\nprint(lang_detect(df.iloc[0].title, df.iloc[0].abstract))\nprint(lang_detect(df.iloc[0].title, np.nan))\nprint(lang_detect(np.nan, df.iloc[0].title))\nprint(lang_detect(np.nan, np.nan))","f2e5e0c2":"df.head().title","58202266":"df.head().apply(lambda x: lang_detect(x.title, x.abstract), axis = 1)","9cf265b8":"df['lang'] = df.apply(lambda x: lang_detect(x.title, x.abstract), axis = 1)","4ba4c1ec":"df['lang'].value_counts().plot(kind=\"barh\")","86c83b3f":"df['lang'].value_counts()","059b1817":"df['lang'].value_counts()\/n_records","48a423c3":"show_cols = ['lang','source_x','title','abstract','publish_time','journal','WHO #Covidence']","1b0ff9a8":"df[df['lang']=='en'][show_cols].head()","c76f781e":"df[df['lang']=='fr'][show_cols]","d00ec6f5":"df[df['lang']=='es'][show_cols]","f1bb7f26":"df[df['lang']=='de'][show_cols]","11123ebf":"df[df['lang']=='it'][show_cols]","c64d6999":"df[df['lang']=='zh-cn'][show_cols]","68d938b9":"df['lang'].apply(lambda x: x not in ['en','fr','es','de'])","aeffa0f5":"df[df['lang'].apply(lambda x: x not in ['en','fr','es','de','it'])][show_cols]","f40d9403":"### Exploring some results by language","ec3841c2":"We plot title length and abstract length. Further exploration shows that some of the shorter text are incorrect extractions.","0438f17f":"## Language Distribution\n\nFinally, we have a look to the language distribution and some of the results to assert the classification. \n\nResults seemed good enough","d6562cae":"## Data validation and exploration","aa4e4eea":"## Language detection\n\nWe start by using langtedect library and use the first predicted language. \n","cd789097":"There are some errors, at least when abstract is not available. As far as title is the only available text and it is short some articles get tagged as other language than English. \n\nThere are only three articles in Chinese. Chinese articles seem to have English title. \n\n** Should the Chinese literature be better represented? ** I am sure that Chinese scientist do publish in Englisg, but is it possible that a relevant part of the literature could also be in Chinese, specially as they suffered the crisis first. \n\nCan we collect data on Chinese journal?  ","370f9afd":"There are a few articles that have no title (0.4%), a larger proportion that have no abstract (20%) and even some do not have text. For those, withouth text there are no text mining to use if you don't get the full text.","5743a1de":"## Data Loading","a6d74e6c":"Almost all articles are ain English, about 96% of them. The percentage seems increasing with time.  \n\nSecond language is French (1%) and then Spanish, German and Italian. ","bbfc26ca":"There are some errors, at least when abstract is not available. ","0ed9a7dd":"# Exploratory - Language use and text properties\n\n## Objective \nScientific literature is usually written in English, however due to the nature of this crisis some articles are in several languages. \n\nThe aim of the notebook is to **explore language distribution** on the dataset and generate a data attribute. \n\nLanguage may be helpful: \n - To focus text analysis in English documents \n - Validate if prioritization of English documents is filtering meaningful info. \n - Decide if additional effort is worth on the analysis of non-english document\n - Help people with appropiate language skills (reading or analysis resources) to focus on part of the collection. \n\n## Procedure \n - Use a language detection library to tag each record. \n - Title and abstract are used for language detection. Access to full text may only provide marginal improvement. \n - Some quality issues on the content of title and abstract fields were discovered.  \n    \n## Conclusion \n  - Most articles are in English, with small proportions in French, Spanish, German or Italian\n  - A visual inspection of results shows that results are quite accurate for Englush even if the text are not long.  \n  - Nevertheless, for those records that have no abstract and short text, English articles get tagged as other languages. \n  - There are only three article in Chinese (abstract, title is in English)\n  - **Should Chinese be better represented? Could we missed something?**\n  - As English records are the vast majority, probably we should ignore language detection so far and work with the full set after cleaning empty records. \n","e021c6a4":"**TODO**: Some more filtering on title and abstract may be required for further analysis. \n  - Journal names \n  - Appendix, Image, Index, Authors, Not available, Announcement  \n  \nErrors may be introduced in the metadata extraction step, but maybe no action is required. Alternatively: use stopwords. **","ec7114e2":"We already see that some of the colums that may help to detect language have values that are not strings, like NaN or may be empty.  \nSome other records may have been incorrectly extracted, they have TOC as content (Table of Contents??). \nSo we define a helper function to filter."}}