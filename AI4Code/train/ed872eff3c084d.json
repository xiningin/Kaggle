{"cell_type":{"93126ca1":"code","eb84a8a1":"code","192fc6cb":"code","bcc828d6":"code","ada49fc2":"code","423aa1bd":"code","466ef0f3":"code","4b37d9d5":"code","e89445c1":"code","3bf8fa92":"code","98621478":"code","a7dffb83":"code","07c64e53":"code","c934b21a":"code","adf6bda4":"code","b72fbb21":"code","2908a55f":"code","46bb49c6":"code","583545eb":"code","95bea427":"code","09063bc1":"code","afc28fa8":"code","55f0fc85":"code","97de3623":"code","001acbc0":"code","81a477b4":"code","ff86e2e5":"code","f36393e1":"code","5930e276":"code","9e9d2ba5":"code","4b88a2d3":"code","2597e348":"code","5b329255":"code","df080b4f":"code","a1f17c85":"code","596f5476":"code","622549c4":"code","8d9f01ec":"code","2a25b162":"code","759a0dc0":"code","02df2085":"code","67d4bb2b":"code","6ece4e98":"markdown","0c31d78e":"markdown","02402404":"markdown","2e681dca":"markdown","4d21d4bf":"markdown","eab99389":"markdown","6e455e2e":"markdown","4928e2f8":"markdown","b87628e0":"markdown","1aa98f7f":"markdown","2c89278a":"markdown","892b7091":"markdown","7c0b9f60":"markdown","962aa320":"markdown","25278c84":"markdown","ac44183e":"markdown","c16beeec":"markdown","c095b41c":"markdown","08d17b19":"markdown","d887c30a":"markdown","3cf16615":"markdown","8916ea24":"markdown","c7542599":"markdown","be2f607e":"markdown","68ecccd0":"markdown","2d64763b":"markdown","cfbb9a27":"markdown","06f91ef1":"markdown","7bb5f4fc":"markdown","9617c001":"markdown","86f9d3d5":"markdown","d541d281":"markdown","2439d54b":"markdown","7a186c25":"markdown","2da0e5f2":"markdown","05361669":"markdown","f1177fd9":"markdown","f65b4ecf":"markdown","9ed8d3ec":"markdown","c6041e6b":"markdown","d934e423":"markdown","4f07de59":"markdown","6e5b4975":"markdown","7c71f69c":"markdown","8f5f92f8":"markdown","727b8b6f":"markdown"},"source":{"93126ca1":"!pip install pmdarima","eb84a8a1":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# import statsmodels\nfrom statsmodels.tsa.seasonal import STL\nfrom statsmodels.tsa.api import STLForecast, SARIMAX\nfrom statsmodels.tsa.arima.model import ARIMA\n\n# import sklearn\n\nimport pmdarima as pm\nfrom pmdarima.model_selection import train_test_split\n\n\nfrom sklearn.metrics import mean_squared_error as MSE\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.neural_network import MLPRegressor\n\nimport warnings\nwarnings.filterwarnings('ignore')","192fc6cb":"data = pd.read_csv('..\/input\/australian-beer-production\/datasets_56102_107707_monthly-beer-production-in-austr.csv')","bcc828d6":"data.head(3)","ada49fc2":"data.tail(3)","423aa1bd":"data.shape","466ef0f3":"data.dtypes","4b37d9d5":"data","e89445c1":"data['year_month'] = pd.to_datetime(data['Month']).dt.to_period('M')\ndata.index = data['year_month']\ndata.drop(columns=['Month','year_month'], inplace=True)","3bf8fa92":"data.rename(columns={'Monthly beer production':'AMT_BEER'}, inplace=True)\ndata.sort_index(inplace=True)\ndata.head(3)","98621478":"print(data.index.max() - data.index.min())\nprint(data.index.duplicated().sum())","a7dffb83":"data.plot(figsize=(15,5))\nplt.grid()\nplt.title('Monthly production of beer in Australia')\nplt.ylabel('Amount (unknown units)')\nplt.show()","07c64e53":"data['pct_change'] = data['AMT_BEER'].pct_change()\ndata['log_return'] = np.log(1 + data['pct_change'])\ndata[['AMT_BEER','log_return']].plot(secondary_y=['log_return'], figsize=(15,5))\nplt.show()","c934b21a":"y = data.loc[:,'AMT_BEER'].dropna()","adf6bda4":"train, test = train_test_split(y, train_size=400)","b72fbb21":"# This line below needs to be here because seasonal trend decomposition does not work with period index \ntrain.index=train.index.to_timestamp()\nstl = STL(train, period=12, robust=True)\nres_STL = stl.fit()\nfig = res_STL.plot()","2908a55f":"def SMAPE(a, f):\n    return 1\/len(a) * np.sum(2 * np.abs(f-a) \/ (np.abs(a) + np.abs(f))*100)","46bb49c6":"model = pm.auto_arima(train, seasonal=True, m=12, trace=True, stepwise=True)\nforecasts = model.predict(test.shape[0])","583545eb":"model.get_params()","95bea427":"x = np.arange(y.shape[0])\nplt.figure(figsize=(15,5))\nplt.plot(x[:400], train, c='blue', label='train')\nplt.plot(x[400:], forecasts, c='green', label='prediction')\nplt.plot(x[400:], test, c='red', label='test')\nplt.grid()\nplt.title('Monthly production of beer in Australia with SARIMA prediction')\nplt.ylabel('Amount (unknown units)')\nplt.xlabel('Number of months from 1956-01')\nplt.legend()\nplt.show()","09063bc1":"auto_arima_MSE = MSE(test.values,forecasts)\nauto_arima_SMAPE = SMAPE(test.values,forecasts)\n\nprint(\"  MSE: \", auto_arima_MSE)\nprint(\"SMAPE: \", auto_arima_SMAPE)","afc28fa8":"# train.index=train.index.to_timestamp()\nstlf = STLForecast(train, ARIMA, model_kwargs={\"order\": (2, 1, 3)}, robust=True)\nres_stl = stlf.fit()\nforecasts = res_stl.forecast(76)\nforecasts.plot(label='prediction', color='green')\ntest.plot(label='test', color='red')\nplt.title('Monthly production of beer in Australia with SARIMA using rolling prediction')\nplt.ylabel('Amount (unknown units)')\nplt.xlabel('Number of months from 1956-01')\nplt.legend()\nplt.grid()\nplt.show()","55f0fc85":"STLForecast_MSE = MSE(test.values,forecasts)\nSTLForecast_SMAPE = SMAPE(test.values,forecasts)\n\nprint(\"  MSE: \", STLForecast_MSE)\nprint(\"SMAPE: \", STLForecast_SMAPE)","97de3623":"history = [x for x in train]\npredictions = []\n\nfor t in range(len(test)):\n\tmodel = SARIMAX(history, order=(2,1,3), seasonal_order=(2,0,1,12))\n\tmodel_fit = model.fit(max_iter=200, disp=0)\n\toutput = model_fit.forecast()\n\tyhat = output[0]\n\tpredictions.append(yhat)\n\tobs = test[t]\n\thistory.append(obs)\n\tprint('predicted=%f, expected=%f' % (yhat, obs))","001acbc0":"# plot\nplt.plot(test.values, label = 'test', color='green')\nplt.plot(predictions, color='red', label = 'prediction')\nplt.title('Monthly production of beer in Australia with SARIMA using rolling prediction')\nplt.ylabel('Amount (unknown units)')\nplt.xlabel('Number of months from 1956-01')\nplt.legend()\nplt.grid()\nplt.show()","81a477b4":"Rolling_SARIMA_MSE = MSE(test.values,predictions)\nRolling_SARIMA_SMAPE = SMAPE(test.values,predictions)\n\nprint(\"  MSE: \", Rolling_SARIMA_MSE)\nprint(\"SMAPE: \", Rolling_SARIMA_SMAPE)","ff86e2e5":"data['month'] = data.index.month","f36393e1":"dum_df = pd.get_dummies(data, columns=[\"month\"], prefix=[\"month_n\"] )\ndum_df.head(3)","5930e276":"dum_df['year_ago'] = dum_df['AMT_BEER'].shift(12)\ndum_df['last_month'] = dum_df['AMT_BEER'].shift(1)\ndum_df['one_year_trend'] = dum_df['last_month'] - dum_df['year_ago']\ndum_df.head(3)","9e9d2ba5":"dum_df.dropna(inplace=True)\ndum_df.head(3)","4b88a2d3":"X_train = dum_df.loc[:'1989-05',['month_n_1','month_n_2','month_n_3','month_n_4','month_n_5','month_n_6','month_n_7','month_n_8','month_n_9','month_n_10','month_n_11','month_n_12','year_ago','last_month','one_year_trend']]\ny_train = dum_df.loc[:'1989-05','AMT_BEER']\n\nX_test = dum_df.loc['1989-05':,['month_n_1','month_n_2','month_n_3','month_n_4','month_n_5','month_n_6','month_n_7','month_n_8','month_n_9','month_n_10','month_n_11','month_n_12','year_ago','last_month','one_year_trend']]\ny_test = dum_df.loc['1989-05':,'AMT_BEER']","2597e348":"reg = LinearRegression().fit(X_train, y_train)\npredictions = reg.predict(X_test)","5b329255":"reg.coef_","df080b4f":"pd.DataFrame(data={'prediction':predictions}, index=y_test.index).plot(c='green')\ny_test.plot(label='test', c='red')\nplt.title('Monthly production of beer in Australia using Multiple linear regression')\nplt.ylabel('Amount (unknown units)')\nplt.xlabel('Year - Month')\nplt.legend()\nplt.grid()\nplt.show()","a1f17c85":"MLR_MSE = MSE(y_test.values,predictions)\nMLR_SMAPE = SMAPE(y_test.values,predictions)\n\nprint(\"  MSE: \", MLR_MSE)\nprint(\"SMAPE: \", MLR_SMAPE)","596f5476":"nn = MLPRegressor(solver='lbfgs', alpha=1e-4, hidden_layer_sizes=(15,5), max_iter=1000 ,random_state=1)\nnn.fit(X_train, y_train)\npredictions = nn.predict(X_test)","622549c4":"pd.DataFrame(data={'prediction':predictions}, index=y_test.index).plot(c='green')\ny_test.plot(c='red', label='test')\nplt.title('Monthly production of beer in Australia using Neural Network')\nplt.ylabel('Amount (unknown units)')\nplt.xlabel('Year - Month')\nplt.legend()\nplt.grid()\nplt.show()","8d9f01ec":"NN_MSE = MSE(y_test.values,predictions)\nNN_SMAPE = SMAPE(y_test.values,predictions)\n\nprint(\"  MSE: \", NN_MSE)\nprint(\"SMAPE: \", NN_SMAPE)","2a25b162":"model = pm.auto_arima(data['AMT_BEER'], seasonal=True, m=12, trace=True, stepwise=True)\nforecasts = model.predict(16)","759a0dc0":"infer_df = pd.DataFrame(data=pd.date_range(start='1995-09', end='1996-12', freq='MS'), columns=['year_month'])\ninfer_df.index = infer_df['year_month'].dt.to_period('M')\ninfer_df['OOS Prediction'] = forecasts\ninfer_df.drop(columns='year_month', inplace=True)","02df2085":"infer_df","67d4bb2b":"ax = data.loc['1990':,'AMT_BEER'].plot()\ninfer_df.plot(label='training data', ax=ax)\n\nplt.title('Monthly production of beer in Out of sample Foreast wit a slice of historical data')\nplt.ylabel('Amount (unknown units)')\nplt.xlabel('Year - Month')\nplt.legend()\nplt.grid()\nplt.show()\n","6ece4e98":"Check for possible missing data points or duplicated rows","0c31d78e":"Rename the column (name too long) and sort index, if for some reason the data was shuffled to begin with, inspect it again","02402404":"Observe the parameters of SARIMA, we will feed them into the STLForecast procedure for comparison","2e681dca":"Here we create a dataframe structure so we can place the OOS forecasts in it for plotting.","4d21d4bf":"Make a split into train and test (I would call this a validate set) datasets, this way we can compare a couple models and choose to make the final 1996 prediction with one that performed the best on the test (validate).  \nThe size 400 for the training size and remaining 76 for testing is quite arbitrary.  \nWe could have chosen any percentage from 10-30%","eab99389":"Upon inspection of order and seasonal order we find that the optimization for AIC yielded a plausible set of parameters.   \nIn a perfect world I would like to keep the complexity of the model a bit lower, but we can proceed.\nThe parameters fitted were slightly different when using my local machine - the source of this inconsistency can be quite various.  \n\nThe same can be said for the seasonal parameters, where no differencing is necessary to create stationarity.","6e455e2e":"### End of file","4928e2f8":"The results from the neural net are very similar to the MLR model.  \nThis can be expected, since the nature of the problem seems to be linear. \nWe could play around with some architecure changes, but in this case, I am satisfied with the results.","b87628e0":"Make sure before we go into modeling that no NaN values are present","1aa98f7f":"# Australian Beer Production Forecast","2c89278a":"Remove rows with missing values","892b7091":"Here we visualize the SARIMA forecast with the train and ground truth","7c0b9f60":"### Final Predictions using SARIMA method","962aa320":"Considering the testing metrics, nature of the data, and my personal experience in this matter, I choose to use SARIMA model for the OOS forecast.  \nWe will once again use the auto_arima method, but this time, we will feed the whole dataset into its training procedure.\nStarting with the most recent datapoint (1995-08) we perform a 16 step prediction to cover the last part of year 1995 and the whole of 1996.","25278c84":"Inspect the shape and data types of the tabular data","ac44183e":"From the graph above we can state that seasonality has been captured, but the model failed to take into account a slight downward trend.  \nThis is possibly due to this method not being set up as a rolling forecast.  \nWe will take a look at rolling forecasts below.","c16beeec":"Further feature engineering","c095b41c":"The goal of this exercise is to predict Australian beer production for the year 1996.\nData supplied consist of only one (endogenous) variable. This makes it a autoregressive problem, so we have to tackle it with proper tools.","08d17b19":"### SARIMA using auto_arima","d887c30a":"## Data Exploration\n\nLoad the dataset","3cf16615":"Lets calculate the symmetric mean average percentage error for this model","8916ea24":"Visualising the final OOS forecast with a bit aof historical data","c7542599":"## **Forecasting**\n\nGiven the type of data we have, I would advise to go directly to SARIMA as the model of choice, since it allows for seasonality and trend modeling.  \nFor the training and forcasting part, we can use library called pmdarima. It is a wrapper around statsmodels own SARIMAX class.  \nUtilizing this library gives us great time advantage since using an \"auto\" method to search through possible parameter choices is very fast.  \nOne does not have to spend time on analysis of ACF or PCF, which from my experience dont provide definitive parameters for SARIMA, only a guidance to start a parameter grid search.  \nThe choice for seasonality period in this type of problem is straightfoward and we set is as 12 - the number of months in one period (year).\n\nAs for the models that can be explored the availabile techniques are various.  \n* AR, MA, ARMA, ARIMA - all good candidates, but since we observe strong seasonality these are suboptimal choices\n* SARIMA, SARIMAX - combining autoreggresive integrated moving average model with the same for seasonality gives us the best chances. Since we dont have any exogenous variables, we won't be using SARIMAX \n* discrete Fourier transforms - this model is very popular in physics, but I personally have limited experience with it\n* Multiple linear regression - we create features comparable to SARIMA process\n* Neural Network - could be of feedforward design, or LSTM seq2seq architecture. These however, need a lot of data to train on, which is not our case.\n\nAll things considered, I will pursue to compare 3 different techniques - SARIMA, multiple linear regression, and a Feedforward Neural Network.  \nThere are several metrics that could be useful to gauge the performance of the model.  Since it is a regression problem, we will use Mean squared error (MSE) and symetric mean average percentage error (SMAPE)","be2f607e":"Now that the timeseries is split, we can look at how the residuals change with time, more \"rare\" events occur, seasonality also grew since 1968, but proportionally to the mean level","68ecccd0":"Inspect the tabular data","2d64763b":"Definition of train and test datasets","cfbb9a27":"Create some stationarity in the input data and observe if it can help our prediction task","06f91ef1":"## Out of sample prediction - year 1996","7bb5f4fc":"We one-hot encode month to be used as a variable","9617c001":"### Multiple linear regression","86f9d3d5":"Just to visualise again what we are working with, we use seasonal trend decomposition to split the timeseries into trend, seasonality and residuals","d541d281":"When researching the work done on this subject I came across a method that utilizes the seasonal decompose STL class from statsmodels.  \nFor illustrative purposes I leave here the results from this STLForecast method for comparison.","2439d54b":"### Feedforward Neural Network\n\nWe can experiment with lightweight NN architectures, which teoretically can learn non-linear relationships with 2 hidden layers.  \nThe net has to be small, since the number of data points is small and we risk overfitting problems.  \nThis is why I chose a shallow architecture with few neurons","7a186c25":"definition of SMAPE","2da0e5f2":"Month is in object format, for ease of use we can cast it as datetime or period index","05361669":"### SARIMA implemented in STLForecast","f1177fd9":"The results from STLForecast method are similar but a bit worse than auto-arima approach","f65b4ecf":"Both metrics have dropped, as we expected. With shortened prediction horizont the ARIMA structure yields smaller errors.  \nThis information is useful, since our task is to predict production only one year ahead - the whole year 1996","9ed8d3ec":"Imports","c6041e6b":"We start with creating features that can be usefull, such as lagged observations, some trend definition, and other","d934e423":"This is also a rolling prediction because of the way we crafted features.  \nTherefore when comparing the SARIMA rolling and MLR we can conclude that SARIMA procedure yields better results.  \nThe performance of this model could be further improved upon by crafting other features such as moving averages, min\/max, volatility or just more lags of the endogenous variable","4f07de59":"#### Rolling prediction\n\nSo far we have used n-step prediction from fixed point in time, no new \"rolling\" are used.  \nThese predictions underestimate the real-life performance of the model since as new data arrives we have the ability to spot any new trend or extreme changes and retrain\/adjust our modelling strategy.  \nThis is why a rolling forecast (always only a couple of steps ahead) might serve as a beter gauge of the real world application.\nFrom the auto arima method we learned the optimal parameters of SARIMA sa we keep them fixed with each timestep","6e5b4975":"We can comment on the beer production from inspecting the visualisation of timeseries. Several features are obvious.\n1) the time series is not stationary (we could use some test to test the hypothesis)\n2) there has been a regime change from upward trending to lightly downward trending \n3) the volatility seem to increase in time (we could analyse the changes or residuals after fitting trendline)\n\nAll of these observations can be tested, but by no means alone bring us closer to making the best possible OOS prediction for 1996.\nI have been speculating with multiple methods of countering this problem, such as transforming the timeseries to start with.\nWe could perform some transformation, differencing, smoothing, outlier deletion, or removal of old data, but in this case none brought much benefit.\nI will continue in this notebook to show my final few methods only.","7c71f69c":"Define and fit a multiple linear regression model","8f5f92f8":"Check the coefficients for huge or very small weights which might indicate unimportant or overfitted variables.  \nIn this case the weights are well behaved.","727b8b6f":"Plot the timeseries"}}