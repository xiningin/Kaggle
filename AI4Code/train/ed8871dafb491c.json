{"cell_type":{"b0803ebd":"code","05b0b11b":"code","99192bfd":"code","6b9df99a":"code","d94b51f5":"code","7b0ca6f3":"code","a1dce49c":"code","6570a5ff":"code","22f1ad5c":"code","e308ce23":"code","b1f9acff":"code","48f4f0b0":"code","eb7dee34":"code","23eaea77":"code","3673a45b":"code","8dd9442e":"code","37948b8c":"code","58bda70b":"code","13b671c5":"code","936cd73e":"code","0f7c8bfc":"code","1a39ce43":"code","80ec7339":"code","bf360185":"code","ef55fa8b":"code","cab9939e":"code","58da3663":"code","2f9c6a2f":"markdown","e67e9acd":"markdown","96c8288b":"markdown","2715a165":"markdown","72d0096b":"markdown","e8eac0d5":"markdown","ac6a34be":"markdown","495bbc37":"markdown","92b196c1":"markdown","1edc4c95":"markdown","387eea6b":"markdown","14b20925":"markdown","49804e9c":"markdown","0fda164b":"markdown","aa24516a":"markdown","36ace2ad":"markdown","d273de45":"markdown","76a7000a":"markdown","abc22203":"markdown","f86c6690":"markdown","1ec430e7":"markdown","1efbfb67":"markdown","1077a3bd":"markdown","b583b68e":"markdown","14f2bc06":"markdown","eef61646":"markdown","7bef3d7c":"markdown","a38d2dc0":"markdown","f5f1f34a":"markdown","518e14dd":"markdown"},"source":{"b0803ebd":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\nplt.style.use('ggplot')\n%matplotlib inline","05b0b11b":"df = pd.read_csv('..\/input\/train.csv')\ndf_test = pd.read_csv('..\/input\/test.csv')\nall_features = df.columns.tolist()","99192bfd":"df.head()","6b9df99a":"df.info()","d94b51f5":"df.describe()","7b0ca6f3":"df_X = df[['Pclass', 'Age', 'SibSp', 'Parch', 'Fare']]","a1dce49c":"_ = pd.plotting.scatter_matrix(df_X, figsize=[10,10], s=150, marker='D')","6570a5ff":"plt.figure()\nsns.countplot(x='Embarked', hue='Embarked', data=df)\nplt.xticks([0,1,2],df['Embarked'].drop_duplicates().tolist())","22f1ad5c":"df['Embarked'].loc[df['Embarked'].isnull()] = 'S'\ndf['Embarked'].count()","e308ce23":"from sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\ndf['Sex'] = le.fit_transform(df['Sex'])\ndf['Embarked'] = le.fit_transform(df['Embarked'])\nplt.figure(figsize=(12,12))\nsns.heatmap(df.corr(), square=True, annot=True, cmap='PRGn')","b1f9acff":"from sklearn.impute import SimpleImputer\nimp = SimpleImputer(missing_values=np.nan)\n\nto_impute = df[['Age']]\nimputed = imp.fit_transform(to_impute)\ndf['Age'] = pd.Series(imputed.reshape(891,))\nplt.figure(figsize=(12,12))\nsns.heatmap(df.corr(), square=True, annot=True, cmap='PRGn')","48f4f0b0":"from sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2\n\nX = df.select_dtypes(['number'])\nX = X.drop(columns=['PassengerId','Survived'])\ny = df['Survived']\n\n# apply SelectKBest class to extract top 10 best features\nbestfeatures = SelectKBest(score_func=chi2, k=5)\nfit = bestfeatures.fit(X,y)\ndfscores = pd.DataFrame(fit.scores_)\ndfcolumns = pd.DataFrame(X.columns)\n\n# concat two dataframes for better visualization \nfeatureScores = pd.concat([dfcolumns,dfscores],axis=1)\nfeatureScores.columns = ['Specs','Score']  # naming the dataframe columns\nprint(featureScores.nlargest(5,'Score'))  #print 5 best features","eb7dee34":"from sklearn.ensemble import ExtraTreesClassifier\n\nmodel = ExtraTreesClassifier(n_estimators=100)\nmodel.fit(X,y)\nprint(model.feature_importances_) #use inbuilt class feature_importances of tree based classifiers\n\n#plot graph of feature importances for better visualization\nfeat_importances = pd.Series(model.feature_importances_, index=X.columns)\nfeat_importances.nlargest(5).plot(kind='barh')","23eaea77":"# if we want to play with different combinations of features\nprint(all_features)","3673a45b":"list_of_features = ['Sex', 'Fare', 'Pclass', 'Age', 'Embarked']\ndf = X[list_of_features]\ndf.head()","8dd9442e":"df_onehot = df.copy()\ndf_onehot = pd.get_dummies(df_onehot, columns=['Pclass'], prefix=['class'])\ndf_onehot = pd.get_dummies(df_onehot, columns=['Embarked'], prefix=['emb'])","37948b8c":"from sklearn.preprocessing import scale\n\ndf_onehot['Fare'] = scale(df_onehot['Fare'])\ndf_onehot['Age'] = scale(df_onehot['Age'])\n# df_onehot['Parch'] = scale(df_onehot['Parch'])\n\ny = y\nX = df_onehot","58bda70b":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state=42, stratify=y)","13b671c5":"from sklearn.svm import SVC\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier, RandomForestClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import RandomizedSearchCV\n\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\n\n# Hyperparameters and classifiers:\n# SVC\nsvm_c_space = [0.1,0.075, 0.05,0.025]\nsvm_kernels = ['linear', 'poly', 'rbf', 'sigmoid']\nsvm_gamma = [1,25, 1, 0.75, 0.5]\nparam_grid_svc = dict(C=svm_c_space,\n                      kernel=svm_kernels,\n                      gamma=svm_gamma)\nclf_svc = SVC()\n\n# Logistic Regression\nc_space = np.logspace(0,2,10)\nsolvers = ['liblinear', 'newton-cg', 'lbfgs', 'sag', 'saga']\nparam_grid_logreg = dict(C=c_space,\n                         solver=solvers)\nclf_logreg = LogisticRegression()\n\n# KNearest Neigbors\nn_neighbors = range(1,12)\nparam_grid_knn = dict(n_neighbors=n_neighbors)\nclf_knn = KNeighborsClassifier()\n\n# Random Forest\nestimators_space = [100,250,500]\nmin_sample_splits = range(2,8)\nmin_sample_leaves = range(1,5)\nmax_features = ['sqrt', 'log2', None]\nparam_grid_forest = dict(n_estimators=estimators_space, \n                        min_samples_split=min_sample_splits,\n                        min_samples_leaf=min_sample_leaves,\n                        max_features=max_features)\nclf_forest = RandomForestClassifier()\n\n# Decision Tree\nmin_sample_splits = range(2,12)\nmin_sample_leaves = range(1,8)\nmax_features = ['sqrt', 'log2', None]\nparam_grid_tree = dict(min_samples_split=min_sample_splits,\n                       min_samples_leaf=min_sample_leaves,\n                       max_features=max_features)\nclf_tree = DecisionTreeClassifier()\n\n# AdaBoost\n# ada_estimators = [25,50,100]\nada_lr = [1, 0.5, 0.1]\nparam_grid_ada = dict(n_estimators=estimators_space,\n                     learning_rate=ada_lr)\nclf_ada = AdaBoostClassifier()\n\n# GradientBoosting\ngb_lr = [0.2, 0.1, 0.01]\nparam_grid_gb = dict(n_estimators=estimators_space,\n                     learning_rate=gb_lr)\nclf_gb = GradientBoostingClassifier()\n\n# LinearDiscriminantAnalysis\nlda_solvers = ['svd', 'lsqr', 'eigen']\nparam_grid_lda = dict(solver=lda_solvers)\nclf_lda = LinearDiscriminantAnalysis()","936cd73e":"# creating loop to test all classifiers\nmodels_to_test = ['GradientBoosting',\n                  #'LDA',\n                  #'LogisticRegression',\n                  #'KNN',\n                  #'SVC',\n                  'RandomForest',\n                  #'DecisionTree',\n                  'AdaBoost'\n                 ]\nclassifier_dict = dict(LogisticRegression=clf_logreg,\n                      KNN=clf_knn,\n                      SVC=clf_svc,\n                      RandomForest=clf_forest,\n                      DecisionTree=clf_tree,\n                      AdaBoost=clf_ada,\n                      GradientBoosting=clf_gb,\n                      LDA=clf_lda\n                      )\nparam_grid_dict = dict(LogisticRegression=param_grid_logreg,\n                       KNN=param_grid_knn,\n                       SVC=param_grid_svc,\n                       RandomForest=param_grid_forest,\n                       DecisionTree=param_grid_tree,\n                       AdaBoost=param_grid_ada,\n                       GradientBoosting=param_grid_gb,\n                       LDA=param_grid_lda\n                      )\ncv = 5\nscore_dict = {}\nparams_dict = {}\nclassification_report_dict = {}\nconf_matr_dict = {}\nbest_est_dict = {}\n\nfor model in models_to_test:\n    # classifier = RandomizedSearchCV(classifier_dict[model], param_grid_dict[model], cv=cv, n_jobs=-1)\n    classifier = GridSearchCV(classifier_dict[model], param_grid_dict[model], cv=cv, n_jobs=-1)\n    \n    classifier.fit(X_train, y_train)\n    y_pred = classifier.predict(X_test)\n    \n    # Print the tuned parameters and score\n    print(\" === Start report for classifier {} ===\".format(model))\n    score_dict[model] = classifier.best_score_\n    print(\"Tuned Parameters: {}\".format(classifier.best_params_)) \n    params_dict = classifier.best_params_\n    print(\"Best score is {}\".format(classifier.best_score_))\n\n    # Compute metrics\n    classification_report_dict[model] = classification_report(y_test, y_pred)\n    print(\"Classification report for {}\".format(model))\n    print(classification_report(y_test, y_pred))\n    conf_matr_dict[model] = confusion_matrix(y_test, y_pred)\n    print(\"Confusion matrix for {}\".format(model))\n    print(confusion_matrix(y_test, y_pred))\n    print(\" === End of report for classifier {} === \\n\".format(model))\n    \n    # Add best estimator to the dict\n    best_est_dict[model] = classifier.best_estimator_\n    \n# Creating summary report\nsummary_cols = ['Best Score']\nsummary = pd.DataFrame.from_dict(score_dict, orient='index')\nsummary.index.name = 'Classifier'\nsummary.columns = summary_cols\nsummary = summary.reset_index()","0f7c8bfc":"# Visualizing results\nplt.xlabel('Best score')\nplt.title('Classifier Comparison')\n\nsns.barplot(x='Best Score', y='Classifier', data=summary)","1a39ce43":"# Get the name of the best performing model\nname_of_best_model = summary['Classifier'][summary['Best Score'] == summary['Best Score'].max()].tolist()[0]\nclf_selected = best_est_dict[name_of_best_model]\nprint(\"Selected model - {}\".format(clf_selected))","80ec7339":"df_to_test = pd.read_csv('..\/input\/test.csv')\ndf_to_test.head()","bf360185":"df_to_test.info()","ef55fa8b":"# Drop useless columns\ndf_test = df_to_test[list_of_features]\n\n# Encode categorical values\ndf_test['Sex'] = le.fit_transform(df_test['Sex'])\ndf_test = pd.get_dummies(df_test, columns=['Embarked'], prefix=['emb'])\ndf_test = pd.get_dummies(df_test, columns=['Pclass'], prefix=['class'])\n\n# Impute missing values\ndf_test['Age'] = df_test['Age'].fillna(0)\nimp.fit_transform(np.array(df_test['Age']).reshape(1,-1))\ndf_test['Fare'] = df_test['Fare'].fillna(0)\nimp.fit_transform(np.array(df_test['Fare']).reshape(1,-1))\n\n# Scale\ndf_test['Fare'] = scale(df_test['Fare'])\ndf_test['Age'] = scale(df_test['Age'])\n# df_test['Parch'] = scale(df_test['Parch'])\ndf_test.head()","cab9939e":"# Predict the labels of the test set\ny_pred = clf_selected.predict(df_test)","58da3663":"df_to_test['Survived'] = pd.Series(y_pred)\npd.DataFrame(df_to_test[['PassengerId','Survived']]).to_csv('predictions.csv', index=False)\ndf_to_test.head()","2f9c6a2f":"What the hell is 'Parch'? Heading back to description\n\nAnswer: parents \/ children on board of Titanic","e67e9acd":"Using OneHot encoder encoding feature 'Pclass' and 'Embarked'","96c8288b":"Not the cleanest implementation, not all possible work done, especially with feature engineering, not the highest score, not the best example, but it may give you some hint on where to direct your thoughts and what can be improved in your own model.","2715a165":"## Building the model","72d0096b":"Here we encode our categorical values","e8eac0d5":"## Feature Selection","ac6a34be":"As we remember there were two missing values for 'Embarked' feature let's assign them value 'S' as it won't change anything, but will remove headache in the next steps","495bbc37":"Preprocessing hold-out dataset","92b196c1":"Selecting our feature for predictions and trying different scenarios","1edc4c95":"### Univariate Selection","387eea6b":"After model has been trained and selected the best one let's make our predictions on hold-out dataset","14b20925":"## Testing on hold-out dataset","49804e9c":"## Visual EDA","0fda164b":"Adding scaling to numeric features 'Fare' and 'Age'","aa24516a":"And here we see the rise of the 'Age'. What remains clear is that 'Sex' and 'Fare' are the most influential features. (Sex and Fare are influential in real life too BTW :D)","36ace2ad":"**Insights from .info():**\n* Age, Cabin and Embarked have missing values\n* dtypes: float64(2): Fare, Age; int64(5): PassengerID, Survived, Pclass, SibSp, Parch; object(5): the rest","d273de45":"What if missing values in Age column make it seem unimportant? Will deal with that and plot heatmap again to see if that changes anything.","76a7000a":"Trying different models using GridSearchCV and RandomizedSearchCV\n\nImporting models and creating parameters grids","abc22203":"Creating array of predicted outcomes from our model","f86c6690":"And this test says Fare is the most important feature. Interesting. Moving forward.\n\n### Feature importance","1ec430e7":"**Insights from .describe():**\n* Pclass ranges from 1 to 3 -> OneHotEncode","1efbfb67":"Well, no change at all. Which kind of make sense when you impute missing values with means.\n\nSeems like we can select only Pclass, Sex and Fare for our model to make predictions. Although, seeing that Pclass and Fare are actually correlated we can even drop Fare.","1077a3bd":"But let's check the power of our features with other methods.","b583b68e":"Handling missing values in 'Age' column. Let's impute them with means.","14f2bc06":"### Automatically selecting best model and it's parameters","eef61646":"Creating train and test split to be able to evaluate model performance later","7bef3d7c":"After running this code multiple times the best classificators in this scenario are RandomForest and GradientBoost, so the other models are commented out. Experience :D","a38d2dc0":"Exporting data in the format of competition","f5f1f34a":"## Selecting models and checking their performance\n\nCreating a loop that will test all models and will save results of each test","518e14dd":"## **Numerical EDA**"}}