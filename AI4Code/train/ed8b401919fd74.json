{"cell_type":{"5412afa4":"code","608ba138":"code","d0316c38":"code","32910186":"code","7d9a8024":"code","f297fd30":"code","b3772d85":"code","d94076b5":"code","1b464026":"code","b5211c07":"code","83f450a7":"code","04ead992":"markdown","499af4d9":"markdown","a71d6ae5":"markdown","48b8c5fa":"markdown","e5054d19":"markdown","b7779418":"markdown","4366ab1e":"markdown","8cb6cd84":"markdown","0800f5ed":"markdown"},"source":{"5412afa4":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n%matplotlib inline\nimport matplotlib.pyplot as plt  # Matlab-style plotting\nimport seaborn as sns\nimport warnings\ndef ignore_warn(*args, **kwargs):\n    pass\nwarnings.warn = ignore_warn #ignore annoying warning (from sklearn and seaborn)\n\n\nfrom scipy import stats\nfrom scipy.stats import norm, skew #for some statistics\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))","608ba138":"from sklearn.model_selection import KFold, cross_val_score, train_test_split\n\n#Validation function\ndef rmsle_cv(X, y, model, n_folds = 10):\n    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(train.values)\n    rmse = np.sqrt(-cross_val_score(model, X, y, scoring=\"neg_mean_squared_error\", cv = kf))\n    return(rmse)","d0316c38":"import statsmodels.api as sm\ntrain = pd.read_csv('..\/input\/preprocessed_training.csv')\nprint('Feature types: {}'.format(train.dtypes.unique())) # all features are numerical\n\nprice = train['SalePrice']\ntrain.drop('SalePrice', axis=1, inplace=True);","32910186":"kf = KFold(n_splits=10, random_state=42, shuffle=True)\nprint(kf)  \n\nresults = []\n\nfor train_index, test_index in kf.split(train):\n    X_train, X_test = train.iloc[train_index], train.iloc[test_index]\n    y_train, y_test = price.iloc[train_index], price.iloc[test_index]\n    glm_gaussian = sm.GLM(y_train, X_train)\n    mod = glm_gaussian.fit()\n    pred_test = mod.predict(X_test)\n    results.append(np.sqrt(np.mean((pred_test-y_test)**2)))\n    \nprint(\"GLM score: {:.4f} ({:.4f})\\n\".format(np.mean(results), np.std(results)))","7d9a8024":"log_price = np.log1p(price)\n\nkf = KFold(n_splits=10, random_state=42, shuffle=True)\nprint(kf)  \n\nresults = []\n\nfor train_index, test_index in kf.split(train):\n    X_train, X_test = train.iloc[train_index], train.iloc[test_index]\n    y_train, y_test = log_price.iloc[train_index], log_price.iloc[test_index]\n    glm_gaussian = sm.GLM(y_train, X_train)\n    mod = glm_gaussian.fit()\n    pred_test = mod.predict(X_test)\n    results.append(np.sqrt(np.mean((pred_test-y_test)**2)))\n    \nprint(\"GLM score: {:.4f} ({:.4f})\\n\".format(np.mean(results), np.std(results)))\n","f297fd30":"(np.exp(0.1168)-1)*100","b3772d85":"from sklearn.metrics import mean_absolute_error\n\nkf = KFold(n_splits=10, random_state=42, shuffle=True)\nprint(kf)  \n\nresults = []\n\nfor train_index, test_index in kf.split(train):\n    X_train, X_test = train.iloc[train_index], train.iloc[test_index]\n    y_train, y_test = log_price.iloc[train_index], log_price.iloc[test_index]\n    glm_gaussian = sm.GLM(y_train, X_train)\n    mod = glm_gaussian.fit()\n    pred_test = mod.predict(X_test)\n    results.append(mean_absolute_error(pred_test, y_test))\n    \nprint(\"GLM score: {:.4f} ({:.4f})\".format(np.mean(results), np.std(results)))\nprint(\"Mean percentage error: {:.4f}\\n\".format((np.exp(np.mean(results))-1)*100))\n","d94076b5":"glm_gaussian = sm.GLM(log_price, train)\nmod = glm_gaussian.fit()\npred_test = mod.predict(train)\n\nfig, ax = plt.subplots()\nax.scatter(x=log_price, y=log_price - pred_test)\nplt.ylabel('Residuals', fontsize=13)\nplt.xlabel('log(y)', fontsize=13)\nax.axhline(y=0)\nplt.title('Residual plot')\nplt.show()\n\nsns.distplot(log_price - pred_test)\nplt.title('Residual distribution')","1b464026":"from sklearn.ensemble import GradientBoostingRegressor\n\nGBoost = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05,\n                                   max_depth=4, max_features='sqrt',\n                                   min_samples_leaf=15, min_samples_split=10, \n                                   loss='huber', random_state =5)","b5211c07":"score = rmsle_cv(train, log_price, GBoost)\nprint(\"Gradient Boosting score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","83f450a7":"from sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\nscaled_df = scaler.fit_transform(train)\nscaled_df = pd.DataFrame(scaled_df, columns=train.columns)\n\nglm_gaussian = sm.GLM(price, scaled_df)\nmod = glm_gaussian.fit()\nmod.summary()","04ead992":"The RMSE is quite high! We will get better results using the log(x+1) transformation, as discussed in the EDA kernel.","499af4d9":"We obtain a value similar to the RMSE from more complex models in other kernels. It is possible to obtain better results by ensembling, but we will not explore it here.\n\nBut what does a rmse of 0.11 represent? ","a71d6ae5":"# Testing different models based on the preprocessed dataset\n\nHi all, I'm continuing my slighly different approach to the housing problem, which I began with the kernel [New approach to EDA and feature creation](https:\/\/www.kaggle.com\/pretorias\/new-approach-to-eda-and-feature-creation). This kernel imports the training dataset obtained after the preprocessing. Note that I have skipped applying the transformations to the test dataset, as the goal of both of my kernels is to try new ideas and learn Python commands. You will have to make some modifications if your ultimate goal is to make a submission.\n\nIn this kernel I will fit different common models (without ensembling) and I will compare their cross-validated performance. After that I will optimize the hyper-parameters of one or two promising models.\n\nLets first load the common data analysis libraries, we will load modelling libraries as we go.","48b8c5fa":"Almost no difference in comparison with the linear model!","e5054d19":"The residuals look quite good, there are some extreme values, especially for the cheaper houses, but the distribution looks normal.","b7779418":"To evaluate the goodness of fit, lets fit a model with all the data and analyze the residuals. Keep in mind that a good model fit doesn't automatically translate to good predictions (that's why we crossvalidate).","4366ab1e":"# GLM model\n\nWe will begin with GLMs, that are simple, flexible and powerful and thus a good starting point.\n\nI have seen that unfortunately the integration of GLMs in scikit is a bit poor, therefore we will calculate the cross-validated RMSE by hand. Luckily it is quite easy!\n\nWe will begin with a GLM with a normal probability distribution and the identity as link function, i.e. an ordinary linear model.","8cb6cd84":"We are speaking about a 12% error \"on average\" (larger errors have a bigger weight in the RMSE). Just out of curiosity lets calculate the MAE (mean absolute error), which is easier to interpret.","0800f5ed":"# Gradient Boosting Regression\n\nLets train a more complex model for comparison. I will use the same parameters used in the kernel of Serigne."}}