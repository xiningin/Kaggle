{"cell_type":{"ca6e84e9":"code","3fbf16fc":"code","d14ada78":"code","91b8d33a":"code","f76c9512":"code","374cc69a":"code","8a233ea2":"code","2b3372af":"code","b3182355":"code","ad598684":"code","49bed4e0":"code","58c4e0ee":"code","314ffad9":"code","4704f63f":"code","e30adc31":"code","1d4eb5a3":"code","1b0a20dd":"code","5cb60255":"code","d6fec8bd":"code","50faef0f":"code","7d84348c":"code","c096e53a":"code","d2e4a655":"code","60f07e4d":"code","825b8c25":"code","066ece27":"code","c835a9c5":"code","5c1199fc":"code","4c0126a2":"code","55e48d1e":"code","9d5fdade":"code","8525c46e":"code","b9e27a72":"code","82e84f10":"code","3d1a5c98":"code","7e590453":"code","252ac267":"code","3be41d5c":"code","2d585887":"code","69fd4ed0":"markdown","d1ed4f43":"markdown","a9b468df":"markdown","6d706dd1":"markdown"},"source":{"ca6e84e9":"#importing the libraries\nimport numpy as np\nimport matplotlib.pyplot as plt \nimport pandas as pd","3fbf16fc":"#importing the training set\ndataset_train = pd.read_csv(\"..\/input\/google-stock-price\/Google_Stock_Price_Train.csv\")\ndataset_train.head()","d14ada78":"dataset_train.shape","91b8d33a":"#Extract the Open Column and convert to an array for forecasting\ntraining_set =  dataset_train.iloc[:,1:2].values #.values converts it to an array","f76c9512":"#Feature Scaling\nfrom sklearn.preprocessing import MinMaxScaler\nsc = MinMaxScaler(feature_range=(0,1), copy = True)\n\ntraining_set_scaled = sc.fit_transform(training_set)","374cc69a":"#Number of timesteps\n#Create a data structure with 60 timesteps and 1 output \n#Look at the last 60 timesteps (last 60 days) to learn and try to predict the next timestep\n\nx_train = []\ny_train = []\n\nfor i in range(60,1258):\n    x_train.append(training_set_scaled[i-60:i,0])\n    y_train.append(training_set_scaled[i,0])\n\nx_train, y_train = np.array(x_train), np.array(y_train)\n\n","8a233ea2":"#Add new dimensions that's needed for your forecast (Reshape function to add a dimension to a numpy array)\n\nx_train = np.reshape(x_train, (x_train.shape[0],x_train.shape[1],1)) #Last 1 is the number of predictors which is 1 the open stock price\n","2b3372af":"#import libraries\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.layers import LSTM\nfrom tensorflow.keras.layers import Dropout","b3182355":"#Initialize the RNN\nregressor = Sequential()","ad598684":"x_train.shape[0], x_train.shape[1] ","49bed4e0":"#Adding the first LSTM layer and some dropout regularization to avoid overfitting\n\n#Add first LSTM layer\nregressor.add(LSTM(units= 50, return_sequences=True, input_shape = (x_train.shape[1],1) )) #No of LSTM cells = units, return_sequences = True because we are building a stacked LSTM which will have several LSTM layers, when you are done adding LSTM layers set it to False whch is the default, input_shape (timesteps,predictors)\n#Add dropout regularization\nregressor.add(Dropout(rate = 0.2)) #rate of neurons you want to drop during regularization (during each iteration of the forward and back propagation)\n","58c4e0ee":"#Add Second LSTM Layer and dropout regularization \nregressor.add(LSTM(units= 50, return_sequences=True )) #No of LSTM cells = units, return_sequences = True because we are building a stacked LSTM which will have several LSTM layers, when you are done adding LSTM layers set it to False whch is the default \n#Add dropout regularization\nregressor.add(Dropout(rate = 0.2)) #rate of neurons you want to drop during regularization (during each iteration of the forward and back propagation)\n","314ffad9":"#Add Third LSTM Layer and dropout regularization \nregressor.add(LSTM(units= 50, return_sequences=True )) #No of LSTM cells = units, return_sequences = True because we are building a stacked LSTM which will have several LSTM layers, when you are done adding LSTM layers set it to False whch is the default \n#Add dropout regularization\nregressor.add(Dropout(rate = 0.2)) #rate of neurons you want to drop during regularization (during each iteration of the forward and back propagation)\n","4704f63f":"#Add Fourth LSTM Layer and dropout regularization \n#return_sequences=False because this is our last LSTM layer\nregressor.add(LSTM(units= 50, return_sequences=False )) #No of LSTM cells = units, return_sequences = True because we are done adding LSTM layers \n#Add dropout regularization\nregressor.add(Dropout(rate = 0.2)) #rate of neurons you want to drop during regularization (during each iteration of the forward and back propagation)\n","e30adc31":"#Add the output layer for full connection\n\nregressor.add(Dense(units=1 ))","1d4eb5a3":"#Compiling the RNN\n\nregressor.compile(optimizer=\"adam\", loss = \"mean_squared_error\") #optimizer=\"rmsprop\" recommended for RNN but adam is always a safe and good choice\n#loss = \"mean_squared_error\" for regression ","1b0a20dd":"#Fitting the RNN to the Training set\n\nregressor.fit(x_train,y_train, epochs= 100, batch_size=32) #Experimented with 50 and 100 is where there was convergence with the loss meaning the last 20 to 30 epochs, the loss didin't change much\n","5cb60255":"#Getting the real stock price of 2017\n\n#importing the test set\ndataset_test = pd.read_csv(\"..\/input\/google-stock-price\/Google_Stock_Price_Test.csv\")\n\nreal_stock_price =  dataset_test.iloc[:,1:2].values #.values converts it to an array\n","d6fec8bd":"#Concatenating the training dataset and test dataset by row to form a total dataset\ndataset_total = pd.concat((dataset_train[\"Open\"], dataset_test[\"Open\"]), axis = 0)\n\n#Extract last 60 data from the training dataset + all the data from the test dataset (60 because we trained the model with 60 timesteps)\ninputs = dataset_total[len(dataset_total) - len(dataset_test) - 60:].values\n\ninputs = inputs.reshape(-1,1)\n\n#Feature Scaling\ninputs = sc.transform(inputs)\n","50faef0f":"inputs.shape","7d84348c":"#Reshape the input data into the shape the RNN model was trained on \n\nx_test = []\nfor i in range(60,80): #test data contains 80 rows of data\n    x_test.append(inputs[i-60:i,0])\n\nx_test  = np.array(x_test) \n\nx_test = np.reshape(x_test, (x_test.shape[0],x_test.shape[1],1)) #Converts to the 3d format that can go into the RNN as input\n\n","c096e53a":"predicted_stock_price = regressor.predict(x_test)\npredicted_stock_price  = sc.inverse_transform(predicted_stock_price )","d2e4a655":"plt.plot(predicted_stock_price, color=\"red\", label= \"Predicted\")\nplt.plot(real_stock_price, color=\"green\", label= \"Real\")\nplt.xlabel(\"Period\")\nplt.ylabel(\"Stock Price\")\nplt.title(\"Stock Price Forecast\")\nplt.legend()\nplt.show()\n\n","60f07e4d":"#Model Evaluation\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import r2_score\n\n\nimport math \n\n\n\nMAE_RNN = mean_absolute_error(real_stock_price,predicted_stock_price) #8.477951782226564\n\nMSE_RNN = mean_squared_error(real_stock_price,predicted_stock_price) #131.51032080751082\n\nRMSE_RNN = math.sqrt(MSE_RNN) #11.467794940942692\n\nR2_RNN = r2_score(real_stock_price,predicted_stock_price) #0.39490875968951855\n\nreal_stock_price_mean = real_stock_price.mean()  #807.5260000000001\n\n\nprint(\"Mean Absolute Error:\", MAE_RNN)\nprint(\"Mean Squared Error:\", MSE_RNN)\nprint(\"Root Mean Squared Error:\", RMSE_RNN)\nprint(\"R Squared:\", R2_RNN)\nprint(\"Real Stock Price Mean:\", real_stock_price_mean)\nprint(\"RMSE_RNN\/REAL_STOCK_PRICE_MEAN:\", RMSE_RNN\/real_stock_price_mean) #0.014201146391500325\n\n","825b8c25":"#importing the libraries\nimport numpy as np\nimport matplotlib.pyplot as plt \nimport pandas as pd","066ece27":"#importing the training set\ndf = pd.read_csv(\"..\/input\/google-stock-price\/Google_Stock_Price_Train.csv\",index_col= 'Date', parse_dates=True)\n\ndf.head()\n ","c835a9c5":"print('Shape of data', df.shape)","5c1199fc":"#Plot data\ndf['Open'].plot(figsize=(12,5))","4c0126a2":"pip install pmdarima","55e48d1e":"#Get best p,d,q value for ARIMA model\n\nfrom pmdarima import auto_arima \n#Ignore harmless warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n\n#The goal is to minimize the AIC and get the best order with the lowest AIC\n\nstepwise_fit = auto_arima(df['Open'], trace=True, suppress_warnings=True)\nstepwise_fit.summary()\n \n\n#Best model:  ARIMA(0,1,0)(0,0,0)[0]\n\n","9d5fdade":"print(df.shape)\n","8525c46e":"#257 is 20% of the total rows of data which i want to split into the test sample\nfrom statsmodels.tsa.arima_model import ARIMA\ntrain=df.iloc[:-257]\ntest=df.iloc[-257:]\nprint(train.shape, test.shape)","b9e27a72":"\"\"\"\nTrain the model with the best model parameters gotten earlier\n\"\"\"\n\nmodel = ARIMA(train['Open'],order=(0,1,0) )\nmodel = model.fit()\nmodel.summary()\n","82e84f10":"\"\"\"\nMake Predictions on Test Set\n\"\"\"\n\nstart = len(train)\nend = len(train) + len(test) - 1\npred = model.predict(start=start,end=end,typ='levels')\nprint(pred)\n\n#Add the dates as index from the data frame\n\npred.index=df.index[start:end+1]\nprint(pred)\n","3d1a5c98":"from sklearn.metrics import mean_absolute_error\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import r2_score\n\n\nimport math \n\nfrom sklearn.metrics import mean_squared_error \nfrom math import sqrt \n \nMAE = mean_absolute_error(test['Open'], pred) #65.16489906613461\nMSE = mean_squared_error(test['Open'], pred) #4877.095560904092\nRMSE = math.sqrt(MSE) #69.83620522983828\nR2 = r2_score(test['Open'], pred) #-3.0842872575999865\nreal_arima_stock_price_mean = test['Open'].mean()\n\n\nprint(\"Mean Absolute Error:\", MAE)\nprint(\"Mean Squared Error:\", MSE)\nprint(\"Root Mean Squared Error:\", RMSE)\nprint(\"R Squared:\", R2)\nprint(\"Real Stock Price Mean:\", real_arima_stock_price_mean)\nprint(\"RMSE\/REAL_STOCK_PRICE_MEAN:\", RMSE\/real_arima_stock_price_mean) #0.0938522023072058","7e590453":"\"\"\"\nNow that we know that the model is good, we retrain on the entire data and not just training data\n\"\"\"\n\nmodel2 = ARIMA(df['Open'], order=(0,1,0))\nmodel2= model2.fit()\ndf.tail()\n#My data ends on 2016-12-30 ","252ac267":"len(df) , len(df)+19","3be41d5c":"#Last date in total date is 2016-12-30 so we will forecast for the next 20 days to compare results with RNN\npred= model2.predict(start=len(df),end=len(df)+19,typ='levels').rename('ARIMA Predictions')\nprint(pred)\n\n\n#Add the dates as index for the predictions\npred.index=dataset_test[\"Date\"].values\nprint(pred)\n\n","2d585887":"#Model Evaluation\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import r2_score\n\n\nimport math \n\n\n\nMAE_AR = mean_absolute_error(real_stock_price,pred) #RNN:8.477951782226564 AR: 21.384797136038213\n\nMSE_AR = mean_squared_error(real_stock_price,pred) #RNN: 131.51032080751082 AR: 615.985950151971\n\nRMSE_AR = math.sqrt(MSE_AR) #RNN: 11.467794940942692 AR: 24.819064248113204\n\nR2_AR = r2_score(real_stock_price,pred) #RNN: 0.39490875968951855 AR: -1.8342087548918768\n\nreal_stock_price_mean = real_stock_price.mean()  #RNN: 807.5260000000001 AR: 0.030734693679352987\n\n\nprint(\"Mean Absolute Error:\", MAE_AR)\nprint(\"Mean Squared Error:\", MSE_AR)\nprint(\"Root Mean Squared Error:\", RMSE_AR)\nprint(\"R Squared:\", R2_AR)\nprint(\"Real Stock Price Mean:\", real_stock_price_mean)\nprint(\"RMSE_RNN\/REAL_STOCK_PRICE_MEAN:\", RMSE_AR\/real_stock_price_mean) #RNN: 0.014201146391500325 AR: 0.030734693679352987","69fd4ed0":"**Building the RNN**","d1ed4f43":"**ARIMA**","a9b468df":"**Making the predictions and visualizing the results**","6d706dd1":"**RNN performed a lot better than ARIMA with lower errors.**"}}