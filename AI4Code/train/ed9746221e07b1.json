{"cell_type":{"96327652":"code","3b579cf9":"code","fe705c23":"code","53060418":"code","0ac19a9c":"code","40dd216b":"code","a046f631":"code","78acb970":"code","37e49a98":"code","e958c658":"code","5c77562c":"code","7fe2c259":"code","cbdb084e":"code","a05f82b7":"code","811a0a4e":"code","bd556194":"code","93e1333e":"code","d07e21b4":"code","735566ca":"code","c56a4057":"code","48b4a814":"code","4976f4fc":"code","71ab353d":"code","f47d8a48":"code","584399e6":"code","c01542e5":"code","4f6bf1ea":"markdown","9e37242f":"markdown","6700152b":"markdown","ab255837":"markdown","d9d4b2fa":"markdown","8bac1730":"markdown","3024b68d":"markdown","6c7e563d":"markdown","d1f9bf3e":"markdown"},"source":{"96327652":"pip install google.colab","3b579cf9":"import cv2\nimport matplotlib.pyplot as plt\nfrom google.colab.patches import cv2_imshow\nimport numpy as np","fe705c23":"image = cv2.imread('..\/input\/fotoss\/megan.jpg')\ncv2_imshow(image)","53060418":"image.shape","0ac19a9c":"type(image)","40dd216b":"image_blob = cv2.dnn.blobFromImage(image = image, scalefactor = 1.0\/255, \n                                    size = (image.shape[1], image.shape[0]))","a046f631":"image_blob.shape ","78acb970":"type(image_blob)","37e49a98":"network = cv2.dnn.readNetFromCaffe('..\/input\/caffe-deep-learning-freamework\/pose_deploy_linevec_faster_4_stages.prototxt',  \n                                   '..\/input\/caffe-deep-learning-freamework\/pose_iter_160000.caffemodel')","e958c658":"network.getLayerNames() ","5c77562c":"len(network.getLayerNames())","7fe2c259":"network.setInput(image_blob)\noutput = network.forward()","cbdb084e":"output.shape","a05f82b7":"width_position = output.shape[3]\nposition_height = output.shape[2]","811a0a4e":"width_position, position_height","bd556194":"number_points = 15  \npoints = []   \nthreshold = 0.1   \nfor i in range(number_points):\n  trust_map = output[0, i, :, :]\n  _, confidence, _, point = cv2.minMaxLoc(trust_map)\n  x = int((image.shape[1] * point[0]) \/ width_position)\n  y = int((image.shape[0] * point[1]) \/ position_height)\n\n  if confidence > threshold:\n    cv2.circle(image, (x, y), 5, (0,255,0), thickness = -1)\n    cv2.putText(image, '{}'.format(i), (x, y), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0,0,255))\n    points.append((x, y))\n  else:\n    points.append(None)","93e1333e":"points","d07e21b4":"plt.figure(figsize=(14,10))\nplt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB));","735566ca":"dot_connections = [[0,1], [1,2], [2,3], [3,4], [1,5], [5,6], [6,7], [1, 14],  # Fazemos a liga\u00e7\u00f5es entre os pontos;\n                  [14,8], [8,9], [9,10], [14,11], [11,12],[12,13]]","c56a4057":"dot_connections","48b4a814":"for connection in dot_connections:\n\n  partA = connection[0]\n  partB = connection[1]\n  \n  if points[partA] and points[partB]:\n    cv2.line(image, points[partA], points[partB], (255,0,0))","4976f4fc":"plt.figure(figsize=(14,10))\nplt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB));","71ab353d":"image2 = cv2.imread('..\/input\/fotoss\/player.jpg')\ncv2_imshow(image2)","f47d8a48":"image2 = cv2.imread('..\/input\/fotoss\/player.jpg')\nimage_blob2 = cv2.dnn.blobFromImage(image = image2, scalefactor = 1.0 \/ 255, size = (image2.shape[1], image2.shape[0]))\nnetwork.setInput(image_blob2)\noutput2 = network.forward()\nwidth_position = output2.shape[3]\nposition_height = output2.shape[2]\nnumber_points = 15\npoints = []\nthreshold = 0.1\nfor i in range(number_points):\n  trust_map = output2[0, i, :, :]\n  _, confidence, _, point = cv2.minMaxLoc(trust_map) \n  x = int((image2.shape[1] * point[0]) \/ width_position)\n  y = int((image2.shape[0] * point[1]) \/ position_height)\n  \n  if confidence > threshold:\n    cv2.circle(image2, (x, y), 3, (0,255,0), thickness = -1)\n    cv2.putText(image2, \"{}\".format(i), (x, y), cv2.FONT_HERSHEY_SIMPLEX, .3, (0, 0, 255))\n    cv2.putText(image2, '{}-{}'.format(point[0], point[1]), (x, y + 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255,0,255))\n    points.append((x, y))\n  else:\n    points.append(None)\n\nplt.figure(figsize = [14,10])\nplt.imshow(cv2.cvtColor(image2, cv2.COLOR_BGR2RGB));","584399e6":"def check_arms_up(points):\n  head, right_wrist, left_wrist = 0, 0, 0\n  for i, ponto in enumerate(points):\n    \n    if i == 0:\n      head = point[1]\n    elif i == 4:\n      right_wrist = point[1]\n    elif i == 7:\n      left_pulse = point[1]\n\n  if right_wrist < head and left_pulse < head:\n    return True\n  else:\n    return False","c01542e5":"check_arms_up(points)","4f6bf1ea":"## Image loading","9e37242f":"# Motion detection (arms above the head)","6700152b":"## Arms above your head in images","ab255837":"# Detection of Body Points","d9d4b2fa":"## Import from libraries","8bac1730":"* Here are the convolutional networks within the trained file","3024b68d":"## Trained neural network loading\n\n- Caffe Deep Learning framework: https:\/\/caffe.berkeleyvision.org\/","6c7e563d":"## Prediction of body points","d1f9bf3e":"# **If you find this notebook useful, support with an upvote** \ud83d\udc4d"}}