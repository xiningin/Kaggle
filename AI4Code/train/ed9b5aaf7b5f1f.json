{"cell_type":{"ce73aa10":"code","ee0aeb6d":"code","405c0f3c":"code","a69a248f":"code","89112894":"code","c7875a7b":"code","f9d7bd74":"code","85a77ed7":"code","a548c066":"code","94e4572d":"code","f37a837d":"code","ed5cfd7a":"code","d3cbbf70":"code","32e8eaaa":"code","c5453938":"markdown","f960ced2":"markdown","1b580b1c":"markdown","a744513f":"markdown","ec158216":"markdown","675dd3cb":"markdown","3ca1eaea":"markdown","bc1dc71f":"markdown","e0e2fdc2":"markdown","981f3c30":"markdown","c4252ec5":"markdown","9b80efd7":"markdown","2584506c":"markdown","f68cd8c1":"markdown"},"source":{"ce73aa10":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nplt.style.use('ggplot')\npath = '..\/input\/'\nSP_df = pd.read_csv(path+'StudentsPerformance.csv')\nSP_df.head()","ee0aeb6d":"\nin_cols = ['gender', 'race\/ethnicity', 'parental level of education', 'lunch',\n       'test preparation course']\nfig,axarr = plt.subplots(2,3,figsize=(12,6))\naxarr[-1, -1].axis('off')\nfor id,col in enumerate(in_cols):\n    ax = axarr.flat[id]\n    sns.countplot(x=col, data=SP_df,ax=ax)\n    ax.set_title(col,fontsize=14)\n    ax.set_xlabel('')\n    plt.setp(ax.get_xticklabels(), rotation=25,ha='right')\nplt.tight_layout()\nplt.show()","405c0f3c":"Low_df = SP_df[(SP_df['math score']<40) | (SP_df['reading score']<40) | (SP_df['writing score']<40)]\nin_cols = ['gender', 'race\/ethnicity', 'parental level of education', 'lunch',\n       'test preparation course']\nfig,axarr = plt.subplots(2,3,figsize=(12,6))\naxarr[-1, -1].axis('off')\nfor id,col in enumerate(in_cols):\n    ax = axarr.flat[id]\n    sns.countplot(x=col, data=Low_df,ax=ax)\n    ax.set_title(col,fontsize=14)\n    ax.set_xlabel('')\n    plt.setp(ax.get_xticklabels(), rotation=25,ha='right')\nplt.tight_layout()\nplt.show()","a69a248f":"score_cols = ['math score', 'reading score','writing score']\nfrom scipy.stats import norm\n\ndef Plot_Dist(df,col):\n    fig,axarr = plt.subplots(1,2,figsize=(12,4))\n    # plot distribution\n    sns.distplot(df[col], fit=norm, kde=False,ax=axarr[0])\n    #Q-Q plot\n    from statsmodels.graphics.gofplots import qqplot\n    qqplot(SP_df['math score'],line='s',ax=axarr[1])\n    fig.suptitle(col+' distribution',fontsize=14)\n    plt.show()\n\nPlot_Dist(SP_df,col='math score')\nPlot_Dist(SP_df,col='reading score')\nPlot_Dist(SP_df,col='writing score')\n\n\nax1=sns.jointplot(x=\"math score\", y=\"reading score\", data=SP_df)\nplt.show()\n\nax2=sns.jointplot(x=\"math score\", y=\"writing score\", data=SP_df)\nplt.show()","89112894":"def Plot_Set(df,xcol,ycols):\n    df = df.sort_values(by=xcol)\n    fig,axarr = plt.subplots(1,3,figsize=(12,5))\n    for id,ycol in enumerate(ycols):\n        medians = df.groupby([xcol])[ycol].median().values\n        median_labels = [str(np.round(s, 2)) for s in medians]\n        pos = range(len(medians))\n        sns.boxplot(x=xcol, y=ycol, data=df,width=0.5,palette='Set3',ax=axarr[id],linewidth=0.5)\n        for tick,label in zip(pos,axarr[id].get_xticklabels()):\n            axarr[id].text(pos[tick], medians[tick] + 0.5, median_labels[tick], horizontalalignment='center', size='medium', color='k', weight='semibold')\n        axarr[id].set_ylim([0,105])\n        plt.setp(axarr[id].get_xticklabels(), rotation=25,ha='right')\n    #fig.suptitle('Variation of Scores with '+xcol,fontsize=16,y=1.05)\n    plt.tight_layout()\n    plt.show()","c7875a7b":"Plot_Set(SP_df,xcol='gender',ycols=['math score','reading score','writing score'])","f9d7bd74":"Plot_Set(SP_df,xcol='race\/ethnicity',ycols=['math score','reading score','writing score'])","85a77ed7":"Plot_Set(SP_df,xcol='parental level of education',ycols=['math score','reading score','writing score'])","a548c066":"Plot_Set(SP_df,xcol='lunch',ycols=['math score','reading score','writing score'])","94e4572d":"Plot_Set(SP_df,xcol='test preparation course',ycols=['math score','reading score','writing score'])","f37a837d":"xcol = 'lunch'\nycols = ['math score','reading score','writing score']\n\nSP_df = SP_df.sort_values(by=xcol)\nfig,axarr = plt.subplots(1,3,figsize=(12,5))\nfor id,ycol in enumerate(ycols):\n    sns.boxplot(x=xcol, y=ycol, hue='test preparation course', data=SP_df,width=0.5,palette='Set3',ax=axarr[id],linewidth=0.5)\n    axarr[id].set_ylim([0,105])\n    plt.setp(axarr[id].get_xticklabels(), rotation=25,ha='right')\n#fig.suptitle('Variation of Scores with '+xcol,fontsize=16,y=1.05)\nplt.tight_layout()\nplt.show()\n\n# Hypothesis Testing\ndf_LunchFree_PrepNo  = SP_df[(SP_df['lunch'].str.contains('free')) & (SP_df['test preparation course'].str.contains('none'))]\ndf_LunchFree_PrepYes = SP_df[(SP_df['lunch'].str.contains('free')) & (SP_df['test preparation course'].str.contains('complete'))]\n\ndf_LunchStd_PrepNo  = SP_df[(SP_df['lunch'].str.contains('stan')) & (SP_df['test preparation course'].str.contains('none'))]\ndf_LunchStd_PrepYes = SP_df[(SP_df['lunch'].str.contains('stan')) & (SP_df['test preparation course'].str.contains('complete'))]\n\ndef CompMeans(df1,df2,ct,yparam,ax):\n    import statsmodels.stats.api as sms\n    yval = 0.25\n    cols = ['math score','reading score','writing score']\n    meanlist = []\n    CFlist   = []\n    for col in cols:\n        X1 = df1[col]\n        X2 = df2[col]\n        cm = sms.CompareMeans(sms.DescrStatsW(X1), sms.DescrStatsW(X2))\n        means_diff = X1.mean() - X2.mean()\n        CF = cm.tconfint_diff(usevar='unequal')\n        print('impact of test prep on '+col+' for lunch type: '+yparam)\n        print(cm.summary())\n        x1 = CF[0]\n        x2 = CF[1]\n        ax.plot( [x1,x2],[yval,yval], marker='|',markersize=12, color=ct[0], linewidth=12)\n        ax.plot(means_diff,yval,marker='d',color=ct[1],markersize=12)\n        ax.annotate(col,xy=(x1-1.5, yval), xycoords='data')\n        yval +=0.25\n        ax.set_ylabel(yparam,fontsize=14)\n        ax.set_ylim([0,1])\n        ax.yaxis.set_ticklabels([])\n        \nfig,axarr = plt.subplots(2,1,figsize=(8,4),sharex = True)\n\nct = ['pink','red']\nyparam = 'free\/reduced'\nCompMeans(df_LunchFree_PrepYes,df_LunchFree_PrepNo,ct,yparam,axarr[0])\nct = ['skyblue','blue']\nyparam = 'standard'\nCompMeans(df_LunchStd_PrepYes,df_LunchStd_PrepNo,ct,yparam,axarr[1])\nplt.xlabel('Mean Difference +\/- Confidence Interval',fontsize=14)\nplt.xlim([0,15])\nfig.suptitle('Impact of Test preparation on Scores (Difference in Means)',fontsize=16)\nplt.show()","ed5cfd7a":"from sklearn.model_selection import train_test_split\nSP_df['Total Score'] = SP_df['math score'] + SP_df['reading score'] + SP_df['writing score']\ninput_cols = ['gender', 'race\/ethnicity', 'parental level of education', 'lunch', 'test preparation course']\n\n# Convert Total Scores to Grades\nFScore = 40*3 #Total Score Cutoff for Passing Grade\nEScore = 50*3 #Total Score Cutoff for E Grade\nDScore = 60*3 #Total Score Cutoff for D Grade\nCScore = 70*3 #Total Score Cutoff for C Grade\nBScore = 80*3 #Total Score Cutoff for B Grade\n\n\nSP_df.loc[SP_df['Total Score']<FScore,'Grade']  = 'F'\nSP_df.loc[(SP_df['Total Score']>=FScore) & (SP_df['Total Score']<EScore),'Grade'] = 'E'\nSP_df.loc[(SP_df['Total Score']>=EScore) & (SP_df['Total Score']<DScore),'Grade'] = 'D'\nSP_df.loc[(SP_df['Total Score']>=DScore) & (SP_df['Total Score']<CScore),'Grade'] = 'C'\nSP_df.loc[(SP_df['Total Score']>=CScore) & (SP_df['Total Score']<BScore),'Grade'] = 'B'\nSP_df.loc[(SP_df['Total Score']>=BScore),'Grade'] = 'A'\n\ntarget_col = ['Grade']\n\nX = pd.get_dummies(SP_df[input_cols])\ny = np.array(SP_df[target_col]).ravel()\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n\nfrom sklearn import preprocessing\nlb = preprocessing.LabelBinarizer()\nlb.fit(y)\n\ny_trainLB = lb.transform(y_train)\ny_testLB = lb.transform(y_test)","d3cbbf70":"from sklearn.linear_model import LogisticRegression\n\nclf = LogisticRegression(random_state=0, solver='lbfgs',fit_intercept=False, multi_class='multinomial')\nclf.fit(X_train, y_train)\n\n# Make predictions using the testing set\ny_prob = clf.predict_proba(X_test)\ny_pred = [x[1] for x in y_prob]\n\n## Plot ROC Curve for all classes\nimport scikitplot as skplt\nimport matplotlib.pyplot as plt\n\ny_true = y_test# ground truth labels\ny_probas = y_prob# predicted probabilities generated by sklearn classifier\nskplt.metrics.plot_roc(y_true, y_probas,figsize=(8,8))\nplt.show()\n\n\nFimp = pd.DataFrame({'Features':X.columns.tolist(),'Coefficient':clf.coef_[0]})\nFimp['odds'] = np.exp(Fimp['Coefficient'])\nFimp = Fimp.sort_values(by=['odds'],ascending=False)\nFimp","32e8eaaa":"from skopt.space import Real, Integer\nfrom skopt.utils import use_named_args\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import cross_val_score\n\nclf = RandomForestClassifier(n_estimators=10, max_depth=5,random_state=0)\n\nn_features = X_test.shape[1]\n# The list of hyper-parameters we want to optimize. For each one we define the bounds,\n# the corresponding scikit-learn parameter name, as well as how to sample values\n# from that dimension (`'log-uniform'` for the learning rate)\nspace  = [Integer(50, 500, name='n_estimators'),\n          Integer(1, n_features, name='max_features'),\n          Integer(1, 50, name='max_depth')]\n\n# this decorator allows your objective function to receive a the parameters as\n# keyword arguments. This is particularly convenient when you want to set scikit-learn\n# estimator parameters\n@use_named_args(space)\ndef objective(**params):\n    clf.set_params(**params)\n    return -np.mean(cross_val_score(clf, X_train, y_trainLB, cv=5, n_jobs=-1,scoring=\"neg_mean_absolute_error\"))\n\nfrom skopt import gp_minimize\nres_gp = gp_minimize(objective, space, n_calls=20, random_state=0)\n\n\"Best score=%.4f\" % res_gp.fun\nprint(\"\"\"Best parameters:\n- n_estimators=%d\n- max_features=%d\n- max_depth=%d\"\"\" % (res_gp.x[0], res_gp.x[1], res_gp.x[2]))\n\nfrom skopt.plots import plot_convergence\n%matplotlib inline\nplot_convergence(res_gp);\n\nfrom sklearn.metrics import mean_squared_error, r2_score\nbest_params = {'n_estimators':res_gp.x[0],'max_features':res_gp.x[1],'max_depth':res_gp.x[2]}\n\nclf = RandomForestClassifier(**best_params)\nclf.fit(X_train,y_train)\n\n# Make predictions using the testing set\ny_prob = clf.predict_proba(X_test)\ny_pred = [x[1] for x in y_prob]\n\n## Plot ROC Curve for all classes\nimport scikitplot as skplt\nimport matplotlib.pyplot as plt\n\ny_true = y_test# ground truth labels\ny_probas = y_prob# predicted probabilities generated by sklearn classifier\nskplt.metrics.plot_roc(y_true, y_probas,figsize=(8,8))\nplt.show()\n\nFimp = pd.DataFrame({'Features':X.columns.tolist(),'Importance':clf.feature_importances_})\nFimp = Fimp.sort_values(by=['Importance'],ascending=False)\nFimp","c5453938":"#### Variation of Scores with race\/ethnicity\nRace has a significant influence on test scores. For all subjects, students in group E perform better than students from other ethnicity.  ","f960ced2":"### Predictive Modeling\n+ To build a predictive model, we group the scores into 6 grade classes (A - F). This is used as the target variable.\n+ The independant variables are one-hot encoded for use in the scikit-learn model building process.\n","1b580b1c":"## Dataset\nThe dataset consists of math, reading and writing scores of students along with some influencing factors such as gender, race, parents education, lunch type, and if the student took test preparation courses. \n\n## Goals\n#### Inference\n* Identify interesting correlations within the dataset. \n* The lunch column is an interesting factor I didnt expect in such a dataset. I want to explore its impact on the scores? \n\n### Prediction\n* Try different models in predicting scores based on the input factors.\n\n## Key Takeaways\n\n* Whether the student gets the standard lunch or free\/reduced lunch has an impact on the scores. It is evident students from lower income families on an average have 5-10 points lower scores than those who can afford standard lunches. \n* Test preparation provides a stastically signficant improvent in test scores. However, the difference in means are consistently higher for students with 'free\/reduced' lunch type. This indicates test preparation has an outsized impact on scores for students from low income families (6 points increase for math and more than 11 points increase for writing).\n* Predictive models suggest the most important variables driving higher scores are \n    * lunch = 'standard'\n    * test preparation course = 'completed'\n    * parental education level = 'bachelors\/masters degree'\n    * race\/ethnic_group = 'E'","a744513f":"### How does test preparation affect scores for students from different income levels (based on lunch type) ?\n* I use student t-tests to measure the impact of test preparation on student scores with free\/reduced vs standard lunch \ntypes. \n* Results below suggest (based on very low p-values) irrespective of the lunch type, test preparation provides a stastically signficant improvent in test scores.\n* However, the difference in means are consistently higher for students with 'free\/reduced' lunch type. This indicates test preparation has an outsized impact on scores for students from low income families (6 points increase for math and more than 11 points increase for writing).","ec158216":"### Random Forest Classifier\n* A tree based ensemble method is tried to compare against logistic regression model.\n* We use skopt to conduct hyper parameter optimization and find the most optimal set.\n* The model performs worse having AUC = 0.63 (compared to logistic regression), with Grade = 'C' target class having the highest classification error (with lowest AUC).\n* The model coefficients provide an idea of important factors. The most important unique variables driving higher scores are \n    * lunch = 'standard'\n    * test preparation course = 'none'\n    * race\/ethnicity group  = 'E'","675dd3cb":"#### Variation of Scores with Gender of the student\nFemale students beat male counter parts in reading and writing. In math, boys on an average do better than girls.","3ca1eaea":"## Thanks for going through the kernel. Hope it is useful!","bc1dc71f":"#### Variation of Scores with test preparation course\nStudents who completed a preparation course prior to the test consistently outperform students who didnt take such a course","e0e2fdc2":"#### Variation of Scores with parental level of education\nEducation level of parents has a direct impact on the test scores. Higher the education level of the parent, higher the student scores.","981f3c30":"#### Logistic Regression\n* Note that the intercept was set to False to remove issues with collinearity with one-hot encoding.\n* The model has an average AUC = 0.67, with Grade = 'C' having the highest classification error (with lowest AUC).\n* The model coefficients provide an idea of important factors. The most important variables driving higher scores are \n    * lunch = 'standard'\n    * test preparation course = 'completed'\n    * parental education level = 'bachelor degree'","c4252ec5":"### Categorical count of independent variables","9b80efd7":"#### Variation of Scores with lunch type\n* Many schools in the US offer free lunch for students coming from poor families (Ref: https:\/\/www.fns.usda.gov\/nslp\/national-school-lunch-program-nslp)\n* Whether the student gets the standard lunch or free\/reduced lunch has an impact on the scores. It is evident students from lower income families on an average have 5-10 points lower scores than those who can afford standard lunches. ","2584506c":"### What factors correlate with low scores?\n* Here we look at count plots for students with scores below 40 in atleast one subject\n+ `Lunch = Free\/reduced Lunch`, `test preparation = 'none'` and `parental level of education != 'masters degree'` has high correlation with low scores.","f68cd8c1":"### Distribution of student scores\n* All the scores are approximately normally distributed. Q-Q plots show skewness in both directions, indicating deviation from normal distribution in those regions.\n* Joint distributions show strong correlation between test scores among different subjects which is not surprising."}}