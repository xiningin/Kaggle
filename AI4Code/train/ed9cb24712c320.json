{"cell_type":{"ed5aceb0":"code","a58dff3e":"code","26039548":"code","ab85911d":"code","d916abdd":"code","84cc7a81":"code","2e77ff81":"code","c06ef8ca":"code","a0e0c216":"code","af428b9c":"code","99d2e64b":"code","9ec6c995":"code","575dcbbb":"code","16d41a57":"code","73b289d9":"markdown","ba60ca54":"markdown","5e63a9e4":"markdown","82ff53e7":"markdown","e747297b":"markdown","b0fdf649":"markdown","cf4c6a7d":"markdown","c4256a8e":"markdown","1ca77b4e":"markdown","7cab3bc2":"markdown","baeea1bd":"markdown","c30ab438":"markdown"},"source":{"ed5aceb0":"#Libraries\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport requests\nfrom urllib.parse import urlencode\nfrom matplotlib.patches import Rectangle\nfrom matplotlib import gridspec\nfrom matplotlib import rcParams","a58dff3e":"#functions\ndef file_load(link,sep): #load data from yandex disk\n    base_url = 'https:\/\/cloud-api.yandex.net\/v1\/disk\/public\/resources\/download?'\n    public_key = link \n    final_url = base_url + urlencode(dict(public_key=public_key))\n    response = requests.get(final_url)\n    download_url = response.json()['href']\n    df = pd.read_csv(download_url, sep=sep)\n    return df\n        \ndef columns_group(df,list_dict):\n    for i in list_dict:\n        list_col=list(i.keys())\n        for j, val in enumerate(list_col[:-1]):\n            df[i[val]]=df[list_col[j:]].sum(axis=1)\n        \ndef column_group_total(df,list_col):\n    for i in list_col:\n        df[i+'_Total'] = df[df.columns[df.columns.str.contains(i)]].sum(axis=1)\n\ndef heatmap_q (df, ax, order_q, order_q_short, title):\n    sns.heatmap(df.loc[order_q], center=0, cmap=\"vlag\", ax=ax, annot=True, cbar=False, yticklabels=order_q_short)\n    ax.add_patch(Rectangle((0,df.loc[order_q].reset_index()['EmancipVal'].idxmax()),1,1, fill=False, edgecolor='white', lw=3))\n    ax.set(title=title)  \n\ndef scatter_country(df,list_y, list_ylabel, list_title, list_annot, figsize1):\n    fig, ax=plt.subplots(1,len(list_y),figsize=figsize1)\n    for i, val in enumerate(list_y):\n        sns.scatterplot(data=df, x='EmancipVal',y=val, hue = 'Grp', palette=palette_country_grp, legend=False, ax=ax[i])\n        #ax[i].axhline(df[val].mean(), color='gray', linewidth=1)\n        ax[i].set(title=list_title[i],ylabel=list_ylabel[i])\n        ax[i].spines['right'].set_color('none')\n        ax[i].spines['top'].set_color('none')\n        for val2 in list_annot[i]:\n            ind=df[df.Country==val2].index[0]\n            ax[i].annotate(df2['Country'].loc[ind], (df2['EmancipVal'].loc[ind], df2[val].loc[ind]+0.008)) \n    fig.tight_layout()\n\ndef bars_by_grp (df1,df2,col, order_x, title1,title2):    \n    d=pd.DataFrame(df1.groupby(['Grp',col])[col].count().div(df1.groupby(['Grp'])[col].count())).rename(columns={col: 'Distribution'}).reset_index()\n    g = sns.FacetGrid(data=d, col=\"Grp\", hue=\"Grp\", gridspec_kws={\"wspace\":0}, palette=palette_country_grp, col_order=['LOW','MIDDLE','HIGH'])\n    g.map(sns.barplot, col, \"Distribution\", order = order_x)\n    g.set_xticklabels(rotation=90)\n    ttl=g.fig.suptitle(title1)\n    ttl.set_position([.5, 1.05])\n    #g.set_frame_on(False)\n    d=pd.DataFrame((df1.groupby(['Grp',col])[col].count().div(df1.groupby(['Grp'])[col].count())-\\\n     df2.groupby([col])[col].count().div(df2[col].count()))).rename(columns={col: 'Distribution'}).reset_index()\n    g = sns.FacetGrid(data=d, col=\"Grp\", hue=\"Grp\", gridspec_kws={\"wspace\":0}, palette=palette_country_grp, col_order=['LOW','MIDDLE','HIGH'])\n    g.map(sns.barplot, col, \"Distribution\", order = order_x)\n    g.set_xticklabels(rotation=90)\n    ttl=g.fig.suptitle(title2)\n    ttl.set_position([.5, 1.05])","26039548":"#dictionaries and variabels\npalette_country_grp=['#f28830','#c6c6c6','#008294']\n\nrcParams['axes.titlesize']=15\nrcParams['axes.labelsize']=12\nrcParams['axes.titlepad']=10\nrcParams['axes.spines.right'] = False\nrcParams['axes.spines.top'] = False\n\ndict_country={'Russian':'Russia','Taiwan ROC':'Taiwan','Hong Kong SAR':'Hong Kong','United Kingdom of Great Britain and Northern Ireland':'United Kingdom', 'United States of America':'United States', \\\n    'Hong Kong (S.A.R.)':'Hong Kong', 'Iran, Islamic Republic of...':'Iran', 'Viet Nam':'Vietnam',\n    'Czech Rep.':'Czech Republic','Hong Kong, China':'Hong Kong','Macao, China':'Macao','Korea, Rep.':'South Korea','Dominican Rep.':'Dominican Republic'\n             , 'Czech Republic (Czechia)':'Czech Republic'\n             ,'Czechia':'Czech Republic','Hong Kong China (SAR)':'Hong Kong',' Hong Kong, China (SAR)':'Hong Kong','Iran (Islamic Republic of)':'Iran','Korea (Republic of)':'South Korea','Russian Federation':'Russia','Viet Nam':'Vietnam'}\n\ndict_q1={'Q1_25-29':'Q1_25+','Q1_30-34':'Q1_30+', 'Q1_35-39':'Q1_35+','Q1_40-44':'Q1_40+','Q1_45-49':'Q1_45+',\n       'Q1_50-54':'Q1_50+','Q1_55-59':'Q1_55+','Q1_60-69':'Q1_60+','Q1_70+':'Q1_40+'}\ndict_q6={'Q6_1-3 years':'Q6_1+ years','Q6_3-5 years':'Q6_3+ years','Q6_5-10 years':'Q6_5+ years',\n       'Q6_10-20 years':'Q6_10+ years','Q6_20+ years':'Q6_20+ years'}\ndict_q13={'Q13_2-5 times':'Q13_2+ times','Q13_6-25 times':'Q13_6+ times', 'Q13_More than 25 times':'Q13_25+ times'}\ndict_q15={'Q15_1-2 years':'Q15_1+ years','Q15_2-3 years':'Q15_2+ years','Q15_3-4 years':'Q15_3+ years',\n       'Q15_4-5 years':'Q15_4+ years','Q15_5-10 years':'Q15_5+ years','Q15_10-20 years':'Q15_10+ years','Q15_20 or more years':'Q15_20+ years'}\ndict_q21={'Q21_250-999 employees':'Q21_250+ employees','Q21_1000-9,999 employees':'Q21_1000+ employees',\n       'Q21_10,000 or more employees':'Q21_10000+ employees'}\ndict_q22={'Q22_3-4':'Q22_3+','Q22_5-9':'Q22_5+','Q22_10-14':'Q22_10+','Q22_15-19':'Q22_15+', 'Q22_20+':'Q22_20+'}\ndict_q25={'Q25_2,000-2,999':'Q25_2,000$+', 'Q25_3,000-3,999':'Q25_3,000$+', 'Q25_4,000-4,999':'Q25_4,000$+', 'Q25_5,000-7,499':'Q25_5,000$+', \n       'Q25_7,500-9,999':'Q25_7,000$+',\n       'Q25_10,000-14,999':'Q25_10,000$+','Q25_15,000-19,999':'Q25_15,000$+', 'Q25_20,000-24,999':'Q25_20,000$+', 'Q25_25,000-29,999':'Q25_25,000$+',\n       'Q25_30,000-39,999':'Q25_30,000$+', 'Q25_40,000-49,999':'Q25_40,000$+', 'Q25_50,000-59,999':'Q25_50,000$+', 'Q25_60,000-69,999':'Q25_60,000$+', \n       'Q25_70,000-79,999':'Q25_70,000$+', 'Q25_80,000-89,999':'Q25_80,000$+', 'Q25_90,000-99,999':'Q25_90,000$+','Q25_100,000-124,999':'Q25_100,000$+', \n       'Q25_125,000-149,999':'Q25_125,000$+',  'Q25_150,000-199,999':'Q25_150,000$+', 'Q25_200,000-249,999':'Q25_200,000$+',  'Q25_250,000-299,999':'Q25_250,000$+', \n       'Q25_300,000-499,999':'Q25_300,000$+', 'Q25_$500,000-999,999':'Q25_500,000$+','Q25_>$1,000,000':'Q25_$1,000,000$+'}\ndict_q26={'Q26_$100-$999':'Q26_$100+', 'Q26_$1000-$9,999':'Q26_$1000+', 'Q26_$10,000-$99,999':'Q26_$10,000+', 'Q26_$100,000 or more ($USD)':'Q26_$100,000+'}\n\norder_q1=['Q1_18-21','Q1_22-24','Q1_25-29','Q1_30-34','Q1_35-39','Q1_40-44','Q1_45-49','Q1_50-54',\n    'Q1_55-59','Q1_60-69','Q1_25+','Q1_30+','Q1_35+','Q1_40+','Q1_45+','Q1_50+','Q1_55+','Q1_60+','Q1_70+']\norder_q1_short=['18-21','22-24','25-29','30-34','35-39','40-44','45-49','50-54',\n    '55-59','60-69','25+','30+','35+','40+','45+','50+','55+','60+','70+']\n\norder_q6=['Q6_I have never written code','Q6_< 1 years','Q6_1-3 years', 'Q6_3-5 years','Q6_5-10 years','Q6_10-20 years',\n 'Q6_1+ years','Q6_3+ years','Q6_5+ years','Q6_10+ years','Q6_20+ years']\norder_q6_short=['I have never written code','< 1 years','1-3 years', '3-5 years','5-10 years','10-20 years',\n '1+ years','3+ years','5+ years','10+ years','20+ years']\n\norder_q15=[ 'Q15_I do not use machine learning methods','Q15_Under 1 year','Q15_1-2 years','Q15_2-3 years',\n 'Q15_3-4 years','Q15_4-5 years','Q15_5-10 years','Q15_10-20 years','Q15_1+ years','Q15_2+ years','Q15_3+ years','Q15_4+ years','Q15_5+ years',\n 'Q15_10+ years', 'Q15_20 or more years']\norder_q15_short=[ 'I do not use ML methods','Under 1 year','1-2 years','2-3 years',\n '3-4 years','4-5 years','5-10 years','10-20 years','1+ years','2+ years','3+ years','4+ years','5+ years',\n '10+ years', '20 or more years']\n\norder_q22=['Q22_0', 'Q22_1-2','Q22_3-4', 'Q22_5-9', 'Q22_10-14', 'Q22_15-19', 'Q22_3+', 'Q22_5+', 'Q22_10+', 'Q22_15+', 'Q22_20+']\norder_q22_short=['0', '1-2','3-4', '5-9', '10-14', '15-19', '3+', '5+', '10+', '15+', '20+']\n\norder_q23=['Q23_I do not know', 'Q23_No (we do not use ML methods)',\n        'Q23_We use ML methods for generating insights (but do not put working models into production)',\n       'Q23_We are exploring ML methods (and may one day put a model into production)',\n       'Q23_We recently started using ML methods (i.e., models in production for less than 2 years)',\n       'Q23_We have well established ML methods (i.e., models in production for more than 2 years)'\n       ]\norder_q24=['Q24_Part_1_Analyze and understand data to influence product or business decisions',\n       'Q24_Part_2_Build and\/or run the data infrastructure that my business uses for storing, analyzing, and operationalizing data',\n       'Q24_Part_3_Build prototypes to explore applying machine learning to new areas',\n       'Q24_Part_4_Build and\/or run a machine learning service that operationally improves my product or workflows',\n       'Q24_Part_5_Experimentation and iteration to improve existing ML models',\n       'Q24_Part_6_Do research that advances the state of the art of machine learning',\n       'Q24_Part_7_None of these activities are an important part of my role at work',\n       'Q24_OTHER_Other', 'Q24__Total']\n\norder_q23_short=['I do not know', 'Do not use',\n        'For generating insights',\n       'We are exploring ML methods',\n       'Models in production for less than 2 years',\n       'Models in production for more than 2 years'\n       ]\norder_q24_short=['Analyze and understand data',\n       'Build and\/or run the data infrastructure',\n       'Build prototypes to explore applying machine learning to new areas',\n       'Build and\/or run a machine learning service',\n       'Experimentation to improve existing ML models',\n       'Do research that advances the state of the art of machine learning',\n       'None of these activities',\n       'Other', 'Total']\n\norder_q25 = ['Q25_$0-999',  'Q25_1,000-1,999', 'Q25_2,000-2,999', 'Q25_3,000-3,999', 'Q25_4,000-4,999','Q25_5,000-7,499','Q25_7,500-9,999',\n       'Q25_10,000-14,999','Q25_15,000-19,999','Q25_20,000-24,999', 'Q25_25,000-29,999','Q25_30,000-39,999', 'Q25_40,000-49,999', 'Q25_50,000-59,999', 'Q25_60,000-69,999',\n        'Q25_70,000-79,999', 'Q25_80,000-89,999','Q25_90,000-99,999','Q25_100,000-124,999', 'Q25_125,000-149,999',\n        'Q25_150,000-199,999','Q25_200,000-249,999', 'Q25_250,000-299,999', 'Q25_300,000-499,999', 'Q25_$500,000-999,999', \n        'Q25_2,000$+', 'Q25_3,000$+','Q25_4,000$+', 'Q25_5,000$+', 'Q25_7,000$+', 'Q25_10,000$+','Q25_15,000$+', 'Q25_20,000$+', 'Q25_25,000$+', 'Q25_30,000$+',\n       'Q25_40,000$+', 'Q25_50,000$+', 'Q25_60,000$+', 'Q25_70,000$+','Q25_80,000$+', 'Q25_90,000$+', 'Q25_100,000$+', 'Q25_125,000$+',\n       'Q25_150,000$+', 'Q25_200,000$+', 'Q25_250,000$+', 'Q25_300,000$+','Q25_500,000$+', 'Q25_>$1,000,000']\norder_q25_1 = ['Q25_$0-999',  'Q25_1,000-1,999', 'Q25_2,000-2,999', 'Q25_3,000-3,999', 'Q25_4,000-4,999','Q25_5,000-7,499','Q25_7,500-9,999',\n       'Q25_10,000-14,999','Q25_15,000-19,999','Q25_20,000-24,999', 'Q25_25,000-29,999','Q25_30,000-39,999', 'Q25_40,000-49,999', 'Q25_50,000-59,999', 'Q25_60,000-69,999',\n        'Q25_70,000-79,999', 'Q25_80,000-89,999','Q25_90,000-99,999','Q25_100,000-124,999', 'Q25_125,000-149,999',\n        'Q25_150,000-199,999','Q25_200,000-249,999', 'Q25_250,000-299,999', 'Q25_300,000-499,999', 'Q25_$500,000-999,999']\norder_q25_2 = ['Q25_2,000$+', 'Q25_3,000$+','Q25_4,000$+', 'Q25_5,000$+', 'Q25_7,000$+', 'Q25_10,000$+','Q25_15,000$+', 'Q25_20,000$+', 'Q25_25,000$+', 'Q25_30,000$+',\n       'Q25_40,000$+', 'Q25_50,000$+', 'Q25_60,000$+', 'Q25_70,000$+','Q25_80,000$+', 'Q25_90,000$+', 'Q25_100,000$+', 'Q25_125,000$+',\n       'Q25_150,000$+', 'Q25_200,000$+', 'Q25_250,000$+', 'Q25_300,000$+','Q25_500,000$+', 'Q25_>$1,000,000']\norder_q26 =['Q26_$0 ($USD)', 'Q26_$1-$99', 'Q26_$100-$999', 'Q26_$1000-$9,999', 'Q26_$10,000-$99,999',     \n       'Q26_$100+', 'Q26_$1000+', 'Q26_$10,000+','Q26_$100,000 or more ($USD)']\norder_q26_short =['$0 ($USD)', '$1-$99', '$100-$999', '$1000-$9,999', '$10,000-$99,999',     \n       '$100+', '$1000+', '$10,000+','$100,000 or more ($USD)']\n\norder_q25_short = ['$0-999',  '1,000-1,999', '2,000-2,999', '3,000-3,999', '4,000-4,999','5,000-7,499','7,500-9,999',\n       '10,000-14,999','15,000-19,999','20,000-24,999', '25,000-29,999','30,000-39,999', '40,000-49,999', '50,000-59,999', '60,000-69,999',\n        '70,000-79,999', '80,000-89,999','90,000-99,999','100,000-124,999', '125,000-149,999',\n        '150,000-199,999','200,000-249,999', '250,000-299,999', '300,000-499,999', '$500,000-999,999', \n        '2,000$+', '3,000$+','4,000$+', '5,000$+', '7,000$+', '10,000$+','15,000$+', '20,000$+', '25,000$+', '30,000$+',\n       '40,000$+', '50,000$+', '60,000$+', '70,000$+','80,000$+', '90,000$+', '100,000$+', '125,000$+',\n       '150,000$+', '200,000$+', '250,000$+', '300,000$+','500,000$+', '>$1,000,000']\norder_q25_1_short = ['$0-999',  '1,000-1,999', '2,000-2,999', '3,000-3,999', '4,000-4,999','5,000-7,499','7,500-9,999',\n       '10,000-14,999','15,000-19,999','20,000-24,999', '25,000-29,999','30,000-39,999', '40,000-49,999', '50,000-59,999', '60,000-69,999',\n        '70,000-79,999', '80,000-89,999','90,000-99,999','100,000-124,999', '125,000-149,999',\n        '150,000-199,999','200,000-249,999', '250,000-299,999', '300,000-499,999', '$500,000-999,999']\norder_q25_2_short = ['2,000$+', '3,000$+','4,000$+', '5,000$+', '7,000$+', '10,000$+','15,000$+', '20,000$+', '25,000$+', '30,000$+',\n       '40,000$+', '50,000$+', '60,000$+', '70,000$+','80,000$+', '90,000$+', '100,000$+', '125,000$+',\n       '150,000$+', '200,000$+', '250,000$+', '300,000$+','500,000$+', '>$1,000,000']\norder_q26_short =['$0 ($USD)', '$1-$99', '$100-$999', '$1000-$9,999', '$10,000-$99,999',     \n       '$100+', '$1000+', '$10,000+','$100,000 or more ($USD)']\n","ab85911d":"#data load and transform\ndf_responses = pd.read_csv('\/kaggle\/input\/kaggle-survey-2021\/kaggle_survey_2021_responses.csv') #kaggle survey \ndf_questions=df_responses[0:1].T\ndf_responses=df_responses[1:]\ndf_responses['Q3'].replace(dict_country, inplace=True)\n\n#WVS data World cultural map values from https:\/\/www.worldvaluessurvey.org\/WVSNewsShow.jsp?ID=428 (CulturalMapFinalEVSWVS_1981-2021_v20201102.xls)\ndf_world_values = file_load('https:\/\/disk.yandex.ru\/d\/Uk3vy-7swzeB4w' ,',') \ndf_world_values['SurvSelfVal'] = df_world_values.SurvSAgg.replace({',': '.'}, regex=True).astype(float)\ndf_world_values['TradSecVal'] = df_world_values.TradAgg.replace({',': '.'}, regex=True).astype(float)\ndf_world_values['Country'].replace(dict_country, inplace = True)\ndf_world_values = pd.merge(df_world_values,df_world_values.groupby(['Country'])['year'].max('year'),how = 'left', right_index=True, left_on = 'Country')\ndf_world_values = df_world_values[(df_world_values['year_x']==df_world_values['year_y'])&(df_world_values['year_x']>2000)][['Country','SurvSelfVal','TradSecVal']]\n\n#WVS data Welzel values from https:\/\/www.worldvaluessurvey.org\/WVSNewsShow.jsp?ID=428 (CulturalMapFinalEVSWVS_1981-2021_v20201102.xls)\ndf_welzel = file_load('https:\/\/disk.yandex.ru\/d\/EXooh-fIJMAnDA' ,',') \ndf_welzel['SecularVal'] = df_welzel['SACSECVAL'].astype(float)\ndf_welzel['EmancipVal'] = df_welzel['RESEMAVAL'].astype(float)\ndf_welzel['Country'].replace(dict_country, inplace = True)\ndf_welzel = pd.merge(df_welzel,df_welzel.groupby(['Country'])['year'].max('year'),how = 'left', on = 'Country')\ndf_welzel = df_welzel[(df_welzel['year_x']==df_welzel['year_y'])&(df_welzel['year_x']>2000)][['Country','EmancipVal','SecularVal']]\n\n#Indexes by counties\ndf_quality_of_life = pd.read_csv('\/kaggle\/input\/countries-dataset-2020\/Quality of life index by countries 2020.csv')\ndf_quality_of_life=df_quality_of_life[['Country','Quality of Life Index','Purchasing Power Index','Cost of Living Index']]\n\n#Share of people above 65 by countries\ndf_age_structure = pd.read_csv('\/kaggle\/input\/countries-dataset-2020\/Coutries age structure.csv')\ndf_age_structure['Above65']=(100- df_age_structure['Age 0 to 14 Years'].replace('%','', regex=True).astype(float)\\\n                              -df_age_structure['Age 15 to 64 Years'].replace('%','', regex=True).astype(float))\/100\ndf_age_structure=df_age_structure[['Country','Above65']]\n\n#Median age by countries\ndf_median_age = pd.read_csv('\/kaggle\/input\/average-age-of-countries\/MedianAge.csv')\ndf_median_age.replace(dict_country, inplace = True)\ndf_median_age=df_median_age[['Country','2020']]\n\n#Population and median age by countries\ndf_population = pd.read_csv('\/kaggle\/input\/population-by-country-2020\/population_by_country_2020.csv')\ndf_population.rename(columns={'Country (or dependency)': 'Country','Population (2020)':'Population'}, inplace = True)\ndf_population.replace(dict_country, inplace = True)\ndf_population = pd.merge(df_population,df_median_age, how='left', on='Country')\ndf_population['MedAge']=df_population['2020'].fillna(df_population['Med. Age'])\ndf_population=df_population[['Country','MedAge','Population']]","d916abdd":"#Preparing data sets\ndf2=pd.get_dummies(df_responses,columns=df_responses.columns.drop(['Q3','Time from Start to Finish (seconds)'])).groupby('Q3').sum()\ndf2['RespCnt']=df2['Q2_Man']+df2['Q2_Nonbinary']+df2['Q2_Prefer not to say']+df2['Q2_Prefer to self-describe']+df2['Q2_Woman']\n\ndf2=df2[df2.columns[0:-1]].div(df2.RespCnt, axis=0)\ncolumns_group(df2,[dict_q1,dict_q6,dict_q13,dict_q15,dict_q21,dict_q22,dict_q25,dict_q26])\nlist1=['Q7_','Q9_','Q10_','Q12_','Q14_','Q16_','Q17_','Q18_','Q19_','Q24_','Q39_','Q40_','Q42_','Q27_A','Q29_A','Q30_A','Q31_A','Q32_A','Q34_A','Q36_A','Q37_A','Q38_A','Q27_B','Q29_B','Q30_B','Q31_B','Q32_B','Q34_B','Q36_B','Q37_B','Q38_B',]\ncolumn_group_total(df2,list1)\n\ndf2.index.rename('Country',inplace=True)\ndf2.reset_index(inplace=True)\n\ndf3=pd.merge(pd.DataFrame(df2.Country).rename(columns={'Q3':'Country'}),df_world_values, how='left', on ='Country')\ndf3=pd.merge(df3,df_welzel, how='left', on ='Country')\ndf3=pd.merge(df3,df_quality_of_life, how='left', on ='Country')\ndf3=pd.merge(df3,df_age_structure, how='left', on ='Country')\ndf3=pd.merge(df3,df_population, how='left', on ='Country')\ndf3=pd.merge(df3,pd.DataFrame(df_responses.groupby(['Q3'])['Q3'].count()).rename(columns={'Q3':'RespCnt'}), how='left', left_on ='Country', right_index=True)\ndf3['RespShare']=df3['RespCnt']\/df3['Population']\ndf_indexes=df3\ndf3=pd.merge(df3,df2, how='left', on ='Country')\n\ndf_corr1=df3.dropna().corr('spearman')[:8].T\ndf3=df_corr1.abs()\ndf3['QNum']=df3.index.str.replace('_B_','B_').str.rsplit('_').str[0]\ndf3=pd.merge(pd.DataFrame(df_responses.columns.str.replace('_B_','B_').str.rsplit('_').str[0].drop_duplicates()[1:]),df3.groupby(['QNum']).max(),left_on=0,right_on='QNum')\ndf3.set_index(0,inplace=True)\n\ndf2=pd.merge(df_indexes[['Country','EmancipVal','RespShare']],df2, how='right', on ='Country')\ndf2.insert(2,'Grp','MIDDLE')\ndf2.loc[df2.Country.isin(df2.sort_values('EmancipVal').head(20).Country), 'Grp'] = 'LOW'\ndf2.loc[df2.Country.isin(df2.sort_values('EmancipVal',ascending=False).head(20).Country), 'Grp'] = 'HIGH'\ndf2.sort_values(by=['EmancipVal'],inplace = True,ascending=False)\n\ndf1=pd.merge(df2[['Country','Grp','EmancipVal']].dropna(),df_responses, how='inner',left_on='Country',right_on='Q3')\ndf_corr=df2.set_index('Country').corr('spearman')[:1].T","84cc7a81":"df=df3.loc[df3[((df3>=0.7)&(df3<1))|(df3<=-0.7)].dropna(how='all').index.to_list()].T\nfig,ax = plt.subplots(1,3, figsize=(25, 3), facecolor='w', edgecolor='k', gridspec_kw={'width_ratios': [1.5,1,4]})\nsns.heatmap(df, cmap=\"flare\", ax=ax[2])\nax[2].set_title('Correlation (white box means max by question)') \n\nval_max = df.idxmax(axis=0)\ncol_q = df.columns\n\ndf[df>0.7].T.count().plot.bar(color='#702663', ax=ax[0])\nax[0].set_title('Count of questions with correlation >0.7')\n\nval_max.value_counts().plot.bar(color='#702663', ax=ax[1])\nax[1].set_title('Number of questions with max correlation')\n\nfor col, variable in enumerate(col_q):\n    position = df.index.get_loc(val_max[variable])\n    ax[2].add_patch(Rectangle((col, position),1,1, fill=False, edgecolor='white', lw=1))\n  \n\nfig.subplots_adjust(left=None, bottom=None, right=None, top=None, wspace=0.4, hspace=0.3)","2e77ff81":"df=df2[['Country','EmancipVal','Grp']].dropna(axis=0)\nlist_grp=['HIGH','MIDDLE','LOW']\nf, ax = plt.subplots(1, 3, figsize=(15,8))\nfor i, var in enumerate(list_grp):\n    sns.barplot(x=df[df.Grp==var]['EmancipVal'], y=df[df.Grp==var]['Country'], palette=[palette_country_grp[i]], ax=ax[i])\n    ax[i].set(title=var,xlim=(0,0.8),ylabel=None)\nf.tight_layout()","c06ef8ca":"fig = plt.figure(figsize =(20,3))\ngs1 = fig.add_gridspec(1,1,bottom = -1, left=0.05, right=0.1, wspace=0.05)\nax1 = plt.subplot(gs1[0,0])\ngs2 = fig.add_gridspec(1,1,bottom = -0.5,left=0.3, right=0.35, wspace=0.05)\nax2 = plt.subplot(gs2[0,0])\ngs3 = fig.add_gridspec(1,1,bottom = -0.7,left=0.55, right=0.6, wspace=0.05)\nax3 = plt.subplot(gs3[0,0])\n\nheatmap_q (df_corr, ax1, order_q1, order_q1_short, 'Q1 Age')\nheatmap_q (df_corr, ax2, order_q6, order_q6_short, 'Q6 Writing code and\/or programming')\nheatmap_q (df_corr, ax3, order_q15, order_q15_short, 'Q15 Using machine learning methods')","a0e0c216":"list_annot=[['China','Ecuador','Germany','Czech Republic','Denmark'] ,['Iraq','Ethiopia','Denmark'] ,['Iraq','Japan']]\nlist_title=['Q1 Age','Q6 Years of writing code and\/or programming','Q15 Years of using machine learning methods']\nscatter_country(df2,['Q1_40+','Q6_5+ years','Q15_3+ years'],['40+','5+ years','3+ years'],list_title,list_annot,(20,5))","af428b9c":"bars_by_grp(df1,df_responses,'Q1',list(df1['Q1'].sort_values().unique()),'Q1 Distribution by the coyntry groups','Difference from the total distribution')","99d2e64b":"list_title=['Q22 Individuals are responsible for data science','Q23 ML methods into the business','Q24 Main activities at work']\n\nfig = plt.figure(figsize =(20,2))\ngs1 = fig.add_gridspec(1,1,bottom = -1, left=0.05, right=0.1, wspace=0.05)\nax1 = plt.subplot(gs1[0,0])\ngs2 = fig.add_gridspec(1,1,bottom = -0.4,left=0.35, right=0.4, wspace=0.05)\nax2 = plt.subplot(gs2[0,0])\ngs3 = fig.add_gridspec(1,1,bottom = -0.7,left=0.65, right=0.7, wspace=0.05)\nax3 = plt.subplot(gs3[0,0])\n\nheatmap_q (df_corr, ax1, order_q22, order_q22_short, list_title[0])\nheatmap_q (df_corr, ax2, order_q23, order_q23_short, list_title[1])\nheatmap_q (df_corr, ax3, order_q24, order_q24_short, list_title[2])","9ec6c995":"list_annot=[['Czech Republic','Denmark'] ,['Israel','Ecuador','Japan'] ,['Romania','Japan','Sweden','Norway']]\nlist_y=['Q22_5+','Q23_We have well established ML methods (i.e., models in production for more than 2 years)','Q24_Part_3_Build prototypes to explore applying machine learning to new areas']\nlist_ylabels=['5+ employees','We have well established ML methods','Build prototypes to explore applying ML to new areas']\nscatter_country(df2,list_y,list_ylabels, list_title,list_annot,(20,5))","575dcbbb":"fig = plt.figure(figsize =(20,3))\ngs1 = fig.add_gridspec(1,1,bottom = -1, left=0.05, right=0.1, wspace=0.05)\nax1 = plt.subplot(gs1[0,0])\ngs2 = fig.add_gridspec(1,1,bottom = -1,left=0.2, right=0.25, wspace=0.05)\nax2 = plt.subplot(gs2[0,0])\ngs3 = fig.add_gridspec(1,1,bottom = -0.4,left=0.55, right=0.6, wspace=0.05)\nax3 = plt.subplot(gs3[0,0])\n\n#heatmap_q (df_corr, ax1, order_q25_1, order_q25_1, \nsns.heatmap(df_corr.loc[order_q25_1], center=0, cmap=\"vlag\", ax=ax1, annot=True, cbar=False, yticklabels=order_q25_1_short)\nax1.set(title='Q25 What is your current yearly compensation (approximate $USD)?')\nheatmap_q (df_corr, ax2, order_q25_2, order_q25_2_short, '')\nheatmap_q (df_corr, ax3, order_q26, order_q26_short, 'Q26 Money spent on machine learning \\n and\/or cloud computing services in the past 5 year')","16d41a57":"list_annot=[['Saudi Arabia','Ireland','Norway'] ,['Ireland','Chile','Kazakhstan'] ]\nlist_title=['Q25 \u0421urrent yearly compensation','Q26 Money spent on machine learning \\n and\/or cloud computing services in the past 5 year']\nscatter_country(df2,['Q25_20,000$+','Q26_$1000+'],['20,000$+','$1000+'],list_title,list_annot,(15,5))","73b289d9":"If we look at the general situation by groups, we can see that in the **HIGH** and **MIDDLE** groups the maximum of respondents is **25-29**, while in the **LOW** group the maximum is in the **18-21** group. The strongest differences are in groups **18-21** and **22-24**.\n\n","ba60ca54":"We could consider other issues in a similar way, but we won't, because the deadline for the competition is already close, and my child wants to eat)\n\nTherefore, let's go straight to the next group of questions. These questions relate to work: \n* Q22 Approximately how many individuals are responsible for data science workloads at your place of business?\n* Q23 Does your current employer incorporate machine learning methods into their business?\n* Q24 Select any activities that make up an important part of your role at work: (Select all that apply)\n\nOn **Q22**, the largest correlation is in the **5+ group**. On **Q23**, the largest correlation is in the group **\"We have well established ML methods (i.e., models in production for more than 2 years)\"**. On **Q24**, the largest correlation is in the **\"Build prototypes to explore applying machine learning to new area\"**.","5e63a9e4":"The graph of the dependence of **EmancipVal** and the **20,000\\$+ group** shows that **Saudi Arabia** has a much larger proportion of employees with salaries of **20,000\\$+** than countries with a similar EmancipVal index. In **Chile** and **Ireland**, a lot of people have spent more than **$1000+** on DS over the past 5 years compared to countries similar in the index.","82ff53e7":"Let's look at the graphs for the groups with the strongest correlation. The graphs show that there really is a linear relationship.\n\nThere are also outliers: in **Germany** and **Denmark**, the proportion of respondents aged **40+** is less than in countries with a similar index. **Denmark** has a smaller proportion of people in programming for **longer than 5 years**. **Japan** has a smaller proportion of people in ML for **longer than 3 years**. In **Iraq**, a large proportion of people with **programming and ML experience**, but this may be due to a small number of respondents (43 people). A large proportion of the **40+** group in **Ecuador** may also be associated with a small number of respondents.\n","e747297b":"Let's look at the main dependencies (with a correlation greater than 0.8).\n\n## Main dependencies\n\nTo begin with, we will divide the countries according to the increasing **EmancipVal** into 3 groups: low medium and high.\nWe know the index for 60 countries out of 64 countries in the Kaggle Survey, so we get three equal groups.","b0fdf649":"Let's take the first three parameters: age, experience in programming and experience in machine learning. The tables below show which groups of respondents have the strongest connection with values.\nBy age, the largest correlation is in the **40+ group**. By experience in programming the largest correlation is in the **5+ years group**. By experience in ML the largest correlation is in the **3+ years group**.","cf4c6a7d":"## Conclusion\n\nSo, we found some dependencies, and also of course we didn't find some other dependencies. Based on this, we can assume that value characteristics **affect who** gets into the DS community. And value characteristics **do not affect the tools and methods** that data scientists use in their work.\n\nThe main conclusion is that it seems that not only economic parameters affect the DS. The value characteristics of the country is important for the formation of the DS community.\n\n## References\n1. https:\/\/www.worldvaluessurvey.org\/wvs.jsp\n2. https:\/\/www.numbeo.com\/cost-of-living\/\n3. https:\/\/en.wikipedia.org\/wiki\/World_Values_Survey\n","c4256a8e":"![7a96ca60-faa5-4557-9b76-17d4ab3aba6d.jfif](attachment:6ee8bb69-588c-4943-81e7-a4ec5a1d1c14.jfif)","1ca77b4e":"Let's look at the graphs for the groups with the strongest correlation. The graphs show a linear relationship. You can also see that in the **Czech Republic** a large proportion of respondents from firms with **5+** employees in data science. In **Ecuador**, there are no respondents from firms with **well established ML methods**. And in **Romania**, a large proportion of respondents are engaged in **building prototypes to explore applying machine learning to new area**.","7cab3bc2":"# World Values and Data Science Community","baeea1bd":"And finally, let's move on to money. The strongest correlation for **annual compensation (Q25)** in the **20,000\\$+ group**. According to **the money spent on ML and\/or cloud computing services (Q26)**, the strongest correlation is in the **1000\\$+ group**.","c30ab438":"## Introduction\n\nThe World Values Survey (WVS) is a global research project that explores people's values and beliefs, how they change over time, and what social and political impact they have. Since 1981 a worldwide network of social scientists have conducted representative national surveys as part of WVS in almost 100 countries.\n\nAnalysis of WVS data made by political scientists Ronald Inglehart and Christian Welzel asserts that there are two major dimensions of cross cultural variation in the world:\n* Traditional values versus secular-rational values and\n* Survival values versus self-expression values\n\nChristian Welzel also introduced the concepts of emancipative values and secular values. Emancipative values are an updated version of self-expression values. Secular values are an updated version of traditional versus secular rational values.[2]\n\nHow do you think there is any connection between these values and the DS community?\n\n## The main value\n\nLet's compare the characteristics of the countries with the responses of kaggle survey 2021.\nFor comparison, let's take four values from the WVS:\n* Traditional values versus secular-rational values (**TradSecVal**) [1]\n* Survival values versus self-expression values (**SurvSelfVal**) [1]\n* Sacred versus secural values (**SecularVal**) [1]\n* Obedient versus emancipative values (**EmancipVal**) [1]\n\nAdditionally, we will take several parameters from other sources:\n* Quality of Life Index[3]\n* Purchasing Power Index[3]\n* Cost of Living Index[3]\n* Median age by country (**MedAge**)\n* The proportion of people over 65 by country (**Above65**)\n\nIt turns out that the Kaggle Survey data correlate most with the **EmancipVal**, **Quality of Life Index** and **Above65** indicators. But this is based on the number of questions. At the same time, **EmancipVal** has the strongest correlation on most issues."}}