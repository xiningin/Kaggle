{"cell_type":{"f2f25bf8":"code","7f618c4d":"code","33a762dc":"code","1f52d913":"code","edcefb07":"code","debbc377":"code","b1934e89":"code","4be3a09d":"code","43be03c1":"code","06f97f8f":"code","440fa4e0":"code","bf5f4a5f":"code","f96447ba":"code","fa5298fc":"code","a8fcbd67":"code","47adab4b":"code","96ae4a63":"code","612f4270":"code","21acec93":"code","9abc0cab":"code","1edd5070":"code","140a9347":"code","8cd6e2d3":"code","32bab640":"code","497ed8a3":"code","11be47b7":"code","cfe29142":"code","d3064d00":"code","df40af72":"code","de15bbd4":"code","8c21492c":"code","922a42d4":"code","b7bab77a":"code","576e6fa0":"code","72ab1a91":"code","6aaa1b44":"code","82dc5e91":"code","8e15a464":"code","22b70d7b":"code","befc66dd":"code","d6ad0251":"code","0f3f0fda":"code","d33be837":"code","a9403322":"code","2771b4a6":"code","e23fc7ef":"code","00cc4a69":"code","76a7223e":"code","11ff9d61":"code","1757c79b":"code","d01f3726":"code","bc9f817e":"code","a3a73abf":"code","bec2c6af":"code","b1153bc9":"code","da2952ac":"code","f86a1dff":"code","a7cd4653":"code","38392f74":"code","04f811cc":"code","2a6b4765":"code","d8fd53f2":"code","be9b4adc":"code","d4f562bc":"code","40faab66":"code","7e3f1832":"code","66850afd":"code","2f8094cd":"code","d4245b9f":"code","09d4c5df":"code","ef22f672":"code","8fbafdf7":"code","c02087a8":"code","0f28bb31":"code","0fe739aa":"markdown","699ee68b":"markdown","9f8da1f5":"markdown","18897ddc":"markdown","5c1a401b":"markdown","04646da2":"markdown","f2d89354":"markdown","0e8370fd":"markdown","1a80f7f6":"markdown","0d86bfe3":"markdown","35a358be":"markdown","26390d56":"markdown","318a271f":"markdown","f2effcb7":"markdown","a1cc680c":"markdown","c00d872c":"markdown","5c7f8527":"markdown","37fb598d":"markdown","729cef5b":"markdown","19ffa739":"markdown","7906a38d":"markdown","df9a9a36":"markdown"},"source":{"f2f25bf8":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import cross_val_score, cross_validate\n\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import mean_absolute_error\n\n\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import LogisticRegression\n\n\n\n\ndf = pd.read_csv(\"..\/input\/predict-test-scores-of-students\/test_scores.csv\")","7f618c4d":"df.head()","33a762dc":"df.describe()","1f52d913":"df.isnull().sum()","edcefb07":"df.dtypes","debbc377":"df.info()","b1934e89":"df.school.unique()","4be3a09d":"df.n_student.unique()","43be03c1":"df.posttest.unique()","06f97f8f":"df.pretest.unique()","440fa4e0":"print(df.corr())","bf5f4a5f":"corr = df.corr()\n \nsns.heatmap(corr, annot=True, linewidth=.6, linecolor=\"blue\")\n\nplt.show()","f96447ba":"for x in [\"school\"]:\n    for val in df[x].unique():\n        count = df[x].value_counts()[val]\n        percent = df[x].value_counts(normalize=True)[val] * 100\n        print(f\"{val} - Count: {count}, Percentage: {percent:.2f}%\")\n    print()","fa5298fc":"for x in [\"school_setting\", \"school_type\", \"teaching_method\", \"gender\", \"lunch\"]:\n    for val in df[x].unique():\n        count = df[x].value_counts()[val]\n        percent = df[x].value_counts(normalize=True)[val] * 100\n        print(f\"{val} - Count: {count}, Percentage: {percent:.2f}%\")\n    print()","a8fcbd67":"sns.distplot(df[\"n_student\"])","47adab4b":"sns.distplot(df[\"pretest\"])","96ae4a63":"sns.distplot(df[\"posttest\"])","612f4270":"sns.distplot(df[[\"pretest\",\"posttest\"]])","21acec93":"sns.pairplot(df, x_vars=[\"n_student\"], y_vars=[\"posttest\"],height=8, aspect=1.5, kind=\"reg\");","9abc0cab":"sns.pairplot(df, x_vars=[\"n_student\"], y_vars=[\"pretest\"],height=8, aspect=1.5, kind=\"reg\");","1edd5070":"sns.displot(df, x=\"pretest\", hue=\"n_student\", kind=\"kde\",palette=\"Set1\")","140a9347":"sns.displot(df, x=\"posttest\", hue=\"n_student\", kind=\"kde\",palette=\"Set1\")","8cd6e2d3":"sns.lmplot(x=\"pretest\", y=\"posttest\", hue=\"n_student\", col=\"school\", data=df, palette=\"Set1\");","32bab640":"sns.lmplot(x=\"pretest\", y=\"posttest\", hue=\"n_student\", col=\"school_type\", data=df, palette=\"Set1\");","497ed8a3":"sns.lmplot(x=\"pretest\", y=\"posttest\", hue=\"n_student\", col=\"school_setting\", data=df, palette=\"Set1\");","11be47b7":"sns.relplot(x=\"pretest\", y=\"posttest\", hue=\"n_student\", col=\"gender\", data=df,  palette=\"Set1\");","cfe29142":"sns.relplot(x=\"pretest\", y=\"posttest\", hue=\"n_student\", col=\"teaching_method\", ci=None, kind=\"scatter\", data=df, palette=\"Set1\");","d3064d00":"sns.barplot(x=\"n_student\", y=\"pretest\", hue=\"lunch\",data=df,palette=\"Set1\");","df40af72":"\nsns.barplot(x=\"n_student\", y=\"posttest\", hue=\"lunch\",data=df,palette=\"Set2\");\n#sns.lmplot(x=\"pretest\", y=\"posttest\", hue=\"n_student\", col=\"lunch\", data=df, palette=\"Set1\");","de15bbd4":"sns.relplot(x=\"n_student\", y=\"pretest\", hue=\"gender\",style=\"lunch\",col=\"teaching_method\", ci=None, kind=\"line\", data=df, palette=\"Set1\");\n","8c21492c":"sns.relplot(x=\"n_student\", y=\"posttest\", hue=\"gender\", style=\"lunch\",col=\"teaching_method\", ci=None, kind=\"line\", data=df, palette=\"Set2\");\n","922a42d4":"sns.pairplot(df[['school_setting', 'school_type', 'teaching_method', 'n_student', 'gender', 'lunch', 'pretest', 'posttest']])\n","b7bab77a":"sns.pairplot(df[['school_setting', 'school_type', 'teaching_method', 'n_student', 'gender', 'lunch', 'pretest', 'posttest']], kind=\"kde\")\n","576e6fa0":"df2 = df.drop(['classroom','student_id'], axis = 1)\ndf2.head()","72ab1a91":"features = pd.get_dummies(df2)\n\nfeatures.rename(columns = {'school_type_Non-public' : 'school_type_Non_public','lunch_Does not qualify':'lunch_Does_not_qualify', 'lunch_Qualifies for reduced\/free lunch':'lunch_Qualifies_for_reduced\/free_lunch'}, inplace = True)\n\n\nfeatures.head()","6aaa1b44":"X = features.drop('posttest', axis=1)\ny = features[\"posttest\"]","82dc5e91":"y.head()","8e15a464":"# Splitting the data set \n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","22b70d7b":"X_train","befc66dd":"y_train","d6ad0251":"X_test","0f3f0fda":"y_test","d33be837":"from sklearn.metrics import explained_variance_score, mean_absolute_error\n\ndef test_score(y_test, y_pred):\n    \"\"\"Helper function for evaluation metrics.\"\"\"\n    accuracy = explained_variance_score(y_test, y_pred) * 100\n    mae = round(mean_absolute_error(y_test, y_pred), 2)\n    print(f\"\"\"accuracy: {accuracy:.2f}\"\"\")\n    print(f\"\"\"MAE: {mae:.2f}\"\"\")\n  \n    return accuracy","a9403322":"accuracy_scores = np.zeros(11, dtype=\"float64\")","2771b4a6":"#Linear Regression\nfrom sklearn.linear_model import LinearRegression\n\nreg = LinearRegression().fit(X_train, y_train)\ny_pred = reg.predict(X_test)\naccuracy_scores[0] = test_score(y_test, y_pred)","e23fc7ef":"#Lasso Regression\nfrom sklearn.linear_model import LassoCV\n\nreg1 = LassoCV().fit(X_train, y_train)\ny_pred1 = reg1.predict(X_test)\naccuracy_scores[1] = test_score(y_test, y_pred1)","00cc4a69":"#Descision Tree Regression\nfrom sklearn.tree import DecisionTreeRegressor\n\nreg2 = DecisionTreeRegressor().fit(X_train, y_train)\ny_pred2 = reg2.predict(X_test)\naccuracy_scores[2] = test_score(y_test, y_pred2)","76a7223e":"#Support Vector Regressor\nfrom sklearn.svm import SVR\n\nreg3 = SVR().fit(X_train, y_train)\ny_pred3 = reg3.predict(X_test)\naccuracy_scores[3] = test_score(y_test, y_pred3)","11ff9d61":"#Random Forest Regressor\nfrom sklearn.ensemble import RandomForestRegressor\n\nreg4 = RandomForestRegressor().fit(X_train, y_train)\ny_pred4 = reg4.predict(X_test)\naccuracy_scores[4] = test_score(y_test, y_pred4)","1757c79b":"#Gradient Boosting Regressor\nfrom sklearn.ensemble import GradientBoostingRegressor\n\nreg5 = GradientBoostingRegressor()\n#n_estimators=100, random_state=42\nreg5.fit(X_train, y_train)\ny_pred5 = reg5.predict(X_test)\naccuracy_scores[5] = test_score(y_test, y_pred5)","d01f3726":"#XGBoost Regressor\nfrom xgboost import XGBRegressor\n\nxg_model = XGBRegressor()\nxg_model.fit(X_train, y_train)\nxg_pred = xg_model.predict(X_test)\n\naccuracy_scores[6] = test_score(y_test, xg_pred)","bc9f817e":"#LightGBM Regressor\nimport lightgbm \nlgb_model = lightgbm.LGBMRegressor()\nlgb_model.fit(X_train, y_train)\nlgb_pred = lgb_model.predict(X_test)\n\naccuracy_scores[7] = test_score(y_test, lgb_pred)","a3a73abf":"#AdaBoost Regressor\nfrom sklearn.ensemble import AdaBoostRegressor\nABR_model = AdaBoostRegressor()\nABR_model.fit(X_train, y_train)\nABR_pred = ABR_model.predict(X_test)\n\naccuracy_scores[8] = test_score(y_test, ABR_pred)\n","bec2c6af":"#Regression with Tensorflow\n#pip install --upgrade tensorflow\nimport tensorflow as tf\nfrom sklearn.metrics import mean_squared_error\nfrom math import sqrt\nfrom sklearn.preprocessing import StandardScaler\nfrom tensorflow.keras.layers import Input, Dense, Activation,Dropout\nfrom tensorflow.keras.models import Model\nprint(tf.__version__)\n\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)\n\ninput_layer = Input(shape=(X.shape[1],))\ndense_layer_1 = Dense(100, activation='relu')(input_layer)\ndense_layer_2 = Dense(50, activation='relu')(dense_layer_1)\ndense_layer_3 = Dense(25, activation='relu')(dense_layer_2)\noutput = Dense(1)(dense_layer_3)\n\nmodel = Model(inputs=input_layer, outputs=output)\nmodel.compile(loss=\"mean_squared_error\" , optimizer=\"adam\", metrics=[\"mae\"])\n","b1153bc9":"print(model.summary())","da2952ac":"history = model.fit(X_train, y_train, batch_size=128, epochs=100, verbose=1, validation_split=0.2)","f86a1dff":"model.evaluate(X_test, y_test)","a7cd4653":"tensor_pred = model.predict(X_test)\n\naccuracy_scores[9] = test_score(y_test, tensor_pred)","38392f74":"test_predictions = model.predict(X_test).flatten()\n\na = plt.axes(aspect='equal')\nplt.scatter(y_test, tensor_pred)\nplt.xlabel('True Values [MPG]')\nplt.ylabel('Predictions [MPG]')\nlims = [0, 100]\nplt.xlim(lims)\nplt.ylim(lims)\n_ = plt.plot(lims, lims)","04f811cc":"error = test_predictions - y_pred\nplt.hist(error, bins=25)\nplt.xlabel('Prediction Error [MPG]')\n_ = plt.ylabel('Count')","2a6b4765":"from sklearn.model_selection import KFold\nlgb_model = lightgbm.LGBMRegressor()\nkfold_validation=KFold(10)\n","d8fd53f2":"import numpy as np\nfrom sklearn.model_selection import cross_val_score\nresults=cross_val_score(lgb_model,X,y,cv=kfold_validation)\nprint(results)\nprint(np.mean(results))","be9b4adc":"from sklearn.model_selection import StratifiedKFold\nskfold=StratifiedKFold(n_splits=5)\nlgb_model=lightgbm.LGBMRegressor()\nscores=cross_val_score(lgb_model,X,y,cv=skfold)\nprint(np.mean(scores))","d4f562bc":"## We use this parameters in LightGBM regressor\n## Hyper Parameter Optimization\nn_estimators = [100, 500, 900, 1100, 1500]  #Number of Decision Trees\nmax_depth = [2, 3, 5, 10, 15]\nbase_score=[0.25,0.5,0.75,1]\nbooster=['gbtree','gblinear'] #By default it select gbtree but i gave just to see how it performs\nlearning_rate=[0.05,0.1,0.15,0.20]\nmin_child_weight=[1,2,3,4]\n\n# Define the grid of hyperparameters to search\nhyperparameter_grid = {\n    'n_estimators': n_estimators,\n    'max_depth':max_depth,\n    'learning_rate':learning_rate,\n    'min_child_weight':min_child_weight,\n    'booster':booster,\n    'base_score':base_score\n    }","40faab66":"from sklearn.model_selection import RandomizedSearchCV\nrandom_cv = RandomizedSearchCV(estimator=lgb_model,\n            param_distributions=hyperparameter_grid,\n            cv=5, n_iter=50,\n            scoring = 'neg_mean_absolute_error',n_jobs = 4,\n            verbose = 5, \n            return_train_score = True,\n            random_state=42)","7e3f1832":"random_cv.fit(X_train,y_train)","66850afd":"random_cv.best_estimator_","2f8094cd":"random_cv.best_estimator_","d4245b9f":"lgb_reg = lightgbm.LGBMRegressor(base_score=1, booster='gbtree', learning_rate=0.2, max_depth=2,\n              min_child_weight=3, n_estimators=1500)","09d4c5df":"lgb_reg.fit(X_train, y_train)","ef22f672":"lgb_pred1 = lgb_reg.predict(X_test)\n\n#accuracy_scores[10] = test_score(y_test, lgb_pred)","8fbafdf7":"accuracy_scores[10] = test_score(y_test, lgb_pred1)","c02087a8":"sns.set_style('whitegrid')\n\nmodels = [\"Linear Regression\",\"Lasso Regressor\",\"Decision Tree Regressor\",\"Support Vector Regressor\",\"Random Forest Regressor\",\"Gradient boost Regressor\",\"XGBoost Regressor\",\"LightGBM REgressor\",\"Ada Boost Regressor\",\"Tensor Regressor\",\"XG Boost Hyper\"]\n\n\nplt.figure(figsize=(11, 11))\nsns.barplot(x=accuracy_scores, y=models)\n\n\nplt.xlabel(\"Model_Name\")\nplt.xticks(rotation = -90)\nplt.ylabel(\"Accuracy\")\n\nplt.show()","0f28bb31":"sns.set_style('whitegrid')\nmodels = [\"Linear Regression\",\"Lasso Regressor\",\"Decision Tree Regressor\",\"Support Vector Regressor\",\"Random Forest Regressor\",\"Gradient boost Regressor\",\"XGBoost Regressor\",\"LightGBM REgressor\",\"Tensor Regressor\",\"AdaBoost Regressor\"]\n\nmae = [\"2.50\",\"2.61\",\"3.23\",\"3.38\",\"2.63\",\"2.48\",\"2.48\",\"2.48\",\"2.62\",\"2.82\"]\n\nplt.figure(figsize=(11, 11))\nsns.relplot(x=models, y=mae)\n\nplt.xlabel(\"Model_Name\")\nplt.xticks(rotation = -90)\nplt.ylabel(\"Accuracy\")\n\nplt.show()","0fe739aa":"Mean Squared Error (MSE) and Mean Absolute Error (MAE) are common loss functions used for regression problems. Mean Absolute Error is less sensitive to outliers. Different loss functions are used for classification problems.","699ee68b":"Classes has more in doesnot qualified for lunch even they have high test scores","9f8da1f5":"It looks like the model predicts reasonably well.\n\nNow take a look at the error distribution:","18897ddc":"As observed, urban area has major number of students who took both pretest and posttest and also urban area students has highest score in both pretest and posttest scores","5c1a401b":"We can clearly observe from below regressions that when strength of class is less then the pretest and posttest score is increased\n","04646da2":"Class 26 and 17 stands top scorers in experimental teaching in posttest\nclass 16 and 25 stands top scorers in standard teaching in pretest\n","f2d89354":"understanding the probability distribution between the variables of a dataframe\n\n\nWhen a distribution has lower variability, the values in a dataset are more consistent. However, when the variability is higher, the data points are more dissimilar and extreme values become more likely. Consequently, understanding variability helps you grasp the likelihood of unusual events.","0e8370fd":"Make predictions\nFinally, predict have a look at the errors made by the model when making predictions on the test set:","1a80f7f6":"Conclusion\n\n\nFrom the above comparision of all algorithms with thier acccuracy and mean_absolute_error\n\nLightGBM boosting model performs well with accuracy of 95.04 and mae of 2.48\n\nIf you clearly observe the accuracy and mean_absolute error yielded by boosting algorithms are similar nearly so, Boosting models works well on the dataset.","0d86bfe3":"Here i check with cross validation and hyper parameter tunning","35a358be":"Both Male and female has highest test scores and class 17 & 26 stands first place in top scores in both phases","26390d56":"When applied to an entire dataframe, the corr() function returns a dataframe of  pair-wise correlation between the columns. We can see that there\u2019s a weak negative correlation between scores of n_students and pretest\/posttest. Also, notice that the values on the diagonal are 1s, this is because each column is perfectly correlated with itself.\n\nA positive correlation indicates that the values tend to increase with one another\nA negative correlation indicates that values in one set tend to decrease with an increase in the other set","318a271f":"##Data Modeling","f2effcb7":"The above plot is a uniform distribution of values in the \u2018posttest\u2019 feature. Thus, the feature is perfectly formatted with mean and median values close to each other.","a1cc680c":"From the above regression, i observed the IDGFP has highest test scores and GOOBU, KZKKE, VVTA schools has less test scores","c00d872c":"*Our* both test scores are in normal distribution\nso, our data can yied good machine learning model","5c7f8527":"Comparing above two charts for students I observed the rate of posttest score increased than pretest score who qualified for lunch \n\nlarge number of population of students who doesnot qualified for lunch has highest pretest and posttest score.","37fb598d":"From the above plot number of students in a class feature has a non-uniform in distribution. The mean and median values are values apart. As you can see the \u2018n_student\u2019 plot is right-skewed (long tail on the right) it has its mean greater than its median.\n\n","729cef5b":"If you observe clearly the uniform distribution of classes are high and non-uniform distribution of classes is low in the pretest scores\n\n","19ffa739":"If you observe clearly the uniform distribution of classes are high and non-uniform distribution of classes is low in the posttest scores\n\n","7906a38d":"The aboveplot is a uniform distribution of values in the \u2018pretest\u2019 feature. Thus, the feature is perfectly formatted with mean and median values close to each other.\n\n","df9a9a36":"Hyper Parameter tuning for LightGBM model\n"}}