{"cell_type":{"9182e582":"code","d5a95bca":"code","5ee9817e":"code","39d395a1":"code","42300500":"code","41dc14fa":"code","1b5a7d49":"code","45c0e053":"code","a990764f":"code","7c16a906":"code","b009941d":"code","eb3730eb":"code","c9a94763":"code","a3cd99d2":"code","44d3cdd7":"code","97b099ce":"code","0108a346":"code","305dcca2":"code","c2e6be43":"code","fc94b1c4":"code","35774f47":"code","00fd7030":"code","15310116":"code","826ae2c9":"code","4320916e":"markdown","2c6c7981":"markdown","96e07b6b":"markdown","140f4837":"markdown","bdcc4389":"markdown","a97e7c68":"markdown","5e35027d":"markdown","b99a4f20":"markdown","b630ecac":"markdown","c5abc0c9":"markdown","3c6eb48a":"markdown","0eb04b3e":"markdown","df1b784d":"markdown","f659a58f":"markdown","37444400":"markdown","c7cc05e7":"markdown","badda281":"markdown","2f9554e1":"markdown","dfa40a78":"markdown","b4439e99":"markdown","039fc97f":"markdown","5fe8cca1":"markdown","2a3c982b":"markdown","4f3d4188":"markdown","4718e249":"markdown","7312fd10":"markdown","10f1fe40":"markdown"},"source":{"9182e582":"import numpy as np, pandas as pd, seaborn as sns\nimport matplotlib.pyplot as plt\nfrom statsmodels.tsa.stattools import adfuller\nfrom pandas import Series\nimport datetime","d5a95bca":"item_cats = pd.read_csv('..\/input\/competitive-data-science-predict-future-sales\/item_categories.csv')\nitems = pd.read_csv('..\/input\/competitive-data-science-predict-future-sales\/items.csv')\nsales_train = pd.read_csv('..\/input\/competitive-data-science-predict-future-sales\/sales_train.csv')\nshops = pd.read_csv('..\/input\/competitive-data-science-predict-future-sales\/shops.csv')\ntest_df = pd.read_csv('..\/input\/competitive-data-science-predict-future-sales\/test.csv')","5ee9817e":"sales_train.head()","39d395a1":"sales_train.dtypes","42300500":"sales_train.date = sales_train.date.apply(lambda x: datetime.datetime.strptime(x, '%d.%m.%Y'))\nsales_train.dtypes","41dc14fa":"from IPython.display import display\ndisplay(sales_train.head()) # show first 5 rows\ndisplay(sales_train.shape) # number of rows, columns\ndisplay(sales_train.isnull().any()) # How many empty values in each column\ndisplay(sales_train.describe()) # Summary statistics","1b5a7d49":"\"\"\"\nIn this cell we are having a look at the total sales for the company 1C.\nIt appears as though there is a downward trend and seasonality.\n\"\"\"\nts = sales_train.groupby(['date_block_num'])['item_cnt_day'].sum()\nts.astype(float)\n\nrolling_mean = ts.rolling(window = 12).mean() # rolling average of 12 months\nrolling_std = ts.rolling(window = 12).std() # rolling std of 12 months\n\nplt.figure(figsize=(16,8))\nplt.title('Total Sales of 1C')\nplt.xlabel('Month')\nplt.ylabel('Units Sold')\nplt.plot(ts, color = 'blue', label = 'Sales')\nplt.plot(rolling_mean, color = 'red', label = 'Rolling Mean')\nplt.plot(rolling_std, color = 'black', label = 'Rolling Std')\nplt.legend(loc = 'best')\nplt.show()","45c0e053":"\"\"\"\nIn this cell we perform the ADF test to check for stationarity. The\nADF tests the null hypothesis that a unit root is present in the\ntime series. i.e. if the p-value is less than 5%, you can reject the\nnull hypothesis and assume that the data is stationary.\n\"\"\"\n\ndef adf_test(ts):\n    print('ADF test results:')\n    adf = adfuller(ts, autolag  = 'AIC')\n    adf_out = pd.Series(adf[0:4], index=['Test Statistic',\n                                        'p-value','#Lags Used',\n                                        'Number of Observations Used'])\n    for key, val in adf[4].items():\n        adf_out['Critical Value (%s)' %key] = val\n    print(adf_out)\n    \nadf_test(ts)","a990764f":"def difference(df, interval=1):\n    diff = [] # Create empty list\n    for i in range(interval, len(df)): # Iterate over every lag\n        val = df[i] - df[i - interval] # Take the difference between consective terms\n        diff.append(val) # Add the new values to the end of the list\n    return Series(diff) # Return the differenced values as a time series","7c16a906":"\"\"\"\nBelow the original time series is plotted, the same as the plot above.\n\"\"\"\nts.astype(float)\nplt.figure(figsize=(16,16))\nplt.subplot(311)\nplt.title('Original')\nplt.xlabel('Month')\nplt.ylabel('Units Sold')\nplt.plot(ts) # Plot the original time series\n\n\"\"\"\nBelow the new differenced time series is plotted.\n\"\"\"\nnew_ts = difference(ts) # difference the time series\nplt.subplot(312)\nplt.title('Post-differencing')\nplt.xlabel('Month')\nplt.ylabel('Units Sold')\nplt.plot(new_ts)\nplt.plot()\n\n\"\"\"\nBelow the time series is de-seasonalised (assuming the seasonality\n12 months long)\n\"\"\"\nds_ts = difference(ts, interval = 12)\nplt.subplot(313)\nplt.title('After De-seasonalising')\nplt.xlabel('Month')\nplt.ylabel('Units Sold')\nplt.plot(ds_ts)\nplt.plot","b009941d":"print('Differenced')\nadf_test(new_ts)\n\nprint('\\nDeseasonalised')\nadf_test(ds_ts)","eb3730eb":"from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\nplot_acf(difference(difference(ts)));\nplt.title('2nd Order Differencing ACF')\nplt.show()","c9a94763":"from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\nplot_acf(ds_ts);\nplt.title('Deseasonalised ACF')\nplt.show()","a3cd99d2":"'''\nfrom pmdarima.arima.utils import ndiffs, nsdiffs\n\n# Normal Differencing:\n\n# ADF test\nd_adf = ndiffs(ts, test='adf') # = 1\n\n# KPSS test\nd_kpss = ndiffs(ts, test='kpss') # = 2\n\n# PP test\nd_pp = ndiffs(ts, test='pp') # = 0\n\nprint('Difference Estimations:\\nADF:%s KPSS:%s PP:%s' % (d_adf,d_kpss,d_pp))\n'''","44d3cdd7":"'''\nLooping over possible values of p and q and measuring their AIC.\n\nAIC can be thought of like mean squared error, it measures on average\nhow far off the prediction is from the actual result.\n'''\nimport statsmodels.api as sm\nimport warnings\n\nrng = range(5)\nbest_aic = np.inf\nbest_model = None\nbest_order = None\n\nwarnings.filterwarnings('ignore')\n\nfor p in rng:\n    for q in rng:\n        temp_model = sm.tsa.statespace.SARIMAX(ds_ts, order = (p, 0, q)) # Try out different vals of p & q\n        results = temp_model.fit()\n        temp_aic = results.aic\n        if temp_aic < best_aic: # If model outperforms prev attempts, save the order\n            best_aic = temp_aic\n            best_order = (p, 0, q)\n            best_model = temp_model\n\nprint('Best AIC: %s | Best order: %s' % (best_aic, best_order))\n\nwarnings.warn('Reinstating warnings')","97b099ce":"\"\"\"\nSo in the above code cell we determined that p & q were best set at 1.\nEarlier on with the ADF test we found that we needed to perform a seasonal difference with the interval set to 12.\n\nWe supplied the SARIMAX function with 3 parameters here; order, trend and seasonal order.\n* The order parameter is just a copy of the results above.\n* I chose the trend through trial and error, setting it to 't' gave me the best results.\n* The seasonal order is (P,D,Q,m) where m is the number of time steps, 12 in our case. We set d to 1 because we only need 1\n  seasonal difference and p & q are already used in the order parameter. We could supply seasonal P & Q but it's important\n  not to make the model too complex and cause overfitting.\n\"\"\"\nsarima_model = sm.tsa.statespace.SARIMAX(ts, order = (1,0,1),trend = 't', seasonal_order=(0,1,0,12))\nresults = sarima_model.fit()\nprint(results.aic)","0108a346":"'''\nWe'll predict from the 22nd month, 2 years into the future.\n'''\nfrom statsmodels.tsa.statespace.sarimax import SARIMAXResults\n\n\npreds = SARIMAXResults.predict(results, start = 33, end = 46)\n\n\nax = ts.plot(label = 'Observed')\npreds.plot(ax = ax, label = 'SARIMA forecast')\nplt.legend()\nplt.title('1C Sales')\nax.set_xlabel('Month')\nax.set_ylabel('Units Sold')\nplt.show()","305dcca2":"'''\nBefore forecasting we need to add the dates back into the time-series\n'''\nts.index = pd.date_range(start = '2013-01-01', \n                         end = '2015-10-01', \n                         freq = 'MS')\nts = ts.reset_index()\nts.head()","c2e6be43":"from fbprophet import Prophet # Import the package\n\n# Prophet requires you to name your columns the following:\nts.columns = ['ds','y']\nprophet_model = Prophet(yearly_seasonality = True) # As determined in stationarity testing\nprophet_model.fit(ts)\n\n# We'll predict 12 months into the future\n# 'MS' = month start\nfuture = prophet_model.make_future_dataframe(periods = 12, freq = 'MS')\nforecast = prophet_model.predict(future)\nforecast.head()","fc94b1c4":"prophet_model.plot(forecast);\nplt.title('1C Sales - Prophet Forecast')\nplt.xlabel('Date')\nplt.ylabel('Units Sold')\nplt.show()","35774f47":"prophet_model.plot_components(forecast)","00fd7030":"ts = sales_train.groupby(['date_block_num'])['item_cnt_day'].sum()\nax = ts.plot(label = 'Observed')\npreds.plot(ax = ax, label = 'SARIMA forecast', alpha = 0.9, linestyle = '-')\nforecast.yhat[33:46].plot(ax = ax, label = 'Prophet forecast', alpha = 0.9, linestyle = '--')\n\nplt.legend()\nplt.title('1C Sales')\nax.set_xlabel('Month')\nax.set_ylabel('Units Sold')\nplt.show()","15310116":"\"\"\"\nSorry for the lack of annotations in this cell, almost all of it is copied and pasted from previous cells, aside from some \npretty formatting.\n\"\"\"\nimport matplotlib\n\nfig=plt.figure(figsize=(12,8), dpi= 60, facecolor='w', edgecolor='k')\n\nsales_train['Rev'] = sales_train['item_cnt_day'] * sales_train['item_price']\nts = sales_train.groupby(['date_block_num'])['Rev'].sum()\nax = ts.plot(label = 'Observed')\n\nsarima_rev_model = sm.tsa.statespace.SARIMAX(ts, order = (1,0,1),trend = 't', seasonal_order=(0,1,0,12))\nrev_results = sarima_rev_model.fit()\nsarima_rev_preds = SARIMAXResults.predict(rev_results, start = 33, end = 46)\n\n\nts.index = pd.date_range(start = '2013-01-01', \n                         end = '2015-10-01', \n                         freq = 'MS')\nts = ts.reset_index()\nts.head()\nts.columns = ['ds','y']\nprophet_model = Prophet(yearly_seasonality = True) # As determined in stationarity testing\nprophet_model.fit(ts)\nfuture = prophet_model.make_future_dataframe(periods = 12, freq = 'MS')\nforecast = prophet_model.predict(future)\n\nforecast.yhat[33:46].plot(ax = ax, label = 'Prophet forecast', alpha = 0.9, linestyle = '--')\nsarima_rev_preds.plot(ax = ax, label = 'SARIMA forecast', alpha = 0.9, linestyle = '-')\n\n\nplt.ticklabel_format(style = 'plain')\nax.get_yaxis().set_major_formatter(\n    matplotlib.ticker.FuncFormatter(lambda x, p: format(int(x), ',')))\n\nplt.legend()\nplt.title('1C Sales')\nax.set_xlabel('Month')\nax.set_ylabel('Revenue ($)')\nplt.show()","826ae2c9":"# Verifying the length of the dataset\nprint(len(sales_train))\n\n\n# Roughly 3 million records\n","4320916e":"# Time series Analysis\n\n## Stationarity\n\n**definition**: The statistical properties of a stationary time series do not change over time. i.e. 2 points in a time series are related to each other by only how far apart they are & not by the direction (each point is independent).\n\nEssentially, the mean, variance, and covariance should remain constant over time. If the data has a trend, it isn't stationary.\n\nThe reason it's important, without going into the math, is that many models rely on stationarity and assume that the data is too.\n\nYou can test for stationarity with the following tests:\n* Augmented Dicky Fuller (ADF)\n* KPSS\n* Philips-Perron (PP)\n\nFor our data I will be performing an ADF test.","2c6c7981":"### Considerations\nYou have to be careful not to over-difference the time series. An over-differenced series may still be stationary, but will affect the model parameters (settings).\n\nYou should aim to use the minimum necessary differences to achieve stationarity.\n\n**How do you know if a time series is over differenced?** Optimaly, the Autocorrelation Function (ACF) plot should reach 0 quickly, as seen below. If the first lag (the second pole on the PACF plot) is too far in the negative, then it is probably over-differenced.","96e07b6b":"# Introduction","140f4837":"## How does the SARIMA model work?\nThere are 3 important terms in ARIMA models: p, d & q\n* **p** is the order of the AR term\n* **q** is the order of the MA term\n* **d** is the number of times differencing is required to make the time series stationary\n* **s** the seasonal component is comprised of:\n    * P - The seasonal autoregressive order\n    * D - The seasonal difference order\n    * Q - The Seasonal moving average order\n    * m - The number of time steps in a single seasonal period\n\n**What do these terms mean?**\nThe AR part in ARIMA is a linear regression model that uses its own lags (previous time steps) as predictors. For a linear regression model to be effective you need the predictors to be independent of each other (not correlated), i.e. the time series needs to be stationary.\n\nA common and effective way to make a time series stationary is to difference it (subtract the previous value from the current value). Depending on how complex the series is you may need more than one differencing. **d** is the minimum number of differences needed for the data to be stationary, so if it is stationary by default; d = 0.\n\n**p** is the order of the AR term and refers to the number of lags (time steps) of Y (the dependent (the variable you're trying to forecast)) to be used as predictors.\n\n**q** is the order of the MA terms and refers to the number of lagged forecast errors that should go into the ARIMA model.\n\nAn ARIMA model is a model that is differenced at least once and combines the MA and AR terms.\n\npredicted Yt = Constant + linear combination of lags of Y (up to p lags) + linear combination of lagged forecast errors (up to q lags)\n\n[source: https:\/\/www.machinelearningplus.com\/time-series\/arima-model-time-series-forecasting-python\/ ]","bdcc4389":"# Prophet Forecasting\n\nIn February 2017 Facebook's Data Science team open sourced their forecasting library \"Prophet\". It's a highly optimised package to quickly perform forecasting on non-stationary data.","a97e7c68":"### Reading in Data","5e35027d":"# Finishing up\n\nIt seems as though SARIMA does a better job of generalising and appears to be the simpler model, although Prophet is much easier to implement.\n\nLet's use this model to predict revenue rather than units sold as it's far more useful information.","b99a4f20":"# Data Handling","b630ecac":"I'm sure you're well aware of the value of accurate forecasts, but producing them isn't easy. In this document I'll try to outline two basic univariate time series forecasting methods in simple and easy to understand language, assuming you have a basic knowledge of statistics and python.\n\n**Time series data definition**: Data collected on the same metrics or same objects at regular time intervals. It could be stock market records or sales records.\n\n**Univariate Time Series Forecasting**: Only using the previous values in a time series to predict future values (not using any outside variables).","c5abc0c9":"### Estimating the differencing term (d)\n\nIt is possible to use packages to estimate the number of differences required. We can use the function \"ndiffs()\" to perform a test of stationarity for different levels of d (and different tests) and estimate the number of differences required to make the time series stationary. As seen by the results below it doesn't always work, we know from the above tests that d is neither 1,2 or 0.","3c6eb48a":"The ADF test of the deseasonalised data is below 5%, we can therefore reject the null hypothesis and assume the deseasonalised series is stationary. ","0eb04b3e":"As previously described, the first lag goes far into the negative, suggesting that it is over-differenced.\n\nThe deseasonalised series is a much better series to work on:","df1b784d":"Let's take a deeper look at our sales dataframe:","f659a58f":"### Importing Packages","37444400":"The best practice is to split the data into a training and testing set prior to fitting the model to validate it's accuracy, however I do want to keep this brief.","c7cc05e7":"Ok so let's have a look at an over-differenced series:","badda281":"## Data Exploration","2f9554e1":"Let's test the differenced and deseasonalised series:","dfa40a78":"## Differencing\n\n**definition**: Differencing is a transformation of a time series, taking the difference between consecutive terms in a series. It can be used to remove time dependency and stabilise the mean, reducing trends and seasonality.\n\n","b4439e99":"### Inspecting the data","039fc97f":"## Forecasting Sales for 1C","5fe8cca1":"**Autocorrelation:** autocorrelation summarises the strength of a relationship with an observation in a time series with observations at previous steps.\n\nIn simpler terms: correlation is the strength of a relationship between 2 variables (-1 -> 1), because the correlation of the time series observations are calculated with values of the same series at prior time steps, this is called a serial correlation or *autocorrelation*.\n\n**How to read the above graph:** The ACF plot shows the lag value along the x-axis & the correlation on the y-axis (betweeen -1 and 1). By default the plot_acf function has a 95% confidence interval cone in light blue, suggesting that values outside of this cone are likely a correlation and not a statistical fluke.","2a3c982b":"The p-value is 14.3%, we therefore can't assume stationarity. ","4f3d4188":"We need to change the date into a datetime variable","4718e249":"## Finding the AR term (p)\nWe find p by analysing the Partial AutoCorrelation Function (PACF) plot.\n\n**PACF explanation:** Autocorrelation for an observation & another observation at a prior time step is comprised of both the direct correlation & indirect correlations. The indirect correlations are a linear function of the correlation of the observation with observations at intervening time steps.\n\nIt is these indirect correlations that the PACF seeks to remove. The correlation between point Y0 and Y1 will have seome inertia and affect points later on.\n\nIn short, the PACF kind of conveys the pure correlation between an observation and the series. That way you will know if the obsevation is needed in the AR term or not.\n\n**How do we find p?:** Any autocorrelation in a stationary time series can be fixed by adding enough AR terms. So we initially take the order of the AR term to be equal to the number of lags that cross the significance limit in the PACF plot.\n\nTime series analysis is a bit of an art, there isn't a set methodlogy that you have to follow, many people analyse the ACF and PACF plots to find certain patterns that may give away the right order, but it is also possible to systematically find the correct order, although it is rather computationally intensive. If you'd like to read more about that, there's a fantastic article on the topic [here](https:\/\/www.machinelearningplus.com\/time-series\/arima-model-time-series-forecasting-python\/)","7312fd10":"# SARIMA Modeling","10f1fe40":"Now that the time series is differenced, we can move on to building our models.\n\nSeasonal AutoRegressive Integrated Moving Average modeling is an old statistical model that combines a moving average (MA), an auto regressive (AR) model and a seasonal component.\n* MA: Assumes that the next value in the series is a function of the average of the previous n values.\n* AR: Assumes that the next value in the series is a function of the errors (difference in the mean) in the previous n values.\n\nPros:\n* Very effective; remains close to cutting edge performance\n* Simple to implement and not computationally intensive\n\nCons:\n* Not very intuitive\n* No way to build in our understanding about how our data works:\n    * random walk element\n    * external regressors"}}