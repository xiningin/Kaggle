{"cell_type":{"9401e15a":"code","1ea81cdb":"code","a23a0c51":"code","5b833b95":"code","f81e785e":"code","edd70503":"code","fbfb4831":"code","643d8789":"code","e3dffed9":"code","43763b7a":"code","62e20361":"code","66498435":"code","a2a5dd97":"code","67fa6ecc":"code","4b4fd444":"code","ad28430f":"code","ab650304":"code","4a21940e":"code","fd37383e":"code","3ec0e835":"code","0887ec30":"code","b4204f44":"code","25b52aee":"code","23365597":"code","6550e1c3":"markdown","2f15a4df":"markdown","2210e642":"markdown","be68bd34":"markdown","20cdbc62":"markdown","dc2dafcd":"markdown"},"source":{"9401e15a":"%%capture\n\n# https:\/\/www.kaggle.com\/bguberfain\/openai-clip-with-train\/notebook\n\nimport sys\n!cp -r ..\/input\/openai-clip\/CLIP\/CLIP-main \/tmp\/\n\n# Kaggle likes to unpack .gz files in datasets... so we have to pack it back\n!gzip -c \/tmp\/CLIP-main\/clip\/bpe_simple_vocab_16e6.txt > \/tmp\/CLIP-main\/clip\/bpe_simple_vocab_16e6.txt.gz\nsys.path.append('\/tmp\/CLIP-main')\n\n!pip install ..\/input\/openai-clip\/ftfy-5.9\/ftfy-5.9\n!pip install ..\/input\/openai-clip\/torch-1.7.1+cu110-cp37-cp37m-linux_x86_64.whl \\\n             ..\/input\/openai-clip\/torchvision-0.8.2+cu110-cp37-cp37m-linux_x86_64.whl \\\n             ..\/input\/faiss-163\/faiss_gpu-1.6.3-cp37-cp37m-manylinux2010_x86_64.whl","1ea81cdb":"import torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom pathlib import Path\nfrom os.path import join\nimport numpy as np\nimport pandas as pd\nimport clip, os, skimage\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom PIL import Image\nfrom tqdm import tqdm\n\nclip.available_models()","a23a0c51":"model, preprocess = clip.load(\"..\/input\/openai-clip\/ViT-B-32.pt\", jit=False)\nmodel = model.cuda().eval()\ninput_resolution = model.visual.input_resolution\ncontext_length = model.context_length\nvocab_size = model.vocab_size\n\nprint(\"Model parameters:\", f\"{np.sum([int(np.prod(p.shape)) for p in model.parameters()]):,}\")\nprint(\"Input resolution:\", input_resolution)\nprint(\"Context length:\", context_length)\nprint(\"Vocab size:\", vocab_size)","5b833b95":"train_image_path = Path(\"..\/input\/petfinder-pawpularity-score\/train\")\nfile_names = [f.name for f in train_image_path.iterdir() if f.suffix == \".jpg\"]","f81e785e":"original_images = []\nimages = []\nplt.figure(figsize=(15, 12))\n\nfor filename in file_names[:9]:\n    image = Image.open(join(train_image_path, filename))\n  \n    plt.subplot(3, 3, len(images) + 1)\n    plt.imshow(image)\n    plt.xticks([])\n    plt.yticks([])\n\n    original_images.append(image)\n    images.append(preprocess(image))","edd70503":"texts = ['Cute',\n         'Funny',\n         'Derp', # let's see if this works\n         'Small',\n         'Happy',\n         'Sad',\n         'Aggressive',\n         'Friendly',\n         'Old',\n         'Young',\n         'Love']","fbfb4831":"image_input = torch.tensor(np.stack(images)).cuda()\ntext_tokens = clip.tokenize(texts).cuda()\n\n# text_tokens = clip.tokenize([f\"A {w} photo of a\" + w for w in texts]).cuda()","643d8789":"with torch.no_grad():\n    image_features = model.encode_image(image_input).float()\n    text_features = model.encode_text(text_tokens).float()\n\nimage_features \/= image_features.norm(dim=-1, keepdim=True)\ntext_features \/= text_features.norm(dim=-1, keepdim=True)\n\nsimilarity_matrix = torch.inner(text_features, image_features).cpu()","e3dffed9":"count = len(texts)\n\nplt.figure(figsize=(20, 16))\nplt.imshow(similarity_matrix, vmin=0.1, vmax=0.3, cmap = 'RdBu')\n\nplt.yticks(range(count), texts, fontsize=18)\nplt.xticks([])\n\nfor i, image in enumerate(original_images):\n    plt.imshow(image, extent=(i - 0.5, i + 0.5, -1.6, -0.6), origin=\"lower\")\nfor x in range(similarity_matrix.shape[1]):\n    for y in range(similarity_matrix.shape[0]):\n        plt.text(x, y, f\"{similarity_matrix[y, x]:.2f}\", ha=\"center\", va=\"center\", size=12)\n\nfor side in [\"left\", \"top\", \"right\", \"bottom\"]:\n    plt.gca().spines[side].set_visible(False)\n\nplt.xlim([-0.5, count - 0.5])\nplt.ylim([count + 0.5, -2])\n\nplt.title(\"Cosine similarity matrix between text and image features\", size=20, loc='left')\nplt.show()","43763b7a":"class PetDataset(Dataset):\n    def __init__(self, path):\n        self.path = path\n        self.files = [f for f in path.iterdir() if f.suffix == \".jpg\"]\n\n    def __len__(self):\n        return len(self.files)\n\n    def __getitem__(self, idx):\n        _im_path = self.files[idx]\n        _img = Image.open(_im_path)\n        _img = preprocess(_img)\n        return _img, _im_path.name.split('.')[0]","62e20361":"def create_similarity_features(dl):\n    features = []\n    names = []\n    with torch.no_grad():\n        for xb, name in dl:\n            xb = xb.cuda()\n            xb = model.encode_image(xb)\n            xb \/= xb.norm(dim=-1, keepdim=True)\n            sim_matrix = torch.inner(text_features, xb.float()).cpu().numpy()\n            features.append(sim_matrix)\n            names.append(name)\n    return features, names","66498435":"ds = PetDataset(train_image_path)\ndl = DataLoader(ds, batch_size = 400, shuffle=False)\ntrain_features, train_names = create_similarity_features(dl)","a2a5dd97":"train_features_df = pd.DataFrame(np.hstack(train_features).T,\n                           index = np.hstack(train_names).T,\n                           columns = texts)\ndf_corr = train_features_df.corr()\nplt.figure(figsize=(13,8))\n\nplt.title(\"Correlation matrix between engineered features\", size=20, loc='left') \nsns.heatmap(df_corr, cmap='RdBu', annot=True, linewidths=2)","67fa6ecc":"# saving training features for later use\ntrain_features_df.to_csv('clip_features.csv')","4b4fd444":"from sklearn.metrics import mean_squared_error\nfrom xgboost import XGBRegressor\nimport optuna\nfrom functools import partial","ad28430f":"def run(trial, fold, df, useful_features):\n    learning_rate = trial.suggest_float(\"learning_rate\", 1e-2, 0.25, log=True)\n    reg_lambda = trial.suggest_loguniform(\"reg_lambda\", 1e-8, 100.0)\n    reg_alpha = trial.suggest_loguniform(\"reg_alpha\", 1e-8, 100.0)\n    subsample = trial.suggest_float(\"subsample\", 0.1, 1.0)\n    colsample_bytree = trial.suggest_float(\"colsample_bytree\", 0.1, 1.0)\n    max_depth = trial.suggest_int(\"max_depth\", 1, 7)\n    \n    xtrain = df[df.kfold != fold].reset_index(drop=True)\n    xvalid = df[df.kfold == fold].reset_index(drop=True)\n\n    ytrain = xtrain.Pawpularity\n    yvalid = xvalid.Pawpularity\n\n    xtrain = xtrain[useful_features]\n    xvalid = xvalid[useful_features]\n\n    model = XGBRegressor(\n        random_state=42,\n        tree_method=\"gpu_hist\",\n        gpu_id=1,\n        n_estimators=10000,\n        predictor=\"gpu_predictor\",\n        learning_rate=learning_rate,\n        reg_lambda=reg_lambda,\n        reg_alpha=reg_alpha,\n        subsample=subsample,\n        colsample_bytree=colsample_bytree,\n        max_depth=max_depth,\n    )\n    model.fit(xtrain, ytrain, early_stopping_rounds=300, eval_set=[(xvalid, yvalid)], verbose=1000)\n    preds_valid = model.predict(xvalid)\n    rmse = mean_squared_error(yvalid, preds_valid, squared=False)\n    return rmse","ab650304":"train_df = pd.read_csv(\"..\/input\/same-old-creating-folds\/train_10folds.csv\")\ntest_df = pd.read_csv(\"..\/input\/petfinder-pawpularity-score\/test.csv\")\nsample_submission = pd.read_csv(\"..\/input\/petfinder-pawpularity-score\/sample_submission.csv\")\ntest_image_path = train_image_path = Path(\"..\/input\/petfinder-pawpularity-score\/test\")\n\nuseful_features = [\n    'Subject Focus', 'Eyes', 'Face', 'Near', 'Action', 'Accessory',\n    'Group', 'Collage', 'Human', 'Occlusion', 'Info', 'Blur',\n    'Cute', 'Funny', 'Derp', 'Small', 'Happy', 'Sad', 'Aggressive',\n    'Friendly', 'Old', 'Young', 'Love']","4a21940e":"ds = PetDataset(test_image_path)\ndl = DataLoader(ds, batch_size = 400, shuffle=False)\ntest_features, test_names = create_similarity_features(dl)\n\ntest_features_df = pd.DataFrame(np.hstack(test_features).T,\n                                index = np.hstack(test_names).T,\n                                columns = texts)","fd37383e":"train_df = train_df.join(train_features_df, on = 'Id')\ntest_df = test_df.join(test_features_df, on = 'Id')","3ec0e835":"opt_fun = partial(\n    run,\n    fold=0,\n    df=train_df,\n    useful_features=useful_features,\n)\n\nstudy = optuna.create_study(direction=\"minimize\")\nstudy.optimize(opt_fun, n_trials=200)\nprint(study.best_params)","0887ec30":"study.best_value, study.best_params","b4204f44":"def generate_predictions(params, fold, df, df_test, useful_features):    \n    xtrain = df[df.kfold != fold].reset_index(drop=True)\n    xvalid = df[df.kfold == fold].reset_index(drop=True)\n    xtest = df_test.copy()\n\n    ytrain = xtrain.Pawpularity\n    yvalid = xvalid.Pawpularity\n\n    xtrain = xtrain[useful_features]\n    xvalid = xvalid[useful_features]\n    xtest = xtest[useful_features]\n\n    model = XGBRegressor(\n        random_state=42,\n        tree_method=\"gpu_hist\",\n        gpu_id=1,\n        n_estimators=10000,\n        predictor=\"gpu_predictor\",\n        **params,\n    )\n    model.fit(xtrain, ytrain, early_stopping_rounds=300, eval_set=[(xvalid, yvalid)], verbose=1000)\n    preds_valid = model.predict(xvalid)\n    test_preds = model.predict(xtest)\n    rmse = mean_squared_error(yvalid, preds_valid, squared=False)\n    print(rmse)\n    return test_preds","25b52aee":"final_predictions = []\nfor fold_ in range(10):\n    final_predictions.append(\n        generate_predictions(\n            study.best_params,\n            fold=fold_,\n            df=train_df,\n            df_test=test_df,\n            useful_features=useful_features,\n        )\n    )","23365597":"final_predictions = np.mean(np.column_stack(final_predictions), axis=1)\nsample_submission.Pawpularity = final_predictions\nsample_submission.to_csv(\"submission.csv\", index=False)","6550e1c3":"# Visualising the Data","2f15a4df":"# Introduction\nMy first thought in this competition was that **cuteness** would be a big deal in predicting animal popularity  \nThis notebook explores using CLIP's multi-modal representations (and understanding of abstract concepts) for feature engineering","2210e642":"# K-Fold Training and Search\nXGB training and hyperparameter search code come from Abhishek Thakur's notebook:  \nhttps:\/\/www.kaggle.com\/abhishek\/optuna-xgboost-meta-features-only","be68bd34":"# Labelling Training Set","20cdbc62":"---","dc2dafcd":"# Prompt Feature Engineering\n[Multimodal Neurons in Artificial Neural Networks](https:\/\/openai.com\/blog\/multimodal-neurons\/) shows that CLIP can respond to abstract concepts, such as emotions or geographical regions. With some priors around what makes animals popular, this could create some interesting features.  \n\nIt's possible information around concepts could be extracted from an image, with well-defined prompts  \nWe use the cosine similarity between language and image embeddings to extract features for modelling here"}}