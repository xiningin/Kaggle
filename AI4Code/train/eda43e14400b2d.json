{"cell_type":{"f63bb7fa":"code","9af257a5":"code","662a24b0":"code","abc4a41e":"code","40fecdb0":"code","81bffb16":"code","e4ecb961":"code","b61251ac":"code","8bb25cac":"code","ca53f58b":"code","6664a31c":"code","e63e89c0":"code","ae261043":"markdown","18713f10":"markdown","b62f8368":"markdown","1d052390":"markdown","04b2b4b9":"markdown","7d43c14b":"markdown"},"source":{"f63bb7fa":"import pandas as pd\nimport numpy as np\nimport transformers\nimport torch\nfrom torch import nn\nfrom transformers import DistilBertTokenizer, DistilBertModel\nfrom sklearn import model_selection\nfrom tqdm import tqdm","9af257a5":"class Config:\n    MAX_LEN=512\n    TRAIN_BATCH_SIZE=16\n    VALID_BATCH_SIZE=16\n    TOKENIZER = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n    CSV_PATH = '..\/input\/complete-tweet-sentiment-extraction-data\/tweet_dataset.csv'\n    EPOCHS = 5","662a24b0":"class CommentModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.bert = DistilBertModel.from_pretrained('distilbert-base-uncased')\n        self.dropouts = nn.ModuleList([nn.Dropout(0.15) for _ in range(5)])\n        self.l1 = nn.Linear(768,3) # for sentiment\n        self.l2 = nn.Linear(768,13) # for emotion\n    def forward(self,ids,mask):\n        x = self.bert(input_ids=ids,attention_mask=mask)[0]\n        for i,dropout in enumerate(self.dropouts):\n            if i == 0:\n                out_sum = dropout(x)\n            else:\n                out_sum += dropout(x)\n        out = out_sum\/len(self.dropouts)\n        out = torch.mean(out,dim=1)\n        sentiment = self.l1(out)\n        emotion = self.l2(out)\n        return sentiment,emotion","abc4a41e":"class TweetDataset:\n    \"\"\"\n    Dataset which stores the tweets and returns them as processed features\n    \"\"\"\n    def __init__(self, tweet,emotion_label,sentiment_label):\n        self.tweet = tweet\n        self.emotion_label = emotion_label\n        self.sentiment_label = sentiment_label\n        self.tokenizer = Config.TOKENIZER\n        self.max_len = Config.MAX_LEN\n    \n    def __len__(self):\n        return len(self.tweet)\n\n    def __getitem__(self, item):\n        tweet = str(self.tweet[item]).strip()\n        tweet = \" \".join(tweet.split())\n\n        inputs = self.tokenizer.encode_plus(\n            tweet,\n            None,\n            add_special_tokens=True,\n            max_length=self.max_len,\n            pad_to_max_length=True,\n        )\n\n        ids = inputs[\"input_ids\"]\n        mask = inputs[\"attention_mask\"]\n\n        return {\n            'ids': torch.tensor(ids),\n            'mask':torch.tensor(mask),\n            'emotion_label':torch.tensor(self.emotion_label[item]),\n            'sentiment_label':torch.tensor(self.sentiment_label[item])\n        }","40fecdb0":"class AverageMeter:\n    \"\"\"\n    Computes and stores the average and current value\n    \"\"\"\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum \/ self.count\n\nclass EarlyStopping:\n    def __init__(self, patience=7, mode=\"max\", delta=0.001):\n        self.patience = patience\n        self.counter = 0\n        self.mode = mode\n        self.best_score = None\n        self.early_stop = False\n        self.delta = delta\n        if self.mode == \"min\":\n            self.val_score = np.Inf\n        else:\n            self.val_score = -np.Inf\n\n    def __call__(self, epoch_score, model, model_path):\n\n        if self.mode == \"min\":\n            score = -1.0 * epoch_score\n        else:\n            score = np.copy(epoch_score)\n\n        if self.best_score is None:\n            self.best_score = score\n            self.save_checkpoint(epoch_score, model, model_path)\n        elif score < self.best_score + self.delta:\n            self.counter += 1\n            print('EarlyStopping counter: {} out of {}'.format(self.counter, self.patience))\n            if self.counter >= self.patience:\n                self.early_stop = True\n        else:\n            self.best_score = score\n            self.save_checkpoint(epoch_score, model, model_path)\n            self.counter = 0\n\n    def save_checkpoint(self, epoch_score, model, model_path):\n        if epoch_score not in [-np.inf, np.inf, -np.nan, np.nan]:\n            print('Validation score improved ({} --> {}). Saving model!'.format(self.val_score, epoch_score))\n            torch.save(model.state_dict(), model_path)\n        self.val_score = epoch_score","81bffb16":"def loss_fn(pred_sentiment,sentiment,pred_emotion,emotion):\n    loss_fct = nn.CrossEntropyLoss()\n    loss1 = loss_fct(pred_sentiment,sentiment)\n    loss2 = loss_fct(pred_emotion,emotion)\n    return loss1 + loss2","e4ecb961":"def train_fn(data_loader, model, optimizer, device, scheduler=None):\n    model.train()\n    losses = AverageMeter()\n    accuracy1 = AverageMeter()\n    accuracy2 = AverageMeter()\n\n    tk0 = tqdm(data_loader, total=len(data_loader))\n    \n    for bi, d in enumerate(tk0):\n\n        ids = d[\"ids\"]\n        mask = d[\"mask\"]\n        sentiment = d[\"sentiment_label\"]\n        emotion = d[\"emotion_label\"]\n\n        ids = ids.to(device, dtype=torch.long)\n        mask = mask.to(device, dtype=torch.long)\n        sentiment = sentiment.to(device,dtype=torch.long)\n        emotion = emotion.to(device,dtype=torch.long)\n\n        model.zero_grad()\n        pred_sentiment,pred_emotion= model(\n            ids=ids,\n            mask=mask,\n        )\n        loss = loss_fn(pred_sentiment,sentiment,pred_emotion,emotion)\n        loss.backward()\n        optimizer.step()\n        scheduler.step()\n        \n        pred_sentiment = torch.argmax(torch.softmax(pred_sentiment,dim=-1),dim=-1)\n        pred_emotion = torch.argmax(torch.softmax(pred_emotion,dim=-1),dim=-1)\n        \n        s_accuracy = (pred_sentiment == sentiment).float().mean()\n        e_accuracy = (pred_emotion == emotion).float().mean()\n    \n        losses.update(loss.item(), ids.size(0))\n        accuracy1.update(s_accuracy.item(),ids.size(0))\n        accuracy2.update(e_accuracy.item(),ids.size(0))\n        \n        tk0.set_postfix(loss=losses.avg,sentiment_accuracy=accuracy1.avg,emotion_accuracy=accuracy2.avg)","b61251ac":"def eval_fn(data_loader, model, device):\n    model.eval()\n    losses = AverageMeter()\n    accuracy1 = AverageMeter()\n    accuracy2 = AverageMeter()\n\n    tk0 = tqdm(data_loader, total=len(data_loader))\n    with torch.no_grad():\n        for bi, d in enumerate(tk0):\n\n            ids = d[\"ids\"]\n            mask = d[\"mask\"]\n            sentiment = d[\"sentiment_label\"]\n            emotion = d[\"emotion_label\"]\n\n            ids = ids.to(device, dtype=torch.long)\n            mask = mask.to(device, dtype=torch.long)\n            sentiment = sentiment.to(device,dtype=torch.long)\n            emotion = emotion.to(device,dtype=torch.long)\n\n            pred_sentiment,pred_emotion= model(\n                ids=ids,\n                mask=mask,\n            )\n            loss = loss_fn(pred_sentiment,sentiment,pred_emotion,emotion)\n\n            pred_sentiment = torch.argmax(torch.softmax(pred_sentiment,dim=-1),dim=-1)\n            pred_emotion = torch.argmax(torch.softmax(pred_emotion,dim=-1),dim=-1)\n\n            s_accuracy = (pred_sentiment == sentiment).float().mean()\n            e_accuracy = (pred_emotion == emotion).float().mean()\n\n            losses.update(loss.item(), ids.size(0))\n            accuracy1.update(s_accuracy.item(),ids.size(0))\n            accuracy2.update(e_accuracy.item(),ids.size(0))\n\n            tk0.set_postfix(loss=losses.avg,sentiment_accuracy=accuracy1.avg,emotion_accuracy=accuracy2.avg)\n    print(f'sentiment:{accuracy1.avg} emotion:{accuracy2.avg}')\n    return (accuracy1.avg + accuracy2.avg)\/2","8bb25cac":"def deEmojify(inputString):\n    return inputString.encode('ascii', 'ignore').decode('ascii')\n\nemotion_dict = {'anger':0,'boredom':1,'empty':2,'enthusiasm':3,'fun':4,\n                'happiness':5,'hate':6,'love':7,'neutral':8,'relief':9,\n                'sadness':10,'surprise':11,'worry':12}\n\nsentiment_dict = {'negative':0,'neutral':1,'positive':2}","ca53f58b":"df = pd.read_csv(Config.CSV_PATH)\ndf = df.dropna(subset=['text','new_sentiment','sentiment'])\ndf['text'] = df['text'].apply(deEmojify)\ndf = df[df['text']!='']\ndf = df.sample(frac=1).reset_index(drop=True)\ndf['emotion_label'] = df['sentiment'].apply(lambda x : emotion_dict[x])\ndf['sentiment_label'] = df['new_sentiment'].apply(lambda x :sentiment_dict[x])\nkf = model_selection.StratifiedKFold(n_splits=5)\nfor fold, (trn_, val_) in enumerate(kf.split(X=df, y=df.sentiment.values)):\n    print(len(trn_), len(val_))\n    df.loc[val_, 'kfold'] = fold\n","6664a31c":"df.head()","e63e89c0":"fold = 0\ndf_train = df[df.kfold != fold].reset_index(drop=True)\ndf_valid = df[df.kfold == fold].reset_index(drop=True)\n\ntrain_dataset = TweetDataset(\n    tweet=df_train.text.values,\n    sentiment_label=df_train.sentiment_label.values,\n    emotion_label=df_train.emotion_label.values\n)\n\nvalid_dataset = TweetDataset(\n    tweet=df_valid.text.values,\n    sentiment_label=df_valid.sentiment_label.values,\n    emotion_label=df_valid.emotion_label.values\n)\n\ntrain_data_loader = torch.utils.data.DataLoader(\n    train_dataset,\n    batch_size=Config.TRAIN_BATCH_SIZE,\n    num_workers=4,\n    shuffle=True\n)\n\nvalid_data_loader = torch.utils.data.DataLoader(\n    valid_dataset,\n    batch_size=Config.VALID_BATCH_SIZE,\n    num_workers=2\n)\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nmodel = CommentModel()\nmodel.to(device)\n\nnum_train_steps = int(len(df_train) \/ Config.TRAIN_BATCH_SIZE * Config.EPOCHS)\nparam_optimizer = list(model.named_parameters())\nno_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\noptimizer_parameters = [\n    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.001},\n    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0},\n]\noptimizer = transformers.AdamW(optimizer_parameters, lr=6e-5)\nscheduler = transformers.get_linear_schedule_with_warmup(\n    optimizer, \n    num_warmup_steps=200, \n    num_training_steps=num_train_steps\n)\n\nes = EarlyStopping(patience=2, mode=\"max\")\nprint(f\"Training is Starting for fold={fold}\")\n\n# I'm training only for 3 epochs even though I specified 5!!!\nfor epoch in range(Config.EPOCHS):\n    train_fn(train_data_loader, model, optimizer, device, scheduler=scheduler)\n    total_accuracy = eval_fn(valid_data_loader, model, device)\n    print(f\"Total accuracy = {total_accuracy}\")\n    es(total_accuracy, model, model_path=f\"model.bin\")\n    if es.early_stop:\n        print(\"Early stopping\")\n        break","ae261043":"# Import stuff","18713f10":"# Dataset","b62f8368":"# Train function","1d052390":"# Clean data","04b2b4b9":"# Configurations","7d43c14b":"# Evaluate Function"}}