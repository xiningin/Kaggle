{"cell_type":{"abc06af6":"code","68a36143":"code","a8077539":"code","19e5e83e":"code","057a2e1a":"code","38f3208c":"code","a8d13c34":"code","a75528ab":"code","2efbcc86":"code","15e15244":"code","2cb0489b":"code","13860abc":"code","c1bc024e":"code","4166f987":"code","5453a8f7":"code","b36decc5":"code","0de5c570":"code","33db1dab":"code","b0cc3c0f":"code","c3254ec3":"code","a37e8ed9":"code","ed7b40d4":"code","cf9377e5":"code","f3c28ad5":"code","f24af289":"code","57f5bc34":"code","e5b89fc5":"code","cdf88848":"code","ce6761db":"code","bdd5075d":"code","7408eb54":"code","2a3dfa0a":"code","adf963a7":"code","e5804b5e":"code","576ed225":"code","085a390c":"code","032d7217":"code","54728210":"code","8fc56457":"code","58853d91":"code","bbfe45ac":"code","9a44ea89":"code","893819df":"code","73e529de":"code","059f1a21":"code","4b243dd2":"code","2611575d":"code","d7d48640":"code","b3ef11f1":"code","83849eeb":"code","989a2323":"code","e547072b":"code","c5ceb4be":"code","219c8efa":"code","62053d98":"code","24d8187c":"code","6584d128":"code","c3bd738e":"code","78649049":"code","5255410f":"code","061e19ef":"code","835050aa":"code","fd250e0f":"code","dbe7d736":"code","5af031d1":"code","8978455a":"code","42d61c80":"code","d0e30683":"code","df029023":"code","1244ca5e":"code","d4d1aa4a":"code","a07df97e":"code","6a84b0c3":"code","235ce60e":"code","e7d99387":"code","ab6736ac":"code","f4ac3025":"code","d8321e26":"code","76c2564a":"code","52b567e9":"code","6ad37a62":"code","cb408eb0":"code","d5d5d57b":"code","d91c2776":"code","5c637468":"code","c52a936a":"code","5bf309d9":"code","d57d1871":"code","149c2105":"code","18e947d8":"code","36ee2589":"code","4c8c8f45":"code","e86f1656":"code","017efcd5":"markdown","cd0df3d9":"markdown","4b27df42":"markdown","5b71c2c9":"markdown","83649479":"markdown","38755e15":"markdown","54fa0790":"markdown","928333e3":"markdown","b4a56c52":"markdown","6a77444a":"markdown","629f1a6a":"markdown","11e87791":"markdown","ce654cb0":"markdown","1b018117":"markdown","0687413e":"markdown","d9e1aca1":"markdown","1a067281":"markdown","3347301a":"markdown","3c0a86b8":"markdown","75032080":"markdown","c62340d8":"markdown","e4cb7d46":"markdown","a58fb98c":"markdown","e0c30c23":"markdown","09f73cb2":"markdown","5c01b53f":"markdown","345bf8c4":"markdown","ab6eaadb":"markdown","49796acd":"markdown","fb8c8f71":"markdown","e450cd41":"markdown","27d6fc80":"markdown","2ea74092":"markdown","f33a39ac":"markdown","4efd9faa":"markdown","dcb4266c":"markdown","82fe1340":"markdown","25a39d38":"markdown","86e05525":"markdown","3e9b33c7":"markdown","006eced6":"markdown","02917e30":"markdown","90f6d2d0":"markdown","94813233":"markdown","744ece3d":"markdown","c9e3728d":"markdown","ee46d7ef":"markdown","0e7705a4":"markdown","3ebe57e8":"markdown","4b2bf552":"markdown"},"source":{"abc06af6":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom imblearn.over_sampling import RandomOverSampler\nsns.set(style='whitegrid')","68a36143":"train=pd.read_csv('..\/input\/health-insurance-cross-sell-prediction\/train.csv')\ntest=pd.read_csv('..\/input\/health-insurance-cross-sell-prediction\/test.csv')\nprint(\"Training data shape: \", train.shape) # 381109 rows, 12 columns\nprint(\"Test data shape: \", test.shape)   # 127037 rows, 11 columns (missing the response column deliberately)","a8077539":"train.head() # starts at id #1","19e5e83e":"test.head() # starts at id #381110","057a2e1a":"train.isnull().sum() # no null values","38f3208c":"train.dtypes","a8d13c34":"for column in ['Region_Code','Policy_Sales_Channel']:\n    train[column] = train[column].astype('int')\n    test[column] = test[column].astype('int')","a75528ab":"numerical_columns=['Age', 'Region_Code','Annual_Premium','Vintage']\ncategorical_columns=['Gender','Driving_License','Previously_Insured','Vehicle_Age','Vehicle_Damage','Response']\ntrain[numerical_columns].describe()","2efbcc86":"for category in categorical_columns:\n    print(train[category].value_counts(), '\\n______________________\\n')","15e15244":"sns.countplot(train.Response)","2cb0489b":"train.Response.value_counts()","13860abc":"train.Response.value_counts()[1]\/(train.Response.value_counts()[1]+train.Response.value_counts()[0])","c1bc024e":"print(\"Total age distribution:\\n\\n\",\n      train.Age.describe(),\n      \"\\n_______________________\\n\\n\",\n      \"Age distribution where Response = 1:\\n\\n\",\n      train.Age.loc[train.Response == 1].describe())","4166f987":"sns.set(rc={'figure.figsize':(11.7,8.27)})\nsns.distplot(train.Age, label = \"Total customers\", bins=65)\nsns.distplot(train.Age.loc[train.Response == 1], label = \"Customers who purchased vehicle insurance\", bins=64)\nplt.legend()","5453a8f7":"# There are 22 people over the age of 83, and none of them wanted vehicle insurance.  Maybe it would be too expensive for them.\n# The model will likely predict a response of 0 for the very old as well as the very young.\ntrain.loc[train.Age >83]","b36decc5":"sns.distplot(train.Annual_Premium, label = \"Total customers\")\nsns.distplot(train.Annual_Premium.loc[train.Response == 1], label = \"Customers who purchased vehicle insurance\")\nplt.legend()","0de5c570":"sns.scatterplot(x=train['Age'],y=train['Annual_Premium']) # There does not appear to be much correlation between age and annual premium","33db1dab":"df=train.groupby(['Gender','Response'])['id'].count().to_frame().rename(columns={'id':'count'}).reset_index()\ng = sns.catplot(x=\"Gender\", y=\"count\",col=\"Response\",\n                data=df, kind=\"bar\",\n                height=4, aspect=.7);","b0cc3c0f":"df=train.groupby(['Previously_Insured','Response'])['id'].count().to_frame().rename(columns={'id':'count'}).reset_index()\ng = sns.catplot(x=\"Previously_Insured\", y=\"count\",col=\"Response\",\n                data=df, kind=\"bar\",\n                height=4, aspect=.7);","c3254ec3":"df","a37e8ed9":"train.groupby(['Previously_Insured','Response'])['id'].count()","ed7b40d4":"df=train.groupby(['Vehicle_Age','Response'])['id'].count().to_frame().rename(columns={'id':'count'}).reset_index()\ndf","cf9377e5":"g = sns.catplot(x=\"Vehicle_Age\", y=\"count\",col=\"Response\",\n                data=df, kind=\"bar\",\n                height=4, aspect=.7);","f3c28ad5":"df=train.groupby(['Vehicle_Damage','Response'])['id'].count().to_frame().rename(columns={'id':'count'}).reset_index()\ndf","f24af289":"df2=pd.DataFrame({'total': train['Vehicle_Damage'].value_counts(), 'Response=1':train.loc[train['Response'] == 1,'Vehicle_Damage'].value_counts()})\ndf2['Response Rate'] = df2['Response=1']\/df2['total']\ndf2","57f5bc34":"g = sns.catplot(x=\"Vehicle_Damage\", y=\"count\",col=\"Response\",\n                data=df, kind=\"bar\",\n                height=4, aspect=.7);","e5b89fc5":"sns.distplot(train.Vintage, label = \"Total customers\", bins = 30)\nsns.distplot(train.Vintage.loc[train.Response == 1], label = \"Customers who purchased vehicle insurance\", bins=30)\nplt.legend() # the slight sawtooth pattern is just a result of binning, nothing else","cdf88848":"df=pd.DataFrame({'total': train['Region_Code'].value_counts(), 'Response=1':train.loc[train['Response'] == 1,'Region_Code'].value_counts()})\ndf['Response Rate'] = df['Response=1']\/df['total']\ndf.sort_values('Response Rate')\n#As we can see, different regions have very different response rates, ranging from about 4% to 19%.  Those regions however were smaller samples than other ones, and may be outliers.","ce6761db":"df=pd.DataFrame({'total': train['Policy_Sales_Channel'].value_counts(), 'Response=1':train.loc[train['Response'] == 1,'Policy_Sales_Channel'].value_counts()})\ndf['Response Rate'] = df['Response=1']\/df['total']\ndf.sort_values('Response Rate')","bdd5075d":"df.loc[df['Response Rate'].isnull(),:]","7408eb54":"df.sort_values('Response Rate').dropna()\n# Response rates by sales channel vary from 2 to 100%, though some of these are outliers due to small samples.  Channel 152 is noteworthy because it has almost\n# 135,000 samples, and its response rate is only 2.86%.","2a3dfa0a":"df.sort_values('total', ascending = False).dropna().iloc[:20] # Take the 20 top sales channels by number of customers.  We don't care about the puny ones.\n# The top 5 channels make up about 300,000 of 382,000 total customers, and they vary widely in response rate from about 2% to 20%.  Clearly this is important information","adf963a7":"num_feat = ['Age','Vintage']\ncat_feat = ['Gender', 'Driving_License', 'Previously_Insured', 'Vehicle_Age_lt_1_Year','Vehicle_Age_gt_2_Years','Vehicle_Damage_Yes','Region_Code','Policy_Sales_Channel']\ntrain['Gender'] = train['Gender'].map( {'Female': 0, 'Male': 1} ).astype(int)\ntrain=pd.get_dummies(train,drop_first=True)","e5804b5e":"train=train.rename(columns={\"Vehicle_Age_< 1 Year\": \"Vehicle_Age_lt_1_Year\", \"Vehicle_Age_> 2 Years\": \"Vehicle_Age_gt_2_Years\"})\ntrain['Vehicle_Age_lt_1_Year']=train['Vehicle_Age_lt_1_Year'].astype('int')\ntrain['Vehicle_Age_gt_2_Years']=train['Vehicle_Age_gt_2_Years'].astype('int')\ntrain['Vehicle_Damage_Yes']=train['Vehicle_Damage_Yes'].astype('int')","576ed225":"from sklearn.preprocessing import LabelEncoder, StandardScaler, MinMaxScaler, RobustScaler\nss = StandardScaler()\ntrain[num_feat] = ss.fit_transform(train[num_feat])\n\n\nmm = MinMaxScaler()\ntrain[['Annual_Premium']] = mm.fit_transform(train[['Annual_Premium']])","085a390c":"train=train.drop('id',axis=1) # drop the id column\n\nfor column in cat_feat:\n    train[column] = train[column].astype('str')\n    \ntest['Gender'] = test['Gender'].map( {'Female': 0, 'Male': 1} ).astype(int)\ntest=pd.get_dummies(test,drop_first=True)\ntest=test.rename(columns={\"Vehicle_Age_< 1 Year\": \"Vehicle_Age_lt_1_Year\", \"Vehicle_Age_> 2 Years\": \"Vehicle_Age_gt_2_Years\"})\ntest['Vehicle_Age_lt_1_Year']=test['Vehicle_Age_lt_1_Year'].astype('int')\ntest['Vehicle_Age_gt_2_Years']=test['Vehicle_Age_gt_2_Years'].astype('int')\ntest['Vehicle_Damage_Yes']=test['Vehicle_Damage_Yes'].astype('int')","032d7217":"from sklearn.preprocessing import LabelEncoder, StandardScaler, MinMaxScaler, RobustScaler\nss = StandardScaler()\ntest[num_feat] = ss.fit_transform(test[num_feat])\n\n\nmm = MinMaxScaler()\ntest[['Annual_Premium']] = mm.fit_transform(test[['Annual_Premium']])","54728210":"for column in cat_feat:\n    test[column] = test[column].astype('str')","8fc56457":"from sklearn.model_selection import train_test_split\n\ntrain_target=train['Response']\ntrain=train.drop(['Response'], axis = 1)\nx_train,x_test,y_train,y_test = train_test_split(train,train_target, random_state = 0)","58853d91":"id=test.id\ntest=test.drop('id',axis=1)","bbfe45ac":"from sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.ensemble import RandomForestClassifier\n#from catboost import CatBoostClassifier\n#from scipy.stats import randint\nimport pickle\n#import xgboost as xgb\n#import lightgbm as lgb\nfrom sklearn.metrics import accuracy_score\n# import packages for hyperparameters tuning\nfrom hyperopt import STATUS_OK, Trials, fmin, hp, tpe\nfrom sklearn.model_selection import train_test_split, RandomizedSearchCV, StratifiedKFold, KFold, GridSearchCV\nfrom sklearn.metrics import f1_score, roc_auc_score,accuracy_score,confusion_matrix, precision_recall_curve, auc, roc_curve, recall_score, classification_report ","9a44ea89":"%pylab inline","893819df":"x_train.dtypes","73e529de":"random_search = {'criterion': ['entropy', 'gini'],\n               'max_depth': [2,3,4,5,6,7,10],\n               'min_samples_leaf': [4, 6, 8],\n               'min_samples_split': [5, 7,10],\n               'n_estimators': [300]}\n\nclf = RandomForestClassifier()\nmodel = RandomizedSearchCV(estimator = clf, param_distributions = random_search, n_iter = 10, \n                               cv = 4, verbose= 1, random_state= 101, n_jobs = -1)\nmodel.fit(x_train,y_train)","059f1a21":"filename = 'rf_model.sav'\npickle.dump(model, open(filename, 'wb'))","4b243dd2":"rf_load = pickle.load(open(filename, 'rb'))","2611575d":"rf_load","d7d48640":"y_pred=model.predict(x_test)","b3ef11f1":"y_pred[:300] # this model appears to predict 0 for everything","83849eeb":"np.unique(y_pred, return_counts=True) # Yes, it predicts 0 for everything in this case (your result may be a bit different)","989a2323":"list(y_pred).count(0)","e547072b":"np.sort(model.predict_proba(x_test)[:,1]) # this function yields a 2d array with the probability of \"0\" in the 0th column and of \"1\" in the 1st column. Hence the [:,1]","c5ceb4be":"sns.distplot(model.predict_proba(x_test)[:,1])","219c8efa":"print (classification_report(y_test, y_pred)) # By predicting 0 for everybody, the model achieves about 88% accuracy","62053d98":"y_score = model.predict_proba(x_test)[:,1]\nfpr, tpr, _ = roc_curve(y_test, y_score)  \n\ntitle('Random Forest ROC curve: Insurance Purchse')\nxlabel('FPR (Precision)') # false positive rate\nylabel('TPR (Recall)')   # true positive rate\n\nplot(fpr,tpr)\nplot((0,1), ls='dashed',color='black')\nplt.show()\nprint ('Area under curve (AUC): ', auc(fpr,tpr)) # about 0.855","24d8187c":"sns.distplot(y_score) # same plot from earlier","6584d128":"score = roc_auc_score(y_test, y_score)\nprint(\"auc-roc score on Test data\",score) # 0.855","c3bd738e":"train=pd.read_csv('..\/input\/health-insurance-cross-sell-prediction\/train.csv')\ntest=pd.read_csv('..\/input\/health-insurance-cross-sell-prediction\/test.csv')\n\ntrain['Gender'] = train['Gender'].map( {'Female': 0, 'Male': 1} ).astype(int)\ntest['Gender'] = test['Gender'].map( {'Female': 0, 'Male': 1} ).astype(int)\n\nfor column in ['Region_Code','Policy_Sales_Channel']:\n    train[column] = train[column].astype('int')\n    test[column] = test[column].astype('int')\n    \nid=test.id  # capture id for later, test id only, for final submission\n    \ntrain=train.drop('id',axis=1) # drop the id column\ntest=test.drop('id',axis=1)\n\ncat_feat = ['Gender', 'Driving_License', 'Previously_Insured', 'Vehicle_Age', 'Vehicle_Damage',\n 'Region_Code', 'Policy_Sales_Channel']\n\nfor column in cat_feat:\n    train[column] = train[column].astype('str')\n\nfor column in cat_feat:\n    test[column] = test[column].astype('str')\n\ntrain['Age'] = train['Age']\/\/5 # divide all ages by 5 to get them into bins of 5 years each for the dummy variables\ntrain['Age'] = train['Age'].astype(str)\n\ntest['Age'] = test['Age']\/\/5\ntest['Age'] = test['Age'].astype(str)\n\ntrain['Annual_Premium'] = ((train['Annual_Premium'])\/\/1000)**0.5\/\/1 #bin the annual premium into about 20 bins, with smaller bin sizes for smaller amounts\ntrain['Annual_Premium'] = train['Annual_Premium'].astype(str)\ntest['Annual_Premium'] = ((test['Annual_Premium'])\/\/1000)**0.5\/\/1\ntest['Annual_Premium'] = test['Annual_Premium'].astype(str)\n\ntrain=pd.get_dummies(train,drop_first=True)\ntest=pd.get_dummies(test,drop_first=True)","78649049":"from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n\nmm = MinMaxScaler()\ntrain[['Vintage']] = mm.fit_transform(train[['Vintage']])\ntest[['Vintage']] = mm.fit_transform(test[['Vintage']])  # This simply reduces Vintage to smaller numbers.  They are not expected to make a difference to the model because the distribution is flat","5255410f":"print(train.shape,\n      test.shape) \n\n#(381109, 228)\n#(127037, 217)","061e19ef":"rejectColumns = []\ni = 0\nfor name in list(train.columns):\n    if name not in list(test.columns):\n        print(name)\n        rejectColumns.append(name)\nprint(i)\nprint(rejectColumns)","835050aa":"rejectColumns.remove('Response')\nrejectColumns","fd250e0f":"for name in train.columns:\n    #print(name)\n    if name in rejectColumns:\n        print(name)\n        train = train.drop(name,axis = 1)","dbe7d736":"i = 0\nrejectColumns = []\nfor name in list(test.columns):\n    if name not in list(train.columns):\n        print(name)\n        rejectColumns.append(name)\n        i += 1\nprint(i)\nprint(rejectColumns)","5af031d1":"for name in test.columns:\n    #print(name)\n    if name in rejectColumns:\n        print(name)\n        test = test.drop(name,axis = 1)","8978455a":"print(train.shape,\n      test.shape) \n\n#(381109, 216)\n#(127037, 215)","42d61c80":"from sklearn.model_selection import train_test_split\ntrain_target=train['Response']\ntrain=train.drop(['Response'], axis = 1)\nx_train,x_test,y_train,y_test = train_test_split(train,train_target, random_state = 0)","d0e30683":"y_test.value_counts()","df029023":"import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n\nprint(tf.__version__)\n","1244ca5e":"# Deep Learning Libraries\nfrom keras.layers import Dense\nfrom keras.models import Sequential\nfrom keras.callbacks import EarlyStopping\nfrom sklearn.utils import class_weight","d4d1aa4a":"y_train","a07df97e":"y_train2 = np.array([y_train,1-y_train])\ny_train2 = np.transpose(y_train2)\ny_train2\n\ny_test2 = np.array([y_test,1-y_test])\ny_test2 = np.transpose(y_test2)\ny_test2","6a84b0c3":"y_test2[:15]","235ce60e":"ratio = y_train.value_counts()[1]\/len(y_train)\nEPOCHS = 5\nBATCH_SIZE = 128\n\n#n_cols = predictors.shape[1]\nearly_stopping_monitor = EarlyStopping(patience=2)\nclass_weight = {0:1-ratio, 1:ratio-0.1}#{0:ratio, 1:1-ratio}\n\nmodel = Sequential()\nmodel.add(Dense(24, activation='relu', input_shape = (x_train.shape[1],)))\nmodel.add(Dense(20, activation='relu'))\nmodel.add(Dense(2, activation='softmax'))\n\nmodel.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])\nmodel.fit(x_train, y_train2, batch_size = BATCH_SIZE, epochs=EPOCHS, validation_split=0.1, #callbacks = [early_stopping_monitor],\n          class_weight=class_weight)\n# bs320 ep 100, ROC 0.845\n# bs32 ep 10, ROC 0.855\n# bs32 ep 10, val 0.2, ROC 0.8556\n# bs32 ep 10, val 0.1, ROC 0.8553\n# bs32 ep 10, val 0.3, ROC 0.8547\n# bs32 ep 10, val 0.2, ROC 0.8549\n# bs3200 ep 100, val 0.2, ROC 0.851\n# bs3200 ep 100, val 0.2, ROC 0.845\n# bs16 ep 3, val 0.2, ROC 0.8546\n# bs32 ep 3, val 0.2, ROC 0.8558\n# bs64 ep 3, val 0.2, ROC 0.8559\n# bs64 ep 3, val 0.2, ROC 0.8550\n# bs32 ep 3, val 0.2, ROC 0.8557\n# bs16 ep 3, val 0.2, ROC 0.8552\n# bs32 ep 3, val 0.2, ROC 0.8548\n# bs32 ep 5, val 0.2, ROC 0.8544\n# bs32 ep 5, val 0.2, ROC 0.8545\n# bs16 ep 5, val 0.2, ROC 0.8543\n# bs64 ep 5, val 0.2, ROC 0.8549\n# bs128 ep 5, val 0.2, ROC 0.8549\n# bs128 ep 5, val 0.1, ROC 0.8551\n# bs128 ep 5, val 0.3, ROC 0.8548\n# bs128 ep 5, val 0.05, ROC 0.8541\n\n","e7d99387":"# for use in Evaluation Notebook\n# model.save('.\/tfModelInsurance.h5') \n# x_train.to_csv('.\/x_train.csv')","ab6736ac":"model.get_weights() #6 \/ 234[24, 24, 24 ...], 24, 24[20, 20 ...], 20, 20[2, 2 ...], 2: [ 0.02806583, -0.02806598]","f4ac3025":"model.get_weights()[0][2]","d8321e26":"layer1wtsum = []\nfor i in range(234):\n    layer1wtsum.append(sum(model.get_weights()[0][i]))","76c2564a":"layer1wtsumdf = pd.DataFrame({'column': train.columns, 'weightsum': layer1wtsum})","52b567e9":"layer1wtsumdf[:20]","6ad37a62":"layer1wtsumdf.sort_values(by='weightsum')[-15:]","cb408eb0":"print(\"Evaluate on test data\")\nresults = model.evaluate(x_test, y_test2, batch_size=128)\nprint(\"test loss, test acc:\", results)\n\n# Generate predictions (probabilities -- the output of the last layer)\n# on new data using `predict`\nprint(\"Generate predictions for 3 samples\")\npredictions = model.predict(x_test[:3])\nprint(\"predictions shape:\\n\", predictions)","d5d5d57b":"y_test3 = y_test2\ny_testpred = model.predict(x_test)[:,0]\noffset = 0.1 #distance from 0.5 to serve as cutoff for prediction in probabilities\ny_test3[:,1] = np.around(y_testpred-offset)\n# 0.2 -> 23.8% positive, 0.15 -> 28.9%, 0.1 -> 32.8%, \ntp = 0\nfp = 0\nfn = 0\ntn = 0\n\nfor i in range(len(y_test3)):\n    if all(y_test3[i] == [1,1]):\n        tp += 1\n    elif all(y_test3[i] == [0,1]):\n        fp += 1\n    elif all(y_test3[i] == [1,0]):\n        fn += 1\n    elif all(y_test3[i] == [0,0]):\n        tn += 1\n        \nprint(tp, fp, fn, tn, tp+fp+fn+tn, len(y_test3))","d91c2776":"cm = np.array([[tp, fn],[fp, tn]]) # confusion matrix\ncm","5c637468":"y_test.value_counts()[1]\/len(y_test)","c52a936a":"(tp+fp)\/(len(y_test3)) # how many things are being predicted to be positive.  It appears to be too many positive predictions\n# This step is not strictly necessary.  The final submission will assign each test id a probability between 0 and 1, not a prediction of exactly 0 or 1\n# This was just an exercise","5bf309d9":"from sklearn.metrics import f1_score, roc_auc_score,accuracy_score,confusion_matrix, precision_recall_curve, auc, roc_curve, recall_score, classification_report \nimport matplotlib.pyplot as plt","d57d1871":"y_score = model.predict(x_test)[:,0]\nfpr, tpr, _ = roc_curve(y_test, y_score)\n\nplt.title('Dense Neural Net ROC curve')\nplt.xlabel('FPR (Precision)')\nplt.ylabel('TPR (Recall)')\n\nplt.plot(fpr,tpr)\nplt.plot((0,1), ls='dashed',color='black')\nplt.show()\nprint ('Area under curve (AUC): ', auc(fpr,tpr))\n# yields AUC of 0.855, very similar to random forest","149c2105":"train_target2 = np.array([train_target,1-train_target])\ntrain_target2 = np.transpose(train_target2)\ntrain_target2","18e947d8":"ratio = np.unique(train_target2[:,0], return_counts=True)[1][1]\/len(train_target2)\nEPOCHS = 10\nBATCH_SIZE = 128\n\n#n_cols = predictors.shape[1]\nearly_stopping_monitor = EarlyStopping(patience=2)\nclass_weight = {0:1-ratio, 1:ratio-0.1}#{0:ratio, 1:1-ratio}\n\nmodel = Sequential()\nmodel.add(Dense(25, activation='relu', input_shape = (train.shape[1],)))\nmodel.add(Dense(20, activation='relu'))\nmodel.add(Dense(2, activation='softmax'))\n\nmodel.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])\nmodel.fit(train, train_target2, batch_size = BATCH_SIZE, epochs=EPOCHS, validation_split=0.1, #callbacks = [early_stopping_monitor],\n          class_weight=class_weight)","36ee2589":"score = model.predict(test)[:,0]\nsns.distplot(score)","4c8c8f45":"y_score = model.predict(train)[:,0]\nfpr, tpr, _ = roc_curve(train_target, y_score)\n\nplt.title('Dense Neural Net ROC curve')\nplt.xlabel('FPR (Precision)')\nplt.ylabel('TPR (Recall)')\n\nplt.plot(fpr,tpr)\nplt.plot((0,1), ls='dashed',color='black')\nplt.show()\nprint ('Area under curve (AUC): ', auc(fpr,tpr))\n# The AUC is 0.862 for the training data.  The test AUC is expected to be 0.855 as it was for the previous ROC where the model had not seen the test data.  \n# The difference between 0.862 and 0.855 is small.  This is a good sign.  If overfitting had occurred, it would be much larger.","e86f1656":"submission = pd.DataFrame(data = {'id': id, 'Response': score})\nsubmission.to_csv('vehicle_insurance_tensorflow1.csv', index = False)\nsubmission.head()","017efcd5":"### Above we can see the list of columns present in training but not in test.  They are in a handful of policy sales channels and 2 extremely high annual premium bins.  They will be deleted.  It's also important to note that \"Response\" is included, but we definitely don't want to delete it because we will need those numbers.","cd0df3d9":"### This set of predictions will be evaluated on Kaggle after submission, and a ROC will be calculated and compared to that of other models.","4b27df42":"# Random Forest Classifier","5b71c2c9":"## ROC Curve & AUC of Random forest classifier\n* The ROC curve sorts all of the probabilities for each customer, then compares them to the customer's actual choices.\n* Ideally the AUC would be 1.00","83649479":"## Damage Vehicle and Response\n* Customers with damage are far more interested in vehicle insurance.  This will also help the model rule out about half of customers.  Don't have vehicle damage?  Then we won't bother asking you if you want vehicle insurance.","38755e15":"<font size=\"+3\" color='#053c96'><b> Problem Statement<\/b><\/font>","54fa0790":"<font size=\"+3\" color='#540b11'><b> Data Modelling and Evaluation <\/b> <\/font>","928333e3":"## Vintage\n\n* Number of days customer has been associated with the company\n* Has flat, even distribution for both total customers and customers who purchased vehicle insurance\n* not expected to influence the predictive models","b4a56c52":"## Region and Sales Channel\n* Response rate varies from about 4% to 19% with region\n* Response rate varies from about 2% to 19% among the major sales channels, and 0% to 100% for minor ones\nThis should help the model a great deal to predict a buyer.","6a77444a":"## Response and Vehicle age\n* Response rate appears to be highest with older cars, lowest with new cars\n* A large majory of the cars are <2 years old, making this an interesting sample set.  The average vehicle age in most countries is around 10 years old.","629f1a6a":"<font size=\"+3\" color='#053c96'><b>This Notebook will cover - <\/b><\/font>\n### 1. Exploratory Data Analysis\n#### * Analysis of Each Variable\n### 2. Data Modelling and Evaluation\n#### * Random Forest with Sci-Kit Learn\n#### * Dense Neural Network with Tensorflow\/Keras\n### 3. Generation of Submission CSV","11e87791":"* Older customers buy vehicle insurance at a much higher rate than younger customers.\n    Almost half of total customers are under 35, but they only make up about 1\/4 of those who buy vehicle insurance.\n* The spikes every 5 years are the result of combinations of 2 x 1-year intervals, and can be ignored.\n* It would be the best use of resources to target customers over 35 with vehicle insurance offers, since those under 35 are less likely to be interested.\n* On the chart, customers between 30 and 63 are higher-represented among those who purchase vehicle insurance.  Customers both younger and older than that range are lower-represented.","ce654cb0":"* The training data is 381,109 rows long, and the test dataset is 127,037.  \n* The training dataset has a \"response\" column, but the test dataset does not.\n* \"Vintage\" refers to how many days the customer has been with the company","1b018117":"### A variety of variations were tried to the following properties:\n* Epochs\n* Batch size\n* Activation function (relu or sigmoid)\n* Validation split\n* Hidden layer neuron number\n\nThe optimum arrangement was found below","0687413e":"<font size=\"+3\" color='#053c96'><b>Bussiness Goal<\/b><\/font>","d9e1aca1":"# Data Preprocessing for Random Forest\n* Male and Female are converted to integers\n* Vehicle Age is given dummy variables and converted to integer\n* Vehicle damage is converted to integer.\n* Age and annual premium are scaled to numbers closer to 0","1a067281":"<img src=\"https:\/\/i.pinimg.com\/originals\/e2\/d7\/c7\/e2d7c71b09ae9041c310cb6b2e2918da.gif\">","3347301a":"## Response by Previously_Insured\n* People generally only buy our vehicle insurance if they don't already have vehicle insurance","3c0a86b8":"## Classification Report ","75032080":"### Only 158 people with previous insurance switched to our company.  The number is so small they do not even show up on the bar chart.  This is very useful to the model, because it will tend to rule out everyone with previous insurance, which is about half of the customers.","c62340d8":"Our client is an Insurance company that has provided Health Insurance to its customers now they need your help in building a model to predict whether the policyholders (customers) from past year will also be interested in Vehicle Insurance provided by the company.\n\nAn insurance policy is an arrangement by which a company undertakes to provide a guarantee of compensation for specified loss, damage, illness, or death in return for the payment of a specified premium. A premium is a sum of money that the customer needs to pay regularly to an insurance company for this guarantee.\n\nFor example, you may pay a premium of Rs. 5000 each year for a health insurance cover of Rs. 200,000\/- so that if, God forbid, you fall ill and need to be hospitalised in that year, the insurance provider company will bear the cost of hospitalisation etc. for upto Rs. 200,000. Now if you are wondering how can company bear such high hospitalisation cost when it charges a premium of only Rs. 5000\/-, that is where the concept of probabilities comes in picture. For example, like you, there may be 100 customers who would be paying a premium of Rs. 5000 every year, but only a few of them (say 2-3) would get hospitalised that year and not everyone. This way everyone shares the risk of everyone else.\n\nJust like medical insurance, there is vehicle insurance where every year customer needs to pay a premium of certain amount to insurance provider company so that in case of unfortunate accident by the vehicle, the insurance provider company will provide a compensation (called \u2018sum assured\u2019) to the customer.","e4cb7d46":"## Confusion matrix","a58fb98c":"## Target Variable (Response)","e0c30c23":"### The Tensorflow dense neural net yields a very similar AUC to that of the random forest, 0.855 for both.\n### XGBoost and Catboost are other programs similar to the random forest, not included in this notebook but available elsewhere for this dataset.","09f73cb2":"# Import Libraries and Dataset","5c01b53f":"### Because the model predicts 0 for all customers, it is necessary to find another way to evaluate it.  This is where \"predict_proba\" comes in.  \"Predict_proba\" assigns each customer a probability that they will choose to buy more insurance, rather than a simple 1 or 0.  In the sorted data below, the highest probability in the dataset is 49.7%.  Your results may vary.","345bf8c4":"### It appears there were 2 policy sales channels present in test but not in training.  These also need to be removed.","ab6eaadb":"# Exploratory Data Analysis","49796acd":"### The NaN values are likely 0.  These sales channels likely will not make much difference because the total number of customers in them is very low as demonstrated below.","fb8c8f71":"## Age Distribution of Customers\n* There is a clear difference in response by age.  Positive respondants tend to be 30 - 63 years old.  The youngest customers have a very low response rate.","e450cd41":"<font size=\"+1\" color='blue'><b> I hope you enjoyed this kernel , Please don't forget to appreciate me with an Upvote.<\/b><\/font>","27d6fc80":"### The model shows a probability distribution for all customers.  About half of customers are clustered around 0, and most of the rest are clustered around 0.3.  Since all of the probabilities assigned are <0.5, the \"predict\" function predicted 0 for everybody.\n* Even if a customer has every trait to maximize his or her probability of buying vehicle insurance, they still have <50% chance of buying it.\n* By listing each customer in order of their respective probability, the business can define a cutoff point beyond which they will decide it is not worth the cost to target those customers to offer them vehicle insurance.","2ea74092":"Build a model to predict whether a customer would be interested in Vehicle Insurance is extremely helpful for the company because it can then accordingly plan its communication strategy to reach out to those customers and optimise its business model and revenue.\n\nIn order to predict whether the customer would be interested in Vehicle insurance, you have information about demographics (gender, age, region code type), Vehicles (Vehicle Age, Damage), Policy (Premium, sourcing channel) etc.","f33a39ac":"## Dealing with Unbalanced Data\ny_train contains all the responses, but it is not in the right format for the proper tensorflow model.  If this were a balanced dataset, we could make a model with 1 output neuron to represent probability.  But the data is 88% \"0\", very unbalanced, and we just saw that the random forest predicted \"0\" for everybody.  Tensorflow will do the same thing.  The way around this problem is to create a model with 2 output neurons, one for \"0\" and one for \"1\", weighting them according to the proportions of each one available.  But for 2 output neurons, the output data needs to be 2-dimensional, not 1-dimensional as y_train is.  We can make y_train and y_test 2-dimensional below.","4efd9faa":"## Age Vs Annual premium \n* Little correlation, but we will consider it in the models later","dcb4266c":"* Only 12% of 381109 customers purchase the additional vehicle insurance when offered.\n* This probably reflects most people's inherent resistance to most advertisements.  But we still have a large enough sample size of 46710 positive responses that it will be significant.  Let's see if this response rate correlates with any other variables.","82fe1340":"# Evaluate Model ","25a39d38":"# Training of TF Model for Final Submission\n### train_target has to be made 2-dimensional, just like y_train before.  All training data will be used, not split as it was for the foregoing model.","86e05525":"## Check for missing values\n* No null values","3e9b33c7":"## Save model","006eced6":"### The data in Region_Code and Policy_Sales_Channel is in floats, but the actual values are integers, so they will be converted to int.  This is overall good practice and will make it easier to convert them to dummy variables later.","02917e30":"### The train and test datasets have a different number of columns, and this will not work for the model.  Thisis the result of a few of the less common regions or policy sales channels that exist in one and not the other.  We will simply delete these columns, since the number of customers in them is very low.","90f6d2d0":"![image.png](attachment:image.png)","94813233":"## Gender and Response\n* Gender makes little difference to response, but it will be included in the models","744ece3d":"### This is exactly as expected.  Test has 1 fewer column because it should lack 'Response'.  We will break out response into its own variable next.","c9e3728d":"# Tensorflow Neural Network\n* The test and train dataframes are reworked from the beginning to yield the proper format.  \n* There are over 200 dummy variables, mostly for locations and sales channels.","ee46d7ef":"## Observations:\n* The customers range in age from 20 to 85 years old, and half of them are 25-49\n* Half of the customers in this dataset have been with the company for 82 to 227 days\n* A slight majority are male\n* The vast majority have a driving license, about 99.8%\n* Most vehicles are <2 years old\n* Over half of vehicles have damage of some kind\n* Only a little over 1 in 10 customers responded by buying vehicle insurance when asked","0e7705a4":"# Weights and Biases\n\nFor those curious, this is how to extract weights and biases from the DNN model.  The neural network is a black box, and works in mysterious ways.  The weights do not clearly identify which variables are the most important.  It would take a great deal more investigation to learn what specific job each neuron is doing.","3ebe57e8":"### The following code optimized a random forest for accuracy.  It achieves an AUC of 0.855, which is difficult to improve upon.","4b2bf552":"### \"id\" will be kept in its own dataframe for the submission later, and not used by the model"}}