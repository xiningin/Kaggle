{"cell_type":{"7dfdcf23":"code","4b42c823":"code","746d20f4":"code","4d9f86d8":"code","0cd90387":"code","96822212":"code","ae8832cb":"code","d7fa7705":"code","3a54955d":"code","d5e95340":"code","1786a729":"code","c3bfdbc5":"code","3517cfaf":"code","5e4baac1":"code","18616995":"code","740d728c":"code","52a47817":"code","3a11aecc":"code","4c7dbe7e":"code","55216418":"code","89f92703":"code","211becc8":"code","38ec94bf":"code","fcc23f67":"code","0b7427c5":"code","411e9ed3":"code","9c92a8ce":"code","b07b5573":"code","b301690e":"markdown","2a3c60fd":"markdown","c70eb33e":"markdown","5d7d9a79":"markdown","da58d8b6":"markdown","89d97c22":"markdown","50eed175":"markdown","52f4141c":"markdown","77cf1a9a":"markdown","21881358":"markdown","6e1c4d66":"markdown","6171860c":"markdown","fd05d75a":"markdown","b161cfd1":"markdown","2bf82717":"markdown","51909f6c":"markdown","ed0c5c76":"markdown","8b334412":"markdown","84d4e8b3":"markdown","cc0843c1":"markdown","c9118c08":"markdown","0c561ea4":"markdown","7e08e212":"markdown","697519ed":"markdown","79673d7f":"markdown","18d52a43":"markdown","2f4b2857":"markdown","98635441":"markdown","d2794ea2":"markdown"},"source":{"7dfdcf23":"import os\n\n\n!apt-get install openjdk-11-jdk-headless -qq > \/dev\/null\n!wget -q https:\/\/downloads.apache.org\/spark\/spark-3.1.2\/spark-3.1.2-bin-hadoop2.7.tgz\n!tar -xvf spark-3.1.2-bin-hadoop2.7.tgz\n\nos.environ[\"JAVA_HOME\"] = \"\/usr\/lib\/jvm\/java-11-openjdk-amd64\"\nos.environ[\"SPARK_HOME\"] = \".\/spark-3.1.2-bin-hadoop2.7\"\n\n!pip install -q pyspark sparknlp findspark\n\nimport findspark\n\n\nfindspark.init()","4b42c823":"!spark-shell --version 2>&1 | sed -n '6,13'p","746d20f4":"from pyspark.sql import SparkSession\n\nfrom pyspark.sql.functions import *\nfrom pyspark.sql.types import *\n\n\nspark = SparkSession.builder.master(\"local[*]\")\\\n            .appName('Unity Question Rating Prediction with Apache Spark')\\\n            .config(\"spark.driver.maxResultSize\", \"0\")\\\n            .config(\"spark.kryoserializer.buffer.max\", \"2000M\")\\\n            .config(\"spark.jars.packages\", \"com.johnsnowlabs.nlp:spark-nlp_2.12:3.0.1\")\\\n            .config('spark.driver.memory', '16g').getOrCreate()\n\nspark.sparkContext.setLogLevel(\"ERROR\")\nspark","4d9f86d8":"spark","0cd90387":"schema = StructType([\n    StructField(\"title\", StringType(), False),\n    StructField(\"body_markdown\", StringType(), False),\n    StructField(\"best_answer_markdown\", StringType(), False)\n])\n\nqa_dataset = spark.read.format(\"json\").schema(schema).load(\"..\/input\/unity3d-faq\/answers.json\")\nqa_dataset.limit(10).toPandas()","96822212":"import re\n\n\nre_code_blocks = re.compile(r'(```)[\\n\\r\\t\\S ]+(```)')\nre_code_lines = re.compile(r'(`)[\\n\\r\\t\\S ]+(`)')\nre_user_references = re.compile(r'@[\\S]+')\nre_hashtags = re.compile(r'(#)([\\S]+)')\nre_common_line_separator = re.compile(r'[\\n\\r\\t]+')\nre_punctuation = re.compile(r'[!\"#$%&\\(\\)\\*\\+,\\.\/:;<=>?@\\\\^_`{|}~\\[\\]-]+')\nre_non_unicode = re.compile(r'[^\\u0000-\\u007F]+')\nre_digits_in_words = re.compile(r'\\b(\\w)*(\\d)(\\w)*\\b')\n\nre_redundant_spaces = re.compile(r'[ ]+')","ae8832cb":"def preprocessing_pipeline(text: str, keep_code=False, keep_user=False) -> str:\n    code_token = '[CODE]' if keep_code else ''\n    user_token = '[USER]' if keep_user else ''\n    \n    steps = [\n        # Convert sentences to lowercase\n        lambda text: text.lower(),\n        # Substitute { code blocks } with [CODE] token\n        lambda text: re_code_lines.sub(code_token, re_code_blocks.sub(code_token, text)),\n        # Substitute @user_references with user token\n        lambda text: re_user_references.sub(user_token, text),\n        # Substitute #hashtags with corresponding words\n        lambda text: re_hashtags.sub(r'\\2', text),\n        # Substitute common line separator symbols [\\n, \\r, \\t] with spaces\n        lambda text: re_common_line_separator.sub(' ', text),\n        # Remove punctuation\n        lambda text: re_punctuation.sub(' ', text),\n        # Remove Unicode symbols\n        lambda text: re_non_unicode.sub(' ', text),\n        # Remove words that contain digits\n        lambda text: re_digits_in_words.sub('', text),\n        # Remove redundant spaces between each word\n        lambda text: re_redundant_spaces.sub(' ', text),\n        # Strip string\n        lambda text: text.strip()\n    ]\n    \n    for step in steps:\n        text = step(text)\n        \n    return text\n\n\npreprocessing_pipeline_udf = udf(lambda x: preprocessing_pipeline(x), StringType())\njoin_str_columns_udf = udf(lambda columns: \" \".join(columns), StringType())\njoin_tokens_udf = udf(lambda x: ' '.join(x), StringType())\ncharacters_count_udf = udf(lambda x: len(x), IntegerType())","d7fa7705":"dirty_string = \"This is the #title I sent      \\n \\n \\r \\t @him AnD I'm still w8ing for the [response]...\"\n\npreprocessing_pipeline(dirty_string)","3a54955d":"qa_dataset = qa_dataset.select(\n    preprocessing_pipeline_udf(\n        join_str_columns_udf(\n            array(\"title\", \"body_markdown\")\n        )\n    ).alias(\"question\"),\n    col(\"best_answer_markdown\").alias(\"answer\")\n)\nqa_dataset.limit(10).toPandas()","d5e95340":"import seaborn as sns\n\nimport matplotlib.pyplot as plt\nimport matplotlib.ticker as ticker\n\n\nMAX_SENTENCE_LENGTH = 1000\n\n# We assume that tokens are separated by whitespace\nsentence_length_udf = udf(lambda x: len(x.split()), IntegerType())\n\n# Sentence length dataframe\nlen_dataset = qa_dataset.select(\n    col(\"question\"),\n    col(\"answer\"),\n    sentence_length_udf(col(\"question\")).alias(\"question_len\")\n).filter(col(\"question_len\") < MAX_SENTENCE_LENGTH)\n\nstratified_dataset = len_dataset.select(col(\"question\"), col(\"answer\"))\nsamples_count = len_dataset.count()\n\n# Calculating length distribution\nlen_distro = len_dataset.groupBy(\n    col(\"question_len\")\n).agg({\"question_len\": \"count\"}).select(\n    col(\"question_len\").alias(\"Sentence length\"),\n    col(\"count(question_len)\").alias(\"Length frequencies\")\n).orderBy(col(\"Sentence length\").asc())\n\n# Displaying distro as barchart\nlen_distro_df = len_distro.toPandas()\nf, ax = plt.subplots(1, 1, figsize=(16, 9))\n\n# Displaying mean and STD of the distribution\nsns.boxplot(x=len_distro_df[\"Sentence length\"], ax=ax)\nax.set_title(f\"Mean and STD of distribution for {samples_count} samples\")\n\nplt.show()","1786a729":"from wordcloud import WordCloud\nfrom sklearn.feature_extraction import text\n\n\nsplit_into_tokens_udf = udf(lambda x: x.split(), ArrayType(StringType()))\n\nstop_words = list(text.ENGLISH_STOP_WORDS)\nwc = WordCloud(stopwords=stop_words, background_color=\"white\", colormap=\"Dark2\")\n\nword_frequencies = stratified_dataset.select(\n    explode(split_into_tokens_udf(col(\"question\"))).alias(\"token\")\n).groupBy(\n    col(\"token\")\n).agg({\"token\": \"count\"}).filter(\n    ~col(\"token\").isin(stop_words)\n).select(\n    col(\"token\"),\n    col(\"count(token)\").alias(\"freq\"),\n)\n\nwc.generate_from_frequencies({row.token : row.freq for row in word_frequencies.collect() })\n\nplt.figure(figsize=(20, 12))\nplt.imshow(wc, interpolation=\"bilinear\")\nplt.axis(\"off\")\nplt.show()","c3bfdbc5":"import sparknlp\n\nfrom sparknlp import DocumentAssembler, Finisher\nfrom sparknlp.annotator import Tokenizer, LemmatizerModel, StopWordsCleaner\nfrom pyspark.ml import Pipeline\n\n\nsparknlp.start()","3517cfaf":"text_processing_pipeline = Pipeline().setStages([\n    DocumentAssembler().setInputCol(\"question\").setOutputCol(\"doc\"),\n    Tokenizer().setInputCols([\"doc\"]).setOutputCol(\"tokens_annotations\"),\n    LemmatizerModel.pretrained().setInputCols([\"tokens_annotations\"]).setOutputCol(\"lemma_annotations\"),\n    Finisher().setInputCols([\"lemma_annotations\"]).setOutputCols([\"normtokens\"]).setOutputAsArray(True)\n])\n\ntoken_preprocessor = text_processing_pipeline.fit(stratified_dataset)\nprocessed_dataframe = token_preprocessor.transform(stratified_dataset).select(col(\"normtokens\").alias(\"question\"), col(\"answer\"))\nprocessed_dataframe = processed_dataframe.select(join_tokens_udf(col(\"question\")).alias(\"question\"), col(\"answer\")).filter(characters_count_udf(col(\"question\")) > 0)\n\nprocessed_dataframe_preview = processed_dataframe.limit(10).toPandas()","5e4baac1":"processed_dataframe_preview","18616995":"from sparknlp.base import *\nfrom sparknlp.annotator import *\n\n\ndocument_assembler = DocumentAssembler().setInputCol(\"question\").setOutputCol(\"document\")\nsentence_embedder = BertSentenceEmbeddings.pretrained(\"sent_small_bert_L8_512\").setInputCols([\"document\"]).setOutputCol(\"sentence_embeddings\")","740d728c":"from pyspark.ml import Transformer\nfrom pyspark.ml.linalg import Vectors, VectorUDT\nfrom pyspark.ml.util import DefaultParamsReadable, DefaultParamsWritable\nfrom pyspark.ml.param.shared import HasInputCol, HasOutputCol, Param, Params, TypeConverters\n\n\nclass ExtractEmbeddingsTransformer(\n        Transformer, HasInputCol, HasOutputCol,\n        DefaultParamsReadable, DefaultParamsWritable):\n\n    @keyword_only\n    def __init__(self, inputCol=None, outputCol=None):\n        super(ExtractEmbeddingsTransformer, self).__init__()\n        kwargs = self._input_kwargs\n        self.setParams(**kwargs)\n\n    @keyword_only\n    def setParams(self, inputCol=None, outputCol=None):\n        kwargs = self._input_kwargs\n        return self._set(**kwargs)\n\n    def setInputCol(self, value):\n        return self._set(inputCol=value)\n\n    def setOutputCol(self, value):\n        return self._set(outputCol=value)\n\n    def _transform(self, dataset):\n        to_vector = udf(lambda a: Vectors.dense(a[0].embeddings), VectorUDT())\n        \n        out_col = self.getOutputCol()\n        in_col = self.getInputCol()\n        return dataset.withColumn(out_col, to_vector(in_col))\n    \n\nemb_transformer = ExtractEmbeddingsTransformer(inputCol=\"sentence_embeddings\", outputCol=\"embeddings\")","52a47817":"embedding_pipeline = Pipeline(stages=[\n    document_assembler,\n    sentence_embedder,\n    emb_transformer\n])\n\nembedding_transformer = embedding_pipeline.fit(processed_dataframe)\nembeddings = embedding_transformer.transform(processed_dataframe)\n\nembeddings_preview = embeddings.select(col(\"question\"), col(\"embeddings\")).limit(10).toPandas()","3a11aecc":"embeddings_preview","4c7dbe7e":"embeddings.select(\n    col(\"question\"),\n    col(\"answer\"),\n    col(\"embeddings\")\n).coalesce(48).write.parquet(\".\/output\/questions_answering\/dataframes\/embeddings_dataframe\")\n\nembedding_transformer.write().overwrite().save(\".\/output\/questions_answering\/pipelines\/embedding_transformer\")\ntoken_preprocessor.write().overwrite().save(\".\/output\/questions_answering\/pipelines\/token_preprocessor\")","55216418":"embeddings = spark.read.parquet(\".\/output\/questions_answering\/dataframes\/embeddings_dataframe\")\n\nembedding_transformer = PipelineModel.load(\".\/output\/questions_answering\/pipelines\/embedding_transformer\")\ntoken_preprocessor = PipelineModel.load(\".\/output\/questions_answering\/pipelines\/token_preprocessor\")\n\nembeddings.limit(10).toPandas()","89f92703":"import numpy as np\n\nfrom sklearn.metrics.pairwise import cosine_similarity\n\n\ndef calculate_embeddings(query: str):\n    query = preprocessing_pipeline(query)\n\n    query_dataframe = spark.sparkContext.parallelize([(query,)]).toDF([\"question\"])\n    query_dataframe = token_preprocessor.transform(query_dataframe).select(\n        col(\"normtokens\").alias(\"question\")\n    ).select(\n        join_tokens_udf(col(\"question\")).alias(\"question\")\n    )\n\n    return embedding_transformer.transform(query_dataframe).select(col(\"embeddings\")).first().embeddings\n\n\ncosine_similarity_udf = udf(lambda record, query: cosine_similarity(np.array([record]),\n                                                                    np.array([query])).item(), FloatType())","211becc8":"from IPython.display import display, Markdown\n\n\nquery = \"\"\"\n'If' Statement in javascript giving me problems I'm trying to write a 'IF' statement for my health\nsystem heres the code I have. ``` var healthmin = 0; var health = 100; var GUIhealth : GUIText; var\ntext = \"Health: \"; function Update(){ if (healthmin == 1) { health--; GUIhealth.text = text +\nhealth.ToString(); } else { GUIhealth.text = text + health.ToString(); } function OnTriggerEnter(\nother : Collider ) { if (other.tag == \"zom\") { var healthmin = 1; } } function OnTriggerExit( other\n: Collider ) { if (other.tag == \"zom\") { var healthmin = 0; } } ``` `var healthmin = 0; var health\n= 100; var GUIhealth : GUIText; var text = \"Health: \"; function Update(){ if (healthmin == 1) {\nhealth--; GUIhealth.text = text + health.ToString(); } else { GUIhealth.text = text + health.ToString();\n} function OnTriggerEnter( other : Collider ) { if (other.tag == \"zom\") { var healthmin = 1; } }\nfunction OnTriggerExit( other : Collider ) { if (other.tag == \"zom\") { var healthmin = 0; } }`but\nits giving me that this error \"Assets\/playerhealth.js(21,10): BCE0044: expecting (, found\n'OnTriggerEnter'.\" but I know there is nothing wrong with the 'OnTriggerEnter' Function because\nIf i take the 'If' statement for the 'healthmin' on Function Update it works fine, There is something\nwrong with the Function Updates 'IF' Statement that I can't figure out, what am I doing wrong here?\n\"\"\"\ntop_queries_count = 5\n\n\nquery_embeddings = calculate_embeddings(query)\nembeddings_to_compare = embeddings.withColumn('query', array([lit(i) for i in\n                                                              query_embeddings.toArray().tolist()]))\n\n\nresults = embeddings_to_compare.select(\n    cosine_similarity_udf(col(\"embeddings\"), col(\"query\")).alias(\"similarity_score\"),\n    col(\"question\"),\n    col(\"answer\")\n).orderBy(\n    col(\"similarity_score\").desc()\n).limit(top_queries_count).select(\n    col(\"similarity_score\"),\n    col(\"question\"),\n    col(\"answer\")\n).collect()\n\ndisplay(Markdown(\"\\n\\n\\n\".join([f\"**Similarity**: {result.similarity_score}\" + \"\\n\\n\" +\n                                f\"**Similar question**: {result.question}\" + \"\\n\\n\" +\n                                f\"**Answer**: {result.answer}\" for result in results])))","38ec94bf":"def hits_count(pos_ranks, k):\n    return np.sum([int(rank <= k) for rank in pos_ranks]) \/ len(pos_ranks)","fcc23f67":"def dcg_score(pos_ranks, k):\n    return np.sum([1 \/ (np.log2(1.0 + rank)) * int(rank <= k) for rank in pos_ranks]) \/ len(pos_ranks)","0b7427c5":"import pandas as pd\n\n# Closest queries to the positive record\neval_user_queries = [\"How do i animate my character in blender so that the animations shows up in unity on my character object?\",\n                     \"Collision detection without Rigidbody?\"]\n\n# First record is positive\neval_candidates = [[\n    (\"Animating character with blender and transfer animations to Unity.\", 1),\n    (\"Any way to detect if a button is held down when the scene loads?\", 2),\n    (\"Booleans of prefabs are interfering with each other.\", 3),\n    (\"Can I detect if my controller is using DirectInput or XInput?\", 4)\n], [\n    (\"Does anyone know how to detect collision without using a Rigidbody?\", 1),\n    (\"Difference between a capitalized component in AddComponent and Destroy.\", 2),\n    (\"How to run a function when the Scene first loads?\", 3),\n    (\"Game doesn't recognize Time.DeltaTime?\", 4)\n]]\n\n\ndef evaluate_ranking(eval_query, eval_candidate):\n    query_embeddings = calculate_embeddings(eval_query)\n    \n    # Important to shuffle evaluation candidates to ensure algorithm correctness\n    eval_candidate = eval_candidate.sample(False, 1.0)\n\n    normalized_candidates = token_preprocessor.transform(eval_candidate.select(\n        preprocessing_pipeline_udf(col(\"question\")).alias(\"question\"), col(\"rank\")\n    )).select(\n        join_tokens_udf(col(\"normtokens\")).alias(\"question\"), col(\"rank\")\n    )\n\n    embeddedd_candidates = embedding_transformer.transform(normalized_candidates).select(col(\"question\"),\n                                                                                         col(\"embeddings\"),\n                                                                                         col(\"rank\"))\n    embeddings_to_compare = embeddedd_candidates.withColumn('query',\n                                                            array([lit(i) for i in query_embeddings.toArray().tolist()]))\n\n    return embeddings_to_compare.select(\n        cosine_similarity_udf(col(\"embeddings\"), col(\"query\")).alias(\"similarity_score\"),\n        col(\"question\"), col(\"rank\")\n    ).orderBy(\n        col(\"similarity_score\").desc()\n    ).select(\n        col(\"similarity_score\"),\n        col(\"question\"), col(\"rank\")\n    ).first().asDict()\n    \nranks = [evaluate_ranking(query, spark.sparkContext.parallelize(candidate).toDF([\"question\", \"rank\"])).get(\"rank\")\n         for query, candidate in zip(eval_user_queries, eval_candidates)]\n    \npd.DataFrame([[k, dcg_score(ranks, k), hits_count(ranks, k)]\n              for k in [1, 2]], columns=['K', 'DCG@K', 'Hits@K']).style.hide_index()","411e9ed3":"previews_dataframe = spark.read.option(\"header\",True)\\\n                        .csv(\"..\/input\/unity3d-faq\/previews.csv\")\\\n                        .select(col(\"norm_title\").alias(\"query\"))","9c92a8ce":"from pyspark.sql import Window\n\n\ndef sample_and_index(df, fraction):\n    window = Window.orderBy(col('question'))\n    df = df.sample(False, fraction).rdd.zipWithIndex()\\\n            .toDF([\"question\", \"rank\"]).select(join_str_columns_udf(col(\"question\")).alias(\"question\"),\n                                               (col(\"rank\") + 1).alias(\"rank\"))\n    \n    return df\n\n\nruns = 10\nfraction = 0.05\nranks = []\n\nfor _ in range(runs): \n    sample_with_ranks = sample_and_index(previews_dataframe, fraction)\n    rank = evaluate_ranking(sample_with_ranks.first().question, sample_with_ranks).get(\"rank\")\n    ranks.append(rank)","b07b5573":"pd.DataFrame([[k, dcg_score(ranks, k), hits_count(ranks, k)]\n              for k in [1, 5, 10, 100]], columns=['K', 'DCG@K', 'Hits@K']).style.hide_index()","b301690e":"Demonstration of the ranking system by TOP K similar queries.","2a3c60fd":"Let's check evaluation metrics on the test examples.","c70eb33e":"The main advantages of `Apache Spark` over other solutions:\n  - Simplicity;\n  - ML and analytics tools;\n  - Real-time streaming;\n  - Increase in performance (due to the use of RAM resources in the calculation);\n\nCheck if Apache Spark is installed by calling `spark-shell`.","5d7d9a79":"Now when we got all question embeddings we can compare user query hidden representation with representations stored in the dataframe. We will use `cosine distance` to rank candidate questions.","da58d8b6":"Preprocessing results.","89d97c22":"Create Apache Spark session.","50eed175":"# Unity Forum Questions Answering with Apache Spark\n\n![unity-wallpaper.jpg](attachment:cdb00867-a9b9-4b0a-99ed-6f9c74538032.jpg)\n\n*Unity 3D Engine Logo \u00a9 2021 [Unity Technologies](https:\/\/unity.com\/)*\n\n## Acknowledgements\n\n- Articles:\n    - [MapReduce: Simplified Data Processing on Large Clusters](https:\/\/static.googleusercontent.com\/media\/research.google.com\/en\/\/archive\/mapreduce-osdi04.pdf)\n    - [Analytics Vidhya. A Complete Guide for Creating Machine Learning Pipelines using PySpark MLlib](https:\/\/www.analyticsvidhya.com\/blog\/2021\/05\/a-complete-guide-for-creating-machine-learning-pipelines-using-pyspark-mllib-on-google-colab\/)\n    - [Towards Data Science. BERT For Measuring Text Similarity](https:\/\/towardsdatascience.com\/bert-for-measuring-text-similarity-eec91c6bf9e1)\n    - [BERT and the Transformer Architecture](https:\/\/neptune.ai\/blog\/bert-and-the-transformer-architecture-reshaping-the-ai-landscape)\n- Courses:\n    - [HSE: Natural Language Processing](https:\/\/www.coursera.org\/learn\/language-processing)\n- Datasources:\n    - [Unity Answers Forum Questions](https:\/\/www.kaggle.com\/antonkozyriev\/unity3d-faq)\n\n## Data analysis with Apache Spark\n\nAll data processing is performed on the `Apache Spark` computational engine for streaming and batching processing of the Big Data using the paradigm, similar to **MAP-REDUCE**, which is a part of ecosystem of `Hadoop`. Unlike pure Python solutions (which does all processing in the instructions interpreter mode), `Apache Spark` performs effective procedure of data parallelization and fast computation using *RDD (Resilient Distributed Dataset)*.\n\n**MAP-REDUCE** is a programming model that allows you to perform simple and efficient distribution of computational tasks on each node (core) of a computing device or cluster. The model consists of basic steps:\n  - **MAP** - performs the procedure of distribution of each value in the dataset to create a new data structure;\n  - **REDUCE** - performs the procedure of combining the data structure into a new value;\n\nThe input and output for each step are represented as a `key-value` pair, and the procedures themselves are specified according to the following pattern:\n\n```\nmap (in_key, in_value)\n    list (out_key, intermediate_value)\n    \nreduce (out_key, list (intermediate_value))\n    list (out_value)\n```\n\nThe **MAP** function creates an intermediate value for the intended source key, the **REDUCE** function combines all intermediate values for a specific key. The classic **MAP-REDUCE** works in synchronous mode. The input data is separated and several `map ()` tasks are performed in parallel. After all `map ()` procedures are completed, all intermediate values are combined for all unique keys, performing several `reduce ()` tasks in parallel.\n\n## Goal of the notebook\n\nBuild a machine learning model that calculates similarity between *user query* and *questions* in the loaded dataframe, then returns answer with the **highest similarity score** for *question*.\n\n## Environment preparation\n\nBecause `Spark NLP` models aren't supported by new versions of `Apache Spark`, we will install version `3.1.2` as a temporary fix. In order to interact with `Apache Spark` computational engine, the client library `pyspark` should be installed. Let's install extra natural language processing package sparknlp for `Apache Spark`.","52f4141c":"## Thank you for your attention\n\nThis notebook is a part of a more significant academic project. It aims to create a **Unity3D Assistant** DL model for question answering. It may contain mistakes, so I'd appreciate any advice on enhancing the existing codebase and model.","77cf1a9a":"## Data cleaning\n\nFirst, let's create corresponding dataframes and load text data.","21881358":"## Feature engineering and model training\n\nOf course, it won't be efficient to compare `user query` with each recording in the dataset by the presence of the same words. We need to capture the context of both by their hidden representations, which we can find by transforming text into the vector form. There are two common approaches:\n- **Frequency encoding** (TF-IDF, Count vectorization);\n- **Distributional semantics** (Word2Vec, GloVe, Doc2Vec, BERT-encoding, ELMO-encoding);\n\nThe pretrained DL encoders are SOTA-solutions in this field, so here we will focus on the BERT-encoders as the embedding technique. To get desired hidden representations in a vector form of the `query` we can pass the token representations (where a character or sentence is assigned to the integer) through encoder. BERT (Bidirectional Encoder Representations from Transformers) is a deep learning model, based on the transformer architecture:\n\n![transformer.png](attachment:transformer.png)\n\n*[Transformer architecture](https:\/\/neptune.ai\/blog\/bert-and-the-transformer-architecture-reshaping-the-ai-landscape)*\n\nWe need only **Encoder** part of the transformer to generate sentence embeddings.","6e1c4d66":"Building preprocessing pipeline.","6171860c":"We will use `Apache Spark` user defined functions with RegExp to perform preliminary text processing:\n - Convert sentences to lowercase;\n - Substitute `{ code blocks }` with `[CODE]` token;\n - Substitute `@user_references` with `[USER]` token;\n - Substitute `#hashtags` with corresponding words;\n - Substitute common line separator symbols `[\\n, \\r, \\t]` with spaces;\n - Remove punctuation;\n - Remove Unicode symbols;\n - Remove words that contain digits;\n - Remove redundant spaces between each word;\n - Strip string;","fd05d75a":"Display spark session configs.","b161cfd1":"Let's restore serialized dataframe.","2bf82717":"Check the pipeline on a test string.","51909f6c":"Applying processing pipelines on the dataframe.","ed0c5c76":"The ranking system shows a good performance on the test example so far. To ensure the efficiency of the model, we evaluate similarity metrics on each record in the dataset.\n\n## Evaluation of text similarity\n\nIf the ranking system produces a relevant mapping of the `query` to some `hidden representations`, the cosine similarity between the closest `queries` should be greater than for the arbitrary ones. So in the testing phase for each similar `query`, we can sample $ M $ random negative records from dataset and find out the position of the correct duplicate. Following examples of records generation where $ M = 3 $:\n\n| User query | Positive record | Negative records |\n|:---:|:---:|:---:|\n| How do i animate my character in blender so that the animations shows up in unity on my character object? | Animating character with blender and transfer animations to Unity. | 1. Any way to detect if a button is held down when the scene loads?<br>2. Booleans of prefabs are interfering with each other.<br>3. Can I detect if my controller is using DirectInput or XInput? |\n| Collision detection without Rigidbody? | Does anyone know how to detect collision without using a Rigidbody? | 1. Difference between a capitalized component in AddComponent and Destroy.<br>2. How to run a function when the Scene first loads?<br>3. Game doesn't recognize Time.DeltaTime? |\n\nNote, that we do not expect positive record to be on the top of the ranking results with each sample, so let us consider the place of the best candidate in the sorted list of candidates and formulate a metric based on it. We can fix some $ K $ \u2014 a reasonalble number of top-ranked elements and $ N $ \u2014 a number of queries (size of the sample).\n\n### Hits@K\n\nThe first simple metric will be a number of correct hits for some $ K $:\n\n$$ \\text{Hits@K} = \\frac{1}{N}\\sum_{i=1}^N \\, [pos_i \\in topK(q_i)] $$\n\n$ q_i $ - the i-th query, $ pos_i $ - positive record.","8b334412":"Now let's examine the words distributions over dataframe to determine which are the most common. We will use wordcloud as a word distribution visualization tool. As expected, the dataframe contains mostly tokens related to the game development (unity, object, script e.t.c.).","84d4e8b3":"### DCG@K\nThe second one is a simplified Discounted Cumulative Gain Metric:\n\n$$ \\text{DCG@K} = \\frac{1}{N} \\sum_{i=1}^N\\frac{1}{\\log_2(1+rank_{pos_i})}\\cdot[rank_{pos_i} \\le K] $$\n\nwhere $ rank_{pos_i} $ is a position of the positive record in the sorted list of the nearest records for the query $q_i$.","cc0843c1":"`BertSentenceEmbeddings` model along with sentence **embeddings** outputs additional metadata, which we don't need now. In order to return **embeddings** only from the pipeline we need to build a custom `Transformer`.","c9118c08":"Text processing phases as UDFs.","0c561ea4":"It's important to highlight that dataframe may contain various forms of the same token, for instance single form of token `game` and plural forms of the same token `games`. Before jumping into **feature engineering**, it is necessary to preprocess input text and reduce amount of similar tokens to simplify final model. We consider following steps:\n- Tokenizaton - splitting sentences by tokens: \"This is a sample sentence\" $ \\to $ [This, is, a, sample, sentence]\n- Lemmatization - reduction similar tokens to the same lemma (root) using predefined dictionary: cats $ \\to $ cat, fishes $ \\to $ fish, wolves $ \\to $ wolf\n- Stopwords - removing words with no context meaning: \"a\", \"the\", \"is\" ...","7e08e212":"The $ K $ value will be in set $ \\{ 1, 5, 10, 100 \\} $. As we can see from the results, the ranking system matched all correct representations from test set.","697519ed":"For the further usages we will store `question` embeddings dataframe and transformation pipelines on a filesystem.","79673d7f":"Embeddings results.","18d52a43":"Now we test the ranking system on the real scale, considering the records from `previews.csv` as the testing set.","2f4b2857":"Evaluation metrics for test set.","98635441":"Question embeddings pipeline.","d2794ea2":"## EDA\n\nBefore building models and feature engineering, we should examine *user queries* data and collect basic metrics, such as:\n - Sentence length distribution;\n - Most common words in queries (and words-outliers);\n \nLet's start first with sentence length distribution. We also consider `user queries` longer than 1000 words as outliers."}}