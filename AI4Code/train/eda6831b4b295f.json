{"cell_type":{"9cf2bcbd":"code","8b88ded3":"code","3680ffcf":"code","e1d034e8":"code","eef9104f":"code","a3bc84f7":"code","9b1a51b7":"code","c56a397f":"code","c997d750":"code","53ccf821":"code","f2c52620":"code","e3f0dce1":"code","3cf83ece":"code","34339322":"markdown","e78b1787":"markdown","7109cb39":"markdown"},"source":{"9cf2bcbd":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","8b88ded3":"! pip install git+https:\/\/www.github.com\/keras-team\/keras-contrib.git","3680ffcf":"from keras_contrib.layers.normalization.instancenormalization import InstanceNormalization\nfrom keras.layers import Input,Dense,Reshape,Flatten,Dropout,Concatenate\nfrom keras.layers import BatchNormalization,Activation,ZeroPadding2D,Add\nfrom keras.layers import LeakyReLU\nfrom keras.layers.convolutional import Conv2D,UpSampling2D,Conv2DTranspose\nfrom keras.layers.merge import add\n\nfrom keras.models import Model,Sequential\nfrom keras.initializers import RandomNormal\nfrom keras.optimizers import Adam\nfrom keras import backend as K\n\nfrom keras.utils import plot_model\n\nimport matplotlib as mpl\nmpl.rcParams['axes.grid'] = False\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style(\"darkgrid\")\n\nimport datetime","e1d034e8":"import tensorflow as tf\nimport tensorflow_datasets as tfds\ntfds.disable_progress_bar()\nAUTOTUNE = tf.data.experimental.AUTOTUNE\n\n#Download data of two different domains(cezanne paintings,real photos)\ndataset, metadata = tfds.load('cycle_gan\/cezanne2photo',\n                              with_info=True, as_supervised=True)\ntrain_cezanne,train_photo = dataset['trainA'],dataset['trainB']\ntest_cezanne,test_photo = dataset['testA'],dataset['testB']\n\n#Hyperparameters\nBUFFER_SIZE = 1000\nBATCH_SIZE = 1\nIMG_WIDTH = 128\nIMG_HEIGHT = 128\nIMG_CHANNEL = 3\n\n#functions which will be applied to input image dataset before entering model\ndef random_crop(image):\n    cropped = tf.image.random_crop(image,size=[IMG_HEIGHT,IMG_WIDTH,3])\n    return cropped\n\ndef normalize(image):\n    image = tf.cast(image,tf.float32)\n    image = (image\/127.5)-1\n    return image\n\ndef random_jitter(image):\n#     bigger = tf.image.resize(image,[144,144],method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n    cropped = random_crop(image)\n    image = tf.image.random_flip_left_right(cropped)\n    return image\n\n#preprocess functions (summarized)\ndef preprocess_image_train(image, label):\n    image = random_jitter(image)\n    image = normalize(image)\n    return image\n\ndef preprocess_image_test(image, label):\n    image = normalize(image)\n    return image\n\ntrain_cezanne = train_cezanne.map(preprocess_image_train,num_parallel_calls=AUTOTUNE).cache().shuffle(BUFFER_SIZE).batch(1)\ntrain_photo = train_photo.map(preprocess_image_train,num_parallel_calls=AUTOTUNE).cache().shuffle(BUFFER_SIZE).batch(1)\n\ntest_cezanne = test_cezanne.map(preprocess_image_train,num_parallel_calls=AUTOTUNE).cache().shuffle(BUFFER_SIZE).batch(1)\ntest_photo = test_photo.map(preprocess_image_train,num_parallel_calls=AUTOTUNE).cache().shuffle(BUFFER_SIZE).batch(1)\n\nsample_cezanne = next(iter(train_cezanne))\nsample_photo = next(iter(train_photo))\n\n\nplt.figure(figsize=(8,8))\n\nplt.subplot(221)\nplt.title('sample cezanne')\nplt.axis('off')\nplt.imshow(sample_cezanne[0]*0.5+0.5)\n\nplt.subplot(222)\nplt.title(\"sample cezanne + jittering\")\nplt.axis('off')\nplt.imshow(random_jitter(sample_cezanne[0])*0.5+0.5)\n\nplt.subplot(223)\nplt.title('sample photo')\nplt.axis('off')\nplt.imshow(sample_photo[0]*0.5+0.5)\n\nplt.subplot(224)\nplt.title(\"sample photo + jittering\")\nplt.axis('off')\nplt.imshow(random_jitter(sample_photo[0])*0.5+0.5)\n\nplt.show()","eef9104f":"## code for Reflection Padding 2D layer\nfrom keras.layers import Layer,InputSpec\nimport keras.backend as K\n\nclass ReflectionPadding2D(Layer):\n    def __init__(self,padding=(1,1),**kwargs):\n        self.padding = tuple(padding)\n        self.input_spec = [InputSpec(ndim=4)]\n        super(ReflectionPadding2D,self).__init__(*kwargs)\n        \n    def compute_output_shape(self,s):\n        return (s[0],s[1]+2*self.padding[0],s[2]+2*self.padding[1],s[3])\n    \n    def call(self,x,mask=None):\n        w_pad,h_pad = self.padding\n        return tf.pad(x,[[0,0],[h_pad,h_pad],[w_pad,w_pad],[0,0]],'REFLECT')\n        ","a3bc84f7":"class CycleGAN():\n    def __init__(self,input_dim,lr,weight_valid,weight_reconstr,weight_id,gen_n_filters,disc_n_filters,generator):\n        self.input_dim = input_dim\n        self.lr = lr\n        self.weight_valid = weight_valid\n        self.weight_reconstr = weight_reconstr\n        self.weight_id = weight_id\n        self.gen_n_filters = gen_n_filters\n        self.disc_n_filters =disc_n_filters\n        self.g_type = generator # either 'unet' or 'resnet'\n        \n        self.d_losses = []\n        self.d_accs = []\n        self.g_losses = []\n        self.steps = 1\n        self.epochs = 0\n        \n        ##label shape when training Discriminator0\n        patch = int(self.input_dim[0]\/2**3)\n        self.disc_patch = (patch,patch,1)\n        \n        self.weight_init = RandomNormal(mean=0.,stddev=0.02)\n        \n        self.compile_models()\n        \n    def compile_models(self):\n        ## Build Discriminators\n        self.D_A = self.build_discriminator()\n        self.D_B = self.build_discriminator()\n        \n        ## Compile Discriminators\n        self.D_A.compile(loss='mse',optimizer=Adam(self.lr,0.5),metrics=['accuracy'])\n        self.D_B.compile(loss='mse',optimizer=Adam(self.lr,0.5),metrics=['accuracy'])\n        \n        ## Build Generators\n        if self.g_type == 'unet':\n            self.G_AB = self.build_generator_unet()\n            self.G_BA = self.build_generator_unet()\n        elif self.g_type == 'resnet':\n            self.G_AB = self.build_generator_resnet()\n            self.G_BA = self.build_generator_resnet()\n        else:\n            print(\"Generator keyword should be either 'resnet' or 'unet'\")\n            raise ValueError\n        \n        ## Build Combined (G and D connected so that you can train G)\n        \n        # freeze Discriminators\n        self.D_A.trainable = False\n        self.D_B.trainable = False\n        \n        # original images\n        img_A = Input(shape=self.input_dim)\n        img_B = Input(shape=self.input_dim)\n        \n        # generated images\n        fake_A = self.G_BA(img_B)\n        fake_B = self.G_AB(img_A)\n        \n        # reconstructed images\n        reconstr_A = self.G_BA(fake_B)\n        reconstr_B = self.G_AB(fake_A)\n        \n        # identity images\n        id_A = self.G_BA(img_A)\n        id_B = self.G_AB(img_B)\n        \n        # resulting patches of fake images on Discriminators\n        valid_A = self.D_A(fake_A)\n        valid_B = self.D_B(fake_B)\n        \n        ## Compile Combined\n        \n        self.combined = Model([img_A,img_B],[valid_A,valid_B,\\\n                                            reconstr_A,reconstr_B,\\\n                                            id_A,id_B])\n        self.combined.compile(loss=['mse','mse','mae','mae','mae','mae'],\\\n                             loss_weights=[self.weight_valid,self.weight_valid,\\\n                                          self.weight_reconstr,self.weight_reconstr,\\\n                                          self.weight_id,self.weight_id],\\\n                             optimizer=Adam(0.0002,0.5))\n        # unfreeze Discriminators\n        self.D_A.trainable = True\n        self.D_B.trainable = True\n        \n        \n    def build_discriminator(self):\n        def conv4(layer_input,filters,stride=2,norm=True):\n            H = Conv2D(filters,kernel_size=4,strides=stride,padding='same',kernel_initializer=self.weight_init)(layer_input)\n            if norm:\n                H = InstanceNormalization(axis=-1,center=False,scale=False)(H)\n            H = LeakyReLU(0.2)(H)\n            return H\n        \n        X = Input(shape=self.input_dim) # 256x256x3\n        \n        H = conv4(X,self.disc_n_filters,stride=2,norm=False) #128x128x32\n        H = conv4(H,self.disc_n_filters*2,stride=2,norm=True) #64x64x64\n        H = conv4(H,self.disc_n_filters*4,stride=2,norm=True) #32x32x128\n        H = conv4(H,self.disc_n_filters*8,stride=1,norm=True) #32x32x256\n        \n        Y = Conv2D(1,kernel_size=4,strides=1,padding='same',kernel_initializer=self.weight_init)(H) #32x32x1\n        \n        model = Model(X,Y)\n        \n        return model\n        \n    ## unet generator \n    def build_generator_unet(self):\n        def downsample(layer_input,filters):\n            H = Conv2D(filters=filters,kernel_size=4,strides=2,padding='same',kernel_initializer=self.weight_init)(layer_input)\n            H = InstanceNormalization(axis=-1,center=False,scale=False)(H)\n            H = Activation('relu')(H)\n            \n            return H\n        \n        def upsample(layer_input,skip_input,filters):\n            H = UpSampling2D(size=2)(layer_input)\n            H = Conv2D(filters=filters,kernel_size=4,strides=1,padding='same',kernel_initializer=self.weight_init)(H)\n            H = InstanceNormalization(axis=-1,center=False,scale=False)(H)\n            H = Activation('relu')(H)\n            H = Concatenate()([H,skip_input])\n            \n            return H\n        \n        X = Input(shape=self.input_dim)        #256x256x3\n        \n        d1 = downsample(X,self.gen_n_filters)   #128x128x32\n        d2 = downsample(d1,self.gen_n_filters*2) #64x64x64\n        d3 = downsample(d2,self.gen_n_filters*4) #32x32x128\n        d4 = downsample(d3,self.gen_n_filters*8) #16x16x256\n    \n        u1 = upsample(d4,d3,self.gen_n_filters*4)\n        u2 = upsample(u1,d2,self.gen_n_filters*2)\n        u3 = upsample(u2,d1,self.gen_n_filters)\n        \n        u4 = UpSampling2D(size=2)(u3)\n        Y = Conv2D(self.input_dim[2],kernel_size=4,strides=1,padding='same',kernel_initializer=self.weight_init,activation='tanh')(u4)\n        \n        model = Model(X,Y)\n        \n        return model\n    \n    ## resnet generator\n    def build_generator_resnet(self):\n        \n        def residual_block(layer_input,filters):\n            shortcut = layer_input\n            \n            H = ReflectionPadding2D(padding=(1,1))(layer_input)\n            H = Conv2D(filters=filters,kernel_size=3,strides=1,padding='valid',kernel_initializer=self.weight_init)(H)\n            H = InstanceNormalization(axis=-1,center=False,scale=False)(H)\n            H = Activation('relu')(H)\n            \n            H = ReflectionPadding2D(padding=(1,1))(H)\n            H = Conv2D(filters=filters,kernel_size=3,strides=1,padding='valid',kernel_initializer=self.weight_init)(H)\n            H = InstanceNormalization(axis=-1,center=False,scale=False)(H)\n            \n            Y = add([shortcut,H])\n            \n            return Y\n        \n        def downsample(layer_input,filters):\n            H = Conv2D(filters=filters,kernel_size=3,strides=2,padding='same',kernel_initializer=self.weight_init)(layer_input)\n            H = InstanceNormalization(axis=-1,center=False,scale=False)(H)\n            H = Activation('relu')(H)\n            return H\n            \n        def upsample(layer_input,filters):\n            H = Conv2DTranspose(filters,kernel_size=3,strides=2,padding='same',kernel_initializer=self.weight_init)(layer_input)\n            H = InstanceNormalization(axis=-1,center=False,scale=False)(H)\n            H = Activation('relu')(H)\n            return H\n            \n        X = Input(shape=self.input_dim)\n        \n        H = ReflectionPadding2D(padding=(3,3))(X)\n        H = Conv2D(filters=self.gen_n_filters,kernel_size=7,strides=1,padding='valid',kernel_initializer=self.weight_init)(H)\n        H = InstanceNormalization(axis=-1,center=False,scale=False)(H)\n        H = Activation('relu')(H)\n        \n        d1 = downsample(H,self.gen_n_filters*2)\n        d2 = downsample(d1,self.gen_n_filters*4)\n        \n        r1 = residual_block(d2,self.gen_n_filters*4)\n        r2 = residual_block(r1,self.gen_n_filters*4)\n        r3 = residual_block(r2,self.gen_n_filters*4)\n        r4 = residual_block(r3,self.gen_n_filters*4)\n        r5 = residual_block(r4,self.gen_n_filters*4)\n        r6 = residual_block(r5,self.gen_n_filters*4)\n        r7 = residual_block(r6,self.gen_n_filters*4)\n        r8 = residual_block(r7,self.gen_n_filters*4)\n        r9 = residual_block(r8,self.gen_n_filters*4)\n        \n        u1 = upsample(r9,self.gen_n_filters*2)\n        u2 = upsample(u1,self.gen_n_filters)\n        \n        H = ReflectionPadding2D(padding=(3,3))(u2)\n        H = Conv2D(3,kernel_size=7,strides=1,padding='valid',kernel_initializer=self.weight_init)(H)\n        H = InstanceNormalization(axis=-1,center=False,scale=False)(H)\n        Y = Activation('tanh')(H)\n            \n        model = Model(X,Y)\n        return model\n        \n    def train_discriminators(self,img_A,img_B,ones,zeros):\n        fake_A = self.G_BA.predict(img_B)\n        fake_B = self.G_AB.predict(img_A)\n        \n        D_A_loss_real = self.D_A.train_on_batch(img_A,ones)\n        D_A_loss_fake = self.D_A.train_on_batch(fake_A,zeros)\n        D_A_loss = 0.5*np.add(D_A_loss_real,D_A_loss_fake)\n        \n        D_B_loss_real = self.D_B.train_on_batch(img_B,ones)\n        D_B_loss_fake = self.D_B.train_on_batch(fake_B,zeros)\n        D_B_loss = 0.5*np.add(D_B_loss_real,D_B_loss_fake)\n        \n        D_loss_total = 0.5*np.add(D_A_loss,D_B_loss)\n        \n        return D_loss_total[0],D_loss_total[1]\n        \n    def train_generators(self,img_A,img_B,ones,zeros):\n        \n        return self.combined.train_on_batch([img_A,img_B],[ones,ones,img_A,img_B,img_A,img_B])\n        \n    def train_one_batch(self,img_A,img_B,batch_size=1):\n        ones = np.ones((batch_size,)+self.disc_patch)\n        zeros = np.ones((batch_size,)+self.disc_patch)\n        \n        D_loss,D_acc = self.train_discriminators(img_A,img_B,ones,zeros)\n        G_loss = self.train_generators(img_A,img_B,ones,zeros)[0]\n        \n        self.d_losses.append(D_loss)\n        self.d_accs.append(D_acc)\n        self.g_losses.append(G_loss)\n        self.steps += 1\n        \n        return D_loss,G_loss","9b1a51b7":"cycle = CycleGAN(input_dim=(IMG_HEIGHT,IMG_WIDTH,IMG_CHANNEL),lr=0.0007,weight_valid=1,\\\n                 weight_reconstr=10,weight_id=4,gen_n_filters=32,disc_n_filters=32,generator='unet')","c56a397f":"# \ud604\uc7ac \uc808\ub300 \uacbd\ub85c \/kaggle\/working\n# input \ud30c\uc77c\ub4e4\uc740 \/kaggle\/input \uc5d0 \uc800\uc7a5\ub418\uc5b4\uc788\uc74c\n! mkdir checkpoints\n\n## ckpt path \uc808\ub300\uacbd\ub85c: \/kaggle\/working\/checkpoints\n## ckpt path \uc0c1\ub300\uacbd\ub85c: .\/checkpoints\nckpt_path = \".\/checkpoints\"\n\nckpt = tf.train.Checkpoint(D_A=cycle.D_A, D_B=cycle.D_B, G_AB=cycle.G_AB, G_BA=cycle.G_BA)\nckpt_manager = tf.train.CheckpointManager(ckpt,ckpt_path,max_to_keep=1)\n\nif ckpt_manager.latest_checkpoint:\n    ckpt.restore(ckpt_manager.latest_checkpoint)","c997d750":"def transfer(model,sample_cezanne,sample_photo):\n    cezanne2photo = model.G_AB(sample_cezanne)\n    photo2cezanne = model.G_BA(sample_photo)\n    \n    plt.figure(figsize=(8,8))\n    \n    plt.subplot(221)\n    plt.title(\"sample cezanne\")\n    plt.axis('off')\n    plt.imshow(sample_cezanne[0]*0.5+0.5)\n    \n    plt.subplot(222)\n    plt.title(f\"sample cezanne to photo\")\n    plt.axis('off')\n    plt.imshow(cezanne2photo[0]*0.5+0.5)\n    \n    plt.subplot(223)\n    plt.title('sample photo')\n    plt.axis('off')\n    plt.imshow(sample_photo[0]*0.5+0.5)\n    \n    plt.subplot(224)\n    plt.title(f'sample photo to cezanne')\n    plt.axis('off')\n    plt.imshow(photo2cezanne[0]*0.5+0.5)\n    \n    plt.show()","53ccf821":"EPOCHS = 240\n\nD_losses = []\nG_losses = []\n\nfor epoch in range(EPOCHS):\n    D_loss_sum = 0\n    G_loss_sum = 0\n    count = 0\n    start = datetime.datetime.now()\n    \n    for img_A,img_B in tf.data.Dataset.zip((train_cezanne,train_photo)):\n        D_loss,G_loss = cycle.train_one_batch(img_A,img_B,batch_size=BATCH_SIZE)\n        if count%20 == 0:\n            print('.',end='')\n        D_loss_sum += D_loss\n        G_loss_sum += G_loss\n        count +=1\n        \n    end = datetime.datetime.now()\n    elapsed = end-start\n    \n    cycle.steps = 1\n    cycle.epochs += 1\n    D_losses.append(D_loss_sum\/count)\n    G_losses.append(G_loss_sum\/count)\n    \n    print(f\"[Epoch: {epoch+1} \/ {EPOCHS}]  [D loss: {np.round(np.mean(D_losses),2)}]  [G loss: {np.round(np.mean(D_losses),2)}] [time:{elapsed}]\")\n    \n    if (epoch+1)%200 == 0:\n        save_path = ckpt_manager.save()\n        print(f\"Saving Checkpoint for epoch {epoch+1} at {save_path}\")\n    \n    if epoch+1 in [10,30,60,120,220]:\n        print(f\"Results of Generators after {epoch+1}epochs\")\n        transfer(model=cycle,sample_cezanne=sample_cezanne,sample_photo=sample_photo)","f2c52620":"# plot changes in losses\n\nplt.figure(figsize=(12,12))\n\nplt.subplot(211)\nplt.title(\"Dicriminator Loss\")\nplt.plot(range(len(D_losses)),D_losses)\n\nplt.subplot(212)\nplt.title(\"Generator Loss\")\nplt.plot(range(len(G_losses)),G_losses)\n\nplt.show()","e3f0dce1":"## Generator applied on train image dataset\n\nfor A,B in zip(train_cezanne.take(4),train_photo.take(4)):\n    transfer(model=cycle,sample_cezanne=A,sample_photo=B)\n    print('========================================'*2,'\\n')","3cf83ece":"## Generator applied on test image dataset\n\nfor A,B in zip(test_cezanne.take(4),test_photo.take(4)):\n    transfer(cycle,A,B)\n    print('========================================'*2,'\\n')","34339322":"**Checkpoint object and CheckpointManager object**","e78b1787":"**Function which will be used to display progress in generators while being trained**","7109cb39":"**CycelGAN object building codes**"}}