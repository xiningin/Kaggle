{"cell_type":{"e3d9a695":"code","90d73743":"code","0f102085":"code","b2432c24":"code","75c28cc0":"code","19bc711a":"code","24d168ad":"code","4b253e4d":"code","771c661c":"code","46a7c80f":"code","b133bfa1":"code","58100d4e":"code","37c93b8b":"code","c576ecc9":"code","13a684bc":"code","cc33a7a9":"code","a9578a82":"code","7e8cb3a0":"code","3ca3c0de":"code","cbfd56be":"code","d03e5ff4":"code","f5c0c4f9":"code","3619d184":"code","c78c816e":"code","7c047a88":"code","a542e226":"code","eefde663":"markdown","25a79584":"markdown","69a350bf":"markdown","7206d80b":"markdown","eceb248c":"markdown","fdaa683a":"markdown"},"source":{"e3d9a695":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","90d73743":"images_fpath='\/kaggle\/input\/semantic-drone-dataset\/dataset\/semantic_drone_dataset\/original_images\/'\nmasks_fpath='\/kaggle\/input\/semantic-drone-dataset\/RGB_color_image_masks\/RGB_color_image_masks\/'\n#masks_fpath= '\/kaggle\/input\/semantic-drone-dataset\/dataset\/semantic_drone_dataset\/label_images_semantic\/'\n\n#test_fpath='..\/input\/3d_images\/'\n#test_masks_fpath='..\/input\/3d_masks\/'\nprint(os.listdir('\/kaggle\/input\/semantic-drone-dataset\/dataset\/semantic_drone_dataset\/original_images\/'))\nprint(os.listdir('\/kaggle\/input\/semantic-drone-dataset\/RGB_color_image_masks\/RGB_color_image_masks\/'))\n#print(os.listdir('\/kaggle\/input\/semantic-drone-dataset\/dataset\/semantic_drone_dataset\/label_images_semantic\/'))","0f102085":"from PIL import Image\nimport cv2\nimport os\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom PIL import Image\n\n#for building and training the U-Net model\nfrom tensorflow.keras.layers import Conv2D, Input, Concatenate, MaxPooling2D, Conv2DTranspose\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom tensorflow.keras.models import Model\n\nprint(\"Loaded all libraries\")","b2432c24":"print(\"No. of images =\",len(os.listdir(images_fpath)))\nprint(\"No. of image masks =\",len(os.listdir(masks_fpath)))","75c28cc0":"#1 example let's pick image id '000'\ndef plot_img_and_mask(id):\n    img = cv2.imread(images_fpath + id + '.jpg')\n    img_mask = cv2.imread(masks_fpath + id + '.png')\n    plt.figure(figsize=(10,10))\n    plt.subplot(1,3,1,title='Actual image')\n    plt.imshow(img,cmap='gray')\n    plt.subplot(1,3,3,title='Image mask')\n    plt.imshow(img_mask,cmap='gray')\n    \nplot_img_and_mask('000')","19bc711a":"plot_img_and_mask('140')","24d168ad":"plot_img_and_mask('160')","4b253e4d":"import re\ndef sorted_alphanum(data):\n    convert = lambda text: int(text) if text.isdigit() else text.lower()\n    alphanum_key = lambda key: [ convert(c) for c in re.split('([0-9]+)', key) ] \n    return sorted(data, key=alphanum_key)\n","771c661c":"def load_images_masks(path_images,path_masks):\n    images = os.listdir(path_images)\n    masks = os.listdir(path_masks)\n    images_list = []\n    masks_list = []\n    print(len(images))\n\n    \n    images2 = sorted_alphanum(images)\n    masks2 = sorted_alphanum(masks)\n    \n    \n    for i in images2:\n        im = Image.open(path_images + i)\n        im1 = np.array(im.resize((128,128)))\/255\n        images_list.append(im1)\n \n    for i in masks2:\n        mask = Image.open(path_masks + i)\n        mask1 = np.array(mask.resize((128,128)))\/255\n        masks_list.append(mask1)\n        \n    return images_list,masks_list","46a7c80f":"X,y=load_images_masks(images_fpath,masks_fpath)","b133bfa1":"#quick verification whether all the data is initialized or not\nprint(len(os.listdir(images_fpath)),len(X),len(y))\nprint(X[0].shape,y[0].shape)","58100d4e":"#check data once they are loaded\nplt.figure(figsize=(10,10))\nplt.subplot(1,3,1,title='Original image')\nplt.imshow(X[100],cmap='viridis')\n\nplt.subplot(1,3,3,title='Image mask')\nplt.imshow(y[100],cmap='viridis')","37c93b8b":"X = np.array(X)\ny = np.array(y)","c576ecc9":"print(X.shape)\nprint(y.shape)","13a684bc":"#import all libraries for UNET model\nfrom tensorflow.keras.layers import Conv2D, Input, Concatenate, MaxPooling2D, Conv2DTranspose\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom tensorflow.keras.models import Model","cc33a7a9":"# Build U-Net model - Define layers\ninputs = Input(shape=(128, 128, 3))\n\nc1 = Conv2D(8, (3, 3), activation='relu', padding='same') (inputs)\nc1 = Conv2D(8, (3, 3), activation='relu', padding='same') (c1)\np1 = MaxPooling2D((2, 2)) (c1)\n\nc2 = Conv2D(16, (3, 3), activation='relu', padding='same') (p1)\nc2 = Conv2D(16, (3, 3), activation='relu', padding='same') (c2)\np2 = MaxPooling2D((2, 2)) (c2)\n\nc3 = Conv2D(32, (3, 3), activation='relu', padding='same') (p2)\nc3 = Conv2D(32, (3, 3), activation='relu', padding='same') (c3)\np3 = MaxPooling2D((2, 2)) (c3)\n\nc4 = Conv2D(64, (3, 3), activation='relu', padding='same') (p3)\nc4 = Conv2D(64, (3, 3), activation='relu', padding='same') (c4)\np4 = MaxPooling2D(pool_size=(2, 2)) (c4)\n\nc5 = Conv2D(128, (3, 3), activation='relu', padding='same') (p4)\nc5 = Conv2D(128, (3, 3), activation='relu', padding='same') (c5)\n\nu6 = Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same') (c5)\nu6 = Concatenate()([u6, c4])\nc6 = Conv2D(64, (3, 3), activation='relu', padding='same') (u6)\nc6 = Conv2D(64, (3, 3), activation='relu', padding='same') (c6)\n\nu7 = Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same') (c6)\nu7 = Concatenate()([u7, c3])\nc7 = Conv2D(32, (3, 3), activation='relu', padding='same') (u7)\nc7 = Conv2D(32, (3, 3), activation='relu', padding='same') (c7)\n\nu8 = Conv2DTranspose(16, (2, 2), strides=(2, 2), padding='same') (c7)\nu8 = Concatenate()([u8, c2])\nc8 = Conv2D(16, (3, 3), activation='relu', padding='same') (u8)\nc8 = Conv2D(16, (3, 3), activation='relu', padding='same') (c8)\n\nu9 = Conv2DTranspose(8, (2, 2), strides=(2, 2), padding='same') (c8)\nu9 = Concatenate()([u9, c1]) #removed ,axis=3\nc9 = Conv2D(8, (3, 3), activation='relu', padding='same') (u9)\nc9 = Conv2D(8, (3, 3), activation='relu', padding='same') (c9)\n\noutputs = Conv2D(3, (1, 1), activation='relu') (c9)","a9578a82":"model = Model(inputs=[inputs], outputs=[outputs])\nmodel.compile(optimizer='adam', loss='binary_crossentropy')\nmodel.summary()","7e8cb3a0":"from sklearn.model_selection import train_test_split","3ca3c0de":"x_train,x_test,y_train,y_test = train_test_split(X,y,test_size=0.1)","cbfd56be":"#early_stop = EarlyStopping(patience=5)\n#check_point = ModelCheckpoint('model.hdf5',save_best_only=True)\n#model.fit(x_train, y_train, epochs=20, callbacks=[early_stop,check_point])\nmodel.fit(x_train, y_train, epochs=100)","d03e5ff4":"from keras.utils import plot_model\nplot_model(model,to_file='model.png')","f5c0c4f9":"x_train.shape","3619d184":"pred_test = model.predict(x_test, verbose=1)\n#print(y_train[0])","c78c816e":"import matplotlib.pyplot as plt","7c047a88":"plt.figure(1 , figsize = (15, 9))\nn = 0 \nfor i in range(4):\n    n += 1 \n    #r = np.random.randint(0, x_test.shape[0], 1)\n    r=[i]\n    plt.subplot(2, 2, n)\n    plt.subplots_adjust(hspace = 0.5, wspace = 0.5)\n    plt.imshow(x_test[r[0]])\n    plt.imshow(pred_test[r[0]], alpha=0.5)\n    \n\nplt.suptitle(\"Annotated images...\")\nplt.show()","a542e226":"plt.figure(1 , figsize = (15, 9))\nn = 0 \nfor i in range(6):\n    n += 1 \n    #r = np.random.randint(0, x_test.shape[0], 1)\n    r=[i]\n    plt.subplot(3, 2, n)\n    plt.subplots_adjust(hspace = 0.5, wspace = 0.5)\n    plt.imshow(x_test[r[0]])\n    plt.imshow(y_test[r[0]], alpha=0.5)","eefde663":"def initialize_img_data(folder):\n    lst=[] \n    for image in os.listdir(folder):\n        #load image in grayscale\n        img= cv2.imread(folder+\"\/\"+image)\n        #convert to array\n        img_array=Image.fromarray(img)\n        #resize image\n        resize_img = img_array.resize((128 , 128))\n        #divide by 255 -> scaling data\n        norm_img=np.array(resize_img)\/255\n        #expand dimensions\n        img_array = norm_img.reshape((128,128))#np.expand_dims(norm_img,axis=3)\n        lst.append(img_array)\n        del img, img_array,resize_img,norm_img\n        lst.sort()\n    return lst\n    \n","25a79584":"# Semantic Segmentation Exercise - I\nData Set: https:\/\/www.kaggle.com\/bulentsiyah\/semantic-drone-dataset\nGenerate the mask images for Original images. Labeled mask images are available in labeled_images_semantic. \nHints: \nNeed to choose the proper activation function. \nFeel free to add additional layers.\nTake 10% test data\n","69a350bf":"# End your code here...","7206d80b":"# Start your code here....","eceb248c":"Start your code here....","fdaa683a":"\ndef load_images_masks(path_images,path_masks):\n    images = os.listdir(path_images)\n    masks = os.listdir(path_masks)\n    images_list = []\n    mask_list = []\n\n    for i in images:\n        im = Image.open(path_images + i)\n        im1 = np.array(im.resize((128,128)))\/255\n        images_list.append(im1)\n    del im,im1\n    for i in masks:\n        mask = Image.open(path_masks + i)\n        mask1 = np.array(mask.resize((128,128)))\/255\n        mask_list.append(mask1)\n    del mask,mask1\n    return images_list,mask_list"}}