{"cell_type":{"9d54b3e6":"code","4853f4e3":"code","38dc430b":"code","0493aaeb":"code","7fa7be85":"code","0daf0ce5":"code","5eb6b064":"code","fd8b2f98":"code","b9398c8d":"code","0ae566c0":"code","0c364411":"code","f9a4ca59":"code","745784ad":"code","37f716b2":"code","b14e0722":"code","33ced438":"code","c682611d":"code","4473b0df":"code","46c29f7c":"code","8d94ce19":"code","bfdb7bdb":"code","88734e48":"markdown","31f9e8fb":"markdown","0e2ab886":"markdown","9c527c6c":"markdown","59eb578a":"markdown","de554e2d":"markdown","fc58b479":"markdown"},"source":{"9d54b3e6":"import os\nimport numpy as np\nimport pandas as pd\nimport warnings\nwarnings.simplefilter(\"ignore\")","4853f4e3":"train = pd.read_csv(\"..\/input\/tps-feb-eda-fe\/train_data.csv\")\ntest = pd.read_csv(\"..\/input\/tps-feb-eda-fe\/test_data.csv\")","38dc430b":"train","0493aaeb":"test","7fa7be85":"cat_columns = [f\"cat{i}\" for i in range(10)]","0daf0ce5":"X = train.drop([\"target\"], axis=1)\nX_test = test\ny = train.target\n\nprint(X.shape)\nprint(X_test.shape)\nprint(y.shape)","5eb6b064":"X","fd8b2f98":"X_test","b9398c8d":"y","0ae566c0":"from sklearn.model_selection import KFold\nimport lightgbm as lgb\n\nSEED = 8970365","0c364411":"kf = KFold(n_splits=5, shuffle=True, random_state=SEED)","f9a4ca59":"# \u30d1\u30e9\u30e1\u30fc\u30bf\u306e\u5024\u306f\u4ed6\u306e\u65b9\u306eNotebook\u3092\u53c2\u8003\u306b\u3057\u307e\u3057\u305f\u3002\u611f\u8b1d\u3057\u307e\u3059\u3002\n# The value of the parameter was taken from another person's Notebook.\n# I appreciate it.\n\nparams_lgb = {\n    \"task\": \"train\",\n    \"boosting_type\": \"gbdt\",\n    \"objective\": \"regression\",\n    \"metric\": \"rmse\",\n    \"learning_rate\": 0.003899156646724397,\n    \"num_leaves\": 63,\n    \"max_depth\": 99,\n    \"feature_fraction\": 0.2256038826485174,\n    \"bagging_fraction\": 0.8805303688019942,\n    \"min_child_samples\": 290,\n    \"reg_alpha\": 9.562925363678952,\n    \"reg_lambda\": 9.355810045480153,\n    \"max_bin\": 882,\n    \"min_data_per_group\": 127,\n    \"bagging_freq\": 1,\n    \"cat_smooth\": 96,\n    \"cat_l2\": 19,\n    \"verbosity\": -1,\n    \"bagging_seed\": SEED,\n    \"feature_fraction_seed\": SEED,\n    \"seed\": SEED\n}","745784ad":"# \u4e88\u6e2c\u5024\u3092\u683c\u7d0d\u3059\u308bdf\n# df to store the predicted value\npreds_lgb = pd.DataFrame()\n\nX[cat_columns] = X[cat_columns].astype(\"category\")\nX_test[cat_columns] = X_test[cat_columns].astype(\"category\")\n\nfor k, (tr_id, vl_id) in enumerate(kf.split(X, y)):\n    print(\"=\"*50)\n    print(f\"               KFold{k+1}\")\n    print(\"=\"*50)\n    \n    X_train, X_val = X.iloc[tr_id, :], X.iloc[vl_id, :]\n    y_train, y_val = y.iloc[tr_id], y.iloc[vl_id]\n    \n    lgb_train = lgb.Dataset(X_train, y_train)\n    lgb_val = lgb.Dataset(X_val, y_val)\n    \n    model_lgb = lgb.train(params=params_lgb,\n                          train_set=lgb_train,\n                          valid_sets=lgb_val,\n                          num_boost_round=100000,\n                          early_stopping_rounds=200,\n                          verbose_eval=1000)\n    \n    pred_lgb = model_lgb.predict(X_test, num_iteration=model_lgb.best_iteration)\n    pred_lgb = pd.DataFrame(pred_lgb)\n    \n    # \u4e88\u6e2c\u5024\u3092\u6a2a\u306b\u9023\u7d50\u3057\u3066\u3044\u304f\n    # Concatenate the predictions horizontally\n    preds_lgb = pd.concat([preds_lgb, pred_lgb], axis=1)","37f716b2":"preds_lgb","b14e0722":"# \u5e73\u5747\u3092\u8a08\u7b97\u3057\u3066\u3001\u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\u306b\u5bfe\u3059\u308b\u7591\u4f3c\u30e9\u30d9\u30eb\u3068\u3059\u308b\n# Calculate the mean and use it as a pseudo labels for the test data\n\nlabel = preds_lgb.mean(axis=1)\nlabel","33ced438":"# \u3082\u3068\u3082\u3068\u306e\u5b66\u7fd2\u30c7\u30fc\u30bfX, y\u306b\u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\u3068\u7591\u4f3c\u30e9\u30d9\u30eb\u3092\u7e26\u306b\u9023\u7d50\u3059\u308b\u3002\n# \u3053\u308c\u3092\u65b0\u305f\u306a\u5b66\u7fd2\u30c7\u30fc\u30bf\u3068\u3059\u308b\n# Concatenate the test data and pseudo labels to the original training data X, y.\n# Make this the new training data.\n\nX = pd.concat([X, X_test], axis=0).reset_index(drop=True)\ny = pd.concat([y, label], axis=0).reset_index(drop=True)\n\nprint(\"X.shape: \", X.shape)\nprint(\"y.shape: \", y.shape)","c682611d":"X","4473b0df":"y","46c29f7c":"# \u6700\u7d42\u4e88\u6e2c\u5024\u3092\u683c\u7d0d\u3059\u308bdf\n# df to store the final prediction\npreds_lgb = pd.DataFrame()\n\nX[cat_columns] = X[cat_columns].astype(\"category\")\nX_test[cat_columns] = X_test[cat_columns].astype(\"category\")\n\nfor k, (tr_id, vl_id) in enumerate(kf.split(X, y)):\n    print(\"=\"*50)\n    print(f\"               KFold{k+1}\")\n    print(\"=\"*50)\n    \n    X_train, X_val = X.iloc[tr_id, :], X.iloc[vl_id, :]\n    y_train, y_val = y.iloc[tr_id], y.iloc[vl_id]\n    \n    lgb_train = lgb.Dataset(X_train, y_train)\n    lgb_val = lgb.Dataset(X_val, y_val)\n    \n    model_lgb = lgb.train(params=params_lgb,\n                          train_set=lgb_train,\n                          valid_sets=lgb_val,\n                          num_boost_round=100000,\n                          early_stopping_rounds=200,\n                          verbose_eval=1000)\n    \n    pred_lgb = model_lgb.predict(X_test, num_iteration=model_lgb.best_iteration)\n    pred_lgb = pd.DataFrame(pred_lgb)\n    preds_lgb = pd.concat([preds_lgb, pred_lgb], axis=1)","8d94ce19":"submission = pd.read_csv(\"..\/input\/tabular-playground-series-feb-2021\/sample_submission.csv\")\n\n#\u3000\u4e88\u6e2c\u5024\u306e\u5e73\u5747\u3092\u8a08\u7b97\u3057\u3066\u3001\u6700\u7d42\u7684\u306a\u4e88\u6e2c\u5024\u3068\u3059\u308b\n# Calculate the average of the predictions to get the final prediction.\npred = preds_lgb.mean(axis=1)\nsubmission.target = pred\n\nsubmission.head()","bfdb7bdb":"submission.to_csv(\"submission_pseudo_lgb_5.csv\", index=False)","88734e48":"## \u7591\u4f3c\u30e9\u30d9\u30ea\u30f3\u30b0 \/ Pseudo Labeling\n\u534a\u6559\u5e2b\u3042\u308a\u5b66\u7fd2\u306e\u624b\u6cd5\u306e\u4e00\u3064 \/ One of the methods of semi-supervised learning  \n  \n\u7591\u4f3c\u30e9\u30d9\u30ea\u30f3\u30b0\u306f2\u6bb5\u968e\u306e\u69cb\u6210\u306b\u306a\u3063\u3066\u3044\u308b\u3002  \n\u307e\u305a\u3001\u4f55\u3089\u304b\u306e\u4e88\u6e2c\u30e2\u30c7\u30eb\u3092\u7528\u610f\u3059\u308b\uff08\u4eca\u56de\u306fLightGBM\uff09\u3002  \n\u7b2c1\u6bb5\u968e\u3067\u306f\u3001\u30e2\u30c7\u30eb\u3092\u5b66\u7fd2\u3055\u305b\u305f\u5f8c\u3001\u666e\u901a\u306b\u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\u306b\u5bfe\u3057\u3066\u4e88\u6e2c\u3092\u884c\u3046\u3002\u305d\u306e\u4e88\u6e2c\u5024\u3092\u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\u306b\u5bfe\u3059\u308b\u7591\u4f3c\u30e9\u30d9\u30eb\u3068\u3059\u308b\u3002\u3064\u307e\u308a\u3001\u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\u306b\u5bfe\u3059\u308b\u4e88\u6e2c\u5024\u3092\u7591\u4f3c\u7684\u306b\u76ee\u7684\u5909\u6570(label\u30fbtarget)\u3068\u3057\u3066\u6271\u3046\u3002  \n\u7b2c2\u6bb5\u968e\u3067\u306f\u3001\"\u3082\u3068\u3082\u3068\u306e\u5b66\u7fd2\u30c7\u30fc\u30bf\u306b\u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\u3092\u5408\u4f53\u3055\u305b\u305f\u3082\u306e\"\u3092\u5b66\u7fd2\u30c7\u30fc\u30bf\u3068\u3057\u3066\u7528\u3044\u3066\u3001\u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\u306b\u5bfe\u3059\u308b\u4e88\u6e2c\u3092\u884c\u3046\u3002  \n  \nThe pseudo labeling consists of two steps.  \nFirst, prepare some kind of prediction model (LightGBM in this case).  \nIn the 1st stage, we train the model and make a prediction for the test data. The predicted value is used as a pseudo-label for the test data. In other words, the predicted value for the test data is treated as a pseudo target variable (label\/target).  \nIn the 2nd stage, predictions are made for the test data using the \"original training data combined with the test data\" as the training data. ","31f9e8fb":"\u307e\u305a\u666e\u901a\u306b\u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\u306b\u5bfe\u3057\u3066\u4e88\u6e2c\u3092\u884c\u3046\u3002  \nFirst, make a prediction for the test data as usual.","0e2ab886":"\u3053\u306eNotebook\u3067\u306fPseudo Labeling(\u7591\u4f3c\u30e9\u30d9\u30ea\u30f3\u30b0)\u306b\u3064\u3044\u3066\u8aac\u660e\u3057\u3066\u3044\u307e\u3059\u300211\u4f4d\u306e\u89e3\u6cd5\u306e\u6982\u8981\u306b\u3064\u3044\u3066\u306f\u3053\u3061\u3089\u306eNotebook\u3092\u53c2\u7167\u3057\u3066\u304f\u3060\u3055\u3044\u3002  \n\u3010\u65e5\u672c\u8a9e&English\u3011TPS Feb 11th place solution  \nhttps:\/\/www.kaggle.com\/maostack\/english-tps-feb-11th-place-solution\n  \nThis Notebook describes Pseudo Labeling, see this Notebook for an overview of the 11th solution.  \n\u3010\u65e5\u672c\u8a9e&English\u3011TPS Feb 11th place solution  \nhttps:\/\/www.kaggle.com\/maostack\/english-tps-feb-11th-place-solution","9c527c6c":"\u3053\u306eNotebook\u3068\u540c\u3058\u4e88\u6e2c\u65b9\u6cd5\u3067\u3001\u30d1\u30e9\u30e1\u30fc\u30bf\u306e\u5024\u3092\u5909\u3048\u305f\u308a\u3001\u30b7\u30fc\u30c9\u5024\u3092\u5909\u3048\u305f\u308a\u3001early stopping rounds\u3092\u5909\u3048\u305f\u308a\u306a\u3069\u3092\u3057\u306a\u304c\u3089\u3001\u8907\u6570\u306e\u4e88\u6e2c\u63d0\u51fa\u30d5\u30a1\u30a4\u30eb\u3092\u4f5c\u308b\u3002\u6700\u5f8c\u306b\u305d\u308c\u3089\u3092\u30a2\u30f3\u30b5\u30f3\u30d6\u30eb\u3059\u308b\u3002  \n\u4ee5\u4e0a\u304c11\u4f4d\u306e\u89e3\u6cd5\u3067\u3059\u3002\n  \nUsing the same prediction method as in this Notebook, create multiple prediction submission files, changing the paramters, seed value and early stopping rounds etc.. Finally, ensemble them.  \nThis is the 11th solution.","59eb578a":"# \u30b7\u30f3\u30d7\u30eb\u306a11\u4f4d\u306e\u89e3\u6cd5\u3010\u8a73\u7d30\u3011\n# very simple 11th place solution\u3010Details\u3011","de554e2d":"\u5225\u306eNotebook\u3067\u65e2\u306b\u524d\u51e6\u7406\u3092\u3057\u305f\u30c7\u30fc\u30bf\u3092train, test\u3068\u3057\u3066\u8aad\u307f\u8fbc\u3093\u3067\u3044\u307e\u3059\u3002  \n\u524d\u51e6\u7406\u3068\u3057\u3066\u884c\u3063\u305f\u3053\u3068\u306f\u3001  \n\u30fbtarget\u304c\u5916\u308c\u5024\u306e\u884c\u3092\u9664\u5916(target\u304c4\u3088\u308a\u5c0f\u3055\u3044\u884c\u3092\u9664\u5916)  \n\u30fb\u5909\u6570\"cat6\"\u306b\u3064\u3044\u3066\"G\"\u306f\u5b66\u7fd2\u30c7\u30fc\u30bf\u306b\u3057\u304b\u5b58\u5728\u3057\u306a\u3044(\u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\u3067\u5024\u304cG\u3092\u3068\u308b\u30c7\u30fc\u30bf\u304c\u5b58\u5728\u3057\u306a\u3044)\u306e\u3067\u3001cat6\u306e\u5024\u304cG\u306e\u884c\u3092\u9664\u5916  \n\u30fb\u30ab\u30c6\u30b4\u30ea\u5909\u6570\u306b\u5bfe\u3059\u308bLabel Encoding  \n\u30fbcont\u5217\u306b\u5bfe\u3059\u308bRankGauss\u5909\u63db  \n\u3067\u3059\u3002\u6700\u5f8c\u306eRankGauss\u5909\u63db\u306f\u3001\u6c7a\u5b9a\u6728\u7cfb\u306e\u30e2\u30c7\u30eb\u306b\u306f\u5f71\u97ff\u3092\u4e0e\u3048\u306a\u3044\u306e\u3067\u3057\u306a\u304f\u3066\u3082\u3044\u3044\u306e\u3067\u3059\u304c\u4e00\u5fdc\u3057\u3066\u304a\u304d\u307e\u3057\u305f\u3002  \n\u9664\u5916\u5f8c\u306e\u5b66\u7fd2\u30c7\u30fc\u30bf\u306e\u30c7\u30fc\u30bf\u6570\u306f299963\u306b\u306a\u308a\u307e\u3057\u305f\u300237\u884c\u6e1b\u3063\u305f\u3002\u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\u306e\u6570\u306f\u5909\u308f\u3063\u3066\u3044\u306a\u3044\u3002  \n  \nThe data that has already been preprocessed in another Notebook is loaded as train and test.  \nWhat we did as preprocessing was  \n\u30fbExclude rows where target is an outlier (exclude rows where target is less than 4)  \n\u30fbFor the variable \"cat6\", \"G\" exists only in the training data (there is no data that takes the value G in the test data), so the line with the value G in cat6 is excluded.  \n\u30fbLabel Encoding for categorical variables  \n\u30fbRankGauss transform for cont columns  \nThe RankGauss transformation is not necessary because it does not affect the decision tree model, but I did it just in case.  \nAfter preprocessing, the number of data in the training data is now 299963. 37 rows have been reduced. The number of test data has not changed.","fc58b479":"# Submission"}}