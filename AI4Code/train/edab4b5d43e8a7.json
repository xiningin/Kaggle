{"cell_type":{"1a0f3efb":"code","b41af15b":"code","5621eaf4":"code","20ecda94":"code","82496580":"code","c01dd18d":"code","3a97f670":"code","b911cea0":"code","d4c1c581":"code","2188e1ce":"code","1cd8f1f7":"code","46f681fa":"code","d09226a5":"code","81910726":"code","34bb60c2":"code","69c27c46":"code","41c2dd50":"code","cf8b28bc":"code","d16525ea":"code","0a5724c6":"code","79f067a8":"code","85931fbe":"code","f5ede897":"code","68ddf982":"code","9a7f4746":"code","8a3e9b93":"code","7dc69058":"markdown","1c86d181":"markdown","8d9627eb":"markdown","d5488d8b":"markdown","bf25eaaf":"markdown"},"source":{"1a0f3efb":"%matplotlib inline\n#\u9019\u662fjuoyter notebook\u7684magic word\u02d9\n\nimport matplotlib\nimport matplotlib.pyplot as plt\nfrom IPython import display","b41af15b":"import os\n#\u5224\u65b7\u662f\u5426\u5728jupyter notebook\u4e0a\ndef is_in_ipython():\n    \"Is the code running in the ipython environment (jupyter including)\"\n    program_name = os.path.basename(os.getenv('_', ''))\n\n    if ('jupyter-notebook' in program_name or # jupyter-notebook\n        'ipython'          in program_name or # ipython\n        'jupyter' in program_name or  # jupyter\n        'JPY_PARENT_PID'   in os.environ):    # ipython-notebook\n        return True\n    else:\n        return False\n\n\n#\u5224\u65b7\u662f\u5426\u5728colab\u4e0a\ndef is_in_colab():\n    if not is_in_ipython(): return False\n    try:\n        from google import colab\n        return True\n    except: return False\n\n#\u5224\u65b7\u662f\u5426\u5728kaggke_kernal\u4e0a\ndef is_in_kaggle_kernal():\n    if 'kaggle' in os.environ['PYTHONPATH']:\n        return True\n    else:\n        return False\n\nif is_in_colab():\n    from google.colab import drive\n    drive.mount('\/content\/gdrive')\n\nos.environ['TRIDENT_BACKEND'] = 'pytorch'\nkaggle_kernal=None\nif is_in_kaggle_kernal():\n    os.environ['TRIDENT_HOME'] = '.\/trident'\n    \nelif is_in_colab():\n    os.environ['TRIDENT_HOME'] = '\/content\/gdrive\/My Drive\/trident'\n  \n","5621eaf4":"os.listdir('..\/input\/trident')","20ecda94":"#\u70ba\u78ba\u4fdd\u5b89\u88dd\u6700\u65b0\u7248 \n!pip install cupy\n!pip uninstall tridentx -y\n!pip install ..\/input\/trident\/tridentx-0.7.3.21-py3-none-any.whl\n\nimport re\nimport pandas\nimport json\nimport copy\n\nimport cupy as np\n#\u8abf\u7528trident api\n\nimport random\nfrom tqdm import tqdm\nimport scipy\nimport time\nimport glob\n","82496580":"import trident as T\nfrom trident import *","c01dd18d":"human_faces=glob.glob('..\/input\/face-recognition-dataset\/Extracted Faces\/Extracted Faces\/*\/*.*g')\nanime_faces=glob.glob('..\/input\/animefacedataset\/images\/*.*g')\nrandom.shuffle(anime_faces)\nprint(len(human_faces))\nprint(len(anime_faces))","3a97f670":"# imshow\nfor human_face in human_faces[0:3]:\n    im = cv2.imread(human_face)\n    im2 = im[:,:,::-1]\n    plt.imshow(im2)\n    plt.show()\n\nfor anime_face in anime_faces[0:3]:\n    im = cv2.imread(anime_face)\n    im2 = im[:,:,::-1]\n    plt.imshow(im2)\n    plt.show()","b911cea0":"!pip install face_recognition\n!pip install imutils\n#\u4e0b\u8f09\u9810\u8a13\u7df4\u52d5\u756b\u4eba\u81c9\u6aa2\u6e2c\ndownload_file_from_google_drive('1NckKw7elDjQTllRxttO87WY7cnQwdMqz',  filename='checkpoint_landmark_191116.pth.tar')\n\nimport torch\nimport torch.nn as nn\nimport torch.utils.model_zoo as model_zoo\nfrom collections import OrderedDict\n\n# https:\/\/github.com\/kanosawa\/anime_face_landmark_detection\n# Cacaded Face Alignment\nclass CFA(nn.Module):\n    def __init__(self, output_channel_num, checkpoint_name=None):\n        super(CFA, self).__init__()\n\n        self.output_channel_num = output_channel_num\n        self.stage_channel_num = 128\n        self.stage_num = 2\n\n        self.features = nn.Sequential(\n            nn.Conv2d(  3,  64, kernel_size=3, dilation=1, padding=1), nn.ReLU(inplace=True),\n            nn.Conv2d( 64,  64, kernel_size=3, dilation=1, padding=1), nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=2, stride=2),\n            nn.Conv2d( 64, 128, kernel_size=3, dilation=1, padding=1), nn.ReLU(inplace=True),\n            nn.Conv2d(128, 128, kernel_size=3, dilation=1, padding=1), nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=2, stride=2),\n            nn.Conv2d(128, 256, kernel_size=3, dilation=1, padding=1), nn.ReLU(inplace=True),\n            nn.Conv2d(256, 256, kernel_size=3, dilation=1, padding=1), nn.ReLU(inplace=True),\n\n            # nn.Conv2d(256, 256, kernel_size=3, dilation=1, padding=1), nn.ReLU(inplace=True),\n            # nn.MaxPool2d(kernel_size=2, stride=2),\n            # nn.Conv2d(256, 512, kernel_size=3, dilation=1, padding=1), nn.ReLU(inplace=True),\n            # nn.Conv2d(512, 512, kernel_size=3, dilation=1, padding=1), nn.ReLU(inplace=True),\n            # nn.Conv2d(512, 512, kernel_size=3, dilation=1, padding=1), nn.ReLU(inplace=True))\n\n            nn.Conv2d(256, 256, kernel_size=3, dilation=1, padding=1), nn.ReLU(inplace=True))\n        \n        self.CFM_features = nn.Sequential(\n            #nn.Conv2d(512, 256, kernel_size=3, padding=1), nn.ReLU(inplace=True),\n            nn.Conv2d(256, 256, kernel_size=3, padding=1), nn.ReLU(inplace=True),\n            nn.Conv2d(256, self.stage_channel_num, kernel_size=3, padding=1), nn.ReLU(inplace=True))\n\n        # cascaded regression\n        stages = [self.make_stage(self.stage_channel_num)]\n        for _ in range(1, self.stage_num):\n            stages.append(self.make_stage(self.stage_channel_num + self.output_channel_num))\n        self.stages = nn.ModuleList(stages)\n        \n        # initialize weights\n        if checkpoint_name:\n            snapshot = torch.load(checkpoint_name,map_location=torch.device('cuda'))\n            self.load_state_dict(snapshot['state_dict'])\n        else:\n            self.load_weight_from_dict()\n    \n\n    def forward(self, x):\n        feature = self.features(x)\n        feature = self.CFM_features(feature)\n        heatmaps = [self.stages[0](feature)]\n        for i in range(1, self.stage_num):\n            heatmaps.append(self.stages[i](torch.cat([feature, heatmaps[i - 1]], 1)))\n        return heatmaps\n    \n\n    def make_stage(self, nChannels_in):\n        layers = []\n        layers.append(nn.Conv2d(nChannels_in, self.stage_channel_num, kernel_size=3, padding=1))\n        layers.append(nn.ReLU(inplace=True))\n        for _ in range(4):\n            layers.append(nn.Conv2d(self.stage_channel_num, self.stage_channel_num, kernel_size=3, padding=1))\n            layers.append(nn.ReLU(inplace=True))\n        layers.append(nn.Conv2d(self.stage_channel_num, self.output_channel_num, kernel_size=3, padding=1))\n        return nn.Sequential(*layers)\n\n\n    def load_weight_from_dict(self):\n        model_urls = 'https:\/\/download.pytorch.org\/models\/vgg16-397923af.pth'\n        weight_state_dict = model_zoo.load_url(model_urls)\n        all_parameter = self.state_dict()\n        all_weights   = []\n        for key, value in all_parameter.items():\n            if key in weight_state_dict:\n                all_weights.append((key, weight_state_dict[key]))\n            else:\n                all_weights.append((key, value))\n        all_weights = OrderedDict(all_weights)\n        self.load_state_dict(all_weights)","d4c1c581":"# model.summary\n# print(CFA(output_channel_num=3))","2188e1ce":"import numpy as np\nimport torch\nfrom torchvision import transforms\nimport cv2\nfrom PIL import Image, ImageDraw,ImageFont\n\n \n\n# param\nnum_landmark = 24\n\ncheckpoint_name =os.path.join(get_trident_dir(),'downloads','checkpoint_landmark_191116.pth.tar')\n\n\n# detector\nanime_landmark_detector = CFA(output_channel_num=num_landmark + 1, checkpoint_name=checkpoint_name).cuda()\nanime_landmark_detector.trainable=False\n\ndef detect_anime_faces(img_path):\n    img_width = 128\n    # transform\n    normalize   = transforms.Normalize(mean=[0.5, 0.5, 0.5],\n                                       std=[0.5, 0.5, 0.5])\n    train_transform = [transforms.ToTensor(), normalize]\n    train_transform = transforms.Compose(train_transform)\n\n    # input image & detect face\n    img = cv2.imread(img_path)\n    img = Image.fromarray(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n    draw = ImageDraw.Draw(img)\n    ImageDraw.Draw(img)\n   \n    x_, y_, w_, h_ =0,0,img.width,img.height\n    \n    # adjust face size\n    x = max(x_ - w_ \/ 8, 0)\n    rx = min(x_ + w_ * 9 \/ 8, img.width)\n    y = max(y_ - h_ \/ 4, 0)\n    by = y_ + h_\n    w = rx - x\n    h = by - y\n    \n\n    # draw result of face detection\n    draw.rectangle((x, y, x + w, y + h), outline=(0, 255, 0), width=3)\n\n    # transform image\n    img_tmp = img.crop((x, y, x+w, y+h))\n    img_tmp = img_tmp.resize((img_width, img_width), Image.BICUBIC)\n    img_tmp = train_transform(img_tmp)\n    img_tmp = img_tmp.unsqueeze(0).cuda()\n\n    # estimate heatmap\n    heatmaps = anime_landmark_detector(img_tmp)\n    \n    ## \u70ba\u4ec0\u9ebc\u53d6 -1??\n    heatmaps = heatmaps[-1].cpu().detach().numpy()[0]\n    font = ImageFont.truetype(r'DejaVuSans.ttf', 8)  \n    \n    # calculate landmark position\n    for i in range(num_landmark):\n        heatmaps_tmp = cv2.resize(heatmaps[i], (img_width, img_width), interpolation=cv2.INTER_CUBIC)\n        landmark = np.unravel_index(np.argmax(heatmaps_tmp), heatmaps_tmp.shape)\n        landmark_y = landmark[0] * h \/ img_width\n        landmark_x = landmark[1] * w \/ img_width\n        print(i,'({0},{1})'.format(landmark_x,landmark_y))\n        \n\n        # draw landmarks\n        draw.ellipse((x + landmark_x - 1, y + landmark_y - 1, x + landmark_x + 1, y + landmark_y + 1), fill=(255, 0, 0))\n        if 5<=i<10:\n            draw.text((x + landmark_x - 1, y + landmark_y - 1),str(i), font=font,fill=(0, 64, 128))\n    return img.resize((img.width*5,img.height*5))\n\n# display.display(detect_anime_faces(random.choice(anime_faces)))","1cd8f1f7":"# display.display(detect_anime_faces(random.choice(anime_faces)))","46f681fa":"import dlib\nimport imutils\nfrom imutils.face_utils import *\ndownload_file_from_google_drive('1Nw45nQtnrykB-P6QyIHm70XriJUlSsII',  filename='shape_predictor_68_face_landmarks.dat')\n#\u5ba3\u544a\u81c9\u90e8\u5075\u6e2c\u5668\uff0c\u4ee5\u53ca\u8f09\u5165\u9810\u8a13\u7df4\u7684\u81c9\u90e8\u7279\u5fb5\u9ede\u6a21\u578b\ndetector = dlib.get_frontal_face_detector()\nhuman_landmark_detector = dlib.shape_predictor(os.path.join(get_trident_dir(),'downloads','shape_predictor_68_face_landmarks.dat'))\n\ndef detect_human_faces(imag_path):\n    img=cv2.imread(imag_path)\n    face_rects = detector(img,2)\n    for i, d in enumerate(face_rects):\n        #\u8b80\u53d6\u6846\u5de6\u4e0a\u53f3\u4e0b\u5ea7\u6a19\n        x1 = d.left()\n        y1 = d.top()\n        x2 = d.right()\n        y2 = d.bottom()\n        #\u6839\u64da\u6b64\u5ea7\u6a19\u7bc4\u570d\u8b80\u53d6\u81c9\u90e8\u7279\u5fb5\u9ede\n        shape = human_landmark_detector(img, d)\n        #\u5c07\u7279\u5fb5\u9ede\u8f49\u70banumpy\n        shape = shape_to_np(shape)# (68,2)\n        #\u6309\u7167\u7279\u5fb5\u9ede\u756b\u5713\n        for (x, y) in shape:\n            cv2.circle(img, (x, y), 2, (215, 120, 0), -1)\n        #\u5728\u81c9\u90e8\u4f4d\u7f6e\u6253\u6846\n        cv2.rectangle(img, (x1, y1), (x2, y2), (0, 255, 0), 4, cv2.LINE_AA)\n        img=cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\n    return array2image(img)","d09226a5":"detect_human_faces(random.choice(human_faces))","81910726":"#\u5169\u8005\u7684\u5b9a\u7fa9\u5728\u773c\u775b\u90e8\u5206\u5dee\u7570\u8f03\u5927\uff0c\u56e0\u6b64\u50c5\u63a1\u53d6\u5b9a\u7fa9\u76f8\u5bb9\u7684\u9ede #\u9019\u88e1\u53ea\u6709 18 \u500brrrrr\nlandmark_mapping=OrderedDict()\nlandmark_mapping[0]=0\nlandmark_mapping[1]=8\nlandmark_mapping[2]=16\nlandmark_mapping[3]=17\nlandmark_mapping[4]=19\nlandmark_mapping[5]=21\nlandmark_mapping[6]=22\nlandmark_mapping[7]=24\nlandmark_mapping[8]=26\nlandmark_mapping[9]=33\nlandmark_mapping[10]=36\nlandmark_mapping[12]=39\nlandmark_mapping[15]=42\nlandmark_mapping[17]=45\nlandmark_mapping[20]=48\nlandmark_mapping[21]=51\nlandmark_mapping[22]=54\nlandmark_mapping[23]=57\n","34bb60c2":"   \n    \ndef img2landmarks_human(img_data):\n    results=[]\n    for n in range(len(img_data)):\n        landmarks=[]\n        img=reverse_image_backend_adaption(img_data[n]*127.5+127.5).astype(np.uint8)\n        \n        img=cv2.cvtColor(img,cv2.COLOR_RGB2BGR)\n        \n        face_rects = detector(img,2)\n        \n        \n        for i, d in enumerate(face_rects):\n            if i==0:\n                #\u8b80\u53d6\u6846\u5de6\u4e0a\u53f3\u4e0b\u5ea7\u6a19\n                x1 = d.left()\n                y1 = d.top()\n                x2 = d.right()\n                y2 = d.bottom()\n                #\u6839\u64da\u6b64\u5ea7\u6a19\u7bc4\u570d\u8b80\u53d6\u81c9\u90e8\u7279\u5fb5\u9ede\n                shape = human_landmark_detector(img, d)\n\n                #\u5c07\u7279\u5fb5\u9ede\u8f49\u70banumpy\n                shape = shape_to_np(shape)# (68,2)\n\n                for k in landmark_mapping.keys():\n                    v=landmark_mapping[k]\n                    landmarks.append([shape[v,0]\/128.0,shape[v,1]\/128.0])\n        if len(landmarks)!=18:\n            results.append(np.zeros(36,dtype=np.float32))\n        else:\n            results.append(np.array(landmarks,dtype=np.float32).reshape(-1))\n    \n    \n    results= np.stack(results,axis=0)\n    return results\n\ndef img2landmarks_anime(img_data):\n    img_width=128\n    results=[]\n    for n in range(len(img_data)):\n        landmarks=[]\n        img=img_data[n]\n        \n        heatmaps = anime_landmark_detector(to_tensor(expand_dims(img,0)).cuda())\n        heatmaps = heatmaps[-1].cpu().detach().numpy()[0]\n        \n\n        # calculate landmark position\n        for i in range(24):\n\n            heatmaps_tmp = cv2.resize(heatmaps[i], (img_width, img_width), interpolation=cv2.INTER_CUBIC)\n            landmark = np.unravel_index(np.argmax(heatmaps_tmp), heatmaps_tmp.shape)\n            landmark_y = landmark[0] \/ img_width\n            landmark_x = landmark[1]  \/ img_width \n            if i in landmark_mapping.keys():\n                landmarks.append([landmark_x,landmark_y])\n        results.append(np.array(landmarks,dtype=np.float32).reshape(-1))\n  \n    results= np.stack(results,axis=0)\n    return results\n            ","69c27c46":"ds1=ImageDataset(human_faces,symbol='human_faces')\nds2=ImageDataset(anime_faces,symbol='anime_faces')\n\ndata_provider=DataProvider(traindata=Iterator(data=ds1,unpair=ds2))\ndata_provider.image_transform_funcs=[\n                     Resize((128,128)),\n                     RandomAdjustGamma(gamma_range=(0.6,1.5)),\n                     RandomAdjustContrast(value_range=(0.6, 1.5)),\n                     RandomAdjustHue(value_range=(-0.2, 0.2)),\n                     RandomAdjustSaturation(scale=(0.6,1.4)),#\u8abf\u6574\u98fd\u548c\u5ea6\n                     Normalize(127.5,127.5)]","41c2dd50":"human_imgs,anime_imgs=data_provider.next()\n\nprint(human_imgs.shape)\nprint(anime_imgs.shape)\npred_human_landmarks=img2landmarks_human(human_imgs)\nprint(pred_human_landmarks.shape,pred_human_landmarks.dtype)\npred_anime_landmarks=img2landmarks_anime(anime_imgs)\n\nprint(pred_anime_landmarks.shape,pred_anime_landmarks.dtype)","cf8b28bc":"data_provider.preview_images()","d16525ea":"encoder=Sequential(\n    Conv2d_Block((5,5),32,strides=1,auto_pad=True,activation='leaky_relu',normalization='batch',use_bias=False),#(32,128,128)\n    Conv2d_Block((3,3),64,strides=2,auto_pad=True,activation='leaky_relu',normalization='batch',use_bias=False),#(64,64,64)\n    Conv2d_Block((3,3),64,strides=2,auto_pad=True,activation='leaky_relu',normalization='batch',use_bias=False),#(64,32,32)\n    Conv2d_Block((3,3),128,strides=2,auto_pad=True,activation='leaky_relu',normalization='batch',use_bias=False,dropout_rate=0.2),#(128,16,16)\n    Conv2d_Block((3,3),128,strides=2,auto_pad=True,activation='leaky_relu',normalization='batch',use_bias=False),#(128,8,8)\n    Conv2d_Block((3,3),128,strides=1,auto_pad=True,activation='leaky_relu',normalization=None,use_bias=False),#(128,4,4)\n    Reshape((-1)), #(2048)\n    Dense(256,activation=None,use_bias=False),\n    L2Norm(),\n)\nencoder.share_memory()\n\ndecoder_human=ModuleDict({\n'human_output':\n    Sequential(\n        Dense(128*4*4,activation=None,use_bias=False), #(2048))\n        BatchNorm(),\n        Reshape((128,4,4)), #(128,4,4)\n        Conv2d_Block((3,3),128,strides=1,auto_pad=True,activation='leaky_relu',normalization='batch',use_bias=False) ,#((128,4,4))\n        Upsampling2d(scale_factor=2,mode='pixel_shuffle'),\n        Conv2d_Block((3,3),128,strides=1,auto_pad=True,activation='leaky_relu',normalization='batch',use_bias=False),#(128,8,8)\n        Upsampling2d(scale_factor=2,mode='pixel_shuffle'),\n        Conv2d_Block((3,3),128,strides=1,auto_pad=True,activation='leaky_relu',normalization='batch',use_bias=False,dropout_rate=0.2),#(128,16,16)\n        Upsampling2d(scale_factor=2,mode='pixel_shuffle'),\n        Conv2d_Block((3,3),128,strides=1,auto_pad=True,activation='leaky_relu',normalization='batch',use_bias=False),#(128,32,32)\n        Upsampling2d(scale_factor=2,mode='pixel_shuffle'),\n        Conv2d_Block((3,3),128,strides=1,auto_pad=True,activation='leaky_relu',normalization='batch',use_bias=False),#(128,64,64)\n        Upsampling2d(scale_factor=2,mode='pixel_shuffle'),\n        Conv2d_Block((3,3),128,strides=1,auto_pad=True,activation='leaky_relu',normalization='batch',use_bias=False),#(128,128,128)\n        Conv2d((1,1),3,strides=1,auto_pad=True,activation='tanh',use_bias=False)\n    ),\n'human_landmarks_output':\n    Dense((18*2),activation='sigmoid ')\n},is_multicasting=True)\n\ndecoder_anime=ModuleDict({\n'anime_output':Sequential(\n    Dense(128*4*4,activation=None,use_bias=False), #(2048))\n    BatchNorm(),\n    Reshape((128,4,4)), #(128,4,4)\n    Conv2d_Block((3,3),128,strides=1,auto_pad=True,activation='leaky_relu',normalization='batch',use_bias=False) ,#((128,4,4))\n    Upsampling2d(scale_factor=2,mode='pixel_shuffle'),\n    Conv2d_Block((3,3),128,strides=1,auto_pad=True,activation='leaky_relu',normalization='batch',use_bias=False),#(128,8,8)\n    Upsampling2d(scale_factor=2,mode='pixel_shuffle'),\n    Conv2d_Block((3,3),128,strides=1,auto_pad=True,activation='leaky_relu',normalization='batch',use_bias=False,dropout_rate=0.2),#(128,16,16)\n    Upsampling2d(scale_factor=2,mode='pixel_shuffle'),\n    Conv2d_Block((3,3),128,strides=1,auto_pad=True,activation='leaky_relu',normalization='batch',use_bias=False),#(128,32,32)\n    Upsampling2d(scale_factor=2,mode='pixel_shuffle'),\n    Conv2d_Block((3,3),128,strides=1,auto_pad=True,activation='leaky_relu',normalization='batch',use_bias=False),#(128,64,64)\n    Upsampling2d(scale_factor=2,mode='pixel_shuffle'),\n    Conv2d_Block((3,3),128,strides=1,auto_pad=True,activation='leaky_relu',normalization='batch',use_bias=False),#(128,128,128)\n    Conv2d((1,1),3,strides=1,auto_pad=True,activation='tanh',use_bias=False)\n    ),\n'anime_landmarks_output':\n    Dense((18*2),activation='sigmoid ')\n},is_multicasting=True)","0a5724c6":"test = Model(input_shape=(3,128,128), output=\n\nSequential(\n    Conv2d_Block((5,5),32,strides=1,auto_pad=True,activation='leaky_relu',normalization='batch',use_bias=False),#(32,128,128)\n    Conv2d_Block((3,3),64,strides=2,auto_pad=True,activation='leaky_relu',normalization='batch',use_bias=False),#(64,64,64)\n    Conv2d_Block((3,3),64,strides=2,auto_pad=True,activation='leaky_relu',normalization='batch',use_bias=False),#(64,32,32)\n    Conv2d_Block((3,3),128,strides=2,auto_pad=True,activation='leaky_relu',normalization='batch',use_bias=False,dropout_rate=0.2),#(128,16,16)\n    Conv2d_Block((3,3),128,strides=2,auto_pad=True,activation='leaky_relu',normalization='batch',use_bias=False),#(128,8,8)\n    Conv2d_Block((3,3),128,strides=1,auto_pad=True,activation='leaky_relu',normalization=None,use_bias=False),#(128,4,4)\n    Reshape((-1)), #(2048)\n    Dense(256,activation=None,use_bias=False),\n    L2Norm(),\n    ModuleDict({\n'human_output':\n    Sequential(\n        Dense(128*4*4,activation=None,use_bias=False), #(2048))\n        BatchNorm(),\n        Reshape((128,4,4)), #(128,4,4)\n        Conv2d_Block((3,3),128,strides=1,auto_pad=True,activation='leaky_relu',normalization='batch',use_bias=False) ,#((128,4,4))\n        Upsampling2d(scale_factor=2,mode='pixel_shuffle'),\n        Conv2d_Block((3,3),128,strides=1,auto_pad=True,activation='leaky_relu',normalization='batch',use_bias=False),#(128,8,8)\n        Upsampling2d(scale_factor=2,mode='pixel_shuffle'),\n        Conv2d_Block((3,3),128,strides=1,auto_pad=True,activation='leaky_relu',normalization='batch',use_bias=False,dropout_rate=0.2),#(128,16,16)\n        Upsampling2d(scale_factor=2,mode='pixel_shuffle'),\n        Conv2d_Block((3,3),128,strides=1,auto_pad=True,activation='leaky_relu',normalization='batch',use_bias=False),#(128,32,32)\n        Upsampling2d(scale_factor=2,mode='pixel_shuffle'),\n        Conv2d_Block((3,3),128,strides=1,auto_pad=True,activation='leaky_relu',normalization='batch',use_bias=False),#(128,64,64)\n        Upsampling2d(scale_factor=2,mode='pixel_shuffle'),\n        Conv2d_Block((3,3),128,strides=1,auto_pad=True,activation='leaky_relu',normalization='batch',use_bias=False),#(128,128,128)\n        Conv2d((1,1),3,strides=1,auto_pad=True,activation='tanh',use_bias=False)\n    ),\n'human_landmarks_output':\n    Dense((18*2),activation='sigmoid')\n},is_multicasting=True)\n))\ntest.summary()","79f067a8":"autoencoder_human=Model(input_shape=(3,128,128), output=Sequential(encoder,decoder_human))\nautoencoder_anime=Model(input_shape=(3,128,128), output=Sequential(encoder,decoder_anime))\nprint(autoencoder_human.signature)\nprint(autoencoder_anime.signature)\nautoencoder_human.summary()\nautoencoder_anime.summary()\n#autoencoder_combine.summary()","85931fbe":"import cv2\ndef update_datafeed_human(training_context):\n    training_context['data_feed']=OrderedDict()\n    training_context['data_feed']['x']='human_faces'\n    training_context['data_feed']['output']='human_output'\n    training_context['data_feed']['target']='human_faces'\n    training_context['train_data']['human_landmarks']=to_tensor(img2landmarks_human(to_numpy(training_context['train_data']['human_faces']).copy()))\n    \ndef update_datafeed_anime(training_context):\n    training_context['data_feed']=OrderedDict()\n    training_context['data_feed']['x']='anime_faces'\n    training_context['data_feed']['output']='anime_output'\n    training_context['data_feed']['target']='anime_faces'\n    training_context['train_data']['anime_landmarks']=to_tensor(img2landmarks_anime(to_numpy(training_context['train_data']['anime_faces']).copy()))\n    if training_context['steps']>0 and training_context['steps']%len(data_provider.traindata.data)==0:\n        random.shuffle(data_provider.traindata.unpair.items)\n    \n    \ndef draw_human(training_context):\n    model=training_context['current_model']\n    model.eval()\n    train_data=training_context['train_data']\n    input_data=to_numpy(train_data['human_faces'])\n    result_data=model(input_data)\n    output_data=to_numpy(result_data['human_output'])\n    landmark_data=np.round(to_numpy(result_data['human_landmarks_output']).reshape((len(input_data),18,2))*128).astype(np.uint8)\n    input_list=[]\n    output_list=[]\n    landmarks_list=[]\n    for i in range(4):\n        input_list.append(reverse_image_backend_adaption(np.clip(input_data[i]*127.5+127.5,0,255)))\n        img=reverse_image_backend_adaption(np.clip(output_data[i]*127.5+127.5,0,255)).astype(np.float32).copy()\n        output_list.append(img.copy())\n        #print(img.shape)\n        #output=cv2.cvtColor(output,cv2.COLOR_RGB2GBR)\n        for (x, y) in landmark_data[i]:\n            \n            cv2.circle(img, (x, y), 4, (255, 255, 0), -1)\n        #output=cv2.cvtColor(output,cv2.COLOR_BGR2RGB)\n        landmarks_list.append(img)\n    input_list=np.concatenate(input_list,axis=1)\n    output_list=np.concatenate(output_list,axis=1)\n    landmarks_list=np.concatenate(landmarks_list,axis=1)\n    model.train()\n    final=np.concatenate([input_list,output_list,landmarks_list],axis=0)\n\n    display.display(array2image(final))\n        \n    \ndef draw_anime(training_context):\n    model=training_context['current_model']\n    model.eval()\n    train_data=training_context['train_data']\n    input_data=to_numpy(train_data['anime_faces'])\n    result_data=model(input_data)\n    output_data=to_numpy(result_data['anime_output'])\n    landmark_data=np.round(to_numpy(result_data['anime_landmarks_output']).reshape((len(input_data),18,2))*128).astype(np.uint8)\n    input_list=[]\n    output_list=[]\n    landmarks_list=[]\n    for i in range(4):\n        input_list.append(reverse_image_backend_adaption(np.clip(input_data[i]*127.5+127.5,0,255)))\n        img=reverse_image_backend_adaption(np.clip(output_data[i]*127.5+127.5,0,255)).astype(np.float32).copy()\n        output_list.append(img.copy())\n        #print(img.shape)\n        #output=cv2.cvtColor(output,cv2.COLOR_RGB2GBR)\n        for (x, y) in landmark_data[i]:\n            cv2.circle(img, (x, y), 4, (255, 255, 0), -1)\n        #output=cv2.cvtColor(output,cv2.COLOR_BGR2RGB)\n        landmarks_list.append(img)\n    input_list=np.concatenate(input_list,axis=1)\n    output_list=np.concatenate(output_list,axis=1)\n    landmarks_list=np.concatenate(landmarks_list,axis=1)\n    model.train()\n    final=np.concatenate([input_list,output_list,landmarks_list],axis=0)\n\n    display.display(array2image(final))\n    \n    \n\n    \n# autoencoder_human.training_context['data_feed']=OrderedDict()\n# autoencoder_human.training_context['data_feed']['x']='human_faces'\n# autoencoder_human.training_context['data_feed']['output']='human_output'\n# autoencoder_human.training_context['data_feed']['target']='human_faces'\n\n\n# autoencoder_anime.training_context['data_feed']=OrderedDict()\n# autoencoder_anime.training_context['data_feed']['x']='anime_faces'\n# autoencoder_anime.training_context['data_feed']['output']='anime_output'\n# autoencoder_anime.training_context['data_feed']['target']='anime_faces'","f5ede897":"# def huber_loss(e,d):\n#     return (abs(e)<=d)*e**2\/2 + (e>d)*d*(abs(e)-d\/2)\n\ndef anime_landmark_huber_loss(anime_landmarks_output,anime_landmarks):\n    d =1\n    e = anime_landmarks_output - anime_landmarks\n    return (abs(e)<=d)*e**2\/2 + (e>d)*d*(abs(e)-d\/2)\n\ndef human_landmark_huber_loss(human_landmarks_output,human_landmarks):\n    d =1\n    e = human_landmarks_output - human_landmarks\n    return (abs(e)<=d)*e**2\/2 + (e>d)*d*(abs(e)-d\/2)","68ddf982":"def anime_landmark_loss(anime_landmarks_output,anime_landmarks):\n    return ((anime_landmarks_output-anime_landmarks)**2).sum()\/len(anime_landmarks_output)\n\ndef human_landmark_loss(human_landmarks_output,human_landmarks):\n    agg_target=human_landmarks.sum(-1)\n    mask=agg_target!=0\n    return ((human_landmarks_output[mask,:]-human_landmarks[mask,:])**2).sum()\/mask.float().sum()\n\ndef anime_rmse_landmarks(anime_landmarks_output,anime_landmarks):\n    return rmse(anime_landmarks_output,anime_landmarks)\n\ndef human_rmse_landmarks(human_landmarks_output,human_landmarks):\n    agg_target=human_landmarks.sum(-1)\n    mask=agg_target!=0\n    return rmse(human_landmarks_output[mask,:],human_landmarks[mask,:])\n\n# def huber_loss(y_true, y_pred, clip_delta=1.0):\n#     error = y_true - y_pred\n#     cond  = tf.keras.backend.abs(error) < clip_delta\n\n#     squared_loss = 0.5 * tf.keras.backend.square(error)\n#     linear_loss  = clip_delta * (tf.keras.backend.abs(error) - 0.5 * clip_delta)\n\n#     return tf.where(cond, squared_loss, linear_loss)\n\nimport numpy as np\nimport tensorflow as tf\n\n# def anime_landmark_huber_loss(anime_landmarks_output, anime_landmarks, clip_delta=1.0):\n#     error = anime_landmarks_output - anime_landmarks\n#     cond  = tf.keras.backend.abs(error) < clip_delta\n#     squared_loss = 0.5 * tf.keras.backend.square(error)\n#     linear_loss  = clip_delta * (tf.keras.backend.abs(error) - 0.5 * clip_delta)\n#     return tf.where(cond, squared_loss, linear_loss)\n\n# def human_landmark_huber_loss(human_landmarks_output, human_landmarks, clip_delta=1.0):\n#     error = human_landmarks_output - human_landmarks\n#     cond  = tf.keras.backend.abs(error) < clip_delta\n#     squared_loss = 0.5 * tf.keras.backend.square(error)\n#     linear_loss  = clip_delta * (tf.keras.backend.abs(error) - 0.5 * clip_delta)\n#     return tf.where(cond, squared_loss, linear_loss)\n\nautoencoder_human.with_optimizer(optimizer=DiffGrad,lr=2e-4,betas=(0.9, 0.999))\\\n    .with_loss(L1Loss,loss_weight=8)\\\n    .with_loss(L2Loss,loss_weight=16)\\\n    .with_loss(human_landmark_loss,loss_weight=0.4)\\\n    .with_loss(MS_SSIMLoss(window_size=11, window_sigma=1.5, data_range=255),loss_weight=16)\\\n    .with_metric(rmse,name='rmse')\\\n    .with_metric(psnr,name='psnr')\\\n    .with_metric(human_rmse_landmarks,name='landmark_rmse',print_only=True)\\\n    .with_regularizer('l2',reg_weight=1e-5)\\\n    .with_initializer(kaiming_normal)\\\n    .with_model_save_path('.\/Models\/autoencoder_human.pth')\\\n    .with_accumulate_grads(5) \\\n    .with_learning_rate_scheduler(CosineLR(frequency=3000,unit='batch',max_lr=2e-4, min_lr=1e-8))\\\n    .trigger_when(when='on_batch_end',frequency=100,unit='batch',action=draw_human)\\\n    .trigger_when(when='on_data_received',frequency=1,unit='batch',action=update_datafeed_human)\\\n    .with_automatic_mixed_precision_training()\n\nautoencoder_anime.with_optimizer(optimizer=DiffGrad,lr=2e-4,betas=(0.9, 0.999))\\\n    .with_loss(L1Loss,loss_weight=8)\\\n    .with_loss(L2Loss,loss_weight=16)\\\n    .with_loss(anime_landmark_loss,loss_weight=0.4)\\\n    .with_loss(MS_SSIMLoss(window_size=11, window_sigma=1.5, data_range=255),loss_weight=16)\\\n    .with_loss(EdgeLoss,loss_weight=1)\\\n    .with_metric(rmse,name='rmse')\\\n    .with_metric(psnr,name='psnr')\\\n    .with_metric(anime_rmse_landmarks,name='landmark_rmse',print_only=True)\\\n    .with_regularizer('l2',reg_weight=1e-5)\\\n    .with_initializer(kaiming_normal)\\\n    .with_model_save_path('.\/Models\/autoencoder_anime.pth')\\\n    .with_accumulate_grads(5) \\\n    .with_learning_rate_scheduler(CosineLR(frequency=3000,unit='batch',max_lr=2e-4, min_lr=1e-8))\\\n    .trigger_when(when='on_batch_end',frequency=100,unit='batch',action=draw_anime)\\\n    .trigger_when(when='on_data_received',frequency=1,unit='batch',action=update_datafeed_anime)\\\n    .with_automatic_mixed_precision_training()\n\n\n#\u5982\u679c\u8981\u9084\u539f\u5b58\u6a94\u7684\u6a21\u578b\uff0c\u8a18\u5f97\u8981\u5728with_initializer\u4e4b\u5f8c\u518d\u9084\u539f\uff0c\u5426\u5247\u88ab\u521d\u59cb\u5316\u6389\u7b49\u65bc\u767d\u8a13\u7df4\u4e86\u3002\n\n#\u9019\u6bb5\u8f09\u5165\u6a21\u578b\u70ba\u4f55\u5beb\u7684\u6bd4\u8f03\u8907\u96dc\uff0c\u56e0\u70ba\u9019\u6b21\u6d89\u53ca\u5230\u6a21\u578b\u7d50\u69cb\u4e5f\u767c\u751f\u4e86\u8b8a\u5316\uff0c\u6240\u4ee5\u7576\u4ed6\u6aa2\u67e5\u5230\u9019\u500b\u6a21\u578b\u7d50\u69cb\u4e0d\u4e00\u81f4(\u6211\u5011\u4e4b\u524d\u7248\u672c\u7684autoencoder)\n#\u9019\u908a\u6703\u76f4\u63a5\u5354\u52a9\u4fee\u6b63\u7d50\u69cb\u8b8a\u66f4\uff0c\u9019\u6a23\u5927\u5bb6\u5c31\u53ef\u4ee5\u6cbf\u7528\u4e4b\u524d\u8a13\u7df4\u7684\u6a21\u578b\u4e86\nis_resume=True\nif is_resume and os.path.exists('.\/Models\/autoencoder_human.pth'):\n    try:\n        autoencoder_human.load_model('.\/Models\/autoencoder_human.pth')\n    except:\n        with torch.no_grad():\n            old_model = torch.load('.\/Models\/autoencoder_human.pth',map_location=torch.device('cuda'))\n            for (k, v), (k1, v1) in zip(old_model[1].named_parameters(), autoencoder_human.model[1]['human_output'].named_parameters()):\n                if int_shape(v) == int_shape(v1) and k1.replace('1.human_output.', '1.') == k:\n                    v1 = v\n                    print(k1, ' load weight success')\n                else:\n                    print(k1, ' load weight fail')\n\nelif os.path.exists('..\/input\/face-avatar\/Models\/autoencoder_human.pth'):\n\n    try:\n        autoencoder_human.load_model('..\/input\/face-avatar\/Models\/autoencoder_human.pth')\n    except:\n        old_model = torch.load('..\/input\/face-avatar\/Models\/autoencoder_human.pth',map_location=torch.device('cuda'))\n        with torch.no_grad():\n            for (k, v), (k1, v1) in zip(old_model[1].named_parameters(), autoencoder_human.model[1]['human_output'].named_parameters()):\n                if int_shape(v) == int_shape(v1) and k1.replace('1.human_output.', '1.') == k:\n                    v1.copy_(v)\n                    print(k1, ' load weight success')\n                else:\n                    print(k1, ' load weight fail')\n\nif os.path.exists('.\/Models\/autoencoder_anime.pth'):\n    try:\n        autoencoder_anime.load_model('.\/Models\/autoencoder_anime.pth')\n    except:\n        \n        old_model = torch.load('.\/Models\/autoencoder_anime.pth')\n        with torch.no_grad():\n            for (k, v), (k1, v1) in zip(old_model[1].named_parameters(), autoencoder_anime.model[1]['anime_output'].named_parameters()):\n                if int_shape(v) == int_shape(v1) and k1.replace('1.anime_output.', '1.') == k:\n                    v1 = v\n                    print(k1, ' load weight success')\n                else:\n                    print(k1, ' load weight fail')\n\nelif os.path.exists('..\/input\/face-avatar\/Models\/autoencoder_anime.pth'):\n\n    try:\n        autoencoder_anime.load_model('..\/input\/face-avatar\/Models\/autoencoder_anime.pth')\n    except:\n        old_model = torch.load('..\/input\/face-avatar\/Models\/autoencoder_anime.pth',map_location=torch.device('cuda'))\n        with torch.no_grad():\n            for (k, v), (k1, v1) in zip(old_model[1].named_parameters(), autoencoder_anime.model[1]['anime_output'].named_parameters()):\n                if int_shape(v) == int_shape(v1) and k1.replace('1.anime_output.', '1.') == k:\n                    v1.copy_(v)\n                    print(k1, ' load weight success')\n                else:\n                    print(k1, ' load weight fail')\n    \n#\u78ba\u8a8d\u4e00\u4e0b\u5169\u500b\u81ea\u7de8\u78bc\u5668\u7684\u7de8\u78bc\u5668\u6b0a\u503c\u662f\u5426\u4e00\u81f4\nfor p1,p2 in zip(autoencoder_human.model[0].parameters(),autoencoder_anime.model[0].parameters()):\n    print(np.array_equal(to_numpy(p1),to_numpy(p2)))\n","9a7f4746":"plan=TrainingPlan()\\\n    .add_training_item(autoencoder_anime,name='anime')\\\n    .add_training_item(autoencoder_human,name='human')\\\n    .with_data_loader(data_provider)\\\n    .repeat_epochs(500)\\\n    .with_batch_size(64)\\\n    .print_progress_scheduling(10,unit='batch')\\\n    .save_model_scheduling(100,unit='batch')\\\n    .display_loss_metric_curve_scheduling(frequency=100,unit='batch',imshow=True)","8a3e9b93":"plan.start_now()","7dc69058":"\u63a5\u4e0b\u4f86\u5247\u662f\u8981\u5ba3\u544aencoder\u4ee5\u53ca\u5169\u500bdecoder\u7684\u5c0d\u61c9\u7d50\u69cb\u3002\u5728\u6b64\uff0cdecoder\u4e0d\u5efa\u8b70\u4f7f\u7528TransConv_2d\uff0c\u56e0\u70ba\u5bb9\u6613\u7522\u751f\u683c\u5b50\u72c0\u7684\u507d\u5f71\u3002\u5efa\u8b70\u6539\u4f7f\u7528Pixel_shuffle\uff0c\u4f46\u662f\u5176\u7f3a\u9ede\u662f\u5b83\u5f97\u9760\u901a\u9053\u6578\u8b8a\u6210\u539f\u4f86\u76841\/4\u4f86\u63db\u53d6\u5716\u7247\u9577\u5bec\u500d\u589e\uff0c\u6240\u4ee5\u4f7f\u7528\u5b8cPixel_shuffle\uff0c\u8981\u9032\u884c\u5347\u7dad\u7684\u52d5\u4f5c\uff0c\u9019\u6a23\u901a\u9053\u6578\u624d\u4e0d\u6703\u5feb\u901f\u6d88\u5931\u3002","1c86d181":"\u5728\u6b64\u6211\u5011\u52a0\u5165\u5169\u500b\u6578\u64da\u96c6\u505a\u70ba\u8cc7\u6599\u4f86\u6e90\uff0c\u771f\u4eba\u81c9\u7167\u7247\u4f86\u81ea\u65bc\u300cface-recognition-dataset\u300d\uff0c\u5c0d\u8a71\u4eba\u81c9\u4f86\u81ea\u65bc\u300canimefacedataset\u300d\uff0c\u900f\u904eglob\u8a9e\u6cd5\u6293\u53d6\u6240\u6709\u5716\u6a94\u8def\u5f91\uff0c\u4e26\u8a08\u7b97\u5716\u7247\u6578\u91cf\u3002","8d9627eb":"\u9084\u8a18\u5f97\u8ab2\u7a0b\u4e2d\u63d0\u5230\u5982\u4f55\u63d0\u5347\u7279\u5fb5\u5411\u91cf\u7684\u8868\u9054\u80fd\u529b\uff0c\u90a3\u5c31\u662f\u7d66\u5b83\u76f8\u95dc\u7684\u5b50\u4efb\u52d9\uff0c\u9019\u6a23\u6a21\u578b\u5728\u8a13\u7df4\u904e\u7a0b\u4e2d\uff0c\u5c31\u6703\u5c07\u6b64\u4efb\u52d9\u6d89\u53ca\u7684\u77e5\u8b58\u8207\u4fe1\u606f\u69ae\u7279\u5fb5\u5411\u91cf\u4e4b\u4e2d\u3002\u90a3\u95dc\u65bc\u4eba\u81c9(\u5c24\u5176\u662f\u9700\u8981\u878d\u5408\u7684\u662f\u771f\u4eba\u8207\u52d5\u756b\u7684\u81c9)\uff0c\u90a3\u4f86\u6709\u751a\u9ebc\u6bd4\u4eba\u81c9\u7279\u5fb5\u9ede\u6aa2\u6e2c\u66f4\u597d\u7684\u5b50\u4efb\u52d9\u5462\u3002\n\n\u53ea\u4e0d\u904e\u4eba\u81c9\u7279\u5fb5\u9ede\u6aa2\u6e2c\u4e0d\u96e3\u627e\uff0c\u4f46\u662f\u52d5\u756b\u4eba\u81c9\u7279\u5fb5\u9ede\u6aa2\u6e2c\u5c31\u6bd4\u8f03\u7a00\u5c11\u4e86\u3002\u4f46\u9084\u662f\u88ab\u6211\u627e\u5230\u4e86\u4e00\u500b\u7bc4\u4f8b\u3002","d5488d8b":"\u6309\u7167\u5716\u793a\uff0c\u53ef\u4ee5\u5224\u65b7\u52d5\u756b\u4eba\u81c924\u95dc\u9375\u9ede\u7684\u5b9a\u7fa9\u5206\u5225\u662f(\u4ee5\u4e0b\u5de6\u53f3\u6307\u6211\u5011\u8996\u89d2\u770b\u5230\u4e4b\u5de6\u53f3\uff0c\u975e\u4eba\u7269\u81ea\u8eab\u8996\u89d2\u4e4b\u5de6\u53f3):  \n0: \u5de6\u8033(\u5716\u7247\u5de6\u65b9)  \n1: \u4e0b\u5df4  \n2: \u53f3\u8033  \n3: \u5de6\u7709\u5c3e  \n4: \u5de6\u7709\u4e2d\u5fc3  \n5: \u5de6\u7709\u982d  \n6: \u53f3\u7709\u982d  \n7: \u53f3\u7709\u4e2d\u5fc3  \n8: \u53f3\u7709\u5c3e  \n9: \u9f3b\u982d  \n10: \u5de6\u773c\u5916\u5074  \n11: \u5de6\u773c\u4e0a\u65b9  \n12: \u5de6\u773c\u4e0a\u65b9\u5167\u5074  \n13: \u5de6\u773c\u4e0b\u65b9  \n14: \u5de6\u773c\u77b3\u5b54   \n15: \u53f3\u773c\u4e0a\u65b9\u5167\u5074  \n16: \u53f3\u773c\u4e0a\u65b9   \n17: \u53f3\u773c\u5916\u5074   \n18: \u53f3\u773c\u4e0b\u65b9  \n19: \u53f3\u773c\u77b3\u5b54  \n20: \u5634\u5507\u5de6\u5074    \n21: \u5634\u5507\u4e0a\u65b9  \n22: \u5634\u5507\u53f3\u5074  \n23: \u5634\u5507\u4e0b\u65b9  \n","bf25eaaf":"\u7e3d\u5171\u4e00\u500bencoder\uff0c\u7531\u65bc\u771f\u4eba\u8207\u52d5\u756b\u4eba\u7269\u5171\u7528\u540c\u4e00\u500bencoder\uff0c\u6240\u4ee5\u7de8\u78bc\u5668\u6703\u50be\u5411\u4fdd\u7559\u771f\u4eba\u8207\u52d5\u756b\u5171\u540c\u7684\u7279\u6027(\u81c9\u7684\u4fe1\u606f)\uff0c\u4f46\u662f\u6709\u5169\u500b\u7368\u7acb\u7684\u89e3\u78bc\u5668\uff0c\u4e00\u500b\u662fdecoder_human\u8ca0\u8cac\u89e3\u78bc\u771f\u4eba\u5716\u7247\uff0c\u4e00\u500b\u662fdecoder_anime\u8ca0\u8cac\u89e3\u78bc\u52d5\u756b\u4eba\u7269\u3002\u6211\u5011\u7e3d\u5171\u53ef\u4ee5\u8a2d\u8a08\u51fa\u5169\u500b\u81ea\u7de8\u78bc\u5668\uff0c\u5169\u8005\u5171\u7528\u76f8\u540c\u7684\u7de8\u78bc\u5668\u3002\u70ba\u4e86\u907f\u514d\u8de8\u57f7\u884c\u7e8c\u5b58\u53d6\u7570\u5e38\uff0c\u7de8\u78bc\u5668\u8981\u57f7\u884cshare_memory()\u64cd\u4f5c\u3002"}}