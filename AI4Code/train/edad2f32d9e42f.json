{"cell_type":{"50878808":"code","51b1effa":"code","8fb97668":"code","a520021c":"code","c102a002":"code","9c4f7410":"code","7881c3fe":"code","d823c4a8":"code","8a2ec0b7":"code","82a9b44b":"code","434b47f9":"code","65990f91":"code","7441cbad":"code","7610847b":"code","50411182":"code","d714a241":"code","77803ed4":"code","05e518e8":"code","9dd15cca":"code","2c69a1c2":"code","9e44b33d":"code","71df2592":"code","34da0487":"code","437be15c":"code","884073e6":"code","140eb7cc":"code","66948f0e":"code","ff942e20":"code","921b8f34":"code","917ce392":"code","0025de5f":"code","54eafb26":"code","af00a603":"code","983c3f9b":"code","28f23c45":"code","eeea19b4":"code","484d51d4":"code","8d6d4c8b":"code","8b2629d1":"code","a1c77db8":"markdown","04682c88":"markdown","2574fb85":"markdown","c248d17d":"markdown","c1fde30a":"markdown","e8025781":"markdown","2a606156":"markdown","089a919a":"markdown","0f230336":"markdown","8518fe5a":"markdown","15c8d1fc":"markdown","0e4aed6b":"markdown"},"source":{"50878808":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","51b1effa":"# imports\n\nimport os\nimport re\nimport json\nimport warnings\n\nimport pandas as pd\nimport numpy as np\nfrom nltk.stem import PorterStemmer\nfrom nltk.tokenize import RegexpTokenizer\nfrom matplotlib import pyplot as plt\nfrom sklearn.externals import joblib\nfrom sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, RandomizedSearchCV\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import BernoulliNB\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.svm import LinearSVC\nfrom IPython.display import FileLink\n\n# config\n\nwarnings.filterwarnings('ignore')\ndata_path = '..\/input\/'\n%matplotlib inline\n# os.chdir(data_path)","8fb97668":"lbl_train = pd.read_csv(data_path+'labeledTrainData.tsv', sep='\\t')\nprint(\"Shape: {}\".format(lbl_train.shape))\nprint(lbl_train.columns)","a520021c":"unlbl_train = pd.read_csv(data_path+'unlabeledTrainData.tsv', sep='\\t', error_bad_lines=False)\nprint(\"Shape: {}\".format(unlbl_train.shape))\nprint(unlbl_train.columns)","c102a002":"test = pd.read_csv(data_path+'testData.tsv', sep='\\t')\nprint(f'Shape: {test.shape}')\nprint(test.columns)","9c4f7410":"samplesub = pd.read_csv(data_path+'.\/sampleSubmission.csv')\nsamplesub.head(3)","7881c3fe":"# random positive\/negative review\n\nprint(f'pos:\\n{lbl_train.review[np.random.randint(0, 25000)]}')\nprint(f'neg:\\n{test.review[np.random.randint(0, 25000)]}')","d823c4a8":"# Average number of words in pos & neg reviews\n\navg_pos_words = lbl_train[lbl_train.sentiment==1].review.apply(lambda x: len(x.split())).mean()\navg_neg_words = lbl_train[lbl_train.sentiment==0].review.apply(lambda x: len(x.split())).mean()\n\nplt.figure(figsize=(10, 3))\nplt.barh(['Positive', 'Negative'], [avg_pos_words, avg_neg_words], height=0.5)\nplt.xticks(np.arange(0, 300, 25))\nplt.xlabel('Average Number of words')\nplt.ylabel('Sentiment')\nplt.show()","8a2ec0b7":"def clean_review(review):\n    # remove line breaks\n    review = re.sub(r'<br \/>', '', review)\n    # remove punctuations\/tokenize\n    tokenizer = RegexpTokenizer(r'\\w+')\n    review = tokenizer.tokenize(review)\n    # apply stemming\n    stemmer = PorterStemmer()\n    review = ' '.join([stemmer.stem(y) for y in review])\n    return review\n\n# clean train, test and unlabeled train\nlbl_train.review = lbl_train.review.apply(lambda x: clean_review(x))\ntest.review = test.review.apply(lambda x: clean_review(x))\nunlbl_train = unlbl_train.review.apply(lambda x: clean_review(x))","82a9b44b":"X_train, X_test, y_train, y_test = train_test_split(lbl_train.review, lbl_train.sentiment,\n                                                    test_size=0.2, random_state=13)\nX_train.shape, X_test.shape","434b47f9":"%%time\n\nlg_pipeline = Pipeline([\n    ('tfidf', TfidfVectorizer(max_df=0.9)),\n    ('lg', LogisticRegression(n_jobs=-1))\n])\n\nlg_pipeline.fit(X_train, y_train)\nprint(f'Accuracy: {np.mean(lg_pipeline.predict(X_test)==y_test)}')","65990f91":"%%time\n\nrfc_pipeline = Pipeline([\n    ('tfidf', TfidfVectorizer(max_df=0.9)),\n    ('rfc', RandomForestClassifier(n_estimators=100, n_jobs=-1))\n])\n\nrfc_pipeline.fit(X_train, y_train)\nprint(f\"Accuracy: {np.mean(rfc_pipeline.predict(X_test)==y_test)}\")","7441cbad":"%%time\n\nnb_pipeline = Pipeline([\n    ('tfidf', TfidfVectorizer(max_df=0.9)),\n    ('nb', BernoulliNB())\n])\n\nnb_pipeline.fit(X_train, y_train)\nprint(f'Accuracy: {np.mean(nb_pipeline.predict(X_test)==y_test)}')","7610847b":"# parameter tuning\n\nlg_params = {\n    'tfidf__ngram_range': [(1, 1), (1, 2), (1, 3)],\n    'lg__C': [0.1, 1, 10],\n}\n\nrfc_params = {\n    'tfidf__ngram_range': [(1, 1), (1, 2), (1, 3)],\n    'rfc__max_features': ['auto', 'sqrt'],\n    'rfc__bootstrap': [True, False],\n    'rfc__n_estimators': [200, 400, 600],\n    'rfc__min_samples_split': [2, 5, 10],\n    'rfc__min_samples_leaf': [1, 2, 4]\n}","50411182":"%%time\n\nlg_grid = GridSearchCV(lg_pipeline, param_grid=lg_params, cv=3, verbose=True, n_jobs=-1)\n# lg_grid.fit(X_train, y_train)\n# lg_grid.best_params_\n# print(f\"Logistic gridsearch accuracy: {np.mean(lg_grid.predict(X_test)==y_test)}\")","d714a241":"%%time\n\nrfc_grid = RandomizedSearchCV(rfc_pipeline, param_distributions=rfc_params, cv=3, verbose=True, n_jobs=-1)\n# rfc_grid.fit(X_train, y_train)\n# rfc_grid.best_params_\n# print(f\"Accuracy: {np.mean(rfc_grid.predict(X_test)==y_test)}\")","77803ed4":"lg_best_params = {'lg__C': 10, 'tfidf__ngram_range': (1, 2)}","05e518e8":"rfc_best_params = {'tfidf__ngram_range': (1, 3),\n 'rfc__n_estimators': 400,\n 'rfc__min_samples_split': 5,\n 'rfc__min_samples_leaf': 4,\n 'rfc__max_features': 'sqrt',\n 'rfc__bootstrap': False}","9dd15cca":"lg_pipeline.set_params(**lg_best_params)\nrfc_pipeline.set_params(**rfc_best_params)\n\nlg_pipeline.fit(lbl_train.review, lbl_train.sentiment)\nrfc_pipeline.fit(lbl_train.review, lbl_train.sentiment)","2c69a1c2":"nb_pipeline.set_params(tfidf__ngram_range=(1,3))\nnb_pipeline.fit(lbl_train.review, lbl_train.sentiment)","9e44b33d":"lg_preds = lg_pipeline.predict(test.review)\nrfc_preds = rfc_pipeline.predict(test.review)","71df2592":"nb_preds = nb_pipeline.predict(test.review)","34da0487":"# Pick the test prediction by voting\n\npredictions = pd.DataFrame({'lg': lg_preds, 'rfc': rfc_preds, 'nb': nb_preds}).mode(axis=1).rename(columns={0: 'sentiment'})\nsubmission = pd.DataFrame({'id': test.id.values, 'sentiment': predictions.sentiment.values})","437be15c":"submission.to_csv('submission2.csv', index=False)\n# FileLink('.\/submission2.csv')","884073e6":"from keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Embedding, Flatten, Dense, Dropout, BatchNormalization\nfrom keras.layers import LSTM, Bidirectional, GlobalMaxPool1D\nfrom keras.models import Sequential\nfrom keras.optimizers import Adam","140eb7cc":"# Let's fix vocab size to 20000\nvocab_size = 10000\n\nt = Tokenizer(num_words=vocab_size)\nt.fit_on_texts(lbl_train.review)","66948f0e":"# create sequences to feed into Neural network model\nsequences = t.texts_to_sequences(lbl_train.review)\n\n# As the average length of all reviews is around 250, \n# lets the keep the input dim to 250 and pad the sequences if it is less that 250 words\nsequences = pad_sequences(sequences, maxlen=150)","ff942e20":"# Now the length of each review is 350, and there are 25000 items\nsequences.shape","921b8f34":"# Network architecture\nmodel = Sequential()\nmodel.add(Embedding(input_dim=vocab_size, output_dim=128, input_length=150, name='embed'))\nmodel.add(Bidirectional(LSTM(32, return_sequences=True, name='lstm')))\nmodel.add(GlobalMaxPool1D(name='gmax1'))\nmodel.add(Dense(20, name='dense1'))\n# model.add(Flatten(name='flatten'))\nmodel.add(Dropout(0.05, name='drop1'))\nmodel.add(Dense(1, activation='sigmoid', name='softmax'))","917ce392":"model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\nmodel.fit(sequences, lbl_train.sentiment.values, validation_split=0.2, epochs=5, batch_size=128, verbose=2)","0025de5f":"model.summary()","54eafb26":"test_sequences = t.texts_to_sequences(test.review)\ntest_sequences = pad_sequences(test_sequences, maxlen=150)","af00a603":"len(test_sequences)","983c3f9b":"test_sequences.shape","28f23c45":"preds = model.predict(test_sequences)","eeea19b4":"preds = (preds>0.5)","484d51d4":"preds = [int(p) for p in preds]","8d6d4c8b":"submission2 = pd.DataFrame({'id': test.id.values, 'sentiment': preds})\nsubmission2.to_csv('submission2.csv', index=False)","8b2629d1":"FileLink('.\/submission2.csv')","a1c77db8":"### TF-IDF","04682c88":"## Avengers Ensemble","2574fb85":"## Data load..","c248d17d":"## Word Embeddings","c1fde30a":"We will use a plain Implementation of RNN with a embedding layer, and will move to LSTM","e8025781":"1. As always, Logistic is fast and produced best baseline results\n2. Random forest and Naive bayes are similar","2a606156":"# IMDB movie reviews\n1. Build conventional model using tf-idf feature extraction\n2. Use word embeddings and deep learning","089a919a":"**Average number of words for both positive and negative words are almost same**","0f230336":">Some observations\n1. Remove HTML tags like line breaks <br\\>\n2. Remove punctuations","8518fe5a":"## Model building..","15c8d1fc":"## Analysis..","0e4aed6b":"**Commenting the grid search fit method, as it takes more time. However, the results I have got have been saved later in notebook**"}}