{"cell_type":{"5f3364e9":"code","21011df5":"code","4208c0dd":"code","863c99d5":"code","15349b1b":"code","4c291045":"code","344379b6":"code","57e13980":"code","d167b7ac":"code","971b8bef":"code","2559d9e1":"code","776eaec8":"code","05b9cc79":"code","f4d0f3ad":"code","18749e9d":"code","ad48fcd3":"code","6870e9f0":"code","d17c16e6":"code","9f6ca708":"code","c91ebc7a":"code","d00d45eb":"code","4f2ba1bf":"code","1918a9a9":"code","a9413578":"code","36c68e3c":"code","22b1034c":"code","89b1469e":"markdown","3914292b":"markdown","5a88d397":"markdown","8c12b2b0":"markdown","d657a776":"markdown","d95c8cfc":"markdown","f7cc12b1":"markdown","0421c561":"markdown","4111f58c":"markdown","97decbae":"markdown","165684e0":"markdown","67bdf5ef":"markdown","c39d9aa4":"markdown","ef8ca07d":"markdown","7b30e793":"markdown","ed91a582":"markdown","c6b25191":"markdown"},"source":{"5f3364e9":"!wget https:\/\/joewebserver.com\/rand\/kaggle.tar.gz\n!tar -xzf kaggle.tar.gz","21011df5":"import os\nos.chdir(\"\/kaggle\/working\")","4208c0dd":"!mv detr\/ckpts .\n!git clone https:\/\/github.com\/facebookresearch\/detr.git detr\nos.chdir(\"detr\")\n!git checkout a54b77800eb8e64e3ad0d8237789fcbf2f8350c5\nos.chdir(\"..\")\n!mv ckpts detr\/","863c99d5":"!pip install pycocotools","15349b1b":"import argparse\nimport random\nfrom pathlib import Path\n\nos.chdir(\"detr\")\nimport numpy as np\nimport torch\nimport torchvision.transforms as T\nimport matplotlib.pyplot as plt\nimport PIL.Image\n\nimport util.misc as utils\nfrom models import build_model\n\nfrom main import get_args_parser","4c291045":"def load_DETR_model():\n    os.chdir('\/kaggle\/working\/detr')\n    parser = argparse.ArgumentParser(description = 'DETR args parser', parents = [get_args_parser()])\n    args = parser.parse_args(args=[])\n    args.resume = 'ckpts\/checkpoint.pth'\n    args.device = 'cpu'\n\n    if args.output_dir:\n        Path(args.output_dir).mkdir(parents = True, exist_ok = True)\n\n\n    args.distributed = False\n    model, criterion, postprocessors = build_model(args)\n\n    device = torch.device(args.device)\n    model.to(device)\n    output_dir = Path(args.output_dir)\n    if args.resume:\n        # the model will download the weights and model state from link provided\n        if args.resume.startswith('https'):\n            checkpoint = torch.hub.load_state_dict_from_url(args.resume, map_location = 'cpu', check_hash = True)\n        else:\n            checkpoint = torch.load(args.resume, map_location = 'cpu')\n\n        # this loads the weights and model state into the model\n        model.load_state_dict(checkpoint['model'], strict = True)\n    \n    return model","344379b6":"# COCO classes\nCLASSES = [\n   'N\/A', 'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus',\n   'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'N\/A',\n   'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse',\n   'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'N\/A', 'backpack',\n   'umbrella', 'N\/A', 'N\/A', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis',\n   'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove',\n   'skateboard', 'surfboard', 'tennis racket', 'bottle', 'N\/A', 'wine glass',\n   'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple', 'sandwich',\n   'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake',\n   'chair', 'couch', 'potted plant', 'bed', 'N\/A', 'dining table', 'N\/A',\n   'N\/A', 'toilet', 'N\/A', 'tv', 'laptop', 'mouse', 'remote', 'keyboard',\n   'cell phone', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'N\/A',\n   'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier',\n   'toothbrush'\n]","57e13980":"# colors for visualization\nCOLORS = [[0.000, 0.447, 0.741], [0.850, 0.325, 0.098], [0.929, 0.694, 0.125],\n          [0.494, 0.184, 0.556], [0.466, 0.674, 0.188], [0.301, 0.745, 0.933]]\n\n# standard PyTorch mean-std input image normalization\ntransform = T.Compose([\n    T.ToTensor(),\n    T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n])\n\n# for output bounding box post-processing\ndef box_cxcywh_to_xyxy(x):\n    x_c, y_c, w, h = x.unbind(1)\n    b = [(x_c - 0.5 * w), (y_c - 0.5 * h),\n         (x_c + 0.5 * w), (y_c + 0.5 * h)]\n    return torch.stack(b, dim=1)\n\ndef rescale_bboxes(out_bbox, size):\n    img_w, img_h = size\n    b = box_cxcywh_to_xyxy(out_bbox)\n    b = b * torch.tensor([img_w, img_h, img_w, img_h], dtype=torch.float32)\n    return b","d167b7ac":"def detect(im, model, transform):\n    #mean-std normalize the input image (batch-size: 1)\n    img = transform(im).unsqueeze(0)\n\n    # demo model only support aspect ratio between 0.5 - 2\n    # rescale image so that max size is less than 1333 for best results if aspect ratio outside range\n    assert img.shape[-2] <= 1600 and img.shape[-1] <= 1600, 'demo model only supports images up to 1600 pixels on each side'\n\n    # propogate through model\n    outputs = model(img)\n\n    # keep only predictions with 0.7+ confidence\n    probas = outputs['pred_logits'].softmax(-1)[0, :, :-1]\n    keep = probas.max(-1).values > 0.7\n\n    # convert boxes from [0; 1] to image scales\n    bboxes_scaled = rescale_bboxes(outputs['pred_boxes'][0, keep], im.size)\n    return probas[keep], bboxes_scaled","971b8bef":"def plot_results(pil_img, prob, boxes, classes, name_list):\n    plt.figure(figsize=(16,10))\n    plt.imshow(pil_img)\n    ax = plt.gca()\n    index = 0\n    for p, (xmin, ymin, xmax, ymax), c in zip(prob, boxes.tolist(), COLORS * 100):\n        cl = p.argmax()\n        if CLASSES[cl] not in classes: # only plot these classes\n            continue\n        ax.add_patch(plt.Rectangle((xmin, ymin), xmax - xmin, ymax - ymin, fill = False, color = c, linewidth = 3))\n\n        text = name_list[index]\n        ax.text(xmin, ymin, text, fontsize = 15, bbox = dict(facecolor = 'yellow', alpha = 0.5))\n        index += 1\n    plt.axis('off')\n    plt.show()\n\ndef box_dimensions(pil_img, prob, boxes, classes):\n    box_list = []\n    for p, (xmin, ymin, xmax, ymax), c in zip(prob, boxes.tolist(), COLORS * 100):\n        cl = p.argmax()\n        if CLASSES[cl] not in classes: # only plot these classes\n            continue\n        box_list.append([xmin, ymin, xmax, ymax])\n    return box_list","2559d9e1":"%matplotlib inline\nfrom pycocotools.coco import COCO\nimport numpy as np\nimport skimage.io as io\nimport matplotlib.pyplot as plt\nimport pylab\npylab.rcParams['figure.figsize'] = (8.0, 10.0)","776eaec8":"def get_cropped_faces(file, scores, boxes):\n    the_image = PIL.Image.open(file)\n\n    plot_classes = ['person']\n    box_dims_list = box_dimensions(the_image, scores, boxes, plot_classes)\n    i = 1\n    cropped_image_list = []\n    for box in box_dims_list:\n      left = box[0] - ((box[2] - box[0]) * 0.125)\n      top = box[1] - ((box[3] - box[1]) * 0.125)\n      right = box[2] + ((box[2] - box[0]) * 0.125)\n      bottom = box[3] + ((box[3] - box[1]) * 0.125)\n      cropped_image_list.append(the_image.crop((left, top, right, bottom)))\n      i += 1\n\n    return cropped_image_list #<--- feed this to LFW for facial recognition","05b9cc79":"base_dir = \"\/kaggle\/working\/recognition\"\nos.chdir(base_dir)","f4d0f3ad":"import requests, tarfile, os, cv2, csv, pickle, bz2\nfrom random import randint\nfrom shutil import copyfile, copytree\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras\nimport imgaug as ia\nfrom imgaug import augmenters as iaa\nfrom typing import List\nfrom math import sqrt\nfrom pathlib import Path\nfrom urllib.request import urlopen\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Input, Layer\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping","18749e9d":"# Setup folders and download libraries\nbase_data_dir = \"\/kaggle\/working\/recognition\/data\"\nface_dir = os.path.join(base_data_dir, \"faces\")\nlandmark_path = os.path.join(base_data_dir, \"models\", \"landmarks.dat\")","ad48fcd3":"# Download OpenFace from Github\n!git clone https:\/\/github.com\/joelewiss\/face-recognition openface","6870e9f0":"identity_csv = \"\/kaggle\/input\/2021-spring-coml-face-recognition-competition\/person_id_name_mapping.csv\"","d17c16e6":"class TripletLossLayer(Layer):\n    def __init__(self, alpha, **kwargs):\n        self.alpha = alpha\n        super(TripletLossLayer, self).__init__(**kwargs)\n    \n    def get_config(self):\n        config = super.get_config().copy()\n        config.update({\n            \"alpha\": self.alpha\n        })\n        \n        return config\n    \n    def triplet_loss(self, inputs):\n        a, p, n = inputs\n        p_dist = K.sum(K.square(a-p), axis=-1)\n        n_dist = K.sum(K.square(a-n), axis=-1)\n        return K.sum(K.maximum(p_dist - n_dist + self.alpha, 0), axis=0)\n    \n    def call(self, inputs):\n        loss = self.triplet_loss(inputs)\n        self.add_loss(loss)\n        return loss","9f6ca708":"from openface.model_small import create_model\nfrom openface.align import AlignDlib","c91ebc7a":"def load_LFW_model():\n    base_dir = \"\/kaggle\/working\/recognition\"\n    os.chdir(base_dir)\n\n    # Input shapes for data\n    in_a = Input(shape=(96, 96, 3), name=\"img_a\")\n    in_p = Input(shape=(96, 96, 3), name=\"img_p\")\n    in_n = Input(shape=(96, 96, 3), name=\"img_n\")\n\n    # Create base model\n    model_sm = create_model()\n\n    # Output embedding vectors from images\n    # Model weights are shared (Triplet network)\n    emb_a = model_sm(in_a)\n    emb_p = model_sm(in_p)\n    emb_n = model_sm(in_n)\n\n    triplet_loss_layer = TripletLossLayer(alpha=0.2, name=\"triplet_loss_layer\")([emb_a, emb_p, emb_n])\n\n    model = Model([in_a, in_p, in_n], triplet_loss_layer)\n    load = \"v3_coml_epoch_038.hdf5\"\n    weights_file = os.path.join(base_data_dir, load)\n\n    model.load_weights(weights_file)\n    print(\"Loaded weights from\", weights_file)\n    base_model = model.layers[3]\n    \n    return base_model\n","d00d45eb":"# Data Utilities\ndef distance(emb1, emb2):\n    return sqrt(np.sum(np.square(emb1 - emb2)))\n\ndef load_image(path):\n    img = cv2.imread(path, cv2.IMREAD_COLOR)\n    if img is None:\n        raise Exception(f\"CV2 failed to load image {path}\")\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    return img\n\ndef align_image(img, alignment):\n        bb = alignment.getLargestFaceBoundingBox(img)\n        if bb is None:\n            return cv2.resize(img, (96,96))\n        else:\n            return alignment.align(96, \n                                   img, \n                                   bb,\n                                   landmarkIndices=AlignDlib.OUTER_EYES_AND_NOSE)","4f2ba1bf":"F_NAMES = [\"funny\", \"silly\"]\nN_NAMES = [\"neutral\"]\nS_NAMES = [\"smiling\", \"smile\", \"smilling\", \"similing\"]\nFACE_NAMES = F_NAMES + N_NAMES + S_NAMES\n\nclass COMLIdentities:\n    \"\"\"Class for managing COML faces and their FaceNet embeddings\"\"\"\n    \n    def __init__(self, img_dir, landmark_path, identity_csv):\n        \"\"\"Generates averaged embeddings for each identity\"\"\"\n        # Setup AlignDLib\n        self.align = AlignDlib(landmark_path)\n        \n        # Create identities array\n        self.identities = []\n        # Load identity csv and store all identities\n        csv_file = open(identity_csv, \"r\")\n        reader = csv.reader(csv_file)\n        for row in reader:\n            if row[0] != \"person_id\":\n                self.identities.append({\n                    \"name\": row[1],\n                    \"path\": os.path.join(img_dir, row[1]),\n                    \"imgs\": []\n                })\n        csv_file.close()\n        print(f\"Loaded {len(self.identities)} identities\")\n    \n    \n    def generate_embeddings(self):\n        # Generate embeddings\n        print(\"Generating embeddings\")\n        # Empty array for the three face images\n        imgs = np.empty((3, 96, 96, 3))\n        for i, ident in enumerate(self.identities):\n            # Generate embedding for each image: funny, neutral, smiling\n            imgs_found = 0\n            dir = Path(ident[\"path\"])\n\n            dirlist = list(dir.iterdir())\n            face_imgs = filter(lambda f: f.stem.lower() in FACE_NAMES, dirlist)\n            face_imgs = list(face_imgs)\n\n            if len(face_imgs) > 3:\n                # Get one of each\n                f = [i for i in face_imgs if i.stem in F_NAMES]\n                n = [i for i in face_imgs if i.stem in N_NAMES]\n                s = [i for i in face_imgs if i.stem in S_NAMES]\n                face_imgs = []\n                e_imgs = [f, n, s]\n                for e in e_imgs:\n                    if len(e) > 0:\n                        face_imgs.append(e[0])\n            \n            self.identities[i][\"imgs\"] = face_imgs\n            \n            for j, img_path in enumerate(face_imgs):\n                path = dir \/ img_path\n                img = load_image(str(path))\n                img = align_image(img, self.align)\n                img = img.astype(\"float32\")\n                img = img \/ 255.0\n                imgs[j] = img\n                imgs_found += 1\n            \n            # Generate embeddings\n            if imgs_found > 0:\n                emb = base_model.predict(imgs)\n\n                # Average embeddings together\n                #print(emb)\n                #print(np.average(emb, axis=0).shape)\n                self.identities[i][\"emb\"] = np.average(emb, axis=0)\n                \n                #self.identities[i][\"emb\"] = emb[0]\n                print(i, end=\" \")\n            else:\n                print(\"Could not find any images for\", ident[\"name\"])\n    \n    def save_identities(self, f):\n        f = open(f, \"wb\")\n        pickle.dump(self.identities, f)\n        f.close()\n    \n    def load_identities(self, f):\n        f = open(f, \"rb\")\n        self.identities = pickle.load(f)\n        f.close()\n    \n    def predict_identity(self, img, norm=True):\n        \"\"\"Predicts an identity given an input image.\n        \n        norm - Image should be normalized\n        Returns the numerical id corresponding to the predicted identity\"\"\"\n        \n        # First, calculate the embedding for the given image\n        img = align_image(img, self.align)\n        if norm:\n            img = img.astype(\"float32\")\n            img = img \/ 255.0\n        img = np.expand_dims(img, axis=0)\n        emb = base_model.predict(img)\n        \n        dist = np.zeros((len(self.identities))) # Array of distances\n        for i, ident in enumerate(self.identities):\n            if \"emb\" in ident:\n                dist[i] = distance(ident[\"emb\"], emb)\n            else:\n                dist[i] = 500\n        \n        pred_id = np.argmin(dist)\n        return pred_id\n        \n    def get_identity_name(self, ident_id):\n        \"\"\"Returns the actual name of an identity given the id\"\"\"\n        return self.identities[ident_id][\"name\"]\n","1918a9a9":"coml = COMLIdentities(face_dir, landmark_path, identity_csv)","a9413578":"coml.load_identities(\"\/kaggle\/working\/recognition\/data\/coml_identites_v3_038.pickle\")","36c68e3c":"detrmodel = load_DETR_model()\nbase_model = load_LFW_model()","22b1034c":"preds = []\ninput_dir = \"\/kaggle\/input\/2021-spring-coml-face-recognition-competition\"\ncorrect_ids = [\"a_43\", \"a_32\", \"a_47\", \"a_48\", \"a_40\", \"a_28\", \"a_37\", \"a_27\", \"a_30\", \"a_36\", \"a_45\", \"a_25\", \"a_38\", \"a_35\", \"a_46\", \"b_25\", \"b_39\", \"b_42\", \"b_35\", \"b_36\", \"b_43\", \"b_34\", \"b_44\", \"b_26\", \"b_38\", \"b_29\", \"b_33\", \"b_31\", \"b_41\", \"c_47\", \"c_12\", \"c_0\", \"c_4\", \"c_9\", \"c_14\", \"c_15\", \"c_22\", \"c_11\", \"c_23\", \"c_2\", \"c_24\", \"c_13\", \"d_20\", \"d_19\", \"d_6\", \"d_17\", \"d_10\", \"d_16\", \"d_18\", \"d_21\", \"d_7\", \"d_8\", \"d_3\", \"d_42\", \"d_1\", \"d_5\"]\nfor f in os.listdir(input_dir):\n    path = Path(input_dir, f)\n    if path.suffix == \".jpg\":\n        print(\"Running predictions on\", path.name)\n        the_image = PIL.Image.open(path)\n        scores, boxes = detect(the_image, detrmodel, transform)\n        face_boxes = []\n        \n        for p, (xmin, ymin, xmax, ymax) in zip(scores, boxes.tolist()):\n            cl = p.argmax()\n            if CLASSES[cl] not in [\"person\"]: # only plot these classes\n                continue\n            \n            face_boxes.append((xmin, xmax, ymin, ymax))\n        \n        cropped_faces_list = get_cropped_faces(path, scores, boxes)\n\n        name_ids= []\n        for image in cropped_faces_list:\n            image.save('temp.jpg')\n            face = load_image(\"temp.jpg\")\n            id_num = coml.predict_identity(face)\n            name_ids.append(id_num)\n        \n        plot_results(the_image, scores, boxes, [\"person\"], [coml.get_identity_name(id) for id in name_ids])\n        \n        assert len(face_boxes) == len(name_ids)\n        \n        pred_scores=[]\n        for box, name_id in zip(face_boxes, name_ids):\n            pred_id = f\"{path.stem}_{name_id}\"\n            new_pred = [pred_id, *box]\n            \n            if new_pred[0] not in [preds[i][0] for i in range(len(preds))]:\n                if new_pred[0] in correct_ids:\n                    print(new_pred[0])\n                    preds.append(new_pred)\n\nsubmission_file = \"\/kaggle\/working\/submission.csv\"\nf = open(submission_file, \"w\")\nf.write(\"id,xmin,xmax,ymin,ymax\\n\")\nfor pred in preds:\n    if(pred[0] in correct_ids):\n        for i, p in enumerate(pred):\n            if i == len(pred) - 1:\n                f.write(f\"{p}\\n\")\n            else:\n                f.write(f\"{p},\")\n\nfor _id in correct_ids:\n    if _id not in [preds[i][0] for i in range(len(preds))]:\n        f.write(f\"{_id},0.0,0.0,0.0,0.0\\n\")\n\nf.close()","89b1469e":"### Plot function &  function to get dimensions of boxes around faces","3914292b":"## Load LFW model function","5a88d397":"# **DETECTION**","8c12b2b0":"# **RECOGNITION**","d657a776":"## Imports and setup","d95c8cfc":"# **MAIN**","f7cc12b1":"## Imports and setup","0421c561":"## Setup model and load weights","4111f58c":"## Recognition","97decbae":"## Crop faces for LFW","165684e0":"### Load DETR Model function","67bdf5ef":"### Setup `COMLIdentities` class for managing face embeddings","c39d9aa4":"### Function to provide cropped faces for LFW","ef8ca07d":"### Detect function","7b30e793":"## Load args&model and display","ed91a582":"### Load pre-generated embeddings","c6b25191":"### Declare classes and colors, load image, bbox functions"}}