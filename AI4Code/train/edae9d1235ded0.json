{"cell_type":{"93a3a168":"code","78b4e231":"code","8d48d05b":"code","0e8bed7d":"code","7a7a0577":"code","e87a4091":"code","ae088303":"code","13f4bfde":"code","ef8814c6":"code","c8527ff2":"code","aa2bccfd":"code","6a53c16a":"code","b35db0fe":"code","7cb58531":"code","00874ac9":"code","c696a5cb":"code","980e86e8":"code","c8dca169":"markdown","9c3305bc":"markdown","28fa9962":"markdown","49941dae":"markdown","26701fbf":"markdown","d2a06700":"markdown","56d98618":"markdown","74461938":"markdown","1a2ac653":"markdown","70289c14":"markdown","993da924":"markdown","15533885":"markdown","61b2850a":"markdown","28cb2c82":"markdown","46a524b0":"markdown","bd453bdd":"markdown","bb367b84":"markdown","5cd30a12":"markdown","d89ee8ca":"markdown","b9cbf3f7":"markdown","13d1949e":"markdown","ada664de":"markdown","97470c46":"markdown","5dd958ee":"markdown","f43ed07d":"markdown","549ac26d":"markdown"},"source":{"93a3a168":"# Let's start off importing some python tools\nimport pandas as pd\nimport numpy as np\nimport random as rnd\nfrom tabulate import tabulate\n\n# Visualization libraries \nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom matplotlib.lines import Line2D\nimport plotly as py\nimport plotly.express as px\nimport plotly.graph_objects as go\n\n#!pip install heatmapz # Really great heatmap visualization from https:\/\/pypi.org\/project\/heatmapz\/\n#from heatmap import heatmap, corrplot\n%matplotlib inline\n#%matplotlib notebook\n\n# ML tools\nfrom sklearn import preprocessing\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn import svm\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB, BernoulliNB,CategoricalNB,MultinomialNB\nfrom sklearn.linear_model import Perceptron, SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_absolute_error, accuracy_score\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import jaccard_score\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import log_loss\n\n# Importing Datasets to pandas DataFrames\ntrain_df = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ntest_df = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\ncomplete_df = [train_df, test_df] #Helpful when cleaning both dataframes at once\n\n# Completing or deleting missing values in the dataset\n# I am filling in median age, mode embark, and mediam fare for missing values\nfor datadf in complete_df:    \n    #complete missing age with median\n    datadf['Age'].fillna(datadf['Age'].median(), inplace = True)\n\n    #complete embarked with mode\n    datadf['Embarked'].fillna(datadf['Embarked'].mode()[0], inplace = True)\n\n    #complete missing fare with median\n    datadf['Fare'].fillna(datadf['Fare'].median(), inplace = True)\n    \n# Getting rid of irrelevant columns: Passanger ID, Cabin number, and Ticker number\ndrop_column = ['PassengerId','Cabin', 'Ticket']\nfor datadf in complete_df:    \n    datadf.drop(drop_column, axis=1, inplace = True)\n    \n# Ok, time to visualize the data!\nimport warnings\nimport numpy as np\nwarnings.simplefilter(action='ignore', category=FutureWarning)\nprint('x' in np.arange(5))   #returns False, without Warning","78b4e231":"# General age and sex distribution with added swarmplot\nplt.figure(figsize=(12,8))\n\n# sns plot would add a default legend of 0 or 1 for Survived column, so change it to \"Died\" and \"Survived\"\ncustom = [Line2D([], [], marker='o', color='#023EFF', linestyle='None'),\n          Line2D([], [], marker='o', color='#FF7C00', linestyle='None')]\nsns.swarmplot(x=\"Age\", y=\"Sex\", hue=\"Survived\", data=train_df, palette=\"bright\")\nax = sns.boxplot(x=\"Age\", y=\"Sex\", data=train_df, color='white')\n\n# Making the boxplot edge colors black for better contrast\nfor i,box in enumerate(ax.artists):\n    box.set_edgecolor('black')\n    box.set_facecolor('white')\n    # iterate over whiskers and median lines\n    for j in range(6*i,6*(i+1)):\n         ax.lines[j].set_color('black')\n            \n# Setting the text for legend, overall font size, and title of figure            \nplt.legend(custom, ['Died', 'Survived'], loc='upper right')\nplt.rc('font', size=20)\nplt.rc('axes', titlesize=20)\n\n# This plot gives a nice view of the phrase \"Women and Children first!\"","8d48d05b":"# Let's see how port of embarkation and fare correlate\nplt.figure(figsize=(12,8))\n\n# sns plot would add a default legend of 0 or 1 for Survived column, so change it to \"Died\" and \"Survived\"\ncustom = [Line2D([], [], marker='o', color='#023EFF', linestyle='None'),\n          Line2D([], [], marker='o', color='#FF7C00', linestyle='None')]\nsns.swarmplot(x=\"Fare\", y=\"Embarked\", hue=\"Survived\", data=train_df, palette=\"bright\")\nax = sns.boxplot(x=\"Fare\", y=\"Embarked\", data=train_df, color='white')\n\n# Making the boxplot edge colors black for better contrast\nfor i,box in enumerate(ax.artists):\n    box.set_edgecolor('black')\n    box.set_facecolor('white')\n    # iterate over whiskers and median lines\n    for j in range(6*i,6*(i+1)):\n         ax.lines[j].set_color('black')\n            \n# Setting the text for legend, overall font size, and title of figure            \nplt.legend(custom, ['Died', 'Survived'], loc='upper right')\nplt.rc('font', size=20)\nplt.rc('axes', titlesize=20)","0e8bed7d":"train_df[\"Survived\"] = train_df[\"Survived\"].astype(str)\nfig = px.scatter(train_df, x=\"Age\", y=\"Fare\", color=\"Survived\",hover_data=['SibSp','Parch','Pclass','Embarked'],\n                 marginal_x=\"box\", marginal_y=\"box\" ,title=\"Box plots of Age and Fare\",hover_name='Name')\nfig.update_layout(legend_title_text='Survived? 0=No, 1=Yes')\nfig","7a7a0577":"train_df = train_df.sort_values(by=['Pclass'])\nfig = px.scatter(train_df, x=\"Age\", y=\"Fare\",animation_frame=\"Pclass\",\n                 color=\"Embarked\", hover_name=\"Name\", facet_col=\"Survived\",\n           title=\"Scatter Plots of Age and Fare\",log_x=False, size_max=45)\nfig.show()","e87a4091":"train_df[\"Survived\"] = train_df[\"Survived\"].astype(str)\ntrain_df = train_df.sort_values(by=['Survived'])\nfig = px.scatter_3d(train_df, x='Age', y='Fare', z='Pclass', size = train_df['SibSp']+train_df['Parch'],\n              color='Survived', size_max=40,\n              symbol='Sex', opacity=0.8,hover_name=\"Name\",hover_data=['SibSp','Parch','Embarked'])\n# tight layout\nfig.update_layout(margin=dict(l=0, r=0, b=0, t=0))","ae088303":"# 3D plot based on Age, Fare, and Family size (siblings+spouse+children+parents in z)\ntrain_df[\"Survived\"] = train_df[\"Survived\"].astype(str)\ntrain_df = train_df.sort_values(by=['Survived'])\n\nfig = px.scatter_3d(train_df, x='Age', y='Fare', z=train_df['SibSp']+train_df['Parch'],\n              color='Survived', size_max=10,\n              symbol='Sex', opacity=0.7,hover_name=\"Name\",hover_data=['SibSp','Parch','Embarked'])\n# tight layout\nfig.update_layout(margin=dict(l=0, r=0, b=0, t=0))","13f4bfde":"train_df['Sex'].replace(to_replace=['male','female'], value=[0,1],inplace=True)\ntrain_df['Embarked'].replace(to_replace=['C','Q', 'S'], value=[0,1,2],inplace=True)\n\ntest_df['Sex'].replace(to_replace=['male','female'], value=[0,1],inplace=True)\ntest_df['Embarked'].replace(to_replace=['C','Q', 'S'], value=[0,1,2],inplace=True)","ef8814c6":"X = train_df\ny = train_df['Survived'].values\nX.drop(['Name'],axis = 1, inplace = True)\nX.drop(['Survived'],axis = 1, inplace = True)\n\nX_submit = test_df\nX_submit.drop(['Name'],axis = 1, inplace = True)\n\nX = preprocessing.StandardScaler().fit(X).transform(X)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)","c8527ff2":"# From sklearn.tree import DecisionTreeClassifier\nDT_model = DecisionTreeClassifier(criterion=\"entropy\")\nDT_model.fit(X_train,y_train)\n\nDT_yhat = DT_model.predict(X_test)\n\nprint(\"DT accuracy: %.2f\" % accuracy_score(y_test, DT_yhat))\nprint(\"DT Jaccard index: %.2f\" % jaccard_score(y_test, DT_yhat,pos_label='1'))\nprint(\"DT F1-score: %.2f\" % f1_score(y_test, DT_yhat, average='weighted') )","aa2bccfd":"RanFor_model = RandomForestClassifier(n_estimators=10,random_state=1).fit(X_train,y_train)\nRanFor_yhat = RanFor_model.predict(X_test)\n\nprint(\"Random Forest accuracy: %.2f\" % accuracy_score(y_test, RanFor_yhat))\nprint(\"Random Forest Jaccard index: %.2f\" % jaccard_score(y_test, RanFor_yhat,pos_label='1'))\nprint(\"Random Forest F1-score: %.2f\" % f1_score(y_test, RanFor_yhat, average='weighted') )","6a53c16a":"# From sklearn import svm.SVC()\nSVM_model = SVC()\nSVM_model.fit(X_train,y_train)\n\nSVM_yhat = SVM_model.predict(X_test)\n\nprint(\"SVM accuracy: %.2f\" % accuracy_score(y_test, SVM_yhat))\nprint(\"SVM Jaccard index: %.2f\" % jaccard_score(y_test, SVM_yhat,pos_label='1'))\nprint(\"SVM F1-score: %.2f\" % f1_score(y_test, SVM_yhat, average='weighted') )","b35db0fe":"LR_model = LogisticRegression(C=0.01).fit(X_train,y_train)\nLR_yhat = LR_model.predict(X_test)\n\nprint(\"LR accuracy: %.2f\" % accuracy_score(y_test, LR_yhat))\nprint(\"LR Jaccard index: %.2f\" % jaccard_score(y_test, LR_yhat,pos_label='1'))\nprint(\"LR F1-score: %.2f\" % f1_score(y_test, LR_yhat, average='weighted') )","7cb58531":"NB_model = BernoulliNB(2).fit(X_train,y_train)\nNB_yhat = NB_model.predict(X_test)\n\nprint(\"NB accuracy: %.2f\" % accuracy_score(y_test, NB_yhat))\nprint(\"NB Jaccard index: %.2f\" % jaccard_score(y_test, NB_yhat,pos_label='1'))\nprint(\"NB F1-score: %.2f\" % f1_score(y_test, NB_yhat, average='weighted') )","00874ac9":"XGB_model=XGBClassifier(max_depth=3).fit(X_train,y_train)\nXGB_pred=XGB_model.predict(X_test)\n    \nprint(\"XGB accuracy: %.2f\" % accuracy_score(y_test, XGB_pred))\nprint(\"XGB Jaccard index: %.2f\" % jaccard_score(y_test, XGB_pred,pos_label='1'))\nprint(\"XGB F1-score: %.2f\" % f1_score(y_test, XGB_pred, average='weighted') )    \n    ","c696a5cb":"from tabulate import tabulate\ndata = [['Decision Tree', accuracy_score(y_test, DT_yhat), jaccard_score(y_test, DT_yhat,pos_label='1'), f1_score(y_test, DT_yhat, average='weighted')],\n['Random Forest Classifier', accuracy_score(y_test, RanFor_yhat), jaccard_score(y_test, RanFor_yhat,pos_label='1'), f1_score(y_test, RanFor_yhat, average='weighted')],\n['Support Vector Machine', accuracy_score(y_test, SVM_yhat), jaccard_score(y_test, SVM_yhat,pos_label='1'), f1_score(y_test, SVM_yhat, average='weighted')],\n['Logistic Regression', accuracy_score(y_test, LR_yhat), jaccard_score(y_test, LR_yhat,pos_label='1'), f1_score(y_test, LR_yhat, average='weighted')],\n['Bernoulli Naive_Bayes', accuracy_score(y_test, NB_yhat), jaccard_score(y_test, NB_yhat,pos_label='1'), f1_score(y_test, NB_yhat, average='weighted')],\n['XGB Classifier', accuracy_score(y_test, XGB_pred), jaccard_score(y_test, XGB_pred,pos_label='1'), f1_score(y_test, XGB_pred, average='weighted')]]\nprint (tabulate(data, headers=[\"Model\", \"Accuracy\", \"Jaccard score\", \"F1-Score\"]))","980e86e8":"prediction_submit = RanFor_model.predict(X_submit)\n\ntestdata = pd.read_csv(\"\/kaggle\/input\/titanic\/gender_submission.csv\")\noutput = pd.DataFrame({'PassengerId': testdata.PassengerId, 'Survived': prediction_submit})\noutput.to_csv('my_submission.csv', index=False)\nprint(\"Your submission was successfully saved!\")","c8dca169":"### Importing tools, data, and cleaning up the data.","9c3305bc":"### Highest accuracy and F1-score were obtained from Random Forest and XGB Classifier.","28fa9962":"### Model comparison:","49941dae":"### Random Forest Classifier","26701fbf":"### For more interactive plots, \"*plotpy*\" express library provides quick and simple visualization. A 2D scatter plot may also contain built in histograms or box plots. By clicking on the legend marker groups, you can select to display only certain results. Hovering over individual points yields the full parameters for each point.","d2a06700":"### Feature (X) and label (y) selection. Then normalization of data to give data zero mean and unit variance. X_submit is the data for final submission. ","56d98618":"### Before we try various models, the data needs some additional pre-processing. Specifically, we should covert categorical features to numerical values (Sex, Embarked).","74461938":"> ## Data Visualization <a class=\"anchor\" id=\"second-section\"><\/a>","1a2ac653":"### *Note that additional data and correlations may have been extracted from the Name column by parsing the title, first and last name. For example, the \"Mrs.\" and \"Miss\" title would imply a woman being married or unmarried. However, I did not go into such detail.\n    ","70289c14":"## Evaluation of ML Models <a class=\"anchor\" id=\"third-section\"><\/a>","993da924":"## Starting housekeeping<a class=\"anchor\" id=\"first-section\"><\/a>","15533885":"### For distribution of data, statistical analysis is much more powerful visually than in a giant chart. In the first plot, I am using simple \"*seaborn*\" functions to combine a swarmplot and boxplot to highlight the age and sex distribution. Individual people are colorcoded to show if they survived, as this parameter is our target. Box plot is shown around the datapoints. \n\n### Sometimes simple is best.","61b2850a":"### Support Vector Machine","28cb2c82":"### Decision Tree","46a524b0":"### Hi there! This is my first Kaggle notebook, and instead of doing the clich\u00e9 basic scripts or highly advanced ML scripts (since I am not an expert), I decided to add some fun interactive plots and visualizations to showcase the Titanic dataset and evaluate some generic algorithms. \n\n`Aesthetics are a moral imperative`","bd453bdd":"### Can we visualize the data in 3d? This would make more sense with other types of data, but we can try with Titanic passengers. Try moving the 3d plot around with your mouse. \n\n### Here, the size of the data point is total family members (siblings + spouse + children + parents)","bb367b84":"<img src=\"https:\/\/media.giphy.com\/media\/g2e22SIHPcQw0\/giphy.gif\">","5cd30a12":"<img src=\"https:\/\/imgs.xkcd.com\/comics\/machine_learning.png\">","d89ee8ca":"> ## Quick rundown:\n* [Starting housekeeping](#first-section)\n* [Data Visualization](#second-section)\n* [Evaluation of ML Models](#third-section)\n* [Submission](#fourth-section)","b9cbf3f7":"### Logistic Regression","13d1949e":"# ***Interactive*** Titanic Notebook","ada664de":"### Bernoulli Naive Bayes","97470c46":"### XGBClassifier","5dd958ee":"### How does the passenger distribution vary for the three Ticket Classes (1st, 2nd, 3rd)? Pull slider to find out! The colors help distinguish port of embarkation. (Survived 0=No, 1=Yes)","f43ed07d":"### How can we see number of family members of the passengers? Here, z axis is total family members (siblings + spouse + children + parents)","549ac26d":"## Submission <a class=\"anchor\" id=\"fourth-section\"><\/a>"}}