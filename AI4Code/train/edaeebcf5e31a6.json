{"cell_type":{"1bd13c64":"code","fbbd9d7e":"code","3a8d44b3":"code","1d8754c4":"code","18f7d595":"code","839cc0e9":"code","4596d6e1":"code","fff73108":"code","cd3b5a53":"code","386d0458":"code","5715ddb5":"code","194c8267":"code","ee0cbf51":"code","26ded1df":"code","01bbcbe1":"code","2ce6632e":"code","e59f970c":"code","3764292b":"code","c4f0b44c":"code","ce5ae7b8":"code","9b1a4ceb":"code","2426deb8":"code","5fb71ca9":"code","7906f596":"code","523c485b":"code","0f478032":"code","7936bc1b":"code","ab39fc7a":"code","18b0ef3e":"code","ed8d5d59":"code","6a00e693":"code","a6b2a53c":"code","7722a1dc":"code","2227edfd":"code","ecf1288c":"code","efd19c8f":"code","9c15803e":"code","e331a675":"code","45700efa":"code","0b2c31e7":"code","c7d1afc6":"code","75e8786a":"code","0804c62f":"code","7856207b":"code","5c642d16":"code","af210972":"code","0eacaff7":"code","2b54542b":"code","18a7b87a":"code","8d92a113":"code","a44db592":"code","0d2dcbda":"code","5eafbee6":"code","1d9a4efd":"code","a800d3f8":"code","158fa93f":"code","35e9cd68":"code","8c2085bd":"code","b4553962":"code","8fcdd2d0":"code","f72b6ca1":"markdown","a51f7a48":"markdown","2dc93160":"markdown","731544e3":"markdown","e22d45ff":"markdown","53a350a3":"markdown","835fccfb":"markdown","73b91d81":"markdown","9b53fe57":"markdown","f7f2882b":"markdown","689191d9":"markdown","172faaa7":"markdown","58b3e98c":"markdown","e8e55688":"markdown","e373ec4b":"markdown","07a05f4a":"markdown","3aaa1223":"markdown","866207a5":"markdown","3e34b3db":"markdown","14d6f12d":"markdown","7f4b2cdf":"markdown","755cb949":"markdown","9d7d3f07":"markdown","83b79c9b":"markdown","ec15a775":"markdown","50f3e510":"markdown","a368441d":"markdown","ca033b63":"markdown","fe23a6d4":"markdown","d4dca124":"markdown","78cba110":"markdown","d65112e3":"markdown","c6ff0cf0":"markdown","4eeef8bf":"markdown","6cec234e":"markdown","38754000":"markdown","aec611c8":"markdown"},"source":{"1bd13c64":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","fbbd9d7e":"!pip install pyspark","3a8d44b3":"from pyspark.sql import SparkSession\nfrom pyspark.ml import Pipeline\nimport pyspark.sql.functions as F\nfrom pyspark.sql.types import DoubleType, StringType, StructType, StructField\nfrom pyspark.ml.feature import StringIndexer, VectorAssembler, QuantileDiscretizer\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator\nfrom pyspark import SparkContext\nfrom pyspark.ml.classification import RandomForestClassifier\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator","1d8754c4":"# build spark session\nspark = SparkSession.builder.appName(\"Spark on santander-customer-satisfaction\").getOrCreate()","18f7d595":"# Loading data to spark session\ntrain_spark = spark.read.csv(\"..\/input\/santander-customer-satisfaction\/train.csv\", header=\"true\", inferSchema=\"true\")\ntest_spark = spark.read.csv(\"..\/input\/santander-customer-satisfaction\/test.csv\", header=\"true\", inferSchema=\"true\")\n\n# Loading data panda to simplify data visuallization \n\n\ntrain = pd.read_csv('..\/input\/santander-customer-satisfaction\/train.csv')\n\ntest = pd.read_csv('..\/input\/santander-customer-satisfaction\/test.csv')","839cc0e9":"train_spark.printSchema()","4596d6e1":"train_spark.describe().limit(5).toPandas()","fff73108":"train_spark.describe().toPandas()","cd3b5a53":"train_spark.limit(5).toPandas()","386d0458":"from pyspark.sql.functions import col\ndf_ID_count = train_spark.groupBy(\"ID\").count().orderBy('count', ascending=False)\n#df_ID_count = train_spark.groupBy(\"ID\").count().filter(\"`count` >= 10\").sort(col(\"count\").desc())\ndf_ID_count.show(20)","5715ddb5":"##########warning take long time do not start it unless need to review each column on by one #####################\ndef describe_Column(df, column, target='TARGET', numRows=20):\n    df.groupby(column).agg(F.count(column).alias('count'), \n                           F.mean(target).alias('mean'), \n                           F.stddev(target).alias('stddev'),\n                           F.min(target).alias('min'), \n                           F.max(target).alias('max')\n                          ).orderBy('count', ascending=False).show(numRows)\n    \n\n#for column, typ in train_spark_reduce.dtypes:\n    #print(column)\n    #describe_Column(train_spark_reduce, column)\n    # please comment out lines above to use ","194c8267":"#check the type\ntype(df_ID_count)","ee0cbf51":"df_Target_count = train_spark.groupBy(\"TARGET\").count()\ndf_Target_count.show()\ntype(df_Target_count)","26ded1df":"import pyspark.sql.functions as f\nfrom pyspark.sql.window import Window\ndf_Target_count = df_Target_count.withColumn('ratio', f.col('count')\/f.sum('count').over(Window.partitionBy()))\ndf_Target_count.orderBy('ratio', ascending=False).show()","01bbcbe1":"import seaborn as sns\nfrom matplotlib import pyplot as plt\n\nprint('Histogram plot ')\nsns.countplot('TARGET', data=train_spark.toPandas())\nplt.title('Target size', fontsize=14)\nplt.show()\n\nprint(\"Dataset is imbalanced\")","2ce6632e":"#Assuming ID is not correlated with customer satisfaction so i drop it\ntrain_spark_NoID = train_spark.drop('ID')\ntrain_spark_NoID.limit(5).toPandas()","e59f970c":"print(\"Before dorp duplicate count: \",train_spark_NoID.count())\n#drop duplicate \ntrain_spark_NoID_NoDupRow = train_spark_NoID.dropDuplicates()\nprint(\"After dorp duplicate count: \",train_spark_NoID_NoDupRow.count())","3764292b":"train_spark_NoID_NoDupRow.distinct().count()","c4f0b44c":"# remove duplicated columns\nremove = []\ncols = train.columns\nfor i in range(len(cols)-1):\n    v = train[cols[i]].values\n    for j in range(i+1,len(cols)):\n        if np.array_equal(v,train[cols[j]].values):\n            remove.append(cols[j])","ce5ae7b8":"print(\"Before dorp duplicate column count: \",len(train_spark_NoID_NoDupRow.columns))\ntrain_spark_NoID_NoDup = train_spark_NoID_NoDupRow.drop(*remove)\nprint(\"After dorp duplicate column count: \",len(train_spark_NoID_NoDup.columns))","9b1a4ceb":"train_spark_drop1Distinct = train_spark_NoID_NoDup","2426deb8":"# count name of werid number for each columns and sort by count\ncount_series = train[train<-100000].count()\ndf_count=count_series.to_frame().T\ndf_count.max().sort_values(ascending=False).head()\n\n","5fb71ca9":"# only var3 column has the strange value -999999\n# train_spark_drop1Distinct.filter(train_spark_drop1Distinct.var3 == -999999).toPandas()","7906f596":"train_spark_drop = train_spark_drop1Distinct.withColumn('var3', F.when(train_spark_drop1Distinct['var3']< -100000,train['var3'].median()).otherwise(train_spark_drop1Distinct['var3']))","523c485b":"#train_spark_drop.filter(train_spark_drop.var3 == -999999).toPandas()\ntrain_spark_drop.describe().toPandas()","0f478032":"from pyspark.sql.functions import*\ntrain_spark_drop.select([count(when(isnan(c), c)).alias(c) for c in train_spark_drop.columns]).toPandas()","7936bc1b":"import seaborn as sns\nfrom matplotlib import pyplot as plt\n\nprint('Histogram plot after process data set')\nsns.countplot('TARGET', data=train_spark_drop.toPandas())\nplt.title('Target size', fontsize=14)\nplt.show()","ab39fc7a":"import pyspark.sql.functions as f\nfrom pyspark.sql.window import Window\ndf_Target_count_2 = train_spark_drop.groupBy(\"TARGET\").count()\ndf_Target_count_2.show()\ndf_Target_count_2 = df_Target_count_2.withColumn('ratio', f.col('count')\/f.sum('count').over(Window.partitionBy()))\ndf_Target_count_2.orderBy('ratio', ascending=False).show()","18b0ef3e":"from time import *\nstart_time = time()\n\ntrain_spark_1 = train_spark_drop.filter(\"TARGET =1\")\ntrain_spark_0_OG = train_spark_drop.filter(\"TARGET =0\")\nratio = train_spark_1.count()\/train_spark_0_OG.count()\nprint(\"Before Undersample 1 and 0: \",ratio)\ntrain_spark_0, train_spark_dump = train_spark_0_OG.randomSplit([ratio,1-ratio])\n\n#concate two dataframe together\ntrain_spark_Undersample= train_spark_0.union(train_spark_1)\nratio_Undersample = train_spark_Undersample.filter(\"TARGET =1\").count()\/train_spark_Undersample.filter(\"TARGET =0\").count()\nprint(\"After Undersample 1 and 0: \",ratio_Undersample)\nend_time = time()\nelapsed_time = end_time - start_time\nprint(\"Time for this session: %.3f seconds\" % elapsed_time)","ed8d5d59":"import seaborn as sns\nfrom matplotlib import pyplot as plt\n\nprint('After Normalize Target distribution ')\nsns.countplot('TARGET', data=train_spark_Undersample.toPandas())\nplt.title('Target size', fontsize=14)\nplt.show()\ntrain_spark_Undersample.groupBy(\"TARGET\").count().show()\n","6a00e693":"feature_undersample = VectorAssembler(inputCols=train_spark_Undersample.columns[:-1],outputCol=\"features\")\nfeature_vector_undersample= feature_undersample.transform(train_spark_Undersample)","a6b2a53c":"(trainingData_undersample, testData_undersample) = feature_vector_undersample.randomSplit([0.8, 0.2],seed = 11)","7722a1dc":"## Logistic Regression\nfrom pyspark.ml.classification import LogisticRegression\nlr = LogisticRegression(labelCol=\"TARGET\", featuresCol=\"features\", maxIter=5)\nlrModel = lr.fit(trainingData_undersample)\nimport matplotlib.pyplot as plt\nimport numpy as np\nbeta = np.sort(lrModel.coefficients)\nplt.plot(beta, label =\"LogisticRegression\")\nplt.ylabel(\"Beta Coefficients\")\nplt.legend(loc=\"lower right\")\nplt.show()\n\n","2227edfd":"trainSet = lrModel.summary\nroc = trainSet.roc.toPandas()\nplt.plot(roc[\"FPR\"],roc[\"TPR\"], \"-r\", label=\"Logistic Regression ROC Curve\")\nplt.legend(loc=\"lower right\")\nplt.ylabel(\"False Positive Rate\")\nplt.xlabel(\"True Positive Rate\")\nplt.title(\"ROC Curve\")\nplt.show()\nprint(\"TrainSet areaUnderROC: \" + str(trainSet.areaUnderROC))","ecf1288c":"## Random Forest Classifier\nfrom pyspark.ml.classification import RandomForestClassifier\n# Creating RandomForest model.\nrf = RandomForestClassifier(labelCol=\"TARGET\", featuresCol=\"features\", numTrees=2)\n## train the model\nrfModel = rf.fit(trainingData_undersample)\n## make predictions\npredictions = rfModel.transform(testData_undersample)\nrfPredictions = predictions.select(\"TARGET\", \"prediction\", \"probability\")\nrfPredictions.show(10)\n","efd19c8f":"from pyspark.mllib.evaluation import BinaryClassificationMetrics\n# Python: https:\/\/spark.apache.org\/docs\/latest\/api\/python\/_modules\/pyspark\/mllib\/common.html\nclass CurveMetrics(BinaryClassificationMetrics):\n    def __init__(self, *args):\n        super(CurveMetrics, self).__init__(*args)\n\n    def _to_list(self, rdd):\n        points = []\n        # Note this collect could be inefficient for large datasets \n        # considering there may be one probability per datapoint (at most)\n        # The Scala version takes a numBins parameter, \n        # but it doesn't seem possible to pass this from Python to Java\n        for row in rdd.collect():\n            # Results are returned as type scala.Tuple2, \n            # which doesn't appear to have a py4j mapping\n            points += [(float(row._1()), float(row._2()))]\n        return points\n\n    def get_curve(self, method):\n        rdd = getattr(self._java_model, method)().toJavaRDD()\n        return self._to_list(rdd)","9c15803e":"# the probability of getting the output either as 0 or 1\n# Returns as a list (false positive rate, true positive rate)\npreds = predictions.select(\"TARGET\",\"probability\").rdd.map(lambda row: (float(row[\"probability\"][1]), float(row[\"TARGET\"])))\npoints = CurveMetrics(preds).get_curve('roc')\n\nplt.figure()\nx_val = [x[0] for x in points]\ny_val = [x[1] for x in points]\nplt.title(\"ROC\")\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.plot(x_val, y_val, \"-r\", label=\"Random Forest Regression ROC Curve\")\nplt.legend(loc=\"lower right\")","e331a675":"## evaluate the Rnadom Forest Classifier\nfrom pyspark.ml.evaluation import BinaryClassificationEvaluator\nevaluator = BinaryClassificationEvaluator(labelCol=\"TARGET\")\nevaluator.evaluate(predictions)\nprint(\"Random Forest Test areaUnderROC: {}\".format(evaluator.evaluate(predictions)))","45700efa":"from pyspark.ml import Pipeline\n## Gradient-Boosted Tree Classifier\nfrom pyspark.ml.classification import GBTClassifier\nstages = []\ngbt = GBTClassifier(labelCol=\"TARGET\", featuresCol=\"features\",maxIter=5)\npipeline = Pipeline(stages=stages+[gbt])\ngbtModel = pipeline.fit(trainingData_undersample)","0b2c31e7":"from pyspark.ml.evaluation import MulticlassClassificationEvaluator\npredictions =gbtModel.transform(testData_undersample)\n# Show predictions\npredictions.select(\"TARGET\", \"prediction\", \"probability\").show(10)","c7d1afc6":"# the probability of getting the output either as 0 or 1\n# Returns as a list (false positive rate, true positive rate)\npreds_GBT = predictions.select(\"TARGET\",\"probability\").rdd.map(lambda row: (float(row[\"probability\"][1]), float(row[\"TARGET\"])))\npoints_GBT = CurveMetrics(preds_GBT).get_curve('roc')\n\nplt.figure()\nx_val = [x[0] for x in points_GBT]\ny_val = [x[1] for x in points_GBT]\nplt.title(\"ROC\")\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.plot(x_val, y_val,\"-r\", label=\"Gradient-Boosted Regression ROC Curve\")\nplt.legend(loc=\"lower right\")","75e8786a":"evaluator = BinaryClassificationEvaluator(labelCol=\"TARGET\")\nprint(\"GBT Test Area Under ROC:\"  + str(evaluator.evaluate(predictions, {evaluator.metricName: \"areaUnderROC\"})))","0804c62f":"(trainingData_spark, testData_spark) = train_spark_drop.randomSplit([0.8, 0.2],seed = 11)","7856207b":"from time import *\nstart_time = time()\n\ntrain_spark_1_over_OG = trainingData_spark.filter(\"TARGET =1\")\ntrain_spark_0_over = trainingData_spark.filter(\"TARGET =0\")\nratio = train_spark_1_over_OG.count()\/train_spark_0_over.count()\nprint(\"Before oversample ratio 1 and 0: \",ratio)\nsampleRatio = train_spark_0_over.count()\/trainingData_spark.count()\nprint(\"sampleRatio:\", sampleRatio)\n\n# duplicate the minority rows\n# explode_range\nexplode_range = range(int(train_spark_0_over.count()\/train_spark_1_over_OG.count()))\ntrain_spark_1_over = train_spark_1_over_OG.withColumn(\"dummy\", explode(array([lit(x) for x in explode_range]))).drop('dummy')\n#print(train_spark_1_over.count())\n\n#concate two dataframe together\ntrain_spark_oversample= train_spark_1_over.union(train_spark_0_over)\nratio_oversample = (train_spark_oversample.filter(\"TARGET =1\")).count()\/(train_spark_oversample.filter(\"TARGET =0\")).count()\nprint(\"After oversample ratio 1 and 0: \",ratio_oversample)\nend_time = time()\nelapsed_time = end_time - start_time\nprint(\"Time for this session: %.3f seconds\" % elapsed_time)","5c642d16":"import seaborn as sns\nfrom matplotlib import pyplot as plt\n\nprint('After Normalize Target distribution ')\nsns.countplot('TARGET', data=train_spark_oversample.toPandas())\nplt.title('Target size', fontsize=14)\nplt.show()\ntrain_spark_oversample.groupBy(\"TARGET\").count().show()","af210972":"# train_spark_oversample, testData_spark\nfeature_oversample_train = VectorAssembler(inputCols=train_spark_oversample.columns[:-1],outputCol=\"features\")\ntrainingData_oversample= feature_oversample_train.transform(train_spark_oversample)\n\nfeature_oversample_test = VectorAssembler(inputCols=testData_spark.columns[:-1],outputCol=\"features\")\ntestData_oversample= feature_oversample_test.transform(testData_spark)\n\n\n#trainingData_oversample, testData_oversample\n\n\n\n","0eacaff7":"## Logistic Regression\nfrom pyspark.ml.classification import LogisticRegression\nlr = LogisticRegression(labelCol=\"TARGET\", featuresCol=\"features\", maxIter=5)\nlrModel = lr.fit(trainingData_oversample)\nimport matplotlib.pyplot as plt\nimport numpy as np\nbeta = np.sort(lrModel.coefficients)\nplt.plot(beta, label =\"LogisticRegression\")\nplt.ylabel(\"Beta Coefficients\")\nplt.legend(loc=\"lower right\")\nplt.show()","2b54542b":"trainSet = lrModel.summary\nroc = trainSet.roc.toPandas()\nplt.plot(roc[\"FPR\"],roc[\"TPR\"], \"-r\", label=\"Logistic Regression ROC Curve\")\nplt.legend(loc=\"lower right\")\nplt.ylabel(\"False Positive Rate\")\nplt.xlabel(\"True Positive Rate\")\nplt.title(\"ROC Curve\")\nplt.show()\nprint(\"TrainSet areaUnderROC: \" + str(trainSet.areaUnderROC))","18a7b87a":"#trainingData_oversample, testData_oversample\n## Random Forest Classifier\nfrom pyspark.ml.classification import RandomForestClassifier\n# Creating RandomForest model.\nrf = RandomForestClassifier(labelCol=\"TARGET\", featuresCol=\"features\", numTrees=2)\n## train the model\nrfModel = rf.fit(trainingData_oversample)\n## make predictions\npredictions = rfModel.transform(testData_oversample)\nrfPredictions = predictions.select(\"TARGET\", \"prediction\", \"probability\")\nrfPredictions.show(10)","8d92a113":"# the probability of getting the output either as 0 or 1\n# Returns as a list (false positive rate, true positive rate)\npreds = predictions.select(\"TARGET\",\"probability\").rdd.map(lambda row: (float(row[\"probability\"][1]), float(row[\"TARGET\"])))\npoints = CurveMetrics(preds).get_curve('roc')\n\nplt.figure()\nx_val = [x[0] for x in points]\ny_val = [x[1] for x in points]\nplt.title(\"ROC\")\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.plot(x_val, y_val, \"-r\", label=\"Random Forest Regression ROC Curve\")\nplt.legend(loc=\"lower right\")","a44db592":"## evaluate the Rnadom Forest Classifier\nfrom pyspark.ml.evaluation import BinaryClassificationEvaluator\nevaluator = BinaryClassificationEvaluator(labelCol=\"TARGET\")\nevaluator.evaluate(predictions)\nprint(\"Random Forest Test areaUnderROC: {}\".format(evaluator.evaluate(predictions)))","0d2dcbda":"#trainingData_oversample, testData_oversample\nfrom pyspark.ml import Pipeline\n## Gradient-Boosted Tree Classifier\nfrom pyspark.ml.classification import GBTClassifier\nstages = []\ngbt = GBTClassifier(labelCol=\"TARGET\", featuresCol=\"features\",maxIter=5)\npipeline = Pipeline(stages=stages+[gbt])\ngbtModel = pipeline.fit(trainingData_oversample)","5eafbee6":"from pyspark.ml.evaluation import MulticlassClassificationEvaluator\npredictions =gbtModel.transform(testData_oversample)\n# Show predictions\npredictions.select(\"TARGET\", \"prediction\", \"probability\").show(10)","1d9a4efd":"# the probability of getting the output either as 0 or 1\n# Returns as a list (false positive rate, true positive rate)\npreds_GBT = predictions.select(\"TARGET\",\"probability\").rdd.map(lambda row: (float(row[\"probability\"][1]), float(row[\"TARGET\"])))\npoints_GBT = CurveMetrics(preds_GBT).get_curve('roc')\n\nplt.figure()\nx_val = [x[0] for x in points_GBT]\ny_val = [x[1] for x in points_GBT]\nplt.title(\"ROC\")\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.plot(x_val, y_val,\"-r\", label=\"Gradient-Boosted Regression ROC Curve\")\nplt.legend(loc=\"lower right\")","a800d3f8":"evaluator = BinaryClassificationEvaluator(labelCol=\"TARGET\")\nprint(\"GBT Test Area Under ROC:\"  + str(evaluator.evaluate(predictions, {evaluator.metricName: \"areaUnderROC\"})))","158fa93f":"# inspire from https:\/\/stackoverflow.com\/questions\/51831874\/how-to-get-correlation-matrix-values-pyspark\/51834729\nfrom pyspark.ml.stat import Correlation\nfrom pyspark.ml.feature import VectorAssembler\n\n#drop target column\ncorr_df = train_spark_oversample.drop(\"TARGET\")\n# copying columns names\ncolumn_names = corr_df.columns\n\n\n# get correlation matrix\n#matrix = Correlation.corr(feature_vector)\n\n\nvector_col = \"corr_features\"\nassembler = VectorAssembler(inputCols=train_spark_oversample.columns[:-1], \n                            outputCol=vector_col)\nfeature_vector = assembler.transform(train_spark_oversample).select(vector_col)\nmatrix = Correlation.corr(feature_vector, vector_col)\n\n# Setting column names of datafram\n\nconvert_matrix = (matrix.collect()[0][0]).toArray().tolist()\ndf_matrix  = pd.DataFrame( convert_matrix, column_names)\n\n\n\n","35e9cd68":"df_matrix  = pd.DataFrame( convert_matrix, column_names,column_names)","8c2085bd":"df_matrix","b4553962":"import matplotlib.ticker as ticker\nimport matplotlib.cm as cm\nimport matplotlib as mpl\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n\ndf_matrix_04 = df_matrix[(df_matrix[:]>0.5)|(df_matrix[:]<-0.5)]\ndf_matrix_04.head(20)\nfig, ax = plt.subplots(figsize=(10,10)) \nsns.heatmap(df_matrix_04, cmap=\"YlGnBu\")\n\n","8fcdd2d0":"# collection of coloumns which need to be reomoved \n\ncols_to_drop_1 = []\n \nfor col in range(len(df_matrix.columns)):\n    for row in range(col):\n        if (df_matrix.iloc[row,col] >0.5 \\\n            or df_matrix.iloc[row,col] < -0.5) \\\n            and (df_matrix.columns[row] not in cols_to_drop_1):\n            cols_to_drop_1.append(df_matrix.columns[col])\n\ntrain_spark_filter = train_spark_oversample.drop(*cols_to_drop_1)\n\nprint(\"Number of cols dropped: \",  len(cols_to_drop_1))\nprint(\"Number of cols train_spark_filter: \",  len(train_spark_filter.columns))\n# there is duplicate coloumn name therefore the size the big, need sometime to look into this. \n","f72b6ca1":" #  1.Overview and understand data ","a51f7a48":"# 6. Modelling\n# 6.1 Logistic Regression","2dc93160":"# 3.Normalize Imbalanced data\n* Most machine learning algorithms work best when the number of samples in each class are about equal. This is because most algorithms are designed to maximize accuracy and reduce error.","731544e3":"warning-----------------------------------------------------------------------------------------------warning\n*  code below is just extra things which I couldn't utilize. Could have good usage","e22d45ff":"> function below will go through each column one by one to do the describe, it is really good pratic but it takes long time to process. Should not be used unless it requires to go through each parameters one by one. ","53a350a3":"# Data cleanning (replace strange value in columns)\n* the value -99999 looks werid\/strange value, may need to replace","835fccfb":"# Feature Assembly","73b91d81":"# Transfer spark to panda dataframe due to bad visibility in the kaggle Kernel. Maybe there is solution to have better display. ","9b53fe57":"# 4.Modelling ","f7f2882b":"#summary of the data","689191d9":"# Feature assembly","172faaa7":"> Here I check number of rows for each ID?\n* Conclusion: there is one row for one ID\n\n","58b3e98c":"# 6.3 Gradient-Boosted Tree Classifier","e8e55688":"from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\nparamGrid = ParamGridBuilder()\\\n .addGrid(lr.aggregationDepth,[2,5,10])\\\n .addGrid(lr.elasticNetParam,[0.0, 0.5, 1.0])\\\n .addGrid(lr.fitIntercept,[False, True])\\\n .addGrid(lr.maxIter,[10, 100, 1000])\\\n .addGrid(lr.regParam,[0.01, 0.5, 2.0]) \\\n .build()","e373ec4b":"# Data cleanning (drop duplicate columns)","07a05f4a":"Model tuning, it find the best model or parameters for a given dataset to improve the performance.","3aaa1223":"# Data cleanning (drop duplicate rows)\n* maybe it is not good to drop, since different customer may have exist same profile","866207a5":"# Resampling techniques \u2014 Undersample majority class\nStratified Sampling can be used as well ","3e34b3db":"# Data cleanning (remove irrelevant)","14d6f12d":"# Data split","7f4b2cdf":"# 2.Processing and cleaning data","755cb949":"# Important Note\n\nAlways split into test and train sets BEFORE trying oversampling techniques! Oversampling before splitting the data can allow the exact same observations to be present in both the test and train sets. This can allow our model to simply memorize specific data points and cause overfitting and poor generalization to the test data.","9d7d3f07":"Understand the sample ratio. Balance between unsatisfied customers (1) and satisfied customers (0).","83b79c9b":"cv = CrossValidator(estimator=lr, estimatorParamMaps=paramGrid, evaluator=evaluator, numFolds=2)\n# Run cross validations\ncvModel = cv.fit(trainingData_undersample)\npredict_train=cvModel.transform(trainingData_undersample)\npredict_test=cvModel.transform(testData_undersample)\nprint(\"Cross-validation areaUnderROC for train set is {}\".format(evaluator.evaluate(predict_train)))\nprint(\"Cross-validation areaUnderROC for test set is {}\".format(evaluator.evaluate(predict_test)))","ec15a775":"# Resampling Techniques \u2014 Oversample minority class\n* Oversampling can be defined as adding more copies of the minority class. Oversampling can be a good choice when you don\u2019t have a ton of data to work with.\n\n* panda dataframe I would use SMOTE to oversample (I used SMOTE for panda analysis which I did)","50f3e510":"# 6.2 RandomForest","a368441d":"print(\"Before dorp column count: \",len(train_spark_NoID_NoDup.columns))\n#from pyspark.sql.functions import * is need for the countDistinct\nfrom pyspark.sql.functions import *\n#apply countDistinct on each column\ncol_counts = train_spark_NoID_NoDup.agg(*(countDistinct(col(c)).alias(c) for c in train_spark_NoID_NoDup.columns)).collect()[0].asDict()\n\n#select the cols with Distinct count=1 in an array\ncols_to_drop = [col for col in train_spark_NoID_NoDup.columns if col_counts[col] == 1 ]\n\n#drop the selected column\ntrain_spark_drop1Distinct = train_spark_NoID_NoDup.drop(*cols_to_drop)\nprint('Number of cols dropped: ',len(cols_to_drop))\nprint(\"After dorp column count after removing distince count =1 : \",len(train_spark_drop1Distinct.columns))","ca033b63":"# 4.2 RandomForest\nRandom Forests are a group of decision trees, that uses Mojority of Votingfor each of the decision tree. This algorithm provides less risk of overfitting by combining decision trees.","fe23a6d4":"# Feature selection Matrix (extra need bit more understanding and usage)\n* find it interesting, but no time to understand optimiza the usage\n* try it out with undersample data, since the data volumn is not high.","d4dca124":"# After study and understand the data with Panda, I did smililar data engineering here with Pyspark ","78cba110":"# 4.3 Gradient-Boosted Tree Classifier\nGradient-Boosted Tree Classifiers are also a group of decision trees and they iteratively train decision trees in order to minimize a loss function.","d65112e3":"# 5. ParamGridBuilder and CrossValidator\n find the best model or parameters for a given dataset to improve the performance\n# *I did run the code because it is time consuming and PC power.*****","c6ff0cf0":"Besides Logistic Regression (like Decision Trees or Random Forest which lack a model summary), therefore I used class CurveMetrics(BinaryClassificationMetrics):","4eeef8bf":"# 4.1 Logistic Regression","6cec234e":"Target column is 1 and 0 only,  It equals one for unsatisfied customers and 0 for satisfied customers.","38754000":"# Data cleanning (remove distinct count =1 )\n* code below used pure spark dataframe, if it is in panda dataframe, it will be calcuate each column STD.\n* Scripte is really slow, need improvement\n* script below only get one distinct column therefore, I remove it from running ","aec611c8":"# Check Null\n* there is no null value columns"}}