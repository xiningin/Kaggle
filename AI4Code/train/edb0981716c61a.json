{"cell_type":{"f72061a5":"code","69365b2d":"code","b0666bb7":"code","2d0b4985":"code","21a1635c":"code","d4b8e334":"code","061d29d3":"code","677a53f0":"code","213a0f03":"code","120ac079":"code","60153742":"code","57ec6cd1":"code","a6d1cb14":"code","bf89fbd4":"code","1ac0a6db":"code","11c3890e":"code","121b222c":"code","31c5374a":"markdown","034c3bfe":"markdown","e3c8a5a0":"markdown","1f90047b":"markdown","4f032f96":"markdown","e8a65cba":"markdown","4b7d47ab":"markdown"},"source":{"f72061a5":"# Loading all packages \nimport os\nimport json\nfrom pprint import pprint\nfrom copy import deepcopy\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nimport tensorflow_hub as hub\nimport tensorflow_text\nimport glob\nimport matplotlib.pyplot as plt\nimport scipy as sc\nimport warnings\nimport faiss  \nimport requests\nimport pickle\nfrom sklearn.metrics.pairwise import cosine_similarity\nplt.style.use('ggplot')\nimport re\n\nimport dash\nimport dash_core_components as dcc\nimport dash_html_components as html\nimport pandas as pd\nfrom dash.dependencies import Input, Output, State\nfrom flask import Flask\nimport os\nimport requests\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\nimport re\nimport string\nfrom nltk.tokenize import word_tokenize, sent_tokenize\nimport dash_bootstrap_components as dbc","69365b2d":"## Helper Functions\ndef format_name(author):\n    middle_name = \" \".join(author['middle'])\n    \n    if author['middle']:\n        return \" \".join([author['first'], middle_name, author['last']])\n    else:\n        return \" \".join([author['first'], author['last']])\n\n\ndef format_affiliation(affiliation):\n    text = []\n    location = affiliation.get('location')\n    if location:\n        text.extend(list(affiliation['location'].values()))\n    \n    institution = affiliation.get('institution')\n    if institution:\n        text = [institution] + text\n    return \", \".join(text)\n\ndef format_authors(authors, with_affiliation=False):\n    name_ls = []\n    \n    for author in authors:\n        name = format_name(author)\n        if with_affiliation:\n            affiliation = format_affiliation(author['affiliation'])\n            if affiliation:\n                name_ls.append(f\"{name} ({affiliation})\")\n            else:\n                name_ls.append(name)\n        else:\n            name_ls.append(name)\n    \n    return \", \".join(name_ls)\n\ndef format_body(body_text):\n    texts = [(di['section'], di['text']) for di in body_text]\n    texts_di = {di['section']: \"\" for di in body_text}\n    \n    for section, text in texts:\n        texts_di[section] += text\n\n    body = \"\"\n\n    for section, text in texts_di.items():\n        body += section\n        body += \"\\n\\n\"\n        body += text\n        body += \"\\n\\n\"\n    \n    return body\n\ndef format_bib(bibs):\n    if type(bibs) == dict:\n        bibs = list(bibs.values())\n    bibs = deepcopy(bibs)\n    formatted = []\n    \n    for bib in bibs:\n        bib['authors'] = format_authors(\n            bib['authors'], \n            with_affiliation=False\n        )\n        formatted_ls = [str(bib[k]) for k in ['title', 'authors', 'venue', 'year']]\n        formatted.append(\", \".join(formatted_ls))\n\n    return \"; \".join(formatted)\n\ndef load_files(dirname):\n    filenames = os.listdir(dirname)\n    raw_files = []\n\n    for filename in tqdm(filenames):\n        filename = dirname + filename\n        file = json.load(open(filename, 'rb'))\n        raw_files.append(file)\n    \n    return raw_files\n\ndef generate_clean_df(all_files):\n    cleaned_files = []\n    \n    for file in tqdm(all_files):\n        if 'abstract' in file:\n            features = [\n                file['paper_id'],\n                file['metadata']['title'],\n                format_authors(file['metadata']['authors']),\n                format_authors(file['metadata']['authors'], \n                               with_affiliation=True),\n                format_body(file['abstract']),\n                format_body(file['body_text']),\n                format_bib(file['bib_entries']),\n                file['metadata']['authors'],\n                file['bib_entries']\n            ]\n        else:\n            features = [\n                file['paper_id'],\n                file['metadata']['title'],\n                format_authors(file['metadata']['authors']),\n                format_authors(file['metadata']['authors'], \n                               with_affiliation=True),\n                format_body(file['body_text']),\n                format_body(file['body_text']),\n                format_bib(file['bib_entries']),\n                file['metadata']['authors'],\n                file['bib_entries']\n            \n            ]\n\n        cleaned_files.append(features)\n\n    col_names = ['paper_id', 'title', 'authors',\n                 'affiliations', 'abstract', 'text', \n                 'bibliography','raw_authors','raw_bibliography']\n\n    clean_df = pd.DataFrame(cleaned_files, columns=col_names)\n    clean_df.head()\n    \n    return clean_df\n\n","b0666bb7":"biorxiv_dir = '\/kaggle\/input\/CORD-19-research-challenge\/biorxiv_medrxiv\/biorxiv_medrxiv\/pdf_json\/'\nfilenames = os.listdir(biorxiv_dir)\nprint(\"Number of articles retrieved from biorxiv:\", len(filenames))\n\nall_files = []\n\nfor filename in filenames:\n    filename = biorxiv_dir + filename\n    file = json.load(open(filename, 'rb'))\n    all_files.append(file)\n","2d0b4985":"### Biorxiv: Generate CSV\n\ncleaned_files = []\n\nfor file in tqdm(all_files):\n    features = [\n        file['paper_id'],\n        file['metadata']['title'],\n        format_authors(file['metadata']['authors']),\n        format_authors(file['metadata']['authors'], \n                       with_affiliation=True),\n        format_body(file['abstract']),\n        format_body(file['body_text']),\n        format_bib(file['bib_entries']),\n        file['metadata']['authors'],\n        file['bib_entries']\n    ]\n    \n    cleaned_files.append(features)\n\ncol_names = [\n    'paper_id', \n    'title', \n    'authors',\n    'affiliations', \n    'abstract', \n    'text', \n    'bibliography',\n    'raw_authors',\n    'raw_bibliography'\n]\n\nclean_df = pd.DataFrame(cleaned_files, columns=col_names)\nclean_df.head()","21a1635c":"#Reading all CSV files and Concatenating final result\npmc_dir = '\/kaggle\/input\/CORD-19-research-challenge\/custom_license\/custom_license\/pdf_json\/'\npmc_files = load_files(pmc_dir)\npmc_df = generate_clean_df(pmc_files)\n# pmc_df.to_csv('clean_pmc.csv', index=False)\n\n\npmc_dir_1 = '\/kaggle\/input\/CORD-19-research-challenge\/custom_license\/custom_license\/pmc_json\/'\npmc_files_1 = load_files(pmc_dir_1)\n\npmc_df_1 = generate_clean_df(pmc_files_1)\n# pmc_df.to_csv('clean_pmc.csv', index=False)\n\n\ncomm_dir = '\/kaggle\/input\/CORD-19-research-challenge\/comm_use_subset\/comm_use_subset\/pdf_json\/'\ncomm_files = load_files(comm_dir)\ncomm_df = generate_clean_df(comm_files)\n\n\n\ncomm_dir_1 = '\/kaggle\/input\/CORD-19-research-challenge\/comm_use_subset\/comm_use_subset\/pmc_json\/'\ncomm_files_1 = load_files(comm_dir_1)\ncomm_df_1 = generate_clean_df(comm_files_1)\n\n\n\nnoncomm_dir = '\/kaggle\/input\/CORD-19-research-challenge\/noncomm_use_subset\/noncomm_use_subset\/pdf_json\/'\nnoncomm_files = load_files(noncomm_dir)\nnoncomm_df = generate_clean_df(noncomm_files)\n\n\n\nnoncomm_dir_1 = '\/kaggle\/input\/CORD-19-research-challenge\/noncomm_use_subset\/noncomm_use_subset\/pmc_json\/'\nnoncomm_files_1 = load_files(noncomm_dir_1)\nnoncomm_df_1 = generate_clean_df(noncomm_files_1)\n\n\n\ndf_covid_new = pd.concat([clean_df,pmc_df,pmc_df_1,comm_df,comm_df_1,noncomm_df,noncomm_df_1],axis=0,ignore_index=True)\n","d4b8e334":"#Reading Metadata file\nroot_path = '\/kaggle\/input\/CORD-19-research-challenge\/'\nmetadata_path = f'{root_path}\/metadata.csv'\nmeta_df = pd.read_csv(metadata_path, dtype={\n    'pubmed_id': str,\n    'Microsoft Academic Paper ID': str, \n    'doi': str\n})\n\ndf_covid_old = pd.merge(df_covid_new,meta_df[['sha','url']],left_on='paper_id',right_on='sha',how='left')\n\n## Saving the Doc information with their URLs.\ndf_covid_new[['paper_id','title','url']].to_csv('\/kaggle\/output\/df_docid_with_url.csv')\n","061d29d3":"#Reading all CSV files and Concatenating final result\nembed = hub.load(\"https:\/\/tfhub.dev\/google\/universal-sentence-encoder-large\/5\")","677a53f0":"## Creating a dictionary with Document as Key and Paragraphs as text\nshort_paragraph=[]\ndict1 = {}\nfor i in range(len(df_covid_old)):\n#     if dict1[df_covid.loc[i,'paper_id']] is not null:\n    dict1[df_covid_old.loc[i,'paper_id']] = re.split(r'(?:\\r?\\n){1,}', df_covid_old.loc[i,'text'])","213a0f03":"## Create Vector Embedding for all the Text Documents and storing in a dictionary with key as docId and values as paragraph embeddings\ndict_vector_old = {}\nfor key in list(dict1.keys()):\n    try:\n        dict_vector_old[key] = embed(dict1[key])\n        print(len(dict_vector_old))\n    except:\n        continue","120ac079":"## Matching Vector and Text Documents (if embedding vector generation fails, we are ignoring the document)        \ndict1_old= {}\nfor key in list(dict_vector_old.keys()):\n    if key in dict1:\n        dict1_old[key]=dict1[key]\n    print(len(dict1_old))","60153742":"## Storing the paragraph embeddings as pickle file for further use at \/kaggle\/output\/\nwith open('\/kaggle\/output\/dict1_text_v6.pickle', 'wb') as handle:\n    pickle.dump(dict1_old, handle, protocol=pickle.HIGHEST_PROTOCOL)\nwith open('\/kaggle\/output\/dict_vector_v6.pickle', 'wb') as handle:\n    pickle.dump(dict_vector_old, handle, protocol=pickle.HIGHEST_PROTOCOL)","57ec6cd1":"df_covid = df_covid_old\n\n## Reading the paragraph embeddings pickle files\nwith open('\/kaggle\/output\/dict1_text_v6.pickle', 'rb') as handle:\n    dict1_text_v1 = pickle.load(handle)\n\nwith open('\/kaggle\/output\/dict_vector_v6.pickle', 'rb') as handle:\n    dict_vector = pickle.load(handle)","a6d1cb14":"\n## Building the index for semantic search for faiss\nindex = faiss.IndexFlatL2(512)   # build the index\nfor vector in list(dict_vector.keys()):\n    index.add(dict_vector[vector].numpy())                  # add vectors to the index\n","bf89fbd4":"## Creating a list of documents with their docid and paragraph text to get the results\ntext=[]\ndocId=[]\nfor key in list(dict1_text_v1.keys()):\n\n    text.extend(dict1_text_v1[key])\n    doc=[key]*len(dict1_text_v1[key])\n    docId.extend(doc)\n    ","1ac0a6db":"## Enter the required query to be searched upon\n## Below is the query to search \"Seasonality of transmission of corona virus\"\nsearch = [''' Seasonality of transmission of corona virus''']\n\n## Creating the embedding vectors for the query\npredictions = embed(search)\n\nk = 10                         # we want to see 10 nearest neighbors\nD, I = index.search(np.array(predictions,dtype='float32'), k) # sanity check\nprint(I)\nprint(D)\n\nfor i in range(k):\n    print(text[I[0][i]])\n    print(docId[I[0][i]])\n    print('\\n')\n    ","11c3890e":"# Supporting function for calculation of Text summarization using Extraction-based text summarization\nexternal_stylesheets=[dbc.themes.BOOTSTRAP]\n\nserver = Flask(__name__)\nserver.secret_key = os.environ.get('secret_key', 'secret')\napp = dash.Dash(name = __name__, server = server, external_stylesheets = external_stylesheets)\n#external_stylesheets = ['https:\/\/codepen.io\/chriddyp\/pen\/bWLwgP.css']\n\n#app.config['suppress_callback_exceptions'] = True\n\ndf = pd.read_csv('https:\/\/raw.githubusercontent.com\/rahulpoddar\/dash-deploy-exp\/master\/TASK1_annotated_1_v3.csv', encoding='latin1')\n\ntasks = df['Task Name'].unique().tolist()\n\ndef data_prep(inpt): \n    clean_data = []\n    article3 = ' '.join(inpt)\n    result=re.sub(\"\\d+\\.\", \" \", article3)\n    clean_data.append(result)\n            \n    clean_data = pd.DataFrame(clean_data)\n    clean_data.columns = ['Remediation']\n    clean_data['Remediation'] = clean_data['Remediation'].astype('str')\n\n    clean_data1 = clean_data['Remediation']\n    clean_data2 = []\n    regex = r\"(?<!\\d)[-,_;:()](?!\\d)\"\n    for i in range(1):\n        result2 = re.sub(regex,'',clean_data1.loc[i])\n        clean_data2.append(result2)\n    clean_data2 = pd.DataFrame(clean_data2)\n    clean_data2.columns = ['Remediation']\n    clean_data2['Remediation'] = clean_data2['Remediation'].astype('str')\n    \n    return (clean_data2)\n\ndef _create_dictionary_table(text_string) -> dict:\n   \n    # Removing stop words\n    stop_words = set(stopwords.words(\"english\"))\n        \n    words = word_tokenize(text_string)\n    \n    # Reducing words to their root form\n    stem = PorterStemmer()\n    \n    # Creating dictionary for the word frequency table\n    frequency_table = dict()\n    for wd in words:\n        wd = stem.stem(wd)\n        if wd in stop_words:\n            continue\n        if wd in frequency_table:\n            frequency_table[wd] += 1\n        else:\n            frequency_table[wd] = 1\n\n    return frequency_table\n\ndef _calculate_sentence_scores(sentences, frequency_table) -> dict:   \n\n    # Algorithm for scoring a sentence by its words\n    sentence_weight = dict()\n\n    for sentence in sentences:\n        sentence_wordcount = (len(word_tokenize(sentence)))\n        sentence_wordcount_without_stop_words = 0\n        for word_weight in frequency_table:\n            if word_weight in sentence.lower():\n                sentence_wordcount_without_stop_words += 1\n                if sentence[:7] in sentence_weight:\n                    sentence_weight[sentence[:7]] += frequency_table[word_weight]\n                else:\n                    sentence_weight[sentence[:7]] = frequency_table[word_weight]\n\n        sentence_weight[sentence[:7]] = sentence_weight[sentence[:7]] \/(sentence_wordcount_without_stop_words)\n      \n    return sentence_weight\n\ndef _calculate_average_score(sentence_weight) -> int:\n   \n    # Calculating the average score for the sentences\n    sum_values = 0\n    for entry in sentence_weight:\n        sum_values += sentence_weight[entry]\n\n    # Getting sentence average value from source text\n    average_score = (sum_values \/ (len(sentence_weight)))\n\n    return average_score\n\ndef _get_article_summary(sentences, sentence_weight, threshold):\n    sentence_counter = 0\n    article_summary = ''\n\n    for sentence in sentences:\n        if sentence[:7] in sentence_weight and sentence_weight[sentence[:7]] >= (threshold):\n            article_summary += \" \" + sentence\n            sentence_counter += 1\n\n    return article_summary\n\ndef _run_article_summary(article):\n    \n    #creating a dictionary for the word frequency table\n    frequency_table = _create_dictionary_table(article)\n\n    #tokenizing the sentences\n    sentences = sent_tokenize(article)\n\n    #algorithm for scoring a sentence by its words\n    sentence_scores = _calculate_sentence_scores(sentences, frequency_table)\n\n    #getting the threshold\n    threshold = _calculate_average_score(sentence_scores)\n\n    #producing the summary\n    article_summary = _get_article_summary(sentences, sentence_scores, 1 * threshold)\n\n    return article_summary\n\ndef _output(inpt):\n    new = []\n    df = data_prep(inpt)\n    df_rem = df['Remediation']\n    #sentences = sent_tokenize(df_rem[0])\n    summary_results = _run_article_summary(df_rem[0])\n    new.append(summary_results)\n    return(new)\n'''\ndef generate_summary(task):\n    return 'Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.'\n'''","121b222c":"def generate_table(dff):\n    rows = []\n    for i in range(len(dff)):\n        row = []\n        for col in ['Title', 'Output']:\n            value = dff.iloc[i][col]\n            url = dff.iloc[i]['URL']\n            if col == 'Title':\n                cell = html.Td(html.A(href=url, children = value))\n            else:\n                cell = html.Td(children = value)\n            row.append(cell)\n        rows.append(html.Tr(row))\n    return dbc.Table(\n        # Header\n        [html.Tr([html.Th(col,  style={'text-align':'center'}) for col in ['Title', 'Search Output']]) ] +\n        # Body\n        rows,\n        bordered=True,\n        dark=False,\n        hover=True,\n        responsive=True,\n        striped=True,\n    )\n\n\napp.layout = html.Div([\n        html.Div([\n        html.H1('COVID-19 Open Research Dataset Challenge (CORD-19)', style = {'margin-left': '10%', 'margin-top': '5%'}),\n        html.Hr(),\n        html.Div([\n        html.H3('Type a general query (e.g. \"What is Corona Virus?\"):'),\n        html.Br(),\n        dbc.Input(id = 'general-search', type = 'text', placeholder = 'Type a query', value = ''),\n        html.Br(),\n        dbc.Button(id='submit-button-state', n_clicks=0, children='Submit', color = \"primary\", className=\"mr-2\", style = {'margin-left': '46%'}),\n        ], style = {'width': '80%', 'margin': 'auto'}),\n        html.Hr(),\n        html.Div([html.H3('OR')],style = {'margin-left': '48%'}),\n        html.Hr(),\n        html.Div([\n        html.H3('Select a task:'),\n        dcc.Dropdown(\n        id='task-dropdown',\n        options=[\n            {'label': i, 'value': i} for i in tasks \n        ],\n        placeholder=\"Select a task\",\n    ),\n        html.Br(),\n        html.Div([\n                html.H3('Select a sub-task:'),\n                dcc.Dropdown(\n                        id='sub-task-dropdown',\n                        placeholder = \"Select a sub-task\",\n                        ),\n                ], id = 'sub-task'),\n                ], style = {'margin-left': '10%','margin-right': '10%'}),\n    ]),\n    html.Hr(),\n    html.Div([\n                html.H3('Response Summary'),\n                html.Div(id = 'task-summary'),\n                html.Div(id = 'query-summary')\n                        ], style = {'margin-left': '10%','margin-right': '10%'}),\n    html.Hr(),\n    html.Div([\n            html.H3('Search Results'),\n            html.Div(id = 'task-results'),\n            html.Div(id = 'query-results')\n            ], id = 'search-results-main', style = {'margin-left': '10%','margin-right': '10%'}),\n    html.Hr(),\n])\n\n@app.callback(\n    Output('sub-task-dropdown', 'options'),\n    [Input('task-dropdown', 'value')])\ndef set_subtask_options(selected_task):\n    if selected_task != None:\n        dff = df[df['Task Name'] == selected_task]\n        options = dff['Sub-tasks'].unique().tolist()\n        return [{'label': i, 'value': i} for i in options]\n    else:\n        return [{'label': i, 'value': i} for i in []]\n    \n@app.callback(\n    Output('sub-task-dropdown', 'value'),\n    [Input('sub-task-dropdown', 'options')])\ndef set_subtask_value(available_options):\n    if available_options != []:\n        return available_options[0]['value']\n    else:\n        return ''\n    \n@app.callback(\n    Output('task-summary', 'children'),\n    [Input('sub-task-dropdown', 'value')])\ndef update_taks_summary(value):\n    if value != '':\n        dff = df[df['Sub-tasks'] == value]\n        return _output(dff['Output'].tolist())[0]\n    else:\n        return ''\n\n\n@app.callback(\n    Output('task-results', 'children'),\n    [Input('sub-task-dropdown', 'value')])\ndef update_taks_results(value):\n    if value != '':\n        dff = df[df['Sub-tasks'] == value]\n        return generate_table(dff)\n    else:\n        return ''\n    \n@app.callback(\n        Output('query-results', 'children'),\n         [Input('submit-button-state', 'n_clicks')],\n         [State('general-search', 'value')]\n         )\ndef populate_search_results(n_clicks, value):\n    if value != '':\n        query = value\n        response = requests.post(\"https:\/\/nlp.biano-ai.com\/develop\/test\", json={\"texts\": [query]})\n        predictions = response.json()['predictions']\n        pred_df = pd.DataFrame(predictions[0])\n        pred_df.columns = ['Distance', 'Document ID', 'Output', 'Title', 'URL']\n        return generate_table(pred_df)\n    else:\n        return ''\n\n@app.callback(\n        Output('query-summary', 'children'),\n         [Input('submit-button-state', 'n_clicks')],\n         [State('general-search', 'value')]\n         )\ndef generate_search_summary(n_clicks, value):\n    if value != '':\n        query = value\n        response = requests.post(\"https:\/\/nlp.biano-ai.com\/develop\/test\", json={\"texts\": [query]})\n        predictions = response.json()['predictions']\n        pred_df = pd.DataFrame(predictions[0])\n        return _output(pred_df['text'].tolist())[0]\n    else:\n        return ''\n\nif __name__ == '__main__':\n    app.run_server(debug=True)","31c5374a":"# METHODOLOGY :  <br>\n\n**1) CREATION OF EMBEDDINGS**\n\n         a. Parsed 59,000 documents (source: https:\/\/www.kaggle.com\/allen-institute-for-ai\/CORD-19-research-challenge)\n         b. Created embedding vectors for each paragraph (~10million), using universal sentence encoder, (https:\/\/tfhub.dev\/google\/universal-sentence-encoder\/5)\n                  \n**2) SEMANTIC SIMILARITY MATCH**\n\n        a. Sub-task questions mined for answers using similarity match from \u201cfaiss\u201d library.\n        b. Reduce false positives by further refining search of articles in 2a which contains Covid-19 like terms (Coronavirus disease 2019, COVID-19, SARS-CoV-2, Severe acute respiratory syndrome coronavirus 2, 2019-nCoV, SARSr-CoV)\n        c. Reduce false positives by further validation from scientific scientists in Novartis based on context \n                  \n**3) TEXT SUMMARIZATION & VISUALIZATION**\n\n       a. Extraction-based text summarization performed on the results from 2c, in an automated way. \n       (source: https:\/\/blog.floydhub.com\/gentle-introduction-to-text-summarization-in-machine-learning\/)\n         i) Convert Paragraph to sentences and calculate word weightage.\n        ii) Calculate the average word weightage by dividing the sum of weightage by total number of words.\n       iii) Select the sentence with the highest average weight.\n       b. Create an API using dash in order to view the results and extend for more advanced search.**","034c3bfe":"# RESULTS & OUTCOME: <br>\n**1) Summary:** \nWe find that SARS-CoV-2 lineages were imported multiple times into Guangdong during the second half of January 2020 (Figure 4). Three clusters (C, D, E) have earlier tMRCAs that coincide with the start of the Guangdong epidemic and two (A, B) have later tMRCAs, around the time of the epidemic peak in the province (Figure 4). The average time between the tMRCA and the earliest sequence collection date in each cluster was approximately 10.5 days. The observed duration of each phylogenetic cluster (tMRCA to most recently sampled sequence) ranged from 13.8 (cluster B) to 45.5 (cluster D) days. The clusters with earlier tMRCAs contain more sequences sampled from travellers outside of China, possibly reflecting a decrease in air passenger travel from Guangdong after January 2020 (Flightradar 2020). The median tMRCA estimate of the COVID-19 pandemic was 29th November 2019. the degree of diversification of 2019-nCoV was much smaller when compared with influenza A viruses. \n\n \n\n COVID-19 is thought to be transmitted from the animals, though it has not yet been clear exactly from which animal, however the animals have been the sources of transmission as described in Fig. 1 . The recent finding shows that SARS-CoV-2 is 96% identical to a bat coronavirus. It might also be desirable to monitor farm animals and pet cats for infection with 2019-nCoV, since their ACE2 receptor responsible for 2019-nCoV binding differs in only a few amino acids from human ACE2. Surveillance might prevent the virus establishing itself in another animal species that is in close contact to humans. In addition, in light of the fact that there are multiple species of CoVs circulating in wildlife species and that these animals are constantly interacting with each other, host-species expansion or interspecies transmission of new CoV to humans seems to be inevitable.\n\n**2) NOVARTIS_COVID-19_SEARCH **: https:\/\/dash-app-deploy.herokuapp.com\/ <br>","e3c8a5a0":"# 2. SEMANTIC SIMILARITY MATCH","1f90047b":"OBJECTIVE\n# What do we know about virus genetics, origin, and evolution?","4f032f96":"# 1. Creating Embeddings","e8a65cba":"# 3. Text Summarization and Vizualization \n","4b7d47ab":"**SOLUTION SUMMARY**\n# Scalable, fast and efficient search developed and aided with domain experts. Created embedding from 60K journal articles, augmented with summarization on top relevant sections for task specific questions. \n\n# Scalable, to allow for more questions beyond tasks please use the intelligent search and summarization engine,https:\/\/dash-app-deploy.herokuapp.com\/"}}