{"cell_type":{"d74af8f7":"code","51378909":"code","14e8d7b0":"code","7c3ae320":"code","99a91bd7":"code","9d40b56f":"code","8ea82f35":"code","8c4c3281":"code","ffd65acb":"code","f38c71af":"code","f5735bc1":"code","47d6d3d4":"code","ee659c2e":"code","9cf1d616":"code","d5096f61":"code","56083f6e":"code","a6808ca7":"code","bce9f429":"code","a96d3ba3":"code","c5af7338":"code","cecc3800":"code","ce7b71cb":"code","3e739e0a":"code","788f5c2d":"code","1d78220b":"code","874cc94a":"code","b4f0ccc4":"code","8948a4ba":"code","c6ade5fe":"code","4bf12221":"code","8891bb95":"code","b1a90c74":"code","dd9e43b7":"code","157a74e2":"code","32215cea":"code","374a18b2":"code","48b1cd02":"code","fff1dab9":"code","e201b2ba":"code","c85f77fb":"code","0b50fe26":"markdown","91e42346":"markdown","80895ec7":"markdown","52fc5c83":"markdown","b531fe69":"markdown","7fab96bb":"markdown","15184356":"markdown","46e0d61b":"markdown"},"source":{"d74af8f7":"# Importing the Imp libraries for model making and analysis\n\nimport numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport statsmodels.api as sm\nfrom statsmodels.tsa.stattools import adfuller\nfrom statsmodels.graphics.tsaplots import plot_acf, plot_pacf\nfrom sklearn.metrics import mean_squared_error\nfrom math import sqrt\nimport warnings\n\nwarnings.filterwarnings('ignore')\n%matplotlib inline","51378909":"import os\n# Reading the dataset \n# Get the current working directory (cwd)\ncwd = os.getcwd()   \n# Get all the files in that directory\nfiles = os.listdir(cwd) \nprint(\"Files in %r: %s\" % (cwd, files))","14e8d7b0":"data = pd.read_csv(\"..\/input\/time-series-data-on-climate-change\/EditedByCity.csv\",delimiter=\",\")\ndata","7c3ae320":"data.info()","99a91bd7":"delhi = data.loc[data['City'] == ('New Delhi' or \"Delhi\"), ['Date','AverageTemperature']]\ndelhi.columns = ['Date','Temp']\n\ndelhi['Date'] = pd.to_datetime(delhi['Date'])\ndelhi.reset_index(drop=True, inplace=True)\ndelhi.set_index('Date', inplace=True)\ndelhi","9d40b56f":"delhi['Month'] = delhi.index.month\ndelhi['Year'] = delhi.index.year\ndelhi","8ea82f35":"plt.figure(figsize=(22,6))\nsns.lineplot(x=delhi.index, y=delhi['Temp'])\nplt.title('Temperature Variation in Delhi from 1980 until 2013')\nplt.xlabel(\"Date\",fontsize=14)\nplt.ylabel(\"Temperature in C'\",fontsize=14)\nplt.legend()\nplt.show()\n","8c4c3281":"# For better analysis and getting more insight on trends and pattern of temperature change in the New Delhi\n# We will be creating a SpreadSheet\/Pivot Table to map temperature change in each month \npivot = pd.pivot_table(delhi, values='Temp', index='Month', columns='Year', aggfunc='mean')\npivot","ffd65acb":"\npivot.plot(figsize=(20,6), \n           colormap=\"coolwarm\")\nplt.title('Yearly Delhi temperatures')\nplt.xlabel('Months')\nplt.ylabel('Temperatures')\n\nplt.xticks([x for x in range(1,13)])\nplt.legend().remove()\nplt.show()\n","f38c71af":"monthly_seasonality = pivot.mean(axis=1)\nmonthly_seasonality.plot(figsize=(20,6))\nplt.title('Monthly Temperatures in Delhi')\nplt.xlabel('Months',fontsize=14)\nplt.ylabel('Temperature',fontsize=14)\nplt.xticks([x for x in range(1,13)])\n\nplt.show()","f5735bc1":"\nyear_avg = pd.pivot_table(delhi, values='Temp', index='Year', aggfunc='mean')\n# The rolling mean of Temp in last 10 years\n# Rolling mean window is 3\n\nyear_avg['10 Years MA'] = year_avg['Temp'].rolling(3).mean()\nyear_avg[['Temp','10 Years MA']].plot(kind=\"line\",\n                                      figsize=(20,6),\n                                      colormap=\"cool\",\n                                      marker='o',)\n\nplt.title('Yearly AVG Temperatures in Delhi')\nplt.xlabel('Years')\nplt.ylabel('Temperature')\nplt.xticks([x for x in range(1980,2012,2)])\n\nplt.show()","47d6d3d4":"max(year_avg['Temp'])-min(year_avg['Temp'])","ee659c2e":"# Before moving forward, lets first split the dataset into Training, Validation and Testing\n# Now since its not any random sequence data therfore use of Random value and sample bias doesnt apply here\n# We can simply split the data using simple slicing and cutting operation on DataFrame\n","9cf1d616":"# Train Size is 200\ndf_train = delhi[-300:]\n# Val size is 100\ndf_val = delhi[200:300]\n# Test Size is 100\ndf_test = delhi[:-200]","d5096f61":"print(\"Training Dataset: \",len(df_train))\nprint(\"Validation Dataset: \",len(df_val))\nprint(\"Testing Dataset: \",len(df_test))","56083f6e":"# No Null value\ndf_train.isnull().sum()","a6808ca7":"from statsmodels.tsa.stattools import adfuller\n\nprint('Augmented Ducky Fuller Test Results')\n\ntest_data = adfuller(df_train.iloc[:,0].values,autolag='AIC')\n\ndata_output = pd.Series(test_data[0:4],index=['Test Statistic','p-value','Lags Used','Number of Observation Used'])\n\nfor key,value in test_data[4].items():\n    \n    data_output['Critical value (%s)'%key] = value\n    \nprint(data_output)","bce9f429":"import math\ndef check_stationarity(y, lags_plots=48, figsize=(22,8)):\n       \n    # Creating plots of the DF\n    y = pd.Series(y)\n    fig = plt.figure()\n\n    ax1 = plt.subplot2grid((3, 3), (0, 0), colspan=2)\n    ax2 = plt.subplot2grid((3, 3), (1, 0))\n    ax3 = plt.subplot2grid((3, 3), (1, 1))\n    ax4 = plt.subplot2grid((3, 3), (2, 0), colspan=2)\n\n    y.plot(ax=ax1, figsize=figsize)\n   \n    ax1.set_title('Delhi Temperature Variation')\n    plot_acf(y, lags=lags_plots, zero=False, ax=ax2);\n    plot_pacf(y, lags=lags_plots, zero=False, ax=ax3);\n    sns.distplot(y, bins=int(sqrt(len(y))), ax=ax4)\n    ax4.set_title('Distribution Chart')\n\n    plt.tight_layout()\n    \n\n    \n    print('Results of Dickey-Fuller Test:')\n    adfinput = adfuller(y)\n    adftest = pd.Series(adfinput[0:4], index=['Test Statistic','p-value','Lags Used','Number of Observations Used'])\n    adftest = round(adftest,4)\n    \n    for key, value in adfinput[4].items():\n        adftest[\"Critical Value (%s)\"%key] = value.round(4)\n        \n    print(adftest)\n    \n    if adftest[0].round(2) < adftest[5].round(2):\n        print('\\nThe Test Statistics is lower than the Critical Value of 5%.\\nThe serie seems to be stationary')\n    else:\n        print(\"\\nThe Test Statistics is higher than the Critical Value of 5%.\\nThe serie isn't stationary\")","a96d3ba3":"# The first approach is to check the series without any transformation\n\ncheck_stationarity(df_train[\"Temp\"])","c5af7338":"#Now we break the data into sub section\n\nfrom statsmodels.tsa.seasonal import seasonal_decompose\n\ndecomp= seasonal_decompose(df_train[\"Temp\"],period=3)\ntrend = decomp.trend\nseasonal = decomp.seasonal\nresidual = decomp.resid\n\nplt.subplot(411)\nplt.figure(figsize=(20,8))\nplt.plot(df_train[\"Temp\"],'r-')\nplt.xlabel('Original')\nplt.figure(figsize=(20,8))\n\nplt.subplot(412)\nplt.plot(trend,'k-')\nplt.xlabel('Trend')\nplt.figure(figsize=(20,8))\n\nplt.subplot(413)\nplt.plot(seasonal,'g-')\nplt.xlabel('Seasonal')\nplt.figure(figsize=(20,8))\n\nplt.subplot(414)\nplt.plot(residual,'y-')\nplt.xlabel('Residual')\nplt.figure(figsize=(20,8))\n\nplt.tight_layout()","cecc3800":"# Pandas dataframe.diff() is used to find the first discrete difference of objects over the given axis. \n# We can provide a period value to shift for forming the difference.\ncheck_stationarity(df_train['Temp'].diff(12).dropna())","ce7b71cb":"def walk_forward(training_set, validation_set, params):\n    '''\n    Params: it's a tuple where you put together the following SARIMA parameters: ((pdq), (PDQS), trend)\n    '''\n    history = [x for x in training_set.values]\n    prediction = list()\n    \n    # Using the SARIMA parameters and fitting the data\n    pdq, PDQS, trend = params\n\n    #Forecasting one period ahead in the validation set\n    for week in range(len(validation_set)):\n        model = sm.tsa.statespace.SARIMAX(history, order=pdq, seasonal_order=PDQS, trend=trend)\n        result = model.fit(disp=False)\n        yhat = result.predict(start=len(history), end=len(history))\n        prediction.append(yhat[0])\n        history.append(validation_set[week])\n        \n    return prediction","3e739e0a":"# Let's test it in the validation set\ndf_val['Pred'] = walk_forward(df_train['Temp'], df_val['Temp'], ((3,0,0),(0,1,1,12),'c'))","788f5c2d":"# Measuring the error of the prediction\ndef measure_rmse(y_true, y_pred):\n    return sqrt(mean_squared_error(y_true,y_pred))\n\nrmse_pred = measure_rmse(df_val['Temp'], df_val['Pred'])\n\nprint(f\"The RMSE of the SARIMA(3,0,0),(0,1,1,12),'c' model was {round(rmse_pred,4)} celsius degrees\")\n#print(f\"It's a decrease of {round((rmse_pred\/rmse_base-1)*100,2)}% in the RMSE\")","1d78220b":"# Creating the error column\ndf_val['Error'] = df_val['Temp'] - df_val['Pred']\ndf_val","874cc94a":"def plot_error(data, figsize=(20,8)):\n    '''\n    There must have 3 columns following this order: Temperature, Prediction, Error\n    '''\n    plt.figure(figsize=figsize)\n    ax1 = plt.subplot2grid((2,2), (0,0))\n    ax2 = plt.subplot2grid((2,2), (0,1))\n    ax3 = plt.subplot2grid((2,2), (1,0))\n    ax4 = plt.subplot2grid((2,2), (1,1))\n    \n    #Plotting the Current and Predicted values\n    ax1.plot(data.iloc[:,0:2])\n    ax1.legend(['Real','Pred'])\n    ax1.set_title('Current and Predicted Values')\n    \n    # Residual vs Predicted values\n    ax2.scatter(data.iloc[:,1], data.iloc[:,2])\n    ax2.set_xlabel('Predicted Values')\n    ax2.set_ylabel('Errors')\n    ax2.set_title('Errors versus Predicted Values')\n    \n    ## QQ Plot of the residual\n    sm.graphics.qqplot(data.iloc[:,2], line='r', ax=ax3)\n    \n    # Autocorrelation plot of the residual\n    plot_acf(data.iloc[:,2], lags=(len(data.iloc[:,2])-1),zero=False, ax=ax4)\n    plt.tight_layout()\n    plt.show()","b4f0ccc4":"df_val.drop(['Month','Year'], axis=1, inplace=True)\n","8948a4ba":"df_val","c6ade5fe":"plot_error(df_val)","4bf12221":"#Creating the new concatenating the training and validation set:\n\nfuture = pd.concat([df_train['Temp'], df_val['Temp']])\nfuture","8891bb95":"# Using the same parameters of the fitted model\n\nmodel = sm.tsa.statespace.SARIMAX(future, order=(3,0,0), seasonal_order=(0,1,1,12), trend='c')\nresult = model.fit(disp=False)\n","b1a90c74":"df_test['Pred'] = result.predict(start=1, end=len(df_test))\ndf_test[df_test.isna().any(axis=1)]\n","dd9e43b7":"df_test_plot=df_test.dropna(how='any' ,axis=0)\ndf_test_plot","157a74e2":"df_test_plot[['Temp', 'Pred']].plot(kind=\"line\",\n                                      figsize=(20,6),\n                                      colormap=\"cool\",\n                                      marker='o',)\n\nplt.title('Current Values compared to the Extrapolated Ones')\n\nplt.xlabel('Year')\nplt.ylabel('Temperature')\n\nplt.show()","32215cea":"result.summary()","374a18b2":"fc = result.forecast(len(df_test), alpha=0.05) \nfc","48b1cd02":"from statsmodels.tsa.stattools import acf\n#  Accuracy metrics\n\ndef forecast_accuracy(forecast, actual):\n    corr = np.corrcoef(forecast, actual)[0,1]   # corr\n    mins = np.amin(np.hstack([forecast[:,None], \n                              actual[:,None]]), axis=1)\n    maxs = np.amax(np.hstack([forecast[:,None], \n                              actual[:,None]]), axis=1)\n    minmax = 1 - np.mean(mins\/maxs)             # minmax\n    acf1 = acf(fc-df_test['Temp'])[1]                      # ACF1\n    #return({'mape':mape, 'me':me, 'mae': mae, 'mpe': mpe, 'rmse':rmse, 'acf1':acf1,'corr':corr, 'minmax':minmax})\n    \n    return({'corr':corr,\n            'minmax':minmax})\n\nforecast_accuracy(fc, df_test['Temp'])","fff1dab9":"temp=df_test['Temp'].values\ntemp","e201b2ba":"fc_values=fc.values\nfc_values","c85f77fb":"mape = np.mean(np.abs(fc_values - temp)\/np.abs(temp))  \nprint('mape:',mape)\nme = np.mean(fc_values - temp)             # ME\nprint('me:',me)\nmae = np.mean(np.abs(fc_values - temp))    # MAE\nprint('mae:',mae)\n\nmpe = np.mean((fc_values - temp)\/temp)   # MPE\nprint('mpe:',mpe)\nrmse = np.mean((fc_values - temp)**2)**.5  # RMSE\nprint('rmse:',rmse)\ncorr = np.corrcoef(fc_values, temp)[0,1]   # corr\nprint('corr:',corr)\nmins = np.amin(np.hstack([fc_values[:,None],temp[:,None]]), axis=1)\n\nmaxs = np.amax(np.hstack([fc_values[:,None], temp[:,None]]), axis=1)\n\nminmax = 1 - np.mean(mins\/maxs)\nprint('minmax:',minmax)","0b50fe26":"## What is SARIMA?\n\n- Seasonal Autoregressive Integrated Moving Average, SARIMA or Seasonal ARIMA, is an extension of ARIMA that explicitly supports univariate time series data with a seasonal component.\n\n- It adds three new hyperparameters to specify the autoregression (AR), differencing (I) and moving average (MA) for the seasonal component of the series, as well as an additional parameter for the period of the seasonality.\n\n- How to Configure SARIMA\nConfiguring a SARIMA requires selecting hyperparameters for both the trend and seasonal elements of the series.\n\n- Trend Elements\n    There are three trend elements that require configuration. They are the same as the ARIMA model; specifically:\n        p: Trend autoregression order.\n        d: Trend difference order.\n        q: Trend moving average order.","91e42346":"### ADCF Test: ADCF stands for Augmented Dickey-Fuller test which is a statistical unit root test. It gives us various values which can help us identifying stationarity.\n - It comprises Test Statistics & some critical values for some confidence levels. If the Test statistics is less than the critical values, we can reject the null hypothesis & say that the series is stationary. \n - The Null hypothesis says that time series is non-stationary. THE ADCF test also gives us a p-value. According to the null hypothesis, lower values of p is better.","80895ec7":"## Therefore we can say that the Average Change in Temperature happened across 1980-2013 is 2.0010833333333338","52fc5c83":"> ## As test statistic has lesser value than critical values. So,we can say that time series is stationary.\n","b531fe69":"## It is clearly visible through the plot that their exist a Temperature rise in month of May, June and July and the temperature descent in month of November and December\n","7fab96bb":"## We can see some trends in the dataset. Let see it thru some Time Analysis Theory","15184356":"## Now, Lets try to make a Machine Learning Model","46e0d61b":"<h1> We are going to use the Clean and Pre-processed Data <\/h1>"}}