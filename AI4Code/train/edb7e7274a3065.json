{"cell_type":{"74fd6008":"code","8c748d97":"code","1701706a":"code","c85b902f":"code","10cecd74":"code","bc2a222a":"code","190ad5aa":"code","b39eacaa":"code","1f70b323":"code","c7055a9e":"code","d48d855a":"code","a12d815e":"code","1346e2ec":"code","bb150191":"code","92042cac":"code","095c0035":"code","b99b9d96":"code","96463f8a":"code","7aa36766":"code","17439bcd":"code","9ec74032":"code","2880e016":"code","30796a16":"code","28da5a1e":"code","70742f7b":"code","5026af5c":"code","5a8c6092":"code","3454e784":"code","641df35e":"code","4367098b":"code","01f19eb1":"code","e10c19cc":"code","321f7a0d":"code","9c353e73":"code","7a84b7f4":"code","6cd0c71a":"code","268da6a7":"code","3dbeb28d":"code","b7f34ed1":"code","373d49f0":"code","cc9bf439":"code","24e6cdd5":"code","6861f0ce":"code","a9c08e4a":"code","25b8f750":"code","b0e830d9":"code","e7fa3cf0":"code","a896ed63":"code","7a7a476b":"code","a35b9758":"code","c15c609b":"code","2eb48255":"code","8939d0ef":"code","c76640eb":"code","24bdeff6":"code","6af21e1d":"code","ce8efd97":"code","224b6d99":"code","b2476481":"markdown","346bb72a":"markdown","b2e5fbc6":"markdown","9b59712d":"markdown","17c6bcbd":"markdown","26840d33":"markdown","762bbce7":"markdown","6f0581b5":"markdown","40689c29":"markdown","14f94eaa":"markdown","df6854b5":"markdown","0e8312da":"markdown","153bcd13":"markdown","7cd651d9":"markdown"},"source":{"74fd6008":"# !pip install --upgrade pip","8c748d97":"# !pip3 install lightgbm","1701706a":"#### basic\nimport pandas as pd\nimport numpy as np\n\n#### Visulization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()\n%matplotlib inline\n\n#### ML\nimport sklearn\nfrom sklearn import tree\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import StratifiedKFold, KFold, TimeSeriesSplit, train_test_split\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.tree import DecisionTreeClassifier\nimport lightgbm as lgb\n# import xgboost as xgb\n\n#### Others\nimport datetime\nimport os, warnings, random\nwarnings.filterwarnings('ignore')","c85b902f":"transaction_train = pd.read_csv('..\/input\/ieee-fraud-detection\/train_transaction.csv',index_col='TransactionID')\ntransaction_test = pd.read_csv('..\/input\/ieee-fraud-detection\/test_transaction.csv',index_col='TransactionID')\nidentity_train = pd.read_csv('..\/input\/ieee-fraud-detection\/train_identity.csv',index_col='TransactionID')\nidentity_test = pd.read_csv('..\/input\/ieee-fraud-detection\/test_identity.csv',index_col='TransactionID')","10cecd74":"print('training set # for transaction: ' + str(len(transaction_train)))\ntransaction_train.head()","bc2a222a":"print('test set # for transaction: ' + str(len(transaction_test)))\ntransaction_test.head()","190ad5aa":"print('training set # for identity: ' + str(len(identity_train)))\nidentity_train.head(3)","b39eacaa":"train = pd.merge(transaction_train, identity_train, on='TransactionID', how='left',indicator = True)\ntest = pd.merge(transaction_test, identity_test, on='TransactionID', how='left',indicator = True)","1f70b323":"train['_merge'].value_counts()","c7055a9e":"print(len(train))\ntrain.head()","d48d855a":"#helper functions\n\n## Seeder\n# :seed to make all processes deterministic \ndef seed_everything(seed=0):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n\n## Memory Reducer                                     \ndef reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() \/ 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() \/ 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) \/ start_mem))\n    return df","a12d815e":"SEED = 42\nseed_everything(SEED)","1346e2ec":"train = reduce_mem_usage(train)\ntest = reduce_mem_usage(test)","bb150191":"#################################################################################\n# Converting Strings to ints(or floats if nan in column) using frequency encoding\n# We will be able to use these columns as category or as numerical feature\ncat_cols = ['DeviceType', 'DeviceInfo', 'ProductCD', \n            'card1', 'card2', 'card3',  'card4','card5', 'card6','addr1', 'addr2']\n\nfor col in cat_cols:\n    print('Encoding', col)\n    temp_df = pd.concat([train[[col]], test[[col]]])\n    col_encoded = temp_df[col].value_counts().to_dict()   \n    train[col] = train[col].map(col_encoded)\n    test[col]  = test[col].map(col_encoded)\n    print(col_encoded)","92042cac":"#################################################################################\n# Converting Strings to ints(or floats if nan in column) using frequency encoding\n# for id information in indentity table\n# encoding seperately\n\nid_cols = ['id_12', 'id_13', 'id_14', 'id_15', 'id_16', 'id_17', 'id_18', 'id_19', 'id_20', 'id_21', \n            'id_22', 'id_23', 'id_24', 'id_25', 'id_26', 'id_27', 'id_28', 'id_29',\n            'id_30', 'id_31', 'id_32', 'id_33', 'id_34', 'id_35', 'id_36', 'id_37', 'id_38']\n\nfor col in id_cols:\n    print('Encoding', col)\n    print('training set:')\n    col_encoded = train[col].value_counts().to_dict()\n    print(col_encoded)\n    train[col] = train[col].map(col_encoded)\n    \n    print('test set:')\n    col_encoded = test[col].value_counts().to_dict()\n    test[col]  = test[col].map(col_encoded)\n    print(col_encoded)","095c0035":"# M columns\n#################################################################################\n# Converting Strings to ints(or floats if nan in column)\n\nfor col in ['M1','M2','M3','M5','M6','M7','M8','M9']:\n    train[col] = train[col].map({'T':1, 'F':0})\n    test[col]  = test[col].map({'T':1, 'F':0})\n\nfor col in ['M4']:\n    print('Encoding', col)\n    temp_df = pd.concat([train[[col]], test[[col]]])\n    col_encoded = temp_df[col].value_counts().to_dict()   \n    train[col] = train[col].map(col_encoded)\n    test[col]  = test[col].map(col_encoded)\n    print(col_encoded)","b99b9d96":"#final minification\ntrain = reduce_mem_usage(train)\ntest = reduce_mem_usage(test)","96463f8a":"##export\n#train.to_pickle('train_mini.pkl')\n#test.to_pickle('test_mini.pkl')","7aa36766":"#train = pd.read_pickle('train_mini.pkl')\n#test = pd.read_pickle('test_mini.pkl')","17439bcd":"base_columns = list(train) + list(identity_train)","9ec74032":"START_DATE = datetime.datetime.strptime('2017-11-30', '%Y-%m-%d')\nfor df in [train, test]:\n    # Temporary\n    df['DT'] = df['TransactionDT'].apply(lambda x: (START_DATE + datetime.timedelta(seconds = x)))\n    df['DT_M'] = (df['DT'].dt.year-2017)*12 + df['DT'].dt.month\n    df['DT_W'] = (df['DT'].dt.year-2017)*52 + df['DT'].dt.weekofyear\n    df['DT_D'] = (df['DT'].dt.year-2017)*365 + df['DT'].dt.dayofyear\n    \n    df['Transaction_hour'] = df['DT'].dt.hour\n    df['Transaction_day_of_week'] = df['DT'].dt.dayofweek\n    df['DT_day'] = df['DT'].dt.day\n    \n    # D9 column\n    df['D9'] = np.where(df['D9'].isna(),0,1)","2880e016":"######################################################\n#calculate transaction amount by group\ntrain['TransactionAmt_to_mean_card1'] = train['TransactionAmt'] \/ train.groupby(['card1'])['TransactionAmt'].transform('mean')\ntrain['TransactionAmt_to_mean_card4'] = train['TransactionAmt'] \/ train.groupby(['card4'])['TransactionAmt'].transform('mean')\ntrain['TransactionAmt_to_std_card1'] = train['TransactionAmt'] \/ train.groupby(['card1'])['TransactionAmt'].transform('std')\ntrain['TransactionAmt_to_std_card4'] = train['TransactionAmt'] \/ train.groupby(['card4'])['TransactionAmt'].transform('std')\n\ntest['TransactionAmt_to_mean_card1'] = test['TransactionAmt'] \/ test.groupby(['card1'])['TransactionAmt'].transform('mean')\ntest['TransactionAmt_to_mean_card4'] = test['TransactionAmt'] \/ test.groupby(['card4'])['TransactionAmt'].transform('mean')\ntest['TransactionAmt_to_std_card1'] = test['TransactionAmt'] \/ test.groupby(['card1'])['TransactionAmt'].transform('std')\ntest['TransactionAmt_to_std_card4'] = test['TransactionAmt'] \/ test.groupby(['card4'])['TransactionAmt'].transform('std')\n\ntrain['id_02_to_mean_card1'] = train['id_02'] \/ train.groupby(['card1'])['id_02'].transform('mean')\ntrain['id_02_to_mean_card4'] = train['id_02'] \/ train.groupby(['card4'])['id_02'].transform('mean')\ntrain['id_02_to_std_card1'] = train['id_02'] \/ train.groupby(['card1'])['id_02'].transform('std')\ntrain['id_02_to_std_card4'] = train['id_02'] \/ train.groupby(['card4'])['id_02'].transform('std')\n\ntest['id_02_to_mean_card1'] = test['id_02'] \/ test.groupby(['card1'])['id_02'].transform('mean')\ntest['id_02_to_mean_card4'] = test['id_02'] \/ test.groupby(['card4'])['id_02'].transform('mean')\ntest['id_02_to_std_card1'] = test['id_02'] \/ test.groupby(['card1'])['id_02'].transform('std')\ntest['id_02_to_std_card4'] = test['id_02'] \/ test.groupby(['card4'])['id_02'].transform('std')\n\ntrain['D15_to_mean_card1'] = train['D15'] \/ train.groupby(['card1'])['D15'].transform('mean')\ntrain['D15_to_mean_card4'] = train['D15'] \/ train.groupby(['card4'])['D15'].transform('mean')\ntrain['D15_to_std_card1'] = train['D15'] \/ train.groupby(['card1'])['D15'].transform('std')\ntrain['D15_to_std_card4'] = train['D15'] \/ train.groupby(['card4'])['D15'].transform('std')\n\ntest['D15_to_mean_card1'] = test['D15'] \/ test.groupby(['card1'])['D15'].transform('mean')\ntest['D15_to_mean_card4'] = test['D15'] \/ test.groupby(['card4'])['D15'].transform('mean')\ntest['D15_to_std_card1'] = test['D15'] \/ test.groupby(['card1'])['D15'].transform('std')\ntest['D15_to_std_card4'] = test['D15'] \/ test.groupby(['card4'])['D15'].transform('std')\n\ntrain['D15_to_mean_addr1'] = train['D15'] \/ train.groupby(['addr1'])['D15'].transform('mean')\ntrain['D15_to_std_addr1'] = train['D15'] \/ train.groupby(['addr1'])['D15'].transform('std')\n\ntest['D15_to_mean_addr1'] = test['D15'] \/ test.groupby(['addr1'])['D15'].transform('mean')\ntest['D15_to_std_addr1'] = test['D15'] \/ test.groupby(['addr1'])['D15'].transform('std')","30796a16":"\n# New feature - decimal part of the transaction amount.\ntrain['TransactionAmt_decimal'] = ((train['TransactionAmt'] - train['TransactionAmt'].astype(int)) * 1000).astype(int)\ntest['TransactionAmt_decimal'] = ((test['TransactionAmt'] - test['TransactionAmt'].astype(int)) * 1000).astype(int)\n\n# New feature - day of week in which a transaction happened.\n# train['Transaction_day_of_week'] = np.floor((train['TransactionDT'] \/ (3600 * 24) - 1) % 7)\n# test['Transaction_day_of_week'] = np.floor((test['TransactionDT'] \/ (3600 * 24) - 1) % 7)\n\n# # New feature - hour of the day in which a transaction happened.\n# train['Transaction_hour'] = np.floor(train['TransactionDT'] \/ 3600) % 24\n# test['Transaction_hour'] = np.floor(test['TransactionDT'] \/ 3600) % 24\n\n# Some arbitrary features interaction\nfor feature in ['id_02__id_20', 'id_02__D8', 'D11__DeviceInfo', 'DeviceInfo__P_emaildomain', 'P_emaildomain__C2', \n                'card2__dist1', 'card1__card5', 'card2__id_20', 'card5__P_emaildomain', 'addr1__card1']:\n\n    f1, f2 = feature.split('__')\n    train[feature] = train[f1].astype(str) + '_' + train[f2].astype(str)\n    test[feature] = test[f1].astype(str) + '_' + test[f2].astype(str)\n\n    le = LabelEncoder()\n    le.fit(list(train[feature].astype(str).values) + list(test[feature].astype(str).values))\n    train[feature] = le.transform(list(train[feature].astype(str).values))\n    test[feature] = le.transform(list(test[feature].astype(str).values))","28da5a1e":"#new feature (8.21)\n#add more transaction amount groupby\ntrain['TransactionAmt_to_mean_card2'] = train['TransactionAmt'] \/ train.groupby(['card2'])['TransactionAmt'].transform('mean')\ntrain['TransactionAmt_to_std_card2'] = train['TransactionAmt'] \/ train.groupby(['card2'])['TransactionAmt'].transform('std')\n\ntest['TransactionAmt_to_mean_card2'] = test['TransactionAmt'] \/ test.groupby(['card2'])['TransactionAmt'].transform('mean')\ntest['TransactionAmt_to_std_card2'] = test['TransactionAmt'] \/ test.groupby(['card2'])['TransactionAmt'].transform('std')\n","70742f7b":"#new feature (8.21)\n#add transaction amount groupby product type(productCD)\ntrain['TransactionAmt_to_mean_product'] = train['TransactionAmt'] \/ train.groupby(['ProductCD'])['TransactionAmt'].transform('mean')\ntrain['TransactionAmt_to_std_product'] = train['TransactionAmt'] \/ train.groupby(['ProductCD'])['TransactionAmt'].transform('std')\n\ntest['TransactionAmt_to_mean_product'] = test['TransactionAmt'] \/ test.groupby(['ProductCD'])['TransactionAmt'].transform('mean')\ntest['TransactionAmt_to_std_product'] = test['TransactionAmt'] \/ test.groupby(['ProductCD'])['TransactionAmt'].transform('std')","5026af5c":"#new feature(8.21)\n# New feature: max & min transaction amount by groups\ntrain['TransactionAmt_to_max_card1'] = train['TransactionAmt'] \/ train.groupby(['card1'])['TransactionAmt'].transform('max')\ntrain['TransactionAmt_to_max_card4'] = train['TransactionAmt'] \/ train.groupby(['card4'])['TransactionAmt'].transform('max')\ntrain['TransactionAmt_to_max_card2'] = train['TransactionAmt'] \/ train.groupby(['card2'])['TransactionAmt'].transform('max')\ntrain['TransactionAmt_to_min_card1'] = train['TransactionAmt'] \/ train.groupby(['card1'])['TransactionAmt'].transform('min')\ntrain['TransactionAmt_to_min_card4'] = train['TransactionAmt'] \/ train.groupby(['card4'])['TransactionAmt'].transform('min')\ntrain['TransactionAmt_to_min_card2'] = train['TransactionAmt'] \/ train.groupby(['card2'])['TransactionAmt'].transform('min')\n\ntest['TransactionAmt_to_max_card1'] = test['TransactionAmt'] \/ test.groupby(['card1'])['TransactionAmt'].transform('max')\ntest['TransactionAmt_to_max_card4'] = test['TransactionAmt'] \/ test.groupby(['card4'])['TransactionAmt'].transform('max')\ntest['TransactionAmt_to_max_card2'] = test['TransactionAmt'] \/ test.groupby(['card2'])['TransactionAmt'].transform('max')\ntest['TransactionAmt_to_min_card1'] = test['TransactionAmt'] \/ test.groupby(['card1'])['TransactionAmt'].transform('min')\ntest['TransactionAmt_to_min_card4'] = test['TransactionAmt'] \/ test.groupby(['card4'])['TransactionAmt'].transform('min')\ntest['TransactionAmt_to_min_card2'] = test['TransactionAmt'] \/ test.groupby(['card2'])['TransactionAmt'].transform('min')","5a8c6092":"# New feature - log of transaction amount. ()\ntrain['TransactionAmt'] = np.log(train['TransactionAmt'])\ntest['TransactionAmt'] = np.log(test['TransactionAmt'])","3454e784":"#new feature(8.21)\n#interaction between product type and transaction time, location info(address, distance)\nfor feature in ['Transaction_day_of_week__ProductCD', 'Transaction_hour__ProductCD', \n                'addr1__ProductCD', 'addr2__ProductCD', 'dist1__ProductCD', 'dist2__ProductCD']:\n\n    f1, f2 = feature.split('__')\n    train[feature] = train[f1].astype(str) + '_' + train[f2].astype(str)\n    test[feature] = test[f1].astype(str) + '_' + test[f2].astype(str)\n\n    le = LabelEncoder()\n    le.fit(list(train[feature].astype(str).values) + list(test[feature].astype(str).values))\n    train[feature] = le.transform(list(train[feature].astype(str).values))\n    test[feature] = le.transform(list(test[feature].astype(str).values))","641df35e":"#new feature(8.21)\n#https:\/\/www.kaggle.com\/nroman\/eda-for-cis-fraud-detection#New-feature:-number-of-NaN's\n#https:\/\/www.kaggle.com\/c\/ieee-fraud-detection\/discussion\/105130#latest-604661\n#number of NaNs \n#V1 ~ V11\n#V12 ~ V34\n#V12 ~ V34\n#V35 ~ V52\n#V53 ~ V74\n#V75 ~ V94\n#V95 ~ V137\n#V138 ~ V166 (high null ratio)\n#V167 ~ V216 (high null ratio)\n#V217 ~ V278 (high null ratio, 2 different null ratios)\n#V279 ~ V321 (2 different null ratios)\n#V322 ~ V339 (high null ratio)\n#haven't figure out how to use this finding\ntrain['Total_nulls'] = train.isnull().sum(axis=1)\ntest['Total_nulls'] = test.isnull().sum(axis=1)","4367098b":"#new feature(8.21)\n\n\n########################### 'P_emaildomain' - 'R_emaildomain'\np = 'P_emaildomain'\nr = 'R_emaildomain'\nuknown = 'email_not_provided'\n\nfor df in [train, test]:\n    df[p] = df[p].fillna(uknown)\n    df[r] = df[r].fillna(uknown)\n    \n    # Check if P_emaildomain matches R_emaildomain\n    df['email_check'] = np.where((df[p]==df[r])&(df[p]!=uknown),1,0)\n\n    \n# https:\/\/www.kaggle.com\/c\/ieee-fraud-detection\/discussion\/100499\n# bin email address\n# do not use frequency encoding for email before\nemails = {'gmail': 'google', 'att.net': 'att', 'twc.com': 'spectrum', 'scranton.edu': 'other', \n          'optonline.net': 'other', 'hotmail.co.uk': 'microsoft', 'comcast.net': 'other', 'yahoo.com.mx': 'yahoo', \n          'yahoo.fr': 'yahoo', 'yahoo.es': 'yahoo', 'charter.net': 'spectrum', 'live.com': 'microsoft', \n          'aim.com': 'aol', 'hotmail.de': 'microsoft', 'centurylink.net': 'centurylink', 'gmail.com': 'google', \n          'me.com': 'apple', 'earthlink.net': 'other', 'gmx.de': 'other', 'web.de': 'other', 'cfl.rr.com': 'other', \n          'hotmail.com': 'microsoft', 'protonmail.com': 'other', 'hotmail.fr': 'microsoft', 'windstream.net': 'other', \n          'outlook.es': 'microsoft', 'yahoo.co.jp': 'yahoo', 'yahoo.de': 'yahoo', 'servicios-ta.com': 'other', \n          'netzero.net': 'other', 'suddenlink.net': 'other', 'roadrunner.com': 'other', 'sc.rr.com': 'other', \n          'live.fr': 'microsoft', 'verizon.net': 'yahoo', 'msn.com': 'microsoft', 'q.com': 'centurylink', \n          'prodigy.net.mx': 'att', 'frontier.com': 'yahoo', 'anonymous.com': 'other', 'rocketmail.com': 'yahoo', \n          'sbcglobal.net': 'att', 'frontiernet.net': 'yahoo', 'ymail.com': 'yahoo', 'outlook.com': 'microsoft', \n          'mail.com': 'other', 'bellsouth.net': 'other', 'embarqmail.com': 'centurylink', 'cableone.net': 'other', \n          'hotmail.es': 'microsoft', 'mac.com': 'apple', 'yahoo.co.uk': 'yahoo', 'netzero.com': 'other', \n          'yahoo.com': 'yahoo', 'live.com.mx': 'microsoft', 'ptd.net': 'other', 'cox.net': 'other', \n          'aol.com': 'aol', 'juno.com': 'other', 'icloud.com': 'apple'}\nus_emails = ['gmail', 'net', 'edu']\n\n\nfor c in ['P_emaildomain', 'R_emaildomain']:\n    train[c + '_bin'] = train[c].map(emails)\n    test[c + '_bin'] = test[c].map(emails)\n    \n    train[c + '_suffix'] = train[c].map(lambda x: str(x).split('.')[-1])\n    test[c + '_suffix'] = test[c].map(lambda x: str(x).split('.')[-1])\n    \n    train[c + '_suffix'] = train[c + '_suffix'].map(lambda x: x if str(x) not in us_emails else 'us')\n    test[c + '_suffix'] = test[c + '_suffix'].map(lambda x: x if str(x) not in us_emails else 'us')\n\nfor col in ['P_emaildomain', 'R_emaildomain','P_emaildomain_bin', 'R_emaildomain_bin',\n            'P_emaildomain_suffix', 'R_emaildomain_suffix']:\n    print('Encoding', col)\n    temp_df = pd.concat([train[[col]], test[[col]]])\n    col_encoded = temp_df[col].value_counts().to_dict()   \n    train[col] = train[col].map(col_encoded)\n    test[col]  = test[col].map(col_encoded)\n    print(col_encoded)","01f19eb1":"train.head()","e10c19cc":"# #############8.24 \n# ########################### Model Features \n# ## We can use set().difference() but the order matters\n# ## Matters only for deterministic results\n# ## In case of remove() we will not change order\n# ## even if variable will be renamed\n# ## please see this link to see how set is ordered\n# ## https:\/\/stackoverflow.com\/questions\/12165200\/order-of-unordered-python-sets\n# TARGET = 'isFraud'\n# rm_cols = [\n#     'TransactionID','TransactionDT', # These columns are pure noise right now\n#      TARGET,                         # Not target in features))\n#     'uid','uid2','uid3',             # Our new client uID -> very noisy data\n#     'bank_type',                     # Victims bank could differ by time\n#     'DT','DT_M','DT_W','DT_D',       # Temporary Variables\n#     'DT_hour','DT_day_week','DT_day',\n#     'DT_D_total','DT_W_total','DT_M_total',\n#     'id_30','id_31','id_33',\n# ]\n\n\n# ########################### Features elimination \n# from scipy.stats import ks_2samp\n# features_check = []\n# columns_to_check = set(list(train)).difference(rm_cols)\n# for i in columns_to_check:\n#     features_check.append(ks_2samp(test[i], train[i])[1])\n\n# features_check = pd.Series(features_check, index=columns_to_check).sort_values() \n# features_discard = list(features_check[features_check==0].index)\n# print(features_discard)\n\n# # We will reset this list for now (use local test drop),\n# # Good droping will be in other kernels\n# # with better checking\n# features_discard = [] \n\n# # Final features list\n# features_columns = [col for col in list(train_df) if col not in rm_cols + features_discard]","321f7a0d":"for column in train:\n    total = len(train)\n    print('{0} : {1}'.format(column, train[column].isnull().sum()\/total))","9c353e73":"def drop_sparse_column(threshold, df_train, df_test):\n    new_train = df_train.copy()\n    new_test = df_test.copy()\n    total = len(df_train)\n    for column in df_train:\n        percent = df_train[column].isnull().sum()\/total\n        if percent > threshold:\n            new_train = new_train.drop(columns = [column])\n            new_test = new_test.drop(columns = [column])\n    return new_train, new_test","7a84b7f4":"train, test = drop_sparse_column(0.97, train,test)","6cd0c71a":"def drop_single_dominant(threshold, df_train, df_test):\n    new_train = df_train.copy()\n    isfraud = new_train['isFraud']\n    new_train = new_train.drop(columns = ['isFraud'])\n    new_test = df_test.copy()\n    for column in new_train:\n        if train[column].value_counts(dropna = False, normalize = True).values[0] > threshold:\n            new_train = new_train.drop(columns = [column])\n            new_test = new_test.drop(columns = [column])\n    new_train['isFraud'] = isfraud\n    return new_train, new_test","268da6a7":"train, test = drop_single_dominant(0.97, train, test)","3dbeb28d":"def drop_one_value(df_train, df_test):\n    new_train = df_train.copy()\n    new_test = df_test.copy()\n    for column in new_train:\n        if train[column].nunique() <= 1:\n            new_train = new_train.drop(columns = [column])\n            new_test = new_test.drop(columns = [column])\n    return new_train, new_test","b7f34ed1":"train, test = drop_one_value(train, test)","373d49f0":"train.head()","cc9bf439":"useful_features = ['TransactionAmt', 'ProductCD', 'card1', 'card2', 'card3', 'card4', 'card5', 'card6', 'addr1', 'addr2', 'dist1',\n                   'P_emaildomain', 'R_emaildomain', 'C1', 'C2', 'C4', 'C5', 'C6', 'C7', 'C8', 'C9', 'C10', 'C11', 'C12', 'C13',\n                   'C14', 'D1', 'D2', 'D3', 'D4', 'D5', 'D6', 'D8', 'D9', 'D10', 'D11', 'D12', 'D13', 'D14', 'D15', 'M2', 'M3',\n                   'M4', 'M5', 'M6', 'M7', 'M8', 'M9', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10', 'V11', 'V12', 'V13', 'V17',\n                   'V19', 'V20', 'V29', 'V30', 'V33', 'V34', 'V35', 'V36', 'V37', 'V38', 'V40', 'V44', 'V45', 'V46', 'V47', 'V48',\n                   'V49', 'V51', 'V52', 'V53', 'V54', 'V56', 'V58', 'V59', 'V60', 'V61', 'V62', 'V63', 'V64', 'V69', 'V70', 'V71',\n                   'V72', 'V73', 'V74', 'V75', 'V76', 'V78', 'V80', 'V81', 'V82', 'V83', 'V84', 'V85', 'V87', 'V90', 'V91', 'V92',\n                   'V93', 'V94', 'V95', 'V96', 'V97', 'V99', 'V100', 'V126', 'V127', 'V128', 'V130', 'V131', 'V138', 'V139', 'V140',\n                   'V143', 'V145', 'V146', 'V147', 'V149', 'V150', 'V151', 'V152', 'V154', 'V156', 'V158', 'V159', 'V160', 'V161',\n                   'V162', 'V163', 'V164', 'V165', 'V166', 'V167', 'V169', 'V170', 'V171', 'V172', 'V173', 'V175', 'V176', 'V177',\n                   'V178', 'V180', 'V182', 'V184', 'V187', 'V188', 'V189', 'V195', 'V197', 'V200', 'V201', 'V202', 'V203', 'V204',\n                   'V205', 'V206', 'V207', 'V208', 'V209', 'V210', 'V212', 'V213', 'V214', 'V215', 'V216', 'V217', 'V219', 'V220',\n                   'V221', 'V222', 'V223', 'V224', 'V225', 'V226', 'V227', 'V228', 'V229', 'V231', 'V233', 'V234', 'V238', 'V239',\n                   'V242', 'V243', 'V244', 'V245', 'V246', 'V247', 'V249', 'V251', 'V253', 'V256', 'V257', 'V258', 'V259', 'V261',\n                   'V262', 'V263', 'V264', 'V265', 'V266', 'V267', 'V268', 'V270', 'V271', 'V272', 'V273', 'V274', 'V275', 'V276',\n                   'V277', 'V278', 'V279', 'V280', 'V282', 'V283', 'V285', 'V287', 'V288', 'V289', 'V291', 'V292', 'V294', 'V303',\n                   'V304', 'V306', 'V307', 'V308', 'V310', 'V312', 'V313', 'V314', 'V315', 'V317', 'V322', 'V323', 'V324', 'V326',\n                   'V329', 'V331', 'V332', 'V333', 'V335', 'V336', 'V338', 'id_01', 'id_02', 'id_03', 'id_05', 'id_06', 'id_09',\n                   'id_11', 'id_12', 'id_13', 'id_14', 'id_15', 'id_17', 'id_19', 'id_20', 'id_30', 'id_31', 'id_32', 'id_33',\n                   'id_36', 'id_37', 'id_38', 'DeviceType', 'DeviceInfo', 'device_name', 'device_version', 'OS_id_30', 'version_id_30',\n                   'browser_id_31', 'version_id_31', 'screen_width', 'screen_height', 'had_id']","24e6cdd5":"for col in train:\n    if col not in useful_features:\n        print(\"'{}'\".format(col)+',',end='')","6861f0ce":"####### Random Sampling\n#train = train.sample(n=10000, replace=True, random_state=1)\n#test = test.sample(n=1000, replace=True, random_state=2)","a9c08e4a":"### Final feature selection","25b8f750":"train_X = train.drop(['TransactionDT','isFraud','_merge'],axis=1)\ntrain_Y = train['isFraud']\ntest_X = test.drop(['TransactionDT','_merge'],axis=1)","b0e830d9":"train_X = train_X.drop(['DT'],axis=1)\ntest_X = test_X.drop(['DT'],axis=1)","e7fa3cf0":"# train_X = train_X.drop(['DeviceInfo_device', 'DeviceInfo_version', 'id_30_device', 'id_30_version', 'id_31_device'],axis=1)\n# test_X = test_X.drop(['DeviceInfo_device', 'DeviceInfo_version', 'id_30_device', 'id_30_version', 'id_31_device'],axis=1)","a896ed63":"train_X = train_X.drop(['DT_D'],axis=1)\ntest_X = test_X.drop(['DT_D'],axis=1)\n","7a7a476b":"train_X.head()","a35b9758":"#train_X.to_csv('train_FE.csv',index=False)\n#test_X.to_csv('test_FE.csv',index=False)","c15c609b":"#train_Y.to_csv('train_FE_Y_3.csv',index=False,header=False)","2eb48255":"len(train_Y)","8939d0ef":"#use new_train & new_test to train model\n\n# Set Parameters\n# params = {'num_leaves': 2**8,\n#           'min_child_weight': 0.03454472573214212,\n#           'feature_fraction': 0.3797454081646243,\n#           'bagging_fraction': 0.4181193142567742,\n#           'min_data_in_leaf': 106,\n#           'objective': 'binary',\n#           'max_depth': -1,\n#           'learning_rate': 0.01,\n#           \"boosting_type\": \"gbdt\",\n#           \"bagging_seed\": 11,\n#           \"metric\": 'auc',\n#           \"verbosity\": -1,\n#           'reg_alpha': 0.3899927210061127,\n#           'reg_lambda': 0.6485237330340494,\n#           'random_state': 47,\n#           'colsample_bytree': 0.7,\n#           'n_estimators':800,\n#           'max_bin':255\n          \n#          }\n\nparams = {'num_leaves': 2**8,\n          'min_child_weight': 0.03454472573214212,\n          'feature_fraction': 0.3797454081646243,\n          'bagging_fraction': 0.4181193142567742,\n          'min_data_in_leaf': 106,\n          'objective': 'binary',\n          'max_depth': -1,\n          'learning_rate': 0.01,\n          \"boosting_type\": \"gbdt\",\n          \"bagging_seed\": 11,\n          \"metric\": 'auc',\n          \"verbosity\": -1,\n          'reg_alpha': 0.3899927210061127,\n          'reg_lambda': 0.6485237330340494,\n          'random_state': 47,\n         }\n# lgb_params = {\n#                     'objective':'binary',\n#                     'boosting_type':'gbdt',\n#                     'metric':'auc',\n#                     'n_jobs':-1,\n#                     'learning_rate':0.01,\n#                     'num_leaves': 2**8,\n#                     'max_depth':-1,\n#                     'tree_learner':'serial',\n#                     'colsample_bytree': 0.7,\n#                     'subsample_freq':1,\n#                     'subsample':0.7,\n#                     'n_estimators':800,\n#                     'max_bin':255,\n#                     'verbose':-1,\n#                     'seed': SEED,\n#                     'early_stopping_rounds':100, \n#                 }","c76640eb":"%%time\nimport gc\n\n\n\n##### Cross Validation\nNFOLDS = 5\nfolds = KFold(n_splits=NFOLDS,random_state=42)\n\ncolumns = train_X.columns\nsplits = folds.split(train_X, train_Y)\n\ny_pred_test_vectors = np.zeros(test_X.shape[0])\ny_pred_valid_vectors = np.zeros(train_X.shape[0])\nscore = 0\n\n\nfeature_importances = pd.DataFrame()\nfeature_importances['feature'] = columns\n  \nfor fold_n, (train_index, valid_index) in enumerate(splits):\n    X_train, X_valid = train_X[columns].iloc[train_index], train_X[columns].iloc[valid_index]\n    y_train, y_valid = train_Y.iloc[train_index], train_Y.iloc[valid_index]\n    \n    dtrain = lgb.Dataset(X_train, label=y_train)\n    dvalid = lgb.Dataset(X_valid, label=y_valid)\n\n    clf = lgb.train(params, dtrain, 10000, valid_sets = [dtrain, dvalid], verbose_eval=200, early_stopping_rounds=500)\n    \n    feature_importances[f'fold_{fold_n + 1}'] = clf.feature_importance()\n    \n    y_pred_valid = clf.predict(X_valid)\n    y_pred_valid_vectors[valid_index] = y_pred_valid\n    print(f\"Fold {fold_n + 1} | AUC: {roc_auc_score(y_valid, y_pred_valid)}\")\n    \n    score += roc_auc_score(y_valid, y_pred_valid) \/ NFOLDS\n    y_pred_test_vectors += clf.predict(test_X) \/ NFOLDS\n    \n    del X_train, X_valid, y_train, y_valid\n    \n    #gabage collector\n    gc.collect()\n    \nprint(f\"\\nMean AUC = {score}\")\nprint(f\"Out of folds AUC = {roc_auc_score(train_Y, y_pred_valid_vectors)}\")","24bdeff6":"pred = pd.read_csv('..\/input\/ieee-fraud-detection\/sample_submission.csv')\n\npred['isFraud'] = y_pred_test_vectors\npred.to_csv(\"submission_5.csv\", index=False)","6af21e1d":"feature_importances['average'] = feature_importances.mean(axis = 1)\nfeature_importances","ce8efd97":"feature_importances['average'] = feature_importances.mean(axis = 1)\nplt.figure(figsize=(16, 16))\nsns.barplot(data=feature_importances.sort_values(by='average', ascending=False).head(50), x='average', y='feature');\nplt.title('50 TOP feature importance over {} folds average'.format(folds.n_splits));","224b6d99":"# the final submission\n","b2476481":"## Drop single value dominant columns","346bb72a":"# <a id='1'> 1. Data Loading & Memory Reduce","b2e5fbc6":"# <a id='5'> 5. Model training","9b59712d":"Then, merge identity table to the transcation table to enrich features ","17c6bcbd":"Not all transactions can be found corresponding indentity","26840d33":"## Useful features\n### Not run this cell","762bbce7":"Several ideas:\n- use PCA to reduce # of columns (for groups, e.g. card 1-6, id, etc.) https:\/\/www.kaggle.com\/kabure\/almost-complete-feature-engineering-ieee-data#V-Features\n- aggregation https:\/\/www.kaggle.com\/artgor\/eda-and-models#Feature-engineering; https:\/\/www.kaggle.com\/kyakovlev\/ieee-gb-2-make-amount-useful-again?scriptVersionId=18889353\n- focus on important features https:\/\/www.kaggle.com\/nroman\/eda-for-cis-fraud-detection\n- frequency encoding https:\/\/www.kaggle.com\/kyakovlev\/ieee-gb-2-make-amount-useful-again?scriptVersionId=18941048\n- target mean? https:\/\/www.kaggle.com\/kyakovlev\/ieee-gb-2-make-amount-useful-again?scriptVersionId=18941048\n\nOthers:\n- M columns: all binary (except M4), can generate M_sum, M_na\n- C columns: C columns are some counts, based on client identity. Most popular Value is \"1\". can generate C_sum, C_na, or check if C counts is valid or not\n- split id: e.g. device info https:\/\/www.kaggle.com\/davidcairuz\/feature-engineering-lightgbm-corrected\n- transaction time: https:\/\/www.kaggle.com\/davidcairuz\/feature-engineering-lightgbm-corrected\n","6f0581b5":"## Drop one value columns","40689c29":"- <a href='#1'> 1. Intro, Data Loading & Memory Reduce\n- <a href='#2'> 2. EDA\n- <a href='#4'> 3. Feature Engineering\n- <a href='#3'> 4. Preprocessing\n- <a href='#5'> 5. Training\n\n__Transaction Table__ <br>\n* TransactionDT: timedelta from a given reference datetime (not an actual timestamp)<br>\n* TransactionAMT: transaction payment amount in USD<br>\n* ProductCD __(Categorical)__: product code, the product for each transaction<br>\n* card1 - card6 __(Categorical)__: payment card information, such as card type, card category, issue bank, country, etc.<br>\n* addr __(Categorical)__: address<br>\n* dist: distance<br>\n* P_ and (R__) emaildomain __(Categorical)__: purchaser and recipient email domain<br>\n* C1-C14: counting, such as how many addresses are found to be associated with the payment card, etc. The actual meaning is masked.<br>\n* D1-D15: timedelta, such as days between previous transaction, etc.<br>\n* M1-M9 __(Categorical)__: match, such as names on card and address, etc.<br>\n* Vxxx: Vesta engineered rich features, including ranking, counting, and other entity relations.<br>\n\n__Identity Table__<br>\nVariables in this table are identity information \u2013 network connection information (IP, ISP, Proxy, etc) and digital signature (UA\/browser\/os\/version, etc) associated with transactions. <br>\nCategorical Features:\n* DeviceType\n* DeviceInfo\n* id12 - id38","14f94eaa":"## Load data","df6854b5":"## Memory Reduction","0e8312da":"# <a id='3'> 3. Feature Engineering","153bcd13":"# <a id='4'> 4. Preprocessing\n## Drop columns with too many NaN","7cd651d9":"Before we perform merge, we need to do some feature engineering to the identity dataset"}}