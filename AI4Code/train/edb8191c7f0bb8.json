{"cell_type":{"7df31c8a":"code","16e3e5c2":"code","54dd7385":"code","de7a1010":"code","65c69466":"code","4e8f5969":"code","dc8973a8":"code","ef727a83":"code","7d63c8c8":"code","ee848471":"code","5c38182e":"code","dd7cfc19":"code","d00aa459":"code","a7cac6bb":"code","b5c3a724":"code","a8c85c46":"code","0ce92f21":"code","45f15bbe":"code","e74b6444":"code","66a260fd":"code","4b0896a6":"code","b145958c":"code","c874c544":"code","5a2507f5":"code","967b46d4":"code","69b2cce8":"code","3805965d":"code","848e8173":"code","002682cb":"code","3f3a8062":"code","324c5930":"code","d6f198dc":"code","0a29950a":"code","28363b2d":"code","7e38b24a":"code","dd9a7e88":"code","1e2f5e5f":"code","2db1d9df":"code","fdfe5a45":"code","cca2642e":"code","ca947455":"code","f1ffb3cb":"code","0bd51b60":"code","795282a1":"code","67b6a633":"code","e8f98833":"code","dfc738f8":"code","da217619":"code","f56bf809":"code","ee9d5c68":"code","310eb226":"code","30f9491b":"code","c7d82f75":"code","520ae18e":"markdown","3b1a7a43":"markdown","178f99d7":"markdown","a6f22a10":"markdown","cff78785":"markdown","7083f42d":"markdown","7337253e":"markdown","a3ff23f5":"markdown","fa9dd5b8":"markdown","7c5aa58e":"markdown","61e76835":"markdown","3ef3d87f":"markdown","bf1eccec":"markdown","7cea123a":"markdown","c8e5fa07":"markdown","dcdb79d5":"markdown","33174c77":"markdown","a7c67202":"markdown"},"source":{"7df31c8a":"# Basic\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Other\n#import scipy.stats as stats\nfrom scipy.stats import skew\nfrom scipy.special import boxcox1p\nfrom scipy.stats import boxcox_normmax\n\n#Algorithms\nfrom sklearn import ensemble, tree, svm, naive_bayes, neighbors, linear_model, gaussian_process, neural_network\nimport xgboost as xgb\nfrom xgboost.sklearn import XGBClassifier\n\n# Model\nfrom sklearn.metrics import accuracy_score, f1_score, auc, roc_curve, roc_auc_score\nfrom sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV, cross_val_score\n\n#System\nimport os\nimport warnings\n\n#Configure Defaults\nwarnings.filterwarnings('ignore')\n%matplotlib inline\npd.set_option('display.float_format', lambda x: '{:.3f}'.format(x))\n#pd.set_option('display.max_columns', 500)","16e3e5c2":"print(os.listdir(\"..\/input\"))","54dd7385":"train = pd.read_csv('..\/input\/train.csv')\ntrain.shape","de7a1010":"test = pd.read_csv('..\/input\/test.csv')\ntest.shape","65c69466":"desc = open('..\/input\/data_description.txt', 'r')\nprint(desc.read())","4e8f5969":"train.head()","dc8973a8":"train.columns","ef727a83":"# Target variable\nsns.distplot(train['SalePrice'], fit=stats.norm)","7d63c8c8":"# Correct the skew\ntrain.SalePrice = np.log1p(train.SalePrice)","ee848471":"# Check target distribution\nsns.distplot(train['SalePrice'], fit=stats.norm)","5c38182e":"# Save test ID\ntest_ID = test['Id']\n\n# Save train target\ntarget = train.SalePrice.values\n\n# Get split marker\nsplit = len(train)\n\n# Concate data\ndata = pd.concat((train, test)).reset_index(drop=True)\n\n# Drop index\ndata.drop(\"Id\", axis=1, inplace=True)\n\n# Drop target column\ndata.drop(\"SalePrice\", axis=1, inplace=True)\n\n# Check\ndata.shape","dd7cfc19":"plt.scatter(train.GrLivArea, train.SalePrice)","d00aa459":"train = train[train.GrLivArea < 4000]\nplt.scatter(train.GrLivArea, train.SalePrice)","a7cac6bb":"sns.heatmap(data.isnull())","b5c3a724":"na = data.isnull().sum() \/ len(data) * 100\nna.sort_values(ascending=False).head(10)","a8c85c46":"features = data\n\n# Some of the non-numeric predictors are stored as numbers; we convert them into strings \nfeatures['MSSubClass'] = features['MSSubClass'].apply(str)\nfeatures['YrSold'] = features['YrSold'].astype(str)\nfeatures['MoSold'] = features['MoSold'].astype(str)\n\nfeatures['Functional'] = features['Functional'].fillna('Typ')\nfeatures['Electrical'] = features['Electrical'].fillna(\"SBrkr\")\nfeatures['KitchenQual'] = features['KitchenQual'].fillna(\"TA\")\nfeatures['Exterior1st'] = features['Exterior1st'].fillna(features['Exterior1st'].mode()[0])\nfeatures['Exterior2nd'] = features['Exterior2nd'].fillna(features['Exterior2nd'].mode()[0])\nfeatures['SaleType'] = features['SaleType'].fillna(features['SaleType'].mode()[0])\n\nfeatures[\"PoolQC\"] = features[\"PoolQC\"].fillna(\"None\")\n\nfor col in ('GarageYrBlt', 'GarageArea', 'GarageCars'):\n    features[col] = features[col].fillna(0)\nfor col in ['GarageType', 'GarageFinish', 'GarageQual', 'GarageCond']:\n    features[col] = features[col].fillna('None')\nfor col in ('BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2'):\n    features[col] = features[col].fillna('None')\n\nfeatures['MSZoning'] = features.groupby('MSSubClass')['MSZoning'].transform(lambda x: x.fillna(x.mode()[0]))\n\nobjects = []\nfor i in features.columns:\n    if features[i].dtype == object:\n        objects.append(i)\n\nfeatures.update(features[objects].fillna('None'))\n\nfeatures['LotFrontage'] = features.groupby('Neighborhood')['LotFrontage'].transform(lambda x: x.fillna(x.median()))\n\n# Filling in the rest of the NA's\n\nnumeric_dtypes = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\nnumerics = []\nfor i in features.columns:\n    if features[i].dtype in numeric_dtypes:\n        numerics.append(i)\nfeatures.update(features[numerics].fillna(0))\n\nnumeric_dtypes = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\nnumerics2 = []\nfor i in features.columns:\n    if features[i].dtype in numeric_dtypes:\n        numerics2.append(i)\n\nskew_features = features[numerics2].apply(lambda x: skew(x)).sort_values(ascending=False)\n\nhigh_skew = skew_features[skew_features > 0.5]\nskew_index = high_skew.index\n\nfor i in skew_index:\n    features[i] = boxcox1p(features[i], boxcox_normmax(features[i] + 1))\n\nfeatures = features.drop(['Utilities', 'Street', 'PoolQC',], axis=1)\n\nfeatures['YrBltAndRemod']=features['YearBuilt']+features['YearRemodAdd']\nfeatures['TotalSF']=features['TotalBsmtSF'] + features['1stFlrSF'] + features['2ndFlrSF']\n\nfeatures['Total_sqr_footage'] = (features['BsmtFinSF1'] + features['BsmtFinSF2'] +\n                                 features['1stFlrSF'] + features['2ndFlrSF'])\n\nfeatures['Total_Bathrooms'] = (features['FullBath'] + (0.5 * features['HalfBath']) +\n                               features['BsmtFullBath'] + (0.5 * features['BsmtHalfBath']))\n\nfeatures['Total_porch_sf'] = (features['OpenPorchSF'] + features['3SsnPorch'] +\n                              features['EnclosedPorch'] + features['ScreenPorch'] +\n                              features['WoodDeckSF'])\n\n# simplified features\nfeatures['haspool'] = features['PoolArea'].apply(lambda x: 1 if x > 0 else 0)\nfeatures['has2ndfloor'] = features['2ndFlrSF'].apply(lambda x: 1 if x > 0 else 0)\nfeatures['hasgarage'] = features['GarageArea'].apply(lambda x: 1 if x > 0 else 0)\nfeatures['hasbsmt'] = features['TotalBsmtSF'].apply(lambda x: 1 if x > 0 else 0)\nfeatures['hasfireplace'] = features['Fireplaces'].apply(lambda x: 1 if x > 0 else 0)\n\ndata = features","0ce92f21":"'''# Drop mostly empty features\ndata = data.drop(['PoolQC', 'MiscFeature', 'Alley', 'Fence'], axis = 1)'''","45f15bbe":"'''# Set None = NAN for the feature\ndata['FireplaceQu'].replace('None', np.nan, inplace=True)\n\n# Create boolean flag for roughly 50% NAs\ndata[\"FireplaceQu_Flag\"] = data[\"FireplaceQu\"].notnull().astype('int')\n\n# Drop original\ndata = data.drop(['FireplaceQu'], axis = 1)'''","e74b6444":"'''# Fill median into ~1\/6 NAs\ndata.LotFrontage = data.LotFrontage.fillna(data.LotFrontage.median())'''","66a260fd":"'''# Select columns due to theirs data type\nfloat_col = data.select_dtypes('float')\nint_col = data.select_dtypes('int')\nobject_col = data.select_dtypes('object')'''","4b0896a6":"'''# Remove and impute numerical features\nfor f in float_col: data[f] = data[f].fillna(data[f].median())\n   #if data[f].isnull().sum() \/ data.shape[0] > 0.1667: del data[f] # Remove 1\/6+ of NANs\n   #else: data[f] = data[f].fillna(data[f].mean()) # Impute others with a mean value'''","b145958c":"'''# Remove and impute numerical features\nfor i in int_col: data[i] = data[i].fillna(data[i].mode()[0])\n   #if data[i].isnull().sum() \/ data.shape[0] > 0.1667: del data[f] # Remove 1\/6+ of NANs\n   #else: data[i] = data[i].fillna(data[i].mode()[0]) # Impute others with a mean value'''","c874c544":"'''for o in object_col: data[o] = data[o].fillna('Unknown')'''","5a2507f5":"'''# These are actually categorical\nnum_cat = ['MSSubClass', 'OverallCond', 'YrSold', 'MoSold']\ndata[num_cat] = data[num_cat].astype(str)'''","967b46d4":"'''# Create new features\ndata['TotalSF'] = data['TotalBsmtSF'] + data['1stFlrSF'] + data['2ndFlrSF']\n\ndata['Total_sqr_footage'] = (data['BsmtFinSF1'] + data['BsmtFinSF2'] +\n                                 data['1stFlrSF'] + data['2ndFlrSF'])\n\ndata['Total_Bathrooms'] = (data['FullBath'] + (0.5*data['HalfBath']) + \n                               data['BsmtFullBath'] + (0.5*data['BsmtHalfBath']))\n\ndata['Total_porch_sf'] = (data['OpenPorchSF'] + data['3SsnPorch'] +\n                              data['EnclosedPorch'] + data['ScreenPorch'] +\n                             data['WoodDeckSF'])'''","69b2cce8":"'''# Create boolean flags\ndata['Pool_Flag'] = data['PoolArea'].apply(lambda x: 1 if x > 0 else 0)\ndata['2ndfloor_Flag'] = data['2ndFlrSF'].apply(lambda x: 1 if x > 0 else 0)\ndata['Garage_Flag'] = data['GarageArea'].apply(lambda x: 1 if x > 0 else 0)\ndata['Bsmt_Flag'] = data['TotalBsmtSF'].apply(lambda x: 1 if x > 0 else 0)\ndata['Fireplace_Flag'] = data['Fireplaces'].apply(lambda x: 1 if x > 0 else 0)'''","3805965d":"'''num_f = data.dtypes[data.dtypes != \"object\"].index\n\n# Check the skew of all numerical features\nskew_f = data[num_f].apply(lambda x: skew(x.dropna())).sort_values(ascending=False)\nskew = pd.DataFrame({'Skew' :skew_f})\nskew.head(10)'''","848e8173":"'''from scipy.special import boxcox1p\n\nskew = skew[abs(skew) > 0.5]\nprint(\"There are {} skewed numerical features to Box Cox transform\".format(skew.shape[0]))\n\nskew_f = skew.index\nlam = 0.15\nfor f in skew_f:\n    #all_data[feat] += 1\n    data[f] = boxcox1p(data[f], lam)'''","002682cb":"data = pd.get_dummies(data, prefix_sep='_', drop_first=True) # Drop originall feature to avoid multi-collinearity","3f3a8062":"data.shape","324c5930":"# Remove high-variance features\n'''from sklearn.feature_selection import VarianceThreshold\nsel = VarianceThreshold(threshold=(.8 * (1 - .8)))\nsel.fit_transform(data)''' # Array is the output","d6f198dc":"# TODO: feature selection","0a29950a":"data.head()","28363b2d":"# Split data\ntrain = data[:split]\ntest = data[split:]\n\n# Get train variables for a model\nx = train\ny = target\n\n# Train data split\nX_train, X_test, y_train, y_test = train_test_split(x,y, test_size=0.22, random_state=101)","7e38b24a":"from sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC\nfrom sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split\nfrom sklearn.metrics import mean_squared_error\nimport xgboost as xgb\nimport lightgbm as lgb\n\nfrom mlxtend.regressor import StackingCVRegressor","dd9a7e88":"#Validation function\nn_folds = 5\n\ndef rmsle_cv(model, X, y):\n    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(X)\n    rmse= np.sqrt(-cross_val_score(model, X, y, scoring=\"neg_mean_squared_error\", cv = kf))\n    return(rmse)\n\ndef rmsle(y, y_pred):\n    return np.sqrt(mean_squared_error(y, y_pred))","1e2f5e5f":"KRR = KernelRidge(alpha=0.6, kernel='polynomial', degree=2, coef0=2.5)","2db1d9df":"lasso = make_pipeline(RobustScaler(), Lasso(alpha =0.0005, random_state=1))","fdfe5a45":"ENet = make_pipeline(RobustScaler(), ElasticNet(alpha=0.0005, l1_ratio=.9, random_state=3))","cca2642e":"#svr = make_pipeline(RobustScaler(), SVR(C= 20, epsilon= 0.008, gamma=0.0003,))","ca947455":"XGB = xgb.XGBRegressor(learning_rate=0.01,n_estimators=3460,\n                                     max_depth=3, min_child_weight=0,\n                                     gamma=0, subsample=0.7,\n                                     colsample_bytree=0.7,\n                                     objective='reg:linear', nthread=-1,\n                                     scale_pos_weight=1, seed=27,\n                                     reg_alpha=0.00006)","f1ffb3cb":"LGBM = lgb.LGBMRegressor(objective='regression', \n                                       num_leaves=4,\n                                       learning_rate=0.01, \n                                       n_estimators=5000,\n                                       max_bin=200, \n                                       bagging_fraction=0.75,\n                                       bagging_freq=5, \n                                       bagging_seed=7,\n                                       feature_fraction=0.2,\n                                       feature_fraction_seed=7,\n                                       verbose=-1,\n                                       )","0bd51b60":"stack = StackingCVRegressor(regressors=(KRR, lasso, ENet, XGB, LGBM),\n                                meta_regressor=XGB,\n                                use_features_in_secondary=True)","795282a1":"models = [KRR, lasso, ENet, XGB, LGBM]\n\nfor m in models:\n    score = rmsle_cv(m, X_train, y_train)\n    print(\"Model score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","67b6a633":"models.append(stack)\n\nfor m in models:\n    m.fit(X_train, y_train)","e8f98833":"'''print('START Fit')\n\nprint('stack_gen')\nstack_gen_model = stack_gen.fit(np.array(X), np.array(y))'''","dfc738f8":"def assembly(X):\n    result = 0\n    for m in models:\n        result += m.predict(X) \n    return result \/ len(models) # Avg predict value of all models","da217619":"def assembly(X):\n    return ((0.1 * KRR.predict(X)) + \\\n            (0.1 * lasso.predict(X)) + \\\n            (0.1 * ENet.predict(X)) + \\\n            (0.1 * XGB.predict(X)) + \\\n            (0.1 * LGBM.predict(X)) + \\\n            (0.5 * stack.predict(np.array(X))))","f56bf809":"'''print('RMSLE score on train data:')\nprint(rmsle(y, blend_models_predict(X)))'''","ee9d5c68":"print('RMSLE score on train data:')\nprint(rmsle(y_train, assembly(X_train)))","310eb226":"pred = np.floor(np.expm1(assembly(test)))","30f9491b":"sub = pd.DataFrame()\nsub['Id'] = test_ID\nsub['SalePrice'] = pred\nsub.to_csv('submission.csv',index=False)","c7d82f75":"sub.head()","520ae18e":"### Skewed features","3b1a7a43":"### Split data","178f99d7":"### Remove outliers","a6f22a10":"We'll leave other outliers in the traning dataset as they might be present in the test data as well. Instead, we'll focus on making the model more robust.","cff78785":"### Feature engineering","7083f42d":"The target variable is somewhat skewed to the right.","7337253e":"## Load Data","a3ff23f5":"### Missing data","fa9dd5b8":"### Target distribution","7c5aa58e":"### Model","61e76835":"### Box Cox transformation","3ef3d87f":"## Data Pre-processing","bf1eccec":"### Feature Encoding","7cea123a":"### Concat datasets","c8e5fa07":"### Train","dcdb79d5":"### Define algorithms","33174c77":"Now the variable is normally distributed as the models likes.","a7c67202":"### Score Algorithms"}}