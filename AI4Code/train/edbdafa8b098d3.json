{"cell_type":{"c62d2e50":"code","6d548f8d":"code","b6756ce6":"code","43fe00b8":"code","28b4d0c6":"code","37bd15e4":"code","c56ad423":"code","bf031422":"code","7dca6a7f":"code","01db3861":"code","6a94c5a2":"code","8d37c094":"code","f73b14e5":"code","5fcdcc43":"code","e07293c5":"code","56cb6fcc":"code","eb5f742d":"code","ad532a32":"code","98b03080":"code","180284ac":"markdown","7539b474":"markdown","40ac9109":"markdown"},"source":{"c62d2e50":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nfrom collections import Counter\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","6d548f8d":"data = pd.read_csv('\/kaggle\/input\/heart-disease-uci\/heart.csv')","b6756ce6":"data.head()","43fe00b8":"data.info()","28b4d0c6":"df = data.drop(['age','sex'],axis = 1)","37bd15e4":"y = df.target.values\nx_data = df.drop(['target'],axis = 1)","c56ad423":"x = (x_data - np.min(x_data)) \/ (np.max(x_data) - np.min(x_data)) # normalization\nx.head()","bf031422":"from sklearn.model_selection import train_test_split","7dca6a7f":"x_train,x_test,y_train,y_test = train_test_split(x,y,test_size = 0.2,random_state = 42)\nx_train = x_train.T\nx_test = x_test.T\ny_train = y_train.T\ny_test = y_test.T\nprint('x_train : ',x_train.shape)\nprint('x_test  : ',y_train.shape)\nprint('y_train : ',x_test.shape)\nprint('y_test  : ',y_test.shape)","01db3861":"from sklearn.linear_model import LogisticRegression","6a94c5a2":"lr = LogisticRegression()\nlr.fit(x_train.T,y_train.T)\nprint(\"Accuary: % {}\".format(lr.score(x_test.T,y_test.T)))","8d37c094":"# 1 -  initialize_weights_and_bias\n# 2 -  sigmoid\n# 3 -  forward_and_backward\n# 4 -  update\n# 5 -  Predict\n# 6 -  Logistic Regression","f73b14e5":"# 1\ndef initialize(demintion):\n    w = np.full((demintion,1),0.01)\n    b = 0.0\n    return w,b","5fcdcc43":"# 2\ndef sigmoid(z):\n    y_head = 1\/(1+np.exp(-z))\n    return y_head","e07293c5":"# 3\ndef forward_and_backward(w,b,x_train,y_train):\n    # forward\n    z = np.dot(w.T,x_train)+b\n    y_head = sigmoid(z)\n    loss = -y_train * np.log(y_head) - (1-y_train) * np.log(1-y_head)\n    cost = np.sum((loss)) \/ x_train.shape[1]\n    # backward\n    derivative_weight = (np.dot(x_train,(y_head - y_train).T)) \/ x_train.shape[1]\n    derivative_bias = np.sum(y_head - y_train) \/ x_train.shape[1]\n    gradient = {'derivative_weight':derivative_weight,'derivative_bias':derivative_bias}\n    return cost,gradient","56cb6fcc":"# 4\ndef update(w,b,x_train,y_train,learning_rate,number_of_iteration):\n    cost_list = []\n    cost_list2 = []\n    index = []\n    for i in range(number_of_iteration):\n        cost,gradient = forward_and_backward(w,b,x_train,y_train)\n        cost_list.append(cost)\n        w = w - learning_rate * gradient['derivative_weight']\n        b = b - learning_rate * gradient['derivative_bias']\n        if i % 10 == 0:\n            cost_list2.append(cost)\n            index.append(i)\n            if i == np.max(number_of_iteration):\n                print('After iteration cost {} {}'.format(i,cost))\n    parametres = {'weight':w,'bias':b}\n    plt.subplots(figsize = (9,6))\n    plt.plot(index,cost_list2)\n    plt.grid()\n    plt.xlabel('Number of Iteration',fontsize = 15)\n    plt.ylabel('Cost',fontsize = 15)\n    plt.plot()\n    return cost_list,parametres","eb5f742d":"# 5 \ndef predict(w,b,x_test):\n    z = sigmoid(np.dot(w.T,x_test)+b)\n    y_predict = np.zeros((1,x_test.shape[1]))\n    for i in range(z.shape[1]):\n        if z[0,i] <= 0.5:\n            y_predict[0,i] = 0\n        else:\n            y_predict[0,i] = 1\n    return y_predict","ad532a32":"# 6\ndef logistic(x_train,y_train,x_test,y_test,learning_rate,number_of_iteration):\n    demintion = x_train.shape[0]\n    w,b = initialize(demintion)\n    cost_list,parametres= update(w,b,x_train,y_train,learning_rate,number_of_iteration)\n    y_predict_test = predict(parametres['weight'],parametres['bias'],x_test)\n    print(' Test Accuary: % {}'.format(100 - np.mean(np.abs(y_predict_test - y_test)) * 100))","98b03080":"logistic(x_train,y_train,x_test,y_test,learning_rate = 1,number_of_iteration = 300)","180284ac":"Sort Way For Logistic Regression","7539b474":"Long War For Logistic Regression","40ac9109":"Sample with Target"}}