{"cell_type":{"b00583b3":"code","d2cae04b":"code","79630a57":"code","0668bd91":"code","87908264":"code","c5a308cd":"code","95086697":"code","0dbcab49":"code","facb85cd":"code","82e56b0b":"code","44c70b5b":"code","2d34186c":"code","3b3ab68d":"code","23774060":"code","6da92907":"code","ca4807fd":"code","32cd69fa":"code","df2a3b52":"code","b1b83a03":"code","c17d2ce6":"code","b3d549de":"code","e0f202aa":"code","ae1507a3":"code","76f1cf1a":"code","5072f45c":"code","f980a0f2":"code","53192177":"code","4ebd7042":"code","216cfe9b":"code","c79dcb4a":"code","0e0c4dae":"code","86b6d51b":"code","aa364eb1":"code","3f4f3045":"code","f64e511a":"markdown","28408594":"markdown","33b99af7":"markdown","0868643b":"markdown","018201e7":"markdown","916f338b":"markdown","0edcb9e4":"markdown","8e3a5c4f":"markdown","884eac69":"markdown","0ec79cf0":"markdown","5b63f55d":"markdown","2c77bece":"markdown","645bb4aa":"markdown","33546064":"markdown","6aeb59b0":"markdown","40fff233":"markdown","2582c1a5":"markdown","f997263e":"markdown","68a16a1f":"markdown","86b78afa":"markdown","a8821a06":"markdown"},"source":{"b00583b3":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","d2cae04b":"# Importing a dataset into a dataframe\ndf = pd.read_csv(os.path.join(dirname, filename))","79630a57":"df.nunique().nsmallest(10)","0668bd91":"# EmployeeCount, Over18 and StandardHours variables have zero variance\n# We can drop these variables\n\ndf.drop(['StandardHours'], axis=1, inplace=True)\ndf.drop(['EmployeeCount'], axis=1, inplace=True)\ndf.drop(['Over18'], axis=1, inplace=True)","87908264":"df.EmployeeNumber.values","c5a308cd":"df.isnull().sum()\n\n# No Null values present in the dataset","95086697":"df[df.duplicated()]\n\n# No Duplicate values present in the dataset","0dbcab49":"corrmat = df.corr()\ncorrmat","facb85cd":"# The dataset contains large number of columns to be accomodated in a single HeatMap, hence dividing it in 3 HeatMaps\n# Importing the required packages for Visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nh1 = corrmat.loc['Age':'NumCompaniesWorked', 'Age':'NumCompaniesWorked']\nh2 = corrmat.loc['PercentSalaryHike':, 'Age':'NumCompaniesWorked']\nh3 = corrmat.loc['PercentSalaryHike':, 'PercentSalaryHike':]\n\nsns.set(rc = {'figure.figsize':(15,8)})\nsns.heatmap(h1, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 11}, cmap=\"YlGnBu\", \n            linewidths=0.5, linecolor='blue')\n\n# Insight - MonthlyIncome and JobLevel are highly correlated","82e56b0b":"sns.heatmap(h2, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 11}, cmap=\"YlGnBu\", \n            linewidths=0.5, linecolor='blue')\n\n# Insights:\n# TotalWorkingYears and JobLevel are highly correlated\n# TotalWorkingYears and MonthlyIncome are highly correlated","44c70b5b":"sns.heatmap(h3, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 11}, cmap=\"YlGnBu\", \n            linewidths=0.5, linecolor='blue')\n\n# Insights:\n# PercentSalaryHike and PerformanceRating are highly correlated\n# YearsInCurrentRole and YearsAtCompany are highly correlated\n# YearsWithCurrManager and YearsAtCompany are highly correlated","2d34186c":"# We need to drop the correlated variables as it shall affect Logistic Regressions and SVM outputs\n\ndf_final = df.drop(['JobLevel','TotalWorkingYears','YearsInCurrentRole', 'YearsWithCurrManager' , 'PercentSalaryHike'], axis=1)\ndf_final","3b3ab68d":"df_final.columns","23774060":"from sklearn import preprocessing\n\ndef preprocessor(df):\n    res_df = df.copy()\n    le = preprocessing.LabelEncoder()\n    \n    res_df['BusinessTravel'] = le.fit_transform(res_df['BusinessTravel'])\n    res_df['Department'] = le.fit_transform(res_df['Department'])\n    res_df['Education'] = le.fit_transform(res_df['Education'])\n    res_df['EducationField'] = le.fit_transform(res_df['EducationField'])\n    res_df['JobRole'] = le.fit_transform(res_df['JobRole'])\n    res_df['Gender'] = le.fit_transform(res_df['Gender'])\n    res_df['MaritalStatus'] = le.fit_transform(res_df['MaritalStatus'])\n    res_df['OverTime'] = le.fit_transform(res_df['OverTime'])\n    res_df['Attrition'] = le.fit_transform(res_df['Attrition'])\n    return res_df","6da92907":"encoded_df = preprocessor(df_final)","ca4807fd":"\nX = encoded_df.drop(['Attrition'],axis =1)\ny = encoded_df['Attrition']\ny","32cd69fa":"from sklearn.preprocessing import StandardScaler\n\nsc = StandardScaler()\nX = sc.fit_transform(X)","df2a3b52":"mean = np.mean(X, axis=0)\nprint('Mean: (%d, %d)' % (mean[0], mean[1]))\nstandard_deviation = np.std(X, axis=0)\nprint('Standard deviation: (%d, %d)' % (standard_deviation[0], standard_deviation[1]))","b1b83a03":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)","c17d2ce6":"from sklearn.linear_model import LogisticRegression\n\n# instantiate the model\nlogreg = LogisticRegression()\n# fit the model with data\nlogreg.fit(X_train,y_train)","b3d549de":"from sklearn import metrics\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import roc_curve, auc\n\ny_pred=logreg.predict(X_test)\ncnf_matrix = metrics.confusion_matrix(y_test, y_pred)\nclass_names=[0,1] # name  of classes\nfig, ax = plt.subplots()\ntick_marks = np.arange(len(class_names))\nplt.xticks(tick_marks, class_names)\nplt.yticks(tick_marks, class_names)\n# create heatmap\nsns.heatmap(pd.DataFrame(cnf_matrix), annot=True, cmap=\"YlGnBu\" ,fmt='g')\nax.xaxis.set_label_position(\"top\")\nplt.tight_layout()\nplt.title('Confusion matrix', y=1.1)\nplt.ylabel('Actual label')\nplt.xlabel('Predicted label')","e0f202aa":"print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\nprint(\"Precision:\",metrics.precision_score(y_test, y_pred))\nprint(\"Recall:\",metrics.recall_score(y_test, y_pred))","ae1507a3":"y_pred_proba = logreg.predict_proba(X_test)[::,1]\nfpr, tpr, _ = metrics.roc_curve(y_test,  y_pred_proba)\nauc = metrics.roc_auc_score(y_test, y_pred_proba)\nplt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc))\nplt.legend(loc=4)\nplt.show()","76f1cf1a":"from sklearn.svm import SVC\nfrom sklearn import metrics\n\nsvc=SVC() # Default hyperparameters\nsvc.fit(X_train,y_train)\ny_pred=svc.predict(X_test)\nprint('Accuracy Score:')\nprint(metrics.accuracy_score(y_test,y_pred))","5072f45c":"svc=SVC(kernel='linear')\nsvc.fit(X_train,y_train)\ny_pred=svc.predict(X_test)\nprint('Accuracy Score:')\nprint(metrics.accuracy_score(y_test,y_pred))","f980a0f2":"svc=SVC(kernel='poly')\nsvc.fit(X_train,y_train)\ny_pred=svc.predict(X_test)\nprint('Accuracy Score:')\nprint(metrics.accuracy_score(y_test,y_pred))","53192177":"svc=SVC(kernel='rbf')\nsvc.fit(X_train,y_train)\ny_pred=svc.predict(X_test)\nprint('Accuracy Score:')\nprint(metrics.accuracy_score(y_test,y_pred))","4ebd7042":"from sklearn.svm import SVC\nsvm_model= SVC()\n","216cfe9b":"tuned_parameters = {\n 'C': (np.arange(2,3,0.1)) , 'kernel': ['linear', 'poly', 'rbf'],\n 'C': (np.arange(2,3,0.1)) , 'gamma': [0.01,0.02,0.03,0.04,0.05], 'kernel': ['linear', 'poly', 'rbf'],\n 'degree': [2,3,4] ,'gamma':[0.01,0.1,1], 'C':(np.arange(2,3,0.1)) , 'kernel':['poly', 'linear','rbf']\n    }","c79dcb4a":"from sklearn.model_selection import GridSearchCV\n\nmodel_svm = GridSearchCV(svm_model, tuned_parameters,cv=10,scoring='accuracy')","0e0c4dae":"model_svm.fit(X_train, y_train)\nprint(model_svm.best_score_)","86b6d51b":"print(model_svm.best_params_)","aa364eb1":"y_pred=model_svm.predict(X_test)\ncnf_matrix = metrics.confusion_matrix(y_test, y_pred)\nclass_names=[0,1] # name  of classes\nfig, ax = plt.subplots()\ntick_marks = np.arange(len(class_names))\nplt.xticks(tick_marks, class_names)\nplt.yticks(tick_marks, class_names)\n# create heatmap\nsns.heatmap(pd.DataFrame(cnf_matrix), annot=True, cmap=\"YlGnBu\" ,fmt='g')\nax.xaxis.set_label_position(\"top\")\nplt.tight_layout()\nplt.title('Confusion matrix', y=1.1)\nplt.ylabel('Actual label')\nplt.xlabel('Predicted label')","3f4f3045":"print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\nprint(\"Precision:\",metrics.precision_score(y_test, y_pred))\nprint(\"Recall:\",metrics.recall_score(y_test, y_pred))","f64e511a":"#### Null value Check","28408594":"# Logistic Regression Model Implementation","33b99af7":"# Support Vector Machines Implementation","0868643b":"### Area Under the Curve","018201e7":"#### Duplicate Check","916f338b":"# Model Performance Analysis ","0edcb9e4":"#### Feature Selection","8e3a5c4f":"### Radial Kernel","884eac69":"### Implementation with Detafult HyperParameters","0ec79cf0":"### Polynomial Kernel","5b63f55d":"#### Label Encoding","2c77bece":"### Linear Kernel","645bb4aa":"#### Standardiation of Features","33546064":"#### Train - Test Split","6aeb59b0":"### Accuracy, Precision and Recall","40fff233":"# Data Pre-processing","2582c1a5":"#### Zero Variance Check","f997263e":"# Data Cleaning","68a16a1f":"#### Correlation Check","86b78afa":"### Optimizing the HyperParameters using GridSearchCV","a8821a06":"### Confusion Matrix"}}