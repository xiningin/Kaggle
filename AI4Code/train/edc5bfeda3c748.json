{"cell_type":{"3b9cfc8d":"code","1ec04dd6":"code","31db8496":"code","e6f26f7a":"code","01784099":"code","7f16c207":"code","86866b9c":"code","29caa139":"code","280e832e":"code","967887b1":"code","6e8b91c9":"code","b33d76f8":"code","8f70a9e3":"code","407cfa8b":"markdown","ca7cf98e":"markdown","c4e6093d":"markdown","b639c19e":"markdown","fb20e8ff":"markdown","880c7575":"markdown","0a3f6e40":"markdown","6b096b48":"markdown","a6176a42":"markdown","a0b1447c":"markdown","1bab962b":"markdown","a6b02f6e":"markdown","c745fb54":"markdown","711f35ba":"markdown","ebca591e":"markdown","a9872057":"markdown","9e8ff51f":"markdown"},"source":{"3b9cfc8d":"import numpy as np\nimport pandas as pd\n\nfrom sklearn import metrics\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\nfrom sklearn.linear_model import LinearRegression,Ridge \nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.decomposition import TruncatedSVD\n\nimport nltk\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\nfrom nltk.stem import WordNetLemmatizer\nSTOPWORDS = set(stopwords.words('english'))\nLEMMATIZER = WordNetLemmatizer()\nimport string\nimport re\n\n##parallel processing\nimport dask\n\nimport spacy\nfrom tqdm import tqdm","1ec04dd6":"datapath = \"\/kaggle\/input\/commonlitreadabilityprize\/\"\nsub_df = pd.read_csv(f\"{datapath}\/sample_submission.csv\")\ntrain_df = pd.read_csv(f\"{datapath}\/train.csv\")\ntest_df = pd.read_csv(f\"{datapath}\/test.csv\")\ntrain_df.shape, test_df.shape, sub_df.shape","31db8496":"# %%time\n# def preprocess_stemming(text):\n#     tokens = word_tokenize(text)\n#     stems = []\n#     for w in tokens:\n#         stems.append(PorterStemmer().stem(w))\n#     return stems\n\n# def process_lemmatization(text):\n#     tokens = word_tokenize(text)\n#     lemmas = []\n#     for w in tokens:\n#         word1 = LEMMATIZER.lemmatize(w, pos = \"n\")\n#         word2 = LEMMATIZER.lemmatize(word1, pos = \"v\")\n#         word3 = LEMMATIZER.lemmatize(word2, pos = (\"a\"))\n#         lemmas.append(word3)\n#     return \" \".join(lemmas)\n\n# ### to remove stopwords\n# train_df['excerpt'] = train_df['excerpt'].apply(lambda text: \" \".join([val for val in word_tokenize(text) if val not in STOPWORDS]))\n\n# # To perform lemmatization\n# ##using dask to speedup\n# #tasks = train_df['excerpt'].apply(lambda text: (dask.delayed(process_lemmatization)(text)))\n# #train_df['excerpt'] = dask.compute(*tasks)\n# train_df['excerpt'] = train_df['excerpt'].apply(lambda text: process_lemmatization(text))\n\n# #string punctuation removal\n# train_df['excerpt'] = train_df['excerpt'].apply(lambda text: text.translate(str.maketrans('', '', string.punctuation)))\n# ## Removes links\n# train_df['excerpt'] = train_df['excerpt'].apply(lambda text: re.sub('https?:\/\/\\S+|www\\.\\S+', '', text))\n# ## Removes numbers\n# train_df['excerpt'] = train_df['excerpt'].apply(lambda text: re.sub(r'[^\\D\\s]','',text))","e6f26f7a":"train_df.head(3)","01784099":"vectorizer = TfidfVectorizer()\nX_train = vectorizer.fit_transform(train_df['excerpt'])\nX_test = vectorizer.transform(test_df['excerpt'])\ny = train_df['target']","7f16c207":"LR_tfidf = LinearRegression().fit(X_train, y)\nLR_tfidf.score(X_train, y)\ny_train_lr_pred = LR_tfidf.predict(X_train)\ntest_lr_pred = LR_tfidf.predict(X_test)\nsub_df['target'] = test_lr_pred\n\nprint('Mean Absolute Error:', metrics.mean_absolute_error(y, y_train_lr_pred))\nprint('Mean Squared Error:', metrics.mean_squared_error(y, y_train_lr_pred))\nprint('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y,y_train_lr_pred)))\n\nprint(sub_df.head())\nsub_df.to_csv('LR_submission.csv', index=False)\n\n## 1.01 - with preprocessing \n## 0.72 - without preprocessing","86866b9c":"%%time\nrf_tfidf = RandomForestRegressor().fit(X_train, y)\ny_train_rf_pred = rf_tfidf.predict(X_train)\ntest_rf_pred = rf_tfidf.predict(X_test)\nsub_df['target'] = test_rf_pred\n\nprint('Mean Absolute Error:', metrics.mean_absolute_error(y, y_train_rf_pred))\nprint('Mean Squared Error:', metrics.mean_squared_error(y, y_train_rf_pred))\nprint('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y,y_train_rf_pred)))\n\nprint(sub_df.head())\nsub_df.to_csv('submission.csv', index=False) \n## 0.92 - with preprocessing \n## 0.81 - without preprocessing","29caa139":"%%time\nsvdT = TruncatedSVD(n_components=400)\nsvd_X_train = svdT.fit_transform(X_train)\nsvd_X_test = svdT.transform(X_test)","280e832e":"%%time\nsvd_rf_tfidf = RandomForestRegressor().fit(svd_X_train, y)\ny_train_svdrf_pred = svd_rf_tfidf.predict(svd_X_train)\ntest_svdrf_pred = svd_rf_tfidf.predict(svd_X_test)\nsub_df['target'] = test_svdrf_pred\n\nprint('Mean Absolute Error:', metrics.mean_absolute_error(y, y_train_svdrf_pred))\nprint('Mean Squared Error:', metrics.mean_squared_error(y, y_train_svdrf_pred))\nprint('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y,y_train_svdrf_pred)))\n\nprint(sub_df.head())\nsub_df.to_csv('submission.csv', index=False) \n## 0.773 - without preprocessing","967887b1":"%%time\nregressor = Ridge(fit_intercept=True, normalize=False)\nscores = cross_val_score(regressor, svd_X_train, y, cv=5, \n                         scoring='neg_root_mean_squared_error')\nprint(f'Average Root mean squared error: {np.abs(np.mean(scores))}')\n\nregressor = regressor.fit(svd_X_train, y)\ntest_df['target'] = regressor.predict(svd_X_test)\ntest_df[['id','target']].to_csv('submission.csv', index=False)\n## 0.722 - without preprocessing","6e8b91c9":"%%time\nRANDOM_STATE = 147\nnlp = spacy.load('en_core_web_lg')\n\nwith nlp.disable_pipes():\n    X_train = np.vstack([nlp(text).vector for text in tqdm(train_df['excerpt'])])\n    y = train_df['target']\n    print(f'Shape of Train vectors: {X_train.shape}')\n\n    X_test = np.vstack([nlp(text).vector for text in tqdm(test_df['excerpt'])])\n    print(f'Shape of Test vectors: {X_test.shape}')\n    \nregressor = Ridge(fit_intercept=True, normalize=False)\nscores = cross_val_score(regressor, X_train, y, cv=5, \n                         scoring='neg_root_mean_squared_error')\nprint(f'Average Root mean squared error: {np.abs(np.mean(scores))}')\n\nregressor = regressor.fit(X_train, y)\ntest_df['target'] = regressor.predict(X_test)\ntest_df[['id','target']].to_csv('submission.csv', index=False)","b33d76f8":"import tensorflow as tf\nimport tensorflow_hub as hub\n\n#embed = hub.load(\"https:\/\/tfhub.dev\/google\/universal-sentence-encoder\/4\")\nembed = hub.load(\"..\/input\/universalsentenceencodermodels\/universal-sentence-encoder-models\/use-large\")\n# embeddings = embed([\n#     \"The quick brown fox jumps over the lazy dog.\",\n#     \"I am a sentence for which I would like to get its embedding\"])\n# print(embeddings)","8f70a9e3":"%%time\nRANDOM_STATE = 147\n\nX_train = np.vstack([embed([text]) for text in tqdm(train_df['excerpt'])])\ny = train_df['target']\nprint(f'Shape of Train vectors: {X_train.shape}')\n\nX_test = np.vstack([embed([text]) for text in tqdm(test_df['excerpt'])])\nprint(f'Shape of Test vectors: {X_test.shape}')\n    \nregressor = Ridge(fit_intercept=True, normalize=False)\nscores = cross_val_score(regressor, X_train, y, cv=5, \n                         scoring='neg_root_mean_squared_error')\nprint(f'Average Root mean squared error: {np.abs(np.mean(scores))}')\n\nregressor = regressor.fit(X_train, y)\ntest_df['target'] = regressor.predict(X_test)\ntest_df[['id','target']].to_csv('submission.csv', index=False)","407cfa8b":"All Credits to Sumit Kumar @anaverageengineer https:\/\/www.kaggle.com\/anaverageengineer\/comlrp-baseline-for-complete-beginners","ca7cf98e":"**Please note that I am getting better public LB score without using any of preprocessing steps compared to using processing steps, for default parameters**","c4e6093d":"#### 5.Spacy vectors + Ridge Regression","b639c19e":"This notebook gives submission for below models,\n\n1. TFIDF + Linear Regression\n2. TFIDF + Random Forest Regressor\n3. TFIDF + SVD + Random Forest regression\n4. TFIDF + SVD + Ridge regression\n5. Spacy vectors + Ridge Regression\n6. Universal Sentence Encoder + Ridge Regression\n7. Spacy vectors + Pytorch Regressor [To Do]\n8. Spacy vectors + Pytorch Ranker [To Do]\n9. Pytorch LSTM [To Do]","fb20e8ff":"#### 4. TFIDF+SVD+Ridge Regressor","880c7575":"#### 6.Universal Sentence Encoder + Ridge Regression","0a3f6e40":"#### 3. TFIDF+SVD+ RandomForestRegressor","6b096b48":"##### SVD","a6176a42":"### Load Libraries","a0b1447c":"#### TFIDF","1bab962b":"### Text preprocessing (Cleaning)","a6b02f6e":"#### 2. RandomForestRegressor","c745fb54":"1. Remove stop words\n2. stemming and lemmatization\n3. Remove punctuation\/change punctuation\n4. Remove URLs(links)\n5. Remove Numbers ","711f35ba":"### Read Data","ebca591e":"#### 1. Linear Regression","a9872057":"## Models","9e8ff51f":"The inspiration to write this notebook is from [here](https:\/\/www.kaggle.com\/c\/commonlitreadabilityprize\/discussion\/240871). This notebook goes through the implementation of **NON Transformers** models, before going and trying with transformers models.\nThis notebook is currently in progress, "}}