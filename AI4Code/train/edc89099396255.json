{"cell_type":{"5ad3be42":"code","4ed17202":"code","91b9f43a":"code","72ad3769":"code","e56926dd":"code","b3751c75":"code","acbbacd4":"code","ec764112":"code","513cdcd0":"code","25853fe4":"code","f75e9f48":"code","12433a12":"code","91ee3f89":"code","7fc73504":"code","7c3ca605":"code","3ee52f24":"code","c2cfd18f":"code","110423af":"code","16d3d188":"code","1609780a":"code","386cf0ec":"code","f74c51b8":"code","e8243a0f":"code","a7a521ef":"code","56d76f95":"code","662232d5":"code","9623d25e":"code","aab351b3":"code","7f4bc2c2":"code","369ab3f8":"code","4eeb74de":"code","6a3389ed":"code","baa54ffc":"code","78c79644":"code","88b20430":"code","08043e6e":"code","4698e17c":"code","acaa320f":"code","c6f7c28c":"code","57189ffa":"code","9bfa2bc8":"code","3d67aec2":"code","53372220":"code","7bcc0214":"code","55a53a6d":"code","9f71ce4e":"code","41f17783":"markdown","6388a688":"markdown","c96385c6":"markdown","b4b501cb":"markdown","d9570c3f":"markdown","e5d79f3a":"markdown","aa58f5a9":"markdown","acec5b99":"markdown","9bc8d306":"markdown","0a2b30c0":"markdown","29eba9b5":"markdown","31a49a4c":"markdown","11aa1b2c":"markdown","331b4f83":"markdown","e3c6cf15":"markdown","2442c7fa":"markdown","41c901cf":"markdown","6b8e241f":"markdown","cc233b16":"markdown","8c62ca96":"markdown","48d1c635":"markdown","5a847b3a":"markdown","32e22fab":"markdown","572b4f69":"markdown","f65081e1":"markdown","561aea82":"markdown","026148d5":"markdown","6933526e":"markdown","a4102cbf":"markdown","fc4e99ca":"markdown","188d213d":"markdown","abb022d4":"markdown","94445520":"markdown","031bb5e0":"markdown"},"source":{"5ad3be42":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nfrom sklearn.ensemble import RandomForestRegressor as rf_reg\nfrom sklearn.model_selection import RandomizedSearchCV as randomCV\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error as MSE\nfrom sklearn import feature_selection\nfrom sklearn.linear_model import LinearRegression as l_reg\nfrom sklearn.decomposition import PCA\nfrom sklearn.metrics import r2_score\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.model_selection import cross_validate\nfrom plotnine import *\nfrom matplotlib import gridspec\nfrom sklearn.preprocessing import StandardScaler\nimport pprint","4ed17202":"df = pd.read_csv('..\/input\/eergy-efficiency-dataset\/ENB2012_data.csv')\ndf.head()","91b9f43a":"df.columns=[\"relative_compactness\",\"surface_area\",\"wall_area\",\"roof_area\",\"overall_height\",\"orientaion\",\n                   \"glazing_area\",\"glazing_area_dist\",\"heating_load\",\"cooling_load\"]\ndf","72ad3769":"df.describe()","e56926dd":"df.loc[df[\"glazing_area\"]==0].describe()","b3751c75":"df.loc[df[\"glazing_area_dist\"]==0].describe()","acbbacd4":"df.hist(figsize=(15,15))\nplt.show()","ec764112":"df[\"log_heating_load\"]=np.log(df[\"heating_load\"])\ndf[\"log_heating_load\"].hist(bins=6)\nplt.show()","513cdcd0":"df[\"log_cooling_load\"]=np.log(df[\"cooling_load\"])\ndf[\"log_cooling_load\"].hist(bins=6)\nplt.show()","25853fe4":"sns.pairplot(df)\nplt.show()","f75e9f48":"corr = df.corr()\nmask = np.zeros_like(corr, dtype=bool)\nmask[np.triu_indices_from(mask)] = True\nf, ax = plt.subplots(figsize=(12, 10))\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\nsns.heatmap(corr, mask=mask,cmap=cmap, vmax=.9, center=0, square=True, linewidths=.5, annot=True,cbar_kws={\"shrink\": .5})\nplt.show()","12433a12":"df_f=df.copy()\ndf_f.drop([\"heating_load\",\"cooling_load\"],axis=1,inplace=True)\n#energy_df_f.drop([\"log_heating_load\",\"cooling_load\"],axis=1,inplace=True)\n\nenergy_X=df_f.iloc[:,:-2]\nenergy_Y=df_f.loc[:,[\"log_heating_load\"]]\n#energy_Y=energy_df_f.loc[:,[\"heating_load\"]]\n\ntrain_X,test_X,train_Y,test_Y=\\\ntrain_test_split(energy_X,energy_Y,test_size=0.20,random_state=48)\n\nprint(train_X.shape)\nprint(test_X.shape)","91ee3f89":"def rf_regr_cv_model(min_sample_split_in,min_sample_leaf_in,max_feature_in):\n    rf_grid={\"min_samples_split\":min_sample_split_in,\n             \"min_samples_leaf\":min_sample_leaf_in,\"max_features\":max_feature_in}\n    regr = rf_reg(max_depth=3, random_state=48)\n    rf_reg_cv = randomCV(regr, rf_grid, random_state=48,scoring='neg_root_mean_squared_error',cv=5)\n    return rf_reg_cv","7fc73504":"min_sample_split=np.arange(10,35,5)\nmin_sample_leaf=np.arange(10,35,5)\nmax_feature=np.arange(3,7,1)\n\nrf_reg_search=rf_regr_cv_model(min_sample_split_in=min_sample_split,min_sample_leaf_in=min_sample_leaf,\n                            max_feature_in=max_feature)","7c3ca605":"rf_reg_search.fit(train_X,np.ravel(train_Y))","3ee52f24":"print(\"Best parameters set:\",rf_reg_search.best_params_)\nprint(\"Best score:\",rf_reg_search.best_score_)","c2cfd18f":"regr_rf_best=rf_reg(min_samples_split= 30, min_samples_leaf=10, max_features=6,max_depth=3,random_state=48)\nregr_rf_best.fit(train_X,np.ravel(train_Y))","110423af":"predicted_train_Y=regr_rf_best.predict(train_X)\npredicted_test_Y=regr_rf_best.predict(test_X)\nprint(\"RMSE for Train set:\",MSE(predicted_train_Y,train_Y,squared=False))\nprint(\"RMSE for Test set:\",MSE(predicted_test_Y,test_Y,squared=False))","16d3d188":"feature_list=list(train_X.columns)\nfeature_impt=list(regr_rf_best.feature_importances_)\nfeature_impt_dict=dict(zip(feature_list,feature_impt))\nfeature_impt_dict=dict(sorted(feature_impt_dict.items(), key=lambda item: item[1],reverse=True))\nfeature_impt_dict","1609780a":"final_feature_list=[\"relative_compactness\",\"overall_height\",\"glazing_area\",\"wall_area\",\"roof_area\",\"surface_area\"]","386cf0ec":"def l_reg_cv(train_X,train_Y,feature_list):\n    rmse_list_train=[]\n    rmse_list_test=[]\n    r2_list_train=[]\n    r2_list_test=[]\n    for i in range(1,len(feature_list)+1):\n        train_X_temp=train_X.loc[:,feature_list[:i]]\n        cv_results_temp = cross_validate(l_reg(), train_X.loc[:,final_feature_list[:i]],train_Y, \n                            cv=5,scoring=[\"neg_root_mean_squared_error\",\"r2\"],return_train_score=True)\n        mean_rmse_train=np.mean(cv_results_temp[\"train_neg_root_mean_squared_error\"])\n        mean_r2_train=np.mean(cv_results_temp[\"train_r2\"])\n        mean_rmse_test=np.mean(cv_results_temp[\"test_neg_root_mean_squared_error\"])\n        mean_r2_test=np.mean(cv_results_temp[\"test_r2\"])\n        rmse_list_train.append(mean_rmse_train)\n        r2_list_train.append(mean_r2_train)\n        rmse_list_test.append(mean_rmse_test)\n        r2_list_test.append(mean_r2_test)\n        rmse_df=pd.DataFrame(zip(rmse_list_train,rmse_list_test,r2_list_train,r2_list_test))\n        rmse_df.columns=[\"Mean RMSE Train\",\"Mean RMSE Test\",\"Mean R2 Train\",\"Mean R2 Test\"]\n        rmse_df.index=rmse_df.index+1\n    return rmse_df","f74c51b8":"rmse_cv=l_reg_cv(train_X,np.ravel(train_Y),final_feature_list)\nrmse_cv","e8243a0f":"fig,ax=plt.subplots(1,2,figsize=(10,5))\nsns.lineplot(data=rmse_cv.iloc[:,:2],ax=ax[0])\nsns.lineplot(data=rmse_cv.iloc[:,2:],ax=ax[1])\nax[0].set_title(\"Mean Negative RMSE \\n Based on Number of Features\")\nax[1].set_title(\"Mean R2 Based on Number of Features\")\nplt.show()","a7a521ef":"l_reg_best=l_reg()\nl_reg_best.fit(train_X.loc[:,final_feature_list[:5]],np.ravel(train_Y))\npred_train_Y_best=l_reg_best.predict(train_X.loc[:,final_feature_list[:5]])\npred_test_Y_best=l_reg_best.predict(test_X.loc[:,final_feature_list[:5]])","56d76f95":"print(\"RMSE for Train set:\",MSE(pred_train_Y_best,train_Y,squared=False))\nprint(\"RMSE for Test set:\",MSE(pred_test_Y_best,test_Y,squared=False))","662232d5":"print(\"R2 for Train set:\",r2_score(pred_train_Y_best,train_Y))\nprint(\"R2 for Test set:\",r2_score(pred_test_Y_best,test_Y))","9623d25e":"dict(zip(final_feature_list[:5],np.exp(l_reg_best.coef_)))","aab351b3":"def predictVSactual(actual_y,y_predict,title_label):\n    fig,ax=plt.subplots(1,len(actual_y),figsize=(15,15))\n    for i,col in enumerate(actual_y,0):\n        ax[i].plot(np.ravel(actual_y[i]),\n                   np.ravel(y_predict[i]),'o',markeredgecolor=\"black\")\n        ax[i].set_title(title_label[i])\n        ax[i].set_xlabel('Actual Values')\n        ax[i].set_ylabel('Predicted Values')\n        ax[i].set(aspect='equal')\n        x=ax[i].get_xlim()\n        y=ax[i].get_xlim()\n        ax[i].plot(x,y, ls=\"--\", c=\".3\")\n    return fig,ax","7f4bc2c2":"actual_y_energy=[train_Y,test_Y]\npred_y_energy=[pred_train_Y_best,pred_test_Y_best]","369ab3f8":"predictVSactual(actual_y_energy,pred_y_energy,\n                [\"Scatter Plot: Prediction Comparison (Train)\",\"Scatter Plot: Prediction Comparison (Test)\"])\nplt.show()","4eeb74de":"def residual_plot(actual_y,predict_y,title_label):\n    fig,ax=plt.subplots(1,len(actual_y),figsize=(10,5))\n    for i,col in enumerate(actual_y,0):\n        sns.residplot(x=actual_y[i], y=predict_y[i], lowess=True, color=\"g\",ax=ax[i])\n        ax[i].set_title(title_label[i])\n    return fig,ax","6a3389ed":"residual_plot(actual_y_energy,pred_y_energy,[\"Train\",\"Test\"])\nplt.show()","baa54ffc":"raw_pred_err_list=[]\n\nfor i in range(0,len(actual_y_energy)):\n    list_temp=[]\n    list_temp=actual_y_energy[i].to_numpy().ravel()-pred_y_energy[i]\n    raw_pred_err_list.append(list_temp)\nraw_pred_err_label=[\"Raw Prediction Errors (Train)\",\"Raw Prediction Errors (Test)\"]","78c79644":"def raw_predict_err_hist(err_predict_list,bin_no,title_label):\n    fig,ax=plt.subplots(1,len(err_predict_list),figsize=(10,5))\n    for i,col in enumerate(err_predict_list,0):\n        sns.histplot(x=err_predict_list[i],bins=bin_no,kde=True,ax=ax[i])\n        ax[i].set_title(title_label[i])\n    return fig,ax","88b20430":"raw_predict_err_hist(raw_pred_err_list,bin_no=7,title_label=raw_pred_err_label)\nplt.show()","08043e6e":"energy2_X=df_f.iloc[:,:-2]\nenergy2_Y=df_f.loc[:,[\"log_cooling_load\"]]\n#energy_Y=energy_df_f.loc[:,[\"heating_load\"]]\n\nenergy2_train_X,energy2_test_X,energy2_train_Y,energy2_test_Y=\\\ntrain_test_split(energy2_X,energy2_Y,test_size=0.20,random_state=48)","4698e17c":"rmse_cv2=l_reg_cv(energy2_train_X,np.ravel(energy2_train_Y),final_feature_list)\nrmse_cv2","acaa320f":"fig,ax=plt.subplots(1,2,figsize=(10,5))\nsns.lineplot(data=rmse_cv2.iloc[:,:2],ax=ax[0])\nsns.lineplot(data=rmse_cv2.iloc[:,2:],ax=ax[1])\nax[0].set_title(\"Mean Negative RMSE \\n Based on Number of Features\")\nax[1].set_title(\"Mean R2 Based on Number of Features\")\nplt.show()","c6f7c28c":"l_reg2_best=l_reg()\nl_reg2_best.fit(energy2_train_X.loc[:,final_feature_list[:5]],np.ravel(energy2_train_Y))\npred2_train_Y_best=l_reg2_best.predict(energy2_train_X.loc[:,final_feature_list[:5]])\npred2_test_Y_best=l_reg2_best.predict(energy2_test_X.loc[:,final_feature_list[:5]])","57189ffa":"print(\"R2 for Train set:\",r2_score(pred2_train_Y_best,energy2_train_Y))\nprint(\"R2 for Test set:\",r2_score(pred2_test_Y_best,energy2_test_Y))","9bfa2bc8":"actual2_y_energy=[energy2_train_Y,energy2_test_Y]\npred2_y_energy=[pred2_train_Y_best,pred2_test_Y_best]\n\npredictVSactual(actual2_y_energy,pred2_y_energy,\n                [\"Scatter Plot: Prediction Comparison (Train)\",\"Scatter Plot: Prediction Comparison (Test)\"])\nplt.show()","3d67aec2":"residual_plot(actual2_y_energy,pred2_y_energy,[\"Train\",\"Test\"])\nplt.show()","53372220":"raw_pred2_err_list=[]\n\nfor i in range(0,len(actual2_y_energy)):\n    list_temp=[]\n    list_temp=actual2_y_energy[i].to_numpy().ravel()-pred2_y_energy[i]\n    raw_pred2_err_list.append(list_temp)\nraw_pred2_err_label=[\"Raw Prediction Errors (Train)\",\"Raw Prediction Errors (Test)\"]","7bcc0214":"raw_predict_err_hist(raw_pred2_err_list,bin_no=8,title_label=raw_pred2_err_label)\nplt.show()","55a53a6d":"print(\"Log heating load as dependent variable:\")\ndict(zip(final_feature_list[:5],np.exp(l_reg_best.coef_)))","9f71ce4e":"print(\"Log cooling load as dependent variable:\")\ndict(zip(final_feature_list[:5],np.exp(l_reg2_best.coef_)))","41f17783":"The function above is to do a linear regression model fitting based on the feature list. The data with independent variables (train_X) will be sliced according to the variables in the feature list and fit into the model with dependent variable. Then, RMSE and R2 will be calculated for each set of independent variables fitted into the model to find out which set of independent variables is the most optimal to fit into the model.","6388a688":"# Linear Regression - Energy Efficiency Dataset\nThis notebook will explore the dataset and use linear regression to explain the relationship between the independent variables and dependent variable (heating load) with random forest regressor to be used as a model for feature selection","c96385c6":"Looking at the histogram above, the residuals are normally distributed with long left tails. Most prediction errors are in the range of -0.1 to 0.1.\n\n# Using Raw Data (Log_cooling_load)\nFor this section, cooling load is used to find out the relationships for the same features with cooling load","b4b501cb":"After log transformation on heating and cooling loads, both variables' distributions look better but both show bimodal distribution as two peaks are formed.","d9570c3f":"Looking at the residual plots, they indicated that the residuals are in the range of -0.35 to 0.4. There are some outliers in train and test datasets as there are some instances with residuals greater than 0.3 or lesser than -0.3. The residual plots do not show any particular trends in the residuals.","e5d79f3a":"Based on the coefficients above:\n\nHeating load will be increased by a multiplicative factor of 2.43 when relative compactness increases by 1.\nHeating load will be increased by a multiplicative factor of 1.30 when overall height increases by 1.\nHeating load will be increased by a multiplicative factor of 2.86 when glazing area increases by 1.\nWall area and roof area do affect the heating load but lesser magnitude compared to the previous 3 factors.\nTherefore, lower building with low relative compactness and small glazing area requires less energy to warm up the indoor environment. Smaller wall area and roof area also reduce energy required to warm up the indoor environment.","aa58f5a9":"Looking at the histograms above, the residuals are approximately normally distributed. The prediction errors seem to be larger using log cooling load compared to log heating load.\n\n# Interpretation on Regression Coefficients","acec5b99":"Looking at heating load and cooling, they seem to be heavily skewed to the right. Therefore, log transformation will be done on heating load and cooling load to make them more normalised in term of distribution","9bc8d306":"The difference between test and train is at least 0.02 which is quite small.\n\nTherefore, the current model should be sufficient to predict the energy efficiency of a building in term of log heating load as the model can explain 90% of the variation in the data according to R2 and has small RMSE.","0a2b30c0":"The range of the residuals is between -0.3 to 0.3. The residual plots do not show any particular trends in the residuals.","29eba9b5":"The difference between test and train in RMSE is at least 0.01.","31a49a4c":"Based on the coefficients above:\n\nHeating load will be increased by a multiplicative factor of 2.43 when relative compactness increases by 1.\nHeating load will be increased by a multiplicative factor of 1.30 when overall height increases by 1.\nHeating load will be increased by a multiplicative factor of 2.86 when glazing area increases by 1.\nWall area and roof area affect the heating load but lesser magnitude compared to the previous 3 factors.\n# Model Diagnostics\nThis section will look at the prediction performance of the model by plotting scatter plot for comparison between actual and predicted values, histogram for prediction errors and residual plot.","11aa1b2c":"A feature list is created with the roof area and surface area as the last 2 features as linear regression will be done using forward selection by adding variables one by one according to the feature list and selecting the smallest RMSE and biggest r2.\n\n# Forward Selection Linear Regression\nThis section will conduct linear regression model fitting using forward selection by adding variables one by one into the model. The best model will be the one with the biggest negative RMSE and biggest r2.","331b4f83":"Using the rf_regr_cv_model and the hyperparameters declared, train dataset will be fitted into the model to determine the best set of hyperparameters for the model.","e3c6cf15":"Adding column name for each column in the dataframe for clearer understanding and easier data slicing using pandas","2442c7fa":"The difference between test and train in RMSE is at least 0.01 which is quite small.","41c901cf":"Looking at the graphs and the table above, linear regression with 5 features seems to be better as it has higher negative mean RMSE and higher mean R2 in test set compared to others. Furthermore, means for negative RMSE and R2 in test slightly decreased when using 6 features.\n\nTherefore,the most optimal number of features to be used in linear regression is 5 and the selected 5 features are relative_compactness, overall_height, glazing_area, wall_area and roof_area.\n\n# Best Fit Linear Regression Model\nThis section will be refitted the linear regression model using the top 5 features in the previous section.","6b8e241f":"Based on the randomised CV search result above, the model performs the best when:\n\nthe sample split at 30\neach leaf has 10 instances\n6 features are used\nThe model is refitted with the best set of hyperparameters found in randomised CV search.","cc233b16":"The function above is to plot two scatter plots side by side with train on the left and test on the right using actual values and predicted values that store in list as inputs.","8c62ca96":"R2 for test is lower when using log cooling load as dependent variable instead of log heating load.\n# Model Diagnostics","48d1c635":"Looking at the model performance in term of RMSE, the model seems to be overfitting as RMSE in test is higher than in train. But, the difference is quite small, around 0.02.\n\n# Feature Selection\nThis section will conduct feature selection using weight importance calculated from the random forest regressor to select the features to be used in linear regression","5a847b3a":"Similar to using log heating load as dependent variable, the model seems to be underestimated the cooling load as most points are at the right side of the diagonal line.","32e22fab":"The function above is to create a randomised cross-validation search process using random forest regressor as base model to tune the hyperparameters in the model. The hyperparameters to be tuned are minimum sample split, minimum sample size in the leaf and maximum features to be used in each regression tree. The model is regularised with maximum depth of 3 and using seed number 48 to prevent overfitting. Root mean squared error (RMSE) is used as the scoring criteria to determine the best set of hyperparameters.","572b4f69":"relative compactness is highly correlated to surface area, roof area and overall height. Therefore, feature selection is required to reduce the number of features that are highly correlated.\n\nheating load is highly correlated to cooling load which suggested that only 1 of them can be used as dependent factor to determine energy efficiency of the building. Therefore, log heating load is selected as dependent variable to determine energy efficiency in term of heating load.\n\n# Using Raw Data (Log_heating_load)\nUsing the dataset above with log heating load as the main dependent variable, the data is split into train and test datasets with the ratio of 80:20. After splitting, feature selection and model fitting using linear regression will be done to test the performance of linear regression after feature selection.","f65081e1":"Train dataset has 614 instances while test dataset has 154 instances with 8 variables including log heating load after dropping heating load and cooling load.\n\n# Randomised Grid Search for Random Forest Regressor\nThis section will use the data with log heating load as the main dependent variable to do a randomised cross validation search for random forest regressor to tune the hyperparameters for random forest regressor. The random forest regressor is used as feature selection model as the model can calculate the weight importance for each variable based on the proportion of each variable is used in the model to partition the data in a way that the predicted value is closer to the actual value in dependent variable.","561aea82":"Based on the 2 summary tables above, when glazing area or glazing area distribution is 0, the other also contain value 0. So, if glazing area or glazing area distribution are valued at 0 for a building, it can be assumed that the building itself do not have glazing area.","026148d5":"Based on the coefficients above:\n\nCooling load will be reduced by a multiplicative factor of 0.36 when relative compactness increases by 1.\nCooling load will be increased by a multiplicative factor of 1.23 when overall height increases by 1.\nCooling load will be increased by a multiplicative factor of 1.88 when glazing area increases by 1.\nWall area and roof area do affect the cooling load but lesser magnitude compared to the previous 3 factors.\nTherefore, lower building with high relative compactness and small glazing area requires less energy to cool down the indoor environment. Smaller wall area and bigger roof area also reduce energy required to cool down the indoor environment.","6933526e":"There are no missing values for each variable as the counts for each variable are the same. However, glazing area and glazing area distribution have values 0 for some instances.","a4102cbf":"Looking at the scatter plot, the model seems to be underestimated the heating load as more points situated at the right side of the diagonal line.","fc4e99ca":"Looking at the list above, relative compactness, surface area, overall height, roof area and glazing area are the top 5 features. However, there are 2 pairs of variables that are high correlated:\n\nrelative compactness with surface area\nroof area with surface area\noverall height with roof area\nrelative compactness with overall height","188d213d":"# Loading Data","abb022d4":"Looking at the table and graphs above, the appropriate number of features are 5 as using 6 features do not show any great improvement on RMSE and R2 for both train and test datasets.","94445520":"# Imports","031bb5e0":"# Exploring and Transforming Data\nThis section will explore data through summary table, histogram and correlation matrix to understand the data, explore the relationship between the variables and check whether the data contain any missing values."}}