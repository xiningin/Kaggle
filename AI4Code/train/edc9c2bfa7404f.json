{"cell_type":{"6f321183":"code","f7ed1f63":"code","44b924a9":"code","8c769c35":"code","a27bd702":"code","609b3a3a":"code","9f943f48":"code","30c958e5":"code","6d449c81":"code","3a2456ba":"code","08cc3ecc":"code","647c9477":"code","dd59987a":"code","57ffec1b":"code","6b61bd2d":"code","d3fbb178":"code","66340796":"code","9c0e1220":"code","3cee1a2d":"code","5a03d915":"code","879e8108":"code","47e883b7":"code","34184898":"code","452d0b6c":"code","6b7b399e":"code","226842e4":"code","602fa217":"code","75012c07":"code","2329ea54":"code","f8d1ec0c":"code","6f56b48e":"code","10188176":"code","c70ecb02":"code","78cf1f27":"code","68bac945":"code","3bd4fcd0":"code","244070eb":"code","a0bdc1bd":"code","559a084c":"code","bb9796b9":"code","dd6025f2":"code","bdc8f7a5":"code","f09ba03a":"code","8d326f70":"code","0001c26c":"code","a926730e":"code","1b24aaf2":"code","f370166a":"code","931ff310":"code","eb9bff93":"code","6aa14937":"code","f2f6c373":"code","f9a009f4":"code","4737e624":"code","dff76c50":"code","7f1e0fbd":"code","739db054":"code","d614ce9b":"code","4404d2f5":"code","05c84f27":"code","2a2b7051":"code","4d47d001":"code","1503a3ae":"code","1a5a7827":"code","3fa139a6":"code","94db417e":"code","ed67d600":"code","96d9c54e":"code","82b37a60":"code","399d2480":"code","b2d904fb":"code","e744a67b":"code","3422cb39":"code","07bd8019":"code","fb0897a4":"code","a3c5e941":"code","96e8219a":"code","9d638422":"code","81cad2a9":"code","1593f205":"code","dbd65410":"code","f3540254":"code","370e8bdc":"code","abab1513":"code","6211f739":"code","572ab919":"code","95eddf00":"code","179e0dbd":"code","6cad2964":"code","e1caf077":"code","9041079e":"code","0e455457":"code","988d6c28":"code","2d95e88d":"code","091bc18e":"code","534916b4":"code","f100f0c8":"code","84aa5771":"code","70d7fd1a":"code","f8b9df37":"code","919c981f":"code","72151698":"code","383acb24":"code","486efab6":"code","db4b1157":"code","55ef86ea":"code","c3ac5d30":"code","bffa2685":"code","2574a995":"code","0da4a11d":"code","421bb6bc":"code","7e2c2895":"code","a94a5a8e":"code","7e34d3d8":"code","eee2afc9":"code","6e0b24da":"markdown","88de373e":"markdown","040b0255":"markdown","ef679422":"markdown","55f8f397":"markdown","c7b1e47f":"markdown","6225c62e":"markdown","540f88e1":"markdown","3507b9bf":"markdown","8260c757":"markdown","c150fa2d":"markdown","8ad64f7b":"markdown","4e86a533":"markdown","a683c919":"markdown","2320a53f":"markdown","252ff5fc":"markdown","c7df2ae3":"markdown","3cfb5094":"markdown","22772d7e":"markdown","278ee080":"markdown","02870e46":"markdown","1e1e45c7":"markdown","b808b8bf":"markdown","2f889520":"markdown","b570219f":"markdown","414d944b":"markdown","980d3e54":"markdown","75c80026":"markdown","5d4df8f0":"markdown","428efec3":"markdown","863f5f0f":"markdown","5d2edf03":"markdown","8a179673":"markdown","20ee55f3":"markdown","6ed381da":"markdown","4dbc0555":"markdown","3d85919d":"markdown","84f3d654":"markdown","a0375f53":"markdown","acb24dba":"markdown","6fcd9a85":"markdown","e1ae35c0":"markdown","223c1bd5":"markdown","f9691582":"markdown","59d874b4":"markdown","fa13e89b":"markdown","8f85fe96":"markdown","a4f5161b":"markdown","7cedf6a2":"markdown","a133b9b1":"markdown","eea3301b":"markdown","03b31e53":"markdown","885af3bf":"markdown","513445ba":"markdown","d1e16b3e":"markdown","23b678b0":"markdown","0d68c267":"markdown","4bc50d63":"markdown","12afe617":"markdown","4553bd7b":"markdown","813d280e":"markdown","2c0c0d7a":"markdown","ceb9645f":"markdown","b3277cb9":"markdown","05147463":"markdown","8422f817":"markdown","68aba823":"markdown","a560c1f2":"markdown","ad4fa1ca":"markdown","492f347d":"markdown","0fba896d":"markdown","43666b6e":"markdown","ceee1cb8":"markdown","aad8212d":"markdown","8f4c71fa":"markdown","67c9651f":"markdown","86f7406c":"markdown","9cf69058":"markdown","1f3605bf":"markdown","6c570b6c":"markdown","daeec066":"markdown","1a203695":"markdown","d5b9332a":"markdown","3d353afc":"markdown","17d44294":"markdown","b60f698a":"markdown","100a8a32":"markdown","37270e71":"markdown","44f7cd71":"markdown","03a7045a":"markdown","ca444f85":"markdown","09fe3a0d":"markdown","62917daa":"markdown","aee67c91":"markdown","5351a14c":"markdown","6e4a8288":"markdown","23ec9196":"markdown","b78c69e7":"markdown","587e4175":"markdown","f1030ef5":"markdown","ca640385":"markdown","ff9dfdfa":"markdown","79fc1f0e":"markdown","af97b19f":"markdown","59032090":"markdown","d0db1d90":"markdown","18093c94":"markdown","18e85af3":"markdown","40442bdc":"markdown","9158eb23":"markdown","77534e0d":"markdown","ef24ba81":"markdown","f5b45d4d":"markdown","6dfabb0a":"markdown","b50272b8":"markdown","a9d232e4":"markdown","ef645ab2":"markdown","57196d12":"markdown","3c5f8143":"markdown","283e21d3":"markdown","c3d0b59e":"markdown","5b209fe1":"markdown","3d83f91a":"markdown","538c9212":"markdown","1dee63cc":"markdown","c9bf186f":"markdown","cbeb5838":"markdown","18844d3a":"markdown","613ddc87":"markdown","61e887c1":"markdown","f6ed6f7d":"markdown","903b3176":"markdown","8ebffa90":"markdown","97d260a8":"markdown","c0c2f384":"markdown","8669c75b":"markdown","78480935":"markdown","276110d0":"markdown","27b21739":"markdown"},"source":{"6f321183":"\n# Loading Data\nimport pandas as pd\nimport numpy as np\nimport nltk\nimport string\nimport re\nimport time\n\n# Data Preprocessing\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import stopwords\nfrom sklearn.utils import resample\nfrom nltk.tokenize import word_tokenize, TreebankWordTokenizer\nfrom nltk.stem import WordNetLemmatizer\n\n# Model Building\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\nfrom sklearn.svm import LinearSVC\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import GridSearchCV, cross_val_score\n\n# Model Evaluation\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import precision_score\nfrom sklearn.model_selection import GridSearchCV, cross_val_score\nfrom sklearn.multiclass import OneVsRestClassifier, OneVsOneClassifier\n#from scikitplot.metrics import plot_roc, plot_confusion_matrix\n\n# Explore Data Analysis\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom wordcloud import WordCloud, STOPWORDS\nfrom matplotlib.pyplot import rcParams\n\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n\n","f7ed1f63":"#download libraries\nnltk.download('stopwords')\nnltk.download('wordnet')\nstop_words = set(stopwords.words('english'))\nsns.set_style('whitegrid')","44b924a9":"from google.colab import files \n  \n  \nuploaded = files.upload()","8c769c35":"train = pd.read_csv('\/content\/train.csv')\ntest = pd.read_csv('\/content\/test.csv') ","a27bd702":"#create a copy of the origional data\nftrain = train.copy()\nftest = test.copy()","609b3a3a":"print('There are', len(ftrain), 'rows and',ftrain.shape[1], 'columns in the train set.')\nprint('There are', len(ftest), 'rows and',ftest.shape[1], 'columns in the test set.')","9f943f48":"#test data\nftest.isnull().sum()","30c958e5":"#train data\nftrain.isnull().sum()","6d449c81":"#extract the value counts per sentiment class\na = ftrain.sentiment.value_counts()\n#calculate the percentage of each sentiment class\nb = 100*ftrain.sentiment.value_counts()\/len(ftrain.sentiment)\nb = round(b,2)\ndata = pd.concat([a,b],axis =1,)\ndata.columns = ['Value Count', 'Percentage']\ndata","3a2456ba":"sns.countplot(x='sentiment',data=ftrain,palette='rainbow')","08cc3ecc":"#brief description of the train data\nftrain.message.describe()","647c9477":"#brief description of the test data\nftest.message.describe()","dd59987a":"#description of the data per sentiment class\nftrain[['sentiment','message']].groupby('sentiment').describe()","57ffec1b":"ftrain['length'] = ftrain['message'].apply(len)\nftrain.head()","6b61bd2d":"#creating a lenght column\nftest['length'] = ftest['message'].apply(len)\nftest.head()","d3fbb178":"sns.distplot(ftrain['length'],bins=30,kde=False,color='#440154')","66340796":"ftrain['length'].describe()","9c0e1220":"#print the longest tweet in the train data\nftrain[ftrain['length'] == 208]['message'].iloc[0]","3cee1a2d":"sns.distplot(ftest['length'],bins=30,kde=False,color='#20A387')","5a03d915":"ftest['length'].describe()","879e8108":"ftest[ftest['length'] == 623]['message'].iloc[0]","47e883b7":"g = sns.FacetGrid(ftrain,col='sentiment')\ng.map(plt.hist,'length')","34184898":"#convert the test to numerical values \ncv = CountVectorizer(stop_words = 'english')\nwords = cv.fit_transform(ftrain.message)\n\nsum_words = words.sum(axis=0)\n#create a frequency of most occuring words\nwords_freq = [(word, sum_words[0, i]) for word, i in cv.vocabulary_.items()]\nwords_freq = sorted(words_freq, key = lambda x: x[1], reverse = True)\n#create a dataframe of the words and frequency \nfrequency = pd.DataFrame(words_freq, columns=['word', 'freq'])\n\nfrequency.head(30).plot(x='word', y='freq', kind='bar', figsize=(15, 7), color = '#440154')\nplt.title(\"Train : Most Frequently Occuring Words - Top 30\",size=15)","452d0b6c":"cv = CountVectorizer(stop_words = 'english')\nwords = cv.fit_transform(ftest.message)\n\nsum_words = words.sum(axis=0)\n\nwords_freq = [(word, sum_words[0, i]) for word, i in cv.vocabulary_.items()]\nwords_freq = sorted(words_freq, key = lambda x: x[1], reverse = True)\n\nfrequency = pd.DataFrame(words_freq, columns=['word', 'freq'])\n\nfrequency.head(30).plot(x='word', y='freq', kind='bar', figsize=(15, 7), color = '#20A387')\nplt.title(\"Test : Most Frequently Occuring Words - Top 30\", size =15)","6b7b399e":"#creating a word cloud from the data\nwordcloud = WordCloud(background_color = 'white', \n                      width = 1000, height = 1000).generate_from_frequencies(dict(words_freq))\n\nplt.figure(figsize=(8,8))\nplt.title(\"WordCloud - Vocabulary from tweets\")\nplt.imshow(wordcloud)","226842e4":"def mentions(text):\n    \"\"\"\n    The function extracts all the \n    mentions from the message columns\n    \"\"\"\n    line=re.findall(r'(?<=@)\\w+',text)\n    return \" \".join(line)","602fa217":"#creating a mentions column\nftrain['mentions']=ftrain['message'].apply(lambda x:mentions(x))\n\ntrain_neg = ftrain.loc[ftrain['sentiment'] == -1]\ntrain0 = ftrain.loc[ftrain['sentiment'] == 0]\ntrain1 = ftrain.loc[ftrain['sentiment'] == 1]\ntrain2 = ftrain.loc[ftrain['sentiment'] == 2]","75012c07":"#counting the mentions in the data\ntemp_neg= train_neg['mentions'].value_counts()[:][1:11]\n\ntemp_neg =temp_neg.to_frame().reset_index().rename(columns={'index':'Mentions','mentions':'count'})\n\nplt.figure(figsize=(16,5))\nx= temp_neg['Mentions']\ny= temp_neg['count']\n\nplt.title('Sentiment Class -1',size =15)\nsns.barplot(x=y,y=x,color='#ff7f00')","2329ea54":"#counting the mentions in the data\ntemp0= train0['mentions'].value_counts()[:][1:11]\ntemp0 =temp0.to_frame().reset_index().rename(columns={'index':'Mentions','mentions':'count'})\nplt.figure(figsize=(16,5))\n\nx= temp0['Mentions']\ny= temp0['count']\n\nplt.title('Sentiment Class 0',size =15)\nsns.barplot(x=y,y=x,color='#fb9a99')","f8d1ec0c":"#counting the mentions in the data\ntemp1= train1['mentions'].value_counts()[:][1:11]\ntemp1 =temp1.to_frame().reset_index().rename(columns={'index':'Mentions','mentions':'count'})\nplt.figure(figsize=(16,5))\n\nx= temp1['Mentions']\ny= temp1['count']\nplt.title('Sentiment Class 1',size =15)\nsns.barplot(x=y,y=x,color='#33a02c')","6f56b48e":"#counting the mentions in the data\ntemp2= train2['mentions'].value_counts()[:][1:11]\ntemp2 =temp2.to_frame().reset_index().rename(columns={'index':'Mentions','mentions':'count'})\nplt.figure(figsize=(16,5))\n\nx= temp2['Mentions']\ny= temp2['count']\nplt.title('Sentiment Class 2',size =15)\nsns.barplot(x=y,y=x,color='#b2df8a')","10188176":"# collecting the hashtags\n\ndef hashtag_extract(x):\n    \"\"\"\n    The function extract the hashtags\n    from the messages column\n    \"\"\"\n    hashtags = []    \n    for i in x:\n        ht = re.findall(r\"#(\\w+)\", i)\n        hashtags.append(ht)\n    return hashtags","c70ecb02":"# extracting hashtags from train tweets\nHT_train_neg = hashtag_extract(ftrain['message'][ftrain['sentiment'] == -1])\nHT_train0 = hashtag_extract(ftrain['message'][ftrain['sentiment'] == 0])\nHT_train1 = hashtag_extract(ftrain['message'][ftrain['sentiment'] == 1])\nHT_train2 = hashtag_extract(ftrain['message'][ftrain['sentiment'] == 2])\n\n\n# unnesting list\nHT_train_neg = sum(HT_train_neg,[])\nHT_train0 = sum(HT_train0,[])\nHT_train1 = sum(HT_train1,[])\nHT_train2 = sum(HT_train2,[])","78cf1f27":"#creating a frequency distribution of the hashtags\na = nltk.FreqDist(HT_train_neg)\nd = pd.DataFrame({'Hashtag': list(a.keys()),\n                  'Count': list(a.values())})\n\n# selecting top 10 most frequent hashtags     \nd = d.nlargest(columns=\"Count\", n = 10) \nplt.figure(figsize=(16,5))\nax = sns.barplot(data=d, x= \"Hashtag\", y = \"Count\", color ='#ff7f00')\nax.set(ylabel = 'Count')\nplt.show()","68bac945":"#An example of a sentiment found within class -1 tweets\nftrain[ftrain['sentiment'] == -1]['message'].iloc[67]","3bd4fcd0":"#creating a frequency distribution of the hashtags\na = nltk.FreqDist(HT_train0)\nc = pd.DataFrame({'Hashtag': list(a.keys()),\n                  'Count': list(a.values())})\n\n# selecting top 10 most frequent hashtags \nc = c.nlargest(columns=\"Count\", n = 10) \nplt.figure(figsize=(16,5))\nax = sns.barplot(data=c, x= \"Hashtag\", y = \"Count\",color ='#fb9a99')\nax.set(ylabel = 'Count')\nplt.show()","244070eb":"#An example of a sentiment found within class 0 tweets\nftrain[ftrain['sentiment'] == 0]['message'].iloc[184]","a0bdc1bd":"#An example of a sentiment found within class 0 tweets\nftrain[ftrain['sentiment'] == 0]['message'].iloc[197]","559a084c":"#creating a frequency distribution of the hashtags\na = nltk.FreqDist(HT_train1)\nd = pd.DataFrame({'Hashtag': list(a.keys()),\n                  'Count': list(a.values())})\n\n# selecting top 10 most frequent hashtags     \nd = d.nlargest(columns=\"Count\", n = 10) \nplt.figure(figsize=(16,5))\nax = sns.barplot(data=d, x= \"Hashtag\", y = \"Count\",color ='#33a02c')\nax.set(ylabel = 'Count')\nplt.show()","bb9796b9":"#An example of a sentiment found within class 1 tweets\nftrain[ftrain['sentiment'] == 1]['message'].iloc[89]","dd6025f2":"#creating a frequency distribution of the hashtags\na = nltk.FreqDist(HT_train2)\nd = pd.DataFrame({'Hashtag': list(a.keys()),\n                  'Count': list(a.values())})\n\n# selecting top 10 most frequent hashtags     \nd = d.nlargest(columns=\"Count\", n = 10) \nplt.figure(figsize=(16,5))\nax = sns.barplot(data=d, x= \"Hashtag\", y = \"Count\",color='#b2df8a')\nax.set(ylabel = 'Count')\nplt.show()","bdc8f7a5":"#An example of a sentiment found within class 2 tweets\nftrain[ftrain['sentiment'] == 2]['message'].iloc[1]","f09ba03a":"#class_1 the PRO class\n#class 2 the NEWS class\n#class 0 NUETRAL class\n#class -1 the ANTI class","8d326f70":"from IPython.display import Image\nImage('resampling.png', width=\"800\" ,height=\"400\")","0001c26c":"def resambling(df):\n    \"\"\"\n        The functions takes in dataframe and resample the classses base on class size.\n        The class size is a average of the datasets among the classes.\n        This function resamples by downsampling classes with observations greater than the class size and\n        upsampling the classes with observations smaller than the class size.\n    \"\"\"\n    df = df.copy()\n    class_2 = df[df['sentiment'] == 2]  \n    class_1 = df[df['sentiment'] == 1]  \n    class_0 = df[df['sentiment'] == 0]  \n    class_n1 = df[df['sentiment'] == -1] \n    class_size = int((len(class_1)+len(class_2)+len(class_0)+len(class_n1))\/4)\n    # Downsampling class_1 the PRO class\n    rclass_1 = resample(class_1, replace=True, n_samples=class_size, random_state=42)\n    #upsampling class 2 the NEWS class\n    rclass_2 = resample(class_2, replace=True, n_samples=class_size, random_state=42)\n    #upsampling class 0 NUETRAL class\n    rclass_0 = resample(class_0, replace=True, n_samples=class_size, random_state=42)\n    #upsampling class -1 the ANTI class\n    rclass_n1 = resample(class_n1, replace=True, n_samples=class_size, random_state=42)\n    sampled_df = pd.concat([rclass_2, rclass_1, rclass_0, rclass_n1])\n    \n    return sampled_df","a926730e":"Resampled_train_df = resambling(ftrain)","1b24aaf2":"news=Resampled_train_df[Resampled_train_df.sentiment == 2].shape[0]\npro =Resampled_train_df[Resampled_train_df.sentiment == 1].shape[0]\nneutral=Resampled_train_df[Resampled_train_df.sentiment == 0].shape[0]\nanti =Resampled_train_df[Resampled_train_df.sentiment == -1].shape[0]\n#visualising\nplt.figure(1,figsize=(14,8))\nplt.bar([\"News\", \"Pro\", \"Neutral\" , \"Anti\"],[news, pro, neutral , anti])\nplt.xlabel('Tweet_class')\nplt.ylabel('Sentiment counts')\nplt.title('Class Distributions')\nplt.show()","f370166a":"# Creating copies of dataframes\ntrain_copy = Resampled_train_df.copy()\ntest_copy = ftest.copy()","931ff310":"def cleaner(tweet):\n    \"\"\"\n    this function takes in a dataframe and perform the following:\n    -Convert letters to lowercases\n    -remove URL links\n    -remove # from hashtags\n    -remove numbers\n    -remove punctuation\n    from the text field then return a clean dataframe \n    \"\"\"\n    tweet = tweet.lower()\n    to_del = [\n        r\"@[\\w]*\",  # strip account mentions\n        r\"http(s?):\\\/\\\/.*\\\/\\w*\",  # strip URLs\n        r\"#\\w*\",  # strip hashtags\n        r\"\\d+\",  # delete numeric values\n        r\"U+FFFD\",  # remove the \"character note present\" diamond\n    ]\n    for key in to_del:\n        tweet = re.sub(key, \"\", tweet)\n    \n    # strip punctuation and special characters\n    tweet = re.sub(r\"[,.;':@#?!\\&\/$]+\\ *\", \" \", tweet)\n    # strip excess white-space\n    tweet = re.sub(r\"\\s\\s+\", \" \", tweet)\n    \n    return tweet.lstrip(\" \")","eb9bff93":"train_copy['message'] = train_copy['message'].apply(cleaner)","6aa14937":"train_copy.tail(5)","f2f6c373":"stop_word = stopwords.words('english')\ntrain_copy['message'] = train_copy['message'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_word)]))","f9a009f4":"train_copy.head(5)","4737e624":"tokeniser = TreebankWordTokenizer()\ntrain_copy['tokens'] = train_copy['message'].apply(tokeniser.tokenize)","dff76c50":"train_copy.head(5)","7f1e0fbd":"def lemmas(words, lemmatizer):\n    return [lemmatizer.lemmatize(word) for word in words]","739db054":"lemmatizer = WordNetLemmatizer()\ntrain_copy['lemma'] = train_copy['tokens'].apply(lemmas, args=(lemmatizer, ))","d614ce9b":"train_copy.head(5)","4404d2f5":"y = train_copy['sentiment']\nX = train_copy['message']","05c84f27":"vectorizer = TfidfVectorizer(ngram_range=(1,2), min_df=2, stop_words=\"english\")\nX_vectorized = vectorizer.fit_transform(X)","2a2b7051":"X_train,X_val,y_train,y_val = train_test_split(X_vectorized,y,test_size=.3,shuffle=True, stratify=y, random_state=11)","4d47d001":"rfc = RandomForestClassifier(n_estimators=100,random_state=42)\nrfc.fit(X_train, y_train)","1503a3ae":"lmc = LogisticRegression(multi_class='ovr')\nlmc.fit(X_train, y_train)","1a5a7827":"dtc = DecisionTreeClassifier(random_state=42)","3fa139a6":"dtc.fit(X_train, y_train)","94db417e":"svc = SVC(kernel='linear')\nsvc.fit(X_train, y_train)","ed67d600":"# Random forest Predict\nrfc_pred = rfc.predict(X_val)\n# Multi-class Logistic Predict\nlmc_pred = lmc.predict(X_val)\n#Decision Tree Predict\ndtc_pred = dtc.predict(X_val)\n# Support vector Machine Predict\nsvc_pred = svc.predict(X_val)\n","96d9c54e":"testx = test_copy['message']\ntest_vect = vectorizer.transform(testx)","82b37a60":"# Random Forest\nrfc_pred_t = rfc.predict(test_vect)\n# Multi-class Logistic Predict\nlmc_pred_t = lmc.predict(test_vect)\n#Decision Tree Predict\ndtc_pred_t = dtc.predict(test_vect)\n# Support vector Machine Predict\nsvc_pred_t = svc.predict(test_vect)\n","399d2480":"labels = [-1, 0,1,2]\n\npd.DataFrame(data=confusion_matrix(y_val, rfc_pred), index=labels, columns=labels)","b2d904fb":"labels = [-1, 0,1,2]\n\npd.DataFrame(data=confusion_matrix(y_val, lmc_pred), index=labels, columns=labels)","e744a67b":"labels = [-1, 0,1,2]\n\npd.DataFrame(data=confusion_matrix(y_val, dtc_pred), index=labels, columns=labels)","3422cb39":"labels = [-1, 0,1,2]\n\npd.DataFrame(data=confusion_matrix(y_val, svc_pred), index=labels, columns=labels)","07bd8019":"print(classification_report(y_val, rfc_pred, target_names=['Anti', 'Nuetral','Pro','News']))","fb0897a4":"print(classification_report(y_val, lmc_pred))","a3c5e941":"print(classification_report(y_val, dtc_pred))","96e8219a":"print(classification_report(y_val, svc_pred))","9d638422":"rfc_f1=f1_score(y_val, rfc_pred, average=\"macro\")","81cad2a9":"lmc_f1=f1_score(y_val, lmc_pred, average=\"macro\")","1593f205":"dtc_f1=f1_score(y_val, dtc_pred, average=\"macro\")","dbd65410":"svc_f1=f1_score(y_val, svc_pred, average=\"macro\")","f3540254":"test['sentiment'] = svc_pred_t","370e8bdc":"test.head()","abab1513":"test[['tweetid','sentiment']].to_csv('testsubmission.csv', index=False)","6211f739":"# Ignore warnings\nimport warnings\nwarnings.simplefilter(action='ignore')\n\n# Install Prerequisites\n# import sys\n# import nltk\n# !{sys.executable} -m pip install bs4 lxml wordcloud scikit-learn scikit-plot\n# nltk.download('vader_lexicon')\n\n# Exploratory Data Analysis\nimport re\nimport ast\nimport time\nimport nltk\nimport pickle\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\n#from textblob import TextBlob\nimport matplotlib.pyplot as plt\n#from wordcloud import WordCloud\nfrom nltk.sentiment import SentimentIntensityAnalyzer\n\n# Data Preprocessing\nimport string\nfrom bs4 import BeautifulSoup\nfrom collections import Counter\nfrom nltk.corpus import wordnet\nfrom nltk.corpus import stopwords\nfrom sklearn.utils import resample\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.tokenize import TweetTokenizer \nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n# Classification Models\nfrom sklearn.svm import LinearSVC, SVC\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\n\n# Performance Evaluation\nfrom sklearn.metrics import roc_curve, auc\nfrom sklearn.preprocessing import label_binarize\nfrom sklearn.model_selection import GridSearchCV\n#from scikitplot.metrics import plot_roc, plot_confusion_matrix\nfrom sklearn.multiclass import OneVsRestClassifier, OneVsOneClassifier\nfrom sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score, classification_report, confusion_matrix\n\n# Display\n%matplotlib inline\nsns.set(font_scale=1)\nsns.set_style(\"white\")","572ab919":"from google.colab import files \n  \n  \nuploaded = files.upload()","95eddf00":"train_data = pd.read_csv('train.csv')\ntest_data = pd.read_csv('test.csv')\n#train_data = pd.read_csv('\/train.csv')\n#test_data = pd.read_csv('\/test.csv')\ndf_train = train_data.copy() #For EDA on raw data\ndf_test = test_data.copy()","179e0dbd":"train_data.head()","6cad2964":"test_data.head()","e1caf077":"# Final Cleaning\ndef sentiment_changer(df):\n    \"\"\"\n    Change key words to reflect the general sentiment associated with it.\n    \"\"\"\n    df['message'] = df['message'].apply(lambda x: x.replace('global', 'negative'))\n    df['message'] = df['message'].apply(lambda x: x.replace('climate', 'positive'))\n    df['message'] = df['message'].apply(lambda x: x.replace('MAGA', 'negative'))\n    return df['message']\n\ntrain_data['message'] = sentiment_changer(train_data)\ntest_data['message'] = sentiment_changer(test_data)\n\ndef clean(df):\n    \"\"\"\n    Apply data cleaning steps to raw data.\n    \"\"\"\n    df['token'] = df['message'].apply(TweetTokenizer().tokenize) ## first we tokenize\n    df['punc'] = df['token'].apply(lambda x : [i for i in x if i not in string.punctuation])## remove punctuations\n    df['dig'] = df['punc'].apply(lambda x: [i for i in x if i not in list(string.digits)]) ## remove digits\n    df['final'] = df['dig'].apply(lambda x: [i for i in x if len(i) > 1]) ## remove all words with only 1 character\n    return df['final']\n\ntrain_data['final'] = clean(train_data)\ntest_data['final'] = clean(test_data)","9041079e":"import nltk\nnltk.download('wordnet')","0e455457":"def get_part_of_speech(word):\n    \"\"\"\n    Find part of speech of word if part of speech is either noun, verb, adjective etc and add it to a list.\n    \"\"\"\n    probable_part_of_speech = wordnet.synsets(word) ## finding word that is most similar (synonyms) for semantic reasoning\n    pos_counts = Counter() ## instantiating our counter class\n    pos_counts[\"n\"] = len([i for i in probable_part_of_speech if i.pos()==\"n\"])\n    pos_counts[\"v\"] = len([i for i in probable_part_of_speech if i.pos()==\"v\"])\n    pos_counts[\"a\"] = len([i for i in probable_part_of_speech if i.pos()==\"a\"])\n    pos_counts[\"r\"] = len([i for i in probable_part_of_speech if i.pos()==\"r\"])\n    most_likely_part_of_speech = pos_counts.most_common(1)[0][0] ## will extract the most likely part of speech from the list\n    return most_likely_part_of_speech\n\nnormalizer = WordNetLemmatizer()\n\ntrain_data['final'] = train_data['final'].apply(lambda x: [normalizer.lemmatize(token, get_part_of_speech(token)) for token in x])\ntest_data['final'] = test_data['final'].apply(lambda x: [normalizer.lemmatize(token, get_part_of_speech(token)) for token in x])","988d6c28":"X = train_data['final']\ny = train_data['sentiment']\nX_test = test_data['final']\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.1, random_state = 42)","2d95e88d":"X_train = list(X_train.apply(' '.join))\nX_val = list(X_val.apply(' '.join))\n\nvectorizer = TfidfVectorizer(sublinear_tf=True, smooth_idf = True, max_df = 0.3, min_df = 5, ngram_range = (1, 2))\nvectorizer.fit(X_train)\n\n# vect_save_path = \"TfidfVectorizer.pkl\"\n# with open(vect_save_path,'wb') as file:\n#     pickle.dump(vectorizer,file)\n\nX_train = vectorizer.transform(X_train)\nX_val = vectorizer.transform(X_val)","091bc18e":"modelstart = time.time()\nlogreg = LogisticRegression(C=1000, multi_class='ovr', solver='saga', random_state=42, max_iter=10)\nlogreg.fit(X_train, y_train)\ny_pred = logreg.predict(X_val)\nlogreg_f1 = round(f1_score(y_val, y_pred, average='weighted'),2)\nprint('Accuracy %s' % accuracy_score(y_pred, y_val))\nprint(\"Model Runtime: %0.2f seconds\"%((time.time() - modelstart)))\nreport = classification_report(y_val, y_pred, output_dict=True)\nresults = pd.DataFrame(report).transpose()\nresults","534916b4":"modelstart= time.time()\nmultinb = MultinomialNB()\nmultinb.fit(X_train, y_train)\ny_pred = multinb.predict(X_val)\nmultinb_f1 = round(f1_score(y_val, y_pred, average='weighted'),2)\nprint('Accuracy %s' % accuracy_score(y_pred, y_val))\nprint(\"Model Runtime: %0.2f seconds\"%((time.time() - modelstart)))\nreport = classification_report(y_val, y_pred, output_dict=True)\nresults = pd.DataFrame(report).transpose()\n# results.to_csv(\"multinb_report.csv\")\nresults","f100f0c8":"modelstart = time.time()\nrf = RandomForestClassifier(max_features=4, random_state=42)\nrf.fit(X_train, y_train)\ny_pred = rf.predict(X_val)\nrf_f1 = round(f1_score(y_val, y_pred, average='weighted'),2)\nprint('Accuracy %s' % accuracy_score(y_pred, y_val))\nprint(\"Model Runtime: %0.2f seconds\"%((time.time() - modelstart)))\nreport = classification_report(y_val, y_pred, output_dict=True)\npd.DataFrame(report).transpose()","84aa5771":"modelstart = time.time()\nsvc = SVC(gamma = 0.8, C = 10, random_state=42)\nsvc.fit(X_train, y_train)\ny_pred = svc.predict(X_val)\nsvc_f1 = round(f1_score(y_val, y_pred, average='weighted'),2)\nprint('Accuracy %s' % accuracy_score(y_pred, y_val))\nprint(\"Model Runtime: %0.2f seconds\"%((time.time() - modelstart)))\nreport = classification_report(y_val, y_pred, output_dict=True)\nresults = pd.DataFrame(report).transpose()\nresults","70d7fd1a":"name = 'svm.pkl'\n\nwith open (name, 'wb') as file:\n  ","f8b9df37":"modelstart = time.time() \nlinsvc = LinearSVC()\nlinsvc.fit(X_train, y_train)\ny_pred = linsvc.predict(X_val)\nlinsvc_f1 = round(f1_score(y_val, y_pred, average='weighted'),2)\nprint('Accuracy %s' % accuracy_score(y_pred, y_val))\nprint(\"Model Runtime: %0.2f seconds\"%((time.time() - modelstart)))\nreport = classification_report(y_val, y_pred, output_dict=True)\nresults = pd.DataFrame(report).transpose()\nresults","919c981f":"modelstart = time.time()\nkn = KNeighborsClassifier(n_neighbors=1)\nkn.fit(X_train, y_train)\ny_pred = kn.predict(X_val)\nkn_f1 = round(f1_score(y_val, y_pred, average='weighted'),2)\nprint('accuracy %s' % accuracy_score(y_pred, y_val))\nprint(\"Model Runtime: %0.2f seconds\"%((time.time() - modelstart)))\nreport = classification_report(y_val, y_pred, output_dict=True)\nresults = pd.DataFrame(report).transpose()\nresults","72151698":"modelstart = time.time()\ndt = DecisionTreeClassifier(random_state=42)    \ndt.fit(X_train, y_train)\ny_pred = dt.predict(X_val)\ndt_f1 = round(f1_score(y_val, y_pred, average='weighted'),2)\nprint('accuracy %s' % accuracy_score(y_pred, y_val))\nprint(\"Model Runtime: %0.2f seconds\"%((time.time() - modelstart)))\nreport = classification_report(y_val, y_pred, output_dict=True)\npd.DataFrame(report).transpose()","383acb24":"modelstart = time.time()\nad = AdaBoostClassifier(random_state=42)\nad.fit(X_train, y_train)\ny_pred = ad.predict(X_val)\nad_f1 = round(f1_score(y_val, y_pred, average='weighted'),2)\nprint('accuracy %s' % accuracy_score(y_pred, y_val))\nprint(\"Model Runtime: %0.2f seconds\"%((time.time() - modelstart)))\nreport = classification_report(y_val, y_pred, output_dict=True)\npd.DataFrame(report).transpose()","486efab6":"# Compare Weighted F1-Scores Between Models\nfig,axis = plt.subplots(figsize=(10, 5))\nrmse_x = ['Multinomial Naive Bayes','Logistic Regression','Random Forest Classifier','Support Vector Classifier','Linear SVC','K Neighbours Classifier','Decision Tree Classifier','AdaBoost Classifier']\nrmse_y = [multinb_f1,logreg_f1,rf_f1,svc_f1,linsvc_f1,kn_f1,dt_f1,ad_f1]\nax = sns.barplot(x=rmse_x, y=rmse_y,palette='winter')\nplt.title('Weighted F1-Score Per Classification Model',fontsize=14)\nplt.xticks(rotation=90)\nplt.ylabel('Weighted F1-Score')\nfor p in ax.patches:\n    ax.text(p.get_x() + p.get_width()\/2, p.get_y() + p.get_height(), round(p.get_height(),2), fontsize=12, ha=\"center\", va='bottom')\n    \nplt.show()","db4b1157":"LogisticRegression().get_params()","55ef86ea":"param_grid = {'C': [1000], #[100,1000]\n              'max_iter': [10], #[10,100]\n              'multi_class': ['ovr'], #['ovr', 'multinomial']\n              'random_state': [42],\n              'solver': ['saga']} #['saga','lbfgs']\ngrid_LR = GridSearchCV(LogisticRegression(), param_grid, scoring='f1_weighted', cv=5, n_jobs=-1)\ngrid_LR.fit(X_train, y_train)\ny_pred = grid_LR.predict(X_val)\nprint(\"Best parameters:\")\nprint(grid_LR.best_params_)\nprint('accuracy %s' % accuracy_score(y_pred, y_val))\nprint(classification_report(y_val, y_pred))","c3ac5d30":"LinearSVC().get_params()","bffa2685":"param_grid = {'C': [100],#[0.1,1,10,100,1000]\n              'max_iter': [10], #[10,100]\n              'multi_class' : ['ovr'], #['crammer_singer', 'ovr']\n              'random_state': [42]} \ngrid_LSVC = GridSearchCV(LinearSVC(), param_grid, scoring='f1_weighted', cv=5, n_jobs=-1)\ngrid_LSVC.fit(X_train, y_train)\ny_pred = grid_LSVC.predict(X_val)\nprint(grid_LSVC.best_params_)\nprint('accuracy %s' % accuracy_score(y_pred, y_val))\nprint(classification_report(y_val, y_pred))","2574a995":"SVC().get_params()","0da4a11d":"param_grid = {'C': [10],#[0.1,1,10,100,1000]\n              'gamma': [0.8], #[0.8,1]\n              'kernel': ['rbf'], #['linear','rbf']\n              'random_state': [42]} \ngrid_SVC = GridSearchCV(SVC(), param_grid, scoring='f1_weighted', cv=5, n_jobs=-1)\ngrid_SVC.fit(X_train, y_train)\ny_pred = grid_SVC.predict(X_val)\nprint(grid_SVC.best_params_)\nprint('accuracy %s' % accuracy_score(y_pred, y_val))\nprint(classification_report(y_val, y_pred))","421bb6bc":"y_pred = svc.predict(X_val)\nprint('accuracy %s' % accuracy_score(y_pred, y_val))\nprint(classification_report(y_val, y_pred))","7e2c2895":"# If model doesn't take \"probability=True\" as an argument (e.g. LinearSVC)\novr =  OneVsRestClassifier(SVC(random_state=42,class_weight='balanced'))\n\ny_train_binarized = label_binarize(y_train, classes=[-1, 0, 1, 2])\ny_val_binarized = label_binarize(y_val, classes=[-1, 0, 1, 2])\nn_classes = 4\n\novr.fit(X_train, y_train_binarized)\n\n# decision_function predicts a \u201csoft\u201d score for each sample in relation to each class, \n# rather than the \u201chard\u201d categorical prediction produced by predict. Its input is \n# usually only some observed data, X.\ny_probas = ovr.decision_function(X_val)\n\nplot_roc(y_val, y_probas,figsize=(8,8),cmap='winter_r')\nplt.show()","a94a5a8e":"# Make prediction on test data\nX = train_data['final']\ny = train_data['sentiment']\nX_test = test_data['final']\n\nX = list(X.apply(' '.join))\nX_test = list(X_test.apply(' '.join))\n\nvectorizer = TfidfVectorizer(sublinear_tf=True, smooth_idf = True, max_df = 0.3, min_df = 5, ngram_range = (1, 2))\nvectorizer.fit(X)\n\nX = vectorizer.transform(X)\nX_test = vectorizer.transform(X_test)\n\nsvc = SVC(gamma=0.8, C=10, random_state=42)\nsvc.fit(X, y)\ny_test = svc.predict(X_test)","7e34d3d8":"# Number of Tweets Per Sentiment Class\nfig, axis = plt.subplots(ncols=2, figsize=(15, 5))\n\nax = sns.countplot(y_test,palette='winter',ax=axis[0])\naxis[0].set_title('Number of Tweets Per Sentiment Class',fontsize=14)\naxis[0].set_xlabel('Sentiment Class')\naxis[0].set_ylabel('Tweets')\nfor p in ax.patches:\n    ax.text(p.get_x() + p.get_width()\/2., p.get_height(), '%d' % int(p.get_height()), fontsize=11, ha='center', va='bottom')\n\nresults = pd.DataFrame({\"tweetid\":test_data['tweetid'],\"sentiment\": y_test})\nresults['sentiment'].value_counts().plot.pie(autopct='%1.1f%%',colormap='winter_r',ax=axis[1])\naxis[1].set_title('Proportion of Tweets Per Sentiment Class',fontsize=14)\naxis[1].set_ylabel('Sentiment Class')\n    \nplt.show()","eee2afc9":"# Create Kaggle Submission File\nresults = pd.DataFrame({\"tweetid\":test_data['tweetid'],\"sentiment\": y_test})\nresults.to_csv(\"Team2_final_submission.csv\", index=False)","6e0b24da":"### 6.1.3 Decision Tree Classifier","88de373e":"### 4.3.1 Random Forest Classifier","040b0255":"## 2.1 The distribution of climate change sentiments \n\n\n\n\nUnderstanding the distribution of sentiments surrounding climate change on Twitter communicates that there are different views on climate change hence the different classes associated with these views\/sentiments.","ef679422":"## Decision Tree Classifier","55f8f397":"### 6.1.1 Random Forest Classifier","c7b1e47f":"1. Installations and Imports\n2. Explore Data Analysis\n3. Data Preprocessing\n4. Model Building\n5. Model Evaluation\n6. Submition\n7. Conclusion\n8. References","6225c62e":"### 1.3 Import Data","540f88e1":"### The length of tweets per sentiment class","3507b9bf":"## Support Vector Classifier","8260c757":"## Lemmatisation","c150fa2d":"adding a sentiment column to our original test df","8ad64f7b":"# 5. Model Building","4e86a533":"### Results","a683c919":"Preprocessing involves the elimination of trivial or less informative data, which does not contribute to the sentiment classification. To understand the process of eliminating less informed data, it is important to understand what matters in sentiment analysis. Words are the most important part, however, when it comes to things like punctuation, you cannot get the sentiment from punctuation. Therefore, punctuation does not matter in sentiment analysis. In addition, tweet elements such as images, videos, URLs, usernames, emojis do not contribute to the polarity of the tweet (whether positive or negative). However, this is only true for machine learning models.\n\n**Techniques that we are going to use to clean our data**\n\n- Removing Noise\n- Stop Words\n- Tokenisation\n- Lemmatisation Normalization\n","2320a53f":"#### Linear SVC","252ff5fc":"TFIDF assigns word frequency scores, these scores try to highlight words of greater interest. The TFIDFVectorizer will tokenize the documents, learn the vocabulary and inverse document frequency weightings, and allow you to encode new documents.","c7df2ae3":"## 6.2 Classification Report","3cfb5094":"### 2.4.2 The top 10 influencial Twitter accounts per Sentiment Class\n\nThe accounts that recieved the most mentions are Twitter accounts that have engaged with the climate change topic.Twitter users mention these accounts when reposting(retweeting) the twitter accounts sentiment on climate change or responding to the twitter accounts comment on climate change.Within the data these Twitter accounts have played a vital role in fueling the climate change debate on Twitter.","22772d7e":"# v. Save Output","278ee080":"The simplest way to transform text documents into numerical data, known as Bag of Words, is to consider a number of features equal to the number of words in the corpus. Then, for each document, the value of each feature is given by the number of occurrences of that word within the current document.","02870e46":"# Climate Change Belief Analysis\n**Team 2 JHB July 2020**\n\n","1e1e45c7":"## 4.2 Splitting out the X variable from the target","b808b8bf":"Adding a column of the tweets length\/character count to the data","2f889520":"In text analysis, eliminating noise  is the most important part of getting the data into usable format. \n\nWe will remove noise with the following steps.\n- Convert letters to lowercases\n- Remove URL links \n- Remove hashtag\/numbers\n- Remove punctuation","b570219f":"### 3.2.4 Lemmatisation","414d944b":"# 4. Text Feature Extraction","980d3e54":"### 6.1.2 Logistic Classifier","75c80026":"## K Neighbours Classifier","5d4df8f0":"# 2.5 The key findings from the EDA\n\n* There are polarised views on climate change on twitter\n\n* Within the data there exists a class imbalance,this will be considered in the preprocessing and model training section\n\n* An analysis of the hashtags has shown that the tweets in class 1 believe in climate change,class 2 believe and inform people about climate change,class 0 are more neutral and tend to downplay the existence of climate change and class -1 do not believe that climate change exists.\n\n","428efec3":"The keyword that is used the most when discussing climate change is #climate followed by #climatechange.#Trump is a prominent hashtag in class 0 as well.Donald Trump's views on climate change is discussed in the class.An interesting hashtag used by people is #BeforeTheFlood which is a movie that depicts the impacts of climate change on the Earth,as well as #amreading people use this hashtage to tell mention what they a book or article they are currently reading. The sentiments within class 0 are open conversations surrounding climate change including people asking questions about climate change as well as sarcasm.","863f5f0f":"## 2.3 The distribution of the tweets length in the data","5d2edf03":"We will load our data as a Pandas DataFrame","8a179673":"### 6.2.2 Logistic Classifier","20ee55f3":"### 4.3.3 Decision Tree Classifier","6ed381da":"As seen in the bar graph, sentiment class 1 has the highest number of tweets in the train data accounting for 8530 tweets(53.92%).The lowest sentiment class is class -1 which accounts for 1296 tweets (8.19%).The distribution of sentiments classes are imbalanced because the classes do not have the same ammount of tweets in their class as seen in dataframe which compares the value counts and percentage of each sentiment class.\n\nThe class imbalance of the training data has an impact on the classification made on the unseen data (testing data) in the modeling phase.A class imbalance could result in the model classifying most of the tweets into sentiment class 1 since the model gets better a classifying class 1 tweets as the model has more evidence  of class 1 tweets.This will be taken into consideration in the preprocessing and modeling section of the notebook.","4dbc0555":"# ii. Modelling","3d85919d":"# 1. Installations and Imports","84f3d654":"**Logistic Regression**","a0375f53":"### 5.1 Splitting the training data into a training and validation set","acb24dba":"# 7. Submitions","6fcd9a85":"## Split Training and Validation Sets","e1ae35c0":"# The Based Model ","223c1bd5":"### 3.2.3 Tokenisation","f9691582":"### 4.3.2 Logistic Classifier","59d874b4":"### 6.2.1 Random Forest Classifier","fa13e89b":"### 6.3.3 Decision Tree Classifier","8f85fe96":"#### 2.4.3.4 Top 10 hashtags used in Sentiment class 2 tweets","a4f5161b":"The tweets length in the train data lie between 208 characters and 14 characters.The average length of tweets is 123 characters.The longest tweet on climate change in the train data contrains 208 words.The longest tweet stands out from the average length of tweets on climate change which is 123 words.The cell illustrates that the tweet with the most words is simply made up of only a few actual words this will be taken into consideration in the preprocessing section of the notebook to ensure that any noise in the tweets are removed.","7cedf6a2":"#### 2.4.3.3 Top 10 hashtags used in Sentiment class 1 tweets","a133b9b1":"### 6.3.4 Support vector machine Classifier","eea3301b":"Random forests is a supervised learning algorithm. Random forests creates decision trees on randomly selected data samples, gets prediction from each tree and selects the best solution by means of voting. It also provides a pretty good indicator of the feature importance. It is said that the more trees it has, the more robust a forest is. ","03b31e53":"Before we begin with data cleaning we created copies of the dataframe which allows us to make some changes without changing the original dataframe","885af3bf":"We used a TF-IDF vectorizer to compute a weight for each word token by its level of importance and vectorize it and we used a radial basis function support vector classifier (SVC) to train our model. After a bit of hyperparameter tuning, we found the following parameters to work well: {'C': 10, 'gamma': 0.8, 'kernel': 'rbf', 'random_state': 42}. A token pattern of alphanumeric words performed best and since the average tweet has around 17 words, an n-gram of 1 to 2 performs best in capturing semantic meaning. The SVC parameters were chosen because the radial basis function performs better than a Linear SVC at splitting up the areas in which the different semantic lies. This is possibly due to the fact that the classification is not binary.","513445ba":"### 6.2.3 Decision Tree Classifier","d1e16b3e":"The AdaBoost classifier is an iterative ensemble method that begins by fitting a classifier on the original dataset and then fits additional copies of the classifier on the same dataset. In the second step, the weights of incorrectly classified instances are adjusted such that subsequent classifiers focus more on difficult cases.","23b678b0":"Training data: Data that contains a known label. The model is trained on this data to be able to generalize unlabeled data.\nValidation data: A subset of the training data that is used to assess how well the algorithm was trained on the training data.\nTest data: Data that is used to provide an unbiased evaluation of the final model fit on the training dataset.","0d68c267":"1. https:\/\/towardsdatascience.com\/tokenization-for-natural-language-processing-a179a891bad4\n2. https:\/\/towardsdatascience.com\/machine-learning-text-processing-1d5a2d638958\n3. ","4bc50d63":"# Notebook outline","12afe617":"### 5.2 Model Fitting","4553bd7b":"# 8. References","813d280e":"#### Combining Both Random Sampling Techniques\n\nCombining both random sampling methods can occasionally result in overall improved performance in comparison to the methods being performed in isolation. In this predict we will balance our data by using both methods oversampling and undersampling method. The class size is determined by the average of data points. If a class is less than the average the class will be upsampled and if the class is greater than the average, then the class will be downsampled.","2c0c0d7a":"### 2.4.3 An analysis of the Hashtags used  per sentiment class\n\nA hashtags is written using the '#' symbol.Its main function is to categorize tweets based on a keyword or a topic associated with the hashtag used. According to the 'Twitter Help Center' website people use hashtags before a relevant phrase or keyword. \n\nThe hashtags used in the climate change tweets highlight the people's interest in the climate change topic.The hashtags that were used communicate that people have divided opinions on climate change.This is relfected in the hashtags used within each sentiment class.  ","ceb9645f":"In class -1 the hashtag that was used the most is #MAGA and the second highest being #climate.These keywords were the most used when people were discussing their sentiments concerning climate change.Other interesting hashtags that form part of the top ten hashtags used in class one are #fakenews and #ClimateScam which insinuate that some of the people who were tweeting about climate change believe that is is simply fake news or a scam. The third highest hashtag used is #Trump when discussing climate change. The class focuses more on discussing climate change as being linked to politics hence the hashtag that has been used the most is #MAGA as well as the example of one of the tweets provided in the cell above.","b3277cb9":"The training data set was split into a training set and an evaluation set. The evaluation set will be used to evaluate the model before evaluated using the test dataset.","05147463":"Decision tree machine learning models represent data by partitioning it into different sections based on questions asked of independent variables in the data. Training data is placed at the root node and is then partitioned into smaller subsets which form the 'branches' of the tree.","8422f817":"The training data set is split into training and validation dataset. A validation dataset is a sample of data held back from training the model and is used to give an estimate of model skill while tuning the model\u2019s hyperparameters. The validation dataset is different from the test dataset that is also held back from the training of the model but is instead used to give an unbiased estimate of the skill of the final tuned model when comparing or selecting between final models.","68aba823":"A Support Vector Classifier is a discriminative classifier formally defined by a separating hyperplane. When labelled training data is passed to the model, also known as supervised learning, the algorithm outputs an optimal hyperplane which categorizes new data.","a560c1f2":"## 5.2.1 Data tranformation with Vectorizer","ad4fa1ca":"From the performance metrics, we see that the **Support Vector Classifier** performed the best on our validation set, closely followed by the **Linear SVC** and **Logistic Regression** models. The K Neighbours Classifier significantly performed the worst, which may be due to the k value that was selected for the model. To ensure that we get a robust measure of classifier performance, we will apply cross validation and hyperparameter tuning on the top three performing models.","492f347d":"### ROC Curves and AUC\nROC curves show the trade-off between sensitivity and specificity of a classification model. Classifiers that produce curves closer to the top-left corner indicate a better performance. The area under the ROC curve (AUC), is equivalent to the probability that a randomly chosen positive instance is ranked higher than a randomly chosen negative instance. High AUC values are a general measure of good predictive accuracy.","0fba896d":"### Resampling\nWe addressed the problem of imbalanced training data by resampling the data before building our models. A class size was determined based on the second largest sentiment class and other classes were either upsampled or downsampled according to the class size. However, resampling the data did not improve the performance of the models and we therefore excluded it.","43666b6e":"#### Support Vector Classifier","ceee1cb8":"The opinions on climate change in class 1 shift towards climate change does exist as the conversations in this class discuss a movie called Before the flood.The movie highlights the impact of climate change on the Earth.As well as using the hashtag  #ActOnClimate, the tweets associated with the hastag on Twitter mainly discuss ways to combat climate change (http:\/\/www.tweepy.net\/hashtag\/ActOnClimate). ","aad8212d":"The stop words are the most common words like \"if\", \"but\", \"we\", \"he\", \"she\" and \"she\". We can usually remove these words without changing the semantics of any text, and doing so often (but not always) improves the performance of a model. Removing these stop words becomes much more useful when we use longer sequences of words as model features.","8f4c71fa":"Random forest models are an example of an ensemble method that is built on decision trees (i.e. it relies on aggregating the results of an ensemble of decision trees). Decision tree machine learning models represent data by partitioning it into different sections based on questions asked of independent variables in the data. Training data is placed at the root node and is then partitioned into smaller subsets which form the 'branches' of the tree. In random forest models, the trees are randomized and the model returns the mean prediction of all the individual trees.","67c9651f":"## 5.2 Data tranformation with TfidfVectorizer","86f7406c":"# 2. Exploratory Data Analysis (EDA)\n\nThe section is an exploration of the data through an analysis of the different Climate Change sentiments that people have on Twitter.\n\n**Techniques that we are going to use to analyse our data**\n\n- Understanding the distribution of sentiments\n- An analysis of the Tweets statistics\n- Understanding the length of our tweets\n- The main topics on climate change","9cf69058":"#### Performance Metrics","1f3605bf":"Test data","6c570b6c":"## 5.2 Model evaluation using test data","daeec066":"#### 2.4.3.1 Top 10 hashtags used in Sentiment class  -1 tweets","1a203695":"The objective of a Linear Support Vector Classifier is to return a \"best fit\" hyperplane that categorises the data. It is similar to SVC with the kernel parameter set to \u2019linear\u2019, but it is implemented in terms of liblinear rather than libsvm, so it has more flexibility in the choice of penalties and loss functions and can scale better to large numbers of samples.","d5b9332a":"## 4.1 Bag of words","3d353afc":"\n\n\n\n## 2.2 An overview of tweets statistics","17d44294":"# 5. Model Evaluation","b60f698a":"### 1.2 Imports","100a8a32":"### 2.4.1 Top 30 used words in the tweets","37270e71":"### 3.2.2 Removing Stop Words","44f7cd71":"Tweets that are part of sentiment class one have have the highest length frequency as compared to the other classes. ","03a7045a":"### 1.1 Installations","ca444f85":"### Hyperparameter Tuning of Best Models\n**Cross validation** is a technique used to test the accuracy of a model's prediction on unseen data (validation sets). This is important because it can assist in picking up issues such as over\/underfitting and selection bias. We used the K-fold technique to perform cross validation. \n\n**Hyperparameter tuning** is the process by which a set of ideal hyperparameters are chosen for a model. A hyperparameter is a parameter for which the value is set manually and tuned to control the algorithm's learning process.","09fe3a0d":"## Multinomial Naive Bayes","62917daa":"### 6.1.4 Support vector machine Classifier","aee67c91":"# 3. Data Preprocessing","5351a14c":"## 5.1 Model evaluation using validation data","6e4a8288":"In this section, we will be cover the process of building a base model starting from the preprocessing of data up to the model building and evaluation.","23ec9196":"# i. Data Cleaning","b78c69e7":"#### 2.4.3.2 Top 10 hashtags used in Sentiment class 0 tweets","587e4175":"### Performance Metrics of Best Models\n\nWe built and tested eight different classification models and compared their performance using a statistical measure known as the weighted F1 score, which takes into account the proportions of each class fed into the model. This is a weighted average of the precision and recall of the model and is the measure that will be used to test the accuracy of our Kaggle output. \n\n#### Precision\n\nWhen it predicts \"True\", how often is it correct? \n\n$$ Precision = \\frac{TP}{TP \\space + FP} = \\frac{TP}{Total \\space Predicted \\space Positive} $$\n\n#### Recall\n\nWhen the outcome is actually \"True\", how often do we predict it as such?\n\n$$ Recall = \\frac{TP}{TP \\space + FN} = \\frac{TP}{Total \\space Actual \\space Positive}$$\n\n#### F1 Score\n\nWeighted average of precision and recall. \n\n$$F_1 = 2 \\times \\frac {Precision \\space \\times \\space Recall }{Precision \\space + \\space Recall }$$","f1030ef5":"Tokenization is a process of breaking the raw text into small chunks. Tokenization breaks the raw text into words, sentences called tokens. A token is a piece of a whole, so a word is a token in a sentence, and a sentence is a token in a paragraph. Tokenization is the process of splitting a string into a list of tokens. These tokens help in understanding the context or developing the model for the NLP. The tokenization helps in interpreting the meaning of the text by analyzing the sequence of the words. For example, the text \u201cIt is raining\u201d can be tokenized into \u2018It\u2019, \u2018is\u2019, \u2018raining\u2019","ca640385":"# Introduction","ff9dfdfa":"## Linear SVC","79fc1f0e":" Train data","af97b19f":"### 6.3.1 Random Forest Classifier","59032090":"## 6.1 Confusion Matrix","d0db1d90":"The K Neighbours Classifier is a classifier that implements the k-nearest neighbours vote. In classification, the output is a class membership. An object is classified by a plurality vote of its neighbours, with the object being assigned to the class most common among its k-nearest neighbours.","18093c94":"The tweet in the test data are betweet 7 characters and 623 characters.On average the tweets in the test data are 123 characters.The longest tweets seem to have soe discrepency because twitter's word limit  on tweets in 280 characters however the longest tweet exceeds this limit.The longest tweet in the data is simply made up of only a few actual words this will be taken into consideration in the preprocessing section of the notebook to ensure that any noise in the tweets are removed ","18e85af3":"### 6.2.4 Support vector machine Classifier","40442bdc":"In this project, we succeeded in building a supervised machine learning model that is able to classify whether or not a person believes in climate change, based on their novel tweet data. Our top performing model has a weighted F1 score of 0.78, based on our validation set, and the results from our testing set are in line with what was observed in the training set. We think that it is possible that the number of Pro tweets is related to the fact that \"97% or more of actively publishing climate scientists agree: climate-warming trends over the past century are extremely likely due to human activities.\" ([Nasa](https:\/\/climate.nasa.gov\/scientific-consensus\/#*))\n\n**Impact investing** is an emerging field that refers to investments made into companies and organisations with the intention to generate measurable social or environmental impact alongside financial return. Many companies are built around lessening one\u2019s environmental impact or carbon footprint and they offer products and services that are environmentally friendly and sustainable, in line with their values and ideals. These companies would like to determine how people perceive climate change and whether or not they believe it is a real threat. Our model provides a valuable solution to this problem and can add to their market research efforts in gauging how their product or service may be received. It gives companies access to a broad base of consumer sentiment, spanning multiple demographic and geographic categories, thus increasing their insights and informing future marketing strategies.\n\nFrom our exploratory data analysis, we can draw some marketing-related insights. For maximum reach in marketing campaigns that target a specific group of people that have a certain stance with regard to climate change, a marketing team can consider the following:\n\n\n\n\n  \nThe rise of impact investment has caused companies to focus on generating a positive social and environmental impact in addition to financial returns. It would assist companies to ally their brand and products with the Pro climate change movement. Pro climate change tweets tend to have a wider reach than other classes. Not only is it an ethical stance but it has potential to increase exposure of the brand on Twitter. Their tweets could be used to add their voice to the fight against global warming and thus be expressed as a negative sentiment or possibly nuetral. This could maximize their reach even further and also introduces other considerations, such as financial rewards due to carbon taxes.\n\n\nTwitter hashtags are a powerful tool that companies can use to reach a larger audience,this can be achieved by engaging in the conversations on climate change by utilising the hashtags that the users that are Pro-Climate change use.This will communicate the company's commitment to man-made climate change.The image of a company plays a vital role in differentiating a company from its competitors as it communicates who it caters to. This minor yet impactful act of using pro-climate change hashtags will improve the image of the business.","9158eb23":"Logistic regression is a statistical model that makes use of a logistic function to model a binary dependent variable, however, multiclass classification with logistic regression can be done through the one-vs-rest scheme in which a separate model is trained for each class to predict whether an observation is that class or not (thus making it a binary classification problem).","77534e0d":"## logistic regression","ef24ba81":"Checking for null values in the data","f5b45d4d":"## Random Forest Classifier","6dfabb0a":"### 4.3.4 Support vector machine Classifier","b50272b8":"### 3.2.1 Removing Noise","a9d232e4":"## iii. Model Performance","ef645ab2":"### Background  \n\nIn a [research article](https:\/\/www.barrons.com\/articles\/two-thirds-of-north-americans-prefer-eco-friendly-brands-study-finds-51578661728) conducted, 19,000 customers from 28 countries where given a poll to find out how individual shopping decisions are changing. Nearly 70% of consumers in the U.S. and Canada find that it is important for a company or brand to be sustainable or eco-friendly. More than a third (40%) of the respondents globally said that they are purpose-driven consumers, who select brands based on how well they align with their personal beliefs.\n\nMany companies are built around lessening their environmental impact or carbon footprint. They offer products and services that are environmentally friendly and sustainable, in line with their values and ideals. They would like to determine how people perceive climate change and whether or not they believe it is a real threat. This would add to their market research efforts in gauging how their product\/service may be received.  \n\nThe goal of this challenge is to build a Classification Machine Learning model that will determine whether a person believes in Climate Change using tweet data. This model will provide insights of public opinion of Climate Change & consumer sentiment to companies looking to market their new or improved products or services to consumers, in response to CER.\n\nAs the demand for sustainable, eco-friendly products and services by consumers increases, a sentiment classification model that identifies these potential customers is key and could be used any business or organisation committed to carbon neutrality & wanting to inform marketing strategies. This includes, but is not limited to companies in the retail, automotive, government, agriculture & food, pharmaceutical spheres. The model could also be used by sectors in government wanting to identify the various belief sentiments in order to better direct environmental awareness and education campaigns in alignment with their legislative directives and climate change response plans.\n\n\n### Problem statement  \n\nBuild a machine learning model that is able to classify whether or not an individual believes in man-made climate change based on historical tweet data to increase insights about customers and inform future marketing strategies.\n\nYou can find the project overview [here](https:\/\/www.kaggle.com\/c\/climate-change-edsa2020-21).","57196d12":"### 6.3.2 Logistic Classifier","3c5f8143":"The TfidfVectorizer transforms text to feature vectors that can be used as input to a classification model.","283e21d3":"Creating an output csv for submission","c3d0b59e":"# iv. Model Analysis","5b209fe1":"We ","3d83f91a":"Logistic Regression is a supervised machine learning classification algorithm that is used to predict the probability of a categorical dependent variable. In logistic regression, the dependent variable is a binary variable that contains data coded as 1 (yes, success, etc.) or 0 (no, failure, etc.). In other words, the logistic regression model predicts P(Y=1) as a function of X. Logistic regression is good for binary classes, but in our case there is more than two classes. One-vs-rest(or OvR) approach will be used to combine the logistic regression models. In the OvR case, a separate logistic regression model has trained for each label that the response variable takes on.","538c9212":"# 8. Tuned and Improved Model","1dee63cc":"Lemmatisation aims to remove inflectional word endings to return the base or dictionary form of a word, also known as \"lemma\". We used the WordNetLemmatizer() from nltk, as well as by way of applying part of speech.","c9bf186f":"\n## 2.4 The main topics surrounding the climate change tweets\n\nAn understanding of the main topics dicussed in the climate change discussion on twitter is essential as it illustrates the sentiments attatched to climate change. This is done through extracting the most frequently used words and hashtags.","cbeb5838":"## AdaBoost Classifier","18844d3a":"# 6. Model Analysis","613ddc87":"## 5.2.2 Making predictions on the test set ","61e887c1":"The base model will be evaluated using the validation dataset that was kept aside from the training data.  After a test dataset will be used to make predictions. That will help us to understand whether we are overfitting our model or not.","f6ed6f7d":"The opinions in class one mainly focus on the climate this is evident in the high hashtag count of the word #climate, the second highest is #enviroment .The class is mainly focused on informing people about climate change and its effect on the enviroment.","903b3176":"###3.1 Dealing with Class Imbalance - Resampling","8ebffa90":"## 6.3 Overall f1-score","97d260a8":"Import python libraries","c0c2f384":"The EDA highlighted that there is a class imbalance within the data.In training classification model, it is preferable for all classes to have a relatively even split of observations. However, in the wild, classification datasets often come with unevenly distributed observations with one class or set of classes having way more observations than others.This will negatively affecting the accuracy score of the model. Therefore resampling is necessary before training a model with this data.\n\nResampling methods aim at modifying the dataset in order to reduce the discrepancy among the sizes of the classes. In this regard, two scenarios are proposed: one that eliminates instances from the majority class - called undersampling, and one that generates instances for the minority class - called over-sampling. They both have there pros and cons.In other words, Both oversampling and undersampling involve introducing a bias to select more samples from one class than from another, to compensate for an imbalance that is either already present in the data, or likely to develop if a purely random sample were taken. Pykes mentined that \"the random oversampling may increase the likelihood of overfitting occurring since it makes exact copies of the minority class examples. In this way, a symbolic classifier, for instance, might construct rules that are apparently accurate, but actually cove one replicated example\" and \u201cIn random under-sampling (potentially), vast quantities of data are discarded. This can be highly problematic, as the loss of such data can make the decision boundary between the minority and majority instances harder to learn, resulting in a loss in classification performance.\u201d","8669c75b":"## Feature Extraction","78480935":"The Multinomial Naive Bayes model estimates the conditional probability of a particular feature given a class and uses a multinomial distribution for each of the features. The model assumes that each feature makes an independent and equal contribution to the outcome.","276110d0":"# 8. Conclusion","27b21739":"## 3.2 Text Cleaning"}}