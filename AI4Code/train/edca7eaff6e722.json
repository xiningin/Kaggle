{"cell_type":{"f008e753":"code","89e8b3c0":"code","96b55aba":"code","7b6955f5":"code","eee03d7e":"code","9cd6a9e4":"code","6c964542":"code","f932b10d":"code","1822a027":"code","b2d5e9c5":"code","30d510e8":"code","15410e55":"code","e12b446b":"code","7d5925c8":"code","c74977ec":"code","74f13620":"code","e2208e95":"code","e8b99888":"code","2d05ba19":"code","860c0fb3":"code","2f266a64":"code","de1bf2d0":"code","4258d350":"code","fcab5591":"code","dc18be9b":"code","fd6685d4":"code","176a4fcb":"code","45d247fd":"code","a91847f3":"code","1a86f6e3":"code","9eabaada":"code","b319b3a7":"code","ce3abb1c":"code","e6ece8a2":"code","39bbd018":"code","1d8ce617":"code","69718cdc":"code","3b83535b":"code","68252645":"code","374d9f0a":"code","9d33eca3":"code","d300ec96":"code","0d2e7581":"code","4ea08b88":"code","91616f05":"code","6eb1087e":"markdown","1cc0b13e":"markdown","9652dd16":"markdown","061a5b03":"markdown","eb665692":"markdown","1eee7ab2":"markdown","f99eb147":"markdown","71432216":"markdown","3f6f9a1b":"markdown","0f15fcf8":"markdown"},"source":{"f008e753":"# !curl https:\/\/raw.githubusercontent.com\/pytorch\/xla\/master\/contrib\/scripts\/env-setup.py -o pytorch-xla-env-setup.py\n# !python pytorch-xla-env-setup.py --version nightly --apt-packages libomp5 libopenblas-dev","89e8b3c0":"import warnings\nimport torch_xla\nimport torch_xla.debug.metrics as met\nimport torch_xla.distributed.parallel_loader as pl\nimport torch_xla.utils.utils as xu\nimport torch_xla.core.xla_model as xm\nimport torch_xla.distributed.xla_multiprocessing as xmp\nimport torch_xla.test.test_utils as test_utils\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","96b55aba":"import os\nos.environ['XLA_USE_BF16']=\"1\"\nos.environ['XLA_TENSOR_ALLOCATOR_MAXSIZE'] = '100000000'\n\nimport torch\nimport pandas as pd\nfrom scipy import stats\nimport numpy as np\n\nfrom tqdm import tqdm\nfrom collections import OrderedDict, namedtuple\nimport torch.nn as nn\nfrom torch.optim import lr_scheduler\nimport joblib\n\nimport logging\nimport transformers\nfrom transformers import AdamW, get_linear_schedule_with_warmup, get_constant_schedule\nfrom transformers import AutoTokenizer, AutoModel, AutoConfig\nimport sys\nfrom sklearn import metrics, model_selection\nfrom fastai.text import *","7b6955f5":"INPUT_DATA_PATH = Path(\"\/kaggle\/input\/xlmrobertabase\/xlm_roberta_large_processed\/\"); INPUT_DATA_PATH.ls()","eee03d7e":"train_inputs = pd.read_pickle(INPUT_DATA_PATH\/\"translated_inputs.pkl\")\nvalid_inputs = pd.read_pickle(INPUT_DATA_PATH\/\"valid_inputs.pkl\")\ntest_inputs = pd.read_pickle(INPUT_DATA_PATH\/\"test_inputs.pkl\")","9cd6a9e4":"class JIGSAWDataset(Dataset):\n    def __init__(self, inputs, tokenizer=None, is_test=False, do_tfms:Dict=None, pseudo_inputs=None):\n\n        # eval\n        self.inputs = inputs\n\n        # augmentation\n        self.is_test = is_test\n        self.tokenizer = tokenizer\n        self.do_tfms = do_tfms\n        self.pseudo_inputs = pseudo_inputs\n        if self.pseudo_inputs: self.pseudo_idxs = list(range(len(self.pseudo_inputs)))\n\n    def __getitem__(self, i):\n        'fastai requires (xb, yb) to return'\n\n        input_ids = tensor(self.inputs['input_ids'][i])\n        attention_mask = tensor(self.inputs['attention_mask'][i])\n\n        if not self.is_test:\n            toxic = self.inputs['toxic'][i]\n\n#             if self.do_tfms:\n#                 if self.pseudo_inputs and (np.random.uniform() < self.do_tfms[\"random_replace_with_pseudo\"][\"p\"]):\n#                     rand_idx = np.random.choice(self.pseudo_idxs)\n\n#                     input_ids = tensor(self.pseudo_inputs[rand_idx]['input_ids'])\n#                     attention_mask = tensor(self.pseudo_inputs[rand_idx]['attention_mask'])\n#                     start_position, end_position = self.pseudo_inputs[rand_idx]['start_end_tok_idxs']\n#                     start_position, end_position = tensor(start_position), tensor(end_position)\n\n#                 else:\n#                     augmentor = TSEDataAugmentor(self.tokenizer,\n#                              input_ids,\n#                              attention_mask,\n#                              start_position, end_position)\n\n#                     if np.random.uniform() < self.do_tfms[\"random_left_truncate\"][\"p\"]:\n#                         augmentor.random_left_truncate()\n#                     if np.random.uniform() < self.do_tfms[\"random_right_truncate\"][\"p\"]:\n#                         augmentor.random_right_truncate()\n#                     if np.random.uniform() < self.do_tfms[\"random_replace_with_mask\"][\"p\"]:\n#                         augmentor.random_replace_with_mask(self.do_tfms[\"random_replace_with_mask\"][\"mask_p\"])\n\n#                     input_ids = augmentor.input_ids\n#                     attention_mask = augmentor.attention_mask\n#                     start_position, end_position = tensor(augmentor.ans_start_pos), tensor(augmentor.ans_end_pos)\n\n\n        xb = (input_ids, attention_mask)\n        if not self.is_test: yb = toxic\n        else: yb = 0\n\n        return xb, yb\n\n    def __len__(self): return len(self.inputs)","6c964542":"train_ds = JIGSAWDataset(train_inputs)\nvalid_ds = JIGSAWDataset(valid_inputs)\ntest_ds = JIGSAWDataset(test_inputs)","f932b10d":"train_ds[0]","1822a027":"class JigsawArrayDataset(torch.utils.data.Dataset):\n    def __init__(self, input_ids:np.array, attention_mask:np.array, toxic:np.array=None):\n        self.input_ids = input_ids\n        self.attention_mask = attention_mask\n        self.toxic = toxic\n    \n    def __getitem__(self, idx):\n        xb = (tensor(self.input_ids[idx]), tensor(self.attention_mask[idx]))\n        yb = tensor(0.) if self.toxic is None else tensor(self.toxic[idx])\n        return xb,yb    \n        \n    def __len__(self):\n        return len(self.input_ids)","b2d5e9c5":"XLM_PROCESSED_PATH = Path(\"\/kaggle\/input\/xlmrobertabase\/xlm_roberta_processed\/\"); XLM_PROCESSED_PATH.ls()","30d510e8":"# # remove eng\n# train_input_ids = train_input_ids[train_lang != \"en\"]\n# train_attetion_mask = train_attetion_mask[train_lang != \"en\"]\n# train_toxic = train_toxic[train_lang != \"en\"]\n# train_lang = train_lang[train_lang != \"en\"]","15410e55":"# labels for stratified batch sampler\ntrain_stratify_labels = array(s1+s2 for (s1,s2) in zip(train_lang, train_toxic.astype(str)))\nlabels2int = {v:k for k,v in enumerate(np.unique(train_stratify_labels))}\nlabels = [labels2int[o] for o in train_stratify_labels]\nbalanced_sampler = BalanceClassSampler(labels)","e12b446b":"labels2int","7d5925c8":"train_ds = JigsawArrayDataset(train_input_ids, train_attetion_mask, train_toxic)","c74977ec":"# del train_input_ids, train_attetion_mask, train_toxic, train_lang\n# del train_stratify_labels, labels2int, labels\n# gc.collect()","74f13620":"# valid_ds\nvalid_ds = JigsawArrayDataset(*[np.load(XLM_PROCESSED_PATH\/'valid_inputs\/input_ids.npy'),\n                                np.load(XLM_PROCESSED_PATH\/'valid_inputs\/attention_mask.npy'),\n                                np.load(XLM_PROCESSED_PATH\/'valid_inputs\/toxic.npy')])","e2208e95":"len(train_ds), len(valid_ds)","e8b99888":"def get_xlm_roberta(modelname=\"xlm-roberta-base\"):        \n    conf = AutoConfig.from_pretrained(modelname)\n    conf.output_hidden_states = True\n    model = AutoModel.from_pretrained(modelname, config=conf)\n    return model","2d05ba19":"class Head(Module):\n    \"Concat Pool over sequence\"\n    def __init__(self, modelname=\"xlm-roberta-base\", p=0.5):\n        \n        self.d0 = nn.Dropout(p)\n        if modelname == \"xlm-roberta-base\": self.l0 = nn.Linear(768*4, 2)\n        elif modelname == \"xlm-roberta-large\": self.l0 = nn.Linear(1024*4, 2)\n        else: raise Exception(\"Invalid model\")\n        \n    def forward(self, x):\n        x = self.d0(x)\n        x = torch.cat([x.permute(0,-1,-2).mean(-1), \n                       x.permute(0,-1,-2).max(-1).values], -1)\n        x = self.l0(x) \n        return x\n\nclass JigsawModel(Module):\n    def __init__(self, model, head):\n        self.sequence_model = model\n        self.head = head\n\n    def forward(self, *xargs):\n        inp = {}\n        inp[\"input_ids\"] = xargs[0]\n        inp[\"attention_mask\"] = xargs[1]\n        _, _, hidden_states = self.sequence_model(**inp)\n        # feed last 2 hidden states\n        x = torch.cat(hidden_states[-2:], -1)\n        return self.head(x)","860c0fb3":"modelname = \"xlm-roberta-large\"\nmodel = get_xlm_roberta(modelname=modelname)\nhead = Head(modelname=modelname)\njigsaw_model = JigsawModel(model, head)","2f266a64":"# xb,yb = train_ds[:3]\n# out = jigsaw_model(*xb)\n# out","de1bf2d0":"# loss_fn = nn.CrossEntropyLoss()\nloss_fn = LabelSmoothingCrossEntropy(0.1)\n# def loss_fn(outputs, targets): return loss(outputs, targets)","4258d350":"def reduce_fn(vals): return sum(vals) \/ len(vals)","fcab5591":"def train_fn(data_loader, model, optimizer, device, num_batches, scheduler=None):\n    model.train()\n    tk0 = tqdm(data_loader, total=num_batches, desc=\"Training\", disable=not xm.is_master_ordinal())\n    for bi, (xb,yb) in enumerate(tk0):\n\n        input_ids, attention_mask = xb\n        input_ids = input_ids.to(device, dtype=torch.long)\n        attention_mask = attention_mask.to(device, dtype=torch.long)\n        yb = yb.to(device, dtype=torch.float)\n        \n        model.zero_grad()\n        out = model(input_ids, attention_mask)\n        \n        loss = loss_fn(out, yb)\n        loss.backward()\n        xm.optimizer_step(optimizer)\n        scheduler.step()\n        print_loss = xm.mesh_reduce('loss_reduce', loss, reduce_fn)\n        tk0.set_postfix(loss=print_loss.item())   ","dc18be9b":"from sklearn.metrics import roc_auc_score\n\ndef eval_fn(data_loader, model, device, num_batches):\n    model.eval()\n    preds, targs = [], []\n   \n    with torch.no_grad():\n        tk0 = tqdm(data_loader, total=num_batches, desc=\"Evaluating\", disable=not xm.is_master_ordinal())\n        for bi, (xb,yb) in enumerate(tk0):\n\n            input_ids, attention_mask = xb\n            input_ids = input_ids.to(device, dtype=torch.long)\n            attention_mask = attention_mask.to(device, dtype=torch.long)\n            yb = yb.to(device, dtype=torch.float)\n            out = model(input_ids, attention_mask)\n\n            preds.append(to_cpu(out.softmax(-1)[:,1]))\n            targs.append(to_cpu(yb))\n\n    return roc_auc_score(torch.cat(targs), torch.cat(preds))","fd6685d4":"from torch.optim.lr_scheduler import *","176a4fcb":"def get_optimizer(model, opt_func=AdamW, lr=1e-5):\n    param_optimizer = list(model.named_parameters())\n    no_decay = [\"bias\", \"LayerNorm.bias\",\"LayerNorm.weight\"]\n    optimizer_parameters = [\n        {\n            'params': [\n                p for n, p in param_optimizer if not any(\n                    nd in n for nd in no_decay\n                )\n            ], \n         'weight_decay': 0.001\n        },\n        {\n            'params': [\n                p for n, p in param_optimizer if any(\n                    nd in n for nd in no_decay\n                )\n            ], \n            'weight_decay': 0.0\n        },\n    ]\n    optimizer = opt_func(optimizer_parameters, lr=lr)\n    return optimizer","45d247fd":"from catalyst.data.sampler import DistributedSamplerWrapper, BalanceClassSampler\nfrom torch.utils.data.distributed import DistributedSampler\nfrom torch.utils.data import DataLoader","a91847f3":"class TPULearner:\n    def __init__(self, model:Module, train_ds:Dataset, valid_ds:Dataset, test_ds:Dataset, opt_func, sched_func=None, sampler=None, bs=128):\n        self.model = model\n        self.train_ds, self.valid_ds, self.test_ds = train_ds, valid_ds, test_ds\n        self.bs = bs\n        self.sampler = sampler\n        self.opt_func = opt_func\n        self.sched_func = sched_func\n    \n    \n    @property\n    def device(self): return xm.xla_device()\n\n    @property\n    def xmodel(self): return self.model.to(self.device)\n    \n    @property\n    def opt(self): return self.opt_func(self.xmodel)\n    \n    \n    \n    \n    @property\n    def train_dl(self):\n        if self.sampler:\n            train_sampler = DistributedSamplerWrapper(self.sampler, num_replicas=xm.xrt_world_size(), rank=xm.get_ordinal(), shuffle=True)\n        else: \n            train_sampler = DistributedSampler(self.train_ds, num_replicas=xm.xrt_world_size(), rank=xm.get_ordinal(), shuffle=True)\n        train_dl = DataLoader(self.train_ds, batch_size=self.bs, sampler=train_sampler, drop_last=True, num_workers=2)\n        return train_dl    \n    @property\n    def train_pl(self): return pl.ParallelLoader(self.train_dl, [self.device]).per_device_loader(self.device)\n        \n        \n    @property\n    def valid_dl(self):\n        valid_sampler = DistributedSampler(self.valid_ds, num_replicas=xm.xrt_world_size(), rank=xm.get_ordinal(), shuffle=False)\n        valid_dl = DataLoader(self.valid_ds, batch_size=self.bs*2, sampler=valid_sampler,drop_last=False,num_workers=1)\n        return valid_dl\n    @property\n    def valid_pl(self): return pl.ParallelLoader(self.valid_dl, [self.device]).per_device_loader(self.device)\n\n    \n    @property\n    def test_dl(self): raise NotImplementedError    \n    @property\n    def test_pl(self): raise NotImplementedError    ","1a86f6e3":"def train(self, model, opt, scheduler):\n    \"Train a single epoch with model and opt\"\n    model.train()\n    \n    tk0 = tqdm(self.train_pl, total=len(self.train_pl), desc=\"Training\", disable=not xm.is_master_ordinal())\n    \n    for bi, (xb,yb) in enumerate(tk0):\n        \n        if not is_listy(xb): xb = listify(xb)\n        xb = [x.to(self.device) for x in xb]\n        yb = yb.to(self.device)\n\n        model.zero_grad()\n        out = model(*xb)\n\n        loss = loss_fn(out, yb)\n        loss.backward()\n        xm.optimizer_step(opt)\n        scheduler.step()\n        \n\n        print_loss = xm.mesh_reduce('loss_reduce', loss, reduce_fn)\n        tk0.set_postfix(loss=print_loss.item())   ","9eabaada":"from sklearn.metrics import roc_auc_score\n\ndef auc(preds, targs):\n    return roc_auc_score(targs, preds.softmax(-1)[:,1])\n\ndef validate(self, model, eval_func):\n    model.eval()\n    preds, targs = [], []\n    with torch.no_grad():\n        tk0 = tqdm(self.valid_pl, total=len(self.valid_pl), desc=\"Evaluating\", disable=not xm.is_master_ordinal())\n        for bi, (xb,yb) in enumerate(tk0):\n\n            if not is_listy(xb): xb = listify(xb)\n            xb = [x.to(self.device) for x in xb]\n            yb = yb.to(self.device)\n            out = model(*xb)\n\n            preds.append(to_cpu(out))\n            targs.append(to_cpu(yb))\n\n    score = eval_func(torch.cat(preds), torch.cat(targs))\n    reduced_score = xm.mesh_reduce('reduced_score', score, reduce_fn)\n    xm.master_print(f'{eval_func.__name__}={reduced_score}')\n    return score","b319b3a7":"def fit(self, epochs=2, eval_func=auc, modelname=\"mymodel\"):\n    \n    if not hasattr(self, \"best_score\"): self.best_score = 0    \n    opt = self.opt # get optim for the device\n    model = self.xmodel # get model for the device\n    scheduler = self.sched_func(opt) # get scheduler for the device\n    \n    \n    for i in range(epochs):        \n        self.train(model, opt, scheduler)\n        score = self.validate(model, eval_func)\n       \n        if score > self.best_score:\n            xm.master_print(\"Model Improved!!! Saving Model\")\n            xm.save(model.state_dict(), f\"{modelname}.pth\")\n            self.best_score = score","ce3abb1c":"def lr_find(self, start_lr=1e-10, end_lr=10, num_it=200, stop_div = True):\n    \n    model = self.xmodel # get model for the device\n    opt = OptimWrapper(self.opt) # get optim for the device\n    sched = Scheduler((start_lr, end_lr), num_it, annealing_exp)\n    \n    self.losses, self.lrs = tensor([0]), tensor([0])\n    \n    model.train()\n    tk0 = tqdm(self.train_pl, total=len(self.train_pl), desc=\"Training\", disable=not xm.is_master_ordinal())\n    for bi, (xb,yb) in enumerate(tk0):\n\n        opt.lr = sched.step()\n        \n        input_ids, attention_mask = xb\n        input_ids = input_ids.to(self.device, dtype=torch.long)\n        attention_mask = attention_mask.to(self.device, dtype=torch.long)\n        yb = yb.to(self.device, dtype=torch.float)\n\n        model.zero_grad()\n        out = model(input_ids, attention_mask)\n\n        loss = loss_fn(out, yb)\n        loss.backward()\n        xm.optimizer_step(opt)\n\n        print_loss = xm.mesh_reduce('loss_reduce', loss, reduce_fn)\n        tk0.set_postfix(loss=print_loss.item())   \n        \n        if xm.is_master_ordinal():   \n            self.losses = torch.cat([self.losses, tensor([print_loss.item()])])\n            self.lrs = torch.cat([self.lrs, tensor([opt.lr])])\n                        \n        if sched.is_done or (torch.isnan(print_loss)): \n            break\n    \n    xm.master_print(\"Stopping lr finder...\")\n    xm.save(self.losses, \"lr_find_losses.pt\")\n    xm.save(self.lrs, \"lr_find_lrs.pt\")   ","e6ece8a2":"TPULearner.train = train\nTPULearner.validate = validate\nTPULearner.fit = fit\nTPULearner.lr_find = lr_find","39bbd018":"# opt_func = partial(get_optimizer, opt_func=AdamW, lr=5e-5)\n# tpu_learner = TPULearner(jigsaw_model, train_ds, valid_ds, None, opt_func, balanced_sampler)\n\n# def _mp_fn(rank, flags):\n#     torch.set_default_tensor_type('torch.FloatTensor')\n#     tpu_learner.lr_find()\n\n# FLAGS={}\n# res = xmp.spawn(_mp_fn, args=(FLAGS,), nprocs=8, start_method='fork')","1d8ce617":"# losses = torch.load(\"\/kaggle\/working\/lr_find_losses.pt\")[1:]\n# lrs = torch.load(\"\/kaggle\/working\/lr_find_lrs.pt\")[1:]","69718cdc":"# smoothen_loss = SmoothenValue(0.98)\n# smooth_losses = []\n# for l in losses:\n#     smoothen_loss.add_value(l)\n#     smooth_losses.append(smoothen_loss.smooth)","3b83535b":"# plt.plot(lrs[10:-30], smooth_losses[10:-30])","68252645":"# bs = 128\n# epochs = 4\n# max_lr = 1e-5","374d9f0a":"# total_steps = int(len(balanced_sampler) \/ bs \/ 8 * epochs); total_steps","9d33eca3":"# sched_func = partial(OneCycleLR, max_lr=max_lr, total_steps=total_steps)\n# tpu_learner = TPULearner(jigsaw_model, train_ds, valid_ds, None, get_optimizer, sched_func, balanced_sampler, bs)","d300ec96":"# def _mp_fn(rank, flags):\n#     torch.set_default_tensor_type('torch.FloatTensor')\n#     tpu_learner.fit(epochs, modelname=\"ft-translated\")\n\n# FLAGS={}\n# res = xmp.spawn(_mp_fn, args=(FLAGS,), nprocs=8, start_method='fork')","0d2e7581":"class config:\n    TRAIN_BATCH_SIZE = 32\n    VALID_BATCH_SIZE = 64\n    EPOCHS = 4\n    LEARNING_RATE = 1e-4\n    MODEL_NAME = \"large_finetuned_translated_data\"\n    TRAINING_DS = train_ds\n    VALIDATION_DS = valid_ds","4ea08b88":"def run():\n\n    device = xm.xla_device()\n    model = jigsaw_model.to(device)\n        \n    trn_ds, val_ds = config.TRAINING_DS, config.VALIDATION_DS\n    \n\n    train_sampler = DistributedSamplerWrapper(\n      balanced_sampler,\n      num_replicas=xm.xrt_world_size(),\n      rank=xm.get_ordinal(),\n      shuffle=True\n    )\n\n    train_data_loader = DataLoader(\n        trn_ds,\n        batch_size=config.TRAIN_BATCH_SIZE,\n        sampler=train_sampler,\n        drop_last=True,\n        num_workers=2\n    )\n\n    valid_sampler = DistributedSampler(\n      val_ds,\n      num_replicas=xm.xrt_world_size(),\n      rank=xm.get_ordinal(),\n      shuffle=False\n    )\n\n    valid_data_loader = DataLoader(\n        val_ds,\n        batch_size=config.VALID_BATCH_SIZE,\n        sampler=valid_sampler,\n        drop_last=False,\n        num_workers=1\n    )\n\n    optimizer = get_optimizer(model, AdamW, lr=config.LEARNING_RATE)\n\n    num_train_steps = int(len(balanced_sampler) \/ config.TRAIN_BATCH_SIZE \/ xm.xrt_world_size() * config.EPOCHS)\n    \n    scheduler = get_linear_schedule_with_warmup(\n        optimizer,\n        num_warmup_steps=int(num_train_steps*0.15),\n        num_training_steps=num_train_steps\n    )\n\n    best_auc = 0\n    xm.master_print(\"Training is Starting....\")\n\n    for epoch in range(config.EPOCHS):\n        para_loader = pl.ParallelLoader(train_data_loader, [device]).per_device_loader(device)\n        trn_num_batches = len(para_loader)\n        train_fn(\n            para_loader, \n            model, \n            optimizer, \n            device,\n            trn_num_batches,\n            scheduler\n        )\n\n        para_loader = pl.ParallelLoader(valid_data_loader, [device]).per_device_loader(device)\n        val_num_batches = len(para_loader)\n        targs_preds = eval_fn(\n            para_loader, \n            model, \n            device,\n            val_num_batches\n        )\n        \n        auc = xm.mesh_reduce('auc_reduce', targs_preds, reduce_fn)\n        xm.master_print(f'Epoch={epoch}, AUC={auc}')\n        if auc > best_auc:\n            xm.master_print(\"Model Improved!!! Saving Model\")\n            xm.save(model.state_dict(), f\"{config.MODEL_NAME}.bin\")\n            best_auc = auc","91616f05":"def _mp_fn(rank, flags):\n    torch.set_default_tensor_type('torch.FloatTensor')\n    a = run()\n\nFLAGS={}\nxmp.spawn(_mp_fn, args=(FLAGS,), nprocs=8, start_method='fork')","6eb1087e":"### TPU Imports","1cc0b13e":"### fit_one_cycle()","9652dd16":"# Training","061a5b03":"### Imports","eb665692":"### Dataset","1eee7ab2":"### TPULearner","f99eb147":"### lr_find()\n\nUncomment and run the following cells to get the optimal learning rate from the graph.","71432216":"### Model","3f6f9a1b":"### Old Training","0f15fcf8":"### fin"}}