{"cell_type":{"ec49080e":"code","b1234788":"code","dcdcb2da":"code","fe2a4b3d":"code","3029c72f":"code","d8926a00":"code","bebefc93":"code","5bec54c8":"code","86e2df78":"code","161e835a":"code","06b7389b":"code","0499cce7":"code","6b47faa1":"code","b0309d21":"code","80dc16f0":"code","b9120390":"code","ab5eb2a5":"code","9e7bb14f":"code","6d623cc1":"code","1bdb89f5":"markdown","ef6f97dc":"markdown","39564861":"markdown","35f05787":"markdown","f67110fd":"markdown","3521bdef":"markdown","ac18364b":"markdown","a67d8ffa":"markdown","dcfdfbda":"markdown"},"source":{"ec49080e":"import torch\nimport torch.autograd as autograd\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\n\ntorch.manual_seed(1)","b1234788":"#Pytorh maps rows of the input instead of the columns. In f(x) = Ax+b, ith row of the output is the map of the ith row of the input plus bias term","dcdcb2da":"lin = nn.Linear(5, 3) #from R^5 to R^3\ndata = torch.randn(2, 5)\nprint( lin(data))","fe2a4b3d":"data = torch.randn(2, 2)\nprint( data )\nprint(F.relu(data))","3029c72f":"data = torch.randn(5)\nprint( data )\nprint( F.softmax( data, dim=0))\nprint( F.softmax( data, dim=0).sum())\n\nprint( F.log_softmax(data, dim=0)) #softmax is a logarithm of each softmax\nprint( torch.exp( F.log_softmax(data, dim=0)) )","d8926a00":"data = [(\"me gusta comer en la cafeteria\".split(), \"SPANISH\"),\n        (\"Give it to me\".split(), \"ENGLISH\"),\n        (\"No creo que sea una buena idea\".split(), \"SPANISH\"),\n        (\"No it is not a good idea to get lost at sea\".split(), \"ENGLISH\")]","bebefc93":"test_data = [(\"Yo creo que si\".split(), \"SPANISH\"),\n             (\"it is lost on me\".split(), \"ENGLISH\")]","5bec54c8":"#word_to_ix maps each word onto an integer which will be the words index in the bag of words vector","86e2df78":"word_to_ix = {}\nfor sent, _ in data + test_data:\n    for word in sent:\n        if word not in word_to_ix:\n            word_to_ix[word] = len(word_to_ix)\n","161e835a":"VOCAB_SIZE = len(word_to_ix)\nNUM_LABELS = 2","06b7389b":"class BoWClassifier(nn.Module): #inherits from nn.Module\n    def __init__(self, num_labels, vocab_size): #it knows that it has to map input of size vocab_size (sentences) onto output of size num_labels (which are labels)\n        super(BoWClassifier, self).__init__()\n        self.linear = nn.Linear(vocab_size, num_labels)\n    \n    def forward(self, bow_vec):\n        return F.log_softmax( self.linear(bow_vec), dim= 1 ) #does log_softmax along column vectors. Here obvious but in general not.","0499cce7":"def make_bow_vector(sentence, word_to_ix):\n        num_labels = len(word_to_ix)\n        vec = torch.zeros(1, num_labels)\n        for word in sentence: #Iterate over words in the sentence; for each new ord, incease the counter by one 1\n            vec[0][ word_to_ix[word] ] += 1\n        return vec","6b47faa1":"def make_target( label, label_to_ix ):\n    return torch.LongTensor( [ label_to_ix[label] ] )","b0309d21":"model = BoWClassifier( NUM_LABELS, VOCAB_SIZE)","80dc16f0":"#for param in model.parameters():\n #   print(param)","b9120390":"'''with torch.no_grad():\n    sample = data[0]\n    bow_vec = make_bow_vector(sample[0], word_to_ix)\n    log_probs = model(bow_vec)\n    print( log_probs )'''","ab5eb2a5":"label_to_ix = {\"SPANISH\":0, \"ENGLISH\":1}","9e7bb14f":"with torch.no_grad(): #run on test data to see before-after\n    for instance, label in test_data:\n        bow_vec = make_bow_vector( instance, word_to_ix )\n        log_probs = model(bow_vec)\n        print( log_probs)\n        \n\nprint( next(model.parameters())[:, word_to_ix[\"creo\"]] )\nloss_function = nn.NLLLoss()\noptimizer = optim.SGD(model.parameters(), lr=0.1)\n\nfor epoch in range(100):\n    for instance, label in data:\n        model.zero_grad() #Clear gradients to prevent accumulation\n        \n        bow_vec = make_bow_vector( instance, word_to_ix )\n        target = make_target( label, label_to_ix )\n        \n        log_probs = model( bow_vec )\n        \n        loss = loss_function( log_probs, target)\n        loss.backward()\n        optimizer.step()\n        \nwith torch.no_grad():\n    for instance, label in test_data:\n        bow_vec = make_bow_vector(instance, word_to_ix)\n        log_probs = model(bow_vec)\n        print(log_probs)\nprint( next(model.parameters())[:,word_to_ix[\"creo\"]] )\n                ","6d623cc1":"#This work is based on the code from the website https:\/\/pytorch.org\/tutorials\/beginner\/nlp\/deep_learning_tutorial.html#sphx-glr-beginner-nlp-deep-learning-tutorial-py\n#and contains my interpretation of the explainations provided on the website.","1bdb89f5":"**CREATING NETWORK COMPONENTS IN PYTORCH**","ef6f97dc":"*DEFINE CLASSIFIER*","39564861":"1. GET DATA","35f05787":"*CREATE A METHOD OF TRANSFORMING INPUT*","f67110fd":"*CREATE A METHOD OF TRANSFORMING OUTPUT*","3521bdef":"**SOFTMAX and PROBABILITIES**","ac18364b":"***EXAMPLE: LOGISTIC REGRESSION BAG-OF-WORDS CLASSIFIER***","a67d8ffa":"**AFFINE MAPS**","dcfdfbda":"*CREATE A REPRESENTATION OF INPUT*"}}