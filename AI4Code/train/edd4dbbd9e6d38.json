{"cell_type":{"d9e31da5":"code","59a938da":"code","c8227eb0":"code","bd9b54c5":"code","765e48e2":"code","28357bd1":"code","c307a398":"code","1829ddc4":"code","fa0feffd":"code","4d21c968":"code","d53ca6bd":"code","96b7bcb7":"code","d4ce3cbd":"code","e8447896":"code","e6404908":"code","5df146ee":"code","00ddb133":"code","71d0efe5":"code","432e9826":"code","4fbee3b2":"code","ed626f65":"code","aea01c2c":"code","bcaf9ea6":"code","99db817c":"code","3f7d3229":"code","b896c435":"code","5ed15b2e":"code","dc3c7b8c":"code","9e5b3298":"code","bd950b3a":"code","97d1353e":"code","f4d46dcf":"code","875eba4e":"code","fca0df2c":"code","5be78412":"code","42f03b1e":"code","ab7dbfa9":"code","11b91439":"code","2efbf4a4":"code","3fd7c825":"code","fbbf2445":"code","9f2f7085":"code","f2b47cf9":"code","2696ef62":"code","14ea0bef":"code","1983835f":"code","259e762e":"code","3e8efc28":"code","008ee478":"code","3e8e9b22":"code","10bb4087":"code","bdf8a93d":"code","78847936":"code","d6258b98":"code","cd4ac4c7":"code","643eca3a":"code","68416ee6":"code","7e48f0ba":"code","2f21c34f":"code","b641b728":"code","ec2af0d2":"code","ebadd36d":"code","92ef0ec4":"code","cd416308":"code","b9a04b63":"code","009eb59c":"code","e608e484":"code","e32e6ae1":"code","c8e25070":"code","08b3c1d5":"code","c0175e84":"code","575993e5":"code","68c64e42":"code","5eae1e0a":"code","71b025ec":"code","51fef47f":"code","1eca90d0":"code","abaf59e3":"code","1e5df1b4":"code","88e55162":"code","38e28c5e":"code","7b844d16":"code","110f387f":"code","b9783eaf":"code","3981a82c":"code","9b65010d":"code","50180c78":"code","31ee9f0e":"code","17d65d0f":"code","a51c9e27":"code","93b5d107":"code","3cb35da4":"code","a8d473e6":"code","59d1332f":"code","c4618d33":"code","3a2deab7":"code","71b343e2":"code","b9654131":"code","4ad47833":"code","b2d31a1e":"code","0efa2426":"code","7de0d258":"code","93f8eacb":"code","08b7c616":"code","092c258f":"code","c7a9ff15":"code","21ee1ab9":"code","13ba41cc":"code","4f1de4d7":"code","2e7e363d":"code","25e8a92d":"code","970d878e":"code","35e76dde":"code","1e1ea759":"code","70a17162":"code","0bb92864":"code","445cbcef":"code","13ddf8a8":"code","4b3580c1":"code","ccd699dc":"code","02387742":"code","681cd4c1":"code","f78cd67d":"code","8d51fa2c":"code","52ea8724":"code","f716caec":"code","17d1eb2b":"code","d2e2ec0a":"code","2f14ad6d":"code","a7016662":"code","3233ca37":"code","d0b64dfb":"code","f1828d49":"code","741bd0b0":"code","7bbbc0a8":"code","4a720284":"code","55785507":"code","c16196b7":"code","5dc72499":"code","add2533b":"code","97df5500":"code","7c3b5e47":"code","6cab89f0":"code","01770a3d":"code","c6d5d76d":"code","9e74164d":"code","3830c123":"code","424e6fd1":"code","d995ea3f":"code","9cf685ac":"code","bb3974a9":"code","c6ad5a7f":"code","0e8d5568":"code","1f54d2cb":"code","46f30cbe":"code","c384728d":"markdown","6694e6bf":"markdown","7feba773":"markdown","6ae4841d":"markdown","19d6266f":"markdown","533e4575":"markdown","fc4ddefb":"markdown","4ae6efe9":"markdown","3b03fc93":"markdown","980ea751":"markdown","3edca84c":"markdown","89b9dd3b":"markdown","677798d7":"markdown","327f6bfa":"markdown","0a706396":"markdown","599e028e":"markdown","4a943e39":"markdown","b7265667":"markdown","e56c3cf4":"markdown","e4bb1348":"markdown","f6f27ad8":"markdown","8355affd":"markdown","63a795b9":"markdown","2d3d9e19":"markdown","8a8f83ee":"markdown","613d540c":"markdown","ce2a62aa":"markdown","558a860c":"markdown","49dcf13c":"markdown","901a76b1":"markdown","82866e64":"markdown","7121eed1":"markdown","c743d8fc":"markdown","e128eacc":"markdown","93934e82":"markdown","6b9abc8f":"markdown","69d82b46":"markdown","8ae4f5d2":"markdown","d398480c":"markdown","8f5ecbd2":"markdown","fac9b47c":"markdown","ae1b978d":"markdown","549d3e12":"markdown","53306fd7":"markdown","4385efb5":"markdown","7d40bb9a":"markdown","5fee50cc":"markdown","81990bd5":"markdown","6caabdc8":"markdown","6cdbb8c8":"markdown","ec5a1b63":"markdown","ff081a68":"markdown","5d8e0ce2":"markdown","b0e8f3d5":"markdown","3bad40b9":"markdown","90a86b4b":"markdown","4f3ec58c":"markdown","1965c3ee":"markdown","ecea74c8":"markdown","6cc8f956":"markdown","8f077356":"markdown","1bc81a66":"markdown","ab0eb5b7":"markdown","ddc1a2bd":"markdown","b82bd588":"markdown","e0fdf9e7":"markdown","b0deec1b":"markdown","8fb9702e":"markdown","5bf161a6":"markdown","20addb98":"markdown","e539f885":"markdown","96d6f0a7":"markdown","177bd072":"markdown","04d87ce0":"markdown","1ccbb3ad":"markdown","3e3a9dc6":"markdown","4223a628":"markdown","328aa765":"markdown","b0cff8ea":"markdown","164121c5":"markdown","2c9806e4":"markdown","9df4ce28":"markdown","2c20fdb0":"markdown","9849ac16":"markdown","7d17d592":"markdown","cdddf7e6":"markdown","824af3db":"markdown","ad3b8581":"markdown","bdbe761f":"markdown","1df5d614":"markdown","12493d38":"markdown","8b2d60c9":"markdown","3abb312f":"markdown","ca3a8470":"markdown","b38a7b7f":"markdown","19bb4bbd":"markdown","7a4d0c59":"markdown","85c617c8":"markdown","8717872f":"markdown","19c239bf":"markdown","5cc205a0":"markdown","378c2a18":"markdown","85095899":"markdown","a5790544":"markdown","9bae764e":"markdown","3893a998":"markdown","cdad677c":"markdown","7438f1b2":"markdown","744e3f24":"markdown","6cb18182":"markdown","86b12b1f":"markdown","3c4e02a7":"markdown","9f925beb":"markdown","f184e608":"markdown","bb73f106":"markdown","9b00937f":"markdown","c1ca32e4":"markdown","5f3d3fad":"markdown","9a6c0c04":"markdown","d8508f49":"markdown","df4548eb":"markdown","53c3cfe0":"markdown","7d4e7c27":"markdown","20b5a588":"markdown","ffb89b3d":"markdown","728fc170":"markdown","7cc4296b":"markdown","b1151016":"markdown","4ece9e33":"markdown","823b710c":"markdown","2cdee6b9":"markdown","6ccbcc71":"markdown","5c60c054":"markdown","2a53f943":"markdown","efa9750b":"markdown","6766d292":"markdown","079bc2e7":"markdown","3d0076a0":"markdown","dbc93bde":"markdown","1752e541":"markdown","5e9b9219":"markdown","ac8daf4f":"markdown","90c43fe7":"markdown","d5208be2":"markdown","8fbcc5b5":"markdown","77993709":"markdown","abcb3014":"markdown","bd81a055":"markdown","ec5ebfa3":"markdown","6a97ba73":"markdown","751e2ab9":"markdown","bba82b90":"markdown","fbbbd089":"markdown","ef510c2b":"markdown","910ef1a0":"markdown","7f7f41cd":"markdown","a5b8ac62":"markdown","6141272a":"markdown","22c820bc":"markdown","53c68846":"markdown","c30bc16a":"markdown","5a9f5a3f":"markdown","61be1e6e":"markdown","e2e6a50f":"markdown","457912a2":"markdown","c1e3da99":"markdown","9a791292":"markdown","2add686d":"markdown","10dd6ccb":"markdown","5ec8a80c":"markdown","647d9504":"markdown","f7fd7788":"markdown","a35e5cc6":"markdown","8b6c5c0a":"markdown","783682a7":"markdown","e81c90a9":"markdown","6a1f03e9":"markdown","b8fde6f6":"markdown","7ceff75e":"markdown","42876341":"markdown","1dbd9664":"markdown","8625d90a":"markdown","67aa8787":"markdown","8871bacf":"markdown","222354c7":"markdown","b47ff468":"markdown","da7656a5":"markdown","1f408073":"markdown","83e8d678":"markdown","9ccc8ee0":"markdown","4e97d5f6":"markdown","11871cba":"markdown","af9945ac":"markdown","433be6ae":"markdown","32aee1c7":"markdown","70ed99c7":"markdown","c34806a9":"markdown","640c9601":"markdown","fd0d9a09":"markdown","4d76bc7f":"markdown","915f0d6b":"markdown","463115d3":"markdown","f842cac4":"markdown","ab46ba7d":"markdown","57c33d92":"markdown","deda30f5":"markdown","298a34bb":"markdown","a1e18618":"markdown","96ee8c42":"markdown","329bc2de":"markdown","e35e4951":"markdown","06e006b0":"markdown","ccc46143":"markdown","656409a8":"markdown","50db58e7":"markdown","4caf965c":"markdown","e6e1369e":"markdown"},"source":{"d9e31da5":"# Libraries\nimport os\nimport numpy as np\nimport pandas as pd\nimport re\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport plotly.offline as pyo\nimport plotly.graph_objects as go\nimport plotly.express as px\nfrom plotly.offline import init_notebook_mode\nimport plotly.figure_factory as ff\nfrom plotly import subplots\ninit_notebook_mode(connected=True)\n%matplotlib inline\nfrom textwrap import fill, wrap\nimport re\n\nplt.rcParams['figure.dpi'] = 150 # higher resolution","59a938da":"# Load data\ndfo = pd.read_csv('\/kaggle\/input\/kaggle-survey-2020\/kaggle_survey_2020_responses.csv')\nprint(dfo.shape)\n\npd.set_option('display.max_columns',100)\ndfo.head()","c8227eb0":"dfo.describe()","bd9b54c5":"# Function to forestall UserWarning\nimport warnings\ndef fxn():\n    warnings.warn(\"UserWarning arose\", UserWarning)","765e48e2":"# Function to create distribution of a variable based on 2 conditions\ndef fd(data, data1=None, criteria1=None, data2=None, criteria2=None, title='criteria', \n       strip=False, contains=False, otherthan1=False, otherthan2=False):\n    \"\"\"Create distribution of a variable based on 2 conditions.(order is important)\"\"\"\n    tempdf1 = data.copy()\n    if data1 is not None:\n        if isinstance(data1, pd.Series):\n            cdf1 = data1.copy().to_frame()\n        else:\n            cdf1 = data1.copy()\n    else:\n        cdf1 = None\n            \n    if data2 is not None:\n        if isinstance(data2, pd.Series):\n            cdf2 = data2.copy().to_frame()\n        else:\n            cdf2 = data2.copy()\n    else:\n        cdf2 = None\n    \n    if len(data.shape) > 1:\n        if data.shape[1] > 1:\n            tempdf1.columns = tempdf1.mode().values.tolist()[0]\n    if strip==True:\n        for df in [cdf1, cdf2]:\n            if df is not None:\n                for c in df:\n                    df[c] = df[c].apply(lambda x: x.strip() if isinstance(x, str) else x)\n\n    \n    if ((criteria1==None) & (criteria2==None)):\n        tempdf = tempdf1.copy()\n        case = 1\n    elif criteria1==None:\n        tempdf = pd.concat([cdf2, tempdf1], axis=1)\n        case = 2\n    elif criteria2==None:\n        tempdf = pd.concat([cdf1, tempdf1], axis=1)\n        case = 2\n    else:\n        tempdf = pd.concat([cdf1, cdf2, tempdf1], axis=1)\n        case = 3\n        \n    if case == 3:\n        if ((otherthan1 is True) and (otherthan2 is True)):\n            if contains==True:\n                tempdf = tempdf[~(tempdf.iloc[:,0].str.contains(r''+criteria1, regex=True, na=False)) &\n                            ~(tempdf.iloc[:,1].str.contains(r''+criteria2, regex=True, na=False))]\n            else:\n                tempdf = tempdf[(tempdf.iloc[:,0] != criteria1) & (tempdf.iloc[:,1] != criteria2)]\n        else:\n            if contains==True:\n                tempdf = tempdf[(tempdf.iloc[:,0].str.contains(r''+criteria1, regex=True, na=False)) &\n                            (tempdf.iloc[:,1].str.contains(r''+criteria2, regex=True, na=False))]\n            else:\n                tempdf = tempdf[(tempdf.iloc[:,0] == criteria1) & (tempdf.iloc[:,1] == criteria2)]\n        tempdf.drop(tempdf.columns[[0,1]], axis=1, inplace=True)\n            \n\n    if case == 2:\n        crit = criteria2 if criteria1==None else criteria1\n        otherthan = otherthan2 if criteria1==None else otherthan1\n        if otherthan:\n            if contains==True:\n                tempdf = tempdf[~tempdf.iloc[:,0].str.contains(r''+crit, regex=True, na=False)]\n            else:\n                tempdf = tempdf[tempdf.iloc[:,0] != crit]\n        else:\n            if contains==True:\n                tempdf = tempdf[tempdf.iloc[:,0].str.contains(r''+crit, regex=True, na=False)]\n            else:\n                tempdf = tempdf[tempdf.iloc[:,0] == crit]\n        tempdf.drop(tempdf.columns[0], axis=1, inplace=True)\n            \n\n    if len(data.shape) > 1:\n        if data.shape[1] > 1:\n            tempdf = tempdf.dropna(how='all')\n            n = tempdf.shape[0]\n\n            fd = tempdf.count().reset_index()\n            fd.columns = [title, 'freq']\n            fd['proportion'] = round(fd['freq']*100\/n, 2)\n            return fd\n\n    fd = tempdf.value_counts().reset_index()\n    fd.columns = [title, 'freq']\n    fd['proportion'] = (tempdf.value_counts(normalize=True).values*100).round(2)\n    return fd","28357bd1":"# Function to create frequency distribution (multi-answer questions)\ndef fd_maq(data, title='criteria'):\n    \"\"\"Create frequency distribution for questions allowing multiple answers.\"\"\"\n    tempdf = data.copy()\n    tempdf.columns = tempdf.mode().values.tolist()[0]\n    tempdf = tempdf.dropna(how='all')\n    n = tempdf.shape[0]\n    fd = tempdf.count().reset_index()\n    fd.columns = [title, 'freq']\n    fd['proportion'] = round(fd['freq']*100\/n, 2)\n    return fd\n\n# Function to create frequency distribution (single-answer questions)\ndef fd_saq(data, title='criteria', ordered=True):\n    \"\"\"Create frequency distribution for questions allowing single answer.\"\"\"\n    tempdf = data.dropna()\n    n = tempdf.shape[0]\n    fd = tempdf.value_counts(sort=ordered).reset_index()\n    fd.columns = [title, 'freq']\n    fd['proportion'] = round(fd['freq']*100\/n, 2)\n    return fd\n\n# Function to create subset according to Country\ndef country_subset(data, country):\n    \"\"\"Create subset according to country.\"\"\"\n    tempdf1 = data.copy()\n    tempdf1['country'] = df['Q3'].copy()\n    tempdf = tempdf1[tempdf1['country'] == country]\n    tempdf.drop('country', axis=1, inplace=True)\n    return tempdf\n\n# Function to sort dataframe with custom order\ndef custom_sort(df, col, ordered_list, reverse=False):\n    \"\"\"Sort dataframe with custom order list.\"\"\"\n    tempdf1 = df.copy()\n    tempdf1 = tempdf1.set_index(col)\n    if reverse:\n        custom_order = ordered_list[::-1]\n    else:\n        custom_order = ordered_list\n    tempdf = tempdf1.loc[ordered_list, :].reset_index()\n    return tempdf\n\n# Function to sort dataframe with custom order (2)\ndef custom_sort2(df, col, ordered_list, reverse=False):\n    \"\"\"Sort dataframe with custom order list.\"\"\"\n    tempdf1 = df.copy()\n    if reverse:\n        custom_order = pd.DataFrame(ordered_list[::-1])\n    else:\n        custom_order = pd.DataFrame(ordered_list)\n    tempdf = custom_order.merge(tempdf1, how='left', left_on=0, right_on=col)\n    tempdf.drop(0, axis=1, inplace=True)\n    tempdf.dropna(how='all', inplace=True)\n    tempdf.reset_index(drop=True, inplace=True)\n    return tempdf","c307a398":"# Function to create differential proportion graph\ndef differential_graph(qn, graph_title='Differential Proportion', order=None):\n    \"\"\"Create differential proportion graph for India and USA.\"\"\"\n    tempdf = qn.copy() \n    # Create value_counts table\n    qn_fd = qn.value_counts().rename('World').to_frame()\n    qni_fd = country_subset(qn,'India').value_counts().rename('India').to_frame()\n    qnu_fd = country_subset(qn,'United States of America').value_counts().rename('USA').to_frame()\n    qn_fd['India'] = qn_fd.index.map(qni_fd['India'])\n    qn_fd['USA'] = qn_fd.index.map(qnu_fd['USA'])\n    qn_fd.index.rename('criteria', inplace=True)\n    qn_fd.reset_index(inplace=True)\n\n    # India - Differential proportion compared with overall overall proportion of 0.292\n    diff = ((qn_fd['India']\/qn_fd['World']) - 0.292).round(4)*100\n    clrs = list(np.where(diff.values < 0, '#B51A62', '#37659E'))\n\n    # Plot\n    fig1 = go.Figure(go.Bar(x=qn_fd['criteria'], y=diff.values, text=diff.values.round(3), \n                           marker_color=clrs))\n    title1 = f\"India - Change in {graph_title} compared with overall sample proportion (29.2%)<br>(in percentage points (%p))\"\n    fig1.update_layout(title={'text':title1, 'x':0.5, 'xanchor':'center'}, plot_bgcolor='#fff')\n    fig1.update_traces(texttemplate='%{text:i} %p', textposition='outside', textfont_size=9, \n                      textfont_color='black')\n    if order is not None:\n        fig1.update_xaxes(categoryorder='array', categoryarray=order, visible=True)\n    \n    \n    # USA - Differential proportion compared with overall overall proportion of 0.112\n    diff = ((qn_fd['USA']\/qn_fd['World']) - 0.112).round(4)*100\n    clrs = list(np.where(diff.values < 0, '#B51A62', '#37659E'))\n\n    # Plot\n    fig2 = go.Figure(go.Bar(x=qn_fd['criteria'], y=diff.values, text=diff.values.round(3), \n                           marker_color=clrs))\n    title2 = f\"USA - Change in {graph_title} compared with overall sample proportion (11.2%)<br>(in percentage points (%p))\"\n    fig2.update_layout(title={'text':title2, 'x':0.5, 'xanchor':'center'}, plot_bgcolor='#fff')\n    fig2.update_traces(texttemplate='%{text:i} %p', textposition='outside', textfont_size=9, \n                      textfont_color='black')\n    if order is not None:\n        fig2.update_xaxes(categoryorder='array', categoryarray=order, visible=True)\n    \n    return fig1, fig2","1829ddc4":"# Function to find % change in proportion of India and USA distributions\ndef diff_ind_usa(data, data2=None, criteria2=None, title='criteria', strip=False, contains=False, otherthan1=False, otherthan2=False):\n    \"\"\"Different in proportions for India and USA distributions.\"\"\"\n    o = fd(data, data2=data2, criteria2=criteria2, title=title, strip=strip, contains=contains, otherthan1=otherthan1, otherthan2=otherthan2)\n    i = fd(data, data1=qd['Q3'], criteria1='India', data2=data2, criteria2=criteria2, title=title, strip=strip, contains=contains, otherthan1=otherthan1, otherthan2=otherthan2)\n    u = fd(data, data1=qd['Q3'], criteria1='United States of America', data2=data2, criteria2=criteria2, title=title, strip=strip, contains=contains, otherthan1=otherthan1, otherthan2=otherthan2)\n    comp = o.iloc[:,[0,2]].set_index('criteria').rename(columns={'proportion':'world'})\n    comp['india'] = comp.index.map(i.set_index('criteria')['proportion'])\n    comp['usa'] = comp.index.map(u.set_index('criteria')['proportion'])\n    comp['diff %'] = ((comp['india'] - comp['usa'])\/comp['world']).round(4)*100\n    return comp","fa0feffd":"# Fuction to build graph with USA and India distributions and filters\ndef build_graph(qn, qn2=None, graph_title='Comparitive Graph', ascending=False, order=None, \n                counts=False, label_angle=-15, title='criteria', hgt=600, wd=900, xsize=9, \n                adjust_margin=True, xmargin=None, tmargin=None):\n    \"\"\"Create plotly interactive graph with USA-India options\"\"\"\n    qdf = qn.copy()\n    output = 'freq' if counts==True else 'proportion'\n\n    # Create freqency distribution table\n    kind = ''\n    if len(qdf.shape) > 1:\n        if qdf.shape[1] > 1:\n            kind = 'df'\n            \n    if kind == 'df':\n        qdf.columns = qdf.mode().values.tolist()[0]\n        classes = qdf.columns\n        q_fds = fd_maq(qdf, title).sort_values(by='freq', ascending=False)\n        q_fdsi = fd_maq(country_subset(qdf, 'India'), title).sort_values(by='freq', ascending=False)\n        q_fdsu = fd_maq(country_subset(qdf, 'United States of America'), title).sort_values(by='freq', ascending=False)\n    else:\n        classes = qdf.iloc[:,0].unique()\n        q_fds = fd_saq(qdf, title).sort_values(by='freq', ascending=False)\n        q_fdsi = fd_saq(country_subset(qdf, 'India'), title).sort_values(by='freq', ascending=False)\n        q_fdsu = fd_saq(country_subset(qdf, 'United States of America'), title).sort_values(by='freq', ascending=False)\n        \n    # Comparitive Bar Chart    \n    fig = go.Figure()\n    name1 = 'World' if qn2 is None else '(legends applicable<br>when unfiltered)<br>World<br><br>'\n    fig.add_trace(go.Bar(x=q_fds[title], y=q_fds[output], name=name1,\n                         marker_color='thistle', text=q_fds[output]))\n    fig.add_trace(go.Bar(x=q_fdsi[title], y=q_fdsi[output], name='India', \n                         marker_color='#37659E', text=q_fdsi[output]))\n    fig.add_trace(go.Bar(x=q_fdsu[title], y=q_fdsu[output], name='USA',\n                         marker_color='teal', text=q_fdsu[output]))\n\n    \n    if qn2 is not None:\n        qdf2 = qn2.copy()\n        kind2=''\n        classes = qdf2.iloc[:,0].unique()\n        if len(qdf2.shape) > 1:\n            if qdf2.shape[1] > 1:\n                kind2 = 'df'\n                classes = qdf2.mode().values.tolist()[0]\n\n        button1=[]\n        button1.append(dict(method='restyle', label='All samples', visible=True,\n                            args=[{'y':[q_fds[output]], 'text':[q_fds[output]],\n                                   'x':[q_fds[title]],'type':'bar'},[0]]))\n\n        for i in range(len(list(classes))):\n            r = list(classes)[i]\n            for c in ['.','India','United States of America']:\n                cnt = c if c in ['India','United States of America'] else None\n                fd_r = fd(qdf, qn2, r, qd['Q3'], cnt) if kind2!='df' else fd(qdf, qn2.iloc[:,i], r, qd['Q3'], cnt)\n                button1.append(dict(method='restyle', label=str(r)[0:25]+' - '+str('World' if c == '.' else 'USA' if c == 'United States of America' else c), visible=True,\n                                   args=[{'y':[fd_r[output]],'x':[fd_r[title]],'type':'bar',\n                                          'text':[fd_r[output]]},[0]]))\n\n        button2=[]\n        button2.append(dict(method='restyle', label='All samples', visible=True,\n                            args=[{'y':[q_fds[output]], 'text':[q_fds[output]],\n                                   'x':[q_fds[title]],'type':'bar'},[1]]))\n\n        for i in range(len(list(classes))):\n            r = list(classes)[i]\n            for c in ['.','India','United States of America']:\n                cnt = c if c in ['India','United States of America'] else None\n                fd_r = fd(qdf, qn2, r, qd['Q3'], cnt) if kind2!='df' else fd(qdf, qn2.iloc[:,i], r, qd['Q3'], cnt)\n                button2.append(dict(method='restyle', label=str(r)[0:25]+' - '+str('World' if c == '.' else 'USA' if c == 'United States of America' else c), visible=True,\n                                   args=[{'y':[fd_r[output]],'x':[fd_r[title]],'type':'bar',\n                                          'text':[fd_r[output]]},[1]]))\n       \n        button3=[]\n        button3.append(dict(method='restyle', label='All samples', visible=True,\n                            args=[{'y':[q_fds[output]], 'text':[q_fds[output]],\n                                   'x':[q_fds[title]],'type':'bar'},[2]]))\n\n        for i in range(len(list(classes))):\n            r = list(classes)[i]\n            for c in ['.','India','United States of America']:\n                cnt = c if c in ['India','United States of America'] else None\n                fd_r = fd(qdf, qn2, r, qd['Q3'], cnt) if kind2!='df' else fd(qdf, qn2.iloc[:,i], r, qd['Q3'], cnt)\n                button3.append(dict(method='restyle', label=str(r)[0:25]+' - '+str('World' if c == '.' else 'USA' if c == 'United States of America' else c), visible=True,\n                                   args=[{'y':[fd_r[output]],'x':[fd_r[title]],'type':'bar',\n                                          'text':[fd_r[output]]},[2]]))\n    \n        button_layer_1_height = 1.16\n        updatemenus = list([dict(buttons=button1, direction='down',pad={'r':10,'t':10}, showactive=True,\n                                x=0.02, xanchor='left', y=button_layer_1_height, yanchor='top'),\n                            dict(buttons=button2, direction='down',pad={'r':10,'t':10}, showactive=True,\n                                x=0.45, xanchor='left', y=button_layer_1_height, yanchor='top'),\n                            dict(buttons=button3, direction='down',pad={'r':10,'t':10}, showactive=True,\n                                x=0.88, xanchor='left', y=button_layer_1_height, yanchor='top')])\n\n        fig.update_layout(updatemenus=updatemenus)\n        fig.update_layout(annotations=[dict(text=\"1\", x=0.0, xref=\"paper\", showarrow=False,\n                                            y=button_layer_1_height-0.05, yref=\"paper\"), \n                                       dict(text=\"2\", x=0.435, xref=\"paper\", showarrow=False,\n                                            y=button_layer_1_height-0.05, yref=\"paper\"),\n                                       dict(text=\"3\", x=0.87, xref=\"paper\", showarrow=False,\n                                            y=button_layer_1_height-0.05, yref=\"paper\")])\n\n\n    if ascending==False:\n        if order == None:\n            fig.update_xaxes(categoryorder='array', categoryarray=qdf.columns)\n        else:\n            fig.update_xaxes(categoryorder='array', categoryarray=order)\n    \n    fig.update_layout(plot_bgcolor='#fff')\n    title_text = str(graph_title)+'<br>(% of respondents)' if counts==False else str(graph_title)+'<br>(No. of respondents)'\n    fig.update_layout(title={'text':title_text,\n                             'x':0.49, 'xanchor':'center','y':0.97, 'yanchor':'top'})\n    fig.update_layout(autosize=False, width=wd, height=hgt, xaxis_tickangle=label_angle)\n    fig.update_traces(texttemplate='%{text:.2s}', textposition='outside', textfont_size=xsize)\n    if tmargin is not None:\n        fig.update_layout(margin=dict(t=tmargin))\n    if adjust_margin is False:\n        fig.update_xaxes(automargin=False)\n    if xmargin is not None:\n        fig.update_layout(margin=dict(b=xmargin))\n    \n    fig.show()\n\n# Notebook referenced : 1","4d21c968":"# Function to create data subset based on upto 2 conditions\ndef create_subset(data, data1=None, criteria1=None, data2=None, criteria2=None, title='criteria', \n                  strip=False, contains=False, otherthan1=False, otherthan2=False):\n    \"\"\"Create subset of data based on upto 2 conditions.\"\"\"\n    tempdf1 = data.copy()\n    if data1 is not None:\n        if isinstance(data1, pd.Series):\n            cdf1 = data1.copy().to_frame()\n        else:\n            cdf1 = data1.copy()\n    else:\n        cdf1 = None\n            \n    if data2 is not None:\n        if isinstance(data2, pd.Series):\n            cdf2 = data2.copy().to_frame()\n        else:\n            cdf2 = data2.copy()\n    else:\n        cdf2 = None\n    \n    if strip==True:\n        for df in [cdf1, cdf2]:\n            if df is not None:\n                for c in df:\n                    df[c] = df[c].apply(lambda x: x.strip() if isinstance(x, str) else x)\n\n    \n    if ((criteria1==None) & (criteria2==None)):\n        tempdf = tempdf1.copy()\n        case = 1\n    elif criteria1==None:\n        tempdf = pd.concat([cdf2, tempdf1], axis=1)\n        case = 2\n    elif criteria2==None:\n        tempdf = pd.concat([cdf1, tempdf1], axis=1)\n        case = 2\n    else:\n        tempdf = pd.concat([cdf1, cdf2, tempdf1], axis=1)\n        case = 3\n        \n    if case == 3:\n        if ((otherthan1 is True) and (otherthan2 is True)):\n            if contains==True:\n                tempdf = tempdf[~(tempdf.iloc[:,0].str.contains(r''+criteria1, regex=True, na=False)) &\n                            ~(tempdf.iloc[:,1].str.contains(r''+criteria2, regex=True, na=False))]\n            else:\n                tempdf = tempdf[(tempdf.iloc[:,0] != criteria1) & (tempdf.iloc[:,1] != criteria2)]\n        else:\n            if contains==True:\n                tempdf = tempdf[(tempdf.iloc[:,0].str.contains(r''+criteria1, regex=True, na=False)) &\n                            (tempdf.iloc[:,1].str.contains(r''+criteria2, regex=True, na=False))]\n            else:\n                tempdf = tempdf[(tempdf.iloc[:,0] == criteria1) & (tempdf.iloc[:,1] == criteria2)]\n        tempdf.drop(tempdf.columns[[0,1]], axis=1, inplace=True)\n            \n\n    if case == 2:\n        crit = criteria2 if criteria1==None else criteria1\n        otherthan = otherthan2 if criteria1==None else otherthan1\n        if otherthan:\n            if contains==True:\n                tempdf = tempdf[~tempdf.iloc[:,0].str.contains(r''+crit, regex=True, na=False)]\n            else:\n                tempdf = tempdf[tempdf.iloc[:,0] != crit]\n        else:\n            if contains==True:\n                tempdf = tempdf[tempdf.iloc[:,0].str.contains(r''+crit, regex=True, na=False)]\n            else:\n                tempdf = tempdf[tempdf.iloc[:,0] == crit]\n        tempdf.drop(tempdf.columns[0], axis=1, inplace=True)\n            \n\n    if len(data.shape) > 1:\n        if data.shape[1] > 1:\n            tempdf = tempdf.dropna(how='all')\n            return tempdf\n\n    return tempdf","d53ca6bd":"# Function to create frequency distribution on upto 2 conditions and with overall weights of the criteria column\ndef fd2(data, data1=None, criteria1=None, data2=None, criteria2=None, title='criteria', \n        strip=False, contains=False):\n    \"\"\"Create distribution of a variable based on 2 conditions and get proportion of criteria population.\"\"\"\n    tempdf1 = data.copy()\n    if data1 is not None:\n        if isinstance(data1, pd.Series):\n            cdf1 = data1.copy().to_frame()\n        else:\n            cdf1 = data1.copy()\n    else:\n        cdf1 = None\n            \n    if data2 is not None:\n        if isinstance(data2, pd.Series):\n            cdf2 = data2.copy().to_frame()\n        else:\n            cdf2 = data2.copy()\n    else:\n        cdf2 = None\n    \n    if len(data.shape) > 1:\n        if data.shape[1] > 1:\n            tempdf1.columns = tempdf1.mode().values.tolist()[0]\n            full_fd = tempdf1.count()\n        else:\n            full_fd = tempdf1.value_counts().reset_index()\n            full_fd.columns = ['criteria','values']\n            full_fd = full_fd.set_index('criteria')\n    else:\n        full_fd = tempdf1.value_counts()\n        full_fd.columns = ['criteria','values']\n        full_fd = full_fd.set_index('criteria')\n\n    if strip==True:\n        for df in [cdf1, cdf2]:\n            if df is not None:\n                for c in df:\n                    df[c] = df[c].apply(lambda x: x.strip() if isinstance(x, str) else x)\n\n    \n    if ((criteria1==None) & (criteria2==None)):\n        tempdf = tempdf1.copy()\n        case = 1\n    elif criteria1==None:\n        tempdf = pd.concat([cdf2, tempdf1], axis=1)\n        case = 2\n    elif criteria2==None:\n        tempdf = pd.concat([cdf1, tempdf1], axis=1)\n        case = 2\n    else:\n        tempdf = pd.concat([cdf1, cdf2, tempdf1], axis=1)\n        case = 3\n    \n    if case == 3:\n        if contains==True:\n            tempdf = tempdf[(tempdf.iloc[:,0].str.contains(r''+criteria1, regex=True, na=False)) &\n                        (tempdf.iloc[:,1].str.contains(r''+criteria2, regex=True, na=False))]\n        else:\n            tempdf = tempdf[(tempdf.iloc[:,0] == criteria1) & (tempdf.iloc[:,1] == criteria2)]\n        tempdf.drop(tempdf.columns[[0,1]], axis=1, inplace=True)\n          \n    if case == 2:\n        crit = criteria2 if criteria1==None else criteria1\n        if contains==True:\n            tempdf = tempdf[tempdf.iloc[:,0].str.contains(r''+crit, regex=True, na=False)]\n        else:\n            tempdf = tempdf[tempdf.iloc[:,0] == crit]\n        tempdf.drop(tempdf.columns[0], axis=1, inplace=True)\n\n    if len(data.shape) > 1:\n        if data.shape[1] > 1:\n            tempdf = tempdf.dropna(how='all')\n            n = tempdf.shape[0]\n            fd = tempdf.count().reset_index()\n            fd.columns = [title, 'freq']\n            fd['proportion'] = round(fd['freq']*100\/n, 2)\n            fd['criteria_weight'] = round(fd['freq']\/fd.iloc[:,0].map(full_fd), 4)*100\n            return fd\n\n    fd = tempdf.value_counts().reset_index()\n    fd.columns = [title, 'freq']\n    fd['proportion'] = (tempdf.value_counts(normalize=True).values*100).round(2)\n    fd['criteria_weight'] = round(fd['freq']\/fd.iloc[:,0].map(full_fd['values']), 4)*100\n    return fd","96b7bcb7":"# Save questions separately\nqtext = [i+' - '+j for i,j in zip([re.split('_Part|_OTHER',c)[0] for c in dfo.columns], dfo.iloc[0,:])]\n\n# Generate unique question list\nqtext2 = [re.split('\\(Select all that apply\\)|- Selected Choice', q)[0].strip() for q in qtext]\nques = []\n[ques.append(q) for q in qtext2 if q not in ques]\n\nprint(len(ques))\nques","d4ce3cbd":"# Dictionary of questions and their answers\nqans = {}\nqn = [re.split('_Part|_OTHER',c)[0] for c in dfo.columns]\nkeys = []\n[keys.append(q) for q in qn if q not in keys]\nfor k in keys:\n    if k in ['Q1','Q2','Q3']:\n        qans[k] = list(dfo[k].unique())\n    else:\n        if len([c for c in dfo.columns if c.startswith(k)]) == 1:\n            qans[k] = list(dfo[k].unique())\n        else:\n            quest = []\n            [quest.append(dfo.loc[0,c]) if len(quest) == 0 else None for c in dfo.columns if c.startswith(k)]\n            quest[0] = re.split('\\(Select all that apply\\)|- Selected Choice', quest[0])[0]\n            vals = [dfo[c].mode().values[0] for c in dfo.columns if c.startswith(k)]\n            quest = quest+vals\n            qans[k] = list(quest)","e8447896":"# Single-answer question\nqans['Q5']","e6404908":"# Multi-answer question\nqans['Q7']","5df146ee":"# Proportion of missing values in columns\npd.set_option('display.max_rows', dfo.shape[1])\n(dfo.loc[:,'Q1':'Q6'].isna().sum()*100 \/ dfo.shape[0]).round(2)","00ddb133":"# Remove the row with question text\ndf = dfo.drop(dfo.index[0])\ndf.head(1)","71d0efe5":"# List of unique questions\nqnums = list(dict.fromkeys([q.split('_')[0] for q in df.columns]))\nprint(qnums)","432e9826":"# Create dictionary\nqd = {}\nfor q in qnums:\n    if q in ['Q1','Q2','Q3']:    # not used c.startswith to prevent clubbing of columns Q10+, Q20 etc.\n        qd[q] = df[[q]]\n    else:\n        qd[q] = df[[c for c in df.columns if c.startswith(q)]]\n\n# Notebook referenced : 1","4fbee3b2":"# Age distribution\nwith warnings.catch_warnings():\n    warnings.simplefilter(\"ignore\")\n    fxn()\n    \n    fig, ax1 = plt.subplots(figsize=(13,5), facecolor='w')\n    q1_fd = fd_saq(qd['Q1'], 'Age groups', ordered=False)\n    labels = q1_fd['proportion']\n    ax1.bar(q1_fd['Age groups'], q1_fd['freq'], width=0.5, color='#37659E', edgecolor='teal', \n            linewidth=0.3)\n\n    for i in q1_fd.index:\n        ax1.annotate(f\"{labels[i]}%\", xy=(i, q1_fd['freq'][i] + 100), va='center',\n                    ha='center', fontfamily='serif', fontsize=9, color='#444444')\n\n    fig.text(0.40,0.95, 'Age Distribution', fontfamily='serif', fontweight='bold', fontsize=12)\n    ax1.set_ylim(0,4200)\n    ax1.set_xticklabels(q1_fd['Age groups'], fontfamily='serif', fontsize=9)\n    ax1.set_yticklabels(np.arange(0,4001,500),fontfamily='serif', fontsize=9)\n#     ax1.grid(axis='y', linestyle='-', alpha=0.2)\n    for s in ['top', 'left', 'right']:\n        ax1.spines[s].set_visible(False)\n\n# Notebook referenced : 2","ed626f65":"# Gender frequency distribution\nq2_fd = fd_saq(qd['Q2'], 'gender')\nq2_fd","aea01c2c":"# Pie chart\nfig = px.pie(q2_fd, values=q2_fd.freq, names=q2_fd.gender, title='Gender Distribution',\n             color_discrete_sequence=px.colors.sequential.RdBu_r,\n             hover_data=['proportion'], labels={'proportion':'proportion'})\nfig.update_traces(textposition='inside', textinfo='percent+label')\nfig.update_layout(autosize=False, width=600, height=400)\nfig.show()","bcaf9ea6":"# Create age subset for Man & Woman\ndf_ag = df[['Q1','Q2']].copy()\ndf_ag.loc[~df_ag.Q2.isin(['Man','Woman']),'Q2'] = 'Others'\nq1q2 = df_ag[df_ag['Q2'] != 'Others'].groupby(['Q2'])['Q1'].value_counts().unstack().sort_index()\nman = q1q2.loc['Man']\nwoman = q1q2.loc['Woman']","99db817c":"# Stacked bar chart\nfig, ax = plt.subplots(1,1,figsize=(13,5), facecolor='w')\nax.bar(man.index, man, width=0.5, label='Males', color='#C1D3E9')\nax.bar(woman.index, -woman, width=0.5, label='Females', color='#37659E')\nax.set_ylim(-1200,3500)\n\nfor i in man.index:\n    ax.annotate(f\"{man[i]}\", xy=(i,man[i]+100), va='center', ha='center', fontfamily='serif',\n                fontweight='light', fontsize=9, color='#444444')\n    \nfor i in woman.index:\n    ax.annotate(f\"{woman[i]}\", xy=(i,-woman[i]-100), va='center', ha='center', fontfamily='serif',\n                fontweight='light', fontsize=9, color='#444444')\n\nfor m in ['top','bottom','left','right']:\n    ax.spines[m].set_visible(False)\n\nfig.text(0.40,0.95, 'Gender-Age Distribution', fontfamily='serif', fontweight='bold', fontsize=12)\nax.set_yticks([])\nax.legend()\nplt.show()\n\n# Notebook referenced : 2","3f7d3229":"man\/woman","b896c435":"# Frequency Distribution of countries\nq3_fd = fd_saq(df['Q3'], 'country')\n\n# No. of countries\nprint(f\"Unique: {df['Q3'].nunique()}\")\n\n# Frequency Distribution of countries with respondents >300 (1.5% of sample)\nq3_fd[q3_fd['freq']>300]","5ed15b2e":"# Choropleth map\nfig = px.choropleth(q3_fd, locations='country', color='freq', locationmode='country names',\n                    color_continuous_scale='thermal', range_color = [0, 900])\nfig.show()\n\n# Notebook referenced : 3","dc3c7b8c":"# Subsets of data for the 2 top countries by respondents\ndfi = pd.DataFrame(df[df['Q3']=='India'])\ndfu = pd.DataFrame(df[df['Q3']=='United States of America'])\nprint(dfi.shape, dfu.shape)","9e5b3298":"# Import country continent dataset\n# source : https:\/\/www.kaggle.com\/statchaitya\/country-to-continent\ncontinents = pd.read_csv('\/kaggle\/input\/countrydataset\/countryContinent.csv', encoding = \"ISO-8859-1\")\ncontinents.head(1)","bd950b3a":"# Sub regions\nprint(continents['sub_region'].nunique())\nprint(continents['sub_region'].unique())\n\n# Region dictionary\nregion_dict = {k:v for k,v in zip(continents['country'], continents['sub_region'])}","97d1353e":"# Create separate dataframe\nq3_cont = pd.DataFrame({'country':df['Q3']})\n# Map continents\nq3_cont['continent'] = q3_cont['country'].map(continents.set_index('country')['continent'])\n# Map region\nq3_cont['sub_region'] = q3_cont['country'].map(region_dict)\nq3_cont.head(1)","f4d46dcf":"# Replace longer\/obscure names with shorter\/familiar names\ncountry_names_dict = {'United States of America':'USA',\n                      'United Kingdom of Great Britain and Northern Ireland':'UK',\n                      'Iran, Islamic Republic of...':'Iran',\n                      'Republic of Korea':'North Korea'}\n\nq3_cont['country'] = q3_cont['country'].replace(country_names_dict)","875eba4e":"# Unmapped countries\nprint(q3_cont.isna().sum())\nunmapped = q3_cont['country'][q3_cont['continent'].isna()].unique().tolist()\nunmapped","fca0df2c":"# Track unmapped countries in continent data\ncontinents[continents['country'].str.contains(r'Russia|Korea|Taiwan|Iran')]","5be78412":"# Continent \/ Sub-region dict for unmapped countries\nregion_dict2 = {'Russia':['Europe','Eastern Europe'], 'South Korea':['Asia','Easter Asia'],\n                'Taiwan':['Asia','Eastern Asia'], 'Iran':['Asia','Southern Asia'],\n                'North Korea':['Asia','Eastern Asia'], 'Other':['Other','Other']}","42f03b1e":"# Insert data for unmapped countries using region dict\nfor r in q3_cont[q3_cont['continent'].isna()].index:\n    q3_cont.loc[r, 'continent'] = region_dict2[q3_cont.loc[r,'country']][0]\n    q3_cont.loc[r, 'sub_region'] = region_dict2[q3_cont.loc[r,'country']][1]","ab7dbfa9":"# Unmapped countries\nprint(q3_cont.isna().sum())\nq3_cont['country'][q3_cont['continent'].isna()].unique()","11b91439":"# Confirm replacement\nq3_cont[q3_cont['country'].str.contains('Taiwan', na=False)].head(1)","2efbf4a4":"# Region-wise frequency distribution\nq3_cont_fd = fd_saq(q3_cont['sub_region'], title='sub_region')\n\n# Plotly bar graph\nfig = px.bar(q3_cont_fd, x='sub_region',y='freq', text='proportion', height=500, \n             title='Region-wise Distribution')\nfig.update_layout({'plot_bgcolor': 'rgba(0, 0, 0, 0)'})\nfig.update_traces(marker=dict(color='#37659E', opacity=1, line=dict(color='teal', width=0.3)))\nfig.update_traces(texttemplate='%{text:i}%', textposition='outside')\nfig.show()","3fd7c825":"qans['Q4']","fbbf2445":"# Setting custom order\nQ4_ans = [\"I prefer not to answer\",\"No formal education past high school\",\n          \"Some college\/university study without earning a bachelor\u2019s degree\",\n          \"Professional degree\", \"Bachelor\u2019s degree\",\"Master\u2019s degree\",\"Doctoral degree\"]\nQ4_ans_r = Q4_ans[::-1]\n\nq4_fd = fd_saq(df['Q4'],'education').set_index('education').loc[Q4_ans].reset_index()\nq4i_fd = fd_saq(df['Q4'],'education').set_index('education').loc[Q4_ans].reset_index()\nq4u_fd = fd_saq(dfu['Q4'],'education').set_index('education').loc[Q4_ans].reset_index()","9f2f7085":"# Bar plot\nfig = plt.figure(figsize=(6,6), facecolor='w') # create figure\ngs = fig.add_gridspec(2, 2)\ngs.update(wspace=0.3, hspace=0.3)\nax0 = fig.add_subplot(gs[0, 0:2])\nax1 = fig.add_subplot(gs[1, 0], ylim=(0, 3500)) # create axes\nax2 = fig.add_subplot(gs[1, 1]) # create axes\n\nxlbl = q4_fd['education'].values.tolist()\n\nax0.bar(q4_fd['education'], q4_fd['freq'], width=0.5, label=xlbl, \n        color=sns.color_palette('mako',len(q4_fd)))\nax1.bar(q4i_fd['education'], q4i_fd['freq'], width=0.5, label=xlbl, \n        color=sns.color_palette('mako',len(q4_fd)))\nax2.bar(q4u_fd['education'], q4u_fd['freq'], width=0.5, label=xlbl, \n        color=sns.color_palette('mako',len(q4_fd)))\n\nclrs = dict(zip(q4_fd['education'].values.tolist(), sns.color_palette('mako',len(q4_fd))))\nlabels = list(clrs.keys())\nhandles = [plt.Rectangle((0,0),1,1, color=clrs[label]) for label in labels]\nlabels = [str(i)+' '+l for i,l in zip(range(1,len(q4_fd)+1), labels)]\nlabels = ['\\n   '.join(wrap(l, 30)) for l in labels]\nax0.legend(handles, labels, loc=0, prop={'size':7}, frameon=False, \n           bbox_to_anchor=(1.1, 0.5), borderaxespad=0)\n# ax2.legend(handles, labels, loc=0, prop={'size':7}, frameon=False, \n#            bbox_to_anchor=(1.45, 1), borderaxespad=0)\n\nfig.text(0.31,0.95, 'Maximum Level of Education', fontfamily='serif', fontweight='bold',fontsize=10)\nax0.text(ax0.get_xlim()[1]*0.4, ax0.get_ylim()[1]*1.03,'World', fontfamily='serif', fontsize=9)\nax1.text(ax1.get_xlim()[1]*0.4, ax1.get_ylim()[1]*1.03,'India', fontfamily='serif', fontsize=9)\nax2.text(ax2.get_xlim()[1]*0.4, ax2.get_ylim()[1]*1.03,'USA*', fontfamily='serif', fontsize=9)\nfig.text(0.2,0.07, '*Please note that the scale of plots for USA is different from India', \n         fontfamily='serif',fontsize=7)\n\nfor s in [\"top\",\"right\",\"left\"]:\n    ax1.spines[s].set_visible(False)\n    ax2.spines[s].set_visible(False)\n    ax0.spines[s].set_visible(False)\n\n\nfor f in [ax0,ax1,ax2]:\n    f.set_xticklabels(range(1,len(q4_fd)+1), fontsize=6)\n    f.yaxis.set_tick_params(labelsize=6)\n    f.tick_params(bottom=False)\n\n# Notebook referenced : 4","f2b47cf9":"# WN 1 - Country-wise survey distribution\nq3_fd.head(2)","2696ef62":"# WN 2 - Distribution of Education\nq4_fd.iloc[::-1]","14ea0bef":"# WN 3 - Country-wise distribution of 'No formal education past high school'\nfd(qd['Q3'], qd['Q4'], 'high', contains=True).head()","1983835f":"# WN 4 - Country-wise distribution of 'Some college .... without Bachelor's'\nfd(qd['Q3'], qd['Q4'], 'without', contains=True).head()","259e762e":"# WN 5 - Country-wise distribution of 'Doctoral degree'\nfd(qd['Q3'], qd['Q4'], \"Doctoral\", contains=True).head()","3e8efc28":"# WN 6 - Country-wise distribution of 'Bachelor's degree'\nfd(qd['Q3'], qd['Q4'], \"Bachelor\", contains=True).head()","008ee478":"# WN 7 - Educational distribution without India\ncustom_sort2(fd(qd['Q4'], qd['Q3'],'India', otherthan1=True), 'criteria', Q4_ans_r)","3e8e9b22":"# Country-wise distribution of 'Master's degree'\nfd(qd['Q3'], qd['Q4'], 'Master', contains=True).head()","10bb4087":"# WN 8 - Educational Distribution - USA (with sample weights of classes in criteria)\ncustom_sort2(fd2(qd['Q4'], qd['Q3'], 'United States of America'), 'criteria', ordered_list=Q4_ans_r)","bdf8a93d":"# WN 9 - Educational distribution - India\ncustom_sort2(fd(qd['Q4'], qd['Q3'],'India'), 'criteria', Q4_ans_r)","78847936":"# Difference in internal educational distribution of India vis-a-vis USA\ndiff_ind_usa(qd['Q4'])","d6258b98":"Q5_ans = ['Student','Currently not employed','Other','Business Analyst','Data Analyst',\n          'Product\/Project Manager','Software Engineer','DBA\/Database Engineer','Data Engineer',\n          'Research Scientist','Machine Learning Engineer','Statistician','Data Scientist']\nQ5_ans_r = Q5_ans[::-1]\nq5_fd = fd_saq(df['Q5'],'job_role').set_index('job_role').loc[Q5_ans].reset_index()\nq5i_fd = fd_saq(dfi['Q5'],'job_role').set_index('job_role').loc[Q5_ans].reset_index()\nq5u_fd = fd_saq(dfu['Q5'],'job_role').set_index('job_role').loc[Q5_ans].reset_index()","cd4ac4c7":"# Bar plot\nfig = plt.figure(figsize=(6,6), facecolor='w')\ngs = fig.add_gridspec(2, 2)\ngs.update(wspace=0.3, hspace=0.3)\nax0 = fig.add_subplot(gs[0, 0:2])\nax1 = fig.add_subplot(gs[1, 0])\nax2 = fig.add_subplot(gs[1, 1])\n\nxlbl = q5_fd['job_role'].values.tolist()\n\nax0.bar(q5_fd['job_role'], q5_fd['freq'], width=0.5, label=xlbl, \n        color=sns.color_palette('mako',len(q5_fd)))\nax1.bar(q5i_fd['job_role'], q5i_fd['freq'], width=0.5, label=xlbl, \n        color=sns.color_palette('mako',len(q5_fd)))\nax2.bar(q5u_fd['job_role'], q5u_fd['freq'], width=0.5, label=xlbl, \n        color=sns.color_palette('mako',len(q5_fd)))\n\nclrs = dict(zip(q5_fd['job_role'].values.tolist(), sns.color_palette('mako',len(q5_fd))))\nlabels = list(clrs.keys())\nhandles = [plt.Rectangle((0,0),1,1, color=clrs[label]) for label in labels]\nlabels = [str(i)+' '+l for i,l in zip(range(1,len(q5_fd)+1), labels)]\nlabels = ['\\n'.join(wrap(l, 30)) for l in labels]\nax0.legend(handles, labels, loc=0, prop={'size':7}, frameon=False, \n           bbox_to_anchor=(1.1, 0.5), borderaxespad=0)\n\nfig.text(0.44,0.95, 'Job Role', fontfamily='serif', fontweight='bold',fontsize=10)\nax0.text(ax0.get_xlim()[1]*0.4, ax0.get_ylim()[1]*1.03,'World', fontfamily='serif', fontsize=9)\nax1.text(ax1.get_xlim()[1]*0.4, ax1.get_ylim()[1],'India', fontfamily='serif', fontsize=9)\nax2.text(ax2.get_xlim()[1]*0.4, ax2.get_ylim()[1],'USA*', fontfamily='serif', fontsize=9)\nfig.text(0.2,0.05, '*Please note that the scale of plots for USA is different from India', \n         fontfamily='serif',fontsize=7)\n\nfor s in [\"top\",\"right\",\"left\"]:\n    ax1.spines[s].set_visible(False)\n    ax2.spines[s].set_visible(False)\n    ax0.spines[s].set_visible(False)\n\nfor f in [ax0,ax1,ax2]:\n    f.set_xticklabels(range(1,len(q5_fd)+1), fontsize=6)\n    f.yaxis.set_tick_params(labelsize=6)\n    f.tick_params(bottom=False)","643eca3a":"# Distribution of Job-roles within sample (Overall, India, USA)\nbuild_graph(qd['Q5'], hgt=500, graph_title='Job-Role distribution')","68416ee6":"## India - Differential proportion compared with overall proportion of 0.292\ndiff = ((q5i_fd['freq']\/q5_fd['freq']) - 0.292).round(4)*100\nclrs = list(np.where(diff.values < 0, '#B51A62', '#37659E'))\n\n# Plot\nfig = go.Figure(go.Bar(x=q5_fd['job_role'], y=diff.values, text=diff.values.round(3), \n                       marker_color=clrs))\nfig.update_layout(title={'text':'India - Change in Job Role proportion compared with'+\n                        ' overall sample proportion (29.2%)<br>(in percentage points (%p))', \n                         'x':0.5, 'xanchor':'center'}, plot_bgcolor='#fff')\nfig.update_traces(texttemplate='%{text:i} %p', textposition='outside', textfont_size=9, \n                  textfont_color='black')\nfig.update_xaxes(categoryorder='array', categoryarray=q5i_fd['job_role'], visible=True)\nfig.show()\n\n\n## USA - Differential proportion compared with overall proportion of 0.112\ndiff = ((q5u_fd['freq']\/q5_fd['freq']) - 0.112).round(4)*100\nclrs = list(np.where(diff.values < 0, '#B51A62', '#37659E'))\n\n# Plot\nfig = go.Figure(go.Bar(x=q5_fd['job_role'], y=diff.values, text=diff.values.round(3), \n                       marker_color=clrs))\nfig.update_layout(title={'text':'USA - Change in Job Role proportion compared with'+\n                        ' Overall sample proportion (11.2%)<br>(in percentage points (%p))', \n                         'x':0.5, 'xanchor':'center'}, plot_bgcolor='#fff')\nfig.update_traces(texttemplate='%{text:i} %p', textposition='outside', textfont_size=9, \n                  textfont_color='black')\nfig.update_xaxes(categoryorder='array', categoryarray=q5i_fd['job_role'])\nfig.show()","7e48f0ba":"# Frequency distribution - Students\nfd(qd['Q3'], qd['Q5'], 'Student').head(2)","2f21c34f":"# Frequency distribution - Statistician\nfd(qd['Q3'], qd['Q5'], 'Statistician').head(3)","b641b728":"# Frequency distribution - Research Scientist\nfd(qd['Q3'], qd['Q5'], 'Research Scientist').head(2)","ec2af0d2":"# Frequency distribution - Product\/Project Manager\nfd(qd['Q3'], qd['Q5'], 'Product\/Project Manager').head(2)","ebadd36d":"# Frequency distribution - Data Engineer\nfd(qd['Q3'], qd['Q5'], 'Data Engineer').head(2)","92ef0ec4":"Q6_ans = ['I have never written code','< 1 years','1-2 years','3-5 years','5-10 years',\n          '10-20 years','20+ years']\nQ6_ans_r = Q6_ans[::-1]\nq6_fd = fd_saq(df['Q6'],'prog_exp').set_index('prog_exp').loc[Q6_ans].reset_index()\nq6i_fd = fd_saq(dfi['Q6'],'prog_exp').set_index('prog_exp').loc[Q6_ans].reset_index()\nq6u_fd = fd_saq(dfu['Q6'],'prog_exp').set_index('prog_exp').loc[Q6_ans].reset_index()","cd416308":"# Bar plot\nfig = plt.figure(figsize=(6,6), facecolor='w')\ngs = fig.add_gridspec(2, 2)\ngs.update(wspace=0.3, hspace=0.35)\nax0 = fig.add_subplot(gs[0, 0:2])\nax1 = fig.add_subplot(gs[1, 0])\nax2 = fig.add_subplot(gs[1, 1])\n\nxlbl = q6_fd['prog_exp'].values.tolist()\n\nax0.bar(q6_fd['prog_exp'], q6_fd['freq'], width=0.5, label=xlbl, \n        color=sns.color_palette('mako',len(q6_fd)))\nax1.bar(q6i_fd['prog_exp'], q6i_fd['freq'], width=0.5, label=xlbl, \n        color=sns.color_palette('mako',len(q6_fd)))\nax2.bar(q6u_fd['prog_exp'], q6u_fd['freq'], width=0.5, label=xlbl, \n        color=sns.color_palette('mako',len(q6_fd)))\n\nclrs = dict(zip(q6_fd['prog_exp'].values.tolist(), sns.color_palette('mako',len(q6_fd))))\nlabels = list(clrs.keys())\nhandles = [plt.Rectangle((0,0),1,1, color=clrs[label]) for label in labels]\nax0.legend(handles, labels, loc=0, prop={'size':7}, frameon=False, \n           bbox_to_anchor=(1.1, 0.5), borderaxespad=0)\n\nfig.text(0.35,0.95, 'Programming Experience', fontfamily='serif', fontweight='bold',fontsize=10)\nax0.text(ax0.get_xlim()[1]*0.4, ax0.get_ylim()[1]*1.03,'World', fontfamily='serif', fontsize=9)\nax1.text(ax1.get_xlim()[1]*0.4, ax1.get_ylim()[1],'India', fontfamily='serif', fontsize=9)\nax2.text(ax2.get_xlim()[1]*0.4, ax2.get_ylim()[1],'USA*', fontfamily='serif', fontsize=9)\nfig.text(0.2,0.02, '*Please note that the scale of plots for USA is different from India', \n         fontfamily='serif',fontsize=7)\n\nfor s in [\"top\",\"right\",\"left\"]:\n    ax1.spines[s].set_visible(False)\n    ax2.spines[s].set_visible(False)\n    ax0.spines[s].set_visible(False)\n\nfor f in [ax0,ax1,ax2]:\n    f.set_xticklabels(['Never']+labels[1:], fontsize=6, rotation=45)\n    f.yaxis.set_tick_params(labelsize=6)\n    f.tick_params(bottom=False)","b9a04b63":"a,b = differential_graph(qd['Q6'], order=Q6_ans)\na.update_layout(autosize=False, width=850, height=450)\nb.update_layout(autosize=False, width=850, height=450)\na.show()\nb.show()","009eb59c":"# Frequency distribution - World\nq6_fd","e608e484":"# Frequency distribution - India\nq6i_fd","e32e6ae1":"# Frequency distribution - USA\nq6u_fd","c8e25070":"# Heatmap\nfig = px.density_heatmap(df, x='Q5',y='Q4', color_continuous_scale='Blues',\n                         category_orders={'Q5':Q5_ans, 'Q4':Q4_ans_r})\nfig.update_layout(title={'text':'Education & Job Role', 'x':0.68, 'xanchor': 'center'})\nfig.update_layout(autosize=False, width=990, height=500)\nfig.show()","08b3c1d5":"# Shortened names\nrole_dict = {'Doctoral degree': 'Doctoral degree',\n             'Master\u2019s degree': 'Master\u2019s degree',\n             'Bachelor\u2019s degree': 'Bachelor\u2019s degree',\n             'Professional degree': 'Professional degree',\n             'Some college\/university study without earning a bachelor\u2019s degree': 'Study without degree',\n             'No formal education past high school': 'High school',\n             'I prefer not to answer': 'No answer'}","c0175e84":"# Percentage Distribution\nedu_role1 = pd.DataFrame()\nedu_role1['Overall'] = df['Q4'].value_counts(normalize=True, sort=False)\nfor role in Q5_ans:\n    edu_role1[role] = df[df.Q5==role]['Q4'].value_counts(normalize=True, sort=False)\nedu_role = edu_role1.reindex(Q4_ans_r, copy=True)\nedu_role.round(4)*100","575993e5":"# Stacked Bar Chart\nfig = go.Figure()\nclrs = ['rgba'+str(clr) for clr in sns.color_palette('ocean_r',len(edu_role.index))]\nclrs1 = sns.color_palette('ocean_r', len(edu_role.index))\n\n# Subplot for color key\ncolorscale = ff.create_annotated_heatmap(z=[[1,2,3,4,5,6,7]],\n                annotation_text=[[\"<span style='font-size:12px; font-family: Tahoma'>\"+text+\"<\/span>\" \n                    for text in [re.sub('\\n','<br>',fill(role_dict[w],14)) for w in edu_role.index]]],\n                colorscale=clrs,\n                font_colors = ['white','white','white','white','white','white','white'],\n                xgap = 0.05, showscale = False)\n\ntrcs = ['trace'+str(i) for i in range(1,len(edu_role.index)+1)]\ni = 0\nfor trc in trcs:\n    globals()[trc] = go.Bar(y=edu_role.columns, x=edu_role.iloc[i,:], orientation=\"h\",\n                            name=edu_role.index[i], marker=dict(color=clrs[i]))\n    i += 1\n\nfig = subplots.make_subplots(rows=2, cols=1, shared_yaxes=True, shared_xaxes=False, \n                             horizontal_spacing = 0, vertical_spacing = 0.01, \n                             row_heights=[0.18, 0.82])\nfig.append_trace(colorscale.data[0],1,1)\n\nfor trc in trcs:\n    fig.append_trace(globals()[trc],2,1)\n\norder = edu_role.loc['Doctoral degree'].sort_values(ascending=True).keys().tolist()\norder.remove('Overall')\nfig.update_layout(\n    title={'text':'Education & Job Role - Percentage Distribution', 'x':0.57,'xanchor': 'center'},\n    yaxis2={'categoryorder':'array', 'categoryarray':order+['Overall']},\n    barmode=\"relative\", bargap = 0.05,\n    plot_bgcolor = '#fff',\n    xaxis = dict(title=\"<span style='font-size:13px; font-family:Helvetica'><b>Color Keys: <\/b>Educational qualifications by job role<\/span>\", \n                 side=\"top\",title_standoff=0, domain=[0,0.95], showticklabels = False),\n    yaxis = dict(domain=[0.85,1], showticklabels = False),\n    xaxis2 = dict(domain=[0,1], tickformat = '%'),\n    legend=dict(orientation=\"h\"), showlegend=False,\n    autosize=False, width=850, height=600,\n    margin=dict(l=0, r=0, b=0, pad=3))\n\n# Workaround to show annotations with ff.create_annotated_heatmap() subplots.\nannot1 = list(colorscale.layout.annotations)\nfor k in range(len(annot1)):\n    annot1[k]['xref'] = 'x'\n    annot1[k]['yref'] = 'y'\nfig.update_layout(annotations=annot1) \n\nfig.show()\n\n# Notebook referenced : 5","68c64e42":"# Role-wise distribution of Educational Qualifications\nbuild_graph(qd['Q4'], qd['Q5'], 'Distribution of Educational Qualifications', order=Q4_ans_r, hgt=550)","5eae1e0a":"# Heatmap\nfig = px.density_heatmap(df, x='Q6',y='Q4', color_continuous_scale='Blues',\n                         category_orders={'Q4':Q4_ans_r, 'Q6':Q6_ans})\nfig.update_layout(title={'text':'Education & Coding Experience', 'x':0.68, 'xanchor': 'center'})\nfig.show()","71b025ec":"# Heatmap\nfig = px.density_heatmap(df, x='Q6',y='Q5', color_continuous_scale='Blues',\n                         category_orders={'Q5':Q5_ans, 'Q6':Q6_ans})\nfig.update_layout(title={'text':'Job role & Coding Experience', 'x':0.58, 'xanchor': 'center'})\nfig.update_layout(autosize=False, width=800, height=550)\nfig.update_yaxes(dict(ticks = \"outside\", tickcolor='white', ticklen=0))\nfig.update_xaxes(domain=[0.2, 1])\nfig.show()","51fef47f":"# Comparitive bar chart (interactive)\nbuild_graph(qd['Q6'], qd['Q5'], 'Coding Experience', order=Q6_ans, label_angle=-10, hgt=450)","1eca90d0":"# Frequency distribution for Programming Experience - Data Scientist\ncustom_sort2(fd(qd['Q6'], qd['Q5'], 'Data Scientist', qd['Q3'], 'India'), 'criteria', Q6_ans)","abaf59e3":"# Frequency distribution for Programming Experience - Data Scientist\ncustom_sort2(fd(qd['Q6'], qd['Q5'], 'Data Scientist', qd['Q3'], 'United States of America'), \n             'criteria', Q6_ans)","1e5df1b4":"# Frequency distribution\nq7_fd = fd(qd['Q7'], title='language')\nq7i_fd = fd(qd['Q7'], qd['Q3'], 'India', title='language')\nq7u_fd = fd(qd['Q7'], qd['Q3'], 'United States of America', title='language')","88e55162":"# Bar chart with plotly go\nfig = go.Figure([go.Bar(x = q7_fd.language, y = q7_fd.freq, marker_color='#37659E',\n                       marker_line_color='teal',marker_line_width=0.3, opacity=1, text=q7_fd.freq)])\nfig.update_layout(title={'text':'Regularly Used Programming Language', 'x':0.5, 'xanchor': 'center'})\nfig.update_layout(autosize=False, width=750, height=480)\nfig.update_layout({'plot_bgcolor': '#fff'})\nfig.update_traces(texttemplate='%{text:.2}', textposition='outside', textfont_size=9)\nfig.show()","38e28c5e":"# Comparitive bar plot - Overall v India v USA\nbuild_graph(qd['Q7'], hgt=450, graph_title='Regularly Used Programming Language')","7b844d16":"# Role-wise distribution of preferred programming language\nbuild_graph(qd['Q7'], qd['Q5'], 'Preferred Programming Language', hgt=500, label_angle=0)","110f387f":"fd(qd['Q7'], qd['Q5'], 'Data Scientist')","b9783eaf":"fd2(qd['Q5'], qd['Q7']['Q7_Part_1'], 'Python')","3981a82c":"# Interactive bar plot\nbuild_graph(qd['Q8'], qd['Q5'], hgt=500, \n            graph_title='Language Recommended for Initiation to Aspiring Data Scientists')","9b65010d":"# Interactive Bar plot\nbuild_graph(qd['Q9'], qd['Q5'], graph_title='IDEs Used Regularly', hgt=500, label_angle=15, \n            ascending=True)","50180c78":"# Interactive Bar Chart\nbuild_graph(qd['Q10'], qd['Q5'], graph_title='Regularly used Hosted Notebooks', hgt=480, \n            label_angle=15, ascending=True)","31ee9f0e":"qans['Q11']","17d65d0f":"# Interactive Bar Chart\nbuild_graph(qd['Q11'], qd['Q5'], graph_title='Most Used Computing Platform', hgt=550, \n            label_angle=10, ascending=True)","a51c9e27":"fd2(qd['Q5'], qd['Q11'], 'deep', contains=True)","93b5d107":"qans['Q12']","3cb35da4":"# Interactive Bar Chart\nbuild_graph(qd['Q12'], qd['Q5'], graph_title='Specialized Hardware Regularly Used', hgt=480, \n            label_angle=0, ascending=True)","a8d473e6":"qans['Q13']","59d1332f":"Q13_ans = ['Never','Once','2-5 times','6-25 times','More than 25 times']\n\n# Interactive Bar Chart\nbuild_graph(qd['Q13'], graph_title='TPU Usage in life', hgt=450, label_angle=0, order=Q13_ans)","c4618d33":"qans['Q14']","3a2deab7":"# Interactive Bar Chart\nbuild_graph(qd['Q14'], qd['Q5'], graph_title='Visualization Libraries used Regularly', hgt=500, \n            ascending=True)","71b343e2":"qans['Q15']","b9654131":"Q15_ans = ['I do not use machine learning methods','Under 1 year','1-2 years','2-3 years','3-4 years',\n           '4-5 years','5-10 years','10-20 years''20 or more years']","4ad47833":"# Interactive Bar Chart\nbuild_graph(qd['Q15'], qd['Q5'], graph_title='Machine Learning Methods Usage Time', hgt=480, \n            label_angle=15, order=Q15_ans)","b2d31a1e":"Q15_ans = ['I do not use machine learning methods','Under 1 year','1-2 years','2-3 years','3-4 years',\n          '4-5 years','5-10 years','10-20 years','20 or more years']\n\n# Interactive Bar Chart\nbuild_graph(qd['Q15'], qd['Q5'], graph_title='Machine Learning Methods Usage Time', hgt=480, \n            label_angle=15, order=Q15_ans)","0efa2426":"# Interactive Bar Chart\nbuild_graph(qd['Q16'], qd['Q5'], graph_title='Machine Learning Frameworks Regularly Used', hgt=480, \n            label_angle=15)","7de0d258":"# Bar Chart\nbuild_graph(qd['Q17'], graph_title='ML Algorithms Used Regularly', hgt=500, label_angle=15)\n\n# Bar Chart - Data Scientists\nbuild_graph(create_subset(qd['Q17'], qd['Q5'], 'Data Scientist'), label_angle=15, hgt=500,\n            graph_title='Data Scientists - Machine Learning Algorithms Regularly Used')","93f8eacb":"# Bar Chart\nbuild_graph(qd['Q18'], graph_title='Computer Vision Methods Used Regularly', hgt=480, \n            label_angle=12, adjust_margin=False, xmargin=120)\n\n# Bar Chart - Data Scientists\nbuild_graph(create_subset(qd['Q18'], qd['Q5'], 'Data Scientist'), label_angle=12, hgt=480,\n            graph_title='Data Scientists - Computer Vision Methods Used Regularly', \n            adjust_margin=False, xmargin=120)","08b7c616":"diff_ind_usa(qd['Q18'], qd['Q5'], 'Data Scientist')","092c258f":"qans['Q19']","c7a9ff15":"# Bar Chart\nbuild_graph(qd['Q19'], graph_title='NLP Methods Used Regularly', hgt=480, label_angle=12)\n\n# Bar Chart - Data Scientists\nbuild_graph(create_subset(qd['Q19'], qd['Q5'], 'Data Scientist'), label_angle=12, hgt=480,\n            graph_title='Data Scientists - NLP Methods Used Regularly')","21ee1ab9":"# Order\nQ20_ans = ['0-49 employees','50-249 employees','250-999 employees','1000-9,999 employees',\n           '10,000 or more employees']\n\n# Bar Chart\nbuild_graph(qd['Q20'], graph_title='Company Size', label_angle=15, hgt=400, order=Q20_ans)\n\n# Bar Chart - Data Scientists\nbuild_graph(create_subset(qd['Q20'], qd['Q5'], 'Data Scientist'), label_angle=15, hgt=400,\n            graph_title='Data Scientists - Company Size', order=Q20_ans)","13ba41cc":"custom_sort(diff_ind_usa(qd['Q20'], qd['Q5'], 'Data Scientist').reset_index(), 'criteria', Q20_ans)","4f1de4d7":"# Order\nQ21_ans = ['0','1-2','3-4','5-9','10-14','15-19','20+']\n\n# Bar Chart\nbuild_graph(qd['Q21'], graph_title='Individuals Engaged in Data Science Work at Workplace', \n            label_angle=0, hgt=420, order=Q21_ans)\n\n# Bar Chart - Data Scientists\nbuild_graph(create_subset(qd['Q21'], qd['Q5'], 'Data Scientist'), label_angle=0,hgt=420, order=Q21_ans,\n            graph_title='Data Scientists - Individuals Engaged in Data Science Work at Workplace')","2e7e363d":"custom_sort(diff_ind_usa(qd['Q21'], qd['Q5'], 'Data Scientist').reset_index(), 'criteria', Q21_ans)","25e8a92d":"Q22_ans = ['I do not know',\n           'No (we do not use ML methods)',\n           'We use ML methods for generating insights (but do not put working models into production)',\n           'We are exploring ML methods (and may one day put a model into production)', \n           'We recently started using ML methods (i.e., models in production for less than 2 years)',\n           'We have well established ML methods (i.e., models in production for more than 2 years)']","970d878e":"# Bar Chart\nbuild_graph(qd['Q22'], graph_title='ML Methods Incorporation in Business', label_angle=15, hgt=500, \n            order=Q22_ans, adjust_margin=False, xmargin=150)\n\n# Bar Chart - Data Scientists\nbuild_graph(create_subset(qd['Q22'], qd['Q5'], 'Data Scientist'), label_angle=15, hgt=500,\n            graph_title='Data Scientists - ML Methods Incorporation in Business', order=Q22_ans,\n            adjust_margin=False, xmargin=150)","35e76dde":"qans['Q23']","1e1ea759":"# Bar Chart\nbuild_graph(qd['Q23'], graph_title='Important Job Role Activities', label_angle=15, hgt=480,\n            adjust_margin=False, xmargin=150)\n\n# Bar Chart - Data Scientists\nbuild_graph(create_subset(qd['Q23'], qd['Q5'], 'Data Scientist'), label_angle=15, hgt=480,\n            graph_title='Data Scientists - Important Job Role Activities', \n            adjust_margin=False, xmargin=150)","70a17162":"Q24_ans = ['$0-999','1,000-1,999','2,000-2,999','3,000-3,999','4,000-4,999','5,000-7,499',\n           '7,500-9,999','10,000-14,999','15,000-19,999','20,000-24,999','25,000-29,999',\n           '30,000-39,999','40,000-49,999','50,000-59,999','60,000-69,999','70,000-79,999',\n           '80,000-89,999','90,000-99,999','150,000-199,999','200,000-249,999',\n           '250,000-299,999','300,000-500,000','> $500,000']","0bb92864":"# Bar Chart\nbuild_graph(qd['Q24'], graph_title='Current Yearly Compensation', \n            label_angle=-25, hgt=420, order=Q24_ans, wd=960)\n\n# Bar Chart - Data Scientists\nbuild_graph(create_subset(qd['Q24'], qd['Q5'], 'Data Scientist'),\n            graph_title='Data Scientists - Current Yearly Compensation',\n            label_angle=-25, hgt=420, order=Q24_ans, wd=960)","445cbcef":"qans['Q25']","13ddf8a8":"Q25_ans1 = ['$0 ($USD)','$1-$99','$100-$999','$1000-$9,999','$10,000-$99,999',\n           '$100,000 or more ($USD)']\nQ25_ans = [s.replace('$','') for s in Q25_ans1]\nprint(Q25_ans)","4b3580c1":"# Bar Chart\nq25_df = qd['Q25'].replace({'\\$':''}, regex = True)\nbuild_graph(q25_df, graph_title='Money Spent on ML \/ Cloud-computing Services (past 5 yrs)', \n            label_angle=-25, hgt=420, order=Q25_ans)\n\n# Bar Chart - Data Scientists\nbuild_graph(create_subset(q25_df, qd['Q5'], 'Data Scientist'),\n            graph_title='Data Scientists - Money Spent on ML \/ Cloud-computing Services (past 5 yrs)',\n            label_angle=-25, hgt=420, order=Q25_ans)","ccd699dc":"qans['Q26_A']","02387742":"# Bar Chart\nbuild_graph(qd['Q26'].iloc[:,0:12], graph_title='Cloud-computing Platform Used Regularly', \n            label_angle=17, hgt=420)\n\n# Bar Chart - Data Scientists\nbuild_graph(create_subset(qd['Q26'].iloc[:,0:12], qd['Q5'], 'Data Scientist'),\n            graph_title='Data Scientists - Cloud-computing Platform Used Regularly',\n            label_angle=17, hgt=420)","681cd4c1":"# Bar Chart\nbuild_graph(qd['Q26'].iloc[:,12:], graph_title='Cloud-computing Platform to Familiarize with (within next 2 yrs)', \n            label_angle=17, hgt=420)\n\n# Bar Chart - Data Scientists\nbuild_graph(create_subset(qd['Q26'].iloc[:,12:], qd['Q5'], 'Data Scientist'),\n            graph_title='Data Scientists - Cloud-computing Platform to Familiarize with (within next 2 yrs)',\n            label_angle=17, hgt=420)","f78cd67d":"qans['Q27_A']","8d51fa2c":"# Bar Chart\nbuild_graph(qd['Q27'].iloc[:,0:12], graph_title='Cloud-computing Products Used Regularly', \n            label_angle=17, hgt=420)\n\n# Bar Chart - Data Scientists\nbuild_graph(create_subset(qd['Q27'].iloc[:,0:12], qd['Q5'], 'Data Scientist'),\n            graph_title='Data Scientists - Cloud-computing Products Used Regularly',\n            label_angle=17, hgt=420)","52ea8724":"# Bar Chart\nbuild_graph(qd['Q27'].iloc[:,12:], graph_title='Cloud-computing Products to Familiarize with (within next 2 yrs)', \n            label_angle=17, hgt=420)\n\n# Bar Chart - Data Scientists\nbuild_graph(create_subset(qd['Q27'].iloc[:,12:], qd['Q5'], 'Data Scientist'),\n            graph_title='Data Scientists - Cloud-computing Products to Familiarize with (within next 2 yrs)',\n            label_angle=17, hgt=420)","f716caec":"qans['Q28_A']","17d1eb2b":"# Bar Chart\nbuild_graph(qd['Q28'].iloc[:,0:11], graph_title='Machine Learning Products Used Regularly', \n            label_angle=17, hgt=420)\n\n# Bar Chart - Data Scientists\nbuild_graph(create_subset(qd['Q28'].iloc[:,0:11], qd['Q5'], 'Data Scientist'),\n            graph_title='Data Scientists - Machine Learning Products Used Regularly',\n            label_angle=17, hgt=420)","d2e2ec0a":"# Bar Chart\nbuild_graph(qd['Q28'].iloc[:,11:], graph_title='Machine Learning Products to Familiarize with (next 2 yrs)', \n            label_angle=17, hgt=420)\n\n# Bar Chart - Data Scientists\nbuild_graph(create_subset(qd['Q28'].iloc[:,11:], qd['Q5'], 'Data Scientist'),\n            graph_title='Data Scientists - Machine Learning Products to Familiarize with (next 2 yrs)',\n            label_angle=17, hgt=420)","2f14ad6d":"qans['Q29_A']","a7016662":"Q29_ans = ['MySQL ','PostgresSQL ','SQLite ','Oracle Database ','MongoDB ','Snowflake ','IBM Db2 ',\n           'Microsoft SQL Server ','Microsoft Access ','Microsoft Azure Data Lake Storage ',\n           'Amazon Redshift ','Amazon Athena ','Amazon DynamoDB ','Google Cloud BigQuery ',\n           'Google Cloud SQL ','Google Cloud Firestore ','None','Other']","3233ca37":"# Bar Chart\nbuild_graph(qd['Q29'].iloc[:,0:18], graph_title='Big Data Products Used Regularly', \n            label_angle=17, hgt=420, order=Q29_ans)\n\n# Bar Chart - Data Scientists\nbuild_graph(create_subset(qd['Q29'].iloc[:,0:18], qd['Q5'], 'Data Scientist'),\n            graph_title='Data Scientists - Big Data Products Used Regularly',\n            label_angle=17, hgt=420, order=Q29_ans)","d0b64dfb":"# Bar Chart\nbuild_graph(qd['Q29'].iloc[:,18:], graph_title='Big Data Products to Familiarize with (next 2 yrs)', \n            label_angle=17, hgt=420, order=Q29_ans)\n\n# Bar Chart - Data Scientists\nbuild_graph(create_subset(qd['Q29'].iloc[:,18:], qd['Q5'], 'Data Scientist'),\n            graph_title='Data Scientists - Big Data Products to Familiarize with (next 2 yrs)',\n            label_angle=17, hgt=420, order=Q29_ans)","f1828d49":"qans['Q30']","741bd0b0":"# Bar Chart\nbuild_graph(qd['Q30'], graph_title='Big Data Products Most Used', \n            label_angle=17, hgt=420)\n\n# Bar Chart - Data Scientists\nbuild_graph(create_subset(qd['Q30'], qd['Q5'], 'Data Scientist'),\n            graph_title='Data Scientists - Big Data Products Most Used',\n            label_angle=17, hgt=420)","7bbbc0a8":"qans['Q31_A']","4a720284":"Q31_ans = ['Amazon QuickSight','Microsoft Power BI','Google Data Studio','Looker','Tableau',\n           'Salesforce','Einstein Analytics','Qlik','Domo','TIBCO Spotfire','Alteryx ','Sisense ',\n           'SAP Analytics Cloud ','Other','None']","55785507":"# Bar Chart\nbuild_graph(qd['Q31'].iloc[:,0:15], graph_title='Business Intelligence Tools Used Regularly', \n            label_angle=17, hgt=420, order=Q31_ans)\n\n# Bar Chart - Data Scientists\nbuild_graph(create_subset(qd['Q31'].iloc[:,0:15], qd['Q5'], 'Data Scientist'),\n            graph_title='Data Scientists - Business Intelligence Tools Used Regularly',\n            label_angle=17, hgt=420, order=Q31_ans)","c16196b7":"# Bar Chart\nbuild_graph(qd['Q31'].iloc[:,15:], graph_title='Business Intelligence Tools to Familiarize with (next 2 yrs)', \n            label_angle=17, hgt=420, order=Q31_ans)\n\n# Bar Chart - Data Scientists\nbuild_graph(create_subset(qd['Q31'].iloc[:,15:], qd['Q5'], 'Data Scientist'),\n            graph_title='Data Scientists - Business Intelligence Tools to Familiarize with (next 2 yrs)',\n            label_angle=17, hgt=420, order=Q31_ans)","5dc72499":"# Bar Chart\nbuild_graph(qd['Q32'], graph_title='Business Intelligence Tools Most Used', \n            label_angle=17, hgt=420, order=Q31_ans)\n\n# Bar Chart - Data Scientists\nbuild_graph(create_subset(qd['Q32'], qd['Q5'], 'Data Scientist'),\n            graph_title='Data Scientists - Business Intelligence Tools Most Used',\n            label_angle=17, hgt=420, order=Q31_ans)","add2533b":"qans['Q33_A']","97df5500":"# Bar Chart\nbuild_graph(qd['Q33'].iloc[:,0:8], graph_title='Category of Automated ML Tools Used Regularly', \n            label_angle=15, hgt=500, adjust_margin=False, xmargin=150)\n\n# Bar Chart - Data Scientists\nbuild_graph(create_subset(qd['Q33'].iloc[:,0:8], qd['Q5'], 'Data Scientist'),\n            graph_title='Data Scientists - Category of Automated ML Tools Used Regularly',\n            label_angle=15, hgt=500, adjust_margin=False, xmargin=150)","7c3b5e47":"# Bar Chart\nbuild_graph(qd['Q33'].iloc[:,8:], graph_title='Category of Automated ML Tools to Familiarize with (next 2 yrs)', \n            label_angle=15, hgt=500, adjust_margin=False, xmargin=150)\n\n# Bar Chart - Data Scientists\nbuild_graph(create_subset(qd['Q33'].iloc[:,8:], qd['Q5'], 'Data Scientist'),\n            graph_title='Data Scientists - Category of Automated ML Tools to Familiarize with (next 2 yrs)',\n            label_angle=15, hgt=500, adjust_margin=False, xmargin=150)","6cab89f0":"qans['Q34_A']","01770a3d":"# Bar Chart\nbuild_graph(qd['Q34'].iloc[:,0:12], graph_title='Automated ML Tools Used Regularly', \n            label_angle=17, hgt=420)\n\n# Bar Chart - Data Scientists\nbuild_graph(create_subset(qd['Q34'].iloc[:,0:12], qd['Q5'], 'Data Scientist'),\n            graph_title='Data Scientists - Automated ML Tools Used Regularly',\n            label_angle=17, hgt=420)","c6d5d76d":"# Bar Chart\nbuild_graph(qd['Q34'].iloc[:,12:], graph_title='Automated ML Tools to Familiarize with (next 2 yrs)', \n            label_angle=17, hgt=420)\n\n# Bar Chart - Data Scientists\nbuild_graph(create_subset(qd['Q34'].iloc[:,12:], qd['Q5'], 'Data Scientist'),\n            graph_title='Data Scientists - Automated ML Tools to Familiarize with (next 2 yrs)',\n            label_angle=17, hgt=420)","9e74164d":"# Bar Chart\nbuild_graph(qd['Q35'].iloc[:,0:11], graph_title='ML Experiments Mangement Tools Used', \n            label_angle=15, hgt=420)\n\n# Bar Chart - Data Scientists\nbuild_graph(create_subset(qd['Q35'].iloc[:,0:11], qd['Q5'], 'Data Scientist'),\n            graph_title='Data Scientists - ML Experiments Mangement Tools Used',\n            label_angle=15, hgt=420)","3830c123":"# Bar Chart\nbuild_graph(qd['Q35'].iloc[:,11:], graph_title='ML Experiment Mangement Tools to Familiarize with (next 2 yrs)', \n            label_angle=15, hgt=420)\n\n# Bar Chart - Data Scientists\nbuild_graph(create_subset(qd['Q35'].iloc[:,11:], qd['Q5'], 'Data Scientist'),\n            graph_title='Data Scientists - ML Experiment Mangement Tools to Familiarize with (next 2 yrs)',\n            label_angle=15, hgt=420)","424e6fd1":"qans['Q36']","d995ea3f":"# Bar Chart\nbuild_graph(qd['Q36'], graph_title='Platforms used for Sharing Data Science Project \/ Applications', \n            label_angle=15, hgt=400)\n\n# Bar Chart - Data Scientists\nbuild_graph(create_subset(qd['Q36'], qd['Q5'], 'Data Scientist'),\n            graph_title='Data Scientists - Platforms used for Sharing Data Science Project \/ Applications',\n            label_angle=15, hgt=400)","9cf685ac":"qans['Q37']","bb3974a9":"# Bar Chart\nbuild_graph(qd['Q37'], graph_title='Platforms used for Data Science Learning', label_angle=15, \n            hgt=500, adjust_margin=False, xmargin=100)\n\n# Bar Chart - Data Scientists\nbuild_graph(create_subset(qd['Q37'], qd['Q5'], 'Data Scientist'), label_angle=15,\n            graph_title='Data Scientists - Platforms used for Data Science Learning',\n            hgt=500, adjust_margin=False, xmargin=100)","c6ad5a7f":"qans['Q38']","0e8d5568":"# Bar Chart\nbuild_graph(qd['Q38'], graph_title='Primary Tool for Data Analysis', label_angle=15, \n            hgt=500, adjust_margin=False, xmargin=150)\n\n# Bar Chart - Data Scientists\nbuild_graph(create_subset(qd['Q38'], qd['Q5'], 'Data Scientist'), label_angle=15,\n            graph_title='Data Scientists - Primary Tool for Data Analysis',\n            hgt=500, adjust_margin=False, xmargin=150)","1f54d2cb":"qans['Q39']","46f30cbe":"# Bar Chart\nbuild_graph(qd['Q39'], graph_title='Favorite Media Sources for Data Science', label_angle=15, \n            hgt=500, adjust_margin=False, xmargin=150)\n\n# Bar Chart - Data Scientists\nbuild_graph(create_subset(qd['Q39'], qd['Q5'], 'Data Scientist'), label_angle=15,\n            graph_title='Data Scientists - Favorite Media Sources for Data Science',\n            hgt=500, adjust_margin=False, xmargin=150)","c384728d":"### Q17 - ML Algorithms","6694e6bf":"<div class=\"alert alert-block alert-info\">\n<a id='obs-q5'><\/a>\n<b>Observations:<\/b><br>\n    - Students comprise the largest group in the survey sample (27%) and Data Scientists the 2nd largest (14%).<br>\n    - In this criteria, the difference between the distributions for India and USA is even more evident.<br><br> \n    <b>India<\/b><br>\n    - India's overall proportion in the survey sample is 29%. <br>\n    - India's proportion significantly changes from its  overall proportion in the roles of:<br>\n    &emsp; <i>Significantly more:<\/i><br>\n    &emsp;\u2022 Student : 43%<br>\n    &emsp; <i>Significantly less:<\/i><br>\n    &emsp;\u2022 Statistician : 12%<br>\n    &emsp;\u2022 Research Scientist : 12%<br>\n    &emsp;\u2022 Data Engineer : 11%<br>\n    &emsp;\u2022 Product\/Project Manager : 11%<br><br>\n    <b>USA<\/b><br>\n    - USA's overall proportion in the survey sample is 11%. <br>\n    - USA's proportion is significantly different in the following roles:<br>\n    &emsp; <i>Significantly more:<\/i><br>\n    &emsp;\u2022 Product\/Project Manager : 17%<br>\n    &emsp;\u2022 Data Engineer : 16%<br>\n    &emsp; <i>Significantly less:<\/i><br>\n    &emsp;\u2022 Student : 7%<br>\n\n<i>Workings below<\/i>\n<\/div>","7feba773":"### Q29 - Big Data Products","6ae4841d":"###### ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------","19d6266f":"#### 33A - Automated ML Tools Category","533e4575":"[back to top](#toc)","fc4ddefb":"###### ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------","4ae6efe9":"[back to top](#toc)","3b03fc93":"###### ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------","980ea751":"###### ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------","3edca84c":"#### 27A - Product used regularly","89b9dd3b":"[back to top](#toc)","677798d7":"###### ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------","327f6bfa":"### 2. Import data","0a706396":"<div class=\"alert alert-block alert-info\">\n<a id='obs-q14'><\/a>\n<b>Observations:<\/b><br>\n    - <b>Overall<\/b> : Matplotlib (75%), Seaborn (54%), Plotly (25%) and ggplot2 (25%).<br>\n    -  <b>India<\/b> : Matplotlib (83%), Seaborn (64%), Plotly (28%) and ggplot2 (23%).<br>\n    -  <b>USA<\/b> : Matplotlib (67%), Seaborn (47%), ggplot2 (34%) and Plotly (27%).<br>\n    &emsp; Higher ggplot\/ggplot2 usage in USA is commensurate with the fact that ggplot is an R package, which has a higher usage in USA.<br>\n    -  <b>Data Scientists<\/b><br> \n    &ensp;- <b>Overall<\/b> : Matplotlib (82%), Seaborn (67%), Plotly (41%) and ggplot2 (37%).<br>\n    &ensp;-  <b>India<\/b> : Matplotlib (90%), Seaborn (76%), Plotly (47%) and ggplot2 (37%).<br>\n    &ensp;-  <b>USA<\/b> : Matplotlib (73%), Seaborn (58%), ggplot2 (47%) and Plotly (38%).<br>\n    &ensp;- Matplotlib, Seaborn & ggplot have a jump of around 7-12 %p in usage proportion for data scientists, while plotly has a slightly higher jump of around 11-19 %p.\n<\/div>","599e028e":"###### ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------","4a943e39":"###### ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------","b7265667":"###### ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------","e56c3cf4":"### Q18 - Computer Vision Methods","e4bb1348":"###### ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------","f6f27ad8":"### Notebooks Referenced\n1. https:\/\/www.kaggle.com\/kenjee\/kaggle-project-from-scratch<br>\n2. https:\/\/www.kaggle.com\/subinium\/kaggle-2020-visualization-analysis<br>\n3. https:\/\/www.kaggle.com\/paultimothymooney\/2020-kaggle-data-science-machine-learning-survey<br>\n4. https:\/\/www.kaggle.com\/dwin183287\/kagglers-seen-by-continents<br>\n5. https:\/\/www.kaggle.com\/spitfire2nd\/enthusiast-to-data-professional-what-changes<br>\n6. https:\/\/www.kaggle.com\/kenjee\/analyzing-gender-and-earning-potential-in-tech","8355affd":"### Q31 - Business Intelligence Tools","63a795b9":"### 1. Prerequisites","2d3d9e19":"##### '################ workings","8a8f83ee":"<div class=\"alert alert-block alert-info\">\n<a id='obs-q17'><\/a>\n<b>Observations:<\/b><br>\n    - Usage of Decision Trees or Random Forests by Data Scientists seems to be more than their overall usage.\n<\/div>","613d540c":"[back to top](#toc)","ce2a62aa":"###### ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------","558a860c":"### Q5 - Job role","49dcf13c":"#### 33B - Automated ML Tools to familiarize with","901a76b1":"<div class=\"alert alert-block alert-info\">\n<a id='obs-qxx'><\/a>\n<b>Observations:<\/b><br>\n    <i>Any comprehensive comparison of the compensational distribution between India and USA would require the conversion of compensation data according to purchasing power parity of the 2 countries. This is a point to be improved upon later.<\/i>\n<\/div>","82866e64":"<div class=\"alert alert-block alert-info\">\n<a id='obs-q11'><\/a>\n<b>Observations:<\/b><br>\n    <b>Overall<\/b><br>\n    - Personal computer is the most used computing platform (Global-78%, India-80%, USA-75%)<br>\n    - A significant minority primarily uses cloud-computing (Global-14%, India-13%, USA-17%).<br>\n    <b>Data Scientists<\/b><br> \n    - Those working as data scientists have a :<br>\n    &emsp;&nbsp;\u2022 slightly lesser primary usage of personal computers (Global-67%, India-70%, USA-58%), and<br> \n    &emsp;&nbsp;\u2022 slightly higher primary usage of cloud-computing platforms (Global-24%, India-20%, USA-34%).<br> \n    - Data scientists from USA primarily use cloud-computing platforms at almost twice the overall cloud-computing usage rate in USA .<br>\n    <b>Deep learning workstation<\/b><br> \n    - ML Engineers (12%) and Research Scientists (12%) are the roles with a small but significant number of users that primarily use a deep learning workstation.<br>\n    <br><i>Workings below<\/i>\n<\/div>","7121eed1":"[back to top](#toc)","c743d8fc":"[back to top](#toc)","e128eacc":"###### ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------","93934e82":"[back to top](#toc)","6b9abc8f":"## 5. Data preprocessing","69d82b46":"### Q15 - Machine Learning Methods","8ae4f5d2":"[back to top](#toc)","d398480c":"###### Map region to country","8f5ecbd2":"<div class=\"alert alert-block alert-info\">\n<a id='obs-q4q5'><\/a>\n<b>Observations:<\/b><br>\n    1. <b>Students<\/b><br>\n    &emsp;<i>Overall<\/i><br>\n    &emsp;\u2022 largest proportion : Bachelor's : 50%<br>\n    &emsp;<i>India<\/i><br>\n    &emsp;\u2022 largest proportion : Bachelor's : 63%<br>\n    &emsp;<i>USA<\/i><br>\n    &emsp;\u2022 largest proportion : Master's : 44%<br>\n    &emsp;\u2022 with Bachelor's degree - 27%, which is significantly lower than the overall Bachelor's student percentage of 50%.<br>\n    2. <b>Data Scientist<\/b><br>\n    &emsp;<i>Overall<\/i><br>\n    &emsp;\u2022 largest proportion : Master's : 51%<br>\n    &emsp;\u2022 Doctoral : 17%, Bachelor's : 24%<br>\n    &emsp;<i>India<\/i><br>\n    &emsp;\u2022 largest proportion : Master's : 44%<br>\n    &emsp;\u2022 Doctoral : 5%, Bachelor's : 42%<br> \n    &emsp;<i>USA<\/i><br>\n    &emsp;\u2022 largest proportion : Master's : 52%<br>\n    &emsp;\u2022 Doctoral : 30%, Bachelor's : 15%<br> \n    3. <b>Statistician<\/b><br>\n    &emsp;<i>Overall<\/i><br>\n    &emsp;\u2022 largest proportion : Master's : 40%<br>\n    &emsp;\u2022 Doctoral : 28%, Bachelor's : 23%<br>\n    &emsp;<i>India<\/i><br>\n    &emsp;\u2022 largest proportion : Master's : 62%<br>\n    &emsp;\u2022 Doctoral : 21%, Bachelor's : 9%<br> \n    &emsp;<i>USA<\/i><br>\n    &emsp;\u2022 largest proportion : Doctoral : 47%<br>\n    &emsp;\u2022 Master's : 37%, Bachelor's : 8%<br> \n    \n<\/div>","fac9b47c":"##### Region-wise frequency distribution","ae1b978d":"### Q4 & Q6 - Education & Coding experience","549d3e12":"### Q39 - Favorite Media Sources","53306fd7":"###### ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------","4385efb5":"### Q34 - Automated ML tools","7d40bb9a":"###### ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------","5fee50cc":"The data is for an industry-wide survey that presents a truly comprehensive view of the state of data science and machine learning and has 20,036 repondants from all over the world.<br>\n\n**The challenge objective**: tell a data story about a subset of the data science community represented in this survey, through a combination of both narrative text and data exploration.\n\n**Survey structure**: 39+ questions and 20,036 responses.<br>Responses to multiple choice questions (only a single choice can be selected) were recorded in individual columns. Responses to multiple selection questions (multiple choices can be selected) were split into multiple columns (with one column per answer choice).\n\n<a href=\"https:\/\/www.kaggle.com\/c\/kaggle-survey-2020\">Link to competition page<\/a>","81990bd5":"[back to top](#toc)","6caabdc8":"##### '################ workings","6cdbb8c8":"### Q25 - Money spent on ML \/ Cloud-computing","ec5a1b63":"##### b) Stacked Bar Chart","ff081a68":"### Q36 - Public sharing platforms","5d8e0ce2":"<div class=\"alert alert-block alert-info\">\n<a id='obs-q1q2'><\/a>\n<b>Observations:<\/b><br>\n    - Considering the ratios of no. of men per woman in the sample, the ratios get increasingly larger (> 4.3) in the 30+ age groups, whereas in the younger age groups (18-30), the ratios range between 3.2 to 3.8, which is less than the overall ratio (\u2248 4).<br>\nThis augurs well for the correction of gender imbalance in this domain.\n<\/div>","b0e8f3d5":"<div class=\"alert alert-block alert-info\">\n<a id='obs-q3.2'><\/a>\n<b>Observations:<\/b><br>\n    - Regions of Southern Asia (1.953b), Northern America (0.37b), and Eastern Asia (1.681b) make up 52% of the sample and are home to 51% of the world population (7.842b) as well.<br>\n    \n<i>Population figures (as of Jan 2021):<\/i>&ensp;\n    <a href=\"https:\/\/www.worldometers.info\/world-population\/southern-asia-population\/\">Souther Asia<\/a>,&nbsp;\n    <a href=\"https:\/\/www.worldometers.info\/world-population\/northern-america-population\/\">Northern America Asia<\/a>,&nbsp;\n    <a href=\"https:\/\/www.worldometers.info\/world-population\/eastern-asia-population\/\">East Asia<\/a>,&nbsp;\n    <a href=\"https:\/\/www.worldometers.info\/world-population\/\">World<\/a>\n<\/div>","3bad40b9":"### Q8 - Recommended Language","90a86b4b":"### Q20 - Company size","4f3ec58c":"###### ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------","1965c3ee":"##### #'################ workings'","ecea74c8":"<div class=\"alert alert-block alert-info\">\n<a id='obs-q5q6'><\/a>\n<b>Observations:<\/b><br>\n    1. <b>Data Scientist<\/b><br>\n    &emsp;<i>Overall<\/i> - Distribution is slightly left-skewed, with 50% having programming experience of 3-10 yrs.<br>\n    &emsp;<i>India<\/i> - Distribution is roughly symmetrical, with 50% having 3-10 yrs experience.<br>\n    &emsp;<i>USA<\/i> - Distribution is negatively skewed, with 54% having 3-10 yrs experience and 90%+ with experience &gt;3 yrs.<br>\n    2. <b>Statistician<\/b><br>\n    &emsp;<i>Overall<\/i> - Distribution is spread out.<br>\n    &emsp;<i>India<\/i> - Approx. 88% have experience &lt;5 yrs.<br>\n    &emsp;<i>USA<\/i> - Approx. 85% have experience &gt;3 yrs.<br>\n    \n    \n<\/div>","6cc8f956":"##### '################ workings","8f077356":"<div class=\"alert alert-block alert-info\">\n<a id='obs-q9'><\/a>\n<b>Observations:<\/b><br>Top IDEs<br>\n    - <b>Globally<\/b><br>\n    &emsp;\u2022 Jupyter : 64% (India : 74%, USA : 60%).<br>\n    &emsp;\u2022 VSCode : 33% (India : 33%, USA : 30%).<br>\n    &emsp;\u2022 PyCharm : 29% (India : 30%, USA : 23%).<br>\n    &emsp;\u2022 RStudio : 22% (India : 18%, USA : 33%)<br>\n    -  <b>India<\/b> : Jupyter, VSCode, PyCharm<br>\n    -  <b>USA<\/b> : Jupyter, RStudio, VSCode<br>\n    -  <b>Data Scientists<\/b><br> \n    &emsp;\u2022 Jupyter : 74% (India : 75%, USA : 70%).<br>\n    &emsp;\u2022 VSCode : 33% (India : 22%, USA : 29%).<br>\n    &emsp;\u2022 PyCharm : 32% (India : 22%, USA : 26%).<br>\n    &emsp;\u2022 RStudio : 31% (India : 25%, USA : 41%)<br>\n    - Jupyter is the undisputedly preferred IDE.\n<\/div>","1bc81a66":"[back to top](#toc)","ab0eb5b7":"[back to top](#toc)","ddc1a2bd":"### Gender-Age Distribution","b82bd588":"[back to top](#toc)","e0fdf9e7":"<div class=\"alert alert-block alert-info\">\n<a id='obs-qxx'><\/a>\n<b>Observations:<\/b><br>\n    - <b>Overall<\/b><br>\n    &ensp;- xx<br>\n    &emsp;\u2022 xx : xx% (India : xx%, USA : xx%).<br>\n    -  <b>India<\/b> : <br>\n    -  <b>USA<\/b> : <br>\n    -  <b>Data Scientists<\/b><br> \n    &ensp;- xx<br>\n    &emsp;\u2022 xx : xx% (India : xx%, USA : xx%).<br>\n    \n<\/div>","b0deec1b":"<div class=\"alert alert-block alert-info\">\n<a id='obs-q18'><\/a>\n<b>Observations:<\/b><br>\n    <b>Data Scientists<\/b><br>\n    - Roughly 90% of Indian Data Scientists use some computer vision method regularly.<br>\n    - A considerably larger proportion of US Data Scientists does not use any computer vision methods regularly, when compared with India or overall figure.\n<\/div>","8fb9702e":"### Q5 & Q7 - Role & Language","5bf161a6":"###### ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------","20addb98":"#### 31B - Business Intelligence tools to familiarize with","e539f885":"### Q4 - Education","96d6f0a7":"#### 26A - Platform regularly used","177bd072":"[back to top](#toc)","04d87ce0":"###### ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------","1ccbb3ad":"<div class=\"alert alert-block alert-info\">\n<a id='obs-q4q6'><\/a>\n<b>Observations:<\/b><br>\n    - Majority of the involvement in the survey was by those who have learned coding within the last 5 yrs and have a Bachelor's or Master's degree (\u224850%).\n<\/div>","3e3a9dc6":"## 4. Questions in the survey","4223a628":"[back to top](#toc)","328aa765":"[back to top](#toc)","b0cff8ea":"###### ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------","164121c5":"###### ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------","2c9806e4":"### Q27 - Cloud-computing products","9df4ce28":"### 3. Defined functions","2c20fdb0":"### Q2 - Gender","9849ac16":"###### ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------","7d17d592":"#### 27B - Product to familiarize with","cdddf7e6":"###### ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------","824af3db":"#### 31A - Business Intelligence tools used regularly","ad3b8581":"###### ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------","bdbe761f":"### Q13 - TPU usage in life","1df5d614":"[back to top](#toc)","12493d38":"<div class=\"alert alert-block alert-info\">\n<a id='obs-q15'><\/a>\n<b>Observations:<\/b><br>\n    - <b>Overall<\/b> : 60% have been using ML methods for under 2 yrs and 26% for 2+ yrs.<br>\n    -  <b>India<\/b> : India has a large no. of new users with approx. 71% that have been using ML methods for under 2 years (49% under 1 yr) and only 16% that have been using for 2+ yrs.<br>\n    -  <b>USA<\/b> : Respondents from USA have a longer exposure to ML methods with only 42% with less than 2 yrs usage period. 45% have been using for 2+ yrs. There is a significant difference in the proportion of respondents that have been using ML methods for &gt;3 yrs in USA compared with India and overall. US has a larger proportion of experienced users.<br>\n    -  <b>Data Scientists<\/b><br> \n    &ensp;- <b>Overall<\/b> : Data scientists have been conversant with ML methods for a longer time than the overall data suggests. Only 40% have &lt;2 yrs experience. 42% have 3+ yrs usage experience. <br>\n    &ensp;- <b>USA<\/b> : Data scientists from USA have a left-skewed distribution, i.e. they generally have more experience. Approx. 49% have been using ML methods for &gt;4 yrs.<br>\n<\/div>","8b2d60c9":"[back to top](#toc)","3abb312f":"<a id=\"toc\"><\/a>\n## Table of Contents\n1. [Prerequisites](#1.-Prerequisites)\n2. [Import data](#2.-Import-data)\n3. [Defined functions](#3.-Defined-functions)\n4. [Questions in the survey](#4.-Questions-in-the-survey)\n5. [Data preprocessing](#5.-Data-preprocessing)\n6. [Question analysis](#6.-Question-analysis)\n    - [Q1 - Age](#Q1---Age)\n    - [Q2 - Gender](#Q2---Gender)\n      - [Observations](#obs-q2.1)\n      - [Gender-Age Distribution](#Gender-Age-Distribution)\n        - [Observations](#obs-q1q2)\n    - [Q3 - Country of residence](#Q3---Country-of-residence)\n      - [Observations](#obs-q3.1)\n      - [Region-wise frequency distribution](#Region-wise-frequency-distribution)\n        - [Observations](#obs-q3.2)\n    - [Q4 - Education](#Q4---Education)\n      - [Observations](#obs-q4)\n    - [Q5 - Job role](#Q5---Job-role)\n      - [Observations](#obs-q5)\n    - [Q6 - Programming experience](#Q6---Programming-experience)\n      - [Observations](#obs-q6)\n    - [Q4 & Q5 - Education & Job role](#Q4-&-Q5---Education-&-Job-role)\n      - [a) Heatmap](#a%29-Heatmap)\n      - [b) Stacked Bar Chart](#b%29-Stacked-Bar-Chart)\n      - [c) Comparitive bar chart](#c%29-Comparitive-bar-chart)\n      - [Observations](#obs-q4q5)\n    - [Q4 & Q6 - Education & Coding experience](#Q4-&-Q6---Education-&-Coding-experience)\n      - [Observations](#obs-q4q6)\n    - [Q5 & Q6 - Job role and Coding experience](#Q5-&-Q6---Job-role-and-Coding-experience)\n      - [Observations](#obs-q5q6)\n    - [Q7 - Regularly used programming language](#Q7---Regularly-used-programming-language)\n      - [Observations](#obs-q7)\n    - [Q5 & Q7 - Role & Language](#Q5-&-Q7---Role-&-Language)\n      - [Observations](#obs-q5q7)\n    - [Q8 - Recommended Language](#Q8---Recommended-Language)\n      - [Observations](#obs-q8)\n    - [Q9 - IDE Used](#Q9---IDE-Used)\n      - [Observations](#obs-q9)\n    - [Q10 - Hosted notebook products regularly used](#Q10---Hosted-notebook-products-regularly-used)\n      - [Observations](#obs-q10)\n    - [Q11 - Computing platform](#Q11---Computing-platform)\n      - [Observations](#obs-q11)\n    - [Q12 - Specialized hardware](#Q12---Specialized-hardware)\n    - [Q13 - TPU usage in life](#Q13---TPU-usage-in-life)\n    - [Q14 - Visualization libraries](#Q14---Visualization-libraries)\n      - [Observations](#obs-q14)\n    - [Q15 - Machine Learning Methods](#Q15---Machine-Learning-Methods)\n      - [Observations](#obs-q15)\n    - [Q16 - Machine Learning Frameworks](#Q16---Machine-Learning-Frameworks)\n      - [Observations](#obs-q16)\n    - [Q17 - ML Algorithms](#Q17---ML-Algorithms)\n    - [Q18 - Computer Vision Methods](#Q18---Computer-Vision-Methods)\n      - [Observations](#obs-q18)\n    - [Q19 - Natural Language Processing](#Q19---Natural-Language-Processing)\n    - [Q20 - Company size](#Q20---Company-size)\n    - [Q21 - Individuals engaged in Data Science work at workplace](#Q21---Individuals-engaged-in-Data-Science-work-at-workplace)\n      - [Observations](#obs-q21)\n    - [Q22 - ML methods used in business](#Q22---ML-methods-used-in-business)\n      - [Observations](#obs-q22)\n    - [Q23 - Important work activities](#Q23---Important-work-activities)\n    - [Q24 - Yearly Compensation](#Q24---Yearly-Compensation)\n    - [Q25 - Money spent on ML \/ Cloud-computing](#Q25---Money-spent-on-ML-\/-Cloud-computing)\n    - [Q26 - Cloud-computing platform](#Q26---Cloud-computing-platform)\n    - [Q27 - Cloud-computing products](#Q27---Cloud-computing-products)\n    - [Q28 - Machine learning products](#Q28---Machine-learning-products)\n    - [Q29 - Big Data Products](#Q29---Big-Data-Products)\n    - [Q30 - Most used big data products](#Q30---Most-used-big-data-products)\n    - [Q31 - Business Intelligence Tools](#Q31---Business-Intelligence-Tools)\n    - [Q32 - Business Intilligence tools used most often](#Q32---Business-Intilligence-tools-used-most-often)\n    - [Q33 - Automated ML Tools Category](#Q33---Automated-ML-Tools-Category)\n    - [Q34 - Automated ML tools](#Q34---Automated-ML-tools)\n    - [Q35 - ML experiments](#Q35---ML-experiments)\n    - [Q36 - Public sharing platforms](#Q36---Public-sharing-platforms)\n    - [Q37 - DS learning platforms](#Q37---DS-learning-platforms)\n    - [Q38 - Data Analysis Tool](#Q38---Data-Analysis-Tool)\n    - [Q39 - Favorite Media Sources](#Q39---Favorite-Media-Sources)\n\n7. [Notes](#Notes)\n8. [References](#Notebooks-Referenced)","ca3a8470":"### Q19 - Natural Language Processing","b38a7b7f":"[back to top](#toc)","19bb4bbd":"### Q5 & Q6 - Job role and Coding experience","7a4d0c59":"##### '################ workings'","85c617c8":"##### '################ workings'","8717872f":"### Q38 - Data Analysis Tool","19c239bf":"###### ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------","5cc205a0":"#### 35B - Tools to familiarize with","378c2a18":"### Q10 - Hosted notebook products regularly used","85095899":"### Q11 - Computing platform","a5790544":"[back to top](#toc)","9bae764e":"[back to top](#toc)","3893a998":"<div class=\"alert alert-block alert-info\">\n<a id='obs-q6'><\/a>\n<b>Observations:<\/b><br>\n    - Around 70% of the respondants have programming experience of &lt; 5 years.<br>\n    - Shape of the distribution for India is denser towards classes with programming experience &lt;5 yrs, with approx. 84% respondents falling within those classes.<br>\n    - Shape of the distribution for USA is denser towards classes with programming experience &gt;3 yrs, with approx. 73% respondents falling within those classes.<br>\n    - Looking at the differential proportion graphs below, we can see that: <br>\n    &emsp;\u2022 India has very few respondents with 10+ yrs expericence<br>\n    &emsp;\u2022 USA has significantly more respondents with 10+ yrs expericence<br>\n\n<i>Workings below<\/i>\n<\/div>","cdad677c":"#### 29A - Big data products used regularly","7438f1b2":"<div class=\"alert alert-block alert-info\">\n<a id='obs-q20'><\/a>\n<b>Observations:<\/b><br>\n    - Largest proportion of Indian Data Scientists work in companies with less than 50 employees.<br>\n<\/div>","744e3f24":"##### Pie chart","6cb18182":"### Q7 - Regularly used programming language","86b12b1f":"[back to top](#toc)","3c4e02a7":"[back to top](#toc)","9f925beb":"<header>\n<h1><center>Exploratory Data Analysis of the<br>2020 Kaggle Machine Learning & Data Science Survey<\/center><\/h1>\n<h5><center>Focused on the distributions for India vis-a-vis USA and Data Scientist vis-a-vis General trends<\/center><\/h5>\n<\/header>","f184e608":"### Q35 - ML experiments","bb73f106":"<div class=\"alert alert-block alert-info\">\n<a id='obs-q21'><\/a>\n<b>Observations:<\/b><br>\n    - For both India (25%) and USA (33%), largest proportion of respondents belong to organizations that have at least 20 individuals engaged in data science work at their place of work. This indicates an organized setup dedicated to Data Science.<br>\n    - <b>Data Scientists<\/b><br>\n    &emsp;\u2022 USA : Largest proportion &rarr; 20+ individuals &rarr; 37%<br>\n    &emsp; &emsp; &emsp; &nbsp; Lowest proportion &rarr; 0 individuals &rarr; 2%<br>\n    &emsp;\u2022 India : Largest proportion &rarr; 20+ individuals &rarr; 25%<br>\n    &emsp; &emsp; &emsp; &nbsp; Lowest proportion &rarr; 15-19 individuals &rarr; 3.1%<br>\n    &emsp; The proportion for respondents that have 20+ individuals engaged in Data Science work at their workplace is notably larger for USA (37%) than overall (23%) and for India (25%).<br> \n    &emsp; The proportion for respondents that have 0 individuals engaged in Data Science work at their workplace is notably larger for India (14%) than for US (2.2%).\n<\/div>","9b00937f":"[back to top](#toc)","c1ca32e4":"[back to top](#toc)","5f3d3fad":"###### ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------","9a6c0c04":"[back to top](#toc)","d8508f49":"<div class=\"alert alert-block alert-info\">\n<a id='obs-q1'><\/a>\n<b>Observations:<\/b><br>\n    - Most of the survey participants belong to younger age groups with 70% aged below 35.\n<\/div>","df4548eb":"<div class=\"alert alert-block alert-info\">\n<a id='obs-q10'><\/a>\n<b>Observations:<\/b><br>Most-used hosted notebooks :<br>\n    - <b>Globally<\/b><br>\n    &emsp;\u2022 Colab Notebooks : 37% (India : 47%, USA : 25%).<br>\n    &emsp;\u2022 Kaggle Notebooks : 35% (India : 43%, USA : 24%).<br>\n    &emsp;\u2022 None : 31% (India : 22%, USA : 43%).<br>\n    &emsp;\u2022 Binder\/JupyterHub : 12% (India : 15%, USA : 14%)<br>\n    -  <b>India<\/b> : Colab Notebooks, Kaggle Notebooks, None<br>\n    -  <b>USA<\/b> : None, Colab Notebooks, Kaggle Notebooks<br>\n    -  <b>Data Scientists<\/b><br> \n    &emsp;&nbsp;\u2022 Colab Notebooks : 41% (India : 55%, USA : 25%).<br>\n    &emsp;&nbsp;\u2022 Kaggle Notebooks : 36% (India : 49%, USA : 20%).<br>\n    &emsp;&nbsp;\u2022 None : 26% (India : 13%, USA : 39%).<br>\n    &emsp;&nbsp;\u2022 Binder\/JupyterHub : 13% (India : 16%, USA : 15%)<br>\n    &emsp;- Indian data scientists use Colab and Kaggle Notebooks much more than their global or US contempories do.<br> \n    &emsp;-  A signigicant number of data scientists in USA do not use any hosted notebook platform.\n<\/div>","53c3cfe0":"[back to top](#toc)","7d4e7c27":"###### ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------","20b5a588":"###### ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------","ffb89b3d":"#### 34B - Automated ML tools to familiarize with","728fc170":"##### Remove non-essential data","7cc4296b":"##### '################ workings","b1151016":"<div class=\"alert alert-block alert-info\">\n<a id='obs-q4'><\/a>\n<b>Observations:<\/b><br>\n    - Participants having (or planning to complete) Master's are the largest group.<br>\n    - The distributions for India and USA are noticably different.<br><br> \n    <b>India<\/b><br>\n    - India's overall representation in the survey sample is 29%. <br>\n    - It's considerably under-representated in the educational categories of:<br>\n    &emsp;\u2022 No formal education past high school : 12% (WN 3)<br>\n    &emsp;\u2022 Some college\/university study without earning a Bachelor's : 19% (WN 4)<br>\n    &emsp;\u2022 Doctoral degree : 12% (WN 5)<br>\n    - India has a larger proportion of Bachelor's (43%) than the overall proportion (36%). (WN 6)<br>\n    - In fact, the proportion of Bachelor's drops by 7 percentage points (or 7pp or 7%p) to 28.5%, if the  sample's from India are not taken into account. (WN 7)<br>\n    - And the proportion of Master's and Doctoral degree's increases by approx. 3%p each. (WN 7)<br><br>\n    <b>USA<\/b><br>\n    - USA's overall representation in the survey sample is 11%. <br>\n    - It has a significantly larger representation in the eduacational category of Doctoral (18%) and a  lesser representation in Bachelor's (8%), High School (5%) and Professionl (6%). (WN 8)<br>\n\n<i><b>Note: <\/b> percentage points have been denoted by %p.<\/i><br><br>\n<i>Workings below<\/i>\n<\/div>","4ece9e33":"###### ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------","823b710c":"[back to top](#toc)","2cdee6b9":"## 6. Question analysis","6ccbcc71":"###### ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------","5c60c054":"<div class=\"alert alert-block alert-info\">\n<a id='obs-q22'><\/a>\n<b>Observations:<\/b><br>\n    &nbsp;<b>World<\/b><br>\n    - Highest proportion &rarr; Exploring ML : 21%<br>\n    - Second highest  &rarr; ML not used : 20%<br>\n    &nbsp;<b>India<\/b><br>\n    - Highest proportion &rarr; Exploring ML : 22%<br>\n    - Second highest  &rarr; Well established ML : 18%<br>\n    &nbsp;<b>USA<\/b><br>\n    - Highest proportion &rarr; Well established ML : 26%<br>\n    - Second highest  &rarr; Exploring ML : 16%<br>\n    &nbsp;<b>Data Scientists<\/b><br>\n    - The proportion of Data Scientists working at places with active incorporation of ML in business is larger than the general trend.<br>\n    - 55% respondents work in places which have incorporated ML methods in their business (compared with 33% overall).<br>\n    - The proportion is significantly larger for US at 67% (46% with well-established ML incorporation) than for India (54%).<br>\n<\/div>","2a53f943":"<div class=\"alert alert-block alert-info\">\n<a id='obs-q7'><\/a>\n<b>Observations:<\/b><br>\n    - The overall trends and the trends in India and USA seem to be more or less similar.<br>\n    - USA has higher usage of R, SQL and Bash and lesser usage of C and C++, as compared with India and the overall trend.<br>\n    - India has a higher usage C.\n<\/div>","efa9750b":"###### ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------","6766d292":"### Q24 - Yearly Compensation","079bc2e7":"<div class=\"alert alert-block alert-info\">\n<a id='obs-q2.1'><\/a>\n<b>Observations:<\/b><br>\n    - The gender ratio observed here corroborates the perception of under-representation of women in STEM domains.\n<\/div>","3d0076a0":"#### 28B - ML Product to familiarize with","dbc93bde":"##### #'################ workings'","1752e541":"#### 34A - Automated ML tools regularly used","5e9b9219":"[back to top](#toc)","ac8daf4f":"[back to top](#toc)","90c43fe7":"[back to top](#toc)","d5208be2":"[back to top](#toc)","8fbcc5b5":"###### ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------","77993709":"#### 26B - Platform to familiarize with","abcb3014":"[back to top](#toc)","bd81a055":"###### ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------","ec5ebfa3":"###### ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------","6a97ba73":"[back to top](#toc)","751e2ab9":"### Q9 - IDE Used","bba82b90":"##### #'################ workings'","fbbbd089":"###### ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------","ef510c2b":"<div class=\"alert alert-block alert-info\">\n<a id='obs-q16'><\/a>\n<b>Observations:<\/b><br>\n    - <b>Overall<\/b><br>\n    &emsp;\u2022 Scikit-Learn (72%), TensorFlow (49%), Keras (44%) are amongst the frameworks regularly used by majority of the respondents.<br>\n    &emsp;\u2022 India and USA have largely similar usage patterns. However, Indians usage of TenserFlow (54%) and Keras (49%) is notably more than their US counterparts (40% & 30% respectively).<br>\n    -  <b>Data Scientists<\/b><br> \n    &emsp;&nbsp;\u2022 Data Scientistis usage of Scikit-Learn (83%), Tensorflow (51%), Keras (51%) is slightly higher. Usage of Xgboost (48%) is a significantly higher than overall (28%).<br>\n    &emsp;&nbsp;\u2022 India and USA comparitive patterns are same as noted above.\n<\/div>","910ef1a0":"###### ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------","7f7f41cd":"[back to top](#toc)","a5b8ac62":"[back to top](#toc)","6141272a":"### Q32 - Business Intilligence tools used most often","22c820bc":"[back to top](#toc)","53c68846":"##### '################ workings'","c30bc16a":"###### ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------","5a9f5a3f":"### Q33 - Automated ML Tools Category","61be1e6e":"### Q26 - Cloud-computing platform","e2e6a50f":"[back to top](#toc)","457912a2":"###### ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------","c1e3da99":"##### c) Comparitive bar chart","9a791292":"###### ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------","2add686d":"###### ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------","10dd6ccb":"[back to top](#toc)","5ec8a80c":"###### ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------","647d9504":"###### ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------","f7fd7788":"<div class=\"alert alert-block alert-info\">\n<a id='obs-q8'><\/a>\n<b>Observations:<\/b><br>\n    - Python is the most recommended initiation language across the board with 80% global recommendation, (84% in India, and 74% in USA), with R being a distant 2nd with 7% recommending it as the ideal jump-start for a data science path.<br>\n    - Data Scientists themselves recommend Python as the ideal starting point with 80% globally, 87% in India & 81% in USA recommending it.<br>\n    &ensp;Again, R (global : 8%, India : 1%, USA : 3%) and SQL (global : 7%, India : 4%, USA : 10%) are the distant runners-up.\n<\/div>","a35e5cc6":"###### ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------","8b6c5c0a":"[back to top](#toc)","783682a7":"[back to top](#toc)","e81c90a9":"##### a) Heatmap","6a1f03e9":"##### #'################ workings","b8fde6f6":"### Q1 - Age","7ceff75e":"##### Notes\nI have deliberately kept, within this notebook, some of the code which was part of my learning journey.  \nI've incorporated more efficited codes\/methods as the notebook progresses.\n\n***Points of improvement:***  \na) Q24 Compensation : comparisons to be made according to purchasing power parity.","42876341":"###### ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------","1dbd9664":"### Check for missing data\n**Note:** Because of the way this dataset is structured, we can expect plenty of missing values in most of the columns. But we will still perform this check for the columns that ideally should not have any missing values.","8625d90a":"[back to top](#toc)","67aa8787":"<div class=\"alert alert-block alert-info\">\n<a id='obs-q5q7'><\/a>\n<b>Observations:<\/b><br>\n    1. <b>Data Scientist<\/b><br>\n    &emsp;- Data Scientists display a very high usage of Python with 94% of the respondents using it on a regular basis. The trend is similarly high in India (98%) and USA (89%).<br>\n    &emsp;- Data Scientists in USA have a higher usage for using:<br>\n    &emsp;&nbsp; \u2022 SQL (72%) when compared with India (50%) or overall world trend (56%).<br>\n    &emsp;&nbsp; \u2022 Bash (24%) when compared with India (7%)  or overall world trend (16%).<br>\n    &emsp;- India has a higher usage of :<br>\n    &emsp;&nbsp; \u2022 C (18%) when compared with USA (4%) and overall (9%).<br>\n    &emsp;&nbsp; \u2022 C++ (17%) when compared with USA (6%).<br>\n    2. <b>Statistician<\/b><br>\n    &emsp;- Statisticians use R (69%) more than Python (63%) and use SQL (29%) less often than the overall trend (42%).<br>\n    &emsp;- Usage patterns of programming languages are similar for Indian and USA statisticians.<br>\n    &emsp;- Usage of R by statisticians in both India & USA (\u224881%) is higher than the overall usage (69%).<br>\n    &emsp;- Usage of C by Indian statisticians (22%) is slightly higher than global usage (10%).<br>\n    3. <b>Student<\/b><br>\n    &emsp;- Python is the language most used by students globally (90%) and this the trend is mirrored in India (93%) and USA (87%) as well.<br>\n    &emsp;- Students in USA have a higher regular usage of R (39%) compared with India (14%) and global usage (19%).<br>\n    &emsp;- Students in India have a considerably higher usage of C at 46%. The global usage is 31% and merely 12% in USA.<br>\n    &emsp;- C++ usage is at 45% in India, 18% in USA and 34% globally.<br>\n    4. <b>Software Engineer<\/b><br>\n    &emsp;- High usage of Python (78%) and SQL (52%) and very few use R regularly (8%).<br>\n    &emsp;- High usage of Java (38%) and Javascript (44%) <br>\n    &emsp;- USA particulary has a higher usage of Bash at 29%, with India at 9% and global usage at 16%.<br>\n    5. <b>Data Analyst<\/b><br>\n    &emsp;- High usage of Python (83%) and SQL (60%).<br>\n    &emsp;- USA has a higher usage of :<br>\n    &emsp;&nbsp;\u2022  R (45%) compared with India (31%) and global (35%).<br>\n    &emsp;&nbsp;\u2022  SQL (74%) compared with India (59%) and global (60%).<br>\n    6. <b>Machine Learning Engineer<\/b><br>\n    &emsp;- MLEs have a very high regular usage of Python at 97%.<br>\n    &emsp;- 32% of the MLEs of USA use Bash regularly whereas global MLE usage for Bash is at 15% and is 6% in India.<br>\n    7. <b>Business Analyst<\/b><br>\n    &emsp;- Python (77%) and SQL (59%) are the regularly used programming languages for a large no. of Business Analysts globally.<br>\n    &emsp;- Indian BAs use Python (87%) more often than the BAs in USA (67%).\n<\/div>","8871bacf":"### Q16 - Machine Learning Frameworks","222354c7":"[back to top](#toc)","b47ff468":"#### 35A - Tools used to manage ML experiments","da7656a5":"[back to top](#toc)","1f408073":"### Q12 - Specialized hardware","83e8d678":"### Q30 - Most used big data products","9ccc8ee0":"##### Create subsets\nWe will create subsets for each question as a dictionary, where all the relevant columns for the question will be included as a list in value.\n\n<i>Notebook referenced : 1<\/i>","4e97d5f6":"[back to top](#toc)","11871cba":"##### #'################ workings","af9945ac":"### Q28 - Machine learning products","433be6ae":"[back to top](#toc)","32aee1c7":"### Q6 - Programming experience","70ed99c7":"[back to top](#toc)","c34806a9":"###### ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------","640c9601":"### Q3 - Country of residence","fd0d9a09":"###### ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------","4d76bc7f":"#### 28A - ML Product used regularly","915f0d6b":"### Q14 - Visualization libraries","463115d3":"### Q21 - Individuals engaged in data science work at workplace","f842cac4":"### Region-wise Distribution","ab46ba7d":"###### ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------","57c33d92":"### Q37 - DS learning platforms","deda30f5":"###### ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------","298a34bb":"### Q23 - Important work activities","a1e18618":"###### ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------","96ee8c42":"[back to top](#toc)","329bc2de":"<div class=\"alert alert-block alert-info\">\n<a id='obs-q3.1'><\/a>\n<b>Observations:<\/b><br>\n    - The survey sample has respondents from 54+ countries.<br>\n    - India and USA are the largest contributors to the sample, with every other country constituting < 4% of the sample.\n<\/div>","e35e4951":"### Q4 & Q5 - Education & Job role","06e006b0":"### Q22 - ML methods used in business","ccc46143":"#### 29B - Big data products to familiarize with","656409a8":"###### ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------","50db58e7":"###### ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------","4caf965c":"[back to top](#toc)","e6e1369e":"###### ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------"}}