{"cell_type":{"0d6c8c4e":"code","2ea77592":"code","219fd451":"code","bba7c909":"code","59563551":"code","53614b83":"code","afc37a26":"code","c5329c36":"code","c7fe950d":"code","ea5b3208":"code","6d65c78d":"code","054f5b9c":"code","1299c5ae":"code","2d60fd89":"code","ac87b3a3":"code","6c0b064a":"code","3ed9c206":"code","dec735da":"code","6036231c":"code","ce489a0a":"code","a124bbee":"code","a2d9b883":"code","4609a184":"code","04e3a52e":"code","90e63995":"code","7cd10bc7":"code","f893d7d0":"code","9dacb8ba":"code","87c7683f":"code","f60bef80":"code","13b8fa26":"code","7bc1ecc9":"code","cf931ebc":"code","b45d2802":"code","c8024102":"code","4e441087":"code","476e2b18":"code","738d7dca":"code","55d341f9":"code","87a0151a":"code","30c79a28":"code","29f71580":"markdown","607a92d9":"markdown","d948d00c":"markdown"},"source":{"0d6c8c4e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nimport xml.dom.minidom\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom lxml import etree\nfrom matplotlib.patches import Rectangle\nimport time\nimport glob\nimport os\n# for dirname, _, filenames in os.walk('.'):\n#     for filename in filenames:\n# #         if 'h5' in filename:\n#         print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","2ea77592":"# gpus = tf.config.experimental.list_physical_devices(device_type='GPU')\n# tf.config.experimental.set_virtual_device_configuration(\n#     gpus[0],\n#     [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=1536)])","219fd451":"!ls \/kaggle\/input\/the-oxfordiiit-pet-dataset\/images\/images  | wc -l\n# \u663e\u793a\u6709 7393\u4e2a\u6587\u4ef6\uff0c\u4f46\u4e8b\u5b9e\u4e0a\u53ea\u6709 7390\u4e2a jpg\u6587\u4ef6","bba7c909":"!ls \/kaggle\/input\/the-oxfordiiit-pet-dataset\/images\/images | grep  'jpg' | wc -l","59563551":"!ls \/kaggle\/input\/the-oxfordiiit-pet-dataset\/images\/images | grep  -v 'jpg'\n# \u8fd93\u4e2a\u6587\u4ef6\u4e0d\u4f1a\u7528\u4f5c\u8bad\u7ec3","53614b83":"! ls \/kaggle\/input\/the-oxfordiiit-pet-dataset\/annotations\/annotations\/xmls | grep \"xml\" | wc -l\n# 3686\u4e2a xml \u6587\u4ef6\uff0c\u7528\u4f5c\u8bad\u7ec3\u96c6\u6807\u7b7e","afc37a26":"!ls \/kaggle\/input\/the-oxfordiiit-pet-dataset\/images\/images | head","c5329c36":"! ls \/kaggle\/input\/the-oxfordiiit-pet-dataset\/annotations\/annotations\/xmls | head","c7fe950d":"dom = xml.dom.minidom.parse(\"\/kaggle\/input\/the-oxfordiiit-pet-dataset\/annotations\/annotations\/xmls\/Abyssinian_1.xml\") # or xml.dom.minidom.parseString(xml_string)\npretty_xml_as_string = dom.toprettyxml()\nprint(pretty_xml_as_string)\n# name:  \u6240\u5c5e\u7684\u5927\u7c7b\u522b\uff08\u4e5f\u662f\u8fd9\u6b21\u5206\u7c7b\u7684\u7c7b\u522b\uff09;   xmin: \u5de6\u4e0b\u89d2\u70b9 x\u503c; ymax: \u53f3\u4e0a\u89d2 y\u503c......","ea5b3208":"all_image_paths = glob.glob(\"\/kaggle\/input\/the-oxfordiiit-pet-dataset\/images\/images\/*.jpg\")\nall_xml_paths = glob.glob(\"\/kaggle\/input\/the-oxfordiiit-pet-dataset\/annotations\/annotations\/xmls\/*.xml\")","6d65c78d":"len(all_image_paths), len(all_xml_paths)","054f5b9c":"images_names = [ path.split('\/')[-1].split('.jpg')[0] for path in all_image_paths ] \n#\u6240\u6709\u56fe\u7247\u6587\u4ef6\u540d\uff0c['scottish_terrier_57','Abyssinian_79','pug_4','miniature_pinscher_124','scottish_terrier_45',......]\nxmls_names = [ path.split('\/')[-1].split('.xml')[0] for path in all_xml_paths ]\n#\u6240\u6709xml\u6587\u4ef6\u540d ['american_bulldog_173', 'beagle_129', 'Siamese_109', 'pug_110', 'Persian_106',......]\n\nnames = list(set(images_names)&set(xmls_names))\nnonames = list(  set(images_names) -  set(xmls_names))\n# \u6240\u6709\u6709\u5bf9\u5e94xml\u6807\u7b7e\u6587\u4ef6\u7684 \u6587\u4ef6\u540d\uff0c \u53ea\u6709\u8fd9\u4e9b\u6587\u4ef6\u540d\u5f00\u5934\u7684jpg\u6587\u4ef6\u624d\u6709xml\u6587\u4ef6\uff0c\u8fd9\u4e9b\u6587\u4ef6\u624d\u4f1a\u5f53\u4f5c\u8bad\u7ec3\u6570\u636e\n#['havanese_115','staffordshire_bull_terrier_125','leonberger_177','yorkshire_terrier_162','japanese_chin_188',....])\nlen(names), names[:5], len(nonames), nonames[:5]","1299c5ae":"train_image_paths =  [image_path for image_path in all_image_paths if image_path.split('\/')[-1].split('.jpg')[0] in names]\n# \u83b7\u53d6\u6240\u6709\u6709xml\u6807\u7b7e\u6587\u4ef6\u7684 jpg\u6587\u4ef6\u8def\u5f84\ntrain_image_paths[:5]","2d60fd89":"# \u73b0\u5728\u505a\u4e2a\u6392\u5e8f\uff0c \u4f7f\u5f97 image\u6587\u4ef6\u548c xml\u6587\u4ef6\u4e00\u4e00\u5bf9\u5e94\ntrain_image_paths.sort(key=lambda path: path.split('\/')[-1].split('.jpg')[0])\nall_xml_paths.sort(key=lambda path: path.split('\/')[-1].split('.xml')[0])\nprint(train_image_paths[127], all_xml_paths[127] , '\\n',train_image_paths[599], all_xml_paths[599])\nprint(train_image_paths[2375], all_xml_paths[2375] , '\\n',train_image_paths[1864], all_xml_paths[1864])","ac87b3a3":"def get_labels(label_path):\n    # \u4ecexml\u6587\u4ef6\u83b7\u53d6\u5bf9\u5e94\u7684\u6807\u7b7e\n#     xml = open(r'{}'.format(label_path)).read()\n    with open(label_path, 'r') as f:\n        xml = f.read()\n        sel = etree.HTML(xml)\n        width = int(sel.xpath('\/\/size\/width\/text()')[0])\n        height = int(sel.xpath('\/\/size\/height\/text()')[0])\n        xmin = int(sel.xpath('\/\/bndbox\/xmin\/text()')[0])\n        ymin = int(sel.xpath('\/\/bndbox\/ymin\/text()')[0])\n        xmax = int(sel.xpath('\/\/bndbox\/xmax\/text()')[0])\n        ymax = int(sel.xpath('\/\/bndbox\/ymax\/text()')[0])\n    return [xmin\/width, ymin\/height, xmax\/width, ymax\/height]","6c0b064a":"RESIZE = 224\n# \u56fe\u7247\u5904\u7406\ndef read_jpg(image_path):\n    img = tf.io.read_file(image_path)\n    img = tf.image.decode_jpeg(img, channels=3)\n    return img\n\ndef normalize(input_image, resize=RESIZE):\n    input_image = tf.image.resize(input_image, [resize, resize])\n    input_image = tf.cast(input_image, tf.float32)\/127.5 - 1\n    return input_image\n\n@tf.function\ndef load_image(image_path):\n    input_image = read_jpg(image_path)\n    input_image = normalize(input_image)\n    return input_image","3ed9c206":"labels = [get_labels(path) for path in all_xml_paths]\n# labels:   [ [0.555, 0.18, 0.7083333333333334, 0.395], [0.192, 0.21, 0.768, 0.582],......]\nlabels[:2]","dec735da":"test_path = [  image_path for image_path in all_image_paths if image_path.split('\/')[-1].split('.jpg')[0] not in names  ]\ntest_path[:5]","6036231c":"# xmin_labels, ymin_labels, xmax_label2, ymax_labels = list(zip(*labels))","ce489a0a":"# type(xmin_labels), len(xmin_labels), xmin_labels[:3]\n#xmin_labels  \u5b58\u50a8\u4e86\u6240\u6709 image\u6587\u4ef6\u5bf9\u5e94\u7684 xmin\u5750\u6807\uff08\u8fdb\u8fc7\u4e86get_labels\u51fd\u6570\u5904\u7406\u7684\uff0c\u4e5f\u5c31\u662f\u76f8\u5bf9\u5750\u6807\uff09\uff0c ymin_labels\uff0cxmax_label2\uff0cymax_labels\u4ea6\u5982\u662f","a124bbee":"index = np.random.permutation(len(train_image_paths))\n# \u6253\u6563\u987a\u5e8f\ntrain_images = np.array(train_image_paths)[index]\ntrain_labels = np.array(labels)[index]\n# xmin_labels = np.array(xmin_labels)[index]\n# ymin_labels = np.array(ymin_labels)[index]\n# xmax_label2 = np.array(xmax_label2)[index]\n# ymax_labels = np.array(ymax_labels)[index]","a2d9b883":"train_images[:5], train_labels[:5]","4609a184":"# dataset_labels = tf.data.Dataset.from_tensor_slices(( xmin_labels, ymin_labels,  xmax_label2, ymax_labels))\ndataset_labels = tf.data.Dataset.from_tensor_slices(train_labels)\ndataset_images = tf.data.Dataset.from_tensor_slices(train_images)\n\ndataset_images = dataset_images.map(load_image, num_parallel_calls=tf.data.experimental.AUTOTUNE)   #\u5904\u7406\u56fe\u7247\ndataset_total = tf.data.Dataset.zip((dataset_images, dataset_labels))\n\ndataset_test = tf.data.Dataset.from_tensor_slices(test_path)","04e3a52e":"dataset_total","90e63995":"# \u5206\u5272 \u8bad\u7ec3\u96c6\u548c\u9a8c\u8bc1\u96c6\n\nval_count = int(len(train_image_paths)*0.2)\ntrain_count = len(train_image_paths) - val_count\ndataset_train = dataset_total.skip(val_count)\ndataset_val = dataset_total.take(val_count)\nval_count","7cd10bc7":"SCALE = 224 \nBATCH_SIZE = 8\nBUFFER_SIZE = 300\nTRAIN_STEPS_PER_EPOCH = train_count \/\/ BATCH_SIZE\nVALIDATION_STEPS = val_count \/\/ BATCH_SIZE\n\ntrain_dataset = dataset_train.shuffle(BUFFER_SIZE).batch(BATCH_SIZE).repeat()\ntrain_dataset = train_dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\nval_dataset = dataset_val.batch(BATCH_SIZE)\n","f893d7d0":"dataset_test = dataset_test.map(load_image, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n","9dacb8ba":"test_dataset = dataset_test.batch(8)","87c7683f":"train_dataset, val_dataset, test_dataset","f60bef80":"# \u4ece \u6570\u636e\u96c6\u4e2d\u6311\u51fa\u4e00\u4e2a\u770b\u770b\u663e\u793a\u662f\u5426\u6b63\u786e\n# def show_img(img, label, idx=0):\n#     plt.imshow(tf.keras.preprocessing.image.array_to_img(img[idx]))\n#     out1, out2, out3, out4 = label\n#     xmin, ymin, xmax, ymax = (out1[idx]).numpy()*RESIZE, (out2[idx]).numpy()*RESIZE, (out3[idx]).numpy()*RESIZE, (out4[idx]).numpy()*RESIZE\n#     rect = Rectangle((xmin, ymin), (xmax-xmin), (ymax-ymin), fill=False, color='red')\n#     ax = plt.gca()\n#     ax.axes.add_patch(rect)\ndef show_img(img, labels, idx=0):\n    plt.imshow(tf.keras.preprocessing.image.array_to_img(img[idx]))\n    label= labels[idx]\n    xmin, ymin, xmax, ymax = (label[0]).numpy()*RESIZE, (label[1]).numpy()*RESIZE, (label[2]).numpy()*RESIZE, (label[3]).numpy()*RESIZE\n    rect = Rectangle((xmin, ymin), (xmax-xmin), (ymax-ymin), fill=False, color='red')\n    ax = plt.gca()\n    ax.axes.add_patch(rect)\n\n\n\nfor img, label in train_dataset.take(1):\n    show_img(img, label, 7)  ","13b8fa26":"for img, label in val_dataset.take(1):\n    show_img(img, label, 3)      \n# OK\uff0c \u90fd\u6ca1\u95ee\u9898\uff0c \u6570\u636e\u5df2\u7ecf\u51c6\u5907\u5c31\u7eea\u3002 ","7bc1ecc9":"xception = tf.keras.applications.Xception(weights='imagenet', include_top=False,input_shape=(RESIZE, RESIZE, 3))","cf931ebc":"inputs = tf.keras.layers.Input(shape=(RESIZE, RESIZE, 3))\n\nh_layer = xception(inputs)\nh_layer = tf.keras.layers.GlobalAveragePooling2D()(h_layer)\nh_layer = tf.keras.layers.Dense(2048, activation='relu')(h_layer)\nh_layer = tf.keras.layers.Dense(256, activation='relu')(h_layer)\n\n# xmin = tf.keras.layers.Dense(1)(h_layer)\n# ymin = tf.keras.layers.Dense(1)(h_layer)\n# xmax = tf.keras.layers.Dense(1)(h_layer)\n# ymax = tf.keras.layers.Dense(1)(h_layer)\n\n# predictions = [xmin, ymin, xmax, ymax]\npredictions = tf.keras.layers.Dense(4)(h_layer)\n# \u56e0\u4e3a\u6807\u7b7e\u8ddf\u539f\u6765\u4e0d\u4e00\u6837\uff0c\u8fd9\u91cc\u4e0d\u5728\u751f\u6210 4\u4e2a\u8f93\u51fa\uff0c\u800c\u662f\u6700\u540e\u4e00\u5c42 \u7684Dense_unit = 4, \u76f8\u5f53\u4e8e\u8f93\u51fa\u4e86\u4e00\u4e2a\u5217\u8868\uff0c\u8fd9\u4e2a\u5217\u8868\u5305\u542b4\u4e2a\u503c\n\nmodel = tf.keras.models.Model(inputs=inputs, outputs=predictions)","b45d2802":"model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001), loss='mse',metrics=['mae'])","c8024102":"EPOCHS = 50","4e441087":"history = model.fit(train_dataset, epochs=EPOCHS,steps_per_epoch=TRAIN_STEPS_PER_EPOCH,\n                    validation_steps=VALIDATION_STEPS,\n                    validation_data=val_dataset)","476e2b18":"loss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs = range(EPOCHS)\n\nplt.figure()\nplt.plot(epochs, loss, 'r', label='Training loss')\nplt.plot(epochs, val_loss, 'bo', label='Validation loss')\nplt.title('Training and Validation Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss Value')\nplt.ylim([0, 1])\nplt.legend()\nplt.show()","738d7dca":"model.save('\/kaggle\/working\/detect_v3.h5')","55d341f9":"plt.figure(figsize=(8, 24))\nfor img, _ in val_dataset.take(1):\n    labels= model.predict(img)\n    for i in range(6):\n        plt.subplot(6, 1, i+1)\n        plt.imshow(tf.keras.preprocessing.image.array_to_img(img[i]))\n        xmin = labels[i][0]*224\n        ymin = labels[i][1]*224\n        xmax = labels[i][2]*224\n        ymax = labels[i][3]*224\n        rect = Rectangle((xmin, ymin), (xmax-xmin), (ymax-ymin), fill=False, color='blue')\n        ax = plt.gca()\n        ax.axes.add_patch(rect)\n        ","87a0151a":"plt.figure(figsize=(8, 24))\nfor img in test_dataset.take(1):\n    labels= model.predict(img)\n    print(labels)\n    for i in range(6):\n        plt.subplot(6, 1, i+1)\n        plt.imshow(tf.keras.preprocessing.image.array_to_img(img[i]))\n        xmin = labels[i][0]*224\n        ymin = labels[i][1]*224\n        xmax = labels[i][2]*224\n        ymax = labels[i][3]*224\n        rect = Rectangle((xmin, ymin), (xmax-xmin), (ymax-ymin), fill=False, color='blue')\n        ax = plt.gca()\n        ax.axes.add_patch(rect)\n        ","30c79a28":"!ls","29f71580":"                                                                                            \u6a21\u578b\u521b\u5efa","607a92d9":"\u5148\u770b\u4e00\u4e0b\u76ee\u5f55\u7ed3\u6784\n\/kaggle\/input\/the-oxfordiiit-pet-dataset\/ \u4e0b\u67092\u4e2a\u6587\u4ef6\u5939\uff0c\u5206\u522b\u662f: annotations,  images\n\n\/kaggle\/input\/the-oxfordiiit-pet-dataset\/images\/images \u8fd9\u4e2a\u76ee\u5f55\u4e0b\u653e\u4e86\u6240\u6709\u7684\u56fe\u7247\u6587\u4ef6\n\n\/kaggle\/input\/the-oxfordiiit-pet-dataset\/annotations\/annotations \u76ee\u5f55\u4e0b\u6709 README\tlist.txt  test.txt  trainval.txt  trimaps  xmls\uff0c \u5176\u4e2d\nxmls\uff0c trimaps\u662f\u6587\u4ef6\u5939\uff0c xmls\u6587\u4ef6\u4e0b\u5b58\u653e\u4e86\u90e8\u5206\u56fe\u7247\u5bf9\u5e94\u7684\u6807\u7b7e(xml\u683c\u5f0f\uff0c\u4e00\u4e2a\u56fe\u7247\u6587\u4ef6\u5bf9\u5e94\u4e00\u4e2axml\u6587\u4ef6).  \u7531\u4e8e\u5e76\u975e\u6240\u6709\u56fe\u7247\u90fd\u6709xml\u6807\u7b7e\uff0c\u6240\u4ee5\u8bad\u7ec3\u96c6\u53ea\u9009\u62e9\u6709xml\u6807\u7b7e\u7684\u56fe\u7247\u8fdb\u884c\n\u8bad\u7ec3\u3002 \u5176\u4f59\u6587\u4ef6list.txt  test.txt  trainval.txt\u7b49\u6211\u4eec\u4e0d\u7528\uff0c\u5ffd\u7565\u6389\uff08\u91cc\u9762\u53ef\u80fd\u5b58\u50a8\u4e86\u5df2\u6709\u6807\u7b7e\u662f\u54ea\u4e9b\u56fe\u7247\uff0c\u4e0d\u8fc7\u6211\u6ca1\u68c0\u67e5\uff09\n\nxml\u6587\u4ef6\u548c jpg\u6587\u4ef6\uff0c\u5982\u679c\u6587\u4ef6\u540d\u76f8\u540c\uff08\u4e0d\u542b\u540e\u7f00\uff09, \u8bf4\u660e\u5bf9\u5e94\u7684jpg\u6587\u4ef6\u6709\u6807\u7b7e\uff0c \u6ca1\u6709\u5bf9\u5e94\u7684xml\u6587\u4ef6\u7684\u90fd\u662f\u65e0\u6807\u7b7e\u7684jpg","d948d00c":"# labels\u8fd9\u91cc\u76f4\u63a5\u7b80\u5355\u4e00\u70b9\uff0c \u4f7f\u7528 \u4e8c\u7ef4\u5217\u8868\uff0c \u5185\u5c42\u5217\u8868\u91cc\u662f\u6bcf\u4e2a\u56fe\u7247\u5bf9\u5e94\u7684\u6807\u7b7e\uff084\u4e2a\u5750\u6807\uff09"}}