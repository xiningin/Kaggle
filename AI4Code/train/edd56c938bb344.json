{"cell_type":{"c6d8f917":"code","e73ea564":"code","743407eb":"code","4a338ee5":"code","6fa5c463":"code","e427ed20":"code","cad8ee2e":"code","7f22093e":"code","f182573b":"code","50a67ef6":"code","aa50e079":"code","dd493fae":"code","b3250743":"code","5bf10390":"code","45e8a62f":"code","25b2db86":"code","1b5b9a3a":"code","266f2cea":"code","cedc8298":"code","37d6a966":"code","c2d6f402":"code","d0949aed":"markdown","6a134f96":"markdown","57b9ecac":"markdown","c487a6ab":"markdown","d37c0422":"markdown","fd945d53":"markdown","b766f216":"markdown","eab7ff96":"markdown","7a146d5c":"markdown","a675d8c4":"markdown","a7965b1c":"markdown","b0f18d55":"markdown","abe92e75":"markdown","adb3569c":"markdown","4156a6cc":"markdown","d9530ad1":"markdown","2b4fcdfe":"markdown","03edbb48":"markdown","8bc49a08":"markdown","cb43e893":"markdown","bcb2b587":"markdown","f3c8bc11":"markdown","e642c5bb":"markdown","946966ec":"markdown","24493518":"markdown","67a62631":"markdown","69aa0c81":"markdown","a0a940fd":"markdown","96175705":"markdown","a4973eee":"markdown","a87cbf93":"markdown","8c4d655a":"markdown"},"source":{"c6d8f917":"import warnings\nwarnings.simplefilter(action='ignore', category=Warning)\nimport numpy as np\nimport pandas as pd\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nfrom sklearn.neighbors import LocalOutlierFactor, KNeighborsRegressor\nfrom sklearn.impute import KNNImputer\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\nfrom sklearn.svm import SVR\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, VotingRegressor\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\nfrom sklearn.model_selection import GridSearchCV, cross_val_score\n\n# GEREKL\u0130 G\u00d6STER\u0130 AYARLAMALARIN YAPILMASI\n\npd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', None)\npd.set_option('display.float_format', lambda x: '%.3f' % x)\npd.set_option('display.width', 500)","e73ea564":"df_ = pd.read_csv(\"..\/input\/hitters\/Hitters.csv\")\ndf = df_.copy()","743407eb":"def check_df(dataframe):\n    print(\"##################### Shape #####################\")\n    print(f\"Rows: {dataframe.shape[0]}\")\n    print(f\"Columns: {dataframe.shape[1]}\")\n    print(\"##################### Types #####################\")\n    print(dataframe.dtypes)\n    print(\"####################### NA ######################\")\n    print(dataframe.isnull().sum())\n    print(\"########### Descriptive - Quantiles #############\")\n    print(dataframe.describe(percentiles=[0, 0.05, 0.50, 0.75, 0.95, 0.99]).T)\n    print(\"##################### Head ######################\")\n    print(dataframe.head())\n    \ncheck_df(df)","4a338ee5":"def grab_col_names(dataframe, cat_th=10, car_th=20):\n    cat_cols = [col for col in dataframe.columns if dataframe[col].dtypes == \"O\"]\n    num_but_cat = [col for col in dataframe.columns if dataframe[col].nunique() < cat_th and\n                   dataframe[col].dtypes != \"O\"]\n    cat_but_car = [col for col in dataframe.columns if dataframe[col].nunique() > car_th and\n                   dataframe[col].dtypes == \"O\"]\n    cat_cols = cat_cols + num_but_cat\n    cat_cols = [col for col in cat_cols if col not in cat_but_car]\n    num_cols = [col for col in dataframe.columns if dataframe[col].dtypes != \"O\"]\n    num_cols = [col for col in num_cols if col not in num_but_cat]\n\n    print(f\"Observations: {dataframe.shape[0]}\")\n    print(f\"Variables: {dataframe.shape[1]}\")\n    print(f'cat_cols: {len(cat_cols)}')\n    print(f'num_cols: {len(num_cols)}')\n    print(f'cat_but_car: {len(cat_but_car)}')\n    print(f'num_but_cat: {len(num_but_cat)}')\n    return cat_cols, num_cols, cat_but_car\n\ncat_cols, num_cols, cat_but_car = grab_col_names(df)","6fa5c463":"def outlier_thresholds(dataframe, col_name, q1=0.25, q3=0.75):\n    quartile1 = dataframe[col_name].quantile(q1)\n    quartile3 = dataframe[col_name].quantile(q3)\n    interquantile_range = quartile3 - quartile1\n    up_limit = quartile3 + 1.5 * interquantile_range\n    low_limit = quartile1 - 1.5 * interquantile_range\n    return low_limit, up_limit\n\n\n\ndef grab_outliers(dataframe, col_name):\n    low, up = outlier_thresholds(dataframe, col_name)\n\n    if dataframe[((dataframe[col_name] < low) | (dataframe[col_name] > up))].shape[0] > 10:\n\n        print(str(col_name) + \" variable have too much outliers: \" + str(dataframe[((dataframe[col_name] < low) | (dataframe[col_name] > up))].shape[0]))\n        \n        \n    elif (dataframe[((dataframe[col_name] < low) | (dataframe[col_name] > up))].shape[0] < 10) & \\\n            (dataframe[((dataframe[col_name] < low) | (dataframe[col_name] > up))].shape[0] > 0):\n\n        print(str(col_name) + \" variable have less than 10 outlier values: \" + str(dataframe[((dataframe[col_name] < low) | (dataframe[col_name] > up))].shape[0]))\n        \n    else:\n        print(str(col_name) + \" variable does not have outlier values\")\n\n\nfor col in num_cols:\n    grab_outliers(df, col)","e427ed20":"def replace_with_thresholds(dataframe, variable):\n    low_limit, up_limit = outlier_thresholds(dataframe, variable)\n    dataframe.loc[(dataframe[variable] < low_limit), variable] = low_limit\n    dataframe.loc[(dataframe[variable] > up_limit), variable] = up_limit\n\nfor col in num_cols:\n    replace_with_thresholds(df, col)","cad8ee2e":"# REMOVING NaN VALUES FROM TARGET VARIABLE:\n\n\ndf.dropna(inplace=True)","7f22093e":"for i in num_cols:\n    fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(20, 4))\n\n    sns.histplot(df[i], bins=30, ax=axes[0], color=\"navy\")\n    axes[0].set_title(i)\n\n    sns.boxplot(df[i], ax=axes[1], color=\"aqua\")\n    axes[1].set_title(i)\n    plt.show()","f182573b":"def cat_summary(dataframe, col_name, plot=False):\n    print(pd.DataFrame({col_name: dataframe[col_name].value_counts(),\n                        \"Ratio\": 100 * dataframe[col_name].value_counts() \/ len(dataframe)}))\n    print(\"##########################################\")\n    if plot:\n        sns.countplot(x=dataframe[col_name], data=dataframe)\n        plt.show()\n\n\nfor col in cat_cols:\n    cat_summary(df, col, True)","50a67ef6":"dff = df.select_dtypes(include=['float64', 'int64'])\nclf = LocalOutlierFactor(n_neighbors=20)\nclf.fit_predict(dff)\ndff_scores = clf.negative_outlier_factor_\nscores = pd.DataFrame(np.sort(dff_scores))\nscores.plot(stacked=True, xlim=[0, 60], style='.-')\nplt.show()\nth = np.sort(dff_scores)[3]\nclf_index = df[dff_scores < th].index\ndf.drop(index=clf_index, inplace=True)\n","aa50e079":"\n# Runs\/AtBat - Points won per hits in 1986-1987 season\ndf[\"Runs_AtBat\"] = df[\"Runs\"] \/ df[\"AtBat\"] * 100\ndf[\"Runs_AtBat_Year\"] = df[\"Runs_AtBat\"] \/ df[\"Years\"]\n\n# HmRun\/AtBat - Perfect hits per hits in 1986-1987 season\ndf[\"HmRun_AtBat\"] = df[\"HmRun\"] \/ df[\"AtBat\"] * 100\ndf[\"HmRun_AtBat_Year\"] = df[\"HmRun_AtBat\"] \/ df[\"Years\"]\n\n# CHits\/CAtBats - Successful hits per total hits of player's entire career\ndf[\"CHits_CAtBat\"] = df[\"CHits\"] \/ df[\"CAtBat\"] * 100\ndf[\"CHits_CAtBat_Year\"] = df[\"CHits_CAtBat\"] \/ df[\"Years\"]\n\n# CHmRun\/CAtBat - Perfect hits per total hits of player's entire career\ndf[\"CHmRun_CAtBat\"] = df[\"CHmRun\"] \/ df[\"CAtBat\"] * 100\ndf[\"CHmRun_CAtBat_Year\"] = df[\"CHmRun_CAtBat\"] \/ df[\"Years\"]\n\n# Errors\/CWalks - Player's error per mistakes done to the opponent\ndf[\"Error_Mean\"] = df[\"Errors\"] \/ df[\"CWalks\"] * 100\ndf['Error_Mean'] = df['Error_Mean'].replace(np.inf, 0)\ndf[\"Error_Mean_Year\"] = df[\"Error_Mean\"] \/ df[\"Years\"]\n\n# Ultimate_Power - Mean of player's hits, perfect hits and scores won in it's entire career:\ndf[\"Ultimate_Power\"] = (df[\"CHits\"] + df[\"CHmRun\"] + df[\"CRuns\"]) \/ 3\ndf[\"Ultimate_Power_Year\"] = df[\"Ultimate_Power\"] \/ df[\"Years\"]\n\n#CHits_CRBI\ndf[\"CHits_CRBI\"] = df[\"CHits\"] \/ df[\"CRBI\"] * 100\n\n# Error_CAT\ndf.loc[(df[\"Error_Mean\"] < 1), \"Error_CAT\"] = \"very_low_error\"\ndf.loc[(df[\"Error_Mean\"] >= 1) & (df[\"Error_Mean\"] < 2), \"Error_CAT\"] = \"low_error\"\ndf.loc[(df[\"Error_Mean\"] >= 2) & (df[\"Error_Mean\"] < 4.5), \"Error_CAT\"] = \"normal_error\"\ndf.loc[(df[\"Error_Mean\"] >= 4.5) & (df[\"Error_Mean\"] < 9), \"Error_CAT\"] = \"high_error\"\ndf.loc[(df[\"Error_Mean\"] >= 9), \"Error_CAT\"] = \"very_high_error\"\n\n# CAtBat_CAT\ndf.loc[(df[\"CAtBat\"] < 500), \"CAtBat_CAT\"] = \"very_low_score\"\ndf.loc[(df[\"CAtBat\"] >= 500) & (df[\"CAtBat\"] < 2000), \"CAtBat_CAT\"] = \"low_score\"\ndf.loc[(df[\"CAtBat\"] >= 2000) & (df[\"CAtBat\"] < 3000), \"CAtBat_CAT\"] = \"normal_score\"\ndf.loc[(df[\"CAtBat\"] >= 3000) & (df[\"CAtBat\"] < 5000), \"CAtBat_CAT\"] = \"high_score\"\ndf.loc[(df[\"CAtBat\"] >= 5000), \"CAtBat_CAT\"] = \"very_high_score\"\n\n# Division_CHits\ndf.loc[(df[\"Division\"] == \"E\") & (df[\"CHits\"] < 350), \"Division_CHits\"] = \"E_very_low_Chits\"\ndf.loc[(df[\"Division\"] == \"E\") & (df[\"CHits\"] >= 350) & (df[\"CHits\"] < 650), \"Division_CHits\"] = \"E_low_Chits\"\ndf.loc[(df[\"Division\"] == \"E\") & (df[\"CHits\"] >= 650) & (df[\"CHits\"] < 1000), \"Division_CHits\"] = \"E_mean_Chits\"\ndf.loc[(df[\"Division\"] == \"E\") & (df[\"CHits\"] >= 1000) & (df[\"CHits\"] < 1500), \"Division_CHits\"] = \"E_high_Chits\"\ndf.loc[(df[\"Division\"] == \"E\") & (df[\"CHits\"] >= 1500), \"Division_CHits\"] = \"E_very_high_Chits\"\ndf.loc[(df[\"Division\"] == \"W\") & (df[\"CHits\"] < 300), \"Division_CHits\"] = \"W_very_low_Chits\"\ndf.loc[(df[\"Division\"] == \"W\") & (df[\"CHits\"] >= 300) & (df[\"CHits\"] < 600), \"Division_CHits\"] = \"W_low_Chits\"\ndf.loc[(df[\"Division\"] == \"W\") & (df[\"CHits\"] >= 600) & (df[\"CHits\"] < 950), \"Division_CHits\"] = \"W_mean_Chits\"\ndf.loc[(df[\"Division\"] == \"W\") & (df[\"CHits\"] >= 950) & (df[\"CHits\"] < 1400), \"Division_CHits\"] = \"W_high_Chits\"\ndf.loc[(df[\"Division\"] == \"W\") & (df[\"CHits\"] >= 1400), \"Division_CHits\"] = \"W_very_high_Chits\"\n\n# Runs_Assists\ndf[\"Runs_Assists\"] = df[\"Runs\"] \/ df[\"Assists\"] * 100\ndf['Runs_Assists'] = df['Runs_Assists'].replace(np.inf, 0)\n\n\ncat_cols, num_cols, cat_but_car = grab_col_names(df)","dd493fae":"def one_hot_encoder(dataframe, categorical_cols, drop_first=True):\n    dataframe = pd.get_dummies(dataframe, columns=categorical_cols, drop_first=drop_first)\n    return dataframe\n\ndf = one_hot_encoder(df, cat_cols)\n\n\ncat_cols, num_cols, cat_but_car = grab_col_names(df)","b3250743":"X = df.drop(['Salary'], axis=1)\ny = df[\"Salary\"]","5bf10390":"models = [('LR', LinearRegression()),\n          (\"Ridge\", Ridge()),\n          (\"Lasso\", Lasso()),\n          (\"ElasticNet\", ElasticNet()),\n          ('KNN', KNeighborsRegressor()),\n          ('CART', DecisionTreeRegressor()),\n          ('RF', RandomForestRegressor()),\n          ('SVR', SVR()),\n          ('GBM', GradientBoostingRegressor()),\n          (\"XGBoost\", XGBRegressor(objective='reg:squarederror')),\n          (\"LightGBM\", LGBMRegressor())]\n\nfor name, regressor in models:\n    rmse = np.mean(np.sqrt(-cross_val_score(regressor, X, y, cv=10, scoring=\"neg_mean_squared_error\")))\n    print(f\"RMSE: {round(rmse, 2)} ({name}) \")","45e8a62f":"gbm_params = {\"learning_rate\": [0.001, 0.01, 0.1],\n              \"max_depth\": [3, 8],\n              \"n_estimators\": [100, 500, 1000],\n              \"subsample\": [1, 0.5, 0.7]}\n\n\nrf_params = {\"max_depth\": [5, 8, 15, None],\n             \"max_features\": [5, 7, \"auto\"],\n             \"min_samples_split\": [2, 8, 15, 20],\n             \"n_estimators\": [100, 200, 500]}\n\n\nregressors = [(\"GBM\", GradientBoostingRegressor(), gbm_params),\n              (\"RF\", RandomForestRegressor(), rf_params)]\n\n\n\n\nbest_models = {}\n\nfor name, regressor, params in regressors:\n    print(f\"########## Base RMSE score for: {name} ##########\")\n    rmse = np.mean(np.sqrt(-cross_val_score(regressor, X, y, cv=10, scoring=\"neg_mean_squared_error\")))\n    print(f\"RMSE score: {round(rmse, 2)} ({name}) \")\n\n    gs_best = GridSearchCV(regressor, params, cv=10, n_jobs=-1, verbose=False).fit(X, y)\n\n    final_model = regressor.set_params(**gs_best.best_params_)\n    rmse = np.mean(np.sqrt(-cross_val_score(final_model, X, y, cv=10, scoring=\"neg_mean_squared_error\")))\n    print(f\"########## RMSE score after 10-CV and best params: {name} ##########\")\n    print(f\"RMSE score: {round(rmse, 2)} ({name}) \")\n\n    print(f\"{name} best params: {gs_best.best_params_}\", end=\"\\n\\n\")\n\n    best_models[name] = final_model\n","25b2db86":"voting_reg = VotingRegressor(estimators=[('GBM', best_models[\"GBM\"]), ('RF', best_models[\"RF\"])])\n\nvoting_reg.fit(X, y)\n\ncombined_final_score = np.mean(np.sqrt(-cross_val_score(voting_reg, X, y, cv=10, scoring=\"neg_mean_squared_error\")))\n\nprint(f\"After using GBM and RF models with their best params, final combined RMSE score is: {round(combined_final_score, 2)}\")","1b5b9a3a":"df_y_pred = pd.DataFrame(voting_reg.predict(X), columns = ['Salary_pred'])\ndf_y_pred.head(10)","266f2cea":"df_y = df.filter([\"Salary\"])\ndf_y.head(10)","cedc8298":"graph = sns.regplot(x=df_y, y=df_y_pred, scatter_kws={'color': 'b', 's': 5}, ci=False, color=\"r\")\ngraph.set_title(f\"GBM & RF combined RMSE: {round(combined_final_score, 2)}\")\ngraph.set_xlabel(\"Salary\")\ngraph.set_ylabel(\"Predicted Salary\")\nplt.xlim(0, 1700)\nplt.ylim(bottom=0)\nplt.show()","37d6a966":"gbm_model = GradientBoostingRegressor(random_state=17)\n\nfirst_rmse = np.mean(np.sqrt(-cross_val_score(gbm_model, X, y, cv=10, scoring=\"neg_mean_squared_error\")))\ngbm_best_grid = GridSearchCV(gbm_model, gbm_params, cv=10, n_jobs=-1, verbose=False).fit(X, y)\nfinal_model = gbm_model.set_params(**gbm_best_grid.best_params_)\nfinal_rmse = np.mean(np.sqrt(-cross_val_score(final_model, X, y, cv=10, scoring=\"neg_mean_squared_error\")))\n\ngbm_final = gbm_model.set_params(**gbm_best_grid.best_params_, random_state=17).fit(X, y)\n\nprint(f\"After choosing the best model as GBM and best parameters as follows: {gbm_best_grid.best_params_}, the final model's RMSE is calculated at {round(final_rmse, 2)}\")","c2d6f402":"def plot_importance(model, features, num=len(X)):\n    feature_imp = pd.DataFrame({'Value': model.feature_importances_, 'Feature': features.columns})\n    plt.figure(figsize=(10, 10))\n    sns.set(font_scale=1)\n    sns.barplot(x=\"Value\", y=\"Feature\", data=feature_imp.sort_values(by=\"Value\", ascending=False), color=\"navy\")\n    plt.title('Features')\n    plt.tight_layout()\n    plt.show()\n\n\nplot_importance(gbm_final, X)","d0949aed":"**Setting \"X\" as independent variables without dependent variable \"Salary\" and \"y\" as dependent variable \"Salary\":**","6a134f96":"**Comment:**\n\nOur dataset have 20 variables, 3 of them has been categorized as categoric and 17 of them as numeric.","57b9ecac":"# Best model's score (GBM)","c487a6ab":"# Feature Engineering","d37c0422":"# Automated Hyperparameter Optimization","fd945d53":"**Comment:**\n\n**Types:** Most of the variables are in the int64 type. Only 3 of them are object and the target variable is in the float64 type.\n\n**NA:** Only target variable \"Salary\" has 59 NaN values. **NaN VALUES CANNOT BE FILLED AS THIS IS IS VIOLATION OF TRANSPARENCY AND WILL CAUSE BIAS TO THE TARGET VARIABLE.**\n\n**Quantiles:** Looking to variable's 75, 95, 99 and max values we can assume that there are outlier values in the dataset.","b766f216":"# Categoric variable analysis","eab7ff96":"**Comment:**\n\nWhen we assign outlier threshold at 0.25\/0.75, I've found the size of outlier values in each variable as shown above.\n\nWith the replace_with_thresholds function outliers have been replaced with thresholds.","7a146d5c":"**Follow me on [LinkedIn](https:\/\/www.linkedin.com\/in\/blirind\/) and keep reading my data projects in [Kaggle](https:\/\/www.kaggle.com\/blirinddanjolli).**\n\n**Thank you for your reading and support!**\n\n**This work has been done with the support of** [VBO](https:\/\/www.veribilimiokulu.com\/), [Miuul](https:\/\/miuul.com\/),  [Vahit Keskin](https:\/\/www.linkedin.com\/in\/vahitkeskin\/), [O\u011fuz Erdo\u011fan](https:\/\/www.linkedin.com\/in\/oguzerdo\/), [Hande K\u00fc\u00e7\u00fckbulut](https:\/\/www.linkedin.com\/in\/hande-kucukbulut\/), [Mehmet Tuzcu](https:\/\/www.linkedin.com\/in\/mehmettuzcu\/) & [Burak Do\u011frul](https:\/\/www.linkedin.com\/in\/burakdogrul\/).","a675d8c4":"**Here is the end of the machine learning of Hitters dataset with various models.**\n\n**The results show that using Gradient Boosting Regressor (GBM) with parameters as follows\" {'learning_rate': 0.01, 'max_depth': 8, 'n_estimators': 1000, 'subsample': 0.5} gives us the lowest RMSE for target variable \"Salary\".**","a7965b1c":"![Baseball.jpg](attachment:fd39f14f-77f0-4028-ab99-d342c8d4c0a5.jpg)","b0f18d55":"As I've found GBM and RF as best predicting models, Voting Regressor allow us to stack and generate ensemble learning of dataset. So with Voting Regressor, multiply machine learning algorithms can be used together to reach a lower RMSE level.","abe92e75":"# One-Hot Encoding","adb3569c":"One-Hot Encoding must be done because the machine learning cannot measure string values.","4156a6cc":"# Plot importance","d9530ad1":"# Outlier Analysis","2b4fcdfe":"# Removing NaN from Salary","03edbb48":"# Modelling","8bc49a08":"**Comment:**\n\nAbove I have removed values from 0 to 3rd as they classified as \"hidden outliers\".","cb43e893":"**2 out of 5 most important features are the ones we have generated from independent variables of dataset.**","bcb2b587":"First 10 **predicted** target values:","f3c8bc11":"# Machine Learning with Hitters dataset (RMSE ~ 200)\n\n\nIn the previous Hitters analysis [HERE](https:\/\/www.kaggle.com\/blirinddanjolli\/linear-regression-hitters) I've used Linear Regression to predict target variable \"Salary\" and the lowest 10-fold cross validated RMSE has been found at 289.70. But with other Machine Learning models, we are able to predict the target variable with lower deviation. \n\n**Business Problem:**\n\nHitters dataset contains Salary information and a batch of information about baseball players in 1986. Can a machine learning project be carried out to estimate player's salary?\n\n**Dataset Story:**\n\nThis dataset was originally taken from the StatLib library at Carnegie Mellon University. The dataset is part of the data used in the 1988 ASA Graphics Division Poster Session. Salary data are from Sports Illustrated, April 20, 1987.\n\n**Variables:**\n\nAtBat: 1986-1987 the number of times a baseball bat hits the ball\n\nHits: 1986-1987 hits\n\nHmRun: 1986-1987 perfect hits\n\nRuns: 1986-1987 points won to the team\n\nRBI: The number of players a batsman walked when he hit\n\nWalks: Number of mistakes made by the opposing player\n\nYear: How long the player has played in the major league (years)\n\nCAtBat: Number of ball hits throughout the player's career\n\nCHits: Number of hits hit during the player's career\n\nCHmRun: Number of perfect hits during the player's career\n\nCRuns: Player scoring points for his team throughout his career\n\nCRBI: The number of players the player has made to walk during his career\n\nCWalks: The number of mistakes the player has made to the opposing player during his career\n\nLeague: Has a factor that indicates ownership of the player's seasonal levels (A or N)\n\nDivision: A factor with additional contributing indicator E and W levels for 1986 season\n\nPutOuts: Helping your teammate in-game\n\nAssists: 1986-1987 counts as assists\n\nErrors: Number of player errors in 1986-1987\n\nSalary: The player earned money 1986-1987 (thousand)\n\nNewLeague: A factor showing the league of a player used in 1987 and has A & N levels\n\n\n**The algorithms below are written in way to predict Hitters \"Salary\" variable with RMSE ~200.**","e642c5bb":"# Importing dataset","946966ec":"# Numerical variable analysis","24493518":"# Stacking & Ensemble Learning","67a62631":"# Necessary libraries","69aa0c81":"**Check for outliers (hidden outliers) with Local Outlier Factor:**","a0a940fd":"# Graph of Salary & Predicted Salary","96175705":"**Plot importance of GBM:**","a4973eee":"# Exploratory Data Analysis","a87cbf93":"I have found Gradient Boosting Regressor (GBM) and Random Forest Regressor (RF) as models with lowest RMSE so I've decided to use only these two models after automated hyperparameter optimization.\n\n**What is Automated Hyperparameter Optimization?**\n\nAll models have their own hyperparameters which aren't extracted from the dataset but given from tester (us).\n\nAs default, all models have their own hyperparameters set at a point. For example GBM's hyperparameters are set as follows: *(loss='ls', learning_rate=0.1, n_estimators=100, subsample=1.0, criterion='friedman_mse', min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0., max_depth=3, min_impurity_decrease=0., min_impurity_split=None, init=None, random_state=None, max_features=None, alpha=0.9, verbose=0, max_leaf_nodes=None, warm_start=False, validation_fraction=0.1, n_iter_no_change=None, tol=1e-4, ccp_alpha=0.0)*.\n\nFor GBM model I've set learning_rate, max_depth, n_estimators and subsample's default hyperparameters along with other different levels for each of them.\n\nFor RF model I've set max_depth, max_features, min_samples_split and n_estimators's default hyperparameters along with other different levels for each of them.","8c4d655a":"First 10 values of **real** Salary variable:"}}