{"cell_type":{"03dd24dd":"code","baf8302c":"code","d5e604f8":"code","da189d09":"code","2dc13d13":"code","4f0fcdf4":"code","d80dfe1f":"code","0466b003":"code","7e0da575":"code","c0752dd8":"code","382c9e94":"code","676bdb08":"code","3b2af321":"code","033db427":"code","3fef8915":"code","f1994aa4":"code","36523731":"code","a580aac3":"code","f92c9ab0":"code","cccfa3dc":"code","e76d6bff":"code","d9365e3b":"code","375b8638":"code","d3a9802e":"code","967b1200":"code","27692f8d":"code","75f8d736":"code","9ee1bdd5":"code","2eb4b773":"code","701684cd":"code","16332688":"code","8667eadf":"code","29d7c87b":"code","fa94c5f2":"code","b2e7537b":"code","74a152a3":"code","c1683deb":"code","9a5acc60":"code","b252341b":"code","7990ad32":"code","2078b003":"code","9944e34d":"code","e990bae9":"code","cdc69446":"code","d0e79cad":"code","afd553eb":"code","a1b9275d":"code","31530fea":"code","169795ef":"code","47da7dd1":"code","0c2448bc":"code","5e35222c":"code","cc7e70fb":"code","f666aefa":"code","9fdd027e":"code","0eaa02c3":"code","82deb1a5":"code","9b344f30":"code","ec936e31":"code","fa4a391f":"code","61221a28":"code","1cbb80c5":"code","f4931d52":"code","01628bad":"code","edc21eeb":"code","b85139a7":"code","faba018e":"code","c28b32c6":"code","0bad7a8b":"code","0db8636f":"code","0681981b":"code","a06b3e43":"code","c0173426":"code","9861cccc":"code","749370d1":"code","2eb776ae":"code","10078b23":"code","467fd16d":"code","03013766":"code","6521267b":"code","3e93fe93":"code","7ddc4e21":"code","68597d85":"code","8e0b2c93":"code","35499f53":"code","de3090a2":"code","bda5fe7c":"code","d66e2aa5":"code","b6fb9a99":"code","3d7691ff":"code","89b70ddb":"code","39fe1551":"code","fbcf2a1f":"code","69cd66a8":"code","83d4b918":"code","a5f968d5":"code","792305d0":"code","dfc817f4":"code","b822fb1c":"code","97f756b5":"markdown","94dccf46":"markdown","30528c9b":"markdown","7fc23250":"markdown","511e66a2":"markdown","e2fc7f62":"markdown","5b79e6b9":"markdown","768d2860":"markdown","c47646d0":"markdown","77725bc4":"markdown","0fc95cc0":"markdown","64a4ff7f":"markdown","2f25332c":"markdown","acc4ce74":"markdown","1c037951":"markdown","7a042376":"markdown","989320db":"markdown","715ddc5d":"markdown","cf6c4816":"markdown","cc4525b6":"markdown","c912027f":"markdown","85070964":"markdown","eb0e0711":"markdown","3ff34ef9":"markdown","9d3064fb":"markdown","b45bd173":"markdown","31d0ccb3":"markdown","fa63febf":"markdown","ce7670fb":"markdown","00c049bc":"markdown","2fb4d941":"markdown","d7019f65":"markdown","bec367f1":"markdown","7774c5c4":"markdown","b868e991":"markdown","0ccb1615":"markdown","1eb66228":"markdown","fc931463":"markdown","b4877fe3":"markdown","ee4c40fe":"markdown","c309d68f":"markdown","bc47571e":"markdown","b41b1666":"markdown","a95afe7c":"markdown","7f9f74a4":"markdown","75ab1e13":"markdown","f24d94e1":"markdown","3987e4ad":"markdown","ffb787b2":"markdown","40ebb90d":"markdown","fa91c86b":"markdown","eb1a58c4":"markdown","14417c5e":"markdown","76e70203":"markdown","90ab8837":"markdown","0a0deda4":"markdown","8688c91e":"markdown","2b533c84":"markdown","9298011c":"markdown","38c2a055":"markdown","6ee7cfde":"markdown","3d78f44c":"markdown","9f69622e":"markdown","9c40b75e":"markdown","210faf69":"markdown","9deb402b":"markdown","26005fa6":"markdown","7a3e1f3a":"markdown","cdec9389":"markdown","5a2378b9":"markdown","15cddc5a":"markdown","405d87fc":"markdown","423bfda6":"markdown","80e71a04":"markdown","50eafe6a":"markdown","4a66461f":"markdown","13ce52b9":"markdown","0dcd8947":"markdown","4e923e22":"markdown","f90bd6cb":"markdown","bb9d3b9f":"markdown","dc73f0b1":"markdown","77250980":"markdown","ddfd1e02":"markdown","f2571f1e":"markdown","85f52fd7":"markdown","8357d41d":"markdown","e04d3199":"markdown","8b26d191":"markdown","859c819e":"markdown","5aee07e4":"markdown","f0122f71":"markdown","5ad71c87":"markdown","100fc1ee":"markdown"},"source":{"03dd24dd":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport folium\nfrom folium.plugins import FastMarkerCluster\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n\n\n%matplotlib inline\n\n\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","baf8302c":"df = pd.read_csv('\/kaggle\/input\/new-york-city-airbnb-open-data\/AB_NYC_2019.csv', index_col='id')\nlistings = pd.read_csv('\/kaggle\/input\/ab-ny-august-2019\/listings.csv', index_col='id')","d5e604f8":"df.head()","da189d09":"listings.head()","2dc13d13":"target_columns = [\"property_type\", \"accommodates\",  \"review_scores_value\", \"review_scores_cleanliness\", \"review_scores_location\", \"review_scores_accuracy\", \"review_scores_communication\", \"review_scores_checkin\", \"review_scores_rating\", \"maximum_nights\", \"host_is_superhost\", \"host_response_time\", \"host_response_rate\",  'bathrooms', 'bedrooms', 'beds']\ndata = pd.merge(df, listings[target_columns], on='id', how='left')\ndata.info()","4f0fcdf4":"data.isnull().sum()","d80dfe1f":"data.describe()","0466b003":"data['reviews_per_month'] = data['number_of_reviews'] \/ 12\n\nreviews = ['review_scores_value', 'review_scores_cleanliness', 'review_scores_location', 'review_scores_accuracy', 'review_scores_communication', 'review_scores_checkin', 'review_scores_rating']\nfor i in reviews:\n  data[i].fillna(data[i].mean(), inplace=True)\n\ndata['accommodates'].fillna(data['accommodates'].mean(), inplace=True)\ndata['maximum_nights'].fillna(data['maximum_nights'].mean(), inplace=True)\n\ndata.drop(columns=['last_review'], inplace=True)\n\ncat_columns = ['host_response_time', 'host_response_rate', 'property_type', 'host_is_superhost']\n\nfor i in cat_columns:\n  data[i].fillna(data[i].value_counts().idxmax(), inplace=True)\n\na = ['bathrooms', 'beds', 'bedrooms']\nfor i in a:\n  data[i].fillna(data[i].mean(), inplace=True)\n\n\ndata.isnull().sum()\n","7e0da575":"plt.figure(figsize=(6,6))\nsns.boxplot(y=data['price'])\nplt.title(\"Distribution of Price\")\nplt.show()","c0752dd8":"mean = data['price'].mean()\nstd = data['price'].std()\nupper_limit = mean + 3 * std\ndata = data[data['price'] <= upper_limit]","382c9e94":"plt.figure(figsize=(6,6))\nsns.boxplot(y=data['price'])\nplt.title(\"Distribution of Price\")\nplt.show()","676bdb08":"data.head()","3b2af321":"plt.figure(figsize=(6,6))\nnumbers = data['neighbourhood_group'].value_counts()\nplt.pie(numbers.values, labels=numbers.index, colors=['b', 'r', 'g', 'cyan', 'gray'], autopct='%1.1f%%')\nplt.title('Numbers in Each Neigbourhoods')","033db427":"plt.figure(figsize=(6,6))\nnumbers = data['room_type'].value_counts()\nplt.pie(numbers.values, labels=numbers.index, colors=['cyan', 'green', 'pink'], autopct='%1.1f%%', shadow=True,startangle=90)\nplt.title('Numbers in Each Room Types')","3fef8915":"fig = plt.figure(figsize=(15,6))\n\nax1 = fig.add_subplot(121)\nsns.scatterplot(data['longitude'], data['latitude'], hue=data['neighbourhood_group'], ax=ax1)\nax1.set_title('Distribution in Map')\n\nax2 = fig.add_subplot(122)\nsns.scatterplot(data['longitude'], data['latitude'], hue=data['room_type'], ax=ax2)\nax2.set_title('Distribution of Room Types in the Map')\n\nplt.show()","f1994aa4":"plt.figure(figsize=(10,4))\nsns.countplot(data['neighbourhood_group'], hue=data['room_type'])\nplt.title('Distribution of the room types in each neighbourhood group')\nplt.show()","36523731":"### Price Distribution in Each Neighbourhood","a580aac3":"plt.figure(figsize=(15,6))\nsns.boxplot(data=data, x='neighbourhood_group', y='price', palette='GnBu_d')\nplt.title('Density and distribution of prices for each neighbourhood group', fontsize=15)\nplt.xlabel('Neighbourhood group')\nplt.ylabel(\"Price\")","f92c9ab0":"plt.figure(figsize=(15,6))\nsns.boxplot(data=data, x='room_type', y='price', palette='GnBu_d')\nplt.title('Density and distribution of prices for each Room Type', fontsize=15)\nplt.xlabel('Room Type')\nplt.ylabel(\"Price\")","cccfa3dc":"latitudes = np.array(data['latitude'])\nlongitudes = np.array(data['longitude'])\nla_mean = latitudes.mean()\nlo_mean = longitudes.mean()\nlocations = list(zip(latitudes, longitudes))\n\nm = folium.Map(location=[la_mean, lo_mean], zoom_start= 11.5)\nFastMarkerCluster(data=locations).add_to(m)\nm","e76d6bff":"plt.figure(figsize=(6, 6))\nsns.distplot(data['accommodates'])\nplt.title('Distribution of Accommodates in New York')\nplt.show()","d9365e3b":"plt.figure(figsize=(6, 6))\nsns.distplot(data['price'], kde=False)\nplt.title('Distribution of price')","375b8638":"data['log_price'] = np.log10(data['price'] + 1)\nplt.figure(figsize=(6, 6))\nsns.distplot(data['log_price'], kde=False)\nplt.title('Distribution of price in Logarithm')","d3a9802e":"a = data.groupby('neighbourhood')['price'].mean().sort_values(ascending=True).head(20)\nd = data.groupby('neighbourhood')['price'].mean().sort_values(ascending=False).head(20)","967b1200":"fig = plt.figure(figsize=(20,10))\n\nax1 = fig.add_subplot(121)\nsns.barplot(y=a.index, x=a.values, ax=ax1)\nax1.set_title('The cheapest 20 neighbourhood')\n\nax2 = fig.add_subplot(122)\nsns.barplot(y=d.index, x=d.values, ax=ax2)\nax2.set_title('The most expensive 20 neighbourhood')\nplt.show()","27692f8d":"fig, axes = plt.subplots(nrows=2, ncols=3, figsize=(15, 9))\n\nfor ax, name in zip(axes.flatten(), reviews):\n  ax.hist(data[name], bins=20)\n  ax.set_title(f\"Distribution of {name}\")\n\nplt.show()\n\n","75f8d736":"fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(15, 4))\n\nrooms = ['bedrooms', 'bathrooms']\n\nfor ax, name in zip(axes.flatten(), rooms):\n  ax.hist(data[name], bins=20)\n  ax.set_title(f\"Distribution of {name}\")\n\nplt.show()\n","9ee1bdd5":"plt.figure(figsize=(6,6))\n\nplt.hist(data['availability_365'], bins=20)\n\nplt.title(\"Distribution of Availability in 365 Days\")\n\nplt.show()","2eb4b773":"corr = data.corr()\nmask = np.triu(np.ones_like(corr, dtype=np.bool))\nf, ax = plt.subplots(figsize=(11, 9))\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\nsns.heatmap(corr, mask=mask, cmap=cmap, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})","701684cd":"numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n\nnewdf = data.select_dtypes(include=numerics)\nnrows = int(len(newdf.columns) \/ 3) + 1\nfig, axes = plt.subplots(nrows=nrows, ncols=3, figsize=(24, 6*nrows))\nfig.subplots_adjust(hspace=0.5)\n\nfor ax, name in zip(axes.flatten(), newdf.columns):\n  \n  sns.regplot(x=name, y='price', data=newdf, ax=ax)\n  ax.set_title(f\"Correlation between {name} and the price\")\n\nplt.show()","16332688":"a = data.groupby(['neighbourhood_group', 'neighbourhood'])['price'].mean().sort_values(ascending=False).head(50)\na = a.reset_index()\na","8667eadf":"plt.figure(figsize=(12, 6))\ndf_pivot = data.pivot_table(values='price', index='room_type', columns='neighbourhood_group', aggfunc='mean')\nsns.heatmap(df_pivot, annot=True, fmt='.1f', cmap='Blues')\nplt.suptitle('Mean Price')\nplt.plot()","29d7c87b":"a = data.groupby(['neighbourhood_group', 'property_type'])['price'].mean().sort_values(ascending=False).head(20)\na = a.reset_index()\na","fa94c5f2":"data = data[data.price > 0]\ndata.columns","b2e7537b":"data.drop(columns=['name', 'host_id', 'host_name', 'reviews_per_month'], inplace=True)","74a152a3":"data.dtypes","c1683deb":"from sklearn.preprocessing import LabelEncoder\n\ncategorical = data.select_dtypes(include=['object']).columns\n\nfor i in categorical:\n  data[i] = LabelEncoder().fit_transform(data[i])\n\n\ndata.dtypes","9a5acc60":"data = data.reset_index(drop=True)\n\ndata.head()","b252341b":"data.drop(columns=['log_price'], inplace=True)","7990ad32":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\n\nX = data.drop(columns=['price'])\ny = data['price']\n\ncolumns = X.columns\nscaler = StandardScaler()\nX[columns] = scaler.fit_transform(X[columns])\n\n\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nprint(f\"There are {X_train.shape[0]} traning data\")\nprint(f\"There are {X_test.shape[0]} test data\")\n\n\nX.head()","2078b003":"from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.neighbors import KNeighborsRegressor\n\n\nknn = KNeighborsRegressor(5, metric=\"euclidean\")\nknn.fit(X_train, y_train)\ny_pred = knn.predict(X_test)\n\nmse = mean_squared_error(y_pred, y_test)\nmae = mean_absolute_error(y_pred, y_test)\nrmse = np.sqrt(mse)\nr2 = r2_score(y_test, y_pred)\n\nprint(\"Mean Squared Error: {}\".format(mse))\nprint(\"Mean Absolute Error: {}\".format(mae))\nprint(\"Root Mean Absolute Error: {}\".format(rmse))\nprint(\"R2 score: {}\".format(r2))","9944e34d":"prediction_dictionaries = {'KNN-Default': y_pred}","e990bae9":"prediction_list = pd.DataFrame({'Actual Values': np.array(y_test).flatten(), 'KNN-Default': y_pred.flatten()}).head(20)\nprediction_list.set_index('Actual Values', inplace=True)\nprediction_list","cdc69446":"error_dict = {'KNN Default': [mse, r2]}\nerror_list = pd.DataFrame()\nerror_list['KNN Default'] = [mse, r2]\nerror_list.reset_index(inplace=True, drop=True)\n#error_list.rename(columns={0: 'MSE KNN Default'}, inplace=True)\nerror_list.index =['Mean Squared Error', 'R2 Score']\nerror_list.T","d0e79cad":"plt.figure(figsize=(6,6))\nsns.regplot(y_pred, y_test)\nplt.title(\"The correlation line between predictions and the actual values\")\nplt.show()","afd553eb":"def plot_all_r2():\n  length = len(prediction_dictionaries)\n  n_col = 2\n  if length < 2:\n    n_col = length % 2\n  \n  nrow = 1\n  if(length > 2):\n    nrow = int(length \/ 2) \n    if length % 2 != 0:\n      nrow+=1\n  \n  fig, axes = plt.subplots(nrow, n_col, figsize=( 16, 3 * length))\n  for ax, key in zip(axes.flatten(), prediction_dictionaries.keys()):\n    sns.regplot(prediction_dictionaries[key], y_test, ax=ax)\n    ax.set_title(\"The correlation line in {}\".format(key))\n  plt.show()","a1b9275d":"#Bu niye uzun s\u00fcrd\u00fc acaba? Bir task a 36s (Bakaca\u011f\u0131m)\nfrom sklearn.model_selection import GridSearchCV\nparam_grid = {'p': [1, 2],  \n              'n_neighbors' : [ 5, 10, 15]\n              } \n\ngrid_knn = GridSearchCV(KNeighborsRegressor(n_jobs=-1), param_grid, refit = True, verbose = 10, n_jobs=-1, cv=5,scoring=\"neg_mean_squared_error\") \n\ngrid_knn.fit(X, y)\n\n","31530fea":"print(f\"Best parameters are {grid_knn.best_params_}\") \nprint(\"Best score is {}\".format(grid_knn.best_score_ * -1))\nprint(\"Best model is {}\".format(grid_knn.best_estimator_))\n#print(\"The score for hyperparameter tuning are {}\".format(grid.cv_results_))","169795ef":"knr_best = KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski',\n                    metric_params=None, n_jobs=-1, n_neighbors=15, p=1,\n                    weights='uniform')\nknr_best.fit(X_train, y_train)\ny_pred_best = knr_best.predict(X_test)\n\nmse_knn_best = mean_squared_error(y_pred_best, y_test)\nmae_knn_best = mean_absolute_error(y_pred, y_test)\nrmse_knn_best = np.sqrt(mse)\nr2_knn_best = r2_score(y_test, y_pred_best)\n\nprint(\"Mean Squared Error: {}\".format(mse_knn_best))\nprint(\"Mean Absolute Error: {}\".format(mae_knn_best))\nprint(\"Root Mean Absolute Error: {}\".format(rmse_knn_best))\nprint(\"R2 score: {}\".format(r2_knn_best))","47da7dd1":"prediction_dictionaries['Knn-Best']   = y_pred_best","0c2448bc":"prediction_list['KNN-Best'] = y_pred_best[:20]\nprediction_list","5e35222c":"error_list['MSE KNN-Best'] = [mse_knn_best, r2_knn_best]\nerror_list.T","cc7e70fb":"from sklearn.svm import LinearSVR, SVR\nclf_svr = LinearSVR()\nclf_svr.fit(X_train, y_train)\n\npreds_svr = clf_svr.predict(X_test)\n\nmse_svr = mean_squared_error(preds_svr, y_test)\nmae_svr = mean_absolute_error(preds_svr, y_test)\nrmse_svr = np.sqrt(mse_svr)\nr2_svr = r2_score(y_test, preds_svr)\n\nprint(\"Mean Squared Error: {}\".format(mse_svr))\nprint(\"Mean Absolute Error: {}\".format(mae_svr))\nprint(\"Root Mean Absolute Error: {}\".format(rmse_svr))\nprint(\"R2 Score: {}\".format(r2_svr))","f666aefa":"prediction_dictionaries['SVR - Default'] = preds_svr\nplot_all_r2()","9fdd027e":"prediction_list['SVR-Default'] = np.array(preds_svr[:20])\nprediction_list","0eaa02c3":"error_list['SVR Default'] = [mse_svr, r2_svr]\nerror_list.T","82deb1a5":"param_grid = {'C': [0.1, 1, 10, 100, 1000],  \n              'loss': ['epsilon_insensitive', 'squared_epsilon_insensitive'], \n              'dual': [True, False],\n              'tol': [0.0001, 0.00001]} \n\ngrid = GridSearchCV(LinearSVR(), param_grid, refit = True, verbose = 10, n_jobs=-1, cv=5,scoring=\"neg_mean_squared_error\") \n\ngrid.fit(X, y)","9b344f30":"print(f\"Best parameters are {grid.best_params_}\") \nprint(\"Best score is {}\".format(grid.best_score_ * -1))\nprint(\"Best model is {}\".format(grid.best_estimator_))\nprint(\"scores {}\".format(grid.cv_results_['mean_test_score']))","ec936e31":"svr_best = LinearSVR(C=1, dual=True, epsilon=0.0, fit_intercept=True,\n          intercept_scaling=1.0, loss='epsilon_insensitive', max_iter=1000,\n          random_state=None, tol=0.0001, verbose=0)\n\nsvr_best.fit(X_train, y_train)\n\npreds_svr_best = svr_best.predict(X_test)\n\nmse_svr_best = mean_squared_error(preds_svr_best, y_test)\nmae_svr_best = mean_absolute_error(preds_svr_best, y_test)\nrmse_svr_best = np.sqrt(mse_svr_best)\nr2_svr_best = r2_score(y_test, preds_svr_best)\n\nprint(\"Mean Squared Error: {}\".format(mse_svr_best))\nprint(\"Mean Absolute Error: {}\".format(mae_svr_best))\nprint(\"Root Mean Absolute Error: {}\".format(rmse_svr_best))\nprint(\"R2 Score: {}\".format(r2_svr_best))","fa4a391f":"prediction_dictionaries['SVR - Best'] = preds_svr_best\nplot_all_r2()","61221a28":"prediction_list['SVR-Best'] = np.array(preds_svr_best[:20])\nprediction_list","1cbb80c5":"error_list['SVR Best'] = [mse_svr_best, r2_svr_best]\nerror_list.T","f4931d52":"!pip install -q git+https:\/\/github.com\/tensorflow\/docs","01628bad":"import tensorflow as tf\n\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nimport tensorflow_docs as tfdocs\nimport tensorflow_docs.plots\nimport tensorflow_docs.modeling\n\nprint(tf.__version__)","edc21eeb":"def build_model():\n  model = keras.Sequential([\n    layers.Dense(64, activation='relu', input_shape=([X_train.shape[1]])),\n    layers.Dense(64, activation='relu'),\n    layers.Dense(1)\n  ])\n\n  optimizer = tf.keras.optimizers.RMSprop(0.001)\n\n  model.compile(loss='mse',\n                optimizer=optimizer,\n                metrics=['mae', 'mse'])\n  return model","b85139a7":"model = build_model()\n\nhistory = model.fit(\n  X_train, y_train,\n  epochs=200, validation_split = 0.2, verbose=0, callbacks=[tfdocs.modeling.EpochDots()])","faba018e":"model.summary()","c28b32c6":"hist = pd.DataFrame(history.history)\nplt.figure(figsize=(10, 6))\nplt.plot(hist.mse)\nplt.title(\"MSE Graph in Neural Network\")\nplt.show()","0bad7a8b":"preds_nn = model.predict(X_test)\n\nmse_nn = mean_squared_error(y_test, preds_nn)\nmae_nn = mean_absolute_error(y_test, preds_nn)\nrmse_nn = np.sqrt(mse_nn)\nr2_nn = r2_score(y_test, preds_nn)\n\nprint(\"Mean Squared Error: {}\".format(mse_nn))\nprint(\"Mean Absolute Error: {}\".format(mae_nn))\nprint(\"Root Mean Absolute Error: {}\".format(rmse_nn))\nprint(\"R2 score: {}\".format(r2_nn))\n","0db8636f":"prediction_dictionaries['Neural Network']  = preds_nn\nplot_all_r2()","0681981b":"prediction_list['Neural Network'] = np.array(preds_nn[:20])\nprediction_list","a06b3e43":"error_list['Neural Network'] = [mse_nn, r2_nn]\nerror_list.T","c0173426":"from sklearn import tree\nfrom sklearn import metrics\n\ntree_model = tree.DecisionTreeRegressor()\ntree_model.fit(X_train, y_train) # x -> features, y->target (price)\ntree_model_prediction = tree_model.predict(X_test)\n\nsee_result = pd.DataFrame({\n    'Actual': y_test, \n    'Predicted': tree_model_prediction\n    })\n\ntree_mse = metrics.mean_squared_error(y_test, tree_model_prediction)\ntree_mae = metrics.mean_absolute_error(y_test, tree_model_prediction)\ntree_rmse = np.sqrt(tree_mse)\ntree_r2 = metrics.r2_score(y_test, tree_model_prediction)\n\nprint(\"Mean Squared Error: {}\".format(tree_mse))\nprint(\"Mean Absolute Error: {}\".format(tree_mae))\nprint(\"Root Mean Absolute Error: {}\".format(tree_rmse))\nprint(\"R2 score: {}\".format(tree_r2))","9861cccc":"prediction_dictionaries['Decision Tree - Default'] = tree_model_prediction\nplot_all_r2()","749370d1":"prediction_list['Decision Tree - Default'] = np.array(preds_svr[:20])\nprediction_list","2eb776ae":"error_list['Decision Tree - Default'] = [tree_mse, tree_r2]\nerror_list.T","10078b23":"parameters = {\n    'max_depth': [1, 2, 3, 4, 5, 6, 7, 8],\n    'min_samples_leaf': [1, 2, 3, 4, 5],\n    'min_samples_split': [2, 3, 4, 5],\n}\ntree_grid = GridSearchCV(tree_model, parameters, refit = True, verbose = 1, n_jobs=-1, cv=5, scoring=\"neg_mean_squared_error\") \ntree_grid.fit(X, y)","467fd16d":"print(f\"Best parameters are {tree_grid.best_params_}\") \nprint(\"Best MSE is {}\".format(tree_grid.best_score_ * -1))","03013766":"tree_model_best = tree.DecisionTreeRegressor(max_depth = 7, min_samples_leaf = 4, min_samples_split = 4)\ntree_model_best.fit(X_train, y_train) \ntree_model_prediction_best = tree_model_best.predict(X_test)\n\ntree_mse_best = metrics.mean_squared_error(y_test, tree_model_prediction_best)\ntree_mae_best = metrics.mean_absolute_error(y_test, tree_model_prediction_best)\ntree_rmse_best = np.sqrt(tree_mse_best)\ntree_r2_best = metrics.r2_score(y_test, tree_model_prediction_best)\n\nprint(\"Mean Squared Error: {}\".format(tree_mse_best))\nprint(\"Mean Absolute Error: {}\".format(tree_mae_best))\nprint(\"Root Mean Absolute Error: {}\".format(tree_rmse_best))\nprint(\"R2 score: {}\".format(tree_r2_best))","6521267b":"prediction_dictionaries['Decision Tree - Best'] = tree_model_prediction_best\nplot_all_r2()","3e93fe93":"prediction_list['Decision Tree - Best'] = np.array(preds_svr_best[:20])\nprediction_list","7ddc4e21":"error_list['Decision Tree - Best'] = [tree_mse_best, tree_r2_best]\nerror_list.T","68597d85":"from sklearn.ensemble import RandomForestRegressor\nforest_model = RandomForestRegressor(random_state=42) #n_estimators is 100 by default\nforest_model.fit(X_train, y_train)\nforest_model_prediction = forest_model.predict(X_test)\n\nforest_mse = metrics.mean_squared_error(y_test, forest_model_prediction)\nforest_mae = metrics.mean_absolute_error(y_test, forest_model_prediction)\nforest_rmse = np.sqrt(forest_mse)\nforest_r2 = metrics.r2_score(y_test, forest_model_prediction)\n\nprint(\"Mean Squared Error: {}\".format(forest_mse))\nprint(\"Mean Absolute Error: {}\".format(forest_mae))\nprint(\"Root Mean Absolute Error: {}\".format(forest_rmse))\nprint(\"R2 score: {}\".format(forest_r2))","8e0b2c93":"prediction_dictionaries['Random Forest - Default'] = forest_model_prediction\nplot_all_r2()","35499f53":"prediction_list['Random Forest - Default'] = np.array(preds_svr[:20])\nprediction_list","de3090a2":"error_list['Random Forest - Default'] = [forest_mse, forest_r2]\nerror_list.T","bda5fe7c":"from sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import cross_val_score\n\n\nlinear_model = LinearRegression().fit(X_train, y_train)\nlinear_model_prediction = linear_model.predict(X_test)\n\nlinear_mse = metrics.mean_squared_error(y_test, linear_model_prediction)\nlinear_mae = metrics.mean_absolute_error(y_test, linear_model_prediction)\nlinear_rmse = np.sqrt(linear_mse)\nlinear_r2 = metrics.r2_score(y_test, linear_model_prediction)\n\nprint(\"Mean Squared Error: {}\".format(linear_mse))\nprint(\"Mean Absolute Error: {}\".format(linear_mae))\nprint(\"Root Mean Absolute Error: {}\".format(linear_rmse))\nprint(\"R2 score: {}\".format(linear_r2))","d66e2aa5":"prediction_dictionaries['Linear Regression - Default'] = linear_model_prediction\nplot_all_r2()","b6fb9a99":"prediction_list['Linear Regression - Default'] = np.array(preds_svr[:20])\nprediction_list","3d7691ff":"error_list['Linear Regression - Default'] = [linear_mse, linear_r2]\nerror_list.T","89b70ddb":"from sklearn.linear_model import Ridge\n\nalpha = [0.00001, 0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 250, 500, 750, 1000, 1500, 2500, 5000, 10000, 100000, 1000000]\nparam_grid = {\n    'alpha': alpha\n}\n\nridge = Ridge(alpha=1).fit(X_train, y_train)\nscores = cross_val_score(ridge, X_train, y_train, cv=5, scoring='r2')\nscores_mse = cross_val_score(ridge, X_train, y_train, cv=5, scoring='neg_mean_squared_error')\n\nprint(\"CV Mean for Ridge (r2): \", np.mean(scores))\nprint(\"CV Mean for Ridge (mse): \", np.mean(scores_mse) * -1)","39fe1551":"grid_mse = GridSearchCV(estimator=ridge, param_grid=param_grid, scoring='neg_mean_squared_error', verbose=1, n_jobs=-1)\ngrid_result_mse = grid_mse.fit(X_train, y_train)\n\ngrid_r2 = GridSearchCV(estimator=ridge, param_grid=param_grid, scoring='r2', verbose=1, n_jobs=-1)\ngrid_result_r2 = grid_r2.fit(X_train, y_train)","fbcf2a1f":"print('Best Score for mse: ', grid_mse.best_score_ * -1)\nprint('Best Params for mse: ', grid_mse.best_params_)\nprint()\nprint('Best Score for r2: ', grid_r2.best_score_)\nprint('Best Params for r2: ', grid_r2.best_params_)","69cd66a8":"ridge_best = Ridge(alpha=500).fit(X_train, y_train)\nridge_best.fit(X_train, y_train)\nridge_pred = ridge_best.predict(X_test)\n\nridge_mse_best = metrics.mean_squared_error(y_test, ridge_pred)\nridge_mae_best = metrics.mean_absolute_error(y_test, ridge_pred)\nridge_rmse_best = np.sqrt(ridge_mse_best)\nridge_r2_best = metrics.r2_score(y_test, ridge_pred)\n\nprint(\"Mean Squared Error: {}\".format(ridge_mse_best))\nprint(\"Mean Absolute Error: {}\".format(ridge_mae_best))\nprint(\"Root Mean Absolute Error: {}\".format(ridge_rmse_best))\nprint(\"R2 score: {}\".format(ridge_r2_best))","83d4b918":"dict_val = {\n    'Linear Model': [linear_r2, linear_mse],\n    'Ridge': [ridge_r2_best, ridge_mse_best]\n}\nres_df_linear_ridge = pd.DataFrame(dict_val, index=['R2', 'MSE'])\nres_df_linear_ridge","a5f968d5":"prediction_dictionaries['Ridge'] = ridge_pred","792305d0":"plot_all_r2()","dfc817f4":"prediction_list['Ridge'] = np.array(preds_svr[:20])\nprediction_list","b822fb1c":"error_list['Ridge'] = [ridge_mse_best, ridge_r2_best]\nerror_list.T","97f756b5":"#### LINEAR REGRESSION","94dccf46":"### Distribution of the Rooms in the Map\n\nCredits to:  [Erik Bruin](https:\/\/www.kaggle.com\/erikbruin\/airbnb-the-amsterdam-story-with-interactive-maps)","30528c9b":"Let's see some of the scores of the neural network to see if it performed better than the traditional machine learning models. ","7fc23250":"##### HYPERPARAMETER TUNING FOR KNN\n\nSince we have hyperparamaters such as number of nearest neighbour, or distance metric, we will apply hyperparamater tuning to see if we can decrease the error. Let's test it by:\n\n\n*   Number of Neighbours(1, .... ,10)\n*   LDistance Function (1: Euclidean, 2: Manhattan)\n\nCross Validations: \n\n*   And since we split the data by 80 training, 20 test data. We will do 100 \/ 20 = 5 cross validation splits.\n\n  \n\nWe will use GridSearchCV which searchs the model with all possible combinations with using cross validation. It calculates all the scores and finds the model which performed best \n\n","511e66a2":"### Correlation Graphics Between Each Column and the Price","e2fc7f62":"#### RESULTS & DISCUSSION","5b79e6b9":"Random Forest model gives us the best score in R2 as well as MSE. However, as we described in Random Forest section, running time of the Random Forest model is slightly more than other models. Neural Network performed similar to Random Forest and two of them performed best among the other models. \n\nIf we do not have time restriction, KNN or Neural Network can be used. Both give us noticeable performance and accuracy. If we had more data, neural network would perform better than the current performance","768d2860":"There are every kind of information in our dataset. There is 365 day, there is 0 day also. The data is right skewed. ","c47646d0":"## REFERENCES\n\n\n\n1.   https:\/\/www.airbnb.com\/help\/article\/2503\/what-is-airbnb-and-how-does-it-work\n\n2. https:\/\/automating-gis-processes.github.io\/site\/notebooks\/L3\/nearest-neighbor-faster.html\n\n3. https:\/\/data.ny.gov\/Transportation\/NYC-Transit-Subway-Entrance-And-Exit-Data\/i9wp-a4ja\/data\n\n4.   http:\/\/go.euromonitor.com\/rs\/805-KOK-719\/images\/wpTop100Cities19.pdf?mkt_tok=eyJpIjoiTVRsaU5Ua3lObVUwT1RVdyIsInQiOiJ2RWRQREV6S1wvN0U5N0R3aXI2MjZrRmhsRFpRc1RSNFZBUDljXC9wcFBVWmZcL2thaVRSYm5pOHJERVVxcXpTWVNHbUd1SjhHa0NqQ0g4bGk1a2JreDIzME5UbFFrSFpTblhoczVDcm91OXRSZmh4ZVUyc1dyaVlUeFppWHZNT1U5SiJ9\n\n5. https:\/\/www.kaggle.com\/erikbruin\/airbnb-the-amsterdam-story-with-interactive-maps\n\n6. https:\/\/public.opendatasoft.com\/explore\/dataset\/airbnb-listings\/table\/?disjunctive.host_verifications&disjunctive.amenities&disjunctive.features&refine.city=New+York\n\n7. https:\/\/towardsdatascience.com\/linear-regression-models-4a3d14b8d368\n\n8. https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.tree.DecisionTreeRegressor.html","77725bc4":"Our purpose is predict the price based on the given dataset. Since price is a continous variable we need to do a regression task. Therefore, we will use the Regression models of the traditional models such as KNN, SVM. \n\nWe are not after accuracy, because we are doing a regression task, not a classification task. Our main metric will be mean squared error and we will try to minimize it with some methods such as hyperparameter tuning. \n\nAlmost in every model we have sections such as\n\n\n\n\nBefore Hyperparameter Tuning( We are trying model with default \n\n\n\n*   Train model with the default parameters\n\n*   See the error and r2 score\n\n*   Compare with other models and results\n\nAfter Hyperparameter Tuning\n\n\n\n*   Determine the parameter grid\n*   Do a gridsearch to get best model parameters.\n* Compare with the default model and other models.\n\n\n\n","0fc95cc0":"### Distribution of the Price","64a4ff7f":"The error is 7370 which is slightly worse than KNN. Now let's see the points","2f25332c":"#### SUPPORT VECTOR MACHINE","acc4ce74":"### Distrbution of None Numerical Columns","1c037951":"What has been done in this section:\n\n\n1.   Trained in Various Models\n2.   In Each model, we tried to minimize the mean_squared_error with using hyperparameter tuning and cross validation.\n3. Showed results in each model.\n4. Model comparison, (which model performed best)\n\n","7a042376":"### Correlation Map Between Numerical Variables","989320db":"### Most Expensive and The Cheapest Neighbourhoods","715ddc5d":"It can be seen that there is a strong correlation in the reviews part. But there is no strong correlation between reviews scores and prices. Because most of the host's review scores are between 8 - 10. However, there is a strong correlation between (bedrooms, bathrooms and bed) and price. ","cf6c4816":"The error found in linear regression is 6704. Let's see that does it possible to decrease error by hyperparameter tuning for linear regression.","cc4525b6":"**If you want to test it, you can use the [website](https:\/\/airbnb-prediction-app.herokuapp.com\/) to see some predictions **","c912027f":"Reviews Scores are mostly around 8 - 10. Since the most of the houses review scores are high, it might not be a good feature for price prediction. But that is just an assumption. We will see if there is a correlation between review scores and the price after a few cells.","85070964":"###### BEFORE HYPERPARAMETER TUNING","eb0e0711":"Credits to: [Devakumar kp](https:\/\/www.kaggle.com\/imdevskp\/nyc-airbnb-open-data-eda)","3ff34ef9":"## MACHINE LEARNING","9d3064fb":"R2 score is 0.537 which means there is a correlation between the predictions and the actual values. The mean squared Error is 5661. ","b45bd173":"#### DECISION TREE\nDecision Tree can be used for classification and regression. Here, we use Decision Tree for regression.","31d0ccb3":"Manhattan has the most room. Mostly the room types in Manhattan is Entire Home which is an interesting part. Because in the other neighbourhoods, the private room number is higher than the entire home.","fa63febf":"**What's Done**\n\n\n1.   Handling the Missing Values\n2.   Dropping the Outliers\n\n","ce7670fb":"##### APPLY HYPERPARAMETER TUNING AND CROSS VALIDATION","00c049bc":"### Distribution of room types in the Map.","2fb4d941":"### MODEL TRAINING","d7019f65":"#### ENCODING","bec367f1":"### DATA PREPARATION","7774c5c4":"### Room Type Distribution","b868e991":"It is not an unusual condition. It makes sense that entire home is more expensive than other and it also makes sense that private room is more expensive than the shared room.","0ccb1615":"One of the reason **Manhattan is more expensive than others** might be the **room type**. We already know that **entire homes** are **more expensive** than others. Manhattan has the most entire home room type.","1eb66228":"As we can see from the graph, Neural network tries to reduce te error. Let' see some of the results","fc931463":"The group points became less in the best model of Decision Tree. And this is directly affected the performance as we can see correlation line from the graph. And the mean squared error is less in the best model because model performed better. More points collapse around 100-200 price scale. Since the data is mostly around 0-200, the performance is better than the default model of the Decision Tree.","b4877fe3":"IT can be seen that neural network is clearly better than the SVR. But, Neural network and the Knn are similar. The correlation line shows that there is a strong correlation between neural network predictions and the actual values. As more data comes the neural network will perform better. ","ee4c40fe":"### Neighbourhood Distribution","c309d68f":"### Distribution of Room Types in Each Neighbourhood","bc47571e":"When we plot the price in logarithmic case, the distribution likes the normal distribution. The skewed data turned to like a normal distribution. ","b41b1666":"Manhattan has the most house. Brooklyn follows Manhattan with a big portion. Mostly the rooms are distributed in Brooklyn and Manhattan","a95afe7c":"**It can be seen that in every neighbourhood price of Entire Home > Private Room > Shared Room.**","7f9f74a4":"### Distribution of Availability","75ab1e13":"It can be seen that **KNN performed better mostly** (we can see it also from MSE scores). **Hyperparameter tuning for SVR does perform better than the default model.** But there is not much difference. The reason might be that there is no much difference between models in terms of parameters and the difference is not enough to get a huge difference. ","f24d94e1":"As the graph shows there is a correlation between predictions and the actual values. It means model predictions consistent with the actual values. The other comment is that the line in KNN_best is better than the first model. The reason for that second model checks for more neighbours, therefore the result for the second model is more consistent.","3987e4ad":"It can be seen that mean of prices in **Manhattan** are higher than the rest. Let' see which room is the most expensive and which room type is the cheapest.","ffb787b2":"R2 score has significant increase from 0.29 to 0.53.","40ebb90d":"Let's determine train data and target column. After that scale the data.\nSplit the data as:\n\n\n*   Training: 80%\n*   Test : 20%","fa91c86b":"#### RANDOM FOREST\nRandom Forest is supervised machine learning algorithms which can be used for regression. ","eb1a58c4":"**Villa's, Boutique Hotel are the common property types which are expensive in every Neighbourhood Group.**","14417c5e":"WE Can see a improvement in the best model in both of the metrics.","76e70203":"Mostly houses are 2 accommodates","90ab8837":"As we can see above the neighbourhood_group, neighbourhood, room type, host_is_superhost, host_response time and host_response rate are all categorical columns. Let's encode them, se we can use them in training.","0a0deda4":"The error is 8509 for default Decision Tree Regressor. Although this error is less than Knn's result, it is still more than SVM's result.\n","8688c91e":"There is a relationships between **accommodates** and the **price.** It can be seen easily above 2 parts. It is natural to have that kind of correlation. Because, when the number of people that can stay in a place, the price will increase naturally. And the there is a correlation between **some of the review scores and the prices.** There is also a correlation between **bathrooms, bathrooms and bed, to price.** There is also a **negative correlation between distance to nearest station and the price.** When the distance increases, the price decreases. We already know that, the shorter distance is an important feature for the houses nowadays.","2b533c84":"LinearSVR is support vector machine for regression problems. Therefore we will use first default paramaters and we will apply hyperparameter tuning. In most regression cases LinearSVR perform faster and more accurate results. \n\nSource: [SkLearn- LinearSVR ](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.svm.LinearSVR.html)","9298011c":"#### K NEAREST NEIGHBOUR","38c2a055":"#### Evalution Metrics\n\n*MSE (mean squared error)*: $\\frac{1}{N}\\sum_i \\, (y_{true_i} - y_{pred_i})^2 \\,$\n\n\n*RMSE (root mean squared error)*: $\\sqrt{\\frac{1}{N}\\sum_i \\, (y_{true_i} - y_{pred_i})^2}$\n\n\n*MAE (mean absolute error)*: $\\frac{1}{N} \\sum_i \\, |y_{true_i} - y_{pred_i}| \\,$\n\nSince we are measuring the errors, we want to **minimize** these metrics.","6ee7cfde":"### Price Distribution in Each Room Type","3d78f44c":"### Distribution of the Accommodates","9f69622e":"We use different techniques for validation our linear regression model called *Ridge*.\n   \nSince we cannot apply hyperparameter tuning into Linear Regression directly, we will use Ridge in order to apply Hyperparameter tuning.","9c40b75e":"Decision Tree is the worst so far","210faf69":"There are mostly entire home and private room. There is also a small portion in shared rooms.","9deb402b":"### Distribution of the Rooms","26005fa6":"###### BEFORE HYPERPARAMETER TUNING","7a3e1f3a":"### Handling Missing Data","cdec9389":"#### NEURAL NETWORKS","5a2378b9":"##### APPLY HYPERPARAMETER TUNING AND CROSS VALIDATION","15cddc5a":"### Distribution of the Reviews","405d87fc":"Let's merge the datasets to use the additional information comes from the listings.csv","423bfda6":"## WHAT CAN BE DONE TO INCREASE ACCURACY\n\n\n\n*   There could be more data.\n*   Models can be trained in lots of parameters to see which one is best. Since it took so much time, we did not train in every possible of combination.","80e71a04":"Hyperparameter tuning takes too much time, approximately at least 40 minutes, to finish for random forest model. Therefore, we didn't apply Hyperparameter tuning.\n\nActually, default random forest gives the best solution among the models that we tried so far. However, it has some performance issues compared to other models.\n","50eafe6a":"## CONCLUSION\n\n\n\n\n1.   We trained the data in several models with hyperparameter tuning to see best parameters for each model.\n2.  In most of the models, they performed similar but neural network performed best.\n\n3. The errors is slightly better than the Kaggle Results. ","4a66461f":"##### BEFORE HYPERPARAMATER TUNING","13ce52b9":"**K NEAREST NEIGHBOUR** can be used for ***regression***. But there is a better model which is **KNeighborsRegressor** which is used for regression tasks. Therefore, we will try ***KNeighborsRegressor*** for our price prediction goal. ","0dcd8947":"There are so many outlier in the price. Let's drop them.","4e923e22":"It can be seen that prices are mostly around between 100 and 200 dollars.\nPrice data is right skewed. Let's plot the Price Distribution in Logarithm to see what happens","f90bd6cb":"We will not need name, host_id, host_name. Because those columns does not affect price.","bb9d3b9f":"1.   The **weekly price** and the **monthly price** columns are mostly missing. We can drop it. \n2.   We can fullfill the **reviews_per_month** just dividing **number reviews** by 12.\n\n3. Since there is not much standart deviation we can fullfill the **review_scores** with it's mean.\n\n4. We can fullfill the **response time** with the mean value\n\n5. The other object columns can be filled the most occurring element in that column.\n\n6. Badrooms, Bed, Bathroom can be replaced with it's mean","dc73f0b1":"The parameters for the Linear SVR:\n\n\n\n*   **loss:**  Specifies the loss function (Either L1 Loss or l2 loss)\n*   **C:** Regularization parameter. ('C': [0.1, 1, 10, 100, 1000])\n\n\n*   **dual:** dual or primal optimization problem. (Either True or False)\n\n\nWhat we are going to do:\n\n\n\n*   Use grid search to fit all those parameters\n*   In each iteration use 5 cross validation points","77250980":"### Handling Outliers\n","ddfd1e02":"After using Grid Search CV, we found that alpha=500 is the best parameter for Ridge. We can see the effect of Hyperparameter Tuning compared to default Ridge. However, we do not have significant between Ridge and Linear Regression model.","f2571f1e":"#### Dropping Unnecessary Columns","85f52fd7":"The mean squared error is slightly better than the default model. But there is not much difference. ","8357d41d":"The distribution builds up around 1. This means in an average home there is one room and 1 bathroom. ","e04d3199":"##### APPLY HYPERPARAMETER TUNING AND CROSS VALIDATION\n\nParameters that is used for Decision Tree Regressor:\n\n\n1.   **max_depth**: The maximum depth of the tree. If None, then nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples.\n2.   **min_samples_leaf**:The minimum number of samples required to be at a leaf node.\n3.   **min_samples_split**:The minimum number of samples required to split an internal node.\n\n[Reference to explanation of parameters](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.tree.DecisionTreeRegressor.html)\n\n\nWhat we are going to do:\n\n1.   Use grid search to fit all those parameters\n2.   In each iteration use 5 cross validation points\n","8b26d191":"## DATA EXPLORATION\n\nIn this section what we will do:\n\n*   Visualization of Some Columns\n*   Understand if There is a Correlation Between tthe Variables\n*   Understand the behaviour the data\n","859c819e":"The numbers are slightly worse than the KNN with best performance. Numbers are not bad, but let's see if we can improve the performance with hyperparameter tuning. ","5aee07e4":"## Data Handling","f0122f71":"R2 score is almost 0.6 which shows a correlation between predictions and the actual values. Also the mean squared error is slightly less than the previous models which is a good thing because we want the error as minimum as possible. ","5ad71c87":"The points are more spreaded in the Random Forest. Since the Random Forest runs lots of Decision Tree's and get the best tree as the classifier, it is very natural to have a better performance than the Decision Tree. The correlation line performed better than the rest. Since the points are more spreaded and in correct places the mean squared error is less than the rest. Random Forest performed best for now. But let's see other models if we can improve.","100fc1ee":"Neural Network is the best so far"}}