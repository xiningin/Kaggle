{"cell_type":{"75a06904":"code","f51c2a91":"code","10283440":"code","a7c37da0":"code","85963fa8":"code","cc2a187c":"code","3a7a7652":"code","81422e66":"code","a3834553":"code","f7f9deeb":"code","d6f909f9":"code","168339e5":"code","19d665ef":"code","ba02140c":"code","4e86ffa0":"code","6de90203":"code","8cea9675":"code","20568678":"code","266b55d3":"code","31db5ba8":"code","1108e8ff":"code","e47de3df":"code","fb3416e8":"code","c6855b77":"code","ac7e30ec":"code","1372ee45":"code","51100bea":"code","c2962b11":"code","ff49d6de":"code","d7ed5493":"code","b8885957":"code","57d3ee9e":"code","da23d109":"code","e2d8430c":"code","9b8dfd26":"code","2bf0bf19":"code","de35b5cb":"code","f4eadde6":"code","42d6bc48":"code","aff0e4af":"code","8041a4a8":"code","9cba9c35":"markdown","2c5cbfbd":"markdown","c6d7d2af":"markdown","4e716893":"markdown","183bbbdc":"markdown","834c61eb":"markdown","6071d005":"markdown","21aa88c9":"markdown","0950bdc3":"markdown","d78fdf3d":"markdown","4f4298b2":"markdown","3dfe4f59":"markdown","a6a9bd0d":"markdown","dad733a1":"markdown","a52ee9c2":"markdown","e53043e3":"markdown","e2f87fa4":"markdown","5080c976":"markdown","0cdbe40d":"markdown","082a82b8":"markdown","7a5f25c2":"markdown","a3c78e96":"markdown","f7a50877":"markdown","790fd131":"markdown","8c6e5e4b":"markdown","36246d31":"markdown","c8d9f8b0":"markdown","4e85d911":"markdown","5130eb92":"markdown","591e3f3e":"markdown","99526bb0":"markdown","6364fe89":"markdown","e4c9b3e2":"markdown","cb04cf1f":"markdown","5f619b20":"markdown","7ab50382":"markdown","4e628de1":"markdown","050a2117":"markdown","3eae688f":"markdown","62b243c3":"markdown","cdc3a13b":"markdown","68306aed":"markdown","4247864b":"markdown","53c49d48":"markdown","2ea12e3e":"markdown","762ae299":"markdown","c28eb224":"markdown","28265f0b":"markdown","4ebfe7f1":"markdown","27732e0c":"markdown","3efec876":"markdown","bc0b545c":"markdown","d916993f":"markdown","73925095":"markdown","67abf7ff":"markdown","2dad95e3":"markdown","70c7bc82":"markdown","51f39e82":"markdown","e0e0e098":"markdown","3df33bcc":"markdown","ee9f83d0":"markdown","4f142792":"markdown","8e7b818f":"markdown"},"source":{"75a06904":"#install required libraries\nimport pandas as pd\nimport numpy as np\n\n#data visualization packages\nimport matplotlib.pyplot as plt\n\n#keras packages\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Convolution2D\nfrom keras.layers import MaxPooling2D\nfrom keras.layers import Flatten\nfrom keras.layers import Dense\nfrom keras.wrappers.scikit_learn import KerasClassifier\nfrom keras.layers import Dropout\n\n#model evaluation packages\nfrom sklearn.metrics import f1_score, roc_auc_score, log_loss\nfrom sklearn.model_selection import cross_val_score, cross_validate\n\n#other packages\nimport time as time\nfrom IPython.display import display, Markdown\nfrom IPython.display import display\nfrom time import sleep\nfrom IPython.display import Markdown as md","f51c2a91":"#read mnist fashion dataset\nmnist = keras.datasets.fashion_mnist\n(X_train, y_train), (X_test, y_test) = mnist.load_data()\nprint(X_train.shape, y_train.shape, X_test.shape, y_test.shape)","10283440":"#reshape data from 3-D to 2-D array\nX_train = X_train.reshape(60000, 784)\nX_test = X_test.reshape(10000, 784)","a7c37da0":"#feature scaling\nfrom sklearn.preprocessing import MinMaxScaler\nminmax = MinMaxScaler()\n\n#fit and transform training dataset\nX_train = minmax.fit_transform(X_train)\n\n#transform testing dataset\nX_test = minmax.transform(X_test)","85963fa8":"print('Number of unique classes: ', len(np.unique(y_train)))\nprint('Classes: ', np.unique(y_train))","cc2a187c":"fig, axes = plt.subplots(nrows=2, ncols=5,figsize=(15,5))          #create subplot\nax = axes.ravel()\nfor i in range(10):\n    ax[i].imshow(X_train[i].reshape(28,28))                        #print image\n    ax[i].title.set_text('Class: ' + str(y_train[i]))              #print class\nplt.subplots_adjust(hspace=0.5)                                    #increase horizontal space\nplt.show()                                                         #display image","3a7a7652":"#initializing CNN model\nclassifier_e25 = Sequential()\n\n#add 1st hidden layer\nclassifier_e25.add(Dense(input_dim = X_train.shape[1], units = 256, kernel_initializer='uniform', activation='relu'))\n\n#add output layer\nclassifier_e25.add(Dense(units = 10, kernel_initializer='uniform', activation='softmax'))","81422e66":"#compile the neural network\nclassifier_e25.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n\n#model summary\ndisplay(Markdown('<br>**\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*  Model  Summary  \\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\***<br>'))\nclassifier_e25.summary()","a3834553":"#include time details\ndh = display('',display_id=True)\ndh.update(md(\"<br>Training is in progress.....\"))\nt1 = time.time()\n\n#fit training dataset into the model\nclassifier_e25_fit = classifier_e25.fit(X_train, y_train, epochs=25, verbose=0)\ntt = time.time()-t1\ndh.update(md(\"<br>Training is completed! Total training time: **{} seconds**\".format(round(tt,3))))\n\ndisplay(Markdown('<br>**\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*     Training Summary    \\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\***<br>'))\n\n#plot the graphs\n#accuracy graph\nfig, axes = plt.subplots(nrows=1, ncols=2,figsize=(15,5))\nax = axes.ravel()\nax[0].plot(range(0,classifier_e25_fit.params['epochs']), [acc * 100 for acc in classifier_e25_fit.history['accuracy']], label='Accuracy')\nax[0].set_title('Accuracy vs. epoch', fontsize=15)\nax[0].set_ylabel('Accuracy', fontsize=15)\nax[0].set_xlabel('epoch', fontsize=15)\nax[0].legend()\n\n#losso graph\nax[1].plot(range(0,classifier_e25_fit.params['epochs']), classifier_e25_fit.history['loss'], label='Loss', color='r')\nax[1].set_title('Loss vs. epoch', fontsize=15)\nax[1].set_ylabel('Loss', fontsize=15)\nax[1].set_xlabel('epoch', fontsize=15)\nax[1].legend()\n\n#display the graph\nplt.show()","f7f9deeb":"#include timing information\ndh = display('',display_id=True)\ndh.update(md(\"<br>Model evaluation is in progress...\"))\nt2 = time.time()\n\n#evaluate the model for testing dataset\ntest_loss_e25 = classifier_e25.evaluate(X_test, y_test, verbose=0)\net = time.time()-t2\ndh.update(md(\"<br>Model evaluation is completed! Total evaluation time: **{} seconds**\".format(round(et,3))))\n\ndisplay(Markdown('<br>**\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\* Model Evaluation Summary \\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\***'))\n\n#calculate evaluation parameters\nf1_e25 = f1_score(y_test, classifier_e25.predict_classes(X_test), average='micro')\nroc_e25 = roc_auc_score(y_test, classifier_e25.predict_proba(X_test), multi_class='ovo')\n\n#create evaluation dataframe\nstats_e25 = pd.DataFrame({'Test accuracy' : round(test_loss_e25[1]*100,3),\n                      'F1 score'      : round(f1_e25,3),\n                      'ROC AUC score' : round(roc_e25,3),\n                      'Total Loss'    : round(test_loss_e25[0],3)}, index=[0])\n\n#print evaluation dataframe\ndisplay(stats_e25)","d6f909f9":"def model_cv(epoch, cv):\n    '''Function for cross validation'''\n    \n    #Model Initializing, Compiling and Fitting\n    def build_classifier():\n        classifier = Sequential()\n        classifier.add(Dense(input_dim = X_train.shape[1], units = 256, kernel_initializer='uniform', activation='relu'))\n        classifier.add(Dense(units = 10, kernel_initializer='uniform', activation='softmax'))\n        classifier.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n        return classifier\n\n    #model summary\n    display(Markdown('<br>**\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*  Model  Summary  \\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\***<br>'))\n    build_classifier().summary()\n\n    #create KerasClassifier object\n    classifier_cv = KerasClassifier(build_fn=build_classifier, batch_size=32, epochs=epoch, verbose=0)\n    scoring = {'acc' : 'accuracy',\n                    'f1'  : 'f1_micro',\n                    'roc' : 'roc_auc_ovo',\n                    'loss': 'neg_log_loss'}\n\n    #include timing information\n    dh = display('',display_id=True)\n    dh.update(md(\"<br>Training is in progress.....\"))\n    t1 = time.time()\n    \n    #perform cross validation\n    scores = cross_validate(classifier_cv, X_train, y_train, cv=cv, scoring=scoring, verbose=0, return_train_score=True)\n    tt = time.time()-t1\n    dh.update(md(\"<br>Training is completed! Total training time: **{} seconds**\".format(round(tt,3))))\n    display(Markdown('<br>**\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*     Training Summary    \\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\***<br>'))\n\n    #plot graphs\n    #accuracy graph\n    fig, axes = plt.subplots(nrows=1, ncols=2,figsize=(15,5))\n    ax = axes.ravel()\n    ax[0].plot(range(1,len(scores['train_acc'])+1), [acc * 100 for acc in scores['train_acc']], label='Accuracy')\n    ax[0].set_title('Accuracy vs. Cross Validation', fontsize=15)\n    ax[0].set_ylabel('Accuracy', fontsize=15)\n    ax[0].set_xlabel('Cross Validation', fontsize=15)\n    ax[0].legend()\n\n    #loss graph\n    ax[1].plot(range(1,len(scores['train_loss'])+1), np.abs(scores['train_loss']), label='Loss', color='r')\n    ax[1].set_title('Loss vs. Cross Validation', fontsize=15)\n    ax[1].set_ylabel('Loss', fontsize=15)\n    ax[1].set_xlabel('Cross Validation', fontsize=15)\n    ax[1].legend()\n\n    #display the graph\n    plt.show()\n\n\n    #Evaluating the model\n    dh = display('',display_id=True)\n    dh.update(md(\"<br><br>Model evaluation is completed! Total evaluation time: **{} seconds**\".format(round(np.sum(scores['score_time']),3))))\n\n    display(Markdown('<br>**\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\* Model Evaluation \\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\***'))\n    \n    #create the model evaluation dataframe\n    stats = pd.DataFrame({'Test accuracy' : round(np.mean(scores['test_acc'])*100,3),\n                          'F1 score'      : round(np.mean(scores['test_f1']),3), \n                          'ROC AUC score' : round(np.mean(scores['test_roc']),3),\n                          'Total Loss'    : round(np.abs(np.mean(scores['test_loss'])),3)}, index=[0])\n    #print the dataframe\n    display(stats)\n\n    #return the classifier and evaluation parameter details\n    return scores, stats","168339e5":"#run the model for 5-Fold cross validation\nscores_5cv, stats_5cv = model_cv(epoch=25, cv=5)","19d665ef":"#run the model for 10-Fold cross validation\nscores_10cv, stats_10cv = model_cv(epoch=25, cv=10)","ba02140c":"def add_value_labels(ax, spacing=5):\n    '''add label details on each bar graph'''\n    \n    for rect in ax.patches:\n        # Get X and Y placement of label from rect.\n        y_value = rect.get_height()\n        x_value = rect.get_x() + rect.get_width() \/ 2\n\n        # Number of points between bar and label. Change to your liking.\n        space = spacing\n        # Vertical alignment for positive values\n        va = 'bottom'\n\n        # If value of bar is negative: Place label below bar\n        if y_value < 0:\n            # Invert space to place label below\n            space *= -1\n            # Vertically align label at top\n            va = 'top'\n\n        # Use Y value as label and format number with one decimal place\n        label = \"{:.1f}\".format(y_value)\n\n        # Create annotation\n        ax.annotate(\n            label,                      # Use `label` as label\n            (x_value, y_value),         # Place label at end of the bar\n            xytext=(0, space),          # Vertically shift label by `space`\n            textcoords=\"offset points\", # Interpret `xytext` as offset in points\n            ha='center',                # Horizontally center label\n            va=va)                      # Vertically align label differently for\n                                        # positive and negative values.","4e86ffa0":"#Plot the graph\nx_axis = ['cv=1', 'cv=5', 'cv=10']\ny_axis = [classifier_e25_fit.history['accuracy'][-1]*100, np.mean(scores_5cv['train_acc']*100), np.mean(scores_10cv['train_acc']*100)]\n\n#create series with y_axis values\nfreq_series = pd.Series(y_axis)\n\ndisplay(Markdown('<br>**\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*    Graph Plot    \\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\***<br><br>'))\nplt.figure(figsize=(12,6))                                    #figure size\nax = freq_series.plot(kind='bar')                             #plot the type of graph\nplt.xlabel('Number of Cross Validation', fontsize=15)         #xlabel\nplt.ylabel('Training Accuracy', fontsize=15)                  #ylabel\nplt.ylim(min(y_axis)-1,max(y_axis)+1)                         #limit the y_axis dynamically\nplt.title('Bar graph for Number of Cross Validation vs. Training Accuracy', fontsize=15)    #title\nax.set_xticklabels(x_axis)                                    #x-ticks\n\n# Put labels on each bar graph\nadd_value_labels(ax)  ","6de90203":"#Plot the graph\nx_axis = ['cv=1', 'cv=5', 'cv=10']\ny_axis = [stats_e25['Test accuracy'][0], stats_5cv['Test accuracy'][0], stats_10cv['Test accuracy'][0]]\n\n#create series with y_axis values\nfreq_series = pd.Series(y_axis)\n\ndisplay(Markdown('<br>**\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*    Graph Plot    \\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\***<br><br>'))\nplt.figure(figsize=(12,6))\nax = freq_series.plot(kind='bar')\nplt.xlabel('Number of Cross Validation', fontsize=15)\nplt.ylabel('Test Accuracy', fontsize=15)\nplt.ylim(min(y_axis)-1,max(y_axis)+1)\nplt.title('Bar graph for Number of Cross Validation vs. Test Accuracy', fontsize=15)\nax.set_xticklabels(x_axis)\n\n# Put labels on each bar graph\nadd_value_labels(ax)","8cea9675":"def model_epcoh(epoch):\n    '''Function to run Neural Network for different epochs'''\n    \n    #Model Initializing, Compiling and Fitting\n    classifier = Sequential()\n    classifier.add(Dense(input_dim = X_train.shape[1], units = 256, kernel_initializer='uniform', activation='relu'))\n    classifier.add(Dense(units = 10, kernel_initializer='uniform', activation='softmax'))\n    classifier.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n    display(Markdown('<br>**\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*  Model  Summary  \\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\***<br>'))\n    classifier.summary()\n\n    #include timing details\n    dh = display('',display_id=True)\n    dh.update(md(\"<br>Training is in progress.....\"))\n    t1 = time.time()\n    #fit the model with training dataset\n    classifier_fit = classifier.fit(X_train, y_train, epochs=epoch, verbose=0)\n    tt = time.time()-t1\n    dh.update(md(\"<br>Training is completed! Total training time: **{} seconds**\".format(round(tt,3))))\n\n    display(Markdown('<br>**\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*     Training Summary    \\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\***<br>'))\n\n    #plot the graph\n    #accuracy graph\n    fig, axes = plt.subplots(nrows=1, ncols=2,figsize=(15,5))\n    ax = axes.ravel()\n    ax[0].plot(range(0,classifier_fit.params['epochs']), [acc * 100 for acc in classifier_fit.history['accuracy']], label='Accuracy')\n    ax[0].set_title('Accuracy vs. epoch', fontsize=15)\n    ax[0].set_ylabel('Accuracy', fontsize=15)\n    ax[0].set_xlabel('epoch', fontsize=15)\n    ax[0].legend()\n\n    #loss graph\n    ax[1].plot(range(0,classifier_fit.params['epochs']), classifier_fit.history['loss'], label='Loss', color='r')\n    ax[1].set_title('Loss vs. epoch', fontsize=15)\n    ax[1].set_ylabel('Loss', fontsize=15)\n    ax[1].set_xlabel('epoch', fontsize=15)\n    ax[1].legend()\n\n    #display the graph\n    plt.show()\n\n    #Evaluating the model    \n    dh = display('',display_id=True)\n    dh.update(md(\"<br>Model evaluation is in progress...\"))\n    t2 = time.time()\n    \n    #model evaluation\n    test_loss = classifier.evaluate(X_test, y_test, verbose=0)\n    et = time.time()-t2\n    dh.update(md(\"<br>Model evaluation is completed! Total evaluation time: **{} seconds**\".format(round(et,3))))\n    display(Markdown('<br>**\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*    Model Evaluation Summary    \\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\***'))\n\n    #calculate the evaluation parameter\n    f1 = f1_score(y_test, classifier.predict_classes(X_test), average='micro')\n    roc = roc_auc_score(y_test, classifier.predict_proba(X_test), multi_class='ovo')\n\n    #create the model evaluation dataframe\n    stats = pd.DataFrame({'Test accuracy' : round(test_loss[1]*100,3),\n                          'F1 score'      : round(f1,3),\n                          'ROC AUC score' : round(roc,3),\n                          'Total Loss'    : round(test_loss[0],3)}, index=[0])\n    \n    #print the dataframe\n    display(stats)\n    \n    #return the classifier and model evaluation details\n    return classifier_fit, stats","20568678":"#run the model for 50 epochs\nclassifier_e50, stats_e50 = model_epcoh(50)","266b55d3":"#run the model for 100 epochs\nclassifier_e100, stats_e100 = model_epcoh(100)","31db5ba8":"#run the model for 200 epochs\nclassifier_e200, stats_e200 = model_epcoh(200)","1108e8ff":"#Plot the graph\nx_axis = ['epoch=25', 'epoch=50', 'epoch=100', 'epoch=200']\ny_axis = [classifier_e25_fit.history['accuracy'][-1]*100, classifier_e50.history['accuracy'][-1]*100, classifier_e100.history['accuracy'][-1]*100, classifier_e200.history['accuracy'][-1]*100]\n\n#create series with y_axis values\nfreq_series = pd.Series(y_axis)\n\n#plot the graph\ndisplay(Markdown('<br>**\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*    Graph Plot    \\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\***<br><br>'))\nplt.figure(figsize=(12,6))\nax = freq_series.plot(kind='bar')\nplt.xlabel('Number of Epochs', fontsize=15)\nplt.ylabel('Training Accuracy', fontsize=15)\nplt.ylim(min(y_axis)-1,max(y_axis)+1)\nplt.title('Bar graph for Number of Epochs vs. Training Accuracy', fontsize=15)\nax.set_xticklabels(x_axis)\n\n# add labels for each bar graph\nadd_value_labels(ax)","e47de3df":"#Plot the graph\nx_axis = ['epoch=25', 'epoch=50', 'epoch=100', 'epoch=200']\ny_axis = [stats_e25['Test accuracy'][0], stats_e50['Test accuracy'][0], stats_e100['Test accuracy'][0], stats_e200['Test accuracy'][0]]\n\n#create series with y_axis values\nfreq_series = pd.Series(y_axis)\n\n#plot the graph\ndisplay(Markdown('<br>**\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*    Graph Plot    \\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\***<br><br>'))\nplt.figure(figsize=(12,6))\nax = freq_series.plot(kind='bar')\nplt.xlabel('Number of Epochs', fontsize=15)\nplt.ylabel('Test Accuracy', fontsize=15)\nplt.ylim(min(y_axis)-1,max(y_axis)+1)\nplt.title('Bar graph for Number of Epochs vs. Test Accuracy', fontsize=15)\nax.set_xticklabels(x_axis)\n\n#add labels for each bar graph\nadd_value_labels(ax)","fb3416e8":"#Model Initializing, Compiling and Fitting\nclassifier_2dl = Sequential()\nclassifier_2dl.add(Dense(input_dim = X_train.shape[1], units = 256, kernel_initializer='uniform', activation='relu'))\nclassifier_2dl.add(Dense(units = 128, kernel_initializer='uniform', activation='relu'))\nclassifier_2dl.add(Dense(units = 10, kernel_initializer='uniform', activation='softmax'))\nclassifier_2dl.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\ndisplay(Markdown('<br>**\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*  Model  Summary  \\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\***<br>'))\nclassifier_2dl.summary()\n\n#include timing details\ndh = display('',display_id=True)\ndh.update(md(\"<br>Training is in progress.....\"))\nt1 = time.time()\n\n#fit the model with training dataset\nclassifier_2dl_fit = classifier_2dl.fit(X_train, y_train, epochs=50, verbose=0)  #batch_size=32 (default)\ntt = time.time()-t1\ndh.update(md(\"<br>Training is completed! Total training time: **{} seconds**\".format(round(tt,3))))\ndisplay(Markdown('<br>**\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*     Training Summary    \\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\***<br>'))\n\n#plot the graph\n#accuracy graph\nfig, axes = plt.subplots(nrows=1, ncols=2,figsize=(15,5))\nax = axes.ravel()\nax[0].plot(range(0,classifier_2dl_fit.params['epochs']), [acc * 100 for acc in classifier_2dl_fit.history['accuracy']], label='Accuracy')\nax[0].set_title('Accuracy vs. epoch', fontsize=15)\nax[0].set_ylabel('Accuracy', fontsize=15)\nax[0].set_xlabel('epoch', fontsize=15)\nax[0].legend()\n\n#loss graph\nax[1].plot(range(0,classifier_2dl_fit.params['epochs']), classifier_2dl_fit.history['loss'], label='Loss', color='r')\nax[1].set_title('Loss vs. epoch', fontsize=15)\nax[1].set_ylabel('Loss', fontsize=15)\nax[1].set_xlabel('epoch', fontsize=15)\nax[1].legend()\n\n#display the graph\nplt.show()\n\n#Evaluating the model\ndh = display('',display_id=True)\ndh.update(md(\"<br>Model evaluation is in progress...\"))\nt2 = time.time()\n\n#model evaluation\ntest_loss_2dl = classifier_2dl.evaluate(X_test, y_test, verbose=0)\net = time.time()-t2\ndh.update(md(\"<br>Model evaluation is completed! Total evaluation time: **{} seconds**\".format(round(et,3))))\ndisplay(Markdown('<br>**\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*    Model Evaluation Summary    \\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\***'))\n\n#calculate the model evaluation parameter\nf1_2dl = f1_score(y_test, classifier_2dl.predict_classes(X_test), average='micro')\nroc_2dl = roc_auc_score(y_test, classifier_2dl.predict_proba(X_test), multi_class='ovo')\n\n#create the model evaluation dataframe\nstats_2dl = pd.DataFrame({'Test accuracy' : round(test_loss_2dl[1]*100,3),\n                      'F1 score'      : round(f1_2dl,3),\n                      'ROC AUC score' : round(roc_2dl,3),\n                      'Total Loss'    : round(test_loss_2dl[0],3)}, index=[0])\n\n#print the dataframe\ndisplay(stats_2dl)","c6855b77":"#Model Initializing, Compiling and Fitting\nclassifier_3dl = Sequential()\nclassifier_3dl.add(Dense(input_dim = X_train.shape[1], units = 256, kernel_initializer='uniform', activation='relu'))\nclassifier_3dl.add(Dense(units = 128, kernel_initializer='uniform', activation='relu'))\nclassifier_3dl.add(Dense(units = 256, kernel_initializer='uniform', activation='relu'))\nclassifier_3dl.add(Dense(units = 10, kernel_initializer='uniform', activation='softmax'))\nclassifier_3dl.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\ndisplay(Markdown('<br>**\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*  Model  Summary  \\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\***<br>'))\nclassifier_3dl.summary()\n\n#include timing details\ndh = display('',display_id=True)\ndh.update(md(\"<br>Training is in progress.....\"))\nt1 = time.time()\n\n#fit the model with training dataset\nclassifier_3dl_fit = classifier_3dl.fit(X_train, y_train, epochs=50, verbose=0)  #batch_size=32 (default)\ntt = time.time()-t1\ndh.update(md(\"<br>Training is completed! Total training time: **{} seconds**\".format(round(tt,3))))\ndisplay(Markdown('<br>**\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*     Training Summary    \\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\***<br>'))\n\n#plot the graph\n#accuracy graph\nfig, axes = plt.subplots(nrows=1, ncols=2,figsize=(15,5))\nax = axes.ravel()\nax[0].plot(range(0,classifier_3dl_fit.params['epochs']), [acc * 100 for acc in classifier_3dl_fit.history['accuracy']], label='Accuracy')\nax[0].set_title('Accuracy vs. epoch', fontsize=15)\nax[0].set_ylabel('Accuracy', fontsize=15)\nax[0].set_xlabel('epoch', fontsize=15)\nax[0].legend()\n\n#loss graph\nax[1].plot(range(0,classifier_3dl_fit.params['epochs']), classifier_3dl_fit.history['loss'], label='Loss', color='r')\nax[1].set_title('Loss vs. epoch', fontsize=15)\nax[1].set_ylabel('Loss', fontsize=15)\nax[1].set_xlabel('epoch', fontsize=15)\nax[1].legend()\n\n#display the graph\nplt.show()\n\n#Evaluate the model\ndh = display('',display_id=True)\ndh.update(md(\"<br>Model evaluation is in progress...\"))\nt2 = time.time()\n\n#model evaluation\ntest_loss_3dl = classifier_3dl.evaluate(X_test, y_test, verbose=0)\net = time.time()-t2\ndh.update(md(\"<br>Model evaluation is completed! Total evaluation time: **{} seconds**\".format(round(et,3))))\ndisplay(Markdown('<br>**\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*    Model Evaluation Summary    \\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\***'))\n\n#calculate the model evaluation parameter\nf1_3dl = f1_score(y_test, classifier_3dl.predict_classes(X_test), average='micro')\nroc_3dl = roc_auc_score(y_test, classifier_3dl.predict_proba(X_test), multi_class='ovo')\n\n#create the model evaluation dataframe\nstats_3dl = pd.DataFrame({'Test accuracy' : round(test_loss_3dl[1]*100,3),\n                      'F1 score'      : round(f1_3dl,3),\n                      'ROC AUC score' : round(roc_3dl,3),\n                      'Total Loss'    : round(test_loss_3dl[0],3)}, index=[0])\n\n#print the dataframe\ndisplay(stats_3dl)","ac7e30ec":"#Plot the graph\nx_axis = ['1', '2', '3']\ny_axis = [classifier_e50.history['accuracy'][-1]*100, classifier_2dl_fit.history['accuracy'][-1]*100, classifier_3dl_fit.history['accuracy'][-1]*100]\n\n#create series with y_axis values\nfreq_series = pd.Series(y_axis)\n\n#plot the graph\ndisplay(Markdown('<br>**\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*    Graph Plot    \\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\***<br><br>'))\nplt.figure(figsize=(12,6))\nax = freq_series.plot(kind='bar')\nplt.xlabel('Number of Dense Layer', fontsize=15)\nplt.ylabel('Training Accuracy', fontsize=15)\nplt.ylim(min(y_axis)-1,max(y_axis)+1)\nplt.title('Bar graph for Number of Dense Layer vs. Training Accuracy', fontsize=15)\nax.set_xticklabels(x_axis)\n\n# add labels for each bar graph\nadd_value_labels(ax)","1372ee45":"#Plot the graph\nx_axis = ['1', '2', '3']\ny_axis = [stats_e50['Test accuracy'][0], stats_2dl['Test accuracy'][0], stats_3dl['Test accuracy'][0]]\n\n#create series with y_axis values\nfreq_series = pd.Series(y_axis)\n\n#plot the graph\ndisplay(Markdown('<br>**\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*    Graph Plot    \\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\***<br><br>'))\nplt.figure(figsize=(12,6))\nax = freq_series.plot(kind='bar')\nplt.xlabel('Number of Dense Layer', fontsize=15)\nplt.ylabel('Test Accuracy', fontsize=15)\nplt.ylim(min(y_axis)-1,max(y_axis)+1)\nplt.title('Bar graph for Number of Dense Layer vs. Test Accuracy', fontsize=15)\nax.set_xticklabels(x_axis)\n\n# add labels for each bar graph\nadd_value_labels(ax)","51100bea":"def model_dropout(rate):\n    '''Neural Network Model with Dropout'''\n    \n    #Model Initializing, Compiling and Fitting\n    classifier = Sequential()\n    classifier.add(Dense(input_dim = X_train.shape[1], units = 256, kernel_initializer='uniform', activation='relu'))\n    classifier.add(Dropout(rate = rate))\n    classifier.add(Dense(units = 128, kernel_initializer='uniform', activation='relu'))\n    classifier.add(Dropout(rate = rate))\n    classifier.add(Dense(units = 10, kernel_initializer='uniform', activation='softmax'))\n    classifier.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n\n    #model summary\n    display(Markdown('<br>**\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*  Model  Summary  \\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\***<br>'))\n    classifier.summary()\n\n    #include timing details\n    dh = display('',display_id=True)\n    dh.update(md(\"<br>Training is in progress.....\"))\n    t1 = time.time()\n    \n    #fit the model with training dataset\n    classifier_fit = classifier.fit(X_train, y_train, epochs=50, verbose=0)\n    tt = time.time()-t1\n    dh.update(md(\"<br>Training is completed! Total training time: **{} seconds**\".format(round(tt,3))))\n    display(Markdown('<br>**\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*     Training Summary    \\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\***<br>'))\n\n    #plot the graph\n    #accuracy graph\n    fig, axes = plt.subplots(nrows=1, ncols=2,figsize=(15,5))\n    ax = axes.ravel()\n    ax[0].plot(range(0,classifier_fit.params['epochs']), [acc * 100 for acc in classifier_fit.history['accuracy']], label='Accuracy')\n    ax[0].set_title('Accuracy vs. epoch', fontsize=15)\n    ax[0].set_ylabel('Accuracy', fontsize=15)\n    ax[0].set_xlabel('epoch', fontsize=15)\n    ax[0].legend()\n\n    #loss graph\n    ax[1].plot(range(0,classifier_fit.params['epochs']), classifier_fit.history['loss'], label='Loss', color='r')\n    ax[1].set_title('Loss vs. epoch', fontsize=15)\n    ax[1].set_ylabel('Loss', fontsize=15)\n    ax[1].set_xlabel('epoch', fontsize=15)\n    ax[1].legend()\n\n    #display the graph\n    plt.show()\n\n    #Evaluae the model\n    dh = display('',display_id=True)\n    dh.update(md(\"<br>Model evaluation is in progress...\"))\n    t2 = time.time()\n    \n    #model evaluation\n    test_loss = classifier.evaluate(X_test, y_test, verbose=0)\n    et = time.time()-t2\n    dh.update(md(\"<br>Model evaluation is completed! Total evaluation time: **{} seconds**\".format(round(et,3))))\n    display(Markdown('<br>**\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*    Model Evaluation Summary    \\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\***'))\n\n    #calculate the model evaluation parameters\n    f1 = f1_score(y_test, classifier.predict_classes(X_test), average='micro')\n    roc = roc_auc_score(y_test, classifier.predict_proba(X_test), multi_class='ovo')\n\n    #create model evaluation dataframe\n    stats = pd.DataFrame({'Test accuracy' : round(test_loss[1]*100,3),\n                          'F1 score'      : round(f1,3),\n                          'ROC AUC score' : round(roc,3),\n                          'Total Loss'    : round(test_loss[0],3)}, index=[0])\n    \n    #print the dataframe\n    display(stats)\n    \n    #return the classifier and model evaluation details\n    return classifier_fit, stats","c2962b11":"#run the neural network model with dropout with rate=0.1\nclassifier_1d, stats_1d = model_dropout(0.1)","ff49d6de":"#run the neural network model with dropout with rate=0.2\nclassifier_2d, stats_2d = model_dropout(0.2)","d7ed5493":"#run the neural network model with dropout with rate=0.3\nclassifier_3d, stats_3d = model_dropout(0.3)","b8885957":"#Plot the model\nx_axis = ['rate=0.0', 'rate=0.1', 'rate=0.2', 'rate=0.3']\ny_axis = [classifier_e50.history['accuracy'][-1]*100, classifier_1d.history['accuracy'][-1]*100, classifier_2d.history['accuracy'][-1]*100, classifier_3d.history['accuracy'][-1]*100]\n\n#create series with y_axis values\nfreq_series = pd.Series(y_axis)\n\n#plot the graph\ndisplay(Markdown('<br>**\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*    Graph Plot    \\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\***<br><br>'))\nplt.figure(figsize=(12,6))\nax = freq_series.plot(kind='bar')\nplt.xlabel('Dropout Rates', fontsize=15)\nplt.ylabel('Training Accuracy', fontsize=15)\nplt.ylim(min(y_axis)-1,max(y_axis)+1)\nplt.title('Bar graph for Dropout Rates vs. Training Accuracy', fontsize=15)\nax.set_xticklabels(x_axis)\n\n#add label for each graph\nadd_value_labels(ax)","57d3ee9e":"#Plotting\nx_axis = ['rate=0.0', 'rate=0.1', 'rate=0.2', 'rate=0.3']\ny_axis = [stats_e50['Test accuracy'][0], stats_1d['Test accuracy'][0], stats_2d['Test accuracy'][0], stats_3d['Test accuracy'][0]]\n\n#create series with y_axis values\nfreq_series = pd.Series(y_axis)\n\n#plot the graph\ndisplay(Markdown('<br>**\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*    Graph Plot    \\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\***<br><br>'))\nplt.figure(figsize=(12,6))\nax = freq_series.plot(kind='bar')\nplt.xlabel('Dropout Rates', fontsize=15)\nplt.ylabel('Test Accuracy', fontsize=15)\nplt.ylim(min(y_axis)-1,max(y_axis)+1)\nplt.title('Bar graph for Dropout Rates vs. Test Accuracy', fontsize=15)\nax.set_xticklabels(x_axis)\n\n#add label for each graph\nadd_value_labels(ax)","da23d109":"#read mnist dataset\nmnist = keras.datasets.fashion_mnist\n(X_train, y_train), (X_test, y_test) = mnist.load_data()\nprint(X_train.shape, y_train.shape, X_test.shape, y_test.shape)\n\n#reshape the dataframe\nX_train=X_train.reshape(60000, 28, 28, 1)\nX_test = X_test.reshape(10000, 28, 28, 1)\n\n#feature scaling\nX_train=X_train \/ 255.0\nX_test=X_test\/255.0\n\n#print the shape of each dataframe\nprint(X_train.shape, y_train.shape, X_test.shape, y_test.shape)","e2d8430c":"def model_cnn(count=1):\n    '''Convolution Neural Network'''\n    \n    #Model Initializing, Compiling and Fitting\n    classifier = Sequential()\n    \n    #convolution layer\n    classifier.add(Convolution2D(32, (3,3), activation='relu', input_shape=(28, 28, 1)))\n    \n    #max-pooling layer\n    classifier.add(MaxPooling2D(2,2))\n    \n    #in case of multiple convolution layer\n    if count>1:\n        for i in range(count-1):\n            classifier.add(Convolution2D(32, (3,3), activation='relu'))\n            classifier.add(MaxPooling2D(2,2))\n            \n    #flatten layer\n    classifier.add(Flatten())\n    \n    #fully connected layer\n    #dense (hidden) layer\n    classifier.add(Dense(units = 256, kernel_initializer='uniform', activation='relu'))\n    \n    #output layer\n    classifier.add(Dense(units = 10, kernel_initializer='uniform', activation='softmax'))\n    \n    #compile the model\n    classifier.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n\n    #model summary\n    display(Markdown('<br>**\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*  Model  Summary  \\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\***<br>'))\n    classifier.summary()\n\n    #include timing details\n    dh = display('',display_id=True)\n    dh.update(md(\"<br>Training is in progress.....\"))\n    t1 = time.time()\n    \n    #fit the model with training dataset\n    classifier_fit = classifier.fit(X_train, y_train, epochs=50, batch_size=32, verbose=0)\n    tt = time.time()-t1\n    dh.update(md(\"<br>Training is completed! Total training time: **{} seconds**\".format(round(tt,3))))\n    display(Markdown('<br>**\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*     Training Summary    \\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\***<br>'))\n\n    #plot the graph\n    #accuracy graph\n    fig, axes = plt.subplots(nrows=1, ncols=2,figsize=(15,5))\n    ax = axes.ravel()\n    ax[0].plot(range(0,classifier_fit.params['epochs']), [acc * 100 for acc in classifier_fit.history['accuracy']], label='Accuracy')\n    ax[0].set_title('Accuracy vs. epoch', fontsize=15)\n    ax[0].set_ylabel('Accuracy', fontsize=15)\n    ax[0].set_xlabel('epoch', fontsize=15)\n    ax[0].legend()\n\n    #loss graph\n    ax[1].plot(range(0,classifier_fit.params['epochs']), classifier_fit.history['loss'], label='Loss', color='r')\n    ax[1].set_title('Loss vs. epoch', fontsize=15)\n    ax[1].set_ylabel('Loss', fontsize=15)\n    ax[1].set_xlabel('epoch', fontsize=15)\n    ax[1].legend()\n\n    #display the graph\n    plt.show()\n\n    #Evaluate the model\n    dh = display('',display_id=True)\n    dh.update(md(\"<br>Model evaluation is in progress...\"))\n    t2 = time.time()\n    \n    #model evaluation\n    test_loss = classifier.evaluate(X_test, y_test, verbose=0)\n    et = time.time()-t2\n    dh.update(md(\"<br>Model evaluation is completed! Total evaluation time: **{} seconds**\".format(round(et,3))))\n    display(Markdown('<br>**\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*    Model Evaluation Summary    \\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\***'))\n\n    #calculate the model evaluation parameters\n    f1 = f1_score(y_test, classifier.predict_classes(X_test), average='micro')\n    roc = roc_auc_score(y_test, classifier.predict_proba(X_test), multi_class='ovo')\n\n    #create model evaluation dtaaframe\n    stats = pd.DataFrame({'Test accuracy' : round(test_loss[1]*100,3),\n                          'F1 score'      : round(f1,3),\n                          'ROC AUC score' : round(roc,3),\n                          'Total Loss'    : round(test_loss[0],3)}, index=[0])\n    \n    #print the dataframe\n    display(stats)\n    \n    #return the classifier and model evaluation details\n    return classifier_fit, stats","9b8dfd26":"#run the CNN model with 1 layer\nclassifier_1cnn, stats_1cnn = model_cnn(1)","2bf0bf19":"#run the CNN model with 2 layer\nclassifier_2cnn, stats_2cnn = model_cnn(2)","de35b5cb":"#run the CNN model with 3 layer\nclassifier_3cnn, stats_3cnn = model_cnn(3)","f4eadde6":"#Plot the graph\nx_axis = ['0', '1', '2', '3']\ny_axis = [classifier_e50.history['accuracy'][-1]*100, classifier_1cnn.history['accuracy'][-1]*100, classifier_2cnn.history['accuracy'][-1]*100, classifier_3cnn.history['accuracy'][-1]*100]\n\n#create series with y_axis values\nfreq_series = pd.Series(y_axis)\n\n#plot the graph\ndisplay(Markdown('<br>**\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*    Graph Plot    \\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\***<br><br>'))\nplt.figure(figsize=(12,6))\nax = freq_series.plot(kind='bar')\nplt.xlabel('Number of CNN Layer', fontsize=15)\nplt.ylabel('Training Accuracy', fontsize=15)\nplt.ylim(min(y_axis)-1,max(y_axis)+1)\nplt.title('Bar graph for Number of CNN Layer vs. Training Accuracy', fontsize=15)\nax.set_xticklabels(x_axis)\n\n#add label for each graph\nadd_value_labels(ax)","42d6bc48":"#Plot the graph\nx_axis = ['0', '1', '2', '3']\ny_axis = [stats_e50['Test accuracy'][0], stats_1cnn['Test accuracy'][0], stats_2cnn['Test accuracy'][0], stats_3cnn['Test accuracy'][0]]\n\n#create series with y_axis values\nfreq_series = pd.Series(y_axis)\n\n#plot the graph\ndisplay(Markdown('<br>**\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*    Graph Plot    \\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\***<br><br>'))\nplt.figure(figsize=(12,6))\nax = freq_series.plot(kind='bar')\nplt.xlabel('Number of CNN Layer', fontsize=15)\nplt.ylabel('Test Accuracy', fontsize=15)\nplt.ylim(min(y_axis)-1,max(y_axis)+1)\nplt.title('Bar graph for Number of CNN Layer vs. Test Accuracy', fontsize=15)\nax.set_xticklabels(x_axis)\n\n#add label for each graph\nadd_value_labels(ax)","aff0e4af":"#Plot the graph\nx_axis = ['cv=1 epoch=25', \n          'cv=5 epoch=25', \n          'cv=10 epoch=25', \n          'epoch=25', \n          'epoch=50', \n          'epoch=100', \n          'epoch=200', \n          '1 Dense Layer', \n          '2 Dense Layer', \n          '3 Dense Layer', \n          'Dropout rate=0.0', \n          'Dropout rate=0.1', \n          'Dropout rate=0.2', \n          'Dropout rate=0.3', \n          '0 CNN Layer', \n          '1 CNN Layer', \n          '2 CNN Layer', \n          '3 CNN Layer']\n          \ny_axis = [classifier_e25_fit.history['accuracy'][-1]*100, \n          np.mean(scores_5cv['train_acc']*100), \n          np.mean(scores_10cv['train_acc']*100),\n          classifier_e25_fit.history['accuracy'][-1]*100, \n          classifier_e50.history['accuracy'][-1]*100, \n          classifier_e100.history['accuracy'][-1]*100, \n          classifier_e200.history['accuracy'][-1]*100,\n          classifier_e50.history['accuracy'][-1]*100, \n          classifier_2dl_fit.history['accuracy'][-1]*100, \n          classifier_3dl_fit.history['accuracy'][-1]*100,\n          classifier_e50.history['accuracy'][-1]*100, \n          classifier_1d.history['accuracy'][-1]*100, \n          classifier_2d.history['accuracy'][-1]*100, \n          classifier_3d.history['accuracy'][-1]*100,\n          classifier_e50.history['accuracy'][-1]*100, \n          classifier_1cnn.history['accuracy'][-1]*100, \n          classifier_2cnn.history['accuracy'][-1]*100, \n          classifier_3cnn.history['accuracy'][-1]*100]\n\n#create series with y_axis values\nfreq_series = pd.Series(y_axis)\n\n#plot the graph\ndisplay(Markdown('<br>**\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*    Graph Plot    \\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\***<br><br>'))\nplt.figure(figsize=(12,6))\nax = freq_series.plot(kind='bar')\nplt.xlabel('Models', fontsize=15)\nplt.ylabel('Training Accuracy', fontsize=15)\nplt.ylim(min(y_axis)-1,max(y_axis)+1)\nplt.title('Bar graph for All Models vs. Training Accuracy', fontsize=15)\nax.set_xticklabels(x_axis)\n\n#add label for each graph\nadd_value_labels(ax)","8041a4a8":"#Plot the model\nx_axis = ['cv=1 epoch=25', \n          'cv=5 epoch=25', \n          'cv=10 epoch=25', \n          'epoch=25', \n          'epoch=50', \n          'epoch=100', \n          'epoch=200', \n          '1 Dense Layer', \n          '2 Dense Layer', \n          '3 Dense Layer', \n          'Dropout rate=0.0', \n          'Dropout rate=0.1', \n          'Dropout rate=0.2', \n          'Dropout rate=0.3', \n          '0 CNN Layer', \n          '1 CNN Layer', \n          '2 CNN Layer', \n          '3 CNN Layer']\n          \ny_axis = [stats_e25['Test accuracy'][0], \n          stats_5cv['Test accuracy'][0], \n          stats_10cv['Test accuracy'][0], \n          stats_e25['Test accuracy'][0], \n          stats_e50['Test accuracy'][0], \n          stats_e100['Test accuracy'][0], \n          stats_e200['Test accuracy'][0], \n          stats_e50['Test accuracy'][0], \n          stats_2dl['Test accuracy'][0], \n          stats_3dl['Test accuracy'][0], \n          stats_e50['Test accuracy'][0], \n          stats_1d['Test accuracy'][0], \n          stats_2d['Test accuracy'][0], \n          stats_3d['Test accuracy'][0], \n          stats_e50['Test accuracy'][0], \n          stats_1cnn['Test accuracy'][0], \n          stats_2cnn['Test accuracy'][0], \n          stats_3cnn['Test accuracy'][0]]\n\n#create series with y_axis values\nfreq_series = pd.Series(y_axis)\n\n#plot the graph\ndisplay(Markdown('<br>**\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*    Graph Plot    \\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\***<br><br>'))\nplt.figure(figsize=(12,6))\nax = freq_series.plot(kind='bar')\nplt.xlabel('Models', fontsize=15)\nplt.ylabel('Test Accuracy', fontsize=15)\nplt.ylim(min(y_axis)-1,max(y_axis)+1)\nplt.title('Bar graph for All Models vs. Test Accuracy', fontsize=15)\nax.set_xticklabels(x_axis)\n\n#add label for each graph\nadd_value_labels(ax)","9cba9c35":"<b>Dropout<\/b> is a regularization technique for neural network models. Dropout is a technique where we randomly selected neurons are ignored during training. This means that their contribution to the activation of downstream neurons is temporally removed on the forward pass and any weight updates are not applied to the neuron on the backward pass.<br><br>\nBelow we have performed and compared the performance of the models having dropout rates 0.1,0.2 and 0.3 respectively.","2c5cbfbd":"### Neural Network with Dropout: ","c6d7d2af":"<b>We have introduced dropout and shown the impact of dropurate rate = 0.1 on accuracy and loss in the graphs above. <br>\nWe have also depicted the performance using the scoring metrics above.<\/b>","4e716893":"To train our model we ran it for 25 epochs, we have plotted the graphs highlighting the trends of Accuracy and loss with respect to the number of epochs. <br><br>They detailed key inferences generated when we increase the number of epochs from 0 to 100 are mentioned below under the Areas of improvement. <br>\n","183bbbdc":"<b>We have introduced convolution layers and shown the impact of adding 3 convolution layers on accuracy and loss in the graphs above. <br>\nWe have also depicted the performance using the scoring metrics above.<\/b>","834c61eb":"### Comparison between all models with varying Cross Validation folds","6071d005":"## 5) Convolution Layer","21aa88c9":"## MNIST Fashion Dataset","0950bdc3":"CNN is a class of neural networks and have proven to have performed exceptionally on the image classification tasks. We now add demonstrate the use of a convolutional neural network for image classification. We vary the number of convolution layers from 1 to 3 and present the corresponding effect on the performance of the model.","d78fdf3d":"## Evaluation of ANN Model","4f4298b2":"<b>We have shown above the impact of increasing the dense layers to 3 on accuracy and loss in the graphs above. <br>\nWe have also depicted the performance using the scoring metrics above.<\/b>","3dfe4f59":"<h2> Key Inferences <\/h2><br>\n1. The highest accuracy for both test and train data is achieved for the neural network for 1 Convolution layer. <br>\n2. We get a significant increase of 3% in train data accuracy when we add a convolution layer to our neural networ.<br>\n3. The difference in the test accuracy for the model with 1 convolution layer and 2 convolution layer is 0.1% <br>\n4. We can further improve the model by introducing the Dropout Layer in the fully connected layer.","a6a9bd0d":"### epochs=50","dad733a1":"<b>We have shown above the impact of increasing the dense layers to 2 on accuracy and loss in the graphs above. <br>\nWe have also depicted the performance using the scoring metrics above.<\/b>","a52ee9c2":"### Convolution Neural Network: ","e53043e3":"<b>We have shown above the accuracy and loss of the model on the train data in the graphs for the model with 25 epochs and 10 fold cross validation<br>\nWe have also depicted the performance using the scoring metrics above.<\/b>","e2f87fa4":"### Neural Network (2 Dense Layer): ","5080c976":"### epochs=100","0cdbe40d":"### 1) Cross Validation","082a82b8":"### Comparison between all models with varying Dropout rates","7a5f25c2":"The first improvement that we can add to our model is <b> Cross Validation<\/b>.<br>\n\nBelow we have made our cross validation function which takes in the value for number of epochs and thhe number of cross validation folds. We have kept the number of epochs constant at 25 and compare the performance of the model for the various values of cross validation folds.<br>\n\nWe comapare two values of Cross Validation folds namely 5 and 10. The performance of the model with cross validation on the train data for the respective values of folds is shown below the implementation.","a3c78e96":"<h2> Acknowledgment <\/h2><br>\nI would like to express our gratitude to my Professor Dr. Timothy Havens, who helped us along the project with his insightful notes and lectures. I would also like to thank Maxwell D'souza for helping me in developing this python notebook.","f7a50877":"### 3) Adding an extra Dense Layer","790fd131":"<b>We have shown above the impact of increasing the number of epochs to 50 on accuracy and loss in the graphs above. <br>\nWe have also depicted the performance using the scoring metrics above.<\/b>","8c6e5e4b":"<h2> Conclusion<\/h2><br>\nIn this project we have successfully demonstrated the use of an artificial neural network for the purpose of classfication. We then proceeded to ramp up our model by expermenting and adding various features and tuning the hyperparameters. The effect of these subtle changes to the model were shown and evaluated based on the various evaluation metrics. We also implemented a convolutional neural network which is well known for it's image classification abilities. \n\n","36246d31":"## Compiling ANN Model","c8d9f8b0":"The dataset used is a dataset of Zalando's article images 28x28 grayscale images of 10 fashion categories, consisting of a training set of 60,000 images along with a test set of 10,000 images. <br>\n\nThe input images are of the shape 28X28 which we reshape to a single vector of length 784. The values defining the image range from 0 to 255, we have used the min max scaler to scale them between 0 and 1.<br><br>\n\nThe class labels are:\n<table class=\"tg\">\n  <tr>\n    <th class=\"tg-0lax\">Label<\/th>\n    <th class=\"tg-0lax\">Description<\/th>\n  <\/tr>\n  <tr>\n    <td class=\"tg-0lax\">0<\/td>\n    <td class=\"tg-0lax\">T-shirt\/top<\/td>\n  <\/tr>\n  <tr>\n    <td class=\"tg-0lax\">1<\/td>\n    <td class=\"tg-0lax\">Trouser<\/td>\n  <\/tr>\n  <tr>\n    <td class=\"tg-0lax\">2<\/td>\n    <td class=\"tg-0lax\">Pullover<\/td>\n  <\/tr>\n  <tr>\n    <td class=\"tg-0lax\">3<\/td>\n    <td class=\"tg-0lax\">Dress<\/td>\n  <\/tr>\n  <tr>\n    <td class=\"tg-0lax\">4<\/td>\n    <td class=\"tg-0lax\">Coat<\/td>\n  <\/tr>\n  <tr>\n    <td class=\"tg-0lax\">5<\/td>\n    <td class=\"tg-0lax\">Sandal<\/td>\n  <\/tr>\n  <tr>\n    <td class=\"tg-0lax\">6<\/td>\n    <td class=\"tg-0lax\">Shirt<\/td>\n  <\/tr>\n  <tr>\n    <td class=\"tg-0lax\">7<\/td>\n    <td class=\"tg-0lax\">Sneaker<\/td>\n  <\/tr>\n  <tr>\n    <td class=\"tg-0lax\">8<\/td>\n    <td class=\"tg-0lax\">Bag<\/td>\n  <\/tr>\n  <tr>\n    <td class=\"tg-0lax\">9<\/td>\n    <td class=\"tg-0lax\">Ankle boot<\/td>\n  <\/tr>\n<\/table>\n","4e85d911":"### Comparison between all models with varying number of CNN layers","5130eb92":"### Data Preparation","591e3f3e":"### Neural Network (3 Dense Layer): ","99526bb0":"### 2) Increasing number of epochs\nBelow we experiment by increasing the number of epochs from 25 to 50 to 100 to 200. We aim to keep the number of epochs as high as possible and terminate the training based on the error rate. If the accuracy reduces it means that our model is overfitting and we should limit the number of epochs.<br>\nThe comparison of the test and train accuracies with respect to the values of epochs is shown after the implementation.","6364fe89":"<h2> Key Inferences <\/h2><br>\n1. As we can see above that the CNN model with 1 convolution layers achieves the highest accuracy on the train and test data.<br>\n2. For the test data we can see that we increase the number of epochs the accuracy increases, but only upto a certain threshold after which the accuracy starts decreasing. <br>\n3. The model with 3 CNN layers achieves the lowest accuracy of 88.2%. <br>\n4. With the increase in the dropout rate from 0.0 to 0.3, the accuracy decreases.","e4c9b3e2":"## Comparison of all models mentioned above","cb04cf1f":"Artificial neural networks have two main hyperparameters that control the architecture of the network: the number of layers and the number of nodes in each hidden layer.<br>\nIn the Neural Network models below we add 1 and 2 new dense layers to our existing model and compare the performance of the model with 1, 2 and 3 dense layers.","5f619b20":"### Dropout rate=0.1 and Dense Layer=2","7ab50382":"<h2> Key Inferences<\/h2><br>\n1. The test accuracy of our model is 88.25%.<br><br>\n2. Our model has achieved an F1 score of 0.888. F1 scores range from the worst value 0 to the best value 1. The F1 score is the weighted average of the precison and recall. In our case, where we have a multiclass classification, the F1 score the average of average of the F1 score of each class.<br><br>\n3. The AUC-ROC curve is a performance measure for classification problems. ROC is a probabbility curve and AUC represents degree or measure of separability. Wehave achieved and AUC-ROC score of 0.991<br><br>\n4. The total loss for the test data is 0.399. ","4e628de1":"### Number of CNN layer = 3","050a2117":"<b>We have shown above the impact of increasing the number of epochs to 100 on accuracy and loss in the graphs above. <br>\nWe have also depicted the performance using the scoring metrics above.<\/b>","3eae688f":"### epochs=200","62b243c3":"### Data Visualization","cdc3a13b":"<h3> key Inferences<\/h3><br>\n1. The accuracy of the model with varying number of epochs depends on the error rate. As we've seen in the bar chart above, the training accuracy increases considerably with the increase of epochs.<br>\n2. Although the training accuracy of model with 100 epochs is greater than the model with 50 epochs, the testing accuarcy doesn't reflect that.<br>\n3. The accuracy of the model with 200 epochs is the highest for the train data.<br>\n4. The accuarcy of the model with 200 epochs is the highest for the test data.<br>\n\n","68306aed":"We have implemented the sequential model as our neural network model. The sequential model is a linear stack of layers, we have added layers to the network by using the .add() method. <br><br>\nkernel_initializer defines which statistical distribution or function to use for initialising the weights. In case of statistical distribution, the library will generate numbers from that statistical distribution and use as starting weights. In our code above we have used uniform distribution to initialize the weights. <br><br>\nIn a neural network, the activation function is responsible for transforming the summed weighted input from the node into the activation of the node or output for that input. The rectified linear activation (Relu) function is a piecewise linear function that will output the input directly if is positive, otherwise, it will output zero. The rectified linear activation function overcomes the vanishing gradient problem, allowing models to learn faster and perform better.<br><br>\nThe output of the softmax function is equivalent to a categorical probability distribution, it tells you the probability that any of the classes are true. The class with the highest probability is chosen as the output.\n<br>","4247864b":"## Training ANN Model","53c49d48":"<b>We have introduced dropout and shown the impact of dropurate rate = 0.3 on accuracy and loss in the graphs above. <br>\nWe have also depicted the performance using the scoring metrics above.<\/b>","2ea12e3e":"## Area of Improvement","762ae299":"<b>We have introduced convolution layers and shown the impact of adding 1 convolution layer on accuracy and loss in the graphs above. <br>\nWe have also depicted the performance using the scoring metrics above.<\/b>","c28eb224":"### Number of CNN layer = 2","28265f0b":"<b>Optimization<\/b> is the task of searching for parameters that minimize our loss function. We open use categorical crossentropy  when it is a multiclass classification task.<br> <b>Cross entropy<\/b> is a loss function, used to measure the dissimilarity between the distribution of observed class labels and the predicted probabilities of class membership.<br>In our model we have impemeneted <b>sparse categorical crossentropy<\/b> since we have intergers numbered from 0-9 as our class labels. We have implemeneted <b>Adam<\/b> which is an optimization algorithm that can be used instead of the classical stochastic gradient descent procedure to update network weights iterative based on training data.","4ebfe7f1":"### Dropout rate=0.2 and Dense Layer=2","27732e0c":"<b>We have introduced convolution layers and shown the impact of adding 2 convolution layers on accuracy and loss in the graphs above. <br>\nWe have also depicted the performance using the scoring metrics above.<\/b>","3efec876":"## Neural Network Model","bc0b545c":"### Neural Network (1 Dense Layer): ","d916993f":"<h3> key Inferences<\/h3><br>\n1. The accuracy of the model for the various folds depends on the distribution of the data. For eg :  If the model hasn't encountered enough images of Trousers in the training and encounters a large number of images trousers in the trousers in the validation set then the accuracy drops significantly for that fold.<br>\n2. The accuracy of 5 fold cross validation is the highest for the train data.<br>\n3. The accuarcy of 10 fold cross validation is the highest for the test data.<br>\n\n","73925095":"### Data Preparation","67abf7ff":"<h3> key Inferences<\/h3><br>\n1. The train accuracy is highest for the model with 2 dense layers.<br>\n2. The test accuracy is highest for the model with 3 dense layers. <br>\n3. We varied the number of nerurons or units in the new dense layers that we added to our original model.<br>\n\n","2dad95e3":"### Number of CNN layer = 1","70c7bc82":"<b>We have shown above the impact of increasing the number of epochs to 200 on accuracy and loss in the graphs above. <br>\nWe have also depicted the performance using the scoring metrics above.<\/b>","51f39e82":"## 4) Dropout","e0e0e098":"### Comparison between all models with varying epoch values","3df33bcc":"<h3> key Inferences<\/h3><br>\n1. The train accuracy is highest for the model with a dropout rate of 0.0<br>\n2. The test accuracy is highest for the model with a dropout rate of 0.2. <br>\n3. Applying dropout before the last layer is not advised as the network has no ability to recitify the errors induced by dropout before the classification happens.<br>\n4. Our network is relatively shallow compared to the dataset, hence reguariztaion may not have been required but we did it regardless to experiment.\n\n","ee9f83d0":"### Dropout rate=0.3 and Dense Layer=2","4f142792":"<b>We have introduced dropout and shown the impact of dropurate rate = 0.2 on accuracy and loss in the graphs above. <br>\nWe have also depicted the performance using the scoring metrics above.<\/b>","8e7b818f":"### Comparison between all models with varying Dense layers"}}