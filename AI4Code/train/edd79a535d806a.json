{"cell_type":{"ded12b11":"code","9e4e1176":"code","4afc0b50":"code","a0dc01e9":"code","273071ee":"code","3377dac5":"code","1e2c9436":"code","0b66109a":"code","b4506f02":"code","001215d8":"code","7da17730":"code","2a13eb5a":"code","debceccd":"code","b9394c24":"code","b54abe5e":"code","0250913d":"code","ee70a31f":"code","4aae01e9":"code","2bcd1cf2":"code","fc4dce96":"code","08fea8f9":"markdown","d19a9248":"markdown","e235ff0a":"markdown","6ee1a87c":"markdown","a132052b":"markdown"},"source":{"ded12b11":"import os\nfrom pathlib import Path\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport math\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import cross_val_score, train_test_split\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import LabelEncoder\n\n%matplotlib inline\n# Any results you write to the current directory are saved as output.","9e4e1176":"INPUT_PATH = Path('..','input')\nTRAIN_PATH = Path(INPUT_PATH, 'train.csv')\nTEST_PATH = Path(INPUT_PATH, 'test.csv')\n\ntrain = pd.read_csv(TRAIN_PATH, index_col='id', parse_dates=['pickup_datetime', 'dropoff_datetime'])\ntrain.head()","4afc0b50":"test = pd.read_csv(TEST_PATH, index_col='id', parse_dates=['pickup_datetime'])\ntest.head()","a0dc01e9":"def extract_date_info(df, cols):\n    for col in cols:\n        df[col + '_month'] = df[col].dt.month\n        df[col + '_week'] = df[col].dt.week\n        df[col + '_dow'] = df[col].dt.dayofweek\n        df[col + '_hour'] = df[col].dt.hour\n        df[col + '_date'] = df[col].dt.date\n    return df","273071ee":"train = extract_date_info(train, ['pickup_datetime', 'dropoff_datetime'])\ntest = extract_date_info(test, ['pickup_datetime'])\ntrain.head()","3377dac5":"#it seems that we have some very long trips that prevent us from looking at the ditribution let's plot all trip < 2h\nfig, ax = plt.subplots(figsize=(18,8))\n(train.loc[train['trip_duration'] < 2*3600, 'trip_duration'] \/60).hist(bins= 100, ax=ax);\n#seems like most of the traject are below 40 mins","1e2c9436":"# I'm now going to add the distance in my dataframe using the flight distance as the crow flies\n# got the distance function from: https:\/\/janakiev.com\/blog\/gps-points-distance-python\/\n\ndef haversine(lat1, lon1, lat2, lon2):\n    R = 6372800  # Earth radius in meters  \n    phi1, phi2 = math.radians(lat1), math.radians(lat2) \n    dphi       = math.radians(lat2 - lat1)\n    dlambda    = math.radians(lon2 - lon1)\n    \n    a = math.sin(dphi\/2)**2 + \\\n        math.cos(phi1)*math.cos(phi2)*math.sin(dlambda\/2)**2\n    \n    return 2*R*math.atan2(math.sqrt(a), math.sqrt(1 - a))","0b66109a":"train['distance'] = train.apply(lambda row: haversine(row['pickup_latitude'], row['pickup_longitude'], row['dropoff_latitude'], row['dropoff_longitude']), axis=1)","b4506f02":"fig, ax = plt.subplots(figsize=(15,8))\nax.plot(train.groupby('pickup_datetime_date').count())\nax.plot(test.groupby('pickup_datetime_date').count());","001215d8":"# next step : Speed\n\n# train['speed'] = (train['distance'] \/ train['trip_duration'])*3,6 ","7da17730":"train.head()","2a13eb5a":"X = train.drop(columns=['pickup_datetime', 'dropoff_datetime', 'trip_duration', \n                                 'pickup_datetime_date', 'dropoff_datetime_month',\n                                 'dropoff_datetime_week','dropoff_datetime_dow',\n                                 'dropoff_datetime_hour', 'dropoff_datetime_date'])","debceccd":"le = LabelEncoder()\nle.fit(X['store_and_fwd_flag'])\nX['store_and_fwd_flag'] = le.transform(X['store_and_fwd_flag'])\n\ny = np.log1p(train['trip_duration'])","b9394c24":"X.info()","b54abe5e":"X_test = test.drop(columns=['pickup_datetime', 'pickup_datetime_date'])\n\nX_test['store_and_fwd_flag'] = le.transform(X_test['store_and_fwd_flag'])\n\nX_test['distance'] = test.apply(lambda row: haversine(row['pickup_latitude'], row['pickup_longitude'], row['dropoff_latitude'], row['dropoff_longitude']), axis=1)\nX_test.info()","0250913d":"rf = RandomForestRegressor(n_estimators=150, n_jobs=-1)\n#Loss =  cross_val_score(rf, X, y, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\n#np.mean(np.sqrt(-Loss))","ee70a31f":"rf.fit(X, y)\nlog_pred = rf.predict(X_test)\npred = np.expm1(log_pred)","4aae01e9":"SUBMIT_PATH = Path(INPUT_PATH, 'sample_submission.csv')\nsubmit = pd.read_csv(SUBMIT_PATH)\n\narr_id = submit['id']\nsubmission = pd.DataFrame({'id': arr_id, 'trip_duration': pred})\nsubmission.head()","2bcd1cf2":"fi_dict = {\n    'feats': X.columns,\n    'feature_importance': rf.feature_importances_\n}\nfi = pd.DataFrame(fi_dict).set_index('feats').sort_values(\n    'feature_importance', ascending=False)\nfi.sort_values(\n    'feature_importance', ascending=True).tail(10).plot.barh();","fc4dce96":"submission.to_csv(\"submission.csv\", index=False)","08fea8f9":"Date extraction","d19a9248":"The 2 dataframes exactly overlaps on time so I will use this in my feature engineering and calulate a average speed per day","e235ff0a":"# Processing test.set","6ee1a87c":"# Feature selection + preprocess","a132052b":"# EDA"}}