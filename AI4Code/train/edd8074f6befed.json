{"cell_type":{"31a165ff":"code","20700935":"code","f30baa8c":"code","72cca8be":"code","983b331c":"code","8df437e4":"code","b01e464a":"code","a74b8a13":"code","75b7d76f":"code","d3c6de2b":"code","ce79119c":"code","62c7a329":"code","c321c4d4":"code","9582f195":"code","eaa048ef":"code","26b69a6a":"code","e7123ba8":"code","897dce2a":"code","e9df9291":"code","6870e37f":"code","7b345b87":"code","ba0dede4":"code","38ed6162":"code","65fbf9c4":"code","c93318be":"code","5b638d4e":"code","75a19ca7":"code","20179f87":"code","fa2e48e4":"code","16636bb5":"code","edd4d59b":"code","0f3fff16":"code","e8feb701":"code","dc40303c":"code","14de5d2e":"code","c2602e70":"code","7026859a":"code","2a55e01a":"code","477068a0":"code","da486219":"code","3e93899e":"code","56f3dfc2":"code","bf8e6177":"code","07684bb0":"code","01bef022":"code","c72d6f20":"code","2ae42942":"code","ef80f761":"code","0e303c73":"code","274a12ed":"code","830e4ca0":"code","138c7e83":"markdown","e750a6a3":"markdown","cb899b56":"markdown","fc1ef32f":"markdown","43fef443":"markdown","c53fca7f":"markdown","6e623295":"markdown","f96bce90":"markdown","a0953bbe":"markdown","d5a24d99":"markdown","3458bae3":"markdown"},"source":{"31a165ff":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nimport time\nimport csv\nimport traceback\nimport pandas as pd\nimport numpy as np\nimport dask.dataframe as dd #reading large datasets out of memory\n\n# visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","20700935":"\n#import the files with appropriate encoding\n\napplication = pd.read_csv(\"..\/input\/application_train.csv\")\nbureau = pd.read_csv(\"..\/input\/bureau.csv\")\nbureau_balance = pd.read_csv(\"..\/input\/bureau_balance.csv\")\ncredit_card_balance = pd.read_csv(\"..\/input\/credit_card_balance.csv\",encoding='utf-8')\nHomeCredit_col = pd.read_csv(\"..\/input\/HomeCredit_columns_description.csv\",encoding='latin-1')\n#installments_payments = pd.read_csv(\"..\/input\/installments_payments.csv\")\nPOS_CASH_balance = pd.read_csv(\"..\/input\/POS_CASH_balance.csv\")\nprevious_application = pd.read_csv(\"..\/input\/previous_application.csv\")\napplication_test = pd.read_csv(\"..\/input\/application_test.csv\")","f30baa8c":"application.head()","72cca8be":"application_test.head(2)","983b331c":"HomeCredit_col.head(219) #The description file !!!","8df437e4":"def get_description(keyword_list): #Let's make a function that returns the description for selected items only\n    \n    description = HomeCredit_col[['Row','Table','Description']]   \n    df = description.loc[description['Row'].str.contains(keyword_list[0], case=False)]\n    for i in range(1, len(keyword_list)):\n        df = df.append(description.loc[description['Row'].str.contains(keyword_list[i], case=False)])\n    return(df)","b01e464a":"list(application.columns.values) #Let's see whats in this first application file...","a74b8a13":"#These items sound the most interesting\n\nkeyword_list_application = ['SK_ID_CURR','TARGET','CODE_GENDER','DAYS_EMPLOYED','CNT_CHILDREN',\n                                  'DAYS_BIRTH','AMT_INCOME_TOTAL','NAME_INCOME_TYPE',\n                                  'NAME_HOUSING_TYPE','DAYS_LAST_PHONE_CHANGE']","75b7d76f":"#Check if the description matches with what we thought it would be\n\nget_description(keyword_list_application)","d3c6de2b":"keyword_list_bureau = ['SK_ID_CURR', 'CREDIT_ACTIVE','AMT_CREDIT_SUM', 'CREDIT_DAY_OVERDUE']\nget_description(keyword_list_bureau)","ce79119c":"selected_col_bureau = bureau[keyword_list_bureau]","62c7a329":"#create a new dataframe with the selected features only\nselected_col_application = application[keyword_list_application] ","c321c4d4":"list(previous_application.columns.values)","9582f195":"keyword_list_previous_application = ['SK_ID_CURR', 'RATE_INTEREST_PRIMARY', 'AMT_CREDIT','CODE_REJECT_REASON' ]\nget_description(keyword_list_previous_application)","eaa048ef":"selected_col_prev_app = previous_application[keyword_list_previous_application]","26b69a6a":"merged = selected_col_application.merge(selected_col_bureau, left_on='SK_ID_CURR', right_on='SK_ID_CURR', how='inner')\nmerged = merged.merge(selected_col_prev_app, left_on='SK_ID_CURR', right_on='SK_ID_CURR', how='inner')\nmerged = merged.drop_duplicates('SK_ID_CURR', keep='first')\n\nmerged.head()","e7123ba8":"merged['Age'] = merged['DAYS_BIRTH']\/-365\nmerged['Phone_age'] = merged['DAYS_LAST_PHONE_CHANGE']\/-365\nmerged.drop('DAYS_LAST_PHONE_CHANGE', axis = 1)\nmerged.drop('DAYS_BIRTH', axis = 1)\n\nmerged.head()","897dce2a":"merged.describe()","e9df9291":"merged['Family_size'] = 0 \nmerged.loc[merged['CNT_CHILDREN'] > 0 , 'Family_size'] = 1 #Normal\nmerged.loc[merged['CNT_CHILDREN'] > 3 , 'Family_size'] = 2 #Large","6870e37f":"merged['Income\/credit'] = merged['AMT_INCOME_TOTAL']\/merged['AMT_CREDIT_SUM']","7b345b87":"merged['Income_category'] = \"Low\"\nmerged.loc[merged['AMT_INCOME_TOTAL'] > 112500 , 'Income_category'] = \"Below_average\"\nmerged.loc[merged['AMT_INCOME_TOTAL'] > 157500 , 'Income_category'] = \"Above_average\"\nmerged.loc[merged['AMT_INCOME_TOTAL'] > 202500 , 'Income_category'] = \"High\"\nmerged.reset_index(drop = True)\nmerged.head()","ba0dede4":"merged.describe()","38ed6162":"frequency = merged.describe()['TARGET'][1] # since we have only zeros and ones the mean is the frequency\nprint(\"Frequency of Target = 1 is : \", frequency)","65fbf9c4":"merged[['Income_category', 'TARGET']].groupby(['Income_category'], as_index=False).mean().sort_values(by='TARGET', ascending=False)","c93318be":"merged[['CODE_GENDER', 'TARGET']].groupby(['CODE_GENDER'], as_index=False).mean().sort_values(by='TARGET', ascending=False)","5b638d4e":"merged[['NAME_INCOME_TYPE', 'TARGET']].groupby(['NAME_INCOME_TYPE'], as_index=False).mean().sort_values(by='TARGET', ascending=False)","75a19ca7":"merged[['NAME_HOUSING_TYPE', 'TARGET']].groupby(['NAME_HOUSING_TYPE'], as_index=False).mean().sort_values(by='TARGET', ascending=False)","20179f87":"merged[['CNT_CHILDREN', 'TARGET']].groupby(['CNT_CHILDREN'], as_index=False).mean().sort_values(by='TARGET', ascending=False)","fa2e48e4":"merged[['CODE_REJECT_REASON', 'TARGET']].groupby(['CODE_REJECT_REASON'], as_index=False).mean().sort_values(by='TARGET', ascending=False)","16636bb5":"g = sns.FacetGrid(merged, col='TARGET')\ng.map(plt.hist, 'Age', bins=20)","edd4d59b":"g = sns.FacetGrid(merged, col='TARGET')\ng.map(plt.hist, 'Phone_age', bins=20)","0f3fff16":"grid = sns.FacetGrid(merged, col='TARGET', row='CODE_GENDER', size=2.2, aspect=1.6)\ngrid.map(plt.hist, 'Age', alpha=.5, bins=20)\ngrid.add_legend();","e8feb701":"grid = sns.FacetGrid(merged, row='Income_category', size=2.2, aspect=5)\ngrid.map(sns.pointplot, 'NAME_HOUSING_TYPE', 'TARGET', 'CODE_GENDER', palette='deep')\ngrid.add_legend()","dc40303c":"grid = sns.FacetGrid(merged, row='Family_size', col='TARGET', size=2.2, aspect=1.6)\ngrid.map(sns.barplot, 'CODE_GENDER', 'AMT_CREDIT_SUM', alpha=.5, ci=None)\ngrid.add_legend()","14de5d2e":"import os\nimport csv\nimport traceback\nimport shutil\nimport tensorflow as tf\nfrom tensorflow import keras\n\n\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier","c2602e70":"def scale(feature):\n    min_x = merged.min()[feature]\n    max_x = merged.max()[feature]\n    \n    merged[feature] = (merged[feature] - min_x) \/ (max_x - min_x)\n\n","7026859a":"scale('DAYS_EMPLOYED')\nscale('CNT_CHILDREN')\nscale('AMT_INCOME_TOTAL')\nscale('DAYS_LAST_PHONE_CHANGE')\nscale('AMT_CREDIT_SUM')\nscale('CREDIT_DAY_OVERDUE')\nscale('RATE_INTEREST_PRIMARY')\nscale('AMT_CREDIT')\nscale('Age')\nscale('Family_size')\nscale('Income\/credit')\nscale('Phone_age')\nscale('DAYS_BIRTH')","2a55e01a":"cat_columns = merged.select_dtypes(['object']).columns\ncat_columns","477068a0":"merged[cat_columns] = merged[cat_columns].astype('category')\n\nmerged['NAME_HOUSING_TYPE'] = merged['NAME_HOUSING_TYPE'].cat.codes\nmerged['NAME_INCOME_TYPE'] = merged['NAME_INCOME_TYPE'].cat.codes\nmerged['CODE_GENDER'] = merged['CODE_GENDER'].cat.codes\nmerged['CREDIT_ACTIVE'] = merged['CREDIT_ACTIVE'].cat.codes\nmerged['CODE_REJECT_REASON'] = merged['CODE_REJECT_REASON'].cat.codes\nmerged['Income_category'] = merged['Income_category'].cat.codes","da486219":"merged.set_index('SK_ID_CURR', inplace=True)","3e93899e":"merged = merged.dropna() #dropping all null values\n#merged = merged.reset_index(drop = True)\nmerged = merged.drop('RATE_INTEREST_PRIMARY',axis = 1)","56f3dfc2":"np.random.seed(seed=1) #makes result reproducible\nmsk = np.random.rand(len(merged)) < 0.8\ntraindf = merged[msk]\nevaldf = merged[~msk]","bf8e6177":"X_train = traindf.drop('TARGET', axis = 1)\n\nY_train = traindf['TARGET']\n\nX_test = evaldf.drop('TARGET', axis = 1)\nY_test = evaldf['TARGET']","07684bb0":"X_train.shape, Y_train.shape, X_test.shape","01bef022":"#KNeighborsClassifier\n\nknn = KNeighborsClassifier(n_neighbors = 3)\nknn.fit(X_train, Y_train)\nY_pred = knn.predict(X_test)\nacc_knn = round(knn.score(X_train, Y_train) * 100, 2)\nacc_knn\n\nacc_knn_test = round(knn.score(X_test, Y_test) * 100, 2)\nacc_knn_test\n\nprint('Train:',acc_knn, 'Test:', acc_knn_test)","c72d6f20":"#Support vector machine\n\nsvc = SVC()\nsvc.fit(X_train, Y_train)\nY_pred = svc.predict(X_test)\nacc_svc = round(svc.score(X_train, Y_train) * 100, 2)\nacc_svc\n\nacc_svc_test = round(svc.score(X_test, Y_test) * 100, 2)\nacc_svc_test\n\nprint('Train:',acc_svc, 'Test:', acc_svc_test)","2ae42942":"#random_forest\n\nrandom_forest = RandomForestClassifier(n_estimators=120)\nrandom_forest.fit(X_train, Y_train)\nY_pred = random_forest.predict(X_test)\nrandom_forest.score(X_train, Y_train)\nacc_random_forest = round(random_forest.score(X_train, Y_train) * 100, 2)\nacc_random_forest\n\nacc_random_forest_test = round(random_forest.score(X_test, Y_test) * 100, 2)\nacc_random_forest_test\n\nprint('Train:',acc_random_forest, 'Test:', acc_random_forest_test)","ef80f761":"# Decision Tree\n\ndecision_tree = DecisionTreeClassifier()\ndecision_tree.fit(X_train, Y_train)\nY_pred = decision_tree.predict(X_test)\nacc_decision_tree = round(decision_tree.score(X_train, Y_train) * 100, 2)\n\nacc_decision_tree_Test = round(decision_tree.score(X_test, Y_test) * 100, 2)\nacc_decision_tree_Test\n\nprint('Train:',acc_decision_tree, 'Test:', acc_decision_tree_Test)","0e303c73":"submission = pd.DataFrame({\n        \"Real\": evaldf[\"TARGET\"],\n        \"Prediction\": Y_pred\n    })","274a12ed":"submission.head()","830e4ca0":"submission.describe()","138c7e83":"### Building the model\n\nWe separate the labels from the test data.\n\nWe start 4 well know machine learing algorithms and compare their score before going for the Decision Tree model. \n\nWe achieve a score of around 85% true positives meaning we can predict correctly 85% of the time correctly if someone is a bad payer, and we predict more than 99% of the time a good payer.\n\nThe model is highly improvable if we would use more of the avilable features, better preanalysis and eventually an other more complex model. \n\n<strong> Yet we can be proud of the result since we can eliminate 85% of the bad payers conserving most of the good ones as clients <\/strong>.","e750a6a3":"<h2 style=\"color: blue;\"> Selecting the most interesting features <\/h2>\n \n Lets select the most interesting features since we cannot possibly analyse everything manually. \n \nIdea: Why not using tensorflow in a second part to identify the most important weights using a DNN classifier?","cb899b56":"### Correlating categorical and numerical features\n\n\n","fc1ef32f":"### Analyze by pivoting features\nTo confirm some of our observations and assumptions, we can quickly analyze our feature correlations by pivoting features against each other. We can only do so at this stage for features which do not have any empty values\n\n- <strong>Income category <\/strong>: the 4 different salary categories we created earlier have surprisingly all the same correlation, not giving us much informations\n- <strong>Gender <\/strong> : low correlation, man have a slitly higher risk of defaulting than women\n- <strong>Income type <\/strong> : low correlation, possible error with \"student\" and \"unemployed\", considering removing those\n- <strong>Housing type<\/strong> : perceptible correlation (>0.1) \"Rented appartement\" and \"living with parents\" seem to be positively correlated with defaulting\n- <strong> NB of children <\/strong> : Missleading, we have an intersting correlation for 6 children but not 7 and 8 and a correlation of 1 for 11 and 9 children, considering dropping this feature\n- <strong> Code reject reason <\/strong> : SCOFR, LIMIT, SYSTEM, HC have all correlation > 0.1 on one hand and the other reject reasons < 0.094","43fef443":" <h2 style=\"color: blue;\"> Creating new features <\/h2>","c53fca7f":"### Analyze by visualizing data\n\nNow we can continue confirming some of our assumptions using visualizations for analyzing the data.\n\n<strong>Correlating numerical features<\/strong>\n\nHistogram chart is useful for analyzing continous numerical variables like Age where banding or ranges will help identify useful patterns. The histogram can indicate distribution of samples using automatically defined bins or equally ranged bands. \n\n- <strong>Age <\/strong>: we see that most payement difficulties are between age 20 and 45\n- <strong>Phone age <\/strong>: correlation seems relatively low, yet people changing phones the most frequently have a higher defaulting risk\n- <strong>Gender <\/strong>: On the defaulting side, both men and women seems having the same risk distribution. Yet considering the good payers, women have two hills, with a local minima between 40 and 50 years compared to men who have a continous \"blobby formation\" formation but degressive after 40 years old ","6e623295":" <h2 style=\"color: blue;\"> Merging the selected columns<\/h2>","f96bce90":"### Feature scaling","a0953bbe":" <h3 style=\"color: blue;\"> Have a glanz at the data<\/h4>","d5a24d99":" <h1 style=\"color: blue;\"> II. Anlysing<\/h1>\n \nFirst, whats the matter? The company we're analysing data for is oviously trying to predict the column \"TARGET\" which is a categorical feature. Target = 1 are clients with payment difficulties. \n \nIn our analysis, we will focus on the factors that determine more likely that the column TARGET is equal to 1. \n\n\n\nLet's start with the selected colums first.","3458bae3":" <h1 style=\"color: blue;\"> III. Predicting <\/h1>\n \n We will use our data set and split it in test and train (again) since we have lots of rows already in the existing train set and since we must have had done the same operations on the test set and the train set. \n \n Go for tensorflow and moreover the high level api KERAS"}}