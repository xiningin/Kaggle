{"cell_type":{"c5802235":"code","0f40c20e":"code","20d54f11":"code","7a7ca6dd":"code","3ee2a05b":"code","638292ea":"code","cf087234":"code","3031f799":"code","06c61db4":"code","c2305a3d":"code","1283fb6d":"code","c1dcf2cb":"code","85eac955":"code","9f52bc8a":"code","70c26815":"code","04062dce":"code","420a642c":"code","3a2b6cb9":"code","e48e7a1c":"code","406512c0":"code","3ab2512f":"code","4e4d843c":"code","d0d53363":"code","b049863d":"code","9aa1a82d":"code","f99445c1":"code","64952738":"code","ca0b97d7":"code","76aff83c":"code","593cd631":"code","948721f8":"code","b97d81bd":"code","61e3005f":"code","ac571e3e":"code","c1386c5f":"code","2ec422ba":"code","b99df714":"code","479233b7":"code","595250da":"code","016912bb":"code","3ce461ed":"code","e34cda17":"code","9363dc3d":"code","33edd6f6":"code","1a13203b":"code","f7a2ac0b":"code","50f7b2e0":"code","ec239854":"code","f205e8ad":"code","bcdf5426":"code","d9a5a218":"code","89d77a9f":"code","16472f89":"code","b1e7f2d1":"code","763caaf3":"code","35127a9d":"code","12dd7718":"code","48c352ba":"code","060d29cf":"code","8dc35dbc":"code","45644fb0":"code","e9c34801":"code","1e095256":"code","fc045b3f":"code","12dc8cd7":"code","db740277":"code","bfa11bb4":"code","47da8202":"code","98f77b7f":"code","59308b2c":"code","0810df22":"code","12dd7c7d":"code","1031559e":"code","fcf8d054":"code","6cebd56b":"code","99a8d822":"code","1c77df9c":"code","7019c26f":"code","4ea04cf9":"code","9241bc79":"code","b038fa86":"code","07c28efb":"code","80953cbe":"code","d9105318":"code","8d05a0d4":"code","28630ebc":"code","065c7ea7":"code","343d6a13":"code","0aca1b37":"code","fd240f5e":"code","3a9e7a2e":"code","f7e6c3a9":"code","c3b31b94":"code","6493dd72":"code","52307be5":"code","18985a0b":"code","1dd1a9c7":"code","c04639ad":"code","ba1c9b54":"markdown","0b4af0f3":"markdown","3b46f60d":"markdown","10dd8671":"markdown","151333df":"markdown","6eefd958":"markdown","b2a7099c":"markdown","c8f07d9d":"markdown","1c56c91f":"markdown","b9d7744b":"markdown","03723546":"markdown","65015ae9":"markdown","6a21039d":"markdown","913913f5":"markdown","941624f1":"markdown","9b77df22":"markdown","5b6b4302":"markdown","ce6e9783":"markdown","e2a3b442":"markdown","7c9fe754":"markdown","af3061a3":"markdown","2029f8bd":"markdown","07d1e975":"markdown","a78213a1":"markdown","7f44dbc0":"markdown"},"source":{"c5802235":"#Our Goal\n#Given historical data on loans given out with information on whether or not the borrower defaulted (charge-off), can we build a model thatcan predict wether or nor a borrower will pay back their loan? This way in the future when we get a new potential customer we can assess whether or not they are likely to pay back the loan. Keep in mind classification metrics when evaluating the performance of your model!\n\n#The \"loan_status\" column contains our label.\nimport pandas as pd\ndata_info = pd.read_csv('..\/input\/lendingclub-data-sets\/lending_club_info.csv',index_col='LoanStatNew')\nprint(data_info.loc['revol_util']['Description'])","0f40c20e":"def feat_info(col_name):\n    print(data_info.loc[col_name]['Description'])","20d54f11":"feat_info('mort_acc')","7a7ca6dd":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# might be needed depending on your version of Jupyter\n%matplotlib inline","3ee2a05b":"df = pd.read_csv('..\/input\/lendingclub-data-sets\/lending_club_loan_two.csv')\n","638292ea":"df.info()","cf087234":"sns.countplot(x='loan_status',data=df)","3031f799":"plt.figure(figsize=(12,4))\nsns.distplot(df['loan_amnt'],kde=False,bins=40)\nplt.xlim(0,45000)","06c61db4":"df.corr()","c2305a3d":"plt.figure(figsize=(12,7))\nsns.heatmap(df.corr(),annot=True,cmap='viridis')\nplt.ylim(10, 0)","1283fb6d":"feat_info('installment')","c1dcf2cb":"feat_info('loan_amnt')","85eac955":"sns.scatterplot(x='installment',y='loan_amnt',data=df,)","9f52bc8a":"sns.boxplot(x='loan_status',y='loan_amnt',data=df)","70c26815":"df.groupby('loan_status')['loan_amnt'].describe()","04062dce":"sorted(df['grade'].unique())","420a642c":"sorted(df['sub_grade'].unique())","3a2b6cb9":"sns.countplot(x='grade',data=df,hue='loan_status')","e48e7a1c":"plt.figure(figsize=(12,4))\nsubgrade_order = sorted(df['sub_grade'].unique())\nsns.countplot(x='sub_grade',data=df,order = subgrade_order,palette='coolwarm' )","406512c0":"plt.figure(figsize=(12,4))\nsubgrade_order = sorted(df['sub_grade'].unique())\nsns.countplot(x='sub_grade',data=df,order = subgrade_order,palette='coolwarm' ,hue='loan_status')","3ab2512f":"f_and_g = df[(df['grade']=='G') | (df['grade']=='F')]\n\nplt.figure(figsize=(12,4))\nsubgrade_order = sorted(f_and_g['sub_grade'].unique())\nsns.countplot(x='sub_grade',data=f_and_g,order = subgrade_order,hue='loan_status')","4e4d843c":"df['loan_status'].unique()","d0d53363":"df['loan_repaid'] = df['loan_status'].map({'Fully Paid':1,'Charged Off':0})","b049863d":"df[['loan_repaid','loan_status']]","9aa1a82d":"df.corr()['loan_repaid'].sort_values().drop('loan_repaid').plot(kind='bar')","f99445c1":"df.head()","64952738":"len(df)","ca0b97d7":"df.isnull().sum()","76aff83c":"100* df.isnull().sum()\/len(df)","593cd631":"feat_info('emp_title')\nprint('\\n')\nfeat_info('emp_length')","948721f8":"df['emp_title'].nunique()","b97d81bd":"df['emp_title'].value_counts()","61e3005f":"df = df.drop('emp_title',axis=1)","ac571e3e":"sorted(df['emp_length'].dropna().unique())","c1386c5f":"emp_length_order = [ '< 1 year',\n                      '1 year',\n                     '2 years',\n                     '3 years',\n                     '4 years',\n                     '5 years',\n                     '6 years',\n                     '7 years',\n                     '8 years',\n                     '9 years',\n                     '10+ years']","2ec422ba":"plt.figure(figsize=(12,4))\n\nsns.countplot(x='emp_length',data=df,order=emp_length_order)","b99df714":"plt.figure(figsize=(12,4))\nsns.countplot(x='emp_length',data=df,order=emp_length_order,hue='loan_status')","479233b7":"emp_co = df[df['loan_status']==\"Charged Off\"].groupby(\"emp_length\").count()['loan_status']","595250da":"emp_fp = df[df['loan_status']==\"Fully Paid\"].groupby(\"emp_length\").count()['loan_status']","016912bb":"emp_len = emp_co\/emp_fp","3ce461ed":"emp_len","e34cda17":"emp_len.plot(kind='bar')","9363dc3d":"df = df.drop('emp_length',axis=1)","33edd6f6":"df.isnull().sum()","1a13203b":"df['purpose'].head(10)","f7a2ac0b":"df['title'].head(10)","50f7b2e0":"df = df.drop('title',axis=1)","ec239854":"feat_info('mort_acc')","f205e8ad":"df['mort_acc'].value_counts()","bcdf5426":"print(\"Correlation with the mort_acc column\")\ndf.corr()['mort_acc'].sort_values()","d9a5a218":"print(\"Mean of mort_acc column per total_acc\")\ndf.groupby('total_acc').mean()['mort_acc']","89d77a9f":"total_acc_avg = df.groupby('total_acc').mean()['mort_acc']","16472f89":"total_acc_avg[2.0]","b1e7f2d1":"def fill_mort_acc(total_acc,mort_acc):\n    '''\n    Accepts the total_acc and mort_acc values for the row.\n    Checks if the mort_acc is NaN , if so, it returns the avg mort_acc value\n    for the corresponding total_acc value for that row.\n    \n    total_acc_avg here should be a Series or dictionary containing the mapping of the\n    groupby averages of mort_acc per total_acc values.\n    '''\n    if np.isnan(mort_acc):\n        return total_acc_avg[total_acc]\n    else:\n        return mort_acc","763caaf3":"df['mort_acc'] = df.apply(lambda x: fill_mort_acc(x['total_acc'], x['mort_acc']), axis=1)","35127a9d":"df.isnull().sum()","12dd7718":"df = df.dropna()","48c352ba":"df.isnull().sum()","060d29cf":"df.select_dtypes(['object']).columns","8dc35dbc":"df['term'].value_counts()","45644fb0":"# Or just use .map()\ndf['term'] = df['term'].apply(lambda term: int(term[:3]))","e9c34801":"subgrade_dummies = pd.get_dummies(df['sub_grade'],drop_first=True)","1e095256":"df = pd.concat([df.drop('sub_grade',axis=1),subgrade_dummies],axis=1)","fc045b3f":"df.columns","12dc8cd7":"df.select_dtypes(['object']).columns","db740277":"dummies = pd.get_dummies(df[['verification_status', 'application_type','initial_list_status','purpose' ]],drop_first=True)\ndf = df.drop(['verification_status', 'application_type','initial_list_status','purpose'],axis=1)\ndf = pd.concat([df,dummies],axis=1)","bfa11bb4":"df['home_ownership'].value_counts()","47da8202":"df['home_ownership']=df['home_ownership'].replace(['NONE', 'ANY'], 'OTHER')\n\ndummies = pd.get_dummies(df['home_ownership'],drop_first=True)\ndf = df.drop('home_ownership',axis=1)\ndf = pd.concat([df,dummies],axis=1)","98f77b7f":"df['zip_code'] = df['address'].apply(lambda address:address[-5:])","59308b2c":"dummies = pd.get_dummies(df['zip_code'],drop_first=True)\ndf = df.drop(['zip_code','address'],axis=1)\ndf = pd.concat([df,dummies],axis=1)","0810df22":"df = df.drop('issue_d',axis=1)","12dd7c7d":"df['earliest_cr_year'] = df['earliest_cr_line'].apply(lambda date:int(date[-4:]))\ndf = df.drop('earliest_cr_line',axis=1)","1031559e":"df.select_dtypes(['object']).columns","fcf8d054":"from sklearn.model_selection import train_test_split","6cebd56b":"df = df.drop('loan_status',axis=1)","99a8d822":"df['grade']","1c77df9c":"dummies1 = pd.get_dummies(df['grade'],drop_first=True)\ndf = df.drop(['grade'],axis=1)\ndf = pd.concat([df,dummies1],axis=1)","7019c26f":"X = df.drop('loan_repaid',axis=1).values\ny = df['loan_repaid'].values","4ea04cf9":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=101)","9241bc79":"from sklearn.preprocessing import MinMaxScaler","b038fa86":"scaler = MinMaxScaler()","07c28efb":"X_train = scaler.fit_transform(X_train)","80953cbe":"X_test = scaler.transform(X_test)","d9105318":"import tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Activation,Dropout\nfrom tensorflow.keras.constraints import max_norm","8d05a0d4":"# CODE HERE\nmodel = Sequential()\n\n# Choose whatever number of layers\/neurons you want.\n","28630ebc":"model = Sequential()\n\n\n# input layer\nmodel.add(Dense(78,  activation='relu'))\nmodel.add(Dropout(0.2))\n\n# hidden layer\nmodel.add(Dense(39, activation='relu'))\nmodel.add(Dropout(0.2))\n\n# hidden layer\nmodel.add(Dense(19, activation='relu'))\nmodel.add(Dropout(0.2))\n\n# output layer\nmodel.add(Dense(units=1,activation='sigmoid'))\n\n# Compile model\nmodel.compile(loss='binary_crossentropy', optimizer='adam')","065c7ea7":"model.fit(x=X_train, \n          y=y_train, \n          epochs=25,\n          batch_size=256,\n          validation_data=(X_test, y_test), \n          )","343d6a13":"from tensorflow.keras.models import load_model","0aca1b37":"model.save('full_data_project_model.h5')  ","fd240f5e":"losses = pd.DataFrame(model.history.history)","3a9e7a2e":"losses[['loss','val_loss']].plot()","f7e6c3a9":"from sklearn.metrics import classification_report,confusion_matrix","c3b31b94":"predictions = model.predict_classes(X_test)","6493dd72":"print(classification_report(y_test,predictions))","52307be5":"confusion_matrix(y_test,predictions)","18985a0b":"import random\nrandom.seed(101)\nrandom_ind = random.randint(0,len(df))\n\nnew_customer = df.drop('loan_repaid',axis=1).iloc[random_ind]\nnew_customer","1dd1a9c7":"model.predict_classes(new_customer.values.reshape(1,84))","c04639ad":"df.iloc[random_ind]['loan_repaid']","ba1c9b54":"**Convert these columns: ['verification_status', 'application_type','initial_list_status','purpose'] into dummy variables and concatenate them with the original dataframe. Remember to set drop_first=True and to drop the original columns.**","0b4af0f3":"**Missing Data\nLet's explore this missing data columns. We use a variety of factors to decide whether or not they would be useful, to see if we should keep, discard, or fill in the missing data.**","3b46f60d":"## Train Test Split","10dd8671":"**drop the load_status column we created earlier, since its a duplicate of the loan_repaid column. We'll use the loan_repaid column since its already in 0s and 1s.**","151333df":"**Looks like the total_acc feature correlates with the mort_acc , this makes sense! Let's try this fillna() approach. We will group the dataframe by the total_acc and calculate the mean value for the mort_acc per total_acc entry.**","6eefd958":"Convert these to dummy variables, but replace NONE and ANY with OTHER, so that we end up with just 4 categories, MORTGAGE, RENT, OWN, OTHER. Then concatenate them with the original dataframe. Remember to set drop_first=True and to drop the original columns.","b2a7099c":"**There are many ways we could deal with this missing data. We could attempt to build a simple model to fill it in, such as a linear model, we could just fill it in based on the mean of the other columns, or you could even bin the columns into categories and then set NaN as its own category. There is no 100% correct approach! Let's review the other columns to see which most highly correlates to mort_acc**","c8f07d9d":"**Build a sequential model to will be trained on the data. You have unlimited options here, but here is what the solution uses: a model that goes 78 --> 39 --> 19--> 1 output neuron.**","1c56c91f":"** Loading Data and other imports**","b9d7744b":"**Let's feature engineer a zip code column from the address in the data set. Create a column called 'zip_code' that extracts the zip code from the address column.**","03723546":" **TASK: Now check, did this person actually end up paying back their loan?**","65015ae9":"**TASK: Charge off rates are extremely similar across all employment lengths. Go ahead and drop the emp_length column.**","6a21039d":"**TASK: The title column is simply a string subcategory\/description of the purpose column. Go ahead and drop the title column.**","913913f5":"## Normalizing the Data\n\n**Use a MinMaxScaler to normalize the feature data X_train and X_test. Recall we don't want data leakge from the test set so we only fit on the X_train data.**","941624f1":"**TASK: Given the customer below, would you offer this person a loan?**","9b77df22":"**Section 2: Data PreProcessing\nSection Goals: Remove or fill any missing data. Remove unnecessary or repetitive features. Convert categorical string features to dummy variables.**","5b6b4302":"**Categorical Variables and Dummy Variables\nWe're done working with the missing data! Now we just need to deal with the string values due to the categorical columns.**","ce6e9783":"### issue_d \n\n**This would be data leakage, we wouldn't know beforehand whether or not a loan would be issued when using our model, so in theory we wouldn't have an issue_date, drop this feature.**","e2a3b442":"df=df.drop('grade',axis=1)","7c9fe754":"# Creating the Model\n\n**Run the cell below to import the necessary Keras functions.**","af3061a3":"**revol_util and the pub_rec_bankruptcies have missing data points, but they account for less than 0.5% of the total data. Go ahead and remove the rows that are missing those values in those columns with dropna().**","2029f8bd":"**Section 1: Exploratory Data Analysis\nOVERALL GOAL: Get an understanding for which variables are important, view summary statistics, and visualize the data\n\n**","07d1e975":"### earliest_cr_line\n**This appears to be a historical time stamp feature. Extract the year from this feature using a .apply function, then convert it to a numeric feature. Set this new data to a feature column called 'earliest_cr_year'.Then drop the earliest_cr_line feature.**","a78213a1":"# Section 3: Evaluating Model Performance.\n\n**Plot out the validation loss versus the training loss.**","7f44dbc0":"**Convert the subgrade into dummy variables. Then concatenate these new columns to the original dataframe. Remember to drop the original subgrade column and to add drop_first=True to your get_dummies call.**"}}