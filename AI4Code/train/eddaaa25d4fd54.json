{"cell_type":{"881782e3":"code","313bcf8a":"code","b40a3ae1":"code","11561149":"code","2bc355ff":"code","70a77310":"code","3ecf3b6a":"code","ef195cf4":"code","0b0978dc":"code","db9ae337":"code","659897d3":"code","56b234ff":"code","0ee1c6ce":"code","bcd1c717":"code","974e4c36":"code","a105515b":"code","bb41b371":"code","3097a1da":"code","85d09a60":"code","624615c5":"code","f9e59ec0":"code","8476dbe2":"code","047dba2a":"code","72d2d75b":"code","1b05f71a":"code","d767285c":"code","84823839":"code","420040d9":"code","5b3fe704":"code","284fe3fe":"code","55094552":"code","982ed0e3":"code","d411a037":"code","2a1537db":"code","33242101":"code","b38c69f7":"code","4269374c":"code","814dc503":"code","eed74f2a":"markdown","a7c282be":"markdown","592b1b42":"markdown","af0a0594":"markdown","8df0e7bd":"markdown","2aea3dab":"markdown","b8e90c9a":"markdown","58dbfaa3":"markdown","ce6d850d":"markdown","92e46b76":"markdown","a1e9e9e6":"markdown"},"source":{"881782e3":"!pip uninstall -y typing\n!pip install  \"git+https:\/\/github.com\/dreamquark-ai\/tabnet.git@develop#egg=pytorch_tabnet\" --upgrade","313bcf8a":"import pandas as pd\nimport numpy as np\nimport time\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import log_loss\n#from sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom sklearn.preprocessing import LabelEncoder\n\n#from pytorch_tabnet.multitask import TabNetMultiTaskClassifier\nfrom pytorch_tabnet.tab_model import TabNetClassifier\nfrom pytorch_tabnet.pretraining import TabNetPretrainer\nimport torch\n\nimport optuna\nfrom optuna.visualization import plot_optimization_history, plot_param_importances\n\nfrom matplotlib import pyplot as plt\n%matplotlib inline","b40a3ae1":"train=pd.read_csv('..\/input\/tabular-playground-series-jun-2021\/train.csv')\ntest=pd.read_csv('..\/input\/tabular-playground-series-jun-2021\/test.csv')\nsub=pd.read_csv('..\/input\/tabular-playground-series-jun-2021\/sample_submission.csv')","11561149":"conditions = [\n    (train.target == \"Class_1\"), (train.target == \"Class_2\"), (train.target == \"Class_3\"),\n    (train.target == \"Class_4\"), (train.target == \"Class_5\"), (train.target == \"Class_6\"),\n    (train.target == \"Class_7\"), (train.target == \"Class_8\"), (train.target == \"Class_9\")\n]\nchoices = [0, 1, 2, 3, 4, 5, 6, 7, 8]\ntrain[\"target\"] = np.select(conditions, choices)","2bc355ff":"full = pd.concat([train, test], axis=0)\nfull.iloc[:,1:] = full.iloc[:,1:].applymap(str)","70a77310":"nunique = full.nunique()\ntypes = full.dtypes\n\ncategorical_columns = []\ncategorical_dims =  {}\nfor col in full.drop(['id', 'target'], axis=1).columns:\n    if types[col] == 'object' or nunique[col] < 200:\n        print(col, full[col].nunique())\n        l_enc = LabelEncoder()\n        full[col] = full[col].fillna(\"X\")\n        full[col] = l_enc.fit_transform(full[col].values)\n        categorical_columns.append(col)\n        categorical_dims[col] = len(l_enc.classes_)","3ecf3b6a":"unused_feat = ['Set', 'id']\n\nfeatures = [ col for col in full.columns if col not in unused_feat+['target']] \n\ncat_idxs = [ i for i, f in enumerate(features) if f in categorical_columns]\ncat_dims = [ categorical_dims[f] for i, f in enumerate(features) if f in categorical_columns]\ncat_emb_dims = np.ceil(np.log(cat_dims)).astype(np.int).tolist()","ef195cf4":"X = full[features].values[full.id.isin(train.id)]\ny = full['target'].values[full.id.isin(train.id)]\n\nX_test = full[features].values[full.id.isin(test.id)]\ny_test = full['target'].values[full.id.isin(test.id)]","0b0978dc":"oof_lightautoml=pd.read_csv('..\/input\/tps-jun2021-lightautoml\/oof_lightautoml.csv')\nsub_lightautoml=pd.read_csv('..\/input\/tps-jun2021-lightautoml\/sub_lightautoml.csv')\n\noof_lightautoml = oof_lightautoml.drop('id', axis=1)\noof_lightautoml.columns = ['pred_lightautoml' + str(i) for i in range(1, 10)]\n\nsub_lightautoml = sub_lightautoml.drop('id', axis=1)\nsub_lightautoml.columns = ['pred_lightautoml' + str(i) for i in range(1, 10)]\n\nX = pd.concat([pd.DataFrame(X), oof_lightautoml], axis=1).values\nX_test = pd.concat([pd.DataFrame(X_test), sub_lightautoml], axis=1).values","db9ae337":"# TabNetPretrainer\nunsupervised_model = TabNetPretrainer(\n    n_d=64, n_a=64,\n    n_steps=3,\n    n_independent=1,\n    n_shared=1,\n    cat_idxs=cat_idxs,\n    cat_dims=cat_dims,\n    cat_emb_dim=cat_emb_dims,\n    gamma=1.2,\n    lambda_sparse=0.,\n    optimizer_fn=torch.optim.Adam,\n    optimizer_params=dict(lr=2e-2),\n    mask_type='sparsemax', \n    scheduler_params=dict(mode=\"min\",\n                          patience=3,\n                          min_lr=1e-5,\n                          factor=0.5,),\n    scheduler_fn=torch.optim.lr_scheduler.ReduceLROnPlateau,\n    verbose=1\n)","659897d3":"unsupervised_model.fit(\n    X_train=X_test,\n    eval_set=[X],\n    max_epochs=30 , \n    patience=25,\n    batch_size=256,\n    virtual_batch_size=256,\n    num_workers=1,\n    drop_last=True,\n    pretraining_ratio=0.5\n\n)","56b234ff":"# Make reconstruction from a dataset\nreconstructed_X, embedded_X = unsupervised_model.predict(X)\nassert(reconstructed_X.shape==embedded_X.shape)","0ee1c6ce":"#unsupervised_explain_matrix, unsupervised_masks = unsupervised_model.explain(X)","bcd1c717":"#fig, axs = plt.subplots(1, 3, figsize=(20,20))\n#\n#for i in range(3):\n#    axs[i].imshow(unsupervised_masks[i][:50])\n#    axs[i].set_title(f\"mask {i}\")","974e4c36":"## Save and load\nunsupervised_model.save_model('.\/test_pretrain')\nloaded_pretrain = TabNetPretrainer()\nloaded_pretrain.load_model('.\/test_pretrain.zip')","a105515b":"N_SPLITS=5\n\nskf = StratifiedKFold(n_splits=N_SPLITS, random_state=2021, shuffle=True)\ntab_pred = 0\ntab_oof = np.zeros((X.shape[0], 9))\n\nfor fold, (train_index, valid_index) in enumerate(skf.split(X, y)):\n    print(f\"\u279c FOLD :{fold}\")\n    X_train, X_valid = X[train_index], X[valid_index]\n    y_train, y_valid = y[train_index], y[valid_index]\n    \n    start = time.time()\n    \n    clf = TabNetClassifier(n_d=64,\n                           n_a=64,\n                           n_steps=3,\n                           gamma=1.2,\n                           n_independent=1,\n                           n_shared=1,\n                           lambda_sparse=1e-5,\n                           seed=0,\n                           clip_value=2,\n                           cat_idxs=cat_idxs,\n                           cat_dims=cat_dims,\n                           cat_emb_dim=cat_emb_dims,\n                           optimizer_fn=torch.optim.Adam,\n                           optimizer_params=dict(lr=1e-1, weight_decay=1e-5),\n                           scheduler_params=dict(mode='min',\n                                                        factor=0.5,\n                                                        patience=3,\n                                                        is_batch_level=False,),\n                           scheduler_fn=torch.optim.lr_scheduler.ReduceLROnPlateau,\n                           mask_type='sparsemax',\n                           verbose=1\n                          )\n\n    clf.fit(\n        X_train=X_train, y_train=y_train,\n        eval_set=[(X_train, y_train), (X_valid, y_valid)],\n        eval_name=['train', 'valid'],\n        eval_metric=['logloss'],\n        max_epochs=100 ,\n        batch_size=2048, \n        virtual_batch_size=256,\n        num_workers=0,\n        drop_last=True,\n        pin_memory=True,\n        patience=10,\n        from_unsupervised=loaded_pretrain\n    )\n    \n    tab_oof[valid_index,:] = clf.predict_proba(X_valid)\n    tab_pred += clf.predict_proba(X_test)\/N_SPLITS\n    \n    tab_logloss = log_loss(y_valid, tab_oof[valid_index])\n    print(f\"score: {tab_logloss:.6f} \")\n    print(f\"elapsed: {time.time()-start:.2f} sec\\n\")\n    \n    del clf\n    \ntab_logloss = log_loss(y, tab_oof)\nprint(f\"Final logloss score: {tab_logloss} \u2714\ufe0f \")","bb41b371":"sub.iloc[:, 1:] = tab_pred\nsub.to_csv(\"sub_tab_default.csv\", index=False)","3097a1da":"oof_tab = pd.concat([train.id,\n                     pd.DataFrame(tab_oof, \n                                  columns=[\"Class_1\", \"Class_2\", \"Class_3\",\n                                           \"Class_4\", \"Class_5\", \"Class_6\",\n                                           \"Class_7\", \"Class_8\", \"Class_9\"])],\n                    axis=1)\noof_tab.to_csv(\"oof_tab_optuned.csv\", index=False)","85d09a60":"#X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.20, random_state=314, stratify=y)\n#\n#X_train, y_train = X_train.values, y_train.values\n#X_val, y_val = X_val.values, y_val.values\n#\n#scaler = MinMaxScaler()\n#scaler.fit(X_train)\n#\n#X_train = scaler.transform(X_train)\n#X_val = scaler.transform(X_val)\n#X_test = scaler.transform(X_test)","624615c5":"## TabNetPretrainer\n#unsupervised_model = TabNetPretrainer(\n#    optimizer_fn=torch.optim.Adam,\n#    optimizer_params=dict(lr=2e-2),\n#    mask_type='sparsemax'\n#)\n#\n#unsupervised_model.fit(\n#    X_train=X_train,\n#    eval_set=[X_val],\n#    pretraining_ratio=0.5,\n#)","f9e59ec0":"#model = TabNetClassifier(verbose = 1)\n#\n#model.fit(\n#    X_train=X_train, y_train=y_train,\n#    eval_set=[(X_train, y_train), (X_val, y_val)],\n#    eval_name=['train', 'val'],\n#    eval_metric=['logloss'],\n#    max_epochs=30, \n#    patience=15,\n#    from_unsupervised=unsupervised_model\n#)","8476dbe2":"#fig, axs = plt.subplots(nrows=1, ncols=3, figsize=(12, 5))\n#\n## plot losses\n#axs[0].plot(model.history['loss'])\n#\n## plot logloss\n#axs[1].plot(model.history['train_logloss'])\n#axs[1].plot(model.history['val_logloss'])\n#\n## plot learning rates\n#axs[2].plot(model.history['lr'])\n#\n#fig.tight_layout()\n#plt.show()","047dba2a":"#val_preds = model.predict_proba(X_val)\n#val_logloss = [log_loss(y_pred=task_pred, y_true=y_val[:,1])\n#             for task_idx, task_pred in enumerate(val_preds)]\n#\n#np.mean(val_logloss)","72d2d75b":"#task_preds = model.predict_proba(X_test)\n#tab_pred = np.mean(task_preds, axis=0)","1b05f71a":"#sub.iloc[:, 1:] = tab_pred\n#sub.to_csv(\"sub_tab_default.csv\", index=False)","d767285c":"#def objective(trial):\n#    \n#    global X, y, X_test\n#    \n#    hyperparams = {\n#        'n_a_d': trial.suggest_categorical('n_a_d', [8, 16, 24, 32, 64, 128]),\n#        'n_steps': trial.suggest_int('n_steps', 3, 10, 1),\n#        'gamma': trial.suggest_categorical('gamma', [1.0, 1.2, 1.5, 2.0]),\n#        'lambda': trial.suggest_categorical('lambda', [0.1, 0.01, 0.001, 0.0001, 0.00001, 0.000001, 0]),\n#        'batch_size': trial.suggest_categorical('batch_size', [1024, 2048, 4096, 8192, 16384, 32768]),\n#        'virtual_batch_size': trial.suggest_categorical('virtual_batch_size', [128, 256, 512, 1024]),\n#        'lr': trial.suggest_categorical('lr', [0.005, 0.01, 0.02, 0.025]),\n#        'gamma_decay': trial.suggest_categorical('gamma_decay', [0.4, 0.8, 0.9, 0.95]),\n#        #'mask_type': trial.suggest_categorical('mask_type', ['entmax', 'sparsemax']),\n#        'batch_momentum': trial.suggest_categorical('batch_momentum', [0.6, 0.7, 0.8, 0.9, 0.95, 0.98]),\n#    }\n#    \n#    model = TabNetMultiTaskClassifier(\n#        n_d=hyperparams['n_a_d'],\n#        n_a=hyperparams['n_a_d'],\n#        gamma=hyperparams['gamma'],\n#        optimizer_fn=torch.optim.Adam,\n#        optimizer_params={'lr':hyperparams['lr']},\n#        scheduler_params={\"step_size\":hyperparams['n_steps'],\n#                          \"gamma\":hyperparams['gamma_decay']},\n#        scheduler_fn=torch.optim.lr_scheduler.StepLR,\n#        mask_type='entmax',\n#        lambda_sparse=hyperparams['lambda'],\n#        momentum=hyperparams['batch_momentum'],\n#        verbose = 0\n#    )\n#\n#    model.fit(\n#        X_train=X_train, y_train=y_train,\n#        eval_set=[(X_train, y_train), (X_val, y_val)],\n#        eval_name=['train', 'val'],\n#        max_epochs=MAX_EPOCHS, \n#        patience=PATIENCE,\n#        batch_size=hyperparams['batch_size'],\n#        virtual_batch_size=hyperparams['virtual_batch_size'],\n#        num_workers=0,\n#        drop_last=False\n#    )\n#\n#    val_preds = model.predict_proba(X_val)\n#    val_logloss = [log_loss(y_pred=task_pred, y_true=y_val[:,task_idx])\n#                 for task_idx, task_pred in enumerate(val_preds)]\n#    \n#    del model\n#\n#    return np.mean(val_logloss)","84823839":"#study = optuna.create_study(direction='minimize',\n#                            sampler=optuna.samplers.TPESampler(multivariate=True, seed=123))\n#\n#study.optimize(objective, \n#               timeout=60*60*6, \n#               #n_trials=2, \n#               gc_after_trial=False)","420040d9":"#study.best_value","5b3fe704":"#plot_optimization_history(study)","284fe3fe":"#optuna.visualization.plot_parallel_coordinate(study)","55094552":"#plot_param_importances(study)","982ed0e3":"#study.best_params","d411a037":"#tab_oof = np.zeros((X.shape[0], 9))\n#tab_pred = 0\n#\n#for fold, (train_idx, val_idx) in enumerate(kf.split(X=X, y=y)):\n#    print(f\"\u279c FOLD :{fold}\")\n#    X_train = X.values[train_idx]\n#    y_train = y.values[train_idx]\n#    X_val = X.values[val_idx]\n#    y_val = y.values[val_idx]\n#    \n#    y_train = y_train.reshape(-1, 1)\n#    y_train = np.hstack([y_train]*NB_TASKS)\n#\n#    y_val = y_val.reshape(-1, 1)\n#    y_val = np.hstack([y_val]*NB_TASKS)\n#\n#    scaler = StandardScaler()\n#    scaler.fit(X_train)\n#    \n#    X_train = scaler.transform(X_train)\n#    X_val = scaler.transform(X_val)\n#    X_test = scaler.transform(X_test)\n#    \n#    \n#    start = time.time()\n#    \n#    model = TabNetMultiTaskClassifier(\n#        n_d=study.best_params['n_a_d'],\n#        n_a=study.best_params['n_a_d'],\n#        optimizer_fn=torch.optim.Adam,\n#        optimizer_params=dict(lr=0.2),\n#        scheduler_params={\"step_size\":study.best_params['n_steps'],\n#                          \"gamma\":study.best_params['gamma']},\n#        scheduler_fn=torch.optim.lr_scheduler.StepLR,\n#        mask_type='entmax',\n#        lambda_sparse=study.best_params['lambda'],\n#        momentum=study.best_params['batch_momentum'],\n#        verbose = 1\n#    )\n#    \n#    model.fit(\n#        X_train=X_train, y_train=y_train,\n#        eval_set=[(X_train, y_train), (X_val, y_val)],\n#        eval_name=['train', 'val'],\n#        max_epochs=MAX_EPOCHS, \n#        patience=PATIENCE,\n#        batch_size=BATCH_SIZE, \n#        virtual_batch_size=VIRTUAL_BATCH_SIZE,\n#        num_workers=0,\n#        drop_last=False\n#    )\n#    \n#    val_preds = model.predict_proba(X_val)\n#    val_logloss = [log_loss(y_pred=task_pred, y_true=y_val[:,task_idx])\n#                 for task_idx, task_pred in enumerate(val_preds)]\n#    \n#    tab_oof[val_idx,:] = np.mean(val_preds, axis=0)\n#    \n#    task_pred = model.predict_proba(X_test)\n#    tab_pred += np.mean(task_pred, axis=0) \/ K\n#    \n#    print(f\"score: {np.mean(val_logloss):.6f} \")\n#    print(f\"elapsed: {time.time()-start:.2f} sec\\n\")\n#    \n#    del model\n#\n#tab_logloss = log_loss(y, tab_oof)\n#print(f\"Final logloss score: {tab_logloss} \u2714\ufe0f \")","2a1537db":"#PATIENCE = 15","33242101":"#final_model = TabNetMultiTaskClassifier(\n#    n_d=study.best_params['n_a_d'],\n#    n_a=study.best_params['n_a_d'],\n#    gamma=study.best_params['gamma'],\n#    optimizer_fn=torch.optim.Adam,\n#    optimizer_params=dict(lr=study.best_params['lr']),\n#    scheduler_params={\"step_size\":study.best_params['n_steps'],\n#                      \"gamma\":study.best_params['gamma_decay']},\n#    scheduler_fn=torch.optim.lr_scheduler.StepLR,\n#    mask_type='entmax',\n#    lambda_sparse=study.best_params['lambda'],\n#    momentum=study.best_params['batch_momentum'],\n#    verbose = 1\n#)\n#\n#final_model.fit(\n#    X_train=X_train, y_train=y_train,\n#    eval_set=[(X_train, y_train), (X_val, y_val)],\n#    eval_name=['train', 'val'],\n#    max_epochs=MAX_EPOCHS, \n#    patience=PATIENCE,\n#    batch_size=study.best_params['batch_size'], \n#    virtual_batch_size=study.best_params['virtual_batch_size']\n#)\n#\n#task_pred = final_model.predict_proba(X_test)\n#tab_pred = np.mean(task_pred, axis=0)","b38c69f7":"#fig, axs = plt.subplots(nrows=1, ncols=3, figsize=(12, 5))\n#\n## plot losses\n#axs[0].plot(final_model.history['loss'])\n#\n## plot logloss\n#axs[1].plot(final_model.history['train_logloss'])\n#axs[1].plot(final_model.history['val_logloss'])\n#\n## plot learning rates\n#axs[2].plot(final_model.history['lr'])\n#\n#fig.tight_layout()\n#plt.show()","4269374c":"#sub.iloc[:, 1:] = tab_pred\n#sub.to_csv(\"sub_tab_optuned.csv\", index=False)","814dc503":"#oof_tab = pd.concat([train.id,\n#                     pd.DataFrame(tab_oof, \n#                              columns=[\"Class_1\", \"Class_2\", \"Class_3\",\n#                                       \"Class_4\", \"Class_5\", \"Class_6\",\n#                                       \"Class_7\", \"Class_8\", \"Class_9\"])],\n#                    axis=1)\n#oof_tab.to_csv(\"oof_tab_optuned.csv\", index=False)","eed74f2a":"# Self Supervised Training","a7c282be":"# Simple preprocessing","592b1b42":"# Define cat features for embeddings","af0a0594":"# Draft","8df0e7bd":"# Sub","2aea3dab":" # Load Dependencies","b8e90c9a":"# Sources\n\n- https:\/\/arxiv.org\/pdf\/1908.07442.pdf\n- https:\/\/reposhub.com\/python\/deep-learning\/dreamquark-ai-tabnet.html\n- https:\/\/towardsdatascience.com\/modelling-tabular-data-with-googles-tabnet-ba7315897bfb\n- https:\/\/github.com\/google-research\/google-research\/blob\/master\/tabnet\/tabnet_model.py\n- https:\/\/github.com\/hussius\/tabnet_fork\/blob\/master\/opt_tabnet.py\n- https:\/\/www.kaggle.com\/optimo\/tabnetbaseline\/","58dbfaa3":"see: https:\/\/www.kaggle.com\/gomes555\/tps-jun2021-lightautoml","ce6d850d":"# Baseline","92e46b76":"# Optuna","a1e9e9e6":"# Final Model"}}