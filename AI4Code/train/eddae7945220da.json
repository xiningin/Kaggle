{"cell_type":{"6832ffd7":"code","6b036633":"code","af2a2bb9":"code","b62c052b":"code","533f6d27":"code","10984031":"code","f74b885e":"code","8ed05752":"code","c45da161":"code","f58ea0aa":"code","b368a7f7":"code","0b0d4ddb":"code","d03245d6":"code","38a7cb8e":"code","28e0b59d":"code","6d3a37df":"code","462c0718":"code","4d5e465a":"code","f35ba052":"code","ddf770d1":"code","0ff0d1a2":"code","713ab46f":"code","8185d3b4":"code","82c0e188":"code","6d44b01f":"code","1e203c07":"code","f59818b4":"code","d5229be9":"code","d09bd1bf":"code","ce091eba":"code","eb2b753d":"code","efb40da0":"code","deb61638":"code","d091226a":"code","3a2b6297":"code","7313e2a5":"code","b0f4dae8":"code","1fa7d0cb":"code","a1b16d78":"code","bdac20ef":"code","75c91de1":"code","b61827a5":"code","55ea7d9c":"code","bc2fbe8a":"code","d433a061":"code","cd0f303d":"code","73f831eb":"code","1f261257":"code","963d130b":"code","7762c83a":"code","5c4ff6e6":"code","d0426a0b":"code","53ca4df3":"code","4d00db9e":"code","95ae5363":"code","c1ce1fc1":"code","afdc2be1":"code","efaa67d8":"code","339c4725":"code","1f226935":"code","34be934e":"code","8a7842f7":"code","0f68c431":"code","1b71caa6":"code","bfdc4aec":"code","5fabb331":"code","0631c4f2":"code","18f1e023":"code","0bf61618":"code","cee8b452":"code","c83f0e9e":"markdown","95b98870":"markdown","a3fa1472":"markdown","9bda1076":"markdown","c5ef6c1e":"markdown","30992507":"markdown","e7730b47":"markdown","647e0d47":"markdown","34f480c2":"markdown","b0d11b29":"markdown","e62210ec":"markdown","4f3c06c2":"markdown","19a8cb0e":"markdown","6bb669e4":"markdown","8f372a86":"markdown","61b41692":"markdown","deab21c6":"markdown","d1059eb4":"markdown","9cd5ed52":"markdown","33495f4d":"markdown","dab14277":"markdown","477dc4fc":"markdown","ba427123":"markdown","834d8e2a":"markdown","0af1f3ed":"markdown","a74c71ae":"markdown","a9a2f728":"markdown","d139b943":"markdown","f3853278":"markdown","a34aef85":"markdown","3e054be5":"markdown","61664950":"markdown","9f515aa1":"markdown","757d5c17":"markdown","5f7fd129":"markdown","e746ac7c":"markdown","65c638ed":"markdown","44022db8":"markdown","1fad29c4":"markdown","3d29a097":"markdown","ddf02029":"markdown","02ce7af9":"markdown"},"source":{"6832ffd7":"import numpy as np \nimport pandas as pd\nimport seaborn as sns\n\nimport matplotlib.pyplot as plt\nimport plotly.figure_factory as ff\nimport plotly.express as px\n\n%matplotlib inline","6b036633":"from sklearn.metrics import log_loss\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import roc_auc_score, roc_curve, auc\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom mlxtend.preprocessing import minmax_scaling\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler\n\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.model_selection import StratifiedKFold, KFold, LeaveOneGroupOut\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.feature_selection import mutual_info_classif","af2a2bb9":"from lightgbm import LGBMClassifier\n\nfrom sklearn.experimental import enable_hist_gradient_boosting\nfrom sklearn.ensemble import HistGradientBoostingClassifier","b62c052b":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","533f6d27":"def roc_auc(true_list, pred_list, a, b):\n    \n    fpr, tpr, _ = roc_curve(true_list, pred_list)    \n    roc_auc = auc(fpr, tpr)\n\n    print(f'FPR: {fpr}')\n    print(f'TPR: {tpr}')\n    #print(f'{list(zip(fpr,tpr))}') \n    print(f'\\n>>>>> ROC_AUC: %0.6f <<<<<' %roc_auc)\n    \n    plt.style.use('seaborn-whitegrid')\n    plt.figure(figsize=(a, b), facecolor='lightgray')\n    plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve')\n    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n    plt.xlim([-0.01, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title(f'\\nThe area under the ROC curve\\n')\n    plt.legend(loc=\"lower right\")\n    plt.show()","10984031":"true_list  = np.array([1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0])\n\npred_list1 = np.array([0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5])\n\n# The results are quite similar:\n# pred_list1 = np.array([1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0])\n# pred_list1 = np.array([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0])\n# pred_list1 = np.array([0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8])\n# pred_list1 = np.array([0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3])\n# pred_list1 = np.array([500, 500, 500, 500, 500, 500, 500, 500, 500, 500])\n# pred_list1 = np.array([-50, -50, -50, -50, -50, -50, -50, -50, -50, -50])\n# pred_list1 = np.array([500, -50, 500, -50, 500, -50, 500, -50, 500, 500])\n# This means that this type of evaluation does not have the ability to detect some errors.\n\nroc_auc(true_list, pred_list1, 7, 7)","f74b885e":"true_list  = np.array([1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0])\n\npred_list2 = np.array([1.0, 1.0, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5])\n\n# The results are quite similar:\n# pred_list2 = np.array([0.6, 0.8, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5])\n# pred_list2 = np.array([0.8, 0.6, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5])\n# pred_list2 = np.array([120, 850, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5]) \n# pred_list2 = np.array([850, 120, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5]) \n# This type of evaluation does not have the ability to detect changes in some rankings.\n\nroc_auc(true_list, pred_list2, 7, 7)","8ed05752":"true_list  = np.array([1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0])\n\npred_list3 = np.array([0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.0, 0.0])\n\n# The results are quite similar:\n# pred_list3 = np.array([0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.2, 0.4])\n# pred_list3 = np.array([0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.4, 0.2])\n# pred_list3 = np.array([0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, -25, -85])\n# pred_list3 = np.array([0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, -85, -25])\n# This type of evaluation does not have the ability to detect changes in some rankings.\n\nroc_auc(true_list, pred_list3, 7, 7)","c45da161":"true_list  = np.array([1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0])\n\npred_list4 = np.array([0.0, 0.0, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5])\n\n# The results are quite similar:\n# pred_list4 = np.array([0.2, 0.4, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5])\n# pred_list4 = np.array([0.4, 0.2, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5])\n# pred_list4 = np.array([-35, -65, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5]) \n# pred_list4 = np.array([-65, -35, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5]) \n# This type of evaluation does not have the ability to detect changes in some rankings.\n\nroc_auc(true_list, pred_list4, 7, 7)","f58ea0aa":"true_list  = np.array([1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0])\n\npred_list5 = np.array([0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 1.0, 1.0])\n\n# The results are quite similar:\n# pred_list5 = np.array([0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.6, 0.8])\n# pred_list5 = np.array([0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.8, 0.6])\n# pred_list5 = np.array([0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 150, 670])\n# pred_list5 = np.array([0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 670, 150])\n# This type of evaluation does not have the ability to detect changes in some rankings.\n\nroc_auc(true_list, pred_list5, 7, 7)","b368a7f7":"import datatable as dt \n\nDF1 = dt.fread('..\/input\/tabular-playground-series-oct-2021\/train.csv').to_pandas()\n\nDF2 = dt.fread('..\/input\/tabular-playground-series-oct-2021\/test.csv').to_pandas()\n\nSAM = dt.fread('..\/input\/tabular-playground-series-oct-2021\/sample_submission.csv').to_pandas()\n\ndisplay(DF1.shape, DF2.shape, SAM.shape)","0b0d4ddb":"# Check Null Values\nMV1 = DF1.isnull().sum()\nMV2 = DF2.isnull().sum()\n\nprint(f'Missing Value DF1:\\n{MV1[MV1 > 0]}\\n')\nprint(f'Missing Value DF2:\\n{MV2[MV2 > 0]}\\n')","d03245d6":"display(DF1, DF2)\n\n# display(DF1.describe().transpose())\n# display(DF2.describe().transpose())","38a7cb8e":"print('=' * 40)\nDF1.info(memory_usage='deep')\nprint('=' * 40)\nDF2.info(memory_usage='deep')\nprint('=' * 40)","28e0b59d":"DF1['target'].value_counts().plot(figsize=(4, 4), kind='bar')","6d3a37df":"DF1['target'].value_counts().plot(figsize=(6, 6), kind='pie')\n\nDF1['target'].value_counts(normalize=True)","462c0718":"def reduce_memory_usage(df, verbose=True):\n    numerics = [\"int8\", \"int16\", \"int32\", \"int64\", \"float16\", \"float32\", \"float64\"]\n    start_mem = df.memory_usage().sum() \/ 1024 ** 2\n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == \"int\":\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)\n            else:\n                if (\n                    c_min > np.finfo(np.float16).min\n                    and c_max < np.finfo(np.float16).max\n                ):\n                    df[col] = df[col].astype(np.float16)\n                elif (\n                    c_min > np.finfo(np.float32).min\n                    and c_max < np.finfo(np.float32).max\n                ):\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n    end_mem = df.memory_usage().sum() \/ 1024 ** 2\n    if verbose:\n        print(\n            \"Mem. usage decreased to {:.2f} Mb ({:.1f}% reduction)\".format(\n                end_mem, 100 * (start_mem - end_mem) \/ start_mem\n            )\n        )\n    return df","4d5e465a":"DF1 = reduce_memory_usage(DF1)\n\nDF2 = reduce_memory_usage(DF2)","f35ba052":"X  = DF1.drop(columns = ['id','target'])\n\nXX = DF2.drop(columns = ['id'])\n\ny  = DF1.target\n\ndisplay(X, XX, y)","ddf770d1":"import gc\ndel DF1\ndel DF2\ngc.collect()","0ff0d1a2":"features = X.columns\n\nprint(features)\nprint(f'\\n>>>>> Number: {len(features)} <<<<<') ","713ab46f":"categ_featu = [col for col in X.columns if X[col].dtype == 'bool']\n\nnumer_featu = [col for col in X.columns if X[col].dtype != 'bool']\n\nprint(categ_featu)\nprint(f'\\n>>>>> Number: {len(categ_featu)} <<<<<\\n')\n\nprint(numer_featu)\nprint(f'\\n>>>>> Number: {len(numer_featu)} <<<<<\\n')","8185d3b4":"X['categ_sum'] = X[categ_featu].sum(axis=1)\n\nXX['categ_sum'] = XX[categ_featu].sum(axis=1)\n                                        \nnumer_featu += ['categ_sum'] \n\nprint(numer_featu)\nprint(f'\\n>>>>> Number: {len(numer_featu)} <<<<<\\n')","82c0e188":"hist_data = [XX['categ_sum'], X['categ_sum']]  \ngroup_labels = ['XX_categ_sum', 'X_categ_sum']\n    \nfig = ff.create_distplot(hist_data, group_labels, bin_size=.2, show_hist=False, show_rug=False) \nfig.show()","6d44b01f":"X[numer_featu].dtypes","1e203c07":"X[categ_featu].dtypes","f59818b4":"# for c in categ_featu:    \n#    X[c] = X[c].astype('category')  \n#    XX[c] = XX[c].astype('category') \n\n# X[categ_featu].dtypes","d5229be9":"categorical_mask = ([False] * 22  +\n                    [True]  * 1   +\n                    [False] * 20  +\n                    [True]  * 1   +\n                    [False] * 198 +\n                    [True]  * 43  +\n                    [False] * 1   )\n\nprint(categorical_mask)\nprint(f'\\n>>>>> Number: {len(categorical_mask)} <<<<<\\n')","d09bd1bf":"# display(X, XX)","ce091eba":"model = LGBMClassifier(n_estimators= 15000, \n                       learning_rate= 0.008, \n                       objective= 'binary',                      \n                       metric= 'auc',                       \n                       reg_alpha= 10,\n                       reg_lambda= 0.1,                     \n                       num_leaves= 31,\n                       max_depth= -1,\n                       subsample= 0.6,\n                       subsample_freq= 1, \n                       colsample_bytree= 0.4,\n                       min_child_weight= 256,\n                       min_child_samples= 20, \n                       random_state= 123)","eb2b753d":"N = 0  # Counter\nF = 10 # Number of Splits\npred = np.zeros(XX.shape[0])\npred_select = np.zeros(XX.shape[0])\n\nskf = StratifiedKFold(n_splits= F, shuffle= True, random_state= 123)","efb40da0":"for fold, (train_idx, valid_idx) in enumerate(skf.split(X, y)):\n    \n    print(f':::::::::::::: f o l d >>> {fold} :::::::::::::::\\n')\n    \n    train_X, train_y = X.iloc[train_idx], y.iloc[train_idx]\n    valid_X, valid_y = X.iloc[valid_idx], y.iloc[valid_idx]\n    \n    model.fit(train_X, train_y,\n              eval_metric= 'auc',\n              eval_set= [(valid_X, valid_y)], \n              early_stopping_rounds= 200,\n              verbose= 500)\n    \n    oof_predict = model.predict(valid_X)\n    accuracy = accuracy_score(valid_y, oof_predict)   \n    print(f'\\nPrediction: {oof_predict}\\n')\n    print(f'Accuracy: {accuracy}\\n')\n    roc_auc(valid_y, oof_predict, 7, 7)\n   \n    \n    print(f':::::::::: Main Results - fold {fold} ::::::::::\\n')\n    oof_pred = model.predict_proba(valid_X)[:, -1]\n    print(f'Prediction: {oof_pred}\\n')\n    roc_auc(valid_y, oof_pred, 7, 7)\n    \n    # hist_data = [oof_pred, oof_predict, valid_y]  \n    # group_labels = ['Predict_proba', 'Predict', 'Correct Value']  \n    # fig = ff.create_distplot(hist_data, group_labels, bin_size=.2, show_hist=False, show_rug=False) \n    # fig.show()                                             \n\n    pred += model.predict_proba(XX)[:, -1]\n    \n    if (accuracy > 0.7696):  \n        pred_select += model.predict_proba(XX)[:, -1]\n        N = N + 1\n        \n    if (fold == 0):\n        train_X_test = train_X.copy()\n        np.save('train_X_test.npy', train_X_test)\n        \n        train_y_test = train_y.copy()\n        np.save('train_y_test.npy', train_y_test)\n        \n        valid_X_test = valid_X.copy()\n        np.save('valid_X_test.npy', valid_X_test)\n        \n        valid_y_test = valid_y.copy()\n        np.save('valid_y_test.npy', valid_y_test)\n        \n        oof_pred_test = oof_pred.copy()\n        np.save('oof_pred_test.npy', oof_pred_test)\n        \n        oof_predict_test = oof_predict.copy()\n        np.save('oof_predict_test.npy', oof_predict_test)\n        \n                                   \nprint(f'::::::::::::::: F I N A L  R E S U L T ::::::::::::::::\\n')\n\npred = pred \/ F\npred_select = pred_select \/ N\n\nprint(f'Number of Total Folds: {F}')\nprint(f'Number of Selected Folds: {N}')\nprint('=' * 70, '\\n')\n\nprint('Based on Total Folds:')\nprint(f'Min: {pred.min()}, Max: {pred.max()}\\n')\npred = np.clip(pred, y.min(), y.max())\nnp.save('pred.npy', pred)\nprint(pred, pred.shape)\nprint('=' * 70, '\\n')\n\nprint('Based on Selected Folds:')\nprint(f'Min: {pred_select.min()}, Max: {pred_select.max()}\\n')\npred_select = np.clip(pred_select, y.min(), y.max())\nnp.save('pred_select.npy', pred_select)\nprint(pred_select, pred_select.shape)\nprint('=' * 70, '\\n')\n\nhist_data = [pred_select, pred]  \ngroup_labels = ['Based on Selected Folds', 'Based on Total Folds']    \nfig = ff.create_distplot(hist_data, group_labels, bin_size=.2, show_hist=False, show_rug=False) \nfig.show() \n\ngc.collect()  ","deb61638":"sub1 = SAM.copy()\n\nsub1['target'] = pred\ndisplay(sub1)","d091226a":"sub2_select = SAM.copy()\n\nsub2_select['target'] = pred_select\ndisplay(sub2_select)","3a2b6297":"sub1.to_csv(\"submission1.csv\",index=False)\nsub2_select.to_csv(\"submission2_select.csv\",index=False)\n!ls","7313e2a5":"modelt= LGBMClassifier(n_estimators= 8000, \n                       learning_rate= 0.008, \n                       objective= 'binary',                      \n                       metric= 'auc',                       \n                       reg_alpha= 10,\n                       reg_lambda= 0.1,                     \n                       num_leaves= 31,\n                       max_depth= -1,\n                       subsample= 0.6,\n                       subsample_freq= 1, \n                       colsample_bytree= 0.4,\n                       min_child_weight= 256,\n                       min_child_samples= 20, \n                       random_state= 123)","b0f4dae8":"modelt.fit(X, y, eval_metric= 'auc', verbose= 200)","1fa7d0cb":"predt = modelt.predict_proba(XX)[:, -1]\n\nprint(f'Min: {predt.min()}, Max: {predt.max()}\\n')  \npredt = np.clip(predt, y.min(), y.max())\nprint(predt, predt.shape)","a1b16d78":"hist_data = [predt]  \ngroup_labels = ['Predicted (For all Data)']\n    \nfig = ff.create_distplot(hist_data, group_labels, bin_size=.2, show_hist=False, show_rug=False) \nfig.show()","bdac20ef":"sub3_all_data = SAM.copy()\n\nsub3_all_data['target'] = predt\ndisplay(sub3_all_data)","75c91de1":"sub3_all_data.to_csv(\"submission3_all_data.csv\",index=False)\n!ls","b61827a5":"pred_ense_a = (pred * 0.80) + (predt * 0.20)\n\nprint(pred_ense_a, pred_ense_a.shape)","55ea7d9c":"pred_ense_b = (pred_select * 0.80) + (predt * 0.20)\n\nprint(pred_ense_b, pred_ense_b.shape)","bc2fbe8a":"sub4_ense_a = SAM.copy()\n\nsub4_ense_a['target'] = pred_ense_a\ndisplay(sub4_ense_a)","d433a061":"sub5_ense_b = SAM.copy()\n\nsub5_ense_b['target'] = pred_ense_b\ndisplay(sub5_ense_b)","cd0f303d":"sub4_ense_a.to_csv(\"submission4_ense_a.csv\",index=False)\nsub5_ense_b.to_csv(\"submission5_ense_b.csv\",index=False)\n!ls","73f831eb":"oof_pred_test1 = oof_pred_test.copy()\n\noof_pred_test1","1f261257":"for i in range(len(oof_pred_test1)):\n    \n    oof_pred_test1[i] = oof_pred_test1[i] ** 4\n                \noof_pred_test1        ","963d130b":"hist_data = [oof_pred_test1, oof_pred_test]  \ngroup_labels = ['Results to the power of four', 'Predicted Results']\n    \nfig = ff.create_distplot(hist_data, group_labels, bin_size=.2, show_hist=False, show_rug=False) \nfig.show()","7762c83a":"# Before the changes\nroc_auc(valid_y_test, oof_pred_test, 7, 7)","5c4ff6e6":"# After the changes\nroc_auc(valid_y_test, oof_pred_test1, 7, 7)","d0426a0b":"print(oof_pred_test1.min(), oof_pred_test1.max())","53ca4df3":"oof_pred_test2 = oof_pred_test.copy()\n\noof_pred_test2","4d00db9e":"for i in range(len(oof_pred_test2)):\n    \n    if (oof_pred_test2[i] < 0.5):        \n        oof_pred_test2[i] = oof_pred_test1[i] ** 1.2\n        \n    if (oof_pred_test2[i] > 0.5):        \n        oof_pred_test2[i] = oof_pred_test1[i] ** 0.8\n        \noof_pred_test2","95ae5363":"hist_data = [oof_pred_test2, oof_pred_test]  \ngroup_labels = ['Results Changed', 'Predicted Results']\n    \nfig = ff.create_distplot(hist_data, group_labels, bin_size=.2, show_hist=False, show_rug=False) \nfig.show()","c1ce1fc1":"# Before the changes\nroc_auc(valid_y_test, oof_pred_test, 7, 7)","afdc2be1":"# After the changes\nroc_auc(valid_y_test, oof_pred_test2, 7, 7)","efaa67d8":"print(oof_pred_test2.min(), oof_pred_test2.max())","339c4725":"X[categ_featu]","1f226935":"model.fit(train_X_test[categ_featu], train_y_test,     \n          eval_metric= 'auc',\n          eval_set= [(valid_X_test[categ_featu], valid_y_test)], \n          early_stopping_rounds= 200,\n          verbose= 200)   ","34be934e":"oof_predict_test3 = model.predict(valid_X_test[categ_featu])\n\naccuracy = accuracy_score(valid_y_test, oof_predict_test3) \n\nprint(f'\\nPrediction: {oof_predict_test3}\\n')\n\nprint(f'Accuracy: {accuracy}\\n')\n\nroc_auc(valid_y_test, oof_predict_test3, 7, 7)","8a7842f7":"oof_pred_test3 = model.predict_proba(valid_X_test[categ_featu])[:, -1]\n\nprint(f'\\nPrediction: {oof_pred_test3}\\n')\n\nroc_auc(valid_y_test, oof_pred_test3, 7, 7)   ","0f68c431":"hist_data = [oof_pred_test3, oof_predict_test3, valid_y_test] \ngroup_labels = ['Predict_proba', 'Predict', 'Correct Value']  \n\nfig = ff.create_distplot(hist_data, group_labels, bin_size=.2, show_hist=False, show_rug=False) \nfig.show() ","1b71caa6":"X[numer_featu]","bfdc4aec":"model.fit(train_X_test[numer_featu], train_y_test,     \n          eval_metric= 'auc',\n          eval_set= [(valid_X_test[numer_featu], valid_y_test)], \n          early_stopping_rounds= 200,\n          verbose= 200) ","5fabb331":"oof_predict_test4 = model.predict(valid_X_test[numer_featu])\n\naccuracy = accuracy_score(valid_y_test, oof_predict_test4) \n\nprint(f'\\nPrediction: {oof_predict_test4}\\n')\n\nprint(f'Accuracy: {accuracy}\\n')\n\nroc_auc(valid_y_test, oof_predict_test4, 7, 7)","0631c4f2":"oof_pred_test4 = model.predict_proba(valid_X_test[numer_featu])[:, -1]\n\nprint(f'\\nPrediction: {oof_pred_test4}\\n')\n\nroc_auc(valid_y_test, oof_pred_test4, 7, 7)   ","18f1e023":"hist_data = [oof_pred_test4, oof_predict_test4, valid_y_test] \ngroup_labels = ['Predict_proba', 'Predict', 'Correct Value'] \n\nfig = ff.create_distplot(hist_data, group_labels, bin_size=.2, show_hist=False, show_rug=False) \nfig.show() ","0bf61618":"oof_pred_test5 = (oof_pred_test3 * 0.5) + (oof_pred_test4 * 0.5)\n\nprint(f'\\nPrediction: {oof_pred_test5}\\n')\n\nroc_auc(valid_y_test, oof_pred_test5, 7, 7)  ","cee8b452":"hist_data = [oof_pred_test, oof_pred_test3, oof_pred_test4, oof_pred_test5]  \ngroup_labels = ['Main Result', 'Categorical', 'Numerical', 'Categorical + Numerical']  \n\nfig = ff.create_distplot(hist_data, group_labels, bin_size=.2, show_hist=False, show_rug=False) \nfig.show()","c83f0e9e":"## Model (For all Data)","95b98870":"### Prepare datasets for modeling","a3fa1472":"<div class=\"alert alert-success\">  \n<\/div>","9bda1076":"### For more information, refer to the following address:\n\n### [Learning from imbalanced data.](http:\/\/www.jeremyjordan.me\/imbalanced-data\/)","c5ef6c1e":"# **<span style=\"color:darkgray;\">Use Categorical Features and Numerical Features separately<\/span>** \n\n## Test: 3 >>> Only Categorical Features","30992507":"## Test: 4 >>> Only Numerical Features","e7730b47":"# **<span style=\"color:darkgray;\">Ensembling<\/span>** \n","647e0d47":"<div class=\"alert alert-success\">  \n<\/div>","34f480c2":"<div class=\"alert alert-success\">  \n<\/div>","b0d11b29":"<img src=\"https:\/\/raw.githubusercontent.com\/MehranKazeminia\/fifa-worldcup-2018\/master\/roc403.jpg\">","e62210ec":"<div class=\"alert alert-success\">  \n<\/div>","4f3c06c2":"<div class=\"alert alert-success\">  \n<\/div>","19a8cb0e":"<div class=\"alert alert-success\">  \n<\/div>","6bb669e4":"<div class=\"alert alert-success\">\n    <h3 align=\"center\">If you find this work useful, please don't forget upvoting :)<\/h3>\n<\/div>","8f372a86":"#### Hello everyone,\n\n#### In this notebook, we examine the following questions and try to find the answers to these questions.\n\n#### **1- Is it possible to ignore some \"folds\" in the \"cross-validation\" method? And is it possible that this will cause our score to increase?**\n\n#### Sometimes local maxima or local minima lock our calculations, or for any other reason, our algorithm for one or more \"folds\" is not well trained. If we ignore these \"folds\", we may be able to get a better score in the prediction phase for test data.\n\n#### In this notebook, we defined ten \"folds\" for \"cross-validation\" and considered all \"folds\" for the first time. The second time, we excluded five \"folds\" that had less \"accuracy\". This improved our score in the forecast phase for test data. Of course, this may not always be true. However, it is conceivable that perhaps this simple task in some challenges can easily improve our score.\n\n#### **2- Is it possible to use all the training data first and then use it as a \"fold\" in \"cross-validation\" when teaching the algorithm? And can this work improve our score?**\n\n#### In this notebook, we did exactly that with \"Ensembling\" and got a better score. However, you should always try, and this may not always improve your score.\n\n#### Please note that you will need to make some adjustments again. For example, if you are using LightGBM, you will need to set the value of n_estimators again to use all the training data. Because \"valid_X\" and \"valid_y\" no longer exist. However, we hope our code is easy to understand.\n\n#### **3- Can we train the algorithm separately using \"Categorical Features\" and \"Numerical Features\"? And then \"Ensemble\" the results? And is it possible that this will improve our score?**\n\n#### We did this at the end of this notebook with the titles \"test3\", \"test4\" and \"test5\". The form of the histogram curves of the results is very interesting and follows the mathematical logic of this topic. It's an interesting experience, but don't expect it to improve your score. Because it does not help the decisions of the algorithm and reduces its certainty.\n\n#### **4- What are the weaknesses of AUC Evaluation?**\n\n#### We have addressed this issue at the beginning of the notebook as well as at the end of the notebook under the titles \"test1\" and \"test2\".\n\n#### Please note that we can not change the ranking of some results or change the value of some results without a reason, even if \"AUC Evaluation\" is not sensitive to these shifts and changes. This means that, as you can see in this notebook, \"AUC Evaluation\" may give a good score to completely wrong results.\n\n### **Good Luck.**\n","61b41692":"<div class=\"alert alert-success\">  \n<\/div>","deab21c6":"# **<span style=\"color:darkgray;\">Competition Evaluation<\/span>**\n\n#### Submissions are evaluated on area under the **ROC curve** between the predicted probability and the observed target.","d1059eb4":"## Test: 5 >>> Only Categorical Features + Only Numerical Features","9cd5ed52":"<div class=\"alert alert-success\">  \n<\/div>","33495f4d":"# **<span style=\"color:darkgray;\">LGBMClassifier<\/span>** \n\n## Model (Cross Validation)","dab14277":"<div class=\"alert alert-success\">  \n<\/div>","477dc4fc":"<div class=\"alert alert-success\">  \n<\/div>","ba427123":"#### submission1.csv >>>>>>>>>> Public Score >>>> 0.85633 >>>> The classic \"cross-validation\" method\n\n#### submission2_select.csv >>>>> Public Score >>>> 0.85634 >>>> Remove bad folds\n\n#### submission3_all_data.csv >>>> Public Score >>>> 0.85625 >>>> Use all data\n\n#### submission4_ense_a.csv >>>>> Public Score >>>> 0.85634 >>>> Blend the first with the third\n\n#### submission5_ense_b.csv >>>>> Public Score >>>> 0.85635 >>>> Blend the second with the third","834d8e2a":"### But evaluating the main results and evaluating the changed results are still the same.","0af1f3ed":"<div> \n    <h2 align=\"center\">Tabular Playground Series - Oct 2021<\/h2> \n    <h1 align=\"center\">LightGBM & AUC Evaluation [ROC curve]<\/h1>            \n    <h4 align=\"center\">By: Somayyeh Gholami & Mehran Kazeminia<\/h4>\n<\/div>","a74c71ae":"<div class=\"alert alert-success\">  \n<\/div>","a9a2f728":"<div class=\"alert alert-success\">  \n<\/div>","d139b943":"<div class=\"alert alert-success\">  \n<\/div>","f3853278":"<div class=\"alert alert-success\">  \n<\/div>","a34aef85":"## Test: 2 >>> Results Changed","3e054be5":"# **<span style=\"color:darkgray;\">Another example of poor evaluation<\/span>** \n\n## Test: 1 >>> Results to the power of four","61664950":"![](https:\/\/www.jeremyjordan.me\/content\/images\/2018\/11\/roc_cutoff-1.gif)","9f515aa1":"<div class=\"alert alert-success\">  \n<\/div>","757d5c17":"### Data Augmentation","5f7fd129":"# **<span style=\"color:darkgray;\">Features<\/span>**","e746ac7c":"### But the evaluation for the main results and the evaluation for the results to the power of four are exactly equal.","65c638ed":"# **<span style=\"color:darkgray;\">Compressing<\/span>**\n\n#### Thanks to: **@gemartin** & **@lucamassaron**","44022db8":"# **<span style=\"color:darkgray;\">Challenge Datasets<\/span>**\n","1fad29c4":"<div class=\"alert alert-success\">  \n<\/div>","3d29a097":"<div class=\"alert alert-success\">  \n<\/div>","ddf02029":"### Categorical Features & Numerical Features","02ce7af9":"<div>\n    <h1 align=\"center\"> >>> End of Submissions<<< <\/h1>\n<\/div>"}}