{"cell_type":{"1cea59e3":"code","7962b2f2":"code","ddf44a0b":"code","379a186c":"code","9840f98e":"markdown","3f493649":"markdown","6f8db830":"markdown","53d73038":"markdown","f017a802":"markdown","22f9d470":"markdown"},"source":{"1cea59e3":"from IPython.display import Image\nImage('..\/input\/easymoneygrupo5\/amazon.png', height = 150)","7962b2f2":"import pandas as pd \nimport numpy as np ","ddf44a0b":"DIR_PATH = '\/Users\/carlosperezricardo\/Documents\/data\/'\nDIR_PATH = '..\/input\/easymoneygrupo5\/'\n\ncommercial_activity = pd.read_csv(DIR_PATH+'commercial_activity_df.csv', encoding='utf-8')\ncommercial_activity.drop(columns=['Unnamed: 0'], inplace=True)\n\nproducts = pd.read_csv(DIR_PATH+'products_df.csv', encoding='utf-8')\nproducts.drop(columns=['Unnamed: 0'], inplace=True)\n\nsociodemographic = pd.read_csv(DIR_PATH+'sociodemographic_df.csv', encoding='utf-8')\nsociodemographic.drop(columns=['Unnamed: 0'], inplace=True)\n\nLAST_DATE = \"2018-05-28\"","379a186c":"commercial_activity = commercial_activity[commercial_activity['pk_partition']==LAST_DATE]\nproducts = products[products['pk_partition']==LAST_DATE]\nsociodemographic = sociodemographic[sociodemographic['pk_partition']==LAST_DATE]\n\nprint('El commercial_activity_df ocupa {:1f} MB'.format(commercial_activity.memory_usage(index=True).sum()\/1048576))\nprint('El products_df ocupa {:1f} MB'.format(products.memory_usage(index=True).sum()\/1048576))\nprint('El sociodemographic_df ocupa {:1f} MB'.format(sociodemographic.memory_usage(index=True).sum()\/1048576))","9840f98e":"## Modelos de Machine Learning <a class=\"anchor\" id=\"3\"><\/a>\n\n[Tabla de Contenidos](#0) \n\nLos modelos desarrollados y la frecuencia de cada uso que se espera de cada uno de estos ser\u00e1n los siguientes:\n\n- Modelo de clusterizaci\u00f3n de la cartera actual de clientes. (semestral o incluso anual) - Tarea 2\n- Modelo de recomendaci\u00f3n: Motor de recomendaci\u00f3n y Modelo de Compra. (mensual) - Tarea 3\n- Modelo de segmentaci\u00f3n de emails. (mensual) - Tarea 4\n\nPara el caso del Modelo de segmentaci\u00f3n de emails, al ser ser un dataset con pocos datos este modelo podr\u00eda ser realizado en local obteniendo los datos de las bases de datos. Mientras que los otros modelos necesitan mucha capacidad de pre processing y preparaci\u00f3n de los datos por lo que ser\u00eda interesante utilizar el servicio <a href=\"https:\/\/aws.amazon.com\/es\/rds\/postgresql\/pricing\/\">AWS Lambda<\/a> (Ejecute c\u00f3digo sin tener que pensar en los servidores o los cl\u00fasteres. Pague solo por lo que utiliza). \n\nEl precio de AWS Lambda depende de su uso sus precios para Solicitudes es de 0,20 USD por un mill\u00f3n de solicitudes y el precio de Duraci\u00f3n es de 0,0000166667 USD por cada GB\/segundo. El precio de la duraci\u00f3n depende del volumen de memoria asignado a la funci\u00f3n. Puede asignar a la funci\u00f3n cualquier volumen de memoria desde los 128 MB a los 10, 240 MB en incrementos de 1 MB. Por ejemplo para 128 Memoria (MB) el Precio por 1 ms es de 0,0000000021 USD.\n\nAdem\u00e1s tambi\u00e9n se propone desarrollar los siguientes modelos. \n\n- Modelo de Churn. Identificar qu\u00e9 clientes son m\u00e1s proclives a abandonar la compa\u00f1\u00eda antes de que se marchen.\n\nLos Modelos de Machine Learning pueden ser entrenados e implementados en <a href=\"https:\/\/aws.amazon.com\/es\/getting-started\/hands-on\/build-train-deploy-machine-learning-model-sagemaker\/\">Amazon SageMaker<\/a> o utilizando el servicio AWS Lambda dado que los modelos tampoco son muy complicados. Empresas como <a href=\"https:\/\/www.h2o.ai\/\">H20.ai<\/a> ofrecen servicios para la optimizaci\u00f3n de hiperpar\u00e1metros, utilizarlos para la obtenci\u00f3n de los mejores par\u00e1metros para el modelo de Compra ser\u00eda interesante. ","3f493649":"# Tarea 3: Especificaciones de la producci\u00f3n \n\nEn esta subtarea se pide detallar especificaciones sobre la plataforma de producci\u00f3n para poder operar estos modelos a medio plazo. \n\n## Tabla de Contenidos <a class=\"anchor\" id=\"0\"><\/a>\n\n1. [Data Warehouse](#1) <br>\n    1.1 [Especificaciones del data warehouse](#11) <br>\n2. [Hosting del dashboard](#2) <br>\n3. [Modelos de Machine Learning ](#3) <br>\n\nLos modelos o productos a productivizar son los siguientes:\n\n- **Dashboard.** Este producto permitir\u00e1 a los empleados de la empresa visualizar el estado de la performance actual e hist\u00f3rica de easyMoney pudiendo obtener una imagen global y detallada del n\u00famero de altas, bajas, ingresos, an\u00e1lisis de los clientes... Este producto contiene tanto la parte de an\u00e1lisis donde puede visualizarse la informaci\u00f3n de productos, clientes, cashflow, etc. como la parte de segmentaci\u00f3n en la Tarea 2 y 4, donde se puede visualizar la informaci\u00f3n de cada cluster.\n- **Recomendaci\u00f3n.** Este modelo permite obtener las recomendaciones de productos para cada cliente o tipo de clientes. Estas recomendaciones despu\u00e9s son pasadas al Modelo de Compra para determinar su probabilidad de \u00e9xito. \n- **Modelo de Compra.** El Modelo de Compra consiste en un clasficador binario que permite obtener la probabilidad de compra dadas el estado y caracter\u00edsticas del cliente. \n- **Personalizaci\u00f3n.**  Este modelo se trata de un modelo de segmentaci\u00f3n que permite determinar las creatividades de los clientes a los que se espera enviar un email con el objetivo de que acaben comprando un producto.\n\nResulta complicado estimar el precio total de la producci\u00f3n pues los precios de los servicios var\u00edan en funci\u00f3n de su utilizaci\u00f3n. Se ha apostado por Amazon como nuestro proveedor en Cloud, en los siguientes puntos se determinan los precios generales como los precios seg\u00fan su uso. Habr\u00eda que profundizar en cada una de los puntos e incluso no se descarta contactar con Amazon para determinar cu\u00e1les son nuestras necesidades y qu\u00e9 producto se ajusta mejor a \u00e9stas; pudiendo finalmente dar un precio estimado correcto.","6f8db830":"### Especificaciones del data warehouse <a class=\"anchor\" id=\"11\"><\/a>\n\n[Tabla de Contenidos](#0) \n\nPara el Data Warehouse se opta por PostgreSQL utilizando el almacenamiento en cloud de Amazon con <a href=\"https:\/\/aws.amazon.com\/es\/rds\/postgresql\/pricing\/\">Amazon RDS<\/a>, similarmente al utilizado en el dashboard en la Tarea 1. El precio aproximado ser\u00eda de unos 10 euros (10,439 USD) al mes contratando el db.t3.micro que es una instancia. Tambi\u00e9n existen servicios on-premise, sin embargo el plan b\u00e1sico est\u00e1 perfectamente dimensionado para los requerimientos actuales. Si en el futuro aumenta la ingesta y el paquete actual no satisface la demanda y se puede aumentar las dimensiones pagando m\u00e1s.\n\nAdem\u00e1s ser\u00eda interesante contratar un servicio de <a href=\"https:\/\/aws.amazon.com\/s3\/pricing\/?trkCampaign=acq_paid_search_brand&sc_channel=PS&sc_campaign=acquisition_IBERIA&sc_publisher=Google&sc_category=Storage&sc_country=IBERIA&sc_geo=EMEA&sc_outcome=acq&sc_detail=s3%20cloud%20storage%20pricing&sc_content={ad%20group}&sc_matchtype=e&sc_segment=495089203403&sc_medium=ACQ-P|PS-GO|Brand|Desktop|SU|Storage|S3|IBERIA|EN|Sitelink&s_kwcid=AL!4422!3!495089203403!e!!g!!s3%20cloud%20storage%20pricing&ef_id=Cj0KCQjw78yFBhCZARIsAOxgSx16rja8WMTrcBsH3IintCiWOFg3JgtIr8wEhPMRPHCMvF4KxwY6EfwaAmB1EALw_wcB:G:s&s_kwcid=AL!4422!3!495089203403!e!!g!!s3%20cloud%20storage%20pricing\">Amazon S3<\/a>, para el almacenamiento de datos que no sean estrictamente en SQL como ficheros csv, txt, pickles, modelos de Machine Learning o im\u00e1genes; archivos que a menudo necesitan los modelos. Para un S3 Standard el precio para los primeros 50 TB \/ Month 0.0245 USD por GB. ","53d73038":"## Hosting del dashboard <a class=\"anchor\" id=\"2\"><\/a>\n\n[Tabla de Contenidos](#0) \n\nPara el hosting del dashboard y las visualizaciones de las segmentaciones se ha utilizado PythonAnywhere donde se pagan 6.05 USD (5 euros) en su versi\u00f3n m\u00e1s b\u00e1sica que presenta algunas limitaciones en cuanto a utilizaci\u00f3n de la CPU y memoria. Sin embargo, ser\u00eda interesante pasar a otro lenguaje web como Javascript donde se puede mejorar la calidad de la webs y permite realizar acciones y web m\u00e1s complicadas (Dash presenta sus limitaciones). \n\nPara el hosting se optar\u00eda por otro producto de Amazon para as\u00ed tener todo con la misma compa\u00f1\u00eda (si se desea ver en web) o utilizar alguna herramienta de Business Intelligence como PowerBI o Tableau. \n\nLos precios son los siguientes: \n- PowerBI. Tiene versi\u00f3n y \u00fanicamente se cobra 10 USD al editor (s\u00f3lo para Windows).\n- Tableau. Tableau ofrece una versi\u00f3n Viewer con un precio de 12 USD y otra Explorer de 35 USD.\n- Amazon. Ser\u00eda un producto realizado de manera interna, se necesitan conocimientos de Web Development (JavaScript, CSS, React, Backend, conociemiento para conexi\u00f3n con base de datos...). \n\nSi finalmente se opta por el dashboard en formato web ser\u00eda necesario reducir la latencia de la p\u00e1gina web, es decir el tiempo que tarda en leer los datos, procesarlos y cargarlos. Por ejemplo para el caso del dashboard (Tarea 1) es r\u00e1pido dado que todas las tablas son preprocesadas y preparadas y algunas im\u00e1genes son mostradas en est\u00e1tico pero para el caso de las visualizaciones de las segmentaciones el tiempo de carga es excesivo al tener que leer los datos y hacer los subsets por clusters y mostrar las im\u00e1genes. ","f017a802":"## Data Warehouse <a class=\"anchor\" id=\"1\"><\/a>\n\n[Tabla de Contenidos](#0) \n\nEl Data Warehouse es el lugar donde se almacenan las tablas con los datos con la informaci\u00f3n demogr\u00e1fica de los clientes, su actividad comercial y los productos que han contratado. Adem\u00e1s de tablas intermedias con datos ya preprocesados para la ingesta de datos para el dashboard, de esta manera la velocidad de carga en el dashboard se ve reducida incre\u00edblemente. \n\nAl empezar el proyecto se nos brindaron los 3 archivos siguientes:\n\n- sociodemographic_df.csv (301 MB)\n- commercial_activity_df.csv (383 MB)\n- products_df.csv (365 MB)\n\nSin embargo a lo largo de proyecto se han generado tablas intermedias para la visualizaci\u00f3n del dashboard y resultados del resto de tareas (altas, bajas, n\u00famero de clientes activos, altas pendientes de cobro...), tablas de fuentes exteriores (provincias o la informaci\u00f3n de pa\u00edses) y resultados obtenidos de los modelos desarrollados. \n\nPara poder dimensionalizar el data warehouse se estudia cu\u00e1l es la cantidad de datos por mes (es decir su velocidad). La estructura de tablas nos parece adecuada, aunque en algunas de ellas se propone aumentar la velocidad de ingesta de datos con el fin de poder obtener la informaci\u00f3n del estado de los clientes antes para poder reaccionar lo m\u00e1s r\u00e1pido posible ante cualquier tipo de obst\u00e1culo o problema y recoger feedback del plan de marketing cuanto antes; y no pasado un mes, cuando probablemente sea tarde para cualquier tipo de acci\u00f3n.\n\nActualmente, la cantidad de datos recibidos de manera mensual es la siguiente:\n- sociodemographic_df.csv (13.347 MB\/mes)\n- commercial_activity_df.csv (34.322 MB\/mes)\n- products_df.csv (17.161 MB\/mes)","22f9d470":"Se propone aumentar la velocidad de la ingesta de datos de la tabla products_df pasando de mensual a semanal. Por lo que la cantidad de datos mensuales se multiplicar\u00e1 por 4. De manera que products_df pasar\u00e1 de 34.3 MB a 137.3 MB. \n\nPara la tabla products_df ser\u00eda una buena idea cambiar el m\u00e9todo de almacenamiento actual, en lugar de guardar el status de todos los productos para cada cliente, se propone guardar \u00fanicamente los cambios de status (ya sea debido a altas o bajas), de esta manera no se almacena informaci\u00f3n innecesaria de clientes que no tienen actividad alguna; y el tama\u00f1o del dataset se ver\u00e1 reducido (y m\u00e1s si pensamos a largo plazo). Aunque el sistema actual es f\u00e1cil de interpretar y de trabajar con \u00e9l. "}}