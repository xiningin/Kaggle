{"cell_type":{"bf6b3f66":"code","dc163921":"code","a726e22b":"code","3c2cc6ee":"code","d8659ac1":"code","5cfbd4ef":"code","9ee84912":"code","e0b6aec6":"code","dc137aaa":"code","c46f68bb":"code","62af0033":"code","d28c4ce2":"code","cca0010d":"code","5cca966f":"code","1474b9fe":"code","2beafa94":"code","85b3ce56":"code","bb12e50c":"code","f8773809":"code","b0c37638":"code","6ca0c4b2":"code","e6953615":"code","013f7032":"code","2994d9b7":"code","ac378b90":"code","e8f721e8":"code","4f82e39b":"code","8359ae23":"code","a53e8837":"code","69c85943":"code","8fadddeb":"code","d47b7d79":"code","bba91e53":"code","1921c005":"code","7f2a37e3":"code","6bd8170e":"code","e07b0566":"code","eee077b9":"code","80a468b1":"markdown","3dd00428":"markdown","cb668309":"markdown","be879d93":"markdown","efbd9669":"markdown","f78705b5":"markdown","2cec52bd":"markdown","f99a2171":"markdown","678bcfbd":"markdown","06b58133":"markdown","ce029989":"markdown","d78db0b6":"markdown","b2ccdfab":"markdown","e76102b3":"markdown","70181023":"markdown","2fbe21eb":"markdown","b8ab16a2":"markdown","90ba2a2e":"markdown","a734bca2":"markdown","479cff71":"markdown","b6f0883c":"markdown","6e004706":"markdown","db249063":"markdown","b786a56a":"markdown","404564a1":"markdown","b22e1801":"markdown","f1b139d9":"markdown","9c5ed438":"markdown","feb73972":"markdown","0eb7ba2b":"markdown","0bab2de0":"markdown","90e90694":"markdown","de491356":"markdown","7fb053a8":"markdown"},"source":{"bf6b3f66":"import numpy as np\nimport pandas as pd\nimport os\nimport string\nimport datetime as dt\nfrom collections import Counter\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom numpy.linalg import svd\nfrom numpy import diag\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import roc_curve, auc\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import confusion_matrix\n# display all outputs\nfrom IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = \"all\"\n%matplotlib inline\n\nfrom pprint import pprint","dc163921":"# Importing data\ndf= pd.read_csv(\"\/kaggle\/input\/womens-ecommerce-clothing-reviews\/Womens Clothing E-Commerce Reviews.csv\", index_col=0)\ndf.columns= df.columns.str.replace(\" \", \"_\")\ndf.head()\ndf.isnull().sum()","a726e22b":"corpus_list = list(df[df.Review_Text.notnull()][\"Review_Text\"])\n# Reducing corpus to reduce running time\ncorpus_list= corpus_list[:500]\n\ncorpus_list[:5]","3c2cc6ee":"import nltk\nfrom nltk.corpus import stopwords\n\nstoplist= stopwords.words('english')\nstoplist[:5]","d8659ac1":"from nltk.tokenize import sent_tokenize, word_tokenize\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import wordnet\n\nwordnet_lemmatizer= WordNetLemmatizer()\n\ndef get_pos_tag(tag):\n    if tag.startswith('N') or tag.startswith('J'):\n        return wordnet.NOUN\n    elif tag.startswith('V'):\n        return wordnet.VERB\n    elif tag.startswith('R'):\n        return wordnet.ADV\n    else:\n        return wordnet.NOUN #default case","5cfbd4ef":"def preprocess(doc, stopwords):\n    \"\"\" 1. Removes Punctuations\n        2. Removes Stopwords\n        3. Converts into lowercase\n        4. Lemmatizes words\n    \"\"\"\n    punctuation= list(string.punctuation)\n    doc_tokens= nltk.word_tokenize(doc)    \n    word_tokens= [word.lower() for word in doc_tokens if not (word in punctuation or word in stoplist or len(word)<3)]\n    \n    #lemmatize    \n    pos_tags=nltk.pos_tag(word_tokens)\n    doc_words=[wordnet_lemmatizer.lemmatize(word, pos=get_pos_tag(tag)) for word, tag in pos_tags]\n    \n    return doc_words","9ee84912":"#This Function will return 10 examples of reviews containing that particular word\ndef documents_which_have(word, document_list, stoplist): #since stoplist is globally defined we don't really need to pass it within every function!\n    \n    print(\"Documents which contains the word '\", word, \"' are: \")\n    example_counter=0\n    document_words=[]\n    for document in document_list:\n        document_words=preprocess(document,stoplist)\n        \n        word_set= set(document_words)\n        if word in word_set:\n            print(document,\"\\n\")\n            example_counter+=1\n        \n        if example_counter==5:\n            break\n        \n# Function Call:\n# documents_which_have(\"summer\",corpus_list, stoplist)","e0b6aec6":"#Selecting business stopwords\nbusiness_stopwords= [\"i'm\",\"would\", \"look\", \"ordered\", \"wear\", \"fit\", \"one\", \"fits\",\"bought\", \"looks\", \"also\", \"got\", \"think\", \"even\",\n                     \"tried\", \"get\", \"could\", \"made\",\"way\",\"still\", \"runs\",\"true\" ,\"right\", \"see\",\"online\",\"wearing\", \"however\", \"design\",\"purchased\",\"feel\",\"go\",\n                     \"enough\",\"model\",\"though\",\"price\",\"looked\",\"person\",\"better\",\"first\",\"going\",\"try\", \"body\" \"bottom\",\"time\",\"many\",\"looking\",\"around\",\"thought\",\n                     \"make\",\"wanted\",\"saw\",\"makes\",\"went\",\"find\",\"found\",\"buy\",\"nan\",\"i've\", \"since\",\"seems\",\"ok\", \"girl\", \"woman\"]\n\nstoplist= stoplist+business_stopwords\nprint(\"New length of stoplist after addition of manually defined stopwords: \", len(stoplist))","dc137aaa":"a= \"running ran geese playing with cats\"\npreprocess(a, stoplist)\n# nltk.pos_tag(nltk.word_tokenize(a))","c46f68bb":"#Creating a list of all words present in all reviews: i.e: word_list\nword_list=[]\n\nfor sentence in corpus_list:\n        word_list.extend(preprocess(sentence,stoplist))\n        word_list= list(set(word_list))\n\nprint(\"Total no. of unique words present in our corpus are:\", len(word_list))","62af0033":"#Onefunction to return word_list(sorted bog), and corpus\ndef BOW_model(sentence_list, stoplist):\n    \n    sentence_words=[]\n    total_count={}\n    \n    for sentence in sentence_list:\n        sentence_words.append(preprocess(sentence, stoplist))\n        \n        word_set= set(sentence_words[-1]) #Pick the most recent sentence\n        for word in word_set:\n            total_count[word]= total_count.get(word,0)+1\n            \n    sorted_bog= sorted(list(total_count.items()), key=lambda x:x[1] ,reverse= True)\n    return sorted_bog, sentence_words\n\nsorted_bog, corpus_words= BOW_model(corpus_list, stoplist)\n\nprint(f\"Total unique words in our corpus:{len(sorted_bog)},\\n sorted bog:{sorted_bog[:10]}\")","d28c4ce2":"def inv_doc_freq(corpus_words):\n    number_docs= len(corpus_words)\n    \n    document_count={}\n    \n    for document in corpus_words:\n        word_set= set(document)\n        \n        for word in word_set:\n            document_count[word]= document_count.get(word,0)+1\n            \n    IDF={}\n    \n    for word in document_count:\n        IDF[word]= np.log(number_docs\/document_count[word])\n        \n    return IDF\n\nIDF= inv_doc_freq(corpus_words)\nsorted_IDF= sorted(list(IDF.items()), key=lambda x:x[1])\nprint(\"Calibrating IDF scores. \\n Top IDF scores representing rarely occuring words:\\n \",sorted_IDF[-10:])","cca0010d":"rare_review_words= [t[0] for t in sorted_IDF if round(t[1])==round(max(IDF.values()))] #Using List comprehension!\nlen(rare_review_words)","5cca966f":"#Recomputing stoplist\nstoplist= stoplist+ rare_review_words\n\n#Recomputing Bag of words and Corpus words:\nsorted_bog, corpus_words= BOW_model(corpus_list, stoplist)\nprint(\"The rare words are removed from our corpus and the new list of bog now contains\", len(sorted_bog),\"unique words!\")\nlen(sorted_bog), len(corpus_words)","1474b9fe":"def tf_idf(corpus_words):\n    IDF= inv_doc_freq(corpus_words)\n        \n    TFIDF=[]\n    \n    for document in corpus_words:\n        TFIDF.append(Counter(document))\n    \n    for document in TFIDF:\n        for word in document:\n            document[word]= document[word]* IDF[word]\n    \n    return TFIDF\n\nTFIDF= tf_idf(corpus_words)\nTFIDF[:2]","2beafa94":"word_list=[]\n\nfor sentence in corpus_list:\n        word_list.extend(preprocess(sentence,stoplist))\n        word_list= list(set(word_list))\n\nprint(\"Total no. of unique words present in our corpus are:\", len(word_list))\n","85b3ce56":"def build_vocabulary(TFIDF):\n    word_set= set()\n    \n    for document in TFIDF:\n        for key in document:\n            word_set.add(key)\n            ## set add will add the element only if it isn't already present\n    \n    word_dict= dict(zip(word_list, range(len(word_list))))\n    \n    return word_dict\n\nword_dict= build_vocabulary(TFIDF)\nword_dict","bb12e50c":"def term_document_matrix(corpus_words):\n    \n    \"\"\" This TDM contains word frequency for every (word, document) pair\"\"\"\n    \n    tdm=[]\n    \n    cv= CountVectorizer(tokenizer= lambda doc:doc, lowercase=False, ngram_range=(1,1))\n    \n    tdm= cv.fit_transform(corpus_words)\n    \n    feature_names= cv.get_feature_names()\n    \n    return np.transpose(tdm), feature_names    \n\ntdm, feature_names= term_document_matrix(corpus_words)\n\nprint(\"Our dataset has:\\n%u unique words\\n%u documents\"%(tdm.shape))","f8773809":"tdm.shape","b0c37638":"def term_document_matrix_TFIDF(TFIDF, word_list, word_dict):   #Length of both the TD matrix should be the same!\n    \n    \"\"\" This TDM contains the TFIDF score for every (word, document) pair \"\"\"\n    vocabulary_size = len(word_dict)\n    number_documents = len(TFIDF)\n    TDM = np.zeros((vocabulary_size, number_documents))\n    \n    for doc in range(number_documents):\n        document = TFIDF[doc]\n\n        for word in document.keys():\n            pos= word_dict.get(word,-1) #To handle cases: words which have been removed from word dict(as they were rare words!)\n            if pos==-1:\n                continue\n\n            TDM[pos, doc] = document[word]\n    \n    return TDM\n\nTDM_tfidf = term_document_matrix_TFIDF(TFIDF, word_list, word_dict)\nprint(\"Our dataset has:\\n%u unique words\\n%u documents\"%(TDM_tfidf.shape))","6ca0c4b2":"new_review= ['great','fit','pink','skirt']","e6953615":"def find_related_docs(review, TDM):\n    new_vector= np.zeros(TDM.shape[1])\n    \n    for word in review:\n        pos= word_dict.get(word,-1)\n        if pos==-1:\n            continue\n        new_vector+= TDM[pos,:]\n        \n    # After the loop completion, new_vector will tell us which documents are activated by this word!\n    # Let's extract the list of documents sorted by activation\n    \n    doc_list= sorted(zip(range(TDM.shape[1]), new_vector), key= lambda x:x[1], reverse=True)\n    \n    return doc_list\n\nrelated= find_related_docs(new_review, TDM_tfidf)\nrelated[:3]","013f7032":"#Let's look at the top 5 results\nprint(\"Top 5 reviews related to \", new_review, \"are: \\n\")\nfor review, score in related[:3]:\n    print(round(score), review, \" \".join(corpus_words[review]),\"\\n\")","2994d9b7":"df.loc[(df.Recommended_IND==1), 'Rating'].value_counts(normalize= True)","ac378b90":"df.loc[(df.Recommended_IND==0), 'Rating'].value_counts(normalize= True)","e8f721e8":"from sklearn.model_selection import train_test_split\ndf.dropna(subset=['Review_Text'], inplace=True)\nx= df['Review_Text']\ny= df['Recommended_IND']","4f82e39b":"train_data,test_data = train_test_split(df,train_size=0.8,random_state=0)\n\n# sorted_bog, corpus_words= BOW_model(corpus_list, stoplist)\ntrain_bow, train_corpus= BOW_model(train_data['Review_Text'], stoplist)\ntest_bow, test_corpus= BOW_model(test_data['Review_Text'], stoplist)\n\n\nvectorizer= CountVectorizer(tokenizer= lambda doc:doc, lowercase=False, ngram_range=(1,1))\nx_train = vectorizer.fit_transform(train_corpus)\ny_train = train_data['Recommended_IND']\nx_test = vectorizer.transform(test_corpus)\ny_test = test_data['Recommended_IND']\n\nx_train.shape, x_test.shape, y_train.shape, y_test.shape","8359ae23":"start= dt.datetime.now()\nlr=LogisticRegression()\nlr.fit(x_train, y_train)\nprint(\"Elapsed time:\", str(dt.datetime.now()- start))","a53e8837":"start= dt.datetime.now()\nnb= MultinomialNB()\nnb.fit(x_train, y_train)\nprint(\"Elapsed time:\", str(dt.datetime.now()- start))","69c85943":"start= dt.datetime.now()\nsvm= SVC()\nsvm.fit(x_train, y_train)\nprint(\"Elapsed time:\", str(dt.datetime.now()- start))","8fadddeb":"start= dt.datetime.now()\nnn= MLPClassifier()\nnn.fit(x_train, y_train)\nprint(\"Elapsed time:\", str(dt.datetime.now()- start))","d47b7d79":"#Define a dataframe for the prediction probabilities of the model\ndf2= train_data.copy()\ndf2['Logistic_Regression']= lr.predict(x_train)\ndf2['Naive_Bayes']= nb.predict(x_train)\ndf2['SVM']= svm.predict(x_train)\ndf2['Neural_network']= nn.predict(x_train)\ndf2.head()","bba91e53":"pred_lr= lr.predict_proba(x_test)[:,1]\nfpr_lr, tpr_lr,_= roc_curve(y_test, pred_lr)\nroc_auc_lr= auc(fpr_lr, tpr_lr)\n\npred_nb= nb.predict_proba(x_test)[:,1]\nfpr_nb, tpr_nb,_= roc_curve(y_test, pred_nb)\nroc_auc_nb= auc(fpr_nb, tpr_nb)\n\npred_svm= svm.decision_function(x_test)\nfpr_svm, tpr_svm,_= roc_curve(y_test, pred_svm)\nroc_auc_svm= auc(fpr_svm, tpr_svm)\n\npred_nn= nn.predict_proba(x_test)[:,1]\nfpr_nn, tpr_nn,_= roc_curve(y_test, pred_nn)\nroc_auc_nn= auc(fpr_nn, tpr_nn)\n\nf, axes = plt.subplots(2, 2,figsize=(15,10))\n\naxes[0,0].plot(fpr_lr, tpr_lr, color='darkred', lw=2, label='ROC curve (area = {:0.2f})'.format(roc_auc_lr))\naxes[0,0].plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\naxes[0,0].set(xlim=[-0.01, 1.0], ylim=[-0.01, 1.05])\naxes[0,0].set(xlabel ='False Positive Rate', ylabel = 'True Positive Rate', title = 'Logistic Regression')\naxes[0,0].legend(loc='lower right', fontsize=13)\n\naxes[0,1].plot(fpr_nb, tpr_nb, color='darkred', lw=2, label='ROC curve (area = {:0.2f})'.format(roc_auc_nb))\naxes[0,1].plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\naxes[0,1].set(xlim=[-0.01, 1.0], ylim=[-0.01, 1.05])\naxes[0,1].set(xlabel ='False Positive Rate', ylabel = 'True Positive Rate', title = 'Naive Bayes')\naxes[0,1].legend(loc='lower right', fontsize=13)\n\naxes[1,0].plot(fpr_svm, tpr_svm, color='darkred', lw=2, label='ROC curve (area = {:0.2f})'.format(roc_auc_svm))\naxes[1,0].plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\naxes[1,0].set(xlim=[-0.01, 1.0], ylim=[-0.01, 1.05])\naxes[1,0].set(xlabel ='False Positive Rate', ylabel = 'True Positive Rate', title = 'Support Vector Machine')\naxes[1,0].legend(loc='lower right', fontsize=13)\n\naxes[1,1].plot(fpr_nn, tpr_nn, color='darkred', lw=2, label='ROC curve (area = {:0.2f})'.format(roc_auc_nn))\naxes[1,1].plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\naxes[1,1].set(xlim=[-0.01, 1.0], ylim=[-0.01, 1.05])\naxes[1,1].set(xlabel ='False Positive Rate', ylabel = 'True Positive Rate', title = 'Neural Network')\naxes[1,1].legend(loc='lower right', fontsize=13)","1921c005":"# preparation for the confusion matrix\nlr_cm=confusion_matrix(y_test.values, lr.predict(x_test))\nnb_cm=confusion_matrix(y_test.values, nb.predict(x_test))\nsvm_cm=confusion_matrix(y_test.values, svm.predict(x_test))\nnn_cm=confusion_matrix(y_test.values, nn.predict(x_test))\n\nplt.figure(figsize=(15,12))\nplt.suptitle(\"Confusion Matrices\",fontsize=24)\n\nplt.subplot(2,2,1)\nplt.title(\"Logistic Regression\")\nsns.heatmap(lr_cm, annot = True, cmap=\"Greens\",cbar=False);\n\nplt.subplot(2,2,2)\nplt.title(\"Naive Bayes\")\nsns.heatmap(nb_cm, annot = True, cmap=\"Greens\",cbar=False);\n\nplt.subplot(2,2,3)\nplt.title(\"Support Vector Machine (SVM)\")\nsns.heatmap(svm_cm, annot = True, cmap=\"Greens\",cbar=False);\n\nplt.subplot(2,2,4)\nplt.title(\"Neural Network\")\nsns.heatmap(nn_cm, annot = True, cmap=\"Greens\",cbar=False);","7f2a37e3":"import sklearn.metrics as mt\n\nprint(\"Logistic Regression\")\nprint(mt.classification_report(y_test, lr.predict(x_test)))\nprint(\"\\n Naive Bayes\")\nprint(mt.classification_report(y_test, nb.predict(x_test)))\nprint(\"\\n Support Vector Machine (SVM)\")\nprint(mt.classification_report(y_test, svm.predict(x_test)))\nprint(\"\\n Neural Network\")\nprint(mt.classification_report(y_test, nn.predict(x_test)))","6bd8170e":"u, sigma, vt= svd(TDM_tfidf)","e07b0566":"m,n = TDM_tfidf.shape\nprint(u.shape, sigma.shape, vt.shape, m, n)","eee077b9":"k=10\n\n#Convert the vector of singular values into a diagonal matrix\nsigma_k= sigma[:k]\nSk= diag(sigma_k)\n\n#Drop the extraneaous dimensions in the other two matrices\nuk= u[:,:k]\nvtk= vt[:k,:]\n\nprint(uk.shape, Sk.shape, vtk.shape)","80a468b1":"### Term-Document Matrix <a id =\"7\"><\/a>\nNow use the TFIDF matrix and our vocabulary to generate the Term Document matrix. This is just a matter of rearranging the values in our (sparse) TFIDF matrix into the full TD matrix.","3dd00428":"stemming\n\nstudy, studying- study\nplayer, player, playing- play","cb668309":"**One way of determining the relative importance of a word is to see how often it appears across multiple documents. Words that are relevant to a specific topic are more likely to appear in documents about that topic and much less in documents about other topics (TF). On the other hand, less meaningful words (like the) will be common across documents about any subject.(IDF)**","be879d93":"The resulting matrices have the expected shapes, except the singular value matrix, sigma that is simply a vector. This is an optimization of the numpy function meant to save memory.","efbd9669":"### Bag of words Representation <a id =\"3\"><\/a>","f78705b5":"### Evaluating Models","2cec52bd":"Naive Bayes","f99a2171":"### STOPWORDS <a id=\"2.1\"><\/a>","678bcfbd":"In this way, we preserve the most significant (higher sigma values) latent dimensions of the dataset while discarding the rest. The idea is simple, SVD projects the original dataset into an internal space which is a linear combination of the original one. Each dimension in this space can be thought of as an underlying topic in the data. By preserving only the most dominant topics in effect we are simplifying the data and, potentially, removing noise.","06b58133":"We can now reduce the internal dimension of these matrices to say, 10","ce029989":"Support Vector Machine (SVM)","d78db0b6":"### Building a Sentiment Classifier <a id =\"9\"><\/a> <br>\n\nGiven a new review text, you will be able to accurately classify it into positive or negative sentiment, having a model being trained on both classes. This is similar to Spam or Not-Spam classifier in gmail.","b2ccdfab":"**55% of words in the vocab appear to have occured only once and are either typos\/carry redundant information- Rare review words!**","e76102b3":"## Importing Dataset and EDA <a id=\"eda\"><\/a>","70181023":"### ROC Curve and AUC","2fbe21eb":"## NLP on Women's clothing reviews <a id =\"8\"><\/a>\n1. Extract all the numbers. Check what % of ladies have indeed mentioned their weights and what is their average rating or major keywords? all reviews where women have mentioned their weights(search for letter- lb\/lbs or a 3 digit number) and heights. - Can be done using REGEX\n2. Recommendation Prediction\n3. Extract Semantically related words- Extract reviews related to a single review- ESA\n4. Is there any correlation between user's rating and reviews length ?\n\ndf['Review Text']=df['Review Text'].astype(str) <br>\ndf['Review Length']=df['Review Text'].apply(len)\n\n- Is it possible to identify such a thing as: Women above 40 who is really happy will write more or equal to women below 40. Women above 40 will write a smaller review when they are pissed off with a product, compared to a younger lady who will critisize a product in detail.","b8ab16a2":"Neural Network","90ba2a2e":"#### Word Normalization Techniques: Lemmatization <a id =\"2.2\"><\/a>\nStemming is the process of reducing inflection in words to their root forms such as mapping a group of words to the same stem even if the stem itself is not a valid word in the Language.\n<br>\n**Lemmatization**, unlike Stemming, reduces the inflected words properly ensuring that the root word belongs to the language. In Lemmatization root word is called Lemma. ","a734bca2":"### Latent Semantic Analysis\nLSA analysis, sometimes refered to as Latent Semantic Indexing, relies on the well known Singular Value Decomposition. Singular Values (and Singular Vectors) can be thought of as being a generalization of the more common eigenvalues (and eigenvectors) for the case of non-square matrices.","479cff71":"# ![image.png](attachment:image.png)","b6f0883c":"### Explicit Semantic Analysis\/ Find related documents\nIn ESA we make use of the TD matrix of our corpus as a knowledge base that we can use to look up related documents.","6e004706":"Logistic Regression","db249063":"### Confusion Matrix","b786a56a":"### Precision - Recall - F1-Score","404564a1":"Looking above, it is evident that Recommended_IND classifies 4 and 5 Rating as positive sentiment(1) and 1,2,3 as negative sentiment(0).","b22e1801":"### Table of Contents <a id= \"toc\"> <\/a>\n<pre>\n[1. Importing Data, EDA and corpus list](#eda) <br>\n[2. Cleaning Dataset](#clean)<br>\n   [2.1. Stopwords](#2.1)<br>\n       [a. Adding business stopwords](#2.1.a)<br>\n       [b. Computing rare occuring words from IDF scores](2.1.b)<br>\n    [2.2. Stemming](#2.2)<br>\n    [2.3. Lemmatization](#2.3)<br>\n    [2.4. Extract words](#2.4)<br>\n[3. Bag of Words (BOW) reprentation](#3)<br>\n[4. IDF](#4)<br>\n[5. TFIDF](#5)<br>\n[6. Building a Vocab](#6)<br>\n[7. Term- document matrix](#7)<br>\n[8. NLP on reviews](#8)<br>\n[9. Sentiment Classifier](#9)<br>","f1b139d9":"### Cleaning Dataset <a id=\"clean\"><\/a>","9c5ed438":"### TF-IDF <a id =\"5\"><\/a>","feb73972":"### Manually adding business stopwords <a id =\"2.1.a\"><\/a>\n\nFew words would be helpful in sentiment analysis and hence we will not consider them as stopwords (ex: 'really',\"much\" can be used a valence modifier!)\n\nCheckout the \"documents_which_have\" function to justify the new business stopwords addition!\nex: \"one\"- used in contexts such as: \"the one\", \"this one\", \"tricky one\"\n\nNeed to use lemmatizers to combine all variations- ex: fit\/fits, wear\/wearing, look\/looking\nalso similar meaning words like buy\/bought\/purchased","0eb7ba2b":"<pre>\n**Observation**\nIDF is said to measure the informativeness of the word. It will be very low for the most occurring words such as stop words.\nHowever the high IDF words, which are really rare occurences- may also be mistakes- and could be irrelevant.\nIt is very noticiable from above that words with highest IDF scores seem to have Incorrect spelling and less importance.\nHence we can probably remove the top IDF words, based on their TF-IDF scores, also we can manually check the sentences they appear in to verify their importance using the function:\nexample: \ndocuments_which_have(\"grrrrrrrrrrr\",list(df.Review_Text), stoplist)","0bab2de0":"**More coming up! Please upvote if you find it intuitive and helpful. Take care!**","90e90694":"### Inverse Document Frequency(IDF) <a id =\"4\"><\/a>","de491356":"High values of **TF-IDF** represent that they have occurence only in that specific document, now it could be a mistake\/typo or could be an important topic only for that document.<br>\nIn the above cases: <br>\nHigh Values- typo- \"sooo\",\"happened\",\"bc\" <br>\nHigh Values- important for topic - \"petite\",\"silky\"","7fb053a8":"### Computing rare words  <a id =\"2.1.b\"><\/a>\n\nWe consider words that occur only once (thus have highest IDF) as rare."}}