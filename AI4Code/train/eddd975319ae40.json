{"cell_type":{"d406271e":"code","7a1ff51d":"code","ac0ccfad":"code","75ac5f35":"code","13f2368e":"code","9158c816":"code","48e530f6":"code","e4e6ae76":"code","308f00f2":"code","c7d22f42":"code","6470028f":"code","67da5cde":"code","3e5b92d2":"code","f1c3252b":"code","50328d17":"code","42c95969":"code","ed8c1e9a":"code","b3eceb4d":"code","e5882c40":"code","a71f4025":"code","52f8c1f8":"code","8b22e64f":"code","01c37fed":"code","3c366da2":"markdown","8049f800":"markdown","22be3efe":"markdown","e232de05":"markdown","03769d1c":"markdown","0135b8a8":"markdown","9118f0ea":"markdown","994528b8":"markdown","3a9276b6":"markdown","51730382":"markdown","8e05e310":"markdown","e9fce850":"markdown"},"source":{"d406271e":"# Imports\nimport os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Machine learning\nfrom sklearn.model_selection import train_test_split\nfrom keras.layers import Conv2D, Dense, Dropout, Flatten, BatchNormalization\nfrom keras.optimizers import Adam\nfrom keras.metrics import categorical_accuracy\nfrom keras.losses import categorical_crossentropy\nfrom keras.models import Sequential","7a1ff51d":"# Paths\ninput_path = os.path.join('..', 'input')\nclassmap_path = os.path.join(input_path, 'k49_classmap.csv')\n\nk49_train_imgs_path = os.path.join(input_path, 'k49-train-imgs.npz')\nk49_train_labels_path = os.path.join(input_path, 'k49-train-labels.npz')\nk49_test_imgs_path = os.path.join(input_path, 'k49-test-imgs.npz')\nk49_test_labels_path = os.path.join(input_path, 'k49-test-labels.npz')\n\n# Learning\nn_classes = 49\nlearning_rate = 0.001\nimage_shape = (28, 28, 1)\nn_epochs = 25","ac0ccfad":"# Classmap loading : pandas dataframe that links an index to an hiragana\n# Here 0 is a 'a', 1 a 'i', 2 a 'u', ...\nk49_classmap = pd.read_csv(classmap_path)\nk49_classmap.head()","75ac5f35":"# Data loading\ntrain_imgs = np.load(k49_train_imgs_path)['arr_0']\ntrain_labels = np.load(k49_train_labels_path)['arr_0']\ntest_imgs = np.load(k49_test_imgs_path)['arr_0']\ntest_labels = np.load(k49_test_labels_path)['arr_0']","13f2368e":"# Data visualization : let's plot some hiraganas\nn = 7\nfig, axs = plt.subplots(nrows=n, ncols=n, sharex=True, sharey=True, figsize=(8, 8))\nfor i in range(n**2):\n    ax = axs[i \/\/ n, i % n]\n    ax.imshow(train_imgs[i], cmap='Greys')\n    ax.axis('off')\nplt.tight_layout()\nplt.show()","9158c816":"# Let's plot the data repartition between all our classes\ntrain_labels_s = pd.Series(train_labels)\ntrain_labels_s.value_counts(sort=False).plot.bar()\n# here I still need to find a way to enlarge this plot !","48e530f6":"# Using expand_dims to get a nominal deep learning format for all images\n# (28, 28) --> (28, 28, 1)\ntrain_imgs = np.expand_dims(train_imgs, axis=-1)\ntest_imgs = np.expand_dims(test_imgs, axis=-1)","e4e6ae76":"# creation of a training set and a validation one\nx_train, x_val, y_train, y_val = train_test_split(train_imgs, train_labels, test_size=0.10)","308f00f2":"# One hot encoding util function\ndef one_hot_encoding(y):\n    y_res = np.zeros((len(y), n_classes))\n    for i in range(len(y)):\n        y_res[i][y[i]] = 1\n    return y_res","c7d22f42":"# Get the labels in a one hot encoded version\ny_train = one_hot_encoding(y_train)\ny_val = one_hot_encoding(y_val)","6470028f":"# Model definition - simple CNN model\ndef define_cnn_model(input_shape, output_nodes):\n    model = Sequential()\n\n    model.add(Conv2D(32, (5, 5), strides=(1, 1), activation='relu', input_shape=input_shape))\n    model.add(Conv2D(32, (5, 5), strides=(2, 2), activation='relu'))\n    model.add(BatchNormalization())\n    model.add(Dropout(0.3))\n\n    model.add(Conv2D(64, (3, 3), strides=(1, 1), activation='relu'))\n    model.add(Conv2D(64, (3, 3), strides=(1, 1), activation='relu'))\n    model.add(BatchNormalization())\n    model.add(Dropout(0.3))\n\n    model.add(Conv2D(96, (3, 3), strides=(1, 1), activation='relu'))\n    model.add(Conv2D(96, (3, 3), strides=(1, 1), activation='relu'))\n    model.add(BatchNormalization())\n    model.add(Dropout(0.3))\n\n    model.add(Flatten())\n    model.add(Dense(512, activation='relu'))\n    model.add(Dense(256, activation='relu'))\n    model.add(Dense(output_nodes, activation='softmax'))\n    \n    return model\n\nmodel = define_cnn_model(image_shape, n_classes)\nmodel.compile(optimizer=Adam(learning_rate), loss=categorical_crossentropy, metrics=[categorical_accuracy])","67da5cde":"# Let's look at our model\nmodel.summary()","3e5b92d2":"training_recap = model.fit(x_train, y_train, epochs=n_epochs, validation_data=(x_val, y_val), batch_size=128)","f1c3252b":"history = training_recap.history\ne = [i for i in range(1, n_epochs+1)]\nloss = history['loss']\nval_loss = history['val_loss']\n\nplt.plot(e, loss, val_loss)\nplt.title('Training and validation losses')\nplt.show()","50328d17":"# Prediction on test set\nprint(\"Categorical accuracy : {:.3f}\".format(model.evaluate(test_imgs, one_hot_encoding(test_labels))[1]))\ny_pred = model.predict(test_imgs)\ny_pred = np.argmax(y_pred,axis=1)","42c95969":"# Let's visualize some predictions\nn = 5\nfig, axs = plt.subplots(nrows=n, ncols=n, sharex=True, sharey=True, figsize=(15, 15))\nfor i in range(n**2):\n    ax = axs[i \/\/ n, i % n]\n    ax.imshow(test_imgs[i, :, :, 0], cmap='Greys')\n    ax.set_title('Class: {}, Predicted: {}'.format(test_labels[i], y_pred[i]))\nplt.show()","ed8c1e9a":"from sklearn.metrics import classification_report\n\nprint(classification_report(test_labels, y_pred, target_names=k49_classmap['char']))","b3eceb4d":"# Definition of the penalization weights\ntrain_labels_s = pd.Series(train_labels)\nn_sample_per_class = train_labels_s.value_counts(sort=False)\nmax_sample_per_class = n_sample_per_class.max()\nweights = np.array([max_sample_per_class \/ w for w in n_sample_per_class])\nweights = weights \/ weights.max()\n# print(weights)","e5882c40":"# custom loss function\n\"\"\"\nA weighted version of categorical_crossentropy for keras (2.0.6). This lets you apply a weight to unbalanced classes.\n@url: https:\/\/gist.github.com\/wassname\/ce364fddfc8a025bfab4348cf5de852d\n@author: wassname\n\"\"\"\nfrom keras import backend as K\ndef weighted_categorical_crossentropy(weights):\n    \"\"\"\n    A weighted version of keras.objectives.categorical_crossentropy\n    \n    Variables:\n        weights: numpy array of shape (C,) where C is the number of classes\n    \n    Usage:\n        weights = np.array([0.5,2,10]) # Class one at 0.5, class 2 twice the normal weights, class 3 10x.\n        loss = weighted_categorical_crossentropy(weights)\n        model.compile(loss=loss,optimizer='adam')\n    \"\"\"\n    \n    weights = K.variable(weights)\n        \n    def loss(y_true, y_pred):\n        # scale predictions so that the class probas of each sample sum to 1\n        y_pred \/= K.sum(y_pred, axis=-1, keepdims=True)\n        # clip to prevent NaN's and Inf's\n        y_pred = K.clip(y_pred, K.epsilon(), 1 - K.epsilon())\n        # calc\n        loss = y_true * K.log(y_pred) * weights\n        loss = -K.sum(loss, -1)\n        return loss\n    \n    return loss\n\ncustom_categorical_crossentropy = weighted_categorical_crossentropy(weights)","a71f4025":"# Redefinintion of our model\nmodel_custom_loss = define_cnn_model(image_shape, n_classes)\nmodel_custom_loss.compile(optimizer=Adam(learning_rate), loss=custom_categorical_crossentropy, metrics=[categorical_accuracy])","52f8c1f8":"training_recap_c = model_custom_loss.fit(x_train, y_train, epochs=n_epochs, validation_data=(x_val, y_val), batch_size=128)","8b22e64f":"# Prediction on test set\nprint(\"Categorical accuracy : {:.3f}\".format(model_custom_loss.evaluate(test_imgs, one_hot_encoding(test_labels))[1]))\ny_pred = model_custom_loss.predict(test_imgs)\ny_pred = np.argmax(y_pred,axis=1)","01c37fed":"print(classification_report(test_labels, y_pred, target_names=k49_classmap['char']))","3c366da2":"Do not hesitate to post any questions in the comments. And feel free to upvote if you liked this kernel :)\n\nStill to do :\n- find a solution to class imbalance (with a custom loss function)\n- a better layout for this kernel","8049f800":"### Classmap loading\n\nThe classmap gives us a link between the class indexes and the actual Japanese character.","22be3efe":"## Custom loss function\n\nHere, I will use the Keras backend to create a custom loss function in order to tackle the unbalanced data that we have. To do that, I will keep the categorical crossentropy structure, to which I will add some weights to penalize more errors in the low sample classes.","e232de05":"The majority of all classes have around 6000 samples. That is good for the training phase. However, some classes have less than 1000 samples, which may cause a lack of accuracy in their prediction. For the first model, I will keep this repartition.","03769d1c":"## Data loading\n\nI just use the numpy load function to get the data inside the .npz files. The ['arr_0'] statement outputs the actual array stored inside the file.","0135b8a8":"# K49 MNIST\nThe K49 MNIST is a MNIST like dataset. It is formed by 49 different classes, 49 different hiraganas. Hiraganas are a Japanese alphabet, the first one that every Japanese person learns at school ([Hiragana](https:\/\/en.wikipedia.org\/wiki\/Hiragana)).\n\nIn this notebook, I will try to create a small CNN to classify these hiraganas.","9118f0ea":"## Data processing\n\nThe next four cells transform the data into a valid deep learning format. First, I use the expand_dims numpy function to get a 3D tensor representation of the data (height, width, channel).\nThen I create a training set and a validation set. And finally I one-hot encode the output so the CNN model can understand it.","994528b8":"Thanks to this graph, I think I'm not overfitting. That's a good thing. Moreover, it won't be useful to add more epochs.","3a9276b6":"Now, to get some insights on how my model works on the whole test set, I will use the classification report from sklearn.","51730382":"## Settings\n\nHere, in the next few cells, I will just define some settings:\n* Paths to read the input data\n* n_classes : the number of classes of our output\n* learning_rate : our optimizer starting learning rate\n* image_shape : the shape of the images that I will feed into the CNN\n* n_epochs : the number of training epochs","8e05e310":"This gives us 95% accuracy, on a 49-class problem with less than 15 minutes spend in training. It's not too bad. But this score can be improved I think by finding a solution to the unbalanced class repartition of this dataset. By looking at the classification report, I remarked that the classes with the lower accuracy are the ones with the less samples.","e9fce850":"## CNN model definition\n\nThe model is composed of three blocks of two convolution layers. Each block uses BatchNormalization to facilitate the training phase. Then, the last block of the model is composed of three fully connected layers, the last one giving us a probability for each class, thanks to the softmax activation."}}