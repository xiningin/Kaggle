{"cell_type":{"97c0ff0e":"code","c2be5e7c":"code","3b7b570a":"code","859979db":"code","c28da619":"code","a269add5":"code","4e8d7c35":"code","37bfb864":"code","28e49b41":"code","2df2bfee":"code","142b62e7":"code","65a7be38":"code","6507e8a4":"code","67c4c081":"code","0d2102fc":"code","e72ae8da":"code","0e5467dc":"code","2d11947c":"code","596d54a5":"code","0d3eb0ac":"code","d96b6bcf":"code","ac525abb":"code","21c608f4":"code","5fac375f":"code","4a19f376":"code","69d327b9":"code","8c324a7d":"code","3640bcda":"code","85c357d6":"code","d6107efe":"code","ab892701":"code","896089d6":"code","5f22b460":"code","46acb07b":"markdown","abee7a28":"markdown","e9129616":"markdown","69c34dbd":"markdown","c849a2c5":"markdown","93dabfcf":"markdown","370f4e8c":"markdown","4ee26de9":"markdown","8377d034":"markdown","93973e15":"markdown","103a125f":"markdown","f5b007b0":"markdown","ba572e67":"markdown","4d4d789b":"markdown","8b849010":"markdown","098a0f6a":"markdown"},"source":{"97c0ff0e":"import pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn import metrics\nfrom sklearn.datasets import make_moons, make_circles, make_classification\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.gaussian_process import GaussianProcessClassifier\nfrom sklearn.gaussian_process.kernels import RBF\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import precision_recall_curve\nfrom sklearn.metrics import average_precision_score\nfrom sklearn.metrics import precision_recall_curve\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom inspect import signature\nfrom sklearn.metrics import *\nfrom sklearn.utils.multiclass import unique_labels\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout\nfrom keras.wrappers.scikit_learn import KerasClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom keras import optimizers\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.decomposition import PCA","c2be5e7c":"file1 = \"\/kaggle\/input\/telecom-customer\/Telecom_customer churn.csv\"","3b7b570a":"df = pd.read_csv(file1)\ndf_o = df.copy() #I just made a copy for easier experimentation during code writing.","859979db":"## all experiments for exploration are discarded as mainly it was a combination of excel and tableau before I decided to use all features.","c28da619":"##","a269add5":"df = df_o.copy()","4e8d7c35":"def plot_confusion_matrix(y_true, y_pred, classes,\n                          normalize=True,\n                          title=None,\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    if not title:\n        if normalize:\n            title = 'Normalized confusion matrix'\n        else:\n            title = 'Confusion matrix, without normalization'\n\n    # Compute confusion matrix\n    cm = confusion_matrix(y_true, y_pred)\n    # Only use the labels that appear in the data\n    #classes = classes[unique_labels(y_true, y_pred)]\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    print(cm)\n\n    fig, ax = plt.subplots()\n    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n    ax.figure.colorbar(im, ax=ax)\n    # We want to show all ticks...\n    ax.set(xticks=np.arange(cm.shape[1]),\n           yticks=np.arange(cm.shape[0]),\n           title=title,\n           ylabel='True label',\n           xlabel='Predicted label')\n\n    # Rotate the tick labels and set their alignment.\n    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n             rotation_mode=\"anchor\")\n\n    # Loop over data dimensions and create text annotations.\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() \/ 2.\n    for i in range(cm.shape[0]):\n        for j in range(cm.shape[1]):\n            ax.text(j, i, format(cm[i, j], fmt),\n                    ha=\"center\", va=\"center\",\n                    color=\"white\" if cm[i, j] > thresh else \"black\")\n    fig.tight_layout()\n    return ax\n\ndef plot_roc(y_true, y_pred, title):\n    fpr, tpr, thresholds = metrics.roc_curve(y_test, y_pred)\n    #print(fpr, tpr, prec, rec)\n    plt.plot(fpr, tpr)\n    plt.plot(fpr,tpr,linestyle = \"dotted\",\n             color = \"royalblue\",linewidth = 2,\n             label = \"AUC = \" + str(np.around(roc_auc_score(y_true,y_pred),3)))\n    plt.legend(loc='best')\n    plt.plot([0,1], [0,1])\n    plt.xticks(np.arange(0,1.1,0.1))\n    plt.yticks(np.arange(0,1.1,0.1))\n    plt.grid(b=True, which='both')\n    plt.title(title)\n    plt.xticks(np.arange(0, 1.1, 0.1))\n    plt.yticks(np.arange(0, 1.1, 0.1))\n    plt.show()","37bfb864":"# first of all, lets try to see the non-numeric fields.\n# list them down and explore 1 by 1 please\ndf.columns[df.dtypes.values == \"object\"]","28e49b41":"categ_nominal = ['new_cell', 'asl_flag', 'prizm_social_one', 'area', 'dualband', 'refurb_new', 'hnd_webcap',\n                 'ownrent', 'dwlltype', 'marital', 'infobase', 'HHstatin', 'dwllsize', 'ethnic',\n                 'kid0_2', 'kid3_5', 'kid6_10', 'kid11_15', 'kid16_17', 'creditcd']\n\ncateg_ordinal_ordered = ['crclscod']  #order this as alphabet sort and then assign numbers.","2df2bfee":"#pd.get_dummies(df['new_cell'], prefix=\"new_cell\", dummy_na=True)\n\n#df_p = df.copy()\n\nfor i in categ_nominal:\n    df = pd.concat([df, pd.get_dummies(df[i], prefix=i, dummy_na=True)], sort=False, axis=1)","142b62e7":"df = df.drop(categ_nominal, axis=1)","65a7be38":"for i in categ_ordinal_ordered:\n    s_a = sorted(df[i].unique())\n    s_a_dict = {i:x for x,i in enumerate(s_a)}\n    df[i] = df[i].map(s_a_dict)","6507e8a4":"df.columns[df.dtypes.values == \"object\"] #recheck if there are any more object types remaining in the dataframe","67c4c081":"df['vce_blk_rate'] = 0\ndf.loc[ df['plcd_vce_Mean'] > 0, 'vce_blk_rate'] = df['blck_vce_Mean'] \/ df['plcd_vce_Mean']\n\ndf['vce_drp_rate'] = 0\ndf.loc[ df['plcd_vce_Mean'] > 0, 'vce_drp_rate'] = df['drop_vce_Mean'] \/ df['plcd_vce_Mean']\n\ndf['dat_blk_rate'] = 0\ndf.loc[ df['plcd_dat_Mean'] > 0, 'dat_blk_rate'] = df['blck_dat_Mean'] \/ df['plcd_dat_Mean']\n\ndf['dat_drp_rate'] = 0\ndf.loc[ df['plcd_dat_Mean'] > 0, 'dat_drp_rate'] = df['drop_dat_Mean'] \/ df['plcd_dat_Mean']\n\ndf['vce_cmpt_rate'] = 0\ndf.loc[ df['plcd_vce_Mean'] > 0, 'vce_cmpt_rate'] = df['comp_vce_Mean'] \/ df['plcd_vce_Mean']\n\ndf['dat_cmpt_rate'] = 0\ndf.loc[ df['plcd_dat_Mean'] > 0, 'dat_cmpt_rate'] = df['comp_dat_Mean'] \/ df['plcd_dat_Mean']\n\ndf['tot_cmpt_rate'] = 0\ndf.loc[ df['attempt_Mean'] > 0, 'tot_cmpt_rate'] = df['complete_Mean'] \/ df['attempt_Mean']\n\ndf['tot_drp_blk_rate'] = 0\ndf.loc[ df['attempt_Mean'] > 0, 'tot_drp_blk_rate'] = df['drop_blk_Mean'] \/ df['attempt_Mean']\n\ndf['vce_dat_ratio'] = 0\ndf.loc[ (df['plcd_vce_Mean'] + df['plcd_dat_Mean']) > 0, 'tot_drp_blk_rate'] = df['plcd_vce_Mean'] \/  (df['plcd_vce_Mean'] + df['plcd_dat_Mean'])\n\ndf['diff_3mon_overall_mou'] = 0\ndf.loc[ (df['avgmou'] == df['avgmou']) & (df['avg3mou'] == df['avg3mou']), 'diff_3mon_overall_mou'] = (df['avg3mou'] - df['avgmou']) \/ df['avgmou']\n\ndf['diff_3mon_overall_qty'] = 0\ndf.loc[ (df['avgqty'] == df['avgqty']) & (df['avg3qty'] == df['avg3qty']), 'diff_3mon_overall_qty'] = (df['avg3qty'] - df['avgqty']) \/ df['avgqty']\n\ndf['diff_3mon_overall_rev'] = 0\ndf.loc[ (df['avgrev'] == df['avgrev']) & (df['avg3rev'] == df['avg3rev']), 'diff_3mon_overall_rev'] = (df['avg3rev'] - df['avgrev']) \/ df['avgrev']\n\ndf['diff_6mon_overall_mou'] = 0\ndf.loc[ (df['avgmou'] == df['avgmou']) & (df['avg6mou'] == df['avg6mou']), 'diff_6mon_overall_mou'] = (df['avg6mou'] - df['avgmou']) \/ df['avgmou']\n\ndf['diff_6mon_overall_qty'] = 0\ndf.loc[ (df['avgqty'] == df['avgqty']) & (df['avg6qty'] == df['avg6qty']), 'diff_6mon_overall_qty'] = (df['avg6qty'] - df['avgqty']) \/ df['avgqty']\n\ndf['diff_6mon_overall_rev'] = 0\ndf.loc[ (df['avgrev'] == df['avgrev']) & (df['avg6rev'] == df['avg6rev']), 'diff_6mon_overall_rev'] = (df['avg6rev'] - df['avgrev']) \/ df['avgrev']\n\ndf['total_nulls'] = 0\ndf.loc[:, 'total_nulls'] = np.sum(pd.isnull(df), axis=1)\n\ndf['eqpdays_digitized'] = np.digitize(df['eqpdays'], bins=[-10, 30, 60, 90, 120, 150, 180, 210, 240, 270, 300, 330, 360, 390, 5000])","0d2102fc":"corr_mat = df.corr(method='spearman')\n# nothing\n#sorted(corr_mat['churn'], reverse=True)\n\n# nothing\n","e72ae8da":"corr_mat.loc[ corr_mat['churn'] == 1, 'churn'] = np.nan\ndev = 1\ns_6 = corr_mat['churn'].mean() + corr_mat['churn'].std() * dev\ns__6 = corr_mat['churn'].mean() - corr_mat['churn'].std() * dev\nprint(s_6, s__6)\n","0e5467dc":"reduced_features = corr_mat[(corr_mat['churn'] >= s_6) | (corr_mat['churn'] <= s__6)].index.values.tolist()\nreduced_features.extend(['churn'])","2d11947c":"df = df[reduced_features].copy() #reducing the features based on standard deviation on the spearman correlation","596d54a5":"df_o = df.copy()","0d3eb0ac":"RANDOM_STATE = 91\nTEST_SIZE = 0.3\nmodels = dict() #trained models will be kept in this dict as \"ModelName\": Model\n\nclassifiers = {\n    \"Decision trees\" : DecisionTreeClassifier(max_depth=5),\n    \"Random Forest\" : RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),\n    \"Adaboost\" : AdaBoostClassifier(n_estimators=200, learning_rate=0.01),\n}","d96b6bcf":"df = df_o.copy() #restore the worked on dataset for multiple running of only below cells\n","ac525abb":"from sklearn.impute import SimpleImputer\n#Check different NA handling options.\n\nOPTION = 4\nTECH = 'median' #technique to apply for OPTION 2 setting\n\nif OPTION == 1:\n    #Option 1, drop na\n    df = df.dropna(axis=1)\nelif OPTION == 2:\n    #Take a fill approach\n    #NA handling using different techniques just to see the impact.\n    if TECH == 'median':\n        df = df.fillna(value=df.median())\n        print(\"Applying median to all nan values.\")\n    elif TECH == 'mean':\n        df = df.fillna(value=df.mean())\n        print(\"Applying mean to all nan values.\")\n    elif TECH == 'mode':\n        df = df.fillna(value=df.mode())\n        print(\"Applying mode to all nan values.\")\n    else:\n        df = df.fillna(value=0)\n        print(\"Applying 0 to all nan values.\")\nelif OPTION == 3: #impute\n    my_imputer = SimpleImputer()\n    df_i = my_imputer.fit_transform(df)\n    df = pd.DataFrame(df_i, columns=df.columns.values)\nelif OPTION == 4:\n    # make copy to avoid changing original data (when Imputing)\n    new_data = df.copy()\n\n    # make new columns indicating what will be imputed\n    cols_with_missing = (col for col in new_data.columns if new_data[col].isnull().any())\n    for col in cols_with_missing:\n        new_data[col + '_was_missing'] = new_data[col].isnull()\n    # Imputation\n    my_imputer = SimpleImputer(strategy=TECH)\n    #print(\"new_data now \", new_data.shape, \" columns \", new_data.columns.values)\n    new_data = pd.DataFrame(my_imputer.fit_transform(new_data), columns=new_data.columns)\n    #print(\"new_data now \", new_data.shape, \" columns \", new_data.columns.values)\n    #new_data.columns = df.columns\n    df = new_data.copy()\n    #print(\"DF shape now \", df.shape, \" columns \", df.columns.values)","21c608f4":"from sklearn.preprocessing import MinMaxScaler\nsc = MinMaxScaler()\n\ndf_X = sc.fit_transform(df)\ndf = pd.DataFrame(df_X, columns=df.columns.values)","5fac375f":"y = df['churn']\n# make sure that Customer_ID is not present in the data set as ideally this should not be a feature to predict churn if data was well randomized\n# so we will drop the churn column and the Customer_ID (if exists) from the dataframe for training purposes.\ndf = df.drop([x for x in ['churn', 'Customer_ID'] if x in df.columns], axis=1)","4a19f376":"pca = PCA(n_components=2, svd_solver='full')\ndf_pca = pca.fit_transform(df)\n\n\n\ndf_o[\"pca1\"] = df_pca[:, 0]\ndf_o[\"pca2\"] = df_pca[:, 1]\n\nplt.subplots(figsize=(10,8))\nsns.scatterplot(data=df_o, x=\"pca1\", y=\"pca2\", hue=\"churn\")\nplt.show()","69d327b9":"X_train, X_test, y_train, y_test = \\\n        train_test_split(df, y, test_size=TEST_SIZE, random_state=RANDOM_STATE)","8c324a7d":"import xgboost as xgb\n\ndtrain = xgb.DMatrix(X_train, label=y_train)\ndtest = xgb.DMatrix(X_test, label=y_test)\n\nparam = {'max_depth': 9, 'objective':'binary:hinge', 'grow_policy' : 'lossguide', 'predictor' : 'gpu_predictor',\n        'booster' : 'dart', 'rate_drop' : 0.02, 'tree_method' : 'gpu_hist'}\n\nparam_2 = {'booster': 'dart',\n         'max_depth': 8, 'learning_rate': 0.1,\n         'objective': 'binary:logistic',\n         'sample_type': 'weighted',#select dropped trees based on weight\n         'normalize_type': 'tree',\n         'rate_drop': 0.1,\n         'skip_drop': 0.5}\n\nwatchlist = [(dtest, 'eval'), (dtrain, 'train')]\nnum_round = 200\n\n\nbst = xgb.train(param_2, dtrain, num_round, watchlist)\nmodels['XGB'] = bst\nplot_roc(y_test, bst.predict(dtest), \"XGB\")","3640bcda":"## Warning, this is a pretty big model for a simple task. But, no matter the configurations tried, I was unable to make it perform better than the XGB.\nmodel = Sequential()\nmodel.add(Dense(512, input_shape=(X_train.shape[1],), activation='relu'))\nmodel.add(Dropout(0.3))\nmodel.add(Dense(256, activation='relu'))\nmodel.add(Dropout(0.3))\nmodel.add(Dense(128, activation='relu'))\nmodel.add(Dropout(0.3))\nmodel.add(Dense(64, activation='relu'))\nmodel.add(Dropout(0.3))\nmodel.add(Dense(32, activation='relu'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(16, activation='relu'))\nmodel.add(Dropout(0.3))\nmodel.add(Dense(1, activation='sigmoid'))\n\n# Compile model\nmodel.compile(loss='binary_crossentropy', optimizer=optimizers.RMSprop(lr=0.0001), metrics=['accuracy'], )\n\nmodel.fit(X_train, y_train, epochs=20, batch_size=20, validation_data=(X_test, y_test))\nmodels['Keras'] = model\n\nplot_roc(y_test, model.predict(X_test), title=\"Keras \")","85c357d6":"incorrect_indexes = []\n\n\nfor name in classifiers.keys():\n    print(\"Starting with \", name)\n    clf = classifiers[name]\n    clf.fit(X_train, y_train)\n    score = clf.score(X_test, y_test)\n    y_pred = clf.predict(X_test)\n    prec = precision_score(y_test, y_pred)\n    rec = recall_score(y_test, y_pred)\n    CM = confusion_matrix(y_test, y_pred)\n    print(X_test[y_pred != y_test].index.values)\n    print(\"Name {}, Score {}, Precision {}, Recall {}\".format(name, score, prec, rec))\n    print(CM)\n    np.set_printoptions(precision=2)\n\n    # Plot non-normalized confusion matrix\n    plot_confusion_matrix(y_test, y_pred, classes=[\"loyal\", \"churn\"],\n                          title='Confusion matrix, with normalization', normalize=True)\n    plt.show()\n    \n    models[name] = clf\n    \n    plot_roc(y_test, y_pred, title=\"Clf:\" + name)\n    ","d6107efe":"from sklearn.ensemble import GradientBoostingClassifier\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.ensemble import VotingClassifier\n\nclflr = LogisticRegression(solver='lbfgs', multi_class='multinomial',\n                          random_state=1)\n\nclfrf = RandomForestClassifier(n_estimators=300, random_state=1)\n\nclfgb = GradientBoostingClassifier(learning_rate=0.05)\n\nclfad = AdaBoostClassifier(n_estimators=300)\n\n","ab892701":"eclf1 = VotingClassifier(estimators=[\n        ('lr', clflr), ('rf', clfrf), ('gnb', clfgb), ('adab', clfad)], voting='soft')\n\neclf1 = eclf1.fit(X_train, y_train)\n\ny_pred = eclf1.predict(X_test)\n\nmodels['VotingSoft'] = eclf1\nplot_roc(y_test, y_pred, \"Voting - Soft\")","896089d6":"eclf2 = VotingClassifier(estimators=[\n        ('lr', clflr), ('rf', clfrf), ('gnb', clfgb), ('adab', clfad)], voting='hard')\n\neclf2 = eclf1.fit(X_train, y_train)\n\ny_pred = eclf1.predict(X_test)\n\nmodels['VotingHard'] = eclf1\nplot_roc(y_test, y_pred, \"Voting - Hard\")","5f22b460":"for mname in models.keys():\n    print(mname)\n    if mname.lower().find(\"xgb\") > -1:\n        plot_roc(y_test, models[mname].predict(dtest), title=mname) #xgb has a different dtest\n    else:\n        plot_roc(y_test, models[mname].predict(X_test), title=mname)\n    ","46acb07b":"# We will now reset the df and start the feature processing. (Exploration work will be unsaved)","abee7a28":"# Let us try a Voting Classifier as well","e9129616":"# XGBoost Below","69c34dbd":"# RandomForest, DecisionTrees, Adaboost below","c849a2c5":"# Finally the end. We will now try to plot the ROC curves for all the classifiers in one place","93dabfcf":"# Feature scaling. Simple approach to MinMaxScaler choosen here as the distribution of all the columns is almost similar with limited outliers.","370f4e8c":"#### Warning, this is a pretty big model for a simple task. But, no matter the configurations tried, I was unable to make it perform better than the XGB. Please if someone can make a better DNN for this classification, kindly let me know.","4ee26de9":"# **ML Starting below**","8377d034":"#### I believe that seeing the PCA plots, the data correlation for churn and non-churn is too high for models to pick up better patterns. Another possibility is that I am missing some clear opportunities to make the model better. (feature engineering maybe??)","93973e15":"# Perform any data exploration below.","103a125f":"### Separate the target label from the dataset.","f5b007b0":"# Below are the new features we are building","ba572e67":"# This is a simplistic attempt to get basic baseline from different models\n## Models used include\n* XGBoost\n* Keras\n* RandomForest\n* DecisionTrees\n* AdaBoost\n* VotingClassifier (Hard + Soft)\n* Others tried but discarded include MLP, Gaussian, NB \n\n## New Feature were created as well\n* Current code only keeps the features which provide a spearman correlation of +\/-1 sigma to churn. Everything else is dropped. You can experiment with all features or change the sigma threshold in the code (I did not get any major difference in results)","4d4d789b":"# Keras Model Below","8b849010":"# Train test split. (For now no stratified splitting is happening)","098a0f6a":"# Optional PCA plot below (2-d). tSNE is too time consuming."}}