{"cell_type":{"53aed47a":"code","699e66e2":"code","ea61b77d":"code","5ff60638":"code","a046201d":"code","57e78097":"code","01f9f6d5":"code","a8887567":"code","c5b618fa":"code","2ff8732d":"code","bfd1e11f":"code","4c98f565":"code","3ee31558":"code","6feed4f0":"code","e9898b47":"code","2ddd5279":"code","5c0745e5":"code","baa181e5":"code","54b408b8":"code","bb18140e":"markdown","3888237a":"markdown"},"source":{"53aed47a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","699e66e2":"import numpy as np\nimport pandas as pd\nimport graphviz\nimport seaborn as sns\nfrom sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor, export_graphviz\nfrom sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\nimport os\nimport missingno as msno","ea61b77d":"df = pd.read_csv(\"..\/input\/winequality-red.csv\")\ndf.head()","5ff60638":"df.tail()","a046201d":"df.describe()","57e78097":"import matplotlib.pyplot as plt\n# msno.bar(df)\n# plt.show()","01f9f6d5":"\nmsno.bar(df)\nplt.show()","a8887567":"df.info()","c5b618fa":"import graphviz","2ff8732d":"import matplotlib.pyplot as plt","bfd1e11f":"# plt.rc('axis',lw = 1.5)\n# plt.rc('xtick', labelsize = 14)\n# plt.rc('ytick', labelsize = 14)\n# plt.rc('xtick.major', size = 5, width = 3)\n# plt.rc('ytick.major', size = 5, width = 3)","4c98f565":"from sklearn.datasets import load_wine\nwine = load_wine()\nX= wine.data\ny = wine.target","3ee31558":"tree = DecisionTreeClassifier(max_depth = 2, random_state = 0)\ntree.fit(X,y)","6feed4f0":"import graphviz","e9898b47":"dot_data = export_graphviz(tree,\n                out_file = None,\n                feature_names = wine.feature_names,\n                class_names=wine.target_names,\n                rounded = True,\n                filled = True)\n\ngraph = graphviz.Source(dot_data)\ngraph.render() \ngraph","2ddd5279":"wine = load_wine()\nX = wine.data[:,[6,12]] # flavanoids and proline\ny = wine.target\n\n# random_state is set to guarantee consistent result. You should remove it when running your own code.\ntree1 = DecisionTreeClassifier(random_state=5) \ntree1.fit(X,y)","5c0745e5":"# preparing to plot the decision boundaries\nx0min, x0max = X[:,0].min()-1, X[:,0].max()+1\nx1min, x1max = X[:,1].min()-10, X[:,1].max()+10\nxx0, xx1 = np.meshgrid(np.arange(x0min,x0max,0.02),np.arange(x1min, x1max,0.2))\nZ = tree1.predict(np.c_[xx0.ravel(), xx1.ravel()])\nZ = Z.reshape(xx0.shape)","baa181e5":"plt.subplots(figsize=(12,10))\nplt.contourf(xx0, xx1, Z, cmap=plt.cm.RdYlBu)\nplot_colors = \"ryb\"\nn_classes = 3\nfor i, color in zip(range(n_classes), plot_colors):\n    idx = np.where(y == i)\n    plt.scatter(X[idx, 0], X[idx, 1], c=color, label=wine.target_names[i],\n                cmap=plt.cm.RdYlBu, edgecolor='black', s=30)\nplt.legend(fontsize=18)\nplt.xlabel('flavanoids', fontsize = 18)\nplt.ylabel('proline', fontsize = 18)\nplt.show()","54b408b8":"# limit maximum tree depth\ntree1 = DecisionTreeClassifier(max_depth=3,random_state=5) \ntree1.fit(X,y)\n\n# limit maximum number of leaf nodes\ntree2 = DecisionTreeClassifier(max_leaf_nodes=4,random_state=5) \ntree2.fit(X,y)\n\nx0min, x0max = X[:,0].min()-1, X[:,0].max()+1\nx1min, x1max = X[:,1].min()-10, X[:,1].max()+10\nxx0, xx1 = np.meshgrid(np.arange(x0min,x0max,0.02),np.arange(x1min, x1max,0.2))\n\nZ1 = tree1.predict(np.c_[xx0.ravel(), xx1.ravel()])\nZ1 = Z1.reshape(xx0.shape)\nZ2 = tree2.predict(np.c_[xx0.ravel(), xx1.ravel()])\nZ2 = Z2.reshape(xx0.shape)\n\nfig,ax = plt.subplots(nrows=1, ncols=2, figsize=(15,6))\nax[0].contourf(xx0, xx1, Z1, cmap=plt.cm.RdYlBu)\nax[1].contourf(xx0, xx1, Z2, cmap=plt.cm.RdYlBu)\nplot_colors = \"ryb\"\nn_classes = 3\nfor i, color in zip(range(n_classes), plot_colors):\n    idx = np.where(y == i)\n    ax[0].scatter(X[idx, 0], X[idx, 1], c=color, label=wine.target_names[i],\n                cmap=plt.cm.RdYlBu, edgecolor='black', s=30)\n    ax[1].scatter(X[idx, 0], X[idx, 1], c=color, label=wine.target_names[i],\n                cmap=plt.cm.RdYlBu, edgecolor='black', s=30)\nax[0].legend(fontsize=14)\nax[0].set_xlabel('flavanoids', fontsize = 18)\nax[0].set_ylabel('proline', fontsize = 18)\nax[0].set_ylim(260,1690)\nax[0].set_title('max_depth = 3', fontsize = 14)\nax[1].legend(fontsize=14)\nax[1].set_xlabel('flavanoids', fontsize = 18)\nax[1].set_ylabel('proline', fontsize = 18)\nax[1].set_ylim(260,1690)\nax[1].set_title('max_leaf_nodes = 4', fontsize = 14)\nplt.show()","bb18140e":"we can plot decision boundaries as follows","3888237a":"As you can see, the root node (depth=0) chose \"proline\" as the splitting feature, and 755.0 as the threshold value. The two depth 1 nodes chose \"od280\/od315_of_diluted_wines\" and \"flavanoids\" as the their splitting feature, respectively. The four leaf nodes (depth=2) each predicts a class - two of them predict class_2, one predicts class_1 and one predicts class_0. You can also see other node attributes such as \"gini\"and \"value\" shown in the graph. These attributes are explained below:\nIn each node, \"samples\" represents how many training instances fall in that node. For example, there are 67 samples that has proline > 755.0. \nThe attribute \"value\" gives how many instances fall in each class for each node. For example, in the leftmost leaf node, there are 0 instances in class_0, 6 instances in class_1, and 40 instances in class_2. \nEach node is also assigned a \"class\" based on which class has the majority of the instances in that node. Therefore, even though the root node has comparable number of instances in each of the three classes, the root node's class label is \"class_1\" since 71 > 59 > 48. \nTo make a prediction, you keep going along the tree until you reach a leaf node. The probability of each class is the fraction of training samples of each class in a leaf node. For example, the bottom left leaf node predicts class_2 with a probaility of \n40\n46\n\n\u2248\n4046\u2248\n87.0%. This attribute can be obtained through DecisionTreeClassifier's predict_proba() function. Note that all samples that fall in the same leaf node share the same prediction probability."}}