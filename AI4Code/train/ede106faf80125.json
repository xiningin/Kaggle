{"cell_type":{"6519b5ac":"code","517639cf":"code","123fc460":"code","fa5aae2e":"code","0ee8e86f":"code","f777d303":"code","40d8b348":"code","fa3d5257":"code","a2b316e5":"code","30ee85d6":"code","3157765d":"code","dee417fa":"code","fb860822":"code","69c54639":"code","c26d5b9a":"code","967432a9":"code","f092ae8f":"code","f6f6fe11":"code","cab08036":"code","1b7ead70":"code","b9848e77":"code","821d147d":"code","9dd91045":"code","12c302f5":"code","fcaf5e7d":"code","4b621f77":"code","35bf086f":"code","08b525e8":"code","5e053601":"code","49d4ae59":"code","e73c31b9":"code","6142e8cd":"code","c122b87d":"code","4045cb9c":"code","288a863a":"code","a2a5b424":"code","27b138a0":"code","c4f2a4d7":"code","3a1d7ecc":"code","80b68cc0":"code","cb1d4c08":"code","03fd33d7":"code","3ed0d7e6":"code","514c6e89":"markdown","ff579f59":"markdown","7330d703":"markdown","a8c7895e":"markdown","9258a4d4":"markdown"},"source":{"6519b5ac":"import numpy as np\nimport pandas as pd\n\nfrom sklearn.feature_selection import mutual_info_regression\nfrom sklearn.preprocessing import LabelEncoder, OrdinalEncoder, StandardScaler\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.decomposition import PCA\nfrom sklearn.mixture import GaussianMixture\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, VotingRegressor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.svm import SVR\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom xgboost import XGBRegressor","517639cf":"pd.set_option('display.max_columns', 500)\nnp.set_printoptions(precision=2, suppress = True)\npd.set_option(\"precision\", 2)","123fc460":"input_data = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\nval_data = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')","fa5aae2e":"X = input_data.iloc[:,1:-1]\nY = input_data.iloc[:,-1]","0ee8e86f":"X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2) ","f777d303":"def one_hot_encode(df, col):\n    df1 = pd.get_dummies(df[col], prefix = col)\n    df = pd.concat([df, df1])\n    return df\n\ndef one_hot_encode_and_mul(df, cat_col, num_col):\n    df1 = pd.get_dummies(df[cat_col], prefix = cat_col)\n    df2 = df1.mul(df[num_col], axis = 0)\n    df = pd.concat([df, df2], axis = 1)\n    df = df.drop([cat_col, num_col], axis = 1)\n    return df","40d8b348":"X_preprocessed = X_train.copy()\nX_test_preprocessed = X_test.copy()\nval_preprocessed = val_data.copy()","fa3d5257":"sns.lmplot(data = input_data, x = 'BsmtFinSF1', y = 'SalePrice', col = 'BsmtFinType1', sharex = False)","a2b316e5":"X_preprocessed = one_hot_encode_and_mul(X_preprocessed, 'BsmtFinType1', 'BsmtFinSF1')\nX_test_preprocessed = one_hot_encode_and_mul(X_test_preprocessed, 'BsmtFinType1', 'BsmtFinSF1')\nval_preprocessed = one_hot_encode_and_mul(val_preprocessed, 'BsmtFinType1', 'BsmtFinSF1')","30ee85d6":"features = X_preprocessed.columns","3157765d":"X_preprocessed.head()","dee417fa":"numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n\nnumerical_features = list(X_preprocessed.select_dtypes(include=numerics).columns) ","fb860822":"X_preprocessed.replace(['Ex', 'Gd', 'TA', 'Av', 'Fa', 'Po', 'Mn', 'Yes', 'No', 'NA'], \n                       [5,    4,    3,    3,    2,    1,    1,    1,     0,    0], inplace = True)\nX_test_preprocessed.replace(['Ex', 'Gd', 'TA', 'Av', 'Fa', 'Po', 'Mn', 'Yes', 'No', 'NA'], \n                       [5,    4,    3,    3,    2,    1,    1,    1,     0,    0], inplace = True)\nval_preprocessed.replace(['Ex', 'Gd', 'TA', 'Av', 'Fa', 'Po', 'Mn', 'Yes', 'No', 'NA'], \n                       [5,    4,    3,    3,    2,    1,    1,    1,     0,    0], inplace = True)","69c54639":"num_imputer = SimpleImputer(strategy = 'median', missing_values = np.NaN)\nscale = StandardScaler()\ncat_imputer = SimpleImputer(strategy = 'most_frequent', missing_values = np.NaN)\noe = OrdinalEncoder()","c26d5b9a":"for col in numerical_features:\n    X_preprocessed[col] = num_imputer.fit_transform(X_preprocessed[[col]])\n    X_preprocessed[col] = scale.fit_transform(X_preprocessed[[col]])\n    \nfor col in features:\n    if col not in numerical_features:\n        X_preprocessed[col] = cat_imputer.fit_transform(X_preprocessed[[col]])\n        X_preprocessed[col] = oe.fit_transform(X_preprocessed[[col]])","967432a9":"for col in numerical_features:\n    X_test_preprocessed[col] = num_imputer.transform(X_test_preprocessed[[col]])\n    X_test_preprocessed[col] = scale.fit_transform(X_test_preprocessed[[col]])\n    \nfor col in features:\n    if col not in numerical_features:\n        X_test_preprocessed[col] = cat_imputer.fit_transform(X_test_preprocessed[[col]])\n        X_test_preprocessed[col] = oe.fit_transform(X_test_preprocessed[[col]])","f092ae8f":"for col in numerical_features:\n    val_preprocessed[col] = num_imputer.transform(val_preprocessed[[col]])\n    val_preprocessed[col] = scale.fit_transform(val_preprocessed[[col]])\n    \nfor col in features:\n    if col not in numerical_features:\n        val_preprocessed[col] = cat_imputer.fit_transform(val_preprocessed[[col]])\n        val_preprocessed[col] = oe.fit_transform(val_preprocessed[[col]])","f6f6fe11":"X_preprocessed.describe()","cab08036":"mi_scores = mutual_info_regression(X_preprocessed,Y_train)\nmi = pd.DataFrame(list(zip(list(features), list(mi_scores))), columns = ['features', 'mi scores'])\nmi = mi.sort_values(by = 'mi scores', ascending = False)\nthreshold = 0.05\nmi['greater'] = mi['mi scores'] > threshold\nplt.figure(figsize = (6,16))\nsns.barplot(data = mi, y = 'features', x = 'mi scores', hue = 'greater')\nplt.axvline(x = threshold, c = 'red')","1b7ead70":"high_mi_features = mi[mi['mi scores'] > threshold]['features']\nhigh_mi_data = X_preprocessed[high_mi_features]","b9848e77":"high_mi_test_data = X_test_preprocessed[high_mi_features]","821d147d":"high_mi_val_data = val_preprocessed[high_mi_features]","9dd91045":"table = X_preprocessed.corr()\nplt.figure(figsize = (12,12))\nsns.heatmap(table, center = 0, vmax = 1, vmin = -1)","12c302f5":"pca_x = X_preprocessed.copy()","fcaf5e7d":"pca = PCA(n_components = 12, copy = True)\npca.fit(pca_x)\n\npca_components = [\"pca \" + str(i) for i in range(12)]\n\nfiltered_features = features[sum(abs(pca.components_)) > 0.5]\n\npca_table = pd.DataFrame(pca.components_, index = pca_components, columns = features)\n\nplt.figure(figsize = (30,8))\nsns.heatmap(pca_table[filtered_features],\n           center = 0, annot = False)\nplt.show()","4b621f77":"plt.barh(pca_components, pca.explained_variance_ratio_)","35bf086f":"pca_x = pca.fit_transform(pca_x)\nmi_scores = mutual_info_regression(pca_x,Y_train)\nmi = pd.DataFrame(list(zip(list(pca_components), list(mi_scores))), columns = ['features', 'mi scores'])\nthreshold = 0.05\nmi['greater'] = mi['mi scores'] > threshold\nplt.figure(figsize = (6,16))\nsns.barplot(data = mi, y = 'features', x = 'mi scores', hue = 'greater')\nplt.axvline(x = threshold, c = 'red')","08b525e8":"pca_data = pd.DataFrame(pca_x[:, 0:4], columns = pca_components[0:4])","5e053601":"pca_data.describe()","49d4ae59":"pca_copy = X_test_preprocessed.copy()\npca_test_data = pca.transform(pca_copy)\npca_test_data = pd.DataFrame(pca_test_data[:, 0:4], columns = pca_components[0:4])","e73c31b9":"covariance_type = ['full', 'tied', 'diag', 'spherical']\nn_components = list(range(10, 50, 5))\n\nbic = np.zeros((len(covariance_type), len(n_components)))\n\nbest_gmm = None\nbest_bic = 0\n\nfor i in range(len(covariance_type)):\n    for j in range(len(n_components)):\n        gmm = GaussianMixture(covariance_type = covariance_type[i], n_components = n_components[j])\n        gmm.fit(X_preprocessed)\n        bic[i,j] = gmm.bic(X_preprocessed)\n        if best_bic < bic[i,j]:\n            best_gmm = gmm\n        \nsns.heatmap(bic, center = 0)","6142e8cd":"gmm_final = GaussianMixture(covariance_type = 'diag', n_components = 25)\ngmm_final.fit(X_preprocessed, Y_train)\ngmm_data = gmm_final.predict_proba(X_preprocessed)","c122b87d":"gmm_test_data = gmm_final.predict_proba(X_test_preprocessed)","4045cb9c":"def regression(input_data, test_data):    \n    linear_regression = LinearRegression()\n    linear_regression.fit(input_data, Y_train)\n    train_score = linear_regression.score(input_data, Y_train)\n    test_score = linear_regression.score(test_data, Y_test)\n\n    print('Linear regression')\n    print('Training performance -> {}'.format(train_score))\n    print('Test performance -> {}'.format(test_score))\n\n    DecisionTree = DecisionTreeRegressor(max_depth = 10)\n    DecisionTree.fit(input_data, Y_train)\n    train_score = DecisionTree.score(input_data, Y_train)\n    test_score = DecisionTree.score(test_data, Y_test)\n\n    print('\\nDecision Tree with max depth - {} and \\\"mse\\\" split criteria'.format(DecisionTree.tree_.max_depth))\n    print('Training performance -> {}'.format(train_score))\n    print('Test performance -> {}'.format(test_score))\n\n    DecisionTree = DecisionTreeRegressor(max_depth = 10, criterion = 'friedman_mse')\n    DecisionTree.fit(input_data, Y_train)\n    train_score = DecisionTree.score(input_data, Y_train)\n    test_score = DecisionTree.score(test_data, Y_test)\n\n    print('\\nDecision Tree with max depth - {} and \\\"friedman_mse\\\" split criteria'.format(DecisionTree.tree_.max_depth))\n    print('Training performance -> {}'.format(train_score))\n    print('Test performance -> {}'.format(test_score))\n\n\n    RandomForest = RandomForestRegressor(max_depth = 10)\n    RandomForest.fit(input_data, Y_train)\n    train_score = RandomForest.score(input_data, Y_train)\n    test_score = RandomForest.score(test_data, Y_test)\n\n    print('\\nRandom Forest with max depth - 10')\n    print('Training performance -> {}'.format(train_score))\n    print('Test performance -> {}'.format(test_score))\n\n    GBoost = GradientBoostingRegressor()\n    GBoost.fit(input_data, Y_train)\n    train_score = GBoost.score(input_data, Y_train)\n    test_score = GBoost.score(test_data, Y_test)\n\n    print('\\nGradient Boost with n_estimators - {} '.format(GBoost.n_estimators_))\n    print('Training performance -> {}'.format(train_score))\n    print('Test performance -> {}'.format(test_score))\n    \n    XGBoost = XGBRegressor(n_estimators  = 100, max_depth = 10, learning_rate = 0.2)\n    XGBoost.fit(input_data, Y_train)\n    train_score = XGBoost.score(input_data, Y_train)\n    test_score = XGBoost.score(test_data, Y_test)\n\n    print('\\nXGBoost with max depth - 10 and 100 estimators')\n    print('Training performance -> {}'.format(train_score))\n    print('Test performance -> {}'.format(test_score))\n\n    knn = KNeighborsRegressor(weights = 'distance')\n    knn.fit(input_data, Y_train)\n    train_score = knn.score(input_data, Y_train)\n    test_score = knn.score(test_data, Y_test)\n\n    print('\\nknn')\n    print('Training performance -> {}'.format(train_score))\n    print('Test performance -> {}'.format(test_score))\n    \n    svm = SVR(kernel = 'rbf', C = 0.5, tol = 1e-5)\n    svm.fit(input_data, Y_train)\n    train_score = svm.score(input_data, Y_train)\n    test_score = svm.score(test_data, Y_test)\n\n    print('\\nSupport Vector')\n    print('Training performance -> {}'.format(train_score))\n    print('Test performance -> {}'.format(test_score))","288a863a":"input_data = X_preprocessed\ntest_data = X_test_preprocessed\n\nregression(input_data, test_data)","a2a5b424":"input_data = high_mi_data\ntest_data = high_mi_test_data\n\nregression(input_data, test_data)","27b138a0":"input_data = pca_data\ntest_data = pca_test_data\n\nregression(input_data, test_data)","c4f2a4d7":"input_data = pd.concat([X_preprocessed.reset_index(), pca_data], axis = 1)\ntest_data = pd.concat([X_test_preprocessed.reset_index(), pca_test_data], axis = 1)\n\nregression(input_data, test_data)","3a1d7ecc":"input_data = gmm_data\ntest_data = gmm_test_data\n\nregression(input_data, test_data)","80b68cc0":"input_data = high_mi_data\ntest_data = high_mi_test_data\nGBoost = GradientBoostingRegressor()\nGBoost.fit(input_data, Y_train)","cb1d4c08":"y_pred = GBoost.predict(high_mi_val_data)","03fd33d7":"y_pred","3ed0d7e6":"pd.DataFrame(list(zip(list(val_data.iloc[:, 0]), list(y_pred))), columns = ['Id', 'SalePrice']).to_csv('submission.csv', index = False)","514c6e89":"1. PCA0 not really contributting much.\n\n2. PCA2 -- Looking very similar to total carpet area. definitely should be added to features","ff579f59":"feature will be selected in 3 ways:\n1. High Mutual Info features will be chosen for regressor\n\n2. PCA will be used to combine highly coorelated features --> high likeliness\n\n3. Gaussian Mixture will be created to check for clusters --> High likeliness of clusters","7330d703":"# Feature Engineering","a8c7895e":"Different nature for different type 1","9258a4d4":"KNN               - Rejected - data not enough probably\n\nLinear Regression - Rejected - Can't handle complex data\n\nDecision Tree     - Rejected - Overfitting and linear\n\nRandom Forest     - Rejected - Good performance. Other Regressor Outperformed\n\nXGB               - Rejected - Very good performance.\n\nGradient Boost    - Accepted - Best performance Overall."}}