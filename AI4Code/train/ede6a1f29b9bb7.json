{"cell_type":{"ec8dac07":"code","21f41c37":"code","eee69672":"code","d73e4ed0":"code","6bfe5aa2":"code","43ea5748":"code","ff4b364f":"code","9569781a":"code","1729d4c1":"code","9c00be9d":"code","e36e441d":"code","e401bed6":"code","30c55656":"code","bd30eef8":"code","b630a42e":"code","a0c83652":"code","24024833":"code","8f32663e":"code","96db5d8f":"code","1c6a2897":"code","4cfdb76f":"code","cb95ce79":"code","1a8a0230":"code","b9ca5d7f":"code","93a3d4e8":"code","f705fd71":"code","525c0789":"code","754fdcb2":"code","b5ffffdc":"code","9dbf7655":"code","eb048de8":"code","def3bc69":"code","63345d62":"code","18d680d2":"code","dba9c008":"code","51350591":"code","7f4b33e1":"code","36f6b808":"code","ee7440f7":"code","d2a3fbbb":"code","1552fd93":"code","3dfb1a76":"code","4368aecb":"code","327e4d1c":"code","e45a9bf5":"code","4aa0f350":"code","f9117e9f":"code","80007000":"code","5bbb3f42":"code","8a2ef8be":"code","3b5c391e":"code","879a889a":"code","54b1eb00":"code","0364e1fb":"code","6164bd3b":"markdown","f83ac9ac":"markdown","4b5b5234":"markdown","6fd0165a":"markdown","553883c4":"markdown","c2889d66":"markdown","122daafd":"markdown","784be036":"markdown"},"source":{"ec8dac07":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport tqdm as tqdm\nimport random\nfrom time import sleep\nimport pprint\nimport joblib\nfrom functools import partial\n\n# Suppressing warnings because of skopt verbosity\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n#plotly visualisation\nimport plotly \nimport plotly.graph_objs as go\nimport plotly.io as pio\nfrom plotly.subplots import make_subplots\nimport plotly.express as px\nfrom plotly.offline import iplot, init_notebook_mode\nimport cufflinks as cf\nimport plotly.figure_factory as ff \nfrom plotly.offline import iplot\nfrom plotly import tools\n\n\n#Machine learning\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.preprocessing import StandardScaler, RobustScaler, OrdinalEncoder, PolynomialFeatures,OneHotEncoder,FunctionTransformer\n#To Pipeline the process \nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import make_pipeline,Pipeline\nfrom sklearn.linear_model import LinearRegression, SGDRegressor, ElasticNet, Lasso, Ridge\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import learning_curve\nfrom sklearn.feature_selection import SelectKBest, f_regression\nfrom sklearn.svm import SVR\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV\nfrom sklearn.ensemble import BaggingRegressor, AdaBoostRegressor,GradientBoostingRegressor, RandomForestRegressor,  GradientBoostingRegressor\nfrom xgboost import XGBRegressor, XGBClassifier\nfrom sklearn.model_selection import StratifiedKFold, cross_validate, train_test_split, KFold, cross_val_score\nfrom sklearn.ensemble import StackingRegressor\nfrom sklearn.feature_selection import mutual_info_regression\nfrom sklearn.impute import SimpleImputer\nfrom lightgbm import LGBMRegressor\nimport xgboost as xgb\nimport lightgbm as lgb\nfrom mlxtend.regressor import StackingCVRegressor\nfrom sklearn.linear_model import SGDRegressor\n\nfrom sklearn.metrics import make_scorer\n\n# Skopt functions\nfrom skopt import BayesSearchCV\nfrom skopt import gp_minimize # Bayesian optimization using Gaussian Processes\nfrom skopt.space import Real, Categorical, Integer\nfrom skopt.utils import use_named_args # decorator to convert a list of parameters to named arguments\nfrom skopt.callbacks import DeadlineStopper # Stop the optimization before running out of a fixed budget of time.\nfrom skopt.callbacks import VerboseCallback # Callback to control the verbosity\nfrom skopt.callbacks import DeltaXStopper # Stop the optimization If the last two positions at which the objective has been evaluated are less than delta\n#tuning hyperparameters\nfrom bayes_opt import BayesianOptimization\n\n# import packages for hyperparameters tuning\nfrom hyperopt import STATUS_OK, Trials, fmin, hp, tpe,hp\nfrom hyperopt.pyll import scope\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","21f41c37":"# Load dataset\ndf_train = pd.read_csv(\"\/kaggle\/input\/30-days-of-ml\/train.csv\", index_col=0)\ndf_test =  pd.read_csv(\"\/kaggle\/input\/30-days-of-ml\/test.csv\",index_col=0)\ndf_sub =  pd.read_csv(\"\/kaggle\/input\/30-days-of-ml\/sample_submission.csv\",index_col=0)\n\nprint(f\"train set has : {df_train.shape} shape\")\nprint(f\"test set has : {df_test.shape} shape\")\nprint(f\"sample submission set has : {df_sub.shape} shape\")","eee69672":"# Add display data to see all your columns\n# Configuration to see all features\npd.set_option('display.max_row', 27)\npd.set_option('display.max_column', 27)","d73e4ed0":"# Take a look on the train dataset\ndf_train.head()","6bfe5aa2":"# Take a look on the test dataset\ndf_test.head()","43ea5748":"# Take a look on sub dataframe for the submission\ndf_sub.head()","ff4b364f":"# Merge data\ndf_test_full = pd.merge(df_test,df_sub,on = \"id\")\nprint(f\"df_test_full shape : {df_test_full.shape}\")\ndf_test_full.head()","9569781a":"df = df_train.copy()","1729d4c1":"df.dtypes.value_counts().plot.pie()\ndf.info()","9c00be9d":"#finding the unique values in each column\nfor col in df.columns:\n    print('We have {} unique values in {} column'.format(len(df[col].unique()),col))\n    print('__'*30)\n\n","e36e441d":"df.columns","e401bed6":"#describe our data\ndf[df.select_dtypes(exclude='object').columns].describe().\\\nstyle.background_gradient(axis=1,cmap=sns.light_palette('green', as_cmap=True))","30c55656":"# Observe missiing value\n#find the null values in each column\n(df.isnull().sum()\/df.shape[0]*100).sort_values(ascending=False).to_frame().rename(columns={0:'Null values'})","bd30eef8":"#lets see the correlation between columns and target column\ncorr = df.corr()\ncorr['target'].sort_values(ascending=False)[1:].to_frame()\\\n.style.background_gradient(axis=1,cmap=sns.light_palette('green', as_cmap=True))\n\n","b630a42e":"#lets create a dataframe for the numeric columns with high skewness\n\nskewness = pd.DataFrame()\n\nnum_cols = []\nfor col in df.select_dtypes(exclude='object'):\n    num_cols.append(col)\n\nskewness[['Positive Columns','Skewness(+v)']] = df[num_cols].skew().sort_values(ascending=False)[:10].reset_index()\nskewness[['Negative Columns','Skewness(-v)']] = df[num_cols].skew().sort_values(ascending=True)[:10].reset_index()\n\nskewness.columns = pd.MultiIndex.from_tuples([('Positive Skewness', 'Columns'), ('Positive Skewness', 'Skewness'),\n                                              ('Negative Skewness', 'Columns'), ('Negative Skewness', 'Skewness')])\nskewness","a0c83652":"# take a look on your target with Log distribution\nfig, axes = plt.subplots(1, 2, sharex=False, figsize=(14,5))\nsns.histplot(ax=axes[0],data=df, x=\"target\", kde=True, bins=20)\naxes[0].set_title('Normal Target Distribution')\nsns.histplot(ax=axes[1],data=df, x=np.log1p(df.target),color='g', kde=True, bins=50)\naxes[1].set_title('Log Target Distribution')","24024833":"# Take a look on the numerical distributions : float type with Log distribution\nsns.set_style('whitegrid')\n\n#  plot Numerical Data\n# Calculating required amount of rows to display all feature plots\ncolumns = df.drop([\"target\"],axis=1).select_dtypes(exclude=\"object\").columns.values\ncols = 4\nrows = len(columns) \/\/ cols + 1\n\nfig, axes = plt.subplots(ncols=cols, nrows=rows, figsize=(18,8))\n\n#fig, axes = plt.subplots(4,4, figsize=(18, 8));\nplt.subplots_adjust(hspace = 0.7, wspace=0.4)\nfig.suptitle('Numerical Float Distributions vs Log Numerical Float Distributions', fontsize=20)\n\n# Take a look on the numerical distributions\na = len(df.select_dtypes('float').columns)  # number of rows\n\nfor i,col in zip(range(a),df.drop('target',axis = 1).select_dtypes('float')):\n    sns.histplot(df[col], ax=axes[i\/\/cols][i%rows],bins=20);\n    axes[i\/\/cols][i%rows].set_title(col+' Distribution')\n\nfor i,col in zip(range(a),df.drop('target',axis = 1).select_dtypes('float')):\n    sns.histplot(np.log1p(df[col]), ax=axes[i\/\/cols][i%rows], bins=20, color='g');\n    axes[i\/\/cols][i%rows].set_title(col+' Distribution')\n\n","8f32663e":"# \"Cardinality\" means the number of unique values in a column\n# Select categorical columns with relatively low cardinality (convenient but arbitrary)\ncategorical_features  = [col for col in df.columns if\n                    df[col].nunique() <= 15 and \n                    df[col].dtype == \"object\"]\n\n# Select numerical columns\nnumeric_features  = [col for col in df.columns if \n                df[col].dtype in ['int64', 'float64']\n                 ]\n\n# Keep selected columns only\nmy_features = categorical_features + numeric_features\n\n#\nprint('categorical_features:', categorical_features)\nprint('numeric_features:', numeric_features)\nprint('my_features:', my_features)\n\n\n#remove target column from Numeric features\nnumeric_features.remove('target')\nprint('numeric_features minus target column:', numeric_features)","96db5d8f":"## Box Plot for Outliers\nfig = plt.figure(figsize=(18,6))\nsns.boxplot(data=df[numeric_features], orient=\"h\", palette=\"Set2\");\nplt.xticks(fontsize= 14)\nplt.title('Box plot of numerical columns', fontsize=16);","1c6a2897":"sns.boxplot(data=df[['target']], orient=\"h\", palette=\"Set2\" );\nplt.xticks(fontsize= 14)\nplt.title('Box plot of target column', fontsize=16);","4cfdb76f":"# Deal with Outliers\n\nfrom scipy import stats\n\n\ndef treatoutliers(df=None, columns=None, factor=1.5, method='IQR', treatment='cap'):\n\n    for column in columns:\n        if method == 'STD':\n            permissable_std = factor * df[column].std()\n            col_mean = df[column].mean()\n            floor, ceil = col_mean - permissable_std, col_mean + permissable_std\n        elif method == 'IQR':\n            Q1 = df[column].quantile(0.25)\n            Q3 = df[column].quantile(0.75)\n            IQR = Q3 - Q1\n            floor, ceil = Q1 - factor * IQR, Q3 + factor * IQR\n#         print(floor, ceil)\n        if treatment == 'remove':\n            print(treatment, column)\n            df = df[(df[column] >= floor) & (df[column] <= ceil)]\n        elif treatment == 'cap':\n            print(treatment, column)\n            df[column] = df[column].clip(floor, ceil)\n\n    return df\n    \n#remove outliere from target column \n#for colName in [['target']]:\n    #X_full = treatoutliers(df=X_full,columns=colName, treatment='remove')         \n    \n#Quantile-based Flooring and Capping\nfor colName in [['target','cont0','cont6','cont8']]:\n    df = treatoutliers(df=df,columns=colName, treatment='cap')      \n    \ndf.info()\n\n","cb95ce79":"sns.boxplot(data=df[['target']], orient=\"h\", palette=\"Set2\" );\nplt.xticks(fontsize= 14)\nplt.title('Box plot of target column after handling Outliers', fontsize=16);\n","1a8a0230":"## Box Plot for Outliers\nfig = plt.figure(figsize=(18,6))\nsns.boxplot(data=df[numeric_features], orient=\"h\", palette=\"Set2\");\nplt.xticks(fontsize= 14)\nplt.title('Box plot of numerical columns after handling Outliers', fontsize=16);","b9ca5d7f":"# take a look on object variables\nfor col in df.select_dtypes(\"object\").columns:\n    print(f\"{col} ----------------{df[col].unique()}\")","93a3d4e8":"# new feature :\ndf['target_bins'] = pd.cut(df['target'],bins=6, labels=False)\ndf['target_bins'].value_counts()\nsns.scatterplot(x='target_bins',y='target',data=df)\n","f705fd71":"dico_bins_target = {\n    0 : '0-2',\n    1 : '2-4',\n    2 : \"4-6\",\n    3 : \"6-8\",\n    4 : \"7-8\",\n    5 : \"9-10\"\n}\n\ndf['target_bins'] = df['target_bins'].map(dico_bins_target)","525c0789":"# Creat a new function to better visualization of our categorical features\n\ndef visualisation_data(dataset):\n    \n    #Visualization on your Data\n\n    #  plot Numerical Data\n\n    a = len(dataset.select_dtypes(include='object').columns)  # number of rows\n    b = 2  # number of columns\n    c = 1  # initialize plot counter\n\n\n    fig = plt.figure(figsize=(22,28))\n    # Adding some distance between plots\n    plt.subplots_adjust(hspace = 0.3)\n    \n    for i in dataset.select_dtypes(include='object'):\n        if i != 'target_bins':\n            plt.subplot(a, b, c)\n            sns.heatmap(pd.crosstab(df['target_bins'], dataset[i]), annot=True, fmt='d')\n            plt.xlabel(i)\n            c = c + 1\n            \n            plt.subplot(a, b, c)\n            sns.countplot(x=i, palette=\"ch:.25\", data=dataset)\n            plt.xlabel(i)\n            c = c + 1\n            \n    plt.show()","754fdcb2":"%%time\nvisualisation_data(df)","b5ffffdc":"# Colors to be used for plots\ncolors = [\"lightcoral\", \"sandybrown\", \"darkorange\", \"mediumseagreen\",\n          \"lightseagreen\", \"cornflowerblue\", \"mediumpurple\", \"palevioletred\",\n          \"lightskyblue\", \"sandybrown\", \"yellowgreen\", \"indianred\",\n          \"lightsteelblue\", \"mediumorchid\", \"deepskyblue\"]\n\n","9dbf7655":"columns = df.drop([\"target\"], axis=1).columns.values\n\n# Calculating required amount of rows to display all feature plots\ncols = 4\nrows = len(columns) \/\/ cols + 1\n\nfig, axs = plt.subplots(ncols=cols, nrows=rows, figsize=(16,20), sharex=False)\n\n# Adding some distance between plots\nplt.subplots_adjust(hspace = 0.3)\n\ni=0\nfor r in np.arange(0, rows, 1):\n    for c in np.arange(0, cols, 1):\n        if i >= len(columns):\n            axs[r, c].set_visible(False)\n        else:\n            scatter = axs[r, c].scatter(df[columns[i]].values,\n                                        df[\"target\"],\n                                        color=random.choice(colors))\n            axs[r, c].set_title(columns[i], fontsize=14, pad=5)\n            axs[r, c].tick_params(axis=\"y\", labelsize=11)\n            axs[r, c].tick_params(axis=\"x\", labelsize=11)\n                                  \n        i+=1\nplt.show()","eb048de8":"# Encoding categorical features with OrdinalEncoder\n\nfor col in df.select_dtypes('object'):\n    encoder = OrdinalEncoder()\n    df[col] = encoder.fit_transform(np.array(df[col]).reshape(-1, 1))\n\n# Calculatin correlation values\ndf = df.corr().round(2)\n\n# Mask to hide upper-right part of plot as it is a duplicate\nmask = np.zeros_like(df)\nmask[np.triu_indices_from(mask)] = True\n\n# Making a plot\nplt.figure(figsize=(14,14))\nax = sns.heatmap(df, annot=True, mask=mask, cmap=\"RdBu\", annot_kws={\"weight\": \"normal\", \"fontsize\":9})\nax.set_title(\"Feature correlation heatmap\", fontsize=17)\nplt.setp(ax.get_xticklabels(), rotation=90, ha=\"right\",\n         rotation_mode=\"anchor\", weight=\"normal\")\nplt.setp(ax.get_yticklabels(), weight=\"normal\",\n         rotation_mode=\"anchor\", rotation=0, ha=\"right\")\nplt.show();\n\n","def3bc69":"def feature_engineering(df):\n    \n    #df['cont50'] = df['cont5'] + df['cont0']\n    #df['cont70'] = df['cont7'] + df['cont0']\n    df = df.drop(['cat4'], axis=1)\n\n    return df","63345d62":"# Remove rows with missing target, separate target from predictors\ny = df_train['target']\ndf_train.drop(['target'], axis=1, inplace=True)\n\n\n\ndf_train = feature_engineering(df_train)\ndf_test = feature_engineering(df_test)\n\n# Preview features\ndf_train.head()","18d680d2":"# Break off validation set from training data\nX_train, X_valid, y_train, y_valid = train_test_split(df_train, y, \n                                                                train_size=0.9, test_size=0.1,\n                                                                random_state=0)","dba9c008":"# \"Cardinality\" means the number of unique values in a column\n# Select categorical columns with relatively low cardinality (convenient but arbitrary)\ncategorical_features  = [cname for cname in X_train.columns if\n                    X_train[cname].nunique() <= 15 and \n                    X_train[cname].dtype == \"object\"]\n\n# Select numerical columns\nnumeric_features  = [cname for cname in X_train.columns if \n                X_train[cname].dtype in ['int64', 'float64']\n                 ]\n\n# Keep selected columns only\nmy_features = categorical_features + numeric_features\n\n#\nprint('categorical_features', categorical_features)\nprint('numeric_features', numeric_features)\nprint('my_features', my_features)","51350591":"X_train = X_train[my_features]\nX_valid = X_valid[my_features]\nX_test = df_test[my_features]\n\ndf_X_train = X_train.copy()\ndf_X_valid = X_valid.copy()\ndf_X_test = X_test.copy()\n\nprint(f'X_train shape : {X_train.shape}')\nprint(f'X_valid shape : {X_valid.shape}')\nprint(f'X_test shape : {X_test.shape}')","7f4b33e1":"#random seed\nrans = 0\n\ndef log_transform(x):\n    return np.log(x + 1)\n\n\ntransformer = FunctionTransformer(log_transform)\n\n\n# Preprocessing for numerical data\nnumerical_transformer = Pipeline(steps=[\n       ('imputer', SimpleImputer(strategy='mean'))\n       ,('transformer', transformer)\n      ,('scaler', StandardScaler())\n])\n\n# Preprocessing for categorical data\ncategorical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='most_frequent')) \n    # ,('onehot', OneHotEncoder(handle_unknown='ignore',sparse=False))\n    ,('ordinal', OrdinalEncoder())\n])\n\n\n# Bundle preprocessing for numerical and categorical data\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('cat', categorical_transformer, categorical_features),\n        ('num', numerical_transformer, numeric_features),\n    ],\n    remainder=\"passthrough\"\n  )","36f6b808":"%%time\nX_train = pd.DataFrame(preprocessor.fit_transform(X_train))\nX_valid = pd.DataFrame(preprocessor.fit_transform(X_valid))\nX_test = pd.DataFrame(preprocessor.fit_transform(X_test))\n","ee7440f7":"# Fit transform removed index; put it back\nX_train = pd.DataFrame(data=X_train.values, columns=df_X_train.columns)\nX_valid = pd.DataFrame(data=X_valid.values, columns=df_X_valid.columns)\nX_test = pd.DataFrame(data=X_test.values, columns=df_X_test.columns)","d2a3fbbb":"X_train.head()","1552fd93":"from matplotlib import pyplot\n# feature selection\ndef select_features(X_train, y_train, X_test):\n\t# configure to select all features\n\tfs = SelectKBest(score_func=f_regression, k='all')\n\t# learn relationship from training data\n\tfs.fit(X_train, y_train)\n\t# transform train input data\n\tX_train_fs = fs.transform(X_train)\n\t# transform test input data\n\tX_test_fs = fs.transform(X_test)\n\treturn X_train_fs, X_test_fs, fs\n\n\n# feature selection\nX_train_fs, X_test_fs, fs = select_features(X_train, y_train, X_test)\n# what are scores for the features\nfor i in range(len(fs.scores_)):\n\tprint('Feature %d - %s : %f' % (i,pd.DataFrame(X_train).columns[i], fs.scores_[i]))\n# plot the scores\npyplot.bar([i for i in range(len(fs.scores_))], fs.scores_)\npyplot.show()","3dfb1a76":"#Best Features \nk = 0\nfor i in range(len(fs.scores_)):\n    if fs.scores_[i] > 100:\n        print('Feature %d - %s : %f' % (i,pd.DataFrame(X_train).columns[i], fs.scores_[i]))\n        k += 1\n        \nprint(f'best k value : {k}')","4368aecb":"#Init preprocessor\n# preprocessor = make_pipeline(preprocessor,SelectKBest(f_regression,k=k))","327e4d1c":"#Init preprocessor\n#preprocessor = make_pipeline(SelectKBest(f_regression,k=k))","e45a9bf5":"# Evalution Model \ndef evaluation(model, name_model):\n    \n    model.fit(X_train,y_train)\n    #model_score = model.score(X_train,y_train)\n    ypred = model.predict(X_test)\n    ypred = model.predict(X_valid)\n    \n    N, train_score, val_score,fit_times,score_times = learning_curve(model, X_train, y_train,shuffle = True,\n                                              cv=5,scoring='neg_root_mean_squared_error',\n                                              train_sizes=np.linspace(0.1, 1, 10),random_state=0)\n    \n    \n    #print('Training scores:\\n\\n', train_score)\n    #print('\\n', '-' * 70) # separator to make the output easy to read\n    #print('\\nValidation scores:\\n\\n', val_score)\n    \n    train_scores_mean = -train_score.mean(axis = 1)\n    validation_scores_mean = -val_score.mean(axis = 1)\n    #fit_times_mean = fit_times.mean ()\n    #score_times_mean = score_times.mean()\n    \n    print(f'Model :{name_model}')\n    #print('\\n', '-' * 20) # separator\n    #print('Mean training scores\\n\\n', pd.Series(train_scores_mean, index = N))\n    #print('\\n', '-' * 20) # separator\n    #print('\\nMean validation scores\\n\\n',pd.Series(validation_scores_mean, index = N))\n    #print('\\n', '-' * 20) # separator\n    \n    #print(f'Score R2 : {model_score}')\n    print('Mean training scores : ', train_scores_mean.mean())\n    print('Mean Validation scores : ', validation_scores_mean.mean())\n    #print(f'Mean fit time : {fit_times_mean}s \/ Mean score time : {score_times_mean}')\n\n    #print('MSE:', mean_squared_error(y_test, ypred))\n    print(f'RMSE: ', mean_squared_error(y_valid, ypred, squared=\"False\"))\n    \n    print('\\n', '-' * 20) # separator\n    print('\\n','-' * 20) # separator\n    plt.figure(figsize=(12, 8))\n    plt.plot(N, train_score.mean(axis=1), label='train score')\n    plt.plot(N, val_score.mean(axis=1), label='validation score')\n    plt.xlabel('Number of train size')\n    plt.ylabel('neg_root_mean_squared_error')\n    plt.title(name_model)\n    plt.legend()","4aa0f350":"# Elastic = make_pipeline(preprocessor, ElasticNet(alpha=0.0005, l1_ratio=0.9,random_state=0))\n# Lasso_model = make_pipeline(preprocessor,Lasso(alpha =0.0005,random_state=0))\n# Ridge_model = make_pipeline(preprocessor, Ridge(random_state=0))\n# RandomForest = make_pipeline(preprocessor, RandomForestRegressor(random_state=0))\n# Adaboost = make_pipeline(preprocessor, AdaBoostRegressor(random_state=0))\n# XGboost = make_pipeline(preprocessor, XGBRegressor())\n# GradientBoosting = make_pipeline(preprocessor, GradientBoostingRegressor(random_state=0))\n# LGBM = make_pipeline(preprocessor, LGBMRegressor(random_state=0))","f9117e9f":"# define a dict of model\n# dict_of_models = {\n#                 'Elastic': Elastic,\n#                 'Lasso_model': Lasso_model,\n#                 'Ridge_model': Ridge_model,\n#                 'XGboost' : XGboost,\n#               'GradientBoosting': GradientBoosting,\n#                 'LGBM': LGBM\n#}\n\n","80007000":"# %%time\n\n# print(\"Evaluation :\")\n\n# for name, model in dict_of_models.items():\n#    evaluation(model, name)\n\n","5bbb3f42":"%%time\n\ndef bayes_parameter_opt_lgb(X, y, init_round=20, opt_round=25, n_folds=5, random_seed=6,n_estimators=1000, output_process=False):\n    # prepare data\n    train_data = lgb.Dataset(data=X, label=y, free_raw_data=False)\n    # parameters\n    def lgb_eval(learning_rate,num_leaves, feature_fraction, bagging_fraction, max_depth, max_bin, min_data_in_leaf,lambda_l1,lambda_l2,min_split_gain):\n        params = {'boosting':'gbdt','application':'regression', 'metric':'rmse','force_col_wise': 'true'}\n        params['learning_rate'] = max(min(learning_rate, 1), 0)\n        params[\"num_leaves\"] = int(round(num_leaves))\n        params['feature_fraction'] = max(min(feature_fraction, 1), 0)\n        params['bagging_fraction'] = max(min(bagging_fraction, 1), 0)\n        params['max_depth'] = int(round(max_depth))\n        params['max_bin'] = int(round(max_bin))\n        params['min_data_in_leaf'] = int(round(min_data_in_leaf))\n        #params['min_sum_hessian_in_leaf'] = min_sum_hessian_in_leaf\n        #params['subsample'] = max(min(subsample, 1), 0)\n        params['verbose'] = -1\n        params['lambda_l1'] = max(min(lambda_l1,1), 0)\n        params['lambda_l2'] = max(min(lambda_l2,1), 0)\n        params['min_split_gain'] = int(round(min_split_gain))\n        #params['bagging_freq'] = int(round(bagging_freq))\n        #params['min_child_weight'] = int(round(min_child_weight))\n        \n        cv_result = lgb.cv(params, train_data, nfold=n_folds, seed=random_seed, stratified=False, verbose_eval =200, metrics=['rmse'])\n        return -np.min(cv_result['rmse-mean'])\n        \n    lgbBO = BayesianOptimization(lgb_eval, {'learning_rate': (0.001, 0.5),\n                                            'num_leaves': (31, 80),\n                                            'feature_fraction': (0.1, 1.0),\n                                            'bagging_fraction': (0.01, 1.0),\n                                            'max_depth': (25, 40),\n                                            'max_bin':(300,600),\n                                            'min_data_in_leaf': (20, 40),\n                                            #'min_sum_hessian_in_leaf':(0,100),\n                                            'lambda_l1': (0.1, 5),\n                                            'lambda_l2': (0.1, 3),\n                                            'min_split_gain': (0.001, 0.01),\n                                            #'min_child_weight': (5, 50),\n                                            #'subsample': (0.01, 1.0),\n                                            #'bagging_freq' : (3, 6)\n                                            },random_state=0)\n\n    \n    #n_iter: How many steps of bayesian optimization you want to perform. The more steps the more likely to find a good maximum you are.\n    #init_points: How many steps of random exploration you want to perform. Random exploration can help by diversifying the exploration space.\n    \n    lgbBO.maximize(init_points=init_round, n_iter=opt_round)\n    \n    model_rmse =[]\n    for model in range(len( lgbBO.res)):\n        model_rmse.append(lgbBO.res[model]['target'])\n    \n    # return best parameters\n    return lgbBO.res[pd.Series(model_rmse).idxmax()]['target'],lgbBO.res[pd.Series(model_rmse).idxmax()]['params']\n\nopt_params = bayes_parameter_opt_lgb(X_train, y_train, init_round=20, opt_round =25, n_folds=5, random_seed=6,n_estimators=1000)","8a2ef8be":"opt_params[1][\"num_leaves\"] = int(round(opt_params[1][\"num_leaves\"]))\nopt_params[1]['max_depth'] = int(opt_params[1]['max_depth'])\nopt_params[1]['min_data_in_leaf'] = int(round(opt_params[1]['min_data_in_leaf']))\nopt_params[1]['max_bin'] = int(round(opt_params[1]['max_bin']))\nopt_params[1]['objective']='regression'\nopt_params[1]['metric']='rmse'\nopt_params[1]['boosting']='gbdt'\nopt_params[1]['is_unbalance']=True\nopt_params[1]['boost_from_average']=False\nopt_params[1]['verbose'] = -1\nopt_params[1]['force_col_wise']=True\n# opt_params[1]['bagging_freq']=int(round(opt_params[1]['bagging_freq']))\n# opt_params[1]['subsample']=True\nopt_params=opt_params[1]\nopt_params","3b5c391e":"\n%%time \n\ntarget= y_train\n\nX_train = pd.DataFrame(X_train)\nX_test = pd.DataFrame(X_test)\nfeatures= [c for c in X_train.columns]\n\n\nfolds = KFold(n_splits=10, shuffle=True, random_state=0)\noof = np.zeros(len(X_train))\npred = np.zeros(len(X_test))\nfeature_importance_df = pd.DataFrame()\n\nfor fold_, (trn_idx, val_idx) in enumerate(folds.split(X_train, target)):\n    print(\"Fold {}\".format(fold_))\n    trn_data = lgb.Dataset(X_train.iloc[trn_idx][features], label=target.iloc[trn_idx])\n    val_data = lgb.Dataset(X_train.iloc[val_idx][features], label=target.iloc[val_idx])\n\n    num_round = 15000\n    clf = lgb.train(opt_params, trn_data, num_round, valid_sets = [trn_data, val_data], verbose_eval=500, early_stopping_rounds = 250)\n    oof[val_idx] = clf.predict(X_train.iloc[val_idx][features], num_iteration=clf.best_iteration)\n    \n    fold_importance_df = pd.DataFrame()\n    fold_importance_df[\"Feature\"] = features\n    fold_importance_df[\"importance\"] = clf.feature_importance()\n    fold_importance_df[\"fold\"] = fold_ + 1\n    feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n    \n    pred += clf.predict(X_test[features], num_iteration=clf.best_iteration) \/ folds.n_splits\n\nprint(\"CV RMSE: {:<8.5f}\".format(mean_squared_error(target, oof,squared=False)))\n\n","879a889a":"cols = (feature_importance_df[[\"Feature\", \"importance\"]]\n        .groupby(\"Feature\")\n        .mean()\n        .sort_values(by=\"importance\", ascending=False)[:20].index)\n\nbest_features = feature_importance_df.loc[feature_importance_df.Feature.isin(cols)]\n\nplt.figure(figsize=(20,28))\nsns.barplot(x=\"importance\", y=\"Feature\", data=best_features.sort_values(by=\"importance\",ascending=False))\nplt.title('Features importance (averaged\/folds)')\nplt.tight_layout()\nplt.savefig('Feature_Importance.png')\n","54b1eb00":"# Take a look on XGBOOST\nfinal_xgboost = XGBRegressor(objective='reg:squarederror',\n                             n_estimators=140,\n                             learning_rate=0.14,\n                             max_depth = 5,\n                             min_child_weight = 0.01,\n                             #silent = 0.1,\n                             subsample = 0.9,\n                             colsample_bytree =0.2,\n                             tree_method='gpu_hist',\n                             predictor='gpu_predictor',\n                             random_state=0)\n\npipeline_xgb = Pipeline(steps=[('preprocessor', preprocessor),\n                      ('model', final_xgboost)\n                     ])\n\npipeline_xgb.fit(X_train, y_train)\npreds = pipeline_xgb.predict(X_valid)\nmsqrte_xgb = mean_squared_error(y_valid, preds, squared=False)\nprint('RMSE:', msqrte_xgb)","0364e1fb":"predictions = pd.DataFrame()\ndf_test = df_test.reset_index()\npredictions[\"id\"] = df_test[\"id\"]\npredictions[\"target\"] = pred\n\npredictions.to_csv('submission.csv', index=False, header=predictions.columns)\npredictions.head()","6164bd3b":"### Take on look on different model behaviour : LGBM \/ XGBOOST \/ Gradient Boosting","f83ac9ac":"# II - PREPROCESSING DATA","4b5b5234":"# I - EDA (Exploration Data Analysis)","6fd0165a":"\"cont0\" \/ \"cont6\" and \"cont8\" has some outliers","553883c4":"# III - MODELING","c2889d66":"# III - MODELING","122daafd":"# HYPERPARAMETERS LGBM","784be036":"#### Ref  : \/ https:\/\/www.kaggle.com\/sgedela\/30-days-of-ml-model-averaging-ensemble\n"}}