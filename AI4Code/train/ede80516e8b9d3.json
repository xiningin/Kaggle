{"cell_type":{"9b720734":"code","7beda999":"code","fc83be3f":"code","a6969b5b":"code","48f5d548":"code","9fd20f5b":"code","aa8bdd99":"code","682b2a3b":"code","67ad382f":"code","a3bffef8":"code","52e038f3":"code","a83fd727":"code","0cacfebd":"code","9bad8c7f":"code","9bf4cd2f":"code","f59b2e83":"code","9359aad7":"code","c71d3bce":"code","c2d6c040":"code","8c1c8d8d":"code","791949d2":"code","7a72e79f":"code","e4ec3813":"code","2dcc2a22":"code","2db98e7b":"code","4dae93e0":"markdown","0a80e16d":"markdown","edf4005b":"markdown","6d3db3dc":"markdown"},"source":{"9b720734":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","7beda999":"!pip install --upgrade scikit-learn\n# Librraries Used in this notebook\nimport pandas as pd\nimport numpy as np \nimport matplotlib.pyplot as plt\n\n# SMOTE \nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.model_selection import train_test_split, RandomizedSearchCV\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, accuracy_score, classification_report, confusion_matrix","fc83be3f":"df= pd.read_csv(\"..\/input\/creditcardfraud\/creditcard.csv\")\ndf.head()","a6969b5b":"print('No Frauds', round(df['Class'].value_counts()[0]\/len(df) * 100,2), '% of the dataset')\nprint('Frauds', round(df['Class'].value_counts()[1]\/len(df) * 100,2), '% of the dataset')","48f5d548":"# Scaling the values of Time and Amount in a range of -1 to 1\nfrom sklearn.preprocessing import StandardScaler , RobustScaler\n\nstd_scaler = StandardScaler()\nrob_scaler = RobustScaler()\n\ndf['scaled_amount'] = rob_scaler.fit_transform(df['Amount'].values.reshape(-1,1))\ndf['scaled_time'] = rob_scaler.fit_transform(df['Time'].values.reshape(-1,1))\n\ndf.drop(['Time','Amount'], axis=1, inplace=True)\n","9fd20f5b":"df.head()","aa8bdd99":"#  Splitng the data set to training an test Set\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import StratifiedShuffleSplit, StratifiedKFold\n\nX = df.drop('Class',axis = 1)\ny = df['Class']\nsss = StratifiedKFold(n_splits=5, random_state=None, shuffle=False)\n\n\nfor train_index , test_index in sss.split(X, y):\n        X_train , X_test = X.iloc[train_index], X.iloc[test_index]\n        y_train , y_test = y.iloc[train_index], y.iloc[test_index]\n        print(\"X_train shape:\", train_index,\"y_train shape:\", test_index)\n\n\noriginal_Xtrain = X_train.values\noriginal_Xtest = X_test.values\noriginal_ytrain = y_train.values\noriginal_ytest = y_test.values\n","682b2a3b":"from imblearn.over_sampling import SMOTE\nfrom sklearn.model_selection import train_test_split, RandomizedSearchCV\nfrom imblearn.pipeline import make_pipeline as imbalanced_make_pipeline\n\nprint('Length of X (train): {} | Length of y (train): {}'.format(len(original_Xtrain), len(original_ytrain)))\nprint('Length of X (test): {} | Length of y (test): {}'.format(len(original_Xtest), len(original_ytest)))\n\n# List to append the score and then find the average\naccuracy_lst = []\nprecision_lst = []\nrecall_lst = []\nf1_lst = []\nauc_lst = []\n\n# Classifier with optimal parameters\n# log_reg_sm = grid_log_reg.best_estimator_\nlog_reg_sm = LogisticRegression()\n\n\nlog_reg_params = {\"penalty\": ['l1', 'l2'], 'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]}\n\n\nrand_log_reg = RandomizedSearchCV(LogisticRegression(), log_reg_params, n_iter=4)\n\n\n# Implementing SMOTE Technique \n# Cross Validating the right way\n# Parameters \nlog_reg_params = {\"penalty\": ['l1', 'l2'], 'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]}\nfor train, test in sss.split(original_Xtrain, original_ytrain):\n    pipeline = imbalanced_make_pipeline(SMOTE(sampling_strategy='minority'), rand_log_reg) # SMOTE happens during Cross Validation not before..\n    model = pipeline.fit(original_Xtrain[train], original_ytrain[train])\n    best_est = rand_log_reg.best_estimator_\n    prediction = best_est.predict(original_Xtrain[test])\n    \n    accuracy_lst.append(pipeline.score(original_Xtrain[test], original_ytrain[test]))\n    precision_lst.append(precision_score(original_ytrain[test], prediction))\n    recall_lst.append(recall_score(original_ytrain[test], prediction))\n    f1_lst.append(f1_score(original_ytrain[test], prediction))\n    auc_lst.append(roc_auc_score(original_ytrain[test], prediction))\n    \nprint('---' * 45)\nprint('')\nprint(\"accuracy: {}\".format(np.mean(accuracy_lst)))\nprint(\"precision: {}\".format(np.mean(precision_lst)))\nprint(\"recall: {}\".format(np.mean(recall_lst)))\nprint(\"f1: {}\".format(np.mean(f1_lst)))\nprint('---' * 45)","67ad382f":"sm = SMOTE(sampling_strategy='minority', random_state=42)\n# Xsm_train, ysm_train = sm.fit_sample(X_train, y_train)\n\n\n# This will be the data were we are going to \nXsm_train, ysm_train = sm.fit_resample(original_Xtrain, original_ytrain)","a3bffef8":"#importing the keras libraries\nimport keras\nfrom keras import backend as k\nfrom keras.models import Sequential\nfrom keras.layers import Activation\nfrom keras.layers.core import Dense\nfrom keras.optimizers import Adam\nfrom keras.metrics import categorical_crossentropy","52e038f3":"n_inputs = Xsm_train.shape[1]\n\noversample_model = Sequential([\n    Dense(n_inputs, input_shape=(n_inputs, ), activation='relu'),\n    Dense(32, activation='relu'),\n    Dense(32, activation='relu'),\n    Dense(2, activation='softmax')\n])\nprint(Xsm_train.shape)","a83fd727":"oversample_model.compile(Adam(lr=0.001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])","0cacfebd":"oversample_model.fit(Xsm_train, ysm_train, validation_split=0.2, batch_size=300, epochs=20, shuffle=True, verbose=2)","9bad8c7f":"oversample_predictions = oversample_model.predict(original_Xtest, batch_size=200, verbose=0)","9bf4cd2f":"oversample_fraud_predictions = oversample_model.predict_classes(original_Xtest, batch_size=200, verbose=0)","f59b2e83":"import itertools\n\n# Create a confusion matrix\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    print(cm)\n\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title, fontsize=14)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    \noversample_smote = confusion_matrix(original_ytest, oversample_fraud_predictions)\nactual_cm = confusion_matrix(original_ytest, original_ytest)\nlabels = ['No Fraud', 'Fraud']\noversample_smote\nfig = plt.figure(figsize=(16,8))\n\nfig.add_subplot(221)\nplot_confusion_matrix(oversample_smote, labels, title=\"OverSample (SMOTE) \\n Confusion Matrix\", cmap=plt.cm.Oranges)\n\nfig.add_subplot(222)\nplot_confusion_matrix(actual_cm, labels, title=\"Confusion Matrix \\n (with 100% accuracy)\", cmap=plt.cm.Greens)","9359aad7":"print('Oversampling and DNN: ', roc_auc_score(original_ytest, oversample_fraud_predictions))\nprint('Oversmapling and :')\nprint(classification_report(original_ytest, oversample_fraud_predictions))","c71d3bce":"n_inputs = X_train.shape[1]\n\nundersample_model = Sequential([\n    Dense(n_inputs, input_shape=(n_inputs, ), activation='relu'),\n    Dense(32, activation='relu'),\n    Dense(32, activation='relu'),    \n    Dense(2, activation='softmax')\n])","c2d6c040":"undersample_model.summary()","8c1c8d8d":"undersample_model.compile(Adam(lr=0.001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])","791949d2":"undersample_model.fit(X_train, y_train, validation_split=0.2, batch_size=25, epochs=20, shuffle=True, verbose=2)","7a72e79f":"undersample_predictions = undersample_model.predict(original_Xtest, batch_size=200, verbose=0)","e4ec3813":"undersample_fraud_predictions = undersample_model.predict_classes(original_Xtest, batch_size=200, verbose=0)","2dcc2a22":"undersample_cm = confusion_matrix(original_ytest, undersample_fraud_predictions)\n# actual_cm = confusion_matrix(original_ytest, original_ytest)\nlabels = ['No Fraud', 'Fraud']\n\nfig = plt.figure(figsize=(8,8))\n\n# fig.add_subplot()\nplot_confusion_matrix(undersample_cm, labels, title=\"Random UnderSample \\n Confusion Matrix\", cmap=plt.cm.Reds)\n\n# fig.add_subplot(222)\n# plot_confusion_matrix(actual_cm, labels, title=\"Confusion Matrix \\n (with 100% accuracy)\", cmap=plt.cm.Greens)","2db98e7b":"# ROC - AUC of Undersampling and DNN\nprint('UNderSampling and DNN: ', roc_auc_score(original_ytest, undersample_fraud_predictions))\nprint(classification_report(original_ytest, undersample_fraud_predictions))","4dae93e0":"Scaling Time and  Amount Information\n","0a80e16d":"# SMOTE and DNN","edf4005b":"# **UnderSampling Technique**","6d3db3dc":"#### **Refernces:**\n* [1] : [Principal Component Analysis](https:\/\/en.wikipedia.org\/wiki\/Principal_component_analysis)\n* [2] : [Dataset](https:\/\/www.kaggle.com\/mlg-ulb\/creditcardfraud)\n* [3] : Stratified KFold model Sampling Technique: Introduction to Machine Learning with Python - Oreilly\n"}}