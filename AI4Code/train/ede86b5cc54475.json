{"cell_type":{"d95c5b4c":"code","46117a6f":"code","20ddc48b":"code","b8ec9d3b":"code","86794791":"code","d3ed83d1":"code","a23b0679":"code","62014ca8":"code","07a58583":"code","7759a08f":"code","623355c6":"code","557c28ba":"code","f1c824a6":"code","f57d96fb":"code","80bbb9db":"code","93c6ba99":"code","eec2e3ff":"code","47126538":"code","1493a5a1":"code","f344e677":"code","e07ea096":"code","be4b2e1c":"markdown","7db7466b":"markdown","e2987924":"markdown","0ad2b7f9":"markdown","09770650":"markdown","72f072ec":"markdown"},"source":{"d95c5b4c":"#import necessary libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nimport io\nfrom time import time","46117a6f":"#import dataset into dataFrame using Pandas\ndata= pd.read_csv('..\/input\/data.csv')\ndata.head()","20ddc48b":"sns.countplot(data['diagnosis'],label=\"Count\")","b8ec9d3b":"#visualize correlations by heatmap\nf,ax=plt.subplots(figsize = (18,18))\nsns.heatmap(data.corr(),annot= True,linewidths=0.5,fmt = \".1f\",ax=ax)\nplt.xticks(rotation=90)\nplt.yticks(rotation=0)\nplt.title('Correlation Map')\nplt.show()","86794791":"#drop unnecessary data columns\ndata.drop(['Unnamed: 32','id'], axis=1,inplace=True)","d3ed83d1":"#split data into features and labels (with all varaibles)\nX = np.array(data.drop(['diagnosis'],1))\ny=data['diagnosis']","a23b0679":"#encode y\ny_en = y.replace({'B':0, 'M':1})\ny_en= np.array(y_en)","62014ca8":"# the features taking into consideration the correlations between varaibles\n\nX_1 = np.array(data.drop(['diagnosis','perimeter_mean','area_mean','radius_worst','area_worst','perimeter_worst'],1))","07a58583":"# split the dataset into training and testing sets (feature selection mode)\nfrom sklearn.model_selection import train_test_split\nX_train1, X_test1, y_train1, y_test1 = train_test_split(X_1,y_en, test_size = 0.25, random_state = 0)\nprint(\"Training set with feature selection has {} samples.\".format(X_train1.shape[0]))\nprint(\"Testing set with feature selection has {} samples.\".format(X_test1.shape[0]))","7759a08f":"# create a function to calculate accuracy and f beta score for each model\nfrom sklearn.metrics import fbeta_score, accuracy_score,confusion_matrix\ndef test_predict(clf, X_train, y_train, X_test, y_test): \n    \n    results = {}\n    clf = clf.fit(X_train,y_train)\n    #  Get the predictions on the test set(X_test),\n    predictions_test = clf.predict(X_test)\n    #  Compute accuracy on test set using accuracy_score()\n    accuracy=results['acc_test'] = accuracy_score(y_test, predictions_test)\n    #  Compute F-score on the test set which is y_test\n    fbeta=results['f_test'] = fbeta_score(y_test,predictions_test, beta = 0.5)\n    # Success\n    print(\"accuracy for {} model is {}\".format(clf.__class__.__name__,results['acc_test']))\n    print(\"F beta for {} model is {}\".format(clf.__class__.__name__,results['f_test']))\n    print('------------------------------------------------------------------')\n    # Return the results\n    return results","623355c6":"# Import the models from sklearn\nfrom xgboost import XGBClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\n# Initialize the models\nclf1 = XGBClassifier(random_state =15)\nclf2= SVC(random_state =15)\nclf3 = RandomForestClassifier(random_state =15)\n# Collect results on the clf\nresults = {}\nfor clf in [clf1, clf2, clf3]:\n    clf_name = clf.__class__.__name__\n    results[clf_name] = {}       \n    test_predict(clf, X_train1, y_train1, X_test1, y_test1)","557c28ba":"def enable_plotly_in_cell():\n  import IPython\n  from plotly.offline import init_notebook_mode\n  display(IPython.core.display.HTML('''<script src=\"\/static\/components\/requirejs\/require.js\"><\/script>'''))\n  init_notebook_mode(connected=False)","f1c824a6":"# Compare Algorithms\nimport matplotlib.pyplot as plt\nfig = plt.figure()\nfig.suptitle( 'Algorithm Comparison' )\nax = fig.add_subplot(111)\nplt.boxplot([(0.9622641509433962,0.0),(0.8703071672354951, 0.0),(0.9195402298850575, 0.0)])\nax.set_xticklabels(['XGBClassifier','SVM','Random Forest'])\nplt.show()","f57d96fb":"#split the data into traning and testing sets\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X,y_en, test_size = 0.25, random_state = 0)","80bbb9db":"# train the models and calculate scores with all varaibles\nfor clf in [clf1, clf2, clf3]:\n    clf_name = clf.__class__.__name__\n    results[clf_name] = {}       \n    test_predict(clf, X_train, y_train, X_test, y_test)","93c6ba99":"#plot scores in table \nfrom plotly.offline import iplot\nimport plotly.graph_objs as go\nenable_plotly_in_cell()\ntrac = go.Table(\n    header=dict(values=['Classifier', 'Accuracy Score','F-beta Score']),\n    cells=dict(values=[['XGBClassifier','SVM','Random Forest'],\n                       [0.9790209790209791,0.6293706293706294,0.965034965034965],\n                      [0.9770114942528735,0.0,0.968379446640316]]))\n\ntabl = [trac] \n\niplot(tabl)","eec2e3ff":"fig = plt.figure()\nfig.suptitle( 'Algorithm Comparison' )\nax = fig.add_subplot(111)\nplt.boxplot([(0.9770114942528735,0.0),(0.0, 0.0),(0.968379446640316, 0.0)])\nax.set_xticklabels(['XGBClassifier','SVM','Random Forest'])\nplt.show()","47126538":"#optimize the final selected model using grid search\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import make_scorer, r2_score, fbeta_score\n#Initialize the classifier\nclf = XGBClassifier(random_state =42)\nparameters = {'max_depth':[6],'n_estimators':[100], 'learning_rate' : [0.1], 'min_samples_split':[7],'gamma':[0.01],'min_child_weight':[1],'Max_delta_step':[1],'colsample_bytree':[1],'reg_lambda':[1]}\n\nscorer = make_scorer(fbeta_score, beta=0.5)\n\ngrid_obj = GridSearchCV(clf,parameters,scoring=scorer)\n\ngrid_fit = grid_obj.fit(X_train,y_train)\n\nbest_clf = grid_fit.best_estimator_\n\npredictions = (clf.fit(X_train, y_train)).predict(X_test)\nbest_predictions = best_clf.predict(X_test)\n\n# Report the before-and-afterscores\nprint(\"Unoptimized model\\n------\")\nprint(\"Accuracy score on testing data: {:.4f}\".format(accuracy_score(y_test, predictions)))\nprint(\"F-score on testing data: {:.4f}\".format(fbeta_score(y_test, predictions, beta = 0.5)))\nprint(\"\\nOptimized Model\\n------\")\nprint(\"Final accuracy score on the testing data: {:.4f}\".format(accuracy_score(y_test, best_predictions)))\nprint(\"Final F-score on the testing data: {:.4f}\".format(fbeta_score(y_test, best_predictions, beta = 0.5)))","1493a5a1":"#plot scores in table \nfrom plotly.offline import iplot\nimport plotly.graph_objs as go\nenable_plotly_in_cell()\ntr = go.Table(\n    header=dict(values=['Model', 'Accuracy Score','F-beta Score']),\n    cells=dict(values=[['Unoptimized','Optimized'],\n                       [0.9790,0.9860],\n                      [0.9770,0.9811]]))\n\nopt = [tr] \n\niplot(opt)","f344e677":"fig = plt.figure()\nfig.suptitle( 'Algorithm Comparison' )\nax = fig.add_subplot(111)\nplt.boxplot([(0.9770,0.0),(0.9811, 0.0)])\nax.set_xticklabels(['Unoptimized','Optimized'])\nplt.show()","e07ea096":"cm = confusion_matrix(y_test,best_clf.predict(X_test))\nsns.heatmap(cm,annot=True,fmt=\"d\")\nplt.title('Optimized model results')","be4b2e1c":"# All Varaibles Scores","7db7466b":"# Feature Selection Scores","e2987924":"# Model Optimization","0ad2b7f9":"We can see that the scores of **XGBClassifier** are the best , but let's first train the classifiers with all features and see the results","09770650":"With taking all varaibles into account , we can see that SVM accuracy score is very low comapred to the previous score, and f beta score is a zero, so i think it is not a solution for this problem.\n\n---\n\nAlso we can find XGBClassifier and Random Forset scores are improved compared to previous scores.\n\n\n---\n\nand XGBClassifier has the best scores in both .","72f072ec":"from the heat map above we can figure out that , (radius_mean,perimeter_mean, area_mean,radius_worst,area_worst,perimeter_worst) are highly correalted so we can use one feature instead of all of them"}}