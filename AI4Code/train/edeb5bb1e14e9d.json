{"cell_type":{"9dd3e585":"code","54ffc0fd":"code","11149458":"code","dfc97c5f":"code","3d39f399":"code","ddadca46":"code","d603aaac":"code","2cf56db1":"code","59e882ee":"code","27e5ab09":"code","c64fdf0f":"markdown","26d15a4a":"markdown","cef3cfdf":"markdown","10095c39":"markdown","96dd3f99":"markdown","4c37accc":"markdown","c6efd1e2":"markdown","19a68f43":"markdown","4c40d3c8":"markdown"},"source":{"9dd3e585":"\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport PIL.Image\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","54ffc0fd":"train = pd.read_csv(\"\/kaggle\/input\/bengaliai-cv19\/train.csv\")","11149458":"train.head()","dfc97c5f":"import dask.dataframe as dd","3d39f399":"%%time\ndf_dd = dd.read_parquet('\/kaggle\/input\/bengaliai-cv19\/train_image_data_1.parquet')","ddadca46":"%%time\ndf = pd.read_parquet('\/kaggle\/input\/bengaliai-cv19\/train_image_data_1.parquet')","d603aaac":"class_map_df = pd.read_csv('\/kaggle\/input\/bengaliai-cv19\/class_map.csv')\nclass_map_df.shape","2cf56db1":"class_map_df.head()","59e882ee":"class_map_df.component_type.value_counts()","27e5ab09":"train.describe()","c64fdf0f":"### Class Map","26d15a4a":"You can read Parquet with `read_parquet()` of `pandas`  or `read_parquet()` of `dask`","cef3cfdf":"1. ### So, Parquet is really fast and efficient\n\n","10095c39":"Let's time `pandas` vs `dask` and see how's reading parquet doing ","96dd3f99":"# Typical Training File","4c37accc":"### WIP, Hopefully more tomorrow! Cheers! ","c6efd1e2":"# The more interesting Parquet files, Do you know What's Parquet?","19a68f43":"### Apache Parquet is a columnar binary format that is easy to split into multiple files (easier for parallel loading) and is generally much simpler to deal with than HDF5 (from the library\u2019s perspective).\n\n![](https:\/\/external-content.duckduckgo.com\/iu\/?u=https%3A%2F%2Fimage.slidesharecdn.com%2Falexlevensonhadoopsummitv3-150609221151-lva1-app6891%2F95%2Fslide-48-1024.jpg&f=1&nofb=1)\n\n###  It is also a common format used by other big data systems like Apache Spark and Apache Impala, and so it is useful to interchange with other systems:\n\n### References - https:\/\/www.slideshare.net\/alexlevenson\/hadoop-summit-2015-performance-optimization-at-scale\/46-APACHE_PARQUETColumnar_storage_for_the","4c40d3c8":"![](https:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/3\/31\/%E0%A6%AC%E0%A6%BE%E0%A6%82%E0%A6%B2%E0%A6%BE.svg\/400px-%E0%A6%AC%E0%A6%BE%E0%A6%82%E0%A6%B2%E0%A6%BE.svg.png)"}}