{"cell_type":{"aec4b03e":"code","3770adac":"code","2e37a5b5":"code","7610a168":"code","07ee7477":"code","f64ed654":"code","fea16daa":"code","a3b9d41c":"code","996df313":"code","c3f3a596":"code","8408f86f":"code","f9ccaa9e":"code","a2ad6645":"code","72815a7f":"code","024ff90c":"code","e6c47f98":"code","d31e27d2":"code","16ede916":"code","ee0b15b4":"code","73dd4f0b":"code","a596d2d7":"code","ba1afb1f":"code","9b5f7e0f":"code","0be8cdd4":"code","2cda6bae":"code","09825662":"code","840ac2e2":"code","d72bd64b":"code","e668d169":"code","6c1d16b2":"code","f43f559d":"code","ca25b160":"code","8b468e77":"code","2227c31f":"code","a8e8abe9":"code","af6e3b0a":"code","1810ec3d":"code","8dc6d09d":"code","b0f53143":"code","a51b98ef":"code","d72e2bcd":"code","b94e2989":"code","0cf8938e":"code","b5170bf5":"code","9d6aa9f6":"code","8dbe0b08":"code","dc408e0b":"code","f4e6b72d":"code","c7d605fb":"code","084792fc":"code","8c92a837":"code","b83a321c":"code","80e448c8":"code","51492b67":"code","7dfaf39a":"code","289ba6d8":"markdown","b46890be":"markdown","d4ee0194":"markdown","57e33e28":"markdown","48558843":"markdown","e02c4c5b":"markdown","ec93e299":"markdown","f46979bc":"markdown","9554feee":"markdown","5124b720":"markdown","19714a40":"markdown","eb512c0c":"markdown","d2500bc4":"markdown","e78941b6":"markdown","05b7a3ff":"markdown","c811309b":"markdown","b4e2ca9e":"markdown","5eb4a1d4":"markdown","6564c29d":"markdown","fb90fa17":"markdown","3fd6fc0e":"markdown","e3f2a8b6":"markdown","556ae914":"markdown","7db16582":"markdown","507d1349":"markdown","d22684b8":"markdown","b37072ed":"markdown","a2c5527c":"markdown","f39b0884":"markdown","9929b889":"markdown","ab0a8fe5":"markdown","ef7228ec":"markdown","11a4b0f8":"markdown","711f23c0":"markdown","0c28a96b":"markdown","e3163e7c":"markdown","77ee0f9e":"markdown","f0698b8f":"markdown","0e401d31":"markdown","0298f69b":"markdown","1aa1cafe":"markdown","0fd390a7":"markdown","c8a70e88":"markdown","147ae98d":"markdown","5db6bd5d":"markdown","d81e8d88":"markdown","c94b8f5b":"markdown","09f2b2a1":"markdown","5cd550c4":"markdown","055dec29":"markdown","ea36285e":"markdown","4c3f9c3a":"markdown"},"source":{"aec4b03e":"!pip install textstat\n!pip install chart_studio","3770adac":"import string\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport random\nfrom collections import defaultdict\nimport matplotlib.pyplot as plt\nimport seaborn as sns\ncolor = sns.color_palette()\nfrom tqdm import tqdm\n%matplotlib inline\n\n\nimport nltk\nnltk.download('punkt') # one time execution\nimport re\nfrom nltk.corpus import stopwords \nfrom nltk.tokenize import word_tokenize   \nstop_words = stopwords.words('english')\nremove_words = set(stopwords.words('english')) \n\nfrom plotly import tools\nimport chart_studio.plotly as py\nfrom plotly.offline import init_notebook_mode, iplot\ninit_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.figure_factory as ff\n\nimport time\nimport pyLDAvis.sklearn\nfrom pylab import bone, pcolor, colorbar, plot, show, rcParams, savefig\nimport textstat\n\nimport spacy\nfrom spacy.lang.en.stop_words import STOP_WORDS\nfrom gensim.summarization.summarizer import summarize\nfrom spacy.lang.en import English\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.decomposition import NMF, LatentDirichletAllocation, TruncatedSVD\nfrom statistics import *\nimport concurrent.futures\n\npd.options.mode.chained_assignment = None\npd.options.display.max_columns = 999\n\npunctuations = string.punctuation\nstopwords = list(STOP_WORDS)\n\nparser = English()\n\ndef plot_readability(a,title,bins=0.1,colors=['#3A4750']):\n    #reference and credits : https:\/\/www.kaggle.com\/thebrownviking20\/analyzing-quora-for-the-insinceres\n    trace1 = ff.create_distplot([a], [\" Abstract \"], bin_size=bins, colors=colors, show_rug=False)\n    trace1['layout'].update(title=title)\n    iplot(trace1, filename='Distplot')\n    table_data= [[\"Statistical Measures\",\"Abstract\"],\n                [\"Mean\",mean(a)],\n                [\"Standard Deviation\",pstdev(a)],\n                [\"Variance\",pvariance(a)],\n                [\"Median\",median(a)],\n                [\"Maximum value\",max(a)],\n                [\"Minimum value\",min(a)]]\n    trace2 = ff.create_table(table_data)\n    iplot(trace2, filename='Table')\n    \npunctuations = string.punctuation\nstopwords = list(STOP_WORDS)\n\nparser = English()\ndef spacy_tokenizer(sentence):\n    #reference and credits : https:\/\/www.kaggle.com\/thebrownviking20\/analyzing-quora-for-the-insinceres\n    mytokens = parser(sentence)\n    mytokens = [ word.lemma_.lower().strip() if word.lemma_ != \"-PRON-\" else word.lower_ for word in mytokens ]\n    mytokens = [ word for word in mytokens if word not in stopwords and word not in punctuations ]\n    mytokens = \" \".join([i for i in mytokens])\n    return mytokens\n\n\n#references and credits : https:\/\/www.geeksforgeeks.org\/print-colors-python-terminal\/\ndef prRed(skk): print(\"\\033[91m {}\\033[00m\" .format(skk)) \ndef prGreen(skk): print(\"\\033[92m {}\\033[00m\" .format(skk)) \ndef prYellow(skk): print(\"\\033[93m {}\\033[00m\" .format(skk)) \ndef prLightPurple(skk): print(\"\\033[94m {}\\033[00m\" .format(skk)) \ndef prPurple(skk): print(\"\\033[95m {}\\033[00m\" .format(skk)) \ndef prCyan(skk): print(\"\\033[96m {}\\033[00m\" .format(skk)) \ndef prLightGray(skk): print(\"\\033[97m {}\\033[00m\" .format(skk)) \ndef prBlack(skk): print(\"\\033[98m {}\\033[00m\" .format(skk)) \n\nimport os\nimport warnings\nwarnings.filterwarnings('ignore')","2e37a5b5":"#data set credits : https:\/\/www.kaggle.com\/xhlulu\/cord-19-eda-parse-json-and-generate-clean-csv\nbiorxiv_data = pd.read_csv('\/kaggle\/input\/cord-19-eda-parse-json-and-generate-clean-csv\/biorxiv_clean.csv')\nclean_comm_data = pd.read_csv('\/kaggle\/input\/cord-19-eda-parse-json-and-generate-clean-csv\/clean_comm_use.csv')\nclean_noncomm_data = pd.read_csv('\/kaggle\/input\/cord-19-eda-parse-json-and-generate-clean-csv\/clean_noncomm_use.csv')\nclean_pmc_data = pd.read_csv('\/kaggle\/input\/cord-19-eda-parse-json-and-generate-clean-csv\/clean_pmc.csv')","7610a168":"biorxiv_data.head(3)","07ee7477":"clean_comm_data.head(3)","f64ed654":"clean_noncomm_data.head(3)","fea16daa":"clean_pmc_data.head(3)","a3b9d41c":"from wordcloud import WordCloud, STOPWORDS\n\n# Thanks : https:\/\/www.kaggle.com\/aashita\/word-clouds-of-various-shapes ##\ndef plot_wordcloud(text, mask=None, max_words=200, max_font_size=50, figure_size=(15.0,15.0), \n                   title = None, title_size=20, image_color=False,color = color):\n    stopwords = set(STOPWORDS)\n    more_stopwords = {'one', 'br', 'Po', 'th', 'sayi', 'fo', 'Unknown'}\n    stopwords = stopwords.union(more_stopwords)\n\n    wordcloud = WordCloud(background_color=color,\n                    stopwords = stopwords,\n                    max_words = max_words,\n                    max_font_size = max_font_size, \n                    random_state = 42,\n                    width=800, \n                    height=400,\n                    mask = mask)\n    wordcloud.generate(str(text))\n    \n    plt.figure(figsize=figure_size)\n    if image_color:\n        image_colors = ImageColorGenerator(mask);\n        plt.imshow(wordcloud.recolor(color_func=image_colors), interpolation=\"bilinear\");\n        plt.title(title, fontdict={'size': title_size,  \n                                  'verticalalignment': 'bottom'})\n    else:\n        plt.imshow(wordcloud);\n        plt.title(title, fontdict={'size': title_size, 'color': 'black', \n                                  'verticalalignment': 'bottom'})\n    plt.axis('off');\n    plt.tight_layout()  \n    \nplot_wordcloud(biorxiv_data['authors'].values, title=\"Word Cloud of Authors in biorxiv medrxiv Data\",color = 'black')","996df313":"plot_wordcloud(clean_comm_data['authors'].values, title=\"Word Cloud of Authors in comm use subset Data\",color = 'white')","c3f3a596":"plot_wordcloud(clean_noncomm_data['authors'].values, title=\"Word Cloud of Authors in Non common use subset Data\",color = 'red')","8408f86f":"plot_wordcloud(clean_pmc_data['authors'].values, title=\"Word Cloud of Authors pmc_custom Data\",color = 'violet')","f9ccaa9e":"plot_wordcloud(biorxiv_data['affiliations'].values, title=\"Word Cloud of Affiliations in biorxiv medrxiv Data \",color = 'green')","a2ad6645":"plot_wordcloud(clean_comm_data['affiliations'].values, title=\"Word Cloud of Affiliations in comm use subset Data\",color = 'orange')","72815a7f":"plot_wordcloud(clean_noncomm_data['affiliations'].values, title=\"Word Cloud of affiliations in Non common use subset Data\",color = 'brown')","024ff90c":"plot_wordcloud(clean_pmc_data['affiliations'].values, title=\"Word Cloud of Affiliations in pmc_custom Data\",color = 'gray')","e6c47f98":"df1 = biorxiv_data['abstract'].dropna()\ndf3 = clean_comm_data[\"abstract\"].dropna()\ndf2 = clean_noncomm_data[\"abstract\"].dropna()\ndf4 = clean_pmc_data[\"abstract\"].dropna()\n\n## custom function for ngram generation ##\ndef generate_ngrams(text, n_gram=1):\n    #Reference and credits: https:\/\/www.kaggle.com\/sudalairajkumar\/simple-exploration-notebook-qiqc\n    token = [token for token in text.lower().split(\" \") if token != \"\" if token not in STOPWORDS]\n    ngrams = zip(*[token[i:] for i in range(n_gram)])\n    return [\" \".join(ngram) for ngram in ngrams]\n\n## custom function for horizontal bar chart ##\ndef horizontal_bar_chart(df, color):\n    trace = go.Bar(\n        y=df[\"word\"].values[::-1],\n        x=df[\"wordcount\"].values[::-1],\n        showlegend=False,\n        orientation = 'h',\n        marker=dict(\n            color=color,\n        ),\n    )\n    return trace\n\n## Get the bar chart from sincere questions ##\nfreq_dict = defaultdict(int)\nfor sent in df1:\n    for word in generate_ngrams(sent):\n        freq_dict[word] += 1\nfd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfd_sorted.columns = [\"word\", \"wordcount\"]\ntrace0 = horizontal_bar_chart(fd_sorted.head(25), 'green')\n\n## Get the bar chart from insincere questions ##\nfreq_dict = defaultdict(int)\nfor sent in df2:\n    for word in generate_ngrams(sent):\n        freq_dict[word] += 1\nfd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfd_sorted.columns = [\"word\", \"wordcount\"]\ntrace1 = horizontal_bar_chart(fd_sorted.head(25), 'orange')\n\nfreq_dict = defaultdict(int)\nfor sent in df3:\n    for word in generate_ngrams(sent):\n        freq_dict[word] += 1\nfd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfd_sorted.columns = [\"word\", \"wordcount\"]\ntrace2 = horizontal_bar_chart(fd_sorted.head(25), 'black')\n\nfreq_dict = defaultdict(int)\nfor sent in df4:\n    for word in generate_ngrams(sent):\n        freq_dict[word] += 1\nfd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfd_sorted.columns = [\"word\", \"wordcount\"]\ntrace3 = horizontal_bar_chart(fd_sorted.head(25), 'red')\n\n\n\n# Creating two subplots\nfig = tools.make_subplots(rows=2, cols=2, vertical_spacing=0.04,\n                          subplot_titles=[\"Frequent words in biorxiv_data\", \n                                          \"Frequent words in clean_comm_data\",\n                                          \"Frequent words in clean_noncomm_data\",\n                                          \"Frequent words in clean_pmc_data\"])\nfig.append_trace(trace0, 1, 1)\nfig.append_trace(trace1, 1, 2)\nfig.append_trace(trace2, 2, 1)\nfig.append_trace(trace3, 2, 2)\n\n\n\nfig['layout'].update(height=1000, width=800, paper_bgcolor='rgb(233,233,233)', title=\"Word Count Plots of Abstracts\")\niplot(fig, filename='word-plots')\n","d31e27d2":"freq_dict = defaultdict(int)\nfor sent in df1:\n    for word in generate_ngrams(sent,2):\n        freq_dict[word] += 1\nfd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfd_sorted.columns = [\"word\", \"wordcount\"]\ntrace0 = horizontal_bar_chart(fd_sorted.head(25), 'gray')\n\n\nfreq_dict = defaultdict(int)\nfor sent in df2:\n    for word in generate_ngrams(sent,2):\n        freq_dict[word] += 1\nfd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfd_sorted.columns = [\"word\", \"wordcount\"]\ntrace1 = horizontal_bar_chart(fd_sorted.head(25), 'orange')\n\nfreq_dict = defaultdict(int)\nfor sent in df3:\n    for word in generate_ngrams(sent,2):\n        freq_dict[word] += 1\nfd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfd_sorted.columns = [\"word\", \"wordcount\"]\ntrace2 = horizontal_bar_chart(fd_sorted.head(25), 'brown')\n\n\nfreq_dict = defaultdict(int)\nfor sent in df4:\n    for word in generate_ngrams(sent,2):\n        freq_dict[word] += 1\nfd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfd_sorted.columns = [\"word\", \"wordcount\"]\ntrace3 = horizontal_bar_chart(fd_sorted.head(25), 'pink')\n\n\n# Creating two subplots\nfig = tools.make_subplots(rows=2, cols=2, vertical_spacing=0.04,horizontal_spacing=0.25,\n                          subplot_titles=[\"Frequent words in biorxiv_data\", \n                                          \"Frequent words in clean_comm_data\",\n                                          \"Frequent words in clean_noncomm_data\",\n                                          \"Frequent words in clean_pmc_data\"])\nfig.append_trace(trace0, 1, 1)\nfig.append_trace(trace1, 1, 2)\nfig.append_trace(trace2, 2, 1)\nfig.append_trace(trace3, 2, 2)\n\n\nfig['layout'].update(height=1000, width=800, paper_bgcolor='rgb(233,233,233)', title=\"Bigram Count Plots of Abstracts\")\niplot(fig, filename='word-plots')","16ede916":"for sent in df1:\n    for word in generate_ngrams(sent,3):\n        freq_dict[word] += 1\nfd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfd_sorted.columns = [\"word\", \"wordcount\"]\ntrace0 = horizontal_bar_chart(fd_sorted.head(25), 'blue')\n\n\nfreq_dict = defaultdict(int)\nfor sent in df2:\n    for word in generate_ngrams(sent,3):\n        freq_dict[word] += 1\nfd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfd_sorted.columns = [\"word\", \"wordcount\"]\ntrace1 = horizontal_bar_chart(fd_sorted.head(25), 'green')\n\nfreq_dict = defaultdict(int)\nfor sent in df3:\n    for word in generate_ngrams(sent,3):\n        freq_dict[word] += 1\nfd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfd_sorted.columns = [\"word\", \"wordcount\"]\ntrace2 = horizontal_bar_chart(fd_sorted.head(25), 'violet')\n\nfreq_dict = defaultdict(int)\nfor sent in df4:\n    for word in generate_ngrams(sent,3):\n        freq_dict[word] += 1\nfd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfd_sorted.columns = [\"word\", \"wordcount\"]\ntrace3 = horizontal_bar_chart(fd_sorted.head(25), 'red')\n\n\n\n\n# Creating two subplots\nfig = tools.make_subplots(rows=4, cols=1, vertical_spacing=0.04, horizontal_spacing=0.05,\n                          subplot_titles=[\"Frequent words in biorxiv_data\", \n                                          \"Frequent words in clean_comm_data\",\n                                          \"Frequent words in clean_noncomm_data\",\n                                          \"Frequent words in clean_pmc_data\"])\nfig.append_trace(trace0, 1, 1)\nfig.append_trace(trace1, 2, 1)\nfig.append_trace(trace2, 3, 1)\nfig.append_trace(trace3, 4, 1)\nfig['layout'].update(height=1200, width=1200, paper_bgcolor='rgb(233,233,233)', title=\"Trigram Count Plots\")\niplot(fig, filename='word-plots')","ee0b15b4":"\ndf1 = biorxiv_data['text'].dropna()\ndf3 = clean_comm_data[\"text\"].dropna()\ndf2 = clean_noncomm_data[\"text\"].dropna()\ndf4 = clean_pmc_data[\"text\"].dropna()\n\n## custom function for ngram generation ##\ndef generate_ngrams(text, n_gram=1):\n    token = [token for token in text.lower().split(\" \") if token != \"\" if token not in STOPWORDS]\n    ngrams = zip(*[token[i:] for i in range(n_gram)])\n    return [\" \".join(ngram) for ngram in ngrams]\n\n## custom function for horizontal bar chart ##\ndef horizontal_bar_chart(df, color):\n    trace = go.Bar(\n        y=df[\"word\"].values[::-1],\n        x=df[\"wordcount\"].values[::-1],\n        showlegend=False,\n        orientation = 'h',\n        marker=dict(\n            color=color,\n        ),\n    )\n    return trace\n\n## Get the bar chart from sincere questions ##\nfreq_dict = defaultdict(int)\nfor sent in df1:\n    for word in generate_ngrams(sent):\n        freq_dict[word] += 1\nfd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfd_sorted.columns = [\"word\", \"wordcount\"]\ntrace0 = horizontal_bar_chart(fd_sorted.head(25), 'orange')\n\n## Get the bar chart from insincere questions ##\nfreq_dict = defaultdict(int)\nfor sent in df2:\n    for word in generate_ngrams(sent):\n        freq_dict[word] += 1\nfd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfd_sorted.columns = [\"word\", \"wordcount\"]\ntrace1 = horizontal_bar_chart(fd_sorted.head(25), 'green')\n\nfreq_dict = defaultdict(int)\nfor sent in df3:\n    for word in generate_ngrams(sent):\n        freq_dict[word] += 1\nfd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfd_sorted.columns = [\"word\", \"wordcount\"]\ntrace2 = horizontal_bar_chart(fd_sorted.head(25), 'blue')\n\nfreq_dict = defaultdict(int)\nfor sent in df4:\n    for word in generate_ngrams(sent):\n        freq_dict[word] += 1\nfd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfd_sorted.columns = [\"word\", \"wordcount\"]\ntrace3 = horizontal_bar_chart(fd_sorted.head(25), 'red')\n\n\n\n# Creating two subplots\nfig = tools.make_subplots(rows=2, cols=2, vertical_spacing=0.04,\n                          subplot_titles=[\"Frequent words in biorxiv_data\", \n                                          \"Frequent words in clean_comm_data\",\n                                          \"Frequent words in clean_noncomm_data\",\n                                          \"Frequent words in clean_pmc_data\"])\nfig.append_trace(trace0, 1, 1)\nfig.append_trace(trace1, 1, 2)\nfig.append_trace(trace2, 2, 1)\nfig.append_trace(trace3, 2, 2)\n\n\n\nfig['layout'].update(height=1000, width=800, paper_bgcolor='rgb(233,233,233)', title=\"Word Count Plots of Texts in papers\")\niplot(fig, filename='word-plots')\n","73dd4f0b":"freq_dict = defaultdict(int)\nfor sent in df1:\n    for word in generate_ngrams(sent,2):\n        freq_dict[word] += 1\nfd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfd_sorted.columns = [\"word\", \"wordcount\"]\ntrace0 = horizontal_bar_chart(fd_sorted.head(25), 'yellow')\n\n\nfreq_dict = defaultdict(int)\nfor sent in df2:\n    for word in generate_ngrams(sent,2):\n        freq_dict[word] += 1\nfd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfd_sorted.columns = [\"word\", \"wordcount\"]\ntrace1 = horizontal_bar_chart(fd_sorted.head(25), 'red')\n\nfreq_dict = defaultdict(int)\nfor sent in df3:\n    for word in generate_ngrams(sent,2):\n        freq_dict[word] += 1\nfd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfd_sorted.columns = [\"word\", \"wordcount\"]\ntrace2 = horizontal_bar_chart(fd_sorted.head(25), 'brown')\n\n\nfreq_dict = defaultdict(int)\nfor sent in df4:\n    for word in generate_ngrams(sent,2):\n        freq_dict[word] += 1\nfd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfd_sorted.columns = [\"word\", \"wordcount\"]\ntrace3 = horizontal_bar_chart(fd_sorted.head(25), 'blue')\n\n\n# Creating two subplots\nfig = tools.make_subplots(rows=2, cols=2, vertical_spacing=0.04,horizontal_spacing=0.25,\n                          subplot_titles=[\"Frequent words in biorxiv_data\", \n                                          \"Frequent words in clean_comm_data\",\n                                          \"Frequent words in clean_noncomm_data\",\n                                          \"Frequent words in clean_pmc_data\"])\nfig.append_trace(trace0, 1, 1)\nfig.append_trace(trace1, 1, 2)\nfig.append_trace(trace2, 2, 1)\nfig.append_trace(trace3, 2, 2)\n\n\nfig['layout'].update(height=1000, width=800, paper_bgcolor='rgb(233,233,233)', title=\"Bigram Count Plots of Texts in papers\")\niplot(fig, filename='word-plots')","a596d2d7":"%%time\ntext_1 = biorxiv_data[\"abstract\"].dropna().apply(spacy_tokenizer)\ntext_2 = clean_comm_data[\"abstract\"].dropna().apply(spacy_tokenizer)\ntext_3 = clean_noncomm_data['abstract'].dropna().apply(spacy_tokenizer)\ntext_4 = clean_pmc_data['abstract'].dropna().apply(spacy_tokenizer)\n#count vectorization\nvectorizer_1= CountVectorizer(min_df=5, max_df=0.9, stop_words='english', lowercase=True, token_pattern='[a-zA-Z\\-][a-zA-Z\\-]{2,}')\nvectorizer_2= CountVectorizer(min_df=5, max_df=0.9, stop_words='english', lowercase=True, token_pattern='[a-zA-Z\\-][a-zA-Z\\-]{2,}')\nvectorizer_3 =  CountVectorizer(min_df=5, max_df=0.9, stop_words='english', lowercase=True, token_pattern='[a-zA-Z\\-][a-zA-Z\\-]{2,}')\nvectorizer_4= CountVectorizer(min_df=5, max_df=0.9, stop_words='english', lowercase=True, token_pattern='[a-zA-Z\\-][a-zA-Z\\-]{2,}')\n\ntext1_vectorized = vectorizer_1.fit_transform(text_1)\ntext2_vectorized = vectorizer_2.fit_transform(text_2)\ntext3_vectorized = vectorizer_3.fit_transform(text_3)\ntext4_vectorized = vectorizer_4.fit_transform(text_4)","ba1afb1f":"%%time\nlda1 = LatentDirichletAllocation(n_components=5, max_iter=5, learning_method='online',verbose=True)\nlda2= LatentDirichletAllocation(n_components=5, max_iter=5, learning_method='online',verbose=True)\nlda3 = LatentDirichletAllocation(n_components=5, max_iter=5, learning_method='online',verbose=True)\nlda4 = LatentDirichletAllocation(n_components=5, max_iter=5, learning_method='online',verbose=True)\n\nlda_1 = lda1.fit_transform(text1_vectorized)\nlda_2 = lda2.fit_transform(text2_vectorized)\nlda_3 = lda3.fit_transform(text3_vectorized)\nlda_4 = lda4.fit_transform(text4_vectorized)","9b5f7e0f":"def selected_topics(model, vectorizer, top_n=10):\n    for idx, topic in enumerate(model.components_):\n        print(\"Topic %d:\" % (idx))\n        print([(vectorizer.get_feature_names()[i], topic[i])\n                        for i in topic.argsort()[:-top_n - 1:-1]]) ","0be8cdd4":"print(\"LDA Model of Bioarvix data Abstracts:\")\nselected_topics(lda1, vectorizer_1)","2cda6bae":"print(\"LDA Model of clean_comm data Abstracts:\")\nselected_topics(lda2, vectorizer_2)","09825662":"print(\"LDA Model of clean_noncomm data Abstracts:\")\nselected_topics(lda3, vectorizer_3)","840ac2e2":"print(\"LDA Model of clean_pmc data Abstracts:\")\nselected_topics(lda4, vectorizer_4)","d72bd64b":"%%time\npyLDAvis.enable_notebook()\ndash = pyLDAvis.sklearn.prepare(lda1, text1_vectorized, vectorizer_1, mds='tsne')\ndash","e668d169":"%%time\npyLDAvis.enable_notebook()\ndash = pyLDAvis.sklearn.prepare(lda2, text2_vectorized, vectorizer_2, mds='tsne')\ndash","6c1d16b2":"%%time\npyLDAvis.enable_notebook()\ndash = pyLDAvis.sklearn.prepare(lda3, text3_vectorized, vectorizer_3, mds='tsne')\ndash","f43f559d":"%%time\npyLDAvis.enable_notebook()\ndash = pyLDAvis.sklearn.prepare(lda4, text4_vectorized, vectorizer_4, mds='tsne')\ndash","ca25b160":"del text_1,text_2,text_3,text_4,vectorizer_1,vectorizer_2,vectorizer_3,vectorizer_4,text1_vectorized,text2_vectorized ,text3_vectorized ,text4_vectorized ","8b468e77":"tqdm.pandas()\nfre_1 = np.array(biorxiv_data[\"abstract\"].dropna().apply(textstat.flesch_reading_ease))\nplot_readability(fre_1,\"Flesch Reading Ease\",20)","2227c31f":"tqdm.pandas()\nfre_2 = np.array(clean_comm_data[\"abstract\"].dropna().apply(textstat.flesch_reading_ease))\nplot_readability(fre_2,\"Flesch Reading Ease\",20,colors = ['#8D99AE'] )","a8e8abe9":"tqdm.pandas()\nfre_3 = np.array(clean_noncomm_data[\"abstract\"].dropna().apply(textstat.flesch_reading_ease))\nplot_readability(fre_3,\"Flesch Reading Ease\",20,colors = ['#C65D17'])","af6e3b0a":"tqdm.pandas()\nfre_4 = np.array(clean_pmc_data[\"abstract\"].dropna().apply(textstat.flesch_reading_ease))\nplot_readability(fre_4,\"Flesch Reading Ease\",20,colors = ['#DDB967'])","1810ec3d":"dcr_ = np.array(biorxiv_data[\"abstract\"].dropna().apply(textstat.dale_chall_readability_score))\nplot_readability(dcr_,\"Dale-Chall Readability Score\",1,['#C65D17'])","8dc6d09d":"dcr_ = np.array(clean_comm_data[\"abstract\"].dropna().apply(textstat.dale_chall_readability_score))\nplot_readability(dcr_,\"Dale-Chall Readability Score\",1,['#DDB967'])","b0f53143":"dcr_ = np.array(clean_noncomm_data[\"abstract\"].dropna().apply(textstat.dale_chall_readability_score))\nplot_readability(dcr_,\"Dale-Chall Readability Score\",1,['#8D99AE'])","a51b98ef":"dcr_ = np.array(clean_pmc_data[\"abstract\"].dropna().apply(textstat.dale_chall_readability_score))\nplot_readability(dcr_,\"Dale-Chall Readability Score\",1,['#EF233C'])","d72e2bcd":"ari_ = np.array(biorxiv_data[\"abstract\"].dropna().apply(textstat.coleman_liau_index))\nplot_readability(ari_,\"Automated Readability Index\",10,['#2B2D42'])","b94e2989":"ari_ = np.array(clean_comm_data[\"abstract\"].dropna().apply(textstat.coleman_liau_index))\nplot_readability(ari_,\"Automated Readability Index\",10,['#FF934F'])","0cf8938e":"ari_ = np.array(clean_noncomm_data[\"abstract\"].dropna().apply(textstat.coleman_liau_index))\nplot_readability(ari_,\"Automated Readability Index\",10,['#488286'])","b5170bf5":"ari_ = np.array(clean_pmc_data[\"abstract\"].dropna().apply(textstat.coleman_liau_index))\nplot_readability(ari_,\"Automated Readability Index\",10,['#8491A3'])","9d6aa9f6":"cli_ = np.array(biorxiv_data[\"abstract\"].dropna().apply(textstat.coleman_liau_index))\nplot_readability(cli_,\"The Coleman-Liau Index\",10,['#e8434e'])","8dbe0b08":"cli_ = np.array(clean_comm_data[\"abstract\"].dropna().apply(textstat.coleman_liau_index))\nplot_readability(cli_,\"The Coleman-Liau Index\",10,['#a36f72'])","dc408e0b":"cli_ = np.array(clean_noncomm_data[\"abstract\"].dropna().apply(textstat.coleman_liau_index))\nplot_readability(cli_,\"The Coleman-Liau Index\",10,['#2d5c5a'])","f4e6b72d":"cli_ = np.array(clean_pmc_data[\"abstract\"].dropna().apply(textstat.coleman_liau_index))\nplot_readability(cli_,\"The Coleman-Liau Index\",10,['#64c48c'])","c7d605fb":"fog_ = np.array(biorxiv_data[\"abstract\"].dropna().apply(textstat.gunning_fog))\nplot_readability(fog_,\"The Fog Scale (Gunning FOG Formula)\",4,['#d98714'])","084792fc":"fog_ = np.array(clean_comm_data[\"abstract\"].dropna().apply(textstat.gunning_fog))\nplot_readability(fog_,\"The Fog Scale (Gunning FOG Formula)\",4,['#7dd609'])","8c92a837":"fog_ = np.array(clean_noncomm_data[\"abstract\"].dropna().apply(textstat.gunning_fog))\nplot_readability(fog_,\"The Fog Scale (Gunning FOG Formula)\",4,['#E2D58B'])","b83a321c":"fog_ = np.array(clean_pmc_data[\"abstract\"].dropna().apply(textstat.gunning_fog))\nplot_readability(fog_,\"The Fog Scale (Gunning FOG Formula)\",4,['#612620'])","80e448c8":"#lets make one dataframe.\nall_data = pd.concat([biorxiv_data,clean_comm_data,clean_noncomm_data,clean_pmc_data]).reset_index()","51492b67":"text = all_data['text'][10]\nprint('Text before summarizations:')\nprCyan(text)","7dfaf39a":"print('Text after summarization:')\nprPurple(summarize(text,word_count = 250))","289ba6d8":"### Obseravtions: <br>\n* Here plots Speak more than words!! The graphs says it all.","b46890be":"## 2.4 Word Cloud of Authors in pmc_custom Data","d4ee0194":"## 2.3 Word Cloud of Authors in Non common use subset Data","57e33e28":"## Observations: \n* We can see from above word frequency plots most the top words consits of Virus,Respiratory,disease,Transmission etc..,","48558843":"<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n  <h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\"  role=\"tab\" aria-controls=\"home\">Notebook Content:<\/h3>\n  <a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#one_\" role=\"tab\" aria-controls=\"profile\">1. Importing DataFrames<span class=\"badge badge-primary badge-pill\">1<\/span><\/a>\n  <a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#two_\" role=\"tab\" aria-controls=\"messages\">2. Word Clouds of Authors<span class=\"badge badge-primary badge-pill\">2<\/span><\/a>\n  <a class=\"list-group-item list-group-item-action\"  data-toggle=\"list\" href=\"#three_\" role=\"tab\" aria-controls=\"settings\">3. Word Clouds of Affiliations<span class=\"badge badge-primary badge-pill\">3<\/span><\/a>\n   <a class=\"list-group-item list-group-item-action\"  data-toggle=\"list\" href=\"#four_\" role=\"tab\" aria-controls=\"settings\">4. Ngram Analysis<span class=\"badge badge-primary badge-pill\">4<\/span><\/a>\n   <a class=\"list-group-item list-group-item-action\"  data-toggle=\"list\" href=\"#five_\" role=\"tab\" aria-controls=\"settings\">5. Topic modelling<span class=\"badge badge-primary badge-pill\">5<\/span><\/a>\n    <a class=\"list-group-item list-group-item-action\"  data-toggle=\"list\" href=\"#six_\" role=\"tab\" aria-controls=\"settings\">6. Readability tests on Abstracts<span class=\"badge badge-primary badge-pill\">6<\/span><\/a>\n    <a class=\"list-group-item list-group-item-action\"  data-toggle=\"list\" href=\"#seven_\" role=\"tab\" aria-controls=\"settings\">7. Summarising Texts to Understand the content Fastly<span class=\"badge badge-primary badge-pill\">7<\/span><\/a>\n    \n    \n  ","e02c4c5b":"## 3.3 Word Cloud of Affiliations in Non common use subset Data","ec93e299":"#### Therefore we can say that Topic modelling will definitely help paper readers and researchers to understand current research.","f46979bc":"<font color='red'>If you find this kernel useful please consider upvoting it \ud83d\ude0a which keeps me motivated for doing hard work and to produce more quality content.<\/font>","9554feee":"<a id='four_'><\/a>\n# 4. Ngram Analysis","5124b720":"### 4.1.2 Bigram analysis of Abstracts","19714a40":"## 3.1 Word Cloud of Affiliations in biorxiv medrxiv Data","eb512c0c":"### Key Observations: <br>\n1. Topic 1 is related to protein expression,Gene expression and viruses. <br>\n2. Topic 2 is about how Covid-19 is effecting humans with respect to Age,also which human parts are more effected(respiratory system). <br>\n3. Topic 3 is about Related research , Public health. <br> \n4. Topic 4 is closely related to Coronvirus effects on humans also how it is transmitted. <br>\n5. The significant increase in Covid cases and treatment was related to Topic 5. <br>","d2500bc4":"* The plots says it all!","e78941b6":"### Readability is the ease with which a reader can understand a written text. In natural language processing, the readability of text depends on its content. It focuses on the words we choose, and how we put them into sentences and paragraphs for the readers to comprehend.","05b7a3ff":"## 6.2 Dale-Chall Readability Score <br>\n* Different from other tests, since it uses a lookup table of the most commonly used 3000 English words. Thus it returns the grade level using the New Dale-Chall Formula. <br>\n* The formula for calculating the raw score of the Dale\u2013Chall readability score is given below: <br>\n![](https:\/\/wikimedia.org\/api\/rest_v1\/media\/math\/render\/svg\/0541f1e629f0c06796c5a5babb3fac8d100a858c)\n\n* Score - Understood by\n\n** 4.9 or lower - average 4th-grade student or lower ** <br>\n** 5.0\u20135.9 - average 5th or 6th-grade student ** <br>\n** 6.0\u20136.9 - average 7th or 8th-grade student ** <br>\n** 7.0\u20137.9 - average 9th or 10th-grade student ** <br>\n** 8.0\u20138.9 - average 11th or 12th-grade student ** <br>\n** 9.0\u20139.9 - average 13th to 15th-grade (college) student and above** <br>\n\n* Read More : [Wikipedia](https:\/\/en.wikipedia.org\/wiki\/Dale%E2%80%93Chall_readability_formula)","c811309b":"### Observations: <br>\n* We can observe from above Automated Readability Index Plots that the mean values are about close to ~16.0 which means that the person who holds college degree or higher can understand the papers and their contents.","b4e2ca9e":"## 6.4 The Coleman-Liau Index <br>\n* Returns the grade level of the text using the Coleman-Liau Formula. This is a grade formula in that a score of 9.3 means that a ninth grader would be able to read the document. <br>\n* The Coleman\u2013Liau index is calculated with the following formula: <br>\n![](https:\/\/wikimedia.org\/api\/rest_v1\/media\/math\/render\/svg\/cae44bbb96eaaca26e6aaf3b65c342f69f3d49ce) <br>\nL is the average number of letters per 100 words and S is the average number of sentences per 100 words. <br>\n* Read More : [Wikipedia](https:\/\/en.wikipedia.org\/wiki\/Coleman%E2%80%93Liau_index)","5eb4a1d4":"## Thanks for reading.Alot more to come stay tuned and be safe \ud83d\ude0a. <br>\n<font color='red'>If you find this kernel useful please consider upvoting it \ud83d\ude0a which keeps me motivated for doing hard work and to produce more quality content.<\/font>","6564c29d":"<a id='seven_'><\/a>\n# 7. Summarising Texts to Understand the content Fastly","fb90fa17":"Yes, we can see there is alot of text here.So,what we can do is summarising the text so that its get easier to paper readers,researchers to fastly read and under stand the information in that text.","3fd6fc0e":"### 4.1.3 Trigram analysis of Abstracts","e3f2a8b6":"<a href=\"#top\" class=\"btn btn-primary btn-lg active\" role=\"button\" aria-pressed=\"true\">Go to TOP<\/a>","556ae914":"<a id='five_'><\/a>\n# 5. Topic modelling.\n* Topic modeling is a machine learning technique that automatically analyzes text data to determine cluster words for a set of documents. This is known as \u2018unsupervised\u2019 machine learning because it doesn\u2019t require a predefined list of tags or training data that\u2019s been previously classified by humans. <br>\n* Topic modeling involves counting words and grouping similar word patterns to infer topics within unstructured data. <br>\n* Read More : [Here](https:\/\/monkeylearn.com\/blog\/introduction-to-topic-modeling\/) <br>","7db16582":"## Observations:\n* We can observe from above bigram plot that most of words contains Respiratory syndrome as we know corona will effect our respiratory system.","507d1349":"We can see from the above output that the text was summarised to 250 words (you can choose on your own). Now its very easy to read and grasp the content in the text. It will be defenitely useful for the researchers and paper readers to grasp the content very fastly.","d22684b8":"<a id='two_'><\/a>\n# 2. Word Clouds of Authors","b37072ed":"### Key Observations: <br>\n1. Topic 1 is towards Virues,Protein,how human immune system damages,also how humans react to drugs. <br>\n2. Topic 2 is about public health,epidemic(a widespread occurrence of an infectious disease in a community at a particular time),Trasmission of Covid. <br>\n3. Topic 3 is closely related to Human Gene Sequencing,Pedv(Porcine epidemic diarrhea virus). <br>\n4. Topic 4 is about how to detect Covid-19 and how to Diagnosis the Covid-19. <br>\n5. I think topic 5 is about how to detect the Covid-19 we can see that there was more about how Covid will effect the Respiratory system. <br>\n","a2c5527c":"## 2.1 Word Cloud of Authors in biorxiv medrxiv Data","f39b0884":" <a id='six_'><\/a>\n # 6. Readability tests on Abstracts","9929b889":"<a id='one_'><\/a>\n# 1. Importing DataFrames","ab0a8fe5":"### 4.2.1 Word frequency Plots of texts in papers","ef7228ec":"<a id='three_'><\/a>\n# 3. Word Clouds of Affiliations","11a4b0f8":"## 2.2 Word Cloud of Authors in comm use subset Data","711f23c0":"## 6.3 Automated Readability Index <br>\n* Returns the ARI (Automated Readability Index) which outputs a number that approximates the grade level needed to comprehend the text.For example if the ARI is 6.5, then the grade level to comprehend the text is 6th to 7th grade.\n* Formula to calculate ARI: <br>\n![](https:\/\/wikimedia.org\/api\/rest_v1\/media\/math\/render\/svg\/878d1640d23781351133cad73bdf27bdf8bfe2fd) <br>\n* Read More: [Wikipedia](https:\/\/en.wikipedia.org\/wiki\/Automated_readability_index)","0c28a96b":"### Key Observations: \n1. Topic 1 infers to the Outbreak of Covid,Risk,Cure and its dangerous effects on human system. <br>\n2. Topic 2 is related to Gene Sequences. <br>\n3. Topic 3 is towards Symptoms of Patients who tested positive also about HBov (Human bocavirus is the name given to all viruses in the genus Bocaparvovirus of virus family Parvoviridae that are known to infect humans). <br>\n4. Topic 4 is about zoonotic disease(A zoonosis is an infectious disease caused by bacteria, viruses, or parasites that spread from non-human animals to humans). <br>\n5. Topic 5 Related to animal experiment. <br>","e3163e7c":"### Obseravtions:\n* We can observe papers text contains matter about t-cells.T cells are also called T lymphocyte, type of leukocyte (white blood cell) that is an essential part of the immune system. T cells are one of two primary types of lymphocytes\u2014B cells being the second type\u2014that determine the specificity of immune response to antigens (foreign substances) in the body.\n* Also about Influenza Virus, commonly known as \"the flu\", is an infectious disease caused by an influenza virus. Symptoms can be mild to severe. The most common symptoms include: high fever, runny nose, sore throat, muscle and joint pain, headache, coughing, and feeling tired.Which are also the symptoms of Corona Virus.","77ee0f9e":"### Observations:\n* From above Dale-Chall Readability Score tests we can observe that mean score was around ~9.7 which means these abstracts can understood by high grade students.","f0698b8f":"## 6.5 The Fog Scale (Gunning FOG Formula) <br>\n* In linguistics, the Gunning fog index is a readability test for English writing. The index estimates the years of formal education a person needs to understand the text on the first reading. For instance, a fog index of 12 requires the reading level of a United States high school senior (around 18 years old). <br>\n* The formula to calculate Fog scale: <br>\n![](https:\/\/wikimedia.org\/api\/rest_v1\/media\/math\/render\/svg\/84cd504cf61d43230ef59fbd0ecf201796e5e577)\n* Read more : [Wikipedia](https:\/\/en.wikipedia.org\/wiki\/Gunning_fog_index)","0e401d31":"<a id='top'><\/a>\n## Introduction: <br>\n#### Hi welcome to this notebook in this notebook we are gonna do analysis of papers with state of art text analytics methods by using COVID-19 Open Research Dataset.<br>\n\n* What is coronavirus disease 2019? <br>\nCoronavirus disease 2019 (COVID-19) is a respiratory illness that can spread from person to person. The virus that causes COVID-19 is a novel coronavirus that was first identified during an investigation into an outbreak in Wuhan, China. <br>\n\n* Can I get COVID-19? <br>\nUnfortunately Yes. COVID-19 is spreading from person to person in parts of the world. Risk of infection from the virus that causes COVID-19 is higher for people who are close contacts of someone known to have COVID-19, for example healthcare workers, or household members. Other people at higher risk for infection are those who live in or have recently been in an area with ongoing spread of COVID-19. <br>\n\n* How does COVID-19 spread? <br>\nThe virus that causes COVID-19 probably emerged from an animal source, but is now spreading from person to person. The virus is thought to spread mainly between people who are in close contact with one another (within about 6 feet) through respiratory droplets produced when an infected person coughs or sneezes. It also may be possible that a person can get COVID-19 by touching a surface or object that has the virus on it and then touching their own mouth, nose, or possibly their eyes, but this is not thought to be the main way the virus spreads.\n\n* Is there a vaccine? <br>\nThere is currently no vaccine to protect against COVID-19. The best way to prevent infection is to take everyday preventive actions, like avoiding close contact with people who are sick and washing your hands often. <br>\n\n* Is there a treatment? <br>\nThere is no specific antiviral treatment for COVID-19.People with COVID-19 can seek medical care to help relieve symptoms. <br>\n\n#### Dont get panic and be safe.\n","0298f69b":"\n<h1 style=\"text-align:center;font-size:200%;;\">Don't Panic<\/h1>\n<img src=\"https:\/\/media2.giphy.com\/media\/Y4WQSe2EucDEEUCKWu\/giphy.gif\" width=\"300\" height=\"300\" align=\"center\"\/>","1aa1cafe":"## 4.2 Ngram Analysis of Text in papers","0fd390a7":"### Observations: <br>\n* The above Coleman-Liau Index histogram plots says that the mean values of CLI was about ~16 which means the person with the age of about 25 years who completed his degree can read and understand the papers.","c8a70e88":"\n<h1 style=\"text-align:center;font-size:125%;;\">Follow these steps and protect yourself and your loving ones<\/h1>\n<img src=\"https:\/\/global.unitednations.entermediadb.net\/assets\/mediadb\/services\/module\/asset\/downloads\/preset\/Libraries\/Graphics+Library\/29-01-2020-WHO-CoronaVirus-anim-public-advice.gif\/image1170x530cropped.jpg\" width=\"500\" height=\"500\" align=\"center\"\/>","147ae98d":"## 6.1 The Flesch Reading Ease formula <br>\n* In the Flesch reading-ease test, higher scores indicate material that is easier to read; lower numbers mark passages that are more difficult to read. The formula for the Flesch reading-ease score (FRES) test is\n![](https:\/\/wikimedia.org\/api\/rest_v1\/media\/math\/render\/svg\/bd4916e193d2f96fa3b74ee258aaa6fe242e110e)","5db6bd5d":"## 4.1 Ngram Analysis of Abstracts","d81e8d88":"### Key Observations: <br>\n#### We can infer from the above Topic modelling is that: <br>\n1. The topic 1 is about Epidemiology.Epidemiology is the branch of medicine which deals with the incidence, distribution, and possible control of diseases and other factors relating to health. <br>\n2. The topic 2 is more about Protein,Human Cells,Genes also drug tests on Mice. <br>\n3. Topic 3 is towards genetic and bioinformatic. <br>\n4. Topic 4 is about Pneumonia,Respiratory diseases and clinical medicine. <br>\n5. Topic 5 is about SARS(Severe Acute Respiratory Syndrome),nCov (novel corono virus),Mers (Middle East respiratory syndrome-related coronavirus),Epitope(he part of an antigen that is recognized by the immune system, specifically by antibodies, B cells, or T cells). ","c94b8f5b":"### 4.1.1 Word Frequency analysis of abstracts","09f2b2a1":"### Observations: <br>\n*  From the above Flesch Reading Ease formula on abstracts, we can obsereve that the mean value was around 26 for all the data which infers that it is bit confusing to read and understand the abstracts.","5cd550c4":"### 4.2.2 Bigram Plots of texts in papers","055dec29":"## 3.4 Word Cloud of Affiliations in pmc_custom Data","ea36285e":"## 3.2 Word Cloud of Affiliations in comm use subset Data","4c3f9c3a":"## Score - Difficulty <br>\n* 90-100 - Very Easy\n* 80-89 - Easy\n* 70-79 - Fairly Easy\n* 60-69 - Standard\n* 50-59 - Fairly Difficult\n* 30-49 - Difficult\n* 0-29 - Very Confusing\n\nRead More : [Wikipedia](https:\/\/en.wikipedia.org\/wiki\/Flesch%E2%80%93Kincaid_readability_tests)"}}