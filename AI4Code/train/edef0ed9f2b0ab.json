{"cell_type":{"443d93f7":"code","cebea53e":"code","97018e98":"code","1a85d1ad":"code","22ecfe59":"code","2edc1974":"code","372466c2":"code","74b0edf7":"code","88879971":"code","4dfa764d":"code","6f82b442":"code","a373b24b":"code","c3e9cfb2":"code","47183cb2":"code","112ab5e6":"code","bf12dbc5":"code","3fc136df":"code","284f9565":"code","7709e017":"code","bc8e3ec4":"code","ccc2a612":"code","ec759057":"code","e70e9f8f":"code","1491e5c1":"code","df61376d":"code","a21d7c7b":"code","2de01dde":"code","7a263c06":"code","5ba294b3":"code","4622c747":"code","5333013c":"code","ae6225eb":"code","f20ecb22":"markdown","7846a76c":"markdown","e798e6d8":"markdown","e0de48bc":"markdown","9bb7ee34":"markdown","d41a2b66":"markdown","05cd24bb":"markdown","1ad94402":"markdown","99820cc8":"markdown","0d84ff2b":"markdown","b1ef7ce6":"markdown","4415acab":"markdown","71d2caee":"markdown","2552d477":"markdown","268bedc9":"markdown","ca8ebf79":"markdown","f4a12bb6":"markdown","456d31ba":"markdown","0afb4d44":"markdown","0f1a0910":"markdown","630eb4e6":"markdown","2e613f6e":"markdown","c36bda6a":"markdown","4d55aa40":"markdown","60212057":"markdown","a3a75aff":"markdown","266c04a8":"markdown","611a9c76":"markdown","057c0721":"markdown","d526470d":"markdown","852a7132":"markdown","e9ac0cce":"markdown"},"source":{"443d93f7":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","cebea53e":"df=pd.read_csv(\"\/kaggle\/input\/foreign-exchange-rates-per-dollar-20002019\/Foreign_Exchange_Rates.csv\")","97018e98":"df.head()","1a85d1ad":"df=df.drop('Unnamed: 0',axis=1)","22ecfe59":"df.describe(include='all')","2edc1974":"df.info(verbose=True)","372466c2":"col_list=list(df)","74b0edf7":"#excluding the \"0\" the column, date column, as it will be converted to date-time object.\ndf[col_list[1:]] = df[col_list[1:]].apply(pd.to_numeric, errors='coerce')","88879971":"df['Time Serie']=pd.to_datetime(df['Time Serie'])","4dfa764d":"df.info(verbose=True)","6f82b442":"df_melted=df.melt(id_vars=[\"Time Serie\"], \n        var_name=\"Currency type\", \n        value_name=\"Value\")","a373b24b":"df_melted.head()","c3e9cfb2":"import seaborn as sns; sns.set()\nimport matplotlib.pyplot as plt\n\nsns.set_style(\"white\")\nplt.figure(figsize=(20,10))\nsns.lineplot(x=\"Time Serie\", y=\"Value\", hue=\"Currency type\",palette='deep',data=df_melted)\nplt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)","47183cb2":"df_melted_kor=df_melted.loc[df_melted['Currency type'] == \"KOREA - WON\/US$\"]\ndf_melted_nonkor=df_melted.loc[df_melted['Currency type'] != \"KOREA - WON\/US$\"]","112ab5e6":"import seaborn as sns; sns.set()\nimport matplotlib.pyplot as plt\n\nsns.set_style(\"white\")\nfig, ax1 = plt.subplots(figsize=(20,10))\nax2 = ax1.twinx()\n\ng=sns.lineplot(x=\"Time Serie\", y=\"Value\", hue=\"Currency type\",ax=ax1,data=df_melted_nonkor)\ng.legend(bbox_to_anchor=(-0.05, 1), loc=0, borderaxespad=0.)\n\n#Using the dashes for korea data to stand out\ng1=sns.lineplot(x=\"Time Serie\", y=\"Value\", color=\"coral\",label=\"KOREA - WON\/US$\",ax=ax2,data=df_melted_kor)\ng1.lines[0].set_linestyle(\"--\")\ng1.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)","bf12dbc5":"kor_data = df_melted_kor.drop(\"Currency type\",axis=1).rename(columns={'Time Serie': 'ds', 'Value': 'y'})","3fc136df":"kor_data","284f9565":"#splitting 80% training and 20% testing\n\ntrain_kor_data, test_kor_data = np.split(kor_data, [int(.8*len(df))])","7709e017":"from fbprophet import Prophet\n\nprophet_basic=Prophet()\nprophet_basic.fit(train_kor_data)\ntrain_kor_future= prophet_basic.make_future_dataframe(periods=4*365)","bc8e3ec4":"train_kor_forecast=prophet_basic.predict(train_kor_future)\nfig1 =prophet_basic.plot(train_kor_forecast)","ccc2a612":"from fbprophet.plot import add_changepoints_to_plot\nfig = prophet_basic.plot(train_kor_forecast)\na = add_changepoints_to_plot(fig.gca(), prophet_basic, train_kor_forecast)","ec759057":"train_kor_forecast.head()","e70e9f8f":"train_kor_yhat=train_kor_forecast['yhat']","1491e5c1":"train_kor_yhat_1460=train_kor_yhat[-1460:]\ntrain_kor_yhat_1460","df61376d":"test_kor_data","a21d7c7b":"merged_kor_data=test_kor_data.merge(train_kor_forecast,left_on='ds',right_on='ds')","2de01dde":"merged_kor_data.head()","7a263c06":"test_kor_data_2=merged_kor_data[['ds','y']].copy()\ntrain_kor_yhat_2=merged_kor_data[['ds','yhat']].copy()","5ba294b3":"test_kor_data_2.isnull().sum()","4622c747":"test_kor_data_2['y']=test_kor_data_2['y'].fillna(test_kor_data_2['y'].median())","5333013c":"from sklearn.metrics import mean_squared_error\nfrom math import sqrt\nmse=mean_squared_error(test_kor_data_2[\"y\"],train_kor_yhat_2[\"yhat\"])\nrmse=sqrt(mse)\n\nprint(\"RMSE :\", rmse)","ae6225eb":"import matplotlib.pyplot as plt\nplt.figure(figsize=(20,10))\nplt.title(\"Korea Won currecy exchange rate predictions vs actual test\")\npredictions, = plt.plot(train_kor_yhat_2[\"ds\"],train_kor_yhat_2[\"yhat\"],label=\"Predictions\")\ntest, = plt.plot(test_kor_data_2['ds'],test_kor_data_2['y'],label=\"Test\")\nplt.legend(loc='upper right')","f20ecb22":"One thing I noticed was that this result which is supposed to cotain the mean, std and min values for all currencies do not actually have anything. This is weird as we clearly have numbers in our original dataframe. It could be because these numbers are not float values. Let's check the datatype with an 'info' method.","7846a76c":"Checking the data type one more time.","e798e6d8":"Now extracting the yhat values and saving it to a series.","e0de48bc":"Ah, we can see that all of our values in the dataframe are not floats but strings. Let's turn the number columns into numerical feature columns.","9bb7ee34":"# 2. EDA","d41a2b66":"Let's grab the last 1,460 rows to compare against the test dataset.","05cd24bb":"Prophet requires you to have two columns, \"ds\" and \"y\" with the dates and values respectively.","1ad94402":"Looking good. Let's plot the dataset then.","99820cc8":"Let's fit the train data set and create future dataframe for 4 years (4*365)","0d84ff2b":"Now let's take a look at our test dataset.","b1ef7ce6":"# 1. Introduction","4415acab":"Then let's take care of Time series column data by converting it to date_time.","71d2caee":"We can see that our predictions were not bad although they did not caputer the sudden drops around 2018-01. I am surpirsed how good facebook's Prophet is at analyzing time series with simple tricks like this!","2552d477":"The data was split into 80% of training data and 20% of test data.","268bedc9":"Now it looks correct :) Alright onto the real EDA!","ca8ebf79":"Let's also display the changepoints (points where real time series frequently have abrupt changes in their trajectories) to see if the model correctly recognized them.","f4a12bb6":"# 3. Time series analysis","456d31ba":"Alright. Now we can call the predict method. This will assign each row in our 'future' dataframe a predicted value, which it names yhat. Additionally, it will show lower\/upper bounds of uncertainty, called yhat_lower and yhat_upper. (ref:https:\/\/www.interviewqs.com\/ddi_code_snippets\/prophet_intro)\n\nI will also plot the predicted train dataset.","0afb4d44":"Now we should not forget that the test dataframe had some NaN values. Let's how many null values there are.","0f1a0910":"In this project, I am going to explore datasets (looking at each currecy exchange rate as a funciton of time) and then create regression models using various algorithms (GradientDescent, SVM, etc) to predict future currecy exchange rate behaviors.","630eb4e6":"Let's look at the statistics of dataframe.","2e613f6e":"Let's change the structure of the dataframe such that we list all the currency types in one column. The same goes for the \"value\" attribute.","c36bda6a":"This RMSE value is not significnatly bad, given that the Korean Won data values were populated around ~1,000. Let's plot both preditions and actual test data together.","4d55aa40":"Alright. Let's compare the predicted yhat values with the test data to see how good our fitting was.\nBut before we do this we need to ensure that the length of test dataframe and the yhat dataset (train_kor_yhat_1460) should match. Let's merge them first based on the \"ds\" values.","60212057":"It seems like Korea-Won data's high value pushed other data to the bottom, making it hard to analyze.\n\nLet's separate Korea-Won data from the orignal dataframe to distinguish it from the rest of the dataset.","a3a75aff":"Looks like the \"unnamed:0\" is the same as our index column, so let's drop it first","266c04a8":"We can see there are 45 NaN values. Let's fill these in with median values.","611a9c76":"Facebook released time series forecasting tool called Prophet in 2017. This was designed to analyze time-series on different time scales such as yearly, weekly and daily. Let's use this on our Korea data for time series analysis.","057c0721":"Ok now let's see how good our Prophet predictions were based on RMSE","d526470d":"Based on this plot, it seems like Korea Won had experiened a large spike during year 2008 when there was a global economic crsis.\n\nLet's create some regression models based on these datasets for each country","852a7132":"It seems the model correctly recognized the abrupt change points.\nLet's look at the predicted train dataframe.","e9ac0cce":"Ok. Let's separate them into a predicted dataset and test dataset one more time."}}