{"cell_type":{"edcdafdc":"code","5e45cebd":"code","d2b7cb28":"code","b3ff3f0c":"code","b56d0d74":"code","1add1b54":"code","a6767293":"code","827e1fea":"code","006061e8":"code","fee5e398":"code","c9bae903":"code","8ce7f162":"code","e5638162":"code","d45bb05e":"code","aea7abb3":"code","18bd3237":"code","8f1563cb":"code","21ea5fc1":"code","56f1f545":"code","1b307061":"code","64503e8f":"code","3286947f":"code","5c0123ba":"code","7ea006a2":"code","1ea05127":"code","505932c9":"code","a5381a6d":"code","8dc6fcd2":"code","71612d89":"code","de4dc29d":"code","936ed1c7":"code","e632e85c":"code","fd0278d1":"code","d7ae970c":"code","a239ee95":"code","0e24a926":"code","394615cd":"code","730e00e2":"code","688ce004":"code","b33f6033":"code","400b8f3d":"code","21d0ef9a":"code","fd59e364":"code","2706d697":"code","f4817663":"code","b0464611":"code","2ff296b6":"code","d803cf62":"code","33652c64":"code","d6cada22":"code","6c47a934":"code","4f1d372f":"code","a1458cd5":"code","2397c8f4":"code","88690a6a":"code","dd111524":"code","fbfb4080":"code","968fce47":"code","06b19cb0":"code","b57fcadd":"code","95961bbc":"code","176f0c75":"code","6e65647c":"code","17603611":"code","eb8b6c92":"code","14dbda70":"code","d373fe71":"code","297941a3":"code","eecf943c":"code","4a48b933":"code","cf8c0d48":"code","a4382a47":"code","aa061d17":"code","f84910be":"code","fac712a3":"code","3dc46e8f":"code","07653bc6":"code","c8b7e9a4":"code","4ec5e4ff":"code","07f2447e":"code","7e74edbe":"code","dfd3a6c4":"code","538d35bb":"code","5564e6e5":"code","66f431e2":"code","0bab1e04":"code","16060884":"code","f121269a":"code","8bd29f08":"code","d562eae6":"code","08275b59":"code","4538e4ca":"code","edb36291":"code","8ca9a71a":"code","73750da8":"code","c27da71f":"code","16553fc4":"code","6bfeefa4":"code","829ec123":"code","dcab4d83":"code","d75171d2":"code","0ff98221":"code","a5048313":"markdown","3058ea06":"markdown","0d9bdcd2":"markdown","bfddfa9c":"markdown","0ee5fcae":"markdown","790ad15b":"markdown","ad5e8775":"markdown","8ffb1f48":"markdown","4c08c453":"markdown","671e2d59":"markdown"},"source":{"edcdafdc":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","5e45cebd":"# Python \u22653.5 is required\nimport sys\nassert sys.version_info >= (3, 5)\n\n# Scikit-Learn \u22650.20 is required\nimport sklearn\nassert sklearn.__version__ >= \"0.20\"\n\n# import usual libraries \nimport pandas as pd\nimport numpy as np\n\n# visualization libraries\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\n# consistent sized plots \nfrom pylab import rcParams\nrcParams['figure.figsize'] = 12,5\nrcParams['axes.labelsize']= 14\nrcParams['xtick.labelsize'] =12\nrcParams['ytick.labelsize'] = 12 \n\n# handle operating system dependencies \nimport os\n\n# handle unwanted warnings \nimport warnings\nwarnings.filterwarnings(action='ignore',message='^internal gelsd')","d2b7cb28":"# READ THE TRAINING DATA AND TEST DATA INTO DATAFRAMES\ntrain_credit = pd.read_csv('..\/input\/south-german-credit-prediction\/train.csv')\ntest_credit = pd.read_csv('..\/input\/south-german-credit-prediction\/test.csv') ","b3ff3f0c":"train_credit.head(20)","b56d0d74":"train_credit.columns","1add1b54":"# assign the corresponding english column names\ntrain_credit.columns = ['Id','status', 'duration', 'credit_history', 'purpose', 'amount', 'savings', 'employment_duration', 'installment_rate', 'personal_status_sex', 'other_debtors', 'present_residence', 'property', 'age', 'other_installment_plans', 'housing', 'number_credits', 'job', 'people_liable', 'telephone', 'foreign_worker', 'credit_risk']\ntest_credit.columns =  ['Id','status', 'duration', 'credit_history', 'purpose', 'amount', 'savings', 'employment_duration', 'installment_rate', 'personal_status_sex', 'other_debtors', 'present_residence', 'property', 'age', 'other_installment_plans', 'housing', 'number_credits', 'job', 'people_liable', 'telephone', 'foreign_worker']","a6767293":"# check the dataframe after rename of the columns\ntrain_credit.head(5)","827e1fea":"train_credit.describe().transpose()","006061e8":"# check info \ntrain_credit.info()","fee5e398":"# explicit check for any null values in the dataframe \ntrain_credit.isnull().sum()","c9bae903":"sns.countplot(train_credit['credit_risk'])","8ce7f162":"sns.countplot(train_credit['foreign_worker'],hue=train_credit['credit_risk'])","e5638162":"sns.countplot(train_credit['credit_risk'],hue=train_credit['purpose'])","d45bb05e":"plt.hist(train_credit['amount'],bins=30);","aea7abb3":"# check the bad loans\ntrain_credit[train_credit['credit_risk']==0]","18bd3237":"plt.hist(train_credit[train_credit['credit_risk']==0]['amount'])\nplt.title('Bad Loans Amount Histogram')","8f1563cb":"# check the good and bad loan risk \ntrain_credit['credit_risk'].value_counts()","21ea5fc1":"data= train_credit.copy()","56f1f545":"# drop the Id column as it is not useful for the model \ndata.drop(['Id'],inplace=True,axis=1)","1b307061":"data.head(3)","64503e8f":"log_amount = np.log(data['amount'])\nsns.distplot(log_amount,bins=20)","3286947f":"# import the varios classifier models\nfrom sklearn.linear_model import SGDClassifier,LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier,GradientBoostingClassifier,AdaBoostClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import BaggingClassifier\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import cross_val_predict, cross_val_score,KFold, RepeatedStratifiedKFold","5c0123ba":"new_data = train_credit.copy()","7ea006a2":"new_data.info()","1ea05127":"new_data.drop('Id',axis=1,inplace=True)","505932c9":"new_data['log_amount'] = round(np.log(new_data['amount']),2)","a5381a6d":"new_data.drop('amount',axis=1,inplace=True)","8dc6fcd2":"new_data['log_age'] =  round(np.log(new_data['age']),2)","71612d89":"new_data['log_duration'] = round(np.log(new_data['duration']),2)","de4dc29d":"new_data.drop(['age','duration'],axis=1,inplace=True)","936ed1c7":"new_data.head()","e632e85c":"new_data.tail()","fd0278d1":"X_full = new_data.drop('credit_risk',axis=1)\ny_full = new_data['credit_risk']","d7ae970c":"# this will ensure that the data is randomized and then split into train and test \n# alternatively StratifiedRandomSplit is also recommended\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X_full, y_full, test_size=0.2, random_state=42)","a239ee95":"# try various models and pick the best one for further tuning \ndef cross_validate(X = X_train,y = y_train):\n    \n    warnings.filterwarnings(action='ignore',message='')\n\n    models = []\n    models.append(('RF',RandomForestClassifier()))\n    models.append(('GB',GradientBoostingClassifier()))\n    models.append(('SVC',SVC()))\n    models.append(('SGD',SGDClassifier()))\n    models.append(('LogReg',LogisticRegression()))\n    models.append(('AdaBoost',AdaBoostClassifier()))\n    models.append(('Bag',BaggingClassifier()))\n    models.append(('xgboost',XGBClassifier()))\n    models.append(('lightgbm',LGBMClassifier()))\n    models.append(('Dtree',DecisionTreeClassifier()))\n    \n\n    results = []\n    names = []\n    scoring ='accuracy'\n\n    for name,model in models:\n        #kfold = KFold(n_splits=10,random_state=42)\n        kfold = RepeatedStratifiedKFold(n_splits=10,random_state=42,n_repeats=3)\n        cv_results = cross_val_score(model,X,y,cv=kfold,scoring=scoring)\n        results.append(cv_results)\n        names.append(name)\n        print (f'Model:{name},Mean: {cv_results.mean()},Std Dev: {cv_results.std()}')","0e24a926":"cross_validate(X_train,y_train)","394615cd":"test_sub = test_credit.copy()","730e00e2":"test_sub.drop('Id',axis=1,inplace=True)","688ce004":"test_sub['log_amount'] = round(np.log(test_sub['amount']),2)","b33f6033":"test_sub['log_age'] =  round(np.log(test_sub['age']),2)","400b8f3d":"test_sub['log_duration'] = round(np.log(test_sub['duration']),2)","21d0ef9a":"test_sub.drop(['amount','age','duration'],axis=1,inplace=True)","fd59e364":"test_sub.head()","2706d697":"from imblearn.over_sampling import ADASYN\nfrom imblearn.over_sampling import SMOTE # use either ADASYN or SMOTE\nfrom collections import Counter","f4817663":"ada = ADASYN(sampling_strategy='minority',random_state=42,n_neighbors=7)\nX_res,y_res = ada.fit_resample(X_train,y_train)\nCounter(y_res)","b0464611":"from sklearn.preprocessing import StandardScaler\nscalar = StandardScaler()","2ff296b6":"cross_validate(X_res,y_res)","d803cf62":"from sklearn.model_selection import GridSearchCV,RandomizedSearchCV","33652c64":"param_grid = [{'n_estimators': [3, 10, 30], 'max_depth': [2, 4, 6, 8],'booster': ['gbtree','dart'],\n              'learning_rate':[0.3,0.5,0.01,0.1]}]","d6cada22":"xgb_clf = XGBClassifier(random_state=42)\n\ngrid_search = GridSearchCV(xgb_clf, param_grid=param_grid, cv=5,\n                           scoring='accuracy',\n                           return_train_score=True)\ngrid_search.fit(X_res,y_res)","6c47a934":"grid_search.best_params_","4f1d372f":"from scipy.stats import randint\n\nparam_distribs = {\n        'n_estimators': randint(low=1, high=500),\n        'max_depth': randint(low=1, high=10),\n        'max_features':randint(low=1,high=10),\n        \n    }\n\nrf_clf = RandomForestClassifier(random_state=42)\nrnd_search = RandomizedSearchCV(rf_clf, param_distributions=param_distribs,\n                                n_iter=10, cv=5, scoring='accuracy', random_state=42)\nrnd_search.fit(X_res,y_res)","a1458cd5":"rnd_search.best_params_","2397c8f4":"rf_clf = RandomForestClassifier(random_state=42,max_depth=8,max_features=6,n_estimators=386)","88690a6a":"# hyper parameters selcted based on grid search \nxgb_clf =  XGBClassifier(n_estimators=30,max_depth=8,random_state=42,learning_rate=0.3,\n                        booster='gbtree')","dd111524":"svc_clf = SVC(random_state=42)   # with default paramters","fbfb4080":"gb_clf = GradientBoostingClassifier(random_state=42) # default parameters","968fce47":"bag_clf = BaggingClassifier(random_state=42,base_estimator=XGBClassifier())","06b19cb0":"xgb_clf.fit(X_res,y_res)","b57fcadd":"rf_clf.fit(X_res,y_res)","95961bbc":"svc_clf.fit(X_res,y_res)","176f0c75":"gb_clf.fit(X_res,y_res)","6e65647c":"bag_clf.fit(X_res,y_res)","17603611":"predictions_train_xgb = xgb_clf.predict(X_test)","eb8b6c92":"predictions_train_rf = rf_clf.predict(X_test)","14dbda70":"predictions_train_svc = svc_clf.predict(X_test)","d373fe71":"predictions_train_gb = gb_clf.predict(X_test)","297941a3":"predictions_train_bag = bag_clf.predict(X_test)","eecf943c":"from sklearn.metrics import accuracy_score,confusion_matrix,precision_score,recall_score\nprint('Accuracy XGBoost...{}'.format(accuracy_score(y_test,predictions_train_xgb)))\nprint('Accuracy RForest...{}'.format(accuracy_score(y_test,predictions_train_rf)))\nprint('Accuracy SupportVector...{}'.format(accuracy_score(y_test,predictions_train_svc)))\nprint('Accuracy GBoost...{}'.format(accuracy_score(y_test,predictions_train_gb)))\nprint('Accuracy Bagging...{}'.format(accuracy_score(y_test,predictions_train_gb)))","4a48b933":"print('Precision XGBoost...{}'.format(precision_score(y_test,predictions_train_xgb)))\nprint('Precision RForest...{}'.format(precision_score(y_test,predictions_train_rf)))\nprint('Precision SupportVector...{}'.format(precision_score(y_test,predictions_train_svc)))\nprint('Precision GBoost...{}'.format(precision_score(y_test,predictions_train_gb)))\nprint('Precision Bagging...{}'.format(precision_score(y_test,predictions_train_gb)))","cf8c0d48":"print('Recall XGBoost...{}'.format(recall_score(y_test,predictions_train_xgb)))\nprint('Recall RForest...{}'.format(recall_score(y_test,predictions_train_rf)))\nprint('Recall SupportVector...{}'.format(recall_score(y_test,predictions_train_svc)))\nprint('Recall GBoost...{}'.format(recall_score(y_test,predictions_train_gb)))\nprint('Recall Bagging...{}'.format(recall_score(y_test,predictions_train_gb)))","a4382a47":"print('XGBoost_Confusion Matrix')\nprint(confusion_matrix(y_test,predictions_train_xgb))\nprint('RandomForest_Confusion Matrix')\nprint(confusion_matrix(y_test,predictions_train_rf))\nprint('SupportVector_Confusion Matrix')\nprint(confusion_matrix(y_test,predictions_train_svc))\nprint('GradientBoosting_Confusion Matrix')\nprint(confusion_matrix(y_test,predictions_train_gb))\nprint('Bagging_Confusion Matrix')\nprint(confusion_matrix(y_test,predictions_train_gb))","aa061d17":"train_oversample = pd.concat([X_res,X_test],axis=0)","f84910be":"test_oversample = pd.concat([y_res,y_test],axis=0)","fac712a3":"train_oversample.shape","3dc46e8f":"train_oversample.columns","07653bc6":"test_oversample.shape # contains 0 and 1 for the credit risk","c8b7e9a4":"train_oversample = scalar.fit_transform(train_oversample)","4ec5e4ff":"xgb_clf.fit(train_oversample,test_oversample)","07f2447e":"rf_clf.fit(train_oversample,test_oversample)","7e74edbe":"svc_clf.fit(train_oversample,test_oversample)","dfd3a6c4":"gb_clf.fit(train_oversample,test_oversample)","538d35bb":"bag_clf.fit(train_oversample,test_oversample)","5564e6e5":"predictions_final_xgb = xgb_clf.predict(scalar.transform(test_sub))\npredictions_final_rf = rf_clf.predict(scalar.transform(test_sub))\npredictions_final_svc = svc_clf.predict(scalar.transform(test_sub))\npredictions_final_gb = gb_clf.predict(scalar.transform(test_sub))\npredictions_final_bag = bag_clf.predict(scalar.transform(test_sub))","66f431e2":"s_xgb = pd.Series(predictions_final_xgb, name='XGB')\ns_rf = pd.Series(predictions_final_rf, name='RF')\ns_svc = pd.Series(predictions_final_svc, name='SVC')\ns_gb = pd.Series(predictions_final_gb, name='GB')\ns_bag = pd.Series(predictions_final_bag, name='BAG')\nidx = test_credit['Id']","0bab1e04":"model_pred = pd.concat([idx,s_xgb,s_rf,s_svc,s_gb,s_bag],axis=1)\nmodel_pred.head()","16060884":"model_pred['vote'] = model_pred[['XGB','RF','SVC','GB','BAG']].sum(axis=1)","f121269a":"model_pred.head()","8bd29f08":"# criteria to select the final credit risk score \ndef vote(vote_sum):\n    if vote_sum >=2:\n        return 1\n    else:\n        return 0","d562eae6":"model_pred['kredit'] = model_pred['vote'].apply(vote)","08275b59":"model_pred.tail()","4538e4ca":"submission = model_pred.drop(['XGB','RF','SVC','GB','BAG','vote'],axis=1)","edb36291":"submission.tail(20)","8ca9a71a":"submission.to_csv('AggregatedModel_Predictions.csv',index=False)","73750da8":"scalar = StandardScaler()\nX_res_sc =  scalar.fit_transform(X_res)\nX_test_sc = scalar.transform(X_test)","c27da71f":"X_train_ar = np.asarray(X_res_sc)\ny_train_ar = np.asarray(y_res)\nX_test_ar = np.asarray(X_test_sc)\ny_test_ar = np.asarray(y_test)","16553fc4":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense,Flatten,Dropout\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow import keras","6bfeefa4":"keras.backend.clear_session()\nmodel_ann = Sequential()\n\n# add 3 dense layers and the final output layer\nmodel_ann.add(Flatten())\nmodel_ann.add(Dense(units=300)) # activations relu,tanh,elu all resulted in exploding val_loss\nmodel_ann.add(Dense(units=200))\nmodel_ann.add(Dense(units=100))\nmodel_ann.add(Dropout(0.5))\nmodel_ann.add(Dense(units=10))\n\n\n# final output layer\nmodel_ann.add(Dense(units=1,activation='sigmoid'))\n\n# compile the model\nmodel_ann.compile(optimizer='nadam',metrics=['accuracy'],loss='binary_crossentropy')","829ec123":"early_stop = EarlyStopping(patience=50,monitor='val_loss',restore_best_weights=True)","dcab4d83":"model_ann.fit(X_train_ar,y_train_ar,epochs=300,callbacks=[early_stop],\n             validation_data=(X_test_ar,y_test_ar))","d75171d2":"predictions_ann = model_ann.predict_classes(X_test_ar)","0ff98221":"print(accuracy_score(y_test,predictions_ann))","a5048313":"### Train the models on the entire training set & then predict on the submission test set ","3058ea06":"### Balance the data by Oversampling using ADASYN Library","0d9bdcd2":"- The data is organized where the last 200 rows has credit risk as 0\n- Hence it is important to shuffle the dataset","bfddfa9c":"### ***Train Artificial Neural Network - Optional Step***","0ee5fcae":"### Multiple Model Voting\n- If atleast 2 models predict credit risk as 1, then final score is 1 else it is 0","790ad15b":"### Prepare data for Machine Learning Models\n- age can be converted to ordinal data, for simplicity went with log transformation\n- further improvement --> create data preparation and transformation pipeline","ad5e8775":"- As expected there are way more people with good credit compare to bad credit risk\n- The dataset is not balanced and accuracy of the model won't be good indication unless trained with balanced data","8ffb1f48":"- The badloans is right skewed and with maximum bad loans under 7500\n- Highest bad loan could be for the business (can be checked quickly)","4c08c453":"### Prepare the test data ","671e2d59":"- Based on the above result we will use the best models to fit and predict the score"}}