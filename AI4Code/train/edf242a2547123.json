{"cell_type":{"e9836518":"code","10512aff":"code","e66bee17":"code","c2aabc3f":"code","e9e09129":"code","962e7732":"code","db830050":"code","1d249aa1":"code","0de005b0":"code","6b3b5658":"code","cf96a9b7":"code","ff8728ec":"code","204bfc33":"markdown","321c5282":"markdown","db6e6d41":"markdown","619a39a8":"markdown"},"source":{"e9836518":"# Standard imports \nimport tensorflow as tf\nimport keras \nimport matplotlib.pyplot as plt\nimport numpy as np\nimport cv2\nimport os\nimport sys\nimport random","10512aff":"from keras.preprocessing.image import ImageDataGenerator\ntest_dir = '..\/input\/neuron-cy5-images\/neuron cy5 test data\/Neuron Cy5 Test Data'\ninput_size = 331\n\n# Keras data generator to load image samples in batches\ndata_gen = ImageDataGenerator(samplewise_center=True,\n                              samplewise_std_normalization=True)\ntest_gen = data_gen.flow_from_directory(test_dir,\n                                        target_size=(input_size,input_size),\n                                        color_mode='grayscale',\n                                        class_mode='categorical',\n                                        batch_size=1,\n                                        shuffle=True)\n\nclasses = dict((v, k) for k, v in test_gen.class_indices.items())\nnum_classes = len(classes)\nnum_samples = len(test_gen)","e66bee17":"from tensorflow.python.keras.models import Model\nfrom tensorflow.python.keras.applications import VGG19\nfrom tensorflow.python.keras.layers import GlobalMaxPooling2D, Dense\n\n# Create a VGG19 architecture\npretrained_model = VGG19(include_top=False,\n                         pooling='none',\n                         input_shape=(input_size, input_size, 3),\n                         weights=None)\nx = GlobalMaxPooling2D()(pretrained_model.output)\nx = Dense(2048, activation='relu')(x)\nx = Dense(2048, activation='relu')(x)\noutput = Dense(num_classes, activation='softmax')(x)\nvgg19_model = Model(pretrained_model.input, output)\n\n# Create new model with modified config which accepts the input shape: [input_size, input_size, 1]\ncfg = vgg19_model.get_config()\ncfg['layers'][0]['config']['batch_input_shape'] = (None, input_size, input_size, 1)\nmodel = Model.from_config(cfg)\n\n# Load in the weights from training\nweights_dir = '..\/input\/fitting-deeper-networks-vgg19\/VGG19_weights.h5'\nmodel.load_weights(weights_dir)\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics= [])","c2aabc3f":"# Registering GuidedRelu as a tensorflow gradient\ntry:\n    @tf.RegisterGradient('GuidedRelu')\n    def _guided_backprop(op, grad):\n        dtype = op.outputs[0].dtype\n        gate_g = tf.cast(grad > 0., dtype)\n        gate_y = tf.cast(op.outputs[0] > 0, dtype)\n        return gate_y * gate_g * grad\nexcept KeyError: #KeyError is raised if 'GuidedRelu' has already been registered as a gradient\n    pass","e9e09129":"from tensorflow.python.keras.activations import linear\n\ncfg = model.get_config()\ng = tf.get_default_graph()\n# Compiling the model within this loop implements Guided Backprop\nwith g.gradient_override_map({'Relu': 'GuidedRelu'}):\n    # Copying model using it's config\n    guid_model = Model.from_config(cfg)\n    # Replacing the activation on the last layer with linear\n    guid_model.layers[-1].activation = linear\n    guid_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])","962e7732":"def lcm(a,b):\n    '''\n    Returns the lowest common multiple of inputs a & b.\n    '''\n    from math import gcd\n    return (a*b)\/\/gcd(a,b)","db830050":"def unaug_img(img):\n    '''\n    Returns image array with pixel intensities confined to [0,1].\n    '''\n    img -= img.min()\n    img \/= img.max()\n    return img","1d249aa1":"from tensorflow.python.keras import backend as K\ndef generate_gradcam(img, class_pred, model, last_conv_layer): \n    '''\n    Generates Grad-CAM of img with respect to class_pred, given the Keras model and it's last_conv_layer.\n    '''\n    # Creating the mapping as per the paper\n    with tf.Session() as sess:\n        sess.run(tf.global_variables_initializer())\n        class_output = model.output[:, class_pred]\n        grads = K.gradients(class_output, last_conv_layer.output)[0]\n        weightings = K.mean(grads, axis=(0,1,2))\n        # Keras function taking image input and returning the class feature weights and feature map \n        iterate = K.function([model.input], [weightings, last_conv_layer.output[0]])\n        pooled_grads_value, conv_layer_output_value = iterate([np.reshape(img,(1, input_size, input_size, 1))])\n    # Weighted addition of the feature maps\n    for i in range(conv_layer_output_value.shape[-1]):\n            conv_layer_output_value[:, :, i] *= pooled_grads_value[i]\n    # Global Average Pooling\n    heatmap = np.mean(conv_layer_output_value, axis=-1) \n    # Relu\n    heatmap = np.maximum(heatmap,0) \n    # Normalize for visualisation\n    heatmap \/= np.max(heatmap) \n    map_size = lcm(img.shape[0],heatmap.shape[0]) # Assumes square images\n    # Resize image and convert to RGB\n    img = cv2.resize(img, (map_size,map_size))\n    img = np.uint8(255*img) \n    img = cv2.cvtColor(img,cv2.COLOR_GRAY2RGB)\n    # Resize heatmap and apply colourmap\n    heatmap = cv2.resize(heatmap, (map_size,map_size))\n    heatmap = np.uint8(255*heatmap)\n    heatmap_jet = cv2.applyColorMap(255-heatmap, cv2.COLORMAP_JET)\n    # Remove pixels for which the class activation is low\n    heatmap_jet[np.where(heatmap < 75)] = 0\n    # Superimpose Grad-CAM with image\n    superimposed_img = cv2.addWeighted(img, 0.7, heatmap_jet, 0.3, 0)\n    return superimposed_img","0de005b0":"y_true = np.empty([num_samples, 2])\ny_pred = np.empty([num_samples, 2])\nX = np.empty([num_samples, input_size, input_size, 1])\n# Get next image from generator and get prediction\nfor i in range(num_samples):\n    prog = ' Progress: '+str(i+1)+'\/'+str(num_samples)\n    X[i,...], y_true[i,:] = next(test_gen)\n    y_pred[i,:] = model.predict(np.expand_dims(X[i,...],axis=0), steps=1)\n    sys.stdout.write('\\r'+prog)\nsys.stdout.write('\\rDone                ')","6b3b5658":"# The last convolutional layer in VGG19 is called 'block5_conv4 in Keras'\nlast_conv_layer = guid_model.get_layer('block5_conv4')\nmax_imgs = np.empty([num_classes, input_size, input_size, 1])\n\nfig, ax = plt.subplots(2, num_classes, figsize=(15,15))\nfor i in range(num_classes):\n    #Get most confident prediction\n    idx = np.argmax(y_pred[:,i])\n    max_imgs[i] = X[idx,...]\n    # Display image\n    ax[0,i].imshow(unaug_img(max_imgs[i,...,0]), cmap='gray')\n    ax[0,i].set_title(classes[i]+' max. confidence: \\n'+str(y_pred[idx,i]), fontsize=15)\n    # Generate Grad-CAM and display it\n    gradcam = generate_gradcam(max_imgs[i], i, guid_model, last_conv_layer)\n    ax[1,i].imshow(gradcam)\n    ax[1,i].set_title(classes[i]+' GradCAM', fontsize=15)","cf96a9b7":"from tensorflow.python.keras import backend as K\ndef generate_saliency(img, class_pred, model):\n    '''\n    Generates the Saliency map of img with respect to class_pred, given the Keras model.\n    '''\n    inp = model.input\n    class_outp = model.output[:, class_pred]\n    sal = K.gradients(tf.reduce_sum(class_outp), inp)[0]\n    # Keras function returning the saliency map given an image input\n    sal_fn = K.function([inp], [sal])\n    # Generating the saliency map and normalizing it\n    img_sal = sal_fn([np.resize(img, (1, input_size, input_size, 1))])[0]\n    img_sal = np.abs(img_sal)\n    img_sal \/= img_sal.max()\n    return img_sal","ff8728ec":"fig, ax = plt.subplots(1, num_classes, figsize=(20,10))\nfor i in range(num_classes):\n    #Generating and showing the Saliency Map for most confident predictions\n    with tf.Session() as sess:\n        sess.run(tf.global_variables_initializer())\n        saliency = generate_saliency(max_imgs[i], i, guid_model)\n    ax[i].imshow(saliency[0,...,0], cmap='jet')\n    ax[i].set_title(classes[i]+' Saliency', fontsize=15)","204bfc33":"The Grad-CAM appears to show the model is indeed \"looking\" at the Neurons! To get a finer attention map, the [Saliency Map](https:\/\/arxiv.org\/abs\/1312.6034) is created. Using Guided BackPropagation and the linear activations in the final layer as before improves the output of the map.","321c5282":"The first visualisation technique is [Grad-CAM](https:\/\/arxiv.org\/abs\/1610.02391). The hope is that this class mapping will highlight neurons - an indication that the CNN is \"looking\" here as opposed to at some other feature.\n\nTo implement this, the model is duplicated with modified gradients and linear activations on the final layer. The modified gradient gives an implementation of [Guided BackPropagation](https:\/\/arxiv.org\/abs\/1412.6806) and only backpropagates gradients at nodes which had positive activation during the forward pass. The linear activation in the final Dense layer ensures that when computing derivatives of a class output that features which minimise the other classes aren't included; as would happen if the final activation were left as softmax.","db6e6d41":"To verify that the model has learnt a feature of the data as opposed to an artifact, some visualisation methods are implemented.\n\nFirst,  the test generator is defined and the model is loaded with the trained weights.","619a39a8":"Predictions for each image in the test set are generated and the Grad-CAM for the most confident prediction of each class is shown."}}