{"cell_type":{"eae6df26":"code","4ae2d858":"code","e1bbcf12":"code","c66eaee4":"code","4aa9a590":"code","e9f79e06":"code","2720c09b":"code","879a0145":"code","c6cba901":"code","127d6749":"code","e1831c48":"markdown","f7a65105":"markdown","da2ebce6":"markdown","ecb64450":"markdown"},"source":{"eae6df26":"import pandas as pd\nimport numpy as np\nimport random\n\nimport torch\nimport torch.optim as optim\nimport torchvision.datasets as data\nimport torchvision.transforms as transforms\n\nfrom torch.utils.data import TensorDataset, DataLoader\nfrom sklearn.preprocessing import MinMaxScaler","4ae2d858":"device = torch.device('cuda')\ntorch.manual_seed(777)\nrandom.seed(777)\ntorch.cuda.manual_seed_all(777)\n\nlearning_rate = 0.1\ntraining_epochs = 8000\nbatch_size = 200\n#drop_prob = 0.3","e1bbcf12":"xy_train = pd.read_csv('train_seoul_grandpark.csv', header = None, skiprows=1, usecols=range(1, 8))\n\nx_data = xy_train.loc[: , 1:6]\ny_data = xy_train.loc[: , [7]]\n\nx_data = np.array(x_data)\ny_data = np.array(y_data)\n\nscaler = MinMaxScaler()\nx_data = scaler.fit_transform(x_data)\n\nx_train = torch.FloatTensor(x_data).to(device)\ny_train = torch.FloatTensor(y_data).to(device) ","c66eaee4":"train_dataset = TensorDataset(x_train, y_train)\ndata_loader = torch.utils.data.DataLoader(dataset = train_dataset,\n                                           batch_size = batch_size, \n                                           shuffle = True, \n                                           drop_last = True)","4aa9a590":"linear1 = torch.nn.Linear(6, 4,bias=True)\nlinear2 = torch.nn.Linear(4, 4,bias=True)\nlinear3 = torch.nn.Linear(4, 1,bias=True)\nrelu = torch.nn.ReLU()","e9f79e06":"torch.nn.init.kaiming_normal_(linear1.weight)\ntorch.nn.init.kaiming_normal_(linear2.weight)\ntorch.nn.init.kaiming_normal_(linear3.weight)\n\nmodel = torch.nn.Sequential(linear1,relu,\n                            linear2,relu,\n                            linear3\n                            ).to(device)","2720c09b":"loss = torch.nn.MSELoss().to(device)\noptimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n\nlosses = []\nmodel_history = []\nerr_history = []\n\ntotal_batch = len(data_loader)\n\nfor epoch in range(training_epochs + 1):\n  avg_cost = 0\n  #model.train()\n  \n  for X, Y in data_loader:\n    X = X.to(device)\n    Y = Y.to(device)\n\n    optimizer.zero_grad()\n    hypothesis = model(X)\n    cost = loss(hypothesis, Y)\n    cost.backward()\n    optimizer.step()\n\n    avg_cost += cost \/ total_batch\n    \n  model_history.append(model)\n  err_history.append(avg_cost)\n  \n  if epoch % 100 == 0:  \n    print('Epoch:', '%d' % (epoch + 1), 'Cost =', '{:.9f}'.format(avg_cost))\n  losses.append(cost.item())\nprint('Learning finished')","879a0145":"best_model = model_history[np.argmin(err_history)]","c6cba901":"xy_test = pd.read_csv('test_seoul_grandpark.csv', header = None, skiprows=1, usecols = range(1, 7))\nx_data = xy_test.loc[:, 1:6]\nx_data = np.array(x_data)\nx_data = scaler.transform(x_data)\nx_test = torch.FloatTensor(x_data).to(device)\n\nwith torch.no_grad():\n    #model.eval()     \n    predict = best_model(x_test)","127d6749":"submit = pd.read_csv('submit_sample.csv')\nsubmit['Expected'] = submit['Expected'].astype(float)\nfor i in range(len(predict)):\n  submit['Expected'][i] = predict[i]\nsubmit.to_csv('submit.csv', mode = 'w', index = False, header = True)\nsubmit","e1831c48":"# \ucc28\uc774\uc810 (2)\nkaiming_normal_ \ucd08\uae30\ud654 \uc0ac\uc6a9","f7a65105":"# \uacf5\uaca9 \uc131\uacf5 \ucf54\ub4dc\uc640\uc758 \ucc28\uc774\uc810\n- NN \uc7ac\uc124\uacc4 (5-layer -> 3 layer)\n- kaiming he \ucd08\uae30\ud654 \uc774\uc6a9 (xavier_normal_ -> kaiming_normal_)","da2ebce6":"# \ucc28\uc774\uc810 (1)\nNN \uc7ac\uc124\uacc4 : 3\uacc4\uce35 \ub525\ub7ec\ub2dd \ubaa8\ub378 \uc0ac\uc6a9","ecb64450":"# **1\ucc28 \ubc29\uc5b4 \ucf54\ub4dc**"}}