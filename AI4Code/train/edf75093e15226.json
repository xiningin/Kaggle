{"cell_type":{"42a1a519":"code","2c854e72":"code","be3c3994":"code","e74e50bb":"code","6e7b174c":"code","fc38a8fb":"code","673a7496":"code","2d473b64":"code","5edd51c5":"code","13d71b2b":"code","2176c4bc":"code","c20926ad":"code","272ecd0d":"code","cffdec89":"code","ab4a172c":"code","bc7ceee6":"code","27423996":"code","91946d33":"code","ee976988":"code","865a901c":"code","159c0a13":"code","0df3aadd":"code","1603a17b":"code","0e7bb1ff":"code","57015024":"code","9aac9f63":"code","7b244828":"code","a2bb4137":"markdown","a102a27d":"markdown","f5cc467e":"markdown","d8c8d0b8":"markdown","e3e03b4a":"markdown","9ade02a8":"markdown","d19abfaa":"markdown","8fdd47b5":"markdown","0b463d83":"markdown","644f1ad0":"markdown","d7d622f7":"markdown","840d574b":"markdown","329b1752":"markdown"},"source":{"42a1a519":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","2c854e72":"import os\nimport csv\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns","be3c3994":"from IPython.display import Markdown, display\ndef printmd(string):\n    display(Markdown(string))\n#printmd('**bold**')","e74e50bb":"data_path = \"..\/input\/train.csv\"","6e7b174c":"data_raw = pd.read_csv(data_path)\n#data_raw = data_raw.loc[np.random.choice(data_raw.index, size=2000)]\ndata_raw.shape","fc38a8fb":"\nprint(\"Number of rows in data =\",data_raw.shape[0])\nprint(\"Number of columns in data =\",data_raw.shape[1])\nprint(\"\\n\")\nprintmd(\"**Sample data:**\")\ndata_raw.head()","673a7496":"missing_values_check = data_raw.isnull().sum()\nprint(missing_values_check)","2d473b64":"# Comments with no label are considered to be clean comments.\n# Creating seperate column in dataframe to identify clean comments.\n\n# We use axis=1 to count row-wise and axis=0 to count column wise\n\nrowSums = data_raw.iloc[:,2:].sum(axis=1)\nclean_comments_count = (rowSums==0).sum(axis=0)\n\nprint(\"Total number of comments = \",len(data_raw))\nprint(\"Number of clean comments = \",clean_comments_count)\nprint(\"Number of comments with labels =\",(len(data_raw)-clean_comments_count))","5edd51c5":"categories = list(data_raw.columns.values)\ncategories = categories[2:]\nprint(categories)\n","13d71b2b":"# Calculating number of comments in each category\n\ncounts = []\nfor category in categories:\n    counts.append((category, data_raw[category].sum()))\ndf_stats = pd.DataFrame(counts, columns=['category', 'number of comments'])\ndf_stats","2176c4bc":"sns.set(font_scale = 2)\nplt.figure(figsize=(15,8))\n\nax= sns.barplot(categories, data_raw.iloc[:,2:].sum().values)\n\nplt.title(\"Comments in each category\", fontsize=24)\nplt.ylabel('Number of comments', fontsize=18)\nplt.xlabel('Comment Type ', fontsize=18)\n\n#adding the text labels\nrects = ax.patches\nlabels = data_raw.iloc[:,2:].sum().values\nfor rect, label in zip(rects, labels):\n    height = rect.get_height()\n    ax.text(rect.get_x() + rect.get_width()\/2, height + 5, label, ha='center', va='bottom', fontsize=18)\n\nplt.show()","c20926ad":"rowSums = data_raw.iloc[:,2:].sum(axis=1)\nmultiLabel_counts = rowSums.value_counts()\nmultiLabel_counts = multiLabel_counts.iloc[1:]\n\nsns.set(font_scale = 2)\nplt.figure(figsize=(15,8))\n\nax = sns.barplot(multiLabel_counts.index, multiLabel_counts.values)\n\nplt.title(\"Comments having multiple labels \")\nplt.ylabel('Number of comments', fontsize=18)\nplt.xlabel('Number of labels', fontsize=18)\n\n#adding the text labels\nrects = ax.patches\nlabels = multiLabel_counts.values\nfor rect, label in zip(rects, labels):\n    height = rect.get_height()\n    ax.text(rect.get_x() + rect.get_width()\/2, height + 5, label, ha='center', va='bottom')\n\nplt.show()","272ecd0d":"data = data_raw\ndata = data_raw.loc[np.random.choice(data_raw.index, size=2000)]\ndata.shape","cffdec89":"import nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem.snowball import SnowballStemmer\nimport re\n\nimport sys\nimport warnings\n\nif not sys.warnoptions:\n    warnings.simplefilter(\"ignore\")","ab4a172c":"def cleanHtml(sentence):\n    cleanr = re.compile('<.*?>')\n    cleantext = re.sub(cleanr, ' ', str(sentence))\n    return cleantext\n\n\ndef cleanPunc(sentence): #function to clean the word of any punctuation or special characters\n    cleaned = re.sub(r'[?|!|\\'|\"|#]',r'',sentence)\n    cleaned = re.sub(r'[.|,|)|(|\\|\/]',r' ',cleaned)\n    cleaned = cleaned.strip()\n    cleaned = cleaned.replace(\"\\n\",\" \")\n    return cleaned\n\n\ndef keepAlpha(sentence):\n    alpha_sent = \"\"\n    for word in sentence.split():\n        alpha_word = re.sub('[^a-z A-Z]+', ' ', word)\n        alpha_sent += alpha_word\n        alpha_sent += \" \"\n    alpha_sent = alpha_sent.strip()\n    return alpha_sent","bc7ceee6":"data['comment_text'] = data['comment_text'].str.lower()\ndata['comment_text'] = data['comment_text'].apply(cleanHtml)\ndata['comment_text'] = data['comment_text'].apply(cleanPunc)\ndata['comment_text'] = data['comment_text'].apply(keepAlpha)\ndata.head()","27423996":"stop_words = set(stopwords.words('english'))\nstop_words.update(['zero','one','two','three','four','five','six','seven','eight','nine','ten','may','also','across','among','beside','however','yet','within'])\nre_stop_words = re.compile(r\"\\b(\" + \"|\".join(stop_words) + \")\\\\W\", re.I)\ndef removeStopWords(sentence):\n    global re_stop_words\n    return re_stop_words.sub(\" \", sentence)\n\ndata['comment_text'] = data['comment_text'].apply(removeStopWords)\ndata.head()","91946d33":"stemmer = SnowballStemmer(\"english\")\ndef stemming(sentence):\n    stemSentence = \"\"\n    for word in sentence.split():\n        stem = stemmer.stem(word)\n        stemSentence += stem\n        stemSentence += \" \"\n    stemSentence = stemSentence.strip()\n    return stemSentence\n\ndata['comment_text'] = data['comment_text'].apply(stemming)\ndata.head()","ee976988":"from sklearn.model_selection import train_test_split\n\ntrain, test = train_test_split(data, random_state=42, test_size=0.30, shuffle=True)\n\nprint(train.shape)\nprint(test.shape)","865a901c":"train_text = train['comment_text']\ntest_text = test['comment_text']","159c0a13":"from sklearn.feature_extraction.text import TfidfVectorizer\nvectorizer = TfidfVectorizer(strip_accents='unicode', analyzer='word', ngram_range=(1,3), norm='l2')\nvectorizer.fit(train_text)\nvectorizer.fit(test_text)","0df3aadd":"x_train = vectorizer.transform(train_text)\ny_train = train.drop(labels = ['id','comment_text'], axis=1)\n\nx_test = vectorizer.transform(test_text)\ny_test = test.drop(labels = ['id','comment_text'], axis=1)","1603a17b":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.multiclass import OneVsRestClassifier","0e7bb1ff":"%%time\n\n# Using pipeline for applying logistic regression and one vs rest classifier\nLogReg_pipeline = Pipeline([\n                ('clf', OneVsRestClassifier(LogisticRegression(solver='sag'), n_jobs=-1)),\n            ])\n\nfor category in categories:\n    printmd('**Processing {} comments...**'.format(category))\n    \n    # Training logistic regression model on train data\n    LogReg_pipeline.fit(x_train, train[category])\n    \n    # calculating test accuracy\n    prediction = LogReg_pipeline.predict(x_test)\n    print('Test accuracy is {}'.format(accuracy_score(test[category], prediction)))\n    print(\"\\n\")","57015024":"%%time\n\n# using binary relevance\nfrom skmultilearn.problem_transform import BinaryRelevance\nfrom sklearn.naive_bayes import GaussianNB\n\n# initialize binary relevance multi-label classifier\n# with a gaussian naive bayes base classifier\nclassifier = BinaryRelevance(GaussianNB())\n\n# train\nclassifier.fit(x_train, y_train)\n\n# predict\npredictions = classifier.predict(x_test)\n\n# accuracy\nprint(\"Accuracy = \",accuracy_score(y_test,predictions))\nprint(\"\\n\")","9aac9f63":"# using classifier chains\nfrom skmultilearn.problem_transform import ClassifierChain\nfrom sklearn.linear_model import LogisticRegression","7b244828":"%%time\n\n# initialize classifier chains multi-label classifier\nclassifier = ClassifierChain(LogisticRegression())\n\n# Training logistic regression model on train data\nclassifier.fit(x_train, y_train)\n\n# predict\npredictions = classifier.predict(x_test)\n\n# accuracy\nprint(\"Accuracy = \",accuracy_score(y_test,predictions))\nprint(\"\\n\")","a2bb4137":"# 2. Data Pre-Processing","a102a27d":"## Checking for missing values","f5cc467e":"## Train-Test Split","d8c8d0b8":" # 3. Multi-Label Classification\n##  Multiple Binary Classifications - (One Vs Rest Classifier)","e3e03b4a":"##  Removing Stop Words","9ade02a8":"## Calculating number of comments having multiple labels","d19abfaa":"## Calculating number of comments under each label","8fdd47b5":"## Multiple Binary Classifications - (Binary Relevance)","0b463d83":"## Stemming","644f1ad0":"## Classifier Chains","d7d622f7":"##  Cleaning Data","840d574b":"## TF-IDF","329b1752":"# 1. EDA"}}