{"cell_type":{"9b081f2f":"code","87d3e504":"code","2e11d822":"code","73545a54":"code","c3541394":"code","c2143d7b":"code","0ba700f1":"code","2669adef":"code","e3da645b":"code","051fefed":"code","d1f92128":"code","6816f791":"code","93c41d33":"code","89ae3b76":"code","f22a62a9":"code","3af69398":"code","0f00963f":"code","40896e61":"code","88ac68f4":"code","dc738f34":"code","211c9dd0":"code","1287b0f1":"code","f2648f0e":"code","9a409ab2":"code","c17c0b36":"code","1867ebee":"code","fa96f434":"code","15045c35":"code","9acb7c39":"code","0437d34b":"code","b4f8f334":"code","aa9f3391":"code","a931fb9b":"code","1893df12":"code","b4201149":"code","e86eef9d":"code","95d4ae4d":"code","abea5635":"code","b685c85f":"code","aab76d74":"code","69a0b147":"code","af349922":"code","7620d26b":"code","121eaeb3":"code","24a91e53":"code","98fe3617":"code","a274b4e6":"code","4ccc9357":"code","17c861ec":"code","d309a8fc":"code","04729d59":"code","7f7089d2":"code","4038efed":"code","a141affb":"code","12b7f975":"code","6c37a6e8":"code","b299d442":"code","8dbd032e":"code","48328165":"code","875714d9":"code","45d5d767":"code","a70974b6":"code","aaec952a":"code","b16a3c5c":"code","87a2770f":"code","b806d06e":"code","6479b32b":"code","a04c1df8":"code","5bfb5313":"code","15a00dac":"code","293192c2":"code","e103aa54":"code","3449acf8":"code","fc5e294a":"code","f217909b":"code","6567483f":"code","839723fd":"code","9e657e23":"code","854eb4de":"code","5f3dd9c5":"code","c9efc1a4":"code","331f60e4":"code","85092d13":"code","74ea217c":"code","d9e4cd0e":"code","8438b526":"code","fa8d5508":"code","ea9e5231":"code","37360356":"code","f66c8f32":"code","7184d050":"code","ba262388":"code","8db0fe5a":"code","f1aa6210":"code","6bab1e1e":"code","cd11cf32":"code","8e2df098":"code","a7848d58":"code","e26ebd7a":"code","4a4c74e2":"code","949f63f7":"code","a95fef97":"code","b496eba2":"code","eec37b7b":"code","bdb56199":"code","3e305ce5":"code","f7b9e3c6":"code","3332aeaa":"code","8e6ef0d7":"code","7b4d0f00":"code","70d1e39e":"code","175c88d7":"code","ee108101":"code","e504add1":"code","14633d5f":"code","7cfd9afa":"code","ce492a0b":"code","5bce73d2":"code","f4e59608":"code","7a824bc9":"code","839cbf9d":"code","7336a5ad":"code","fde3e02c":"code","5f091c0d":"code","c6aaa3e8":"code","bf3e6954":"code","09a8234d":"code","260880b8":"code","0d915b11":"code","4165a3a9":"code","ee021e2e":"code","3a5645fa":"code","dd33b55e":"code","3af34ea6":"code","e168f13f":"code","09c5da18":"code","5be1189e":"code","5cf9c174":"code","56cc452f":"code","8dec19f0":"code","2b224e3a":"code","8fdd9b03":"code","bc01bbd9":"code","a530fe2a":"code","8c8d5e14":"code","650c81cb":"code","943a31e8":"code","d540105f":"code","44e887d1":"code","0f7f6c58":"code","dbeac8db":"code","7a68b542":"code","70b3501d":"code","e1267b46":"code","40f0e817":"code","2e91e693":"code","56a8c50a":"code","4f344c32":"code","240d07d3":"code","56ed92fe":"code","6815561d":"code","df887ea7":"code","b3c3d898":"code","28388937":"code","72c89bc7":"code","4b0c100e":"code","9f6701ba":"code","94129809":"code","73d969c1":"code","a73b7a37":"code","e96660c1":"code","3e0e5e78":"code","b41a43f5":"code","2049f444":"code","38d669e5":"code","0c78fac8":"code","ac777ddc":"code","b1ff9522":"code","d1128252":"code","f5663a77":"code","438080a0":"code","63191a30":"code","03f6fb4a":"markdown","17ee954d":"markdown","578bd4f5":"markdown","79b185dd":"markdown","76e44593":"markdown","cfb84d08":"markdown","8a7842cf":"markdown","267687de":"markdown","c250d74d":"markdown","9745a565":"markdown","5c988c65":"markdown","ea7bff30":"markdown","db536fb2":"markdown","50667d03":"markdown","c0df2e8b":"markdown","5c1849d4":"markdown","9aaa4485":"markdown","f5cff2e9":"markdown","58b9d0c6":"markdown","1d4449a5":"markdown","9f576425":"markdown","3e86e8b6":"markdown","c29863b7":"markdown","a2c7bbbe":"markdown","56ecbf8b":"markdown","97013412":"markdown","59fc1a8c":"markdown","1dccfc44":"markdown","5f0df09f":"markdown","7ba2d7dc":"markdown","78d55c3d":"markdown","c7efd212":"markdown","274fa755":"markdown","5920b2bc":"markdown","90ca005c":"markdown","000e1931":"markdown","8d92f932":"markdown","eee82a8f":"markdown","3347ae37":"markdown","43606953":"markdown","a830ab0a":"markdown","7299c4c7":"markdown","b3753cde":"markdown","4373ae21":"markdown","4ece3a8d":"markdown","30c57279":"markdown","b9636e47":"markdown","c6bbad51":"markdown","c8c747f8":"markdown","cd88e319":"markdown","b8b3f873":"markdown","318d6d63":"markdown","ead3922d":"markdown","a368f8e2":"markdown","78d9ba3a":"markdown","571bd5d5":"markdown","76553f9d":"markdown","c8d80d8e":"markdown","239c0bbb":"markdown","14dfc75c":"markdown","a3fd1bae":"markdown","c42afe4b":"markdown","d183fb50":"markdown","77ef6455":"markdown","70045908":"markdown","02ab9668":"markdown","413264c2":"markdown","14a8a309":"markdown","f95ebc06":"markdown","18e97803":"markdown","4d1b3d6c":"markdown","7f04ef47":"markdown","ec47ccbe":"markdown","2e586061":"markdown","2a66866a":"markdown","03434ba3":"markdown","7044d79f":"markdown","9f494d35":"markdown","d653d3ef":"markdown","242336be":"markdown","1e01a87a":"markdown","657f0715":"markdown","0257c547":"markdown","e2f8a088":"markdown","884fd39f":"markdown","24b4fc67":"markdown","11059a0e":"markdown","1e5db2ab":"markdown","9bfa5425":"markdown","b9f12641":"markdown","d82ea0e5":"markdown","048d6d7b":"markdown","944ed139":"markdown","453a0527":"markdown","bf1e1b8a":"markdown","c1c0e915":"markdown","50eba9f8":"markdown","8dbf9df5":"markdown","ceee3e0b":"markdown","d6655bf7":"markdown","ae2ce3bb":"markdown","3916dcc0":"markdown","819143b6":"markdown","3c56829e":"markdown","62da1d5b":"markdown","a2b0db73":"markdown","82db64db":"markdown","49373bf6":"markdown","d7ff8880":"markdown","a7270b7a":"markdown","ec6c63a7":"markdown","aab0e7ed":"markdown","e22c9956":"markdown","208398e4":"markdown","d0a403d7":"markdown","679bfdd2":"markdown","c99e26cf":"markdown","c66d94cf":"markdown","c9bcfffc":"markdown","34ab8914":"markdown","c1380705":"markdown","2aa2891a":"markdown","957f169f":"markdown","ff42cdbf":"markdown","aba53e50":"markdown","1205bc54":"markdown","ad41c705":"markdown","630e9c82":"markdown","4e218ca0":"markdown","a0b3b32d":"markdown","133bccf4":"markdown","9864491d":"markdown","2b1550c6":"markdown","0169d74f":"markdown","1adf0cb5":"markdown","cbab6020":"markdown","242e3934":"markdown","9c1a25fd":"markdown","11996bcb":"markdown","651228d5":"markdown","e83b99ef":"markdown","9649f6ad":"markdown","1b081ec1":"markdown"},"source":{"9b081f2f":"import warnings\nwarnings.filterwarnings('ignore')","87d3e504":"#Data Analysis & Data wrangling\nimport numpy as np\nimport pandas as pd\nfrom collections import Counter\nfrom math import sqrt\n\n#Visualization\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport matplotlib.style as style\nimport matplotlib.gridspec as gridspec\nimport seaborn as sns\n%matplotlib inline\n\n# Plot Style\nsns.set_context(\"paper\")\nstyle.use('fivethirtyeight')\n\n# Stats\nfrom scipy import stats\nfrom scipy.stats import skew, norm\nfrom scipy.special import boxcox1p\nfrom scipy.stats import boxcox_normmax\n\n# Machine Learning Libraries\nimport statsmodels.api as sm\n\n#Sci-kit learn libraries\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split,GridSearchCV,KFold,cross_val_score\nfrom sklearn import metrics\nfrom sklearn.feature_selection import RFE\nfrom sklearn.metrics import recall_score, accuracy_score, confusion_matrix, f1_score,classification_report\nfrom sklearn.metrics import precision_score, auc, roc_auc_score, roc_curve, precision_recall_curve,plot_roc_curve\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.decomposition import PCA, IncrementalPCA\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\n\n#Data Imbalance Treatment Libraries\nfrom imblearn.over_sampling import SMOTE, RandomOverSampler\nfrom imblearn.under_sampling import NearMiss, RandomUnderSampler\nfrom imblearn.combine import SMOTEENN, SMOTETomek","2e11d822":"# To display all the columns\npd.options.display.max_columns = None\n# To display all the rows\npd.options.display.max_rows = None\n# change the cell width\nfrom IPython.core.display import display, HTML\ndisplay(HTML(\"<style>.container { width:100% !important; }<\/style>\"))","73545a54":"# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","c3541394":"#importing the dataset\ntelecom = pd.read_csv('\/kaggle\/input\/indian-south-east-asian-market-telecom-churn\/telecom_churn_data.csv')\n\n#checking the top 5 rows\ntelecom.head()","c2143d7b":"#checking the bottom 5 rows\ntelecom.tail()","0ba700f1":"#database dimension\nprint(\"Database dimension     :\",telecom.shape)\nprint(\"Database size          :\",telecom.size)\nprint(\"Number of Row          :\",len(telecom.index))\nprint(\"Number of Columns      :\",len(telecom.columns))","2669adef":"#checking numerical columns statistics\ntelecom.describe()","e3da645b":"#info about the column types etc. \ntelecom.info(verbose = True)","051fefed":"print(\"Total number of duplicate values of mobile numbers :\" , telecom.duplicated(subset = 'mobile_number').sum())","d1f92128":"# Creating a copy of original dataframe for duplicate check\ntelecom_dup = telecom\n\n# Checking for duplicates and dropping the entire duplicate row if any\ntelecom_dup.drop_duplicates(subset=None, inplace=True)\nprint (\"Original Dataframe Shape                      : \", telecom.shape)\nprint (\"Shape of Dataframe after removing duplicates  : \", telecom_dup.shape)","6816f791":"# Plotting the dataframe to check null values and its distribution\nplt.figure(figsize = (18,8))\nsns.heatmap(telecom.isnull(),cbar = False)\nplt.show()","93c41d33":"# Function to check column wise null values and % of null values\ndef null_calc(df):\n    #Column wise null values in train data set \n    null_perc = pd.DataFrame(round((df.isnull().sum())*100\/df.shape[0],2)).reset_index()\n    null_perc.columns = ['Column Name', 'Null Values Percentage']\n    null_value = pd.DataFrame(df.isnull().sum()).reset_index()\n    null_value.columns = ['Column Name', 'Null Values']\n    null_df = pd.merge(null_value, null_perc, on='Column Name')\n    null_df['Null Values Percentage'] = pd.to_numeric(null_df['Null Values Percentage'], errors='coerce')\n    null_df = null_df.loc[null_df['Null Values Percentage'] > 0.00] # Keeping only null columns info\n    null_df.sort_values(\"Null Values\", ascending = False, inplace=True, ignore_index=True)\n    return null_df","89ae3b76":"# Checking the null values of the telecom dataframe\nnull_telecom = null_calc(telecom)\nnull_telecom","f22a62a9":"# How many columns have null values ? \nprint(\"Total number of columns which have null values are :\", null_telecom.shape[0])\n\n# How many columns have more than 70% null values?\nprint(\"Total number of columns which have null values more than 70% :\", len(null_telecom[null_telecom['Null Values Percentage']>70]))","3af69398":"# Checking if data recharge null row indexes matches with same customers data of null values in last recharge data information for month of June\nif telecom['total_rech_data_6'].isnull().equals(telecom['date_of_last_rech_data_6'].isnull()):\n    print('The indexes for NULL values for month 6 are equal')","0f00963f":"# Imputing the data for total_rech_data_6 and av_rech_amt_data_6 as 0\ntelecom['total_rech_data_6'].fillna(0, inplace=True)\ntelecom['av_rech_amt_data_6'].fillna(0, inplace=True)","40896e61":"# Checking if data recharge null row indexes matches with same customers data of null values in last recharge data information for month of July(7)\nif telecom['total_rech_data_7'].isnull().equals(telecom['date_of_last_rech_data_7'].isnull()):\n    print('The indexes for NULL values for month 7 are equal')\n\n# Checking if data recharge null row indexes matches with same customers data of null values in last recharge data information for month of August(8)\nif telecom['total_rech_data_8'].isnull().equals(telecom['date_of_last_rech_data_8'].isnull()):\n    print('The indexes for NULL values for month 8 are equal')    ","88ac68f4":"# Imputing the data for total_rech_data_7 and av_rech_amt_data_7 as 0\ntelecom['total_rech_data_7'].fillna(0, inplace=True)\ntelecom['av_rech_amt_data_7'].fillna(0, inplace=True)\n\n# Imputing the data for total_rech_data_8 and av_rech_amt_data_8 as 0\ntelecom['total_rech_data_8'].fillna(0, inplace=True)\ntelecom['av_rech_amt_data_8'].fillna(0, inplace=True)","dc738f34":"# Let's check the missing values again\nnull_telecom = null_calc(telecom)\nnull_telecom","211c9dd0":"# Checking the index matching for month 6\narpu_2g_6_index = telecom['arpu_2g_6'].isnull()\narpu_3g_6_index = telecom['arpu_3g_6'].isnull()\nnight_pck_user_6_index = telecom['night_pck_user_6'].isnull()\ncount_rech_2g_6_index = telecom['count_rech_2g_6'].isnull()\ncount_rech_3g_6_index = telecom['count_rech_3g_6'].isnull()\nfb_user_6_index = telecom['fb_user_6'].isnull()\n\n# If all the above objects\/arrays are equal for all entries, then we can confirm that the missing values are all from the\n# same observations\/index.\n\nif arpu_2g_6_index.equals(arpu_2g_6_index) & arpu_3g_6_index.equals(night_pck_user_6_index) & night_pck_user_6_index.equals(count_rech_2g_6_index) & count_rech_2g_6_index.equals(count_rech_3g_6_index) & count_rech_3g_6_index.equals(fb_user_6_index):\n    print('The indexes for NULL values for month 6 are equal')","1287b0f1":"# Checking the index matching for month 7\narpu_2g_7_index = telecom['arpu_2g_7'].isnull()\narpu_3g_7_index = telecom['arpu_3g_7'].isnull()\nnight_pck_user_7_index = telecom['night_pck_user_7'].isnull()\ncount_rech_2g_7_index = telecom['count_rech_2g_7'].isnull()\ncount_rech_3g_7_index = telecom['count_rech_3g_7'].isnull()\nfb_user_7_index = telecom['fb_user_7'].isnull()\n\n# If all the above objects\/arrays are equal for all entries, then we can confirm that the missing values are all from the\n# same observations\/index.\n\nif arpu_2g_7_index.equals(arpu_2g_7_index) & arpu_3g_7_index.equals(night_pck_user_7_index) & night_pck_user_7_index.equals(count_rech_2g_7_index) & count_rech_2g_7_index.equals(count_rech_3g_7_index) & count_rech_3g_7_index.equals(fb_user_7_index):\n    print('The indexes for NULL values for month 7 are equal')","f2648f0e":"# Checking the index matching for month 8\narpu_2g_8_index = telecom['arpu_2g_8'].isnull()\narpu_3g_8_index = telecom['arpu_3g_8'].isnull()\nnight_pck_user_8_index = telecom['night_pck_user_8'].isnull()\ncount_rech_2g_8_index = telecom['count_rech_2g_8'].isnull()\ncount_rech_3g_8_index = telecom['count_rech_3g_8'].isnull()\nfb_user_8_index = telecom['fb_user_8'].isnull()\n\n# If all the above objects\/arrays are equal for all entries, then we can confirm that the missing values are all from the\n# same observations\/index.\n\nif arpu_2g_8_index.equals(arpu_2g_8_index) & arpu_3g_8_index.equals(night_pck_user_8_index) & night_pck_user_8_index.equals(count_rech_2g_8_index) & count_rech_2g_8_index.equals(count_rech_3g_8_index) & count_rech_3g_8_index.equals(fb_user_8_index):\n    print('The indexes for NULL values for month 8 are equal')","9a409ab2":"cols_to_impute = ['fb_user_6','fb_user_7','fb_user_8','fb_user_9',\n                  'night_pck_user_6','night_pck_user_7','night_pck_user_8','night_pck_user_9',\n                  'arpu_3g_6','arpu_3g_7','arpu_3g_8','arpu_3g_9',\n                  'arpu_2g_6','arpu_2g_7','arpu_2g_8','arpu_2g_9',\n                  'count_rech_2g_6','count_rech_2g_7','count_rech_2g_8','count_rech_2g_9',\n                  'count_rech_3g_6','count_rech_3g_7','count_rech_3g_8','count_rech_3g_9']\nfor col in cols_to_impute:\n    telecom[col].fillna(0, inplace=True)","c17c0b36":"# Let's check the remaining null value columns\nnull_telecom = null_calc(telecom)\nnull_telecom","1867ebee":"# How many columns have null values ? \nprint(\"Total number of columns which have null values are :\", null_telecom.shape[0])\n\n# How many columns have more than 70% null values?\nprint(\"Total number of columns which have null values more than 70% :\", len(null_telecom[null_telecom['Null Values Percentage']>70]))","fa96f434":"# Rename month named vbc columns to format 6,7,8 and 9\n\ntelecom.rename(columns = {'jun_vbc_3g':'vbc_3g_6',\n                          'jul_vbc_3g':'vbc_3g_7',\n                          'aug_vbc_3g':'vbc_3g_8',\n                          'sep_vbc_3g':'vbc_3g_9'}, inplace=True)","15045c35":"telecom[['arpu_6', 'arpu_7', 'arpu_8']].describe()","9acb7c39":"# Total number of such observations for month 6,7,8\nprint('Total observations with negative arpu values for month 6 :' , len(telecom[telecom['arpu_6']<0]))\nprint('Total observations with negative arpu values for month 7 :' , len(telecom[telecom['arpu_7']<0]))\nprint('Total observations with negative arpu values for month 8 :' , len(telecom[telecom['arpu_8']<0]))","0437d34b":"# Let's delete the rows where arpu_X with negative values\n\ntelecom = telecom[(telecom['arpu_6'] >= 0) & \n                  (telecom['arpu_7'] >= 0) & \n                  (telecom['arpu_8'] >= 0)]\ntelecom.shape","b4f8f334":"telecom[['arpu_6','arpu_7','arpu_8']].describe()","aa9f3391":"# calculating total_data_rech_amt\ntelecom['total_data_rech_amt_6'] = telecom['total_rech_data_6'] * telecom['av_rech_amt_data_6']\ntelecom['total_data_rech_amt_7'] = telecom['total_rech_data_7'] * telecom['av_rech_amt_data_7']\ntelecom['total_data_rech_amt_8'] = telecom['total_rech_data_8'] * telecom['av_rech_amt_data_8']\ntelecom['total_data_rech_amt_9'] = telecom['total_rech_data_9'] * telecom['av_rech_amt_data_9']","a931fb9b":"# Lets compute the average recharge amount for the month 6 & 7. \n#This total amount is equal to the sum of talk time recharge and data recharge amounts for the respective months.\n\navg_recharge_amount_month_6_7 = telecom[['total_data_rech_amt_6','total_data_rech_amt_7','total_rech_amt_6',\n                                             'total_rech_amt_7']].mean(axis = 1)\n\namount_70th_percentile = np.percentile(avg_recharge_amount_month_6_7, 70)\n\nprint(\"70th percentile of the average recharge amount in the first two months is - \", amount_70th_percentile)","1893df12":"# Filtering the high value customers\n\ntele_highval_cust = telecom[avg_recharge_amount_month_6_7 >= amount_70th_percentile]\n\n# resetting the index\ntele_highval_cust = tele_highval_cust.reset_index(drop=True)\n\ntele_highval_cust.head()","b4201149":"# high value customer database dimension\nprint(\"Database dimension     :\",tele_highval_cust.shape)\nprint(\"Database size          :\",tele_highval_cust.size)\nprint(\"Number of Row          :\",len(tele_highval_cust.index))\nprint(\"Number of Columns      :\",len(tele_highval_cust.columns))","e86eef9d":"tele_highval_cust['churn'] = tele_highval_cust.apply(lambda x: 1 if((x.total_ic_mou_9 == 0) & \n                                                          (x.total_og_mou_9 == 0) and \n                                                          (x.vol_2g_mb_9 == 0) and \n                                                          (x.vol_3g_mb_9 == 0)) else 0, axis=1)","95d4ae4d":"# Let's check if our analysis matches with what we wanted to calculate for churn : \ntele_highval_cust[['total_ic_mou_9', 'total_og_mou_9', 'vol_2g_mb_9', 'vol_3g_mb_9','churn']].head()","abea5635":"# We will try to get all the columns in the 'tele_highval_cust' dataset where the column have '_9' in the name.\n\ncols_with_9 = [col for col in tele_highval_cust.columns if '_9' in col]\ncols_with_9","b685c85f":"# We will be dropping the 9 month data. Let's check how many column data we are about to drop \nprint(\"Number of columns to be dropped : \", len(cols_with_9))","aab76d74":"#dropping the columns with 9th month data\ntele_highval_cust.drop(cols_with_9,axis = 1, inplace = True)\nprint(\"Number of Columns remaining:\",len(tele_highval_cust.columns))","69a0b147":"# now let's check the remaining null values for rest of the columns \nnull_telecom = null_calc(tele_highval_cust)\nnull_telecom","af349922":"# How many columns have more than 40% null values?\nprint(\"Total number of columns which have null values more than 40% :\", len(null_telecom[null_telecom['Null Values Percentage']>40]))","7620d26b":"#dropping the date_of_last_rech_data_X columns\ntele_highval_cust.drop(['date_of_last_rech_data_6', 'date_of_last_rech_data_7', 'date_of_last_rech_data_8'],axis = 1, inplace = True)\nprint(\"Number of Columns remaining:\",len(tele_highval_cust.columns))","121eaeb3":"# checking the data distribution of these 3 columns : \ntele_highval_cust[['max_rech_data_6', 'max_rech_data_7' , 'max_rech_data_8']].describe()","24a91e53":"# How many rows are null ?\nprint (\"Total NULL values for max_rech_data_6 column : \",tele_highval_cust.max_rech_data_6.isnull().sum())\nprint (\"Total NULL values for max_rech_data_7 column : \",tele_highval_cust.max_rech_data_7.isnull().sum())\nprint (\"Total NULL values for max_rech_data_8 column : \",tele_highval_cust.max_rech_data_8.isnull().sum())","98fe3617":"for col in ['max_rech_data_6', 'max_rech_data_7', 'max_rech_data_8']:\n    tele_highval_cust[col].fillna(0, inplace=True)","a274b4e6":"# now let's check the remaining null values for rest of the columns \nnull_telecom = null_calc(tele_highval_cust)\nnull_telecom","4ccc9357":"# Delete all the date columns\ntele_highval_cust.drop(['date_of_last_rech_6','date_of_last_rech_7','date_of_last_rech_8',\n                        'last_date_of_month_6','last_date_of_month_7','last_date_of_month_8'], axis = 1, inplace = True)\ntele_highval_cust.shape","17c861ec":"# Let's store the null columns as a list first\ncols_null = tele_highval_cust.columns[tele_highval_cust.isna().any()].tolist()\nprint(cols_null)","d309a8fc":"# Let's impute these data with 0\n\nfor col in cols_null:\n    tele_highval_cust[col].fillna(0, inplace=True)   ","04729d59":"# Now let't check if there are any columns which have null values\nnull_telecom = null_calc(tele_highval_cust)\nnull_telecom","7f7089d2":"# Let's check what is our dataframe size currently\ntele_highval_cust.shape","4038efed":"#Variance \ncolumns_with_0_variance = tele_highval_cust.var() == 0\nprint(\"Total columns with ZERO variance are : \", columns_with_0_variance.sum())\n\ncolumn_name_with_0_variance = columns_with_0_variance[columns_with_0_variance == 1].index\nprint(column_name_with_0_variance)","a141affb":"# We will drop these columns where variance is 0\ntele_highval_cust.drop(column_name_with_0_variance,axis=1, inplace = True)\ntele_highval_cust.shape","12b7f975":"# Let's drop individual columns whose totals are available as a different attribute\n\nindividual_cols = ['loc_ic_t2t_mou_6', 'loc_ic_t2t_mou_7', 'loc_ic_t2t_mou_8',\n                   'loc_ic_t2m_mou_6', 'loc_ic_t2m_mou_7', 'loc_ic_t2m_mou_8',\n                   'loc_ic_t2f_mou_6', 'loc_ic_t2f_mou_7', 'loc_ic_t2f_mou_8',\n                   'std_ic_t2t_mou_6', 'std_ic_t2t_mou_7', 'std_ic_t2t_mou_8',\n                   'std_ic_t2m_mou_6', 'std_ic_t2m_mou_7', 'std_ic_t2m_mou_8',\n                   'std_ic_t2f_mou_6', 'std_ic_t2f_mou_7', 'std_ic_t2f_mou_8',\n                   'loc_og_t2t_mou_6', 'loc_og_t2t_mou_7', 'loc_og_t2t_mou_8',\n                   'loc_og_t2m_mou_6', 'loc_og_t2m_mou_7', 'loc_og_t2m_mou_8',\n                   'loc_og_t2f_mou_6', 'loc_og_t2f_mou_7', 'loc_og_t2f_mou_8',\n                   'loc_og_t2c_mou_6', 'loc_og_t2c_mou_7', 'loc_og_t2c_mou_8',\n                   'std_og_t2t_mou_6', 'std_og_t2t_mou_7', 'std_og_t2t_mou_8',\n                   'std_og_t2m_mou_6', 'std_og_t2m_mou_7', 'std_og_t2m_mou_8',\n                   'std_og_t2f_mou_6', 'std_og_t2f_mou_7', 'std_og_t2f_mou_8',\n                   'last_day_rch_amt_6', 'last_day_rch_amt_7', 'last_day_rch_amt_8',\n                   'arpu_3g_6', 'arpu_3g_7', 'arpu_3g_8',\n                   'arpu_2g_6', 'arpu_2g_7', 'arpu_2g_8',\n                   'av_rech_amt_data_6', 'av_rech_amt_data_7', 'av_rech_amt_data_8',\n                   'mobile_number']\n\ntele_highval_cust.drop(individual_cols, axis = 1, inplace = True)\n\ntele_highval_cust.shape","6c37a6e8":"# Checking the unique values in the dataframe\n#funtion for creation a dataframe to record the columns, their unique values and unique value counts\n\ndef frequency_info(df):\n    df_result = pd.DataFrame(columns=[\"columns\",\"values\",\"unique_values\"])\n    \n    df_temp=pd.DataFrame()\n    for value in df.columns:\n        df_temp[\"columns\"] = [value]\n        df_temp[\"values\"] = [df[value].unique()]\n        df_temp[\"unique_values\"] = df[value].nunique()\n        df_result = df_result.append(df_temp)\n    \n    df_result.sort_values(\"unique_values\", ascending =True, inplace=True)\n    df_result.set_index(\"columns\", inplace=True)\n    return df_result","b299d442":"# Let's check the unique values to identify which can be categorical columns\nfrequency_info(tele_highval_cust)","8dbd032e":"# We will convert the columns which have 2 values to an ordered categorical column and convert it to int type\ncategory_list = ['night_pck_user_6', 'night_pck_user_7', 'night_pck_user_8', 'fb_user_6', 'fb_user_7', 'fb_user_8']\n\ntele_highval_cust[category_list] = tele_highval_cust[category_list].astype(int)\n\ntele_highval_cust[category_list].info()\n","48328165":"cat_cols = ['night_pck_user_6', 'night_pck_user_7', 'night_pck_user_8', \n            'fb_user_6', 'fb_user_7', 'fb_user_8', \n            'monthly_2g_6', 'monthly_2g_7','monthly_2g_8',\n            'sachet_2g_6','sachet_2g_7','sachet_2g_8',\n            'sachet_3g_6','sachet_3g_7','sachet_3g_8',\n            'monthly_3g_6','monthly_3g_7','monthly_3g_8'] \nnum_cols = tele_highval_cust.dtypes[(tele_highval_cust.dtypes == \"float64\") | (tele_highval_cust.dtypes == \"int64\") | (tele_highval_cust.dtypes == \"int32\")].index.to_list()\nfor col in cat_cols:\n    num_cols.remove(col) \nnum_cols.remove('churn')","875714d9":"print(\"CATEGORICAL FEATURES : \\n {} \\n\\n\".format(cat_cols))\nprint(\"NUMERICAL FEATURES : \\n {} \\n\\n\".format(num_cols))","45d5d767":"# Checking the customer churn distribution\nax = (tele_highval_cust['churn'].value_counts()*100.0 \/len(tele_highval_cust)).plot.pie(autopct='%.3f%%', \n                                                                                        labels = ['No', 'Yes'],\n                                                                                        colors =['g','r'],\n                                                                                        figsize =(5,5), \n                                                                                        fontsize = 12 )                                                                           \n\nax.set_ylabel('Churn',fontsize = 12)\nax.set_title('Churn Distribution', fontsize = 12)\nplt.show()","a70974b6":"#Plotting a countplot with all the variables\nfig, axes = plt.subplots(round(len(cat_cols) \/ 3), 3, figsize=(15, 15))\n\ntotal = float(len(tele_highval_cust))\nfor i, ax in enumerate(fig.axes):\n    if i < len(cat_cols):\n        ax.set_xticklabels(ax.xaxis.get_majorticklabels(), rotation=45)\n        sns.countplot(x=cat_cols[i], alpha=0.7, data=tele_highval_cust, hue=tele_highval_cust['churn'],palette=['g','r'], ax=ax)\n        # Renaming the Legends from 0\/1 to Not Churn\/Churn\n        ax.legend(['Not Churn','Churn'],loc=\"upper right\")\n        # adjusting font size of X-Labels and Y-Labels\n        ax.set_xlabel(cat_cols[i],fontsize=12)\n        ax.set_ylabel('count',fontsize=12)\n        # adding annotations\n        for p in ax.patches:\n            height = p.get_height()\n            ax.text(p.get_x()+p.get_width()\/2.,\n                    height + 3,\n                    '{:1.2f}'.format(height\/total),\n                    ha=\"center\",\n                    fontsize=12)\n        \nfig.tight_layout()","aaec952a":"# Data Distribution\nplt.figure(figsize = (18,6))\nplt.subplot(1,3,1)\nsns.distplot(tele_highval_cust.loc[tele_highval_cust['churn']==0,'aon'],hist=True, color = 'g')\nsns.distplot(tele_highval_cust.loc[tele_highval_cust['churn']==1,'aon'],hist=True, color='r')\nplt.legend(['Not Churn','Churn'])\n\nplt.subplot(1,3,2)\nplt.hist(tele_highval_cust.loc[tele_highval_cust['churn']==0,'aon'],color = 'g')\nplt.hist(tele_highval_cust.loc[tele_highval_cust['churn']==1,'aon'],color = 'r')\nplt.legend(['Not Churn','Churn'])\n\nplt.subplot(1,3,3)\nsns.boxplot(y='aon', data=tele_highval_cust, x='churn',palette =['g','r'])\n\nplt.show()","b16a3c5c":"fig, axes = plt.subplots(round(len(num_cols) \/ 6), 6, figsize=(15, 45))\n\nfor i, ax in enumerate(fig.axes):\n    if i < len(num_cols):\n        ax.hist(tele_highval_cust.loc[tele_highval_cust['churn']==0, num_cols[i]],color='g')\n        ax.hist(tele_highval_cust.loc[tele_highval_cust['churn']==1, num_cols[i]],color='r')\n        # adjusting font size of X-Labels and Y-Labels\n        ax.set_xlabel(num_cols[i],fontsize=12)\n        ax.legend(['Not Churn','Churn'],loc = 'best')        \nplt.show()","87a2770f":"# Create box plots for all numeric features\nfig, axes = plt.subplots(round(len(num_cols) \/ 6), 6, figsize=(18, 40))\n\nfor i, ax in enumerate(fig.axes):\n    if i < len(num_cols):\n        ax.set_xticklabels(ax.xaxis.get_majorticklabels(), rotation=45)\n        sns.boxplot(y=num_cols[i], data=tele_highval_cust, x='churn', ax=ax,palette = ['g','r'])\n        ax.set_ylabel(num_cols[i],fontsize=12)\n        ax.set_xlabel('churn',fontsize=12)\n\nfig.tight_layout()","b806d06e":"# Checking the correlation matrix\ncorrmat = tele_highval_cust.corr()\nf, ax = plt.subplots(figsize=(25, 20))\nsns.heatmap(corrmat, vmax=.8,cmap=\"RdYlGn\",linewidth =1)\nplt.show()","6479b32b":"# Checking the top correlated features\ncorr_matrix = tele_highval_cust.corr().abs()\n\n#the matrix is symmetric so we need to extract upper triangle matrix without diagonal (k = 1)\nupper_triangle = (corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool)))\n\nhighly_correlated_features = [column for column in upper_triangle.columns if any(upper_triangle[column] > 0.80)]\nprint(\"List of highly correlated features from the above plot - \\n\\n\", highly_correlated_features)\nprint(\"\\n\\nTotal features with high correlation - \", len(highly_correlated_features))","a04c1df8":"#Find skewed numerical features\nskew_features = tele_highval_cust[num_cols].apply(lambda x: skew(x)).sort_values(ascending=False)\n\nhigh_skew = skew_features[skew_features > 0.2]\nskew_index = high_skew.index\n\nprint(\"There are {} numerical features with Skew > 0.2 \".format(high_skew.shape[0]))\nskewness = pd.DataFrame({'Skew' :high_skew})\nskew_features","5bfb5313":"# We will use Boxcox transformation for numerical variables \n\nfor i in skew_index:\n    tele_highval_cust[i] = boxcox1p(tele_highval_cust[i], boxcox_normmax(tele_highval_cust[i] + 1))","15a00dac":"# Create box plots for all numeric features\nfig, axes = plt.subplots(round(len(num_cols) \/ 6), 6, figsize=(18, 40))\n\nfor i, ax in enumerate(fig.axes):\n    if i < len(num_cols):\n        ax.set_xticklabels(ax.xaxis.get_majorticklabels(), rotation=45)\n        sns.boxplot(y=num_cols[i], data=tele_highval_cust, x='churn', ax=ax,palette = ['g','r'])\n        ax.set_ylabel(num_cols[i],fontsize=12)\n        ax.set_xlabel('churn',fontsize=12)\n\nfig.tight_layout()","293192c2":"tele_highval_cust.columns","e103aa54":"# Custom Function to derive new good phase columns and drop the original columns \ndef derive_good_action_phase(df, col):\n    \n    col_6 = col + \"_6\"\n    col_7 = col + \"_7\"\n    col_8 = col + \"_8\"\n    good_phase_col = col + \"_good_phase\"\n    action_phase_col = col + \"_action_phase\"\n    \n    df[good_phase_col] = (df[col_6] + df[col_7])\/2\n    df[action_phase_col] = df[col_8]\n    \n    df.drop([col_6, col_7, col_8], axis = 1, inplace = True)\n    \n    return df","3449acf8":"# Creating features for good phase and action phase column : \n\ncols = [\"arpu\",\"onnet_mou\",\"offnet_mou\",\"roam_ic_mou\",\"roam_og_mou\",\"loc_og_mou\",\"std_og_mou\",\n        \"isd_og_mou\",\"spl_og_mou\",\"og_others\",\"total_og_mou\",\"loc_ic_mou\",\"std_ic_mou\",\n        \"spl_ic_mou\",\"isd_ic_mou\",\"ic_others\",\"total_ic_mou\",\"total_rech_num\",\"total_rech_amt\",\n        \"max_rech_amt\",\"total_rech_data\",\"max_rech_data\",\"count_rech_2g\",\"count_rech_3g\",\n        \"vol_2g_mb\",\"vol_3g_mb\",\"monthly_2g\",\"sachet_2g\",\"sachet_3g\",\n        \"monthly_3g\",\"vbc_3g\",\"total_data_rech_amt\"]\n\nfor col in cols : \n    tele_highval_cust = derive_good_action_phase(tele_highval_cust, col)\n    print (col)","fc5e294a":"#Checking the dataframe after conversion\ntele_highval_cust.head()","f217909b":"#Checking the new shape of the dataframe\ntele_highval_cust.shape","6567483f":"# We will use sklearn train-test-split we will convert the main dataset to train and test dataset with 70% and 30% split\n\n#Target variable\nX = tele_highval_cust.drop('churn', axis = 1)\ny = tele_highval_cust[['churn']]\n\n# Splitting the data into train and test\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 100)\n\n#Checkiong the shape of the created Train & Test DFs\nprint(\" Shape of X_train is : \",X_train.shape)\nprint(\" Shape of y_train is : \",y_train.shape)\nprint(\" Shape of X_test is  : \",X_test.shape)\nprint(\" Shape of y_test is  : \",y_test.shape)","839723fd":"# Storing the non binary column names as a list for performaing StandardScaler\nfeature_col =[]\ndata_col = tele_highval_cust.columns.to_list()\n\nfor col in data_col:\n    if tele_highval_cust[col].nunique() > 2:\n        feature_col.append(col)\n        \nfeature_col","9e657e23":"#Using StandardScaler for Scaling the X_train and X_test\nscaler = StandardScaler()\n\nscaled_data = scaler.fit_transform(X_train[feature_col])\n\nX_train[feature_col] = scaler.fit_transform(X_train[feature_col])\nX_test[feature_col] = scaler.transform(X_test[feature_col])\n\n# Checking the scaled dataframe\nX_train.head()","854eb4de":"\n# Checking the scaled dataframe\nX_test.head()","5f3dd9c5":"from imblearn.over_sampling import SMOTE\n\n#oversampling minority class using smote - SMOTEENN\nsampling = SMOTEENN(random_state = 0)\nX_train_resampled, y_train_resampled = sampling.fit_sample(X_train, y_train)\nX_train_resampled = pd.DataFrame(data = X_train_resampled)\ny_train_resampled = pd.DataFrame(data = y_train_resampled)\nprint (\" Shape of the X_train after resampling : \",X_train_resampled.shape)\nprint (\" Shape of the Y_train after resampling : \",y_train_resampled.shape)","c9efc1a4":"# RandomOverSampler\nX_r_over_sampled, y_r_over_sampled = RandomOverSampler(random_state = 100).fit_sample(X_train, y_train)\n\nX_r_over_sampled = pd.DataFrame(data = X_r_over_sampled)\ny_r_over_sampled = pd.DataFrame(data = y_r_over_sampled)\nprint (\" Shape of the X_train after Random Over Sampling : \",X_r_over_sampled.shape)\nprint (\" Shape of the Y_train after Random Over Sampling : \",y_r_over_sampled.shape)","331f60e4":"# RandomUnderSampler\nX_r_under_sampled, y_r_under_sampled = RandomUnderSampler(random_state = 100).fit_sample(X_train, y_train)\n\nX_r_under_sampled = pd.DataFrame(data = X_r_under_sampled)\ny_r_under_sampled = pd.DataFrame(data = y_r_under_sampled)\nprint (\" Shape of the X_train after Random Under Sampling : \",X_r_under_sampled.shape)\nprint (\" Shape of the Y_train after Random Under Sampling : \",y_r_under_sampled.shape)","85092d13":"# SMOTE\nX_smote_samples, y_smote_samples = SMOTE(n_jobs = -1, random_state = 100).fit_sample(X_train, y_train)\n\nX_smote_samples = pd.DataFrame(data = X_smote_samples)\ny_smote_samples = pd.DataFrame(data = y_smote_samples)\nprint (\" Shape of the X_train after SMOTE : \",X_smote_samples.shape)\nprint (\" Shape of the Y_train after SMOTE : \",y_smote_samples.shape)","74ea217c":"# function for running RFE on Logistic Regression Model\ndef run_rfe_on_logistic(X_train, y_train, no_of_features):\n    logreg = LogisticRegression()\n    rfe = RFE(logreg, no_of_features)             # running RFE with the given number of variables as output\n    rfe = rfe.fit(X_train, y_train)\n    col = X_train.columns[rfe.support_]\n    return col","d9e4cd0e":"# Build the logistic model with StatsModels \ndef build_logistic_model(X_train_logistic,Y_train_logistic):\n    X_train_sm = sm.add_constant(X_train_logistic)\n    logistic_model = sm.GLM(Y_train_logistic,X_train_sm, family = sm.families.Binomial())\n    logistic_model = logistic_model.fit()\n    return logistic_model, X_train_sm","8438b526":"# Function for plotting precision, recall , accuracy curve \ndef predict_train_using_logistic_model(logistic_model, X_train_sm, y_train):\n    y_train_pred = logistic_model.predict(X_train_sm)\n    y_train_pred = y_train_pred.values.reshape(-1)\n    y_train_actual = y_train.values.reshape(-1)\n    y_train_pred_final = pd.DataFrame({'Churn_Actual':y_train_actual, 'Churn_pred_prob':y_train_pred})\n    \n    # create metrics DF for different cut off\n    numbers = [float(x)\/10 for x in range(10)]\n    for i in numbers:\n        y_train_pred_final[i]= y_train_pred_final['Churn_pred_prob'].map(lambda x: 1 if x > i else 0)\n        \n    # Now let's calculate accuracy sensitivity and specificity for various probability cutoffs.\n    metrics_df = pd.DataFrame( columns = ['prob','accuracy','sensi','speci','prec','rec'])\n    num = [0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]\n    for i in num:\n        cm1 = metrics.confusion_matrix(y_train_pred_final['Churn_Actual'], y_train_pred_final[i] )\n        total1=sum(sum(cm1))\n        accuracy = (cm1[0,0]+cm1[1,1])\/total1\n        speci = cm1[0,0]\/(cm1[0,0]+cm1[0,1])\n        sensi = cm1[1,1]\/(cm1[1,0]+cm1[1,1])\n        prec = cm1[1,1] \/ (cm1[1,1] + cm1[0,1])\n        rec = cm1[1,1] \/ (cm1[1,0] + cm1[1,1])\n        metrics_df.loc[i] =[ i ,accuracy,sensi,speci, prec, rec]\n        \n    plt.figure(figsize=(15,7))\n    \n    # plotting the graphs \n    plt.subplot(1,2,1)\n    sns.lineplot(data=metrics_df)\n    #cutoff_df.plot.line(x='prob', y=['accuracy','sensi','speci','prec','rec'])\n    plt.xticks(np.arange(0,1,step=0.05),size=8)\n    plt.yticks(size=12)\n    plt.legend(loc='lower right')\n    plt.title('Metrics plot')\n    plt.xlabel('Probability of churn')\n    plt.ylabel('Metrics values')\n \n    # AUC-ROC Curve \n    fpr, tpr, thresholds = metrics.roc_curve(y_train_pred_final['Churn_Actual'], \n                                             y_train_pred_final['Churn_pred_prob'],\n                                             drop_intermediate = False )\n    auc_score = metrics.roc_auc_score(y_train_pred_final['Churn_Actual'],\n                                      y_train_pred_final['Churn_pred_prob'] )\n    plt.subplot(1,2,2)\n    plt.plot( fpr, tpr, label='ROC curve (area = %0.2f)' % auc_score )\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate or [1 - True Negative Rate]')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver operating characteristic example')\n    plt.legend(loc=\"lower right\")\n    plt.show()\n    \n    return y_train_pred_final","fa8d5508":"# Function to generate model evaluation metrics and graphs\ndef classification_algo_metrics(y_actual, y_pred):\n    print(\"Classification report:\\n\", classification_report(y_actual,y_pred))\n    \n    accuracy = round(accuracy_score(y_actual, y_pred),4)\n    precision = round(precision_score(y_actual, y_pred),4)\n    recall = round(recall_score(y_actual, y_pred),4)\n    f1 = round(f1_score(y_actual, y_pred),4)\n    conf_matrix = confusion_matrix(y_actual, y_pred) # confusion matrix\n    model_roc_auc = round(roc_auc_score(y_actual, y_pred),4) # roc_auc_score\n    \n    print(\"Accuracy Score   : \", accuracy)\n    print(\"Precision Score  : \", precision)\n    print(\"Recall Score     : \", recall) \n    print(\"F1 Score         : \", f1)  \n    print(\"Area under curve : \", model_roc_auc,\"\\n\")\n     \n    # Confusion Matrix\n    cm = metrics.confusion_matrix( y_actual, y_pred, [0,1] )\n    sns.heatmap(cm, annot=True, fmt='.0f', cmap=\"PuBu\",\n    xticklabels = [\"Not Churned\", \"Churned\"] ,\n    yticklabels = [\"Not Churned\", \"Churned\"] )\n    plt.ylabel('True labels')\n    plt.xlabel('Predicted labels')\n    plt.show()  \n    return (accuracy, precision, recall, f1, model_roc_auc)","ea9e5231":"def predict_test_using_logistic_model(logistic_model, X_test, y_test, cutoff):\n    X_test_sm = sm.add_constant(X_test) # adding contant value \n    y_test_pred = logistic_model.predict(X_test_sm) # Predicting on the test data set\n    y_test_pred = y_test_pred.values.reshape(-1)\n    y_test_pred = pd.DataFrame(y_test_pred) # Converting y_pred to a dataframe which is an array\n    y_test_df = pd.DataFrame(y_test) # Converting y_test to dataframe\n    \n    # Removing index for both dataframes to append them side by side \n    y_test_pred.reset_index(drop=True, inplace=True)\n    y_test_df.reset_index(drop=True, inplace=True)\n\n    # Appending y_test_df and y_pred_1\n    y_pred_final = pd.concat([y_test_df, y_test_pred],axis=1)\n    # Renaming the column \n    y_pred_final= y_pred_final.rename(columns={ 0 : 'Churned_Prob'})\n    # Making prediction of churn or not based on cutoff selected\n    y_pred_final['final_predicted'] = y_pred_final.Churned_Prob.map(lambda x: 1 if x > cutoff else 0)\n    return y_pred_final","37360356":"# create an Empty DataFrame to store results\nresults = pd.DataFrame()","f66c8f32":"# Step 1: Run RFE\nimbalanced_rfe_features = run_rfe_on_logistic(X_train, y_train, no_of_features=25)\nX_train_rfe_imbalanced = X_train[imbalanced_rfe_features]\nX_train_rfe_imbalanced.shape","7184d050":"# Step 2: Build the logistic model using RFE selected columns with StatsModels \nlogistic_model_imbalanced, X_train_sm_imbalanced = build_logistic_model(X_train_rfe_imbalanced, y_train)\nlogistic_model_imbalanced.summary()","ba262388":"# Step 3: Predict using the training data\ny_train_pred_final = predict_train_using_logistic_model(logistic_model_imbalanced, X_train_sm_imbalanced, y_train)","8db0fe5a":"# Step 4: Decide the cutoff based on the metrics plot\ncutoff = 0.05 \ny_train_pred_final['final_predicted_1'] = y_train_pred_final['Churn_pred_prob'].map( lambda x: 1 if x > cutoff else 0)\n# deleting the unnecessary columns of all other cutoff\ny_train_pred_final.drop([0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9],axis = 1, inplace = True) ","f1aa6210":"# Step 5: Evaluate the metrics on the training data set\nprint(\"Evaluation on training data set: \\n\")\nclassification_algo_metrics(y_train, y_train_pred_final['final_predicted_1'])","6bab1e1e":"# Step 6: Predict on the test dataset\n# Selecting only the columns used in final model of Train Dataset\ny_test_pred_final = predict_test_using_logistic_model(logistic_model_imbalanced, \n                                                      X_test[imbalanced_rfe_features], \n                                                      y_test, cutoff=0.05) \ny_test_pred_final.head()","cd11cf32":"# Step 7: Evaluate the metrics on the test data set\nprint(\"Evaluation on test data set: \\n\")\naccuracy, precision, recall, f1, model_roc_auc = classification_algo_metrics(y_test, y_test_pred_final['final_predicted'])\n\ntempResults = pd.DataFrame({'Model':['Logistic Regression on imbalanced data without PCA'], \n                            'Accuracy': [accuracy],\n                            'Precision': [precision], \n                            'Recall': [recall], \n                            'F1 score': [f1], \n                            'Area under ROC curve': [model_roc_auc] })\nresults = pd.concat([results, tempResults])\nresults = results[['Model', 'Accuracy', 'Precision', 'Recall', 'F1 score', 'Area under ROC curve']]\nresults","8e2df098":"# Step 1: Run RFE\nsmote_rfe_features = run_rfe_on_logistic(X_smote_samples, y_smote_samples, no_of_features=25)\nX_train_rfe_smote = X_smote_samples[smote_rfe_features]\nX_train_rfe_smote.shape","a7848d58":"# Step 2: Build the logistic model using RFE selected columns with StatsModels \nlogistic_model_smote, X_train_sm_smote = build_logistic_model(X_train_rfe_smote, y_smote_samples)\nlogistic_model_smote.summary()","e26ebd7a":"# Step 3: Predict using the training data\ny_train_pred_final = predict_train_using_logistic_model(logistic_model_smote, X_train_sm_smote, y_smote_samples)","4a4c74e2":"# Step 4: Decide the cutoff based on the metrics plot\ncutoff = 0.46\ny_train_pred_final['final_predicted_1'] = y_train_pred_final['Churn_pred_prob'].map( lambda x: 1 if x > cutoff else 0)\ny_train_pred_final.drop([0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9],axis = 1, inplace = True) # deleting the unnecessary columns of all other cutoff","949f63f7":"# Step 5: Evaluate the metrics on the training data set\nprint(\"Evaluation on training data set: \\n\")\nclassification_algo_metrics(y_smote_samples, y_train_pred_final['final_predicted_1'])","a95fef97":"# Step 6: Predict on the test dataset\ny_test_pred_final = predict_test_using_logistic_model(logistic_model_smote, X_test[smote_rfe_features], y_test, cutoff=0.46) # Selecting only the columns used in final model of Train Dataset\ny_test_pred_final.head()","b496eba2":"# Step 7: Evaluate the metrics on the test data set\nprint(\"Evaluation on test data set: \\n\")\naccuracy, precision, recall, f1, model_roc_auc = classification_algo_metrics(y_test, y_test_pred_final['final_predicted'])\n\ntempResults = pd.DataFrame({'Model':['Logistic Regression with SMOTE & without PCA'], \n                            'Accuracy': [accuracy],\n                            'Precision': [precision],\n                            'Recall': [recall], \n                            'F1 score': [f1], \n                            'Area under ROC curve': [model_roc_auc] })\n\nresults = pd.concat([results, tempResults])\nresults = results[['Model', 'Accuracy', 'Precision', 'Recall', 'F1 score', 'Area under ROC curve']]\nresults","eec37b7b":"def perform_PCA(X):\n    \n    pca = PCA(svd_solver = 'randomized', random_state = 100)\n\n    #Doing the PCA on the train data\n    pca.fit(X)\n    \n    # Making the screeplot - plotting the cumulative variance against the number of components\n    fig = plt.figure(figsize = (10,6))\n\n    plt.plot(np.cumsum(pca.explained_variance_ratio_))\n\n    plt.xlabel('number of components')\n    plt.ylabel('cumulative explained variance')\n    plt.show()","bdb56199":"def perform_incremental_PCA(X_train, y_train, components):\n    \n    # Using incremental PCA for efficiency - saves a lot of time on larger datasets\n\n    pca_final = IncrementalPCA(n_components = components)\n\n    X_train_pca = pca_final.fit_transform(X_train)\n    X_test_pca = pca_final.transform(X_test)\n\n    X_train_pca = pd.DataFrame(data = X_train_pca)\n    X_test_pca = pd.DataFrame(data = X_test_pca)\n\n    print(\"Shape of X train PCA : \", X_train_pca.shape)\n    print(\"Shape of Y train PCA : \", y_train.shape)\n    print(\"Shape of X test PCA : \", X_test_pca.shape)\n    print(\"Shape of Y test PCA : \", y_test.shape)\n    \n    #creating correlation matrix for the principal components\n    corrmat = np.corrcoef(X_train_pca.transpose())\n\n    #plotting the correlation matrix\n    plt.figure(figsize = (10,8))\n    sns.heatmap(corrmat, annot = False,cmap=\"RdYlGn\",linewidth =1)\n    plt.show()\n    \n    # 1s -> 0s in diagonals\n    corrmat_nodiag = corrmat - np.diagflat(corrmat.diagonal())\n    print(\"max corr:\",corrmat_nodiag.max(), \", min corr: \", corrmat_nodiag.min(),)\n    \n    return X_train_pca, X_test_pca","3e305ce5":"# Step 1.1: Perform PCA on the whole training dataset to identify number of features\nperform_PCA(X_r_over_sampled)","f7b9e3c6":"# Step 1.2 : Create 25 Principal components\nX_train_pca_ro, X_test_pca_ro = perform_incremental_PCA(X_r_over_sampled, y_r_over_sampled, 25)","3332aeaa":"# Step 2: Build the logistic model using PCA Components with StatsModels \nlogistic_model_pca_r_over_balanced, X_train_sm_pca_r_over_balanced = build_logistic_model(X_train_pca_ro, y_r_over_sampled.values.ravel())\nlogistic_model_pca_r_over_balanced.summary()","8e6ef0d7":"# Step 3: Predict using the training data\ny_train_pred_final = predict_train_using_logistic_model(logistic_model_pca_r_over_balanced, X_train_sm_pca_r_over_balanced, y_r_over_sampled)","7b4d0f00":"# Step 4: Decide the cutoff based on the metrics plot\ncutoff = 0.45 \ny_train_pred_final['final_predicted_1'] = y_train_pred_final['Churn_pred_prob'].map( lambda x: 1 if x > cutoff else 0)\ny_train_pred_final.drop([0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9],axis = 1, inplace = True) # deleting the unnecessary columns of all other cutoff","70d1e39e":"# Step 5: Evaluate the metrics on the training data set\nprint(\"Evaluation on training data set: \\n\")\nclassification_algo_metrics(y_r_over_sampled, y_train_pred_final['final_predicted_1'])","175c88d7":"# Step 6: Predict on the test dataset\n# Selecting only the columns used in final model of Train Dataset\ny_test_pred_final = predict_test_using_logistic_model(logistic_model_pca_r_over_balanced, X_test_pca_ro, y_test, cutoff=0.45) \ny_test_pred_final.head()","ee108101":"# Step 7: Evaluate the metrics on the test data set\nprint(\"Evaluation on test data set: \\n\")\naccuracy, precision, recall, f1, model_roc_auc = classification_algo_metrics(y_test, y_test_pred_final['final_predicted'])\n\ntempResults = pd.DataFrame({'Model':['Logistic Regression with PCA & Random over sampling'], \n                            'Accuracy': [accuracy],\n                            'Precision': [precision], \n                            'Recall': [recall], \n                            'F1 score': [f1], \n                            'Area under ROC curve': [model_roc_auc] })\n\nresults = pd.concat([results, tempResults])\nresults = results[['Model', 'Accuracy', 'Precision', 'Recall', 'F1 score', 'Area under ROC curve']]\nresults","e504add1":"# Step 1.1: Perform PCA on the whole training dataset to identify number of features\nperform_PCA(X_r_under_sampled)","14633d5f":"X_train_pca_ru, X_test_pca_run = perform_incremental_PCA(X_r_under_sampled, y_r_under_sampled, 25)","7cfd9afa":"# Step 2: Build the logistic model using PCA Components with StatsModels \nlogistic_model_pca_under_sampled, X_train_sm_pca_under_sampled = build_logistic_model(X_train_pca_ru, y_r_under_sampled.values.ravel())\n# Skip as the features are PCA uninterpretable components\n# logistic_model_pca_under_sampled.summary() ","ce492a0b":"# Step 3: Predict using the training data\ny_train_pred_final = predict_train_using_logistic_model(logistic_model_pca_under_sampled, X_train_sm_pca_under_sampled, y_r_under_sampled)","5bce73d2":"# Step 4: Decide the cutoff based on the metrics plot\ncutoff = 0.45 \ny_train_pred_final['final_predicted_1'] = y_train_pred_final['Churn_pred_prob'].map( lambda x: 1 if x > cutoff else 0)\n# deleting the unnecessary columns of all other cutoff\ny_train_pred_final.drop([0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9],axis = 1, inplace = True) ","f4e59608":"# Step 5: Evaluate the metrics on the training data set\nprint(\"Evaluation on training data set: \\n\")\nclassification_algo_metrics(y_r_under_sampled, y_train_pred_final['final_predicted_1'])","7a824bc9":"# Step 6: Predict on the test dataset\n# Selecting only the columns used in final model of Train Dataset\ny_test_pred_final = predict_test_using_logistic_model(logistic_model_pca_under_sampled, X_test_pca_run, y_test, cutoff=0.45) \ny_test_pred_final.head()","839cbf9d":"# Step 7: Evaluate the metrics on the test data set\nprint(\"Evaluation on test data set: \\n\")\naccuracy, precision, recall, f1, model_roc_auc = classification_algo_metrics(y_test, y_test_pred_final['final_predicted'])\n\ntempResults = pd.DataFrame({'Model':['Logistic Regression with PCA & Random under sampling'],\n                            'Accuracy': [accuracy],\n                            'Precision': [precision], \n                            'Recall': [recall], \n                            'F1 score': [f1], \n                            'Area under ROC curve': [model_roc_auc] })\n\nresults = pd.concat([results, tempResults])\nresults = results[['Model', 'Accuracy', 'Precision', 'Recall', 'F1 score', 'Area under ROC curve']]\nresults","7336a5ad":"# Step 1.1: Perform PCA on the whole training dataset to identify number of features\nperform_PCA(X_smote_samples)","fde3e02c":"X_train.shape","5f091c0d":"# Step 1.2 : Create 25 Principal components\nX_train_pca_smote, X_test_pca_smote = perform_incremental_PCA(X_smote_samples, y_smote_samples, 25)","c6aaa3e8":"# Step 2: Build the logistic model using PCA Components with StatsModels \nlogistic_model_pca_smote, X_train_sm_pca_smote = build_logistic_model(X_train_pca_smote, y_smote_samples.values.ravel())\n# Skip as the features are PCA uninterpretable components\n# logistic_model_pca_smote.summary() ","bf3e6954":"# Step 3: Predict using the training data\ny_train_pred_final = predict_train_using_logistic_model(logistic_model_pca_smote, X_train_sm_pca_smote, y_smote_samples)","09a8234d":"# Step 4: Decide the cutoff based on the metrics plot\ncutoff = 0.45 \ny_train_pred_final['final_predicted_1'] = y_train_pred_final['Churn_pred_prob'].map( lambda x: 1 if x > cutoff else 0)\n# deleting the unnecessary columns of all other cutoff\ny_train_pred_final.drop([0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9],axis = 1, inplace = True) ","260880b8":"# Step 5: Evaluate the metrics on the training data set\nprint(\"Evaluation on training data set: \\n\")\nclassification_algo_metrics(y_smote_samples, y_train_pred_final['final_predicted_1'])","0d915b11":"# Step 6: Predict on the test dataset\n# Selecting only the columns used in final model of Train Dataset\ny_test_pred_final = predict_test_using_logistic_model(logistic_model_pca_smote, X_test_pca_smote, y_test, cutoff=0.45) \ny_test_pred_final.head()","4165a3a9":"# Step 7: Evaluate the metrics on the test data set\nprint(\"Evaluation on test data set: \\n\")\naccuracy, precision, recall, f1, model_roc_auc = classification_algo_metrics(y_test, y_test_pred_final['final_predicted'])\n\ntempResults = pd.DataFrame({'Model':['Logistic Regression with PCA & SMOTE'], \n                            'Accuracy': [accuracy],\n                            'Precision': [precision],\n                            'Recall': [recall],\n                            'F1 score': [f1],\n                            'Area under ROC curve': [model_roc_auc] })\n\nresults = pd.concat([results, tempResults])\nresults = results[['Model', 'Accuracy', 'Precision', 'Recall', 'F1 score', 'Area under ROC curve']]\nresults","ee021e2e":"# Step 1.1: Perform PCA on the whole training dataset to identify number of features\nperform_PCA(X_train_resampled)","3a5645fa":"# Step 1.2 : Create 25 Principal components\nX_train_pca_smoteen, X_test_pca_smoteen = perform_incremental_PCA(X_train_resampled, y_train_resampled, 25)","dd33b55e":"# Step 2: Build the logistic model using PCA Components with StatsModels \nlogistic_model_pca_smoteen, X_train_sm_pca_smoteen = build_logistic_model(X_train_pca_smoteen, y_train_resampled.values.ravel())\n# Skip as the features are PCA uninterpretable components\n# logistic_model_pca_smoteen.summary() ","3af34ea6":"# Step 3: Predict using the training data\ny_train_pred_final = predict_train_using_logistic_model(logistic_model_pca_smoteen, X_train_sm_pca_smoteen, y_train_resampled)","e168f13f":"# Step 4: Decide the cutoff based on the metrics plot\ncutoff = 0.46\ny_train_pred_final['final_predicted_1'] = y_train_pred_final['Churn_pred_prob'].map( lambda x: 1 if x > cutoff else 0)\n# deleting the unnecessary columns of all other cutoff\ny_train_pred_final.drop([0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9],axis = 1, inplace = True) ","09c5da18":"# Step 5: Evaluate the metrics on the training data set\nprint(\"Evaluation on training data set: \\n\")\nclassification_algo_metrics(y_train_resampled, y_train_pred_final['final_predicted_1'])","5be1189e":"# Step 6: Predict on the test dataset\n# Selecting only the columns used in final model of Train Dataset\ny_test_pred_final = predict_test_using_logistic_model(logistic_model_pca_smoteen, X_test_pca_smoteen, y_test, cutoff=0.46) \ny_test_pred_final.head()","5cf9c174":"# Step 7: Evaluate the metrics on the test data set\nprint(\"Evaluation on test data set: \\n\")\naccuracy, precision, recall, f1, model_roc_auc = classification_algo_metrics(y_test, y_test_pred_final['final_predicted'])\n\ntempResults = pd.DataFrame({'Model':['Logistic Regression with PCA & SMOTEENN'], \n                            'Accuracy': [accuracy],\n                            'Precision': [precision], \n                            'Recall': [recall], \n                            'F1 score': [f1], \n                            'Area under ROC curve': [model_roc_auc] })\n\nresults = pd.concat([results, tempResults])\nresults = results[['Model', 'Accuracy', 'Precision', 'Recall', 'F1 score', 'Area under ROC curve']]\nresults","56cc452f":"# Step 1: Create 25 principal components\nX_train_pca_reg, X_test_pca_reg = perform_incremental_PCA(X_train_resampled, y_train_resampled, 25)","8dec19f0":"# Step 2.1: Hyper parameter tuning: Decide whether to apply l1 or l2 penalty using GridSearchCV\nlr = LogisticRegression(n_jobs = -1, random_state = 100)\nparameter = 'penalty'\nparam_grid = {parameter: ['l1', 'l2']} # parameters to build the model on\n\ngc = GridSearchCV(estimator = lr, param_grid = param_grid, scoring = 'roc_auc', n_jobs = 15, cv = 5, verbose = 2)\ngc = gc.fit(X_train_resampled.values, y_train_resampled.values.ravel())\n\n# scores of GridSearch CV\nscores = gc.cv_results_    \n\n# Get the best value\ngc.best_params_","2b224e3a":"# Step 2.2: Hyper parameter tuning: Decide penalty value of 'c' for L2 regularization\nlr = LogisticRegression(penalty = 'l2', n_jobs = -1, random_state = 100)\nparameter = 'C'\nparam_grid = {parameter: [1, 10, 100, 1000]} # parameters to build the model on\n\ngc = GridSearchCV(estimator = lr, param_grid = param_grid, scoring = 'roc_auc', n_jobs = 15, cv = 5, verbose = 2)\ngc = gc.fit(X_train_resampled.values, y_train_resampled.values.ravel())\n\n# scores of GridSearch CV\nscores = gc.cv_results_\n\n# Plot the scores\ncol = \"param_\" + parameter\n    \n#plt.plot(scores[col], scores[\"mean_train_score\"], label = \"training accuracy\")\nplt.plot(scores[col], scores[\"mean_test_score\"], label = \"test accuracy\")\n\nplt.xlabel(parameter)\nplt.ylabel(\"Accuracy\")\n\nplt.legend()\nplt.show()\n\n# Get the best value\ngc.best_params_","8fdd9b03":"# Step 3: Build logistic model with regularization penalty and Principal components\n\nlr = LogisticRegression(penalty = 'l2', C = 1, n_jobs = -1, random_state = 100)\nlogistic_model_pca_smoteen_reg = lr.fit(X_train_pca_reg.values, y_train_resampled.values.ravel())","bc01bbd9":"# Step 4: Predict using the training data\ny_train_pred_final = logistic_model_pca_smoteen_reg.predict(X_train_pca_reg.values)\ny_train_pred_final","a530fe2a":"# Step 5: Evaluate the metrics on the training data set\nprint(\"Evaluation on training data set: \\n\")\nclassification_algo_metrics(y_train_resampled, y_train_pred_final)","8c8d5e14":"# Step 6: Predict on the test dataset\ny_test_pred_final = logistic_model_pca_smoteen_reg.predict(X_test_pca_reg.values)\ny_test_pred_final","650c81cb":"# Step 7: Evaluate the metrics on the test data set\nprint(\"Evaluation on test data set: \\n\")\naccuracy, precision, recall, f1, model_roc_auc = classification_algo_metrics(y_test, y_test_pred_final)\n\ntempResults = pd.DataFrame({'Model':['Logistic Regression with PCA, SMOTEEN, L2 regularization'], \n                            'Accuracy': [accuracy],\n                            'Precision': [precision], \n                            'Recall': [recall], \n                            'F1 score': [f1],\n                            'Area under ROC curve': [model_roc_auc] })\n\nresults = pd.concat([results, tempResults])\nresults = results[['Model', 'Accuracy', 'Precision', 'Recall', 'F1 score', 'Area under ROC curve']]\nresults","943a31e8":"#%%time\n#params = { 'C' :[0.1,0.3,0.5],\n#           'gamma':[0.3,0.5,0.7]\n#         }\n\n\n#g_search_svm = GridSearchCV(estimator = SVC(), scoring = 'recall', cv=3,\n#                                                           param_grid = params)\n#g_search_svm.fit(X_train_resampled,y_train_resampled)\n#print(\"SVM Best Score : \" ,g_search_svm.best_score_)\n#print(\"SVM Best Params : \" ,g_search_svm.best_params_)","d540105f":"# We we create a new model for SVM with hyperparameter tuning process selected best parameters \n#best parameters : 'C' : 0.1 'gamma':0.5\nmodel_svm = SVC(C = 0.1,gamma = 0.5,kernel = 'linear', probability = True, random_state = 100)\nmodel_svm.fit(X_train_resampled,y_train_resampled)","44e887d1":"# Checking the performance on the train dataset\ny_train_svm = model_svm.predict(X_train)\nprint(\"Evaluation on training data set: \\n\")\nclassification_algo_metrics(y_train,y_train_svm)","0f7f6c58":"#Checking the AUC ROC Curve\nplot_roc_curve(model_svm, X_train_resampled,y_train_resampled)\nplt.show()","dbeac8db":"# Evaluate the metrics on the test data set\n\ny_test_svm = model_svm.predict(X_test)\nprint(\"Evaluation on test data set: \\n\")\n\naccuracy, precision, recall, f1, model_roc_auc = classification_algo_metrics(y_test,y_test_svm)\n\ntempResults = pd.DataFrame({'Model':['SVM with SMOTEENN'], \n                            'Accuracy': [accuracy],\n                            'Precision': [precision],\n                            'Recall': [recall], \n                            'F1 score': [f1], \n                            'Area under ROC curve': [model_roc_auc] })\n\nresults = pd.concat([results, tempResults])\nresults = results[['Model', 'Accuracy', 'Precision', 'Recall', 'F1 score', 'Area under ROC curve']]\nresults","7a68b542":"# Running RandomForestClassifier without hyper parameter tuning with the actual train and test data data\nmodel_rf = RandomForestClassifier(random_state = 100, n_estimators= 100,n_jobs =-1, class_weight = {0:1,1:9})\nmodel_rf.fit(X_train,y_train)\n","70b3501d":"# Checking the performance of the train dataset\ny_train_rf = model_rf.predict(X_train)\nprint(\"Evaluation on training data set: \\n\")\nclassification_algo_metrics(y_train,y_train_rf)","e1267b46":"#Checking the AUC ROC Curve\nplot_roc_curve(model_rf, X_train_resampled,y_train_resampled)\nplt.show()","40f0e817":"# Checking the performance on the test dataset\ny_test_rf = model_rf.predict(X_test)\n\nprint(\"Evaluation on test data set: \\n\")\n\naccuracy, precision, recall, f1, model_roc_auc = classification_algo_metrics(y_test,y_test_rf)\n\ntempResults = pd.DataFrame({'Model':['Random Forest with class_weight'], \n                            'Accuracy': [accuracy],\n                            'Precision': [precision],\n                            'Recall': [recall], \n                            'F1 score': [f1], \n                            'Area under ROC curve': [model_roc_auc] })\n\nresults = pd.concat([results, tempResults])\nresults = results[['Model', 'Accuracy', 'Precision', 'Recall', 'F1 score', 'Area under ROC curve']]\nresults","2e91e693":"# Running RandomForestClassifier without hyper parameter tuning with the resampled data\nmodel_rf = RandomForestClassifier(random_state = 100, n_estimators= 100,n_jobs =-1)\nmodel_rf.fit(X_train_resampled,y_train_resampled)\n","56a8c50a":"# Checking the performance of the train dataset\ny_train_rf = model_rf.predict(X_train)\nprint(\"Evaluation on training data set: \\n\")\nclassification_algo_metrics(y_train,y_train_rf)","4f344c32":"#Checking the AUC ROC Curve\nplot_roc_curve(model_rf, X_train_resampled,y_train_resampled)\nplt.show()","240d07d3":"# Checking the performance on the test dataset\ny_test_rf = model_rf.predict(X_test)\nprint(\"Evaluation on test data set: \\n\")\n\naccuracy, precision, recall, f1, model_roc_auc = classification_algo_metrics(y_test,y_test_rf)\n\ntempResults = pd.DataFrame({'Model':['Random Forest with SMOTEENN'], \n                            'Accuracy': [accuracy],\n                            'Precision': [precision],\n                            'Recall': [recall], \n                            'F1 score': [f1], \n                            'Area under ROC curve': [model_roc_auc] })\n\nresults = pd.concat([results, tempResults])\nresults = results[['Model', 'Accuracy', 'Precision', 'Recall', 'F1 score', 'Area under ROC curve']]\nresults","56ed92fe":"# Now let's do hyperparametertuning for RandomForest and try to find best parameters to improve the precision and recall score : \n#params = { 'n_estimators':[500,1000],\n#          'max_features': ['auto','sqrt'] ,\n#          'max_depth' : [10,20,30] , \n#          'min_samples_leaf':[50,100,150],\n#          'min_samples_split':[100,150,200]\n#          'criterion':['gini','entropy']\n#         }\n#g_search_rf = GridSearchCV(estimator = RandomForestClassifier(random_state = 100,n_jobs = -1), scoring = 'recall', cv=3,\n#                                                           param_grid = params)\n#g_search_rf.fit(X_train_resampled,y_train_resampled)\n#print(\"Random Forest Best Score : \" ,g_search_rf.best_score_)\n#print(\"Random Forest Best Params : \" ,g_search_rf.best_params_)","6815561d":"# Let's build the model using the hyperparameters we got from GridSearchCV\nmodel_rf = RandomForestClassifier(random_state = 100, n_jobs =-1,\n                                  n_estimators= 500,\n                                  max_depth = 30,\n                                  max_features = 'auto',\n                                  min_samples_leaf = 100,\n                                  min_samples_split=200,\n                                  criterion ='gini')\nmodel_rf.fit(X_train_resampled,y_train_resampled)","df887ea7":"# Checking the performance of the train dataset\ny_train_rf = model_rf.predict(X_train)\nprint(\"Evaluation on training data set: \\n\")\nclassification_algo_metrics(y_train,y_train_rf)","b3c3d898":"#Checking the AUC ROC Curve\nplot_roc_curve(model_rf, X_train_resampled,y_train_resampled)\nplt.show()","28388937":"# Checking the performance on the test dataset\ny_test_rf = model_rf.predict(X_test)\n\nprint(\"Evaluation on test data set: \\n\")\n\naccuracy, precision, recall, f1, model_roc_auc = classification_algo_metrics(y_test,y_test_rf)\n\ntempResults = pd.DataFrame({'Model':['Random Forest with SMOTEENN & Hyperparameter tuned'], \n                            'Accuracy': [accuracy],\n                            'Precision': [precision],\n                            'Recall': [recall], \n                            'F1 score': [f1], \n                            'Area under ROC curve': [model_roc_auc] })\n\nresults = pd.concat([results, tempResults])\nresults = results[['Model', 'Accuracy', 'Precision', 'Recall', 'F1 score', 'Area under ROC curve']]\nresults","72c89bc7":"# Now let's do hyperparametertuning for RandomForest and try to find best parameters to improve the precision and recall score :\n\n#params = { 'n_estimators':[300,500,1000],\n#           'learning_rate': [0.05,0.1,0.5,1]\n#          \n#         }\n#g_search_ada = GridSearchCV(estimator = AdaBoostClassifier(random_state = 100), scoring = 'recall', cv=3,\n##                                                           param_grid = params)\n#g_search_ada.fit(X_train_resampled,y_train_resampled)\n#print(\"AdaBoost Best Score : \" ,g_search_ada.best_score_)\n#print(\"AdaBoost Best Params : \" ,g_search_ada.best_params_)","4b0c100e":"model_ada = AdaBoostClassifier(learning_rate=0.05,n_estimators = 500)\nmodel_ada.fit(X_train_resampled,y_train_resampled)","9f6701ba":"# Checking the performance of the train dataset\ny_train_ada = model_ada.predict(X_train)\nprint(\"Evaluation on training data set: \\n\")\nclassification_algo_metrics(y_train,y_train_ada)","94129809":"#Checking the AUC ROC Curve\nplot_roc_curve(model_ada, X_train_resampled,y_train_resampled)\nplt.show()","73d969c1":"# Checking the performance on the test dataset\ny_test_ada = model_ada.predict(X_test)\n\nprint(\"Evaluation on test data set: \\n\")\n\naccuracy, precision, recall, f1, model_roc_auc = classification_algo_metrics(y_test,y_test_ada)\n\ntempResults = pd.DataFrame({'Model':['AdaBoost with SMOTEENN & Hyperparameter tuned'], \n                            'Accuracy': [accuracy],\n                            'Precision': [precision],\n                            'Recall': [recall], \n                            'F1 score': [f1], \n                            'Area under ROC curve': [model_roc_auc] })\n\nresults = pd.concat([results, tempResults])\nresults = results[['Model', 'Accuracy', 'Precision', 'Recall', 'F1 score', 'Area under ROC curve']]\nresults","a73b7a37":"#params_xgb = {\n#        'min_child_weight': [ 5, 10],\n #       'gamma': [0.1,0.6, 1, 2],\n #       'subsample': [0.6,0.8, 1.0],\n#        'colsample_bytree': [0.6,0.8, 1.0],\n#        'max_depth': [3, 4, 5]\n#        'reg_alpha':[1,5,10,15],\n#        'reg_'lambda':[1,2,5]\n#        }\n#xgb = XGBClassifier(learning_rate=0.01, n_estimators=300,silent=True, nthread=1)\n#g_search_xgb = GridSearchCV(estimator = xgb, param_grid = params_xgb, scoring='recall',n_jobs=-1,cv=3)\n\n#g_search_xgb.fit(X_train_resampled,y_train_resampled)\n\n#print(\"XGBoost Best Score : \" ,g_search_xgb.best_score_)\n#print(\"XGBoost Best Params : \" ,g_search_xgb.best_params_)","e96660c1":"model_xgb = XGBClassifier(learning_rate=0.01, \n                          n_estimators=300, \n                          max_depth=4, \n                          min_child_weight=10, \n                          gamma=0.6,\n                          nthread=4, \n                          subsample=0.4,\n                          colsample_bytree=0.6,\n                          reg_alpha=10,\n                          reg_lambda=2)\nmodel_xgb.fit(X_train_resampled,y_train_resampled)","3e0e5e78":"# Checking the performance of the train dataset\ny_train_xgb = model_xgb.predict(X_train)\nprint(\"Evaluation on training data set: \\n\")\nclassification_algo_metrics(y_train,y_train_xgb)","b41a43f5":"#Checking the AUC ROC Curve\nplot_roc_curve(model_xgb, X_train_resampled,y_train_resampled)\nplt.show()","2049f444":"# Checking the performance on the test dataset\ny_test_xgb = model_xgb.predict(X_test)\n\nprint(\"Evaluation on test data set: \\n\")\n\naccuracy, precision, recall, f1, model_roc_auc = classification_algo_metrics(y_test,y_test_xgb)\n\ntempResults = pd.DataFrame({'Model':['XGBoost with SMOTEENN & Hyperparameter tuned'], \n                            'Accuracy': [accuracy],\n                            'Precision': [precision],\n                            'Recall': [recall], \n                            'F1 score': [f1], \n                            'Area under ROC curve': [model_roc_auc] })\n\nresults = pd.concat([results, tempResults])\nresults = results[['Model', 'Accuracy', 'Precision', 'Recall', 'F1 score', 'Area under ROC curve']]\nresults","38d669e5":"\n#params_lgb ={'num_leaves': [50,70,100], \n#         'max_depth' :[10,20,30],\n#         'learning_rate': [0.1,0.5,1],\n#         'n_estimators':[600]\n#         'min_child_samples': [20,50], \n#         'subsample': [0.1,0.3,0.5,1], \n#         'colsample_bytree': [0.1,0.5,1]\n#        }\n\n#lgb = LGBMClassifier(objective = 'binary', n_jobs = -1, random_state = 100)\n#g_search_lgb = GridSearchCV(estimator = lgb, param_grid = params_lgb, scoring='recall',cv=3)\n\n#g_search_lgb.fit(X_train_resampled,y_train_resampled)\n\n#print(\"LightGBM Best Score : \" ,g_search_lgb.best_score_)\n#print(\"LightGBM Best Params : \" ,g_search_lgb.best_params_)\n","0c78fac8":"model_lgbm = LGBMClassifier(boosting_type='gbdt',\n                            n_estimators=300,\n                            learning_rate=0.0008, \n                            max_depth=20, \n                            min_child_samples=100,\n                            num_leaves=50, \n                            objective='binary', \n                            random_state=100,\n                            subsample=1,\n                            colsample_bytree=0.3,\n                            n_jobs=-1,\n                            silent=True)\nmodel_lgbm.fit(X_train_resampled,y_train_resampled)","ac777ddc":"# Checking the performance of the train dataset\ny_train_lgbm = model_lgbm.predict(X_train)\nprint(\"Evaluation on training data set: \\n\")\nclassification_algo_metrics(y_train,y_train_lgbm)","b1ff9522":"#Checking the AUC ROC Curve\nplot_roc_curve(model_lgbm, X_train_resampled,y_train_resampled)\nplt.show()","d1128252":"# Checking the performance on the test dataset\ny_test_lgbm = model_lgbm.predict(X_test)\n\nprint(\"Evaluation on test data set: \\n\")\n\naccuracy, precision, recall, f1, model_roc_auc = classification_algo_metrics(y_test,y_test_lgbm)\n\ntempResults = pd.DataFrame({'Model':['Light GBM with SMOTEENN & Hyperparameter tuned'], \n                            'Accuracy': [accuracy],\n                            'Precision': [precision],\n                            'Recall': [recall], \n                            'F1 score': [f1], \n                            'Area under ROC curve': [model_roc_auc] })\n\nresults = pd.concat([results, tempResults])\nresults = results[['Model', 'Accuracy', 'Precision', 'Recall', 'F1 score', 'Area under ROC curve']]\nresults","f5663a77":"# Rearranging the dataframe \nresults = results.sort_values(\"Recall\", ascending = False).reset_index(drop=True)\nresults","438080a0":"feature_imp = pd.DataFrame(sorted(zip(model_lgbm.feature_importances_,X.columns)), columns=['Value','Feature'])\n\nplt.figure(figsize=(15, 10))\nsns.barplot(x=\"Value\", y=\"Feature\", data=feature_imp.sort_values(by=\"Value\", ascending=False))\nplt.title('LightGBM Features')\nplt.tight_layout()\nplt.show()","63191a30":"# Top 20 features : \nfeature_imp=feature_imp.sort_values(\"Value\",ascending = False)\nfeature_imp.head(20)","03f6fb4a":"<div class=\"alert alert-block alert-info\">\n    <span style='font-family:Georgia'>\n        <b>Insight: <\/b><br>\n         <ul>\n             <li>From the night_pck_user variables, it can be inferred that almost 99% of users are not using nightly pack in all the 3 months.<\/li>\n             <li>Similarly from the fb user variables, it can be inferred that almost 52 % of users are not using FB in all the 3 months.<\/li>\n             <li>82% of the users have no monthly_2g and monthly_3g packs in all the 3 months. Customers who have opted for the 1 month plan have almost not churned out, which is a good indicator. Also this count is decreasing across the months.<\/li>\n             <li>Around 70% of the Users have not opted for sachet_2g and sachet_3g packs across the months and this trend is also observed to be decreasing and these users have churned out a lot compared to the users who use these sachet packs.<\/li>\n        <\/ul>\n    <\/span>    \n<\/div>","17ee954d":"<div class=\"alert alert-block alert-info\">\n    <span style='font-family:Georgia'>\n        <b>Insight: <\/b><br>\n            In the class-weight method of RandomForest, the train dataset recall score is 1 where as in test data set it is 0.42. This signifies, that the model is over-trained. We need to conduct hyperparameter tuning to get better result. But before we do hyperparameter tuning, we will try another model with resampled data and without specifying class_weight.  \n    <\/span>    \n<\/div>","578bd4f5":"<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\" title=\"go to Colors\">Go to TOC<\/a>","79b185dd":"<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\" title=\"go to Colors\">Go to TOC<\/a>","76e44593":"<a id=\"setup\"><\/a>\n<h2>   \n      <font color = purple >\n            <span style='font-family:Georgia'>\n            2.3 Setting up Jupyter View\n            <\/span>   \n        <\/font>    \n<\/h2>","cfb84d08":"<div class=\"alert alert-block alert-info\">\n    <span style='font-family:Georgia'>\n        <b>Insight: <\/b><br>\n            This is significant improvement from other RF models or the SVM Model. Let's check if we can get better results with boosting models or not.  \n    <\/span>    \n<\/div>","8a7842cf":"<div class=\"alert alert-block alert-warning\">\n    <span style='font-family:Georgia'>\n        <b>Strategy: <\/b><br>\n        From the above analysis, we can see the maximum recharge value is 1555 and minimum recharge value is 1. We can consider that null value means the customer has not recharged that month. We will impute 0 for the null values for these three columns. \n    <\/span>\n<\/div>","267687de":"<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\" title=\"go to Colors\">Go to TOC<\/a>","c250d74d":"<a id=\"xgb\"><\/a>\n<h2>   \n      <font color = purple >\n            <span style='font-family:Georgia'>\n           7.5 XGBoost\n            <\/span>   \n        <\/font>    \n<\/h2>","9745a565":"<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\" title=\"go to Colors\">Go to TOC<\/a>","5c988c65":"<div>\n    <span style='font-family:Georgia'>\n       A customer having Mobile internet usage missing value would mostly mean that the customer is not using that particular service. This would also mean that the person would not be using any add-on services that would require a mobile internet pack.<br><br>\n        So with this inference we can impute the missing values related to columns for mobile data with 0.\n    <\/span>\n<\/div>","ea7bff30":"<a id=\"nullimpute\"><\/a>\n<h2>   \n      <font color = purple >\n            <span style='font-family:Georgia'>\n            4.2 Null Value Imputation\n            <\/span>   \n        <\/font>    \n<\/h2>","db536fb2":"<div class=\"alert alert-block alert-info\">\n    <span style='font-family:Georgia'>\n        <b>Insight: <\/b><br>\n         The above statistical description of the ARPU - Average Revenue Per User column gives some important insight. The MINIMUM value for all the arpu related columns are NEGATIVE. Now as per the definition of ARPU which says -<br><br>\n        \"The average revenue per user is the average billing per customer earned by the telecom company every month\".<br><br>\n        Now the revenue generated from a user cannot be negative. If a customer is not using any services then arpu for the person would be zero (rather that being negative). Now if arpu is negative for any row, then that would mean that is a wrong\/corrupt data. It will be of no use to us for analysis. We will drop such observations.<br><br>\n        Let's go ahead and see how many such observations we have for months 6, 7 & 8.\n    <\/span>    \n<\/div>","50667d03":"<h3>   \n      <font color = darkgreen>\n            <span style='font-family:Georgia'>\n            Understanding the Business Objective & Data :\n            <\/span>   \n        <\/font>    \n<\/h3>\n<div>\n    <span style='font-family:Georgia'>\n        The dataset contains customer-level information for a span of four consecutive months - June, July, August and September. The months are encoded as 6, 7, 8 and 9, respectively. The business objective is to predict the churn in the last (i.e. the ninth) month using the data (features) from the first three months. To do this task well, understanding the typical customer behaviour during churn will be helpful.\n    <\/span>\n<\/div>\n<hr>\n<h3>   \n      <font color = darkgreen>\n            <span style='font-family:Georgia'>\n            Understanding Customer Behavior during Churn :\n            <\/span>   \n        <\/font>    \n<\/h3>\n<div>\n    <span style='font-family:Georgia'>\n        Customers usually do not decide to switch to another competitor instantly, but rather over a period of time (this is especially applicable to high-value customers). In churn prediction, we assume that there are three phases of customer lifecycle :<br>\n        <ul>\n            <li><b><font color = 'blue'>The \u2018good\u2019 phase: <\/font><\/b> In this phase, the customer is happy with the service and behaves as usual.<\/li>\n            <li><b><font color = 'blue'>The \u2018action\u2019 phase:<\/font><\/b> The customer experience starts to sore in this phase, for e.g. he\/she gets a compelling offer from a  competitor, faces unjust charges, becomes unhappy with service quality etc. In this phase, the customer usually shows different behaviour than the \u2018good\u2019 months. Also, it is crucial to identify high-churn-risk customers in this phase, since some corrective actions can be taken at this point (such as matching the competitor\u2019s offer\/improving the service quality etc.)<\/li>\n            <li><b><font color = 'blue'>The \u2018churn\u2019 phase:<\/font><\/b> In this phase, the customer is said to have churned. We define churn based on this phase. Also, it is important to note that at the time of prediction (i.e. the action months), this data is not available to us for prediction. Thus, after tagging churn as 1\/0 based on this phase, we discard all data corresponding to this phase.<\/li>\n        <\/ul>\n        In this case, since we are working over a four-month window, the first two months are the \u2018good\u2019 phase, the third month is the \u2018action\u2019 phase, while the fourth month is the \u2018churn\u2019 phase.\n    <\/span>\n<\/div>","c0df2e8b":"<a id=\"logreg4\"><\/a>\n<h3>   \n      <font color = darkgreen >\n            <span style='font-family:Georgia'>\n           7.1.4 Logistic Regression with PCA and handling imbalance using random under sampling technique \n            <\/span>   \n        <\/font>    \n<\/h3>","5c1849d4":"<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\" title=\"go to Colors\">Go to TOC<\/a>","9aaa4485":"<a id=\"toc\"><\/a>\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h1 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\"  role=\"tab\" aria-controls=\"home\">\n    <span style='font-family:Georgia'>\n    Table of Content\n    <\/span>\n<\/h1>\n<span style='font-family:Georgia'>\n    <ul>\n        <li><a href='#intro'>1. Introduction<\/a><\/li>\n        <ul>\n            <li><a href='#background'>1.1 Problem Statement<\/a><\/li>\n            <li><a href='#goal'>1.2 Business Goal<\/a><\/li>\n        <\/ul>\n        <li><a href='#libraries'>2. Python Libraries<\/a><\/li>\n        <ul>\n            <li><a href='#warning'>2.1 Suppress Warnings<\/a><\/li>\n            <li><a href='#python'>2.2 Python Libraries<\/a><\/li>\n            <li><a href='#setup'>2.3 Setting up Jupyter View<\/a><\/li>   \n        <\/ul>\n        <li><a href='#import'>3. Reading & Understanding the data<\/a><\/li>\n        <ul>\n            <li><a href='#input'>3.1 Importing the input file<\/a><\/li>\n            <li><a href='#inspect'>3.2 Inspecting the dataframe<\/a><\/li>\n            <li><a href='#duplicate'>3.3 Duplicate Analysis<\/a><\/li>\n        <\/ul>\n         <li><a href='#dataclean'>4. Data Cleaning<\/a><\/li>\n        <ul>\n            <li><a href='#nullcal'>4.1 Null Value Calculation <\/a><\/li>\n            <li><a href='#nullimpute'>4.2 Null Value Imputation <\/a><\/li>\n            <li><a href='#highcust'>4.3 High Value Customer <\/a><\/li>\n            <li><a href='#churncal'>4.4 Customer Churn Calculation <\/a><\/li>\n            <li><a href='#highcustnull'>4.5 Null Imputation for High Value Customer Data Frame <\/a><\/li>\n            <li><a href='#delcol'>4.6 Delete Unnecessary Columns <\/a><\/li>\n            <li><a href='#catcol'>4.7 Categorical Columns Conversion <\/a><\/li>\n        <\/ul>\n        <li><a href='#eda'>5. Exploratory Data Analysis <\/a><\/li>\n        <ul>\n            <li><a href='#imbalance'>5.1 Imbalance Analysis <\/a><\/li>\n            <li><a href='#catcoleda'>5.2 Categorical Data Analysis <\/a><\/li>\n            <li><a href='#nulcoleda'>5.3 Numerical Data Analysis <\/a><\/li>\n        <\/ul>\n        <li><a href='#dataprep'>6. Data Preparation <\/a><\/li>\n        <ul>\n            <li><a href='#outlier'>6.1 Outliers Treatment <\/a><\/li>\n            <li><a href='#featureeng'>6.2 Feature Engineering <\/a><\/li>\n            <li><a href='#split'>6.3 Train- Test Split <\/a><\/li>\n            <li><a href='#scaling'>6.4 Feature Scaling <\/a><\/li>\n            <li><a href='#imbalancetreat'>6.5 Data Imbalance Treatment <\/a><\/li>\n        <\/ul>\n        <li><a href='#modelbuild'>7. Model Building <\/a><\/li>\n        <ul>\n            <li><a href='#logreg'>7.1 Logistic Regression <\/a><\/li>\n            <ul>\n                <li><a href='#logreg1'>7.1.1 Logistic Regression without PCA and no imbalance technique <\/a><\/li>\n                <li><a href='#logreg2'>7.1.2 Logistic Regression without PCA and handling imbalance using SMOTE technique <\/a><\/li>\n                <li><a href='#logreg3'>7.1.3 Logistic Regression with PCA and handling imbalance using random over sampling technique <\/a><\/li>\n                <li><a href='#logreg4'>7.1.4 Logistic Regression with PCA and handling imbalance using random under sampling technique <\/a><\/li>\n                <li><a href='#logreg5'>7.1.5 Logistic Regression with PCA and handling imbalance using SMOTE technique <\/a><\/li>\n                <li><a href='#logreg6'>7.1.6 Logistic Regression with PCA and handling imbalance using SMOTEEN technique <\/a><\/li>\n                <li><a href='#logreg7'>7.1.7 Logistic Regression with PCA and handling imbalance using SMOTEEN technique with regularization penalty <\/a><\/li>\n            <\/ul>\n            <li><a href='#svm'>7.2 Support Vector Machine (SVM) <\/a><\/li>\n            <li><a href='#rf'>7.3 Random Forest <\/a><\/li>\n            <li><a href='#ada'>7.4 AdaBoost <\/a><\/li>\n            <li><a href='#xgb'>7.5 XGBoost <\/a><\/li>\n            <li><a href='#lgbm'>7.6 LightGBM <\/a><\/li>\n        <\/ul>\n        <li><a href='#modelselect'>8. Final Model Selection <\/a><\/li>\n        <ul>\n            <li><a href='#featureimp'>8.1 Feature Importance <\/a><\/li>\n            <li><a href='#businessreco'>8.2 Business Recommendation <\/a><\/li>\n        <\/ul>\n    <\/ul>\n<\/span>","f5cff2e9":"<h2>   \n      <font color = blue >\n            <span style='font-family:Georgia'>\n                <i>If you like the notebook, please upvote and leave your feedback in the comments' section !<\/i>\n            <\/span>   \n        <\/font>    \n<\/h2>","58b9d0c6":"<a id=\"inspect\"><\/a>\n<h2 name='libraries'>   \n      <font color = purple >\n            <span style='font-family:Georgia'>\n            3.2 Inspect Dataframe\n            <\/span>   \n        <\/font>    \n<\/h2>","1d4449a5":"<div class=\"alert alert-block alert-warning\">\n    <span style='font-family:Georgia'>\n        <b>Strategy: <\/b><br>\n            For the model building, we have to consider many factors. Our target for the model is to identify all high value customers who have potential to churn. For that we have to minimize False Negative. To achieve this, we need to build our models to maximize Recall value as much as possible. As the telecom company wants to retain their customer as well as the profit, we will not focus much on False Positive. i.e. we will trade Precision value (increase of False Positive) of the model to maximize Recall. <br><br>\n        For model building we will be using following different models : <br>\n        <ol>\n            <li> Logistic Regression <\/li>\n            <li> Support Vector Machine <\/li>\n            <li> Random Forest <\/li>\n            <li> AdaBoost <\/li>\n            <li> XGBoost <\/li>\n            <li> LightGBM <\/li>\n        <\/ol>\n        Now, as our data is imbalanced, we will use RandomUnderSampler, RandomOverSample, SMOTE and SMOTEENN methods for resampling of X_train and Y_train.<br>\n        From the correlation heatmap, we know that our data have multicolinearity. To select the best impactful features, we will use RFE. But RFE does not ensure that multicollinearity is not present among data, which makes the model interpretation incorrect, though the model prediction would be correct.<br>\n        In the next model for Logistic Regression, we will also use Principal Component Analysis or PCA for dimension reduction. <br>\n        As rest of the models are not that affected by multicolinearity, we will use original dataset with imbalance treatment using SMOTEENN and Hyper-Parameter Tuning using GridSearchCV method to derive the final models in each type. As our target is not to overtrain the model, we will use certain components in each  model to ensure that the model is conservative enough to avoid overtraining. \n    <\/span>    \n<\/div>","9f576425":"<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\" title=\"go to Colors\">Go to TOC<\/a>","3e86e8b6":"\n<div>\n    <span style='font-family:Georgia'> \n        <h3><font color = blue> PCA <\/font><\/h3> \n        <b> STEPS : <\/b><br>\n        <ol>\n            <li> Perform PCA on the whole training dataset to identify number of features <\/li>\n            <li> Create 25 Principal components covering 90% of the data.<\/li>\n            <li> Predict using the training data<\/li>\n            <li> Decide the probability cutoff to decide if a customer has churned or not<\/li>\n            <li> Evaluate the metrics on the training data prediction<\/li>\n            <li> Predict using the test data and the decided cutoff <\/li>\n            <li> Evaluate the metrics on the test data prediction<\/li>\n        <\/ol>\n    <\/span>\n<\/div>","c29863b7":"<div class=\"alert alert-block alert-info\">\n    <span style='font-family:Georgia'>\n        <b>Insight: <\/b><br>\n          This plot shows that as the user has been with the network for more period, chances of churn is less and this could be focussed as one of the parameters to retain the customers. Lets check the other numerical data distribution. This is also shown in the boxplot where 25th to 75th percentile of churn customers have lower value than non-churn customers. \n    <\/span>    \n<\/div>","a2c7bbbe":"<a id=\"imbalancetreat\"><\/a>\n<h2>   \n      <font color = purple >\n            <span style='font-family:Georgia'>\n           6.5 Data Imbalance Treatment\n            <\/span>   \n        <\/font>    \n<\/h2>","56ecbf8b":"<div class=\"alert alert-block alert-info\">\n    <span style='font-family:Georgia'>\n        <b>Insight: <\/b><br>\n         The data types of the columns are float, integer and object type. We have 179 columsn with float values, 35 columns with integer values and 12 columns with Object Values. Some of the columns are actually Date, they need to be converted to Date Type format. We can see there are some null values in the column. Let's inspect the null values first \n    <\/span>    \n<\/div>","97013412":"<div>\n    <span style='font-family:Georgia'>\n    Based on top 20 features , 14 out of 20 features are from action phase, where as rest 6 features are from good phase. We can propose following business decisision which will help retain the high value customer who may be planning to leave : <br>\n        <ul>\n            <li><b><font color = 'blue'> Incoming Calls : <\/font><\/b> Total incoming minutes, local & std incoming minutes are in top 10 features. It shows that more incoming calls have high impact on retaining the customer. Incoming calls (local & std) should be made free for all customer <\/li>\n            <li><b><font color = 'blue'> Total Recharge Number : <\/font><\/b> Total Recharge Number (total_rech_num) is 4th highest coefficient. It shows that customers with high frequency of recharge numbers tends to stay. The company should launch small value top-up option for calls. Many times customer may hesitate to top up (prepaid option) with higher value amount. For these customer retention, small value top-up will be ideal. The similar logic is applicable for Total Recharge Number for data (total_rech_data) which has the 6th largest coefficient. Similar small top-up values for data might be useful. Also a combination of calling & data which is cost efficient might attract customers to stay with the telecom company <\/li>\n            <li><b><font color = 'blue'> Outgoing Calls : <\/font><\/b> Local, STD and Total outgoing call minutes are among the top 20 feature. It shows that higher the outgoing calls, the better chances of the customer to stay with the telecom company. To ensure that, telecom company should launch various discounted offer on outgoing calls. Lower price\/minute or free outgoing calls to same telecom company provided connections will also benefit in retaining customer group. <\/li>\n            <li><b><font color = 'blue'> Roaming : <\/font><\/b> Incoming and Outgoing calls during roaming are critical factor as these are part of top 20 coefficients. Incoming call roaming charges should be made free within the country. STD outgoing call should be same and shouldn't attract extra cost if the roaming option is within the country. For outside country incoming and outgoing roaming as well as data connection, company should launch special country wise roaming packages valid for 7 days, 15 days, 1 month etc. These benefits will attract customer to stay with the telecom company for longer periof<\/li>\n            <li><b><font color = 'blue'> Total Amount Recharge : <\/font><\/b> Total Amount Recharge on calls and data also have high coefficent value. Telecom company should launch discounted price for higher \/longer terms (days\/month etc) recharge options on calls and data. This may ensure that the customer is ready to available long terms plans which also indicates that the customer may tend to stay longer. <\/li>\n            <li> Introduce more services to the existing customer or informing them and generating more average revenue would lessen their chances to churn out. <\/li>\n        <\/ul>\n        In general, when the above features have a decline trend from the good phase to the action phase, then the customer care executives can reach them out and try to understand if there is any issues. This step would involve an additional smaller cost but this would help to prevent the high value customer to churn out, who generates average revenue per month atleast more than Rs. 240.75. <br>\n        If the goal is to engage and talk to the customers to prevent them from churning, its ok to engage with those who are mistakenly tagged as 'not churned,' ( False Positives) as it does not cause any negative problem. It could potentially make them even happier for the extra attention they are getting.\n    <\/span>\n<\/div>","59fc1a8c":"<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\" title=\"go to Colors\">Go to TOC<\/a>","1dccfc44":"<a id=\"input\"><\/a>\n<h2 name='libraries'>   \n      <font color = purple >\n            <span style='font-family:Georgia'>\n            3.1 Importing the Input file\n            <\/span>   \n        <\/font>    \n<\/h2>","5f0df09f":"<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\" title=\"go to Colors\">Go to TOC<\/a>","7ba2d7dc":"<div class=\"alert alert-block alert-warning\">\n    <span style='font-family:Georgia'>\n        <b>Strategy:<\/b><br>\n        <ul>\n            <li>There are 40 columns which have more than 70% null values. Ideally we can remove these columns from calculation as imputing values with such high null values will not result in accurate predictor values. But, we need to first analyze the types of columns that have more than 70% null values.<\/li>\n            <li>Some of the columns like night_pack_X or fb_user_X (where X signified the month value 6\/7\/8\/9) have high null values. In these cases, we can consider that the particular customer did not take those packages or they did not opt for social media utilities. For such columns, we can impute null values with 0.<\/li>\n            <li>We also have to calculate the <b> High Value Customer <\/b> where as High value customers can be considered as those who have <b>recharged with an amount more than or equal to X, where X is the 70th percentile of the average recharge amount in the first two months<\/b>, which signifies the good phase.<\/li> \n        <\/ul>\n    <\/span>\n    <br>\n    <span style='font-family:Georgia'>\n        Thus, we have to first treat the columns related to recharge amount to identify high value customer. <br>\n        <ul>\n            <li>total_rech_data_6        - 74.85 % missing values <\/li>\n            <li>total_rech_data_7        - 74.43 % missing values <\/li>\n            <li>av_rech_amt_data_6       - 74.85 % missing values <\/li>\n            <li>av_rech_amt_data_7       - 74.43 % missing values <\/li>\n            <li>date_of_last_rech_data_6 - 74.85 % missing values <\/li>\n            <li>date_of_last_rech_data_7 - 74.43 % missing values <\/li>\n        <\/ul>\n    <\/span>\n<\/div>","78d55c3d":"<div class=\"alert alert-block alert-info\">\n    <span style='font-family:Georgia'>\n        <b>Insight: <\/b><br>\n         We can clearly see that out of the first 5 high value customer row data, first two customer stopped using all calls and data and thus are tagged as churn. The 3rd and 4th customer did not have data but had incoming and outgoing calls. Thus they are marked as 0 which means not churned. The 5th customer had data as well as calls, thus it is also rightly marked as not churned or 0 <br>\n        We have successfully identified which customers are churning on 9th month. \n    <\/span>    \n<\/div>","c7efd212":"<a id=\"logreg\"><\/a>\n<h2>   \n      <font color = purple >\n            <span style='font-family:Georgia'>\n           7.1 Logistic Regression\n            <\/span>   \n        <\/font>    \n<\/h2>","274fa755":"<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\" title=\"go to Colors\">Go to TOC<\/a>","5920b2bc":"<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\" title=\"go to Colors\">Go to TOC<\/a>","90ca005c":"<div class=\"alert alert-block alert-info\">\n    <span style='font-family:Georgia'>\n        <b>Insight: <\/b><br>\n         We have successfully imputed all the null values in all the columns. Next let's analyze columns which have only 1 value or 0 variance. We can drop these columns as they will not have any predictive power\n    <\/span>    \n<\/div>","000e1931":"<a id=\"python\"><\/a>\n<h2>   \n      <font color = purple >\n            <span style='font-family:Georgia'>\n            2.2 Import Python Libraries:\n            <\/span>   \n        <\/font>    \n<\/h2>","8d92f932":"<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\" title=\"go to Colors\">Go to TOC<\/a>","eee82a8f":"<a id=\"split\"><\/a>\n<h2>   \n      <font color = purple >\n            <span style='font-family:Georgia'>\n           6.3 Train- Test Split\n            <\/span>   \n        <\/font>    \n<\/h2>","3347ae37":"<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\" title=\"go to Colors\">Go to TOC<\/a>","43606953":"<div class=\"alert alert-block alert-info\">\n    <span style='font-family:Georgia'>\n        <b>Insight: <\/b><br>\n            There is no correlation between any two components which is good for proceeding \n    <\/span>    \n<\/div>","a830ab0a":"<div class=\"alert alert-block alert-warning\">\n    <span style='font-family:Georgia'>\n        <b>Strategy: <\/b><br>\n         Now we need to identify churn customers (churn=1, else 0) based on the fourth month (_9) data. Customers who have not made any calls (either incoming or outgoing) AND have not used mobile internet even once in the churn phase can be considered as churned customer. The attributes we need to use to tag churners are : br>\n        <ul>\n            <li>total_ic_mou_9<\/li>\n            <li>total_og_mou_9<\/li>\n            <li>vol_2g_mb_9<\/li>\n            <li>vol_3g_mb_9<\/li>\n        <\/ul>\n        So any customer who does not use any of the facility (calls or mobile data) during the 9th month, will have the row sum for the attributes equal to zero. This customer can be tagged as Churn (1) else the customer will be tagged as Not Churn (0).\n    <\/span>    \n<\/div>","7299c4c7":"<div>\n    <span style='font-family:Georgia'>\n        So we see that the two indexes object are equal and we can safely conclude that no data recharge was done for month 7 and 8 & the missing values can be imputed with 0.<br><br>Also as the total data recharge for the month is 0, we can impute 0 for each of 'total_rech_data_7', av_rech_amt_data_7, 'total_rech_data_8' & 'av_rech_amt_data_8' columns as well.\n    <\/span>\n<\/div>\n    ","b3753cde":"<div class=\"alert alert-block alert-warning\">\n    <span style='font-family:Georgia'>\n        <b>Strategy: <\/b><br>\n        Looking at the problem statement, attributes total_ic_mou_9, total_og_mou_9, vol_2g_mb_9 and vol_3g_mb_9 are used to tag churners. So, it is clearly evident from the problem statement that the individual incoming and outgoing attributes are not used for data analysis. Dropping the individual columns (whose totals are already available like incoming, outgoing, arpu, etc) can help us in better analysis. Also, dropping these individual columns will help in removing the multicollinearity. \n        We will also delete the mobile_number column as it will not be needed for prediction purpose. \n    <\/span>\n<\/div>","4373ae21":"<a id=\"logreg2\"><\/a>\n<h3>   \n      <font color = darkgreen >\n            <span style='font-family:Georgia'>\n           7.1.2 Logistic Regression without PCA and handling imbalance using SMOTE technique\n            <\/span>   \n        <\/font>    \n<\/h3>","4ece3a8d":"<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\" title=\"go to Colors\">Go to TOC<\/a>","30c57279":"<a id=\"logreg6\"><\/a>\n<h3>   \n      <font color = darkgreen >\n            <span style='font-family:Georgia'>\n           7.1.6 Logistic Regression with PCA and handling imbalance using SMOTEEN technique \n            <\/span>   \n        <\/font>    \n<\/h3>","b9636e47":"<a id=\"logreg3\"><\/a>\n<h3>   \n      <font color = darkgreen >\n            <span style='font-family:Georgia'>\n           7.1.3 Logistic Regression with PCA and handling imbalance using random over sampling technique\n            <\/span>   \n        <\/font>    \n<\/h3>","c6bbad51":"<div class=\"alert alert-block alert-info\">\n    <span style='font-family:Georgia'>\n        <b>Insight: <\/b><br>\n            From the above plots we can see that top 10 importance features are based on action phase mostly (9 out of 10). Top impacting features are total incoming minute, maximum recharge amount, total local,std incoming minute, total & local outgoing minutes, total recharge amount for data  etc for the action phase or month 8.\n    <\/span>    \n<\/div>","c8c747f8":"<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\" title=\"go to Colors\">Go to TOC<\/a>","cd88e319":"There is no correlation between any two components which is good for proceeding","b8b3f873":"<a id=\"dataclean\"><\/a>\n<h1>   \n      <font color = blue >\n            <span style='font-family:Georgia'>\n            4. Data Cleaning\n            <\/span>   \n        <\/font>    \n<\/h1>","318d6d63":"<div class=\"alert alert-block alert-info\">\n    <span style='font-family:Georgia'>\n        <b>Insight: <\/b><br>\n            Looks like less than 25 components are enough to describe 90% of the variance in the dataset. Let's choose 25 components for modeling. \n    <\/span>    \n<\/div>","ead3922d":"<a id=\"featureeng\"><\/a>\n<h2>   \n      <font color = purple >\n            <span style='font-family:Georgia'>\n           6.2 Feature Engineering\n            <\/span>   \n        <\/font>    \n<\/h2>","a368f8e2":"<div class=\"alert alert-block alert-warning\">\n    <span style='font-family:Georgia'>\n        <b>Strategy: <\/b><br>\n         Next, we need to remove the data for the 9th month as they will not be used in prediction purpose as these values are not available for the model. They are only considered to calculate whether customer has churned or not. \n    <\/span>\n<\/div>","78d9ba3a":"<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\" title=\"go to Colors\">Go to TOC<\/a>","571bd5d5":"<a id=\"catcoleda\"><\/a>\n<h2>   \n      <font color = purple >\n            <span style='font-family:Georgia'>\n            5.2 Categorical Data Analysis\n            <\/span>   \n        <\/font>    \n<\/h2>","76553f9d":"<a id=\"featureimp\"><\/a>\n<h2>   \n      <font color = blue >\n            <span style='font-family:Georgia'>\n            8.1 Feature Importance\n            <\/span>   \n        <\/font>    \n<\/h2>","c8d80d8e":"<div>\n    <span style='font-family:Georgia'>\n      First we will segregate categorical and numerical columns. The categorical columns in this data frames are ordinal categorical columns. We will convert all these columns to integer type.  \n    <\/span>\n<\/div>","239c0bbb":"<div class=\"alert alert-block alert-warning\">\n    <span style='font-family:Georgia'>\n        <b>Strategy: <\/b><br>\n         We can follow the same method for confirmation and data imputation for month 7th and 8th. \n    <\/span>    \n<\/div>","14dfc75c":"<div class=\"alert alert-block alert-warning\">\n    <span style='font-family:Georgia'>\n        <b>Strategy: <\/b><br>\n        Some of the columns have binary data - 0\/1. We need to identify them and convert them as Ordered Categorical Column.\n    <\/span>\n<\/div>","a3fd1bae":"<a id=\"imbalance\"><\/a>\n<h2>   \n      <font color = purple >\n            <span style='font-family:Georgia'>\n            5.1 Imbalance Analysis\n            <\/span>   \n        <\/font>    \n<\/h2>\n<div>\n    <span style='font-family:Georgia'>\n      We need to check the imbalance in Customer Churn Data. If the data is highly imbalanced, we need to use proper methods to negate the effect of imbalance in our prediction models. \n    <\/span>\n<\/div>","c42afe4b":"<a id=\"catcol\"><\/a>\n<h2>   \n      <font color = purple >\n            <span style='font-family:Georgia'>\n            4.7 Categorical Columns Conversion\n            <\/span>   \n        <\/font>    \n<\/h2>","d183fb50":"<a id=\"nullcal\"><\/a>\n<h2>   \n      <font color = purple >\n            <span style='font-family:Georgia'>\n            4.1 Null Value Calculation\n            <\/span>   \n        <\/font>    \n<\/h2>","77ef6455":"<div>\n    <span style='font-family:Georgia'>\n      Next we will review 'max_rech_data_6', 'max_rech_data_7' & 'max_rech_data_8'\n    <\/span>\n<\/div>","70045908":"<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\" title=\"go to Colors\">Go to TOC<\/a>","02ab9668":"<div class=\"alert alert-block alert-info\">\n    <span style='font-family:Georgia'>\n        <b>Insight: <\/b><br>\n          <ul>\n              <li><b>arpu:<\/b> People who have churned out seem to have average revenue per user as zero in all the 3 months.<\/li>\n              <li><b>ONNET:<\/b> All kind of calls within the same operator network - minutes of voice calls Calls within same operator is very less for churned customers<\/li>\n              <li><b>OFFNET:<\/b> All kind of calls outside the operator T network - minutes of voice calls This seems to be higher than ONNET calls.<\/li>\n              <li> Roaming incoming and outgoing calls have been higher indicating that the customers could have churned due to the location change. Similarly local incoming and outgoing calls are less.<\/li>\n              <li> Special incoming calls is high in the 8th month for the churned customers.<\/li>\n              <li> total_rech_amt is getting less with each month.<\/li>\n              <li>3g usage has been increasing among churned customers, hence increasing better 3g connectivity might help to reduce the churn<\/li>\n              <li> Data is not normally distributed<\/li>\n        <\/ul>\n    <\/span>    \n<\/div>","413264c2":"<div>\n    <span style='font-family:Georgia'>\n       Now we will target some of the columns like arpu_3g_X, night_pck_use_Xm, fb_user_X columns for null value imputation. Based on the data description, we can say that if the data is missing, we can consider the customer has not opted for these facilities. We can impute null values as 0 here. But before that we need to verify if our assumption is correct or not. We will check if the missing indices are same for the following variables or not : <br>\n        <ul>\n            <li>count_rech_2g_X<\/li>\n            <li>count_rech_3g_X<\/li>\n            <li>arpu_2g_X<\/li>\n            <li>arpu_3g_X<\/li>\n            <li>night_pck_user_X<\/li>\n            <li>fb_user_X<\/li>\n        <\/ul>\n    <\/span>\n<\/div>","14a8a309":"<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\" title=\"go to Colors\">Go to TOC<\/a>","f95ebc06":"<div>\n    <span style='font-family:Georgia'>\n      We have deleted 1412 rows of data which 1.4% of overall data. This will not impact much on our analysis. Now let's try to identify high value customers next. \n    <\/span>e\n<\/div>","18e97803":"<a id=\"import\"><\/a>\n<h1>   \n      <font color = blue >\n            <span style='font-family:Georgia'>\n            3. Reading & Understanding the data\n            <\/span>   \n        <\/font>    \n<\/h1>","4d1b3d6c":"<a id=\"logreg7\"><\/a>\n<h3>   \n      <font color = darkgreen >\n            <span style='font-family:Georgia'>\n           7.1.7 Logistic Regression with PCA and handling imbalance using SMOTEEN technique with regularization penalty \n            <\/span>   \n        <\/font>    \n<\/h3>","7f04ef47":"<div class=\"alert alert-block alert-info\">\n    <span style='font-family:Georgia'>\n        <b>Insight: <\/b><br>\n         So we see that the two indexes are equal and we can safely conclude that no data recharge was done for that month and the 'total_rech_data_6' missing values can be imputed with 0. Also as the total data recharge for the month is 0, we can impute 0 for 'av_rech_amt_data_6' column as well.es.\n    <\/span>    \n<\/div>","ec47ccbe":"<a id=\"background\"><\/a>\n<h2>   \n      <font color = purple >\n            <span style='font-family:Georgia'>\n            1.1 Problem Statement\n            <\/span>   \n        <\/font>    \n<\/h2>","2e586061":"<a id=\"numcoleda\"><\/a>\n<h2>   \n      <font color = purple >\n            <span style='font-family:Georgia'>\n            5.3 Numerical Data Analysis\n            <\/span>   \n        <\/font>    \n<\/h2>","2a66866a":"<a id=\"highcust\"><\/a>\n<h2>   \n      <font color = purple >\n            <span style='font-family:Georgia'>\n            4.3 High Value Customer\n            <\/span>   \n        <\/font>    \n<\/h2>","03434ba3":"<a id=\"goal\"><\/a>\n<h2>   \n      <font color = purple >\n            <span style='font-family:Georgia'>\n            1.2 Business Goal\n            <\/span>   \n        <\/font>    \n<\/h2>","7044d79f":"<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\" title=\"go to Colors\">Go to TOC<\/a>","9f494d35":"<div>\n    <span style='font-family:Georgia'>\n        <b> STEPS : <\/b><br>\n        <ol>\n            <li> Run RFE and get 25 variables. <\/li>\n            <li>Build logistic regression model using features on the training dataset without handling imbalance.<\/li>\n            <li> Predict using the training data<\/li>\n            <li> Decide the probability cutoff to decide if a customer has churned or not<\/li>\n            <li> Evaluate the metrics on the training data prediction<\/li>\n            <li> Predict using the test data and the decided cutoff <\/li>\n            <li> Evaluate the metrics on the test data prediction<\/li>\n        <\/ol>\n    <\/span>\n<\/div>","d653d3ef":"<a id=\"outlier\"><\/a>\n<h2>   \n      <font color = purple >\n            <span style='font-family:Georgia'>\n           6.1 Outliers Treatment\n            <\/span>   \n        <\/font>    \n<\/h2>","242336be":"<div class=\"alert alert-block alert-info\">\n    <span style='font-family:Georgia'>\n        <b>Insight: <\/b><br>\n         We have successfully deleted unnecessary columns. Now, we can proceed to do exploratory data analysis. \n    <\/span>    \n<\/div>","1e01a87a":"<div>\n    <span style='font-family:Georgia'>\n        Random forest is an extension of bagging that also randomly selects subsets of features used in each data sample. Both bagging and random forests have proven effective on a wide range of different predictive modeling problems.Although effective, they are not suited to classification problems with a skewed class distribution. We will two approach with RandomForest. First method is by using class_weight based on imbalance data. Second is to use resampled data using SMOTEENN technique. \n    <\/span>\n<\/div>","657f0715":"<div class=\"alert alert-block alert-info\">\n    <span style='font-family:Georgia'>\n        <b>Insight: <\/b><br>\n         We have treated 29 columns with high null values(>70%) till now. \n    <\/span>    \n<\/div>","0257c547":"<a id=\"ada\"><\/a>\n<h2>   \n      <font color = purple >\n            <span style='font-family:Georgia'>\n           7.4 AdaBoost\n            <\/span>   \n        <\/font>    \n<\/h2>","e2f8a088":"![](https:\/\/www.jvstoronto.org\/wp-content\/uploads\/2017\/05\/dreamstime_xl_68665429-1.jpg)","884fd39f":"<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\" title=\"go to Colors\">Go to TOC<\/a>","24b4fc67":"<div class=\"alert alert-block alert-info\">\n    <span style='font-family:Georgia'>\n        <b>Insight: <\/b><br>\n         Data is highly imbalanced. We need to use Random Over Sampling or Random Under Sampling or SMOTE technique for data sampling before building the model. \n    <\/span>    \n<\/div>","11059a0e":"<div class=\"alert alert-block alert-warning\">\n    <span style='font-family:Georgia'>\n        <b>Note: <\/b><br>\n            For SVM, RandomForest, Adaboost, XGBoost and LightGBM Model, we have performed hyperparameter tuning using GridSearchCV. Due to high amount of time taken to run the tuning, we have commented those codes out. The best parameters are hardcoded after the hyperparameter tuning portion. Please uncomment and run the functions if you want to generate results of best parameters from hyperparameter tuning process.   \n    <\/span>    \n<\/div>","1e5db2ab":"<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\" title=\"go to Colors\">Go to TOC<\/a>","9bfa5425":"<a id=\"svm\"><\/a>\n<h2>   \n      <font color = purple >\n            <span style='font-family:Georgia'>\n           7.2 Support Vector Machine (SVM)\n            <\/span>   \n        <\/font>    \n<\/h2>","b9f12641":"<div>\n    <span style='font-family:Georgia'>\nLooks like less than 25 components are enough to describe 90% of the variance in the dataset. Let's choose 25 components for modeling.\n    <\/span>\n<\/div>\n    ","d82ea0e5":"<a id=\"rf\"><\/a>\n<h2>   \n      <font color = purple >\n            <span style='font-family:Georgia'>\n           7.3 Random Forest\n            <\/span>   \n        <\/font>    \n<\/h2>","048d6d7b":"<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\" title=\"go to Colors\">Go to TOC<\/a>","944ed139":"<div>\n    <span style='font-family:Georgia'>\n       The imbalance data treatment is necessary to increase the precision and recall factors of the model. The following algorithms will be used for treating the imbalanced data set. \n        <ol>\n            <li><font color ='blue'> Under Sampling :<\/font> <\/li>\n            <ul>\n                <li><b><font color ='green'> Random Under Sampling :<\/font><\/b> involved randomly selecting examples from the majority class and deleting them from the training dataset.<\/li>\n            <\/ul>\n            <li><font color ='blue'> Over Sampling :<\/font> <\/li>\n            <ul>\n                <li><b><font color ='green'> Random Over Sampling :<\/font><\/b>  generates new samples by random resampling with replacement of under represented class<\/li>\n                <li><b><font color ='green'> Synthetic Minority Oversampling (SMOTE):<\/font><\/b> works by selecting examples that are close in the feature space, drawing a line between the examples in the feature space and drawing a new sample at a point along that line.<\/li>\n            <\/ul>\n            <li><font color ='blue'> Combined Under & Over Sampling :<\/font> <\/li>\n            <ul>\n                <li><b><font color ='green'> SMOTEENN :<\/font><\/b>  SMOTE + Edited NearestNeighbors <\/li>\n                <li><b><font color ='green'> SMOTETomek:<\/font><\/b> SMOTE + Tomek Links<\/li>\n            <\/ul>\n        <\/ol>\n    <\/span>\n    <br>\n    <span style='font-family:Georgia'>\n        SMOTE allows to generate samples. However, this method of over-sampling does not have any knowledge regarding the underlying distribution. Therefore, some noisy samples can be generated, e.g. when the different classes cannot be well separated. Hence, it can be beneficial to apply an under-sampling algorithm to clean the noisy samples. Imbalanced-learn provides two ready-to-use combined samplers: SMOTEENN and SMOTETomek. <br><br>\n        SMOTEENN is more aggressive at downsampling the majority class than Tomek Links, providing more in-depth cleaning. They apply the method, removing examples from both the majority and minority classes. <br><br>\n        It is not possible to check different sampling techniques on very cost sensitive Machine Learning models like SVM, Decision Trees, Random Forest. For this Case Study, we will particularily use SMOTEENN sampling technique to handle imbalanced dataset as it is uses both over-sampling and under-sampling method and helps in cleaning noisy samples.\n    <\/span>\n<\/div>","453a0527":"<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\" title=\"go to Colors\">Go to TOC<\/a>","bf1e1b8a":"<h3>   \n      <font color = darkgreen>\n            <span style='font-family:Georgia'>\n            Business Problem Overview :\n            <\/span>   \n        <\/font>    \n<\/h3>\n<div>\n    <span style='font-family:Georgia'>\n        In the telecom industry, customers are able to choose from multiple service providers and actively switch from one operator to another. In this highly competitive market, the telecommunications industry experiences an average of 15-25% annual churn rate. Given the fact that it costs 5-10 times more to acquire a new customer than to retain an existing one, <b>customer retention<\/b> has now become even more important than customer acquisition.<br>\n        For many incumbent operators, retaining high profitable customers is the number one business goal.To reduce customer churn, telecom companies need to predict which customers are at high risk of churn.<br>\n        In this project, we will analyse customer-level data of a leading telecom firm, build predictive models to identify customers at high risk of churn and identify the main indicators of churn.\n    <\/span>\n<\/div>\n<hr>\n<h3>   \n      <font color = darkgreen>\n            <span style='font-family:Georgia'>\n            Understanding & Defining Churn :\n            <\/span>   \n        <\/font>    \n<\/h3>\n<div>\n    <span style='font-family:Georgia'>\n        There are two main models of payment in the telecom industry - <b>Postpaid<\/b> (customers pay a monthly\/annual bill after using the services) and <b>Prepaid<\/b> (customers pay\/recharge with a certain amount in advance and then use the services).<br>\n        In the postpaid model, when customers want to switch to another operator, they usually inform the existing operator to terminate the services, and we directly know that this is an instance of churn.<br>\n        However, in the prepaid model, customers who want to switch to another network can simply stop using the services without any notice, and it is hard to know whether someone has actually churned or is simply not using the services temporarily (e.g. someone may be on a trip abroad for a month or two and then intend to resume using the services again).<br>\n        Thus, churn prediction is usually more critical (and non-trivial) for prepaid customers, and the term \u2018churn\u2019 should be defined carefully.  Also, prepaid is the most common model in India and southeast Asia, while postpaid is more common in Europe in North America.This project is based on the Indian and Southeast Asian market.\n    <\/span>\n<\/div>\n<hr>\n<h3>   \n      <font color = darkgreen>\n            <span style='font-family:Georgia'>\n            Definitions of Churn :\n            <\/span>   \n        <\/font>    \n<\/h3>\n<div>\n    <span style='font-family:Georgia'>\n        There are various ways to define churn, such as:<br>\n        <ol>\n            <li><b><font color = 'blue'>Revenue-based churn:<\/font><\/b> Customers who have not utilised any revenue-generating facilities such as mobile internet, outgoing calls, SMS etc. over a given period of time. One could also use aggregate metrics such as \u2018customers who have generated less than INR 4 per month in total\/average\/median revenue\u2019.<br><br>\n                The main shortcoming of this definition is that there are customers who only receive calls\/SMSes from their wage-earning counterparts, i.e. they don\u2019t generate revenue but use the services. For example, many users in rural areas only receive calls from their wage-earning siblings in urban areas.<\/li><br>\n            <li><b><font color = 'blue'>Usage-based churn:<\/font><\/b> Customers who have not done any usage, either incoming or outgoing - in terms of calls, internet etc. over a period of time.<br><br>\n                A potential shortcoming of this definition is that when the customer has stopped using the services for a while, it may be too late to take any corrective actions to retain them. For e.g., if you define churn based on a \u2018two-months zero usage\u2019 period, predicting churn could be useless since by that time the customer would have already switched to another operator.<\/li>\n        <\/ol>\n        In this project, we will use the usage-based definition to define churn.\n    <\/span>\n<\/div>\n<hr>\n<h3>   \n      <font color = darkgreen>\n            <span style='font-family:Georgia'>\n            High Value Churn :\n            <\/span>   \n        <\/font>    \n<\/h3>\n<div>\n    <span style='font-family:Georgia'>\n        In the Indian and the southeast Asian market, approximately 80% of revenue comes from the top 20% customers (called high-value customers). Thus, if we can reduce churn of the high-value customers, we will be able to reduce significant revenue leakage.<br>\n        In this project, we will define high-value customers based on a certain metric and predict churn only on high-value customers.\n    <\/span>\n<\/div>","c1c0e915":"<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\" title=\"go to Colors\">Go to TOC<\/a>","50eba9f8":"<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\" title=\"go to Colors\">Go to TOC<\/a>","8dbf9df5":"<div class=\"alert alert-block alert-info\">\n    <span style='font-family:Georgia'>\n        <b>Insight: <\/b><br>\n         All Mobile number data are unique and there are no duplicate values.\n    <\/span>    \n<\/div>","ceee3e0b":"<div>\n    <span style='font-family:Georgia'>\n        There are three date columns date_of_last_rech_6, date_of_last_rech_7 and date_of_last_rech_8.#It is not possible to impute these data. We will drop these columns as it will not be useful for prediction. Also another 3 date columns of last_date_of_month_X has same value in all row. These are not useful so we will drop them. \n    <\/span>\n<\/div>\n        ","d6655bf7":"<span style='font-family:Georgia'>\n    <font color = green>\n        <h3> Confusion Matrix : <\/h3>\n    <\/font>\n    <table>\n    <thead>\n    <tr><th><\/th><th>Predicted Negative(0)<\/th><th>Predicted Positive(1)<\/th><\/tr>\n    <\/thead>\n    <tbody>\n        <tr><td><b>Actual Negative(0)<\/b><\/td><td><font color = blue>True Negative (TN)<\/font> <\/td><td><font color = blue>False Postive (FP)<\/font><\/td><\/tr>\n        <tr><td><b>Actual Positive(1)<\/b><\/td><td><font color = blue>False Negative (FN)<\/font><\/td><td><font color = blue>True Positive (TP)<\/font><\/td><\/tr>\n    <\/tbody>\n<\/table>\n<\/span>\n<hr>\n<div>\n    <span style=\"font-size:18\" >\n        <span style ='font-family:Georgia'>\n            <font color = blue >\n                <math> Accuracy = <\/math>\n                <span style=\"display: inline-block;vertical-align: middle;\">\n                    <div style=\"text-align: center;border-bottom: 1px solid black;\">(TP + TN)<\/div>\n                    <div style=\"text-align: center;\">(TP + TN + FP + FN)<\/div>\n                <\/span>\n            <\/font>\n        <\/span>\n    <\/span>\n<\/div>\n<hr>\n<div>\n    <span style=\"font-size:18\" >\n        <span style ='font-family:Georgia'>\n            <font color = blue >\n                <math> Precision = <\/math>\n                <span style=\"display: inline-block;vertical-align: middle;\">\n                    <div style=\"text-align: center;border-bottom: 1px solid black;\">TP<\/div>\n                    <div style=\"text-align: center;\">(TP + FP)<\/div>\n                <\/span>\n            <\/font>\n        <\/span>\n    <\/span>\n<\/div>    \n<hr>\n<div>\n    <span style=\"font-size:18\" >\n        <span style ='font-family:Georgia'>\n            <font color = blue >\n                <math> Recall = <\/math>\n                <span style=\"display: inline-block;vertical-align: middle;\">\n                    <div style=\"text-align: center;border-bottom: 1px solid black;\">TP<\/div>\n                    <div style=\"text-align: center;\">(TP + FN)<\/div>\n                <\/span>\n            <\/font>\n        <\/span>\n    <\/span>\n<\/div>\n<hr>\n<div>\n    <span style=\"font-size:18\" >\n        <span style ='font-family:Georgia'>\n            <font color = blue >\n                <math> F Measure (F1) = 2 * <\/math>\n                <span style=\"display: inline-block;vertical-align: middle;\">\n                    <div style=\"text-align: center;border-bottom: 1px solid black;\"> Precision * Recall <\/div>\n                    <div style=\"text-align: center;\">(Precision + Recall)<\/div>\n                <\/span>\n            <\/font>\n        <\/span>\n    <\/span>\n<\/div>\n<hr>\n<div>\n    <span style=\"font-size:18\" >\n        <span style ='font-family:Georgia'>\n            <font color = blue >\n                <ul>\n                    <li><b>TPR (True Positive Rate)<\/b> = TP\/(TP + FN)<\/li>\n                    <li><b>TNR (True Negative Rate)<\/b> = TN\/(TN + FP)<\/li>\n                    <li><b>FPR (False Positive Rate)<\/b> = FP\/(TN + FP)<\/li>\n                    <li><b>FNR (False Negative Rate)<\/b> = FN\/(TP + FN)<\/li>\n                <\/ul>\n            <\/font>\n        <\/span>\n    <\/span>\n<\/div>    \n    ","ae2ce3bb":"<div>\n    <span style='font-family:Georgia'>\n      We need to predict churn only for the high-value customers. Define high-value customers as follows: Those who have recharged with an amount more than or equal to X, where X is the 70th percentile of the average recharge amount in the first two months which are the good phases.\n    <\/span>\n<\/div>","3916dcc0":"<div class=\"alert alert-block alert-warning\">\n    <span style='font-family:Georgia'>\n        <b>Strategy: <\/b><br>\n         Now we have values for 'total_rechdata' and 'av_rech_amtdata' (for months 6, 7, 8 & 9). Using these 2 values, we can derive new features for the respective months called total_data_rech_amt which equals total_rechdata * av_rech_amtdata <br>\n        Also this new feature would help us in computing the total data recharge amount : 'total_data_rech_amt' for the months 6, 7, 8 & 9. \n    <\/span>    \n<\/div>","819143b6":"<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\" title=\"go to Colors\">Go to TOC<\/a>","3c56829e":"<div class=\"alert alert-block alert-warning\">\n    <span style='font-family:Georgia'>\n        <b>Strategy: <\/b><br>\n            As our target is to reduce high value customer churn, it is necessary that we reduce the False Negative and not worry about False Positive. To reduce False Negative we will choose the model which provides highest Recall value. As we are not targeting to reduce False Positives, i.e. customers who are identified as potential churn but will not actually churn, then we will not focus on Precision and F1 Score.   \n    <\/span>    \n<\/div>","62da1d5b":"<div>\n    <span style='font-family:Georgia'>\n      There are 6 columns which have high null values. To impute data for these columns, we need to review each column individually and understand the data. First we will review 'date_of_last_rech_data_6', 'date_of_last_rech_data_7', 'date_of_last_rech_data_8' columns. These are date columns and it is not possible to impute the null values, thus we will drop the columns. \n    <\/span>\n<\/div>","a2b0db73":"<a id=\"businessreco\"><\/a>\n<h2>   \n      <font color = blue >\n            <span style='font-family:Georgia'>\n            8.2 Business Recommendation\n            <\/span>   \n        <\/font>    \n<\/h2>","82db64db":"<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\" title=\"go to Colors\">Go to TOC<\/a>","49373bf6":"<a id=\"duplicate\"><\/a>\n<h2>   \n      <font color = purple >\n            <span style='font-family:Georgia'>\n            3.3 Duplicate Analysis\n            <\/span>   \n        <\/font>    \n<\/h2>","d7ff8880":"<div class=\"alert alert-block alert-info\">\n    <span style='font-family:Georgia'>\n        <b>Insight: <\/b><br>\n         We have successfully identified 'High Value Customers' and created a separate dataframe containing High Value Customers' data. \n    <\/span>    \n<\/div>","a7270b7a":"<div class=\"alert alert-block alert-warning\">\n    <span style='font-family:Georgia'>\n        <b>Strategy: <\/b><br>\n            Most of the columns have outliers. We will first check the skewness of the data and then perform box-cox transformation on the data to treat the outliers as much as possible. \n    <\/span>    \n<\/div>","ec6c63a7":"<a id=\"warning\"><\/a>\n<h2>   \n      <font color = purple >\n            <span style='font-family:Georgia'>\n            2.1 Suppress Warnings:\n            <\/span>   \n        <\/font>    \n<\/h2>","aab0e7ed":"<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\" title=\"go to Colors\">Go to TOC<\/a>","e22c9956":"<div class=\"alert alert-block alert-warning\">\n    <span style='font-family:Georgia'>\n        <b>Strategy: <\/b><br>\n         Before we start identifying high value customers, let's review if the revenue data has any data discripancies or not. \n    <\/span>\n<\/div>","208398e4":"<div class=\"alert alert-block alert-info\">\n    <span style='font-family:Georgia'>\n        <b>Insight: <\/b><br>\n            We have treated most of the numerical columns data for outlier now. Next Step is to conduct feature engineering. \n    <\/span>    \n<\/div>","d0a403d7":"<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\" title=\"go to Colors\">Go to TOC<\/a>","679bfdd2":"<div>\n    <span style='font-family:Georgia'>\n        Looks like less than 25 components are enough to describe 90% of the variance in the dataset. Let's choose 25 components for modeling.\n    <\/span>\n<\/div>","c99e26cf":"<a id=\"modelselect\"><\/a>\n<h1>   \n      <font color = blue >\n            <span style='font-family:Georgia'>\n            8. Final Model Selection\n            <\/span>   \n        <\/font>    \n<\/h1>","c66d94cf":"<div class=\"alert alert-block alert-info\">\n    <span style='font-family:Georgia'>\n        <b>Insight: <\/b><br>\n            With resampling, the model performance has slightly improved where test dataset recall is 0.75. But this is still not sufficient. Next we will do hyper parameter tuning and build a model with best parameters from GridSearchCV.  \n    <\/span>    \n<\/div>","c9bcfffc":"<a id=\"churncal\"><\/a>\n<h2>   \n      <font color = purple >\n            <span style='font-family:Georgia'>\n            4.4 Customer Churn Calculation\n            <\/span>   \n        <\/font>    \n<\/h2>","34ab8914":"<div>\n    <span style='font-family:Georgia'>\n        As we can see from the summary, PCA models have PCA components as features and so its not interpretable. We can skip printing this part in the further PCA models. \n    <\/span>\n<\/div>","c1380705":"<a id=\"logreg5\"><\/a>\n<h3>   \n      <font color = darkgreen >\n            <span style='font-family:Georgia'>\n           7.1.5 Logistic Regression with PCA and handling imbalance using SMOTE technique \n            <\/span>   \n        <\/font>    \n<\/h3>","2aa2891a":"<div class=\"alert alert-block alert-info\">\n    <span style='font-family:Georgia'>\n        <b>Insight: <\/b><br>\n         We have successfully imputed high null value columns. For the remaining 95 columns with null values, they contain MOU - Minutes of usage - voice call data. Now, a null value in these columns can be considered as 0, meaning the customer has not utilized these facilities. <br><br>\n        If we look into the columns properly, we can notice that the MOU null values for month 8th is significantly more than month 6th and 7th (almost doubled). This may signify that the customers are now starting to leave the telecom company.\n    <\/span>    \n<\/div>","957f169f":"<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\" title=\"go to Colors\">Go to TOC<\/a>","ff42cdbf":"<a id=\"eda\"><\/a>\n<h1>   \n      <font color = blue >\n            <span style='font-family:Georgia'>\n            5. Exploratory Data Analysis\n            <\/span>   \n        <\/font>    \n<\/h1>","aba53e50":"<a id=\"lgbm\"><\/a>\n<h2>   \n      <font color = purple >\n            <span style='font-family:Georgia'>\n           7.6 LightGBM\n            <\/span>   \n        <\/font>    \n<\/h2>","1205bc54":"<a id=\"dataprep\"><\/a>\n<h1>   \n      <font color = blue >\n            <span style='font-family:Georgia'>\n            6. Data Preparation\n            <\/span>   \n        <\/font>    \n<\/h1>","ad41c705":"<a id=\"delcol\"><\/a>\n<h2>   \n      <font color = purple >\n            <span style='font-family:Georgia'>\n            4.6 Delete Unnecessary Columns\n            <\/span>   \n        <\/font>    \n<\/h2>","630e9c82":"<a id=\"modelbuild\"><\/a>\n<h1>   \n      <font color = blue >\n            <span style='font-family:Georgia'>\n            7. Model Building\n            <\/span>   \n        <\/font>    \n<\/h1>","4e218ca0":"<a id=\"libraries\"><\/a>\n<h1>   \n      <font color = blue >\n            <span style='font-family:Georgia'>\n            2. Python Libraries:\n            <\/span>   \n        <\/font>    \n<\/h1>","a0b3b32d":"<h1 style=\"text-align:center\">   \n      <font color = purple >\n            <span style='font-family:Georgia'>\n                Telecom Churn - Indian & South East Asian Market : A Case Study \n            <\/span>   \n        <\/font>    \n<\/h1>\n<h3 style=\"text-align:right\">   \n      <font color = gray >\n            <span style='font-family:Georgia'>\n                By : Amrita Chatterjee & Padma A.\n            <\/span>   \n        <\/font>    \n<\/h3>\n<hr style=\"width:100%;height:5px;border-width:0;color:gray;background-color:gray\">\n<center><img src=\"https:\/\/kranthi.me\/wp-content\/uploads\/2020\/04\/Telecom_Churn_Prediction-e1587281300645.jpg\"><\/center>","133bccf4":"<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\" title=\"go to Colors\">Go to TOC<\/a>","9864491d":"<a id=\"intro\"><\/a>\n<h1>   \n      <font color = blue >\n            <span style='font-family:Georgia'>\n            1. Introduction:\n            <\/span>   \n        <\/font>    \n<\/h1>","2b1550c6":"<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\" title=\"go to Colors\">Go to TOC<\/a>","0169d74f":"<a id=\"highcustnull\"><\/a>\n<h2>   \n      <font color = purple >\n            <span style='font-family:Georgia'>\n            4.5 Null Imputation for High Value Customer Data Frame\n            <\/span>   \n        <\/font>    \n<\/h2>","1adf0cb5":"<a id=\"logreg1\"><\/a>\n<h3>   \n      <font color = darkgreen >\n            <span style='font-family:Georgia'>\n           7.1.1 Logistic Regression without PCA and no imbalance technique\n            <\/span>   \n        <\/font>    \n<\/h3>","cbab6020":"<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\" title=\"go to Colors\">Go to TOC<\/a>","242e3934":"<div class=\"alert alert-block alert-info\">\n    <span style='font-family:Georgia'>\n        <b>Insight: <\/b><br>\n           <ul>\n               <li> Most of the columns have outliers. <\/li>\n               <li> The data is not normally distributed. <\/li>\n               <li> The variance is very high among the columns. We can apply boxcox transformation <\/li>\n               <li> roam_ic_mou variables have higher value for churned customers than the non-churned customers indicating customers who travel or move to other places and receive more calls tend to churn out. std_og_mou variables also follow this pattern. <\/li>\n               <li> roam_og_mou variables do not follow this pattern indicating customers who travel and make the calls stay active in the network. <\/li>\n               <li> total_og variables indicate there is a steady decrease in the outgoing calls made by the churned customers. <\/li>\n               <li> spl_ic variables show that there has been a decrease in the special incoming calls in the 8th month which might have played a role for churn.<\/li>\n               <li> count_rech_2g and count_rech_3g variables show that there is a decrease during the 8th month <\/li>\n        <\/ul>\n    <\/span>    \n<\/div>","9c1a25fd":"<div class=\"alert alert-block alert-warning\">\n    <span style='font-family:Georgia'>\n        <b>Strategy: <\/b><br>\n            Month 6 and 7th are good phase. We will use a average value of Month 6 and 7 to determine goodphase values. This will also reduce the number of features for model building. \n    <\/span>    \n<\/div>","11996bcb":"<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\" title=\"go to Colors\">Go to TOC<\/a>","651228d5":"<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\" title=\"go to Colors\">Go to TOC<\/a>","e83b99ef":"<div class=\"alert alert-block alert-info\">\n    <span style='font-family:Georgia'>\n        <b>Insight: <\/b><br>\n            We have many high performance models which is providing satisfactory Recall value of the test dataset. Amongst them following models gave best results:<br>\n        <ul>\n            <li> Light GBM with SMOTEENN and Hyperparameter tuning<\/li>\n            <li> Logistic Regression with PCA & SMOTEENN <\/li>\n            <li> Logistic Regression with PCA, SMOTEENN & L2 Regularization <\/li> \n            <li> SVM with SMOTEENN <\/li> \n        <\/ul>\n        As Logistic Regression models are with PCA, we loose the interpretability of the features which impacts most in customer churn decision. We will use <b>Light GBM with SMOTEENN<\/b> as out final model to find feature importance. \n    <\/span>    \n<\/div>","9649f6ad":"<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\" title=\"go to Colors\">Go to TOC<\/a>","1b081ec1":"<a id=\"scaling\"><\/a>\n<h2>   \n      <font color = purple >\n            <span style='font-family:Georgia'>\n           6.4 Feature Scaling\n            <\/span>   \n        <\/font>    \n<\/h2>"}}