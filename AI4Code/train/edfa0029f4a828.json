{"cell_type":{"699d568a":"code","992a937d":"code","a1711112":"code","3530748c":"code","541aad44":"code","588b97d5":"code","020e883f":"code","b4ea21a4":"code","3e9dd3bb":"code","9dbe6c97":"code","ff2b9a90":"code","bc9763ed":"code","9eb69870":"code","60da9142":"code","f9a3f031":"markdown","15963781":"markdown","40119ed5":"markdown","6c6e366e":"markdown","32b8350b":"markdown","2c374dbe":"markdown","818f1a38":"markdown","545223dc":"markdown","ab0c6024":"markdown","549b2d77":"markdown"},"source":{"699d568a":"import numpy as np\nimport pandas as pd \nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","992a937d":"train = pd.read_csv('\/kaggle\/input\/cat-in-the-dat-ii\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/cat-in-the-dat-ii\/test.csv')\ntest.insert(1, 'target', 0)","a1711112":"features = [_f for _f in train if _f not in ['id', 'target']]\n\ndef factor_encoding(train, test):\n    \n    assert sorted(train.columns) == sorted(test.columns)\n    \n    full = pd.concat([train, test], axis=0, sort=False)\n    # Factorize everything\n    for f in full:\n        full[f], _ = pd.factorize(full[f])\n        full[f] += 1  # make sure no negative\n        \n    return full.iloc[:train.shape[0]], full.iloc[train.shape[0]:]\n\ntrain_f, test_f = factor_encoding(train[features], test[features])","3530748c":"class LibFFMEncoder(object):\n    def __init__(self):\n        self.encoder = 1\n        self.encoding = {}\n\n    def encode_for_libffm(self, row):\n        txt = f\"{row[0]}\"\n        for i, r in enumerate(row[1:]):\n            try:\n                txt += f' {i+1}:{self.encoding[(i, r)]}:1'\n            except KeyError:\n                self.encoding[(i, r)] = self.encoder\n                self.encoder += 1\n                txt += f' {i+1}:{self.encoding[(i, r)]}:1'\n\n        return txt\n\n# Create files for testing and OOF\nfrom sklearn.model_selection import KFold\nfold_ids = [\n    [trn_, val_] for (trn_, val_) in KFold(5,True,1).split(train)\n]\nfor fold_, (trn_, val_) in enumerate(fold_ids):\n    # Fit the encoder\n    encoder = LibFFMEncoder()\n    libffm_format_trn = pd.concat([train['target'].iloc[trn_], train_f.iloc[trn_]], axis=1).apply(\n        lambda row: encoder.encode_for_libffm(row), raw=True, axis=1\n    )\n    # Encode validation set\n    libffm_format_val = pd.concat([train['target'].iloc[val_], train_f.iloc[val_]], axis=1).apply(\n        lambda row: encoder.encode_for_libffm(row), raw=True, axis=1\n    )\n    \n    print(train['target'].iloc[trn_].shape, train['target'].iloc[val_].shape, libffm_format_val.shape)\n    \n    libffm_format_trn.to_csv(f'libffm_trn_fold_{fold_+1}.txt', index=False, header=False)\n    libffm_format_val.to_csv(f'libffm_val_fold_{fold_+1}.txt', index=False, header=False)\n    \n    \n# Create files for final model\nencoder = LibFFMEncoder()\nlibffm_format_trn = pd.concat([train['target'], train_f], axis=1).apply(\n        lambda row: encoder.encode_for_libffm(row), raw=True, axis=1\n)\nlibffm_format_tst = pd.concat([test['target'], test_f], axis=1).apply(\n    lambda row: encoder.encode_for_libffm(row), raw=True, axis=1\n)\n\nlibffm_format_trn.to_csv(f'libffm_trn.txt', index=False, header=False)\nlibffm_format_tst.to_csv(f'libffm_tst.txt', index=False, header=False)","541aad44":"!cp \/kaggle\/input\/libffm-binaries\/ffm-train .\n!cp \/kaggle\/input\/libffm-binaries\/ffm-predict .\n!chmod u+x ffm-train\n!chmod u+x ffm-predict","588b97d5":"from sklearn.metrics import log_loss, roc_auc_score\n\n!.\/ffm-train -p libffm_val_fold_1.txt -r 0.05 -l 0.00001 -k 50 -t 7 libffm_trn_fold_1.txt libffm_fold_1_model\n!.\/ffm-predict libffm_val_fold_1.txt libffm_fold_1_model val_preds_fold_1.txt\n(\n    log_loss(train['target'].iloc[fold_ids[0][1]], pd.read_csv('val_preds_fold_1.txt', header=None).values[:,0]),\n    roc_auc_score(train['target'].iloc[fold_ids[0][1]], pd.read_csv('val_preds_fold_1.txt', header=None).values[:,0])\n)","020e883f":"!.\/ffm-train -p libffm_val_fold_2.txt -r 0.05 -l 0.00001 -k 50 -t 7 libffm_trn_fold_2.txt libffm_fold_2_model\n!.\/ffm-predict libffm_val_fold_2.txt libffm_fold_2_model val_preds_fold_2.txt\n(\n    log_loss(train['target'].iloc[fold_ids[1][1]], pd.read_csv('val_preds_fold_2.txt', header=None).values[:,0]),\n    roc_auc_score(train['target'].iloc[fold_ids[1][1]], pd.read_csv('val_preds_fold_2.txt', header=None).values[:,0])\n)","b4ea21a4":"!.\/ffm-train -p libffm_val_fold_3.txt -r 0.05 -l 0.00001 -k 50 -t 7 libffm_trn_fold_3.txt libffm_fold_3_model\n!.\/ffm-predict libffm_val_fold_3.txt libffm_fold_3_model val_preds_fold_3.txt\n(\n    log_loss(train['target'].iloc[fold_ids[2][1]], pd.read_csv('val_preds_fold_3.txt', header=None).values[:,0]),\n    roc_auc_score(train['target'].iloc[fold_ids[2][1]], pd.read_csv('val_preds_fold_3.txt', header=None).values[:,0])\n)","3e9dd3bb":"!.\/ffm-train -p libffm_val_fold_4.txt -r 0.05 -l 0.00001 -k 50 -t 7 libffm_trn_fold_4.txt libffm_fold_4_model\n!.\/ffm-predict libffm_val_fold_4.txt libffm_fold_4_model val_preds_fold_4.txt\n(\n    log_loss(train['target'].iloc[fold_ids[3][1]], pd.read_csv('val_preds_fold_4.txt', header=None).values[:,0]),\n    roc_auc_score(train['target'].iloc[fold_ids[3][1]], pd.read_csv('val_preds_fold_4.txt', header=None).values[:,0])\n)","9dbe6c97":"!.\/ffm-train -p libffm_val_fold_5.txt -r 0.05 -l 0.00001 -k 50 -t 7 libffm_trn_fold_5.txt libffm_fold_5_model\n!.\/ffm-predict libffm_val_fold_5.txt libffm_fold_5_model val_preds_fold_5.txt\n(\n    log_loss(train['target'].iloc[fold_ids[4][1]], pd.read_csv('val_preds_fold_5.txt', header=None).values[:,0]),\n    roc_auc_score(train['target'].iloc[fold_ids[4][1]], pd.read_csv('val_preds_fold_5.txt', header=None).values[:,0])\n)","ff2b9a90":"oof_preds = np.zeros(train.shape[0])\nfor fold_, (_, val_) in enumerate(fold_ids):\n    oof_preds[val_] = pd.read_csv(f'val_preds_fold_{fold_+1}.txt', header=None).values[:, 0]\noof_score = roc_auc_score(train['target'], oof_preds)\nprint(oof_score)","bc9763ed":"!.\/ffm-train -r 0.05 -l 0.00001 -k 50 -t 7 libffm_trn.txt libffm_model","9eb69870":"!.\/ffm-predict libffm_tst.txt libffm_model tst_preds.txt","60da9142":"submission = test[['id']].copy()\nsubmission['target'] = pd.read_csv('tst_preds.txt', header=None).values[:,0]\nsubmission.to_csv('libffm_prediction.csv', index=False)","f9a3f031":"## Label Encode to ease creation of libffm format","15963781":"## Compute OOF score","40119ed5":"## Create LibFFM files\n\n\nThe data format of LIBFFM has a very special format (taken from [libffm page](https:\/\/github.com\/ycjuan\/libffm)):\n```\n<label> <field1>:<feature1>:<value1> <field2>:<feature2>:<value2> ...\n.\n.\n.\n```\n\n`field` and `feature` should be non-negative integers.\n\nIt is important to understand the difference between `field` and `feature`. For example, if we have a raw data like this:\n\n| Click | Advertiser | Publisher |\n|:-----:|:----------:|:---------:|\n|    0 |       Nike |       CNN |\n|    1 |       ESPN |       BBC |\n\nHere, we have \n \n - 2 fields: Advertiser and Publisher\n - 4 features: Advertiser-Nike, Advertiser-ESPN, Publisher-CNN, Publisher-BBC\n\nUsually you will need to build two dictionares, one for field and one for features, like this:\n    \n    DictField[Advertiser] -> 0\n    DictField[Publisher]  -> 1\n    \n    DictFeature[Advertiser-Nike] -> 0\n    DictFeature[Publisher-CNN]   -> 1\n    DictFeature[Advertiser-ESPN] -> 2\n    DictFeature[Publisher-BBC]   -> 3\n\nThen, you can generate FFM format data:\n\n    0 0:0:1 1:1:1\n    1 0:2:1 1:3:1\n\nNote that because these features are categorical, the values here are all ones.\n\nThe class defined below go through all features and rows and update a python dicts as new values are encountered.","6c6e366e":"## Read the data","32b8350b":"## Prepare submission","2c374dbe":"## Predict for test set","818f1a38":"## Introduction\n\n[Field-Aware Factorization](https:\/\/www.csie.ntu.edu.tw\/~cjlin\/libffm) is a powerful representation learning.\n\n[Github here.](https:\/\/github.com\/ycjuan\/libffm)\n\nThis notebook demonstrates a way to use libffm binaries into a Kaggle kernel.\n\nRelease Notes :\n - V4 : New version with Out-of-Fold\n - V6 : fixed the encoder, previous version was kind of a regularizer :) \n ","545223dc":"## Train a libffm model","ab0c6024":"## Make ffm-train and ffm-predict excutable","549b2d77":"## Run OOF"}}