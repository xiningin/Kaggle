{"cell_type":{"9eb0f9f4":"code","b18ceb91":"code","6cdae832":"code","6f3bebbe":"code","0b6e95de":"code","8b9ab1b2":"code","b78ad52c":"code","13bea1ed":"code","71da1d4e":"code","37fead60":"code","a813cd8c":"code","801fa53f":"code","1b8ce3c6":"code","8a409d78":"code","a6948950":"code","cf00bc69":"code","d36a7b96":"code","2d065abc":"code","38ad2def":"code","8673f4c8":"code","e7532436":"code","cf8a0138":"code","46b4b86e":"code","1784c514":"code","b4101cea":"code","8175401f":"code","4defc49e":"code","080d8292":"code","855207aa":"code","51704756":"code","cd6fed8f":"code","ae25a655":"code","e962e2ad":"code","34d484d9":"markdown","1cf95b7b":"markdown","4dceaecf":"markdown","26923e28":"markdown","81284379":"markdown","f75f55a1":"markdown","be711fc7":"markdown","c0abad6b":"markdown","3c675762":"markdown","f5931179":"markdown","d04766a0":"markdown","c45c54a0":"markdown","cd08215a":"markdown","b6ca5b51":"markdown","d500f0cd":"markdown","828fc795":"markdown","c0912238":"markdown","d40e65a4":"markdown","4f8607b3":"markdown","1c88919f":"markdown","5cfd912b":"markdown","7132ddc1":"markdown","ff509787":"markdown","a023e82a":"markdown","381f096c":"markdown","795e7bd3":"markdown","c43a2900":"markdown","37734101":"markdown","d54afbcb":"markdown","7f31f04b":"markdown","033ad9d6":"markdown","2c57a73d":"markdown","52f8a12a":"markdown","b60f921c":"markdown","82d6bb9e":"markdown","a3af7a8b":"markdown","e7cee361":"markdown","2e9c3036":"markdown","92100ca6":"markdown","ef0028c7":"markdown","420f8745":"markdown","6cd07f55":"markdown","ec48da8d":"markdown","d2f0036a":"markdown","c87f5821":"markdown","8740eebc":"markdown","79bc5ef4":"markdown","df63e7fd":"markdown","1b8f1671":"markdown","897b7c15":"markdown","8f741ad3":"markdown"},"source":{"9eb0f9f4":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt # for creating plots\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","b18ceb91":"#we import some preprocessing methods for train\/test split and feature encoding, as well as an accuracy checker\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\nfrom sklearn.metrics import accuracy_score","6cdae832":"#read data and isolate only data on class and survival\n#there are no missing values\ntitanic_data = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\nX = titanic_data.Pclass.to_numpy().reshape(-1,1)\ny = titanic_data.Survived.to_numpy()\n\n#split into training and validation data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,random_state=0)\n\n#now let's try to calculate the parameters of the model from the training data\n#we first calculate the $P(C_k)$'s\n#1 means survived, 0 means perished\np_c1 = y_train.mean()\np_c0 = 1 - p_c1\n\nprint('P(C0) = ',p_c0)\nprint('P(C1) = ',p_c1)\nprint()\n\n#now calculate the $P(x|C_k)$'s\n#first we give the training sets conditional on the survival status\nX_train_c0 = X_train[y_train == 0]\nX_train_c1 = X_train[y_train == 1]\n\n#and then extract conditional probabilities from it\np_1_given_c0 = np.mean(X_train_c0 == 1)\np_2_given_c0 = np.mean(X_train_c0 == 2)\np_3_given_c0 = np.mean(X_train_c0 == 3)\n\np_1_given_c1 = np.mean(X_train_c1 == 1)\np_2_given_c1 = np.mean(X_train_c1 == 2)\np_3_given_c1 = np.mean(X_train_c1 == 3)\n\n#display relevant calculations\nprint('P(x=1|C0) = ',p_1_given_c0)\nprint('P(x=2|C0) = ',p_2_given_c0)\nprint('P(x=3|C0) = ',p_3_given_c0)\nprint()\nprint('P(x=1|C1) = ',p_1_given_c1)\nprint('P(x=2|C1) = ',p_2_given_c1)\nprint('P(x=3|C1) = ',p_3_given_c1)\nprint()\nprint('Input x = 1: ',\n      '\\n\\tP(C_0 | x=1) ~ ',p_1_given_c0*p_c0,\n      '\\n\\tP(C_1 | x=1) ~ ',p_1_given_c1*p_c1)\nprint('Input x = 2: ',\n      '\\n\\tP(C_0 | x=2) ~ ',p_2_given_c0*p_c0,\n      '\\n\\tP(C_1 | x=2) ~ ',p_2_given_c1*p_c1)\nprint('Input x = 3: ',\n      '\\n\\tP(C_0 | x=3) ~ ',p_3_given_c0*p_c0,\n      '\\n\\tP(C_1 | x=3) ~ ',p_3_given_c1*p_c1)","6f3bebbe":"#training the model and predicting\n#the alpha parameter is a smoothing parameter for the case that some features do not appear at all in the test data\n#we set this to zero since this does not happen here\nfrom sklearn.naive_bayes import CategoricalNB\nclf = CategoricalNB(alpha=1.0e-10)\nclf.fit(X_train,y_train)\n\n#the parameters of the model\nprint('Probabilities p(C0) and p(C1): ')\nprint('\\t',[np.exp(x) for x in clf.class_log_prior_.tolist()])\nprint('Probabilities of class 1, 2, or 3 given C0: ')\nprint('\\t',[np.exp(x) for x in clf.feature_log_prob_[0][0].tolist()])\nprint('Probabilities of class 1, 2, or 3 given C1: ')\nprint('\\t',[np.exp(x) for x in clf.feature_log_prob_[0][1].tolist()])","0b6e95de":"#predicting on some sample data\ny_pred = clf.predict(X_test)\npredictions = pd.DataFrame({'class':X_test.flatten(),'predict_proba':clf.predict_proba(X_test)[:,1],'predict':y_pred,'survived':y_test})\nprint('Some sample predictions: ')\nprint(predictions)\nprint()\n\n#evaluating the accuracy\nprint('Accuracy: ',accuracy_score(y_pred,y_test))","8b9ab1b2":"# data entry\ndata = pd.DataFrame()\ndata['Height'] = [6,5.92,5.58,5.92,5,5.5,5.42,5.75]\ndata['Weight'] = [180,190,170,165,100,150,130,150]\ndata['Foot_Size'] = [12,11,12,10,6,8,7,9]\ndata['Gender'] = [0,0,0,0,1,1,1,1]\n\n# View the data\ndata","b78ad52c":"#setting up the model\nfrom sklearn.naive_bayes import GaussianNB\nclf = GaussianNB()\n\n#formatting the training data\nX_train = data[['Height','Weight','Foot_Size']]\ny_train = [0,0,0,0,1,1,1,1]\n\n#fitting the model\nclf.fit(X_train,y_train)\n\n#how to retrieve properites of the model\nprint('Summary of parameters of our model: ')\nprint('\\tClasses to predict: ', clf.classes_)\nprint('\\tPrior on each class: ', clf.class_prior_)\nprint('\\tMeans for feature in each class: ')\nprint(clf.theta_)\nprint('\\tStandard deviations for feature in each class: ')\nprint(clf.sigma_)","13bea1ed":"#predicting on some sample data\nprint('Some sample predictions: ')\ntest_data = pd.DataFrame({'Height':[5.67,5],'Weight':[145,110],'Foot_Size':[12,7]})\ntest_results = pd.DataFrame({'gender':clf.predict(test_data)})\nprint(test_data.join(test_results))","71da1d4e":"logit = np.arange(-5.0, 5.0, 0.01)\np = 1\/(1+np.exp(-logit))\nplt.plot(logit, p)\n\nplt.xlabel('logit(p)')\nplt.ylabel('p')\nplt.grid(True)\nplt.show()","37fead60":"# data entry\ndata = pd.DataFrame()\ndata['Hours_Studied'] = [0.50,0.75,1.00,1.25,1.50,1.75,1.75,2.00,2.25,2.50,2.75,3.00,3.25,3.50,4.00,4.25,4.50,4.75,5.00,5.50]\ndata['Pass'] = [0,0,0,0,0,0,1,0,1,0,1,0,1,0,1,1,1,1,1,1]\n\n#setting up the model\nfrom sklearn.linear_model import LogisticRegression\nmodel = LogisticRegression(penalty='none') #Penalty sets the regularization used when finding an MLE for the betas. We found that regularization messed with the results too much. We could alternatively keep regularization but set the regularization parameter C to be very large\n\n#formatting the training data\nX_train = data[['Hours_Studied']]\ny_train = [0,0,0,0,0,0,1,0,1,0,1,0,1,0,1,1,1,1,1,1]\n\n#fitting the model\nmodel.fit(X_train,y_train)\n\n#how to retrieve properites of the model\nprint('Summary of parameters of our model: ')\nprint('\\tIntercept: ', model.intercept_)\nprint('\\tCoefficients: ', model.coef_)","a813cd8c":"x = np.arange(0.0, 6.0, 0.01).reshape(-1,1)\nlogit = model.decision_function(x)\np = 1\/(1+np.exp(-logit))\nplt.plot(x, p)\nplt.plot(X_train,y_train,'o')\n\nplt.xlabel('Hours Studied')\nplt.ylabel('Prob of Success')\nplt.grid(True)\nplt.show()","801fa53f":"#predicting on some sample data\n\nprint('Some sample predictions: ')\ntest_data = pd.DataFrame({'Hours_Studied':[1,2,3,4,5]})\ntest_probs = pd.DataFrame({'Pass_Probability':model.predict_proba(test_data)[:,1]})\ntest_results = pd.DataFrame({'Pass_Prediction':model.predict(test_data)})\nprint(test_data.join(test_probs).join(test_results).set_index('Hours_Studied'))","1b8ce3c6":"    p_0 = np.arange(0, 1.0, 0.01)\n    g = 1 - p_0 ** 2 - (1 - p_0) ** 2\n    plt.plot(p_0, g)\n\n    plt.xlabel('p_0')\n    plt.ylabel('Gini coefficient')\n    plt.grid(True)\n    plt.show()","8a409d78":"    p_0 = np.arange(0.001, 1.0, 0.01)\n    E = - p_0 * np.log2(p_0) - (1-p_0) * np.log2(1-p_0)\n    plt.plot(p_0, E)\n\n    plt.xlabel('p_0')\n    plt.ylabel('E')\n    plt.grid(True)\n    plt.show()","a6948950":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt # for creating plots\n#we import some preprocessing methods for train\/test split and feature encoding, as well as an accuracy checker\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\nfrom sklearn.metrics import accuracy_score\n#reading the example data on tennis matches\ndf = pd.read_csv('\/kaggle\/input\/play-tennis\/play_tennis.csv')\nprint('Shape: ', df.shape)\nprint('Sample of the training data:')\nprint(df.head())","cf00bc69":"#break the data up into features and dependent variable\nindep_vars = ['outlook','temp','humidity','wind']\nX = df[indep_vars]\ny = df['play']\n\n#perform a train\/test split\n#unfortunately the dataset is pretty small so there isn't much room for validation\nX_train, X_test, y_train, y_test = train_test_split(X,y,train_size=9,random_state=0)#encode the categorical variables numerically\n\n#encode the catecorical data numerically\nX_encoder = OneHotEncoder(sparse = False)\nX_train_enc = pd.DataFrame(X_encoder.fit_transform(X_train),\n                           columns = X_encoder.get_feature_names(indep_vars) )\nX_test_enc = pd.DataFrame(X_encoder.transform(X_test),\n                           columns = X_encoder.get_feature_names(indep_vars) )\n\ny_encoder = LabelEncoder()\ny_train_enc = pd.DataFrame(y_encoder.fit_transform(y_train),columns = ['play'])\ny_test_enc = pd.DataFrame(y_encoder.transform(y_test),columns = ['play'])","d36a7b96":"#building and fitting the model\n#criterion is the attrivute selection measure discussed above\n#without some pruning the model simply memorizes the dataset\nfrom sklearn.tree import DecisionTreeClassifier\ntree = DecisionTreeClassifier(criterion='entropy',min_samples_leaf=3)\ntree.fit(X_train_enc,y_train_enc)\n\n#predicting whether or not we play using the decision tree\n#since we used pruning, the leaves do not have homogeneous outputs\n#hence, predict_proba tells you the probability of falling into each class once you've followed your way through the tree\ny_pred_proba = pd.DataFrame(tree.predict_proba(X_test_enc)[:,1],columns=['play_proba'])\ny_pred = pd.DataFrame(y_encoder.inverse_transform(tree.predict(X_test_enc)),columns=['prediction'])\n\n#display results\nX_test = pd.DataFrame(X_encoder.inverse_transform(X_test_enc),columns=indep_vars)\ny_test = pd.DataFrame(y_encoder.inverse_transform(np.ravel(y_test_enc)),columns=['play'])\naccuracy = accuracy_score(y_test,y_pred)\n\nprint('Predictions:')\nprint(X_test.join(y_pred_proba).join(y_pred).join(y_test))\nprint()\nprint('Accuracy: ',accuracy)","2d065abc":"#as well as displaying them\nfrom sklearn.tree import plot_tree\n\nfig = plt.figure(figsize=(10,8))\nplot_tree(tree, \n                   feature_names=X_encoder.get_feature_names(['outlook','temp','humidity','wind']),  \n                   class_names=['yes','no'],\n                   filled=True)\nplt.show()","38ad2def":"from sklearn.ensemble import RandomForestClassifier\n\n#reading the titanic data\n#since the point of this is to provide a simple example, we only keep those features that don't need much engineering\n#for same reason, don't do any imputing and just drop all rows with null values\ndf = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ndf = df[['Age','SibSp','Parch','Fare','Pclass','Sex','Embarked','Survived']].dropna()\nX_num = df[['Age','SibSp','Parch','Fare']]\nX_cat = df[['Pclass','Sex','Embarked']]\n\n#to plug this into a random forest classifier, which requires numeric inputs, we need to do some feature encoding\nX_encoder = OneHotEncoder(sparse=False)\nX_cat_enc = pd.DataFrame(X_encoder.fit_transform(X_cat),\n                           columns = X_encoder.get_feature_names(['Pclass','Sex','Embarked']) )\n\n#combine the categorical features with the numerical ones to create a single dataframe of training data\n#recall the index is meaningful after having droped na's, so we need to make them consistent before joining\nX_cat_enc.index = X_num.index\nX = X_num.join(X_cat_enc)\ny = np.ravel(df[['Survived']])\n\n#perform a train\/test split\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,random_state=0)\n\n#training a random forest classifier with n_estimators=500 randomly generated trees\n#note bootstrap = True by default\nmodel = RandomForestClassifier(n_estimators=100,max_samples=0.2,max_features=3,random_state=0)\nmodel.fit(X_train,y_train)\n\n#now let's see how accurate the predictions are\ny_pred = pd.DataFrame(model.predict(X_test),columns = ['Survived'])\nscore = accuracy_score(y_pred,y_test)\nprint('Percentage correct in our model: ',score * 100 , '%')","8673f4c8":"#import the data from the sklearn prepackaged datasets\nfrom sklearn import datasets\niris = datasets.load_iris()\n\n#package this into a dataframe with column labels to keep track of what information is what\niris_features = pd.DataFrame(iris.data,\n                                columns = ['Sepal_Length', 'Sepal_Width', 'Petal_Length', 'Petal_Width'])\niris_target = pd.DataFrame(iris.target,\n                            columns = ['Species'])\nprint('The iris dataset:')\nprint(iris_features.join(iris_target))","e7532436":"#We intend to use only the first two features for our model\nX = iris.data[:,:2]\ny = iris.target\n\n#creating a train\/test split\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,random_state=0)","cf8a0138":"#import the relevant method from sklearn\nfrom sklearn.neighbors import KNeighborsClassifier\n\n#create and fit the model\nclf = KNeighborsClassifier(n_neighbors=1, weights='uniform')\nclf.fit(X_train, y_train)\n\n#make predictions with the model and compare them to the actual result\ny_pred = clf.predict(X_test)\ny_pred_proba = clf.predict_proba(X_test)\n\n#printing the results\nmodel_validation = np.concatenate((X_test,y_pred.reshape(-1,1),y_test.reshape(-1,1)),axis=1)\nresults = pd.DataFrame(model_validation,columns=['Sepal_Length','Sepal_Width','Species_Predicted','Species']).iloc[-10:]\nprint('Comparing predictions made by the model to the true result:')\nprint(results)\nprint()\n\n#accuracy evaluation\naccuracy = accuracy_score(y_pred,y_test)\nprint('Accuracy: ', 100 * accuracy , '%')","46b4b86e":"pd.DataFrame(clf.predict_proba(X_test),columns=[['prob_0','prob_1','prob_2']]).iloc[-10:]","1784c514":"#NB: Much of this plotting code was lifted from a very helpful example which I found online but have since lost the link to.\n#If anyone could point out the source I would really appreciate it!\n\n#we will represent iris species with color, this requires a ListedColormap\nfrom matplotlib.colors import ListedColormap\ncmap_light = ListedColormap(['orange', 'cyan', 'cornflowerblue'])\ncmap_bold = ListedColormap(['darkorange', 'c', 'darkblue'])\n\n#generating the domain we will plot and making predictions for every point in a mesh of that domain\nx_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\ny_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\nxx, yy = np.meshgrid(np.arange(x_min, x_max, .02),\n                     np.arange(y_min, y_max, .02))\nZ = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n\n#now let's make the plot, to do so we first have to reshape Z, which is just a 1d array, to match the shape of xx and yy\n#then plot the color coded mesh\nZ = Z.reshape(xx.shape)\nplt.figure(figsize=(8, 6), dpi=100)\nplt.pcolormesh(xx, yy, Z, cmap=cmap_light)\n\n#also plot the training points to see the behavior\nplt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cmap_bold,\n                edgecolor='k', s=20)\nplt.xlim(xx.min(), xx.max())\nplt.xlabel('Sepal Length')\nplt.ylim(yy.min(), yy.max())\nplt.ylabel('Sepal Width')\nplt.title(\"Iris Species Classification\")\n\nplt.show()","b4101cea":"#import the data from the sklearn prepackaged datasets\nfrom sklearn import datasets\niris = datasets.load_iris()\n\n#package this into a dataframe with column labels to keep track of what information is what\niris_features = pd.DataFrame(iris.data,\n                                columns = ['Sepal_Length', 'Sepal_Width', 'Petal_Length', 'Petal_Width'])\niris_target = pd.DataFrame(iris.target,\n                            columns = ['Species'])\nprint('Sample of the iris dataset')\nprint(iris_features.join(iris_target).head())\n","8175401f":"#We intend to use only the first two features for our model\nX = iris.data[:,:2]\ny = iris.target\n\n#creating a train\/test split\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,random_state=0)","4defc49e":"from sklearn.svm import SVC\n\nclf = SVC(C=1.0,kernel='linear',decision_function_shape='ovr',gamma=1.0,coef0=0.5,degree=4)\nclf.fit(X_train,y_train)\n\n#make predictions with the model and compare them to the actual result\ny_pred = clf.predict(X_test)\n\n#printing the results\nmodel_validation = np.concatenate((X_test,y_pred.reshape(-1,1),y_test.reshape(-1,1)),axis=1)\nresults = pd.DataFrame(model_validation,columns=['Sepal_Length','Sepal_Width','Species_Predicted','Species']).iloc[-10:]\nprint('Comparing predictions made by the model to the true result:')\nprint(results)\nprint()\n\n#accuracy evaluation\naccuracy = accuracy_score(y_pred,y_test)\nprint('Accuracy: ', 100 * accuracy , '%')","080d8292":"#we will represent iris species with color, this requires a ListedColormap\nfrom matplotlib.colors import ListedColormap\ncmap_light = ListedColormap(['orange', 'cyan', 'cornflowerblue'])\ncmap_bold = ListedColormap(['darkorange', 'c', 'darkblue'])\n\n#generating the domain we will plot and making predictions for every point in a mesh of that domain\nx_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\ny_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\nxx, yy = np.meshgrid(np.arange(x_min, x_max, .02),\n                     np.arange(y_min, y_max, .02))\nZ = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n\n#now let's make the plot, to do so we first have to reshape Z, which is just a 1d array, to match the shape of xx and yy\n#then plot the color coded mesh\nZ = Z.reshape(xx.shape)\nplt.figure(figsize=(8, 6), dpi=100)\nplt.pcolormesh(xx, yy, Z, cmap=cmap_light)\n\n#also plot the training points\nplt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cmap_bold,\n                edgecolor='k', s=20)\nplt.xlim(xx.min(), xx.max())\nplt.xlabel('Sepal Length')\nplt.ylim(yy.min(), yy.max())\nplt.ylabel('Sepal Width')\nplt.title(\"Iris Species Classification\")\n\nplt.show()","855207aa":"#load the dataset and take a look\ndataset = pd.read_csv('..\/input\/pima-indians-diabetes-database\/diabetes.csv')\nprint(dataset.head())","51704756":"#loading the necessary methods\nfrom xgboost import XGBClassifier\n\n#separate out the features and the dependent variable\nX = dataset.iloc[:,:-1]\ny = dataset.iloc[:,-1]\n\n#split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,random_state=0)\n\n#now create the model and train it\n#eval_set is provided for cross validation across the training and test sets\n#eval_metric then specifies which measures of error we want to keep track of\nclf = XGBClassifier(n_estimators=100,learning_rate=0.1,reg_lambda=1,min_split_loss=0)\neval_set=[(X_train,y_train),(X_test,y_test)]\nclf.fit(X_train,y_train,\n        eval_metric=['logloss','error'],eval_set=eval_set,verbose=False)\n\n#now lets make some predictions on the test set and see how they fare\ny_pred = clf.predict(X_test)\naccuracy = accuracy_score(y_test, y_pred)\nprint('Accuracy on the test set data:',100*accuracy,'%')","cd6fed8f":"# retrieve performance metrics\nresults = clf.evals_result()\nepochs = len(results['validation_0']['logloss'])\nx_axis = range(0, epochs)\n# plot log loss\nfig, ax = plt.subplots()\nax.plot(x_axis, results['validation_0']['logloss'], label='Train')\nax.plot(x_axis, results['validation_1']['logloss'], label='Test')\nax.legend()\nplt.xlabel('Rounds')\nplt.ylabel('Log Loss')\nplt.title('XGBoost Log Loss')\nplt.show()\n# plot error\nfig, ax = plt.subplots()\nax.plot(x_axis, results['validation_0']['error'], label='Train')\nax.plot(x_axis, results['validation_1']['error'], label='Test')\nax.legend()\nplt.xlabel('Rounds')\nplt.ylabel('Error')\nplt.title('XGBoost Error')\nplt.show()","ae25a655":"#load the dataset and take a look\ndataset = pd.read_csv('..\/input\/pima-indians-diabetes-database\/diabetes.csv')\n\n#separate out the features and the dependent variable\nX = dataset.iloc[:,:-1]\ny = dataset.iloc[:,-1]\n\n#split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,random_state=0)\n\n#loading the necessary methods\nfrom sklearn.ensemble import AdaBoostClassifier\n\n#now create the model and train it\n#eval_set is provided for cross validation across the training and test sets\n#eval_metric then specifies which measures of error we want to keep track of\nn_estimators = 110\nlearning_rate = 1.0\nrandom_state = 0\n\nalgs = ['SAMME', 'SAMME.R']\nmodels = [None]*len(algs)\n\nfor i in np.arange(len(algs)):\n    models[i] = AdaBoostClassifier(n_estimators=n_estimators,learning_rate=learning_rate,algorithm=algs[i],random_state=random_state)\n    models[i].fit(X_train,y_train)\n    \n    y_pred = models[i].predict(X_test)\n    error_rate = 1 -  accuracy_score(y_test,y_pred)\n    print('{} Error Rate: '.format(algs[i]), error_rate)","e962e2ad":"for i in np.arange(len(algs)):\n    error_train = np.zeros(n_estimators)\n    error_test = np.zeros(n_estimators)\n    \n    for j, y_pred_train in enumerate(models[i].staged_predict(X_train)):\n        error_train[j] = 1 - accuracy_score(y_train,y_pred_train)\n    \n    for j, y_pred_test in enumerate(models[i].staged_predict(X_test)):\n        error_test[j] = 1 - accuracy_score(y_test,y_pred_test)\n\n    iterations = np.arange(n_estimators)\n    plt.plot(iterations,error_train,label='train error')\n    plt.plot(iterations,error_test,label='test error')\n    \n    plt.xlabel('iterations')\n    plt.ylabel('error rate')\n    plt.title('{} validation'.format(algs[i]))\n    plt.legend()\n    plt.grid(True)\n    \n    plt.show()","34d484d9":"We don't really have enough data to perform a good train\/test split of the data, so just for illustrative purposes lets put in some data by hand and see how to retrieve predictions.","1cf95b7b":"## Example\n\nNow let's use `XGBoost` to produce such a model. The example taken from [here](https:\/\/machinelearningmastery.com\/develop-first-xgboost-model-python-scikit-learn\/). The dataset we use is some health data for females of pima indian ancestry and whether or not they have developed diabetes. Let's take a quick look.","4dceaecf":"Fortunately there is no data cleaning to do. Let's create an `XGBClassifier` then and train it. A sample of important parameters for the model is (for others see [here](https:\/\/xgboost.readthedocs.io\/en\/latest\/python\/python_api.html#module-xgboost.sklearn)):\n\n* `n_estimators` is the number of trees generated in the ensemble\n* `learning_rate` is $\\epsilon$ from above discustion\n* `reg_lambda` is the regularization parameter $\\lambda$\n* `min_split_loss` is the regularization parameter $\\gamma$","26923e28":"# <div id=\"naive\">Naive Bayes<\/div>\n\n## Summary\n\nThe principle source I used here was the Wikipedia article on [naive Bayes classifiers](https:\/\/en.wikipedia.org\/wiki\/Naive_Bayes_classifier).\n\nNaive Bayes is a conditional probability model. That is, given a feature vector $\\mathbf x$ it produces the conditional probability $P(y|\\mathbf x)$ that the dependent variable $y$ has a given value, given that the feature vector is $\\mathbf x$. To turn this into a classifier, we simply take the value of $y$ which maximizes $P(y|\\mathbf x)$ .\n\nThe most straightforward thing we could do is to  estimate this probability directly from the training data as follows\n\\begin{align}\n    P(y | \\mathbf x ) = \\frac{\\text{no of samples with feature vector $\\mathbf x$ and dependent variable $y$}}{\\text{no of samples with feature vector $\\mathbf x$}}\n\\end{align}\n\nIn practice, this is rarely feasible for the following reasons:\n\n* We might want to make a prediction based on a novel collection of features $\\mathbf x$ that isn't in the training data. For example, when a continuous $x_i$ takes a new value, or when $x_i$ and $x_j$ come in a pair not present in the training data.\n* Even if $\\mathbf x$ is in the training data, the numbers of such samples may be small and we might not expect the above estimate to be a good approximation to the true population value of $P(y | \\mathbf x )$\n\nThese issues are especially likely if the number of features is large or if the features may take a large number of values. In particular, if there are continuous features we will almost always be querying $\\mathbf x$'s that do not apper in the training data.\n\nThe idea behind a Naive Bayes model is to get around this by using Bayes theorem to invert the conditional probability\n\n\\begin{align}\n    P(y | \\mathbf x) = \\frac{P(\\mathbf x | y) P(\\mathbf y)}{P(x)}\n\\end{align}\n\nand then calculate the numerator using the simplifying assumption that the various features in $\\mathbf x$ are independent (conditional on $y$)\n\n\\begin{align}\n    P(\\mathbf x | y ) = P ( x_1 | y ) \\cdots P ( x_n | y ) .\n\\end{align}\n\nWe then have\n\n\\begin{align}\n\n    P(y | \\mathbf x) \\propto P ( x_1 | y ) \\cdots P ( x_n | y ) P(\\mathbf y)\n\\end{align}\n\nwhere we have thrown out an unimportant $y$ independent constant. The right hand side of this expression is less likely to have the problems mentioned above since the estimates of each factor will be based off of a larger number of samples from the training data.\n\nOne can check the independence of two continuous variables by evaluating the correlation\n\n\\begin{align}\n    \\text{corr}^{(y)} (X_i,X_j) = \\frac{E^{(y)}[(X_1 - \\mu_{X_1})(X_2 - \\mu_{X_2})]}{\\sigma^{(y)}_{X_1} \\sigma^{(y)}_{X_2}}\n\\end{align}\n\nwith the training data. Here the superscripts indicate that everything is conditional on $Y=y$. This is valued in $[-1,1]$, and if $X_1$ and $X_2$ are independent, the correlation must be zero (though the converse does not hold). Furthermore, the correlation is  $\\pm 1$, iff $X_1$ and $X_2$ are lineraly related.\n\nLater: Correlation measures for two categorical variables and for one continuous and one categorical variable. It looks like a good discussion can be found [here](https:\/\/datascience.stackexchange.com\/questions\/893\/how-to-get-correlation-between-two-categorical-variable-and-a-categorical-variab)\n\n### Notes:\n* Highly accurate when independence assumption is justified and if few sample probabilities near zero, but independence is a big assumption.\n* Classifier may be trained in time linear in features\/samples.\n* When the estimated probability of a feature in a class is 0, must do some regularization. Even if it is very small, model is highly sensitive to these small values.","81284379":"Now let's prepare our data for modelling","f75f55a1":"### Example\n\nAs a simple example, let's use some data from the Titanic Kaggle competition, and try to use Naive Bayes to predict whether or not a passenger has survived based entirely off of what class ticket he purchased. We then have classes\n\n$$C_\\text{perished} = 0 , \\qquad  C_\\text{survived} = 1,$$\n\nand we wish to calculate\n\n$$P ( C_k | x ) \\propto P(x | C_k) P(C_k)$$\n\nfor each $x \\in \\{1,2,3\\}$.\n\nIn the following code cell we implement naive Bayes by hand to illustrate how it works","be711fc7":"Here we demonstrate how to graphically display the decision tree using the `plot_tree` function. Note that all of the leaves are \"pure\", that is all the training observations belonging to a particular leaf are of the same type. For large data sets this is a sign of overfitting and we we may want to prune the tree.","c0abad6b":"## Example\n\nIn this example we take data on weather conditions and whether or not a scheduled tennis match was held that day. We show how to encode the data numerically so that the model can be trained, how to train the model, and make some sample predictions. This example was found [here](https:\/\/towardsdatascience.com\/understanding-decision-tree-classification-with-scikit-learn-2ddf272731bd). First read the data.","3c675762":"Finally, let's display the behavior of the model graphically as before","f5931179":"## Attribute Selection Measures (ASM)\n\nLet's discuss the purity\/impurity measure $H(Q)$ used to select the splitting at each node.\n\n\n### The Gini index\nThe Gini index of a node $Q$ is defined to be\n\n$$H(Q) = \\sum_{k=0,1} p(k|Q) ( 1 - p(k|Q) )= 1 - p(0|Q)^2 - p(1|Q)^2$$\n\nwhere $k$ labels the classification outcome and $p(k|Q)$ is the probability that an observation falling in the node $Q$ has classification outcome $k$\n\n$$p(k|Q) = \\frac{\\text{# of training observations at node $Q$ that are of type $k$}}{\\text{# of training observations in the node $Q$}}  .$$\n\nMore precisely, it is the estimation of the probability from the training data.\n\n\nThe gini index is a measure of how homogeneous the node is, i.e. it is a function of the training data that takes value 0 when a node contains observations all with a single outcome, and is a maximum when the probability of being in a given category is $ 1 \/2$. Splits in the decision tree are chosen to minimize $G(Q,\\theta)$.","d04766a0":"# Random Forests\n\nA random forest classifier is an ensemble classifier constructed from a large number of randomly generated decision tree classifiers. This randomness greatly reduces variance in the predictions at a slight cost in bias and so is a good way to prevent overfiting. Randomness is introduced to the model in two ways:\n\n1) Only train on a randomly selected subset of the training data (with replacement). This is called \"bootstraping\". Sub-sample size is controlled with the `max_samples` parameter if `bootstrap=True`. This can be an integer number of samples, or a float between 0 and 1, in which case it is interpreted as a fraction of samples to use from the training data.\n\n2) In training each tree, only use a randomly selected subset of features of size `max_features` when deciding on how to split a node.\n\nResults from the different decision trees are then averaged weighted by the probability that each tree gives for being in each class of the classification problem. The mode (most likely result) of the resulting probability distribution is then selected. A more detailed discussion of random forests can be found [here](https:\/\/scikit-learn.org\/stable\/modules\/ensemble.html#forests-of-randomized-trees).","c45c54a0":"# <div id='tree'>Decision Trees<\/div>\n\nA decision tree is essentially a flow chart where each node represents a binary choice based off some feature. Each branch coming from a node represents the decision taken. The terminal nodes are called leaves and label which category a given input is predicted to be in. Example taken from [here](https:\/\/towardsai.net\/p\/programming\/decision-trees-explained-with-a-practical-example-fe47872d3b53)\n\n<img src=\"https:\/\/cdn-images-1.medium.com\/max\/824\/0*J2l5dvJ2jqRwGDfG.png\" width=\"400px\">\n\nSee [here](https:\/\/scikit-learn.org\/stable\/modules\/tree.html#tree-algorithms-id3-c4-5-c5-0-and-cart) for a more detailed discussion of the decision tree algorithm used by `sklearn`.\n\nIn `sklearn`, decision trees are constructed from a data set via the CART (Classification and Regression Tree) algorithm which we now outline. In words, this is a \"top-down\" algorithm that iteratively produces a binary splitting at each node, starting from the top or \"root\" node containing all data, so as to maximize the amount of information gained at each node.\n\nThough this algorithm works for an arbitrary finite classification problem, we will focus on binary classification.","cd08215a":"## Example\nWe work through the example found [here](https:\/\/en.wikipedia.org\/wiki\/Logistic_regression#Examples). In this example we are given data on how long 20 students study for a test and whether they passed or not. We fit a logistic regression model, which allows us to obtain the probability that a student passes as a function of how long they study. Finally, we show how to retrieve the relevant parameters of the model","b6ca5b51":"Note that the model finds a probability distribution function that fits the data best. If we want to actually have a classifier, we need set a threshold probability, above which we predict success, and below which we predict failure. It's easy to retrieve this from `model.decision_function()`. This is simply the function $\\beta_0 + \\beta_1 x_1 + \\cdots \\beta_n x_n$. Selecting 1 when this is positive and 0 when it is negative gives a classifier with threshold probability $ 1 \/ 2$. To get another threshold probability we simply need to shift the y-intercept. The method `model.predict()` does this without any shift. Here's a demonstration on how to retrieve some predictions on sample data.","d500f0cd":"# Example\n\nLet's demonstrate how to use `RandomForestClassifier` from `sklearn`. Random forests are best used when there's large amounts of data and the danger of overfitting, so lets use the Titanic dataset again and try to predict survival using both bootstrapping and `max_features`. The number of trees generated is `n_estimators`.","828fc795":"# Modeling Overview (Classification)\n\nThis is a set of personal notes on popular classification models used in data science for future reference. The goal is to discuss the main features of a number of classification algorithms without going into too much detail, and demonstrate how they are implemented in python. Since this is intended for my own personal use, the questions and confusions I address will be unique to my own background and experience. Nonetheless, I hope that others studying to enter data science will find this a convenient reference in their own data science journeys. Questions and comments are always appreciated! For those that found this notebook useful, I also recommend refering to my notes on [regression algorithms](https:\/\/www.kaggle.com\/michaelgeracie\/modelling-overview-regression) and [clustering algorithms](https:\/\/www.kaggle.com\/michaelgeracie\/modelling-overview-clustering).\n\nWe will use the following conventions throughout. The training data is given by a set of $m$ numerical features $\\mathbf x_j \\in \\mathbb R^n$ where $j = \\{ 1 , \\dots , m \\}$. We will sometimes denote feature vectors using the bold face vector notation just given, and sometimes use components. The $i$th feature of the $j$th training sample is then $x_{ij} = (\\mathbf x_j )_i$ where $i \\in \\{ 1 , \\dots , n\\}$. The features may be real valued or categorical, and if the $i$th feature is categorical, that component will be labelled by some subset $ C \\subseteq \\mathbb Z$ of the integers. \n\nTo date we have covered the following algorithms. The treatment of each is not exhaustive and I hope to come back later to address important aspects of these models that I'm glossing over now. \n\n## 1) Naive Bayes\n## 2) Logistic Regression\n## 3) Decision Trees\n## 4) Random Forests\n## 5) $k$-Nearest Neighbors\n## 6) Support Vector Classifiers\n## 7) XGBoost\n## 8) AdaBoost\n\nA special thanks to Ken Jee whose notebook [Titanic Project Example](https:\/\/www.kaggle.com\/kenjee\/titanic-project-example) has helped many lerners including myself begin their data science journeys by collecting a number of applied classification algorithms in one place. This is the starting point for this notebook, which will hopefully expand as I encounter more \"in the wild\".\n\nThe following is the standard code to start up a notebook","c0912238":"# Support Vector Classifier\n\nSupport vector classifiers work by viewing the training set $\\mathbf x_j$, $j=1,...,m$ as points in $\\mathbb R^n$ (where $n$ is the number of features), and trying to divide regions associated with different categories with \"domain walls\". In the simplest case, categories are seperable by an $n-1$ plane. We then choose that $n-1$ plane such that it has the greatest distance between itself and the nearest training points of any class. This plane is called the \"maximal-margin hyperplane\" and the region bounded by parallel planes passing through the closest points is called the \"margin\". The plane is determined by the closest $n+1$ training examples, which are called the \"support vectors\". In this discussion we will focus on binary classification so that there is only one maximal-margin hyperplane. The image below is taken from `sklearn`'s discussion of support vector machines, which provides an excellent overview of the method and can be found [here](https:\/\/scikit-learn.org\/stable\/modules\/svm.html#classification).\n\n<img src=\"https:\/\/scikit-learn.org\/stable\/_images\/sphx_glr_plot_separating_hyperplane_0011.png\">\n\n\nIf the classes cannot be separated by a hyperplane, but can be to a good approximation, we still use this method, but the margin will be allowed to contain some points from the training set. The maximal-margin hyperplane is then defined by some \"cost function\", which minimized by some optimal plane. This cost function should induce a greater cost when the margin is small (we want to separate the classes as well as possible) as well as a greater cost the deeper the training set impinges on the marginal region.\n\nThe above strategy constructs a binary classifier. When building a classifier to predict more than two classes, one may use either a \"one-vs-one\" or \"ovo\" approach or a \"one-versus-rest\" or \"ovr\" approach. In the ovo approach a decision boundary is created from data for each pair of categories. In the ovr approach, a decision boundary is created comparing each category to everything that is not in that category. I haven't taken the time yet to figure out what happens when a point is in the positive result region for multiple or no categories.","d40e65a4":"Let's take a look at how the model assigns probabilities verses hours studied. ","4f8607b3":"Hence we predict that a first class passenger survives, but second and third class passengers perish.\n\nNow, we implement this using sklearn in the code below. We see that it exactly matches our by-hand calculation.","1c88919f":"Since the features are continuous, we need some model for $P(x_i |C_k)$ in order to get its value for any possible $x_i$. In this case, it's reasonable to assume all features are gaussian distributed and select their means and variances according to the training data. We are then postulating\n\n$$p(x_i | C_k) \\propto \\frac{1}{\\sqrt{2 \\pi \\sigma^2_{ki}}}e^{- \\frac{(x_i - \\mu_{ki})^2}{2 \\sigma^2_{ki}}}$$\n\nwhere here $\\mu_{ki}$ and $\\sigma^2_{ki}$ are the means and Bessel corrected variance from the training data for the feature $x_i$ conditioned on $C_k$. Of course, for other problems, different probability distributions will be relevant.\n\nWe implement this proceedure using `sklearn` in the example below and not bother carrying the proceedure out by hand.","5cfd912b":"## Example\n\nIn this example, we demonstrate how to use the `KNeighborsClassifier` from `sklearn` on the iris dataset provided by `sklearn`. The iris dataset gives the sepal length, sepal width, petal length, and petal width of 150 irises and which species of iris that flower is from. The data set only includes data from three species, Setosa, Versicolour, and Virginica, encoded as 0, 1, and 2 respectively. For ease of presentation, we will only use two of these features. We begin by loading the data","7132ddc1":"## Example\n\nAgain we train the model on the Pima indians data set. To see how the two AdaBoost algorithms compare, we run the following code","ff509787":"Now we prepare the data using, encoding the categorical data numerically since the decision tree classier requires numerical inputs for making splits.","a023e82a":"A very efficient way to demonstrate the behavior of the model is to graph the domains that are mapped to a particular category. We do that here, superimposed with the training data. Play around with the parameters of the model. The behavior is particularly clear when `n_neighbors=1`.","381f096c":"Here we demonstrate how to see the performance behavior throughout training.","795e7bd3":"# AdaBoost\n\nFor a good discussion of the mathematics behind AdaBoost, see [here](https:\/\/en.wikipedia.org\/wiki\/AdaBoost). [This](https:\/\/medium.com\/analytics-vidhya\/add-power-to-your-model-with-adaboost-algorithm-ff3951c8de0) is also a good source. The original paper with the SAMME algorithm used by `sklearn` can be found [here](https:\/\/web.stanford.edu\/~hastie\/Papers\/samme.pdf).\n\nLike XGBoost, AdaBoost is a boosting algorithm, that is it creates a sequence of so-called \"weak\" classifiers, this ensemble of classifiers is used to make a prediction, and then each successive classifier is chosen to correct the results of the ensemble in the best possible way. These weak classifiers may be of any type, however, they are often depth=1 decision trees, or stumps (this is the `sklearn` default). Moreover, the way in which their performance is evaluated and added to the ensemble is different. The main idea is that if the ensemble misclassifies a training example, the next tree should put more weight on that example. Hence computational time is spent focussing on learning patterns in the data that have not been learned yet (though it seems to me this would also encourage over-fitting and oversensitivity to outliers to me).\n\nNow let's give the details. Suppose we have training data $\\{(\\mathbf x_j , y_j )\\}$ where $y_j \\in \\{ 1 , - 1 \\}$ is a binary classificaton. In the first step we create a decision stump with the lowest weighted Gini index\n\n\\begin{align}\n    \\frac{| Q_L |}{|Q|} G(Q_L ) + \\frac{|Q_R|}{|Q|} G(Q_R) .\n\\end{align}\nHere $Q$ is the collection of data points in a given node\\leaf, L and R denote the two leaves in the stump, and $G$ is the Gini index of a single node. We let\n\\begin{align}\n    C_1 ( \\mathbf x) = \\alpha_1 k_1 (\\mathbf x)\n\\end{align}\nwhere $k_1$ is this decision stump and $\\alpha_1$ is a constant we show how to get later (it doesn't matter right now as this will just be an overall scale). A point is classified as $+1$ if $C_1 \\geq 0$ and $-1$ otherwise.\n\n\n\nNow suppose at the $m$th step we are given $m-1$ decision stumps $k_a$, $a=1,...,m-1$ with weights $\\alpha_a$. Importantly, the $k_a$'s are interpreted as classifiers: they return $\\pm 1$, not a probability that the result is a $+1$. Given a feature vector $\\mathbf x$, the ensemble makes a weighted vote of these classifiers\n\n\\begin{align}\n    C_{(m-1)} (\\mathbf x) = \\alpha_1 k_1 (\\mathbf x) + \\cdots + \\alpha_{m-1} k_{m-1} ( \\mathbf x )\n\\end{align}\n\nand returns the sign of $C_{(m-1)} (\\mathbf x)$.\n\n\nOur goal is to add to the ensemble an $m$th classifier $k_m$ with weight $\\alpha_m$ so that\n\n\\begin{align}\n    C_m ( \\mathbf x ) = C_{m-1} ( \\mathbf x ) + \\alpha_m k_m ( \\mathbf x ) .\n\\end{align}\n\n\nWe will discuss how to create the stump in a moment, but once we have the stump, the weight $\\alpha_m$ will be chosen to minimize some loss function, where the loss function is chosen to put greater weight on the points in the training data that were misclassified at the $(m-1)$th step. A loss function that accomplishes this is the exponential loss\n\n\\begin{align}\n    L(y_j, C_m) = \\sum_{j=1}^{n_\\text{samples}} e^{- y_j C_m (\\mathbf x_j) }\n\\end{align}\n\nThe expression in the exponential is called the \"amount of say\". This is larger when $\\mathbf x_j$ is misclassified ($-y_j C_m (\\mathbf x_j)$ is positive) and indeed is sensitive to \"how badly\" it's been misclassified. Let\n\n\\begin{align}\n    w^{(m)}_j = e^{-y_j C_{m-1} ( \\mathbf x_j)} = w^{(m-1)}_j e^{-y_j \\alpha_{m-1} k_{m-1} ( \\mathbf x_j)}.\n\\end{align}\n\nThis parameterizes how badly the $(m-1)$th instance of the classifier mis-classifies the $j$th training sample. Rearranging the loss function, we have\n\n\\begin{align}\n    L(y_j, C_m) &= \\sum_{j=1}^{n_\\text{samples}} w^{(m)}_j e^{- y_j \\alpha_m k_m (\\mathbf x_j) } \\nonumber \\\\\n        &= e^{- \\alpha_m } \\sum_{j=1}^{n_\\text{samples}} w^{(m)}_j + (e^{\\alpha_m} - e^{- \\alpha_m} ) \\sum_{y_j \\neq k_m (\\mathbf x_j )} w^{(m)}_j\n\\end{align}\n\n\nLet's introduce the \"weighted error rate\" of $k_m$\n\n\\begin{align}\n    \\epsilon_m = \\sum_{y_j \\neq k_m (\\mathbf x_j)} w^{(m)}_j \\big \/ \\sum_j w^{(m)}_j .\n\\end{align}\n\nThen given a fixed tree $k_m$, the loss function is minimized as a function of $\\alpha_m$ by\n\n\\begin{align}\n    \\alpha_m = \\frac 1 2 \\ln \\left( \\frac{1-\\epsilon_m}{\\epsilon_m} \\right) .\n\\end{align}\n\nNote that stumps with higher accuracy are given greater weight. Note that the `sklearn` implementation with the `SAMME` algorithm replaces the $1\/2$ out front with a learning rate parameter $l$. This can be accomplished in the loss function by putting an overall $l$ factor in the exponential. See [here](https:\/\/stats.stackexchange.com\/questions\/82323\/shrinkage-parameter-in-adaboost?noredirect=1&lq=1).\n\nIn choosing $\\alpha_m$, the model already gives greater weight to samples that were miscalssified by $C_{m-1}$ since each sample enters the loss function with prefactor $w^{(m)}_j = e^{- y_j C_{m-1} ( \\mathbf x_j)}$. However, apparently the SAMME algorithm used in `sklearn`'s implementation of AdABoost goes beyond this, emphasizing the misclassified data points in in training each individual classifier $k_m$ by weighting the training samples with weight $w^{(m)}_j$. This goes beyond simple gradient boosting i.e. the logic is not merely the minimization of a loss function when combining the individual classifiers. Instead, each individual classifier being used in the boosting process is itself focussing more on the samples miss-classified in the previous step.\n\n\nThe stumps are selected to so that the split has the lowest Gini index. In computing the proper split, we should give more weight to samples that $C_{m-1}$ misclassified in the following way. We could do this by calculating a weighted Gini index using the weights $w^{(m)}_j$, but apparently in the AdaBoost package, it is done by sampling from the original data set at step 0 with probabilities $\\propto w^{(m)}_j$ and creating a new sample training set of the same size, then using the unweighted Gini index.\n\nWhat we have just described is known as the SAMME algorith. `sklearn`'s implementation also allows for using the SAMME.R algoritm, which may be found described in detail [here](https:\/\/web.stanford.edu\/~hastie\/Papers\/samme.pdf). The SAMME.R algorithm tends to converge to higher accuracy models quicker by using weak estimators that output a probability rather than simply a classification, however, we have not had time to figure out the details.","c43a2900":"Now that the data is encoded we train the model and make some predictions that we check against the test data. Note that we have decided to prune the tree as the dataset is quite small and a decision tree classifier can easily simply memorize the dataset. It's worth playing around with this parameter. For example, if we use `min_samples_leaf=1`, i.e. no pruning, one sees below that the tree simply memorizes the dataset. Moreover, the accuracy is 0.0 when applied to the test data! However, with `min_samples_leaf=3` we get a tree with only a single decision node, but an accuracy of 0.6 on the test data.","37734101":"Now let's create our $k$-nearest neighbors model. One can play around with $k$ and well as the weighting to see how this affects the model. We find it particularly easy to see the behavior of the model in the upcoming graphic when we take $k=1$, a nearest neighbor model.","d54afbcb":"## The primal problem\n\nThe above is a reasonable approach if the categories are approximately separated by a good hyperplace, that is, if the minimum of the cost function is a small number. If this is not the case, we need to use some non-linear dividing region. This is accomplished by mapping the feature space into some other, possibly higher dimensional space $\\phi: \\mathbb R^n \\rightarrow \\mathbb R^p$ where the dividing region is better approximated as linear and then carrying out the above algorithm there. The map $\\phi$ is called a feature map, and should be a non-linear map. We then seek to minimize\n\n\\begin{align}\n    C \\sum_{j=1}^m \\zeta_j + \\frac 1 2 || \\mathbf w ||_2^2 ,\\nonumber \\\\\n    \\text{subject to} ~~ y_j ( \\mathbf w^T \\phi (\\mathbf x_j) + b ) \\geq 1 - \\zeta_j , \\nonumber \\\\\n    \\text{where} ~~ \\zeta_j \\geq 0 \n\\end{align}\n\nover $\\mathbf w, b, \\zeta_j$.","7f31f04b":"## Building trees\n\nAt the end of the day though, for speed and simplicity, `XGBoost` minimizes the Taylor approximation to the objective function. Let the objective function at step $k$ be\n\n\\begin{align}\n    O = \\sum_{a=1}^k O^{(a)}\n\\end{align}\n\nwith\n\n\\begin{align}\n    O^{(a)} = \\sum_{b=1}^{t^{(a)}} \\left( G^{(a)}_b w^{(a)}_b + \\frac 1 2 ( H^{(a)}_b + \\lambda ) (w^{(a)}_b)^2 \\right) + \\gamma t^{(a)}\n\\end{align}\n\nbeing the contribution from the $a$th tree. Here $G_b$ and $H_b$ are the Taylor coefficients of the objective function introduced above. They are associated to each leaf of a tree and take the values\n\n\\begin{align}\n    G^{(a)}_b = - \\sum_j' ( y_j - p^{(a-1)}_j) ,\n    &&H^{(a)}_b = \\sum_j'  p^{(a-1)}_j ( 1 - p^{(a-1)}_j ).\n\\end{align}\n\nHere the prime denotes that we sum over $(\\mathbf x_j, y_j)$ in the training set that lie in this leaf.\nIn other words, the titular gradient $G^{(a)}_b$ is the sum of the \"residues\" of the training data points that lie in the $b$th leaf.\n\nNow let's turn to building the trees in the enseble. For now, consider a tree $T^{(a)}$ with a given structure of nodes and branches. We can minimize its \"score\" $O^{(a)}$ by selecting weights\n\n\\begin{align}\n    w^{(a)}_b = - \\frac{G_b}{H_b + \\lambda}\n\\end{align}\n\nin which case the tree has a score\n\n\\begin{align}\n    O^{(a)} = - \\frac{G_b^2}{H_b + \\lambda} + \\gamma t^{(a)}\n\\end{align}\n\nTo actually select a good tree, we take the top-down approach discussed above in the construction of decision trees. Starting from the root, we work our way downward, trying to split a leaf into a node with two branches. We introduce a new splitting if the \"gain\" is greater than zero\n\n\\begin{align}\n    \\text{Gain} &= \\text{Objective function before split} - \\text{Objective function after split}\\nonumber \\\\\n        &= \\frac 1 2 \\left( \\frac{G_L^2}{H_L + \\lambda} + \\frac{G_R^2}{H_R + \\lambda} - \\frac{(G_L + G_R)^2}{H_L + H_R + \\lambda }\\right) - \\gamma\n\\end{align}\n\nwhere $L$ and $R$ denote the left and right leaves that you get after the splitting. In the presence of many possible splittings of a leaf, we take the one with the greatest gain. Note that the effect of $\\gamma$ is to give a bias to the gain function, so that larger values of $\\gamma$ tend to prune the tree and prevent over-fitting. Similarly, larger values of $\\lambda$ decrease the positive contributions to the gain and so discourage adding new branches.\n\nApparently, XGBoost only accounts for $\\gamma$ after the tree is made, not at each step. Hence the tree is constructed as if $\\gamma$ were zero, and then one goes back and looks at all of the lowest descision nodes. If the gain is greater than $\\gamma$, that decision node is maintained, otherwise it is removed. We then proceed up the tree in this way. This is quite different from pruning at each step: we might have a split at the root node for instance whose gain does not exceed $\\gamma$, however if we never reach the root node from this bottom up proceedure, the split is maintained.","033ad9d6":"It looks like the model has overfit a bit and we should stop after about 20 rounds.","2c57a73d":"I believe the first entry in the last two lists is for none of the above. Let's also make some sample predictions and see how the model did for illustrations sake. Below we run the model on the validation data, displaying the predicted probability that an individual survives, the prediction itself, and whether or not that person did. We then evaluate the overall accuracy.","52f8a12a":"### The entropy\n\nThe entropy of a node $Q$ is defined to be\n\n$$ H(Q) = - \\sum_{k=0,1} p ( k | Q ) \\log_2 p ( k | Q ) .$$\n\nAgain, we create decision nodes so as to minimize $G(Q,\\theta)$. The entropy has the same features as the Gini coefficient that made it a good measure of homogeneity, or \"information\". It however has strong theoretical advantages which we will not discuss here.","b60f921c":"## Pruning \nWith large data sets and enough features it is possible for the decision tree to overfit the data. In the most extreme case, the tree may simply memorize the training set with one leaf per training observation $(x_{ij}, y_j)$. Hence we will want to \"prune\" the tree by removing branches. This is easily done using the `DecisionTreeClassifier()` method of `sklearn` by feeding it arguments that give the algorithm an earlier stopping condition. Some examples of these are\n\n* `min_samples_leaf`: A split will only be considered if it leaves at least `min_samples_leaf` in each of the left and right branches. The default value is 1.\n* `min_samples_split`: The minimum number of samples required at a node for the algorithm to considre splitting it. The default is 2.\n* `min_impurity_decrease`: The algorithm will only split a node if the impurity of the node is decreased by at least this amount. Default is 0.\n* `max_features`: Considers at most this number of features when creating a split. These features are randomly selected from the set of remaining features. Default is `None`.","82d6bb9e":"## The proceedure\n\nThe initial tree $T^{(0)}$ is just a root, sending $\\mathbf x \\rightarrow f^{(0)}$ (a constant typically set to 0) for any input feature vector. Now suppose we are given an ensemble of trees $\\{ T^{(0)}, \\dots , T^{(k)} \\}$ and let $p^{(k)}_j$ be it's output probability of success for each feature vector $\\mathbf x_j$. We evaluate our success with an \"objective function\"\n\n$$\n    O = \\sum_{j=1}^m L ( y_j , p^{(k)}_j) + \\sum_{a=1}^k \\Omega ( T^{(a)} )\n$$\n\nand try to choose the parameters for our new tree $T^{(k+1)}$ so that it's contribution to $O$ is minimized. Here $L$ is a loss function that captures how well the model matches the training data and $\\Omega$ is a \"regularization function\" to prevent over-fitting.","a3af7a8b":"## Loss and regularization\nThe loss function used for classification problems is\n\n\\begin{align}\n    L(y,p) &= - \\ln ( p^y (1-p)^{1-y}) .\n\\end{align}\n\nThis is the negative-log likelyhood of the Bernoulli parameter $p$ given a measurement of the dependent variable $y$. Hence if $p$ seems likely given the observation $y$, the loss is lower, if $p$ seems unlikely given $y$, the loss is higher. We take the log since since then the errors found in multiple trials add up, that is, the log-likelihood of many observations is just the sum of the log-likelihoods of each observation individually.\n\nA good regularization function will be larger for \"more complicated\" trees, so that it's cost in the objective function prevents over-fitting.\nLet $t$ be the number of leaves in a tree $T$ and let $w_b$ be the numerical outputs of each leaf with $b$ labelling the leaves. The regularization term used is\n\n\\begin{align}\n    \\Omega ( T ) = \\gamma t + \\frac 1 2 \\lambda \\sum_{b=1}^t w^2_b\n\\end{align}\n\nfor some parameters $\\gamma$ and $\\lambda$ which must be specified. Adding this to the objective function makes the model tend to prefer trees with a smaller number of leaves and weight more concentrated in a few of them.","e7cee361":"# $k$-Nearest Neighbors\n\nFor $k$-nearet neighbors, the features may be continuous or categorical, in which case the feature is encoded by integers. The dependent variable $y$ is categorical and can lie in some set $C = \\{ 1, \\dots , N \\}$. Now suppose we are given a data point with feature vectore $\\mathbf x$. The $k$-nearest neighbors algorithm classifies this data point by comparing it with samples in the training data that are closest to it in some sense. When the features are continuous, we typically use the euclidean metric to measure distance. When they are categorical, a good measure is the Hamming distance, which simply adds up how many features are different. Given $k$, the algorithm computes which $k$ training points $\\mathbf x_{j_a}$, $a = 1,...,k$ are the closest to $\\mathbf x$ in $\\mathbb R^n$ and then makes a decision as to which class the test sample lies in by taking a weighted sum\n\n$$P(y= p | \\mathbf x) = \\sum_{a=1}^k w_a I_{p}(\\mathbf x_{j_a}) .$$\n\nHere $I_{p} (\\mathbf x_j )$ is an indicator function telling whether or not a given point $\\mathbf x_j$ from the training set has $y_j = p$ and $w_a$ is a probability measure so that $\\sum_{a=1}^k w_a = 1$. This then gives a probability that the point $x_i$ has $y = p$. The classifier then returns the category with the greatest probability.\n\nThere are many different weightings one may choose. The two most common are the uniform weighting where $w_a = 1\/k$ for all $a$ and the distance weighting where $w_a$ is proportial to the inverse of the euclidean distance of $\\mathbf x_{j_a}$ from $\\mathbf x$.\n\nThe distance measure is also highly customizable, for example, scaling the coordinates can make certain features more or less important. With categorical variables using the Hamming distance, we may want to weight certain features more than other. In more sophisticated applications we may want to \"learn\" the best distance measure for a model, but a good starting point is to scale all variables to have mean 0 and unit variance so the model weights them approximately euqally.\n\nOur source for details on the $k$-nearest neighbors algorithm can be found [here](https:\/\/scikit-learn.org\/stable\/modules\/neighbors.html#).\nWe will work through an example with continuous features and three-category classification originally found [here](https:\/\/scikit-learn.org\/stable\/auto_examples\/neighbors\/plot_classification.html#sphx-glr-auto-examples-neighbors-plot-classification-py).","2e9c3036":"## Linear support vector classifiers\n\nTo forumlate the optization problem, let the two categories by represented by $y= \\pm 1$ and let the hyperplane in question be defined by the equation\n\n$$ \\mathbf w^T \\mathbf x + b = 0$$\n\nwhere $\\mathbf w \\in \\mathbb R^n$ is the (un-normalized) normal vector to the plane. The marginal region is bounded by parallel hyperplanes\n\n\n$$ \\mathbf w^T \\mathbf x + b = 1$$\n$$ \\mathbf w^T \\mathbf x + b = -1$$\n\nThe first equation is satisfied by support vectors $\\mathbf x_j$ with $y_j =1$ and the second by support vectors with $y_j = -1$. Note that this parameterization fixes the normalization of $b$ and $\\mathbf w$, which are left unfixed by the first equation. The width of the marginal region is then $\\frac{2}{||\\mathbf w||}$.\n\n<img src=\"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/7\/72\/SVM_margin.png\" style=\"background-color:white\" width = \"500\">\n\nNow a training point $\\mathbf x_j$ of class $y_j$ crosses the planes bounding the marginal region into iff\n\n\\begin{align}\n    y_j ( \\mathbf w^T \\mathbf x_j + b ) = 1 - \\zeta_j\n\\end{align}\n\nfor some $\\zeta_j \\geq 0$. The distance it impinges is given by $\\frac{\\zeta_j}{|| \\mathbf w ||}$.\n\nHence we seek to minimize\n\n\\begin{align}\n    C \\sum_{j=1}^m \\zeta_j + \\frac 1 2 || \\mathbf w ||^2 ,\\nonumber \\\\\n    \\text{subject to} ~~ y_i ( \\mathbf w^T \\mathbf x_j + b ) \\geq 1 - \\zeta_j , \\nonumber \\\\\n    \\text{where} ~~ \\zeta_j \\geq 0 \n\\end{align}\n\nover $\\mathbf w, b, \\zeta_j$. Here $C$ is some constant controlling the relative cost of keeping the regions well separated and of having impinging points. This is the so called \"primal problem\".","92100ca6":"To get some more insight on how the model develops as we add more trees, we display plots of the error and logloss (or negative log-likelihood). The later is just the $L$ function introduced above, evaluated at each stage in the process. Note that `clf.evals_result()` is a dictionary whose first key, `'validation_0'` or `'validation_1'` indicates whether we are looking at the training or test set, and whose second key is one of the loss functions from `eval_metric`. Given these keys, the values are simply that of the given loss function at each round.","ef0028c7":"## The dual problem\n\nFor computational purposes it is apparently convenient to switch to the dual problem. For the details on how this works we refer to the \"Constraints, Lagrange Multipliers, and Duality\" note (those interested can message me). In the dual problem, rather than seeking to minimized a constrained lagrangian, we seek to maximize the dual lagrangian, which is a function of the lagrange multipliers that implement the constraints\n\n\\begin{align}\n    q(\\alpha) = \\sum_{j=1}^m \\alpha_j - \\frac 1 2 \\sum_{i=1}^m \\sum_{j=1}^m \\alpha_i \\alpha_j y_i y_j \\mathbf x^T_i \\mathbf x_j .\n\\end{align}\n\nSince the lagrange multipliers implement inequality rather than equality constraints, there are some constraints that still come along for the ride in the dual problem. These are\n\n\\begin{align}\n    0 \\leq \\alpha_j \\leq C ,\n    \\qquad \\qquad\n    \\sum_{j=1}^m \\alpha_j y_j = 0 .\n\\end{align}\n\nNon-zero $\\alpha_j$'s correspond to support vectors, that is, vectors at or within the marginal planes.\n\n\nOnce the maximum is found, the maximal-margin hyperplane can be retrieved via\n\n\\begin{align}\n    \\mathbf w = \\sum_{j=1}^m \\alpha_j y_j \\mathbf x_j\n\\end{align}\n\nand\n\n\\begin{align}\n\tb = y_j  - \\mathbf w^T \\mathbf x_j.\n\\end{align}\n\nThe equation for $b$ is for $j$ such that $0 < \\alpha_j < C$, that is, a support vector.","420f8745":"# XGBoost Classification\n\nFor more details on this algorithm see [here](https:\/\/xgboost.readthedocs.io\/en\/latest\/tutorials\/model.html). XGBoost and regular gradient boost are essentially the same algorithm, but XGBoost has regularization, more optimizations, and built in cross-validation. For a list of differences see [here](https:\/\/www.analyticsvidhya.com\/blog\/2016\/03\/complete-guide-parameter-tuning-xgboost-with-codes-python\/).\n\nAs usual, suppose we are given a training set $\\mathbf x_{j}$ with dependent variable $y_j \\in \\{ 0 , 1 \\}$ where $j = 1 , \\dots , m$ and the $\\mathbf x$'s lie in a n-dimensional feature space. In this algorithm, our goal is to create a rule that takes a given feature vector $\\mathbf x$ and outputs a probability $p_\\mathbf{x} \\in [0,1]$ that the result is a \"success\", that is, $y=1$.\n\nOur plan is to build many trees $T^{(k)}$. Each tree will produces a numerical output for a given feature vector $\\mathbf x$, which we denote $f^{(k)}(\\mathbf x)$. The tree ensemble will be built iteratively with each tree correcting the result for $p_\\mathbf{x}$ produced by all the previous trees. As such, each tree is not indidually interpretretable as it is say in a random forest. Each tree adds it's own numerical contribution to the log-odds of a positive result\n\n\\begin{align}\n    l = \\ln \\left( \\frac p {1-p} \\right).\n\\end{align}\n\nWe add these contributions to log-odds rather than the probability since this is valued in $\\mathbb R$, not $[0,1]$, so we don't have to worry about leaving a certain region.\nThus we get a sequence of predictions\n\n\\begin{align}\n    l^{(0)}_\\mathbf{x} &= 0 , \\nonumber \\\\\n    l^{(1)}_\\mathbf{x} &= l^{(0)}_\\mathbf{x} + \\epsilon f^{(1)} ( \\mathbf x), \\nonumber \\\\\n    &\\qquad \\cdots \\nonumber \\\\\n    l^{(k)}_\\mathbf{x} &= l^{(k-1)}_\\mathbf{x} + \\epsilon f^{(k)}(\\mathbf x ) = \\epsilon \\sum_{a = 1}^k f^{(a)} ( \\mathbf x )\n\\end{align}\n\nwith better and better accuracy (we will discuss how to measure accuracy in a bit). ","6cd07f55":"# <div id='logit'>Logistic Regression<\/div>\n\n\nLogistic regression models are used to make binary categorical predictions $Y \\in \\{0,1\\}$. The basic idea is we model $Y$ as a Bernoulli random variable with some probability $p$ for 'success' 1\n\n$$P(y)=p^y(1-p)^{1-y} .$$\n\nThe parameter $p$ will then depend on the features $x_i$. These independent variables may be either continuous or categorical, but if they are categorical they should be binary. Our goal is to then find some reasonable function $p(x_i)$. A good starting point is\n\n$$\\ln \\left( \\frac{p}{1-p} \\right) = \\beta_0 + \\beta_1 x_1 + \\cdots + \\beta_n x_n$$\n\nthat is, the log-odds are linear in the features. $p$ as a function of the log-odds is shown below","ec48da8d":"Here we demonstrate the probability that the model associates to each sample being in a particular class. It's worth playing around with $k$ and the weighting to see what this does to the probabilities (for instance, for $k=1$, the probability will always be 0 or 1). In the end, the model selects the class with the greatest probability","d2f0036a":"## Example\n\nWe again use the iris dataset and try to build a classifier using a support vector classifier that predicts the species of an iris based off of only the first two features, the sepal length and sepal width. Below we load the data","c87f5821":"And now train the model and make predictions","8740eebc":"We create a support vector classifier and train it on the data prepared above. The kernel is specified to be linear so that we use the linear classification discussed above. We will return to this later to better understand the non-linear kernels, but for now, one can play with some of the other opetions. It's also worth playing around with the relative cost parameter $C$ introduced above. Since we have a tri-partite classification problem, we need to specify how the planes are chosen. We choose a \"one-versus-rest\" proceedure as discussed above (this is also the default).\n\nFirst let's get the data ready","79bc5ef4":"This is a reasonable formula since both the left and right hand sides are unbounded in $\\mathbb R$, however, the linearity assumption is a big one. We can solve for $p$\n\n$$p = \\frac{1}{1 + e^{- \\beta^T x}}.$$\n\nwhere we have denoted $x= ( 1 ~ x_1 ~\\dots~ x_n)^T$ and $\\beta = ( \\beta_0 ~ \\beta_1 ~ \\dots ~ \\beta_n)^T$.\n\nA choice of $\\beta$ that best fits the data is given by minimizing the log-likelihood of the set of observations provided in the training set. See the [documentation](https:\/\/scikit-learn.org\/stable\/modules\/linear_model.html#logistic-regression) for more details along with a discussion of regularization techniques to prevent overfitting. These are important but we will add discussion on this at a later date.","df63e7fd":"## Gaussian Naive Bayes\n\nThe above example was performed without any assumptions on the form of the conditional distributions $P(x_i|C_k)$. Rather, we calculated these conditional probabilities using the training data. This was easy to do since the feature data was discrete with few classes. However, in many cases this will not be so easy. For instance, suppose we are given the training data below (this example is pulled from [here](https:\/\/chrisalbon.com\/machine_learning\/naive_bayes\/naive_bayes_classifier_from_scratch\/)) and wish to use a naive Bayes classifier to predict the gender of a person of given height, weight, and foot size. Note these are not anticipated to be statistically independent, so naive Bayes may be a poor model, but it works for instructional purposes. Here 0 means male and 1 female","1b8f1671":"Now let's load some packages that we will be using throughout","897b7c15":"## The Algorithm\n\nLet $x_{ij}$ for $i = 1 , . . . , n$ and $j=1,...,m$ be a set of $n$ numerical features for $m$ (training) data points. For binary categorical features $i$, $x_{ij}$ is a 0 or a 1 for for all data points $j$. Let $y_j$ for $j=1,...,m$ be the dependent binary categorical variable for each sample, again encoded as a 0 or 1. The proceedure is to go through the data and select features $i$ and \"splits\" in that feature $\\theta_i$ such that there is maximum \"information gain\" in the split according to some measure.\n\nMore precisely, supose we have are given a binary tree. If $Q \\subseteq \\{ 1,...,m\\}$ is a subset of samples present at some leaf, and $\\theta = (i,\\theta_i)$ is a given split, then we create two braches, one leading to $Q_\\text{left} (\\theta)$ and the other to $Q_\\text{right}(\\theta)$ where\n\n$$Q_\\text{left} (\\theta) = \\{ j \\in Q | x_{ij} \\leq \\theta_i \\} ,$$\n$$Q_\\text{right} (\\theta)= \\{ j \\in Q | x_{ij} > \\theta_i \\} .$$\n\nWe then evaluate \"gain\", or the weighted average\n\n$$G(Q,\\theta)= H(Q) - \\frac{|Q_\\text{left} (\\theta)|}{|Q|} H (Q_\\text{left} (\\theta)) - \\frac{|Q_\\text{right} (\\theta)|}{|Q|} H (Q_\\text{right} (\\theta))$$\n\nof some attribute selection measure (AMS) $H(Q)$, discussed below. We iterate through all splitings (we will not discuss the details of how these are chosen), choose the splitting $\\theta = (i , \\theta_i)$ that maximized $G(Q,\\theta)$ and create a new tree where the leaf $Q$ is replaced by this binary node.\n\nTo create the tree, we start with a single \"root\" node $Q = \\{ 1,...,m\\}$ that contains the entire data set and apply the above algorithm iteratively until we\n\n* run out of features\n* every leaf is homogeneous, i.e. the data contained in each leaf is all of a single class\n* a pruning condition has been reached (to be discussed)","8f741ad3":"## The kernel trick\n\nFrom the dual problem, it is clear that the problem only depends on the so called Gram matrix, the matrix of inner products $x^T_i x_j$. This is nice because there is (apparently) a large computational cost to computing dot products, and we do not have to search over more than $m^2$. This is also a useful point of generalization for building non-linear support vector classifiers, that is classifiers without hyper-plane boundaries.\n\nThe way this is done is the so-called kernel trick, where the Gram matrix is replaced in the dual problem with some symmetric \"kernel\" $K_(\\mathbf x_i , \\mathbf x_j )$. We then solve the same primal problem and retrieve the maximal-margin hyperplane via\n\\begin{align}\n    y = \\sum_{j=1}^m \\alpha_j y_j K ( \\mathbf x_j , \\mathbf x ) + b\n\\end{align}\n\nwhere\n\n\\begin{align}\n\tb = y_j  - \\sum_{k=1}^m \\alpha_k y_k K ( \\mathbf x_k , \\mathbf x_j ).\n\\end{align}\n\nfor some support vector $j$.\n\nThe idea behind the kernel is that it is an inner product in an auxiliary feature space $K_(\\mathbf x_i , \\mathbf x_j ) = \\phi ( \\mathbf x_i )^T \\phi ( \\mathbf x_j)$ where $\\phi$ is a non-linear map into a possibly higher-dimensional space. If a map can be found that better separates the classes linearly, we then carry out the linear support vector algorithm there. A large computational savings comes from realizing we do not need to perform this map explicity, we only need to know $K$ itself. Some common kernels built into `sklearn` are:\n\n* `linear`: $K_(\\mathbf x_i , \\mathbf x_j ) = \\mathbf x_i^T \\mathbf x_j$\n* `poly`: $K_(\\mathbf x_i , \\mathbf x_j ) = (\\gamma x_i^T \\mathbf x_j + r )^d$\n* `rbf`: $K_(\\mathbf x_i , \\mathbf x_j ) = \\exp \\left(- \\gamma || x_i - x_j ||^2_2 \\right)$\n* `sigmoid`: $K_(\\mathbf x_i , \\mathbf x_j ) = \\tanh \\left(\\gamma || x_i - x_j ||^2_2 + r \\right)$\n\nHere $\\gamma$ is always set by parameter `gamma`, $r$ by `coef0` and $d$ by `degree`. Different kernels will be good at creating classification boundaries of different shapes, but we have not investigated how they perform yet.\n"}}