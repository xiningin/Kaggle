{"cell_type":{"ac051776":"code","1210dad2":"code","ab0d0915":"code","388f7ce9":"code","81032793":"code","dc09b00b":"code","80df0566":"code","33f7c5df":"code","c91af759":"code","8da6a675":"code","b8663241":"code","55392ee2":"code","4f300d11":"code","35b7086d":"code","667da8ad":"code","ccf4c6a1":"code","7f0be5c1":"code","55dfb2a1":"code","45ac7614":"code","b238fa3b":"code","0dce9f28":"code","19a17c4b":"code","f69d7ce0":"code","e02ad2f6":"code","1043c11b":"code","47a6026c":"code","af240798":"code","565cd537":"code","5272232d":"code","d215ee93":"code","a16b42b5":"code","f0c47d16":"code","93e8de14":"code","665a94d1":"code","407ef409":"code","8b83f1e9":"code","191b343a":"code","db6137e1":"code","81cf13f4":"code","3be0f5f5":"code","a7d88cce":"code","edde7b73":"code","fd58c124":"code","b6bae535":"code","db0649b6":"code","47080b05":"code","04915700":"code","8f9df1fc":"code","c02e705a":"code","f71601c6":"code","7d8bb987":"code","4c5cccd1":"code","8f57e278":"code","6a1d3264":"code","fbf4d01a":"code","fcba3e80":"code","881f14c9":"code","39d04f72":"code","468615ac":"code","d5da5f21":"code","1e0a8a90":"code","1ff27f51":"code","70df0db8":"code","ed323f0d":"code","98e040e7":"code","aba6d704":"code","28916f60":"code","a8b5072c":"code","18733db1":"code","f9ec0b5a":"code","0c912a20":"code","3f180ff1":"code","73b3f6e7":"code","a3c8f787":"code","2776341d":"code","eb895b38":"code","9ed890ac":"code","2504dbfb":"code","df05afcd":"code","46c0878c":"code","ad94f2aa":"code","27800bba":"code","87a286b6":"code","696ea664":"code","59142092":"code","ab19173e":"code","42cf2c3c":"code","c7ed5b46":"code","04c5ba11":"code","ebc6d3fd":"code","2a5f4d86":"code","0109ce13":"code","bba7d4cc":"code","124e9f38":"code","3bf08822":"code","9f3d86c4":"code","f3d9582c":"code","4d558228":"code","b3729775":"code","d91024a7":"code","1896fc51":"code","0e43a02b":"code","9f6a2412":"code","d56888f0":"code","25f46965":"code","4e9c1b33":"code","5986c0d4":"code","ab42d340":"code","291b60e4":"code","fa570b98":"code","41b64760":"code","f8966811":"code","20597afa":"code","4ca62b33":"code","c7df157c":"code","578d25fd":"code","5e904ec9":"code","4fcf57bb":"code","7ea832a2":"code","9ae2a384":"code","0733fc5d":"code","4d5aef8d":"code","f3f5abe1":"code","c5e34755":"code","edda926b":"code","666b75f0":"code","dc1f923a":"code","857e9c36":"code","876b458c":"code","fdbe6fd4":"code","4e5fd471":"code","b5266677":"code","1fe7d918":"code","91ae5110":"code","529db471":"code","a290fd8c":"code","a3a761a3":"code","a8b42133":"code","c7763caa":"code","64109af6":"code","d19f99a6":"code","45d65813":"code","763d38a8":"code","0bb61d52":"code","b297f14d":"code","7cda1588":"code","9465d029":"code","8fb7bece":"code","db1404d8":"code","d03c49ec":"code","fb90e044":"code","1e1da33d":"code","8e341e51":"code","5a08b8ff":"code","034f5d2b":"code","2f63565b":"code","c165415a":"markdown","9753a35e":"markdown","8fcfb490":"markdown","9337c192":"markdown","02d107a7":"markdown","b0e0be85":"markdown","d2f3d1ff":"markdown","e36f58af":"markdown","d1785e25":"markdown","050a25b3":"markdown","7b2eca6b":"markdown","f14f840d":"markdown","57bb34ac":"markdown","d30a49d0":"markdown","0bee5204":"markdown","daabc136":"markdown","3e793f2d":"markdown","7ac06235":"markdown","f0d0ed28":"markdown","9818c90e":"markdown","47305b9b":"markdown","47184d17":"markdown","5f8c9a34":"markdown"},"source":{"ac051776":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","1210dad2":"#importing libraries \n\n\n# Data Manipulation\nimport numpy as np\nimport pandas as pd\n\n# Feature selection\nfrom sklearn.feature_selection import VarianceThreshold\n\n\n# Visualization \nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# for Q-Q plots\nimport scipy.stats as stats\n\n#import model libraries\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\n#import model\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.svm import SVR\nfrom xgboost import XGBRegressor\n\n\n\n#import accuracy\nfrom sklearn import metrics\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\n\n\n\nimport warnings\nwarnings.filterwarnings('ignore')","ab0d0915":"# load data\n\ntrain=pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')","388f7ce9":"train.drop(columns=['Id'], inplace= True)","81032793":"train.head()","dc09b00b":"train.shape","80df0566":"train.info()","33f7c5df":"#encoding categorical variable\n\nenc_ordCat= { 'LotShape': {'Reg': 3 , 'IR1': 2 , 'IR2': 1 , 'IR3': 0 },\n              'LandSlope': {'Gtl': 2 , 'Mod': 1 , 'Sev': 0},\n              'ExterQual': {'Ex': 4 , 'Gd': 3 , 'TA': 2 , 'Fa': 1 , 'Po': 0},\n              'ExterCond': {'Ex': 4 , 'Gd': 3 , 'TA': 2 , 'Fa': 1 , 'Po': 0},\n              'BsmtQual': {'Ex': 5 , 'Gd': 4 , 'TA': 3 , 'Fa': 2 , 'Po': 1 , 'NA': 0},\n              'BsmtCond': {'Ex': 5 , 'Gd': 4 , 'TA': 3 , 'Fa': 2 , 'Po': 1 , 'NA': 0},\n              'BsmtExposure': {'Gd': 4 , 'Av': 3 , 'Mn': 2 , 'No': 1 , 'NA': 0},\n              'BsmtFinType1': {'GLQ': 6 , 'ALQ': 5 , 'BLQ': 4 , 'Rec': 3 , 'LwQ': 2 , 'Unf': 1 , 'NA': 0},\n              'BsmtFinType2': {'GLQ': 6 , 'ALQ': 5 , 'BLQ': 4 , 'Rec': 3 , 'LwQ': 2 , 'Unf': 1 , 'NA': 0},\n              'HeatingQC': {'Ex': 4 , 'Gd': 3 , 'TA': 2 , 'Fa': 1 , 'Po': 0},\n              'CentralAir': {'Y': 1 , 'N': 0},\n              'KitchenQual': {'Ex': 4 , 'Gd': 3 , 'TA': 2 , 'Fa': 1 , 'Po': 0},\n              'Functional': {'Typ': 7, 'Min1': 6 , 'Min2': 5 , 'Mod': 4 , 'Maj1': 3 , 'Maj2': 2 , 'Sev': 1 , 'Sal': 0},\n              'FireplaceQu': {'Ex': 5 , 'Gd': 4 , 'TA': 3 , 'Fa': 2 , 'Po': 1 , 'NA': 0},\n              'GarageType': {'2Types': 6 , 'Attchd': 5 , 'Basment': 4 , 'BuiltIn': 3 , 'CarPort': 2 , 'Detchd': 1 , 'NA': 0},\n              'GarageFinish': {'Fin': 3 , 'RFn': 2 , 'Unf': 1 , 'NA': 0 },\n              'GarageQual': {'Ex': 5 , 'Gd': 4 , 'TA': 3 , 'Fa': 2 , 'Po': 1 , 'NA': 0},\n              'GarageCond': {'Ex': 5 , 'Gd': 4 , 'TA': 3 , 'Fa': 2 , 'Po': 1 , 'NA': 0},\n              'PavedDrive': {'Y': 2 , 'P': 1 , 'N': 0}\n}","c91af759":"li=list(enc_ordCat.keys())\nli","8da6a675":"train=train.replace(enc_ordCat)","b8663241":"train[li] = train[li].apply(pd.to_numeric)\n","55392ee2":"train_cat=list(train.columns[train.dtypes=='object'])\ntrain_num=list(train.columns[train.dtypes!='object'])","4f300d11":"train[train_cat].head(1)","35b7086d":"train_num","667da8ad":"train[train_num].isnull().sum().sort_values(ascending=False)","ccf4c6a1":"# 1. FireplaceQu \n\nplt.scatter(x=train['FireplaceQu'], y=train['SalePrice'], alpha=0.5)\n\n\n# Decorate\nplt.title('Fire place Quality before replacing missing value')\nplt.xlabel('FireplaceQu')\nplt.ylabel('SalePrice')\nplt.show()","7f0be5c1":"train.FireplaceQu.value_counts()","55dfb2a1":"miss_val = np.where(train['FireplaceQu'].isnull(), 0 ,None)","45ac7614":"plt.scatter(x=train['FireplaceQu'],y=train['SalePrice'], color='blue', marker=\"+\", label='old')\nplt.scatter(x=miss_val,y=train['SalePrice'],  marker=\"*\", color='red', label='latest')\n\n\nplt.title('Fire place Quality before replacing missing value')\nplt.xlabel('FireplaceQu')\nplt.ylabel('SalePrice')\nplt.legend(loc='upper left');\nplt.show()","b238fa3b":"# 2. LotFrontage\n\nplt.scatter(train['LotFrontage'], train['SalePrice'], alpha=0.5)\n\n# Decorate\nplt.title('LotcFrontage before replacing missing value')\nplt.xlabel('LotFrontage')\nplt.ylabel('SalePrice')\nplt.show()","0dce9f28":"train['LotFrontage'].mean()","19a17c4b":"miss_val=np.where(train['LotFrontage'].isnull(),train['LotFrontage'].mean(),None)","f69d7ce0":"# most of the data is in between 50 to 100, lets replace the missing value with mean.\n\ntrain['LotFrontage']=train['LotFrontage'].fillna(train['LotFrontage'].mean())","e02ad2f6":"plt.scatter(x=train['LotFrontage'],y=train['SalePrice'], color='blue', marker=\"+\", label='old')\nplt.scatter(x=miss_val,y=train['SalePrice'],  marker=\"*\", color='red', label='latest')\n\n\n# Decorate\nplt.title('LotcFrontage before replacing missing value')\nplt.xlabel('LotFrontage')\nplt.ylabel('SalePrice')\nplt.legend(loc='upper left');\nplt.show()","1043c11b":"# 3. GarageFinish\n\nplt.scatter( train['GarageFinish'], train['SalePrice'], alpha=0.5)\n\n# Decorate\nplt.title('Garage Finish before replacing missing value')\nplt.xlabel('GarageFinish')\nplt.ylabel('SalePrice')\nplt.show()","47a6026c":"train['GarageFinish'].mean()","af240798":"miss_val=np.where(train['GarageFinish'].isnull(),train['GarageFinish'].mean(),None)","565cd537":"sns.countplot(train['GarageFinish'])","5272232d":"# Lets replae with mean value\n\ntrain['GarageFinish']=train['GarageFinish'].fillna(train['GarageFinish'].mean())","d215ee93":"plt.scatter(x=train['GarageFinish'],y=train['SalePrice'], color='blue', marker=\"+\", label='old')\nplt.scatter(x=miss_val,y=train['SalePrice'],  marker=\"*\", color='red', label='latest')\n\n\n# Decorate\nplt.title('Garage Finish before replacing missing value')\nplt.xlabel('GarageFinish')\nplt.ylabel('SalePrice')\nplt.legend(loc='upper left');\nplt.show()","a16b42b5":"# 4. GarageYrBlt\n\nplt.scatter(train['GarageYrBlt'], train['SalePrice'], alpha=0.5)\n\n# Decorate\nplt.title('GarageYrBlt before replacing missing value')\nplt.xlabel('GarageYrBlt')\nplt.ylabel('SalePrice')\nplt.show()","f0c47d16":"train['GarageYrBlt'].mode()","93e8de14":"train['GarageYrBlt'].mean()","665a94d1":"plt.scatter(train['YearBuilt'], train['SalePrice'], alpha=0.5)","407ef409":"plt.scatter(train['GarageYrBlt'],train['YearBuilt'])","8b83f1e9":"train[train['GarageYrBlt'].isnull()][['GarageYrBlt','YearBuilt']]","191b343a":"miss_val=np.zeros(train.shape[0])","db6137e1":"indx=list(train[train['GarageYrBlt'].isnull()][['GarageYrBlt','YearBuilt']].index)","81cf13f4":"miss_val==0","3be0f5f5":"for i in indx:\n    miss_val[i]=train['YearBuilt'][i]\n    train['GarageYrBlt'][i]=train['YearBuilt'][i]\n   ","a7d88cce":"miss_val=np.where(miss_val==0, None, miss_val)","edde7b73":"plt.scatter(x=train['GarageYrBlt'],y=train['SalePrice'], color='blue', marker=\"+\", label='old')\nplt.scatter(x=miss_val,y=train['SalePrice'],  marker=\"*\", color='red', label='latest')\n\n\n# Decorate\nplt.title('GarageYrBlt after replacing missing value')\nplt.xlabel('GarageYrBlt')\nplt.ylabel('SalePrice')\nplt.legend(loc='upper left');\nplt.show()","fd58c124":"train[train_num].isnull().sum().sort_values(ascending=False)","b6bae535":"# 'GarageType'\n\ntrain.GarageType.value_counts()","db0649b6":"plt.scatter(train['GarageType'],train['SalePrice'], alpha=0.5)\n\n# Decorate\nplt.title('GarageType after replacing missing value')\nplt.xlabel('GarageType')\nplt.ylabel('SalePrice')\nplt.show()","47080b05":"train['GarageType'].mean()","04915700":"miss_val=np.where(train['GarageType'].isnull(),train['GarageType'].mode()[0] ,None)","8f9df1fc":"# lets replace with mode()\n\ntrain['GarageType']=train['GarageType'].fillna(train['GarageType'].mode()[0])","c02e705a":"plt.scatter(x=train['GarageType'],y=train['SalePrice'], color='blue', marker=\"+\", label='old')\nplt.scatter(x=miss_val,y=train['SalePrice'],  marker=\"*\", color='red', label='latest')\n\n\n# Decorate\nplt.title('GarageType after replacing missing value')\nplt.xlabel('GarageType')\nplt.ylabel('SalePrice')\nplt.legend(loc='upper left');\nplt.show()","f71601c6":"## 'GarageQual'\n\nplt.scatter(train['GarageQual'],train['SalePrice'], alpha=0.5)\n\n# Decorate\nplt.title('GarageQual after replacing missing value')\nplt.xlabel('GarageQual')\nplt.ylabel('SalePrice')\nplt.show()","7d8bb987":"train.GarageQual.value_counts()","4c5cccd1":"# lets replace with mode()\n\ntrain['GarageQual']=train['GarageQual'].fillna(train['GarageQual'].mode()[0])","8f57e278":"# GarageCond\n\ntrain.GarageCond.value_counts()","6a1d3264":"train['GarageCond']=train['GarageCond'].fillna(train['GarageCond'].mode()[0])","fbf4d01a":"# BsmtExposure\n\ntrain.BsmtExposure.value_counts()","fcba3e80":"train['BsmtExposure']=train['BsmtExposure'].fillna(train['BsmtExposure'].mode()[0])","881f14c9":"# BsmtFinType2\n\ntrain.BsmtFinType2.value_counts()","39d04f72":"train['BsmtFinType2']=train['BsmtFinType2'].fillna(train['BsmtFinType2'].mode()[0])","468615ac":"# BsmtCond\n\ntrain.BsmtCond.value_counts()","d5da5f21":"train['BsmtCond']=train['BsmtCond'].fillna(train['BsmtCond'].mode()[0])","1e0a8a90":"train[train_num].isnull().sum().sort_values(ascending=False)","1ff27f51":"# BsmtQual\n\ntrain['BsmtQual'].value_counts()","70df0db8":"train['BsmtQual'].mean()","ed323f0d":"train['BsmtQual']=train['BsmtQual'].fillna(train['BsmtQual'].mean())","98e040e7":"# BsmtFinType1\n\ntrain['BsmtFinType1'].value_counts()","aba6d704":"train['BsmtFinType1'].mean()","28916f60":"train['BsmtFinType1']=train['BsmtFinType1'].fillna(train['BsmtFinType1'].mean())","a8b5072c":"# MasVnrArea\n\nplt.scatter(train['MasVnrArea'], train['SalePrice'], alpha=0.5)\n\n# Decorate\nplt.title('MasVnrArea after replacing missing value')\nplt.xlabel('MasVnrArea')\nplt.ylabel('SalePrice')\nplt.show()","18733db1":"train['MasVnrArea'].mean()","f9ec0b5a":"train['MasVnrArea']=train['MasVnrArea'].fillna(train['MasVnrArea'].mean())","0c912a20":"train[train_num].isnull().sum().sort_values(ascending=False)","3f180ff1":"train[train_cat].isnull().sum().sort_values(ascending=False)","73b3f6e7":"miss_feat= list(train.columns[train.isnull().any()])","a3c8f787":"miss_feat","2776341d":"## replace all the missin value with text 'missing'\n\nfor col in miss_feat:\n    train[col]=np.where(train[col].isnull(),'missing', train[col])","eb895b38":"train.isnull().sum().sort_values(ascending=False)","9ed890ac":"train.shape","2504dbfb":"column=train.columns","df05afcd":"column","46c0878c":"def similar_feat(arr,st):\n    \n    li=[]\n    \n    for ele in arr:\n        if st in ele:\n            li.append(ele)\n    return li","ad94f2aa":"basement=similar_feat(column,'Bsmt')\nprint(basement)","27800bba":"def print_value_count(feat):\n    \n    for ele in feat:\n        print('the different value of {} is {}'.format(ele, train[ele].nunique()))\n        print(train[ele].value_counts(),'\\n')\n        ","87a286b6":"print_value_count(basement)","696ea664":"def add_feat(feat,feat_list):\n    \n    train[feat]=pd.Series([0]*1460)\n    for ele in feat_list:\n        train[feat]=train[feat]+ train[ele]","59142092":"add_feat('BsmtScore',['BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1',  'BsmtFinType2', 'BsmtFullBath', 'BsmtHalfBath'])","ab19173e":"train[['BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1',  'BsmtFinType2', 'BsmtFullBath', 'BsmtHalfBath','BsmtScore']]","42cf2c3c":"# Now for the features related to Basement area, we can observe that (BsmtFinSF1 + BsmtFinSF2) + BsmtUnfSF = TotalBsmtSF\n\n# So here only keeping BsmtUnfSF , TotalBsmtSF would be enough , i mean keeping any 2 of these features should suffice.","c7ed5b46":"train[['BsmtFinSF1','BsmtFinSF2','BsmtUnfSF','TotalBsmtSF']]","04c5ba11":"# Lets drop the remaining features for Basement.\n\ntrain.drop(['BsmtFinSF1','BsmtFinSF2','BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', 'BsmtUnfSF', 'TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath'], axis=1 , inplace=True)","ebc6d3fd":"# Similary Garage have also many number of features\n\ngarage=similar_feat(column,'Garage')\nprint(garage)","2a5f4d86":"def print_value_count(feat):\n    \n    for ele in feat:\n        print('the different value of {} is {}'.format(ele, train[ele].nunique()))\n        print(train[ele].value_counts(),'\\n')\n        ","0109ce13":"print_value_count(garage)","bba7d4cc":"add_feat('GarageScore',['GarageType', 'GarageFinish', 'GarageCars', 'GarageQual', 'GarageCond'])","124e9f38":"train[['GarageType', 'GarageFinish', 'GarageCars', 'GarageQual', 'GarageCond','GarageScore']]","3bf08822":"train.drop(['GarageType', 'GarageFinish', 'GarageCars', 'GarageQual', 'GarageCond'],axis=1, inplace=True)","9f3d86c4":"train.columns","f3d9582c":"# combine ExterCond and ExterQual to one feature\n\nadd_feat('ExtrScore',['ExterCond', 'ExterQual'])","4d558228":"train[['ExterCond', 'ExterQual','ExtrScore']]","b3729775":"# combine ExterCond and ExterQual to one feature\n\nadd_feat('OvrallScore',['OverallQual', 'OverallCond'])","d91024a7":"train[['OverallQual', 'OverallCond','OvrallScore']]","1896fc51":"train.drop(['OverallQual', 'OverallCond','ExterQual','ExtrScore'], axis=1, inplace=True)","0e43a02b":"train.columns","9f6a2412":"# Similary Garage have also many number of features\n\nLot=similar_feat(column,'Lot')\nprint(Lot)","d56888f0":"def print_value_count(feat):\n    \n    for ele in feat:\n        print('the different value of {} is {}'.format(ele, train[ele].nunique()))\n        print(train[ele].value_counts(),'\\n')\n        ","25f46965":"print_value_count(Lot)","4e9c1b33":"## these above features cannot be combine","5986c0d4":"train.columns","ab42d340":"train.Fireplaces.value_counts()","291b60e4":"train_cat=list(train.columns[train.dtypes=='object'])\ntrain_num=list(train.columns[train.dtypes!='object'])","fa570b98":"# Checking ZERO variance for categorical features\n\n# to find variables that contain only 1 label\/value\n# we use the nunique() method from pandas, which returns the number\n# of different values in a variable.\n\nconstant_feat_cat = [feat for feat in train[train_cat].columns if train[train_cat][feat].nunique() == 1]\n\nconstant_feat_cat","41b64760":"# Checking ZERO variance for numerical features\n\n\n# This method works for only numeric features.\n# short and easy: find constant features\n\n# in this dataset, all features are numeric,\n\n\nconstant_feat_num = [feat for feat in train[train_num].columns if train[train_num][feat].std() == 0]\n\nconstant_feat_num","f8966811":"len(train_num)","20597afa":"# method 1:\n\nsel = VarianceThreshold(threshold=0.05)  \n\nsel.fit(train[train_num])  # fit finds the features with low variance","4ca62b33":"sel.get_support()","c7df157c":"col_to_drop=train[train_num].columns[~sel.get_support()][0]","578d25fd":"col_to_drop","5e904ec9":"train.drop(col_to_drop,axis=1,inplace=True)","4fcf57bb":"# Method 2: find the columns which have more than 99% same value.\n\ndef find_low_var_col(df,threshold):\n    low_var_col=[]\n    row=df.shape[0]\n    for col in df.columns:\n        mode_count=(df[col]==(df[col].mode()[0])).sum()\n        if ((mode_count\/row)*100) > threshold :\n            low_var_col.append(col)\n            \n    return low_var_col","7ea832a2":"train_low_var=find_low_var_col(train, 99)","9ae2a384":"train_low_var","0733fc5d":"train.drop(train_low_var,axis=1,inplace=True)","4d5aef8d":"train.shape","f3f5abe1":"train_cat=list(train.columns[train.dtypes=='object'])\ntrain_num=list(train.columns[train.dtypes!='object'])","c5e34755":"# check for duplicated features in the training set\ndef find_duplicate_feat(df):\n \n        duplicated_feat = []\n        for i in range(0, len(df.columns)):\n\n            col_1 = df.columns[i]\n\n            for col_2 in df.columns[i + 1:]:\n                if (col_2 not in duplicated_feat) and (df[col_1].equals(df[col_2])):\n                    duplicated_feat.append(col_2)\n\n        return duplicated_feat","edda926b":"train_num_dup=find_duplicate_feat(train[train_num])","666b75f0":"train_num_dup","dc1f923a":"train_cat_dup=find_duplicate_feat(train[train_cat])","857e9c36":"train_cat_dup","876b458c":"plt.figure(figsize=(40,40))\nsns.heatmap(train.corr(),annot=True, mask=np.triu(train.corr()))\nplt.ylim(40,0)","fdbe6fd4":"# find list of high correlated features with other features. to overcome multicollinearity\n\ndef high_corr_feat():\n    \n    feat = set()  # Set of all features which are highely correlated to other feature.\n    corr_matrix = train.corr()\n    \n    for i in range(1,len(corr_matrix.columns)):\n        for j in range(i):\n    \n            if ((corr_matrix.iloc[i, j]) > 0.8) or ((corr_matrix.iloc[i, j])<-0.4):\n                colname = corr_matrix.columns[i]  # getting the name of column\n                feat.add(colname)\n    \n    return list(feat)","4e5fd471":"train_hgh_corr_col=high_corr_feat()","b5266677":"train_hgh_corr_col","1fe7d918":"train.drop(train_hgh_corr_col, axis=1, inplace=True)","91ae5110":"train.head(2)","529db471":"# drop dependent feature.\n\ny=train['SalePrice']\n\ntrain=train.drop('SalePrice', axis=1)","a290fd8c":"# function to create distribution, histogram, Q-Q plot and boxplot\n\n\ndef diagnostic_plots(df):\n    # function takes a dataframe (df) and\n    # the list of variables of interest as arguments\n\n    col=df._get_numeric_data().columns\n    \n    for ele in col:\n        \n        # define figure size\n        plt.figure(figsize=(16, 4))\n        \n        # distribution\n        plt.subplot(1, 4, 1)\n        sns.distplot(df[ele])\n        plt.title('Distribution')\n        \n        # histogram\n        plt.subplot(1, 4, 2)\n        sns.histplot(df[ele], bins=30)\n        plt.title('Histogram')\n\n        # Q-Q plot\n        plt.subplot(1, 4, 3)\n        stats.probplot(df[ele], dist=\"norm\", plot=plt)\n        plt.ylabel('Variable quantiles')\n\n        # boxplot\n        plt.subplot(1, 4, 4)\n        sns.boxplot(y=df[ele])\n        plt.title('Boxplot')\n        \n        \n        plt.show()\n\n    ","a3a761a3":"diagnostic_plots(train)","a8b42133":"\ndef print_skew(df):\n    \n    df=df._get_numeric_data()\n    for col in df.columns:\n        print(col,' ', df[col].skew())","c7763caa":"print_skew(train)","64109af6":"# Remove skewness\n\n# for normal symmetry the skewness should be fairly betwen -0.5 to 0.5\n\ndef remove_skew(df):\n    \n    col=df._get_numeric_data().columns\n    \n    for ele in col:\n        \n        if abs(df[ele].skew())>0.75:\n            df[ele]=np.log(df[ele]+1)\n","d19f99a6":"remove_skew(train)","45d65813":"print_skew(train)","763d38a8":"diagnostic_plots(train)","0bb61d52":"def mod_outlier(df):\n        \n        num_col = df._get_numeric_data().columns\n        \n        for col in num_col:\n\n            #q1 = df[col].quantile(0.25)\n            #q3 = df[col].quantile(0.75)\n\n            iqr = df[col].quantile(0.75) - df[col].quantile(0.25)    # iqr= q3-q1\n\n            lower_bound = df[col].quantile(0.25) -(2 * iqr) \n            upper_bound = df[col].quantile(0.75) +(2 * iqr)\n\n            df[col]= np.where(df[col] > upper_bound, upper_bound , np.where(df[col] < lower_bound, lower_bound, df[col]))    \n","b297f14d":"mod_outlier(train)","7cda1588":"train=pd.get_dummies(train, drop_first=True, columns=train_cat)","9465d029":"X_train, X_test, y_train, y_test = train_test_split(train, y, test_size = 0.2, random_state = 43)","8fb7bece":"#Common for all model\n\n#feature scaling\n\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)\n","db1404d8":"def accuracy(yt,yp):\n    return round(metrics.r2_score(yt, yp)*100 , 2)\n\n\nmodel_acc={}","d03c49ec":"regressor = RandomForestRegressor(n_estimators = 100, random_state = 43)\nregressor.fit(X_train, y_train)\n\ny_pred = (regressor.predict(X_test))\n\nmodel_acc['Random Forest']=accuracy(y_test, y_pred)","fb90e044":"regressor = DecisionTreeRegressor(random_state = 43)\nregressor.fit(X_train, y_train)\n\ny_pred = (regressor.predict(X_test))\n\nmodel_acc['Decision Tree']=accuracy(y_test, y_pred)","1e1da33d":"regressor = LinearRegression()\nregressor.fit(X_train, y_train)\n\ny_pred = (regressor.predict(X_test))\n\nmodel_acc['MultipleRegression Module']=accuracy(y_test, y_pred)","8e341e51":"regressor = SVR(kernel = 'linear')\nregressor.fit(X_train, y_train)\n\ny_pred = (regressor.predict(X_test))\n\nmodel_acc['SVM']=accuracy(y_test, y_pred)","5a08b8ff":"regressor = XGBRegressor()\nregressor.fit(X_train,y_train)\n\ny_pred = (regressor.predict(X_test))\n\nmodel_acc['XGBOOST']=accuracy(y_test, y_pred)","034f5d2b":"model_acc","2f63565b":"'The End'","c165415a":"## Deal with outlier","9753a35e":"## Remove constant features","8fcfb490":"## Load Data","9337c192":"## Split numeric and categorical feature","02d107a7":"### Check for duplicate features","b0e0be85":"## decision tree","d2f3d1ff":"# House price","e36f58af":"## Feature scaling","d1785e25":"#### Splitting the dataset into the Training set and Test set","050a25b3":"#### All the missing value is handled","7b2eca6b":"## Replace missing value for numerical features","f14f840d":"#### From above analysis we can conclude that all the quality and condition features can be combine to single feature say 'BsmtScore'. ","57bb34ac":"## Quasi feature selection (remove features having very low variance)","d30a49d0":"#### Done with numerical missing value, Lets see categorical missing value","0bee5204":"## SVM","daabc136":"#### Now Lets combine some of the features to single feature.","3e793f2d":"##### from the above graph it seems like the garage was built in same year in which the house was build, so lets map the same year for garage.","7ac06235":"## MultipleRegression module","f0d0ed28":"#### Lets deal with one feature at time","9818c90e":"## Correlation feature selection","47305b9b":"#### Here also we can combine some features like ['GarageType', 'GarageFinish', 'GarageCars', 'GarageQual', 'GarageCond'] to single feature say GarageScore","47184d17":"## XGBOOST","5f8c9a34":"## Random Forest"}}