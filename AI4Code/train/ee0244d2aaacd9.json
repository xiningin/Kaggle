{"cell_type":{"2dbddc4f":"code","829bf9c9":"code","5a9a806c":"code","1cb76f57":"code","605e2bd1":"code","67081e49":"code","2f8f3296":"code","60ea5c84":"code","6c4b8583":"code","a9936160":"code","87eb657c":"code","736568b1":"code","0970d1d8":"code","fa46813b":"code","61b6c123":"code","cdd43e1a":"code","6b073b04":"code","24acec68":"code","12486776":"code","f9826d57":"code","c66710fa":"code","ebb2ff04":"code","d37819ed":"code","061c6bf4":"code","9c63d3b6":"code","9ce0a20a":"code","644c45f7":"code","a9c9a883":"code","f2979298":"code","3ff60183":"code","b6ecd0a3":"code","4984c4a6":"code","b53d6785":"code","d0a84851":"code","e366667b":"code","2f32039b":"code","512e2ca6":"code","96830354":"code","4c23c231":"code","bf387ebc":"code","6c636573":"code","8d33e43b":"code","9aad55db":"code","08e9eb64":"code","a092c00d":"code","6291a463":"code","3e3f635d":"markdown","b4d2eec4":"markdown","7fbcccfd":"markdown"},"source":{"2dbddc4f":"import numpy as np\nimport pandas as pd\nimport datetime as dt\n\nfrom datetime import date\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom xgboost import XGBClassifier\n\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\n\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder\n\nimport pickle","829bf9c9":"df = pd.read_csv(\"..\/input\/fraudulent_claims_data.csv\")\ndf.head()","5a9a806c":"df.shape","1cb76f57":"df.info()","605e2bd1":"df['fraud_reported'].value_counts()","67081e49":"df['collision_type']=df['collision_type'].replace(\"?\",\"Not Applicable\")\ndf['police_report_available']=df['police_report_available'].replace(\"?\",\"Unknown\")\ndf['property_damage']=df['property_damage'].replace(\"?\",\"Not Applicable\")","2f8f3296":"df[['incident_date','policy_bind_date']] = df[['incident_date','policy_bind_date']].apply(pd.to_datetime)\ndf['duration_bw_inception_incident'] = (df['incident_date'] - df['policy_bind_date']).dt.days","60ea5c84":"df.head()","6c4b8583":"df=df.drop(['policy_number','policy_bind_date','insured_zip','authorities_contacted','umbrella_limit','insured_hobbies', \n            'capital-gains', 'capital-loss','injury_claim', 'property_claim', 'vehicle_claim', 'incident_date', 'auto_model',\n            'policy_csl','insured_education_level','insured_occupation', '_c39','incident_location', \n            'policy_state','incident_type','collision_type','property_damage', 'policy_deductable', 'policy_annual_premium', \n            'number_of_vehicles_involved', 'bodily_injuries', 'incident_state', 'incident_city', 'incident_hour_of_the_day', 'total_claim_amount',\n            'auto_year', 'auto_make', 'duration_bw_inception_incident'],axis=1)","a9936160":"# year_ = date.today().year\n# df['auto_year'] = year_ - df['auto_year']","87eb657c":"df.shape","736568b1":"df.columns","0970d1d8":"# x = df.drop(['fraud_reported'],axis=1)\n# y = df['fraud_reported']","fa46813b":"# from sklearn.compose import ColumnTransformer\n# from sklearn.preprocessing import OneHotEncoder\n# ct = ColumnTransformer(transformers=[('one_hot_encoder', OneHotEncoder(), [3,4,5,6,7,8,9,12,15,17])], remainder='passthrough')\n# df = np.array(ct.fit_transform(df))","61b6c123":"df = pd.get_dummies(df,columns=['insured_sex','insured_relationship',\n                                'incident_severity',\n                                'police_report_available'],drop_first=True)","cdd43e1a":"x = df.drop(['fraud_reported'],axis=1)\ny = df['fraud_reported']","6b073b04":"x_upsample, y_upsample  = SMOTE().fit_resample(x, y)\n\nprint(x_upsample.shape)\nprint(y_upsample.shape)","24acec68":"y_upsample.value_counts()","12486776":"sc=StandardScaler()\nx_scale=sc.fit_transform(x_upsample)","f9826d57":"#0.95\nfrom sklearn.decomposition import PCA\npca=PCA(n_components=0.95)\nx_scaled=pca.fit_transform(x_scale)","c66710fa":"x_train,x_test,y_train,y_test=train_test_split(x_scaled,y_upsample,test_size=0.3)","ebb2ff04":"rf = RandomForestClassifier()\nknn = KNeighborsClassifier()\nsvm = SVC()\nxgb = XGBClassifier()","d37819ed":"for i in [rf, knn, svm]:\n    i.fit(x_train, y_train)\n    y_pred = i.predict(x_test)\n    print(accuracy_score(y_test,y_pred))","061c6bf4":"# import pickle\n\n# Pkl_Filename = \"grid_xgboost.pkl\"  \n\n# with open(Pkl_Filename, 'wb') as file:  \n#     pickle.dump(grid, file)","9c63d3b6":"# import matplotlib.pyplot as plt\n# from sklearn.cluster import KMeans\n# wcss = []\n# for i in range(1, 11):\n#     kmeans = KMeans(n_clusters = i, init = 'k-means++', random_state = 42)\n#     kmeans.fit(x)\n#     wcss.append(kmeans.inertia_)\n# plt.plot(range(1, 11), wcss)\n# plt.title('The Elbow Method')\n# plt.xlabel('Number of clusters')\n# plt.ylabel('WCSS')\n# plt.show()","9ce0a20a":"# kmeans = KMeans(n_clusters = 2, init = 'k-means++', random_state = 42)\n# y_kmeans = kmeans.fit_predict(x)","644c45f7":"# plt.scatter(x[y_kmeans == 0, 0], x[y_kmeans == 0, 1], s = 100, c = 'red', label = 'Cluster 1')\n# plt.scatter(x[y_kmeans == 1, 0], x[y_kmeans == 1, 1], s = 100, c = 'blue', label = 'Cluster 2')\n# plt.scatter(x[y_kmeans == 2, 0], x[y_kmeans == 2, 1], s = 100, c = 'green', label = 'Cluster 3')\n# plt.scatter(x[y_kmeans == 3, 0], x[y_kmeans == 3, 1], s = 100, c = 'cyan', label = 'Cluster 4')\n# plt.scatter(x[y_kmeans == 4, 0], x[y_kmeans == 4, 1], s = 100, c = 'magenta', label = 'Cluster 5')\n# plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s = 300, c = 'yellow', label = 'Centroids')\n# plt.title('Clusters of customers')\n# plt.xlabel('Annual Income (k$)')\n# plt.ylabel('Spending Score (1-100)')\n# plt.legend()\n# plt.show()","a9c9a883":"#Hyperparamenter tuning\nfrom sklearn.model_selection import GridSearchCV","f2979298":"parameters={'criterion':['gini','entropy'], 'max_depth': np.arange(1,30)}\ngrid=GridSearchCV(rf,parameters)\ngrid.fit(x_train,y_train)\nmodel=grid.best_estimator_","3ff60183":"grid.best_score_","b6ecd0a3":"param_grid = {\"kernel\": ['rbf','sigmoid'],\n             \"C\":[0.1,0.5,1.0],\n             \"random_state\":[0,100,200,300]}\ngrid = GridSearchCV(estimator=svm, param_grid=param_grid, cv=5,  verbose=3)\ngrid.fit(x_train,y_train)","4984c4a6":"model2 = grid.best_estimator_\ngrid.best_score_","b53d6785":"param_grid = {\"n_estimators\": [10, 50, 100, 130], \"criterion\": ['gini', 'entropy'],\n                               \"max_depth\": range(2, 10, 1)}\n\ngrid = GridSearchCV(estimator=xgb, param_grid=param_grid, cv=5,  verbose=3,n_jobs=-1)","d0a84851":"grid.fit(x_train,y_train)\nmodel3 = grid.best_estimator_\ngrid.best_score_","e366667b":"# import pickle\n\n# Pkl_Filename = \"grid_xgboost.pkl\"  \n\n# with open(Pkl_Filename, 'wb') as file:  \n#     pickle.dump(grid, file)","2f32039b":"from sklearn.preprocessing import OneHotEncoder,LabelEncoder,StandardScaler\nlabelEncoder1 = LabelEncoder()\nlabelEncoder2 = LabelEncoder()\nlabelEncoder3 = LabelEncoder()\nlabelEncoder4 = LabelEncoder()\nlabelEncoder5 = LabelEncoder()\nlabelEncoder6 = LabelEncoder()\nlabelEncoder7 = LabelEncoder()\nlabelEncoder8 = LabelEncoder()\nlabelEncoder9 = LabelEncoder()\nlabelEncoder10 = LabelEncoder()\nlabelEncoder11 = LabelEncoder()","512e2ca6":"# 'insured_sex','incident_type','collision_type','insured_relationship',\n#                                 'incident_severity','incident_state','incident_city','property_damage',\n#                                 'police_report_available', 'auto_make'\nlabelEncoder1.fit(df['insured_sex'])\n# labelEncoder2.fit(df['incident_type'])\n# labelEncoder3.fit(df['collision_type'])\nlabelEncoder4.fit(df['insured_relationship'])\nlabelEncoder5.fit(df['incident_severity'])\n# labelEncoder6.fit(df['incident_state'])\n# labelEncoder7.fit(df['incident_city'])\n# labelEncoder8.fit(df['property_damage'])\nlabelEncoder9.fit(df['police_report_available'])\n# labelEncoder10.fit(df['auto_make'])\nlabelEncoder11.fit(df['fraud_reported'])","96830354":"df['insured_sex'] = labelEncoder1.transform(df['insured_sex'])\n# df['incident_type'] = labelEncoder2.transform(df['incident_type'])\n# df['collision_type'] = labelEncoder3.transform(df['collision_type'])\ndf['insured_relationship'] = labelEncoder4.transform(df['insured_relationship'])\ndf['incident_severity'] = labelEncoder5.transform(df['incident_severity'])\n# df['incident_state'] = labelEncoder6.transform(df['incident_state'])\n# df['incident_city'] = labelEncoder7.transform(df['incident_city'])\n# df['property_damage'] = labelEncoder8.transform(df['property_damage'])\ndf['police_report_available'] = labelEncoder9.transform(df['police_report_available'])\n# df['auto_make'] = labelEncoder10.transform(df['auto_make'])\ndf['fraud_reported'] = labelEncoder11.transform(df['fraud_reported'])","4c23c231":"df.info()","bf387ebc":"x = np.array(df.drop(['fraud_reported'],1).astype(float))\ny = np.array(df['fraud_reported'])","6c636573":"x_upsample, y_upsample  = SMOTE().fit_resample(x, y)\n\nprint(x_upsample.shape)\nprint(y_upsample.shape)","8d33e43b":"sc=StandardScaler()\nx_scale=sc.fit_transform(x_upsample)","9aad55db":"from sklearn.decomposition import PCA\npca=PCA(n_components=0.95)\nx_scaled=pca.fit_transform(x_scale)","08e9eb64":"x_train,x_test,y_train,y_test=train_test_split(x_scaled,y_upsample,test_size=0.3)","a092c00d":"import matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans\n\nkmeans = KMeans(n_clusters=2, max_iter=600, algorithm = 'auto')\nkmeans.fit(x_scale)","6291a463":"correct = 0\nfor i in range(len(x)):\n    predict_me = np.array(x[i].astype(float))\n    predict_me = predict_me.reshape(-1, len(predict_me))\n    prediction = kmeans.predict(predict_me)\n    if prediction[0] == y[i]:\n        correct += 1\n\nprint(correct\/len(x))","3e3f635d":"# **Using SVM in gridsearch**","b4d2eec4":"# **Using XGBoost in gridsearch**","7fbcccfd":"# **Using Random Forest in gridsearch**"}}