{"cell_type":{"eacb9922":"code","e46d8f9d":"code","2ecbf957":"code","e5c48840":"code","81abd615":"code","ba0690f2":"code","1d6ceab3":"code","ca3df049":"code","c2c109ca":"code","1d97ddde":"code","828966a1":"code","521ac52a":"code","c172aee1":"code","94af0f08":"code","20d988f7":"code","28fef816":"code","8894aa49":"code","a5ca4a2d":"code","6bdda194":"code","69fdfa54":"code","eb735bdf":"code","633ecd97":"code","aa574162":"code","ab1d9029":"code","be4ea4f4":"code","28a01a36":"markdown","d4b0f932":"markdown","8cd5bbef":"markdown","899ffd32":"markdown","75cfa93c":"markdown","4c66eefe":"markdown","4eea6011":"markdown"},"source":{"eacb9922":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt","e46d8f9d":"train = pd.read_csv(\"..\/input\/creditcardfraud\/creditcard.csv\")","2ecbf957":"train","e5c48840":"x = train.corrwith(train[\"Class\"]).to_dict()","81abd615":"del x['Class']","ba0690f2":"features = []\nfor k,v in x.items():\n    if abs(v)>0.1:\n        print(f\"{k} : {v:.2f}\")\n        features.append(k)","1d6ceab3":"x = train[features]\ny = train['Class']","ca3df049":"y.value_counts()","c2c109ca":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nx = scaler.fit_transform(x)","1d97ddde":"# splitting the dataset into train and test dataset with 4:1 ratio (80%-20%)\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size = .2, random_state = 26,stratify=y)","828966a1":"from sklearn.linear_model import LogisticRegression\n\n# Create instance of model\nlreg = LogisticRegression()\n# Pass training data into model\nlreg.fit(x_train, y_train)","521ac52a":"# Getting prediciton on x_test\ny_pred_lreg = lreg.predict(x_test)","c172aee1":"# Scoring our model\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score,f1_score, precision_score, recall_score\n\n# Confusion Matrix\nprint('Logistic Regression')\nprint('\\n')\nprint('Confusion Matrix')\nprint(confusion_matrix(y_test, y_pred_lreg))\nprint('--'*50)\n\n# Classification Report\nprint('Classification Report')\nprint(classification_report(y_test,y_pred_lreg))\n\n\n# Accuracy of our model\nprint('--'*50)\nlogreg_accuracy = round(accuracy_score(y_test, y_pred_lreg) * 100,8)\nprint('Accuracy = ', logreg_accuracy,'%')\n","94af0f08":"%%time\nfrom sklearn.svm import SVC\n# Instantiate the model\nsvc = SVC()\n# Fit the model on training data\nsvc.fit(x_train, y_train)","20d988f7":"# Getting the predictions for x_test\ny_pred_svc = svc.predict(x_test)","28fef816":"print('Support Vector Classifier')\nprint('\\n')\n# Confusion matrix\nprint('Confusion Matrix')\nprint(confusion_matrix(y_test, y_pred_svc))\nprint('--'*50)\n\n# Classification report\nprint('Classification Report')\nprint(classification_report(y_test, y_pred_svc))\n\n# Accuracy\nprint('--'*50)\nsvc_accuracy = round(accuracy_score(y_test, y_pred_svc)*100,8)\nprint('Accuracy = ', svc_accuracy,'%')","8894aa49":"%%time\nfrom sklearn.neighbors import KNeighborsClassifier\n\n# in knn we need to select a value of nearest neighbour, for now lets use a for loop. If accuarcy\n# is better than other models then we would search for optimal parameter\n\nerror_rate = []\n\nfor i in range (2,15):\n    knn = KNeighborsClassifier(n_neighbors = i)\n    knn.fit(x_train, y_train)\n    pred_i = knn.predict(x_test)\n    error_rate.append(np.mean(pred_i != y_test))\n\n# Plot error rate\nplt.figure(figsize = (10,6))\nplt.plot(range(2,15), error_rate, color = 'blue', linestyle = '--', marker = 'o', \n        markerfacecolor = 'green', markersize = 10)\n\nplt.title('Error Rate vs K Value')\nplt.xlabel('K')\nplt.ylabel('Error Rate')\nplt.show()","a5ca4a2d":"# now using above data to train with n_neighbors having least error rate\n\nn_value = 0\nmin_error = float('inf')\nfor idx,error in enumerate(error_rate):\n    if min_error>error:\n        min_error=error\n        n_value=idx+2\n\nknn = KNeighborsClassifier(n_neighbors = n_value)\n# Fit new KNN on training data\nknn.fit(x_train, y_train)","6bdda194":"# Predict KNN\ny_pred_knn_op = knn.predict(x_test)","69fdfa54":"print('K-Nearest Neighbors(KNN)')\nprint('k =',n_value)\n\n# Confusion Matrix\nprint('\\n')\nprint(confusion_matrix(y_test, y_pred_knn_op))\n\n# Classification Report\nprint('--'*50)\nprint('Classfication Report',classification_report(y_test, y_pred_knn_op))\n\n# Accuracy\nprint('--'*50)\nknn_op_accuracy =round(accuracy_score(y_test, y_pred_knn_op)*100,8)\nprint('Accuracy = ',knn_op_accuracy,'%')","eb735bdf":"from sklearn.ensemble import RandomForestClassifier\n\n# Create model object\nrfc = RandomForestClassifier(n_estimators = 250,n_jobs=-1)\n# Fit model to training data\nrfc.fit(x_train,y_train)\ny_pred_rfc = rfc.predict(x_test)","633ecd97":"print('Random Forest')\n# Confusion matrix\nprint('\\n')\nprint('Confusion Matrix')\nprint(confusion_matrix(y_test, y_pred_rfc))\n\n# Classification report\nprint('--'*50)\nprint('Classification Report')\nprint(classification_report(y_test, y_pred_rfc))\n\n# Accuracy\nprint('--'*50)\nrf_accuracy = round(accuracy_score(y_test, y_pred_rfc)*100,8)\nprint('Accuracy = ', rf_accuracy,'%')","aa574162":"from xgboost import XGBClassifier\n\n# Create model object\nxgb = XGBClassifier(n_jobs=-1)\n\n# Fit model to training data\nxgb.fit(x_train, y_train)\ny_pred_xgb = xgb.predict(x_test)","ab1d9029":"print('XGBoost Classifer')\n# Confusion matrix\nprint('\\n')\nprint('Confusion Matrix')\nprint(confusion_matrix(y_test, y_pred_xgb))\n\n# Classification report\nprint('--'*50)\nprint('Classification Report')\nprint(classification_report(y_test, y_pred_xgb))\n\n# Accuracy\nprint('--'*50)\nxgb_accuracy = round(accuracy_score(y_test, y_pred_xgb)*100,8)\nprint('Accuracy = ', xgb_accuracy,'%')","be4ea4f4":"models = pd.DataFrame({\n     'Model': ['Logistic Regression', 'Linear SVC', \n               'K-Nearest Neighbors', 'Random Forest','XGBoost Classifier'],\n    'Score': [logreg_accuracy, svc_accuracy, \n               knn_op_accuracy, rf_accuracy,xgb_accuracy]})\nmodels.sort_values(by='Score', ascending=False)","28a01a36":"### RANDOM FOREST","d4b0f932":"**We have a accuracy of 99.90%**","8cd5bbef":"### LINEAR SUPPORT VECTOR CLASSIFIER","899ffd32":"### XGBoost Classifier","75cfa93c":"## Training on different algorithms","4c66eefe":"### K-NEAREST NEIGHBORS","4eea6011":"### Logistic Regression"}}