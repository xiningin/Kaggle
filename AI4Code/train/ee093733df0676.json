{"cell_type":{"15e17803":"code","6c7be89e":"code","b2a9617a":"code","4d451444":"code","ad28e4a5":"code","0dfbae61":"code","e4f90dee":"code","8b4b3667":"code","dc8e001c":"code","4dc2ce1f":"code","558290f7":"code","51ade2a0":"code","8cf8a9bb":"code","69dbce7f":"code","c0c7cd46":"code","98dd6238":"code","f9141453":"code","ce21b7f1":"code","02c41e6b":"code","fbf349f6":"code","77efedf3":"code","49b48c23":"code","00ad5771":"code","0035bf2a":"code","4a0cd2fa":"code","9a9c0f65":"code","ed265869":"code","33a08b7e":"code","707ec0da":"code","0062fb8a":"code","45750bc9":"code","bbfa4b1c":"code","584c246f":"code","14ab92f4":"code","d71c2ccf":"code","e0d0cc5e":"code","7faa256b":"code","ab39d590":"code","a31110c1":"code","f95d80e3":"code","1e550bcd":"code","096d2514":"code","82cb7cf7":"code","135312d0":"code","f6aaa1e2":"code","674d1ad8":"code","62fb62d4":"code","1e63096f":"code","f2ebb113":"code","6f020691":"code","5e1763d9":"code","8f242e82":"code","3c63c5cc":"code","3e0c72e7":"code","c0be6611":"code","f743ac44":"code","c45e2d16":"code","1a2a09ec":"code","2183e0d6":"code","e4967857":"code","332dcdc9":"code","fda8299e":"code","62013cb0":"code","830f45ae":"code","677c5d32":"code","a20232b1":"code","c829171a":"code","a710eecc":"code","41883b71":"code","6c2f291b":"code","42926bd0":"code","ba12547b":"code","feca9603":"code","1b2e98ba":"code","8662e47a":"code","4517bb9d":"code","490c32ea":"code","81ffd43a":"code","2bea5a2d":"code","8a4d797f":"code","ba5527a6":"code","41ba8e02":"code","d7d54d1c":"code","f563e1c4":"code","a12de2f7":"code","b3ddb33d":"code","dd7860d9":"code","6cefeab7":"code","99d3088d":"code","08db02b4":"markdown","3cb87817":"markdown","fe9c24e3":"markdown","0ff1c888":"markdown","9847a507":"markdown","7b7f89ad":"markdown","66fd9f7f":"markdown","e66b77bd":"markdown","b47d5a6a":"markdown","4500cad1":"markdown","adf5cfb8":"markdown","35a792c8":"markdown","b4745c29":"markdown","65b54f3e":"markdown","51dac7aa":"markdown","4dd298a8":"markdown","c9e567bd":"markdown","123b9da9":"markdown","111acb03":"markdown","72fae29a":"markdown","1bcd9563":"markdown","f5c6d68f":"markdown","036d1103":"markdown","61259cde":"markdown","ee726186":"markdown","b56a3c6d":"markdown","a11603a0":"markdown","55a991f4":"markdown","d8c042a0":"markdown","75dc94bd":"markdown","94b5241f":"markdown","db9cbe76":"markdown","be4416a5":"markdown","d4a79070":"markdown","b5a4bbff":"markdown","8179d1d4":"markdown","373f14db":"markdown","d16bb5ce":"markdown","4b723b2d":"markdown","485b9b76":"markdown","d0b60af1":"markdown","19f2350b":"markdown","06f3a7fb":"markdown","3cae3079":"markdown","e4b24711":"markdown","8ecc38bd":"markdown","7d2169fd":"markdown","1bbf207a":"markdown","e4c63331":"markdown","09718ae7":"markdown","2407159b":"markdown","ee225af7":"markdown","7d1193cf":"markdown","f2dab74f":"markdown","ca16056b":"markdown","dc269c18":"markdown","a61cd088":"markdown","ad9c5cf1":"markdown","df71a4e5":"markdown","1233d2ff":"markdown","21a4c8fb":"markdown","585acbc7":"markdown","9151befa":"markdown","251316f2":"markdown","3c3508b7":"markdown","cb224ca8":"markdown","b095856c":"markdown","5d28931e":"markdown","b3ec534b":"markdown","4c0a1fa0":"markdown","53f08d4e":"markdown","9a7e9dcb":"markdown","e5b0f81c":"markdown","f2a79608":"markdown","43c86962":"markdown","62fb2bce":"markdown","ff5891d4":"markdown","ebd622a6":"markdown","89868ab8":"markdown","a8d20788":"markdown"},"source":{"15e17803":"# Install Package yang Dibutuhkan\n!pip install -q --upgrade pip\n!pip install -q efficientnet  # EfficientNet\n!pip -q install sastrawi      # Santrawi\n\n# Imports\nimport os, random, re, string, emoji\nfrom timeit import default_timer\nfrom tqdm.notebook import tqdm\n\n# Kaggle Datasets for checking GCS\nfrom kaggle_datasets import KaggleDatasets\n\n# Scientific tools\nimport numpy as np\nimport pandas as pd\n\n# Plotting tools\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nplt.style.use(\"seaborn-notebook\")\n\n# Interactive \nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\n\n# Image processing\nfrom PIL import Image\nfrom skimage.transform import rotate\n\n# Tensorflow and Keras\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.image import load_img, img_to_array\n\n# Scikit Learn\nfrom sklearn.utils import shuffle\nfrom sklearn.metrics import *\nfrom sklearn.model_selection import *\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n# SEED ALL\nSEED = 42\n\nos.environ['PYTHONHASHSEED'] = str(SEED)\nrandom.seed(SEED)\nnp.random.seed(SEED)\n\nos.environ['TF_DETERMINISTIC_OPS'] = str(SEED)\ntf.random.set_seed(SEED)\n\n# GCS PATH\nGCS_PATH = KaggleDatasets().get_gcs_path('data-bdc')\n\n# Out\nprint(f'Using Tensorflow Version       : {tf.__version__}')\nprint(f'Google Cloud Storage Data Path : {GCS_PATH}')","6c7be89e":"train = pd.read_excel('..\/input\/data-bdc\/Data BDC - Satria Data 2020\/Data Latih\/Data Latih BDC.xlsx')\ntest = pd.read_excel('..\/input\/data-bdc\/Data BDC - Satria Data 2020\/Data Uji\/Data Uji BDC.xlsx')\ntrain.head()","b2a9617a":"!wget -q https:\/\/raw.githubusercontent.com\/Hyuto\/BDC-Satria-Data\/master\/Preprocess%20code\/RPU.py\n!wget -q https:\/\/raw.githubusercontent.com\/Hyuto\/BDC-Satria-Data\/master\/Preprocess%20code\/Preprocess.py","4d451444":"# Train & Test Image PATH\nTRAIN_PATH = '..\/input\/data-bdc\/Data BDC - Satria Data 2020\/Data Latih\/File Gambar Data Latih\/'\nTEST_PATH = '..\/input\/data-bdc\/Data BDC - Satria Data 2020\/Data Uji\/File Gambar Data Uji\/'","ad28e4a5":"# Apply to nama file\nTRAIN_IMG = [TRAIN_PATH + x for x in train['nama file gambar'].values]\nTEST_IMG = [TEST_PATH + x for x in test['nama file gambar'].values]","0dfbae61":"def check_missing(files, return_missing_ID = True):\n    \"\"\"\n    Mengecek keberadaan data gambar berdasarkan direktori\n    \"\"\"\n    missing = []\n    for file in files:\n        if not os.path.isfile(file):\n            missing.append(file)\n    print(f'[INFO] Missing {len(missing)} file')\n    if return_missing_ID:\n        return sorted([int(x.split('\/')[-1][:-4]) for x in missing])","e4f90dee":"# Missing pada data TRAIN\nmissing_train = check_missing(TRAIN_IMG)\nmissing_train","8b4b3667":"# Missing pada data TEST\nmissing_test = check_missing(TEST_IMG)\nmissing_test","dc8e001c":"def fixing_extensions(missing, path):\n    \"\"\"\n    Membenarkan ekstensi file gambar yang di anggap hilang\n    dari direktori.\n    \"\"\"\n    res = []\n    for miss in missing:\n        fixed = False\n        list_dir = os.listdir(path)\n        for i in range(len(list_dir)):\n            if miss == int(list_dir[i][:-4]):\n                fixed = True\n                res.append((miss, list_dir[i]))\n                break\n        if not fixed:\n            res.append((miss, '404')) # Not Found\n    return res","4dc2ce1f":"# Apply function\nfixed_missing_train = fixing_extensions(missing_train, TRAIN_PATH)\nfixed_missing_test = fixing_extensions(missing_test, TEST_PATH)","558290f7":"# Apply to train DF\nfor Id, filename in fixed_missing_train:\n    index = train.ID.tolist().index(Id)\n    train.loc[index, 'nama file gambar'] = filename\n\n# Apply to test DF\nfor Id, filename in fixed_missing_test:\n    index = test.ID.tolist().index(Id)\n    test.loc[index, 'nama file gambar'] = filename","51ade2a0":"TRAIN_X = [TRAIN_PATH + x for x in train['nama file gambar'].values if x != '404']\nTRAIN_y = train.label.values","8cf8a9bb":"def read_and_resize(path):\n    \"\"\"\n    Read & Resize data gambar\n    \"\"\"\n    img = Image.open(path)\n    img.resize((256, 256), Image.ANTIALIAS)\n    return img\n\ndef show_images(list_dir, label, load_image = read_and_resize, seed = SEED):\n    \"\"\"\n    Menampilkan Gambar Secara acak berdasarkan kelasnya \n    masing - masing sebanyak 5 buah.\n    \"\"\"\n    random.seed(seed)\n    data_0 = random.sample([x for x in zip(list_dir, label) if x[1] == 0], 5)\n    data_1 = random.sample([x for x in zip(list_dir, label) if x[1] == 1], 5)\n    fig, axes = plt.subplots(2, 5, figsize = (20, 10))\n    for i in range(2):\n        if i == 0:\n            data = data_0\n        else:\n            data = data_1\n        for j in range(5):\n            img = load_image(data[j][0])\n            axes[i, j].imshow(img)\n            axes[i, j].set_title(f'Label : {data[j][1]}', fontsize = 14)\n            axes[i, j].axis('off')\n    fig.tight_layout()\n    plt.show()","69dbce7f":"show_images(TRAIN_X, TRAIN_y)","c0c7cd46":"def load_and_preprocess_image(path: str, size = [256, 256]):\n    \"\"\"\n    Load & Preprocess data gambar\n    \"\"\"\n    image = img_to_array(load_img(path))\n    img = tf.convert_to_tensor(image, dtype=tf.float32)\n    shapes = tf.shape(img)\n    h, w = shapes[-3], shapes[-2]\n    dim = tf.minimum(h, w)\n    img = tf.image.resize_with_crop_or_pad(img, dim, dim)\n    img = tf.image.resize(img, size)\n    img = tf.cast(img, tf.float32) \/ 255.0\n    return img.numpy()","98dd6238":"show_images(TRAIN_X, TRAIN_y, load_and_preprocess_image)","f9141453":"data_augmentation = lambda x : (\n    rotate(x, random.randint(-70, 70), mode='reflect')\n)","ce21b7f1":"def AUG_test(X):\n    \"\"\"\n    Plot gambar dengan fungsi Augmentasi\n    \"\"\"\n    X = load_and_preprocess_image(X)\n    fig, axes = plt.subplots(1, 5, figsize = (20,10))\n    axes[0].imshow(X)\n    axes[0].set_title('Actual', fontsize = 14)\n    axes[0].axis('off')\n    for i in range(1, 5):\n        aug = data_augmentation(X)\n        axes[i].imshow(aug)\n        axes[i].set_title('Augmented', fontsize = 14)\n        axes[i].axis('off')\n    fig.tight_layout()\n    return plt.show()","02c41e6b":"for i in random.sample(TRAIN_X, 2):\n    AUG_test(i)","fbf349f6":"from Preprocess import *","77efedf3":"train['text'] = (train['judul'] + ' ' + train['narasi']).apply(lambda x : x.lower())\ntest['text'] = (test['judul'] + ' ' + test['narasi']).apply(lambda x : x.lower())\ntrain.head()","49b48c23":"train[train.duplicated(subset=['text'], keep=False)]","00ad5771":"# Drop duplicates row\ntrain = train.drop_duplicates(subset =\"text\").reset_index()","0035bf2a":"def clean_up(arr):\n    r\"\"\"\n    Cleanup \\n and lowering text\n    \"\"\"\n    for i in range(len(arr)):\n        arr[i] = arr[i].lower()\n        arr[i] = re.sub('\\n', ' ', arr[i])\n        arr[i] = ' '.join(arr[i].split())\n    return arr","4a0cd2fa":"# Init text\ntrain_text = clean_up(train.text.values)\ntest_text = clean_up(test.text.values)","9a9c0f65":"%%time\nFE = FeatureExtraction()\nFE_test = FeatureExtraction()\nFE.fit(train_text, train.label.values)\nFE_test.fit(test_text)","ed265869":"FE.get_table('urls', return_prop = True)","33a08b7e":"FE.get_table('hashtags', return_prop = True)","707ec0da":"FE.get_table('tags', return_prop = True)","0062fb8a":"FE.get_table('emojis', return_prop = True)","45750bc9":"# Build Mask Code\nFE.build_mask_code(0)\nFE_test.build_mask_code(0)\n# Apply to data\ntrain_text = FE.encode(train_text)    # Train\ntest_text = FE_test.encode(test_text) # Test","bbfa4b1c":"# Add more string punctuation\nstring.punctuation += '\u2018\u2019\u2026\u201c\u201d\u2013'","584c246f":"def normalize(array):\n    \"\"\"\n    Normalize text\n    \"\"\"\n    punc, arr = string.punctuation, array.copy()\n    for i in range(len(arr)):\n        temp = list(arr[i])\n        for j in range(1, len(temp) - 1):\n            if (temp[j] in punc) and not\\\n            all([x in string.digits for x in [temp[j-1], temp[j+1]]]):\n                temp[j] = ' ' + temp[j] + ' '\n            elif (temp[j] in string.ascii_lowercase) and (temp[j + 1] \\\n            in string.digits or temp[j + 1] in string.punctuation):\n                temp[j] += ' '\n        arr[i] = ''.join(temp)\n        arr[i] = ' '.join(arr[i].split())\n    return arr","14ab92f4":"# Apply to text\ntrain_text = normalize(train_text) # Train\ntest_text = normalize(test_text)   # Test","d71c2ccf":"def remove_punc(arr, punc_):\n    \"\"\"\n    Remove string punctuation\n    \"\"\"\n    return asarray([x.translate(str.maketrans('', '', punc_))\n                    for x in arr])","e0d0cc5e":"# Apply to data\ntrain_text = FE.decode(train_text)    # Train\ntrain_text = remove_punc(train_text, string.punctuation)\ntest_text = FE_test.decode(test_text) # Test\ntest_text = remove_punc(test_text, string.punctuation)","7faa256b":"def deemojized(arr):\n    \"\"\"\n    De Emojized text\n    \"\"\"\n    for i in range(len(arr)):\n        arr[i] = emoji.demojize(arr[i])\n        arr[i] = re.sub(':', ' ', arr[i])\n        arr[i] = re.sub('_', ' ', arr[i])\n        arr[i] = ' '.join(arr[i].split())\n    return arr","ab39d590":"# Apply to data\ntrain_text = deemojized(train_text) # Train\ntest_text = deemojized(test_text)   # Test","a31110c1":"# Count Vectorizer\ncount = CountVectorizer()\ncount.fit(train_text)","f95d80e3":"# Vocabulary to DataFrame\nvocab = pd.DataFrame({'Vocab' : list(count.vocabulary_.keys()), \n                     'Word Index' : list(count.vocabulary_.values())})\nvocab = vocab.sort_values(by=['Word Index'], ascending = False)\nvocab[:30].style.hide_index()","1e550bcd":"spellchecker = SpellChecker()\nspellchecker.fit('cc-hand-fixed')","096d2514":"# Apply to Data\ntrain_text = spellchecker.transform(train_text) # Train\ntest_text = spellchecker.transform(test_text)   # Test","82cb7cf7":"from spacy.lang.id.stop_words import STOP_WORDS as ID\nfrom spacy.lang.en.stop_words import STOP_WORDS as EN\nfrom Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n\nstopwords = set(StopWordRemoverFactory().get_stop_words())\nstopwords.update(EN)\nstopwords.update(ID)","135312d0":"def RUnecesarry(array, stopwords):\n    \"\"\"\n    Remove Unnecessary words\n    \"\"\"\n    arr = array.copy()\n    for i in range(len(arr)):\n        temp = arr[i].split()\n        temp = [x for x in temp if not (len(x) == 1 and \\\n                (x in string.ascii_lowercase or x in string.digits))]\n        temp = [x for x in temp if x not in stopwords]\n        arr[i] = ' '.join(temp).lower()\n    return arr","f6aaa1e2":"# Apply to data\ntrain_text = RUnecesarry(train_text, stopwords) # Train\ntest_text = RUnecesarry(test_text, stopwords)   # Test","674d1ad8":"for i in [0, 803, 1002]:\n    print(f\"Actual : \\n{train.text.values[i]}\\n\\nPreprocessed : \\n{train_text[i]}\")\n    print(''.rjust(80, '-'))","62fb62d4":"train.info()","1e63096f":"def FixMonth(text):\n    \"\"\"\n    Fungsi untuk merubah kata 'Okt' menjadi 'Oct' dan 'Agu' menjadi 'Aug'\n    \"\"\"\n    if type(text) == str:\n        if 'Okt' in text: return text[:3]+'Oct'+text[6:]\n        else: return text[:3]+'Aug'+text[6:]\n    else:\n        return text","f2ebb113":"# Apply to data\ntrain['tanggal'] = train['tanggal'].apply(FixMonth)\ntrain['tanggal'] = pd.to_datetime(train['tanggal'])","6f020691":"fig_1 = go.Figure(go.Pie(labels=['Bukan Hoax', 'Hoax'],\n                       values=train.groupby('label').size().values, \n                       textinfo='percent+label+value', \n                       textfont_color='#ffffff', \n                       marker_colors=['#0db7c5','#d03850'])\n               )\nfig_1.update_layout(\n    title={\n        'text': \"Frekuensi Perkelasnya\",\n        'y':0.93,\n        'x':0.5,\n        'font_size':23},\n    width=550, height=550\n)\n\nfig_1.show()","5e1763d9":"# Membuat Kolom baru untuk menyimpan nilai tahun dan bulan\ntrain['year'] = [i.year for i in train.tanggal]\ntrain['month'] = [i.month for i in train.tanggal]\n# Grouping \ndata_waktu = (train.groupby(['year', 'month', 'label']).size().reset_index()\n              .pivot_table(columns='label', index=['year','month'], values=0))","8f242e82":"fig_2 = go.Figure(data=[ \n    go.Bar(name='Bukan Hoax', \n           x=[str(i) for i in data_waktu.index], \n           y=data_waktu[0].values, \n           marker_color='#0db7c5'),\n    go.Bar(name='Hoax',\n           x=[str(i) for i in data_waktu.index], \n           y=data_waktu[1].values, \n           marker_color='#d03850')\n])\n\nfig_2.update_layout(barmode='stack', \n                  xaxis=dict(dtick=1,\n                             showgrid=False, \n                             title='(Tahun, Bulan)'),\n                  legend=dict(x=0.006,y=0.97, \n                              bgcolor='rgba(255,255,255,0)', \n                              bordercolor='rgba(0,0,0,0)',\n                              font_color='#090919',\n                              font_size=14),\n                  title=dict(text='Sebaran Hoax per Waktu',\n                             x=0.5,\n                             y=0.9,\n                             font_size=23),\n                  width=1000,height=550\n                 )\n\nfig_2.show()","3c63c5cc":"def find_words(texts, label, builder = CountVectorizer(min_df=3, max_df=0.9,\n                                                       ngram_range=(1,2))):\n    \"\"\"\n    Memeriksa kalimat berdasarkan kelasnya untuk membuat vocab\n    \"\"\"\n    builder.fit(texts)\n    n_class, res = len(set(label)), {}\n    for i in tqdm(range(len(texts))):\n        for vocab in texts[i].split():\n            if vocab in builder.vocabulary_:\n                if vocab not in res:\n                    res[vocab] = [0] * n_class + [0]\n                res[vocab][label[i]] += 1\n                res[vocab][-1] += 1\n    df = pd.DataFrame({'kata' : list(res.keys())})\n    for i in range(n_class):\n        df[f'Kelas_{i}'] = [res[x][i] for x in res]\n    df['Frekuensi'] = [res[x][-1] for x in res]\n    return df.sort_values(by = ['Frekuensi'], ascending = False).reset_index(drop=True)","3e0c72e7":"# Vocab DF per class\nwords = find_words(train_text, train.label.values)\nwords['Max_prop'] = words[['Kelas_0', 'Kelas_1']].max(axis = 1) \/ words['Frekuensi']\nwords.head(10)","c0be6611":"_1 = words[(words['Kelas_1'] > words['Kelas_0'])]\n\nplt.figure(figsize = (12,7))\nsns.barplot(x = 'kata', y = 'Frekuensi', data = _1[:25])\nplt.title('Frekuensi Kata yang Sering Muncul Pada Label Hoax', fontsize = 14)\nplt.xticks(rotation = 90)\nplt.show()","f743ac44":"from wordcloud import WordCloud, ImageColorGenerator\n\n__1 = {}\nfor i in range(len(_1)):\n    __1[_1.kata.values[i]] = _1.Max_prop.values[i]\n    \nwordcloud = WordCloud(width = 4000, height = 3000, min_font_size = 5, \n                      background_color = 'white', colormap = 'inferno').fit_words(__1)\nplt.figure(figsize = (10, 8)) \nplt.imshow(wordcloud) \nplt.axis(\"off\") \nplt.tight_layout(pad = 0)\nplt.show() ","c45e2d16":"_0 = words[(words['Kelas_0'] > words['Kelas_1'])]\n\nplt.figure(figsize = (12,7))\nsns.barplot(x = 'kata', y = 'Frekuensi', data = _0[:25])\nplt.title('Frekuensi Kata yang Sering Muncul Pada Label Bukan Hoax', fontsize = 14)\nplt.xticks(rotation = 90)\nplt.show()","1a2a09ec":"__0 = {}\nfor i in range(len(_0)):\n    __0[_0.kata.values[i]] = _0.Max_prop.values[i]\n    \nwordcloud = WordCloud(width = 4000, height = 3000, min_font_size = 5, \n                      background_color = 'white', colormap = 'Blues').fit_words(__0)\n\nplt.figure(figsize = (10, 8)) \nplt.imshow(wordcloud) \nplt.axis(\"off\") \nplt.tight_layout(pad = 0) \nplt.show() ","2183e0d6":"try:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n    BATCH_SIZE = 16 * strategy.num_replicas_in_sync\nelse:\n    strategy = tf.distribute.get_strategy()\n    BATCH_SIZE = 32 * strategy.num_replicas_in_sync\n    \nAUTO = tf.data.experimental.AUTOTUNE\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)","e4967857":"# Train Dataset\ntrain_prep = pd.read_csv('..\/input\/data-bdc\/Preprocess and Up Sample\/Up-Sample-0-by-200%\/Keterangan.csv')\n# Valid Data\nvalid_prep = pd.read_csv('..\/input\/data-bdc\/Validitas\/Keterangan.csv')","332dcdc9":"# Spesifikasi PATH data train pada GCS\nAUG_PATH = GCS_PATH + '\/Preprocess and Up Sample\/'\n\n# Data Up Sample 200% \nTRAIN_X_, TRAIN_y_ = shuffle([AUG_PATH + x for x in train_prep.DIR.values],\n                             train_prep.label.values, random_state = SEED)\n\n# Spesifikasi PATH data Valid pada GCS\nVAL_X = [GCS_PATH + '\/' + x for x in valid_prep.DIR.values]\nVAL_y = valid_prep.label.values\n\n# Spesifikasi PATH data test pada GCS\nTEST_PATH = GCS_PATH + '\/Data BDC - Satria Data 2020\/Data Uji\/File Gambar Data Uji\/'\nTEST_X = [TEST_PATH + x for x in test['nama file gambar'].values]","fda8299e":"def decode_image(filename, label=None, image_size=(512, 512)):\n    \"\"\"\n    Decode Image from String Path Tensor\n    \"\"\"\n    bits = tf.io.read_file(filename)\n    image = tf.image.decode_jpeg(bits, channels=3)\n    image = tf.cast(image, tf.float32) \/ 255.0\n    \n    if label is None: # if test\n        shapes = tf.shape(image)\n        h, w = shapes[-3], shapes[-2]\n        dim = tf.minimum(h, w)\n        image = tf.image.resize_with_crop_or_pad(image, dim, dim)\n        image = tf.image.resize(image, image_size)\n        return image\n    else:\n        image = tf.image.resize(image, image_size)\n        return image, label","62013cb0":"# TF Train Dataset\ntrain_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices((TRAIN_X_, TRAIN_y_))\n    .map(decode_image, num_parallel_calls=AUTO)\n    .cache()\n    .repeat()\n    .shuffle(1024)\n    .batch(BATCH_SIZE)\n    .prefetch(AUTO)\n)\n\n# TF Valid Dataset\nvalid_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices((VAL_X, VAL_y))\n    .map(decode_image, num_parallel_calls=AUTO)\n    .batch(BATCH_SIZE)\n    .cache()\n    .prefetch(AUTO)\n)\n\n# TF Test Dataset\ntest_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices((TEST_X))\n    .map(decode_image, num_parallel_calls=AUTO)\n    .batch(BATCH_SIZE)\n)","830f45ae":"# Tensorflow - Keras Layers\nimport tensorflow.keras.backend as K\nimport tensorflow.keras.layers as L\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import ModelCheckpoint\n\n# EfficientNet\nimport efficientnet.tfkeras as efn","677c5d32":"def Make_EfficientNet_model():\n    \"\"\"\n    https:\/\/www.kaggle.com\/xhlulu\/flowers-tpu-concise-efficientnet-b7\n    \"\"\"\n    model = tf.keras.Sequential([\n        efn.EfficientNetB7(            # EfficientnetB7\n            input_shape=(512, 512, 3),\n            weights='noisy-student',\n            include_top=False\n        ),\n        L.GlobalAveragePooling2D(),\n        L.Dense(512, activation= 'relu'), \n        L.Dropout(0.2), \n        L.Dense(1, activation='sigmoid')\n    ])\n    model.compile(optimizer='adam', loss = 'binary_crossentropy', metrics=['accuracy'])\n    return model","a20232b1":"with strategy.scope():\n    model = Make_EfficientNet_model()","c829171a":"# Config\nEPOCHS = 10\nSTEPS_PER_EPOCH = len(TRAIN_X_) \/\/ BATCH_SIZE\ncheckpoint = ModelCheckpoint('EfficientNetB7_best_model.h5', monitor='val_accuracy',\n                             save_best_only=True, save_weights_only=True, mode='max')\n\n# Fitting Model\nprint(f'[INFO] Fitting Model')\nhistory = model.fit(train_dataset, epochs = EPOCHS, \n                    steps_per_epoch = STEPS_PER_EPOCH,\n                    validation_data = valid_dataset,\n                    callbacks = [checkpoint])","a710eecc":"fig, (ax1, ax2) = plt.subplots(2, 1, figsize = (12, 10))\nax1.plot(range(1, EPOCHS + 1), history.history['loss'], label = 'loss')\nax1.plot(range(1, EPOCHS + 1), history.history['val_loss'], label = 'val_loss')\nax1.set_title('Loss at Training', fontsize = 14)\nax1.legend()\nax2.plot(range(1, EPOCHS + 1), history.history['accuracy'], label = 'accuracy')\nax2.plot(range(1, EPOCHS + 1), history.history['val_accuracy'], label = 'val_accuracy')\nax2.set_title('Accuracy at Training', fontsize = 14)\nax2.legend()\nfig.show()","41883b71":"# model.load_weights('EfficientNetB7_best_model.h5')\nmodel.load_weights('..\/input\/modelku\/Image Model\/200%_best_model.h5') # Jika menggunakan Model kami","6c2f291b":"# Valid \nval_pred = model.predict(np.concatenate([x for x, y in valid_dataset], axis=0))\nval_pred_classes = np.array(val_pred.flatten() >= .5, dtype = 'int')","42926bd0":"print(f'Accuracy Valid Data : {accuracy_score(VAL_y, val_pred_classes)}')\nprint(f'F1 Score Valid Data : {f1_score(VAL_y, val_pred_classes)}')","ba12547b":"plt.figure(figsize = (7, 7))\nsns.heatmap(confusion_matrix(VAL_y, val_pred_classes, normalize = 'true'), \n            annot=True, cmap=plt.cm.Blues)\nplt.title('Normalized Confussion Matrix Valid Data')\nplt.xlabel('Predicted')\nplt.ylabel('Actual')\nplt.show()","feca9603":"EfficientNet_pred = model.predict(test_dataset)","1b2e98ba":"K.clear_session()\ntf.tpu.experimental.initialize_tpu_system(tpu)","8662e47a":"from transformers import *","4517bb9d":"model_name='cahya\/bert-base-indonesian-522M'\ntokenizer = BertTokenizer.from_pretrained(model_name)","490c32ea":"index = random.randint(0, len(train_text))\nencode = tokenizer.encode_plus(train_text[index], return_attention_mask = True,\n                               return_token_type_ids=True)\nprint('Actual : ')\nprint(train_text[index])\nprint(''.rjust(80, '-'))\nprint('Token IDS : ')\nprint(encode[\"input_ids\"])\nprint(''.rjust(80, '-'))\nprint('Attention Mask : ')\nprint(encode['attention_mask'])\nprint(''.rjust(80, '-'))\nprint('Token Type IDS : ')\nprint(encode['token_type_ids'])","81ffd43a":"plt.figure(figsize=(7,7))\nsns.distplot([len(tokenizer.encode(x)) for x in train_text], label = 'train')\nplt.title('Distplot of Text Lenght', fontsize=14)\nplt.legend()\nplt.show()","2bea5a2d":"max_len = 250","8a4d797f":"def regular_encode(texts, tokenizer = tokenizer, maxlen=max_len):\n    \"\"\"\n    Encoding data teks\n    \"\"\"\n    enc_di = tokenizer.batch_encode_plus(\n        texts, \n        return_attention_masks=True, \n        return_token_type_ids=True,\n        pad_to_max_length=True,\n        max_length=maxlen\n    )\n    \n    return {x : np.asarray(enc_di[x]) for x in enc_di}","ba5527a6":"def build_model(transformer, loss='binary_crossentropy', max_len=max_len):\n    input_ids = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=\"input_ids\")\n    attention_mask = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name='attention_mask')\n    token_type_ids = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name='token_type_ids')\n    \n    sequence_output, pooled_output = transformer([input_ids, attention_mask, token_type_ids])\n    cls_token = sequence_output[:, 0, :]\n    x = L.Dropout(0.3)(cls_token)\n    out = L.Dense(1, activation='sigmoid')(x)\n    \n    model = tf.keras.Model(inputs=[input_ids, attention_mask, token_type_ids], outputs=out)\n    model.compile(Adam(lr=3e-5), loss=loss, metrics=['accuracy'])\n    \n    return model","41ba8e02":"# Load Folding Dataset from responsitory\nfolding = pd.read_csv('https:\/\/raw.githubusercontent.com\/Hyuto\/BDC-Satria-Data\/master\/Folding.csv')\nfolding.head()","d7d54d1c":"# Init\nscores_valid, scores_test, CMS = [], [], []\nHISTORY = []\n\n# Test Dataset\ntest_dataset = (tf.data.Dataset\n    .from_tensor_slices((regular_encode(test_text)))\n    .batch(BATCH_SIZE)\n)\n\nfor i in range(1, 11):\n    if i in [3, 6]: # Ambil Fold 3 dan 6\n        K.clear_session()\n        tf.tpu.experimental.initialize_tpu_system(tpu)\n        \n        fold = f'Fold {i}'\n        print(f'[INFO] {fold}')\n\n        # Split Dataset per Fold\n        x_train = train_text[folding[fold].values == 1]\n        y_train = train.label.values[folding[fold].values == 1]\n        x_val = train_text[folding[fold].values == 0]\n        y_val = train.label.values[folding[fold].values == 0]\n\n        # Encoding & to TF Dataset\n        train_dataset = (tf.data.Dataset\n            .from_tensor_slices((regular_encode(x_train), y_train))\n            .batch(BATCH_SIZE)\n            .cache()\n            .repeat()\n            .shuffle(1024)\n            .prefetch(AUTO)\n        )\n        valid_dataset = (tf.data.Dataset\n            .from_tensor_slices((regular_encode(x_val), y_val))\n            .batch(BATCH_SIZE)\n            .cache()\n            .prefetch(AUTO)\n        )\n        \n        # Training\n        with strategy.scope(): # Build & Scoope Model\n            BERT = TFBertModel.from_pretrained(model_name)\n            model = build_model(BERT)\n            \n        checkpoint = ModelCheckpoint(f'Fold_{i}_best_model.h5', monitor='val_accuracy', \n                                     save_best_only=True, save_weights_only=True, \n                                     mode='max')\n        \n        history = model.fit(train_dataset, epochs = 15, \n                            steps_per_epoch = len(x_train)\/\/BATCH_SIZE,\n                            validation_data = valid_dataset,\n                            callbacks = [checkpoint])\n\n        # Load Weights\n        # model.load_weights(f'Fold_{i}_best_model.h5') # Trained Best Weights\n        model.load_weights(f'..\/input\/modelku\/{fold}_best_model.h5') # Catatan Cakrawala Weights\n\n        # Predict\n        pred_val  = model.predict(valid_dataset)  # Valid\n        pred_test = model.predict(test_dataset)   # Test\n\n        scores_valid.append(pred_val.flatten())\n        scores_test.append(pred_test.flatten())\n        CMS.append(confusion_matrix(y_val, np.array(pred_val.flatten() >= .5, dtype='int'),\n                                    normalize = 'true'))\n        HISTORY.append(history.history)\n\nprint(f'[INFO] Done')","f563e1c4":"fig, (ax1, ax2) = plt.subplots(2, 1, figsize = (12, 10))\nfor i, fold in enumerate([3, 6]):\n    ax1.plot(range(1, 16), HISTORY[i]['loss'], label = f'Fold {fold} loss')\n    ax1.plot(range(1, 16), HISTORY[i]['val_loss'], label = f'Fold {fold} val_loss')\n    ax2.plot(range(1, 16), HISTORY[i]['accuracy'], label = f'Fold {fold} accuracy')\n    ax2.plot(range(1, 16), HISTORY[i]['val_accuracy'], label = f'Fold {fold} val_accuracy')\nax1.set_title('Loss at Training', fontsize = 14)\nax2.set_title('Accuracy at Training', fontsize = 14)\nax1.legend()\nax2.legend()\nfig.show()","a12de2f7":"# Plotting Confussion Matrix\nfig, ax = plt.subplots(1, 2, figsize = (14, 6))\ni = 0\nfor j in range(10):\n    if j in [2, 5]:\n        sns.heatmap(CMS[i], annot=True, cmap = plt.cm.Blues, ax = ax[i])\n        ax[i].set_title(f'Normalized Confussion Matrix Valid Data Fold {j + 1}')\n        ax[i].set_xlabel('Predicted')\n        ax[i].set_ylabel('Actual')\n        i += 1\nfig.show()","b3ddb33d":"ensemble = (EfficientNet_pred.flatten() + scores_test[0] + scores_test[1]) \/ 3","dd7860d9":"plt.figure(figsize = (8,6))\nsns.distplot(ensemble)\nplt.title('Distribusi Sebaran Peluang Prediksi', fontsize = 14)\nplt.show()","6cefeab7":"test['prediksi'] = np.array(ensemble > .65, dtype = 'int')\nsubmission_temp = pd.read_csv('..\/input\/data-bdc\/Submission Template.csv')[['ID']]\nhasil = submission_temp.merge(test[['ID', 'prediksi']], on = 'ID')\nhasil.head()","99d3088d":"hasil.to_csv('Catatan Cakrawala BDC - Ensemble.csv', index = False)","08db02b4":"`Transform` untuk apply pada data.","3cb87817":"# Ensemble<a class=\"anchor\" id=\"chapter4\"><\/a>\n> Ensemble is methods use multiple learning algorithms to obtain better predictive performance than could be obtained from any of the constituent learning algorithms alone.\n\nUntuk mendapatkan hasil yang lebih baik kami melakukan `ensembling` pada hasil prediksi model kami. Hasil model yang di `ensembling` adalah hasil peramalan data test pada model `EfficientNetB7` dan model `Bert` pada fold `3` dan `6`. Metode ensemble yang digunakan adalah dengan mengambil rata - rata probabilitas dari hasil prediksian ketiga model tersebut.<a class=\"anchor\" id=\"chapter4_1\"><\/a>","fe9c24e3":"### Memprediksi Data Test\nMenggunakan model untuk memprediksi `test dataset`.","0ff1c888":"### Membuat Model\nMembuat model yang akan digunakan. Berikut adalah arsitektur model yang akan digunakan.\n\n<img src=\"https:\/\/raw.githubusercontent.com\/Hyuto\/BDC-Satria-Data\/master\/Sample%20Images\/model_1.png\" alt=\"Efficentnet_1\" width=\"500\" height=\"500\" style=\"display: block; margin-left: auto; margin-right: auto;\"\/>","9847a507":"### Normalisasi Tanda Baca<a class=\"anchor\" id=\"chapter1_2_4\"><\/a>\nMelakukan normalisasi tanda baca pada text. Karena banyak peletakan tanda baca yang salah pada text yang akan mengganggu `tokenizer` dalam melakukan segmentasi pada kalimat.\n```\n# Contoh\nBudi membayar2.000 ban yang dibelinya senilai rp.2.000.000\n\n# Preprocessed\nBudi membayar 2.000 ban yang dibelinya senilai rp. 2.000.000\n```","7b7f89ad":"Confussion Matrix","66fd9f7f":"### Mengecek Kata yang Misspell (typo)<a class=\"anchor\" id=\"chapter1_2_7\"><\/a>\nMengecek kata-kata yang misspell atau typo serta kata - kata singkatan. \n\n1. Membangun Vocabulary<br>\nMembangun Vocabulary dari data untuk di cek secara **Manual**","e66b77bd":"Berikut adalah Barplot untuk kata - kata pada kelas `0` yang mempunyai frekuensi kumunculan lebih dari kelas `1`.","b47d5a6a":"Spesifikasikan direktori file gambar dengan pada GCS","4500cad1":"### Sesuaikan Path data gambar\nMenyesuaikan direktori data gambar","adf5cfb8":"### Mengecek Sequence Terpanjang\nKarena panjang dari data teks tidak sama. Maka perlu di lakukan padding untuk menyamakan panjang dari sequance. \n\nContoh:\n```\njokowi ahok bagikan => Encode IDS => [3, 15071, 29708] => Padding to 5 => [3, 15071, 29708, 0, 0]\n```\nUntuk melakukan padding perlu dicari sequence terpanjang pada data sebagai acuannya.","35a792c8":"tambahkan Path ke nama file gambar","b4745c29":"### Cross Validation\n\nMetode yang digunakan untuk CV adalah `StratifiedKflod`. Metode ini digunakan untuk mengatasi kelas pada data yang imbalance. `StratifiedKfold` ini tidak jauh berbeda dengan `Kfold` hanya pada saat pembagian menjadi data train dan data valid, proporsi masing masing kelas tetap sama pada kedua data tersebut.\n<img src=\"https:\/\/raw.githubusercontent.com\/Hyuto\/BDC-Satria-Data\/master\/Sample%20Images\/StratifiedKfold.png\" alt=\"StratifiedKfold\" width=\"600\" height=\"800\" style=\"display: block; margin-left: auto; margin-right: auto;\"\/>\n\n`n_split` yang digunakan sebesar 10. Dataset Folding kami bisa di daoat dengan menggunakan:\n```\ncv = StratifiedKFold(n_splits=10)\nfor fold, (train_ind, val_ind) in enumerate(cv.split(train_text, train.label.values)):\n    x_train, y_train = train_text[train_ind], train.label.values[train_ind]\n    x_val, y_val = train_text[val_ind], train.label.values[val_ind]\n```","65b54f3e":"# BDC - Satria Data 2020\n## Daftar Isi\n1. [Preprocessing Data](#chapter1)\n    - [Data Gambar](#chapter1_1)\n        1. [Mengecek Missing File](#chapter1_1_1)\n        1. [Preprocess & Resizing Image Data](#chapter1_1_2)\n        1. [Image Augmentation](#chapter1_1_3)\n        1. [Up-Sampling Data](#chapter1_1_4)\n    - [Data Teks](#chapter1_2)\n        1. [Mengecek Duplicated Value](#chapter1_2_1)\n        1. [Mengekstrack Features](#chapter1_2_2)\n        1. [Masking Feature (Encode)](#chapter1_2_3)\n        1. [Normalisasi Tanda Baca](#chapter1_2_4)\n        1. [Decode Masking & Menghapus Tanda Baca](#chapter1_2_5)\n        1. [De-Emojized](#chapter1_2_6)\n        1. [Mengecek Misspellwords (typo)](#chapter1_2_7)\n        1. [Menghapus Stopwords](#chapter1_2_8)\n1. [Explorasi Data](#chapter2)\n    1. [Mengecek Frekuensi Data Setiap Kelas](#chapter2_1)\n    1. [Mengecek Frekuensi Kelas perWaktu](#chapter2_1)\n    1. [Mengecek Kata - Kata Setiap Kelas](#chapter2_1)\n1. [Pemodelan](#chapter3)\n    1. [Pemodelan Data Gambar (EfficientNet)](#chapter3_1)\n    1. [Pemodelan Data Teks (BERT)](#chapter3_2)\n1. [Ensemble](#chapter4)\n    1. [Ensembling](#chapter4_1)\n    1. [Mencari Threshold](#chapter4_1)\n1. [Kesimpulan & Saran](#chapter5)\n\n## Task\nMendeteksi apakah suatu berita adalah hoax atau bukan hoax (`Binary Classification`).\n\n## Data\n* Data Gambar\n* Data Text (judul dan narasi berita)","51dac7aa":"Spesifikasi ulang nama file gambar","4dd298a8":"Meninjau `train history` pada model","c9e567bd":"## Pemodelan Data Teks (BERT)<a class=\"anchor\" id=\"chapter3_2\"><\/a>\nBERT(Bidirectional Encoder Representations from Transformers) adalah pretrained model karya Jacob Devlin, Ming-Wei Chang, Kenton Lee dan Kristina Toutanova. Model ini berupa transformator dua arah yang telah dilatih sebelumnya.\n\nPublished Paper : [BERT](https:\/\/arxiv.org\/abs\/1810.04805)\n\nBert yang digunakan pada notebook ini adalah `bert-base-indonesian` buatan `Cahya Wirawan` yang berupa pre-trained BERT-base model pada 522MB data wikipedia indonesia dengan vocabulary sebesar 32.000. Keterangan lebih lanjut bisa dibaca [di sini](https:\/\/huggingface.co\/cahya\/bert-base-indonesian-522M)","123b9da9":"### Load weights model\nLoad `weights` terbaik yang telah disimpan. \n\n#### Note :\n* Untuk mendapatkan hasil yang sama dengan yang di submit pada website `BDC - Satria Data` bisa menggunakan `weights` yang telah kami simpan [disini](https:\/\/www.kaggle.com\/pencarikebahagiaan\/modelku). \n* Untuk melihat proses training menggunakan data dengan kriteria Up-Sampling lainnya untuk perbandingan silahkan kunjugi notebook pada responsitory kami [di sini](https:\/\/github.com\/Hyuto\/BDC-Satria-Data\/tree\/master\/Notebooks)","111acb03":"Confussion Matrix prediksian model terhadap data valid per - `foldnya`","72fae29a":"### Memprediksi dan Mengevaluasi Data\nMemprediksi data train dan data valid lalu dilakukan pengevaluasian untuk melihat kebaikan model.","1bcd9563":"### Image Augmentation<a class=\"anchor\" id=\"chapter1_1_3\"><\/a>\nKarena data yang ada tidak seimbang maka perlu dilakukan penambahan data. Metode yang digunakan untuk menambahkan data yaitu augmentasi data gambar pada kelas yang sedikit dengan melakukan `random rotation`.\n\nMetode Augmentasi yang digunakan.\n```\nRotasi Acak dengan nilai rotasi di random pada kisaran -70 sd 70 derajad\n```","f5c6d68f":"## Mengecek Frekuensi Setiap Kelas<a class=\"anchor\" id=\"chapter2_1\"><\/a>\nMenegecek frekuensi tiap kelasnya pada data.","036d1103":"## Membuat Submission\nMembuat Submission File.","61259cde":"## Pemodelan Data Gambar (EfficientNet)<a class=\"anchor\" id=\"chapter3_1\"><\/a>\nModel yang digunakan untuk data gambar yaitu pretrained Deep Learning model yang cukup popular yaitu `EfficientNet`. `EfficientNet` digunakan karena kemampuannya yang cukup mengesankan dalam mengklasifikasi data gambar pada `ImageNet`.\n<img src=\"https:\/\/raw.githubusercontent.com\/tensorflow\/tpu\/master\/models\/official\/efficientnet\/g3doc\/params.png\" alt=\"Efficentnet\" width=\"600\" height=\"600\" style=\"display: block; margin-left: auto; margin-right: auto;\"\/>\n\nModel yang akan digunakan pada notebook ini untuk data gambar adalah `EfficientNetB7` dengan weight `noise-student`.\n\nPublished Paper : [EfficientNet](https:\/\/arxiv.org\/abs\/1905.11946)\n### Load Image Data - TF Dataset\nLoad data gambar ke bentuk Tensor lalu alokasikan ke TF Dataset untuk effisiensi memori. Data gambar yang digunakan adalah data hasil upsampling dengan menggunakan augmentasi pada kelas `0` sebesar `200%`.","ee726186":"### Masking Feature (Encode)<a class=\"anchor\" id=\"chapter1_2_3\"><\/a>\nMasking feature pada text. Feauture yang sudah di `.fit` pada data sebelumnya, proses ini dilakukan karena pada feature ini terdapat peletakan tanda baca yang random \/ tidak beraturan, sedangkan pada proses berikutnya akan dilakukan penormalisasian tanda baca sehingga akan mengganggu proses tersebut. Maka dari itu perlu dilakukan masking agar proses normalisasi kalimat berjalan dengan lancar.\n```\n# Contoh\nWebsite Google adalah http:\/\/google.com\/\nTrending #MosiTidakPercaya\n@jokowi adalah presiden RI\nLucu \ud83d\ude02\n\n# Encode\nWebsite Google adalah MASKURLS1MASK\nTrending MASKHASHTAGS1MASK\nMASKTAGS1MASK adalah presiden RI\nLucu MASKEMOJIS1MASK\n```","b56a3c6d":"### Mengecek Kata - Kata yang Muncul Perkelasnya<a class=\"anchor\" id=\"chapter2_3\"><\/a>\nMengecek frekuensi kata - kata yang muncul berdasarkan kelasnya.","a11603a0":"# Catatan Cakrawala Notebook - BDC Satria Data\n### Author :\n* Dimas Kuncoro Jati\n* Muhammad Amanda\n* Wahyu Setianto\n\n### Running Environment\n* Platform    : Kaggle\n* Accelerator : TPU\n* Data :\n    1. [BDC - Satria Data Catatan Cakrawala](https:\/\/www.kaggle.com\/wahyusetianto\/data-bdc)<br>\n    Data original dan data yang telah di preprocess oleh tim Catatan Cakrawala.\n    1. [Catatan Cakrawala Model](https:\/\/www.kaggle.com\/pencarikebahagiaan\/modelku) [Optional]<br>\n    Model terbaik yang telah di train tim Catatan Cakrawala\n\n### Note :\n* Seluruh kode dapat diakses melalui responsitory Github kami [di sini](https:\/\/github.com\/Hyuto\/BDC-Satria-Data)\n* Notebook ini juga bisa diakses [di sini](https:\/\/www.kaggle.com\/wahyusetianto\/bdc-main-notebook)","55a991f4":"Ditinjau dari grafik diatas bahwa:\n1. 53% dari keseluruhan data berasal dari tahun 2019 - 2020\n1. 95% data pada tahun 2019 - 2020 adalah Hoax","d8c042a0":"Meninjau `train history` pada model setiap `fold` - nya","75dc94bd":"### De-Emojized<a class=\"anchor\" id=\"chapter1_2_6\"><\/a>\nMengubah kode emoji yang ada pada text menjadi kata kata yang dapat dimengerti.\n```\n\ud83d\ude4f -> folded hands\n\ud83d\ude03 -> grinning face with big eyes\n```","94b5241f":"### Training Model\nMelakukan pelatihan terhadap model dari data train. Pelatihan dilakukan dengan ketentuan:\n```\nEPOCHS = 10\nSTEPS_PER_EPOCHS = len(TRAIN_X_) \/\/ BATCH_SIZE:128 (Jika menggunakan TPU)\n```\nPelatihan dilakukan dengan memperhatikan nilai dari `val_accuracy`, `epoch` yang memiliki nilai `val_accuracy` terbaik akan disave `weights` - nya dan akan di load pada saat melakukan evaluasi dan prediksi nantinya.","db9cbe76":"## Checking Missing File<a class=\"anchor\" id=\"chapter1_1_1\"><\/a>\nMengecek apakah ada file yang hilang \/ tidak bisa terbaca pada direktori gambar.","be4416a5":"Karena jumlah data yang cukup kecil dan sepertinya setiap feature memiliki informasi yang cukup berpengaruh terhadap kelasnya, kami memutuskan untuk tidak menghapus satupun feature yang ada.","d4a79070":"### Preprocess and Resizing<a class=\"anchor\" id=\"chapter1_1_2\"><\/a>\nUntuk mengatasi data gambar dengan resolusi yang berbeda - beda maka dilakukan dengan metode croping pada bagian `center` gambar dan `resizing` gambar tersebut ke ukuran yang di tetapkan.","b5a4bbff":"### Menampilkan Data Gambar\nMenampilkan beberapa data gambar yang ada","8179d1d4":"## Mencari Threshold<a class=\"anchor\" id=\"chapter4_2\"><\/a>\n> The decision for converting a predicted probability or scoring into a class label is governed by a parameter referred to as the \u201cthreshold\u201d The default value for the threshold is 0.5 for normalized predicted probabilities or scores in the range between 0 or 1.\n\nKami mendapatkan beberapa indikasi bahwa untuk mendapatkan hasil yang lebih baik perlu dilakukan penggeseran terhadap tresholdnya:\n1. Kelas pada label yang tidak berimbang<br>\nKelas `0` pada label memiliki frekuensi yang sangat kecil, berbeda jauh jika dibandingkan dengan pada kelas `1`. Kami menduga hal ini berpengaruh terhadap berat masing - masing kelas pada saat dilakukan pemodelan `tidak sama`.\n1. Nilai loss dan akurasi yang berbanding lurus pada saat training<br>\nPada saat training kami mendapatkan ketika `val_loss` (validation loss) menaik `val_accuracy` juga tetap menaik untuk beberapa saat. Hal ini cukup aneh karena seharusnya ketika nilai loss menaik maka nilai akurasi menurun. Hal ini membawa kami pada kesimpulan bahwa `threshold`-nya perlu untuk dirubah. ","373f14db":"Inisialisasi `TF Dataset`.","d16bb5ce":"Plotting WordCloud","4b723b2d":"# Preprocessing Data<a class=\"anchor\" id=\"chapter1\"><\/a>\nMelakukan preprocessing data sebelum diolah","485b9b76":"Melihat hasil dari tahap preprocess data text","d0b60af1":"Mengecek distribusi peluang `ensemble`","19f2350b":"Clear session & free up memory.","06f3a7fb":"Load Transformers terlebih dahulu","3cae3079":"Dapat dilihat dari tabel diatas bahwa data terdiri dari beberapa bahasa karena terdapat beberapa kata yang bukan berasal dari bahasa Indonesia seperti `\uff9f\uff8f\uffbb`, `\u52a0\u6cb9jiayou`, `\u09b9\u09df`, `\u09b8\u09b2`, dll.\n\n2. Mengeksport Vocabulary <br>\nMengeksport vocabulary telah dibangun kedalam file `.txt` untuk dapat dilakukan pengecekan secara manual terhadap kata-kata tersebut.\n```\nf = open(\"vocab.txt\", \"w\")\nf.write(\" \\n\".join(sorted(vocab.Vocab.values)))\nf.close()\n```\n\nSelanjutnya, mengimport file yang berisi kata - kata yang telah di cek secara manual lalu mengapikasikannya pada data text menggunakan kelas `SpellChecker`, fitting didapat dilakukan pada file yang diinginkan atau dapat menggunakan file yang telah di benarkan oleh tim Catatan Cakrawala dengan mengisikan `cc-hand-fixed` pada saat fitting class.","e4b24711":"Dari grafik diatas didapatkan kesimpulan bahwa panjang sequence data tersebar pada rentang 0 - 220 yang memusat pada rentang 0 - 100. Maka dari itu akan digunakan `250` sebagai acuan maximum dari panjang sequencenya","8ecc38bd":"### Load Dataset\nLoad dataset ke memory","7d2169fd":"Build & Scope model ke `TPU` & Compile model.","1bbf207a":"Dapat dilihat bahwa kelas 1 (`Hoax`) jauh lebih banyak dari kelas 0 (`Bukan Hoax`) atau terdapat `imbalance class` pada data. Untuk penanganannya kami menggunakan `Stratify` saat melakukan `Cross Validation` sehingga proporsi pada subset `latih` dan pada subset `validasi` sama.","e4c63331":"Berikut adalah Barplot untuk kata - kata pada kelas `1` yang mempunyai frekuensi kumunculan lebih dari kelas `0`.","09718ae7":"### Mengekstrack Feature dari Text<a class=\"anchor\" id=\"chapter1_2_2\"><\/a>\nMengekstrack feature seperti `URL`, `Hashtag`, `Tag`, dan `Emoji` dari text untuk melihat informasi dan frekuensi kemunculannya dalam setiap kelas pada data","2407159b":"Menampilkan hasil dari augmentasi.","ee225af7":"### Decode & Menghapus Tanda Baca<a class=\"anchor\" id=\"chapter1_2_5\"><\/a>\nMen-Decode (mengembalikan feature) serta menghapus tanda baca yang ada pada text.\n```\n# Contoh\nWebsite Google adalah MASKURLS1MASK\n\n# Decode\nWebsite Google adalah http:\/\/google.com\/\n\n# Remove Punctuation\nWebsite Google adalah httpgooglecom\n```","7d1193cf":"## First Things First\n* Install & Load Library\n* Global SEED-ing\n* Keeping Google Cloud Storage PATH","f2dab74f":"**Decoding image** : mengubah data gambar menjadi tensor.","ca16056b":"## Accelerator Detection\nMenggunakan GPU atau TPU dari Kaggle sebagai Accelerator.\n\nNote : Pastikan untuk menyalakan GPU \/ TPU sebagai Accelator, untuk effisiensi waktu pelatihan model.","dc269c18":"Load EfficientNet","a61cd088":"# Explorasi Data<a class=\"anchor\" id=\"chapter2\"><\/a>\nMelakukan explorasi pada data untuk mendapatkan informasi - informasi lebih jauh dari data.","ad9c5cf1":"Mengevaluasi model dengan `Accuracy Score` dan `F1 Score`.","df71a4e5":"### Load Bert Tokenizer\nLoad Bert Tokenizer dari library `transformers`","1233d2ff":"Menghapus data yang berduplikasi","21a4c8fb":"## Download Fungsi Tambahan\nMendownload fungsi yang sudah dibuat dari responsitory kami.","585acbc7":"## Melihat Sebaran Kelas per Waktunya<a class=\"anchor\" id=\"chapter2_2\"><\/a>\nMelihat sebaran kelas perwaktunya.\n1. Membuat kolom baru untuk menyimpan tahun & bulan\n1. Mengelompokkan berdasarkan tahun, bulan, dan labelnya","9151befa":"Setelah nama file yang hilang dibenarkan, apply nama file yang benar ke main dataframe.","251316f2":"## Data Gambar<a class=\"anchor\" id=\"chapter1_1\"><\/a>\nMelakukan preprocessing pada data gambar","3c3508b7":"Dapat dilihat gambar memiliki resolusi yang `berbeda - beda`.","cb224ca8":"Merubah tipe pada kolom tanggal menjadi `datetime`. Kolom tanggal tidak dapat langsung dirubah tipe datanya menjadi `datetime` dikarenakan ada nya beberapa element yang mengandung kata `Okt` dan `Agu` sehingga harus diubah menjadi `Oct` dan `Aug`","b095856c":"### Mengecek Duplicated Values<a class=\"anchor\" id=\"chapter1_2_1\"><\/a>\nMengecek keberadaan data yang berduplikasi.","5d28931e":"Dapat dilihat dari plot diatas bahwa sebaran peluang mulai memusat pada selang `0.65` - `1`.\n\nMaka dari itu kami putuskan untuk mengambil `0.65` sebagai `threshold`-nya.","b3ec534b":"# Kesimpulan & Saran<a class=\"anchor\" id=\"chapter5\"><\/a>\n## Kesimpulan\n1. Data memiliki frekuensi kelas yang `tidak berimbang` sehingga perlu di perhatikan saat pembuatan model\n1. Dengan menggukan model yang kami buat didapatkan hasil yang cukup baik dengan f1 score 94 % namun memerlukan komputasi yang cukup berat dan biaya yang mahal untuk ukuran data yang tidak terlalu besar.\n\n## Saran\n1. Jika memungkinkan diperlukan `penambahan data` pada label bukan hoax agar frekuensi setiap kelas pada data tidak berbanding cukup jauh\n1. Menggunakan model `multimodal` karena hasil pada ensembel model gambar dan model teks menunjukkan hasil yang lebih baik.\n\n\u00a9 Catatan Cakrawala 2020","4c0a1fa0":"Pada notebook ini pengolahan data text dilakukan menggunakan `judul` dan `narasi`, sehingga perlu dilakukan penyatuan kolom `judul` dan kolom `narasi` dipisahkan dengan spasi.","53f08d4e":"### Membuat Model\nMembuat model yang digunakan untuk memprediksi data teks, berikut adalah arsitekturnya\n<img src=\"https:\/\/raw.githubusercontent.com\/Hyuto\/BDC-Satria-Data\/master\/Sample%20Images\/model_2.png\" alt=\"BertModel\" width=\"700\" height=\"800\" style=\"display: block; margin-left: auto; margin-right: auto;\"\/>","9a7e9dcb":"## Data Text<a class=\"anchor\" id=\"chapter1_2\"><\/a>\nMelakukan preprocessing pada data text. \n\nLoad fungsi pada file `Preprocess.py` terlebih dahulu","e5b0f81c":"### Up Sampling Data<a class=\"anchor\" id=\"chapter1_1_4\"><\/a>\nMengapply Preprocess & Augmentasi ke data gambar dan menyimpan gambar pada direktori baru.\nMenggunakan Fungsi tambahan yang sudah di download pada file `RPU.py`\n\nFungsi akan mengembalikan List direktori gambar dan juga labelnya.\n\n`ApplyAUG -> IMAGE_DIR, Label`\n\nContoh Penggunaan\n```\nfrom RPU import ApplyAUG\n\nTRAIN_X, TRAIN_y = (\n       ApplyAUG(TRAIN_X,               # List atau Array direktori dari gambar\n                TRAIN_y,               # List atau Array kelas(label) dari TRAIN_X [One Hot Encoding]\n                PATH,                  # Direktori data gambar\n                up_sample_ratio = 1,   # Rasio Up Sample\n                up_sample_class = '0', # Spesifikasi Class yang akan di Up Sample  \n                data_aug = data_augmentation,  # Fungsi Augmentasi\n                LP = load_and_preprocess_image # Fungsi Load & Preprocess\n                )\n)\n```\nAtau bisa juga menggunakan shell\n```\n!python RPU.py PATH SIZE TEST_SIZE UP_SAMPLES UP_SAMPLE_CLASS\n```\nNamun pada kernel ini tidak akan di gunakan fungsi tersebut. Melaikan menggunakan hasil dari fungsi tersebut, karena kernel ini di run pada TPU & TPU tidak dapat membaca file diluar dari `GCS` (lokal direktori).\n\nNote : Up Sample data pada Notebook ini didapatkan dengan command berikut\n```\n!python RPU.py '..\/input\/data-bdc\/Data BDC - Satria Data 2020\/Data Latih\/File Gambar Data Latih\/' \\\n                SIZE:512 TEST_SIZE:0.15 UP_SAMPLES:0.5-1-2 UP_SAMPLE_CLASS:0\n```","f2a79608":"### Train & Evaluate Model\nMelakukan `training` pada train data. Train dilakukan pada `fold 3` & `fold 6` karena kedua fold tersebut memberikan nilai akurasi yang cukup baik dari model pada `fold` lainnya. Training memiliki konfihurasi sebagai berikut:\n```\nepochs = 15\nsteps_per_epoch = len(x_train)\/\/BATCH_SIZE:128 (jika menggunakann TPU)\n```\nTraining dilakukan dengan memperhatikan nilai `val_accuracy` pada setiap epochnya. Epoch dengan score `val_accuracy` tertinggi akan di save `weight` - nya untuk diload saat melakukan evaluasi dan pemrediksian terhadap data test.","43c86962":"Plotting WordCloud","62fb2bce":"load `folding.csv` dari responsitory kami.","ff5891d4":"# Pemodelan<a class=\"anchor\" id=\"chapter3\"><\/a>\nMembuat model untuk memprediksi kelas `0` (`tidak hoax`) dan kelas `1` (`hoax`) pada data test.","ebd622a6":"### Memhapus Kata yang Kurang Penting & Stopwords<a class=\"anchor\" id=\"chapter1_2_8\"><\/a>\nMenghapus kata - kata yang kurang penting seperti kata yang hanya terdiri dari 1 karakter dan Stopwords. Untuk Stopwords kami menggabungkan stopwords bahasa Indonesia dari `Spacy` dan `Sastrawi` serta bahasa Inggris dari `Spacy`.","89868ab8":"Terdapat 9 file gambar pada data train dan 2 file gambar pada data test yang hilang. Hal ini diduga karena nama file berbeda pada direktori gambar dengan nama file yang ada pada dataframe. Maka dari itu kami melakukan pengecekan nama file pada direktori gambar dengan menyamakan `ID`-nya.","a8d20788":"### Encode Teks\nData teks akan di encode menjadi 3 tipe untuk input kedalam model.\n* input ids adalah ID \/ nomor yang merepresentasikan kata\n* attention mask adalah keterangan dari ID yang harus diperhatikan ditandakan oleh nomor 1.\n* token type id adalah ID yang berhubungan dengan multiple sequance."}}