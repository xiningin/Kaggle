{"cell_type":{"9a512cdc":"code","d62bb650":"code","6fd414ff":"code","8333f14e":"code","44d558cc":"code","0e762c77":"code","2a2ebe60":"code","9d61e3fa":"code","f72b73a9":"code","1750b924":"code","3726eb9f":"code","649e0167":"code","7d5f13eb":"markdown","dee57061":"markdown","7cd2b32d":"markdown","393bbcaa":"markdown","a76f6138":"markdown","e15f8e88":"markdown","aad8bb39":"markdown","00a25991":"markdown","487134d5":"markdown","11aac213":"markdown"},"source":{"9a512cdc":"import time\nimport numpy as np \nimport pandas as pd \nfrom matplotlib import pyplot as plt\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import GradientBoostingClassifier","d62bb650":"train = pd.read_csv('\/kaggle\/input\/digit-recognizer\/train.csv')","6fd414ff":"X = train\ny = (train['label']).values\nX.drop(columns=['label'], inplace=True)\nX = X.values","8333f14e":"x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\nprint(x_train.shape, y_train.shape, x_test.shape, y_test.shape)","44d558cc":"unique, count = np.unique(y_train, return_counts=True)\nplt.figure(figsize=(20,2))\nplt.bar(unique, count, data='True', color = 'gray')","0e762c77":"T = 50\n\nerr_nn = []\nerr_dnn = []\n\nnn_loss = []\ndnn_loss = []\n\nfit_ti_nn = []\nfit_ti_dnn = []\n","2a2ebe60":"\nfor i in range(1, T + 1):\n\n    nn = MLPClassifier(hidden_layer_sizes=(i,),\n                        solver=\"adam\",\n                        random_state=2)\n    nn.fit(x_train, y_train)\n    nn_loss.append(np.mean(nn.loss_curve_, axis=0))\n    pipe_nn = Pipeline([(\"scaler\", StandardScaler()), (\"clf\", nn)])\n    n0 = time.time()\n    pipe_nn.fit(x_train, y_train)\n    n1 = time.time()\n    fit_ti_nn.append(n1 - n0)\n    err_nn.append(pipe_nn.score(x_test, y_test))\n\n                  \n\n    dnn = MLPClassifier(hidden_layer_sizes=(i, i),\n                    solver=\"adam\",\n                    random_state=2)\n    dnn.fit(x_train, y_train)\n    dnn_loss.append(np.mean(dnn.loss_curve_, axis=0))\n    pipe_dnn = Pipeline([(\"scaler\", StandardScaler()), (\"clf\", dnn)])\n    d0 = time.time()\n    pipe_dnn.fit(x_train, y_train)\n    d1 = time.time()\n    fit_ti_dnn.append(d1 - d0)\n    err_dnn.append(pipe_dnn.score(x_test, y_test))","9d61e3fa":"t = range(1, T + 1)","f72b73a9":"\nnn_loss = np.mean(nn_loss, axis=0)\ndnn_loss = np.mean(dnn_loss, axis=0)\n\n\nnn = np.mean(err_nn, axis=0)\ndn = np.mean(err_dnn, axis=0)\n\n\nnn_d = np.std(err_nn, axis=0)\ndnn_d = np.std(err_dnn, axis=0)\n\n\nnn_ti = np.mean(fit_ti_nn, axis=0)\ndnn_ti = np.mean(fit_ti_dnn, axis=0)","1750b924":"plt.plot(nn_loss, '-', label='NN',  color=\"tab:orange\")\nplt.plot(dnn_loss, '-', label='Deep-NN', color=\"tab:green\")\nplt.legend('NN', 'Deep-NN')\nplt.title(\"Loss curve\")\nplt.show()","3726eb9f":"plt.plot(t, nn, t, dn)\nplt.legend([\"NN\", \"Deep-NN\"])\nplt.fill_between(t, nn - nn_d, nn + nn_d, color=\"tab:orange\", alpha=0.3)\nplt.fill_between(t, dn - dnn_d, dn + dnn_d, color=\"tab:green\", alpha=0.3)\nplt.autoscale(enable=True, axis='both')\nplt.yticks(np.arange(0, 1, step=0.09))\nplt.ylabel(\"Accuracy\")\nplt.xlabel(\"Number of neurons\")\nplt.title(\"Average generalization accuracy\")\nplt.show()","649e0167":"plt.plot(t, gb_ti)\nplt.plot(t, nn_ti)\nplt.plot(t, dnn_ti)\nplt.legend([\"GBNN\", \"NN\", \"Deep-NN\"])\nplt.ylabel(\"Time (Second)\")\nplt.xlabel(\"Number of neurons\")\nplt.title(\"Training time average\")\nplt.show()","7d5f13eb":"# Training the models\nTrained two models by considering the StratifiedKFold as a splitting method","dee57061":"# Splitting the input\nFor the sake of validation part","7cd2b32d":"# Accuracy trend","393bbcaa":"# Importing the libs","a76f6138":"# Defining the inputsImporting the input","e15f8e88":"# Loss curve","aad8bb39":"# Class distribution","00a25991":"# Importing the input","487134d5":"# Fit time trend","11aac213":"# About this NoteBook\n##### At first glance, one will see that all available notebooks have done using CNN considering the MNIST dataset, but you can not find a notebook that uses other models to train them over this problem.\n##### Therefore, in the following notebook, I skipped the studied topics such as plotting the images of the datasets, convolutional layers of Keras, and data distribution, instead, I tried to train two other models and compare their performance.\n##### Instead of CNN from the Keras library, I implemented a Deep neural network and the shallow one from The Sklearn library. The comparison has been done in terms of accuracy, loss, and training time.\n\n#### The layers and neurons have been reduced due to the computational cost.\n"}}