{"cell_type":{"5eb4234d":"code","07677099":"code","a221223c":"code","cadeef4a":"code","d2510f69":"code","99ffb010":"code","9b1332c8":"code","d607df19":"code","333d2c20":"code","30e665e8":"code","75e7c336":"code","4e4b00f2":"code","173e44ba":"code","9a3529b2":"code","0e784681":"code","18cd144b":"code","37afe620":"code","087adc03":"code","81456bbb":"code","aa977c7b":"code","488a7b8d":"code","4a9da0e9":"code","cdc84dfa":"code","8dee5e17":"code","ec137b41":"code","7b908dc1":"code","46db676b":"code","b8a265f9":"code","0da72288":"code","2d3348fc":"code","1e05cc72":"code","d61236bc":"code","39295923":"code","7f899f7a":"code","4535a108":"code","b9b76fe8":"code","e3b097cf":"code","bf4ce6e2":"code","d6956ca5":"code","ef9f4822":"code","06340c9e":"code","0487f48a":"code","bae372d7":"code","0cf5e8bf":"markdown","dcc33147":"markdown","38850ec2":"markdown","67ccbe81":"markdown","d01ac031":"markdown","666af597":"markdown","b161c3b0":"markdown","729381a8":"markdown","d017cfc2":"markdown","398ad8c5":"markdown","b0a11a63":"markdown","a8524d85":"markdown","60bdc33d":"markdown","d4e52d0a":"markdown","84118c3c":"markdown","c6d17a00":"markdown"},"source":{"5eb4234d":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom matplotlib.pyplot import figure\nimport plotly.graph_objs as go\nfrom plotly.offline import init_notebook_mode, iplot\nfrom sklearn.utils import shuffle\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import LabelEncoder\nimport time\nimport os\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","07677099":"df = pd.read_csv('\/kaggle\/input\/mushroom-classification\/mushrooms.csv')\npd.options.display.max_columns = None\ndf.head()","a221223c":"percent_missing = df.isnull().sum() * 100 \/ len(df)\nmissing_values = pd.DataFrame({'percent_missing': percent_missing})\nmissing_values.sort_values(by ='percent_missing' , ascending=False)","cadeef4a":"sns.set(style=\"ticks\")\nf = sns.countplot(x=\"class\", data=df, palette=\"bwr\")\nplt.show()","d2510f69":"df['class'].value_counts()","99ffb010":"X = df.drop(['class'], axis = 1)\nY = df['class']","9b1332c8":"X = pd.get_dummies(X, prefix_sep='_')\nX.head()","d607df19":"len(X.columns)","333d2c20":"Y = LabelEncoder().fit_transform(Y)\n#np.set_printoptions(threshold=np.inf)\nY","30e665e8":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report,confusion_matrix\nfrom sklearn.ensemble import RandomForestClassifier\n\nX = StandardScaler().fit_transform(X)","75e7c336":"def forest_test(X, Y):\n    X_Train, X_Test, Y_Train, Y_Test = train_test_split(X, Y, test_size = 0.30, random_state = 101)\n    start = time.process_time()\n    trainedforest = RandomForestClassifier(n_estimators=700).fit(X_Train,Y_Train)\n    print(time.process_time() - start)\n    predictionforest = trainedforest.predict(X_Test)\n    print(confusion_matrix(Y_Test,predictionforest))\n    print(classification_report(Y_Test,predictionforest))","4e4b00f2":"def complete_test_2D(X, Y, plot_name = ''):\n    Small_df = pd.DataFrame(data = X, columns = ['C1', 'C2'])\n    Small_df = pd.concat([Small_df, df['class']], axis = 1)\n    Small_df['class'] = LabelEncoder().fit_transform(Small_df['class'])\n    forest_test(X, Y)\n    data = []\n    for clas, col, name in zip((1, 0), ['red', 'darkblue'], ['Poisonous', 'Edible']):\n\n        trace = dict(\n            type='scatter',\n            x= Small_df.loc[Small_df['class'] == clas, 'C1'],\n            y= Small_df.loc[Small_df['class'] == clas, 'C2'],\n            mode= 'markers',\n            name= name,\n            marker=dict(\n                color=col,\n                size=12,\n                line=dict(\n                    color='rgba(217, 217, 217, 0.14)',\n                    width=0.5),\n                opacity=0.8)\n        )\n        data.append(trace)\n\n    layout = dict(\n            title= plot_name + ' 2D Dimensionality Reduction',\n            xaxis=dict(title='C1', showline=False),\n            yaxis=dict(title='C2', showline=False)\n    )\n    fig = dict(data=data, layout=layout)\n    iplot(fig)","173e44ba":"def complete_test_3D(X, Y, plot_name = ''):\n    Small_df = pd.DataFrame(data = X, columns = ['C1', 'C2', 'C3'])\n    Small_df = pd.concat([Small_df, df['class']], axis = 1)\n    Small_df['class'] = LabelEncoder().fit_transform(Small_df['class'])\n    forest_test(X, Y)\n    data = []\n    for clas, col, name in zip((1, 0), ['red', 'darkblue'], ['Poisonous', 'Edible']):\n\n        trace = dict(\n            type='scatter3d',\n            x= Small_df.loc[Small_df['class'] == clas, 'C1'],\n            y= Small_df.loc[Small_df['class'] == clas, 'C2'],\n            z= Small_df.loc[Small_df['class'] == clas, 'C3'],\n            mode= 'markers',\n            name= name\n        )\n        data.append(trace)\n\n    layout = {\n        \"scene\": {\n          \"xaxis\": {\n            \"title\": \"C1\", \n            \"showline\": False\n          }, \n          \"yaxis\": {\n            \"title\": \"C2\", \n            \"showline\": False\n          }, \n          \"zaxis\": {\n            \"title\": \"C3\", \n            \"showline\": False\n          }\n        }, \n        \"title\": plot_name + ' 3D Dimensionality Reduction'\n    }\n    fig = dict(data=data, layout=layout)\n    iplot(fig)","9a3529b2":"forest_test(X, Y)","0e784681":"from sklearn.decomposition import PCA\n\npca = PCA(n_components=2)\nX_pca = pca.fit_transform(X)\nPCA_df = pd.DataFrame(data = X_pca, columns = ['PC1', 'PC2'])\nPCA_df = pd.concat([PCA_df, df['class']], axis = 1)\nPCA_df['class'] = LabelEncoder().fit_transform(PCA_df['class'])\nPCA_df.head()","18cd144b":"figure(num=None, figsize=(8, 8), dpi=80, facecolor='w', edgecolor='k')\n\nclasses = [1, 0]\ncolors = ['r', 'b']\nfor clas, color in zip(classes, colors):\n    plt.scatter(PCA_df.loc[PCA_df['class'] == clas, 'PC1'], PCA_df.loc[PCA_df['class'] == clas, 'PC2'], c = color)\n    \nplt.xlabel('Principal Component 1', fontsize = 12)\nplt.ylabel('Principal Component 2', fontsize = 12)\nplt.title('2D PCA', fontsize = 15)\nplt.legend(['Poisonous', 'Edible'])\nplt.grid()","37afe620":"pca.explained_variance_ratio_","087adc03":"complete_test_2D(X_pca, Y, 'PCA')","81456bbb":"var_ratio = pca.explained_variance_ratio_\ncum_var_ratio = np.cumsum(var_ratio)\n\ntrace1 = dict(\n    type='bar',\n    x=['PC %s' %i for i in range(1,5)],\n    y=var_ratio,\n    name='Individual'\n)\n\ntrace2 = dict(\n    type='scatter',\n    x=['PC %s' %i for i in range(1,5)], \n    y=cum_var_ratio,\n    name='Cumulative'\n)\n\ndata = [trace1, trace2]\n\nlayout=dict(\n    title='Explained variance Ratio by each principal components',\n    yaxis=dict(\n        title='Explained variance ratio in percent'\n    ),\n    annotations=list([\n        dict(\n            x=1.16,\n            y=1.05,\n            xref='paper',\n            yref='paper',\n            showarrow=False,\n        )\n    ])\n)\n\nfig = dict(data=data, layout=layout)\niplot(fig)","aa977c7b":"from itertools import product\n\nX_Reduced, X_Test_Reduced, Y_Reduced, Y_Test_Reduced = train_test_split(X_pca, Y, test_size = 0.30, random_state = 101)\ntrainedforest = RandomForestClassifier(n_estimators=700).fit(X_Reduced,Y_Reduced)\n\nx_min, x_max = X_Reduced[:, 0].min() - 1, X_Reduced[:, 0].max() + 1\ny_min, y_max = X_Reduced[:, 1].min() - 1, X_Reduced[:, 1].max() + 1\nxx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1), np.arange(y_min, y_max, 0.1))\nZ = trainedforest.predict(np.c_[xx.ravel(), yy.ravel()])\nZ = Z.reshape(xx.shape)\nplt.contourf(xx, yy, Z,cmap=plt.cm.coolwarm, alpha=0.4)\nplt.scatter(X_Reduced[:, 0], X_Reduced[:, 1], c=Y_Reduced, s=20, edgecolor='k')\nplt.xlabel('Principal Component 1', fontsize = 12)\nplt.ylabel('Principal Component 2', fontsize = 12)\nplt.title('Random Forest', fontsize = 15)\nplt.show()","488a7b8d":"pca = PCA(n_components=3)\nX_pca = pca.fit_transform(X)\ncomplete_test_3D(X_pca, Y, 'PCA')","4a9da0e9":"var_ratio = pca.explained_variance_ratio_\ncum_var_ratio = np.cumsum(var_ratio)\n\ntrace1 = dict(\n    type='bar',\n    x=['PC %s' %i for i in range(1,5)],\n    y=var_ratio,\n    name='Individual'\n)\n\ntrace2 = dict(\n    type='scatter',\n    x=['PC %s' %i for i in range(1,5)], \n    y=cum_var_ratio,\n    name='Cumulative'\n)\n\ndata = [trace1, trace2]\n\nlayout=dict(\n    title='Explained variance Ratio by each principal components',\n    yaxis=dict(\n        title='Explained variance ratio in percent'\n    ),\n    annotations=list([\n        dict(\n            x=1.16,\n            y=1.05,\n            xref='paper',\n            yref='paper',\n            showarrow=False,\n        )\n    ])\n)\n\nfig = dict(data=data, layout=layout)\niplot(fig)","cdc84dfa":"from sklearn.manifold import TSNE\n\ntime_start = time.time()\ntsne = TSNE(n_components=2, verbose=1, perplexity=40, n_iter=300)\nX_tsne = tsne.fit_transform(X)\nprint('t-SNE done! Time elapsed: {} seconds'.format(time.time()-time_start))","8dee5e17":"sns.scatterplot(\n    x=X_tsne[:,0], y=X_tsne[:,1],\n    hue=Y,\n    palette=sns.color_palette(\"hls\", 2),\n    data=df,\n    legend=\"full\",\n    alpha=0.3\n)","ec137b41":"complete_test_2D(X_tsne, Y, 't-SNE')","7b908dc1":"tsne = TSNE(n_components=3, verbose=1, perplexity=40, n_iter=300)\nX_tsne = tsne.fit_transform(X)\ncomplete_test_3D(X_tsne, Y, 't-SNE')","46db676b":"from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n\nlda = LinearDiscriminantAnalysis(n_components=1)\n\n# run an LDA and use it to transform the features\nX_lda = lda.fit(X, Y).transform(X)\nprint('Original number of features:', X.shape[1])\nprint('Reduced number of features:', X_lda.shape[1])","b8a265f9":"lda.explained_variance_ratio_","0da72288":"forest_test(X_lda, Y)","2d3348fc":"X_Reduced, X_Test_Reduced, Y_Reduced, Y_Test_Reduced = train_test_split(X_lda, Y, test_size = 0.30, random_state = 101)\n\nstart = time.process_time()\nlda = LinearDiscriminantAnalysis().fit(X_Reduced,Y_Reduced)\nprint(time.process_time() - start)\npredictionlda = lda.predict(X_Test_Reduced)\nprint(confusion_matrix(Y_Test_Reduced,predictionlda))\nprint(classification_report(Y_Test_Reduced,predictionlda))","1e05cc72":"LDA_df = pd.DataFrame(data = X_lda, columns = ['LDA1'])\nLDA_df = pd.concat([LDA_df, df['class']], axis = 1)\nLDA_df['class'] = LabelEncoder().fit_transform(LDA_df['class'])\n\nfigure(num=None, figsize=(8, 8), dpi=80, facecolor='w', edgecolor='k')\nsns.distplot(LDA_df.loc[LDA_df['class'] == 0]['LDA1'], label = 'Edible', hist=True, kde=True, rug=True)\nsns.distplot(LDA_df.loc[LDA_df['class'] == 1]['LDA1'], label = 'Poisonous', hist=True, kde=True, rug=True)\nplt.legend(loc='upper right')","d61236bc":"sns.jointplot(x=\"LDA1\", y=\"class\", data=LDA_df, kind=\"kde\")","39295923":"grid = sns.JointGrid(x='LDA1', y='class', data=LDA_df)\n\ng = grid.plot_joint(sns.scatterplot, hue='class', data=LDA_df)\nsns.kdeplot(LDA_df.loc[LDA_df['class']== 0, 'LDA1'], ax=g.ax_marg_x, legend=False)\nsns.kdeplot(LDA_df.loc[LDA_df['class']== 1, 'LDA1'], ax=g.ax_marg_x, legend=False)\nsns.kdeplot(LDA_df.loc[LDA_df['class']== 0, 'LDA1'], ax=g.ax_marg_y, vertical=True, legend=False)\nsns.kdeplot(LDA_df.loc[LDA_df['class']== 1, 'LDA1'], ax=g.ax_marg_y, vertical=True, legend=False)","7f899f7a":"from sklearn.decomposition import FastICA\n\nica = FastICA(n_components=2)\nX_ica = ica.fit_transform(X)\nprint('Original number of features:', X.shape[1])\nprint('Reduced number of features:', X_ica.shape[1])","4535a108":"complete_test_2D(X_ica, Y, 'ICA')","b9b76fe8":"ica = FastICA(n_components=3)\nX_ica = ica.fit_transform(X)\n\ncomplete_test_3D(X_ica, Y, 'ICA')","e3b097cf":"from sklearn.manifold import LocallyLinearEmbedding\n\nembedding = LocallyLinearEmbedding(n_components=2)\nX_lle = embedding.fit_transform(X)\nprint('Original number of features:', X.shape[1])\nprint('Reduced number of features:', X_lle.shape[1])","bf4ce6e2":"complete_test_2D(X_lle, Y, 'LLE')","d6956ca5":"embedding = LocallyLinearEmbedding(n_components=3)\nX_lle = embedding.fit_transform(X)\ncomplete_test_3D(X_lle, Y, 'LLE')","ef9f4822":"from keras.layers import Input, Dense\nfrom keras.models import Model\n\ninput_layer = Input(shape=(X.shape[1],))\nencoded = Dense(2, activation='relu')(input_layer)\ndecoded = Dense(X.shape[1], activation='softmax')(encoded)\nautoencoder = Model(input_layer, decoded)\nautoencoder.compile(optimizer='adam', loss='binary_crossentropy')\n\nX1, X2, Y1, Y2 = train_test_split(X, X, test_size=0.3, random_state=101)\n\nautoencoder.fit(X1, Y1,\n                epochs=100,\n                batch_size=300,\n                shuffle=True,\n                verbose = 0,\n                validation_data=(X2, Y2))\n\nencoder = Model(input_layer, encoded)\nX_ae = encoder.predict(X)","06340c9e":"complete_test_2D(X_ae, Y, 'AE')","0487f48a":"input_layer = Input(shape=(X.shape[1],))\nencoded = Dense(3, activation='relu')(input_layer)\ndecoded = Dense(X.shape[1], activation='softmax')(encoded)\nautoencoder = Model(input_layer, decoded)\nautoencoder.compile(optimizer='adam', loss='binary_crossentropy')\n\nX1, X2, Y1, Y2 = train_test_split(X, X, test_size=0.3, random_state=101)\n\nautoencoder.fit(X1, Y1,\n                epochs=100,\n                batch_size=300,\n                shuffle=True,\n                verbose = 0,\n                validation_data=(X2, Y2))\n\nencoder = Model(input_layer, encoded)\nX_ae = encoder.predict(X)","bae372d7":"complete_test_3D(X_ae, Y, 'AE')","0cf5e8bf":"### Indipendent Component Analysis (ICA)","dcc33147":"## Preprocessing","38850ec2":"### Locally Linear Embedding (LLE)","67ccbe81":"## Machine Learning","d01ac031":"LDA is supervised learning dimensionality reduction technique and Machine Learning classifier.\u00a0\n\nLDA aims to maximize the distance between the mean of each class and minimize the spreading within the class itself. LDA uses therefore within classes and between classes as measures. This is a good choice because maximizing the distance between the means of each class when projecting the data in a lower-dimensional space can lead to better classification results (thanks to the reduced overlap between the different classes).\u00a0\n\nWhen using LDA, is assumed that the input data follows a Gaussian Distribution (like in this case), therefore applying LDA to not Gaussian data can possibly lead to poor classification results.","666af597":"### t-Distributed Stochastic Neighbor Embedding (t-SNE)","b161c3b0":"ICA is a linear dimensionality reduction method which takes as input data a mixture of independent components and it aims to correctly identify each of them (deleting all the unnecessary noise). Two input features can be considered independent if both their linear and not linear dependance is equal to zero.\n\nIndependent Component Analysis is commonly used particularly in medical applications such as EEG and fMRI analysis to separate useful signals from unhelpful ones.\n\nAs a simple example of an ICA application, let's consider we are given an audio registration in which there are two different people talking. Using ICA we could, for example, try to identify the two different independent components in the registration (the two different people). In this way, we could make our unsupervised learning algorithm recognise between the different speakers in the conversation.","729381a8":"We have considered so far methods such as PCA and LDA, which are able to perform really well in case of linear relationships between the different features, we will now move on considering how to deal with non-linear cases.\nLocally Linear Embedding is a dimensionality reduction technique based on Manifold Learning. A Manifold is an object of D dimensions which is embedded in an higher-dimensional space. Manifold Learning aims then to make this object representable in its original D dimensions instead of being represented in an unnecessary greater space.\n\nA typical example used to explain Manifold Learning in Machine Learning is the Swiss Roll Manifold. We are given as input some data which has a distribution resembling the one of a roll (in a 3D space), and we can then unroll it so that to reduce our data into a two-dimensional space.\n\nSome examples of Manifold Learning algorithms are: Isomap, Locally Linear Embedding, Modified Locally Linear Embedding, Hessian Eigenmapping, etc\u2026\n\nAccording to the Scikit-learn documentation:\n> Locally linear embedding (LLE) seeks a lower-dimensional projection of the data which preserves distances within local neighborhoods. It can be thought of as a series of local Principal Component Analyses which are globally compared to find the best non-linear embedding.","d017cfc2":"PCA is one of the most used linear dimensionality reduction technique. When using PCA, we take as input our original data and try to find a combination of the input features which can best summarize the original data distribution so that to reduce its original dimensions. PCA is able to do this by maximizing variances and minimizing the reconstruction error by looking at pair wised distances. In PCA, our original data is projected into a set of orthogonal axes and each of the axes gets ranked in order of importance.\u00a0\n\nPCA is an unsupervised learning algorithm, therefore it doesn't care about the data labels but only about variation. This can lead in some cases to misclassification of data.","398ad8c5":"### Principal Component Analysis (PCA)","b0a11a63":"### Autoencoders","a8524d85":"### Linear Discriminant Analysis (LDA)","60bdc33d":"# Feature Extraction","d4e52d0a":"In this notebook, I will make use of the Mushroom Classification dataset to try to predict if a Mushroom is poisonous or not by looking at the given features. I will successivelly try differerent feature extraction techniques to see how this can affect training times and overall model accuracy.\n\nReducing the number of features in a dataset, can lead to:\n\n- Accuracy improvements\n- Overfitting risk reduction\n- Speed up in training\n- Improved Data Visualization","84118c3c":"Autoencoders are a family of Machine Learning algorithms which can be used as a dimensionality reduction technique. The main difference between Autoencoders and other dimensionality reduction techniques is that Autoencoders use non-linear transformations to project data from a high dimension to a lower one.\u00a0\nThere exist different types of Autoencoders such as:\u00a0\n- Denoising Autoencoder\u00a0\n- Variational Autoencoder\n- Convolutional Autoencoder\n- Sparse Autoencoder\n\nIn this example, we will start by building a basic Autoencoder. The basic architecture of an Autoencoder can be broken down into 2 main components:\n- Encoder: takes the input data and compress it, so that to remove all the possible noise and unhelpful information. The output of the Encoder stage is usually called bottleneck or latent-space.\u00a0\n- Decoder: takes as input the encoded latent space and tries to reproduce the original Autoencoder input using just it's compressed form (the encoded latent space).\n\nIf all the input features are independent of each other, then the Autoencoder will find particularly difficult to encode and decode to input data into a lower-dimensional space.\n\nFor this example, I decided to use ReLu as the activation function for the encoding stage and Softmax for the decoding stage. If I wouldn't have used non-linear activation functions, then the Autoencoder would have tried to reduce the input data using a linear transformation (therefore giving us a result similar to if we would have used PCA).","c6d17a00":"t-SNE is non-linear dimensionality reduction technique which is typically used to visualize high dimensional datasets. Some of the main applications of t-SNE are Natural Language Processing (NLP), speech processing, etc\u2026\u00a0\n\n\nt-SNE works by minimizing the divergence between a distribution constituted by the pairwise probability similarities of the input features in the original high dimensional space and its equivalent in the reduced low dimensional space. t-SNE makes then use of the Kullback-Leiber (KL) divergence in order to measure the dissimilarity of the two different distributions. The KL divergence is then minimized using gradient descent.\u00a0\n\nWhen using t-SNE, the higher dimensional space is modelled using a Gaussian Distribution, while the lower-dimensional space is modelled using a Student's t-distribution. This is done, in order to avoid an imbalance in the neighbouring points distance distribution caused by the translation into a lower-dimensional space."}}