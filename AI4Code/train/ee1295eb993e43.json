{"cell_type":{"6ea8100c":"code","ebc31651":"code","a1b4e5e6":"code","8fa1ae06":"code","399ec94a":"code","b27c32d2":"code","af39ac9a":"code","c7bf66dc":"code","f4961787":"code","faa44599":"code","a0c50d1f":"code","050af5da":"code","7b04fc0c":"code","c200b399":"code","9c589c2a":"code","d9e3da9a":"code","8be688c8":"code","5f8a8db4":"code","23dc5458":"code","2fdfc9d1":"code","ba784683":"code","9ce12702":"code","a8e99088":"code","a0b22e9c":"code","871e3521":"code","aa025236":"code","f9575e74":"code","4a025342":"code","b9a36f16":"code","d94f3535":"code","c9a864c4":"markdown","a457eaf2":"markdown"},"source":{"6ea8100c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","ebc31651":"from fastai.tabular.all import *\nfrom sklearn.model_selection import KFold, train_test_split\nfrom sklearn.metrics import roc_auc_score, roc_curve, auc, mean_absolute_error, mean_squared_error\nfrom sklearn.ensemble import RandomForestRegressor\nfrom catboost import CatBoostRegressor\nfrom xgboost import XGBClassifier, XGBRFRegressor\nimport lightgbm as lgb\nimport seaborn as sns\nimport optuna\n","a1b4e5e6":"df_train = pd.read_csv(\"..\/input\/song-popularity-prediction\/train.csv\")\ndf_test = pd.read_csv(\"..\/input\/song-popularity-prediction\/test.csv\")\nsubmission = pd.read_csv(\"..\/input\/song-popularity-prediction\/sample_submission.csv\")","8fa1ae06":"df_train.drop('id', axis=1,inplace=True)\ndf_test.drop('id', axis=1,inplace=True)","399ec94a":"\nFEATURES = [\n    \"song_duration_ms\",\n    \"acousticness\",\n    \"danceability\",\n    \"energy\",\n    \"instrumentalness\",\n    \"key\",\n    \"liveness\",\n    \"loudness\",\n    \"audio_mode\",\n    \"speechiness\",\n    \"tempo\",\n    \"time_signature\",\n    \"audio_valence\",\n]","b27c32d2":"len(FEATURES)","af39ac9a":"df_train.hist(bins=20,figsize=(20,20))","c7bf66dc":"df_test.hist(bins=20,figsize=(20,20))","f4961787":"df_train.instrumentalness=np.log(df_train.instrumentalness)\ndf_train.liveness=np.log(df_train.liveness)\ndf_train.speechiness=np.log(df_train.speechiness)\ndf_train.acousticness=np.log(df_train.acousticness)","faa44599":"df_test.instrumentalness=np.log(df_test.instrumentalness)\ndf_test.liveness=np.log(df_test.liveness)\ndf_test.speechiness=np.log(df_test.speechiness)\ndf_test.acousticness=np.log(df_test.acousticness)","a0c50d1f":"from sklearn.experimental import enable_iterative_imputer  # noqa\nfrom sklearn.impute import IterativeImputer","050af5da":"it_imputer  = IterativeImputer(max_iter=1000)\n\ntrain_imp = it_imputer.fit_transform(df_train[FEATURES])\ntest_imp = it_imputer.transform(df_test[FEATURES])\n\n# Create Train\/Test imputed dataframe\nimp_train_df = pd.DataFrame(train_imp, columns=FEATURES)\nimp_test_df = pd.DataFrame(test_imp, columns=FEATURES)","7b04fc0c":"imp_train_df.isna().sum(),imp_train_df.shape","c200b399":"imp_test_df.isna().sum(),imp_test_df.shape","9c589c2a":"idx = list(range(len(df_train)))\ndep_var = 'song_popularity'","d9e3da9a":"X = imp_train_df[FEATURES]\ny = df_train[dep_var]","8be688c8":"X.shape,y.shape","5f8a8db4":"#optimization configuration is from  https:\/\/www.kaggle.com\/dienhoa\/xgboost-song-popularity\n\ndef run(trial, data=X, target=y):\n    x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=10)\n    params = {\n        'max_depth': trial.suggest_int('max_depth', 6, 15),\n        'n_estimators': trial.suggest_int('n_estimators', 100, 20000, 500),\n        'learning_rate': trial.suggest_float('learning_rate', 0.005, 0.1),\n        'subsample': trial.suggest_discrete_uniform('subsample', 0.2, 0.9, 0.1),\n        'colsample_bytree': trial.suggest_discrete_uniform('colsample_bytree', 0.2, 0.9, 0.1),\n        'colsample_bylevel': trial.suggest_discrete_uniform('colsample_bylevel', 0.2, 0.9, 0.1),\n        'min_child_weight': trial.suggest_loguniform('min_child_weight', 1e-4, 1e4),\n        'reg_lambda': trial.suggest_loguniform('reg_lambda', 1e-4, 1e4),\n        'reg_alpha': trial.suggest_loguniform('reg_alpha', 1e-4, 1e4),\n        'gamma': trial.suggest_loguniform('gamma', 1e-4, 1e4),\n        'objective': 'binary:logistic',\n         }\n    \n    model = XGBClassifier(**params, booster= 'gbtree',\n                            eval_metric = 'auc',\n                            tree_method= 'gpu_hist',\n                            predictor=\"gpu_predictor\",\n                            random_state=9, \n                            gpu_id = 0,\n                            use_label_encoder=False)\n    model.fit(x_train,y_train,eval_set=[(x_test,y_test)], early_stopping_rounds=300, verbose=False)\n    \n    preds = model.predict_proba(x_test)[:,1]\n    fpr, tpr, _ = roc_curve(y_test, preds)\n    score = auc(fpr, tpr)\n    \n    return score","23dc5458":"study = optuna.create_study(direction='maximize')\nstudy.optimize(run,n_trials=100)","2fdfc9d1":"study.best_params","ba784683":"xgb_params=study.best_params","9ce12702":"preds_fold = []\nroc_fold = []\nfor i in range(100):\n    random.shuffle(idx)\n    train_idx = idx[:int(len(df_train)*0.8)]\n    valid_idx = idx[int(len(df_train)*0.8):]\n    x_train = X.iloc[train_idx]\n    y_train = y.iloc[train_idx]\n    x_valid = X.iloc[valid_idx]\n    y_valid = y.iloc[valid_idx]\n    \n    \n    \n    model = XGBClassifier(**xgb_params, booster= 'gbtree',\n                        eval_metric = 'auc',\n                        tree_method= 'gpu_hist',\n                        predictor=\"gpu_predictor\",\n                        random_state=i, \n                        use_label_encoder=False)\n                             \n    model.fit(x_train,y_train,early_stopping_rounds=100,eval_set=[(x_valid,y_valid)],verbose=False)\n    preds_valid = model.predict_proba(x_valid)\n    roc = roc_auc_score(y_valid,preds_valid[:,1])\n    roc_fold.append(roc)\n    print(F'fold {i}: ROC AUC {roc}')\n    preds_test = model.predict_proba(imp_test_df)[:,1]\n    preds_fold.append(preds_test)","a8e99088":"roc_fold = np.array(roc_fold)","a0b22e9c":"np.mean(roc_fold), np.std(roc_fold)","871e3521":"preds_fold = np.array(preds_fold)","aa025236":"final_preds = np.mean(preds_fold, axis=0)","f9575e74":"xgb_submission = submission.copy()","4a025342":"xgb_submission['song_popularity'] = final_preds","b9a36f16":"xgb_submission.head()","d94f3535":"xgb_submission.to_csv(\"submission.csv\",index=False)\nxgb_submission.head()","c9a864c4":"# Do Some transformation on the features","a457eaf2":"# Happy Learning !!!"}}