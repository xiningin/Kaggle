{"cell_type":{"132b7478":"code","0d9e9775":"code","eb94e983":"code","96e93c1c":"code","42e0e3ed":"code","8364844d":"code","08a79aaf":"code","c488fc26":"code","f1b56f89":"code","77785546":"code","b7e5a670":"code","cd87592c":"code","1028b19a":"code","6ba014a9":"code","57ca82dc":"code","9f19c43b":"code","99e7ee1b":"code","8dd38edd":"code","0e203307":"code","7e1a06d2":"code","54d0cd91":"code","07a5c211":"code","340b768e":"code","dfddb1eb":"code","af0ea796":"code","99bd2126":"code","4105d8f5":"code","d63088d2":"code","f390c14d":"code","a2bc228d":"code","26c0f0c8":"code","7f99acd9":"code","282fb597":"code","f4922f22":"code","5ac81c34":"code","cf5cd592":"code","0eec9b5a":"code","f66873c2":"code","6b078646":"code","04535e3e":"code","8438e824":"code","2c752d0a":"code","77a9dcd8":"code","ddcd83cc":"code","f036296f":"markdown","7afe0bd9":"markdown","f1fab8ec":"markdown","b1a83928":"markdown","ba57acdd":"markdown","bb684f27":"markdown","b8c26aa8":"markdown","0b11c5ce":"markdown","ddb42ce5":"markdown","9f9561a1":"markdown","f28abb6e":"markdown","67c4ec3a":"markdown","36dd2b6f":"markdown","93cb584c":"markdown","1f119768":"markdown","57afcf08":"markdown","fd6696f9":"markdown"},"source":{"132b7478":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","0d9e9775":"import numpy as np\nimport pandas as pd\nimport random\nseed = 44  \nrandom.seed(seed)\nnp.random.seed(seed)","eb94e983":"import pandas as pd\nimport numpy as np\n\nimport seaborn as sns\nsns.set_style(\"whitegrid\", {'axes.grid' : False})\nimport os\nfrom tqdm.notebook import tqdm\nimport gc\nfrom lightgbm import LGBMClassifier\nfrom sklearn.model_selection import train_test_split\nimport seaborn as sns; sns.set()\n%matplotlib inline\nfrom matplotlib import pyplot as plt\nfrom sklearn.model_selection import ParameterGrid","96e93c1c":"TARGET_COL = \"target\"\ndf = pd.read_csv(\"\/kaggle\/input\/tabular-data-march-2021\/train(march).csv\")\nprint(df.shape)\ntest = pd.read_csv(\"\/kaggle\/input\/tabular-data-march-2021\/test(march).csv\")\nprint(test.shape)\nsubmission = pd.read_csv(\"\/kaggle\/input\/tabular-data-march-2021\/sample_submission(march).csv\")\nprint(submission.shape)","42e0e3ed":"df['label']='train'\ntest['label']='test'\nframes = [df,test]\njoin_df = pd.concat(frames, keys=['x', 'y'])\nassert len(join_df) == len(df) + len(test)","8364844d":"join_df['cat0_count']= join_df.groupby([\"cat0\"])[\"id\"].transform(\"count\")\njoin_df['cat1_count']= join_df.groupby([\"cat1\"])[\"id\"].transform(\"count\")\njoin_df['cat2_count']= join_df.groupby([\"cat2\"])[\"id\"].transform(\"count\")\njoin_df['cat3_count']= join_df.groupby([\"cat3\"])[\"id\"].transform(\"count\")\njoin_df['cat4_count']= join_df.groupby([\"cat4\"])[\"id\"].transform(\"count\")\njoin_df['cat5_count']= join_df.groupby([\"cat5\"])[\"id\"].transform(\"count\")\njoin_df['cat6_count']= join_df.groupby([\"cat6\"])[\"id\"].transform(\"count\")\njoin_df['cat7_count']= join_df.groupby([\"cat7\"])[\"id\"].transform(\"count\")\njoin_df['cat8_count']= join_df.groupby([\"cat8\"])[\"id\"].transform(\"count\")\njoin_df['cat9_count']= join_df.groupby([\"cat9\"])[\"id\"].transform(\"count\")\njoin_df['cat10_count']= join_df.groupby([\"cat10\"])[\"id\"].transform(\"count\")\njoin_df['cat11_count']= join_df.groupby([\"cat11\"])[\"id\"].transform(\"count\")\njoin_df['cat12_count']= join_df.groupby([\"cat12\"])[\"id\"].transform(\"count\")\njoin_df['cat13_count']= join_df.groupby([\"cat13\"])[\"id\"].transform(\"count\")\njoin_df['cat14_count']= join_df.groupby([\"cat14\"])[\"id\"].transform(\"count\")\njoin_df['cat15_count']= join_df.groupby([\"cat15\"])[\"id\"].transform(\"count\")\njoin_df['cat16_count']= join_df.groupby([\"cat16\"])[\"id\"].transform(\"count\")\njoin_df['cat17_count']= join_df.groupby([\"cat17\"])[\"id\"].transform(\"count\")\njoin_df['cat18_count']= join_df.groupby([\"cat18\"])[\"id\"].transform(\"count\")","08a79aaf":"join_df['cont0_cont7'] = join_df['cont0']*join_df['cont7']\njoin_df['cont0_cont10'] = join_df['cont0']*join_df['cont10']\njoin_df['cont1_cont2'] = join_df['cont1']*join_df['cont2']\njoin_df['cont1_cont8'] = join_df['cont1']*join_df['cont8']\njoin_df['cont2_cont7'] = join_df['cont2']*join_df['cont7']\njoin_df['cont2_cont8'] = join_df['cont2']*join_df['cont8']\njoin_df['cont3_cont7'] = join_df['cont3']*join_df['cont7']\njoin_df['cont3_cont10'] = join_df['cont3']*join_df['cont10']\njoin_df['cont7_cont10'] = join_df['cont7']*join_df['cont10']","c488fc26":"join_df.shape","f1b56f89":"join_df.head()","77785546":"#Helps with reducing memory usage\n\ndef reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() \/ 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() \/ 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) \/ start_mem))\n    return df","b7e5a670":"join_df.drop(['id'],inplace=True,axis=1)","cd87592c":"## Print the categorical columns\nprint([c for c in df.columns if (1<df[c].nunique()) & (df[c].dtype != np.number)& (df[c].dtype != int) ])","1028b19a":"## Print the categorical columns\nnumerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n\nnewdf = join_df.select_dtypes(include=numerics)\nnumeric_cols = newdf.columns","6ba014a9":"numeric_cols ","57ca82dc":"# Need to do column by column due to memory constraints\ncategorical_cols =  ['cat0', 'cat1', 'cat2', 'cat3', 'cat4', 'cat5', 'cat6', 'cat7', 'cat8', 'cat9',\n                     'cat10', 'cat11', 'cat12', 'cat13', 'cat14', 'cat15', 'cat16', 'cat17', 'cat18']\nfor i, v in tqdm(enumerate(categorical_cols)):\n    join_df[v] = join_df[v].fillna(join_df[v].value_counts().index[0])\n","9f19c43b":"join_df[categorical_cols].isna().sum()","99e7ee1b":"from sklearn.preprocessing import OrdinalEncoder\n\n# In loop to minimize memory use\nfor i, v in tqdm(enumerate(categorical_cols)):\n    join_df[v] = OrdinalEncoder(dtype=\"int\").fit_transform(join_df[[v]])\n    \n\ngc.collect()","8dd38edd":"train = join_df[join_df['label']==\"train\"]\npredict = join_df[join_df['label']=='test']","0e203307":"train.reset_index(inplace=True)\ntrain.drop(['level_0','level_1','label'],inplace=True,axis =1 )\n\npredict.reset_index(inplace=True)\npredict.drop(['level_0','level_1','target','label'],inplace=True,axis=1)","7e1a06d2":"train.shape, predict.shape","54d0cd91":"train = reduce_mem_usage(train)\npredict = reduce_mem_usage(predict)","07a5c211":"train.head()","340b768e":"test.head()","dfddb1eb":"train['cat2_cat11'] = train['cat2']*train['cat11']\ntrain['cat15_cat16'] = train['cat15']*train['cat16']\n\npredict['cat2_cat11'] = predict['cat2']*predict['cat11']\npredict['cat15_cat16'] = predict['cat15']*predict['cat16']","af0ea796":"train.head()","99bd2126":"print(train.shape,predict.shape)","4105d8f5":"train[TARGET_COL].value_counts()\/len(train)","d63088d2":"features = list(train) \ntarget = 'target'                 \nfeatures.remove('target')","f390c14d":"!pip install bayesian-optimization","a2bc228d":"# Libraries\nimport numpy as np \nimport pandas as pd \n\n# Metrics and modeling\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split, StratifiedKFold,KFold\nfrom bayes_opt import BayesianOptimization\nfrom datetime import datetime\nfrom sklearn.metrics import precision_score, recall_score, confusion_matrix, accuracy_score, roc_auc_score, f1_score, roc_curve, auc,precision_recall_curve\nfrom sklearn import metrics\nfrom sklearn import preprocessing\n\n# Lgbm\nimport lightgbm as lgb\n\n# Suppr warning\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport itertools\nfrom scipy import interp\n\n# Plots\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom matplotlib import rcParams","26c0f0c8":"# Cut train and validation\nbayesian_tr_idx, bayesian_val_idx = train_test_split(train, test_size = 0.2, random_state = 42, stratify = train[target])\nbayesian_tr_idx = bayesian_tr_idx.index\nbayesian_val_idx = bayesian_val_idx.index","7f99acd9":"print(bayesian_tr_idx.shape, bayesian_val_idx.shape)","282fb597":"# Black Box LGBM \ndef LGB_bayesian(\n    #learning_rate,\n    num_leaves, \n    bagging_fraction,\n    feature_fraction,\n    min_child_weight, \n    min_data_in_leaf,\n    scale_pos_weight,\n    max_depth,\n    reg_alpha,\n    reg_lambda\n     ):\n    \n    # LightGBM expects next four parameters need to be integer. \n    num_leaves = int(num_leaves)\n    min_data_in_leaf = int(min_data_in_leaf)\n    scale_pos_weight = int(scale_pos_weight)\n    max_depth = int(max_depth)\n\n    assert type(num_leaves) == int\n    assert type(min_data_in_leaf) == int\n    assert type(scale_pos_weight) == int\n    assert type(max_depth) == int\n    \n\n    param = {\n              'num_leaves': num_leaves, \n              'min_data_in_leaf': min_data_in_leaf,\n              'min_child_weight': min_child_weight,\n              'scale_pos_weight': scale_pos_weight,\n              'bagging_fraction' : bagging_fraction,\n              'feature_fraction' : feature_fraction,\n              #'learning_rate' : learning_rate,\n              'max_depth': max_depth,\n              'reg_alpha': reg_alpha,\n              'reg_lambda': reg_lambda,\n              'objective': 'binary',\n              'save_binary': True,\n              'zero_as_missing': True,\n              'seed': 1337,\n              'feature_fraction_seed': 1337,\n              'bagging_seed': 1337,\n              'drop_seed': 1337,\n              'data_random_seed': 1337,\n              'boosting_type': 'gbdt',  #dart\n              'verbose': 1,\n              #'is_unbalance': True,  # Note: Either of scale_pos_weight or is_unbalance must be used\n              'boost_from_average': True,\n              'metric':'auc'}    \n    \n    oof = np.zeros(len(train))\n    trn_data= lgb.Dataset(train.iloc[bayesian_tr_idx][features].values, label=train.iloc[bayesian_tr_idx][target].values)\n    val_data= lgb.Dataset(train.iloc[bayesian_val_idx][features].values, label=train.iloc[bayesian_val_idx][target].values)\n\n    clf = lgb.train(param, trn_data,  num_boost_round=50, valid_sets = [trn_data, val_data], verbose_eval=0, early_stopping_rounds = 50)\n    \n    oof[bayesian_val_idx]  = clf.predict(train.iloc[bayesian_val_idx][features].values, num_iteration=clf.best_iteration)  \n    \n    score = roc_auc_score(train.iloc[bayesian_val_idx][target].values, oof[bayesian_val_idx])\n\n    return score\n","f4922f22":"bounds_LGB = {\n    #'learning_rate': (0.001,0.005),\n    'num_leaves'       : (80, 800),               \n    'bagging_fraction' : (0.6, 0.8),\n    'feature_fraction' : (0.2, 0.5),\n    'min_child_weight': (0.0013, 0.0023),   \n    'min_data_in_leaf': (100, 106),               \n    'scale_pos_weight': (3, 5),\n    'max_depth':(24,35),   # 24,30                       \n    'reg_alpha': (1.4, 1.5), \n    'reg_lambda': (1.2, 1.4)\n    \n}","5ac81c34":"LGB_BO = BayesianOptimization(LGB_bayesian, bounds_LGB, random_state=42)","cf5cd592":"init_points = 10\nn_iter = 10  ","0eec9b5a":"   print('-' * 130)\n\nwith warnings.catch_warnings():\n    warnings.filterwarnings('ignore')\n    LGB_BO.maximize(init_points=init_points, n_iter=n_iter, acq='ucb', xi=0.0, alpha=1e-6)","f66873c2":"LGB_BO.max['target']","6b078646":"LGB_BO.max['params']","04535e3e":"# Confusion matrix \ndef plot_confusion_matrix(cm, classes,\n                          normalize = False,\n                          title = 'Confusion matrix\"',\n                          cmap = plt.cm.Blues) :\n    plt.imshow(cm, interpolation = 'nearest', cmap = cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation = 0)\n    plt.yticks(tick_marks, classes)\n\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])) :\n        plt.text(j, i, cm[i, j],\n                 horizontalalignment = 'center',\n                 color = 'white' if cm[i, j] > thresh else 'black')\n \n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')","8438e824":"param_lgb = {\n        'min_data_in_leaf': int(LGB_BO.max['params']['min_data_in_leaf']), \n        'num_leaves': int(LGB_BO.max['params']['num_leaves']), \n        #'learning_rate': LGB_BO.max['params']['learning_rate'],\n        'min_child_weight': LGB_BO.max['params']['min_child_weight'],\n        'bagging_fraction': LGB_BO.max['params']['bagging_fraction'], \n        'feature_fraction': LGB_BO.max['params']['feature_fraction'],\n        'reg_lambda': LGB_BO.max['params']['reg_lambda'],\n        'reg_alpha': LGB_BO.max['params']['reg_alpha'],\n        'max_depth': int(LGB_BO.max['params']['max_depth']),\n        'scale_pos_weight': int(LGB_BO.max['params']['scale_pos_weight']),\n        'objective': 'binary',\n        'save_binary': True,\n        'seed': 1337,\n        'feature_fraction_seed': 1337,\n        'bagging_seed': 1337,\n        'drop_seed': 1337,\n        'data_random_seed': 1337,\n        'boosting_type': 'gbdt',   # dart\n        'verbose': 1,\n        #'is_unbalance': True, Note: Either 'is_unbalance' or 'scale_pos_weight' must be used.\n        'boost_from_average': True,\n        'metric':'auc'\n    }","2c752d0a":"plt.rcParams[\"axes.grid\"] = True\n\nnfold = 10                  \nskf = StratifiedKFold(n_splits=nfold, shuffle=True, random_state=42)\n\noof = np.zeros(len(train))\nmean_fpr = np.linspace(0,1,100)\ncms= []\ntprs = []\naucs = []\ny_real = []\ny_proba = []\nrecalls = []\nroc_aucs = []\nf1_scores = []\naccuracies = []\nprecisions = []\npredictions = np.zeros(len(predict))\nfeature_importance_df = pd.DataFrame()\n\ni = 1\nfor train_idx, valid_idx in skf.split(train, train.target.values):\n    print(\"\\nfold {}\".format(i))\n    trn_data = lgb.Dataset(train.iloc[train_idx][features].values,\n                                   label=train.iloc[train_idx][target].values\n                                   )\n    val_data = lgb.Dataset(train.iloc[valid_idx][features].values,\n                                   label=train.iloc[valid_idx][target].values\n                                   )   \n    \n    clf = lgb.train(param_lgb, trn_data, num_boost_round = 10000, valid_sets = [trn_data, val_data], verbose_eval = 100, early_stopping_rounds = 100)\n    oof[valid_idx] = clf.predict(train.iloc[valid_idx][features].values) \n    \n    predictions += clf.predict(predict[features]) \/ nfold\n    \n    # Scores \n    roc_aucs.append(roc_auc_score(train.iloc[valid_idx][target].values, oof[valid_idx]))\n    accuracies.append(accuracy_score(train.iloc[valid_idx][target].values, oof[valid_idx].round()))\n    recalls.append(recall_score(train.iloc[valid_idx][target].values, oof[valid_idx].round()))\n    precisions.append(precision_score(train.iloc[valid_idx][target].values ,oof[valid_idx].round()))\n    f1_scores.append(f1_score(train.iloc[valid_idx][target].values, oof[valid_idx].round()))\n    \n    # Roc curve by folds\n    f = plt.figure(1)\n    fpr, tpr, t = roc_curve(train.iloc[valid_idx][target].values, oof[valid_idx])\n    tprs.append(interp(mean_fpr, fpr, tpr))\n    roc_auc = auc(fpr, tpr)\n    aucs.append(roc_auc)\n    plt.plot(fpr, tpr, lw=2, alpha=0.3, label='ROC fold %d (AUC = %0.4f)' % (i,roc_auc))\n    \n    # Precion recall by folds\n    g = plt.figure(2)\n    precision, recall, _ = precision_recall_curve(train.iloc[valid_idx][target].values, oof[valid_idx])\n    y_real.append(train.iloc[valid_idx][target].values)\n    y_proba.append(oof[valid_idx])\n    plt.plot(recall, precision, lw=2, alpha=0.3, label='P|R fold %d' % (i))  \n    \n    i= i+1\n    \n    # Confusion matrix by folds\n    cms.append(confusion_matrix(train.iloc[valid_idx][target].values, oof[valid_idx].round()))\n    \n    # Features imp\n    fold_importance_df = pd.DataFrame()\n    fold_importance_df[\"Feature\"] = features\n    fold_importance_df[\"importance\"] = clf.feature_importance()\n    fold_importance_df[\"fold\"] = nfold + 1\n    feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n\n# Metrics\nprint(\n        '\\nCV roc score        : {0:.4f}, std: {1:.4f}.'.format(np.mean(roc_aucs), np.std(roc_aucs)),\n        '\\nCV accuracy score   : {0:.4f}, std: {1:.4f}.'.format(np.mean(accuracies), np.std(accuracies)),\n        '\\nCV recall score     : {0:.4f}, std: {1:.4f}.'.format(np.mean(recalls), np.std(recalls)),\n        '\\nCV precision score  : {0:.4f}, std: {1:.4f}.'.format(np.mean(precisions), np.std(precisions)),\n        '\\nCV f1 score         : {0:.4f}, std: {1:.4f}.'.format(np.mean(f1_scores), np.std(f1_scores))\n)\n\n#ROC \nf = plt.figure(1)\nplt.plot([0,1],[0,1],linestyle = '--',lw = 2,color = 'grey')\nmean_tpr = np.mean(tprs, axis=0)\nmean_auc = auc(mean_fpr, mean_tpr)\nplt.plot(mean_fpr, mean_tpr, color='blue',\n         label=r'Mean ROC (AUC = %0.4f)' % (np.mean(roc_aucs)),lw=2, alpha=1)\n\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('LGB ROC curve by folds')\nplt.legend(loc=\"lower right\")\n\n# PR plt\ng = plt.figure(2)\nplt.plot([0,1],[1,0],linestyle = '--',lw = 2,color = 'grey')\ny_real = np.concatenate(y_real)\ny_proba = np.concatenate(y_proba)\nprecision, recall, _ = precision_recall_curve(y_real, y_proba)\nplt.plot(recall, precision, color='blue',\n         label=r'Mean P|R')\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.title('P|R curve by folds')\nplt.legend(loc=\"lower left\")\n\n# Confusion maxtrix & metrics\nplt.rcParams[\"axes.grid\"] = False\ncm = np.average(cms, axis=0) \nclass_names = [0,1]\nplt.figure()\nplot_confusion_matrix(cm, \n                      classes=class_names, \n                      title= 'LGB Confusion matrix [averaged\/folds]')\nplt.show()","77a9dcd8":"plt.style.use('dark_background')\ncols = (feature_importance_df[[\"Feature\", \"importance\"]]\n    .groupby(\"Feature\")\n    .mean()\n    .sort_values(by=\"importance\", ascending=False)[:60].index)\nbest_features = feature_importance_df.loc[feature_importance_df.Feature.isin(cols)]\n\nplt.figure(figsize=(10,10))\nsns.barplot(x=\"importance\", y=\"Feature\", data=best_features.sort_values(by=\"importance\",ascending=False),\n        edgecolor=('white'), linewidth=2, palette=\"rocket\")\nplt.title('LGB Features importance (averaged\/folds)', fontsize=18)\nplt.tight_layout()","ddcd83cc":"test[TARGET_COL] = predictions\ntest[[\"id\",\"target\"]].to_csv(\"LGBM_bay-opt_10kfds_sub.csv\",index=False)                \ntest[[\"id\",\"target\"]].head()","f036296f":"### Seperating train and predict(test)","7afe0bd9":"# Submission:","f1fab8ec":"### Fill categorical columns","b1a83928":"# Feature Importance","ba57acdd":"### Joining train and test to ensure encodings done correctly","bb684f27":"# Bayesian Optimization","b8c26aa8":"# MODEL: \n \n# LGBMClassifier with Bayesian-Optimization","0b11c5ce":"### Load the Data","ddb42ce5":"### Import the Libraries","9f9561a1":"# Groupby count categorical variables using 'id'","f28abb6e":"# Categorical Features Interactions:","67c4ec3a":"# Continous Features Interactions:","36dd2b6f":"# LGB + best hyperparameters","93cb584c":"# Convert categorical\/binary variables to OrdinalEncoders","1f119768":"**Helps with reducing memory usage**","57afcf08":"# Confusion Matrix","fd6696f9":"# Libraries"}}