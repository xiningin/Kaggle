{"cell_type":{"52f9727a":"code","20273a10":"code","434aedf6":"code","356a6f66":"code","ddc50181":"code","d31f3117":"code","1559748f":"code","6a6df922":"code","8071a4de":"code","41a98865":"code","15f4ec80":"code","77254349":"code","61ff63e1":"code","18b89026":"code","333c0dcb":"code","ceda5fd3":"code","9b04114c":"code","7828566e":"code","9d16350a":"code","530a125d":"code","5268c26d":"code","ab7fe82e":"code","664cd737":"code","bd9bcc6b":"code","8a73c8a2":"code","6d6d74e4":"code","d250c8dd":"code","5867101b":"markdown","d80f156c":"markdown","5483b6d8":"markdown","d7da55ca":"markdown"},"source":{"52f9727a":"import numpy as np\nimport pandas as pd\nfrom patsy import dmatrices\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\nimport matplotlib.pyplot as plt","20273a10":"data = pd.read_csv(\"..\/input\/HR_comma_sep.csv\")","434aedf6":"data.dtypes","356a6f66":"data.head()","ddc50181":"pd.crosstab(data.salary, data.left).plot(kind='bar')\nplt.show()","d31f3117":"q = pd.crosstab(data.salary, data.left)\nprint(q)","1559748f":"print(q.sum(1)) # add each row","6a6df922":"q.div(q.sum(1), axis = 0)","8071a4de":"q.div(q.sum(1), axis = 0).plot(kind='bar', stacked = True)","41a98865":"data[data.left == 0].satisfaction_level.hist()\nplt.show()","15f4ec80":"data[data.left == 1].satisfaction_level.hist()\nplt.show()","77254349":"y, X = dmatrices('left~satisfaction_level+last_evaluation+number_project+average_montly_hours+time_spend_company+Work_accident+promotion_last_5years+C(sales)+C(salary)', data, return_type='dataframe')","61ff63e1":"X.head()","18b89026":"y.head()","333c0dcb":"X = np.asmatrix(X)\ny = np.ravel(y)","ceda5fd3":"X[:2]","9b04114c":"X.shape","7828566e":"for i in range(1, X.shape[1]): # after dmatrices\uff0cfirst column is \"intercept\" column,it's all \"1\"\n    xmin = X[:,i].min()\n    xmax = X[:,i].max()\n    X[:, i] = (X[:, i] - xmin) \/ (xmax - xmin)","9d16350a":"np.random.seed(1) # \u4fdd\u8bc1\u6bcf\u6b21\u8fd0\u884c\u7ed3\u679c\u4e00\u81f4\nalpha = 1  # learning rate\nbeta = np.random.randn(X.shape[1]) # \u968f\u673a\u521d\u59cb\u5316\u53c2\u6570beta, \u7136\u540e \u5728\u4e0b\u9762\u7684\u8bad\u7ec3\u4e2d \u4e0d\u65ad\u7684\u62df\u5408\u3002 randn\u8868\u793a\u6b63\u6001\u5206\u5e03\nbeta","530a125d":"# \u4e0b\u9762\u4e24\u884c \u5c31\u662f\u770b\u4e00\u4e0b prob.ravel\u7684\u6548\u679c\uff0cravel \u4e0d\u80fd\u7701\u7565","5268c26d":"prob = np.array(1. \/ (1 + np.exp(-np.matmul(X, beta))))\nprob","ab7fe82e":"prob.ravel()","664cd737":"\nfor T in range(200):\n    # 1. \u8868\u793a\u9700\u8981float \n    # numpy.ravel https:\/\/docs.scipy.org\/doc\/numpy-1.15.1\/reference\/generated\/numpy.rave.html#numpy.ravel\n    # Note: \u867d\u7136matmul\u8868\u793a\u77e9\u9635\u76f8\u4e58\uff0c \u4f46\u662f\u7531\u4e8e beta \u662f\u5217\u5411\u91cf\uff0c\u6240\u4ee5\u5c31\u662f\u70b9\u79ef\n    # \n    prob = np.array(1. \/ (1 + np.exp(-np.matmul(X, beta)))).ravel()  # \u6839\u636e\u5f53\u524dbeta\u9884\u6d4b\u79bb\u804c\u7684\u6982\u7387. matmul: matrix multiply \n    prob_y = list(zip(prob, y))\n    loss = -sum([np.log(p) if y == 1 else np.log(1 - p) for p, y in prob_y]) \/ len(y) # \u8ba1\u7b97\u635f\u5931\u51fd\u6570\u7684\u503c. cross entrophy loss function \n    error_rate = 0\n    for i in range(len(y)):\n        if ((prob[i] > 0.5 and y[i] == 0) or (prob[i] <= 0.5 and y[i] == 1)):\n            error_rate += 1;\n    error_rate \/= len(y)\n    \n    if T % 5 ==0 :\n        print('T=' + str(T) + ' loss=' + str(loss) + ' error=' + str(error_rate))\n    # \u8ba1\u7b97\u635f\u5931\u51fd\u6570\u5173\u4e8ebeta\u6bcf\u4e2a\u5206\u91cf\u7684\u5bfc\u6570\n    deriv = np.zeros(X.shape[1]) # \u6bcf\u4e2a\u5206\u91cf\u7684\u5bfc\u6570 \u4e5f\u5c31\u662f\u6bcf\u4e2afeature. deriv\u662f\u5411\u91cf\n    \n    # \u5bf9\u5e94logistic regression \u8bfe\u4ef62\uff1a Logistic Regression\u68af\u5ea6\u8ba1\u7b97 page \u91cc\u7684\u6c42\u504f\u5bfc\u7684\u516c\u5f0f\n    # \u5176\u5b9e\u8d28\uff0c\u6bcf\u4e2asample data \u90fd\u8981\u8d21\u732e deriv. prob[i] \u5176\u5b9e\u5c31\u662f\u4e0a\u9762\u6839\u636e\u516c\u5f0f 1 \/ (1 + e ^ (-betaX))\u8ba1\u7b97\u51fa\u6765\u7684\u6982\u7387\n    for i in range(len(y)):\n        deriv += np.asarray(X[i,:]).ravel() * (prob[i] - y[i])\n    deriv \/= len(y)\n    # \u6cbf\u5bfc\u6570\u76f8\u53cd\u65b9\u5411\u4fee\u6539beta\n    beta -= alpha * deriv","bd9bcc6b":"Xtrain,Xvali,ytrain,yvali=train_test_split(X, y, test_size=0.2, random_state=3)","8a73c8a2":"np.random.seed(1)\nalpha = 5 # learning rate\nbeta = np.random.randn(Xtrain.shape[1]) # \u968f\u673a\u521d\u59cb\u5316\u53c2\u6570beta\nerror_rates_train=[]\nerror_rates_vali=[]\nfor T in range(200):\n    prob = np.array(1. \/ (1 + np.exp(-np.matmul(Xtrain, beta)))).ravel()  # \u6839\u636e\u5f53\u524dbeta\u9884\u6d4b\u79bb\u804c\u7684\u6982\u7387\n    prob_y = list(zip(prob, ytrain))\n    loss = -sum([np.log(p) if y == 1 else np.log(1 - p) for p, y in prob_y]) \/ len(ytrain) # \u8ba1\u7b97\u635f\u5931\u51fd\u6570\u7684\u503c\n    error_rate = 0\n    for i in range(len(ytrain)):\n        if ((prob[i] > 0.5 and ytrain[i] == 0) or (prob[i] <= 0.5 and ytrain[i] == 1)):\n            error_rate += 1;\n    error_rate \/= len(ytrain)\n    error_rates_train.append(error_rate)\n    \n    prob_vali = np.array(1. \/ (1 + np.exp(-np.matmul(Xvali, beta)))).ravel()  # \u6839\u636e\u5f53\u524dbeta\u9884\u6d4b\u79bb\u804c\u7684\u6982\u7387\n    prob_y_vali = list(zip(prob_vali, yvali))\n    loss_vali = -sum([np.log(p) if y == 1 else np.log(1 - p) for p, y in prob_y_vali]) \/ len(yvali) # \u8ba1\u7b97\u635f\u5931\u51fd\u6570\u7684\u503c\n    error_rate_vali = 0\n    for i in range(len(yvali)):\n        if ((prob_vali[i] > 0.5 and yvali[i] == 0) or (prob_vali[i] <= 0.5 and yvali[i] == 1)):\n            error_rate_vali += 1\n    error_rate_vali \/= len(yvali)\n    error_rates_vali.append(error_rate_vali)\n    \n    if T % 5 ==0 :\n        print('T=' + str(T) + ' loss=' + str(loss) + ' error=' + str(error_rate)+ ' error_vali=' + str(error_rate_vali))\n    \n    # Note: \u53ea\u5728train set \u4e0a\u8ba1\u7b97\u5bfc\u6570\uff0c \u800c\u4e0d\u9700\u8981\u5728valid set \u4e0a\u9762\u8ba1\u7b97\u5bfc\u6570\uff0c\u800c\u8ba1\u7b97\u51fa\u6765\u7684 \u65b0\u7684beta value \u5c06\u4f1a\u88ab\u7528\u5728both train and valid\u4e0a\u9762 \uff08\u53cd\u5411\u4f20\u64ad\u4fee\u6539\u53c2\u6570\u65e8\u5728training\u4e0a\uff09\n    # \u8ba1\u7b97\u635f\u5931\u51fd\u6570\u5173\u4e8ebeta\u6bcf\u4e2a\u5206\u91cf\u7684\u5bfc\u6570\n    deriv = np.zeros(Xtrain.shape[1])\n    for i in range(len(ytrain)):\n        deriv += np.asarray(Xtrain[i,:]).ravel() * (prob[i] - ytrain[i])\n    deriv \/= len(ytrain)\n    # \u6cbf\u5bfc\u6570\u76f8\u53cd\u65b9\u5411\u4fee\u6539beta\n    beta -= alpha * deriv","6d6d74e4":"plt.plot(range(50,200), error_rates_train[50:], 'r^', range(50, 200), error_rates_vali[50:], 'bs')\nplt.show()","d250c8dd":"np.random.seed(1)\nalpha = 1  # learning rate\nbeta = np.random.randn(X.shape[1]) # \u968f\u673a\u521d\u59cb\u5316\u53c2\u6570beta\n\n#dF\/dbeta0\nprob = np.array(1. \/ (1 + np.exp(-np.matmul(X, beta)))).ravel()  # \u6839\u636e\u5f53\u524dbeta\u9884\u6d4b\u79bb\u804c\u7684\u6982\u7387\nprob_y = list(zip(prob, y))\nloss = -sum([np.log(p) if y == 1 else np.log(1. - p) for p, y in prob_y]) \/ len(y) # \u8ba1\u7b97\u635f\u5931\u51fd\u6570\u7684\u503c\nderiv = np.zeros(X.shape[1])\nfor i in range(len(y)):\n    deriv += np.asarray(X[i,:]).ravel() * (prob[i] - y[i])\nderiv \/= len(y)\nprint('We calculated ' + str(deriv[0]))\n \ndelta = 0.0001 # \u8be6\u5355\u4e0e \u7528\u5bfc\u6570\u5b9a\u4e49\uff0c \u6c42\u6781\u9650\u7684\u5206\u6bcd\u90e8\u5206\nbeta[0] += delta # \u552f\u4e00\u6539\u53d8\u7684\u4e1c\u897f\uff0c\u7136\u540e\u4e0b\u9762\u6839\u636e\u8fd9\u4e2a\u6539\u53d8\u7684\u7684beta \u518d\u6765\u8ba1\u7b97\u4e00\u6b21loss \nprob = np.array(1. \/ (1 + np.exp(-np.matmul(X, beta)))).ravel()  # \u6839\u636e\u5f53\u524dbeta\u9884\u6d4b\u79bb\u804c\u7684\u6982\u7387\nprob_y = list(zip(prob, y))\nloss2 = -sum([np.log(p) if y == 1 else np.log(1. - p) for p, y in prob_y]) \/ len(y) # \u8ba1\u7b97\u635f\u5931\u51fd\u6570\u7684\u503c\n# \u5bfc\u6570\u7684\u5b9a\u4e49\u6765\u8ba1\u7b97deriv \u7136\u540e\u6bd4\u8f83\nshouldbe = (loss2 - loss) \/ delta # (F(b0+delta,b1,...,bn) - F(b0,...bn)) \/ delta\nprint('According to definition of gradient, it is ' + str(shouldbe))","5867101b":"\u5c06\u6240\u6709\u5217\u7684\u503c\u5f52\u4e00\u5316\u5230[0,1]\u533a\u95f4, \u6709\u52a9\u4e8egradient descent - \u533a\u95f4\u7f29\u653e","d80f156c":"# \u68af\u5ea6\u68c0\u67e5","5483b6d8":"[](http:\/\/)# **\u8fc7\u62df\u5408\u5b9e\u9a8c**","d7da55ca":"# Logistic Regression \u6a21\u578b\u4f18\u5316\uff0c\u8fc7\u62df\u5408\u5b9e\u9a8c\uff0c\u68af\u5ea6\u68c0\u67e5"}}