{"cell_type":{"69eb5db7":"code","c47391f4":"code","b3062793":"code","66ff1158":"code","b2758d45":"code","98d4d970":"code","0c92a330":"code","1a8d6e93":"code","489942f2":"code","f77e82bb":"code","14795be7":"code","8cf50ed2":"code","6510012f":"markdown","8f0de6ec":"markdown","e9892253":"markdown","923a04e1":"markdown","53e460b1":"markdown","80ce735a":"markdown","3ab025da":"markdown","1ea7fb8f":"markdown","74b1cdac":"markdown","ae383eef":"markdown","ea384f3a":"markdown","f5459862":"markdown","7fb0b7f8":"markdown"},"source":{"69eb5db7":"import pandas as pd\n\ntrain_file_path = \"..\/input\/home-data-for-ml-course\/train.csv\"\ntrain = pd.read_csv(train_file_path)\n\ntest_file_path = \"..\/input\/home-data-for-ml-course\/test.csv\"\ntest = pd.read_csv(test_file_path)","c47391f4":"train.shape","b3062793":"train.describe()","66ff1158":"train.info()","b2758d45":"categorical_columns = set(train.select_dtypes(include=[object]))\nfor column in categorical_columns:\n    print(\"Unique values for column {}: {}\".format(column, train[column].nunique()))","98d4d970":"target_col = \"SalePrice\"\ntrain_x = train.drop(target_col, axis=1)\ntrain_y = train[target_col]\ntest_x = test.copy()","0c92a330":"for column in train_x.columns:\n    if column in categorical_columns:\n        train_x[column].fillna(\"Undefined\", inplace=True)\n        test_x[column].fillna(\"Undefined\", inplace=True)\n    else:\n        train_mean = train_x[column].mean()\n        train_x[column].fillna(train_mean, inplace=True)\n        test_x[column].fillna(train_mean, inplace=True)","1a8d6e93":"from sklearn.preprocessing import OneHotEncoder\n\nencoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\nencoder.fit(train_x[categorical_columns])\none_hot_columns = encoder.get_feature_names(list(categorical_columns))\ntrain_one_hot = pd.DataFrame(encoder.transform(train_x[categorical_columns]), \n                             columns=one_hot_columns)\ntest_one_hot = pd.DataFrame(encoder.transform(test_x[categorical_columns]), \n                             columns=one_hot_columns)\n\ntrain_x.drop(columns=categorical_columns, axis=1, inplace=True)\ntest_x.drop(columns=categorical_columns, axis=1, inplace=True)\n\ntrain_x[train_one_hot.columns] = train_one_hot\ntest_x[train_one_hot.columns] = test_one_hot","489942f2":"from sklearn.preprocessing import StandardScaler, MinMaxScaler\n\nscaler = StandardScaler()\nscaler.fit(train_x)\ntrain_x = pd.DataFrame(scaler.transform(train_x), columns=train_x.columns)\ntest_x = pd.DataFrame(scaler.transform(test_x), columns=test_x.columns)","f77e82bb":"from sklearn.ensemble import GradientBoostingRegressor\n\nmodel = GradientBoostingRegressor(random_state=1, n_estimators=200, min_samples_split=3)\nmodel.fit(train_x, train_y)\n\nmodel.predict(test_x)","14795be7":"test_preds = model.predict(test_x)\ntest_preds","8cf50ed2":"output = pd.DataFrame({'Id': test.Id,\n                       'SalePrice': test_preds})\noutput.to_csv('submission.csv', index=False)\noutput.head()","6510012f":"## Data exploration","8f0de6ec":"### Missing value imputation\n\nFor categorical variables, NA is preserved as an additional category (e.g. in Fence it means there is no fence, in PoolQC in means the house has no pool).\n\nFor numeric variables, NaN is replaced with the training set's mean value, both for training and test, to avoid leakage of test information.","e9892253":"### One-hot encoding for categorical variables\n\nOne binary feature per unique categorical value; just one with value 1 (based on the category), while the rest are 0.","923a04e1":"## Prediction and submission\n\nGenerate predictions and output it as expected for the submission.","53e460b1":"\n## Data loading\n\nJust loading training and test data.","80ce735a":"### Standarization\n\nThe standard-deviation scaler is used to preserve distribution information, while keeping all numeric values across features within a similar numeric range, so they have similar influence in the model. It doesn't affect one-hot values, which already are 0 or 1.","3ab025da":"## Model training\n\nA Gradient Boosting algorithm is used since this family of methods is known to obtain top results in many supervised learning problems with tabular data.","1ea7fb8f":"Most categorical variables have a small number of possibles categories, so using one-hot encoding on them (creating one dummy variables per unique value) should be reasonable. \n\nAn exception may be the Neighborhood variable, with 25 variables, but it is kept in the features since the location in a city is likely to influence the price.","74b1cdac":"# Housing Prices Prediction\n\nThis notebook contains a straight-forward (and by no means exhaustive) approach for solving this regression problem:\n* Minimal data exploration.\n* Data pre-processing.\n* Model training and prediction.","ae383eef":"There are 1460 training examples with 80 possible features plus the target variable, and some fields have missing values that will need handling.","ea384f3a":"## Pre-processing\n\nAs of now all features will be kept.","f5459862":"We can see that we have different data types: \n* Numeric fields (int or float) in different orders of mean magnitude, so standarization will be advisable.\n* Objects representing categories, which will need pre-processing to be compatible with sklearn's fit function for models.","7fb0b7f8":"## Further work\n\nThis was a simple approach to solve the problem, which could serve as base code to implement a more thorough one, which could include:\n\n* Feature-wise policy for missing value imputation, based on a more detailed EDA, which may lead to discarding some from the starting.\n* Automatic feature selection\/weighting.\n* Usage of a validation data split (or cross-validation) for the optimization of hyperparameters of the gradient boosting algorithm or alternative ML models, such as MLP or random forest."}}