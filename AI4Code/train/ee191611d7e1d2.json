{"cell_type":{"f4c0ba77":"code","66765b9d":"code","07ea2f0b":"code","a3d673dd":"code","cdab0115":"code","06d21da7":"code","3a35a589":"code","b87c1650":"code","39a0fa19":"code","5ec76847":"code","c94e98b1":"code","3376ffec":"code","bc444648":"code","b6b41c68":"code","0e4e6285":"code","08c8231a":"code","ef9709ee":"code","10ef34fe":"code","03ac3635":"code","a51d5f71":"code","0586e224":"code","6075d432":"code","c2bddc6f":"markdown","b617c393":"markdown","ba614143":"markdown","ac9dafc9":"markdown"},"source":{"f4c0ba77":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Set up matplotlib style \nplt.style.use('ggplot')\n\n# Libraries for wordcloud making and image importing\nfrom wordcloud import WordCloud, ImageColorGenerator\nfrom PIL import Image\n\n# And libraries for data transformation\nimport datetime\nfrom string import punctuation","66765b9d":"data = pd.read_csv('..\/input\/amazon-alexa-reviews\/amazon_alexa.tsv', delimiter='\\t')","07ea2f0b":"data.head()","a3d673dd":"for col in data.columns:\n    print('The distribution of column: ',col)\n    print(data[col].value_counts()[:10])\n    print('--------------------------------')","cdab0115":"sns.countplot(y=data.rating,orient='h',order=[5,4,3,2,1])","06d21da7":"data['length'] = data['verified_reviews'].apply(lambda x: len(x))","3a35a589":"data[np.isin(data['rating'].tolist(),[3,4])]['length'].hist(bins=25)\nplt.title('Length distribution of Rating 3 or 4')","b87c1650":"data[np.isin(data['rating'].tolist(),[1,2,5])]['length'].hist(bins=25)\nplt.title('Length distribution of Rating 3 or 4')","39a0fa19":"!pip install textatistic","5ec76847":"from textatistic import Textatistic","c94e98b1":"def Textatistic_edited(x):\n    try:\n        return Textatistic(x).scores['flesch_score']\n    except ZeroDivisionError:\n        return 0","3376ffec":"data['readability'] = data['verified_reviews'].apply(lambda x: Textatistic_edited(x))","bc444648":"data[np.isin(data['rating'].tolist(),[3,4])]['readability'].hist(bins=25)\nplt.title('Readability distribution of Rating 3 or 4')","b6b41c68":"data[np.isin(data['rating'].tolist(),[1,2,5])]['readability'].hist(bins=25)\nplt.title('Readability distribution of Rating 3 or 4')","0e4e6285":"plt.figure(figsize=(16, 4))\n\nplt.subplot(1,2,1)\ndata[data['rating']==3]['readability'].hist(bins=25)\nplt.title('Readability distribution of Rating 3')\n\nplt.subplot(1,2,2)\ndata[data['rating']==4]['readability'].hist(bins=25)\nplt.title('Readability distribution of Rating 3')","08c8231a":"from scipy.cluster.vq import kmeans, vq\nfrom numpy import random\n\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nimport re\nimport string\n\nfrom sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\nfrom spacy.lang.en.stop_words import STOP_WORDS","ef9709ee":"from sklearn.feature_extraction.text import TfidfVectorizer\n\nvectorizer = TfidfVectorizer()\nvectors = vectorizer.fit_transform(data['verified_reviews'])","10ef34fe":"stop_words_0 = set(stopwords.words('english')) \nstop_words = ['and', 'in', 'of', 'or', 'with','to','on','a','love','use','alexa','music']\n\ndef remove_noise(text):\n    tokens = word_tokenize(text)\n    clean_tokens = []\n    lemmatizer=WordNetLemmatizer()\n    for token in tokens:\n        token = re.sub('[!\"#$%&\\'()*+,-.\/:;<=>?@[\\\\]^_`{|}~]+', '', token)\n        token = lemmatizer.lemmatize(token.lower())\n        if len(token) > 1 and token not in stop_words_0 and token not in stop_words:\n            clean_tokens.append(token)\n            \n    return clean_tokens","03ac3635":"tfidf_vectorizer = TfidfVectorizer(max_features=100,tokenizer=remove_noise)\n\n# Use the .fit_transform() method on the list plots\ntfidf_matrix = tfidf_vectorizer.fit_transform(data['verified_reviews'])","a51d5f71":"random.seed = 123","0586e224":"distortions = []\nnum_clusters = range(2, 25)\n\n# Create a list of distortions from the kmeans function\nfor i in num_clusters:\n    cluster_centers, distortion = kmeans(tfidf_matrix.todense(),i)\n    distortions.append(distortion)\n\n# Create a data frame with two lists - num_clusters, distortions\nelbow_plot = pd.DataFrame({'num_clusters': num_clusters, 'distortions': distortions})\n\n# Creat a line plot of num_clusters and distortions\nsns.lineplot(x='num_clusters', y='distortions', data = elbow_plot)\nplt.xticks(num_clusters)\nplt.title('Clusters and Distortions')\nplt.show()\n","6075d432":"cluster_centers, distortion = kmeans(tfidf_matrix.todense(),6)\n\n# Generate terms from the tfidf_vectorizer object\nterms = tfidf_vectorizer.get_feature_names()\n\nfor i in range(6):\n    # Sort the terms and print top 10 terms\n    center_terms = dict(zip(terms, list(cluster_centers[i])))\n    sorted_terms = sorted(center_terms, key=center_terms.get, reverse=True)\n    print(sorted_terms[:5])","c2bddc6f":"Well, it seems that the complexity of rating 3 and rating 4 is pretty close. I'll pause this topic here and move on.","b617c393":"**I've read a research on Amazon** saying that the best-selling products' review is actually in shape of F, meaning though rating 5 occupies the most of the share, the amount of 4 or even 3 need to be obvious enough to show the diversity in opinions and customers. I think this is also becuase **the serious reviewers give rating 3 and 4 to good products very often**. Thus, to see enough rating in 3 and 4 somehow shows the products are valued by these reviewers.\n\nTo validate this, I'll use [Flesch\u2013Kincaid readability](https:\/\/en.wikipedia.org\/wiki\/Flesch%E2%80%93Kincaid_readability_tests) to see if the words used by rating 3 and 4 more complicated than the others.","ba614143":"### **Recently, I've been active on Google Maps to give reviews on locations.** \n**When I give reviews and check out others' reviews in the past few days, I've been thinking the bar of random reviewers and the bar of serious reviewers must be different. Then how can I know if serious reviewers think this place is good.**\n\nThus, today, I would like to do the sentiment analysis on this Alex reviews dataset to figure out this questions and try to use clustering to split the type of reviewers by the content of their reviews.","ac9dafc9":"**OMG! I didn't expect this obvious difference!**\n\nBut I assumption seems something. Let's see more."}}