{"cell_type":{"ef83eb2d":"code","41a92967":"code","94658ba5":"code","e5b05479":"code","b51b4f3a":"code","b87712c1":"code","e8dc644d":"code","acd68a24":"code","f1ff9fff":"code","449e5a2c":"code","dac75660":"code","23a5b223":"code","3a014b06":"code","b4510bb8":"code","2503618e":"code","4271679b":"code","17861a1a":"code","49956f3d":"code","3caf1852":"code","0cd92541":"code","3fe6d3a6":"code","58ce84e0":"code","d47a49e8":"code","7c56a5fd":"code","6b240bd6":"code","41056c5d":"code","a82e9304":"code","f98ea204":"code","57398768":"code","80e44994":"code","5a51ce79":"code","d604baa0":"code","3199b24a":"code","27239ffc":"code","42f48538":"code","2f08b9b4":"code","69c49762":"code","9ae2208c":"code","f6b7875a":"code","ec1e0808":"code","a3b4bc54":"code","cb634db5":"code","88abe402":"code","03db903c":"code","d62e602c":"code","d5257b94":"code","0849a519":"code","5314b6d9":"code","753063b6":"code","86fb6f23":"code","3cf5324e":"code","7ea0d630":"code","7bc38a79":"code","5fef62be":"code","5fd85d2d":"code","255298ea":"code","f38b9cba":"code","690e90b4":"code","8e5f11ea":"code","e4019a3a":"code","34cb8511":"code","ac9f7e78":"markdown","7409e1fa":"markdown","576bff8c":"markdown","f6e4bc29":"markdown","aa8bd1d0":"markdown","6d53f16a":"markdown","05f67e21":"markdown","7401128c":"markdown","8ff51306":"markdown","e748d192":"markdown","75d9814f":"markdown","16d7bd49":"markdown","e03f4f45":"markdown","4f92ca2a":"markdown","7e163c2f":"markdown","7cbaf089":"markdown","5682eb99":"markdown","2e77dde4":"markdown","24f432fd":"markdown","9ef0c63c":"markdown","df449562":"markdown","3be09211":"markdown","dbad5c88":"markdown","42d90231":"markdown","448245d3":"markdown","46237a20":"markdown","5f758552":"markdown","14e4fff6":"markdown","7ca9e6b2":"markdown","129707b3":"markdown","ff5df032":"markdown","853a837a":"markdown","73df5c14":"markdown","150d22f1":"markdown","959241b1":"markdown","3dffae11":"markdown","93a84929":"markdown","3473d438":"markdown","af2d6f55":"markdown","b4ce6248":"markdown","3078d8ed":"markdown","c2e881cd":"markdown","ba9530d7":"markdown","dcb215ab":"markdown","f32bdaa1":"markdown","7adc0925":"markdown","92587d48":"markdown","3c035c56":"markdown","b9553f91":"markdown","7534f4de":"markdown","72b49b46":"markdown","b5e4e295":"markdown","f2db3794":"markdown","d9b9a9b6":"markdown","339a1710":"markdown","957f575d":"markdown","af84f115":"markdown","6d47c6f7":"markdown","e4d59c96":"markdown","8f277fbb":"markdown","5b0b2e13":"markdown","feae3d7a":"markdown","404f19a5":"markdown","27f957d3":"markdown","fa00eb2a":"markdown","af2ae8c0":"markdown","2dc17f29":"markdown","2428f394":"markdown","eb1689bd":"markdown","0fec5953":"markdown","1dec6e53":"markdown","c4f17e42":"markdown","e9aa659e":"markdown","f4212ebd":"markdown","b7eaa187":"markdown","fc509bd7":"markdown","f051b4b0":"markdown"},"source":{"ef83eb2d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","41a92967":"import sys\n!cp ..\/input\/rapids\/rapids.0.11.0 \/opt\/conda\/envs\/rapids.tar.gz\n!cd \/opt\/conda\/envs\/ && tar -xzvf rapids.tar.gz\nsys.path = [\"\/opt\/conda\/envs\/rapids\/lib\/python3.6\/site-packages\"] + sys.path\nsys.path = [\"\/opt\/conda\/envs\/rapids\/lib\/python3.6\"] + sys.path\nsys.path = [\"\/opt\/conda\/envs\/rapids\/lib\"] + sys.path \n!cp \/opt\/conda\/envs\/rapids\/lib\/libxgboost.so \/opt\/conda\/lib\/","94658ba5":"import pandas as pd\ndf = pd.read_csv(\"..\/input\/hotel-booking-demand\/hotel_bookings.csv\")","e5b05479":"df.head().transpose()","b51b4f3a":"df.info()","b87712c1":"df.describe().transpose()","e8dc644d":"import seaborn as sns\nsns.countplot(x='reservation_status',data=df)","acd68a24":"plt.figure(figsize=(10,4))\nsns.distplot(df['stays_in_weekend_nights'],label=\"Weekend nights stays\",axlabel=False, kde=False)\nsns.distplot(df['stays_in_week_nights'],label=\"Week nights stays\",axlabel=False, kde=False)\nplt.legend()","f1ff9fff":"plt.figure(figsize=(10,4))\nsns.distplot(df['stays_in_weekend_nights'],label=\"Weekend nights stays\",axlabel=False, kde=False,bins=50)\nsns.distplot(df['stays_in_week_nights'],label=\"Week nights stays\",axlabel=False, kde=False,bins=150)\nplt.xlim(0,12)\nplt.legend()","449e5a2c":"sns.boxplot(x='reservation_status',y='stays_in_weekend_nights',data=df)","dac75660":"sns.boxplot(x='reservation_status',y='stays_in_week_nights',data=df)","23a5b223":"sns.boxplot(x='reservation_status',y='lead_time',data=df)","3a014b06":"plt.figure(figsize=(15,5))\nsns.countplot(x='arrival_date_month',data=df,hue='reservation_status')","b4510bb8":"df['total_guests']=df['adults']+df['children']+df['babies']","2503618e":"plt.figure(figsize=(15,5))\nsns.countplot(x='total_guests',data=df,hue='reservation_status')","4271679b":"fig, ax =plt.subplots(nrows=1, ncols=3, figsize=(20,10))\ng=sns.countplot(x='adults',data=df,hue='reservation_status',ax=ax[0])\ng.legend_.remove()\ng=sns.countplot(x='children',data=df,hue='reservation_status',ax=ax[1])\ng.legend_.remove()\ng=sns.countplot(x='babies',data=df,hue='reservation_status',ax=ax[2])\ng.legend(loc='right', bbox_to_anchor=(1.4, 0.5), ncol=1)\n\nfig.show()","17861a1a":"df=df.drop('reservation_status',axis=1)","49956f3d":"df['is_canceled']=df['is_canceled'].replace([0,1],[\"no\",\"yes\"])","3caf1852":"#Extracting categorical feature columns\ncols = df.columns\nnum_cols = df._get_numeric_data().columns\ncat_cols=list(set(cols) - set(num_cols))","0cd92541":"df_cat=df[cat_cols]","3fe6d3a6":"df_cat.columns","58ce84e0":"import itertools\nimport scipy\n\ndef cramers_corrected_stat(confusion_matrix):\n    \"\"\"\n    Calculates the corrected Cramer's V statistic\n    \n    Args:\n        confusion_matrix: The confusion matrix of the variables to calculate the statistic on\n    \n    Returns:\n        The corrected Cramer's V statistic\n    \"\"\"\n    \n    #chi2, _, _, _ = scipy.stats.chi2_contingency(confusion_matrix)\n    chi2 = scipy.stats.chi2_contingency(confusion_matrix)[0]\n    n = confusion_matrix.sum().sum()\n    phi2 = chi2\/n\n    r, k = confusion_matrix.shape\n    phi2corr = max(0, phi2 - ((k-1)*(r-1))\/(n-1))    \n    rcorr = r - ((r-1)**2)\/(n-1)\n    kcorr = k - ((k-1)**2)\/(n-1)\n    \n    return np.sqrt(phi2corr \/ min((kcorr-1), (rcorr-1)))\n\n#Getting the list of columns in the dataframe\ncols = list(df_cat.columns.values)\n\n#Creating an empty array to append to as we will go through iteratations to calculate correlations of every combination of variables.\nemp_arr = np.zeros((len(cols),len(cols)))\n\n#Iteraiting dataframe using itertools\n#itertools.combinations() : Given an array of size n, generate and print all possible combinations of r elements in array.\nfor col1, col2 in itertools.combinations(cols, 2):\n    A, B = df_cat[col1], df_cat[col2]\n    idx1, idx2 = cols.index(col1), cols.index(col2)\n    conf_mat = pd.crosstab(A,B) \n    #appending results to emp_array\n    emp_arr[idx1, idx2] = cramers_corrected_stat(conf_mat.values)\n    emp_arr[idx2, idx1] = emp_arr[idx1, idx2]\n\n#creating a correlation matrix\ncorr = pd.DataFrame(emp_arr, index=cols, columns=cols)\n\n# Mask to get lower triangle\nmask = np.zeros_like(corr, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n\ncmap = sns.cubehelix_palette(light=1, as_cmap=True)\n\n# Draw the heatmap with the mask \nfig = plt.figure(figsize=(12, 6))\n\nsns.heatmap(corr, mask=mask, cmap=cmap, square=True, linewidths=.5, cbar_kws={\"shrink\": .5}, annot=True)\n\nplt.show()","d47a49e8":"plt.figure(figsize=(15,5))\nsns.countplot(x='deposit_type',data=df,hue='is_canceled')","7c56a5fd":"df['is_canceled']=df['is_canceled'].replace([\"no\",\"yes\"],[0,1])","6b240bd6":"df.head()","41056c5d":"plt.figure(figsize=(20,10))\nsns.heatmap(df.corr(),annot=True,cmap='viridis')","a82e9304":"df.corr()['is_canceled'][:-1].sort_values().plot(kind='bar')","f98ea204":"# Separate features and predicted value\nX_cat = df_cat.drop(\"is_canceled\", axis=1)\ny_cat = df_cat[\"is_canceled\"].eq('yes').mul(1)","57398768":"X_cat.head()","80e44994":"X_cat['country'].fillna(\"No Country\", inplace = True)","5a51ce79":"from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n\ndef labelencode(df):\n    le = LabelEncoder()\n    return df.apply(le.fit_transform)\n\ndef onehotencode(df):\n    onehot = OneHotEncoder()\n    return onehot.fit_transform(df).toarray()\n\nX_2 = labelencode(X_cat)\nonehotlabels = onehotencode(X_2)","d604baa0":"X_2.head().transpose()","3199b24a":"from sklearn.decomposition import PCA\npca = PCA(n_components=3)\n\n# Perform PCA on the one-hot encoded labels\nX_pca = pca.fit_transform(onehotlabels)","27239ffc":"ex_variance=np.var(X_pca,axis=0)\nex_variance_ratio = ex_variance\/np.sum(ex_variance)\nprint(ex_variance_ratio)","42f48538":"pca = PCA(n_components=len(df_cat.columns))\n\n# Perform PCA on the one-hot encoded labels\nX_pca = pca.fit_transform(onehotlabels)\n\n# Rebuild it in its original dimension\nX_pca_reconst = pca.inverse_transform(X_pca)\n\n#plotting\n\nplt.figure(figsize=(12,12))\n\nplt.scatter(X_pca[y_cat==0, 0], X_pca[y_cat==0, 1], color='red', alpha=0.5,label='Not canceled')\nplt.scatter(X_pca[y_cat==1, 0], X_pca[y_cat==1, 1], color='blue', alpha=0.5,label='Canceled')\n\nplt.title(\"PCA\")\nplt.ylabel('Principal component Y')\nplt.xlabel('Principal component X')\nplt.legend()\nplt.show()","2f08b9b4":"pca = PCA(n_components=len(df_cat.columns))\nX_pca = pca.fit_transform(onehotlabels)\n\npercentage_var_explained = pca.explained_variance_ \/ np.sum(pca.explained_variance_);\ncum_var_explained = np.cumsum(percentage_var_explained)\n\n# Plot the PCA spectrum\nplt.figure(1, figsize=(6, 4))\nplt.clf()\nplt.plot(cum_var_explained, linewidth=2)\nplt.grid()\nplt.xlabel('n_components')\nplt.ylabel('Cumulative_explained_variance')\nplt.show()","69c49762":"import cudf, cuml\nfrom cuml.manifold import TSNE\nimport time\n\ntime_start = time.time()\n\ntsne = TSNE(n_components=2,verbose=1, learning_rate=300,perplexity = 50,early_exaggeration = 24,init = 'random',  random_state=2019)\n\nn = 20000  # for 20000 random indices\nindex = np.random.choice(onehotlabels.shape[0], n, replace=False)  \n\nX_tsne=tsne.fit_transform(onehotlabels[index])\ny_tsne=y_cat[index]\n\nprint('t-SNE done! Time elapsed: {} seconds'.format(time.time()-time_start))","9ae2208c":"import matplotlib.pyplot as plt\nplt.figure(figsize=(12,12))\n\nplt.scatter(X_tsne[y_tsne==0, 0], X_tsne[y_tsne==0, 1], color='red', alpha=0.5,label='Not canceled')\nplt.scatter(X_tsne[y_tsne==1, 0], X_tsne[y_tsne==1, 1], color='blue', alpha=0.5,label='Canceled')\nplt.title(\"TSNE\")\nplt.ylabel('Principal component Y')\nplt.xlabel('Principal component X')\nplt.legend()\nplt.show()","f6b7875a":"#getting the numerical feature columns one more time\ncols = df.columns\nnum_cols = df._get_numeric_data().columns\n\n#selecting numerical features\ndf_num=df[num_cols].drop('is_canceled',axis=1)\n\n#selecting target ('is_canceled' column)\ny_num=y_cat","ec1e0808":"df_num.columns","a3b4bc54":"df_num=df_num.fillna(df_num.median())","cb634db5":"from sklearn.preprocessing import StandardScaler\n# Standardizing the features\ndf_num_standard = StandardScaler().fit_transform(df_num.values)\n\n#replacing the X_num dataframe with the standardized dataframe\ndf_num[:] = df_num_standard","88abe402":"df_num.head().transpose()","03db903c":"pca = PCA(n_components=len(df_num.columns))\n\n# Perform PCA on the one-hot encoded labels\ndf_pca_num = pca.fit_transform(df_num)\n\n#plotting\n\nplt.figure(figsize=(12,12))\n\nplt.scatter(df_pca_num[y_num==0, 0], df_pca_num[y_num==0, 1], color='red', alpha=0.5,label='Not canceled')\nplt.scatter(df_pca_num[y_num==1, 0], df_pca_num[y_num==1, 1], color='blue', alpha=0.5,label='Canceled')\n\nplt.title(\"PCA\")\nplt.ylabel('Principal component Y')\nplt.xlabel('Principal component X')\nplt.legend()\nplt.show()","d62e602c":"import cudf, cuml\nfrom cuml.manifold import TSNE\nimport time\n\ntime_start = time.time()\n\ntsne = TSNE(n_components=2,verbose=1, learning_rate=300,perplexity = 50,early_exaggeration = 24,init = 'random',  random_state=2019)\n\nn = 20000  # for 2 random indices\nindex = np.random.choice(df_num_standard.shape[0], n, replace=False)  \n\nX_tsne=tsne.fit_transform(df_num_standard[index])\ny_tsne=y_num[index]\n\nprint('t-SNE done! Time elapsed: {} seconds'.format(time.time()-time_start))","d5257b94":"plt.figure(figsize=(12,12))\n\nplt.scatter(X_tsne[y_tsne==0, 0], X_tsne[y_tsne==0, 1], color='red', alpha=0.5,label='Not canceled')\nplt.scatter(X_tsne[y_tsne==1, 0], X_tsne[y_tsne==1, 1], color='blue', alpha=0.5,label='Canceled')\nplt.title(\"TSNE\")\nplt.ylabel('Principal component Y')\nplt.xlabel('Principal component X')\nplt.legend()\nplt.show()","0849a519":"#concatenating numerically converted categorical and numerical feature arrays\nX_arr=np.concatenate((onehotlabels, df_num_standard), axis=1)\ny_arr = df['is_canceled'].values","5314b6d9":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X_arr,y_arr,test_size=0.25,random_state=2019)","753063b6":"import tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Activation,Dropout","86fb6f23":"X_train.shape","3cf5324e":"model = Sequential()\n\n#adding dropout layers for improved learning\nmodel.add(Dense(units=30,activation='relu'))\nmodel.add(Dropout(0.5))\n\nmodel.add(Dense(units=20,activation='relu'))\nmodel.add(Dropout(0.5))\n\nmodel.add(Dense(units=10,activation='relu'))\nmodel.add(Dropout(0.5))\n\nmodel.add(Dense(units=1,activation='sigmoid'))\n\n# For a binary classification problem\nmodel.compile(loss='binary_crossentropy', optimizer='adam',metrics=['accuracy'])","7ea0d630":"#Putting early_stop in to prevent overfitting\nfrom tensorflow.keras.callbacks import EarlyStopping\nearly_stop = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=5)\n\nmodel.fit(x=X_train, \n          y=y_train, \n          epochs=100,\n          validation_data=(X_test, y_test), verbose=1,callbacks=[early_stop]\n          )","7bc38a79":"model_loss = pd.DataFrame(model.history.history)","5fef62be":"model_loss.plot()","5fd85d2d":"predictions = model.predict_classes(X_test)","255298ea":"from sklearn.metrics import classification_report,confusion_matrix","f38b9cba":"print(classification_report(y_test,predictions))","690e90b4":"print(confusion_matrix(y_test,predictions))","8e5f11ea":"predictions[0]","e4019a3a":"y_test[0]","34cb8511":"if predictions[12345] == y_test[12345]:\n    print(\"Prediction and test value match\")\nelse:\n    print(\"Prediction and test value do NOT match\")","ac9f7e78":"Quickly looking at X_cat..","7409e1fa":"To make sure we don\u2019t burden our machine in terms of memory and power\/time we will only use the 20,000 samples to run the algorithm.","576bff8c":"## 4.1 Model creation : layer setup + compiling","f6e4bc29":"## 3.1 Categorical features only","aa8bd1d0":"Now we can find the explained_variance_ratio, which shows the amount of information or variance each principal component holds after projecting the data to a lower dimensional subspace.","6d53f16a":"Let's look at this side by side for each category of guests (adults, children and babies)","05f67e21":"Looking at datasets.","7401128c":"# 2. Data Exploration","8ff51306":"Installing NVIDIA Rapids (CUDF & CUML) for rapid PCA and T-SNE analysis later.","e748d192":"I can see that the canceled reservation had longer lead time than the checked out reservation.","75d9814f":"Let's create layers to run our ANN! I am going to use \"relu\" as my activation function, \"binary_crossentropy\" as a loss function and \"adam\" as an optimizer during compiling.","16d7bd49":"This means that the principal component 1 holds 44.2% of the information while the principal component 2 holds only 32.9% of the information. Summing them up, we will have ~77% of information.","e03f4f45":"I will first create a new dataframe that contains only categorical attributes. Up to this point, I used \"reservation_status\" to figure out whether the room has been booked or not. But actually I can use \"is_cancelled\" attribute which simply shows 0 for not canceled and 1 for canceled.\n\nLet's drop \"reservation_status\" column from the original data frame, turn these 0 and 1 to categorical values (no and yes), and run Cramer's V analysis again.","4f92ca2a":"Let's change the bin sizes and axis limits to show the plot better.","7e163c2f":"We can see that some features have \"119390\" counts while other features such as \"agent\" or \"company\" have less counts, which indicates these features have null values.","7cbaf089":"Let's see how many hotels have been checked out based on a histogram.","5682eb99":"Again, the two are overlapping with each other a lot so I am not entirely sure if our classification will work accurately. Let's give it a shot with our ANN models anyway for my practice :)","2e77dde4":"What about another prediction? Like.. 12345th prediction? Let's compare the predicted value and actual test value.","24f432fd":"Alright. Looks like we really did not overfit based on the trend of the validation loss (didn't swing back up)","9ef0c63c":"The leadtime attribute had the strongest linear correlation with cancellation.","df449562":"Let's plot our model history (accuracy and loss) to see how our epochs worked.","3be09211":"## 2.1 Plotting for association","dbad5c88":"Despite of not so ideal results we got from PCA and TSNE analysis on both categorical and numerical features, let's try to predict cancellation using the ANN claffisication algorithm for practice :)","42d90231":"# 4. Running ANN classification algorithm","448245d3":"We can see that we need about 8 components to represent 90% of the dataset.","46237a20":"The ideal pattern would be distinct clusters where the clusters are not overlapping with each other a lot.","5f758552":"## 3.2 What about numerical features?","14e4fff6":"Creating predctions.","7ca9e6b2":"## 4.2 Training the model","129707b3":"For the missing numerical data, let's Panda's fillna feature. We will fill in with the median values.","ff5df032":"Let's look at the correlation values in a bar graph.","853a837a":"No linear correlation was found between the total # of guests and reservation status, but it seems like there was a very large number of guests that had booked a room, which makes sense because the \"df.describe\" table above shows that the max of adults was 55.","73df5c14":"Let's look at the numerical features the same way we looked at the categorical features for the PCA analysis.\n\nFirst, I will choose the numerical features only and go through standardization.","150d22f1":"Let's first concatanate our numerically converted categorical features (those that went through onehotencoder) and standardized numerical features toegether. Since we will need arrays for our ANN, we will concatanate the arrays, not the dataframe.","959241b1":"# 1. Introduction","3dffae11":"## 4.3 Make predictions & Model evaluation","93a84929":"From this confusion matrix on test set, we can see that both false negative and false positive cases are fairly small compared to the size of the whole test dataset.","3473d438":"There is Cramer's V model based on the chi squared satistic that can show how strongly nominal variables are associated with one another. This is very similar to correlation coefficient where 0 means no linear correlation and 1 means strong linear correlation.\n\nI refered two posts on this : (1) https:\/\/towardsdatascience.com\/the-search-for-categorical-correlation-a1cf7f1888c9 (2) https:\/\/sonj.me\/projects\/2018\/09\/05\/poisonous-mushroom-classification.html","af2d6f55":"We can see that it expected \"0\" which means no cancellation.","b4ce6248":"What about the relationship with \"lead time\"?","3078d8ed":"Splitting the data into train and test sets","c2e881cd":"Still the two clusters are overlapping with each other a lot.","ba9530d7":"## 2.3 Correlation for numerical features","dcb215ab":"We can check the relationship between the number of guests and reservation status. For this, let's create a column that sums up the number of adults, children and babies in our dataframe.","f32bdaa1":"# 3. Dimensionality reduction : PCA & TSNE","7adc0925":"Ah, we can see that with non-refundable deposits guests tend to cancel less than they would do with the no deposit cases.","92587d48":"What about TSNE analysis on the numerical features?","3c035c56":"Aside from the bruteforce way of checking each attribute looking at the plots as shown below, is there a mathematical way to select the parameters that are strongly correlated with each other? The issue here is that reservation_status are linked with some categorical attributes so we cannot simply use \"df.corr()\" to accurately get the correlation matrix.","b9553f91":"Not a super clear relationship..","7534f4de":"Let's run PCA analysis when the number of principal components is equal to the number of all attributes in the categorical feature dataset.","72b49b46":"Nice! Even though the clustering did not look ideal in our PCA and TSNE analysis, deep learning was able to figure it out!\nWe can use this trained model to predict whether a hotel room will be canceled or not given various input features :)","b5e4e295":"Next, let's take a look at \"deposit_type\" attribute as it showed the highest correlation with the target variable. The reservation_status_date effect was already looked at in the previous section where we saw an intersting trend that people cancel less during the winter time.","f2db3794":"Checking the train feature shape to see if the splitting was done as intended","d9b9a9b6":"Let's run PCA on this dataset.","339a1710":"Examining the test label shows that this classification is correct:","957f575d":"Then, how many principal components we would have needed to represent the whole dataset? Let's plot the cumuluative explained variances as a function of principal components, when the # of principal components is set equal to the number of total categorical features we have in the dataframe.","af84f115":"Will there be any month effect? Let's see.","6d47c6f7":"## 2.2 Cramer's V for categorical features","e4d59c96":"Let's first choose 3 principal components to try PCA.","8f277fbb":"We can more clearly see that both lead_time and total_of_special_requests had the strongest linear correlations with is_canceled target variable.","5b0b2e13":"Let's look at the distribution plot for two typees of stays (weekend and weekdays nights) just to see how they are distributed.","feae3d7a":"The overlapping between the two clusters is smaller in TSNE than in PCA but still not super distinct unfortunately. This could be attributed to the sample size, learning rate or perplexity we chose.","404f19a5":"Running standardization on this dataframe using StandardScaler and replacing the orignial dataframe with the standardized one.","27f957d3":"Let's take a look at X_2 to see how the labels have been converted to numerical digits instead.","fa00eb2a":"Running labelencoder and onehotencoder to convert to numerical features.","af2ae8c0":"We saw during our data exploration that the \"country\" feature had some missing values. Let's take care of this before moving along.","2dc17f29":"Let's take a look at the confusion matrix to evalute our classificatio work.","2428f394":"Our f1-score is not bad at all :)","eb1689bd":"This medium post (https:\/\/towardsdatascience.com\/dimensionality-reduction-toolbox-in-python-9a18995927cd) explains pretty well about TSNE (and other dimension reduction features too): T-SNE reduces dimensionality while trying to keep similar instances close and dissimilar instances apart. It is mostly used for visualization, in particular to visualize clusters of instances in high-dimensional space.","0fec5953":"As the additional means of verification, let's look at the very first prediction.","1dec6e53":"Let's quickly glance at numerical features's correlations. Before that we need to re-convert \"is_canceled\" attribute to numerical values.","c4f17e42":"Creating functions to run Cramer's V analysis for categorical features","e9aa659e":"Now I want to run PCA analysis on categorical features to see if we can really reduce our dataset dimensionality. For PCA to run effectively, let's convert categorical features to numerical ones using Scikit-learn. This requires running integer encoding first follwed by OneHotEncoding.","f4212ebd":"There is TSNE analysis that is known to improve distinction although it is quite resource-heavy. This is why I installed CUDF and CUML at the beginning of this kernel to utilize GPU.","b7eaa187":"Ah, interesting. It seems people canel less during the winter season (November - January). It could be due to the seasonal effect where (1) people generally travel less (so less cancelling) and (2) hotel prices go up so people do not want to pay additional fees when cancelling.","fc509bd7":"In this project, I am going to build a model to predict whether the hotel will be booked or not.","f051b4b0":"Let's create box plots for each stay category (weekdays and weekends) showing the relationship between # of stay nights and reservation status"}}