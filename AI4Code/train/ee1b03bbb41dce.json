{"cell_type":{"a9a1ed32":"code","bfdf7513":"code","9c262ae5":"code","785cb714":"code","5b794325":"code","b7f436c4":"code","76e34148":"code","f5fe62b3":"code","9390618f":"code","c6987d0b":"code","035bf1ed":"code","ac8bbf3c":"code","e4f2370e":"code","4e21569f":"code","1689db3b":"code","f278442f":"code","587c5974":"code","2c1ce1f5":"code","fc22cc25":"code","620e060d":"code","b37b4bff":"code","e249e462":"code","0e61fb9b":"code","3145d55f":"code","8cf9e106":"code","ff0ddc74":"code","2f093c69":"code","bf27787b":"code","6d447fda":"code","e1a9592e":"code","21e55795":"code","ca086585":"code","3c5674bf":"code","58509b2d":"code","5c86ef13":"code","f1b34832":"code","756e7ba3":"code","a6fe63e5":"code","5b72689a":"code","a5986675":"code","7ae8dec1":"code","215dd604":"code","3b599500":"code","9f4f5835":"code","06a223c1":"code","5f5e63ff":"code","7bb41ff2":"markdown","3f530727":"markdown","309d63f9":"markdown","9866f856":"markdown","a2f215ed":"markdown","69af1b23":"markdown","8be88d33":"markdown","2d19d9d0":"markdown","16f93e8c":"markdown","5e4e77ee":"markdown","37041ce0":"markdown","6fe05ef7":"markdown","e3245b79":"markdown","de7f8f76":"markdown","462edfe0":"markdown","7f6bf9a2":"markdown","353a91cd":"markdown","8627fb35":"markdown","5364950f":"markdown","0ee4c782":"markdown","64394b2c":"markdown","85d9b4dd":"markdown","eb6a40fc":"markdown","3c320833":"markdown","ce3ab1ab":"markdown","08ea8b9f":"markdown","e7e9a49e":"markdown","e5809ff2":"markdown","3816b3fc":"markdown","0113cfc5":"markdown","ebe03d59":"markdown","da6b15e9":"markdown","52752928":"markdown","25bec3ae":"markdown","80f406cb":"markdown","33331b3a":"markdown","404099b6":"markdown","0eaf6a41":"markdown","e6a02522":"markdown","e9f4a96a":"markdown"},"source":{"a9a1ed32":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nplt.rc('font',size = 14)\nimport seaborn as sns\nsns.set(style='white')\nsns.set(style = 'whitegrid',color_codes = True)\nimport warnings\nwarnings.simplefilter(action='ignore')","bfdf7513":"train_df = pd.read_csv('..\/input\/titanic\/train.csv')\ntest_df = pd.read_csv('..\/input\/titanic\/test.csv')\ntrain_df.head()","9c262ae5":"print(\"The total number of passengers are : \",train_df.shape[0])","785cb714":"test_df.head()","5b794325":"print(\"The total number of passengers in the test is : \",test_df.shape[0])","b7f436c4":"train_df.isnull().sum()","76e34148":"print(\"The percentage of missing values in the Age Column is\",(train_df['Age'].isnull().sum()\/train_df.shape[0])*100, end=' ')\nprint(\"%\")","f5fe62b3":"ax=train_df['Age'].hist(bins=15,density=True,alpha=0.6,color='teal')\ntrain_df['Age'].plot(kind='density',color='teal')\n#sns.kdeplot(train_df['Age'],color='teal')\nax.set(xlabel='Age')\nplt.xlim(-10,85)\nplt.show()","9390618f":"print(\"The mean of the 'Age' variable is\",train_df['Age'].mean(skipna= True))\nprint(\"The median of the 'Age' variable is\",train_df['Age'].median(skipna= True))","c6987d0b":"print(\"The percentage missing values in the cabin variable is\",(train_df['Cabin'].isnull().sum()\/train_df['Cabin'].shape[0]*100))","035bf1ed":"print(\"The percentage of missing values in the Embarked column is\",train_df['Embarked'].isnull().sum()\/train_df.shape[0]*100)","ac8bbf3c":"print(\"The onboarded passengers are (S = SouthHampton C = Chernbourg Q =Queenstown)\")\nprint(train_df['Embarked'].value_counts())\nsns.countplot(y=train_df['Embarked'],alpha=0.7,palette='Set2')\nplt.show()","e4f2370e":"print(\"The most onboarded Embarked person is\",train_df['Embarked'].value_counts().idxmax())","4e21569f":"train_data = train_df.copy()\ntrain_data['Age'].fillna(train_data['Age'].median(skipna=True),inplace=True)\ntrain_data['Embarked'].fillna(train_data['Embarked'].value_counts().idxmax(),inplace=True)\ntrain_data.drop('Cabin',axis=1,inplace=True)","1689db3b":"train_data.isnull().sum()","f278442f":"train_data.head()","587c5974":"plt.figure(figsize=(15,8))\nax=train_df['Age'].hist(bins=15,color='teal',density=True,stacked=True,alpha=0.6)\ntrain_df['Age'].plot(kind='density',color='teal')\nax=train_data['Age'].hist(bins=15,color='yellow',density=True,stacked=True,alpha=0.6)\ntrain_data['Age'].plot(kind='density',color='yellow')\nax.legend(['Raw Data','Adjusted Data'])\nax.set(xlabel='Age')\nplt.xlim(-10,85)\nplt.show()","2c1ce1f5":"train_data['TravelAlone']=np.where((train_data['SibSp']+train_data['Parch'])>0,0,1)\ntrain_data.drop(['SibSp','Parch'],axis=1,inplace=True)","fc22cc25":"train_data.head()","620e060d":"training_data = pd.get_dummies(train_data,columns=['Pclass','Embarked','Sex'])\ntraining_data.head()","b37b4bff":"training_data.drop(columns=['PassengerId','Name','Ticket'],axis=1,inplace=True)","e249e462":"training_data.head()","0e61fb9b":"test_df.isnull().sum()","3145d55f":"test_df=pd.read_csv('..\/input\/titanic\/test.csv')","8cf9e106":"test_data = test_df.copy()\ntest_data[\"Age\"].fillna(train_df[\"Age\"].median(skipna=True), inplace=True)\ntest_data[\"Fare\"].fillna(train_df[\"Fare\"].median(skipna=True), inplace=True)\ntest_data.drop('Cabin', axis=1, inplace=True)\n\ntest_data['TravelAlone']=np.where((test_data[\"SibSp\"]+test_data[\"Parch\"])>0, 0, 1)\n\ntest_data.drop('SibSp', axis=1, inplace=True)\ntest_data.drop('Parch', axis=1, inplace=True)\n\ntesting = pd.get_dummies(test_data, columns=[\"Pclass\",\"Embarked\",\"Sex\"])\ntesting.drop('Sex_female', axis=1, inplace=True)\ntesting.drop('PassengerId', axis=1, inplace=True)\ntesting.drop('Name', axis=1, inplace=True)\ntesting.drop('Ticket', axis=1, inplace=True)\n\nfinal_test = testing\nfinal_test.head()","ff0ddc74":"plt.figure(figsize=(15,8))\nax = sns.kdeplot(training_data['Age'][training_data['Survived']==1],color='darkturquoise',shade=True)\nax= sns.kdeplot(training_data['Age'][training_data['Survived']==0],color='yellow',shade=True)\nplt.legend(['Survived','Not Survived'])\nplt.xlabel('Age')\nplt.title(\"Distribution of Age in hue of Survived and Non-survived persons\")\nplt.show()","2f093c69":"plt.figure(figsize=(24,8))\nsurvival_byage = training_data[['Age','Survived']].groupby('Age',as_index=False).mean()\ng=sns.barplot(survival_byage['Age'],survival_byage['Survived'],color='darkturquoise')\nplt.show()","bf27787b":"training_data['Minor']=np.where(training_data['Age']<=16,1,0)\nfinal_test['Minor']=np.where(final_test['Age']<=16,1,0)","6d447fda":"plt.figure(figsize=(20,8))\nax=sns.kdeplot(training_data['Fare'][training_data['Survived']==1],color='darkturquoise',shade=True)\nsns.kdeplot(training_data['Fare'][training_data['Survived']==0],color='yellow',shade=True)\nplt.legend(['Survived','Not survived'])\nplt.xlabel('Fare')\nplt.title(\"Distribution of Fare in hue of Survival rate\")\nplt.xlim(-20,200)\nplt.show()","e1a9592e":"training_data.head()","21e55795":"sns.barplot(train_df['Pclass'],train_df['Survived'],color='darkturquoise')\nplt.show()","ca086585":"sns.barplot('Embarked', 'Survived', data=train_df, color=\"teal\")\nplt.show()","3c5674bf":"sns.barplot(training_data[\"TravelAlone\"],training_data['Survived'],palette='rainbow')\nplt.show()","58509b2d":"train_df.columns","5c86ef13":"sns.barplot(train_df['Sex'],train_df['Survived'],color='yellow')\nplt.show()","f1b34832":"training_data.columns","756e7ba3":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.feature_selection import RFE\ncol = ['Age','Fare','TravelAlone','Pclass_1','Pclass_2','Pclass_3','Embarked_Q','Embarked_S','Embarked_C','Sex_male','Minor']\nX= training_data[col]\ny=training_data['Survived']\nmodel =LogisticRegression()\nrfe=RFE(model,8)\nrfe=rfe.fit(X,y)\nprint(\"Selected features are %s \"%list(X.columns[rfe.support_]))","a6fe63e5":"from sklearn.feature_selection import RFECV\nrfecv=RFECV(estimator= LogisticRegression(),step = 1,cv=10,scoring='accuracy')\nrfecv.fit(X,y)\n\nprint(\"Optimal number of features %d\"% rfecv.n_features_)\nprint(\"Selected features %s \" % list(X.columns[rfecv.support_]))\n\nplt.figure(figsize=(10,6))\nplt.xlabel(\"Number of Features \")\nplt.ylabel(\"The accuracy score \")\nplt.plot(range(1,len(rfecv.grid_scores_ )+1),rfecv.grid_scores_)\nplt.show()","5b72689a":"Selected_feature = ['Age', 'TravelAlone', 'Pclass_1', 'Pclass_2', 'Pclass_3', 'Embarked_Q', 'Embarked_S', 'Embarked_C', 'Sex_male', 'Minor'] \nX=training_data[Selected_feature]\nplt.subplots(figsize=(10,7))\nsns.heatmap(X.corr(),annot=True,cmap=\"RdYlGn\")\nplt.show()","a5986675":"from sklearn.model_selection import train_test_split,cross_val_score\nfrom sklearn.metrics import accuracy_score,classification_report,auc,roc_curve,log_loss,precision_recall_curve,confusion_matrix,precision_score,recall_score\nX = training_data[Selected_feature]\ny = training_data['Survived']\nx_train,x_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=2)\n\nlogreg=LogisticRegression()\nlogreg.fit(x_train,y_train)\ny_pred = logreg.predict(x_test)\ny_pred_proba = logreg.predict_proba(x_test)[:,1]\nprint(\"Train\/test split : \")\n[fpr,tpr,thr] = roc_curve(y_test,y_pred_proba)\nprint(\"Logistic Regression accuracy is %2.3f\" % accuracy_score(y_test,y_pred))\nprint(\"Logistic Regression log is %2.3f\" % log_loss(y_test,y_pred))\nprint(\"Logistic Regression auc is %2.3f\" % auc(fpr,tpr))\n\nidx = np.min(np.where(tpr>0.95))\nplt.figure()\nplt.plot(fpr,tpr,color='coral',label='ROC Curve area = (%0.3f)' % auc(fpr,tpr))\nplt.plot([0,1],[0,1],'k--')\nplt.plot([0,fpr[idx]],[tpr[idx],tpr[idx]],'k--',color='blue')\nplt.plot([fpr[idx],fpr[idx]],[0,tpr[idx]],'k--',color='blue')\nplt.xlim([0.0,1.0])\nplt.ylim([0.0,1.0])\nplt.xlabel(\"False Positive Rate (1 - specifictiy)\",fontsize=14)\nplt.ylabel(\"True Positive Rate (Recall)\",fontsize=14)\nplt.title('Receiver Operating Characteristic (ROC) curve')\nplt.show()","7ae8dec1":"logreg=LogisticRegression()\nscores_accuracy = cross_val_score(logreg,X,y,cv=10,scoring='accuracy')\nscores_log_loss = cross_val_score(logreg,X,y,cv=10,scoring='neg_log_loss')\nscores_auc=cross_val_score(logreg,X,y,cv=10,scoring='roc_auc')\nprint(\"K-FOld cross validation results\")\nprint(\"Logistic Regression : accuracy  %2.3f\" %scores_accuracy.mean())\nprint(\"Logistic Regression : log loss %2.3f\" %scores_log_loss.mean())\nprint(\"Logistic Regression : AUC is %2.3f\"%scores_auc.mean())","215dd604":"from sklearn.model_selection import cross_validate\nscoring = {'accuracy':'accuracy','log_loss':'neg_log_loss','auc':'roc_auc'}\nmodelCV = LogisticRegression()\nresults = cross_validate(modelCV,X,y,cv=10,scoring =list(scoring.values()),return_train_score=False)\nprint(\"K-FOld cross validation results :\")\nfor sc in range(len(scoring)):\n    print(modelCV.__class__.__name__+\" average %s: %.3f (+\/-%.3f)\" % (list(scoring.keys())[sc], -results['test_%s' % list(scoring.values())[sc]].mean()\n                               if list(scoring.values())[sc]=='neg_log_loss' \n                               else results['test_%s' % list(scoring.values())[sc]].mean(), \n                               results['test_%s' % list(scoring.values())[sc]].std()))","3b599500":"from sklearn.model_selection import GridSearchCV\nX=training_data[Selected_feature]\nparam_grid={'C':np.arange(1e-05,3,0.1)}\nscoring = {'Accuracy':'accuracy','AUC':'roc_auc','Log_loss':'neg_log_loss'}\ngs = GridSearchCV(LogisticRegression(),return_train_score=True,param_grid=param_grid,scoring=scoring,cv=10,refit='Accuracy')\ngs.fit(X,y)\nresults=gs.cv_results_\n\nprint(\"best_estimator : \"+str(gs.best_estimator_))\nprint(\"best_estimator : \"+str(gs.best_params_))\nprint(\"best score : \",gs.best_score_)\nprint(\"-\"*20)\n\nplt.figure(figsize=(10,10))\nplt.title(\"GridSearchCV Evaluation using multiple scores\",fontsize=16)\nplt.xlabel(\"Inverse of Regularization strength : C\")\nplt.ylabel(\"Score\")\nplt.grid()\n\nax=plt.axes()\nax.set_xlim(0,param_grid['C'].max())\nax.set_ylim(0.35,1.0)\n\nX_axis = np.array(results['param_C'].data,dtype=float)\nfor scorer,color in zip((list(scoring.keys())),['g','k','b']):\n    for sample,style in (('train','--'),('test','-')):\n        sample_score_mean = -results['mean_%s_%s'%(sample,scorer)] if scoring[scorer]=='neg_log_loss' else results['mean_%s_%s'%(sample,scorer)]\n        sample_score_std = results['std_%s_%s'%(sample,scorer)]\n        ax.fill_between(X_axis,sample_score_mean-sample_score_std,sample_score_mean +sample_score_std,alpha=0.1 if sample=='test' else 0,color=color)\n        ax.plot(X_axis, sample_score_mean, style, color=color,alpha = 1 if sample=='test' else 0.7,label='%s (%s)' % (scorer,sample))\n        best_index = np.nonzero(results['rank_test_%s' % scorer]==1)[0][0]\n        best_score = -results['mean_test_%s' %scorer][best_index] if scoring[scorer]=='neg_log_loss' else results['mean_test_%s'% scorer][best_index]\n        ax.plot([X_axis[best_index],]*2,[0,best_score],linestyle='-.',color=color,marker='x',markeredgewidth=3,ms=8)\n        ax.annotate(\"%0.2f\"% best_score,(X_axis[best_index],best_score+0.005))\n        \nplt.legend(loc='best')\nplt.grid('off')\nplt.show()","9f4f5835":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.pipeline import Pipeline\n\nC = np.arange(1e-05,5.5,0.1)\nscoring = {'Accuracy': 'accuracy', 'AUC': 'roc_auc', 'Log_loss': 'neg_log_loss'}\nlog_reg = LogisticRegression()\nstd_scale = StandardScaler(with_mean=False, with_std=False)\n#std_scale = StandardScaler()\n\n#Defining the CV method: Using the Repeated Stratified K Fold\n###############################################################################\n\nn_folds=5\nn_repeats=5\n\nrskfold = RepeatedStratifiedKFold(n_splits=n_folds, n_repeats=n_repeats, random_state=2)\n\n\nlog_clf_pipe = Pipeline(steps=[('scale',std_scale), ('clf',log_reg)])\n\nlog_clf = GridSearchCV(estimator=log_clf_pipe, cv=rskfold,\n              scoring=scoring, return_train_score=True,\n              param_grid=dict(clf__C=C), refit='Accuracy')\n\nlog_clf.fit(X, y)\nresults = log_clf.cv_results_\n\nprint('='*20)\nprint(\"best params: \" + str(log_clf.best_estimator_))\nprint(\"best params: \" + str(log_clf.best_params_))\nprint('best score:', log_clf.best_score_)\nprint('='*20)","06a223c1":"# Now, the use of RepeatedStratifiedKFolding comes into picture\nn_folds = 5\nn_repeats = 5\n\nrskfold = RepeatedStratifiedKFold(n_splits=n_folds,n_repeats=n_repeats,random_state=2)\nlog_clf_pipe=Pipeline(steps=[('scale',std_scale),(\"clf\",log_reg)])\n\nlog_clf=GridSearchCV(estimator=log_clf_pipe,cv=rskfold,scoring=scoring,return_train_score=True,param_grid=dict(clf__C=C),refit='Accuracy')\nlog_clf.fit(X,y)\nresults=log_clf.cv_results_\nprint('='*20)\nprint(\"best params: \" + str(log_clf.best_estimator_))\nprint(\"best params: \" + str(log_clf.best_params_))\nprint('best score:', log_clf.best_score_)\nprint('='*20)","5f5e63ff":"# Submission\nfinal_test['Survived'] = log_clf.predict(final_test[Selected_feature])\nfinal_test['PassengerId'] = test_df['PassengerId']\n\nsubmission = final_test[['PassengerId','Survived']]\n\nsubmission.to_csv(\"submission.csv\", index=False)\n\nsubmission.tail()","7bb41ff2":"<a id='t5.'><\/a>\n# 5. Pipeline\n\n### So, now let us sum the whole process, in a single program\n1. Scaling the Data\n2. Apply the logistic Regression model on it\n3. Use the 5 fold technique","3f530727":"<a id='t4.2.3.'><\/a>\n# 4.2.3 Model evalution using cross_validation function\n","309d63f9":"<a id='t3.4.'><\/a>\n# 3.4 Embarked Variable","9866f856":"<a id='t4.3.'><\/a>\n# 4.3 GridSearchCV\n\n### I want the program itself to find out the best parameters in the basis of the Criteria of Accuracy that I will define :), made my life easy","a2f215ed":"<a id=\"t2.2.\"><\/a>\n# 2.2 Cabin missing values","69af1b23":"<a id='t3.2.'><\/a>\n# 3.2 Exploration of Fare","8be88d33":"## So, now we came to know that :\n#### If you travel alone, you would have a high survival rate, but who woud travel alone on such an amazing journey without a partner.\n#### Also,there is a very obvious difference in the survival rate of the gender, clearly being female greatly increased your chances of survival.\n\n\n### * This completes our Exploratory Data Analysis Part, if something more can be done, please let me know in the comment section :), thanks\n\n### * Now, now -> The ML part comes into picture","2d19d9d0":"<a id='t4.2.2.'><\/a>\n## 4.2.2 K-Fold cross validation using cross_val_score() function","16f93e8c":"### Did you see that,in the survived category (blue shaded region), the minor category provides the information that, it had a high rate of survival, and it is backed by a reason that for parents, the wife and children are the first priority and then, they themselves follows","5e4e77ee":"<a id='t4.1.'><\/a>\n## 4.1 Feature Selection\n### Atleast, this is an important part, I should know that which columns to consider, and which to eliminate (because I only need to keep those columns, which provide the maximum information in helping me predict the survival rate :)  )\n\n### So, the different techniques for elimination of the Features are :","37041ce0":"# **Exploratory Data Analysis alongwith Logistic Regression**\n#### This kernel is basically just an elaborated (in my style) version of [Titanic: logistic regression with python](https:\/\/www.kaggle.com\/mnassrib\/titanic-logistic-regression-with-python)'s analysis, and this was done, so as to again start the practice of Data Science. However, I am open to all the positive and the negative feedbacks, that you would like to provide, in order to improvise the quality of the notebook.\n\n## The content of the notebook are:\n0. [Framing the Question](#t0.)\n1. [Importing the Libraries and reading the Data](#t1.)\n2. [Data Quality and Missing Values](#t2.)\n    * [Age-Missing Values](#t2.1.)\n    * [Cabin Missing Values](#t2.2.)\n    * [Embarked Variabla](#t2.3.)\n    * [Final Adjustement](#t2.4.)\n        * [Additional Variables](#t2.4.1.)\n    * [Handling the categorical features](#t2.5.)\n3. [Exploratory Data Analysis](#t3.)\n    * [Age Exploration](#t3.1.)\n    * [Exploration of Fare](#t3.2.)\n    * [Exploration of Passenger Class](#t3.3.)\n    * [Embarked Variable](#t3.4.)\n    * [The variable - Travel Alone](#t3.5.)\n4. [Logistic Regression and Results](#t4.)\n    * [Feature Selection](#t4.1.)\n        * [Recursive Feature Elimination](#t4.1.1.)\n        * [Recursive Feature Elimination, but this time with the help of Cross Validation (RFECV)](#t4.1.2.)\n    * [Model Evaluation](#t4.2.)\n        * [Model Evaluation using 'train_test_split()'](#t4.2.1.)\n        * [K Fold Cross Validation using 'cross_val_score()'](#t4.2.2.)\n        * [Model Evaluation using Cross Validation function](#t4.2.3.)\n    * [Grid Search CV](#t4.3.)\n5. [Pipeline](#t5.)","6fe05ef7":"<a id='t4.'><\/a>\n# 4. Logistic Regression and Results","e3245b79":"### Okay, so it means that the passengers who paid less are more likely to survive due to the fact that, the class would be less secured in comparison to the higher classes","de7f8f76":"So, the missing values are approximately 20 percentage of the data, I am thinking of replacing with the central measures, let us see (according to the distribution)\n\nLet us see what the Age Variable has to offer","462edfe0":"<a id=\"t2.1.\"><\/a>\n## 2.1 Age - Missing Value","7f6bf9a2":"<a id=\"t2.4.\"><\/a>\n# 2.4 Final Adjustment\n1. For the Age column -> 28 (for null values i.e the median)\n2. For the Embarked   -> S\n3. For the Cabin, simply drop it","353a91cd":"<a id=\"t1.\"><\/a>\n# 1. Importing the libraries and reading the Data\n\nOf course, we need some tools to deal with the problem, so let us import some standard libraries, and as we move further we will see which libraries needs to be imported, but for now let us import standard libraries :)","8627fb35":"### Means only 0.22 Percentage of the values are missing, and for cateogrical data, as for now I only know one thing i.e replacing it with the mode value, **i.e df.value_counts().idxmax()**\n\n### Okay, we will replace it with the mode :)","5364950f":"### Remember : The goal is to predict the column \"Survived\" using different Machine Learning Algorithms (our case -> Logistic Regression)\n<a id=\"t2.\"><\/a>\n# 2. Data Quality and Missing Values","0ee4c782":"<a id= 't3.1.'><\/a>\n## 3.1 Age Exploration","64394b2c":"So, we will ignore the Cabin Variable -> Now to Embarked Variable\n<a id=\"t2.3.\"><\/a>\n# 2.3 Embarked Variable","85d9b4dd":"# Applying the same to the test data","eb6a40fc":"Okay, so we have right (somewhat) skewed data, hence we will be using median to impute the missing values","3c320833":"<a id=\"t2.6.\"><\/a>\n# 2.6 Handling the categorical Feature","ce3ab1ab":"<a id=\"t0.\"><\/a>\n# 0. Framing the Question\n### **The sinking of the Titanic is one of the most infamous shipwrecks in history**\n\n* On April 15, 1912, during her maiden voyage, the widely considered \u201cunsinkable\u201d RMS Titanic sank after colliding with an iceberg. Unfortunately, there weren\u2019t enough lifeboats for everyone onboard, resulting in the death of 1502 out of 2224 passengers and crew.\n\n* While there was some element of luck involved in surviving, it seems some groups of people were more likely to survive than others.\n\n* In this challenge, we ask you to build a predictive model that answers the question: \u201cwhat sorts of people were more likely to survive?\u201d using passenger data (ie name, age, gender, socio-economic class, etc).\n\n<img src = 'https:\/\/thumbor.forbes.com\/thumbor\/fit-in\/1200x0\/filters%3Aformat%28jpg%29\/https%3A%2F%2Fspecials-images.forbesimg.com%2Fdam%2Fimageserve%2F877330410%2F0x0.jpg%3Ffit%3Dscale' width=\"800\" height=\"500\">\n\n\n# Inference:\nSo, from the above passage mentioned in the **Overview** of the problem statement, we can figure out that we need to use the features column such as (name, age, gender, socio-economic class, etc) and then we need to predict the survival class (i.e dead or not dead)\n\nExactly, this is a classification problem, tooo...... smarttttt\n\nOhhhh, so what all things we came to know (let us make a point, so that it looks like a ordered table)\n\n1. We infered that the problem is a classification problem\n2. We also came to know about the back story of the problem, i.e that problem is based on the sinking of the world famous ship **The Titanic** which was considered to be unsinkable, and from the sinkage, some class of people (could be divided into the class such as age group, gender, travelling with spouse, travelling alone, the boarding place,etc, we will see it next)\n3. So. coming up next is the Loading data, and the EDA Part (ofcourse the Pipeline and the Logistic Regression part will come, but first of **EDA**, so let us goooooooooooooooo onto exploring the Titanic Dataset","08ea8b9f":"<a id='t3.6.'><\/a>\n# 3.6 Gender Variable","e7e9a49e":"<a id='t3.5.'><\/a>\n# 3.5 The variable : Travel Alone","e5809ff2":"### Okay, so when selecting 10 features, the accuracy is at its peak, so, definitely, I am a greedy person (and currently in my college the topic of Greedy Algorithm is about to complete, so as a greedy approach, I will consider 10 features in my bucket list )","3816b3fc":"### So, the adjusted data has the shift in the distribution towards the median, and we know the obvious reason for that :)\n\n\n<a id=\"t2.5.\"><\/a>\n# 2.5 Additional Variable\n-> For the Siblings and Parental-Children, we will make a new variable which will indicate that whether the person was travelling along or with some other person :)","0113cfc5":"<a id='t3.'><\/a>\n# 3 Exploratory Data Analysis","ebe03d59":"## So, hope you enjoyed the notebook, definitely there are more kernels, but this was my first step, so your feedback will be appreciated, and I will definitely implement it in my upcoming kernels, thanks a lot for coming to the end :)\n\n## More detailed version can be found out from [Titanic: logistic regression with python](https:\/\/www.kaggle.com\/mnassrib\/titanic-logistic-regression-with-python) \n\n## Sorry for the typos that have had encrypted while typing\n\n# End","da6b15e9":"<a id='t4.1.1.'><\/a>\n### 4.1.1 Recursive Feature Elimination\n\n* Given an external estimator that assigns weights to features (e.g., the coefficients of a linear model), the goal of recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through any specific attribute (such as coef_, feature_importances_) or callable. \n* Then, the least important features are pruned from current set of features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached.\n\nRFECV performs RFE in a cross-validation loop to find the optimal number of features.","52752928":"<a id='t4.1.2'><\/a>\n## 4.1.2 Feature Ranking with the same recursive approach but now this time, with the help of Cross Validation (RFECV)\n\nRFECV performs recursive cross-validation to find the optimal number of features on the Logistic Regression model, hence, the features will be selected based on the basis of CV on LogisticRegression","25bec3ae":"# A note from myside:\n\nThe notebook will be almost similar to the one that is mentioned above, however I wanted to practice the problems again, after a long time, and hence, thought that it would be best to start from here, all the  credits goes to the author of the above mentioned notebook, thanks and \n\nEnjoy the notebook    :)","80f406cb":"### So, the feature shrinking part has been done, now what ??\n\n### Since, the feature handling has been done, now the part of train_test_split, K Fold Cross Validation, and GridSearchCV comes into picture\n\n<a id='t4.2.'><\/a>\n# 4.2 Model Evaluation\n","33331b3a":"<a id='t3.3.'><\/a>\n# 3.3 Exploration of Passenger Class","404099b6":"<a id='t4.2.1.'><\/a>\n## 4.2.1 Model Evaluation using train_test_split","0eaf6a41":"Ok,since the survival rate of childrens are more, we will also consider the minor category","e6a02522":"So, the **Age** && **Cabin** are the columns containing the Null Values, so immindiate thought that came to my mind was Impute Age (according to the distibution, i.e on the basis of skewness) and remove the **Cabin** column, but before that let us see what it has to offer :)","e9f4a96a":"## If , I were to travel in Titanic, I would have definitely boarded from **Cherbourg** :), okay jokes apart, so from the above plots, what we can infer is :\n#### 1. In the age exploration column, we came to know that being a minority is also a good point which sould be taken into consideration while predicting the survival rate\n#### 2. In the exploration of fare, we came to know that high fare -> good survival rate, and we know the reason, (I do not know why these dogs are roaming in my Kaggle Notebook, it is just distracting me :( )\n#### 3. In the passenger class, the most survival rate were of the PClass 1, which indeed means rich people and finally in the Embarked column, we came to know that Chernbourg was the boarding place from where the maximum people survived \n"}}