{"cell_type":{"2cd0ff95":"code","fb73392d":"code","882062a3":"code","6f972782":"code","196418d7":"code","81bda0c3":"code","66463fad":"code","a8f5384b":"code","7a8b4745":"code","13a0216f":"code","e6c0277b":"code","d283ac62":"code","57319b85":"code","00834a7f":"code","fe5c5cfc":"code","87f46363":"code","baf95f3a":"code","2dc4ca9a":"code","10f460f2":"code","1101c104":"markdown","70736653":"markdown","e05899df":"markdown","2d128931":"markdown","4d88b41e":"markdown","42e5a483":"markdown","ceaa974e":"markdown","6ae0fdab":"markdown","e38f6577":"markdown","10da9668":"markdown","6c627107":"markdown","8122c754":"markdown","81e99151":"markdown","38ee4352":"markdown"},"source":{"2cd0ff95":"import numpy as np \nimport pandas as pd\n\nfrom PIL import Image\n\nimport torch\nimport torch.nn as nn\nimport torch.utils.data as D\nfrom torch.optim.lr_scheduler import ExponentialLR\n\nfrom torchvision import models, transforms as T\n\nfrom ignite.engine import Events, create_supervised_evaluator, create_supervised_trainer\nfrom ignite.metrics import Loss, Accuracy\nfrom ignite.contrib.handlers.tqdm_logger import ProgressBar\nfrom ignite.handlers import  EarlyStopping, ModelCheckpoint\n\nfrom tqdm import tqdm_notebook\n\nfrom sklearn.model_selection import train_test_split\n\nimport warnings\nwarnings.filterwarnings('ignore')","fb73392d":"path_data = '..\/input'\ndevice = 'cuda'\nbatch_size = 32\ntorch.manual_seed(0)","882062a3":"class ImagesDS(D.Dataset):\n    def __init__(self, df, img_dir, mode='train', site=1, channels=[1,2,3,4,5,6]):\n        self.records = df.to_records(index=False)\n        self.channels = channels\n        self.site = site\n        self.mode = mode\n        self.img_dir = img_dir\n        self.len = df.shape[0]\n        \n    @staticmethod\n    def _load_img_as_tensor(file_name):\n        with Image.open(file_name) as img:\n            return T.ToTensor()(img)\n\n    def _get_img_path(self, index, channel):\n        experiment, well, plate = self.records[index].experiment, self.records[index].well, self.records[index].plate\n        return '\/'.join([self.img_dir,self.mode,experiment,f'Plate{plate}',f'{well}_s{self.site}_w{channel}.png'])\n        \n    def __getitem__(self, index):\n        paths = [self._get_img_path(index, ch) for ch in self.channels]\n        img = torch.cat([self._load_img_as_tensor(img_path) for img_path in paths])\n        if self.mode == 'train':\n            return img, int(self.records[index].sirna)\n        else:\n            return img, self.records[index].id_code\n\n    def __len__(self):\n        return self.len","6f972782":"df = pd.read_csv(path_data+'\/train.csv')\ndf_train, df_val = train_test_split(df, test_size = 0.025, random_state=42)\ndf_test = pd.read_csv(path_data+'\/test.csv')","196418d7":"ds = ImagesDS(df_train, path_data, mode='train')\nds_val = ImagesDS(df_val, path_data, mode='train')\nds_test = ImagesDS(df_test, path_data, mode='test')","81bda0c3":"classes = 1108\nmodel = models.resnet18(pretrained=True)\nnum_ftrs = model.fc.in_features\nmodel.fc = torch.nn.Linear(num_ftrs, classes)\n\n# let's make our model work with 6 channels\ntrained_kernel = model.conv1.weight\nnew_conv = nn.Conv2d(6, 64, kernel_size=7, stride=2, padding=3, bias=False)\nwith torch.no_grad():\n    new_conv.weight[:,:] = torch.stack([torch.mean(trained_kernel, 1)]*6, dim=1)\nmodel.conv1 = new_conv","66463fad":"loader = D.DataLoader(ds, batch_size=batch_size, shuffle=True, num_workers=4)\nval_loader = D.DataLoader(ds_val, batch_size=batch_size, shuffle=True, num_workers=4)\ntloader = D.DataLoader(ds_test, batch_size=batch_size, shuffle=False, num_workers=4)","a8f5384b":"criterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.0003)","7a8b4745":"metrics = {\n    'loss': Loss(criterion),\n    'accuracy': Accuracy(),\n}\n\n\ntrainer = create_supervised_trainer(model, optimizer, criterion, device=device)\nval_evaluator = create_supervised_evaluator(model, metrics=metrics, device=device)","13a0216f":"@trainer.on(Events.EPOCH_COMPLETED)\ndef compute_and_display_val_metrics(engine):\n    epoch = engine.state.epoch\n    metrics = val_evaluator.run(val_loader).metrics\n    print(\"Validation Results - Epoch: {}  Average Loss: {:.4f} | Accuracy: {:.4f} \"\n          .format(engine.state.epoch, \n                      metrics['loss'], \n                      metrics['accuracy']))","e6c0277b":"lr_scheduler = ExponentialLR(optimizer, gamma=0.95)\n\n@trainer.on(Events.EPOCH_COMPLETED)\ndef update_lr_scheduler(engine):\n    lr_scheduler.step()\n    lr = float(optimizer.param_groups[0]['lr'])\n    print(\"Learning rate: {}\".format(lr))","d283ac62":"handler = EarlyStopping(patience=6, score_function=lambda engine: engine.state.metrics['accuracy'], trainer=trainer)\nval_evaluator.add_event_handler(Events.COMPLETED, handler)","57319b85":"@trainer.on(Events.EPOCH_STARTED)\ndef turn_on_layers(engine):\n    epoch = engine.state.epoch\n    if epoch == 1:\n        for name, child in model.named_children():\n            if name == 'fc':\n                pbar.log_message(name + ' is unfrozen')\n                for param in child.parameters():\n                    param.requires_grad = True\n            else:\n                pbar.log_message(name + ' is frozen')\n                for param in child.parameters():\n                    param.requires_grad = False\n    if epoch == 3:\n        pbar.log_message(\"Turn on all the layers\")\n        for name, child in model.named_children():\n            for param in child.parameters():\n                param.requires_grad = True","00834a7f":"checkpoints = ModelCheckpoint('models', 'Model', save_interval=3, n_saved=3, create_dir=True)\ntrainer.add_event_handler(Events.EPOCH_COMPLETED, checkpoints, {'ResNet18': model})","fe5c5cfc":"pbar = ProgressBar(bar_format='')\npbar.attach(trainer, output_transform=lambda x: {'loss': x})","87f46363":"import os\nif not 'KAGGLE_WORKING_DIR' in os.environ:  #  If we are not on kaggle server\n    from ignite.contrib.handlers.tensorboard_logger import *\n    tb_logger = TensorboardLogger(\"board\/ResNet18\")\n    tb_logger.attach(trainer, log_handler=OutputHandler(tag=\"training\", output_transform=lambda loss: {'loss': loss}),\n                     event_name=Events.ITERATION_COMPLETED)\n\n    tb_logger.attach(val_evaluator, log_handler=OutputHandler(tag=\"validation\", metric_names=[\"accuracy\", \"loss\"],\n                     another_engine=trainer),event_name=Events.EPOCH_COMPLETED)\n\n    tb_logger.attach(trainer, log_handler=OptimizerParamsHandler(optimizer), event_name=Events.ITERATION_STARTED)\n\n    tb_logger.attach(trainer, log_handler=GradsHistHandler(model), event_name=Events.EPOCH_COMPLETED)\n    tb_logger.close()","baf95f3a":"trainer.run(loader, max_epochs=50)","2dc4ca9a":"model.eval()\nwith torch.no_grad():\n    preds = np.empty(0)\n    for x, _ in tqdm_notebook(tloader): \n        x = x.to(device)\n        output = model(x)\n        idx = output.max(dim=-1)[1].cpu().numpy()\n        preds = np.append(preds, idx, axis=0)","10f460f2":"submission = pd.read_csv(path_data + '\/test.csv')\nsubmission['sirna'] = preds.astype(int)\nsubmission.to_csv('submission.csv', index=False, columns=['id_code','sirna'])","1101c104":"## Train model","70736653":"## Define dataset and model","e05899df":"In this problem I think it's better not to use the same learning rate during all the training, so let's make it decrease after each epoch","2d128931":"Finally, we are ready to start training","4d88b41e":"## Prediction for test","42e5a483":"We definitely need early stopping, I don't want to tune the number of epochs by hand","ceaa974e":"### Ignite magic starts here","6ae0fdab":"Let's warmup our last linear layer by freezing all the other layers for a couple of epochs","e38f6577":"Also, let's save our model's weights after some epochs to be able to use them later","10da9668":"## Load libraries","6c627107":"Let's log some interesting information about our learning process to Tensorboard\n(Does not work in kaggle kernels, you need to have TensorboadX installed)","8122c754":"And we obviously need beautiful tqdm-based progress bars for our training process","81e99151":"Let's define which metrics we will use and create magic objects to train and validate our model","38ee4352":"Attach to our trainer a function to run a validator at the end of each epoch"}}