{"cell_type":{"5ce25d18":"code","5282c92b":"code","18f0cd6d":"code","4cde8291":"code","05f8ccc8":"code","be5e2a9b":"code","964a4bf2":"code","13574221":"code","73aea7ca":"code","82a5a9dc":"code","88bf22e7":"code","587a98cd":"code","0fb9d20f":"code","97db7e91":"code","921f740d":"code","40593296":"code","16001766":"code","c119f58c":"code","56a2f319":"code","8a6c9c76":"code","0096b278":"markdown","0fbc3551":"markdown","84d5aeb0":"markdown","3dd4e6f7":"markdown","1b6ca7bf":"markdown","54b90bb0":"markdown","1972cb8e":"markdown","868d1d75":"markdown","903ef228":"markdown","221a5d15":"markdown","54fd1edd":"markdown","48d6068b":"markdown","8b679ff7":"markdown","c8289918":"markdown","228727e9":"markdown","d6172c79":"markdown","bf67b5a1":"markdown","33cea3f0":"markdown","483eebb6":"markdown","e3858eff":"markdown","1c507fe7":"markdown"},"source":{"5ce25d18":"# Importing required libraries\nimport itertools\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nfrom scipy import stats\nfrom pandas import DataFrame\nplt.style.use('fivethirtyeight')\nimport pandas as pd\nimport statsmodels.api as sm\nimport seaborn as sns\nfrom sklearn.metrics import mean_squared_error\nfrom math import sqrt\nfrom statsmodels.tsa.api import ExponentialSmoothing\n\n\n","5282c92b":"# There are 2 products named X and Y. We will forecast only the sales of product X.\ndf = pd.read_excel(\"..\/input\/alcoholic-beverage-sales-in-unidentified-regions\/XY_2013_2017.xlsx\",parse_dates=True, index_col=1)\nTotal = df.iloc[:,1:2]\nTotal.drop([\"Y\u0131l-Ay\"],inplace=True)\nTotal= Total.astype(float)\nTotal.columns=[\"Liters\"]\nTotal.index=pd.to_datetime(Total.index)\nTrain=Total.iloc[:48,:]\nTest=Total.iloc[48:,]\nTrain.tail()","18f0cd6d":"\nfrom pandas import read_csv\nfrom matplotlib import pyplot\nfrom statsmodels.graphics.tsaplots import plot_acf\n\nplot_acf(Total, lags=50)\npyplot.show()","4cde8291":"Total.plot(figsize=(15,6))\nplt.show()","05f8ccc8":"from pylab import rcParams\nrcParams['figure.figsize'] = 18, 8\ndecomposition = sm.tsa.seasonal_decompose(Total.iloc[:,0], model='multiplicative')\nfig = decomposition.plot()\nplt.show()","be5e2a9b":"Total","964a4bf2":"Bc_Train,fitted_lambda=stats.boxcox(Total[\"Liters\"])\nprint(\"Lambda Value is:  %f \" %fitted_lambda)\nBc_Test = stats.boxcox(Test[\"Liters\"], fitted_lambda)","13574221":"import seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nsns.scatterplot(data=Bc_Train)","73aea7ca":"fig,ax =plt.subplots(1,3)\nsns.distplot(Train, ax=ax[0]).set_title('Original Train Data')\nsns.distplot(Bc_Train, ax=ax[1]).set_title('Transformed Train Data')\nsns.distplot(Bc_Test, ax=ax[2]).set_title('Transformed Test Data')\nplt.show()","82a5a9dc":"# defining a root mean squared error method to compare the success of different methods\ndef rmse(actual,forecast):\n    rms = sqrt(mean_squared_error(actual, forecast))\n    return rms","88bf22e7":"warnings.filterwarnings(\"ignore\")\nfit1 = ExponentialSmoothing(Train[\"Liters\"], seasonal_periods=12, trend='add', seasonal='add').fit(use_boxcox=True)\nfit2 = ExponentialSmoothing(Train[\"Liters\"], seasonal_periods=12, trend='add', seasonal='mul').fit(use_boxcox=True)\nfit3 = ExponentialSmoothing(Train[\"Liters\"], seasonal_periods=12, trend='mul', seasonal='mul').fit(use_boxcox=True)\nfit4 = ExponentialSmoothing(Train[\"Liters\"], seasonal_periods=12, trend='add', seasonal='mul', damped=True).fit(use_boxcox=True)\n\nresults=pd.DataFrame(index=[r\"$\\alpha$\",r\"$\\beta$\",r\"$\\phi$\",r\"$\\gamma$\",r\"$l_0$\",\"$b_0$\",\"SSE\",\"RMSE\"])\nparams = ['smoothing_level', 'smoothing_slope', 'damping_slope', 'smoothing_seasonal', 'initial_level', 'initial_slope']\nresults[\"Additive\"]       = [fit1.params[p] for p in params] + [fit1.sse] + [rmse(Test[\"Liters\"],fit1.forecast(12))]\nresults[\"Mult. Seasonal\"] = [fit2.params[p] for p in params] + [fit2.sse] + [rmse(Test[\"Liters\"],fit2.forecast(12))]\nresults[\"Multiplicative\"]   = [fit3.params[p] for p in params] + [fit3.sse] + [rmse(Test[\"Liters\"],fit3.forecast(12))]\nresults[\"Multiplica Dam\"] = [fit4.params[p] for p in params] + [fit4.sse] + [rmse(Test[\"Liters\"],fit4.forecast(12))]\n                                                                \nax = Test[\"Liters\"].plot(figsize=(15,8), marker='o', color='black', title=\"Forecasts from Holt-Winters' multiplicative method\" )\nax.set_ylabel(\"Liters\")\nax.set_xlabel(\"Month\")\nTrain[\"Liters\"].plot(ax=ax, color= \"black\")\nfit1.fittedvalues.plot(ax=ax, style='--', color='red')\nfit2.fittedvalues.plot(ax=ax, style='--', color='green')\n\nfit1.forecast(12).rename('Holt-Winters (add-add-seasonal)').plot(ax=ax, style='--', marker='o', color='red', legend=True)\nfit2.forecast(12).rename('Holt-Winters (add-mul-seasonal)').plot(ax=ax, style='--', marker='o', color='green', legend=True)\n\nplt.show()\nprint(\"Figure 1: Forecasting Sales using Holt-Winters method with both additive and multiplicative seasonality.\")\n\nresults                                                                ","587a98cd":"fit4.params","0fb9d20f":"def find_best_sarima(train, eval_metric):\n    \n    p = d = q = range(0, 2)\n    pdq = list(itertools.product(p, d, q))\n    seasonal_pdq = [(x[0], x[1], x[2], 12) for x in list(itertools.product(p, d, q))]\n\n    counter = 0\n    myDict = {}\n    \n    for param in pdq:\n        for param_seasonal in seasonal_pdq:\n            try:\n                counter += 1\n                mod = sm.tsa.statespace.SARIMAX(train,\n                                                order=param,\n                                                seasonal_order=param_seasonal,\n                                                enforce_stationarity=False,\n                                                enforce_invertibility=False)\n\n                results = mod.fit()\n                myDict[counter] = [results.aic, results.bic, param, param_seasonal]\n\n            except:\n                continue\n                \n    dict_to_df = pd.DataFrame.from_dict(myDict,orient='index')\n    \n    if eval_metric == 'aic':\n        best_run = dict_to_df[dict_to_df[0] == dict_to_df[0].min()].index.values\n        best_run = best_run[0]\n    elif eval_metric == 'bic':\n        best_run = dict_to_df[dict_to_df[1] == dict_to_df[1].min()].index.values\n        best_run = best_run[0]\n            \n    model = sm.tsa.statespace.SARIMAX(train,\n                                      order=myDict[best_run][2],\n                                      seasonal_order=myDict[best_run][3],\n                                      enforce_stationarity=False,\n                                      enforce_invertibility=False).fit()\n    \n    best_model = {'model':model, \n                  'aic':model.aic,\n                  'bic':model.bic,\n                  'order':myDict[best_run][2], \n                  'seasonal_order':myDict[best_run][3]}\n    \n    return best_model","97db7e91":"best = find_best_sarima(Bc_Train, 'aic')\nprint(\"Best SARIMA Model for our data is: {}x{} with aic= {} and bic= {}\".format(best[\"order\"], best[\"seasonal_order\"], best[\"aic\"], best[\"bic\"]))\nbest","921f740d":"mod = sm.tsa.statespace.SARIMAX(Bc_Train,\n                                order=(1, 1, 1),\n                                seasonal_order=(1, 1, 0, 12),\n                                enforce_stationarity=False,\n                                enforce_invertibility=False)\nresults = mod.fit()\nresults.plot_diagnostics(figsize=(16, 8))\nplt.show()","40593296":"pred = best['model'].predict(start=48, end=59, dynamic=True)\npred","16001766":"# invert a boxcox transform for one value\ndef power_transform_invert_value(value, lam):\n\tfrom math import log\n\tfrom math import exp\n\t# log case\n\tif lam == 0:\n\t\treturn exp(value)\n\t# all other cases\n\treturn exp(log(lam * value + 1) \/ lam)","c119f58c":"# Invert all the predicted values\no_pred=[]\nfor prediction in pred:\n    o_pred.append(power_transform_invert_value(prediction,fitted_lambda))\npred=pd.DataFrame(o_pred,index=Test.index, columns=[\"Liters\"])    \npred","56a2f319":"ax = Test[\"Liters\"].rename(\"Original Data\").plot(figsize=(15,8), marker='o', color='black', title=\"Forecasts from (1, 1, 1)x(1, 1, 0, 12) SARIMA model\",legend=True)\nax.set_ylabel(\"Liters\")\nax.set_xlabel(\"Month\")\nTrain[\"Liters\"].plot(ax=ax, color= \"black\")\npred[\"Liters\"].rename(\"SARIMA Forecast\").plot(ax=ax, style='--', color='red', legend=True)\n\n\n\n\n\nplt.show()\n\n","8a6c9c76":"print('The Root Mean Squared Error of our forecasts is {}'.format(rmse(Test[\"Liters\"],pred[\"Liters\"])))","0096b278":"I didn't use transformed data because ExponantialSmoothing method already has a use_boxcox option which saves me from the burden of inverting the transformed values for method evaluation","0fbc3551":"# FORECASTING WITH SARIMA","84d5aeb0":"### This is the end of our notebook","3dd4e6f7":"**This notebook is created to forecast the sales amount of a beverage company's product in Turkey. Due to secrecy reasons company only gives the data of how many liters are sold in a month and nothing else. Since we don't have much information about the data (Product name, exact regions that data is collected from, weather of these spesific regions, campaigns, etc.) we can't dig deeper into data and find usefull features by applying feature engineering. Therefore we have a Univariate Time Series which means there is only one variable that changes over time. Methods for forecasting Univariate time series are installed as default by many companies because they are not complex and they give decent results. We will be using 2 of the probably most popular methods to make a 12 periods(month) forecast on our data and compare their results with error measures.**","1b6ca7bf":"# Forecasting with HOLT W\u0130NTERS' SEASONAL METHOD (Exponantial Smoothing)","54b90bb0":"### Plot has sharp peaks especially at the last year, our time series is not stationary and it is multiplicative.","1972cb8e":"### Basic Inspections on Data","868d1d75":"##### Values above are the precition from our SARIMA model. Now lets see it on a plot.","903ef228":"#### Yearly seasonality and a positive trend is observed. These are importand factors which decide whether a model is good for our data or not. Residuals are positively skewed but looks close to normally distributed which means most of the change in data is explained by trend and seasonality","221a5d15":"##### One can observe that in the first plot, data was skewed to left, that could cause biased predictions, On the second and third plots, distribution is closer to normal distribution","54fd1edd":"_find_best_arima_ method found the best SARIMA model as _SARIMA(111X110)_12_ according to akaike information criterion(aic). We can do further investigation on the resudiuls of the data to confirm that our model is not missing out any meaningfull information.","48d6068b":"#### We will decompose data to its 3 components in order to understand the data better\n##### \u2013 trend: how things are overall changing\n##### \u2013 seasonality: how things change within a given period e.g. a year, month, week, day\n##### \u2013 residual: activity not explained by the trend or the seasonal value","8b679ff7":"##### Code below is taken mostly taken https:\/\/www.statsmodels.org\/stable\/examples\/notebooks\/generated\/exponential_smoothing.html#Holt's-Winters-Seasonal which is statsmodels' website. I added a RMSE row to the results dataframe for future comparison purpose.","c8289918":"##### Predictions of the model is below. We have to invert the box-cox transformed values to their original scales before anything.","228727e9":"Since data is not stationary, we have to apply a transformation method like log transform, square root, cube, etc. In this example we will apply Box-Cox Transformation to obtain a stationary time series which means the time series will have constant mean, constant variance and autocovariance does not depend on time. Most models assumes that the time series data is stationary and we have to ensure that for our data.","d6172c79":"### Downloading data and splitting Train and Test Data (48 months for Training, 12 Months for Test)","bf67b5a1":"The final sum of squared errors achieved in optimizing(SSE) is lowest for multiplicative seasonality model as expected. Damping slightly helps but it is not a significant difference. You can see below that damping slop is almost 1. For forecasting windows that are longer then 1 year, damping could have helped more maybe","33cea3f0":"ARIMA method has 3 components named AR(p) I(d) MA(q). One should investigate ACF and PACF plots to determine these parameters and try a few of the models in order to find the best one by comparing their A\u0130C scores. You can take a look at https:\/\/www.machinelearningplus.com\/time-series\/arima-model-time-series-forecasting-python\/ website to understand how the ARIMA methodology works and what does these parametes mean. In this notebook, we will not dive into these and instead will use the code below to loop through some parameter combinations of models and do parameter optimization in order to find the best model. The code below is derived from https:\/\/machinelearningmastery.com\/grid-search-arima-hyperparameters-with-python\/ website where we can understand how to make grid search on python for ARIMA hyperparameters.","483eebb6":"##### Top-Left: _Residuals are distribution looks random as expeckted._\n##### Top-Right: _Kernel Density Estimate(KDE) curve is close to normal curve although it doesn't fit perfectly as in many practical cases._\n##### Bottom-Left: _Sample quantiles(blue dots) are aligning well with the normal distribution quantiles(red line). This means again residuals are almost normally_ _distributed._\n##### Bottom-Right: _Correlogram is another name for autocorrelation function. We can see the instant drop from the first period. This Autocorrelation plot shows that there_ _is no relation between the residual observations on any time lag._","e3858eff":"#### We come to the conclution that Holt Winters' Seasonal method gave a slightly better result when compared with the RMSE.\nWhen doing forecasting, it is usually a good practice to try forecasting in different windows* and compare the errors. For a forecasting of 12 months, Holt Winters' method worked a little better but perhaps if we did a 6 months long forecasting, SARIMA could yield a better result. Therefore it is important to check in different windows.","1c507fe7":"### Now we are convinced that our model is good. It is time to plot our results and calculate the error."}}