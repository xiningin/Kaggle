{"cell_type":{"6486c9db":"code","be624c3d":"code","98cee5da":"code","10b30092":"code","4862a2ca":"code","35a1ad0d":"code","4d042758":"code","6c7d7e4f":"code","c42e810a":"code","84870f12":"code","f7cbde19":"code","54845250":"code","dbfcd1c1":"code","c6bde749":"code","d2ae0d32":"code","1e18fb79":"code","7557ed23":"code","9e8d033c":"code","2694494d":"code","dd7361c5":"code","71087591":"code","0bb13ceb":"code","c5aa6d2e":"code","e73484f8":"code","dccb55ce":"code","ac6b934e":"code","a1e9adcf":"code","eb8251d8":"code","6746a4b1":"code","b9f28cae":"code","54292cf3":"code","ba697448":"code","00c31c66":"code","5f32ab35":"code","d45e402a":"code","89934986":"code","19bba854":"code","47ca2968":"code","b2bc5315":"code","7fbd5272":"code","4bdf095f":"code","2ac5b1b1":"code","72cf8ab1":"markdown","ef3818a8":"markdown","d3627f42":"markdown","edac52ea":"markdown","6ccf2594":"markdown","e6d112c4":"markdown","f80f0f80":"markdown","b503173a":"markdown","91e7beca":"markdown","726d3d73":"markdown","afd95959":"markdown"},"source":{"6486c9db":"from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn import decomposition, ensemble\n\nimport pandas as pd, xgboost, numpy as np, textblob, string\nfrom keras.preprocessing import text, sequence\nfrom keras import layers, models, optimizers\n# imports\nfrom sqlalchemy import create_engine\n# import psycopg2\nfrom sklearn import linear_model\nfrom sklearn import ensemble\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.preprocessing import PolynomialFeatures\nimport sqlite3\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy.special import factorial\n%matplotlib inline\nimport re\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB, MultinomialNB\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.model_selection import cross_val_score\nimport math\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import cohen_kappa_score\nimport datetime\nfrom itertools import combinations\nfrom sklearn import metrics\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\nfrom sklearn.gaussian_process import GaussianProcessClassifier\nfrom sklearn.gaussian_process.kernels import RBF\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.metrics import roc_curve, auc , roc_auc_score,confusion_matrix\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.svm import LinearSVC\nfrom sklearn.preprocessing import label_binarize\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix\nfrom collections import Counter\nimport operator\nimport matplotlib.pyplot as plt\nfrom sklearn_pandas import DataFrameMapper\nfrom nltk import tokenize\nfrom sklearn.feature_extraction.text import TfidfVectorizer","be624c3d":"test = pd.read_csv('..\/input\/test.csv')\ntrain = pd.read_csv('..\/input\/train.csv')\nps = train","98cee5da":"# Function to tokenise whole speech into sentences\ndef tok(speech):\n    speech = tokenize.sent_tokenize(speech)\n    return speech\nps['sent'] = ps['text'].apply(tok)\nps.head()","10b30092":"# Transpose sentences to 1 row per sentence\ntesting = ps.drop('text',axis=1)\nnew = (testing['sent'].apply(lambda x: pd.Series(x)).stack().reset_index(level=1, drop=True).to_frame('sent').join(testing[['president','year']], how='left'))","4862a2ca":"# Encoding President Labels and reset index\nnew['pres_id'] = new['president'].factorize()[0]\nnew = new.reset_index()\nnew = new.drop(['index'],axis= 1)","35a1ad0d":"# Test out if sentence is correct\nnew['sent'][0]","4d042758":"# Additional Features\nnew['char_count'] = new['sent'].apply(len)\nnew['word_count'] = new['sent'].apply(lambda x: len(x.split()))\nnew['word_density'] = new['char_count'] \/ (new['word_count']+1)\nnew['punctuation_count'] = new['sent'].apply(lambda x: len(\"\".join(_ for _ in x if _ in string.punctuation))) \nnew['title_word_count'] = new['sent'].apply(lambda x: len([wrd for wrd in x.split() if wrd.istitle()]))\nnew['upper_case_word_count'] = new['sent'].apply(lambda x: len([wrd for wrd in x.split() if wrd.isupper()]))\n\n#Parts of Speech tagging\npos_family = {\n    'noun' : ['NN','NNS','NNP','NNPS'],\n    'pron' : ['PRP','PRP$','WP','WP$'],\n    'verb' : ['VB','VBD','VBG','VBN','VBP','VBZ'],\n    'adj' :  ['JJ','JJR','JJS'],\n    'adv' : ['RB','RBR','RBS','WRB']\n}\n# Function to apply parts of speech\ndef check_pos_tag(x, flag):\n    cnt = 0\n    try:\n        wiki = textblob.TextBlob(x)\n        for tup in wiki.tags:\n            ppo = list(tup)[1]\n            if ppo in pos_family[flag]:\n                cnt += 1\n    except:\n        pass\n    return cnt\nnew['noun_count'] = new['sent'].apply(lambda x: check_pos_tag(x, 'noun'))\nnew['verb_count'] = new['sent'].apply(lambda x: check_pos_tag(x, 'verb'))\nnew['adj_count'] = new['sent'].apply(lambda x: check_pos_tag(x, 'adj'))\nnew['adv_count'] = new['sent'].apply(lambda x: check_pos_tag(x, 'adv'))\nnew['pron_count'] = new['sent'].apply(lambda x: check_pos_tag(x, 'pron'))","6c7d7e4f":"# Save base engineered feature set\nnew.to_csv('sentence_data.csv')\nnew = pd.read_csv('sentence_data.csv')","c42e810a":"def remove_punctuation(s):\n    s = ''.join([i for i in s if i not in frozenset(string.punctuation + '0123456789')]).lower()\n    return s\n\nnew['sent'] = new['sent'].apply(remove_punctuation)\n# make lower case\nnew = pd.read_csv('sentence_data.csv')\nnew = new.drop(['Unnamed: 0'],axis =1)","84870f12":"new.head()","f7cbde19":"# Initiate object to vectorize of text and mapping of additional features to that sparse matrix\nmapper = DataFrameMapper([\n     ('sent', TfidfVectorizer(stop_words='english',\n                        ngram_range=(1, 2),\n#                        min_df=0.1,\n                       max_df=0.8,\n                       max_features=500\n#                         tfidf=True,\n#                         smooth=True\n                       )),\n     ('char_count', None),\n     ('word_count',None),\n     ('word_density', None),\n     ('punctuation_count', None),\n     ('title_word_count', None),\n     ('upper_case_word_count', None),\n     ('noun_count', None),\n     ('verb_count', None),\n     ('adj_count', None),\n     ('adv_count', None),\n     ('pron_count', None),\n ])","54845250":"# Apply mapping from cell above\nfeatures = mapper.fit_transform(new)\ncategories = new['pres_id']\n \n# Split the data between train and test\nX_train, X_test, y_train, y_test = train_test_split(features,categories,test_size=0.3,train_size=0.7, random_state = 42)\n \n# Test run to see if features are working\n# clf = RandomForestClassifier(random_state=0)\n# clf.fit(features, categories)\n# predicted = clf.predict(X_test)\n# print(X_test,y_test, predicted)","dbfcd1c1":"X_train.shape","c6bde749":"# For XG Boost\n\ndata_dmatrix = xgboost.DMatrix(data=features,label=categories)\nnames = [\n#     'Logistic Regression', \n#          'Nearest Neighbors', \n#          'Linear SVM',\n#          'RBF SVM', \n#          'Naive Bayes',\n#          'LDA',\n#          \"QDA\",          \n#          \"Decision Tree\",\n         \"XG Boost\",\n         \"Random Forest\" \n#          \"AdaBoost\", \n#          \"Neural Net\"\n]\n\nclassifiers = [\n#     LogisticRegression(), \n#     KNeighborsClassifier(n_neighbors=10),\n#     SVC(kernel=\"linear\"),\n#     SVC(kernel=\"rbf\"),    \n#     GaussianNB(),    \n#     LinearDiscriminantAnalysis(),\n#     QuadraticDiscriminantAnalysis(),    \n#     DecisionTreeClassifier(),\n    xgboost.XGBClassifier(),\n    RandomForestClassifier(n_estimators=10)\n#     AdaBoostClassifier(learning_rate=0.01),\n#     MLPClassifier(learning_rate=0.001)    \n]","d2ae0d32":"results = []\n\nmodels = {}\nconfusion = {}\nclass_report = {}\n\nfor name, clf in zip(names, classifiers):    \n    print ('Fitting {:s} model...'.format(name))\n    run_time = %timeit -q -o clf.fit(X_train, y_train)\n    \n    print ('... predicting')\n    y_pred = clf.predict(X_train)   \n    y_pred_test = clf.predict(X_test)\n    \n    print ('... scoring')\n    accuracy_train  = metrics.accuracy_score(y_train, y_pred)\n    precision_train = metrics.precision_score(y_train, y_pred,average='weighted')\n    recall_train    = metrics.recall_score(y_train, y_pred,average='weighted')\n    accuracy_test  = metrics.accuracy_score(y_test, y_pred_test)\n    precision_test = metrics.precision_score(y_test, y_pred_test,average='weighted')\n    recall_test    = metrics.recall_score(y_test, y_pred_test,average='weighted')\n    \n    f1_train        = metrics.f1_score(y_train, y_pred,average='weighted')    \n    f1_test   = metrics.f1_score(y_test, y_pred_test,average='weighted')\n    cohen_kappa = cohen_kappa_score(y_test,y_pred_test)\n    \n    # save the results to dictionaries\n    models[name] = clf    \n#     confusion[name] = metrics.confusion_matrix(y_train, y_pred)\n#     class_report[name] = metrics.classification_report(y_train, y_pred)\n    confusion[name] = metrics.confusion_matrix(y_test, y_pred_test)\n    class_report[name] = metrics.classification_report(y_test, y_pred_test)\n    \n    results.append([name, \n                    accuracy_train, precision_train, recall_train,\n                    accuracy_test, precision_test, recall_test,\n                    f1_train,\n                    f1_test,\n                    cohen_kappa,\n                    run_time.best])\n\n    \nresults = pd.DataFrame(results, columns=['Classifier', \n                                         'Accuracy_train', 'Precision_train', 'Recall_train',\n                                         'Accuracy_test', 'Precision_test', 'Recall_test',\n                                         'F1 Train', \n                                         'F1 Test',\n                                         'Cohen Kappa test',\n                                         'Train Time'])\nresults.set_index('Classifier', inplace= True)\nprint('Donezo')","1e18fb79":"results.sort_values('F1 Test', ascending=False)","7557ed23":"fig, ax = plt.subplots(1, 2, figsize=(10, 5))\nresults.sort_values('F1 Test', ascending=False, inplace=True)\nresults.plot(y=['F1 Test'], kind='bar', ax=ax[0], xlim=[0,1.1])\nresults.plot(y='Train Time', kind='bar', ax=ax[1])","9e8d033c":"# Confusion Matrix\nconfusion['Random Forest']","2694494d":"# Classification Report\nprint(class_report['Random Forest'])","dd7361c5":"# Cross Validation\nmodel = models['Random Forest']\nprint(cross_val_score(models['Random Forest'], features,categories))","71087591":"cv = []\nfor name, model in models.items():\n    print(name)\n    scores = cross_val_score(model, X=features, y=categories, cv=10)\n    print(\"Accuracy: {:0.2f} (+\/- {:0.2f})\".format(scores.mean(), scores.std()))\n    cv.append([name, scores.mean(), scores.std() ])\n    print('                                             ')\ncv = pd.DataFrame(cv, columns=['Model', 'CV_Mean', 'CV_Std_Dev'])\ncv.set_index('Model', inplace=True)","0bb13ceb":"cv.plot(y='CV_Mean', yerr='CV_Std_Dev',kind='bar', ylim=[0, 1])","c5aa6d2e":"# # Parameter Grid\nparam_grid = {\n                'n_estimators' : [300, 350],\n#             'bootstrap' : [True,False],\n              'min_samples_leaf' :[2,3],\n#               'learning_rate' : [0.01],\n                 'max_depth':[10,15],\n#                 'min_child_weight' : [1,2,3],\n#                 'objective':['multi:softmax'],\n                \"max_features\" : [250,350]\n             }","e73484f8":"grid_search = GridSearchCV(RandomForestClassifier(),param_grid,cv=3)","dccb55ce":"grid_search.fit(X_train,y_train)","ac6b934e":"grid_search.best_params_","a1e9adcf":"y_pred_gs = grid_search.predict(X_test)","eb8251d8":"# some metrics\nprint('accuracy score')\nprint(accuracy_score(y_test,y_pred_gs))\nprint('\\n')\nprint('confusion_matrix')\nprint(confusion_matrix(y_test,y_pred_gs))\nprint('\\n')\nprint('classification_report')\nprint(classification_report(y_test,y_pred_gs))\nprint('\\n')\nprint('cohen_kappa_score')\nprint(cohen_kappa_score(y_test,y_pred_gs))","6746a4b1":"test.head()","b9f28cae":"test.head()\nnew = test","54292cf3":"new['sent'] = new['text']\nnew.drop('text',axis=1,inplace=True)","ba697448":"new.head(2)","00c31c66":"# Additional Features\nnew['char_count'] = new['sent'].apply(len)\nnew['word_count'] = new['sent'].apply(lambda x: len(x.split()))\nnew['word_density'] = new['char_count'] \/ (new['word_count']+1)\nnew['punctuation_count'] = new['sent'].apply(lambda x: len(\"\".join(_ for _ in x if _ in string.punctuation))) \nnew['title_word_count'] = new['sent'].apply(lambda x: len([wrd for wrd in x.split() if wrd.istitle()]))\nnew['upper_case_word_count'] = new['sent'].apply(lambda x: len([wrd for wrd in x.split() if wrd.isupper()]))\n\n#Parts of Speech tagging\npos_family = {\n    'noun' : ['NN','NNS','NNP','NNPS'],\n    'pron' : ['PRP','PRP$','WP','WP$'],\n    'verb' : ['VB','VBD','VBG','VBN','VBP','VBZ'],\n    'adj' :  ['JJ','JJR','JJS'],\n    'adv' : ['RB','RBR','RBS','WRB']\n}\n# Function to apply parts of speech\ndef check_pos_tag(x, flag):\n    cnt = 0\n    try:\n        wiki = textblob.TextBlob(x)\n        for tup in wiki.tags:\n            ppo = list(tup)[1]\n            if ppo in pos_family[flag]:\n                cnt += 1\n    except:\n        pass\n    return cnt\nnew['noun_count'] = new['sent'].apply(lambda x: check_pos_tag(x, 'noun'))\nnew['verb_count'] = new['sent'].apply(lambda x: check_pos_tag(x, 'verb'))\nnew['adj_count'] = new['sent'].apply(lambda x: check_pos_tag(x, 'adj'))\nnew['adv_count'] = new['sent'].apply(lambda x: check_pos_tag(x, 'adv'))\nnew['pron_count'] = new['sent'].apply(lambda x: check_pos_tag(x, 'pron'))","5f32ab35":"def remove_punctuation(s):\n    s = ''.join([i for i in s if i not in frozenset(string.punctuation + '0123456789')]).lower()\n    return s\n\nnew['sent'] = new['sent'].apply(remove_punctuation)\n# make lower case","d45e402a":"new.head()","89934986":"# Initiate object to vectorize of text and mapping of additional features to that sparse matrix\nmapper = DataFrameMapper([\n     ('sent', TfidfVectorizer(stop_words='english',\n                        ngram_range=(1, 2),\n#                        min_df=0.1,\n                       max_df=0.8,\n                       max_features=500\n#                         tfidf=True,\n#                         smooth=True\n                       )),\n     ('char_count', None),\n     ('word_count',None),\n     ('word_density', None),\n     ('punctuation_count', None),\n     ('title_word_count', None),\n     ('upper_case_word_count', None),\n     ('noun_count', None),\n     ('verb_count', None),\n     ('adj_count', None),\n     ('adv_count', None),\n     ('pron_count', None),\n ])","19bba854":"features = mapper.fit_transform(new)\n# categories = new['pres_id']","47ca2968":"grid_search.predict(features)","b2bc5315":"test_final = pd.read_csv('..\/input\/test.csv')","7fbd5272":"test_final['president'] = grid_search.predict(features)\ntest_final.drop('text', axis =1, inplace = True)","4bdf095f":"test_final","2ac5b1b1":"# Output Final File\ntest_final.to_csv('test_output.csv')","72cf8ab1":"### Feature Engineering","ef3818a8":"### Feature Engineering","d3627f42":"### Pre Processing","edac52ea":"### Modeling","6ccf2594":"NLP","e6d112c4":"### Hyperparameter Tuning","f80f0f80":"### Access Data","b503173a":"### Model Evaluation","91e7beca":"### Pre Processing","726d3d73":"### Prediction and Submission","afd95959":"### Import Libraries"}}