{"cell_type":{"4e5021ef":"code","86f617d2":"code","6e4e1661":"code","dbab8445":"code","bb8a1727":"code","b913b333":"code","dd6e7616":"code","2e727e95":"code","8d126a2a":"code","8200518e":"code","42e2ea5d":"code","7d6b9d3a":"code","31992f5d":"code","c1d2a75a":"code","f886dd52":"code","ba4c5219":"code","3a8e25c9":"code","3936e2ec":"code","deb74ced":"code","47c182f0":"code","f8cf12ec":"code","fd670d7c":"code","282e7ed2":"code","0fdb7c77":"code","f1836607":"code","ce738103":"code","ee5f6d98":"code","7dabf874":"code","2a1245ea":"code","164df5e4":"code","eb307d05":"code","075e7bc9":"code","9eb0687c":"code","5f7446c4":"code","dfe0f200":"code","38e8721c":"code","189e1f6d":"code","64c4e43b":"code","e7d7c058":"code","5466eb19":"code","425000af":"code","65916447":"code","fd24c207":"code","62d573a4":"code","4eac9fc7":"code","cce6caca":"code","428b403a":"code","be7c128a":"code","a8c93c6d":"code","567d3e52":"code","5143ec32":"code","cec5ec6a":"code","19d5eb40":"code","ddb9c115":"code","353fb079":"code","8478065a":"code","d14de8d0":"code","b53760a4":"code","c0eb5a2b":"code","48dcbbf9":"code","0351b6c2":"code","4d57a8ea":"code","e6f9a9a3":"code","31533e2c":"code","a28554f4":"code","eacf18b9":"code","e7766450":"code","5d95098c":"code","f7d3ea46":"code","a7c9143a":"code","05ea402d":"code","3cf9b505":"code","e37e77dc":"code","aa1dc2b7":"markdown","84c65f11":"markdown","7dc3e786":"markdown","ab1d718a":"markdown","d92fedb7":"markdown","7ea4b550":"markdown","725bdac1":"markdown","d13dd628":"markdown","edccefa6":"markdown","d50f614c":"markdown","0944596e":"markdown","11c817f1":"markdown","79b17e17":"markdown","8033d430":"markdown","da25dd5a":"markdown","0510e539":"markdown","2a485002":"markdown","26180f62":"markdown","bd2ae652":"markdown","cb99ec26":"markdown","435e4ef2":"markdown","ce229308":"markdown","02d254de":"markdown","77359b40":"markdown","b979d503":"markdown","76ab7072":"markdown","c5049ee6":"markdown","c36c9e46":"markdown","cf145991":"markdown","812d8e7e":"markdown","8f63e7d2":"markdown","977edb7d":"markdown","4fe4d995":"markdown","79a4500e":"markdown","53f347ff":"markdown","59cf6394":"markdown","7c8b04c4":"markdown","8bf33d25":"markdown","7e736203":"markdown","89b3af27":"markdown","d8b76cf4":"markdown","bce8f694":"markdown","ff3ad5fc":"markdown","a01d6f33":"markdown","6117aee4":"markdown","6d359827":"markdown","d9e7c698":"markdown","17ba141f":"markdown","fd875311":"markdown","06d73e27":"markdown","fd37c6cc":"markdown","8911c2f4":"markdown","f497324f":"markdown","f7567896":"markdown","47013174":"markdown","4303d128":"markdown","6e112c48":"markdown","cba62980":"markdown","c0e8e9a7":"markdown"},"source":{"4e5021ef":"!pip install seaborn==0.11.0 ","86f617d2":"!pip install https:\/\/github.com\/pandas-profiling\/pandas-profiling\/archive\/master.zip","6e4e1661":"# libraries used\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport re\nimport pandas_profiling as pp\n\n# Encoders\nfrom category_encoders import TargetEncoder\nfrom sklearn.preprocessing import LabelEncoder\n\n\nfrom sklearn.preprocessing import PolynomialFeatures, StandardScaler, MinMaxScaler, RobustScaler\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.model_selection import GridSearchCV, train_test_split, RepeatedStratifiedKFold, cross_val_score\nfrom sklearn.model_selection import StratifiedKFold, StratifiedShuffleSplit\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import accuracy_score\n\n# Machine learning Models\nfrom sklearn.linear_model import LogisticRegression\nfrom xgboost import XGBClassifier\nfrom tpot import TPOTClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n\n\n# imports to mute warnings\npd.options.display.max_rows=200\npd.set_option('mode.chained_assignment', None)\n\nfrom tpot import TPOTClassifier\n\nfrom warnings import simplefilter\nfrom sklearn.exceptions import ConvergenceWarning\nsimplefilter(\"ignore\", category=ConvergenceWarning)\nsimplefilter(\"ignore\", category=RuntimeWarning)","dbab8445":"palette_1 = sns.color_palette('Accent', 6)\npalette_2 = sns.color_palette('Set1', 6)\npalette_3 = sns.color_palette('BrBG')\npalette_4 = sns.color_palette('CMRmap')\npalette_5 = sns.color_palette('Paired', 6)\npalette_6 = sns.color_palette('RdYlBu')\npalette_binary_1 = sns.color_palette('Accent_r', 2)\npalette_binary_2 = sns.color_palette('Set1', 2)\npalette_binary_3 = sns.color_palette('Set2', 2)","bb8a1727":"train = pd.read_csv('\/kaggle\/input\/titanic\/train.csv', index_col='PassengerId')\ntest = pd.read_csv('\/kaggle\/input\/titanic\/test.csv', index_col='PassengerId')","b913b333":"profile = pp.ProfileReport(train,title=\"data exploration\",explorative=True) \nprofile.to_file(\"output.html\")","dd6e7616":"fig, ax = plt.subplots(1, 2, figsize=(16, 8))\nsns.set_style('ticks')\n\nsns.kdeplot(data=train, x='Age', hue='Sex', fill=True, palette=palette_binary_3, ax=ax[0])\nsns.boxenplot(data=train, x='Sex', y='Age', ax=ax[1], palette=palette_2)\n\nsns.despine()\nplt.show()","2e727e95":"fig, ax = plt.subplots(1, 3, figsize=(16, 6))\nsns.set_style('ticks')\n\nsns.kdeplot(data=train, x='Fare', hue='Sex', fill=True, palette=palette_binary_2, ax=ax[0])\nsns.boxenplot(data=train, x='Sex', y='Fare', ax=ax[1], palette=palette_5)\n\nsns.violinplot(data=train, x='Sex', y='Fare', ax=ax[2], palette=palette_3)\n\nsns.despine()\nplt.show()","8d126a2a":"fig, ax = plt.subplots(1, 3, figsize=(16, 6))\n\nsns.boxenplot(data=train, x='Pclass', y='Age', hue='Sex', dodge=True, ax=ax[0], palette=palette_1)\n\nsns.violinplot(data=train, x='Pclass', y='Age', hue='Sex', split=True, ax=ax[1], palette=palette_6)\n\nsns.stripplot(data=train, x='Pclass', y='Age', hue='Sex', dodge=True, ax=ax[2], palette=palette_binary_3)\n\nsns.despine()\nplt.show()","8200518e":"train.isna().sum()","42e2ea5d":"test.isna().sum()","7d6b9d3a":"plt.figure(figsize=(12, 8))\nsns.set(font_scale=1.5)\nsns.set_style('white')\n\ncorr = train.corr()\n\n# Generate a mask for the upper triangle\nmask = np.triu(np.ones_like(corr, dtype=bool))\n\nsns.heatmap(corr, cmap='coolwarm', annot=True, mask=mask, annot_kws={'size':15})\nplt.show()","31992f5d":"plt.figure(figsize=(15, 8))\nsns.set_style('ticks')\nPROPS = {\n    'boxprops':{'facecolor':'none', 'edgecolor':'black', 'linewidth':0.3},\n    'medianprops':{'color':'black', 'linewidth':1.5},\n    'whiskerprops':{'color':'black', 'linewidth':0.3},\n    'capprops':{'color':'black', 'linewidth':0.3},\n}\n\nsns.boxplot(x='Pclass', data=train, y='Age', hue='Sex', showfliers=False,  **PROPS)\nsns.stripplot(data=train, x='Pclass', hue='Sex', y='Age', palette=palette_binary_3, dodge=True)\n\nsns.despine()\nplt.legend(loc='upper right')\nplt.show()","c1d2a75a":"plt.figure(figsize=(15, 8))\nsns.set_style('ticks')\nsns.boxplot(x='Pclass', data=train, y='Age', hue='Sex', showfliers=False)\nsns.stripplot(data=train, x='Pclass', hue='Sex', y='Age', palette=palette_binary_3, dodge=True)\n\nsns.despine()\nplt.legend(loc='upper right')\nplt.show()","f886dd52":"temp = train.copy()\ntemp['Cabin'] = temp.Cabin.str.extract(pat='([A-Z])')\n\nfig, ax = plt.subplots(1, 2, figsize=(16, 8))\nsns.set_style('ticks')\n\nsns.countplot(data=temp, x='Cabin', hue='Pclass', ax=ax[0], palette=palette_1)\nax[0].set_title('Pclass-Cabin Proportions', x=0.28, y=1.04, size=25)\n\ntemp.Cabin.fillna('missing', inplace=True)\ntemp_missing = temp.loc[temp.Cabin == 'missing']\n\nsns.countplot(data=temp_missing, x='Cabin', hue='Pclass', palette=palette_1)\nax[1].set_title('Missing Cabin proportions', x=0.27, y=1.04, size=25)\n\nsns.despine()\nplt.show()","ba4c5219":"temp = train.copy()\ntemp['Cabin'] = temp.Cabin.str.extract(pat='([A-Z])')\ntemp.Cabin.fillna('missing', inplace=True)\n\nsns.set(font_scale=1.4)\nsns.set_style('white')\n\nfig, ax = plt.subplots(2, 2, figsize=(16, 17))\nsns.countplot(data=temp, x='Cabin', hue='Pclass', ax=ax[0, 0], palette=palette_5)\nax[0, 0].set_title('Cabin-Pclass proportions', x=0.28, y=1.04, size=25)\n\n\n\ntemp.Cabin.replace({'A':'ABC', 'B':'ABC', 'C':'ABC', \n                    'D':'DE', 'E':'DE', 'F':'FG', \n                    'G':'FG', 'T':'ABC', 'missing':'M'}, inplace=True)\n\n\nsns.countplot(data=temp, x='Cabin', hue='Pclass', ax=ax[0, 1], palette=palette_5)\nax[0, 1].set_title('Deck-Pclass Proportions', x=0.27, y=1.04, size=25)\nax[0, 1].set_xlabel('Deck', size=18)\n\n\n\nsns.barplot(data=temp, x='Cabin', y='Survived', hue='Pclass', ax=ax[1, 0], palette=palette_5)\nax[1, 0].set_title('Deck Survival rate', x=0.18, y=1.02, size=25)\nax[1, 0].set_xlabel('Deck', size=18)\n\n\nsns.barplot(data=temp, x='Cabin', y='Fare', hue='Pclass', ax=ax[1, 1], palette=palette_5)\nax[1, 1].set_title('Deck Fare', x=0.1, y=1.02, size=25)\nax[1, 1].set_xlabel('Deck', size=18)\n\nsns.despine()\nplt.show()","3a8e25c9":"def imputer(df):\n    \n    # imputing missing age values\n    \n    age_impute_series = df.groupby(['Pclass', 'Sex','SibSp']).Age.transform('mean')\n    df.Age.fillna(age_impute_series, inplace=True)\n    \n    # imputing Cabin missing value.\n    df.Cabin = df.Cabin.str.extract(pat='([A-Z])')\n    \n    df.Cabin.fillna('M', inplace=True)\n    \n    df['Deck'] = df.Cabin.replace({'A':'ABC', 'B':'ABC', 'C':'ABC', 'D':'DE', 'E':'DE', 'F':'FG', \n                                   'G':'FG', 'T':'ABC'}) \n    \n    df.drop('Cabin', axis=1, inplace=True)\n    \n    # lets just finally fill all the left over missig value with the mode of the feature.\n    for feature in df.columns:\n        df[feature].fillna(df[feature].mode()[0], inplace=True)\n        \n    return df","3936e2ec":"train_imputed = imputer(train.copy())\ntest_imputed = imputer(test.copy())","deb74ced":"cat_features = ['Pclass', 'Sex', 'SibSp', 'Parch', 'Deck', 'Embarked']\n\nplt.figure(figsize=(16, 14))\nsns.set(font_scale= 1.2)\nsns.set_style('ticks')\n\nfor i, feature in enumerate(cat_features):\n    plt.subplot(2, 3, i+1)\n    sns.countplot(data=train_imputed, x=feature, hue='Survived', palette=palette_6)  \n    \nsns.despine()","47c182f0":"num_features = ['Fare', 'Age']\nsns.set_style('white')\n\nplt.figure(figsize=(16, 14))\nfor i, feature in enumerate(num_features):\n    plt.subplot(2, 2, i+1)\n    plt.hist(x=[train_imputed[feature][train_imputed['Survived'] == 1], train_imputed[feature][train_imputed['Survived']==0]],\n            stacked=True, label=['Survived', 'Not Survived'], bins=20, color=['orange', 'b'])\n    plt.legend()\n    plt.xlabel(f'{feature}', fontsize=15)\n    plt.ylabel('Count', fontsize=15)\n\nfor i, feature in enumerate(num_features):\n    plt.subplot(2, 2, i+3)\n    sns.kdeplot(data=train_imputed, x=feature, hue='Survived', fill=True, palette=palette_binary_3)\n    \nsns.despine()","f8cf12ec":"temp = train_imputed.copy()\ntemp['Fare'] = pd.qcut(temp.Fare, 7)\nsns.set_style('ticks')\n\nplt.figure(figsize=(15, 8))\nsns.countplot(data=temp, x='Fare', hue='Survived', palette=palette_binary_2)\n\nplt.ylabel('Count', fontsize=18, labelpad=5)\nplt.xlabel('Fare', fontsize=18, labelpad=10)\nplt.tick_params(axis='x', labelsize=13)\n\nplt.title('Fare Categories Survival Count plot', y=1.03, fontsize=25, loc='Left')\nsns.despine()\nplt.show()","fd670d7c":"temp = train_imputed.copy()\ntemp['Age_cat'] = pd.cut(temp.Age, bins=[0, 5, 24, 30, 36, np.inf])\ntemp['Age_qcat'] = pd.qcut(temp.Age, 15, precision=2)\n\nfig, ax = plt.subplots(2, 1, figsize=(15, 14))\nsns.set_style('ticks')\n\nsns.countplot(data=temp, x='Age_qcat', hue='Survived', ax=ax[0], palette=palette_binary_3)\nax[0].tick_params(axis='x', labelsize=9)\n\nsns.countplot(data=temp, x='Age_cat', hue='Survived', ax=ax[1], palette=palette_binary_3)\n\nsns.despine()\nplt.show()","282e7ed2":"temp = train_imputed.copy()\ntemp['Ticket_type'] = pd.Series(np.where(temp.Ticket.str.contains(pat='[A-Z]'), 'alphabet', 'numeric'), index=temp.index)\ntemp['Fare_cat'] = pd.qcut(temp.Fare, 7, precision=2)\n\ny=1.01\nsize=17\nfig, ax = plt.subplots(3, 2, figsize=(16, 19))\nplt.subplots_adjust(hspace=0.3)\n\nsns.set(font_scale=1)\nsns.set_style('ticks')\n\nsns.countplot(data=temp, x='Embarked', hue='Ticket_type', ax=ax[0, 0], palette=palette_3)\nax[0, 0].set_title('Ticket Type V\/s Embarked', size=size, loc='Left', y=y)\n\n\nsns.countplot(data=temp, x='Pclass', hue='Ticket_type', ax=ax[0, 1], palette=palette_3)\nax[0, 1].set_title('Pclass V\/s Ticket Type', size=size, loc='Left', y=y)\n\nsns.countplot(data=temp, x='Embarked', hue='Pclass', ax=ax[1, 0], palette=palette_3)\nax[1, 0].set_title('Pclass V\/s Embarked', size=size, loc='Left', y=y)\n\nsns.countplot(data=temp, x='Fare_cat', hue='Ticket_type', ax=ax[1, 1], palette=palette_3)\nax[1, 1].set_title('Fare Category V\/s Ticket type', size=size, loc='Left', y=y)\nax[1, 1].tick_params(axis='x', labelsize=9.5)\n\nsns.countplot(data=temp, x='Ticket_type', hue='Survived', ax=ax[2, 0], palette=palette_3)\nax[2, 0].set_title('Ticket_type - Survived Plot', size=size, loc='Left', y=y)\n\nsns.countplot(data=temp, x='Deck', hue='Ticket_type', ax=ax[2, 1], palette=palette_3)\nax[2, 1].set_title('Ticket_type - Deck Plot', size=size, loc='Left', y=y)\n\nsns.despine()\nplt.show()","0fdb7c77":"def ticket_extractor(ticket):\n    alpha = re.sub('\\d', '', ticket)\n    if alpha:\n        return alpha\n    else:\n        num = re.search('\\d{1,9}', ticket)\n        return ticket\n    \ntemp = train_imputed.copy()\ntemp['Ticket_extracted'] = temp.Ticket.apply(ticket_extractor)\nfor i in range(len(temp.Ticket)):\n    try:\n        int(temp.Ticket_extracted.iloc[i])\n        temp.Ticket_extracted.iloc[i] = f'Num_{len(temp.Ticket_extracted.iloc[i])}'\n    except:\n        continue\n        \nfor label, pattern in [('_ca', 'C[.]?A[.]?'), ('_PC', 'PC'), ('_SOTON', 'SOTON'), ('_STON', 'STON'), \n                       ('_WC', 'W[.]?[\/]?C'), ('_SC', 'S[.]?C[.]?'), ('_A', 'A[.]?'), ('_SOC', 'S[.]?O[.]?[\/]?C'), \n                       ('_PP', 'PP'), ('_FC', '(F.C.|F.C.C.)'), ('_LS_number', 'Num_(6|7)'), ('_SS_number', 'Num_(3|4|5)'), \n                       ('rare', '^[^_]')]:\n    temp.Ticket_extracted[temp.Ticket_extracted.str.contains(pattern)] = label\n    \ntemp['Ticket_extracted'].value_counts(dropna=False)\n\nfig, ax = plt.subplots(2, 1, figsize=(15, 15))\nsns.countplot(data=temp, x='Ticket_extracted', hue='Survived', ax=ax[0], palette=palette_6)\nax[0].set_title('Extracted Ticket - Survival \"count\" plot', size=20, loc='Left', y=1.03)\n\nsns.barplot(data=temp, x='Ticket_extracted', y='Survived', ax=ax[1], palette=palette_6)\nax[1].set_title('Extracted Ticket - Survival \"chance\" plot', size=20, loc='Left', y=1.03)\n\nsns.despine()\nplt.show()","f1836607":"temp['Ticket_frequency'] =temp.Ticket.map(temp.Ticket.value_counts(dropna=False))\ntemp['Fare_cat'] = pd.qcut(temp.Fare, 7, precision=2)\n\ny=1.03\nsize=25\nfig, ax = plt.subplots(2, 1, figsize=(15, 15))\nsns.set(font_scale=1.4)\nsns.set_style('ticks')\n\nsns.countplot(data=temp, x='Ticket_frequency', hue='Survived', ax=ax[0], palette=palette_2)\nax[0].set_title('Ticket frequency - Survived Plot', loc='Left', y=y, size=size)\nax[0].set_xlabel('')\nax[0].set_ylabel('')\n\nsns.countplot(data=temp, x='Fare_cat', hue='Ticket_frequency', ax=ax[1], palette=palette_2)\nax[1].set_title('Ticket frequency - Fare category Plot', loc='Left', y=y, size=size)\nax[1].set_xlabel('')\nax[1].set_ylabel('')\n\nsns.despine()\nplt.show()","ce738103":"temp = train_imputed.copy()\ntemp['Ticket_frequency'] =temp.Ticket.map(temp.Ticket.value_counts(dropna=False))\ntemp['Fare_cat'] = pd.qcut(temp.Fare, 7, precision=2)\ntemp['Fare_per_individual'] = temp['Fare'] \/ temp['Ticket_frequency']\ntemp['Fare_per_individual_cat'] = pd.qcut(temp.Fare_per_individual, 7, precision=2)\n\ny = 1.03\nsize = 18\nsns.set(font_scale=1.2)\nsns.set_style('ticks')\nfig, ax = plt.subplots(1, 2, figsize=(16, 7), sharey=True)\n\nsns.kdeplot(data=temp, x='Fare_per_individual', hue='Survived', clip=[0, 200], fill=True, ax=ax[0], palette=palette_binary_3)\n\nsns.kdeplot(data=temp, x='Fare', hue='Survived', clip=[0, 200],  fill=True, ax=ax[1], palette=palette_binary_3)\nax[1].set_ylabel('')\nsns.despine()\n\nfig, ax = plt.subplots(1, 2, figsize=(16, 7), sharey=True)\n\nsns.countplot(data=temp, x='Fare_per_individual_cat', hue='Survived', ax=ax[0], palette=palette_binary_3)\nax[0].tick_params(axis='x', labelsize=9.5)\n#ax[0].set_ylabel('')\n#ax[0].set_xlabel('')\n\nsns.countplot(data=temp, x='Fare_cat', hue='Survived', ax=ax[1], palette=palette_binary_3)\nax[1].tick_params(axis='x', labelsize=9.5)\nax[1].set_ylabel('')\n#ax[1].set_xlabel('')\n\nsns.despine()\nplt.show()","ee5f6d98":"temp = train_imputed.copy()\ntemp['Family_size'] = temp['SibSp'] + temp['Parch'] + 1\nsns.set_style('ticks')\n\nfig, ax = plt.subplots(1, 1, figsize=(15, 8))\nsns.countplot(data=temp, x='Family_size', hue='Survived', ax=ax, palette=palette_5)\nax.set_title('Family Size - Survived Plot', size=25, loc='Left', y=1.04)\n\nsns.despine()\nplt.show()","7dabf874":"temp['Family_size_cat'] = temp['Family_size'].replace({1:'alone', 2:'small_family', 3:'small_family', 4:'small_family'\n                                                      ,5:'large_family', 6:'large_family', 7:'large_family'\n                                                      ,8:'large_family', 9:'large_family', 10:'large_family', \n                                                       11:'large_family'})\n\nfig, ax = plt.subplots(1, 1, figsize=(15, 8))\nsns.set_style('ticks')\n\nsns.countplot(data=temp, x='Family_size_cat', hue='Survived', ax=ax, palette=palette_binary_2)\nax.set_title('Family Category - Survived Plot', size=25, loc='Left', y=1.04)\n\nsns.despine()\nplt.show()","2a1245ea":"temp = train_imputed.copy()\ntemp['Name_length'] = temp.Name.str.replace(pat='[^a-zA-Z]', repl='').str.len()\nsns.set_style('ticks')\n\nfig, ax = plt.subplots(1, 1, figsize=(16, 6))\nsns.histplot(data=temp, x='Name_length', hue='Survived', kde=True, fill=True, ax=ax, palette=palette_binary_3)\nax.set_title('Name Length - Survived Plot', size=20, loc='Left', y=1.03)\n\nsns.despine()\nplt.show()","164df5e4":"temp = train_imputed.copy()\n\ntemp['Title'] = temp.Name.str.extract(pat='([a-zA-Z]+\\.)')\n\ntemp.Title[~temp.Title.isin(['Mr.', 'Miss.', 'Mrs.', 'Master.'])] = 'rare'","eb307d05":"fig, ax = plt.subplots(1, 1, figsize=(15, 8))\nsns.set_style('ticks')\n\nsns.countplot(data=temp, x='Title', hue='Survived', ax=ax, palette=palette_6)\nax.set_title('Title - Survived Plot', loc='Left', size=25, y=1.03)\n\nsns.despine()\nplt.show()","075e7bc9":"temp = train_imputed.copy()\ntemp['Family_name'] = temp.Name.str.split(',', n=1, expand=True).iloc[:, 0]\ntemp['Family_size'] = temp['SibSp'] + temp['Parch'] + 1\ntemp['Ticket_frequency'] = temp.Ticket.map(temp.Ticket.value_counts()) \n\ntemp['Family_survival_rate'] = temp.Family_name.map(temp.groupby(['Family_name']).Survived.median())\ntemp['Ticket_group_survival_rate'] = temp.Ticket.map(temp.groupby(['Ticket']).Survived.median())\n\n\nfig, ax = plt.subplots(1, 2, figsize=(15, 8), sharey=True)\n\nsns.countplot(data=temp[temp['Family_size'] > 1], x='Family_survival_rate', hue='Survived', ax=ax[0], palette=palette_5)\n# we will only take into account those instance with Family size > 1 for visualization to explore the effectiveness of the \n# Survival rate feature\n\nsns.countplot(data=temp[temp['Ticket_frequency'] > 1], x='Ticket_group_survival_rate', hue='Survived', ax=ax[1], palette=palette_5)\n# we will only take into account those instance with Ticket_frequency > 1 for visualization to explore the effectiveness of the \n# Survival rate feature\n\nsns.despine()\nplt.show()","9eb0687c":"def feature_creator(df_train, df_test):\n    \n    # creating binned fare feature\n    df_train['Fare_cat'] = pd.qcut(df_train['Fare'], 7)\n    df_test['Fare_cat'] = pd.qcut(df_test['Fare'], 7)\n    \n    df_train['Fare_cat'] = LabelEncoder().fit_transform(df_train['Fare_cat'])\n    df_test['Fare_cat'] = LabelEncoder().fit_transform(df_test['Fare_cat'])\n    \n    \n    \n    # creating binned age feature\n    df_train['Age_cat'] = pd.cut(df_train.Age, bins=[0, 5, 24, 30, 36, np.inf])\n    df_test['Age_cat'] = pd.cut(df_test.Age, bins=[0, 5, 24, 30, 36, np.inf])\n    \n    df_train['Age_cat'] = LabelEncoder().fit_transform(df_train['Age_cat'])\n    df_test['Age_cat'] = LabelEncoder().fit_transform(df_test['Age_cat'])\n    \n    \n    \n    # Title feature\n    for df in [df_train, df_test]:\n        df['Title'] = df.Name.str.extract(pat='([a-zA-Z]+\\.)')\n        df.Title[~df.Title.isin(['Mr.', 'Miss.', 'Mrs.', 'Master.'])] = 'rare'\n        \n        \n\n    # creating Ticket frequency feature\n    df_full = pd.concat([df_train, df_test], axis=0)\n    df_full['Ticket_frequency'] = df_full.Ticket.map(df_full.Ticket.value_counts())\n    df_train = df_full.loc[df_train.index]\n    df_test = df_full.loc[df_test.index].loc[df_test.index].drop('Survived', axis=1)\n    \n    \n    \n    # creating extracted ticket feature\n    for df in [df_train, df_test]:\n        \n        df['Ticket_extracted'] = df.Ticket.apply(ticket_extractor)\n        for i in range(len(df.Ticket)):\n            try:\n                int(df.Ticket_extracted.iloc[i])\n                df.Ticket_extracted.iloc[i] = f'Num_{len(df.Ticket_extracted.iloc[i])}'\n            except:\n                continue\n\n        for label, pattern in [('_ca', 'C[.]?A[.]?'), ('_PC', 'PC'), ('_SOTON', 'SOTON'), ('_STON', 'STON'), \n                               ('_WC', 'W[.]?[\/]?C'), ('_SC', 'S[.]?C[.]?'), ('_A', 'A[.]?'), ('_SOC', 'S[.]?O[.]?[\/]?C'), \n                               ('_PP', 'PP'), ('_FC', '(F.C.|F.C.C.)'), ('_LS_number', 'Num_(6|7)'), \n                               ('_SS_number', 'Num_(3|4|5)'), ('rare', '^[^_]')]:\n            df.Ticket_extracted[df.Ticket_extracted.str.contains(pattern)] = label\n            \n            \n    \n    # Fare per individual feature\n    #df_train['Fare_per_individual'] = df_train['Fare'] \/ df_train['Ticket_frequency']\n    #df_test['Fare_per_individual'] = df_test['Fare'] \/ df_test['Ticket_frequency']\n    \n    #df_train['Fare_per_individual_cat'] = pd.qcut(df_train['Fare_per_individual'], 7)\n    #df_test['Fare_per_individual_cat'] = pd.qcut(df_test['Fare_per_individual'], 7)\n    \n    #df_train['Fare_per_individual_cat'] = LabelEncoder().fit_transform(df_train['Fare_per_individual_cat'])\n    #df_test['Fare_per_individual_cat'] = LabelEncoder().fit_transform(df_test['Fare_per_individual_cat'])\n    \n    \n    \n    # Family size feature\n    df_train['Family_size'] = df_train['SibSp'] + df_train['Parch'] + 1\n    df_test['Family_size'] = df_test['SibSp'] + df_test['Parch'] + 1\n    \n    \n    \n    # creating binned family sized feature\n    df_train['Family_size_cat'] = df_train.Family_size.replace({1:'alone', 2:'small_family', 3:'small_family', 4:'small_family'\n                                                               ,5:'large_family', 6:'large_family', 7:'large_family'\n                                                               ,8:'large_family', 9:'large_family', 10:'large_family'\n                                                               ,11:'large_family'})\n    \n    df_test['Family_size_cat'] = df_test.Family_size.replace({1:'alone', 2:'small_family', 3:'small_family', 4:'small_family'\n                                                               ,5:'large_family', 6:'large_family', 7:'large_family'\n                                                               ,8:'large_family', 9:'large_family', 10:'large_family' \n                                                               ,11:'large_family'})\n    \n    \n    \n    # Name length feature\n    df_train['Name_length'] = df_train.Name.str.replace(pat='[^a-zA-Z]', repl='').str.len()\n    df_test['Name_length'] = df_test.Name.str.replace(pat='[^a-zA-Z]', repl='').str.len()\n    \n    \n    \n    # survival rate feature\n    df_full = pd.concat([df_train, df_test], axis=0).reset_index()\n\n    df_full['Family_name'] = df_full.Name.str.split(',', n=1, expand=True).iloc[:, 0]\n    \n    df_full['Family_Survival'] = 0.5\n    \n    # the following Survival rate code has been copied from https:\/\/www.kaggle.com\/konstantinmasich\/titanic-0-82-0-83\n    for grp, grp_df in df_full.groupby(['Family_name', 'Fare']):\n        if (len(grp_df) != 1):\n            # Fare and Family_name are used as category to group families together, as these features should be same for\n            # families and once we get the groups we filter for grp_df which have ore than 1 length, hence a family.\n            for ind, row in grp_df.iterrows():\n                smax = grp_df.drop(ind)['Survived'].max()\n                smin = grp_df.drop(ind)['Survived'].min()\n                passID = row['PassengerId']\n                if (smax == 1.0):\n                    df_full.loc[df_full['PassengerId'] == passID, 'Family_Survival'] = 1\n                elif (smin==0.0):\n                    df_full.loc[df_full['PassengerId'] == passID, 'Family_Survival'] = 0\n\n    for _, grp_df in df_full.groupby('Ticket'):\n        if (len(grp_df) != 1):\n            for ind, row in grp_df.iterrows():\n                if (row['Family_Survival'] == 0) | (row['Family_Survival']== 0.5):\n                    smax = grp_df.drop(ind)['Survived'].max()\n                    smin = grp_df.drop(ind)['Survived'].min()\n                    passID = row['PassengerId']\n                    if (smax == 1.0):\n                        df_full.loc[df_full['PassengerId'] == passID, 'Family_Survival'] = 1\n                    elif (smin==0.0):\n                        df_full.loc[df_full['PassengerId'] == passID, 'Family_Survival'] = 0\n     \n    df_full.set_index('PassengerId', inplace=True)\n    df_train = df_full[:891]\n    df_test = df_full[891:]\n    \n    \n    # finally we will drop the family name feature we created to help us create the Survival rate feature.\n    df_train.drop(['Family_name'], axis=1, inplace=True)\n    df_test.drop(['Family_name', 'Survived'], axis=1, inplace=True)\n    \n    # lets finally drop the survived from our training set and create the target variable\n    y_train =df_train.Survived\n    df_train.drop('Survived', axis=1, inplace=True)\n    \n    return df_train, y_train, df_test","5f7446c4":"X_train, y_train, X_test = feature_creator(train_imputed.copy(), test_imputed.copy())\ny_train = y_train.astype(int)","dfe0f200":"X_train.head(5)","38e8721c":"fig, ax = plt.subplots(1, 2, figsize=(16, 6))\n\n\nsize = 20\ny = 1.03\nX_train['Family_Survival'] = np.round(X_train.Family_Survival, 2)\n\nsns.countplot(data=pd.concat([X_train, y_train], axis=1), x='Family_Survival', hue='Survived', ax=ax[0], palette=palette_2)\nax[0].set_title('Survival rate in training set', loc='Left', size=size, y=y)\n\n\nX_test.Family_Survival = np.round(X_test.Family_Survival, 2)\nsns.countplot(data=X_test, x='Family_Survival', ax=ax[1], palette=palette_2)\nax[1].set_title('Survival rate - Count plot test set', loc='Left', size=size, y=y)\n\nsns.despine()\nplt.show()","189e1f6d":"# if we wish to create a transformer which we can grid search in a pipeline, that transformer has to inherit from the \n# BaseEstimator class, we will also make our FeatureEngineering transformer inherit from the TransformerMixin which just \n# provides the functionality of fit_transform() method to our transformer.\n\nclass FeatureEngineering(BaseEstimator, TransformerMixin):\n    def __init__(self, bin_fare=False, bin_age=True, family_size=True, bin_family_size=True,\n                 drop_Name_length=False, drop_Ticket_frequency=False, drop_all=True, drop_Family_Survival=True, \n                 drop_Ticket_extracted=False, scaling='StandardScaler', target_encode_title=True, test=False):\n        self.bin_fare = bin_fare\n        self.bin_age = bin_age\n        #self.bin_fare_individual_cat = bin_fare_individual_cat\n        self.family_size = family_size\n        self.bin_family_size = bin_family_size\n        self.drop_Name_length = drop_Name_length\n        self.drop_Ticket_frequency = drop_Ticket_frequency\n        self.drop_all = drop_all\n        self.drop_Ticket_extracted = drop_Ticket_extracted\n        self.scaling = scaling\n        self.drop_Family_Survival = drop_Family_Survival\n        self.target_encode_title = target_encode_title\n        self.transformer = None\n        self.drop_list = []\n        self.test = test\n        \n    def fit(self, X, y=None):\n        return self # the fit method usually return self only, if not sure, try fitting a scikit learn estimator\/transformer.. \n                    # only the fitted object will be returned, fit() will include code in case of these transformers and\n                    # estimators but we do not need to have some fitted variables on our dataset before we go for transform,\n                    # or atleast the way i am doing it i don't need to. :) \n    \n    def transform(self, X, y=None):\n        \n        X['Pclass'] = X['Pclass'].astype(str) # import convert numeric values to str if we wish to use get_dummies on them.\n        dummies = pd.get_dummies(X.loc[:, ['Pclass', 'Sex']], drop_first=True)\n        X = pd.concat([X, dummies], axis=1)\n        X.drop(['Pclass', 'Sex'], axis=1, inplace=True)\n        \n        # lets also drop Name and Ticket feature as we have extracted what we needed!!\n        X.drop(['Name', 'Ticket'], axis=1, inplace=True)\n        \n        \n        \n        \n        if self.bin_fare:\n            # so if we self.bin_fare = True we want the binned fare, so drop the continuous fare feature.\n            self.drop_list.append('Fare')\n            \n        else:\n            self.drop_list.append('Fare_cat')\n            \n        \n        if self.bin_age:\n            self.drop_list.append('Age')\n        \n        else:\n            self.drop_list.append('Age_cat')\n            \n            \n        #if self.bin_fare_individual_cat:\n            #self.drop_list.append('Fare_per_individual')\n            \n        #else:\n            #self.drop_list.append('Fare_per_individual_cat')\n                \n                \n                \n                \n        if self.family_size:\n            self.drop_list.extend(['SibSp', 'Parch'])\n            \n            if self.bin_family_size:\n                self.drop_list.append('Family_size')\n                \n            else:\n                self.drop_list.append('Family_size_cat')\n                \n        else:\n            self.drop_list.extend(['Family_size', 'Family_size_cat'])\n            \n        if self.drop_Name_length:\n            X.drop('Name_length', axis=1, inplace=True)\n            \n        if self.drop_Ticket_frequency:\n            X.drop('Ticket_frequency', axis=1, inplace=True)\n            \n        if self.drop_Family_Survival:\n            X.drop('Family_Survival', axis=1, inplace=True)\n            \n        if self.drop_all:\n            X.drop(self.drop_list, axis=1, inplace=True)\n            \n       \n        self.cols = ['Title', 'Ticket_extracted'] if self.target_encode_title else ['Ticket_extracted']\n        \n        if not self.test and len(X) > 400:\n            # we want the encoder object to be fitted with traing set only so we will check 'not self.test' and we also do not\n            # want to program to pass through the if statement in the case of cross validation test is being considered, \n            # hence we also check len(X) > 400 assuming the cross val test size will always stay less than 400.\n            self.target_encoder = TargetEncoder(cols=self.cols, smoothing=5)\n            X.loc[:, self.cols] = self.target_encoder.fit_transform(X.loc[:, self.cols], y_train.loc[X.index])\n\n        else:\n            X.loc[:, self.cols] = self.target_encoder.transform(X.loc[:, self.cols])\n                \n                \n        if self.drop_Ticket_extracted:\n            X.drop('Ticket_extracted', axis=1, inplace=True)\n        \n        # finally lets convert all remaining categorical features into on hot features\n        X = pd.get_dummies(X, drop_first=True)  \n     \n        # only features with no. of unique values as 2, will one hot encoded features and we do not want to apply Normalization\n        # on them(reason explained in a bit)\n        features_to_scale = [feature for feature in X.columns if X[feature].nunique() != 2]  \n        if not self.test and len(X) > 400:    \n            if self.scaling == 'StandardScaler':\n                self.transformer = StandardScaler()\n                X.loc[:, features_to_scale] = self.transformer.fit_transform(X.loc[:, features_to_scale])\n                \n            elif self.scaling == 'RobustScaler':\n                self.transformer = RobustScaler()\n                X.loc[:, features_to_scale] = self.transformer.fit_transform(X.loc[:, features_to_scale])\n                \n            elif self.scaling == 'MinMaxScaler':\n                self.transformer = MinMaxScaler()\n                X.loc[:, features_to_scale] = self.transformer.fit_transform(X.loc[:, features_to_scale])\n        \n        else:\n            X.loc[:, features_to_scale] = self.transformer.transform(X.loc[:, features_to_scale])\n            \n        return X","64c4e43b":"def results(grid, n=10):\n    for index, row in pd.DataFrame(grid.cv_results_)[['params', \n                                                      'mean_test_score']].nlargest(n=n, columns='mean_test_score').iterrows():\n        print(f'{row[0]} : {row[1]}')","e7d7c058":"param_grid_pipeline = {'feature_engineering__bin_fare':[True, False],\n                       'feature_engineering__bin_age':[True, False],\n                       #'feature_engineering__bin_fare_individual_cat':[True, False],\n                       'feature_engineering__family_size':[True, False],\n                       'feature_engineering__bin_family_size':[True, False],\n                       'feature_engineering__drop_Name_length':[False],\n                       'feature_engineering__drop_Ticket_frequency':[False],\n                       'feature_engineering__drop_all':[True],\n                       'feature_engineering__drop_Ticket_extracted':[True, False],\n                       'feature_engineering__target_encode_title':[True, False],\n                       'feature_engineering__scaling':['StandardScaler']} \n\n\n\npipeline_ = Pipeline([('feature_engineering', FeatureEngineering()),\n                 ('model', LogisticRegression())])\n\ncv = StratifiedShuffleSplit(n_splits=20, test_size=0.15, random_state=101)\n\ngrid = GridSearchCV(pipeline_, param_grid_pipeline, \n                    cv=cv.split(X_train, X_train.Deck),\n                    scoring='accuracy', verbose=2, n_jobs=-1)\n\ngrid.fit(X_train.copy(), y_train)","5466eb19":"pd.DataFrame(grid.cv_results_)['mean_test_score'].isna().sum() # check if error occured while using a feature combination or not.","425000af":"results(grid, n=60)","65916447":"fe = FeatureEngineering(bin_fare=False, bin_age=True, family_size=True, bin_family_size=True,\n                 drop_Name_length=False, drop_Ticket_frequency=False, drop_all=True, drop_Family_Survival=False, \n                 drop_Ticket_extracted=False, scaling='StandardScaler', target_encode_title=False, test=False)","fd24c207":"X_train_fe = fe.fit_transform(X_train.copy())\nX_train_fe","62d573a4":"fe.test = True","4eac9fc7":"X_test_fe = fe.transform(X_test.copy())\nX_test_fe","cce6caca":"def parameter_plot(model, X, y, n_estimators=[100, 200, 300, 400, 600, 900], \n                   hyper_param=None, **kwargs):\n    param_name, param_vals = hyper_param\n    param_grid = {'n_estimators':n_estimators,\n                  f'{param_name}':param_vals}\n    \n    grid = GridSearchCV(model(**kwargs), param_grid, \n                        cv=RepeatedStratifiedKFold(n_splits=5, n_repeats=2, random_state=42), \n                        scoring='accuracy', n_jobs=-1, verbose=2)\n    grid.fit(X, y)\n    results = pd.DataFrame(grid.cv_results_)['mean_test_score'].values\n    results = results.reshape(len(param_vals), len(n_estimators))\n    \n    plt.figure(figsize=(15, 9))\n    for i in range(1, len(param_vals) + 1):\n        plt.plot(n_estimators, results[i-1], label=f'{param_name} - {param_vals[i-1]}')\n      \n    plt.legend()\n    plt.show()","428b403a":"def learning_curve_plotter(Model, X, y, params_1, params_2, step=50):\n    \n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n    \n    plt.figure(figsize=(16, 7))\n    for i, (name, params) in enumerate([params_1, params_2]):\n        train_score = []\n        val_score = []\n        plt.subplot(1, 2, i+1)\n        for j in range(100, len(X_train), step):\n            model = Model(**params).fit(X_train[:j], y_train[:j])\n            train_score.append(model.score(X_train[:j], y_train[:j]))\n            val_score.append(model.score(X_test, y_test))\n            \n        plt.plot(train_score, 'r-', label='Training accuracy')\n        plt.plot(val_score, 'b-', label='Validation accuracy')\n        plt.title(f'{name}')\n        plt.xlabel('Training set size')\n        plt.ylabel('Accuracy')\n        plt.legend()\n            \n    plt.show()","be7c128a":"parameter_plot(XGBClassifier, X_train_fe, y_train, hyper_param=('max_depth', [3, 4, 5]))","a8c93c6d":"parameter_plot(XGBClassifier, X_train_fe, y_train, \n               hyper_param=('learning_rate', [0.01, 0.03, 0.05]), max_depth=3) ","567d3e52":"param_grid_xgb = {'n_estimators':[400, 600],\n                  'learning_rate':[0.01, 0.03],\n                  'max_depth':[3, 4],\n                  'subsample':[0.5, 0.7],\n                  'colsample_bylevel':[0.5, 0.7],\n                  'reg_lambda':[15, None],\n                 }","5143ec32":"grid_xgb = GridSearchCV(XGBClassifier(), param_grid_xgb, \n                        cv=RepeatedStratifiedKFold(n_splits=5, n_repeats=2, random_state=42),\n                              scoring='accuracy', verbose=2, n_jobs=-1)","cec5ec6a":"grid_xgb.fit(X_train_fe, y_train)","19d5eb40":"results(grid_xgb, n=60)","ddb9c115":"params_1 = ('Regularized model',{'colsample_bylevel': 0.7, 'learning_rate': 0.03, 'max_depth': 3, 'n_estimators': 400,\n                                 'reg_lambda': 15, 'subsample': 0.5})\nparams_2 = ('Best model', {'colsample_bylevel': 0.7, 'learning_rate': 0.03, 'max_depth': 4, 'n_estimators': 600, \n                           'reg_lambda': None, 'subsample': 0.7} )\n\nlearning_curve_plotter(XGBClassifier, X_train_fe, y_train, params_1, params_2, step=50)","353fb079":"model = XGBClassifier(**{'colsample_bylevel': 0.7, 'learning_rate': 0.03, 'max_depth': 3, 'n_estimators': 400,\n                                 'reg_lambda': 15, 'subsample': 0.5}).fit(X_train_fe, y_train)","8478065a":"y_preds = model.predict(X_test_fe)","d14de8d0":"submission = pd.DataFrame({'PassengerId':test.index, \n              'Survived':y_preds})","b53760a4":"submission.to_csv('submission.csv', index=False)","c0eb5a2b":"pd.read_csv('submission.csv')","48dcbbf9":"parameter_plot(RandomForestClassifier, X_train_fe, y_train, hyper_param=('max_depth', [3, 4, 5, 6]))","0351b6c2":"parameter_plot(RandomForestClassifier, X_train_fe, y_train, \n               hyper_param=(\"criterion\",['gini', 'entropy']), max_depth=3) ","4d57a8ea":"param_grid_RF = {'n_estimators':[250,400, 600],\n                  'max_depth':[3, 4],\n                  'criterion':['gini', 'entropy'],\n                 }","e6f9a9a3":"grid_RF = GridSearchCV(RandomForestClassifier(), param_grid_RF, \n                        cv=RepeatedStratifiedKFold(n_splits=6, n_repeats=2, random_state=42),\n                              scoring='accuracy', verbose=2, n_jobs=-1)","31533e2c":"grid_RF.fit(X_train_fe, y_train)","a28554f4":"y_preds = grid_RF.predict(X_test_fe)","eacf18b9":"submission_RF = pd.DataFrame({'PassengerId':test.index, \n              'Survived':y_preds})","e7766450":"submission_RF.to_csv('submissionRF.csv', index=False)","5d95098c":"pd.read_csv('submissionRF.csv')","f7d3ea46":"cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=3)\nmodel = TPOTClassifier(scoring='accuracy',generations=50, population_size=50, cv=cv,verbosity=2,n_jobs=-1)\nmodel.fit(X_train_fe, y_train)","a7c9143a":"y_preds = model.predict(X_test_fe)","05ea402d":"model.export('best_model_tpot.py')","3cf9b505":"submission_autoML = pd.DataFrame({'PassengerId':test.index, \n              'Survived':y_preds})","e37e77dc":"submission_autoML.to_csv('submissionAutoML.csv', index=False)","aa1dc2b7":"One of the features that can be derived from the Name feature is the Name length, the idea behind this is that people with higher status usually had longer names, we can use this fact to create a feature with name lengths.","84c65f11":"### AutoML Tpot","7dc3e786":"This is an important EDA step, to explore the distribution of values of feature with the Target, this EDA step provides us with good understanding of the features and helps us uncover the possibility of feature simplification, combinination etc.","ab1d718a":"### RandomForest","d92fedb7":"Now the next features with missing value is the Cabin feature, but the Cabin feature has a lot of missing value for us to arrive at an acceptable conclusion to impute missing values using correlation, and the feature has to many missing values for us to simply impute the missing values with the mean or median for the values of the feature.","7ea4b550":"From the Fare plot we can see that as the fare increases the survival likelihood also increases, we might be able to simplify the Fare feature into bins and capture this trend in a better way, it might or might not be helpful to do so, so we will have to cross check this decision by **grid searching this through a pipeline**!! \n\nAge feature also shows possibility of simplification as we can see that at different age brackets the ratio of survived likelihood to not survived likelihood is varying alot... Lets bin them and see all this in action.","725bdac1":"***This is the most important feature that almost all the top leaderboard submissions will have.***\n\n\nIntuition behind the Survival rate feature:\n\nIt was observed that usually people belonging to a family and Ticket group survived together or died together, this can help us assign survival rate for instances in test set which belonged to a family or ticket group in training set, this can help us make it easier for the model predict few more instances and boost the score... All this made more clear by the visualiations below!!\n","d13dd628":"From the above plot, max_depth of 3 - 4 looks good!!","edccefa6":"### Ticket based features","d50f614c":"Lets impute the missing values as per the above understandings...","0944596e":"Looks decent, what attracts my attention is the ticket A, PC, CA, Large_Serial_number and Small_Serial_number as they are quite a few of these tickets to be confident in the survival chances shown by these tickets. This feature might help us improve the probability of survival of few passengers and may push there probability of survival over 50% if they actually survived, to get classified as survived, which is actually the problem with many instances as they get classified as not survived, i.e probability of survival is lower than 50%.","11c817f1":"# Feature Engineering","79b17e17":"While tunning hyperparameters of Xgboost our main focus will be on best max depth and tree range, once we find that out we will go and look for the best learning rates, to do this we will be using the function parameter_plot()","8033d430":"### Survival Rate","da25dd5a":"You can see that we are only scaling the features which are not like the one hot encoded columns or in general which do not have zeros as most of the column entries, this is because if we were to pass the one hot encoded columns through the a StandardScaler or RobustScaler, all the zeros will get converted to some other value when scaled and the one hot encoding will be wrecked.. MinMaxScaler will retain it only if the min value is 0, We can understand this property of scaling transformers when we look at there functions, StandardScaler subtracts mean of all values by the value being scaled in numerator, but MinMaxScaler subtracts the min value by the value being scaled, hence if we are scaling 0 and 0 is the min value we retain 0, But that wont be the case for the StandardScaler.... So look out for this detail.","0510e539":"### Predictions","2a485002":"## Missing value imputation\n\n","26180f62":"Looks great!! the survival pattern is quite clear... but can we simplify and combine the family size 2-4 as 'small family' and family size of 1 as 'alone' to simplify the feature?","bd2ae652":"For the titanc dataset many have observed that binning the continuous features helped improve the results slightly, as you can see that binning can let you express the survival chances a little better and in a simpler way, So always test this out with other datasets you work with and test out whether it helps or not!!","cb99ec26":"# Hyper parameter Tuning","435e4ef2":"### Name Title","ce229308":"One way to impute values would be to just fill in the Cabins using the proportions as shown above. The missing values for Cabins in Pclass 1 as we see in the right plot, are less than all the other classes and these missing values will be imputed into the Cabins A,B,C,D,E only as they are the only Cabins where the Pclass 1 passengers are present, depending upon the already available Pclass 1 passenger propotions in these Cabins as shown left plot, But imputing values like this will create noise in already available information!! We might get a great result if the missing values got imputed correctly, But that will depend upon luck. So we should just impute the missing value as 'missing', atleast we retain the already available information free of noise.\n\nWe should also just simplify this feature and Combine Cabin A, B, C as deck ABC and D, E as DE and F, G as FG.","02d254de":"Ticket feature contains tickets with few of them with alphabets and some of them with only numbers, we could extract this relation but it might not be useful as it may be the case that this relation is related to the Embarked feature and depending upon where you Embarked you might have ticket code in only numbers or with some alphabets... rather we can also extract the frequency of tickets, there are people with same ticket numbers which shows that group of friends and family travelled together and with same ticket number.\n\nfirst lets check out if just dividing ticket into ticket with alphabets and only numeric serial code helps or not... Then we will perform even finer ticket categorization.","77359b40":"### Age - Sex distributions","b979d503":"### Pclass - Sex - Age distributions","76ab7072":"The above plot shows that Mrs., Miss. and Master. titles had better chances if survival, chances of survival is less than that for survival in case of Mr. and rare title, but it is worse for the Mr. title.","c5049ee6":"In the data provided to us, each individual has a SibSp and Parch, so if we add these features together for an individual and add 1 to it, 1 being the individual themself, we can create a new feature Family_size.\n\nIt is an important feature as it reveals that passengers with family size 2 - 4 had a better survival rate than passengers travelling alone or who had larger families.","c36c9e46":"### Family Size Feature","cf145991":"### Sex - Fare distributions","812d8e7e":"## Feature creation through EDA\n\n\n### Target Distribution","8f63e7d2":"So we only have missing values in Age, Cabin for both train and test, Embarked and Fare have missing values for train and test respectively, we can impute them simply using median or mean of the whole feature.","977edb7d":"# Data Understanding \n\n\nDuring this chapter, we\u2019ll be going through the data comprehension by describing its\ncontent explore it, in order to gain some insights through primary intuition","4fe4d995":"From the these set of params lets select the best params and the params we think are less prone to overfitting..\n\nIn Gradient boosting more learning rate, more number of trees, more subsample and colsample_bylevel or node or tree and no reg_lambda or alpha... causes overfitting.","79a4500e":"The feature looks more balanced now and we can see that now we can say that deck 'M' is the deck where most of the pclass 2 and 3 are present whereas deck ABC is where most pclass 1 are present. We can see from the plots that deck DE had the highest survival rate!!! and deck 'M' has the lowest survival rate. \n\nThe great thing about this simplification from 'Cabin' to 'Deck' is that we have cornered the Pclass 2 and 3 passengers which had the lowest survival rates in new deck M we created!! in all the other decks it seems that they had better chance of survival!! Compare the **Deck-Pclass Proportions** and the **Deck Survival rate**, you can clearly see that those few Pclass 2 and 3 passenger in deck DE and FG have good survival chance, most of the Pclass 2 and 3 passengers are in the Deck 'M' and their survival rate is not good!!\n\nThis feature helps to harness this pattern!!!","53f347ff":"**Now we are ready to ask questions!!!** Lets create FeatureEngineering class and use it in a pipeline with Logistic Regression(we can use other estimators too, but i prefer to use a simple one to keep computational expense low and search fast), we will pass the questions we wish to ask as the parameter to this class in the pipeline and grid search the whole feature space to narrow down our search for the best dataset!!\n\n* Is binned Fare feature better or the continuous one?\n* is Binned Age feature better of the continuous one?\n* Should we keep the SibSp and Parch rather than the Combined Family size feature?\n* If we should keep the Family_size feature, should we simplify it?\n* if we simplify the Family_size feature, should we one hot encode it or Target encode it?\n* What features should we drop?\n* Should we use StandardScaler, MinMaxScaler or RobustScaler?\n\nBefore we get to the above Q\/A setion, lets make few transformations i have noted below.\n* Pclass and Sex to one hot encodings\n* Drop Name and Ticket","59cf6394":"### Dataset creation","7c8b04c4":"As seen from above, no general trend is visible by categorizing ticket with or without alphabets. There are few points to note though: \n* Mostly numeric tickets were sold at the 'Q' which is Queenstown.\n* Proportion of ticket with alphabets bought is less for Pclass 3 as seen from 'Pclass V\/s Ticket type plot'.\n\n\nTicket type does not show any patterns in the Fare V\/s Ticket type plot..... it was kind of disappointing :(\n\nOverall there is no pattern in Ticket type V\/s Fare, Deck, Pclass.... This feature does not look that good.\n\n\nFinally, it may become useful... again.. you already know by now :D...we will have to test it out in Dataset creation section using grid search.\n\n\nNow lets extract even finer ticket types, maybe model finds some use out of it... we should check it out!!","8bf33d25":"**Findings:**\n* We can clearly see from the above plot that the survival chance for people with ticket frequency from 2-4 was more.\n* We were also able to uncover new information, it seems that higher frequency mean higher ticket fare which is actually the cummulative ticket price for all passenger, which means that Fare column does not contain the fare for an individual person but the total fare for a group with same ticket!! \n* we might be able to derive a new feature which is fare_per_individual, lets check it out if it is good or not!!","7e736203":"### EDA conclusion","89b3af27":"**Findings :**\n* From the above plot we can see that the probability distribution function did not show much difference, for the fare_per_individual it just got squeezed to the left and the corresponding values in the y axis were scaled equally...\n* But when we tried out making fare_per_individual category, we can see that fare_per_individual category gave us a better steady increase in survival rate from left to righ!!! \n\nUntil now we have Pointed out a lot of possibilities for feature engineering and there so many tough decisions to make as per what to keep and what to drop, we only want the best, informative features to achieve best results, But there is no shortcut to this, we cannot just simply know what works best with the given data and our models.... We will have to test it all out!! Though the Feature Engineering class will help us to quickly narrow down our search for the best data sets by quickly dropping out dataset made by selecting features which are not good. ","d8b76cf4":"from the second plot we can see that (0.0, 5.0] is a great age bracket for survival, whereas (24.0, 34.0] is the worst, for the (30.0, 36.0] category chances seems to 50-50, i have also plotted a qcut plot with 15 quantiles, just to show that how i visualized the age distributions in the second plot, for the second plot i wanted to have categories with some significant info gain, the above plot shows some good survival rate distribution for the categories we have selected after we went through the first plot.... Just a small trick to bin features to get better info gain for beginners to keep in mind.. \n\n**After testing out**:\n\nThe Age_cat feature improved the results for XGBClassifier and the GradientBoostingCLassifier significantly, observed 1% improvement from continuous age feature to Age_cat!!!","bce8f694":"### XGBoost","ff3ad5fc":"***Missing value imputation***\n* imputing Age feature\n* imputing Embarked feature\n\nWe were able to find better ways to impute the Age and Embarked feature with reasonable support from the present data explored via Visualizations, you should too explore the data through EDA and find out best ways to impute data rather straight away using sklearn.impute or .fillna()\n\n\n***Feature creation through EDA***\n\nwe were able to explore feature simplification, combination possibilities and possibilities to derive new features with proper support for these decisions from the present data...\n * Bin Fare feature to simplify and express patterns\n * Bin Age feature to simplify and express patterns\n * Better way to extract information from Ticket feature and creation of Ticket_frequency feature and Ticket_extracted feature\n * through the Ticket frequency feature we were able to gain information regarding Fare_per_individual feature and possibility of binning it.\n * Family size feature and possibility of simplification\n * Name length feature\n * Survival rate feature\n \nuntil now we have created few features and explored possibilities for simplification, we have many possible ways to create the final dataset. There are a lot of decision we will have to make now, some of them are as follows:\n* Is binned Fare feature better or the continuous one?\n* is Binned Age feature better of the continuous one?\n* Should we keep the SibSp and Parch rather than the Combined Family size feature?\n* If we should keep the Family_size feature, should we simplify it?\n* if we simplify the Family_size feature, should we one hot encode it or Target encode it?\n* What features should we drop?\n* Should we use StandardScaler, MinMaxScaler or RobustScaler?\n\n\nThese are some of good questions that come to my mind... There can be many more, but lets keep things simple or the class will become difficult to debug XD\n \n \n \n","a01d6f33":"# EDA : Exploratory Data Analysis","6117aee4":"### Binning the Fare feature","6d359827":"Everything looks great, there are trends which all the categories in categorical feature follow with target. We can observe that it will be beneficial to combine the SibSp and Parch into a common feature named Family_size and further simplification into category such as 'Alone', 'Small_family', 'large_family' can help us capture the trend better, which is that for small family the survival rate is better, a person is more likely to survive if they had 2 to 4 Family size(Parch + SibSp)!!","d9e7c698":"From the above plot, max_depth of 3 looks good!!","17ba141f":"Great!! we can see that with increase in the name length the survival rate increases!! from about name_length 26 more pople with name length more than 26 survived than not survived.","fd875311":"### Binning age feature","06d73e27":"we passed in the max depth of 3 as we found in the last plot to get a better estimate of learning rate as per max depth 3, from the above plots lower learning rates look good!! you can also see that as number of trees increase the accuracy goes down.","fd37c6cc":"We can see that the median age for each gender is slightly different and there is a general trend of decrease in median age for both the genders from Pclass 1 to 3.","8911c2f4":"**Findings:**\n* Looking at the above results, model loved the family size feature and specially when binned!!\n* bin fare feature was also liked, though it gets a little confused between binning and not.\n* it likes the Name length too.\n* Ticket_frequency was liked too.\n* I also let the model decide whether it wishes to work with all the features or we should drop the feature we appended in the self.drop_list, it did not like not dropping them.. which was what we expected.\n* it is a little confused between RobustScaler and StandardScaler, lets test out both..\n\nso finally, we will create dataset using following features...\n\n* bin_fare = [True, False] -- finally we pick **False**\n* family_size = True \n* bin_family_size = True\n* drop_Name_length = False \n* drop_Ticket_frequency = True\n* drop_all = True  \n* scaling = ['StandardScaler', 'RobustScaler'] -- finally we pick **StandardScaler**","f497324f":"Yup!! looks great, this is the trend we observed in the plot in last section. Binning helped us express this relation better or we can say that we are able it visualize this feature better as humans, but maybe the continuous form is better as per the algorithm... But atleast we know what we wish to check out by asking this question to our model that what it likes and what it does not(*More on this in Data set creation section*)!! \n\nIf we wish to use the feature as above, we will have to perform Ordinal encoding, where we encode the higher fare category with higher numeric value, for eg:- (-0.001, 7.55] -> 1, (7.55, 7.854] -> 2, (7.854, 8.05] -> 3 and so on. \n\n**Warning**:-\n\nWe can use LabelEncoder from scikit learn api to perform ordinal encoding only because there is order already present in our category which can be correctly understood by the LabelEncoder. But if the order present cannot be understood by the LabelEncoder for eg: Freezing, Cold, Warm, Hot, Lava Hot will be encoded as 2, 1, 5, 3, 4 on the basis of alphabetical order, which is not the correct order for this ordinal feature, it should be 1, 2, 3, 4, 5!! \n\n**Always beware of this trap!!!**","f7567896":"Pclass correlates best with the age feature, so we should use the pclass to impute the missing age values!! now lets check if there are any relationship of age with respect to pclass and sex combined","47013174":"What is of importance to us is the left plot where with certain confidence we have assigned the survival rates to the instances and we have about 175 instances for which we have some certainity of final outcome.","4303d128":"Well, things are crystal clear now!! those passengers which were assigned survival rate 0 because the median survival of the group was 0 actually only very few of them survived!! same the case when we see the passengers with median family survival rate 1, majority survived!!\n\n\n***Now how does this feature help us get better results??***\n\nFirst we have to understand that getting an accuracy score of 80% and getting to top 4% of the leaderboard is possible just by tunning model hyper parameters well and performing good feature encoding practices, but moving up from there is very difficult unless you have feature engineered very good features such as the one we are creating right now... The difference of 80% to 84% accuracy is just of about 2-4 more correct survival predictions. i got up to 79 % with just stacking and good encoding practice, [check it out](https:\/\/www.kaggle.com\/awwalmalhi\/comprehensive-beginners-guide-to-top-6)\n\nLets say we have few family members or ticket group members which were not in training set but in test set, we can easily, with confidence assign a survival rate to them in the test set depending upon the above results and turns out that ususally people are able to push from 75% to 81-83% with this feature!! just by predicting few more stubborn instances as 1!!","6e112c48":"# Titanic ","cba62980":"Yup, things look simpler and neat!! But again, it looks good to us but model might not find it as helpful as we think, maybe it could have found patterns with those family size numbers and some other feature, which it would not be able to harness as easily now, you never know... what we can be sure about is that we have made the feature simpler, if we one hot encode it we will have fewer dimensions, which is a good thing. Lets say we had many features which had to be one hot encoded but there cardinality(number of different categories in categorical feature) was high, label encoding was not an option and if we one hot encoded the feature we would be struck by curse of dimensionality, in such case if all the categorical features could be simplified in such a easy, beautiful way, nothing like that!!\n\nBut we do not have that problem at present, we have a small dataset and not aot of features with high cardinality, so we will have to figure out whether this feature helps or not. ","c0e8e9a7":"### Name Length Feature"}}