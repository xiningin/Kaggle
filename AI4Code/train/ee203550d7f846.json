{"cell_type":{"4c0f1971":"code","d0a674fb":"code","e48dcddc":"code","6be00656":"code","7744dde2":"code","384a4fc5":"code","ac77d146":"code","56434cec":"code","9634b98f":"code","9ebf4215":"code","5506c706":"code","0415ed6a":"code","8d5d6c5e":"code","98ea3953":"code","8bef455e":"code","5150727e":"code","bfda64f0":"code","d845a2cc":"code","72c51d24":"code","50fb5c4d":"code","c4e657a8":"code","5f31314b":"code","9c864b5f":"code","a788a07b":"code","4110e6a3":"code","093f89a9":"code","eb04c038":"code","4b18ca17":"code","11d80e5b":"code","a2d8d145":"code","a3d98333":"code","079d7c12":"code","c215dabd":"code","ce65b932":"code","f5b5eba9":"code","1ebd7b4d":"code","775e15af":"code","3fc33543":"code","893ed622":"code","2356b112":"code","6c51c10b":"code","3dc97c3b":"code","ae7ff57b":"code","b3d9dfd7":"code","144b5308":"code","23f135c0":"code","443f17de":"markdown","09d8dd55":"markdown","e8916494":"markdown","bdfb9d22":"markdown","721aa426":"markdown","6a68b6f3":"markdown","afe7244b":"markdown","a27e7ab0":"markdown"},"source":{"4c0f1971":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport csv","d0a674fb":"df = pd.read_csv(\"\/kaggle\/input\/ibm-hr-analytics-attrition-dataset\/WA_Fn-UseC_-HR-Employee-Attrition.csv\")\n\n#print the rows of data\n\ndf.head(8)","e48dcddc":"df.info()","6be00656":"#view some statistics\ndf.describe()","7744dde2":"#get the number of rows and columns\ndf.shape\n#1470 employees (rows) and 35 data points (columns)","384a4fc5":"#Get the column data types\ndf.dtypes","ac77d146":"#Get the count of the empty values for each columns\ndf.isna().sum()","56434cec":"#Check for any missing\/null values in data\ndf.isnull().values.any()","9634b98f":"#A count of number of Employee Attrition\ndf['Attrition'].value_counts()\n#So we have still 1233 employees still at the company and 237 that left the company","9ebf4215":"#visualize the Attrition\nsns.countplot(df['Attrition'])","5506c706":"(1233 - 237) \/1233\n#If the model just guessed no all the time would get us accuracy would be around 80 percent","0415ed6a":"for column in df.columns:\n  print(f\"{column}: Number of unique values {df[column].nunique()}\")","8d5d6c5e":"df.drop(['EmployeeCount','EmployeeNumber','Over18','StandardHours'], axis=\"columns\", inplace=True)","98ea3953":"#Print all the datatypes and the unique vales\nfor column in df.columns:\n  if df[column].dtype == object:\n    print(str(column)+' : '+ str(df[column].unique()))\n    print (df[column].value_counts())\n    print('***********************************************************************************')","8bef455e":"#The correlation\ndf.corr()","5150727e":"disc_col = []\nfor column in df.columns:\n    if df[column].dtypes != object and df[column].nunique() < 30:\n        print(f\"{column} : {df[column].unique()}\")\n        disc_col.append(column)\n        print(\"**********************************************************************************************\")","bfda64f0":"cont_col = []\nfor column in df.columns:\n    if df[column].dtypes != object and df[column].nunique() > 30:\n        print(f\"{column} : Minimum: {df[column].min()}, Maximum: {df[column].max()}\")\n        cont_col.append(column)","d845a2cc":"#Transform non-numerical into numerical columns\nfrom sklearn.preprocessing import LabelEncoder\nfor column in df.columns:\n  if df[column].dtype == np.number:\n    continue\n  df[column] = LabelEncoder().fit_transform(df[column])","72c51d24":"df","50fb5c4d":"#Distance from home\nimport matplotlib.pyplot as plt\nplt.subplots(figsize=(14,8))\nsns.countplot(x='DistanceFromHome', hue='Attrition', data = df)","c4e657a8":"#Relationship Satisfaction\nimport matplotlib.pyplot as plt\nplt.subplots(figsize=(14,8))\nsns.countplot(x='RelationshipSatisfaction', hue='Attrition', data = df)","5f31314b":"#Number of employees that left and stayed by age \nimport matplotlib.pyplot as plt\nplt.subplots(figsize=(14,8))\nsns.countplot(x='Age', hue='Attrition', data = df)","9c864b5f":"#Does Job level affects it?\nimport matplotlib.pyplot as plt\nplt.subplots(figsize=(14,8))\nsns.countplot(x='JobLevel', hue='Attrition', data = df)","a788a07b":"#Percentage Salary Hike\nimport matplotlib.pyplot as plt\nplt.subplots(figsize=(14,8))\nsns.countplot(x='PercentSalaryHike', hue='Attrition', data = df)","4110e6a3":"#Stock Options\nimport matplotlib.pyplot as plt\nplt.subplots(figsize=(14,8))\nsns.countplot(x='StockOptionLevel', hue='Attrition', data = df)","093f89a9":"#Years at the company\nimport matplotlib.pyplot as plt\nplt.subplots(figsize=(14,8))\nsns.countplot(x='YearsAtCompany', hue='Attrition', data = df)","eb04c038":"#Total working years\nimport matplotlib.pyplot as plt\nplt.subplots(figsize=(14,8))\nsns.countplot(x='TotalWorkingYears', hue='Attrition', data = df)","4b18ca17":"#Male vs Female\nimport matplotlib.pyplot as plt\nplt.subplots(figsize=(14,8))\nsns.countplot(x='Gender', hue='Attrition', data = df)","11d80e5b":"#MaritalStatus \nimport matplotlib.pyplot as plt\nplt.subplots(figsize=(14,8))\nsns.countplot(x='MaritalStatus', hue='Attrition', data = df)","a2d8d145":"#Overtime\nimport matplotlib.pyplot as plt\nplt.subplots(figsize=(14,8))\nsns.countplot(x='OverTime', hue='Attrition', data = df)","a3d98333":"#Department\nimport matplotlib.pyplot as plt\nplt.subplots(figsize=(14,8))\nsns.countplot(x='Department', hue='Attrition', data = df)","079d7c12":"#Degree of study\nimport matplotlib.pyplot as plt\nplt.subplots(figsize=(14,8))\nsns.countplot(x='EducationField', hue='Attrition', data = df)","c215dabd":"#Graph the correlation\nplt.figure(figsize=(15,15))\nsns.heatmap(df.corr(), annot=True, fmt='.0%')","ce65b932":"#Graph the Attrition\ncol = df.corr().nlargest(20, \"Attrition\").Attrition.index\nplt.figure(figsize=(15, 15))\nsns.heatmap(df[col].corr(), annot=True, cmap=\"RdYlGn\", annot_kws={\"size\":10})","f5b5eba9":"#Transform data into dummies\ndummy_col = [column for column in df.drop('Attrition', axis=1).columns if df[column].nunique() < 20]\ndata = pd.get_dummies(df, columns=dummy_col, drop_first=True, dtype='uint8')\ndata.info()","1ebd7b4d":"print(data.shape)\n\n# Remove duplicate Features\ndata = data.T.drop_duplicates()\ndata = data.T\n\n# Remove Duplicate Rows\ndata.drop_duplicates(inplace=True)\n\nprint(data.shape)","775e15af":"data.drop('Attrition', axis=1).corrwith(data.Attrition).sort_values().plot(kind='barh', figsize=(10, 30))","3fc33543":"feature_correlation = data.drop('Attrition', axis=1).corrwith(data.Attrition).sort_values()\nmodel_col = feature_correlation[np.abs(feature_correlation) > 0.02].index\nlen(model_col)","893ed622":"from sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.preprocessing import StandardScaler\n\nX = data.drop('Attrition', axis=1)\ny = data.Attrition\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n\nscaler = StandardScaler()\nX_train_std = scaler.fit_transform(X_train)\nX_test_std = scaler.transform(X_test)\nX_std = scaler.transform(X)","2356b112":"def feature_imp(df, model):\n    fi = pd.DataFrame()\n    fi[\"feature\"] = df.columns\n    fi[\"importance\"] = model.feature_importances_\n    return fi.sort_values(by=\"importance\", ascending=False)","6c51c10b":"y_test.value_counts()[0] \/ y_test.shape[0]\n#83 percent that our employee will stay based on the imbalanced data.","3dc97c3b":"stay = (y_train.value_counts()[0] \/ y_train.shape)[0]\nleave = (y_train.value_counts()[1] \/ y_train.shape)[0]\n\nprint(\"**************TRAIN*************\")\nprint(f\"Staying Rate: {stay * 100:.2f}%\")\nprint(f\"Quitting Rate: {leave * 100 :.2f}%\")\n\nstay = (y_test.value_counts()[0] \/ y_test.shape)[0]\nleave = (y_test.value_counts()[1] \/ y_test.shape)[0]\n\nprint(\"****************TEST****************\")\nprint(f\"Staying Rate: {stay * 100:.2f}%\")\nprint(f\"Quitting Rate: {leave * 100 :.2f}%\")","ae7ff57b":"from sklearn.metrics import confusion_matrix, accuracy_score, classification_report, roc_auc_score\n\ndef evaluate(model, X_train, X_test, y_train, y_test):\n    y_test_pred = model.predict(X_test)\n    y_train_pred = model.predict(X_train)\n\n    print(\"Training Results: \\n\")\n    clf_report1 = pd.DataFrame(classification_report(y_train, y_train_pred, output_dict=True))\n    print(f\"Confusion Matrix:\\n\\n{confusion_matrix(y_train, y_train_pred)}\\n\")\n    print(f\"Accuracy:\\n\\n{accuracy_score(y_train, y_train_pred):.4f}\\n\")\n    print(f\"Classification:\\n\\n{clf_report1}\\n\")\n\n    print(\"Test Results: \\n\")\n    clf_report2 = pd.DataFrame(classification_report(y_test, y_test_pred, output_dict=True))\n    print(f\"Confusion Matrix:\\n\\n{confusion_matrix(y_test, y_test_pred)}\\n\")\n    print(f\"Accuracy:\\n\\n{accuracy_score(y_test, y_test_pred):.4f}\\n\")\n    print(f\"Classification:\\n\\n{clf_report2}\\n\")","b3d9dfd7":"#Logistic Regression\n\nfrom sklearn.linear_model import LogisticRegression\n\nlr_clf = LogisticRegression(solver='liblinear', penalty='l1')\nlr_clf.fit(X_train_std, y_train)\n\nevaluate(lr_clf, X_train_std, X_test_std, y_train, y_test)","144b5308":"#RandomForestClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n\nrf_clf = RandomForestClassifier(n_estimators=100, bootstrap=False,)\nrf_clf.fit(X_train, y_train)\nevaluate(rf_clf, X_train, X_test, y_train, y_test)","23f135c0":"param_grid = dict(\n    n_estimators= [100, 500, 900], \n    max_features= ['auto', 'sqrt'],\n    max_depth= [2, 3, 5, 10, 15, None], \n    min_samples_split= [2, 5, 10],\n    min_samples_leaf= [1, 2, 4], \n    bootstrap= [True, False])\n\nrf_clf = RandomForestClassifier(random_state=42)\nsearch = GridSearchCV(rf_clf, param_grid=param_grid, scoring='roc_auc', cv=5, verbose=1, n_jobs=-1)\nsearch.fit(X_train, y_train)\n\nrf_clf = RandomForestClassifier(**search.best_params_, random_state=42)\nrf_clf.fit(X_train, y_train)\nevaluate(rf_clf, X_train, X_test, y_train, y_test)","443f17de":"There are couple of columns that only have one unique value. So we will drop those: 'EmployeeCount', 'Over18', and 'StandardHours'\n\nThere a column called 'EmployeeNumber,' which is no help for us either. That column will be dropped also.","09d8dd55":"Divorced = 0 Married = 1 Divorced = 2","e8916494":"Male = 0 and Female = 1","bdfb9d22":"Human Resources = 0 Research and development = 1 Sales = 2","721aa426":"No = 0 Yes = 1","6a68b6f3":"Human Resources = 0 Life Sciences = 1 Marketing = 2 Medical = 3 Other = 4 Technical Degree = 5","afe7244b":"Conclusion:\n\n* The workers with low job level, monthly income, years at company, total\n* working years are the higher risk group.\n* Research and Development department has a high retention rate.\n* Human Resources and Technical Degree are more likely to quit\n* Males are more likely to quit.\n* Lab Techs, Sales, and Human Resources are more likely to quit.\n* Single people are more likely to quit.\n* People who take on Overtime are extremely likely to quit.","a27e7ab0":"The Break down of the analysis\n\n* Age is correlated with the total working hours\n* Monthly income correlates with the total working hours\n* Monthly income correlates with with job level\n* Job level correlates with total working hours"}}