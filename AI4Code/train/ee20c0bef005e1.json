{"cell_type":{"c3afce45":"code","9fb8c4ca":"code","fa3e1743":"code","e8753033":"code","ffea9a33":"code","d23cd072":"code","82e43fd5":"code","b91dd62e":"code","b9865c1f":"code","a1427579":"code","4627285b":"code","d0b02ba5":"code","45bac95b":"code","26a26d62":"code","11a7cc5d":"code","0c54b378":"code","4d91d1ab":"code","2629e87b":"code","56ae881e":"code","04c213af":"code","8f0bedf9":"code","947c8091":"code","03eb4967":"code","f69b4169":"code","9d502ca0":"code","e4a1456a":"code","d2e564e9":"code","840efb0b":"code","5d0fdc00":"code","2c55530a":"code","a59a4544":"code","c25bfef5":"code","74743401":"code","ffe8f2b9":"code","09abd5d4":"markdown","6d12487a":"markdown","be76dd99":"markdown","5f7d8faf":"markdown","24f2d830":"markdown","d6c0a77a":"markdown","5917e0eb":"markdown","8a00129e":"markdown","012530dd":"markdown","92cd45ec":"markdown","60fc4ea5":"markdown","f1d08ee2":"markdown","754ce0a1":"markdown","ec6d2693":"markdown","779fb346":"markdown","ad77667d":"markdown","b4af34b3":"markdown","a1fb03a9":"markdown"},"source":{"c3afce45":"import torch\nimport os\nimport re\n\nimport numpy as np\nfrom torch.utils.data import Dataset, DataLoader\nfrom PIL import Image\n\nimport torchvision\n","9fb8c4ca":"!unzip -qo ..\/input\/dogs-vs-cats-redux-kernels-edition\/test.zip\n!unzip -qo ..\/input\/dogs-vs-cats-redux-kernels-edition\/train.zip","fa3e1743":"!ls","e8753033":"import torchvision.transforms as transforms\n\nmy_transform = transforms.Compose([\n    transforms.Resize((224,224)),\n    transforms.ToTensor(),\n    transforms.Normalize(\n        mean=[0.485, 0.456, 0.406],\n        std=[0.229,0.224,0.225],\n    ),\n   \n])","ffea9a33":"os.listdir(\".\/train\")[:5]","d23cd072":"img = Image.open(\".\/train\/cat.10366.jpg\")\nimg = my_transform(img)","82e43fd5":"class CatsDogsDataset(Dataset):\n    def __init__(self, rootdir, transform=my_transform, partial_data=None):\n        self.rootdir = rootdir\n        self.transform = transform\n        if partial_data is not None:\n            self.images = os.listdir(rootdir)[:partial_data]\n        else :\n            self.images = os.listdir(rootdir)\n        \n    def __len__(self):\n        return len(self.images)\n    \n    def __getitem__(self, index):\n        \n#         print(os.path.join(self.rootdir,self.images[index]))\n        img = Image.open(os.path.join(self.rootdir,self.images[index])).convert(\"RGB\")\n        \n#         image = np.array(img)\n        \n        if self.transform:\n            img = self.transform(img)\n        \n        if \"cat\" in self.images[index]:\n            label = 0\n            \n        elif \"dog\" in self.images[index]:\n            label = 1\n        else :\n            label = -1\n            \n        return img, label","b91dd62e":"train_dataset = CatsDogsDataset(rootdir=\".\/train\", partial_data=5000) \n# train_dataset = CatsDogsDataset(rootdir=\".\/train\")","b9865c1f":"train_dataset.__len__()","a1427579":"x, y = train_dataset[100]\nx.shape, y","4627285b":"import matplotlib.pyplot as plt\n\nplt.imshow(x.permute(1,2,0))","d0b02ba5":"batch_size = 20\nlearning_rate = 1e-4 #0.001\nnum_epochs = 1\nnum_workers = 4\npin_memory =True\ncheckpoint_file = \"checkpoint.tar\"\nweight_decay = 1e-4\n","45bac95b":"train_dataloader = DataLoader(dataset=train_dataset, shuffle=True, batch_size=batch_size)","26a26d62":"from torchvision.models import resnet18","11a7cc5d":"model = resnet18(pretrained=True)","0c54b378":"for param in model.parameters():\n    param.requires_grad = False","4d91d1ab":"import torch.nn as nn","2629e87b":"model.fc = nn.Linear(in_features=512, out_features=2, bias=True)","56ae881e":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","04c213af":"model = model.to(device=device)","8f0bedf9":"model","947c8091":"loss_criterion = torch.nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr= learning_rate)","03eb4967":"from tqdm import tqdm\n\nfor epoch in range(num_epochs):\n    for data, target in tqdm(train_dataloader):\n        data = data.to(device=device)\n        target = target.to(device=device)\n        \n        score = model(data)\n        \n        loss = loss_criterion(score, target)\n        optimizer.zero_grad()\n        \n        loss.backward()\n        \n        optimizer.step()\n    \n    print(f\"For epoch: {epoch} loss: {loss}\")\n        ","f69b4169":"def check_accuracy(model, loader):\n    model.train()\n    \n    total_correct = 0\n    total_predictions = 0\n    \n    for x, y in tqdm(loader):\n        x = x.to(device=device)\n        y = y.to(device=device)\n        \n        \n        score = model(x)\n        z, predictions = score.max(1)\n        \n#         print(f\"score shape : {score.shape} , z: {z}, predictions: {predictions}, predictions shape: {predictions.shape}\")\n        \n        total_correct = (y==predictions).sum()\n        total_predictions += predictions.shape[0]\n    \n    print(f\"Out of {total_predictions}, total correct: {total_correct} with an accuracy of {float(total_correct\/total_predictions)* 100}\")","9d502ca0":"check_accuracy(model, train_dataloader)","e4a1456a":"import pandas as pd\n\nsubmission = pd.read_csv('..\/input\/dogs-vs-cats-redux-kernels-edition\/sample_submission.csv')\nsubmission.head()","d2e564e9":"class testDataset(Dataset):\n    def __init__(self, rootdir, transform=my_transform, partial_data=None):\n        self.rootdir = rootdir\n        self.transform = transform\n        if partial_data is not None:\n            self.images = os.listdir(rootdir)[:partial_data]\n        else :\n            self.images = os.listdir(rootdir)\n        \n    def __len__(self):\n        return len(self.images)\n    \n    def __getitem__(self, index):\n        \n#         print(os.path.join(self.rootdir,self.images[index]))\n        img = Image.open(os.path.join(self.rootdir,self.images[index])).convert(\"RGB\")\n        \n#         image = np.array(img)\n        \n        if self.transform:\n            img = self.transform(img)\n        \n#         if \"cat\" in self.images[index]:\n#             label = 0\n            \n#         elif \"dog\" in self.images[index]:\n#             label = 1\n#         else :\n#             label = -1\n            \n        return img","840efb0b":"test_dataset = testDataset(\".\/test\")","5d0fdc00":"len(submission), test_dataset.__len__()","2c55530a":"test_dataloader = DataLoader(dataset=test_dataset, batch_size=1, shuffle=False)","a59a4544":"\n\npredictions = []\nfor x in tqdm(test_dataloader):\n    x = x.to(device=device)\n    \n    score = model(x)\n    _, prediction = score.max(1)\n    \n    predictions.append(prediction)\n    ","c25bfef5":"len(predictions)","74743401":"submission[\"label\"] = [int(x) for x in predictions]\nsubmission.head()","ffe8f2b9":"submission.to_csv(\"my_submission.csv\", index=False)","09abd5d4":"Okay now we create the dataset class. All dataset classes are supposed to have three functions, __init__ (initialiser) , __len__ (returns length of dataset) and __getitem__ (to get individual items), basically if you implement Dataset from pytorch with these three functions, you hava a pytorch Dataset.","6d12487a":"batch size decides how many of your data items are taken together in a single batch, while you apply the back propagation.","be76dd99":"lets create a transform. A transform does exactly as it sounds it transforms the image to a form, in this case since we want the images to be of uniform size we resize it, then convert it into a tensor followed by normalise. The values I copied from other kernel. Its basically something you do.","5f7d8faf":"Lets just check whether our transform works on individual images","24f2d830":"Since the dataset provided is zippped, lets unzip it first.","d6c0a77a":"https:\/\/www.youtube.com\/watch?v=2zptQGC-pHY&list=PLhhyoLH6IjfxkVb3Yx4mjLdwMWP5S_Hct&index=2","5917e0eb":"lets intiatiate our dataset class.","8a00129e":"### Hyperparameters","012530dd":"Lets get the dataloader","92cd45ec":"# Is data accessible ?","60fc4ea5":"# Okay. Now onto the model,","f1d08ee2":"That is bad performance , but at least it was quick, lets work on inference.\n\n# Inference!","754ce0a1":"# Prepping the data for the model","ec6d2693":"## The pytorch dataset and dataloader","779fb346":"Enumerating the directory we see that it has unzipped","ad77667d":"is it working?","b4af34b3":"Lets import some of the libraries immediately required.","a1fb03a9":"lets look at what is actually in the train directory, the file names have their labels in the name itself. here are 5 of the files in train directory"}}