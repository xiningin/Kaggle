{"cell_type":{"fae8c730":"code","a218b915":"code","ba7db9b8":"code","d152baad":"code","9bad1165":"code","b454dbbb":"code","473d83f7":"code","466f0a9c":"code","5cbf61e4":"code","dd1e3cf7":"code","17f00888":"code","403553e7":"code","685ee4f3":"code","196130bb":"code","bbd03ea1":"code","438c20a6":"code","c4ad771a":"code","0a98550f":"code","40ade2f1":"code","42c56e04":"code","92c9d8d2":"code","347934d3":"code","6d268dcc":"code","b291243a":"code","dc995c40":"code","5180660a":"markdown","6912b020":"markdown","ef9f6812":"markdown","02af3edc":"markdown","ea7e452a":"markdown","7ec7cda0":"markdown","f8759af9":"markdown","b34eedb6":"markdown","6735c793":"markdown","29be4e6a":"markdown","4b8471f7":"markdown","9ddac4b4":"markdown","18a9dde9":"markdown","83f7beda":"markdown","e8312427":"markdown","a24221cc":"markdown","1bf0298c":"markdown"},"source":{"fae8c730":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import StratifiedKFold, cross_val_score, learning_curve, train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import LabelEncoder,StandardScaler\nfrom sklearn.pipeline import Pipeline\nimport random\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nfrom IPython.display import Image","a218b915":"Image(filename='..\/input\/overfitting\/overfitting_1.png')","ba7db9b8":"#Data Import\ntrain = pd.read_csv(\"..\/input\/iris\/Iris.csv\")\ntrain.head()","d152baad":"#Shuffling the data since the labels are arranged in an order\nrandom.seed(11)\ntrain = train.iloc[random.sample(range(len(train)),len(train))].reset_index(drop=True)\ntrain","9bad1165":"train.shape #Dataset is small so using a complex model may lead to overfitting, so we considered LogisticRegressor as it's a pretty simple model","b454dbbb":"train.dtypes # All the data types look good ","473d83f7":"train.describe(include=\"all\")","466f0a9c":"np.round(train.isnull().mean(),2)*10","5cbf61e4":"r = c = 0\nfig,ax = plt.subplots(2,2,figsize=(14,14))\nfor n,i in enumerate(train.columns[1:-1]):\n    sns.boxplot(x=\"Species\",y=i,data=train,ax=ax[r,c])\n    ax[r,c].set_title(f\"Relationship Between {i} & Species\")\n    c += 1\n    if (n+1)%2==0:\n        c = 0\n        r += 1\nplt.show()","dd1e3cf7":"#Splitting the data into input and target features\nX = train.iloc[:,1:-1].copy()\ny = train.iloc[:,-1]","17f00888":"X# Input features","403553e7":"y #Target feature. Should LabelEncode this","685ee4f3":"y.value_counts() #Well balanced dataset","196130bb":"y.value_counts().plot(kind=\"bar\")\nplt.title(\"Class distribution\")\nplt.ylabel(\"Frequency\");","bbd03ea1":"sns.heatmap(X.corr(),annot=True,fmt=\"0.1f\",cbar=False,mask=np.triu(X.corr()));","438c20a6":"X_vif  = X.copy()\nvif =  [variance_inflation_factor(X_vif.values, i) for i in range(X_vif.shape[1])]\nprint(vif)","c4ad771a":"#removing first feature\nX_vif = X_vif.iloc[:,1:].copy()","0a98550f":"vif =  [variance_inflation_factor(X_vif.values, i) for i in range(X_vif.shape[1])]\nprint(vif)","40ade2f1":"#Removing 2nd feature\nX_vif = X_vif.iloc[:,[0,2]].copy()","42c56e04":"#No multicollinearity\nvif =  [variance_inflation_factor(X_vif.values, i) for i in range(X_vif.shape[1])]\nprint(vif)","92c9d8d2":"#The function below builds the model and returns cross validation scores, train score and learning curve data\ndef learn_curve(X,y,c):\n    ''' param X: Matrix of input features\n        param y: Vector of Target\/Label\n        c: Inverse Regularization variable to control overfitting (high value causes overfitting, low value causes underfitting)\n    '''\n    '''We aren't splitting the data into train and test because we will use StratifiedKFoldCV.\n       KFold CV is a preffered metho compared to hold out CV, since the model is tested on all the examples.\n       Hold out CV is preferred when the model takes too long to train and we have a huge test set that truly represents the universe\n    '''\n    \n    le = LabelEncoder() # Label encoding the target\n    sc = StandardScaler() # Scaling the input features\n    y = le.fit_transform(y)#Label Encoding the target\n    \n    log_reg = LogisticRegression(max_iter=200,random_state=11,C=c) # LogisticRegression model\n    \n    # Pipeline with scaling and classification as steps, must use a pipelne since we are using KFoldCV\n    lr = Pipeline(steps=(['scaler',sc],\n                        ['classifier',log_reg]))\n    \n    \n    cv = StratifiedKFold(n_splits=5,random_state=11,shuffle=True) # Creating a StratifiedKFold object with 5 folds\n    cv_scores = cross_val_score(lr,X,y,scoring=\"accuracy\",cv=cv) # Storing the CV scores (accuracy) of each fold\n    \n    \n    lr.fit(X,y) # Fitting the model\n\n    train_score = lr.score(X,y) # Scoring the model on train set\n    \n    #Building the learning curve\n    train_size,train_scores,test_scores = learning_curve(estimator=lr,X=X,y=y,cv=cv,scoring=\"accuracy\",random_state=11)\n    train_scores = 1-np.mean(train_scores,axis=1)#converting the accuracy score to misclassification rate\n    test_scores = 1-np.mean(test_scores,axis=1)#converting the accuracy score to misclassification rate\n    lc = pd.DataFrame({\"Training_size\":train_size,\"Training_loss\":train_scores,\"Validation_loss\":test_scores}).melt(id_vars=\"Training_size\")\n    return {\"cv_scores\":cv_scores,\n           \"train_score\":train_score,\n           \"learning_curve\":lc}","347934d3":"lc = learn_curve(X,y,1)\nprint(f'Cross Validation Accuracies:\\n{\"-\"*25}\\n{list(lc[\"cv_scores\"])}\\n\\n\\\nMean Cross Validation Accuracy:\\n{\"-\"*25}\\n{np.mean(lc[\"cv_scores\"])}\\n\\n\\\nStandard Deviation of Cross Validation Accuracy:\\n{\"-\"*25}\\n{np.std(lc[\"cv_scores\"])}\\n\\n\\\nTraining Accuracy:\\n{\"-\"*15}\\n{lc[\"train_score\"]}\\n\\n')\nsns.lineplot(data=lc[\"learning_curve\"],x=\"Training_size\",y=\"value\",hue=\"variable\")\nplt.title(\"Learning Curve of Good Fit Model\")\nplt.ylabel(\"Misclassification Rate\/Loss\");","6d268dcc":"lc = learn_curve(X,y,10000)\nprint(f'Cross Validation Accuracies:\\n{\"-\"*25}\\n{list(lc[\"cv_scores\"])}\\n\\n\\\nMean Cross Validation Accuracy:\\n{\"-\"*25}\\n{np.mean(lc[\"cv_scores\"])}\\n\\n\\\nStandard Deviation of Cross Validation Accuracy:\\n{\"-\"*25}\\n{np.std(lc[\"cv_scores\"])} (High Variance)\\n\\n\\\nTraining Accuracy:\\n{\"-\"*15}\\n{lc[\"train_score\"]}\\n\\n')\nsns.lineplot(data=lc[\"learning_curve\"],x=\"Training_size\",y=\"value\",hue=\"variable\")\nplt.title(\"Learning Curve of an Overfit Model\")\nplt.ylabel(\"Misclassification Rate\/Loss\");","b291243a":"lc = learn_curve(X,y,1\/10000)\nprint(f'Cross Validation Accuracies:\\n{\"-\"*25}\\n{list(lc[\"cv_scores\"])}\\n\\n\\\nMean Cross Validation Accuracy:\\n{\"-\"*25}\\n{np.mean(lc[\"cv_scores\"])}\\n\\n\\\nStandard Deviation of Cross Validation Accuracy:\\n{\"-\"*25}\\n{np.std(lc[\"cv_scores\"])} (Low variance)\\n\\n\\\nTraining Accuracy:\\n{\"-\"*15}\\n{lc[\"train_score\"]}\\n\\n')\nsns.lineplot(data=lc[\"learning_curve\"],x=\"Training_size\",y=\"value\",hue=\"variable\")\nplt.title(\"Learning Curve of an Underfit Model\")\nplt.ylabel(\"Misclassification Rate\/Loss\");","dc995c40":"fig,ax = plt.subplots(3,3,figsize=(15,26))\nr=c=0\nfor n,i in enumerate([0.0001,0.001,0.01,0.1,1,10,100,1000,10000]):\n    lc = learn_curve(X,y,i)\n    sns.lineplot(data=lc[\"learning_curve\"],x=\"Training_size\",y=\"value\",hue=\"variable\",ax=ax[r,c])\n    ax[r,c].set_title(f'C: {i:.4f}\\nTraining Accuracy: {lc[\"train_score\"]:.1f}\\nValidation Accuracy: {np.mean(lc[\"cv_scores\"]):.1f}')\n    ax[r,c].set_ylabel(\"loss\")\n    c+=1\n    if (n+1)%3==0:\n        c=0\n        r += 1\nplt.show()    ","5180660a":"<a id=\"gm\"><\/a><h2><u>Learning Curve of a Good Fit Model<\/u><\/h2>\n<p>Below we'll use the function created above to get a good fit model. We'll set the inverse regularization variable\/parameter 'c' to 1 (i.e. we are not performing any regularization)","6912b020":"<a id=\"lc\"><\/a><h2><u>Introduction to Learning Curve<\/u><\/h2>\n<p>Learning curve is a plot that plots the training and validation loss for a sample of training examples by incrementally increasing them. Learning curves helps us in identifying if adding additional training examples could improve the validation score or not. If a model is overfit then adding additional training examples might improve the model performance on unseen data. Similarly, if a model is underfit then adding training examples doesn't help","ef9f6812":"<a id=\"vif\"><\/a><h2>Identifying and handling Multicollinearity using VIF<\/h2>\n<h3>The correlation matrix below shows high multicollinearity among input features. VIF > 5 or 10 implies multicollinearity.","02af3edc":"<a id=\"om\"><\/a><h2><u>Learning Curve of an Overfit Model<\/u><\/h2>\n<p>Below we'll use the function created above to get an overfit model. We'll set the inverse regularization variable\/parameter 'c' to 10000 (high value of 'c' causes overfitting)","ea7e452a":"<h2>For this notebook, I'm ignoring multicollinearity as I'm unable to overfit the model (since we've reduced the number of input features). So, I'm considering all the input features for this problem.","7ec7cda0":"<a id=\"ref\"><\/a><h2>Overfitting and Underfitting Refresher<\/h2>\n\n<h3><u>Overfitting (aka Variance):<\/u><\/h3><p>A model is said to be overfit if the model is overtrained on the data such that it even learns the noise from the data. In the figure below, the third image shows overfitting where the model has learnt each and every example so perfectly that it misclassifies an unseen\/new example. For a model that's overfit we have a perfect\/close to perfect training set score while a poor test\/validation score.<\/p>\n\n<h4><u>Reasons for overfitting<\/u><\/h4>\n<ol><li>Using a complex model for a simple problem which picks up the noise from the data. Example: Using a neural network on Iris dataset<\/li>\n<li>Small datasets, as the training set may not be a right representation of the universe<\/li><\/ol>\n    \n<h3><u>Underfitting (aka Bias):<\/u><\/h3><p>A model is said to be underfit if the model is unable to learn the patterns in the data properly. In the figure below, the first image shows overfitting where the model hasn't fully learnt each and every example. In such cases we see a low score on both the training set and test\/validation set<\/p>\n\n<h4><u>Reasons for underfitting<\/u><\/h4>\n<ol><li>Using a simple model for a complex problem which doesn't pick up all the patterns from the data. Example: Using a logistic regression for image classification<\/li>\n<li>The underlying data has no inherent pattern. Example, trying to predict a student's marks with his father's weight.<\/li>","f8759af9":"<h4>The standard deviation in cross validation accuracy is high compared to underfit and good fit model. Training accuracy is higher than cross validation accuracy, typical to an overfit model, but not too high to detect overfitting. But Overfitting can be detected from the learning curve.<\/h4>\n<h3><u>Interpreting the training loss<\/u><\/h3>\n<h4>Learning curve of an overfit model has a very low training loss at the beginning which gradually increases very slightly upon adding training examples and doesn't flatten.<\/h4>\n<h3><u>Interpreting the validation loss<\/u><\/h3>\n<h4>Learning curve of an overfit model has a high validation loss at the beginning which gradually lowers upon adding training examples and doesn't flatten, indicating addition of more training examples can improve the model performance on unseen data<\/h4>\n\nWe can also see that the training and validation losses are far away from each other, which may come close to each other upon adding additional training data\n\n\n<h2><u>Typical Features<\/u><\/h2><h3><ul><li>Train and Validation loss far away from each other.<\/li>\n<li>Gradually decreasing validation loss (without flattening)<\/li>\n    <li>Very low training loss that's very slightly increasing","b34eedb6":"<h1>Effective Approach To Identify Overfitting & Underfitting<\/h1><hr>","6735c793":"Image Source: https:\/\/www.geeksforgeeks.org\/underfitting-and-overfitting-in-machine-learning\/","29be4e6a":"No Missing Values","4b8471f7":"<h4>The standard deviation in cross validation accuracy is low compared to overfit and good fit model. Underfitting can be detected from the learning curve.<\/h4>\n<h3><u>Interpreting the training loss<\/u><\/h3>\n<h4>Learning curve of an underfit model has a low training loss at the beginning which gradually increases upon adding training examples and suddenly falls to the minimum at the end.<\/h4>\n<h3><u>Interpreting the validation loss<\/u><\/h3>\n<h4>Learning curve of an underfit model has a high validation loss at the beginning which gradually lowers upon adding training examples and suddenly falls to the minimum at the end, indicating addition of more training examples can't improve the model performance on unseen data<\/h4>\n\nWe can also see that the training and validation losses are very close to each other at the end. Adding additional training examples doesn't improve the performance on unseen data.\n\n<h2><u>Typical Features<\/u><\/h2><h3><ul><li>Increasing Training Loss.<\/li>\n<li>Training and Validation Loss very close to each other at the end<\/li>\n<li>Sudden dip in the training and validation losses at the end","9ddac4b4":"<a id=\"model\"><\/a><h2>Modeling (Iris data) using Logistic Regression<\/h2>\n<p>In this section we'll fit a LogisticRegressor to the Iris dataset and identify underfitting, overfitting and good fit.","18a9dde9":"<h3>The above plot shows the learning curves for various values of 'C' along with training and validation accuracies.<br><br>\n    \nWe can see the model began to reduce underfitting at C=0.01 and was a good fit at C=1. Thereafter, it began overfitting from C=100.","83f7beda":"<h2>Table of Contents<\/h2><hr>\n<ol><li><h3><a href=\"#ref\">Overfitting and Underfitting Refresher<\/a><\/h3><\/li>\n<li><h3><a href=\"#model\">Modeling (Iris data) using Logistic Regression<\/a><\/h3><\/li>\n<li><h3><a href=\"#vif\">Identifying and handling Multicollinearity using VIF<\/a><\/h3><\/li>\n<li><h3><a href=\"#lc\">Introduction to Learning Curve<\/a><\/h3><\/li>\n<li><h3><a href=\"#gm\">Learning Curve of a Good Fit Model<\/a><\/h3><\/li>\n<li><h3><a href=\"#om\">Learning Curve of an Overfit Model<\/a><\/h3><\/li>\n<li><h3><a href=\"#um\">Learning Curve of an Underfit Model<\/a><\/h3><\/li>","e8312427":"<h3>There exists a strong relationship between target and input features","a24221cc":"<a id=\"um\"><\/a><h2><u>Learning Curve of an Underfit Model<\/u><\/h2>\n<p>Below we'll use the function created above to get an underfit model. We'll set the inverse regularization variable\/parameter 'c' to 1\/10000 (low value of 'c' causes underfitting)","1bf0298c":"<h4>The cross validation accuracy and training accuracy calculated above are close to each other<\/h4>\n<h3><u>Interpreting the training loss<\/u><\/h3>\n<h4>Learning curve of a good fit model has a moderately high training loss at the beginning which gradually lowers upon adding training examples and flattens gradually indicating addition of more training examples doesn't improve the model performance on training data<\/h4>\n<h3><u>Interpreting the validation loss<\/u><\/h3>\n<h4>Learning curve of a good fit model has a high validation loss at the beginning which gradually lowers upon adding training examples and flattens gradually indicating addition of more training examples doesn't improve the model performance on unseen data<\/h4>\n\nWe can also see that upon adding a reasonable number of training examples, both the training and validation loss are close to each other\n\n<h2><u>Typical Features<\/u><\/h2><h3><ul><li>Train and Validation loss close to each other with validation loss > training loss.<\/li>\n<li>Initially reducing training and validation loss and a pretty flat training and validation loss after some point"}}