{"cell_type":{"f225880c":"code","ad15c13b":"code","c8c8b81d":"code","aff172cd":"code","fe1858ef":"code","5288f4cd":"code","afa93d5b":"code","1e960222":"code","16d3a9a3":"code","1beef483":"code","8fd8eea9":"code","5483c8e9":"code","cc9fe173":"code","4a0ba093":"code","f898637b":"code","e9f4964e":"code","f709c416":"code","2b1fce2c":"code","ee3cf3e5":"code","9edcc298":"code","4331846f":"markdown","049a9d0f":"markdown","c828e557":"markdown","3dcfd1dd":"markdown","7bf345a4":"markdown","430028f6":"markdown","7075b8c4":"markdown","febf8dc3":"markdown","8de9fcaa":"markdown","1f800959":"markdown","41f85ae7":"markdown","b696e5f4":"markdown","a17a2756":"markdown","181e179f":"markdown","964cf66d":"markdown","7ec16771":"markdown","3eac0f33":"markdown"},"source":{"f225880c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","ad15c13b":"import sys\nimport pickle\nsys.path.append(\"..\/tools\/\")\n\n#from tester import dump_classifier_and_data\nimport matplotlib.pyplot as plt\nimport scipy\n%matplotlib inline\n\n\nfrom sklearn.feature_selection import SelectKBest, f_classif\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.svm import SVC\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.svm import SVC\nfrom sklearn.decomposition import PCA\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import GridSearchCV, StratifiedShuffleSplit","c8c8b81d":"def featureFormat( dictionary, features, remove_NaN=True, remove_all_zeroes=True, remove_any_zeroes=False, sort_keys = False):\n    return_list = []\n    if isinstance(sort_keys, str):\n        import pickle\n        keys = pickle.load(open(sort_keys, \"rb\"))\n    elif sort_keys:\n        keys = sorted(dictionary.keys())\n    else:\n        keys = dictionary.keys()\n\n    for key in keys:\n        tmp_list = []\n        for feature in features:\n            try:\n                dictionary[key][feature]\n            except KeyError:\n                print (\"error: key \", feature, \" not present\")\n                return\n            value = dictionary[key][feature]\n            if value==\"NaN\" and remove_NaN:\n                value = 0\n            tmp_list.append( float(value) )\n\n        # Logic for deciding whether or not to add the data point.\n        append = True\n        # exclude 'poi' class as criteria.\n        if features[0] == 'poi':\n            test_list = tmp_list[1:]\n        else:\n            test_list = tmp_list\n        ### if all features are zero and you want to remove\n        ### data points that are all zero, do that here\n        if remove_all_zeroes:\n            append = False\n            for item in test_list:\n                if item != 0 and item != \"NaN\":\n                    append = True\n                    break\n        ### if any features for a given data point are zero\n        ### and you want to remove data points with any zeroes,\n        ### handle that here\n        if remove_any_zeroes:\n            if 0 in test_list or \"NaN\" in test_list:\n                append = False\n        ### Append the data point if flagged for addition.\n        if append:\n            return_list.append( np.array(tmp_list) )\n\n    return np.array(return_list)","aff172cd":"def targetFeatureSplit( data ):\n    target = []\n    features = []\n    for item in data:\n        target.append( item[0] )\n        features.append( item[1:] )\n\n    return target, features","fe1858ef":"email_features_list=['from_messages',\n    'from_poi_to_this_person',\n    'from_this_person_to_poi',\n    'shared_receipt_with_poi',\n    'to_messages',\n    ]\n\nfinancial_features_list=['bonus',\n    'deferral_payments',\n    'deferred_income',\n    'director_fees',\n    'exercised_stock_options',\n    'expenses',\n    'loan_advances',\n    'long_term_incentive',\n    'other',\n    'restricted_stock',\n    'restricted_stock_deferred',\n    'salary',\n    'total_payments',\n    'total_stock_value',\n]\n\nfeatures_list = ['poi']+email_features_list + financial_features_list \n### Load the dictionary containing the dataset\nimport pickle\noriginal = \"\/kaggle\/input\/enron-person-of-interest-dataset\/final_project_dataset.pkl\"\ndestination = \"final_project_dataset_unix.pkl\"\n\ncontent = ''\noutsize = 0\nwith open(original, 'rb') as infile:\n    content = infile.read()\nwith open(destination, 'wb') as output:\n    for line in content.splitlines():\n        outsize += len(line) + 1\n        output.write(line + str.encode('\\n'))\n        \ndata_dict = pickle.load(open(\"final_project_dataset_unix.pkl\", \"rb\"))\ndata_dict","5288f4cd":"\nprint ('Exploratory Data Analysis')\ndata_dict.keys()\nprint ('Total number of data points= {0}'.format(len(data_dict.keys())))\n\ncount_poi=0\nfor name in data_dict.keys():\n    if data_dict[name]['poi']==True:\n        count_poi+=1\n\nprint ('Number of Persons of Interest: {0}'.format(count_poi))\nprint ('Number of Non-Person of Interest: {0}'.format(len(data_dict.keys())-count_poi))","afa93d5b":"all_features=data_dict['BAXTER JOHN C'].keys()\nprint ('Total Features everyone on the list has:', len(all_features))\n\nmissing={}\nfor feature in all_features:\n    missing[feature]=0\n\nfor person in data_dict:\n    records=0\n    for feature in all_features:\n        if data_dict[person][feature]=='NaN':\n            missing[feature]+=1\n        else:\n            records+=1\n\nprint ('Number of Missing Values for each Feature:')\nfor feature in all_features:\n    print (feature, missing[feature])","1e960222":"def PlotOutlier(data_dict, ax, ay):\n    data = featureFormat(data_dict, [ax,ay,'poi'])\n    for point in data:\n        x = point[0]\n        y = point[1]\n        poi=point[2]\n        if poi:\n            color='red'\n        else:\n            color='yellow'\n\n        plt.scatter( x, y, color=color )\n    plt.xlabel(ax)\n    plt.ylabel(ay)\n    plt.show()\nPlotOutlier(data_dict, 'from_poi_to_this_person','from_this_person_to_poi')\nPlotOutlier(data_dict, 'total_payments', 'total_stock_value')\nPlotOutlier(data_dict, 'from_messages','to_messages')\nPlotOutlier(data_dict, 'salary','bonus')","16d3a9a3":"def remove_outliers(data_dict, outliers):\n    for outlier in outliers:\n        data_dict.pop(outlier, 0)\noutliers =['TOTAL', 'THE TRAVEL AGENCY IN THE PARK', 'LOCKHARD EUGENE E']#latter two has almost all nan values\nremove_outliers(data_dict, outliers)\ndata_dict","1beef483":"my_dataset = data_dict\ndef computeFraction( poi_messages, all_messages ):\n    \"\"\" given a number messages to\/from POI (numerator) \n        and number of all messages to\/from a person (denominator),\n        return the fraction of messages to\/from that person\n        that are from\/to a POI\n   \"\"\"\n    fraction = 0.\n    if all_messages =='NaN':\n        return fraction\n    if poi_messages=='NaN':\n        return fraction\n        \n    fraction=float(poi_messages)\/float(all_messages)\n    return fraction\nsubmit_dict={}\nfor name in my_dataset:\n\n    data_point = my_dataset[name]\n    from_poi_to_this_person = data_point[\"from_poi_to_this_person\"]\n    to_messages = data_point[\"to_messages\"]\n    fraction_from_poi = computeFraction( from_poi_to_this_person, to_messages )\n    data_point[\"fraction_from_poi\"] = fraction_from_poi\n\n\n    from_this_person_to_poi = data_point[\"from_this_person_to_poi\"]\n    from_messages = data_point[\"from_messages\"]\n    fraction_to_poi = computeFraction( from_this_person_to_poi, from_messages )\n    submit_dict[name]={\"from_poi_to_this_person\":fraction_from_poi,\n                       \"from_this_person_to_poi\":fraction_to_poi}\n    data_point[\"fraction_to_poi\"] = fraction_to_poi","8fd8eea9":"my_feature_list=features_list+['fraction_from_poi','fraction_to_poi']\n\nfor x in range(len(my_feature_list)): \n    print (my_feature_list[x])","5483c8e9":"\ndef getkbest(data_dict, features_list, k):\n    data=featureFormat(my_dataset, features_list)\n    labels, features = targetFeatureSplit(data)\n    selection=SelectKBest(k=k).fit(features,labels)\n    scores=selection.scores_\n    unsorted_pairs = zip(features_list[1:], scores)\n    sorted_pairs=list(reversed(sorted(unsorted_pairs, key=lambda x: x[1])))\n    selection_best = dict(sorted_pairs[:k])\n    return selection_best\nnum=12 \nbest_features = getkbest(my_dataset, my_feature_list, num)\nprint ('Selected features and their scores: ', best_features)","cc9fe173":"l=best_features\ndef getList(dict): \n    list = [] \n    for key in dict.keys(): \n        list.append(key) \n          \n    return list\nl=getList(l)      \nmy_feature_list = ['poi'] + l\n#type(l)\nprint (\"{0} selected features: {1}\\n\".format(len(my_feature_list) - 1, my_feature_list[1:]))","4a0ba093":"data = featureFormat(my_dataset, features_list, sort_keys = True)\nlabels, features = targetFeatureSplit(data)\nfrom sklearn import preprocessing\nscaler=preprocessing.MinMaxScaler()\nfeatures=scaler.fit_transform(features)\n\n\nfrom sklearn.tree import DecisionTreeClassifier\n\nclf_d=Pipeline([\n    ('standardscaler',StandardScaler()),\n    ('pca',PCA()),\n    ('clf_d',DecisionTreeClassifier(criterion='gini', max_depth=6, min_samples_leaf=2, min_samples_split=7, splitter='best',random_state=42))])\n\nfrom sklearn.linear_model import LogisticRegression\n\nclf_p=Pipeline([\n    ('standardscaler', StandardScaler()),\n    ('classifier', LogisticRegression(penalty='l2', tol=0.01, C=0.0000001, random_state=42))])\n\nfrom sklearn.cluster import KMeans\nclf_k=Pipeline([\n    ('standardscaler',StandardScaler()),\n    ('pca',PCA()),\n    ('clf_k',KMeans(n_clusters=2, random_state=42, tol=0.000001))])\n\nfrom sklearn.svm import SVC\nclf_s=Pipeline([\n    ('standardscaler',StandardScaler()),\n    ('pca',PCA()),\n    ('clf_s',SVC(kernel='rbf',C = 100,random_state = 42))])\n\n\nfrom sklearn.naive_bayes import GaussianNB\nclf_g=Pipeline(steps=[\n    ('standardscaler',StandardScaler()),\n    ('pca',PCA()),\n    ('clf_g',GaussianNB())])\n\nfrom sklearn.ensemble import RandomForestClassifier\nclf_rf =Pipeline( [\n    ('standardscaler',StandardScaler()),\n    ('pca',PCA()),\n    ('clf_rf',RandomForestClassifier())])","f898637b":"from sklearn.model_selection import cross_validate\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, precision_score, recall_score, classification_report,accuracy_score\ndef evaluate(clf, features, labels, num=1):\n    print (clf)\n    accuracy=[]\n    precision=[]\n    recall=[]\n    for trial in range(num):\n        features_train, features_test, labels_train, labels_test=train_test_split(features, labels, test_size=0.3, random_state=42)\n        clf=clf.fit(features_train, labels_train)\n        pred=clf.predict(features_test)\n        accuracy.append(accuracy_score(labels_test,pred))\n        precision.append(precision_score(labels_test, pred))\n        recall.append(recall_score(labels_test, pred))\n    print ('precision: {}'.format(np.mean(precision)))\n    print ('recall: {}'.format(np.mean(recall)))\n    print ('accuracy: {}'.format(np.mean(accuracy)))\n    return np.mean(precision), np.mean(recall), confusion_matrix(labels_test, pred),classification_report(labels_test, pred)","e9f4964e":"print ('KMeans: ',evaluate(clf_k, features, labels))","f709c416":"print ('Linear Regression: ', evaluate(clf_p, features, labels))","2b1fce2c":"print ('Random Forest: ',evaluate(clf_rf, features, labels))","ee3cf3e5":"print ('Decision Tree: ', evaluate(clf_d, features, labels))","9edcc298":"print ('SVC: ', evaluate(clf_s, features, labels))","4331846f":"# ***CREATING THE DATA DICTIONARY***","049a9d0f":"# ***PLOTING THE VARIOUS OUTLIERS***","c828e557":"# ***DISPLAYING THE VARIOUS FEATURES***","3dcfd1dd":"***PRINTING ACCURACY, PRECISION AND RECALL FOR LINEAR REGRESSION***","7bf345a4":"# ***REMOVING THE OUTLIERS FROM THE DATA DICTIONARY***","430028f6":"# ***DEFINING THE featureFormat FUNCTION***","7075b8c4":"# ***EXTRACTING OUT THE K-BEST FEATURES***","febf8dc3":"# ***DEFINING THE FUNCTION TO SEPARATE THE TARGET VALUES FROM THE FEATURES***","8de9fcaa":"# ***EXPLORING THE DATASET***","1f800959":"***PRINTING ACCURACY, PRECISION AND RECALL FOR RANDOM FOREST REGRESSION***","41f85ae7":"***PRINTING ACCURACY, PRECISION AND RECALL FOR DECISION TREE ALGO***","b696e5f4":"***PRINTING ACCURACY, PRECISON AND RECALL FOR SUPPORT VECTOR CALSSIFIER***","a17a2756":"***PRINTING ACCURACY, PRECISION AND RECALL FOR K-MEANS ALGO***","181e179f":"# ***PRINTING THE ACCURACY USING THE DIFFREENT ALOGOS***","964cf66d":"# ***IMPORTING ALL THE NECESSARY LIBRARIES***","7ec16771":"# ***DEFINING THE PIPELINES FOR DIFFERENT ALGOS***","3eac0f33":"# ***DEFINING THE EVALUATION FUNCTION***"}}