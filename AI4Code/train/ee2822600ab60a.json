{"cell_type":{"08d4d4fd":"code","88a73ef4":"code","93a3c12d":"code","3d01d0e0":"code","e1bdc29e":"code","0f8fa1d2":"code","4266b17e":"code","a1c473b5":"code","0cb24a04":"code","a37de7f6":"code","1e9271cd":"code","bee9b5f2":"code","d5787b38":"code","73993118":"code","6df2412f":"code","ced76eeb":"code","8bc7e9dc":"code","8c0027dc":"code","0de7273a":"code","5d4d1430":"code","880f70c3":"code","c80b440f":"code","5fb73d72":"code","b3c622e0":"code","9e0b6f4a":"code","333e3c2e":"code","511e51bb":"code","ee4a7451":"code","90a35221":"markdown","a0547e7a":"markdown","51d642f4":"markdown","21b714a0":"markdown","231db69a":"markdown","91bea783":"markdown","986a81e2":"markdown"},"source":{"08d4d4fd":"import re\nimport random\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom pathlib import Path","88a73ef4":"path = Path(\"\/kaggle\/input\/bms-molecular-translation\/train_labels.csv\")\ntrain_labels = pd.read_csv(path)","93a3c12d":"inchis = train_labels[\"InChI\"].tolist()","3d01d0e0":"# Firstly, lets take a subset of the full dataset to speed things up a bit\n\nrandom.seed(1)\nnum_samples = 500000\nrand_inchis = random.sample(inchis, k=num_samples)","e1bdc29e":"# Check lengths using naive (single character) tokenisation\n\nnaive_inchi_lens = [len(inchi) for inchi in rand_inchis]\n\nprint(f\"Max length: {max(naive_inchi_lens)}\")\nprint(f\"Min length: {min(naive_inchi_lens)}\")\nprint(f\"Avg length: {sum(naive_inchi_lens) \/ len(naive_inchi_lens)}\")","0f8fa1d2":"plt.hist(naive_inchi_lens, bins=100)\nplt.show()","4266b17e":"regex = \"InChI=1S|\/|[0-9]|-|\\+|,|\\(|\\)|[A-Z][a-z]|.\"\nprog = re.compile(regex)","a1c473b5":"tokens = [prog.findall(inchi) for inchi in rand_inchis]","0cb24a04":"# Have a look at some examples :)\n\nfor i, ts in enumerate(tokens[:10]):\n    print(rand_inchis[i])\n    print(ts)\n    print()","a37de7f6":"regex_inchi_lens = [len(ts) for ts in tokens]\n\nprint(f\"Max length: {max(regex_inchi_lens)}\")\nprint(f\"Min length: {min(regex_inchi_lens)}\")\nprint(f\"Avg length: {sum(regex_inchi_lens) \/ len(regex_inchi_lens)}\")","1e9271cd":"plt.hist(regex_inchi_lens, bins=100)\nplt.show()","bee9b5f2":"regex = \"InChI=1S|\/|[0-9]{3}|[0-9]{2}|[0-9]|-|\\+|,|\\(|\\)|[A-Z][a-z]|.\"\nprog = re.compile(regex)","d5787b38":"tokens = [prog.findall(inchi) for inchi in rand_inchis]","73993118":"for i, ts in enumerate(tokens[:10]):\n    print(rand_inchis[i])\n    print(ts)\n    print()","6df2412f":"regex_inchi_lens = [len(ts) for ts in tokens]\n\nprint(f\"Max length: {max(regex_inchi_lens)}\")\nprint(f\"Min length: {min(regex_inchi_lens)}\")\nprint(f\"Avg length: {sum(regex_inchi_lens) \/ len(regex_inchi_lens)}\")","ced76eeb":"plt.hist(naive_inchi_lens, bins=100)\nplt.hist(regex_inchi_lens, bins=100)\nplt.show()","8bc7e9dc":"!conda install -y rdkit -c rdkit","8c0027dc":"from rdkit import Chem","0de7273a":"# Lets take a few examples first...\n\nsmall_sample_inchis = random.sample(rand_inchis, k=5)\nmols = [Chem.rdinchi.InchiToMol(inchi)[0] for inchi in small_sample_inchis]\nsmiles = [Chem.MolToSmiles(mol) for mol in mols]","5d4d1430":"for inchi, smi in zip(rand_inchis, smiles):\n    print(inchi)\n    print(smi)\n    print()","880f70c3":"def process_inchi(inchi):\n    mol = Chem.rdinchi.InchiToMol(inchi)[0]\n    smi = Chem.MolToSmiles(mol)\n    return smi","c80b440f":"# This will take a few minutes...\n\nsmiles = [process_inchi(inchi) for inchi in rand_inchis]","5fb73d72":"# Make sure everything worked as expected\n\ninvalids = [smi is None or smi == \"\" for smi in smiles]\nprint(f\"Number of invalid mols: {sum(invalids)}\")","b3c622e0":"# Now we need a new tokeniser for SMILES\n# Here's one possible tokenisation scheme\n\nsmi_regex = \"\\[|\\]|Br|Cl|\\(|\\)|\\.|=|#|-|\\+|\\\\\\\\|\\\/|:|~|@|\\?|>|\\*|\\$|[0-9]{2}|[0-9]|.\"\nsmi_prog = re.compile(smi_regex)","9e0b6f4a":"smi_tokens = [smi_prog.findall(smi) for smi in smiles]","333e3c2e":"for smi, ts in zip(smiles[:10], smi_tokens[:10]):\n    print(smi)\n    print(ts)\n    print()","511e51bb":"smiles_lens = [len(ts) for ts in smi_tokens]\n\nprint(f\"Max length: {max(smiles_lens)}\")\nprint(f\"Min length: {min(smiles_lens)}\")\nprint(f\"Avg length: {sum(smiles_lens) \/ len(smiles_lens)}\")","ee4a7451":"plt.hist(naive_inchi_lens, bins=100)\nplt.hist(regex_inchi_lens, bins=100)\nplt.hist(smiles_lens, bins=100)\nplt.show()","90a35221":"We can see a much bigger improvement here, crucially the longest sequences is now only 273 tokens (vs the 386 we had before). But perhaps we can do even better using other molecular notations...","a0547e7a":"### Tokenising SMILES\n\nSMILES (**S**implified **M**olecular-**I**nput **L**ine-**E**ntry **S**ystem)[1] is another commonly used molecular representation, which can be easily constructed from a molecule's InChI string. Similarly, it is straightforward to convert a SMILES to InChI. Unlike InChI strings, however, a molecule can have multiple valid SMILES representations. However, we may be able to use SMILES strings to significantly reduce the number of tokens in a molecular representation.\n\n[1] Weininger, David. \"SMILES, a chemical language and information system. 1. Introduction to methodology and encoding rules.\" Journal of chemical information and computer sciences 28.1 (1988): 31-36.","51d642f4":"InChIs can be very long strings since each molecular structure is required to have a unique InChI. As you can see above, a naive approach to tokenising the InChIs will lead to some very long sequences. Below we show how we can use knowledge of the InChI standard to produce shorter sequences.\n\nFirstly, a regex which splits off the start of the InChI string as well as atoms with two characters.","21b714a0":"We can see a small improvement in sequence length, but most of this probably comes from shortening the first 8 characters into one token. Perhaps we can merge 2 and 3 digit numbers together into a single token (with the drawback of having a larger vocabulary)...","231db69a":"# InChI Tokenisation\n\nHere we explore a couple of tokenisation strategies for InChI strings for the BMS Molecular Translation challenge in an attempt to keep the sequences as short as possible while retaining the molecular information.\n\nShortened sequences will not only speed up training, they reduce memory usage (important for transformer models since memory use scales quadratically with sequence length). Additionally, shorter sequences may also improve model accuracy since the dependcies between tokens are, in general, shorter. Shorter sequences, will however often lead to larger tokeniser vocabularies.","91bea783":"Using SMILES leads to a very significant reduction in sequence length from the original InChI strings and converting between the representations is straightforward. However, SMILES strings come with the disadvantage that each molecule can be represented in many different ways.","986a81e2":"First, we need to employ the help of RDKit!"}}