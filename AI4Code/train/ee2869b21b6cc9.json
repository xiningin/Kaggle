{"cell_type":{"06b4a877":"code","80de8c76":"code","195e9624":"code","f97f83c8":"code","b42f8fa5":"code","764f2dc1":"code","f84a1994":"code","e46a5bab":"code","48c881d7":"code","b99fd864":"code","b978e25b":"code","fae1e697":"code","bd520f48":"code","b0523526":"code","8613e5b0":"code","a7831af0":"code","793cbd2a":"code","3c8525f2":"code","6ef7f28f":"code","229315bc":"code","98c86e50":"code","34392c54":"code","1bad0669":"code","8bae3bc2":"code","9544a89b":"code","505c7d07":"code","c00c6c29":"code","e4d084fe":"code","a1e07254":"code","bf2a4c63":"code","90ab6124":"code","ddcbebca":"code","d50acb23":"code","4bc5b461":"code","dc66665c":"code","c4febceb":"code","c5903014":"code","544c82a8":"code","56c751fb":"code","a4c0ab41":"markdown","47ca0e9e":"markdown","5a8a4283":"markdown","76699420":"markdown","f14d03ac":"markdown","dd2d17f9":"markdown","cae9ce88":"markdown","54be7d2b":"markdown","6a3c473f":"markdown","bd2b26f3":"markdown","848fa3a9":"markdown","d7348e4e":"markdown","96ddacc6":"markdown","4d5d6abd":"markdown","57963383":"markdown","39111e20":"markdown","ee44f37a":"markdown","8049b813":"markdown","b7e4b2c5":"markdown","e7f54dbc":"markdown","044e76cf":"markdown","72e6ba95":"markdown"},"source":{"06b4a877":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","80de8c76":"traindf = pd.read_csv(\"..\/input\/train.csv\")\ntraindf.tail(10)","195e9624":"\ntraindf.isnull().sum()","f97f83c8":"traindf['Age'].describe()\nfrom sklearn.impute import SimpleImputer\nimp = SimpleImputer(missing_values=np.nan, strategy='mean')\ntraindf['Age'] = imp.fit_transform(traindf[['Age']])\n\ntraindf['Age'].describe()","b42f8fa5":"traindf[['Fare', 'Age']].describe()\nfrom sklearn.preprocessing import MinMaxScaler\nmms = MinMaxScaler()\ntraindf[['Fare', 'Age']] = mms.fit_transform(traindf[['Fare', 'Age']])\ntraindf[['Fare', 'Age']].describe()","764f2dc1":"traindf = pd.get_dummies(traindf, columns = ['Embarked', 'Sex'])\n#Once encoded let's see. \ntraindf.head()","f84a1994":"#The most simple one :D\ntraindf['ratioFareClass'] = traindf['Fare']\/traindf['Pclass']\n\ntraindf.head()","e46a5bab":"features = ['Age', 'Fare', 'Pclass', 'Embarked_C', 'Embarked_S', 'Embarked_Q', 'Sex_female', 'Sex_male', 'ratioFareClass']\nfrom sklearn.model_selection import train_test_split\nX_train, X_other, y_train, y_other = train_test_split(traindf[features],\n                                                    traindf[\"Survived\"],\n                                                    test_size=0.4,\n                                                    stratify=traindf['Survived'])\n\nX_test, X_valid, y_test, y_valid = train_test_split(X_other,\n                                                    y_other,\n                                                    test_size=0.5,\n                                                    stratify=y_other)","48c881d7":"print(\"Length of datasets:\\nTrain: %d\\nTest: %d\\nValidation: %d\" % (len(X_train), len(X_test), len(X_valid)) )","b99fd864":"from sklearn.model_selection import cross_val_score\ndef validate(model, X_train, y_train, k=10):\n    result = 'K-fold cross validation:\\n'\n    scores = cross_val_score(estimator=model,\n                             X=X_train,\n                             y=y_train,\n                             cv=k,\n                             n_jobs=1)\n    for i, score in enumerate(scores):\n        result += \"Iteration %d:\\t%.3f\\n\" % (i, score)\n    result += 'CV accuracy:\\t%.3f +\/- %.3f' % (np.mean(scores), np.std(scores))\n    return result","b978e25b":"import matplotlib.pyplot as plt\nfrom sklearn.model_selection import learning_curve\n\ndef learningCurve(model, X_train, y_train, k=10):\n    train_sizes, train_scores, test_scores =\\\n                    learning_curve(estimator=model,\n                                   X=X_train,\n                                   y=y_train,\n                                   train_sizes=np.linspace(0.1, 1.0, 10),\n                                   cv=k,\n                                   n_jobs=1)\n\n    train_mean = np.mean(train_scores, axis=1)\n    train_std = np.std(train_scores, axis=1)\n    test_mean = np.mean(test_scores, axis=1)\n    test_std = np.std(test_scores, axis=1)\n    \n    plt.rcParams[\"figure.figsize\"] = [6,6]\n    fsize=14\n    plt.xticks(fontsize=fsize)\n    plt.yticks(fontsize=fsize)\n    plt.plot(train_sizes, train_mean,\n             color='blue', marker='o',\n             markersize=5, label='training accuracy')\n    plt.fill_between(train_sizes,\n                     train_mean + train_std,\n                     train_mean - train_std,\n                     alpha=0.15, color='blue')\n\n    plt.plot(train_sizes, test_mean,\n             color='green', linestyle='--',\n             marker='s', markersize=5,\n             label='validation accuracy')\n\n    plt.fill_between(train_sizes,\n                     test_mean + test_std,\n                     test_mean - test_std,\n                     alpha=0.15, color='green')\n\n    plt.grid()\n    plt.xlabel('Number of training samples', fontsize=fsize)\n    plt.ylabel('Accuracy', fontsize=fsize)\n    plt.legend(loc='lower right')\n    plt.ylim([0.4, 1.03])\n    plt.tight_layout()\n    plt.show()","fae1e697":"from sklearn.model_selection import validation_curve\n\ndef validationCurve(model, X_train, y_train,p_name, p_range, k=10, scale=False):\n    train_scores, test_scores = validation_curve(\n                    estimator=model, \n                    X=X_train, \n                    y=y_train, \n                    param_name=p_name,\n                    param_range=p_range,\n                    cv=k)\n\n    train_mean = np.mean(train_scores, axis=1)\n    train_std = np.std(train_scores, axis=1)\n    test_mean = np.mean(test_scores, axis=1)\n    test_std = np.std(test_scores, axis=1)\n    plt.rcParams[\"figure.figsize\"] = [6,6]\n    fsize=14\n    plt.xticks(fontsize=fsize)\n    plt.yticks(fontsize=fsize)\n    plt.plot(p_range, train_mean, \n             color='blue', marker='o', \n             markersize=5, label='training accuracy')\n\n    plt.fill_between(p_range, train_mean + train_std,\n                     train_mean - train_std, alpha=0.15,\n                     color='blue')\n\n    plt.plot(p_range, test_mean, \n             color='green', linestyle='--', \n             marker='s', markersize=5, \n             label='validation accuracy')\n\n    plt.fill_between(p_range, \n                     test_mean + test_std,\n                     test_mean - test_std, \n                     alpha=0.15, color='green')\n\n    plt.grid()\n    if scale:\n        plt.xscale('log')\n    plt.legend(loc='lower right')\n    plt.xlabel('Parameter %s' % p_name, fontsize=fsize)\n    plt.ylabel('Accuracy', fontsize=fsize)\n    plt.ylim([0.7, 1.0])\n    plt.tight_layout()\n    plt.show()","bd520f48":"from sklearn.metrics import roc_curve, roc_auc_score\n\ndef rocCurve(model, X_test, y_test):\n    y_scores = model.predict_proba(X_test)[:,1]\n    fpr, tpr, thresholds = roc_curve(y_test, y_scores)\n    roc_auc = roc_auc_score(y_test, y_scores)\n    plt.figure()\n    lw = 2\n    plt.plot(fpr, tpr, color='darkorange',\n             lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)\n    plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.rcParams[\"figure.figsize\"] = [8,8]\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver operating characteristic example')\n    plt.legend(loc=\"lower right\")\n    plt.show()\n    ","b0523526":"\nfrom sklearn.metrics import confusion_matrix\n\ndef confusionMatrix(model, X_train, y_train, X_test, y_test): \n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n    confmat = confusion_matrix(y_true=y_test, y_pred=y_pred)\n    print(confmat)\n    fig, ax = plt.subplots(figsize=(6, 6))\n    ax.matshow(confmat, cmap=plt.cm.Blues, alpha=0.8)\n    for i in range(confmat.shape[0]):\n        for j in range(confmat.shape[1]):\n            ax.text(x=j, y=i, s=confmat[i, j], va='center', ha='center')\n\n    plt.xlabel('Predicted label')\n    plt.ylabel('True label')\n    plt.tight_layout()\n    plt.show()","8613e5b0":"#http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.linear_model.LogisticRegression.html\nfrom sklearn.linear_model import LogisticRegression\nlr = LogisticRegression(random_state=0, solver='lbfgs',\n                         multi_class='multinomial').fit(X_train, y_train)\n\n\nprint(\"Logistic Regression score (Train): {0:.2}\".format(lr.score(X_train, y_train)))\nprint(\"Logistic Regression score (Test): {0:.2}\".format(lr.score(X_test, y_test)))\nprint(validate(lr, X_train, y_train))","a7831af0":"learningCurve(lr, X_train, y_train)","793cbd2a":"rocCurve(lr, X_test, y_test)","3c8525f2":"confusionMatrix(lr, X_train, y_train, X_test, y_test)","6ef7f28f":"#http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.neighbors.KNeighborsClassifier.html\nfrom sklearn.neighbors import KNeighborsClassifier\nneigh = KNeighborsClassifier(n_neighbors=3)\nneigh.fit(X_train, y_train)\nprint(\"KNN score (Train): {0:.2}\".format(neigh.score(X_train, y_train)))\nprint(\"KNN score (Test): {0:.2}\".format(neigh.score(X_test, y_test)))\nprint(validate(neigh, X_train, y_train))","229315bc":"learningCurve(neigh, X_train, y_train)","98c86e50":"rocCurve(neigh, X_train, y_train)","34392c54":"confusionMatrix(neigh, X_train, y_train, X_test, y_test)","1bad0669":"#http:\/\/scikit-learn.org\/stable\/modules\/svm.html\nfrom sklearn.svm import SVC\nsvclass = SVC(probability=True)\nsvclass.fit(X_train, y_train) \nprint(\"SVM score (Train): {0:.2}\".format(svclass.score(X_train, y_train)))\nprint(\"SVM score (Test): {0:.2}\".format(svclass.score(X_test, y_test)))\nprint(validate(svclass, X_train, y_train))\n","8bae3bc2":"learningCurve(svclass, X_train, y_train)","9544a89b":"rocCurve(svclass, X_test, y_test)","505c7d07":"confusionMatrix(svclass, X_train, y_train, X_test, y_test)","c00c6c29":"#http:\/\/scikit-learn.org\/stable\/modules\/tree.html\nfrom sklearn import tree\ndt = tree.DecisionTreeClassifier()\ndt = dt.fit(X_train, y_train)\nprint(\"Decision Tree score (Train): {0:.2}\".format(dt.score(X_train, y_train)))\nprint(\"Decision Tree score (Test): {0:.2}\".format(dt.score(X_test, y_test)))\nprint(validate(dt, X_train, y_train))\n","e4d084fe":"learningCurve(dt, X_train, y_train)","a1e07254":"rocCurve(dt, X_train, y_train)","bf2a4c63":"confusionMatrix(dt, X_train, y_train, X_test, y_test)","90ab6124":"# http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.RandomForestClassifier.html\nfrom sklearn.ensemble import RandomForestClassifier\n\nforest = RandomForestClassifier(n_estimators=3,\n                                criterion='gini',\n                                max_depth=3,\n                                min_samples_split=10,\n                                min_samples_leaf=5,\n                                random_state=0)\nX_train.head()\nforest.fit(X_train, y_train)\nprint(\"Random Forest score (Train): {0:.2}\".format(forest.score(X_train, y_train)))\nprint(\"Random Forest score (Test): {0:.2}\".format(forest.score(X_test, y_test)))\nprint(validate(forest, X_train, y_train))\n","ddcbebca":"learningCurve(forest, X_train, y_train)","d50acb23":"rocCurve(forest, X_test, y_test)","4bc5b461":"confusionMatrix(forest, X_train, y_train, X_test, y_test)","dc66665c":"# C parameter: Inverse of regularization strength; must be a positive float. Like in support vector machines, smaller values specify stronger regularization.\n# More info: https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.linear_model.LogisticRegression.html\nvalidationCurve(lr, X_train, y_train, p_name='C', p_range=[0.001, 0.01, 0.1, 1.0, 10.0, 100.0], scale=True)","c4febceb":"validationCurve(neigh, X_train, y_train, p_name='n_neighbors', p_range=[1,2,3,4,5,10,15,20])","c5903014":"validationCurve(dt, X_train, y_train, p_name='max_depth', p_range=[1,2,3,4,5,10,20])","544c82a8":"validationCurve(forest, X_train, y_train, p_name='n_estimators', p_range=[1,2,3,4,5,10,20])","56c751fb":"from sklearn import svm, datasets\nfrom sklearn.model_selection import GridSearchCV\n\nparameters = {'n_estimators':[1,2,3,4,5,6,7,8,9,10],\n              'max_depth': [1,2,3,4,5,10,20] }\n\n# parameters = {'n_estimators':[1,2,3,4,5,6,7,8,9,10],\n#               'max_depth': [1,2,3,4,5,10,20],\n#               'min_samples_split': [2,3,4,5,6,7,8,9,10],\n#               'min_samples_leaf': [2,3,4,5,6,7,8,9,10]\n#              }\ngs = GridSearchCV(estimator=forest,\n                     param_grid=parameters,\n                     scoring='accuracy',\n                     cv=10)\ngs.fit(X_train,y_train)                            \nprint(gs.best_score_)\nprint(\"Best parameters: \" + str(gs.best_params_))\n\nclf = gs.best_estimator_\nclf.fit(X_train, y_train)\nprint('Train accuracy: %.3f' % clf.score(X_train, y_train))\nprint('Test accuracy: %.3f' % clf.score(X_test, y_test))\nprint('Validation accuracy: %.3f' % clf.score(X_valid, y_valid))","a4c0ab41":"## Imputer","47ca0e9e":"## Partitioning a dataset into separate train, test and validation sets","5a8a4283":"### Decision tree","76699420":"## Hyperparameter tuning. \n### Manual search trhough validation Curves","f14d03ac":"### KNN Classifier","dd2d17f9":"### Decision tree: max_depth. ","cae9ce88":"## K-fold Cross validation function validate(model, X_train, y_train, k=10)","54be7d2b":"### Support Vector Machines (Support Vector Classifier)","6a3c473f":"### Random forests: n_estimators","bd2b26f3":"### Confusion Matrix function","848fa3a9":"## Encoding of categorical variables (some examples)","d7348e4e":"**RRC**: First of all, load our dataset.","96ddacc6":"## Learning curves","4d5d6abd":"# Training ","57963383":"## Validation Curves","39111e20":"## Scaling ordinal features\n","ee44f37a":"## Transformations","8049b813":"### Random Forest Classifier","b7e4b2c5":"## **Preprocessing**\n## Identification of missings\n","e7f54dbc":"### KNN: n_neighbors","044e76cf":"### Logistic Regression Classifier","72e6ba95":"### ROC Curve"}}