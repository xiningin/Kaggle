{"cell_type":{"3e9ed01a":"code","aecd848d":"code","23feabf3":"code","d5bef366":"code","8601d073":"code","fddd83a8":"code","93f5a57d":"code","855c55b4":"code","a4246912":"code","c8f050a0":"code","1449101c":"code","88e7fd70":"code","08506fd6":"code","10731ca3":"markdown","423697c9":"markdown","09332c6c":"markdown","c27e9d5c":"markdown","ccfdffeb":"markdown","098568e9":"markdown","253164e8":"markdown","266f0ea8":"markdown"},"source":{"3e9ed01a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","aecd848d":"import pandas as pd\nimport numpy as np\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')\n%matplotlib inline\n\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score, cross_val_predict\nfrom sklearn.metrics import roc_curve, roc_auc_score, accuracy_score, f1_score, confusion_matrix\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import IsolationForest","23feabf3":"data = pd.read_csv(\"\/kaggle\/input\/creditcardfraud\/creditcard.csv\")\nprint(\"Dimensions:\",data.shape)\ndata.head()","d5bef366":"data.describe()","8601d073":"def checkNull(data):\n    return data.isnull().any()\ndef checkDatatype(data):\n    return data.dtypes\nprint(checkNull(data))\nprint(\"=================\")\nprint(checkDatatype(data))","fddd83a8":"# No null values and no object datatype.\ndef fraudDetection(data):\n    fraud, not_fraud = 0, 0\n    for i in data['Class']:\n        if i == 0:\n            fraud += 1\n        else:\n            not_fraud += 1\n    return (fraud*100)\/data.shape[0], fraud, (not_fraud*100)\/data.shape[0], not_fraud\nfraud_percentage, fraud, notfraud_detection, not_fraud = fraudDetection(data)\nsns.countplot('Class', data=data)\nplt.title(\"Non fraud transaction VS fraud transaction\")\nplt.xlabel(\"0->Not Fraud, 1-> Fraud\")","93f5a57d":"# From the description, we can see that Time and Amount columns are not skewed properly.\n# Plotting a distribution plot.\nfig, ax = plt.subplots(1, 2, figsize=(12, 4))\nsns.distplot(data['Amount'].values, ax = ax[0])\nax[0].set_title(\"Distribution plot of Amounts\")\nsns.distplot(data['Time'].values, ax = ax[1])\nax[1].set_title(\"Distribution plot of Time\")","855c55b4":"# We need to normalize Amount and Time \n\nrobust_scaler = RobustScaler()\ndata['normalized_amount'] = robust_scaler.fit_transform(data['Amount'].values.reshape(-1, 1))\ndata['normalized_time'] = robust_scaler.fit_transform(data['Time'].values.reshape(-1, 1))\nnew_data = data.drop(['Time', 'Amount'], axis=1)\nnew_data.head()","a4246912":"# Train test split\n\ny = data['Class'].values\nnew_data_no_class = new_data.drop(['Class'], axis=1)\nX = new_data_no_class.values\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=9)","c8f050a0":"# Stratified split to equate fraud and non-fraud transactions\n\nstartified_k_fold = StratifiedKFold(n_splits=9, random_state=9)\nfor i, j in startified_k_fold.split(X, y):\n    X_train_skf, X_test_skf = new_data.drop('Class', axis=1).iloc[i], new_data.drop('Class', axis=1).iloc[j]\n    y_train_skf, y_test_skf = new_data['Class'].iloc[i], new_data['Class'].iloc[j]\nX_train_skf","1449101c":"fraud_data = new_data.loc[new_data['Class']==1]\nnon_fraud_data = new_data.loc[new_data['Class']==0][:fraud_data.shape[0]]\ndata_distributed = pd.concat([fraud_data, non_fraud_data])\ndata_distributed = data_distributed.sample(frac = 1, random_state = 9)\nsns.countplot('Class', data=data_distributed)\nplt.title(\"Distribution\")\ndata_distributed.describe()","88e7fd70":"data_X = data_distributed.drop('Class', axis=1)\ndata_y = data_distributed['Class']\n\nX_train, X_test, y_train, y_test = train_test_split(data_X, data_y, test_size=0.33, random_state=9)\nX_train, X_test, y_train, y_test = X_train.values, X_test.values, y_train.values, y_test.values","08506fd6":"# Logistic Regression\n\nmodel = LogisticRegression()\nmodel.fit(X_train, y_train)\nscore = cross_val_score(model, X_train, y_train, cv=5)\ncv_predict = cross_val_predict(model, X_train, y_train, cv=5, method=\"decision_function\")\n\nprint(\"Training score:\",round(score.mean()*100, 2),\"%\")\nprint(\"ROC score:\",round(roc_auc_score(y_train, cv_predict)*100, 2),\"%\")\n\ny_pred = model.predict(X_test)\naccuracy = model.score(X_test, y_test)*100\n\nf1_score_ = f1_score(y_test, y_pred)*100\nprint(\"Confusion Matrix of the dataset:\")\ndf_cm = pd.DataFrame(confusion_matrix(y_test, y_pred), ['Actual No','Actual Yes'],['Pred No','Pred Yes'])\nsns.set(font_scale=1.4)\nsns.heatmap(df_cm, annot=True, cmap=\"coolwarm\", fmt='d')\nplt.show()\nprint(f'Accuracy percetage of the the dataset: {round(accuracy, 2)}%')\nprint(f'F1-Score percetage of the the dataset: {round(f1_score_, 2)}%')\n","10731ca3":"#### Importing Libraries","423697c9":"#### Train and test split from the distributed dataset","09332c6c":"#### To get the best model, it would best if the number of fraud and non-fraud dataset are equal.","c27e9d5c":"### Logistic regression to detect the fraudulent dataset.","ccfdffeb":"### Normalizing amount and time features","098568e9":"### Data cleaning, checking for null values and finding out the datatype of each column ","253164e8":"### Training and testing split","266f0ea8":"### Importing data"}}