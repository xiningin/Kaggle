{"cell_type":{"448fe7be":"code","9b766282":"code","2b6cf22d":"code","cee3b1a9":"code","2da3c3cb":"code","cbabfa9a":"code","db3a6506":"code","0819b5fe":"code","d13532a8":"code","9a1b85e5":"code","d2fdb986":"code","17b98ec3":"code","348139cf":"code","93db70a0":"code","7fdbc3a6":"code","936e54b2":"code","2383c3da":"code","cab9ed0f":"code","eb929396":"code","7b28da89":"code","a728a25e":"code","853f492e":"code","7fc0bcac":"code","b1a557d4":"code","8609612d":"code","1806d333":"code","2acc53e0":"markdown","ff712faa":"markdown","cc0cea56":"markdown","8e50e8d9":"markdown","14f57082":"markdown","813623bc":"markdown","9c45284a":"markdown","7b79b980":"markdown","4a66356e":"markdown","82cf4c84":"markdown","a6992024":"markdown","b0bb3cc3":"markdown","5b761272":"markdown","d1898407":"markdown","0191e8c1":"markdown","fe7d0957":"markdown","d959cc71":"markdown","2fb65016":"markdown","d46491c9":"markdown","5bcab432":"markdown"},"source":{"448fe7be":"import torch\ntorch.manual_seed(0)\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nplt.style.use('seaborn-whitegrid')\n\nimport matplotlib as mpl\nmpl.rcParams['figure.figsize']=(15,3.4)","9b766282":"path = '..\/input\/gtzan-musicspeech-collection'\n\nbollywood_audio = '..\/input\/musicspeechclassificationfromstreams'","2b6cf22d":"import librosa\n\naudio_datafile = path + '\/music_wav\/bagpipe.wav'\nx, sf = librosa.load(audio_datafile)\nprint(f'shape: {x.shape}\\nsampling freq: {sf}\\ntime duration: len\/sf = {x.shape[0]\/sf} seconds')","cee3b1a9":"plt.title('plot of a sound sinal from a .wav file')\nplt.plot(x[:1024])","2da3c3cb":"import IPython.display as ipd\nipd.Audio(audio_datafile)","cbabfa9a":"import librosa.display\n#plt.figure(figsize=(14, 5))\nlibrosa.display.waveplot(x, sr=sf)\nplt.title('a music waveplot with librosa.display.waveplot')","db3a6506":"speech = bollywood_audio + '\/housefull_dialog2.wav'\n\nx, sf = librosa.load(speech)\n\nprint(f'shape: {x.shape}\\nsampling freq: {sf}\\ntime duration: len\/sf = {x.shape[0]\/sf} seconds')\nplt.title('plot of a sound sinal from a .wav file')\nplt.plot(x[:2048]);","0819b5fe":"music = bollywood_audio + '\/housefull_song1.wav'\n\nx, sf = librosa.load(music)\n\nprint(f'shape: {x.shape}\\nsampling freq: {sf}\\ntime duration: len\/sf = {x.shape[0]\/sf} seconds')\nplt.title('plot of a sound sinal from a .wav file')\nplt.plot(x[:2048]);","d13532a8":"speech_file = path + '\/speech_wav\/god.wav'\n\nx, sf = librosa.load(speech_file)\n\nprint(f'shape: {x.shape}\\nsampling freq: {sf}\\ntime duration: len\/sf = {x.shape[0]\/sf} seconds')\nplt.title('plot of a sound sinal from a .wav file')\nplt.plot(x[:1024]);","9a1b85e5":"import IPython.display as ipd\nipd.Audio(speech_file)","d2fdb986":"import librosa.display\n#plt.figure(figsize=(14, 5))\nlibrosa.display.waveplot(x, sr=sf)\nplt.title('a speech waveplot')","17b98ec3":"import os\n\nclass_names = ['music', 'speech']\ndata_dir = {'music' : path + '\/music_wav', \n            'speech': path + '\/speech_wav'}\n\nwav_files = {cls_name: [] for cls_name in class_names}\nfor cls_name in class_names:\n    folder = data_dir[cls_name]\n    filelist = os.listdir(folder)\n    for filename in filelist:\n        if filename[-4:] == '.wav':\n            wav_files[cls_name].append(os.path.join(folder, filename))\n            \nfileNames = os.listdir(bollywood_audio)\nbolly_wav_files = [os.path.join(bollywood_audio, file) for file in fileNames]\nprint (bolly_wav_files)","348139cf":"print([len(wav_files[c]) for c in class_names])\nwav_files['music'][:3]","93db70a0":"np.random.seed(1)\n\nfile_list = {'train': [], 'val': []}\nfor class_id, c in enumerate(class_names):\n    n_data = len(wav_files[c])\n    rindx = np.random.permutation(n_data)\n    n_validation = int(0.25*n_data)\n    v_indx = rindx[:n_validation]\n    t_indx = rindx[n_validation:]\n    file_list['train'] += [ (wav_files[c][k], class_id) for k in t_indx ] \n    file_list['val'] += [ (wav_files[c][k], class_id) for k in v_indx ] \n#     file_list['']\n\nbolly_tuples = []\nfor file in bolly_wav_files:\n    if 'song' in file:\n        bolly_tuples.append((file, 0))\n    else:\n        bolly_tuples.append((file, 1))\n","7fdbc3a6":"[ len(file_list[tv]) for tv in ['train', 'val'] ]","936e54b2":"import torch\nimport numpy as np\nimport librosa # audio file manipulation\n\nclass MSDataset(torch.utils.data.Dataset):\n    \"\"\" Music\/Speech Classification \n        filelist: [(file_path, class_id)]\n        sample_time: time duration to sample from .wav file\n                     the sample is extracted somewhere in the middle of the whole sequence\n                     similar to data augmentation\n                     \n         Validation dataset: the first segment of the sequence is used.\n                             Another option is to apply several segments and accumulate multiple inferences\n    \"\"\"\n    def __init__(self, filelist, sample_sec=5., is_train=True):\n        self.filelist = filelist\n        self.time_duration = sample_sec\n        self.is_train = is_train\n        \n        _, sf = librosa.load(filelist[0][0])\n        self.sf = sf\n        self.n_features = int(self.time_duration * sf)\n        \n    def __len__(self):\n        return len(self.filelist)\n    \n    def __getitem__(self, i):\n        # 1. load the file\n        # 2. sample a segment of the length from the whole seq\n        # 3. return segment, id\n        audio_file, class_id = self.filelist[i]\n        x, sf = librosa.load(audio_file)\n\n        if self.is_train:\n            k = np.random.randint(low=0, high=x.shape[0]-self.n_features) # choose the start index\n        else:\n            k = 0\n        \n        x = torch.from_numpy(x[k:k+self.n_features]).reshape(1,-1)\n        \n        return x, class_id\n    \n    def load(self, audio_file):\n        return librosa.load(audio_file)","2383c3da":"ds = MSDataset(file_list['train'], sample_sec=5, is_train=True)\n# c = file_list['train'][1]\n# print(c)\nx, label = ds[-1]\n\nprint('dataset length: ', len(ds), x.dtype, x.shape, label)#label.dtype, label.shape)\n\nplt.title(f'Audio segment, randomly sampled. src len: {x.shape} label: {label}');\nplt.plot(x.reshape(-1)); ","cab9ed0f":"sample_sec = 2\nbatch_size = 32\n\ndata_loader = {tv: \n                   torch.utils.data.DataLoader(MSDataset(file_list[tv], sample_sec=sample_sec, is_train=tv=='train'),\n                                               batch_size=batch_size,\n                                               shuffle=True,\n                                               )\n               for tv in ['train', 'val']}\n#\ndata_loader","eb929396":"import torch.nn as nn\n\nclass BasicBlock(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super().__init__()\n        kernel_size = 17\n        self.Conv = nn.Conv1d(in_channels=in_channels,\n                              out_channels=out_channels,\n                              kernel_size=kernel_size,\n                              stride=3)\n        self.BatchNorm = nn.BatchNorm1d(out_channels)\n        self.ELU = nn.ELU()\n    \n\n    def forward(self, x):\n        x = self.Conv(x)\n        x = self.BatchNorm(x)\n        x = self.ELU(x)\n\n        return x\n    \n    \nclass MyModel(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super().__init__()\n        \n        self.Hidden_1 = BasicBlock(in_channels, 100)  \n        self.Hidden_2 = BasicBlock(100, out_channels)\n        self.Pooling = nn.AdaptiveAvgPool1d(1)\n        self.Flatten = nn.Flatten()\n        self.activation = nn.LogSoftmax(dim = 1)\n\n    def forward(self, x):\n        x = self.Hidden_1(x)\n        x = self.Hidden_2(x)\n        x = self.Pooling(x)\n        x = self.Flatten(x)\n        x = self.activation(x)\n        \n        return x","7b28da89":"n_channels = 1\nn_targets  = 2\n\nmodel = MyModel(n_channels, n_targets)\nmodel","a728a25e":"def training_loop(n_epochs, optim, model, loss_fn, dl_train, dl_val, hist=None):\n    if hist is not None:\n        pass\n    else:\n        hist = {'tloss': [], 'tacc': [], 'vloss': [], 'vacc': []}\n    best_acc = 0\n    for epoch in range(1, n_epochs+1):\n        tr_loss, tr_acc = 0., 0.\n        n_data = 0\n        for im_batch, label_batch in dl_train: # minibatch\n            im_batch, label_batch = im_batch.to(device), label_batch.to(device)\n            ypred = model(im_batch)\n            loss_train = loss_fn(ypred, label_batch)\n        \n            optim.zero_grad()\n            loss_train.backward()\n            optim.step()\n   \n            # accumulate correct prediction\n            tr_acc  += (torch.argmax(ypred.detach(), dim=1) == label_batch).sum().item() # number of correct predictions\n            tr_loss += loss_train.item() * im_batch.shape[0]\n            n_data  += im_batch.shape[0]\n        #\n        # statistics\n        tr_loss \/= n_data\n        tr_acc  \/= n_data\n        #\n        val_loss, val_acc = performance(model, loss_fn, dl_val)\n        \n        if epoch <= 5 or epoch % 10 == 0 or epoch == n_epochs:\n             print(f'Epoch {epoch}, tloss {tr_loss:.2f} t_acc: {tr_acc:.4f}  vloss {val_loss:.2f}  v_acc: {val_acc:.4f}')\n#         else:\n#             if best_acc < val_acc:\n#                 best_acc = val_acc\n#                 print(' best val accuracy updated: ', best_acc)\n        #\n        # record for history return\n        hist['tloss'].append(tr_loss)\n        hist['vloss'].append(val_loss) \n        hist['tacc'].append(tr_acc)\n        hist['vacc'].append(val_acc)\n        \n    print ('finished training_loop().')\n    return hist\n#\n\ndef performance(model, loss_fn, dataloader):\n    model.eval()\n    with torch.no_grad():\n        loss, acc, n = 0., 0., 0.\n        for x, y in dataloader:\n            x, y = x.to(device), y.to(device)\n            ypred = model(x)\n            loss += loss_fn(ypred, y).item() * len(y)\n            p = torch.argmax(ypred, dim=1)\n            acc += (p == y).sum().item()\n            n += len(y)\n        #\n    loss \/= n\n    acc \/= n\n    return loss, acc\n#\ndef plot_history(history):\n    fig, axes = plt.subplots(1,2, figsize=(16,6))\n    axes[0].set_title('Loss'); \n    axes[0].plot(history['tloss'], label='train'); axes[0].plot(history['vloss'], label='val')\n    axes[0].legend()\n    max_vacc = max(history['vacc'])\n    axes[1].set_title(f'Acc. vbest: {max_vacc:.2f}')\n    axes[1].plot(history['tacc'], label='train'); axes[1].plot(history['vacc'], label='val')\n    axes[1].legend()\n#","853f492e":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(device)\n\n# model\nin_channels = 1\nn_targets = 2\n\nmodel = MyModel(in_channels, n_targets).to(device)\n\n# optim\nlearning_rate = 0.1\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n#optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n\n# loss\ncriterion = nn.CrossEntropyLoss().to(device)\n\n# history\nhistory = None","7fc0bcac":"history = training_loop(230, optimizer, model, criterion, data_loader['train'], data_loader['val'], history)","b1a557d4":"plot_history(history)","8609612d":"bolly_data_set = torch.utils.data.DataLoader(MSDataset(bolly_tuples, sample_sec=sample_sec, is_train=False),\n                                               batch_size=batch_size,\n                                               shuffle=False,\n                                               )\ndef testBollywoodAudio(model, dataloader):\n    model.eval()\n    pred = []\n    with torch.no_grad():\n        for x, y in dataloader:\n            x, y = x.to(device), y.to(device)\n            ypred = model(x)\n            p = torch.argmax(ypred, dim=1)\n            print(p, y)\n            pred.append(p)\n    return pred\n\npredictions = testBollywoodAudio(model, bolly_data_set)\n\npred_arr = predictions[0].cpu().numpy()","1806d333":"from prettytable import PrettyTable\nt = PrettyTable(['File', 'Actual', 'Predicted'])\n\nidx = 0\nfor (path, actual_label) in bolly_tuples:\n    file_name = path.split('\/')[-1]\n#     print('File: {} Actual Label: {} Predicted Label: {}'.format(file_name, actual_label, pred_arr[idx]))\n    t.add_row([file_name, actual_label, pred_arr[idx]])\n    idx += 1\n\nprint(t)","2acc53e0":"## Model, Optimizer & Loss Function","ff712faa":"### About the Data\n\n- A similar dataset which was collected for the purposes of music\/speech discrimination. \n- The dataset consists of 120 tracks, each 30 seconds long. \n- Each class (music\/speech) has 60 examples. \n- The tracks are all 22050Hz Mono 16-bit audio files in .wav format.","cc0cea56":"### Speech sample from bollywood audio ","8e50e8d9":"### Train\/Validation Split","14f57082":"### Sanity Check","813623bc":"### Music sample from bollywood audio","9c45284a":"### Dataloader for train\/validation","7b79b980":"1. # Music\/Speech Classification \n- with `nn.Conv1d()`\n- audio signal processing\n- neural network for classification\n- data: http:\/\/marsyas.info\/downloads\/datasets.html\n- main ref: https:\/\/www.kdnuggets.com\/2020\/02\/audio-data-analysis-deep-learning-python-part-1.html\n- see also [Speaker Verification](https:\/\/github.com\/HarryVolek\/PyTorch_Speaker_Verification)","4a66356e":"End.","82cf4c84":"### Train More","a6992024":"### Sanity Check: Dataloader + Model","b0bb3cc3":"### A music sample from dataset","5b761272":"Except one rest of them are predicted correctly","d1898407":"## First Trial","0191e8c1":"## Trainin Loop","fe7d0957":"## Network Model Design","d959cc71":"----","2fb65016":"### Data file list up","d46491c9":"### Dataset","5bcab432":"## Music Speech Classification\n\n- Each training data is 30 seconds long.\n- We will use only N samples (T = N\/sf seconds) randomly chosen from 30 seconds long sequence for training\/testing.\n    - 1 second = sf samples (22050 samples\/sec,  or 44100 samples\/sec)\n- We will use .wav only. The dataset provide data in two formats: .au and .wav.  "}}