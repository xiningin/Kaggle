{"cell_type":{"a38d2e93":"code","ed488a68":"code","68ab97cd":"code","18080f5b":"code","5d0522ee":"code","7b57d291":"code","fba6b726":"code","6b0d660e":"code","acd273a9":"code","44dfa6a0":"code","9b74d03f":"code","2d5639a4":"code","81d7e52d":"code","938eea1c":"code","7c59fa9d":"code","f79b5c27":"code","1ea1618e":"code","18b0fe1f":"code","220e2bca":"code","e34a8b6a":"code","efd50410":"code","93919de6":"code","d8e35d18":"code","c68b9324":"code","dbaf8d25":"code","586a803d":"code","5ade6f27":"code","58915d44":"code","5dcb5513":"code","fe880e1a":"code","ba7cb07c":"code","612dabc3":"code","16548ac4":"code","33fdb4de":"code","8107eb43":"code","d37908af":"code","e30ef66a":"code","6d47b3e8":"code","5f4a630e":"code","8b62ced7":"code","dd487037":"code","2196378a":"code","3b51448a":"code","0a8f67a4":"code","189dd357":"code","06ce831d":"markdown","96198b72":"markdown","bea14b75":"markdown"},"source":{"a38d2e93":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom matplotlib import pylab\nfrom pylab import *\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.impute import SimpleImputer\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import f_regression\nfrom sklearn.model_selection import train_test_split\nfrom tabulate import tabulate\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import StandardScaler\n\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nimport xgboost as xgb\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.datasets import make_classification\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.svm import SVR\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","ed488a68":"train = \"\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv\"\ntest  = \"\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv\"\n\ntrain_dataset = pd.read_csv(train)\ntest_dataset = pd.read_csv(test)\n\nprint(\"The shape of Train Dataset is : \"+str(train_dataset.shape))\nprint(\"The shape of Test Dataset is : \"+str(test_dataset.shape))\n","68ab97cd":"train_dataset.head(5)","18080f5b":"train_dataset.info()","5d0522ee":"\n    data = train_dataset.select_dtypes(include=[np.number]).interpolate().dropna()\n    X = data.drop(['SalePrice', 'Id'], axis=1)\n    y = np.log(train_dataset.SalePrice)\n\n    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=.33)\n\n    test = SelectKBest(score_func=f_regression, k=4)\n    fit = test.fit(X, y) # ValueError: Unknown label type - https:\/\/stackoverflow.com\/questions\/34246336\/python-randomforest-unknown-label-error - only when using chi2 function\n    # fit = test.fit(X, np.asarray(y, dtype=\"|S6\"))\n\n    np.set_printoptions(precision=3, suppress=True)\n\n    print(\"Features: {features}\".format(features=X.columns))\n    print(\"Scores: {scores}\".format(scores=fit.scores_))\n\n    values = [(value, float(score)) for value, score in sorted(zip(X.columns, fit.scores_), key=lambda x: x[1] * -1)]\n    print(tabulate(values, [\"column\", \"score\"], tablefmt=\"plain\", floatfmt=\".4f\"))\n\n\n    selected_features = fit.transform(X)\n    print(\"Features: {selected_features}\".format(selected_features = selected_features))","7b57d291":"abs(train_dataset.corr()['SalePrice']).sort_values(ascending=False)","fba6b726":"#If the correlation coefficient less than 0.3,it should be drop\n\nlow_coefficient = ['HalfBath',\"LotArea\",\"BsmtFullBath\",\"BsmtUnfSF\",\"BedroomAbvGr\",\"KitchenAbvGr\",\"EnclosedPorch\",\"ScreenPorch\",\"PoolArea\",\"MSSubClass\",\n                  \"OverallCond\",\"MoSold\",\"3SsnPorch\",\"YrSold\",\"LowQualFinSF\",\"MiscVal\",\"BsmtHalfBath\",\"BsmtFinSF2\"]\ndata = pd.concat([train_dataset,test_dataset])\n\ntrain_dataset.drop(low_coefficient,axis = 1,inplace=True)\ntest_dataset.drop(low_coefficient,axis = 1, inplace=True)","6b0d660e":"\ncorrmat = train_dataset.corr()\nk = 10 #number of variables for heatmap\ncols = corrmat.nlargest(k, 'SalePrice')['SalePrice'].index   # nlargest : pick the most powerfull correlation\ncm = np.corrcoef(train_dataset[cols].values.T)\nsns.set(font_scale=1.25)\nhm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10}, yticklabels=cols.values, xticklabels=cols.values)\nplt.show()","acd273a9":"\nsns.pairplot(train_dataset[['SalePrice',\"OverallQual\",\"GrLivArea\",\"GarageCars\",\"GarageArea\",\"TotalBsmtSF\",\"1stFlrSF\",\"FullBath\",\"TotRmsAbvGrd\",\"YearBuilt\"]])","44dfa6a0":"train_dataset.info()","9b74d03f":"pd.options.display.min_rows = 115\nx = train_dataset.isnull().sum()\nx = pd.DataFrame(x)\nx[\"%\"] = x[0]\/len(train_dataset)*100\nx =x.sort_values(\"%\",ascending = False).head(25)\nx","2d5639a4":"# drop column of included +5% missing value \n\ndrop_col = x.index[:6]","81d7e52d":"train_dataset.drop(drop_col,axis = 1,inplace = True)\ntest_dataset.drop(drop_col, axis = 1, inplace = True)","938eea1c":"train_dataset.info()","7c59fa9d":"# Check the Object columns and put in DataFrame with unique counts","f79b5c27":"x = train_dataset.select_dtypes(include='object')","1ea1618e":"object_columns = x.columns\nunique_count = []\nfor i in object_columns:\n    unique_count.append(len(x[i].unique()))\n","18b0fe1f":"object_df = pd.DataFrame(object_columns)\nobject_df['unique_count'] = unique_count\n\nobject_df = object_df.sort_values('unique_count',ascending=False)\n\nobject_df['columns'] = object_df[0]\nobject_df.drop(0,axis=1,inplace = True)\nx = object_df.loc[object_df[\"unique_count\"] > 4]['columns']\nx = list(x)\nx","220e2bca":"train_dataset.drop(x,axis = 1, inplace = True)\ntest_dataset.drop(x, axis = 1, inplace = True)\n","e34a8b6a":"train_dataset.shape","efd50410":"# Build a pipeline to preprocess Na values and One Hot Encoder","93919de6":"train_dataset.drop(['Id','SalePrice'],axis=1,inplace = True)\ntest_dataset.drop('Id',axis = 1, inplace = True)","d8e35d18":"numerical_cols = test_dataset.select_dtypes(exclude = ['object']).columns\ncategorical_cols = test_dataset.select_dtypes(include = ['object'] ).columns\n\n\n# 1. Numerical & Categorical variables transformer\n#Simple Transformer\nnumerical_transformer_simple = SimpleImputer(missing_values=np.nan,strategy='median')\ncategorical_transformer_simple = Pipeline(steps=[\n    ('imputer', SimpleImputer(missing_values=np.nan,strategy = 'most_frequent')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n])\n\n\n\ndata_transformer_simple = ColumnTransformer(\n    transformers=[\n        ('num', numerical_transformer_simple, numerical_cols),\n        ('cat', categorical_transformer_simple, categorical_cols)\n    ])","c68b9324":"X = train_dataset\ny = pd.read_csv(train).SalePrice","dbaf8d25":"X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.33,random_state=42)","586a803d":"saleprice_pipeline_xgb = Pipeline(steps=[\n    ('preprocessor', data_transformer_simple),\n    ('scale', StandardScaler()),\n    ('model',xgb.XGBRegressor())\n])","5ade6f27":"params = {\n        \n        'model__n_estimators' : [200,400],\n        'model__max_depth' : [2,3],\n        'model__colsample_bytree' : [0.4,0.6]\n        }","58915d44":"salePrice_grid = GridSearchCV(saleprice_pipeline_xgb,param_grid = params,cv=3,n_jobs=-1,verbose = 2)","5dcb5513":"salePrice_grid.fit(X_train,y_train)","fe880e1a":"salePrice_grid.best_params_","ba7cb07c":"# To Find RMSE \ny_pred_test_xgb = salePrice_grid.best_estimator_.predict(X_test)","612dabc3":"y_pred = salePrice_grid.best_estimator_.predict(test_dataset)","16548ac4":"\nprint(\"XGBoostTest RMSE  : \" +str(np.sqrt(mean_squared_error(y_test,y_pred_test_xgb))))","33fdb4de":"saleprice_pipeline_log = Pipeline(steps=[\n    ('preprocessor', data_transformer_simple),\n    ('scale', StandardScaler()),\n    ('model',SVR())\n])","8107eb43":"svr_params = {\"model__C\":[0.1,0.4,5,10,20,40]}","d37908af":"salePrice_grid_log = GridSearchCV(saleprice_pipeline_log,param_grid =svr_params,cv=3,n_jobs=-1,verbose = 2)","e30ef66a":"salePrice_grid_log.fit(X_train,y_train)","6d47b3e8":"salePrice_grid_log.best_params_","5f4a630e":"# To Find RMSE \ny_pred_test_log = salePrice_grid_log.best_estimator_.predict(X_test)","8b62ced7":"y_pred2 = salePrice_grid_log.best_estimator_.predict(test_dataset)","dd487037":"print(\"SVR RMSE  : \" +str(np.sqrt(mean_squared_error(y_test,y_pred_test_log))))","2196378a":"Id= pd.read_csv(\"\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv\").Id","3b51448a":"submission = pd.DataFrame(Id, columns = [\"Id\"])\nsubmission[\"SalePrice\"] = y_pred\nsubmission.to_csv(\"\/kaggle\/working\/submission.csv\", index = False)","0a8f67a4":"print(submission.shape)\nprint(submission.isnull().sum())","189dd357":"print(\"Please add your valuable comment and suggestion :) \")","06ce831d":"**1)**","96198b72":"**Introduction**","bea14b75":"**Preprocessing**"}}