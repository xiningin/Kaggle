{"cell_type":{"14461984":"code","1fa4fdf2":"code","c64276a3":"code","b84459ba":"code","370bc69e":"code","7d92ec67":"code","3bc8896c":"code","4623c510":"code","b609ae50":"code","cf6a965d":"code","20f504d8":"code","6ce84f66":"code","2d8b11d6":"code","405e6510":"code","33561eac":"markdown","57882742":"markdown","68b8a016":"markdown","8e510bf1":"markdown","4f196406":"markdown"},"source":{"14461984":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","1fa4fdf2":"train = pd.read_csv('..\/input\/tabular-playground-series-may-2021\/train.csv')\ntest = pd.read_csv('..\/input\/tabular-playground-series-may-2021\/test.csv')\nsample_submission = pd.read_csv('..\/input\/tabular-playground-series-may-2021\/sample_submission.csv')","c64276a3":"# train.describe().T","b84459ba":"# train.info()","370bc69e":"#enc = preprocessing.OrdinalEncoder()\n#train.target = enc.fit_transform(train[['target']])","7d92ec67":"# test.describe().T","3bc8896c":"# test.info()","4623c510":"sample_submission.head()","b609ae50":"y_train = train['target']\nX_train = train.drop(['id','target'], axis=1)\nX_test = test.drop(['id'], axis=1)","cf6a965d":"from sklearn import preprocessing\n\nscaler = preprocessing.StandardScaler().fit(X_train)\nX_train = scaler.transform(X_train)\n\nscaler = preprocessing.StandardScaler().fit(X_test)\nX_test = scaler.transform(X_test)","20f504d8":"\n# Precision: \n    # tp \/ (tp + fp)\n    # fp = typeI error\n    # True positive \/ Predicted condition positive\n    # Among the ones predicted as certain label, actually correct ones.\n    # The precision will be \"how many are correctly classified among that class\"\n    # \u201cfor all instances classified positive, what percent was correct?\u201d\n    # Precision is the ability of a classifier not to label an instance positive that is actually negative. \n\n# Recall\/Sensitivity\/Power of a Test:\n    # tp \/ (tp + fn)\n    # fn = actual label does not belongs predicted label.\n    # True Predicted Positive \/ All Real Positives \n    # Correctly identifying (true pos) a label when sample belongs to that (all pos). \n    # Proportion of positives that are correctly identified\n    # TPR = true positive rate\n    # The recall means \"how many of this class you find over the whole number of element of this class\n    # What percent of the positive cases did you catch?\n    \n# Specificity =(True Negative rate) \n\n# F1 = 2 * (precision * recall) \/ (precision + recall)\n# F1 score reaches its best value at 1 and worst score at 0. \n# What percent of positive predictions were correct?\n\n# support in classification report in reality true instances.  The support is the number of occurrences of each class in y_true.","6ce84f66":"from sklearn.experimental import enable_hist_gradient_boosting\nfrom sklearn.ensemble import HistGradientBoostingClassifier\n\nclf_HistGradientBoostingClassifier = HistGradientBoostingClassifier()\n\nclf_HistGradientBoostingClassifier.fit(X_train, y_train)\n#predictions_b = clf_HistGradientBoostingClassifier.predict(X_test)\npredictions_p = clf_HistGradientBoostingClassifier.predict_proba(X_test)\nsample_submission.iloc[:,1:5] = predictions_p\nsample_submission.to_csv('submission_HistGradientBoostingClassifier.csv',index=False)\n\nfrom sklearn.model_selection import cross_val_score\nscores = cross_val_score(clf_HistGradientBoostingClassifier, X_train, y_train, cv=5, scoring='neg_log_loss')\nprint('Log_loss scores:', -scores)\n#from sklearn.metrics import classification_report, log_loss\n\n#print('Simple clf.score method of the estimator:', clf_HistGradientBoostingClassifier.score(X_train, y_train), '\\n\\n')\n#print('Detailed Classification Report: \\n\\n',classification_report(y_train, clf_HistGradientBoostingClassifier.predict(X_train), zero_division=0))\n#print('LogLoss:',log_loss(y_train, clf_HistGradientBoostingClassifier.predict_proba(X_train)))","2d8b11d6":"# Understanding Multi-class logarithmic loss with Pandas\n\ndf1 = pd.DataFrame(clf_HistGradientBoostingClassifier.predict_proba(X_train), columns=['Class_1','Class_2','Class_3','Class_4'])\ndf1['actuals'] = y_train\ndf1['predictions'] = clf_HistGradientBoostingClassifier.predict(X_train)\ndf1['nl1']=-np.log(df1['Class_1'])\ndf1['nl2']=-np.log(df1['Class_2'])\ndf1['nl3']=-np.log(df1['Class_3'])\ndf1['nl4']=-np.log(df1['Class_4'])\ndf1['Prediction Correct ?'] = (df1['actuals'] == df1['predictions'])*1\ndf1['nl_final'] = df1['nl1']*(df1['actuals']=='Class_1')+df1['nl2']*(df1['actuals']=='Class_2')+df1['nl3']*(df1['actuals']=='Class_3')+df1['nl4']*(df1['actuals']=='Class_4')\nprint('Correct Prediction #:',df1['Prediction Correct ?'].sum())\nprint('Correct Prediction %:',df1['Prediction Correct ?'].sum()\/len(df1))\nprint('nl_final_average:',df1['nl_final'].sum()\/len(df1))\n\ndf1","405e6510":"from catboost import CatBoostClassifier\nclf_CatBoostClassifier = CatBoostClassifier(verbose=0)\nclf_CatBoostClassifier.fit(X_train, y_train)\npredictions_p = clf_CatBoostClassifier.predict_proba(X_test)\nsample_submission.iloc[:,1:5] = predictions_p\nsample_submission.to_csv('submission_CatBoostClassifier.csv',index=False)\n\nscores = cross_val_score(clf_CatBoostClassifier, X_train, y_train, cv=5, scoring='neg_log_loss')\nprint('Log_loss scores:', -scores)\n","33561eac":"import matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nplt.figure(figsize=(10,20))\nsns.boxplot(data=X_train, orient=\"h\");","57882742":"from sklearn.model_selection import train_test_split\n\nX_train_trn, X_train_tst, y_train_trn, y_train_tst = train_test_split( X_train, y_train, test_size=0.1)","68b8a016":"from xgboost import XGBClassifier\nclf_XGBClassifier = XGBClassifier() \nclf_XGBClassifier.fit(X_train, y_train)\npredictions_p = clf_XGBClassifier.predict_proba(X_test)\nsample_submission.iloc[:,1:5] = predictions_p\nsample_submission.to_csv('submission_XGBClassifier.csv',index=False)\n\nscores = cross_val_score(clf_XGBClassifier, X_train, y_train, cv=5, scoring='neg_log_loss')\nprint('Log_loss scores:', -scores)","8e510bf1":"from sklearn.neural_network import MLPClassifier\nclf_MLPClassifier = MLPClassifier(random_state=1, max_iter=1000)\nclf_MLPClassifier.fit(X_train, y_train)\npredictions_p = clf_MLPClassifier.predict_proba(X_test)\nsample_submission.iloc[:,1:5] = predictions_p\nsample_submission.to_csv('submission_MLPClassifier.csv',index=False)\n\nscores = cross_val_score(clf_MLPClassifier, X_train, y_train, cv=5, scoring='neg_log_loss')\nprint('Log_loss scores:', -scores)","4f196406":"from lightgbm import LGBMClassifier\nclf_LGBMClassifier = LGBMClassifier()\nclf_LGBMClassifier.fit(X_train, y_train)\npredictions_p = clf_LGBMClassifier.predict_proba(X_test)\nsample_submission.iloc[:,1:5] = predictions_p\nsample_submission.to_csv('submission_LGBMClassifier.csv',index=False)\n\nscores = cross_val_score(clf_LGBMClassifier, X_train, y_train, cv=5, scoring='neg_log_loss')\nprint('Log_loss scores:', -scores)"}}