{"cell_type":{"0804a78d":"code","d9c122ff":"code","af768d13":"code","c5fd4781":"code","9b3febfe":"code","40a6cd57":"code","8f23a142":"code","59774d3a":"code","c7972323":"code","445acd4b":"code","d743b065":"code","be3629fc":"code","6f1576b4":"code","93c05d41":"code","d3ff6dbf":"code","5c400484":"code","a1834564":"code","d5e89d40":"markdown","e479d92b":"markdown","d3097c8d":"markdown","9bdd9f22":"markdown","38941fbe":"markdown","de61864f":"markdown","3a143ea5":"markdown","48ac658b":"markdown","78ae0693":"markdown"},"source":{"0804a78d":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","d9c122ff":"!pip install ivis\n\n!git clone https:\/\/github.com\/beringresearch\/ivis-explain\n!pip install --editable  .\/ivis-explain","af768d13":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport sys\nimport os\n\nsys.path.append(os.path.abspath('.\/ivis-explain'))\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, average_precision_score, roc_auc_score, classification_report, roc_curve, precision_recall_curve\nfrom sklearn.linear_model import LogisticRegression\n\nfrom ivis import Ivis\nfrom ivis_explanations import LinearExplainer\n\n","c5fd4781":"data = pd.read_csv('\/kaggle\/input\/creditcardfraud\/creditcard.csv')\nY = data['Class']\nX = data.drop(['Class','Time'], axis=1)","9b3febfe":"train_X, test_X, train_Y, test_Y = train_test_split(X, Y, stratify=Y, test_size=0.8, random_state=1234)","40a6cd57":"# standard_scaler = StandardScaler().fit(train_X[['Time', 'Amount']])\n# train_X.loc[:, ['Time', 'Amount']] = standard_scaler.transform(train_X[['Time', 'Amount']])\n# test_X.loc[:, ['Time', 'Amount']] = standard_scaler.transform(test_X[['Time', 'Amount']])\n\nstandard_scaler = StandardScaler().fit(train_X[['Amount']])\ntrain_X.loc[:, ['Amount']] = standard_scaler.transform(train_X[['Amount']])\ntest_X.loc[:, ['Amount']] = standard_scaler.transform(test_X[['Amount']])\nminmax_scaler = MinMaxScaler().fit(train_X)\ntrain_X = minmax_scaler.transform(train_X)\ntest_X = minmax_scaler.transform(test_X)","8f23a142":"ivis = Ivis(embedding_dims=2, model='maaten',\n            k=15, n_epochs_without_progress=5,\n            supervision_weight=0.95,\n            verbose=0)\nivis.fit(train_X, train_Y.values)","59774d3a":"ivis.save_model('ivis-supervised-fraud', overwrite=True)","c7972323":"train_embeddings = ivis.transform(train_X)\ntest_embeddings = ivis.transform(test_X)","445acd4b":"fig, ax = plt.subplots(1, 2, figsize=(17, 7), dpi=200)\nax[0].scatter(x=train_embeddings[:, 0], y=train_embeddings[:, 1], c=train_Y, s=3, cmap='RdYlBu_r')\nax[0].set_xlabel('ivis 1')\nax[0].set_ylabel('ivis 2')\nax[0].set_title('Training Set')\n\nax[1].scatter(x=test_embeddings[:, 0], y=test_embeddings[:, 1], c=test_Y, s=3, cmap='RdYlBu_r')\nax[1].set_xlabel('ivis 1')\nax[1].set_ylabel('ivis 2')\nax[1].set_title('Testing Set')","d743b065":"clf = LogisticRegression(solver=\"lbfgs\").fit(train_embeddings, train_Y)","be3629fc":"labels = clf.predict(test_embeddings)\nproba = clf.predict_proba(test_embeddings)","6f1576b4":"print(classification_report(test_Y, labels))\n\nprint('Confusion Matrix')\nprint(confusion_matrix(test_Y, labels))\nprint('Average Precision: '+str(average_precision_score(test_Y, proba[:, 1])))\nprint('ROC AUC: '+str(roc_auc_score(test_Y, labels)))","93c05d41":"# retrieve just the probabilities for the positive class\npos_probs = proba[:, 1]\n\n# calculate roc curve for model\nfpr, tpr, thresholds = roc_curve(test_Y, pos_probs)\n\n# plot no skill roc curve\nplt.plot([0, 1], [0, 1], linestyle='--', label='No Skill')\n# plot model roc curve\nplt.plot(fpr, tpr, marker='.', label='Logistic')\n# axis labels\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\n# show the legend\nplt.legend()\n# show the plot\nplt.show()","d3ff6dbf":"# calculate the no skill line as the proportion of the positive class\nno_skill = len(Y[Y==1]) \/ len(Y)\n# plot the no skill precision-recall curve\nplt.plot([0, 1], [no_skill, no_skill], linestyle='--', label='No Skill')\n# calculate model precision-recall curve\nprecision, recall, _ = precision_recall_curve(test_Y, pos_probs)\n# plot the model precision-recall curve\nplt.plot(recall, precision, marker='.', label='Logistic')\n# axis labels\nplt.xlabel('Recall')\nplt.ylabel('Precision')\n# show the legend\nplt.legend()\n# show the plot\nplt.show()","5c400484":"# create a histogram of the predicted probabilities\nplt.hist(pos_probs, bins=100)\nplt.show()","a1834564":"explainer = LinearExplainer(ivis)\nexplainer.feature_importances_(train_X)","d5e89d40":"# Linear Classifier\ntrain a simple linear classifier to assess how well ivis learned the class representations.","e479d92b":"# Dimensionality Reduction\n\nNow, we can run ivis using default hyperparameters for supervised embedding problems:","d3097c8d":"Next, because ivis will learn a distance over observations, scaling must be applied to features. \n\nAdditionally, transforming the data to a range [0, 1] allows the neural network to extract more meaningful features.","9bdd9f22":"The Credit Card Fraud dataset is highly skewed, consisting of 492 frauds in a total of 284,807 observations (0.17% fraud cases). The features consist of numerical values from the 28 \u2018Principal Component Analysis (PCA)\u2019 transformed features, as well as Time and Amount of a transaction.\n\nIn this analysis we will train ivis algorithm using a 5% stratified subsample of the dataset. Our previous experiments have shown that ivis can yield >90% accurate embeddings using just 1% of the total data.","38941fbe":"# Based on [https:\/\/bering-ivis.readthedocs.io\/en\/latest\/metric_learning.html ](http:\/\/)","de61864f":"# **Visualisations**","3a143ea5":"# The Precision-Recall Curve for the Logistic Regression","48ac658b":"# **ROC Curve: Plot of False Positive Rate (x) vs. True Positive Rate (y)**","78ae0693":"Embed the training set and extrapolate learnt embeddings to the testing set"}}