{"cell_type":{"590104c3":"code","1ba94c74":"code","0f8fdcc4":"code","94d83c28":"code","cc9f07f8":"code","60b16d21":"code","31a7e360":"code","5820b8c5":"code","dbad4b83":"code","db07bda2":"code","f435bde7":"markdown","8b228d98":"markdown","c18f0fa4":"markdown","a4283e93":"markdown","c14f9c60":"markdown","ce1f0f2f":"markdown","b68fcd97":"markdown","846f5d52":"markdown","05a3219c":"markdown"},"source":{"590104c3":"import time\nimport numpy as np\nimport h5py\nimport matplotlib.pyplot as plt\nimport scipy\nfrom PIL import Image\nfrom scipy import ndimage\nimport skimage\nnp.random.seed(1)","1ba94c74":"def load_data():\n    train_dataset = h5py.File('\/kaggle\/input\/cat-images-dataset\/train_catvnoncat.h5', \"r\")\n    train_set_x_orig = np.array(train_dataset[\"train_set_x\"][:]) \n    train_set_y_orig = np.array(train_dataset[\"train_set_y\"][:]) \n\n    test_dataset = h5py.File('\/kaggle\/input\/cat-images-dataset\/test_catvnoncat.h5', \"r\")\n    test_set_x_orig = np.array(test_dataset[\"test_set_x\"][:]) \n    test_set_y_orig = np.array(test_dataset[\"test_set_y\"][:]) \n\n    classes = np.array(test_dataset[\"list_classes\"][:]) \n    \n    train_set_y_orig = train_set_y_orig.reshape((1, train_set_y_orig.shape[0]))\n    test_set_y_orig = test_set_y_orig.reshape((1, test_set_y_orig.shape[0]))\n    \n    return train_set_x_orig, train_set_y_orig, test_set_x_orig, test_set_y_orig, classes\n\nx_train,y_train,x_test,y_test,classes = load_data()","0f8fdcc4":"index = 19\nplt.imshow(x_train[index])\nprint (\"y = \" + str(y_train[:,index]) + \", it's a '\" + classes[np.squeeze(y_train[:,index])].decode(\"utf-8\") +  \"' picture.\")","94d83c28":"index = 21\nplt.imshow(x_train[index])\nprint (\"y = \" + str(y_train[:,index]) + \", it's a '\" + classes[np.squeeze(y_train[:,index])].decode(\"utf-8\") +  \"' picture.\")","cc9f07f8":"x_train = x_train.reshape(x_train.shape[0], -1).T\nx_train = x_train \/ 255\nx_test = x_test.reshape(x_test.shape[0], -1).T\nx_test = x_test \/ 255\nprint('No. of features: ', x_train.shape[0] ,'\\nNo. of training instances:' , x_train.shape[1])","60b16d21":"def initialize_parameters_deep(layer_dims):\n    np.random.seed(1)\n    parameters={}\n    L=len(layer_dims)\n    for l in range(1,L):\n        parameters['W'+str(l)]=np.random.randn(layer_dims[l],layer_dims[l-1])\/np.sqrt(layer_dims[l-1])\n        parameters['b'+str(l)]=np.zeros((layer_dims[l],1))\n    return parameters\n\ndef linear_forward(A,W,b):\n    Z=W.dot(A)+b\n    cache=(A,W,b)\n    return Z,cache\n\ndef sigmoid(Z):\n    A=1\/(1+np.exp(-Z))\n    cache=Z\n    return A,cache\n\ndef relu(Z):\n    A=np.maximum(0,Z)\n    cache=Z\n    return A,cache\n\ndef linear_activation_forward(A_prev,W,b,activation):\n    if(activation==\"sigmoid\"):\n        Z,linear_cache=linear_forward(A_prev,W,b)\n        A,activation_cache=sigmoid(Z)\n    elif(activation==\"relu\"):\n        Z,linear_cache=linear_forward(A_prev,W,b)\n        A,activation_cache=relu(Z)\n    cache=(linear_cache,activation_cache)\n    return A,cache\n\ndef L_model_forward(X,parameters):\n    L=len(parameters)\/\/2\n    caches=[]\n    A=X\n    for l in range(1,L):\n        A_prev=A\n        A,cache=linear_activation_forward(A_prev,parameters['W'+str(l)],parameters['b'+str(l)],activation=\"relu\")\n        caches.append(cache)\n    AL,cache=linear_activation_forward(A,parameters['W'+str(L)],parameters['b'+str(L)],activation=\"sigmoid\")\n    caches.append(cache)\n    return AL,caches\n\ndef compute_cost(AL,Y):\n    m=Y.shape[1]\n    cost = (1.\/m) * (-np.dot(Y,np.log(AL).T) - np.dot(1-Y, np.log(1-AL).T))\n    cost=np.squeeze(cost)\n    return cost","31a7e360":"def linear_backward(dZ,cache):\n    A_prev,W,b=cache\n    m=A_prev.shape[1]\n    dW = 1.\/m * np.dot(dZ,A_prev.T)\n    db = 1.\/m * np.sum(dZ, axis = 1, keepdims = True)\n    dA_prev = np.dot(W.T,dZ)\n    return dA_prev,dW,db\n\ndef relu_backward(dA, cache):\n    Z = cache\n    dZ = np.array(dA, copy=True)    \n    dZ[Z <= 0] = 0\n    assert (dZ.shape == Z.shape)\n    return dZ\n\ndef sigmoid_backward(dA, cache):\n    Z = cache\n    s = 1\/(1+np.exp(-Z))\n    dZ = dA * s * (1-s)\n    assert (dZ.shape == Z.shape)\n    return dZ\n\ndef linear_activation_backward(dA,cache,activation):\n    linear_cache, activation_cache = cache\n    if activation == \"relu\":\n        dZ = relu_backward(dA, activation_cache)\n        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n    elif activation == \"sigmoid\":\n        dZ = sigmoid_backward(dA, activation_cache)\n        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n    return dA_prev, dW, db\n\ndef L_model_backward(AL, Y, caches):\n    grads = {}\n    L = len(caches) \n    m = AL.shape[1]\n    Y = Y.reshape(AL.shape)\n    dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n    current_cache = caches[L-1]\n    grads[\"dA\" + str(L)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = linear_activation_backward(dAL, current_cache, activation = \"sigmoid\")\n    for l in reversed(range(L-1)):\n        current_cache = caches[l]\n        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads[\"dA\" + str(l + 2)], current_cache, activation = \"relu\")\n        grads[\"dA\" + str(l + 1)] = dA_prev_temp\n        grads[\"dW\" + str(l + 1)] = dW_temp\n        grads[\"db\" + str(l + 1)] = db_temp\n    return grads\n\ndef update_parameters(parameters, grads, learning_rate):\n    L = len(parameters) \/\/2\n    for l in range(L):\n        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate * grads[\"dW\" + str(l+1)]\n        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate * grads[\"db\" + str(l+1)]\n    return parameters","5820b8c5":"def L_layer_model(X, Y, layers_dims, learning_rate = 0.0075, num_iterations = 3000, print_cost=False):#lr was 0.009\n    np.random.seed(1)\n    costs = []              \n    parameters = initialize_parameters_deep(layers_dims)\n    for i in range(0, num_iterations):\n        AL, caches = L_model_forward(X, parameters)\n        cost = compute_cost(AL, Y)\n        grads = L_model_backward(AL, Y, caches)\n        parameters = update_parameters(parameters, grads, learning_rate=learning_rate)\n        if print_cost and i % 100 == 0:\n            print (\"Cost after %i iterations: %f\" %(i, cost))\n        if print_cost and i % 100 == 0:\n            costs.append(cost)\n    plt.plot(np.squeeze(costs))\n    plt.ylabel('cost')\n    plt.xlabel('iterations (per tens)')\n    plt.title(\"Learning rate =\" + str(learning_rate))\n    plt.show()\n    return parameters","dbad4b83":"layers_dims = [12288, 20, 7, 5, 1]\nparameters = L_layer_model(x_train, y_train, layers_dims, num_iterations = 2500, print_cost = True)","db07bda2":"def predict(X, y, parameters):\n    m = X.shape[1]\n    n = len(parameters) \/\/ 2\n    p = np.zeros((1,m))\n    probas, caches = L_model_forward(X, parameters)\n    for i in range(0, probas.shape[1]):\n        if probas[0,i] > 0.5:\n            p[0,i] = 1\n        else:\n            p[0,i] = 0\n    print(\"Accuracy: \"  + str(np.sum((p == y)\/m)))\n    return p\n\npred_train = predict(x_train, y_train, parameters)\npred_test = predict(x_test,y_test, parameters)","f435bde7":"The objective of this kernel is to build a cat detector by training a multilayer Neural Network on the dataset of 209 images. The validation set contains 50 images. The resolution of images is 64 by 64 RGB pixels which make a total of 12,288 input features.","8b228d98":"Calculating gradients and updating parameters...","c18f0fa4":"Making predictions and computing the accuracy on the training and the test datasets:","a4283e93":"Fetching the traing and the test datasets...","c14f9c60":"Defining the model's architecture, training the data, and plotting the learning curve...","ce1f0f2f":"Some example of classes...","b68fcd97":"Initializing and forward propagation...","846f5d52":"Building the optimization function...","05a3219c":"Vectorizing and standardizing the datasets..."}}