{"cell_type":{"614f94a7":"code","70d20463":"code","544eaecd":"code","624e184d":"code","143d6ac6":"code","06dfd7c2":"code","0b73ddfc":"code","a3e38d22":"code","a11b9c68":"code","c0d1c375":"code","0e247c16":"code","7629eac4":"code","229ec716":"code","c4bf95d9":"code","11c21350":"code","116b2917":"code","8aae88a4":"code","274adf7b":"code","430fccce":"code","5d1b6fb9":"code","6df4e721":"code","e3fb23c2":"code","53b6d802":"code","446b7651":"code","a10b0703":"code","b0b0c9ec":"code","546dc3d2":"code","d97df17e":"code","dd6fea6a":"code","fdb84d3f":"code","07524705":"code","78bd58fb":"code","2075de69":"code","0e7335f0":"code","0e9a5e0d":"code","0e22fc60":"code","f4ad5f02":"code","7858b504":"code","f424f5dd":"code","35211ae9":"code","d7c0f5b3":"code","a2b1eed3":"code","7eb258bc":"code","06d936fc":"code","bc04dfb5":"code","57ee84b8":"code","9abcff96":"code","07045b91":"code","cc1522dd":"code","46264123":"code","25ce58e0":"code","4e2611a0":"code","e1cb7b66":"code","73002cbc":"code","08ed00cd":"code","f2430760":"code","9944e481":"code","b4079b63":"code","dda70719":"code","aa7abca9":"code","12b55f74":"code","5098968a":"code","4eec4e48":"code","5fdc1e5b":"code","50e46f7b":"code","2abe2f9c":"code","f1164f25":"code","071b5662":"code","0a8d6f08":"code","080a9388":"code","5fe0d354":"code","c695cfb7":"code","e9c7922d":"code","d559bf80":"code","9647cca3":"code","4556e63d":"code","63c7303f":"code","957ad086":"code","81cd0971":"code","61b396bd":"code","bf413e7a":"code","2105c103":"code","38a72a82":"code","1385ac8e":"code","291bd075":"code","b4f49ae6":"code","97fe0bd8":"code","6d42f398":"code","478037fa":"code","90dc731c":"code","bd722727":"code","999c8fd6":"code","86a5b478":"code","abd9fbda":"code","e1e8943d":"code","fe8f0c70":"code","177bd6ea":"code","de1a408d":"code","8460b234":"code","1777323f":"code","dfe58d94":"code","99ea7d42":"code","8cd68d0d":"code","65042f33":"code","44968b1c":"code","678ca41e":"code","81fa5e84":"code","45a0f582":"code","7574374a":"code","ec2fb9dc":"code","4ad4e25f":"code","beb9b1f6":"code","ecec2bab":"markdown","7d85c41c":"markdown","af1ef5ac":"markdown","8e910e85":"markdown","6c8b0c67":"markdown","4bde03fa":"markdown","593f145a":"markdown","43340913":"markdown","ede59de4":"markdown","ed3b421c":"markdown","c6b7544b":"markdown","95fc38b2":"markdown","3bf602a4":"markdown","d2eec23f":"markdown","6b661dce":"markdown","442b91b5":"markdown","dbf51481":"markdown","f3a75493":"markdown","ac8bec31":"markdown","937e9d4e":"markdown","257e29af":"markdown","8066df77":"markdown","39f534e9":"markdown","0eaba17a":"markdown","dd586b86":"markdown","da068b3e":"markdown","7052add9":"markdown","ba6d90fe":"markdown","bb613792":"markdown","efaa5bc2":"markdown","a9d5bf00":"markdown","4a304938":"markdown","6fb2e5d9":"markdown","572dd0f2":"markdown","023d52b9":"markdown","73bedcd8":"markdown"},"source":{"614f94a7":"import numpy as np\nimport pandas as pd\nimport re\nimport seaborn as sns\nimport warnings\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport gc, sys\ngc.enable()","70d20463":"warnings.simplefilter(\"ignore\")\n# warnings.resetwarnings()","544eaecd":"df_train = pd.read_csv('..\/input\/train_V2.csv', sep=',')\n# df_test = pd.read_csv('..\/input\/test_V2.csv', sep=',')","624e184d":"# Memory saving function credit to https:\/\/www.kaggle.com\/gemartin\/load-data-reduce-memory-usage\ndef reduce_mem_usage(df):\n    # iterate through all the columns of a dataframe and modify the data type\n     #   to reduce memory usage.        \n    \n    #start_mem = df.memory_usage().sum() \/ 1024**2\n    #print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n\n    for col in df.columns:\n        col_type = df[col].dtype\n\n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n\n    #end_mem = df.memory_usage().sum() \/ 1024**2\n    #print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    #print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) \/ start_mem))\n\n    return df","143d6ac6":"df_train = reduce_mem_usage(df_train)","06dfd7c2":"# sampled_matches = pd.Series(df_train.matchId.unique()).sample(frac=1\/4, random_state=42)\n# df_train = df_train[df_train.matchId.isin(sampled_matches)]\n# df_train.groupby(\"matchId\").size().mean()","0b73ddfc":"df_train.head(10)","a3e38d22":"df_train.info()","a11b9c68":"df_train.describe()","c0d1c375":"df_train.loc[pd.notnull(df_train['winPlacePerc']) == False]","0e247c16":"df_train = df_train[pd.notnull(df_train['winPlacePerc'])]","7629eac4":"df_train.loc[pd.notnull(df_train['winPlacePerc']) == False]","229ec716":"total_train = df_train.isnull().sum().sort_values(ascending=False)\npercent = (df_train.isnull().sum()\/df_train.isnull().count()).sort_values(ascending=False)*100\nmissing_data = pd.concat([total_train, percent], axis=1,join='outer', keys=['Total Missing Count', ' % of Total Observations'])\nmissing_data.index.name ='Feature'\nmissing_data.head(10)","c4bf95d9":"df_train = df_train.assign(team_size=df_train.groupby('groupId').groupId.transform('count'))\ndf_train = df_train.assign(max_team_size=df_train.groupby('matchId').team_size.transform('max'))\ndf_train = df_train.assign(match_size=df_train.groupby('matchId').Id.transform('nunique'))","11c21350":"df_train =  df_train.assign(team_indicator = df_train.team_size.apply(lambda x: 5 if x>= 5 else x))\n\ndf_train = pd.get_dummies(df_train, columns=['team_indicator'])\ndummy_cols = ['team_indicator_{}'.format(i) for i in np.arange(1,6)]\ndf_train[dummy_cols] = df_train.groupby('matchId')[dummy_cols].transform('mean')","116b2917":"df_train.loc[df_train.team_indicator_1 >= 0.7, 'game_mode'] = 'solo'\ndf_train.loc[df_train.team_indicator_2 >= 0.6, 'game_mode'] = 'duo'\ndf_train.loc[(df_train.team_indicator_3 + df_train.team_indicator_4) >= 0.5, 'game_mode'] = 'squad'\ndf_train.game_mode = np.where((df_train.team_indicator_5 >= 0.2), 'custom', df_train.game_mode)\ndf_train.game_mode = df_train.game_mode.fillna('custom')","8aae88a4":"print('Shape before dropping custom games: ' + str(df_train.shape))\ndf_train = df_train.loc[df_train['game_mode'] != 'custom']\nprint('Shape after dropping custom games: ' + str(df_train.shape))","274adf7b":"num_feat_train = df_train.select_dtypes(include=[np.number])\nprint(num_feat_train.info())\n\ncorrmat = num_feat_train.corr() \ncols = corrmat.nlargest(25, 'winPlacePerc').index # nlargest : Return this many descending sorted values\ncm = np.corrcoef(num_feat_train[cols].values.T)\n\n# correlation \nsns.set(font_scale=1.25)\nf, ax = plt.subplots(figsize=(15, 12))\nhm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 8}, \n                 yticklabels=cols.values, xticklabels=cols.values)\nplt.show()","430fccce":"def plot_hist(x, title, bin_count=50):\n    \n    fig, ax = plt.subplots(figsize=(13,7))\n    formatter = plt.FuncFormatter(lambda x, y: '{:,.2f}'.format(x))\n    \n    ax.yaxis.set_major_formatter(formatter=formatter)\n    ax.xaxis.set_major_formatter(formatter=formatter)\n    b = bin_count\n\n    ax.set_title(title)\n    sns.distplot(x, bins=b, kde=True, ax=ax, color='darkred')","5d1b6fb9":"print('The average winning percentile is {:.3f}, the median is {:.3f}'.format(df_train['winPlacePerc'].mean(), df_train['winPlacePerc'].median()))","6df4e721":"plt.clf()\nsns.set_style(\"darkgrid\")\nplot_hist(df_train['winPlacePerc'], title='Distribution of winning percentiles')","e3fb23c2":"df_train = df_train.assign(match_mean = df_train.groupby('matchId')['winPlacePerc'].transform('mean'))\ndf_train = df_train.assign(match_median = df_train.groupby('matchId')['winPlacePerc'].transform('median'))","53b6d802":"print('The average match winning percentile is {:.2f}, the median is {:,.2f}'.format(df_train.winPlacePerc.mean(), df_train.winPlacePerc.median()))","446b7651":"plot_hist(df_train.match_mean, title='Distribution of average match winning percentiles')\nplot_hist(df_train.match_median, title='Distribution of median match winning percentiles')","a10b0703":"plot_hist(df_train['boosts'], title='Distribution of boost usage')","b0b0c9ec":"def plot_joint(x_feat, y_feat, df):\n    sns.jointplot(x=x_feat, y=y_feat, data=df, kind='reg', color='darkred')\n    # sns.jointplot(boosts_df['high_boosts'], boosts_df['matchDuration'], kind='reg', color='darkred', scatter_kws={'edgecolor':'w'}, line_kws={'color':'black'})","546dc3d2":"sampled_matches = pd.Series(df_train.matchId.unique()).sample(2000, random_state=42)\ndf_train_sample = df_train[df_train.matchId.isin(sampled_matches)]\ndf_train_sample.groupby(\"matchId\").size().mean()","d97df17e":"# because most losing players don't use very many boosts\n\nboosts_df = pd.concat([df_train_sample['winPlacePerc'], df_train_sample['matchDuration'], df_train_sample['boosts']], axis=1)\n\nboosts_mnPlusStd = boosts_df['boosts'].mean() + boosts_df['boosts'].std()\n# boosts_df['high_boosts'] = [i for i in boosts_df['boosts'] if i > boosts_df['boosts'].mean()]\nboosts_df['high_boosts'] = [i if i > boosts_mnPlusStd else 'None' for i in boosts_df['boosts']]\nboosts_df = pd.DataFrame(boosts_df.loc[boosts_df['high_boosts'] != 'None'].astype(float))","dd6fea6a":"plt.clf()\nsns.set_style(\"darkgrid\")\nsns.set(rc={'figure.figsize':(15,12)})\nplot_joint('winPlacePerc', 'boosts', df_train_sample)","fdb84d3f":"plt.clf()\nsns.set_style(\"darkgrid\")\nsns.set(rc={'figure.figsize':(15,12)})\nplot_joint('high_boosts', 'matchDuration', boosts_df)\n","07524705":"long_boosts_df = pd.DataFrame(boosts_df.loc[boosts_df['matchDuration'] > boosts_df['matchDuration'].mean()].astype(int))\nshort_boosts_df = pd.DataFrame(boosts_df.loc[boosts_df['matchDuration'] < boosts_df['matchDuration'].mean()].astype(int))","78bd58fb":"plt.clf()\nsns.set_style(\"darkgrid\")\nsns.set(rc={'figure.figsize':(15,12)})\nplot_joint('high_boosts', 'matchDuration', long_boosts_df)","2075de69":"plt.clf()\nsns.set_style(\"darkgrid\")\nsns.set(rc={'figure.figsize':(15,12)})\nplot_joint('high_boosts', 'matchDuration', short_boosts_df)","0e7335f0":"plt.clf()\nsns.set_style(\"darkgrid\")\nsns.set(rc={'figure.figsize':(15,12)})\nplot_joint('heals', 'matchDuration', df_train_sample)","0e9a5e0d":"del df_train_sample, boosts_df, long_boosts_df, short_boosts_df\ngc.collect()","0e22fc60":"plot_hist(df_train[df_train['match_size']>=75]['match_size'], title='Distribution of players per game')","f4ad5f02":"# I used this to compare to the original dataset but it was too computationally expensive to justify its usage\n\n# df_train_copy = df_train_copy.assign(team_size_copyset=df_train_copy.groupby('groupId').groupId.transform('count'))\n# df_train_copy = df_train_copy.assign(max_team_size_copyset=df_train_copy.groupby('matchId').team_size_copyset.transform('max'))\n# df_train_copy = df_train_copy.assign(match_size_copyset=df_train_copy.groupby('matchId').Id.transform('nunique'))\n\n\n# print('The largest team in the original training dataset had {} team members'.format(df_train_copy.max_team_size_copyset.max()))\nprint('The largest team after dropping custom games has {} team members'.format(df_train.max_team_size.max()))","7858b504":"plot_hist(df_train.team_size, title='Distribution of team sizes')\nplt.xlim(0,20)","f424f5dd":"plot_hist(df_train.max_team_size, title='Distribution of maximum team size', bin_count=25)\nplt.xlim(0,20)","35211ae9":"plt.clf()\nfig = plt.figure(figsize=(10,7))\nfig.add_subplot(1,1,1)\nfig.autofmt_xdate()\nsns.set_style(\"darkgrid\")\nsns.countplot(x='matchType', data = df_train, palette='Reds_d')","d7c0f5b3":"plt.clf()\nfig = plt.figure(figsize=(10,7))\nfig.add_subplot(1,1,1)\nfig.autofmt_xdate()\nsns.set_style(\"darkgrid\")\nsns.countplot(x='game_mode', data = df_train, palette='Reds_d')","a2b1eed3":"# these objects are eventually used to drop all match type features ...NOT game_mode\n\nmatchType_keep = ['matchType_squad', 'matchType_solo-fpp', 'matchType_squad-fpp', 'matchType_duo-fpp',\n                  'matchType_solo', 'matchType_duo']\nmatchType_drop = ['matchType_squad', 'matchType_solo-fpp', 'matchType_squad-fpp', 'matchType_duo-fpp', \n                  'matchType_solo', 'matchType_duo', 'matchType_normal-squad-fpp', 'matchType_flaretpp', \n                  'matchType_crashfpp', 'matchType_normal-duo-fpp', 'matchType_crashtpp', 'matchType_normal-solo-fpp', \n                  'matchType_flarefpp', 'matchType_normal-solo', 'matchType_normal-squad', 'matchType_normal-duo']\n","7eb258bc":"plt.clf()\nsns.set_style(\"darkgrid\")\nplot_hist(df_train['walkDistance'], title='Distribution of distance traveled by walking per player')","06d936fc":"print('Number of players who traveled more than 8000 by walking: ' + str(len(df_train[df_train['walkDistance'] > 8000])))","bc04dfb5":"def remove_outliers_walk(data):\n    print('Number of walk distance outliers: ' + str(len(data.loc[data['walkDistance'] > 8000])))\n    data.drop(data.loc[data['walkDistance'] > 8000].index, inplace=True)\n    return str('Walk distance outliers removed.')","57ee84b8":"remove_outliers_walk(df_train)","9abcff96":"plt.clf()\nsns.set_style(\"darkgrid\")\nplot_hist(df_train['rideDistance'], title='Distribution of distance traveled by vehicle per player')","07045b91":"print('Number of players who traveled more than 15000 by vehicle: ' + str(len(df_train[df_train['rideDistance'] > 15000])))","cc1522dd":"def remove_outliers_ride(data):\n    print('Number of ride distance outliers: ' + str(len(data.loc[data['rideDistance'] > 15000])))\n    data.drop(data.loc[data['rideDistance'] > 15000].index, inplace=True)\n    return str('Ride distance outliers removed.')","46264123":"remove_outliers_ride(df_train)","25ce58e0":"plt.clf()\nsns.set_style(\"darkgrid\")\nplot_hist(df_train['swimDistance'], title='Distribution of distance swam per player')","4e2611a0":"print('Number of players who swam more than 500: ' + str(len(df_train[df_train['swimDistance'] > 500])))","e1cb7b66":"def remove_outliers_swim(data):\n    print('Number of swim distance outliers: ' + str(len(data.loc[data['swimDistance'] > 500])))\n    data.drop(data.loc[data['swimDistance'] > 500].index, inplace=True)\n    return str('Swim distance outliers removed.')","73002cbc":"remove_outliers_swim(df_train)","08ed00cd":"df_train['total_distance'] = df_train.rideDistance + df_train.swimDistance + df_train.walkDistance","f2430760":"plt.clf()\nsns.set_style(\"darkgrid\")\nplot_hist(df_train['total_distance'], title='Distribution of total distance traveled per player')","9944e481":"def remove_movecheat(data):\n    data['killsWithoutMoving'] = ((data['kills'] > 0) & (data['total_distance'] == 0))\n    print('Number of movement cheaters: ' + str(len(data.loc[data['killsWithoutMoving'] == True])))\n    data.drop(data.loc[data['killsWithoutMoving'] == True].index, inplace=True)\n    return str('Cheaters removed.')","b4079b63":"remove_movecheat(df_train)","dda70719":"plt.clf()\nsns.set_style(\"darkgrid\")\nplot_hist(df_train['roadKills'], title='Distribution of total kills while in vehicle per player')","aa7abca9":"def remove_outliers_road(data):\n    roadk1_mnPlus3Std = data[data['roadKills'] >= 1]['roadKills'].mean() + 3*data[data['roadKills'] >= 1]['roadKills'].std()\n    print('Number of vehicle kill outliers: ' + str(len(data.loc[data['roadKills'] > roadk1_mnPlus3Std])))\n    data.drop(data.loc[data['roadKills'] > roadk1_mnPlus3Std].index, inplace=True)\n    return str('Vehicle kill outliers removed.')","12b55f74":"remove_outliers_road(df_train)","5098968a":"plt.clf()\nsns.set_style(\"darkgrid\")\nplot_hist(df_train['kills'], title='Distribution of total kills per player')","4eec4e48":"def remove_outliers_kills_old(data):\n    kills_mnPlus3Std = data['kills'].mean() + 3*data['kills'].std()\n    print('Number of kill outliers: ' + str(len(data.loc[data['kills'] > kills_mnPlus3Std])))\n    data.drop(data.loc[data['kills'] > kills_mnPlus3Std].index, inplace=True)\n    return str('Kill outliers removed.')","5fdc1e5b":"def remove_outliers_kills(data):\n    print('Number of kill outliers: ' + str(len(data.loc[data['kills'] > 20])))\n    data.drop(data.loc[data['kills'] > 20].index, inplace=True)\n    return str('Kill outliers removed.')","50e46f7b":"remove_outliers_kills(df_train)","2abe2f9c":"plt.clf()\nsns.set_style(\"darkgrid\")\nplot_hist(df_train['longestKill'], title='Distribution of longest kill per player')","f1164f25":"def remove_outliers_longkills(data):\n    print('Number of longest kill outliers: ' + str(len(data.loc[data['longestKill'] > 800])))\n    data.drop(data.loc[data['longestKill'] > 800].index, inplace=True)\n    return str('Longest kill outliers removed.')","071b5662":"remove_outliers_longkills(df_train)","0a8d6f08":"plt.clf()\nsns.set_style(\"darkgrid\")\nplot_hist(df_train['weaponsAcquired'], title='Distribution of weapons acquired per player')","080a9388":"print('Number of players who acquired more than 50 weapons: ' + str(len(df_train[df_train['weaponsAcquired'] > 50])))\n","5fe0d354":"def remove_outliers_weapons(data):\n    print('Number of weapons acquired outliers: ' + str(len(data.loc[data['weaponsAcquired'] > 50])))\n    data.drop(data.loc[data['weaponsAcquired'] > 50].index, inplace=True)\n    return str('Weapons acquired outliers removed.')","c695cfb7":"remove_outliers_weapons(df_train)","e9c7922d":"df_train['heals_boosts'] = df_train['heals'] + df_train['boosts']","d559bf80":"plt.clf()\nsns.set_style(\"darkgrid\")\nplot_hist(df_train['heals_boosts'], title='Distribution of heals and boosts acquired per player')","9647cca3":"print('Number of players who used more than 35 heals\/boosts: ' + str(len(df_train[df_train['weaponsAcquired'] > 35])))","4556e63d":"def remove_outliers_heals_boosts(data):\n    print('Number of heals\/boosts used outliers: ' + str(len(data.loc[data['weaponsAcquired'] > 35])))\n    data.drop(data.loc[data['heals_boosts'] > 35].index, inplace=True)\n    return str('Heals\/boosts outliers removed.')","63c7303f":"remove_outliers_heals_boosts(df_train)","957ad086":"winPoints_clean = df_train.drop(df_train[df_train['winPoints'] < 1].index)\nwinPoints_clean = winPoints_clean['winPoints']\nwinPoints_mean = winPoints_clean.mean()\n\ndf_train['winPoints_imp'] = df_train['winPoints'].replace({0 : winPoints_mean})","81cd0971":"plot_hist(df_train['winPoints'], title='Distribution of winPoints')","61b396bd":"plot_hist(df_train['winPoints_imp'], title='Distribution of Mean-Imputed winPoints')","bf413e7a":"rankPoints_clean = df_train.drop(df_train[df_train['rankPoints'] < 1].index)\nrankPoints_clean = rankPoints_clean['rankPoints']\nrankPoints_mean = rankPoints_clean.mean()\n\ndf_train['rankPoints_imp'] = df_train['rankPoints'].replace({-1 : rankPoints_mean,\n                                                            0 : rankPoints_mean})","2105c103":"plot_hist(df_train['rankPoints'], title='Distribution of rankPoints')","38a72a82":"plot_hist(df_train['rankPoints_imp'], title='Distribution of Mean-Imputed rankPoints')","1385ac8e":"# df_train = pd.get_dummies(df_train, columns=['matchType'])\n\n# matchType_enc = df_train.filter(regex='matchType')\n# matchType_enc.head()","291bd075":"# df_train['solo_matchType'] = df_train['matchType_solo-fpp'] + df_train['matchType_solo']\n\n\ndf_train = df_train.drop(columns=matchType_drop, errors='ignore')","b4f49ae6":"def playersJoined_norm(list_of_cols, df):\n     for column in list_of_cols:\n        df[column] = df[column]*((100-df['match_size'])\/ 100 + 1)\n         ","97fe0bd8":"to_pJnorm = ['DBNOs', 'kills', 'maxPlace'] # 'damageDealt',","6d42f398":"playersJoined_norm(to_pJnorm, df_train)","478037fa":"def matchDuration_wt(list_of_cols, df):\n    for column in list_of_cols:\n        df[column] = (df[column] * df[column])\/ df['matchDuration'].mean()\n        df[column].clip(0)\n        ","90dc731c":"to_matchDwt = ['heals_boosts']","bd722727":"matchDuration_wt(to_matchDwt, df_train)","999c8fd6":"def engineer_features(data):\n    data['max_possible_kills'] = data.match_size - data.team_size\n    # data['total_distance'] = data.rideDistance + data.swimDistance + data.walkDistance\n    data['total_items_acquired'] = data.boosts + data.heals + data.weaponsAcquired\n    data['items_per_distance'] =  data.total_items_acquired\/data.total_distance\n    data['kills_per_distance'] = data.kills\/data.total_distance\n    data['knocked_per_distance'] = data.DBNOs\/data.total_distance\n    data['damage_per_distance'] = data.damageDealt\/data.total_distance\n    data['headshot_kill_rate'] = data.headshotKills\/data.kills\n    data['headshot_kill_rate'] = data['headshot_kill_rate'].fillna(data['headshot_kill_rate'].mean())\n    \n    data['max_kills_by_team'] = data.groupby(['matchId','groupId']).kills.transform('max')\n    data['total_team_damage'] = data.groupby(['matchId','groupId']).damageDealt.transform('sum')\n    data['total_team_kills'] =  data.groupby(['matchId','groupId']).kills.transform('sum')\n    data['total_team_items'] = data.groupby(['matchId','groupId']).total_items_acquired.transform('sum')\n    data['pct_killed'] = data.kills\/data.max_possible_kills\n    data['pct_knocked'] = data.DBNOs\/data.max_possible_kills\n    data['pct_team_killed'] = data.total_team_kills\/data.max_possible_kills\n    data['team_kill_points'] = data.groupby(['matchId','groupId']).killPoints.transform('sum')\n    data['team_kill_rank'] = data.groupby(['matchId','groupId']).killPlace.transform('mean')\n    data['max_kills_match'] = data.groupby('matchId').kills.transform('max')\n    data['total_kills_match'] = data.groupby('matchId').kills.transform('sum')\n    # data['total_distance_match'] = data.groupby('matchId').total_distance.sum()\n    # data['map_has_sea'] =  data.groupby('matchId').swimDistance.transform('sum').apply(lambda x: 1 if x>0 else 0)\n     \n    data = data.join(pd.get_dummies(data['game_mode']))\n    \n    data.fillna(0, axis=1, inplace=True)\n      \n    \n    return data\n","86a5b478":"engineer_features(df_train)","abd9fbda":"df_train = reduce_mem_usage(df_train)","e1e8943d":"def engineer_features2(is_train=True):\n    test_idx = None\n    if is_train: \n        print(\"processing train\")\n        df = df_train           \n\n        df = df[df['maxPlace'] > 1]\n    else:\n        print(\"processing test\")\n        df = df_test\n        test_idx = df['Id']\n    \n\n    \n    print(\"selected features to generate aggregates for\")\n    target = 'winPlacePerc'\n    features = [#'Id',\n                #'groupId',\n                #'matchId',\n                'assists',\n                #'boosts',\n                'damageDealt',\n                'DBNOs',\n                #'headshotKills',\n                #'heals',\n                'killPlace',\n                'killPoints',\n                'kills',\n                'killStreaks',\n                'longestKill',\n                #'matchDuration',\n                'maxPlace',\n                'numGroups',\n                #'rankPoints',\n                'revives',\n                #'rideDistance',\n                #'roadKills',\n                #'swimDistance',\n                'teamKills',\n                'vehicleDestroys',\n                #'walkDistance',\n                'weaponsAcquired',\n                #'winPoints',\n                #'winPlacePerc',\n                'team_size',\n                #'max_team_size',\n                'match_size',\n                'team_indicator_1',\n                'team_indicator_2',\n                'team_indicator_3',\n                'team_indicator_4',\n                #'team_indicator_5',\n                #'game_mode',\n                #'match_mean',\n                #'match_median',\n                'total_distance',\n                #'killsWithoutMoving',\n                'heals_boosts',\n                'winPoints_imp',\n                'rankPoints_imp',\n                'max_possible_kills',\n                'total_items_acquired',\n                'items_per_distance',\n                'kills_per_distance',\n                'knocked_per_distance',\n                'damage_per_distance',\n                'headshot_kill_rate',\n                #'max_kills_by_team',\n                'total_team_damage',\n                'total_team_kills',\n                'total_team_items',\n                'pct_killed',\n                'pct_knocked',\n                'pct_team_killed',\n                'team_kill_points',\n                'team_kill_rank',\n                #'max_kills_match',\n                'total_kills_match',\n                #'duo',\n                #'solo',\n                #'squad'\n    ]\n\n   \n  \n     \n    y = None\n        \n    if is_train: \n        print(\"get target\")\n        y = np.array(df.groupby(['matchId','groupId'])[target].agg('mean'), dtype=np.float64)\n        # features.remove(target)\n    \n    # else:\n    #     y = np.array(df.groupby(['matchId','groupId'])[target].agg('mean'), dtype=np.float64)\n\n    print(\"get group mean feature\")\n    agg = df.groupby(['matchId','groupId'])[features].agg('mean')\n    agg_rank = agg.groupby('matchId')[features].rank(pct=True).reset_index()\n    \n    if is_train: df_out = agg.reset_index()[['matchId','groupId']]\n    else: df_out = df[['matchId','groupId']]\n    #df_out = agg.reset_index()[['matchId','groupId']]\n        \n    df_out = df_out.merge(agg.reset_index(), suffixes=[\"\", \"\"], how='left', on=['matchId', 'groupId'])\n    df_out = df_out.merge(agg_rank, suffixes=[\"_mean\", \"_mean_rank\"], how='left', on=['matchId', 'groupId'])\n    \n    # print(\"get group sum feature\")\n    # agg = df.groupby(['matchId','groupId'])[features].agg('sum')\n    # agg_rank = agg.groupby('matchId')[features].rank(pct=True).reset_index()\n    # df_out = df_out.merge(agg.reset_index(), suffixes=[\"\", \"\"], how='left', on=['matchId', 'groupId'])\n    # df_out = df_out.merge(agg_rank, suffixes=[\"_sum\", \"_sum_rank\"], how='left', on=['matchId', 'groupId'])\n    \n    # print(\"get group sum feature\")\n    # agg = df.groupby(['matchId','groupId'])[features].agg('sum')\n    # agg_rank = agg.groupby('matchId')[features].agg('sum')\n    # df_out = df_out.merge(agg.reset_index(), suffixes=[\"\", \"\"], how='left', on=['matchId', 'groupId'])\n    # df_out = df_out.merge(agg_rank.reset_index(), suffixes=[\"_sum\", \"_sum_pct\"], how='left', on=['matchId', 'groupId'])\n    \n    print(\"get group max feature\")\n    agg = df.groupby(['matchId','groupId'])[features].agg('max')\n    agg_rank = agg.groupby('matchId')[features].rank(pct=True).reset_index()\n    df_out = df_out.merge(agg.reset_index(), suffixes=[\"\", \"\"], how='left', on=['matchId', 'groupId'])\n    df_out = df_out.merge(agg_rank, suffixes=[\"_max\", \"_max_rank\"], how='left', on=['matchId', 'groupId'])\n    \n    print(\"get group min feature\")\n    agg = df.groupby(['matchId','groupId'])[features].agg('min')\n    agg_rank = agg.groupby('matchId')[features].rank(pct=True).reset_index()\n    df_out = df_out.merge(agg.reset_index(), suffixes=[\"\", \"\"], how='left', on=['matchId', 'groupId'])\n    df_out = df_out.merge(agg_rank, suffixes=[\"_min\", \"_min_rank\"], how='left', on=['matchId', 'groupId'])\n    \n    print(\"get group size feature\")\n    agg = df.groupby(['matchId','groupId']).size().reset_index(name='group_size')\n    df_out = df_out.merge(agg, how='left', on=['matchId', 'groupId'])\n    \n    print(\"get match mean feature\")\n    agg = df.groupby(['matchId'])[features].agg('mean').reset_index()\n    df_out = df_out.merge(agg, suffixes=[\"\", \"_match_mean\"], how='left', on=['matchId'])\n    \n    # print(\"get match type feature\")\n    # agg = df.groupby(['matchId'])[matchType.columns].agg('mean').reset_index()\n    # df_out = df_out.merge(agg, suffixes=[\"\", \"_match_type\"], how='left', on=['matchId'])\n    \n    # print(\"get match size feature\")\n    # agg = df.groupby(['matchId']).size().reset_index(name='match_size')\n    # df_out = df_out.merge(agg, how='left', on=['matchId'])\n    \n    # df_out.drop([\"matchId\", \"groupId\"], axis=1, inplace=True)\n\n    X = df_out\n    \n    feature_names = list(df_out.columns)\n\n    del df, df_out, agg, agg_rank\n    gc.collect()\n\n    return X, y, feature_names, test_idx","fe8f0c70":"df_train, y_train, train_columns, _ = engineer_features2(is_train=True)","177bd6ea":"df_train = reduce_mem_usage(df_train)","de1a408d":"to_drop = ['winPoints', 'rankPoints', 'rideDistance', 'walkDistance', 'swimDistance', 'headshotKills',\n           'roadKills', 'max_team_size', 'match_mean', 'match_median', 'team_indicator_5', \n           'game_mode', 'killsWithoutMoving', 'killsWithoutMoving_mean', 'heals', 'boosts',\n           'match_mean_mean', 'rankPoints'] \n           \n\nto_drop_test = ['winPoints', 'rankPoints', 'rideDistance', 'walkDistance', 'swimDistance', 'headshotKills',\n                'roadKills', 'max_team_size', 'team_indicator_5', 'game_mode', 'killsWithoutMoving_mean',\n                'heals', 'boosts', 'match_mean_mean', 'winPlacePerc', 'rankPoints']\n                \n\nto_drop2 = ['Id', 'groupId', 'matchId', 'team_size']\n\ncategorical = ['matchId', 'groupId']","8460b234":"df_train.drop(labels=to_drop,inplace=True,axis=1, errors='ignore')","1777323f":"df_train['winPlacePerc'] = y_train","dfe58d94":"from sklearn.metrics import mean_absolute_error, r2_score\nfrom lightgbm import LGBMRegressor\n\ndef train_validation(df, train_size=0.9):\n    \n    unique_games = df.matchId.unique()\n    train_index = round(int(unique_games.shape[0]*train_size))\n    \n    np.random.seed(42)\n    np.random.shuffle(unique_games)\n    \n    train_Id = unique_games[:train_index]\n    validation_Id = unique_games[train_index:]\n    \n    train = df[df.matchId.isin(train_Id)]\n    validation = df[df.matchId.isin(validation_Id)]\n    \n    return train, validation\n    \ntrain, validation = train_validation(df_train)\n","99ea7d42":"train_weights = (1\/train['team_size'])\nvalidation_weights = (1\/validation['team_size'])\n","8cd68d0d":"train.drop(labels=to_drop2,inplace=True,axis=1, errors='ignore')\nvalidation.drop(labels=to_drop2,inplace=True,axis=1, errors='ignore')\n\nX_train = train.drop(labels=['winPlacePerc'],axis=1)\nX_val = validation.drop(labels=['winPlacePerc'], axis=1)\n\ny_train = train['winPlacePerc']\ny_val = validation['winPlacePerc']\n\nprint(str(X_train.shape) + str(X_val.shape))","65042f33":"del df_train\ngc.collect()","44968b1c":"lgbm = LGBMRegressor(objective='mae', n_estimators=4500,  \n                     learning_rate=0.03, num_leaves=300,\n                     max_depth=14,\n                     n_jobs=-1, random_state=42, verbose=50)\n\nlgb_reg = lgbm.fit(X_train, y_train, eval_set=[(X_val, y_val)], eval_metric='mae', early_stopping_rounds=100, verbose=50)    ","678ca41e":"# lgb_reg.save_model('model.txt', num_iteration=lgb_reg.best_iteration)","81fa5e84":"pd.DataFrame(sorted(zip(lgbm.feature_importances_, X_train.columns)))","45a0f582":"del X_train, y_train, X_val, y_val\ngc.collect()","7574374a":"df_test = pd.read_csv('..\/input\/test_V2.csv', sep=',')\ndf_test = reduce_mem_usage(df_test)","ec2fb9dc":"# test_id = df_test['Id']\n\ndf_test = df_test.assign(team_size=df_test.groupby('groupId').groupId.transform('count'))\ndf_test = df_test.assign(max_team_size=df_test.groupby('matchId').team_size.transform('max'))\ndf_test = df_test.assign(match_size=df_test.groupby('matchId').Id.transform('nunique'))\n\ndf_test =  df_test.assign(team_indicator = df_test.team_size.apply(lambda x: 5 if x>= 5 else x))\n\ndf_test = pd.get_dummies(df_test, columns=['team_indicator'])\ndummy_cols = ['team_indicator_{}'.format(i) for i in np.arange(1,6)]\ndf_test[dummy_cols] = df_test.groupby('matchId')[dummy_cols].transform('mean')\n\ndf_test.loc[df_test.team_indicator_1 >= 0.7, 'game_mode'] = 'solo'\ndf_test.loc[df_test.team_indicator_2 >= 0.6, 'game_mode'] = 'duo'\ndf_test.loc[(df_test.team_indicator_3 + df_test.team_indicator_4) >= 0.5, 'game_mode'] = 'squad'\ndf_test.game_mode = np.where((df_test.team_indicator_5 >= 0.2), 'custom', df_test.game_mode)\ndf_test.game_mode = df_test.game_mode.fillna('custom')\n\n# don't drop custom games for test set, game_mode is dropped in the big function\n\ndf_test['total_distance'] = df_test.rideDistance + df_test.swimDistance + df_test.walkDistance\n\ndf_test['heals_boosts'] = df_test['heals'] + df_test['boosts']\n\nwinPoints_clean = df_test.drop(df_test[df_test['winPoints'] < 1].index)\nwinPoints_clean = winPoints_clean['winPoints']\nwinPoints_mean = winPoints_clean.mean()\n\ndf_test['winPoints_imp'] = df_test['winPoints'].replace({0 : winPoints_mean})\n\n\nrankPoints_clean = df_test.drop(df_test[df_test['rankPoints'] < 1].index)\nrankPoints_clean = rankPoints_clean['rankPoints']\nrankPoints_mean = rankPoints_clean.mean()\n\ndf_test['rankPoints_imp'] = df_test['rankPoints'].replace({-1 : rankPoints_mean})\n\n\ndf_test = pd.get_dummies(df_test, columns=['matchType'])\n\nmatchType_enc = df_test.filter(regex='matchType')\n\nplayersJoined_norm(to_pJnorm, df_test)\n\nmatchDuration_wt(to_matchDwt, df_test)\n\ndf_test = engineer_features(df_test)\ndf_test, _, test_columns, test_id = engineer_features2(is_train=False)\n\ndf_test.drop(labels=to_drop_test,inplace=True, axis=1, errors='ignore')\ndf_test.drop(labels=to_drop2,inplace=True, axis=1, errors='ignore')\n\n\n","4ad4e25f":"df_test = reduce_mem_usage(df_test)","beb9b1f6":"results = pd.DataFrame()\nresults['Id'] = test_id\nresults['winPlacePerc'] = lgbm.predict(df_test, num_iteration=lgbm.best_iteration_)\nresults.winPlacePerc = results.winPlacePerc.clip(0, 1)\n\nresults.to_csv(\"submission.csv\", index=False)\nresults.head(20)","ecec2bab":"I do not play the game so I am unsure of why matchDuration is essentially split at around 1600. However, let's take a look at the relationship between boosts and match duration, by long match duration and short match duration.","7d85c41c":"## Model Development and Training","af1ef5ac":"Not only have we eliminated custom games (roughly) from our test set, we have also accounted for any outliers in team size using the lambda function above.","8e910e85":"## Preliminary Exploratory Data Analysis","6c8b0c67":"I really tried to keep the feature engineering all in one place, but we need to do a bit of work to drop the custom games and we are going to use the features we create here later too.","4bde03fa":"## Team and Match Sizes","593f145a":"I'm reducing memory usage again because these functions created features that take up too much memory and cause us to use up all the RAM available in the kernel if left untouched.","43340913":"It seems that winning players tend to use more boosts. Part of this is likely because they are alive for longer, which is why we will later weight boosts by match duration so that boosts are more important of a predictor as matches go on longer than average.","ede59de4":"I used the code below to sample the training dataset while I was modifying model parameters and whatnot.","ed3b421c":"#### Function to plot distributions","c6b7544b":"#### Distance Traveled","95fc38b2":"#### Normalizing By Match Size\nHere, we normalize some features based on count of players in the match.  Some other Kagglers have normalized damage dealt in this way but I feel that this is not the best technique because damage is not inherently tied to the number of players in the game. Yes, more players means more opportunity to deal damage, but the same player can receive damage, recover, then receive more damage, etc. However, kills and placement probably have more of a relationship with match size. I also normalized knock-outs, which I have not seen done elsewhere. If you're reading this and have any thoughts on the issue, drop them in the comments!","3bf602a4":"rankPoints is highly correlated with some of our other features and I tried dropping it but ended up with worse results. Just a note.","d2eec23f":"There seem to be some players who achieve a kill count that is wildly detached from the rest of the distribution. This is suspicious and we are going to remove them. We will choose 20 as the cutoff point.","6b661dce":"### Weapons Acquired, Longest Kill, and Heals\/Boosts\n\nAccording to the description of the data, the feature measuring longest kill can be misleading because a player can down another player and then drive away before they bleed out and the kill is actually awarded. A high value may also be an indication of cheating. Let's check the distribution and choose a point above which to remove outliers.","442b91b5":"I think it's safe to say that players who pick up an abnormal amount of weapons are not playing the game in a way that helps us to make accurate predictions.","dbf51481":"Many of the ideas below are pulled from these two kernels. Big thank you to the authors of both.\n\nhttps:\/\/www.kaggle.com\/rejasupotaro\/cheaters-and-zombies\n\nhttps:\/\/www.kaggle.com\/carlolepelaars\/pubg-data-exploration-rf-funny-gifs-v2\n\nI feel that most of the processes used in this section are self-explanatory. If anything is not clear, just leave a comment and I will get back to you.\n\n","f3a75493":"Both of the functions below were modified from functions other kernels but I can't find the first one. The second one is modified from the function in [this kernel](https:\/\/www.kaggle.com\/anycode\/simple-nn-baseline-3). We are really letting LightGBM do most of the work here by casting a wide net of potentially valuable features and feeding them into the algorithm for it to decide what is best to use.","ac8bec31":"## Boosts, Heals, and Match Length","937e9d4e":"This is a bit of leftover code from when I was attempting to capture match type and team size in a different way. I am leaving it in as an example of what not to do, and with one line active because I don't want to break anything.","257e29af":"Here, we are taking players who got at least one road kill. Then, we take the mean of that group (the mean road kills for players who had at least one road kill), and add 3 standard deviations to the mean. We remove the group of players who had more road kills than that statistic from our training dataset following the logic that such cases are not representative of player skill or success in a match. Most players do not even get a single road kill. At the very least, the players removed on these grounds were probably just driving around rather than playing strategically.","8066df77":"## Feature Descriptions\n\n**DBNOs** - Number of enemy players knocked.\n\n**assists** - Number of enemy players this player damaged that were killed by teammates.\n\n**boosts** - Number of boost items used.\n\n**damageDealt** - Total damage dealt. Note: Self inflicted damage is subtracted.\n\n**headshotKills** - Number of enemy players killed with headshots.\n\n**heals** - Number of healing items used.\n\n**Id** - Player\u2019s Id\n\n**killPlace** - Ranking in match of number of enemy players killed.\n\n**killPoints** - Kills-based external ranking of player. (Think of this as an Elo ranking where only kills matter.) If there is a value other than -1 in rankPoints, then any 0 in killPoints should be treated as a \u201cNone\u201d.\n\n**killStreaks** - Max number of enemy players killed in a short amount of time.\n\n**kills** - Number of enemy players killed.\n\n**longestKill** - Longest distance between player and player killed at time of death. This may be misleading, as downing a player and driving away may lead to a large longestKill stat.\n\n**matchDuration** - Duration of match in seconds.\n\n**matchId** - ID to identify match. There are no matches that are in both the training and testing set.\n\n**matchType** - String identifying the game mode that the data comes from. The standard modes are \u201csolo\u201d, \u201cduo\u201d, \u201csquad\u201d, \u201csolo-fpp\u201d, \u201cduo-fpp\u201d, and \u201csquad-fpp\u201d; other modes are from events or custom matches.\n\n**rankPoints** - Elo-like ranking of player. This ranking is inconsistent and is being deprecated in the API\u2019s next version, so use with caution. Value of -1 takes place of \u201cNone\u201d.\n\n**revives** - Number of times this player revived teammates.\n\n**rideDistance** - Total distance traveled in vehicles measured in meters.\n\n**roadKills** - Number of kills while in a vehicle.\n\n**swimDistance** - Total distance traveled by swimming measured in meters.\n\n**teamKills** - Number of times this player killed a teammate.\n\n**vehicleDestroys** - Number of vehicles destroyed.\n\n**walkDistance** - Total distance traveled on foot measured in meters.\n\n**weaponsAcquired** - Number of weapons picked up.\n\n**winPoints** - Win-based external ranking of player. (Think of this as an Elo ranking where only winning matters.) If there is a value other than -1 in rankPoints, then any 0 in winPoints should be treated as a \u201cNone\u201d.\n\n**groupId** - ID to identify a group within a match. If the same group of players plays in different matches, they will have a different groupId each time.\n\n**numGroups** - Number of groups we have data for in the match.\n\n**maxPlace** - Worst placement we have data for in the match. This may not match with numGroups, as sometimes the data skips over placements.\n\n**winPlacePerc** - The target of prediction. This is a percentile winning placement, where 1 corresponds to 1st place, and 0 corresponds to last place in the match. It is calculated off of maxPlace, not numGroups, so it is possible to have missing chunks in a match.","39f534e9":"I did a little bit of testing and got better results when I imputed winPoints and rankPoints with the mean of each. I think assuming that players for whom data is missing are of average skill helps to make the model more robust.","0eaba17a":"These objects just contain the names of features that were amalgamated into new features, used only for outlier detection, or were created by the functions but are not logical. ","dd586b86":"This is one of my first public kernels on Kaggle. It may be a bit rough around the edges, with some leftover code here and there, but it got me in the top 25% on the competition leaderboards as of November 5th, 2018. It's been a great learning experience and I'm excited to move on to something new!\n\nI also greatly appreciate any feedback.","da068b3e":"## Outlier Detection and Removal","7052add9":"#### Checking correlation","ba6d90fe":"Just taking a brief look at the data...","bb613792":"Remember, we simplified our categories earlier, from matchType to game_mode.","efaa5bc2":"It's gone! Let's check for other missing data in our train dataset...","a9d5bf00":"I am using a sample here because trying to plot all the points from the full dataset is too computationally expensive.","4a304938":"## Feature Engineering","6fb2e5d9":"## Cleaning\n\nThere is one row of data where the variable we are predicting is missing. Let's isolate it and drop it.","572dd0f2":"First we are going to remove outliers in distance traveled for each of the three features, then combine what is left over into a new feature measuring total distance traveled.","023d52b9":"#### Weighting Boosts\nWeighting boosts by match length. The logic here is that boost usage depends on what stage of progress the game is in, i.e., more boosts in later stages. This means that, as matches get longer than average, the importance of boosts\/heals increases and as matches get shorter, the importance of boosts decreases. I did this because I detected a slight increase in the number of boosts used in longer games in my exploratory data analysis. Intuition also helped, and reading online that people tend to save them for later in the game. Players who use more boosts have been alive for longer (part of why it is such an important predictor). In that same train of thought, players who use all of their boosts at the beginning of the game are more likely to be wiped out by players who save their boosts to use against other skilled players or are good enough that they can easily take out other players in the earliers stages of the game without using their boosts, instead saving them for later\n\nEssentially, what is happening here is that I am assigning more importance to boosts\/heals in longer games in the hopes of catching some of this effect. I also combined heals and boosts a bit earlier in the kernel because they perform a similar function in the game and because this method has been working well in other kernels.\n\nThis seems to add some predictive power when training on a sample, but not a huge amount. Due to time constraints, it is hard to measure the change in predictive power when training on the whole training dataset. We will leave this weight in our data and assume that the effect may be amplified when scaling up to the full-sized training dataset.\n\nDo you feel that this technique is valid? Let me know in the comments. I am eager for feedback.","73bedcd8":"#### Cheaters and\/or Maniacs\n\nThis function removes players who never moved but somehow managed to get kills. In other words, it removes cheaters."}}