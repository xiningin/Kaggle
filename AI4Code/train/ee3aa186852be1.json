{"cell_type":{"1997415b":"code","07dffc39":"code","0a216eda":"code","44c8bb63":"code","df16730e":"code","19279c21":"code","5b10da7a":"code","26b713da":"code","da3fd655":"code","bdeff4da":"code","0f590f2b":"code","5df8afce":"code","96637fe0":"code","4c1e53c4":"code","d9075e54":"code","d8e54226":"code","3872c9f4":"code","0ac39385":"code","beb06503":"code","732b811b":"code","bff53615":"code","8c384b58":"code","737eff87":"code","c61b75f2":"code","a075c1bd":"code","387a80c3":"markdown","0c2cfaa0":"markdown","f5330a2f":"markdown","9406a89a":"markdown","e4ec717d":"markdown","9747bb9f":"markdown","12f0e89f":"markdown","4032ae61":"markdown","c22b0458":"markdown","5751da40":"markdown","403b6993":"markdown","94a51c31":"markdown","e8916f19":"markdown"},"source":{"1997415b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport datetime as dt\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","07dffc39":"df= pd.read_csv('..\/input\/ecommerce-data\/data.csv')\n\n#copy data into new df for analysis\ndata=df.copy()\n\ndata.head()","0a216eda":"data.info()","44c8bb63":"#generate invoice month for each line purchase equal to the first day of the month when the purchase was made\ndata['InvoiceMonth'] = pd.to_datetime(data['InvoiceDate']).to_numpy().astype('datetime64[M]')\n\n#first invoice month for every customer\ndata['CohortMonth'] = data.groupby('CustomerID')['InvoiceMonth'].transform('min')\n\n#drop null values\ndata.dropna(inplace=True)\n\ndata.head()","df16730e":"#drop NaN values\ndata.dropna()\n\n#compute year and month from Invoice Date\ninvoice_year= data['InvoiceMonth'].dt.year.astype('int')\ninvoice_mon= data['InvoiceMonth'].dt.month.astype('int')\n\n#compute year and month from Cohort Date\ncohort_year= data['CohortMonth'].dt.year.astype('int')\ncohort_mon= data['CohortMonth'].dt.month.astype('int')\n\n#find the differences\ndiff_year = invoice_year - cohort_year\ndiff_mon = invoice_mon - cohort_mon\n\n#calculate the cohort index for each invoice\ndata['CohortIndex'] = diff_year * 12 + diff_mon + 1\ndata.head()","19279c21":"#group by cohort month and index and find number of unique customers for each grouping\ngrouped = data.groupby(['CohortMonth', 'CohortIndex'])['CustomerID'].apply(pd.Series.nunique)\\\n                                                                    .reset_index()\n#pivot the data with cohort month as rows and Cohort Index as columns\ngrouped = grouped.pivot(index='CohortMonth', columns='CohortIndex',  values='CustomerID')\ngrouped","5b10da7a":"#divide each column by value of the first(cohort size) to find retention rate\nsize = grouped.iloc[:,0]\nretention_table = grouped.divide(size, axis=0)\n\n#compute the percentage\nretention_table.round(3) * 100","26b713da":"plt.figure(figsize=(10, 8))\n\nsns.heatmap(data = retention_table,\n            annot = True,        \n            fmt = '.0%',         \n            vmin = 0.0,          \n            vmax = 0.5,           \n            cmap = 'BuPu')\nplt.show()","da3fd655":"#copy data into a new dataframe for analysis\ndata_all= df.copy()\n\n#calculating total the total amount for each line item (unit price * Quantity)\ndata_all['amount']= data_all['Quantity']* data_all['UnitPrice']\ndata_all.head()\n\n#convert column to datetime\ndata_all['InvoiceDate']=pd.to_datetime(data_all['InvoiceDate'])\n\n#setting date of analysis= 1 day after the most recent invoice\nanalysis_date = pd.to_datetime(data_all['InvoiceDate'].max())+ dt.timedelta(days=1)\ndata_all.head()","bdeff4da":"#calculate the recency, frequency and Monetary values for each customer\ngrouped = data_all.groupby(['CustomerID'])\\\n                .agg({'InvoiceDate': lambda x: (analysis_date - x.max()).days,\n                      'InvoiceNo': 'count',\n                      'amount': 'sum'})\\\n\n#rename each column to denote the R,F,M Values\ngrouped.rename(columns = {'InvoiceDate': 'R_val',\n                                   'InvoiceNo': 'F_val',\n                                   'amount': 'M_val'}, inplace=True)\ngrouped.reset_index().head()","0f590f2b":"#divide recency metric into 4 quartiles\nr_quartiles = pd.qcut(grouped['R_val'], 4, labels = range(4, 0, -1))\ngrouped = grouped.assign(R_quartile = r_quartiles.values.astype('int'))\n\n#divide frequency metric into 4 quartiles\nf_quartiles = pd.qcut(grouped['F_val'], 4, labels = range(1, 5, 1))\ngrouped = grouped.assign(F_quartile = f_quartiles.values.astype('int'))\n\n#divide monetary metric into 4 quartiles\nm_quartiles = pd.qcut(grouped['M_val'], 4, labels = range(1, 5, 1))\ngrouped = grouped.assign(M_quartile = m_quartiles.values.astype('int'))\n\ngrouped.head()\ngrouped.info()","5df8afce":"#get RFM segment by concatenation of R,F,M values\ngrouped['RFM_Seg']=grouped['R_quartile'].astype('str') + grouped['F_quartile'].astype('str') + grouped['M_quartile'].astype('str')\n\n#calculate RFM by summing R,F,M values\ngrouped['RFM_Score']= grouped[['R_quartile','F_quartile','M_quartile']].sum(axis=1)\ngrouped.head()","96637fe0":"#set sns theme\nsns.set_theme(style=\"whitegrid\")\n\n#set plot size\nfig, ax = plt.subplots(figsize=(20, 5))\n\n#plot count of each RFM segment\nsns.countplot(x=\"RFM_Seg\", data=grouped)\nplt.xticks(rotation=45)\n\nplt.show()","4c1e53c4":"def get_tier(a):\n    if a >9:\n        return 'Platinum'\n    elif (a>6) & (a<=9):\n        return 'Gold'\n    elif (a>3) & (a<=6):\n        return 'Advanced'\n    elif (a>0) & (a<=3):\n        return 'Basic' \n\n#assign a tier to each customer based on the get_tier function logic\ngrouped['Tier']=grouped['RFM_Score'].apply(get_tier)\ngrouped.head()","d9075e54":"df_reset= grouped.reset_index()\ntier_analysis=df_reset.groupby(['Tier'])\\\n        .agg({'R_val': 'mean',\n                      'F_val': 'mean',\n                      'M_val': 'mean'}).round(2)\nprint(tier_analysis,'\\n')\nprint(grouped['Tier'].value_counts())","d8e54226":"#find total customers in each country\ncountry_count=data_all.groupby(['Country']).size().to_frame('Total_Customers_in_country').reset_index()\ncountry_count.head()\n\n#merge with original df to add the 'Country' column\ndf_with_country = df_reset.merge(data_all[['CustomerID','Country']], how='inner', on='CustomerID')\nresult= df_with_country.groupby(['Tier','Country']).size().to_frame('Customer_count').reset_index()\ncountry_data= result.merge(country_count, how='inner', on='Country')\n\n#get filtered dataset for Basic Tier\nbasic_tier= country_data[country_data['Tier'].isin(['Basic'])]\n\n#calculate percentage of Basic Tier customers in each country\nbasic_tier['Basic_Tier_Percentage']= ((basic_tier['Customer_count']\/basic_tier['Total_Customers_in_country'])*100)\\\n                                           .round(3)\n#display top 5 countries with the largest percentage of Basic Tier Customers\nbasic_tier[['Country', 'Basic_Tier_Percentage']].sort_values('Basic_Tier_Percentage', ascending=False).head()","3872c9f4":"#get filtered dataset for Platinum Tier\nplatinum_tier= country_data[country_data['Tier'].isin(['Platinum'])]\n\n#calculate percentage of Platinum Tier customers in each country\nplatinum_tier['Platinum_Tier_Percentage']= ((platinum_tier['Customer_count']\/platinum_tier['Total_Customers_in_country'])*100)\\\n                                            .round(3)\n#display top 5 countries with the largest percentage of Platinum Tier Customers\nplatinum_tier[['Country','Platinum_Tier_Percentage']].sort_values('Platinum_Tier_Percentage', ascending=False).head()","0ac39385":"#plot the skewness of recency metric\nsns.distplot(grouped['R_val'])","beb06503":"#plot the skewness of frequency metric\nsns.distplot(grouped['F_val'])","732b811b":"#plot the skewness of monetary metric\nfig, ax = plt.subplots(figsize=(13, 7))\nsns.distplot(grouped['M_val'])","bff53615":"#log transformation of recency metric\nrecency_log= np.log(grouped['R_val'])\n\n#plot the transformed variable\nsns.distplot(recency_log)\nplt.show()","8c384b58":"#check for variance and mean of the variables\ngrouped.describe()","737eff87":"rfm = grouped[['R_val','F_val','M_val']]\n\n#making all values in M_val positive\nrfm['M_val']=rfm['M_val']+1\n\n#applying logarithmic transformation\nfor c in ['R_val', 'F_val']:\n    rfm[c]= np.log(rfm[c])\n    \n\n#Normalization of variables\nfrom sklearn.preprocessing import StandardScaler\n\necomm_standardized= StandardScaler().fit_transform(rfm)\nrfm[['R_val','F_val','M_val']]=ecomm_standardized.round(2)\n\nrfm.head()","c61b75f2":"#start k-means clusterig\nfrom sklearn.cluster import KMeans\n\nsse = {}\n\n#find the optimum number of clusters from 1 to 10\nfor k in range(1, 11):    \n    kmeans = KMeans(n_clusters=k, random_state=1)    \n    kmeans.fit(rfm)    \n    sse[k] = kmeans.inertia_ \n    \n# Plot SSE for each value of k    \nplt.title('The Elbow Method')\nplt.xlabel('k'); \nplt.ylabel('SSE')\nsns.pointplot(x=list(sse.keys()), y=list(sse.values()))\nplt.show()","a075c1bd":"#fit k-means with 3 clusters\nkmeans = KMeans(n_clusters=3, random_state=1)    \nkmeans.fit(rfm)\n\n#adding column with cluster labels to a new df\ncluster_table = grouped.assign(Cluster=kmeans.labels_)\n\n#group by cluster\nclustered_data = cluster_table.groupby(['Cluster'])\n\n#average RFM values for each cluster\nclustered_data.agg({\n    'R_val': 'mean',\n    'F_val': 'mean',\n    'M_val': 'mean'\n  }).round(2)","387a80c3":" ### 2. RFM Segmentation:\n RFM segmentation technique takes into account the past purchase behaviour and patterns of the customers to divide them into segments. Here 'R', 'F', 'M' denote Recency, Frequency and Monetary analysis respectively.\n \n* Recency: Number of days since a customer's last purchase\n* Frequency: Number of purchases by the customer\n* Monetary: Total amount of money spent by the customer on his purchases\n  \nWe will be sorting the customers into quartiles based on these three metrics, and calculating their RFM score. With the help of this score, we can effectively sort the customers into segments which can be used for targeting particular segments for campaigns, promotions or other personalized experiences.\n \nFor our ease of analysis, we'll be moving forward with the assumption that the date of our RFM analysis is  just the day after the most recent invoice date in the dataset.","0c2cfaa0":"From the above analysis, we can Singapore and Iceland have the most active and profitable customer base with 100% of them belonging to the Platinum Tier.\n\nWhile, Saudi Arabia does not have any customer outside the Basic Tier.","f5330a2f":"Next, let us compute the mean of the R-F-M values of each tier and also their counts.","9406a89a":"Now that we have the R,F,M quartiles, we'll move on to calculating the RFM scores as a sum total of all the three quartiles and assigning RFM segments.","e4ec717d":"Now, let's do an analysis of the tiers country-wise. This will help us understand which are the best and worst performing regions.\n\nWe will find out the top 5 countries with the largest percentage of the customers in the Basic Tier and the top 5 countries with the largest percentage of customers in the Platinum Tier.","9747bb9f":"We can clearly see that segment '111'(Worst customers) and segment '444'(best customers) have the highest count.\n\nNow as per the business requirements, we can analyse each segment. \n\nFirst, we group the customers into different tiers based on their total RFM Score. In decreasing order of their RFM scores, they are as follows:\n1. Platinum\n2. Gold\n3. Advanced\n4. Basic","12f0e89f":"We can see that the tiers have very distinctive properties, and they can be further analysed as per business needs.\n\nA few takeaways:\n\n1. Tiers with higher Recency values: These are the inactive customers. Surveys should be undertaken to understand their experience and pain-points with the store\/app and appropriate measures should be taken in an effort to re-engage them\n\n2. Tiers with lower Frequeny Values: These customers should be targeted with additional offers and campaigns from time to time to increase their frequency of purchase\n\n3. Tiers with lower Monetary Values: For these customers, marketing and pricing strategies need to be formulated to increase their basket value. This can be achieved by offering discounts on a minimum cart price or bulk purchase discounts","4032ae61":"Visualizing the total count of each RFM Segment:","c22b0458":"### 3. K-Means Clustering:\n\nNow, we will be approaching the segmentation using K-Means Clustering, a popular unsupervised learning algorithm. But before we start, we need to process the data to adhere to the following assumptions of K-Means Clustering with the techiques mentioned below:\n\n1. K-Means assumes that the variables are not skewed. We will test our R,F,M values. If they are skewed, we will use logarithmic transformation to eliminate the skewness\n\n2. K-Means assumes that all the variables have a similar mean and variance. Therefore, we will check the range and mean of each of the variables and if they are dissimilar, we will be using the Standard Scalar to normalize them","5751da40":"The above step concludes the pre-processig of the data. Now, we use the elbow criterion method to find the optinum number of clusters.","403b6993":"Let's now visualize the retention rates on a heatmap:","94a51c31":"### 1. Cohort Analysis:\n\nWe first divide the entire data into different cohorts to understand high-level trends. We use time cohort for our analysis here:","e8916f19":"In the table above, the first column values represents the size of every individual cohort, and the subsequent columns represent the number of active customers for that cohort in the subsequent months."}}