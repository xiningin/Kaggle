{"cell_type":{"da64171f":"code","a9cb233f":"code","a7d7e28a":"code","dedbe4cf":"code","62c4900a":"code","3571b552":"code","42d3dc84":"code","527e7a2c":"code","b6a2b415":"code","649f4797":"code","1d832414":"code","b7919afe":"code","2eb0028f":"code","e61f2489":"code","10437e5f":"code","5dbb26f3":"code","dac67c32":"code","feb8b99e":"code","c19a8336":"code","ac87ba53":"code","bbe0a1eb":"code","c4142c56":"code","60acc3b9":"code","43abbf4e":"code","3a3a2c48":"code","e0c7ba2f":"code","6ea4fe7e":"code","2ec5c5e0":"code","283c361f":"code","bc1fd3e1":"code","bf25e20c":"code","637c4e95":"code","a2835214":"markdown","67d09bb5":"markdown","6aa74a3c":"markdown","41cea283":"markdown","144077b3":"markdown","cb5f1127":"markdown","8b154844":"markdown","8b56157b":"markdown","e8d9ca90":"markdown","45aa4334":"markdown","36908308":"markdown","6b7aea4e":"markdown","a2ddd5f5":"markdown","8c905f18":"markdown","73bbe81b":"markdown","7c025519":"markdown","5c26520e":"markdown","30868f97":"markdown","d0937764":"markdown","871bc3e2":"markdown","5907d9f8":"markdown","15782d5e":"markdown","cbb1db5d":"markdown","9dbe3ae8":"markdown","cd54930a":"markdown","796a6df6":"markdown","58286169":"markdown"},"source":{"da64171f":"import numpy as np\nimport pandas as pd\nimport os\nimport re\nimport spacy\nfrom spacy.lang.en import English\nimport nltk\nfrom nltk import word_tokenize, pos_tag\nfrom nltk.stem.wordnet import WordNetLemmatizer\nfrom nltk.corpus import wordnet as wn\nimport gensim\nfrom gensim import corpora\nimport pickle\nfrom html.parser import HTMLParser\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as mpatches\nimport matplotlib.dates as mdates\nfrom collections import Counter\n\ndata_folder = \"..\/input\/\"\n\nemails = pd.read_csv(os.path.join(data_folder, 'emails.csv'))\nquestions = pd.read_csv(os.path.join(data_folder, 'questions.csv'))\nmatches = pd.read_csv(os.path.join(data_folder, 'matches.csv'))\n\nprofs = pd.read_csv(os.path.join(data_folder, 'professionals.csv'))\ntag_users = pd.read_csv(os.path.join(data_folder, 'tag_users.csv'))\nstuds = pd.read_csv(os.path.join(data_folder, 'students.csv'))\ntagq = pd.read_csv(os.path.join(data_folder, 'tag_questions.csv'))\ngroups = pd.read_csv(os.path.join(data_folder, 'groups.csv'))\nemails = pd.read_csv(os.path.join(data_folder, 'emails.csv'))\ngroupm = pd.read_csv(os.path.join(data_folder, 'group_memberships.csv'))\nanswers = pd.read_csv(os.path.join(data_folder, 'answers.csv'))\ncomments = pd.read_csv(os.path.join(data_folder, 'comments.csv'))\nmatches = pd.read_csv(os.path.join(data_folder, 'matches.csv'))\ntags = pd.read_csv(os.path.join(data_folder, 'tags.csv'))\nquestions = pd.read_csv(os.path.join(data_folder, 'questions.csv'))\nschool = pd.read_csv(os.path.join(data_folder, 'school_memberships.csv'))\nhearts = pd.read_csv(os.path.join(data_folder, 'answer_scores.csv'))","a9cb233f":"profs['professionals_date_joined'] = pd.to_datetime(profs['professionals_date_joined'])\nprofs['join_yr']=profs['professionals_date_joined'].dt.year\nprofsyr = profs[['join_yr','professionals_id']].groupby('join_yr').count().reset_index()\n\nplt.bar(profsyr.join_yr, profsyr.professionals_id)\n\nplt.xlabel('Year')\nplt.ylabel('Number of Professionals Added')\nplt.title('Professionals Joining CV')\nplt.show()","a7d7e28a":"profs['professionals_industry']=profs['professionals_industry'].fillna(\"\")\nprofs['professionals_industry'] = profs['professionals_industry'].str.replace('healthcare','health care', flags=re.IGNORECASE, regex=True)\nprof_ind = profs['professionals_industry'].tolist()\nprof_ind=list(filter(lambda a: a != \"\", prof_ind))\nprof_ind=list(filter(lambda a: a != \"and\", prof_ind))\nprof_ind=list(filter(lambda a: a != \"or\", prof_ind))\nprof_tokens=[]\nfor i in prof_ind:\n    words = word_tokenize(i)\n    words=[word.lower() for word in words if word.isalpha()]\n    prof_tokens.append(words)\n    \nprof_tokens = [item for sublist in prof_tokens for item in sublist]\nprof_tokens=list(filter(lambda a: a != \"and\", prof_tokens))\nprof_tokens=list(filter(lambda a: a != \"or\", prof_tokens))","dedbe4cf":"dic=dict(Counter(prof_tokens))\nproft = pd.DataFrame.from_dict(dic, orient=\"index\").reset_index()\nproft.columns=['type','occur']\nproft=proft.sort_values(by=['occur'], ascending=False)\nproft.shape","62c4900a":"proftx=proft.type[:19].tolist()\nproftx.append('others')\nprofty=proft.occur[:19].tolist()\nprofty.append(proft.occur.sum()-proft.occur[:19].sum())\nfig1, ax1 = plt.subplots(figsize=(10, 8))\nax1.pie(profty, labels=proftx, autopct='%1.1f%%',startangle=90)\nax1.axis('equal')\nplt.show()","3571b552":"proft.tail(10)","42d3dc84":"proftlist=proft.type.tolist()\nfor i in range(len(proftlist)):\n    for j in range(i+1, len(proftlist)):\n        word1 = wn.synsets(proftlist[i], 'n')\n        word2 = wn.synsets(proftlist[j], 'n')\n        if (word1 and word2):\n            if (word1[0].wup_similarity(word2[0])>0.95):\n                proftlist[j]=proftlist[i]","527e7a2c":"proft.type=proftlist\nproft = proft.groupby('type')['occur'].sum().reset_index()\nproft.shape","b6a2b415":"ansauth=answers.groupby('answers_author_id')['answers_body'].count().reset_index()\nprofans = pd.merge(profs, ansauth, how=\"left\", left_on='professionals_id', right_on='answers_author_id')\nprofans.shape, profans[~profans['answers_author_id'].isna()].shape","649f4797":"noinfoauth = np.setdiff1d(answers.answers_author_id,profs.professionals_id)\nnoinfoauth.shape, answers[answers['answers_author_id'].isin(noinfoauth)].shape","1d832414":"ans = answers[['answers_id','answers_date_added']]\nans['answers_date_added'] = pd.to_datetime(ans['answers_date_added'])\nans['year']=ans['answers_date_added'].dt.year\nans=ans.groupby('year')['answers_id'].count().reset_index()\nplt.bar(ans.year, ans.answers_id)\n\nplt.xlabel('Year')\nplt.ylabel('Answers Added')\nplt.title('Number of Answers')\nplt.show()","b7919afe":"hearts[hearts['score']!=0].shape","2eb0028f":"ansh = pd.merge(answers, hearts, how=\"left\", left_on='answers_id', right_on='id')\nansh['answers_date_added'] = pd.to_datetime(ansh['answers_date_added'])\nansh['year']=ansh['answers_date_added'].dt.year\nansh = ansh.groupby('year')['score'].mean().reset_index()\nplt.bar(ansh.year, ansh.score)\n\nplt.xlabel('Year')\nplt.ylabel('Average Score')\nplt.title('Trend of Average Answer Scores')\nplt.show()\n","e61f2489":"matches['matches_email_id'].value_counts().head()","10437e5f":"maxq = matches[matches['matches_email_id']==569938]\nmaxq = pd.merge(maxq, questions, how=\"left\", left_on=['matches_question_id'], right_on=['questions_id'])\nmaxq[['matches_email_id','questions_title','questions_body']].head()","5dbb26f3":"fastans = answers.groupby('answers_question_id')['answers_date_added'].min().reset_index()\nqatime=pd.merge(questions, fastans, how=\"left\", left_on='questions_id', right_on='answers_question_id')\nqatime['delta']=pd.to_datetime(qatime['answers_date_added']) -pd.to_datetime(qatime['questions_date_added'])\nbin1=qatime[qatime['delta']<=pd.Timedelta('1 days')].shape[0]\nbin2=qatime[qatime['delta']<=pd.Timedelta('3 days')].shape[0]-bin1\nbin3=qatime[qatime['delta']<=pd.Timedelta('7 days')].shape[0]-bin2\nbin4=qatime[qatime['delta']>pd.Timedelta('7 days')].shape[0]","dac67c32":"labels=['1 day','2-3 days','4-7 days','>1 week']\nprofty=[bin1, bin2, bin3, bin4]\nfig1, ax1 = plt.subplots(figsize=(6, 6))\nax1.pie(profty, labels=labels, autopct='%1.1f%%',startangle=90)\nax1.axis('equal')\nplt.show()","feb8b99e":"class MLStripper(HTMLParser):\n    def __init__(self):\n        self.reset()\n        self.strict = False\n        self.convert_charrefs= True\n        self.fed = []\n    def handle_data(self, d):\n        self.fed.append(d)\n    def get_data(self):\n        return ''.join(self.fed)\n\ndef strip_tags(html):\n    s = MLStripper()\n    s.feed(html)\n    return s.get_data()\n\nuri_re = r'(?i)\\b((?:https?:\/\/|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}\/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:\\'\".,<>?\u00ab\u00bb\u201c\u201d\u2018\u2019]))'\n\ndef strip_html(s):\n    s = s.replace(\"\\n\",\"\")\n    return re.sub(uri_re, ' ', str(s))","c19a8336":"questions['full_text'] = questions['questions_title'] +' '+ questions['questions_body']\nquestions['full_text'] = questions['full_text'].apply(strip_tags)\nanswers = answers[['answers_question_id','answers_body']]\nanswers['answers_body'] = answers['answers_body'].fillna(\"\")\nanswers['answers_body'] = answers['answers_body'].apply(strip_tags)\nanswers['answers_per_q'] = answers.groupby('answers_question_id')['answers_body'].transform(lambda x: '.'.join(x))\nansrs=answers[['answers_question_id','answers_per_q']]\nansrs=ansrs.drop_duplicates()\nqa = pd.merge(questions, ansrs, how=\"left\", left_on='questions_id', right_on='answers_question_id')\nqa['qa'] = qa['full_text'] +' '+ qa['answers_per_q']\nqa['qa']=qa['qa'].fillna(\"\")\nqa=qa[~qa['answers_per_q'].isna()]","ac87ba53":"answers = pd.read_csv(os.path.join(data_folder, 'answers.csv'))\nanswers['answers_body']=answers['answers_body'].fillna(\"\")\nanswers['answers_body'] = answers['answers_body'].apply(strip_tags)\n\nanswers['proans']=answers.groupby('answers_author_id')['answers_body'].transform(lambda x: '.'.join(x))\nanswers=answers[['answers_author_id','proans']]\nanswers=answers.drop_duplicates()","bbe0a1eb":"profans = pd.merge(profs, answers, how=\"left\", left_on='professionals_id', right_on='answers_author_id')\ntag_users = pd.read_csv(os.path.join(data_folder, 'tag_users.csv'))\n\ntag_users=pd.merge(tag_users, tags, how=\"left\", left_on='tag_users_tag_id', right_on='tags_tag_id')\ntag_users['alltags']=tag_users.groupby('tag_users_user_id')['tags_tag_name'].transform(lambda x: '. '.join(x))\ntag_users = tag_users[['tag_users_user_id','alltags']]\ntag_users=tag_users.drop_duplicates()\n\nprofanstag = pd.merge(profans, tag_users, how=\"left\", left_on='professionals_id', right_on='tag_users_user_id')\nprofanstag['allprof']=profanstag['proans']+' '+profanstag['alltags']\nprofanstag['allprof']=profanstag['allprof'].fillna(\"\")","c4142c56":"spacy.load('en')\nstoplist=nltk.corpus.stopwords.words('english')\nstoplist.append('school')\nstoplist.append('college')\nstoplist.append('career')\nstoplist.append('degree')\nstoplist.append('people')\nstoplist.append('experience')\nen_stop = set(stoplist)\ndef tokenize(text):\n    lda_tokens = [token for token, pos in pos_tag(word_tokenize(text)) if pos.startswith('N')]\n    return lda_tokens\n\ndef get_lemma(word):\n    lemma = wn.morphy(word)\n    if lemma is None:\n        return word\n    else:\n        return lemma\n\ndef get_lemma2(word):\n    return WordNetLemmatizer().lemmatize(word)\n\ndef prepare_text_for_lda(text):\n    tokens = tokenize(text)\n    tokens = [token for token in tokens if len(token) > 4]\n    tokens = [token for token in tokens if token not in en_stop]\n    tokens = [get_lemma(token) for token in tokens]\n    return tokens","60acc3b9":"fullcorp = qa['qa'].tolist()+profanstag['allprof'].tolist()","43abbf4e":"text_data = []\nfor line in fullcorp:\n    tokens = prepare_text_for_lda(line[:1000000])\n    text_data.append(tokens)\n    \ndictionary = corpora.Dictionary(text_data)\ncorpus = [dictionary.doc2bow(text) for text in text_data]\n\npickle.dump(corpus, open('corpus.pkl', 'wb'))\ndictionary.save('dictionary.gensim')","3a3a2c48":"NUM_TOPICS = 30\nldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics = NUM_TOPICS, id2word=dictionary, passes=15)\nldamodel.save('model5.gensim')\ntopics = ldamodel.print_topics(num_words=5)\nfor topic in topics:\n    print(topic)","e0c7ba2f":"proftopics = ldamodel.get_document_topics(corpus)\ntopicdf = pd.DataFrame.from_records([{v:k for v, k in row} for row in proftopics])","6ea4fe7e":"topicdf=topicdf.tail(profans.shape[0])","2ec5c5e0":"topicdf['prof_id']=profans['professionals_id']","283c361f":"def getTopicForQuery (question):\n    temp = question.lower()\n    print (temp)\n\n    words = re.findall(r'\\w+', temp, flags = re.UNICODE )\n    words=[word.lower() for word in words if word.isalpha()]\n\n    important_words = []\n    important_words = filter(lambda x: x not in stoplist, words)\n\n    dictionary = corpora.Dictionary.load('dictionary.gensim')\n\n    ques_vec = []\n    ques_vec = dictionary.doc2bow(important_words)\n\n    topic_vec = []\n    topic_vec = ldamodel[ques_vec]\n\n    word_count_array = np.empty((len(topic_vec), 2), dtype = np.object)\n    for i in range(len(topic_vec)):\n        word_count_array[i, 0] = topic_vec[i][0]\n        word_count_array[i, 1] = topic_vec[i][1]\n\n    idx = np.argsort(word_count_array[:, 1])\n    idx = idx[::-1]\n    word_count_array = word_count_array[idx]\n\n    return word_count_array","bc1fd3e1":"question = 'What is the required education for airline pilots? #airline-industry'\ntopix_array = getTopicForQuery (question)\ntopix=[]\nfor each in topix_array:\n    topix.append(each[0])","bf25e20c":"topicdf=topicdf.sort_values(by=topix, ascending=False)","637c4e95":"topicdf.head()['prof_id'].tolist()","a2835214":"Let us take a look at the maximum number of questions per email.","67d09bb5":"We also merge the tags with the answers given by professionals for stronger topic correlations.","6aa74a3c":"Range of Response Times","41cea283":"Our entire corpus will now include the questions+answers, and the professionals+answers.","144077b3":"Answers with Hearts","cb5f1127":"Let us try grouping similar words and reducing the number of industry topics.","8b154844":"The question can be sent to professionals with ids-","8b56157b":"Out of a total of 51,138 answers, only 13837 or 27% have hearts.","e8d9ca90":"Professionals","45aa4334":"We will develop an LDA (Latent  Dirichlet allocation) on the entire corpus of \n* questions and answers, where each document will contain the question with all associated answers, as well as \n* all the answers given by an individual professional. \nThis will give us better association between related terms.\nWe will extract the top topics, and any new question will be classified according to these topics. This will be matched against the professionals' data to get the most relevant professionals.","36908308":"There are 1182 professional industry topics. Let us take a look at the distribution.","6b7aea4e":"Conversely, there are 102 answer authors who have added 1017 answers, but are not listed in the professionals database. Information about their professional affiliation has to be retrieved from analysing their previous answers.","a2ddd5f5":"There are 10,067 professionals who have not answered any questions.","8c905f18":"Least Represented Professions","73bbe81b":"Next, we merge the answers given by each professional to create documents relevant to that professional.","7c025519":"Get the topics for the documents in the entire corpus.","5c26520e":"Functions for data cleaning","30868f97":"Merge questions title with body including tags. Merge all answers given for an individual question. Finally, merge the question with its answers.","d0937764":"A single email may have several unrelated questions, so a more targeted approach would have a better response rate.","871bc3e2":"Get the topics relevant to the professionals.","5907d9f8":"Most Represented Professions","15782d5e":"**Recommendation System**","cbb1db5d":"Tokenize the corpus and create the dictionary mapping.","9dbe3ae8":"Run the LDA model to generate topics.","cd54930a":"**Emails**","796a6df6":"For any given question, find the topics relevant to the question, and sort the professionals database by the topics to find the most relevant professionals.","58286169":"**Exploratory Data Analysis**********"}}