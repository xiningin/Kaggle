{"cell_type":{"0fbc248e":"code","8519caa1":"code","e89af67b":"code","795b67c4":"code","cc43b9d6":"code","58255301":"code","47f9ae52":"code","6146b8fd":"code","486a8566":"markdown","a038fbaa":"markdown","3e16d9e2":"markdown","d2f9d865":"markdown","1e0bdc1e":"markdown"},"source":{"0fbc248e":"from fastai.tabular.all import *","8519caa1":"%%time\nSPLIT_IDX = 2493988 # train\/val split at 80% of time_ids\ndata_df = pd.read_feather('..\/input\/ubiquant-trainfeather-32-bit\/train32.feather')\nval_df = data_df.iloc[SPLIT_IDX:].copy()\n\nftrs = [f'f_{i}' for i in range(300)]\nfeature_tensor = torch.tensor(data_df[ftrs].to_numpy()).cuda()\ntarget_tensor = torch.tensor(data_df.target.to_numpy()).cuda()\n\ndel data_df","e89af67b":"class UbiquantDataset:\n    def __init__(self, feature_tensor, targets):\n        store_attr()\n        self.n_inp = 2\n    def __getitem__(self, idx):\n        return torch.empty(0),self.feature_tensor[idx], self.targets[idx, None]\n    \n    def __len__(self):\n        return len(self.feature_tensor)\n    \nclass UbiDL(DataLoader):\n    def __iter__(self):\n        if self.shuffle:\n            self.__idxs = torch.tensor(range(0,self.n))\n        else:\n            self.__idxs = torch.tensor(range(0,self.n))\n        for batch_start in range(0, self.n, self.bs):\n            if batch_start + self.bs > self.n and self.drop_last:\n                return \n            indices = self.__idxs[batch_start:batch_start+self.bs]\n            yield self.dataset[indices]","795b67c4":"def pearson_coef(data):\n    return data.corr()['target']['preds']\n\nclass CompMetric(AccumMetric):\n    def __init__(self, val_df):\n        super().__init__(None)\n        self.val_df = val_df\n        \n    @property\n    def name(self):\n        return 'Pears'\n        \n    @property\n    def value(self):\n        preds = torch.cat(self.preds)\n        val_df['preds'] = preds.cpu().numpy()\n        return np.mean(self.val_df[['time_id', 'target', 'preds']].groupby('time_id').apply(pearson_coef))","cc43b9d6":"def pearson_loss(x, y):\n    xd = x - x.mean()\n    yd = y - y.mean()\n    nom = (xd*yd).sum()\n    denom = ((xd**2).sum() * (yd**2).sum()).sqrt()\n    return 1 - nom \/ denom","58255301":"ds_train = UbiquantDataset(feature_tensor[:SPLIT_IDX], target_tensor[:SPLIT_IDX])\nds_val = UbiquantDataset(feature_tensor[SPLIT_IDX:], target_tensor[SPLIT_IDX:])\n\ndls = DataLoaders.from_dsets(ds_train, ds_val , bs = 4096,dl_type=UbiDL, num_workers=0)","47f9ae52":"model = TabularModel(emb_szs={}, n_cont=300, out_sz=1, layers = [128,64, 32,16]).cuda()\n\nlearn = Learner(dls, model, loss_func=pearson_loss, metrics = CompMetric(val_df))","6146b8fd":"%%time\nlearn.fit(5, 1e-3)","486a8566":"### Barebone lightweight dataloading","a038fbaa":"### A custom metric and loss function for training","3e16d9e2":"### Load the data from feather and stick on GPU","d2f9d865":"### Use fast.ai Learner for trainig","1e0bdc1e":"The goal of this notebook is to provide a lightweight and fast starting point for training MLP models. It uses a very simple model and is optimized for speed so the whole thing including dataloading and training takes around a minute and half to run on kaggle. Still using the same parameters I got a respectable single model score: `0.143` when trained on the whole dataset.\n\nThings to note:\n- All the data is preloaded into GPU and then directly passed to model without any copying. \n- I'm using a custom version of pearson coefficient for loss.\n- There is no regularization, just a small model and small number of epochs to prevent overfitting.\n- A custom callback allows me to track the competition metric while training.\n- A flat constant learning rate, I found that LR scheduling didn't help with just five epochs.\n- A large batch size makes training fast."}}