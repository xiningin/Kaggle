{"cell_type":{"aa045a3a":"code","a03a7b6b":"code","4b13956a":"code","3a8b20d4":"code","7786eb77":"code","9ebf1b16":"code","a5ebcfb1":"code","5d4bba38":"code","168b3d18":"code","13fb621d":"code","5cfa4839":"code","578c8fd7":"code","b1be5cfd":"code","528c3aee":"code","81c38f44":"code","8e3524d0":"code","4e91075e":"code","3baaba46":"code","a2bd044f":"code","4b054a55":"code","fb8e2571":"code","d14bcb8f":"code","30acb175":"code","be12322d":"code","ce728985":"code","86b04328":"code","1793d006":"code","5685a79b":"code","971a91e0":"code","1c49edfe":"code","dfbf7476":"code","835039a0":"code","d3c00f1c":"code","fa9bce75":"code","09ccb7ba":"code","eab72830":"code","c5e059ad":"code","a43953af":"code","4b66a34f":"code","4d445619":"code","57b9b6a1":"code","71876667":"code","01064998":"code","08934cd1":"markdown","f267c6e8":"markdown","741b0b7b":"markdown","baf427d9":"markdown","39c52e8a":"markdown","490065d8":"markdown","efa8e7f1":"markdown","dc5115a1":"markdown","b63ea376":"markdown"},"source":{"aa045a3a":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom plotly import tools\nimport plotly.plotly as py\nfrom plotly.offline import init_notebook_mode, iplot\ninit_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.figure_factory as ff\nfrom IPython.display import HTML, Image\n\ndf = pd.read_csv('..\/input\/diabetes.csv')","a03a7b6b":"df.head(10)","4b13956a":"df.describe()","3a8b20d4":"f, ax = plt.subplots(1, 2, figsize = (15, 7))\nf.suptitle(\"Diabetes?\", fontsize = 18.)\n_ = df.Outcome.value_counts().plot.bar(ax = ax[0], rot = 0, color = (sns.color_palette()[0], sns.color_palette()[2])).set(xticklabels = [\"No\", \"Yes\"])\n_ = df.Outcome.value_counts().plot.pie(labels = (\"No\", \"Yes\"), autopct = \"%.2f%%\", label = \"\", fontsize = 13., ax = ax[1],\\\ncolors = (sns.color_palette()[0], sns.color_palette()[2]), wedgeprops = {\"linewidth\": 1.5, \"edgecolor\": \"#F7F7F7\"}), ax[1].texts[1].set_color(\"#F7F7F7\"), ax[1].texts[3].set_color(\"#F7F7F7\")","7786eb77":"fig, ax = plt.subplots(4,2, figsize=(25,25))\nsns.distplot(df.Age, bins = 20, ax=ax[0,0]) \nsns.distplot(df.Pregnancies, bins = 20, ax=ax[0,1]) \nsns.distplot(df.Glucose, bins = 20, ax=ax[1,0]) \nsns.distplot(df.BloodPressure, bins = 20, ax=ax[1,1]) \nsns.distplot(df.SkinThickness, bins = 20, ax=ax[2,0])\nsns.distplot(df.Insulin, bins = 20, ax=ax[2,1])\nsns.distplot(df.DiabetesPedigreeFunction, bins = 20, ax=ax[3,0]) \nsns.distplot(df.BMI, bins = 20, ax=ax[3,1]) ","9ebf1b16":"sns.regplot(x='BMI', y= 'Insulin', data=df)","a5ebcfb1":"sns.pairplot(data=df,hue='Outcome')","5d4bba38":"fig,ax = plt.subplots(nrows=4, ncols=2, figsize=(18,18))\nplt.suptitle('Violin Plots',fontsize=24)\nsns.violinplot(x=\"Pregnancies\", data=df,ax=ax[0,0],palette='Set3')\nsns.violinplot(x=\"Glucose\", data=df,ax=ax[0,1],palette='Set3')\nsns.violinplot (x ='BloodPressure', data=df, ax=ax[1,0], palette='Set3')\nsns.violinplot(x='SkinThickness', data=df, ax=ax[1,1],palette='Set3')\nsns.violinplot(x='Insulin', data=df, ax=ax[2,0], palette='Set3')\nsns.violinplot(x='BMI', data=df, ax=ax[2,1],palette='Set3')\nsns.violinplot(x='DiabetesPedigreeFunction', data=df, ax=ax[3,0],palette='Set3')\nsns.violinplot(x='Age', data=df, ax=ax[3,1],palette='Set3')\nplt.show()","168b3d18":"corr=df.corr()\n\nsns.set(font_scale=1.15)\nplt.figure(figsize=(14, 10))\n\nsns.heatmap(corr, vmax=.8, linewidths=0.01,\n            square=True,annot=True,cmap='YlGnBu',linecolor=\"black\")\nplt.title('Correlation between features');","13fb621d":"from sklearn.metrics import accuracy_score\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\n\nX = df.iloc[:, :-1]\ny = df.iloc[:, -1]\n\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)","5cfa4839":"#Model\nDT = DecisionTreeClassifier(max_depth=3)\n\n#fiting the model\nDT.fit(X_train, y_train)\n\n#prediction\ny_pred = DT.predict(X_test)\n\n#Accuracy\nprint(\"Accuracy\", DT.score(X_test, y_test)*100)\n\n#Plot the confusion matrix\nsns.set(font_scale=1.5)\ncm = confusion_matrix(y_test, y_pred)\nsns.heatmap(cm, annot=True, fmt='g')\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\nplt.tight_layout()\nplt.show()\nprint('accuracy   ', (cm[0, 0] + cm[1, 1])\/ (cm[0, 1] + cm[0, 0] + cm[1, 1] + cm[1, 0])*100)\nprint('specificity', cm[0, 0] \/ (cm[0, 1] + cm[0, 0])*100)\nprint('sensitivity', cm[1, 1] \/ (cm[1, 1] + cm[1, 0])*100)","578c8fd7":"from sklearn.tree import export_graphviz\nimport pydot\n\nfeature_list = X.columns.values\n# Save the tree as a png image\nexport_graphviz(DT, out_file = 'diabetes.dot', feature_names = feature_list, rounded = True, precision = 1, filled = True, class_names=['negative','positive'])\n(graph, ) = pydot.graph_from_dot_file('diabetes.dot')\ngraph.write_png('diabetes.png');\nImage('diabetes.png')","b1be5cfd":"feature_import = pd.DataFrame(data=DT.feature_importances_, index=feature_list, columns=['values'])\nfeature_import.sort_values(['values'], ascending=False, inplace=True)\nfeature_import.transpose()","528c3aee":"from sklearn.ensemble import RandomForestClassifier\n\n#Model\nRFC = RandomForestClassifier(n_estimators=500, bootstrap=True)\n\n#fiting the model\nRFC.fit(X_train, y_train)\n\n#prediction\ny_pred = RFC.predict(X_test)\n\n#Accuracy\nprint(\"Accuracy\", RFC.score(X_test, y_test)*100)\n\n#Plot the confusion matrix\nsns.set(font_scale=1.5)\ncm = confusion_matrix(y_test, y_pred)\nsns.heatmap(cm, annot=True, fmt='g')\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\nplt.tight_layout()\nplt.show()\nprint('accuracy   ', (cm[0, 0] + cm[1, 1])\/ (cm[0, 1] + cm[0, 0] + cm[1, 1] + cm[1, 0])*100)\nprint('specificity', cm[0, 0] \/ (cm[0, 1] + cm[0, 0])*100)\nprint('sensitivity', cm[1, 1] \/ (cm[1, 1] + cm[1, 0])*100)","81c38f44":"# Save the tree as a png image\nexport_graphviz(RFC.estimators_[0], out_file = 'diabetes_RFC.dot', feature_names = feature_list, rounded = True, precision = 1, filled = True, class_names=['negative','positive'])\n(graph, ) = pydot.graph_from_dot_file('diabetes_RFC.dot')\ngraph.write_png('diabetes_RFC.png');\nImage('diabetes_RFC.png')","8e3524d0":"feature_import = pd.DataFrame(data=RFC.feature_importances_, index=feature_list, columns=['values'])\nfeature_import.sort_values(['values'], ascending=False, inplace=True)\nfeature_import.transpose()","4e91075e":"from sklearn.ensemble import GradientBoostingClassifier\n\ngbrt = GradientBoostingClassifier(n_estimators=200)\ngbrt.fit(X_train, y_train)\n\ny_pred = gbrt.predict(X_test)\n#Accuracy\nprint(\"Accuracy\", gbrt.score(X_test, y_test)*100)\n\n#Plot the confusion matrix\nsns.set(font_scale=1.5)\ncm = confusion_matrix(y_test, y_pred)\nsns.heatmap(cm, annot=True, fmt='g')\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\nplt.tight_layout()\nplt.show()\nprint('accuracy   ', (cm[0, 0] + cm[1, 1])\/ (cm[0, 1] + cm[0, 0] + cm[1, 1] + cm[1, 0])*100)\nprint('specificity', cm[0, 0] \/ (cm[0, 1] + cm[0, 0])*100)\nprint('sensitivity', cm[1, 1] \/ (cm[1, 1] + cm[1, 0])*100)","3baaba46":"# Save the tree as a png image\nexport_graphviz(gbrt.estimators_[1, 0], out_file = 'diabetes_GB.dot', feature_names = feature_list, rounded = True, precision = 1, filled = True, class_names=['negative','positive'])\n(graph, ) = pydot.graph_from_dot_file('diabetes_GB.dot')\ngraph.write_png('diabetes_GB.png');\nImage('diabetes_GB.png')","a2bd044f":"feature_import = pd.DataFrame(data=gbrt.feature_importances_, index=feature_list, columns=['values'])\nfeature_import.sort_values(['values'], ascending=False, inplace=True)\nfeature_import.transpose()","4b054a55":"from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train_std = sc.fit_transform(X_train)\nX_test_std = sc.transform(X_test)","fb8e2571":"from sklearn.svm import SVC\nsvc = SVC(kernel='linear')\nsvc.fit(X_train_std, y_train)\n\ny_pred = svc.predict(X_test_std)\n#Accuracy\nprint(\"Accuracy\", svc.score(X_test_std, y_test)*100)\n\n#Plot the confusion matrix\nsns.set(font_scale=1.5)\ncm = confusion_matrix(y_test, y_pred)\nsns.heatmap(cm, annot=True, fmt='g')\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\nplt.tight_layout()\nplt.show()\nprint('accuracy   ', (cm[0, 0] + cm[1, 1])\/ (cm[0, 1] + cm[0, 0] + cm[1, 1] + cm[1, 0])*100)\nprint('specificity', cm[0, 0] \/ (cm[0, 1] + cm[0, 0])*100)\nprint('sensitivity', cm[1, 1] \/ (cm[1, 1] + cm[1, 0])*100)","d14bcb8f":"from sklearn.svm import SVC\nsvc = SVC(kernel='poly', degree=3)\nsvc.fit(X_train_std, y_train)\n\ny_pred = svc.predict(X_test_std)\n#Accuracy\nprint(\"Accuracy\", svc.score(X_test_std, y_test)*100)\n\n#Plot the confusion matrix\nsns.set(font_scale=1.5)\ncm = confusion_matrix(y_test, y_pred)\nsns.heatmap(cm, annot=True, fmt='g')\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\nplt.tight_layout()\nplt.show()\nprint('accuracy   ', (cm[0, 0] + cm[1, 1])\/ (cm[0, 1] + cm[0, 0] + cm[1, 1] + cm[1, 0])*100)\nprint('specificity', cm[0, 0] \/ (cm[0, 1] + cm[0, 0])*100)\nprint('sensitivity', cm[1, 1] \/ (cm[1, 1] + cm[1, 0])*100)","30acb175":"from sklearn.svm import SVC\nsvc = SVC(kernel='rbf')\nsvc.fit(X_train_std, y_train)\n\ny_pred = svc.predict(X_test_std)\n#Accuracy\nprint(\"Accuracy\", svc.score(X_test_std, y_test)*100)\n\n#Plot the confusion matrix\nsns.set(font_scale=1.5)\ncm = confusion_matrix(y_test, y_pred)\nsns.heatmap(cm, annot=True, fmt='g')\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\nplt.tight_layout()\nplt.show()\nprint('accuracy   ', (cm[0, 0] + cm[1, 1])\/ (cm[0, 1] + cm[0, 0] + cm[1, 1] + cm[1, 0])*100)\nprint('specificity', cm[0, 0] \/ (cm[0, 1] + cm[0, 0])*100)\nprint('sensitivity', cm[1, 1] \/ (cm[1, 1] + cm[1, 0])*100)","be12322d":"from sklearn.svm import SVC\nsvc = SVC(kernel='sigmoid')\nsvc.fit(X_train_std, y_train)\n\ny_pred = svc.predict(X_test_std)\n#Accuracy\nprint(\"Accuracy\", svc.score(X_test_std, y_test)*100)\n\n#Plot the confusion matrix\nsns.set(font_scale=1.5)\ncm = confusion_matrix(y_test, y_pred)\nsns.heatmap(cm, annot=True, fmt='g')\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\nplt.tight_layout()\nplt.show()\nprint('accuracy   ', (cm[0, 0] + cm[1, 1])\/ (cm[0, 1] + cm[0, 0] + cm[1, 1] + cm[1, 0])*100)\nprint('specificity', cm[0, 0] \/ (cm[0, 1] + cm[0, 0])*100)\nprint('sensitivity', cm[1, 1] \/ (cm[1, 1] + cm[1, 0])*100)","ce728985":"from keras import models\nfrom keras import layers\nfrom keras import optimizers\n\ndef CreateModel(dropout = 0.0): \n    model = models.Sequential()\n    model.add(layers.Dense(128, activation='relu', input_shape=(X_train.values.shape[1],)))\n    model.add(layers.Dropout(dropout))\n    model.add(layers.Dense(64, activation='relu'))\n    model.add(layers.Dropout(dropout))\n    model.add(layers.Dense(32, activation='relu'))\n    model.add(layers.Dropout(dropout))\n    model.add(layers.Dense(1, activation='sigmoid'))\n    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n    return model","86b04328":"model = CreateModel()\nhistory_nostd = model.fit(X_train.values, y_train.values, epochs=30, batch_size=16, validation_data=(X_test.values,y_test.values))","1793d006":"plt.plot(history_nostd.history['acc'],'bo', label='Trainning acc')\nplt.plot(history_nostd.history['val_acc'],'b', label='Validation acc')\nplt.legend()","5685a79b":"y_pred = model.predict(X_test.values)\nplt.scatter(y_pred, y_test)\nplt.ylabel('True Outcome')\nplt.xlabel('DNN Output')\nplt.tight_layout()","971a91e0":"y_pred = (y_pred > 0.5)\n\n#Accuracy\nresults=model.evaluate(X_test, y_test)\nprint(results)\n\n#Plot the confusion matrix\nsns.set(font_scale=1.5)\ncm = confusion_matrix(y_test, y_pred)\nsns.heatmap(cm, annot=True, fmt='g')\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\nplt.tight_layout()\nplt.show()\nprint('accuracy   ', (cm[0, 0] + cm[1, 1])\/ (cm[0, 1] + cm[0, 0] + cm[1, 1] + cm[1, 0])*100)\nprint('specificity', cm[0, 0] \/ (cm[0, 1] + cm[0, 0])*100)\nprint('sensitivity', cm[1, 1] \/ (cm[1, 1] + cm[1, 0])*100)","1c49edfe":"model = CreateModel()\nhistory = model.fit(X_train_std, y_train.values, epochs=30, batch_size=16, validation_data=(X_test_std,y_test.values))","dfbf7476":"plt.plot(history.history['acc'],'bo', label='Trainning acc')\nplt.plot(history.history['val_acc'],'b', label='Validation acc')\nplt.legend()","835039a0":"y_pred = model.predict(X_test_std)\nplt.scatter(y_pred, y_test)\nplt.ylabel('True Outcome')\nplt.xlabel('DNN Output')\nplt.tight_layout()","d3c00f1c":"y_pred = (y_pred > 0.5)\n\n#Accuracy\nresults=model.evaluate(X_test_std, y_test)\nprint(results)\n\n#Plot the confusion matrix\nsns.set(font_scale=1.5)\ncm = confusion_matrix(y_test, y_pred)\nsns.heatmap(cm, annot=True, fmt='g')\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\nplt.tight_layout()\nplt.show()\nprint('accuracy   ', (cm[0, 0] + cm[1, 1])\/ (cm[0, 1] + cm[0, 0] + cm[1, 1] + cm[1, 0])*100)\nprint('specificity', cm[0, 0] \/ (cm[0, 1] + cm[0, 0])*100)\nprint('sensitivity', cm[1, 1] \/ (cm[1, 1] + cm[1, 0])*100)","fa9bce75":"model = CreateModel(dropout=0.5)\nhistory = model.fit(X_train_std, y_train.values, epochs=50, batch_size=64, validation_data=(X_test_std,y_test.values))","09ccb7ba":"plt.plot(history.history['acc'],'bo', label='Trainning acc')\nplt.plot(history.history['val_acc'],'b', label='Validation acc')\nplt.legend()","eab72830":"y_pred = model.predict(X_test_std)\nplt.scatter(y_pred, y_test)\nplt.ylabel('True Outcome')\nplt.xlabel('DNN Output')\nplt.tight_layout()","c5e059ad":"y_pred = (y_pred > 0.5)\n\n#Accuracy\nresults=model.evaluate(X_test_std, y_test)\nprint(results)\n\n#Plot the confusion matrix\nsns.set(font_scale=1.5)\ncm = confusion_matrix(y_test, y_pred)\nsns.heatmap(cm, annot=True, fmt='g')\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\nplt.tight_layout()\nplt.show()\nprint('accuracy   ', (cm[0, 0] + cm[1, 1])\/ (cm[0, 1] + cm[0, 0] + cm[1, 1] + cm[1, 0])*100)\nprint('specificity', cm[0, 0] \/ (cm[0, 1] + cm[0, 0])*100)\nprint('sensitivity', cm[1, 1] \/ (cm[1, 1] + cm[1, 0])*100)","a43953af":"from sklearn.metrics import roc_curve\nfrom sklearn.metrics import auc\ny_pred_keras = model.predict(X_test_std).ravel()\nfpr_keras, tpr_keras, thresholds_keras = roc_curve(y_test, y_pred_keras)\nauc_keras = auc(fpr_keras, tpr_keras)","4b66a34f":"plt.figure(1)\nplt.plot([0, 1], [0, 1], 'k--')\nplt.plot(fpr_keras, tpr_keras, label='DNN (area = {:.3f})'.format(auc_keras))\nplt.xlabel('False positive rate')\nplt.ylabel('True positive rate')\nplt.title('ROC curve')\nplt.legend(loc='best')\nk=0\nfor i,j in zip(fpr_keras,tpr_keras):\n    value = \"{:.{}f}\".format( thresholds_keras[k], 2 ) \n    if (k%4==0) :\n        plt.annotate(value,xy=(i,j), fontsize=10)\n    k=k+1\nplt.show()\n","4d445619":"optimal_idx = np.argmin(np.sqrt(np.square(1-tpr_keras)+np.square(fpr_keras)))\noptimal_threshold = thresholds_keras[optimal_idx]\nprint(optimal_threshold)\nprint(optimal_idx)\nplt.plot(np.sqrt(np.square(1-tpr_keras)+np.square(fpr_keras)))","57b9b6a1":"y_pred = model.predict(X_test_std)\nplt.scatter(y_pred, y_test)\nplt.ylabel('True Outcome')\nplt.xlabel('DNN Output')\nplt.tight_layout()","71876667":"y_pred = (y_pred >= optimal_threshold)\n\n#Accuracy\nfrom sklearn.metrics import accuracy_score\n\nresults =  accuracy_score(y_test, y_pred)\nprint(results)\n\n#Plot the confusion matrix\nsns.set(font_scale=1.5)\ncm = confusion_matrix(y_test, y_pred)\nsns.heatmap(cm, annot=True, fmt='g')\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\nplt.tight_layout()\nplt.show()\nprint('accuracy   ', (cm[0, 0] + cm[1, 1])\/ (cm[0, 1] + cm[0, 0] + cm[1, 1] + cm[1, 0])*100)\nprint('specificity', cm[0, 0] \/ (cm[0, 1] + cm[0, 0])*100)\nprint('sensitivity', cm[1, 1] \/ (cm[1, 1] + cm[1, 0])*100)","01064998":"plt.figure(1)\nplt.plot([0, 1], [0, 1], 'k--')\nplt.plot(1-fpr_keras, tpr_keras, '-bo')\nplt.xlabel('Specificity (true negative rate)')\nplt.ylabel('Sensitiviy (true positive rate)')\nplt.legend(loc='best')\nk=0\nfor i,j in zip(1-fpr_keras,tpr_keras):\n    value = \"{:.{}f}\".format( thresholds_keras[k], 2 ) \n    if (k%3==0) :\n        plt.annotate(value,xy=(i,j), fontsize=10)\n    k=k+1\nplt.show()","08934cd1":"## Decision Tree\n\nDecision tree builds regression or classification models in the form of a tree structure. It breaks down a dataset into smaller and smaller subsets while at the same time an associated decision tree is incrementally developed. The final result is a tree with decision nodes and leaf nodes. A decision node (e.g., Outlook) has two or more branches (e.g., Sunny, Overcast and Rainy), each representing values for the attribute tested. Leaf node (e.g., Hours Played) represents a decision on the numerical target. The topmost decision node in a tree which corresponds to the best predictor called root node. Decision trees can handle both categorical and numerical data. ","f267c6e8":"# Predictive Modeling","741b0b7b":"## Dist Plot\n\nDist Plot helps us to flexibly plot a univariate distribution of observations.","baf427d9":"If You find this notebook useful, **PLEASE UPVOTE **\n\n\n# PIMA Indians Diabetes\n\n\n## Background\n\n**Diabetes**, is a group of metabolic disorders in which there are high blood sugar levels over a prolonged period.  Symptoms of high blood sugar include frequent urination, increased thirst, and increased hunger.  If left untreated, diabetes can cause many complications.  Acute complications can include diabetic ketoacidosis, hyperosmolar hyperglycemic state, or death.  Serious long-term complications include cardiovascular disease, stroke, chronic kidney disease, foot ulcers, and damage to the eyes.\n\nThis **dataset** is originally from the National Institute of Diabetes and Digestive and Kidney Diseases. The objective of the dataset is to diagnostically predict whether or not a patient has diabetes, based on certain diagnostic measurements included in the dataset. Several constraints were placed on the selection of these instances from a larger database. In particular, all patients here are females at least 21 years old of Pima Indian heritage.\n\n## Objective\n\nWe will try to build a machine learning model to accurately predict whether or not the patients in the dataset have diabetes or not?\n\n## Data\n\n\nThe datasets consists of several medical predictor variables and one target variable, Outcome. Predictor variables includes the number of pregnancies the patient has had, their BMI, insulin level, age, and so on.\n\n* **Pregnancies**: Number of times pregnant\n* **Glucose**: Plasma glucose concentration a 2 hours in an oral glucose tolerance test\n* **BloodPressure**: Diastolic blood pressure (mm Hg)\n* **SkinThickness**: Triceps skin fold thickness (mm)\n* **Insulin**: 2-Hour serum insulin (mu U\/ml)\n* **BMI**: Body mass index (weight in kg\/(height in m)^2)\n* **DiabetesPedigreeFunction**: Diabetes pedigree function\n* **Age**: Age (years)\n* **Outcome**: Class variable (0 or 1)\n\n\n","39c52e8a":"# Data Visualization","490065d8":"## Violin Plots \n\n\n\nA violin plot is a method of plotting numeric data. It is similar to box plot with a rotated kernel density plot on each side. Violin plots are similar to box plots, except that they also show the probability density of the data at different values (in the simplest case this could be a histogram).  \n\nA violin plot is more informative than a plain box plot. In fact while a box plot only shows summary statistics such as mean\/median and interquartile ranges, the violin plot shows the full distribution of the data. The difference is particularly useful when the data distribution is multimodal (more than one peak). In this case a violin plot clearly shows the presence of different peaks, their position and relative amplitude. This information could not be represented with a simple box plot which only reports summary statistics. The inner part of a violin plot usually shows the mean (or median) and the interquartile range. \n","efa8e7f1":"\n## Correlation between features","dc5115a1":"## Pair Plots\n\nPair plot is used to understand the best set of features to explain a relationship between two variables or to form the most separated clusters. It also helps to form some simple classification models by drawing some simple lines or make linear separation in our dataset.","b63ea376":"Variables within a dataset can be related for lots of reasons. It can be useful in data analysis and modeling to better understand the relationships between variables. The statistical relationship between two variables is referred to as their correlation. \n\nA correlation could be positive, meaning both variables move in the same direction, or negative, meaning that when one variable\u2019s value increases, the other variables\u2019 values decrease. Correlation can also be neural or zero, meaning that the variables are unrelated."}}