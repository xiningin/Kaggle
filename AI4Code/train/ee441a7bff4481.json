{"cell_type":{"9a86700d":"code","6c598c3e":"code","da64e9fd":"code","2f688810":"code","85128eae":"code","fa4935fe":"code","a9a6f48e":"code","466f4bf2":"code","7fb059e3":"code","478fc16b":"code","497a9ed5":"code","41d2a287":"code","a60e063e":"code","1d6dcffc":"code","1b528f38":"code","6f5fd8d0":"code","9f556936":"code","4e4a8c7f":"code","2eb0316b":"code","9c59755c":"code","9ab31f18":"code","cbb2919a":"code","f0a47e04":"code","17f9598f":"code","f8947f9d":"code","46fb9ad4":"code","3208ade7":"code","fd2af118":"code","a7a4bf62":"code","85d5fd1b":"code","e9ca3604":"code","e65819b0":"code","d1fc88bd":"code","4b656a16":"code","5b538fd3":"code","cb78767e":"code","32caca29":"code","ac9e5f78":"code","18ca5671":"code","a98762ee":"code","5b4b6f2b":"code","d1fb9ed3":"code","9b6e6abf":"code","5dbc5713":"code","802aca5e":"code","47acebd2":"code","b9e82fef":"code","19b68b7b":"code","ecea9fd2":"code","4dc5b402":"code","690af920":"code","f7d86fd9":"code","bd76c61b":"code","8f79afb3":"code","7aac6a05":"code","746ab227":"markdown","d9e5a9d0":"markdown","c12ae034":"markdown","3aba4079":"markdown","a310d2da":"markdown","585229f2":"markdown","adff1926":"markdown","f0950c65":"markdown","dec9a814":"markdown","3be34716":"markdown","be5345d9":"markdown"},"source":{"9a86700d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","6c598c3e":"import tensorflow as tf\n\nfrom tensorflow import feature_column\nfrom tensorflow.keras import layers\nfrom sklearn.model_selection import train_test_split","da64e9fd":"!pip install -q git+https:\/\/github.com\/tensorflow\/docs\n\nimport tensorflow_docs as tfdocs\nimport tensorflow_docs.modeling\nimport tensorflow_docs.plots","2f688810":"import matplotlib.pyplot as plt","85128eae":"train = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')","fa4935fe":"train.info()","a9a6f48e":"train.tail()","466f4bf2":"test.info()","7fb059e3":"test.tail()","478fc16b":"train_labels = train.pop('Survived')","497a9ed5":"train.info()","41d2a287":"age_binned, age_bins = pd.qcut(train['Age'], \n                               q=4,\n                               retbins=True)","a60e063e":"fare_binned, fare_bins = pd.qcut(train['Fare'], \n                               q=4,\n                               retbins=True)","1d6dcffc":"train.Age.fillna(train.Age.median(), inplace=True)","1b528f38":"# transform null values to new value \ntrain.Cabin.fillna(train.Cabin.mode()[0].split()[0], inplace=True)","6f5fd8d0":"train.Embarked.fillna(train.Embarked.mode()[0], inplace=True)","9f556936":"train.info()","4e4a8c7f":"test.Age.fillna(test.Age.median(), inplace=True)","2eb0316b":"# transform null values to new value \ntest.Cabin.fillna(test.Cabin.mode()[0].split()[0], inplace=True)","9c59755c":"test.Fare.fillna(test.Fare.median(), inplace=True)","9ab31f18":"test.info()","cbb2919a":"test['Survived'] = np.nan","f0a47e04":"test.tail()","17f9598f":"test_labels = test.pop('Survived')","f8947f9d":"test_features = test.copy()","46fb9ad4":"train.dtypes","3208ade7":"SEED = 65536\nBATCH = 32","fd2af118":"tf.random.set_seed(SEED)","a7a4bf62":"train_features, val_features, train_labels, val_labels = train_test_split(train, \n                                                                          train_labels, \n                                                                          test_size=0.2,\n                                                                          random_state=SEED,\n                                                                          shuffle=True,\n                                                                          stratify=train_labels)\n\n\nprint(len(train_features), 'train examples')\nprint(len(val_features), 'validation examples')\nprint(len(test_features), 'test examples')","85d5fd1b":"train_features.info()","e9ca3604":"desc = train_features.describe()","e65819b0":"train_ds = tf.data.Dataset.from_tensor_slices((dict(train_features), train_labels))","d1fc88bd":"val_ds = tf.data.Dataset.from_tensor_slices((dict(val_features), val_labels))","4b656a16":"test_ds = tf.data.Dataset.from_tensor_slices((dict(test_features), test_labels))","5b538fd3":"feature_columns = []\n\n# numeric columns\nfor header in ['PassengerId', 'Pclass', 'SibSp', 'Parch', 'Fare', 'Age']:\n  feature_columns.append(feature_column.numeric_column(header)) ","cb78767e":"# bucketized columns\nage = feature_column.numeric_column('Age')\nage_buckets = feature_column.bucketized_column(age, boundaries=[*age_bins])\nfeature_columns.append(age_buckets)\n\nfare = feature_column.numeric_column('Fare')\nfare_buckets = feature_column.bucketized_column(fare, boundaries=[*fare_bins])\nfeature_columns.append(fare_buckets)","32caca29":"# indicator_columns\nindicator_column_names = ['Sex', 'Embarked']\n\nfor col_name in indicator_column_names:\n  categorical_column = feature_column.categorical_column_with_vocabulary_list(\n      col_name, train[col_name].unique())\n  indicator_column = feature_column.indicator_column(categorical_column)\n  feature_columns.append(indicator_column)","ac9e5f78":"# embedding columns\nembedding_column_names = ['Ticket', 'Name', 'Cabin']\n\nfor col_name in embedding_column_names:\n  categorical_column = feature_column.categorical_column_with_vocabulary_list(\n      col_name, train[col_name].unique())\n  embedding_column = feature_column.embedding_column(categorical_column, dimension=64)\n  feature_columns.append(embedding_column)","18ca5671":"gender = feature_column.categorical_column_with_vocabulary_list('Sex', train['Sex'].unique())","a98762ee":"port = feature_column.categorical_column_with_vocabulary_list('Embarked', train['Embarked'].unique())","5b4b6f2b":"class_pass = feature_column.categorical_column_with_vocabulary_list('Pclass', train['Pclass'].unique())","d1fb9ed3":"# crossed columns\nage_gender_feature = feature_column.crossed_column([age_buckets, gender], hash_bucket_size=64)\nfeature_columns.append(feature_column.indicator_column(age_gender_feature))\n\n\nage_port_feature = feature_column.crossed_column([age_buckets, port], hash_bucket_size=64)\nfeature_columns.append(feature_column.indicator_column(age_port_feature))\n\n\nage_class_feature = feature_column.crossed_column([age_buckets, class_pass], hash_bucket_size=64)\nfeature_columns.append(feature_column.indicator_column(age_class_feature))\n\n\nfare_gender_feature = feature_column.crossed_column([fare_buckets, gender], hash_bucket_size=64)\nfeature_columns.append(feature_column.indicator_column(fare_gender_feature))\n\n\n# fare_port_feature = feature_column.crossed_column([fare_buckets, port], hash_bucket_size=64)\n# feature_columns.append(feature_column.indicator_column(fare_port_feature))\n\n\nfare_class_feature = feature_column.crossed_column([fare_buckets, class_pass], hash_bucket_size=64)\nfeature_columns.append(feature_column.indicator_column(fare_class_feature))","9b6e6abf":"feature_layer = tf.keras.layers.DenseFeatures(feature_columns, trainable=True)","5dbc5713":"model = tf.keras.Sequential([\n                  feature_layer,\n                  layers.Dense(128, activation='elu'),\n                  layers.Dense(128, activation='elu'),\n                  layers.Dropout(.2),\n                  layers.Dense(1)\n])\n\nmodel.compile(optimizer='adamax',\n              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n              metrics=[tf.keras.metrics.BinaryAccuracy(),\n                       'accuracy'])","802aca5e":"train_ds = train_ds.shuffle(len(train_features)).batch(BATCH).cache()\nval_ds = val_ds.batch(BATCH).cache()\ntest_ds = test_ds.batch(BATCH).cache()","47acebd2":"stopping = tf.keras.callbacks.EarlyStopping(patience=25,\n                                             monitor='val_accuracy')","b9e82fef":"history = {}","19b68b7b":"history['model_0'] = model.fit(train_ds,\n                                  validation_data=val_ds,\n                                  epochs=200,\n                                  workers=4,\n                                  use_multiprocessing=True,\n                                  callbacks=[stopping])","ecea9fd2":"model.summary()","4dc5b402":"plotter = tfdocs.plots.HistoryPlotter(metric='accuracy', smoothing_std=2)\nplotter.plot(history)\nplt.ylim([0.5, 1.05]);","690af920":"preds = model.predict(test_ds)","f7d86fd9":"preds[-20:]","bd76c61b":"predictions = [1 if i >= 1.5 else 0 for i in preds]\npredictions[-20:]","8f79afb3":"sub = pd.DataFrame(predictions, columns=['Survived'], index=test.PassengerId)\nsub.tail(20)","7aac6a05":"sub.to_csv('submission.csv')","746ab227":"# training data and validation data","d9e5a9d0":"# Train the model","c12ae034":"# Build the Neural Net Model (Dense layers)","3aba4079":"# Use Pandas DataFrames to analyze the data and its structure","a310d2da":"# Run the Test data through the model for predictions","585229f2":"# Tell TensorFlow how to use the features of the data","adff1926":"convert the output of the model to be scored","f0950c65":"# Apply the same transformations to the test data","dec9a814":"# Begin transforming the data for the model","3be34716":"# Import Dependencies","be5345d9":"Random Seeds that are powers of 2 and are sufficiently large seem to perform best"}}