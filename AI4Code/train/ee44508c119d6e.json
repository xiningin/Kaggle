{"cell_type":{"880f87fb":"code","927f13b4":"code","b04996c7":"code","af1d2bff":"code","b13d718d":"code","4406239c":"code","1a517645":"code","dc03b69f":"code","8f71d179":"code","1386dc1b":"code","420f1bb1":"code","e51b6562":"code","6130610a":"code","f4210a5a":"code","b17cc997":"code","e185eec5":"code","c54e10b3":"code","52f3039c":"code","f1a82d53":"code","07f57767":"code","349b0745":"code","542f041e":"markdown"},"source":{"880f87fb":"import tensorflow as tf\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler, Normalizer\nfrom sklearn.decomposition import PCA as sklearnPCA\n\n# Supress unnecessary warnings so that presentation looks clean\nimport warnings\nwarnings.filterwarnings(\"ignore\")","927f13b4":"wbcd = pd.read_csv(\"..\/input\/voteshouse\/house-votes-84.csv\")\nwbcd.head()","b04996c7":"print(\"This WBCD dataset is consisted of\",wbcd.shape)\n","af1d2bff":"sns.countplot(wbcd['party'],label=\"Count\")\n","b13d718d":"corr = wbcd.iloc[:,:].corr()\ncolormap = sns.diverging_palette(220, 10, as_cmap = True)\nplt.figure(figsize=(14,14))\nsns.heatmap(corr, cbar = True,  square = True, annot=True, fmt= '.2f',annot_kws={'size': 8},\n            cmap = colormap, linewidths=0.1, linecolor='white')\nplt.title('Correlation of House Votes Features', y=1.05, size=15)","4406239c":"train,test = train_test_split(wbcd, test_size=0.3, random_state=42)\nprint(\"Training Data :\",train.shape)\nprint(\"Testing Data :\",test.shape)","1a517645":"train_data = train\ntest_data = test\n\n\nprint(\"Training Data :\",train_data.shape)\nprint(\"Testing Data :\",test_data.shape)\n","dc03b69f":"train_x = train_data.iloc[:,0:-1]\ntrain_x = MinMaxScaler().fit_transform(train_x)\nprint(\"Training Data :\", train_x.shape)\n\n# Testing Data\ntest_x = test_data.iloc[:,0:-1]\ntest_x = MinMaxScaler().fit_transform(test_x)\nprint(\"Testing Data :\", test_x.shape)","8f71d179":"train_y = train_data.iloc[:,-1:]\ntrain_y[train_y == 'republican'] = 0\ntrain_y[train_y == 'democrat' ] = 1\nprint(\"Training Data :\", train_y.shape)\n\n# Testing Data\ntest_y = test_data.iloc[:,-1:]\ntest_y[test_y == 'republican'] = 0\ntest_y[test_y == 'democrat' ] = 1\nprint(\"Testing Data :\", test_y.shape)","1386dc1b":"X = tf.placeholder(tf.float32, [None,16])\nY = tf.placeholder(tf.float32, [None, 1])\n","420f1bb1":"# weight\nW = tf.Variable(tf.random_normal([16,1], seed=0), name='weight')\n\n# bias\nb = tf.Variable(tf.random_normal([1], seed=0), name='bias')","e51b6562":"logits = tf.matmul(X,W) + b","6130610a":"hypothesis = tf.nn.sigmoid(logits)\n\ncost_i = tf.nn.sigmoid_cross_entropy_with_logits(logits=logits,labels=Y)\ncost = tf.reduce_mean(cost_i)\n# cost = -tf.reduce_mean(Y * tf.log(hypothesis) + (1 - Y) * tf.log(1 - hypothesis))","f4210a5a":"hypothesis = tf.nn.sigmoid(logits)\n\ncost_i = tf.nn.sigmoid_cross_entropy_with_logits(logits=logits,labels=Y)\ncost = tf.reduce_mean(cost_i)\n# cost = -tf.reduce_mean(Y * tf.log(hypothesis) + (1 - Y) * tf.log(1 - hypothesis))","b17cc997":"train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n","e185eec5":"prediction = tf.cast(hypothesis > 0.5, dtype=tf.float32)\ncorrect_prediction = tf.equal(prediction, Y)\naccuracy = tf.reduce_mean(tf.cast(correct_prediction, dtype=tf.float32))","c54e10b3":"with tf.Session() as sess:\n    sess.run(tf.global_variables_initializer())\n    for step in range(10001):\n        sess.run(train, feed_dict={X: train_x, Y: train_y})\n        if step % 1000 == 0:\n            loss, acc = sess.run([cost, accuracy], feed_dict={X: train_x, Y: train_y})\n            print(\"Step: {:5}\\tLoss: {:.3f}\\tAcc: {:.2%}\".format(step, loss, acc))\n            \n    train_acc = sess.run(accuracy, feed_dict={X: train_x, Y: train_y})\n    test_acc,test_predict,test_correct = sess.run([accuracy,prediction,correct_prediction], feed_dict={X: test_x, Y: test_y})\n    print(\"Model Prediction =\", train_acc)\n    print(\"Test Prediction =\", test_acc)","52f3039c":"def ann_slp():\n    print(\"===========Data Summary===========\")\n    print(\"Training Data :\", train_x.shape)\n    print(\"Testing Data :\", test_x.shape)\n\n    X = tf.placeholder(tf.float32, [None,16])\n    Y = tf.placeholder(tf.float32, [None, 1])\n\n    W = tf.Variable(tf.random_normal([16,1], seed=0), name='weight')\n    b = tf.Variable(tf.random_normal([1], seed=0), name='bias')\n\n    logits = tf.matmul(X,W) + b\n    hypothesis = tf.nn.sigmoid(logits)\n    \n    cost_i = tf.nn.sigmoid_cross_entropy_with_logits(logits=logits,labels=Y)\n    cost = tf.reduce_mean(cost_i)\n\n    train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n\n    prediction = tf.cast(hypothesis > 0.5, dtype=tf.float32)\n    correct_prediction = tf.equal(prediction, Y)\n    accuracy = tf.reduce_mean(tf.cast(correct_prediction, dtype=tf.float32))\n\n    \n    loss_ans= []\n    acc_ans = []\n    print(\"\\n============Processing============\")\n    with tf.Session() as sess:\n        sess.run(tf.global_variables_initializer())\n        for step in range(10001):\n            sess.run(train, feed_dict={X: train_x, Y: train_y})\n            if step % 1000 == 0:\n                loss, acc = sess.run([cost, accuracy], feed_dict={X: train_x, Y: train_y})\n                loss_ans.append(loss)\n                acc_ans.append(acc)\n                print(\"Step: {:5}\\tLoss: {:.3f}\\tAcc: {:.2%}\".format(step, loss, acc))\n\n        train_acc = sess.run(accuracy, feed_dict={X: train_x, Y: train_y})\n        test_acc,test_predict,test_correct = sess.run([accuracy,prediction,correct_prediction], feed_dict={X: test_x, Y: test_y})\n        \n        print(\"\\n============Results============\")\n        print(\"Model Prediction =\", train_acc)\n        print(\"Test Prediction =\", test_acc)\n        \n        return train_acc,test_acc, loss_ans, acc_ans\n    \nann_slp_train_acc, ann_slp_test_acc, loss_ans, acc_ans = ann_slp()","f1a82d53":"def ann_mlp():\n    print(\"===========Data Summary===========\")\n    print(\"Training Data :\", train_x.shape)\n    print(\"Testing Data :\", test_x.shape)\n\n    X = tf.placeholder(tf.float32, [None,16])\n    Y = tf.placeholder(tf.float32, [None, 1])\n\n    # input\n    W1 = tf.Variable(tf.random_normal([16,32], seed=0), name='weight1')\n    b1 = tf.Variable(tf.random_normal([32], seed=0), name='bias1')\n    layer1 = tf.nn.sigmoid(tf.matmul(X,W1) + b1)\n\n    # hidden1\n    W2 = tf.Variable(tf.random_normal([32,32], seed=0), name='weight2')\n    b2 = tf.Variable(tf.random_normal([32], seed=0), name='bias2')\n    layer2 = tf.nn.sigmoid(tf.matmul(layer1,W2) + b2)\n\n    # hidden2\n    W3 = tf.Variable(tf.random_normal([32,48], seed=0), name='weight3')\n    b3 = tf.Variable(tf.random_normal([48], seed=0), name='bias3')\n    layer3 = tf.nn.sigmoid(tf.matmul(layer2,W3) + b3)\n    \n     # hidden3\n    W4 = tf.Variable(tf.random_normal([48,64], seed=0), name='weight4')\n    b4 = tf.Variable(tf.random_normal([64], seed=0), name='bias4')\n    layer4 = tf.nn.sigmoid(tf.matmul(layer3,W4) + b4)\n\n    # output\n    W5 = tf.Variable(tf.random_normal([32,1], seed=0), name='weight5')\n    b5 = tf.Variable(tf.random_normal([1], seed=0), name='bias5')\n    logits = tf.matmul(layer2,W5) + b5\n    hypothesis = tf.nn.sigmoid(logits)\n\n    cost_i = tf.nn.sigmoid_cross_entropy_with_logits(logits=logits,labels=Y)\n    cost = tf.reduce_mean(cost_i)\n\n    train = tf.train.GradientDescentOptimizer(learning_rate=0.001).minimize(cost)\n\n    prediction = tf.cast(hypothesis > 0.5, dtype=tf.float32)\n    correct_prediction = tf.equal(prediction, Y)\n    accuracy = tf.reduce_mean(tf.cast(correct_prediction, dtype=tf.float32))\n\n    loss_ans = []\n    print(\"\\n============Processing============\")\n    with tf.Session() as sess:\n        sess.run(tf.global_variables_initializer())\n        for step in range(10001):\n            sess.run(train, feed_dict={X: train_x, Y: train_y})\n            if step % 1000 == 0:\n                loss, acc = sess.run([cost, accuracy], feed_dict={X: train_x, Y: train_y})\n                loss_ans.append(loss)\n                print(\"Step: {:5}\\tLoss: {:.3f}\\tAcc: {:.2%}\".format(step, loss, acc))\n            \n\n        train_acc = sess.run(accuracy, feed_dict={X: train_x, Y: train_y})\n        test_acc,test_predict,test_correct = sess.run([accuracy,prediction,correct_prediction], feed_dict={X: test_x, Y: test_y})\n        \n        print(\"\\n============Results============\")\n        print(\"Model Prediction =\", train_acc)\n        print(\"Test Prediction =\", test_acc)\n        \n        return train_acc,test_acc, loss_ans\n    \nann_mlp_train_acc, ann_mlp_test_acc , loss_ans = ann_mlp()","07f57767":"loss1 = loss_ans","349b0745":"ax = sns.lineplot([i for i in range(len(loss1))], loss1 , label = \"1 Hidden Layers\")\nax = sns.lineplot([i for i in range(len(loss2))], loss2 , label = \"2 Hidden Layer\" )\nax = sns.lineplot([i for i in range(len(loss3))], loss3, label = \"3 Hidden Layers\")\nax.set(xlabel = \"Loss Count\", ylabel = \"Loss value\")","542f041e":"## MLP"}}