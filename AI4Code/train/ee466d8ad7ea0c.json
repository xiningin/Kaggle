{"cell_type":{"e1d6d3c1":"code","76393777":"code","25d1ff3d":"code","a65641a0":"code","4450d07c":"code","7b3f4108":"code","ed509e39":"code","f39efa2f":"code","fbed596d":"code","a73ff25e":"code","4f36de2d":"code","d86e3e1a":"code","2918f065":"code","0d2e5d92":"code","7aaa2fe9":"code","c529bce1":"code","b74c5156":"code","e9b88968":"code","9318d01d":"code","f9c9a1a1":"code","b02336ee":"code","cd16c4ed":"code","c7a39b4a":"code","bb746bdd":"code","6eea1c81":"code","f5ae9a3e":"code","66b9d0ca":"code","731ed3f5":"code","f950f611":"code","a47e30b8":"code","da27fd95":"code","fbefc0ad":"code","a8ce105f":"code","70e4ef29":"code","a21819df":"markdown","2434c311":"markdown","382a4b1a":"markdown","c5de46d0":"markdown","42d60973":"markdown","826b0eb3":"markdown","faf14d0b":"markdown","0b2fde22":"markdown","582efdf1":"markdown","e0f59e98":"markdown","a51c3583":"markdown","012ff075":"markdown","441827f3":"markdown","b816ec26":"markdown","2211a79a":"markdown","6715ed52":"markdown","f11fa677":"markdown","ba9e2c35":"markdown","a0018f72":"markdown","2a1c0456":"markdown"},"source":{"e1d6d3c1":"import numpy as np \nimport pandas as pd \nimport seaborn as sns\nimport optuna\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport os\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score\nfrom catboost import CatBoostClassifier\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier","76393777":"train = pd.read_csv('..\/input\/tabular-playground-series-mar-2021\/train.csv')\ntest = pd.read_csv('..\/input\/tabular-playground-series-mar-2021\/test.csv')","25d1ff3d":"train.head(3)","a65641a0":"train.info()","4450d07c":"test.head(3)","7b3f4108":"test.info()","ed509e39":"trheat = train.drop('id', axis = 1)\nmatrix = np.triu(trheat.corr())\nplt.figure(figsize=(15, 10))\nsns.heatmap(trheat.corr(), annot = True, cmap = 'YlGn', fmt=\".2f\", mask = matrix, vmin = -1, vmax = 1, linewidths = 0.1, linecolor = 'white')","f39efa2f":"sns.set_style('whitegrid')\nsns.set_palette('Greens_r', 2)\nplt.figure(figsize=(15, 10))\nsns.countplot(x = 'target', data = train)","fbed596d":"X = train.drop(['target', 'id'], axis = 1)\ny = train['target']\n\nID = test['id'] # for submission\ntest = test.drop('id', axis = 1)\n\nnum_cols = X.select_dtypes(include = 'number').columns.to_list() # numerical features\ncat_cols = X.select_dtypes(exclude = 'number').columns.to_list() # categorical features","a73ff25e":"cols = cat_cols + num_cols\nX_objs = len(X)\ndf = pd.concat(objs = [X[cols], test[cols]], axis = 0)\ndf = pd.concat(objs = [X[cols], test[cols]], axis = 0)\ndf = pd.get_dummies(df, columns = cat_cols)\nX = df[:X_objs]\ntest = df[X_objs:]","4f36de2d":"X.head(3)","d86e3e1a":"test.head(3)","2918f065":"def objective(trial, data = X, target = y):\n    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size = 0.2, random_state = 0)\n\n    params = {\n        'max_depth': trial.suggest_int('max_depth', 3, 10),\n        'learning_rate': trial.suggest_categorical('learning_rate', [0.005, 0.02, 0.05, 0.08, 0.1]),\n        'n_estimators': trial.suggest_int('n_estimators', 50, 3000),\n        'max_bin': trial.suggest_int('max_bin', 200, 400),\n        'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 1, 300),\n        'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 0.0001, 1.0, log = True),\n        'subsample': trial.suggest_float('subsample', 0.1, 0.8),\n        'random_seed': 42,\n        'task_type': 'GPU',\n        'loss_function': 'Logloss',\n        'eval_metric': 'AUC',\n        'bootstrap_type': 'Poisson'\n    }\n    \n    model = CatBoostClassifier(**params)  \n    model.fit(X_train, y_train, eval_set = [(X_val,y_val)], early_stopping_rounds = 222, verbose = False)\n    y_pred = model.predict_proba(X_val)[:,1]\n    roc_auc = roc_auc_score(y_val, y_pred)\n\n    return roc_auc","0d2e5d92":"%%time\nstudy = optuna.create_study(direction = 'maximize')\nstudy.optimize(objective, n_trials = 30)\nprint('Number of finished trials:', len(study.trials))\nprint('Best trial:', study.best_trial.params)\nprint('Best value:', study.best_value)","7aaa2fe9":"optuna.visualization.plot_optimization_history(study)","c529bce1":"optuna.visualization.plot_param_importances(study)","b74c5156":"paramsCB = study.best_trial.params\nparamsCB['task_type'] = 'GPU'\nparamsCB['loss_function'] = 'Logloss'\nparamsCB['eval_metric'] = 'AUC'\nparamsCB['random_seed'] = 42\nparamsCB['bootstrap_type'] = 'Poisson'","e9b88968":"%%time\nfrom sklearn.model_selection import KFold\n\nfolds = KFold(n_splits = 10, shuffle = True, random_state = 42)\n\npredictions = np.zeros(len(test))\n\nfor fold, (trn_idx, val_idx) in enumerate(folds.split(X, y)):\n    \n    X_train, X_val = X.iloc[trn_idx], X.iloc[val_idx]\n    y_train, y_val = y.iloc[trn_idx], y.iloc[val_idx]\n\n    model = CatBoostClassifier(**paramsCB)\n   \n    model.fit(X_train, y_train, eval_set = [(X_val, y_val)], verbose = False, early_stopping_rounds = 222)\n    \n    predictions += model.predict_proba(test)[:,1] \/ folds.n_splits ","9318d01d":"submission = pd.DataFrame({'id': ID, 'target': predictions})\nsubmission.to_csv('submissionCB.csv', index = False)","f9c9a1a1":"def objective(trial, data = X, target = y):\n    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size = 0.2, random_state = 0)\n\n    params = {\n        'max_depth': trial.suggest_int('max_depth', 3, 10),\n        'learning_rate': trial.suggest_categorical('learning_rate', [0.005, 0.02, 0.05, 0.08, 0.1]),\n        'n_estimators': trial.suggest_int('n_estimators', 50, 3000),\n        'min_child_weight': trial.suggest_int('min_child_weight', 1, 300),\n        'gamma': trial.suggest_float('gamma', 0.0001, 1.0, log = True),\n        'alpha': trial.suggest_float('alpha', 0.0001, 10.0, log = True),\n        'lambda': trial.suggest_float('lambda', 0.0001, 10.0, log = True),\n        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.1, 0.8),\n        'subsample': trial.suggest_float('subsample', 0.1, 0.8),\n        'tree_method': 'gpu_hist',\n        'booster': 'gbtree',\n        'random_state': 42,\n        'use_label_encoder': False,\n        'eval_metric': 'auc'\n\n    }\n    \n    model = XGBClassifier(**params)  \n    model.fit(X_train, y_train, eval_set = [(X_val,y_val)], early_stopping_rounds = 222, verbose = False)\n    y_pred = model.predict_proba(X_val)[:,1]\n    roc_auc = roc_auc_score(y_val, y_pred)\n\n    return roc_auc","b02336ee":"%%time\nstudy = optuna.create_study(direction = 'maximize')\nstudy.optimize(objective, n_trials = 30)\nprint('Number of finished trials:', len(study.trials))\nprint('Best trial:', study.best_trial.params)\nprint('Best value:', study.best_value)","cd16c4ed":"optuna.visualization.plot_optimization_history(study)","c7a39b4a":"optuna.visualization.plot_param_importances(study)","bb746bdd":"paramsXGB = study.best_trial.params\nparamsXGB['tree_method'] = 'gpu_hist'\nparamsXGB['booster'] = 'gbtree'\nparamsXGB['eval_metric'] = 'auc'\nparamsXGB['random_state'] = 42\nparamsXGB['use_label_encoder'] = False","6eea1c81":"%%time\nfrom sklearn.model_selection import KFold\n\nfolds = KFold(n_splits = 10, shuffle = True, random_state = 42)\n\npredictions = np.zeros(len(test))\n\nfor fold, (trn_idx, val_idx) in enumerate(folds.split(X, y)):\n    \n    X_train, X_val = X.iloc[trn_idx], X.iloc[val_idx]\n    y_train, y_val = y.iloc[trn_idx], y.iloc[val_idx]\n\n    model = XGBClassifier(**paramsXGB)\n   \n    model.fit(X_train, y_train, eval_set = [(X_val, y_val)], eval_metric = 'auc', verbose = False, early_stopping_rounds = 222)\n    \n    predictions += model.predict_proba(test)[:,1] \/ folds.n_splits ","f5ae9a3e":"submission = pd.DataFrame({'id': ID, 'target': predictions})\nsubmission.to_csv('submissionXGB.csv', index = False)","66b9d0ca":"def objective(trial, data = X, target = y):\n    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size = 0.2, random_state = 0)\n\n    params = {\n        'reg_alpha': trial.suggest_float('reg_alpha', 0.001, 10.0),\n        'reg_lambda': trial.suggest_float('reg_lambda', 0.001, 10.0),\n        'num_leaves': trial.suggest_int('num_leaves', 11, 333),\n        'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n        'max_depth': trial.suggest_int('max_depth', 5, 20),\n        'learning_rate': trial.suggest_categorical('learning_rate', [0.01, 0.02, 0.05, 0.005, 0.1]),\n        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.1, 0.5),\n        'n_estimators': trial.suggest_int('n_estimators', 50, 3000),\n        'random_state': 42,\n        'boosting_type': 'gbdt',\n        'metric': 'AUC',\n        'device': 'gpu'\n    }\n    \n    model = LGBMClassifier(**params)  \n    model.fit(X_train, y_train, eval_set = [(X_val,y_val)], early_stopping_rounds = 222, verbose = False)\n    y_pred = model.predict_proba(X_val)[:,1]\n    roc_auc = roc_auc_score(y_val, y_pred)\n\n    return roc_auc","731ed3f5":"%%time\nstudy = optuna.create_study(direction = 'maximize')\nstudy.optimize(objective, n_trials = 30)\nprint('Number of finished trials:', len(study.trials))\nprint('Best trial:', study.best_trial.params)\nprint('Best value:', study.best_value)","f950f611":"optuna.visualization.plot_optimization_history(study)","a47e30b8":"optuna.visualization.plot_param_importances(study)","da27fd95":"paramsLGBM = study.best_trial.params\nparamsLGBM['boosting_type'] = 'gbdt'\nparamsLGBM['metric'] = 'AUC'\nparamsLGBM['random_state'] = 42","fbefc0ad":"%%time\nfrom sklearn.model_selection import KFold\n\nfolds = KFold(n_splits = 10, shuffle = True, random_state = 42)\n\npredictions = np.zeros(len(test))\n\nfor fold, (trn_idx, val_idx) in enumerate(folds.split(X, y)):\n    \n    X_train, X_val = X.iloc[trn_idx], X.iloc[val_idx]\n    y_train, y_val = y.iloc[trn_idx], y.iloc[val_idx]\n\n    model = LGBMClassifier(**paramsLGBM)\n   \n    model.fit(X_train, y_train, eval_set = [(X_val, y_val)], eval_metric = 'auc', verbose = False, early_stopping_rounds = 222)\n    \n    predictions += model.predict_proba(test)[:,1] \/ folds.n_splits ","a8ce105f":"submission = pd.DataFrame({'id': ID, 'target': predictions})\nsubmission.to_csv('submissionLGBM.csv', index = False)","70e4ef29":"results = pd.DataFrame([['CatBoost', 0.89144],\n                  ['XGB', 0.89414],\n                  ['LGBM', 0.89490]], \ncolumns = ['Algorithm', 'BestScore'])\nresults","a21819df":"Let's see what we got","2434c311":"# Preprocessing","382a4b1a":"# CatBoost and optuna's optimization","c5de46d0":"**Visualize optuna's optimization CatBoost**","42d60973":"# LGBM and optuna's optimizaton","826b0eb3":"# Conclusion","faf14d0b":"**Visualize optuna's optimization LGBM**","0b2fde22":"Let's look on count distribution in target","582efdf1":"**Visualize optuna's optimization XGB**","e0f59e98":"**Fit XGB model with best parametrs**","a51c3583":"**Fit CatBoost model with best parametrs**","012ff075":"# EDA","441827f3":"**Fit LGBM model with best parametrs**","b816ec26":"Let's look the correlation between features","2211a79a":"Encoding categorical fetaures","6715ed52":"**In tis notebook I want to check CatBoost, XGB and LGBM which is one the best with Optuna optimization with 30 trials. As a novice, I've heard a lot about how CatBoost is fast, and that XGB is the weapon of champions. Well, it's time to check it out in person.**","f11fa677":"There are no NA values. It's good.","ba9e2c35":"**Well, based on the results in the table above, we can say that CatBoost has the lowest score. XGB showed an average result. LGBM has the best result (so that's why everyone uses it in TPS). Of course, only 30 trials were done with Optuna, and CatBoost may have better result, but this is just my little experiment, where all algorithms are on an equal conditions. Conclusion - if we want to win in TPS competitions on Kaggle and fight for every 0.001 or even 0.0001, then our choice slow, but true - LGBM.**","a0018f72":"There are no NA values too","2a1c0456":"# XGB and optuna's optimization"}}