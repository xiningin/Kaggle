{"cell_type":{"c29c3a5b":"code","f16f4021":"code","3095416b":"code","18299150":"code","4ee7d8cb":"code","d093c8af":"code","291db0d1":"code","8f786d90":"code","321bf7b3":"code","9d601946":"code","d8dff576":"code","972a6a80":"code","3fcd8197":"code","80d5b235":"code","562be3be":"code","dcbd2949":"code","380a78e5":"code","036121a5":"code","c674c7ed":"code","672eb20c":"code","085ef4e9":"code","e2f51e64":"code","668f099c":"code","276ed0bd":"code","71f768fd":"code","1f7224dd":"code","f1e15cea":"code","3ce285e5":"code","32833e82":"code","2596f8b1":"code","3afeb8d2":"code","30f7b29f":"code","8ddaeb8c":"code","55782b08":"code","ad3d6d17":"code","279848ce":"code","dae9c726":"code","1daa5aa7":"code","1dddd1c0":"code","05d7e840":"code","9c9401ec":"code","45c25ed0":"code","ac6c001e":"code","99afc8b3":"code","94805472":"code","618ef96e":"code","4a1fcff1":"code","1962750b":"code","c06c8f11":"code","bea01533":"code","67a8239d":"markdown","309902f7":"markdown","0734070c":"markdown","570da515":"markdown","c9be70cf":"markdown","71c2e385":"markdown","c2d494b5":"markdown","ab988c64":"markdown","523db0ce":"markdown","57cbe817":"markdown","597b900a":"markdown","8d815706":"markdown","3100cb75":"markdown","d8f8f42a":"markdown","c80b5c86":"markdown","75d9e06e":"markdown","647d296c":"markdown","4434b5d3":"markdown","a2c71892":"markdown"},"source":{"c29c3a5b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","f16f4021":"df=pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")","3095416b":"df.head()","18299150":"df.info()","4ee7d8cb":"r=['PassengerId','Name','Ticket','Cabin']\ndf1=df.drop(r,axis=1).copy()\ndf1.head()","d093c8af":"df1.info()","291db0d1":"df1.isnull().sum()","8f786d90":"df1[\"Embarked\"].mode()[0]","321bf7b3":"# Filling missing Embarked values with most common value\n\ndf1[\"Embarked\"]=df1[\"Embarked\"].fillna(df1[\"Embarked\"].mode()[0])","9d601946":"df1[\"Age\"]=df1[\"Age\"].fillna(df1[\"Age\"].mean())","d8dff576":"df1.head(2)","972a6a80":"df1.info()","3fcd8197":"cc=['Sex','Embarked']","80d5b235":"dummy=pd.get_dummies(df1[cc])\ndummy.head()","562be3be":"df1.head()","dcbd2949":"df1=df1.drop(cc,axis=1)","380a78e5":"df2=pd.concat([df1,dummy],axis=1)\ndf2.head()\n","036121a5":"X=df2.iloc[:,1:]\ny=df2.iloc[:,0]","c674c7ed":"X.head()","672eb20c":"y.head()","085ef4e9":"y.value_counts()","e2f51e64":"from sklearn import tree\nfrom sklearn.model_selection import train_test_split\nsn=[]\nscore=[]\nfor i in range(1,100):\n    X_train, X_test, y_train, y_test = train_test_split(X, y,  stratify=y,  test_size=0.25)  # split\n    dtc = tree.DecisionTreeClassifier(random_state=0)# model selection\n    dtc.fit(X_train, y_train)# model learning\n    sn.append(i)\n    score.append(dtc.score(X_test,y_test))\n\nresult=pd.DataFrame()\nresult[\"SN\"]=sn\nresult[\"Score\"]=score","668f099c":"result.describe()","276ed0bd":"result.to_csv(\"r1.csv\")","71f768fd":"from sklearn import preprocessing\n\nminmax_scale=preprocessing.MinMaxScaler(feature_range=(0,1))\n\nX_minmax=minmax_scale.fit_transform(X)","1f7224dd":"X_minmax","f1e15cea":"X1=pd.DataFrame(columns=X.columns,data=X_minmax)\nX1","3ce285e5":"sn=[]\nscore=[]\nfor i in range(1,100):\n    from sklearn.model_selection import train_test_split\n    X_train, X_test, y_train, y_test = train_test_split(X1, y,  stratify=y,  test_size=0.25)\n    dtc = tree.DecisionTreeClassifier(random_state=0)\n    dtc.fit(X_train, y_train)\n    sn.append(i)\n    score.append(dtc.score(X_test,y_test))\n\nresult=pd.DataFrame()\nresult[\"SN\"]=sn\nresult[\"Score\"]=score\nprint(result)    ","32833e82":"result.describe()","2596f8b1":"result.to_csv(\"r2.csv\")","3afeb8d2":"from sklearn import preprocessing\nscaler=preprocessing.StandardScaler()\nX_stdscale=scaler.fit_transform(X)\nX_stdscale","30f7b29f":"X2=pd.DataFrame(columns=X.columns,data=X_stdscale)\nX2","8ddaeb8c":"from sklearn import metrics\nsn=[]\nscore=[]\n#acc=[]\nfor i in range(1,100):\n    from sklearn.model_selection import train_test_split\n    X_train, X_test, y_train, y_test = train_test_split(X2, y,  stratify=y,  test_size=0.25)\n    dtc = tree.DecisionTreeClassifier(random_state=0)\n    dtc.fit(X_train, y_train)\n    sn.append(i)\n    score.append(dtc.score(X_test,y_test))\n    #y_pred=dtc.predict(X_test)\n    #acc.append(metrics.accuracy_score(y_test, y_pred))\n\nresult=pd.DataFrame()\nresult[\"SN\"]=sn\nresult[\"Score\"]=score\n#result[\"Accuracy\"]=acc\nprint(result)    ","55782b08":"#Import scikit-learn metrics module for accuracy calculation\nfrom sklearn import metrics\n# Model Accuracy, how often is the classifier correct?\nprint(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))","ad3d6d17":"result.describe()","279848ce":"result.to_csv(\"r3.csv\")","dae9c726":"import keras\nfrom keras.models import Sequential\nfrom keras.layers import Activation\nfrom keras.layers.core import Dense\nfrom keras.optimizers import Adam\nfrom keras.metrics import categorical_crossentropy\n\n\n","1daa5aa7":"X_train.shape","1dddd1c0":"model = Sequential([\n    Dense(units=8, input_shape=(10,), activation='relu'),\n    Dense(units=8, activation='relu'),\n    Dense(units=7, activation='sigmoid')\n])","05d7e840":"model.compile(\n    optimizer=Adam(learning_rate=0.0001), \n    loss='sparse_categorical_crossentropy', \n    metrics=['accuracy']\n)","9c9401ec":"model.fit(\n    x=X_train, \n    y=y_train, \n    batch_size=10, \n    epochs=30, \n    shuffle=True, \n    verbose=2\n)","45c25ed0":"y_test.values","ac6c001e":"predictions=model.predict_classes(X_test)\ndf=pd.DataFrame()\ndf[\"Actual\"]=y_test.values\ndf[\"Predicted\"]=predictions\ncorrect_predictions = np.nonzero(predictions == y_test.values)[0]\nincorrect_predictions = np.nonzero(predictions != y_test.values)[0]\nprint(len(correct_predictions),\" classified correctly\")\nprint(len(incorrect_predictions),\" classified incorrectly\")","99afc8b3":"df","94805472":"#Import Random Forest Model\nfrom sklearn.ensemble import RandomForestClassifier\n\n#Create a Gaussian Classifier\nclf=RandomForestClassifier(n_estimators=30)\n\n#Train the model using the training sets y_pred=clf.predict(X_test)\nclf.fit(X_train,y_train)\n\nclf.score(X_test,y_test)","618ef96e":"#Import scikit-learn metrics module for accuracy calculation\nfrom sklearn import metrics\n# Model Accuracy, how often is the classifier correct?\ny_pred=clf.predict(X_test)\nprint(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))","4a1fcff1":"#Import Gaussian Naive Bayes model\nfrom sklearn.naive_bayes import GaussianNB\n\n#Create a Gaussian Classifier\nnb = GaussianNB()\n\n# Train the model using the training sets\nnb.fit(X_train,y_train)\n\n#Predict Output\nnb.score(X_test,y_test)","1962750b":"#Import svm model\nfrom sklearn import svm\n\n#Create a svm Classifier\nsv = svm.SVC(kernel='linear') # Linear Kernel\n\n#Train the model using the training sets\nsv.fit(X_train, y_train)\n\n#Predict the response for test dataset\n# y_pred = clf.predict(X_test)\nnb.score(X_test,y_test)","c06c8f11":"from sklearn.neighbors import KNeighborsClassifier\n\nmodel = KNeighborsClassifier(n_neighbors=2)\n\n# Train the model using the training sets\nmodel.fit(X_train, y_train)\n\nmodel.score(X_test,y_test)","bea01533":"#Import Gradient Boosting Classifier model\nfrom sklearn.ensemble import GradientBoostingClassifier\n\n#Create Gradient Boosting Classifier\ngb = GradientBoostingClassifier()\n\n#Train the model using the training sets\ngb.fit(X_train, y_train)\n\n#Predict the response for test dataset\ngb.score(X_test,y_test)","67a8239d":"# Run classification without scaling","309902f7":"# Use minmax scaler","0734070c":"# Predict with model","570da515":"# KNN","c9be70cf":"# Remove columns which you think is not required for learning ","71c2e385":"# Support Vector Machine","c2d494b5":"# Encode Categorical values","ab988c64":"# Random Forest","523db0ce":"# Understand the data","57cbe817":"# Remove CC and replace it with encoded data","597b900a":"# Naive Bayes","8d815706":"# Boosting ","3100cb75":"# Lets use standard scaler and see what happens","d8f8f42a":"# Check whether target is balanced or not","c80b5c86":"# Deep Learning","75d9e06e":"# Read the Data","647d296c":"# Deal with missing data\n\nAge,Cabin and Embarked are missing","4434b5d3":"# Create Encoded Columns","a2c71892":"# Divide into features and Target"}}