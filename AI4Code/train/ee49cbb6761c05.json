{"cell_type":{"73dad360":"code","d64aeea6":"code","bc65ba74":"code","257691ca":"code","08a3106e":"code","c2bc8a8b":"code","42289a9d":"code","3d20fc20":"code","1b40080b":"code","8bb4c71e":"code","050a26a2":"code","66205ee8":"code","00e1df91":"code","e3fc1999":"code","83a0c9e5":"code","5fe7fa88":"code","c034e1c8":"code","08315fe5":"code","208ac9a2":"code","8e410ee3":"code","159f48a0":"code","13865c3d":"code","35194dc4":"code","bfa66360":"code","9b5d15a6":"code","8ab0e6bd":"markdown","8542f94e":"markdown","955cf4ae":"markdown","251cfc2e":"markdown","0456798b":"markdown","22d70713":"markdown","40369ca6":"markdown","92b01697":"markdown","93d4ddf7":"markdown","14aca183":"markdown","124998ea":"markdown","5240dd90":"markdown","54f4c33f":"markdown","18733ba4":"markdown","1565bbe3":"markdown","3e0135c6":"markdown","941cc8d6":"markdown","d71dc0de":"markdown","a865f52e":"markdown","e0659d90":"markdown","45d8f29a":"markdown"},"source":{"73dad360":"import pandas as pd\n\nimport numpy as np\n\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n \n%matplotlib inline \n%config InlineBackend.figure_format = 'retina' \nimport warnings  \nwarnings.filterwarnings('ignore') \nimport os \nprint(os.listdir(\"..\/input\")) ","d64aeea6":"cars=pd.read_csv(\"..\/input\/car data.csv\")\ncars.sample()","bc65ba74":"cars['Car_Name'].value_counts()","257691ca":"cars.isnull().any()","08a3106e":"from sklearn.preprocessing import LabelEncoder\nlb = LabelEncoder()\ncars[\"trans_code\"] = lb.fit_transform(cars[\"Transmission\"])\ncars[\"Transmission\"].replace(cars[\"trans_code\"], inplace=True)\ncars[\"seller_code\"] = lb.fit_transform(cars[\"Seller_Type\"])\ncars[\"Seller_Type\"].replace(cars[\"seller_code\"], inplace=True)\ncars[\"fuel_code\"] = lb.fit_transform(cars[\"Fuel_Type\"])\ncars[\"Fuel_Type\"].replace(cars[\"fuel_code\"], inplace=True)","c2bc8a8b":"year=cars[\"Year\"]\nmile=cars[\"Kms_Driven\"]\nrate=mile\/((2019-year)*365)\ncars[\"Rating\"]=rate","42289a9d":"var = 'Kms_Driven'\ndata = pd.concat([cars[\"Selling_Price\"], cars[var]], axis=1)\ndata.plot.scatter(x=var, y='Selling_Price');","3d20fc20":"var = 'Year'\ndata = pd.concat([cars[\"Selling_Price\"], cars[var]], axis=1)\ndata.plot.scatter(x=var, y='Selling_Price',);","1b40080b":"var = 'Present_Price'\ndata = pd.concat([cars[\"Selling_Price\"], cars[var]], axis=1)\ndata.plot.scatter(x=var, y='Selling_Price',);","8bb4c71e":"from sklearn.model_selection import train_test_split\nimport statsmodels.api as sm","050a26a2":"X=cars[[\"Present_Price\",\"fuel_code\",\"seller_code\",\"trans_code\",\"Year\",\"Kms_Driven\",\"Owner\"]]\ny=cars[\"Selling_Price\"]\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2)\nX = sm.add_constant(X)\nmodel = sm.OLS(y_train.astype(float), X_train.astype(float)).fit()\npredictions = model.predict(X_test)\nmodel.summary()","66205ee8":"X=cars[[\"Present_Price\",\"fuel_code\",\"seller_code\",\"trans_code\",\"Rating\",\"Owner\"]]\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2)\nX = sm.add_constant(X)\nmodel = sm.OLS(y_train.astype(float), X_train.astype(float)).fit()\npredictions = model.predict(X_test)\nmodel.summary()","00e1df91":"X=cars[[\"Present_Price\",\"fuel_code\",\"seller_code\",\"trans_code\",\"Year\",\"Kms_Driven\",\"Owner\"]]\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2)\nX = sm.add_constant(X)\nmodel1 = sm.OLS(y_train.astype(float), X_train.astype(float)).fit()\nr1=model1.rsquared\nprint('R squared value=', +r1)\n\npredictions = model1.predict(X_test)\npredictions=pd.DataFrame(predictions)\npredictions=predictions.reset_index()\ntest_index=y_test.reset_index()[\"Selling_Price\"]\nax=test_index.plot(label=\"originals\",figsize=(12,6),linewidth=2,color=\"r\")\nax=predictions[0].plot(label = \"predictions\",figsize=(12,6),linewidth=2,color=\"g\")\nplt.legend(loc='upper right')\nplt.title(\"Linear Regressor\")\nplt.xlabel(\"index\")\nplt.ylabel(\"values\")\nplt.show()","e3fc1999":"X=cars[[\"Present_Price\",\"fuel_code\",\"trans_code\",\"seller_code\",\"Rating\",\"Owner\"]]\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2)\nX = sm.add_constant(X)\nmodel2 = sm.OLS(y_train.astype(float), X_train.astype(float)).fit()\nr2=model2.rsquared\nprint('R squared value=', +r2)","83a0c9e5":"X=cars[[\"fuel_code\",\"seller_code\",\"trans_code\",\"Year\",\"Kms_Driven\",\"Owner\"]]\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2)\nX = sm.add_constant(X)\nmodel3 = sm.OLS(y_train.astype(float), X_train.astype(float)).fit()\nr3=model3.rsquared\nprint('R squared value=', +r3)","5fe7fa88":"X=cars[[\"Present_Price\",\"fuel_code\",\"trans_code\",\"Year\",\"Kms_Driven\",\"Owner\"]]\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2)\nX = sm.add_constant(X)\nmodel4 = sm.OLS(y_train.astype(float), X_train.astype(float)).fit()\nr4=model4.rsquared\nprint('R squared value=', +r4)","c034e1c8":"X=cars[[\"Present_Price\",\"seller_code\",\"trans_code\",\"Year\",\"Kms_Driven\",\"Owner\"]]\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2)\nX = sm.add_constant(X)\nmodel5 = sm.OLS(y_train.astype(float), X_train.astype(float)).fit()\nr5=model5.rsquared\nprint('R squared value=', +r5)","08315fe5":"X=cars[[\"Present_Price\",\"fuel_code\",\"seller_code\",\"Year\",\"Kms_Driven\",\"Owner\"]]\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2)\nX = sm.add_constant(X)\nmodel6 = sm.OLS(y_train.astype(float), X_train.astype(float)).fit()\nr6=model6.rsquared\nprint('R squared value=', +r6)","208ac9a2":"##Plot results of linear regression by considering different features\n\ndata=[r1,r2,r3,r4,r5,r6]\n\nplt.subplots(figsize = (15,8))\nax = plt.plot(data)\nplt.axis([0, 5, 0.6, 1])\nplt.ylabel('Rsquared value',size=25)\nplt.xlabel('Trial',size=25)\nlabels = (['default', 'w rating', 'w\/o present \\n price','w\/o seller','w\/o fuel','w\/o \\n transmission'])\nval = [0,1,2,3,4,5]  \nplt.xticks(val, labels);\nplt.show()","8e410ee3":"from sklearn.ensemble import AdaBoostRegressor\nfrom sklearn.datasets import make_regression","159f48a0":"regr = AdaBoostRegressor()\nX=cars[[\"Present_Price\",\"fuel_code\",\"trans_code\",\"Year\",\"Kms_Driven\",\"Owner\"]]\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2)\nmodel=regr.fit(X_train, y_train) \nx3=regr.score(X,y)\nprint('R squared value=', +x3)\npredictions = model.predict(X_test)\npredictions=pd.DataFrame(predictions)\n\npredictions=predictions.reset_index()\ntest_index=y_test.reset_index()[\"Selling_Price\"]\nax=test_index.plot(label=\"originals\",figsize=(12,6),linewidth=2,color=\"r\")\nax=predictions[0].plot(label = \"predictions\",figsize=(12,6),linewidth=2,color=\"g\")\nplt.legend(loc='upper right')\nplt.title(\"ADABOOST Regressor\")\nplt.xlabel(\"index\")\nplt.ylabel(\"values\")\nplt.show()","13865c3d":"from sklearn.tree import DecisionTreeRegressor\ndtr = DecisionTreeRegressor()\nfrom sklearn.metrics import r2_score\nfrom sklearn.model_selection import cross_val_score","35194dc4":"X=cars[[\"Present_Price\",\"fuel_code\",\"trans_code\",\"Year\",\"Kms_Driven\",\"Owner\"]]\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.1)\ndtr.fit(X_train,y_train)\npredicts=dtr.predict(X_test)\nprediction=pd.DataFrame(predicts)\nR_2=r2_score(y_test,prediction)\n\n\n    \n    # Printing results  \nprint(dtr,\"\\n\") \nprint(\"R squared value=\",R_2,\"\\n\")\n\n    \n    # Plot for prediction vs originals\ntest_index=y_test.reset_index()[\"Selling_Price\"]\nax=test_index.plot(label=\"originals\",figsize=(12,6),linewidth=2,color=\"r\")\nax=prediction[0].plot(label = \"predictions\",figsize=(12,6),linewidth=2,color=\"g\")\nplt.legend(loc='upper right')\nplt.title(\"Decision Tree Regressor\")\nplt.xlabel(\"index\")\nplt.ylabel(\"values\")\nplt.show()","bfa66360":"regr_2 = AdaBoostRegressor(DecisionTreeRegressor())\n\nmodel=regr_2.fit(X_train, y_train)\n\ny_2 = regr_2.predict(X_test)\n\nx4=regr_2.score(X_test,y_test)\nprint('R squared value=', +x4)\n\npredictions=pd.DataFrame(y_2)\npredictions=predictions.reset_index()\ntest_index=y_test.reset_index()[\"Selling_Price\"]\nax=test_index.plot(label=\"originals\",figsize=(12,6),linewidth=2,color=\"r\")\nax=predictions[0].plot(label = \"predictions\",figsize=(12,6),linewidth=2,color=\"g\")\nplt.legend(loc='upper right')\nplt.title(\"Decision tree with ADABOOST Regressor\")\nplt.xlabel(\"index\")\nplt.ylabel(\"values\")\nplt.show()","9b5d15a6":"from tabulate import tabulate\nprint(tabulate([['Linear regression', r1], ['Adaboost linear regression', x3],['Decision tree regressor',R_2],['Decision tree with adaboost',x4]], headers=['Model used', 'Rsquared value']))","8ab0e6bd":"    AdaBoost regressor  \n\nAdaptive boosting is a boosting technique which helps you combine multiple \u201cweak classifiers\u201d into a single \u201cstrong classifier\u201d. \n\nEach new classifier\/predictor is given a training set where the difficult examples are increasingly represented, this is achieved either through weighting or resampling. ","8542f94e":"We can see that multicollinearity existed between two of our variables Year and Kms driven","955cf4ae":"Similarly, by removing categorical values we observe that we get a slightly higher value of Rsquared when we neglect seller type feature. ","251cfc2e":"Convert categorical variables (Fuel_Type,Transmission,Seller_Type) into or labels into numeric form so as to convert it into the machine-readable form. \n\nMachine learning algorithms can then decide in a better way on how those labels must be operated. \n\nWe use label encoder to transform values of Fuel type, Transmission and Seller type to 0,1 and 2. ","0456798b":"    Year \n\nAs mentioned earlier, older cars tend to lower longevity and hence sell at a lower price compared to newer cars maintained in good condition. ","22d70713":"Now we begin to remove each variable one after the other to see which values give the best result. \nIf we replace Year and Kms_Driven variables with Rating feature that we created the following results are obtained","40369ca6":"Removing Present price we see the performance of the model goes down drastically. :","92b01697":"    Decision tree regressor \n\nDecision tree regressor is a model of decisions and all of their possible results, including outcomes, input costs and utility. \n\nDecision tree algorithm falls under the category of supervised learning algorithms. It works for both continuous as well as categorical output variables. ","93d4ddf7":"*3. Understanding the data and Data Visualization *\n\nPlot relationship with numerical variables. \n\nFollowing variables can play an important role in this problem: \n\n    1. Kms_Driven \n    2. Year\n    3. Present_Price","14aca183":"    Linear regressor  \n\nBuild a linear regression model by ordinary least squares and test it for given data set. \nWe check the significance of different features by comparing their p value to see if it is below the confidence level of 0.05. \n\nFit the model for different combinations of independent variables to see which yields the best results.  ","124998ea":"5. Conclusions \n\nEven though we obtained a fairly high Rsqured value for the linear regression algorithm, our results were improved by adabost technique. \n\nWe tested the model for different combinations of independent variables to figure out which yields the best results. \n\nThe decision tree model turned out to be the best suited algorithm to predict the selling price of used cars. ","5240dd90":"Let us have a look at the number of data entries we have pertining to each vehicle.","54f4c33f":"    Present_Price \n\nWe can observe a linear behavior. There are no outliers above the line because the selling price of a used vehicle is always lower than the present value of a new one. ","18733ba4":"*2. Data handling *\n\nCheck for null values. Since our data has none, we can skip this step. ","1565bbe3":"Removing Fuel type:","3e0135c6":"Removing Transmission:","941cc8d6":"We combine the two variables Year and Kms-Driven to a new feature Rating which is a measure of number of kilometers driven each day.\n\nRating is the ratio of number of kilometers driven in total to the approximate number of days since the vehicle has been purchased.","d71dc0de":"    Kms_Driven \n\nAs expected, this graph shows a negative relation between kilometers driven and the selling price. \nThe more the kilometers driven, the lesser the selling price because this means more wear and tear of various parts of the car which reduces its value. ","a865f52e":"    Decision tree regression with Adaboost ","e0659d90":"*4. Training and testing a model *\n\nWe split the data into training and testing set. \nWe first fit each model on the training set. \nThen we predict the dependent variable in the test set. \nNow we compare the obtained results with the actual data at hand and get a score for the performance of each algorithm. ","45d8f29a":"*1. INTRODUCTION*\n\nWe will be working on the vehicle dataset from cardekho dataset. This dataset contains information about used cars listed on www.cardekho.com.  \n\nWe will predict the selling price of a used car based on various factors such as  \n\n        Kms_Driven     -> kilometers driven \n\n        Year                   -> year of purchase  \n\n        Present_Price  -> present price of a new car \n\n        Fuel_Type         -> type of fuel being used (petrol, diesel, CNG) \n\n        Transmission    -> automatic or manual gear transmission  \n\n        Owner               -> number of previous owners \n\n        Seller_Type       -> dealer or individual  "}}