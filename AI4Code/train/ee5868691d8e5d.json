{"cell_type":{"16d9fb2c":"code","c40fed63":"code","6da1c7de":"code","58c0be6b":"code","298f2859":"code","b0333199":"code","c659d999":"code","09e1b65a":"code","d810ed51":"code","1d84fd6d":"code","2c1db6dd":"code","83fa77f4":"code","4eeb7f96":"code","1d378820":"code","f19d63fd":"code","28bab051":"code","7d6eb6e1":"code","1e41d52d":"code","d0cd4c20":"code","611508ee":"code","4f139b92":"code","9c8a3ad9":"code","e5b517c0":"code","a352aee9":"code","833c5bde":"code","3d1315e5":"code","a3860e8c":"code","3da3c8b6":"code","f39035e0":"code","8c3203db":"code","c78c7b99":"code","f799901b":"code","d72b1467":"code","0709a536":"code","a6bd604b":"code","eb0c16a4":"code","0de90782":"code","e6719f58":"code","65814f3d":"code","71b9d541":"code","ef20acb7":"code","24ac5861":"code","e38c8f49":"code","2f3dd331":"markdown","a5ec5c80":"markdown","c189a27d":"markdown","ba8077f6":"markdown","9690d7ae":"markdown","deb9dd17":"markdown","924986be":"markdown","a64b8168":"markdown","7556e45f":"markdown","37243bc8":"markdown","45d7d3be":"markdown","ba14a849":"markdown","788cab5a":"markdown","9fc80835":"markdown","b6265d68":"markdown","f958f2cc":"markdown","1db757c2":"markdown","74e70403":"markdown","6e696109":"markdown","451be9e9":"markdown","759c3677":"markdown","9f1b60f7":"markdown","06bbb5e2":"markdown"},"source":{"16d9fb2c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","c40fed63":"import pandas as pd\nfrom nltk import *\n!pip install pyspellchecker\n!pip install contractions\n!pip install inflect","6da1c7de":"train = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/test.csv')","58c0be6b":"train.shape, test.shape","298f2859":"import seaborn as sns \nimport matplotlib.pyplot as plt\n\nx = train.target.value_counts()\nsns.barplot(x.index,x)\nplt.gca().set_ylabel('samples')","b0333199":"fig,(ax1,ax2)=plt.subplots(1,2,figsize=(10,5))\n\ntweet_len = train[train['target']==1]['text'].str.len()\nax1.hist(tweet_len,color='blue')\nax1.set_title('disaster tweets')\n\ntweet_len = train[train['target']==0]['text'].str.len()\nax2.hist(tweet_len,color='CRIMSON')\nax2.set_title('Not disaster tweets')\nfig.suptitle('Characters in tweets')\nplt.show()","c659d999":"fig,(ax1,ax2)=plt.subplots(1,2,figsize=(10,5))\n\ntweet_len = train[train['target']==1]['text'].str.split().map(lambda x: len(x))\nax1.hist(tweet_len,color='blue')\nax1.set_title('disaster tweets')\n\n\ntweet_len = train[train['target']==0]['text'].str.split().map(lambda x: len(x))\nax2.hist(tweet_len,color='CRIMSON')\nax2.set_title('Not disaster tweets')\nfig.suptitle('Words in a tweet')\nplt.show()","09e1b65a":"fig,(ax1,ax2)=plt.subplots(1,2,figsize=(10,5))\n\n\nword = train[train['target']==1]['text'].str.split().apply(lambda x : np.mean([len(i) for i in x]))\nsns.distplot(word,ax= ax1,color='red')\nax1.set_title('disaster')\n\n\nword=train[train['target']==0]['text'].str.split().apply(lambda x : np.mean([len(i) for i in x]))\nsns.distplot(word,ax= ax2,color='green')\nax2.set_title('Not disaster')\nfig.suptitle('Average word length in each tweet')","d810ed51":"!pip install nltk","1d84fd6d":"from collections import defaultdict\nimport nltk\nimport nltk.corpus\nfrom nltk.corpus import stopwords\nnltk.download('stopwords')\nstop=set(stopwords.words('english'))\n\ncorpus=[]\n    \nfor x in train['text'].str.split():\n    for i in x:\n        corpus.append(i)\n        \ndic = defaultdict(int)\n\nfor word in corpus:\n    if word not in stop:\n        dic[word]+=1\n\ntop=sorted(dic.items(), key=lambda x:x[1],reverse=True)[:30] \n    \n\n\nx,y=zip(*top)\nplt.rcParams[\"figure.figsize\"] = (20,10)\nplt.bar(x,y , color ='red')","2c1db6dd":"from collections import defaultdict\nimport nltk\nimport nltk.corpus\nfrom nltk.corpus import stopwords\nnltk.download('stopwords')\nstop=set(stopwords.words('english'))\n\ncorpus=[]\n    \nfor x in train['text'].str.split():\n    for i in x:\n        corpus.append(i)\n        \ndic = defaultdict(int)\n\nfor word in corpus:\n    if word in stop:\n        dic[word]+=1\n\ntop=sorted(dic.items(), key=lambda x:x[1],reverse=True)[:30] \n    \n\n\nx,y=zip(*top)\nplt.rcParams[\"figure.figsize\"] = (20,10)\n#plt.bar(x,y , color ='green')","83fa77f4":"plt.figure(figsize=(10,5))\nimport string\ndic = defaultdict(int)\nspecial = string.punctuation\nfor i in (corpus):\n    if i in special:\n        dic[i]+=1\n      \nx, y=zip(*dic.items())\nplt.barh(x,y ,color = 'purple')","4eeb7f96":"train['target_mean'] = train.groupby('keyword')['target'].transform('mean')\n\nfig = plt.figure(figsize=(8, 72), dpi=100)\n\nsns.countplot(y=train.sort_values(by='target_mean', ascending=False)['keyword'],\nhue=train.sort_values(by='target_mean', ascending=False)['target'])\n\nplt.tick_params(axis='x', labelsize=15)\nplt.tick_params(axis='y', labelsize=12)\nplt.legend(loc=1)\nplt.title('Target Distribution in Keywords')\n\nplt.show()","1d378820":"train.drop(columns=['target_mean'], inplace=True)","f19d63fd":"train","28bab051":"import re\nimport contractions\nimport inflect\nfrom spellchecker import SpellChecker\n\ndef cleaner(row):\n    # Remove links\n    new_row = re.sub(r'http\\S+', ' ', row)\n    \n    # Remove users\n    new_row = re.sub(r'@\\S+', ' ', new_row)\n    \n    # Remove hashtags\n    new_row = re.sub(r'#\\S+', ' ', new_row)\n    #new_row = re.sub('#', ' ', new_row)\n    \n    # Remove html\n    html=re.compile(r'<.*?>')\n    new_row = html.sub(r' ', new_row)\n    \n    # Remove punctuation\n    new_row = re.sub(r'[^A-Za-z0-9]+', ' ', new_row)\n    \n    # Remove extra spaces\n    new_row = re.sub(r'\\s\\s+', ' ', new_row)\n    \n    # Contractions\n    new_row = contractions.fix(new_row)\n    \n    # Lowercase\n    new_row = new_row.lower()\n    \n    # Remove special characters\n    new_row = re.sub(r'[^a-z]', ' ', new_row)\n    \n    return new_row","7d6eb6e1":"def preprocess(df):\n    df['keyword'] = df['keyword'].fillna(' ')\n    df['location'] = df['location'].fillna(' ')\n    \n    df['text'] = df['keyword'] + ' ' + df['location'] + ' ' + df['text']\n    df = df.drop(columns=['keyword', 'location'])\n    \n    return df","1e41d52d":"\ndef structure(df):\n    stop = stopwords.words('english')\n    \n    df['no_stopwords'] = df['clean_text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n    df['text_tokens'] = df['no_stopwords'].apply(lambda x: word_tokenize(x))\n    \n    def word_lemmatizer(text):\n        lem_text = [WordNetLemmatizer().lemmatize(i) for i in text]\n        return lem_text\n    \n    df['text_clean_tokens'] = df['text_tokens'].apply(lambda x: word_lemmatizer(x))\n    \n    # Remove words shorter than 3 chars\n    df['text_clean_tokens'] = df['text_clean_tokens'].apply(lambda x: [word for word in x if len(word)>3])\n    \n    df['finished_lemma'] = df['text_clean_tokens'].apply(lambda x: \" \".join(x))\n\n    df = df.drop(columns=['no_stopwords', 'text_tokens', 'text_clean_tokens'])\n    \n    return df","d0cd4c20":"cleaned_train = train\n\ncleaned_train = preprocess(cleaned_train)\ncleaned_train['clean_text'] = cleaned_train.text.apply(lambda x: cleaner(x))\ncleaned_train = structure(cleaned_train)\ndf = cleaned_train","611508ee":"sample_df = df.sample(10)\nsample_df","4f139b92":"for text in sample_df.text.tail(10):\n    print(text, '\\n')","9c8a3ad9":"for text in sample_df.clean_text.tail(10):\n    print(text, '\\n')","e5b517c0":"for text in sample_df.finished_lemma.tail(10):\n    print(text, '\\n')","a352aee9":"from sklearn.model_selection import train_test_split\n\nX_train, X_val, Y_train, Y_val = train_test_split(df['finished_lemma'], \n                                                  df['target'], \n                                                  test_size = 0.25, \n                                                  random_state = 10)\n\nprint(X_train.shape)\nprint(X_val.shape)\nprint(Y_train.shape)\nprint(Y_val.shape)","833c5bde":"!pip install keras","3d1315e5":"import tensorflow as tf\nfrom tensorflow.keras.layers import Dense, Input\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nimport transformers\nfrom tqdm.notebook import tqdm\nfrom tokenizers import BertWordPieceTokenizer","a3860e8c":"from transformers import BertTokenizer\n\ntokenizer = BertTokenizer.from_pretrained('bert-large-uncased')\n\ndef bert_encode(data, maximum_length) :\n    input_ids = []\n    attention_masks = []\n\n    for text in data:\n        encoded = tokenizer.encode_plus(\n            text, \n            max_length=maximum_length,\n            pad_to_max_length=True,\n            return_attention_mask=True,\n        )\n        input_ids.append(encoded['input_ids'])\n        attention_masks.append(encoded['attention_mask'])\n        \n    return np.array(input_ids), np.array(attention_masks)","3da3c8b6":"texts = X_train\n\ntrain_input_ids, train_attention_masks = bert_encode(texts, 60)","f39035e0":"import tensorflow as tf\nfrom tensorflow.keras.optimizers import Adam\n\ndef create_model(bert_model):\n    \n    input_ids = tf.keras.Input(shape=(60,),dtype='int32')\n    attention_masks = tf.keras.Input(shape=(60,),dtype='int32')\n\n    output = bert_model([input_ids, attention_masks])\n    output = output[1]\n    output = tf.keras.layers.Dense(32,activation='relu')(output)\n    output = tf.keras.layers.Dropout(0.2)(output)\n    output = tf.keras.layers.Dense(1,activation='sigmoid')(output)\n    \n    model = tf.keras.models.Model(inputs = [input_ids,attention_masks],outputs = output)\n    model.compile(Adam(lr=1e-5), loss='binary_crossentropy', metrics=['accuracy'])\n    return model","8c3203db":"from transformers import TFBertModel\n\nbert_model = TFBertModel.from_pretrained('bert-base-uncased')","c78c7b99":"model = create_model(bert_model)\nmodel.summary()","f799901b":"checkpoint = tf.keras.callbacks.ModelCheckpoint('large_model.h5',\n                                                monitor='val_loss', \n                                                save_best_only = True, \n                                                save_weights_only = True)\n\nearly_stopping = tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", \n                                                  min_delta=0, \n                                                  patience=0,\n                                                  verbose=0,\n                                                  mode=\"auto\",\n                                                  baseline=None,\n                                                  restore_best_weights=False)\n\nhistory = model.fit(\n    [train_input_ids, train_attention_masks],\n    Y_train,\n    validation_split=0.2, \n    epochs=3,\n    batch_size=10,\n    callbacks = [checkpoint, early_stopping])","d72b1467":"def plot_learning_curves(history, arr):\n    fig, ax = plt.subplots(1, 2, figsize=(20, 5))\n    for idx in range(2):\n        ax[idx].plot(history.history[arr[idx][0]])\n        ax[idx].plot(history.history[arr[idx][1]])\n        ax[idx].legend([arr[idx][0], arr[idx][1]],fontsize=18)\n        ax[idx].set_xlabel('A ',fontsize=16)\n        ax[idx].set_ylabel('B',fontsize=16)\n        ax[idx].set_title(arr[idx][0] + ' X ' + arr[idx][1],fontsize=16)","0709a536":"plot_learning_curves(history, [['loss', 'val_loss'],['accuracy', 'val_accuracy']])","a6bd604b":"def show_metrics(pred_tag, y_test):\n    print(\"F1-score: \", f1_score(pred_tag, y_test))\n    print(\"Precision: \", precision_score(pred_tag, y_test))\n    print(\"Recall: \", recall_score(pred_tag, y_test))\n    print(\"Acuracy: \", accuracy_score(pred_tag, y_test))\n    print(\"-\"*50)\n    print(classification_report(pred_tag, y_test))","eb0c16a4":"texts = X_val\n\nval_inputs = bert_encode(texts, 60)","0de90782":"from sklearn.metrics import (\n    precision_score, \n    recall_score, \n    f1_score, \n    classification_report,\n    accuracy_score\n)\n\npreds = model.predict(val_inputs)\n","e6719f58":"preds_rounded = np.round(preds)\nshow_metrics(preds_rounded, Y_val)","65814f3d":"cleaned_train = test\n\ncleaned_train = preprocess(cleaned_train)\ncleaned_train['clean_text'] = cleaned_train.text.apply(lambda x: cleaner(x))\ncleaned_train = structure(cleaned_train)\ndf_test = cleaned_train\ndf_test","71b9d541":"texts = df_test['finished_lemma']\n\ntest_input_ids = bert_encode(texts, 60)","ef20acb7":"#load model with best losses\nmodel.load_weights('large_model.h5')\n\nfinal_predictions = model.predict(test_input_ids)","24ac5861":"final_predictions = np.round(final_predictions).astype(int)\nsubmission_df = pd.DataFrame()\nsubmission_df['id'] = df_test['id']\nsubmission_df['target'] = final_predictions\nsubmission = submission_df.to_csv('Results.csv',index = False)\n\n\nsubmission_df","e38c8f49":"#submission = submission_df.to_csv('Results.csv',index = False)","2f3dd331":"mnb_classifier = MultinomialNB()\nmnb_classifier.fit(train_features,train_labels)\nmnb_prediction = mnb_classifier.predict(test_features)","a5ec5c80":"# Text Cleaning","c189a27d":"# Splitting dataset","ba8077f6":"# Test set","9690d7ae":"# Preprocessing","deb9dd17":"# TFIDF and MultinomialNB","924986be":"from sklearn import feature_extraction\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n\ntfidf = feature_extraction.text.TfidfVectorizer(encoding='utf-8',\n                       ngram_range=(1,1),\n                       max_features=5000,\n                       norm='l2',\n                       sublinear_tf=True)\n\n\ntrain_features = tfidf.fit_transform(X_train).toarray()\ntest_features = tfidf.fit_transform(X_test).toarray()\n\ntrain_labels = Y_train\ntest_labels = Y_test\n\nprint(train_features.shape)\nprint(train_labels.shape)\nprint(test_features.shape)\nprint(test_labels.shape)","a64b8168":"# Training set","7556e45f":"submission_df = pd.DataFrame()\nsubmission_df['id'] = df_test['id']\nsubmission_df['target'] = final_predictions","37243bc8":"# Preprocess and cleaning of training data","45d7d3be":"print(classification_report(test_labels, mnb_prediction))","ba14a849":"final_predictions = mnb_classifier.predict(test_vectorizer)","788cab5a":"cleaned_train = test\n\ncleaned_train = preprocess(cleaned_train)\ncleaned_train['clean_text'] = cleaned_train.text.apply(lambda x: cleaner(x))\ncleaned_train = structure(cleaned_train)\ndf_test = cleaned_train","9fc80835":"# Trying bert transformers","b6265d68":"# Validation set","f958f2cc":"testing_accuracy = accuracy_score(test_labels, mnb_prediction)\nprint(testing_accuracy)","1db757c2":"# Working with test data","74e70403":"# Display Data","6e696109":"import pandas as pd\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score","451be9e9":"training_accuracy = accuracy_score(train_labels, mnb_classifier.predict(train_features))\nprint(training_accuracy)","759c3677":"test_vectorizer = tfidf.transform(df_test['finished_lemma']).toarray()\ntest_vectorizer.shape","9f1b60f7":"test.shape","06bbb5e2":"# Lemmatize and Stopwords"}}