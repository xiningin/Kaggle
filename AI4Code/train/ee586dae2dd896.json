{"cell_type":{"dc4e48c2":"code","b2ab65ac":"code","557ac5cb":"code","5b4016e3":"code","9564831c":"code","92a1b2d4":"code","53a341c9":"code","04f76f0d":"code","b661fa5d":"code","1f1f3123":"code","d38dc41e":"code","c1bb1a35":"code","bafe8af2":"code","75e72376":"code","46f5c704":"code","6eec2fed":"code","2017a85a":"code","f77ad551":"code","2ddb402f":"code","bbe4cb7d":"code","558c2790":"code","23fbbac7":"code","7bebdc20":"code","473cde4e":"code","934f4e03":"code","b67fe02d":"code","00c8e8db":"code","02a2ce99":"code","633fc61a":"code","b5149ea1":"code","ba7019a2":"code","2fe63993":"code","b90a9b5b":"code","ba5accc7":"code","e6707f86":"code","b8704a7a":"code","5def222d":"code","8e8595a6":"code","7c655cee":"code","bf8e73c2":"code","ee91af5b":"code","a3f5502b":"code","ddf6031e":"code","ad5c88ba":"code","37129405":"code","7589f24d":"code","d557fe22":"code","19d14f0b":"code","c18eb06e":"code","6261eb2e":"code","e33f3736":"code","25489a22":"code","31fd16b2":"code","7ca2d8a4":"code","11e2e765":"code","33f340f1":"code","1215db3f":"code","4d8e1940":"code","d38ee37e":"code","e9d3cbdd":"code","ecd7d095":"code","79afea0c":"code","732f381a":"code","11e1fa38":"code","1bba40f2":"code","184aaf23":"code","f3079bf1":"code","9f5536cf":"code","27a916a8":"code","317e4d79":"code","8efd7024":"markdown","70229448":"markdown","1edfdc43":"markdown","62486132":"markdown","03f30a4d":"markdown","7bd2278c":"markdown","956be316":"markdown","ae6c84e8":"markdown","ddda8f83":"markdown","b3979740":"markdown","2b089c35":"markdown","473add5b":"markdown","ac795845":"markdown","3313a3aa":"markdown","27b8d614":"markdown","3429606b":"markdown","53d1b04f":"markdown","b631adb1":"markdown","5f7da8ed":"markdown","05c517a9":"markdown","e62913d0":"markdown","1e5b63bc":"markdown","4d76da2f":"markdown","fd4d3ab5":"markdown","1d32d4ea":"markdown","96eb1ea2":"markdown","0cdba6d4":"markdown","06894ca0":"markdown","483b356b":"markdown","68ee95d0":"markdown","be980678":"markdown","7d66f387":"markdown","5296138e":"markdown","3bdac307":"markdown","3d82ccfd":"markdown","49f6eee7":"markdown","cbba0028":"markdown","fcdad403":"markdown","1b918a27":"markdown","38aed6f8":"markdown","aa8bafed":"markdown","d0df31bc":"markdown","9690454d":"markdown","a8c1e466":"markdown"},"source":{"dc4e48c2":"import re\nimport gc\nimport os\nimport time\nimport random\nimport operator\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom tqdm import tqdm\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\ntqdm.pandas()\n\nfrom nltk import word_tokenize\nfrom collections import Counter\nfrom sklearn.utils import shuffle\nfrom tqdm import tqdm_notebook as tqdm\nfrom nltk.tokenize import TweetTokenizer\nfrom multiprocessing import Pool, cpu_count\nfrom keras.preprocessing.text import Tokenizer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LinearRegression\nfrom keras.preprocessing.sequence import pad_sequences\nfrom sklearn.metrics import roc_curve, precision_recall_curve, f1_score\nfrom sklearn.model_selection import train_test_split, StratifiedKFold, RepeatedStratifiedKFold\n\nimport torch\nimport torch.nn as nn\nimport torch.utils.data\n\nimport psutil\nimport multiprocessing\nimport markovify as mk\n\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)","b2ab65ac":"def seed_everything(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    tf.set_random_seed(seed)","557ac5cb":"seed = 5583\nseed_everything(seed)\n\nbegin = time.time()\nsns.set_style('whitegrid')","5b4016e3":"GLOVE = '..\/input\/embeddings\/glove.840B.300d\/glove.840B.300d.txt'\nPARAGRAM = '..\/input\/embeddings\/paragram_300_sl999\/paragram_300_sl999.txt'\nFASTTEXT = '..\/input\/embeddings\/wiki-news-300d-1M\/wiki-news-300d-1M.vec'","9564831c":"train_df = pd.read_csv(\"..\/input\/train.csv\")\ntest_df = pd.read_csv(\"..\/input\/test.csv\")\nprint(\"Train shape : \",train_df.shape)\nprint(\"Test shape : \",test_df.shape)","92a1b2d4":"augment_insincere_count = 0","53a341c9":"class MarkovifyTextAugmentor(object):\n    def __init__(self, docs, gen_max_len, filter_min_len=10):\n        \"\"\"\n        Build Markov models from large corpora of text, and generating random sentences from that.\n        \"\"\"\n        self.mk_text_model = mk.Text(docs)\n        self.gen_max_len = gen_max_len\n        self.filter_min_len = filter_min_len\n    \n    def genenerate(self, count):\n        texts = []\n        for _ in range(count):\n            text = self.mk_text_model.make_short_sentence(self.gen_max_len)\n            if text and len(text.split(' ')) > self.filter_min_len:\n                texts.append(text)\n        return texts","04f76f0d":"insincere_df = train_df[train_df['target'] == 1]\ninsincere_texts = insincere_df['question_text'].apply(\" \".join)\nmedian_length = int(insincere_texts.str.len().median())\n\nmk_augmentor = MarkovifyTextAugmentor(insincere_texts, median_length, filter_min_len=6)","b661fa5d":"%%time\ndef gen_text_process(count):\n    return mk_augmentor.genenerate(count)\n\nnum_cores = psutil.cpu_count()  # number of cores on your machine\npool = multiprocessing.Pool(processes=num_cores)\n\napply_results = []\nprocess_counts = augment_insincere_count \/\/ num_cores\nfor _ in range(num_cores):\n    apply_results.append(pool.apply_async(gen_text_process, (process_counts,)))\n\npool.close()\npool.join()\n\ngenerated_texts = []\nfor res in apply_results:\n    generated_texts.extend(res.get())\n    \ngen_df = pd.DataFrame({'question_text': generated_texts, 'target': [1] * len(generated_texts)})","1f1f3123":"print('Generated {} insincere questions'.format(len(generated_texts)))","d38dc41e":"if augment_insincere_count > 0:\n    train_df = pd.concat((train_df, gen_df), axis=0, sort=True)\n    train_df = shuffle(train_df, random_state=seed)","c1bb1a35":"toxic_words = ['4r5e', '5h1t', '5hit', 'a55', 'anal', 'anus', 'ar5e', 'arrse', 'arse', 'ass', 'ass-fucker', 'asses', 'assfucker', 'assfukka', 'asshole', 'assholes', 'asswhole', 'a_s_s', 'b!tch', 'b00bs', 'b17ch', 'b1tch', 'ballbag', 'balls', 'ballsack', 'bastard', 'beastial', 'beastiality', 'bellend', 'bestial', 'bestiality', 'bi+ch', 'biatch', 'bitch', 'bitcher', 'bitchers', 'bitches', 'bitchin', 'bitching', 'bloody', 'blow', 'job', 'blowjob', 'blowjobs', 'boiolas', 'bollock', 'bollok', 'boner', 'boob', 'boobs', 'booobs', 'boooobs', 'booooobs', 'booooooobs', 'breasts', 'buceta', 'bugger', 'bum', 'bunny', 'fucker', 'butt', 'butthole', 'buttmuch', 'buttplug', 'c0ck', 'c0cksucker', 'carpet', 'muncher', 'cawk', 'chink', 'cipa', 'cl1t', 'clit', 'clitoris', 'clits', 'cnut', 'cock', 'cock-sucker', 'cockface', 'cockhead', 'cockmunch', 'cockmuncher', 'cocks', 'cocksuck', 'cocksucked', 'cocksucker', 'cocksucking', 'cocksucks', 'cocksuka', 'cocksukka', 'cok', 'cokmuncher', 'coksucka', 'coon', 'cox', 'crap', 'cum', 'cummer', 'cumming', 'cums', 'cumshot', 'cunilingus', 'cunillingus', 'cunnilingus', 'cunt', 'cuntlick', 'cuntlicker', 'cuntlicking', 'cunts', 'cyalis', 'cyberfuc', 'cyberfuck', 'cyberfucked', 'cyberfucker', 'cyberfuckers', 'cyberfucking', 'd1ck', 'damn', 'dick', 'dickhead', 'dildo', 'dildos', 'dink', 'dinks', 'dirsa', 'dlck', 'dog-fucker', 'doggin', 'dogging', 'donkeyribber', 'doosh', 'duche', 'dyke', 'ejaculate', 'ejaculated', 'ejaculates', 'ejaculating', 'ejaculatings', 'ejaculation', 'ejakulate', 'f', 'u', 'c', 'k', 'f', 'u', 'c', 'k', 'e', 'r', 'f4nny', 'fag', 'fagging', 'faggitt', 'faggot', 'faggs', 'fagot', 'fagots', 'fags', 'fanny', 'fannyflaps', 'fannyfucker', 'fanyy', 'fatass', 'fcuk', 'fcuker', 'fcuking', 'feck', 'fecker', 'felching', 'fellate', 'fellatio', 'fingerfuck', 'fingerfucked', 'fingerfucker', 'fingerfuckers', 'fingerfucking', 'fingerfucks', 'fistfuck', 'fistfucked', 'fistfucker', 'fistfuckers', 'fistfucking', 'fistfuckings', 'fistfucks', 'flange', 'fook', 'fooker', 'fuck', 'fucka', 'fucked', 'fucker', 'fuckers', 'fuckhead', 'fuckheads', 'fuckin', 'fucking', 'fuckings', 'fuckingshitmotherfucker', 'fuckme', 'fucks', 'fuckwhit', 'fuckwit', 'fudge', 'packer', 'fudgepacker', 'fuk', 'fuker', 'fukker', 'fukkin', 'fuks', 'fukwhit', 'fukwit', 'fux', 'fux0r', 'f_u_c_k', 'gangbang', 'gangbanged', 'gangbangs', 'gaylord', 'gaysex', 'goatse', 'God', 'god-dam', 'god-damned', 'goddamn', 'goddamned', 'hardcoresex', 'hell', 'heshe', 'hoar', 'hoare', 'hoer', 'homo', 'hore', 'horniest', 'horny', 'hotsex', 'jack-off', 'jackoff', 'jap', 'jerk-off', 'jism', 'jiz', 'jizm', 'jizz', 'kawk', 'knob', 'knobead', 'knobed', 'knobend', 'knobhead', 'knobjocky', 'knobjokey', 'kock', 'kondum', 'kondums', 'kum', 'kummer', 'kumming', 'kums', 'kunilingus', 'l3i+ch', 'l3itch', 'labia', 'lmfao', 'lust', 'lusting', 'm0f0', 'm0fo', 'm45terbate', 'ma5terb8', 'ma5terbate', 'masochist', 'master-bate', 'masterb8', 'masterbat*', 'masterbat3', 'masterbate', 'masterbation', 'masterbations', 'masturbate', 'mo-fo', 'mof0', 'mofo', 'mothafuck', 'mothafucka', 'mothafuckas', 'mothafuckaz', 'mothafucked', 'mothafucker', 'mothafuckers', 'mothafuckin', 'mothafucking', 'mothafuckings', 'mothafucks', 'mother', 'fucker', 'motherfuck', 'motherfucked', 'motherfucker', 'motherfuckers', 'motherfuckin', 'motherfucking', 'motherfuckings', 'motherfuckka', 'motherfucks', 'muff', 'mutha', 'muthafecker', 'muthafuckker', 'muther', 'mutherfucker', 'n1gga', 'n1gger', 'nazi', 'nigg3r', 'nigg4h', 'nigga', 'niggah', 'niggas', 'niggaz', 'nigger', 'niggers', 'nob', 'nob', 'jokey', 'nobhead', 'nobjocky', 'nobjokey', 'numbnuts', 'nutsack', 'orgasim', 'orgasims', 'orgasm', 'orgasms', 'p0rn', 'pawn', 'pecker', 'penis', 'penisfucker', 'phonesex', 'phuck', 'phuk', 'phuked', 'phuking', 'phukked', 'phukking', 'phuks', 'phuq', 'pigfucker', 'pimpis', 'piss', 'pissed', 'pisser', 'pissers', 'pisses', 'pissflaps', 'pissin', 'pissing', 'pissoff', 'poop', 'porn', 'porno', 'pornography', 'pornos', 'prick', 'pricks', 'pron', 'pube', 'pusse', 'pussi', 'pussies', 'pussy', 'pussys', 'rectum', 'retard', 'rimjaw', 'rimming', 's', 'hit', 's.o.b.', 'sadist', 'schlong', 'screwing', 'scroat', 'scrote', 'scrotum', 'semen', 'sex', 'sh!+', 'sh!t', 'sh1t', 'shag', 'shagger', 'shaggin', 'shagging', 'shemale', 'shi+', 'shit', 'shitdick', 'shite', 'shited', 'shitey', 'shitfuck', 'shitfull', 'shithead', 'shiting', 'shitings', 'shits', 'shitted', 'shitter', 'shitters', 'shitting', 'shittings', 'shitty', 'skank', 'slut', 'sluts', 'smegma', 'smut', 'snatch', 'son-of-a-bitch', 'spac', 'spunk', 's_h_i_t', 't1tt1e5', 't1tties', 'teets', 'teez', 'testical', 'testicle', 'tit', 'titfuck', 'tits', 'titt', 'tittie5', 'tittiefucker', 'titties', 'tittyfuck', 'tittywank', 'titwank', 'tosser', 'turd', 'tw4t', 'twat', 'twathead', 'twatty', 'twunt', 'twunter', 'v14gra', 'v1gra', 'vagina', 'viagra', 'vulva', 'w00se', 'wang', 'wank', 'wanker', 'wanky', 'whoar', 'whore', 'willies', 'willy', 'xrated', 'xxx']","bafe8af2":"def toxic_words_ratio(text, toxic_words=toxic_words):\n    count = 0\n    text = word_tokenize(text)\n    for word in text:\n        count += int(word.lower() in toxic_words)\n    return count \/ len(text)","75e72376":"def build_vocab(texts):\n    vocab = {}\n    for sentence in texts:\n        for word in sentence.split(' '):\n            try:\n                vocab[word] += 1\n            except KeyError:\n                vocab[word] = 1\n    return vocab","46f5c704":"word_count = build_vocab(list(train_df['question_text']) + list(test_df['question_text']))","6eec2fed":"def freq_count(text):\n    text = text.split(\" \")\n    all_count = 0\n    for word in text:\n        all_count += word_count[word]\n    return len(text) \/ all_count","2017a85a":"def make_features(df):    \n    df['total_length'] = df['question_text'].apply(len)\n    df['capitals'] = df['question_text'].apply(lambda comment: sum(1 for c in comment if c.isupper()))\n    df['caps_vs_length'] = df.apply(lambda row: float(row['capitals'])\/float(row['total_length']), axis=1)\n    df['num_words'] = df.question_text.str.count('\\S+')\n    df['num_unique_words'] = df['question_text'].apply(lambda comment: len(set(w for w in comment.split())))\n    df['words_vs_unique'] = df['num_unique_words'] \/ df['num_words']\n    df['toxic_ratio'] = df['question_text'].apply(toxic_words_ratio)\n    df['word_freq'] = df['question_text'].apply(freq_count)","f77ad551":"features = ['caps_vs_length', 'words_vs_unique', 'toxic_ratio', 'total_length', 'word_freq']\nnb_features = len(features)\n\nprint(f'Generated {nb_features} features :', ', '.join(features))","2ddb402f":"%%time\nmake_features(train_df)\nmake_features(test_df)","bbe4cb7d":"features_train = train_df[features].fillna(0)\nfeatures_test = test_df[features].fillna(0)\n\nss = StandardScaler()\nss.fit(np.vstack((features_train, features_test)))\nfeatures_train = ss.transform(features_train)\nfeatures_test = ss.transform(features_test)","558c2790":"# All appearing special characters\nuseful_punct = ['_', '\u2639', '\uff1e', '\u00bd', '\u25b3', '\u00bf', '\u00bc', '\u2206', '\u2265', '\u21d2', '\u00ac', '\u2228', '\uff3e', '\u00b5', '?', '!', '.', ',', '\"', '#', '$', '%', '\\\\', \"'\", '(', ')', '*', '+', '-', '\/', ':', ';', '<', '=', '>', '@', '[', ']', '^', '{', '|', '}', '~', '\u2019', '\u2032', '\u2018', '\u00b0', '\u2192', '\u00a3', '\u00f8', '\u00b4', '\u2191', '\u03b8', '\u00b1', '\u2264', '\u2260', '...', '\u27e8', '\u27e9', '\u2216', '\u207f', '\u2154', '\u2764', '\u270c', '\u2705', '\u2713', '\u2218', '\u00a8', '\u2033', '\u03cd', '\u12a4', '\u120d', '\u122e', '\u12a2', '\u12e8', '\u129d', '\u1295', '\u12a0', '\u1201', '\u2245', '\u03d5', '\u2011', '\ufffc', '\u05bf', '\u304b', '\u304f', '\u308c', '\uff0d', '\u0219', '\u05df', '\u222a', '\u03c6', '\u03c8', '\u22a8', '\u03b2', '\u2220', '\u00ab', '\u00bb', '\u0bae', '\u2248', '\u2070', '\u2077', '\u060c', '\uff1d', '\uff08', '\uff09', '\u0259', '\u0251', '\u02d0', '\u00b9', '\u2153', '\u0159', '\u300a', '\u300b', '\u03c1', '\u2205', '&', '\u00b7', '\u00a9', '\u00a5', '\uff1a', '\u22c5', '\u2193', '\u3001', '\u2502', '\uff0c', '\u30fb', '\u2022', '\u00ae', '`', '\u20ac', '\u2122', '\u203a', '\u2665', '\u2190', '\u00d7', '\u00a7', '\u00c2', '\u2588', '\u00e0', '\u2026', '\u201c', '\u2605', '\u201d', '\u2013', '\u25cf', '\u00e2', '\u25ba', '\u2212', '\u00a2', '\u00b2', '\u2591', '\u00b6', '\u25be', '\u2550', '\u00a6', '\u2551', '\u2015', '\u2593', '\u2014', '\u2039', '\u2500', '\u2592', '\u2295', '\u25bc', '\u25aa', '\u2020', '\u25a0', '\u2580', '\u2584', '\u266b', '\u2606', '\u00e9', '\u00af', '\u2666', '\u00a4', '\u25b2', '\u00e8', '\u00b8', '\u00be', '\u00c3', '\u221e', '\u2219', '\u266a', '\u2569', '\u255a', '\u00b3', '\u2566', '\u2563', '\u2554', '\u2557', '\u25ac', '\u00ef', '\u00d8', '\u2021', '\u221a']\nuseless_punct = ['\u091a', '\u4e0d', '\u09dd', '\u5e73', '\u1820', '\u932f', '\u5224', '\u2219', '\u8a00', '\u03c2', '\u0644', '\u17d2', '\u30b8', '\u3042', '\u5f97', '\u6c34', '\u044c', '\u25e6', '\u521b', '\u5eb7', '\u83ef', '\u1e35', '\u263a', '\u652f', '\u5c31', '\u201e', '\u300d', '\uc5b4', '\u8c08', '\u9648', '\u56e2', '\u817b', '\u6743', '\u5e74', '\u4e1a', '\u30de', '\u092f', '\u0627', '\u58f2', '\u7532', '\u62fc', '\u02c2', '\u1f64', '\u8d2f', '\u4e9a', '\u093f', '\u653e', '\u02bb', '\u1791', '\u0296', '\u9ede', '\u0acd', '\u767a', '\u9752', '\u80fd', '\u6728', '\u0434', '\u5fae', '\u85e4', '\u0303', '\u50d5', '\u5992', '\u035c', '\u1793', '\u0927', '\uc774', '\u5e0c', '\u7279', '\u0921', '\u00a2', '\u6ee2', '\u0e2a', '\ub098', '\u5973', '\u0c15', '\u6ca1', '\u4ec0', '\u0437', '\u5929', '\u5357', '\u02bf', '\u0e04', '\u3082', '\u51f0', '\u6b65', '\u7c4d', '\u897f', '\u0e33', '\u2212', '\u043b', '\u06a4', '\u17c3', '\u865f', '\u0635', '\u0938', '\u00ae', '\u028b', '\u6279', '\u179a', '\uce58', '\u8c22', '\u751f', '\u9053', '\u2550', '\u4e0b', '\u4fc4', '\u0256', '\u89c0', '\u0bb5', '\u2014', '\u06cc', '\u60a8', '\u2665', '\u4e00', '\u3084', '\u2286', '\u028c', '\u8a9e', '\u0e35', '\u5174', '\u60f6', '\u701b', '\u72d0', '\u2074', '\u092a', '\u81e3', '\u0c26', '\u2015', '\u00ec', '\u090c', '\u0c40', '\u81ea', '\u4fe1', '\u5065', '\u53d7', '\u0268', '\uc2dc', '\u05d9', '\u099b', '\u5b1b', '\u6e7e', '\u5403', '\u3061', '\u095c', '\u53cd', '\u7ea2', '\u6709', '\u914d', '\u09c7', '\u17af', '\u5bae', '\u3064', '\u03bc', '\u8a18', '\u53e3', '\u2105\u03b9', '\u094b', '\u72f8', '\u5947', '\u043e', '\u091f', '\u8056', '\u862d', '\u8aad', '\u016b', '\u6a19', '\u8981', '\u178f', '\u8bc6', '\u3067', '\u6c64', '\u307e', '\u0280', '\u5c40', '\u30ea', '\u094d', '\u0e44', '\u5462', '\u5de5', '\u0932', '\u6c92', '\u03c4', '\u17b7', '\u00f6', '\u305b', '\u4f60', '\u3093', '\u30e5', '\u679a', '\u90e8', '\u5927', '\u7f57', '\u09b9', '\u3066', '\u8868', '\u62a5', '\u653b', '\u013a', '\u0e09', '\u2229', '\u5b9d', '\u5bf9', '\u5b57', '\u6587', '\u8fd9', '\u2211', '\u9aea', '\u308a', '\u0e48', '\ub2a5', '\u7f62', '\ub0b4', '\u963b', '\u4e3a', '\u83f2', '\u064a', '\u0928', '\u03af', '\u0266', '\u958b', '\u2020', '\u8339', '\u505a', '\u6771', '\u09a4', '\u306b', '\u062a', '\u6653', '\ud0a4', '\u60b2', '\u0ab8', '\u597d', '\u203a', '\u4e0a', '\u5b58', '\uc5c6', '\ud558', '\u77e5', '\u1792', '\u65af', ' ', '\u6388', '\u0142', '\u50b3', '\u5170', '\u5c01', '\u0bcb', '\u0648', '\u0445', '\u3060', '\u4eba', '\u592a', '\u54c1', '\u6bd2', '\u1873', '\u8840', '\u5e2d', '\u5254', '\u043f', '\u86cb', '\u738b', '\u90a3', '\u68a6', '\u17b8', '\u5f69', '\u7504', '\u0438', '\u67cf', '\u0a28', '\u548c', '\u574a', '\u231a', '\u5e7f', '\u4f9d', '\u222b', '\u012f', '\u6545', '\u015b', '\u090a', '\u51e0', '\u65e5', '\u06a9', '\u97f3', '\u00d7', '\u201d', '\u25be', '\u028a', '\u091c', '\u0e14', '\u0920', '\u0909', '\u308b', '\u6e05', '\u0917', '\u0637', '\u03b4', '\u028f', '\u5b98', '\u221b', '\u09bc', '\u0e49', '\u7537', '\u9a82', '\u590d', '\u2202', '\u30fc', '\u8fc7', '\u09af', '\u4ee5', '\u77ed', '\u7ffb', '\u09b0', '\u6559', '\u5100', '\u025b', '\u2039', '\u3078', '\u00be', '\u5408', '\u5b66', '\u064c', '\ud559', '\u6311', '\u0937', '\u6bd4', '\u4f53', '\u0645', '\u0633', '\u17a2', '\u05ea', '\u8a13', '\u2200', '\u8fce', '\u179c', '\u0254', '\u0668', '\u2592', '\u5316', '\u0c1a', '\u201b', '\u09aa', '\u00ba', '\u0e19', '\uc5c5', '\u8bf4', '\u3054', '\u00b8', '\u20b9', '\u513f', '\ufe20', '\uac8c', '\u9aa8', '\u0e17', '\u090b', '\u30db', '\u8336', '\ub294', '\u0a9c', '\u0e38', '\u7fa1', '\u7bc0', '\u0a2e', '\u0989', '\u756a', '\u09dc', '\u8bb2', '\u315c', '\ub4f1', '\u4f1f', '\u0e08', '\u6211', '\u0e25', '\u3059', '\u3044', '\u1789', '\u770b', '\u010b', '\u2227', '\u092d', '\u0a98', '\u0e31', '\u1798', '\u8857', '\u0aaf', '\u8fd8', '\u9c39', '\u1781', '\u0c41', '\u8a0a', '\u092e', '\u044e', '\u5fa9', '\u6768', '\u0642', '\u0924', '\u91d1', '\u5473', '\u09ac', '\u98ce', '\u610f', '\uba87', '\u4f6c', '\u723e', '\u7cbe', '\u00b6', '\u0c02', '\u4e71', '\u03c7', '\uad50', '\u05d4', '\u59cb', '\u1830', '\u4e86', '\u4e2a', '\u514b', '\u09cd', '\u0e2b', '\u5df2', '\u0283', '\u308f', '\u65b0', '\u8bd1', '\ufe21', '\u672c', '\u0e07', '\u0431', '\u3051', '\u0c3f', '\u660e', '\u00af', '\u904e', '\u0643', '\u1fe5', '\u0641', '\u00df', '\uc11c', '\u8fdb', '\u178a', '\u6837', '\u4e50', '\u5be7', '\u20ac', '\u0e13', '\u30eb', '\u4e61', '\u5b50', '\ufb01', '\u062c', '\u6155', '\u2013', '\u1875', '\u00d8', '\u0361', '\uc81c', '\u03a9', '\u1794', '\u7d55', '\ub208', '\u092b', '\u09ae', '\u0c17', '\u4ed6', '\u03b1', '\u03be', '\u00a7', '\u0b9c', '\u9ece', '\u306d', '\ubcf5', '\u03c0', '\u00fa', '\u9e21', '\u8bdd', '\u4f1a', '\u0995', '\u516b', '\u4e4b', '\ubd81', '\u0646', '\u00a6', '\uac00', '\u05d5', '\u604b', '\u5730', '\u1fc6', '\u8a31', '\u4ea7', '\u0961', '\u0634', '\u093c', '\u91ce', '\u1f75', '\u0252', '\u5567', '\u1799', '\u180c', '\u1828', '\u0628', '\u768e', '\u8001', '\u516c', '\u2606', '\u0935', '\u09bf', '\u179b', '\u0631', '\u1782', '\ud589', '\u1784', '\u03bf', '\u8ba9', '\u17c6', '\u03bb', '\u062e', '\u1f30', '\u5bb6', '\u099f', '\u092c', '\u7406', '\u662f', '\u3081', '\u0930', '\u221a', '\uae30', '\u03bd', '\u7389', '\ud55c', '\u5165', '\u05d3', '\u522b', '\u062f', '\u0e30', '\u7535', '\u0abe', '\u266b', '\u0639', '\u0a82', '\u5835', '\u5ac9', '\u4f0a', '\u3046', '\u5343', '\uad00', '\u7bc7', '\u0915', '\u975e', '\u8363', '\u7cb5', '\u745c', '\u82f1', '\ub97c', '\u7f8e', '\u6761', '`', '\u5b8b', '\u2190', '\uc218', '\u5f8c', '\u2022', '\u00b3', '\u0940', '\uace0', '\u8089', '\u2103', '\u3057', '\u6f22', '\uc2f1', '\u03f5', '\u9001', '\u0647', '\u843d', '\u0c28', '\u1780', '\u0b95', '\u2107', '\u305f', '\u17c7', '\u4e2d', '\u5c04', '\u266a', '\u7b26', '\u1783', '\u8c37', '\u5206', '\u9171', '\u3073', '\u09a5', '\u0629', '\u0433', '\u03c3', '\u3068', '\u695a', '\u80e1', '\u996d', '\u307f', '\u79ae', '\u4e3b', '\u76f4', '\u00f7', '\u5922', '\u027e', '\u099a', '\u20d7', '\u7d71', '\u9ad8', '\u987a', '\u636e', '\u3089', '\u982d', '\u3088', '\u6700', '\u0c3e', '\u0a41', '\u4eb2', '\u179f', '\u82b1', '\u2261', '\u773c', '\u75c5', '\u2026', '\u306e', '\u767c', '\u0bbe', '\u6c5d', '\u2605', '\u6c0f', '\u0e23', '\u666f', '\u1860', '\u8bfb', '\u4ef6', '\u4ef2', '\u09b6', '\u304a', '\u3063', '\u067e', '\u1864', '\u0447', '\u266d', '\u60a0', '\u0902', '\u516d', '\u4e5f', '\u057c', '\u09df', '\u6050', '\u0939', '\u53ef', '\u554a', '\u83ab', '\u4e66', '\u603b', '\u09b7', '\u0584', '\u0302', '\uac04', '\u306a', '\u6b64', '\u611b', '\u0c30', '\u0e43', '\u9673', '\u1f08', '\u0923', '\u671b', '\u0926', '\u8bf7', '\u6cb9', '\u9732', '\ub2c8', '\u015f', '\u5b97', '\u028d', '\u9cf3', '\u0905', '\u908b', '\u7684', '\u1796', '\u706b', '\u093e', '\u0e01', '\u7d04', '\u0b9f', '\u7ae0', '\u9577', '\u5546', '\u53f0', '\u52e2', '\u3055', '\uad6d', '\u00ce', '\u7c21', '\u0908', '\u2208', '\u1e6d', '\u7d93', '\u65cf', '\u0941', '\u5b6b', '\u8eab', '\u5751', '\u09b8', '\u4e48', '\u03b5', '\u5931', '\u6bba', '\u017e', '\u0ab0', '\u304c', '\u624b', '\u17b6', '\u5fc3', '\u0a3e', '\ub85c', '\u671d', '\u4eec', '\u9ed2', '\u6b22', '\u65e9', '\ufe0f', '\u09be', '\u0906', '\u0278', '\u5e38', '\u5feb', '\u6c11', '\ufdfa', '\u17bc', '\u9062', '\u03b7', '\u56fd', '\u65e0', '\u6c5f', '\u0960', '\u300c', '\u09a8', '\u2122', '\u17be', '\u03b6', '\u7d2b', '\u0c46', '\u044f', '\u201c', '\u2668', '\u570b', '\u0947', '\u0e2d', '\u221e']\n\n# Mapping special letters\nletter_mapping = {'\\u200b':' ', '\u0169': \"u\", '\u1ebd': 'e', '\u00e9': \"e\", '\u00e1': \"a\", '\u0137': 'k', '\u00ef': 'i', '\u0179': 'Z', '\u017b': 'Z', '\u0160': 'S', '\u03a0': ' pi ', '\u00d6': 'O', '\u00c9': 'E', '\u00d1': 'N', '\u017d': 'Z', '\u1ec7': 'e', '\u00b2': '2', '\u00c5': 'A', '\u0100': 'A', '\u1ebf': 'e', '\u1ec5': 'e', '\u1ed9': 'o', '\u29fc': '<', '\u29fd': '>', '\u00dc': 'U', '\u0394': 'delta', '\u1ee3': 'o', '\u0130': 'I', '\u042f': 'R', '\u041e': 'O', '\u010c': 'C', '\u041f': 'pi', '\u0412': 'B', '\u03a6': 'phi', '\u1ef5': 'y', '\u0585': 'o', '\u013d': 'L', '\u1ea3': 'a', '\u0393': 'theta', '\u00d3': 'O', '\u00cd': 'I', '\u1ea5': 'a', '\u1ee5': 'u', '\u014c': 'O', '\u039f': 'O', '\u03a3': 'sigma', '\u00c2': 'A', '\u00c3': 'A', '\u15ef': 'w', '\u157c': \"h\", \"\u15e9\": \"a\", \"\u1587\": \"r\", \"\u15ef\": \"w\", \"O\": \"o\", \"\u15f0\": \"m\", \"\u144e\": \"n\", \"\u142f\": \"v\", \"\u043d\": \"h\", \"\u043c\": \"m\", \"o\": \"o\", \"\u0442\": \"t\", \"\u0432\": \"b\", \"\u03c5\": \"u\",  \"\u03b9\": \"i\",\"\u043d\": \"h\", \"\u010d\": \"c\", \"\u0161\": \"s\", \"\u1e25\": \"h\", \"\u0101\": \"a\", \"\u012b\": \"i\", \"\u00e0\": \"a\", \"\u00fd\": \"y\", \"\u00f2\": \"o\", \"\u00e8\": \"e\", \"\u00f9\": \"u\", \"\u00e2\": \"a\", \"\u011f\": \"g\", \"\u00f3\": \"o\", \"\u00ea\": \"e\", \"\u1ea1\": \"a\", \"\u00fc\": \"u\", \"\u00e4\": \"a\", \"\u00ed\": \"i\", \"\u014d\": \"o\", \"\u00f1\": \"n\", \"\u00e7\": \"c\", \"\u00e3\": \"a\", \"\u0107\": \"c\", \"\u00f4\": \"o\", \"\u0441\": \"c\", \"\u011b\": \"e\", \"\u00e6\": \"ae\", \"\u00ee\": \"i\", \"\u0151\": \"o\", \"\u00e5\": \"a\", \"\u00c4\": \"A\", }","23fbbac7":"mispell_dict = {\"trimp\": \"trump\", \"wanket\": \"wanker\",'colour': 'color', 'centre': 'center', 'favourite': 'favorite', 'travelling': 'traveling', 'counselling': 'counseling', 'theatre': 'theater', 'cancelled': 'canceled', 'labour': 'labor', 'organisation': 'organization', 'wwii': 'world war 2', 'citicise': 'criticize', 'youtu ': 'youtube ', 'qoura': 'quora', 'sallary': 'salary', 'whta': 'what', 'narcisist': 'narcissist', 'howdo': 'how do', 'whatare': 'what are', 'howcan': 'how can', 'howmuch': 'how much', 'howmany': 'how many', 'whydo': 'why do', 'doI': 'do I', 'theBest': 'the best', 'howdoes': 'how does', 'mastrubation': 'masturbation', 'mastrubate': 'masturbate', \"mastrubating\": 'masturbating', 'pennis': 'penis', 'etherium': 'ethereum', 'narcissit': 'narcissist', 'bigdata': 'big data', '2k17': '2017', '2k18': '2018', 'qouta': 'quota', 'exboyfriend': 'ex boyfriend', 'airhostess': 'air hostess', \"whst\": 'what', 'watsapp': 'whatsapp', 'demonitisation': 'demonetization', 'demonitization': 'demonetization',}","7bebdc20":"def add_maj(dic):\n    d = dic.copy()\n    for w in dic.keys():\n        try:\n            d[w[0].upper() + w[1:]] = dic[w][0].upper() + dic[w][1:]\n        except:\n            d[w[0].upper()] = dic[w][0].upper()\n    return d","473cde4e":"substitution_dic = {}\nsubstitution_dic.update(mispell_dict)\nsubstitution_dic = add_maj(substitution_dic)\nsubstitution_dic.update(letter_mapping)","934f4e03":"def _get_substitution(sub_dic):\n    sub_re = re.compile('(%s)' % '|'.join(sub_dic.keys()))\n    return sub_dic, sub_re\n\nsubstitutions, substitutions_re = _get_substitution(substitution_dic)\n\ndef replace_substitution(text):\n    def replace(match):\n        return substitutions[match.group(0)]\n    return substitutions_re.sub(replace, text)","b67fe02d":"def clean_apostrophes(x):\n    apostrophes = [\"\u2019\", \"\u2018\", \"\u00b4\", \"`\"]\n    for s in apostrophes:\n        x = re.sub(s, \"'\", x)\n    return x","00c8e8db":"def remove_s(x): \n    if len(x) > 2:\n        return re.sub(\"('$ |'$|'s |'s)\", ' ', x)\n    else:\n        return x","02a2ce99":"spaces = ['\\u200b', '\\u200e', '\\u202a', '\\u202c', '\\ufeff', '\\uf0d8', '\\u2061', '\\x10', '\\x7f', '\\x9d', '\\xad', '\\xa0']\ndef clean_spaces(text):\n    for space in spaces:\n        text = text.replace(space, ' ')\n    text = text.strip()\n    text = re.sub('\\s+', ' ', text)\n    return text","633fc61a":"def decontract(text):\n    text = re.sub(r\"(W|w)on(\\'|\\\u2019)t \", \"will not \", text)\n    text = re.sub(r\"(C|c)an(\\'|\\\u2019)t \", \"can not \", text)\n    text = re.sub(r\"(Y|y)(\\'|\\\u2019)all \", \"you all \", text)\n    text = re.sub(r\"(Y|y)a(\\'|\\\u2019)ll \", \"you all \", text)\n    text = re.sub(r\"(I|i)(\\'|\\\u2019)m \", \"i am \", text)\n    text = re.sub(r\"(A|a)isn(\\'|\\\u2019)t \", \"is not \", text)\n    text = re.sub(r\"n(\\'|\\\u2019)t \", \" not \", text)\n    text = re.sub(r\"(\\'|\\\u2019)re \", \" are \", text)\n    text = re.sub(r\"(\\'|\\\u2019)s \", \" is \", text)\n    text = re.sub(r\"(\\'|\\\u2019)d \", \" would \", text)\n    text = re.sub(r\"(\\'|\\\u2019)ll \", \" will \", text)\n    text = re.sub(r\"(\\'|\\\u2019)t \", \" not \", text)\n    text = re.sub(r\"(\\'|\\\u2019)ve \", \" have \", text)\n    return text","b5149ea1":"def clean_numbers(text):\n    text = re.sub(r'(\\d+)([a-zA-Z])', '\\g<1> \\g<2>', text)\n    text = re.sub(r'(\\d+) (th|st|nd|rd) ', '\\g<1>\\g<2> ', text)\n    text = re.sub(r'(\\d+),(\\d+)', '\\g<1>\\g<2>', text)\n    return text","ba7019a2":"def clean_special_chars(text, punct=useful_punct):\n    for p in punct:\n        text = text.replace(p, f' {p} ')\n    return text","2fe63993":"def clean_latex_tag(text):\n    corr_t = []\n    for t in text.split(\" \"):\n        t = t.strip()\n        if t != '':\n            corr_t.append(t)\n    text = ' '.join(corr_t)\n    text = re.sub('(\\[ math \\]).+(\\[ \/ math \\])', 'mathematical formula', text)\n    return text","b90a9b5b":"def build_vocab(texts):\n    vocab = {}\n    for sentence in texts:\n        for word in sentence.split(' '):\n            try:\n                vocab[word] += 1\n            except KeyError:\n                vocab[word] = 1\n    return vocab","ba5accc7":"from multiprocessing import Pool, cpu_count\nprint(\"Number of available cpu cores: {}\".format(cpu_count()))\n\ndef process_in_parallel(function, list_):\n    with Pool(cpu_count()) as p:\n        tmp = p.map(function, list_)\n    return tmp","e6707f86":"def treat_texts(texts):\n    texts = process_in_parallel(clean_apostrophes, texts)\n    texts = process_in_parallel(replace_substitution, texts)\n    texts = process_in_parallel(decontract, texts)\n    texts = process_in_parallel(remove_s, texts)\n    texts = process_in_parallel(clean_numbers, texts)\n    texts = process_in_parallel(clean_special_chars, texts)\n    texts = process_in_parallel(clean_spaces, texts)\n    texts = process_in_parallel(clean_latex_tag, texts)   \n    return texts","b8704a7a":"%%time\ntrain_df[\"question_text\"] = treat_texts(train_df[\"question_text\"])\ntest_df[\"question_text\"] = treat_texts(test_df[\"question_text\"])","5def222d":"for q in test_df[\"question_text\"][:5]:\n    print(q)","8e8595a6":"vocab = build_vocab(list(train_df[\"question_text\"]) + list(test_df[\"question_text\"]))","7c655cee":"len_voc = None\nmax_len = 70","bf8e73c2":"def make_input_data(X_train, X_test):\n    t = Tokenizer(num_words=len_voc, filters='', lower=False)\n    t.fit_on_texts((np.concatenate((X_train, X_test), axis=0)))\n    X_train = t.texts_to_sequences(X_train)\n    X_test = t.texts_to_sequences(X_test)\n    X_train = pad_sequences(X_train, maxlen=max_len, padding='post', truncating='post')\n    X_test = pad_sequences(X_test, maxlen=max_len, padding='post', truncating='post')\n\n    return X_train, X_test, t.word_index","ee91af5b":"%%time\nX_train, X_test, word_index = make_input_data(train_df['question_text'], test_df['question_text'])","a3f5502b":"len_voc = len(word_index) + 1","ddf6031e":"y_train = train_df['target'].values","ad5c88ba":"sns.countplot(y_train)\nplt.title('Target Repartition', size=15)\nplt.show()","37129405":"def make_embed_index(file, word_index, vocab=[], exceptions={}):\n    def get_coefs(word,*arr): \n        return word, np.asarray(arr, dtype='float32')\n    \n    if file == FASTTEXT:\n        embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(file) if len(o)>100 and o.split(\" \")[0] in word_index )\n    if file == PARAGRAM:\n        embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(file, encoding=\"utf8\", errors='ignore') if len(o)>100 and o.split(\" \")[0] in word_index)\n    else: #GloVe\n        embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(file) if o.split(\" \")[0] in word_index)\n    \n    for word in vocab:\n        try:\n            _ = embeddings_index[word]\n        except:\n            try:\n                embeddings_index[word] = embeddings_index[word.lower()]\n            except:\n                try:\n                    embeddings_index[word] = embeddings_index[word.upper()]\n                except:\n                    try:\n                        embeddings_index[word] = embeddings_index[word[0].upper() + word[1:].lower()]\n                    except:\n                        pass\n            \n    return embeddings_index","7589f24d":"def make_embed_mat(embeddings_index):\n    all_embs = np.stack(embeddings_index.values())\n    emb_mean,emb_std = all_embs.mean(), all_embs.std()\n    embed_size = all_embs.shape[1]\n\n    embedding_matrix = np.random.normal(emb_mean, emb_std, (len(word_index)+1, embed_size))\n    for word, i in word_index.items():\n        embedding_vector = embeddings_index.get(word)\n        if embedding_vector is not None: \n            embedding_matrix[i] = embedding_vector\n\n    return embedding_matrix","d557fe22":"%%time\nglove_index = make_embed_index(GLOVE, word_index, vocab)","19d14f0b":"%%time\nfasttext_index = make_embed_index(FASTTEXT, word_index, vocab)","c18eb06e":"%%time\npara_index = make_embed_index(PARAGRAM, word_index, vocab)","6261eb2e":"def check_coverage(vocab, embeddings_index, word_index=None):\n    known_words = {}\n    unknown_words = {}\n    nb_known_words = 0\n    nb_unknown_words = 0\n    for word in vocab.keys():\n        try:\n            known_words[word] = embeddings_index[word]\n            nb_known_words += vocab[word]\n        except:\n            unknown_words[word] = vocab[word]\n            nb_unknown_words += vocab[word]\n            pass\n\n    print('Found embeddings for {:.2%} of vocab'.format(len(known_words) \/ len(vocab)))\n    print('Found embeddings for  {:.2%} of all text'.format(nb_known_words \/ (nb_known_words + nb_unknown_words)))\n    unknown_words = sorted(unknown_words.items(), key=operator.itemgetter(1))[::-1]\n\n    return unknown_words","e33f3736":"print(\"Glove : \")\noov_glove = check_coverage(vocab, glove_index, word_index)\nprint(\"Paragram : \")\noov_paragram = check_coverage(vocab, para_index, word_index)\nprint(\"FastText : \")\noov_fasttext = check_coverage(vocab, fasttext_index, word_index)","25489a22":"embed_concat = np.concatenate((make_embed_mat(fasttext_index), make_embed_mat(glove_index), make_embed_mat(para_index)), axis=1)","31fd16b2":"t_init = time.time()\nprint(f\"Initialized in {(t_init - begin) \/\/ 60} minutes\")","7ca2d8a4":"class Noise(nn.Module):\n    def __init__(self, mean=0.0, stddev=0.1):\n        super(Noise, self).__init__()\n        self.mean = mean\n        self.stddev = stddev\n\n    def forward(self, input):\n        noise = input.clone().normal_(self.mean, self.stddev)\n        return input + noise","11e2e765":"class Attention(nn.Module):\n    def __init__(self, feature_dim, step_dim, bias=True, **kwargs):\n        super(Attention, self).__init__(**kwargs)\n        self.supports_masking = True\n        self.bias = bias\n        self.feature_dim = feature_dim\n        self.step_dim = step_dim\n        self.features_dim = 0\n        weight = torch.zeros(feature_dim, 1)\n        nn.init.xavier_uniform_(weight)\n        self.weight = nn.Parameter(weight)\n        if bias:\n            self.b = nn.Parameter(torch.zeros(step_dim))\n        \n    def forward(self, x, mask=None):\n        feature_dim = self.feature_dim\n        step_dim = self.step_dim\n        eij = torch.mm(x.contiguous().view(-1, feature_dim), self.weight).view(-1, step_dim)\n        if self.bias:\n            eij = eij + self.b  \n        eij = torch.tanh(eij)\n        a = torch.exp(eij)\n        if mask is not None: \n            a = a * mask\n        a = a \/ torch.sum(a, 1, keepdim=True) + 1e-10\n        weighted_input = x * torch.unsqueeze(a, -1)\n        return torch.sum(weighted_input, 1)","33f340f1":"class Model(nn.Module):\n    def __init__(self, embedding_matrix):\n        super(Model, self).__init__()\n        \n        h1 = 64\n        h2 = 32\n        hd = 32\n        \n        self.embedding = nn.Embedding(embedding_matrix.shape[0], embedding_matrix.shape[1])\n        self.embedding.weight = nn.Parameter(torch.tensor(embedding_matrix, dtype=torch.float32))\n        self.embedding.weight.requires_grad = False\n        self.noise = Noise(stddev=0.05)\n        \n        self.lstm = nn.LSTM(embedding_matrix.shape[1], 2*h1, bidirectional=False, batch_first=True)\n        self.gru = nn.GRU(2*h1, 2*h2, bidirectional=False, batch_first=True)\n        \n        self.lstm_att = Attention(2*h1, max_len)\n        self.gru_att = Attention(2*h2, max_len)\n        \n        self.dropout = nn.Dropout(0.1)\n        self.linear = nn.Linear(6*(h1+h2) + nb_features, hd)\n        self.relu = nn.ReLU()\n        self.bn = nn.BatchNorm1d(hd, momentum=0.5)\n        self.out = nn.Linear(hd, 1)\n    \n    def forward(self, x):\n        embed = self.embedding(x[0])\n        \n        if self.training:\n            embed = torch.squeeze(self.noise(torch.unsqueeze(embed, 0)))\n        \n        lstm, _ = self.lstm(embed)\n        gru, _ = self.gru(lstm)\n        \n        att1 = self.lstm_att(lstm)\n        att2 = self.gru_att(gru)\n        \n        avg_pool1 = torch.mean(lstm, 1)\n        avg_pool2 = torch.mean(gru, 1)\n        max_pool1, _ = torch.max(lstm, 1)\n        max_pool2, _ = torch.max(gru, 1)\n        \n        conc = torch.cat((att1, avg_pool1, max_pool1, att2, avg_pool2, max_pool2, x[1]), 1)\n        conc = self.dropout(conc)\n        conc = self.bn(self.relu(self.linear(conc)))\n        conc = self.dropout(conc)\n        out = self.out(conc)\n        \n        return out","1215db3f":"def sigmoid(x):\n    return 1 \/ (1 + np.exp(-x))","4d8e1940":"def get_lr(epoch):\n    if epoch <= 3:\n        return 0.001\n    else:\n        return 0.0005","d38ee37e":"def plot_history(history, title='Learning Curves'):\n    plt.plot(history['loss'], label='Train loss')\n    try : plt.plot(history['val_loss'], label='Test loss')\n    except: pass\n    plt.title(title, size=15)\n    plt.legend()","e9d3cbdd":"def tweak_threshold(train_preds, y_train):\n    tmp = [0,0,0] # idx, cur, max\n    delta = 0\n    for tmp[0] in np.arange(0.1, 0.501, 0.01):\n        tmp[1] = f1_score(y_train, np.array(train_preds)>tmp[0])\n        if tmp[1] > tmp[2]:\n            delta = tmp[0]\n            tmp[2] = tmp[1]\n    return tmp[2], delta","ecd7d095":"def predict(X, f,model, batch_size=1024):\n    y = np.array([])\n    X = torch.tensor(X, dtype=torch.long).cuda()\n    f = torch.tensor(f, dtype=torch.float32).cuda()\n    Xf = torch.utils.data.TensorDataset(X, f)\n    loader = torch.utils.data.DataLoader(Xf, batch_size=batch_size, shuffle=False)\n    for i, (x, f) in enumerate(loader):\n        y = np.concatenate((y, sigmoid(model([x, f]).detach().cpu().numpy())[:, 0]))\n    return y","79afea0c":"def fit(model, X_train, f_train, y_train, X_val=None, f_val=None, y_val=None, epochs=5, batch_size=512):\n    history = {\"loss\":[], \"val_loss\": []}\n    best_loss = 10\n    model.cuda()\n    \n    loss_fn = torch.nn.BCEWithLogitsLoss(reduction='sum')\n    optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=0.001)\n        \n    X_train = torch.tensor(X_train, dtype=torch.long).cuda()\n    f_train = torch.tensor(f_train, dtype=torch.float32).cuda()\n    y_train = torch.tensor(y_train[:, np.newaxis], dtype=torch.float32).cuda()\n    train = torch.utils.data.TensorDataset(X_train, f_train, y_train)    \n    train_loader = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=True)\n    \n    X_val = torch.tensor(X_val, dtype=torch.long).cuda()\n    f_val = torch.tensor(f_val, dtype=torch.float32).cuda()\n    y_val = torch.tensor(y_val[:, np.newaxis], dtype=torch.float32).cuda()\n    val = torch.utils.data.TensorDataset(X_val, f_val, y_val)\n    val_loader = torch.utils.data.DataLoader(val, batch_size=batch_size, shuffle=False)\n\n    for epoch in range(epochs):\n        model.train()\n        avg_loss = 0\n        start_time = time.time()  \n            \n        lr = get_lr(epoch)\n        for param_group in optimizer.param_groups:\n            param_group['lr'] = lr\n        \n        for x_batch, f_batch, y_batch in train_loader:\n            optimizer.zero_grad()\n            y_pred = model([x_batch, f_batch])\n            loss = loss_fn(y_pred, y_batch)\n            loss.backward()\n            optimizer.step()\n            avg_loss += loss.item() \/ X_train.shape[0]\n    \n        model.eval()\n        avg_val_loss = 0.\n        for i, (x_batch, f_batch, y_batch) in enumerate(val_loader):\n            y_pred = model([x_batch, f_batch]).detach()\n            avg_val_loss += loss_fn(y_pred, y_batch).item() \/ X_val.shape[0]\n        \n        history['loss'].append(avg_loss)\n        history['val_loss'].append(avg_val_loss)\n        elapsed_time = time.time() - start_time \n        print('Epoch {}\/{} \\t lr={} \\t loss={:.4f} \\t val_loss={:.4f} \\t time={:.2f}s'.format(epoch + 1, epochs, lr, avg_loss, avg_val_loss, elapsed_time))\n        \n    return history","732f381a":"def k_fold(model_class, embedding_matrix, X, f, y, X_test, f_test, k=5, batch_size=512, epochs=5, seed=seed):\n    splits = list(StratifiedKFold(n_splits=k, shuffle=True, random_state=420).split(X, y))\n    pred_test = np.zeros(X_test.shape[0])\n    pred_oof = np.zeros(y.shape[0])\n    histories = []\n    \n    for i, (train_idx, val_idx) in enumerate(splits):\n        print(f\"-------------   Fold {i+1}  ------------- \\n\")\n        seed_everything(seed + i)\n        \n        cp_path = f\"{i}_weights.pth.tar\"\n        start_time = time.time()\n        \n        model = model_class(embedding_matrix)\n\n        history = fit(model, X[train_idx], f[train_idx], y[train_idx], X[val_idx], f[val_idx], y[val_idx], epochs=epochs, batch_size=batch_size)\n        histories.append(history)\n\n        pred_oof[val_idx] = predict(X[val_idx], f[val_idx], model)\n        pred_test += predict(X_test, f_test, model) \/ k\n        \n        score, threshold = tweak_threshold(pred_oof[val_idx], y[val_idx])\n        \n        print(f\"\\n Scored {score :.4f} for threshold {threshold :.3f} on validation data\")\n        print(f\"\\n    Done in {(time.time() - start_time) \/ 60 :.1f} minutes \\n\")\n        \n    return pred_test, pred_oof, histories","11e1fa38":"pred_test, pred_oof, histories = k_fold(Model, embed_concat, \n                                        X_train, features_train, y_train, X_test, features_test,\n                                        k=5, batch_size=512, epochs=4)","1bba40f2":"score, threshold = tweak_threshold(pred_oof, y_train)\nprint(f\"Local CV : {score:.4f} for threshold {threshold:.3f}\")","184aaf23":"plt.figure(figsize=(15, 10))\nfor i in range(len(histories)):\n    plt.subplot(3, 2, i+1)\n    plot_history(histories[i], \"Fold \" + str(i+1))\nplt.show()","f3079bf1":"label_test = (pred_test > threshold).astype(int)","9f5536cf":"output = pd.DataFrame({\"qid\": test_df[\"qid\"].values})\noutput['prediction'] = label_test\noutput.to_csv(\"submission.csv\", index=False)","27a916a8":"output.head()","317e4d79":"print(f\"Ended in {(time.time() - begin) \/ 60 :.1f} minutes\")","8efd7024":"### Toxic word ratio","70229448":"#### 's","1edfdc43":"### Learning rate","62486132":"# 26th Place Kernel\nLast version was cleaned for public sharing. The previous one is the one that was submitted.\n\n*Code is a bit dirty, it was my first experience with PyTorch, and with Kaggle competitons in general. \nA lot of improvements can be made.*","03f30a4d":"# Training","7bd2278c":"## Augmenting\n- No augmentation was applied, but I tried augmenting with Markov Chains","956be316":"### Seeding\n- For reproductibility","ae6c84e8":"### Fitting","ddda8f83":"### Applying stuff","b3979740":"## Preprocessing\n- Treating apostrophes\n- Substituting with dic (contractions, misspels, some punctuation)\n- Removing 's and lone '\n- Cleaning numbers\n- Cleaning special characters\n- Removing extra spaces\n- Clean latex tags","2b089c35":"## Text Input\n- Tokenize & pad","473add5b":"### Target","ac795845":"# Submission","3313a3aa":"#### Latex Tag","27b8d614":"### Tweak Threshold","3429606b":"#### Special characters","53d1b04f":"# Embedding matrices\n- Concatenation of Glove, FastText & Paragram","b631adb1":"# Initialization","5f7da8ed":"# Text Data","05c517a9":"# Modeling","e62913d0":"### Predict","1e5b63bc":"## Tools","4d76da2f":"### Learning curves","fd4d3ab5":"#### Apostrophes","1d32d4ea":"### Sigmoid","96eb1ea2":"## Model","0cdba6d4":"### $k$-fold","06894ca0":"## Loading","483b356b":"## Training","68ee95d0":"### Making all features","be980678":"### Filling NaNs and scaling","7d66f387":"### Frequency Ratio","5296138e":"### Noise","3bdac307":"#### All substitutions","3d82ccfd":"#### Substitutions","49f6eee7":"### Data","cbba0028":"### Imports","fcdad403":"### Embeddings paths","1b918a27":"### Coverage","38aed6f8":"## Features\n> Features are taken from one of last year's Jigsaw competiton top scoring kernels\n\n- **Toxic word ratio :** The proportion of words in the sentence that are in a list of words labelled as toxic\n- **Total length :** Length of the sentence as a string\n- **Capital letters ratio :** PROPORTION OF LETTERS WRITTEN IN CAPS.\n- **Unique word ratio :** Proportion of words in the sentence that appear only once\n- **Average word frequency :** Frequency of the words in the sentence in the overall corpus\n","aa8bafed":"### Attention Layer","d0df31bc":"#### Spaces","9690454d":"#### Contractions","a8c1e466":"#### Numbers"}}