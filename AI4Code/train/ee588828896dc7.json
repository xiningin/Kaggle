{"cell_type":{"77bfd833":"code","fadea1ed":"code","d5947d92":"code","39b163ea":"code","d3e69c7d":"code","1a2db71c":"code","92e9d45c":"code","3b6857d7":"code","9d04e190":"code","efb1ecfe":"code","5b57a54c":"code","8eb785e9":"code","7e53d555":"code","02db2abf":"code","3cfabd64":"code","77c7dc11":"code","03439e27":"code","40eeda74":"code","6c4b1919":"code","19a8098b":"code","4fc4c2e4":"code","c8e3fd70":"code","7b795608":"code","4461585f":"code","6cf54af5":"markdown","b520f744":"markdown","6cc9c44d":"markdown","14e92387":"markdown","c6a129e0":"markdown","7878e5ec":"markdown","f460fd16":"markdown"},"source":{"77bfd833":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","fadea1ed":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport tensorflow as tf","d5947d92":"#read train\ntrain = pd.read_csv('..\/input\/sign-language-mnist\/sign_mnist_train\/sign_mnist_train.csv')\nprint(train.shape)","39b163ea":"train.head()","d3e69c7d":"#read test\ntest = pd.read_csv('..\/input\/sign-language-mnist\/sign_mnist_test\/sign_mnist_test.csv')\nprint(test.shape)","1a2db71c":"test.head()","92e9d45c":"x_train = train.drop(labels = [\"label\"], axis = 1)\ny_train = train[\"label\"]\nprint(\"x_train shape\", x_train.shape)\nprint(\"y_train shape\", y_train.shape)","3b6857d7":"x_test = test.drop(labels = [\"label\"], axis = 1)\ny_test = test[\"label\"]\nprint(\"x_test shape \", x_test.shape)\nprint(\"y_test shape\", y_test.shape)","9d04e190":"plt.figure(figsize = (15,7))\nsns.countplot(y_train, palette= \"icefire\")\nplt.title(\"Number of digit Label Pixels\")\ny_train.value_counts()","efb1ecfe":"#plot some samples\nimg = x_train.iloc[41].values\nimg = img.reshape((28,28))\nplt.imshow(img, cmap = \"gray\")\nplt.title(train.iloc[0,0])\nplt.axis(\"off\")\nplt.show()","5b57a54c":"#normalization\nx_train = x_train.astype(\"float32\") \/ 255.0\nx_test = x_test.astype(\"float32\") \/ 255.0\nprint(\"x_train shape : \", x_train.shape)\nprint(\"x_test shape : \", x_test.shape)","8eb785e9":"#Reshape\nx_train = x_train.values.reshape(-1, 28, 28, 1)\nx_test = x_test.values.reshape(-1, 28, 28, 1)\nprint(\"x_train shape : \", x_train.shape)\nprint(\"x_test shape : \", x_test.shape)","7e53d555":"#label encoding\nfrom keras.utils.np_utils import to_categorical\ny_train = to_categorical(y_train, num_classes = 25)\ny_test =  to_categorical(y_test, num_classes = 25)","02db2abf":"#Split the train and the validation set for the fitting\nfrom sklearn.model_selection import train_test_split\nx_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size = 0.1, random_state = 2)\nprint(\"x_train shape\", x_train.shape)\nprint(\"x_val shape\", x_val.shape)\nprint(\"y_train shape\", y_train.shape)\nprint(\"y_val shape\", y_val.shape)","3cfabd64":"from sklearn.metrics import confusion_matrix\nimport itertools\n\nfrom keras.utils.np_utils import to_categorical # convert to one-hot-encoding\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D\nfrom tensorflow.keras.optimizers import RMSprop,Adam\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.layers import BatchNormalization\nfrom keras.callbacks import ReduceLROnPlateau\n\nmodel = Sequential()\n\nmodel.add(Conv2D(32, (3,3), strides = 1, padding = \"Same\",activation =\"relu\", input_shape =(28,28,1)))\nmodel.add(MaxPool2D((2,2),strides = 2,padding =\"Same\"))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.2))\n\n\n\nmodel.add(Conv2D(64, (3,3), strides = 1, padding = \"Same\",activation =\"relu\"))\nmodel.add(MaxPool2D((2,2),strides = 2,padding =\"Same\"))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.2))\n\nmodel.add(Conv2D(64, (3,3), strides = 1, padding = \"Same\",activation =\"relu\"))\nmodel.add(MaxPool2D((2,2),strides = 2,padding =\"Same\"))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.2))\n\n\n\n# fully connected\n\nmodel.add(Flatten())  \nmodel.add(Dense(128, activation = \"relu\"))\nmodel.add(Dropout(0.3))\nmodel.add(Dense(25,activation = \"softmax\"))\nmodel.summary()","77c7dc11":"#Define Optimizer\noptimizer = Adam(lr = 0.001, beta_1 = 0.9, beta_2 = 0.999)","03439e27":"learning_rate_reduction = ReduceLROnPlateau(monitor='val_accuracy', patience = 2, verbose=1,factor=0.5, min_lr=0.00001)","40eeda74":"#compile Model\nmodel.compile(optimizer = optimizer, loss = \"categorical_crossentropy\", metrics = [\"accuracy\"])","6c4b1919":"#Epochs and Batch Size\nepochs = 10\nbatch_size = 120","19a8098b":"\ndatagen = ImageDataGenerator(\n        featurewise_center=False,  # set input mean to 0 over the dataset\n        samplewise_center=False,  # set each sample mean to 0\n        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n        samplewise_std_normalization=False,  # divide each input by its std\n        zca_whitening=False,  # dimesion reduction\n        rotation_range=10,  # randomly rotate images in the range 5 degrees\n        zoom_range = 0.1, # Randomly zoom image 10%\n        width_shift_range=0.1,  # randomly shift images horizontally 10%\n        height_shift_range=0.1,  # randomly shift images vertically 10%\n        horizontal_flip=False,  # randomly flip images\n        vertical_flip=False)  # randomly flip images\n\ndatagen.fit(x_train)\n","4fc4c2e4":"## Fit the model\nhistory = model.fit(datagen.flow(x_train,y_train,\n                                 batch_size = batch_size),\n                                 epochs = epochs, \n                                 validation_data =(x_val,y_val),\n                                 steps_per_epoch = x_train.shape[0]\/\/batch_size,\n                                 callbacks = [learning_rate_reduction])","c8e3fd70":"score = model.evaluate(x_test,y_test,verbose = 1)\nprint(\"Test Loss : \",score[0])\nprint(\"Test Accuracy : \",score[1])","7b795608":"keys=history.history.keys()\nprint(keys)","4461585f":"def show_train_history(hisData,t1,t2): \n    plt.plot(hisData.history[t1])\n    plt.plot(hisData.history[t2])\n    plt.title('History')\n    plt.ylabel('value')\n    plt.xlabel('epoch')\n    plt.legend([t1, t2], loc='upper left')\n    plt.show()\n\nshow_train_history(history, 'loss', 'val_loss')\nshow_train_history(history, 'accuracy', 'val_accuracy')","6cf54af5":"# Data Visualization and Preprocessing","b520f744":"# Creat Model","6cc9c44d":"# Loading the Data Set","14e92387":"# Train Test Split","c6a129e0":"# Normalization, Reshape and Label Encoding","7878e5ec":"# <span style=\"color:cyan;\"> Sign Language MNIST<\/span>\n\n![0_B0VYVuEc9TkE-0Pv.png](attachment:2b682645-9259-4c9b-8ca5-75b690bf19ad.png)\n\n\n**The dataset format is patterned to match closely with the classic MNIST. Each training and test case represents a label (0-25) as a one-to-one map for each alphabetic letter A-Z (and no cases for 9=J or 25=Z because of gesture motions). The training data (27,455 cases) and test data (7172 cases) are approximately half the size of the standard MNIST but otherwise similar with a header row of label, pixel1,pixel2\u2026.pixel784 which represent a single 28x28 pixel image with grayscale values between 0-255. The original hand gesture image data represented multiple users repeating the gesture against different backgrounds.**","f460fd16":"# Data Augmentation"}}