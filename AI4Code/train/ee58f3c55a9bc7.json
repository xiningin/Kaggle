{"cell_type":{"f8f2d322":"code","31c7e33a":"code","dc05ee1f":"code","533206e5":"code","b98be2e9":"code","b332d05e":"code","0c0b4120":"code","5d59b8fb":"code","f692f730":"code","4f0272af":"code","c3f6390e":"code","f94bf87e":"code","f3cf1650":"code","1729ce99":"code","fa57d8fa":"code","58a8a25a":"code","db0058f7":"code","1d44fc9c":"code","be5f7eb2":"code","e88e88e9":"code","888666ce":"code","c7b2c720":"code","dfd25be1":"code","3c055e68":"code","c5d62e13":"code","e9c02fcb":"code","cad9f1d2":"code","4b94f6d2":"code","37187b4a":"code","a0d5b95d":"code","592e4093":"code","6c6d596a":"code","5f91a5e4":"code","0d059611":"code","14949c04":"code","5f65d31e":"code","b217cb79":"code","ea8bf002":"code","96b10f45":"markdown","e3352537":"markdown","c2617575":"markdown","ee17014a":"markdown","50a2be32":"markdown","1e36da25":"markdown","e808ed47":"markdown","ea228b6d":"markdown","0eff78cc":"markdown","b14e9b6f":"markdown","6b3fb916":"markdown","a918b69b":"markdown","16f3265e":"markdown","bba7a0a8":"markdown","44c8ea72":"markdown","f38a3942":"markdown","f708ab9e":"markdown","8f8a2583":"markdown","7bda0a3a":"markdown","95ecbc83":"markdown","a08c5648":"markdown","a46aa3b8":"markdown"},"source":{"f8f2d322":"!pip install git+https:\/\/github.com\/darecophoenixx\/wordroid.sblo.jp","31c7e33a":"mkdir img","dc05ee1f":"%matplotlib inline\nfrom IPython.display import SVG, Image\nfrom tensorflow.keras.utils import model_to_dot","533206e5":"from feature_eng import m01f","b98be2e9":"import os.path\nimport sys\nimport re\nimport itertools\nimport csv\nimport datetime\nimport pickle\nimport random\nfrom collections import defaultdict, Counter\nimport gc\n\nimport matplotlib.pyplot as plt\nfrom matplotlib.ticker import NullFormatter\nimport seaborn as sns\nimport pandas as pd\nimport numpy as np\nimport scipy\nimport gensim\n\nfrom sklearn.metrics import f1_score, classification_report, confusion_matrix, log_loss\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import mixture\nfrom sklearn.decomposition import PCA, FactorAnalysis\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom sklearn.preprocessing import StandardScaler\n\nimport gensim\nfrom tensorflow.keras.preprocessing.sequence import skipgrams\nimport tensorflow as tf","b332d05e":"def hexbin(x, y, color, **kwargs):\n    cmap = sns.light_palette(color, as_cmap=True)\n    plt.hexbin(x, y, cmap=cmap, **kwargs)\ndef scatter(x, y, color, **kwargs):\n    plt.scatter(x, y, marker='.')","0c0b4120":"tgt_dir = ''\nfilename = 'ClusteringOfQuantitativeSurveyData'\n\nimg_cnt = 0\ndef save_img():\n    global img_cnt\n    img_cnt += 1\n    img_name = 'img_{0:03}.jpeg'.format(img_cnt)\n    img_name = os.path.join(tgt_dir, 'img', filename+'_'+img_name)\n    print(img_name)\n    plt.savefig(img_name)","5d59b8fb":"X_df = pd.read_csv('..\/input\/sample-data-wordanddoc2vec\/sample007.csv', index_col=0)\nprint(X_df.shape)\nX_df.head()","f692f730":"plt.figure(figsize=(15, 15))\nplt.imshow(X_df.values.T)\nsave_img()","4f0272af":"f, ax = plt.subplots(1, 1, figsize=(4, 15))\nax = sns.heatmap(X_df.values, vmin=-1, vmax=1, cmap='coolwarm',\n                 #annot=True, fmt='.2f', annot_kws={'size': 8},\n                 ax=ax)\nsave_img()","c3f6390e":"_ = m01f.find_ncomponents_pca(X_df.values)","f94bf87e":"n_sig = 4","f3cf1650":"cor_nonoise = m01f.calc_cor_nonoise(X_df.corr(), n_sig=n_sig)\ncor_nonoise.shape","1729ce99":"f, ax = plt.subplots(1, 1, figsize=(17, 15))\nax = sns.heatmap(cor_nonoise, vmin=-1, vmax=1, cmap='coolwarm',\n                 annot=True, fmt='.2f', annot_kws={'size': 8},\n                 ax=ax)\nsave_img()","fa57d8fa":"f, ax = plt.subplots(1, 1, figsize=(17, 15))\nax = sns.heatmap(X_df.corr(), vmin=-1, vmax=1, cmap='coolwarm',\n                 annot=True, fmt='.2f', annot_kws={'size': 8},\n                 ax=ax)\nsave_img()","58a8a25a":"pca = PCA(n_components=n_sig)\npca.fit(X_df)","db0058f7":"df = pd.DataFrame(pca.components_.T)\nsns.set_context('paper')\ng = sns.PairGrid(df, height=2.5)\ng.map_diag(plt.hist, edgecolor=\"w\")\ng.map_lower(scatter)\ng.map_upper(hexbin)\nsave_img()","1d44fc9c":"loadings_nonoise = m01f.calc_loadings_nonoise(cor_nonoise, n_sig=n_sig)\nprint(loadings_nonoise.shape)\ndf = pd.DataFrame(loadings_nonoise)\nsns.set_context('paper')\ng = sns.PairGrid(df, height=2.5)\ng.map_diag(plt.hist, edgecolor=\"w\")\ng.map_lower(scatter)\ng.map_upper(hexbin)\nsave_img()","be5f7eb2":"%%time\ndf = pd.DataFrame(loadings_nonoise)\nres = m01f.mclust(df)\n\nm01f.plot_mclust(res, figsize=(7,7))\nsave_img()","e88e88e9":"'''specify the number of groups (n_components=)'''\ngm = mixture.GaussianMixture(n_components=5, init_params='kmeans', n_init=30)\ngm.fit(loadings_nonoise)\nnp.argmax(gm.predict_proba(loadings_nonoise), axis=1)","888666ce":"'''Probability of which group each variable belongs to'''\ndf_z = pd.DataFrame(gm.predict_proba(loadings_nonoise), index=X_df.columns)\ndf_z.style.background_gradient(cmap='coolwarm', axis=None, vmin=-1, vmax=1).set_precision(1)","c7b2c720":"df = pd.DataFrame(loadings_nonoise)\ndf['cls'] = ['cls'+str(ee) for ee in np.argmax(gm.predict_proba(loadings_nonoise), axis=1)]\nsns.set_context('paper')\nsns.pairplot(df, markers='o', hue='cls', height=2.5, diag_kind='hist')\nsave_img()","dfd25be1":"fa = FactorAnalysis(n_components=5)\nfa.fit(X_df)\nfa.components_.shape","3c055e68":"df = pd.DataFrame(fa.components_.T, index=X_df.columns)\ndf.style.background_gradient(cmap='coolwarm', axis=None, vmin=-1, vmax=1).set_precision(2)","c5d62e13":"mat_nonoise = m01f.calc_mat_nonoise(X_df.values, n_sig=n_sig)\nX_df_nonoise = pd.DataFrame(mat_nonoise, index=X_df.index, columns=X_df.columns)\nX_df_nonoise.shape","e9c02fcb":"f, ax = plt.subplots(1, 1, figsize=(17, 15))\nax = sns.heatmap(X_df_nonoise.corr(), vmin=-1, vmax=1, cmap='coolwarm',\n                 annot=True, fmt='.2f', annot_kws={'size': 8},\n                 ax=ax)\nsave_img()","cad9f1d2":"scores_nonoise = cosine_similarity(X_df_nonoise.values, loadings_nonoise.T)\nscores_nonoise.shape","4b94f6d2":"df = pd.DataFrame(scores_nonoise)\nsns.set_context('paper')\ng = sns.PairGrid(df, height=2.5)\ng.map_diag(plt.hist, edgecolor=\"w\")\ng.map_lower(scatter)\ng.map_upper(hexbin)\nsave_img()","37187b4a":"df1 = pd.DataFrame(scores_nonoise[:,:5])\ndf1['cls'] = 'row'\ndf2 = pd.DataFrame(loadings_nonoise[:,:5])\ndf2['cls'] = 'col'\ndf = pd.concat([df1, df2])\nsns.set_context('paper')\nsns.pairplot(df, markers=['.']+['s'], hue='cls', height=2.5, diag_kind='hist')\nsave_img()","a0d5b95d":"df = pd.DataFrame(pca.transform(X_df))\nsns.set_context('paper')\ng = sns.PairGrid(df, height=2.5)\ng.map_diag(plt.hist, edgecolor=\"w\")\ng.map_lower(scatter)\ng.map_upper(hexbin)\nsave_img()","592e4093":"%%time\nres = m01f.mclust(scores_nonoise)\nm01f.plot_mclust(res, figsize=(7,7))\nsave_img()","6c6d596a":"gm = mixture.GaussianMixture(n_components=5, init_params='kmeans', n_init=30)\ngm.fit(scores_nonoise)\nnp.argmax(gm.predict_proba(scores_nonoise), axis=1)","5f91a5e4":"df_org2 = X_df.copy()\ndf_org2['cls'] = np.argmax(gm.predict_proba(scores_nonoise), axis=1)\n#df_org2","0d059611":"cnt = df_org2.groupby('cls').size()\ncnt","14949c04":"ll = []\nfor ee in X_df.columns.values:\n    tmp = pd.crosstab(df_org2['cls'], X_df[ee], margins=True, normalize='index')\n    ll.append(tmp.values[:,1])\n\ncross_tab = pd.DataFrame(np.c_[ll], index=X_df.columns, columns=tmp.index)\ncross_tab.style.background_gradient(cmap='coolwarm', axis=None, vmin=-1, vmax=1).set_precision(1)","5f65d31e":"df = pd.DataFrame(scores_nonoise)\ndf['cls'] = ['cls'+str(ee) for ee in df_org2.cls.values]\nsns.set_context('paper')\nsns.pairplot(df, markers='o', hue='cls', height=2.5, diag_kind='hist')\nsave_img()","b217cb79":"idx = np.argsort([int(re.sub('^r', '', ee)) for ee in X_df.index.values])\nplt.figure(figsize=(15, 15))\nplt.imshow(X_df.iloc[idx].values.T)\nsave_img()","ea8bf002":"f, ax = plt.subplots(1, 1, figsize=(4, 15))\nax = sns.heatmap(X_df.iloc[idx].values, vmin=-1, vmax=1, cmap='coolwarm',\n                 #annot=True, fmt='.2f', annot_kws={'size': 8},\n                 ax=ax)\nsave_img()","96b10f45":"# Find number of significant components\n---\n* compares the eigen values of the observed data with that of a random data matrix of the same size as the original.","e3352537":"# Investigation of column-side aggregation\n---","c2617575":"## Calc scores","ee17014a":"# Clustering\n---\n\n## Calc X_df without noise\n```python\ndef calc_mat_nonoise(mat, n_sig=3):\n    ss = StandardScaler()\n    ss.fit(mat)\n    x_sc = ss.transform(mat)\n    u, s, vh = np.linalg.svd(x_sc)\n    x_sc2 = u[:,:n_sig].dot(np.diag(s[:n_sig] * s.sum() \/ s[:n_sig].sum())).dot(vh[:n_sig])\n    return ss.inverse_transform(x_sc2)\n```","50a2be32":"## Correlation of X_df_nonoise","1e36da25":"## Plot loadings without noise (by group)","e808ed47":"# Calc correlation without noise\n---\n```python\ndef calc_cor_nonoise(c, n_sig=3):\n    try:\n        w, v = np.linalg.eig(c)\n    except Exception as e:\n        print(e)\n        c1 = cor_smooth(c)\n        w, v = np.linalg.eig(c1)\n        print('\"Matrix was not positive definite, smoothing was done\"')\n    m = v[:,:n_sig].dot(np.diag(w[:n_sig])).dot(v[:,:n_sig].T)\n    d = np.sqrt(np.diag(m)).reshape((v.shape[0],1))\n    m = m \/ d \/ d.T\n    return m\n```","ea228b6d":"## Plot correlation of original data","0eff78cc":"# Load Sample Data\n---","b14e9b6f":"## Factor Analysis","6b3fb916":"## Show correlation","a918b69b":"## Plot PCA scores of original data","16f3265e":"## Specify the number of groups\n* it seems to be [5]","bba7a0a8":"## Investigation of number of clusters","44c8ea72":"## Loadings of correlation without noise\n```python\ndef calc_loadings_nonoise(c):\n    w, v = np.linalg.eigh(c)\n    idx = np.argsort(w)[::-1]\n    w, v = w[idx], v[:,idx]\n    return v\n```","f38a3942":"# PCA loadings\n---","f708ab9e":"## original data","8f8a2583":"# The number of significant components is [4]\n---","7bda0a3a":"## Probability of which group each variable belongs to","95ecbc83":"# X_df is the following data shuffled in the row direction\n---","a08c5648":"## We will challenge the clustering of survey data mainly represented by 0\/1 data.\n---\n* Find number of significant components\n* Calc correlation without noise\n* PCA loadings\n  * Investigation of column-side aggregation\n* PCA scores\n  * Investigation of number of clusters","a46aa3b8":"# The number of cluster : [5]\n---"}}