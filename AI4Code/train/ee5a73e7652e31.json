{"cell_type":{"39906c6d":"code","3f118a68":"code","9ad4111b":"code","cc690a45":"code","570807b4":"code","62d9b4d4":"code","a0d7b0f4":"code","2675bba3":"code","0909fa12":"code","b00e0299":"code","e7819e50":"code","1f5ee356":"markdown","ef479941":"markdown","1081cfda":"markdown","2a0d39f7":"markdown","511033af":"markdown","887d4ed7":"markdown"},"source":{"39906c6d":"# writing a function to load the json file \n\nimport re\nimport json\nimport pandas as pd\nfrom tqdm import tqdm\n\ndata_file = '..\/input\/arxiv\/arxiv-metadata-oai-snapshot.json'\n\n\"\"\" Using `yield` to load the JSON file in a loop to prevent Python memory issues if JSON is loaded directly\"\"\"\n\ndef get_metadata():\n    with open(data_file, 'r') as f:\n        for line in f:\n            yield line\n\n            \n            \n# we will consider below 3 categories for training \npaper_categories = [\"cs.AI\", # Artificial Intelligence\n                    \"cs.CV\", # Computer Vision and Pattern Recognition\n                    \"cs.LG\"] # Machine Learning\n\n\n\ndef build_dataset(categories=paper_categories):\n    titles = []\n    abstracts = []\n    metadata = get_metadata()\n    for paper in tqdm(metadata):\n        paper_dict = json.loads(paper)\n        category = paper_dict.get('categories')\n        if category in categories:\n            try:\n                year = int(paper_dict.get('journal-ref')[-4:])\n                titles.append(paper_dict.get('title'))\n                abstracts.append(paper_dict.get('abstract').replace(\"\\n\",\"\"))\n            except:\n                pass \n\n    papers = pd.DataFrame({'title': titles,'abstract': abstracts})\n    papers = papers.dropna()\n    papers[\"title\"] = papers[\"title\"].apply(lambda x: re.sub('\\s+',' ', x))\n    papers[\"abstract\"] = papers[\"abstract\"].apply(lambda x: re.sub('\\s+',' ', x))\n\n    del titles, abstracts\n    return papers","3f118a68":"papers = build_dataset()","9ad4111b":"# install simpleT5\n!pip install simplet5","cc690a45":"# simpleT5 expects training and validation dataframes to have 2 columns: \"source_text\" and \"target_text\"\npapers = papers[['abstract','title']]\npapers.columns = [\"source_text\", \"target_text\"]\n\n# let's add a prefix to source_text, to uniquely identify kind of task we are performing on the data, in this case --> \"summarize\"\npapers['source_text'] = \"summarize: \"+ papers['source_text']","570807b4":"# split the data into training and test\nfrom sklearn.model_selection import train_test_split\n\ntrain_df, test_df = train_test_split(papers, test_size=0.1)","62d9b4d4":"# import\nfrom simplet5 import SimpleT5\n\n# instatntiate\nmodel = SimpleT5()\n\n# load\nmodel.from_pretrained(\"t5\",\"t5-base\")\n\n# train\nmodel.train(train_df=train_df, eval_df=test_df, source_max_token_len=512, target_max_token_len=128, max_epochs=5, batch_size=8, use_gpu=True)","a0d7b0f4":"!ls outputs\/","2675bba3":"# load a trained model\nmodel.load_model(\"outputs\/SimpleT5-epoch-4-train-loss-1.1588\", use_gpu=True)\n\n# generate\nmodel.predict(\"summarize: some text you want to test it on\")","0909fa12":"# let's see how it performerd:\nsample_abstracts = test_df.sample(10)\n\nfor i, abstract in sample_abstracts.iterrows():\n    print(f\"===== Abstract =====\")\n    print(abstract['source_text'])\n    summary= model.predict(abstract['source_text'])[0]\n    print(f\"\\n===== Actual Title =====\")\n    print(f\"{abstract['target_text']}\")\n    print(f\"\\n===== Generated Title =====\")\n    print(f\"{summary}\")\n    print(\"\\n +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\\n\")","b00e0299":"res=pd.DataFrame(columns=['index','text','title','query'])\nindex=0\nfor i, abstract in test_df.iterrows():\n    res.loc[i,'index'] = index\n    res.loc[i,'text']  = abstract['source_text']\n    res.loc[i,'title']  = abstract['target_text']\n    res.loc[i,'query'] = model.predict(abstract['source_text'])[0]\n    index+=1\nres.head()","e7819e50":"res.to_csv(\"arxiv_with_query.csv\",index=False)","1f5ee356":"## Inferencing\n**simpleT5** saves your model at every epoch in \"outputs\" folder (default)","ef479941":"### Voila \ud83c\udf89 ! you're done","1081cfda":"# simpleT5\u26a1\ufe0f: Generating one line summary of ArXiv research papers\nIn this notebook, we will see how to generate one line summary of ArXiv research papers with **simpleT5\u26a1\ufe0f**\n\n**simpleT5\u26a1\ufe0f** is built on top of PyTorch-lightning\u26a1\ufe0f and Transformers\ud83e\udd17 that lets you quickly train your T5 models (in just 3 lines of code). It can be used for several NLP tasks such as summarization, QA, QG, translation, text generation, and more.\n\nLet's get started...","2a0d39f7":"## Dataset Prepration","511033af":"**Load the dataset in Pandas DataFrame**","887d4ed7":"## Training with simpleT5\u26a1\ufe0f"}}