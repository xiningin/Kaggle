{"cell_type":{"1b37e1da":"code","87801d0f":"code","d0b2908d":"code","9cd1379f":"code","43261ccc":"code","35684151":"code","cb727cb7":"code","4f97c036":"code","b23f7268":"code","4fae217e":"code","e46d38f7":"code","9fb8a56a":"code","e9929caf":"code","dbfe2710":"code","530e0761":"code","59b886d8":"code","5f4bc36e":"code","3fa0cb18":"code","1b5b2e47":"code","f8d58608":"code","6e338e0f":"code","90edda79":"code","3aa3600a":"code","85198efd":"code","02252592":"code","62ac006a":"code","a83a5bb3":"code","521c4dc4":"code","a78fe627":"code","0f9f134e":"code","b257c135":"code","ec7ec6ad":"code","97737e2a":"code","4539f726":"code","ecf83f4a":"code","250488c3":"code","bc9af6ee":"code","e2b8720f":"code","1c36fdda":"code","b906f2d7":"code","a5ce6203":"code","84bf8188":"code","81310a5b":"code","9d827126":"code","16707fde":"code","8d196f3b":"code","8b4870d4":"code","65d515ac":"code","81b93d48":"code","a9a6dbd3":"code","0e56e474":"code","0c28bd00":"code","2da56e57":"code","1ad9946a":"code","9d5c0bff":"code","8b1b1404":"code","b82b938b":"code","c9f18743":"code","590c0f25":"code","ae1764c3":"code","1036f68d":"code","24a3242b":"markdown","ab07fa58":"markdown","bf77cf13":"markdown","ad05d02a":"markdown","5357716a":"markdown","e6e4b3c1":"markdown","95412b48":"markdown","eae5648d":"markdown","78a2a4e4":"markdown","51b6ad7f":"markdown","288ef544":"markdown","012652d1":"markdown","c1559625":"markdown","dfe5b1be":"markdown","1934eafa":"markdown","bb292d09":"markdown","53bec7da":"markdown","0f15b06f":"markdown","ecaf1f43":"markdown","618ae11f":"markdown","549b362b":"markdown","6e744428":"markdown","dc794407":"markdown","86d17ea2":"markdown","82a19fb3":"markdown","519c4d56":"markdown","90c7c313":"markdown","1107aeae":"markdown","c9e8fb99":"markdown","b873ea3b":"markdown","e7f9e4bd":"markdown","2a92363d":"markdown","144d932e":"markdown","75112c4a":"markdown","c965979c":"markdown","bab77b7f":"markdown","7a9af590":"markdown","59453f3d":"markdown","5539211e":"markdown","c36766c4":"markdown"},"source":{"1b37e1da":"\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\nprint(os.listdir(\"..\/input\"))\nplt.style.use('ggplot') # to plot graphs with gggplot2 style\n# Any results you write to the current directory are saved as output.","87801d0f":"#Reading the dataset on pandas\nstrains = pd.read_csv(\"..\/input\/cannabis.csv\")","d0b2908d":"strains.shape","9cd1379f":"strains.info()","43261ccc":"# check the null value\nstrains.isnull().sum()","35684151":"strains.head()","cb727cb7":"strains['Type']= strains['Type'].astype(str)\n","4f97c036":"print(strains.nunique())","b23f7268":"print(\"Numerical describe of distribuition Type\")\nprint(strains.groupby(\"Type\")[\"Strain\"].count())\nprint(\"Percentage of distribuition Type \")\nprint((strains.groupby(\"Type\")[\"Strain\"].count() \/ len(strains.Type) * 100).round(decimals=2))","4fae217e":"plt.figure(figsize=(10,6))\nsns.countplot(x=\"Type\", data=strains, palette='hls')\nplt.xlabel('Species', fontsize=15)\nplt.ylabel('Count', fontsize=20)\nplt.title(\"Cannabis Species Count \", fontsize=20)\nplt.show()","e46d38f7":"print(\"Top 10 Rating by consumers\")\nprint(strains[\"Rating\"].value_counts().head(10))\n\nplt.figure(figsize=(8,6))\n\n#Total rating distribuition\ng = sns.distplot(strains[\"Rating\"], bins=50)\ng.set_title(\"Rating distribuition\", size = 20)\ng.set_xlabel('Rating', fontsize=15)","9fb8a56a":"print(\"Rating Distribuition by Species Type\")\nprint(pd.crosstab(strains[strains.Rating > 4.0]['Rating'], strains.Type))\n\nplt.figure(figsize=(10,14))\n\n#Let's look the Rating distribuition by Type.\ng = plt.subplot(311)\ng = sns.distplot(strains[(strains.Type == 'hybrid') & \n                               (strains.Rating > 0)][\"Rating\"], color='y')\ng.set_xlabel(\"Rating\", fontsize=15)\ng.set_ylabel(\"Distribuition\", fontsize=15)\ng.set_title(\"Rating Distribuition Hybrids\", fontsize=20)\n\ng1 = plt.subplot(312)\ng1 = sns.distplot(strains[(strains.Type == 'sativa') & \n                               (strains.Rating > 0)][\"Rating\"], color='g')\ng1.set_xlabel(\"Rating\", fontsize=15)\ng1.set_ylabel(\"Distribuition\", fontsize=15)\ng1.set_title(\"Rating Distribuition Sativas\", fontsize=20)\n\ng2 = plt.subplot(313)\ng2 = sns.distplot(strains[(strains.Type == 'indica') & \n                               (strains.Rating > 0)][\"Rating\"], color='r')\ng2.set_xlabel(\"Rating\", fontsize=15)\ng2.set_ylabel(\"Distribuition\", fontsize=15)\ng2.set_title(\"Rating Distribuition Indicas\", fontsize=20)\n\nplt.subplots_adjust(wspace = 0.1, hspace = 0.6,top = 0.9)\n\nplt.show()","e9929caf":"plt.figure(figsize=(10,6))\n#I will now explore the Rating distribuition by Type\ng = sns.boxplot(x=\"Type\",y=\"Rating\",data=strains[strains[\"Rating\"] > 2],palette=\"hls\")\ng.set_title(\"Rating distribuition by Species Type\", fontsize=20)\ng.set_xlabel(\"Species\", fontsize=15)\ng.set_ylabel(\"Rating\", fontsize=15)\nplt.show()","dbfe2710":"#Looking the Rating distribuition description \nprint(\"Rating less than 4: \")\nprint(strains[strains.Rating <= 4].groupby(\"Type\")[\"Strain\"].count())\nprint(\"Rating between 4 and 4.5: \")\nprint(strains[(strains.Rating > 4) & (strains.Rating <= 4.5)].groupby(\"Type\")[\"Strain\"].count())\nprint(\"Top Strains - Rating > 4.5: \")\nprint(strains[strains[\"Rating\"] > 4.5].groupby(\"Type\")[\"Strain\"].count())\nprint(\"Distribuition by type of Ratings equal 5: \")\nprint(strains[strains[\"Rating\"] == 5].groupby(\"Type\")[\"Strain\"].count())\nprint(\"Total of: 2350 different Strains\")","530e0761":"#I will extract the values in Effects and Flavor and pass to a new column\ndf_effect = pd.DataFrame(strains.Effects.str.split(',',4).tolist(),\n             columns = ['Effect_1','Effect_2','Effect_3','Effect_4','Effect_5'])\n\ndf_flavors = pd.DataFrame(strains.Flavor.str.split(',',n=2,expand=True).values.tolist(),\n                          columns = ['Flavor_1','Flavor_2','Flavor_3'])","59b886d8":"#Concatenating the new variables with strains\nstrains = pd.concat([strains, df_effect], axis=1)\nstrains = pd.concat([strains, df_flavors], axis=1)\n\n#Looking the result\nstrains.head()\n\n","5f4bc36e":"strains.columns","3fa0cb18":"print(\"The top 5 First Effects related\")\nprint(strains['Effect_1'].value_counts()[:5])\n\nplt.figure(figsize=(13,6))\n\ng = sns.boxplot(x = 'Effect_1', y=\"Rating\",\n                hue=\"Type\",\n                data=strains[strains[\"Rating\"] > 3],\n                palette=\"hls\")\ng.set_xlabel(\"Related Effect\", fontsize=15)\ng.set_ylabel(\"Rating Distribuition\", fontsize=15)\ng.set_title(\"First Effect Related x Rating by Species Type\",fontsize=20)\n\nplt.show()\n","1b5b2e47":"print(\"The top 5 Second related Effects\")\nprint(strains['Effect_2'].value_counts()[:5])\n\nplt.figure(figsize=(13,6))\n\ng = sns.boxplot(x = 'Effect_2', y=\"Rating\",\n                hue=\"Type\",\n                data=strains[strains[\"Rating\"] > 3],\n                palette=\"hls\")\ng.set_xlabel(\"Related Effect\", fontsize=15)\ng.set_ylabel(\"Rating Distribuition\", fontsize=15)\ng.set_title(\"Second Effect Related x Rating by Species Type\",fontsize=20)\n\nplt.show()","f8d58608":"strains.head()","6e338e0f":"strains.shape","90edda79":"print(\"TOP 10 Flavors related\")\nprint(strains.Flavor_1.value_counts()[:10])\n\nplt.figure(figsize=(14,6))\nsns.countplot('Flavor_1', data=strains)\nplt.xticks(rotation=90)\nplt.xlabel('Flavors', fontsize=15)\nplt.ylabel('Frequency', fontsize=15)\nplt.title(\"First flavors described \", fontsize=20)\nplt.show()","3aa3600a":"#Whats the type with most strains with rating 5?\nprint(\"Percentual of Species with Rating equal 5\")\nfive_rating = (strains[strains[\"Rating\"] == 5].groupby(\"Type\")[\"Strain\"].count() \\\n               \/ len(strains[strains[\"Rating\"] == 5]) *100).round(decimals=2)\nprint(five_rating)\nplt.figure(figsize=(10,6))\ng = sns.countplot(x=\"Type\",data=strains[strains[\"Rating\"] == 5])\ng.set_xlabel('Species', fontsize=15)\ng.set_ylabel('Frequency', fontsize=15)\ng.set_title(\"Distribuition of Types by Rating 5.0  \", fontsize=20)\n\nplt.show()","85198efd":"strains_top = strains[strains[\"Rating\"] == 5]\n\nfig, ax = plt.subplots(2,1, figsize=(12,10))\n\nsns.countplot(x ='Effect_1',data = strains_top,hue=\"Type\",ax=ax[0], palette='hls')\n\nsns.countplot(x ='Flavor_1',data = strains_top,hue=\"Type\",ax=ax[1], palette='hls')\n\nfor ax in fig.axes:\n    plt.sca(ax)\n    plt.xticks(rotation=45)","02252592":"#Let's create subsets by each type and explore their Flavors and Effects\nhibridas = strains[strains.Type == 'hybrid']\nindicas = strains[strains.Type == 'indica']\nsativas = strains[strains.Type == 'sativa']","62ac006a":"#Now we can delete some columns that will not be useful\ndel strains[\"Effects\"]\ndel strains[\"Flavor\"]","a83a5bb3":"#Creating the spliter -- copied by LiamLarsen -- \ndef get_effects(dataframe):\n    ret_dict = {}\n    for list_ef in dataframe.Effects:\n        effects_list = list_ef.split(',')\n        for effect in effects_list:\n            if not effect in ret_dict:\n                ret_dict[effect] = 1\n            else:\n                ret_dict[effect] += 1\n    return ret_dict","521c4dc4":"#Creating the counting of effects\nsativa_effects = get_effects(sativas)\n\n#Let see the distribuition of effects by types\nplt.figure(figsize=(10,8))\nsns.barplot(list(sativa_effects.values()), list(sativa_effects.keys()), orient='h')\nplt.xlabel(\"Count\", fontsize=12)\nplt.ylabel(\"Related effects\", fontsize=12)\nplt.title(\"Sativas strain effects distribution\", fontsize=16)\nplt.show()","a78fe627":"# Couting effects of indicas \nindica_effects = get_effects(indicas)\n\n# Ploting Indica Effects\nplt.figure(figsize=(10,8))\nsns.barplot(list(indica_effects.values()),list(indica_effects.keys()), orient='h')\nplt.xlabel(\"Count\", fontsize=15)\nplt.ylabel(\"Related effects\", fontsize=15)\nplt.title(\"Indica strain effects distribution\", fontsize=20)\nplt.show()","0f9f134e":"hibridas_effects = get_effects(hibridas)\n\n# Ploting Hybrid effects\nplt.figure(figsize=(10,8))\nsns.barplot(list(hibridas_effects.values()),list(hibridas_effects.keys()), orient='h')\nplt.xlabel(\"Count\", fontsize=15)\nplt.ylabel(\"Related effects\", fontsize=15)\nplt.title(\"Hibrids strain effects distribution\", fontsize=20)\nplt.show()","b257c135":"#Creating flavors to cut each flavor by row -- inspired in LiamLarsen --\ndef flavors(df):\n    ret_dict = {}\n    for list_ef in df.Flavor.dropna():\n        flavors_list = list_ef.split(',')\n        for flavor in flavors_list:\n            if not flavor in ret_dict:\n                ret_dict[flavor] = 1\n            else:\n                ret_dict[flavor] += 1\n    return ret_dict","ec7ec6ad":"#Runing flavors counts to sativas\nsativa_flavors = flavors(sativas)\n\nplt.figure(figsize=(10,12))\nsns.barplot(list(sativa_flavors.values()),list(sativa_flavors.keys()), orient='h')\nplt.xlabel(\"Count\", fontsize=12)\nplt.ylabel(\"Most related flavors\", fontsize=12)\nplt.title(\"Sativa flavors distribution\", fontsize=16)\nplt.show()","97737e2a":"indica_flavors = flavors(indicas)\n\nplt.figure(figsize=(10,12))\nsns.barplot(list(indica_flavors.values()),list(indica_flavors.keys()), orient='h')\nplt.xlabel(\"Count\", fontsize=12)\nplt.ylabel(\"Most related flavors\",fontsize=12)\nplt.title(\"Indica flavors distribution\", fontsize=16)\nplt.show()","4539f726":"#Getting hibridas flavors\nhibridas_flavors = flavors(hibridas)\n\nplt.figure(figsize=(10,12))\nsns.barplot(list(hibridas_flavors.values()),list(hibridas_flavors.keys()), alpha=0.8,orient='h')\nplt.xlabel(\"Count\", fontsize=15)\nplt.ylabel(\"Most related flavors\", fontsize=15)\nplt.title(\"Hibrids flavors distribution\", fontsize=20)\nplt.show()","ecf83f4a":"from wordcloud import WordCloud, STOPWORDS\nimport nltk.tokenize as word_tokenize\nimport re\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nimport string\nimport re\nfrom nltk.stem.porter import *\nfrom nltk.tokenize import sent_tokenize\nfrom sklearn.feature_extraction import stop_words","250488c3":"stopwords = set(STOPWORDS)\nnewStopWords = ['strain','effect', 'genetic', 'effects','flavor',\n                'dominant','known','cross']\nstopwords.update(newStopWords)\n\nwordcloud = WordCloud(\n                          background_color='black',\n                          stopwords=stopwords,\n                          max_words=1500,\n                          max_font_size=200, \n                          width=1000, height=600,\n                          random_state=42,\n                         ).generate(\" \".join(strains['Description'].astype(str)))\n\nfig = plt.figure(figsize = (12,12))\nplt.imshow(wordcloud)\nplt.title(\"WORD CLOUD - DESCRIPTION\", fontsize=25)\nplt.axis('off')\nplt.show()\n\n","bc9af6ee":"stopwords = set(STOPWORDS)\nnewStopWords = ['strain','effect', 'genetic', 'sativa', 'effects',\n                'aroma','flavor','dominant','known','cross','genetics']\nstopwords.update(newStopWords)\n\nwordcloud = WordCloud(\n                          background_color='white',\n                          stopwords=stopwords,\n                          max_words=1500,\n                          max_font_size=200, \n                          width=1000, height=600,\n                          random_state=42,\n                         ).generate(\" \".join(strains[strains.Type == 'sativa']['Description'].astype(str)))\n\nfig = plt.figure(figsize = (12,12))\nplt.imshow(wordcloud)\nplt.title(\"WORD CLOUD - SATIVAS\", fontsize=25)\nplt.axis('off')\nplt.show()","e2b8720f":"stopwords = set(STOPWORDS)\nnewStopWords = ['strain','effect', 'genetic', 'indica', 'effects','aroma', \n                'genetics','flavor','dominant','known','cross']\nstopwords.update(newStopWords)\n\nwordcloud = WordCloud(\n                          background_color='black',\n                          stopwords=stopwords,\n                          max_words=1500,\n                          max_font_size=150, \n                          width=1000, height=600,\n                          random_state=42,\n                         ).generate(\" \".join(strains[strains.Type == 'indica']['Description'].astype(str)))\n\nfig = plt.figure(figsize = (12,12))\nplt.imshow(wordcloud)\nplt.title(\"WORD CLOUD - INDICAS\", fontsize=25)\nplt.axis('off')\nplt.show()","1c36fdda":"stopwords = set(STOPWORDS)\nnewStopWords = ['strain','effect', 'genetic', 'hybrid', 'effects', 'aroma',\n                'genetics', 'flavor', 'genetics','cross','dominant','known']\nstopwords.update(newStopWords)\n\nwordcloud = WordCloud(\n                          background_color='white',\n                          stopwords=stopwords,\n                          max_words=1500,\n                          max_font_size=150, \n                          width=1000, height=600,\n                          random_state=42,\n                         ).generate(\" \".join(strains[strains.Type == 'hybrid']['Description'].astype(str)))\n\nfig = plt.figure(figsize = (12,12))\nplt.imshow(wordcloud)\nplt.title(\"WORD CLOUD - HYBRIDS\", fontsize=25)\nplt.axis('off')\nplt.show()\n\n","b906f2d7":"stopwords = set(STOPWORDS)\nnewStopWords = ['strain','effect', 'genetic','effects','cross','genetics',\n                'aroma','consumer','known','dominant']\nstopwords.update(newStopWords)\n\nwordcloud = WordCloud(\n                          background_color='black',\n                          stopwords=stopwords,\n                          max_words=1500,\n                          max_font_size=150, \n                          width=1000, height=600,\n                          random_state=42,\n                         ).generate(\" \".join(strains[strains.Rating == 5]['Description'].astype(str)))\n\nfig = plt.figure(figsize = (12,12))\nplt.imshow(wordcloud)\nplt.title(\"WORD CLOUD - RATING 5\", fontsize=25)\nplt.axis('off')\nplt.show()\n\n","a5ce6203":"# Lets do some transformation in data\n\nprint(strains.head())","84bf8188":"#Transformin the Type in numerical \nstrains[\"Type\"] = pd.factorize(strains[\"Type\"])[0]\ndel strains[\"Description\"]\n# Now we have 3 numerical Types\n# 0 - Hybrid\n# 1 - Sativa\n# 2 - Indica","81310a5b":"# Creating the dummies variable of Effects and Flavors\n#effect_dummy = strains['Effects'].str.get_dummies(sep=',',)\n#flavor_dummy = strains['Flavor'].str.get_dummies(sep=',')\n\ndummy = pd.get_dummies(strains[['Effect_1','Effect_2','Effect_3','Effect_4','Effect_5','Flavor_1','Flavor_2','Flavor_3']])","9d827126":"#Concatenating the result and droping the used variables \nstrains = pd.concat([strains, dummy], axis=1)\n\nstrains = strains.drop(['Strain','Effect_1','Effect_2','Effect_3','Effect_4','Effect_5','Flavor_1','Flavor_2','Flavor_3'], axis=1)\n\nstrains.shape","16707fde":"#Importing the auxiliar and preprocessing librarys \nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report\nfrom sklearn.cross_validation import cross_val_score\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.pipeline import Pipeline\n\nfrom sklearn.model_selection import train_test_split, KFold, cross_validate\nfrom sklearn.metrics import accuracy_score\n\n#Models\nfrom sklearn.cluster import KMeans\nfrom sklearn.decomposition import PCA\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import RidgeClassifier, SGDClassifier, LogisticRegression\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, ExtraTreesClassifier, BaggingClassifier, VotingClassifier, RandomTreesEmbedding","8d196f3b":"strains.head(2)","8b4870d4":"# setting X and y\nX = strains.drop(\"Type\",1)\ny = strains[\"Type\"]\nfeature_name = X.columns.tolist()\nX = X.astype(np.float64, copy=False)\ny = y.astype(np.float64, copy=False)","65d515ac":"#Spliting the variables in train and test \nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state=42)\n\nprint(\"X_train Shape: \", X_train.shape)\nprint(\"X_test Shape: \", X_test.shape)","81b93d48":"thresh = 5 * 10**(-3)\nmodel = GradientBoostingClassifier()\nmodel.fit(X_train, y_train)\n#select features using threshold\nselection = SelectFromModel(model, threshold=thresh, prefit=True)\n\nX_important_train = selection.transform(X_train)\nX_important_test = selection.transform(X_test)","a9a6dbd3":"print(\"X_important_train Shape: \", X_important_train.shape)\nprint(\"X_important_test Shape: \", X_important_test.shape)","0e56e474":"clfs = []\nseed = 3\n\nclfs.append((\"LogReg\", \n             Pipeline([(\"Scaler\", StandardScaler()),\n                       (\"LogReg\", LogisticRegression())])))\n\nclfs.append((\"XGBClassifier\",\n             Pipeline([(\"Scaler\", StandardScaler()),\n                       (\"XGB\", XGBClassifier())]))) \nclfs.append((\"KNN\", \n             Pipeline([(\"Scaler\", StandardScaler()),\n                       (\"KNN\", KNeighborsClassifier())]))) \n\nclfs.append((\"DecisionTreeClassifier\", \n             Pipeline([(\"Scaler\", StandardScaler()),\n                       (\"DecisionTrees\", DecisionTreeClassifier())]))) \n\nclfs.append((\"RandomForestClassifier\", \n             Pipeline([(\"Scaler\", StandardScaler()),\n                       (\"RandomForest\", RandomForestClassifier())]))) \n\nclfs.append((\"GradientBoostingClassifier\", \n             Pipeline([(\"Scaler\", StandardScaler()),\n                       (\"GradientBoosting\", GradientBoostingClassifier(max_features=15, n_estimators=150))]))) \n\nclfs.append((\"RidgeClassifier\", \n             Pipeline([(\"Scaler\", StandardScaler()),\n                       (\"RidgeClassifier\", RidgeClassifier())])))\n\nclfs.append((\"BaggingRidgeClassifier\",\n             Pipeline([(\"Scaler\", StandardScaler()),\n                       (\"BaggingClassifier\", BaggingClassifier())])))\n\nclfs.append((\"ExtraTreesClassifier\",\n             Pipeline([(\"Scaler\", StandardScaler()),\n                       (\"ExtraTrees\", ExtraTreesClassifier())])))\n\n#'neg_mean_absolute_error', 'neg_mean_squared_error','r2'\nscoring = 'accuracy'\nn_folds = 7\n\nresults, names  = [], [] \n\nfor name, model  in clfs:\n    kfold = KFold(n_splits=n_folds, random_state=seed)\n    cv_results = cross_val_score(model, X_important_train, y_train, cv= 5, scoring=scoring, n_jobs=-1)    \n    names.append(name)\n    results.append(cv_results)    \n    msg = \"%s: %f (+\/- %f)\" % (name, cv_results.mean(),  cv_results.std())\n    print(msg)\n    \n# boxplot algorithm comparison\nfig = plt.figure(figsize=(15,6))\nfig.suptitle('Classifier Algorithm Comparison', fontsize=22)\nax = fig.add_subplot(111)\nsns.boxplot(x=names, y=results)\nax.set_xticklabels(names)\nax.set_xlabel(\"Algorithmn\", fontsize=20)\nax.set_ylabel(\"Accuracy of Models\", fontsize=18)\nax.set_xticklabels(ax.get_xticklabels(),rotation=45)\nplt.show()","0c28bd00":"from sklearn.grid_search import GridSearchCV\n\nparams_ridge = {'alpha':[0.001, 0.1, 1.0],\n                'tol':[0.1, 0.01, 0.001], \n                'solver':['auto', 'svd', 'cholesky','lsqr', 'sparse_cg', 'sag', 'saga']}\n\nridge = RidgeClassifier()\n    \nRidge_model = GridSearchCV(estimator = ridge, param_grid=params_ridge, verbose=2, n_jobs = -1)\n\n# Fit the random search model\nRidge_model.fit(X_important_train, y_train)","2da56e57":"# Printing the Training Score\nprint(\"Training score data: \")\nprint(Ridge_model.score(X_important_train, y_train) )\nprint(\"Ridge Best Parameters: \")\nprint(Ridge_model.best_params_ )","1ad9946a":"# Predicting with X_test\nRidge_model = RidgeClassifier(solver='sparse_cg', tol=0.001, alpha=1.0)\nRidge_model.fit(X_important_train, y_train)\ny_pred = Ridge_model.predict(X_important_test)\n\n# Print the results\nprint(accuracy_score(y_test,y_pred))\nprint(confusion_matrix(y_test, y_pred))\nprint(classification_report(y_test, y_pred))","9d5c0bff":"param_xgb = {\n 'n_estimators':[100,150,200],\n 'max_depth':[3,4,5,6],\n 'min_child_weight':[2,3,4,5],\n 'colsample_bytree':[.1, 0.2, 0.3,0.6,0.7,0.8],\n 'colsample_bylevel':[0.2,0.6,0.8]\n}","8b1b1404":"xgb = XGBClassifier()\n\nxgb_model = GridSearchCV(estimator = xgb, \n                        param_grid = param_xgb, \n                        scoring='accuracy',\n                        cv=2,\n                        verbose = 1)\n\nxgb_model.fit(X_important_train, y_train)","b82b938b":"print(\"Results of the GridSearchCV of XGB: \")\nprint(xgb_model.best_params_)\nprint(xgb_model.score(X_important_train, y_train))","c9f18743":"# let's set the best parameters to our model and fit again\nxgb = XGBClassifier(colsample_bylevel=0.6, colsample_bytree=0.1, objective='multi', max_depth= 4, min_child_weight= 2, n_estimators= 200)\nxgb.fit(X_important_train, y_train)\n\n# Predicting with X_test\ny_pred = xgb.predict(X_important_test)\n\n# Print the results\nprint(\"METRICS \\nAccuracy Score: \", accuracy_score(y_test,y_pred))\nprint(confusion_matrix(y_test, y_pred))\nprint(classification_report(y_test, y_pred))","590c0f25":"param_gb = {\n    'n_estimators':[50, 125, 150],\n    'max_depth':[2,3,4],\n    'max_features':[3,4,5,6],\n    'learning_rate':[0.0001, 0.001, 0.01,0.1,1]\n\n}\n\ngb = GradientBoostingClassifier()\n\ngb_model = GridSearchCV(estimator = gb, \n                        param_grid = param_gb, \n                        scoring='accuracy',\n                        cv=5,\n                        verbose = 1)\n\ngb_model.fit(X_important_train, y_train)","ae1764c3":"print(\"Results of the GridSearchCV of Gradient Boosting Classifier: \")\nprint(gb_model.best_params_)\nprint(gb_model.score(X_important_train, y_train))","1036f68d":"gb = GradientBoostingClassifier(learning_rate=.1, max_depth= 3, max_features=6, n_estimators= 150)\ngb.fit(X_important_train, y_train)\n\n# Predicting with X_test\ny_pred = gb.predict(X_important_test)\n\n# Print the results\nprint(\"METRICS \\nAccuracy Score: \", accuracy_score(y_test,y_pred))\nprint(confusion_matrix(y_test, y_pred))\nprint(classification_report(y_test, y_pred))","24a3242b":"- We got a nice improvement in our model compared with the first model without HyperParameters.","ab07fa58":"### Sativas effects","bf77cf13":"We can se the Effects and Flavors are in separated columns... Now I will explore the main related effects","ad05d02a":"Flavor and Description have nan value","5357716a":"WE can see that the Sativa have a less median than Hybrids and indicas","e6e4b3c1":"### Word Cloud Sativas","95412b48":"### Lets try a better look of the Rating Distribuition by species considering just the values higher than 2","eae5648d":"The most frequent values in Hybrid type is: <br>\nEarthy: 549 <br>\nSweet: 534<br>\nCitrus: 301 <br>","78a2a4e4":"### The second most related effects and respective Rating","51b6ad7f":"Sativa and Indica have a similar rating distribuition, and we can see that almost of all species in dataset have rating higher than 4","288ef544":"### Exploring Flavors","012652d1":"### Now let's see the first Flavor related\n\nWe have 33 flavors in total","c1559625":"### 7. Preprocessing dataset:\n\nKnowing all this...Let's get high!\n\nand try to predict the type of strain using flavors, effects and rating?","dfe5b1be":"### Librarys to WordCloud\n","1934eafa":"### Word Cloud Hybrids","bb292d09":"### Now, I will evaluate the best params to XGBoost model","53bec7da":"### Word Clouds","0f15b06f":"### 8. Importing Sklearn and Modeling:","ecaf1f43":"### I will select the top 3 models and set some hyperParameters to try increase their prediction power.\n- The top 3 will be:\n    - GradientBoostingClassifier\n    - XGBClassifier\n    - RidgeClassifier\n\n","618ae11f":"### Hibrids flavors","549b362b":"### 6. Exploring general Flavors and Effects:\n\nNow, let's check the flavors\nI will use the same loop to known the most related flavors","6e744428":"### Word Cloud Rating 5 Strains","dc794407":"### Word Cloud Indicas","86d17ea2":"### EDA","82a19fb3":"### Feature Selection","519c4d56":"### Indicas Flavors\n","90c7c313":"### Let's explore the Strains with Rating equal 5","1107aeae":" Almost all species have rating higher than 4\n\nNow I will Look the distribuition separted by Type\n","c9e8fb99":"### Let's take a look at some models and compare their score","b873ea3b":"5. Some observations:\n\nSome observations:\n\nWe can clearly see that Happy, Uplified, Relaxed, Euphoric have a high ranking at all 3 types\n\nIts interesting that almost 350 people of 440 in Sativas related Happy and Uplifted Effects\n\n    'Happy': 342\n    'Uplifted': 328\n    'Euphoric': 276\n    'Energetic': 268\n\n78% has described Happy to Sativas strains\n\nIndicas we have 699 votes and Relaxed with most frequency at distribuition:\n\n    'Relaxed': 628\n    'Happy': 562\n    'Euphoric': 516\n\n90% has described Relaxed to Indica strains\n\nHybrids We have 1212 votes and distribuition of effects is\n\n    'Happy': 967\n    'Relaxed': 896\n    'Uplifted': 848\n    'Euphoric': 843\n\n80% has described Happy and 74% related Relaxed to Hybrids strains\nVery Interesting!\n\nWe also need to remember that's possible to vote in more than 1 effect or flavor in each vote.\n","e7f9e4bd":"### Exploring the principal effects and Flavors Related in Rating five strains","2a92363d":"### Looking the distribuition of Rating and type by Rating","144d932e":"### Now let's fit and predict with Gradient Boosting Classifier model","75112c4a":"### Indicas effects","c965979c":"### Sativas Flavors","bab77b7f":"### Hibrids flavors","7a9af590":"Most frequent values in indicas\nEarthy: 378 <br>\nSweet: 312<br>\nPungent: 157<br>\nBerry: 145 <br>","59453f3d":"### \n\nCurious!\nWe can see that in all types, the most related flavors are Sweet and Earthly.\nIs important to remember that we have alot of another flavors that are related with this Sweet and Earthly tastes.\n\nWe can also remember that the first cannabis strain was Skunk #1, that have a high earthly and pungent taste.\n\nThe distribuition total of data set is almost nearly of this values:\n\n    hybrid 51.55\n    indica 29.73\n    sativa 18.72\n\nNow I will Explore the total Effects and Flavors related to each strain\n","5539211e":"### Most frequent flavors in Sativas:\nSweet: 207 <br>\nEarthy: 178<br>\nCitrus: 143 <br>","c36766c4":"Now, let's Predict with this model"}}