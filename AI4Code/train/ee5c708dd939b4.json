{"cell_type":{"6fc4cbc0":"code","3cb54b96":"code","badfc64f":"code","416c96dc":"code","90aab48a":"code","634dda5b":"code","8c6ada44":"code","b84bb83d":"code","6397a397":"code","d4126f17":"code","2ef8b43f":"code","892b8af7":"code","d8bb5453":"code","0704e816":"markdown","8de0c516":"markdown","ddab3f8b":"markdown","aaa59153":"markdown","a90e9143":"markdown","2d1db40a":"markdown","bebfd50e":"markdown","839a297b":"markdown"},"source":{"6fc4cbc0":"import pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import OrdinalEncoder, StandardScaler\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import roc_auc_score","3cb54b96":"df_train = pd.read_csv(\"..\/input\/tabular-playground-series-sep-2021\/train.csv\", index_col='id')\ntest = pd.read_csv(\"..\/input\/tabular-playground-series-sep-2021\/test.csv\", index_col='id')\n\nFEATURES = list(df_train.columns[:-1])\nTARGET = df_train.columns[-1]\n\ndf_train.head()","badfc64f":"df_train['n_missing'] = df_train[FEATURES].isna().sum(axis=1)\ntest['n_missing'] = test[FEATURES].isna().sum(axis=1)\n\ndf_train['std'] = df_train[FEATURES].std(axis=1)\ntest['std'] = test[FEATURES].std(axis=1)\n\nFEATURES += ['n_missing', 'std']\nn_missing = df_train['n_missing'].copy()","416c96dc":"from xgboost import XGBClassifier\n\nX = df_train.loc[:, FEATURES]\ny = df_train.loc[:, TARGET]\n\nfinal_test_predictions = []\nfinal_valid_predictions = {}\nkf = KFold(n_splits=10, shuffle=True, random_state=42)\nfor fold, (train_indicies, valid_indicies) in enumerate(kf.split(X=X)):\n    X_train = X.loc[train_indicies]\n    X_valid = X.loc[valid_indicies]\n    X_test = test.copy()\n    \n    valid_ids = X_valid.index.values.tolist()\n    y_train = y.loc[train_indicies]\n    y_valid = y.loc[valid_indicies]\n    \n    \n    scaler = StandardScaler()\n    X_train[FEATURES] = scaler.fit_transform(X_train[FEATURES])\n    X_valid[FEATURES] = scaler.transform(X_valid[FEATURES])\n    X_test[FEATURES] = scaler.transform(X_test[FEATURES])\n    \n    model = XGBClassifier(\n        max_depth=3,\n        subsample=0.5,\n        colsample_bytree=0.5,\n        learning_rate= 0.01187431306013263,\n        n_estimators= 10000,\n        n_jobs=-1,\n        use_label_encoder=False,\n        objective='binary:logistic',\n        tree_method='gpu_hist',  # Use GPU \n        gpu_id=0,\n        predictor='gpu_predictor',\n    )\n    model.fit(X_train, y_train,\n             verbose = False,\n             eval_set = [(X_train, y_train), (X_valid, y_valid)],\n             eval_metric = \"auc\",\n             early_stopping_rounds = 200)\n    \n    preds_valid = model.predict_proba(X_valid)[:,1]\n    preds_test = model.predict_proba(X_test)[:,1]\n    final_test_predictions.append(preds_test)\n    final_valid_predictions.update(dict(zip(valid_ids, preds_valid)))\n    print(fold, roc_auc_score(y_valid, preds_valid))","90aab48a":"final_valid_predictions = pd.DataFrame.from_dict(final_valid_predictions, orient=\"index\").reset_index()\nfinal_valid_predictions.columns = [\"id\", \"pred_1\"]\nfinal_valid_predictions.to_csv(\"train_pred_1.csv\", index=False)\n\nsub = pd.read_csv('..\/input\/tabular-playground-series-sep-2021\/sample_solution.csv')\nsub.claim = np.mean(np.column_stack(final_test_predictions), axis=1)\nsub.columns = [\"id\", \"pred_1\"]\nsub.to_csv(\"test_pred_1.csv\", index=False)","634dda5b":"from lightgbm import LGBMClassifier\n\nX = df_train.loc[:, FEATURES]\ny = df_train.loc[:, TARGET]\n\nfinal_test_predictions = []\nfinal_valid_predictions = {}\nkf = KFold(n_splits=10, shuffle=True, random_state=42)\nfor fold, (train_indicies, valid_indicies) in enumerate(kf.split(X=X)):\n    X_train = X.loc[train_indicies]\n    X_valid = X.loc[valid_indicies]\n    X_test = test.copy()\n    \n    valid_ids = X_valid.index.values.tolist()\n    y_train = y.loc[train_indicies]\n    y_valid = y.loc[valid_indicies]\n    \n    \n    scaler = StandardScaler()\n    X_train[FEATURES] = scaler.fit_transform(X_train[FEATURES])\n    X_valid[FEATURES] = scaler.transform(X_valid[FEATURES])\n    X_test[FEATURES] = scaler.transform(X_test[FEATURES])\n    \n    model = LGBMClassifier(\n        max_depth = 3,\n        num_leaves = 7,\n        n_estimators = 10000,\n        colsample_bytree = 0.3,\n        subsample = 0.5,\n        random_state = 42,\n        reg_alpha=18,\n        reg_lambda=17,\n        learning_rate = 0.095,\n        device = 'gpu',\n        objective= 'binary'\n    )\n    \n    model.fit(X_train, y_train,\n             verbose = False,\n             eval_set = [(X_train, y_train), (X_valid, y_valid)],\n             eval_metric = \"auc\",\n             early_stopping_rounds = 200)\n    \n    preds_valid = model.predict_proba(X_valid)[:,1]\n    preds_test = model.predict_proba(X_test)[:,1]\n    final_test_predictions.append(preds_test)\n    final_valid_predictions.update(dict(zip(valid_ids, preds_valid)))\n    print(fold, roc_auc_score(y_valid, preds_valid))","8c6ada44":"final_valid_predictions = pd.DataFrame.from_dict(final_valid_predictions, orient=\"index\").reset_index()\nfinal_valid_predictions.columns = [\"id\", \"pred_2\"]\nfinal_valid_predictions.to_csv(\"train_pred_2.csv\", index=False)\n\nsub = pd.read_csv('..\/input\/tabular-playground-series-sep-2021\/sample_solution.csv')\nsub.claim = np.mean(np.column_stack(final_test_predictions), axis=1)\nsub.columns = [\"id\", \"pred_2\"]\nsub.to_csv(\"test_pred_2.csv\", index=False)","b84bb83d":"df = pd.read_csv(\"..\/input\/tabular-playground-series-sep-2021\/train.csv\")\ndf_test = pd.read_csv(\"..\/input\/tabular-playground-series-sep-2021\/test.csv\")\n\ndf1 = pd.read_csv(\"train_pred_1.csv\")\ndf2 = pd.read_csv(\"train_pred_2.csv\")\n\ndf_test1 = pd.read_csv(\"test_pred_1.csv\")\ndf_test2 = pd.read_csv(\"test_pred_2.csv\")\n\ndf = df.merge(df1, on=\"id\", how=\"left\")\ndf = df.merge(df2, on=\"id\", how=\"left\")\n\ndf_test = df_test.merge(df_test1, on=\"id\", how=\"left\")\ndf_test = df_test.merge(df_test2, on=\"id\", how=\"left\")\n\ndf.head()","6397a397":"df_test.head(100)","d4126f17":"from sklearn.linear_model import LogisticRegression\n\nFEATURES = [\"pred_1\", \"pred_2\"]\ndf_test = df_test[FEATURES]\n\nfinal_predictions = []\nvalid_predictions = []\nkf = KFold(n_splits=10, shuffle=True, random_state=42)\nfor fold, (train_indicies, valid_indicies) in enumerate(kf.split(X=X)):\n    X_train =  df.loc[train_indicies]\n    X_valid = df.loc[train_indicies]\n    X_test = df_test.copy()\n\n    y_train = X_train.claim\n    y_valid = X_valid.claim\n    \n    scaler = StandardScaler()\n    X_train[FEATURES] = scaler.fit_transform(X_train[FEATURES])\n    X_valid[FEATURES] = scaler.transform(X_valid[FEATURES])\n    X_test[FEATURES] = scaler.transform(X_test[FEATURES])\n    \n    X_train = X_train[FEATURES]\n    X_valid = X_valid[FEATURES]\n    \n    model = LogisticRegression(fit_intercept=False)\n    model.fit(X_train, y_train)\n    \n    preds_valid = model.predict_proba(X_valid)[:,1]\n    preds_test = model.predict_proba(X_test)[:,1]\n    final_predictions.append(preds_test)\n    valid_predictions.append(preds_valid)\n    print(fold, roc_auc_score(y_valid, preds_valid))","2ef8b43f":"preds = np.mean(np.column_stack(final_predictions), axis=1)\n\n# Make predictions\ny_pred = pd.Series(\n    preds,\n    index=sub.id,\n    name=TARGET,\n)\n\n# Create submission file\ny_pred.to_csv(\"submission.csv\")","892b8af7":"y_pred.head(100)","d8bb5453":"print(valid_predictions)","0704e816":"# Step4: Blending","8de0c516":"# Train Model: LGBM","ddab3f8b":"# Step2: Load Data","aaa59153":"# Step3: Train Model: XGBoost #\n\nLet's try out a simple XGBoost model. This algorithm can handle missing values, but you could try imputing them instead.  We use `XGBClassifier` (instead of `XGBRegressor`, for instance), since this is a classification problem.","a90e9143":"# Make Submission #\n\nOur predictions are binary 0 and 1, but you're allowed to submit probabilities instead. In scikit-learn, you would use the `predict_proba` method instead of `predict`.","2d1db40a":"# Welcome to the September 2021 Tabular Playground Competition! #\n\nIn this competition, we predict whether a customer will make an insurance claim.\n\n# Step1: Import Helpful Libraries #","bebfd50e":"The target `'claim'` has binary outcomes: `0` for no claim and `1` for claim.","839a297b":"# Missing Values\nRefer to [TPS Sep 2021 single LGBM](https:\/\/www.kaggle.com\/hiro5299834\/tps-sep-2021-single-lgbm\/notebook) by [@hiro5299834](https:\/\/www.kaggle.com\/hiro5299834)"}}