{"cell_type":{"14e05d72":"code","22618d43":"code","50586171":"code","e05b798e":"code","26b7e269":"code","0a2375cf":"code","193a18bd":"code","5a444317":"code","cac35d34":"code","6ffe06bb":"code","7f73aa4b":"code","818fb806":"code","7185b1e6":"code","c0c52444":"markdown","f31532be":"markdown","27a45039":"markdown","803c4b46":"markdown","357ce9ec":"markdown","9b438a3c":"markdown","6fec4be0":"markdown","e53b3cb7":"markdown","643a2d26":"markdown"},"source":{"14e05d72":"import random\nrandom.seed(123)\n\nimport pandas as pd\nimport numpy as np\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# importing viz packages\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# importing modelling packages\n\nfrom sklearn.model_selection import train_test_split, cross_val_score, cross_val_predict, GridSearchCV, KFold, StratifiedKFold\nfrom sklearn.feature_selection import SelectKBest,f_regression,SelectPercentile,VarianceThreshold\nfrom mlxtend.feature_selection import SequentialFeatureSelector as SFS, ExhaustiveFeatureSelector as EFS\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import MinMaxScaler,StandardScaler,PowerTransformer\nfrom sklearn.decomposition import PCA\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import RandomForestRegressor, VotingRegressor\nfrom xgboost import XGBRegressor\nfrom catboost import CatBoostRegressor\nfrom lightgbm import LGBMRegressor\nfrom sklearn.tree import DecisionTreeRegressor\n\n# Optimisation Packages\n\nimport optuna\nfrom optuna import trial\nfrom optuna.samplers import TPESampler\nimport pprint\nimport joblib\nfrom skopt import BayesSearchCV\nfrom skopt.callbacks import DeadlineStopper, VerboseCallback, DeltaXStopper\nfrom skopt.space import Real, Categorical, Integer\nfrom time import time","22618d43":"# importing train and test\n\ntrain = pd.read_csv(r'..\/input\/tabular-playground-series-aug-2021\/train.csv')\ntest = pd.read_csv(r'..\/input\/tabular-playground-series-aug-2021\/test.csv')","50586171":"# seperating into dependent and independent variables\n\nX = train.drop(['id','loss'],axis=1)\ny = train['loss']\ntest = test.drop(['id'],axis=1)","e05b798e":"# using minmax scaler for scaling data\n\nscaler = MinMaxScaler()\n\nX = scaler.fit_transform(X)\ntest = scaler.transform(test)","26b7e269":"def fit_lgb(trial, x_train, y_train, x_test, y_test):\n    params = {\n        'reg_alpha' : trial.suggest_loguniform('reg_alpha' , 0.47 , 0.5),\n        'reg_lambda' : trial.suggest_loguniform('reg_lambda' , 0.32 , 0.33),\n        'num_leaves' : trial.suggest_int('num_leaves' , 50 , 70),\n        'learning_rate' : trial.suggest_uniform('learning_rate' , 0.03 , 0.04),\n        'max_depth' : trial.suggest_int('max_depth', 30 , 40),\n        'n_estimators' : trial.suggest_int('n_estimators', 100 , 6100),\n        'min_child_weight' : trial.suggest_loguniform('min_child_weight', 0.015 , 0.02),\n        'subsample' : trial.suggest_uniform('subsample' , 0.9 , 1.0), \n        'colsample_bytree' : trial.suggest_loguniform('colsample_bytree', 0.52 , 1),\n        'min_child_samples' : trial.suggest_int('min_child_samples', 76, 80),\n        'metric' : 'rmse',\n        'device_type' : 'gpu',\n    }\n    \n    model = LGBMRegressor(**params,device = 'gpu', random_state=2021)\n    model.fit(x_train, y_train,eval_set=[(x_test,y_test)], early_stopping_rounds=150, verbose=False)\n    \n    y_train_pred = model.predict(x_train)\n    y_test_pred = model.predict(x_test)\n    y_train_pred = np.clip(y_train_pred, 0.1, None)\n    y_test_pred = np.clip(y_test_pred, 0.1, None)\n    \n    log = {\n        \"train rmse\": mean_squared_error(y_train, y_train_pred,squared=False),\n        \"valid rmse\": mean_squared_error(y_test, y_test_pred,squared=False)\n    }\n    \n    return model, log","0a2375cf":"def objective(trial):\n    rmse = 0\n    x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.25)\n    model, log = fit_lgb(trial, x_train, y_train, x_test, y_test)\n    rmse += log['valid rmse']\n        \n    return rmse","193a18bd":"# optuna trials on lgbm - commenting to save run time\n\n#study = optuna.create_study(direction='minimize', study_name='LGBMRegressor')\n#study.optimize(objective,n_trials=10)\n\n#print('Number of finished trials: ', len(study.trials))\n#print('Best trial:')\n#trial = study.best_trial\n\n#print('\\tValue: {}'.format(trial.value))\n#print('\\tParams: ')\n#for key, value in trial.params.items():\n #   print('\\t\\t{}: {}'.format(key, value))","5a444317":"lgb_params =  {'reg_alpha': 0.49296333273117504, 'reg_lambda': 0.32320931014536086, \n               'num_leaves': 54, 'learning_rate': 0.03832217782251515, 'max_depth': 37, 'n_estimators': 2973,\n               'min_child_weight': 0.019808752100234205, 'subsample': 0.9662983672394618, \n               'colsample_bytree': 0.5413818580548442, 'min_child_samples': 80}","cac35d34":"def cross_val(X, y, model, params, folds=10):\n\n    kf = KFold(n_splits=folds, shuffle=True, random_state=2021)\n    for fold, (train_idx, test_idx) in enumerate(kf.split(X, y)):\n        print(f\"Fold: {fold}\")\n        x_train, y_train = X[train_idx], y[train_idx]\n        x_test, y_test = X[test_idx], y[test_idx]\n\n        alg = model(**params,random_state = 2021)\n        alg.fit(x_train, y_train,\n                eval_set=[(x_test, y_test)],\n                early_stopping_rounds=400,\n                verbose=False)\n        pred = alg.predict(x_test)\n        error = mean_squared_error(y_test, pred,squared = False)\n        print(f\" mean_squared_error: {error}\")\n        print(\"-\"*50)\n    \n    return alg","6ffe06bb":"lgb_model = cross_val(X, y, LGBMRegressor, lgb_params)","7f73aa4b":"# initialising my models - I am yet to tune CB properly.\n\ncat = CatBoostRegressor()\nlgb = LGBMRegressor(**lgb_params)","818fb806":"# Voting - LGBM (tuned) + CB (baseline right now) - 0.5 weights\n\nfrom sklearn.ensemble import VotingRegressor\nfolds = KFold(n_splits = 5, random_state = 228, shuffle = True)\n\npredictions = np.zeros(len(test))\n\nfor fold, (trn_idx, val_idx) in enumerate(folds.split(X)):\n    print(f\"Fold: {fold}\")\n    X_train, X_val = X[trn_idx], X[val_idx]\n    y_train, y_val = y[trn_idx], y[val_idx]\n\n    model = VotingRegressor(\n            estimators = [\n                ('lgbm', lgb),\n               ('cat',cat)\n            ],\n            weights = [0.5,0.5]\n        )\n   \n    model.fit(X_train, y_train)\n    pred = model.predict(X_val)\n    error = mean_squared_error(y_val, pred,squared = False)\n    print(f\" mean_squared_error: {error}\")\n    print(\"-\"*50)\n    \n    predictions += model.predict(test) \/ folds.n_splits ","7185b1e6":"sub = pd.read_csv(r'..\/input\/tabular-playground-series-aug-2021\/sample_submission.csv')\nsub['loss'] = predictions\nsub.to_csv(f'vote.csv',index = False)","c0c52444":"<div style=\"background-color:rgba(55, 99, 71, 0.5);\">\n    <h1><center>Best Parameters from my Optuna run<\/center><\/h1>\n<\/div>","f31532be":"<div style=\"background-color:rgba(55, 99, 71, 0.5);\">\n    <h1><center>Data Preprocessing<\/center><\/h1>\n<\/div>","27a45039":"Thanks Ranjeet Shrivastav for sharing your approach. I have used LGBM and CatBoost for final voting (0.5-0.5) and am yet to tune my CatBoost model.\nPlease upvote his work here - https:\/\/www.kaggle.com\/ranjeetshrivastav\/tps-aug-21-optuna-lgb-xgb-cb","803c4b46":"<div style=\"background-color:rgba(55, 99, 71, 0.5);\">\n    <h2><center>Prediction and submission<\/center><\/h2>\n<\/div>","357ce9ec":"<div style=\"background-color:rgba(55, 99, 71, 0.5);\">\n    <h1><center>Data Input<\/center><\/h1>\n<\/div>","9b438a3c":"<div style=\"background-color:rgba(55, 99, 71, 0.5);\">\n    <h2><center>Final Voting<\/center><\/h2>\n<\/div>","6fec4be0":"<div style=\"background-color:rgba(55, 99, 71, 0.5);\">\n    <h1><center>Model Building+Optuna on LightGBM<\/center><\/h1>\n<\/div>","e53b3cb7":"<div style=\"background-color:rgba(55, 99, 71, 0.5);\">\n    <h1><center>Importing Libraries<\/center><\/h1>\n<\/div>","643a2d26":"<div style=\"background-color:rgba(55, 99, 71, 0.5);\">\n    <h2><center>Thanks! Kindly upvote if you liked my basic notebook :)<\/center><\/h2>\n<\/div>"}}