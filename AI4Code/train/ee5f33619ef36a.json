{"cell_type":{"f4349c58":"code","e1fe9f30":"code","69f8f691":"code","773501ec":"code","8829c1ba":"code","eda92923":"code","c90e1024":"code","4fb73ae3":"code","e34876ba":"code","4fc42c59":"code","c075ec53":"code","23be71ef":"code","cc161f47":"code","02df289d":"code","950e3e2f":"code","b57acc7f":"code","45c4e72f":"code","60f85a94":"code","e2c2d812":"code","219679be":"code","8c55c238":"code","331d3b36":"code","6fdbff02":"code","220094cb":"code","eea65a83":"code","8ad4d5ae":"code","4f9c37ab":"code","11ab0895":"code","41ea950d":"code","c236590c":"code","6a98915b":"code","92189b78":"code","3ee5e24f":"code","e3a3e236":"code","8d21c38f":"code","a630f078":"code","581c5ac2":"code","50a38336":"code","f4e23f84":"code","b8654570":"code","0e41203b":"code","786fdb28":"code","a739f947":"code","85854a10":"code","d88e3a25":"code","5adf278c":"code","6c60ef67":"code","1e55dd95":"code","c00020e0":"code","e0b3038a":"code","e11dcec0":"markdown","87a37f92":"markdown","28408d6e":"markdown","d42ae715":"markdown","67d8b01c":"markdown","c8f65451":"markdown","abc5c0c8":"markdown","450b515a":"markdown","ba7605ad":"markdown","435c6390":"markdown","e50ded83":"markdown","3ef21b17":"markdown","31d94343":"markdown","c9ebe37c":"markdown","d687534a":"markdown","9612d336":"markdown","71af5983":"markdown","18f6762e":"markdown","f5203f4f":"markdown","6f9cdb7f":"markdown","90fcb42e":"markdown"},"source":{"f4349c58":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom IPython.display import display\n\n\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","e1fe9f30":"def display_all(df):\n    with pd.option_context(\"display.max_rows\", 1000, \"display.max_columns\", 1000): \n        display(df)","69f8f691":"data = pd.read_csv(\"\/kaggle\/input\/housedata\/output.csv\", low_memory=False , parse_dates = [\"date\"])\ndisplay_all(data.T)# .T(transpoze)","773501ec":"display_all(data.price.describe())","8829c1ba":"print(sum(data[\"price\"] == 0))","eda92923":"for i in range(data.columns.shape[0]):\n    print(data.columns[i],\"feature has {} null values\".format(sum(data[data.columns[i]].isnull())))","c90e1024":"data[\"price_na\"] = data.price == 0 \nsum(data[\"price_na\"] == True)","4fb73ae3":"data.loc[data.price_na == True].head()","e34876ba":"data.loc[data.price == 0,[\"price\"]] = data.price.median()\ndisplay_all(data.describe())","4fc42c59":"print(sum(data[\"price\"] == 0))","c075ec53":"data[\"price\"]","23be71ef":"data[\"price\"] = np.log(data[\"price\"])\ndata[\"price\"]","cc161f47":"data[\"date\"].dt","02df289d":"data[\"date\"].dt.year","950e3e2f":"import re\n#regex library","b57acc7f":"def fixdatecolumn(data,column_name,drop = True, time = True):\n    data_column = data[column_name]\n    column_dtype = data_column.dtype\n    \n    targ_name = re.sub('[Dd]ate$', '', column_name)\n    \n    attr = ['Year', 'Month', 'Week', 'Day', 'Dayofweek', 'Dayofyear',\n            'Is_month_end', 'Is_month_start', 'Is_quarter_end', 'Is_quarter_start', 'Is_year_end', 'Is_year_start']\n    \n    \n    if time: \n        attr = attr + ['Hour', 'Minute', 'Second']\n        \n    for a in attr: \n        data[targ_name + a] = getattr(data_column.dt, a.lower())\n        \n    data[targ_name + 'Elapsed'] = data_column.astype(np.int64) \/\/ 10 ** 9\n    \n    if drop: \n        data.drop(column_name, axis=1, inplace=True)\nfixdatecolumn(data,\"date\")\n","45c4e72f":"display_all(data.head())\n","60f85a94":"from pandas.api.types import is_string_dtype\nfrom pandas.api.types import is_numeric_dtype\nfrom pandas.api.types import is_bool_dtype","e2c2d812":"cat_list = []\ndef train_cats(df):\n    for n,c in df.items():\n        if is_string_dtype(c) or is_bool_dtype(c):# Accualy for random forest, converting boolean dtype to category is unnecessary\n            df[n] = c.astype(\"category\").cat.as_ordered()\n            cat_list.append(n)","219679be":"print(cat_list)","8c55c238":"def apply_cats(df, train):\n    for n, c in df.items():\n        if train[n].dtype == \"category\":\n            df[n] = pd.Categorical(c, categories = train[n].cat.categories, ordered = True)\n            ","331d3b36":"train_cats(data)","6fdbff02":"data[\"city\"].cat.categories","220094cb":"data[\"city\"].cat.codes","eea65a83":"data_for_ohed = data.copy()","8ad4d5ae":"for n, c in data.items():\n        \n    if not is_numeric_dtype(c):\n        data[n] = c.cat.codes+1","4f9c37ab":"y = data.price\ndisplay_all(y.head())","11ab0895":"x = data.drop([\"price\"], axis=1)\ndisplay_all(x.head())","41ea950d":"from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n\nfrom sklearn import metrics","c236590c":"m = RandomForestRegressor(n_jobs=-1) # n_jobs =-1 allows us to use all units of processor\nm.fit(x, y)\nm.score(x,y)","6a98915b":"def split_train_val(df,n): \n    return df[:n].copy(), df[n:].copy()","92189b78":"n_valid = int(np.floor(len(data)*0.2))\nn_train = len(data)-n_valid\nX_train, X_valid = split_train_val(x, n_train)\ny_train, y_valid = split_train_val(y, n_train)\n\nX_train.shape, y_train.shape, X_valid.shape, y_valid.shape\n","3ee5e24f":"import math","e3a3e236":"def rmse(x,y): \n    return math.sqrt(((x-y)**2).mean())","8d21c38f":"def print_score(m):\n    \n    print(f\"RMSE of train set {rmse(m.predict(X_train), y_train)}\")\n    print(f\"RMSE of validation set {rmse(m.predict(X_valid), y_valid)}\")\n    print(f\"R^2 of train set {m.score(X_train, y_train)}\")\n    print(f\"R^2 of validation set {m.score(X_valid, y_valid)}\")","a630f078":"m = RandomForestRegressor(n_jobs=-1)\n%time m.fit(X_train, y_train)\nprint_score(m)","581c5ac2":"best_result = 0\nbest_couple = [0,0]\nfor i in range(10,160,50):\n    for j in range(1,15):\n    \n        m = RandomForestRegressor(n_estimators=i, min_samples_leaf=j, n_jobs=-1)\n        m.fit(X_train, y_train)\n    \n        if m.score(X_valid, y_valid) > best_result :\n            best_result = m.score(X_valid, y_valid)\n            best_couple = [i,j]\n    print(\"{}. epoch,choosing best\".format(i\/50+0.8))\nm = RandomForestRegressor(n_estimators=best_couple[0], min_samples_leaf=best_couple[1], n_jobs=-1, oob_score=True)\nm.fit(X_train, y_train)","50a38336":"print_score(m)","f4e23f84":"m = RandomForestRegressor(n_estimators=best_couple[0], min_samples_leaf=best_couple[1], n_jobs=-1)\nm.fit(X_train, y_train)\nprint_score(m)","b8654570":"import sklearn.ensemble ","0e41203b":"def set_rf_samples(n):\n    \"\"\" Changes Scikit learn's random forests to give each tree a random sample of\n    n random rows.\n    \"\"\"\n    forest._generate_sample_indices = (lambda rs, n_samples:\n        forest.check_random_state(rs).randint(0, n_samples, n))\n\ndef reset_rf_samples():\n    \"\"\" Undoes the changes produced by set_rf_samples.\n    \"\"\"\n    forest._generate_sample_indices = (lambda rs, n_samples:\n        forest.check_random_state(rs).randint(0, n_samples, n_samples))","786fdb28":"set_rf_samples(1000)\nm = RandomForestRegressor(n_estimators=best_couple[0], min_samples_leaf=best_couple[1], max_features=0.5, n_jobs=-1)\n%time m.fit(X_train, y_train)\nprint_score(m)","a739f947":"for i in cat_list:\n    print(i,\"unique values\",x[i].nunique())","85854a10":"fi = pd.DataFrame({'columns':X_train.columns, 'importance':m.feature_importances_}\n                       ).sort_values('importance', ascending=False)\nfi.plot('columns', 'importance', 'barh', figsize=(12,7), legend=False)","d88e3a25":"keep_columns = fi[fi[\"importance\"]<0.005][\"columns\"]; ","5adf278c":"keep_columns","6c60ef67":"print(X_train.columns)","1e55dd95":"for i in X_train.columns :\n    flag = 0\n    for j in keep_columns:\n        if i == j:\n            flag = 1\n    if flag == 1:    \n        X_train.drop([i], axis=1, inplace=True)\n        X_valid.drop([i], axis=1, inplace=True)\n        \n        ","c00020e0":"display_all(X_train)","e0b3038a":"m = RandomForestRegressor(n_estimators=best_couple[0], min_samples_leaf=best_couple[1], max_features=0.5, n_jobs=-1)\n%time m.fit(X_train, y_train)\nprint_score(m)","e11dcec0":"There are 49 sample that are uncorrectly filled. We need to handle this situation in progress.","87a37f92":"After theese process when we look at our dataset we see that we have some columns that have categorical variables. The model needs inputs as a integer. So we need to convert strings to integers. We will map every single unique value to a integer. If a column has 6 different string type unique value like New york , Berlin , Istanbul , Spain , Paris and Ankara theese will be converted to 0,1,2,3,4,5.","28408d6e":"There is no null value except price feature. (We realize the problem above) Now it is time to consider about zeors. We can fill the zeros with random numbers,mean,median or just let them to be zero. The most reasonable choices are mean and median. Median is more powerful option about outliers because the scale of house prices is really large and the standard deviation of the prices is large like scale. Lets fill them with median. There is an important thing here. We need to feed our model with an information column that the price value is synthetic or naturel ( if we filled up with median, that is a synthetic value). So we will create a column price_na. If it is synthetic the value of the column will be True.","d42ae715":"We got really small improvement.","67d8b01c":"Yesssss,finaly we achived somewhere. Lets keep trying what we are doing, I mean some improvements.","c8f65451":"When we look our results our predictions on training set is really good but on validation set the results are not convincing. We need to improve our model.There are some ways that we will apply. If we had larger dataset we can apply bagging. In bagging we create more than one model that are not corralated then we take the mean of their prediction. (Ensembling) But the models should see different parts of our training data. In our case number of samples are not enough to seperate the data to different parts. So we pass this application. Now Training result is realy good but validation is not enough. It may be based on overfitting. We can reduce the depth of trees by using min_samples_leaf. Then using more trees can decrease overfitting.","abc5c0c8":"Above, we see that the minimum price is 0. This is extraordinary and unexpected stiation. So this is a mistake. Lets look at how many sample are there in the dataset like this.\n","450b515a":"OHE ? One hot encoding. Think about that one of the trees wants to divde by city. There is a city that is incredibly expensive like Dubai or Abu Dhabi. Maybe all of the prices above 250.000$ are belong to it. Now in our dataframe dubai matches with some random integer like 34 . Model does not divide like \" this is 34!!! hmm okey take 34 left side and others right side of the tree\". But if we were able to represent our feature like if city == dubai then 1 else 0. Luckily we are able to do this. OHE :)","ba7605ad":"Now lets look at is there any sample that have null value of any feature. ","435c6390":"We handled the categorical variabes for now. Now lets seperate our prediction column(y) and features(x). ","e50ded83":"So to test if we are doing great or it is just overfitting, we must have another dataset to check the result. I want to split train data as %80 train and %20 validation.","3ef21b17":"Below, we will define display_all function to  display all columns. ","31d94343":"We split our data as train and validation. Now the beggining of the notebook we talked abot RMSE. Time to define it.","c9ebe37c":"Above we see that we can not apply OHE. WHY ? 1 unique value can not be divided. Model can consider 2 unique values as True and False. The unique values of other features are to many. Time complexity and memory problems will be seen. Lets skip this one.","d687534a":"It is time to get rid of redundant features. ","9612d336":"When trees make splitting, they split by using the column most reducing the entropy at that time. This make our trees more coralated. If say that while model choosing the feature for splitting you can choose only theese random features. It won't see some features like most likely reducing entropy. This makes trees different from each others.","71af5983":"It is time to build our model,Random Forest.","18f6762e":"We took some actions but our accuracy was not increase :D Now we are gonna something really interesting. Our trees may be still corelated with each other  and this might be result in over-fitting. If we set our trees to see diffent parts of our training data, this might be decreasing corralation. Lets look at what we can do !!! The difference between bagging is that in bagging models see different subsets but in this one trees see different subsets.","f5203f4f":"We will create a model (random forest) and for evalaute that we need a loss function. We will use RMSLE ( Root Mean Squared Log Error ).It is better than RMSE about some topics.\n* Outlier\n* Relative error\n****\nWe will take the logarithm of prediction column (In our case \"PRICE\") then we can apply Root mean squared error.","6f9cdb7f":"We have a column named as date. This column has a lot of information but if we use in this form the model will see just lots of different meaningless unique value. We will divide the ifromation that the column includes to different feautres as year,month,day. With this we can feed the model with more revealing data.","90fcb42e":"Well, the accuracy is amazing... %95.9. But we test this on our traning set. This means model tunned the parameters exactly the numbers it saw. In real world, we do not know it really works."}}