{"cell_type":{"58c3825c":"code","6e749e33":"code","5872dc9b":"code","6a9bea21":"code","6ef14dfc":"code","8d87788d":"code","919c2598":"code","c3077072":"code","8f128a89":"code","c4a676e1":"code","6f7a1c94":"code","2a45b370":"code","db2c977a":"code","41909610":"code","d4769932":"code","7901d79e":"code","d9792f23":"code","634095d2":"code","6c943715":"code","f81c833f":"code","0e7d6f96":"code","7f14b754":"code","33b9e446":"code","bed3ed43":"code","8c4f5ce7":"code","04b3dd67":"code","bc540059":"code","69684c9a":"code","f788f473":"code","98d45971":"code","56ba09e8":"code","a6044c99":"code","96a7af4a":"code","c5cf8e78":"code","fe6dc69d":"code","93b01fe9":"code","998a2b72":"code","a7702e0b":"code","afcb4080":"code","9bc5864e":"code","f42d0a62":"code","9229a33e":"code","5ae46b0d":"code","6e51e3ef":"code","340a2be3":"code","70c963a2":"code","531a1f7b":"code","696a31ee":"code","39d217d4":"code","7c921e88":"code","0b6a33b7":"code","20b96208":"code","15fafc08":"code","47cb0b4b":"code","bcd9f2cc":"code","018ab09d":"code","3f464b77":"code","d76c3a4e":"code","50cc493f":"code","508d808c":"code","e55e4009":"code","2ed4dfd8":"code","e0dc67e5":"code","2bf1ed8f":"code","a4482558":"code","775499fe":"code","1113e87e":"code","6dfe7fbb":"code","ae33172c":"code","afc3e2db":"code","2ce6737d":"code","fa682602":"code","a34c6acd":"code","f49bb3b7":"code","4c3d787c":"code","f49ba1fb":"code","39c053e2":"code","0dc5cdf0":"code","b69fa300":"markdown","5135de62":"markdown","a0e57e8f":"markdown","75d7a2c6":"markdown","72955954":"markdown","af0274be":"markdown","bf161ba4":"markdown","8d61420e":"markdown","260c2799":"markdown","b50d0118":"markdown","86b0faaa":"markdown","57c616dd":"markdown","7e293f98":"markdown","28c9de70":"markdown","abbbf976":"markdown","6f73647a":"markdown","f8b97d99":"markdown","c958427b":"markdown","fca7cc50":"markdown","cfe656ea":"markdown","a46f2d38":"markdown","763f0c2f":"markdown","ed7267b2":"markdown","33a93da5":"markdown","cd188984":"markdown","5dc67a02":"markdown","5882891c":"markdown","9ab7db3a":"markdown","12ceb443":"markdown","77cc1223":"markdown","bbc8e8d1":"markdown","2c189ea6":"markdown","53db532d":"markdown","1cf7e521":"markdown","ab1f73cb":"markdown","93d410f6":"markdown","a8583762":"markdown","df5570d8":"markdown","49721e2c":"markdown","240c1375":"markdown","40e88b46":"markdown","ad92ec86":"markdown","011603cb":"markdown","fd0551a8":"markdown","b23ec677":"markdown","cb24d67e":"markdown","71100e97":"markdown"},"source":{"58c3825c":"import numpy as np\nimport pandas as pd\nimport os\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom tqdm import tqdm_notebook\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.svm import NuSVR, SVR\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.metrics import roc_auc_score\npd.options.display.precision = 15\n\n\nimport xgboost as xgb\nimport time\nimport datetime\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import StratifiedKFold, KFold, RepeatedKFold, GroupKFold, GridSearchCV, train_test_split, TimeSeriesSplit\nfrom sklearn import metrics\nfrom sklearn import linear_model\nimport gc\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport eli5\nimport shap\nfrom IPython.display import HTML","6e749e33":"import os\nimport time\nimport datetime\nimport json\nimport gc\nfrom numba import jit\n\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom tqdm import tqdm_notebook\n\nimport lightgbm as lgb\nimport xgboost as xgb\nfrom catboost import CatBoostRegressor, CatBoostClassifier\nfrom sklearn import metrics\n\nfrom itertools import product\n\nimport altair as alt\nfrom altair.vega import v5\nfrom IPython.display import HTML\n\n# using ideas from this kernel: https:\/\/www.kaggle.com\/notslush\/altair-visualization-2018-stackoverflow-survey\ndef prepare_altair():\n    \"\"\"\n    Helper function to prepare altair for working.\n    \"\"\"\n\n    vega_url = 'https:\/\/cdn.jsdelivr.net\/npm\/vega@' + v5.SCHEMA_VERSION\n    vega_lib_url = 'https:\/\/cdn.jsdelivr.net\/npm\/vega-lib'\n    vega_lite_url = 'https:\/\/cdn.jsdelivr.net\/npm\/vega-lite@' + alt.SCHEMA_VERSION\n    vega_embed_url = 'https:\/\/cdn.jsdelivr.net\/npm\/vega-embed@3'\n    noext = \"?noext\"\n    \n    paths = {\n        'vega': vega_url + noext,\n        'vega-lib': vega_lib_url + noext,\n        'vega-lite': vega_lite_url + noext,\n        'vega-embed': vega_embed_url + noext\n    }\n    \n    workaround = f\"\"\"    requirejs.config({{\n        baseUrl: 'https:\/\/cdn.jsdelivr.net\/npm\/',\n        paths: {paths}\n    }});\n    \"\"\"\n    \n    return workaround\n    \n\ndef add_autoincrement(render_func):\n    # Keep track of unique <div\/> IDs\n    cache = {}\n    def wrapped(chart, id=\"vega-chart\", autoincrement=True):\n        if autoincrement:\n            if id in cache:\n                counter = 1 + cache[id]\n                cache[id] = counter\n            else:\n                cache[id] = 0\n            actual_id = id if cache[id] == 0 else id + '-' + str(cache[id])\n        else:\n            if id not in cache:\n                cache[id] = 0\n            actual_id = id\n        return render_func(chart, id=actual_id)\n    # Cache will stay outside and \n    return wrapped\n           \n\n@add_autoincrement\ndef render(chart, id=\"vega-chart\"):\n    \"\"\"\n    Helper function to plot altair visualizations.\n    \"\"\"\n    chart_str = \"\"\"\n    <div id=\"{id}\"><\/div><script>\n    require([\"vega-embed\"], function(vg_embed) {{\n        const spec = {chart};     \n        vg_embed(\"#{id}\", spec, {{defaultStyle: true}}).catch(console.warn);\n        console.log(\"anything?\");\n    }});\n    console.log(\"really...anything?\");\n    <\/script>\n    \"\"\"\n    return HTML(\n        chart_str.format(\n            id=id,\n            chart=json.dumps(chart) if isinstance(chart, dict) else chart.to_json(indent=None)\n        )\n    )\n    \n## Function to reduce the DF size\ndef reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() \/ 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() \/ 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) \/ start_mem))\n    return df\n\n\n# setting up altair\nworkaround = prepare_altair()\nHTML(\"\".join((\n    \"<script>\",\n    workaround,\n    \"<\/script>\",\n)))","5872dc9b":"%%time\nfolder_path = '..\/input\/'\n\ntrain_identity = pd.read_csv(f'{folder_path}train_identity.csv')\ntrain_transaction = pd.read_csv(f'{folder_path}train_transaction.csv')\ntest_identity = pd.read_csv(f'{folder_path}test_identity.csv')\ntest_transaction = pd.read_csv(f'{folder_path}test_transaction.csv')\n\nsub = pd.read_csv(f'{folder_path}sample_submission.csv')\n\nfix = {o:n for o, n in zip(test_identity.columns, train_identity.columns)}#\"id\" columns in test dataset are different from the train dataset\ntest_identity.rename(columns=fix, inplace=True)\n\n# let's combine the data and work with the whole dataset\ntrain = pd.merge(train_transaction, train_identity, on='TransactionID', how='left')\ntest = pd.merge(test_transaction, test_identity, on='TransactionID', how='left')\n\ndel train_identity, train_transaction, test_identity, test_transaction, fix\n\ntrain = reduce_mem_usage(train)\ntest = reduce_mem_usage(test)","6a9bea21":"print(f'Train dataset has {train.shape[0]} rows and {train.shape[1]} columns.')\nprint(f'Test dataset has {test.shape[0]} rows and {test.shape[1]} columns.')","6ef14dfc":"# V COLUMNS TO LOAD DECIDED BY CORRELATION EDA\n# https:\/\/www.kaggle.com\/cdeotte\/eda-for-columns-v-and-id\nv =  [1, 3, 4, 6, 8, 11]\nv += [13, 14, 17, 20, 23, 26, 27, 30]\nv += [36, 37, 40, 41, 44, 47, 48]\nv += [54, 56, 59, 62, 65, 67, 68, 70]\nv += [76, 78, 80, 82, 86, 88, 89, 91]\n\n#v += [96, 98, 99, 104] #relates to groups, no NAN \nv += [107, 108, 111, 115, 117, 120, 121, 123] # maybe group, no NAN\nv += [124, 127, 129, 130, 136] # relates to groups, no NAN\n\n# LOTS OF NAN BELOW\nv += [138, 139, 142, 147, 156, 162] #b1\nv += [165, 160, 166] #b1\nv += [178, 176, 173, 182] #b2\nv += [187, 203, 205, 207, 215] #b2\nv += [169, 171, 175, 180, 185, 188, 198, 210, 209] #b2\nv += [218, 223, 224, 226, 228, 229, 235] #b3\nv += [240, 258, 257, 253, 252, 260, 261] #b3\nv += [264, 266, 267, 274, 277] #b3\nv += [220, 221, 234, 238, 250, 271] #b3\n\nv += [294, 284, 285, 286, 291, 297] # relates to grous, no NAN\nv += [303, 305, 307, 309, 310, 320] # relates to groups, no NAN\nv += [281, 283, 289, 296, 301, 314] # relates to groups, no NAN\n#v += [332, 325, 335, 338] # b4 lots NAN\n\nuse_Vcols = ['V'+str(x) for x in v]\nVcols = ['V'+str(x) for x in range(1,340)]\ndrop_Vcols = list(set(Vcols) - set(use_Vcols))\n\ntrain.drop(drop_Vcols, axis=1, inplace=True)\ntest.drop(drop_Vcols, axis=1, inplace=True)","8d87788d":"print(f'Train dataset has {train.shape[0]} rows and {train.shape[1]} columns.')\nprint(f'Test dataset has {test.shape[0]} rows and {test.shape[1]} columns.')","919c2598":"print(f'There are {train.isnull().any().sum()} columns in train dataset with missing values.')","c3077072":"one_value_cols = [col for col in train.columns if train[col].nunique() <= 1]\none_value_cols_test = [col for col in test.columns if test[col].nunique() <= 1]\none_value_cols == one_value_cols_test","8f128a89":"print(f'There are {len(one_value_cols)} columns in train dataset with one unique value.')\nprint(f'There are {len(one_value_cols_test)} columns in test dataset with one unique value.')","c4a676e1":"test[one_value_cols_test].describe()","6f7a1c94":"train[['id_01', 'id_02', 'id_03', 'id_04', 'id_05', 'id_06', 'id_07', 'id_08', 'id_09', 'id_10', 'id_11']].describe(include='all')","2a45b370":"plt.hist(train['id_01'], bins=77);\nplt.title('Distribution of id_01 variable');","db2c977a":"s1 = train['id_03'].value_counts(dropna=False, normalize=True).head()\ns2 = train['id_04'].value_counts(dropna=False, normalize=True).head()\ns3 = train['id_05'].value_counts(dropna=False, normalize=True).head()\ns4 = train['id_06'].value_counts(dropna=False, normalize=True).head()\ns5 = train['id_09'].value_counts(dropna=False, normalize=True).head()\ns6 = train['id_10'].value_counts(dropna=False, normalize=True).head()\nprint(pd.concat([s1, s2, s3, s4, s5, s6], axis = 1))","41909610":"train['id_11'].value_counts(dropna=False, normalize=True).head()","d4769932":"plt.hist(train['id_07']);\nplt.title('Distribution of id_07 variable');","7901d79e":"plt.hist(train['id_02']);\nplt.title('Distribution of id_02 variable');","d9792f23":"plt.hist(train['id_08']);\nplt.title('Distribution of id_08 variable');","634095d2":"train[['id_12', 'id_13', 'id_14', 'id_15', 'id_16', 'id_17', 'id_18',\n       'id_19', 'id_20', 'id_21', 'id_22', 'id_23', 'id_24', 'id_25',\n       'id_26', 'id_27', 'id_28', 'id_29', 'id_30', 'id_31', 'id_32',\n       'id_33', 'id_34', 'id_35', 'id_36', 'id_37', 'id_38', 'DeviceType', 'DeviceInfo']].describe(include='all')","6c943715":"charts = {}\nfor i in ['id_12', 'id_15', 'id_16', 'id_28', 'id_29', 'id_30', 'id_31', 'id_32', 'id_33', 'id_34', 'id_35', 'id_36', 'id_37', 'id_38']:\n    feature_count = train[i].value_counts(dropna=False).reset_index().rename(columns={i: 'count', 'index': i})\n    chart = alt.Chart(feature_count).mark_bar().encode(\n                y=alt.Y(f\"{i}:N\", axis=alt.Axis(title=i)),\n                x=alt.X('count:Q', axis=alt.Axis(title='Count')),\n                tooltip=[i, 'count']\n            ).properties(title=f\"Counts of {i}\", width=400)\n    charts[i] = chart                         \n    \nrender((charts['id_12'] | charts['id_15'] | charts['id_16']) & (charts['id_28'] | charts['id_29'] | charts['id_32']) & (charts['id_34'] | charts['id_35'] | charts['id_36']) & (charts['id_37'] | charts['id_38']))","f81c833f":"charts = {}\nfor i in ['id_30', 'id_31', 'id_33', 'DeviceType', 'DeviceInfo']:\n    feature_count = train[i].value_counts(dropna=False)[:40].reset_index().rename(columns={i: 'count', 'index': i})\n    chart = alt.Chart(feature_count).mark_bar().encode(\n                x=alt.X(f\"{i}:N\", axis=alt.Axis(title=i)),\n                y=alt.Y('count:Q', axis=alt.Axis(title='Count')),\n                tooltip=[i, 'count']\n            ).properties(title=f\"Counts of {i}\", width=800)\n    charts[i] = chart\n    \nrender(charts['id_30'] & charts['id_31'] & charts['id_33'] & charts['DeviceType'] & charts['DeviceInfo'])","0e7d6f96":"plt.hist(train['TransactionDT'], label='train');\nplt.hist(test['TransactionDT'], label='test');\nplt.legend();\nplt.title('Distribution of transactiond dates');","7f14b754":"# PLOT ORIGINAL D\nplt.figure(figsize=(15,5))\nplt.scatter(train.TransactionDT,train.D15)\nplt.title('Original D15')\nplt.xlabel('Time')\nplt.ylabel('D15')\nplt.show()","33b9e446":"# NORMALIZE D COLUMNS\nfor i in range(1,16):\n    if i in [1,2,3,5,9]: continue\n    train['D'+str(i)] =  train['D'+str(i)] - train.TransactionDT\/np.float32(24*60*60)\n    test['D'+str(i)] = test['D'+str(i)] - test.TransactionDT\/np.float32(24*60*60) ","bed3ed43":"# PLOT TRANSFORMED D\nplt.figure(figsize=(15,5))\nplt.scatter(train.TransactionDT,train.D15)\nplt.title('Transformed D15')\nplt.xlabel('Time')\nplt.ylabel('D15n')\nplt.show()","8c4f5ce7":"plt.hist(train['TransactionAmt'], label='train');\nplt.hist(test['TransactionAmt'], label='test');\nplt.legend();\nplt.title('Distribution of transaction amount');","04b3dd67":"charts = {}\nfor i in ['ProductCD', 'card4', 'card6', 'M4', 'M1', 'M2', 'M3', 'M5', 'M6', 'M7', 'M8', 'M9']:\n    feature_count = train[i].value_counts(dropna=False).reset_index().rename(columns={i: 'count', 'index': i})\n    chart = alt.Chart(feature_count).mark_bar().encode(\n                y=alt.Y(f\"{i}:N\", axis=alt.Axis(title=i)),\n                x=alt.X('count:Q', axis=alt.Axis(title='Count')),\n                tooltip=[i, 'count']\n            ).properties(title=f\"Counts of {i}\", width=400)\n    charts[i] = chart                         \n    \nrender((charts['ProductCD'] | charts['card4']) & (charts['card6'] | charts['M4']) & (charts['card6'] | charts['M4']) & (charts['M1'] | charts['M2']) & (charts['M3'] | charts['M5']) & (charts['M6'] | charts['M7']) & (charts['M8'] | charts['M9']))","bc540059":"charts = {}\nfor i in ['P_emaildomain', 'R_emaildomain', 'card1', 'card2', 'card3',  'card5', 'addr1', 'addr2']:\n    feature_count = train[i].value_counts(dropna=False).reset_index()[:40].rename(columns={i: 'count', 'index': i})\n    chart = alt.Chart(feature_count).mark_bar().encode(\n                x=alt.X(f\"{i}:N\", axis=alt.Axis(title=i)),\n                y=alt.Y('count:Q', axis=alt.Axis(title='Count')),\n                tooltip=[i, 'count']\n            ).properties(title=f\"Counts of {i}\", width=600)\n    charts[i] = chart\n    \nrender((charts['P_emaildomain'] | charts['R_emaildomain']) & (charts['card1'] | charts['card2']) & (charts['card3'] | charts['card5']) & (charts['addr1'] | charts['addr2']))","69684c9a":"emails = {'gmail': 'google', 'att.net': 'att', 'twc.com': 'spectrum', \n          'scranton.edu': 'other', 'optonline.net': 'other', 'hotmail.co.uk': 'microsoft',\n          'comcast.net': 'other', 'yahoo.com.mx': 'yahoo', 'yahoo.fr': 'yahoo',\n          'yahoo.es': 'yahoo', 'charter.net': 'spectrum', 'live.com': 'microsoft', \n          'aim.com': 'aol', 'hotmail.de': 'microsoft', 'centurylink.net': 'centurylink',\n          'gmail.com': 'google', 'me.com': 'apple', 'earthlink.net': 'other', 'gmx.de': 'other',\n          'web.de': 'other', 'cfl.rr.com': 'other', 'hotmail.com': 'microsoft', \n          'protonmail.com': 'other', 'hotmail.fr': 'microsoft', 'windstream.net': 'other', \n          'outlook.es': 'microsoft', 'yahoo.co.jp': 'yahoo', 'yahoo.de': 'yahoo',\n          'servicios-ta.com': 'other', 'netzero.net': 'other', 'suddenlink.net': 'other',\n          'roadrunner.com': 'other', 'sc.rr.com': 'other', 'live.fr': 'microsoft',\n          'verizon.net': 'yahoo', 'msn.com': 'microsoft', 'q.com': 'centurylink', \n          'prodigy.net.mx': 'att', 'frontier.com': 'yahoo', 'anonymous.com': 'other', \n          'rocketmail.com': 'yahoo', 'sbcglobal.net': 'att', 'frontiernet.net': 'yahoo', \n          'ymail.com': 'yahoo', 'outlook.com': 'microsoft', 'mail.com': 'other', \n          'bellsouth.net': 'other', 'embarqmail.com': 'centurylink', 'cableone.net': 'other', \n          'hotmail.es': 'microsoft', 'mac.com': 'apple', 'yahoo.co.uk': 'yahoo', 'netzero.com': 'other', \n          'yahoo.com': 'yahoo', 'live.com.mx': 'microsoft', 'ptd.net': 'other', 'cox.net': 'other',\n          'aol.com': 'aol', 'juno.com': 'other', 'icloud.com': 'apple'}\n\nus_emails = ['gmail', 'net', 'edu']\n\n# https:\/\/www.kaggle.com\/c\/ieee-fraud-detection\/discussion\/100499#latest-579654\nfor c in ['P_emaildomain', 'R_emaildomain']:\n    train[c + '_bin'] = train[c].map(emails)\n    test[c + '_bin'] = test[c].map(emails)\n    \n    train[c + '_suffix'] = train[c].map(lambda x: str(x).split('.')[-1])\n    test[c + '_suffix'] = test[c].map(lambda x: str(x).split('.')[-1])\n    \n    train[c + '_suffix'] = train[c + '_suffix'].map(lambda x: x if str(x) not in us_emails else 'us')\n    test[c + '_suffix'] = test[c + '_suffix'].map(lambda x: x if str(x) not in us_emails else 'us')","f788f473":"# https:\/\/www.kaggle.com\/cdeotte\/xgb-fraud-with-magic-0-9600\/notebook#Load-Data\n# FREQUENCY ENCODE TOGETHER\ndef encode_FE(df1, df2, cols):\n    for col in cols:\n        df = pd.concat([df1[col],df2[col]])\n        vc = df.value_counts(dropna=True, normalize=True).to_dict()\n        vc[-1] = -1\n        nm = col+'_FE'\n        df1[nm] = df1[col].map(vc)\n        df1[nm] = df1[nm].astype('float32')\n        df2[nm] = df2[col].map(vc)\n        df2[nm] = df2[nm].astype('float32')\n        print(nm,', ',end='')\n        \n# LABEL ENCODE\ndef encode_LE(col,train=train,test=test,verbose=True):\n    df_comb = pd.concat([train[col],test[col]],axis=0)\n    df_comb,_ = df_comb.factorize(sort=True)\n    nm = col\n    if df_comb.max()>32000: \n        train[nm] = df_comb[:len(train)].astype('int32')\n        test[nm] = df_comb[len(train):].astype('int32')\n    else:\n        train[nm] = df_comb[:len(train)].astype('int16')\n        test[nm] = df_comb[len(train):].astype('int16')\n    del df_comb; x=gc.collect()\n    if verbose: print(nm,', ',end='')\n        \n# GROUP AGGREGATION MEAN AND STD\n# https:\/\/www.kaggle.com\/kyakovlev\/ieee-fe-with-some-eda\ndef encode_AG(main_columns, uids, aggregations=['mean'], train_df=train, test_df=test, \n              fillna=True, usena=False):\n    # AGGREGATION OF MAIN WITH UID FOR GIVEN STATISTICS\n    for main_column in main_columns:  \n        for col in uids:\n            for agg_type in aggregations:\n                new_col_name = main_column+'_'+col+'_'+agg_type\n                temp_df = pd.concat([train_df[[col, main_column]], test_df[[col,main_column]]])\n                if usena: temp_df.loc[temp_df[main_column]==-1,main_column] = np.nan\n                temp_df = temp_df.groupby([col])[main_column].agg([agg_type]).reset_index().rename(\n                                                        columns={agg_type: new_col_name})\n\n                temp_df.index = list(temp_df[col])\n                temp_df = temp_df[new_col_name].to_dict()   \n\n                train_df[new_col_name] = train_df[col].map(temp_df).astype('float32')\n                test_df[new_col_name]  = test_df[col].map(temp_df).astype('float32')\n                \n                if fillna:\n                    train_df[new_col_name].fillna(-1,inplace=True)\n                    test_df[new_col_name].fillna(-1,inplace=True)\n                \n                print(\"'\"+new_col_name+\"'\",', ',end='')\n                \n# COMBINE FEATURES\ndef encode_CB(col1,col2,df1=train,df2=test):\n    nm = col1+'_'+col2\n    df1[nm] = df1[col1].astype(str)+'_'+df1[col2].astype(str)\n    df2[nm] = df2[col1].astype(str)+'_'+df2[col2].astype(str) \n    encode_LE(nm,verbose=False)\n    print(nm,', ',end='')\n    \n# GROUP AGGREGATION NUNIQUE\ndef encode_AG2(main_columns, uids, train_df=train, test_df=test):\n    for main_column in main_columns:  \n        for col in uids:\n            comb = pd.concat([train_df[[col]+[main_column]],test_df[[col]+[main_column]]],axis=0)\n            mp = comb.groupby(col)[main_column].agg(['nunique'])['nunique'].to_dict()\n            train_df[col+'_'+main_column+'_ct'] = train_df[col].map(mp).astype('float32')\n            test_df[col+'_'+main_column+'_ct'] = test_df[col].map(mp).astype('float32')\n            print(col+'_'+main_column+'_ct, ',end='')","98d45971":"train['Trans_min_mean'] = train['TransactionAmt'] - train['TransactionAmt'].mean()\ntrain['Trans_min_std'] = train['Trans_min_mean'] \/ train['TransactionAmt'].std()\ntest['Trans_min_mean'] = test['TransactionAmt'] - test['TransactionAmt'].mean()\ntest['Trans_min_std'] = test['Trans_min_mean'] \/ test['TransactionAmt'].std()","56ba09e8":"train['TransactionAmt_log'] = np.log(train['TransactionAmt'])\ntest['TransactionAmt_log'] = np.log(test['TransactionAmt'])","a6044c99":"train['TransactionAmt_to_mean_card1'] = train['TransactionAmt'] \/ train.groupby(['card1'])['TransactionAmt'].transform('mean')\ntrain['TransactionAmt_to_mean_card4'] = train['TransactionAmt'] \/ train.groupby(['card4'])['TransactionAmt'].transform('mean')\ntrain['TransactionAmt_to_std_card1'] = train['TransactionAmt'] \/ train.groupby(['card1'])['TransactionAmt'].transform('std')\ntrain['TransactionAmt_to_std_card4'] = train['TransactionAmt'] \/ train.groupby(['card4'])['TransactionAmt'].transform('std')\n\ntest['TransactionAmt_to_mean_card1'] = test['TransactionAmt'] \/ test.groupby(['card1'])['TransactionAmt'].transform('mean')\ntest['TransactionAmt_to_mean_card4'] = test['TransactionAmt'] \/ test.groupby(['card4'])['TransactionAmt'].transform('mean')\ntest['TransactionAmt_to_std_card1'] = test['TransactionAmt'] \/ test.groupby(['card1'])['TransactionAmt'].transform('std')\ntest['TransactionAmt_to_std_card4'] = test['TransactionAmt'] \/ test.groupby(['card4'])['TransactionAmt'].transform('std')\n\n# train['id_02_to_mean_card1'] = train['id_02'] \/ train.groupby(['card1'])['id_02'].transform('mean')\n# train['id_02_to_mean_card4'] = train['id_02'] \/ train.groupby(['card4'])['id_02'].transform('mean')\n# train['id_02_to_std_card1'] = train['id_02'] \/ train.groupby(['card1'])['id_02'].transform('std')\n# train['id_02_to_std_card4'] = train['id_02'] \/ train.groupby(['card4'])['id_02'].transform('std')\n\n# test['id_02_to_mean_card1'] = test['id_02'] \/ test.groupby(['card1'])['id_02'].transform('mean')\n# test['id_02_to_mean_card4'] = test['id_02'] \/ test.groupby(['card4'])['id_02'].transform('mean')\n# test['id_02_to_std_card1'] = test['id_02'] \/ test.groupby(['card1'])['id_02'].transform('std')\n# test['id_02_to_std_card4'] = test['id_02'] \/ test.groupby(['card4'])['id_02'].transform('std')\n\ntrain['D15_to_mean_card1'] = train['D15'] \/ train.groupby(['card1'])['D15'].transform('mean')\ntrain['D15_to_mean_card4'] = train['D15'] \/ train.groupby(['card4'])['D15'].transform('mean')\ntrain['D15_to_std_card1'] = train['D15'] \/ train.groupby(['card1'])['D15'].transform('std')\ntrain['D15_to_std_card4'] = train['D15'] \/ train.groupby(['card4'])['D15'].transform('std')\n\ntest['D15_to_mean_card1'] = test['D15'] \/ test.groupby(['card1'])['D15'].transform('mean')\ntest['D15_to_mean_card4'] = test['D15'] \/ test.groupby(['card4'])['D15'].transform('mean')\ntest['D15_to_std_card1'] = test['D15'] \/ test.groupby(['card1'])['D15'].transform('std')\ntest['D15_to_std_card4'] = test['D15'] \/ test.groupby(['card4'])['D15'].transform('std')\n\ntrain['D15_to_mean_addr1'] = train['D15'] \/ train.groupby(['addr1'])['D15'].transform('mean')\ntrain['D15_to_mean_addr2'] = train['D15'] \/ train.groupby(['addr2'])['D15'].transform('mean')\ntrain['D15_to_std_addr1'] = train['D15'] \/ train.groupby(['addr1'])['D15'].transform('std')\ntrain['D15_to_std_addr2'] = train['D15'] \/ train.groupby(['addr2'])['D15'].transform('std')\n\ntest['D15_to_mean_addr1'] = test['D15'] \/ test.groupby(['addr1'])['D15'].transform('mean')\ntest['D15_to_mean_addr2'] = test['D15'] \/ test.groupby(['addr2'])['D15'].transform('mean')\ntest['D15_to_std_addr1'] = test['D15'] \/ test.groupby(['addr1'])['D15'].transform('std')\ntest['D15_to_std_addr2'] = test['D15'] \/ test.groupby(['addr2'])['D15'].transform('std')","96a7af4a":"train[['P_emaildomain_1', 'P_emaildomain_2', 'P_emaildomain_3']] = train['P_emaildomain'].str.split('.', expand=True)\ntrain[['R_emaildomain_1', 'R_emaildomain_2', 'R_emaildomain_3']] = train['R_emaildomain'].str.split('.', expand=True)\ntest[['P_emaildomain_1', 'P_emaildomain_2', 'P_emaildomain_3']] = test['P_emaildomain'].str.split('.', expand=True)\ntest[['R_emaildomain_1', 'R_emaildomain_2', 'R_emaildomain_3']] = test['R_emaildomain'].str.split('.', expand=True)","c5cf8e78":"# https:\/\/www.kaggle.com\/cdeotte\/xgb-fraud-with-magic-0-9600\/notebook#Load-Data\n\n%time\n# TRANSACTION AMT CENTS\ntrain['cents'] = (train['TransactionAmt'] - np.floor(train['TransactionAmt'])).astype('float32')\ntest['cents'] = (test['TransactionAmt'] - np.floor(test['TransactionAmt'])).astype('float32')\nprint('cents, ', end='')\n# FREQUENCY ENCODE: ADDR1, CARD1, CARD2, CARD3, P_EMAILDOMAIN\nencode_FE(train,test,['addr1','card1','card2','card3','P_emaildomain'])\n# COMBINE COLUMNS CARD1+ADDR1, CARD1+ADDR1+P_EMAILDOMAIN\nencode_CB('card1','addr1')\nencode_CB('card1_addr1','P_emaildomain')\n# FREQUENCY ENOCDE\nencode_FE(train,test,['card1_addr1','card1_addr1_P_emaildomain'])\n# GROUP AGGREGATE\nencode_AG(['TransactionAmt','D9','D11'],['card1','card1_addr1','card1_addr1_P_emaildomain'],['mean','std'],usena=True)","fe6dc69d":"many_null_cols = [col for col in train.columns if train[col].isnull().sum() \/ train.shape[0] > 0.9]\nmany_null_cols_test = [col for col in test.columns if test[col].isnull().sum() \/ test.shape[0] > 0.9]","93b01fe9":"big_top_value_cols = [col for col in train.columns if train[col].value_counts(dropna=False, normalize=True).values[0] > 0.9]\nbig_top_value_cols_test = [col for col in test.columns if test[col].value_counts(dropna=False, normalize=True).values[0] > 0.9]","998a2b72":"cols_to_drop = list(set(many_null_cols + many_null_cols_test + big_top_value_cols + big_top_value_cols_test + one_value_cols+ one_value_cols_test))\ncols_to_drop.remove('isFraud')\nlen(cols_to_drop)","a7702e0b":"train = train.drop(cols_to_drop, axis=1)\ntest = test.drop(cols_to_drop, axis=1)","afcb4080":"# by https:\/\/www.kaggle.com\/dimartinot\ndef clean_inf_nan(df):\n    return df.replace([np.inf, -np.inf], np.nan)   \n\n# Cleaning infinite values to NaN\ntrain = clean_inf_nan(train)\ntest = clean_inf_nan(test)","9bc5864e":"# All Categorial Columns\nidentity_cat_cols = ['id_12', 'id_13', 'id_14', 'id_15', 'id_16', 'id_17', 'id_18',\n                     'id_19', 'id_20', 'id_21', 'id_22', 'id_23', 'id_24', 'id_25',\n                     'id_26', 'id_27', 'id_28', 'id_29', 'id_30', 'id_31', 'id_32',\n                     'id_33', 'id_34', 'id_35', 'id_36', 'id_37', 'id_38', 'DeviceType', 'DeviceInfo']\ntransaction_cat_cols = ['ProductCD', 'card1', 'card2', 'card3', 'card4', 'card5', 'card6', 'addr1', 'addr2',\n                        'P_emaildomain_1', 'P_emaildomain_2', 'P_emaildomain_3',\n                        'R_emaildomain_1', 'R_emaildomain_2', 'R_emaildomain_3','P_emaildomain', 'R_emaildomain',\n                        'P_emaildomain_bin', 'R_emaildomain_bin', 'P_emaildomain_suffix', 'R_emaildomain_suffix',\n                        'M1', 'M2', 'M3', 'M4', 'M5', 'M6', 'M7', 'M8', 'M9']\n\ncat_cols = identity_cat_cols + transaction_cat_cols\ncat_cols = list(set(cat_cols) & set(train.columns)) # because maybe we have dropped some categorical columns\nencoded_cols = ['addr1','card1','card2','card3']\ncat_cols = list(set(cat_cols) - set(encoded_cols))\n\ncommon_cat_cols = [col for col in test.columns if col in cat_cols]\n\ncat_cols == common_cat_cols","f42d0a62":"print(cat_cols)","9229a33e":"print(common_cat_cols)","5ae46b0d":"cat_train = train[common_cat_cols]\nnum_train = train.drop(cat_cols, axis=1)\n\ncat_test = test[common_cat_cols]\nnum_test = test.drop(common_cat_cols, axis=1)\n\ncat_train.columns == cat_test.columns","6e51e3ef":"# Get the number of unique entries in each column with categorical data\ncat_nunique = list(map(lambda col: cat_train[col].nunique(), common_cat_cols))\nd = dict(zip(common_cat_cols, cat_nunique))\n\n# Print number of unique entries by column, in ascending order\nsorted(d.items(), key=lambda x:x[1])","340a2be3":"# Change the categorical columns' types to Category\ncat_train = cat_train.astype('category')\ncat_test = cat_test.astype('category')\n\nprint(cat_train.dtypes)","70c963a2":"# Columns that will be label-encoded\nlow_cardinality_cols = [col for col in common_cat_cols if cat_train[col].nunique() < 10] # 10 is a number chosen by discretion\n\n# Columns that will be target encoded (or dropped eventally)\nhigh_cardinality_cols = list(set(common_cat_cols) - set(low_cardinality_cols))\n\nprint('Categorical columns that will be label encoded: ', low_cardinality_cols)\nprint('\\nCategorical columns that will be target encoded (or dropped eventally): ', high_cardinality_cols)","531a1f7b":"LE_train = cat_train[low_cardinality_cols]\nLE_test = cat_test[low_cardinality_cols]\n\nprint(LE_train.describe())\nprint(\"---------------\")\nprint(LE_test.describe())","696a31ee":"# label encode the data\nfrom sklearn.preprocessing import LabelEncoder\n\nfor col in low_cardinality_cols:\n    LEncoder = LabelEncoder()\n    LEncoder.fit(list(LE_train[col].astype(str).values) + list(LE_test[col].astype(str).values))\n    LE_train[col] = LEncoder.transform(list(LE_train[col].astype(str).values))\n    LE_test[col] = LEncoder.transform(list(LE_test[col].astype(str).values))\n    \ntrain = num_train.merge(LE_train, how='left', left_index=True, right_index=True)\ntest = num_test.merge(LE_test, how='left', left_index=True, right_index=True)\n\nprint(train[low_cardinality_cols].describe())\nprint(\"---------------\")\nprint(test[low_cardinality_cols].describe())","39d217d4":"# high_cardinality_cols","7c921e88":"# cat_train[high_cardinality_cols].shape","0b6a33b7":"# print(cat_train[high_cardinality_cols].describe())\n# print(\"---------------\")\n# print(cat_test[high_cardinality_cols].describe())","20b96208":"# # target encode the data\n# from category_encoders import MEstimateEncoder\n# TE_train = cat_train[high_cardinality_cols] \n# TE_train = TE_train.merge(train['isFraud'], how='left', left_index=True, right_index=True)\n# TE_test = cat_test[high_cardinality_cols]\n\n# # encoding split\n# X_encode = TE_train.sample(frac=0.3, random_state=0)\n# y_encode = X_encode.pop(\"isFraud\")\n\n# # training split\n# X_pretrain = TE_train.drop(X_encode.index)\n# y_train = X_pretrain.pop(\"isFraud\")\n\n# #apply M-Estimate encoding (the choice of m is based on our previous cardinality investigation)\n# for col in high_cardinality_cols:\n#     if TE_train[col].nunique() < 100:\n#         m = 0.5\n#     elif TE_train[col].nunique() < 550:\n#         m = 2.5\n#     else:\n#         m = 5\n#     encoder = MEstimateEncoder(cols=[col], m=m)\n#     encoder.fit(X_encode, y_encode)\n#     X_pretrain = encoder.transform(X_pretrain)\n#     TE_test = encoder.transform(TE_test)","15fafc08":"# print(X_pretrain[high_cardinality_cols].describe())\n# print(\"---------------\")\n# print(TE_test[high_cardinality_cols].describe())","47cb0b4b":"# X_pretrain.shape","bcd9f2cc":"# train = X_pretrain.merge(train, how='left', left_index=True, right_index=True)\n# test = TE_test.merge(test, how='left', left_index=True, right_index=True)\n\n# print(train[common_cat_cols].describe())\n# print(\"---------------\")\n# print(test[common_cat_cols].describe())","018ab09d":"del num_train, num_test, LE_train, LE_test#, TE_train, TE_test","3f464b77":"train.shape","d76c3a4e":"test.shape","50cc493f":"cols = list(train.columns)\n\n# Remove 7 D columns that are mostly NAN\nfor c in list(set(['D6','D7','D8','D9','D12','D13','D14']) & set(cols)):\n    cols.remove(c)\n    \n# FAILED TIME CONSISTENCY TEST\nfor c in list(set(['C3','M5','id_08','id_33']) & set(cols)):\n    cols.remove(c)\nfor c in list(set(['card4','id_07','id_14','id_21','id_30','id_32','id_34']) & set(cols)):\n    cols.remove(c)\nfor c in list(set(['id_'+str(x) for x in range(22,28)]) & set(cols)):\n    cols.remove(c)","508d808c":"print('NOW USING THE FOLLOWING',len(cols),'FEATURES.')\nnp.array(cols)","e55e4009":"final_train = train[cols].copy()\nfinal_test = test[list(set(cols) - set(['isFraud']))].copy()\n\nprint(f'Our Final Train dataset has {final_train.shape[0]} rows and {final_train.shape[1]} columns.')\nprint(f'Our Final Test dataset has {final_test.shape[0]} rows and {final_test.shape[1]} columns.')","2ed4dfd8":"del train, test","e0dc67e5":"X_train = final_train.copy()\ny_train = X_train.pop('isFraud')\nX_test = final_test.copy()\nX_train,X_test = X_train.align(other=X_test,join='left', axis=1)\n\nprint(X_train.describe())\nprint(X_test.describe())","2bf1ed8f":"idxT = X_train.index[:3*len(X_train)\/\/4]\nidxV = X_train.index[3*len(X_train)\/\/4:]","a4482558":"y_train.value_counts()\/y_train.shape[0]","775499fe":"import xgboost as xgb\nprint(\"XGBoost version:\", xgb.__version__)\n\nclf = xgb.XGBClassifier( \n    n_estimators=2000,\n    max_depth=12, \n    learning_rate=0.02, \n    subsample=0.8,\n    colsample_bytree=0.4, \n    #missing=-1, \n    eval_metric='auc',\n    # USE CPU\n    #nthread=4,\n    #tree_method='hist' \n    # USE GPU\n    tree_method='gpu_hist' \n)\n\nh = clf.fit(X_train.loc[idxT,cols], y_train[idxT], \n            early_stopping_rounds=100,\n            eval_set=[(X_train.loc[idxV,cols],y_train[idxV])],\n            verbose=50)\n            ","1113e87e":"feature_imp = pd.DataFrame(sorted(zip(clf.feature_importances_,cols)), columns=['Value','Feature'])\nplt.figure(figsize=(20, 10))\nsns.barplot(x=\"Value\", y=\"Feature\", data=feature_imp.sort_values(by=\"Value\", ascending=False).iloc[:50])\nplt.title('XGB Most Important Features')\nplt.tight_layout()\nplt.show()\ndel clf, h; x=gc.collect()","6dfe7fbb":"# X_train = final_train.copy()\n# y_train = X_train.pop('isFraud')\n# X_test = final_test.copy()\n# X_train,X_test = X_train.align(other=X_test,join='left', axis=1)","ae33172c":"# from sklearn.model_selection import KFold,TimeSeriesSplit\n# from sklearn.metrics import roc_auc_score\n# from xgboost import plot_importance\n# from sklearn.metrics import make_scorer\n\n# ## Hyperopt modules\n# from hyperopt import fmin, hp, tpe, Trials, space_eval, STATUS_OK, STATUS_RUNNING\n# from functools import partial\n\n# import time\n# def objective(params):\n#     time1 = time.time()\n#     params = {\n#         'max_depth': int(params['max_depth']),\n#         'gamma': \"{:.3f}\".format(params['gamma']),\n#         'subsample': \"{:.2f}\".format(params['subsample']),\n#         'reg_alpha': \"{:.3f}\".format(params['reg_alpha']),\n#         'reg_lambda': \"{:.3f}\".format(params['reg_lambda']),\n#         'learning_rate': \"{:.3f}\".format(params['learning_rate']),\n#         'num_leaves': '{:.3f}'.format(params['num_leaves']),\n#         'colsample_bytree': '{:.3f}'.format(params['colsample_bytree']),\n#         'min_child_samples': '{:.3f}'.format(params['min_child_samples']),\n#         'feature_fraction': '{:.3f}'.format(params['feature_fraction']),\n#         'bagging_fraction': '{:.3f}'.format(params['bagging_fraction'])\n#     }\n\n#     print(\"\\n############## New Run ################\")\n#     print(f\"params = {params}\")\n#     FOLDS = 7\n#     count=1\n#     skf = StratifiedKFold(n_splits=FOLDS, shuffle=True, random_state=42)\n\n#     tss = TimeSeriesSplit(n_splits=FOLDS)\n#     y_preds = np.zeros(sub.shape[0])\n#     y_oof = np.zeros(X_train.shape[0])\n#     score_mean = 0\n#     for tr_idx, val_idx in tss.split(X_train, y_train):\n#         clf = xgb.XGBClassifier(\n#             n_estimators=2000, random_state=4, verbose=True, \n#             tree_method='gpu_hist', early_stopping_rounds=100,\n#             **params\n#         )\n\n#         X_tr, X_vl = X_train.iloc[tr_idx, :], X_train.iloc[val_idx, :]\n#         y_tr, y_vl = y_train.iloc[tr_idx], y_train.iloc[val_idx]\n        \n#         clf.fit(X_tr, y_tr)\n#         #y_pred_train = clf.predict_proba(X_vl)[:,1]\n#         #print(y_pred_train)\n#         score = make_scorer(roc_auc_score, needs_proba=True)(clf, X_vl, y_vl)\n#         # plt.show()\n#         score_mean += score\n#         print(f'{count} CV - score: {round(score, 4)}')\n#         count += 1\n#     time2 = time.time() - time1\n#     print(f\"Total Time Run: {round(time2 \/ 60,2)}\")\n#     gc.collect()\n#     print(f'Mean ROC_AUC: {score_mean \/ FOLDS}')\n#     del X_tr, X_vl, y_tr, y_vl, clf, score\n#     return -(score_mean \/ FOLDS)\n\n\n# space = {\n#     # The maximum depth of a tree, same as GBM.\n#     # Used to control over-fitting as higher depth will allow model \n#     # to learn relations very specific to a particular sample.\n#     # Should be tuned using CV.\n#     # Typical values: 3-10\n#     'max_depth': hp.quniform('max_depth', 7, 23, 1),\n    \n#     # reg_alpha: L1 regularization term. L1 regularization encourages sparsity \n#     # (meaning pulling weights to 0). It can be more useful when the objective\n#     # is logistic regression since you might need help with feature selection.\n#     'reg_alpha':  hp.uniform('reg_alpha', 0.01, 0.4),\n    \n#     # reg_lambda: L2 regularization term. L2 encourages smaller weights, this\n#     # approach can be more useful in tree-models where zeroing \n#     # features might not make much sense.\n#     'reg_lambda': hp.uniform('reg_lambda', 0.01, .4),\n    \n#     # eta: Analogous to learning rate in GBM\n#     # Makes the model more robust by shrinking the weights on each step\n#     # Typical final values to be used: 0.01-0.2\n#     'learning_rate': hp.uniform('learning_rate', 0.01, 0.2),\n    \n#     # colsample_bytree: Similar to max_features in GBM. Denotes the \n#     # fraction of columns to be randomly samples for each tree.\n#     # Typical values: 0.5-1\n#     'colsample_bytree': hp.uniform('colsample_bytree', 0.3, .9),\n    \n#     # A node is split only when the resulting split gives a positive\n#     # reduction in the loss function. Gamma specifies the \n#     # minimum loss reduction required to make a split.\n#     # Makes the algorithm conservative. The values can vary depending on the loss function and should be tuned.\n#     'gamma': hp.uniform('gamma', 0.01, .7),\n    \n#     # more increases accuracy, but may lead to overfitting.\n#     # num_leaves: the number of leaf nodes to use. Having a large number \n#     # of leaves will improve accuracy, but will also lead to overfitting.\n#     'num_leaves': hp.choice('num_leaves', list(range(20, 250, 10))),\n    \n#     # specifies the minimum samples per leaf node.\n#     # the minimum number of samples (data) to group into a leaf. \n#     # The parameter can greatly assist with overfitting: larger sample\n#     # sizes per leaf will reduce overfitting (but may lead to under-fitting).\n#     'min_child_samples': hp.choice('min_child_samples', list(range(100, 250, 10))),\n    \n#     # subsample: represents a fraction of the rows (observations) to be \n#     # considered when building each subtree. Tianqi Chen and Carlos Guestrin\n#     # in their paper A Scalable Tree Boosting System recommend \n#     'subsample': hp.choice('subsample', [0.2, 0.4, 0.5, 0.6, 0.7, .8, .9]),\n    \n#     # randomly select a fraction of the features.\n#     # feature_fraction: controls the subsampling of features used\n#     # for training (as opposed to subsampling the actual training data in \n#     # the case of bagging). Smaller fractions reduce overfitting.\n#     'feature_fraction': hp.uniform('feature_fraction', 0.4, .8),\n    \n#     # randomly bag or subsample training data.\n#     'bagging_fraction': hp.uniform('bagging_fraction', 0.4, .9)\n    \n#     # bagging_fraction and bagging_freq: enables bagging (subsampling) \n#     # of the training data. Both values need to be set for bagging to be used.\n#     # The frequency controls how often (iteration) bagging is used. Smaller\n#     # fractions and frequencies reduce overfitting.\n# }","afc3e2db":"# # Set algoritm parameters\n# best = fmin(fn=objective,\n#             space=space,\n#             algo=tpe.suggest,\n#             max_evals=25)\n\n# # Print best parameters\n# best_params = space_eval(space, best)","2ce6737d":"# best_params['max_depth'] = int(best_params['max_depth'])","fa682602":"# best_params","a34c6acd":"best_params = {'bagging_fraction': 0.5818772519688797,\n 'colsample_bytree': 0.3035307099891744,\n 'feature_fraction': 0.795967379488282,\n 'gamma': 0.6896677451866189,\n 'learning_rate': 0.011336192527320772,\n 'max_depth': 20,\n 'min_child_samples': 140,\n 'num_leaves': 230,\n 'reg_alpha': 0.06035695642758,\n 'reg_lambda': 0.012734543098346575,\n 'subsample': 0.7}","f49bb3b7":"X_train = final_train.copy()\ny_train = X_train.pop('isFraud')\nX_test = final_test.copy()\nX_train,X_test = X_train.align(other=X_test,join='left', axis=1)","4c3d787c":"print(\"XGBoost version:\", xgb.__version__)\n\nclf = xgb.XGBClassifier(\n    n_estimators=2000,\n    **best_params,\n    tree_method='gpu_hist',\n    verbose=50,\n    early_stopping_rounds=100\n)\n\nclf.fit(X_train, y_train)\n\ny_preds = clf.predict_proba(X_test)[:,1] ","f49ba1fb":"feature_imp = pd.DataFrame(sorted(zip(clf.feature_importances_,cols)), columns=['Value','Feature'])\nplt.figure(figsize=(20, 10))\nsns.barplot(x=\"Value\", y=\"Feature\", data=feature_imp.sort_values(by=\"Value\", ascending=False).iloc[:50])\nplt.title('XGB Most Important Features')\nplt.tight_layout()\nplt.show()\ndel clf; x=gc.collect()","39c053e2":"sub['isFraud'] = y_preds\nsub.to_csv('XGB_hypopt_model.csv', index=False)","0dc5cdf0":"plt.hist(sub.isFraud,bins=100)\nplt.ylim((0,5000))\nplt.title('XGB Submission')\nplt.show()","b69fa300":"# 1.\tObjective and Background\n\nIn this kernel, I work with IEEE Fraud Detection competition.\n\nEEE-CIS works across a variety of AI and machine learning areas, including deep neural networks, fuzzy systems, evolutionary computation, and swarm intelligence. Today they\u2019re partnering with the world\u2019s leading payment service company, Vesta Corporation, seeking the best solutions for fraud prevention industry.\n\nSpecifically, **the objective of this kernel is to predict the probability that an online transaction is fraudulent**.\n\n\n*Acknowledgements to inspiring codebooks:*\n1. https:\/\/www.kaggle.com\/artgor\/eda-and-models\/notebook\n2. https:\/\/www.kaggle.com\/kabure\/extensive-eda-and-modeling-xgb-hyperopt\/notebook\n3. https:\/\/www.kaggle.com\/cdeotte\/xgb-fraud-with-magic-0-9600","5135de62":"#### Split into Categorical and Numerical dataset","a0e57e8f":"Most of columns have missing data, which is normal in real world. Also there is one column in the test dataset with one unique value. \n\nThere are a lot of continuous variables and some categorical. Let's have a closer look at them.","75d7a2c6":"**I actually tried target encoding the high cardinality columns, but the local validation turned out to be better off when we just drop them instead. So the following code blocks for target encoding are commented out.**","72955954":"## 5.1 Feature engineering on Numerial Variables\nLet's do some aggregation on top features found in EDA.","af0274be":"### Investigating Cardinality","bf161ba4":"### Predicting X test","8d61420e":"### 4.1.2 EDA on Categorical Features (id_12 - id_38; DeviceType; DeviceInfo) - Identity","260c2799":"### Seting y_pred to csv","b50d0118":"### Prepare the Data for futher Feature Engineering\n","86b0faaa":"## 7.2 XGB Hyperopt\nReference: https:\/\/www.kaggle.com\/kabure\/extensive-eda-and-modeling-xgb-hyperopt\/notebook","57c616dd":"A very important idea: it seems that train and test transaction dates don't overlap, so it would be prudent to use time-based split for validation.\nThis was already noted in abother kernel: https:\/\/www.kaggle.com\/robikscube\/ieee-fraud-detection-first-look-and-eda","7e293f98":"#### EDA for columns Vxxx\nPlease refer to https:\/\/www.kaggle.com\/cdeotte\/eda-for-columns-v-and-id.\nAnd 219 V columns were determined redundant by correlation analysis and removed at the beginning of this kernel.","28c9de70":"### Drop the categorial features that do not exist in the test dataset.","abbbf976":"We have several features showing some kind of \"found\" status and several binary columns.","6f73647a":"#### We face the imbalance data problem, but we will solve it by subsampling!","f8b97d99":"## Mapping Email","c958427b":"So we have two medium-sized datasets with a lot of columns. Train and test data have similar number of rows","fca7cc50":"#### Normalize D Columns\nThe D Columns are \"time deltas\" from some point in the past. We will transform the D Columns into their point in the past. This will stop the D columns from increasing with time. The formula is D15n = Transaction_Day - D15 and Transaction_Day = TransactionDT\/(24*60*60). Afterward we multiple this number by negative one.","cfe656ea":"All of the following features where chosen because each increase local validation. The procedure for engineering features is as follows. First you think of an idea and create a new feature. Then you add it to your model and evaluate whether local validation AUC increases or decreases. If AUC increases keep the feature, otherwise discard the feature.","a46f2d38":"#### Following is our best parameters derived from the XGB HyperOpt.\n\n#### Just to make our kernel run faster(the HyperOpt process takes 4:45:26), the previous XGB HyperOpt have been commented out.","763f0c2f":"# 2.   Roadmap\n\na).\tPreparation: Import Libraries\n\nb).\tPreparation: Define Functions used in this kernel\n\nc).\tData Loading and Overview\n\nd).\tExploratory Data Analysis\n\ne).\tFeaturing Engineering\n\nf).\tFeature Selection\n\ng).\tXBGoost Modelling\n\nh).\tMake and Submit the Predication","ed7267b2":"## Import Libraries","33a93da5":"## 7.1 Local Validation\nFor this competition, we will use local validation. I evaluated features by training on the first 75% of the train data and predicting the last 25% of the train data. ","cd188984":"## 4.1 EDA on Identity Data\n\nLet's start with identity information.\nid_01 - id_11 are continuous variables, id_12 - id_38 are categorical and the last two columns are obviously also categorical.\n\n*Previously*:\n- **Explanation on Identity Data**\n    - Variables in this table are identity information \u2013 network connection information (IP, ISP, Proxy, etc) and digital signature (UA\/browser\/os\/version, etc) associated with transactions.\n    - They're collected by Vesta\u2019s fraud protection system and digital security partners.(The field names are masked and pairwise dictionary will not be provided for privacy protection and contract agreement)\n\n\n- **Categorical Features - Identity**\n    - DeviceType\n    - DeviceInfo\n    - id_12 - id_38","5dc67a02":"# 7. XGBoost Modelling\nHooray! We have 195 final features!\n\nLet's continue with XGBoost Modelling!","5882891c":"### 4.1.1 EDA on Numerical Features (id_01 - id_11) - Identity","9ab7db3a":"We will load all the data except 219 V columns that were determined redundant by correlation analysis https:\/\/www.kaggle.com\/cdeotte\/eda-for-columns-v-and-id.","12ceb443":"### 4.2.1 EDA on Numerical Features (TransactionDT; TransactionAmt; dist; C1-C14; D1-D15; Vxxx) - Transaction","77cc1223":"## 4.2 EDA on Transaction Data\nNow let's have a look at transaction data.\n\n*Previously*:\n- **Numerical Features - Transaction**\n    - TransactionDT: timedelta from a given reference datetime (not an actual timestamp)\n    - TransactionAmt: transaction payment amount in USD\n    - dist: distance\n    - C1-C14: counting, such as how many addresses are found to be associated with the payment card, etc. The actual meaning is masked.\n    - D1-D15: timedelta, such as days between previous transaction, etc.\n    - Vxxx: Vesta engineered rich features, including ranking, counting, and other entity relations.\n\u200b\n\u200b\n- **Categorical Features - Transaction**\n    - ProductCD: product code, the product for each transaction\n    - card1 - card6: : payment card information, such as card type, card category, issue bank, country, etc.\n    - addr1, addr2: address\n    - P_emaildomain: purchaser email domain\n    - R_emaildomain: recipient email domain\n    - M1 - M9: match, such as names on card and address, etc.","bbc8e8d1":"## Functions used in this kernel","2c189ea6":"`id_01` has an interesting distribution: it has 77 unique non-positive values with skewness to 0.","53db532d":"## 7.3 Trainning and Predicting with best Parameters\n","1cf7e521":"`id_03`, `id_04`, `id_05`, `id_06`, `id_09` and `id_10` have over 76% of missing values and over 90% of values are either missing or equal to 0.\nSo maybe we will filter out these features in our feature selection part.","ab1f73cb":"So `card6` is type of card, `card4` is credit card company","93d410f6":"## 5.2 Feature engineering on Categorical Variables Based on Cardinality\nLet's encode the categorical variables based on cardinality!\n- Low Cardinality features: Label Encoding\n- High Cardinality features: Target Encoding \/ Drop\n","a8583762":"Some of features seem to be normalized, and some are not. So if someone wants to normalize all variables, it would be necessary to separate such variables which seem to be already normalized.","df5570d8":"# 6. Feature Selection - Time Consistency\nAfter the above feature engineering, we've come a long way!\nNow, we have 205 features in the train data. We will now check each of our  for \"time consistency\". \n\nThanks to https:\/\/www.kaggle.com\/cdeotte\/xgb-fraud-with-magic-0-9600\/notebook, respective mdoels have been built. Each model is trained on the first month of the training data and only use one feature. The model then predicts the last month of the training data. We want both training AUC and validation AUC to be above AUC = 0.5. It turns out that 19 features fail this test so we will remove them. Additionally we will remove 7 D columns that are mostly NAN. More techniques for feature selection are listed at https:\/\/www.kaggle.com\/c\/ieee-fraud-detection\/discussion\/111308.","49721e2c":"## Encoding Functions\nBelow are 5 encoding functions. (1) encode_FE does frequency encoding where it combines train and test first and then encodes. (2) encode_LE is a label encoded for categorical features (3) encode_AG makes aggregated features such as aggregated mean and std (4) encode_CB combines two columns (5) encode_AG2 makes aggregated features where it counts how many unique values of one feature is within a group.","240c1375":"# 3. Data loading and overview\n\nData is separated into two datasets: information about the **customer identity** and **transaction**, joined by TransactionID. Not all transactions have corresponding identity information.\n\n- **Numerical Features - Transaction**\n    - TransactionDT: timedelta from a given reference datetime (not an actual timestamp)\n    - TransactionAMT: transaction payment amount in USD\n    - dist: distance\n    - C1-C14: counting, such as how many addresses are found to be associated with the payment card, etc. The actual meaning is masked.\n    - D1-D15: timedelta, such as days between previous transaction, etc.\n    - Vxxx: Vesta engineered rich features, including ranking, counting, and other entity relations.\n\n\n- **Categorical Features - Transaction**\n    - ProductCD: product code, the product for each transaction\n    - card1 - card6: : payment card information, such as card type, card category, issue bank, country, etc.\n    - addr1, addr2: address\n    - P_emaildomain: purchaser email domain\n    - R_emaildomain: recipient email domain\n    - M1 - M9: match, such as names on card and address, etc.\n\n\n\n- **Explanation on Identity Data**\n    - Variables in this table are identity information \u2013 network connection information (IP, ISP, Proxy, etc) and digital signature (UA\/browser\/os\/version, etc) associated with transactions.\n    - They're collected by Vesta\u2019s fraud protection system and digital security partners.(The field names are masked and pairwise dictionary will not be provided for privacy protection and contract agreement)\n\n\n- **Categorical Features - Identity**\n    - DeviceType\n    - DeviceInfo\n    - id_12 - id_38\n\n*More details about the data: \nhttps:\/\/www.kaggle.com\/c\/ieee-fraud-detection\/discussion\/101203*","40e88b46":"Our dataset has enough rows to do the following encoding split and label encoding!","ad92ec86":"# 4. Exploratory Data Analysis\n\nI will start EDA on identity data and transaction respectively. The aim is to answer some questions like:\n\n- What type of data we have on our data?\n- How many cols, rows, missing values we have?\n- What's the target distribution?\n- What's the Transactions values distribution of fraud and no fraud transactions?\n- Do we have predominant fraudulent products?\n- What features or target shows some interesting patterns?\n\nAnd a lot of more questions that will raise trought the exploration.","011603cb":"# 5. Feature engineering","fd0551a8":"Here we can see some information about client's device. It is important to be careful here - some of info could be for old devices and may be absent from test data.","b23ec677":"### Defining the HyperOpt function with parameters space and model","cb24d67e":"### 4.2.2 EDA on Categorical Features (ProductCD; card1-card6; addr1, addr2; P_emaildomain; R_emaildomain; M1-M9) - Transaction","71100e97":"22% of values in `id_11` are equal to 100 and 76% are missing. Quite strange."}}