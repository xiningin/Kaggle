{"cell_type":{"d9a51c52":"code","949958fc":"code","413961d6":"code","733d0f4e":"code","2d8dbec6":"code","9d4e9e4a":"code","570de80a":"code","6eb9994d":"code","65e82793":"code","2a51fd74":"code","cbdb164d":"code","77dd0c25":"code","93f71672":"code","d97c799c":"code","52cfef12":"code","75d5618b":"code","a0d784f0":"code","5ece55e5":"code","7ce4741d":"code","d764193c":"code","152b2178":"markdown","a9ecf8d7":"markdown","9dc86d97":"markdown","14e86720":"markdown","6230ac8f":"markdown","b2019d7b":"markdown","75d4cd39":"markdown","38e3e6e9":"markdown","f5b1c5a6":"markdown","b32c59ab":"markdown","6025907a":"markdown","91802f9c":"markdown","6995435a":"markdown"},"source":{"d9a51c52":"input_path = '\/kaggle\/input\/severstal-steel-defect-detection\/'\nbase = '\/kaggle\/input\/severstal-inference-base'\nrequirements_dir = base + '\/requirements\/'","949958fc":"!pip -q config set global.disable-pip-version-check true\n!pip -q install {requirements_dir}Keras_Applications-1.0.8-py3-none-any.whl\n!pip -q install {requirements_dir}efficientnet-1.1.1-py3-none-any.whl","413961d6":"import os, sys, re, gc\nimport numpy as np\nimport pandas as pd\n\nimport tensorflow as tf\nimport tensorflow.keras.layers as L\nimport tensorflow.keras.backend as K\n\n!cp -r {base}\/tpu_segmentation .\/\n!cp -r {base}\/*.py .\/\nfrom tpu_segmentation import *\nfrom severstal_utils import *\n!rm -r tpu_segmentation *.py\n\nAUTO = tf.data.experimental.AUTOTUNE \nstrategy = tf.distribute.get_strategy()\n\nstart_notebook = time()\nprint('Notebook started at: ', current_time_str())\nprint('Tensorflow version: ', tf.__version__)","733d0f4e":"IMAGE_SIZE = (256, 1600) # original image size\ntarget_size = (128, 800) # size used for CNN inputs, same for all CNNs.\ninput_shape = (*target_size, 3) \nN_CLASSES = 4 # types of defects","2d8dbec6":"test_fnames = tf.io.gfile.glob(input_path + 'test_images\/*')\ntest_ids = [x.split('\/')[-1].split('.')[0] for x in test_fnames]\nget_test_path = lambda x: input_path + 'test_images\/' + x + '.jpg'","9d4e9e4a":"def normalize_and_reshape(img, target_size): \n    img = tf.image.resize(img, target_size)\n    img = tf.cast(img, tf.float32) \/ 255.0  \n    img = tf.reshape(img, [*target_size, 3]) \n    return img\n\ndef get_image_and_id(file_name, target_size): \n    img = tf.io.read_file(file_name) \n    img = tf.image.decode_jpeg(img, channels=3) \n    img = normalize_and_reshape(img, target_size)\n    img_id = tf.strings.split(file_name, os.path.sep)[-1]\n    img_id = tf.strings.split(img_id, '.')[0]\n    return img, img_id\n\ndef get_test_dataset(fnames, target_size, batch_size):\n    dataset = tf.data.Dataset.from_tensor_slices(fnames)\n    dataset = dataset.map(lambda file_name: get_image_and_id(file_name, target_size), num_parallel_calls=AUTO)\n    dataset = dataset.batch(batch_size=batch_size, drop_remainder=False)\n    dataset = dataset.prefetch(AUTO) \n    return dataset","570de80a":"df = pd.read_csv(base + '\/weights_meta.csv')\ndf1 = df[df.source == 1]\ndf2 = df[df.source == 2]\n\nbin1 = df1[df1.type == 'bin']\nbin1 = get_best_weights(bin1, 1)\n\nseg1 = df1[df1.type != 'bin']\nseg1 = get_best_weights(seg1, 1)\n\nbin2 = df2[df2.type == 'bin']\nseg2 = df2[df2.type != 'bin']\n\nbin_weights = list(bin2.filename) + list(bin1.filename)\nseg_weights = list(seg2.filename) + list(seg1.filename)","6eb9994d":"!mkdir -p weights\n!unzip -q {base}\/mixed_weights.zip -d weights\n!unzip -q {base}\/binary_weights.zip -d weights\n!unzip -q {base}\/segmentation_weights.zip -d weights","65e82793":"fnames_bin = [get_test_path(i) for i in test_ids]\n\ntest_dataset_bin = get_test_dataset(fnames_bin, target_size=(128, 800), batch_size=16)\nnum_batches = tf.data.experimental.cardinality(test_dataset_bin); \nprint('num of batches', num_batches.numpy())","2a51fd74":"ensemble_outputs = []\nwith strategy.scope():\n    X = L.Input(shape=input_shape)\n    for i, w in enumerate(bin_weights):\n        base_name = w.split('-bin')[0]\n        model = build_classifier(base_name, n_classes = 1, input_shape=input_shape, weights = None, name_suffix='-M{}'.format(i+1))\n        model.load_weights('weights\/' + w)\n        model_output = model(X)\n        ensemble_outputs.append(model_output)\n    \n    Y = L.Average()(ensemble_outputs)\n    binary_ensemble = tf.keras.Model(inputs=X, outputs=Y, name='Binary_Classification_Ensemble')\n    binary_ensemble.compile(optimizer='adam', loss='binary_crossentropy', metrics=[])\n\ndel model, ensemble_outputs, model_output","cbdb164d":"start_preds = time()\nbinary_predictions = binary_ensemble.predict(test_dataset_bin)\n\ndel binary_ensemble\nK.clear_session()\ngc.collect()\n\nprint('Elapsed time (binary predictions) {}'.format(time_passed(start_preds)))","77dd0c25":"ensemble_outputs = []\nwith strategy.scope():\n    X = L.Input(shape=input_shape)\n    for i, w in enumerate(seg_weights):\n        backbone_name = w.split('-unetpp')[0]\n        model = xnet(backbone_name, num_classes = 4, input_shape=input_shape, weights = None)\n        model._name = '{}-M{}'.format(model.name, i+1)\n        model.load_weights('weights\/' + w)\n        model_output = model(X)\n        ensemble_outputs.append(model_output)\n    \n    Y = L.Average()(ensemble_outputs)\n    seg_ensemble = tf.keras.Model(inputs=X, outputs=Y, name='Mask_Segmentation_Ensemble')\n    seg_ensemble.compile(optimizer='adam', loss='binary_crossentropy', metrics=[])\n\ndel model, ensemble_outputs, model_output\n!rm -r weights","93f71672":"THRESHOLD = 0.80\nmasked_indexes = np.where(binary_predictions>=THRESHOLD)[0]\nunmasked_indexes = np.where(binary_predictions<THRESHOLD)[0]\n\nseg_ids = list(np.array(test_ids)[masked_indexes])\nno_seg_ids = list(np.array(test_ids)[unmasked_indexes])\nprint(len(seg_ids), len(no_seg_ids), len(seg_ids) + len(no_seg_ids), len(test_ids))","d97c799c":"fnames_seg = [get_test_path(i) for i in seg_ids]\nbatch_size = 8\ntest_dataset_seg = get_test_dataset(fnames_seg, target_size=target_size, batch_size=batch_size)\nnum_batches = tf.data.experimental.cardinality(test_dataset_seg); \nprint('num of batches', num_batches.numpy())","52cfef12":"n_batches = 1\nsample_preds = seg_ensemble.predict(test_dataset_seg.take(n_batches))\nexamples = retrieve_examples(test_dataset_seg, batch_size*n_batches)\nidx = -1\n\nmask_rgb = [(230, 184, 0), (0, 128, 0), (102, 0, 204), (204, 0, 102)]","75d5618b":"# convert to RLE, then back to MASK to verify functions. \nidx += 1\nprint('Original Prediction, mask shape:', sample_preds[idx].shape)\nplot_image_mask((examples[idx][0], sample_preds[idx]), mask_rgb = mask_rgb)\n\nrle_example = create_rles(sample_preds[idx], IMAGE_SIZE)\nmasks_example = build_mask_array(rle_example, IMAGE_SIZE, n_classes=4)\nprint('Reconverted Prediction, subtle differences due to repeated resizing, mask shape: ', masks_example.shape)\nplot_image_mask((tf.image.resize(examples[idx][0], IMAGE_SIZE), tf.cast(masks_example, tf.float32)), mask_rgb = mask_rgb)","a0d784f0":"for i in range(1, min(6, len(sample_preds))):\n    plot_image_mask((examples[i][0], sample_preds[i]), mask_rgb = mask_rgb)","5ece55e5":"thresh_upper = [0.7,0.7,0.7,0.7]\nthresh_lower = [0.4,0.5,0.4,0.5]\nmin_area = [180, 260, 200, 500]\n\nempty_mask = np.zeros(target_size, int) \n\nrles_dict = {}\n# Fill in all the entries for images with no masks\nfor img_prefix in no_seg_ids:\n    for c in range(N_CLASSES): \n            row_name = '{}.jpg_{}'.format(img_prefix, c+1)\n            rles_dict[row_name]  = ''\n\n# predict, postprocess, convert to rle batch by batch.          \nstart_preds = time()\nfor item in test_dataset_seg:\n    mask_predictions = seg_ensemble.predict(item[0])\n    \n    for k, p in enumerate(mask_predictions):\n        for ch in range(N_CLASSES):\n            ch_probs = p[..., ch]\n            ch_pred = (ch_probs > thresh_upper[ch])\n            if ch_pred.sum() < min_area[ch]:\n                ch_pred = empty_mask.copy()\n            else:\n                ch_pred = (ch_probs > thresh_lower[ch])\n            mask_predictions[k,:,:,ch] = ch_pred\n    \n    img_ids = item[1]\n    ids  = [l.decode('utf-8') for l in img_ids.numpy()]\n    rles = [create_rles(p, IMAGE_SIZE) for p in mask_predictions]\n    \n    for i in range(len(ids)):        \n        rle = rles[i]\n        \n        img_prefix = ids[i]\n        for c in range(N_CLASSES): \n            row_name = '{}.jpg_{}'.format(img_prefix, c+1)\n            rles_dict[row_name]  = rle[c].numpy().decode('utf-8')\n            \nprint('Elapsed time (mask predictions) {}'.format(time_passed(start_preds)))","7ce4741d":"df = pd.DataFrame.from_dict(rles_dict, orient='index')\ndf.reset_index(level=0, inplace=True)\ndf.columns = ['ImageId_ClassId', 'EncodedPixels']\n\ndf.to_csv('submission.csv', index=False)","d764193c":"print('Notebook ended at: ', current_time_str())\nprint('Elapsed time (notebook) {}'.format(time_passed(start_notebook)))","152b2178":"### Build test dataset for mask predictions","a9ecf8d7":"##  Severstal: Steel Defect Detection. Inference Notebook\n\n<br>\n\nMore details about this submission can be found in this [GitHub Repo](https:\/\/github.com\/reyvaz\/steel-defect-segmentation).\n\n<br>\n\n### The solution in this notebook consists on a two-step approach.\n\n1. The first step is to run the images through an ensemble of binary image neural network classifiers to determine whether the piece of steel in the image presents a defect.\n\n2. The second step, runs the same image through an ensemble of segmentation neural networks to identify the location of the defect and identify the type of the defect.\n\nAll 1st and 2nd step neural networks were trained on a K = 5, K-Fold cross-validation distribution of the data, all with the same image size. Random data augmentations were applied to the training partition data for all networks and folds in both stages.\n\n#### Binary Classification\n\nThe ensemble for the binary classification step consists of EfficientNet (Tan & Le 2020) based classifiers versions B0-B5. The classifiers were selected according to their out-of-fold performance during training.\n\n#### Defect Type Classification and Segmentation\n\nThe ensemble for the 2nd step consists of UNet++ (Zhou et al., 2019) based CNNs, all with EfficientNet backbones versions B0-B5. The networks included in the ensemble were selected according to their out-of-fold performance on validation data on images with defects only. \n\n#### Acknowledgements:\n- Thanks to [PAO Severstal](https:\/\/www.severstal.com\/) for providing the dataset.\n\n- This solution was inspired by the Kaggle kernels [Severstal: U-Net++ with EfficientNetB4]( https:\/\/www.kaggle.com\/xhlulu\/severstal-u-net-with-efficientnetb4\/) by user [xhlulu](https:\/\/www.kaggle.com\/xhlulu), and [Unet Plus Plus with EfficientNet Encoder]( https:\/\/www.kaggle.com\/meaninglesslives\/nested-unet-with-efficientnet-encoder?scriptVersionId=0) by [Siddhartha](https:\/\/sidml.github.io\/).\n\n\n#### References:\n\n- Tan, M., & Le, Q. V. (2020). EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks. [arXiv:1905.11946v5](https:\/\/arxiv.org\/abs\/1905.11946v5).\n\n- Zhou, Z., Siddiquee, M., Tajbakhsh, N., & Liang, J. (2019). UNet++: Redesigning Skip Connections to Exploit Multiscale Features in Image Segmentation. [arXiv:1912.05074v2](https:\/\/arxiv.org\/abs\/1912.05074v2).","9dc86d97":"# Binary Predictions","14e86720":"### Build test dataset for binary classification\n\nusing all test images. i.e. all listed in `test_ids`","6230ac8f":"### Assemble Binary Ensemble","b2019d7b":"### Predictions (binary)","75d4cd39":"## Test Dataset","38e3e6e9":"## Constants","f5b1c5a6":"### Assemble Segmentation Ensemble","b32c59ab":"## Weight Data and Extraction","6025907a":"### Visualize Prediction Examples","91802f9c":"# Mask Predictions","6995435a":"### Predictions + post-process (masks)"}}