{"cell_type":{"b302d442":"code","f2e94821":"code","1af3d3a6":"code","cb870d7d":"code","e709fc47":"code","99c2497b":"code","778736de":"code","7a50d597":"code","cf107341":"code","a121713c":"code","48a22066":"code","e40179b7":"code","dc72997b":"code","a37980c8":"code","f9ef2c6f":"code","4ecf72e4":"code","63d7af18":"code","fa3294b5":"code","2f0890d9":"code","3a752510":"code","ce643235":"code","3e6666d1":"code","d407e0ef":"code","151de3f9":"code","466b1708":"code","1d76d26e":"code","6d6e0550":"code","77833666":"markdown","855c622a":"markdown","4fbb37ca":"markdown","77b1e4de":"markdown","5776b52b":"markdown","60cf5378":"markdown","324e2727":"markdown","1e483599":"markdown","76344e96":"markdown","1ea68280":"markdown","f01b131e":"markdown","e9ae4556":"markdown","db2409a2":"markdown","df4084ac":"markdown","80ee0d60":"markdown","4ed35d8e":"markdown"},"source":{"b302d442":"import numpy as np\nimport pandas as pd\n\nfrom matplotlib import pyplot as plt\nimport tensorflow as tf\n\n# from keras.preprocessing.image import load_img\n\nimport keras.backend as K\nfrom tensorflow.keras.callbacks import LearningRateScheduler, ModelCheckpoint, EarlyStopping\n\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Input, concatenate, Conv2D, MaxPooling2D, Activation, UpSampling2D, BatchNormalization\n\nfrom skimage.data import imread\nfrom skimage.transform import resize\nfrom sklearn.model_selection import train_test_split\n\n# Load truncated iamges https:\/\/www.kaggle.com\/c\/airbus-ship-detection\/discussion\/62574#latest-445141\nfrom PIL import ImageFile\nImageFile.LOAD_TRUNCATED_IMAGES = True","f2e94821":"import os\nprint(os.listdir(\"..\/input\"))","1af3d3a6":"import warnings\nwarnings.filterwarnings('ignore')","cb870d7d":"IMG_WIDTH = 768\nIMG_HEIGHT = 768\nIMG_CHANNELS = 3\nTARGET_WIDTH = 128\nTARGET_HEIGHT = 128\nepochs=2\nbatch_size=10\nimage_shape=(768, 768)\nFAST_RUN=True # use for development only\nFAST_PREDICTION=True # use for development only","e709fc47":"df = pd.read_csv(\"..\/input\/train_ship_segmentations_v2.csv\")\nsub_df = pd.read_csv(\"..\/input\/sample_submission_v2.csv\")","99c2497b":"df.shape","778736de":"df.head()","7a50d597":"sub_df.shape","cf107341":"sub_df.head()","a121713c":"# ref: https:\/\/www.kaggle.com\/paulorzp\/run-length-encode-and-decode\nno_mask = np.zeros(image_shape[0]*image_shape[1], dtype=np.uint8)\ndef rle_encode(img):\n    '''\n    img: numpy array, 1 - mask, 0 - background\n    Returns run length as string formated\n    '''\n    pixels = img.flatten()\n    pixels = np.concatenate([[0], pixels, [0]])\n    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n    runs[1::2] -= runs[::2]\n    rle = ' '.join(str(x) for x in runs)\n    return rle\n\ndef rle_decode(mask_rle, shape=image_shape):\n    '''\n    mask_rle: run-length as string formated (start length)\n    shape: (height,width) of array to return \n    Returns numpy array, 1 - mask, 0 - background\n\n    '''\n    if pd.isnull(mask_rle):\n        img = no_mask\n        return img.reshape(shape).T\n    s = mask_rle.split()\n    starts, lengths = [np.asarray(x, dtype=int) for x in (s[0:][::2], s[1:][::2])]\n\n    starts -= 1\n    ends = starts + lengths\n    img = np.zeros(shape[0]*shape[1], dtype=np.uint8)\n    for lo, hi in zip(starts, ends):\n        img[lo:hi] = 1\n    return img.reshape(shape).T","48a22066":"segmentation = df[df.EncodedPixels.notnull()].sample().iloc[0]\nimage = imread('..\/input\/train_v2\/'+segmentation.ImageId)\n\nfig=plt.figure(figsize=(16, 8))\nfig.add_subplot(2, 2, 1)\nplt.imshow(image)\nfig.add_subplot(2, 2, 2)\nplt.imshow(rle_decode(segmentation.EncodedPixels))","e40179b7":"segmentation = df[df.EncodedPixels.isnull()].sample().iloc[0]\nimage = imread('..\/input\/train_v2\/'+segmentation.ImageId)\n\nfig=plt.figure(figsize=(16, 8))\nfig.add_subplot(2, 2, 1)\nplt.imshow(image)\nfig.add_subplot(2, 2, 2)\nplt.imshow(rle_decode(segmentation.EncodedPixels))","dc72997b":"inputs = Input((TARGET_WIDTH , TARGET_HEIGHT, IMG_CHANNELS))\n\n# 128\n\ndown1 = Conv2D(64, (3, 3), padding='same')(inputs)\ndown1 = BatchNormalization()(down1)\ndown1 = Activation('relu')(down1)\ndown1 = Conv2D(64, (3, 3), padding='same')(down1)\ndown1 = BatchNormalization()(down1)\ndown1 = Activation('relu')(down1)\ndown1_pool = MaxPooling2D((2, 2), strides=(2, 2))(down1)\n# 64\n\ndown2 = Conv2D(128, (3, 3), padding='same')(down1_pool)\ndown2 = BatchNormalization()(down2)\ndown2 = Activation('relu')(down2)\ndown2 = Conv2D(128, (3, 3), padding='same')(down2)\ndown2 = BatchNormalization()(down2)\ndown2 = Activation('relu')(down2)\ndown2_pool = MaxPooling2D((2, 2), strides=(2, 2))(down2)\n# 32\n\ndown3 = Conv2D(256, (3, 3), padding='same')(down2_pool)\ndown3 = BatchNormalization()(down3)\ndown3 = Activation('relu')(down3)\ndown3 = Conv2D(256, (3, 3), padding='same')(down3)\ndown3 = BatchNormalization()(down3)\ndown3 = Activation('relu')(down3)\ndown3_pool = MaxPooling2D((2, 2), strides=(2, 2))(down3)\n# 16\n\ndown4 = Conv2D(512, (3, 3), padding='same')(down3_pool)\ndown4 = BatchNormalization()(down4)\ndown4 = Activation('relu')(down4)\ndown4 = Conv2D(512, (3, 3), padding='same')(down4)\ndown4 = BatchNormalization()(down4)\ndown4 = Activation('relu')(down4)\ndown4_pool = MaxPooling2D((2, 2), strides=(2, 2))(down4)\n# 8\n\ncenter = Conv2D(1024, (3, 3), padding='same')(down4_pool)\ncenter = BatchNormalization()(center)\ncenter = Activation('relu')(center)\ncenter = Conv2D(1024, (3, 3), padding='same')(center)\ncenter = BatchNormalization()(center)\ncenter = Activation('relu')(center)\n# center\n\nup4 = UpSampling2D((2, 2))(center)\nup4 = concatenate([down4, up4], axis=3)\nup4 = Conv2D(512, (3, 3), padding='same')(up4)\nup4 = BatchNormalization()(up4)\nup4 = Activation('relu')(up4)\nup4 = Conv2D(512, (3, 3), padding='same')(up4)\nup4 = BatchNormalization()(up4)\nup4 = Activation('relu')(up4)\nup4 = Conv2D(512, (3, 3), padding='same')(up4)\nup4 = BatchNormalization()(up4)\nup4 = Activation('relu')(up4)\n# 16\n\nup3 = UpSampling2D((2, 2))(up4)\nup3 = concatenate([down3, up3], axis=3)\nup3 = Conv2D(256, (3, 3), padding='same')(up3)\nup3 = BatchNormalization()(up3)\nup3 = Activation('relu')(up3)\nup3 = Conv2D(256, (3, 3), padding='same')(up3)\nup3 = BatchNormalization()(up3)\nup3 = Activation('relu')(up3)\nup3 = Conv2D(256, (3, 3), padding='same')(up3)\nup3 = BatchNormalization()(up3)\nup3 = Activation('relu')(up3)\n# 32\n\nup2 = UpSampling2D((2, 2))(up3)\nup2 = concatenate([down2, up2], axis=3)\nup2 = Conv2D(128, (3, 3), padding='same')(up2)\nup2 = BatchNormalization()(up2)\nup2 = Activation('relu')(up2)\nup2 = Conv2D(128, (3, 3), padding='same')(up2)\nup2 = BatchNormalization()(up2)\nup2 = Activation('relu')(up2)\nup2 = Conv2D(128, (3, 3), padding='same')(up2)\nup2 = BatchNormalization()(up2)\nup2 = Activation('relu')(up2)\n# 64\n\nup1 = UpSampling2D((2, 2))(up2)\nup1 = concatenate([down1, up1], axis=3)\nup1 = Conv2D(64, (3, 3), padding='same')(up1)\nup1 = BatchNormalization()(up1)\nup1 = Activation('relu')(up1)\nup1 = Conv2D(64, (3, 3), padding='same')(up1)\nup1 = BatchNormalization()(up1)\nup1 = Activation('relu')(up1)\nup1 = Conv2D(64, (3, 3), padding='same')(up1)\nup1 = BatchNormalization()(up1)\nup1 = Activation('relu')(up1)\n# 128\n\noutputs = Conv2D(1, (1, 1), activation='sigmoid')(up1)\n\nmodel = Model(inputs=inputs, outputs=outputs)\n","a37980c8":"optimizer = tf.train.RMSPropOptimizer(0.0001)","f9ef2c6f":"model.compile(\n    optimizer=optimizer, \n    loss=\"binary_crossentropy\", \n    metrics=[\"accuracy\"]\n)","4ecf72e4":"# use for development to run it faster\nif FAST_RUN:\n    df = df.sample(n=1000).reset_index().drop(columns=[\"index\"]) # after reset index dataframe will have one more column call index\n    \nif FAST_PREDICTION:\n    sub_df = sub_df.sample(n=100).reset_index().drop(columns=[\"index\"]) # after reset index dataframe will have one more column call index\n","63d7af18":"train_df, validate_df = train_test_split(df)","fa3294b5":"def get_image(image_name):\n    img = imread('..\/input\/train_v2\/'+image_name)[:,:,:IMG_CHANNELS]\n    img = resize(img, (TARGET_WIDTH, TARGET_HEIGHT), mode='constant', preserve_range=True)\n    return img\n    \ndef get_mask(code):\n    img = rle_decode(code)\n    img = resize(img, (TARGET_WIDTH, TARGET_HEIGHT, 1), mode='constant', preserve_range=True)\n    return img","2f0890d9":"def create_image_generator(precess_batch_size, data_df):\n    while True:\n        for k, group_df in data_df.groupby(np.arange(data_df.shape[0])\/\/precess_batch_size):\n            imgs = []\n            labels = []\n            for index, row in group_df.iterrows():\n                # images\n                original_img = get_image(row.ImageId) \/ 255.0\n                # masks\n                mask = get_mask(row.EncodedPixels) \/ 255.0\n                \n                imgs.append(original_img)\n                labels.append(mask)\n                \n            imgs = np.array(imgs)\n            labels = np.array(labels)\n            yield imgs, labels","3a752510":"train_generator = create_image_generator(batch_size, train_df)\nvalidate_generator = create_image_generator(batch_size, validate_df)","ce643235":"train_steps=np.ceil(float(train_df.shape[0]) \/ float(batch_size)).astype(int)\nvalidate_steps=np.ceil(float(validate_df.shape[0]) \/ float(batch_size)).astype(int)\n\nhistory = model.fit_generator(\n    train_generator, \n    steps_per_epoch=train_steps,\n    validation_data=validate_generator,\n    validation_steps=validate_steps,\n    epochs=epochs\n)","3e6666d1":"def get_test_image(image_name):\n    img = imread('..\/input\/test_v2\/'+image_name)[:,:,:IMG_CHANNELS]\n    img = resize(img, (TARGET_WIDTH, TARGET_HEIGHT), mode='constant', preserve_range=True)\n    return img\n    \ndef create_test_generator(precess_batch_size):\n    while True:\n        for k, ix in sub_df.groupby(np.arange(sub_df.shape[0])\/\/precess_batch_size):\n            imgs = []\n            labels = []\n            for index, row in ix.iterrows():\n                original_img = get_test_image(row.ImageId) \/ 255.0\n                imgs.append(original_img)\n                \n            imgs = np.array(imgs)\n            yield imgs","d407e0ef":"test_generator = create_test_generator(batch_size)","151de3f9":"test_steps = np.ceil(float(sub_df.shape[0]) \/ float(batch_size)).astype(int)\npredict_mask = model.predict_generator(test_generator, steps=test_steps)","466b1708":"fig=plt.figure(figsize=(16, 8))\nfor index, row in sub_df.head(6).iterrows():\n    origin_image = imread('..\/input\/test_v2\/'+row.ImageId)\n    predicted_image = resize(predict_mask[index], image_shape).reshape(IMG_WIDTH, IMG_HEIGHT) * 255\n    plt.subplot(3, 4, 2*index+1)\n    plt.imshow(origin_image)\n    plt.subplot(3, 4, 2*index+2)\n    plt.imshow(predicted_image)","1d76d26e":"for index, row in sub_df.iterrows():\n    predict = predict_mask[index]\n    resized_predict =  resize(predict, (IMG_WIDTH, IMG_HEIGHT)) * 255\n    mask = resized_predict > 0.5\n    sub_df.at[index,'EncodedPixels'] = rle_encode(mask)","6d6e0550":"sub_df.to_csv(\"submission.csv\", index=False)","77833666":"**Image With Ship**","855c622a":"# Define U-Net Model","4fbb37ca":"**Image without ship**","77b1e4de":"Working in Progress","5776b52b":"# Import Libraries","60cf5378":"# Fit Model","324e2727":"# Split train and validate data","1e483599":"**Hide some the messy warning**","76344e96":"# Define Variables","1ea68280":"# Explore Data","f01b131e":"# Submission","e9ae4556":"# Generator","db2409a2":"# Compile Model","df4084ac":"# Read Data","80ee0d60":"# Prediction","4ed35d8e":"# See our prediction"}}