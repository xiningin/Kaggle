{"cell_type":{"0b5882ee":"code","94d94d92":"code","3b49f227":"code","af272a2d":"code","6b594220":"code","15d35a91":"code","99d5cb7b":"code","1c19b40c":"code","bc56a752":"code","5a1a01b9":"code","f682a40a":"code","5f36961d":"code","f178e8b4":"markdown","00e49dd0":"markdown","9e996f01":"markdown","64259e1b":"markdown","cf25fea4":"markdown","671f0194":"markdown","e9802de7":"markdown"},"source":{"0b5882ee":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# Importing our dataset from CSV file:\ndataset = pd.read_csv(\"..\/input\/student-scores\/student_scores.csv\")\n\n# Now let's explore our dataset:\ndataset.shape","94d94d92":"# Let's take a look at what our dataset actually looks like:\ndataset.head()","3b49f227":"# To see statistical details of the dataset, we can use describe():\ndataset.describe()","af272a2d":"#And finally, let's plot our data points on 2-D graph our dataset \n#and see if we can manually find any relationship between the data:\n\ndataset.plot(x='Hours', y='Scores', style='o')\nplt.title('Hours vs Score')\nplt.xlabel('Hours Studied')\nplt.ylabel('Percentage Score')\nplt.show()\n","6b594220":"# Preparing our data:\n# Divide the data into \"attributes\" and \"labels\". Attributes are the independent variables\n# while labels are dependent variables whose values are to be predicted.\n\nX = dataset.iloc[:, :-1].values # all colomns except the last one (reshape it into column vector)\n \ny = dataset.iloc[:, 1].values # first colomn only\n","15d35a91":"''' \nThe next step is to split this data into training and test sets. \nWe'll do this by using Scikit-Learn's built-in train_test_split() method:\n'''\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n\n# The above script splits 80% of the data to training set while 20% of the data to test set. \n# The test_size variable is where we actually specify the proportion of test set.","99d5cb7b":"# Training the Algorithm:\n\nfrom sklearn.linear_model import LinearRegression\nregressor = LinearRegression()\nregressor.fit(X_train, y_train)","1c19b40c":"# To retrieve the intercept:\nprint(regressor.intercept_)\n\n# For retrieving the slope (coefficient of x):\nprint(regressor.coef_)\n","bc56a752":"# Making Predictions:\n# Now that we have trained our algorithm, it's time to make some predictions.\n\ny_pred = regressor.predict(X_test)    # The y_pred is a numpy array that contains all the predicted values.\n","5a1a01b9":"# To compare the actual output values for X_test with the predicted values, execute the following script:\n\ndf = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred})\ndf\n","f682a40a":"# Plot actual value vs predicted one:\n\nplt.scatter(X_test, y_test)\nplt.plot(X_test, y_pred, color='red')\n\nplt.title('Hours vs Percentage')\nplt.xlabel('Hours Studied')\nplt.ylabel('Percentage Score')\nplt.show()","5f36961d":"from sklearn import metrics\nprint('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred))\nprint('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred))\nprint('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))\nprint('r2_score: ', metrics.r2_score(y_test,y_pred))","f178e8b4":"The final step is to evaluate the performance of algorithm. \nThis step is particularly important to compare how well different algorithms perform on a particular dataset.\nFor regression algorithms, four evaluation metrics are commonly used:","00e49dd0":"# Evaluating the Algorithm:\n\n## Mean Absolute Error\n\n$$\n\\mathrm{MAE}=\\frac{1}{N} \\sum_{i=1}^{N}\\left|y_{i}-\\hat{y}_{i}\\right|\n$$\n\n## Mean Squared Error\n\n$$\n\\mathrm{MSE}=\\frac{1}{N} \\sum_{i=1}^{N}\\left(y_{i}-\\hat{y}_{i}\\right)^{2}\n$$\n\n## r2_score\n\n$$\nR^{2}=1-\\frac{\\mathrm{MSE}(\\text { model })}{\\text { MSE (baseline) }}\n$$\n\n**The MSE of the model is computed as above, while the MSE of the baseline is defined as:**\n\n$$\n\\mathrm{MSE}(\\text { baseline })=\\frac{1}{N} \\sum_{i=1}^{N}\\left(y_{i}-\\bar{y}\\right)^{2}\n$$","9e996f01":"****Ideally, lower RMSE and higher R-squared values are indicative of a good model.  ","64259e1b":"This means that for every one unit of change in hours studied, the change in the score is about 9.91%","cf25fea4":"In the theory section we said that linear regression model basically finds the best value for the intercept (bias) and slope, which results in a line that best fits the data. To see the value of the intercept and slop calculated by the linear regression algorithm for our dataset, execute the following code.","671f0194":"# Supervised learning\n\n# Regression\nis the process of estimating the relationship between input data and the continuous-valued output data. This data is usually in the form of real numbers, and our goal is to estimate the underlying function that governs the mapping from the input to the output.\n\n# Generalized Linear Models \nThe following are a set of methods intended for regression in which the target value is expected to be a linear combination of the input variables. \n\nIn mathematical notion, if `(\ud835\udc66^)` is the predicted value.\n\n**\ud835\udc66^(\ud835\udc64,\ud835\udc65) =\ud835\udc640+\ud835\udc641\ud835\udc651+...+\ud835\udc64\ud835\udc5d\ud835\udc65\ud835\udc5d**\n\nAcross the module (`Scikit-Learn`), we designate the vector `\ud835\udc64= (\ud835\udc641,...,\ud835\udc64\ud835\udc5d)` as `coef_` and `\ud835\udc640` as `intercept_`\n\n# Ordinary Least Squares\nvisualization on: `http:\/\/setosa.io\/ev\/ordinary-least-squares-regression\/`\n\n`LinearRegression` fits a linear model with coefficients `\ud835\udc64= (\ud835\udc641,...,\ud835\udc64\ud835\udc5d)` to minimize the residual sum of squares between the observed responses in the dataset, and the responses predicted by the linear approximation. Mathematically it solves a problem of the form:  \n\n$$\n\\min _{w}\\|X w-y\\|_{2}^{2}\n$$\n\nHowever, coefficient estimates for Ordinary Least Squares rely on the independence of the model terms. When terms\nare correlated and the columns of the design matrix X have an approximate linear dependence, the design matrix\nbecomes close to singular and as a result, the least-squares estimate becomes highly sensitive to random errors in the\nobserved response, producing a large `variance`.","e9802de7":"**From the graph above, we can clearly see that there is a positive linear relation between the number of hours studied and percentage of score.**"}}