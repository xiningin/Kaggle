{"cell_type":{"7822051d":"code","58b7432a":"code","d8e01416":"code","6fbb12e6":"code","48c954c3":"code","e1ddc1c8":"code","7667dbea":"code","4668ffde":"code","c3283fa6":"code","bc99f4ab":"code","04bd0d56":"code","203dc6fb":"code","eaa04351":"code","efb84b81":"code","9d314f8a":"code","2afd5497":"code","49c1f4bc":"code","0e4e76ac":"code","14430593":"code","30a90667":"code","c3ae9eef":"code","ce66d96a":"code","7180cf2c":"code","dd6fff27":"code","2f51becf":"code","f03c07e3":"code","2348975d":"code","3e173ef6":"code","926bade7":"code","c1b98c43":"code","f6ac897c":"code","4e26a969":"code","9c7cefc8":"code","fbce8aa1":"code","4b575015":"code","7390a5b4":"code","386f5920":"code","9c77dfc6":"code","e73faa1e":"code","f2c7a474":"code","fe0078f6":"code","32d689fc":"code","5b9bc108":"code","d04218c1":"code","20466d0b":"code","b5498df2":"code","4d5ef1cd":"code","a5648269":"code","b529fb47":"code","5123e6cf":"code","0c276a1e":"code","5cf1ada4":"code","b00b8fa7":"code","e7b32170":"code","35b3c85c":"code","7805881c":"code","a45b0a5d":"code","f1345b60":"markdown","624ac0a3":"markdown","4dd7732f":"markdown","204f68d2":"markdown","54bd87c6":"markdown","80f3ed65":"markdown","2c3137af":"markdown","d47631ee":"markdown","2d531f93":"markdown","deff4fe6":"markdown","afae7a4d":"markdown","979f558f":"markdown","1020dcf2":"markdown"},"source":{"7822051d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n# libraries for dataset preparation, feature engineering, model training \nfrom sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn import decomposition, ensemble\nfrom sklearn.linear_model import LogisticRegression\nimport pandas, xgboost, numpy, textblob, string\nfrom keras.preprocessing import text, sequence\nfrom keras import layers, models, optimizers\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.models import Sequential, Model\nfrom keras.layers import Activation, Dense, Dropout\nfrom sklearn.preprocessing import LabelBinarizer\nfrom xgboost import XGBClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.svm import LinearSVC\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.multiclass import OneVsRestClassifier\nimport time\nimport os\n\n#Visualization\nimport matplotlib.pyplot as plt \nimport seaborn as sns\n\n#Preprocessing related libraries\nimport string\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.classify import SklearnClassifier\nfrom wordcloud import WordCloud,STOPWORDS\nimport warnings \nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning)\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","58b7432a":"train = pd.read_excel(r\"..\/input\/73stringscasestudyround\/Training Data.xlsx\")","d8e01416":"train.head(2)","6fbb12e6":"#Identifying the first Tag for all rows\nfirst_tag = []\nfor str in train['Industry Classifications']:\n    first_tag.append(str.split(';')[0])","48c954c3":"#Number of Records in First_Tag\nlen(first_tag)","e1ddc1c8":"#Adding a new feature into the training dataset\ntrain['First_Tag'] = first_tag","7667dbea":"train.head(2)","4668ffde":"#Unique Values for Geographic Locations\ntrain['Geographic Locations'].unique()","c3283fa6":"#Unique Values for CompanyStatus\ntrain['Company Status'].unique()","bc99f4ab":"#Unique Values for CompanyType\ntrain['Company Type'].unique()","04bd0d56":"#Abstracting CompanyName,BusinessDescription,IndustryClassification from train into a new dataset\ntrain_ana = train[['Company Name','Business Description','First_Tag']]","203dc6fb":"train_ana.head(2)","eaa04351":"#Adding and additional field named \" Tidy Description\", where we will store the transformed description\ntrain_ana['Tidy_Desc'] = train_ana['Business Description'].str.lower()","efb84b81":"train_ana.head(1)","9d314f8a":"import re\n#Function to remove any additional special characters, if needed.\ndef remove_pattern(input_txt, pattern):\n    r = re.findall(pattern, input_txt)\n    for i in r:\n        input_txt = re.sub(i, '', input_txt)\n        \n    return input_txt","2afd5497":"#Facts: Translate function has been changed from Python 2.x to Python 3.x\n#It now takes only one output\ntrain_ana.Tidy_Desc = train_ana.Tidy_Desc.apply(lambda x: x.translate({ord(c):'' for c in \"1234567890\"}))","49c1f4bc":"train_ana.head(1)","0e4e76ac":"#[!\u201d#$%&\u2019()*+,-.\/:;<=>?@[\\]^_`{|}~]\ntrain_ana.Tidy_Desc=train_ana.Tidy_Desc.apply(lambda x: x.translate({ord(c):'' for c in \"[!\u201d#$%&\u2019()*+,-.\/:;<=>?@[\\]^_`{|}~]\"}))","14430593":"train_ana.head(1)","30a90667":"train_ana.Tidy_Desc=train_ana.Tidy_Desc.apply(lambda x: x.strip())","c3ae9eef":"train_ana.head(1)","ce66d96a":"stop_words = stopwords.words('english')\nfrom nltk.tokenize import word_tokenize","7180cf2c":"# function to remove stopwords\ndef remove_stopwords(sen):\n    sen_new = \" \".join([i for i in sen if i not in stop_words])\n    return sen_new","dd6fff27":"clean_sentences  = [remove_stopwords(r.split()) for r in train_ana['Tidy_Desc']]","2f51becf":"train_ana['Tidy_Desc'] = clean_sentences","f03c07e3":"train_ana.head(1)","2348975d":"#Tokenization\ntokens = train_ana['Tidy_Desc'].apply(lambda x: x.split())\n#Now that we have the removed StopWords and tokenized the Business Descriptions\n#We will now subject the tokenized version to removing stop words, sparse terms, and particular words\n#In some cases, it\u2019s necessary to remove sparse terms or particular words from texts. \n#This task can be done using stop words removal techniques considering that any group of words can be chosen as the stop words.","3e173ef6":"#Stemming\nfrom nltk.stem import PorterStemmer\nstemmer= PorterStemmer()\nStemmed_tokens = tokens.apply(lambda x: [stemmer.stem(i) for i in x]) # stemming","926bade7":"for i in range(len(Stemmed_tokens)):\n    Stemmed_tokens[i] = ' '.join(Stemmed_tokens[i])\n\ntrain_ana['Tidy_Desc_Stemmed'] = Stemmed_tokens","c1b98c43":"train_ana.head(1)","f6ac897c":"#Lemmatization\nfrom nltk.stem import WordNetLemmatizer\nlemmatizer=WordNetLemmatizer()\nLemmatized_tokens = tokens.apply(lambda x: [lemmatizer.lemmatize(i) for i in x]) # Lemmatizing","4e26a969":"for i in range(len(Lemmatized_tokens)):\n    Lemmatized_tokens[i] = ' '.join(Lemmatized_tokens[i])\n\ntrain_ana['Tidy_Desc_Lemma'] = Lemmatized_tokens","9c7cefc8":"train_ana.head(1)","fbce8aa1":"#Before going for Count Vectorization as Feature\n#We would like to check the type of Count of Type of Tags\n\ntrain_ana['First_Tag'].value_counts().head(5)","4b575015":"#Removing the rows with missing Description\ntrain_ana = train_ana[train_ana['First_Tag'] != '-']","7390a5b4":"#set(train_ana['First_Tag'])","386f5920":"#Selecting the first 10 labels\nT10Tag = train_ana['First_Tag'].value_counts().index.tolist()","9c77dfc6":"T10Tag = T10Tag[:10]","e73faa1e":"T10Tag","f2c7a474":"train_10 = train_ana[train_ana['First_Tag'].isin(T10Tag)]","fe0078f6":"train_10.shape","32d689fc":"set(train_10['First_Tag'])","5b9bc108":"from collections import Counter\nCounter(train_10['First_Tag'])","d04218c1":"train_10.columns.tolist()","20466d0b":"train_10_ana = train_10[['Tidy_Desc_Lemma','First_Tag']]","b5498df2":"#pre-processing\nimport re \ndef clean_str(string):\n    \"\"\"\n    Tokenization\/string cleaning for dataset\n    Every dataset is lower cased except\n    \"\"\"\n    string = re.sub(r\"\\n\", \"\", string)    \n    string = re.sub(r\"\\r\", \"\", string) \n    string = re.sub(r\"[0-9]\", \"digit\", string)\n    string = re.sub(r\"\\'\", \"\", string)    \n    string = re.sub(r\"\\\"\", \"\", string)    \n    return string.strip().lower()","4d5ef1cd":"#train test split\nfrom sklearn.model_selection import train_test_split\nX = []\nfor i in range(train_10_ana.shape[0]):\n    X.append(clean_str(train_10_ana.iloc[i][0]))\ny = np.array(train_10_ana[\"First_Tag\"])\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=5)","a5648269":"#feature engineering and model selection\nfrom sklearn.svm import LinearSVC\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier","b529fb47":"#pipeline of feature engineering and model\nmodel = Pipeline([('vectorizer', CountVectorizer()),\n    ('tfidf', TfidfTransformer()),\n    ('clf', OneVsRestClassifier(LinearSVC(class_weight=\"balanced\")))])","5123e6cf":"#paramater selection\nfrom sklearn.model_selection import GridSearchCV\nparameters = {'vectorizer__ngram_range': [(1, 1), (1, 2),(2,2)],\n               'tfidf__use_idf': (True, False)}","0c276a1e":"gs_clf_svm = GridSearchCV(model, parameters, n_jobs=-1)\ngs_clf_svm = gs_clf_svm.fit(X, y)\nprint(gs_clf_svm.best_score_)\nprint(gs_clf_svm.best_params_)\n","5cf1ada4":"#preparing the final pipeline using the selected parameters\nmodel = Pipeline([('vectorizer', CountVectorizer(ngram_range=(1,2))),\n    ('tfidf', TfidfTransformer(use_idf=True)),\n    ('clf', OneVsRestClassifier(LinearSVC(class_weight=\"balanced\")))])","b00b8fa7":"#fit model with training data\nmodel.fit(X_train, y_train)","e7b32170":"#evaluation on test data\npred = model.predict(X_test)","35b3c85c":"model.classes_","7805881c":"from sklearn.metrics import confusion_matrix, accuracy_score\nconfusion_matrix(pred, y_test)","a45b0a5d":"accuracy_score(y_test, pred)","f1345b60":"#### Tokenization\n* Tokenization is the process of splitting the given text into smaller pieces called tokens.","624ac0a3":"## Table of Contents:\n\n1. Dataset Preparation: The first step is the Dataset Preparation step which includes the process of loading a dataset and performing basic pre-processing. The dataset is then splitted into train and validation sets.\n2. Feature Engineering: The next step is the Feature Engineering in which the raw dataset is transformed into flat features which can be used in a machine learning model. This step also includes the process of creating new features from the existing data.\n3. Model Training: The final step is the Model Building step in which a machine learning model is trained on a labelled dataset.\n4. Improve Performance of Text Classifier: In this article, we will also look at the different ways to improve the performance of text classifiers.","4dd7732f":"#### Remove numbers","204f68d2":"#### StopWord Removal + Tokenization ","54bd87c6":"#### Convert text to lowercase","80f3ed65":"* Now let\u2019s stitch these tokens back together.","2c3137af":"#### Count Vectors as features","d47631ee":"### Loading the Training Data","2d531f93":"#### Removing punctuations, accent marks and other diacritics","deff4fe6":"* converting all letters to lower or upper case\n* converting numbers into words or removing numbers\n* removing punctuations, accent marks and other diacritics\n* removing white spaces\n* expanding abbreviations\n* removing stop words, sparse terms, and particular words\n* text canonicalization","afae7a4d":"### Unique Value Count for each feature","979f558f":"### We will now carry out all the Text Preprocessing steps on the 'Business Description' field","1020dcf2":"#### Remove whitespaces"}}