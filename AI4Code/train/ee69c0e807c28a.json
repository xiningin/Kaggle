{"cell_type":{"daa5d24d":"code","99bc82be":"code","8f122f8a":"code","6a0e3d0b":"code","5666d2f3":"code","c1fa738b":"code","27ce7fca":"code","a0b56a7e":"code","d1f51111":"code","6f6aef1c":"code","a8f0e766":"code","d9192198":"code","451baa44":"code","0f61027e":"code","c4aa84d7":"markdown","2f58e9a4":"markdown","3ca1d58d":"markdown","0ff21ae6":"markdown","ef7087a6":"markdown","b70a9889":"markdown"},"source":{"daa5d24d":"# Let's install Feature-engine\n# this package will allow us to quickly remove \n# non-predictive variables\n\n!pip install feature-engine","99bc82be":"import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n# to sample the hyperparameter space based on distributions\nfrom scipy import stats\n\n# I use GBM because it usually out-performs other off-the-shelf \n# classifiers\nfrom sklearn.ensemble import GradientBoostingClassifier\n\n# metric to optimize for the competition\nfrom sklearn.metrics import roc_auc_score\n\n# to optimize the hyperparameters we import the randomized search class\nfrom sklearn.model_selection import (\n    cross_val_score,\n    train_test_split,\n)\n\n# to assemble various procedures in sequence\nfrom sklearn.pipeline import Pipeline\n\n# some methods to work with imbalanced data are based in nearest neighbours\n# and nearest neighbours are sensitive to the magnitude of the features\n# so we need to scale the data\nfrom sklearn.preprocessing import MinMaxScaler\n\n# import selection classes from Feature-engine\n# to reduce the number of features\nfrom feature_engine.selection import (\n    DropDuplicateFeatures,\n    DropConstantFeatures,\n)\n\n\n# over-sampling techniques for imbalanced data\nfrom imblearn.over_sampling import (\n    RandomOverSampler,\n    SMOTENC,\n)\n\n# under-sampling techniques for imbalanced data\nfrom imblearn.under_sampling import (\n    RandomUnderSampler,\n    InstanceHardnessThreshold,\n)\n\n# special ensemble methods to work with imbalanced data\n# we will use those based on boosting, which tend to work better\nfrom imblearn.ensemble import (\n    RUSBoostClassifier,\n    EasyEnsembleClassifier,\n)\n\n# to put the undersampling methods and the GBM together\nfrom imblearn.pipeline import make_pipeline\n\nimport optuna","8f122f8a":"# load the Santander Customer Satisfaction dataset\n\ndata = pd.read_csv('\/kaggle\/input\/santander-customer-satisfaction\/train.csv')","6a0e3d0b":"# separate dataset into train and test sets\n# I split 30:70 mostly to reduce the size of the train set\n# so that this notebook does not run out of memory :_(\n\nX_train, X_test, y_train, y_test = train_test_split(\n    data.drop(labels=['ID','TARGET'], axis=1),\n    data['TARGET'],\n    test_size=0.7,\n    random_state=0)\n\nX_train.shape, X_test.shape","5666d2f3":"# check class imbalance\n\ny_train.value_counts(normalize=True), y_train.value_counts()","c1fa738b":"# to remove constant, quasi-constant and duplicated features\n# we use the transformers from Feature-engine\n\npipe = Pipeline([\n    ('constant', DropConstantFeatures(tol=0.98)), # drops constant and quasi-constant features\n    ('duplicated', DropDuplicateFeatures()), # drops duplicates\n])\n\n# find features to remove\npipe.fit(X_train, y_train)","27ce7fca":"print('Number of original variables: ', X_train.shape[1])\n\n# see how with the pipeline we can apply all transformers in sequence\n# with one line of code, for each data set\nX_train = pipe.transform(X_train)\nX_test = pipe.transform(X_test)\n\nprint('Number of variables after selection: ', X_train.shape[1])","a0b56a7e":"# we need to capture the index of the discrete variables\n# for SMOTENC\n\n# make list of discrete variables\ncat_vars = [var for var in X_train.columns if X_train[var].nunique() <= 10]\n\n# capture the index in the dataframe columns\ncat_vars_index = [cat_vars.index(x) for x in cat_vars]\n\ncat_vars_index[0:6]","d1f51111":"# the objective function takes the hyperparameter space\n# as input, which in Optuna is given by the trial object\n\ndef objective(trial):\n    \n    # the method to use is a hyperparameter to optimize\n    method = trial.suggest_categorical(\n        \"method\",[\"ros\", \"smote\",'rus', 'iht',\n                  'rusboost', 'easyensemble',\n                  'gbm', 'cost_sensitive'],\n    )\n    \n    \n    if method == \"ros\":\n        \n        model = make_pipeline(\n            \n            # random oversampling\n            RandomOverSampler(random_state=1),\n            \n            # GBM\n            GradientBoostingClassifier(\n                n_estimators = trial.suggest_int(\"ros_n_estimators\", 10, 200),\n                max_depth = trial.suggest_int(\"ros_max_depth\", 1, 5),\n                learning_rate = trial.suggest_float('ros_learning_rate', 0.0001, 1),\n                random_state=0,\n            )\n        )\n        \n    if method == \"smote\":\n        \n        model = make_pipeline(\n            \n            # scaler\n            MinMaxScaler(),\n            \n            # smote\n            SMOTENC(random_state=0,\n                   categorical_features=cat_vars_index,\n                   ),\n            \n            # GBM\n            GradientBoostingClassifier(\n                n_estimators = trial.suggest_int(\"smote_n_estimators\", 10, 200),\n                max_depth = trial.suggest_int(\"smote_max_depth\", 1, 5),\n                learning_rate = trial.suggest_float('smote_learning_rate', 0.0001, 1),\n                random_state=0,\n            )\n        )\n        \n    if method == 'rus':\n        \n        model = make_pipeline(\n            \n            # random undersampling\n            RandomUnderSampler(random_state=1),\n            \n            # GBM\n            GradientBoostingClassifier(\n                n_estimators = trial.suggest_int(\"rus_n_estimators\", 10, 200),\n                max_depth = trial.suggest_int(\"rus_max_depth\", 1, 5),\n                learning_rate = trial.suggest_float('rus_learning_rate', 0.0001, 1),\n                random_state=0,\n            )\n        )\n        \n    if method == 'iht':\n        \n        gbm = GradientBoostingClassifier(\n                n_estimators = trial.suggest_int(\"iht_n_estimators\", 10, 200),\n                max_depth = trial.suggest_int(\"iht_max_depth\", 1, 5),\n                learning_rate = trial.suggest_float('iht_learning_rate', 0.0001, 1),\n                random_state=0,\n            )\n            \n        model = make_pipeline(\n            \n            # instance hardness threshold\n            InstanceHardnessThreshold(\n                estimator = gbm,\n                random_state = 1,\n                cv = 2,  # cross validation fold, 2 to speed things up.\n            ),\n        \n            # GBM\n            gbm,\n        )\n        \n        \n    if method == 'rusboost':\n        \n        model = RUSBoostClassifier(\n            n_estimators = trial.suggest_int(\"rusboost_n_estimators\", 5, 30),\n            learning_rate = trial.suggest_float('rusboost_learning_rate', 0.0001, 1),\n            random_state = 2909,\n    )\n        \n    if method=='easyensemble':       \n\n        model = EasyEnsembleClassifier(\n                n_estimators = trial.suggest_int(\"easy_n_estimators\", 5, 30),\n                random_state = 2909,\n            )\n        \n    if method == 'gbm':\n        \n        model = GradientBoostingClassifier(\n                    n_estimators = trial.suggest_int(\"gbm_n_estimators\", 10, 200),\n                    max_depth = trial.suggest_int(\"gbm_max_depth\", 1, 5),\n                    learning_rate = trial.suggest_float('gbm_learning_rate', 0.0001, 1),\n                    random_state = 0,\n            )\n\n    \n    if method == 'cost_sensitive':\n        \n        model = GradientBoostingClassifier(\n                    n_estimators = trial.suggest_int(\"cs_n_estimators\", 10, 200),\n                    max_depth = trial.suggest_int(\"cs_max_depth\", 1, 5),\n                    learning_rate = trial.suggest_float('cs_learning_rate', 0.0001, 1),\n                    random_state = 0,\n            )\n        \n        sample_weight = np.where(y_train == 1, 95, 5)\n        \n        score = cross_val_score(\n            estimator = model,\n            X = X_train,\n            y = y_train,\n            fit_params = {'sample_weight': sample_weight},\n            scoring='roc_auc',\n            cv=3,\n        )\n    \n    else: \n        \n        score = cross_val_score(model, X_train, y_train, scoring='roc_auc', cv=3)\n    \n    roc = score.mean()\n    \n    return roc","6f6aef1c":"# we set up the study\nstudy = optuna.create_study(\n    direction=\"maximize\",\n)\n\n\n# and now we want to maximize the roc-auc\n# we run 15 trials otherwise the kernel runs out of memory.\n\n# the more trials we run the greater the chances to find the best hyperparams\n\nstudy.optimize(objective, n_trials=15)","a8f0e766":"# we find the best parameters here\n\nstudy.best_params","d9192198":"# the best roc-auc\n\nstudy.best_value","451baa44":"# we can find out how many of each method the search tested\n\nresults = study.trials_dataframe()\n\nresults['params_method'].value_counts()","0f61027e":"# we can plot the maximization of the roc-auc\n\nresults['value'].sort_values().reset_index(drop=True).plot()\nplt.title('Convergence plot')\nplt.xlabel('Iteration')\nplt.ylabel('ROC-AUC')","c4aa84d7":"# Predicting Customer Satisfaction with Imbalanced Data and Hyperparameter Optimization\n\nIn a [previous notebook](https:\/\/www.kaggle.com\/solegalli\/customer-satisfaction-with-imbalanced-data) I applied various techniques to improve the performance of models trained on imbalanced datasets. I applied each technique separately, searching for the best hyperparameters in each case, using randomized search.\n\n**But what if, the technique to improve model performance was in itself another hyperparameter?**\n\n**What if we could write code, that automatically was able to find which technique would work best in our data?**\n\nThis is what we are going to do in this notebooks. We will write code where each technique is an additional hyperparameter that we can optimize. And because now training the models turns more computationally costly and we have more hyperparameters, instead of Randomized search we will performe Bayesian optimization of the hyperparameters, a method that guides the search towards more promising values of the hyperparameters.\n\nWe will use Optuna for the optimization, because it allows us to define hyperparameters on the fly, with its \"define-by-run\" API design.\n\nSo, let's get started!\n\nPS: If you want to know more about hyperparameter optimization or working with imbalanced datasets, feel free to check my [online courses](https:\/\/www.trainindata.com\/).\n","2f58e9a4":"In the above plot we see that the search for the best roc-auc has not plateaued, which means that there is still room for improvement, if we run more iterations of the search.","3ca1d58d":"## Load the data","0ff21ae6":"Note how we reduced the size from almost 370 to 130 features.\n\n## Techniques for imbalanced data\n\nWe will test the following methods:\n\n* Random Oversampling\n* Creating synthetic observations with SMOTE\n* Random Undersampling\n* Cleaning noisy observations with Instance Hardness\n* RUSBoost, special ensemble models for imbalanced data\n* Easy Ensemble, special ensemble models for imbalanced data\n* Vanilla GBM\n* GBM with cost sensitive learning\n\n\nWe start by writing the objective function that we want to optimize, which takes:\n\n* the hyperparameters\n* the models\n* the metric to optimize\n* the cross-validation scheme","ef7087a6":"## Target\n\nThe target class is imbalanced. The value 1 refers to un-satisfied customers and 0 to satisfied. So most of Santander's customers are satisfied.","b70a9889":"## Drop constant and duplicated features\n\nThis dataset contains constant and duplicated features. I know this from previous analysis so I will quickly remove these features to reduce the data size.\n\nI will also remove quasi-constant features to reduce the size of the data set, otherwise the kernel runs out of memory.\n\nMore insight about feature selection for this dataset here: https:\/\/www.kaggle.com\/solegalli\/feature-selection-with-feature-engine\n"}}