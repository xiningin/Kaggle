{"cell_type":{"574ffaae":"code","34642179":"code","c900675e":"code","9652a350":"code","a01bda97":"code","8a80ed47":"code","8bc98f28":"code","cc7b763a":"code","43da9d26":"code","a17e9fc3":"code","84d871e8":"code","4bfe6f77":"code","7d91989e":"code","2ff6ceef":"code","9bc5785b":"code","035fd1c7":"code","ae18925f":"code","006adfb6":"code","86e133a3":"code","eab29a0a":"code","e449fce3":"code","7bbbe7bc":"code","da16249f":"code","4a89d5ab":"markdown","dce2f111":"markdown"},"source":{"574ffaae":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","34642179":"train = pd.read_csv('.\/..\/input\/spooky-author-identification\/train.zip')\ntest = pd.read_csv('.\/..\/input\/spooky-author-identification\/test.zip')","c900675e":"train.head()","9652a350":"test.head()","a01bda97":"train.shape,test.shape","8a80ed47":"one_hot = pd.get_dummies(train['author'])","8bc98f28":"one_hot.head()","cc7b763a":"X_train = train['text'].values\ny = train.join(one_hot)[['EAP','HPL','MWS']].values\nX_test = test['text'].values","43da9d26":"import tensorflow as tf\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences","a17e9fc3":"vocab_size = 25000 #Number of unique words to use\nmax_len = 250 #length of sentence ","84d871e8":"tokenizer = Tokenizer(num_words=vocab_size)","4bfe6f77":"tokenizer.fit_on_texts(X_train)","7d91989e":"len(tokenizer.word_index)","2ff6ceef":"tokenized_train = tokenizer.texts_to_sequences(X_train)\ntokenized_test = tokenizer.texts_to_sequences(X_test)","9bc5785b":"X_train = pad_sequences(tokenized_train,maxlen=max_len)\nX_test = pad_sequences(tokenized_test,maxlen=max_len)","035fd1c7":"emd_dim = 32","ae18925f":"model = tf.keras.Sequential([\n    tf.keras.layers.Embedding(vocab_size,emd_dim,input_length=max_len),\n    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(50,return_sequences=True)),\n    tf.keras.layers.GlobalMaxPool1D(),\n    tf.keras.layers.Dense(50, activation='relu'),\n    tf.keras.layers.Dropout(0.1),\n    tf.keras.layers.Dense(3, activation='sigmoid')\n])","006adfb6":"model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])","86e133a3":"model.summary()","eab29a0a":"batch_size = 264\nepochs = 10\nhistory = model.fit(X_train,y, batch_size=batch_size, epochs=epochs, validation_split=0.2)","e449fce3":"import matplotlib.pyplot as plt\n\n\ndef plot_graphs(history, string):\n    plt.plot(history.history[string])\n    plt.plot(history.history['val_'+string])\n    plt.xlabel(\"Epochs\")\n    plt.ylabel(string)\n    plt.legend([string, 'val_'+string])\n    plt.show()","7bbbe7bc":"plot_graphs(history,'accuracy')","da16249f":"plot_graphs(history,'loss')","4a89d5ab":"### Preprocessing for Model","dce2f111":"### Model - LSTM"}}