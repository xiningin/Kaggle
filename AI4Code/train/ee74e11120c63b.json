{"cell_type":{"b058527b":"code","5f5f9027":"code","c762a801":"code","6dbdfc83":"code","68bd73b2":"code","faaeed2b":"code","e488d33e":"code","2c73e9a4":"code","4b3b7840":"code","3bbbad51":"code","b29d8ddc":"code","e11cac7a":"code","189df843":"code","bcb167f5":"code","60c86a26":"code","38797211":"code","152b943a":"code","b6791bce":"code","5d455f7f":"code","454f7cfb":"code","895a93fa":"code","bf2ab5fe":"code","1b5da2d2":"code","65198608":"code","f40125ac":"code","2ef1fadf":"code","4b44bb7a":"code","8abeb14c":"code","66e48adc":"code","632489a9":"code","d0585815":"code","6a44515b":"code","3babd3e0":"code","13d9fcb7":"code","871535d9":"code","41f660f4":"code","706b7dcc":"code","88557f4a":"markdown","948bf58b":"markdown","e8dc2d0f":"markdown","ec051e91":"markdown","74fb9f37":"markdown","8b9bddf9":"markdown","89135995":"markdown","7c924040":"markdown","946b96a9":"markdown","c604b1b1":"markdown","d0d51234":"markdown","fd741b66":"markdown","1e109289":"markdown","f4bdc7a9":"markdown","bc1bd226":"markdown"},"source":{"b058527b":"from fastai.vision import *\nimport torchvision.transforms as T\n\n# from own.show_filts import show_filts","5f5f9027":"import matplotlib.pyplot as plt\nimport math\n\ndef show_filts(model, n_cols=32, figsize=(24,2), global_scale=True):\n    filts = list(model.parameters())[0]\n    filts = filts.detach().cpu().numpy().transpose(0,2,3,1)\n    n_filts = filts.shape[0]\n    \n    n_rows = math.ceil(n_filts\/n_cols)\n    fig, axs = plt.subplots(n_rows, n_cols, figsize=figsize)\n    axs = axs.flatten()\n    for i, ax in enumerate(axs):\n        ax = axs[i]\n        if i < n_filts:\n            filt = filts[i]\n            ax.axis('off')\n            if global_scale:\n                filt = (filt - filts.min())\/max(filts.max() - filts.min(), 1e-6)\n            else:\n                filt = (filt - filt.min())\/max(filt.max() - filt.min(), 1e-6)\n            ax.imshow(filt)\n        else:\n            fig.delaxes(ax)","c762a801":"path = Path('\/media\/nofreewill\/Datasets_nvme\/Visual\/Imagenet-sz\/80')\npath.ls()","6dbdfc83":"class AugDS(Dataset):\n    def __init__(self, folder, tfms, recurse=False):\n        self.x = get_image_files(folder, recurse=recurse)\n        self.tfms = tfms\n    \n    def __len__(self):\n        return len(self.x)\n    \n    def __getitem__(self, idx):\n        img = PIL.Image.open(self.x[idx]).convert('RGB')\n        img1 = self.tfms(img)\n        img2 = self.tfms(img)\n        return torch.stack((img1, img2)), torch.tensor([0, 0])","68bd73b2":"size = 64\ntfms = T.Compose([T.RandomResizedCrop(size, scale=(0.5,1.)),\n                  T.RandomOrder([T.RandomGrayscale(0.1),\n                                 T.ColorJitter(.5,.5,.5,.1),\n                                 T.RandomHorizontalFlip(0.5),\n                                 T.RandomRotation(10., resample=2),\n                                ]),\n                  T.ToTensor(),\n                 ])","faaeed2b":"ds_train = AugDS(path\/'train', recurse=True, tfms=tfms)","e488d33e":"ds_valid = AugDS(path\/'val', recurse=True, tfms=tfms)","2c73e9a4":"def collate_fn(batch):\n    inp = [x[0] for x in batch]\n    targ = [x[1] for x in batch]\n    inp = torch.cat(inp)\n    targ = torch.cat(targ)\n    return inp, targ","4b3b7840":"data = ImageDataBunch.create(ds_train, ds_valid, bs=512, collate_fn=collate_fn)","3bbbad51":"x, y = data.train_ds[0]\nx.shape","b29d8ddc":"T.ToPILImage()(x[0])","e11cac7a":"T.ToPILImage()(x[1])","189df843":"model = models.resnet18()\nmodel.conv1, model.fc","bcb167f5":"h, s = 512, 256\nmodel.fc = nn.Sequential(nn.Linear(model.fc.in_features, h),\n                        nn.ReLU(inplace=True),\n                        nn.Linear(h, s),\n                        )\nmodel.fc","60c86a26":"device = data.device","38797211":"def loss_fn(inp, targ):\n    thau = 1.\n    # Similarities\n    inp_norm = inp \/ inp.norm(dim=1)[:,None]\n    simils = torch.mm(inp_norm, inp_norm.transpose(0,1))\n    # Good and Bad\n    N = len(inp_norm)\n    eye = (2*torch.eye(N)-1).to(device)\n    eye[torch.arange(1,N, step=2), torch.arange(0,N, step=2)] = 1\n    eye[torch.arange(0,N, step=2), torch.arange(1,N, step=2)] = 1\n    #\n    exps = torch.exp(simils)\n    num = ((exps\/thau)*(eye+1)\/2).sum(dim=1)\n    den = ((exps\/thau)*(-eye+1)\/2).sum(dim=1)\n    loss = (-torch.log(num\/den)).mean()\n    return loss","152b943a":"learn = Learner(data, model, loss_func=loss_fn)","b6791bce":"show_filts(learn.model)","5d455f7f":"learn.lr_find()\nlearn.recorder.plot()","454f7cfb":"learn.fit_one_cycle(1, 3e-3)","895a93fa":"show_filts(learn.model)","bf2ab5fe":"show_filts(learn.model, global_scale=False)","1b5da2d2":"learn.recorder.plot_losses()","65198608":"path = untar_data(URLs.IMAGENETTE)\npath.ls()","f40125ac":"data = ImageDataBunch.from_folder(path, valid='val', size=224, ds_tfms=get_transforms())","2ef1fadf":"model.fc","4b44bb7a":"model.fc = nn.Linear(model.fc[0].in_features, data.c)\nmodel.fc","8abeb14c":"learn = Learner(data, learn.model, metrics=[accuracy])","66e48adc":"learn.lr_find()\nlearn.recorder.plot()","632489a9":"learn.fit_one_cycle(4, 1e-3)","d0585815":"show_filts(learn.model)","6a44515b":"model = models.resnet18()\nmodel.fc = nn.Linear(model.fc.in_features, data.c)","3babd3e0":"show_filts(model)","13d9fcb7":"learn = Learner(data, model, metrics=[accuracy])","871535d9":"learn.lr_find()\nlearn.recorder.plot()","41f660f4":"learn.fit_one_cycle(4, 1e-3)","706b7dcc":"show_filts(learn.model)","88557f4a":"# Finetune to IMAGENETTE","948bf58b":"Train loss: 0.748","e8dc2d0f":"# Data","ec051e91":"# Model","74fb9f37":"# Description","8b9bddf9":"Train loss: 0.502","89135995":"# Imports","7c924040":"# Training from random weights","946b96a9":"See the results here:\n\nhttps:\/\/gist.github.com\/nofreewill42\/84d9fb31d06d123879143f8b3699f1c3","c604b1b1":"# Loss","d0d51234":"# Train","fd741b66":"### Sample of augmentation","1e109289":"In this notebook, I pre-train a model on 64x64 images of ImageNet using SimCLR-motivated unsupervised pre-training.\n\nThen I fine-tune it on IMAGENETTE and compare the results with training the model from random weights.","f4bdc7a9":"#### Model first layer visualization","bc1bd226":"Validation accuracy: 79% vs. 84.5%\n\nLooks promising"}}