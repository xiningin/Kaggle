{"cell_type":{"38f9d0ba":"code","b55acbd1":"code","adade2b5":"code","0a2dfbd7":"code","91ff73dd":"code","65caab3d":"code","d16fe139":"code","9b6b8fbf":"code","bb2103b7":"code","d1f5632a":"code","fe425ad1":"code","5a4a42d9":"code","ff91f8a8":"code","58dc77e2":"code","efa9e9eb":"code","30e3ad2c":"code","02dbd961":"code","83815b35":"code","b11afcda":"code","685fa27c":"code","365ec8cf":"code","7dfda60d":"code","90c74fb6":"code","b4aef6f7":"code","308629a7":"code","c69bdd87":"code","dc0e2db4":"code","b3fe2bb0":"code","aef0088d":"code","af83494f":"code","772d9dcf":"code","365a8b83":"code","66761ea9":"code","5804b430":"code","eaca4e5b":"code","e7ac42b3":"code","deb6c1bf":"code","8380d36b":"code","eff01eb1":"code","2490ee7b":"code","e60367cd":"code","911fbc26":"code","b4971491":"code","16dde57b":"code","33626400":"code","5e176818":"code","d1698b67":"code","f3278f76":"code","2cf6a37b":"code","80ce15d2":"code","917f0bdd":"code","e90610f3":"code","98765168":"markdown","740f85e2":"markdown","08f3672a":"markdown","e0902f68":"markdown","74c1854e":"markdown","3437c494":"markdown","c525b512":"markdown","04afc4d0":"markdown","0a3d6536":"markdown","127b6840":"markdown","4bf6c721":"markdown","12407bcc":"markdown","ee34bd1e":"markdown","84fecba1":"markdown","7392144c":"markdown","00a945c8":"markdown","f610fbac":"markdown","2ce642d6":"markdown","8ee3abbf":"markdown","c28c3911":"markdown","4d707a01":"markdown","e5d32d39":"markdown"},"source":{"38f9d0ba":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","b55acbd1":"import re\nimport string\n\nimport nltk\nfrom nltk.probability import FreqDist\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import stopwords\nfrom nltk.stem import SnowballStemmer\nfrom nltk import pos_tag\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.tokenize import word_tokenize\n\nfrom wordcloud import WordCloud\nfrom tqdm.auto import tqdm\nimport matplotlib.style as style\nstyle.use('fivethirtyeight')\nfrom sklearn.metrics import plot_roc_curve\nfrom numpy import interp\nfrom itertools import cycle\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import roc_curve, auc","adade2b5":"df = pd.read_csv('..\/input\/one-million-reddit-confessions\/one-million-reddit-confessions.csv', encoding='ISO-8859-2')\ndf.head()","0a2dfbd7":"df.isnull().sum()","91ff73dd":"#2nd row, 11th column \n\ndf.iloc[1,10]","65caab3d":"df['subreddit.nsfw'].value_counts()","d16fe139":"#4th row, tenth column \n\ndf.iloc[3,9]","9b6b8fbf":"df['subreddit.name'].value_counts()","bb2103b7":"#Code by Lucas Abrah\u00e3o https:\/\/www.kaggle.com\/lucasabrahao\/trabalho-manufatura-an-lise-de-dados-no-brasil\n\ndf[\"domain\"].value_counts()[:10].plot.barh(color='red', title='Domain');","d1f5632a":"#4th row, 11th column \n\ndf.iloc[3,10]","fe425ad1":"#Code by Lucas Abrah\u00e3o https:\/\/www.kaggle.com\/lucasabrahao\/trabalho-manufatura-an-lise-de-dados-no-brasil\n\ndf[\"subreddit.name\"].value_counts().plot.barh(color='green', title='Subreddit Name');","5a4a42d9":"#Code by Leon Wolber https:\/\/www.kaggle.com\/leonwolber\/reddit-nlp-topic-modeling-prediction\n\nplt.figure(figsize=(13,5))\n\nsns.kdeplot(df['score'], shade=  True);","ff91f8a8":"#Code by Leon Wolber https:\/\/www.kaggle.com\/leonwolber\/reddit-nlp-topic-modeling-prediction\n\nprint(len(df[df['score'] < 10]), 'Posts with less than 10 votes')\nprint(len(df[df['score'] > 10]), 'Posts with more than 10 votes')","58dc77e2":"#Code by Leon Wolber https:\/\/www.kaggle.com\/leonwolber\/reddit-nlp-topic-modeling-prediction\n\n# post with the most comments\n\ndf[df['score'] == df['score'].max()]['title'].iloc[0]","efa9e9eb":"#Code by Leon Wolber https:\/\/www.kaggle.com\/leonwolber\/reddit-nlp-topic-modeling-prediction\n\ndef remove_line_breaks(text):\n    text = text.replace('\\r', ' ').replace('\\n', ' ')\n    return text\n\n#remove punctuation\ndef remove_punctuation(text):\n    re_replacements = re.compile(\"__[A-Z]+__\")  # such as __NAME__, __LINK__\n    re_punctuation = re.compile(\"[%s]\" % re.escape(string.punctuation))\n    '''Escape all the characters in pattern except ASCII letters and numbers'''\n    tokens = word_tokenize(text)\n    tokens_zero_punctuation = []\n    for token in tokens:\n        if not re_replacements.match(token):\n            token = re_punctuation.sub(\" \", token)\n        tokens_zero_punctuation.append(token)\n    return ' '.join(tokens_zero_punctuation)\n\ndef remove_special_characters(text):\n    text = re.sub('[^a-zA-z0-9\\s]', '', text)\n    return text\n\ndef lowercase(text):\n    text_low = [token.lower() for token in word_tokenize(text)]\n    return ' '.join(text_low)\n\ndef remove_stopwords(text):\n    stop = set(stopwords.words('english'))\n    word_tokens = nltk.word_tokenize(text)\n    text = \" \".join([word for word in word_tokens if word not in stop])\n    return text\n\n#remobe one character words\ndef remove_one_character_words(text):\n    '''Remove words from dataset that contain only 1 character'''\n    text_high_use = [token for token in word_tokenize(text) if len(token)>1]      \n    return ' '.join(text_high_use)   \n    \n#%%\n# Stemming with 'Snowball stemmer\" package\ndef stem(text):\n    stemmer = nltk.stem.snowball.SnowballStemmer('english')\n    text_stemmed = [stemmer.stem(token) for token in word_tokenize(text)]        \n    return ' '.join(text_stemmed)\n\ndef lemma(text):\n    wordnet_lemmatizer = WordNetLemmatizer()\n    word_tokens = nltk.word_tokenize(text)\n    text_lemma = \" \".join([wordnet_lemmatizer.lemmatize(word) for word in word_tokens])       \n    return ' '.join(text_lemma)\n\n\n#break sentences to individual word list\ndef sentence_word(text):\n    word_tokens = nltk.word_tokenize(text)\n    return word_tokens\n#break paragraphs to sentence token \ndef paragraph_sentence(text):\n    sent_token = nltk.sent_tokenize(text)\n    return sent_token    \n\n\ndef tokenize(text):\n    \"\"\"Return a list of words in a text.\"\"\"\n    return re.findall(r'\\w+', text)\n\ndef remove_numbers(text):\n    no_nums = re.sub(r'\\d+', '', text)\n    return ''.join(no_nums)\n\n\n\ndef clean_text(text):\n    _steps = [\n    remove_line_breaks,\n    remove_one_character_words,\n    remove_special_characters,\n    lowercase,\n    remove_punctuation,\n    remove_stopwords,\n    stem,\n    remove_numbers\n]\n    for step in _steps:\n        text=step(text)\n    return text   \n#%%","30e3ad2c":"#https:\/\/stackoverflow.com\/questions\/55557004\/getting-attributeerror-float-object-has-no-attribute-replace-error-while\n#To avoid with tqdm AttributeError: 'float' object has no attribute\n\ndf[\"title\"] = df[\"title\"].astype(str)\ndf[\"title\"] = [x.replace(':',' ') for x in df[\"title\"]]","02dbd961":"df['clean_title'] = pd.Series([clean_text(i) for i in tqdm(df['title'])])","83815b35":"words = df[\"clean_title\"].values","b11afcda":"#Code by Leon Wolber https:\/\/www.kaggle.com\/leonwolber\/reddit-nlp-topic-modeling-prediction\n\nls = []\n\nfor i in words:\n    ls.append(str(i))","685fa27c":"ls[:5]","365ec8cf":"#Code by Leon Wolber https:\/\/www.kaggle.com\/leonwolber\/reddit-nlp-topic-modeling-prediction\n\n# The wordcloud \nplt.figure(figsize=(16,13))\nwc = WordCloud(background_color=\"black\", max_words=1000, max_font_size= 200,  width=1600, height=800)\nwc.generate(\" \".join(ls))\nplt.title(\"Most discussed terms\", fontsize=20)\nplt.imshow(wc.recolor( colormap= 'viridis' , random_state=17), alpha=0.98, interpolation=\"bilinear\", )\nplt.axis('off')","7dfda60d":"#Code by Leon Wolber https:\/\/www.kaggle.com\/leonwolber\/reddit-nlp-topic-modeling-prediction\n\nmost_pop = df.sort_values('score', ascending =False)[['title', 'score']].head(12)\n\nmost_pop['score1'] = most_pop['score']\/1000","90c74fb6":"#Code by Leon Wolber https:\/\/www.kaggle.com\/leonwolber\/reddit-nlp-topic-modeling-prediction\n\nplt.figure(figsize = (20,25))\n\nsns.barplot(data = most_pop, y = 'title', x = 'score1', color = 'c')\nplt.xticks(fontsize=27, rotation=0)\nplt.yticks(fontsize=31, rotation=0)\nplt.xlabel('Votes in Thousands', fontsize = 21)\nplt.ylabel('')\nplt.title('Most popular posts', fontsize = 30)","b4aef6f7":"df.head()","308629a7":"import gensim\nfrom gensim.utils import simple_preprocess\nfrom gensim.parsing.preprocessing import STOPWORDS\nfrom nltk.stem import WordNetLemmatizer, SnowballStemmer\nfrom nltk.stem.porter import *\nimport numpy as np\nnp.random.seed(2018)\nimport nltk","c69bdd87":"stemmer = SnowballStemmer('english')","dc0e2db4":"nltk.download('wordnet')","b3fe2bb0":"#Code by Leon Wolber https:\/\/www.kaggle.com\/leonwolber\/reddit-nlp-topic-modeling-prediction\n\ndef lemmatize_stemming(text):\n    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n\ndef preprocess(text):\n    result = []\n    for token in gensim.utils.simple_preprocess(text):\n        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n            result.append(lemmatize_stemming(token))\n    return result","aef0088d":"df['title'].iloc[0]","af83494f":"#Code by Leon Wolber https:\/\/www.kaggle.com\/leonwolber\/reddit-nlp-topic-modeling-prediction\n\ndoc_sample = df['title'].iloc[1]\nprint('original document: ')\n\nwords = []\n\nfor word in doc_sample.split(' '):\n    words.append(word)\n    \n    \nprint(words)\nprint('\\n\\n tokenized and lemmatized document: ')\nprint(preprocess(doc_sample))","772d9dcf":"df['clean_title'] = df['clean_title'].astype(str)","365a8b83":"#Code by Leon Wolber https:\/\/www.kaggle.com\/leonwolber\/reddit-nlp-topic-modeling-prediction\n\nwords = []\n\nfor i in df['clean_title']:\n        words.append(i.split(' '))","66761ea9":"#Code by Leon Wolber https:\/\/www.kaggle.com\/leonwolber\/reddit-nlp-topic-modeling-prediction\n\ndictionary = gensim.corpora.Dictionary(words)\n\ncount = 0\nfor k, v in dictionary.iteritems():\n    print(k, v)\n    count += 1\n    if count > 10:\n        break","5804b430":"# Filter out tokens in the dictionary by their frequency.\n\ndictionary.filter_extremes(no_below=15, no_above=0.5, keep_n=100000)","eaca4e5b":"bow_corpus = [dictionary.doc2bow(doc) for doc in words]\nbow_corpus[4310]","e7ac42b3":"#Code by Leon Wolber https:\/\/www.kaggle.com\/leonwolber\/reddit-nlp-topic-modeling-prediction\n\nbow_doc_4310 = bow_corpus[4310]\n\nfor i in range(len(bow_doc_4310)):\n    print(\"Word {} (\\\"{}\\\") appears {} time.\".format(bow_doc_4310[i][0], \n                                               dictionary[bow_doc_4310[i][0]], \nbow_doc_4310[i][1]))","deb6c1bf":"#Code by Leon Wolber https:\/\/www.kaggle.com\/leonwolber\/reddit-nlp-topic-modeling-prediction\n\nfrom gensim import corpora, models\n\ntfidf = models.TfidfModel(bow_corpus)\ncorpus_tfidf = tfidf[bow_corpus]\n\nfrom pprint import pprint\n\nfor doc in corpus_tfidf:\n    pprint(doc)\n    break","8380d36b":"#Code by Leon Wolber https:\/\/www.kaggle.com\/leonwolber\/reddit-nlp-topic-modeling-prediction\n\nlda_model = gensim.models.LdaMulticore(bow_corpus,\n                                       num_topics=10,\n                                       id2word=dictionary,\n                                       passes=2,\n                                       workers=2)","eff01eb1":"#Code by Leon Wolber https:\/\/www.kaggle.com\/leonwolber\/reddit-nlp-topic-modeling-prediction\n\nfor idx, topic in lda_model.print_topics(-1):\n    print('Topic: {} \\nWords: {}'.format(idx, topic))","2490ee7b":"#Code by Leon Wolber https:\/\/www.kaggle.com\/leonwolber\/reddit-nlp-topic-modeling-prediction\n\nlda_model_tfidf = gensim.models.LdaMulticore(corpus_tfidf,\n                                             num_topics=10,\n                                             id2word=dictionary,\n                                             passes=2,\n                                             workers=4)\n\nfor idx, topic in lda_model_tfidf.print_topics(-1):\n    print('Topic: {} Word: {}'.format(idx, topic))","e60367cd":"#4th row, tenth column \n\ndf.iloc[2,10]","911fbc26":"#Code by Leon Wolber https:\/\/www.kaggle.com\/leonwolber\/reddit-nlp-topic-modeling-prediction\n\nunseen_document = 'Im a fucking dunce and want to cringe myself out existence'\nbow_vector = dictionary.doc2bow(preprocess(unseen_document))\n\nfor index, score in sorted(lda_model[bow_vector], key=lambda tup: -1*tup[1]):\n    print(\"Score: {}\\t Topic: {}\".format(score, lda_model.print_topic(index, 5)))","b4971491":"df['subreddit.nsfw'] = df['subreddit.nsfw'].astype(int)","16dde57b":"df['subreddit.nsfw'] = pd.Categorical(df['subreddit.nsfw']) ","33626400":"(df['subreddit.nsfw'].value_counts(normalize=True))","5e176818":"from sklearn.model_selection import train_test_split\nfrom sklearn import model_selection, naive_bayes, svm\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import classification_report,confusion_matrix\nfrom sklearn.feature_extraction.text import TfidfVectorizer","d1698b67":"processed_text = df['clean_title']","f3278f76":"#Code by Leon Wolber https:\/\/www.kaggle.com\/leonwolber\/reddit-nlp-topic-modeling-prediction\n\nvectorizer = TfidfVectorizer()\ntfidf = vectorizer.fit_transform(processed_text)\nprint(tfidf.shape)\nprint('\\n')\n#print(vectorizer.get_feature_names())","2cf6a37b":"y = df['subreddit.nsfw']","80ce15d2":"X_train_tf, X_test_tf, y_train_tf, y_test_tf = train_test_split(tfidf, y, test_size=0.2, random_state=42)","917f0bdd":"#Code by Leon Wolber https:\/\/www.kaggle.com\/leonwolber\/reddit-nlp-topic-modeling-prediction\n\n# fit the training dataset on the NB classifier\nNaive = naive_bayes.MultinomialNB()\nNaive.fit(X_train_tf,y_train_tf)\n# predict the labels on validation dataset\npredictions_NB_tf = Naive.predict(X_test_tf)\n# Use accuracy_score function to get the accuracy\nprint(\"Naive Bayes Accuracy -> \",accuracy_score(predictions_NB_tf, y_test_tf)*100)\nprint(classification_report(predictions_NB_tf,y_test_tf))","e90610f3":"#Code by Leon Wolber https:\/\/www.kaggle.com\/leonwolber\/reddit-nlp-topic-modeling-prediction\n\n#Save for the next since in subreddit.nsfw  ALL is False\n\n#logmodel = LogisticRegression()\n#logmodel.fit(X_train_tf, y_train_tf)\n\n#predictions_LR_tf = logmodel.predict(X_test_tf)\n\n#print(\"LR Accuracy -> \",accuracy_score(predictions_LR_tf, y_test_tf)*100)\n#print(classification_report(predictions_LR_tf,y_test_tf))","98765168":"#Create TF\/IDF again","740f85e2":"#That line below takes a lot of time.","08f3672a":"#TF\/IDF","e0902f68":"\"Investigating the phenomenon of NSFW posts in Reddit\"\n\nAuthors: Enrico Corradini, Antonino Nocera, Domenico Ursino Luca Virgili - https:\/\/doi.org\/10.1016\/j.ins.2021.01.062\n\n\"In this paper, the authors studied the characteristics of NSFW (Not Safe For Work) posts in Reddit, highlighting their differences from SFW (Safe For Work) posts, which have been much more studied in the past literature.\"\n\n\"In their investigation, they studied all Reddit posts from 2019. Through both descriptive analytics techniques and social network analysis techniques, they extracted three findings on the main differences between NSFW and SFW posts in Reddit. Thanks to these findings, they are able to better understand the dynamics (authors, subreddits, readers) behind NSFW posts. In particular, it becomes clear that this is a niche world where authors are strongly cohesive.\"\n\n\"However, at the same time, the most popular ones show a clear opening to new authors, whom they are willing to collaborate with, from the beginning.\"\n\n\"The authors defined three main findings related to posts, authors and subreddits, respectively. Some of these findings are made up of several sub-findings.\n\nThe three findings are the following:\n\nPF (Finding on NSFW posts).\n\nNSFW posts are generally published in much fewer subreddits, have much lower scores and are much less commented than SFW (Safe For Work) posts.\n\nThe scores of comments to NSFW posts are much lower than the ones to SFW (Safe For Work) posts.\n\nAF (Finding on NSFW authors).\n\nNSFW\"\n\nhttps:\/\/www.sciencedirect.com\/science\/article\/abs\/pii\/S0020025521001006#:~:text=One%20aspect%20of%20Reddit%20worth,public%20or%20in%20professional%20contexts.","74c1854e":"![](https:\/\/pics.esmemes.com\/all-of-reddit-funny-memes-ok-boomer-funny-memes-all-65328391.png)esmemes.com","3437c494":"#Create the dictionary\n\nEvery unique word in titles","c525b512":"#Natural Language Processing\n\nTopic Modelling","04afc4d0":"#Using list comprehension we clean the titles and append the cleaned text as columns to the df.","0a3d6536":"#Below, after AttributeError: 'float' object has no attribute 'replace'  Nothing worked any more.\n\nSo I had to add that snippet from StackOverflow.","127b6840":"#Below, another snippet that requires time.","4bf6c721":"![](https:\/\/encrypted-tbn0.gstatic.com\/images?q=tbn:ANd9GcRx1dPY3p9VKPbZEBa6RhfmTI_2mndEOb2FxA&usqp=CAU)ecityworks.com","12407bcc":"#wordCloud","ee34bd1e":"#That's special for my brazilian friends.\n\n![](https:\/\/encrypted-tbn0.gstatic.com\/images?q=tbn:ANd9GcQCKHRkcX_fq3IC5Ia2vsutW00kM7ONNOhsiQ&usqp=CAU)askkury.tumblr.com","84fecba1":"#Show the output of the model","7392144c":"#Predict subreddit.nsfw: Not Safe For Work Subreddits","00a945c8":"#NSFW (Not Safe For Work) posts. NSFW subreddits: not safe for work subreddits\n\nNSFW (Not Safe For Work) posts. This term refers to user-submitted content not suitable to be viewed in public or in professional contexts.\n\nhttps:\/\/www.sciencedirect.com\/science\/article\/abs\/pii\/S0020025521001006#:~:text=One%20aspect%20of%20Reddit%20worth,public%20or%20in%20professional%20contexts.","f610fbac":"#Create Corpus -> term document frequency\n\ndoc2bow() simply counts the number of occurrences of each distinct word, converts the word to its integer word ID and returns the result as a sparse vector.","2ce642d6":"#Acknowledgement\n\nLeon Wolber https:\/\/www.kaggle.com\/leonwolber\/reddit-nlp-topic-modeling-prediction","8ee3abbf":"#Cleaning functions","c28c3911":"#It's all about the money! Post with more comments: 600?!?","4d707a01":"#Check class balance\n\nUnfortunately, all is False.","e5d32d39":"#That post below has score 22. Thanks that the fantasies were only about having friends and Not doing friends or whatever else."}}