{"cell_type":{"0bd8cc7c":"code","ac03114e":"code","e978a3a0":"code","e53d535b":"code","6f945a77":"code","ff0d82fe":"code","03f5566f":"code","54a8fd81":"code","53ebd895":"code","7d7510d5":"code","aad89c0e":"code","165cea49":"code","e0d9f03e":"code","6f399992":"code","b0f63e7a":"code","f2fc5b39":"code","9f97ace5":"code","c52a0d44":"code","efd4bdbb":"code","f15feb01":"code","73a7c063":"code","512130ce":"code","38419123":"code","83472a1b":"code","7052477f":"code","b8eab80e":"code","3302cce9":"code","09f9a269":"code","7e8cb728":"code","b71475c0":"code","f39c7ed8":"code","4a44bc3a":"code","b197cb18":"code","50868428":"code","6116ab6e":"code","9881d771":"code","79c640eb":"code","83690998":"code","95d3efc3":"code","564d42dc":"code","73eebe05":"code","5217ddc8":"code","3dfe2657":"code","dc80d50f":"code","4b400fd8":"code","49153e1e":"code","0c4c6572":"code","e24e6157":"code","e84a2764":"code","01472d6a":"code","3301643d":"code","0f995062":"code","d1a7659f":"code","a593131d":"code","0a139fe5":"code","e15ebbde":"code","f2ff638b":"code","c032ba68":"code","e9e1f659":"code","217665a7":"code","25c01884":"code","da91a178":"code","5b636220":"code","d7ef6235":"code","82571745":"code","d2323c93":"code","82b6ff87":"markdown","2222a081":"markdown","2908c53d":"markdown","133308be":"markdown","24003e0f":"markdown","e2556557":"markdown","b8b1664d":"markdown","fbeb56f3":"markdown","e5bf219a":"markdown","d3302f9a":"markdown","e4538efc":"markdown","1dc75e8a":"markdown","05e966b6":"markdown","81e40796":"markdown","42d73a09":"markdown","16a3d3ce":"markdown","6a2957cc":"markdown","784bf2a4":"markdown","167f104e":"markdown","a3788477":"markdown","a951ce05":"markdown"},"source":{"0bd8cc7c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\ndata_history = [] #do not refresh this unless you want to delete the history pf parameters\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","ac03114e":"%matplotlib inline\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nimport numpy as np\nimport pandas as pd\nimport os\nfrom sklearn.preprocessing import MinMaxScaler\nimport math","e978a3a0":"# from tf.keras.models import Sequential  # This does not work!\nfrom tensorflow.python.keras.models import Sequential\nfrom tensorflow.python.keras.layers import Input, Dense, GRU, Embedding\nfrom tensorflow.python.keras.optimizers import RMSprop\nfrom tensorflow.python.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard, ReduceLROnPlateau","e53d535b":"df = pd.read_csv(\"..\/input\/1_MIN_ALL.txt\", index_col=0, sep = ' ')\ndf = df.tail(1088480)\ndf = df.drop('Per',axis = 1)\ndf['Vol'] = df['Vol'].str.replace(\"'\", '')\ndf['DateTime'] = df['Date']*1000000 + df['Time']\ndf['DateTime'] = pd.to_datetime(df['DateTime'], format=\"%Y%m%d%H%M%S\")\ndf = df.reset_index(drop = True)\ndf = df.set_index('DateTime')\ndf = df.drop(['Date','Time'],axis = 1)\ndf = df\ndf = df.head(300000)\ndf.head()\n","6f945a77":"#df['DayOfYear'] = df.index.dayofyear\ndf['HourOfDay'] = df.index.hour\n#df['MonthOfYear'] = df.index.month","ff0d82fe":"df = df.fillna(method='ffill')","03f5566f":"df[['Open','High','Low','Close']] = np.exp(10*df[['Open','High','Low','Close']]) #See if we can help the model out","54a8fd81":"df.describe()","53ebd895":"df.values.shape","7d7510d5":"target_names = ['Close']","aad89c0e":"shift_mn = 1\nshift_steps = shift_mn * 1  # Number of mn.","165cea49":"df_targets = df[target_names].shift(-shift_steps)","e0d9f03e":"x_data = df.values[0:-shift_steps]\nx_data = x_data.astype('float32')","6f399992":"print(type(x_data))\nprint(\"Shape:\", x_data.shape)","b0f63e7a":"y_data = df_targets.values[:-shift_steps]\ny_data = y_data.astype('float32')","f2fc5b39":"print(type(y_data))\nprint(\"Shape:\", y_data.shape)","9f97ace5":"#nb of data rows in the dataset\nnum_data = len(x_data)\nnum_data","c52a0d44":"train_split = 0.9","efd4bdbb":"#This is the number of observations in the test-set\nnum_train = int(train_split * num_data)\nnum_train","f15feb01":"#These are the input-signals for the training- and test-sets\nx_train = x_data[0:num_train]\nx_test = x_data[num_train:]\nlen(x_train) + len(x_test)","73a7c063":"#These are the output-signals for the training- and test-sets\ny_train = y_data[0:num_train]\ny_test = y_data[num_train:]\nlen(y_train) + len(y_test)","512130ce":"#number of input-signals\nnum_x_signals = x_data.shape[1]\nnum_x_signals","38419123":"#number of output-signals\nnum_y_signals = y_data.shape[1]\nnum_y_signals","83472a1b":"print(\"Min:\", np.min(x_train))\nprint(\"Max:\", np.max(x_train))","7052477f":"x_scaler = MinMaxScaler()","b8eab80e":"x_train_scaled = x_scaler.fit_transform(x_train)","3302cce9":"print(\"Min:\", np.min(x_train_scaled))\nprint(\"Max:\", np.max(x_train_scaled))\n","09f9a269":"x_test_scaled = x_scaler.transform(x_test)","7e8cb728":"y_scaler = MinMaxScaler()\ny_train_scaled = y_scaler.fit_transform(y_train)\ny_test_scaled = y_scaler.transform(y_test)","b71475c0":"print(x_train_scaled.shape)\nprint(y_train_scaled.shape)","f39c7ed8":"def batch_generator(batch_size, sequence_length):\n    \"\"\"\n    Generator function for creating random batches of training-data.\n    \"\"\"\n\n    # Infinite loop.\n    while True:\n        # Allocate a new array for the batch of input-signals.\n        x_shape = (batch_size, sequence_length, num_x_signals)\n        x_batch = np.zeros(shape=x_shape, dtype=np.float16)\n\n        # Allocate a new array for the batch of output-signals.\n        y_shape = (batch_size, sequence_length, num_y_signals)\n        y_batch = np.zeros(shape=y_shape, dtype=np.float16)\n\n        # Fill the batch with random sequences of data.\n        for i in range(batch_size):\n            # Get a random start-index.\n            # This points somewhere into the training-data.\n            idx = np.random.randint(num_train - sequence_length)\n            \n            # Copy the sequences of data starting at this index.\n            x_batch[i] = x_train_scaled[idx:idx+sequence_length]\n            y_batch[i] = y_train_scaled[idx:idx+sequence_length]\n        \n        yield (x_batch, y_batch)\n","4a44bc3a":"batch_size = 64","b197cb18":"# We adjust the length to be  interesting for the model to work on it but not to heavy to crush our computer\nsequence_length = 60 * 24 * 7\nsequence_length","50868428":"generator = batch_generator(batch_size=batch_size,\n                            sequence_length=sequence_length)","6116ab6e":"x_batch, y_batch = next(generator)","9881d771":"print(x_batch.shape)\nprint(y_batch.shape)","79c640eb":"#Lets plot it \n\nbatch = 0   # First sequence in the batch.\nsignal = 0  # First signal from the input-signals.\nseq = x_batch[batch, :, signal]\nplt.plot(seq)","83690998":"#plot of the batch we want to predict\nseq = y_batch[batch, :, signal]\nplt.plot(seq)\n","95d3efc3":"validation_data = (np.expand_dims(x_test_scaled, axis=0),\n                   np.expand_dims(y_test_scaled, axis=0))","564d42dc":"model = Sequential()","73eebe05":"model.add(GRU(units= batch_size*2,\n              return_sequences=True,\n              input_shape=(None, num_x_signals,)))","5217ddc8":"# The GRU outputs a batch of sequences of 512 values. We want to predict 1 output-signals, \n# so we add a fully-connected (or dense) layer which maps 512 values down to only 1 values.\nmodel.add(Dense(num_y_signals, activation='sigmoid'))","3dfe2657":"if False:\n    from tensorflow.python.keras.initializers import RandomUniform\n\n    # Maybe use lower init-ranges.\n    init = RandomUniform(minval=-0.05, maxval=0.05)\n\n    model.add(Dense(num_y_signals,\n                    activation='linear',\n                    kernel_initializer=init))","dc80d50f":"warmup_steps = 50","4b400fd8":"def loss_mse_warmup(y_true, y_pred):\n    \"\"\"\n    Calculate the Mean Squared Error between y_true and y_pred,\n    but ignore the beginning \"warmup\" part of the sequences.\n    \n    y_true is the desired output.\n    y_pred is the model's output.\n    \"\"\"\n\n    # The shape of both input tensors are:\n    # [batch_size, sequence_length, num_y_signals].\n\n    # Ignore the \"warmup\" parts of the sequences\n    # by taking slices of the tensors.\n    y_true_slice = y_true[:, warmup_steps:, :]\n    y_pred_slice = y_pred[:, warmup_steps:, :]\n\n    # These sliced tensors both have this shape:\n    # [batch_size, sequence_length - warmup_steps, num_y_signals]\n\n    # Calculate the MSE loss for each value in these tensors.\n    # This outputs a 3-rank tensor of the same shape.\n    loss = tf.losses.mean_squared_error(labels=y_true_slice,\n                                        predictions=y_pred_slice)\n\n    # Keras may reduce this across the first axis (the batch)\n    # but the semantics are unclear, so to be sure we use\n    # the loss across the entire tensor, we reduce it to a\n    # single scalar with the mean function.\n    loss_mean = tf.reduce_mean(loss)\n\n    return loss_mean\n","49153e1e":"optimizer = RMSprop(lr=1e-3)","0c4c6572":"model.compile(loss=loss_mse_warmup, optimizer=optimizer)","e24e6157":"model.summary()","e84a2764":"#This is the callback for writing checkpoints during training.\npath_checkpoint = '23_checkpoint.keras'\ncallback_checkpoint = ModelCheckpoint(filepath=path_checkpoint,\n                                      monitor='val_loss',\n                                      verbose=1,\n                                      save_weights_only=True,\n                                      save_best_only=True)","01472d6a":"#stopping the model when performance worsens on the valid set\ncallback_early_stopping = EarlyStopping(monitor='val_loss',\n                                        patience=3, verbose=1)\n\n","3301643d":"#This is the callback for writing the TensorBoard log during training.\ncallback_tensorboard = TensorBoard(log_dir='.\/23_logs\/',\n                                   histogram_freq=0,\n                                   write_graph=False)\n\n","0f995062":"#This callback reduces the learning-rate for the optimizer if the validation-loss has not improved since \n# the last epoch (as indicated by patience=0). The learning-rate will be reduced by multiplying it with \n# the given factor. We set a start learning-rate of 1e-3 above, so multiplying it by 0.1 gives a learning-rate of 1e-4. \n# We don't want the learning-rate to go any lower than this.\n\ncallback_reduce_lr = ReduceLROnPlateau(monitor='val_loss',\n                                       factor=0.1,\n                                       min_lr=1e-4,\n                                       patience=0,\n                                       verbose=1)","d1a7659f":"callbacks = [callback_early_stopping,\n             callback_checkpoint,\n             callback_tensorboard,\n             callback_reduce_lr]","a593131d":"epochs = 20\nsteps_per_epoch = 100","0a139fe5":"%%time\nmodel.fit_generator(generator=generator,\n                    epochs=epochs,\n                    steps_per_epoch=steps_per_epoch, #should be 100 ##################\n                    validation_data=validation_data,\n                    callbacks=callbacks)","e15ebbde":"try:\n    model.load_weights(path_checkpoint)\nexcept Exception as error:\n    print(\"Error trying to load checkpoint.\")\n    print(error)","f2ff638b":"loss_test_set = model.evaluate(x=np.expand_dims(x_test_scaled, axis=0),\n                        y=np.expand_dims(y_test_scaled, axis=0))\n","c032ba68":"print(\"loss (test-set):\", loss_test_set)","e9e1f659":"# If you have several metrics you can use this instead.\nif False:\n    for res, metric in zip(result, model.metrics_names):\n        print(\"{0}: {1:.3e}\".format(metric, res))\n","217665a7":"def plot_comparison(start_idx, length=100, train=True):\n    \"\"\"\n    Plot the predicted and true output-signals.\n    \n    :param start_idx: Start-index for the time-series.\n    :param length: Sequence-length to process and plot.\n    :param train: Boolean whether to use training- or test-set.\n    \"\"\"\n    \n    if train:\n        # Use training-data.\n        x = x_train_scaled\n        y_true = y_train\n    else:\n        # Use test-data.\n        x = x_test_scaled\n        y_true = y_test\n    \n    # End-index for the sequences.\n    end_idx = start_idx + length\n    \n    # Select the sequences from the given start-index and\n    # of the given length.\n    x = x[start_idx:end_idx]\n    y_true = y_true[start_idx:end_idx]\n    \n    # Input-signals for the model.\n    x = np.expand_dims(x, axis=0)\n\n    # Use the model to predict the output-signals.\n    y_pred = model.predict(x)\n    \n    # The output of the model is between 0 and 1.\n    # Do an inverse map to get it back to the scale\n    # of the original data-set.\n    y_pred_rescaled = y_scaler.inverse_transform(y_pred[0])\n    \n    # For each output-signal.\n    for signal in range(len(target_names)):\n        # Get the output-signal predicted by the model.\n        signal_pred = y_pred_rescaled[:, signal]\n        \n        # Get the true output-signal from the data-set.\n        signal_true = y_true[:, signal]\n\n        # Make the plotting-canvas bigger.\n        plt.figure(figsize=(15,5))\n        \n        # Plot and compare the two signals.\n        plt.plot(signal_true, label='true')\n        plt.plot(signal_pred, label='pred')\n        \n        # Plot grey box for warmup-period.\n        p = plt.axvspan(0, warmup_steps, facecolor='black', alpha=0.15)\n        \n        # Plot labels etc.\n        plt.ylabel(target_names[signal])\n        plt.legend()\n        plt.show()\n","25c01884":"plot_comparison(start_idx=200000, length=10000, train=True)","da91a178":"plot_comparison(start_idx=200000, length=1000, train=True)\n","5b636220":"def return_pred(start_idx, length=100, train=True):\n    \"\"\"\n    Plot the predicted and true output-signals.\n    \n    :param start_idx: Start-index for the time-series.\n    :param length: Sequence-length to process and plot.\n    :param train: Boolean whether to use training- or test-set.\n    \"\"\"\n    \n    if train:\n        # Use training-data.\n        x = x_train_scaled\n        y_true = y_train\n    else:\n        # Use test-data.\n        x = x_test_scaled\n        y_true = y_test\n    \n    # End-index for the sequences.\n    end_idx = start_idx + length\n    \n    # Select the sequences from the given start-index and\n    # of the given length.\n    x = x[start_idx:end_idx]\n    y_true = y_true[start_idx:end_idx]\n    \n    # Input-signals for the model.\n    x = np.expand_dims(x, axis=0)\n\n    # Use the model to predict the output-signals.\n    y_pred = model.predict(x)\n    \n    # The output of the model is between 0 and 1.\n    # Do an inverse map to get it back to the scale\n    # of the original data-set.\n    y_pred_rescaled = y_scaler.inverse_transform(y_pred[0])\n    y_pred_rescaled[:,0] = np.log(y_pred_rescaled[:,0])\/10\n    y_tc = y_true[:,0]\n    y_tc = np.log(y_true[:,0])\/10\n    \n    result = pd.DataFrame({'Close_pred':y_pred_rescaled[:,0],'Close_true':y_tc})\n    result['rmse'] = np.sqrt((result['Close_pred'] - result['Close_true'])*\n                              (result['Close_pred'] - result['Close_true']))\n    \n    result = result.tail(len(result)-200) #giving at least 50 learning steps to the model\n    return(result)","d7ef6235":"result = return_pred(start_idx = 0, length=10000, train=False)\nresult.head()","82571745":"result.describe()","d2323c93":"rmse = result.describe()['rmse'][1]\ndata_history.append([rmse,loss_test_set,shift_mn,epochs,steps_per_epoch,batch_size,sequence_length])\ndf = pd.DataFrame(data_history,columns=['rmse','trainScore','shift_mn',\n                                        'epochs','steps_per_epoch','batch_size','sequence_length'])\ndf","82b6ff87":"## Generate prediction","2222a081":"Selecting the columns to predict","2908c53d":"## Callbacks \nDuring training we want to save checkpoints and log the progress to TensorBoard so we create the appropriate callbacks for Keras.\nThis is the callback for writing checkpoints during training.","133308be":"# License (MIT)\nCopyright (c) 2018 by Magnus Erik Hvass Pedersen\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and\/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\nThe above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.","24003e0f":"## Validation Set","e2556557":"# Create the RNN","b8b1664d":"## Load Checkpoint\nBecause we use early-stopping when training the model, it is possible that the model's performance has worsened on the test-set for several epochs before training was stopped. We therefore reload the last saved checkpoint, which should have the best performance on the test-set.\n","fbeb56f3":"# Scaling Data\nWe scale the data to a range between 0 and 1 to help the model perform better.","e5bf219a":"We adabt the batch size to use a maximum of the CPU (near 100%)","d3302f9a":"# Train the Recurrent Neural Network\nDon't forget to activate the GPU if you're using the kaggle kernel ! It saves hell of a time !!!","e4538efc":"We test the batch generator to see if it works","1dc75e8a":"A problem with using the Sigmoid activation function, is that we can now only output values in the same range as the training-data.\nFor example, if the training-data only has values between 0.8 and 1.3 then the scaler will map 0.8 to 0 and 1.3 to 1. So if we limit the output of the neural network to be between 0 and 1 using the Sigmoid function, this can only be mapped back to temperature values between 0.8 and 1.3.\n\nWe can use a linear activation function on the output instead. This allows for the output to take on arbitrary values. It might work with the standard initialization for a simple network architecture, but for more complicated network architectures e.g. with more layers, it might be necessary to initialize the weights with smaller values to avoid NaN values during training. You may need to experiment with this to get it working.","05e966b6":"Shifting the data ","81e40796":"# Compiling the model\nThis is a very small model with only two layers. The output shape of (None, None, 3) means that the model will output a batch with an arbitrary number of sequences, each of which has an arbitrary number of observations, and each observation has 3 signals. This corresponds to the 3 target signals we want to predict.\n\n","42d73a09":"### Example from Test-Set\n\nNow consider an example from the test-set. The model has not seen this data during training.\nThe temperature is predicted reasonably well, although the peaks are sometimes inaccurate.\nThe wind-speed has not been predicted so well. The daily oscillation-frequency seems to match, but the center-level and the peaks are quite inaccurate. A guess would be that the wind-speed is difficult to predict from the given input data, so the model has merely learnt to output sinusoidal oscillations in the daily frequency and approximately at the right center-level.\nThe atmospheric pressure is predicted reasonably well, except for a lag and a more noisy signal than the true time-series.","16a3d3ce":"### Adding data to help the model deal with seasonality","6a2957cc":"## Loss Function\nWe will use Mean Squared Error (MSE) as the loss-function that will be minimized. This measures how closely the model's output matches the true output signals.\nHowever, at the beginning of a sequence, the model has only seen input-signals for a few time-steps, so its generated output may be very inaccurate. Using the loss-value for the early time-steps may cause the model to distort its later output. We therefore give the model a \"warmup-period\" of 50 time-steps where we don't use its accuracy in the loss-function, in hope of improving the accuracy for later time-steps.\n","784bf2a4":"## Performance on Test-Set\nWe can now evaluate the model's performance on the test-set. This function expects a batch of data, but we will just use one long time-series for the test-set, so we just expand the array-dimensionality to create a batch with that one sequence.\n","167f104e":"### NumPy Arrays\nSince we shifted the data 1 mn we have to delete the last 1 rows","a3788477":"# Generating Data\nThe data-set has now been prepared as 2-dimensional numpy arrays.","a951ce05":"\nInstead of training the Recurrent Neural Network on the complete sequences of almost 300k observations, we will use the following function to create a batch of shorter sub-sequences picked at random from the training-data."}}