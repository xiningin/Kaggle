{"cell_type":{"aa147b7f":"code","5677858a":"code","b766f885":"code","12034e19":"code","9c9606a9":"code","e5554d43":"code","765859d7":"code","a926ac1e":"code","94705afd":"code","0c74821f":"code","81949f61":"code","77acfa3c":"code","a01cfb00":"code","5ab5bd39":"code","67b66717":"code","a97f4dfc":"code","9a03e436":"code","27be04b3":"code","8d1927bc":"code","60caae62":"code","de014e15":"code","91102cf9":"code","b3dba8d3":"code","318dab3b":"code","e271eeaf":"code","47b41bf6":"code","e1f67c02":"markdown","43017930":"markdown","8143f15b":"markdown","0a3b987c":"markdown","4532e389":"markdown","789ce476":"markdown","c5730178":"markdown","7377547c":"markdown","950573c4":"markdown","aac2964f":"markdown","6e4e166e":"markdown","748113e1":"markdown","8669f904":"markdown","b6220196":"markdown","ff55409d":"markdown","5192cb72":"markdown","05afdb65":"markdown","e2632df0":"markdown","bd8ce2de":"markdown","b3a801ee":"markdown","1a70424c":"markdown","002a8c73":"markdown","84783d70":"markdown","5f4ad152":"markdown","2a34ed46":"markdown","5113b440":"markdown","a36897c1":"markdown","9c52cd7d":"markdown","6e637a85":"markdown","f51b5615":"markdown","260a67c4":"markdown","6062e82f":"markdown","dc8f5553":"markdown","c578c850":"markdown","fb9a808a":"markdown","f23a9bd4":"markdown","6ca526de":"markdown","81ca97f6":"markdown","d069b5cc":"markdown","05a3c846":"markdown","f54600b7":"markdown","34ff1d9b":"markdown","78bd6ed1":"markdown","67ee5fb8":"markdown","403561c6":"markdown","640db634":"markdown","d4ca13b0":"markdown"},"source":{"aa147b7f":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.decomposition import PCA\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import KFold\n\nimport re #regular expression\n\nimport math\nfrom numbers import Number\n\npd.set_option('display.max_columns', None) #to see all the columns of a dataframe","5677858a":"train = pd.read_csv(\"..\/input\/titanic\/train.csv\")\ntest = pd.read_csv(\"..\/input\/titanic\/test.csv\")\ndata = pd.concat([train,test])\ndata.iloc[889:893]","b766f885":"# create dictionary from training names\nwords = train['Name'].str.cat(sep=' ') #concatenates all the names into one string\nwords = words.upper() #change every character to upper case. Makes van and Van the same word in our dictionary.\nwords = re.sub('[.,\"()\/]', ' ', words) #replaces all non-alphanumeric characters with space\nwords = re.sub(' . |  ',' ',words) #removes single characters\nwords = words.split() #creates a list of the words found in the training names. Mr will appear multiple times in this list but Kiamie will happen only once.\ndictionary = pd.DataFrame(words).value_counts().rename_axis('Words').reset_index(name='Freq')\ndictionary = dictionary[dictionary[\"Freq\"]>1]\n\n# Calculate the probability of survival given word\ndictionary[\"Survival\"]=0\nindT = 0 #index in Train dataset\nindD = 0 #index in Dictionary dataset\nfor w in dictionary[\"Words\"]:\n    survived = 0 #re-init survived variable\n    indT = 0\n    for n in train[\"Name\"]:\n        n = n.upper() #change every character to upper case. Makes van and Van the same word in our dictionary.\n        n = re.sub('[.,\"()]', '', n) #replaces all non-alphanumeric characters with space\n        #n = re.sub(' . |  ',' ',n) #removes single character\n        n = n.split()\n        if(w in n):\n            if (train.iloc[indT][\"Survived\"]==1):\n                survived = survived + 1\n                #print(w + \" => \" + str(indD) + \" : \" + str(survived))\n        indT = indT+1\n    dictionary.loc[indD,\"Survival\"]=survived\/dictionary.loc[indD,\"Freq\"]\n    indD = indD + 1 \ndictionary","12034e19":"passengers = pd.DataFrame(0, index=np.arange(data.shape[0]), columns=dictionary[\"Words\"].to_list())\nind = 0\nfor i in data[\"Name\"]:\n    i = i.upper() #change every character to upper case. Makes van and Van the same word in our dictionary.\n    i = re.sub('[.,\"()]', '', i) #replaces non-alphanumeric characters with space\n    #i = re.sub(' . |  ',' ',i)\n    i = i.split()\n    for j in i:\n        passengers.iloc[ind][j]=1\n    ind = ind+1\npassengers","9c9606a9":"c = passengers[dictionary[dictionary[\"Freq\"]>=4][\"Words\"]].corr() #there are 116 words appearing 4 times or more in the traning dataset names\nrc = []\nfor row in c.index:\n    for col in c.columns:\n        if ((row!=col) and (abs(c.loc[row][col])>=0.3)):\n            if(rc.count([row,col])==0):\n                rc.append([row,col])\n                rc.append([col,row])\n                print(row + \" and \" + col + \" have a correlation score of \" + str(c.loc[row][col]))","e5554d43":"from scipy.spatial.distance import pdist, squareform\n\ndistances = pdist(passengers.values, metric='euclidean')\ndist_matrix = squareform(distances)\n\ndist_matrix = pd.DataFrame(data=dist_matrix, columns=passengers.index, index=passengers.index)\ndist_matrix.head()","765859d7":"dist_matrix_non_diag = dist_matrix.mask(np.eye(dist_matrix.shape[0], dtype = bool)) #puts NaN for diagonal elements, necessary because \"passenger N\" 's closest passenger is \"passenger N\"\ni=0\ncp = [] #closest_passengers\nfor e in dist_matrix_non_diag.idxmin():\n    cp.append([i, data.iloc[i][\"Name\"], e, data.iloc[e][\"Name\"]])\n    i = i+1\ncp = pd.DataFrame(data=cp, columns=[\"Passenger Index\", \"Passenger\",\"Closest Passenger Index\", \"Closest Passenger\"])\n#cp[[\"Passenger\", \"Closest Passenger\"]]\ncp","a926ac1e":"X = passengers.copy()\n\n# Scale the dataset\nX_scaled = (X - X.mean(axis=0)) \/ X.std(axis=0)\n\n# Create principal components\npca = PCA()\nX_pca = pca.fit_transform(X_scaled) #X_pca.shape = (1309, 414)\n\n# Convert to dataframe\ncomponent_names = [f\"PC{i+1}\" for i in range(X_pca.shape[1])]\nX_pca = pd.DataFrame(X_pca, columns=component_names)\n\n# Principal Components \/ Words relation\npc = pd.DataFrame(pca.components_.T, columns=component_names, index=dictionary[\"Words\"].to_list())\npc","94705afd":"pc5_check = []\nfor i in range(0,X_pca.shape[0]):\n    pc5_check.append([data.iloc[i][\"Sex\"], X_pca.iloc[i][\"PC5\"]])\npc5_check = pd.DataFrame(data=pc5_check, columns=[\"Sex\",\"PC5\"])\nsns.histplot(data=pc5_check, x=\"PC5\", hue=\"Sex\")\nplt.show()","0c74821f":"X_pca","81949f61":"print(\"passenger \" + train.iloc[0][\"Name\"] + \" is represented by the following vector in the transformed dataset :\")\nprint(X_pca.iloc[0].sort_values())","77acfa3c":"for i in [1, 2, 3, 4, 5, 31, 37, 39]:\n    PCI = \"PC\" + str(i)\n    #print the weights for the current PC\n    weights = pc[(pc[PCI]>0.1) | (pc[PCI]<-0.1)][PCI].sort_values()\n    if(len(weights)==0):\n        weights = pc[(pc[PCI]>0.0001) | (pc[PCI]<-0.0001)][PCI].sort_values()\n    print(\"The main weights\/loadings of \" + str(PCI) + \" are : \")\n    print(weights)","a01cfb00":"def plot_variance(pca, width=8, dpi=100):\n    # Create figure\n    fig, axs = plt.subplots(1, 2)\n    n = pca.n_components_\n    grid = np.arange(1, n + 1)\n    # Explained variance\n    evr = pca.explained_variance_ratio_\n    axs[0].bar(grid, evr)\n    axs[0].set(\n        xlabel=\"Component\", title=\"% Explained Variance\", ylim=(0.0, 1.0)\n    )\n    # Cumulative Variance\n    cv = np.cumsum(evr)\n    axs[1].plot(np.r_[0, grid], np.r_[0, cv], \"o-\")\n    axs[1].set(\n        xlabel=\"Component\", title=\"% Cumulative Variance\", ylim=(0.0, 1.0)\n    )\n    # Set up figure\n    fig.set(figwidth=8, dpi=100)\n    return axs\n\nplot_variance(pca)\nplt.show()","5ab5bd39":"def buildDict(df, str_to_parse=\"Name\"):\n    words = df[str_to_parse].str.cat(sep=' ') #concatenates all the names into one string\n    words = words.upper() #change every character to upper case. Makes van and Van the same word in our dictionary.\n    words = re.sub('[.,\"()]', '', words) #replaces all non-alphanumeric characters with space\n    #words = re.sub(' . |  ',' ',words) #removes single characters\n    words = words.split() #creates a list of the words found in the training names. Mr will appear multiple times in this list but Kiamie will happen only once.\n    dictionary = pd.DataFrame(words).value_counts().rename_axis('Words').reset_index(name='Freq')\n    dictionary = dictionary[dictionary[\"Freq\"]>1]\n    return dictionary\n\ndef vectPassengers(df, dictionary, str_to_parse=\"Name\"):\n    passengers = pd.DataFrame(0, index=np.arange(df.shape[0]), columns=dictionary[\"Words\"].to_list())\n    ind = 0\n    for i in df[str_to_parse]:\n        i = i.upper() #change every character to upper case. Makes van and Van the same word in our dictionary.\n        i = re.sub('[.,\"()]', '', i) #replaces all non-alphanumeric characters with space\n        #i = re.sub(' . |  ',' ',i)\n        i = i.split()\n        for j in i:\n            passengers.iloc[ind][j]=1\n        ind = ind+1\n    return passengers\n\n# Scale and transform the datasets passed in arguments.\n# The 1st dataset is the training set, the 2nd is the Validation or Test set\ndef createTransformedPassengersDF(df_train, df_test):\n    # scale the datasets \n    df_train_scaled = (df_train - df_train.mean(axis=0)) \/ df_train.std(axis=0)\n    df_test_scaled = (df_test - df_train.mean(axis=0)) \/ df_train.std(axis=0)\n    \n    # create the transformed datasets using PCA\n    pca = PCA()\n    df_train_pca = pca.fit_transform(df_train_scaled)\n    df_test_pca = pca.transform(df_test_scaled)\n    component_names = [f\"PC{i+1}\" for i in range(df_train_pca.shape[1])]\n    df_train_pca = pd.DataFrame(df_train_pca, columns=component_names)\n    df_test_pca = pd.DataFrame(df_test_pca, columns=component_names)\n            \n    return df_train_pca, df_test_pca","67b66717":"train = pd.read_csv(\"..\/input\/titanic\/train.csv\")\n\nX_train = train[[\"Name\",\"Survived\"]].copy()\ny_train = X_train.pop(\"Survived\")\n\nN_SPLITS = 10\nresults = []\n\nfor d in range(1,40, 2):\n    for k in range(3,31,2):\n\n        iteration = 1\n        cum_accuracy = 0\n        \n        kf = KFold(n_splits=N_SPLITS, shuffle=True, random_state=42)\n        for train_index, val_index in kf.split(X_train, y_train):\n            X_train_, X_val_ = X_train.iloc[train_index], X_train.iloc[val_index]\n            y_train_, y_val_ = y_train[train_index], y_train[val_index]\n\n            # build dictionary from X_train_\n            dictionary = buildDict(X_train_)\n            #print(\"dictionary has \" + str(len(dictionary[\"Words\"])) + \" words\")\n            \n            # vectorize X_train_ and X_val_ passengers using dictionary\n            X_train_ = vectPassengers(X_train_, dictionary)\n            X_val_ = vectPassengers(X_val_, dictionary)\n            \n            # scale and create the transformed datasets using PCA\n            X_train_pca, X_val_pca = createTransformedPassengersDF(X_train_, X_val_)\n            \n            # Selecting d principal components\n            X_train_pca = X_train_pca.iloc[:,0:d].copy()\n            X_val_pca = X_val_pca.iloc[:,0:d].copy()\n            \n            # train model\n            model = KNeighborsClassifier(n_neighbors=k)\n            model.fit(X_train_pca, y_train_)\n            \n            #evaluate model\n            y_val_preds = model.predict(X_val_pca)\n            accuracy = accuracy_score(y_val_, y_val_preds)\n            #print(str(iteration) + '\/' + str(N_SPLITS) + ' Nb of correctly classified samples: ' + str(accuracy))\n            cum_accuracy = cum_accuracy + accuracy\n            \n            iteration = iteration + 1\n\n        accuracy = cum_accuracy\/N_SPLITS\n        print('k=' + str(k) + ' d=' + str(d) + \" ==> Average nb of correctly classified samples: \" + str(accuracy))\n        results.append([k,d,accuracy])\nresults = pd.DataFrame(data=results, columns=['k','d', 'average nb of correctly classified samples'])","a97f4dfc":"results[results['average nb of correctly classified samples']==results['average nb of correctly classified samples'].max()]","9a03e436":"# prepare datasets\ntrain = pd.read_csv(\"..\/input\/titanic\/train.csv\")\ntest = pd.read_csv(\"..\/input\/titanic\/test.csv\")\nX_train = train[[\"Name\",\"Survived\"]].copy()\ny_train = X_train.pop(\"Survived\")\nX_test=pd.DataFrame(test[\"Name\"].copy())\n\n# choose best parameters\nd=7\nk=11\n\n#create dictionary\ndictionary = buildDict(X_train)\n\n#vectorize passengears\nX_train = vectPassengers(X_train, dictionary)\nX_test = vectPassengers(X_test, dictionary)\n\n#create transformed datasets using pca\nX_train_pca, X_test_pca = createTransformedPassengersDF(X_train, X_test)\n\n# Select d principal components\nX_train_pca = X_train_pca.iloc[:,0:d].copy()\nX_test_pca = X_test_pca.iloc[:,0:d].copy()\n\n# train model with k neighbors\nmodel = KNeighborsClassifier(n_neighbors=k)\nmodel.fit(X_train_pca, y_train)\n            \n#make predicitions\ny_preds = model.predict(X_test_pca)\n\n# Save the preds file\nsubmission = pd.read_csv('..\/input\/titanic\/gender_submission.csv')\nsubmission['Survived'] = y_preds.astype('int')\nsubmission.to_csv('submission_1.csv', index = False)\nprint(\"submission_1.csv file has been saved successfully\")","27be04b3":"train = pd.read_csv(\"..\/input\/titanic\/train.csv\")\ntest = pd.read_csv(\"..\/input\/titanic\/test.csv\")\n\ndef createPassengerString(x):\n    s = \"\"\n    #for f in [\"Pclass\",\"Name\",\"Sex\",\"Age\",\"SibSp\",\"Parch\",\"Ticket\",\"Fare\",\"Cabin\",\"Embarked\"]:\n    for f in [\"Pclass\",\"Name\",\"Sex\",\"Age\",\"Ticket\",\"Fare\",\"Embarked\"]:\n        if(s==\"\"):\n            s = str(x[f])\n        elif (isinstance(x[f], Number)):\n            if (math.isnan(x[f])==False):\n                s = s + \" \" + str(round(x[f]))\n        else:\n            s = s + \" \" + x[f]\n    return s\n\ntrain[\"Passenger\"]= train.apply(createPassengerString, axis=1)\ntest[\"Passenger\"]= test.apply(createPassengerString, axis=1)\ndata = pd.concat([train,test])\ndata.iloc[889:893]","8d1927bc":"# create dictionary from training passengers\nwords = train['Passenger'].str.cat(sep=' ') #concatenates all the names into one string\nwords = words.upper() #change every character to upper case. Makes van and Van the same word in our dictionary.\nwords = re.sub('[.,\"()]', '', words) #replaces all non-alphanumeric characters with space\n#words = re.sub(' . |  ',' ',words) #removes single characters\nwords = words.split() #creates a list of the words found in the training names. Mr will appear multiple times in this list but Kiamie will happen only once.\ndictionary = pd.DataFrame(words).value_counts().rename_axis('Words').reset_index(name='Freq')\ndictionary = dictionary[dictionary[\"Freq\"]>1]\n\n# Calculate the probability of survival given word\ndictionary[\"Survival\"]=0\nindT = 0 #index in Train dataset\nindD = 0 #index in Dictionary dataset\nfor w in dictionary[\"Words\"]:\n    survived = 0 #re-init survived variable\n    indT = 0\n    for n in train[\"Passenger\"]:\n        n = n.upper() #change every character to upper case. Makes van and Van the same word in our dictionary.\n        n = re.sub('[.,\"()]', '', n) #replaces non-alphanumeric characters with space\n        #n = re.sub(' . |  ',' ',n) #removes single character\n        n = n.split()\n        if(w in n):\n            if (train.iloc[indT][\"Survived\"]==1):\n                survived = survived + 1\n                #print(w + \" => \" + str(indD) + \" : \" + str(survived))\n        indT = indT+1\n    dictionary.loc[indD,\"Survival\"]=survived\/dictionary.loc[indD,\"Freq\"]\n    indD = indD + 1 \ndictionary\n#dictionary.to_csv('new_dict.csv')","60caae62":"passengers = pd.DataFrame(0, index=np.arange(data.shape[0]), columns=dictionary[\"Words\"].to_list())\nind = 0\nfor i in data[\"Passenger\"]:\n    i = i.upper() #change every character to upper case. Makes van and Van the same word in our dictionary.\n    i = re.sub('[.,\"()]', '', i) #replaces all non-alphanumeric characters with space\n    #i = re.sub(' . |  ',' ',i)\n    i = i.split()\n    for j in i:\n        passengers.iloc[ind][j]=1\n    ind = ind+1\npassengers","de014e15":"X = passengers.copy()\n\n# Scale the dataset\nX_scaled = (X - X.mean(axis=0)) \/ X.std(axis=0)\n\n# Create principal components\npca = PCA(svd_solver='full')\nX_pca = pca.fit_transform(X_scaled) #X_pca.shape = (1309, 414)\n\n# Convert to dataframe\ncomponent_names = [f\"PC{i+1}\" for i in range(X_pca.shape[1])]\nX_pca = pd.DataFrame(X_pca, columns=component_names)\n\n# Principal Components \/ Words relation\npc = pd.DataFrame(pca.components_.T, columns=component_names, index=dictionary[\"Words\"].to_list())\npc","91102cf9":"X_pca","b3dba8d3":"plot_variance(pca)\nplt.show()","318dab3b":"train = pd.read_csv(\"..\/input\/titanic\/train.csv\")\ntrain[\"Passenger\"]= train.apply(createPassengerString, axis=1)\nX_train = train[[\"Passenger\",\"Survived\"]].copy()\ny_train = X_train.pop(\"Survived\")\n\nN_SPLITS = 10\nresults = []\n\nfor d in range(1,40, 2):\n    for k in range(3,31,2):\n\n        iteration = 1\n        cum_accuracy = 0\n        \n        kf = KFold(n_splits=N_SPLITS, shuffle=True, random_state=42)\n        for train_index, val_index in kf.split(X_train, y_train):\n            X_train_, X_val_ = X_train.iloc[train_index], X_train.iloc[val_index]\n            y_train_, y_val_ = y_train[train_index], y_train[val_index]\n\n            # build dictionary from X_train_\n            dictionary = buildDict(X_train_, str_to_parse=\"Passenger\")\n            #print(\"dictionary has \" + str(len(dictionary[\"Words\"])) + \" words\")\n            \n            # vectorize X_train_ and X_val_ passengers using dictionary\n            X_train_ = vectPassengers(X_train_, dictionary, str_to_parse=\"Passenger\")\n            X_val_ = vectPassengers(X_val_, dictionary, str_to_parse=\"Passenger\")\n            \n            # scale and create the transformed datasets using PCA\n            X_train_pca, X_val_pca = createTransformedPassengersDF(X_train_, X_val_)\n            \n            # Selecting d principal components\n            X_train_pca = X_train_pca.iloc[:,0:d].copy()\n            X_val_pca = X_val_pca.iloc[:,0:d].copy()\n            \n            # train model\n            model = KNeighborsClassifier(n_neighbors=k)\n            model.fit(X_train_pca, y_train_)\n            \n            #evaluate model\n            y_val_preds = model.predict(X_val_pca)\n            accuracy = accuracy_score(y_val_, y_val_preds)\n            #print(str(iteration) + '\/' + str(N_SPLITS) + ' Nb of correctly classified samples: ' + str(accuracy))\n            cum_accuracy = cum_accuracy + accuracy\n            \n            iteration = iteration + 1\n\n        accuracy = cum_accuracy\/N_SPLITS\n        print('k=' + str(k) + ' d=' + str(d) + \" ==> Average nb of correctly classified samples: \" + str(accuracy))\n        results.append([k,d,accuracy])\nresults = pd.DataFrame(data=results, columns=['k','d', 'average nb of correctly classified samples'])","e271eeaf":"results[results['average nb of correctly classified samples']==results['average nb of correctly classified samples'].max()]","47b41bf6":"# prepare datasets\ntrain = pd.read_csv(\"..\/input\/titanic\/train.csv\")\ntest = pd.read_csv(\"..\/input\/titanic\/test.csv\")\ntrain[\"Passenger\"]= train.apply(createPassengerString, axis=1)\ntest[\"Passenger\"]= test.apply(createPassengerString, axis=1)\nX_train = train[[\"Passenger\",\"Survived\"]].copy()\ny_train = X_train.pop(\"Survived\")\nX_test=pd.DataFrame(test[\"Passenger\"].copy())\n\n# choose best parameters\nk=13\nd=7\n\n#create dictionary\ndictionary = buildDict(X_train, str_to_parse=\"Passenger\")\n\n#vectorize passengears\nX_train = vectPassengers(X_train, dictionary, str_to_parse=\"Passenger\")\nX_test = vectPassengers(X_test, dictionary, str_to_parse=\"Passenger\")\n\n#create transformed datasets using pca\nX_train_pca, X_test_pca = createTransformedPassengersDF(X_train, X_test)\n\n# Select d principal components\nX_train_pca = X_train_pca.iloc[:,0:d].copy()\nX_test_pca = X_test_pca.iloc[:,0:d].copy()\n\n# train model with k neighbors\nmodel = KNeighborsClassifier(n_neighbors=k)\nmodel.fit(X_train_pca, y_train)\n            \n#make predicitions\ny_preds = model.predict(X_test_pca)\n\n# Save the preds file\nsubmission = pd.read_csv('..\/input\/titanic\/gender_submission.csv')\nsubmission['Survived'] = y_preds.astype('int')\nsubmission.to_csv('submission_2.csv', index = False)\nprint(\"submission_2.csv file has been saved successfully\")","e1f67c02":"## Transformed passengers dataset","43017930":"# 1. KNN Model","8143f15b":"## Variance explained","0a3b987c":"We'll build a dictionary with all words found in the training dataset names 2 or more times. Naturally, words appearing once will be ignored (not included in the dictionary) as they can not help the prediction.","4532e389":"The best hyperparameters are k=11 and d=7. The CV score with these hyperparameters is 77.3%. \n\nNow we can use the first 7 Principal Components and build our KNN model with n_neighbors=11 then predict the survival for the test passengers and submit the result to Kaggle. ","789ce476":"## Variance explained","c5730178":"We need to be careful to avoid data leakage. Data leakage is when information from outside the training dataset is used to create the model. We are going to do CV, so our training set must not \"contain\" any information from our validation (a.k.a. Out-Of-Fold) set, not from our test set. The 3 functions below take care of data leakage :","7377547c":"# Introduction","950573c4":"Now, let's display the transformed *passengers* dataset:","aac2964f":"**The preparation work is done and we can now try to predict passenger's survival. To achieve this, we'll use PCA plus kNN.**","6e4e166e":"We'll calculate the euclidean distance between passengers.  ","748113e1":"# 2. KNN Model","8669f904":"The best hyperparameters are k=13 and d=7. The CV score with these hyperparameters is 80.8%.\n\nNow we can use the first 7 Principal Components and build our KNN model with n_neighbors=13 then predict the survival for the test passengers and submit the result to Kaggle.","b6220196":"PC1 is 50% SATODE, 50% CASTELLANA, 50% PENACO and lower % for other words  \nPC2 is 52% HEATH, 52% FUTRELLE, 48% JACQUES and lower % for other words  \n...  \nPC5 is -40% MR, 25% MRS, 26% MISS and lower % for other words    \n...   \n\nThe 5th Principal Component seems to capture pretty well the passenger's gender. The plot below representing the PC5 values for males and females shows a (not a perfect but) pretty clear separation between males and females (PC5 has negative values for males and positive values for females) :   ","ff55409d":"<div style=\"display:fill; background-color:#000000;border-radius:5px;\">\n    <p style=\"font-size:300%; color:white;text-align:center\";>2nd solution with all features as a string<\/p>\n<\/div>","5192cb72":"<div style=\"display:fill; background-color:#000000;border-radius:5px;\">\n    <p style=\"font-size:300%; color:white;text-align:center\";>1st solution using the Name only<\/p>\n<\/div>","05afdb65":"## Passengers similarity","e2632df0":"Let's calculate the principal components of the dataframe *passengers* and try to interpret what these Principal Components represent:","bd8ce2de":"**Conclusion : This isn't a high score (0.76) but we did achieve the accuracy of the gender model by implemented an interesting and complex model!!**","b3a801ee":"Last week, I read a notebook [KISS: Small and simple Titanic models](https:\/\/www.kaggle.com\/carlmcbrideellis\/kiss-small-and-simple-titanic-models) from GM [Carl McBride Ellis](https:\/\/www.kaggle.com\/carlmcbrideellis) where he explains the importance of keeping the models Small and Simple and provides examples of such models using the Titanic dataset. I'll recall a few examples here but I do encourage you to read Carl's notebook :\n* The **random model** predicts randomly Survival or Death and scores \u224850%\n* The **Zero Rule classifier** (aka. ZeroR or 0-R) always predicts the majority class of the dataset (i.e. Death) and scores \u224862%. It is against this baseline (and not the random model above) that one should compare the performance of all other models based on this data.\n* The **gender model** predicts that only women survived and scores \u224876%\n* The **gender\/class model** predicts that only women from 1st and 2nd class survived and scores \u224877.5%\n\nIt reminded me a great notebook [Titanic \"Spam\" Filter](https:\/\/www.kaggle.com\/cdeotte\/titanic-spam-filter) from another GM [Chris Deotte](https:\/\/www.kaggle.com\/cdeotte). In this notebook, Chris builds models to check how well one can predict Titanic passengers' survival using only the words in the Name column and ignoring semantic information (without being told which parts were titles, first names, last names, etc). I guess it is one good example of a no-KISS solution but it was a fun idea and it would be great if a purely machine learning algorithm could find the patterns in the name strings (without being told which parts were titles, first names, last names, etc) and could achieve a good score on its own (Chris also showed in [another notebook](https:\/\/www.kaggle.com\/cdeotte\/titanic-using-name-only-0-81818) that one simple model using the passenger name only and semantic information can achieve an impressive 82% score).\n\nIn this notebook, I'll try to replicate Chris solution (written in Python, the original notebook from Chris is in R) and then extend it by adding in the passenger's string new information like age, ticket number... (of course, without using semantic information again) and check if it improves the model score.","1a70424c":"## GridSearch to find best hyperparameters","002a8c73":"# 2. Predict Survival for Test Passengers and Submission to Kaggle","84783d70":"**Conclusion : The LB score improved slightly (0.77) but it isn't a high score. Once again, we did achieve the accuracy of the gender model (and even a bit better) by implemented an interesting and complex model! Naturally, the LB score is far from what can be achieved with a traditional approach (using feature engineering...) but it was the opportunity to test an original approach for this dataset, using techniques e.g. vectorization, pca that are not generally used with this dataset.**","5f4ad152":"The new dictionary contains all words found in the training dataset *passenger* column 2 or more times. It has 681 elements. Now we can one-hot-encode all the passengers using these 681 words. This vectorization of passengers will be stored in the variable named passengers. It has dimension 1309 rows and 681 columns :","2a34ed46":"# 1. Build dictionary","5113b440":"# 1. Principal Component Analysis","a36897c1":"## Principal Components","9c52cd7d":"## GridSearch to find best hyperparameters","6e637a85":"# 2. Principal Component Analysis","f51b5615":"## Principal Components","260a67c4":"## Features\/Words correlation ","6062e82f":"The plots below represent : \n* the % of variance explained in the dataset per Principal Component \n* the % Cumulative Variance\n\nAs we could have expected, each individual component do not explain a large % of variance. E.g. PC1 explains only 0.89% of the variance. ","dc8f5553":"In this section, we will set up our first model (kNN) using the the passengers' principal components. We'll use it to classify unknown passengers using kNN. \n\nFirst let's cross validate and perform a grid search to find the optimal dimension d of principal components and optimal k for kNN : \n- we will search the best hyperparameters d and k using the following ranges d in {1,2, ..., 20} and k in {7,9,11,13,15}\n- we will use a 10 fold validation scheme","c578c850":"Now, we have our dataframe *passengers*. It contains the 1309 passengers each as 1 row. The 414 columns indicate whether Word k is present in that passenger's Name. You can think of each passenger as a vector of zeros and ones of length 414. \n\nE.g. the 1st passenger name is \"Braund, Mr. Owen Harris\". Therefore, the 1st row of our *passengers* dataframe has a 1 in column BRAUND, a 1 in column MR, a 1 in column OWEN, a 1 in column HARRIS, and 410 zeros in the remaining columns.","fb9a808a":"The features\/words of our *passengers* dataset are not much correlated and this is not a surprise :\n- MR is negatively correlated with MISS (resp. MRS) and that makes sense because when you are a MR, you're not a MISS (resp. MRS)\n- ERNEST and CARTER are correlated and the reason is simple: among the 6 passengers having Carter in their names, 4 have Ernest too and inversely, among the 12 passengers having Ernest in their names, 6 have Carter too.\n- The same rationale explains the correlation between HART and BENJAMIN (resp. between CARL and ASPLUND).","f23a9bd4":"## Transformed passengers dataset","6ca526de":"Now, let's display the transformed passengers dataset:","81ca97f6":"At first sight, it looks like a passenger's closest passenger is either a passenger with the same Title or a family member. This is not a bad news as we can expect a kNN classifier model to create clusters of passengers having the same title and use the gender pattern : most males died and most females survived.\n\nOne can also wonder why *Emir, Mr. Farred Chehab* is the closest passenger of several male passengers e.g. *Mr. Owen Braund* or *Mr. Simon Seather.* The reason is simple : *Emir*, *Farred* and *Chehab* are not in the dictionary (as we kept only the words appearing 2 or more times) so the vector representing *Mr. Emir Farred* = the vector for *Mr.* and therefore the distance between *Emir, Mr. Farred Chehab* and *Braund, Mr. Owen Harris* is shortest than the distance between *Braund, Mr. Owen Harris* and his brother *Braund, Mr. Lewis Richard*.","d069b5cc":"## 1. Predict Survival for Test Passengers and Submission to Kaggle","05a3c846":"For fun, let's find the word pairs with \"high\" frequency (>=4) and \"high\" correlation (>0.3): ","f54600b7":"Therefore when Mr. Owen Braund has principal components weights (PC31=-12.9, PC39=-11.8, PC5=-3.6, PC37=13.4, ...) that means he is essentially a Braund, an Owen and a male. You can show the hidden cells below for details. ","34ff1d9b":"# 2. Build dictionary","78bd6ed1":"# 1. Vectorize passengers","67ee5fb8":"We'll apply the same logic but instead of using the passenger Name only, we will use all passengers features as a string. Let's create this new feature *Passenger* and display it:","403561c6":"# 2. Vectorize passengers","640db634":"# Useful Functions","d4ca13b0":"The dictionary contains all words found in the training dataset names 2 or more times. It has 414 elements. Now we can  one-hot-encode all the passengers using these 414 words. This vectorization of passengers will be stored in the variable named *passengers*. It has dimension 1309 rows and 414 columns :"}}