{"cell_type":{"f64e5743":"code","137460e1":"code","69ee6667":"code","f434691a":"code","becf3eb2":"code","0f84a3db":"code","c2db5ebf":"code","2bc4c557":"code","a2eb8453":"code","4b89bb35":"code","808a4761":"code","1b43d1eb":"code","7b1d4ab3":"markdown","eede0e6b":"markdown","b046ed74":"markdown","70b2968b":"markdown","1dfbe060":"markdown","1a6b937c":"markdown","adc7aec6":"markdown","bc59aeae":"markdown","75985d0f":"markdown","3234288a":"markdown","abf002e4":"markdown"},"source":{"f64e5743":"import numpy as np\nimport pandas as pd\nfrom keras.models import Sequential\nfrom keras.layers.core import Dense,Activation,Dropout\nfrom keras.layers import LSTM\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error\nimport matplotlib.pyplot as plt\nimport time \nimport math\ndf = pd.read_csv('..\/input\/IBM.csv',delimiter=',')\ndf=df.set_index(['date'])\ndf.drop(df.columns[[5,6,7,9]],axis=1,inplace=True)\ndf.head(5)","137460e1":"df.describe()","69ee6667":"df.info()","f434691a":"plt.figure(figsize=(20, 5))\nplt.subplot(1,1,1)\nplt.plot(df.open.values,color='blue',label='open')\nplt.plot(df.close.values,color='green',label='close')\nplt.plot(df.low.values,color='yellow',label='low')\nplt.plot(df.high.values,color='red',label='high')\nplt.plot(df.vwap.values,color='black',label='volume weighted average price')\nplt.title('stock price')\nplt.xlabel('time [days]')\nplt.ylabel('price')\nplt.legend(loc='best')\nplt.show()","becf3eb2":"plt.figure(figsize=(10, 5))\nplt.subplot(1,1,1)\nplt.plot(df.volume.values,color='black',label='volume')\nplt.title('stock Volume')\nplt.xlabel('time [days]')\nplt.ylabel('price')\nplt.legend(loc='best')\nplt.show()","0f84a3db":"df1=df\ndf.head()\ndf1.drop(df.columns[[0,1,2,3,4,6]],axis=1,inplace=True)\ndf1\nscaler = MinMaxScaler(feature_range=(0, 1))\ndf1 = scaler.fit_transform(df1)","c2db5ebf":"train_size = int(len(df) * 0.70)\ntest_size = len(df) - train_size\ntrain, test = df1[0:train_size,:], df1[train_size:len(df),:]\nprint(len(train),len(test))","2bc4c557":"def create(dataset, look_back=1):\n    dataX, dataY = [], []\n    for i in range(len(dataset)-look_back-1):\n        a = dataset[i:(i+look_back), 0]\n        dataX.append(a)\n        dataY.append(dataset[i + look_back, 0])\n    return np.array(dataX), np.array(dataY)","a2eb8453":"look_back=1\ntrainX, trainY = create(train, look_back)\ntestX, testY = create(test, look_back)\ntrainX = np.reshape(trainX, (trainX.shape[0], 1, trainX.shape[1]))\ntestX = np.reshape(testX, (testX.shape[0], 1, testX.shape[1]))\ntrainX.shape","4b89bb35":"model = Sequential()\nmodel.add(LSTM(5, input_shape=(1, look_back)))\nmodel.add(Dense(1))\nmodel.compile(loss='mean_squared_error', optimizer='RMSProp')\nmodel.fit(trainX, trainY, epochs=100, batch_size=8, verbose=1)","808a4761":"# make predictions\ntrainPredict = model.predict(trainX)\ntestPredict = model.predict(testX)\n# invert predictions\ntrainPredict = scaler.inverse_transform(trainPredict)\ntrainY = scaler.inverse_transform([trainY])\ntestPredict = scaler.inverse_transform(testPredict)\ntestY = scaler.inverse_transform([testY])\n# calculate root mean squared error\ntrainScore = math.sqrt(mean_squared_error(trainY[0], trainPredict[:,0]))\nprint('Train Score: %.2f RMSE' % (trainScore))\ntestScore = math.sqrt(mean_squared_error(testY[0], testPredict[:,0]))\nprint('Test Score: %.2f RMSE' % (testScore))","1b43d1eb":"\ntrainPredictPlot = np.empty_like(df1)\ntrainPredictPlot[:, :] = np.nan\ntrainPredictPlot[look_back:len(trainPredict)+look_back, :] = trainPredict\n\ntestPredictPlot = np.empty_like(df1)\ntestPredictPlot[:, :] = np.nan\ntestPredictPlot[len(trainPredict)+(look_back*2)+1:len(df1)-1, :] = testPredict\n\nplt.figure(figsize=(20, 5))\nplt.subplot(1,1,1)\nplt.plot(scaler.inverse_transform(df1),color='blue',label='Actual Value')\nplt.plot(trainPredictPlot,color='red',label='Train Prediction')\nplt.plot(testPredictPlot,color='green',label='Test Prediction')\nplt.title('Result')\nplt.xlabel('time [days]')\nplt.ylabel('price')\nplt.legend(loc='best')\nplt.show()\npredict=pd.DataFrame({'Predicted':testPredict[:,0]})\ncc=df1[0:train_size,:]\npredict.to_csv('output.csv')\npredict.to_csv('given.csv')","7b1d4ab3":"Converting dataframe into  a ***specific array structure (numpy)*** to feed the data into **LSTM**.","eede0e6b":"Splitting into *Test data and Train data* in the ratio of **70:30**.","b046ed74":"Even though LSTM model fits good in the data prediction.There are many other factors stocks price depend upon such as **companies policies,countries economic policies,demonetisation,etc** .","70b2968b":"**4 FITTING LSTM MODEL**","1dfbe060":"****1. IMPORTING THE LIBRARIES****","1a6b937c":"Training the **LSTM** model.\n\n**Optimiser used**:**RMSProp**\n\n**Loss Metrics**:**Root Mean Square Error**\n\nAbout RMSProp brief:-\nThe RMSprop optimizer is similar to the gradient descent algorithm with momentum. **The RMSprop optimizer restricts the oscillations in the vertical direction**. Therefore, we can increase our learning rate and our algorithm could take larger steps in the horizontal direction converging faster. The difference between RMSprop and gradient descent is on how the gradients are calculated.  The value of momentum is denoted by beta and is usually set to 0.9. \n","adc7aec6":"**7 CONCLUSION**","bc59aeae":"**2**.**DATA VISUALISATION** **AND ANALYSIS**","75985d0f":"**5 PREDICTION PHASE**","3234288a":"**3** **PRE PROCESSING**\n \nUsing **MinMaxScaler** to **Normalise **the data\n\n **Volume Weighted Average Price** is used for prediction.The rest of the attributes are only for *analysis and visualisation* purpose","abf002e4":"**6** **PERFORMANCE ANALYSIS**\n\nAfter inversion to the norminalised values calculating **RMSE** and displaying the error "}}