{"cell_type":{"cf119ac2":"code","d51aae6c":"code","e395b004":"code","96bf2cd0":"code","afe809d0":"code","467153fd":"code","c6a70ed6":"code","678d7776":"code","3ada29e6":"code","d03cf9ab":"code","8c2fa2bf":"code","c356610e":"code","75a419f7":"code","706754ae":"code","927e4834":"code","e3a293dd":"code","102334c7":"code","8bae4eae":"code","d38878ff":"code","b3e25bd5":"code","1cb0b001":"code","ec637418":"code","3e5823eb":"code","d60cb051":"code","53c3c9ec":"code","73071b22":"code","eb175f24":"markdown","09150d24":"markdown","19aa83a8":"markdown","f6820b9a":"markdown","585d5a41":"markdown","6b27b0a4":"markdown","ddd1d1f3":"markdown","4d81e1e0":"markdown","90342011":"markdown","533cf619":"markdown","26d6b5c2":"markdown","ddb1e3da":"markdown","79267b6f":"markdown","1ca1874a":"markdown","90494405":"markdown","038f82cc":"markdown","6d2dde6b":"markdown","ea8aee25":"markdown","13b417f3":"markdown","45eb7a0d":"markdown","9ca0f919":"markdown","f59accd5":"markdown","83687bc9":"markdown","1303acbb":"markdown"},"source":{"cf119ac2":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","d51aae6c":"import numpy as np\nimport pandas as pa\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.linear_model import LinearRegression as LR\nfrom sklearn.feature_selection import VarianceThreshold\nfrom sklearn.model_selection import train_test_split\n%matplotlib inline","e395b004":"train = pa.read_csv('..\/input\/train.csv')\ntest = pa.read_csv('..\/input\/test.csv')","96bf2cd0":"print ('train shape:'+ str(train.shape))\nprint ('test shape:'+ str(test.shape))\ntrain.head()\n","afe809d0":"test.head()","467153fd":"train.describe()","c6a70ed6":"fig,ax = plt.subplots(1,2, figsize=(14,4))\nax1, ax2 = ax.flatten()\nsns.countplot(train['spacegroup'], palette = 'viridis', ax = ax1)\nsns.countplot(x = train['number_of_total_atoms'], palette = 'magma', ax = ax2)","678d7776":"f,ax = plt.subplots(1,3,figsize=(14,4))\nfeat = train.columns[train.columns.str.startswith('percent')]\ntrain[feat].plot(kind='hist',subplots=True,figsize=(6,6),ax=ax)\nplt.tight_layout()\n","3ada29e6":"f,ax = plt.subplots(2,3,figsize=(14,4))\nfeat = train.columns[train.columns.str.startswith('lattice')]\ntrain[feat].plot(kind='hist',subplots=True,figsize=(6,6),ax=ax)\nplt.tight_layout()","d03cf9ab":"corr = train.corr()\nmask = np.zeros_like(corr, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\nf, ax = plt.subplots(figsize=(11, 9))\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\nsns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})","8c2fa2bf":"plt.figure(figsize=(14,8))\nplt.scatter(train['formation_energy_ev_natom'],train['bandgap_energy_ev'],color=['r','b'])","c356610e":"train['alpha_rad'] = np.radians(train['lattice_angle_alpha_degree'])\ntrain['beta_rad'] = np.radians(train['lattice_angle_beta_degree'])\ntrain['gamma_rad'] = np.radians(train['lattice_angle_gamma_degree'])\n\ntest['alpha_rad'] = np.radians(test['lattice_angle_alpha_degree'])\ntest['beta_rad'] = np.radians(test['lattice_angle_beta_degree'])\ntest['gamma_rad'] = np.radians(test['lattice_angle_gamma_degree'])","75a419f7":"def vol(df):\n    volumn = df['lattice_vector_1_ang']*df['lattice_vector_2_ang']*df['lattice_vector_3_ang']*np.sqrt(\n    1 + 2*np.cos(df['alpha_rad'])*np.cos(df['beta_rad'])*np.cos(df['gamma_rad'])\n    -np.cos(df['alpha_rad'])**2\n    -np.cos(df['beta_rad'])**2\n    -np.cos(df['gamma_rad'])**2)\n    df['volumn'] = volumn\nvol(train)\nvol(test)\n","706754ae":"train['density'] = train['number_of_total_atoms'] \/ train['volumn']\ntest['density'] = test['number_of_total_atoms'] \/ test['volumn']\n\n","927e4834":"def mean_median_feature(df):\n        dmean = df.mean()\n        dmedian = df.median()\n        q1 = df.quantile(0.25)\n        d2 = df.quantile(0.5)\n        q3 = df.quantile(0.75)\n        col = df.columns\n        del_col = ['id','formation_energy_ev_natom','bandgap_energy_ev']\n        col = [w for w in col if w not in del_col]\n        \n        for c in col:\n            df['mean_'+c] = (df[c] > dmean[c]).astype(np.uint8)\n            df['median_'+c] = (df[c] > dmedian[c]).astype(np.uint8)\n            df['q1_'+c] = (df[c] < q1[c]).astype(np.uint8)\n            df['q2_'+c] = (df[c] < q1[c]).astype(np.uint8)\n            df['q3_'+c] = (df[c] > q3[c]).astype(np.uint8)\n            \n        print('Shape',df.shape)\n\n\nmean_median_feature(train)\nmean_median_feature(test)\ntrain.shape","e3a293dd":"def OHE(df1,df2,columns):\n    len = df1.shape[0]\n    df = pa.concat([df1,df2],axis=0)\n    c2,c3 = [], {}\n    print('Categorical variables',columns)\n    for c in columns:\n        c2.append(c)\n        c3[c] = 'ohe_'+c\n        \n    df = pa.get_dummies(data = df, columns = c2, prefix = c3)\n    df1 = df.iloc[:len,:]\n    df2 = df.iloc[len:,:]\n    print('Data size',df1.shape,df2.shape)\n    return df1,df2\ncol = ['spacegroup','number_of_total_atoms']\ntrain1,test1 = OHE(train,test,col)","102334c7":"col = ['formation_energy_ev_natom','bandgap_energy_ev']\nX = train1.drop(['id']+col,axis=1)\ny = train1[col]\nvalid = test1.drop(['id']+col,axis=1)","8bae4eae":"# formation_energy_ev_natom\nfig,ax = plt.subplots(1,2,figsize=(14,4))\nax1,ax2 = ax.flatten()\nsns.distplot(train['formation_energy_ev_natom'],bins=50,ax=ax1,color='b')\nformation_energy_ev_log1 = np.log1p(train['formation_energy_ev_natom'])\nformation_energy_ev_log1 = pa.DataFrame({'log(1+formation_energy_ev)': formation_energy_ev_log1})\nsns.distplot(formation_energy_ev_log1['log(1+formation_energy_ev)'],bins=50,ax=ax2,color='r')","d38878ff":"\nfig,ax = plt.subplots(1,2,figsize=(14,4))\nax1,ax2 = ax.flatten()\nsns.distplot(train['bandgap_energy_ev'],bins=50,ax=ax1,color='b')\nbandgap_energy_ev_log1 = np.log1p(train['bandgap_energy_ev'])\nbandgap_energy_ev_log1 = pa.DataFrame({'log(1+ bandgap_energy_ev_log1)': bandgap_energy_ev_log1})\nsns.distplot(bandgap_energy_ev_log1['log(1+ bandgap_energy_ev_log1)'],bins=50,ax=ax2,color='r')","b3e25bd5":"\ny1 = y.formation_energy_ev_natom\ny1 = np.log1p(y.formation_energy_ev_natom)\ny1 = pa.DataFrame(y1.values, columns=['formation_energy_ev_natom'])\ny2 = y.bandgap_energy_ev\ny2 = np.log1p(y2)\ny2 = pa.DataFrame(y2.values, columns=['bandgap_Eev'])\ny = pa.concat([y.formation_energy_ev_natom, y2], axis=1)","1cb0b001":"X_train1, X_test1, y_train1, y_test1 = train_test_split(X, y1,  train_size=0.8, test_size=0.2, shuffle=True, random_state=400000)\nX_train2, X_test2, y_train2, y_test2 = train_test_split(X, y2,  train_size=0.8, test_size=0.2, shuffle=True, random_state=400000)","ec637418":"clf1 = LR().fit(X_train1, y_train1)\nclf2 = LR().fit(X_train2, y_train2)\npredict_test1 = clf1.predict(X_test1)\npredict_test2 = clf2.predict(X_test2)\npredict_test1 = np.array(predict_test1)\npredict_test2 = np.array(predict_test2)\ny_test1 = y_test1.values\ny_test1 = np.array(y_test1)\ny_test2 = y_test2.values\ny_test2 = np.array(y_test2)","3e5823eb":"y_org1 = np.exp(y_test1)-1\npredic_org1 = np.exp(predict_test1)-1\ny_org2 = np.exp(y_test2)-1\npredic_org2 = np.exp(predict_test2)-1","d60cb051":"def rmsle(y_true,y_pred):\n    assert len(y_true) == len(y_pred)\n    return np.square(np.log(y_pred + 1) - np.log(y_true + 1)).mean() ** 0.5\nrmsle1 = rmsle(y_org1, predic_org1)\nrmsle2 = rmsle(y_org2, predic_org2)\n\nRMSLE = (float(rmsle1 + rmsle2) \/ 2)\nprint('RMSLE:' + str(RMSLE))","53c3c9ec":"plt.scatter(y_org1, predic_org1)\nplt.xlabel('True Values (formation_energy_ev_natom)')\nplt.ylabel('Predictions (formation_energy_ev_natom)')\nplt.axis('equal')\nplt.axis('square')\nplt.xlim([0,plt.xlim()[1]])\nplt.ylim([0,plt.ylim()[1]])\n_ = plt.plot([-100, 100], [-100, 100])","73071b22":"plt.scatter(y_org2, predic_org2)\nplt.xlabel('True Values (bandgap_Eev)')\nplt.ylabel('Predictions (bandgap_Eev)')\nplt.axis('equal')\nplt.axis('square')\nplt.xlim([0,plt.xlim()[1]])\nplt.ylim([0,plt.ylim()[1]])\n_ = plt.plot([-100, 100], [-100, 100])","eb175f24":"As we see, there is a great overlap between the targets, so we will use each target separately.\nAfter testing the model for the first time,we also tried some other methods for optimization. The first one is that we add a new feature **volume** to the 3000 (training and test combined) crystal by following a general **volume** equation for lattice regardless of special length or angle as,\n\n \"number_of_total_atoms\"  to the crystal *volume*, this crystal is Triclinic type, where the valuem is :\nV=(lattice_vector_1_ang*lattice_vector_2_ang*lattice_vector_3_ang)*[1+2*cos(alpha)*cos(beta)*cos(gama)-cos(alpha)^2-cos(beta)^2-cos(gama)^2)^0.5\n![http:\/\/geoclasses.com\/wp-content\/uploads\/2018\/02\/volume-of-crystal-system-.jpg](http:\/\/)\n\n**BUT!! the angle shuld be in RADIANS**\n\nsee linke: http:\/\/geoclasses.com\/notes\/volume-of-crystal-system\/\n\n**Note that this is not simple normalization, normalization factor is not equal for  each  crystal !!**","09150d24":"We can see for the \"formation_energy_ev\" target, almost there are no changes.\nlet see what about \"bandgap_energy_ev\":","19aa83a8":"**1. Physical Introduction:**\n\nIn order to find the optimum composition for transparent crystal, some basic principle should\nbe the basic rule for computational approach. The alloys for transparent crystal should be (AlxGayInz)2NO3N, where x+y+z = 1 and N is an integer (usually between 5 and 100). There are infinite possible combinations for the values of x,y and z so the choice of computational method is the pivotal issue for transparent conductor materials design efficiency. There exist one primary computational method for solid-state physics caled **Density Functional Theory**, which is able to get high accuracy result but requires much computing time even for supercomputers. In this way, the data-driven method will be an alternative way to improve the efficiency for the transparent crystal design process.\n\n\n\n\n\n\n\n\n\n\n\n","f6820b9a":"**2.Data mining comcepts**\n\nThe database consists of 11 features with 2400 observations for *training*, another 400 observations for *test* and two *target* vectors. The ultimate goal is to predict the values of the two vectors based on the 11th features.\n\nA. For training: we will use some of the 2400 observations \nB. For validation : We will use the remaining part.\nC. For prediction: We used 400 test observation.\n","585d5a41":"Transform the angles from \"Deg\" into \"Radians\"","6b27b0a4":"**3.  Data explore and visualization**","ddd1d1f3":"Data after processing:","4d81e1e0":"we can se the relation between the true values and the predicted as linear fitting :","90342011":"Thered, another featuers  added by   covnert  the categorial features ''spacegroup' and ''number_of_total_atoms'' into  binary data using One Hot encoding:\n\n","533cf619":"**7. using Linear Regression for eache \"target\":**","26d6b5c2":"\n    \n  second,    add another features using quantile function in python, where we can divide the data into 3 levels , qun0.25, qun 0.5 and qun 0.75,\n    ","ddb1e3da":"Grate!! in \"gap_energy_ev\" target we can see \"skewed\"!!","79267b6f":"3. transform the target (for caluclate the RMS) into original units (without log(1+x)), using :\n    exp(x)-1:","1ca1874a":"**3. Data processing**\n\nFirst of all, we want to chick if there are no correlations between the two tearget vectors using scattring plot:\n","90494405":"**8. Calculate the  RMSLE:**","038f82cc":"the density:","6d2dde6b":"precintage data","ea8aee25":"**5. Fitting the targets**\n\nFor accurate scores, the target most be normal (Gaussian) distributed, thefore, we can use log(x+1) function in order to transform the targets as Gaussian distributed:","13b417f3":"**This notebook format submitted according to the guidelines of a data mining course belonging to Ben-Gurion University of the Negev, Winter 2018**\n\n**CONTENTS**\n1. Physical introduction\n2. Data mining concepts\n3.  Data explore and visualization\n4. Data processing: using physical methods for \"atoms number\" normalization, add new featuers by quantiles (dividing the range of a probability distribution into continuous intervals with equal probabilities)and add new featuers via convert the categorial features into binary using    \"One Hot encoding\" function\n5. Fitting the targets to Gaussian distribution by log(x+1): using \"numpy.log1p()\n6. hold-out methods  cross validation: using \"train_test_split\"\n7. Linear Regression \n8. RMSLE calculation\n","45eb7a0d":"Categorial data","9ca0f919":"**6. using train-test-split for crros validation**\n\nin order to avoid overfitting, we used hold-out  cross validation!","f59accd5":"discrit data","83687bc9":"Calculate the valume of the crystal","1303acbb":"Correlation plot"}}