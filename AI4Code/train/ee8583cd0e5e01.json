{"cell_type":{"1276e292":"code","2beb237a":"code","5218c2f9":"code","827294ab":"code","77d902d7":"code","9449be7e":"code","59f271bd":"code","b709fea1":"code","7be1d17e":"code","3e4a105c":"code","a301a342":"code","fb06e398":"code","f4c4ed3c":"code","a7cb81dd":"code","451790bd":"code","5275c528":"code","b0f03d5b":"code","b92e6988":"code","22a6f1e3":"code","b4f4c6ee":"code","e291c8b8":"code","f5746911":"code","4e2736ba":"code","fdd83565":"code","72900875":"code","dcb07e9c":"code","15528ea0":"code","0a090739":"code","d3e27373":"code","aaca9571":"code","bcd9723a":"code","c7330b57":"code","17c3102b":"code","bd0ab99b":"code","bc0ec716":"code","5a4017bf":"code","935adae4":"code","cbb8478d":"code","8f412c00":"code","e5491024":"code","cb828178":"code","dde28949":"code","83b7d9db":"code","047157e0":"code","0e64c094":"code","dd3baa1c":"code","22089f69":"code","44741e18":"code","b3130ba7":"code","bb399a54":"code","859ffe01":"code","cf42930b":"code","ffd074fb":"code","906e5285":"code","bd3c2c35":"markdown","bc989f86":"markdown","8d3a8299":"markdown","24b97eb9":"markdown","89fcd7b0":"markdown","f77ec85a":"markdown","81b708c2":"markdown","6010e8ca":"markdown","17b126d2":"markdown","4676c6bd":"markdown","e818625c":"markdown","a4d71d02":"markdown","94324577":"markdown"},"source":{"1276e292":"import tensorflow as tf\nimport numpy as np\nimport os\nimport time\nimport pandas as pd","2beb237a":"data = '..\/input\/resumo-carta-pedro-vaz-de-caminha\/Carta_a_El_Rei_D._Manuel_(ortografia_atualizada).txt'","5218c2f9":"dataset_text = open(data, 'rb').read().decode(encoding = 'utf-8')","827294ab":"print(dataset_text)","77d902d7":"len(dataset_text)","9449be7e":"vocab = sorted(set(dataset_text))","59f271bd":"print('{} unique characters'.format(len(vocab)))","b709fea1":"vocab","7be1d17e":"char2idx = {char: index for index, char in enumerate(vocab)}","3e4a105c":"char2idx","a301a342":"idx2char = np.array(vocab)","fb06e398":"idx2char","f4c4ed3c":"idx2char[10]","a7cb81dd":"char2idx[':']","451790bd":"text_as_int = np.array([char2idx[char] for char in dataset_text])","5275c528":"text_as_int","b0f03d5b":"text_as_int.shape","b92e6988":"print('{} characters mapped to int ---> {}'.format(repr(dataset_text[:13]), text_as_int[:13]))","22a6f1e3":"len(dataset_text)","b4f4c6ee":"seq_length = 100\nexamples_per_epoch = len(dataset_text) \/\/ seq_length\nexamples_per_epoch","e291c8b8":"char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)","f5746911":"char_dataset","4e2736ba":"sequences = char_dataset.batch(seq_length + 1, drop_remainder=True)","fdd83565":"sequences","72900875":"for item in sequences.take(50):\n  print(repr(''.join(idx2char[item.numpy()])))","dcb07e9c":"def split_input_target(chunk):\n  input_text = chunk[:-1]\n  target_text = chunk[1:]\n  return input_text, target_text","15528ea0":"dataset = sequences.map(split_input_target)","0a090739":"for input_example, target_example in dataset.take(10):\n  print('Input data:', repr(''.join(idx2char[input_example.numpy()])))\n  print('Target data:', repr(''.join(idx2char[target_example.numpy()])))","d3e27373":"batch_size = 64\nbuffer_size = 10000","aaca9571":"dataset = dataset.shuffle(buffer_size).batch(batch_size, drop_remainder = True)","bcd9723a":"dataset","c7330b57":"len(vocab)","17c3102b":"vocab_size = len(vocab)","bd0ab99b":"embedding_dim = 256","bc0ec716":"rnn_units = 1024","5a4017bf":"def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n  model = tf.keras.Sequential([tf.keras.layers.Embedding(vocab_size, embedding_dim, batch_input_shape=[batch_size, None]),\n                               tf.keras.layers.LSTM(rnn_units, return_sequences=True, stateful=True, recurrent_initializer='glorot_uniform'),\n                               tf.keras.layers.LSTM(rnn_units, return_sequences=True, stateful=True, recurrent_initializer='glorot_uniform'),\n                               tf.keras.layers.LSTM(rnn_units, return_sequences=True, stateful=True, recurrent_initializer='glorot_uniform'), \n                               tf.keras.layers.Dense(vocab_size)])\n  return model","935adae4":"model = build_model(vocab_size = len(vocab), embedding_dim=embedding_dim, rnn_units=rnn_units, batch_size=batch_size)","cbb8478d":"for input_example_batch, target_example_batch in dataset.take(10):\n  example_batch_predictions = model(input_example_batch)\n  print(example_batch_predictions.shape)","8f412c00":"sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)","e5491024":"sampled_indices","cb828178":"sampled_indices = tf.squeeze(sampled_indices, axis = -1).numpy()","dde28949":"sampled_indices","83b7d9db":"print('Input: \\n', repr(''.join(idx2char[input_example_batch[0]])))\nprint()\nprint('Next char predictions: \\n', repr(''.join(idx2char[sampled_indices])))","047157e0":"def loss(labels, logits):\n  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)","0e64c094":"example_batch_loss = loss(target_example_batch, example_batch_predictions)","dd3baa1c":"example_batch_loss.numpy().mean()","22089f69":"model.compile(optimizer='Adam', loss=loss)","44741e18":"checkpoint_dir = '.\/training_checkpoints'\ncheckpoint_prefix = os.path.join(checkpoint_dir, 'ckpt_{epoch}')\ncheckpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_prefix, save_weights_only=True)","b3130ba7":"epochs = 75\nhistory = model.fit(dataset, epochs = epochs, callbacks=[checkpoint_callback])","bb399a54":"tf.train.latest_checkpoint(checkpoint_dir)","859ffe01":"model = build_model(vocab_size, embedding_dim, rnn_units, batch_size = 1)\nmodel.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\nmodel.build(tf.TensorShape([1, None]))","cf42930b":"model.summary()","ffd074fb":"def generate_text(model, start_string):\n  \n  num_generate = 1000\n\n  \n  input_eval = [char2idx[s] for s in start_string]\n  input_eval = tf.expand_dims(input_eval, 0)\n  \n  text_generated = []\n   \n  temperature = 1.0\n\n  \n  for i in range(num_generate):\n    \n    predictions = model(input_eval)\n\n    \n    predictions = tf.squeeze(predictions, 0)\n    predictions = predictions \/ temperature\n    predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n\n    \n    input_eval = tf.expand_dims([predicted_id], 0)\n\n    text_generated.append(idx2char[predicted_id])\n  \n  return (start_string + ''.join(text_generated))","906e5285":"print(generate_text(model, start_string='Pedro Vaz : '))","bd3c2c35":"# Creation of training examples and batches","bc989f86":"# Text Generation","8d3a8299":"Essa \u00e9 a carta completa de pedro Vaz de Caminha de quando avistou a costa do Brasil.\n\nNeste trabalho foi utilizado as redes neurais recorrentes.\n\nSe for realizado um tratamento com Stop Words ficar\u00e1 ainda melhor, mas eu n\u00e3o utilizei aqui.\n\nNo come\u00e7o era apenas uma camada da rede neural eu aumentei ela em mais cinco camada, mas o notebook come\u00e7ou a travar ent\u00e3o mantive apenas duas.\n\nPara melhorar, poderia ser feitoi o tratamento com as stop words, aumentar as camadas da rede neural e o n\u00famero de ep\u00f3cas, mas vai exigir uma m\u00e1quina um pouco mais forte.\n\nObrigado!","24b97eb9":"# **If you find this notebook useful, support with an upvote** \ud83d\udc4d","89fcd7b0":"# Execution of training","f77ec85a":"# Mapping text to numbers","81b708c2":"**Optimizer and loss function**","6010e8ca":"Restore last checkpoint","17b126d2":"# Database loading and exploration","4676c6bd":"This is the complete letter from Pedro Vaz de Caminha when he sighted the coast of Brazil.\n\nIn this work, recurrent neural networks were used.\n\nIf a treatment with Stop Words is performed, it will be even better, but I didn't use it here.\n\nIn the beginning it was just one layer of the neural network I increased it by five more layers, but the notebook started to crash so I kept only two.\n\nTo improve, it could be done the treatment with stop words, increasing the layers of the neural network and the number of epochs, but it will require a machine a little stronger.\n\nThanks!","e818625c":"# Model training","a4d71d02":"# Prediction Loop","94324577":"# Model building"}}