{"cell_type":{"429f3184":"code","36d5af7d":"code","00c6285d":"code","523d1b1d":"code","4b7d1c9a":"code","2eb12fa9":"code","c8a8feed":"code","4be37b2f":"code","6ecfce81":"code","6a9b687e":"code","d40733e6":"code","e72392f6":"code","1a82e95f":"code","47294acb":"code","53c3aec9":"code","d4cda3a6":"code","c9cbc052":"code","3e3ddb6b":"code","6b9d4006":"code","b2886c68":"code","3a569ef8":"code","4c06071a":"code","deaf29c8":"code","261b50ab":"code","b39c5797":"code","cdf5f5b2":"code","e9e6cc38":"code","0595a564":"code","2c29482f":"code","52484e6a":"code","901421c0":"code","cf867707":"code","a81066fa":"code","b6a3cf0c":"code","ba23ed2b":"code","9b3c3187":"code","939a5c54":"code","68ec3413":"code","fd842b5a":"code","32e69813":"code","3d6ff032":"code","68536986":"code","d9a4f1ba":"code","427674ba":"code","ff32d169":"code","067cb5e6":"code","2cdb03db":"code","d1960f83":"code","3e5611e7":"code","35aa0fd3":"code","cbf4cadb":"code","002981ae":"code","c5d2f52c":"code","19e83304":"code","ea94451b":"code","dbae094a":"code","e57d0e76":"code","5a752f4d":"code","7fff5448":"code","18f1be84":"code","e327595a":"code","464081d4":"code","63dcd38b":"code","f0015af0":"code","2f9ae20f":"code","07f43641":"code","fa2cdf8a":"code","2d243e53":"code","cb47ff26":"code","bc34ab76":"code","3b5051cc":"code","5c437779":"code","8f8e9098":"code","cbb58c8c":"code","009aa575":"code","c484d797":"code","9d050085":"markdown","a39ffb64":"markdown","a1ad43a1":"markdown","4f44cea7":"markdown","0e767ae3":"markdown","426dcd6e":"markdown","09e3b9b9":"markdown","5d5ad4b3":"markdown","7f31bef2":"markdown","f445664c":"markdown","83241b52":"markdown","b4649142":"markdown","af058390":"markdown","98652a23":"markdown","aa0c0edd":"markdown","6994fac9":"markdown","42db7540":"markdown","43a5317a":"markdown","595af8ef":"markdown","e13151b2":"markdown","ab3aaa3a":"markdown","4f437c9b":"markdown","3bf654e3":"markdown","5e58e0bd":"markdown","289e6ccd":"markdown","4eef50d7":"markdown","f4d2839f":"markdown","59b96f22":"markdown","f6caf2fc":"markdown","42c938c4":"markdown","e3e4a50f":"markdown","eefaa5a9":"markdown","23db92dd":"markdown","326cc27f":"markdown","03d5a877":"markdown","ca8e049d":"markdown","21423132":"markdown","de108eb7":"markdown","2c6a9b60":"markdown","5cc541e2":"markdown","7c25a3fb":"markdown","78c0c6ed":"markdown","6f59a3c7":"markdown","4053ad82":"markdown"},"source":{"429f3184":"# Reding the data\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.graph_objects as go","36d5af7d":"#!pip install plotly","00c6285d":"df = pd.read_csv('..\/input\/online-shoppers-intention\/online_shoppers_intention.csv')\ndf.head()","523d1b1d":"df.shape","4b7d1c9a":"#Converting the data types as required\ndf['OperatingSystems'] = df['OperatingSystems'].astype('object')\ndf['Browser'] = df['Browser'].astype('object')\ndf['Region'] = df['Region'].astype('object')\ndf['TrafficType'] = df['TrafficType'].astype('object')\ndf['Revenue']= df['Revenue'].replace({True:'Yes',False:'No'})","2eb12fa9":"df.info()","c8a8feed":"df.describe()","4be37b2f":"df.isnull().sum()","6ecfce81":"for i in df.select_dtypes(include='object').columns:\n    print(i)\n    print(df[i].unique())\n    print()","6a9b687e":"#visualizations with insights\n\nimport plotly.graph_objs as go\nimport plotly.offline as py","d40733e6":"revenue = df['Revenue'].value_counts()\nlabel = revenue.index\nvalue = revenue.values","e72392f6":"colors = ['gold', 'mediumturquoise']\nfig = go.Figure(data=go.Pie(labels=label,values=value,marker=dict(colors=colors)))\n\nfig.update_traces(hoverinfo='label+percent', textfont_size=20,\n                  marker=dict(colors=colors, line=dict(color='#000000', width=2)))\n\nfig.update_layout(font_color=\"black\",font=dict(size=18),title={\n        'text': \"Revenue\",\n        'y':0.9,\n        'x':0.5,\n        'xanchor': 'center',\n        'yanchor': 'top'})\nfig.show()\n","1a82e95f":"page = pd.pivot_table(df,values=['Administrative','Informational','ProductRelated'],index='Revenue',aggfunc='sum')\n\nplt.rcParams['font.family'] = 'sans-serif'\nplt.rcParams['font.sans-serif'] = 'Helvetica'\n\n# set the style of the axes and the text color\nplt.rcParams['axes.edgecolor']='#333F4B'\nplt.rcParams['axes.linewidth']=0.8\nplt.rcParams['xtick.color']='#333F4B'\nplt.rcParams['ytick.color']='#333F4B'\nplt.rcParams['text.color']='#333F4B'\n\npage.T.plot(kind='barh',figsize=(8,5),color=['gold', 'mediumturquoise'])\nplt.title('Page-wise Revenue Conversion',fontweight='bold',size=15,color='black')\nplt.ylabel('Type of Page')\nplt.xlabel('Sum of Visits')\nplt.show()","47294acb":"pd.pivot_table(df,values=['Administrative_Duration','Informational_Duration','ProductRelated_Duration'],index='Revenue').T.plot(kind='barh',figsize=(8,5),color=['gold', 'mediumturquoise'])\nplt.title('Page-wise time (in seconds)',fontweight='bold',size=15,color='black')\nplt.ylabel('Type of Page')\nplt.xlabel('Duration of Visit')\nplt.show()","53c3aec9":"for i in df.select_dtypes(include=['object','bool']).columns:\n    sns.countplot(i,data=df)\n    plt.title(i)\n    plt.show()\n\n# Majority of the sessions in the data pertain to the month of may followed by November\n# Majority of the sessions happened on from systems having Operating System,Browser Number and Traffic type 2 and region 1\n# Majority of the visitors are returning visitors","d4cda3a6":"from mpl_toolkits.mplot3d import Axes3D","c9cbc052":"fig = plt.figure(1, figsize=(8, 6))\nax = Axes3D(fig)\nax.scatter(df['BounceRates'],df['ExitRates'],df['PageValues'],c=df['Revenue'].replace({'Yes':0,'No':1}),\n           cmap=plt.cm.Set1, edgecolor='k',s=40)\nax.set_title(\"3D view of Bounce Rates, Exit Rates and Page Value\",fontweight='bold')\nax.set_xlabel(\"Bounce Rate\")\n#ax.w_xaxis.set_ticklabels([])\nax.set_ylabel(\"Exit Rate\")\n#ax.w_yaxis.set_ticklabels([])\nax.set_zlabel(\"Page Value\")\n#ax.w_zaxis.set_ticklabels([])\nplt.show()","3e3ddb6b":"rev_yes_month = df[df['Revenue']=='Yes']['Month'].value_counts()\nlabel = rev_yes_month.index\nvalues = rev_yes_month.values","6b9d4006":"rev_no_month =  df[df['Revenue']=='No']['Month'].value_counts()","b2886c68":"plt.figure(figsize=(20,12))\nmy_circle=plt.Circle( (0,0), 0.7, color='white')\nplt.pie(rev_yes_month,labels=label,shadow=True,colors=['#ff80bf','#ff9966', '#ffff80', '#b3ccff', '#9467bd', '#8c564b', '#ccffff', '#7f7f7f', '#bcbd22', '#17becf'],autopct='%1.2f%%')\np=plt.gcf()\np.gca().add_artist(my_circle)\nplt.title('Monthly Contribution to Revenue (Revenue Positive Cases)',fontweight= 'bold',size=15)\nplt.show()\n","3a569ef8":"for j in ['OperatingSystems','Browser','Region','TrafficType']:\n    reg = round(df[df['Revenue']=='Yes'][j].value_counts(normalize=True)*100,2)\n    ind = []\n    for i in reg.index:\n        i = str(i)\n        v = j+' '+i\n        ind.append(v)\n        v =''\n\n    from plotly import graph_objects as go\n    fig = go.Figure(\n       go.Funnel(\n          y = ind,\n          x = reg.values\n       )\n    )\n\n    fig.update_layout(font_color=\"black\",font=dict(size=8),title={\n            'text': \"{} (%) with Revenue\".format(j),\n            'y':0.9,\n            'x':0.5,\n            'xanchor': 'center',\n            'yanchor': 'top'})\n    fig.show()","4c06071a":"plt.figure(figsize=(8,5))\nsns.scatterplot(x= 'BounceRates',y='ExitRates',data=df,hue='Revenue',palette='nipy_spectral')\nplt.title('Bounce Rate vs. Exit Rate',fontweight='bold',fontsize=15)\nplt.show()\n\n# Higher bounce rates are generally associated with higher exit rates","deaf29c8":"num = df.select_dtypes(include=np.number)\ncat = df.select_dtypes(exclude=np.number)","261b50ab":"#plotting all numerical variables with the target variable\n\nfor i in num.columns:\n    sns.boxplot(x='Revenue',y=i,data=df)\n    plt.title(i)\n    plt.show()\n#Inferences -     \n# Median number of pages visited and time spent on them for administrative purposes on the website is higher for sessions which yielded revenue\n# It appears that median number of visit to Informational pages and time spent on them is approximately 0. \n# Median number of product related pages visited and time spent on them is higher for sessions which yielded revenue.\n# However, there appears to be a lot of outliers where no-revenue sessions had users visiting a lot of pages \n# and spending time on them but not purchasing the product.\n# Bounce Rates and exit rates of pages were relatively lower for sessions which yielded revenue. \n# This shows that it is important for websites to have a lower bounce rates and exit rates on their pages.\n# Similarly Average Page value of a page that user visited before making a transactions is visibly higher in revenue positive sessions\n# Visually, it appears that closeness to a special day stays at median value of 0 for both revenue and non-revenue cases.\n# This is because majority of the times, there is no closeness to a festival. \n","b39c5797":"#plotting categorical variables with target\nfor i in cat.columns.drop('Revenue'):\n    z = pd.crosstab(df[i],df['Revenue'])\n    z.div(z.sum(1),axis=0).plot(kind='bar',stacked=True)\n    plt.title('Revenue vs {}'.format(i))\n    plt.show()\n    \n# It appears proportion of session which yielded revenue is the highest in november\n# February has the lowest proportion of sessions which yielded revenue\n# Proportion of sessions which yielded revenue is the highest among users having operating system 8 \n# while operating system 3 yields the lowest proportion of these revenue positive sessions\n# Browser 12 has the highest proportion of revenue positive sessions followed by browser 13 while browser 3 has the least contribution\n# Region does not appear to play a significant role in determining whether the customer will purchase or not.\n# Proportion of revenue positive sessions is the highest among new visitors followed by other visitors. \n# Returning visitor sessions have a relatively lower proportion of revenue positive sessions.\n# If it's a weekend, there are slightly more chances of having a sale! Visually, the difference does not appear to be a very big one!\n","cdf5f5b2":"new_df = pd.concat([num,cat],axis=1)\nnew_df.head()","e9e6cc38":"import plotly.express as px","0595a564":"px.parallel_categories(new_df[['OperatingSystems','Browser']])","2c29482f":"px.parallel_categories(new_df[['Revenue','OperatingSystems','Browser']])","52484e6a":"px.parallel_categories(new_df[['VisitorType','Revenue']])","901421c0":"import scipy.stats as stats\nfor i in num.columns:\n    df0 = df[df['Revenue']=='Yes'][i]\n    df1 = df[df['Revenue']=='No'][i]\n    print(i)\n    print('Pvalue:',stats.ttest_ind(df0,df1)[1])\n    print()\n    \n#All numerical columns in the data are statistically significant","cf867707":"for i in cat.columns:\n    z = pd.crosstab(df[i],df['Revenue'])\n    chistat,pval,deg,exp_freq = stats.chi2_contingency(z)\n    print(i)\n    print('Pvalue:',pval)\n    print()","a81066fa":"for i in num.columns:\n    sns.boxplot(df[i])\n    plt.title(i)\n    plt.show()\n    \n    \n#Inference - \n# Although we have a lot of outliers in the data, these are pretty realistic outliers.\n# Therefore, I will not be removing the same. I will be treating the same by applying Power Transformation","b6a3cf0c":"from sklearn.preprocessing import PowerTransformer\npt = PowerTransformer()\npt_num = pd.DataFrame(pt.fit_transform(num),columns=num.columns)\n\nfor i in pt_num.columns:\n    sns.boxplot(pt_num[i])\n    plt.title(i)\n    plt.show()\n    \n#Then impact of outliers has been reduced considerably!","ba23ed2b":"# Removing statistically insignificant variables\n\ncat.drop('Region',axis=1,inplace=True)","9b3c3187":"# Creating a new data Frame without power transformed numerical variables. Will add power transformed values for\n# comparison later\n\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nsc_num = pd.DataFrame(sc.fit_transform(num),columns=num.columns)\n\n\nnew_df = pd.concat([sc_num,cat],axis=1)\n\n\n# numerically encoding Weekend and Revenue\n\nnew_df['Weekend'].replace({False:0,True:1},inplace =True)\nnew_df['Revenue'].replace({'No':0,'Yes':1},inplace =True)\nnew_df.head()","939a5c54":"# One hot encoding \/ dummy variables\n\nsc_df = pd.get_dummies(new_df,drop_first=True)\nsc_df.head()","68ec3413":"y = sc_df['Revenue']\nx = sc_df.drop('Revenue',1)\nfrom sklearn.model_selection import train_test_split\n\nxtrain,xtest,ytrain,ytest = train_test_split(x,y,test_size=0.3,random_state = 2)\nprint('xtrain',xtrain.shape)\nprint('ytrain',ytrain.shape)\nprint('xtest',xtest.shape)\nprint('ytest',ytest.shape)","fd842b5a":"#fitting a base model for comparison with final model\n\nfrom sklearn.linear_model import LogisticRegression\nlog_reg = LogisticRegression(solver='liblinear')\nlog_reg.fit(xtrain,ytrain)\n\n\nfrom sklearn.metrics import accuracy_score,precision_score,recall_score,f1_score,confusion_matrix,roc_auc_score\n\nypred_train = log_reg.predict(xtrain)\nypred_test = log_reg.predict(xtest)\n\nprint('Train Accuracy:',accuracy_score(ytrain,ypred_train))\nprint('Test Accuracy:',accuracy_score(ytest,ypred_test))\n\n# Accuracy on the test set is 0.89. As a base model, this accuracy is pretty decent. \n# Also, accuracy is not a very reliable metric in terms of imbalanced data.\n# Accuracy = (TP+TN)\/(TP+TN +FP+FN)\n\nprint()\nprint('Train Precision:',precision_score(ytrain,ypred_train))\nprint('Test Precision:',precision_score(ytest,ypred_test))\n\nprint()\n\nprint('Train Recall:',recall_score(ytrain,ypred_train))\nprint('Test Recall:',recall_score(ytest,ypred_test))\n\nprint()\n\nprint('Train f1 score:',f1_score(ytrain,ypred_train))\nprint('Test f1 score:',f1_score(ytest,ypred_test))","32e69813":"# ROC AUC Scores\n\nytrain_prob = log_reg.predict_proba(xtrain)[:,1]\nytest_prob = log_reg.predict_proba(xtest)[:,1]\n\nprint('Train ROC AUC Score',roc_auc_score(ytrain,ytrain_prob))\nprint('Test ROC AUC Score',roc_auc_score(ytest,ytest_prob))","3d6ff032":"Final = pd.DataFrame(index= ['Train Accuracy','Test Accuracy','Train Precision','Test Precision','Train Recall',\\\n                     'Test Recall','Train f1 score','Test f1 score','Train ROC AUC Score','Test ROC AUC Score'])\n\nFinal['Base Model'] = [accuracy_score(ytrain,ypred_train),accuracy_score(ytest,ypred_test),precision_score(ytrain,ypred_train),\\\n                      precision_score(ytest,ypred_test),recall_score(ytrain,ypred_train),recall_score(ytest,ypred_test),\\\n                      f1_score(ytrain,ypred_train),f1_score(ytest,ypred_test),roc_auc_score(ytrain,ytrain_prob),\\\n                       roc_auc_score(ytest,ytest_prob)]","68536986":"from sklearn.preprocessing import PowerTransformer\npt = PowerTransformer()\npt_num = pd.DataFrame(pt.fit_transform(num),columns=num.columns)\n\n\npt_df = pd.concat([pt_num,cat],axis=1)\n\n\n# numerically encoding Weekend and Revenue\n\npt_df['Weekend'].replace({False:0,True:1},inplace =True)\npt_df['Revenue'].replace({'No':0,'Yes':1},inplace =True)\npt_df.head()","d9a4f1ba":"# One hot encoding \/ dummy variables\n\npt_df = pd.get_dummies(pt_df,drop_first=True)\npt_df.head()","427674ba":"y = pt_df['Revenue']\nx = pt_df.drop('Revenue',1)\nfrom sklearn.model_selection import train_test_split\n\nxtrain,xtest,ytrain,ytest = train_test_split(x,y,test_size=0.3,random_state = 2)\nprint('xtrain',xtrain.shape)\nprint('ytrain',ytrain.shape)\nprint('xtest',xtest.shape)\nprint('ytest',ytest.shape)","ff32d169":"log_reg = LogisticRegression(solver='liblinear')\nlog_reg.fit(xtrain,ytrain)\nypred_train = log_reg.predict(xtrain)\nypred_test = log_reg.predict(xtest)\n\nprint('Train Accuracy:',accuracy_score(ytrain,ypred_train))\nprint('Test Accuracy:',accuracy_score(ytest,ypred_test))\n\nprint()\nprint('Train Precision:',precision_score(ytrain,ypred_train))\nprint('Test Precision:',precision_score(ytest,ypred_test))\n\nprint()\n\nprint('Train Recall:',recall_score(ytrain,ypred_train))\nprint('Test Recall:',recall_score(ytest,ypred_test))\n\nprint()\n\nprint('Train f1 score:',f1_score(ytrain,ypred_train))\nprint('Test f1 score:',f1_score(ytest,ypred_test))\n\nprint()\n\nytrain_prob = log_reg.predict_proba(xtrain)[:,1]\nytest_prob = log_reg.predict_proba(xtest)[:,1]\n\nprint('Train ROC AUC Score',roc_auc_score(ytrain,ytrain_prob))\nprint('Test ROC AUC Score',roc_auc_score(ytest,ytest_prob))","067cb5e6":"Final['Base Model - Power Transformed'] = [accuracy_score(ytrain,ypred_train),accuracy_score(ytest,ypred_test),precision_score(ytrain,ypred_train),\\\n                      precision_score(ytest,ypred_test),recall_score(ytrain,ypred_train),recall_score(ytest,ypred_test),\\\n                      f1_score(ytrain,ypred_train),f1_score(ytest,ypred_test),roc_auc_score(ytrain,ytrain_prob),\\\n                       roc_auc_score(ytest,ytest_prob)]","2cdb03db":"Final","d1960f83":"from sklearn.metrics import plot_roc_curve\nplot_roc_curve(log_reg,xtrain,ytrain)\nplt.title('Training')\nplt.show()","3e5611e7":"plot_roc_curve(log_reg,xtest,ytest)\nplt.title('Testing')\nplt.show()","35aa0fd3":"# finally plotting confusion matrix for the test results\n\nsns.heatmap(pd.DataFrame(confusion_matrix(ytest,ypred_test)),annot=True,fmt='d',cbar=False)\nplt.title('Confusion matrix')\nplt.show()\n\n#here 167 sessions which actually did not turn into a sale were falsely predicted as revenue positive.\n#here 209 revenue yielding sessions were not able to get detected and were predicted as no revenue sessions\n\n# This needs improvement!","cbf4cadb":"from sklearn.model_selection import GridSearchCV,KFold\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.ensemble import RandomForestClassifier,BaggingClassifier,GradientBoostingClassifier,AdaBoostClassifier,VotingClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom xgboost import XGBClassifier","002981ae":"#Hyper parameter tuning for KNN using Grid Search CV\n# knn = KNeighborsClassifier()\n# params = {'n_neighbors':np.arange(5,100),'weights':['uniform','distance']}\n# #kf = KFold(n_splits=5,shuffle=True, random_state=4)\n# gsv = GridSearchCV(knn,params,cv=5,scoring='roc_auc')\n# gsv.fit(xtrain,ytrain)\n# gsv.best_params_","c5d2f52c":"#Hyper parameter tuning for Decision Tree classifier using Grid Search CV. \n# It is important to restrict the depth of decision trees otherwise it will overfit.\n\n# dt = DecisionTreeClassifier()\n# params = {'max_depth':np.arange(1,11),'criterion':['gini','entropy']}\n# gsv = GridSearchCV(dt,params,cv=5,scoring='roc_auc')\n# gsv.fit(xtrain,ytrain)\n# gsv.best_params_","19e83304":"# Customized grid search for n estimators of Random Forest (for getting the least variance error in scores)\n\n# ve = []\n\n# for i in range(1,101):\n#     rf = RandomForestClassifier(n_estimators=i,random_state=0)\n#     auc = cross_val_score(rf,xtrain,ytrain,cv =5,scoring ='roc_auc')\n#     v = np.std(auc,ddof=1)\n#     ve.append(v)\n# n_est = np.argmin(ve)+1    \n\nprint('Lowest Variance Error for n_estimators:',17)","ea94451b":"# Tuning the hyperparameters other than n estimators using grid search CV\n\n# rf = RandomForestClassifier(n_estimators=17,random_state=0)\n# params = {'max_depth':np.arange(1,11),'criterion':['gini','entropy']}\n# gsv = GridSearchCV(rf,params,cv=5,scoring='roc_auc')\n# gsv.fit(xtrain,ytrain)\n# gsv.best_params_","dbae094a":"\n# Tuning the XGBoost model for max depth & learning rate\n\n# tuned_xgb = XGBClassifier(random_state=2)\n# params = {'max_depth':np.arange(1,11),'learning_rate':[0.1,0.2,0.5,0.75]}\n# gsv = GridSearchCV(tuned_xgb,params,cv=5,scoring='roc_auc')\n# gsv.fit(xtrain,ytrain)\n# gsv.best_params_\n\n# Best Estimators : {'learning_rate': 0.1, 'max_depth': 3}","e57d0e76":"# summarizing models after tuning\/ (wherever applicable)\n\nlog_reg = LogisticRegression(solver='liblinear')\nbagged_log_reg = BaggingClassifier(log_reg,random_state=0)\nboosted_log_reg = AdaBoostClassifier(log_reg,random_state=20)\nknn_tuned = KNeighborsClassifier(n_neighbors=92,weights='distance')\nbagged_knn = BaggingClassifier(knn_tuned,random_state=0)\ndt_reg = DecisionTreeClassifier(max_depth=4,criterion='entropy',random_state=0)\nrf_tuned = RandomForestClassifier(n_estimators=17,criterion='entropy',max_depth=9,random_state=0)\ngbc = GradientBoostingClassifier(random_state=0,n_estimators=100)\nabc = AdaBoostClassifier(random_state=0)\nxgb = XGBClassifier(max_depth=3,learning_rate=0.1,random_state=2)\nvoting = VotingClassifier([('Adaboosting',abc),('XGboost',xgb)],voting='soft')\n","5a752f4d":"models = [('Logistic Regression',log_reg)]\nmodels.append(('Baaged LR',bagged_log_reg))\nmodels.append(('Boosted LR',boosted_log_reg))\nmodels.append(('Tuned KNN',knn_tuned))\nmodels.append(('Bagged KNN',bagged_knn))\nmodels.append(('Regularized DT',dt_reg))\nmodels.append(('Tuned Random Forest',rf_tuned))\nmodels.append(('Gradient Boosting Classifier',gbc))\nmodels.append(('Adaboosting Classifier',abc))\nmodels.append(('XGboost',xgb))\nmodels.append(('Voting Classifier',voting))\nnames = []\nve = []\nbe = []\n\nresults = []\n\nfor name,model in models:\n    names.append(name)\n    auc = cross_val_score(model,xtrain,ytrain,cv =5,scoring ='roc_auc')\n    v = np.std(auc,ddof=1)\n    ve.append(v)\n    b = np.mean(1-auc)\n    be.append(b)\n    results.append(1-auc)    ","7fff5448":"#score card\n\nscore = pd.DataFrame()\nscore['Model'] = names\nscore['Bias Error'] = be\nscore['Variance error'] = ve\nscore","18f1be84":"fig = plt.figure(figsize=(17,8))\nfig.suptitle('Agorithm Comparison in error terms')\nax = fig.add_subplot(111)\nplt.boxplot(results)\nax.set_xticklabels(names,rotation=90)\nplt.show()","e327595a":"from sklearn.feature_selection import RFECV\n\nrfe = RFECV(xgb,cv=3,scoring = 'roc_auc')\nrfe.fit(xtrain,ytrain)","464081d4":"rfe_df = pd.DataFrame(list(xtrain),columns=['Feature'])\nrfe_df['Rank'] = rfe.ranking_\nbest_features = rfe_df[rfe_df['Rank'] == 1]['Feature'].values\nprint('Best Features selected :')\nprint(best_features)","63dcd38b":"xtrain_rfe = xtrain[best_features]\n\nve = []\nbe = []\nauc = cross_val_score(xgb,xtrain_rfe,ytrain,cv =5,scoring ='roc_auc')\nv = np.std(auc,ddof=1)\nve.append(v)\nb = np.mean(1-auc)\nbe.append(b)","f0015af0":"#score card\n\nscore_rfe = pd.DataFrame()\nscore_rfe['Model'] = ['XGBoost with RFECV']\nscore_rfe['Bias Error'] = be\nscore_rfe['Variance error'] = ve\nscore_rfe","2f9ae20f":"score.iloc[[9]].append(score_rfe)","07f43641":"xgb = XGBClassifier(max_depth=3,learning_rate=0.1,random_state=2)\nxgb.fit(xtrain,ytrain)\nypred_train = xgb.predict(xtrain)\nypred_test = xgb.predict(xtest)\n\nprint('Train Accuracy:',accuracy_score(ytrain,ypred_train))\nprint('Test Accuracy:',accuracy_score(ytest,ypred_test))\nprint()\n\nprint('Train Precision:',precision_score(ytrain,ypred_train))\nprint('Test Precision:',precision_score(ytest,ypred_test))\n\nprint()\n\nprint('Train Recall:',recall_score(ytrain,ypred_train))\nprint('Test Recall:',recall_score(ytest,ypred_test))\n\nprint()\n\nprint('Train f1 score:',f1_score(ytrain,ypred_train))\nprint('Test f1 score:',f1_score(ytest,ypred_test))\n\nprint()\n\nytrain_prob = xgb.predict_proba(xtrain)[:,1]\nytest_prob = xgb.predict_proba(xtest)[:,1]\n\nprint('Train ROC AUC Score',roc_auc_score(ytrain,ytrain_prob))\nprint('Test ROC AUC Score',roc_auc_score(ytest,ytest_prob))\n\n\n# This is a good model since all the metrics improved significantly relative to the base model\n\n# accuracy, precision,recall,f1 score and ROC AUC, all have improved significantly","fa2cdf8a":"Final['Final Model without SMOTE'] = [accuracy_score(ytrain,ypred_train),accuracy_score(ytest,ypred_test),precision_score(ytrain,ypred_train),\\\n                      precision_score(ytest,ypred_test),recall_score(ytrain,ypred_train),recall_score(ytest,ypred_test),\\\n                      f1_score(ytrain,ypred_train),f1_score(ytest,ypred_test),roc_auc_score(ytrain,ytrain_prob),\\\n                       roc_auc_score(ytest,ytest_prob)]","2d243e53":"Final","cb47ff26":"# finally plotting confusion matrix for the test results\n\nsns.heatmap(pd.DataFrame(confusion_matrix(ytest,ypred_test)),annot=True,fmt='d',cbar=False)\nplt.title('Confusion matrix')\nplt.show()\n\n#167 sessions which were predicted as revenue positive but were actually revenue negative in the base model dropped to only 135\n#209 sessions which were actually turning into revenue but were not able to get detected in the base model decreased slightly to 203\n\n# there is not a very significant improvement from the base model in terms of reducing the false negatives","bc34ab76":"#final model with SMOTE\n\nfrom imblearn.over_sampling import SMOTE\nsm = SMOTE()\nxsmote,ysmote = sm.fit_resample(xtrain,ytrain)\n\nxgb = XGBClassifier(max_depth=3,learning_rate=0.1,random_state=2)\nxgb.fit(xsmote,ysmote)\nypred_train = xgb.predict(xsmote)\nypred_test = xgb.predict(xtest)\n\nprint('Train Accuracy:',accuracy_score(ysmote,ypred_train))\nprint('Test Accuracy:',accuracy_score(ytest,ypred_test))\nprint()\n\nprint('Train Precision:',precision_score(ysmote,ypred_train))\nprint('Test Precision:',precision_score(ytest,ypred_test))\n\nprint()\n\nprint('Train Recall:',recall_score(ysmote,ypred_train))\nprint('Test Recall:',recall_score(ytest,ypred_test))\n\nprint()\n\nprint('Train f1 score:',f1_score(ysmote,ypred_train))\nprint('Test f1 score:',f1_score(ytest,ypred_test))\n\nprint()\n\nytrain_prob = xgb.predict_proba(xsmote)[:,1]\nytest_prob = xgb.predict_proba(xtest)[:,1]\n\nprint('Train ROC AUC Score',roc_auc_score(ysmote,ytrain_prob))\nprint('Test ROC AUC Score',roc_auc_score(ytest,ytest_prob))\n\n","3b5051cc":"Final['Final Model with SMOTE'] = [accuracy_score(ysmote,ypred_train),accuracy_score(ytest,ypred_test),precision_score(ysmote,ypred_train),\\\n                      precision_score(ytest,ypred_test),recall_score(ysmote,ypred_train),recall_score(ytest,ypred_test),\\\n                      f1_score(ysmote,ypred_train),f1_score(ytest,ypred_test),roc_auc_score(ysmote,ytrain_prob),\\\n                       roc_auc_score(ytest,ytest_prob)]","5c437779":"sns.heatmap(pd.DataFrame(confusion_matrix(ytest,ypred_test)),annot=True,fmt='d',cbar=False)\nplt.title('Confusion matrix')\nplt.show()\n\n#167 sessions which were predicted as revenue positive but were actually revenue negative in the base model increased to 389\n#209 sessions which were actually turning into revenue but were not able to get detected in the base model decrea drastically decreased to 96\n\n# there is a very significant improvement from the base model in terms of reducing the false negatives","8f8e9098":"Final","cbb58c8c":"# business interpretation\/explanation of the model\nxgb = XGBClassifier(max_depth=3,learning_rate=0.1,random_state=2)\nxgb.fit(xtrain,ytrain)\nrank = pd.DataFrame()\nrank['Features'] = xtrain.columns\nrank['Rank'] = xgb.feature_importances_\nrank.sort_values(by='Rank',ascending=False).head(10)","009aa575":"plt.figure(figsize=(21,5))\nsns.barplot(x='Features',y='Rank',data=rank.sort_values(by='Rank',ascending=False).head(15))\nplt.title('Top 15 most important features')\nplt.xticks(rotation=90)\nplt.show()","c484d797":"print('Features adding least value \/ areas not to be focussed on :')\nprint()\nprint(rank.sort_values(by='Rank',ascending=False).tail(28)['Features'].values)","9d050085":"##### A little on Bounce Rates and Exit Rates\n1.\tFor all pageviews to the page, Exit Rate is the percentage that were the last in the session.\n2.\tFor all sessions that start with the page, Bounce Rate is the percentage that were the only one of the session.\n3.\tBounce Rate for a page is based only on sessions that start with that page","a39ffb64":"<a id=\"a\"> <\/a>\n### Problem Statement\nThe client is facing the problem of low rate of sales conversion and seeks to increase revenue from its online shopping platform on the basis of customer\u2019s online activities.\n\n### Objective\nTo predict whether or not a customer is going to generate revenue for the organization on the basis of customer\u2019s online activities using various machine learning algorithms. The goal is to derive insights for determining what drives the consumers to purchase the product in order to increase customer loyalty and retention.","a1ad43a1":"<a id=\"o\"> <\/a>\n## Feature Importance","4f44cea7":"- Majority of the revenue comes from the operating system 2 and browser 2. This suggests that Operating system 2 is the leading operating system (which could possibly be Android) while majoirty of the users on this android would probably use Google Chrome as their browser. (Which suggests that browser 2 Could be Google Chrome)\n\n- Further, it can be noted that Operating system 1 appears to be the second largest contributor to revenue (probably IOS) and is mostly used with browser 1 (probably Safari)","0e767ae3":"##### Inference - Region appears to be a highly insignificant factor in determining revenue. The same has been confirmed by using Boxplot visualizations above as well as Chi-square test. The same shall be removed for the purpose of model building.","426dcd6e":"**XGBoost is the winner and shall be used in all further analysis**","09e3b9b9":"- 84.5% of the transactions did not turn into sales\n- 15.5 % of transactions turned into sales\n- Highly Imblanaced data set. \n- Therefore, we would need to use techniques to handle such data while applying ML algorithms\n","5d5ad4b3":"<a id=\"j\"> <\/a>\n## Applying various parametric,non-parametric & ensemble models with hyperparameter tuning","7f31bef2":"<a id=\"i\"> <\/a>\n## With transformation","f445664c":"**From the above doughnut plot, it can be inferred that November has the most significant contribution to client\u2019s revenue followed by May, December and March. While February, June and July have the least contribution. Client needs to analyze what drives the customers during the peak months and implement similar strategies in the other months.**","83241b52":"<a id=\"g\"> <\/a>\n## Scaling & Encoding","b4649142":"- Precion score means out of total positive predictions (Revenue Positive Sessions), how many are correctly predicted.\n- Recall score mean out of actually revenue positive sessions, how many are correctly predicted\n- f1 score represents harmonic mean of precision and recall and takes into account both the values\n\n- Precision is TP\/(TP+FP)\n- Recall is TP\/(TP+FN)\n\nBusiness objective discussion- \n\nIf business wants to minimize false negatives, here (i.e the objective is to not miss identifying sessions which will yield revenue), then recall becomes important.\n\nIf the business prioritizes minimizing false positives (here it means we do not want to predict no revenue sessions as revenue sessions, then precision becomes important. However, for the current purpose I assume both recall and precision score are important metric and we will take that into account. We will also use other metrics such as AUC-ROC score for model evaluation.\n\n**Although recall could be more important in certain situations, I will use a more robust metric ie. AUC ROC score while using 5 fold cross validation**","af058390":"Here, Red points represent customers who did not bring revenue and Black points represent revenue positive customers. From the above visualization, following points are noted:\n\n- Customers who ended up shopping (Reds) have a relatively lower bounce rate and exit rate than the black points.\n- Customers who ended up shopping have a page value on the higher side relative to the negative classes.","98652a23":"- XGBoost works wonders on this data\n- If the aim of the model is to minimize the the undetected revenue sessions (False Negatives) and at the cost of incorrectly predicting non-revenue sessions as revenue sessions, then SMOTE model should be chosen. \n- If the aim of the company is to just focus on the recall value, then it must go ahead with a SMOTE analysis. Further, there is a high level of overfitting in the SMOTE model.\n\n---\n- The majority of the sessions on the website included visit to product related pages. Further, it has the highest contribution to revenue generation out of all the pages. Therefore, product related pages are of utmost importance to the client as suggested by the number of customer visits. Further, time spent by the customer on product related pages is appearing to be an important feature for determining revenue in our ML algortihm. Busineeses should therefore focus on increasing traffic on these product related pages.\n---\n-  Majority of the sessions in the data pertain to the month of may followed by November. Months such as November,May, December and March have the most significant contribution to client\u2019s revenue. The same is visible in the doughnut plot in the visualization section as well as while analysing the feature importance in the XGBoost model.  Client needs to analyze what drives the customers during the peak months and implement similar strategies in the other months. It is therefore advisable to take maximum benefit out of these months by using aggressive marketing startegies.\n\n- February has the lowest proportion of sessions which yielded revenue\n---\n\nCustomers who ended up shopping had a relatively lower bounce rate and exit rates while Average Page value of a page that user visited before making a transactions is visibly higher relative to the negative classes. Therefore, the focus should be on reducing the exit rates and bounce rates on your webpage as well as improving the factors that determine the page value. \n\n---\nHaving a high bounce rate can mean three things:\n1. The quality of the page is low. There\u2019s nothing inviting to engage with.\n2. Your audience doesn\u2019t match the purpose of the page, as they won\u2019t engage with your page.\n3. Visitors have found the information that they were looking for.\n\nIf the bounce rate amongst new visitors is high, think about how you could improve their engagement with your site. \n\n---\nProportion of revenue positive sessions is the highest among new visitors followed by other visitors. \nReturning visitor sessions have a relatively lower proportion of revenue positive sessions. So if a visitor is a returning visitor, there are more chances of not having a sale. Returning visitor feature also appears among the top 10 most important feature list above.\n\n--- \n\nMedian number of pages visited and time spent on them for administrative purposes on the website is higher for sessions which yielded revenue. Also, number of administrative pages visited is also an important feature in our Final ML model. Therefore, along with product related pages, it is also important to further improve your websites administrative pages.\n\n---\n\n- Majority of the revenue comes from the operating system 2 and browser 2. This suggests that Operating system 2 is the leading operating system (which could possibly be Android) while majoirty of the users on this android would probably use Google Chrome as their browser. (Which suggests that browser 2 Could be Google Chrome)\n\n- Further, it can be noted that Operating system 1 appears to be the second largest contributor to revenue (probably IOS) and is mostly used with browser 1 (probably Safari). Operating system 2 and traffic type 4 appear to be the most significant in our ML model\n\n- Proportion of sessions which yielded revenue is the highest among users having operating system 8 while operating system 3 yields the lowest proportion of these revenue positive sessions. Browser 12 has the highest proportion of revenue positive sessions followed by browser 13 while browser 3 has the least contribution. Although these are not appearing as the most important features in the model, they were appearing to be statistically significant while applying chi-square tests.\n\n---\nRegion does not appear to play a significant role in determining whether the customer will purchase or not.","aa0c0edd":"## Analysis on only Revenue Positive Cases (15.5% Data)","6994fac9":"**All the evaluation metrics except precision are improving by using power transformation. Further even ROC-AUC scores improve in the case of power transformation, I will use this power transformed data for any futher analysis.**","42db7540":"<a id=\"n\"> <\/a>\n## Final Model Test Results with SMOTE","43a5317a":"<a id=\"p\"> <\/a>\n## Conclusion","595af8ef":"<a id=\"d\"> <\/a>\n## Data Visualization & Insights","e13151b2":"![image.png](attachment:image.png)","ab3aaa3a":"<a id=\"f\"> <\/a>\n## Outlier Analysis","4f437c9b":"<a id=\"k\"> <\/a>\n## Algorithm Selection Boxplot","3bf654e3":"A low rate of sales conversion is a matter of concern for any business and needs to be dealt with. The data provides various parameters about the customer\u2019s activities on the website and the same shall be used to identify which factors are more likely to trigger the customer\u2019s decision of purchase.","5e58e0bd":"Conclusion - **XGBoost without feature selection is working the best!**","289e6ccd":"# Table of Content\n\n1. **[Problem Statement & Business Objective](#a)**\n2. **[Data Description](#b)**\n3. **[Reading & Correcting data types](#c)**\n4. **[Data Visualization & Insights](#d)**\n5. **[Statistical Analysis](#e)**\n6. **[Outlier Analysis](#f)**\n7. **[Scaling & Encoding](#g)**\n8. **[Base Model](#h)**\n9. **[Power Transformation](#i)**\n10. **[Applying various parametric,non-parametric & ensemble models with hyperparameter tuning](#j)**\n11. **[Algorithm Selection Boxplot Analysis](#k)**\n12. **[Feature Selection](#l)**\n13. **[Final model without SMOTE](#m)**\n14. **[Final model with SMOTE](#n)**\n15. **[Feature Importance](#o)**\n16. **[Conclusion](#p)**","4eef50d7":"Inference-\n- The majority of the time spent on the website was on Product Related pages. Therefore, product related pages are of utmost importance to the client from the perspective of revenue generation.","f4d2839f":"### Count Plot","59b96f22":"**Abstract: Of the 12,330 sessions in the dataset, 84.5% (10,422) were negative class samples that did not end with shopping, and the rest (1908) were positive class samples ending with shopping.**\n\n** Note - The dataset was formed so that each session\nwould belong to a different user in a 1-year period to avoid\nany tendency to a specific campaign, special day, user\nprofile, or period. **","f6caf2fc":"# Online Shoppers Purchasing Intention Dataset (UCI ML Repository)","42c938c4":"- Although we saw above that new visitors had the highest proportion of sessions turning into revenue, however when it comes to analysing the overall absolute contributor to revenue, it would be the returning visitor type since they are dominant visitors in the session and it is obvious that their proportion would be high in both yes and no transactions.\n\n- Similar parallel category plots can be drawn for other features as well. However, I will limit my data visualization here. Let's get our insights approved by statistical analysis.","e3e4a50f":"After several minutes of hyper-parameter tuning - **'learning_rate': 0.1, 'max_depth': 3**","eefaa5a9":"<a id=\"h\"> <\/a>\n## Base Model - Without Transformation","23db92dd":"Point of action \/ steps to be taken -\n\n1. Apply other parametric and non parametric models\n2. Hyper-Parameter tuning for these models\n3. Applying ensemble techniques such as Adaboost classifier, XGBoost, Gradient boosting\n4. Applying customized grid search for n_estimators wherever necessary\n5. Apply feature selection strategies\n6. Will also use SMOTE analysis to see if the performance on test improves \n\nI will use 5-Fold cross validation strategy to check the bias and variance error in the model and my scoring parameter will be 'ROC-AUC' since it's a more robust metric. The model which performs best will be finally evaluated on the test data.","326cc27f":"After several minutes of hyper-parameter tuning - **criterion='entropy',max_depth=9**","03d5a877":"**Final Conclusion - If the aim of the model is to minimize the the undetected revenue sessions (False Negatives) and at the cost of incorrectly predicting non-revenue sessions as revenue sessions, then SMOTE model should be chosen. \nIf the aim of the company is to just focus on the recall value, then it must go ahead with a SMOTE analysis. Further, there is a high level of overfitting in the SMOTE model.**\n\n-----\n\n**For the current analysis, I will go ahead with a more generalized model i.e without SMOTE**","ca8e049d":"After several minutes of hyper-parameter tuning - \n**n_neighbors=92,weights='distance'**","21423132":"<a id=\"c\"> <\/a>\n## Reading & Correcting the data types","de108eb7":"<a id=\"e\"> <\/a>\n## Statistical Analysis","2c6a9b60":"Inference-\n- The majority of the sessions on the website included visit to product related pages. Further, it has the highest contribution to revenue generation. Therefore, product related pages are of utmost importance to the client as suggested by the number of customer visits.","5cc541e2":"After several minutes of hyper-parameter tuning - **max_depth=4,criterion='entropy'**","7c25a3fb":"<a id=\"m\"> <\/a>\n## Final Model Test Results without using SMOTE","78c0c6ed":"<a id=\"l\"> <\/a>\n## Feature Selection","6f59a3c7":"<a id=\"b\"> <\/a>\n## Dataset Description","4053ad82":"## Applying Base Model - with \/ without transformation\n#### Whether data with or without transformation is to be used in further analysis will be decided here"}}