{"cell_type":{"652af64b":"code","a614387b":"code","c34d7566":"code","edc8f949":"code","aa693fe7":"code","ec84f740":"code","88290d3a":"code","7f044059":"code","e69f1b01":"code","f380e299":"code","59ea32e7":"code","20b9e60f":"code","e7a3a23e":"code","46e535b0":"code","61d99270":"code","70d13c0e":"code","1bf8093f":"code","5a35ff3f":"code","4bcccfa4":"code","c52b6769":"code","a57da782":"code","8ec7d7b4":"code","6e65878b":"code","cdebd578":"code","fad081a0":"code","17771fb3":"code","7d25ad91":"code","85fe3c33":"code","480d27f7":"code","707d6459":"code","e00fadb5":"code","971ddc35":"code","275ff15e":"code","36de138f":"code","22cbc999":"code","f6da6946":"code","dd8a49ca":"code","58247a4f":"code","199a3e7e":"code","12432c51":"code","32e20dd1":"code","b4f224a2":"code","fd23c45f":"code","27539c1a":"code","f7301952":"code","95132311":"code","65a47303":"code","90bf6110":"code","ee066b78":"code","6b3fff1c":"code","58be5975":"code","54a45562":"code","c02b5467":"code","1ca2d287":"code","333d9dcd":"code","7f8c0d37":"code","f45ea031":"code","81550abd":"code","ddfa3513":"code","8ab5ca90":"code","2cbb7ae6":"code","270b5599":"code","1265c03a":"markdown","02dc991c":"markdown","aaefca6a":"markdown","5c4802e3":"markdown","eeedb550":"markdown","33285281":"markdown","6cf7b0cd":"markdown","a19d093f":"markdown","59a3162e":"markdown","4f1eee7d":"markdown","8f1ce311":"markdown","f72054d7":"markdown","d3446f18":"markdown","4722661e":"markdown","0b4b27bc":"markdown","3cebe244":"markdown","53cef8c9":"markdown","510f88e4":"markdown","30046451":"markdown","54d39b1f":"markdown","ab43768c":"markdown","8c7ba261":"markdown","e933a89e":"markdown","fd34d3e0":"markdown","82bc9221":"markdown","827d628c":"markdown","b76ea713":"markdown","e12f3330":"markdown","f773c99f":"markdown","90791d95":"markdown","46a61d45":"markdown","7e3b25bf":"markdown","89c129a4":"markdown","5dcdd348":"markdown","6caf72ae":"markdown","d7c45054":"markdown","0c462748":"markdown","4842056f":"markdown","94f9d964":"markdown","37504d3f":"markdown","f24c2f79":"markdown","bbb4cbcf":"markdown","4fcacbd6":"markdown"},"source":{"652af64b":"from warnings import simplefilter\nsimplefilter(action='ignore', category=FutureWarning)\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas_profiling\nfrom mlens.visualization import corrmat\n\nfrom sklearn.ensemble import AdaBoostClassifier, BaggingClassifier, ExtraTreesClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier, VotingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, classification_report\nfrom sklearn.model_selection import cross_val_score, train_test_split, GridSearchCV, RandomizedSearchCV\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.preprocessing import StandardScaler\n\nimport re\nfrom collections import Counter\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","a614387b":"# I didn't like the outcome of sklearn.preprocessing.LabelEncoder so I made my own label encoder\ndef label_encode(df, column_name):\n    ordered_column = np.sort(df[column_name].unique())\n    df[column_name] = df[column_name].map(\n        dict(zip(np.sort(df[column_name].unique()),[x for x in range(len(df[column_name].unique()))]))\n    )\n    return df\n\n# This functions helps us get insights on the relation between variables\ndef compare(df,column_name, with_table=False, with_graph=True, compare_to='Survived'):\n    if with_table:\n        print(df[df[compare_to] < 3].groupby([compare_to,column_name]).size().sort_index())\n    if with_graph:\n        g = sns.FacetGrid(df, col=compare_to).map(sns.distplot, column_name)\n\n# This function display the correlation of all variables to the target label\ndef show_correlation(df, column_name='Survived'):\n    return df.corr()[column_name].apply(abs).sort_values(na_position='first').reset_index()\n\n# This function helps us find the outliers\ndef get_IQR(df, column_name):\n    Q3 = df[column_name].quantile(0.75)\n    Q1 = df[column_name].quantile(0.25)\n    IQR = Q3 - Q1\n    return Q1, Q3, IQR\n\n# This function detects the outliers.\ndef detect_outliers(df, n, features):\n        outlier_indices = []\n        for col in features:\n            Q1, Q3, IQR = get_IQR(df, col)\n            outlier_step = 1.5 * IQR\n            outlier_list_col = df[(df[col] < Q1 - outlier_step) | (df[col] > Q3 + outlier_step )].index\n            outlier_indices.extend(outlier_list_col)\n\n        outlier_indices = Counter(outlier_indices)        \n        multiple_outliers = list( k for k, v in outlier_indices.items() if v > n )\n\n        return multiple_outliers","c34d7566":"train_df = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntest_df = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\n\ntest_df['Survived'] = 3\n\ntrain_df = train_df[train_df.columns]\ntest_df = test_df[train_df.columns]\n\nall_set = pd.concat([train_df, test_df], ignore_index=True)","edc8f949":"all_set.profile_report()","aa693fe7":"def get_title(name):\n    return name.split(',')[1].split('.')[0].strip()\n    \nall_set['Title'] = all_set['Name'].apply(get_title)","ec84f740":"compare(all_set, 'Title', True, False)","88290d3a":"all_set['Surname'] = all_set['Name'].apply(lambda x: x.split(',')[0].strip())","7f044059":"all_set['FamilySurvival'] = 0.5\n\nfor surname in all_set['Surname'].unique():\n    df = all_set[all_set['Surname'] == surname]\n    if df.shape[0] > 1:\n        smin = df['Survived'].min()\n        smax = df['Survived'].max()\n        for idx, row in df.iterrows():\n            passengerid = row['PassengerId']\n            if smax == 1.0:\n                all_set.loc[all_set['PassengerId'] == passengerid, 'FamilySurvival'] = 1.0\n            elif smin == 0.0:\n                all_set.loc[all_set['PassengerId'] == passengerid, 'FamilySurvival'] = 0.0","e69f1b01":"all_set['Embarked'].value_counts()","f380e299":"all_set['Embarked'] = all_set['Embarked'].fillna('S') ","59ea32e7":"compare(all_set, 'Embarked', True, False)","20b9e60f":"all_set[all_set['Fare'].isna()]","e7a3a23e":"all_set['Fare'].fillna(all_set[all_set['Embarked'] == 'S']['Fare'].mean(), inplace=True)","46e535b0":"compare(all_set, 'Fare', False, True)","61d99270":"all_set['Deck'] = all_set['Cabin'].apply(lambda x: x[0] if type(x) == str else '')","70d13c0e":"all_set[all_set['Deck'] == 'G']","1bf8093f":"compare(all_set, 'Deck', True, False)","5a35ff3f":"deck_fare = {}\nfor deck in all_set['Deck'].unique():\n    if len(deck) == 1 and deck in 'ABCDEF': # The passengers were distributed between decks A and F\n        deck_fare[deck] = all_set[\n            (all_set['Cabin'].apply(lambda x: True if type(x) == str else False)) &\n            (all_set['Deck'] == deck)\n        ]['Fare'].mean()\ndeck_fare","4bcccfa4":"def find_deck(fare):\n    dist = 1000\n    res = 'F'\n    for key in deck_fare.keys():\n        new_dist = np.abs(fare - deck_fare[key])\n        if new_dist < dist:\n            dist = new_dist\n            res = key\n    return res\n\nall_set.loc[all_set['Cabin'].isna(), 'Deck'] = all_set['Fare'].apply(find_deck)","c52b6769":"compare(all_set, 'Deck', True, False)","a57da782":"all_set['Family'] = 1 + all_set['SibSp'] + all_set['Parch']\nall_set['Alone'] = all_set['Family'].apply(lambda x: 1 if x == 1 else 0)","8ec7d7b4":"compare(all_set, 'Family')","6e65878b":"age_by_title = {}\nfor title in all_set['Title'].unique():\n    age_by_title[title] = all_set[\n        (all_set['Age'].apply(lambda x: True if type(x) == float else False)) &\n        (all_set['Title'] == title)\n    ]['Age'].mean()\nage_by_title","cdebd578":"compare(all_set[all_set['Age'].isna()], 'Family', False, True, 'Title')","fad081a0":"all_set.loc[all_set['Age'].isna(), 'Age'] = all_set['Title'].apply(lambda x: age_by_title[x])","17771fb3":"compare(all_set, 'Age')","7d25ad91":"show_correlation(all_set)","85fe3c33":"all_set.dtypes","480d27f7":"all_set['AgeBin'] = pd.qcut(all_set['Age'], 5)\nall_set['AgeCode'] = all_set['AgeBin']\nall_set = label_encode(all_set, 'AgeCode')","707d6459":"all_set['AgeDist'] = 1\/ np.exp(np.abs (all_set['Age'] - all_set[all_set['Survived'] == 1]['Age'].median()))","e00fadb5":"compare(all_set, 'AgeDist')","971ddc35":"all_set = label_encode(all_set, 'Sex')\ncompare(all_set, 'Sex')","275ff15e":"all_set = label_encode(all_set, 'Embarked')\ncompare(all_set, 'Embarked')","36de138f":"all_set = label_encode(all_set, 'Deck')\ncompare(all_set, 'Deck')","22cbc999":"all_set['Title'].value_counts()","f6da6946":"dict(zip(all_set['Title'].unique(),['Rare' for x in range(len(all_set['Title'].unique()))]))","dd8a49ca":"all_set['Title'] = all_set['Title'].map({\n    'Mr': 'Mr', 'Mrs': 'Mrs', 'Miss': 'Miss', 'Master': 'Master',\n    'Don': 'Rare', 'Rev': 'Rare', 'Dr': 'Rare', 'Mme': 'Rare',\n    'Ms': 'Rare', 'Major': 'Rare', 'Lady': 'Rare', 'Sir': 'Rare',\n    'Mlle': 'Rare', 'Col': 'Rare', 'Capt': 'Rare', 'the Countess': 'Rare',\n    'Jonkheer': 'Rare', 'Dona': 'Rare'})","58247a4f":"all_set = label_encode(all_set, 'Title')\ncompare(all_set, 'Title')","199a3e7e":"all_set['FarePerFamilyMember'] = all_set['Fare'] \/ all_set['Family']\nall_set['FareBin'] = pd.qcut(all_set['Fare'], 5)\nall_set['FareCode'] = all_set['FareBin']\nall_set = label_encode(all_set, 'FareCode')\nall_set['Fare'] = all_set['Fare'].apply(lambda x: np.log(x) if x > 0 else np.log(3.0))","12432c51":"fig, ax = plt.subplots(figsize=(14,14))\ng = sns.heatmap(all_set[\n    all_set.dtypes.reset_index()[ all_set.dtypes.reset_index()[0] != 'object']['index'].unique()\n].corr(), annot=True, cmap='coolwarm')","32e20dd1":"all_set.profile_report()","b4f224a2":"# 0.76555\n# all_tmp = all_set.drop(columns=['Age', 'Cabin', 'Fare', 'Name', 'Surname', 'Ticket', 'PassengerId', 'Embarked', 'SibSp', 'Parch'])\n# all_tmp = all_set[corr[corr['Survived'] > 0.02]['index'].unique()]\n\n# The following features will be dropped based on previous tests.\nall_tmp = all_set.drop(columns=['PassengerId', 'Name', 'Ticket', 'Cabin', 'AgeBin', 'AgeCode', 'AgeDist', 'FareBin', 'FareCode'])","fd23c45f":"all_set.columns","27539c1a":"corrmat(all_tmp.corr(), inflate=False)\nplt.show()","f7301952":"all_dum = pd.get_dummies(all_tmp, drop_first=True)\nprint(all_dum.shape)\n\ntrain_set = all_dum[all_dum['Survived'] < 3].copy()\ntest_set = all_dum[all_dum['Survived'] == 3].copy()","95132311":"X = train_set.copy()\ntry:\n    X.drop(columns=['Survived'], inplace=True)\nexcept:\n    pass\ny = train_set['Survived'].copy()\n\nscaler = StandardScaler().fit(X)\nscl_X = scaler.transform(X)\n\ntX = test_set.copy()\ntry:\n    tX.drop(columns=['Survived'], inplace=True)\nexcept:\n    pass\n\nscl_tX = scaler.transform(tX)","65a47303":"train_X, val_X, train_y, val_y = train_test_split(scl_X, y, test_size=0.33, random_state=42, stratify=y)","90bf6110":"rfc = RandomForestClassifier(n_estimators=100)\nrfc.fit(train_X, train_y)\nscore = rfc.score(val_X, val_y)\nprint('RandomForestClassifier =', score)\npred_y = rfc.predict(val_X)\ntarget_names = ['DEAD', 'SURVIVED']\nprint(classification_report(val_y, pred_y,target_names=target_names))","ee066b78":"best_model = 0\nbest_score = score","6b3fff1c":"clfs = []\nclfs.append(('ada', AdaBoostClassifier))\nclfs.append(('bag', BaggingClassifier))\nclfs.append(('rnd', RandomForestClassifier))\nclfs.append(('knn', KNeighborsClassifier))\nclfs.append(('mlp', MLPClassifier))\nclfs.append(('ext', ExtraTreesClassifier))\nclfs.append(('log', LogisticRegression))\nclfs.append(('gbm', GradientBoostingClassifier))\n    \nparams = []\nparams.append({'n_estimators': np.arange(10,500,10), 'learning_rate':[float(x\/100.) for x in np.arange(1,10)]})\nparams.append({'n_estimators': np.arange(10,500,10)})\nparams.append({'n_estimators': np.arange(10,500,10)})\nparams.append({'n_neighbors': np.arange(3,15)})\nparams.append({'hidden_layer_sizes': [(100,),(200,),(300,),(400,),(500,)]})\nparams.append({'n_estimators': np.arange(10,200,10)})\nparams.append({'max_iter': np.arange(10,500,10)})\nparams.append({'n_estimators': np.arange(10,500,10), 'learning_rate':[float(x\/100.) for x in np.arange(1,10)], 'max_depth':np.arange(3,10)})","58be5975":"best_estimators = []\nbest_params = []\nestimators_weights = []\nk = len(params)\nfor idx in range(len(clfs)):\n    gs = RandomizedSearchCV(clfs[idx][1](), params[idx], cv=5)\n    gs.fit(train_X, train_y)\n    \n    estimators_weights.append(gs.score(val_X, val_y))\n    best_estimators.append(gs.best_estimator_)\n    best_params.append(gs.best_params_)\n    \n    print(k, clfs[idx][0], gs.best_params_)\n    k -= 1","54a45562":"estimator_list = list(zip([name for (name, model) in clfs], best_estimators))","c02b5467":"vclf = VotingClassifier(estimator_list, voting='hard', weights=estimators_weights, n_jobs=-1)\nvclf.fit(train_X, train_y)\nscore = vclf.score(val_X, val_y)\n\nprint('VotingClassifier =', score)\n\nif score > best_score:\n    best_model = 1\n    best_score = score","1ca2d287":"pred_y = vclf.predict(val_X)\ntarget_names = ['DEAD', 'SURVIVED']\nprint(classification_report(val_y, pred_y,target_names=target_names))","333d9dcd":"fit_X = np.zeros((train_y.shape[0], len(best_estimators)))\nfit_X = pd.DataFrame(fit_X)\n\npred_X = np.zeros((val_y.shape[0], len(best_estimators)))\npred_X = pd.DataFrame(pred_X)\n\ntest_X = np.zeros((scl_tX.shape[0], len(best_estimators)))\ntest_X = pd.DataFrame(test_X)\n\nprint(\"Fitting models.\")\ncols = list()\nfor i, (name, m) in enumerate(estimator_list):\n    print(\"%s...\" % name, end=\" \", flush=False)\n    \n    fit_X.iloc[:, i] = m.predict_proba(train_X)[:, 1]\n    pred_X.iloc[:, i] = m.predict_proba(val_X)[:, 1]\n    test_X.iloc[:, i] = m.predict_proba(scl_tX)[:, 1]\n    \n    cols.append(name)\n    print(\"done\")\n\nfit_X.columns = cols\npred_X.columns = cols\ntest_X.columns = cols","7f8c0d37":"corrmat(pred_X.corr(), inflate=False)\nplt.show()","f45ea031":"corrmat(test_X.corr(), inflate=False)\nplt.show()","81550abd":"meta_estimator = GradientBoostingClassifier()\nmeta_params = {'n_estimators': np.arange(10,500,10), 'learning_rate':[float(x\/100.) for x in np.arange(1,10)], 'max_depth':np.arange(3,10)}\n\nmeta_estimator = RandomizedSearchCV(GradientBoostingClassifier(), meta_params, cv=5)\nmeta_estimator.fit(fit_X, train_y)\n\nscore = meta_estimator.score(pred_X, val_y)\n\nprint('MetaEstimator =', score)\n\nif score > best_score:\n    best_model = 2\n    best_score = score","ddfa3513":"pred_y = meta_estimator.predict(pred_X)\ntarget_names = ['DEAD', 'SURVIVED']\nprint(classification_report(val_y, pred_y,target_names=target_names))","8ab5ca90":"models = ['RandomForestClassifier', 'VotingClassifier', 'MetaEstimator']\nprint('Best model = ', models[best_model])","2cbb7ae6":"if best_model == 0:\n    predictions = rfc.predict(scl_tX)\nelif best_model == 1:\n    predictions = vclf.predict(scl_tX)\nelse:\n    predictions = meta_estimator.predict(test_X)\n\nPassengerId = test_df['PassengerId'].values\n\nresults = pd.DataFrame({ 'PassengerId': PassengerId, 'Survived': predictions })\nresults.to_csv('results.csv', index=False)","270b5599":"!head -n 10 results.csv","1265c03a":"Let's see how the features behave.","02dc991c":"### - Embarked","aaefca6a":"### - Name","5c4802e3":"**Fare** has a lot of unique values. Creating a categorical feature derived from it may help.","eeedb550":"I like to use a **VotingClassifier** to predict the results. I've tried writing my own ensemble models but this one provided better results. It is also easier to maintain.","33285281":"There was a family in the boiler room as we can see below.","6cf7b0cd":"Some people have paid the fare of other family members. If we divide the **Fare** paid for the **Family** size, perhaps we'll have a good feature.","a19d093f":"I've tried some classifiers, **GradientBoostingClassifier** performed best. Before training it let's find the best hyperparameters again.","59a3162e":"Now it is time to create all the possible categorical features and see how they affect our predictions.","4f1eee7d":"We can extract a new feature, **Title**, from the feature **Name**.","8f1ce311":"Let's start fitting our data to a **RandomForestClassifier** to have an idea of what to expect.","f72054d7":"Since there is only one value missing and no information on **Fare** or **Cabin**, I will use the fare mean of those embarked in Southampton, which represents most of the passengers.","d3446f18":"# Split train test datasets","4722661e":"Before the real training begins, let's find the best hyperparameters using **RandomizedSearchCV** which is faster than **GridSearchCV** and the results are similar.","0b4b27bc":"# Data preprocessing","3cebe244":"Now, on to training.","53cef8c9":"### - Cabin","510f88e4":"Since we already have a list of estimators, why now create a meta estimator?\n\nThe meta estimator will predict the results based on the results provided by the previous estimators.","30046451":"# Data loading","54d39b1f":"And use these means to determine in which deck a passenger was allocated by finding the mean value closest to the fare paid by the passenger.","ab43768c":"# Predict results","8c7ba261":"Now the numbers seem to reflect reality a little better. Most passengers where on deck F as expected.","e933a89e":"### - Age","fd34d3e0":"Now that we have the predictions of three classification models, let's choose the best one and be done with it.","82bc9221":"Most people embarked on Southampton so...","827d628c":"### - Fare","b76ea713":"# Utility functions","e12f3330":"Instead of worring about the **Cabin**, let's think about **Decks**. The passengers were distributed in decks A to F.","f773c99f":"The model may favor features with bigger numbers over the rest. So let's scale the features.","90791d95":"Let's remove some **Titles** before encoding. Let's rename the titles with less than 10 occurrences to **Rare**.","46a61d45":"Now we want to calculate the missing ages. Let's do this by title means.","7e3b25bf":"The remaining categorical features will become columns one hot style.","89c129a4":"If we sum the number of siblings and the number of parents and 1 (self) we have the size of the family onboard. If this size is equal to 1(one), the individual was alone.","5dcdd348":"**AgeDist** is higher if the individual **Age** is closer to the median Age of survivors.","6caf72ae":"Let's calculate the fare mean amongst decks.","d7c45054":"As we can see bellow, most of the passengers have no deck information.","0c462748":"We can also extract another new feature, **Surname**, from the feature **Name**.","4842056f":"The following page helps us have some insights about the provided data: https:\/\/www.encyclopedia-titanica.org","94f9d964":"# Feature scaling","37504d3f":"### - SibSp & Parch","f24c2f79":"# Encode features","bbb4cbcf":"Let's find out if individuals of the same family had the same rate of survival by creating another new feature, **FamilySurvival**, from the feature **Surname**.","4fcacbd6":"# Model training"}}