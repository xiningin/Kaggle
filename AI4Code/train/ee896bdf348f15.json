{"cell_type":{"74c85b68":"code","ebd8d20f":"code","e28dfa78":"code","0e27a6e1":"code","3e5e736e":"code","709f99e1":"code","0c7e6395":"code","e9c5ad1d":"code","83f83656":"code","591e22c5":"code","99e7e2cc":"code","54cf64b7":"code","45e69357":"code","dc0c678d":"code","1987e912":"code","07bf5164":"code","a0f593d5":"code","2e7ef4d5":"code","a8bc31f7":"code","627d62b4":"code","dbdd096d":"code","bb718289":"code","efe2405d":"code","3abe3abc":"code","7a9d9577":"code","fdcd3375":"code","e3a3ba0a":"code","d59f19a4":"code","9e71479b":"code","f54c0e7e":"code","46fbe261":"code","b845acf7":"code","332f8fdc":"code","55e812d7":"code","a07d35a5":"code","0e76bfb4":"code","5d229518":"code","b41de5cf":"code","b529fe37":"code","255ebc5f":"code","cd9b003f":"code","3f205752":"code","b42ad693":"code","e2092647":"code","68c9d297":"code","960bf73c":"code","0f7ab2f0":"code","47f04b3d":"code","aaf7836b":"code","ada51899":"code","8a39512d":"code","ebef9639":"code","e14d831a":"code","ec44cff7":"code","64284abb":"markdown","76422af2":"markdown","008c5934":"markdown","cb0971c8":"markdown","68175a0e":"markdown","4c7d717a":"markdown","ee14d845":"markdown","07a3e949":"markdown","528497b1":"markdown","25eb5c79":"markdown","bdcca486":"markdown","70816b6d":"markdown","37219099":"markdown","28109d05":"markdown","a940b0ba":"markdown","3084d098":"markdown","de824d4e":"markdown","9698a5bd":"markdown","521cdc99":"markdown","d7d221eb":"markdown","9b1e8562":"markdown","a4fdc4ef":"markdown","e818a57a":"markdown","4564eb7f":"markdown","b382eec5":"markdown","b821ac02":"markdown","06be8864":"markdown","7f4d069e":"markdown","55a03087":"markdown","5d0e77b6":"markdown","9241b2fd":"markdown","5f99df45":"markdown","0019e6da":"markdown","505de1cf":"markdown","c32e4182":"markdown","0920ee5f":"markdown"},"source":{"74c85b68":"# System\nimport os\n\n# Time\nimport time\nimport datetime\n\n# Numerical\nimport numpy as np\nimport pandas as pd\nimport random\n\n# Tools\nimport itertools\nfrom collections import Counter\n\n# NLP\nimport re\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize \nfrom nltk.stem import SnowballStemmer\nfrom nltk.stem import PorterStemmer\nfrom nltk.stem import WordNetLemmatizer\n# from pywsd.utils import lemmatize_sentence\n\n# Preprocessing\nfrom sklearn import preprocessing\nfrom sklearn.utils import class_weight as cw\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom bs4 import BeautifulSoup\n\n# Model Selection\nfrom sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n\n# Machine Learning Models\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import svm\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier, BaggingClassifier, ExtraTreesClassifier\nfrom sklearn.tree import DecisionTreeClassifier\n\n# Evaluation Metrics\nfrom sklearn import metrics \nfrom sklearn.metrics import f1_score, accuracy_score,confusion_matrix,classification_report\n\n# Deep Learing Preprocessing - Keras\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing import sequence\nfrom keras.utils import to_categorical\n\n# Deep Learning Model - Keras\nfrom keras.models import Model\nfrom keras.models import Sequential\n\n# Deep Learning Model - Keras - CNN\nfrom keras.layers import Conv1D, Conv2D, Convolution1D, MaxPooling1D, SeparableConv1D, SpatialDropout1D, \\\n    GlobalAvgPool1D, GlobalMaxPool1D, GlobalMaxPooling1D \nfrom keras.layers.pooling import _GlobalPooling1D\nfrom keras.layers import MaxPooling2D, GlobalMaxPooling2D, GlobalAveragePooling2D\n\nfrom keras.layers import MaxPooling3D, GlobalMaxPooling3D, GlobalAveragePooling3D\n\n\n\n# Deep Learning Model - Keras - RNN\nfrom keras.layers import Embedding, LSTM, Bidirectional\n\n# Deep Learning Model - Keras - General\nfrom keras.layers import Input, Add, concatenate, Dense, Activation, BatchNormalization, Dropout, Flatten\nfrom keras.layers import LeakyReLU, PReLU, Lambda, Multiply\n\n\n\n# Deep Learning Parameters - Keras\nfrom keras.optimizers import RMSprop, Adam\n\n# Deep Learning Callbacs - Keras\nfrom keras.callbacks import ModelCheckpoint, EarlyStopping, TensorBoard, ReduceLROnPlateau\n\n# Visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n%matplotlib inline\n\nprint(os.listdir(\"..\/input\"))","ebd8d20f":"# print date and time for given type of representation\ndef date_time(x):\n    if x==1:\n        return 'Timestamp: {:%Y-%m-%d %H:%M:%S}'.format(datetime.datetime.now())\n    if x==2:    \n        return 'Timestamp: {:%Y-%b-%d %H:%M:%S}'.format(datetime.datetime.now())\n    if x==3:  \n        return 'Date now: %s' % datetime.datetime.now()\n    if x==4:  \n        return 'Date today: %s' % datetime.date.today() ","e28dfa78":"input_directory = r\"..\/input\/nlp-getting-started\/\"\noutput_directory = r\"..\/output\/\"\n\nif not os.path.exists(output_directory):\n    os.mkdir(output_directory)\n    \nfigure_directory = \"..\/output\/figures\"\nif not os.path.exists(figure_directory):\n    os.mkdir(figure_directory)\n    \n    \nfile_name_pred_batch = figure_directory + r\"\/result\"\nfile_name_pred_sample = figure_directory + r\"\/sample\"","0e27a6e1":"# Load data\n\ndf_train = pd.read_csv(\"..\/input\/nlp-getting-started\/train.csv\")\ndf_test = pd.read_csv(\"..\/input\/nlp-getting-started\/test.csv\")\nsub_sample = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/sample_submission.csv\")\n\nprint (df_train.shape, df_test.shape, sub_sample.shape)","3e5e736e":"df_train.head()","709f99e1":"df_train.info()","0c7e6395":"print (df_train.keyword.nunique(), df_train.location.nunique())\nprint (df_test.keyword.nunique(), df_test.location.nunique())","e9c5ad1d":"# Check if train and test have the same set of keywords\nset(df_train.keyword.unique()) - set(df_test.keyword.unique())","83f83656":"c = 10\nfor i in range(c):\n    r = random.randint(0, len(df_train)-1)\n    r=i\n    print(r,\" : \", df_train.iloc[r].text, \"\\n\")","591e22c5":"columns = df_train.columns\ncolumns","99e7e2cc":"figsize=(20, 5)\n\nticksize = 18\ntitlesize = ticksize + 8\nlabelsize = ticksize + 5\n\nparams = {'figure.figsize' : figsize,\n          'axes.labelsize' : labelsize,\n          'axes.titlesize' : titlesize,\n          'xtick.labelsize': ticksize,\n          'ytick.labelsize': ticksize}\n\nplt.rcParams.update(params)\n\ncol = \"target\"\nxlabel = \"Real vs Not\"\nylabel = \"Count\"\n\nsns.countplot(x=df_train[col])\nplt.title(\"Real vs Not Count\")\n# plt.xticks(rotation=90)\nplt.xlabel(xlabel)\nplt.ylabel(ylabel)","54cf64b7":"figsize=(18, 5)\n\nticksize = 14\ntitlesize = ticksize + 8\nlabelsize = ticksize + 5\n\nxlabel = \"Train Feature\"\nylabel = \"Null Count\"\n\ntitle = \"Train Feature Null Count\"\n\n\nparams = {'figure.figsize' : figsize,\n          'axes.labelsize' : labelsize,\n          'axes.titlesize' : titlesize,\n          'xtick.labelsize': ticksize,\n          'ytick.labelsize': ticksize}\n\nplt.rcParams.update(params)\n\ndf_train.isnull().sum().plot(kind=\"bar\")\nplt.title(title)\nplt.xlabel(xlabel)\nplt.ylabel(ylabel)\nplt.xticks(rotation=90)\nplt.plot()","45e69357":"figsize=(18, 5)\n\nticksize = 14\ntitlesize = ticksize + 8\nlabelsize = ticksize + 5\n\nxlabel = \"Test Feature\"\nylabel = \"Null Count\"\n\ntitle = \"Test Feature Null Count\"\n\n\nparams = {'figure.figsize' : figsize,\n          'axes.labelsize' : labelsize,\n          'axes.titlesize' : titlesize,\n          'xtick.labelsize': ticksize,\n          'ytick.labelsize': ticksize}\n\nplt.rcParams.update(params)\n\ndf_train.isnull().sum().plot(kind=\"bar\")\nplt.title(title)\nplt.xlabel(xlabel)\nplt.ylabel(ylabel)\nplt.xticks(rotation=90)\nplt.plot()\n\n\ndf_test.isnull().sum().plot(kind=\"bar\")\nplt.title(title)\nplt.xlabel(xlabel)\nplt.ylabel(ylabel)\nplt.xticks(rotation=90)\nplt.plot()","dc0c678d":"figsize=(22, 5)\n\nticksize = 14\ntitlesize = ticksize + 8\nlabelsize = ticksize + 5\n\n# xlabel = \"Train Feature\"\n# ylabel = \"Null Count\"\n\n# title = \"Train Feature Null Count\"\n\n\nparams = {'figure.figsize' : figsize,\n          'axes.labelsize' : labelsize,\n          'axes.titlesize' : titlesize,\n          'xtick.labelsize': ticksize,\n          'ytick.labelsize': ticksize}\n\nplt.rcParams.update(params)\n\nkw = df_train.keyword.value_counts().head(10)\nkw_d = df_train[df_train.target==1].keyword.value_counts().head(10)\nkw_nd = df_train[df_train.target==0].keyword.value_counts().head(10)\n\n# plt.figure(figsize=(13,5))\nsns.barplot(kw, kw.index)\nplt.title('Top keywords for tweets')\nplt.show()\nplt.subplot(121)\nsns.barplot(kw_d, kw_d.index)\nplt.title('Top keywords for disaster tweets')\nplt.subplot(122)\nsns.barplot(kw_nd, kw_nd.index)\nplt.title('Top keywords for non-disaster tweets')\nplt.show()","1987e912":"figsize=(22, 5)\n\nticksize = 14\ntitlesize = ticksize + 8\nlabelsize = ticksize + 5\n\n# xlabel = \"Train Feature\"\n# ylabel = \"Null Count\"\n\n# title = \"Train Feature Null Count\"\n\n\nparams = {'figure.figsize' : figsize,\n          'axes.labelsize' : labelsize,\n          'axes.titlesize' : titlesize,\n          'xtick.labelsize': ticksize,\n          'ytick.labelsize': ticksize}\n\nplt.rcParams.update(params)\n\nkw = df_train.location.value_counts().head(10)\nkw_d = df_train[df_train.target==1].location.value_counts().head(10)\nkw_nd = df_train[df_train.target==0].location.value_counts().head(10)\n\n# plt.figure(figsize=(13,5))\nsns.barplot(kw, kw.index)\nplt.title('Top locations for tweets')\nplt.show()\nplt.subplot(121)\nsns.barplot(kw_d, kw_d.index)\nplt.title('Top locations for disaster tweets')\nplt.subplot(122)\nsns.barplot(kw_nd, kw_nd.index)\nplt.title('Top locations for non-disaster tweets')\nplt.show()","07bf5164":"# Tweet length\ndf_train['text_len'] = df_train['text'].apply(len)\ndf_test['text_len'] = df_test['text'].apply(len)\n# Word count\ndf_train[\"word_count\"] = df_train[\"text\"].apply(lambda x: len(str(x).split()))\ndf_test[\"word_count\"] = df_test[\"text\"].apply(lambda x: len(str(x).split()))\n# Unique word count\ndf_train[\"unique_word_count\"] = df_train[\"text\"].apply(lambda x: len(set(str(x).split())))\ndf_test[\"unique_word_count\"] = df_test[\"text\"].apply(lambda x: len(set(str(x).split())))\n# Count of hashtags (#)\ndf_train['hash_count'] = df_train['text'].apply(lambda x: str(x).count(\"#\"))\ndf_test['hash_count'] = df_test['text'].apply(lambda x: str(x).count(\"#\"))\n# Count of mentions (@)\ndf_train['mention_count'] = df_train['text'].apply(lambda x: str(x).count(\"@\"))\ndf_test['mention_count'] = df_test['text'].apply(lambda x: str(x).count(\"@\"))\n\ndf_train.head()","a0f593d5":"df_train = df_train[['id', 'keyword', 'location', 'text', 'text_len', 'word_count',\n                     'unique_word_count', 'hash_count', 'mention_count', 'target']]\ndf_train.head()","2e7ef4d5":"df_test.head()","a8bc31f7":"# Tweet length\ndf_train['text_modified'] = df_train['text']\ndf_train['text_modified'] = df_train['text_modified'].apply(lambda x: re.sub(r\"(%20)\", r\" \", x, flags=re.MULTILINE))\ndf_train['text_modified'] = df_train['text_modified'].apply(lambda x: re.sub(r'^https?:\\\/\\\/.*[\\r\\n]*', 'http:\/\/', x, flags=re.MULTILINE))\ndf_train['text_modified'] = df_train['text_modified'].apply(lambda x: re.sub(r\"([^a-zA-Z1-9#])\", r\" \\1 \", x, flags=re.MULTILINE))\ndf_train['text_modified'] = df_train['text_modified'].apply(lambda x: re.sub(r\"\\s+\", \" \", x, flags=re.MULTILINE))\n\n\ndf_train = df_train[['id', 'keyword', 'location', 'text', 'text_modified', 'text_len', 'word_count',\n                     'unique_word_count', 'hash_count', 'mention_count', 'target']]\ndf_train.head()","627d62b4":"for i in range(10):\n    r = random.randrange(len(df_train)-1)\n    print(df_train.target[r], \" : \", df_train.text_modified\t[r])","dbdd096d":"# from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(df_train.loc[:,'id':'mention_count'], df_train['target'], test_size=0.2, random_state=0)","bb718289":"from sklearn.naive_bayes import GaussianNB\n\nvectorizer = TfidfVectorizer(\n#     input='content', \n#     encoding='utf-8', \n#     decode_error='strict', \n#     strip_accents=None, \n#     lowercase=False,\n#     preprocessor=None,\n#     tokenizer=None, \n#     analyzer='word',\n    stop_words=None, \n    ngram_range=(1, 2), \n    max_df=0.6, \n    min_df=4, \n    max_features=None, \n#     vocabulary=None, \n#     binary=False, \n    norm='l2', \n    use_idf=True, \n    smooth_idf=True,\n    sublinear_tf=True\n)\n\nX_train_tf = vectorizer.fit_transform(X_train.text.tolist()).toarray()\ny_train = y_train\n\n# print(train_X.shape)\n# print(len(Y_train))\n\n\n############################\n\ngnb = GaussianNB()\n\nX_test_tf = vectorizer.transform(X_test.text.tolist()).toarray()\n\ny_pred = gnb.fit(X_train_tf, y_train).predict(X_test_tf)\n\nprint(\"Number of mislabeled points out of a total %d points : %d\" % (X_test.shape[0], (y_test != y_pred).sum()))\nprint(\"Percentage of Correct points : %.2f\" % (100-((y_test != y_pred).sum())\/(len(y_test))*100))","efe2405d":"from sklearn.naive_bayes import GaussianNB\n\nvectorizer = TfidfVectorizer(\n#     input='content', \n#     encoding='utf-8', \n#     decode_error='strict', \n#     strip_accents=None, \n    lowercase=False,\n#     preprocessor=None,\n#     tokenizer=None, \n#     analyzer='word',\n    stop_words=None, \n    ngram_range=(1, 2), \n    max_df=.6, \n    min_df=4, \n    max_features=None, \n#     vocabulary=None, \n#     binary=False, \n    norm='l2', \n    use_idf=True, \n    smooth_idf=True,\n    sublinear_tf=True\n)\n\nX_train_tf = vectorizer.fit_transform(X_train.text_modified.tolist()).toarray()\ny_train = y_train\n\n# print(train_X.shape)\n# print(len(Y_train))\n\n\n############################\n\ngnb = GaussianNB()\n\nX_test_tf = vectorizer.transform(X_test.text_modified.tolist()).toarray()\n\ny_pred = gnb.fit(X_train_tf, y_train).predict(X_test_tf)\n\nprint(\"Number of mislabeled points out of a total %d points : %d\" % (X_test.shape[0], (y_test != y_pred).sum()))\nprint(\"Percentage of Correct points : %.2f\" % (100-((y_test != y_pred).sum())\/(len(y_test))*100))","3abe3abc":"# Most stupid model: Just to test submission\nfrom sklearn.linear_model import LogisticRegression\n\n\nX_train_at = X_train.loc[:,'text_len':'mention_count']\nX_test_at = X_test.loc[:,'text_len':'mention_count']\n\n\nmodel = LogisticRegression()\nmodel.fit(X_train_at, y_train)\ny_pred = model.predict(X_test_at)\n\nfrom sklearn.metrics import f1_score\nprint('macro: ', f1_score(y_test, y_pred, average='macro'))\nprint('micro: ', f1_score(y_test, y_pred, average='micro'))\nprint('weighted: ', f1_score(y_test, y_pred, average='weighted'))\nprint('None: ', f1_score(y_test, y_pred, average=None))\n# f1_score(y_test, y_pred, zero_division=1)","7a9d9577":"main_model_dir = output_directory + r\"models\/\"\nmain_log_dir = output_directory + r\"logs\/\"\n\ntry:\n    os.mkdir(main_model_dir)\nexcept:\n    print(\"Could not create main model directory\")\n    \ntry:\n    os.mkdir(main_log_dir)\nexcept:\n    print(\"Could not create main log directory\")\n\n\n\nmodel_dir = main_model_dir + time.strftime('%Y-%m-%d %H-%M-%S') + \"\/\"\nlog_dir = main_log_dir + time.strftime('%Y-%m-%d %H-%M-%S')\n\n\ntry:\n    os.mkdir(model_dir)\nexcept:\n    print(\"Could not create model directory\")\n    \ntry:\n    os.mkdir(log_dir)\nexcept:\n    print(\"Could not create log directory\")\n    \nmodel_file = model_dir + \"{epoch:02d}-val_acc-{val_acc:.2f}-val_loss-{val_loss:.2f}.hdf5\"","fdcd3375":"print(\"Settting Callbacks\")\n\ncheckpoint = ModelCheckpoint(\n    model_file, \n    monitor='val_accuracy', \n    save_best_only=True)\n\nearly_stopping = EarlyStopping(\n    monitor='val_loss',\n    patience=5,\n    verbose=1,\n    restore_best_weights=True)\n\n\nreduce_lr = ReduceLROnPlateau(\n    monitor='val_loss',\n    factor=0.5,\n    patience=2,\n    verbose=1)\n\n\n# callbacks = [checkpoint, reduce_lr, early_stopping]\n\ncallbacks = [reduce_lr, early_stopping]\n\nprint(\"Set Callbacks at \", date_time(1))","e3a3ba0a":"# Calculate Class Weights\ndef get_weight(y):\n    class_weight_current =  cw.compute_class_weight('balanced', np.unique(y), y)\n    return class_weight_current","d59f19a4":"X = df_train.text\nY = df_train.target\n\nlabel_encoder = LabelEncoder()\n\nY = label_encoder.fit_transform(Y)\nprint(Y)\nY = to_categorical(Y)\nprint(Y)\n# Y = Y.reshape(-1, 1)\nY","9e71479b":"X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.15)\n\nmax_words = len(set(\" \".join(X_train).split()))\nmax_len = X_train.apply(lambda x: len(x)).max()\n\n# max_words = 1000\n# max_len = 150\nmax_words, max_len","f54c0e7e":"tokenizer = Tokenizer(num_words=max_words)\n\ntokenizer.fit_on_texts(X_train)\n\nX_train_seq = tokenizer.texts_to_sequences(X_train)\nX_train_seq = sequence.pad_sequences(X_train_seq, maxlen=max_len)","46fbe261":"class_weight = get_weight(np.argmax(Y_train ,axis=1))\nclass_weight","b845acf7":"# def get_rnn_model(num_class=2):\n#     model = Sequential()\n    \n#     model.add(Embedding(max_words, 100, input_length=max_len))\n#     model.add(LSTM(256))\n    \n#     model.add(Dropout(0.5))\n#     model.add(BatchNormalization())\n#     model.add(Dropout(0.5))\n    \n#     model.add(Dense(512, activation='relu'))\n    \n#     model.add(Dropout(0.5))\n#     model.add(BatchNormalization())\n#     model.add(Dropout(0.5))\n    \n#     if num_class>=2:\n#         model.add(Dense(num_class, activation='softmax'))\n#     else:\n#         model.add(Dense(1, activation='sigmoid'))\n    \n#     model.summary()\n    \n#     return model\n\ndef get_rnn_model(num_class=2):\n    model = Sequential()\n    \n    model.add(Embedding(max_words, 100, input_length=max_len))\n    \n    model.add(Dropout(0.2))\n    model.add(Bidirectional(LSTM(256)))\n    model.add(Dropout(0.2))\n    \n    model.add(Dense(512, activation='relu'))\n    \n    model.add(Dropout(0.5))\n    model.add(BatchNormalization())\n    \n    if num_class>=2:\n        model.add(Dense(num_class, activation='softmax'))\n    else:\n        model.add(Dense(1, activation='sigmoid'))\n    \n    model.summary()\n    \n    return model\n\n\ndef get_cnn_model(num_class=2):   \n    model = Sequential()\n    \n    model.add(Embedding(max_words, 100, input_length=max_len))\n    \n    model.add(Conv1D(1024, 3, padding='valid', activation='relu', strides=1))\n    model.add(GlobalMaxPooling1D())\n    \n    \n    model.add(Dropout(0.5))\n    model.add(BatchNormalization())\n    model.add(Dropout(0.5))\n    \n    model.add(Dense(2048, activation='relu'))\n    \n    model.add(Dropout(0.5))\n    model.add(BatchNormalization())\n    model.add(Dropout(0.5))\n    \n    if num_class>=2:\n        model.add(Dense(num_class, activation='softmax'))\n    else:\n        model.add(Dense(1, activation='sigmoid'))\n    \n    model.summary()\n    return model","332f8fdc":"def plot_performance(history=None, figure_directory=None):\n    xlabel = 'Epoch'\n    legends = ['Training', 'Validation']\n\n#     ylim_pad = [0.1, 0.005]\n    ylim_pad = [0, 0.5]\n\n\n    plt.figure(figsize=(20, 5))\n\n    # Plot training & validation Accuracy values\n\n    y1 = history.history['accuracy']\n    y2 = history.history['val_accuracy']\n\n    min_y = min(min(y1), min(y2))-ylim_pad[0]\n    max_y = max(max(y1), max(y2))+ylim_pad[0]\n    \n    min_y = 0\n    max_y = 1\n\n\n    plt.subplot(121)\n\n    plt.plot(y1)\n    plt.plot(y2)\n\n    plt.title('Model Accuracy\\n'+date_time(1), fontsize=17)\n    plt.xlabel(xlabel, fontsize=15)\n    plt.ylabel('Accuracy', fontsize=15)\n    plt.ylim(min_y, max_y)\n    plt.legend(legends, loc='upper left')\n    plt.grid()\n\n\n    # Plot training & validation loss values\n\n    y1 = history.history['loss']\n    y2 = history.history['val_loss']\n\n    min_y = min(min(y1), min(y2))-ylim_pad[1]\n    max_y = max(max(y1), max(y2))+ylim_pad[1]\n\n#     min_y = 0\n#     max_y = .8\n\n    plt.subplot(122)\n\n    plt.plot(y1)\n    plt.plot(y2)\n\n    plt.title('Model Loss\\n'+date_time(1), fontsize=17)\n    plt.xlabel(xlabel, fontsize=15)\n    plt.ylabel('Loss', fontsize=15)\n    plt.ylim(min_y, max_y)\n    plt.legend(legends, loc='upper left')\n    plt.grid()\n    \n    if figure_directory:\n        plt.savefig(figure_directory+\"\/history\")\n\n    plt.show()","55e812d7":"num_class = 2\nmodel1 = get_rnn_model(num_class=num_class)","a07d35a5":"loss = 'categorical_crossentropy'\n# loss = 'binary_crossentropy'\nmetrics = ['accuracy']","0e76bfb4":"print(\"Starting...\\n\")\n\nstart_time = time.time()\nprint(date_time(1))\n\nprint(\"\\n\\nCompliling Model ...\\n\")\nlearning_rate = 0.0001\noptimizer = Adam(learning_rate)\n# optimizer = Adam()\n\nmodel1.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n\nverbose = 1\nepochs = 100\nbatch_size = 16\nvalidation_split = 0.2\n\nprint(\"Trainning Model ...\\n\")\n\nhistory1 = model1.fit(\n    X_train_seq,\n    Y_train,\n    batch_size=batch_size,\n    epochs=epochs,\n    verbose=verbose,\n    callbacks=callbacks,\n    validation_split=validation_split,\n    class_weight=class_weight\n    )\n\nelapsed_time = time.time() - start_time\nelapsed_time = time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_time))\n\nprint(\"\\nElapsed Time: \" + elapsed_time)\nprint(\"Completed Model Trainning\", date_time(1))","5d229518":"plot_performance(history=history1)","b41de5cf":"num_class = 2\nmodel2 = get_cnn_model(num_class=num_class)","b529fe37":"print(\"Starting...\\n\")\n\nstart_time = time.time()\nprint(date_time(1))\n\nprint(\"\\n\\nCompliling Model ...\\n\")\nlearning_rate = 0.0001\noptimizer = Adam(learning_rate)\n# optimizer = Adam()\n\nmodel2.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n\nverbose = 1\nepochs = 100\nbatch_size = 16\nvalidation_split = 0.2\n\nprint(\"Trainning Model ...\\n\")\n\nhistory2 = model2.fit(\n    X_train_seq,\n    Y_train,\n    batch_size=batch_size,\n    epochs=epochs,\n    verbose=verbose,\n    callbacks=callbacks,\n    validation_split=validation_split,\n    class_weight =class_weight\n    )\n\nelapsed_time = time.time() - start_time\nelapsed_time = time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_time))\n\nprint(\"\\nElapsed Time: \" + elapsed_time)\nprint(\"Completed Model Trainning\", date_time(1))","255ebc5f":"plot_performance(history=history2)","cd9b003f":"test_X_seq = tokenizer.texts_to_sequences(X_test)\ntest_X_seq = sequence.pad_sequences(test_X_seq, maxlen=max_len)\naccuracy1 = model1.evaluate(test_X_seq, Y_test)\naccuracy2 = model2.evaluate(test_X_seq, Y_test)","3f205752":"print(\"Model Performance of RNN (Test Accuracy):\")\nprint('Accuracy: {:0.2f}%\\nLoss: {:0.3f}\\n'.format(accuracy1[1]*100, accuracy1[0]))\n\nprint(\"\\nModel Performance of RNN (Test Accuracy):\")\nprint('v: {:0.2f}%\\nLoss: {:0.3f}\\n'.format(accuracy2[1]*100, accuracy2[0]))","b42ad693":"ypreds1 = model1.predict_classes(test_X_seq, verbose=1)\nypreds2 = model2.predict_classes(test_X_seq, verbose=1)","e2092647":"def plot_model_performace(result):\n    sns.set_style(\"ticks\")\n    figsize=(22, 6)\n\n    ticksize = 12\n    titlesize = ticksize + 8\n    labelsize = ticksize + 5\n\n    xlabel = \"Model\"\n    ylabel = \"Score\"\n\n    title = \"Model Performance\"\n\n    params = {'figure.figsize' : figsize,\n              'axes.labelsize' : labelsize,\n              'axes.titlesize' : titlesize,\n              'xtick.labelsize': ticksize,\n              'ytick.labelsize': ticksize}\n\n    plt.rcParams.update(params)\n\n    col1 = \"model\"\n    col2 = \"score\"\n    sns.barplot(x=col1, y=col2, data=result)\n    plt.title(title.title())\n    plt.xlabel(xlabel)\n    plt.ylabel(ylabel)\n    plt.xticks(rotation=90)\n    plt.grid()\n    plt.plot()\n    plt.show()\n    print(result)","68c9d297":"# print(classification_report(Y_test, ypreds1))","960bf73c":"# plot_confusion_matrix(Y_test, ypreds1, title=\"RNN\")","0f7ab2f0":"# print(classification_report(Y_test, ypreds2))","47f04b3d":"# plot_confusion_matrix(Y_test, ypreds2, title=\"CNN\")","aaf7836b":"result = pd.DataFrame({'model': 'RNN', 'score': accuracy1[1]*100}, index=[-1])\nrow2 = pd.DataFrame({'model': 'CNN', 'score': accuracy2[1]*100}, index=[-1])\nresult = pd.concat([row2, result.ix[:]]).reset_index(drop=True)","ada51899":"plot_model_performace(result)","8a39512d":"df_train.head()","ebef9639":"from sklearn.naive_bayes import GaussianNB\n\nvectorizer = TfidfVectorizer(\n#     input='content', \n#     encoding='utf-8', \n#     decode_error='strict', \n#     strip_accents=None, \n#     lowercase=False,\n#     preprocessor=None,\n#     tokenizer=None, \n#     analyzer='word',\n    stop_words=None, \n    ngram_range=(1, 2), \n    max_df=0.6, \n    min_df=4, \n    max_features=None, \n#     vocabulary=None, \n#     binary=False, \n    norm='l2', \n    use_idf=True, \n    smooth_idf=True,\n    sublinear_tf=True\n)\n\nX_train_tf = vectorizer.fit_transform(df_train.text.tolist()).toarray()\n\ny_train = df_train.target.tolist()\n\n# print(train_X.shape)\n# print(len(Y_train))\n\n\n############################\n\ngnb = GaussianNB()\n\nX_test_tf = vectorizer.transform(df_test.text.tolist()).toarray()\n\ny_pred = gnb.fit(X_train_tf, y_train).predict(X_test_tf)","e14d831a":"df_test.head()","ec44cff7":"sample_list = list(sub_sample.id)\n\npred_dict = dict((key, value) for (key, value) in zip(df_test.id, y_pred))\n\npred_list_new = [pred_dict[f] for f in sample_list]\n\ntest_df = pd.DataFrame({'id': sample_list, 'target': pred_list_new})\n\ntest_df.to_csv(\"submission.csv\", header=True, index=False)","64284abb":"Top locations for tweets","76422af2":"## 4.1 Gaussian Naive Bayes and TFIDF Vectorizer ","008c5934":"# 3. Explorarory Data Analysis","cb0971c8":"### 4.1.2 Modified Text\nUsing Gassuan Naive Bayes, as machine learning algorithm and TFIDF vectorizer on modified text","68175a0e":"Lets see","4c7d717a":"##### 4.3.5.1.2  Visualization","ee14d845":"# 2. Read Data","07a3e949":"Lets see few tweet texts","528497b1":"# 1. Import","25eb5c79":"### 14.3.6 Inference\/ Prediction","bdcca486":"## 4.3. Deep Learning","70816b6d":"### 4.3.3. Preprocessing","37219099":"## 4. Training Model","28109d05":"# 4. Feature Extraction","a940b0ba":"# Final","3084d098":"#### 4.3.5.2.2 Visualization","de824d4e":"## 4.2 Logistic Regression of derived numerical features","9698a5bd":"### 4.3.2 Callback Settings","521cdc99":"### 4.3.1 Output Configuration","d7d221eb":"# Reference:\n1. [Text Preprocessing and Machine Learning Modeling](https:\/\/www.kaggle.com\/futurist\/text-preprocessing-and-machine-learning-modeling)\n2. [keras mlp cnn test for text classification](https:\/\/www.kaggle.com\/jacklinggu\/keras-mlp-cnn-test-for-text-classification)","9b1e8562":"Number of null values in feature columns","a4fdc4ef":"### 4.1.1 Original Text\nUsing Gassuan Naive Bayes, as machine learning algorithm and TFIDF vectorizer on original text","e818a57a":"# Real or Not? NLP with Disaster Tweets","4564eb7f":"Number of tweets for disaster and non disaster in dataset","b382eec5":"Lets see first few columns of the dataset","b821ac02":"### 4.3.7 Evaluation","06be8864":"### 4.3.5 Model Trainning","7f4d069e":"Top keywords for tweets","55a03087":"#### 10.5.1.2 Visualization","5d0e77b6":"### 4.3.4. Model","9241b2fd":"There are many null values in location and keyword","5f99df45":"#### 4.3.5.2. CNN","0019e6da":"#### 4.3.5.1 RNN","505de1cf":"## 3.2 Visualize Data","c32e4182":"![](https:\/\/img3.goodfon.com\/wallpaper\/nbig\/f\/29\/katastrofa-razrusheniya-zdaniya.jpg)","0920ee5f":"Following are the unique values of location and keyword"}}