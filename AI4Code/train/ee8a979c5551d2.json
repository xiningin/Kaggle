{"cell_type":{"3424b95a":"code","27fb1940":"code","84271dc0":"code","1c48e2ff":"code","7cae6833":"code","8849a9dd":"code","55a5d971":"code","a07ac950":"code","7852bfe6":"code","b6f22635":"code","59eb8dbe":"code","88f75d66":"code","f1874d07":"code","a2b495e0":"code","482bd907":"code","554d1968":"code","b9421c88":"code","0c91aa5b":"code","c17b7b24":"code","33ee5747":"code","3c323ba0":"code","cb021234":"code","310c30dd":"code","67150526":"code","8459764c":"code","28d2acf3":"code","6a36eca1":"code","2ece9c4b":"code","1d94f703":"code","2fc55ce2":"code","ae176809":"code","6fe26407":"code","54a4d353":"code","d5e75fd1":"code","030febef":"code","243d35e6":"code","2f705f3c":"code","b7b6827f":"code","3b5eddb6":"code","9aa6cc1a":"code","010e51f2":"code","ea352b62":"code","e47146a3":"code","5ced7d94":"code","275ada09":"code","e4745094":"code","fe4475bc":"code","45b61216":"code","d058ee1a":"code","792558aa":"code","ace7016b":"code","28d9c5bf":"code","aef2c481":"code","861e38b5":"code","a5550f0f":"code","8dfda13b":"code","5e3482f3":"code","276f7731":"code","ca2c33aa":"code","c2c4f734":"code","79fc7700":"code","4045c316":"code","5cf88e7e":"code","9d6a4cf8":"code","217db813":"code","cffdf060":"code","7561e049":"code","10f24ef8":"code","c736ba68":"code","77f54f95":"code","58f1e371":"code","64326f70":"code","887e3fff":"code","4e1ccb27":"code","0109b2dc":"code","a7f8e60e":"code","5b99cf4d":"code","25a77966":"code","f4f1a3c6":"code","15df6aea":"code","9047cbd2":"code","973b6bc7":"code","e9192b0d":"code","eee76deb":"code","cc62eb2c":"code","23b7c61b":"code","b8dd58c3":"code","be4cae39":"code","d059a2ca":"code","c1da6759":"code","8e5955c5":"code","5caa9f90":"code","57230a01":"code","a2fdd2e4":"code","1bd81564":"code","70395f81":"markdown","8773322e":"markdown","1128b557":"markdown","85ba41dd":"markdown","331c71b2":"markdown","3b077831":"markdown","a9698b21":"markdown","f83aec5e":"markdown","097b91b2":"markdown","89948512":"markdown","8e3d1637":"markdown","b5e16784":"markdown","a24199d7":"markdown","946fcacc":"markdown","d5077839":"markdown","6932f6cc":"markdown","29332b22":"markdown","efb9b9f1":"markdown","f75559f4":"markdown","2f5c41a0":"markdown","851d507e":"markdown","7d2c0e40":"markdown","8f461aa5":"markdown","009009de":"markdown","845bdc32":"markdown","3f4e7ef6":"markdown","dce7e08d":"markdown","6dc2d362":"markdown","ab08b7af":"markdown","ceddd0d7":"markdown"},"source":{"3424b95a":"#importing warnings filter\nimport warnings\nwarnings.filterwarnings('ignore')\n","27fb1940":"#Importing all the neccessary libraries\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LinearRegression\n\nimport statsmodels.api as sm\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\nfrom sklearn.metrics import r2_score","84271dc0":"pd.set_option('display.max_columns' , 100)\n","1c48e2ff":"#Reading the data\n\nbike = pd.read_csv(\"..\/input\/boom-bikes-sharing-dataset\/day.csv\")\nbike.head()","7cae6833":"#Checking the shape\nbike.shape","8849a9dd":"# Checking mathematiacal distribution of the variables\nbike.describe()","55a5d971":"#Checking null values and datatypes\nbike.info()","a07ac950":"#Dropping Instant as it is not needed\n\nbike.drop(\"instant\", axis=1 , inplace=True)","7852bfe6":"#Dropping Casual and Registrerd as they both contribute to our Target variable, \"Count\". And there is no point in keeping 3 variables when the needful can be done by using just one.\n\nbike.drop(\"casual\", axis=1 , inplace=True)\nbike.drop(\"registered\", axis=1 , inplace=True)\nbike.head()","b6f22635":"#Dropping Date column as it is not required anymore. We have month and year column\nbike.drop(\"dteday\", axis=1 , inplace=True)","59eb8dbe":"bike.describe()","88f75d66":"bike.shape","f1874d07":"bike.info()","a2b495e0":"bike.head()","482bd907":"# Dropping Holiday as well. Because, 0 in working day already reflects a holiday\n\nbike.drop(\"holiday\" , inplace=True , axis=1)\nbike.head()","554d1968":"sns.pairplot(bike[[\"cnt\" , \"temp\" , \"atemp\" , \"hum\", \"windspeed\"]])\nplt.show()","b9421c88":"plt.figure(figsize = (18,12))\nsns.heatmap(bike.corr() , annot=True , cmap = 'YlGnBu')\nplt.show()","0c91aa5b":"# Checking the correlation between temp and atemp using scatter plot\n\nplt.figure(figsize= [16,12])\nsns.scatterplot(bike.temp , bike.atemp , alpha=0.7)\nplt.title(\"Temp v\/s Atemp\" , fontdict={'fontsize' : 20 , 'color' : \"Green\" })\nplt.show()","c17b7b24":"# It can be seen that both the variables are highly correlated. Hence one should be dropped. Else Multicollinearity may occur\n\nbike.drop(\"atemp\" , inplace=True , axis=1)\nbike.head()","33ee5747":"# Renaming the columns for better readibility and understanding\n\n\nbike.rename(columns={\"mnth\" : \"month\" , \"yr\" : \"year\" , \"weekday\" : \"day_of_the_week\" , \"temp\" : \"temperature\" , \"hum\" : \"humidity\" , \"cnt\" :\"count\" , \"weathersit\" : \"weather_situation\" ,\"workingday\" : \"work_day\" , \"windspeed\" : \"wind_speed\" } , inplace=True)\nbike.head()","3c323ba0":"plt.figure(figsize = (20,16))\n\n\nplt.subplot(2, 3, 1 , autoscale_on= True)\nsns.boxplot(x='work_day' , y = \"count\" , data = bike )\n\nplt.subplot(2, 3, 2 , autoscale_on= True)\nsns.boxplot(x='day_of_the_week' , y = \"count\" , data = bike )\n\nplt.subplot(2, 3, 3 , autoscale_on= True)\nsns.boxplot(x='month' , y = \"count\" , data = bike )\n\nplt.subplot(2, 3, 4 , autoscale_on= True)\nsns.boxplot(x='season' , y = \"count\" , data = bike )\n\nplt.subplot(2, 3, 5 , autoscale_on= True)\nsns.boxplot(x='year' , y = \"count\" , data = bike )\n\nplt.subplot(2, 3, 6 , autoscale_on= True)\nsns.boxplot(x='weather_situation' , y = \"count\" , data = bike )\n\nplt.show()","cb021234":"bike.head()","310c30dd":"bike.head()","67150526":"# Mapping Year Column\n\nbike[\"year\"] = np.where(bike[\"year\"] == 0 , 2018 , 2019)\n\nbike.head()","8459764c":"# Mapping Season Column\n\n\nseason_map = {1:'spring', 2:'summer', 3:'fall', 4:'winter'}\n\nbike['season'] = bike['season'].map(season_map)\n\nbike.head()","28d2acf3":"# Mapping Month Column\n\n\nmonth_map = {1:'Jan', 2:'Feb', 3:'Mar', 4:'Apr' , 5:'May', 6:'Jun', 7:'Jul', 8:'Aug', 9:'Sep', 10:'Oct', 11:'Nov', 12:'Dec'}\n\nbike.month = bike.month.map(month_map)\n\nbike.head()","6a36eca1":"# Mapping day_of_the_week Column\n\n\nweek_map = {1:'Monday', 2:'Tuesday', 3:'Wednesday', 4:'Thursday' , 5:'Friday', 6:'Saturday', 7:'Sunday'}\n\nbike.day_of_the_week = bike.day_of_the_week.map(week_map)\n\nbike.head()","2ece9c4b":"# Mapping weather_situation Column\n\n\nweather_map = {1:'Clear', 2:'Mist', 3:'Light_rain_snow', 4:'Heavy_rain_snow'}\n\nbike.weather_situation = bike.weather_situation.map(weather_map)\n\nbike.head()","1d94f703":"# Creating Dummy for season column\n\nseason_dummy = pd.get_dummies(bike['season'], drop_first = True)\n\nseason_dummy.head()","2fc55ce2":"# Concatenation Dummy variable df to original df\n\nbike = pd.concat([bike , season_dummy] , axis=1)\n\nbike.head()","ae176809":"# Creating Dummy for month , day_of_the_week  & weather_situation column\n\n\nmonth_dummy = pd.get_dummies(bike['month'], drop_first = True)\n\nweek_dummy = pd.get_dummies(bike['day_of_the_week'], drop_first = True)\n\nweather_dummy = pd.get_dummies(bike['weather_situation'], drop_first = True)\n","6fe26407":"#Concatenating to original df\n\nbike = pd.concat([bike , month_dummy] , axis=1)\n\nbike = pd.concat([bike , week_dummy] , axis=1)\n\nbike = pd.concat([bike , weather_dummy] , axis=1)\n\nbike.head()","54a4d353":"# Dropping original columns after dummy creation\n\nbike.drop([\"season\" , \"month\" , \"day_of_the_week\" , \"weather_situation\"] , inplace=True , axis=1)\n\nbike.head()","d5e75fd1":"#checking the shape and info about the new dataframe after dummies addition\n\nbike.shape","030febef":"bike.info()","243d35e6":"df_train , df_test = train_test_split(bike , train_size = 0.7 , random_state=100)\n\nprint(df_train.shape)\nprint(df_test.shape)","2f705f3c":"df_train.head()","b7b6827f":"df_test.head()","3b5eddb6":"# Instantiating an object\n\nscaler = MinMaxScaler()\n\n#create a lits of only numerical variables\n\nnum_vars = ['temperature' , 'humidity' , 'wind_speed' , 'count']\n\n# Fit the scaler on data\n\ndf_train[num_vars] = scaler.fit_transform(df_train[num_vars])\ndf_train.head()","9aa6cc1a":"df_train.describe()","010e51f2":"#Checking the correlation between all varaibles\n\nplt.figure(figsize = (24,18))\nsns.heatmap(df_train.corr() , annot=True , cmap = 'YlGnBu')\nplt.show()","ea352b62":"# X_train , y_train\n\n\ny_train = df_train.pop(\"count\")\nX_train = df_train\n\nX_train.head()","e47146a3":"X_train.shape","5ced7d94":"y_train","275ada09":"# Creating the model\nlm = LinearRegression()\nlm.fit(X_train , y_train)\n\nrfe = RFE(lm , 15)\nrfe = rfe.fit(X_train , y_train)","e4745094":"list(zip(X_train.columns , rfe.support_ , rfe.ranking_))","fe4475bc":"# looking at the top 15 variables that RFE chose\n\ncol = X_train.columns[rfe.support_]\n\ncol","45b61216":"# Variables that were not chosen by RFE\n\nX_train.columns[~rfe.support_]","d058ee1a":"# Creating a dataFrame with only variables selected by RFE\n\nX_train_rfe = X_train[col]\nX_train_rfe.head()","792558aa":"# Adding Constant\n\nX_train_rfe = sm.add_constant(X_train_rfe)\nX_train_rfe.head()","ace7016b":"X_train_rfe.info()","28d9c5bf":"# Running the Model and checking the Satistical significance\n\nlm = sm.OLS(y_train , X_train_rfe.astype(float)).fit()","aef2c481":"print(lm.summary())","861e38b5":"vif = pd.DataFrame()\n\nX = X_train_rfe\n\nvif['Features'] = X.columns\n\nvif[\"VIF\"] = [variance_inflation_factor(X.values , i) for i in range(X.shape[1])]\n\nvif.VIF = round(vif[\"VIF\"] , 2)\n\nvif = vif.sort_values(by = \"VIF\" , ascending = False)\n\nvif","a5550f0f":"# Dropping Feb Column as it is insignificant in presence of other selected variables\n\nX_train_rfe_new = X_train_rfe.drop([\"Feb\"] , axis=1)","8dfda13b":"X_train_rfe = sm.add_constant(X_train_rfe_new)","5e3482f3":"lm = sm.OLS(y_train , X_train_rfe).fit()","276f7731":"print(lm.summary())","ca2c33aa":"vif = pd.DataFrame()\n\nX = X_train_rfe\n\nvif['Features'] = X.columns\n\nvif[\"VIF\"] = [variance_inflation_factor(X.values , i) for i in range(X.shape[1])]\n\nvif.VIF = round(vif[\"VIF\"] , 2)\n\nvif = vif.sort_values(by = \"VIF\" , ascending = False)\n\nvif","c2c4f734":"#Dropping Spring Column as it has high VIF value , i.e., >=5\n\nX_train_rfe_new = X_train_rfe.drop([\"spring\"] , axis=1)","79fc7700":"X_train_rfe = sm.add_constant(X_train_rfe_new)","4045c316":"lm = sm.OLS(y_train , X_train_rfe).fit()","5cf88e7e":"print(lm.summary())","9d6a4cf8":"vif = pd.DataFrame()\n\nX = X_train_rfe\n\nvif['Features'] = X.columns\n\nvif[\"VIF\"] = [variance_inflation_factor(X.values , i) for i in range(X.shape[1])]\n\nvif.VIF = round(vif[\"VIF\"] , 2)\n\nvif = vif.sort_values(by = \"VIF\" , ascending = False)\n\nvif","217db813":"#Dropping Dec Column as it is insignificant in presence of other variables\n\nX_train_rfe_new = X_train_rfe.drop([\"Dec\"] , axis=1)","cffdf060":"X_train_rfe = sm.add_constant(X_train_rfe_new)","7561e049":"lm = sm.OLS(y_train , X_train_rfe).fit()","10f24ef8":"print(lm.summary())","c736ba68":"vif = pd.DataFrame()\n\nX = X_train_rfe\n\nvif['Features'] = X.columns\n\nvif[\"VIF\"] = [variance_inflation_factor(X.values , i) for i in range(X.shape[1])]\n\nvif.VIF = round(vif[\"VIF\"] , 2)\n\nvif = vif.sort_values(by = \"VIF\" , ascending = False)\n\nvif","77f54f95":"#Dropping Nov Column as it is insignificant in presence of other variables\n\nX_train_rfe_new = X_train_rfe.drop([\"Nov\"] , axis=1)\n\nX_train_rfe = sm.add_constant(X_train_rfe_new)\n\nlm = sm.OLS(y_train , X_train_rfe).fit()","58f1e371":"print(lm.summary())","64326f70":"vif = pd.DataFrame()\n\nX = X_train_rfe\n\nvif['Features'] = X.columns\n\nvif[\"VIF\"] = [variance_inflation_factor(X.values , i) for i in range(X.shape[1])]\n\nvif.VIF = round(vif[\"VIF\"] , 2)\n\nvif = vif.sort_values(by = \"VIF\" , ascending = False)\n\nvif","887e3fff":"# Droppping Jan Column for decreasing the complexity of the model\n\nX_train_rfe_new = X_train_rfe.drop([\"Jan\"] , axis=1)\n\nX_train_rfe = sm.add_constant(X_train_rfe_new)\n\nlm = sm.OLS(y_train , X_train_rfe).fit()","4e1ccb27":"\nprint(lm.summary())","0109b2dc":"vif = pd.DataFrame()\n\nX = X_train_rfe\n\nvif['Features'] = X.columns\n\nvif[\"VIF\"] = [variance_inflation_factor(X.values , i) for i in range(X.shape[1])]\n\nvif.VIF = round(vif[\"VIF\"] , 2)\n\nvif = vif.sort_values(by = \"VIF\" , ascending = False)\n\nvif","a7f8e60e":"# Droppping Jul Column for decreasing the complexity of the model\n\nX_train_rfe_new = X_train_rfe.drop([\"Jul\"] , axis=1)\n\n\nX_train_rfe = sm.add_constant(X_train_rfe_new)\n\nlm = sm.OLS(y_train , X_train_rfe).fit()","5b99cf4d":"print(lm.summary())","25a77966":"vif = pd.DataFrame()\n\nX = X_train_rfe\n\n\nvif['Features'] = X.columns\n\nvif[\"VIF\"] = [variance_inflation_factor(X.values , i) for i in range(X.shape[1])]\n\nvif.VIF = round(vif[\"VIF\"] , 2)\n\nvif = vif.sort_values(by = \"VIF\" , ascending = False)\n\nvif","f4f1a3c6":"plt.figure(figsize = (12,10))\nsns.heatmap(X_train_rfe.corr() , annot=True, cmap= 'YlOrRd' )\nplt.show()","15df6aea":"# Dropping Humidity as it has higher VIF than Mist\n\nX_train_rfe_new = X_train_rfe.drop([\"humidity\"] , axis=1)\n\n\nX_train_rfe = sm.add_constant(X_train_rfe_new)\n\nlm = sm.OLS(y_train , X_train_rfe).fit()","9047cbd2":"print(lm.summary())","973b6bc7":"vif = pd.DataFrame()\n\nX = X_train_rfe\n\n\nvif['Features'] = X.columns\n\nvif[\"VIF\"] = [variance_inflation_factor(X.values , i) for i in range(X.shape[1])]\n\nvif.VIF = round(vif[\"VIF\"] , 2)\n\nvif = vif.sort_values(by = \"VIF\" , ascending = False)\n\nvif","e9192b0d":"y_train_pred = lm.predict(X_train_rfe)\n","eee76deb":"res = y_train - y_train_pred","cc62eb2c":"plt.figure(figsize=(10,6))\nsns.distplot(res , norm_hist=True)\nplt.xlabel('Errors', fontsize = 14)  \nplt.title('Error Terms Distribustion',fontdict={'fontsize' : 20 , 'color' : \"Green\" }) \nplt.show()","23b7c61b":"df_test.head()","b8dd58c3":"#create a lits of only numerical variables\n\nnum_vars = ['temperature' , 'humidity' , 'wind_speed' , 'count']\n\n# Fit the scaler on data\n\ndf_test[num_vars] = scaler.transform(df_test[num_vars])\ndf_test.head()","be4cae39":"df_test.describe()","d059a2ca":"y_test = df_test.pop('count')\nX_test = df_test","c1da6759":"X_test.info()","8e5955c5":"# Adding a constant to the Test Set\n\n\nX_test_sm = sm.add_constant(X_test)\n\nX_test_sm.head()","5caa9f90":"# Dropping all the columns that were originally dropped from the Training Dataset\n\nX_test_sm = X_test_sm.drop(['work_day', 'Aug','Feb' , 'Dec' , 'Nov' , 'humidity','spring' , 'Jan' , 'Jul', 'Jun', 'Mar', 'May' , 'Oct', 'Monday', 'Saturday', 'Thursday', 'Tuesday', 'Wednesday'] , axis=1)\n\nX_test_sm.head()","57230a01":"#Predictions\n\ny_test_pred = lm.predict(X_test_sm)","a2fdd2e4":"# Calculating the R-squarred value of the predictions\n\nr2_score(y_true= y_test , y_pred= y_test_pred)","1bd81564":"fig = plt.figure(figsize=(12,8) , frameon=False)\nplt.scatter(y_test,y_test_pred)\nplt.title('True_Values v\/s Predicted_Values',fontdict={'fontsize' : 30 , 'color' : \"Blue\" }) \nplt.xlabel('True_values', fontsize=18)\nplt.ylabel('Predicted_values', fontsize=18)  \nplt.show()","70395f81":"# Step 6: Residual Analysis","8773322e":"## The R-Squared predicted by the model on TEST SET (0.826) is very closed to R squared calculated by the model on TRAIN SET (0.7907)\n\n\n## Hence we can conclude that out model is predicting the Dependent variable effectively","1128b557":"# Step 2: Visalizing the data","85ba41dd":"### Inferences:\n- Demand for bikes is the highest in the months of August , September & October\n- Demand for bike is visibly low during Spring Season\n- There was a considerable increment in user base from 2018 to 2019","331c71b2":"# Step 4: Splitting the Data into Training and Testing Sets","3b077831":"### Plotting the scatter plot of Actual values v\/s Predicted values","a9698b21":"### Splitting X_train & y_train","f83aec5e":"##### Subplotting the boxplot for categorical variables with respect to the count variable","097b91b2":"### Importing required libraries","89948512":"# The equation of the best fitted line is as below:\n\n\n$ Count = 0.2332  \\times  Year + 0.5527  \\times  Temperature + 0.0894 \\times Summer + 0.1281 \\times Winter + 0.0978 \\times Sep  - 0.1552 \\times WindSpeed - 0.2785 \\times LightRainSnow - 0.0767 \\times Mist $\n","8e3d1637":"-----------------------------------------------------------------------END------------------------------------------------------------------------------","b5e16784":"##### It is visible that humidity and Mist  are highly correlated to each other. Hence dropping one of them to avoid multicollinearity","a24199d7":"### All the variables in the final model are significanty and the Variance Inflation Factor of each variable is less than the cut-off.  Also, the R-square and Adjusted R-squared value is 82.6% and 82.3% \n\n### Hence we will accept this model\n","946fcacc":"##### The residual values is centered around 0 and practically follws Normal distribution","d5077839":"##### Checking the correlation between every variable","6932f6cc":"# Step 3: Preparing Data","29332b22":"# Step 7: Making Predictions","efb9b9f1":"##### Applying the scaloing on the test set","f75559f4":"#### Using RFE\n\nWe will start building the model using RFE Technique","2f5c41a0":"# Problem Statement\n\nA bike-sharing system is a service in which bikes are made available for shared use to individuals on a short term basis for a price or free. Many bike share systems allow people to borrow a bike from a \"dock\" which is usually computer-controlled wherein the user enters the payment information, and the system unlocks it. This bike can then be returned to another dock belonging to the same system.\n\n\nA US bike-sharing provider BoomBikes has recently suffered considerable dips in their revenues due to the ongoing Corona pandemic. The company is finding it very difficult to sustain in the current market scenario. So, it has decided to come up with a mindful business plan to be able to accelerate its revenue as soon as the ongoing lockdown comes to an end, and the economy restores to a healthy state. \n\n\nIn such an attempt, BoomBikes aspires to understand the demand for shared bikes among the people after this ongoing quarantine situation ends across the nation due to Covid-19. They have planned this to prepare themselves to cater to the people's needs once the situation gets better all around and stand out from other service providers and make huge profits.\n\n\nThey have contracted a consulting company to understand the factors on which the demand for these shared bikes depends. Specifically, they want to understand the factors affecting the demand for these shared bikes in the American market. The company wants to know:\n\nWhich variables are significant in predicting the demand for shared bikes.\nHow well those variables describe the bike demands\nBased on various meteorological surveys and people's styles, the service provider firm has gathered a large dataset on daily bike demands across the American market based on some factors. \n\n\n### Business Goal:\nYou are required to model the demand for shared bikes with the available independent variables. It will be used by the management to understand how exactly the demands vary with different features. They can accordingly manipulate the business strategy to meet the demand levels and meet the customer's expectations. Further, the model will be a good way for management to understand the demand dynamics of a new market. \n\n","851d507e":"#### There are only two Binary variable i.e., year and work_day. Both of them are already in 1\/0 format. Hence no need for Binary variable conversion","7d2c0e40":"# Step 5: Building a linear model","8f461aa5":"### Training the model","009009de":"### Creating Dummy Variables","845bdc32":"# Step 8 : Model Evaluation","3f4e7ef6":"##### Scatterplot to identify the relationship between Count and other continous variables","dce7e08d":"# Step 1: Reading and understanding the dataset","6dc2d362":"### Preparing the Data for modelling\n- Encoding:\n    - Converting Binary variable to 1\/0\n    - Converting Other variables to Dummy Variables\n    \n- Splitting into Test and Train\n- Rescaling the variables","ab08b7af":"### Scaling the Training dataset\n","ceddd0d7":"##### Mapping Categorical values with respective values"}}