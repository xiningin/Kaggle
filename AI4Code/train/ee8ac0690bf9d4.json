{"cell_type":{"b103f18f":"code","1c15ed37":"code","5e242cd7":"code","fd110ca6":"code","68dd9861":"code","93ce55cc":"code","7ec2dc6b":"code","a83d079e":"code","0271b3f4":"code","40ffaf75":"code","8d56a8fd":"code","d5042419":"code","f06baf7c":"code","117077aa":"code","00e3c296":"code","e1ee01a6":"code","a08113a5":"code","61cda5f9":"code","1f07b7b8":"code","d4fab871":"markdown","30e4e6ad":"markdown","e2fe0da9":"markdown","9ec83855":"markdown","4d600a6d":"markdown","ec3dbb92":"markdown","3183aca4":"markdown","af95508d":"markdown","cd2b53e9":"markdown"},"source":{"b103f18f":"# Downloading kaggle env\n!pip install 'kaggle-environments>=0.1.6'","1c15ed37":"# Make Kaggle Env\nfrom kaggle_environments import evaluate, make, utils\n\nenv = make(\"connectx\", debug=True)","5e242cd7":"# Test Out the environment\nenv.render(mode=\"ipython\", width=500, height=450)","fd110ca6":"# Creating Agents\ndef agent(observation, configuration):\n    from random import choice\n    return choice([c for c in range(configuration.columns) if observation.board[c] == 0])","68dd9861":"env.reset()\nenv.run([agent, 'random'])\nenv.render(mode=\"ipython\", width=500, height=450)","93ce55cc":"from collections import defaultdict\nfrom tqdm.notebook import tqdm\nimport numpy as np\nimport random","7ec2dc6b":"LEARNING_RATE = .1\nDISCOUNT_FACTOR = .6\nEPSILON = .99\nSWITCH_PROBABILITY = .5\n\nPAIR = [None, 'negamax']\n\nepisode = 10000\n\nMIN_EPSILON = .1\nEPSILON_DECAY = .9999\nLR_DECAY = .9\nLR_DECAY_STEP = 1000","a83d079e":"def epsilon_greedy(n_action, QTable, state):\n    global EPSILON\n    \n    r = random.uniform(0, 1)\n    if r >= EPSILON:\n        curr_state = tuple(state['board'])\n        return np.argmax([QTable[curr_state][c] if state['board'][c] == 0 else -1e9 for c in range(n_action)])\n    else:\n        return random.choice([c for c in range(n_action) if state['board'][c] == 0])","0271b3f4":"Q_table = defaultdict(lambda: np.zeros(env.configuration.columns))","40ffaf75":"# History\ntotal_reward_per_episode = []\nepoch_per_episode = []\nq_table_row = []","8d56a8fd":"trainer = env.train(PAIR)","d5042419":"for i in tqdm(range(episode)):\n    # Do random change of enemy agent\n    if random.uniform(0, 1) < SWITCH_PROBABILITY:\n        PAIR = PAIR[::-1]\n        trainer = env.train(PAIR)\n    \n    EPSILON = max(MIN_EPSILON, EPSILON * EPSILON_DECAY)\n    state = trainer.reset()\n    done = False\n    \n    epoch, total_reward = 0, 0\n    \n    while not done:\n        action = int(epsilon_greedy(env.configuration.columns, Q_table, state))\n        next_state, reward, done, info = trainer.step(action)\n        \n        if done:\n            if reward == 1:\n                reward = 20\n            elif reward == 0:\n                reward = -20\n            else:\n                reward = 10\n        else:\n            reward = -0.05\n            \n        old_value = Q_table[tuple(state['board'])][action]\n        next_max = np.max(Q_table[tuple(next_state['board'])])\n        \n        new_value = old_value + LEARNING_RATE * (reward + DISCOUNT_FACTOR * next_max - old_value)\n        state = next_state\n        \n        Q_table[tuple(state['board'])][action] = new_value\n        \n        total_reward += reward\n        epoch += 1\n    \n    total_reward_per_episode.append(total_reward)\n    epoch_per_episode.append(epoch)\n    q_table_row.append(len(Q_table))\n    \n    if (i + 1) % LR_DECAY_STEP == 0:\n        LEARNING_RATE *= LR_DECAY","f06baf7c":"len(Q_table)","117077aa":"import matplotlib.pyplot as plt\n\nplt.plot(total_reward_per_episode)\nplt.xlabel('Episode')\nplt.ylabel('Total Rewards')\nplt.show()","00e3c296":"plt.plot(epoch_per_episode)\nplt.xlabel('Episode')\nplt.ylabel('Epoch')\nplt.show()","e1ee01a6":"plt.plot(q_table_row)\nplt.xlabel('Episode')\nplt.ylabel('Q-Table Row')\nplt.show()","a08113a5":"# Extract only the action of the QTable\ntmp_Q_table = Q_table.copy()\naction_dict = dict()\n\nfor a in tmp_Q_table:\n    if np.count_nonzero(tmp_Q_table) > 0:\n        action_dict[a] = int(np.argmax(tmp_Q_table[a]))","61cda5f9":"agent_file = \"\"\"def my_agent(observation, configuration):\n    from random import choice\n    \n    q_table = \"\"\" + str(action_dict).replace(' ', '') + \"\"\"\n    \n    board = observation.board[:]\n    \n    if tuple(board) not in q_table:\n        return choice([c for c in range(configuration.columns) if observation.board[c] == 0])\n        \n    action = q_table[tuple(board)]\n    \n    if observation.board[action] == 0:\n        return choice([c for c in range(configuration.columns) if observation.board[c] == 0])\n        \n    return action\n    \"\"\"","1f07b7b8":"with open('submission.py', 'w') as sf:\n    sf.write(agent_file)","d4fab871":"# Testing the agents","30e4e6ad":"## $\\epsilon-\\text{greedy}$ Policy\n- Get random number $r$ on $[0,1]$\n- $\\max_{Q}(a)$ if $r \\ge \\epsilon$\n- else choose randomly","e2fe0da9":"# Q-Learning Time!\nTrying to implement Q-Learning on the problem of connect X","9ec83855":"# Setup\nDownloading depedencies, and setting up the environment","4d600a6d":"## The algoritm\n- Observe current state, $s$\n- Choose action $a$ based on the selection of the policies\n- Take action $a$ observe the reward $r$ and the new state $s'$\n- Update the Q-value at the Q table using the observed reward and the maximum reward for the next state\n- Set the new state $s'$ as the state $s$ and repeat until state is terminal","ec3dbb92":"## Checking Training Result\nCheck the metrics taken on the training to see how the agent does the training","3183aca4":"## Creating an Agent\nCreate a python script to submit into Kaggle","af95508d":"Setting up all of the parameters that's going to be used on the training of the agent","cd2b53e9":"Initialize Q-Table $Q(s,a)$ arbitrarily"}}