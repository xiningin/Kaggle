{"cell_type":{"8137e136":"code","dc822362":"code","23c63105":"code","a9d70df6":"code","80649a90":"code","af4a07f3":"code","c6ababeb":"code","e6cfef6b":"code","cd532f2f":"code","f6ddcd33":"code","51dbb837":"code","3ce4379e":"code","b42c0b90":"code","f700c062":"code","3ab00105":"code","10005af5":"code","7579fe4f":"code","e80ccc2b":"code","64137f53":"code","c9c10b0b":"code","89fad1b2":"code","cf712d0a":"code","d1fa23b7":"markdown","b65ab34c":"markdown","ced75d08":"markdown","bbe2a91f":"markdown","f1785a20":"markdown","e6a9b400":"markdown","13ab76ca":"markdown","e52edd68":"markdown","f98b2428":"markdown","cfb84097":"markdown","093523c7":"markdown","5cb21373":"markdown","c7026b17":"markdown","47950ad9":"markdown","f25db737":"markdown","1c7b4f81":"markdown"},"source":{"8137e136":"class Config:\n    vocab_size = 15000 # Vocabulary Size\n    sequence_length = 20\n    batch_size = 64\n    validation_split = 0.15\n    embed_dim = 256\n    latent_dim = 256\n    num_heads = 2\n    epochs = 1 # Number of Epochs to train\n    model_path = \"model.h5\"\n    begin_token = \"[start]\"\n    end_token = \"[end]\"\n    use_pretrained_model = True # Download pretrained Model to speedup the training\nconfig = Config()","dc822362":"!pip install -q tensorflow==2.6.0","23c63105":"import pandas as pd\nimport tensorflow as tf\nfrom tensorflow.keras.layers import TextVectorization\nimport pathlib\nimport random\nimport string\nimport re\nimport numpy as np\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nimport os\nimport sklearn\nfrom sklearn.model_selection import train_test_split","a9d70df6":"data = pd.read_csv(\"\/kaggle\/input\/englishspanish-translation-dataset\/data.csv\" if os.path.exists(\"\/kaggle\/input\/englishspanish-translation-dataset\/data.csv\") else \"data.csv\")\ndata.head()","80649a90":"data[\"spanish\"] = data[\"spanish\"].apply(lambda item: config.begin_token + \" \" + item + \" \" + config.end_token)","af4a07f3":"data.head()","c6ababeb":"strip_chars = string.punctuation + \"\u00bf\"\nstrip_chars = strip_chars.replace(\"[\", \"\")\nstrip_chars = strip_chars.replace(\"]\", \"\")\nprint(strip_chars)\ndef spanish_standardize(input_string):\n    lowercase = tf.strings.lower(input_string)\n    return tf.strings.regex_replace(lowercase, \"[%s]\"%re.escape(strip_chars), \"\")\nenglish_vectorization = TextVectorization(\n    max_tokens=config.vocab_size, \n    output_mode=\"int\", \n    output_sequence_length=config.sequence_length,\n)\nspanish_vectorization = TextVectorization(\n    max_tokens=config.vocab_size,\n    output_mode=\"int\",\n    output_sequence_length=config.sequence_length + 1,\n    standardize=spanish_standardize,\n)\nenglish_vectorization.adapt(list(data[\"english\"]))\nspanish_vectorization.adapt(list(data[\"spanish\"]))","e6cfef6b":"def preprocess(english, spanish):\n    return (english, spanish[:, :-1]), spanish[:, 1:]\ndef make_dataset(df, batch_size, mode):\n    english = english_vectorization(list(df[\"english\"]))\n    spanish = spanish_vectorization(list(df[\"spanish\"]))\n    dataset = tf.data.Dataset.from_tensor_slices((english, spanish))\n    if mode == \"train\":\n       dataset = dataset.shuffle(256) \n    dataset = dataset.batch(batch_size)\n    dataset = dataset.map(preprocess)\n    dataset = dataset.take(len(df) \/\/ batch_size).cache().prefetch(16).repeat(1)\n    return dataset","cd532f2f":"train, valid = train_test_split(data, test_size=config.validation_split)\ntrain.shape, valid.shape","f6ddcd33":"train_ds = make_dataset(train, batch_size=config.batch_size, mode=\"train\")\nvalid_ds = make_dataset(valid, batch_size=config.batch_size, mode=\"valid\")","51dbb837":"for batch in train_ds.take(1):\n    print(batch)","3ce4379e":"class FNetEncoder(layers.Layer):\n    def __init__(self, embed_dim, dense_dim, **kwargs):\n        super(FNetEncoder, self).__init__(**kwargs)\n        self.embed_dim = embed_dim\n        self.dense_dim = dense_dim\n        self.dense_proj = keras.Sequential(\n            [\n                layers.Dense(dense_dim, activation=\"relu\"),\n                layers.Dense(embed_dim),\n            ]\n        )\n        self.layernorm_1 = layers.LayerNormalization()\n        self.layernorm_2 = layers.LayerNormalization()\n\n    def call(self, inputs):\n        # Casting the inputs to complex64\n        inp_complex = tf.cast(inputs, tf.complex64)\n        # Projecting the inputs to the frequency domain using FFT2D and\n        # extracting the real part of the output\n        fft = tf.math.real(tf.signal.fft2d(inp_complex))\n        proj_input = self.layernorm_1(inputs + fft)\n        proj_output = self.dense_proj(proj_input)\n        return self.layernorm_2(proj_input + proj_output)","b42c0b90":"class PositionalEmbedding(layers.Layer):\n    def __init__(self, sequence_length, vocab_size, embed_dim, **kwargs):\n        super(PositionalEmbedding, self).__init__(**kwargs)\n        self.token_embeddings = layers.Embedding(\n            input_dim=vocab_size, output_dim=embed_dim\n        )\n        self.position_embeddings = layers.Embedding(\n            input_dim=sequence_length, output_dim=embed_dim\n        )\n        self.sequence_length = sequence_length\n        self.vocab_size = vocab_size\n        self.embed_dim = embed_dim\n\n    def call(self, inputs):\n        length = tf.shape(inputs)[-1]\n        positions = tf.range(start=0, limit=length, delta=1)\n        embedded_tokens = self.token_embeddings(inputs)\n        embedded_positions = self.position_embeddings(positions)\n        return embedded_tokens + embedded_positions\n\n    def compute_mask(self, inputs, mask=None):\n        return tf.math.not_equal(inputs, 0)\n","f700c062":"class FNetDecoder(layers.Layer):\n    def __init__(self, embed_dim, latent_dim, num_heads, **kwargs):\n        super(FNetDecoder, self).__init__(**kwargs)\n        self.embed_dim = embed_dim\n        self.latent_dim = latent_dim\n        self.num_heads = num_heads\n        self.attention_1 = layers.MultiHeadAttention(\n            num_heads=num_heads,\n            key_dim=embed_dim\n        )\n        self.attention_2 = layers.MultiHeadAttention(\n            num_heads=num_heads,\n            key_dim=embed_dim\n        )\n        self.dense_proj = keras.Sequential([\n            layers.Dense(latent_dim, activation=\"relu\"),\n            layers.Dense(embed_dim),\n        ])\n        self.layernorm_1 = layers.LayerNormalization()\n        self.layernorm_2 = layers.LayerNormalization()\n        self.layernorm_3 = layers.LayerNormalization()\n        self.supports_masking = True\n        \n    def call(self, inputs, encoder_outputs, mask = None):\n        causal_mask = self.get_causal_attention_mask(inputs)\n        if mask != None:\n            padding_mask = tf.cast(mask[:, tf.newaxis, :], dtype=\"int32\")\n            padding_mask = tf.minimum(padding_mask, causal_mask)\n        \n        attention_output_1 = self.attention_1(\n            query=inputs, \n            value=inputs, \n            key=inputs, \n            attention_mask=causal_mask\n        )\n        \n        out_1 = self.layernorm_1(inputs + attention_output_1)\n        \n        attention_output_2 = self.attention_2(\n            query=out_1, \n            value=encoder_outputs, \n            key=encoder_outputs, \n            attention_mask=padding_mask\n        )\n        \n        out_2 = self.layernorm_2(out_1 + attention_output_2)\n        \n        proj_output = self.dense_proj(out_2)\n        \n        return self.layernorm_3(out_2 + proj_output)\n    \n    def get_causal_attention_mask(self, inputs):\n        input_shape = tf.shape(inputs)\n        batch_size, sequence_length = input_shape[0], input_shape[1]\n        i = tf.range(sequence_length)[:, tf.newaxis]\n        j = tf.range(sequence_length)\n        mask = tf.cast(i >= j, dtype=\"int32\")\n        mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))\n        mult = tf.concat(\n            [tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)],\n            axis=0\n        )\n        return tf.tile(mask, mult)","3ab00105":"def get_fnet(config):\n    encoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"encoder_inputs\")\n    x = PositionalEmbedding(config.sequence_length, config.vocab_size, config.embed_dim)(encoder_inputs)\n    encoder_outputs = FNetEncoder(config.embed_dim, config.latent_dim)(x)\n    encoder = keras.Model(encoder_inputs, encoder_outputs)\n\n    decoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"decoder_inputs\")\n    encoded_seq_inputs = keras.Input(shape=(None, config.embed_dim), name=\"decoder_state_inputs\")\n    x = PositionalEmbedding(config.sequence_length, config.vocab_size, config.embed_dim)(decoder_inputs)\n    x = FNetDecoder(config.embed_dim, config.latent_dim, config.num_heads)(x, encoded_seq_inputs)\n    x = layers.Dropout(0.5)(x)\n    decoder_outputs = layers.Dense(config.vocab_size, activation=\"softmax\")(x)\n    decoder = keras.Model([decoder_inputs, encoded_seq_inputs], decoder_outputs, name=\"outputs\")\n\n    decoder_outputs = decoder([decoder_inputs, encoder_outputs])\n    fnet = keras.Model(\n        [encoder_inputs, decoder_inputs], decoder_outputs, name=\"fnet\"\n    )\n    return fnet","10005af5":"fnet = get_fnet(config)","7579fe4f":"fnet.summary()","e80ccc2b":"keras.utils.plot_model(fnet, show_shapes=True)","64137f53":"fnet.compile(\n    \"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]\n)","c9c10b0b":"checkpoint = keras.callbacks.ModelCheckpoint(config.model_path, save_weights_only=True, save_best_only=True)\nearly_stopping = keras.callbacks.EarlyStopping(patience=5)\nreduce_lr = keras.callbacks.ReduceLROnPlateau(patience=5, min_delta=1e-4, min_lr=1e-4)\nif config.use_pretrained_model:\n    model_path = keras.utils.get_file(config.model_path, \"https:\/\/github.com\/LoniQin\/English-Spanish-Transaltion-FNet\/raw\/main\/\" + config.model_path)\n    fnet.load_weights(model_path)\n    fnet.fit(train_ds.take(100), epochs=1, validation_data=valid_ds.take(10), callbacks=[checkpoint, reduce_lr])\nelse:\n    fnet.fit(train_ds, epochs=config.epochs, validation_data=valid_ds, callbacks=[checkpoint, reduce_lr])","89fad1b2":"spanish_vocab = spanish_vectorization.get_vocabulary()\nspanish_index_lookup = dict(zip(range(len(spanish_vocab)), spanish_vocab))\ndef decode_sequence(fnet, input_sentence):\n    tokenized_input_sentence = english_vectorization([input_sentence])\n    decoded_sentence = config.begin_token\n    for i in range(config.sequence_length):\n        tokenized_target_sentence = spanish_vectorization([decoded_sentence])[:, :-1]\n        predictions = fnet([tokenized_input_sentence, tokenized_target_sentence])\n\n        sampled_token_index = np.argmax(predictions[0, i, :])\n        sampled_token = spanish_index_lookup[sampled_token_index]\n        decoded_sentence += \" \" + sampled_token\n\n        if sampled_token == config.end_token:\n            break\n    return decoded_sentence","cf712d0a":"fnet.load_weights(config.model_path)\nfor i in np.random.choice(len(data), 10):\n    item = data.iloc[i]\n    translated = decode_sequence(fnet, item[\"english\"])\n    print(\"English:\", item[\"english\"])\n    print(\"Spanish:\", item[\"spanish\"])\n    print(\"Translated:\", translated)","d1fa23b7":"## Configuration","b65ab34c":"## Setup","ced75d08":"## References\n- [FNet: Mixing Tokens with Fourier Transforms](https:\/\/arxiv.org\/abs\/2105.03824v3)\n- [Attention Is All You Need](https:\/\/arxiv.org\/abs\/1706.03762v5)\n- [Text Generation using FNet](https:\/\/keras.io\/examples\/nlp\/text_generation_fnet\/)\n","bbe2a91f":"Let's visualize the Model.","f1785a20":"### FNet Decoder","e6a9b400":"### The FNet Encoder","13ab76ca":"### The FNet Model","e52edd68":"# English-Spanish Translation: FNet\n## Table of Contents\n* Overview\n* Configuration\n* Setup\n* Import datasets\n* Model Development\n* Transalation\n* References","f98b2428":"## Translation\n","cfb84097":"### Positional Embedding","093523c7":"### Model Training\n","5cb21373":"Install latest version of TensorFlow. ","c7026b17":"## Import datasets","47950ad9":"## Model Development","f25db737":"## Overview\nIn this Notebook, I will develop a English-Spanish Translation Model using FNet Transformer Encoder-Decoder Architecture from scratch. Please be noted that running this notebook is 10 times slower than Google Colab when using GPU. So I set this notebook download the pretrained Model to speedup the training.","1c7b4f81":"I have pretrained the Model. You may use it to speedup the training."}}