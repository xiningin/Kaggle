{"cell_type":{"0024a547":"code","53c134cc":"code","dfaf0a8d":"code","40cea7a2":"code","1b01d68a":"code","a16df5fc":"code","bff11269":"code","ff4eb60f":"code","5f0f5b09":"code","e31a9589":"code","430ac0ec":"code","e1db507b":"code","5fb5af4e":"code","0e48a73c":"code","c8ca41a8":"code","342b8714":"code","b485ae69":"code","76120b15":"code","1cc97596":"code","dc15ed3e":"code","ecd2e012":"code","fde7ee17":"code","7f9f06b4":"code","1837837d":"code","db119734":"code","10ac9944":"code","736097d7":"code","1cc5db37":"code","a4999083":"code","96fb308e":"code","edb5d20c":"code","065f2f84":"code","f48500af":"code","4b14fcde":"code","4ac47d91":"code","8e8edeba":"code","e7430e0d":"code","c1befce5":"code","1bc2ecb2":"code","557b2143":"code","9070f2dd":"code","e967fa34":"code","bc4227f0":"code","fa4d2512":"code","5f74f1a5":"code","6e907a62":"code","d6d9696f":"code","97dd4fd7":"code","4a037bb4":"code","24465f97":"code","d4cbd37d":"code","944ea012":"code","d6d7281c":"code","ac1e04ef":"code","4089a0d1":"code","e493536e":"code","c4379792":"code","b66fc891":"code","d9a0778e":"code","e1118b0b":"code","b02e5ae6":"code","f6fb87d3":"code","85ed331f":"code","9b19801c":"markdown","6d171b93":"markdown","7ea758a4":"markdown","48a5b407":"markdown","39a85e63":"markdown","80edd748":"markdown","ce06ecaf":"markdown","f5f0a200":"markdown","d8b63f27":"markdown","508b4450":"markdown","7ea3e83b":"markdown","a3843f17":"markdown","15567574":"markdown","abeea8c7":"markdown","fbbdb65f":"markdown","3850b2ef":"markdown","15cf2af1":"markdown","8f474096":"markdown","ef1a05d6":"markdown","e2c4c53b":"markdown","4d7d1342":"markdown","f2f81436":"markdown","a491b4f7":"markdown","bcbfeba1":"markdown","3062b874":"markdown","daa56d91":"markdown","944ab586":"markdown","b3cce0ac":"markdown","5ec65d7c":"markdown","1a7619cf":"markdown","3cb7488a":"markdown","5d89a30e":"markdown","a460feda":"markdown","38ad97c1":"markdown","3d6afad2":"markdown","22b6d676":"markdown","37e1a7bc":"markdown","b351e0d6":"markdown","94c989f4":"markdown","18e57460":"markdown","bb3343a9":"markdown","19858593":"markdown","37ebd8aa":"markdown","da17f994":"markdown","c94d22c3":"markdown","d9778724":"markdown","430664ca":"markdown","673e2751":"markdown","403ed750":"markdown","9132e729":"markdown","c2c7b77a":"markdown","2e5ecc27":"markdown","48175933":"markdown","2f38353c":"markdown","b4b66867":"markdown","7d84da3b":"markdown","ad306fa5":"markdown","a720b61b":"markdown","0f7908f4":"markdown","aee9546c":"markdown","9f58639c":"markdown","351a925e":"markdown","fdb1876b":"markdown"},"source":{"0024a547":"import numpy as np\nimport pandas as pd","53c134cc":"!pip install scikit-learn==0.23.0\n\nfrom numpy.ma import MaskedArray\nimport sklearn.utils.fixes\n\nsklearn.utils.fixes.MaskedArray = MaskedArray","dfaf0a8d":"df = pd.read_csv(\"..\/input\/adult-pmr3508\/train_data.csv\", index_col=['Id'], na_values=\"?\")","40cea7a2":"df.head()","1b01d68a":"df.info()","a16df5fc":"df.describe()","bff11269":"import matplotlib.pyplot as plt\nimport seaborn as sns","ff4eb60f":"# Copia \"df\" para \"df_analysis\"\ndf_analysis = df.copy()","5f0f5b09":"# Importando o LabelEncoder\nfrom sklearn.preprocessing import LabelEncoder\n\n# Instanciando o LabelEncoder\nle = LabelEncoder()\n\n# Modificando o nosso dataframe, transformando a vari\u00e1vel de classe em 0s e 1s\ndf_analysis['income'] = le.fit_transform(df_analysis['income'])","e31a9589":"df_analysis['income']","430ac0ec":"mask = np.triu(np.ones_like(df_analysis.corr(), dtype=np.bool))\n\nplt.figure(figsize=(10,10))\n\nsns.heatmap(df_analysis.corr(), mask=mask, square = True, annot=True, vmin=-1, vmax=1, cmap='autumn')\nplt.show()","e1db507b":"sns.distplot(df_analysis['age']);","5fb5af4e":"sns.catplot(x=\"income\", y=\"hours.per.week\", data=df_analysis);","0e48a73c":"sns.catplot(x=\"income\", y=\"hours.per.week\", kind=\"boxen\", data=df_analysis);","c8ca41a8":"sns.catplot(x=\"income\", y=\"education.num\", kind=\"boxen\", data=df_analysis);","342b8714":"sns.catplot(x=\"income\", y=\"age\", kind=\"boxen\", data=df_analysis);","b485ae69":"sns.catplot(x=\"income\", y=\"capital.gain\", kind=\"boxen\", data=df_analysis);","76120b15":"sns.catplot(x=\"income\", y=\"capital.loss\", kind=\"boxen\", data=df_analysis);","1cc97596":"sns.catplot(x=\"income\", y=\"capital.gain\", data=df_analysis);","dc15ed3e":"sns.catplot(x=\"income\", y=\"capital.loss\", data=df_analysis);","ecd2e012":"df_analysis.describe()","fde7ee17":"sns.catplot(y=\"sex\", x=\"income\", kind=\"bar\", data=df_analysis);","7f9f06b4":"sns.catplot(y=\"race\", x=\"income\", kind=\"bar\", data=df_analysis);","1837837d":"sns.catplot(y=\"workclass\", x=\"income\", kind=\"bar\", data=df_analysis);","db119734":"sns.catplot(y=\"marital.status\", x=\"income\", kind=\"bar\", data=df_analysis);","10ac9944":"sns.catplot(y=\"occupation\", x=\"income\", kind=\"bar\", data=df_analysis);","736097d7":"sns.catplot(y=\"native.country\", x=\"income\", kind=\"bar\", data=df_analysis);","1cc5db37":"df_analysis[\"native.country\"].value_counts()","a4999083":"df.drop_duplicates(keep='first', inplace=True)","96fb308e":"df = df.drop(['fnlwgt', 'native.country', 'education'], axis=1)","edb5d20c":"df.head()","065f2f84":"# Removendo a nossa vari\u00e1vel de classe\nY_train = df.pop('income')\n\nX_train = df","f48500af":"X_train.head()","4b14fcde":"# Seleciona as vari\u00e1veis num\u00e9ricas\nnumerical_cols = list(X_train.select_dtypes(include=[np.number]).columns.values)\n\n# Remove as vari\u00e1veis num\u00e9ricas esparsas\nnumerical_cols.remove('capital.gain')\nnumerical_cols.remove('capital.loss')\n\n# Seleciona as vari\u00e1veis num\u00e9ricas esparsas\nsparse_cols = ['capital.gain', 'capital.loss']\n\n# Seleciona as vari\u00e1veis categ\u00f3ricas\ncategorical_cols = list(X_train.select_dtypes(exclude=[np.number]).columns.values)\n\n# Mostrando as diferentes sele\u00e7\u00f5es\nprint(\"Colunas num\u00e9ricas: \", numerical_cols)\nprint(\"Colunas esparsas: \", sparse_cols)\nprint(\"Colunas categ\u00f3ricas: \", categorical_cols)","4ac47d91":"from sklearn.impute import SimpleImputer\n\n# Inicializa nosso Imputer\nsimple_imputer = SimpleImputer(strategy='most_frequent')","8e8edeba":"# Cria um array com um dado faltante\narray = np.array([[\"Female\"],\n         [\"Male\"],\n         [np.nan],\n         [\"Female\"]], dtype=object)\n\n# Preenche o dado faltante com o Imputer\nnew_array = simple_imputer.fit_transform(array)\n\nprint(new_array)","e7430e0d":"from sklearn.preprocessing import OneHotEncoder\n\n# Inicializa nosso Encoder\none_hot = OneHotEncoder(sparse=False)","c1befce5":"# Cria um array com dados categ\u00f3ricos\narray = np.array([[\"Female\"],\n         [\"Male\"],\n         [\"Female\"],\n         [\"Female\"]], dtype=object)\n\n# Transforma o nosso array\nnew_array = one_hot.fit_transform(array)\n\nnew_array","1bc2ecb2":"from sklearn.pipeline import Pipeline\n\n# Cria a nossa pipeline categ\u00f3rica\ncategorical_pipeline = Pipeline(steps = [\n    ('imputer', SimpleImputer(strategy = 'most_frequent')),\n    ('onehot', OneHotEncoder(drop='if_binary'))\n])","557b2143":"# Cria um array com dados categ\u00f3ricos\narray = np.array([[\"Female\"],\n         [\"Male\"],\n         [np.nan],\n         [\"Female\"]], dtype=object)\n\n# Transforma o nosso array\nnew_array = categorical_pipeline.fit_transform(array)","9070f2dd":"from sklearn.impute import KNNImputer\n\n# Cria o nosso KNNImputer com 5 vizinhos\nknn_imputer = KNNImputer(n_neighbors=5)","e967fa34":"# Cria o nosso array com dados faltantes\narray = [[1, 2, np.nan], [3, 4, 3], [np.nan, 6, 5], [8, 8, 7]]\n\n# Preenche os dados faltantes\nnew_array = knn_imputer.fit_transform(array)\n\nnew_array","bc4227f0":"from sklearn.preprocessing import StandardScaler\n\n# Cria o nosso StandardScaler\nscaler = StandardScaler()","fa4d2512":"# Cria um array num\u00e9rico\narray = [[-3, 0], [0, 0], [3, 1], [0, 1]]\n\n# Normaliza nosso array\nnew_array = scaler.fit_transform(array)\n\nnew_array","5f74f1a5":"# Cria a nossa pipeline num\u00e9rica\nnumerical_pipeline = Pipeline(steps = [\n    ('imputer', KNNImputer(n_neighbors=10, weights=\"uniform\")),\n    ('scaler', StandardScaler())\n])","6e907a62":"from sklearn.preprocessing import RobustScaler\n\nsparse_pipeline = Pipeline(steps = [\n    ('imputer', KNNImputer(n_neighbors=10, weights=\"uniform\")),\n    ('scaler', RobustScaler())\n])","d6d9696f":"from sklearn.compose import ColumnTransformer\n\n# Cria o nosso Pr\u00e9-Processador\n\n# Cada pipeline est\u00e1 associada a suas respectivas colunas no datast\npreprocessor = ColumnTransformer(transformers = [\n    ('num', numerical_pipeline, numerical_cols),\n    ('spr', sparse_pipeline, sparse_cols),\n    ('cat', categorical_pipeline, categorical_cols)\n])","97dd4fd7":"X_train = preprocessor.fit_transform(X_train)","4a037bb4":"from sklearn.neighbors import KNeighborsClassifier\n\n# Instancia nosso classificador\nknn = KNeighborsClassifier(n_neighbors=20)","24465f97":"from sklearn.model_selection import cross_val_score\n\nscore = cross_val_score(knn, X_train, Y_train, cv = 5, scoring=\"accuracy\")\nprint(\"Acur\u00e1cia com cross validation:\", score.mean())","d4cbd37d":"# Quantidades de vizinhos a serem testadas\nneighbors = [15, 20, 25, 30, 35]\n\n# Dicion\u00e1rio para guardar as pontua\u00e7\u00f5es de cada hiperpar\u00e2metro\nneighbors_scores = {}\n\nfor n_neighbors in neighbors:\n    # Calcula a m\u00e9dia de acur\u00e1cia de cada classificador\n    score = cross_val_score(KNeighborsClassifier(n_neighbors=n_neighbors), X_train, Y_train, cv = 5, scoring=\"accuracy\").mean()\n    \n    # Guarda essa acur\u00e1cia\n    neighbors_scores[n_neighbors] = score\n\n# Obt\u00e9m a quantidade de vizinhos com o melhor desempenho\nbest_n = max(neighbors_scores, key=neighbors_scores.get)\n\nprint(\"Melhor hiperpar\u00e2metro: \", best_n)\nprint(\"Melhor acur\u00e1cia: \", neighbors_scores[best_n])","944ea012":"# Importa o Bayes Search:\nfrom skopt import BayesSearchCV\n\nfrom sklearn.model_selection import ShuffleSplit\n\n# Importa o espa\u00e7o de busca inteiro\nfrom skopt.space import Integer\n\ncv = ShuffleSplit(n_splits=5, test_size=0.2, random_state=42)\n\n\n# Cria o Bayes Search:\nbayes_search_cv = BayesSearchCV(estimator = KNeighborsClassifier(),\n                              search_spaces = {'n_neighbors': Integer(10, 50),}, # Vizinhos de 10 a 50\n                              cv = cv,\n                              n_iter = 12, random_state=42)\n\n# Realizando a otimiza\u00e7\u00e3o por BayesSearch:\nbayes_search_cv.fit(X_train, Y_train)\n\nbest_param = bayes_search_cv.best_params_['n_neighbors']\n\nprint('Melhor quantidade de vizinhos: {}'.format(bayes_search_cv.best_params_['n_neighbors']))\nprint('Desempenho do melhor modelo: {}'.format(round(bayes_search_cv.best_score_,5)))","d6d7281c":"knn = KNeighborsClassifier(n_neighbors=26)","ac1e04ef":"knn.fit(X_train, Y_train)","4089a0d1":"test_data = pd.read_csv(\"..\/input\/adult-pmr3508\/test_data.csv\", index_col=['Id'], na_values=\"?\")","e493536e":"X_test = test_data.drop(['fnlwgt', 'native.country', 'education'], axis=1)","c4379792":"X_test = preprocessor.transform(X_test)","b66fc891":"predictions = knn.predict(X_test)","d9a0778e":"predictions","e1118b0b":"submission = pd.DataFrame()","b02e5ae6":"submission[0] = test_data.index\nsubmission[1] = predictions\nsubmission.columns = ['Id','income']","f6fb87d3":"submission.head()","85ed331f":"submission.to_csv('submission.csv',index = False)","9b19801c":"<a id=\"section-two\"><\/a>\n## \ud83d\udcc8 Visualiza\u00e7\u00e3o de Dados\n\nAp\u00f3s a importa\u00e7\u00e3o, podemos come\u00e7ar o processo de **Visualiza\u00e7\u00e3o de Dados**, no qual tentaremos entender melhor a disposi\u00e7\u00e3o de nossos dados e definir como vamos trabalhar com eles. \n\n**[Leitura Recomendada - Visualiza\u00e7\u00e3o e An\u00e1lise de Dados](https:\/\/medium.com\/turing-talks\/como-visualizar-e-analisar-dados-com-python-f209bfbae68e)**\n\nNosso primeiro passo ser\u00e1 importar as bibliotecas *matplotlib* e *seaborn*, muito \u00fateis para a constru\u00e7\u00e3o de gr\u00e1ficos.","6d171b93":"Finalmente, podemos aplicar todos esses passos no nosso dataset com apenas uma linha de c\u00f3digo:","7ea758a4":"E vamos ajust\u00e1-lo para o nosso dataset!","48a5b407":"Entretanto, vamos focar em analisar a distribui\u00e7\u00e3o de cada vari\u00e1vel em rela\u00e7\u00e3o \u00e0 vari\u00e1vel de classe **'income'**, o que pode ser feito por meio do m\u00e9todo `catplot`.\n\nCom o catplot, podemos tra\u00e7ar um scatter plot simples das nossas vari\u00e1veis:","39a85e63":"![](https:\/\/i.pinimg.com\/originals\/4e\/9e\/6f\/4e9e6f979347906a426adcbe57fd3259.gif)\n\nMuito obrigado por acompanhar esse notebook at\u00e9 o final!\n\nV\u00e1rios conte\u00fados aplicados na an\u00e1lise n\u00e3o seria poss\u00edvel sem [o material do Grupo Turing](https:\/\/medium.com\/turing-talks), que recomendo muito.\n\nSe quiser acompanhar nossos projetos, tamb\u00e9m temos um [GitHub](https:\/\/github.com\/orgs\/GrupoTuring\/dashboard) e um [site](http:\/\/www.grupoturing.com.br\/)!","80edd748":"Em seguida, vamos remover algumas colunas que n\u00e3o consideramos t\u00e3o importantes:","ce06ecaf":"Olhando por cima, todas as nossas vari\u00e1veis parecem estar ligadas de alguma forma com a renda dos indiv\u00edduos.\n\nPrincipalmente nesse \u00faltimo gr\u00e1fico, voc\u00ea deve ter percebido as linhas pretas em cima de cada uma das barras. Essas linhas representam a varia\u00e7\u00e3o de renda dentro de cada grupo. A maioria dos pa\u00edses possui uma varia\u00e7\u00e3o muito grande, t\u00e3o grande que n\u00e3o podemos ter muita certeza de seu valor real.\n\nQuando analisamos a quantidade de indiv\u00edduos de cada na\u00e7\u00e3o no nosso dataset, essa varia\u00e7\u00e3o come\u00e7a a fazer mais sentido:","f5f0a200":"A desigualdade tamb\u00e9m est\u00e1 presente nesse caso. A etnia de cada indiv\u00edduo parece ter uma rela\u00e7\u00e3o bem grande com sua probabilidade de ganhar mais de 50k.","d8b63f27":"Em seguida, podemos tra\u00e7ar o mesmo gr\u00e1fico para as diferentes etnias do nosso dataset:","508b4450":"Conseguimos obter nosso resultado!","7ea3e83b":"Com o m\u00e9todo `info`, podemos observar o tipo de cada vari\u00e1vel da nossa tabela, bem como quantos valores n\u00e3o faltantes n\u00f3s temos.","a3843f17":"Cada quadrado do heatmap representa o coeficiente de correla\u00e7\u00e3o entre as vari\u00e1veis correspondentes nos eixos X e Y, e quanto mais pr\u00f3xima de amarelo for sua cor, maior a correla\u00e7\u00e3o positiva. Por exemplo, a correla\u00e7\u00e3o entre o **\"education.num\"** e a **\"income\"** \u00e9 de **0.34**, por isto sua cor \u00e9 mais clara que as outras.\n\nNo gr\u00e1fico, podemos observar que todas as nossas vari\u00e1veis possuem algum grau de correla\u00e7\u00e3o com a **\"income\"**, o valor que queremos predizer, exceto pela **\"fnlwgt\"**, que n\u00e3o parece estar relacionada com nenhuma outra vari\u00e1vel. Em fun\u00e7\u00e3o disto, parece n\u00e3o existir muito valor em utilizar essa vari\u00e1vel como feature, j\u00e1 que ela n\u00e3o ajudar\u00e1 a diferenciar os nossos dados.","15567574":"A primeira limpeza que faremos \u00e9 remover os dados duplicados. Para isto, podemos utilizar o m\u00e9todo `drop_duplicates` do nosso dataframe:","abeea8c7":"Como \u00e9 poss\u00edvel perceber, esse tipo de visualiza\u00e7\u00e3o acaba n\u00e3o sendo t\u00e3o \u00fatil quando temos muitos dados. Apesar disso, podemos ver que indiv\u00edduos que trabalham poucas horas por semana tendem a ganhar menos de 50k por ano.\n\nPara visualizar melhor nossas distribui\u00e7\u00f5es, vamos utilizar os 'boxen plots', como apresentado a seguir:","fbbdb65f":"<a id=\"subsection-two-two\"><\/a>\n### \ud83d\udcca Visualizando Vari\u00e1veis Categ\u00f3ricas\n\nPara visualizar as vari\u00e1veis categ\u00f3ricas, precisaremos de outros tipos de gr\u00e1ficos para representar claramente nossas diferentes categorias. Nesse caso, vamos recorrer aos gr\u00e1ficos de barras para comparar a renda de v\u00e1rios grupos.\n\nO m\u00e9todo `catplot` da biblioteca *seaborn* \u00e9 uma \u00f3tima op\u00e7\u00e3o para tra\u00e7ar os gr\u00e1ficos com vari\u00e1veis categ\u00f3ricas, e s\u00f3 precisamos especificar o par\u00e2metro `kind` para obter v\u00e1rios tipos de gr\u00e1ficos diferentes.\n\nPrimeiramente, vamos representar o gr\u00e1fico do **sexo** de cada pessoa pela probabilidade de cada uma ganhar mais de 50k por ano:","3850b2ef":"### Dados de Teste\n\nFinalmente, agora podemos predizer os nossos dados teste e enviar uma submiss\u00e3o para o Kaggle.\n\nPrimeiramente, vamos importar os nossos dados:","15cf2af1":"Nosso modelo conseguiu um resultado muito bom para 20 vizinhos! Uma acur\u00e1cia de **86,8%**.\n\nAgora, vamos testar outras quantidades de vizinhos para o nosso KNN, e descobrir qual nos d\u00e1 um melhor resultado.","8f474096":"Em seguida, vamos remover as colunas que ignoramos anteriormente:","ef1a05d6":"Finalmente! S\u00f3 basta exportar nosso DataFrame:","e2c4c53b":"Dando uma olhada inicial nos nossos dados num\u00e9ricos, j\u00e1 podemos perceber algumas coisas interessantes. As vari\u00e1veis **'capital-gain'** e **'capital.loss'** parecem ser bem esparsas, com a maioria de seus dados sendo iguais a 0. Al\u00e9m disso, o **'capital.gain'** especialmente parece ter alguns outliers, em fun\u00e7\u00e3o de seu grande desvio padr\u00e3o e de seu m\u00e1ximo de **99999.0**.","4d7d1342":"Em seguida, vamos utilizar o `StandardScaler` para normalizar nossas vari\u00e1veis. \n\nEsse transformador padroniza as nossas features alterando sua m\u00e9dia para 0 e seu desvio padr\u00e3o para 1. Dessa forma, todas as vari\u00e1veis estar\u00e3o em uma mesma **'escala'**.","f2f81436":"<a id=\"section-four\"><\/a>\n## \ud83d\udd2e Predi\u00e7\u00e3o\n\nTerminada a limpeza e o pr\u00e9-processamento dos dados, podemos enfim aplicar o nosso modelo de predi\u00e7\u00e3o, nesse caso, um **classificador KNN**. Caso voc\u00ea n\u00e3o esteja familiarizado com esse modelo, recomendo a leitura do seguinte texto:\n\n**[Leitura Recomendada - KNN](https:\/\/medium.com\/turing-talks\/turing-talks-13-modelo-de-predi%C3%A7%C3%A3o-knn-3be880c9b9d1)**\n\nPara utilizar esse modelo, vamos instanciar a classe `KNeighborsClassifier` do **scikit-learn**:","a491b4f7":"Depois vamos preench\u00ea-lo com o \u00edndice de cada dado e seu respectivo r\u00f3tulo:","bcbfeba1":"Temos um caso bem extremo de dados desbalanceados, a **grande maioria** dos nossos indiv\u00edduos \u00e9 dos Estados Unidos, 90% da quantidade total. Para efeito de compara\u00e7\u00e3o, o M\u00e9xico representa somente 2% do nosso dataset.\n\nApesar de existirem maneiras de lidar com esse tipo de problema, para simplificar optei por remover essa coluna.","3062b874":"Aplicaremos o preprocessamento:","daa56d91":"### Cross-Validation\n\nPara estimar a acur\u00e1cia do nosso modelo, vamos utilizar o m\u00e9todo chamado **\"Cross-Validation\"**, ou **\"Valida\u00e7\u00e3o Cruzada\"**. \n\n![image.png](attachment:image.png)\n\nNele, particionamos os nossos dados em k divis\u00f5es e treinamos o nosso modelo em k-1 delas, sempre deixando uma \u00faltima livre para testar o desempenho. Como repetimos esse processo k vezes, ao final temos uma boa no\u00e7\u00e3o da acur\u00e1cia esperada.","944ab586":"<a id=\"section-one\"><\/a>\n## \ud83c\udfb2 Importando os Dados\n\nAntes de come\u00e7ar a fazer qualquer an\u00e1lise, precisamos antes importar os nossos dados. Para isto, utilizamos o m\u00e9todo `read_csv` da biblioteca **pandas** com o local do nosso arquivo. Tamb\u00e9m especificaremos que a coluna 'Id' s\u00e3o os \u00edndices da nossa tabela, e que os elementos com '?' ser\u00e3o considerados valores faltantes.","b3cce0ac":"Agora a nossa coluna **income** cont\u00e9m somente valores num\u00e9ricos, com os quais poderemos trabalhar muito mais facilmente.","5ec65d7c":"Em seguida, optei por copiar nosso dataframe para um *df_analysis*, j\u00e1 que ser\u00e1 preciso fazer uma pequena mudan\u00e7a para melhor visualizar os dados.","1a7619cf":"Quando usamos o m\u00e9todo `describe` do DataFrame no in\u00edcio do notebook, percebemos que essas duas vari\u00e1veis pareciam bem esparsas e com alguns outliers. Com esses gr\u00e1ficos, essa distribui\u00e7\u00e3o fica ainda mais clara. \u00c9 importante manter esse fato em mente, depois vamos utiliz\u00e1-lo para fazer o pr\u00e9-processamento dos nossos dados.","3cb7488a":"Nosso array foi completado com o dado **'Female'**, a moda da nossa vari\u00e1vel!\n\nEm seguida, precisaremos transformar os dados categ\u00f3ricos em num\u00e9ricos para que o nosso modelo consiga interpret\u00e1-lo. Para isto, vamos utilizar o `OneHotEncoder`.\n\nEsse encoder transforma a nossa vari\u00e1vel com N classes em N vari\u00e1veis bin\u00e1rias, indicando se o nosso dado pertence \u00e0quela classe. Com o seguinte exemplo essa ideia fica mais clara:","5d89a30e":"**\u00c9 uma diferen\u00e7a bem grande!** \n\nEnquanto **30%** dos homens ganham mais que 50k por ano, somente **10%** das mulheres ultrapassam esse total.","a460feda":"E prediremos a nossa vari\u00e1vel de classe:","38ad97c1":"<a id=\"section-three\"><\/a>\n## \ud83e\uddf9 Limpeza de Dados\n\nAgora que temos uma boa no\u00e7\u00e3o dos nossos dados, podemos come\u00e7ar a prepara\u00e7\u00e3o e a limpeza deles para facilitar o trabalho do nosso modelo. Caso queria ler um pouco mais sobre esse processo, voc\u00ea pode dar uma lida nesse texto:\n\n[Leitura Recomendada](https:\/\/medium.com\/turing-talks\/como-fazer-uma-limpeza-de-dados-completa-em-python-7abc9dfc19b8)","3d6afad2":"Agora, podemos usar essa Pipeline para aplicar os dois passos anteriores com apenas um `fit_transform`:","22b6d676":"## \ud83d\udcc4 Submiss\u00e3o\n\nPara submeter nossa predi\u00e7\u00e3o, vamos export\u00e1-la no formato de `.csv`. Para isso, primeiro vamos criar um DataFrame:","37e1a7bc":"Antes de qualquer coisa, vamos importar duas bibliotecas bem importantes para trabalhar com os nossos dados: *numpy* e *pandas*. Caso voc\u00ea n\u00e3o esteja muito familiarizado com essas bibliotecas, recomendo a leitura do seguinte texto:\n\n[Leitura Recomendada - Bibliotecas de Data Science](https:\/\/medium.com\/turing-talks\/turing-talks-6-data-science-libraries-6c2599838b3e)","b351e0d6":"#### Dados Num\u00e9ricos Esparsos\n\nPor fim, temos as colunas 'capital.gain' e 'capital.loss', que se diferem muito dos outros dados num\u00e9ricos por serem bem mais esparsas e possu\u00edrem mais outliers.\n\nPara pr\u00e9process\u00e1-las, vamos adotar os mesmos passos da Pipeline num\u00e9rica, apenas trocando o `StandardScaler` por um `RobustScaler`:","94c989f4":"Parece que a nossa melhor quantidade de vizinhos foi o pr\u00f3prio **20**!","18e57460":"Esses dois \u00faltimos gr\u00e1ficos, de **'capital.gain'** e **'capital.loss'**, certamente parecem diferir bastante dos anteriores. Quando tra\u00e7amos novamente os scatter plots, o motivo fica bem mais claro:","bb3343a9":"Podemos agora separar os nossos dados em X e Y, sendo X as nossas vari\u00e1veis independentes e Y a nossa vari\u00e1vel de classe:","19858593":"### Bayes Search\n\nPara escolher a melhor quantidade de vizinhos, tamb\u00e9m podemos utilizar outras t\u00e9cnicas de otimiza\u00e7\u00e3o de hiperpar\u00e2metros, como o **Bayes Search**! Se quiser ler mais sobre o assunto, recomendo o seguinte texto:\n\n[Leitura Recomendada - Otimiza\u00e7\u00e3o de Hiperpar\u00e2metros](https:\/\/medium.com\/turing-talks\/modelos-de-predi%C3%A7%C3%A3o-otimiza%C3%A7%C3%A3o-de-hiperpar%C3%A2metros-em-python-3436fc55016e)\n\n![Bayes Search](https:\/\/miro.medium.com\/max\/875\/1*sJ3B_jLUzuGkBbKNwZUGtQ.gif)","37ebd8aa":"Assim fica o nosso novo dataframe sem a vari\u00e1vel de classe:","da17f994":"Nosso resultado \u00e9 esse:","c94d22c3":"#### Dados Categ\u00f3ricos\n\nO primeiro passo que tomaremos \u00e9 completar os dados faltantes do nosso dataset, para o qual usaremos a classe `SimpleImputer` do **scikit-learn**.\n\nO `SimpleImputer` \u00e9 um **transformador** que preenche os dados faltantes de cada vari\u00e1vel de acordo com uma estrat\u00e9gia que podemos escolher:\n\n - `mean` - Preenche os dados faltantes com a m\u00e9dia da coluna\n - `median` - Preenche os dados faltantes com a mediana da coluna\n - `most_frequent` - Preenche os dados faltantes com a moda da coluna\n \nPara os nossos dados categ\u00f3ricos, escolheremos a estrat\u00e9gia `most_frequent`!\n\nA seguir, temos uma explica\u00e7\u00e3o do funcionamento desse Imputer:","d9778724":"Vamos fazer o mesmo para as outras vari\u00e1veis categ\u00f3ricas do nosso dataset:","430664ca":"Para analisar a distribui\u00e7\u00e3o de cada vari\u00e1vel, podemos utilizar o m\u00e9todo `distplot`:","673e2751":"Com uma busca mais robusta, obtivemos uma quantidade ideal de **26** vizinhos!\n\nAgora vamos criar um classificador KNN com esse valor:","403ed750":"<a id=\"subsection-three-one\"><\/a>\n### \u2697\ufe0f Pr\u00e9-Processamento\n\nPara processar nossos dados, vamos divid\u00ed-los em tr\u00eas partes diferentes: os dados **categ\u00f3ricos**, os dados **esparsos** e os dados **num\u00e9ricos**. Dessa forma, poderemos trabalhar com eles de maneira diferente.","9132e729":"Em seguida, vamos reverter nosso **scikit-learn** para a vers\u00e3o 0.23.0 e realizar um pequeno bugfix. Esse passo s\u00f3 \u00e9 necess\u00e1rio porque a vers\u00e3o atual do sklearn conflita com uma biblioteca que utilizaremos durante a predi\u00e7\u00e3o, a **scikit-optimize**.","c2c7b77a":"Como voc\u00ea consegue ver, temos **14** vari\u00e1veis diferentes: treze independentes e uma de classe. Essa vari\u00e1vel de classe \u00e9 o que queremos prever, a renda anual de cada pessoa (ou 'income').\n\nEssas 14 vari\u00e1veis est\u00e3o divididas entre vari\u00e1veis ***num\u00e9ricas*** e vari\u00e1veis ***categ\u00f3ricas***. As ***num\u00e9ricas***, como a quantidade de horas trabalhadas por semana (ou 'hours.per.week'), s\u00e3o representadas por n\u00fameros, o que faz com que elas sejam facilmente compreendidas pelo nosso computador. J\u00e1 as ***categ\u00f3ricas***, como o sexo de cada indiv\u00edduo (ou 'sex'), est\u00e3o divididas em categorias ('Male' e 'Female') e portanto s\u00e3o um pouco mais complicadas de se trabalhar inicialmente. Na limpeza de dados, n\u00f3s deveremos de alguma maneira transformar elas em n\u00fameros para que o nosso estimador consiga fazer c\u00e1lculos.","2e5ecc27":"Por fim, agora podemos criar a nossa **Pipeline** num\u00e9rica:","48175933":"Ao final da transforma\u00e7\u00e3o, possu\u00edmos duas colunas num\u00e9ricas diferentes! A primeira indica se o nosso indiv\u00edduo \u00e9 do sexo feminino enquanto a segunda indica se ele \u00e9 do sexo masculino. Note que apenas uma dessas colunas pode ser igual a **1** para cada linha do nosso array.","2f38353c":"Agora que entendemos os nossos transformadores categ\u00f3ricos, podemos construir a nossa **Pipeline** categ\u00f3rica. \n\nUma Pipeline nada mais \u00e9 que uma sequ\u00eancia de transformadores do scikit-learn. Dessa forma, na c\u00e9lula a seguir estaremos criando uma Pipeline com os dois passos detalhados acima:\n\n - Um SimpleImputer\n - Um OneHotEncoder","b4b66867":"Agora conseguimos perceber melhor a distribui\u00e7\u00e3o das horas semanais bem como sua rela\u00e7\u00e3o com a renda de cada pessoa. Vamos tamb\u00e9m tra\u00e7ar esse gr\u00e1fico para nossas outras vari\u00e1veis:","7d84da3b":"# \ud83d\udcc8 An\u00e1lise e Predi\u00e7\u00e3o - Dataset Adult\n\n### Bernardo Coutinho - PMR3508-2020-151\n\n### \u00cdndice\n- [\ud83c\udfb2 Importando os Dados](#section-one)\n- [\ud83d\udcc8 Visualiza\u00e7\u00e3o de Dados](#section-two)\n  - [\ud83d\udd22 Visualizando Vari\u00e1veis Num\u00e9ricas](#subsection-two-one)\n  - [\ud83d\udcca Visualizando Vari\u00e1veis Categ\u00f3ricas](#subsection-two-two)\n- [\ud83e\uddf9 Limpeza de Dados](#section-three)\n  - [\u2697\ufe0f Pr\u00e9-Processamento](#subsection-three-one)\n- [\ud83d\udd2e Predi\u00e7\u00e3o](#section-four)","ad306fa5":"#### Dados Num\u00e9ricos\n\nPara os nossos dados num\u00e9ricos, tamb\u00e9m come\u00e7aremos com o preenchimento de dados faltantes. No entanto, nesse caso vamos utilizar um outro tipo de imputer, o `KNNImputer`.\n\nEsse Imputer preenche os dados faltantes aplicando o pr\u00f3prio k-Nearest Neighbors, utilizando a coluna com os dados faltantes como vari\u00e1vel de classe.","a720b61b":"Vamos utilizar a classe **LabelEncoder** do **scikit-learn** para transformar nossa vari\u00e1vel de classe **income** em num\u00e9rica, mapeando todos os valores \"<=50K\" e \">50K\" em 0s e 1s.","0f7908f4":"#### Juntando tudo\n\nCom todas as nossas Pipelines definidas, podemos junt\u00e1-las em apenas um transformador, que utilizaremos para pr\u00e9processar o dataset. Para tal, usaremos o `ColumnTransformer`, que aplicar\u00e1 as diferentes pipelines em suas respectivas colunas:","aee9546c":"Por fim, podemos utilizar o m\u00e9todo `describe` para obter algumas estat\u00edsticas mais simples sobre os nossos dados:","9f58639c":"<a id=\"subsection-two-one\"><\/a>\n### \ud83d\udd22 Visualizando Vari\u00e1veis Num\u00e9ricas\n\nComo primeiro gr\u00e1fico, vamos construir um **heatmap** representando a correla\u00e7\u00e3o entre as vari\u00e1veis num\u00e9ricas do nosso DataFrame:","351a925e":"Em seguida, podemos dar uma primeira olhada nos nossos dados com o m\u00e9todo `head` do nosso dataframe.","fdb1876b":"Agora, podemos ver como ficou o nosso dataframe:"}}