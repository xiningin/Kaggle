{"cell_type":{"492211c4":"code","00dd502f":"code","25debd02":"code","c9e348bf":"code","1b40add6":"code","fe3aa6e0":"code","476e760c":"code","5f70c3d2":"code","92cdd38a":"code","c0531517":"code","ba027836":"code","1f4c5e5a":"code","1bf20959":"code","3c3d9bde":"code","bc30446b":"code","a3f5c3e9":"code","4eebe988":"code","2871dbea":"code","70e762cc":"code","9e98beb7":"code","49b94018":"code","f25dd45f":"code","a18c8447":"code","1de4d638":"code","f8b8c815":"markdown","335b6037":"markdown","0576f3fb":"markdown","288cdc24":"markdown","19de9b40":"markdown","29c8ddda":"markdown","2ad449eb":"markdown","6d15c7fe":"markdown","67c18c1e":"markdown","a873ea0f":"markdown","51412f19":"markdown","da8c67bf":"markdown","ad2a5875":"markdown"},"source":{"492211c4":"import tensorflow as tf\nimport numpy as np\nfrom tensorflow.keras.datasets import imdb","00dd502f":"number_of_words = 20000\nmax_len = 100","25debd02":"(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=number_of_words)","c9e348bf":"X_train.shape","1b40add6":"X_train","fe3aa6e0":"X_train[0]","476e760c":"y_train","5f70c3d2":"len(X_train[0])","92cdd38a":"len(X_train[5])","c0531517":"X_train = tf.keras.preprocessing.sequence.pad_sequences(X_train, maxlen=max_len)","ba027836":"len(X_train[0])","1f4c5e5a":"len(X_train[1])","1bf20959":"X_test = tf.keras.preprocessing.sequence.pad_sequences(X_test, maxlen=max_len)","3c3d9bde":"model = tf.keras.Sequential()","bc30446b":"X_train.shape[1]","a3f5c3e9":"model.add(tf.keras.layers.Embedding(input_dim=number_of_words, output_dim=128, input_shape=(X_train.shape[1],)))","4eebe988":"model.add(tf.keras.layers.LSTM(units=128, activation='tanh'))","2871dbea":"model.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))","70e762cc":"model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])","9e98beb7":"model.summary()","49b94018":"model.fit(X_train, y_train, epochs=3, batch_size=128)","f25dd45f":"test_loss, test_acurracy = model.evaluate(X_test, y_test)","a18c8447":"print(\"Test accuracy: {}\".format(test_acurracy))","1de4d638":"test_loss","f8b8c815":"## Building the Recurrent Neural Network","335b6037":"# If you find this notebook useful, support with an upvote \ud83d\udc4d","0576f3fb":"### Adding the embedding layer","288cdc24":"### Training the model","19de9b40":"### Adding the LSTM layer\n\n-: 128\n- activation: tanh","29c8ddda":"### Compiling the model","2ad449eb":"### Filling the sequences (texts) to have the same size","6d15c7fe":"### Defining the model","67c18c1e":"# Processing","a873ea0f":"# Import from libraries","51412f19":"### Loading IMDB database","da8c67bf":"### Evaluating the model","ad2a5875":"### Adding the output layer\n\n-: 1\n- activation: sigmoid"}}