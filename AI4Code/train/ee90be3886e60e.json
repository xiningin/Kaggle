{"cell_type":{"efd3bb77":"code","c29e8767":"code","4e98bffb":"code","8f376c42":"code","81f37ffe":"code","7a9eecc1":"code","e5ec13a9":"code","46798e71":"code","95eb101b":"code","08ac49cd":"code","3b55cdde":"code","d4003801":"code","f05f9c91":"code","630542c0":"code","b9b4ccb4":"code","7f1f8aa5":"code","a4578d10":"code","99a88223":"code","12417489":"code","71b81ecc":"code","441980e9":"code","e431e768":"code","2be2bd96":"markdown","c1b53c9d":"markdown","9a068adf":"markdown","ca57d5b2":"markdown","2e4e26cb":"markdown","db332127":"markdown","312888e5":"markdown","f393e881":"markdown"},"source":{"efd3bb77":"import numpy as np\nimport pandas as pd","c29e8767":"label = ['P', 'NP', 'P','NP', 'NP', 'NP', 'P','NP', 'NP', 'NP', 'NP','NP', 'P', 'NP', 'P','NP', 'P', 'NP', 'P','NP', 'P', 'NP', 'P','NP','P', 'NP', 'NP','NP', 'P', 'NP', 'P', 'NP', 'P','NP', 'P', 'NP', 'NP','NP', 'P', 'NP', 'P','NP',  'P', 'NP', 'NP','P', 'P', 'P', 'P','P', 'P', 'NP', 'P','NP','P', 'NP', 'P','NP', 'P', 'NP']\n\nsex = ['M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'F', 'M', 'F', 'M', 'F', 'M', 'F', 'M', 'F', 'M', 'F', 'F', 'F', 'F', 'F','M', 'F', 'F', 'F', 'F', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'F', 'M', 'F', 'M', 'F', 'M', 'F', 'M', 'F', 'M', 'F', 'F', 'F', 'F', 'F']\n\ncls = ['IX', 'IX', 'IX', 'IX', 'IX', 'IX', 'IX', 'IX', 'IX', 'IX', 'IX', 'IX', 'X', 'IX', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'IX', 'IX', 'IX', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'IX', 'IX', 'IX', 'X', 'IX', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'IX']\n\nheight = [5, 6, 5, 6, 5, 6, 5, 6,5, 6, 5, 6,6,6, 5, 5, 6, 5, 6, 5, 6, 5, 6,5, 6, 5, 6,6,6, 5,5, 6, 5, 6, 5, 6, 5, 6,5, 6, 5, 6,6,6, 5,5, 6, 5, 6, 5, 6, 5, 6,5, 6, 5, 6,6,6, 5]\n\nweight = [50, 58, 50, 58, 50, 58, 50, 58, 50, 58, 50, 58, 50, 58, 50, 58, 50, 58, 50, 58,50, 58, 50, 58, 50, 58, 50, 58, 50, 58,50, 58, 50, 58, 50, 58, 50, 58, 50, 58,50, 58, 50, 58, 50, 58, 50, 58, 50, 58,50, 58, 50, 58, 50, 58, 50, 58, 50, 58]\n\n# Data frame created using the above list\ndf = pd.DataFrame({'Weight': weight, 'Height': height, 'Class': cls, 'Sex': sex, 'label': label})","4e98bffb":"df.info()","8f376c42":"df.head() ","81f37ffe":"# We have converted the categorical variable to numerical variable\ncode = {'P': 1, 'NP': 0}\ndf['new_label'] = df['label'].map(code) \nnew_s = pd.get_dummies(df.Sex)\nnew_c = pd.get_dummies(df.Class)\ndf[new_s.columns] = new_s\ndf[new_c.columns] = new_c","7a9eecc1":"df.head()","e5ec13a9":"new_df = df[['F', 'M', 'IX', 'X', 'Weight', 'Height','new_label']]\nnew_df.head()","46798e71":"from sklearn import tree\nmodel = tree.DecisionTreeClassifier(random_state = 23)\n\nfeature = new_df.drop(['new_label'], axis = 1)\nlabel = new_df.new_label","95eb101b":"model.fit(feature, label) ","08ac49cd":"import graphviz\ngraph = tree.export_graphviz(model, out_file=None, filled=True)\ngraphviz.Source(graph) ","3b55cdde":"model.fit(feature, label) ","d4003801":"from sklearn import tree\nmodel1 = tree.DecisionTreeClassifier(min_samples_leaf = 5, random_state = 23)\nmodel1.fit(feature, label) ","f05f9c91":"import graphviz\ngraph1 = tree.export_graphviz(model1, out_file=None, filled=True)\ngraphviz.Source(graph1) \n# Did you see the change in the tree structure. Now, lets try some other parameters and see how does it affect the tree structure.","630542c0":"from sklearn import tree\nmodel2 = tree.DecisionTreeClassifier(min_samples_split = 15, random_state = 23)\nmodel2.fit(feature, label) ","b9b4ccb4":"import graphviz\ngraph2 = tree.export_graphviz(model2, out_file=None, filled=True)\ngraphviz.Source(graph2) ","7f1f8aa5":"# Did you see the change in the tree structure. \n# Now, lets try to find out the best parameter by using the grid search CV","a4578d10":"from sklearn.model_selection import GridSearchCV\n\nparam1 = {'min_samples_leaf': [2,3,4],\n         'min_samples_split': [2,3,5,10,12,14],\n         'max_depth': [2,3,4,56],\n         'criterion': ['gini', 'entropy'],\n         'max_features':[2,3,4]}\n\nCV = GridSearchCV(model, param1)\nCV.fit(feature, label)","99a88223":"best = CV.best_estimator_\nbest","12417489":"graphCV = tree.export_graphviz(best, filled=True, out_file=None)\ngraphviz.Source(graphCV)","71b81ecc":"from sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import accuracy_score\n\naccuracy_score(best.predict(feature), label)","441980e9":"cross_val_score(best, feature, label, cv=5)","e431e768":"# The accuracy score is 85% which is pretty good and cross validation score seems to be pretty consistant. \n# Hopefully this kenrel might help you to understand decsion tree better. Let me know if you have any questions.","2be2bd96":"# Decision Tree - Introduction","c1b53c9d":"This kernal is inspired by the article that was posted on Analytics Vidya. You can find the [link here](https:\/\/www.analyticsvidhya.com\/blog\/2016\/04\/complete-tutorial-tree-based-modeling-scratch-in-python\/). \n\n\n**What is decision Tree?**\n\nThe decision tree is a type of supervised machine learning algorithm that is mostly used to solve classification problems, but none the less we can also use it for regression problems. The output of the decision tree is explained in simple tree kind graph\/structure that is easy to interpret and we can actually see what is happening and this is not some kind of black-box approach. We will see the graphs in the coding section.\n\n**Common Decision Tree Terminology**\n\n**Root Node** - Also known as parent node and it represents  the entire length of the datasets and all the branches start from the root\/parent node.\n\n**Branch\/Splitter** - Branch divides the root node into sub-nodes. Also, we can say branch splits the data sets into sub datasets based on the some **algorithms**.\n\n**Decision Node** - Decision Nodes are further divided into sub-nodes. If there is no division possible, then it will be leaf-node\n\n**Leaf\/ Terminal Node**:  Nodes where no division is possible is known as Leaf Node.\n\n**Algorithms** - There are few algorithm based on which root\/decision nodes are divided into further sub-nodes. The most common algorithm is 'Gini'  & 'Entropy\n\n**Gini & Impurity** - Gini and Entropy are the measure of impurity. The basic idea behind these algorithms is to minimize the missclassification. The formulae for Gini and entropy is given below.\n\n> Gini = p^2 + q^2\n\n> Entropy = -plog2p - qlog2q ","9a068adf":"Let's understand the important parameters of the decision tree algorithm and how does it affect the algorithm and results.\n\n> **criterion**:  It is the algorithm that we have discussed above and its value is 'Gini' & 'Entropy'.\n\n> **max_depth**: The depth of the above algorithm is 4.  More depth will lead to overfitting as it will try to understand the relationship with all the samples.\n\n> **max_features**: The number of features to consider while searching for a best split. These will be randomly selected. \n\n> **max_lead_nodes**: Maximum number of end nodes or terminal nodes that a decision tree should have.\n\n> **min_samples_leaf**: After the split the minimum number of sub-samples has to be there in a sub\/terminal\/end node.\n\n> **min_samples_split**: Minumum number of samples required to carry out the split.\n\nLet's see how does these parameters effects the tree structure","ca57d5b2":"# Grid Search CV","2e4e26cb":"# Important Parameters","db332127":"# **min_samples_leaf**","312888e5":"We have manually created the datasets to explain the concepts of Decision Tree. \nLet\u2019s say we have a sample of 60 students with four variables Gender (M\/ F), Class( IX\/ X), Height (5 to 6 ft) and Weight(50\/ 58). 30 out of these 60 play cricket in leisure time. Now, I want to create a model to predict who will play cricket during leisure period? ","f393e881":"# min_samples_split"}}