{"cell_type":{"97ecec7b":"code","86f26a06":"code","d61693c9":"code","ed01d0b7":"code","23993fed":"code","84d49604":"code","873fd101":"code","89b3f3c7":"code","f2cc06bd":"markdown","7a47e3b6":"markdown","2058fc54":"markdown","e24c7740":"markdown","bbc4963b":"markdown","3114b1ed":"markdown","1b5cc697":"markdown","8f039365":"markdown","4e5b2056":"markdown","2a5064b8":"markdown"},"source":{"97ecec7b":"# data analysis\nimport numpy as np\nimport pandas as pd\n\n# visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import ListedColormap\n%matplotlib inline\n\n# machine learning\nfrom sklearn.datasets import make_moons\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","86f26a06":"# plot the decision function\n# use at the end of this notebook\n\ndef plot_decision_regions(X, y, classifier=None, resolution=0.02,ax=None):\n\n    if ax is None:\n        ax = plt.gca()\n    # setup marker generator and color map\n    markers = ('o', 'v', 's', '^', 'v')\n    colors = ('navy', 'orangered', 'lightgreen', 'red', 'blue')\n    cmap = ListedColormap(colors[:len(np.unique(y))])\n\n    # plot the decision surface\n    if classifier is not None:\n        x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n        x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n        xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution),\n                               np.arange(x2_min, x2_max, resolution))\n        Z = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T)\n        Z = Z.reshape(xx1.shape)\n        ax.contourf(xx1, xx2, Z, alpha=0.6, cmap=cmap)\n        plt.xlim(xx1.min(), xx1.max())\n        plt.ylim(xx2.min(), xx2.max())\n\n    # scatter plot\n    for idx, cl in enumerate(np.unique(y)):\n        ax.scatter(x=X[y == cl, 0], \n                    y=X[y == cl, 1],\n                    alpha=1, \n                    c=colors[idx],\n                    marker=markers[idx], \n                    label=cl, \n                    edgecolor='black')","d61693c9":"# parameters for the generated data\nNOISE = 0.3        # Data generation\nSAMPLES = 300       # Data generation\nTEST_SIZE = 0.30    # Split Train\/test\n\n# Generate the data\nX, y = make_moons(n_samples=SAMPLES, noise=NOISE, random_state=42)\nplot_decision_regions(X,y)\n\n# Split the data between train and test\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=TEST_SIZE,random_state=42)\n\n# parameters of RandomForest (max_depth,n_estimators)\nparameters=np.array([[2,1],[2,10],[2,100],[5,1],[5,10],[5,100],[15,1],[15,10],[15,100]])\n\n# init\nmodels=[]\ntrain_scores=[]\ntest_scores=[]","ed01d0b7":"# Train the models for each instance of parameters\nfor params in parameters:    \n    # train\n    forest = RandomForestClassifier(max_depth = params[0], n_estimators = params[1], random_state = 42)    \n    model = forest.fit(X_train, y_train)\n    # save the model and the scores\n    train_score = accuracy_score(y_train,model.predict(X_train))\n    test_score = accuracy_score(y_test,model.predict(X_test))\n    models.append(model)\n    train_scores.append(train_score)\n    test_scores.append(test_score)\n    # print scores\n    print(\"max_depth={} \\t n_estimators={} \\t train_score={:.4f} \\t test_score={:.4f}\".format(params[0],params[1],train_score,test_score))","23993fed":"max_depth = np.unique(parameters[:,0])\nn_estimators  = np.unique(parameters[:,1])\ntrain_scores_2d = np.around(np.array(train_scores).reshape(3,3),decimals=4)\ntest_scores_2d =  np.around(np.array(test_scores).reshape(3,3),decimals=4)\n\ndef plot_heap_map(X,x_params,y_params,title,ax):\n    #fig, ax = plt.subplots()\n    im = ax.imshow(X)\n    ax.set_xticks(np.arange(len(x_params)))\n    ax.set_yticks(np.arange(len(y_params)))\n    ax.set_xticklabels(x_params)\n    ax.set_yticklabels(y_params)\n    for i in range(len(x_params)):\n        for j in range(len(y_params)):\n            text = ax.text(j, i, X[i, j],\n                           ha=\"center\", va=\"center\", color=\"r\")\n    ax.set_title(title)\nfig, axes = plt.subplots(1, 2, figsize=(15, 15))\nplot_heap_map(train_scores_2d,n_estimators,max_depth,'train_scores',axes[0])\nplot_heap_map(test_scores_2d,n_estimators,max_depth,'test_scores',axes[1])\nplt.show()","84d49604":"# Plot the result    \ndef plot_all_decision_regions(X,y):    \n    fig, axes = plt.subplots(3, 3, figsize=(15, 15))\n    for i, a in zip(range(9), axes.ravel()):\n        a.set_title(\"max_depth={} n_estimators={}\".format(parameters[i][0],parameters[i][1]))\n        plot_decision_regions(X, y,classifier=models[i],resolution=0.02,ax=a)\n    plt.show()\n","873fd101":"plot_all_decision_regions(X_train,y_train)","89b3f3c7":"plot_all_decision_regions(X_test,y_test)","f2cc06bd":"## Plot the heat map of the scores","7a47e3b6":"# Overfitting vs Underfitting with RandomForest\n\n> The purpose of this notebook is to illustrate the tradeoff between Overfitting vs Underfitting.\n>\n> The model is trained on a collection of parameters, from the least complex to the most complex\n\n**We observe the effect of the increase in the complexity of the model on the scores of the training and test data**\n","2058fc54":"> The more complex the model, the better the score on the training data\n\n> But if the model is too complex, the score of the test data becomes worse\n\n**The best compromise is for the parameter couple max_depth = 5 , n_estimators = 10**","e24c7740":"### Plot the decision surface for the train data","bbc4963b":"## Prerequisites\n### Imports","3114b1ed":"## Plot the decision surface for each model \nWe can visualize what is an overfit model or an underfit model by plotting the decision surfaces","1b5cc697":"### Data generation\nThe function make_moons from sklearn.datasets generates two interleaving half circles","8f039365":"## Train the model with RandomForestClassifier for each couple of parameters","4e5b2056":"### Plot the decision surface for the test data","2a5064b8":"### Utility function : plot the decision surface\nThis code is largely inspired by the code found in scikit-learn.org\n\nhttps:\/\/scikit-learn.org\/stable\/auto_examples\/tree\/plot_iris_dtc.html#sphx-glr-auto-examples-tree-plot-iris-dtc-py "}}