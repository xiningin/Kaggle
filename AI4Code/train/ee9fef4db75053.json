{"cell_type":{"3497910c":"code","22c8779a":"code","4ffa378a":"code","c8b2ec55":"code","40def5f1":"code","297ff37f":"code","cf34f0ca":"code","4bbf4d71":"code","cf9eacf1":"code","3559209d":"markdown","1c9a852b":"markdown","b352d8f6":"markdown","e4df7195":"markdown","9df97319":"markdown","64c7b0eb":"markdown","1dc5109c":"markdown","797c0bae":"markdown","125e35ce":"markdown","bf810aa6":"markdown","d24523e8":"markdown"},"source":{"3497910c":"import numpy as np\nimport pandas as pd\nfrom matplotlib import pyplot as plt\n\nTRAIN_DATA_PATH = \"\/kaggle\/input\/mnist-in-csv\/mnist_train.csv\"\nTEST_DATA_PATH = \"\/kaggle\/input\/mnist-in-csv\/mnist_test.csv\"\n\nraw_train_data = pd.read_csv(TRAIN_DATA_PATH)\nraw_test_data = pd.read_csv(TEST_DATA_PATH)","22c8779a":"print(f\"Train Set Shape: {raw_train_data.shape}, Missing Data: {raw_train_data.isnull().values.any()}\")\nprint(f\"Test Set Shape: {raw_test_data.shape}, Missing Data: {raw_test_data.isnull().values.any()}\\n\")\n\nprint(raw_train_data.info())\n\nraw_train_data.head(3)","4ffa378a":"# create a temporary DataFrame for visualization purposes\nplot_ten_df = raw_train_data.drop(\"label\", axis=1).iloc[0:10, :]\nplt.rcParams['figure.figsize'] = [15, 15]\n\n# visualize the first 10 digits in the train set \nfor index in range(10):\n    plt.subplot(1, 10, index+1)\n    # reshape pixel arragement to 28 x 28\n    digit_array = np.asarray(plot_ten_df.iloc[index]).reshape(28, 28)\n    plt.imshow(digit_array, cmap=\"binary\")\n    plt.title(raw_train_data[\"label\"].iloc[index], fontsize=16)\n    plt.axis(\"off\")","c8b2ec55":"# separate the pixels and the label\n# cast int pixels to float\nX_train = np.array(raw_train_data.drop(\"label\", axis=1)).astype(float)\ny_train = np.array(raw_train_data[\"label\"])\n\nX_test = np.array(raw_test_data.drop(\"label\", axis=1)).astype(float)\ny_test = np.array(raw_test_data[\"label\"])\n\n# divide by 255 to normalize \n# reshape arrays to 28 x 28 to match the pixel format\nX_train = (X_train \/ 255).reshape(60000, 28, 28)\nX_test = (X_test \/ 255).reshape(10000, 28, 28) ","40def5f1":"from sklearn.model_selection import train_test_split\n\n# random_state=42 for reproducibility\nX_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.15, random_state=42)\n\n# verify the set sizes are as expected\nprint(f\"X_train Shape: {X_train.shape}, y_train Shape: {y_train.shape}\")\nprint(f\"X_val Shape: {X_val.shape}, y_val Shape: {y_val.shape}\")\nprint(f\"X_test Shape: {X_test.shape}, y_test Shape: {y_test.shape}\")","297ff37f":"import tensorflow as tf\n\n# random_seed=42 for reproducibility\nnp.random.seed(42)\ntf.random.set_seed(42)\n\n# build the MLP architecture\nmlp_model = tf.keras.models.Sequential([\n                tf.keras.layers.Flatten(input_shape=[28, 28], name=\"input_layer\"),\n                tf.keras.layers.Dense(150, activation=\"relu\", name=\"hidden_layer1\"),\n                tf.keras.layers.Dense(100, activation=\"relu\", name=\"hidden_layer2\"),\n                tf.keras.layers.Dense(50, activation=\"relu\", name=\"hidden_layer3\"),\n                tf.keras.layers.Dense(10, activation=\"softmax\", name=\"output_layer\")\n])\n\n# compile MLP model\nmlp_model.compile(loss=\"sparse_categorical_crossentropy\",\n                  optimizer=\"sgd\",\n                  metrics=[\"accuracy\"])\n\n# display a breakdown of the MLP model\nmlp_model.summary()","cf34f0ca":"# define an early stopping callback, monitoring the validation cross-entropy loss function\nval_stop = tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=10, restore_best_weights=True)\n\n# train the MLP\ntraining_progress = mlp_model.fit(X_train, y_train, epochs=1000,\n                                  validation_data=(X_val, y_val),\n                                  callbacks=[val_stop])","4bbf4d71":"# define a function to plot the MLP training history\ndef training_plots(training_progress: dict):\n    \n    plt.rcParams['figure.figsize'] = [20, 8]\n    \n    plt.subplot(1, 2, 1)\n    plt.plot(training_progress[\"accuracy\"], \"g\", label=\"Train Accuracy\")\n    plt.plot(training_progress[\"val_accuracy\"], \"b\", label=\"Validation Accuracy\")\n    plt.title(\"MNIST MLP Accuracy Plot\", fontsize=16)\n    plt.xlabel(\"Epoch\", fontsize=16)\n    plt.ylabel(\"Accuracy\", fontsize=16)\n    plt.legend(fontsize=16)\n    \n    plt.subplot(1, 2, 2)\n    plt.plot(training_progress[\"loss\"], \"g\", label=\"Train Loss\")\n    plt.plot(training_progress[\"val_loss\"], \"b\", label=\"Validation Loss\")\n    plt.title(\"MNIST MLP Cross-Entropy Loss Plot\", fontsize=16)\n    plt.xlabel(\"Epoch\", fontsize=16)\n    plt.ylabel(\"Loss\", fontsize=16)\n    plt.legend(fontsize=16)\n    \ntraining_plots(training_progress.history)","cf9eacf1":"test_accuracy = mlp_model.evaluate(X_test, y_test, verbose=0)[1]\n\nfinal_results = pd.DataFrame({\"Model\": [\"Multilayer Perception (MLP)\"],\n                              \"Train Accuracy\": [0.9985],\n                              \"Validation Accuracy\": [0.9760],\n                              \"Test Accuracy\": [test_accuracy]})\n\nprint(final_results.to_string(index=False))","3559209d":"Gain a basic understanding of the data.","1c9a852b":"The test accuracy is quite close to the validation accuracy, as expected, since no hyperparameter tuning was performed. If we did perform hyperparameter tuning, the MLP would use the hyperparameters that perform the best on the validation set which would likely result in a decreased performance on the test set. \n\nTo conclude, the simple MLP achieved ~97.7% accuracy on the MNIST dataset.","b352d8f6":"The train and test sets contain 60,000 and 10,000 entries, respectively. Importantly, there are no missing data points. A quick glance at the first 5 entries of the train set show that 1\/785 columns pertains to the label and the other 784\/785 pertains to the pixels of the MNIST digit. Finally, the only datatype in the dataset is \"int64\". \n\nLet's next visualize some of the MNIST digits.","e4df7195":"Next, let's plot the model training progression.","9df97319":"We are now ready to construct the neural network! The Keras API will be be used to build the MLP.","64c7b0eb":"Next, let's separate the pixels and the label from the train and test set. The train set will also be split to generate a validation set.","1dc5109c":"Key features of the MLP:\n\n1) Input layer 28 x 28 to match the MNIST pixel format\n\n2) Output layer has 10 neurons as there are 10 possible MNIST digits --> [0-9]\n\nThe MLP is now ready for training! Let's define an early stopping callback that monitors the validation set cross entropy loss function. If the cost doesn't decrease after 10 epochs, stop model training as further improvements are unlikely. By using this callback, we can set the number of epochs during model training to a large number, mitigating concerns over underfitting\/overfitting as there is now a built-in stopping mechanism. ","797c0bae":"# MNIST MLP Classifier\n\n\n## Multiclass Classifier\n\nThis Jupyter notebook builds a simple Multilayer Perceptron (MLP) classifier using Keras for the MNIST dataset. \n\n## Dataset\n\nhttps:\/\/www.kaggle.com\/oddrationale\/mnist-in-csv\n\n\n## Objective\n\nUse the MNIST dataset to build a MLP for classification.","125e35ce":"As expected, each entry in the dataset maps to a 28 x 28 pixel handwritten digit. The corresponding labels are already encoded and represent the digit number.\n\n\nAt the moment, each pixel is represented by an \"int64\" with values [0-255]. We will use Gradient Descent for MLP training which is sensitive to feature scales. Therefore, let's normalize the pixel values to speed up Gradient Descent convergence.","bf810aa6":"Import standard data processing and visualization libraries.","d24523e8":"Key Observations:\n\n1) Accuracy on the train set almost reaches 1\n\n2) Accuracy on the validation set is lower than the train set, as expected, but is not significantly worse\n\n3) The callback mechanism worked as intended as val_loss plateaus at ~20 epochs\n\nLet's now evaluate the MLP on the test set."}}