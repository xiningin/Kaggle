{"cell_type":{"8721fd9c":"code","413964d5":"code","77f466f1":"code","b290070a":"code","2e4fa3c1":"code","9ab133ee":"code","4b87d09a":"code","9f584ccd":"code","efa1492a":"code","b965eac8":"code","a4af4aab":"code","d230d579":"code","cdba6f3d":"code","57c11cb6":"code","52bc403d":"code","81b2fc52":"code","f9555761":"code","14f66a76":"code","cbca1e9c":"code","9d13802c":"code","abc2ae47":"code","714e29a0":"code","70500288":"code","10a94be4":"code","dfba428d":"code","98388a1e":"code","b767bc8c":"code","353ec3f0":"code","cd60ffa5":"code","d7abd741":"code","54465d3b":"code","3438a1f3":"code","7fb77afe":"code","a0e8706d":"code","192a28dd":"code","df6f1aef":"code","4100c133":"code","d96a58c6":"code","29c40cff":"code","4eeaed4a":"code","7cd12aa8":"code","ff8b7ece":"code","d3a0c9b3":"code","a10ea51a":"code","04bbf07a":"code","73dc9ed8":"code","70488091":"code","44414d99":"markdown","4dc80cfb":"markdown","5bfa1f54":"markdown","30a5dc19":"markdown","aa5b4ed0":"markdown","92c1fee6":"markdown","798f81d0":"markdown","2e224264":"markdown","f4c43b1c":"markdown","80ea4bbb":"markdown","ff68f901":"markdown","c1e6befe":"markdown","a6e8891d":"markdown","82bb9159":"markdown","fa4c0097":"markdown","0e338e92":"markdown","21cffab6":"markdown"},"source":{"8721fd9c":"import datetime\nimport catboost\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport lightgbm as lgb\nimport tensorflow as tf\nfrom catboost import Pool\nfrom tensorflow import keras\nfrom datetime import timedelta\nfrom tqdm.notebook import tqdm\nimport matplotlib.pyplot as plt\nfrom tensorflow.keras import layers\nfrom sklearn.impute import KNNImputer\nfrom sklearn.model_selection import KFold\nfrom decimal import Decimal, ROUND_HALF_UP\nfrom bayes_opt import BayesianOptimization\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import mean_absolute_error\nfrom catboost import CatBoost, CatBoostRegressor\nfrom sklearn.preprocessing import PowerTransformer\nfrom sklearn.model_selection import learning_curve\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import train_test_split","413964d5":"#\u6700\u5927\u8868\u793a\u884c\u6570\u306e\u6307\u5b9a\uff08\u3053\u3053\u3067\u306f200\u5217\u3092\u6307\u5b9a\uff09\npd.set_option('display.max_rows', 200)\n#\u6700\u5927\u8868\u793a\u5217\u6570\u306e\u6307\u5b9a\uff08\u3053\u3053\u3067\u306f50\u5217\u3092\u6307\u5b9a\uff09\npd.set_option('display.max_columns', 260)","77f466f1":"# \u30a8\u30dd\u30c3\u30af\u304c\u7d42\u308f\u308b\u3054\u3068\u306b\u30c9\u30c3\u30c8\u3092\u4e00\u3064\u51fa\u529b\u3059\u308b\u3053\u3068\u3067\u9032\u6357\u3092\u8868\u793a\nclass PrintDot(keras.callbacks.Callback):\n  def on_epoch_end(self, epoch, logs):\n    if epoch % 100 == 0: print('')\n    print('.', end='')","b290070a":"#\u30b9\u30b1\u30fc\u30ea\u30f3\u30b0\ndef norm(sample):\n  return (sample - sample.mean()) \/ sample.std()","2e4fa3c1":"#\u5bbf\u6cca\u65bd\u8a2d\u9593\u306e\u8ddd\u96e2\u7b97\u51fa\ndef cal_epsilon(lat, lon, ipsilon):\n    num = lat.shape[0]\n    ipsilon_list = []\n    for j in tqdm(range(num)):\n        euclid = ((lat - lat[j]) ** 2 + (lon - lon[j]) ** 2) ** 0.5\n        ipsilon_list.append(len(euclid[euclid < ipsilon]))\n        \n    return np.array(ipsilon_list)","9ab133ee":"#\u5b66\u7fd2\u3001\u691c\u8a3c\u66f2\u7dda\u306e\u8868\u793a\ndef plot_history(history):\n    hist = pd.DataFrame(history.history)\n    hist['epoch'] = history.epoch\n    \n    plt.figure()\n    plt.xlabel('Epoch')\n    plt.ylabel('Mean Abs Error [price]')\n    plt.plot(hist['epoch'], hist['mae'],label='Train Error')\n    plt.plot(hist['epoch'], hist['val_mae'],label = 'Val Error')\n    \n    plt.legend()\n    \n    plt.figure()\n    plt.xlabel('Epoch')\n    plt.ylabel('Mean Square Error [price]')\n    plt.plot(hist['epoch'], hist['mse'],\n             label='Train Error')\n    plt.plot(hist['epoch'], hist['val_mse'],label = 'Val Error')\n    \n    plt.legend()\n    plt.show()","4b87d09a":"#\u30cf\u30a4\u30d1\u30e9\u30c1\u30e5\u30fc\u30cb\u30f3\u30b0\u7528\u95a2\u6570\ndef cat_hyp(depth, bagging_temperature):\n  params = {\"iterations\": 100,\n            \"learning_rate\": 0.05,\n            \"eval_metric\": \"RMSE\", #MAE -> RMSE \n            \"loss_function\": \"RMSE\", #MAE -> RMSE \n            \"verbose\": False} # Default Parameters\n  params[ \"depth\"] = int(round(depth)) \n  params[\"bagging_temperature\"] = bagging_temperature\n  \n  cat_feat = ['host_is_superhost','host_neighbourhood',\n'host_has_profile_pic','host_identity_verified','neighbourhood_cleansed',\n'property_type','room_type','bathrooms_text','instant_bookable',\n'private_bath','shared_bath'] # Categorical features list\n  cv_dataset = catboost.Pool(data=x_train_df,\n                  label=y_train_df,\n                  cat_features=cat_feat)\n\n  scores = catboost.cv(cv_dataset,\n              params,\n              fold_count=5,\n              plot=\"True\")\n\n  #print(scores)\n\n  return np.min(scores['test-RMSE-mean'])*(-1)  # \u6700\u5c0f\u5316\u3057\u305f\u3044\u306e\u3067-1\u639b\u3051\u3066test-MAE-mean\u3092\u8fd4\u3059 #MAE -> RMSE\n","9f584ccd":"#\u30e9\u30d9\u30eb\u30a8\u30f3\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0\ndef label_encoding(df, cols):\n    for c in cols:\n        if df[c].dtype == 'object':\n            lbl = LabelEncoder()\n            df[c] = lbl.fit_transform(df[c].fillna('NA').astype(str))\n    return df","efa1492a":"# \u914d\u5e03\u3055\u308c\u305f\u30c7\u30fc\u30bf\u3092\u8aad\u307f\u8fbc\u3080\ntrain_data = pd.read_csv(\"..\/input\/the-datascience-cup-beta\/train.csv\", index_col=0)\ntest_data = pd.read_csv(\"..\/input\/the-datascience-cup-beta\/test.csv\", index_col=0)\n# \u524d\u51e6\u7406\u306e\u305f\u3081\u3001\u7d50\u5408\ntotal_data = pd.concat([train_data, test_data], sort=False)\ntotal_data.head()","b965eac8":"#\u524d\u51e6\u7406\u958b\u59cb\n#\u90aa\u9b54\u306a%\u3092\u524a\u9664\ntotal_data['host_response_rate'] = total_data['host_response_rate'].replace('%', '', regex=True).astype('int', errors='ignore')\ntotal_data['host_acceptance_rate'] = total_data['host_acceptance_rate'].replace('%', '', regex=True).astype('int', errors='ignore')","a4af4aab":"#\u65e5\u6642\u7cfb\u51e6\u7406\ncols_dttm = ['host_since', 'first_review', 'last_review']\nfor c in cols_dttm:\n    total_data[c] = pd.to_datetime(total_data[c])\n    \n# first_review \u304b\u3089 last_review\u307e\u3067\u306e\u65e5\u6570\u306a\u3069\n#  \/ timedelta(days=1) \u306f\u65e5\u6570\u3092\u6570\u5024\u578b\u306b\u3059\u308b\u305f\u3081\u306e\u3082\u306e\u3067\u3059\ntotal_data['first_review'] = (total_data['first_review'] - total_data['host_since']) \/ timedelta(days=1)\ntotal_data['last_review'] = (total_data['last_review'] - total_data['host_since']) \/ timedelta(days=1)\ntotal_data['host_since'] = (total_data['host_since'] - total_data['host_since'].min()) \/ timedelta(days=1)\ntotal_data['num_days_from_first_to_last_review'] = (total_data['last_review'] - total_data['first_review'])\n#total_data['review_per_time'] = total_data['review_per_time'].replace(np.inf, 0)\n\n#\u30ec\u30b9\u30dd\u30f3\u30b9\u30bf\u30a4\u30e0\u306e\u6570\u5024\u5316\ntotal_data['host_response_time'] = total_data['host_response_time'].str.replace('within an hour', '1')\ntotal_data['host_response_time'] = total_data['host_response_time'].str.replace('within a few hours', '3')\ntotal_data['host_response_time'] = total_data['host_response_time'].str.replace('within a day', '24')\ntotal_data['host_response_time'] = total_data['host_response_time'].str.replace('a few days or more', '96')","d230d579":"# \u6d74\u5ba4\/\u6d17\u9762\u6240\u306e\u6570\u3001private\u304b\u5426\u304b\ntotal_data['num_baths'] = total_data['bathrooms_text'].str.extract(r'([0-9]+\\.?[0-9]*)').astype('float', errors='ignore')\ntotal_data['private_bath'] = total_data['bathrooms_text'].str.contains(r'private|Private') * 1\ntotal_data['shared_bath'] = total_data['bathrooms_text'].str.contains(r'shared|Shared') * 1\n\n# \u4e00\u4eba\u3042\u305f\u308a\u306e\u6d74\u5ba4\/\u6d17\u9762\u6240\u306e\u6570\u3001\u30d9\u30c3\u30c9\u306e\u6570 (\u6700\u5927\u4eba\u6570\u6642)\ntotal_data['beds_per_person'] = total_data['beds'].values \/ total_data['accommodates'].values\ntotal_data['bedrooms_per_person'] = total_data['bedrooms'].values \/ total_data['accommodates'].values\ntotal_data['baths_per_person'] = total_data['num_baths'].values \/ total_data['accommodates'].values\n\n#\u30ec\u30d3\u30e5\u30fc\u6570\u304b\u3089\u30ec\u30d3\u30e5\u30fc\u5e73\u5747\u30b9\u30b3\u30a2\u306e\u5fc3\u8a3c\u3092\u63a8\u6e2c\ntotal_data['review_feeling'] = total_data['review_scores_rating'].fillna(total_data['review_scores_rating'].dropna().mean())*np.log2(total_data['number_of_reviews']+1)**0.5","cdba6f3d":"#\u30ab\u30c6\u30b4\u30ea\u304b\u308b\u5909\u6570\u306e\u30ea\u30b9\u30c8\ncols = ['host_is_superhost', 'host_neighbourhood',\n        'host_has_profile_pic', 'host_identity_verified', 'neighbourhood_cleansed',\n        'property_type', 'room_type', 'bathrooms_text','instant_bookable', 'private_bath', 'shared_bath']","57c11cb6":"# \u8fd1\u304f\u306b\u3069\u308c\u304f\u3089\u3044Airbnb\u767b\u9332\u306e\u5bbf\u6cca\u65bd\u8a2d\u304c\u3042\u308b\u304b\ntotal_data['near_airbnb'] = cal_epsilon(total_data['latitude'], total_data['longitude'], 0.0001)","52bc403d":"#nan\u306e\u88dc\u5b8c\ntotal_imputer= pd.concat([label_encoding(total_data[cols].copy(), cols),total_data.drop(cols, axis=1)], axis=1)\ntotal_imputer2 = label_encoding(total_data, cols) #\u554f\u984c\u306f\u30e9\u30d9\u30eb\u30a8\u30f3\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0\u3067NA\u3082\u30e9\u30d9\u30eb\u5316\ntotal_imputer2 = pd.DataFrame(KNNImputer(n_neighbors=2).fit_transform(total_data))\ntotal_imputer2.columns = total_data.columns","81b2fc52":"#\u5730\u533a\u3054\u3068\u306eprice\u3001\u6700\u5c0f\u5024\u3001\u6700\u5927\u5024\u306a\u3069\u56db\u5206\u4f4d\u70b9\u3001mean\u306e\u5024\u3092\u8a08\u7b97\u3000\u203b\u4e0d\u7d30\u5de5\u306a\u30b3\u30fc\u30c9\ndistrict = total_imputer['neighbourhood_cleansed'].unique()\ntotal_imputer['mid_price'] = np.zeros(total_imputer.shape[0])\ntotal_imputer['mean_price'] = np.zeros(total_imputer.shape[0])\ntotal_imputer['min_price'] = np.zeros(total_imputer.shape[0])\ntotal_imputer['max_price'] = np.zeros(total_imputer.shape[0])\ntotal_imputer['q25_price'] = np.zeros(total_imputer.shape[0])\ntotal_imputer['q75_price'] = np.zeros(total_imputer.shape[0])\n\nfor i in range(len(district)):\n\n    mid_i_price = total_imputer[total_imputer['neighbourhood_cleansed'] == district[i]]['price'].dropna().median()\n    mean_i_price = total_imputer[total_imputer['neighbourhood_cleansed'] == district[i]]['price'].dropna().mean()\n    min_i_price = total_imputer[total_imputer['neighbourhood_cleansed'] == district[i]]['price'].dropna().min()\n    max_i_price = total_imputer[total_imputer['neighbourhood_cleansed'] == district[i]]['price'].dropna().max()\n    q25_i_price = total_imputer[total_imputer['neighbourhood_cleansed'] == district[i]]['price'].dropna().quantile(0.25)\n    q75_i_price = total_imputer[total_imputer['neighbourhood_cleansed'] == district[i]]['price'].dropna().quantile(0.75)\n\n    if(mid_i_price == np.nan):\n        mid_i_price = total_imputer['neighbourhood_cleansed'].dropna().median()\n\n    if(mean_i_price == np.nan):\n        mean_i_price = total_imputer['neighbourhood_cleansed'].dropna().mean()\n\n    if(min_i_price == np.nan):\n        min_i_price = total_imputer['neighbourhood_cleansed'].dropna().min()\n        \n    if(max_i_price == np.nan):\n        max_i_price = total_imputer['neighbourhood_cleansed'].dropna().max()\n\n    if(q25_i_price == np.nan):\n        q25_i_price = total_imputer['neighbourhood_cleansed'].dropna().quantile(0.25)\n        \n    if(q75_i_price == np.nan):\n        q75_i_price = total_imputer['neighbourhood_cleansed'].dropna().quantile(0.75)\n\n    target_ind = total_imputer[total_imputer['neighbourhood_cleansed'] == district[i]].index\n\n    total_imputer.iloc[target_ind, -6] = mid_i_price\n    total_imputer.iloc[target_ind, -5] = mean_i_price\n    total_imputer.iloc[target_ind, -4] = min_i_price\n    total_imputer.iloc[target_ind, -3] = max_i_price\n    total_imputer.iloc[target_ind, -2] = q25_i_price\n    total_imputer.iloc[target_ind, -1] = q75_i_price","f9555761":"#DNN\u7528\u306e\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u4f5c\u6210\u3002\u6975\u529b \u30ab\u30c6\u30b4\u30ea\u30ab\u30eb\u5909\u6570\u3092\u524a\u6e1b\ntf_cols = ['host_is_superhost', 'host_neighbourhood',\n           'neighbourhood_cleansed','property_type']\n\ntotal_imputer2= pd.concat([pd.get_dummies(total_imputer2[tf_cols].copy(), columns=tf_cols),\n                           total_imputer2.drop(cols, axis=1)], axis=1)","14f66a76":"#\u524d\u51e6\u7406\u7d42\u4e86\u3067\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u4f5c\u6210\ntarget = [\"price\"]\nx_train_df = total_imputer[:len(train_data)].copy()\nx_test_df = total_imputer[len(train_data):].copy()\n\ny_train_df = x_train_df[target].values\ny_test_df = x_test_df[target].values\n\n#\u5b66\u7fd2\u30c7\u30fc\u30bf\u304b\u3089\u76ee\u7684\u5909\u6570price\u3092\u524a\u9664\nx_train_df.drop(target, axis=1, inplace=True)\nx_test_df.drop(target, axis=1, inplace=True)","cbca1e9c":"#\u524d\u51e6\u7406\u7d42\u4e86\u3068\u8a00\u3063\u3066\u304a\u304d\u306a\u304c\u3089\u76ee\u7684\u5909\u6570\u306e\u5909\u63db\n\n#yeo-Johnson\u5909\u63db\u306e\u5834\u5408\u3002\u3053\u3044\u3064\u306f\u7cbe\u5ea6\u304c\u60aa\u5316\u3057\u305f\u30fb\u30fb\u30fb\u60b2\u3057\u3044\n#yeojohnson = PowerTransformer()\n#yeojohnson.fit(y_train_df)\n#y_train_df = yeojohnson.transform(y_train_df)\n\n#\u5bfe\u6570\u5909\u63db\u306e\u5834\u5408\ny_train_df[y_train_df == 0] = 1\ny_train_df = np.log(y_train_df) ","9d13802c":"#\u524d\u51e6\u7406\u7d42\u4e86\u3067\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u4f5c\u6210\nx_train_df2 = total_imputer2[:len(train_data)].copy()\nx_test_df2 = total_imputer2[len(train_data):].copy()\n\ny_train_df2 = x_train_df2[target].values\ny_test_df2 = x_test_df2[target].values\n\n#\u5b66\u7fd2\u30c7\u30fc\u30bf\u304b\u3089\u76ee\u7684\u5909\u6570price\u3092\u524a\u9664\nx_train_df2.drop(target, axis=1, inplace=True)\nx_test_df2.drop(target, axis=1, inplace=True)\n\nnormed_train_data = norm(x_train_df2)\nnormed_test_data = norm(x_test_df2)","abc2ae47":"# \u6b20\u640d\u5730\u88dc\u5b8c\nnormed_train_data.fillna(0, inplace=True)\nnormed_test_data.fillna(0, inplace=True)","714e29a0":"#\u88dc\u5b8c\u5f8c\u3001float\u5909\u6570\u306b\u306a\u3063\u3066\u3057\u307e\u3046\u30ab\u30c6\u30b4\u30ea\u5909\u6570\u3092int\u5316\ncategorical_features = ['host_is_superhost','host_neighbourhood',\n'host_has_profile_pic','host_identity_verified','neighbourhood_cleansed',\n'property_type','room_type','bathrooms_text','instant_bookable',\n'private_bath','shared_bath']\n\nfor c in categorical_features:\n    #type(x_train_df['host_neighbourhood'].values[0])\n    x_train_df[c] = x_train_df[c].astype(int)\n    x_test_df[c] = x_test_df[c].astype(int)","70500288":"# CrossValidation\u306e\u305f\u3081\u3001\u30c7\u30fc\u30bf\u5206\u5272\u306e\u6e96\u5099\nFolds = 5\ncv = KFold(n_splits=Folds, random_state=71, shuffle=True)","10a94be4":"c_train = Pool(x_train_df, label=y_train_df,cat_features=categorical_features)  \n\n# Search space\npds = {'depth': (4, 8),\n        'bagging_temperature': (3,10)\n          }\n\n# Surrogate model\noptimizer = BayesianOptimization(cat_hyp, pds, random_state=2100)\n                                  \n# Optimize\nbest_result = optimizer.maximize(init_points=10, n_iter=10)\n\nbest_no = np.argmax(pd.DataFrame(optimizer.res)['target'])\n\n#optimizer.res[best_no]['params']['bagging_temperature']\ndepth_num = int(Decimal(str(optimizer.res[best_no]['params']['depth'])).quantize(Decimal('0'), rounding=ROUND_HALF_UP))","dfba428d":"model = CatBoostRegressor(\n          loss_function='RMSE', #MAE -> MSE\n          nan_mode='Min',\n          bagging_temperature=optimizer.res[best_no]['params']['bagging_temperature']\n        )\n\ngrid = {'learning_rate': [0.01, 0.1],\n        'depth': [4, 6, 10],\n        'l2_leaf_reg': [1, 2, 3, 4, 5]}\n\nrandomized_search_result = model.randomized_search(grid,\n                                                   X=x_train_df,\n                                                   y=y_train_df,\n                                                   cv=5,\n                                                   n_iter=15,\n                                                   plot=False,\n                                                   verbose=0)","98388a1e":"# \u5165\u308c\u7269\ny_preds = [] # \u4e88\u6e2c\u7d50\u679c\u5165\u308c\u5834\nmodels = []  # \u6a5f\u68b0\u5b66\u7fd2\u30e2\u30c7\u30eb\u5165\u308c\u5834\nscores = []  # \u4e88\u6e2c\u7d50\u679c\u3092\u4f7f\u3063\u305f\u30e2\u30c7\u30eb\u306e\u6027\u80fd\u8a55\u4fa1\u7f6e\u304d\u5834\n\nmodel = CatBoostRegressor(\n          loss_function='RMSE', #MAE -> RMSE\n          iterations=5000,\n          random_strength=0.3,\n          nan_mode='Min',\n          l2_leaf_reg=randomized_search_result['params']['l2_leaf_reg'],\n          bagging_temperature=optimizer.res[best_no]['params']['bagging_temperature'],\n          depth=randomized_search_result['params']['depth'],\n          learning_rate=randomized_search_result['params']['learning_rate'],\n          early_stopping_rounds=100,\n          random_seed=77\n        )\n#depth(int) 4\uff5e10\u304c\u6700\u9069\u306b\u306a\u308b\u3053\u3068\u304c\u591a\u3044\u3002\u304a\u3059\u3059\u3081\u306f6\uff5e10\n#l2_leaf_reg(int) \u6b63\u5247\u5316\u4fc2\u6570\u3002\u6b63\u306e\u5024\u306e\u307f\u3002\u3053\u308c\u3092\u4f7f\u7528\u3059\u308b\u3068learning_rate\u304c\u81ea\u52d5\u8a2d\u5b9a\u3067\u306f\u306a\u304f\u306a\u308b\u3002\n#bagging_temperature(float)\n#has_time(bool) \u6642\u7cfb\u5217\u306a\u3069\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306b\u9806\u5e8f\u304c\u3042\u308b\u5834\u5408\u306b\u4f7f\u7528","b767bc8c":"for i, (train_ix, val_ix) in tqdm(enumerate(cv.split(x_train_df, y_train_df))):\n    # \u30c7\u30fc\u30bf\u3092\u5206\u5272\u3057\u3066\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3092\u4f5c\u308b\n    X_train_, y_train_ = x_train_df.iloc[train_ix], y_train_df[train_ix]\n    X_val, y_val = x_train_df.iloc[val_ix], y_train_df[val_ix]    \n    c_train = Pool(X_train_, label=y_train_,cat_features=categorical_features)  \n    c_valid = Pool(X_val,label=y_val, cat_features=categorical_features)\n    # \u5b66\u7fd2\uff01\n    model.fit(c_train,\n    eval_set=c_valid,\n    early_stopping_rounds=100,\n    use_best_model=True,\n    verbose=0,\n    plot=False)\n\n    print(model.tree_count_)\n    print(f\"========================{i+1}\/{Folds} done!========================\")\n\n    # \u6027\u80fd\u8a55\u4fa1\u306e\u305f\u3081\u4e88\u6e2c\n    c_test = Pool(X_val, cat_features=categorical_features)\n    y_pred = model.predict(c_test)\n    # \u4e88\u6e2c\u7d50\u679c\u3092\u3082\u3068\u306b\u30b9\u30b3\u30a2\u3092\u7b97\u51fa\n    # \u5bfe\u6570\u5909\u63db\u306e\u5834\u5408\n    score = mean_absolute_error(np.exp(y_val), np.exp(y_pred))\n\n    # yeo-Johnson\u5909\u63db\u306e\u5834\u5408\n    #score = mean_absolute_error(yeojohnson.inverse_transform(y_val), \n    #                            yeojohnson.inverse_transform(y_pred.reshape(len(y_val),1)))\n    \n    # \u7d50\u679c\u3092\u5165\u308c\u7269\u306b\u8ffd\u52a0\n    y_preds.append(y_pred)\n    scores.append(score)\n    models.append(model)\nprint(\"===========================\u5b66\u7fd2\u5b8c\u4e86!===============================\")\nprint(f'**********************CV Score is {np.array(scores).mean()}**********************')","353ec3f0":"#error\u306b\u306a\u3063\u3066\u3057\u307e\u3046\u30ab\u30c6\u30b4\u30ea\u5909\u6570\u3092float\u5316\nerror_features = ['host_response_time', 'host_response_rate', 'host_acceptance_rate']\n\nfor c in error_features:\n    x_train_df[c] = x_train_df[c].astype(np.float)\n    x_test_df[c] = x_test_df[c].astype(np.float)","cd60ffa5":"### lightgbm\u306e\u30d1\u30a4\u30d1\u30fc\u30d1\u30e9\u30e1\u30fc\u30bf\u30c1\u30e5\u30fc\u30cb\u30f3\u30b0\n# \u30d1\u30e9\u30e1\u30fc\u30bf\u7bc4\u56f2\uff08Tuple\u3067\u7bc4\u56f2\u9078\u629e\uff09\nbayes_params = {'reg_alpha': (0.0001, 0.1),\n                'reg_lambda': (0.0001, 0.1),\n                'num_leaves': (2, 40),\n                'colsample_bytree': (0.2, 1.0),\n                'subsample': (0.2, 1.0),\n                'subsample_freq': (0, 10),\n                'min_child_samples': (0, 30)\n                }\n# \u5bfe\u6570\u30b9\u30b1\u30fc\u30eb\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u5bfe\u6570\u5316\nparam_scales = {'reg_alpha': 'log',\n                'reg_lambda': 'log',\n                'num_leaves': 'linear',\n                'colsample_bytree': 'linear',\n                'subsample': 'linear',\n                'subsample_freq': 'linear',\n                'min_child_samples': 'linear'\n                }\nbayes_params_log = {k: (np.log10(v[0]), np.log10(v[1])) if param_scales[k] == 'log' else v for k, v in bayes_params.items()}\n# \u6574\u6570\u578b\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u6307\u5b9a\nint_params = ['num_leaves', 'subsample_freq', 'min_child_samples']\n\n# \u5b66\u7fd2\u6642fit\u30d1\u30e9\u30e1\u30fc\u30bf\u6307\u5b9a\nfit_params = {'verbose': 0,  # \u5b66\u7fd2\u4e2d\u306e\u30b3\u30de\u30f3\u30c9\u30e9\u30a4\u30f3\u51fa\u529b\n              'early_stopping_rounds': 10,  # \u5b66\u7fd2\u6642\u3001\u8a55\u4fa1\u6307\u6a19\u304c\u3053\u306e\u56de\u6570\u9023\u7d9a\u3067\u6539\u5584\u3057\u306a\u304f\u306a\u3063\u305f\u6642\u70b9\u3067\u30b9\u30c8\u30c3\u30d7\n              'eval_metric': 'rmse',  # early_stopping_rounds\u306e\u8a55\u4fa1\u6307\u6a19\n              'eval_set': [(x_train_df, y_train_df)]  # early_stopping_rounds\u306e\u8a55\u4fa1\u6307\u6a19\u7b97\u51fa\u7528\u30c7\u30fc\u30bf\n              }\n\nmodel=lgb.LGBMRegressor(boosting_type='gbdt', objective='regression',random_state=71, n_estimators=10000)","d7abd741":"def bayes_evaluate(**kwargs):\n    params = kwargs\n    # \u5bfe\u6570\u30b9\u30b1\u30fc\u30eb\u30d1\u30e9\u30e1\u30fc\u30bf\u306f10\u306e\u3079\u304d\u4e57\u3092\u3068\u308b\n    params = {k: np.power(10, v) if param_scales[k] == 'log' else v for k, v in params.items()}\n    # \u6574\u6570\u578b\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u6574\u6570\u5316\n    params = {k: round(v) if k in int_params else v for k, v in params.items()}\n    # \u30e2\u30c7\u30eb\u306b\u30d1\u30e9\u30e1\u30fc\u30bf\u9069\u7528\n    model.set_params(**params)\n    # cross_val_score\u3067\u30af\u30ed\u30b9\u30d0\u30ea\u30c7\u30fc\u30b7\u30e7\u30f3\n    scores = cross_val_score(model, x_train_df, y_train_df, cv=cv,\n                             scoring='neg_mean_squared_error', fit_params=fit_params, n_jobs=-1) #neg_mean_absolute_error\n    val = scores.mean()\n    return val","54465d3b":"# \u30d9\u30a4\u30ba\u6700\u9069\u5316\u3092\u5b9f\u884c\nbo = BayesianOptimization(bayes_evaluate, bayes_params_log, random_state=71)\nbo.maximize(init_points=20, n_iter=80, acq='ei')\n# \u6700\u9069\u30d1\u30e9\u30e1\u30fc\u30bf\u3068\u30b9\u30b3\u30a2\u3092\u53d6\u5f97\nbest_params = bo.max['params']\nbest_score = bo.max['target']\n# \u5bfe\u6570\u30b9\u30b1\u30fc\u30eb\u30d1\u30e9\u30e1\u30fc\u30bf\u306f10\u306e\u3079\u304d\u4e57\u3092\u3068\u308b\nbest_params = {k: np.power(10, v) if param_scales[k] == 'log' else v for k, v in best_params.items()}\n# \u6574\u6570\u578b\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u6574\u6570\u5316\nbest_params = {k: round(v) if k in int_params else v for k, v in best_params.items()}\n# \u6700\u9069\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u8868\u793a\nprint(f'\u6700\u9069\u30d1\u30e9\u30e1\u30fc\u30bf {best_params}\\n\u30b9\u30b3\u30a2 {best_score}')\n\nbest_params['colsample_bytree']\n\n# \u6700\u9069\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u5b66\u7fd2\u5668\u306b\u30bb\u30c3\u30c8\nmodel.set_params(**best_params)","3438a1f3":"# \u5165\u308c\u7269\ny_preds2 = [] # \u4e88\u6e2c\u7d50\u679c\u5165\u308c\u5834\nmodels2 = []  # \u6a5f\u68b0\u5b66\u7fd2\u30e2\u30c7\u30eb\u5165\u308c\u5834\nscores2 = []  # \u4e88\u6e2c\u7d50\u679c\u3092\u4f7f\u3063\u305f\u30e2\u30c7\u30eb\u306e\u6027\u80fd\u8a55\u4fa1\u7f6e\u304d\u5834\n\n# LightGBM\u306e\u30d1\u30e9\u30e1\u30fc\u30bf\u6e96\u5099\nparams2 = {\n    \"objective\": \"regression_l2\", # MAE -> MSE\n    \"eta\": 0.05\n}\n\nparams2.update(best_params)\n\nfor i, (train_ix, val_ix) in tqdm(enumerate(cv.split(x_train_df, y_train_df))):\n    # \u30c7\u30fc\u30bf\u3092\u5206\u5272\u3057\u3066\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3092\u4f5c\u308b\n    X_train_, y_train_ = x_train_df.iloc[train_ix], y_train_df[train_ix]\n    X_val, y_val = x_train_df.iloc[val_ix], y_train_df[val_ix]    \n    train_dataset=lgb.Dataset(X_train_, y_train_)\n    val_dataset=lgb.Dataset(X_val, y_val, reference=train_dataset)\n    evals_result = {} # \u7d50\u679c\u3092\u683c\u7d0d\u3059\u308b\u305f\u3081\u306e\u8f9e\u66f8\n    #print(X_train_)\n    # \u5b66\u7fd2\uff01\n    model=lgb.train(params2,\n                  train_dataset,\n                  valid_sets = [train_dataset, val_dataset],\n                  valid_names = ['train', 'valid'],\n                  num_boost_round=5000,\n                  early_stopping_rounds = 100,\n                  verbose_eval = 100,\n                  evals_result = evals_result)\n    print(f\"========================{i+1}\/{Folds} done!========================\")\n\n    \n    # \u6027\u80fd\u8a55\u4fa1\u306e\u305f\u3081\u4e88\u6e2c\n    y_pred = model.predict(X_val)\n    # \u4e88\u6e2c\u7d50\u679c\u3092\u3082\u3068\u306b\u30b9\u30b3\u30a2\u3092\u7b97\u51fa\n    \n    # \u5bfe\u6570\u5909\u63db\u306e\u5834\u5408\n    score = mean_absolute_error(np.exp(y_val), np.exp(y_pred))\n    # yeo-Johnson\u5909\u63db\u306e\u5834\u5408\n    #score = mean_absolute_error(yeojohnson.inverse_transform(y_val), \n    #                            yeojohnson.inverse_transform(y_pred.reshape(len(y_val),1)))\n    \n    # \u7d50\u679c\u3092\u5165\u308c\u7269\u306b\u8ffd\u52a0\n    y_preds2.append(y_pred)\n    scores2.append(score)\n    models2.append(model)\n    # \u5b66\u7fd2\u66f2\u7dda\u306e\u53ef\u8996\u5316\n    train_metric = evals_result['train']['l2']\n    eval_metric = evals_result['valid']['l2']\n    plt.plot(train_metric, label='train')\n    plt.plot(eval_metric, label='valid')\n    plt.grid()\n    plt.legend()\n    plt.xlabel('rounds')\n    plt.ylabel('score')\n    plt.show()\n\nprint(\"===========================\u5b66\u7fd2\u5b8c\u4e86!===============================\")\nprint(f'**********************CV Score is {np.array(scores2).mean()}**********************')\n","7fb77afe":"EPOCHS = 2000\n\n# patience \u306f\u6539\u5584\u304c\u898b\u3089\u308c\u308b\u304b\u3092\u76e3\u8996\u3059\u308b\u30a8\u30dd\u30c3\u30af\u6570\u3092\u8868\u3059\u30d1\u30e9\u30e1\u30fc\u30bf\u30fc\nearly_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=100)","a0e8706d":"c_train = Pool(x_train_df, label=y_train_df,cat_features=categorical_features) \n\ny_pretrains=[]\nfor model in models:\n    y_pretrain = model.predict(c_train)#\u4e88\u6e2c\n    y_pretrains.append(np.exp(y_pretrain)) #\u5bfe\u6570\u5909\u63db\u306e\u5834\u5408\n    #y_pretrains.append(yeojohnson.inverse_transform(y_pretrain.reshape(len(y_pretrain),1))) #yeo-Johnson\u306e\u5834\u5408\n\ny_precat = np.mean(y_pretrains, axis=0)\ny_precat = (y_precat - y_precat.mean()) \/ y_precat.std()\ny_precat","192a28dd":"y_pretrains2=[]\nfor model in models2:\n    y_pretrain = model.predict(x_train_df)#\u4e88\u6e2c\n    y_pretrains2.append(np.exp(y_pretrain)) #\u5bfe\u6570\u5909\u63db\u306e\u5834\u5408\n    #y_pretrains2.append(yeojohnson.inverse_transform(y_pretrain.reshape(len(y_pretrain),1))) #yeo-Johnson\u306e\u5834\u5408\n    \ny_prebgm = np.mean(y_pretrains2, axis=0)\ny_prebgm = (y_prebgm - y_prebgm.mean()) \/ y_prebgm.std()\ny_prebgm","df6f1aef":"cv = KFold(n_splits=Folds, random_state=17, shuffle=True) #\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3092\u5909\u3048\u3066leakage\u3092\u9632\u6b62\uff1f\n\nnormed_pretrain_data = pd.concat([normed_train_data, pd.DataFrame(y_precat,columns=['cat_price']), pd.DataFrame(y_prebgm,columns=['bgm_price'])], axis=1)","4100c133":"def build_model(sample):\n    model = keras.Sequential([\n        layers.Dense(16, activation='relu', kernel_initializer='he_normal', kernel_regularizer=keras.regularizers.l2(0.001), input_shape=[len(sample.keys())]),\n        keras.layers.BatchNormalization(),\n        layers.Dense(16, activation='relu', kernel_initializer='he_normal', kernel_regularizer=keras.regularizers.l2(0.001)),\n        keras.layers.BatchNormalization(),\n        layers.Dense(1)\n    ])\n\n    optimizer = tf.keras.optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=None, decay=0.0)\n\n    model.compile(loss='mse',\n                  optimizer=optimizer,\n                  metrics=['mae', 'mse'])\n    \n    return model\n\n# https:\/\/yaakublog.com\/dropout_batchnormalization\n# https:\/\/10001ideas.com\/2018\/10\/10\/batch-normalization-%E3%81%A8-dropout-%E3%81%AF%E4%BD%B5%E7%94%A8%E3%81%97%E3%81%AA%E3%81%84%E6%96%B9%E3%81%8C%E8%89%AF%E3%81%84%E3%81%A8%E3%81%84%E3%81%86%E8%A9%B1\/\n\ntf_model = None\ntf_model = build_model(pd.DataFrame(normed_pretrain_data))\ntf_model.summary()","d96a58c6":"# \u5165\u308c\u7269\ny_preds3 = [] # \u4e88\u6e2c\u7d50\u679c\u5165\u308c\u5834\nmodels3 = []  # \u6a5f\u68b0\u5b66\u7fd2\u30e2\u30c7\u30eb\u5165\u308c\u5834\nscores3 = []  # \u4e88\u6e2c\u7d50\u679c\u3092\u4f7f\u3063\u305f\u30e2\u30c7\u30eb\u306e\u6027\u80fd\u8a55\u4fa1\u7f6e\u304d\u5834\n\n# \u30c7\u30fc\u30bf\u3092\u5206\u5272\u3057\u3066\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3092\u4f5c\u308b\nX_train_, X_val, y_train_, y_val = train_test_split(normed_pretrain_data,\n                                                    y_train_df2, test_size=0.20,\n                                                    random_state=93)\n\nhistory = tf_model.fit(X_train_, y_train_, \n                        epochs=EPOCHS, batch_size=16, validation_split = 0.2,\n                        verbose=0,validation_data=(X_val, y_val),\n                        callbacks=[early_stop, PrintDot()])\n\n\n# \u6027\u80fd\u8a55\u4fa1\u306e\u305f\u3081\u4e88\u6e2c\ny_pred = tf_model.predict(X_val)\n# \u4e88\u6e2c\u7d50\u679c\u3092\u3082\u3068\u306b\u30b9\u30b3\u30a2\u3092\u7b97\u51fa\nscore = mean_absolute_error(y_val, y_pred)\n# \u7d50\u679c\u3092\u5165\u308c\u7269\u306b\u8ffd\u52a0\ny_preds3.append(y_pred)\nscores3.append(score)\nmodels3.append(tf_model)\n    \nplot_history(history)\n\nprint(f'**********************Score is {np.array(scores3)}**********************')","29c40cff":"c_test = Pool(x_test_df,cat_features=categorical_features)\n\ny_preds=[]\nfor model in models:\n    y_pred = model.predict(c_test)#\u4e88\u6e2c\n    y_preds.append(y_pred)\ny_sub = np.mean(y_preds, axis=0)\ny_sub= np.exp(y_sub) #\u5bfe\u6570\u5909\u63db\u306e\u5834\u5408\n#y_sub= yeojohnson.inverse_transform(y_sub.reshape(len(y_sub),1))\ny_sub","4eeaed4a":"y_preds2=[]\nfor model in models2:\n    y_pred = model.predict(x_test_df)#\u4e88\u6e2c\n    y_preds2.append(y_pred)\ny_sub2 = np.mean(y_preds2, axis=0)\ny_sub2= np.exp(y_sub2) #\u5bfe\u6570\u5909\u63db\u306e\u5834\u5408\n#y_sub2= yeojohnson.inverse_transform(y_sub2.reshape(len(y_sub2),1))\ny_sub2","7cd12aa8":"normed_y_sub = (y_sub - y_sub.mean()) \/ y_sub.std()\nnormed_y_sub2 = (y_sub2 - y_sub2.mean()) \/ y_sub2.std()","ff8b7ece":"normed_pretest_data = pd.concat([normed_test_data, pd.DataFrame(normed_y_sub,columns=['cat_price'], index=normed_test_data.index),\n                                 pd.DataFrame(normed_y_sub2,columns=['bgm_price'], index=normed_test_data.index)], axis=1)\nnormed_pretest_data","d3a0c9b3":"y_preds3=[]\n\nflag = 0\ni = 0\nfor model in models3:\n    y_pred = model.predict(normed_pretest_data)#\u4e88\u6e2c\n    if(scores3[i] <= 10000):\n        y_preds3.append(y_pred)\n        flag += 1\n        i += 1\n\ny_sub3 = np.mean(y_preds3, axis=0)\nflag","a10ea51a":"y_final_sub = np.zeros(len(y_sub))","04bbf07a":"for i in range(len(y_sub2)):\n    y_final_sub[i] = (y_sub[i] + y_sub2[i])\/2\n    \n    if(y_final_sub[i] <= 0):\n        y_final_sub[i] = 0\n\nif(flag > 0): \n    for i in range(len(y_sub3)):\n        y_final_sub[i] = (y_sub[i] + y_sub2[i] + y_sub3[i])\/3\n        if(y_final_sub[i] <= 0):\n            y_final_sub[i] = 0\n        \n            \ny_final_sub","73dc9ed8":"y_final_sub = y_final_sub.reshape(len(y_final_sub),)\ny_final_sub","70488091":"sub_df=pd.DataFrame({\"index\":test_data.index,\"price\":y_final_sub})\nsub_df.to_csv(\"submission.csv\",index=False)\nsub_df","44414d99":"\u5b66\u7fd2\u7528\u30c7\u30fc\u30bf\u4f5c\u6210","4dc80cfb":"# **\u4fa1\u683c\u306e\u4e88\u6e2c**\n\u6848\u30b5\u30f3\u30d7\u30eb\u306a\u3069\u3057\u3066\u4e88\u6e2c\u3057\u3066\u304f\u3002\u307e\u305a\u306fcatboost","5bfa1f54":"# **\u30c7\u30fc\u30bf\u8aad\u307f\u8fbc\u307f\u3068\u524d\u51e6\u7406**\n\u5171\u6709\u76ee\u7684\u3068\u306f\u7570\u306a\u308b\u306e\u3067\u3001\u3042\u307e\u308a\u8aac\u660e\u3057\u306a\u3044","30a5dc19":"# **Catboost\u306e\u30cf\u30a4\u30d1\u30e9\u30c1\u30e5\u30fc\u30cb\u30f3\u30b0**\ndepth\u3001learning_rate\u3001bagging_temperature\u3001l2_leaf_reg\u30922\u6bb5\u968e\u306b\u308f\u305f\u3063\u3066\u30d9\u30a4\u30ba\u6700\u9069\u5316<br>\n\u30ed\u30b0\u3068\u5b9f\u884c\u6642\u9593\u304c\u6b7b\u306c\u307b\u3069\u9577\u3044\u306e\u3067\u6ce8\u610f\u30fb\u30fb\u30fb\u3002\u4f55\u6545\u304bverbose\u304c\u52b9\u304b\u306a\u3044","aa5b4ed0":"catboost\u306eprice\u4e88\u6e2c","92c1fee6":"# **DNN\u306e\u5b66\u7fd2\u6e96\u5099**\n\u672c\u6765\u300110\u4e07\u884c\u3092\u5207\u308b\u3088\u3046\u306a\u30c7\u30fc\u30bf\u306b\u5bfe\u3057\u3066DNN\u306f\u6b63\u6c17\u306e\u6c99\u6c70\u3067\u306f\u306a\u3044\u3068\u601d\u308f\u308c\u308b\u3057<br>\n\u305d\u3082\u305d\u3082\u53ce\u675f\u3057\u306a\u3044<br>\n<br>\n\u305d\u3053\u3092\u904e\u5b66\u7fd2\u899a\u609f\u3067catboost\u3068lightgbm\u306eprice\u4e88\u6e2c\u5024\u3092\u5165\u308c\u3066<br>\n\u5f37\u5236\u7684\u306b\u5b66\u7fd2\u3092\u53ce\u675f\u3055\u305b\u308b\u7121\u8b00\u306a\u5185\u5bb9","798f81d0":"# **3\u3064\u306e\u6a5f\u68b0\u5b66\u7fd2\u30e2\u30c7\u30eb(Catboost\u3001lightGBM\u3001DNN)\u3092\u7d50\u5408\u3057\u305f\u30e2\u30c7\u30eb\u3067\u4e88\u6e2c\u3059\u308b**\nCatboost\u3068lightGBM\u3067\u4e88\u6e2c\u3057\u305fprice\u3092DNN\u306b\u5165\u308c\u3066\u3001\u66f4\u306bDNN\u3067price\u3092\u4e88\u6e2c\u3059\u308b\u30e2\u30c7\u30eb<br>\n\u203b\u6b63\u78ba\u306b\u306fCatboost\u306b\u306fKNN\u3067\u4e00\u90e8\u88dc\u5b8c\u3057\u305f\u5024\u3092\u5165\u308c\u3066\u3044\u308b\u70ba\u30014\u30e2\u30c7\u30eb\u3067\u30b9\u30bf\u30c3\u30ad\u30f3\u30b02\u56de<br>\n<br>\n\u307e\u305f\u307e\u305f\u30b4\u30ad\u30b2\u30f3\u306b\u4f5c\u3063\u305f\u3060\u3051\u3067\u3001\n\u5b9f\u306fDNN\u304c\u307e\u3063\u305f\u304f\u5f79\u306b\u7acb\u305f\u306a\u3044\uff01<br>\n\u3068\u3044\u3046\u304b\u5b66\u7fd2\u30c7\u30fc\u30bf\u6570\u304b\u3089\u3059\u308b\u3068\u7121\u8b00\u306e\u4e00\u8a00<br>\n<br>\n\u5b9f\u884c\u6642\u9593\u304c\u9577\u3044\u3060\u3051\u3067\u904e\u5b66\u7fd2\u3057\u304b\u3057\u306a\u3044\u7121\u99c4\u306a\u30b3\u30fc\u30c9\u30fb\u30fb\u30fb<br>\n<br>\n\u305f\u3060\u3001\u81ea\u5206\u3078\u306e\u6212\u3081\u3068<br>\nCatboost\u3068lightGBM\u306e\u30cf\u30a4\u30d1\u30e9\u30c1\u30e5\u30fc\u30cb\u30f3\u30b0\u306a\u3069\u306f\u53c2\u8003\u306b\u306a\u308b\u3068\u601d\u3046\u306e\u3067\u30b7\u30a7\u30a2","2e224264":"\u7d9a\u3044\u3066lightgbm","f4c43b1c":"\u4e0d\u7d30\u5de5\u306a\u30b3\u30fc\u30c9\u3060\u304c\u7d50\u679c\u3092\u30a2\u30f3\u30b5\u30f3\u30d6\u30eb<br>\nDNN\u306e\u7d50\u679c\u304c\u60aa\u3044\u6642\u306f\u4f7f\u308f\u305a\u306bcatboost\u3068lightgbm\u306e\u307f\u3067\u30a2\u30f3\u30b5\u30f3\u30d6\u30eb","80ea4bbb":"lightgbm\u306eprice\u4e88\u6e2c","ff68f901":"# **lighgbm\u306e\u30cf\u30a4\u30d1\u30e9\u30c1\u30e5\u30fc\u30cb\u30f3\u30b0**\nreg_alpha\u3001reg_lambda\u3001num_leaves\u3001colsample_bytree\u3001<br>\nsubsample\u3001subsample_freq\u3001min_child_samples\u3092\u30d9\u30a4\u30ba\u6700\u9069\u5316<br>\n<br>\n\u30ed\u30b0\u3068Warning\u304c\u6b7b\u306c\u307b\u3069\u9577\u3044\u306e\u3067\u6ce8\u610f\u30fb\u30fb\u30fb\u3002<br>\n\u4f55\u6545\u304bverbose\u304c\u52b9\u304b\u306a\u3044\u3002Warning\u306f\u76f4\u3059\u6c17\u304c\u8d77\u304d\u306a\u304b\u3063\u305f\u3002","c1e6befe":"# **DNN\u306e\u5b66\u7fd2**\n\u4e2d\u9593\u5c642\u5c64\u3067\u30cb\u30e5\u30fc\u30ed\u30f3\u308216\u500b\u3068\u30b7\u30f3\u30d7\u30eb\u306a\u30e2\u30c7\u30eb","a6e8891d":"# **lightgbm\u306e\u5b66\u7fd2\u3068\u691c\u8a3c**\ncatboost\u3068\u7570\u306a\u3063\u3066\u5b9f\u884c\u306f\u901f\u3044","82bb9159":"# **\u7d50\u679c\u306e\u63d0\u51fa**","fa4c0097":"# **Catboost\u306e\u5b66\u7fd2\u3068\u691c\u8a3c**\n\u30d1\u30a4\u30d1\u30e9\u30c1\u30e5\u30fc\u30cb\u30f3\u30b0\u3067\u6700\u9069\u5316\u3057\u305f\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u4f7f\u3063\u3066\u5b66\u7fd2\u3068\u691c\u8a3c","0e338e92":"# **\u95a2\u6570\u7fa4**\n\u4eca\u306f\u304a\u307e\u3058\u306a\u3044\u3060\u3068\u601d\u3063\u3066\u9802\u3051\u308c\u3070\u826f\u3044","21cffab6":"\u6700\u5f8c\u306bDNN"}}