{"cell_type":{"94d05ecf":"code","72dc7409":"code","415a89f1":"code","d2f45469":"code","9a15499b":"code","46ae2a8c":"code","6c09b733":"code","085b7222":"code","52642537":"code","f1d0bfa0":"code","8bb2703a":"code","7b587b28":"code","f8fdd526":"code","682cdcc1":"code","9711e63a":"code","44cfd25b":"code","4b35fbb9":"code","beac0a1b":"code","1fb3950e":"code","316246b2":"code","50a0027c":"code","83552afd":"code","08a877d2":"code","9e3dd5a0":"code","efda4dff":"code","705d3caf":"code","444d82a7":"code","802fa613":"code","dda7a783":"code","80c2aefa":"code","0958fc1f":"code","18e53c19":"code","a14c2dd5":"code","81a78ac6":"code","a85b4567":"code","4dce1949":"code","08c5a788":"code","cf18f87d":"code","e82fff10":"code","0d4729a5":"code","9ae0fd34":"code","b26ced48":"markdown","65c64fc0":"markdown","24cc9288":"markdown","3fc60f2a":"markdown","e1c52e40":"markdown","c3685f5b":"markdown","5d031ea7":"markdown","e33f1666":"markdown","1fc1b039":"markdown","05f3ae56":"markdown"},"source":{"94d05ecf":"import pandas as pd\nimport numpy as np\nimport sklearn as sc\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import f_classif\n\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import PolynomialFeatures\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.svm import SVR\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\n\nfrom sklearn import metrics\n\nfrom sklearn.model_selection import train_test_split","72dc7409":"df_train = pd.read_csv(\"\/kaggle\/input\/sales-prediction\/train.csv\")  #Import Data Set\ndf_test = pd.read_csv(\"\/kaggle\/input\/sales-prediction\/test.csv\")","415a89f1":"df_train.head()","d2f45469":"y_train = df_train['SalePrice'] # Dependent variable","9a15499b":"x_train = df_train.drop('SalePrice',1)\nx_test = df_test","46ae2a8c":"l_num_col = x_train.select_dtypes(exclude='O').columns # Assigning numeric features\nl_cat_col = x_train.select_dtypes(include='O').columns # Assigning categorical features","6c09b733":"print(x_train.shape)\nprint(x_test.shape)","085b7222":"(x_train[l_cat_col].isnull().sum()*100\/x_train.shape[0]).apply(lambda col : col>0.7) # Check if feature have null values more than 50% of total volume","52642537":"# Drop column which have missing values greater than 70\nx_train[l_cat_col].isnull().sum()*100\/x_train.shape[0] > 70 # Same as above without lambda","f1d0bfa0":"# Taking all the columns where percentage of missing values is less than 70%\nl_cat_col_shortlisted = list(filter(lambda col : x_train[col].isnull().sum()*100\/x_train.shape[0]<70, l_cat_col ))\nl_num_col_shortlisted = list(filter(lambda col : x_train[col].isnull().sum()*100\/x_train.shape[0]<70, l_num_col ))","8bb2703a":"### Fill missing values with most frequent count in categorical features\nx_train_cat = x_train[l_cat_col_shortlisted].apply(lambda x: x.fillna(x.value_counts().index[0]))\nx_test_cat = x_test[l_cat_col_shortlisted].apply(lambda x: x.fillna(x.value_counts().index[0]))","7b587b28":"print(x_test_cat.isnull().sum()) # Check null values after fill","f8fdd526":"### Fill missing values with mean in numeric features\nx_train_num = x_train[l_num_col_shortlisted].apply(lambda x: x.fillna(x.mean()))\nx_test_num = x_test[l_num_col_shortlisted].apply(lambda x: x.fillna(x.mean()))","682cdcc1":"print(x_test_num.isnull().sum()) # Check null values after fill","9711e63a":"###Check multi collinearity between independent features and remove features which have corelated value more than 0.7 i.e 70%\ncorr_matrix = x_train_num.corr().abs()\n#print(corr_matrix)\ncol_corr = set()\nfor i in range(len(corr_matrix.columns)):\n    for j in range(i):\n        #print(\"row: \"+ str(i),\"column: \" + str(j))\n        if(corr_matrix.iloc[i,j] >= 0.7) and (corr_matrix.columns[j] not in col_corr):\n            print(\"Correlated Columns are : {}, {}\".format(corr_matrix.iloc[[i]].index[0], corr_matrix.columns[j]))\n            col_name = corr_matrix.columns[i]\n            col_corr.add(col_name)\nprint(\"Columns to be delete : {}\".format(col_corr))","44cfd25b":"print(x_train_num.shape)\nx_train_num.drop(columns = col_corr, inplace=True) # Remove Multi correlation feature from data frame\nprint(x_train_num.shape)","4b35fbb9":"#Check correlation with dependent variable\ncorrelation = pd.concat([x_train_num,pd.DataFrame(y_train)],axis=1).corr().abs()","beac0a1b":"### Correlation with dependent varaible\n##Now we have to take only those variables which have high correlation with dependent varaible i.e. with sales price\nprint(correlation['SalePrice'])\nthreshold = 0.6\nshortlisted_num_features = [correlation.index[i] for i in np.where(pd.DataFrame(correlation['SalePrice']) >threshold)[0] if correlation.index[i] !='SalePrice']\nprint(\"Shortlisted columns are : {}\".format(shortlisted_num_features))\n","1fb3950e":"x_train[shortlisted_num_features].describe() # Check shortlisted data frame now","316246b2":"## From describe(), it is easily observable that we have values with different scales so need to do standardization \n## Need to starndarize the values\n# Create scaler\nscaler = preprocessing.StandardScaler()\n\n# Transform the feature\nstandardized_scaler = scaler.fit(x_train_num[shortlisted_num_features])\nx_train_standardized = standardized_scaler.transform(x_train_num[shortlisted_num_features])\nx_test_standardized = standardized_scaler.transform(x_test_num[shortlisted_num_features]) # Use same scaler to transform test data\n# Show feature\nx_train_num_stardardized = pd.DataFrame(x_train_standardized, columns = shortlisted_num_features)\nx_test_num_stardardized = pd.DataFrame(x_test_standardized, columns = shortlisted_num_features)\n\nprint(x_train_num_stardardized.shape)\nprint(x_test_num_stardardized.shape)","50a0027c":"x_train_cat.describe(include='all').loc['unique', :] #Check unique values in each categorical feature","83552afd":"x_train_enc = pd.get_dummies(x_train_cat, drop_first=True) # Assign dummy variables and drop first coulumn to avoid dummy variable trap\nx_test_enc = pd.get_dummies(x_test_cat, drop_first=True) ","08a877d2":"## THis is very important step as we will get different unique value in train and test column\n# e.g. Some values are present in train set but not available in test data set or vise versa\n# This can be handle as below .If we use same model over test set then it will throw an error in further step because number of features would be different\nprint(\"Before alignment\")\nprint(x_train_enc.shape)\nprint(x_test_enc.shape)\nx_train_enc,x_test_enc = x_train_enc.align(x_test_enc, join='outer', axis=1, fill_value=0)\n\nprint(\"After alignment\")\nprint(x_train_enc.shape)\nprint(x_test_enc.shape)","9e3dd5a0":"# Here we have categorical independent features and continous dependent varaible so use ANOVA (Analysis of varaince) method for feature selection\n# Create an SelectKBest object to select features with two best ANOVA F-Values\nfvalue_selector = SelectKBest(f_classif, k=3)\n\n# Apply the SelectKBest object to the features and target\nX_kbest = fvalue_selector.fit_transform(x_train_enc, y_train)","efda4dff":"mask = fvalue_selector.get_support() #list of booleans\nshortlisted_cat_features = [] # The list of your K best features\n\nfor bool, feature in zip(mask,  list(x_train_enc.columns.values)):\n    if bool:\n        shortlisted_cat_features.append(feature)","705d3caf":"x_train_new = pd.concat([x_train_num_stardardized, x_train_enc[shortlisted_cat_features]], axis=1)\nx_test_new = pd.concat([x_test_num_stardardized, x_test_enc[shortlisted_cat_features]], axis=1)","444d82a7":"print(x_train_new.shape)\nprint(x_test_new.shape) # Now we have 7 independent features","802fa613":"X_train,X_test,Y_train,Y_test = train_test_split(x_train_new,y_train, test_size=0.2, random_state = 1)","dda7a783":"print(X_train.shape)\nprint(X_test.shape)\nprint(Y_train.shape)\nprint(Y_test.shape)","80c2aefa":"model = LinearRegression() \nfit = model.fit(X_train,Y_train)\nY_test_predict = fit.predict(X_test)\ny_test_predict = fit.predict(x_test_new)","0958fc1f":"sns.residplot(Y_test, Y_test_predict, color=\"g\")","18e53c19":"print(\"Linear Regression\")\nprint(\"R2 Score : {}\".format(metrics.r2_score(Y_test, Y_test_predict)))\nprint('Mean Absolute Error:', metrics.mean_absolute_error(Y_test, Y_test_predict))\nprint('Mean Squared Error:', metrics.mean_squared_error(Y_test, Y_test_predict))\nprint('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(Y_test, Y_test_predict)))","a14c2dd5":"regressor = SVR()\nfit = regressor.fit(X_train,Y_train)\nY_test_predict = fit.predict(X_test)\ny_test_predict = fit.predict(x_test_new)","81a78ac6":"sns.residplot(Y_test, Y_test_predict, color=\"g\")","a85b4567":"print(\"SVR : \")\nprint(\"R2 Score : {}\".format(metrics.r2_score(Y_test, Y_test_predict)))\nprint('Mean Absolute Error:', metrics.mean_absolute_error(Y_test, Y_test_predict))\nprint('Mean Squared Error:', metrics.mean_squared_error(Y_test, Y_test_predict))\nprint('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(Y_test, Y_test_predict)))","4dce1949":"regressor = DecisionTreeRegressor(random_state = 0)\nfit = regressor.fit(X_train,Y_train)\nY_test_predict = fit.predict(X_test)\ny_test_predict = fit.predict(x_test_new)","08c5a788":"sns.residplot(Y_test, Y_test_predict, color=\"g\")","cf18f87d":"print(\"Random Forest Regression : \")\nprint(\"R2 Score : {}\".format(metrics.r2_score(Y_test, Y_test_predict)))\nprint('Mean Absolute Error:', metrics.mean_absolute_error(Y_test, Y_test_predict))\nprint('Mean Squared Error:', metrics.mean_squared_error(Y_test, Y_test_predict))\nprint('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(Y_test, Y_test_predict)))","e82fff10":"regressor = RandomForestRegressor(n_estimators=50, random_state=0)\nfit = regressor.fit(X_train,Y_train)\nY_test_predict = fit.predict(X_test)\ny_test_predict = fit.predict(x_test_new)","0d4729a5":"sns.residplot(Y_test, Y_test_predict, color=\"g\")","9ae0fd34":"print(\"Random Forest Regression : \")\nprint(\"R2 Score : {}\".format(metrics.r2_score(Y_test, Y_test_predict)))\nprint('Mean Absolute Error:', metrics.mean_absolute_error(Y_test, Y_test_predict))\nprint('Mean Squared Error:', metrics.mean_squared_error(Y_test, Y_test_predict))\nprint('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(Y_test, Y_test_predict)))","b26ced48":"Avoid above warning, this is coming because we have some rows with all 0 values during allignment step","65c64fc0":"# Models","24cc9288":"# Split Train and Test data","3fc60f2a":"# Numeric feature data preprocessing","e1c52e40":"# Decision Tree Regression","c3685f5b":"# Random Forest Regression","5d031ea7":"Create new data frame which have shortlisted column","e33f1666":"# Support vector Regression","1fc1b039":"# Categorical feature preprocessing","05f3ae56":"# Linear Regression"}}