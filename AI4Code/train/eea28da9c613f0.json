{"cell_type":{"f283d829":"code","8f2ea038":"code","1fb1d941":"code","d6220a6e":"code","645c403b":"code","79e78cbb":"code","929fd22e":"code","7131bac5":"markdown","71341a25":"markdown","354fd50c":"markdown","09a84d70":"markdown","b5b7c382":"markdown","d751f37a":"markdown"},"source":{"f283d829":"import warnings\nwarnings.filterwarnings('ignore')\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import roc_auc_score","8f2ea038":"train = pd.read_csv('..\/input\/flight_delays_train.csv')\ntest = pd.read_csv('..\/input\/flight_delays_test.csv')","1fb1d941":"train.head()","d6220a6e":"test.head()","645c403b":"X_train = train[['Distance', 'DepTime']].values\ny_train = train['dep_delayed_15min'].map({'Y': 1, 'N': 0}).values\nX_test = test[['Distance', 'DepTime']].values\n\nX_train_part, X_valid, y_train_part, y_valid = \\\n    train_test_split(X_train, y_train, \n                     test_size=0.3, random_state=17)","79e78cbb":"xgb_model = XGBClassifier(seed=17)\n\nxgb_model.fit(X_train_part, y_train_part)\nxgb_valid_pred = xgb_model.predict_proba(X_valid)[:, 1]\n\nroc_auc_score(y_valid, xgb_valid_pred)","929fd22e":"xgb_model.fit(X_train, y_train)\nxgb_test_pred = xgb_model.predict_proba(X_test)[:, 1]\n\npd.Series(xgb_test_pred, \n          name='dep_delayed_15min').to_csv('xgb_2feat.csv', \n                                           index_label='id', header=True)","7131bac5":"The second benchmark in the leaderboard was achieved as follows:\n\n- Features `Distance` and `DepTime` were taken unchanged\n- A feature `Flight` was created from features `Origin` and `Dest`\n- Features `Month`, `DayofMonth`, `DayOfWeek`, `UniqueCarrier` and `Flight` were transformed with OHE (`LabelBinarizer`)\n- Logistic regression and gradient boosting (xgboost) were trained. Xgboost hyperparameters were tuned via cross-validation. First, the hyperparameters responsible for model complexity were optimized, then the number of trees was fixed at 500 and learning step was tuned.\n- Predicted probabilities were made via cross-validation using `cross_val_predict`. A linear mixture of logistic regression and gradient boosting predictions was set in the form $w_1 * p_{logit} + (1 - w_1) * p_{xgb}$, where $p_{logit}$ is a probability of class 1, predicted by logistic regression, and $p_{xgb}$ \u2013 the same for xgboost. $w_1$ weight was selected manually.\n- A similar combination of predictions was made for test set. \n\nFollowing the same steps is not mandatory. That\u2019s just a description of how the result was achieved by the author of this assignment. Perhaps you might not want to follow the same steps, and instead, let\u2019s say, add a couple of good features and train a random forest of a thousand trees.\n\nGood luck!","71341a25":"# <center> Assignment #10 (demo)\n## <center> Gradient boosting\n\nYour task is to beat at least 2 benchmarks in this [Kaggle Inclass competition](https:\/\/www.kaggle.com\/c\/flight-delays-spring-2018). Here you won\u2019t be provided with detailed instructions. We only give you a brief description of how the second benchmark was achieved using Xgboost. Hopefully, at this stage of the course, it's enough for you to take a quick look at the data in order to understand that this is the type of task where gradient boosting will perform well. Most likely it will be Xgboost, however, we\u2019ve got plenty of categorical features here.\n\n<img src=https:\/\/habrastorage.org\/webt\/fs\/42\/ms\/fs42ms0r7qsoj-da4x7yfntwrbq.jpeg width=40% \/>","354fd50c":"We'll train Xgboost with default parameters on part of data and estimate holdout ROC AUC.","09a84d70":"Given flight departure time, carrier's code, departure airport, destination location, and flight distance, you have to predict departure delay for more than 15 minutes. As the simplest benchmark, let's take Xgboost classifier and two features that are easiest to take: DepTime and Distance. Such model results in 0.68202 on the LB.","b5b7c382":"<center>\n<img src=\"https:\/\/habrastorage.org\/files\/fd4\/502\/43d\/fd450243dd604b81b9713213a247aa20.jpg\">\n## Open Machine Learning Course\n<center>Author: [Yury Kashnitsky](https:\/\/www.linkedin.com\/in\/festline\/), Data Scientist @ Mail.Ru Group <br>All content is distributed under the [Creative Commons CC BY-NC-SA 4.0](https:\/\/creativecommons.org\/licenses\/by-nc-sa\/4.0\/) license.","d751f37a":"Now we do the same with the whole training set, make predictions to test set and form a submission file. This is how you beat the first benchmark. "}}