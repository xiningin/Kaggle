{"cell_type":{"706b2321":"code","0acfad69":"code","57932345":"code","97e81bad":"code","6186c05e":"code","ebbdbb1e":"code","0706110d":"code","26425c31":"code","aef4bb29":"code","24941f7d":"code","901314ff":"code","c30eeb57":"code","33133ed6":"code","9b757eb9":"code","2e87b208":"code","9f25b3b2":"code","889d4b59":"code","2af338a9":"code","740364be":"code","0bc6a45d":"code","541f3cef":"code","52578d20":"code","d68108f6":"code","36123032":"code","f98a0ee2":"code","efb9319f":"code","d605f9fe":"code","6664adce":"code","60f157db":"code","adb51b4d":"code","f0c17072":"code","95ca12ac":"code","9c3ff4dc":"markdown","dbd6f0f0":"markdown","3f96ef3f":"markdown","ca3f373f":"markdown","8e47a26e":"markdown","3a46c855":"markdown","8c0595b5":"markdown","cbe19f45":"markdown","cc519491":"markdown","f1370628":"markdown","a0d095da":"markdown","7d674bca":"markdown","23952598":"markdown","9bebeb48":"markdown","5450d9f8":"markdown","1ebb1b09":"markdown","94839bd6":"markdown","13408860":"markdown","fcbc2d3c":"markdown","347add39":"markdown","7c04dc48":"markdown","a9ab3005":"markdown","3dc19a71":"markdown","bf0a6c4a":"markdown","59003d5e":"markdown"},"source":{"706b2321":"# Import necessary libraries\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt   # plotting\nimport seaborn as sns   # plotting heatmap\nimport statsmodels.api as sm  # seasonal trend decomposition\nfrom statsmodels.graphics import tsaplots   # autocorrelation\n\n%matplotlib inline","0acfad69":"# Import data, convert string dates to 'datetime64' and set the date column as index:\ndf = pd.read_csv('..\/input\/test_task_data.csv',\n                 parse_dates=['date'],\n                 infer_datetime_format=True,\n                 index_col='date',\n                 thousands=',',\n                 decimal='.')","57932345":"#  Review the general info on data, paying attention to missing values and dtypes\ndf.info()","97e81bad":"# Let's remove the empty column and look at some examples of data:\ndf = df.drop(columns='Unnamed: 17')\nprint(f'data shape = {df.shape}')\ndf.head()","6186c05e":"# It appears that 'feature_5' has missing values up to 2012-10-18\n# let's fill them backwards\ndf.feature_5 = df.feature_5.fillna(method='bfill')","ebbdbb1e":"# Basic statistics of the data:\ndf.describe()","0706110d":"# Plot the time series\nplt.style.use('fivethirtyeight')\ndf.plot(subplots=True,\n        layout=(6, 3),\n        figsize=(22,22),\n        fontsize=10, \n        linewidth=2,\n        sharex=False,\n        title='Visualization of the original Time Series')\nplt.show()","26425c31":"# Let's also draw a heatmap visualization of the correlation matrix\ncorr_matrix = df.corr(method='spearman')\nf, ax = plt.subplots(figsize=(16,8))\nsns.heatmap(corr_matrix, annot=True, fmt='.2f', linewidth=0.4,\n            annot_kws={\"size\": 10}, cmap='coolwarm', ax=ax)\nplt.xticks(fontsize=10)\nplt.yticks(fontsize=10)\nplt.show()","aef4bb29":"# Run time series decomposition to extract and remove noise from training features\ndict_noise = {}\nfor ts in df.loc[:, 'feature_1':'target_value']:\n    ts_decomp = sm.tsa.seasonal_decompose(df[ts])\n    dict_noise[ts] = ts_decomp.resid\n\n# Convert to a DataFrame\ndf_noise = pd.DataFrame.from_dict(dict_noise).fillna(method='bfill')\ndf_cleaned = df.loc[:, 'feature_1':'target_value'].sub(df_noise)\ndf_cleaned.head()","24941f7d":"# Split train and test data\ntrain_features = df_cleaned.loc['2012-01-02':'2016-12-31', 'feature_1':'target_value']\ntrain_labels = df.loc['2012-01-02':'2016-12-31', 'target_class']\n\ntest_features = df.loc['2017-01-02':'2018-06-19', 'feature_1':'target_value']\ntest_labels = df.loc['2017-01-02':'2018-06-19', 'target_class']\n\n# I want to use a T-days window of input data for predicting target_class\n# It means I need to prepend (T-1) last train records to the 1st test window\nT = 30  # my choice of the timesteps window\n\nprepend_features = train_features.iloc[-(T-1):]\ntest_features = pd.concat([prepend_features, test_features], axis=0)\n\ntrain_features.shape, train_labels.shape, test_features.shape, test_labels.shape","901314ff":"# Create sequences of T timesteps (=sliding window)\n# Normalize sequences X = X\/X_0-1, where X_0 is 1st timestep in the window:\nX_train, y_train = [], []\nfor i in range(train_labels.shape[0] - (T-1)):\n    X_train.append(train_features.iloc[i:i+T].div(train_features.iloc[i]).sub(1).values)\n    y_train.append(train_labels.iloc[i + (T-1)])\nX_train, y_train = np.array(X_train), np.array(y_train).reshape(-1,1)\nprint(f'Train data dimensions: {X_train.shape}, {y_train.shape}')\n\nX_test, y_test = [], []\nfor i in range(test_labels.shape[0]):\n    X_test.append(test_features.iloc[i:i+T].div(test_features.iloc[i]).sub(1).values)\n    y_test.append(test_labels.iloc[i])\nX_test, y_test = np.array(X_test), np.array(y_test).reshape(-1,1)  \n\nprint(f'Test data dimensions: {X_test.shape}, {y_test.shape}')","c30eeb57":"# Import Keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense, LSTM, BatchNormalization\nfrom keras.optimizers import Adam\nfrom keras.callbacks import ReduceLROnPlateau, EarlyStopping\nfrom keras.regularizers import l2\nfrom time import time","33133ed6":"# Let's make a list of CONSTANTS for modelling:\nLAYERS = [16, 16, 16, 1]             # number of units in hidden and output layers\nM_TRAIN = X_train.shape[0]           # number of training examples (2D)\nM_TEST = X_test.shape[0]             # number of test examples (2D),full=X_test.shape[0]\nN = X_train.shape[2]                 # number of features\nBATCH = 256                          # batch size\nEPOCH = 50                           # number of epochs\nLR = 1e-1                            # learning rate of the gradient descent\nLAMBD = 5e-2                         # lambda in L2 regularizaion\nDP = 0.00                            # dropout rate\nRDP = 0.00                           # recurrent dropout rate\nprint(f'layers={LAYERS}, train_examples={M_TRAIN}, test_examples={M_TEST}')\nprint(f'batch = {BATCH}, timesteps = {T}, features = {N}, epochs = {EPOCH}')\nprint(f'lr = {LR}, lambda = {LAMBD}, dropout = {DP}, recurr_dropout = {RDP}')\n\n# Build the Model\nmodel = Sequential()\nmodel.add(LSTM(input_shape=(T,N), units=LAYERS[0],\n               activation='tanh', recurrent_activation='hard_sigmoid',\n               kernel_regularizer=l2(LAMBD), recurrent_regularizer=l2(LAMBD),\n               dropout=DP, recurrent_dropout=RDP,\n               return_sequences=True, return_state=False,\n               stateful=False, unroll=False))\nmodel.add(BatchNormalization())\nmodel.add(LSTM(units=LAYERS[1],\n               activation='tanh', recurrent_activation='hard_sigmoid',\n               kernel_regularizer=l2(LAMBD), recurrent_regularizer=l2(LAMBD),\n               dropout=DP, recurrent_dropout=RDP,\n               return_sequences=True, return_state=False,\n               stateful=False, unroll=False))\nmodel.add(BatchNormalization())\nmodel.add(LSTM(units=LAYERS[2],\n               activation='tanh', recurrent_activation='hard_sigmoid',\n               kernel_regularizer=l2(LAMBD), recurrent_regularizer=l2(LAMBD),\n               dropout=DP, recurrent_dropout=RDP,\n               return_sequences=False, return_state=False,\n               stateful=False, unroll=False))\nmodel.add(BatchNormalization())\nmodel.add(Dense(units=LAYERS[3], activation='sigmoid'))\n\n# Compile the model with Adam optimizer\nmodel.compile(loss='binary_crossentropy', metrics=['accuracy'],\n              optimizer=Adam(lr=LR))\nprint(model.summary())\n\n# Define a learning rate decay method:\nlr_decay = ReduceLROnPlateau(monitor='loss', patience=1, verbose=0, \n                             factor=0.5, min_lr=1e-8)\n# Define Early Stopping:\nearly_stop = EarlyStopping(monitor='val_loss', min_delta=0, \n                           patience=10, verbose=1, mode='auto',\n                           baseline=0, restore_best_weights=False)\n# Train the model. \n# The dataset is small for NN - let's use test_data for validation\nstart = time()\nHistory = model.fit(X_train, y_train, epochs=EPOCH, batch_size=BATCH,\n                    validation_split=0.2, shuffle=True,verbose=0,\n                    callbacks=[lr_decay])\nprint('-'*65)\nprint(f'Training was completed in {time() - start:.2f} secs')\nprint('-'*65)\n# Evaluate the model:\ntrain_acc = History.history['acc'][-1]    #model.evaluate(X_train, y_train, batch_size=M_TRAIN)\ntest_loss, test_acc = model.evaluate(X_test, y_test, batch_size=M_TEST)\nprint('-'*65)\nprint(f'train accuracy = {round(train_acc * 100, 2)}%')\nprint(f'fixed model test score = {round(test_acc * 100, 2)}%')\n\n# Plot the loss and accuracy curves over epochs:\nfig, axs = plt.subplots(nrows=1, ncols=2, figsize=(18,6))\naxs[0].plot(History.history['loss'], color='b', label='Training loss')\naxs[0].plot(History.history['val_loss'], color='r', label='Validation loss')\naxs[0].set_title(\"Loss curves\")\naxs[0].legend(loc='best', shadow=True)\naxs[1].plot(History.history['acc'], color='b', label='Training accuracy')\naxs[1].plot(History.history['val_acc'], color='r', label='Validation accuracy')\naxs[1].set_title(\"Accuracy curves\")\naxs[1].legend(loc='best', shadow=True)\nplt.show()\n\n# Plot predictions vs actual labels for fixed model\nindex = pd.date_range(start='2017-01-02', end='2018-06-19', freq='B')\nfixed_predict = np.round_(model.predict_on_batch(X_test))\nfixed_score = np.sum(fixed_predict == y_test) \/ y_test.shape[0]\nprint(f'Fixed model test score = {round(fixed_score*100, 2)}%')\ndff = pd.DataFrame({'predicted':fixed_predict.squeeze(), 'actual':y_test.squeeze()}, index=index)\nax = dff.plot(figsize=(12,3))\nax.set_title('Fixed model: predicted vs actual values for the target_class')\nplt.show()","9b757eb9":"# Define online model with the pre-trained weights from the fixed model\n# The main reason I reinstantiate the model is to restart decayed learning rate:\nstart = time()\nconfig, weights = model.get_config(), model.get_weights()\nonline_model = Sequential.from_config(config)\nonline_model.set_weights(weights)\nonline_model.compile(loss='binary_crossentropy', optimizer=Adam(lr=LR))\nprint(f'Online model instantiated in {time() - start:.2f} secs')\n\n# Online training (update model with each new data available):\npredictions = []\nstart = time()\nfor t in range(y_test.shape[0]):\n    x = X_test[t].reshape(-1,T,N)  # a \"new\" input is available\n    y_hat = np.round_(model.predict_on_batch(x)) # predict on the \"new\" input\n    predictions.append(y_hat)  # save predictions\n    y = y_test[t].reshape(-1,1)   # a \"new\" label is available\n    model.train_on_batch(x, y)  # runs a single gradient update \nprint(f'Online learning completed in {time() - start:.2f} secs')\n\n# Evaluation of the predictions with online learning\nonline_predict = np.array(predictions).reshape(-1,1)\nonline_score = np.sum(online_predict == y_test) \/ y_test.shape[0]\nprint(f'Online model test score = {round(online_score*100, 2)}%')\n\n# Plot predictions vs actual labels:\nindex = pd.date_range(start='2017-01-02', end='2018-06-19', freq='B')\ndfo = pd.DataFrame({'predicted':online_predict.squeeze(), 'actual':y_test.squeeze()}, index=index)\nax = dfo.plot(figsize=(12,3))\nax.set_title('Online model: predicted vs actual values for the target_class')\nplt.show()","2e87b208":"# Let's define a EDA function for repeated calls on individual time series:\ndef eda(df_name, ts_name):\n    \"\"\" \n    Inputs: df_name - name of the dataframe\n            ts_name - name of the time series in the dataframe\n    Outputs: EDA statistics and plots for individual time series in df_name\n    \"\"\"\n    # Statistics\n    print(f'Statistic of {ts_name} time series')\n    print(df_name[ts_name].describe())\n    \n    # Plotting\n    fig, axs = plt.subplots(nrows=4, ncols=2, figsize=(24,24))\n    fig.suptitle(f'Visualization of the \"{ts_name}\" time series', fontsize=24)\n        \n    # Observed values of the time series against target_class values\n    df_name[ts_name].plot(ylim=[df_name[ts_name].min(), df_name[ts_name].max()],\n                          linewidth=2, ax=axs[0,0])\n    axs[0,0].set_title('Observed values (red\/green where target_class=0\/1)')\n    axs[0,0].set_xlabel('')\n    axs[0,0].fill_between(df_name.index, df_name[ts_name], \n                          where=(df_name.target_class==0),\n                          facecolor='red', alpha=0.5)\n    axs[0,0].fill_between(df_name.index, df_name[ts_name], \n                          where=(df_name.target_class==1),\n                          facecolor='green', alpha=0.5)\n    axs[0,0].axvline('2017-01-01', color='red', linestyle='dashed')\n    \n    # Seasonality, trend and noise in time series data\n    decomp = sm.tsa.seasonal_decompose(df_name[ts_name])\n    decomp.trend.plot(linewidth=2, ax=axs[0,1])\n    axs[0,1].set_title('Trend values')\n    axs[0,1].set_xlabel('')\n    decomp.seasonal.plot(linewidth=2, ax=axs[1,0])\n    axs[1,0].set_title('Seasonal values')\n    axs[1,0].set_xlabel('')\n    decomp.resid.plot(linewidth=2, ax=axs[1,1])\n    axs[1,1].set_title('Residual values')\n    axs[1,1].set_xlabel('')\n    \n    # Distribution of values of time series\n    df_name[ts_name].plot.hist(bins=30, ax=axs[2,0])\n    axs[2, 0].set_title('Histogram')\n    df_name[[ts_name]].boxplot(ax=axs[2,1])\n    axs[2, 1].set_title('Boxplot')\n        \n    # Autocorrelation of time series\n    tsaplots.plot_acf(df_name[ts_name], lags=40, ax=axs[3,0])\n    tsaplots.plot_pacf(df_name[ts_name], lags=40, ax=axs[3,1])\n    \n    plt.tight_layout(rect=[0, 0, 1, 0.95])\n    plt.show()","9f25b3b2":"# Call EDA function to explore the time series\neda(df, 'target_value')","889d4b59":"# Call EDA function to explore the time series\neda(df, 'feature_1')","2af338a9":"# Call EDA function to explore the time series\neda(df, 'feature_2')","740364be":"# Call EDA function to explore the time series\neda(df, 'feature_3')","0bc6a45d":"# Call EDA function to explore the time series\neda(df, 'feature_4')","541f3cef":"# Call EDA function to explore the time series\neda(df, 'feature_5')","52578d20":"# Call EDA function to explore the time series\neda(df, 'feature_6')","d68108f6":"# Call EDA function to explore the time series\neda(df, 'feature_7')","36123032":"# Call EDA function to explore the time series\neda(df, 'feature_8')","f98a0ee2":"# Call EDA function to explore the time series\neda(df, 'feature_9')","efb9319f":"# Call EDA function to explore the time series\neda(df, 'feature_10')","d605f9fe":"# Call EDA function to explore the time series\neda(df, 'feature_11')","6664adce":"# Call EDA function to explore the time series\neda(df, 'feature_12')","60f157db":"# Call EDA function to explore the time series\neda(df, 'feature_13')","adb51b4d":"# Call EDA function to explore the time series\neda(df, 'feature_14')","f0c17072":"# Call EDA function to explore the time series\neda(df, 'feature_15')","95ca12ac":"# Call EDA function to explore the time series\neda(df, 'feature_16')","9c3ff4dc":"# <a name=\"feature_16\"><\/a> A-16. feature_16\n[Back to INDEX](#index)","dbd6f0f0":"## What do we have from the raw data review?\n* Exploratory Data Analysis of individual time series is visualized in the [APPENDIX](#index) below.\n* The raw data contain stochastic time series, including 'target_value'. Predicting\/ making classification based on stochastic variable values may force the model to learn the 'persistence' mode (i.e. yhat(t+1) = y(t)), resulting in little predictive power. Therefore, some sort of 'normalizing' the features time series is required. \n* The raw data are weakly correleted with the target_value and the target_class and among each other with rare exceptions. The predictive power could be in temporal effects. \n* The raw data are at different scales, therefore the normalization of the features should rescale the data to ensure efficient learning of NN models.\n* The raw data contain 'noise', which we are better to remove, at least on the training dataset for more efficient pre-training of the model. As the test dataset will be used in emulating an online learning of the model, where noise cleaning is not practical, I'm not going to remove noise from the test data.\n\nPossible nature | TS_name | Description | Transformation | Rescaling after transformation\n:---: | :-- | :-- | :-- | :--\nclass | target_class | binary (48%-1s, 52%-0s) | None | None\nstock index or indicator like RSI | [target_value](#target_value) | unimodal, bell-shaped, skewed to the right, stochastic trend with values in [48, 91], order +2 autocorr | pc_change or log diff | Standard\nstock index | [feature_1](#feature_1) | unimodal, bell-shaped, slightly skewed to the right, stochastic trend with values in [1432, 2539], order +2 autocorr | pc_change or log diff | Standard\nstock index | [feature_2](#feature_2) | bimodal, stochastic trend with values in [27, 126 ], order +2 autocorr | pc_change or log diff | Standard or MinMax\nstock index | [feature_3](#feature_3) | unimodal, strongly skewed to the right,  stochastic trend with values in [315, 830], order +2 autocorr  | pc_change or log diff | Standard or MinMax\nstock index | [feature_4](#feature_4) | unimodal, skewed to the right,  stochastic trend with values in [1, 6 ], order +2 autocorr | pc_change or log diff | Standard\ntechnical indicator | [feature_5](#feature_5) | unimodal, 5 descrete values  in [100+-0.00002], ordr +2 autocorr |  pc_change or log diff | Standard\ntechnical indicator | [feature_6](#feature_6) | unimodal, bell-shaped, ranging in [100+- 0.04], order +2 autocorr | pc_change or log diff | Standard\ntechnical indicator | [feature_7](#feature_7) | unimodal, skewed to the right, ranging in [100+-0.04], order -29\/+31 autocorr  | pc_change or log diff | Standard\ntechnical indicator | [feature_8](#feature_8) | ranging around 100, occasional spikes in range [-340, +780] - [Q1'12, Q4'16], order 2 autocorr, negative values! | pc_change | Standard or MinMax\ntechnical indicator | [feature_9](#feature_9) | ranging around 100, occasional spikes in range [-3413, +2626] - [Q1'12, Q4'16], order 2 autocorr, negative values! | pc_change | Standard or MinMax\ntechnical indicator | [feature_10](#feature_10) | ranging around 100, occasional spikes in range [-2104, +2206] - [Q1'12, Q4'16], order 2 autocorr, negative values! | pc_change | Standard or MinMax\ntechnical indicator | [feature_11](#feature_11) | ranging around 100, occasional spikes in range [-1321, +1213 ] - [Q4'12-Q1'13], order 2 autocorr, negative values! | pc_change | Standard or MinMax\ntechnical indicator | [feature_12](#feature_12) | ranging around 100, occasional spikes in range [-2933, +2462 ] - [Q4'12-Q1'13], order 2 autocorr, negative values! | pc_change | Standard or MinMax\ntechnical indicator | [feature_13](#feature_13) | ranging around 100, occasional spikes in range [-3206, +2687 ] - [Q4'12-Q1'13], order 2 autocorr, negative values! | pc_change | Standard or MinMax\ntechnical indicator (oscillator) | [feature_14](#feature_14) | unimodal, skewed to the right, ranging in  [100+-0.02], autocorr +32 | pc_change or log diff | Standard\ntechnical indicator (oscillator) | [feature_15](#feature_15) | unimodal, bell-shaped, skewed to the left, ranging in [100+-0.02], autocorr -38 | pc_change or log diff | Standard\ntechnical indicator (oscillator) | [feature_16](#feature_16) | unimodal, bell-shaped, ranging in [100+-0.01], autocorr +14 | pc_change or log diff | Standard","3f96ef3f":"# <a name=\"feature_15\"><\/a> A-15. feature_15\n[Back to INDEX](#index)","ca3f373f":"# <a name=\"1\"><\/a> 1. Load and Review Data\n------------------","8e47a26e":"# <a name=\"4\"><\/a> 4. APPENDIX - Exploratory Data Analysis for Individual Time Series\n### <a name=\"index\"><\/a> INDEX:\n\nReference | Reference | Reference\n:-- | :-- | :--\n[EDA Function](#eda_function) | [A-5. feature_5](#feature_5) | [A-11. feature_11](#feature_11) \n[A-0. target_value](#target_value) | [A-6. feature_6](#feature_6)  | [A-12. feature_12](#feature_12) \n[A-1. feature_1](#feature_1) | [A-7. feature_7](#feature_7) | [A-13. feature_13](#feature_13) \n[A-2. feature_2](#feature_2) | [A-8. feature_8](#feature_8) | [A-14. feature_14](#feature_14) \n[A-3. feature_3](#feature_3) | [A-9. feature_9](#feature_9) | [A-15. feature_15](#feature_15) \n[A-4. feature_4](#feature_4) | [A-10. feature_10](#feature_10) | [A-16. feature_16](#feature_16) ","3a46c855":"# <a name=\"feature_9\"><\/a> A-9. feature_9\n[Back to INDEX](#index)","8c0595b5":"# <a name=\"feature_5\"><\/a> A-5. feature_5\n[Back to INDEX](#index)","cbe19f45":"# <a name=\"target_value\"><\/a> A-0. target_value \n[Back to INDEX](#index)","cc519491":"<h1 id=\"eda_function\"> EDA Function <\/h1>","f1370628":"# <a name=\"feature_13\"><\/a> A-13. feature_13\n[Back to INDEX](#index)","a0d095da":"# 0. Problem description\n--------------------------\nI've got this dataset of financial time series from my freinds at TenViz who's job is the magic of predicting stock market movements. The problem was formulated as follows:\n> ... to predict *target_class* based on values of *target_variable* & available features (dataset 'test_task_data.csv'). Before building a classifier, please pay attention to the nature of features and specific aspects of working with time series. Also, you can use *target_values* to derive useful information and additional features. As a train set use dataset from 2012-01-01 till 2016-12-31, as a test set used from 2017-01-02 till 2018-06-19. Finally, evaluate your model & provide analysis with short comments.\n> The results of the work should contain:\n> * Description of the steps of the solution of the task.\n> * Runnable implementation code in Python.\n> * PDF with the charts\n\n### In this notebook I review the raw time series data and build an online learing classifier based on a stacked LSTM RNN, which can be applied to any new data going forward.\n\n## Contents:\n1. [Load and Review Data](#1)\n2. [Feature Engineering](#2)\n3. [LSTM Model - Batch Training and Predictiction](#3)\n4. [APPENDIX - EDA for Individual Time Series](#4)","7d674bca":"# <a name=\"3\"><\/a> 3. LSTM Model - Batch Training and Predictiction\n-------------------------","23952598":"# <a name=\"feature_1\"><\/a> A-1. feature_1\n[Back to INDEX](#index)","9bebeb48":"# <a name=\"2\"><\/a> 2. Feature Engineering\n-------","5450d9f8":"# <a name=\"feature_6\"><\/a> A-6. feature_6\n[Back to INDEX](#index)","1ebb1b09":"\n### Conclusion: \n* **target_value** is unimodal bell-shaped (slightly skewed to the right); \n* decomposed into trend and stochastic noise;\n* percentage change is likely to have stationarity property\n* alternative - log transformation of the variable\n* it is possible to apply z-score standartization (sklearn StandardScaler)","94839bd6":"# <a name=\"feature_3\"><\/a> A-3. feature_3\n[Back to INDEX](#index)","13408860":"# <a name=\"feature_12\"><\/a> A-12. feature_12\n[Back to INDEX](#index)","fcbc2d3c":"# <a name=\"feature_4\"><\/a> A-4. feature_4\n[Back to INDEX](#index)","347add39":"# <a name=\"feature_2\"><\/a> A-2. feature_2\n[Back to INDEX](#index)","7c04dc48":"# <a name=\"feature_7\"><\/a> A-7. feature_7\n[Back to INDEX](#index)","a9ab3005":"# <a name=\"feature_11\"><\/a> A-11. feature_11\n[Back to INDEX](#index)","3dc19a71":"# <a name=\"feature_14\"><\/a> A-14. feature_14\n[Back to INDEX](#index)","bf0a6c4a":"# <a name=\"feature_8\"><\/a> A-8. feature_8\n[Back to INDEX](#index)","59003d5e":"# <a name=\"feature_10\"><\/a> A-10. feature_10\n[Back to INDEX](#index)"}}