{"cell_type":{"7a937516":"code","4f8a7cad":"code","b2c665fa":"code","79892766":"code","8fee27d9":"code","5a819a84":"code","faae7d1e":"code","fd456cee":"code","ecf52894":"code","1ddedc2a":"code","4b9baebb":"code","414d8182":"code","af7e5f23":"code","9fa50b65":"code","6c66afa1":"code","ee1e350e":"code","9f5e62f0":"code","ce6e9621":"code","552b552f":"code","e7d7c2b9":"code","8545cdb4":"code","c40249fc":"code","7d2454f0":"code","b8b1d4df":"code","44a095d5":"code","402f6e4a":"code","6ca2e716":"code","9c3682fb":"code","7cd0f1f9":"code","36003503":"code","d94ce9eb":"code","eeb52437":"code","efb690d6":"code","6e025803":"code","4e1b519b":"code","5d679825":"code","f143c5a3":"code","fe1c42a3":"code","9f56453d":"code","0750bbee":"code","190963bf":"code","76236867":"code","9f760755":"code","48a9b43d":"code","ecf0ffa9":"code","caf58e17":"code","499b0bf9":"code","45a3ace8":"code","a9b3a076":"code","29732304":"code","11d706a5":"code","127a1a50":"code","48670565":"code","c631ae6d":"code","52274cae":"markdown","829df322":"markdown","2125bf6e":"markdown","4903a0b7":"markdown","f8a036e9":"markdown","6d253d4e":"markdown","dda1a094":"markdown","46936133":"markdown","e5961532":"markdown","e73ed950":"markdown","3f170e64":"markdown","141d7fe1":"markdown","3d63bc78":"markdown","61b82550":"markdown","90bae066":"markdown","495a08b7":"markdown","86855441":"markdown","20aa6bf2":"markdown","829fcd0c":"markdown","51a54f9a":"markdown","92a12adc":"markdown","e81218ef":"markdown","bfbb2289":"markdown","e611c1ca":"markdown","761435ca":"markdown","cff8edcd":"markdown","2455384d":"markdown","7c97c0ce":"markdown","f49f6d23":"markdown","8e70de5f":"markdown","4e5e5a25":"markdown","5923bc7d":"markdown","884b14ba":"markdown","88a950d2":"markdown","46c56f91":"markdown","c2f730ce":"markdown","75ccf02e":"markdown","2109d72a":"markdown","f05d91c3":"markdown","70889773":"markdown","e4b0dea1":"markdown","f3f4fbca":"markdown","709b0f46":"markdown","59a48734":"markdown","ba1c5d55":"markdown","893ca7b8":"markdown","ad187a3d":"markdown","5ec67db7":"markdown","ef791446":"markdown","37e78213":"markdown","647443f9":"markdown","e7082ead":"markdown","1a87027c":"markdown","85b0d09a":"markdown","0de27b6e":"markdown","6b346ae2":"markdown","e1611d79":"markdown","a5e76d50":"markdown","cb2f2082":"markdown"},"source":{"7a937516":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nfrom matplotlib.pyplot import figure\nimport cv2","4f8a7cad":"#The image paths we will use\nlena = r\"\/kaggle\/input\/opencv-samples-images\/data\/lena.jpg\"\nchessboard = r\"\/kaggle\/input\/opencv-samples-images\/data\/chessboard.png\"\nminnions = r\"\/kaggle\/input\/opencv-samples-images\/minions.jpg\"\nbutterfly = r\"\/kaggle\/input\/opencv-samples-images\/data\/butterfly.jpg\"\nhome = r\"\/kaggle\/input\/opencv-samples-images\/data\/home.jpg\"\nbasketball = r\"\/kaggle\/input\/opencv-samples-images\/data\/basketball2.png\"\ncoffee = r\"\/kaggle\/input\/operations-with-opencv\/1coffee.jpg\"\nbuildings = r\"\/kaggle\/input\/opencv-samples-images\/data\/building.jpg\"\nmotion= r\"\/kaggle\/input\/opencv-samples-images\/data\/text_motion.jpg\"\nbox =r\"\/kaggle\/input\/opencv-samples-images\/data\/box_in_scene.png\"\naero=r\"\/kaggle\/input\/opencv-samples-images\/data\/aero1.png\"\nsudoku = r\"\/kaggle\/input\/opencv-samples-images\/data\/sudoku.png\"\ncoridor = r\"\/kaggle\/input\/computer-vision-course\/imagenes\/corridor.jpg\"\nopencv = r\"\/kaggle\/input\/opencv-samples-images\/data\/opencv-logo-white.png\"\nchessboard_hard =r\"\/kaggle\/input\/computer-vision-course\/right\/right_005.jpg\"","b2c665fa":"#Read the images and convert to RGB\nimage_1 = cv2.imread(minnions)\nimage_1 = cv2.cvtColor(image_1, cv2.COLOR_BGR2RGB)\nimage_2 = cv2.imread(box )\nimage_2 = cv2.cvtColor(image_2, cv2.COLOR_BGR2RGB)\nimage_4 = cv2.imread(sudoku)\nimage_4 = cv2.cvtColor(image_4, cv2.COLOR_BGR2RGB)\nimage_5 = cv2.imread(coridor)\nimage_5 = cv2.cvtColor(image_5, cv2.COLOR_BGR2RGB)\nimage_6 = cv2.imread(opencv)\nimage_6 = cv2.cvtColor(image_6, cv2.COLOR_BGR2RGB)\nimage_7 = cv2.imread(chessboard)\nimage_7 = cv2.cvtColor(image_7, cv2.COLOR_BGR2RGB)\n","79892766":"image_3 = cv2.imread(box)\nprint(image_3.shape)\nimage_3 = cv2.cvtColor(image_3, cv2.COLOR_BGR2RGB)\nfigure(figsize=(18,16), dpi=40)\nprint(\"original image\")\nplt.imshow(image_3)","8fee27d9":"kernel= np.array([[0.1, 0, -.2], [.1,  .2, .1], [0.2, 0, -.2]])\nfigure(figsize=(18,16), dpi=40)\ndark= cv2.filter2D(image_3, -1, kernel)\nprint(\"Low contrast image\")\nplt.imshow(dark)","5a819a84":"hist,bins = np.histogram(dark.flatten(),256,[0,256])\ncdf = hist.cumsum()\ncdf_normalized = cdf * float(hist.max()) \/ cdf.max()\n\nfigure(dpi=90)\nplt.plot(cdf_normalized, color = 'b')\nplt.hist(dark.flatten(),256,[0,256], color = 'm')\nplt.xlim([0,256])\nplt.legend(('cdf','histogram'), loc = 'lower right')\nplt.show()","faae7d1e":"darkg  = im = cv2.cvtColor(dark, cv2.COLOR_RGB2GRAY)\ndst = cv2.equalizeHist(darkg)\nfigure(figsize=(18,16), dpi=40)\nprint(\"Look! Seems brighter\")\nplt.imshow(dst,cmap=\"gray\")","fd456cee":"lab= cv2.cvtColor(dark, cv2.COLOR_RGB2LAB)\nl, a, b = cv2.split(lab)\n#-----Applying CLAHE to L-channel-------------------------------------------\nclahe = cv2.createCLAHE(clipLimit=10, tileGridSize=(13,13))\ncl = clahe.apply(l)\n#-----Merge the CLAHE enhanced L-channel with the a and b channel-----------\nlimg = cv2.merge((cl,a,b))\n#-----Converting image from LAB Color model to RGB model--------------------\nfinal = cv2.cvtColor(limg, cv2.COLOR_LAB2BGR)\n\n\ntitles = [\"original\", \"CLAHE\"]\nimages=[dark, final]\nfigure(figsize=(18,16), dpi=70)\n\nfor i in range(2):\n    plt.subplot(1,2,i+1), plt.imshow(images[i], \"gray\")\n    plt.title(titles[i])\n    plt.xticks([]), plt.yticks([])\nplt.show()","ecf52894":"imdic = [ image_3,dark, dst,final]\ntitles = [ \"original\", \"low contrast\",\"traditional hist eq\",\"CLAHE hist eq\"]\nfigure(figsize=(18,16), dpi=70)\n\nfor i in range(4):\n    plt.subplot(2,2,i+1), plt.imshow(imdic[i], \"gray\")\n    plt.title(titles[i])\n    plt.xticks([]), plt.yticks([])\nplt.show()","1ddedc2a":"im=cv2.cvtColor(image_4, cv2.COLOR_RGB2GRAY)\nfigure(figsize=(18,16), dpi=30)\nplt.imshow(im,cmap=\"gray\")\nprint(\"min value of image: \", im.min())\nprint(\"max value of image: \", im.max())","4b9baebb":"ret,threshold = cv2.threshold(im,55,255,cv2.THRESH_BINARY)\nprint(ret)\nfigure(figsize=(18,16), dpi=30)\nplt.imshow(threshold,cmap=\"gray\")\n","414d8182":"ret,trunc = cv2.threshold(im,55,255,cv2.THRESH_TRUNC)\nprint(ret)\nfigure(figsize=(18,16), dpi=30)\nplt.imshow(threshold,cmap=\"gray\")\nprint(trunc.max())","af7e5f23":"ret,threshtozero = cv2.threshold(im,55,255,cv2.THRESH_TOZERO)\nprint(ret)\nfigure(figsize=(18,16), dpi=30)\nplt.imshow(threshtozero,cmap=\"gray\")\n","9fa50b65":"ret,otsu = cv2.threshold(im,555,255,cv2.THRESH_OTSU)\nprint(ret)\nfigure(figsize=(18,16), dpi=40)\nplt.imshow(otsu ,cmap=\"gray\")","6c66afa1":"adaptive = cv2.adaptiveThreshold(im,255,cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY,13,7)\nprint(ret)\nfigure(figsize=(18,16), dpi=40)\nplt.imshow(adaptive ,cmap=\"gray\")","ee1e350e":"titles = ['Original Image', 'Global Thresholding (v = 55)', \"Truncation Threshold(v = 55)\", 'Thresh T0 Zero Thresholding(v = 55)', \"OTSU Threshold(v = 55)\", 'Adaptive Gaussian Thresholding']\nimages = [im,threshold,trunc, threshtozero, otsu, adaptive]\nfigure(figsize=(18,16), dpi=70)\nfor i in range(6):\n    plt.subplot(3,2,i+1),plt.imshow(images[i],'gray')\n    plt.title(titles[i])\n    plt.xticks([]),plt.yticks([])\nplt.show()","9f5e62f0":"img = image_5\nimg=cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)","ce6e9621":"sobelx = cv2.Sobel(img, cv2.CV_64F,1,0, ksize=5)\nfigure(figsize=(18,16), dpi=40)\nplt.imshow(sobelx ,cmap=\"gray\")","552b552f":"sobely = cv2.Sobel(img, cv2.CV_64F,0,1, ksize=5)\nfigure(figsize=(18,16), dpi=40)\nplt.imshow(sobely ,cmap=\"gray\")","e7d7c2b9":"laplacian = cv2.Laplacian ( img, cv2.CV_64F)\nfigure(figsize=(18,16), dpi=40)\nplt.imshow(laplacian ,cmap=\"gray\")","8545cdb4":"weighted  =  cv2.addWeighted( src1 =sobelx, alpha=0.5, src2 = sobely, beta =0.5, gamma = 0 )\n\nfigure(figsize=(18,16), dpi=40)\nplt.imshow(weighted  ,cmap=\"gray\")","c40249fc":"scharrx = cv2.Scharr(img, cv2.CV_64F, 1, 0)\nfigure(figsize=(18,16), dpi=40)\nplt.imshow(scharrx  ,cmap=\"gray\")","7d2454f0":"scharry = cv2.Scharr(img, cv2.CV_64F, 0,1)\nfigure(figsize=(18,16), dpi=40)\nplt.imshow(scharry  ,cmap=\"gray\")","b8b1d4df":"weightedsc  =  cv2.addWeighted( src1 =scharrx, alpha=0.5, src2 = scharry, beta =0.5, gamma = 0 )\n\nfigure(figsize=(18,16), dpi=40)\nplt.imshow(weightedsc  ,cmap=\"gray\")","44a095d5":"titles = ['Original Image', 'X-dir Sobel', \"Y-dir Sobel\", 'Blending Sobel', \"Laplacian Gradient\",\"X-dir Scharr \",\"Y-dir Scharr\",\"Blending Scharr\"]\nimages = [img,sobelx,sobely, weighted, laplacian,scharrx,scharry,weightedsc]\nfigure(figsize=(28,9), dpi=70)\nfor i in range(8):\n    plt.subplot(2,4,i+1),plt.imshow(images[i],'gray')\n    plt.title(titles[i])\n    plt.xticks([]),plt.yticks([])\nplt.show()","402f6e4a":"img1 = image_1\nimg2 = image_6\n\nrows,cols,channels = img2.shape\nroi = img1[0:rows, 0:cols]\nimg2gray = cv2.cvtColor(img2,cv2.COLOR_RGB2GRAY)\nret, mask = cv2.threshold(img2gray, 10, 255, cv2.THRESH_BINARY)\n\nmask_inv = cv2.bitwise_not(mask)\n\nimg1_bg = cv2.bitwise_and(roi,roi,mask = mask_inv)\nimg2_fg = cv2.bitwise_and(img2,img2,mask = mask)\n\ndst = cv2.add(img1_bg,img2_fg)\nimg1[0:rows, 0:cols ] = dst\nprint(\"Minnion also loves openCV\")\nplt.imshow(img1,cmap=\"gray\")","6ca2e716":"im_2 = image_2\nim_1 = cv2.resize(image_1, (96,84))\nim_1_gray = cv2.cvtColor(im_1,cv2.COLOR_RGB2GRAY)","9c3682fb":"mask_inv = cv2.bitwise_not(im_1_gray)\nplt.imshow(mask_inv,cmap='gray', vmin = 0, vmax = 255)","7cd0f1f9":"white_backround = np.full(im_1 .shape,255,dtype=np.uint8)\nprint(white_backround.shape)","36003503":"background = cv2.bitwise_or(white_backround,white_backround,mask=mask_inv)\nprint(background.shape)\nplt.imshow(background)","d94ce9eb":"foreground = cv2.bitwise_or(im_1,im_1,mask=mask_inv)\nprint(\"how cute!\",foreground.shape)\nplt.imshow(foreground)","eeb52437":"print(im_2.shape)\nx,y,channels = im_2.shape\nx_offset =  512-96\ny_offset = 384-84\n\nroi = im_2[y_offset:384,x_offset:512]\nprint(roi .shape)\nplt.imshow(roi)","efb690d6":"minion =cv2.bitwise_or(roi, foreground)\nprint(minion.shape)\nplt.imshow(minion)","6e025803":"large = im_2\nsmall = minion\nlarge[y_offset:y_offset+small.shape[0], x_offset:x_offset+small.shape[1]] = small\nprint(\"It's There!! cute and little\")\nplt.imshow(large)","4e1b519b":"img = image_3\nimg=cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\nprint(\"original shape:\", img.shape)\nedges = cv2.Canny (image=img, threshold1=126, threshold2=128)\nfigure(figsize=(18,16), dpi=30)\nplt.imshow(edges, cmap=\"gray\")\nprint(\"edged shape:\", edges.shape)\n","5d679825":"titles = ['Original Image', 'Canny Edge Detection']\nimages = [img,edges]\nfigure(figsize=(18,16), dpi=60)\nfor i in range(2):\n    plt.subplot(1,2,i+1),plt.imshow(images[i], cmap=\"gray\")\n    plt.title(titles[i])\n    plt.xticks([]),plt.yticks([])\nplt.show()","f143c5a3":"med_val = np.median(img)\nlower = int(max(0,0.7*med_val))\nupper = int(min(255, 1.55*med_val))\n\nblur = cv2.GaussianBlur(img,(5,5),1)\nedges = cv2.Canny(image=blur, threshold1=lower, threshold2=upper)\nfigure(figsize=(18,16), dpi=30)\nplt.title(\"Adaptive Canny and Blur Preporcessed\")\nplt.imshow(edges, cmap=\"gray\")\nprint(edges.shape)","fe1c42a3":"im = cv2.imread(chessboard_hard) # For backup\nim  = cv2.cvtColor(im , cv2.COLOR_BGR2RGB)# For backup","9f56453d":"img2_rgb = cv2.imread(chessboard_hard)\n\nprint(\"original image shape is: \", img2_rgb.shape)\nimg2  = cv2.cvtColor(img2_rgb , cv2.COLOR_BGR2GRAY)\n\nimg2 = np.float32(img2)\n#corner2 = cv2.dilate(corner2,None)\ncorner2 = cv2.cornerHarris(src=img2, blockSize=2, ksize=5, k=0.04)\nprint(\"cornered iamge shape is : \", corner2.shape)\nimg2_rgb[corner2>0.01*corner2.max()] = [255,0,0]#RGB\n\nfigure(figsize=(18,16), dpi=30)\nplt.imshow(img2_rgb )","0750bbee":"img2_rgb2 = cv2.imread(chessboard_hard)\nprint(\"original image shape is: \", img2_rgb2.shape)\nimg2  = cv2.cvtColor(img2_rgb2 , cv2.COLOR_BGR2GRAY)\n#corner2 = cv2.dilate(corner2,None)\ncorners2 = cv2.goodFeaturesToTrack(img2 ,80, 0.01,10 )\ncorners2 = np.uint8(corners2)\n\n\nprint(type(corners2), corners2.shape)","190963bf":"for i in corners2:\n    x,y = i.ravel()\n    cv2.circle(img2_rgb2, (x,y),3, (255,0,0),-1 )\nfigure(figsize=(18,16), dpi=30)    \nplt.imshow(img2_rgb2 )","76236867":"titles = ['Original Image', 'Harris Corner Det', \"Shi Thomasi Corner Det\"]\nimages = [im, img2_rgb, img2_rgb2]\nfigure(figsize=(18,16), dpi=90)\nfor i in range(3):\n    plt.subplot(1,3,i+1),plt.imshow(images[i])\n    plt.title(titles[i])\n    plt.xticks([]),plt.yticks([])\nplt.show()","9f760755":"image_7 = cv2.imread(chessboard)\n#image_7 = cv2.cvtColor(image_7, cv2.COLOR_BGR2RGB)\nplt.imshow(image_7  )","48a9b43d":"grid_size = (7,7)\nfound, corners = cv2.findChessboardCorners(image_7, grid_size )\nprint(found, corners.shape)","ecf0ffa9":"cv2.drawChessboardCorners(image_7,grid_size ,corners, found)\nfigure(figsize=(18,16), dpi=60)    \nplt.imshow(image_7)","caf58e17":"image_8 = cv2.imread( r\"\/kaggle\/input\/opencv-samples-images\/data\/pic1.png\")\nimage_8 = cv2.cvtColor(image_8, cv2.COLOR_BGR2GRAY)\nfigure(figsize=(18,16), dpi=30) \nplt.imshow(image_8, cmap=\"gray\")\nprint(\"Origina shape is: \",image_8.shape)","499b0bf9":"contours, hierarchy = cv2.findContours(image_8, cv2.RETR_CCOMP,cv2.CHAIN_APPROX_SIMPLE )","45a3ace8":"print(\"contours type is: \",type( contours), \"length of the list is: \" ,len(contours))","a9b3a076":"print(\"hierarchy type is: \",type( hierarchy) ,\"shape of hierarchy is: \",hierarchy.shape)","29732304":"print(hierarchy)","11d706a5":"internal = np.zeros(image_8.shape)","127a1a50":"for i in range(len(contours)):\n    #print(hierarchy[0][i][3])\n    if hierarchy[0][i][3] == -1: #check the last value for every column in hierarchy array\n        cv2.drawContours(internal , contours, i ,255, 0 )\n        \nfigure(figsize=(18,16), dpi=40)            \nplt.imshow(internal , cmap=\"gray\")","48670565":"external = np.zeros(image_8.shape)","c631ae6d":"for i in range(len(contours)):\n    #print(hierarchy[0][i][3])\n    if hierarchy[0][i][3] != -1: #check the last value for every column in hierarchy array\n        cv2.drawContours(external , contours, i ,255, 0 )\n        \nfigure(figsize=(18,16), dpi=40)            \nplt.imshow(external , cmap=\"gray\")","52274cae":"**Foot Note: Camera Calibration**\n\nNow that we have our object points and image points, we are ready to go for calibration. We can use the function, cv2.calibrateCamera() which returns the camera matrix, distortion coefficients, rotation and translation vectors.\n\n>ret, mtx, dist, rvecs, tvecs = cv2.calibrateCamera(objpoints, imgpoints, gray.shape[::-1], None, None)","829df322":"\n****$\\color{pink}{\\text{Conclusion of Thresholding }}$****\n\nAdaptive thresholding overcomes the area based problems with creating little bit of noise.","2125bf6e":"\n\nOpenCV has some methods for tracking grids, this is used for the calibration of cameras or real systems. By these methods chessboard similar objects are able to track. \n\nIt's important for this methods that, the object SHOULD BE like chessboard object. Not similar not different. And a chessboard  is 8x8 but for the chessboard image, the corners which are coincide the edge of the image cannot be detected so we will pass 7x7.\n\n\n>cv2.findChessboardCorners(image, grid_size   )\n\nAlgortigm turns:\n\n* A boolean TRUE or FALSE if grid is found or not\n* List of coordinate of the found corners","4903a0b7":"#### ****$\\color{orange}{\\text{Adaptive Gaussian Threshold}}$****\n\n>cv2.adaptiveThreshold(img,max_value,cv2.ADAPTIVE_THRESH_GAUSSIAN_C,.THRESH_BINARY,11,2)","f8a036e9":"****$\\color{pink}{\\text{Conclusion of Gradients }}$****\n\nGradients are help to find the changing points in terms of either color or intensity. It will help to get edges in any direction we want. Sobel operator sometimes called the Sobel\u2013Feldman operator  is used  particularly within edge detection algorithms where it creates an image emphasising edges. Scharr is another good method for edge emphasising too.","6d253d4e":"#### ****$\\color{orange}{\\text{Shi-Thomasi Corner Detection}}$****\n\nIt made some updates to Harris Corner Detection and gives better results.\n\n>cv2.goodFeaturesToTrack(gray_image, max_corner_num , quaility_level, minDistance, )\n\n* If you select \"0\" in max_corner_num, it turns back all the corners it finds. Other, u can give an integer to find the highest possibility owned corners.\n\n*Source:  J. Shi and C. Tomasi (June 1994). \"Good Features to Track\". 9th IEEE Conference on Computer Vision and Pattern Recognition. Springer. pp. 593\u2013600. CiteSeerX 10.1.1.36.2669. doi:10.1109\/CVPR.1994.323794. *","dda1a094":"****$\\color{pink}{\\text{Conclusion of Edge Detection }}$****\n\nEdge detection is one of the mostly used technique in computer vision. It is important to know that edges desired to be detected can be done more precisely with blurring as pre-processing. Moreover, a pipeline can be created to have a better performance in edge detection including morphological operations, contour finding etc.","46936133":"#### ****$\\color{orange}{\\text{OTSU's Threshold}}$****\n\ncv2.THRESH_OTSU uses Otsu algorithm to choose the optimal threshold value","e5961532":"#### ****$\\color{orange}{\\text{Scharr Operator}}$****\n\nThe Sobel operator, sometimes called the Sobel\u2013Feldman operator or Sobel filter, is used in image processing and computer vision, particularly within edge detection algorithms where it creates an image emphasising edges.","e73ed950":"we can find lower and upper bounds adaptively to the image. \n* For the lower threshold; we can define it as either \"0\" or the a specific percentage of the median value. For our example we can select it as 75%. And we turned whichever is greater.\n* For the upper threshold; we can define it either a certain amount of the median value ( greater than 100%) or the max value is 255 (8-bit). And we turned whichever is lower.","3f170e64":"the function of threshold basically apply that, any value below the threshold value turns to \"0\" and any value over the threshold value turns to \"255\".\n\n>cv2.threshold(im,threshold, max_value,type_of_threshold)","141d7fe1":"We got the paths of the images we will use in the notebook, and create a dictionary for some printing purposes.","3d63bc78":"#### ****$\\color{orange}{\\text{Laplace Operator}}$****\n\nLaplacian operator finds both x and y direction gradient.","61b82550":"#### ****$\\color{orange}{\\text{bitwise_and}}$****\n","90bae066":"#### ****$\\color{orange}{\\text{bitwise_not}}$****\n\n","495a08b7":"#### ****$\\color{orange}{\\text{Global Threshold}}$****\n\nWe saw that some CV applications or methods require GRAY scale images. Similarly some applications and methods requires binary (2bit 0-1) images to show general shapes and features. Thresholding is nothing but converting an image to two segments: Black and White.","86855441":"## Section 2.3 Grid Detection <a class=\"anchor\"  id=\"section2_3\"><\/a>","20aa6bf2":"Hi everyone! We continue to deep dive to image processing with OpenCV for Computer Vision, it is a long notebook so keep in tune. If you missed the first part, you are welcome to reach from here :\n\n* Lesson 1: [OpenCV for Computer Vision 1 - Beginner](https:\/\/www.kaggle.com\/volkandl\/opencv-for-computer-vision-1-beginner)\n\nIn this notebook we will see basic processing operations like histogram equalization or bitwise operations and moderate image processing methos like edge detection, corner detection , morphological operations etc. It is really important to know that in many real life problems, always we create a pipeline of image processing-machine learning methods which contains nothing but i will explain here and in the future. So If you like upvote, i need ur upvotes! If you want to learn image procesing this serie is good point to start and improve you from zero to hero!","829fcd0c":"Output only consist of binary values 0 or 255.","51a54f9a":"We will check the last value for every column in hierarchy array, if it is equal to \"-1\" it is internal contours.","92a12adc":"#### ****$\\color{orange}{\\text{X Direction Sobel Gradient}}$****\nx gradient is basically the vertical lines.","e81218ef":"Firstly, cornerHarris fun\n\n> img1[corner1>0.01*corner1.max()=[255,0,0]\n\nMeans, when ever the my corner detection value is greater than the corner detection matrix's max value, turn that point to Red","bfbb2289":"#### ****$\\color{orange}{\\text{Y Direction Sobel Gradient}}$****\ny gradient is basically the horizontal lines.","e611c1ca":"## Section 1.2 Image Thresholding <a class=\"anchor\"  id=\"section1_2\"><\/a>","761435ca":"There are some points that they are not actually corners. However, work really well","cff8edcd":"#### ****$\\color{orange}{\\text{Truncation Threshold}}$****\n\nTruncation Threshold makes that: \n\n* If the value is above the threshold, makes the value equals to threshold\n* IF the value is below the threshold, keep it same\n\n>cv2.THRESH_TRUNC","2455384d":"#### ****$\\color{orange}{\\text{Harris Corner Detection}}$****\n\nHarris\u2019 corner detector takes the differential of the corner score into account with reference to direction directly, instead of using shifting patches for every 45-degree angles, and has been proved to be more accurate in distinguishing between edges and corners.\n\n> cv2.cornerHarris ( source, blockSize, ksize )\n\n* source --> image\n* blockSize --> the neighbourhood size for mathamatical operations; eigen value calculations etc.\n* ksize --> aparture parameter for Sobel operator, kernel size, \n* k --> harris detector free parameter, k=0.04 default\n\n*Source: Harris, C., & Stephens, M. (1988, August). A combined corner and edge detector. In Alvey vision conference (Vol. 15, No. 50, pp. 10-5244).*","7c97c0ce":"## Section 1.3 Gradients <a class=\"anchor\"  id=\"section1_3\"><\/a> \n\nImage gradient is a directional change in the intensity or color in an image. Gradients can be calculated in a specific direction likewise from left to right, from right to left, from top to bottom etc.","f49f6d23":"#### ****$\\color{orange}{\\text{bitwise_or}}$****","8e70de5f":"#### ****$\\color{orange}{\\text{Histograms Equalization in OpenCV  }}$****\nOpenCV has a function for histogram equalization as:\n\n>cv2.equalizeHist()\n\nImportant: It takes an input in gray scale! So we will convert our RGB dark image to GRAY scale.\n\nWe will draw  cumulative distribution function (cdf) which gives us a institution about at which value of the pixels (0-255) the cdf reaches the confidence level. However, we would like to have a linear trend on cdf.","4e5e5a25":"# Chapter 2. Image Processing for Detection <a class=\"anchor\" id=\"chapter2\"><\/a>\n\nThis chapter is the core of the image processing. In many proffesional application like Character Recognition, Face Detection, Object Detection, measurements etc. we will use the techniques like edge detection, corner detection, morphological operations, hough transform etc. For the coming notebooks this chapter is needed to analyzed and studied detailly.","5923bc7d":"> cv2.Sobel(image , cv2.CV_64F, dx, dy, ksize )\n\n* cv2.CV_64F --> Means 64 Floats point precision\n* dx = derivative in x direction. 1 means yes, 0 means no\n* dy = derivative in y direction. 1 means yes, 0 means no\n* ksize is a integer n, which means nxn kernel","884b14ba":"****$\\color{pink}{\\text{Conclusion of bitwise operations }}$****\n\nBitwise operations are mostly used in masking and blending. ","88a950d2":"To find contours we will use the opencv function. It takes the image in GRAY scale and has more options like below.\n\n>cv2.findContours(image , Mode ,Methods )\n\n* Image: An 8-bit GRAY image. Image can be converted t\u0131 gray by Using threshold , adaptiveThreshold, Canny, and others to create a binary image out of a grayscale or RGB image.\n* Mode: Contour retrieval modes:\n    * cv2.RETR_EXTERNAL: Retrieves only the extreme outer contours.\n    * cv2.RETR_LIST: Retrieves all of the contours without establishing any hierarchical relationships.\n    * cv2.RETR_CCOMP: Retrieves all of the contours and organizes them into a two-level hierarchy. At the top level, there are external boundaries of the components. At the second level, there are boundaries of the holes.\n    * cv2.RETR_TREE: Retrieves all of the contours and reconstructs a full hierarchy of nested contours.\n* Methods: Contour approximation method   \n    * cv2.CHAIN_APPROX_NONE : Stores absolutely all the contour points.\n    * cv2.CHAIN_APPROX_SIMPLE : Compresses horizontal, vertical, and diagonal segments and leaves only their end points.\n    * cv2.CHAIN_APPROX_TC89_L1 : Applies one of the flavors of the Teh-Chin chain approximation algorithm[1].\n    \n    \n\nTurns back 2 things:\n\n* Contours : Detected contours. Each contour is stored as a vector of points\n* Hierarchy of sorting: containing information about the image topology. It has as many elements as the number of contours\n\n\n*[1] C-H Teh and Roland T. Chin. On the detection of dominant points on digital curves. Pattern Analysis and Machine Intelligence, IEEE Transactions on, 11(8):859\u2013872, 1989.*","46c56f91":"> cv2.addWeighted( src1 = src1, alpha = 0.5, src2 = src2, beta = 0.5, gamma = 0 )","c2f730ce":"### ****$\\color{orange}{\\text{If You want to be an AI Expert, Knowing Deep learning is never enough alone!}}$****\n\n### $\\color{Pink}{\\text{Table of Contents  }}$\n\n* [Chapter 1. Image Processing Basics](#chapter1)\n    * [Section 1.1 Histogram Equalization ](#section1_1)\n    * [Section 1.2 Image Thresholding](#section1_2)\n    * [Section 1.3 Gradients](#section1_3)\n    * [Section 1.4 Arithmatic Operations with Blending Example](#section1_4)\n\n    \n        \n* [Chapter 2. Image Processing for Detection ](#chapter2)\n    * [Section 2.1 Edge Detection](#section2_1)\n    * [Section 2.1 Corner Detection](#section2_2)\n    * [Section 2.3 Grid Detection](#section2_3)\n    * [Section 2.4 Contour Detection](#section2_4)\n    \n\n    \n\n****$\\color{pink}{\\text{If You like my work, Please upvote  }}$****\n","75ccf02e":"Creating 3 channels full white image with numpy.full() from the original 3 channels image.","2109d72a":"It is attempted to apply a linear trend to our cumulative distribution function in math sense.","f05d91c3":"****$\\color{pink}{\\text{Conclusion of Contour Detection }}$****\n\nBy the help of the opencv we can find the contours of an image. Moreover, we can group them as internal or external. The logic behind that method is to understand if the contours  coincide with the background or not.","70889773":"## Section 2.1 Edge Detection <a class=\"anchor\"  id=\"section2_1\"><\/a>","e4b0dea1":"## Section 1.4 Arithmetic Operations with Blending Example <a class=\"anchor\"  id=\"section1_4\"><\/a>\n\nThis includes the bitwise AND, OR, NOT, and XOR operations. ","f3f4fbca":"****$\\color{pink}{\\text{Conclusion of Corner Detection }}$****\n\nFor corner detection we study two famous algorithms: Harris and Shi Thomasi. It can be seen Shi Thomasi corner detection algorithm gives better results and have a flexibility to select number of corners desired to be detected.","709b0f46":"hierarchy is a numpy array has 4 values for the each contour detected. First 3 gives the location and the last one gives the information about whether the contour is internel or external.","59a48734":"We can define contours as curves which are joining all the continous points which have same color or intesity along a path.","ba1c5d55":"## Section 2.4 Contour Detection <a class=\"anchor\"  id=\"section2_4\"><\/a>","893ca7b8":"Blending with x dir sobel and y dir sobel will give better results. For blending 2 images we will use the cv2.addWeighted() function below. It takes source 1 and source 2 image and blend them together according to mixing parameters of alpha, beta and gamma","ad187a3d":"#### ****$\\color{orange}{\\text{Thresh to Zero Threshold}}$****\n\nThresh to zero threshold makes:\n\n* If the value is above the threshold, keep it same\n* If the value is below the threshold, makes it \"0\"\n\n>cv2.THRESH_TOZERO","5ec67db7":"We will check the last value for every column in hierarchy array, if it is not equal to \"-1\" , it is external contours.","ef791446":"# Chapter 1. Image Processing Basics <a class=\"anchor\" id=\"chapter1\"><\/a>","37e78213":"\nAttention Please: we are adding plt.imshow(image, cmap='gray', vmin = 0, vmax = 255) ! Since plt.imshow is expecting 3 channels of an image, if we do not state cmap=\"gray\" then it will take it as it has 3 same channels. Try without stating cmap, and see the difference :)","647443f9":"## Section 1.1 Histogram Equalization <a class=\"anchor\"  id=\"section1_1\"><\/a>\n\n\nIn some images due to lightining, environmental issues, day time or materials itself. Histogram equalization is such a technique that helps to improve of the contrast of an image. The logic behind of the method is nothing but stretch out the intensity range of the image\n\n![unnamed.jpg](attachment:6b542c0b-bfc4-4cad-af92-917c560b55ce.jpg)","e7082ead":"For better result we will apply blur before the canny edge detector even though it has a blurring step in the function!","1a87027c":"The Canny edge detection is a traditional edge detection algorithm belongs to John F. Canny. It is developed in 1986. \n\nThe Canny edge detection algorithm is composed of 5 steps:\n\n1. Noise reduction\n    * The noise in the image with a 5x5 Gaussian filter. We got a smoothened version of the image.\n2. Gradient calculation\n    * Smoothened image is then filtered with a Sobel kernel in both horizontal and vertical direction to get first derivative in horizontal direction and vertical direction. By doing so we find the edge gradient and direction. Note that: Gradient direction is always perpendicular to edges.\n3. Non-maximum suppression\n\n    * All pixels of the image is done to remove any unwanted pixels which may not constitute the edge. For this, at every pixel, pixel is checked if it is a local maximum in its neighborhood in the direction of gradient. This is called Non-maximum suppression\n\n![Nonmaxsup](https:\/\/docs.opencv.org\/3.4\/nms.jpg)\n\n4. Double threshold & Edge Tracking by Hysteresis\n\n    * In order to eliminate the non-edge foundings and gather the real edges, a discrimination is done.Having defined two threshold values, minVal and maxVal, any edges with intensity gradient more than maxVal are sure to be edges and those below minVal are sure to be non-edges. The non-edges ones are discarded. \n\n![double thres](https:\/\/docs.opencv.org\/3.4\/hysteresis.jpg)\n\n\nBy using OpenCV's cv2.Canny() function we can gather edge detected image. You can insert a RGB image or GRAY scale image to the func. But it will output a binary ( 0 or 255 ) image.\n\n> cv2.Canny( image, threshold1, threshold2)\n\nBut the important question is that: How to find proper threshold values?\n\n\n\n\n*Source: Canny, J. (1986). A computational approach to edge detection. IEEE Transactions on pattern analysis and machine intelligence, (6), 679-698.*","85b0d09a":"## Section 2.2 Corner Detection <a class=\"anchor\"  id=\"section2_2\"><\/a>\n\nCorner is a point or group of points which interprets as junction of two edges. We well investigate two reputative corner detection algorithms:\n\n* Harris Corner Detection\n* Shi-Thomasi Corner Detection","0de27b6e":"![kaggle (2).png](attachment:e12e75e7-3903-43ec-845b-d86d8b9c2046.png)","6b346ae2":"Now we have a dark image which means has a low contrast ditribution image let say. We will work on it to understand the histogram eq. techniques.","e1611d79":"#### ****$\\color{orange}{\\text{Blending with Weighted Function}}$****","a5e76d50":"#### ****$\\color{orange}{\\text{Contrast Limited Adaptive Histogram Equalization (CLAHE)  }}$****\n\nContrast Limited Adaptive Histogram Equalization (CLAHE) differs from adaptive histogram equalization in its contrast limiting. In the case of CLAHE, the contrast limiting procedure is applied to each neighborhood from which a transformation function is derived. LAB color space is a space that L is defined by lightness and the color-opponent dimensions a and b, which are based on the compressed X Y Z color space coordinates. This clip limit depends on the normalization of the histogram or the size of the neighborhood region. The value between 3 and 10 is commonly used as the clip limit.","cb2f2082":"\n****$\\color{pink}{\\text{Conclusion of Histogram Equalization }}$****\n\nCLAHE overcome the overal brightness problem of traditional histogram eq. Moreover, it helps to enchance the image in an adaptive manner."}}