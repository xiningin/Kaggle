{"cell_type":{"eefe57a1":"code","554acf1f":"code","bf87ef21":"code","ea75e7ed":"code","207c8656":"code","4f3f8250":"code","12376ad6":"code","2579fc23":"code","b1bb6688":"code","d579ed9c":"code","77048a67":"code","3a23325e":"code","f8f23cb6":"code","1b86f02d":"code","f59a8f7c":"code","d5a0b1f7":"code","7c49831e":"code","60cf041e":"code","58000f6f":"code","5b249c58":"code","b44db7c9":"code","afd23fc3":"code","a58f74c8":"code","591bf0fc":"code","f967bb2b":"code","4d5bce3c":"code","ec11662b":"code","b0610e22":"code","c15256c2":"code","f8b33047":"markdown","9f5df0fc":"markdown","aef624e8":"markdown","1c967835":"markdown","57d3c638":"markdown","1af10937":"markdown","51d63b37":"markdown","f5d3143a":"markdown"},"source":{"eefe57a1":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","554acf1f":"#import data\ndata=pd.read_csv(\"..\/input\/maaslar.csv\")","bf87ef21":"data.head()","ea75e7ed":"#maas ve e\u011fitim seviyesi aras\u0131nda ili\u015fki kurmak istiyorum\nx=data.iloc[:,1:2] #E\u011fitim seviyesi kolonunu al \/\/slicing\ny=data.iloc[:,2:] #E\u011fitim seviyesi kolonunu al","207c8656":"#linear regression\nfrom sklearn.linear_model import LinearRegression\nlr=LinearRegression()\nlr.fit(x,y) # x' ten y yi \u00f6\u011fren\nprint(type(x))\n\nplt.scatter(x,y) # E\u011fitim seviyesi ev maa\u015f\u0131 2 boyutlu uzaya da\u011f\u0131t\nplt.plot(x,lr.predict(x),color=\"red\") #Her bir x e kar\u015f\u0131l\u0131k gelen tahminler g\u00f6ster","4f3f8250":"#polynomial regression \/\/multiple regression olarak d\u00fc\u015f\u00fcn\u00fclebilir.\nfrom sklearn.preprocessing import PolynomialFeatures\npoly_reg= PolynomialFeatures(degree=2) #x^0, x^1, x^2 yi alacaz ve sistemi bu x lerin \u00e7arpanlar\u0131n\u0131 \u00f6\u011fretecez ama\u00e7 bu. \nx_poly= poly_reg.fit_transform(x)\nprint(x_poly)\n\nlin_reg2= LinearRegression()\nlin_reg2.fit(x_poly,y)\nplt.scatter(x,y)\nplt.plot(x,lin_reg2.predict(x_poly),color=\"red\")\nplt.show()","12376ad6":"#4. dereceden denemek istersek daha iyi sonu\u00e7 alabiliriz. Ama bu veri k\u00fcmesine \u00f6zel bir durum olabilir.\n#polynomial regression \/\/multiple regression olarak d\u00fc\u015f\u00fcn\u00fclebilir.\nfrom sklearn.preprocessing import PolynomialFeatures\npoly_reg= PolynomialFeatures(degree=4) #x^0, x^1, x^2 yi alacaz ve sistemi bu x lerin \u00e7arpanlar\u0131n\u0131 \u00f6\u011fretecez ama\u00e7 bu. \nx_poly= poly_reg.fit_transform(x) #E\u011fitim s\u00fcreci\nprint(x_poly)\n\nlin_reg2= LinearRegression()\nlin_reg2.fit(x_poly,y)\n\n#visualization\nplt.scatter(x,y)\nplt.plot(x,lin_reg2.predict(x_poly),color=\"red\")\nplt.show()","2579fc23":"#tahminler\nprint(lr.predict([[11]]))\nprint(lr.predict([[6]]))\n\n#ploynomial predict i\u00e7in \u00f6ncelikle de\u011fei polyno\u00f6ial d\u00fcnyaya transform etmek gerekiyor\nprint(lin_reg2.predict(poly_reg.fit_transform([[6]])))\nprint(lin_reg2.predict(poly_reg.fit_transform([[11]])))","b1bb6688":"#SVR Support Vector Machine\ndata=pd.read_csv(\"..\/input\/maaslar.csv\")\ndata.head(10)","d579ed9c":"#\u00f6ncelikle \u00f6l\u00e7ekleme i\u015flemi ger\u00e7ekle\u015ftiriyoruz\nfrom sklearn.preprocessing import StandardScaler\nsc1= StandardScaler()\n# x ve y daha \u00f6nce slicing edilmi\u015f kolonlar maa\u015f ve e\u011fitim seviyesi\nx_olcekli= sc1.fit_transform(x)\nsc2= StandardScaler()\ny_olcekli= sc2.fit_transform(y)\n\nfrom sklearn.svm import SVR\n\nsvr_reg= SVR(kernel=\"rbf\")\nsvr_reg.fit(x_olcekli,y_olcekli)\n\nplt.scatter(x_olcekli,y_olcekli, color=\"red\")\nplt.plot(x_olcekli,svr_reg.predict(x_olcekli),color=\"blue\")\nplt.show() #show ile bu plt yi \u00e7izdir ve bitir diyoruz. Demezsek bundan sonrakileri ayn\u0131 plot \u00fczerine \u00e7izer.\nprint(svr_reg.predict([[11]]))\nprint(svr_reg.predict([[6.6]]))\n","77048a67":"#Decision Tree\ndata= pd.read_csv(\"..\/input\/veriler.csv\")","3a23325e":"#Decisin Tree i\u00e7in \u00f6l\u00e7eklemeye gerek yok\nfrom sklearn.tree import DecisionTreeRegressor\nr_dt=DecisionTreeRegressor(random_state=0)\nr_dt.fit(x,y) #Var olan 10 de\u011feri \u00f6\u011frendi\n\nplt.scatter(x,y)\nplt.plot(x,r_dt.predict(x))\nplt.show()","f8f23cb6":"#Tahmin #decision tree bir aral\u0131k olarak tahmin yapar yani belli aral\u0131ktaki de\u011ferler i\u00e7in ayn\u0131 de\u011ferleri d\u00f6nd\u00fcr\u00fcr.\nprint(r_dt.predict([[11]])) #10 dan sonraki t\u00fcm de\u011ferleri 50000 olark tahmin etti.\nprint(r_dt.predict([[6.6]])) # 6.5' dan sonra 7' nin s\u0131n\u0131f\u0131 olan 10000 olarak tahmin etti","1b86f02d":"#Ensemle Learning: Kollektif \u00d6\u011frenme\n#Birden fazla s\u0131n\u0131fland\u0131rma ya da tahmin algoritmas\u0131 ayn\u0131 anda kullan\u0131larak hata oran\u0131 d\u00fc\u015f\u00fcr\u00fclebilir.\n#Rassal Orman a\u011fa\u00e7lar\u0131 biden fazla decision tree kulland\u0131\u011f\u0131 i\u00e7in ensemble learning' dir.\n#Ama\u00e7 veri k\u00fcmesini birden fazla k\u00fc\u00e7\u00fck par\u00e7aya b\u00f6l\u00fcp her par\u00e7adan farkl\u0131 bir karar a\u011fac\u0131 olu\u015fturmak sonras\u0131nda ise sonu\u00e7lar\u0131 birle\u015ftirmek\nfrom sklearn.ensemble import RandomForestRegressor\nrfr= RandomForestRegressor(n_estimators=10, random_state=0) #iki parametre vermemiz gerekir. \n#Biri random_state di\u011feri ise ka\u00e7 tane decision tree \u00e7izilece\u011fini s\u00f6yleyen n_estimator\nrfr.fit(x,y)\n\n#tahmin\nprint(rfr.predict([[6.6]])) #ortak bir de\u011fer d\u00f6nd\u00fcr\u00fcr tahmin a\u015famas\u0131nda\n\n#visualization\nplt.scatter(x,y,color=\"red\")\nplt.plot(x,rfr.predict(x),color=\"blue\")\nplt.show()\n","f59a8f7c":"#R2 de\u011feri. 1' e yakla\u015ft\u0131k\u00e7a model iyi, 0' a yakla\u015ft\u0131k\u00e7a model k\u00f6t\u00fc!","d5a0b1f7":"from sklearn.metrics import r2_score\nprint(\"Random Forest R2 de\u011feri: \", r2_score(y, rfr.predict(x)))\nprint(\"Decision Tree R2 de\u011feri: \", r2_score(y, r_dt.predict(x)))\n#Decision Tree var olan verilere g\u00f6re ezbe yapt\u0131\u011f\u0131 i\u00e7in sonu\u00e7 m\u00fckemmel ama tahminler noktas\u0131nda s\u0131k\u0131nt\u0131. \n#Dolay\u0131s\u0131yla direk r2 score a bak\u0131p yorum yapmak bizi hataya sevkedebilir.\nprint(\"SVR R2 de\u011feri: \", r2_score(y_olcekli, svr_reg.predict(x_olcekli)))\nprint(\"Polynomial Regression R2 de\u011feri: \", r2_score(y, lin_reg2.predict(poly_reg.fit_transform(x))))\nprint(\"Linear Regression R2 de\u011feri: \", r2_score(y, lr.predict(x)))\n","7c49831e":"o2_data= pd.read_csv(\"..\/input\/maaslar_yeni.csv\")\no2_data.head()","60cf041e":"x=o2_data.iloc[:,2:3].values\ny=o2_data.iloc[:,-1:].values","58000f6f":"from sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test= train_test_split(x,y,test_size=0.33)\n\nfrom sklearn.preprocessing import StandardScaler\nsc=StandardScaler()\n\nX_train=sc.fit_transform(x_train)\nX_test=sc.fit_transform(x_test)\nY_train=sc.fit_transform(y_train)\nY_test=sc.fit_transform(y_test)","5b249c58":"#Linear regression \nfrom sklearn.linear_model import LinearRegression\nlr= LinearRegression()\nlr.fit(X_train,y_train)\n\nimport statsmodels.regression.linear_model as sm\nr_ols=sm.OLS(lr.predict(x),x)\nr=r_ols.fit()\nprint(r.summary())\n\ntahmin=lr.predict(X_test)\nprint(\"Tahmin:\\n\",tahmin)\ny_test","b44db7c9":"lr_data=pd.read_csv(\"..\/input\/veriler.csv\")","afd23fc3":"lr_data","a58f74c8":"x= lr_data.iloc[5:,1:4].values #ba\u011f\u0131ms\u0131z de\u011fi\u015fkenler. 5' ten sonraki verileri ald\u0131k \u00e7\u00fcnk\u00fc ilk 5 veri outliers, modeli bozuyor!\ny= lr_data.iloc[5:,4:].values #ba\u011f\u0131ml\u0131 de\u011fi\u015fken\nprint(x)\nprint(y)","591bf0fc":"from sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test= train_test_split(x,y,test_size=0.33, random_state=0)\n\nfrom sklearn.preprocessing import StandardScaler\nsc= StandardScaler()\nX_train=sc.fit_transform(x_train) # x_train' den \u00f6\u011fren ve transform et\nX_test= sc.transform(x_test) # x_test te ise \u00f6\u011frenmi\u015f oldu\u011fun y\u00f6ntemi kullan\n\nfrom sklearn.linear_model import LogisticRegression\nlogr= LogisticRegression(random_state=0)\nlogr.fit(X_train,y_train)\n\ny_pred= logr.predict(X_test)\nprint(y_pred)\nprint(y_test)\n\nfrom sklearn.metrics import confusion_matrix\ncm= confusion_matrix(y_test,y_pred)\nprint(cm)","f967bb2b":"from sklearn.neighbors import KNeighborsClassifier\nknn= KNeighborsClassifier(n_neighbors=5, metric= \"minkowski\")\nknn.fit(X_train,y_train)\ny_pred= knn.predict(X_test)\ncm= confusion_matrix(y_test,y_pred)\nprint(cm)","4d5bce3c":"from sklearn.svm import SVC\nsvc= SVC(kernel=\"rbf\")\nsvc.fit(X_train,y_train)\n\ny_pred= svc.predict(X_test)\ncm=confusion_matrix(y_test,y_pred)\nprint(\"SVC\")\nprint(cm)\nprint(\"print accuracy of SVC algo: \",svc.score(X_test,y_test))\n","ec11662b":"from sklearn.naive_bayes import GaussianNB\ngnb= GaussianNB()\ngnb.fit(X_train,y_train)\n\ny_pred= gnb.predict(X_test)\ncm=confusion_matrix(y_test,y_pred)\nprint(\"GNB\")\nprint(cm)\nprint(\"print accuracy of naive bayes algo: \",gnb.score(X_test,y_test))","b0610e22":"from sklearn.tree import DecisionTreeClassifier\ndtc= DecisionTreeClassifier(criterion=\"entropy\")\ndtc.fit(X_train,y_train)\n\ny_pred=dtc.predict(X_test)\ncm=confusion_matrix(y_test,y_pred)\nprint(\"GNB\")\nprint(cm)\nprint(\"print accuracy of decision tree algo: \",dtc.score(X_test,y_test))","c15256c2":"from sklearn.ensemble import RandomForestClassifier\nrfc= RandomForestClassifier(n_estimators=10, criterion=\"entropy\")\n\nrfc.fit(X_train,y_train)\ny_pred=rfc.predict(X_test)\ncm=confusion_matrix(y_test,y_pred)\nprint(\"Random Forest Tree\")\nprint(cm)\nprint(\"print accuracy of Random Forest Tree algo: \",rfc.score(X_test,y_test))","f8b33047":"# NAIVE BAYES","9f5df0fc":"\u00c7ali\u015fan ID ve unvan al\u0131nmaz. \u00c7ali\u015fan Id al\u0131n\u0131rsa ezberleme olur. 1 nolu \u015fu kadar al\u0131yoru 2 bu kadar gibi...\n\nUnvan\u0131 almaya gerek yok \u00e7\u00fcnk\u00fc unvan seviyesi numerik olarak verilmi\u015f zaten \n\nUnvan seviyesi, k\u0131dem, puan ba\u011f\u0131ms\u0131z de\u011fi\u015fkenler.\n\nMaas ba\u011f\u0131ml\u0131 de\u011fi\u015fkendir.\n","aef624e8":"# RANDOM FOREST CLASSIFIER","1c967835":"# SVM","57d3c638":"## LOGISTIC REGRESSION","1af10937":"# THAT'S ALL","51d63b37":"# KNN ALgoritmas\u0131","f5d3143a":"* # ODEV2\n* #### Veri k\u00fcmesini indirin\n* #### Gerekli \/ Gereksiz ba\u011f\u0131ms\u0131z de\u011fi\u015fkenleri bulunuz\n* #### 5 farkl\u0131 y\u00f6nteme g\u00f6re regresyon modellerini \u00e7\u0131kar\u0131n (MLR,PR,SVR,DT,RF)\n* #### Y\u00f6ntemlerin ba\u015far\u0131lar\u0131n\u0131 kar\u015f\u0131la\u015ft\u0131r\u0131n\n* #### 10 y\u0131l tecribeli ve 100 puan alm\u0131\u015f bir CEO ve ayn\u0131 \u00f6zelliklere sahip bir m\u00fcd\u00fcr\u00fcn maa\u015flar\u0131n\u0131 5 y\u00f6ntemle de tahmin edip sonu\u00e7lar\u0131 yorumlay\u0131n\u0131z\n"}}