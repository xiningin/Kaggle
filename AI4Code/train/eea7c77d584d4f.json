{"cell_type":{"ab4c83b9":"code","a2650452":"code","f8382bc8":"code","5747ab0a":"code","e6bfcace":"code","8d1ad131":"code","c81ab2ba":"code","61c15bac":"code","41f16ccf":"code","a9824a9a":"code","b1037cea":"code","2182c72c":"code","ed6b77fc":"markdown","5f052792":"markdown","5f93dd86":"markdown","a7cf1d4a":"markdown","d1370713":"markdown","017fc2d0":"markdown","c36f4d4e":"markdown","9ac7544a":"markdown","4f56e49d":"markdown"},"source":{"ab4c83b9":"# Importing core libraries\nimport numpy as np\nimport pandas as pd\nfrom time import time\nimport pprint\nimport joblib\nfrom functools import partial\n\n# Suppressing warnings because of skopt verbosity\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Classifier\/Regressor\nfrom xgboost import XGBRegressor, DMatrix\n\n# Model selection\nfrom sklearn.model_selection import KFold, StratifiedKFold\n\n# Metrics\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import make_scorer\n\n# Skopt functions\nfrom skopt import BayesSearchCV\nfrom skopt.callbacks import DeadlineStopper, DeltaYStopper\nfrom skopt.space import Real, Categorical, Integer\n\n# Data processing\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.preprocessing import FunctionTransformer, OneHotEncoder\nfrom sklearn.preprocessing import StandardScaler, KBinsDiscretizer\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import make_pipeline\nfrom scipy.stats.mstats import winsorize","a2650452":"# Loading data \nX_train = pd.read_csv(\"..\/input\/30-days-of-ml\/train.csv\")\nX_test = pd.read_csv(\"..\/input\/30-days-of-ml\/test.csv\")","f8382bc8":"# Preparing data as a tabular matrix\ny_train = X_train.target\nX_train = X_train.set_index('id').drop('target', axis='columns')\nX_test = X_test.set_index('id')","5747ab0a":"# Stratifying the target\ny_stratified = pd.cut(y_train.rank(method='first'), bins=10, labels=False)","e6bfcace":"# Pointing out categorical features\ncategoricals = [item for item in X_train.columns if 'cat' in item]","8d1ad131":"# Dealing with categorical data using get_dummies\ndummies = pd.get_dummies(X_train.append(X_test)[categoricals])\nX_train[dummies.columns] = dummies.iloc[:len(X_train), :]\nX_test[dummies.columns] = dummies.iloc[len(X_train): , :]\ndel(dummies)","c81ab2ba":"# Dealing with categorical data using OrdinalEncoder\nordinal_encoder = OrdinalEncoder()\nX_train[categoricals] = ordinal_encoder.fit_transform(X_train[categoricals])\nX_test[categoricals] = ordinal_encoder.transform(X_test[categoricals])","61c15bac":"# Feature selection (https:\/\/www.kaggle.com\/lucamassaron\/tutorial-feature-selection-with-boruta-shap)\nimportant_features = ['cat8_E', 'cont0', 'cont5', 'cont7', 'cont8', 'cat1_A', 'cont2', 'cont13', \n                      'cont3', 'cont10', 'cont1', 'cont9', 'cont11', 'cat1', 'cat8_C', 'cont6', \n                      'cont12', 'cat5', 'cat3_C', 'cont4', 'cat8']\n\nX_train = X_train[important_features]\nX_test = X_test[important_features]","41f16ccf":"best_params = dict([('colsample_bytree', 0.13641457979856397),\n                    ('learning_rate', 0.09604083138419779),\n                    ('max_depth', 5),\n                    ('n_estimators', 2206),\n                    ('reg_alpha', 68.85614181114505),\n                    ('reg_lambda', 9.260067417192285),\n                    ('subsample', 0.8980218163372579)])","a9824a9a":"# Cross-validation prediction\nfolds = 10\nseeds = 5\nusing_GPU = False\n\npredictions = np.zeros(len(X_test))\nrmse = list()\n\nfor seed in range(seeds): # multiple seeds are everaged for stable results\n    \n    skf = StratifiedKFold(n_splits=folds,\n                          shuffle=True, \n                          random_state=seed)\n\n    for k, (train_idx, val_idx) in enumerate(skf.split(X_train, y_stratified)):\n\n        # Transferring the best parameters to our basic regressor\n        if using_GPU is True:\n            tree_method = 'gpu_hist'\n        else:\n            tree_method = 'hist'\n\n        reg = XGBRegressor(random_state=0, booster='gbtree',  \n                           objective='reg:squarederror', \n                           **best_params,\n                           tree_method=tree_method\n                          )\n        \n        # Winsorizing lower bounds of the training target\n        y_train_w = np.array(winsorize(y_train[train_idx], [0.002, 0.0]))\n\n        reg.fit(X_train.iloc[train_idx, :], y_train_w,\n                eval_set=[(X_train.iloc[val_idx], y_train[val_idx])],\n                early_stopping_rounds=30,\n                verbose=100\n               )\n\n        val_preds = reg.predict(X_train.iloc[val_idx, :])\n        val_rmse = mean_squared_error(y_true=y_train[val_idx], y_pred=val_preds, squared=False)\n        print(f\"Seed {seed} Fold {k} RMSE: {val_rmse:0.5f}\")\n        rmse.append(val_rmse)\n        predictions += reg.predict(X_test).ravel()\n    \npredictions \/= (folds * seeds)\nprint(f\"repeated CV RMSE: {np.mean(rmse):0.5f} (std={np.std(rmse):0.5f})\")","b1037cea":"# Preparing the submission\nsubmission = pd.DataFrame({'id':X_test.index, \n                           'target': predictions})\n\nsubmission.to_csv(\"submission.csv\", index = False)","2182c72c":"submission","ed6b77fc":"In this follow-up of the tutorial, we will apply some of the best results found by optimization, feature engineering and feature selection in order to obtain the best local cv and best public LB.","5f052792":"In optimizing your model for this competition, you may wonder if there is a way to:\n\n * Leverage the information that you get as you explore the hyper-parameter space\n\n * Not necessarily become be an expert of a specific ML algorithm\n\n * Quickly find an optimization","5f93dd86":"# Data preparation","a7cf1d4a":"![image.png](attachment:f8d4cbb2-09ef-4c55-a3f9-a93e6ddd8033.png)","d1370713":"The key idea behind Bayesian optimization is that we optimize a proxy function (the surrogate function) instead than the true objective function (what actually grid search and random search both do). This holds if testing the true objective function is costly (if it is not, then we simply go for random search.\n\nBayesian search balances exploration against exploitation. At start it randomly explores, doing so it builds up a surrogate function of the objective. Based on that surrogate function it exploits an initial approximate knowledge of how the predictor works in order to sample more useful examples and minimize the cost function at a global level, not a local one.\n\nBayesian Optimization uses an acquisition function to tell us how promising an observation will be. In fact, to rule the tradeoff between exploration and exploitation, the algorithm defines an acquisition function that provides a single measure of how useful it would be to try any given point.","017fc2d0":"# Kaggle\u2019s 30 days of machine learning:\n## Scikit-optimize for XGBoost (regression tutorial)\n\n![image.png](attachment:49860e47-cfb0-4ad8-8edd-9bbc7e6d1b0c.png)","c36f4d4e":"The answer is:\n\n**Bayesian Optimization** (*SNOEK, Jasper; LAROCHELLE, Hugo; ADAMS, Ryan P. Practical bayesian optimization of machine learning algorithms. In: Advances in neural information processing systems. 2012. p. 2951-2959*)","9ac7544a":"# Prediction on test data","4f56e49d":"**Machine learning algorithms have lots of knobs, and success often comes from twiddling them a lot.**\n\n*(DOMINGOS, Pedro. A few useful things to know about machine learning.\u00a0Communications of the ACM, 2012, 55.10: 78-87.)*"}}