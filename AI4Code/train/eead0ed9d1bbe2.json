{"cell_type":{"454388bc":"code","fe525672":"code","97136676":"code","ab825eda":"code","b5e862a2":"code","2b31b9a5":"code","1a99ab5f":"code","5d295f20":"code","d7f949c6":"code","eb4cb39f":"code","217ba4c3":"code","7d3f8b77":"code","9913ebc7":"code","7f3f51b8":"code","0ef2b14c":"code","21acd4b6":"code","8644b0eb":"code","c1a42b8c":"code","900c0142":"code","bd4ba945":"code","cc9d1a62":"code","521850b2":"code","f41271b0":"markdown","5288c56d":"markdown","6cc984cc":"markdown","53272ad9":"markdown","e69be7cc":"markdown","a213d6ac":"markdown","8ec5932d":"markdown","d47af244":"markdown","a9bf82f2":"markdown","b14d078a":"markdown","4f306fc7":"markdown","ead50e20":"markdown","15de27e8":"markdown"},"source":{"454388bc":"import pandas as pd\nimport numpy as np\n\nmel_housing_data = pd.read_csv(\"..\/input\/Melbourne_housing_FULL.csv\")\n\n","fe525672":"# 2. View the dataset\n\nmel_housing_data.head()   # first five records","97136676":"# number of rows and colums\n\nmel_housing_data.shape   ","ab825eda":"# 3. See the structure and the summary of the dataset to understand the data.\n\nmel_housing_data.info()  # view data types","b5e862a2":"# object attributes will be considered as categorical variables. \n\n# identify object columns\n\nobject_att = mel_housing_data.select_dtypes(['object']).columns\nobject_att\n\n","2b31b9a5":"# convert object to categorical attributes\n\nfor objects in object_att:\n    mel_housing_data[objects] = mel_housing_data[objects].astype('category')  \n    \n# postcode is float type but it should be categorical\n\nmel_housing_data['Postcode'] = mel_housing_data[\"Postcode\"].astype('category')\n\nmel_housing_data.info()   # objects variables has been identified as categorical attributes.","1a99ab5f":"# Before removing duplicate column values\n\nmel_housing_data.shape","5d295f20":"# to remove duplicate rows for all columns.\n\nmel_housing_data.drop_duplicates(keep=False,inplace=True)\n\n#After removing duplicate values\n\nmel_housing_data.shape","d7f949c6":"# looking for duplicate columns\n# by looking at the data dictionary,Rooms and bedrooms2 must be having similar values. Need to check for duplicacy.\n\nmel_housing_data['Rooms v Bedroom2'] = mel_housing_data['Rooms'] - mel_housing_data['Bedroom2']\nmel_housing_data['Rooms v Bedroom2'].value_counts()\n\n# frequency of 0 difference in Rooms and Bedroom2 is maximum, we shall take this as duplicate.  need to remove it","eb4cb39f":"# drop columns  \"Bedroom2\" and \"Rooms v Bedroom2\"\n\nmel_housing_data = mel_housing_data.drop(['Bedroom2','Rooms v Bedroom2'],1)\n\n# After removing duplicate values\nmel_housing_data.shape","217ba4c3":"mel_housing_data.info()\n\n# by looking at the data types for all the attributes.  \n# 'Data' should be converted into DateTime object. ","7d3f8b77":"# convert Date to datetime object.\n\nmel_housing_data['Date'] = pd.to_datetime(mel_housing_data['Date'])\nmel_housing_data.info()","9913ebc7":"# missing value against each columns.  \n\ntotal_missing = mel_housing_data.isnull().sum().sort_values(ascending=False)\ntotal_missing_df = pd.DataFrame(total_missing)\npercentage_miss = (mel_housing_data.isnull().sum()\/mel_housing_data.isnull().count()*100).sort_values(ascending=False)\nmissing_data = pd.concat([total_missing_df, percentage_miss], axis=1, keys=['total_missing_df', 'percentage_miss'])\n#missing_data\nmissing_data_withoutnan = missing_data[missing_data > 0].dropna()\nmissing_data_withoutnan\n","7f3f51b8":"# total_missing_index = pd.DataFrame(missing_data_withoutnan.index)\n# total_missing_index[0][1]\n\n# for cat in total_missing_index[0]:\n#     #print (cat)\n#     cats = pd.Series(cat)\n#     print (mel_housing_data[cats].mean())\n#     mel_housing_data[cats].fillna(mel_housing_data[cats].mean(), inplace=True)\n","0ef2b14c":"# dropping the missing data.\n\nmel_housing_data = mel_housing_data.dropna()\nmel_housing_data.shape","21acd4b6":"mel_housing_data.describe()","8644b0eb":"\n# we can exclude all the attributes related to category and datetime\nmel_housing_subset = mel_housing_data.select_dtypes(exclude=['category','datetime64[ns]'])\n\n# as per statistical analysis,  SD for latitude and longitude are close to 0.  It means all the values\n# contributing to mean are identical.  so, there won't be any outliers.  we can exclude them. \n\nmel_housing_subset = mel_housing_subset.drop(['Lattitude','Longtitude'],axis=1)\nmel_housing_subset","c1a42b8c":"mel_housing_subset.shape","900c0142":"# describe() will output the percentile distribution of each column\n\nmel_housing_subset.describe()","bd4ba945":"# capping and flooring of 5% on every columns.\n\nimport pandas as pd\nfrom pandas import Series\n\nfrom scipy.stats import mstats\n%matplotlib inline\n\n# Truncate values to the 5th and 95th percentiles\nRooms = pd.Series(mstats.winsorize(mel_housing_subset['Rooms'], limits=[0.05, 0.05])) \nPrice = pd.Series(mstats.winsorize(mel_housing_subset['Price'], limits=[0.05, 0.05])) \nDistance = pd.Series(mstats.winsorize(mel_housing_subset['Distance'], limits=[0.05, 0.05])) \nBathroom = pd.Series(mstats.winsorize(mel_housing_subset['Bathroom'], limits=[0.05, 0.05])) \nCar = pd.Series(mstats.winsorize(mel_housing_subset['Car'], limits=[0.05, 0.05])) \nLandsize = pd.Series(mstats.winsorize(mel_housing_subset['Landsize'], limits=[0.05, 0.05])) \nBuildingArea = pd.Series(mstats.winsorize(mel_housing_subset['BuildingArea'], limits=[0.05, 0.05])) \nYearBuilt = pd.Series(mstats.winsorize(mel_housing_subset['YearBuilt'], limits=[0.05, 0.05])) \nPropertycount = pd.Series(mstats.winsorize(mel_housing_subset['Propertycount'], limits=[0.05, 0.05])) \n\nmel_housing_subset= pd.concat([Rooms, Price, Distance, Bathroom, Car, Landsize, BuildingArea, YearBuilt, Propertycount], axis=1, keys =['Rooms', 'Price', 'Distance', 'Bathroom', 'Car', 'Landsize', 'BuildingArea', 'YearBuilt', 'Propertycount'])\nmel_housing_subset\n","cc9d1a62":"# import seaborn as sns\n# %matplotlib inline\n# sns.boxplot(x=mel_housing_subset['Rooms'])","521850b2":"# from scipy import stats\n# import numpy as np\n# z = np.abs(stats.zscore(mel_housing_subset))\n# print(z)\n\n# mel_housing_subset = mel_housing_subset[(z < 3).all(axis=1)]\n# mel_housing_subset.shape","f41271b0":"Data Understanding\n\n1. Import the data set in Python.","5288c56d":"Missing value treatment: Check which variables have missing values and use appropriate treatments. \n        For each of the variables, find the number of missing values and provide the value that they have been imputed with.","6cc984cc":"Duplicate values: Identify if the datasets have duplicate values or not and remove the duplicate values. \n\nFind out the number of rows present in the dataset\n    Before removing duplicate values\n    After removing duplicate values","53272ad9":"4. Find out the number of:\n\t     a.) Numeric attributes:\n\t     b.) Categorical attributes:","e69be7cc":"Identify the variables : Make a subset of the dataset with all the numeric variables.\n","a213d6ac":"# Data Understanding\n\n1. Import the data set in Python.\n2. View the dataset\n3. See the structure and the summary of the dataset to understand the data.\n4. Find out the number of:\n\t     a.) Numeric attributes:\n\t     b.) Categorical attributes:","8ec5932d":"Outlier Treatment: \n\t\tIdentify the variables : Make a subset of the dataset with all the numeric variables. \n\t\tOutliers : For each variable of this subset, carry out the outlier detection. Find out the percentile distribution of each variable and carry out capping and flooring for outlier values.  ","d47af244":"For how many attributes did you need to change the data type?\n    We changed object type of Date attribute to datetime object.  ","a9bf82f2":"Variable type: Check if all the variables have the correct variable type, based on the data dictionary. If not, then change them.","b14d078a":"Feature Transformation:\n\t\tIdentify variables that have non-linear trends.\n\t\tHow many variables have non-linear trends?\n\t\tTransform them (as required)","4f306fc7":"### Data Preparation Feature Engineering:","ead50e20":"Outliers : For each variable of this subset, carry out the outlier detection. Find out the percentile distribution of each variable and carry out capping and flooring for outlier values.  ","15de27e8":"### Data Preparation : Data Cleaning"}}