{"cell_type":{"b4e91e2e":"code","5d7922ca":"code","711c2370":"code","c3ec6acc":"code","215ac06d":"code","b733c435":"code","e33cc30f":"code","e4dac383":"code","653c7ab8":"code","eae12f6d":"code","af07193c":"code","bb54aff7":"code","0439cd5b":"code","87820b46":"code","ca0f9a5a":"markdown","1e2b5561":"markdown","0b649537":"markdown"},"source":{"b4e91e2e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","5d7922ca":"import sys\nimport numpy\nimport pandas\nimport matplotlib\nimport seaborn\nimport scipy\n\nprint('Python: {}'.format(sys.version))\n","711c2370":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns","c3ec6acc":"data = pd.read_csv('..\/input\/creditcardfraud\/creditcard.csv')\ndata.head()","215ac06d":"data.shape","b733c435":"print(data.columns)","e33cc30f":"# Print the shape of the data\ndata = data.sample(frac=0.1, random_state = 1) # taking only one percent of data \nprint(data.shape)\nprint(data.describe())\n\n# V1 - V28 are the results of a PCA Dimensionality reduction to protect user identities and sensitive features","e4dac383":"# Plot histograms of each parameter \ndata.hist(figsize = (20, 20))\nplt.show()","653c7ab8":"# Determine number of fraud cases in dataset\n\nFraud = data[data['Class'] == 1]\nValid = data[data['Class'] == 0]\n\noutlier_fraction = len(Fraud)\/float(len(Valid))\nprint(outlier_fraction)\n\nprint('Fraud Cases: {}'.format(len(data[data['Class'] == 1])))\nprint('Valid Transactions: {}'.format(len(data[data['Class'] == 0])))","eae12f6d":"# Correlation matrix\ncorrmat = data.corr()\nfig = plt.figure(figsize = (12, 9))\n\nsns.heatmap(corrmat, vmax = .8, square = True)\nplt.show()\n","af07193c":"# Get all the columns from the dataFrame\ncolumns = data.columns.tolist()\n\n# Filter the columns to remove data we do not want\ncolumns = [c for c in columns if c != 'Class']\n\n# Store the variable we'll be predicting on\ntarget = \"Class\"\n\nX = data[columns]\nY = data[target]\n\n# Print shapes\nprint(X.shape)\nprint(Y.shape)","bb54aff7":"import sklearn\nfrom sklearn.metrics import accuracy_score, classification_report\nfrom sklearn.ensemble import IsolationForest\nfrom sklearn.neighbors import LocalOutlierFactor\n\n# define random states\nstate = 1\n\nclassifier = {\n    'isolation_forest':IsolationForest(max_samples=len(X), contamination=outlier_fraction,\n                                      random_state = state),\n    'local_outlierfactor':LocalOutlierFactor(\n    n_neighbors=20 , contamination= outlier_fraction, novelty=True\n    )\n}\n\n","0439cd5b":"# fitting our model \n# the model dont know yet that how many outliers are there in our X data \nplt.figure(figsize=(9, 7))\nn_outliers = len(Fraud)\n\nfor i ,(clf_name, clf) in enumerate(classifier.items()):\n    if clf_name == 'localoutlierfactor':\n        y_pred = clf.fit_predict(X)\n        scores_pred = clf.negative_outlier_factor_\n    else:\n        clf.fit(X)\n        scores_pred = clf.decision_function(X)\n        y_pred = clf.predict(X)\n        \n    # Reshape the prediction values to 0 for valid, 1 for fraud. \n    y_pred[y_pred == 1] = 0\n    y_pred[y_pred == -1] = 1    \n    \n    \n    n_errors = (y_pred != Y).sum()\n    \n    # Run classification metrics\n    print('{}: {}'.format(clf_name, n_errors))\n    print(accuracy_score(Y, y_pred))\n    print(classification_report(Y, y_pred))","87820b46":"# from here we can say that our model couldnt compeletely identify all those fraud cases  , we can try different algorithms and \n# and we can try classification on this as well","ca0f9a5a":"### 3. Unsupervised Outlier Detection\n\nNow that we have processed our data, we can begin deploying our machine learning algorithms. We will use the following techniques:\n\n* #### Local Outlier Factor (LOF)\n\nThe anomaly score of each sample is called Local Outlier Factor. It measures the local deviation of density of a given sample with respect to its neighbors. It is local in that the anomaly score depends on how isolated the object is with respect to the surrounding neighborhood.\n\n* #### Isolation Forest Algorithm\n\nThe IsolationForest \u2018isolates\u2019 observations by randomly selecting a feature and then randomly selecting a split value between the maximum and minimum values of the selected feature.\n\nSince recursive partitioning can be represented by a tree structure, the number of splittings required to isolate a sample is equivalent to the path length from the root node to the terminating node.\n\nThis path length, averaged over a forest of such random trees, is a measure of normality and our decision function.\n\nRandom partitioning produces noticeably shorter paths for anomalies. Hence, when a forest of random trees collectively produce shorter path lengths for particular samples, they are highly likely to be anomalies.","1e2b5561":"* #### 2. The Data Set\nIn the following cells, we will import our dataset from a .csv file as a Pandas DataFrame. Furthermore, we will begin exploring the dataset to gain an understanding of the type, quantity, and distribution of data in our dataset. For this purpose, we will use Pandas' built-in describe feature, as well as parameter histograms and a correlation matrix.","0b649537":"#### Credit Card Fraud Detection\n\nThroughout the financial sector, machine learning algorithms are being developed to detect fraudulent transactions. In this project, that is exactly what we are going to be doing as well. Using a dataset of of nearly 28,500 credit card transactions and multiple unsupervised anomaly detection algorithms, we are going to identify transactions with a high probability of being credit card fraud. In this project, we will build and deploy the following two machine learning algorithms:\n\n* Local Outlier Factor (LOF)\n* Isolation Forest Algorithm\n\nFurthermore, using metrics suchs as precision, recall, and F1-scores, we will investigate why the classification accuracy for these algorithms can be misleading.\n\nIn addition, we will explore the use of data visualization techniques common in data science, such as parameter histograms and correlation matrices, to gain a better understanding of the underlying distribution of data in our data set. Let's get started!"}}