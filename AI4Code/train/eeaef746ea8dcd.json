{"cell_type":{"2b6d5ebf":"code","cc33e7c5":"code","2af13a66":"code","d6a27d85":"code","8f416337":"code","2a9824ef":"code","d05e2ed5":"code","1219b360":"code","23e39938":"code","7ff27758":"code","b4de95cf":"code","918c420d":"code","78743d98":"code","ce968b41":"code","75b9967b":"code","1920dc90":"code","7f5ee9bc":"code","b9c5ccd7":"code","d9a35d72":"code","e131dcbe":"code","e2213b4b":"code","9ab0bf80":"code","fd89d5fb":"code","a090342a":"code","674d2cc7":"markdown","fbed8947":"markdown","77f19825":"markdown","5d41aaf9":"markdown","6d6c7b0e":"markdown","b4904e59":"markdown","7d4acff6":"markdown","c408f1dc":"markdown","02c2fb2a":"markdown","0f000fb4":"markdown","3f97c09c":"markdown","a746fa2d":"markdown","f1c06fad":"markdown"},"source":{"2b6d5ebf":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","cc33e7c5":"%matplotlib inline\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport pandas_profiling as pp\nimport matplotlib.pyplot as plt\nfrom colorama import Fore, Style\nsns.set()","2af13a66":"from sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import confusion_matrix, classification_report, accuracy_score\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split","d6a27d85":"df = pd.read_csv('..\/input\/magic-gamma-telescope-dataset\/telescope_data.csv')","8f416337":"df.head().T","2a9824ef":"df.info()","d05e2ed5":"df.profile_report(\n    title='Profiling Report for the MAGIC Telescope Dataset'\n).to_notebook_iframe()","1219b360":"x = df.drop(['class'], axis=1)\ny = df['class']","23e39938":"sns.pairplot(x)\nplt.show()","7ff27758":"le = LabelEncoder()\ny = le.fit_transform(y)","b4de95cf":"x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25, random_state=0)","918c420d":"scaler = StandardScaler()\nx_train, x_test = scaler.fit_transform(x_train), scaler.transform(x_test)","78743d98":"accuracies = {}","ce968b41":"from sklearn.ensemble import RandomForestClassifier\n\nmodel = RandomForestClassifier(\n    n_estimators=100, \n    criterion='entropy', \n    random_state=0\n)\nmodel.fit(x_train, y_train)\n\ny_pred = model.predict(x_test)\nsns.heatmap(confusion_matrix(y_test, y_pred), cmap='YlGnBu', annot=True)\nplt.show()\naccuracies['RandomForest'] = accuracy_score(y_test, y_pred) * 100\nprint(f\"Accuracy: {accuracies['RandomForest']:.2f}%\\n\")\nprint(classification_report(y_test, y_pred, target_names=['gamma', 'hadron']))","75b9967b":"from sklearn.linear_model import SGDClassifier\n\nmodel = SGDClassifier(random_state=0)\nmodel.fit(x_train, y_train)\n\ny_pred = model.predict(x_test)\nsns.heatmap(confusion_matrix(y_test, y_pred), cmap='YlGnBu', annot=True)\nplt.show()\naccuracies['LogReg'] = accuracy_score(y_test, y_pred) * 100\nprint(f\"Accuracy: {accuracies['LogReg']:.2f}%\\n\")\nprint(classification_report(y_test, y_pred, target_names=['gamma', 'hadron']))","1920dc90":"from sklearn.svm import SVC\n\nmodel = SVC(C=57, random_state=0)\nmodel.fit(x_train, y_train)\n\ny_pred = model.predict(x_test)\nsns.heatmap(confusion_matrix(y_test, y_pred), cmap='YlGnBu', annot=True)\nplt.show()\naccuracies['SVM'] = accuracy_score(y_test, y_pred) * 100\nprint(f\"Accuracy: {accuracies['SVM']:.2f}%\\n\")\nprint(classification_report(y_test, y_pred, target_names=['gamma', 'hadron']))","7f5ee9bc":"from sklearn.tree import DecisionTreeClassifier\nmodel = DecisionTreeClassifier(criterion='entropy', random_state=0)\nmodel.fit(x_train, y_train)\ny_pred = model.predict(x_test)\nsns.heatmap(confusion_matrix(y_test, y_pred), cmap='YlGnBu', annot=True)\nplt.show()\naccuracies['DT'] = accuracy_score(y_test, y_pred) * 100\nprint(f\"Accuracy: {accuracies['DT']:.2f}%\\n\")\nprint(classification_report(y_test, y_pred, target_names=['gamma', 'hadron']))","b9c5ccd7":"import tensorflow as tf\n\nmodel = tf.keras.models.Sequential([\n    tf.keras.layers.Dense(256, activation='relu'),\n    tf.keras.layers.Dropout(0.2),\n    tf.keras.layers.Dense(512, activation='relu'),\n    tf.keras.layers.Dropout(0.2),\n    tf.keras.layers.Dense(256, activation='relu'),\n    tf.keras.layers.Dropout(0.2),\n    tf.keras.layers.Dense(2, activation='sigmoid'),\n])\n\nmodel.compile(\n    optimizer='adam',\n    loss='sparse_categorical_crossentropy',\n    metrics=['accuracy']\n)","d9a35d72":"x_train_nn, x_cv_nn, y_train_nn, y_cv_nn = train_test_split(x_train, y_train, test_size=0.25, random_state=0)","e131dcbe":"num_epochs = 6\nhistory = model.fit(\n    x_train_nn, y_train_nn, epochs=num_epochs, \n    validation_data=(x_cv_nn, y_cv_nn),\n    steps_per_epoch=x_train.shape[0] \/\/ num_epochs,\n    callbacks=[\n        tf.keras.callbacks.ReduceLROnPlateau(patience=2, verbose=2)\n    ]\n)","e2213b4b":"loss_train = history.history['loss']\nloss_validation = history.history['val_loss']\nepochs = range(1, num_epochs + 1)\nplt.plot(epochs, loss_train, 'g', label='Training')\nplt.plot(epochs, loss_validation, 'b', label='Validation')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.title('Loss')\nplt.legend()\nplt.show()","9ab0bf80":"acc_train = history.history['accuracy']\nacc_validation = history.history['val_accuracy']\nepochs = range(1, num_epochs + 1)\nplt.plot(epochs, acc_train, 'g', label='Training')\nplt.plot(epochs, acc_validation, 'b', label='Validation')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.title('Accuracy')\nplt.legend()\nplt.show()","fd89d5fb":"y_pred = model.predict(x_test)\ny_pred = [np.argmax(y) for y in y_pred]\nsns.heatmap(confusion_matrix(y_test, y_pred), cmap='YlGnBu', annot=True)\nplt.show()\naccuracies['NN'] = accuracy_score(y_test, y_pred) * 100\nprint(f\"Accuracy: {accuracies['NN']:.2f}%\\n\")\nprint(classification_report(y_test, y_pred, target_names=['gamma', 'hadron']))","a090342a":"ax = sns.barplot(list(accuracies.keys()), list(accuracies.values()))\nfor p in ax.patches:\n    ax.annotate(\n        f'{p.get_height():2.2f}%', \n        (p.get_x() + p.get_width() \/ 2., p.get_height()), \n        ha = 'center', va = 'center', \n        xytext = (0, -20), textcoords = 'offset points'\n    )\nplt.xlabel('Models')\nplt.ylabel('Accuracy')\nplt.show()","674d2cc7":"# Dependencies","fbed8947":"## Neural Network","77f19825":"# Results","5d41aaf9":"# Feature Scaling","6d6c7b0e":"## Random Forest","b4904e59":"# Model Building and Evaluation","7d4acff6":"## Support Vector Machine","c408f1dc":"# Conclusion","02c2fb2a":"Tree-based Classification Models tend to perform better on the dataset.","0f000fb4":"## Logistic Regression","3f97c09c":"# Train-Test Split","a746fa2d":"## Decision Tree","f1c06fad":"# Data Exploration"}}