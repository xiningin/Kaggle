{"cell_type":{"6767b3a1":"code","0c3d5087":"code","02f91e7a":"code","c93b44ac":"code","64b6c35d":"code","90aad631":"code","538b23da":"code","3f574900":"code","b9e1cf76":"code","1564e7ca":"code","ea38dae6":"code","2b1dfb79":"code","e9043009":"code","920e82f4":"code","13ac9021":"code","c5663b12":"code","4f470575":"code","cb499afb":"code","0563613a":"code","18f41c29":"code","1fa0ee9c":"code","791859f1":"code","441390a2":"code","7f07ea2b":"code","7a036b13":"code","e722eafd":"code","193d1345":"code","fb272a34":"code","f2d8cc18":"code","231b0cf2":"code","43c353a5":"code","54970e24":"code","bb7a78ef":"code","12719a16":"code","a077c986":"code","7697167d":"code","b574313a":"code","aa1b7a94":"code","48c95816":"code","64bdfde8":"code","64242cb3":"code","45c22ce6":"code","dec966aa":"code","b5573f7d":"code","8abbe838":"code","1e350c1e":"code","70917548":"code","1df4439f":"code","58bff997":"code","c8feba5a":"code","8c91b576":"code","b25dd6b4":"code","e9061a10":"code","172bb505":"code","80bbff6b":"code","3c6fc1f6":"code","09481466":"code","005c3282":"code","cf0f82a8":"code","ae175b43":"code","bf67a27a":"code","e9a59843":"code","6525d714":"code","2ebf61d1":"code","e10a39b6":"code","53da60b6":"code","b64ce0ea":"code","d4c7ed68":"code","37a9a767":"code","12db1f60":"code","f5d1980a":"code","619c5d1e":"markdown","5903c2ff":"markdown","2393eb81":"markdown","fd6830f1":"markdown","2c4a1dab":"markdown","f7ca8e18":"markdown","5a261845":"markdown","5472e3bd":"markdown","efe24467":"markdown","113edbdb":"markdown","04fff3e5":"markdown","f0f4d3ee":"markdown","4b90b10f":"markdown","95754031":"markdown","f7d4ce5e":"markdown","a2523b99":"markdown","529bcca9":"markdown","cac09e64":"markdown","305658af":"markdown","4a2be242":"markdown","7cac7913":"markdown","e33a562f":"markdown","7696c820":"markdown","0d91980f":"markdown","acaab7af":"markdown","51e87f57":"markdown","46658316":"markdown","97fc86c9":"markdown","3cfcba75":"markdown","92ffa22e":"markdown","3d5f1448":"markdown"},"source":{"6767b3a1":"# Importing the necessary packages\nimport numpy as np # linear algebra\nimport pandas as pd \nimport numpy as np\nimport os\npd.set_option('display.max_columns',100)\npd.set_option('display.max_rows',100)\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.impute import KNNImputer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.decomposition import IncrementalPCA\nfrom sklearn.linear_model import LinearRegression, Lasso, Ridge\nfrom sklearn.model_selection import GridSearchCV,cross_val_score\nfrom sklearn import linear_model, metrics\nfrom sklearn.feature_selection import RFE\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom IPython.display import Image\n","0c3d5087":"#Load the data set\ndata = pd.read_csv(\"..\/input\/ml-lab-II-c28\/train.csv\")\nunseen = pd.read_csv(\"..\/input\/ml-lab-II-c28\/test.csv\")\nsample = pd.read_csv(\"..\/input\/ml-lab-II-c28\/sample.csv\")\ndata_dict = pd.read_csv(\"..\/input\/ml-lab-II-c28\/data_dictionary.csv\")\nprint(data.shape)\nprint(unseen.shape)\nprint(sample.shape)","02f91e7a":"#Checking the description of the Data\nfor i in data_dict.iterrows():\n    print(i[1]['column'],':',i[1]['description'])","c93b44ac":"Image(filename=\"..\/input\/ml-lab-II-c28\/features.png\")","64b6c35d":"#Head\ndata.head()","90aad631":"#Details of the dataframe\ndata.info()","538b23da":"#checking the Number of Null values\nmissing_data_percent=round(100 *(data.isnull().sum()\/len(data.index)),2)\nmissing_data_percent","3f574900":"#checking null value in descending order\ndata.isnull().sum().sort_values(ascending=False)","b9e1cf76":"# To check each variable datatype \ndata.info(verbose=1,null_counts=True)","1564e7ca":"# checking the shape of the data again \ndata.shape","ea38dae6":"# Checking if there are columns with one unique value since it won't affect our analysis\ndata.nunique(dropna=True)","2b1dfb79":"# imputing missing values\nimpute_features = missing_data_percent[missing_data_percent.gt(0)].index\nimpute_features","e9043009":"# Imputed with KNN Imputer\nimp= KNNImputer()\ndata[impute_features] = imp.fit_transform(data[impute_features])","920e82f4":"#Checking the Null values again \nround(100 *(data.isnull().sum()\/len(data.index)),2)\n","13ac9021":"# choose features and target\nX = data.iloc[:,2:60] # select all columns except URL and TARGET\ny = data[\"shares\"].values.ravel() #target:number of shares\nplt.figure(figsize= (8,6))\nplt.hist(np.log(y),color=\"lightgreen\")\nplt.xlabel(\"Shares\")\nplt.ylabel(\"Count of Shares\")\nplt.title(\"Distribution of log of numbers of shares\")\nplt.show()\n","c5663b12":"# plot the feature correlation heatmap\nplt.figure(figsize=(40,18))\nsns.heatmap(X.corr(),cmap=\"YlGnBu\",annot=True) \nplt.show()","4f470575":"# number of words in the content vs number of shares\nplt.scatter(data[\"n_tokens_content\"], data[\"shares\"])\nplt.title(\"Content Lengths vs Number of Shares\", size=15)\nplt.xlabel(\"Number of Words\", size=12)\nplt.ylabel(\"Number of Shares\", size=12)\nplt.show()","cb499afb":"# text sentiment polarity vs number of shares\nplt.scatter(data[\"global_sentiment_polarity\"], data[\"shares\"])\nplt.title(\"Text Sentiment Polarity vs Number of Shares\", size=15)\nplt.xlabel(\"Polarity Score\", size=12)\nplt.ylabel(\"Number of Shares\", size=12)\nplt.show()","0563613a":"plt.figure(figsize= (10,10))\nshares = []\ncols = ['weekday_is_monday', 'weekday_is_tuesday', 'weekday_is_wednesday','weekday_is_thursday', 'weekday_is_friday', 'weekday_is_saturday',\n       'weekday_is_sunday']\nfor d in cols:\n    shares.append(data[data[d] == 1]['shares'].median())\n\nax = sns.barplot(x = cols, y = shares)\nax.set(xlabel = 'day of week', ylabel = 'median_shares')\nplt.xticks(rotation = 60)\nplt.show()","18f41c29":"plt.figure(figsize= (10,8))\nchannel = []\ncols = ['data_channel_is_lifestyle',\n       'data_channel_is_entertainment', 'data_channel_is_bus',\n       'data_channel_is_socmed', 'data_channel_is_tech',\n       'data_channel_is_world']\nfor d in cols:\n    channel.append(data[data[d]==1]['shares'].median())\n\nax = sns.barplot(x = cols, y = channel)\nax.set(xlabel = 'data_channel', ylabel = 'median_shares')\nplt.xticks(rotation = 60)\nplt.show()","1fa0ee9c":"#checking for outliers\nplt.figure(figsize=(10,8))\nplt.xticks(rotation =45)\nsns.boxplot(data = X)\nplt.show()","791859f1":"# Finding negative value columns.\nnegcols=X.columns[(X<=0).any()]\nnegcols","441390a2":"#converting negative values to positive values .\n\nfor i in negcols:\n    m=X[i].min()\n    name=i \n    print(name)\n    X[name]=((X[i]+1)-m)","7f07ea2b":"# Checking negative columns\nnegcols=X.columns[(X<=0).any()]\nnegcols","7a036b13":"# log transform with constant 10000 for real numbers\nX = np.log((10000 + X))","e722eafd":"# Treating Outlier\nfor col in X.columns:\n    percentiles = X[col].quantile([0.01, 0.99]).values\n    X[col][X[col] <= percentiles[0]] = percentiles[0]\n    X[col][X[col] >= percentiles[1]] = percentiles[1]","193d1345":"#checking for outliers again\nplt.figure(figsize=(15,8))\nplt.xticks(rotation=45)\nsns.boxplot(data = X)\nplt.show()\n","fb272a34":"cols = ['kw_min_min',\n'kw_max_min',\n'kw_avg_min',\n'kw_min_max',\n'kw_max_max',\n'kw_avg_max',\n'kw_min_avg',\n'kw_max_avg',\n'kw_avg_avg',\n'data_channel_is_world',\n 'self_reference_min_shares',\n 'self_reference_max_shares',\n 'data_channel_is_tech',\n 'data_channel_is_socmed',\n'data_channel_is_lifestyle',\n'data_channel_is_entertainment',\n'data_channel_is_bus',\n'data_channel_is_socmed',\n'data_channel_is_tech',\n'data_channel_is_world',\n 'num_keywords',\n'average_token_length',\n'num_videos',\n'num_hrefs',\n'num_self_hrefs',\n'num_imgs',\n'num_videos',\n'n_non_stop_unique_tokens',\n 'n_non_stop_words',\n 'n_unique_tokens',\n 'n_tokens_content']\nX = X.drop(cols, axis=1)","f2d8cc18":"#checking for outliers again\nplt.figure(figsize=(40,10))\nplt.xticks(rotation=45)\nsns.boxplot(data = X)\nplt.show()","231b0cf2":"# Looking at the correlation table\nplt.figure(figsize = (25,15))\nsns.heatmap(X.corr(),cmap=\"YlGnBu\", annot=True)\nplt.show()","43c353a5":"# Distribution for the target Variable\nplt.figure(figsize=(8,8))\nsns.histplot(y)\nplt.show()\n","54970e24":"y = np.log(y)\nsns.histplot(y)\n#sns.histplot(np.log(y))\nplt.show()","bb7a78ef":"scaler = StandardScaler()\n\nX[:] = scaler.fit_transform(X)","12719a16":"plt.figure(figsize=(30,8))\nplt.xticks(rotation=45)\nsns.boxplot(data = X)\nplt.show()","a077c986":"X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size = 0.3, random_state = 42)\nprint(X_train.shape, Y_train.shape)\nprint(X_test.shape, Y_test.shape)","7697167d":"X_train.head()","b574313a":"# Importing RFE and LinearRegression\nfrom sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LinearRegression\nlr = LinearRegression()\nlr.fit(X_train, Y_train)\n\nrfe = RFE(lr, 15)             #selected top 15 features\nrfe = rfe.fit(X_train, Y_train)","aa1b7a94":"list(zip(X_train.columns,rfe.support_,rfe.ranking_))","48c95816":"#The columns selected by RFE\ncol = X_train.columns[rfe.support_]\ncol","64bdfde8":"# will pass the above columns to X train and create a new df\nX_train_rfe = X_train[col]\nX_train_rfe.head()","64242cb3":"# will pass the above columns to X test and create a new df\nX_test_rfe = X_test[col]\nX_test_rfe.head()","45c22ce6":"# Now applying PCA with the selected columns df\npca=PCA(random_state=42)\npca.fit_transform(X_train_rfe)","dec966aa":"# let's see the two components of PCA\ncomponents = pd.DataFrame({'PC1':pca.components_[0],'PC2':pca.components_[1],'Feature':X_train_rfe.columns})\ncomponents","b5573f7d":"#Screeplot for the PCA components\nfig = plt.figure(figsize=[12,8])\nplt.vlines(x=11, ymax=1, ymin=0, colors=\"r\", linestyles=\"--\")\nplt.hlines(y=0.98, xmax=50, xmin=0, colors=\"g\", linestyles=\"--\")\nplt.plot( np.cumsum(pca.explained_variance_ratio_))\nplt.xlabel('number of components')\nplt.ylabel(\"Cumulative variance explained\")\nplt.show()","8abbe838":"#creating a dataframe for PCA on selected RFE columns\ncolnames = list(X_train_rfe.columns)\npca_df = pd.DataFrame({'PC1':pca.components_[0],'PC2':pca.components_[1],'PC3':pca.components_[2],\n                       'PC4':pca.components_[3],'PC5':pca.components_[4],\n                       'PC6':pca.components_[5],'PC7':pca.components_[6],'PC8':pca.components_[7],\n                       'PC9':pca.components_[8],'PC10':pca.components_[9],'PC11':pca.components_[10],'Feature':colnames})\n","1e350c1e":"#creating a dataframe for PCA on selected RFE columns\ncolnames = list(X_test_rfe.columns)\npca_df1 = pd.DataFrame({'PC1':pca.components_[0],'PC2':pca.components_[1],'PC3':pca.components_[2],\n                       'PC4':pca.components_[3],'PC5':pca.components_[4],\n                       'PC6':pca.components_[5],'PC7':pca.components_[6],'PC8':pca.components_[7],\n                       'PC9':pca.components_[8],'PC10':pca.components_[9],'PC11':pca.components_[10],\n                       'PC12':pca.components_[11],'PC13':pca.components_[12],\n                       'PC14':pca.components_[13],'PC15':pca.components_[14],'Feature':colnames})\n","70917548":"#Creating a new df on 15 features for train data\nX_train_pca = X_train_rfe[pca_df.Feature[:11]]\nX_train_pca.head()","1df4439f":"#Creating a new df on 15 features for train data\nX_test_pca = X_test_rfe[pca_df1.Feature[:11]]\nX_test_pca.head()","58bff997":"# now doing final PCA with 11 components\n\npca_final = IncrementalPCA(n_components=11)\ndf_train_pca = pca_final.fit_transform(X_train_pca)\ndf_train_pca.shape","c8feba5a":"#Plotting first 2 PCA components\nsns.scatterplot(x=df_train_pca[:,0], y=df_train_pca[:,1])\nplt.show()","8c91b576":"df_test_pca = pca_final.transform(X_test_pca)\ndf_test_pca.shape","b25dd6b4":"# created a function to get all the valuable metrics\ndef regression_results(y_true, y_pred):\n\n    # Regression metrics\n    explained_variance=metrics.explained_variance_score(y_true, y_pred)\n    mean_absolute_error=metrics.mean_absolute_error(y_true, y_pred) \n    mse=metrics.mean_squared_error(y_true, y_pred) \n    median_absolute_error=metrics.median_absolute_error(y_true, y_pred)\n    r2=metrics.r2_score(y_true, y_pred)\n\n    print('explained_variance: ', round(explained_variance,4))    \n    print('r2: ', round(r2,4))\n    print('MAE: ', round(mean_absolute_error,4))\n    print('MSE: ', round(mse,4))\n    print('RMSE: ', round(np.sqrt(mse),4))","e9061a10":"# Instantiate\nlm = LinearRegression()\n\n# Fit a line\nlm.fit(df_train_pca, Y_train)\n# predict the testing pca data\ntrain_predict = lm.predict(df_train_pca)\ntest_predict = lm.predict(df_test_pca)\n\nprint(\"Train metrics\")\nregression_results(Y_train.reshape(-1,),train_predict)\n\nprint()\nprint(\"Test metrics\")\nregression_results(Y_test.reshape(-1,),test_predict)","172bb505":"params = {'alpha': [0.0001, 0.001, 0.01, 0.05, 0.1, \n 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, 2.0, 3.0, \n 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 20, 50, 100, 500, 1000 ]}\n\nridge = Ridge()\n\n# cross validation\nfolds = 5\nmodel_cv = GridSearchCV(estimator = ridge, \n                        param_grid = params, \n                        scoring= 'neg_mean_absolute_error',  \n                        cv = folds, \n                        return_train_score=True,\n                        verbose = 1)            \nmodel_cv.fit(df_train_pca, Y_train) ","80bbff6b":"# Printing the best hyperparameter alpha\nprint(model_cv.best_params_)","3c6fc1f6":"#Fitting Ridge model for alpha = 10 and printing coefficients which have been penalised\nalpha = 0.0001\nridge = Ridge(alpha=alpha)\n\nridge.fit(df_train_pca, Y_train)\nprint(ridge.coef_)","09481466":"# predict it now...\ntrain_predict = ridge.predict(df_train_pca)\ntest_predict = ridge.predict(df_test_pca)\n\nprint(\"Train metrics\")\nregression_results(Y_train.reshape(-1,),train_predict)\n\nprint()\nprint(\"Test metrics\")\nregression_results(Y_test.reshape(-1,),test_predict)","005c3282":"lasso = Lasso()\n\n# cross validation\nmodel_cv = GridSearchCV(estimator = lasso, \n                        param_grid = params, \n                        scoring= 'neg_mean_absolute_error', \n                        cv = folds, \n                        return_train_score=True,\n                        verbose = 1)            \n\nmodel_cv.fit(df_train_pca, Y_train) ","cf0f82a8":"# Printing the best hyperparameter alpha\nprint(model_cv.best_params_)","ae175b43":"#Fitting Lasso model for alpha = 50 and printing coefficients which have been penalised\n\nalpha =0.0001\nlasso = Lasso(alpha=alpha)\n        \nlasso.fit(df_train_pca, Y_train) ","bf67a27a":"lasso.coef_","e9a59843":"# predict it now...\ntrain_predict = lasso.predict(df_train_pca)\ntest_predict = lasso.predict(df_test_pca)\n\nprint(\"Train metrics\")\nregression_results(Y_train.reshape(-1,),train_predict)\n\nprint()\nprint(\"Test metrics\")\nregression_results(Y_test.reshape(-1,),test_predict)","6525d714":"# checking the unknown data\nunseen.head()","2ebf61d1":"submission_data = unseen.set_index('id')[X_train_pca.columns]\nsubmission_data.shape","e10a39b6":"missing_data_percent1=round(100 *(submission_data.isnull().sum()\/len(submission_data.index)),2)\nmissing_data_percent1","53da60b6":"# imputing missing values\nimpute_features1 = missing_data_percent1[missing_data_percent1.gt(0)].index\nimpute_features1","b64ce0ea":"# Imputed with KNN Imputer\nimp= KNNImputer()\nsubmission_data[impute_features1] = imp.fit_transform(submission_data[impute_features1])","d4c7ed68":"#Checking the Null values again \nround(100 *(submission_data.isnull().sum()\/len(submission_data.index)),2)","37a9a767":"submission_data = pca_final.transform(submission_data)\nsubmission_data.shape","12db1f60":"# We ll go for Ridge regression as it has the lowest RMSE\nunseen[\"Shares\"] = ridge.predict(submission_data)\noutput = unseen[['id','Shares']]\noutput.head()","f5d1980a":"output.to_csv('submission_regression_pca_ridge_lasso_lr.csv',index=False)","619c5d1e":"We wil choose 15 components here","5903c2ff":"All the features are almost positively skewed, we'll log transform the features to neutralize the positive skewness in the dataset.","2393eb81":"## SCALING","fd6830f1":"# BASELINE MODELLING\n","2c4a1dab":"From the graph we can see that on Saturday has got maximum number of shares","f7ca8e18":"### Split into train test","5a261845":"# 3. EDA Analysis","5472e3bd":"When number of words in the content is more then number of shares is more","efe24467":"So, we can see when the text sentiment polarity score is more, teh number of share is also more in such scenarios","113edbdb":"### Target Exploration","04fff3e5":"Let's Understand the feature importance for raw features as well as components to decide top features for modelling.","f0f4d3ee":"# 3. Lasso Regression","4b90b10f":"When the DATA CHANNEL is SOCIAL MEDIA , there are maximum number of shares","95754031":"### Log Transform","f7d4ce5e":"# 2. Data Cleaning","a2523b99":"## Feature Engineering and Selection","529bcca9":"# 2. Ridge Regression","cac09e64":"Since the range of number of share is too large , we will perform log transform and then check the distribution","305658af":"## 1. LINEAR REGRESSION","4a2be242":"From the correlation above, we can see that multicollinearity is less ,so no need to drop any columns","7cac7913":"Finally, lets create a csv file out of this dataset, ensuring to set index=False to avoid an addition column in the csv.","e33a562f":"## 6. CREATING SUBMISSION FILE","7696c820":"# 5 . Model Building","0d91980f":"# 1. Data Understanding, preparation and pre-processing","acaab7af":"### Plotting the correlations for each feature for bivariate analysis","51e87f57":"Applying the transformation on the test set","46658316":"# 4. Outliers Treatment\n","97fc86c9":"### As, we can see we don't have any unique value columns so will not do anything with that","3cfcba75":"## Now we can see that there are no NULL values so we can now go for EDA Analysis","92ffa22e":"After capping also few columns are not showhing good results so dropping them ","3d5f1448":"## We can see so many outliers are there.So let's handle the outliers"}}