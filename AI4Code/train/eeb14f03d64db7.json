{"cell_type":{"00ce181a":"code","6ed50fb0":"code","5c156ab4":"code","386c4440":"code","3a9bfbcb":"code","57fbee14":"code","3d071a52":"code","c6ee6781":"code","95fda417":"code","95631830":"code","a3674bd6":"code","40c0debc":"code","cc9919e9":"code","0ba9e189":"code","2bf200cc":"code","0eb07deb":"code","c87de8a9":"markdown"},"source":{"00ce181a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","6ed50fb0":"train = pd.read_csv(\"..\/input\/30-days-of-ml\/train.csv\")\ntest = pd.read_csv(\"..\/input\/30-days-of-ml\/test.csv\")","5c156ab4":"from sklearn.preprocessing import LabelEncoder\nfeature0_encoder=LabelEncoder()\nfeature1_encoder=LabelEncoder()\nfeature2_encoder=LabelEncoder()\nfeature3_encoder=LabelEncoder()\nfeature4_encoder=LabelEncoder()\nfeature5_encoder=LabelEncoder()\nfeature6_encoder=LabelEncoder()\nfeature7_encoder=LabelEncoder()\nfeature8_encoder=LabelEncoder()\nfeature9_encoder=LabelEncoder()\ntrain['cat0']=feature0_encoder.fit_transform(train['cat0'])\ntrain['cat1']=feature1_encoder.fit_transform(train['cat1'])\ntrain['cat2']=feature2_encoder.fit_transform(train['cat2'])\ntrain['cat3']=feature3_encoder.fit_transform(train['cat3'])\ntrain['cat4']=feature4_encoder.fit_transform(train['cat4'])\ntrain['cat5']=feature5_encoder.fit_transform(train['cat5'])\ntrain['cat6']=feature6_encoder.fit_transform(train['cat6'])\ntrain['cat7']=feature7_encoder.fit_transform(train['cat7'])\ntrain['cat8']=feature8_encoder.fit_transform(train['cat8'])\ntrain['cat9']=feature9_encoder.fit_transform(train['cat9'])\ntest['cat0']=feature0_encoder.transform(test['cat0'])\ntest['cat1']=feature1_encoder.transform(test['cat1'])\ntest['cat2']=feature2_encoder.transform(test['cat2'])\ntest['cat3']=feature3_encoder.transform(test['cat3'])\ntest['cat4']=feature4_encoder.transform(test['cat4'])\ntest['cat5']=feature5_encoder.transform(test['cat5'])\ntest['cat6']=feature6_encoder.transform(test['cat6'])\ntest['cat7']=feature7_encoder.transform(test['cat7'])\ntest['cat8']=feature8_encoder.transform(test['cat8'])\ntest['cat9']=feature9_encoder.transform(test['cat9'])","386c4440":"x_train = train.drop(['id','target'],axis=1)\ny_train = train['target']\ntest_id = test['id']\nx_test = test.drop(['id'],axis=1)","3a9bfbcb":"from catboost import CatBoostRegressor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nimport optuna\nfrom warnings import filterwarnings\n\nfilterwarnings(\"ignore\", category=DeprecationWarning) \nfilterwarnings(\"ignore\", category=FutureWarning) \nfilterwarnings(\"ignore\", category=UserWarning)","57fbee14":"X_train,X_val,Y_train,Y_val = train_test_split(x_train,y_train,random_state=42)","3d071a52":"def check_rmse(model,x_val,y_val):\n    pred = model.predict(x_val)\n    return np.sqrt(mean_squared_error(y_val,pred))","c6ee6781":"X_train, X_eval, Y_train, Y_eval = train_test_split(X_train, Y_train, test_size=0.1, random_state=42)","95fda417":"X_train.shape, Y_train.shape, X_val.shape, Y_val.shape, X_eval.shape, Y_eval.shape","95631830":"def objective(trial):\n    param = {}\n    param['learning_rate'] = trial.suggest_discrete_uniform(\"learning_rate\", 0.001, 0.01, 0.004)\n    param['depth'] = trial.suggest_int('depth', 5, 11)\n    param['l2_leaf_reg'] = trial.suggest_discrete_uniform('l2_leaf_reg', 1.0, 5.0, 0.5)\n    param['min_child_samples'] = trial.suggest_categorical('min_child_samples', [1, 4, 8, 16, 32])\n    param['grow_policy'] = 'Depthwise'\n    param['iterations'] = 8000\n    param['use_best_model'] = True\n    param['eval_metric'] = 'RMSE'\n    param['od_type'] = 'iter'\n    param['od_wait'] = 50\n    param['random_state'] = 42\n    param['logging_level'] = 'Silent'\n    \n    regressor = CatBoostRegressor(**param)\n    regressor.fit(X_train.copy(), Y_train.copy(),\n                  eval_set=[(X_eval.copy(), Y_eval.copy())],\n                  early_stopping_rounds=100)\n    return check_rmse(regressor,X_val,Y_val)","a3674bd6":"%%time\nstudy = optuna.create_study(study_name='catboost-seed')\nstudy.optimize(objective, n_trials=10000, n_jobs=-1, timeout=24000)","40c0debc":"study.best_value","cc9919e9":"study.best_params","0ba9e189":"def submission(model,filename):\n    pred = model.predict(x_test)\n    pred = pd.DataFrame(pred,columns=['target'])\n    sub = pd.concat([test_id,pred],axis=1)\n    sub.set_index('id',inplace=True)\n    sub.to_csv(f\"Submission_file_{filename}.csv\")\n    print(f\"Output File Created : Submission_file_{filename}.csv\")","2bf200cc":"\n%%time\noptimized_regressor = CatBoostRegressor(learning_rate=study.best_params['learning_rate'],\n                                        depth=study.best_params['depth'],\n                                        l2_leaf_reg=study.best_params['l2_leaf_reg'],\n                                        min_child_samples=study.best_params['min_child_samples'],\n                                        grow_policy='Depthwise',\n                                        iterations=8000,\n                                        use_best_model=True,\n                                        eval_metric='RMSE',\n                                        od_type='iter',\n                                        od_wait=50,\n                                        random_state=42,\n                                        logging_level='Silent')\noptimized_regressor.fit(X_train.copy(), Y_train.copy(),\n                        eval_set=[(X_eval.copy(), Y_eval.copy())],\n                        early_stopping_rounds=100)\npred_train = optimized_regressor.predict(X_train.copy())\nprint(f\"rmse on training set : {check_rmse(optimized_regressor,X_train,Y_train)}\")\nprint(f\"rmse on valid set : {check_rmse(optimized_regressor,X_val,Y_val)}\")","0eb07deb":"submission(optimized_regressor,\"Optimized_catboost\")","c87de8a9":"## Hyperparamter Tuning of Catboost Regressor\n\nDataset : 30-days-of-ml\n\nThis notebook contains the code of optimizing CatboostRegressor using optuna library\n\nI have also made a notebook with hyperparameter tuning of random forest and XGBoost with skopt and Bayesian Optimization with lightbgm\nlink : https:\/\/www.kaggle.com\/sankalpsrivastava26\/30-days-ml-bayesianoptimization-rf-xgboost"}}