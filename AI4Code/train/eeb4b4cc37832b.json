{"cell_type":{"c9957f5b":"code","8c892097":"code","18bbc5f9":"code","2fcceabc":"code","b8e4ecfb":"code","b3ce2e33":"code","20255405":"code","d5047df6":"code","34036e81":"code","2afe9878":"code","b91c7e60":"code","640effc8":"code","860102d4":"code","d80a99ab":"code","afc41f23":"code","c83e9852":"code","282c868a":"code","c823964e":"code","5bde7560":"code","05595882":"code","ba49f8a1":"code","871a8269":"code","984096ee":"code","2ae1b3ab":"code","766878fb":"code","ed936403":"code","b9cca089":"code","642dc92a":"code","7949a7aa":"code","bdf821c4":"code","501c851e":"code","86bd9b80":"code","f1742fcb":"code","1738236f":"code","700fbc65":"code","031bdcd9":"code","f30949b1":"code","950f69e6":"code","cc3553f5":"code","3c581bfd":"code","863a524f":"code","6f901960":"code","15c2ecf3":"code","534cbee2":"code","2c387b97":"code","5a524b63":"code","301a92cc":"code","cc2f0f74":"code","b67b6d39":"code","bd7eebfb":"code","24a65d63":"code","c472ae64":"code","ffc97cf1":"code","5626fab5":"code","7e3cad6c":"code","3eeb67dd":"code","59421d8c":"code","58a28f65":"code","eab722d7":"code","a8bc4b73":"code","ce5ee3b6":"code","1236d5f8":"code","53578d38":"code","68d99ec4":"code","6fd449d0":"code","c5091525":"code","f1598313":"code","2e0cbcc4":"code","9e494807":"code","9d002ab4":"code","70aeb85a":"code","fd176a0f":"code","cd1a8cdd":"code","43fc0f14":"code","83c3ae01":"code","d15bd78a":"code","b44e6ae3":"code","424a1f4d":"code","83333ed2":"code","0eccee6d":"code","62c6955d":"code","601d7371":"code","733c7d93":"code","71a851de":"code","1a46b048":"code","a31938a5":"code","d1314120":"code","a2a16a67":"code","76364272":"code","81115f27":"code","f7367b58":"code","2c426a6d":"code","6e57445d":"code","d8a53677":"code","162cbfec":"code","c9829e21":"code","51155a01":"code","51d76d68":"code","d1248f39":"code","b66e5e73":"code","1e3da7b4":"code","96f26c8a":"code","1acc6334":"code","6681fd52":"code","9f11d96a":"code","99fb5e78":"code","4a15bf40":"code","49447556":"code","5c040c88":"code","f61f32d0":"code","953e60c9":"code","b279ca7a":"code","8d0d5128":"code","ecc9023d":"code","8e00ca10":"code","02b92fe0":"code","b93e1c17":"code","975d7572":"code","28e36224":"code","cbb70e4d":"code","3c0bb134":"code","6abd04fe":"code","90514acc":"code","b1d74fbc":"code","9f30e371":"markdown","584081e6":"markdown","3f8590ef":"markdown","7fa997a2":"markdown","b2168e86":"markdown","f4154a65":"markdown","75e77344":"markdown","2addef1e":"markdown","93c9a624":"markdown","01def0f4":"markdown","f0872897":"markdown","248b054b":"markdown","70b0138c":"markdown","876740a5":"markdown","8e081a77":"markdown","6898b555":"markdown","cec9dd7a":"markdown","69e092e9":"markdown","b34451ad":"markdown","d40484e9":"markdown","674fbd86":"markdown","e3aa219e":"markdown","c5d1cd27":"markdown","56e0d567":"markdown"},"source":{"c9957f5b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","8c892097":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom pandas import plotting\n\n#plotly \nimport plotly.offline as py\nimport plotly.graph_objects as go\nfrom plotly.offline import init_notebook_mode, iplot\nfrom plotly import tools\ninit_notebook_mode(connected=True)\nimport plotly.figure_factory as ff\nimport plotly.express as px\n\nfrom sklearn.preprocessing import StandardScaler\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import preprocessing\nfrom sklearn import neighbors\nfrom sklearn.metrics import confusion_matrix,classification_report,precision_score\nfrom sklearn.model_selection import train_test_split\n\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\n\nsns.set(style=\"whitegrid\")\nplt.style.use('fivethirtyeight')\n\nimport warnings\nfrom sklearn.utils.testing import ignore_warnings\nfrom sklearn.exceptions import ConvergenceWarning","18bbc5f9":"df=pd.read_csv('..\/input\/breast-cancer-wisconsin-data\/data.csv')\ndf.head()","2fcceabc":"df=df.drop('Unnamed: 32', axis=1)\n","b8e4ecfb":"diagnosis={'M':1, 'B':0}\ndf['diagnosis']=[diagnosis[x] for x in df['diagnosis']]","b3ce2e33":"k= pd.DataFrame()\nk['df']= df.isnull().sum()\nk.T","20255405":"from scipy import stats\n","d5047df6":"cor_df=pd.DataFrame(columns=['r','p-value'])\nfor col in df:\n   # print(col)\n    if pd.api.types.is_numeric_dtype(df[col]):\n        r, p=stats.pearsonr(df.diagnosis, df[col])\n        cor_df.loc[col]=[r, p]\ncor_df","34036e81":"cor_df.sort_values(by=['p-value'], ascending=False)\n","2afe9878":"col=['symmetry_se', 'texture_se', 'fractal_dimension_mean']\nX=df.drop(col, axis=1)\ny=df['diagnosis']\nX_train, x_test, y_train, y_test=train_test_split(X, y, test_size=0.3, random_state=10)","b91c7e60":"from sklearn import metrics\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import classification_report, confusion_matrix,ConfusionMatrixDisplay\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf","640effc8":"solver=['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']\nerror_rate=[]\nfor s in solver:\n    \n    lr=LogisticRegression(solver=s)\n    lr.fit(X_train, y_train)\n    pred=lr.predict(x_test)\n    error_rate.append(np.mean(pred!=y_test))\nplt.figure(figsize=(15,10))\nplt.plot(solver, error_rate,marker='o', markersize=9)","860102d4":"lr=LogisticRegression(solver='newton-cg')\nlr.fit(X_train, y_train)\n","d80a99ab":"cor = df.corr()\nplt.figure(figsize=(12,12))\nsns.heatmap(cor,cmap='Set1',annot=True)","afc41f23":"corr = df.corr().round(2)\n\n# Mask for the upper triangle\nmask = np.zeros_like(corr, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n\n# Set figure size\nf, ax = plt.subplots(figsize=(20, 20))\n\n# Define custom colormap\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\n\n# Draw the heatmap\nsns.heatmap(corr, mask=mask, cmap=cmap, vmin=-1, vmax=1, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5}, annot=True)\n\nplt.tight_layout()","c83e9852":"y_pred=lr.predict(X_train)\n","282c868a":"print(\"Accuracy Score:-\", metrics.accuracy_score(y_train, y_pred))\nprint(\"F1 Score:-\", metrics.f1_score(y_train, y_pred))\nprint(\"Average Precision Score:-\", metrics.average_precision_score(y_train, y_pred))\nprint(\"Log Loss:-\", metrics.log_loss(y_train, y_pred))\nprint(\"Precision Score:-\", metrics.precision_score(y_train, y_pred))\nprint(\"Recall Score:-\", metrics.recall_score(y_train, y_pred))\nprint(\"ROC-AUC Score:-\", metrics.roc_auc_score(y_train, y_pred))","c823964e":"y_test_pred=lr.predict(x_test)\n","5bde7560":"lr_acc=metrics.accuracy_score(y_test, y_test_pred)\nprint(\"Accuracy Score:-\",lr_acc)\nprint(\"F1 Score:-\", metrics.f1_score(y_test, y_test_pred))\nprint(\"Average Precision Score:-\", metrics.average_precision_score(y_test, y_test_pred))\nprint(\"Log Loss:-\", metrics.log_loss(y_test, y_test_pred))\nprint(\"Precision Score:-\", metrics.precision_score(y_test, y_test_pred))\nprint(\"Recall Score:-\", metrics.recall_score(y_test, y_test_pred))\nprint(\"ROC-AUC Score:-\", metrics.roc_auc_score(y_test, y_test_pred))","05595882":"cfm=confusion_matrix(y_test, y_test_pred)\ndisp = ConfusionMatrixDisplay(confusion_matrix=cfm,\n                               display_labels=lr.classes_)\ndisp.plot()","ba49f8a1":"from sklearn.model_selection import cross_val_score    \nscores = cross_val_score(lr, X, y, cv=10, scoring='accuracy') #cv is cross validation\nprint(scores)\nprint(\"-------------------\")\nprint(scores.mean())","871a8269":"from sklearn.metrics import classification_report\nprint(classification_report(y_test, y_test_pred))","984096ee":"y_test_pred_prob=lr.predict_proba(x_test)[:,1]\ny_test_pred_prob\n\nfrom sklearn.metrics import roc_curve\nmetrics.roc_auc_score(y_test, y_test_pred_prob)","2ae1b3ab":"fpr, tpr,thresholds=roc_curve(y_test,y_test_pred_prob)\nplt.figure(figsize=(10,10))\nplt.plot([0,1],[0,1],'k--')\nplt.plot(fpr, tpr, label='Logistic Regression')\nplt.xlabel(\"fpr (False Possitive rate)\")\nplt.ylabel(\"tpr-(True Positive rate)\")\nplt.title(\"ROC_AUC\")\nplt.show()","766878fb":"from sklearn.metrics import precision_recall_curve\nno_skill=len(y==1)\/len(y)\ny_test_prob=lr.predict_proba(x_test)[:,1]\nplt.figure(figsize=(10,8))\nplt.plot([0,1],[no_skill, no_skill], label=\"No Skill\")\nprecision, recall,_ =precision_recall_curve(y_test, y_test_prob)\nplt.plot(recall, precision, marker='',label=\"Logistic Regression\")\nplt.xlabel(\"Recall\")\nplt.ylabel(\"Precision\")\nplt.title(\"Recall-Precision Curve\")\nplt.legend()\nplt.show()","ed936403":"from sklearn.neighbors import KNeighborsClassifier\ncol=['symmetry_se', 'texture_se', 'fractal_dimension_mean']\nX=df.drop(col, axis=1)\ny=df['diagnosis']\nX_train, x_test, y_train, y_test=train_test_split(X, y, test_size=0.3, random_state=10)","b9cca089":"error_rate=[]\n\nfor i in range(1,11):\n    knn=KNeighborsClassifier(n_neighbors=i)\n    knn.fit(X_train, y_train)\n    pred=knn.predict(x_test)\n    error_rate.append(np.mean(pred!=y_test))\n    \nplt.figure(figsize=(15,10))\nplt.plot(range(1,11), error_rate,marker='o', markersize=9)","642dc92a":"knn=KNeighborsClassifier(n_neighbors=3)\nknn.fit(X_train, y_train)","7949a7aa":"y_pred=knn.predict(X_train)\n","bdf821c4":"print(\"Accuracy Score:-\", metrics.accuracy_score(y_train, y_pred))\nprint(\"F1 Score:-\", metrics.f1_score(y_train, y_pred))\nprint(\"Average Precision Score:-\", metrics.average_precision_score(y_train, y_pred))\nprint(\"Log Loss:-\", metrics.log_loss(y_train, y_pred))\nprint(\"Precision Score:-\", metrics.precision_score(y_train, y_pred))\nprint(\"Recall Score:-\", metrics.recall_score(y_train, y_pred))\nprint(\"ROC-AUC Score:-\", metrics.roc_auc_score(y_train, y_pred))","501c851e":"y_test_pred=knn.predict(x_test)\n","86bd9b80":"knn_acc=metrics.accuracy_score(y_test, y_test_pred)\nprint(\"Accuracy Score:-\",knn_acc)\nprint(\"F1 Score:-\", metrics.f1_score(y_test, y_test_pred))\nprint(\"Average Precision Score:-\", metrics.average_precision_score(y_test, y_test_pred))\nprint(\"Log Loss:-\", metrics.log_loss(y_test, y_test_pred))\nprint(\"Precision Score:-\", metrics.precision_score(y_test, y_test_pred))\nprint(\"Recall Score:-\", metrics.recall_score(y_test, y_test_pred))\nprint(\"ROC-AUC Score:-\", metrics.roc_auc_score(y_test, y_test_pred))","f1742fcb":"cfm=confusion_matrix(y_test, y_test_pred)\ndisp = ConfusionMatrixDisplay(confusion_matrix=cfm,\n                               display_labels=knn.classes_)\ndisp.plot()","1738236f":"from sklearn.metrics import classification_report\nprint(classification_report(y_test, y_test_pred))","700fbc65":"y_test_pred_prob=knn.predict_proba(x_test)[:,1]\ny_test_pred_prob\n\nfrom sklearn.metrics import roc_curve\nmetrics.roc_auc_score(y_test, y_test_pred_prob)","031bdcd9":"fpr, tpr,thresholds=roc_curve(y_test,y_test_pred_prob)\nplt.figure(figsize=(10,10))\nplt.plot([0,1],[0,1],'k--')\nplt.plot(fpr, tpr, label='Logistic Regression')\nplt.xlabel(\"fpr (False Possitive rate)\")\nplt.ylabel(\"tpr-(True Positive rate)\")\nplt.title","f30949b1":"from sklearn.metrics import precision_recall_curve\nno_skill=len(y==1)\/len(y)\ny_test_prob=knn.predict_proba(x_test)[:,1]\nplt.figure(figsize=(10,8))\nplt.plot([0,1],[no_skill, no_skill], label=\"No Skill\")\nprecision, recall,_ =precision_recall_curve(y_test, y_test_prob)\nplt.plot(recall, precision, marker='',label=\"Knn\")\nplt.xlabel(\"Recall\")\nplt.ylabel(\"Precision\")\nplt.title(\"Recall-Precision Curve\")\nplt.legend()\nplt.show()","950f69e6":"# train, cv, test\nX_train, x_test, y_train, y_test=train_test_split(X, y, test_size=0.3, random_state=10)\nX_tr, X_cv, y_tr, y_cv = train_test_split(X, y, test_size=0.3)","cc3553f5":"from sklearn.metrics import accuracy_score\nhist = {\n    'ks': [],\n    'acc_cv': [],\n    'acc_tr': []\n}\n\nfor k in range(1,50,2):\n    knn = KNeighborsClassifier(n_neighbors=k)\n    # fitting cv train\n    knn.fit(X_tr, y_tr)\n    # predict and eval  cv train\n    pred_cv = knn.predict(X_cv)\n    pred_tr = knn.predict(X_tr)\n    acc_cv = accuracy_score(y_cv, pred_cv, normalize=True) * float(100)\n    acc_tr  = accuracy_score(y_tr, pred_tr, normalize=True) * float(100)\n    print(f\"k:{k}\\t val-acc: {acc_cv} \\ttrain-acc: {acc_tr}\")\n    \n    # log\n    hist['ks'].append(k)    \n    hist['acc_cv'].append(acc_cv)    \n    hist['acc_tr'].append(acc_tr)","3c581bfd":"plt.figure(figsize=(20, 7))\n\nplt.plot(hist['ks'], hist['acc_cv'], label='cv')\nplt.plot(hist['ks'], hist['acc_tr'], label='train')\n\nfor k, acc_cv in zip(hist['ks'], hist['acc_cv']):\n    plt.text(k, acc_cv, f'k={k}')\n\nplt.legend()\nplt.show()","863a524f":"k = 31\n\nknn = KNeighborsClassifier(n_neighbors=k)\n# fitting cv train\nknn.fit(X_tr, y_tr)\n# predict and eval  cv train\npred_cv = knn.predict(X_cv)\nacc_cv = accuracy_score(y_cv, pred_cv, normalize=True) * float(100)\nprint(f\"k:{k}\\t val-acc: {acc_cv}\")","6f901960":"from sklearn.model_selection import cross_val_score\n","15c2ecf3":"hist = {\n    'ks': [],\n    'acc_cv': []\n}\n\nfor k in range(0, 60):\n    knn = KNeighborsClassifier(n_neighbors=k) \n    scores = cross_val_score(knn, X_train, y_train, cv=10) # 10-fold\n    # 95% conf-interval: scores.mean() (+\/- 2*scores.std())\n    \n    # log\n    hist['ks'].append(k)\n    hist['acc_cv'].append(scores.mean())","534cbee2":"plt.figure(figsize=(20, 7))\n\nplt.plot(hist['ks'], hist['acc_cv'], label='cv')\n\nfor k, acc_cv in zip(hist['ks'], hist['acc_cv']):\n    plt.text(k, acc_cv, f'k={k}')\n\nplt.legend()\nplt.show()","2c387b97":"knn=KNeighborsClassifier(n_neighbors=16)\nknn.fit(X_train, y_train)","5a524b63":"y_pred=knn.predict(X_train)\n","301a92cc":"print(\"Accuracy Score:-\", metrics.accuracy_score(y_train, y_pred))\nprint(\"F1 Score:-\", metrics.f1_score(y_train, y_pred))\nprint(\"Average Precision Score:-\", metrics.average_precision_score(y_train, y_pred))\nprint(\"Log Loss:-\", metrics.log_loss(y_train, y_pred))\nprint(\"Precision Score:-\", metrics.precision_score(y_train, y_pred))\nprint(\"Recall Score:-\", metrics.recall_score(y_train, y_pred))\nprint(\"ROC-AUC Score:-\", metrics.roc_auc_score(y_train, y_pred))","cc2f0f74":"plt.figure(figsize=(20, 7))\nacc_score=[]\nfor k in range(1,50):\n    knn=KNeighborsClassifier(n_neighbors=k)\n    knn.fit(X_train, y_train)\n    y_pred=knn.predict(X_train)\n    acc_score.append(metrics.accuracy_score(y_train, y_pred))\n    \nplt.plot(range(1, 50),acc_score , label=\"accuracy score\")\n    \n","b67b6d39":"from sklearn.svm import SVC\nfrom sklearn import metrics\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nscaler.fit(X)\nX = scaler.transform(X)","bd7eebfb":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)","24a65d63":"svc=SVC(kernel='poly', C=1.0) #Default hyperparameters\nsvc.fit(X_train,y_train)\ny_pred=svc.predict(X_train)","c472ae64":"print(\"Accuracy Score:-\", metrics.accuracy_score(y_train, y_pred))\nprint(\"F1 Score:-\", metrics.f1_score(y_train, y_pred))\nprint(\"Average Precision Score:-\", metrics.average_precision_score(y_train, y_pred))\nprint(\"Log Loss:-\", metrics.log_loss(y_train, y_pred))\nprint(\"Precision Score:-\", metrics.precision_score(y_train, y_pred))\nprint(\"Recall Score:-\", metrics.recall_score(y_train, y_pred))\nprint(\"ROC-AUC Score:-\", metrics.roc_auc_score(y_train, y_pred))","ffc97cf1":"y_test_pred=svc.predict(X_test)","5626fab5":"svc_acc=metrics.accuracy_score(y_test, y_test_pred)\nprint(\"Accuracy Score:-\",svc_acc)\nprint(\"F1 Score:-\", metrics.f1_score(y_test, y_test_pred))\nprint(\"Average Precision Score:-\", metrics.average_precision_score(y_test, y_test_pred))\nprint(\"Log Loss:-\", metrics.log_loss(y_test, y_test_pred))\nprint(\"Precision Score:-\", metrics.precision_score(y_test, y_test_pred))\nprint(\"Recall Score:-\", metrics.recall_score(y_test, y_test_pred))\nprint(\"ROC-AUC Score:-\", metrics.roc_auc_score(y_test, y_test_pred))","7e3cad6c":"cfm=confusion_matrix(y_test, y_test_pred)\ndisp = ConfusionMatrixDisplay(confusion_matrix=cfm,\n                               display_labels=svc.classes_)\ndisp.plot()","3eeb67dd":"from sklearn.metrics import classification_report\nprint(classification_report(y_test, y_test_pred))","59421d8c":"y_test_pred_prob=lr.predict_proba(X_test)[:,1]\ny_test_pred_prob\n\nfrom sklearn.metrics import roc_curve\nmetrics.roc_auc_score(y_test, y_test_pred_prob)","58a28f65":"fpr, tpr,thresholds=roc_curve(y_test,y_test_pred_prob)\nplt.figure(figsize=(10,10))\nplt.plot([0,1],[0,1],'k--')\nplt.plot(fpr, tpr, label='Logistic Regression')\nplt.xlabel(\"fpr (False Possitive rate)\")\nplt.ylabel(\"tpr-(True Positive rate)\")\nplt.title(\"ROC_AUC\")\nplt.show()","eab722d7":"from sklearn.metrics import precision_recall_curve\nno_skill=len(y==1)\/len(y)\ny_test_prob=knn.predict_proba(X_test)[:,1]\nplt.figure(figsize=(10,8))\nplt.plot([0,1],[no_skill, no_skill], label=\"No Skill\")\nprecision, recall,_ =precision_recall_curve(y_test, y_test_prob)\nplt.plot(recall, precision, marker='',label=\"SVC\")\nplt.xlabel(\"Recall\")\nplt.ylabel(\"Precision\")\nplt.title(\"Recall-Precision Curve\")\nplt.legend()\nplt.show()","a8bc4b73":"from sklearn.tree import DecisionTreeClassifier","ce5ee3b6":"col=['symmetry_se', 'texture_se', 'fractal_dimension_mean']\nX=df.drop(col, axis=1)\ny=df['diagnosis']\nX_train, x_test, y_train, y_test=train_test_split(X, y, test_size=0.3, random_state=10)","1236d5f8":"clf_gini = DecisionTreeClassifier(criterion='gini', max_depth=3, random_state=0)\n\n\n# fit the model\nclf_gini.fit(X_train, y_train)","53578d38":"y_pred_gini = clf_gini.predict(X_train)","68d99ec4":"print(\"Accuracy Score:-\", metrics.accuracy_score(y_train, y_pred_gini))\nprint(\"F1 Score:-\", metrics.f1_score(y_train, y_pred_gini))\nprint(\"Average Precision Score:-\", metrics.average_precision_score(y_train, y_pred_gini))\nprint(\"Log Loss:-\", metrics.log_loss(y_train, y_pred_gini))\nprint(\"Precision Score:-\", metrics.precision_score(y_train, y_pred_gini))\nprint(\"Recall Score:-\", metrics.recall_score(y_train, y_pred_gini))\nprint(\"ROC-AUC Score:-\", metrics.roc_auc_score(y_train, y_pred_gini))","6fd449d0":"y_test_pred= clf_gini.predict(x_test)\n","c5091525":"dc_acc=metrics.accuracy_score(y_test, y_test_pred)\nprint(\"Accuracy Score:-\",dc_acc)\nprint(\"F1 Score:-\", metrics.f1_score(y_test, y_test_pred))\nprint(\"Average Precision Score:-\", metrics.average_precision_score(y_test, y_test_pred))\nprint(\"Log Loss:-\", metrics.log_loss(y_test, y_test_pred))\nprint(\"Precision Score:-\", metrics.precision_score(y_test, y_test_pred))\nprint(\"Recall Score:-\", metrics.recall_score(y_test, y_test_pred))\nprint(\"ROC-AUC Score:-\", metrics.roc_auc_score(y_test, y_test_pred))","f1598313":"cfm=confusion_matrix(y_test, y_test_pred)\ndisp = ConfusionMatrixDisplay(confusion_matrix=cfm,\n                               display_labels=clf_gini.classes_)\ndisp.plot()","2e0cbcc4":"from sklearn.metrics import classification_report\nprint(classification_report(y_test, y_test_pred))","9e494807":"y_test_pred_prob=clf_gini.predict_proba(x_test)[:,1]\ny_test_pred_prob\n\nfrom sklearn.metrics import roc_curve\nmetrics.roc_auc_score(y_test, y_test_pred_prob)","9d002ab4":"fpr, tpr,thresholds=roc_curve(y_test,y_test_pred_prob)\nplt.figure(figsize=(10,10))\nplt.plot([0,1],[0,1],'k--')\nplt.plot(fpr, tpr, label='Logistic Regression')\nplt.xlabel(\"fpr (False Possitive rate)\")\nplt.ylabel(\"tpr-(True Positive rate)\")\nplt.title(\"ROC_AUC\")\nplt.show()","70aeb85a":"from sklearn.metrics import precision_recall_curve\nno_skill=len(y==1)\/len(y)\ny_test_prob=lr.predict_proba(x_test)[:,1]\nplt.figure(figsize=(10,8))\nplt.plot([0,1],[no_skill, no_skill], label=\"No Skill\")\nprecision, recall,_ =precision_recall_curve(y_test, y_test_prob)\nplt.plot(recall, precision, marker='',label=\"Decision Tree\")\nplt.xlabel(\"Recall\")\nplt.ylabel(\"Precision\")\nplt.title(\"Recall-Precision Curve\")\nplt.legend()\nplt.show()","fd176a0f":"plt.figure(figsize=(12,8))\n\nfrom sklearn import tree\n\ntree.plot_tree(clf_gini.fit(X_train, y_train)) ","cd1a8cdd":"clf_en = DecisionTreeClassifier(criterion='entropy', max_depth=3, random_state=0)\n\n\n# fit the model\nclf_en.fit(X_train, y_train)","43fc0f14":"error_rate=[]\nfor i in range(1,11):\n    clf_en = DecisionTreeClassifier(criterion='entropy', max_depth=i, random_state=0)\n\n\n    # fit the model\n    clf_en.fit(X_train, y_train)\n    pred=clf_en.predict(x_test)\n    error_rate.append(np.mean(pred!=y_test))\n    \nplt.figure(figsize=(15,10))\nplt.plot(range(1,11), error_rate,marker='o', markersize=9)\n    ","83c3ae01":"clf_en = DecisionTreeClassifier(criterion='entropy', max_depth=2, random_state=0)\n\n\n# fit the model\nclf_en.fit(X_train, y_train)","d15bd78a":"y_pred=clf_en.predict(X_train)\n","b44e6ae3":"print(\"Accuracy Score:-\", metrics.accuracy_score(y_train, y_pred))\nprint(\"F1 Score:-\", metrics.f1_score(y_train, y_pred))\nprint(\"Average Precision Score:-\", metrics.average_precision_score(y_train, y_pred))\nprint(\"Log Loss:-\", metrics.log_loss(y_train, y_pred))\nprint(\"Precision Score:-\", metrics.precision_score(y_train, y_pred))\nprint(\"Recall Score:-\", metrics.recall_score(y_train, y_pred))\nprint(\"ROC-AUC Score:-\", metrics.roc_auc_score(y_train, y_pred))","424a1f4d":"y_test_pred=clf_en.predict(x_test)\n","83333ed2":"dc_en_acc=metrics.accuracy_score(y_test, y_test_pred)\nprint(\"Accuracy Score:-\",dc_en_acc)\nprint(\"F1 Score:-\", metrics.f1_score(y_test, y_test_pred))\nprint(\"Average Precision Score:-\", metrics.average_precision_score(y_test, y_test_pred))\nprint(\"Log Loss:-\", metrics.log_loss(y_test, y_test_pred))\nprint(\"Precision Score:-\", metrics.precision_score(y_test, y_test_pred))\nprint(\"Recall Score:-\", metrics.recall_score(y_test, y_test_pred))\nprint(\"ROC-AUC Score:-\", metrics.roc_auc_score(y_test, y_test_pred))","0eccee6d":"cfm=confusion_matrix(y_test, y_test_pred)\ndisp = ConfusionMatrixDisplay(confusion_matrix=cfm,\n                               display_labels=clf_en.classes_)\ndisp.plot()","62c6955d":"from sklearn.metrics import classification_report\nprint(classification_report(y_test, y_test_pred))","601d7371":"y_test_pred_prob=clf_gini.predict_proba(x_test)[:,1]\ny_test_pred_prob\n\nfrom sklearn.metrics import roc_curve\nmetrics.roc_auc_score(y_test, y_test_pred_prob)","733c7d93":"fpr, tpr,thresholds=roc_curve(y_test,y_test_pred_prob)\nplt.figure(figsize=(10,10))\nplt.plot([0,1],[0,1],'k--')\nplt.plot(fpr, tpr, label='DEcision Tree with Entropy')\nplt.xlabel(\"fpr (False Possitive rate)\")\nplt.ylabel(\"tpr-(True Positive rate)\")\nplt.title(\"ROC_AUC\")\nplt.show()","71a851de":"col=['symmetry_se', 'texture_se','fractal_dimension_mean', 'smoothness_se']\nX=df.drop(col, axis=1)\ny=df['diagnosis']\nX_train, x_test, y_train, y_test=train_test_split(X, y, test_size=0.3, random_state=10)","1a46b048":"lr=LogisticRegression(solver=\"newton-cg\")\nlr.fit(X_train, y_train)","a31938a5":"y_pred=lr.predict(X_train)\n","d1314120":"print(\"Accuracy Score:-\", metrics.accuracy_score(y_train, y_pred))\nprint(\"F1 Score:-\", metrics.f1_score(y_train, y_pred))\nprint(\"Average Precision Score:-\", metrics.average_precision_score(y_train, y_pred))\nprint(\"Log Loss:-\", metrics.log_loss(y_train, y_pred))\nprint(\"Precision Score:-\", metrics.precision_score(y_train, y_pred))\nprint(\"Recall Score:-\", metrics.recall_score(y_train, y_pred))\nprint(\"ROC-AUC Score:-\", metrics.roc_auc_score(y_train, y_pred))","a2a16a67":"y_test_pred=lr.predict(x_test)","76364272":"lr_acc=metrics.accuracy_score(y_test, y_test_pred)\nprint(\"Accuracy Score:-\",lr_acc)\nprint(\"F1 Score:-\", metrics.f1_score(y_test, y_test_pred))\nprint(\"Average Precision Score:-\", metrics.average_precision_score(y_test, y_test_pred))\nprint(\"Log Loss:-\", metrics.log_loss(y_test, y_test_pred))\nprint(\"Precision Score:-\", metrics.precision_score(y_test, y_test_pred))\nprint(\"Recall Score:-\", metrics.recall_score(y_test, y_test_pred))\nprint(\"ROC-AUC Score:-\", metrics.roc_auc_score(y_test, y_test_pred))","81115f27":"cfm=confusion_matrix(y_test, y_test_pred)\ndisp = ConfusionMatrixDisplay(confusion_matrix=cfm,\n                               display_labels=lr.classes_)\ndisp.plot() ","f7367b58":"from sklearn.metrics import classification_report\nprint(classification_report(y_test, y_test_pred))","2c426a6d":"y_test_pred_prob=lr.predict_proba(x_test)[:,1]\ny_test_pred_prob\n\nfrom sklearn.metrics import roc_curve\nmetrics.roc_auc_score(y_test, y_test_pred_prob)","6e57445d":"fpr, tpr,thresholds=roc_curve(y_test,y_test_pred_prob)\nplt.figure(figsize=(10,10))\nplt.plot([0,1],[0,1],'k--')\nplt.plot(fpr, tpr, label='Logistic Regression')\nplt.xlabel(\"fpr (False Possitive rate)\")\nplt.ylabel(\"tpr-(True Positive rate)\")\nplt.title(\"ROC_AUC\")\nplt.show()","d8a53677":"from sklearn.metrics import precision_recall_curve\nno_skill=len(y==1)\/len(y)\ny_test_prob=lr.predict_proba(x_test)[:,1]\nplt.figure(figsize=(10,8))\nplt.plot([0,1],[no_skill, no_skill], label=\"No Skill\")\nprecision, recall,_ =precision_recall_curve(y_test, y_test_prob)\nplt.plot(recall, precision, marker='',label=\"Logistic Regression\")\nplt.xlabel(\"Recall\")\nplt.ylabel(\"Precision\")\nplt.title(\"Recall-Precision Curve\")\nplt.legend()\nplt.show()","162cbfec":"error_rate=[]\n\nfor i in range(1,11):\n    knn=KNeighborsClassifier(n_neighbors=i)\n    knn.fit(X_train, y_train)\n    pred=knn.predict(x_test)\n    error_rate.append(np.mean(pred!=y_test))\n    \nplt.figure(figsize=(15,10))\nplt.plot(range(1,11), error_rate,marker='o', markersize=9)","c9829e21":"knn=KNeighborsClassifier(n_neighbors=3)\nknn.fit(X_train, y_train)","51155a01":"print(\"Accuracy Score:-\", metrics.accuracy_score(y_train, y_pred))\nprint(\"F1 Score:-\", metrics.f1_score(y_train, y_pred))\nprint(\"Average Precision Score:-\", metrics.average_precision_score(y_train, y_pred))\nprint(\"Log Loss:-\", metrics.log_loss(y_train, y_pred))\nprint(\"Precision Score:-\", metrics.precision_score(y_train, y_pred))\nprint(\"Recall Score:-\", metrics.recall_score(y_train, y_pred))\nprint(\"ROC-AUC Score:-\", metrics.roc_auc_score(y_train, y_pred))","51d76d68":"y_pred=knn.predict(X_train)\n","d1248f39":"print(\"Accuracy Score:-\", metrics.accuracy_score(y_train, y_pred))\nprint(\"F1 Score:-\", metrics.f1_score(y_train, y_pred))\nprint(\"Average Precision Score:-\", metrics.average_precision_score(y_train, y_pred))\nprint(\"Log Loss:-\", metrics.log_loss(y_train, y_pred))\nprint(\"Precision Score:-\", metrics.precision_score(y_train, y_pred))\nprint(\"Recall Score:-\", metrics.recall_score(y_train, y_pred))\nprint(\"ROC-AUC Score:-\", metrics.roc_auc_score(y_train, y_pred))","b66e5e73":"y_test_pred=knn.predict(x_test)\n","1e3da7b4":"knn_acc=metrics.accuracy_score(y_test, y_test_pred)\nprint(\"Accuracy Score:-\",knn_acc)\nprint(\"F1 Score:-\", metrics.f1_score(y_test, y_test_pred))\nprint(\"Average Precision Score:-\", metrics.average_precision_score(y_test, y_test_pred))\nprint(\"Log Loss:-\", metrics.log_loss(y_test, y_test_pred))\nprint(\"Precision Score:-\", metrics.precision_score(y_test, y_test_pred))\nprint(\"Recall Score:-\", metrics.recall_score(y_test, y_test_pred))\nprint(\"ROC-AUC Score:-\", metrics.roc_auc_score(y_test, y_test_pred))","96f26c8a":"cfm=confusion_matrix(y_test, y_test_pred)\ndisp = ConfusionMatrixDisplay(confusion_matrix=cfm,\n                               display_labels=knn.classes_)\ndisp.plot() ","1acc6334":"from sklearn.metrics import classification_report\nprint(classification_report(y_test, y_test_pred))","6681fd52":"y_test_pred_prob=knn.predict_proba(x_test)[:,1]\ny_test_pred_prob\n\nfrom sklearn.metrics import roc_curve\nmetrics.roc_auc_score(y_test, y_test_pred_prob)","9f11d96a":"fpr, tpr,thresholds=roc_curve(y_test,y_test_pred_prob)\nplt.figure(figsize=(10,10))\nplt.plot([0,1],[0,1],'k--')\nplt.plot(fpr, tpr, label='Logistic Regression')\nplt.xlabel(\"fpr (False Possitive rate)\")\nplt.ylabel(\"tpr-(True Positive rate)\")\nplt.title","99fb5e78":"from sklearn.metrics import precision_recall_curve\nno_skill=len(y==1)\/len(y)\ny_test_prob=knn.predict_proba(x_test)[:,1]\nplt.figure(figsize=(10,8))\nplt.plot([0,1],[no_skill, no_skill], label=\"No Skill\")\nprecision, recall,_ =precision_recall_curve(y_test, y_test_prob)\nplt.plot(recall, precision, marker='',label=\"Knn\")\nplt.xlabel(\"Recall\")\nplt.ylabel(\"Precision\")\nplt.title(\"Recall-Precision Curve\")\nplt.legend()\nplt.show()","4a15bf40":"scaler = StandardScaler()\nscaler.fit(X)\nX = scaler.transform(X)","49447556":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)","5c040c88":"svc=SVC(kernel='poly', C=1.0) #Default hyperparameters\nsvc.fit(X_train,y_train)\ny_pred=svc.predict(X_train)","f61f32d0":"print(\"Accuracy Score:-\", metrics.accuracy_score(y_train, y_pred))\nprint(\"F1 Score:-\", metrics.f1_score(y_train, y_pred))\nprint(\"Average Precision Score:-\", metrics.average_precision_score(y_train, y_pred))\nprint(\"Log Loss:-\", metrics.log_loss(y_train, y_pred))\nprint(\"Precision Score:-\", metrics.precision_score(y_train, y_pred))\nprint(\"Recall Score:-\", metrics.recall_score(y_train, y_pred))\nprint(\"ROC-AUC Score:-\", metrics.roc_auc_score(y_train, y_pred))","953e60c9":"y_test_pred=svc.predict(X_test)\n","b279ca7a":"svc_acc=metrics.accuracy_score(y_test, y_test_pred)\nprint(\"Accuracy Score:-\",svc_acc)\nprint(\"F1 Score:-\", metrics.f1_score(y_test, y_test_pred))\nprint(\"Average Precision Score:-\", metrics.average_precision_score(y_test, y_test_pred))\nprint(\"Log Loss:-\", metrics.log_loss(y_test, y_test_pred))\nprint(\"Precision Score:-\", metrics.precision_score(y_test, y_test_pred))\nprint(\"Recall Score:-\", metrics.recall_score(y_test, y_test_pred))\nprint(\"ROC-AUC Score:-\", metrics.roc_auc_score(y_test, y_test_pred))","8d0d5128":"cfm=confusion_matrix(y_test, y_test_pred)\ndisp = ConfusionMatrixDisplay(confusion_matrix=cfm,\n                               display_labels=svc.classes_)\ndisp.plot()","ecc9023d":"from sklearn.metrics import classification_report\nprint(classification_report(y_test, y_test_pred))","8e00ca10":"y_test_pred_prob=lr.predict_proba(X_test)[:,1]\ny_test_pred_prob\n\nfrom sklearn.metrics import roc_curve\nmetrics.roc_auc_score(y_test, y_test_pred_prob)","02b92fe0":"fpr, tpr,thresholds=roc_curve(y_test,y_test_pred_prob)\nplt.figure(figsize=(10,10))\nplt.plot([0,1],[0,1],'k--')\nplt.plot(fpr, tpr, label='Logistic Regression')\nplt.xlabel(\"fpr (False Possitive rate)\")\nplt.ylabel(\"tpr-(True Positive rate)\")\nplt.title(\"ROC_AUC\")\nplt.show()","b93e1c17":"no_skill=len(y==1)\/len(y)\ny_test_prob=knn.predict_proba(X_test)[:,1]\nplt.figure(figsize=(10,8))\nplt.plot([0,1],[no_skill, no_skill], label=\"No Skill\")\nprecision, recall,_ =precision_recall_curve(y_test, y_test_prob)\nplt.plot(recall, precision, marker='',label=\"Knn\")\nplt.xlabel(\"Recall\")\nplt.ylabel(\"Precision\")\nplt.title(\"Recall-Precision Curve\")\nplt.legend()\nplt.show()","975d7572":"col=['symmetry_se', 'texture_se','fractal_dimension_mean', 'smoothness_se']\n\nX=df.drop(col, axis=1)\ny=df['diagnosis']\nX_train, X_test, y_train, y_test=train_test_split(X, y, test_size=0.3, random_state=10)","28e36224":"clf_gini = DecisionTreeClassifier(criterion='gini', max_depth=3, random_state=0)\n\n\n# fit the model\nclf_gini.fit(X_train, y_train)","cbb70e4d":"y_pred_gini = clf_gini.predict(X_train)\n","3c0bb134":"print(\"Accuracy Score:-\", metrics.accuracy_score(y_train, y_pred_gini))\nprint(\"F1 Score:-\", metrics.f1_score(y_train, y_pred_gini))\nprint(\"Average Precision Score:-\", metrics.average_precision_score(y_train, y_pred_gini))\nprint(\"Log Loss:-\", metrics.log_loss(y_train, y_pred_gini))\nprint(\"Precision Score:-\", metrics.precision_score(y_train, y_pred_gini))\nprint(\"Recall Score:-\", metrics.recall_score(y_train, y_pred_gini))\nprint(\"ROC-AUC Score:-\", metrics.roc_auc_score(y_train, y_pred_gini))","6abd04fe":"y_test_pred= clf_gini.predict(X_test)\n","90514acc":"dc_acc=metrics.accuracy_score(y_test, y_test_pred)\nprint(\"Accuracy Score:-\",dc_acc)\nprint(\"F1 Score:-\", metrics.f1_score(y_test, y_test_pred))\nprint(\"Average Precision Score:-\", metrics.average_precision_score(y_test, y_test_pred))\nprint(\"Log Loss:-\", metrics.log_loss(y_test, y_test_pred))\nprint(\"Precision Score:-\", metrics.precision_score(y_test, y_test_pred))\nprint(\"Recall Score:-\", metrics.recall_score(y_test, y_test_pred))\nprint(\"ROC-AUC Score:-\", metrics.roc_auc_score(y_test, y_test_pred))","b1d74fbc":"cfm=confusion_matrix(y_test, y_test_pred)\ndisp = ConfusionMatrixDisplay(confusion_matrix=cfm,\n                               display_labels=clf_gini.classes_)\ndisp.plot()","9f30e371":"# k=31 looks good","584081e6":"# Visualize decision-trees","3f8590ef":"# K-NN","7fa997a2":"# SVC -Support Vector Classifier","b2168e86":"# Conclusion:\n* As we can see that in my previsous notebook the accuracy was 76%, while in this case the accuracy is 87.43%.\n* It is observed that removing the two feature help ub increasing the accuracy of the dataset using KNN model as prediction\n* Log Loss is decreased as it was 8.482 and now it is 4.33\n* ROC-AUC is improved , now it is 0.846 , which is no improvement of previous results in last notbook","f4154a65":"# Conclusion:-\nAs we can see that the optimum K in KNN we get is K=3.","75e77344":"# Obs\n* There is no missing values\n* It is well settled dataset","2addef1e":"# Obs\n* As K increase the accuracy decrease\n* K is hyperparameter ,\n* K-3 is good to find the accuracy of the dataset","93c9a624":"# Decision Tree Classifier with criterion entropy","01def0f4":"# Decision Tree Clasifiers","f0872897":"# K-16 is good","248b054b":"# Logistic Regresion","70b0138c":"# Decision Tree Clasifiers","876740a5":"# KNN","8e081a77":"# Observation:\n* Using the multiple solver we can see that \"newton-cg\" work good\n* Error rate is very low for this ","6898b555":"# Conclusion\n* Uing the multiple classifier we came to the conclusion that\n* Logistic regression works good\n* As other classifier is either making the dataset as underfit or overfit\n* OVERFIT- using the Decision Tree , Random Forest \n* Underfit- KNN, SVM\n","cec9dd7a":"# Conclusion\n* Removing the 3 features those having the high P-value \n* Using the logistic regression , using the different solver and calculating the error rate , we observe that error is low for the solver \"newton-cg\"\n* One of the main thing is that Accuracy score is much higher using the logitic regression \n* Now the accuracy is increased 3 fold the previous score.\n* Now the accuracy score is 0.91","69e092e9":"# Simple Cross validation ","b34451ad":"# Peasron correlation and P-values","d40484e9":"# Missing Value","674fbd86":"# SVC -Support Vector Classifier","e3aa219e":"# K-Fold Cross Validation","c5d1cd27":"# Removing the other features ","56e0d567":"# Performing K-fold cross validation with Logistic Regression"}}