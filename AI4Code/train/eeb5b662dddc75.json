{"cell_type":{"38765d5e":"code","472255d1":"code","b3accd73":"code","8b659d9e":"code","24583a1f":"code","865550f6":"code","04c3f76f":"code","5ab30a4d":"code","b78d2fd6":"code","07b15e79":"code","dacb0f28":"code","032636b0":"code","e76a08a1":"code","2bc82324":"code","ffbe13eb":"code","c53004b0":"code","df51856d":"code","e5658e0a":"code","b7419959":"code","c523bcec":"code","e55d85cd":"code","a297e2a5":"code","8acefa62":"code","f9c38cad":"code","fdff7869":"code","12e58b6d":"code","a1bab177":"code","1812dcab":"code","57bfc1f7":"code","d2c3b241":"code","52e8ccd5":"code","23a53740":"code","5e2f43fe":"code","a88aed79":"code","f91599ca":"code","652a7551":"code","3d92fd16":"code","bd1e3843":"code","2367ad5a":"code","d23396a6":"code","fa5a7e8c":"code","52055982":"code","cf33bd08":"code","f780be8d":"code","808cb694":"code","2afdf304":"code","78edd810":"code","adee129a":"code","2e9b45ea":"code","971e5222":"code","4db7d9df":"code","a00c34b8":"code","70a747de":"code","0878c1ec":"code","55c88a0f":"code","0f31e323":"code","6cce71a5":"code","4ce070e6":"code","b28804ef":"code","85f584cb":"code","e1d8620c":"code","f27957df":"code","cc43dd51":"markdown","0a169b6b":"markdown","ffcb571f":"markdown","1239edee":"markdown","44367ae7":"markdown","dc69cbc7":"markdown","75f89d74":"markdown","3a0efa27":"markdown","cb5d9e9c":"markdown","1d1c93b7":"markdown","c1bed474":"markdown","8fbbd664":"markdown","eab33829":"markdown","f27831ba":"markdown","d8a69ba6":"markdown","62e7e7ad":"markdown","ee90ea56":"markdown","9eff169b":"markdown","df4177d3":"markdown","40cf4cc0":"markdown","02a90606":"markdown","de982048":"markdown","3a018111":"markdown","6e873ed0":"markdown","d66e4ac6":"markdown","a047b4e6":"markdown","50c6dc13":"markdown","2fa5585c":"markdown","f93dc682":"markdown","80f5488f":"markdown","50621428":"markdown","5dbc7e07":"markdown","29053abf":"markdown","1d312fc2":"markdown","892d194a":"markdown","009e9df8":"markdown","1d7e76a2":"markdown","973978cc":"markdown","c4dffccb":"markdown","42b44b65":"markdown","4f634d67":"markdown","a6e0f00d":"markdown"},"source":{"38765d5e":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.linear_model import LinearRegression, Ridge, Lasso\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.impute import SimpleImputer\nimputer=SimpleImputer(missing_values=np.nan,strategy='median')\nfrom sklearn.ensemble import BaggingRegressor\nimport xgboost\nfrom numpy import nan\nfrom numpy import isnan\nfrom sklearn.metrics import mean_squared_error","472255d1":"train_data = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest_data = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')","b3accd73":"train_data","8b659d9e":"train_data.info()","24583a1f":"sns.set(rc={'figure.figsize':(30,10)})\nsns.heatmap(train_data.isnull())\nplt.show()","865550f6":"# total sum of missing values in the given column\nmis_val = train_data.isna().sum()\nmis_val","04c3f76f":"# percentage of missing value\nmis_val_percentage = train_data.isnull().sum() \/ len(train_data) *100\nprint(round(mis_val_percentage, 2))","5ab30a4d":"# make a table with the results\nmis_val_table = pd.concat([mis_val, mis_val_percentage], axis = 1).rename(columns = {0:'Missing Values', 1: '% of Total Values'})\nmis_val_table","b78d2fd6":"mis_val_table= mis_val_table.sort_values(by = '% of Total Values', ascending = False).round(1)\nmis_val_table.head(15)","07b15e79":"# get the columns with more than 10% missing values\nmissing_columns = list(mis_val_table[mis_val_table['% of Total Values'] > 10]. index)\nlen(missing_columns)","dacb0f28":"train_data = train_data.drop(columns = list (missing_columns))\ntrain_data","032636b0":"test_data = test_data.drop(columns = ['PoolQC','MiscFeature','Alley','Fence','FireplaceQu','LotFrontage'])\ntest_data","e76a08a1":"train_data['SalePrice'].hist(bins = 100, grid = False, color = 'blue', figsize = (10,6))\nplt.show()","2bc82324":"train_data.agg({'SalePrice':['mean', 'median','skew','kurtosis', 'std','min','max']})","ffbe13eb":"np.log(train_data['SalePrice']).plot.hist(bins=30, color = 'blue')\nplt.xlabel('Log(sale)', fontsize=20)\nplt.ylabel('Frequency', fontsize=18)\nplt.xticks(fontsize =25)\nplt.yticks(fontsize = 25)\nplt.show()","c53004b0":"train_numerical_subset = train_data.select_dtypes(include = [np.number])\ntrain_numerical_subset","df51856d":"train_numerical_subset.hist(figsize = (30,30), grid = False, color ='blue')\nplt.show()","e5658e0a":"num_rows = len(train_numerical_subset.index)\nlow_information_cols = []\n\nfor col in train_numerical_subset.columns:\n    cnts = train_numerical_subset[col].value_counts(dropna=False)\n    top_pct = (cnts\/num_rows).iloc[0]\n    \n    if top_pct > 0.85:\n        low_information_cols.append(col)\n        print('{0}: {1:.5f}%'.format(col, top_pct*100))\n        print(cnts)\n        print()","b7419959":"train_numerical_subset = train_numerical_subset.drop(columns = ['PoolArea','ScreenPorch',\n                 'EnclosedPorch','MiscVal','3SsnPorch','LowQualFinSF',\n                'BsmtFinSF2', 'KitchenAbvGr','BsmtHalfBath','LowQualFinSF'])\ntrain_numerical_subset","c523bcec":"test_numerical_subset = test_data.select_dtypes(include = [np.number])\ntest_numerical_subset = test_numerical_subset.drop(columns = ['PoolArea','ScreenPorch',\n                 'EnclosedPorch','MiscVal','3SsnPorch','LowQualFinSF',\n                'BsmtFinSF2', 'KitchenAbvGr','BsmtHalfBath','LowQualFinSF'])\ntest_numerical_subset","e55d85cd":"correlation = train_numerical_subset.corr()['SalePrice'].sort_values().dropna()\ncorrelation.head()","a297e2a5":"correlation.tail()","8acefa62":"fig, axes = plt.subplots(9, 3, figsize=(14, 30))\naxe = axes.ravel()\nfor i, col in enumerate(train_numerical_subset.columns.values[:-1]):\n    train_numerical_subset.plot(x=('SalePrice'),y=(col),ax=axe[i], kind = 'scatter', color='blue')\nplt.show()","f9c38cad":"train_categorical_subset = train_data.select_dtypes(exclude = [np.number])\ntrain_categorical_subset","fdff7869":"num_rows = len(train_categorical_subset.index)\nlow_information_cols = []\n\nfor col in train_categorical_subset.columns:\n    cnts = train_categorical_subset[col].value_counts(dropna=False)\n    top_pct = (cnts\/num_rows).iloc[0]\n    \n    if top_pct > 0.85:\n        low_information_cols.append(col)\n        print('{0}: {1:.5f}%'.format(col, top_pct*100))\n        print(cnts)\n        print()","12e58b6d":"train_categorical_subset = train_categorical_subset.drop(columns = ['Street','LandContour',\n            'Utilities','LandSlope','Condition1','Condition2','RoofMatl',\n            'ExterCond','BsmtCond','BsmtFinType2','Heating','CentralAir',\n            'Electrical','Functional', 'GarageQual','GarageCond','PavedDrive',\n            'SaleType'])\ntrain_categorical_subset","a1bab177":"test_categorical_subset = test_data.select_dtypes(exclude = [np.number])\ntest_categorical_subset = test_categorical_subset.drop(columns = ['Street','LandContour',\n            'Utilities','LandSlope','Condition1','Condition2','RoofMatl',\n            'ExterCond','BsmtCond','BsmtFinType2','Heating','CentralAir',\n            'Electrical','Functional', 'GarageQual','GarageCond','PavedDrive',\n            'SaleType'])\ntest_categorical_subset","1812dcab":"np.seterr(divide = 'ignore')\nlog_train_numerical_subset = np.log(train_numerical_subset)\nlog_train_numerical_subset = log_train_numerical_subset.replace(to_replace = float('-inf'), value = 0)\nlog_train_numerical_subset","57bfc1f7":"log_test_numerical_subset = np.log(test_numerical_subset)\nlog_test_numerical_subset = log_test_numerical_subset.replace(to_replace = float('-inf'), value = 0)\nlog_test_numerical_subset","d2c3b241":"train_categorical_subset.describe().T","52e8ccd5":"test_categorical_subset.describe().T","23a53740":"train_categorical_subset['HouseStyle'] = train_categorical_subset['HouseStyle'].replace(['2.5Fin'],train_categorical_subset['HouseStyle'].mode())\ntrain_categorical_subset['Exterior1st'] = train_categorical_subset['Exterior1st'].replace(['Stone'],train_categorical_subset['Exterior1st'].mode())\ntrain_categorical_subset['Exterior1st'] = train_categorical_subset['Exterior1st'].replace(['ImStucc'],train_categorical_subset['Exterior1st'].mode())\ntrain_categorical_subset['Exterior2nd'] = train_categorical_subset['Exterior2nd'].replace(['Other'],train_categorical_subset['Exterior1st'].mode())","5e2f43fe":"train_categorical_subset = pd.get_dummies(train_categorical_subset)\ntest_categorical_subset = pd.get_dummies(test_categorical_subset)\n\nprint(train_categorical_subset.shape)\nprint(test_categorical_subset.shape)","a88aed79":"train_features = pd.concat([log_train_numerical_subset,train_categorical_subset], axis = 1)\ntest_features = pd.concat([log_test_numerical_subset,test_categorical_subset], axis = 1)","f91599ca":"train_features = train_features[:-1]\ntrain_features","652a7551":"test_features","3d92fd16":"train_features_multicollinearity = train_features.corr()\ntrain_features_multicollinearity","bd1e3843":"# checking the any variables who correlation coeffcient is above 70.\nlen(train_features_multicollinearity[train_features_multicollinearity > .70])","2367ad5a":"merged_data = pd.concat([train_features, test_features], axis = 0)\nmerged_data","d23396a6":"train = merged_data.iloc[:1459,:]\ntest = merged_data.iloc[1459:,:]\nprint(train.shape)\nprint(test.shape)","fa5a7e8c":"# dropping sale price column from test set which has null values\ntest =test.drop(['SalePrice'], axis = 1)","52055982":"test.shape","cf33bd08":"X_train = train.drop(['SalePrice', 'Id'], axis = 1)\ny_train = np.array(train['SalePrice']).reshape((-1,1))\nX_test = test.drop(['Id'], axis = 1)\n\nprint(X_train.shape)\nprint(y_train.shape)\nprint(X_test.shape)","f780be8d":"imputer = SimpleImputer(missing_values=nan, strategy='median')\nX_train = imputer.fit_transform(X_train)\nprint('Missing: %d' % isnan(X_train).sum())","808cb694":"y_train = imputer.fit_transform(y_train)\nprint('Missing: %d' % isnan(y_train).sum())","2afdf304":"X_test = imputer.fit_transform(X_test)\nprint('Missing: %d' % isnan(X_test).sum())","78edd810":"# Make sure all values are finite\nprint(np.where(~np.isfinite(X_train)))\nprint(np.where(~np.isfinite(y_train)))\nprint(np.where(~np.isfinite(X_test)))","adee129a":"def RMSE(y_train, y_pred):\n    return mean_squared_error(y_train, y_pred,squared = False)","2e9b45ea":"def fit_and_evaluate (model):\n    model.fit(X_train, y_train.ravel())\n    model_pred = model.predict(X_test)\n    model_RMSE = RMSE(y_train, model_pred)\n    \n    return model_RMSE","971e5222":"# Support Vector Regressor\nsvr = SVR(C=1000, gamma = 0.1)\nsvr_RMSE = fit_and_evaluate(svr)\nsvr_RMSE","4db7d9df":"# Random Forest Regressor\nrandom_forest = RandomForestRegressor(random_state=60)\nrandom_forest_RMSE = fit_and_evaluate(random_forest)\nrandom_forest_RMSE","a00c34b8":"# Gradient Boosting Regressor\ngradient_boosted = GradientBoostingRegressor(learning_rate = 0.1,random_state=60)\ngradient_boosted_RMSE = fit_and_evaluate(gradient_boosted)\ngradient_boosted_RMSE","70a747de":"# Bagging Regressor\nbagging = BaggingRegressor()\nbagging_RMSE = fit_and_evaluate(bagging)\nbagging_RMSE","0878c1ec":"#XGB Regressor\nxboost = xgboost.XGBRegressor(random_state = 60)\nxboost_RMSE = fit_and_evaluate(xboost)\nxboost_RMSE","55c88a0f":"# KNeighbors Regressor\nknn = KNeighborsRegressor(n_neighbors=10)\nknn_RMSE = fit_and_evaluate(knn)\nknn_RMSE","0f31e323":"model_comparison = pd.DataFrame({'model': [ 'Support Vector Machine',\n                                           'Random Forest', 'Gradient Boosted',\n                                            'K-Nearest Neighbors', 'Bagging','XBoost'],\n                                 'RMSE': [svr_RMSE, random_forest_RMSE, \n                                         gradient_boosted_RMSE, knn_RMSE,bagging_RMSE,xboost_RMSE]})\n\nmodel_comparison.sort_values('RMSE', ascending = False).plot(x = 'model',\n                                                             y = 'RMSE', kind = 'barh', color = 'red',\n                                                             edgecolor = 'black', figsize = (10,4))\nplt.xlabel('Root Mean Squared Error')\nplt.title('Model Comparison on Test RMSE')\nplt.show()","6cce71a5":"SVM = SVR().fit(X_train, y_train.ravel())\nscore = SVM.score(X_train, y_train)\nprint('R_squared:', score)\nprint('RMSE:', svr_RMSE)","4ce070e6":"HousePrice_Prediction_with_log = SVM.predict(X_test)\nHousePrice_Prediction_with_log","b28804ef":"HousePrice_Prediction_without_log = np.exp(HousePrice_Prediction_with_log)\nHousePrice_Prediction_without_log","85f584cb":"submission = pd.DataFrame()\nsubmission['Id'] = test_data.Id\nsubmission['SalePrice'] = HousePrice_Prediction_without_log","e1d8620c":"# checking the dataset\nsubmission","f27957df":"submission.to_csv('submission.csv', index = False)","cc43dd51":"The histogram shows us that we have problem with Outliers. The grapgh is positively skewed with a value of 1.88. Other way to check is that mean of positively skewed data will be greater than the median, which is true in the above case.\n\nThe aaveage sale price of a house in the dataset is close to 180,000, with most of the values failing with 130,000 to 215,000 range.","0a169b6b":"# Numerical Features","ffcb571f":"We will compare five different machine learning models using the great Scikit-Learn library:\n\n1. Support Vector Machine Regression\n2. Random Forest Regression\n3. Gradient Boosting Regression\n4. K-Nearest Neighbors Regression\n5. Boosting Regressor\n6. XBoost","1239edee":"As per the result, correlation coefficient value between the explanatory variables are lower than the standard threshold of 0.70, which implies that there is no issue of multicollinearity in the sample.\n\nAccording to Asteriou and Hall (2016), correlation coefficient\nvalue of 0.9 is acceptable, however he argues that the threshold can vary from\nresearch to research.","44367ae7":"EDA is an open-ended process where we make plots and calculate statistics in order to explore our data.\n\nThe purpose is to find anomalies, patterns, trends or relationships. For example, finding a correlation between two variables.\n\nTo begin the EDA, we will focus on a single variable, the saleprice, because this is the target for our machine learning models.","dc69cbc7":"# Correlations between numerical features and target variable (SalePrice)","75f89d74":"# Concatinating both train and test data","3a0efa27":"# Imports","cb5d9e9c":"We will also repeat the same process for test numerical dataset","1d1c93b7":"We will follow the same procedure for the test data set.","c1bed474":"Now that we have explored the trends and relationships within the data, we can work on engineering a set of features for our models.","8fbbd664":"Now both the train and test dataset has same number of unique values","eab33829":"Our final dataset now has 165 features (one of the columns is the target). This is still quite a lot, but mostly it is because we have one-hot encoded the categorical variables.","f27831ba":"# Applying one Hot-Encode to categorical variables","d8a69ba6":"# Support Vector Machine Regressor\n\nSince the Supoort Vector Regressor has the lowest Root Mean Squared Error (RMSE) among the regressor. So we will proceed predicting the house price with SVR.","62e7e7ad":"In order to quantify correlations between the numerical features and the target, we can calculate the pearson correlation coefficient. This is a measure of the strength and direction of a linear relationship between two variables.","ee90ea56":"# Log Transformation of numerical variables","9eff169b":"Now, we will transform the predictions to the correct format because we have taken the logarithm previously. To reverse the natural logarithm, we will use exp function.","df4177d3":"So we will remove these categorical variables having same rows with same values over 85 percent.","40cf4cc0":"# Removing collinear Features","02a90606":"# Work Flow\n\n1. Data Cleaning and formatting\n2. Exploratory data analysis\n3. Feature engineering and selection\n4. Evaluate the best model based on RMSE\n5. Make and submit prediction**","de982048":"# Model to Evaluate","3a018111":"# Step 5: Make a submission","6e873ed0":"The dataframe.info method is a quick way to assess the data by displaying the data types of each column and the number of non-missing values.","d66e4ac6":"Let's check the columns which has too many rows having too many similar values.","a047b4e6":"# Categorical Features","50c6dc13":"We will create a csv that contains the predicted SalePrice for each observation in the test.csv dataset.","2fa5585c":"# Step 2: Exploratory Data Analysis","f93dc682":"# Model Comparison","80f5488f":"In the above histogram, there are many variables such as **PoolArea**,**ScreenPorch**, **EnclosedPorch**, **MiscVal**, **3SsnPorch**, **LowQualFinSF**, **BsmtFinSF2**,and **KitchenAbvGr** having too many rows being too many same values and reflect extreme outliers. Outliers can affect a regression model by pulling our estimated regression line further away from the true population regression line.","50621428":"Multicollinearity is a situation in which predictor variables are extremely correlated to each other. The presense of  multicollinearity in the regression model can inflate\nthe standard errors of coefficients. The artificial increase in standard errors can\nlead to biased, inconsistency, and unreliable in estimation.","5dbc7e07":"Let's move forward to check our features. First, we will check the numerical featurs and make some plots to indenitfy the various attribute to see which of them could be use in the house price prediction.","29053abf":"Before applying one hot encode to categorical variable, we have noticed that the unique values between the train set and test set are not matching. For example, in train set, Housestyle has 8 unique items while it has 7 items in test set. The same problem can be seen also in Exterior2nd, RoofStyle. So we need to remove the excess items from the training set.","1d312fc2":"Lets transform the sample using natural logarithm","892d194a":"Looking at the raw data, we can already see a number of issues, we will have to solve. First of all, there are 81 columns and we don't know whether all of them were needed. Later, we will have to make decisions about how we will approach both the numerical and categorical features.","009e9df8":"# Step 3: Feature Engineering and Selection","1d7e76a2":"We are not going to repeat the same process for test dataset. What we will do is to drop the same column what we have dropped in the training set. The reason is that we need to have the same shape both in the training and testing set for concatination.","973978cc":"# Step 1:  Data Cleaning and Formatting","c4dffccb":"In this project, we will take the following steps for feature engineering:\n\n- Add in the log transformation of the numerical variables\n- One-hot encode the categorical variables\n\nFor feature selection, we will do the following:\n\n- Remove collinear features(multicollinearity)","42b44b65":"# Data Types and Missing Values","4f634d67":"# Split into Training and Testing sets","a6e0f00d":"# Imputing missing values"}}