{"cell_type":{"b3aa51d2":"code","f0aef01f":"code","49583c2e":"code","e132a884":"code","54d37faa":"code","5ea4aefb":"code","ee996c64":"code","bce40a12":"code","285675d4":"code","62e292ff":"code","b7673e4a":"code","5b3cbfac":"code","ed54d812":"code","f314aeb8":"code","ade65252":"code","a690869a":"code","e6a1e3e3":"code","8ab0d052":"code","66d408a0":"code","9c44e6c1":"code","351e6062":"code","6d0171f3":"code","0539368d":"code","75755062":"code","65c2a72b":"code","f282eea3":"code","8065c654":"code","c450a2cb":"code","d707fca3":"code","ad69252b":"code","447e0d63":"code","4e7d5407":"code","b95c9e3a":"code","16cf7780":"code","1f8b2693":"code","77d104e7":"code","5cca25a6":"code","85ef585c":"code","23ba9bfa":"code","b9fc5ff1":"code","5cad203d":"code","7e66fc63":"code","874537c8":"code","5ffe7ae6":"code","759bf4b6":"markdown","30b4c78f":"markdown","f152f50e":"markdown","5a386f99":"markdown","275ce261":"markdown","80d7fc85":"markdown","565b5451":"markdown","a88cbc1e":"markdown","872fa894":"markdown","42fe0148":"markdown","ca03762e":"markdown","b35e836a":"markdown","ab972689":"markdown","48ceaf25":"markdown","3ceca93c":"markdown","e0be310f":"markdown","a8951195":"markdown","93d61194":"markdown","7e29f3db":"markdown","59541597":"markdown","5b0f5de3":"markdown","693bedf4":"markdown","0abbf49c":"markdown","a6117735":"markdown","837d63ff":"markdown","897345d0":"markdown","f3b88e22":"markdown"},"source":{"b3aa51d2":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","f0aef01f":"# linear algebra\nimport numpy as np \n\n# data processing\nimport pandas as pd \n\n# data visualization\nimport seaborn as sns\n%matplotlib inline\nfrom matplotlib import pyplot as plt\nfrom matplotlib import style\n","49583c2e":"test_df = pd.read_csv(\"..\/input\/titanic\/test.csv\")\ntrain_df = pd.read_csv(\"..\/input\/titanic\/train.csv\")\ntrain_df.sample(5)","e132a884":"train_df.info()","54d37faa":"train_df.describe()","5ea4aefb":"train_df.head(10)","ee996c64":"total = train_df.isnull().sum().sort_values(ascending=False)\npercent_1 = train_df.isnull().sum()\/train_df.isnull().count()*100\npercent_2 = (round(percent_1, 1)).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent_2], axis=1, keys=['Total', '%'])\nmissing_data.head(5)","bce40a12":"survived = 'survived'\nnot_survived = 'not survived'\nfig, axes = plt.subplots(nrows=1, ncols=2,figsize=(10, 4))\nwomen = train_df[train_df['Sex']=='female']\nmen = train_df[train_df['Sex']=='male']\nax = sns.distplot(women[women['Survived']==1].Age.dropna(), bins=18, label = survived, ax = axes[0], kde =False)\nax = sns.distplot(women[women['Survived']==0].Age.dropna(), bins=40, label = not_survived, ax = axes[0], kde =False)\nax.legend()\nax.set_title('Female')\nax = sns.distplot(men[men['Survived']==1].Age.dropna(), bins=18, label = survived, ax = axes[1], kde = False)\nax = sns.distplot(men[men['Survived']==0].Age.dropna(), bins=40, label = not_survived, ax = axes[1], kde = False)\nax.legend() \nax.set_title('Male')","285675d4":"FacetGrid = sns.FacetGrid(train_df, row='Embarked', size=4.5, aspect=1.6)\nFacetGrid.map(sns.pointplot, 'Pclass', 'Survived', 'Sex', palette=None,  order=None, hue_order=None )\nFacetGrid.add_legend()","62e292ff":"sns.barplot(x='Pclass', y='Survived', data=train_df)","b7673e4a":"grid = sns.FacetGrid(train_df, col='Survived', row='Pclass', size=2.2, aspect=1.6)\ngrid.map(plt.hist, 'Age', alpha=.5, bins=20)\ngrid.add_legend()","5b3cbfac":"data = [train_df, test_df]\nfor dataset in data:\n    dataset['relatives'] = dataset['SibSp'] + dataset['Parch']\n    dataset.loc[dataset['relatives'] > 0, 'not_alone'] = 0\n    dataset.loc[dataset['relatives'] == 0, 'not_alone'] = 1\n    dataset['not_alone'] = dataset['not_alone'].astype(int)\ntrain_df['not_alone'].value_counts()","ed54d812":"axes = sns.factorplot('relatives','Survived', data=train_df, aspect = 2.5 )","f314aeb8":"train_df = train_df.drop(['PassengerId'], axis=1)","ade65252":"train_df.head()","a690869a":"#dropping the 'Cabin' feature \ntrain_df = train_df.drop(['Cabin'], axis=1)\ntest_df = test_df.drop(['Cabin'], axis=1)","e6a1e3e3":"data = [train_df, test_df]\n\nfor dataset in data:\n    mean = train_df[\"Age\"].mean()\n    std = test_df[\"Age\"].std()\n    is_null = dataset[\"Age\"].isnull().sum()\n    # compute random numbers between the mean, std and is_null\n    rand_age = np.random.randint(mean - std, mean + std, size = is_null)\n    # fill NaN values in Age column with random values generated\n    age_slice = dataset[\"Age\"].copy()\n    age_slice[np.isnan(age_slice)] = rand_age\n    dataset[\"Age\"] = age_slice\n    dataset[\"Age\"] = train_df[\"Age\"].astype(int)\nprint(\"No. of missing value in Age feature :\" ,str(train_df[\"Age\"].isnull().sum()))","8ab0d052":"#Since the Embarked feature has only 2 missing values, we will just fill these with the most common one\ntrain_df['Embarked'].describe()","66d408a0":"common_value = 'S'\ndata = [train_df, test_df]\n\nfor dataset in data:\n    dataset['Embarked'] = dataset['Embarked'].fillna(common_value)","9c44e6c1":"train_df.info()","351e6062":"#Converting \u201cFare\u201d from float to int\ndata = [train_df, test_df]\n\nfor dataset in data:\n    dataset['Fare'] = dataset['Fare'].fillna(0)\n    dataset['Fare'] = dataset['Fare'].astype(int)","6d0171f3":"#Convert \u2018Sex\u2019 feature into numeric\ngenders = {\"male\": 0, \"female\": 1}\ndata = [train_df, test_df]\n\nfor dataset in data:\n    dataset['Sex'] = dataset['Sex'].map(genders)","0539368d":"train_df['Ticket'].describe()","75755062":"train_df = train_df.drop(['Ticket'], axis=1)\ntest_df = test_df.drop(['Ticket'], axis=1)","65c2a72b":"#Convert \u2018Embarked\u2019 feature into numeric\nports = {\"S\": 0, \"C\": 1, \"Q\": 2}\ndata = [train_df, test_df]\n\nfor dataset in data:\n    dataset['Embarked'] = dataset['Embarked'].map(ports)","f282eea3":"train_df = train_df.drop(['Name'], axis=1)\ntest_df = test_df.drop(['Name'], axis=1)","8065c654":"data = [train_df, test_df]\nfor dataset in data:\n    dataset['Age'] = dataset['Age'].astype(int)\n    dataset.loc[ dataset['Age'] <= 11, 'Age'] = 0\n    dataset.loc[(dataset['Age'] > 11) & (dataset['Age'] <= 18), 'Age'] = 1\n    dataset.loc[(dataset['Age'] > 18) & (dataset['Age'] <= 22), 'Age'] = 2\n    dataset.loc[(dataset['Age'] > 22) & (dataset['Age'] <= 27), 'Age'] = 3\n    dataset.loc[(dataset['Age'] > 27) & (dataset['Age'] <= 33), 'Age'] = 4\n    dataset.loc[(dataset['Age'] > 33) & (dataset['Age'] <= 40), 'Age'] = 5\n    dataset.loc[(dataset['Age'] > 40) & (dataset['Age'] <= 66), 'Age'] = 6\n    dataset.loc[ dataset['Age'] > 66, 'Age'] = 6\n\n# let's see how it's distributed \ntrain_df['Age'].value_counts()","c450a2cb":"train_df.head(10)","d707fca3":"#use sklearn \u201cqcut()\u201d function, to see how we can form the categories.\n\ndata = [train_df, test_df]\n\nfor dataset in data:\n    dataset.loc[ dataset['Fare'] <= 7.91, 'Fare'] = 0\n    dataset.loc[(dataset['Fare'] > 7.91) & (dataset['Fare'] <= 14.454), 'Fare'] = 1\n    dataset.loc[(dataset['Fare'] > 14.454) & (dataset['Fare'] <= 31), 'Fare']   = 2\n    dataset.loc[(dataset['Fare'] > 31) & (dataset['Fare'] <= 99), 'Fare']   = 3\n    dataset.loc[(dataset['Fare'] > 99) & (dataset['Fare'] <= 250), 'Fare']   = 4\n    dataset.loc[ dataset['Fare'] > 250, 'Fare'] = 5\n    dataset['Fare'] = dataset['Fare'].astype(int)\n    \ntrain_df[\"Fare\"].value_counts()","ad69252b":"#Age times Class\n\ndata = [train_df, test_df]\nfor dataset in data:\n    dataset['Age_Class']= dataset['Age']* dataset['Pclass']","447e0d63":"#Fare Per Person\n\nfor dataset in data:\n    dataset['Fare_Per_Person'] = dataset['Fare']\/(dataset['relatives']+1)\n    dataset['Fare_Per_Person'] = dataset['Fare_Per_Person'].astype(int)\n# Let's take a last look at the training set, before we start training the models.\ntrain_df.head(7)","4e7d5407":"dfX_train = train_df.drop(\"Survived\", axis=1)\ndfY_train = train_df[\"Survived\"]\ndf_test  = test_df.drop(\"PassengerId\", axis=1).copy()","b95c9e3a":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, Y_train, Y_test = train_test_split(dfX_train,dfY_train)","16cf7780":"from sklearn.ensemble import RandomForestClassifier\nrandom_forest = RandomForestClassifier(n_estimators=20, max_depth=5, criterion='entropy')\nrandom_forest.fit(X_train, Y_train)\n\nY_prediction = random_forest.predict(df_test)\n\nrandom_forest.score(X_train, Y_train)\nacc_random_forest = round(random_forest.score(X_train, Y_train) * 100, 2)","1f8b2693":"print('Score =',str( acc_random_forest))","77d104e7":"random_forest.score(X_train, Y_train)","5cca25a6":"random_forest.score(X_test, Y_test)","85ef585c":"predictions = random_forest.predict(df_test)","23ba9bfa":"output = pd.DataFrame({'PassengerId': test_df.PassengerId, 'Survived': predictions})\noutput.to_csv('my_submission_01.csv', index=False)\nprint(\"Your submission was successfully saved!\")","b9fc5ff1":"from sklearn.tree import DecisionTreeClassifier\ndecision_tree = DecisionTreeClassifier(max_depth=5) \ndecision_tree.fit(X_train, Y_train)  \nY_pred = decision_tree.predict(df_test)  \nacc_decision_tree = round(decision_tree.score(X_train, Y_train) * 100, 2)","5cad203d":"print('Score =',str( acc_decision_tree))","7e66fc63":"decision_tree.score(X_train, Y_train)","874537c8":"decision_tree.score(X_test, Y_test)","5ffe7ae6":"output = pd.DataFrame({'PassengerId': test_df.PassengerId, 'Survived': predictions})\noutput.to_csv('my_submission_02.csv', index=False)\nprint(\"Your submission was successfully saved!\")","759bf4b6":"# Creating Categories\nwithin the features","30b4c78f":"Ticket attribute has 681 unique tickets, it will be a bit tricky to convert them into useful categories. So we will drop it from the dataset.","f152f50e":"Here we can see that you had a high probabilty of survival with 1 to 3 realitves, but a lower one if you had less than 1 or more than 3","5a386f99":"# Data Preprocessing","275ce261":"You can see that men have a high probability of survival when they are between 18 and 30 years old, which is also a little bit true for women but not fully. For women the survival chances are higher between 14 and 40.\nFor men the probability of survival is very low between the age of 5 and 18, but that isn\u2019t true for women. Another thing to note is that infants also have a little bit higher probability of survival.","80d7fc85":"# Applying RandomForest","565b5451":"We can see that 38% out of the training set survived the titanic.","a88cbc1e":"# Age and Sex","872fa894":"'Age' feature will be tricky to deal, it has 177 missing values.'Embarked' two missing values can be easily filled.we might want to drop 'Cabin' from the dataset, since 77% of it are missing.","42fe0148":"# Getting the Data","ca03762e":"# Missing Data","b35e836a":"We can see that Pclass is changing person's survival chances.If the person is in Pclass 1 the chances of survival was very high in comparison to other.","ab972689":"# SibSp and Parch","48ceaf25":"# Importing the Libraries","3ceca93c":"**We can see that, there's a high probability that a person in Pclass 3 will not survive** ","e0be310f":"# Applying Decision Tree","a8951195":"# Pclass","93d61194":"**short description of features**\n1.  survival:    Survival \n1.  PassengerId: Unique Id of a passenger. \n1.  pclass:    Ticket class     \n1.  sex:    Sex     \n1.  Age:    Age in years     \n1.  sibsp:    # of siblings \/ spouses aboard the Titanic     \n1.  parch:    # of parents \/ children aboard the Titanic     \n1.  ticket:    Ticket number     \n1.  fare:    Passenger fare     \n1.  cabin:    Cabin number     \n1.  embarked:    Port of Embarkation","7e29f3db":"**Embarked seems to be correlated with survival, depending on the gender.**\n\nWomen on port Q and on port S have a higher chance of survival. The inverse is true, if they are at port C. Men have a high survival probability if they are on port C, but a low probability if they are on port Q or S.","59541597":"We need to convert a lot of features into numeric ones, so that the machine learning algorithms can process them. Furthermore, we can see that the features have widely different ranges, that we will need to convert into roughly the same scale. We can also spot some more features, that contain missing values (NaN = not a number), that we need to deal with.","5b0f5de3":"# Data Exploration","693bedf4":"**Let everything except \u2018PassengerId\u2019, \u2018Ticket\u2019 and \u2018Name\u2019 would be correlated with a high survival rate.**","0abbf49c":"SibSp and Parch would make more sense as a combined feature, that shows the total number of relatives, a person has on the Titanic.","a6117735":"# Embarked, Pclass and Sex","837d63ff":"# Converting Features","897345d0":"# Creating new Features","f3b88e22":"**For 'Fare'**"}}