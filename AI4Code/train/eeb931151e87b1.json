{"cell_type":{"57e9dcf8":"code","00a3cd5b":"code","5078bf9a":"code","1ff13c19":"code","f9c45751":"code","4d2b5d03":"code","1e67b68c":"markdown","a36b0c4e":"markdown","fa2f2a18":"markdown","9bb13734":"markdown","bdbb663b":"markdown"},"source":{"57e9dcf8":"import numpy as np\nimport pandas as pd\nimport os\nimport sys\nimport random\nimport keras\nimport tensorflow as tf\nimport json\nsys.path.insert(0, '..\/input\/pretrained-bert-including-scripts\/master\/bert-master')\n!cp -r '..\/input\/kerasbert\/keras_bert' '\/kaggle\/working'\nBERT_PRETRAINED_DIR = '..\/input\/pretrained-bert-including-scripts\/uncased_l-12_h-768_a-12\/uncased_L-12_H-768_A-12'\nprint('***** BERT pretrained directory: {} *****'.format(BERT_PRETRAINED_DIR))\nimport tokenization  #Actually keras_bert contains tokenization part, here just for convenience","00a3cd5b":"from keras_bert.keras_bert.bert import get_model\nfrom keras_bert.keras_bert.loader import load_trained_model_from_checkpoint\nfrom keras.optimizers import Adam\nadam = Adam(lr=2e-5,decay=0.01)\nmaxlen = 50\nprint('begin_build')\n\nconfig_file = os.path.join(BERT_PRETRAINED_DIR, 'bert_config.json')\ncheckpoint_file = os.path.join(BERT_PRETRAINED_DIR, 'bert_model.ckpt')\nmodel = load_trained_model_from_checkpoint(config_file, checkpoint_file, training=True,seq_len=maxlen)\nmodel.summary(line_length=120)","5078bf9a":"from keras.layers import Dense,Input,Flatten,concatenate,Dropout,Lambda\nfrom keras.models import Model\nimport keras.backend as K\nimport re\nimport codecs\n\nsequence_output  = model.layers[-6].output\npool_output = Dense(1, activation='sigmoid',kernel_initializer=keras.initializers.TruncatedNormal(stddev=0.02),name = 'real_output')(sequence_output)\nmodel3  = Model(inputs=model.input, outputs=pool_output)\nmodel3.compile(loss='binary_crossentropy', optimizer=adam)\nmodel3.summary()","1ff13c19":"def convert_lines(example, max_seq_length,tokenizer):\n    max_seq_length -=2\n    all_tokens = []\n    longer = 0\n    for i in range(example.shape[0]):\n      tokens_a = tokenizer.tokenize(example[i])\n      if len(tokens_a)>max_seq_length:\n        tokens_a = tokens_a[:max_seq_length]\n        longer += 1\n      one_token = tokenizer.convert_tokens_to_ids([\"[CLS]\"]+tokens_a+[\"[SEP]\"])+[0] * (max_seq_length - len(tokens_a))\n      all_tokens.append(one_token)\n    print(longer)\n    return np.array(all_tokens)\n    \nnb_epochs=1\nbsz = 32\ndict_path = os.path.join(BERT_PRETRAINED_DIR, 'vocab.txt')\ntokenizer = tokenization.FullTokenizer(vocab_file=dict_path, do_lower_case=True)\nprint('build tokenizer done')\ntrain_df = pd.read_csv('..\/input\/jigsaw-unintended-bias-in-toxicity-classification\/train.csv')\ntrain_df = train_df.sample(frac=0.01,random_state = 42)\n#train_df['comment_text'] = train_df['comment_text'].replace({r'\\s+$': '', r'^\\s+': ''}, regex=True).replace(r'\\n',  ' ', regex=True)","f9c45751":"# you can load the weights by this line\nprint('load model')\nmodel3.load_weights('..\/input\/save-bert-fine-tuning-model\/bert_weights.h5')","4d2b5d03":"#load test data\ntest_df = pd.read_csv('..\/input\/jigsaw-unintended-bias-in-toxicity-classification\/test.csv')\n#test_df['comment_text'] = test_df['comment_text'].replace({r'\\s+$': '', r'^\\s+': ''}, regex=True).replace(r'\\n',  ' ', regex=True)\neval_lines = test_df['comment_text'].values\nprint(eval_lines.shape)\nprint('load data done')\ntoken_input2 = convert_lines(eval_lines,maxlen,tokenizer)\nseg_input2 = np.zeros((token_input2.shape[0],maxlen))\nmask_input2 = np.ones((token_input2.shape[0],maxlen))\nprint('test data done')\nprint(token_input2.shape)\nprint(seg_input2.shape)\nprint(mask_input2.shape)\nhehe = model3.predict([token_input2, seg_input2, mask_input2],verbose=1,batch_size=bsz)\nsubmission = pd.read_csv('..\/input\/jigsaw-unintended-bias-in-toxicity-classification\/sample_submission.csv', index_col='id')\nsubmission['prediction'] = hehe\nsubmission.reset_index(drop=False, inplace=True)\nsubmission.to_csv('submission.csv', index=False)","1e67b68c":"## Prepare Data, Training, Predicting\n\nFirst the model need train data like [token_input,seg_input,masked input], here we set all segment input to 0 and all masked input to 1.\n\nStill I am finding a more efficient way to do token-convert-to-ids","a36b0c4e":"If you feel this kernel useful, please upvote this kernel and [save part kernel](https:\/\/www.kaggle.com\/hiromoon166\/save-bert-fine-tuning-model).\nAnd, don't forget to upvote [the great kernel](https:\/\/www.kaggle.com\/httpwwwfszyc\/bert-in-keras-taming) by Yue Zhang.","fa2f2a18":"## Build classification model\n\nAs the Extract layer extracts only the first token where \"['CLS']\" used to be, we just take the layer and connect to the single neuron output.","9bb13734":"This kernel is continued from Save BERT fine-tuning model.\nIn this kernel, you can load the weights from that kernel and make a prediction.","bdbb663b":"## Load raw model"}}