{"cell_type":{"442fb665":"code","89a05fd0":"code","44d2491f":"code","db5775e9":"code","3e06b3e5":"code","19c24c43":"code","cccea901":"code","ae419a48":"code","e92591af":"markdown","1260fb14":"markdown","3afbc0f3":"markdown","4c44827b":"markdown","e6c4fc4f":"markdown","7e0d6957":"markdown","416198e8":"markdown","0322fd3f":"markdown","817d7fee":"markdown"},"source":{"442fb665":"#Generic Libraries\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n#SK-Learn Libraries\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.semi_supervised import LabelSpreading\nfrom sklearn import datasets\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.ensemble import RandomForestClassifier","89a05fd0":"#Load Data\niris = datasets.load_iris()","44d2491f":"print(\"Total Records: \", len(iris.data))","db5775e9":"# Random Number Generator\nrng = np.random.RandomState(0)\n\n# Label Spreading\nlabel_prop_model = LabelSpreading()","3e06b3e5":"#Define How Many Samples Should be Unlabeled\nrandom_unlabeled_points = rng.rand(len(iris.target)) <= 0.5 #Almost 50% samples are unlabeled\n\n#Seperate list for Unlabeled Samples\nUnlabeled = np.copy(iris.target)\nUnlabeled[random_unlabeled_points] = -1","19c24c43":"#Inspect\nprint(Unlabeled.T)","cccea901":"#fit to Label Spreading \nlabel_prop_model.fit(iris.data, Unlabeled)\n\n# Predict the Labels for Unlabeled Samples\npred_lb = label_prop_model.predict(iris.data)\n\n#Accuracy of Prediction\nprint(\"Accuracy of Label Spreading: \",'{:.2%}'.format(label_prop_model.score(iris.data,pred_lb)))","ae419a48":"# Feature& Target  Dataset\nX = iris.data\ny = pred_lb  # labels predicted by Label Spreading\n\n#Dataset Split  [train = 90%, test = 10%]\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=0) \n\n#Define Model\nmodel = RandomForestClassifier(verbose = 0, max_depth=2, random_state=0)\n\n#Fit\nmodel.fit(X_train,y_train)\n\n#Prediction\nrf_pred = model.predict(X_test)\n\n#Accuracy Score\nacc = accuracy_score(y_test, rf_pred)\nprint(\"Random Forest Model Accuracy (after Label Spreading): \",'{:.2%}'.format(acc))","e92591af":"## Label Spreading","1260fb14":"## Libraries","3afbc0f3":"### The IRIS dataset that we have here DOES NOT contain unlabeled samples, hence we'll have to manually induce some unlabeled samples into this dataset","4c44827b":"## Inducing Unlabeled Samples","e6c4fc4f":"## Further Reading\n\n* [Semi-Supervised Learning](https:\/\/scikit-learn.org\/stable\/modules\/label_propagation.html)","7e0d6957":"## Data","416198e8":"## Conclusion\n\nIn this notebook we've learned to:\n\n* Manually induce unlabeled samples in dataset\n* Use Semi-Supervised Learning Estimator (Label Spreading) to predict labels for the unlabeled samples\n* Build Classifier Model using the predicted labels.\n","0322fd3f":"# Semi-Supervised Learning & Label Spreading\n\n[Semi-Supervised Learning ](https:\/\/en.wikipedia.org\/wiki\/Semi-supervised_learning) is a situation wherein the some of the samples in the data are not labeled. The Semi-supervised learning falls between unsupervised learning (with no labeled training data) and supervised learning (with only labeled training data). \n\nTo mitigate this, the sk-learn library has **semi-supervised estimators** that are able to make use of this unlabeled data to better capture the shape of underlying data distribution and generalize better to new samples. These algorithms can perform well when there are very small amount of labeled points and a large amount of unlabeled points.\n\n**Note: It is important to assign an identifier to unlabeled points along with the labeled data when training the model with the fit method. The identifier that this implementation uses is the integer value = -1**\n\n**sk-learn** provides two label propagation models: **LabelPropagation** and **LabelSpreading**. Both work by constructing a similarity graph over all items in the input dataset. \n\n\nIn this notebook, we're going to learn specifically about LabelSpreading.\n\n**Label Spreading** =  It minimizes a loss function that has regularization properties and is often more robust to noise. The algorithm iterates on a modified version of the original graph and normalizes the edge weights by computing the normalized graph Laplacian matrix.\n\nLet's have a look at how all this works with our standard dataset i.e. IRIS.\n\n\n\nAs always, I will keep this notebook fairly clean and well-commented for easy understanding. Please do consider it to UPVOTE if you find it helpful :-)\n","817d7fee":"As we can observe, the Label Spreading Estimator has successfully predicted the Labels for our Unlabeled Samples with an astounding accuracy of **100 %**\n\n\nNow lets use this newly predicted labels and try to build a Random Forest Classifier Model"}}