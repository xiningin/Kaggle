{"cell_type":{"20d54c4d":"code","ef11a549":"code","603f59a5":"code","2cef06ab":"code","704a985e":"code","5e3a1863":"code","fb6f365b":"code","ca114a36":"code","92b2b3b6":"code","f4e22944":"code","83ffa803":"markdown","9198bec9":"markdown","bc45b0d7":"markdown","d89c1638":"markdown","eb30fd61":"markdown","de4f7b1e":"markdown","bd0b2733":"markdown","c4f8f895":"markdown","bba2ccc7":"markdown","defd179b":"markdown","cca7620c":"markdown","e7f8f197":"markdown","605c1049":"markdown","48a73cf9":"markdown","3a71b340":"markdown","7cb01e98":"markdown","0c5d6098":"markdown","ac59f869":"markdown","b8cf4c1d":"markdown"},"source":{"20d54c4d":"%matplotlib inline\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nimport numpy as np","ef11a549":"data = pd.read_csv(\"..\/input\/dataset\/diabetes.csv\")\nfeatures = ['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI', 'DiabetesPedigreeFunction', 'Age']\ntarget = ['Outcome']\nprint(data.shape)\ndata.head()","603f59a5":"for column in data:\n    sorted_col = data.sort_values(column)\n    x_max = sorted_col[column].iloc[-1]\n    x_min = sorted_col[column].iloc[0]\n    range_x = x_max - x_min\n    \n    for i in range(data.shape[0]):\n        data.loc[i, column] = (data[column][i] - x_min) \/ range_x\n    \ndata.head()","2cef06ab":"training = data.loc[:(768 * 80 \/\/ 100)]\ntest = data.loc[(768 * 80 \/\/ 100 + 1):]\n\nx_training, y_training = training[features].values, training[target].values\nx_test, y_test = test[features].values, test[target].values\nprint(x_training.shape, y_training.shape)\nprint(x_test.shape, y_test.shape)","704a985e":"# Make a prediction with weights\ndef predict(row, weights):\n    activation = weights[0]\n    for i in range(len(row)-1):\n        activation += weights[i + 1] * row[i]\n    return 1.0 if activation > 0.0 else 0.0\n \n# Estimate Perceptron weights using stochastic gradient descent\ndef train_weights(train, l_rate, n_epoch):\n    weights = [0.0 for i in range(len(train[0]))]    # Weights[0] is bias\n    for epoch in range(n_epoch):\n        sum_error = 0.0\n        for row in train:\n            prediction = predict(row, weights)\n            error = row[-1] - prediction\n            sum_error += error**2\n            weights[0] = weights[0] + l_rate * error\n            for i in range(len(row)-1):\n                weights[i + 1] = weights[i + 1] + l_rate * error * row[i]\n\n    return weights\n\n# Perceptron Algorithm With Stochastic Gradient Descent\ndef perceptron(train, test, l_rate, n_epoch):\n    predictions = list()\n    weights = train_weights(train, l_rate, n_epoch)\n    for row in test:\n        prediction = predict(row, weights)\n        predictions.append(prediction)\n    return predictions\n\nl_rate = 0.1\nn_epoch = 50\npredicted = perceptron(training.values, test.values, l_rate, n_epoch)\n\nfor i in range(10):\n    print(f'Predicted: {predicted[i]}, Actual:{y_test[i][0]}')","5e3a1863":"# Calculate accuracy percentage\ndef accuracy_metric(actual, predicted):\n    correct = 0\n    for i in range(len(actual)):\n        if actual[i][0] == predicted[i]:\n            correct += 1\n    return correct \/ float(len(actual)) * 100.0\n\naccuracy = accuracy_metric(y_test,predicted)\nprint(f'Accuracy = {accuracy}')","fb6f365b":"# Calculate precision percentage\ndef precision_metric(actual, predicted):\n    correct = 0\n    for i in range(len(actual)):\n        if actual[i][0] == 1 and predicted[i] == 1:\n            correct += 1\n    return correct \/ predicted.count(1) * 100.0\n\nprecision = precision_metric(y_test,predicted)\nprint(f'Precision = {precision}')","ca114a36":"# Calculate recall percentage\ndef recall_metric(actual, predicted):\n    correct = 0\n    for i in range(len(actual)):\n        if actual[i][0] == 1 and predicted[i] == 1:\n            correct += 1\n    return correct \/ test[test['Outcome'] == 1].shape[0] * 100.0\n\nrecall = recall_metric(y_test,predicted)\nprint(f'Recall = {recall}')","92b2b3b6":"from random import randrange, seed\n\n# Create a random subsample from the dataset with replacement\ndef subsample(dataset, ratio=1.0):\n    sample = list()\n    total_sample = round(len(dataset) * ratio)\n    while len(sample) < total_sample:\n        index = randrange(len(dataset))\n        sample.append(dataset[index])\n    return sample\n\n# Make a prediction with a list of bagged data\ndef bagging_predict(samples, test, l_rate, n_epoch):\n    predictions = [perceptron(sample, test, l_rate, n_epoch) for sample in samples]\n    \n    voted_predictions = list()\n    \n    for i in range(len(predictions[0])):\n        count_0 = 0\n        count_1 = 0\n        \n        for prediction in predictions:\n            if prediction[i] == 0.0:\n                count_0 += 1\n            else:\n                count_1 += 1\n        \n        if count_0 <= count_1:\n            voted_predictions.append(1.0) \n        else:\n            voted_predictions.append(0.0) \n    \n    return voted_predictions\n\n# Bootstrap Aggregation Algorithm\ndef bagging(train, test, l_rate, n_epoch, sample_size, total_samples):\n    samples = list()\n    \n    for i in range(total_samples):\n        sample = subsample(train, sample_size)\n        samples.append(sample)\n        \n    prediction = bagging_predict(samples, test, l_rate, n_epoch)\n    return prediction\n\ndef evaluate_bagging(train, test, l_rate, n_epoch, sample_size, total_samples):\n    predicted = bagging(train, test, l_rate, n_epoch, sample_size, total_samples)\n    \n    accuracy = accuracy_metric(y_test, predicted)\n    precision = precision_metric(y_test, predicted)\n    recall = recall_metric(y_test, predicted)\n    \n    return accuracy, precision, recall\n\nseed(1)\nsample_size = 0.50\nl_rate = 0.1\nn_epoch = 20\n\nfor n_sample in [1, 5, 10, 50]:\n    accuracy, precision, recall = evaluate_bagging(training.values, test.values, l_rate, n_epoch, sample_size, n_sample)\n    \n    print(f'Number of Samples: {n_sample}')\n    print(f'Accuracy: {accuracy}')\n    print(f'Precision = {precision}')\n    print(f'Recall = {recall}')\n    print(\"=\"*50)","f4e22944":"from sklearn.neural_network import MLPClassifier\n\n# Make a prediction with a list of bagged data\ndef bagging_predict(samples, test):\n    predictions = list()\n    \n    for sample in samples:\n        x_training = [arr[:8] for arr in sample]\n        y_training = [arr[-1] for arr in sample]\n        \n        clf.fit(x_training, y_training)\n        predictions.append(clf.predict(x_test))\n    \n    voted_predictions = list()\n    \n    for i in range(len(predictions[0])):\n        count_0 = 0\n        count_1 = 0\n        \n        for prediction in predictions:\n            if prediction[i] == 0.0:\n                count_0 += 1\n            else:\n                count_1 += 1\n        \n        if count_0 <= count_1:\n            voted_predictions.append(1.0) \n        else:\n            voted_predictions.append(0.0) \n    \n    return voted_predictions\n\n# Bootstrap Aggregation Algorithm\ndef bagging(train, test, sample_size, total_samples):\n    samples = list()\n    \n    for i in range(total_samples):\n        sample = subsample(train, sample_size)\n        samples.append(sample)\n        \n    prediction = bagging_predict(samples, test)\n    return prediction\n\ndef evaluate_bagging(train, test, sample_size, total_samples):\n    predicted = bagging(train, test, sample_size, total_samples)\n    \n    accuracy = accuracy_metric(y_test, predicted)\n    precision = precision_metric(y_test, predicted)\n    recall = recall_metric(y_test, predicted)\n    \n    return accuracy, precision, recall\n\nseed(2)\nsample_size = 0.50\n\nclf = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(8,2), random_state=2)\n\n\nfor n_sample in [1, 5, 10, 50, 70]:\n    accuracy, precision, recall = evaluate_bagging(training.values, test.values, sample_size, n_sample)\n    \n    print(f'Number of Samples: {n_sample}')\n    print(f'Accuracy: {accuracy}')\n    print(f'Precision = {precision}')\n    print(f'Recall = {recall}')\n    print(\"=\"*50)","83ffa803":"3.With the same proportion of training and testing as question number 1, predict the diabetes dataset with the following conditions.\n\nMake a bagging classifier using the Perceptron you created as a base learner. Make variations on the number of bootstrap samples. From the bagging perceptron prediction, display accuracy, precision and recall per class for each variation of the number of bootstrap samples (can be in the form of tables or graphs).","9198bec9":"#### Neural Network","bc45b0d7":"Reference for Bagging Classifier: \n- https:\/\/machinelearningmastery.com\/implement-bagging-scratch-python\/","d89c1638":"4.With the same proportion of training and testing as question number 1, predict the diabetes dataset with the following conditions.\n\nImplement the bagging classifier that you created with changes to the base learner. In this experiment, use a multilayer perceptron (MLP) as the base learner. You can use a library to implement MLP (Sklearn or Keras). You can vary the number of bootstrap samples, the number of hidden layers, the number of output units, the number of hidden units, or variations on the activation function. Show accuracy, precision and recall per class for each variation you do (can be a table or a graphic).","eb30fd61":"1.Divide the diabetes dataset into training and testing by the proportion of 80:20. ","de4f7b1e":"#### Activation Function\nThe activation function is the sum of the multiplication of the ith data attribute with the attribute weight and then added with bias. If the sum is more than 0 then the prediction yields 1, whereas if it is less than or equal to 0, the prediction will produce 0.\n\n#### Optimal Parameters\nParameters taken such as the learning rate are taken based on the example on the slide in the scele, while the epoch value is obtained after the author has experimented many times to find the maximum value obtained.","bd0b2733":"2.From the prediction results above, calculate the accuracy, precision and recall in each class. If you perform parameter variations, display the results of the evaluation metrics for each parameter variation that you use (can be a table or graph).","c4f8f895":"#### Ensemble Learning","bba2ccc7":"#### Analysis\nThe three algorithm above have `similar values` in terms of accuracy in classifying the diabetes data set (hovering around `70-80%` accuracy), and the precision and recall values of each have varying values depending on the condition the algorithm was run on.\n\n#### Perceptron\nThe perceptron algorithm gave us an accuracy score of 77.8%, while the precision and recall value both gave us a 68% score. This result will vary depending on the starting `weights` and `bias` we set on perceptron and number of `epoch` we allow the algorithm to train the `weights` and the `bias`.\n\nI found out that if we set the `learning rate` of the percepton to be a big number (>= 1) it would create a big gap on each iteration where we train the `weights` and `bias` of our algorithm. If we use a bigger `epoch` number it will allow our algorithm to reach a more optimum `weights` for the perceptron, but if we use a number that is more than necessary, then our algorithm will have a bloated weights that could prove to decrease the accuracy of the prediction.\n\n#### Bagging Perceptron\nThis algorithm uses the same perceptron as the previous algorithm and the things that differentiate this algorithm is the number of training samples and the size of each samples. With enough experiment, I found that if we use more samples to predict the result, it will give us a better accuracy, but if we set the size of each sample to be very small or very big, it will not really help increasing the accuracy of our algorithm because of the bias and variance contained in our sample \n\n#### Bagging MLP\nIn this algorithm, the parameter that could determines the accuracy of our prediction is the `alpha` and `hidden layer` in our `multi layer perceptron`. After experimenting with changin the values of the `hidden layer` and `alpha` of the `multi layer perceptron`, I found some combination gives better accuracy on smaller number of samples while other combination give a irregular result depending on the sample sizes. In the end, I choose a combination that has a consistency in which the larger the number of samples we used, the better accuracy we will get.","defd179b":"General Instruction:\n\n1. Dataset used in this assignment is a diabetes dataset which can be downloaded at  https:\/\/bit.ly\/2M5vioB\n\n\n2. Perform data processing and calculations using the Python programming language. Use the Jupyter notebook template provided to answer questions.\n\n\n3. Libary usage will be restricted to using numpy array and pandas to process the data. Especially for the Multilayer Perceptron implementation, you can use the library (Sklearn, Keras or any other library).\n","cca7620c":"<h2><center>Individual Assignment 4 Data Mining 2019\/2020<\/center><\/h2>","e7f8f197":"Build a Perceptron model to classify the dataset. Use online learning to update weight and bias. Describe the activation function you use and how you chose the optimal parameters (eg learning rate).","605c1049":"*Note: Recall and Precision values are the same because the number of data with outcome = 1 in the prediction and test data is the same*","48a73cf9":"Reference for Multi Layer Perceptron: \n- https:\/\/scikit-learn.org\/stable\/modules\/neural_networks_supervised.html","3a71b340":"#### Diabetes Dataset","7cb01e98":"#### Range Normalization","0c5d6098":"5.Perform comparisons and analyzes of all the results of the experiments you do (Perceptron, BaggingPperceptron, Bagging MLP) to solve the problem of diabetes classification. ","ac59f869":"#### Import Library","b8cf4c1d":"Reference for Neural Network: \n- https:\/\/machinelearningmastery.com\/implement-perceptron-algorithm-scratch-python\/"}}