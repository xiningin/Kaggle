{"cell_type":{"8f324b5e":"code","6b6da373":"code","549da144":"code","458618ff":"code","c6a85376":"code","308cce40":"code","805620c2":"code","e00a2543":"code","dff37a1e":"code","ff1fe221":"code","bb1f5195":"code","a5f15162":"code","62ea4101":"code","dc90d5fe":"code","97b4dab0":"code","0eab16db":"code","eefed3e9":"code","be8ae057":"code","de724aeb":"code","d32196e6":"code","2cba18c1":"code","8d19afa3":"code","f4e97c64":"code","3ee55c70":"code","312b5c06":"code","15edca66":"code","2a10ade3":"code","ce5c8758":"code","4a53c8eb":"code","6143aa4b":"code","85e6a086":"markdown","ad54075c":"markdown","10a42d69":"markdown","ead59d56":"markdown","1b05e394":"markdown","7164db9c":"markdown","68417213":"markdown","18b7e578":"markdown","9ea55a68":"markdown","842f83ba":"markdown","3c4ce571":"markdown","415c9d07":"markdown","9ca2eb53":"markdown","ce4af5f8":"markdown","08ece700":"markdown","492ce104":"markdown","d196271e":"markdown","bb7f3dfc":"markdown","66d2022d":"markdown","8a9ba6c4":"markdown","85a65d56":"markdown","87a33319":"markdown","79cb6eca":"markdown","8d32bf5a":"markdown","e418adba":"markdown","a2d69bd5":"markdown","9e63b3fd":"markdown","a557f452":"markdown","1003f29c":"markdown","a36d34e0":"markdown","728d4916":"markdown","3d1d0bb2":"markdown","41e8c009":"markdown","77fc03f1":"markdown","aa7ad1f0":"markdown","5605bb83":"markdown","6282b539":"markdown"},"source":{"8f324b5e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","6b6da373":"# import os\nimport json\nimport string\nimport numpy as np\nimport pandas as pd\nfrom pandas.io.json import json_normalize\nimport matplotlib.pyplot as plt\nimport seaborn as sns\ncolor = sns.color_palette()\n\n%matplotlib inline\n\nfrom plotly import tools\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\n\nfrom sklearn import model_selection, preprocessing, metrics, ensemble, naive_bayes, linear_model\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.decomposition import TruncatedSVD\nimport lightgbm as lgb\n\npd.options.mode.chained_assignment = None\npd.options.display.max_columns = 999","549da144":"#\u0110\u1ecdc file train v\u00e0 file test\ntrain_df = pd.read_csv(\"..\/input\/quora-insincere-questions-classification\/train.csv\")\ntest_df = pd.read_csv(\"..\/input\/quora-insincere-questions-classification\/test.csv\")\nprint(\"Train shape : \", train_df.shape)\nprint(\"Test shape : \", test_df.shape)","458618ff":"train_df.info()","c6a85376":"test_df.info()","308cce40":"cnt_srs = train_df['target'].value_counts()\ntrace = go.Bar(\n    x=cnt_srs.index,\n    y=cnt_srs.values,\n    marker=dict(\n        color=cnt_srs.values,\n        colorscale = 'Picnic',\n        reversescale = True\n    ),\n)\n\nlayout = go.Layout(\n    title='Target Count',\n    font=dict(size=18)\n)\n\ndata = [trace]\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig, filename=\"TargetCount\")\n","805620c2":"print(\"T\u1ec9 l\u1ec7 ph\u1ea7n tr\u0103m s\u1ed1 c\u00e2u h\u1ecfi Insincere l\u00e0:\", (len(train_df.loc[train_df.target==1])) \/ (len(train_df.loc[train_df.target == 0])) * 100)","e00a2543":"words = train_df['question_text'].apply(lambda x: len(x) - len(''.join(x.split())) + 1)\ntrain_df['words'] = words\nwords = train_df.loc[train_df['words']<200]['words']\nsns.distplot(words, color='g')\nplt.show()","dff37a1e":"print('S\u1ed1 t\u1eeb trung b\u00ecnh c\u1ee7a c\u00e1c c\u00e2u h\u1ecfi trong d\u1eef li\u1ec7u file train l\u00e0 {0:.0f}.'.format(np.mean(train_df['question_text'].apply(lambda x: len(x.split())))))\nprint('S\u1ed1 t\u1eeb trung b\u00ecnh c\u1ee7a c\u00e1c c\u00e2u h\u1ecfi trong d\u1eef li\u1ec7u file test l\u00e0 {0:.0f}.'.format(np.mean(test_df['question_text'].apply(lambda x: len(x.split())))))","ff1fe221":"print('S\u1ed1 t\u1eeb l\u1edbn nh\u1ea5t c\u1ee7a c\u00e1c c\u00e2u h\u1ecfi trong d\u1eef li\u1ec7u file train l\u00e0 {0:.0f}.'.format(np.max(train_df['question_text'].apply(lambda x: len(x.split())))))\nprint('S\u1ed1 t\u1eeb l\u1edbn nh\u1ea5t c\u1ee7a c\u00e1c c\u00e2u h\u1ecfi trong d\u1eef li\u1ec7u file test l\u00e0 {0:.0f}.'.format(np.max(test_df['question_text'].apply(lambda x: len(x.split())))))","bb1f5195":"print('S\u1ed1 k\u00fd t\u1ef1 trung b\u00ecnh c\u1ee7a c\u00e1c c\u00e2u h\u1ecfi trong d\u1eef li\u1ec7u file train l\u00e0 {0:.0f}.'.format(np.mean(train_df['question_text'].apply(lambda x: len(x)))))\nprint('S\u1ed1 k\u00fd t\u1ef1 trung b\u00ecnh c\u1ee7a c\u00e1c c\u00e2u h\u1ecfi trong d\u1eef li\u1ec7u file test l\u00e0 {0:.0f}.'.format(np.mean(test_df['question_text'].apply(lambda x: len(x)))))","a5f15162":"from wordcloud import WordCloud, STOPWORDS\n\n# Thanks : https:\/\/www.kaggle.com\/aashita\/word-clouds-of-various-shapes ##\ndef plot_wordcloud(text, mask=None, max_words=200, max_font_size=100, figure_size=(24.0,16.0), \n                   title = None, title_size=40, image_color=False):\n    stopwords = set(STOPWORDS)\n    more_stopwords = {'one', 'br', 'Po', 'th', 'sayi', 'fo', 'Unknown'}\n    stopwords = stopwords.union(more_stopwords)\n\n    wordcloud = WordCloud(background_color='black',\n                    stopwords = stopwords,\n                    max_words = max_words,\n                    max_font_size = max_font_size, \n                    random_state = 42,\n                    width=800, \n                    height=400,\n                    mask = mask)\n    wordcloud.generate(str(text))\n    \n    plt.figure(figsize=figure_size)\n    if image_color:\n        image_colors = ImageColorGenerator(mask);\n        plt.imshow(wordcloud.recolor(color_func=image_colors), interpolation=\"bilinear\");\n        plt.title(title, fontdict={'size': title_size,  \n                                  'verticalalignment': 'bottom'})\n    else:\n        plt.imshow(wordcloud);\n        plt.title(title, fontdict={'size': title_size, 'color': 'black', \n                                  'verticalalignment': 'bottom'})\n    plt.axis('off');\n    plt.tight_layout()  \n    \nplot_wordcloud(train_df[\"question_text\"], title=\"Word Cloud of Questions\")","62ea4101":"from collections import defaultdict\ntrain1_df = train_df[train_df[\"target\"]==1]\ntrain0_df = train_df[train_df[\"target\"]==0]\n\n## custom function for ngram generation ##\ndef generate_ngrams(text, n_gram=1):\n    token = [token for token in text.lower().split(\" \") if token != \"\" if token not in STOPWORDS]\n    ngrams = zip(*[token[i:] for i in range(n_gram)])\n    return [\" \".join(ngram) for ngram in ngrams]\n\n## custom function for horizontal bar chart ##\ndef horizontal_bar_chart(df, color):\n    trace = go.Bar(\n        y=df[\"word\"].values[::-1],\n        x=df[\"wordcount\"].values[::-1],\n        showlegend=False,\n        orientation = 'h',\n        marker=dict(\n            color=color,\n        ),\n    )\n    return trace\n\n## Get the bar chart from sincere questions ##\nfreq_dict = defaultdict(int)\nfor sent in train0_df[\"question_text\"]:\n    for word in generate_ngrams(sent):\n        freq_dict[word] += 1\nfd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfd_sorted.columns = [\"word\", \"wordcount\"]\ntrace0 = horizontal_bar_chart(fd_sorted.head(50), 'blue')\n\n## Get the bar chart from insincere questions ##\nfreq_dict = defaultdict(int)\nfor sent in train1_df[\"question_text\"]:\n    for word in generate_ngrams(sent):\n        freq_dict[word] += 1\nfd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfd_sorted.columns = [\"word\", \"wordcount\"]\ntrace1 = horizontal_bar_chart(fd_sorted.head(50), 'blue')\n\n# Creating two subplots\nfig = tools.make_subplots(rows=1, cols=2, vertical_spacing=0.04,\n                          subplot_titles=[\"top c\u00e1c t\u1eeb c\u00f3 nhi\u1ec1u trong c\u00e2u h\u1ecfi ch\u00e2n th\u00e0nh\", \n                                          \"top c\u00e1c t\u1eeb c\u00f3 nhi\u1ec1u trong c\u00e2u h\u1ecfi ko ch\u00e2n th\u00e0nh\"])\nfig.append_trace(trace0, 1, 1)\nfig.append_trace(trace1, 1, 2)\nfig['layout'].update(height=1200, width=900, paper_bgcolor='rgb(233,233,233)', title=\"Word Count Plots\")\npy.iplot(fig, filename='word-plots')","dc90d5fe":"from gensim.utils import simple_preprocess \nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\n\nstop_words = set(stopwords.words('english')) \nwordnet_lemmatizer = WordNetLemmatizer()\ndef preprocessing(corpus):\n    res = []\n    for doc in corpus:\n        words = []\n        for word in simple_preprocess(doc):\n            if word not in stop_words:\n                word1 = wordnet_lemmatizer.lemmatize(word, pos = \"n\")\n                word2 = wordnet_lemmatizer.lemmatize(word1,pos = \"v\")\n                word3 = wordnet_lemmatizer.lemmatize(word2, pos = (\"a\"))\n                words.append(word3)\n                pass\n            pass\n        res.append(' '.join(words))        \n        pass\n    return res","97b4dab0":"from gensim.utils import simple_preprocess \nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\n\nstop_words = set(stopwords.words('english')) \nwordnet_lemmatizer = WordNetLemmatizer()\ndef preprocessing(corpus):\n    res = []\n    for doc in corpus:\n        words = []\n        for word in simple_preprocess(doc):\n            if word not in stop_words:\n                word1 = wordnet_lemmatizer.lemmatize(word, pos = \"n\")\n                word2 = wordnet_lemmatizer.lemmatize(word1,pos = \"v\")\n                word3 = wordnet_lemmatizer.lemmatize(word2, pos = (\"a\"))\n                words.append(word3)\n                pass\n            pass\n        res.append(' '.join(words))        \n        pass\n    return res","0eab16db":"train = pd.read_csv(\"..\/input\/quora-insincere-questions-classification\/train.csv\")\ntest = pd.read_csv(\"..\/input\/quora-insincere-questions-classification\/test.csv\")","eefed3e9":"# \u00e1p d\u1ee5ng cho t\u1eadp train v\u00e0 test \ntrain['question_text'] = preprocessing(train['question_text'])\ntest['question_text'] = preprocessing(test['question_text'])","be8ae057":"vector = TfidfVectorizer( ngram_range = (1,2))\ntrain_feature_matrics = vector.fit_transform(train['question_text'].values.astype('U'))\ntest_feature_matrics = vector.transform(test['question_text'].values.astype('U'))","de724aeb":"train_feature_matrics","d32196e6":"# Khai b\u00e1o th\u00eam c\u00e1c th\u01b0 vi\u1ec7n \u0111\u01b0\u1ee3c s\u1eed d\u1ee5ng\nfrom sklearn.metrics import accuracy_score, confusion_matrix, f1_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\ntrain_x, valid_x, train_y, valid_y = train_test_split(train_feature_matrics, train['target'], test_size=0.25, shuffle=False)\nC = [0.01, 0.03, 0.1, 0.3, 1, 3, 10, 30, 100, 300, 1000]","2cba18c1":"for c in C:\n    model = LogisticRegression(solver='liblinear', penalty='l2', C=c)\n    model.fit(train_x, train_y)\n    prediction = model.predict(valid_x)\n    f1 = f1_score(valid_y, prediction)\n    acc = accuracy_score(valid_y, prediction)\n    print(\"Regularization: \", c)\n    print(\"F1 score: \",f1)\n    print(\"Acc score: \",acc)","8d19afa3":"thresholds = [0.5, 0.1, 0.14, 0.15, 0.16, 0.17, 0.25, 0.3, 0.4, 0.55, 0.7]\nc = 30\nmodel = LogisticRegression(solver='liblinear', penalty='l2', C=c)\nmodel.fit(train_x, train_y)\npredict = model.predict_proba(valid_x)[:,1]\nfor t in thresholds:\n    predict_t = np.where(predict > t, 1, 0)\n    f1 = f1_score(valid_y, predict_t)\n    print(\"Threshold: \", t)\n    print(\"F1 score: \", f1)","f4e97c64":"t= 0.15\npredict = model.predict_proba(valid_x)[:,1]\npredict = np.where(predict > t, 1, 0)","3ee55c70":"confusion_matrix(valid_y, predict)","312b5c06":"class_weight = {0: 1., 1: 14.}\nthresholds = [0.1, 0.25, 0.3, 0.35, 0.4, 0.55, 0.6, 0.7, 0.8, 0.85, 0.9]\nc = 30\nmodel = LogisticRegression(solver='liblinear', penalty='l2', C=c, class_weight = class_weight)\nmodel.fit(train_x, train_y)\npredict = model.predict_proba(valid_x)[:,1]\nfor t in thresholds:\n    predict_t = np.where(predict > t, 1, 0)\n    f1 = f1_score(valid_y, predict_t)\n    print(\"Threshold: \", t)\n    print(\"F1 score: \", f1)","15edca66":"# Ki\u1ec3m tra d\u1eef li\u1ec7u c\u00f3 trong t\u1eadp test\ntest.head(20)","2a10ade3":"model = LogisticRegression(solver='liblinear', penalty='l2', C=30)\nmodel.fit(train_feature_matrics, train['target'])\npredict = model.predict_proba(test_feature_matrics)[:,1]\npredict = np.where(predict > 0.17, 1, 0)","ce5c8758":"test['prediction'] = predict","4a53c8eb":"test.head(20)","6143aa4b":"results = test[['qid', 'prediction']]\nresults.to_csv('submission.csv', index=False)","85e6a086":"# Ti\u1ec1n x\u1eed l\u00fd data","ad54075c":"# K\u1ebft qu\u1ea3 d\u1ef1 \u0111o\u00e1n trong t\u1eadp test","10a42d69":"# B\u00e1o c\u00e1o b\u00e0i t\u1eadp l\u1edbn m\u00f4n H\u1ecdc m\u00e1y \n**Gi\u1ea3ng vi\u00ean**: Tr\u1ea7n Qu\u1ed1c Long\n\n**Sinh vi\u00ean:** L\u01b0u V\u0103n V\u01b0\u01a1ng\n\n**MSSV**: 18021446\n\n**Topic**: Quora Insincere Question Classification\n\n\n","ead59d56":"# Ph\u00e2n t\u00edch data\n","1b05e394":"# L\u1ef1a ch\u1ecdn model","7164db9c":"**S\u1ed1 t\u1eeb xu\u1ea5t hi\u1ec7n nhi\u1ec1u nh\u1ea5t**","68417213":"**Nh\u1eadn x\u00e9t** : D\u1eef li\u1ec7u file train kh\u00f4ng c\u00f3 gi\u00e1 tr\u1ecb null.","18b7e578":"**\u0110\u1ecdc c\u00e1c file**","9ea55a68":"**\u0110\u1ecdc c\u00e1c file**","842f83ba":"# K\u1ebft lu\u1eadn ","3c4ce571":"**Nh\u1eadn x\u00e9t**: C\u00f3 th\u1ec3 th\u1ea5y \u0111\u1ed9 d\u00e0i trung b\u00ecnh c\u1ee7a c\u00e1c c\u00e2u h\u1ecfi trong t\u1eadp d\u1eef li\u1ec7u l\u00e0 gi\u1ed1ng nhau, tuy nhi\u00ean c\u00f3 nh\u1eefng c\u00e2u h\u1ecfi kh\u00e1 d\u00e0i trong t\u1eadp d\u1eef li\u1ec7u hu\u1ea5n luy\u1ec7n.","415c9d07":"**T\u1ea1i Threshold = 0.15 , gi\u00e1 tr\u1ecb F1 score l\u00e0 t\u1ed1t nh\u1ea5t cho t\u1eadp val**","9ca2eb53":"**+ T\u00ecm hi\u1ec3u v\u1ec1 F1-score**\n\u0110\u1ea7u ti\u00ean, Precision \u0111\u01b0\u1ee3c \u0111\u1ecbnh ngh\u0129a l\u00e0 t\u1ec9 l\u1ec7 s\u1ed1 \u0111i\u1ec3m positive m\u00f4 h\u00ecnh d\u1ef1 \u0111o\u00e1n \u0111\u00fang tr\u00ean t\u1ed5ng s\u1ed1 \u0111i\u1ec3m m\u00f4 h\u00ecnh d\u1ef1 \u0111o\u00e1n l\u00e0 Positive. Recall \u0111\u01b0\u1ee3c \u0111\u1ecbnh ngh\u0129a l\u00e0 t\u1ec9 l\u1ec7 s\u1ed1 \u0111i\u1ec3m positive m\u00f4 h\u00ecnh d\u1ef1 \u0111o\u00e1n \u0111\u00fang tr\u00ean t\u1ed5ng s\u1ed1 \u0111i\u1ec3m th\u1eadt s\u1ef1 l\u00e0 Positive (hay t\u1ed5ng s\u1ed1 \u0111i\u1ec3m \u0111\u01b0\u1ee3c g\u00e1n nh\u00e3n l\u00e0 positive ban \u0111\u1ea7u).\n\nPrecision c\u00e0ng cao, t\u1ee9c l\u00e0 s\u1ed1 \u0111i\u1ec3m m\u00f4 h\u00ecnh d\u1ef1 \u0111o\u00e1n l\u00e0 positive \u0111\u1ec1u l\u00e0 positive c\u00e0ng nhi\u1ec1u. Precision = 1, t\u1ee9c l\u00e0 t\u1ea5t c\u1ea3 s\u1ed1 \u0111i\u1ec3m m\u00f4 h\u00ecnh d\u1ef1 do\u00e1n l\u00e0 Positive \u0111\u1ec1u \u0111\u00fang, hay kh\u00f4ng c\u00f3 \u0111i\u1ec3m n\u00e0o c\u00f3 nh\u00e3n l\u00e0 Negative m\u00e0 m\u00f4 h\u00ecnh d\u1ef1 \u0111o\u00e1n nh\u1ea7m l\u00e0 Positive.\n\nRecall c\u00e0ng cao, t\u1ee9c l\u00e0 s\u1ed1 \u0111i\u1ec3m l\u00e0 positive b\u1ecb b\u1ecf s\u00f3t c\u00e0ng \u00edt. Recall = 1, t\u1ee9c l\u00e0 t\u1ea5t c\u1ea3 s\u1ed1 \u0111i\u1ec3m c\u00f3 nh\u00e3n l\u00e0 Positive \u0111\u1ec1u \u0111\u01b0\u1ee3c m\u00f4 h\u00ecnh nh\u1eadn ra.\n\nCh\u1ec9 d\u00f9ng Precision, m\u00f4 h\u00ecnh ch\u1ec9 \u0111\u01b0a ra d\u1ef1 \u0111o\u00e1n cho m\u1ed9t \u0111i\u1ec3m m\u00e0 n\u00f3 ch\u1eafc ch\u1eafn nh\u1ea5t. Khi \u0111\u00f3 Precision = 1, tuy nhi\u00ean ta kh\u00f4ng th\u1ec3 n\u00f3i l\u00e0 m\u00f4 h\u00ecnh n\u00e0y t\u1ed1t.Ch\u1ec9 d\u00f9ng Recall, n\u1ebfu m\u00f4 h\u00ecnh d\u1ef1 \u0111o\u00e1n t\u1ea5t c\u1ea3 c\u00e1c \u0111i\u1ec3m \u0111\u1ec1u l\u00e0 Positive. Khi \u0111\u00f3 Recall = 1, tuy nhi\u00ean ta c\u0169ng kh\u00f4ng th\u1ec3 n\u00f3i \u0111\u00e2y l\u00e0 m\u00f4 h\u00ecnh t\u1ed1t. Khi \u0111\u00f3 F1-score \u0111\u01b0\u1ee3c s\u1eed d\u1ee5ng. F1-score l\u00e0 trung b\u00ecnh \u0111i\u1ec1u h\u00f2a (harmonic mean) c\u1ee7a Precision v\u00e0 Recall (gi\u1ea3 s\u1eed hai \u0111\u1ea1i l\u01b0\u1ee3ng n\u00e0y kh\u00e1c 0). F1-score \u0111\u01b0\u1ee3c tinh theo c\u00f4ng th\u1ee9c:\n\n\n![image.png](attachment:910dc982-3c99-4c56-9ac0-90cd198a62eb.png)\n\n\n**F1-score** c\u00f3 gi\u00e1 tr\u1ecb n\u1eb1m trong n\u1eeda kho\u1ea3ng (0,1].F1 c\u00e0ng cao, b\u1ed9 ph\u00e2n l\u1edbp c\u00e0ng t\u1ed1t. Khi c\u1ea3 Recall v\u00e0 Precision \u0111\u1ec1u b\u1eb1ng 1 (t\u1ed1t nh\u1ea5t c\u00f3 th\u1ec3), F1 = 1. Khi c\u1ea3 Recall v\u00e0 Precision \u0111\u1ec1u th\u1ea5p, v\u00ed d\u1ee5 b\u1eb1ng 0.1, F1 = 0.1.\n\n**+  L\u1ef1a ch\u1ecdn m\u00f4 h\u00ecnh hu\u1ea5n luy\u1ec7n: m\u00f4 h\u00ecnh Logistic Regression**\n\n   -  **Gi\u1edbi thi\u1ec7u qua v\u1ec1 Logistic Regression**  \n         Logistic Regression l\u00e0 1 thu\u1eadt to\u00e1n ph\u00e2n lo\u1ea1i \u0111\u01b0\u1ee3c d\u00f9ng \u0111\u1ec3 g\u00e1n c\u00e1c \u0111\u1ed1i t\u01b0\u1ee3ng cho 1 t\u1eadp h\u1ee3p gi\u00e1 tr\u1ecb r\u1eddi r\u1ea1c (nh\u01b0 0, 1, 2, ...). M\u1ed9t v\u00ed d\u1ee5 \u0111i\u1ec3n h\u00ecnh l\u00e0 ph\u00e2n lo\u1ea1i Email, g\u1ed3m c\u00f3 email c\u00f4ng vi\u1ec7c, email gia \u0111\u00ecnh, email spam, ... Giao d\u1ecbch tr\u1ef1c tuy\u1ebfn c\u00f3 l\u00e0 an to\u00e0n hay kh\u00f4ng an to\u00e0n, kh\u1ed1i u l\u00e0nh t\u00ednh hay \u00e1c t\u00ecnh. Thu\u1eadt to\u00e1n tr\u00ean d\u00f9ng h\u00e0m sigmoid logistic \u0111\u1ec3 \u0111\u01b0a ra \u0111\u00e1nh gi\u00e1 theo x\u00e1c su\u1ea5t.\n   -  **H\u1ecdc tf-idf \u0111\u00e3 extract \u1edf tr\u00ean b\u1eb1ng m\u00f4 h\u00ecnh Logistic regression. Th\u1eed tunning tham s\u1ed1 regularize C**\n   ","ce4af5f8":"**K\u1ebft lu\u1eadn: M\u00f4 h\u00ecnh th\u1eed nghi\u1ec7m tr\u00ean t\u1eadp test \u0111\u00e3 cho ra k\u1ebft qu\u1ea3 ph\u00e2n bi\u1ec7t \u0111\u01b0\u1ee3c \u0111\u00e2u l\u00e0 c\u00e2u h\u1ecfi ch\u00e2n th\u00e0nh (prediction = 0) v\u00e0 \u0111\u00e2u l\u00e0 c\u00e2u h\u1ecfi kh\u00f4ng tr\u00e2n th\u00e0nh (prediction = 1).**","08ece700":"**Nh\u1eadn x\u00e9t:**\n\n**File train.csv:**\n* S\u1ed1 d\u00f2ng: 1306122\n* S\u1ed1 c\u1ed9t: 3 (c\u1ed9t qid, question_text, target)\n\n**File test.csv:**\n* S\u1ed1 d\u00f2ng: 375806\n* S\u1ed1 c\u1ed9t: 2 (c\u1ed9t qid, question_text)\n\n**Th\u00f4ng tin v\u1ec1 c\u00e1c tr\u01b0\u1eddng:**\n* C\u1ed9t qid: \u0110\u00e2y ch\u00ednh l\u00e0 id c\u1ee7a c\u00e2u h\u1ecfi, kh\u00f4ng c\u00f3 2 c\u00e2u h\u1ecfi n\u00e0o c\u00f3 id gi\u1ed1ng nhau.\n* C\u1ed9t question_text: \u0110\u00e2y ch\u00ednh l\u00e0 c\u1ed9t ch\u1ee9a c\u00e1c c\u00e2u h\u1ecfi. Ch\u00fang ta s\u1ebd ti\u1ebfn h\u00e0nh clean d\u1eef li\u1ec7u tr\u01b0\u1edbc khi cho v\u00e0o m\u00f4 h\u00ecnh \u0111\u1ec3 hu\u1ea5n luy\u1ec7n.\n* C\u1ed9t target: \u0110\u00e2y ch\u00ednh l\u00e0 c\u1ed9t ch\u1ee9a k\u1ebft qu\u1ea3 \u0111\u00e1nh gi\u00e1 c\u00e2u h\u1ecfi c\u00f3 ph\u1ea3i ch\u00e2n th\u00e0nh hay kh\u00f4ng ch\u00e2n th\u00e0nh, c\u00e2u h\u1ecfi question_text c\u00f3 target = 0 s\u1ebd \u0111\u01b0\u1ee3c \u0111\u00e1nh gi\u00e1 l\u00e0 c\u00e2u h\u1ecfi ch\u00e2n th\u00e0nh, v\u00e0 c\u00e2u h\u1ecfi c\u00f3 target = 1 s\u1ebd \u0111\u01b0\u1ee3c \u0111\u00e1nh gi\u00e1 l\u00e0 kh\u00f4ng ch\u00e2n th\u00e0nh.","492ce104":"# M\u00d4 T\u1ea2 B\u00c0I TO\u00c1N\n* Quora l\u00e0 n\u1ec1n t\u1ea3ng \u0111\u1ec3 m\u1ecdi ng\u01b0\u1eddi c\u00f3 th\u1ec3 h\u1ecdc h\u1ecfi l\u1eabn nhau b\u1eb1ng c\u00e1ch \u0111\u1eb7t c\u00e2u h\u1ecfi v\u00e0 tr\u1ea3 l\u1eddi \u0111\u1ec3 chia s\u1ebb ki\u1ebfn th\u1ee9c. M\u1ee5c \u0111\u00edch c\u1ee7a b\u00e0i to\u00e1n n\u00e0y l\u00e0 \u0111\u1ec3 ph\u00e2n lo\u1ea1i c\u00e1c c\u00e2u h\u1ecfi \u0111\u1eb7t ra l\u00e0 thu\u1ed9c lo\u1ea1i c\u00e2u h\u1ecfi ch\u00e2n th\u00e0nh hay kh\u00f4ng ch\u00e2n th\u00e0nh. \n* Nh\u1eefng c\u00e2u h\u1ecfi kh\u00f4ng ch\u00e2n th\u00e0nh th\u01b0\u1eddng l\u00e0 nh\u1eefng c\u00e2u \u0111\u01b0a ra tuy\u00ean b\u1ed1 quan \u0111i\u1ec3m c\u1ee7a m\u00ecnh h\u01a1n l\u00e0 \u0111\u1ec3 t\u00ecm nh\u1eefng c\u00e2u tr\u1ea3 l\u1eddi c\u00f3 \u00edch: kh\u00f4ng trung l\u1eadp, khi\u00eau kh\u00edch ho\u1eb7c ch\u00ea bai, kh\u00f4ng c\u00f3 c\u0103n c\u1ee9 th\u1ef1c t\u1ebf ho\u1eb7c ch\u1ee9a n\u1ed9i dung khi\u00eau d\u00e2m.\n* Input: C\u00e2u h\u1ecfi d\u1ea1ng text\n* Output: 0\/1 (Sincere\/ Insincere)","d196271e":"**Nh\u1eadn x\u00e9t:** C\u00f3 th\u1ec3 th\u1ea5y , gi\u00e1 tr\u1ecb Regularization t\u1ed1t nh\u1ea5t l\u00e0 30, V\u00ec h\u00e0m predict s\u1ebd tr\u1ea3 v\u1ec1 m\u1ed9t sample l\u00e0 sincere hay kh\u00f4ng b\u1eb1ng c\u00e1ch so s\u00e1nh x\u00e1c su\u1ea5t \u0111\u1ea7u ra v\u1edbi m\u1ed9t gi\u00e1 tr\u1ecb m\u1eb7c \u0111\u1ecbnh threshold = 0.5 (decision bound = 0), ta s\u1ebd th\u1eed thay \u0111\u1ed5i m\u1ee9c threshold n\u00e0y","bb7f3dfc":"**TF-IDF l\u00e0 g\u00ec?**\n\n**TF-IDF (Term Frequency \u2013 Inverse Document Frequency)** l\u00e0 1 k\u0129 thu\u1eadt s\u1eed d\u1ee5ng trong khai ph\u00e1 d\u1eef li\u1ec7u v\u0103n b\u1ea3n. Tr\u1ecdng s\u1ed1 n\u00e0y \u0111\u01b0\u1ee3c s\u1eed d\u1ee5ng \u0111\u1ec3 \u0111\u00e1nh gi\u00e1 t\u1ea7m quan tr\u1ecdng c\u1ee7a m\u1ed9t t\u1eeb trong m\u1ed9t v\u0103n b\u1ea3n. Gi\u00e1 tr\u1ecb cao th\u1ec3 hi\u1ec7n \u0111\u1ed9 quan tr\u1ecdng cao v\u00e0 n\u00f3 ph\u1ee5 thu\u1ed9c v\u00e0o s\u1ed1 l\u1ea7n t\u1eeb xu\u1ea5t hi\u1ec7n trong v\u0103n b\u1ea3n nh\u01b0ng b\u00f9 l\u1ea1i b\u1edfi t\u1ea7n su\u1ea5t c\u1ee7a t\u1eeb \u0111\u00f3 trong t\u1eadp d\u1eef li\u1ec7u. M\u1ed9t v\u00e0i bi\u1ebfn th\u1ec3 c\u1ee7a TF-IDF th\u01b0\u1eddng \u0111\u01b0\u1ee3c s\u1eed d\u1ee5ng trong c\u00e1c h\u1ec7 th\u1ed1ng t\u00ecm ki\u1ebfm nh\u01b0 m\u1ed9t c\u00f4ng c\u1ee5 ch\u00ednh \u0111\u1ec3 \u0111\u00e1nh gi\u00e1 v\u00e0 s\u1eafp x\u1ebfp v\u0103n b\u1ea3n d\u1ef1a v\u00e0o truy v\u1ea5n c\u1ee7a ng\u01b0\u1eddi d\u00f9ng. TF-IDF c\u0169ng \u0111\u01b0\u1ee3c s\u1eed d\u1ee5ng \u0111\u1ec3 l\u1ecdc nh\u1eefng t\u1eeb stopwords trong c\u00e1c b\u00e0i to\u00e1n nh\u01b0 t\u00f3m t\u1eaft v\u0103n b\u1ea3n v\u00e0 ph\u00e2n lo\u1ea1i v\u0103n b\u1ea3n.\n\n**TF** l\u00e0 g\u00ec? **TF:** Term Frequency(T\u1ea7n su\u1ea5t xu\u1ea5t hi\u1ec7n c\u1ee7a t\u1eeb) l\u00e0 s\u1ed1 l\u1ea7n t\u1eeb xu\u1ea5t hi\u1ec7n trong v\u0103n b\u1ea3n. V\u00ec c\u00e1c v\u0103n b\u1ea3n c\u00f3 th\u1ec3 c\u00f3 \u0111\u1ed9 d\u00e0i ng\u1eafn kh\u00e1c nhau n\u00ean m\u1ed9t s\u1ed1 t\u1eeb c\u00f3 th\u1ec3 xu\u1ea5t hi\u1ec7n nhi\u1ec1u l\u1ea7n trong m\u1ed9t v\u0103n b\u1ea3n d\u00e0i h\u01a1n l\u00e0 m\u1ed9t v\u0103n b\u1ea3n ng\u1eafn. Nh\u01b0 v\u1eady, term frequency th\u01b0\u1eddng \u0111\u01b0\u1ee3c chia cho \u0111\u1ed9 d\u00e0i v\u0103n b\u1ea3n( t\u1ed5ng s\u1ed1 t\u1eeb trong m\u1ed9t v\u0103n b\u1ea3n).\n\n![image.png](attachment:a995e734-7976-410d-93db-ddeb619c3547.png)\n\n**Trong \u0111\u00f3:**\n\ntf(t, d): t\u1ea7n su\u1ea5t xu\u1ea5t hi\u1ec7n c\u1ee7a t\u1eeb t trong v\u0103n b\u1ea3n d\n\nf(t, d): S\u1ed1 l\u1ea7n xu\u1ea5t hi\u1ec7n c\u1ee7a t\u1eeb t trong v\u0103n b\u1ea3n d\n\nmax({f(w, d) : w \u2208 d}): S\u1ed1 l\u1ea7n xu\u1ea5t hi\u1ec7n c\u1ee7a t\u1eeb c\u00f3 s\u1ed1 l\u1ea7n xu\u1ea5t hi\u1ec7n nhi\u1ec1u nh\u1ea5t trong v\u0103n b\u1ea3n d\n\n**IDF** l\u00e0 g\u00ec?\n\n**IDF:** Inverse Document Frequency(Ngh\u1ecbch \u0111\u1ea3o t\u1ea7n su\u1ea5t c\u1ee7a v\u0103n b\u1ea3n), gi\u00fap \u0111\u00e1nh gi\u00e1 t\u1ea7m quan tr\u1ecdng c\u1ee7a m\u1ed9t t\u1eeb . Khi t\u00ednh to\u00e1n TF , t\u1ea5t c\u1ea3 c\u00e1c t\u1eeb \u0111\u01b0\u1ee3c coi nh\u01b0 c\u00f3 \u0111\u1ed9 quan tr\u1ecdng b\u1eb1ng nhau. Nh\u01b0ng m\u1ed9t s\u1ed1 t\u1eeb nh\u01b0 \u201cis\u201d, \u201cof\u201d v\u00e0 \u201cthat\u201d th\u01b0\u1eddng xu\u1ea5t hi\u1ec7n r\u1ea5t nhi\u1ec1u l\u1ea7n nh\u01b0ng \u0111\u1ed9 quan tr\u1ecdng l\u00e0 kh\u00f4ng cao. Nh\u01b0 th\u1ebf ch\u00fang ta c\u1ea7n gi\u1ea3m \u0111\u1ed9 quan tr\u1ecdng c\u1ee7a nh\u1eefng t\u1eeb n\u00e0y xu\u1ed1ng.\n\n![image.png](attachment:26b1c17e-377c-435b-911c-46f9f5bdb3fd.png)\n\n\n**Trong \u0111\u00f3:**\n\nidf(t, D): gi\u00e1 tr\u1ecb idf c\u1ee7a t\u1eeb t trong t\u1eadp v\u0103n b\u1ea3n\n\n|D|: T\u1ed5ng s\u1ed1 v\u0103n b\u1ea3n trong t\u1eadp D\n\n|{d \u2208 D : t \u2208 d}|: th\u1ec3 hi\u1ec7n s\u1ed1 v\u0103n b\u1ea3n trong t\u1eadp D c\u00f3 ch\u1ee9a t\u1eeb t.\n\nC\u01a1 s\u1ed1 logarit trong c\u00f4ng th\u1ee9c n\u00e0y kh\u00f4ng thay \u0111\u1ed5i gi\u00e1 tr\u1ecb idf c\u1ee7a t\u1eeb m\u00e0 ch\u1ec9 thu h\u1eb9p kho\u1ea3ng gi\u00e1 tr\u1ecb c\u1ee7a t\u1eeb \u0111\u00f3. V\u00ec thay \u0111\u1ed5i c\u01a1 s\u1ed1 s\u1ebd d\u1eabn \u0111\u1ebfn vi\u1ec7c gi\u00e1 tr\u1ecb c\u1ee7a c\u00e1c t\u1eeb thay \u0111\u1ed5i b\u1edfi m\u1ed9t s\u1ed1 nh\u1ea5t \u0111\u1ecbnh v\u00e0 t\u1ef7 l\u1ec7 gi\u1eefa c\u00e1c tr\u1ecdng l\u01b0\u1ee3ng v\u1edbi nhau s\u1ebd kh\u00f4ng thay \u0111\u1ed5i. (n\u00f3i c\u00e1ch kh\u00e1c, thay \u0111\u1ed5i c\u01a1 s\u1ed1 s\u1ebd kh\u00f4ng \u1ea3nh h\u01b0\u1edfng \u0111\u1ebfn t\u1ef7 l\u1ec7 gi\u1eefa c\u00e1c gi\u00e1 tr\u1ecb IDF). Vi\u1ec7c s\u1eed d\u1ee5ng logarit nh\u1eb1m gi\u00fap gi\u00e1 tr\u1ecb tf-idf c\u1ee7a m\u1ed9t t\u1eeb nh\u1ecf h\u01a1n, do ch\u00fang ta c\u00f3 c\u00f4ng th\u1ee9c t\u00ednh tf-idf c\u1ee7a m\u1ed9t t\u1eeb trong 1 v\u0103n b\u1ea3n l\u00e0 t\u00edch c\u1ee7a tf v\u00e0 idf c\u1ee7a t\u1eeb \u0111\u00f3.\n\nC\u1ee5 th\u1ec3, ch\u00fang ta c\u00f3 c\u00f4ng th\u1ee9c t\u00ednh TF-IDF ho\u00e0n ch\u1ec9nh nh\u01b0 sau:\n\ntfidf(t, d, D) = tf(t, d) x idf(t, D)\n\n**Khi \u0111\u00f3:**\n\nNh\u1eefng t\u1eeb c\u00f3 gi\u00e1 tr\u1ecb TF-IDF cao l\u00e0 nh\u1eefng t\u1eeb xu\u1ea5t hi\u1ec7n nhi\u1ec1u trong v\u0103n b\u1ea3n n\u00e0y, v\u00e0 xu\u1ea5t hi\u1ec7n \u00edt trong c\u00e1c v\u0103n b\u1ea3n kh\u00e1c. Vi\u1ec7c n\u00e0y gi\u00fap l\u1ecdc ra nh\u1eefng t\u1eeb ph\u1ed5 bi\u1ebfn v\u00e0 gi\u1eef l\u1ea1i nh\u1eefng t\u1eeb c\u00f3 gi\u00e1 tr\u1ecb cao (t\u1eeb kho\u00e1 c\u1ee7a v\u0103n b\u1ea3n \u0111\u00f3).","66d2022d":"**T\u00ednh c\u00e1c t\u1eeb xu\u1ea5t hi\u1ec7n nhi\u1ec1u trong c\u00e1c c\u00e2u h\u1ecfi ch\u00e2n th\u00e0nh v\u00e0 kh\u00f4ng ch\u00e2n th\u00e0nh**","8a9ba6c4":"# T\u00edch xu\u1ea5t","85a65d56":"**V\u00ec s\u1ed1 sample c\u00f3 nh\u00e3n 1 ch\u1ec9 chi\u1ebfm 6% t\u1ed5ng s\u1ed1 sample, ta th\u1eed s\u1eed d\u1ee5ng class weight \u0111\u1ec3 c\u1ea3i ti\u1ebfn model. Coi 1 sample c\u00f3 label 1 nh\u01b0 14 samples c\u00f3 label 0.**","87a33319":"**S\u1ed1 t\u1eeb trung b\u00ecnh c\u00f3 trong c\u00e2u**","79cb6eca":"# PH\u00c2N T\u00cdCH D\u1eee LI\u1ec6U\n\n**Import c\u00e1c th\u01b0 vi\u1ec7n c\u1ea7n thi\u1ebft:**","8d32bf5a":"**S\u1ed1 k\u00ed t\u1ef1 trung b\u00ecnh**","e418adba":"**\u0110\u1ea7u ti\u00ean ta s\u1ebd x\u1eed l\u00fd data**","a2d69bd5":"**S\u1ed1 l\u01b0\u1ee3ng t\u1eeb c\u00f3 trong c\u00e2u**","9e63b3fd":"**T\u1ea1o file submission .csv ch\u1ee9a k\u1ebft qu\u1ea3 c\u1ee7a m\u00f4 h\u00ecnh.**","a557f452":"**Nh\u1eadn x\u00e9t:** D\u1eef li\u1ec7u file test kh\u00f4ng c\u00f3 gi\u00e1 tr\u1ecb null.","1003f29c":"**B\u00e2y gi\u1edd , b\u1eaft \u0111\u1ea7u \u0111i v\u00e0o qu\u00e1 tr\u00ecnh hu\u1ea5n luy\u1ec7n**","a36d34e0":"**Model n\u00e0y cho k\u1ebft qu\u1ea3 F1-score kh\u00f4ng t\u1ed1t b\u1eb1ng model tr\u01b0\u1edbc , v\u00ec v\u1eady ta s\u1ebd d\u00f9ng k\u1ebft qu\u1ea3 c\u1ee7a model tr\u01b0\u1edbc \u0111\u1ec3 d\u1ef1 to\u00e1n k\u1ebft qu\u1ea3 cho t\u1eadp test**","728d4916":"# Ph\u00e2n t\u00edch","3d1d0bb2":"**Ti\u1ebfp theo , v\u1ebd bi\u1ec1u \u0111\u1ed3 th\u1ec3 hi\u1ec7n s\u1ef1 ph\u00e2n b\u1ed1 d\u1eef li\u1ec7u trong t\u1eadp train**","41e8c009":"**\u0110\u1ecdc l\u1ea1i file train.csv v\u00e0 test.csv \u0111\u01b0a v\u1ec1 d\u1ea1ng t\u1ec7p 'train' v\u00e0 'test'**","77fc03f1":"**\u0110\u1ea7u ti\u00ean ta s\u1ebd x\u1eed l\u00fd data: B\u1ecf c\u00e1c t\u1eeb kh\u00f4ng mang ngh\u0129a v\u00e0 d\u1ea5u (stopword, punctual), \u0111\u1ed5i ch\u1eef hoa th\u00e0nh ch\u1eef th\u01b0\u1eddng, lemmatize (\u0111\u01b0a t\u1ea5t c\u1ea3 c\u00e1c ch\u1eef v\u1ec1 m\u1ed9t d\u1ea1ng th\u1ed1ng nh\u1ea5t)**","aa7ad1f0":"**NH\u1eacN X\u00c9T:**\n\nM\u1ed9t v\u00e0i t\u1eeb xu\u1ea5t hi\u1ec7n nhi\u1ec1u \u1edf c\u1ea3 hai l\u1edbp nh\u01b0 people, think, many...\nNh\u1eefng top words xu\u1ea5t hi\u1ec7n nhi\u1ec1u nh\u1ea5t \u1edf l\u1edbp c\u00e2u h\u1ecfi ch\u00e2n th\u00e0nh l\u00e0 : best, will, people...\nNh\u1eefng top words xu\u1ea5t hi\u1ec7n nhi\u1ec1u nh\u1ea5t \u1edf l\u1edbp c\u00e2u h\u1ecfi ko ch\u00e2n th\u00e0nh l\u00e0 : people, women, will...","5605bb83":"**\u0110\u00e3 xong qu\u00e1 tr\u00ecnh ti\u1ec1n x\u1eed l\u00fd , l\u00fac n\u00e0y d\u1eef li\u1ec7u c\u01a1 b\u1ea3n \u0111\u00e3 \u0111\u01b0\u1ee3c l\u00e0m s\u1ea1ch , b\u00e2y gi\u1edd ta s\u1ebd ch\u1ecdn l\u1ef1a model \u0111\u1ec3 th\u1ef1c hi\u1ec7n qu\u00e1 tr\u00ecnh train**","6282b539":"**Nh\u1eadn x\u00e9t v\u1ec1 ph\u00e2n l\u1edbp d\u1eef li\u1ec7u:**\n\nD\u1ef1a v\u00e0o bi\u1ec3u \u0111\u1ed3 tr\u00ean, ta c\u00f3 th\u1ec3 th\u1ea5y trong t\u1eadp train, s\u1ed1 l\u01b0\u1ee3ng c\u00e2u h\u1ecfi \u0111c \u0111\u00e1nh nh\u00e3n l\u00e0 thi\u1ebfu ch\u00e2n th\u00e0nh ch\u1ec9 chi\u1ebfm kho\u1ea3ng 6.19% (80810\/1306122 c\u00e2u) so v\u1edbi t\u1ec9 l\u1ec7 r\u1ea5t cao l\u00e0 93.81% l\u00e0 nh\u1eefng c\u00e2u h\u1ecfi ch\u00e2n th\u00e0nh.\n\nT\u1ec9 l\u1ec7 gi\u1eefa 2 nh\u00f3m c\u00e2u h\u1ecfi n\u00e0y r\u01a1i v\u00e0o kho\u1ea3ng 1:15 => T\u1eadp d\u1eef li\u1ec7u b\u1ecb m\u1ea5t c\u00e2n b\u1eb1ng. \u0110i\u1ec1u n\u00e0y s\u1ebd d\u1eabn \u0111\u1ebfn 1 s\u1ed1 v\u1ea5n \u0111\u1ec1 nh\u01b0 sau:\n* \u0110\u00e1nh gi\u00e1 sai ch\u1ea5t l\u01b0\u1ee3ng m\u00f4 h\u00ecnh: V\u1edbi t\u1ec9 l\u1ec7 nh\u01b0 tr\u00ean th\u00ec kh\u00f4ng c\u1ea7n quan t\u00e2m \u0111\u1ebfn m\u00f4 h\u00ecnh ta c\u0169ng c\u00f3 th\u1ec3 \u0111\u1ea1t \u0111\u01b0\u1ee3c \u0111\u1ed9 ch\u00ednh x\u00e1c cao c\u1ee7a metric accuracy, ch\u1ec9 c\u1ea7n t\u1ea5t c\u1ea3 d\u1ef1 \u0111o\u00e1n \u0111\u01b0a ra \u0111\u1ec1u c\u00f3 target = 0 th\u00ec \u0111\u1ed9 ch\u00ednh x\u00e1c \u0111\u00e3 \u0111\u1ea1t g\u1ea7n 94%.\n* M\u00f4 h\u00ecnh d\u1ef1 \u0111o\u00e1n k\u00e9m ch\u00ednh x\u00e1c: V\u00ec \u1edf \u0111\u00e2y, m\u1ee5c ti\u00eau c\u1ee7a b\u00e0i to\u00e1n l\u00e0 x\u00e1c \u0111\u1ecbnh c\u00e1c c\u00e2u h\u1ecfi thi\u1ebfu ch\u00e2n th\u00e0nh, trong khi s\u1ef1 m\u1ea5t c\u00e2n b\u1eb1ng tr\u00ean c\u00f3 th\u1ec3 khi\u1ebfn k\u1ebft qu\u1ea3 d\u1ef1 \u0111o\u00e1n th\u01b0\u1eddng nghi\u00eang v\u1ec1 nh\u00f3m \u0111a s\u1ed1(target = 0) v\u00e0 k\u00e9m hi\u1ec7u qu\u1ea3 tr\u00ean nh\u00f3m thi\u1ec3u s\u1ed1 (target = 1).\n\n Do \u0111\u00f3 kh\u00f4ng n\u00ean l\u1ef1a ch\u1ecdn \u0111\u1ed9 ch\u00ednh x\u00e1c(accuracy) l\u00e0m ch\u1ec9 s\u1ed1 \u0111\u00e1nh gi\u00e1 m\u00f4 h\u00ecnh. Thay v\u00e0o \u0111\u00f3 ta c\u00f3 th\u1ec3 s\u1eed d\u1ee5ng c\u00e1c metric thay th\u1ebf nh\u01b0: **F1_score, Recall,..**\n \n S\u1ed1 c\u00e2u h\u1ecfi \"insincere\" ch\u1ec9 chi\u1ebfm kho\u1ea3ng 6-7% trong t\u1ed5ng s\u1ed1 c\u00e2u h\u1ecfi. D\u1eef li\u1ec7u b\u1ecb m\u1ea5t c\u00e2n b\u1eb1ng kh\u00e1 l\u1edbn, do \u0111\u00f3 \u0111\u1ed9 \u0111o F1 s\u1ebd th\u00edch h\u1ee3p cho nh\u1eefng tr\u01b0\u1eddng h\u1ee3p nh\u01b0 n\u00e0y"}}