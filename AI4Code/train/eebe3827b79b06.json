{"cell_type":{"2eb077b1":"code","10a10871":"code","07c0f81c":"code","f1c9326a":"code","726c3d31":"code","8f7ac888":"code","93876742":"code","389c0601":"code","26cb8d4e":"code","b8aeff6d":"code","ae90db25":"code","8c8d1f0b":"code","7005ea1d":"code","5c0f7530":"code","b058909e":"code","79be6096":"code","748c636c":"code","937b720b":"code","e641c61f":"code","fb0ce9a5":"code","cbc82949":"code","564181c9":"code","cbbca29b":"markdown","23b5cca5":"markdown","e87096b3":"markdown","ca27ac58":"markdown","63a69e6a":"markdown","8d84ba02":"markdown","68774100":"markdown","c2375260":"markdown","0c574f4d":"markdown","fcbb2203":"markdown","03777684":"markdown","73217e34":"markdown","22705ac6":"markdown","48644d08":"markdown","79d36f3d":"markdown","1378f6ed":"markdown"},"source":{"2eb077b1":"import tensorflow as tf\nfrom tensorflow import keras\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\nimport pandas as pd","10a10871":"df = pd.read_csv('\/kaggle\/input\/digit-recognizer\/train.csv')","07c0f81c":"df.head()","f1c9326a":"from sklearn.model_selection import train_test_split","726c3d31":"x_train, x_test, y_train,  y_test = train_test_split(df.iloc[:, 1:], df['label'], test_size = 0.25)","8f7ac888":"batch_size = 32\n\n# # Reshape dataset to flat array with dim 1\n# x_train, y_train, x_test, y_test = mnist_train[0].reshape(60000,784), mnist_train[1], mnist_test[0].reshape(10000,784), mnist_test[1]\n\nx_train = tf.cast(x_train, dtype = 'float32')\nx_test = tf.cast(x_test, dtype = 'float32')\ny_train = tf.cast(y_train, dtype='int32')\ny_test = tf.cast(y_test, dtype='int32')","93876742":"# Let's have a look at out dataset","389c0601":"x_train, y_train","26cb8d4e":"plt.figure(figsize=(10,10))\nfor i in range(25):\n    plt.subplot(5,5,i+1)\n    plt.xticks([])\n    plt.yticks([])\n    plt.grid(False)\n    plt.imshow(tf.reshape(x_train[i], (28,28)), cmap=plt.cm.binary)\n    plt.title(f'label {y_train[i]}')\nplt.show()","b8aeff6d":"train_ds = tf.data.Dataset.from_tensor_slices((x_train\/255.0, y_train)).batch(batch_size).shuffle(len(x_train))\ntest_ds = tf.data.Dataset.from_tensor_slices((x_test\/255.0, y_test)).batch(batch_size).shuffle(len(x_train))","ae90db25":"# In this way tensorflow store our dataset in the batch format\nfor x, y_label in train_ds:\n    print(y_label)\n    break","8c8d1f0b":"def softmax(y_pred):\n    return tf.exp(y_pred)\/tf.reduce_sum(tf.exp(y_pred))","7005ea1d":"def cross_entropy(one_hot_encoded_batch, softmax_output):\n    return  tf.reduce_sum(-1*one_hot_encoded_batch* tf.math.log(softmax_output), axis = 1)","5c0f7530":"# Let's initialize the weights and bias\nnumber_of_features = 784  # No of features in the dataset\nnumber_of_classes = 10    # We have 10 classes ranging from 0-9\n\n# Initializing weights, with mean=0 and standard deviation 0.1\nW = tf.Variable(tf.random.normal((number_of_features, number_of_classes),mean = 0.0, stddev=0.1))\n\n# Initializing bias = 0\nb = tf.Variable(tf.zeros(number_of_classes, dtype='float32'))","b058909e":"from tqdm import tqdm\nimport numpy as np\nfrom sklearn.metrics import accuracy_score","79be6096":"lr = 0.001  # Learning rate for Gradient Descent\ntraining_loss = []  #  training loss after each sample  \npred_y = []  # We will predict value on the basis of equation y = xW + b\nactual_y = [] # Actual class label\nll=[] # Store loss after each epoch\n\nfor epoch in range(3): # Consider to execute this for 6 epochs\n    \n    for i, j in tqdm(train_ds.take(1000)): # Considering 1000*32 = 32000 example output for training\n        \n        for x, y in zip(i,j): # This loop will take one sample at a time\n            \n            X = tf.expand_dims(x, axis = 0)  # We have sample with dim (784,). So we will convert to (1, 784)\n            y_ = y\n\n            with tf.GradientTape() as tape:   #Using tensorflow to calculate Derivative in the next steps\n                y_pred = X @ W + b            # Predicted value of y\n                softmax_output = softmax(y_pred)  # Softmax output, Which is probability distrubution for 10 classes\n\n                one_hot_encoded_batch = tf.one_hot(y_, depth = number_of_classes) # One hot encoding for y label\n                \n                # Cross entropy loss to calculate distance between two probability distrubution\n                loss = cross_entropy(one_hot_encoded_batch, softmax_output) \n        \n                pred_y.append(np.argmax(softmax_output)) # What is prediction base on weight and bais we have\n                actual_y.append( y.numpy()) # Actual label\n    #             print(loss.numpy())\n            \n            # Calculating gradient of loss function w.r.t weight and bias.\n            [dl_dw, dl_db] = tape.gradient(loss, [W, b]) \n            \n            # Updating weight and bias\n            W.assign_sub(lr*dl_dw)\n            b.assign_sub(lr*dl_db)\n            \n            # Collecting training loss\n        training_loss.append(loss)\n    ll.append(loss)","748c636c":"plt.figure(figsize=(15,5))\nplt.plot(np.arange(0, len(training_loss)), training_loss)\nplt.title('Cross-Entropy Loss After Each Batch')\nplt.show()","937b720b":"plt.plot(np.arange(0, len(ll)), ll)\nplt.title('Cross-Entropy Loss After Each Epoch')\nplt.show()","e641c61f":"pred_y = []\nactual_y = []\nfor xt, yt in test_ds:\n    pred_y.append(tf.argmax(softmax(xt @ W + b), axis = 1).numpy())\n    actual_y.append(yt.numpy())","fb0ce9a5":"# Acuuracy of our model 90%, Not too bad\naccuracy_score(np.concatenate(actual_y, axis=0) , np.concatenate(pred_y, axis=0) )","cbc82949":"plt.figure(figsize=(10,10))\nfor i in range(9):\n    plt.subplot(3,3,i+1)\n\n    plt.grid(False)\n    plt.imshow(tf.reshape(x_test[i],(28,28)), cmap=plt.cm.binary)\n#     print(tf.argmax(softmax(tf.expand_dims(x_test[i]\/255.0, 0) @ W + b), axis = 1).numpy())\n    plt.title(f'Predicted Label {tf.argmax(softmax(tf.expand_dims(x_test[i]\/255.0, 0) @ W + b), axis = 1).numpy()[0]}')\nplt.show()","564181c9":"plt.figure(figsize=(10,10))\nimg_count = 0\nfor i in range(10000):\n    if img_count==9:\n        break\n    if tf.argmax(softmax(tf.expand_dims(x_test[i]\/255.0, 0) @ W + b), axis = 1).numpy()[0] != y_test[i]:\n        plt.subplot(3,3,img_count+1)\n\n        plt.grid(False)\n        plt.imshow(tf.reshape(x_test[i],(28,28)), cmap=plt.cm.binary)\n    #     print(tf.argmax(softmax(tf.expand_dims(x_test[i]\/255.0, 0) @ W + b), axis = 1).numpy())\n        plt.title(f'Predicted Label {tf.argmax(softmax(tf.expand_dims(x_test[i]\/255.0, 0) @ W + b), axis = 1).numpy()[0]}')\n        img_count+=1\n    \nplt.show()","cbbca29b":"## Cross-Entropy  \n<a id = 'section_4'><\/a>\n","23b5cca5":"## Reading the Dataset  \n<a id = 'section_2'><\/a>","e87096b3":"## Softmax Function  \n<a id = 'section_3'><\/a>  \nWe know the linear equation y = wX + b ,\nWe get output of the above equation, Which need to be converted into probability so that We can predict the class of image.      \nWe need to convert into probability distrubution, We can do this with softmax function","ca27ac58":"![image.png](attachment:c304f887-681e-4336-9197-2a4e05acd8bf.png)","63a69e6a":"## Visualization  \n<a id = 'section_7'><\/a>  \nIt's always worth it to visualize where we perform well and take a time to appreciate it and try to figure out why we are wrong to particular images.  \n* Predicted Correctly\n* Predicted Wrong","8d84ba02":"## Workflow for solving Softmax regression  \n<a id = 'section_1'><\/a>","68774100":"Cross entropy loss function.  \nBy using softmax function we calculated probability distrubution. Let's explain this with an example.  \nAssume we are predicting three classes (dog, cat, mouse)  \nprobability_distrubution of three classes we get from softmax function are = [0.1, 0.5, 0.4]   \n\n0.1 means- There is 10% probability that given image is Dog.  \n0.5 means- There are 50% probability that given image is Cat.  \n0.4 means- There are 10% probability that given image is Mouse.\n\nWe want to maximize likelihood:-  \n$$ P(Y|X) = \\prod_{i}^{n} P(y^{i} | x^{i}) $$  \n\nNow, Let's assume given image is of Mouse, so our one hot encoded array will be [0, 0, 1],  \nAbove represented array is also shows probability distrubution with 100% probability that given image is of Mouse.  \n\nIn any Machine Learning algorithm we need to calculate Loss, Which shows how wrong we are while predicting. So to calculate this loss , we will use cross entropy loss function.  \n\n$$ l(y_{pred}, y_{True}) = - \\sum_{i}^{n} y_(True) log(y_{pred})$$\n\nSo, By negative log of above equation we get formula for cross entropy, which shows minimizing of negative log likelihood.  \nIn simple tems, It is distance of y_true to the y_pred.  ","c2375260":"## Reference  \n[https:\/\/d2l.ai\/](#https:\/\/d2l.ai\/)","0c574f4d":"## Evaluation of Model  \n<a id='section_6'><\/a>","fcbb2203":"For evaluations method, we can use on clssification data you can check this notebook for clear understanding [KaggleNotebook](https:\/\/www.kaggle.com\/ramsegaurav\/evaluation-methods-for-classification-from-scratch)","03777684":"## Predicted Wrong  \nSo by visualizing we can see some pretty bad images were predicted wrong.","73217e34":"## Predicted Correctly","22705ac6":"By using tensorflow, we convert dataset in the batch format.  \nIt is always prefered to use scaled dataset while passing to machine learning algorithm, So we will divide dataset with  255.0","48644d08":"$$ y_i = {\\exp(y_i) \\over \\sum_{i}^{n} exp(y_i)}$$  \nAs we know sum of the probabilities will be equal to one.  \nwhere \/inline:$$ \\sum_{i}^{n} y_i = 1$$  \n\nIntuition behind Softmax,  \nWe will solve our linear equation $$ y_{pred} = X W + b $$  \nNow, We want predicted value as a probability of a class. So that we can easily classify image.  ","79d36f3d":"## Table of content  \n* [Workflow for solving softmax regression](#section_1)  \n* [Read the dataset](#section_2)   \n* [How Softmax function Works ?](#section_3)     \n* [How Cross Entropy Loss works ?](#section_4)   \n* [Training Model](#section_5)   \n* [Evaluation of Model](#section_6)   \n    * Accuracy\n* [Visualization](#section_7)   \n    * Predicted cporrectly  \n    \n    * Predicted Wrong","1378f6ed":"## Training model  \n<a id = 'section_5'><\/a>"}}