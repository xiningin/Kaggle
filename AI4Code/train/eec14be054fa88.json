{"cell_type":{"57ce4ff0":"code","8c2a7b67":"code","70927cf5":"code","7644aaa1":"code","bf773cdd":"code","a8391a81":"code","f8725e89":"code","c25bca10":"code","7e0aa115":"code","30b83acc":"code","0d4942e4":"code","cb5346a6":"code","3277d9a8":"code","da58aafd":"code","69604267":"code","e47bce1d":"code","2483c880":"code","41d7df75":"code","d1cb5ac8":"code","34f3148b":"code","52b66674":"code","4d029d8a":"code","519afa9d":"code","111278dc":"code","7f145e2c":"code","de9b970e":"code","b8366f59":"code","165c973e":"code","9e4a714b":"code","61d1279b":"code","13ed21d9":"code","153ce88c":"code","c01adfd3":"code","9b44425a":"code","e60e8484":"code","74399d71":"code","28f2ea01":"markdown","173bc7f4":"markdown","419bf400":"markdown","1bed1e46":"markdown","9e373845":"markdown","fc4e7e0a":"markdown","0ecd7922":"markdown","154c2784":"markdown","7b7df243":"markdown","d00fdf0f":"markdown","0fc4b419":"markdown","8b0fc391":"markdown","45bd3179":"markdown","cbb496f8":"markdown","6fa166ad":"markdown","5d8cb7c9":"markdown","8c2f754c":"markdown"},"source":{"57ce4ff0":"! pip install tf-models-official==2.4.0 -q\n! pip install tensorflow-gpu==2.4.1 -q\n! pip install tensorflow-text==2.4.1 -q\n! python -m spacy download en_core_web_sm -q\n! pip install dataprep | grep -v 'already satisfied'","8c2a7b67":"import pandas as pd\nimport numpy as np\nnp.set_printoptions(precision=4)\n\nimport tensorflow as tf\nfrom tensorflow import keras\n\n# Visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom dataprep.eda import plot, plot_diff, plot_correlation, create_report\nfrom dataprep.clean import clean_text\n\n# Preprocessing and Modelling\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport spacy\nimport tensorflow_text as text\nimport tensorflow_hub as hub\nfrom tensorflow.keras.layers import Input, Dense, Flatten, Dropout, concatenate \nfrom tensorflow.keras import Model, regularizers \nfrom tensorflow.keras.metrics import BinaryAccuracy\nfrom tensorflow.keras.losses import BinaryCrossentropy\nfrom official.nlp.optimization import create_optimizer # AdamW optimizer\n# Warning\nimport warnings\nwarnings.filterwarnings('ignore')","70927cf5":"tf.__version__","7644aaa1":"# Random seeds\nimport random\nimport numpy as np\nimport tensorflow as tf\nrandom.seed(319)\nnp.random.seed(319)\ntf.random.set_seed(319)","bf773cdd":"train_full = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/train.csv')\ntest_full = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/test.csv')\n\nprint('Training Set Shape = {}'.format(train_full.shape))\nprint('Training Set Memory Usage = {:.2f}MB'.format(train_full.memory_usage().sum()\/2**20))\n\nprint('Test Set Shape = {}'.format(test_full.shape))\nprint('Test Set Memory Usage = {:.2f}MB'.format(test_full.memory_usage().sum()\/2**20))","a8391a81":"plot(train_full)","f8725e89":"create_report(train_full)","c25bca10":"plot(train_full, 'text')","7e0aa115":"train_full.text","30b83acc":"plot(train_full, \"text\", \"target\")","0d4942e4":"df1 = train_full.text[train_full.target == 0]\ndf2 = train_full.text[train_full.target == 1]\nplot_diff([df1, df2])","cb5346a6":"# Read commited-dataset\ndf_train = pd.read_csv(\"\/kaggle\/input\/disastertweet-prepared2\/train_prepared.csv\")\ndf_test = pd.read_csv(\"\/kaggle\/input\/disastertweet-prepared2\/test_prepared.csv\")","3277d9a8":"# Only apply 'keyword' columns in full data, because other features cleaned in df_train\/test\ntrain_full = clean_text(train_full,'keyword')\ntest_full = clean_text(test_full, 'keyword')","da58aafd":"# Adding cleaned data into df_train\/test\ndf_train['keyword'] = train_full['keyword']\ndf_test['keyword'] = test_full['keyword']","69604267":"# Load Spacy Library\nnlp_spacy = spacy.load('en_core_web_sm')\n# Load the sentence encoder\nsentence_enc = hub.load('https:\/\/tfhub.dev\/google\/universal-sentence-encoder\/4')","e47bce1d":"def extract_keywords(text):\n    potential_keywords = []\n    TOP_KEYWORD = -1\n    # Create a list for keyword parts of speech\n    pos_tag = ['ADJ', 'NOUN', 'PROPN']\n    doc = nlp_spacy(text)\n    \n    for i in doc:\n        if i.pos_ in pos_tag:\n            potential_keywords.append(i.text)\n\n    document_embed = sentence_enc([text])\n    potential_embed = sentence_enc(potential_keywords)    \n    \n    vector_distances = cosine_similarity(document_embed, potential_embed)\n    keyword = [potential_keywords[i] for i in vector_distances.argsort()[0][TOP_KEYWORD:]]\n\n    return keyword\n\ndef keyword_filler(keyword, text):\n    if pd.isnull(keyword):\n        try:\n            keyword = extract_keywords(text)[0]\n        except:\n            keyword = '' \n        \n    return keyword","2483c880":"df_train.keyword = pd.DataFrame(list(map(keyword_filler, df_train.keyword, df_train.text))).astype(str)\ndf_test.keyword = pd.DataFrame(list(map(keyword_filler, df_test.keyword, df_test.text))).astype(str)\n\nprint('Null Training Keywords => ', df_train['keyword'].isnull().any())\nprint('Null Test Keywords => ', df_test['keyword'].isnull().any())","41d7df75":"df_train","d1cb5ac8":"keyword_non_disaster = df_train.keyword[df_train.target==0].value_counts().reset_index()\nsns.barplot(data=keyword_non_disaster[:10], x='keyword', y='index')\nplt.title('Non-Disaster Keyword Frequency (0)')\nplt.xlabel('Frequency')\nplt.ylabel('Top 10 Keywords')\nplt.show()","34f3148b":"keyword_disaster = df_train.keyword[df_train.target==1].value_counts().reset_index()\nsns.barplot(data=keyword_non_disaster[:10], x='keyword', y='index')\nplt.title('Non-Disaster Keyword Frequency (0)')\nplt.xlabel('Frequency')\nplt.ylabel('Top 10 Keywords')\nplt.show()","52b66674":"# Spilt data\nX_train, X_val, y_train, y_val = train_test_split(df_train[['text','keyword']],\n                                                    df_train.target, \n                                                    test_size=0.2, \n                                                    random_state=42)\nX_train.shape, X_val.shape","4d029d8a":"train_ds = tf.data.Dataset.from_tensor_slices((dict(X_train), y_train))\nval_ds = tf.data.Dataset.from_tensor_slices((dict(X_val), y_val))\ntest_ds = tf.data.Dataset.from_tensor_slices(dict(df_test[['text','keyword']]))","519afa9d":"AUTOTUNE = tf.data.experimental.AUTOTUNE\n\nBUFFER_SIZE = 1000\nBATCH_SIZE = 32\nRANDOM_SEED = 319\n\ndef configure_dataset(dataset, shuffle=False, test=False):\n    if shuffle:\n        dataset = dataset.cache()\\\n                        .shuffle(BUFFER_SIZE, seed=RANDOM_SEED, reshuffle_each_iteration=True)\\\n                        .batch(BATCH_SIZE, drop_remainder=True)\\\n                        .prefetch(AUTOTUNE)\n    elif test:\n        dataset = dataset.cache()\\\n                        .batch(BATCH_SIZE, drop_remainder=False)\\\n                        .prefetch(AUTOTUNE)\n    else:\n        dataset = dataset.cache()\\\n                        .batch(BATCH_SIZE, drop_remainder=True)\\\n                        .prefetch(AUTOTUNE)\n    return dataset","111278dc":"a3 = configure_dataset(train_ds, shuffle=True)\ndict3 = []\nfor elem in a3:\n    dict3.append(elem[0]['text'][0])\ndict3[:10]","7f145e2c":"# Configure the datasets\ntrain_ds = configure_dataset(train_ds, shuffle=True)\nval_ds = configure_dataset(val_ds)\ntest_ds = configure_dataset(test_ds, test=True)","de9b970e":"# Free memory\ndel X_train, X_val, y_train, y_val, df_train, df_test, train_full, test_full","b8366f59":"# Bidirectional Encoder Representations from Transformers (BERT).\nbert_encoder_path = \"https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_L-12_H-768_A-12\/4\"\n# Text preprocessing for BERT.\nbert_preprocessor_path = \"https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_preprocess\/3\"\n# Token based text embedding trained on English Google News 200B corpus.\nkeyword_embedding_path = \"https:\/\/tfhub.dev\/google\/nnlm-en-dim128-with-normalization\/2\"","165c973e":"bert_encoder = hub.KerasLayer(bert_encoder_path, trainable=True, name=\"BERT_Encoder\")\nbert_preprocessor = hub.KerasLayer(bert_preprocessor_path, name=\"BERT_Preprocessor\")\nnnlm_embed = hub.KerasLayer(keyword_embedding_path, name=\"NNLM_Embedding\")","9e4a714b":"kernel_initializer = tf.keras.initializers.GlorotNormal(seed=319)\n# Model function\ndef create_model():\n    # Keyword Branch\n    text_input = Input(shape=(), dtype=tf.string, name=\"text\")\n    encoder_inputs = bert_preprocessor(text_input)\n    encoder_outputs = bert_encoder(encoder_inputs)\n    # Pooled output\n    pooled_output = encoder_outputs[\"pooled_output\"]\n    bert_branch = Dropout(0.1,\n                          seed=319,\n                          name=\"BERT_Dropout\")(pooled_output)\n    # Construct keyword layers\n    keyword_input = Input(shape=(), dtype=tf.string, name='keyword')\n    keyword_embed = nnlm_embed(keyword_input)\n    keyword_flat = Flatten(name=\"Keyword_Flatten\")(keyword_embed)\n    keyword_dense1 = Dense(128, \n                          activation='relu',\n                          kernel_initializer=kernel_initializer,\n                          kernel_regularizer=regularizers.l2(1e-4),\n                          name=\"Keyword_Dense1\"\n                         )(keyword_flat)\n    keyword_branch1 = Dropout(0.5,\n                             seed=319,\n                             name='Keyword_dropout1'\n                            )(keyword_dense1)\n    keyword_dense2 = Dense(128, \n                          activation='relu',\n                          kernel_initializer=kernel_initializer,\n                          kernel_regularizer=regularizers.l2(1e-4),\n                          name=\"Keyword_Dense2\"\n                         )(keyword_branch1)\n    keyword_branch2 = Dropout(0.5,\n                             seed=319,\n                             name='Keyword_dropout2'\n                            )(keyword_dense2)\n    keyword_dense3 = Dense(128, \n                          activation='relu',\n                          kernel_initializer=kernel_initializer,\n                          kernel_regularizer=regularizers.l2(1e-4),\n                          name=\"Keyword_Dense3\"\n                         )(keyword_branch2)\n    keyword_branch3 = Dropout(0.5,\n                             seed=319,\n                             name='Keyword_dropout3'\n                            )(keyword_dense3)\n    \n    # Merge the layers and classify\n    merge = concatenate([bert_branch, keyword_branch3], name=\"Concatenate\")\n    dense = Dense(128, \n                  activation='relu',\n                  kernel_initializer=kernel_initializer,\n                  kernel_regularizer=regularizers.l2(1e-4), \n                  name=\"Merged_Dense\")(merge)\n    dropout = Dropout(0.5,\n                      seed=319,\n                      name=\"Merged_Dropout\"\n                     )(dense)\n    clf = Dense(1,\n                activation=\"sigmoid\", \n                kernel_initializer=kernel_initializer,\n                name=\"Classifier\"\n               )(dropout)\n    return Model([text_input, keyword_input], \n                 clf, \n                 name=\"BERT_Classifier\")","61d1279b":"bert_classifier = create_model()\nbert_classifier.summary()","13ed21d9":"keras.utils.plot_model(bert_classifier, \n                      show_shapes=False)","153ce88c":"EPOCHS = 3\nLEARNING_RATE = 5e-5\n\nSTEPS_PER_EPOCH = int(train_ds.unbatch().cardinality().numpy() \/ BATCH_SIZE)\nVAL_STEPS = int(val_ds.unbatch().cardinality().numpy() \/ BATCH_SIZE)\n# Calculate the train and warmup steps for the optimizer\nTRAIN_STEPS = STEPS_PER_EPOCH * EPOCHS\nWARMUP_STEPS = int(TRAIN_STEPS * 0.1)\n\nadamw_optimizer = create_optimizer(\n    init_lr=LEARNING_RATE,\n    num_train_steps=TRAIN_STEPS,\n    num_warmup_steps=WARMUP_STEPS,\n    optimizer_type='adamw'\n)","c01adfd3":"STEPS_PER_EPOCH, VAL_STEPS, TRAIN_STEPS, WARMUP_STEPS","9b44425a":"bert_classifier.compile(loss=BinaryCrossentropy(from_logits=True),\n                   optimizer=adamw_optimizer, \n                   metrics=[BinaryAccuracy(name=\"accuracy\")]\n                  )\nhistory = bert_classifier.fit(train_ds, \n                         epochs=EPOCHS,\n                         steps_per_epoch=STEPS_PER_EPOCH,\n                         validation_data=val_ds,\n                         validation_steps=VAL_STEPS\n                        )","e60e8484":"def submission(model, test):\n    sample_sub = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/sample_submission.csv')\n    predictions =  model.predict(test)\n    y_preds = [ int(i) for i in np.rint(predictions)]\n    sub = pd.DataFrame({'id':sample_sub['id'].values.tolist(),'target':y_preds})\n    sub.to_csv('submission.csv', index=False)","74399d71":"submission(bert_classifier, test_ds)","28f2ea01":"<a id='2'><\/a>\n# <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:150%; text-align:center; border-radius: 15px 50px;\">2. EDA \ud83d\udcca<\/p>\n\n\n[Content](#0)","173bc7f4":"# Visualization the Keyword Frequency","419bf400":"## Training the model again","1bed1e46":"<a id=7 ><\/a>\n# <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:140%; text-align:left; border-radius: 20px 50px;\">7. References<\/p>\n\n[Content](#0)","9e373845":"<a id='3'><\/a>\n# <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:150%; text-align:center; border-radius: 15px 50px;\">3. Data Pre-processing <\/p>\n\nNow we are going to engineering the data to make it easier for the model to clasiffy.\n\nThis section is very important to reduce the dimensions of the problem.\n\n\n\n\n[Content](#0)","fc4e7e0a":"### Range from 120 to 140 characters is the most common in tweet.","0ecd7922":"# AdamW Optimizer","154c2784":"<a id='1'><\/a>\n# <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:150%; text-align:center; border-radius: 15px 50px;\">1. Loading Data \ud83d\udc8e<\/p>\n\nJust load the dataset and global variables for colors and so on.\n\n[Content](#0)","7b7df243":"# If you like this kernel, please upvote and tell me your thought. Thank you @@","d00fdf0f":"<a id=6 ><\/a>\n# <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:140%; text-align:left; border-radius: 20px 50px;\">6. Make a Submission<\/p>\n\n[Content](#0)","0fc4b419":"<a id=0><\/a>\n## <p style=\"background-color:lightblue; font-family:newtimeroman; font-size:120%; text-align:left; border-radius: 15px 50px;\">Table of Content<\/p>\n* [0. Introduction and updates](#0)\n* [1. Loading Data \ud83d\udc8e](#1)\n* [2. EDA \ud83d\udcca](#2)\n* [3. Data Preprocessing](#3)\n* [4. Vectorization](#4)\n    * [4.1 Common Vectorizer Usage](#4.1)\n    * [4.2 If-Idf Term Weightings](#4.2)\n* [5. Transfer Learning with Hugging Face](#5)\n    * [5.1 Tokenization](#5.1)\n    * [5.2 Defining a Model Architecture](#5.2)\n    * [5.3 Training Classification Layer Weights](#5.3)\n    * [5.4 Fine-tuning DistilBert and Training All Weights](#5.4)\n* [6. Make a Submission](#6)\n* [7. References](#7)","8b0fc391":"# Main technics I used in this data\n    * [3.1] Remove 157 duplicated rows\n    * [3.2] Cleaning text\n    * [3.3] Spelling Checker\n    * [3.4] Remove Stemming\n #### Step 3.3 spends a lot time (around 4000s in 4536s in total). \n #### So, I splits Data Preprocessing into [another kernel](https:\/\/www.kaggle.com\/phanttan\/disastertweet-prepareddata). \n #### And the prepared data to save in to [new dataset](https:\/\/www.kaggle.com\/phanttan\/disastertweet-prepared2)\n #### I am so appreciate to you for using\/upvoting it.\n","45bd3179":"# Classifier Model","cbb496f8":"# Create TensorFlow Datasets","6fa166ad":"<a id=0><\/a>\n<font size=\"+3\" color=\"#5bc0de\"><b>Introduction <\/b><\/font><br>\n[Content](#0)\n\nIn this kernel, beside the general steps working with text data as EDA, preprocessing. The workflow in Modelling can divided into 2 main stages:\n1. Defining a Model Architecture with concatenation a keyword column into BERT model\n2. Training Classification Layer Weights.\n\n<a id=1.2 ><\/a>\n<font size=\"+3\" color=\"#5bc0de\"><b>1.2. Update via Versions <\/b><\/font><br>\n[Content](#0)\n\n### Current Version\n* Adding 1 hidden layer in Model to incease accuracy.\n\n\n[Content](#0)","5d8cb7c9":"[Hugging Face Transformers Fine-Tunning DistilBert for Binary Classification Tasks](https:\/\/towardsdatascience.com\/hugging-face-transformers-fine-tuning-distilbert-for-binary-classification-tasks-490f1d192379)\n\n[Keras functional API](https:\/\/keras.io\/guides\/functional_api\/)\n\n[Distil Bert](https:\/\/huggingface.co\/transformers\/model_doc\/distilbert.html)\n\n[Tensorflow Dataset](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/data\/Dataset)\n\n[BERT in TFHub](https:\/\/tfhub.dev\/google\/collections\/bert)\n\n[TensorFlow NLP Modelling Toolkit](https:\/\/github.com\/tensorflow\/models\/tree\/master\/official\/nlp)\n\n[NLP With BERT from Tendorflow](https:\/\/www.tensorflow.org\/text\/tutorials\/fine_tune_bert)\n\n[NLP Optimization](https:\/\/github.com\/tensorflow\/models\/blob\/master\/official\/nlp\/optimization.py)","8c2f754c":"### Dataset is balanced"}}