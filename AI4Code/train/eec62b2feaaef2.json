{"cell_type":{"9b05fbea":"code","117fff17":"code","c8cfb437":"code","df501ec4":"code","1852adc8":"code","f2158ff3":"code","cf8bf5c6":"code","dd0a984d":"code","658e1f5d":"code","2ceee906":"code","aa32b11c":"code","f400f006":"code","d2817aab":"code","c75f9526":"code","5550fc34":"code","6ba96d56":"code","e21a72b1":"code","5c16f11a":"code","390dfd18":"code","4e83e1bd":"code","1f5f1f57":"code","2b55b5a3":"code","468c5ac6":"code","93af3a4d":"markdown","bd528cce":"markdown","9f3fc5b0":"markdown","5d592241":"markdown","e331cb3d":"markdown","d31d7c30":"markdown","6a1fb24d":"markdown","fcb0e3a4":"markdown","84f2cdba":"markdown","db0413ec":"markdown","abd15c2a":"markdown","3a4bdaad":"markdown","c0613192":"markdown","74f96dbd":"markdown","3c5e5ddc":"markdown","6a41607b":"markdown","c91ca501":"markdown","36d938cd":"markdown"},"source":{"9b05fbea":"import warnings\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom scipy.stats.stats import pearsonr\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, recall_score, precision_score\n\n%matplotlib inline\n# Suppress warnings\nwarnings.filterwarnings(\"ignore\")","117fff17":"def cross_validate(estimator, train, validation):\n    X_train = train[0]\n    Y_train = train[1]\n    X_val = validation[0]\n    Y_val = validation[1]\n    train_predictions = classifier.predict(X_train)\n    train_accuracy = accuracy_score(train_predictions, Y_train)\n    train_recall = recall_score(train_predictions, Y_train)\n    train_precision = precision_score(train_predictions, Y_train)\n\n    val_predictions = classifier.predict(X_val)\n    val_accuracy = accuracy_score(val_predictions, Y_val)\n    val_recall = recall_score(val_predictions, Y_val)\n    val_precision = precision_score(val_predictions, Y_val)\n\n    print('Model metrics')\n    print('Accuracy  Train: %.2f, Validation: %.2f' % (train_accuracy, val_accuracy))\n    print('Recall    Train: %.2f, Validation: %.2f' % (train_recall, val_recall))\n    print('Precision Train: %.2f, Validation: %.2f' % (train_precision, val_precision))","c8cfb437":"train_raw = pd.read_csv('..\/input\/train.csv')\ntest_raw = pd.read_csv('..\/input\/test.csv')\ntest_ids = test_raw['PassengerId'].values\n\n# Join data to analyse and process the set as one.\ntrain_raw['train'] = 1\ntest_raw['train'] = 0\ndata = train_raw.append(test_raw, sort=False)","df501ec4":"data.head()","1852adc8":"data.describe()","f2158ff3":"features = ['Age', 'Embarked', 'Fare', 'Parch', 'Pclass', 'Sex', 'SibSp']\ntarget = 'Survived'\n\ndata = data[features + [target] + ['train']]\n# Categorical values need to be transformed into numeric.\ndata['Sex'] = data['Sex'].replace([\"female\", \"male\"], [0, 1])\ndata['Embarked'] = data['Embarked'].replace(['S', 'C', 'Q'], [1, 2, 3])\ndata['Age'] = pd.qcut(data['Age'], 10, labels=False)","cf8bf5c6":"# Split data into train and test.\ntrain = data.query('train == 1')\ntest = data.query('train == 0')\n\n# Drop missing values from the train set.\ntrain.dropna(axis=0, inplace=True)\nlabels = train[target].values","dd0a984d":"train.head()","658e1f5d":"columns = train[features + [target]].columns.tolist()\nnColumns = len(columns)\nresult = pd.DataFrame(np.zeros((nColumns, nColumns)), columns=columns)\n\n# Apply Pearson correlation on each pair of features.\nfor col_a in range(nColumns):\n    for col_b in range(nColumns):\n        result.iloc[[col_a], [col_b]] = pearsonr(train.loc[:, columns[col_a]], train.loc[:,  columns[col_b]])[0]\n        \nfig, ax = plt.subplots(figsize=(10,10))\nax = sns.heatmap(result, yticklabels=columns, vmin=-1, vmax=1, annot=True, fmt='.2f', linewidths=.2)\nax.set_title('PCC - Pearson correlation coefficient')\nplt.show()","2ceee906":"continuous_numeric_features = ['Age', 'Fare', 'Parch', 'SibSp']\nfor feature in continuous_numeric_features:\n    sns.distplot(train[feature])\n    plt.show()","aa32b11c":"train.drop(['train', target, 'Pclass'], axis=1, inplace=True)\ntest.drop(['train', target, 'Pclass'], axis=1, inplace=True)","f400f006":"X_train, X_val, Y_train, Y_val = train_test_split(train, labels, test_size=0.2, random_state=1)","d2817aab":"X_train.head()","c75f9526":"X_train1, X_train2, Y_train1, Y_train2 = train_test_split(X_train, Y_train, test_size=0.3, random_state=12)","5550fc34":"classifier = GaussianNB()","6ba96d56":"classifier.fit(X_train2, Y_train2)","e21a72b1":"print('Metrics with only 30% of train data')\ncross_validate(classifier, (X_train, Y_train), (X_val, Y_val))","5c16f11a":"classifier.partial_fit(X_train1, Y_train1)","390dfd18":"print('Metrics with the remaining 70% of train data')\ncross_validate(classifier, (X_train, Y_train), (X_val, Y_val))","4e83e1bd":"print('Probability of each class')\nprint('Survive = 0: %.2f' % classifier.class_prior_[0])\nprint('Survive = 1: %.2f' % classifier.class_prior_[1])","1f5f1f57":"print('Mean of each feature per class')\nprint('               Age         Embarked   Fare         Parch       Sex         SibSp')\nprint('Survive = 0: %s' % classifier.theta_[0])\nprint('Survive = 1: %s' % classifier.theta_[1])","2b55b5a3":"print('Variance of each feature per class')\nprint('Survive = 0: %s' % classifier.sigma_[0])\nprint('Survive = 1: %s' % classifier.sigma_[1])","468c5ac6":"# Unfortunately sklearn naive Bayes algorithm currently do not make inference with missing data (but should do), so we need to input missing data.\ntest.fillna(test.mean(), inplace=True)\ntest_predictions = classifier.predict(test)\nsubmission = pd.DataFrame({'PassengerId': test_ids})\nsubmission['Survived'] = test_predictions.astype('int')\nsubmission.to_csv('submission.csv', index=False)\nsubmission.head(10)","93af3a4d":"### Split train data into two parts","bd528cce":"### Dependencies","9f3fc5b0":"As you can see our results improved after we updated  the model with the remaining data.\n\nThe sklearn model also give us some interesting options from the model API about the target class.","5d592241":"### Naive Bayes classifier\n\n#### This image may help to understand how a simple Bayesian model works.\n![Example of Bayes theorem aplication](http:\/\/users.sussex.ac.uk\/~christ\/crs\/kr-ist\/copied-pics\/humidity-bayesian-network.png)\n* Given that we have sun, rain and temperature events and we want to predict if will be humid or not, the probability of being humid will be calculated from the other given events ant theirs probabilities.\n* In the boxes we have \"sun\" as a independent event, \"rain\" and \"temp\" as events that depends from \"sun\" and finally \"humid\" that depends from all the other.\n* In the lower right text, we have the probability of \"humid\" being yes or no given that \"sun\" = yes (sun = 100%), so given that is sunny we have 46% probability of being humid and 54% probability of not being humid.\n\n#### How Naive Bayes algorithm works?\n* Convert the data set into a frequency table\n* Create Likelihood table by finding the probabilities.\n* Now, use Naive Bayesian equation to calculate the posterior probability for each class. The class with the highest posterior probability is the outcome of prediction.\n\n\n##### As you can see the whole model is built upon the probabilities of events, that would be our features.","e331cb3d":"### What are the Pros and Cons of Naive Bayes?\n#### Pros:\n* Humans are not good with reasoning in systems with limited or conflicting information. It would be handy if we have something to manage all this limited\/conflicting information.\n* It is easy and fast to predict class of test data set. It also perform well in multi class prediction\n* When assumption of independence holds, a Naive Bayes classifier performs better compare to other models like logistic regression and you need less training data.\n* It perform well in case of categorical input variables compared to numerical variable(s). For numerical variable, normal distribution is assumed (bell curve, which is a strong assumption).\n\n#### Cons:\n* Probably the most notable weakness of BNs is the designing methodology.There is no standard way of building BNs.\n* If categorical variable has a category (in test data set), which was not observed in training data set, then model will assign a 0 (zero) probability and will be unable to make a prediction. This is often known as \u201cZero Frequency\u201d. To solve this, we can use the smoothing technique. One of the simplest smoothing techniques is called Laplace estimation.\n* On the other side naive Bayes is also known as a bad estimator, so the probability outputs from predict_proba are not to be taken too seriously.\n* Another limitation of Naive Bayes is the assumption of independent predictors. In real life, it is almost impossible that we get a set of predictors which are completely independent.\n\n##### The design of a BN can be a considerable amount of effort in complex systems and it is based on the knowledge of the expert(s) who designed it. Although, this disadvantage can be good in another point of view, since BNs can be easily inspected by the designers and has the guarantee that the domain specific information is being used.\n\n### Tips to improve the power of Naive Bayes Model\n#### Here are some tips for improving power of Naive Bayes Model:\n\n* If continuous features do not have normal distribution, we should use transformation or different methods to convert it in normal distribution.\n* If test data set has zero frequency issue, apply smoothing techniques \u201cLaplace Correction\u201d to predict the class of test data set.\n* Remove correlated features, as the highly correlated features are voted twice in the model and it can lead to over inflating importance.\n* Naive Bayes classifiers has limited options for parameter tuning like alpha=1 for smoothing, fit_prior=[True|False] to learn class prior probabilities or not and some other options (look at detail here). I would recommend to focus on your  pre-processing of data and the feature selection.\n* You might think to apply some classifier combination technique like ensembling, bagging and boosting but these methods would not help. Actually, \u201censembling, boosting, bagging\u201d won\u2019t help since their purpose is to reduce variance. Naive Bayes has no variance to minimize.\n\n\n### References\n* [Introduction to Bayesian Networks with Jhonatan de Souza Oliveira - Machine Learning Mastery](https:\/\/machinelearningmastery.com\/introduction-to-bayesian-networks-with-jhonatan-de-souza-oliveira\/)\n* [6 Easy Steps to Learn Naive Bayes Algorithm (with codes in Python and R) - Analytics Vidhya](https:\/\/www.analyticsvidhya.com\/blog\/2017\/09\/naive-bayes-explained\/)\n* [Better Naive Bayes: 12 Tips To Get The Most From The Naive Bayes Algorithm - Machine Learning Mastery](https:\/\/machinelearningmastery.com\/better-naive-bayes\/)","d31d7c30":"Looking at our continuous numeric features we can see that \"Fare\", \"Parch\" and \"SibSp\", have a distribution close to normal, but with a left side skew, \"Age\" have a distribution a a bit different from the other but maybe it's close enough to Gaussian.","6a1fb24d":"### Apply the model on the test data and create submission","fcb0e3a4":"<h1><center>Naive Bayes - Titanic survival<\/center><\/h1>\n<h2><center>Probabilistic Machine Learning<\/center><\/h2>\n<img src=\"https:\/\/embedwistia-a.akamaihd.net\/deliveries\/f3d4e9ac9dbffd8ad349262f30cffada0a41f4af.jpg\" width=\"600\">\n\n#### What are Bayesian Networks?\n* In general, Bayesian Networks (BNs) is a framework for reasoning under uncertainty using probabilities. More formally, a BN is defined as a Directed Acyclic Graph (DAG) and a set of Conditional Probability Tables (CPTs). In practice, a problem domain is initially modeled as a DAG.\n* Naive Bayes assumes that the variables are independent and comes from a Gaussian distribution.\n\n#### The Bayes theorem\n<img src=\"https:\/\/i0.wp.com\/scienceprog.com\/wp-content\/uploads\/2016\/07\/Thomas_Bayes.png?fit=468%2C308&ssl=1\" width=\"400\">\n* P(A|B) is the posterior probability of class (A, target) given predictor (B,  attributes).\n* P(A) is the prior probability of class.\n* P(B|A) is the likelihood which is the probability of predictor given class.\n* P(B) is the prior probability of predictor.\n\n### Now let's apply this knowledge by building a Naive Bayes model on this data set.","84f2cdba":"#### Fit the first part\n* Fitting data here is really fast.","db0413ec":"### Correlation study\n* As we saw Naive Bayes models expect the features to be independent, so let's apply the Pearson correlation coefficient on them to give us a hint about how independent they are from the others.","abd15c2a":"### Overview the data","3a4bdaad":"Our processed train set","c0613192":"### Auxiliary functions","74f96dbd":"One advantage of Bayesian models is that it works well enough with small data, having more would give you more accurate probabilities but it's not data hungry as something like deep learning.\n\n### Pre-process\n* feature selection, data cleaning, feature engineering and data imputation","3c5e5ddc":"### Load data","6a41607b":"#### Update the model with the second part\n* Nice thing about this kind of model, you can update it by just fitting the model again with more data.","c91ca501":"### Split data in train and validation (80% ~ 20%)","36d938cd":"About the correlation between the features, we can see that \"Fare\" and \"Pclass\" seem to be highly related, so i'll remove \"Pclass\". Also features like \"Sex\", \"Pclass\" and \"Fare\" should be good predictors.\n\n### Distribution study\n* Also the model expect the features to come from a Gaussian (or normal) distribution, so let's check that as well."}}