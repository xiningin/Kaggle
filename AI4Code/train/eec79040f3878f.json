{"cell_type":{"4b488d71":"code","cf153e25":"code","28601531":"code","e1691923":"code","d725723d":"code","a57cdc26":"code","4ff93911":"code","e9a2c191":"code","5cd38983":"code","f6f4e952":"code","0db77d57":"code","9a921ea5":"code","046a08f4":"code","9fbb84d4":"code","c296424b":"code","8548dba9":"code","8f89e2c2":"code","aca27d94":"code","5cf23e53":"code","4f7f9c0f":"code","7f4e05e7":"code","a31c4ac2":"code","729406e6":"code","9e99f004":"code","657ab450":"code","b2c60d5e":"code","e7d3da2b":"code","aef06afc":"code","165f4237":"code","68087f7e":"code","d567cf87":"code","2530e778":"code","48844e44":"code","75654cea":"code","fbea6b5b":"code","15b00985":"code","a8e8b7a0":"code","f106627e":"code","4e9edc3d":"code","74354a04":"code","6448cbc7":"code","1c93726b":"code","f20ee436":"code","903523aa":"code","2cf23d28":"code","6b83ae44":"code","17a8dd13":"code","3555021e":"code","c5d642f9":"code","0d19c9ff":"code","7d11c31b":"code","0ab20199":"code","a2ccb346":"code","04801f79":"code","0c21323c":"code","4ce6799e":"code","f53dda0b":"code","0ab095ab":"code","8dd09013":"code","b7e0b895":"code","37f8fa50":"code","eaab5da7":"code","6fd1d79f":"code","f54b2a12":"code","b97ceffe":"code","0a97731a":"code","b6722a9f":"code","73134f77":"code","96b4a257":"code","5b64fede":"code","3e0625bf":"code","31df5c98":"code","f4ceb7e3":"code","7af00929":"code","810f8863":"code","23e52a85":"markdown","d6bca648":"markdown","5e2ac53a":"markdown","294889cf":"markdown","b1ca72ae":"markdown","8824b44e":"markdown","a75e795e":"markdown","59cb0088":"markdown","f31dccba":"markdown","c7b65a0e":"markdown","9ca6f5dd":"markdown","41b82857":"markdown","fbdfbea8":"markdown","004b792f":"markdown","81fb25de":"markdown","9cff549e":"markdown","ffebd7e0":"markdown","5f6ce223":"markdown","26e9154f":"markdown","3eeb1378":"markdown","5279ae39":"markdown","6dbce6a7":"markdown","cf84b642":"markdown","885fbf84":"markdown","3de289eb":"markdown","4a445c9f":"markdown"},"source":{"4b488d71":"# packages\n\n# standard\nimport numpy as np\nimport pandas as pd\nimport time\n\n# plots\nimport matplotlib.pyplot as plt\nimport plotly.express as px\nimport seaborn as sns\nfrom statsmodels.graphics.mosaicplot import mosaic\n\n# ML tools\nimport h2o\nfrom h2o.estimators import H2OGradientBoostingEstimator\nfrom h2o.estimators import H2ORandomForestEstimator","cf153e25":"# import data\ndf_train = pd.read_csv('..\/input\/DATACUP-ROUND2\/XENIA_DATACUP_ROUND2_TRAIN.csv')\ndf_test = pd.read_csv('..\/input\/DATACUP-ROUND2\/XENIA_DATACUP_ROUND2_X_TEST.csv')\ndf_sub = pd.read_csv('..\/input\/DATACUP-ROUND2\/sample_submission_round2.csv')","28601531":"df_train.head()","e1691923":"df_train.info()","d725723d":"df_test.info()","a57cdc26":"# caution: date format is inconsistent!!!\ndf_train.dt","4ff93911":"# let's fix this first\ndf_train.dt = df_train.dt.str.replace('-','\/')\ndf_train.dt = df_train.dt.str.replace('\/0','\/')\n\ndf_test.dt = df_test.dt.str.replace('-','\/')\ndf_test.dt = df_test.dt.str.replace('\/0','\/')","e9a2c191":"df_train['date'] = pd.to_datetime(df_train.dt, dayfirst=True)\ndf_train['year'] = df_train['date'].dt.year\ndf_train['month'] = df_train['date'].dt.month\ndf_train['day'] = df_train['date'].dt.day\ndf_train['hour'] = df_train['date'].dt.hour\ndf_train['weekday'] = df_train['date'].dt.weekday","5cd38983":"df_test['date'] = pd.to_datetime(df_test.dt, dayfirst=True)\ndf_test['year'] = df_test['date'].dt.year\ndf_test['month'] = df_test['date'].dt.month\ndf_test['day'] = df_test['date'].dt.day\ndf_test['hour'] = df_test['date'].dt.hour\ndf_test['weekday'] = df_test['date'].dt.weekday","f6f4e952":"df_train.flakes.value_counts()","0db77d57":"df_test.flakes.value_counts()","9a921ea5":"df_train.holiday.value_counts()","046a08f4":"df_test.holiday.value_counts()","9fbb84d4":"df_train.atmosphere.value_counts()","c296424b":"df_train.atmosphere = df_train.atmosphere.replace({' Fog': 'Fog'})\ndf_train.atmosphere = df_train.atmosphere.replace({'Fog ': 'Fog'})\ndf_train.atmosphere = df_train.atmosphere.replace({' Calm': 'Calm'})\ndf_train.atmosphere = df_train.atmosphere.replace({' Sprinkle': 'Sprinkle'})\ndf_train.atmosphere = df_train.atmosphere.replace({'Percipitation ': 'Precipitation'})","8548dba9":"df_test.atmosphere = df_test.atmosphere.replace({' Fog': 'Fog'})\ndf_test.atmosphere = df_test.atmosphere.replace({'Fog ': 'Fog'})\ndf_test.atmosphere = df_test.atmosphere.replace({' Calm': 'Calm'})\ndf_test.atmosphere = df_test.atmosphere.replace({' Sprinkle': 'Sprinkle'})\ndf_test.atmosphere = df_test.atmosphere.replace({'Percipitation ': 'Precipitation'})","8f89e2c2":"df_test.atmosphere.value_counts()","aca27d94":"df_train.statement.value_counts()","5cf23e53":"df_train.statement = df_train.statement.replace({'Sky is Clear': 'clear sky'})\ndf_train.statement = df_train.statement.replace({'highly raining': 'high rain'})\ndf_train.statement = df_train.statement.replace({'low rain': 'light rain'})","4f7f9c0f":"df_test.statement = df_test.statement.replace({'Sky is Clear': 'clear sky'})\ndf_test.statement = df_test.statement.replace({'highly raining': 'high rain'})\ndf_test.statement = df_test.statement.replace({'low rain': 'light rain'})","7f4e05e7":"df_test.statement.value_counts()","a31c4ac2":"# look at heat distribution\nplt.figure(figsize=(8,2))\nplt.boxplot(df_train.heat, vert=False)\nplt.title('heat')\nplt.grid()\nplt.show()","729406e6":"# remove outlier\ndf_train = df_train[df_train.heat > 100]","9e99f004":"# plot again\nplt.figure(figsize=(8,2))\nplt.boxplot(df_train.heat, vert=False)\nplt.title('heat')\nplt.grid()\nplt.show()","657ab450":"# look at rainfall distribution\nplt.figure(figsize=(8,2))\nplt.boxplot(df_train.rainfall, vert=False)\nplt.title('rainfall')\nplt.grid()\nplt.show()","b2c60d5e":"# remove outlier\ndf_train = df_train[df_train.rainfall < 8000]","e7d3da2b":"# plot again\nplt.figure(figsize=(8,2))\nplt.boxplot(df_train.rainfall, vert=False)\nplt.title('rainfall')\nplt.grid()\nplt.show()","aef06afc":"# negative volumes?\ncheck_me = df_train[df_train.vol<0]\ncheck_me","165f4237":"# remove them\ndf_train = df_train[df_train.vol>=0]\ndf_train.vol.describe()","68087f7e":"# export data prep results\ndf_train.to_csv('train_prep.csv')\ndf_test.to_csv('test_prep.csv')","d567cf87":"# basic stats\ntarget = 'vol'\ndf_train[target].plot(kind='hist', bins=100)\nplt.grid()\nplt.show()","2530e778":"df_train[target].describe()","48844e44":"my_alpha=0.25\nfig, ax = plt.subplots(figsize=(16,4))\nax.scatter(df_train.date, df_train.vol, alpha=my_alpha)\nax.xaxis.set_major_locator(plt.MaxNLocator(20)) # reduce number of x-axis labels\nplt.xticks(rotation=90)\nplt.grid()\nplt.show()","75654cea":"for yy in range(2012,2018):\n    foo = df_train[df_train.year==yy]\n    my_alpha=0.25\n    fig, ax = plt.subplots(figsize=(16,4))\n    ax.scatter(foo.date, foo.vol, alpha=my_alpha)\n    ax.xaxis.set_major_locator(plt.MaxNLocator(20)) # reduce number of x-axis labels\n    plt.xticks(rotation=90)\n    plt.title('Year '+str(yy))\n    plt.grid()\n    plt.show()","fbea6b5b":"features_num = ['heat', 'rainfall', 'flakes', \n                'cloud', 'pa']","15b00985":"df_train[features_num].describe()","a8e8b7a0":"# compare with test set\ndf_test[features_num].describe()","f106627e":"# look also at time features\ndf_train[['year','month','hour','day','weekday']].describe()","4e9edc3d":"# look also at time features - test\ndf_test[['year','month','hour','day','weekday']].describe()","74354a04":"df_test.month.value_counts()","6448cbc7":"# plot distribution of numerical features\nfor f in features_num:\n    plt.figure(figsize=(8,4))\n    df_train[f].plot(kind='hist', bins=50)\n    plt.title(f)\n    plt.grid()\n    plt.show()","1c93726b":"corr_pearson = df_train[features_num].corr(method='pearson')\ncorr_spearman = df_train[features_num].corr(method='spearman')\n\nfig = plt.figure(figsize = (7,5))\nsns.heatmap(corr_pearson, annot=True, cmap='RdYlGn', vmin=-1, vmax=+1)\nplt.title('Pearson Correlation')\nplt.show()\n\nfig = plt.figure(figsize = (7,5))\nsns.heatmap(corr_spearman, annot=True, cmap='RdYlGn', vmin=-1, vmax=+1)\nplt.title('Spearman Correlation')\nplt.show()","f20ee436":"features_cat = ['holiday', 'atmosphere', 'statement',\n                'year', 'month', 'day', 'hour', 'weekday']","903523aa":"# plot distribution of categorical features\nfor f in features_cat:\n    plt.figure(figsize=(14,4))\n    df_train[f].value_counts().plot(kind='bar')\n    plt.title(f)\n    plt.grid()\n    plt.show()","2cf23d28":"for f in features_num + ['year','month','day','hour','weekday']: # show also time features in numerical version\n    c = df_train[f].corr(df_train[target], method='pearson')\n    c = np.round(c,4)\n    plt.figure(figsize=(6,6))\n    plt.scatter(df_train[f], df_train[target], alpha=0.01)\n    plt.title('Target vs ' + f + ' \/ corr = ' + str(c))\n    plt.xlabel(f)\n    plt.ylabel('Target')\n    plt.grid()\n    plt.show()","6b83ae44":"for f in features_cat:\n    plt.figure(figsize=(10,5))\n    sns.boxplot(data=df_train, x=f, y=target)\n    plt.xticks(rotation=90)\n    plt.grid()\n    plt.show()","17a8dd13":"# select predictors\npredictors = features_num + features_cat\n\npredictors.remove('flakes') # flakes not used; is constant 0 on test set\npredictors.remove('year') # year not useful on test set (there always 2019 which is not in training)\n# predictors.remove('day') # potential source of overfitting\npredictors.remove('weekday') # does not seem to make sense, but results are worse with weekday\n\nprint('Number of predictors: ', len(predictors))\nprint(predictors)","3555021e":"# start H2O\nh2o.init(max_mem_size='12G', nthreads=4) # Use maximum of 12 GB RAM and 4 cores","c5d642f9":"# use only months 1 - 8 in training (=> in sync with test set)\ndf_train = df_train[df_train.month <= 8]","0d19c9ff":"# upload data frame in H2O environment\ntrain_hex = h2o.H2OFrame(df_train)\ntest_hex = h2o.H2OFrame(df_test)","7d11c31b":"# define Gradient Boosting model\nn_cv = 5\n\nfit_1 = H2OGradientBoostingEstimator(ntrees = 500, # stop early => avoid potential overfit\n                                     max_depth=4,\n                                     min_rows=5,\n                                     learn_rate=0.05, # default: 0.1\n                                     sample_rate=1,\n                                     col_sample_rate=0.7,\n                                     # categorical_encoding='one_hot_explicit',\n                                     nfolds=n_cv,\n                                     score_each_iteration=True,\n                                     stopping_metric='RMSE',\n                                     stopping_rounds=5,\n                                     stopping_tolerance=0.0001, # default 0.001\n                                     seed=999)\n\n# and train model\nt1 = time.time()\nfit_1.train(x=predictors,\n            y=target,\n            training_frame=train_hex)\nt2 = time.time()\nprint('Elapsed time [s]: ', np.round(t2-t1,2))","0ab20199":"# show cross validation metrics\nfit_1.cross_validation_metrics_summary()","a2ccb346":"# show scoring history - training vs cross validations\nfor i in range(n_cv):\n    cv_model_temp = fit_1.cross_validation_models()[i]\n    df_cv_score_history = cv_model_temp.score_history()\n    my_title = 'CV ' + str(1+i) + ' - Scoring History'\n    plt.scatter(df_cv_score_history.number_of_trees,\n                y=df_cv_score_history.training_rmse, \n                c='blue', label='training')\n    plt.scatter(df_cv_score_history.number_of_trees,\n                y=df_cv_score_history.validation_rmse, \n                c='darkorange', label='validation')\n    plt.title(my_title)\n    plt.xlabel('Number of Trees')\n    plt.ylabel('RMSE')\n    plt.legend()\n    plt.grid()\n    plt.show()","04801f79":"# variable importance - basic version\nfit_1.varimp_plot(-1)","0c21323c":"# variable importance using shap values => see direction as well as severity of feature impact\nt1 = time.time()\nfit_1.shap_summary_plot(train_hex);\nt2 = time.time()\nprint('Elapsed time [s]: ', np.round(t2-t1,2))","4ce6799e":"# predict on training data\npred_train_1 = fit_1.predict(train_hex).as_data_frame()","f53dda0b":"# plot predictions - training data\nplt.hist(pred_train_1, bins=100)\nplt.title('Predictions GBM - Training')\nplt.grid()\nplt.show()","0ab095ab":"plt.scatter(df_train[target], pred_train_1, alpha=0.1)\nplt.title('Scatter GBM Predicted vs Actual (Training)')\nplt.xlabel('Actual')\nplt.ylabel('Predicted')\nplt.grid()\nplt.show()","8dd09013":"# predict on test set\npred_test_1 = fit_1.predict(test_hex).as_data_frame()","b7e0b895":"# plot predictions - test set\nplt.hist(pred_test_1, bins=100)\nplt.title('Predictions GBM - Test')\nplt.grid()\nplt.show()","37f8fa50":"pred_test_1.describe()","eaab5da7":"# submission\ndf_sub_1 = df_sub.copy()\ndf_sub_1.vol = pred_test_1.predict\ndf_sub_1.to_csv('submission_GBM.csv', index=False)","6fd1d79f":"# define (distributed) random forest model\nfit_2 = H2ORandomForestEstimator(ntrees=50,\n                                   max_depth=50,\n                                   min_rows=5,\n                                   nfolds=5,\n                                   score_each_iteration=True,\n                                   stopping_metric='RMSE',\n                                   stopping_rounds=5,\n                                   stopping_tolerance=0.0001, # default 0.001\n                                   seed=999)\n\n# train model\nt1 = time.time()\nfit_2.train(x=predictors,\n              y=target,\n              training_frame=train_hex)\nt2 = time.time()\nprint('Elapsed time [s]: ', np.round(t2-t1,2))","f54b2a12":"# show cross validation metrics\nfit_2.cross_validation_metrics_summary()","b97ceffe":"# show scoring history - training vs cross validations\nfor i in range(n_cv):\n    cv_model_temp = fit_2.cross_validation_models()[i]\n    df_cv_score_history = cv_model_temp.score_history()\n    my_title = 'CV ' + str(1+i) + ' - Scoring History'\n    plt.scatter(df_cv_score_history.number_of_trees,\n                y=df_cv_score_history.training_rmse, \n                c='blue', label='training')\n    plt.scatter(df_cv_score_history.number_of_trees,\n                y=df_cv_score_history.validation_rmse, \n                c='darkorange', label='validation')\n    plt.title(my_title)\n    plt.xlabel('Number of Trees')\n    plt.ylabel('RMSE')\n    plt.legend()\n    plt.grid()\n    plt.show()","0a97731a":"# variable importance - basic version\nfit_2.varimp_plot(-1)","b6722a9f":"# variable importance using shap values => see direction as well as severity of feature impact\nt1 = time.time()\nfit_2.shap_summary_plot(train_hex);\nt2 = time.time()\nprint('Elapsed time [s]: ', np.round(t2-t1,2))","73134f77":"# predict on test set\npred_test_2 = fit_2.predict(test_hex).as_data_frame()","96b4a257":"# plot predictions - test set\nplt.hist(pred_test_2, bins=100)\nplt.title('Predictions DRF - Test')\nplt.grid()\nplt.show()","5b64fede":"pred_test_2.describe()","3e0625bf":"# submission\ndf_sub_2 = df_sub.copy()\ndf_sub_2.vol = pred_test_2.predict\ndf_sub_2.to_csv('submission_DRF.csv', index=False)","31df5c98":"# combine predictions in one data frame\ndf_preds_test = pd.DataFrame({'GBM': pred_test_1.predict, 'DRF': pred_test_2.predict})","f4ceb7e3":"# scatter plot of two prediction sets\nsns.jointplot(data=df_preds_test, x='GBM', y='DRF',\n              joint_kws={'s' : 10},\n              alpha=0.25)\nplt.show()","7af00929":"w = 0.6\npred_test_blend = w*pred_test_1 + (1-w)*pred_test_2\npred_test_blend.describe()","810f8863":"# submission\ndf_sub_3 = df_sub.copy()\ndf_sub_3.vol = pred_test_blend\ndf_sub_3.to_csv('submission_blend.csv', index=False)","23e52a85":"<a id='5'><\/a>\n# Fit GBM Model","d6bca648":"### Holiday (test vs train)","5e2ac53a":"### Categorical Features","294889cf":"### Categorical Features","b1ca72ae":"### Numerical Features","8824b44e":"### Apply model on test set","a75e795e":"### Numerical Features","59cb0088":"<a id='2'><\/a>\n# Target","f31dccba":"#### No missing values!","c7b65a0e":"#### Select training data subset:","9ca6f5dd":"#### Test set is 2019, training is 2012..2017 => need to check for potential trends over time!\n#### Test set has only month 1 to 8!","41b82857":"### Time series plots","fbdfbea8":"# Table of Contents\n* [Data Preparation](#1)\n* [Target](#2)\n* [Features](#3)\n* [Target vs Features](#4)\n* [Fit GBM Model](#5)\n* [Fit Random Forest Model](#6)\n* [Combine](#7)","004b792f":"<a id='4'><\/a>\n# Target vs Features","81fb25de":"### Flakes (test vs train)","9cff549e":"### Statement - clean up levels","ffebd7e0":"### Outlier","5f6ce223":"#### Plot each year separately:","26e9154f":"<a id='7'><\/a>\n# Combine","3eeb1378":"<a id='6'><\/a>\n# Fit Random Forest Model","5279ae39":"### Parse dates","6dbce6a7":"### Atmosphere - clean up levels","cf84b642":"<a id='1'><\/a>\n# Data Preparation","885fbf84":"#### Interesting gap in 2014\/2015...","3de289eb":"#### Correlations","4a445c9f":"<a id='3'><\/a>\n# Features"}}