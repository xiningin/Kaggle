{"cell_type":{"839017fd":"code","0f742adf":"code","6d6f4580":"code","6be8723a":"code","566859ce":"code","facfdae2":"code","31734cec":"code","ef6440d4":"code","209f2901":"code","3cd31161":"code","d4c4a8d1":"code","4769d30e":"code","cefb6f60":"code","ec6eefdd":"code","c68393fd":"code","ab86ac28":"code","3b3224b1":"code","5f9e79b1":"code","7547d0ee":"code","9e3fb6aa":"code","73add1d1":"code","e31aaa6c":"code","23623625":"code","6cb5f8cf":"code","247e1a81":"code","139abeaa":"code","0be2fd69":"code","e3660648":"code","9b73ab76":"code","1784efc3":"code","0e06f42c":"code","76beda74":"code","d872a82d":"code","5ef3d8f1":"code","64040740":"code","c888575e":"code","5a838024":"code","2b73acb1":"code","71e5c2d7":"code","1af69a5e":"code","3c302d07":"code","41ef7f02":"code","cc4a6972":"code","3e5b2eff":"code","448ca4d7":"code","5e4a64fc":"code","12a1c0f7":"code","cb60a0e9":"code","de9035db":"code","723be467":"code","ec083ee5":"code","98245540":"code","6f23aa9d":"code","d5567a64":"code","240f3387":"code","2314dc27":"code","b55dc935":"code","034e70bf":"code","58766f25":"code","09729773":"code","1a36db96":"code","b7bf59a2":"code","38c53718":"code","f78c00b6":"code","4baf1fed":"code","58cea353":"code","43accf9b":"code","c090b5d1":"code","5149e305":"code","b97e4b62":"code","6a0d856f":"code","59bfaf8a":"code","14355550":"code","6fa254f9":"code","43a0687f":"code","671226b7":"code","580a42d1":"code","8fd3a423":"code","6b8bfd62":"markdown","3fdf4602":"markdown","fb6b8156":"markdown","cc148587":"markdown","a1e9b9b0":"markdown","748378d2":"markdown","f0f19f61":"markdown","7ae5171b":"markdown","6aaee426":"markdown","4ab16d4e":"markdown","4e412d1d":"markdown","1c29e73d":"markdown","1440213b":"markdown","a720b9d0":"markdown","27472c15":"markdown","95e28357":"markdown","9f2e5486":"markdown","36bc4a43":"markdown","f306c45e":"markdown","fd90b92f":"markdown","4cace9e7":"markdown","354abd6f":"markdown","02d7df07":"markdown","6792087e":"markdown","9226975a":"markdown","c491328a":"markdown","a9b6c4f8":"markdown","8b484619":"markdown","fd46bbf0":"markdown","721bf76a":"markdown","895e5eea":"markdown","6b683ef2":"markdown","7fcb7ef9":"markdown","02779af7":"markdown","1b0d9f8c":"markdown","956124b9":"markdown","63cdd348":"markdown","1ecaf4e8":"markdown","2b6d65bd":"markdown","c88d9900":"markdown","cb0f2a36":"markdown","7c868717":"markdown","4d27eeed":"markdown","75eede70":"markdown","17cf40da":"markdown","7c961b05":"markdown","208dedf1":"markdown","aab34f5f":"markdown","a757c6fa":"markdown","9b5aabc3":"markdown","2a3fac2d":"markdown","0811fe65":"markdown","e6c0c312":"markdown","66cbf090":"markdown","d11b7f97":"markdown","5d70cfde":"markdown","28b4e4c3":"markdown","2b8a5d9d":"markdown","7cde1927":"markdown","e109a901":"markdown","83fd8157":"markdown","c5617f52":"markdown","55b7e340":"markdown","e7e3d347":"markdown","f09fc4f9":"markdown","bb7d23b8":"markdown","57e29526":"markdown","cf36664b":"markdown","993f4049":"markdown","6275ec40":"markdown","b7198fcf":"markdown","66cfa485":"markdown","a89987b5":"markdown","509bf224":"markdown","3057de87":"markdown","d6326555":"markdown","15de212c":"markdown","3b4c55c5":"markdown","5226cc73":"markdown","ea506943":"markdown","16137fad":"markdown","08b0c302":"markdown","10d887ba":"markdown","941fc9e7":"markdown","431da607":"markdown","06b8839e":"markdown","819187ce":"markdown","1071545e":"markdown","65c9bd85":"markdown","ee80d65c":"markdown","3cac7f32":"markdown","38c9eb7f":"markdown","65e0747a":"markdown"},"source":{"839017fd":"#Import basic libraries\nimport numpy as np\nimport pandas as pd \nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n\n%matplotlib inline\npd.set_option('display.max_columns',None)\nsns.set(style=\"darkgrid\", palette=\"pastel\", color_codes=True)\nsns.set_context('paper')\n\nimport plotly.graph_objs as go\nfrom plotly.subplots import make_subplots\nimport plotly.io as pio\npio.templates.default = \"plotly_dark\"\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.utils import resample \nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.feature_selection import RFE,SelectFromModel\nfrom sklearn.model_selection import KFold,GridSearchCV,RandomizedSearchCV,train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline \nfrom sklearn.naive_bayes import GaussianNB\nfrom scipy.stats import randint as sp_randint\nfrom sklearn.ensemble import RandomForestClassifier ,ExtraTreesClassifier, AdaBoostClassifier,GradientBoostingClassifier\nimport xgboost as xgb\nfrom xgboost import XGBClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import classification_report,confusion_matrix,f1_score,roc_auc_score,roc_curve,accuracy_score\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nimport plotly.figure_factory as ff \n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport random\nrandom.seed(10)\n\n#For displaying images in Kaggle\nfrom IPython.display import Image\nfrom pathlib import Path\ndata_dir = Path('..\/input\/images')\nimport os\nos.listdir(data_dir)","0f742adf":"# Load the data\ndf = pd.read_csv(\"..\/input\/default-of-credit-card-clients-dataset\/UCI_Credit_Card.csv\")\ndf.head(5)","6d6f4580":"df.isnull().sum()","6be8723a":"df.info()","566859ce":"df[['AGE','LIMIT_BAL', 'PAY_0','PAY_2', 'PAY_3', 'PAY_4', 'PAY_5', 'PAY_6', 'BILL_AMT1', 'BILL_AMT2',\n       'BILL_AMT3', 'BILL_AMT4', 'BILL_AMT5', 'BILL_AMT6', 'PAY_AMT1',\n       'PAY_AMT2', 'PAY_AMT3', 'PAY_AMT4', 'PAY_AMT5', 'PAY_AMT6']].describe()","facfdae2":"df[['SEX', 'EDUCATION', 'MARRIAGE']].describe()","31734cec":"df = df.rename(columns={'default.payment.next.month': 'default', \n                        'PAY_0': 'PAY_1'})","ef6440d4":"marriage_count = df['MARRIAGE'].value_counts().reset_index().rename(columns={'index':'index','MARRIAGE':'count'})","209f2901":"fig = go.Figure(go.Bar(\n    x = marriage_count['index'],y = marriage_count['count'],text=marriage_count['count'],marker={'color': marriage_count['count']}\n    ,textposition = \"outside\"))\nfig.update_layout(title_text='Count plot of marriage',xaxis_title=\"Marriage status\",yaxis_title=\"Number of count\")\nfig.show()","3cd31161":"edu_count = df['EDUCATION'].value_counts().reset_index().rename(columns={'index':'index','EDUCATION':'count'})","d4c4a8d1":"edu_count['index'][0] = 'University'\nedu_count['index'][1] = 'Graduate School'\nedu_count['index'][2] = 'High school'\nedu_count['index'][3] = 'Unknown 1'\nedu_count['index'][4] = 'Others'\nedu_count['index'][5] = 'Unknown 2'\nedu_count['index'][6] = 'Unknown 3'","4769d30e":"fig = go.Figure(go.Bar(\n    x = edu_count['index'],y = edu_count['count'],text = edu_count['count'],marker={'color': edu_count['count']}\n    ,textposition = \"outside\"))\nfig.update_layout(title_text='Count plot of education',xaxis_title=\"Education status\",yaxis_title=\"Number of count\")\nfig.show()","cefb6f60":"sex_count = df['SEX'].value_counts().reset_index().rename(columns={'index':'index','SEX':'count'})","ec6eefdd":"sex_count['index'][1] = 'Male'\nsex_count['index'][0] = 'Female'","c68393fd":"fig = go.Figure(go.Bar(\n    x = sex_count['index'],y = sex_count['count'],text=sex_count['count'],marker={'color': sex_count['count']}\n    ,textposition = \"outside\"))\nfig.update_layout(title_text='Count plot of gender',xaxis_title=\"Gender\",yaxis_title=\"Number of count\")\nfig.show()","ab86ac28":"bills = df[['BILL_AMT1','BILL_AMT2', 'BILL_AMT3', 'BILL_AMT4', 'BILL_AMT5', 'BILL_AMT6']]","3b3224b1":"fig = make_subplots(rows= 3, cols=2,subplot_titles=bills.columns.to_list())  \n\nfig.add_trace(go.Histogram(x= df[\"BILL_AMT1\"],name='BILL_AMT1'),row = 1, col = 1)\nfig.add_trace(go.Histogram(x= df[\"BILL_AMT2\"],name='BILL_AMT2'),row = 2, col = 2)\nfig.add_trace(go.Histogram(x= df[\"BILL_AMT3\"],name='BILL_AMT3'),row = 3, col = 1)\nfig.add_trace(go.Histogram(x= df[\"BILL_AMT4\"],name='BILL_AMT4'),row = 1, col = 2)\nfig.add_trace(go.Histogram(x= df[\"BILL_AMT5\"],name='BILL_AMT5'),row = 2, col = 1)\nfig.add_trace(go.Histogram(x= df[\"BILL_AMT6\"],name='BILL_AMT6'),row = 3, col = 2)\n\n\nfig.update_layout(height=600, width=800, title_text=\"Histogram Subplots of Bill Amount\")\nfig.show()","5f9e79b1":"pay = df[['PAY_AMT1','PAY_AMT2', 'PAY_AMT3', 'PAY_AMT4', 'PAY_AMT5', 'PAY_AMT6']]","7547d0ee":"fig = make_subplots(rows= 3, cols=2,subplot_titles=pay.columns.to_list())  \n\nfig.add_trace(go.Histogram(x= df[\"PAY_AMT1\"],name='PAY_AMT1'),row = 1, col = 1)\nfig.add_trace(go.Histogram(x= df[\"PAY_AMT2\"],name='PAY_AMT2'),row = 2, col = 2)\nfig.add_trace(go.Histogram(x= df[\"PAY_AMT3\"],name='PAY_AMT3'),row = 3, col = 1)\nfig.add_trace(go.Histogram(x= df[\"PAY_AMT4\"],name='PAY_AMT4'),row = 1, col = 2)\nfig.add_trace(go.Histogram(x= df[\"PAY_AMT5\"],name='PAY_AMT5'),row = 2, col = 1)\nfig.add_trace(go.Histogram(x= df[\"PAY_AMT6\"],name='PAY_AMT6'),row = 3, col = 2)\n\n\nfig.update_layout(height=600, width=800, title_text=\"Histogram Subplots of Amount of Previous Payment\")\nfig.show()","9e3fb6aa":"pay_s = df[['PAY_1','PAY_2', 'PAY_3', 'PAY_4', 'PAY_5', 'PAY_6']]","73add1d1":"df['PAY_1'].value_counts().index.to_list().sort()","e31aaa6c":"fig = make_subplots(rows= 3, cols=2,subplot_titles=pay_s.columns.to_list())  \n\nfig.add_trace(go.Histogram(x= df[\"PAY_1\"],name='PAY_1'),row = 1, col = 1)\nfig.add_trace(go.Histogram(x= df[\"PAY_2\"],name='PAY_2'),row = 2, col = 2)\nfig.add_trace(go.Histogram(x= df[\"PAY_3\"],name='PAY_3'),row = 3, col = 1)\nfig.add_trace(go.Histogram(x= df[\"PAY_4\"],name='PAY_4'),row = 1, col = 2)\nfig.add_trace(go.Histogram(x= df[\"PAY_5\"],name='PAY_5'),row = 2, col = 1)\nfig.add_trace(go.Histogram(x= df[\"PAY_6\"],name='PAY_6'),row = 3, col = 2)\n\n\nfig.update_layout(bargap=0.2,height=600, width=800, title_text=\"Histogram Subplots of Previous Payment Status\")\nfig.show()","23623625":"fig =  go.Figure(data=[go.Histogram(x= df[\"AGE\"])])\nfig.show()","6cb5f8cf":"fig =  go.Figure(data=[go.Histogram(x= df[\"LIMIT_BAL\"])])\nfig.show()","247e1a81":"target_count = df['default'].value_counts().reset_index().rename(columns={'index':'index','default':'count'})\nfig = go.Figure(go.Bar(\n    x = target_count['index'],y = target_count['count'],text=target_count['count'],marker={'color': target_count['count']}\n    ,textposition = \"outside\"))\nfig.update_layout(title_text='Count plot of defaulter',xaxis_title=\"Status of Defaulting\",yaxis_title=\"Number of count\")\nfig.show()","139abeaa":"plt.figure(figsize = (20,20))\nsns.heatmap(df.corr(),annot = True,square = True)","0be2fd69":"fil = (df.EDUCATION == 5) | (df.EDUCATION == 6) | (df.EDUCATION == 0)\ndf.loc[fil, 'EDUCATION'] = 4\ndf.EDUCATION.value_counts()","e3660648":"df.loc[df.MARRIAGE == 0, 'MARRIAGE'] = 3\ndf.MARRIAGE.value_counts()","9b73ab76":"fil = (df.PAY_1 == -2) | (df.PAY_1 == -1) | (df.PAY_1 == 0)\ndf.loc[fil, 'PAY_1'] = 0\nfil = (df.PAY_2 == -2) | (df.PAY_2 == -1) | (df.PAY_2 == 0)\ndf.loc[fil, 'PAY_2'] = 0\nfil = (df.PAY_3 == -2) | (df.PAY_3 == -1) | (df.PAY_3 == 0)\ndf.loc[fil, 'PAY_3'] = 0\nfil = (df.PAY_4 == -2) | (df.PAY_4 == -1) | (df.PAY_4 == 0)\ndf.loc[fil, 'PAY_4'] = 0\nfil = (df.PAY_5 == -2) | (df.PAY_5 == -1) | (df.PAY_5 == 0)\ndf.loc[fil, 'PAY_5'] = 0\nfil = (df.PAY_6 == -2) | (df.PAY_6 == -1) | (df.PAY_6 == 0)\ndf.loc[fil, 'PAY_6'] = 0","1784efc3":"fig = make_subplots(rows= 3, cols=2,subplot_titles=pay_s.columns.to_list())  \n\nfig.add_trace(go.Histogram(x= df[\"PAY_1\"],name='PAY_1'),row = 1, col = 1)\nfig.add_trace(go.Histogram(x= df[\"PAY_2\"],name='PAY_2'),row = 2, col = 2)\nfig.add_trace(go.Histogram(x= df[\"PAY_3\"],name='PAY_3'),row = 3, col = 1)\nfig.add_trace(go.Histogram(x= df[\"PAY_4\"],name='PAY_4'),row = 1, col = 2)\nfig.add_trace(go.Histogram(x= df[\"PAY_5\"],name='PAY_5'),row = 2, col = 1)\nfig.add_trace(go.Histogram(x= df[\"PAY_6\"],name='PAY_6'),row = 3, col = 2)\n\n\nfig.update_layout(bargap=0.2,height=600, width=800, title_text=\"Histogram Subplots of Previous Payment Status (After clubbing)\")\nfig.show()","0e06f42c":"def cross(Col1, Col2):\n    res = pd.crosstab(df[Col1], df[Col2])\n    #Calculating the percentage of defaulters\n    res['Percentage'] = round((res[res.columns[1]]\/(res[res.columns[0]] + res[res.columns[1]])) * 100,2)\n    print(res)\n    #Plotting a stack bar graph\n    fig = go.Figure(data=[\n    go.Bar(name='Non-Defaulters', x=res.index.to_list(),y=res[0]),\n    go.Bar(name='Defaulters', x=res.index.to_list(), y=res[1],text=(res['Percentage']),textposition = \"outside\")])\n    # Change the bar mode to stack\n    fig.update_layout(barmode='stack',title_text = res.index.name + \" variable v\/s target\" ,xaxis_title=res.index.name,yaxis_title=\"Number of count\")\n    fig.show()","76beda74":"cross(\"SEX\",\"default\")","d872a82d":"cross(\"EDUCATION\",\"default\")","5ef3d8f1":"cross(\"MARRIAGE\",\"default\")","64040740":"fig = go.Figure()\nfig.add_trace(go.Histogram(x = df[df['default'] == 0][\"AGE\"],marker_color=\"green\",name=\"Non-defaulters\"))\nfig.add_trace(go.Histogram(x = df[df['default'] == 1][\"AGE\"],marker_color=\"orange\",name=\"Defaulters\"))\n# Overlay both histograms\nfig.update_layout(barmode='overlay')\n# Reduce opacity to see both histograms\nfig.update_traces(opacity=0.75)\nfig.update_layout(title=\"Distribution of age of defaulters and non-defaulters\",xaxis_title=\"Percentage\",yaxis_title=\"Counts\")\nfig.show()","c888575e":"X = df.drop(['default','ID'], axis=1)  \ny = df['default']\n\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.3, stratify=y, random_state=42)","5a838024":"LR = LogisticRegression(random_state=0)\nLR.fit(X_train, y_train)\ny_pred = LR.predict(X_test)\nprint('Accuracy:', accuracy_score(y_pred,y_test))\n\ncv_scores = cross_val_score(LR, X, y, cv=5)\nprint()\nprint(classification_report(y_test, y_pred))\nprint()\nprint(\"Average 5-Fold CV Score: {}\".format(round(np.mean(cv_scores),4)),\n      \", Standard deviation: {}\".format(round(np.std(cv_scores),4)))","2b73acb1":"df[['SEX','MARRIAGE','EDUCATION']] = df[['SEX','MARRIAGE','EDUCATION']].astype('object')\n\n#One Hot encoding\ndf = pd.get_dummies(df)\ndf.head()","71e5c2d7":"X = df.drop(['default','ID'], axis=1)  \ny = df['default']\n\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.3, stratify=y, random_state=42)","1af69a5e":"# create the training df by remerging X_train and y_train\ndf_train = X_train.join(y_train)","3c302d07":"# Separate majority and minority classes\ndf_majority = df_train[df_train.default == 0]\ndf_minority = df_train[df_train.default == 1]\n\nprint(df_majority.default.count())\nprint(\"-----------\")\nprint(df_minority.default.count())\nprint(\"-----------\")\nprint(df_train.default.value_counts())","41ef7f02":"# Upsample minority class\ndf_minority_upsampled = resample(df_minority, \n                                 replace=True,     # sample with replacement\n                                 n_samples = 16355,    # to match majority class\n                                 random_state= 42) # reproducible results\n# Combine majority class with upsampled minority class\ndf_upsampled = pd.concat([df_majority, df_minority_upsampled])\n# Display new class counts\ndf_upsampled.default.value_counts()","cc4a6972":"# Downsample majority class\ndf_majority_downsampled = resample(df_majority, \n                                 replace=False,    # sample without replacement\n                                 n_samples=4645,     # to match minority class\n                                 random_state=587) # reproducible results\n# Combine minority class with downsampled majority class\ndf_downsampled = pd.concat([df_majority_downsampled, df_minority])\n# Display new class counts\ndf_downsampled.default.value_counts()","3e5b2eff":"sm = SMOTE(random_state=42)\nX_SMOTE, y_SMOTE = sm.fit_sample(X_train, y_train)\nprint(len(y_SMOTE))\nprint(y_SMOTE.sum())","448ca4d7":"def model_eval(algo, Xtrain,ytrain,Xtest,ytest):\n    from sklearn.metrics import confusion_matrix,accuracy_score,roc_auc_score,roc_curve,classification_report\n\n    algo.fit(Xtrain,ytrain)\n    y_pred = algo.predict(Xtrain)\n    y_train_prob = algo.predict_proba(Xtrain)[:,1]\n\n    #print('confusion matrix-train\\n',confusion_matrix(ytrain,y_pred))\n    print('Overall Train Accuracy',accuracy_score(ytrain,y_pred))\n    print('Train AUC Score',roc_auc_score(ytrain,y_train_prob))\n\n    y_test_pred = algo.predict(Xtest)\n    y_test_prob = algo.predict_proba(Xtest)[:,1]\n\n\n    #print('confusion matrix-test\\n',confusion_matrix(ytest,y_test_pred))\n    print('Overall Test Accuracy',accuracy_score(ytest,y_test_pred))\n    print('Test AUC Score',roc_auc_score(ytest,y_test_prob))\n    print('Classification Report of Test\\n',  classification_report(ytest, y_test_pred))\n    \n    \n    kf = KFold(n_splits = 5,shuffle = True,random_state = 42)\n    score=[]\n    for train_idx,test_idx in kf.split(Xtrain,ytrain):\n        xtrain_k,xtest_k = Xtrain.iloc[train_idx,:],Xtrain.iloc[test_idx,:]\n        ytrain_k,ytest_k = ytrain.iloc[train_idx],ytrain.iloc[test_idx]\n        algo.fit(xtrain_k,ytrain_k)\n        y_pred_k = algo.predict(xtest_k)\n        roc = roc_auc_score(ytest_k,y_pred_k)\n        score.append(roc)\n    print('K-Fold scores: %0.03f (+\/- %0.5f)' % (np.mean(score),np.var(score,ddof=1)))\n    \n    f,ax =  plt.subplots(1,2,figsize=(14,6))\n    #plt.figure(figsize=(6,4))\n    ConfMatrix = confusion_matrix(ytest,y_test_pred)\n    sns.heatmap(ConfMatrix,annot=True, cmap='YlGnBu', fmt=\"d\", \n            xticklabels = ['Non-default', 'Default'], \n            yticklabels = ['Non-default', 'Default'],linewidths=.5,ax = ax[0])\n    ax[0].set_ylabel('True label')\n    ax[0].set_xlabel('Predicted label')\n    ax[0].set_title('Confusion Matrix')\n\n    global fpr,tpr,thresholds\n    fpr,tpr,thresholds = roc_curve(ytest,y_test_prob)\n    ax[1].plot(fpr,tpr,color = 'r')\n    ax[1].plot(fpr,fpr,color = 'green')\n    ax[1].set_ylabel('TPR')\n    ax[1].set_xlabel('FPR')\n    ax[1].set_title('ROC Curve')\n    plt.show()","5e4a64fc":"xtrain_data = [X_train,df_upsampled.drop('default',axis = 1),df_downsampled.drop('default',axis = 1),X_SMOTE]\nytrain_data = [y_train,df_upsampled['default'],df_downsampled['default'],y_SMOTE]\nname = ['Normal Sampling' , 'Over Sampling' , 'Under Sampling' , 'SMOTE']\n\nfor i,j,k in zip(xtrain_data,ytrain_data,name):\n    global best_log\n    print('Data is ',k)\n    best_log = []\n    # Setup the hyperparameter grid, (not scaled data)\n    param_grid = {'C': np.logspace(-5, 8, 15)}\n    # Instantiate a logistic regression classifier\n    logreg = LogisticRegression()\n    # Instantiate the RandomizedSearchCV object\n    logreg_cv = RandomizedSearchCV(logreg, param_grid ,scoring = 'roc_auc', cv=5, random_state=0)\n    # Fit it to the data\n    logreg_cv.fit(i,j)\n    best_log.append(logreg_cv.best_params_)\n    # Print the tuned parameters and score\n    print(\"Tuned Logistic Regression Parameters: {}\".format(logreg_cv.best_params_))\n    print(\"_\" * 100)","12a1c0f7":"Image(filename=data_dir\/'cc1.PNG')","cb60a0e9":"param_log =[{'C': 3.727593720314938},{'C': 11787686.347935867},{'C': 0.05179474679231213},{'C': 31.622776601683793}]","de9035db":"xtrain_data = [X_train,df_upsampled.drop('default',axis = 1),df_downsampled.drop('default',axis = 1),X_SMOTE]\nytrain_data = [y_train,df_upsampled['default'],df_downsampled['default'],y_SMOTE]\nname = ['Normal Sampling' , 'Over Sampling' , 'Under Sampling' , 'SMOTE']\nindex = [0,1,2,3]\n\nfor i,j,k,l in zip(xtrain_data,ytrain_data,name,index):\n    print('Data is ',k,' And with hyper parameter ',param_log[l])\n    model_eval(LogisticRegression(**param_log[l],random_state= 42), i,j,X_test,y_test)\n    print(\"_\" * 60)","723be467":"xtrain_data = [X_train,df_upsampled.drop('default',axis = 1),df_downsampled.drop('default',axis = 1),X_SMOTE]\nytrain_data = [y_train,df_upsampled['default'],df_downsampled['default'],y_SMOTE]\nname = ['Normal Sampling' , 'Over Sampling' , 'Under Sampling' , 'SMOTE']\n\nfor i,j,k in zip(xtrain_data,ytrain_data,name):\n    print('Data is ',k)\n    ss = StandardScaler()\n    a = ss.fit_transform(i)\n    xts = pd.DataFrame(a,columns = i.columns)\n    b = ss.transform(X_test)\n    model_eval(GaussianNB(), xts,j,b,y_test)\n    print(\"_\" * 60)","ec083ee5":"xtrain_data = [X_train,df_upsampled.drop('default',axis = 1),df_downsampled.drop('default',axis = 1),X_SMOTE]\nytrain_data = [y_train,df_upsampled['default'],df_downsampled['default'],y_SMOTE]\nname = ['Normal Sampling' , 'Over Sampling' , 'Under Sampling' , 'SMOTE']\n\nfor i,j,k in zip(xtrain_data,ytrain_data,name):\n    global best_xgb\n    ss = StandardScaler()\n    a = ss.fit_transform(i)\n    xts = pd.DataFrame(a,columns = i.columns)\n    best_knn = []\n    print('Data is ',k) # Instantiate a KNN classifier: tree\n    knn = KNeighborsClassifier()\n    # Setup the parameters and distributions to sample from: param_dist\n    params = {'n_neighbors' : sp_randint(1,20),\n             'p': sp_randint(1,5)}\n    rsearch_knn = RandomizedSearchCV(knn, param_distributions = params, cv = 3, random_state = 3 , n_iter = 50,n_jobs = -1)\n    rsearch_knn.fit(xts,j)\n    best_knn.append(rsearch_knn.best_params_)\n    print(\"Tuned KNN Parameters: {}\".format(rsearch_knn.best_params_), \"for\",k)\n    print(\"_\" * 100)","98245540":"Image(filename=data_dir\/'CC2.PNG')","6f23aa9d":"param_knn = [{'n_neighbors': 17, 'p': 3},{'n_neighbors': 1, 'p': 4},{'n_neighbors': 19, 'p': 4},{'n_neighbors': 2, 'p': 1}]","d5567a64":"xtrain_data = [X_train,df_upsampled.drop('default',axis = 1),df_downsampled.drop('default',axis = 1),X_SMOTE]\nytrain_data = [y_train,df_upsampled['default'],df_downsampled['default'],y_SMOTE]\nname = ['Normal Sampling' , 'Over Sampling' , 'Under Sampling' , 'SMOTE']\nindex = [0,1,2,3]\n\nfor i,j,k,l in zip(xtrain_data,ytrain_data,name,index):\n    print('Data is ',k,' And with hyper parameter ',param_knn[l])\n    ss = StandardScaler()\n    a = ss.fit_transform(i)\n    xts = pd.DataFrame(a,columns = i.columns)\n    b = ss.transform(X_test)\n    model_eval(KNeighborsClassifier(**param_knn[l]), xts,j,b,y_test)\n    print(\"_\" * 60)","240f3387":"xtrain_data = [X_train,df_upsampled.drop('default',axis = 1),df_downsampled.drop('default',axis = 1),X_SMOTE]\nytrain_data = [y_train,df_upsampled['default'],df_downsampled['default'],y_SMOTE]\nname = ['Normal Sampling' , 'Over Sampling' , 'Under Sampling' , 'SMOTE']\n\nfor i,j,k in zip(xtrain_data,ytrain_data,name):\n    print('Data is ',k)\n    # Instantiate a Decision Tree classifier: tree\n    dtc = DecisionTreeClassifier(random_state = 42)\n\n    # Setup the parameters and distributions to sample from: param_dist\n    params = {'max_depth': sp_randint(2,20),\n             'min_samples_leaf':sp_randint(1,20),\n             'min_samples_split':sp_randint(2,40),\n             'criterion':['gini','entropy']}\n\n    # Instantiate the RandomizedSearchCV object: tree_cv\n    rsearch_dt = RandomizedSearchCV(dtc, param_distributions= params, cv = 5, scoring = 'roc_auc',n_iter = 100,n_jobs = -1)\n\n    # Fit it to the data\n    rsearch_dt.fit(i,j)\n    print(\"Tuned Decision Tree Parameters: {}\".format(rsearch_dt.best_params_), \"for\",name)\n    print(\"_\" * 100)    ","2314dc27":"Image(filename=data_dir\/'cc3.PNG')","b55dc935":"param_dt = [{'criterion': 'gini', 'max_depth': 6, 'min_samples_leaf': 12, 'min_samples_split': 32},\n           {'criterion': 'gini', 'max_depth': 19, 'min_samples_leaf': 3, 'min_samples_split': 15},\n           {'criterion': 'gini', 'max_depth': 4, 'min_samples_leaf': 17, 'min_samples_split': 28},\n           {'criterion': 'entropy', 'max_depth': 11, 'min_samples_leaf': 19, 'min_samples_split': 18}]","034e70bf":"xtrain_data = [X_train,df_upsampled.drop('default',axis = 1),df_downsampled.drop('default',axis = 1),X_SMOTE]\nytrain_data = [y_train,df_upsampled['default'],df_downsampled['default'],y_SMOTE]\nname = ['Normal Sampling' , 'Over Sampling' , 'Under Sampling' , 'SMOTE']\nindex = [0,1,2,3]\n\nfor i,j,k,l in zip(xtrain_data,ytrain_data,name,index):\n    print('Data is ',k,' And with hyper parameter ',param_dt[l])\n    model_eval(DecisionTreeClassifier(**param_dt[l],random_state= 42), i,j,X_test,y_test)\n    print(\"_\" * 60)","58766f25":"xtrain_data = [X_train,df_upsampled.drop('default',axis = 1),df_downsampled.drop('default',axis = 1),X_SMOTE]\nytrain_data = [y_train,df_upsampled['default'],df_downsampled['default'],y_SMOTE]\nname = ['Normal Sampling' , 'Over Sampling' , 'Under Sampling' , 'SMOTE']\n\nfor i,j,k in zip(xtrain_data,ytrain_data,name):\n    global best_rf\n    best_rf =[]\n    print('Data is ',k)\n    # Instantiate a Random forest classifier: tree\n    rfc = RandomForestClassifier(random_state = 42)\n\n    # Setup the parameters and distributions to sample from: param_dist\n    params = {'n_estimators' : sp_randint(50,200),\n              'max_features' : sp_randint(1,24),'max_depth': sp_randint(2,10),\n             'min_samples_leaf':sp_randint(1,20),\n             'min_samples_split':sp_randint(2,20),\n             'criterion':['gini','entropy']}\n\n    # Instantiate the RandomizedSearchCV object\n    rsearch_rfc = RandomizedSearchCV(rfc, param_distributions= params, cv = 5, scoring = 'roc_auc',n_iter = 200,random_state = 42,n_jobs = -1,return_train_score = True)\n\n    # Fit it to the data\n    rsearch_rfc.fit(i,j)\n    best_rf.append(rsearch_rfc.best_params_)\n    print(\"Tuned Random Tree Parameters: {}\".format(rsearch_rfc.best_params_), \"for\",k)\n    print(\"_\" * 100)","09729773":"Image(filename=data_dir\/'cc4.PNG')","1a36db96":"param_rf = [{'criterion': 'entropy', 'max_depth': 9, 'max_features': 19, 'min_samples_leaf': 7, 'min_samples_split': 7, 'n_estimators': 183},\n           {'criterion': 'entropy', 'max_depth': 9, 'max_features': 22, 'min_samples_leaf': 1, 'min_samples_split': 12, 'n_estimators': 162},\n           {'criterion': 'gini', 'max_depth': 9, 'max_features': 14, 'min_samples_leaf': 16, 'min_samples_split': 15, 'n_estimators': 164},\n           {'criterion': 'entropy', 'max_depth': 9, 'max_features': 15, 'min_samples_leaf': 2, 'min_samples_split': 11, 'n_estimators': 179}]","b7bf59a2":"xtrain_data = [X_train,df_upsampled.drop('default',axis = 1),df_downsampled.drop('default',axis = 1),X_SMOTE]\nytrain_data = [y_train,df_upsampled['default'],df_downsampled['default'],y_SMOTE]\nname = ['Normal Sampling' , 'Over Sampling' , 'Under Sampling' , 'SMOTE']\nindex = [0,1,2,3]\n\nfor i,j,k,l in zip(xtrain_data,ytrain_data,name,index):\n    print('Data is ',k,' And with hyper parameter ',param_dt[l])\n    model_eval(RandomForestClassifier(**param_rf[l],random_state= 42), i,j,X_test,y_test)\n    print(\"_\" * 60)","38c53718":"xtrain_data = [X_train,df_upsampled.drop('default',axis = 1),df_downsampled.drop('default',axis = 1),X_SMOTE]\nytrain_data = [y_train,df_upsampled['default'],df_downsampled['default'],y_SMOTE]\nname = ['Normal Sampling' , 'Over Sampling' , 'Under Sampling' , 'SMOTE']\n\nfor i,j,k in zip(xtrain_data,ytrain_data,name):\n    global best_ada\n    best_ada =[]\n    print('Data is ',k)\n    # Instantiate a Ada Boost classifier\n    ada = AdaBoostClassifier()\n\n    #Creating a grid of hyperparameters\n    param_grid = {'n_estimators': [200,300],\n                  'algorithm': ['SAMME', 'SAMME.R'],\n                  'learning_rate' : [0.5, 0.75, 1.0]}\n\n    #Building a 5 fold CV GridSearchCV object\n    grid_ada = GridSearchCV(ada, param_grid, scoring = 'accuracy', cv=5,n_jobs = -1)\n\n    #Fitting the grid to the training data\n    grid_ada.fit(i,j)\n    best_ada.append(grid_ada.best_params_)\n    print(\"Tuned Ada Boost Parameters: {}\".format(grid_ada.best_params_), \"for\",k)\n    print(\"_\" * 100)","f78c00b6":"Image(filename=data_dir\/'cc5.PNG')","4baf1fed":"param_ada = [{'algorithm': 'SAMME', 'learning_rate': 0.5, 'n_estimators': 300},\n            {'algorithm': 'SAMME.R', 'learning_rate': 1.0, 'n_estimators': 300},\n            {'algorithm': 'SAMME.R', 'learning_rate': 0.5, 'n_estimators': 200},\n            {'algorithm': 'SAMME.R', 'learning_rate': 1.0, 'n_estimators': 300}]","58cea353":"xtrain_data = [X_train,df_upsampled.drop('default',axis = 1),df_downsampled.drop('default',axis = 1),X_SMOTE]\nytrain_data = [y_train,df_upsampled['default'],df_downsampled['default'],y_SMOTE]\nname = ['Normal Sampling' , 'Over Sampling' , 'Under Sampling' , 'SMOTE']\nindex = [0,1,2,3]\n\nfor i,j,k,l in zip(xtrain_data,ytrain_data,name,index):\n    print('Data is ',k,' And with hyper parameter ',param_dt[l])\n    model_eval(AdaBoostClassifier(**param_ada[l]), i,j,X_test,y_test)\n    print(\"_\" * 60)","43accf9b":"xtrain_data = [X_train,df_upsampled.drop('default',axis = 1),df_downsampled.drop('default',axis = 1),X_SMOTE]\nytrain_data = [y_train,df_upsampled['default'],df_downsampled['default'],y_SMOTE]\nname = ['Normal Sampling' , 'Over Sampling' , 'Under Sampling' , 'SMOTE']\n\nfor i,j,k in zip(xtrain_data,ytrain_data,name):\n    global best_gbc\n    best_gbc =[]\n    print('Data is ',k)\n    # Instantiate a Gradient Boost classifier\n    gbc = GradientBoostingClassifier()\n\n    #Creating a grid of hyperparameters\n    param_grid = {'n_estimators': [200,300],\n                  'learning_rate' : [0.5, 0.75, 1.0]}\n\n    #Building a 5 fold CV GridSearchCV object\n    grid_gbc = GridSearchCV(gbc, param_grid, scoring = 'accuracy', cv=5,n_jobs = -1)\n\n    #Fitting the grid to the training data\n    grid_gbc.fit(i,j)\n    best_gbc.append(grid_gbc.best_params_)\n    print(\"Tuned Random Tree Parameters: {}\".format(grid_gbc.best_params_), \"for\",k)\n    print(\"_\" * 100)","c090b5d1":"Image(filename=data_dir\/'cc6.PNG')","5149e305":"param_gbc = [{'learning_rate': 0.5, 'n_estimators': 200},\n            {'learning_rate': 1.0, 'n_estimators': 300},\n            {'learning_rate': 0.5, 'n_estimators': 200},\n            {'learning_rate': 0.5, 'n_estimators': 200}]","b97e4b62":"for i,j,k,l in zip(xtrain_data,ytrain_data,name,index):\n    print('Data is ',k,' And with hyper parameter ',param_dt[l])\n    model_eval(GradientBoostingClassifier(**param_gbc[l]), i,j,X_test,y_test)\n    print(\"_\" * 60)","6a0d856f":"xtrain_data = [X_train,df_upsampled.drop('default',axis = 1),df_downsampled.drop('default',axis = 1),X_SMOTE]\nytrain_data = [y_train,df_upsampled['default'],df_downsampled['default'],y_SMOTE]\nname = ['Normal Sampling' , 'Over Sampling' , 'Under Sampling' , 'SMOTE']\n\nfor i,j,k in zip(xtrain_data,ytrain_data,name):\n    global best_xgb\n    best_xgb =[]\n    print('Data is ',k)  # Instantiate a XGBoost classifier\n    xgb= XGBClassifier()\n\n    param_grid = {\"learning_rate\"    : [0.05, 0.10, 0.15, 0.20, 0.25, 0.30] ,\n                  \"max_depth\"        : [3, 4, 5, 6, 8, 10, 12, 15],\n                 \"min_child_weight\" : [1, 3, 5, 7],\n                 \"gamma\"            : [0.0, 0.1, 0.2 , 0.3, 0.4],\n                 \"colsample_bytree\" : [0.3, 0.4, 0.5 , 0.7] }\n\n    #Building a 5 fold CV GridSearchCV object\n    xgb_RS = RandomizedSearchCV(xgb, param_grid ,cv = 5, scoring = 'roc_auc',n_iter = 200,n_jobs = -1)\n\n    #Fitting the grid to the training data\n    xgb_RS.fit(i,j)\n    best_xgb.append(xgb_RS.best_params_)\n    print(\"Tuned XG Boost Parameters: {}\".format(xgb_RS.best_params_), \"for\",k)\n    print(\"_\" * 100)","59bfaf8a":"Image(filename=data_dir\/'cc7.PNG')","14355550":"param_xgb = [{'min_child_weight': 7, 'max_depth': 5, 'learning_rate': 0.05,\n                            'gamma': 0.2, 'colsample_bytree': 0.5},\n            {'min_child_weight': 1, 'max_depth': 15, 'learning_rate': 0.25,\n                            'gamma': 0.0, 'colsample_bytree': 0.3},\n            {'min_child_weight': 5, 'max_depth': 4, 'learning_rate': 0.05,\n                            'gamma': 0.2, 'colsample_bytree': 0.4},\n            {'min_child_weight': 1, 'max_depth': 15, 'learning_rate': 0.1,\n                            'gamma': 0.0, 'colsample_bytree': 0.4}]","6fa254f9":"for i,j,k,l in zip(xtrain_data,ytrain_data,name,index):\n    print('Data is ',k,' And with hyper parameter ',param_dt[l])\n    model_eval(XGBClassifier(**param_xgb[l]), i,j,X_test,y_test)\n    print(\"_\" * 60)","43a0687f":"dic = {'Model':['XGboost','Random forest','Ada Boost','Gradient Boost','Decision Tree'],\n       'Data':['SMOTE','SMOTE','SMOTE','SMOTE','Under Sampling'],\n       'Accuracy':[0.76,0.81,0.81,0.80,0.78],\n       'K-Fold score':[0.71,0.85,0.86,0.85,0.70],\n       'AUC-ROC Score':[0.78,0.77,0.76,0.74,0.74]}\nscore_df = pd.DataFrame(dic)\nfig =  ff.create_table(score_df)\nfig.show()","671226b7":"def plot_Feature(feature, clf):\n    tmp = pd.DataFrame({'Feature': feature, \n                        'Feature importance': clf.feature_importances_})\n    tmp = tmp.sort_values(by='Feature importance',ascending=False)\n    fig = go.Figure(go.Bar(\n    x=tmp['Feature'],y=tmp['Feature importance'],marker={'color': tmp['Feature importance'], \n    'colorscale': 'Viridis'}\n    ))\n    fig.update_layout(title_text='Feature Importance',xaxis_title=\"Feature\",yaxis_title=\"Importance\")\n    return fig.show()","580a42d1":"#BEST MODEL with BEST PARAMETERS\nparam_r = {'criterion': 'entropy', 'max_depth': 11, 'min_samples_leaf': 19, 'min_samples_split': 18}\nrfc_s = RandomForestClassifier(**param_r,random_state = 42).fit(X_SMOTE,y_SMOTE)","8fd3a423":"plot_Feature(X_SMOTE.columns,rfc_s)","6b8bfd62":"#### Let's first find out the best parameters for all the datasets:\n\n#### It's always recommended to scale the data before using distance based algorithm to make the model not biased","3fdf4602":"# 3) Baseline model:","fb6b8156":"#### The best score for KNN is registered when trained with the under sampling data with an Test AUC score of 0.74 and train K-Fold score of 0.68","cc148587":"#### Random undersampling involves randomly selecting examples from the majority class to delete from the training dataset.\n\n#### This has the effect of reducing the number of examples in the majority class in the transformed version of the training dataset. This process can be repeated until the desired class distribution is achieved, such as an equal number of examples for each class.\n\n#### This approach may be more suitable for those datasets where there is a class imbalance although a sufficient number of examples in the minority class, such a useful model can be fit.","a1e9b9b0":"#### A) Categorical features:","748378d2":"# 5) Model Building:","f0f19f61":"### Statistical description of the features:","7ae5171b":"##### 2) Education:","6aaee426":"#### Education v\/s Defaulters:","4ab16d4e":"#### The best score for Adaboost Classifier is registered when trained with the SMOTE data with an Test AUC score of 0.764 and train K-Fold score of 0.86","4e412d1d":"#### Let's rename the target column to \"default\" and payment status \"PAY_0\" to \"PAY_1\" to make it continous","1c29e73d":"##### The distribution is almost uniformly distributed but a bit right skewed for the defaulters. But it shows that old age people are almost non-defaulters","1440213b":"# 4) Feature Engineering","a720b9d0":"### i) Logistic Regression:","27472c15":"### iii) K-Nearest Neighbours: ","95e28357":"### For further exploration, let's do bivariate analysis of all the features with the target variable:","9f2e5486":"#### B) Numerical variables:","36bc4a43":"#### So our baseline model has an accuracy of 0.77. We have to better this with other models","f306c45e":"# 6) Conclusion:","fd90b92f":"#### Let's first find out the best parameters for all the datasets:","4cace9e7":"### I have deployed the model with the best score using the [TabPy](https:\/\/github.com\/tableau\/TabPy) library in Tableau.\n\n#### TabPy (the Tableau Python Server) is an Analytics Extension implementation which expands Tableau's capabilities by allowing users to execute Python scripts and saved functions via Tableau's table calculations.\n\n#### I will explain the complete procedure of deployment in another notebook. Go ahead and check out how the model works @\n\nhttps:\/\/drive.google.com\/file\/d\/1VbmfYtvsp_anRGf4KIFmjSIHCHQbfZop\/view?usp=sharing","354abd6f":"#### All the bill amounts are highly right skewed, so transformation might be required!","02d7df07":"#### Let's first find out the best parameters for all the datasets:","6792087e":"### The models are performing well for SMOTE data and also undersampled data rather than imbalanced data. Thus before modeling resampling techniques should be used to make sure that the models are not biased.\n\n### Let's see the top 5 most scoring models","9226975a":"# 1) Exploratory Data Anaysis:","c491328a":"##### The 0 in MARRIAGE can be safely categorized as 'Other' (thus 3)","a9b6c4f8":"### viii) XGBoost Classifier","8b484619":"##### It's an interesting correlation plot. The heatmat shows that features are correlated with each other (collinearity), such us like PAY_0,2,3,4,5,6 and BILL_AMT1,2,3,4,5,6. In those cases, the correlation is positive.","fd46bbf0":"##### 1) Numerical features:","721bf76a":"#### Now let's check dtypes of each feature","895e5eea":"##### 3) Sex","6b683ef2":"#### The upsample has the disadvantage of increasing the likelihood of overfitting since it replicates the minority class event. It usually outperform the downsampling. The downsample can discard potentially useful information and the sample can be biased, but it helps improving the run time\n\n#### SMOTE works by selecting examples that are close in the feature space, drawing a line between the examples in the feature space and drawing a new sample at a point along that line. To create a syntetic sample I want to use the SMOTE algorithm, which is an oversampling method which creates syntetic samples from the minority class instead of creating copies. It selects 2 or more similar instances and perturb them one at a time by random amount. This techniques should avoid overfitting problems but it risks adding noise to the model","7fcb7ef9":"## If you like the notebook, do upvote it!\n\n## For any doubts, Do follow me @ [Suraj RP](https:\/\/www.linkedin.com\/in\/suraj-rp\/)","02779af7":"##### 1) Marriage:","1b0d9f8c":"#### Correlation:","956124b9":"##### The 0 (undocumented), 5 and 6 (label unknown) in EDUCATION can also be put in a 'Other' cathegory (thus 4)","63cdd348":"#### Let's first find out the best parameters for all the datasets:","1ecaf4e8":"#### The best AUC score for Logistic Regression is seen in Under sampling data with an Test AUC score of 0.65 and K-fold score of 0.615","2b6d65bd":"##### Looking at the distribution, it's clear that there are more people who are aged between 20-30 and it goes on decreasing after 30. It is also a bit right skewed","c88d9900":"#### There's a lot of imbalance in the sample. So there are many ways to resampling!\n\n#### Resampling involves creating a new transformed version of the training dataset in which the selected examples have a different class distribution. This is a simple and effective strategy for imbalanced classification problems.\n\n#### The simplest strategy is to choose examples for the transformed dataset randomly, called random resampling. There are two main approaches to random resampling for imbalanced classification; they are oversampling and undersampling.\n\n#### A) Random Oversampling: Randomly duplicate examples in the minority class.\n#### B) Random Undersampling: Randomly delete examples in the majority class.\n#### C) SMOTE: Synthetic Minority Oversampling Technique\n\n#### Let's look at them, one by one\n","cb0f2a36":"##### 2) Categorical features:","7c868717":"# 7) Deployment:","4d27eeed":"##### There are a lot of customers who pay he previous amount duly but there are only some of them who pay late","75eede70":"##### 2) Previous payment","17cf40da":"#### Let's first find out the best parameters for all the datasets:","7c961b05":"#### Well it looks like there are no missing values which skips one of the main parts of pre-processing i.e. missing value imputation","208dedf1":"#### The best score for Gradient Boost Classifier is registered when trained with the SMOTE data with an Test AUC score of 0.747 and train K-Fold score of 0.85 after it was standardized","aab34f5f":"# Introduction\n\n## About the dataset:\nThis dataset contains information on default payments, demographic factors, credit data, history of payment, and bill statements of credit card clients in Taiwan from April 2005 to September 2005. \n\n## Features:\nThere are 25 features:\n\n* ID: ID of each client\n* LIMIT_BAL: Amount of given credit in NT dollars (includes individual and family\/supplementary credit\n* SEX: Gender (1=male, 2=female)\n* EDUCATION: (1=graduate school, 2=university, 3=high school, 4=others, 5=unknown, 6=unknown)\n* MARRIAGE: Marital status (1=married, 2=single, 3=others)\n* AGE: Age in years\n* PAY_0: Repayment status in September, 2005 (-1=pay duly, 1=payment delay for one month, 2=payment delay for two months, ... 8=payment delay for eight months, 9=payment delay for nine months and above)\n* PAY_2: Repayment status in August, 2005 (scale same as above)\n* PAY_3: Repayment status in July, 2005 (scale same as above)\n* PAY_4: Repayment status in June, 2005 (scale same as above)\n* PAY_5: Repayment status in May, 2005 (scale same as above)\n* PAY_6: Repayment status in April, 2005 (scale same as above)\n* BILL_AMT1: Amount of bill statement in September, 2005 (NT dollar)\n* BILL_AMT2: Amount of bill statement in August, 2005 (NT dollar)\n* BILL_AMT3: Amount of bill statement in July, 2005 (NT dollar)\n* BILL_AMT4: Amount of bill statement in June, 2005 (NT dollar)\n* BILL_AMT5: Amount of bill statement in May, 2005 (NT dollar)\n* BILL_AMT6: Amount of bill statement in April, 2005 (NT dollar)\n* PAY_AMT1: Amount of previous payment in September, 2005 (NT dollar)\n* PAY_AMT2: Amount of previous payment in August, 2005 (NT dollar)\n* PAY_AMT3: Amount of previous payment in July, 2005 (NT dollar)\n* PAY_AMT4: Amount of previous payment in June, 2005 (NT dollar)\n* PAY_AMT5: Amount of previous payment in May, 2005 (NT dollar)\n* PAY_AMT6: Amount of previous payment in April, 2005 (NT dollar)\n* default.payment.next.month: Default payment (1=yes, 0=no)\n\n## Overview:\n\nThis is one of my early data science projects. I've used my skills like data cleaning, data analysis, data visualization and machine learning. I hope to carry out a research and to build several ML models to predict defaulters of credit card payments and compare their performance.\n\nThis notebook consisits of the following parts:\n\n1) Exploratory Data Analysis\n\n2) Data cleaning and further exploration\n\n3) Building a base model\n\n4) Feature Engineering\n\n5) Building ML model using various algorithms and comparing them\n\n6) Conclusion\n\n7) Deployment\n","a757c6fa":"#### Let's see if the data has any missing values","9b5aabc3":"##### There's not much interesting insights available from the cross tab analysis apart from there is quite less defaulters who has lower than high school level eductation and married people tend to default more!","2a3fac2d":"##### 5) Amount given in credit:","0811fe65":"#### Age distribution of Defaulters:","e6c0c312":"#### The best score for Decision Tree is registered when trained with the under sampling data with an Test AUC score of 0.747 and train K-Fold score of 0.70","66cbf090":"##### More of the people have taken credit between 10k to 400k. Probabaly we can group people as per their taken credit. We will see that in feature engineering!","d11b7f97":"#### Gender v\/s Defaulters:","5d70cfde":"### A) Random Oversampling:","28b4e4c3":"### vi) AdaBoost Classifier","2b8a5d9d":"##### 1) Bill amount:","7cde1927":"##### Education is has label 0 ,label 5 & label 6, which is undocumented. Also most of the customers are well educated!","e109a901":"### It looks like the repayment status and repayment amount are more important features!","83fd8157":"#### Let's first find out the best parameters for all the datasets:","c5617f52":"#####  The data has not a large unbalance with respect of the target value (default).","55b7e340":"##### To do that let's define a function which gives out a cross tab and visualization:","e7e3d347":"#### A baseline model that uses heuristics, simple summary statistics, randomness, or machine learning to create predictions for a dataset. You can use these predictions to measure the baseline's performance (e.g., accuracy)-- this metric will then become what you compare any other machine learning algorithm against. So let's build a logistic regression without any hyperparameter tuning or feature engineering","f09fc4f9":"# 2) Data cleaning and further exploration:","bb7d23b8":"#### C) Target column (Default):","57e29526":"#### The best score for Random forest is registered when trained with the SMOTE data with an Test AUC score of 0.77 and train K-Fold score of 0.85","cf36664b":"#### We have observed that there is imbalance in the samples. Let's see the distribution of the samples in the train dataset","993f4049":"### Let's see the feature importance of our best model by defining a function","6275ec40":"#### Our final dataset is finished, let's split the dataset in 70:30 ratio for train and test","b7198fcf":"### vii) GradientBoosting Classifier","66cfa485":"#### I have pointed out earlier that there are some undocumented labels. Now I will try to clean them up!","a89987b5":"#### Marriage v\/s Defaulters:","509bf224":"#### I will be using the following algorithms with the 4 different datasets and also with hyperparameter tuning the models\n\n### i) Logistic Regression\n### ii) Naive Bayes\n### iii) K-Nearest Neighbours\n### iv) Decision Tree\n### v) Random Forest\n### vi) AdaBoost Classifier\n### vii) GradientBoosting Classifier\n### viii) XGBoost Classifier","3057de87":"#### Let's first find out the best parameters for all the datasets:","d6326555":"### iv) Decision Tree","15de212c":"##### Marriage has label 0, which is undocumented","3b4c55c5":"### ii) Naive bayes:","5226cc73":"#### So we have 4 sets of training data:\n\n#### 1) The normal train data with unbalance\n#### 2) Train data with oversampling\n#### 3) Train data with undersampling\n#### 4) Train data with SMOTE algorithm\n\n#### The evaluation will be based on K fold cross validation of AUC (Area Under The Curve) ROC (Receiver Operating Characteristics) curve score which is considered as the best metric for binary classification and also the test score\n\n#### AUC - ROC curve is a performance measurement for classification problem at various thresholds settings. ROC is a probability curve and AUC represents degree or measure of separability. It tells how much model is capable of distinguishing between classes. Higher the AUC, better the model is at predicting 0s as 0s and 1s as 1s. By analogy, Higher the AUC, better the model is at distinguishing between patients with disease and no disease. The ROC curve is plotted with TPR against the FPR where TPR is on y-axis and FPR is on the x-axis.\n\n#### First let's define a function which evaluates the model with train and test score and also performs a K-fold cross validation on the train dataset","ea506943":"#### The best score for XG Boost Classifier is registered when trained with the under sampling data with an Test AUC score of 0.78 and train K-Fold score of 0.716 after it was standardized","16137fad":"#### To begin with, let's one hot encode all the categorical features. So I will convert them into datatype \"object\"","08b0c302":"### B) Random Undersampling:","10d887ba":"#### The best score for Naive Bayes is registered when trained with the under sampling data with an Test AUC score of 0.74 and train K-Fold score of 0.70 after it was standardized","941fc9e7":"#### Random oversampling involves randomly duplicating examples from the minority class and adding them to the training dataset.\n\n#### Examples from the training dataset are selected randomly with replacement. This means that examples from the minority class can be chosen and added to the new \u201cmore balanced\u201d training dataset multiple times; they are selected from the original training dataset, added to the new training dataset, and then returned or \u201creplaced\u201d in the original dataset, allowing them to be selected again.","431da607":"##### 3) Previous payment status:","06b8839e":"#### Also all the prevoius bill amounts are highly right skewed, so transformation might be required!","819187ce":"##### Histogram after clubbing \"-2\" and \"-1\" to \"0\" label","1071545e":"### C) SMOTE: Synthetic Minority Oversampling Technique","65c9bd85":"## PS: I recommend not to use KAGGLE for finding best parameters as it takes more time. So I have run it in my local machine and posted the snips of it :)","ee80d65c":"##### \"-1\" is paid duly, but there are \"-2\" and \"0\" labels in payment status variable. So let's combine them and put everything as \"0\"","3cac7f32":"### v) Random Forest","38c9eb7f":"##### 4) Age:","65e0747a":"##### It seems that there are more number of female customers"}}