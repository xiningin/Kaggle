{"cell_type":{"afa2f7fe":"code","c02bebc0":"code","76aa6842":"code","289d78f0":"code","3d3cfe2e":"code","81f323a3":"code","1e193e2f":"code","b32e9c4a":"code","f948c1ab":"code","2680c238":"code","6a88838f":"code","899ed7ef":"code","7fa978ac":"code","9ba61ab3":"code","77faed41":"code","88920221":"code","ba080d0c":"code","9b0fbc75":"code","cb1185f9":"code","aaf1578f":"code","0f469b23":"code","feac3ea0":"code","da72e795":"markdown","7f4589e3":"markdown","15b21c4d":"markdown","5a70649e":"markdown","931e64fb":"markdown","82d9ada1":"markdown","36d6c060":"markdown","fb4b1fff":"markdown","4e716475":"markdown","2a069d01":"markdown","c2a35334":"markdown","e96f3591":"markdown","96cdb3b7":"markdown","c7677d65":"markdown","37741718":"markdown","ba184ead":"markdown","c6a4de9f":"markdown","182c7046":"markdown","2934efe4":"markdown","68605268":"markdown","ef6f8e14":"markdown"},"source":{"afa2f7fe":"!pip install -q tensorflow-addons --user","c02bebc0":"!git clone --depth 1 https:\/\/github.com\/google-research\/big_transfer","76aa6842":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nuser_credential = user_secrets.get_gcloud_credential()\nuser_secrets.set_tensorflow_credential(user_credential)","289d78f0":"import sys\nsys.path.append(\"big_transfer\")\n\nfrom bit_tf2 import models\nimport matplotlib.pyplot as plt\nimport tensorflow_addons as tfa\nimport tensorflow as tf\nimport numpy as np\n\nimport crop_resize\nimport re","3d3cfe2e":"try: # Detect TPUs\n    tpu = None\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver() # TPU detection\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.TPUStrategy(tpu)\nexcept ValueError: # Detect GPUs\n    strategy = tf.distribute.MirroredStrategy() \n\nprint(\"Number of accelerators: \", strategy.num_replicas_in_sync)","81f323a3":"AUTO                  = tf.data.AUTOTUNE \n\n# Comes from Table 4 and \"Training setup\" section. \nEPOCHS                = 300\nTEMPERATURE           = 2\nINIT_LR               = 0.01 \nWEIGHT_DECAY          = 1e-5\nCLIP_THRESHOLD        = 1.0\n\nBATCH_SIZE            = 64 * strategy.num_replicas_in_sync\nRESIZE                = 128","1e193e2f":"# We need this to derive the steps per epoch.\n# This comes from this repository https:\/\/github.com\/GoogleCloudPlatform\/training-data-analyst.\ndef count_data_items(filenames):\n    n = [int(re.compile(r\"-([0-9]*)\\.\").search(filename).group(1)) for filename in filenames]\n    return np.sum(n)\n\ntrain_pattern = \"gs:\/\/funmatch-tf\/train\/*.tfrec\"\ntrain_filenames = tf.io.gfile.glob(train_pattern)\nval_pattern = \"gs:\/\/funmatch-tf\/validation\/*.tfrec\"\nval_filenames = tf.io.gfile.glob(val_pattern)\ntest_pattern = \"gs:\/\/funmatch-tf\/test\/*.tfrec\"\ntest_filenames = tf.io.gfile.glob(test_pattern)\n\nDATASET_NUM_TRAIN_EXAMPLES = count_data_items(train_filenames)","b32e9c4a":"print(f\"Global batch size: {BATCH_SIZE}.\")\nprint(f\"Total training examples: {count_data_items(train_filenames)}.\")\nprint(f\"Total validation examples: {count_data_items(val_filenames)}.\")\nprint(f\"Total testing examples: {count_data_items(test_filenames)}.\")","f948c1ab":"# Instead of sampling mixup alpha from a Beta distribution,\n# the authors introduce a more aggressive variant by sampling\n# from a uniform distribution with [0, 1].\ndef mixup(images, labels):\n    alpha = tf.random.uniform([], 0, 1)\n    mixedup_images = (alpha * images +\n                     (1 - alpha) * tf.reverse(images, axis=[0]))\n    return mixedup_images, labels","2680c238":"# Function to read the TFRecords, segregate the images and labels.\ndef read_tfrecord(example, train):\n    features = {\n        \"image\": tf.io.FixedLenFeature([], tf.string), \n        \"class\": tf.io.FixedLenFeature([], tf.int64)\n    }\n    \n    example = tf.io.parse_single_example(example, features)\n    image = tf.image.decode_jpeg(example[\"image\"], channels=3)\n    image = tf.cast(image, tf.float32) \/ 255.0  \n    \n    if train:\n        # Apply \"Inception-style\" cropping and then random \n        # horizontal flipping.\n        image = crop_resize.inception_crop(image)\n        image = tf.image.random_flip_left_right(image)\n    else:\n        # Central fraction amount is from here:\n        # https:\/\/git.io\/J8Kda.\n        image = tf.image.central_crop(image, central_fraction=0.875)  \n        image = tf.image.resize(image, (RESIZE, RESIZE))\n        \n    image = tf.reshape(image, (RESIZE, RESIZE, 3)) # Explicit reshaping for TPUs. \n    class_label = tf.cast(example[\"class\"], tf.int32) # Labels are only used for val\/test accuracy calculation here.\n    return image, class_label\n\n# Load the TFRecords and create tf.data.Dataset.\ndef load_dataset(filenames, train):\n    opt = tf.data.Options()\n    opt.experimental_deterministic = False\n    \n    if not train:\n        # Needed to distribute the validation data properly.\n        opt.experimental_distribute.auto_shard_policy = tf.data.experimental.AutoShardPolicy.DATA\n    \n    dataset = tf.data.TFRecordDataset(filenames, num_parallel_reads=AUTO) \n    dataset = dataset.map(lambda x: (read_tfrecord(x, train)), num_parallel_calls=AUTO)\n    dataset = dataset.with_options(opt)\n    return dataset\n\n# Batch, shuffle, and repeat the dataset and prefetch it\n# well before the current epoch ends.\ndef batch_dataset(filenames, train, batch_size=BATCH_SIZE):\n    dataset = load_dataset(filenames, train)\n    if train:\n        dataset = dataset.repeat().shuffle(BATCH_SIZE*10)\n    \n    dataset = dataset.batch(batch_size)\n    \n    if train:\n        dataset = dataset.map(mixup, num_parallel_calls=AUTO)\n    \n    dataset = dataset.prefetch(AUTO) \n    return dataset","6a88838f":"training_dataset = batch_dataset(train_filenames, True)\nvalidation_dataset = batch_dataset(val_filenames, False)\ntest_dataset = batch_dataset(test_filenames, False)","899ed7ef":"sample_images, _ = next(iter(training_dataset))\nplt.figure(figsize=(10, 10))\nfor n in range(25):\n    ax = plt.subplot(5, 5, n + 1)\n    plt.imshow(sample_images[n].numpy())\n    plt.axis(\"off\")\nplt.show()","7fa978ac":"def get_student():\n    # References:\n    # https:\/\/github.com\/google-research\/big_transfer\/blob\/master\/bit_tf2\/train.py#L97-L104\n    # https:\/\/github.com\/google-research\/big_transfer\/blob\/master\/bit_tf2\/models.py#L273-L278\n    model = models.ResnetV2(\n        num_units=(3, 4, 6, 3),\n        num_outputs=37,\n        filters_factor=4,\n        name=\"resnet\",\n        trainable=True,\n        dtype=tf.float32\n    )\n    return model\n\nmodel = get_student()\nmodel.build((None, RESIZE, RESIZE, 3))\nmodel.summary()","9ba61ab3":"# Last layer does have softmax.\nmodel.layers[-1].get_config()","77faed41":"# def get_resnetv2():\n#     resnet_v2 = tf.keras.applications.ResNet50V2(weights=None,\n#                                                  input_shape=(RESIZE, RESIZE, 3),\n#                                                  classes=37,\n#                                                  classifier_activation=\"linear\")\n#     return resnet_v2","88920221":"# Reference:\n# https:\/\/www.tutorialexample.com\/implement-kl-divergence-loss-in-tensorflow-tensorflow-tutorial\/\ndef kl_divergence(true_p, q):\n    true_prob = tf.nn.softmax(true_p, axis = 1)\n    loss_1 = -tf.nn.softmax_cross_entropy_with_logits(logits=true_p, labels=true_prob)\n    loss_2 = tf.nn.softmax_cross_entropy_with_logits(logits=q, labels=true_prob)   \n    loss = loss_1 + loss_2\n    return loss","ba080d0c":"# Reference:\n# https:\/\/keras.io\/examples\/vision\/knowledge_distillation\/\nclass Distiller(tf.keras.Model):\n    def __init__(self, student, teacher):\n        super(Distiller, self).__init__()\n        self.student = student\n        self.teacher = teacher\n\n    def compile(\n        self,\n        optimizer,\n        metrics,\n        distillation_loss_fn,\n        temperature=TEMPERATURE,\n    ):\n        super(Distiller, self).compile(optimizer=optimizer, metrics=metrics)\n        self.distillation_loss_fn = distillation_loss_fn\n        self.temperature = temperature\n\n    def train_step(self, data):\n        # Unpack data\n        x, _ = data\n\n        # Forward pass of teacher\n        teacher_predictions = self.teacher(x, training=False)\n\n        with tf.GradientTape() as tape:\n            # Forward pass of student\n            student_predictions = self.student(x, training=True)\n\n            # Compute loss\n            distillation_loss = self.distillation_loss_fn(\n                teacher_predictions \/ self.temperature,\n                student_predictions \/ self.temperature\n            )\n            distillation_loss = tf.nn.compute_average_loss(distillation_loss, \n                                                      global_batch_size=BATCH_SIZE)\n\n        # Compute gradients\n        trainable_vars = self.student.trainable_variables\n        gradients = tape.gradient(distillation_loss, trainable_vars)\n\n        # Update weights\n        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n\n        # Report progress\n        results = {m.name: m.result() for m in self.metrics}\n        results.update(\n            {\"distillation_loss\": distillation_loss}\n        )\n        return results\n\n    def test_step(self, data):\n        # Unpack data\n        x, y = data\n\n        # Forward pass of teacher\n        teacher_predictions = self.teacher(x, training=False)\n        student_predictions = self.student(x, training=False)\n\n        # Calculate the loss\n        distillation_loss = self.distillation_loss_fn(\n            teacher_predictions \/ self.temperature,\n            student_predictions \/ self.temperature\n        )\n        distillation_loss = tf.nn.compute_average_loss(distillation_loss, \n                                                      global_batch_size=BATCH_SIZE)\n\n        # Report progress\n        self.compiled_metrics.update_state(y, student_predictions)\n        results = {m.name: m.result() for m in self.metrics}\n        results.update(\n            {\"distillation_loss\": distillation_loss}\n        )\n        return results","9b0fbc75":"# Reference:\n# https:\/\/www.kaggle.com\/ashusma\/training-rfcx-tensorflow-tpu-effnet-b2\n\nclass WarmUpCosine(tf.keras.optimizers.schedules.LearningRateSchedule):\n    def __init__(\n        self, learning_rate_base, total_steps, warmup_learning_rate, warmup_steps\n    ):\n        super(WarmUpCosine, self).__init__()\n\n        self.learning_rate_base = learning_rate_base\n        self.total_steps = total_steps\n        self.warmup_learning_rate = warmup_learning_rate\n        self.warmup_steps = warmup_steps\n        self.pi = tf.constant(np.pi)\n\n    def __call__(self, step):\n        if self.total_steps < self.warmup_steps:\n            raise ValueError(\"Total_steps must be larger or equal to warmup_steps.\")\n        learning_rate = (\n            0.5\n            * self.learning_rate_base\n            * (\n                1\n                + tf.cos(\n                    self.pi\n                    * (tf.cast(step, tf.float32) - self.warmup_steps)\n                    \/ float(self.total_steps - self.warmup_steps)\n                )\n            )\n        )\n\n        if self.warmup_steps > 0:\n            if self.learning_rate_base < self.warmup_learning_rate:\n                raise ValueError(\n                    \"Learning_rate_base must be larger or equal to \"\n                    \"warmup_learning_rate.\"\n                )\n            slope = (\n                self.learning_rate_base - self.warmup_learning_rate\n            ) \/ self.warmup_steps\n            warmup_rate = slope * tf.cast(step, tf.float32) + self.warmup_learning_rate\n            learning_rate = tf.where(\n                step < self.warmup_steps, warmup_rate, learning_rate\n            )\n        return tf.where(\n            step > self.total_steps, 0.0, learning_rate, name=\"learning_rate\"\n        )","cb1185f9":"TOTAL_STEPS = int(DATASET_NUM_TRAIN_EXAMPLES \/ BATCH_SIZE * EPOCHS)\nscheduled_lrs = WarmUpCosine(learning_rate_base=INIT_LR,\n                             total_steps=TOTAL_STEPS,\n                             warmup_learning_rate=0.0,\n                             warmup_steps=1500)\n\nlrs = [scheduled_lrs(step) for step in range(TOTAL_STEPS)]\nplt.plot(lrs)\nplt.xlabel(\"Step\", fontsize=14)\nplt.ylabel(\"LR\", fontsize=14)\nplt.show()","aaf1578f":"# The teacher model gives 90.93% top-1 accuracy on the test set.\n# Authors report 91.03%. \nGCS_PATH_TEACHER = \"gs:\/\/funmatch-tf\/models\/T-r101x3-128\"\n\nwith strategy.scope():\n    optimizer = tfa.optimizers.AdamW(weight_decay=WEIGHT_DECAY, \n                                learning_rate=scheduled_lrs,\n                                clipnorm=CLIP_THRESHOLD)\n    teacher = tf.keras.models.load_model(GCS_PATH_TEACHER)\n    student = get_student()\n    student.build((None, RESIZE, RESIZE, 3))\n    \n    distiller = Distiller(student=student, teacher=teacher)\n    distiller.compile(optimizer,\n        metrics=[tf.keras.metrics.SparseCategoricalAccuracy()],\n        distillation_loss_fn=kl_divergence,\n        temperature=TEMPERATURE)\n    \nhistory = distiller.fit(\n    training_dataset,\n    steps_per_epoch=int(np.ceil(DATASET_NUM_TRAIN_EXAMPLES \/ BATCH_SIZE)),\n    validation_data=validation_dataset,\n    epochs=EPOCHS\n)","0f469b23":"student = distiller.student\n\nwith strategy.scope():\n    student.compile(metrics=[\"accuracy\"])\n    _, top1_accuracy = student.evaluate(test_dataset)\n    \nprint(f\"Top-1 accuracy on the test set: {round(top1_accuracy * 100, 2)}%\")","feac3ea0":"# Be sure to change this path accordingly.\nstudent.save(\"gs:\/\/funmatch-tf\/models\/S-r50x1-128-300\")","da72e795":"![](https:\/\/i.ibb.co\/KjMC6sh\/image.png)\n\n![](https:\/\/i.ibb.co\/KhpTk9P\/image.png)","7f4589e3":"## TPU detection","15b21c4d":"## Visualization","5a70649e":"## Warm-up Cosine LR schedule","931e64fb":"## Data input pipeline","82d9ada1":"## Evaluation","36d6c060":"## Setup\n\nWe need TensorFlow Addons to use the AdamW optimizer. Then we clone the official [big_transfer](https:\/\/github.com\/google-research\/big_transfer) repository to use some utilities. ","fb4b1fff":"## Acknowledgements\n\nHuge thanks to [Lucas Beyer](https:\/\/scholar.google.com\/citations?user=p2gwhK4AAAAJ&hl=en) (first author of the paper) for providing suggestions on the initial version of the implementation. \n\nThanks to the [ML-GDE program](https:\/\/developers.google.com\/programs\/experts\/) for providing GCP credits. ","4e716475":"Here's an expansion of the KL-Divergence loss taken from the paper:\n\n$$\n\\mathrm{KL}\\left(p_{t} \\| p_{s}\\right)=\\sum_{i \\in \\mathcal{C}}\\left[-p_{t, i} \\log p_{s, i}+p_{t, i} \\log p_{t, i}\\right]\n$$","2a069d01":"## Imports","c2a35334":"## Hyperparameters and constants\n\nThis table is from the [original paper](https:\/\/arxiv.org\/pdf\/2106.05237.pdf) and is kept here for reference. ","e96f3591":"`true_p` would be the logits from our teacher and `q` would be the student logits. ","96cdb3b7":"## Data loading\n\nYou can refer to [this notebook](https:\/\/colab.research.google.com\/github\/sayakpaul\/FunMatch-Distillation\/blob\/main\/tfrecords_pets37.ipynb) to know how these TFRecords were generated from the Pets37 dataset. For convenience, I have also hosted them [here](https:\/\/github.com\/sayakpaul\/FunMatch-Distillation\/releases\/tag\/v1.0.0). ","c7677d65":"## Distillation utilities","37741718":"The training accuracy does not matter here. ","ba184ead":"With this we should be able to get to ~81.4%. Score reported in the paper for this setting is 82.75% which is close to what we'd get. Here are some differences to note:\n\n* The authors make use of a bigger teacher model (ResNet152x2) which is slighly better (91.03% accurate) than what we used. \n* The `mixup()` variant we are using would produce a pair of duplicate images if the number of images is even. Now, for 8 workers it would become 8 pairs. This may have led to the reduced performance. Nevertheless, I found the `mixup()` implementation to be an easier read (a part of it was referred from [here](https:\/\/github.com\/google-research\/big_transfer\/blob\/master\/input_pipeline_tf2_or_jax.py)). Also, thanks to Lucas Beyer for pointing this out.\n\nFor full reproducibility, you can find the model weight [here](https:\/\/github.com\/sayakpaul\/FunMatch-Distillation\/releases). ","c6a4de9f":"This notebook shows how to implement the recipes presented in the [Knowledge distillation: A good teacher is patient and consistent](https:\/\/arxiv.org\/pdf\/2106.05237.pdf) paper. \n\nTo continue with the rest of the notebook, you'd need a billing-enabled Google Cloud Platform account with a GCS Bucket. ","182c7046":"## GCP integration\n\nSince we'll be using TPUs we need to authenticate Kaggle to have access to a GCS Bucket to read and write data. You can attach a GCP account from the \"Add-ons\" tab. ","2934efe4":"## Additional notes\n\nThis notebook assumes that you have already trained a good enough teacher model and that your data is stored as TFRecords inside a GCS Bucket. \n\n* [Teacher training notebook](https:\/\/www.kaggle.com\/spsayakpaul\/train-bit)\n* [Data preparation notebook](https:\/\/colab.research.google.com\/drive\/1Eqsh6rGouyeXhgy9pS4bvOXYX6haLLg-?usp=sharing)","68605268":"This is the standard ResNet50 (v2) and is here for experimentation. This differs from the BiT family of ResNets in the following way: they do not use Batch Normalization and instead use Weight Standardization and Group Normalization. ","ef6f8e14":"## Training\n\nTPU takes sometime to get initialized and hence the first epoch might take more time. Rest of the epochs should complete within ~5 seconds."}}