{"cell_type":{"bd0ef715":"code","db469980":"code","c99dd352":"code","03376bae":"code","1d20e605":"code","b74ba38a":"code","bb83b6ce":"code","4498e699":"code","eb445985":"code","2d47c746":"code","11b5fc08":"code","af93215b":"code","ea65ac7b":"code","fb749b7b":"code","c7688b4c":"code","232dbde2":"code","d5c06d6f":"code","21b27a88":"code","b61a5e30":"code","8588d4db":"code","4095abf9":"code","0abcb9de":"code","9de280c8":"markdown","d1145717":"markdown","59ab97c0":"markdown","52133a93":"markdown","d19479c4":"markdown","9ef6dd1f":"markdown","860ee436":"markdown","fe122235":"markdown","cfc2456c":"markdown","0bcaec31":"markdown"},"source":{"bd0ef715":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","db469980":"import numpy as np\nimport pandas as pd\npd.options.display.max_rows = 999\npd.options.display.max_columns = 999\nimport matplotlib.pyplot as plt\nimport seaborn as sns","c99dd352":"df = pd.read_csv('\/kaggle\/input\/automobile-dataset\/Automobile_data.csv')\ndf.head()","03376bae":"# We see there are mnay special charctersin place of missing values.\nmissing_values = ['?','--','-','??','.','unknown']\nauto_df = pd.read_csv('\/kaggle\/input\/automobile-dataset\/Automobile_data.csv', na_values=missing_values)\nauto_df.head()","1d20e605":"# Check for nan values.\nauto_df.isnull().sum()","b74ba38a":"# Drop certain features, which are not necessary.\nauto_df.drop(['normalized-losses'], axis=1,inplace=True)\nauto_df.isnull().sum()","bb83b6ce":"# As we can oserve there are onl few missing vlaues, so we can drop those particular values.\nauto_df.dropna(inplace=True)","4498e699":"# unique brand names.\nauto_df['make'].unique()","eb445985":"plt.figure(figsize=(8,6))\nauto_df['make'].value_counts().plot(kind='barh')\nplt.xlabel('Number of Models')\nplt.ylabel('Company')\nplt.title('Company model frequency');","2d47c746":"auto_df.groupby(['symboling'])['price'].mean().plot(kind='barh')\nplt.xlabel('Average price')\nplt.title('Average pricing in various Risk rating');","11b5fc08":"auto_df['num-of-doors'].value_counts().plot(kind='barh')\nplt.ylabel('Door Count')\nplt.ylabel('Number of Vehicles')\nplt.title('Door count frequency of Vehicles');","af93215b":"plt.subplot(2, 1, 1)\nauto_df['drive-wheels'].value_counts().plot(kind='barh')\nplt.ylabel('Wheel Driven Type')\nplt.xlabel('Number of vehicles')\nplt.title(\"Wheel Driven Type frequency of vehicles\")\n\nplt.subplot(2, 1, 2)\nauto_df.groupby(['drive-wheels'])['price'].mean().plot(kind='barh')\nplt.xlabel('Average Price')\nplt.title('Average Pricing in various Driving wheel types')\n\nplt.tight_layout();","ea65ac7b":"# Corelation matrix\nplt.figure(figsize=(12,12))\nsns.heatmap(auto_df.corr(), annot=True);","fb749b7b":"sns.lmplot('price', 'engine-size', auto_df);","c7688b4c":"auto_df.info()","232dbde2":"X = auto_df.iloc[:,0:-1].values\ny = auto_df.iloc[:,-1].values\n\n# Encoding the Categorical Columns using LabelEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nfor i in range(X.shape[1]):\n    X[:,i] = le.fit_transform(X[:,i])\n\nprint('Number of features after encoding = ',X.shape[1])","d5c06d6f":"from sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LinearRegression\n\nrfe = RFE(estimator = LinearRegression())\nrfe.fit(X,y)\nX = rfe.transform(X)\nX.shape","21b27a88":"from sklearn.model_selection import train_test_split\nfrom sklearn.decomposition import PCA\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import r2_score\n\ni_value=[]\nr2_value=[]\nfor i in range(2,np.size(X,1)+1):\n    \n    # Breaking X and y in Training and Test Set\n    X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=0)\n    \n    # Applying PCA\n    pca = PCA(n_components = i)\n    X_train = pca.fit_transform(X_train)\n    X_test = pca.transform(X_test)\n    \n    # Training And Testing the model\n    regressor = LinearRegression()\n    regressor.fit(X_train,y_train)\n    y_pred = regressor.predict(X_test)\n    r2_value.append(r2_score(y_pred,y_test).round(4))\n    i_value.append(i)\n    \n# Plotting the r2 Score with different number of components for PCA\nplt.plot(i_value,r2_value,marker='o',mfc='red',mec='red',color='blue')\nplt.xlabel('Number of Components')\nplt.ylabel('R2_Score')\nplt.title('No_of_Components VS R2_Score')\nplt.grid(b=None)\nplt.show()","b61a5e30":"from sklearn.model_selection import train_test_split\nfrom sklearn.decomposition import PCA\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import r2_score\n\n# Breaking X and y in training and test set.\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2, random_state=0)\n\n# Applying PCA with best components for PCA\nn = r2_value.index(max(r2_value))+2\npca = PCA(n_components=n)\nX_train = pca.fit_transform(X_train)\nX_test = pca.transform(X_test)\n\nprint(n)","8588d4db":"# Training model\nfrom sklearn.ensemble import RandomForestRegressor\nrfr = RandomForestRegressor()\nrfr.fit(X_train, y_train)\n\n# Prediction of 'price'.\ny_pred = rfr.predict(X_test)","4095abf9":"from sklearn.metrics import r2_score\nprint('R2 score for test set {}'.format(r2_score(y_test, y_pred).round(3)))","0abcb9de":"from sklearn.metrics import mean_squared_error\nfrom math import sqrt\n\nMSE = mean_squared_error(y_test, y_pred)\nRMSE = sqrt(MSE)\n\nprint(f'MSE : {MSE}')\nprint(f'RMSE : {RMSE}')","9de280c8":"### Multi Linear Regression","d1145717":"### Best Features Selection.\n\nRecursive Feature Elimination is used to eliminate those features which are not having significant effect in prediction.","59ab97c0":"### Top 3 vehicles are 'toyota', 'nissan' and 'honda'.","52133a93":"### Dimentionality Reduction\n\nPCA can be thought of as a projection method where data with n_column(features) is projected into subspace with n or fewer features.","d19479c4":"## 1. Importing Libraries","9ef6dd1f":"## 3. ML Model\nAs we can observe there are few object type features, which have to encoded before giving as input to, model.","860ee436":"### Other Metrics to consider for Regression type model.","fe122235":"## Conclusion\nBy performing EDA, we came to know that 'engine-size','curb-weight' and 'horse-price have higher corelation of 0.8 and 'city-mpg', 'highway-mpg' have neagtive correlation indiacted that lower price vehicles are fuel efficient.\n\nApplying RFE reduced the siginificant features to be consiered to 12. After applying Dimentionality Redcution Technique, i nopur case we used PCA total 4 features were selected for final training. At last, Conclusion is by using Random forest regressor, R2 score is '0.857'.","cfc2456c":"As we can obsere the r2_score is saturated at 4 .\n","0bcaec31":"### Train, test and Metrics."}}