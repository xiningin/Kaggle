{"cell_type":{"4176e78b":"code","59e76b81":"code","cf28843e":"code","354e8eb3":"code","9baca9fb":"code","4af295c4":"code","920e8395":"code","a4cbf558":"code","c057e9b2":"code","f80652c0":"code","7a4b2018":"code","453e09c1":"code","05f23403":"code","b88c2acd":"code","24827663":"code","61c06566":"code","ea448274":"code","42bc0df5":"code","36026f59":"code","42ed5b74":"code","85d2c6c6":"code","a4964440":"code","4245f4ad":"code","effb0817":"code","77cdfefc":"code","d79364d9":"code","e7582b08":"code","fe74336c":"code","2e1b9304":"code","4f2901c9":"code","f73d2abe":"code","bdec8999":"code","cabe9766":"code","52f69679":"code","e900ed14":"code","5e37d17c":"code","7f911ff8":"code","f8c441b0":"code","f28d6251":"code","c229fb16":"code","e51e06ce":"code","e0b0c391":"code","6021745a":"code","16b965e6":"code","1094d29f":"code","5be0bd3c":"code","d3cf42a2":"code","acb06233":"code","d87f38c6":"code","0473fd86":"code","65cb8312":"code","42a59edd":"code","0e854beb":"code","6847c416":"code","6aa5604e":"code","8acc3ef9":"code","c321bc99":"code","1a188667":"markdown","184553a1":"markdown","9cc1552c":"markdown","b7b0f526":"markdown","c17ccdcb":"markdown","2e934972":"markdown","0c87942a":"markdown","0cc6ff5e":"markdown","5ad0f5b3":"markdown","2f933a4c":"markdown","02260731":"markdown","b0873af6":"markdown","2ff8dc9c":"markdown","f962b64b":"markdown","0c7da530":"markdown","f1da4e39":"markdown"},"source":{"4176e78b":"from datetime import timedelta\nimport datetime as dt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\nfrom sklearn.model_selection import GridSearchCV, cross_validate, RandomizedSearchCV, validation_curve\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom catboost import CatBoostClassifier\nfrom sklearn.metrics import plot_roc_curve\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import roc_auc_score","59e76b81":"def check_df(dataframe, head=5):\n    print(\"##################### Shape #####################\")\n    print(dataframe.shape)\n\n\n    print(\"##################### Types #####################\")\n    print(dataframe.dtypes)\n\n    print(\"##################### Head #####################\")\n    print(dataframe.head(head))\n\n    print(\"##################### Tail #####################\")\n    print(dataframe.tail(head))\n\n    print(\"##################### NA #####################\")\n    print(dataframe.isnull().sum())\n\n    print(\"##################### Quantiles #####################\")\n    print(dataframe.quantile([0, 0.05, 0.50, 0.95, 0.99, 1]).T)\n\ndef grab_col_names(dataframe, cat_th=10, car_th=20):\n\n    # cat_cols, cat_but_car\n    cat_cols = [col for col in dataframe.columns if dataframe[col].dtypes == \"O\"]\n\n    num_but_cat = [col for col in dataframe.columns if dataframe[col].nunique() < cat_th and\n                   dataframe[col].dtypes != \"O\"]\n\n    cat_but_car = [col for col in dataframe.columns if dataframe[col].nunique() > car_th and\n                   dataframe[col].dtypes == \"O\"]\n\n    cat_cols = cat_cols + num_but_cat\n\n    cat_cols = [col for col in cat_cols if col not in cat_but_car]\n\n    # num_cols\n    num_cols = [col for col in dataframe.columns if dataframe[col].dtypes != \"O\"]\n\n    num_cols = [col for col in num_cols if col not in num_but_cat]\n\n    print(f\"Observations: {dataframe.shape[0]}\")\n    print(f\"Variables: {dataframe.shape[1]}\")\n    print(f'cat_cols: {len(cat_cols)}')\n    print(f'num_cols: {len(num_cols)}')\n    print(f'cat_but_car: {len(cat_but_car)}')\n    print(f'num_but_cat: {len(num_but_cat)}')\n\n    return cat_cols, num_cols, cat_but_car\n\ndef label_encoder(dataframe, binary_col):\n    labelencoder = LabelEncoder()\n    dataframe[binary_col] = labelencoder.fit_transform(dataframe[binary_col])\n    return dataframe\n\ndef one_hot_encoder(dataframe, categorical_cols, drop_first=False):\n    dataframe = pd.get_dummies(dataframe, columns=categorical_cols, drop_first=drop_first)\n    return dataframe","cf28843e":"df = pd.read_csv('..\/input\/hotel-booking-demand\/hotel_bookings.csv')","354e8eb3":"check_df(df)","9baca9fb":"filter = (df.children == 0) & (df.adults == 0) & (df.babies == 0)\ndf = df[~filter]\ndf.drop(columns=['company', 'reservation_status', 'agent'], axis=1, inplace=True)\ndf.dropna(subset=['country', 'children'], axis=0, inplace=True)\ndf.drop(df[df['adr'] < 0].index, inplace=True, axis=0)","4af295c4":"df[df['distribution_channel'] == 'Undefined']\ndf.drop(df[df['distribution_channel'] == 'Undefined'].index, inplace=True, axis=0)","920e8395":"# Feature Extracting\ndef family(data):\n    if ((data['adults'] > 0) & (data['children'] > 0)):\n        val = 1\n    elif ((data['adults'] > 0) & (data['babies'] > 0)):\n        val = 1\n    else:\n        val = 0\n    return val\n\ndf['new_is_family'] = df.apply(family, axis=1)\n\n\ndf.drop(df[df.assigned_room_type == 'L'].index, inplace=True)\ndf.drop(df[df.reserved_room_type == 'L'].index, inplace=True)\n\n\nunique_room_list = list(df.groupby(by='assigned_room_type').agg({'adr': 'mean'}).sort_values(by='adr', ascending=False).index)\nmapper = {}\nk = 10\nfor index, i in enumerate(unique_room_list):\n    mapper[unique_room_list[index]] = k\n    k = k - 1\n\n\ndf['assigned_room_type'].replace(mapper, inplace=True)\ndf['reserved_room_type'].replace(mapper, inplace=True)\n\ndf['new_room_difference'] = df['reserved_room_type'] - df['assigned_room_type']\ndf['new_total_people'] = df['adults'] + df['children'] + df['babies']\ndf['new_total_stay_day'] = df['stays_in_weekend_nights'] + df['stays_in_week_nights']\n\nfrom calendar import month_name\nsorted_months = list(month_name)[1:]\n\nmapper = {}\nfor index, i in enumerate(sorted_months):\n    mapper[i] = index+1\n\ndf['new_month'] = df.arrival_date_month.replace(mapper)\ndf['new_month'].value_counts()\ndf[['new_month', 'arrival_date_month']]\n\ndf.columns\ncols = ['arrival_date_day_of_month', 'new_month', 'arrival_date_year']\ndf['new_arrival_date'] = df[cols].apply(lambda x: '-'.join(x.values.astype(str)), axis=1)\ndf['new_arrival_date'].dtype\ndf['new_arrival_date'] = pd.to_datetime(df['new_arrival_date'])\n\n\nlist_PMS_date = []\nfor i in range(df.shape[0]):\n    list_PMS_date.append(df.new_arrival_date.iloc[i] - timedelta(days=int(df.lead_time.iloc[i])))\n\ndf[\"new_PMS_entering_date\"]=list_PMS_date\ndf['new_special_req_status'] = np.where(df['total_of_special_requests']!= 0, \"Yes\", \"No\")\ndf['new_dist_channel_type'] = np.where(df['distribution_channel']!= \"Direct\", \"Others\", \"Direct\")\n\ndf['new_room_difference_cat'] = np.nan\ndf.loc[df[df['new_room_difference'] > 0].index, 'new_room_difference_cat'] = 1\ndf.loc[df[df['new_room_difference'] < 0].index, 'new_room_difference_cat'] = -1\ndf.loc[df[df['new_room_difference'] == 0].index, 'new_room_difference_cat'] = 0\n\ndf.drop(df[(df['customer_type'] == 'Group') & (df['new_total_people'] == 1)].index, axis=0, inplace=True)\n\ndf['reservation_status_date'] = pd.to_datetime(df['reservation_status_date'])\ndf['reservation_status_date']= df['reservation_status_date'].map(dt.datetime.toordinal)\ndf['new_arrival_date'] = df['new_arrival_date'].map(dt.datetime.toordinal)\ndf['new_PMS_entering_date'] = df['new_PMS_entering_date'].map(dt.datetime.toordinal)\n\ndf['new_is_weekend'] = np.where([(df['stays_in_weekend_nights'] > 0) & (df['stays_in_week_nights'] == 0)], 1, 0)[0]\ndf['new_is_weekday'] = np.where([(df['stays_in_weekend_nights'] == 0) & (df['stays_in_week_nights'] > 0)], 1, 0)[0]\ndf['new_is_weekend_and_weekdays'] = np.where([(df['stays_in_weekend_nights'] > 0) & (df['stays_in_week_nights'] > 0)], 1, 0)[0]\ndf['new_want_parking_space'] = np.where(df['required_car_parking_spaces'] > 0, 1, 0)\ndf['new_special_req_status'] = np.where(df['new_special_req_status'] == 'Yes', 1, 0)\ndf['new_adr_per_person'] = df['adr'] \/ (df['adults']+df['children'])\ndf.columns","a4cbf558":"df.shape","c057e9b2":"# normalize price per night (adr):\nfull_data_guests = df.loc[df[\"is_canceled\"] == 0] # only actual guests\nroom_prices = full_data_guests[[\"hotel\", \"reserved_room_type\", \"new_adr_per_person\"]].sort_values(\"reserved_room_type\")\n\n# boxplot:\nplt.figure(figsize=(12, 8))\nsns.boxplot(x=\"reserved_room_type\",\n            y=\"new_adr_per_person\",\n            hue=\"hotel\",\n            data=room_prices,\n            hue_order=[\"City Hotel\", \"Resort Hotel\"],\n            fliersize=0)\nplt.title(\"Price of room types per night and person\", fontsize=16)\nplt.xlabel(\"Room type\", fontsize=16)\nplt.ylabel(\"Price [EUR]\", fontsize=16)\nplt.legend(loc=\"upper right\")\nplt.ylim(0, 160)\nplt.savefig('your_figure.png')\nplt.show()","f80652c0":"df.drop(columns=['adults', 'babies', 'children', 'total_of_special_requests'], inplace=True, axis=1)\n\ndf.drop(columns=['arrival_date_year', 'arrival_date_month', 'arrival_date_week_number', 'arrival_date_day_of_month',\n                 'stays_in_weekend_nights', 'stays_in_week_nights', 'required_car_parking_spaces',\n                 'distribution_channel', 'reserved_room_type', 'assigned_room_type','reservation_status_date',], axis=1, inplace=True)\n\ndf.columns","7a4b2018":"df.shape","453e09c1":"# encoding\nbinary_cols = [col for col in df.columns if df[col].dtype not in [int, float]\n               and df[col].nunique() == 2]\n\nfor col in binary_cols:\n    label_encoder(df, col)\n\ncat_cols, num_cols, cat_but_car = grab_col_names(df)\nohe_cols = [col for col in cat_cols if 12 >= df[col].nunique() > 2]\nohe_cols.remove('new_room_difference_cat')\ndf = one_hot_encoder(df, ohe_cols, drop_first=True)\n\nle = LabelEncoder()\n# There are more than 300 classes, so I wanted to use label encoder on this feature.\ndf['country'] = le.fit_transform(df['country'])","05f23403":"# Scaler\nscaler = StandardScaler()\ndf[num_cols] = scaler.fit_transform(df[num_cols])\ndf.head(10)","b88c2acd":"X = df.drop(['is_canceled'], axis=1)\ny = df['is_canceled']","24827663":"# RandomizedSearchCV\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier, AdaBoostClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_validate, GridSearchCV\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom catboost import CatBoostClassifier","61c06566":"\nmodels = [('KNN', KNeighborsClassifier()),\n          #(\"SVC\", SVC()),\n          (\"CART\", DecisionTreeClassifier()),\n          (\"RF\", RandomForestClassifier()),\n          ('Adaboost', AdaBoostClassifier()),\n          ('GBM', GradientBoostingClassifier()),\n          #('XGBoost', XGBClassifier()),\n          ('LightGBM', LGBMClassifier())\n          #('CatBoost', CatBoostClassifier(verbose=False))\n          ]\n\nknn_params = {\"n_neighbors\": [2, 10, 20, 40, 50]}\n\nsvc_params = {'C': [0.1, 1, 10, 100],\n              'gamma': [1, 0.1, 0.01, 0.001],\n              'kernel': ['rbf', 'poly', 'sigmoid']}\n\ncart_params = {'max_depth': [1, 5, 10, 15, 20],\n               \"min_samples_split\": [2, 5, 10, 15, 30]}\n\nrf_params = {\"max_depth\": [5, 8, 15, None],\n             \"max_features\": [5, 7, \"auto\"],\n             \"min_samples_split\": [2, 8, 15, 20],\n             \"n_estimators\": [100, 200, 500, 1000]}\n\nada_params = {\n    'n_estimators': [2, 10, 15, 20],\n    'learning_rate': [(0.97 + x \/ 100) for x in range(0, 8)],\n    'algorithm': ['SAMME', 'SAMME.R']}\n\ngbm_params = {\"learning_rate\": [0.01, 0.1],\n              \"n_estimators\": [100, 300, 500, 1500],\n              \"min_samples_split\": [2, 8, 15, 20]}\n\nxgboost_params = {\"learning_rate\": [0.1, 0.01],\n                  \"max_depth\": [5, 8, 12, 20],\n                  \"n_estimators\": [100, 200],\n                  \"colsample_bytree\": [0.5, 0.8, 1]}\n\nlightgbm_params = {\"learning_rate\": [0.01, 0.1],\n                   \"n_estimators\": [100, 300, 500, 1500],\n                   \"colsample_bytree\": [0.5, 0.7, 1]}\n\ncat_params = {\"learning_rate\": [0.1, 0.01],\n              \"max_depth\": [5, 8, 12, 20]}\n","ea448274":"randomcv_models = [('KNN', KNeighborsClassifier(), knn_params),\n                   #(\"SVC\", SVC(), svc_params),\n                   (\"CART\", DecisionTreeClassifier(), cart_params),\n                   (\"RF\", RandomForestClassifier(), rf_params),\n                   ('Adaboost', AdaBoostClassifier(), ada_params),\n                   ('GBM', GradientBoostingClassifier(), gbm_params),\n                   #('XGBoost', XGBClassifier(), xgboost_params),\n                   ('LightGBM', LGBMClassifier(), lightgbm_params)\n                   #('CatBoost', CatBoostClassifier(verbose=False), cat_params)\n                   ]","42bc0df5":"name_param = {}\nfor name, model, params in randomcv_models:\n    rf_random = RandomizedSearchCV(estimator=model,\n                                   param_distributions=params,\n                                   n_iter=100,\n                                   cv=3,\n                                   verbose=2,\n                                   n_jobs=-1)\n    rf_random.fit(X, y)\n    name_param[name] = rf_random.best_params_\n\nfor name in name_param:\n    print(f'#################################### Best Params for {name} ####################################')\n    print(name_param[name])\n","36026f59":"knn_model= KNeighborsClassifier(n_neighbors= 50)\nknn_model.fit(X, y)\ncv_results = cross_validate(knn_model, X, y, cv=10, scoring=[\"accuracy\", \"f1\", \"roc_auc\"])","42ed5b74":"cv_results['test_roc_auc'].mean()","85d2c6c6":"cv_results['test_f1'].mean()","a4964440":"cv_results['test_accuracy'].mean()","4245f4ad":"cart_model= DecisionTreeClassifier(min_samples_split= 2, max_depth =1)\ncart_model.fit(X, y)\ncv_results = cross_validate(cart_model, X, y, cv=10, scoring=[\"accuracy\", \"f1\", \"roc_auc\"])","effb0817":"cv_results['test_roc_auc'].mean()","77cdfefc":"cv_results['test_f1'].mean()","d79364d9":"cv_results['test_accuracy'].mean()","e7582b08":"rf_model = RandomForestClassifier(n_estimators = 100, min_samples_split= 20, min_samples_leaf= 4, max_features= 5, max_depth=5,random_state=17)\nrf_model.fit(X, y)\ncv_results = cross_validate(rf_model, X, y, cv=10, scoring=[\"accuracy\", \"f1\", \"roc_auc\"])","fe74336c":"cv_results['test_roc_auc'].mean()","2e1b9304":"cv_results['test_f1'].mean()","4f2901c9":"cv_results['test_accuracy'].mean()","f73d2abe":"ada_model= AdaBoostClassifier(n_estimators =10, learning_rate= 0.98, algorithm = 'SAMME')\nada_model.fit(X, y)\ncv_results = cross_validate(ada_model, X, y, cv=10, scoring=[\"accuracy\", \"f1\", \"roc_auc\"])","bdec8999":"cv_results['test_roc_auc'].mean()","cabe9766":"cv_results['test_f1'].mean()","52f69679":"cv_results['test_accuracy'].mean()","e900ed14":"gbm_model= GradientBoostingClassifier(n_estimators = 100, min_samples_split= 2, learning_rate = 0.01)\ngbm_model.fit(X, y)\ncv_results = cross_validate(gbm_model, X, y, cv=10, scoring=[\"accuracy\", \"f1\", \"roc_auc\"])","5e37d17c":"cv_results['test_roc_auc'].mean()","7f911ff8":"cv_results['test_f1'].mean()","f8c441b0":"cv_results['test_accuracy'].mean()","f28d6251":"light_gbm_model= LGBMClassifier(n_estimators = 100, learning_rate= 0.01, colsample_bytree = 1)\nlight_gbm_model.fit(X, y)\ncv_results = cross_validate(light_gbm_model, X, y, cv=10, scoring=[\"accuracy\", \"f1\", \"roc_auc\"])","c229fb16":"cv_results['test_roc_auc'].mean()","e51e06ce":"cv_results['test_f1'].mean()","e0b0c391":"cv_results['test_accuracy'].mean()","6021745a":"XGB_model = XGBClassifier(n_estimators= 100, max_depth= 5, learning_rate= 0.01, colsample_bytree= 0.5)\nXGB_model.fit(X, y)\ncv_results = cross_validate(XGB_model, X, y, cv=10, scoring=[\"accuracy\", \"f1\", \"roc_auc\"])","16b965e6":"cv_results['test_roc_auc'].mean()","1094d29f":"cv_results['test_f1'].mean()","5be0bd3c":"cv_results['test_accuracy'].mean()","d3cf42a2":"#CATB_model = CatBoostClassifier (max_depth=12, learning_rate = 0.01)\n#CATB_model.fit(X, y)\n#cv_results = cross_validate(CATB_model, X, y, cv=10, scoring=[\"accuracy\", \"f1\", \"roc_auc\"])","acb06233":"#cv_results['test_roc_auc'].mean()   # 0.5595822247806725","d87f38c6":"#cv_results['test_f1'].mean() # 0.36912609869169916","0473fd86":"#cv_results['test_accuracy'].mean() # 0.5122500079685312","65cb8312":"plt.figure()\n\n# Add the models to the list that you want to view on the ROC plot\nmodels = [\n{\n    'label': 'Random Forest Classifier',\n    'model': RandomForestClassifier(),\n    'auc': 0.8347\n},\n{\n    'label': 'Gradient Boosting',\n    'model': GradientBoostingClassifier(),\n    'auc': 0.8506\n},\n{\n    'label': 'LGBM',\n    'model': LGBMClassifier(),\n    'auc': 0.7863\n},\n{\n    'label': 'KNN',\n    'model': KNeighborsClassifier(),\n    'auc': 0.6648 \n},\n{\n    'label': 'Ada',\n    'model': AdaBoostClassifier(),\n    'auc': 0.8423\n},\n    \n]\n\n# Below for loop iterates through your models list\nfor m in models:\n    model = m['model'] # select the model\n    model.fit(X, y) # train the model\n# Compute False postive rate, and True positive rate\n    fpr, tpr, thresholds = roc_curve(y, model.predict_proba(X)[:,1])\n# Calculate Area under the curve to display on the plot\n# Now, plot the computed values\n    plt.plot(fpr, tpr, label='%s ROC (area = %0.2f)' % (m['label'], m['auc']))\n# Custom settings for the plot \nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('1-Specificity(False Positive Rate)')\nplt.ylabel('Sensitivity(True Positive Rate)')\nplt.title('Receiver Operating Characteristic')\nplt.legend(loc=\"lower right\")\nplt.savefig(r'C:\\Users\\Ayb\u00fcke Hamide AK\\Desktop\\DSMLBC6\\VBO PROJECT\\model_auc.png')\nplt.show()   # Display","42a59edd":"best_models = {\"XGBoost\" : XGBClassifier(n_estimators= 100, max_depth= 5, learning_rate= 0.01, colsample_bytree= 0.5),\n              \"AdaBoost\": AdaBoostClassifier(n_estimators =10, learning_rate= 0.98, algorithm = 'SAMME'),\n              \"GBM\": GradientBoostingClassifier(n_estimators = 100, min_samples_split= 2, learning_rate = 0.01)}","0e854beb":"voting_clf = VotingClassifier(\n    estimators=[('XGBoost', best_models[\"XGBoost\"]),\n                ('AdaBoost', best_models[\"AdaBoost\"]),\n                ('GBM', best_models[\"GBM\"])],\n    voting='soft')\n\nvoting_clf.fit(X, y)\n\ncv_results = cross_validate(voting_clf, X, y, cv=3, scoring=[\"accuracy\", \"f1\", \"roc_auc\"])","6847c416":"cv_results['test_accuracy'].mean()\n","6aa5604e":"cv_results['test_f1'].mean()","8acc3ef9":"cv_results['test_roc_auc'].mean()","c321bc99":"def plot_importance(model, features, num=len(X), save=False):\n    feature_imp = pd.DataFrame({'Value': model.feature_importances_, 'Feature': features.columns})\n    plt.figure(figsize=(10, 10))\n    sns.set(font_scale=1)\n    sns.barplot(x=\"Value\", y=\"Feature\", data=feature_imp.sort_values(by=\"Value\",\n                                                                     ascending=False)[0:num])\n    plt.title('Features')\n    plt.tight_layout()\n    plt.show()\n    if save:\n        plt.savefig('importances.png')\n\n\nplot_importance(rf_model, X)","1a188667":"### CART with Best Parameters","184553a1":"### Feature Importance in Random Forest Classifier Model","9cc1552c":"### XGB Classifier with Best Parameters","b7b0f526":"## FEATURE ENGINEERING","c17ccdcb":"Contract - when the booking has an allotment or other type of contract associated to it;\n\nGroup \u2013 when the booking is associated to a group;\n\nTransient \u2013 when the booking is not part of a group or contract, and is not associated to other transient booking;\n\nTransient-party \u2013 when the booking is transient, but is associated to at least other transient booking","2e934972":"<h1 style=\"background-color:#a83299;font-family:newtimeroman;font-size:300%;text-align:center;border-radius: 20px 20px;font-family:cursive\">Booking Prediction- ML Hyperparameter Optimization<\/h1>","0c87942a":"### Random Forest Classifier with Best Parameters","0cc6ff5e":"### Gradient Boosting Classifier with Best Params","5ad0f5b3":"### Stacking & Ensemble Learning","2f933a4c":"### AdaBoost Classifier with Best Parameters","02260731":"### KNN Classifier with Best Parameters","b0873af6":"## ML Hyperparameter Optimization","2ff8dc9c":"**Best Params for KNN**:  \n\n{'n_neighbors': 50} \n\n**Best Params for CART**: \n\n{'min_samples_split': 2, 'max_depth': 1} \n\n**Best Params for RF** : \n\n{'n_estimators': 100, 'min_samples_split': 2, 'max_features': 5, 'max_depth': 5} \n\n**Best Params for Adaboost** : \n\n{'n_estimators': 10, 'learning_rate': 0.98, 'algorithm': 'SAMME'} \n\n**Best Params for GBM** : \n\n{'n_estimators': 100, 'min_samples_split': 2, 'learning_rate': 0.01} \n\n**Best Params for LightGBM** : \n\n{'n_estimators': 100, 'learning_rate': 0.01, 'colsample_bytree': 1} \n\n**Best Params for XGBoost** : \n\n{'n_estimators': 100, 'max_depth': 5, 'learning_rate': 0.01, 'colsample_bytree': 0.5} \n\n**Best Params for CatBoost** :\n\n{'max_depth': 12, 'learning_rate': 0.01}","f962b64b":"### ROC-AUC Graph","0c7da530":"### CatBoost Classifier with Best Parameters","f1da4e39":"### LightGBM Classifier with Best Params"}}