{"cell_type":{"f4b4adbd":"code","956d7655":"code","40345e36":"code","9ab0630a":"code","56f2acc4":"code","2f899366":"code","7b3ab702":"code","9b8707bc":"code","41f3de92":"markdown","19207b62":"markdown","8e41aecb":"markdown","02faacf6":"markdown","b8c96be7":"markdown","fc5a7cbb":"markdown","cbbba4fc":"markdown"},"source":{"f4b4adbd":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","956d7655":"#lets import required libraries\n\nimport numpy as np #for data preperation to make dataset\nimport pandas as pd  #for dataset making\n\n#for webscraping\nfrom urllib.request import urlopen  #to handle url\nimport urllib\nfrom bs4 import BeautifulSoup  #popular webscraping lib","40345e36":"def get_webpage(link):\n    page=urlopen(link)  #opening url\n    soup=BeautifulSoup(page, 'html.parser') #collecting whole webpage as soup content\n    return soup","9ab0630a":"#call the function with link\ncontent=get_webpage('https:\/\/en.wikipedia.org\/wiki\/List_of_countries_and_dependencies_by_population')\n\n#extract tables from content\ntables=content.find_all('table')\n\nfor table in tables:\n    #lets print what looks like our data using pretify lib\n    print(table.prettify())\n","56f2acc4":"#lets extract the table with class \"wikitable sortable\"\n#table=content.find('table', {'class': 'wikitable sortable plainrowheaders jquery-tablesorter'})\n#extract rows from the table\n#rows=table.find_all('tr') #tr means table row..its html tag for tables\n\ntable = content.find('table', {'class': 'wikitable sortable plainrowheaders jquery-tablesorter'})\nrows = table.find_all('tr')\n\n# List of all links\nfor row in rows:\n    cells = row.find_all('td')\n    if len(cells) > 1:\n        country_link = cells[1].find('a', href=True)\n        #print(country_link.get('href'))","2f899366":"def getallinfo(url):\n    try:\n        country_page = get_webpage('https:\/\/en.wikipedia.org' + url)\n        table = country_page.find('table', {'class': 'infobox geography vcard'})\n        additional_details = []\n        read_content = False\n        for tr in table.find_all('tr'):\n            if (tr.get('class') == ['mergedtoprow'] and not read_content):\n                link = tr.find('a')\n                if (link and (link.get_text().strip() == 'Area' or\n                   (link.get_text().strip() == 'GDP' and tr.find('span').get_text().strip() == '(nominal)'))):\n                    read_content = True\n                if (link and (link.get_text().strip() == 'Population')):\n                    read_content = False\n            elif ((tr.get('class') == ['mergedrow'] or tr.get('class') == ['mergedbottomrow']) and read_content):\n                additional_details.append(tr.find('td').get_text().strip('\\n')) \n                if (tr.find('div').get_text().strip() != '\u2022\\xa0Total area' and\n                   tr.find('div').get_text().strip() != '\u2022\\xa0Total'):\n                    read_content = False\n        return additional_details\n    except Exception as error:\n        print('Error occured: {}'.format(error))\n        return []","7b3ab702":"data_content = []\nfor row in rows:\n    cells = row.find_all('td')\n    if len(cells) > 1:\n        print(cells[1].get_text())\n        country_link = cells[1].find('a')\n        country_info = [cell.text.strip('\\n') for cell in cells]\n        additional_details = getallinfo(country_link)\n        if (len(additional_details) == 4):\n            country_info += additional_details\n            data_content.append(country_info)\n\ndataset = pd.DataFrame(data_content)","9b8707bc":"#give name to your columns\nheaders = rows[0].find_all('th')\nheaders = [header.get_text().strip('\\n') for header in headers]\nheaders += ['Total Area', 'Percentage Water', 'Total Nominal GDP', 'Per Capita GDP']\ndataset.columns = headers\n\ndrop_columns = ['Rank', 'Date', 'Source']\ndataset.drop(drop_columns, axis = 1, inplace = True)\ndataset.sample(3)\n\ndataset.to_csv(\"Dataset.csv\", index = False)","41f3de92":"# In machine learning projects, we firstly need a dataset. Sometimes we need a custom, updated dataset. There webscraping does the magic. We use beautifulsoup, selenium libraries to collect data from our desired website, then analyze data using pandas, keras, scikit-learn etc and make a dataset.","19207b62":"## The table we need has the class 'wikitable sortable'. It has rows of information where the first row has headings and the other rows in succession have information about each country.\n\nNext, we explore the website for each country.","8e41aecb":"# Understanding our data\n## we are gonna collect data from (https:\/\/en.wikipedia.org\/wiki\/List_of_countries_and_dependencies_by_population) where data is in the form of table. We have to find the perfect class of the required table. You can learn html tags.","02faacf6":"# Lets finally build our dataset","b8c96be7":"## Lets scrap all info we need from our previously scrapped \"country_link\" to prepare our dataset","fc5a7cbb":"## Lets create a function that receives an URL and get the required webpage from website using BeautifulSoup","cbbba4fc":"# lets save the data to csv file"}}