{"cell_type":{"a3aeb4ac":"code","8f74380a":"code","289cb3df":"code","8e7d798d":"code","986b0bd2":"code","30d6c96c":"code","bdadf939":"code","282fe888":"code","42f85687":"code","35924d8a":"code","f48d4000":"code","b526388f":"code","2dc333fb":"code","f1cb8001":"code","c481e6ca":"code","aa27be8f":"code","17d1496b":"code","5d94dee2":"code","1d0eae94":"code","039b4fc6":"code","65682c24":"code","95cb7c93":"code","e5268800":"code","9354448e":"code","434631c8":"code","2a76e0e0":"code","6c125509":"code","7807fc83":"code","a5b7b470":"code","bdcbeafd":"code","ed9cbcdc":"code","2bd31931":"code","d2deaab5":"code","2b2c05cc":"code","c049357e":"code","b2f3f5bd":"code","b6a12208":"code","5d75bfcb":"code","e881fcc3":"code","7c6a6789":"markdown","6036a2e8":"markdown","b9e50055":"markdown","8d95132c":"markdown","16b66300":"markdown"},"source":{"a3aeb4ac":"# Import required base libraries.\n\nimport sys\nimport random\nimport time\nimport pickle\n\nimport IPython\n\nimport pandas as pd\nimport matplotlib\nimport numpy as np\nimport scipy as sp\nimport sklearn as sk\nimport plotly\n\nfrom IPython import display\nfrom pathlib import Path\n\nmain_path = Path(\"..\")\n\nprint(f\"Python version: {sys.version}\")\nprint(f\"Pandas version: {pd.__version__}\")\nprint(f\"Matplotlib version: {matplotlib.__version__}\")\nprint(f\"Numpy version: {np.__version__}\")\nprint(f\"Scipy version: {sp.__version__}\")\nprint(f\"IPython version: {IPython.__version__}\")\nprint(f\"Sklearn version: {sk.__version__}\")\n","8f74380a":"# Now import classification models and initialize visualisation tools.\n\n# Algorithms.\nfrom sklearn import (\n    tree, linear_model, neighbors, naive_bayes, ensemble,\n    discriminant_analysis, gaussian_process, neural_network,\n    multiclass,\n)\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\n\n# Preprocessing tools.\nfrom sklearn.preprocessing import (\n    OneHotEncoder, OrdinalEncoder, LabelEncoder, MinMaxScaler,\n)\nfrom sklearn.metrics import (\n    classification_report, f1_score, precision_score,recall_score,\n)\n\n# Statistics\nfrom scipy.stats import boxcox, zscore\n\n# Collinearity analysis.\nfrom statsmodels.stats.outliers_influence import  variance_inflation_factor\n\n# Model selection tools.\nfrom sklearn import feature_selection\nfrom sklearn import model_selection\nfrom sklearn import metrics\n\n# Copy models.\nfrom copy import deepcopy\nfrom sklearn import base\n\n# Visualization tools.\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom plotly import express as px\n\nrnd_state=42\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","289cb3df":"data = pd.read_csv(\n    \"\/kaggle\/input\/fetal-health-classification\/fetal_health.csv\",\n    encoding=\"utf-8\"\n)\ndata.head(10)","8e7d798d":"# Lets see is it balanced or not.\ndata.fetal_health.value_counts()\n\n# 2.0 and 3.0 are minority.\n# If i say 1 for all, approximately 77% of my predictions will be true.","986b0bd2":"data.dtypes","30d6c96c":"data.describe().transpose()","bdadf939":"data.isnull().sum()\n\n# No null values.","282fe888":"# Drop duplicates.\ndata.drop_duplicates(inplace=True, ignore_index=True)\n\ndata.info()\n\n# 13 rows deleted.","42f85687":"data.columns","35924d8a":"# Rename columns.\n\ndata.rename(\n    columns={\n        \"baseline value\": \"baseline_value\",\n        \"fetal_health\": \"class\"\n    },\n    inplace=True\n)\n\ncolumns = list(data.columns)\ncolumns.remove(\"class\")\n\ndata.head(5)","f48d4000":"px.histogram(data, x=\"baseline_value\", color=\"class\")\n\n# Normal distribution type data.","b526388f":"px.histogram(data, x=\"accelerations\", color=\"class\")\n\n# Right skewed.","2dc333fb":"px.histogram(data, x=\"fetal_movement\", color=\"class\")\n\n# Right skewed.","f1cb8001":"px.histogram(data, x=\"uterine_contractions\", color=\"class\")\n\n# Normal like.","c481e6ca":"px.histogram(data, x=\"light_decelerations\", color=\"class\")\n\n# Right skewed.","aa27be8f":"px.histogram(data, x=\"prolongued_decelerations\", color=\"class\")\n\n# Maybe we can categorize it.","17d1496b":"px.histogram(data, x=\"abnormal_short_term_variability\", color=\"class\")\n\n# bi-model, also uniform like.","5d94dee2":"px.histogram(data, x=\"mean_value_of_short_term_variability\", color=\"class\")\n\n# Right skewed.","1d0eae94":"px.histogram(data, x=\"percentage_of_time_with_abnormal_long_term_variability\", color=\"class\")\n\n# Right skewed.","039b4fc6":"px.histogram(data, x=\"mean_value_of_long_term_variability\", color=\"class\")\n\n# Right skewed but no transformation required.","65682c24":"px.histogram(data, x=\"histogram_width\", color=\"class\")\n\n# Bimodel and it seems mixed..","95cb7c93":"px.histogram(data, x=\"histogram_min\", color=\"class\")\n\n# Uniform.","e5268800":"px.histogram(data, x=\"histogram_max\", color=\"class\")\n\n# Normal.","9354448e":"px.histogram(data, x=\"histogram_number_of_peaks\", color=\"class\")","434631c8":"px.histogram(data, x=\"histogram_number_of_zeroes\", color=\"class\")\n\n# Right skewed but i wont transform it.","2a76e0e0":"px.histogram(data, x=\"histogram_mode\", color=\"class\")\n\n# Left skewed normal.","6c125509":"px.histogram(data, x=\"histogram_mean\", color=\"class\")\n\n# Left skewed normal. I wont transform it.","7807fc83":"px.histogram(data, x=\"histogram_median\", color=\"class\")\n\n# Left skewed normal. Mode, mean and median seems like same","a5b7b470":"px.histogram(data, x=\"histogram_variance\", color=\"class\")\n\n# Highly right skewed. This shows less variance in histograms.","bdcbeafd":"px.histogram(data, x=\"histogram_tendency\", color=\"class\")\n\n# Highly right skewed. This shows less variance in histograms.","ed9cbcdc":"# Transferred code for Variance Inflation Factor (VIF)\n\nfrom sklearn.base import BaseEstimator, TransformerMixin\n\nclass ReduceVIF(BaseEstimator, TransformerMixin):\n    def __init__(self, thresh=5.0, impute=False, impute_strategy='median'):\n        # From looking at documentation, values between 5 and 10 are \"okay\".\n        # Above 10 is too high and so should be removed.\n        self.thresh = thresh\n        \n        # The statsmodel function will fail with NaN values, as such we have to impute them.\n        # By default we impute using the median value.\n        # This imputation could be taken out and added as part of an sklearn Pipeline.\n        if impute:\n            self.imputer = Imputer(strategy=impute_strategy)\n\n    def fit(self, X, y=None):\n        print('ReduceVIF fit')\n        if hasattr(self, 'imputer'):\n            self.imputer.fit(X)\n        return self\n\n    def transform(self, X, y=None):\n        print('ReduceVIF transform')\n        columns = X.columns.tolist()\n        if hasattr(self, 'imputer'):\n            X = pd.DataFrame(self.imputer.transform(X), columns=columns)\n        return ReduceVIF.calculate_vif(X, self.thresh)\n\n    @staticmethod\n    def calculate_vif(X, thresh=5.0):\n        # Taken from https:\/\/stats.stackexchange.com\/a\/253620\/53565 and modified\n        dropped=True\n        while dropped:\n            variables = X.columns\n            dropped = False\n            vif = [variance_inflation_factor(X[variables].values, X.columns.get_loc(var)) for var in X.columns]\n            \n            max_vif = max(vif)\n            if max_vif > thresh:\n                maxloc = vif.index(max_vif)\n                print(f'Dropping {X.columns[maxloc]} with vif={max_vif}')\n                X = X.drop([X.columns.tolist()[maxloc]], axis=1)\n                dropped=True\n        return X","2bd31931":"transformer = ReduceVIF()\n\ncorrelated_features_dropped_df = transformer.fit_transform(data[columns], data[\"class\"])\n\ncorrelated_features_dropped_df.columns","d2deaab5":"# Detect important features.\nfeature_selector = XGBClassifier()\nfeature_selector.fit(data[columns], data[\"class\"])\n\n# Create a dataframe for visualization and selection.\nfeature_selection_df = pd.DataFrame(columns=[\"Feature_Name\", \"Importance\", \"Cumulative_Importance\", \"Is_Correlated\"])\nfeature_selection_df[\"Feature_Name\"] = list(columns)\nfeature_selection_df[\"Importance\"] = feature_selector.feature_importances_\nfeature_selection_df.sort_values(inplace=True, ascending=False, by=\"Importance\")\nfeature_selection_df[\"Cumulative_Importance\"] = feature_selection_df.Importance.cumsum()\nfeature_selection_df","2b2c05cc":"feature_selection_df.loc[:, \"Is_Correlated\"] = feature_selection_df[feature_selection_df.isin(list(correlated_features_dropped_df.columns))].Feature_Name.isna()\n\nfeature_selection_df.reset_index(inplace=True, drop=True)\nfeature_selection_df","c049357e":"# Now, columns selected.\n\ncolumns = list(feature_selection_df.iloc[[0, 1, 2, 3, 4, 5, 6, 10, 11, 14, 15, 17]].Feature_Name)\ncolumns","b2f3f5bd":"ml_algorithms = [\n    \n    # Ensemble Methods.\n    ensemble.AdaBoostClassifier(random_state=rnd_state),\n    ensemble.BaggingClassifier(random_state=rnd_state),\n    ensemble.ExtraTreesClassifier(random_state=rnd_state),\n    ensemble.GradientBoostingClassifier(random_state=rnd_state),\n    ensemble.RandomForestClassifier(random_state=rnd_state),\n    XGBClassifier(random_state=rnd_state),\n    LGBMClassifier(random_state=rnd_state),\n    \n    # Gaussian Processes.\n    gaussian_process.GaussianProcessClassifier(random_state=rnd_state),\n    \n    # Generalized Linear Methods.\n    linear_model.LogisticRegressionCV(random_state=rnd_state),\n    \n    # Naive Bayes.\n    naive_bayes.BernoulliNB(),\n    naive_bayes.GaussianNB(),\n    \n    # Nearest Neighbor.\n    neighbors.KNeighborsClassifier(),\n    \n    # Trees.\n    tree.DecisionTreeClassifier(random_state=rnd_state, max_depth=16, min_samples_leaf=1, min_samples_split=.2),\n    tree.ExtraTreeClassifier(random_state=rnd_state),\n    \n    # Discriminant Analysis.\n    discriminant_analysis.LinearDiscriminantAnalysis(),\n    discriminant_analysis.QuadraticDiscriminantAnalysis(),\n    \n    # Neural Networks.\n    neural_network.MLPClassifier(max_iter=300, random_state=rnd_state),\n    \n    # Stacked Methods.\n    ensemble.StackingClassifier(\n        estimators=[\n            (\"adaboost\", ensemble.AdaBoostClassifier(random_state=rnd_state)),\n            (\"gradient\", ensemble.GradientBoostingClassifier(random_state=rnd_state)),\n            (\"knn\", neighbors.KNeighborsClassifier(n_jobs=-1))\n        ],\n        final_estimator=linear_model.LogisticRegression(random_state=rnd_state)\n    )\n]\n\ncv_split = model_selection.StratifiedKFold(n_splits=3, shuffle=True, random_state=rnd_state)\n\nml_columns = [\n    \"Algorithm_Name\",\n    \"Algorithm_Parameters\",\n    \"Train_Balanced_Accuracy_Mean\",\n    \"Test_Balanced_Accuracy_Mean\",\n    \"Train_F1_Weighted_Mean\",\n    \"Test_F1_Weighted_Mean\",\n    \"Train_AUC_ROC_OVR_Weighted_Mean\",\n    \"Test_AUC_ROC_OVR_Weighted_Mean\",\n    \"Train_Balanced_Accuracy\",\n    \"Test_Balanced_Accuracy\",\n    \"Test_Accuracy_3*STD\",\n    \"Time (Mean)\",\n]\n\nml_compare = pd.DataFrame(columns=ml_columns)\n\ndata_used = data[columns]\nlabels = data[\"class\"]\n\nrow_index = 0\nfor alg in ml_algorithms:\n    ml_name = alg.__class__.__name__\n    ml_compare.loc[row_index, \"Algorithm_Name\"] = ml_name\n    ml_compare.loc[row_index, \"Algorithm_Parameters\"] = str(alg.get_params())\n    \n    # Cross validation.\n    cv_results = model_selection.cross_validate(\n        alg,\n        data_used,\n        labels,\n        cv=cv_split,\n        return_train_score=True,\n        scoring=\n            [\n                \"balanced_accuracy\",\n                \"f1_weighted\",\n                \"roc_auc_ovr_weighted\",\n            ],\n    )\n    \n    ml_compare.loc[row_index, \"Time (Mean)\"] = cv_results[\"fit_time\"].mean()\n    ml_compare.loc[row_index, \"Train_Balanced_Accuracy_Mean\"] = cv_results[\"train_balanced_accuracy\"].mean()\n    ml_compare.loc[row_index, \"Test_Balanced_Accuracy_Mean\"] = cv_results[\"test_balanced_accuracy\"].mean()\n    ml_compare.loc[row_index, \"Train_F1_Weighted_Mean\"] = cv_results[\"train_f1_weighted\"].mean()\n    ml_compare.loc[row_index, \"Test_F1_Weighted_Mean\"] = cv_results[\"test_f1_weighted\"].mean()\n    ml_compare.loc[row_index, \"Train_AUC_ROC_OVR_Weighted_Mean\"] = cv_results[\"train_roc_auc_ovr_weighted\"].mean()\n    ml_compare.loc[row_index, \"Test_AUC_ROC_OVR_Weighted_Mean\"] = cv_results[\"test_roc_auc_ovr_weighted\"].mean()\n    \n    ml_compare.loc[row_index, \"Test_Balanced_Accuracy\"] = str(cv_results[\"test_balanced_accuracy\"])\n    ml_compare.loc[row_index, \"Train_Balanced_Accuracy\"] = str(cv_results[\"train_balanced_accuracy\"])\n\n    # Worst case scenario.\n    ml_compare.loc[row_index, \"Test_Accuracy_3*STD\"] = cv_results[\"test_balanced_accuracy\"].std() * 3\n\n    row_index += 1\n\nml_compare.sort_values(by=[\"Test_AUC_ROC_OVR_Weighted_Mean\", \"Test_F1_Weighted_Mean\", \"Test_Balanced_Accuracy_Mean\"], ascending=False, inplace=True)\nml_compare\n\n# NOTE: Average precision can be added for evaluation.","b6a12208":"# Now select the most performant algorithm, then evaluate it.\nml_compare.iloc[0]","5d75bfcb":"# Create a train test split.\nX_train, X_test, y_train, y_test = model_selection.train_test_split(\n    data[columns],\n    data[\"class\"],\n    test_size=.3,\n    train_size=.7,\n    random_state=rnd_state,\n    shuffle=True,\n    stratify=data[\"class\"].values\n)\n\n# Create classifier with default parameters.\nclassifier = multiclass.OneVsRestClassifier(LGBMClassifier(class_weight=\"balanced\", random_state=rnd_state))\nclassifier.fit(X_train, y_train)\n\npredictions = classifier.predict(X_test)\n\nprecision = precision_score(y_test, predictions, average=\"weighted\")\nrecall = recall_score(y_test, predictions, average=\"weighted\")\nauc_roc = ml_compare.iloc[0][\"Test_AUC_ROC_OVR_Weighted_Mean\"]\nf1_weighted = ml_compare.iloc[0][\"Test_F1_Weighted_Mean\"]\n\nprint(f\"AUC ROC: {auc_roc}, F1 - Score: {f1_weighted}, Precision Score: {precision}, Recall Score: {recall}\")","e881fcc3":"# Print classification report.\nprint(classification_report(y_test, predictions, target_names=[\"Normal\", \"Suspect\", \"Pathological\"]))","7c6a6789":"##\u00a0See distributions for each feature.","6036a2e8":"Ok, we know data contains full of numeric values.\nThere is no categorical column to convert it to numeric values.\n\n__Btw 1 -> Normal, 2 -> Suspect, 3 -> Pathological. There is no big or small relation between them.\nSo I used dummy encoding.__","b9e50055":"* __mean_value_of_short_term_variability__ has .25 importance value. It can' t be discarded.  \n* __histogram_mean__ can' t be discarded.  \n* __abnormal_short_term_variabilty__ has 5.66 vif.\n* __histogram_max__, __histogram_mode__, __baseline_value__ has high vif value.\n* __histogram_width__ has __inf__ vif value.\n* __histogram_min__ and __histogram_median__ has high vif value","8d95132c":"## Discriminant Analysis detects collinearity between independent variables.","16b66300":"## Test models and get the first one."}}