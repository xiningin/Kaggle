{"cell_type":{"7d43906b":"code","c6b221ff":"code","4564e78b":"code","9ff7cf0b":"code","ac7469c5":"code","ba3268ed":"code","b6035037":"code","1adf71cb":"code","09eb5743":"code","45de1e51":"code","a70e0770":"code","bddda053":"code","cbb263d2":"code","0e34c25a":"code","2335d25f":"code","7ccbcbb3":"code","966c3a97":"code","704148f0":"code","ef379589":"code","e806e3dc":"code","5e41808e":"code","6f8765ba":"code","30ea28ad":"code","f0419b0e":"code","631bbe2f":"code","0f3cbef3":"code","449e32df":"code","55df7413":"code","c7ff5c64":"code","fa5b3dbc":"code","33618293":"code","1c3d0615":"code","8d393a1d":"code","5edb7fd8":"code","6ea18f00":"code","73071f93":"code","dff94618":"code","ab5a5bd7":"code","41afcf2e":"code","cebacd89":"code","09d5d8e6":"code","b4b20cf7":"code","031de284":"code","27ceedba":"code","bd2e9953":"code","facbee24":"markdown","5dfffc5f":"markdown","fbcd4176":"markdown","8eefd60a":"markdown","ec1ce4de":"markdown","ad1ff1de":"markdown","ad569054":"markdown","a9ffee60":"markdown","d60120bc":"markdown","bb4b890c":"markdown","597762d6":"markdown","2e30fda4":"markdown","315c3d53":"markdown","5f168316":"markdown","df130eab":"markdown","c8ee6174":"markdown","6b12abfe":"markdown","cfffa0b5":"markdown"},"source":{"7d43906b":"# Additional Dependencies\n!pip install barbar torchsummary","c6b221ff":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport time\nimport copy\nimport pickle\n#from barbar import Bar\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport scipy\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom PIL import Image\nimport cv2\n%matplotlib inline\n\nimport torch\nimport torchvision\nfrom torch import nn\nfrom torch.utils.data import DataLoader\nfrom torch.utils.data.dataset import Dataset\nfrom torchvision import transforms\n#from torchsummary import summary\n\nfrom tqdm import tqdm\nfrom pathlib import Path\nimport gc\nRANDOMSTATE = 0\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n# for dirname, _, filenames in os.walk('\/kaggle\/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","4564e78b":"# Find if any accelerator is presented, if yes switch device to use CUDA or else use CPU\ndevice = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\nprint(device)","9ff7cf0b":"# preparing intermediate DataFrame\ndatasetPath = Path('\/kaggle\/input\/cbir-dataset\/dataset\/')\ndf = pd.DataFrame()\n\ndf['image'] = [f for f in os.listdir(datasetPath) if os.path.isfile(os.path.join(datasetPath, f))]\ndf['image'] = '\/kaggle\/input\/cbir-dataset\/dataset\/' + df['image'].astype(str)\n\ndf.head()","ac7469c5":"class CBIRDataset(Dataset):\n    def __init__(self, dataFrame):\n        self.dataFrame = dataFrame\n        \n        self.transformations = transforms.Compose([\n            transforms.ToTensor(),\n            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n        ])\n    \n    def __getitem__(self, key):\n        if isinstance(key, slice):\n            raise NotImplementedError('slicing is not supported')\n        \n        row = self.dataFrame.iloc[key]\n        image = self.transformations(Image.open(row['image']))\n        return image\n    \n    def __len__(self):\n        return len(self.dataFrame.index)","ba3268ed":"# Intermediate Function to process data from the data retrival class\ndef prepare_data(DF):\n    trainDF, validateDF = train_test_split(DF, test_size=0.15, random_state=RANDOMSTATE)\n    train_set = CBIRDataset(trainDF)\n    validate_set = CBIRDataset(validateDF)\n    \n    return train_set, validate_set","b6035037":"class ConvAutoencoder(nn.Module):\n    def __init__(self):\n        super(ConvAutoencoder, self).__init__()\n        self.encoder = nn.Sequential(# in- (N,3,512,512)\n            \n            nn.Conv2d(in_channels=3, \n                      out_channels=16, \n                      kernel_size=(3,3), \n                      stride=3, \n                      padding=1),  # (32,16,171,171)\n            nn.ReLU(True),\n            nn.MaxPool2d(2, stride=2),  # (N,16,85,85)\n            \n            nn.Conv2d(in_channels=16, \n                      out_channels=8, \n                      kernel_size=(3,3), \n                      stride=2, \n                      padding=1),  # (N,8,43,43)\n            nn.ReLU(True),\n            nn.MaxPool2d(2, stride=1)  # (N,8,42,42)\n        )\n        self.decoder = nn.Sequential(\n            \n            nn.ConvTranspose2d(in_channels = 8, \n                               out_channels=16, \n                               kernel_size=(3,3), \n                               stride=2),  # (N,16,85,85)\n            nn.ReLU(True),\n \n            nn.ConvTranspose2d(in_channels=16, \n                               out_channels=8, \n                               kernel_size=(5,5), \n                               stride=3, \n                               padding=1),  # (N,8,255,255)\n            nn.ReLU(True),\n\n            nn.ConvTranspose2d(in_channels=8, \n                               out_channels=3, \n                               kernel_size=(6,6), \n                               stride=2, \n                               padding=1),  # (N,3,512,512)\n            nn.Tanh()\n        )\n\n    def forward(self, x):\n        x = self.encoder(x)\n        x = self.decoder(x)\n        return x","1adf71cb":"class ConvAutoencoder_v2(nn.Module):\n    def __init__(self):\n        super(ConvAutoencoder_v2, self).__init__()\n        self.encoder = nn.Sequential(# in- (N,3,512,512)\n            \n            nn.Conv2d(in_channels=3, \n                      out_channels=64, \n                      kernel_size=(3,3), \n                      stride=1, \n                      padding=1),\n            nn.ReLU(True),\n            nn.Conv2d(in_channels=64, \n                      out_channels=64, \n                      kernel_size=(3,3), \n                      stride=1, \n                      padding=1),\n            nn.ReLU(True),\n            nn.MaxPool2d(2, stride=2), \n            \n            nn.Conv2d(in_channels=64, \n                      out_channels=128, \n                      kernel_size=(3,3), \n                      stride=2, \n                      padding=1),\n            nn.ReLU(True),\n            nn.Conv2d(in_channels=128, \n                      out_channels=128, \n                      kernel_size=(3,3), \n                      stride=1, \n                      padding=0), \n            nn.ReLU(True),\n            nn.MaxPool2d(2, stride=2), \n            \n            nn.Conv2d(in_channels=128, \n                      out_channels=256, \n                      kernel_size=(3,3), \n                      stride=2, \n                      padding=1), \n            nn.ReLU(True),\n            nn.Conv2d(in_channels=256, \n                      out_channels=256, \n                      kernel_size=(3,3), \n                      stride=1, \n                      padding=1), \n            nn.ReLU(True),\n            nn.Conv2d(in_channels=256, \n                      out_channels=256, \n                      kernel_size=(3,3), \n                      stride=1, \n                      padding=1), \n            nn.ReLU(True),\n            nn.MaxPool2d(2, stride=2) \n        )\n        self.decoder = nn.Sequential(\n            \n            nn.ConvTranspose2d(in_channels = 256, \n                               out_channels=256, \n                               kernel_size=(3,3), \n                               stride=1,\n                              padding=1), \n \n            nn.ConvTranspose2d(in_channels=256, \n                               out_channels=256, \n                               kernel_size=(3,3), \n                               stride=1, \n                               padding=1),  \n            nn.ReLU(True),\n\n            nn.ConvTranspose2d(in_channels=256, \n                               out_channels=128, \n                               kernel_size=(3,3), \n                               stride=2, \n                               padding=0),  \n            \n            nn.ConvTranspose2d(in_channels=128, \n                               out_channels=64, \n                               kernel_size=(3,3), \n                               stride=2, \n                               padding=1),  \n            nn.ReLU(True),\n            nn.ConvTranspose2d(in_channels=64, \n                               out_channels=32, \n                               kernel_size=(3,3), \n                               stride=2, \n                               padding=1), \n            \n            nn.ConvTranspose2d(in_channels=32, \n                               out_channels=32, \n                               kernel_size=(3,3), \n                               stride=2, \n                               padding=1),  \n            nn.ReLU(True),\n            \n            nn.ConvTranspose2d(in_channels=32, \n                               out_channels=3, \n                               kernel_size=(4,4), \n                               stride=2, \n                               padding=2),  \n            nn.Tanh()\n        )\n\n    def forward(self, x):\n        x = self.encoder(x)\n        x = self.decoder(x)\n        return x","09eb5743":"summary(ConvAutoencoder_v2().to(device),(3,512,512))","45de1e51":"def load_ckpt(checkpoint_fpath, model, optimizer):\n    \n    # load check point\n    checkpoint = torch.load(checkpoint_fpath)\n\n    # initialize state_dict from checkpoint to model\n    model.load_state_dict(checkpoint['model_state_dict'])\n\n    # initialize optimizer from checkpoint to optimizer\n    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n\n    # initialize valid_loss_min from checkpoint to valid_loss_min\n    #valid_loss_min = checkpoint['valid_loss_min']\n\n    # return model, optimizer, epoch value, min validation loss \n    return model, optimizer, checkpoint['epoch']\n\ndef save_checkpoint(state, filename):\n    \"\"\"Save checkpoint if a new best is achieved\"\"\"\n    print (\"=> Saving a new best\")\n    torch.save(state, filename)  # save checkpoint\n    \ndef train_model(model,  \n                criterion, \n                optimizer, \n                #scheduler, \n                num_epochs):\n    since = time.time()\n    \n    best_model_wts = copy.deepcopy(model.state_dict())\n    best_loss = np.inf\n\n    for epoch in range(num_epochs):\n        print('Epoch {}\/{}'.format(epoch, num_epochs))\n        print('-' * 10)\n\n        # Each epoch has a training and validation phase\n        for phase in ['train', 'val']:\n            if phase == 'train':\n                model.train()  # Set model to training mode\n            else:\n                model.eval()   # Set model to evaluate mode\n\n            running_loss = 0.0\n\n            # Iterate over data.\n            for idx,inputs in enumerate(Bar(dataloaders[phase])):\n                inputs = inputs.to(device)\n\n                # zero the parameter gradients\n                optimizer.zero_grad()\n\n                # forward\n                # track history if only in train\n                with torch.set_grad_enabled(phase == 'train'):\n                    outputs = model(inputs)\n                    loss = criterion(outputs, inputs)\n\n                    # backward + optimize only if in training phase\n                    if phase == 'train':\n                        loss.backward()\n                        optimizer.step()\n\n                # statistics\n                running_loss += loss.item() * inputs.size(0)\n            #if phase == 'train':\n            #    scheduler.step()\n\n            epoch_loss = running_loss \/ dataset_sizes[phase]\n\n            print('{} Loss: {:.4f}'.format(\n                phase, epoch_loss))\n\n            # deep copy the model\n            if phase == 'val' and epoch_loss < best_loss:\n                best_loss = epoch_loss\n                best_model_wts = copy.deepcopy(model.state_dict())\n                save_checkpoint(state={   \n                                    'epoch': epoch,\n                                    'state_dict': model.state_dict(),\n                                    'best_loss': best_loss,\n                                    'optimizer_state_dict':optimizer.state_dict()\n                                },filename='ckpt_epoch_{}.pt'.format(epoch))\n\n        print()\n\n    time_elapsed = time.time() - since\n    print('Training complete in {:.0f}m {:.0f}s'.format(\n        time_elapsed \/\/ 60, time_elapsed % 60))\n    print('Best val Loss: {:4f}'.format(best_loss))\n\n    # load best model weights\n    model.load_state_dict(best_model_wts)\n    return model, optimizer, epoch_loss","a70e0770":"EPOCHS = 150\nNUM_BATCHES = 32\nRETRAIN = False\n\ntrain_set, validate_set = prepare_data(DF=df)\n\ndataloaders = {'train': DataLoader(train_set, batch_size=NUM_BATCHES, shuffle=True, num_workers=1) ,\n                'val':DataLoader(validate_set, batch_size=NUM_BATCHES, num_workers=1)\n                }\n\ndataset_sizes = {'train': len(train_set),'val':len(validate_set)}\n\nmodel = ConvAutoencoder_v2().to(device)\n\ncriterion = nn.MSELoss()\n# Observe that all parameters are being optimized\noptimizer = torch.optim.Adam(model.parameters(), lr=3e-4)\n# Decay LR by a factor of 0.1 every 7 epochs\n#exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)","bddda053":"# If re-training is required:\n# Load the old model\nif RETRAIN == True:\n    # load the saved checkpoint\n    model, optimizer, start_epoch = load_ckpt('..\/input\/cbirpretrained\/conv_autoencoder.pt', model, optimizer)\n    print('Checkpoint Loaded')","cbb263d2":"model, optimizer, loss = train_model(model=model, \n                    criterion=criterion, \n                    optimizer=optimizer, \n                    #scheduler=exp_lr_scheduler,\n                    num_epochs=EPOCHS)","0e34c25a":"# Save the Trained Model\ntorch.save({\n            'epoch': EPOCHS,\n            'model_state_dict': model.state_dict(),\n            'optimizer_state_dict': optimizer.state_dict(),\n            'loss': loss,\n            }, 'conv_autoencoderv2_200ep.pt')","2335d25f":"transformations = transforms.Compose([\n            transforms.ToTensor(),\n            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n        ])","7ccbcbb3":"# Load Model in Evaluation phase\nmodel = ConvAutoencoder_v2().to(device)\nmodel.load_state_dict(torch.load('..\/input\/cbirpretrainedv2\/conv_autoencoderv2_200ep.pt', map_location=device)['model_state_dict'], strict=False)\n\nmodel.eval()","966c3a97":"def get_latent_features(images, transformations):\n    \n    latent_features = np.zeros((4738,256,16,16))\n    #latent_features = np.zeros((4738,8,42,42))\n    \n    for i,image in enumerate(tqdm(images)):\n        tensor = transformations(Image.open(image)).to(device)\n        latent_features[i] = model.encoder(tensor.unsqueeze(0)).cpu().detach().numpy()\n        \n    del tensor\n    gc.collect()\n    return latent_features","704148f0":"images = df.image.values\nlatent_features = get_latent_features(images, transformations)","ef379589":"indexes = list(range(0, 4738))\nfeature_dict = dict(zip(indexes,latent_features))\nindex_dict = {'indexes':indexes,'features':latent_features}","e806e3dc":"# write the data dictionary to disk\n#with open('features.pkl', \"wb\") as f:\n#    f.write(pickle.dumps(index_dict))","5e41808e":"def euclidean(a, b):\n    # compute and return the euclidean distance between two vectors\n    return np.linalg.norm(a - b)","6f8765ba":"def cosine_distance(a,b):\n    return scipy.spatial.distance.cosine(a, b)","30ea28ad":"def perform_search(queryFeatures, index, maxResults=64):\n\n    results = []\n\n    for i in range(0, len(index[\"features\"])):\n        # compute the euclidean distance between our query features\n        # and the features for the current image in our index, then\n        # update our results list with a 2-tuple consisting of the\n        # computed distance and the index of the image\n        d = euclidean(queryFeatures, index[\"features\"][i])\n        results.append((d, i))\n    \n    # sort the results and grab the top ones\n    results = sorted(results)[:maxResults]\n    # return the list of results\n    return results","f0419b0e":"def build_montages(image_list, image_shape, montage_shape):\n\n    if len(image_shape) != 2:\n        raise Exception('image shape must be list or tuple of length 2 (rows, cols)')\n    if len(montage_shape) != 2:\n        raise Exception('montage shape must be list or tuple of length 2 (rows, cols)')\n    image_montages = []\n    # start with black canvas to draw images onto\n    montage_image = np.zeros(shape=(image_shape[1] * (montage_shape[1]), image_shape[0] * montage_shape[0], 3),\n                          dtype=np.uint8)\n    cursor_pos = [0, 0]\n    start_new_img = False\n    for img in image_list:\n        if type(img).__module__ != np.__name__:\n            raise Exception('input of type {} is not a valid numpy array'.format(type(img)))\n        start_new_img = False\n        img = cv2.resize(img, image_shape)\n        # draw image to black canvas\n        montage_image[cursor_pos[1]:cursor_pos[1] + image_shape[1], cursor_pos[0]:cursor_pos[0] + image_shape[0]] = img\n        cursor_pos[0] += image_shape[0]  # increment cursor x position\n        if cursor_pos[0] >= montage_shape[0] * image_shape[0]:\n            cursor_pos[1] += image_shape[1]  # increment cursor y position\n            cursor_pos[0] = 0\n            if cursor_pos[1] >= montage_shape[1] * image_shape[1]:\n                cursor_pos = [0, 0]\n                image_montages.append(montage_image)\n                # reset black canvas\n                montage_image = np.zeros(shape=(image_shape[1] * (montage_shape[1]), image_shape[0] * montage_shape[0], 3),\n                                      dtype=np.uint8)\n                start_new_img = True\n    if start_new_img is False:\n        image_montages.append(montage_image)  # add unfinished montage\n    return image_montages","631bbe2f":"# take the features for the current image, find all similar\n# images in our dataset, and then initialize our list of result\n# images\nfig, ax = plt.subplots(nrows=2,figsize=(15,15))\nqueryIdx = 3166# Input Index for which images \nMAX_RESULTS = 10\n\n\nqueryFeatures = latent_features[queryIdx]\nresults = perform_search(queryFeatures, index_dict, maxResults=MAX_RESULTS)\nimgs = []\n\n# loop over the results\nfor (d, j) in results:\n    img = np.array(Image.open(images[j]))\n    print(j)\n    imgs.append(img)\n\n# display the query image\nax[0].imshow(np.array(Image.open(images[queryIdx])))\n\n# build a montage from the results and display it\nmontage = build_montages(imgs, (512, 512), (5, 2))[0]\nax[1].imshow(montage)","0f3cbef3":"testpath = Path('..\/input\/testcbir\/Test_Images')\ntestdf = pd.DataFrame()\n\ntestdf['image'] = [f for f in os.listdir(testpath) if os.path.isfile(os.path.join(testpath, f))]\ntestdf['image'] = '..\/input\/testcbir\/Test_Images\/' + testdf['image'].astype(str)\n\ntestdf.head()","449e32df":"testimages = testdf.image.values\ntest_latent_features = get_latent_features(testimages, transformations)","55df7413":"test_latent_features.shape","c7ff5c64":"fig, ax = plt.subplots(nrows=2,figsize=(15,15))\nMAX_RESULTS = 10\nqueryIdx = 12\n\nqueryFeatures = test_latent_features[queryIdx]\nresults = perform_search(queryFeatures, index_dict, maxResults=MAX_RESULTS)\nimgs = []\n\n# loop over the results\nfor (d, j) in results:\n    img = np.array(Image.open(images[j]))\n    print(j)\n    imgs.append(img)\n\n# display the query image\nax[0].imshow(np.array(Image.open(testimages[queryIdx])))\n\n# build a montage from the results and display it\nmontage = build_montages(imgs, (512, 512), (5, 2))[0]\nax[1].imshow(montage)","fa5b3dbc":"#!pip install lshashpy3","33618293":"#from lshashpy3 import LSHash","1c3d0615":"## Locality Sensitive Hashing\n# params\n# k = 12 # hash size\n# L = 5  # number of tables\n# d = 14112 # Dimension of Feature vector\n# lsh = LSHash(hash_size=k, input_dim=d, num_hashtables=L)\n\n# # LSH on all the images\n# for idx,vec in tqdm(feature_dict.items()):\n#     lsh.index(vec.flatten(), extra_data=idx)","8d393a1d":"## Exporting as pickle\n#pickle.dump(lsh, open('lsh.p', \"wb\"))","5edb7fd8":"# def get_similar_item(idx, feature_dict, lsh_variable, n_items=10):\n#     response = lsh_variable.query(feature_dict[list(feature_dict.keys())[idx]].flatten(), \n#                      num_results=n_items+1, distance_func='hamming')\n    \n#     imgs = []\n#     for i in range(1, n_items+1):\n#         imgs.append(np.array(Image.open(images[response[i][0][1]])))\n#     return imgs","6ea18f00":"# fig, ax = plt.subplots(nrows=2,figsize=(15,15))\n# queryIdx = 5\n\n# ax[0].imshow(np.array(Image.open(images[queryIdx])))\n\n# montage = build_montages(get_similar_item(queryIdx, feature_dict, lsh,10),(512, 512), (5, 2))[0]\n# ax[1].imshow(montage)","73071f93":"from sklearn.cluster import KMeans, MiniBatchKMeans\nfrom scipy.spatial.distance import cdist\nfrom sklearn.metrics import silhouette_samples, silhouette_score\n\nimport matplotlib.cm as cm\n%matplotlib inline","dff94618":"def get_latent_features1D(images, transformations):\n    \n    latent_features1d = []\n    \n    for i,image in enumerate(tqdm(images)):\n        tensor = transformations(Image.open(image)).to(device)\n        latent_features1d.append(model.encoder(tensor.unsqueeze(0)).cpu().detach().numpy().flatten())\n        \n    del tensor\n    gc.collect()\n    return latent_features1d","ab5a5bd7":"images = df.image.values\nlatent_features1d = get_latent_features1D(images, transformations)","41afcf2e":"latent_features1d = np.array(latent_features1d)","cebacd89":"distortions = [] \ninertias = [] \nmapping1 = {} \nmapping2 = {} \nK = range(4,10) \n  \nfor k in tqdm(K): \n    #Building and fitting the model \n    kmeanModel = KMeans(n_clusters=k).fit(latent_features1d)      \n      \n    distortions.append(sum(np.min(cdist(latent_features1d, kmeanModel.cluster_centers_, \n                      'euclidean'),axis=1)) \/ latent_features1d.shape[0]) \n    inertias.append(kmeanModel.inertia_) \n  \n    mapping1[k] = sum(np.min(cdist(latent_features1d, kmeanModel.cluster_centers_, \n                 'euclidean'),axis=1)) \/ latent_features1d.shape[0] \n    mapping2[k] = kmeanModel.inertia_ ","09d5d8e6":"plt.plot(K, distortions, 'bx-') \nplt.xlabel('Values of K') \nplt.ylabel('Distortion') \nplt.title('The Elbow Method using Distortion') \nplt.show() ","b4b20cf7":"X = np.array(latent_features1d)\nK = range(3,10) \n\nfor n_clusters in tqdm(K):\n    # Create a subplot with 1 row and 2 columns\n    fig, (ax1, ax2) = plt.subplots(1, 2)\n    fig.set_size_inches(18, 7)\n\n    # The 1st subplot is the silhouette plot\n    # The silhouette coefficient can range from -1, 1 but in this example all\n    # lie within [-0.1, 1]\n    ax1.set_xlim([-0.1, 1])\n    # The (n_clusters+1)*10 is for inserting blank space between silhouette\n    # plots of individual clusters, to demarcate them clearly.\n    ax1.set_ylim([0, len(X) + (n_clusters + 1) * 10])\n\n    # Initialize the clusterer with n_clusters value and a random generator\n    # seed of 10 for reproducibility.\n    clusterer = KMeans(n_clusters=n_clusters, random_state=RANDOMSTATE)\n    cluster_labels = clusterer.fit_predict(X)\n\n    # The silhouette_score gives the average value for all the samples.\n    # This gives a perspective into the density and separation of the formed\n    # clusters\n    silhouette_avg = silhouette_score(X, cluster_labels)\n    print(\"For n_clusters =\", n_clusters,\n          \"The average silhouette_score is :\", silhouette_avg)\n\n    # Compute the silhouette scores for each sample\n    sample_silhouette_values = silhouette_samples(X, cluster_labels)\n\n    y_lower = 10\n    for i in range(n_clusters):\n        # Aggregate the silhouette scores for samples belonging to\n        # cluster i, and sort them\n        ith_cluster_silhouette_values = \\\n            sample_silhouette_values[cluster_labels == i]\n\n        ith_cluster_silhouette_values.sort()\n\n        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n        y_upper = y_lower + size_cluster_i\n\n        color = cm.nipy_spectral(float(i) \/ n_clusters)\n        ax1.fill_betweenx(np.arange(y_lower, y_upper),\n                          0, ith_cluster_silhouette_values,\n                          facecolor=color, edgecolor=color, alpha=0.7)\n\n        # Label the silhouette plots with their cluster numbers at the middle\n        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n\n        # Compute the new y_lower for next plot\n        y_lower = y_upper + 10  # 10 for the 0 samples\n\n    ax1.set_title(\"The silhouette plot for the various clusters.\")\n    ax1.set_xlabel(\"The silhouette coefficient values\")\n    ax1.set_ylabel(\"Cluster label\")\n\n    # The vertical line for average silhouette score of all the values\n    ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n\n    ax1.set_yticks([])  # Clear the yaxis labels \/ ticks\n    ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n\n    # 2nd Plot showing the actual clusters formed\n    colors = cm.nipy_spectral(cluster_labels.astype(float) \/ n_clusters)\n    ax2.scatter(X[:, 0], X[:, 1], marker='.', s=30, lw=0, alpha=0.7,\n                c=colors, edgecolor='k')\n\n    # Labeling the clusters\n    centers = clusterer.cluster_centers_\n    # Draw white circles at cluster centers\n    ax2.scatter(centers[:, 0], centers[:, 1], marker='o',\n                c=\"white\", alpha=1, s=200, edgecolor='k')\n\n    for i, c in enumerate(centers):\n        ax2.scatter(c[0], c[1], marker='$%d$' % i, alpha=1,\n                    s=50, edgecolor='k')\n\n    ax2.set_title(\"The visualization of the clustered data.\")\n    ax2.set_xlabel(\"Feature space for the 1st feature\")\n    ax2.set_ylabel(\"Feature space for the 2nd feature\")\n\n    plt.suptitle((\"Silhouette analysis for KMeans clustering on sample data \"\n                  \"with n_clusters = %d\" % n_clusters),\n                 fontsize=14, fontweight='bold')\n\nplt.show()","031de284":"def build_dictionary(xfeatures2d, images, n_clusters):\n    #print('Computing descriptors..')        \n    desc_list = []\n    \n    for image_path in images:\n        image = cv2.imread(image_path)\n        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n        kp, dsc = xfeatures2d.detectAndCompute(gray, None)\n        desc_list.extend(dsc)\n\n    desc = np.array(desc_list)\n    #print('Creating BoW dictionary using K-Means clustering with k={}..'.format(n_clusters))\n    dictionary = MiniBatchKMeans(n_clusters=n_clusters, batch_size=100, verbose=0)\n    dictionary.fit(desc)\n    \n    distortion = sum(np.min(cdist(desc, dictionary.cluster_centers_, \n                      'euclidean'),axis=1)) \/ desc.shape[0]\n    \n    return distortion","27ceedba":"orb = cv2.ORB_create()\nimages = df.image.values\nK = range(4,10)\ndistortions = []\n\nfor k in tqdm(K):\n    distortions.append(build_dictionary(orb, images, n_clusters=k))","bd2e9953":"plt.plot(K, distortions, 'bx-') \nplt.xlabel('Values of K') \nplt.ylabel('Distortion') \nplt.title('The Elbow Method using Distortion') \nplt.show() ","facbee24":"# Inference","5dfffc5f":"# AutoEncoder Model","fbcd4176":"### 2.1 Euclidean Search Method","8eefd60a":"![](https:\/\/hackernoon.com\/hn-images\/1*op0VO_QK4vMtCnXtmigDhA.png)","ec1ce4de":"## High Level Structure of an AutoEncoder","ad1ff1de":"# Training Function","ad569054":"- The Silhouette score isn't significant for any cluster since its close to 0 for every k, that translates to less differentiability for a point to belong to a particular cluster.\n- GMM can help in this case because animals share a lot of similar traits with each other in terms of appearance but we have to get the bottleneck case since an animal can only belong to one cluster, so kmeans will be the way to go but a different feature\/keypoint detection might help identify right number of clusters.","a9ffee60":"# Content Based Image Retrieval (CBIR)\n## Approach:\n\n- By observing the data its pretty clear that an Unsupervised alongwith couple of different Hashing approachs will be the most commendable. Although there are number of techniques in that area as well, we'll focus on Hashing and Auto-Encoder techniques:\n    \n    - ***Latent Feature Extraction***: In this technique we can find feature vectors for every image by creating hooks on a pre-trained network and extracting the vector from previous layers. Other technique devises the use of **AutoEncoders** where the Latent features can be extracted from Encoder itself. For the sake of this data we'll proceed with AutoEncoders. For the retrieval part we'll look into Euclidean based Search (O(NlogN)) and Hashing Based Approaches (O(logN)).\n    <br>\n    - ***Image Hashing Search***: This can be done by:\n        - Uniquely quantify the contents of an image using only a single integer.\n        - Find duplicate or near-duplicate images in a dataset of images based on their computed hashes.<br>\n        <br>\n      This can be accomplished by a specialized data structure called a **VP-Tree**. Using a VP-Tree we can reduce our search complexity from O(nlogn) to O(log n), enabling us to obtain our sub-linear goal!","d60120bc":"- The ORB technique tells us there are 6\/7 major clusters that are persistent in the data","bb4b890c":"## Using SIFT\/SURF\/ORB technique","597762d6":"# Data Preparation","2e30fda4":"<font size=\"3\"> This will be approached with two ways as discussed in the start:\n    - Euclidean Search:\n        - Identifying the Latent Features\n        - Calculating the Euclidean Distance between them\n        - Returning the closest N indexes (of images)\n    \n    - Locality Sensitive Hashing\n        - Create hashes of the feature vector from Encoder\n        - Store it in a Hashing Table\n        - Identify closest images based on hamming distance","315c3d53":"# Clustering of Images","5f168316":"![](https:\/\/www.pyimagesearch.com\/wp-content\/uploads\/2019\/08\/image_hashing_search_engine_steps.png)","df130eab":"### 2.2 LSHashing Method","c8ee6174":"# End Notes\n\n- We started with the approach of AutoEncoders for Image Latent Features extraction followed by Image retrieval using Euclidean Distance which was an O(NlogN) approach (Time-Complexity) to Hashing which gave us an ~O(logN) approach\n\n- Another approach was to use Hashing on features obtained from SIFT, SURF, OBS and building the VP Trees ans search the images in it.","6b12abfe":"## 2. Image Retrieval ","cfffa0b5":"## 1. Indexing"}}