{"cell_type":{"6eb3bb0a":"code","3232566c":"code","6fb7bd91":"code","9def7d80":"code","7d640d2c":"code","c8928c97":"code","649034d2":"code","3fb6bb1f":"code","a0c24354":"code","f4a98bf5":"code","88b035a0":"code","1be8f173":"code","5592008b":"code","bbe06562":"code","5131b6fa":"code","8d85bdd9":"code","27aa0007":"code","3526123a":"code","1145e5ec":"code","7e36612c":"code","041b472d":"code","2cf94787":"code","0c858e28":"code","40f23e6c":"code","6a7521e3":"code","1d9ef3de":"code","fbd1e73c":"code","2ec0f3bb":"markdown","6aa54d91":"markdown","998eb736":"markdown","b298bbc4":"markdown","2203623c":"markdown","64923b43":"markdown","e9db6751":"markdown","6032b3d3":"markdown","f8ab9f0c":"markdown","4b16f535":"markdown","5e399f34":"markdown","c66bd019":"markdown","36a0bd9f":"markdown","656d34bb":"markdown","79b47865":"markdown","4e4e41c6":"markdown","0d02561f":"markdown","cc306101":"markdown","682399b1":"markdown","6c2c7722":"markdown","9a72f6ee":"markdown","73ed91fe":"markdown","beb8388c":"markdown","39bea2e5":"markdown","919db664":"markdown","ac392301":"markdown","bf236342":"markdown","4ea4fd0b":"markdown","67fdd632":"markdown","bd385e02":"markdown","b664ca6c":"markdown","d1082393":"markdown","090f48ff":"markdown"},"source":{"6eb3bb0a":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","3232566c":"import os\nimport warnings\nimport math\nwarnings.filterwarnings('ignore')\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nplt.style.use('fivethirtyeight')\nfrom pylab import rcParams\nrcParams['figure.figsize'] = 10, 6\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.model_selection import train_test_split \nfrom sklearn import linear_model\nfrom sklearn.metrics import recall_score, precision_score, classification_report,accuracy_score,confusion_matrix, roc_curve, auc, roc_curve,accuracy_score,plot_confusion_matrix\nfrom sklearn.preprocessing import StandardScaler, normalize\nfrom scipy import ndimage\nimport seaborn as sns\n","6fb7bd91":"test_data = pd.read_csv('..\/input\/kepler-labelled-time-series-data\/exoTest.csv')\ntrain_data = pd.read_csv('..\/input\/kepler-labelled-time-series-data\/exoTrain.csv')","9def7d80":"train_data","7d640d2c":"categ = {2: 1,1: 0}\ntrain_data.LABEL = [categ[item] for item in train_data.LABEL]\ntest_data.LABEL = [categ[item] for item in test_data.LABEL]","c8928c97":"train_data.describe()","649034d2":"train_data['LABEL'].value_counts()","3fb6bb1f":"plt.figure(figsize=(4,8))\ncolors = [\"0\", \"1\"]\nsns.countplot('LABEL', data=train_data, palette = \"Set2\")\nplt.title('Class Distributions \\n (0: Not Exoplanet || 1: Exoplanet)', fontsize=14)","a0c24354":"sns.heatmap(train_data.isnull())","f4a98bf5":"plt.figure(figsize=(10,8))\nplt.title('Distribution of flux values', fontsize=15)\nplt.xlabel('Flux values')\nplt.ylabel('Flux intensity')\nplt.plot(train_data.iloc[0,])\nplt.plot(train_data.iloc[1,])\nplt.plot(train_data.iloc[2,])\nplt.plot(train_data.iloc[3,])\nplt.show()","88b035a0":"plt.figure(figsize=(15,15))\nsns.heatmap(train_data.corr())\nplt.title('Correlation in the data')\nplt.show()","1be8f173":"labels_1=[100,200,300]\nfor i in labels_1:\n    plt.figure(figsize=(3,3))\n    plt.hist(train_data.iloc[i,:], bins=200)\n    plt.title(\"Gaussian Histogram\")\n    plt.xlabel(\"Flux values\")\n    plt.show()","5592008b":"labels_1=[16,21,25]\nfor i in labels_1:\n    plt.figure(figsize=(3,3))\n    plt.hist(train_data.iloc[i,:], bins=200)\n    plt.title(\"Gaussian Histogram\")\n    plt.xlabel(\"Flux values\")\n    plt.show()","bbe06562":"fig, axes = plt.subplots(1, 5,figsize=(15, 6), sharey=True)\nfig.suptitle('Distribution of FLUX')\n\nsns.boxplot(ax=axes[0], data=train_data, x='LABEL', y='FLUX.1',palette=\"Set2\")\nsns.boxplot(ax=axes[1], data=train_data, x='LABEL', y='FLUX.2',palette=\"Set2\")\nsns.boxplot(ax=axes[2], data=train_data, x='LABEL', y='FLUX.3',palette=\"Set2\")\nsns.boxplot(ax=axes[3], data=train_data, x='LABEL', y='FLUX.4',palette=\"Set2\")\nsns.boxplot(ax=axes[4], data=train_data, x='LABEL', y='FLUX.5',palette=\"Set2\")\n","5131b6fa":"print('Dropping Outliers')\ntrain_data.drop(train_data[train_data['FLUX.1']>250000].index, axis=0, inplace=True)","8d85bdd9":"x_train = train_data.drop([\"LABEL\"],axis=1)\ny_train = train_data[\"LABEL\"]   \nx_test = test_data.drop([\"LABEL\"],axis=1)\ny_test = test_data[\"LABEL\"]","27aa0007":"x_train = normalized = normalize(x_train)\nx_test = normalize(x_test)","3526123a":"x_train = filtered = ndimage.filters.gaussian_filter(x_train, sigma=10)\nx_test = ndimage.filters.gaussian_filter(x_test, sigma=10)","1145e5ec":"#Feature scaling\nstd_scaler = StandardScaler()\nx_train = scaled = std_scaler.fit_transform(x_train)\nx_test = std_scaler.fit_transform(x_test)","7e36612c":"#K-NN\nfrom sklearn.neighbors import KNeighborsClassifier\nknn_model = KNeighborsClassifier()\n\nknn_model.fit(x_train,y_train)\nprediction=knn_model.predict(x_test)\nprint('Validation accuracy of KNN is', accuracy_score(prediction,y_test))\nprint (\"\\nClassification report :\\n\",(classification_report(y_test,prediction)))\n\n#Confusion matrix\nplt.figure(figsize=(13,10))\nplt.subplot(221)\nsns.heatmap(confusion_matrix(y_test,prediction),annot=True,cmap=\"viridis\",fmt = \"d\",linecolor=\"k\",linewidths=3)\nplt.title(\"CONFUSION MATRIX\",fontsize=20)\n\n#ROC curve and Area under the curve plotting\npredicting_probabilites = knn_model.predict_proba(x_test)[:,1]\nfpr,tpr,thresholds = roc_curve(y_test,predicting_probabilites)\nplt.subplot(222)\nplt.plot(fpr,tpr,label = (\"Area_under the curve :\",auc(fpr,tpr)),color = \"r\")\nplt.plot([1,0],[1,0],linestyle = \"dashed\",color =\"k\")\nplt.legend(loc = \"best\")\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title(\"ROC - CURVE & AREA UNDER CURVE\",fontsize=20)","041b472d":"#Logistic Regression\nfrom sklearn.linear_model import LogisticRegression\nlr_model = LogisticRegression(class_weight={0:100, 1:1})\n\nlr_model.fit(x_train,y_train)\n\nprediction=lr_model.predict(x_test)\n\nprint('Validation accuracy of Logistic Regression is', accuracy_score(prediction,y_test))\nprint (\"\\nClassification report :\\n\",(classification_report(y_test,prediction)))\n\n#Confusion matrix\nplt.figure(figsize=(13,10))\nplt.subplot(221)\nsns.heatmap(confusion_matrix(y_test,prediction),annot=True,cmap=\"viridis\",fmt = \"d\",linecolor=\"k\",linewidths=3)\nplt.title(\"CONFUSION MATRIX\",fontsize=20)\n\n#ROC curve and Area under the curve plotting\npredicting_probabilites = lr_model.predict_proba(x_test)[:,1]\nfpr,tpr,thresholds = roc_curve(y_test,predicting_probabilites)\nplt.subplot(222)\nplt.plot(fpr,tpr,label = (\"Area_under the curve :\",auc(fpr,tpr)),color = \"r\")\nplt.plot([1,0],[1,0],linestyle = \"dashed\",color =\"k\")\nplt.legend(loc = \"best\")\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title(\"ROC - CURVE & AREA UNDER CURVE\",fontsize=20)","2cf94787":"#Decision Tree\nfrom sklearn.tree import DecisionTreeClassifier\nds_model = DecisionTreeClassifier(max_depth=5, random_state=13)\n\nds_model.fit(x_train,y_train)\n\nprediction=ds_model.predict(x_test)\n\nprint('Validation accuracy of Decision Tree is', accuracy_score(prediction,y_test))\nprint (\"\\nClassification report :\\n\",(classification_report(y_test,prediction)))\n\n#Confusion matrix\nplt.figure(figsize=(13,10))\nplt.subplot(221)\nsns.heatmap(confusion_matrix(y_test,prediction),annot=True,cmap=\"viridis\",fmt = \"d\",linecolor=\"k\",linewidths=3)\nplt.title(\"CONFUSION MATRIX\",fontsize=20)\n\n#ROC curve and Area under the curve plotting\npredicting_probabilites = ds_model.predict_proba(x_test)[:,1]\nfpr,tpr,thresholds = roc_curve(y_test,predicting_probabilites)\nplt.subplot(222)\nplt.plot(fpr,tpr,label = (\"Area_under the curve :\",auc(fpr,tpr)),color = \"r\")\nplt.plot([1,0],[1,0],linestyle = \"dashed\",color =\"k\")\nplt.legend(loc = \"best\")\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title(\"ROC - CURVE & AREA UNDER CURVE\",fontsize=20)","0c858e28":"from imblearn.over_sampling import SMOTE\nmodel = SMOTE()\nov_train_x,ov_train_y = model.fit_sample(train_data.drop('LABEL',axis=1), train_data['LABEL'])\nov_train_y = ov_train_y.astype('int')\n\nov_train_y.value_counts().reset_index().plot(kind='bar', x='index', y='LABEL')\n","40f23e6c":"train_X, test_X, train_y, test_y = train_test_split(ov_train_x, ov_train_y, test_size=0.33, random_state=42)","6a7521e3":"from sklearn.tree import DecisionTreeClassifier\nds_model = DecisionTreeClassifier(max_depth=5, random_state=13)\n\nds_model.fit(train_X,train_y)\n\nprediction=ds_model.predict(test_X)\n\nprint('Validation accuracy of Decision Tree is', accuracy_score(prediction,test_y))\nprint (\"\\nClassification report :\\n\",(classification_report(test_y,prediction)))\n\n#Confusion matrix\nplt.figure(figsize=(13,10))\nplt.subplot(221)\nsns.heatmap(confusion_matrix(test_y,prediction),annot=True,cmap=\"viridis\",fmt = \"d\",linecolor=\"k\",linewidths=3)\nplt.title(\"CONFUSION MATRIX\",fontsize=20)\n\n#ROC curve and Area under the curve plotting\npredicting_probabilites = ds_model.predict_proba(test_X)[:,1]\nfpr,tpr,thresholds = roc_curve(test_y,predicting_probabilites)\nplt.subplot(222)\nplt.plot(fpr,tpr,label = (\"Area_under the curve :\",auc(fpr,tpr)),color = \"r\")\nplt.plot([1,0],[1,0],linestyle = \"dashed\",color =\"k\")\nplt.legend(loc = \"best\")\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title(\"ROC - CURVE & AREA UNDER CURVE\",fontsize=20)","1d9ef3de":"knn_model = KNeighborsClassifier()\n\nknn_model.fit(train_X,train_y)\n\nprediction=knn_model.predict(test_X)\n\nprint('Validation accuracy of KNN is', accuracy_score(prediction,test_y))\nprint (\"\\nClassification report :\\n\",(classification_report(test_y,prediction)))\n\n#Confusion matrix\nplt.figure(figsize=(13,10))\nplt.subplot(221)\nsns.heatmap(confusion_matrix(test_y,prediction),annot=True,cmap=\"viridis\",fmt = \"d\",linecolor=\"k\",linewidths=3)\nplt.title(\"CONFUSION MATRIX\",fontsize=20)\n\n#ROC curve and Area under the curve plotting\npredicting_probabilites = knn_model.predict_proba(test_X)[:,1]\nfpr,tpr,thresholds = roc_curve(test_y,predicting_probabilites)\nplt.subplot(222)\nplt.plot(fpr,tpr,label = (\"Area_under the curve :\",auc(fpr,tpr)),color = \"r\")\nplt.plot([1,0],[1,0],linestyle = \"dashed\",color =\"k\")\nplt.legend(loc = \"best\")\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title(\"ROC - CURVE & AREA UNDER CURVE\",fontsize=20)","fbd1e73c":"lr_model = LogisticRegression(class_weight={0:100, 1:1})\n\nlr_model.fit(train_X,train_y)\n\nprediction=lr_model.predict(test_X)\n\nprint('Validation accuracy of Logistic Regression is', accuracy_score(prediction,test_y))\nprint (\"\\nClassification report :\\n\",(classification_report(test_y,prediction)))\n\n#Confusion matrix\nplt.figure(figsize=(13,10))\nplt.subplot(221)\nsns.heatmap(confusion_matrix(test_y,prediction),annot=True,cmap=\"viridis\",fmt = \"d\",linecolor=\"k\",linewidths=3)\nplt.title(\"CONFUSION MATRIX\",fontsize=20)\n\n#ROC curve and Area under the curve plotting\npredicting_probabilites = lr_model.predict_proba(test_X)[:,1]\nfpr,tpr,thresholds = roc_curve(test_y,predicting_probabilites)\nplt.subplot(222)\nplt.plot(fpr,tpr,label = (\"Area_under the curve :\",auc(fpr,tpr)),color = \"r\")\nplt.plot([1,0],[1,0],linestyle = \"dashed\",color =\"k\")\nplt.legend(loc = \"best\")\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title(\"ROC - CURVE & AREA UNDER CURVE\",fontsize=20)","2ec0f3bb":"### Apply gaussian filters","6aa54d91":"### Load all the required libraries","998eb736":"### Balancing the class using SMOTE","b298bbc4":"**Data Normalization** is a technique often applied as part of data preparation for machine learning. The goal of normalization is to change the values of numeric columns in the dataset to a common scale, without distorting differences in the ranges of values.","2203623c":"### Model Building","64923b43":"### Let us plot the first 4 rows of the train data and observe the intensity of flux values.","e9db6751":"### plot the Gaussian histogram of non-exoplanets data.","6032b3d3":"So let us first split our dataset and normalize it.","f8ab9f0c":"Since the dataset is highly imbalanced even if the model predicts the same class for all data points the accuracy.\n\nSo let us first balance the classes and model it again.","4b16f535":"## What are exoplanets?\n\nExoplanets are planets beyond our own solar system. Thousands have been discovered in the past two decades, mostly with NASA\u2019s Kepler Space Telescope.\n\nThese exoplanets come in a huge variety of sizes and orbits. Some are gigantic planets hugging close to their parent stars; others are icy, some rocky. NASA and other agencies are looking for a special kind of planet: one that\u2019s the same size as Earth, orbiting a sun-like star in the habitable zone.\n\n\nThe habitable zone is the area around a star where it is not too hot and not too cold for liquid water to exist on the surface of surrounding planets. Imagine if Earth was where Pluto is. The Sun would be barely visible (about the size of a pea) and Earth\u2019s ocean and much of its atmosphere would freeze.","5e399f34":"### Detecting outliers using Boxplot","c66bd019":"It turns out that the data is highly imbalanced so later on we'll be using some sampling techniques to balance the data.","36a0bd9f":"### Plotting heatmap of missing values","656d34bb":"You can also check out my article to understand more about exoplanets at [www.theaidream.com](https:\/\/www.theaidream.com\/post\/exoplanet-exploration-using-machine-learning)","79b47865":"As we can observe after applying SMOTE to balance the classes, our ML models are performing really good.","4e4e41c6":"**Flux :** the intensity of light recived from a planet","0d02561f":"We can clearly see that we dont have any missing values in our dataset.","cc306101":"We have 5050 records of non-exoplanets and 37 records of exoplanets.","682399b1":"Now the target column LABEL consists of two categories 1(Does not represents exoplanet) and 2(represents the presence of exoplanet). So, convert them to binary values for easier processing of data.","6c2c7722":"### Plotting the correlation matrix","9a72f6ee":"### plot Gaussian histogram of the data when exoplanets are present.","73ed91fe":"### Data Normalization","beb8388c":"### Load the train and test data.","39bea2e5":"Now visualize the target column in the train_dataset and get an idea about the class distribution.","919db664":"The next step is to apply gaussian filters to both test and train.\n\n\nIn probability theory, the normal (or Gaussian or Gauss or Laplace\u2013Gauss) distribution is a very common continuous probability distribution. Normal distributions are important in statistics and are often used in the natural and social sciences to represent real-valued random variables whose distributions are not known.","ac392301":"### Split the dataset","bf236342":"we use feature scaling so that all the values remain in the comparable range.","4ea4fd0b":"# Exoplanet exploration using Machine Learning\n","67fdd632":"**SMOTE** **(synthetic minority oversampling technique)** is one of the most commonly used oversampling methods to solve the imbalance problem. It aims to balance class distribution by randomly increasing minority class examples by replicating them. **SMOTE** synthesises new minority instances between existing minority instances.","bd385e02":"### Feature scaling","b664ca6c":"Box Plot is the visual representation of the depicting groups of numerical data through their quartiles. Boxplot is also used for detect the outlier in data set. It captures the summary of the data efficiently with a simple box and whiskers and allows us to compare easily across groups. Boxplot summarizes a sample data using 25th, 50th and 75th percentiles. ","d1082393":"### Split the dataset","090f48ff":"![TRAPPIST-1f_Artist's_Impression.png](attachment:TRAPPIST-1f_Artist's_Impression.png)"}}