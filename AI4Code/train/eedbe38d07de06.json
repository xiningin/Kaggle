{"cell_type":{"2592b97e":"code","962888dc":"code","dde432a9":"code","49af0b3c":"code","bee0118a":"code","2b348dc1":"code","efdd8f2f":"code","ed8af8fe":"code","0ead2c2b":"markdown"},"source":{"2592b97e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","962888dc":"dataset_folder = \"\/kaggle\/input\/fer2013\/\"\n\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport os\nimport pandas as pd\n%matplotlib inline\n\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.layers import Dense, Input, Dropout,Flatten, Conv2D\nfrom tensorflow.keras.layers import BatchNormalization, Activation, MaxPooling2D\nfrom tensorflow.keras.models import Model, Sequential\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\nfrom tensorflow.keras.utils import plot_model\n\nfrom keras.callbacks import CSVLogger\n\nfrom IPython.display import SVG, Image\nimport tensorflow as tf\nprint(\"Tensorflow version:\", tf.__version__)","dde432a9":"!pip install livelossplot\n# from livelossplot import PlotLossesTensorFlowKeras\nfrom livelossplot import PlotLossesKeras\nfrom livelossplot.keras import PlotLossesCallback","49af0b3c":"for expression in os.listdir(dataset_folder + \"\/train\/\"):\n    print(str(len(os.listdir(dataset_folder + \"\/train\/\" + expression))) + \" \" + expression + \" images\")","bee0118a":"from keras import backend as K\n\ndef recall_m(y_true, y_pred):\n    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n    recall = true_positives \/ (possible_positives + K.epsilon())\n    return recall\n\ndef precision_m(y_true, y_pred):\n    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n    precision = true_positives \/ (predicted_positives + K.epsilon())\n    return precision\n\ndef f1_m(y_true, y_pred):\n    precision = precision_m(y_true, y_pred)\n    recall = recall_m(y_true, y_pred)\n    return 2*((precision*recall)\/(precision+recall+K.epsilon()))","2b348dc1":"from keras.initializers import  RandomNormal\n\nimg_size = 48\nbatch_size = 32\n\nclass_weight = {0:1, 1:1, 2:1, 3:1, 4:1, 5:10, 6:1}\n\ndatagen_train = ImageDataGenerator(horizontal_flip = True)\ntrain_generator = datagen_train.flow_from_directory(\n    dataset_folder + \"\/train\/\",\n    target_size=(img_size, img_size),\n    color_mode='grayscale',\n    batch_size=batch_size,\n    class_mode='categorical',\n    shuffle=True\n)\n\ndatagen_validation = ImageDataGenerator(horizontal_flip = True)\nvalidation_generator = datagen_train.flow_from_directory(\n    dataset_folder + \"\/test\/\",\n    target_size=(img_size, img_size),\n    color_mode='grayscale',\n    batch_size=batch_size,\n    class_mode='categorical',\n    shuffle=False\n)\n\n# Constructing CNN structure\nmodel = Sequential()\n\n# 1st convolution layer\nmodel.add(Conv2D(64, (3, 3), activation='relu', padding = 'same', input_shape=(48,48,1), bias_initializer=RandomNormal(stddev=1), kernel_initializer=RandomNormal(stddev=1)))\nmodel.add(Conv2D(64, (3, 3), activation='relu', padding = 'same', input_shape=(48,48,1), bias_initializer=RandomNormal(stddev=1), kernel_initializer=RandomNormal(stddev=1)))\nmodel.add(BatchNormalization())\nmodel.add(MaxPooling2D(pool_size=(3,3), strides=(2, 2)))\nmodel.add(Dropout(0.25))\n\n# 3rd convolution layer\nmodel.add(Conv2D(64, (3, 3), activation='relu', padding = 'same', bias_initializer=RandomNormal(stddev=1), kernel_initializer=RandomNormal(stddev=1)))\nmodel.add(Conv2D(64, (3, 3), activation='relu', padding = 'same', bias_initializer=RandomNormal(stddev=1), kernel_initializer=RandomNormal(stddev=1)))\nmodel.add(BatchNormalization())\nmodel.add(MaxPooling2D(pool_size=(3,3), strides=(2, 2)))\nmodel.add(Dropout(0.25))\n          \n# 5th convolution layer\nmodel.add(Conv2D(128, (3, 3), activation='relu', padding = 'same', bias_initializer=RandomNormal(stddev=1), kernel_initializer=RandomNormal(stddev=1)))\nmodel.add(Conv2D(128, (3, 3), activation='relu', padding = 'same', bias_initializer=RandomNormal(stddev=1), kernel_initializer=RandomNormal(stddev=1)))\nmodel.add(BatchNormalization())\nmodel.add(MaxPooling2D(pool_size=(3,3), strides=(2, 2)))\nmodel.add(Dropout(0.25))\n\n# 7th convolution layer\nmodel.add(Conv2D(256, (3, 3), activation='relu', padding = 'same', bias_initializer=RandomNormal(stddev=1), kernel_initializer=RandomNormal(stddev=1)))\nmodel.add(Conv2D(256, (3, 3), activation='relu', padding = 'same', bias_initializer=RandomNormal(stddev=1), kernel_initializer=RandomNormal(stddev=1)))\nmodel.add(BatchNormalization())\nmodel.add(MaxPooling2D(pool_size=(3,3), strides=(2, 2)))\nmodel.add(Dropout(0.5))\n\n\nmodel.add(Flatten())\n# Fully connected layers\nmodel.add(Dense(2048, activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(7, activation='softmax'))\n\nmodel.compile(loss='categorical_crossentropy', optimizer=Adam(), metrics=['accuracy', f1_m,precision_m, recall_m])\nmodel.summary()","efdd8f2f":"from keras.callbacks import CSVLogger\n\ncsv_logger = CSVLogger(\"model_history_log.csv\", append=True)\n\n","ed8af8fe":"epochs = 200\nsteps_per_epoch = train_generator.n \/\/train_generator.batch_size\nvalidation_steps = validation_generator.n \/\/validation_generator.batch_size\n\ncheckpoint = ModelCheckpoint(\n    \"model_weights.h5\",\n    monitor='val_accuracy',\n    save_weights_only=True,\n    mode='max',\n    verbose=1\n)\nreduce_lr = ReduceLROnPlateau(\n    monitor=\"val_loss\",\n    factor=0.1,\n    patience=2,\n    min_lr=0.001,\n    model='auto'\n)\n\ncallbacks = [PlotLossesCallback(), checkpoint, reduce_lr, csv_logger] #PlotLossesTensorFlowKeras()\n\nhistory = model.fit(\n    x=train_generator,\n    steps_per_epoch=steps_per_epoch,\n    epochs=epochs,\n    validation_data=validation_generator,\n    validation_steps=validation_steps,\n    callbacks=callbacks, \n)\n\nprint(history.history)\n\npd.DataFrame(history.history).to_csv(\"model_4_history.csv\")\n\nmodel_json = model.to_json()\nwith open(\"model_4.json\", \"w\") as json_file:\n    json_file.write(model_json)","0ead2c2b":"# **My Course work**"}}