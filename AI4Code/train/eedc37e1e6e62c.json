{"cell_type":{"b6206a69":"code","0deb3714":"code","c2397084":"code","9c134073":"code","f33cd0f1":"code","1b4dba71":"code","6e2da217":"code","f54b2e14":"code","238df3cd":"code","07aa2f59":"code","fd2a4d37":"code","bb31e2fb":"code","86f2a438":"code","4a889fab":"code","642b72af":"code","9f743f92":"code","ccac53f2":"code","0769174e":"code","f81b613e":"code","0008789c":"code","6186e3a2":"code","460e1c3e":"code","fa6cc5f8":"code","313309c1":"code","2bbb13b2":"code","146abae3":"code","851b3fd5":"code","6a88e59f":"code","4dcc5f32":"code","3f11649d":"code","1f80a4c8":"code","1f849413":"markdown","c713e7d1":"markdown","8a967fde":"markdown","f9f8f3e4":"markdown","64c4c381":"markdown","017d3f83":"markdown","7ffd6e65":"markdown","e94684fa":"markdown","1acdaafb":"markdown","67dfc913":"markdown","b73783ae":"markdown"},"source":{"b6206a69":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \nimport matplotlib.pyplot as plt","0deb3714":"%%time\ndf= pd.read_csv('\/kaggle\/input\/fashionmnist\/fashion-mnist_train.csv')\n\ndisplay(df.head())\nprint()\nprint(df.shape,'\\n')\ndisplay(df.info())\nprint()\ndisplay(df.describe())\n","c2397084":"# Checking the distribution of Label\ndf.label.hist();","9c134073":"y=df.iloc[:,0].values  ### Storing labels in y\nx=df.iloc[:,1:].values ### Storing complete data base except label","f33cd0f1":"# Checking the data type x and y\nprint('Data type of y i.e label       : ', y.dtype)\nprint('Data type of x i.e image pixel : ', x.dtype)","1b4dba71":"x[0]","6e2da217":"## Reshaping the data into 28x28\na= x[0].reshape(28,28)\na","f54b2e14":"plt.imshow(a)","238df3cd":"plt.figure(figsize=(20,20))\nfor i in range(36):\n    a= x[i].reshape(28,28)\n    plt.subplot(6,6,(i+1))\n    plt.imshow(a)","07aa2f59":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(x,y,test_size=0.20)","fd2a4d37":"from sklearn.neighbors import KNeighborsClassifier","bb31e2fb":"'''\n%%time\n################ Choosing a K Value  ##############################################\n\n#Let's go ahead and use the elbow method to pick a good K Value:\n\nerror_rate = []\n\n\n# Will take some time\n\nfor i in range(1,15):  \n    knn = KNeighborsClassifier(n_neighbors=i)\n    knn.fit(X_train,y_train)\n    pred_i = knn.predict(X_test)\n    error_rate.append(np.mean(pred_i != y_test))\n\nplt.figure(figsize=(10,6))\n\nplt.plot(range(1,15),error_rate,color='blue', linestyle='dashed', marker='o',markerfacecolor='red', markersize=10)\n\nplt.title('Error Rate vs. K Value')\n\nplt.xlabel('K')\n\nplt.ylabel('Error Rate')\n''';\n\n# This was taking a lot of time thus had to turn it off","86f2a438":"clf= KNeighborsClassifier(n_neighbors=5)","4a889fab":"clf.fit(X_train,y_train)","642b72af":"%%time\ny_pred= clf.predict(X_test)","9f743f92":"from sklearn.metrics import accuracy_score\nfrom sklearn.metrics import classification_report,confusion_matrix\nfrom sklearn.model_selection import cross_val_score","ccac53f2":"accuracy_score(y_test,y_pred)","0769174e":"print(confusion_matrix(y_test,y_pred))","f81b613e":"print(classification_report(y_test,y_pred))","0008789c":"# Feature Scaling\nfrom sklearn.preprocessing import StandardScaler\nsc= StandardScaler()                             ### this function scale the data to 0 to 1\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)","6186e3a2":"%%time\n\n#Apply PCA\nfrom sklearn.decomposition import PCA\n# Now creating an object \npca=PCA(n_components=None)   # present feature 786, we will create 786 features (n_components=None) too but won't use them all\n\nX_train_new= pca.fit_transform(X_train)\nX_test_new= pca.fit_transform(X_test)\n\n# we will get same shaped  \nprint(X_train.shape)\nprint(X_train_new.shape)","460e1c3e":"pca.explained_variance_ratio_.shape","fa6cc5f8":"pca.explained_variance_ratio_","313309c1":"%%time\n\n# We will now consider top 50 columns \n\n#Apply PCA\nfrom sklearn.decomposition import PCA\n# Now creating an object \npca=PCA(n_components=50)   # present feature 786, we will create top 5 features (n_components=50)\nX_train_new1= pca.fit_transform(X_train)\nX_test_new1= pca.fit_transform(X_test)\n\n# we will get same shaped  \nprint(\"Original Dataset  : \",X_train.shape)\nprint(\"after PCA Dataset : \",X_train_new1.shape)","2bbb13b2":"%%time\nclf.fit(X_train_new1,y_train)","146abae3":"%%time\ny_pred_new= clf.predict(X_test_new1)","851b3fd5":"accuracy_score(y_test, y_pred_new)","6a88e59f":"print(\"Classification Report\",'\\n')\nprint(classification_report(y_test,y_pred_new),'\\n')\nprint()\nprint(\"Classification old Report\",'\\n')\nprint(classification_report(y_test,y_pred),'\\n')\n","4dcc5f32":"%%time\naccuracy=[]  # empty list\nfor i in range(1,100):     # u can try upto 785                    \n    pca=PCA(n_components=i)\n    x_train1= pca.fit_transform(X_train)\n    x_test1 = pca.transform(X_test)\n    clf.fit(x_train1,y_train)\n    y_pred_try= clf.predict(x_test1)\n    t=accuracy_score(y_test,y_pred_try)\n    print('No of dimension : ',i,'  Accuracy = ',t)\n    accuracy.append(t)","3f11649d":"# accuracy","1f80a4c8":"y_pred.shape","1f849413":"# Finding optimum no of Dimensions PCA","c713e7d1":"# Choosing a K Value\nHad to cancel as it was taking too much time","8a967fde":"#### Label seems perfectly distributed","f9f8f3e4":"* Har column mera dataset ka kitna variance explain kar paraha hai ye bata raha hai\n* 1st column can explain 22% variance\n* 2nd column can explain 14% variance\n* 3rd column can explain 5% variance\n* 4th column can explain 4% variance\nand so on","64c4c381":"# Now Using `PCA` we will reduce no of features\n* Step 1 : Feature Scaling (will convert all value between 0-1{Currently it's between 0-225})\n* Step 2 : Apply PCA","017d3f83":"# Evaluation","7ffd6e65":"### Creating train test split","e94684fa":"# Introduction To PCA\nIn this notebook we will try to find why we need to perform PCA  \n[Answer](#99)","1acdaafb":"## Now we will fit KNN on PCA dataset","67dfc913":"<a id=1>\n    \n\n# KNN\n* KNN is used for Non-Linear data set\n* It comes under `Supervised Learning`","b73783ae":"<a id=0>\n    \n\n## Content\n1. [KNN](#1)\n2. [Performing PCA](#2)\n3. [KNN on Steriod](#3)"}}