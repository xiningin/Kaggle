{"cell_type":{"002c0af6":"code","98017bfa":"code","1b7b694e":"code","01d1fd41":"code","8d5d2347":"code","da9bfb58":"code","2e477910":"code","352b9baf":"code","4ac53ecc":"code","e991fcfa":"code","872f201f":"code","1d55e6af":"code","f7761971":"code","95258d81":"code","96ec2d11":"code","4c696107":"code","c482e1e5":"code","b2d5d653":"code","e174cb12":"code","05e5667e":"code","7b5d653d":"code","c3ef1818":"code","d3676f49":"code","41d96a48":"code","8b94d33b":"code","83432beb":"code","e108077e":"code","72f204fe":"code","2306cdd1":"code","f91497de":"code","809c8d16":"code","96eb89d5":"code","20e6ab13":"code","46b294dc":"code","285bdfb6":"code","c9bf12f5":"code","1b018dec":"code","a65fbb1d":"code","32215447":"code","61347222":"code","73a19e46":"code","03341cae":"code","6e5cb1c6":"code","e8a0e26a":"code","c342c291":"code","ac508f48":"code","2ab91c89":"code","878724dc":"code","5bd4d727":"code","7275c24a":"code","658aaa8e":"code","3fbe4444":"code","aca331d0":"code","79df21be":"code","506abe60":"code","15b64a02":"code","30edfafe":"code","6d17546d":"code","c6cc2837":"code","cb398360":"code","27841bb5":"code","56f2e490":"code","1eec3839":"code","1654e74d":"code","70db7610":"code","25650c9f":"code","f0973197":"code","cccda0d8":"code","477cea79":"code","19040a17":"code","a4422798":"code","6b5755fb":"code","80f8b457":"code","a9c989af":"code","3bd20f30":"code","e5d78bba":"code","9bf6dd68":"code","fa9feb47":"code","74a89883":"code","4c6dac62":"code","c494e140":"code","822373f9":"code","63cdef4c":"code","61b744c2":"code","8b65de83":"code","bba39bff":"code","a05ca91c":"code","894fb428":"code","2e5136db":"code","a2529cb3":"code","876b1804":"code","cfe5e8ae":"code","01e8c2ca":"code","60f7f1e3":"code","78f554ae":"code","4636081a":"code","7bbd2316":"code","16a83a50":"code","f1566c9b":"code","5e8a70a0":"code","b1d6bf94":"code","6979f757":"code","00f83659":"code","51fbdbe5":"code","6c88c73b":"code","d8265734":"code","07c58c43":"code","388fdc16":"code","01764917":"code","f19fea57":"code","70793bde":"code","368d2ed1":"code","33c963f0":"code","1dcb1cc9":"code","0c8385dd":"code","ef38c7a2":"code","2d74478c":"code","ece11815":"code","f6fcecf5":"code","ea85ab74":"code","cfde6488":"code","c9a36d77":"code","1c50bf18":"code","0260e4e6":"code","b795fb9b":"code","37d954eb":"code","43a9a395":"code","c1cf3dca":"code","ecaac475":"code","f963abde":"code","9bb9a38f":"code","38622821":"code","2ad4f127":"code","6445ffd1":"code","8e11643d":"code","f535bb8d":"code","4eee8f41":"code","10f970bf":"code","48cad8be":"code","3dfb09f1":"code","155a531a":"code","6a353e82":"code","ac983d4c":"code","31101d35":"code","e920a059":"code","c824ba08":"code","0cb9ae30":"code","e6ff9132":"code","f53d5499":"code","ba041b66":"code","7c01ac68":"code","3cb3a71b":"code","e406f4e5":"code","b0c37b0b":"code","e5c70c08":"code","b6538c2c":"code","b25dc0f1":"code","83827563":"code","7f48a697":"code","3f772307":"code","4b55cb47":"code","a7d9800b":"code","ce3adaba":"code","72285c3a":"code","ada548f3":"code","ad34085e":"code","b13df991":"code","222149da":"code","4fed9cb0":"code","bbc38bc5":"code","48ed2cdf":"code","ad9c17c4":"code","c8844752":"code","3282ffb2":"code","841a25d2":"code","1ecdbd39":"code","17fc3ea4":"code","538484f1":"code","4f918a1a":"code","6fdf2f64":"code","305a8eec":"code","d74fd836":"code","e6908d85":"code","390d4a27":"code","e0940550":"code","58732b5b":"code","917e7e55":"code","b8b19820":"code","4e4ec98b":"markdown","a52f7355":"markdown","2971f048":"markdown","0d4bda91":"markdown","866ef597":"markdown","2eb65d2b":"markdown","bed61617":"markdown","55411577":"markdown","9375f662":"markdown","0724de4e":"markdown","370a76ce":"markdown","57fd67c2":"markdown","011314c5":"markdown","00bf167f":"markdown","2615a054":"markdown","2f6c465c":"markdown","57d2db93":"markdown","4e4c8767":"markdown","d33f9d8d":"markdown","47eeb9b0":"markdown","9c7e0bdc":"markdown","bf496bef":"markdown","16414b49":"markdown","15e51a1d":"markdown","bc752f79":"markdown","9f3b99dc":"markdown","36806ddb":"markdown","da09b776":"markdown","260ad23b":"markdown","9fc0545c":"markdown","477fd071":"markdown","284941c5":"markdown","a4bdeb30":"markdown","df44982c":"markdown","720cac06":"markdown","dfc95bc7":"markdown","12053eae":"markdown","ac521400":"markdown","099d6459":"markdown","34b6a1c6":"markdown","7e08ec03":"markdown","245b688d":"markdown","f8906b63":"markdown","c1ff1fb5":"markdown","5572b14b":"markdown","6d4d2cba":"markdown","bb6e1833":"markdown","5b4b0b21":"markdown","aed0eab6":"markdown","792e30d5":"markdown","a90f06c5":"markdown"},"source":{"002c0af6":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nsecret_value_0 = user_secrets.get_secret(\"bloodcell_dataset\")\nsecret_value_1 = user_secrets.get_secret(\"bloodcell_dataset_darknet\")\n\n!curl -L -q https:\/\/public.roboflow.com\/ds\/$secret_value_0 > roboflow.zip; unzip roboflow.zip; rm roboflow.zip\n\nimport yaml\nwith open(\"data.yaml\", 'r') as stream:\n    num_classes = str(yaml.safe_load(stream)['nc'])\n    \n    \nnum_classes","98017bfa":"\n\n#customize iPython writefile so we can write variables\nfrom IPython.core.magic import register_line_cell_magic\n\n@register_line_cell_magic\ndef writetemplate(line, cell):\n    with open(line, 'w') as f:\n        f.write(cell.format(**globals()))","1b7b694e":"!git clone https:\/\/github.com\/ultralytics\/yolov3  # master branch (default)\n    \n%cd yolov3\n","01d1fd41":"%%writefile data.yaml\n\ntrain: \/kaggle\/working\/train\/images\nval: \/kaggle\/working\/valid\/images\n\nnc: 3\nnames: ['Platelets', 'RBC', 'WBC']","8d5d2347":"!pip install -q -r requirements.txt ","da9bfb58":"!python detect.py --source data\/images --weights yolov3.pt --conf 0.25","2e477910":"from PIL import Image\n\nimg1 = Image.open('runs\/detect\/exp\/zidane.jpg')\nimg1","352b9baf":"img2 = Image.open('runs\/detect\/exp\/bus.jpg')\nimg2","4ac53ecc":"!ls weights","e991fcfa":"!wandb offline","872f201f":"%%writetemplate .\/models\/custom_yolov3.yaml\n\n# parameters\nnc: {num_classes}   # number of classes\ndepth_multiple: 0.33  # model depth multiple\nwidth_multiple: 0.50\n\n# anchors\nanchors:\n  - [10,13, 16,30, 33,23]  # P3\/8\n  - [30,61, 62,45, 59,119]  # P4\/16\n  - [116,90, 156,198, 373,326]  # P5\/32\n\n# darknet53 backbone\nbackbone:\n  # [from, number, module, args]\n  [[-1, 1, Conv, [32, 3, 1]],  # 0\n   [-1, 1, Conv, [64, 3, 2]],  # 1-P1\/2\n   [-1, 1, Bottleneck, [64]],\n   [-1, 1, Conv, [128, 3, 2]],  # 3-P2\/4\n   [-1, 2, Bottleneck, [128]],\n   [-1, 1, Conv, [256, 3, 2]],  # 5-P3\/8\n   [-1, 8, Bottleneck, [256]],\n   [-1, 1, Conv, [512, 3, 2]],  # 7-P4\/16\n   [-1, 8, Bottleneck, [512]],\n   [-1, 1, Conv, [1024, 3, 2]],  # 9-P5\/32\n   [-1, 4, Bottleneck, [1024]],  # 10\n  ]\n\n# YOLOv3 head\nhead:\n  [[-1, 1, Bottleneck, [1024, False]],\n   [-1, 1, Conv, [512, [1, 1]]],\n   [-1, 1, Conv, [1024, 3, 1]],\n   [-1, 1, Conv, [512, 1, 1]],\n   [-1, 1, Conv, [1024, 3, 1]],  # 15 (P5\/32-large)\n\n   [-2, 1, Conv, [256, 1, 1]],\n   [-1, 1, nn.Upsample, [None, 2, 'nearest']],\n   [[-1, 8], 1, Concat, [1]],  # cat backbone P4\n   [-1, 1, Bottleneck, [512, False]],\n   [-1, 1, Bottleneck, [512, False]],\n   [-1, 1, Conv, [256, 1, 1]],\n   [-1, 1, Conv, [512, 3, 1]],  # 22 (P4\/16-medium)\n\n   [-2, 1, Conv, [128, 1, 1]],\n   [-1, 1, nn.Upsample, [None, 2, 'nearest']],\n   [[-1, 6], 1, Concat, [1]],  # cat backbone P3\n   [-1, 1, Bottleneck, [256, False]],\n   [-1, 2, Bottleneck, [256, False]],  # 27 (P3\/8-small)\n\n   [[27, 22, 15], 1, Detect, [nc, anchors]],   # Detect(P3, P4, P5)\n  ]","1d55e6af":"!python train.py --img 416 --batch 16 --epochs 40 --data \/kaggle\/working\/data.yaml --cfg .\/models\/custom_yolov3.yaml --weights yolov3.pt --name yolov3_results  --cache","f7761971":"!ls models","95258d81":"%cd ..\n!git clone https:\/\/github.com\/ultralytics\/yolov5","96ec2d11":"%cd yolov5","4c696107":"!pip install -q -U -r requirements.txt  # install dependencies\n\nimport torch\nfrom IPython.display import Image  # for displaying images\nfrom utils.google_utils import gdrive_download  # for downloading models\/datasets\nprint('torch %s %s' % (torch.__version__, torch.cuda.get_device_properties(0) if torch.cuda.is_available() else 'CPU'))\n","c482e1e5":"!python detect.py --weights yolov5x.pt --img 640 --conf 0.25 --source data\/images\/\n","b2d5d653":"Image(filename='runs\/detect\/exp\/zidane.jpg', width=600)","e174cb12":"Image(filename='runs\/detect\/exp\/bus.jpg', width=600)","05e5667e":"# torch.hub.download_url_to_file('https:\/\/github.com\/ultralytics\/yolov5\/releases\/download\/v1.0\/coco2017val.zip', 'tmp.zip')\n# !unzip -q tmp.zip -d ..\/ && rm tmp.zip","7b5d653d":"# !python test.py --weights yolov5x.pt --data coco.yaml --img 640 --iou 0.65","c3ef1818":"%%writetemplate .\/models\/custom_yolov5s.yaml\n\n# parameters\nnc: {num_classes}  # number of classes\ndepth_multiple: 0.33  # model depth multiple\nwidth_multiple: 0.50  # layer channel multiple\n\n# anchors\nanchors:\n  - [10,13, 16,30, 33,23]  # P3\/8\n  - [30,61, 62,45, 59,119]  # P4\/16\n  - [116,90, 156,198, 373,326]  # P5\/32\n\n# YOLOv5 backbone\nbackbone:\n  # [from, number, module, args]\n  [[-1, 1, Focus, [64, 3]],  # 0-P1\/2\n   [-1, 1, Conv, [128, 3, 2]],  # 1-P2\/4\n   [-1, 3, BottleneckCSP, [128]],\n   [-1, 1, Conv, [256, 3, 2]],  # 3-P3\/8\n   [-1, 9, BottleneckCSP, [256]],\n   [-1, 1, Conv, [512, 3, 2]],  # 5-P4\/16\n   [-1, 9, BottleneckCSP, [512]],\n   [-1, 1, Conv, [1024, 3, 2]],  # 7-P5\/32\n   [-1, 1, SPP, [1024, [5, 9, 13]]],\n   [-1, 3, BottleneckCSP, [1024, False]],  # 9\n  ]\n\n# YOLOv5 head\nhead:\n  [[-1, 1, Conv, [512, 1, 1]],\n   [-1, 1, nn.Upsample, [None, 2, 'nearest']],\n   [[-1, 6], 1, Concat, [1]],  # cat backbone P4\n   [-1, 3, BottleneckCSP, [512, False]],  # 13\n\n   [-1, 1, Conv, [256, 1, 1]],\n   [-1, 1, nn.Upsample, [None, 2, 'nearest']],\n   [[-1, 4], 1, Concat, [1]],  # cat backbone P3\n   [-1, 3, BottleneckCSP, [256, False]],  # 17 (P3\/8-small)\n\n   [-1, 1, Conv, [256, 3, 2]],\n   [[-1, 14], 1, Concat, [1]],  # cat head P4\n   [-1, 3, BottleneckCSP, [512, False]],  # 20 (P4\/16-medium)\n\n   [-1, 1, Conv, [512, 3, 2]],\n   [[-1, 10], 1, Concat, [1]],  # cat head P5\n   [-1, 3, BottleneckCSP, [1024, False]],  # 23 (P5\/32-large)\n\n   [[17, 20, 23], 1, Detect, [nc, anchors]],  # Detect(P3, P4, P5)\n  ]","d3676f49":"# import wandb\n\n# secret_value_1 = user_secrets.get_secret(\"wandb_login\")\n# wandb.login(key = secret_value_1)","41d96a48":"!wandb offline","8b94d33b":"%cd \/kaggle\/working\/yolov5\/\n\n!python train.py --img 416 --batch 16 --epochs 40 --data \/kaggle\/working\/data.yaml --cfg .\/models\/custom_yolov5s.yaml --weights yolov5x.pt --name yolov5s_results  --cache","83432beb":"# torch.hub.download_url_to_file('https:\/\/github.com\/ultralytics\/yolov5\/releases\/download\/v1.0\/coco128.zip', 'tmp.zip')\n# !unzip -q tmp.zip -d ..\/ && rm tmp.zip","e108077e":"# %reload_ext tensorboard\n# %tensorboard --logdir runs\/train","72f204fe":"# !wandb offline","2306cdd1":"# !python train.py --img 640 --batch 16 --epochs 3 --data coco128.yaml --weights yolov5s.pt --nosave --cache","f91497de":"# from utils.plots import plot_results \n# plot_results(save_dir='runs\/train\/exp')  # plot all results*.txt as results.png\n# Image(filename='runs\/train\/exp\/results.png', width=800)","809c8d16":"%cd \/kaggle\/working\n!git clone https:\/\/github.com\/AlexeyAB\/darknet.git","96eb89d5":"%cd darknet","20e6ab13":"# This cell ensures you have the correct architecture for your respective GPU\n# If you command is not found, look through these GPUs, find the respective\n# GPU and add them to the archTypes dictionary\n\n# Tesla V100\n# ARCH= -gencode arch=compute_70,code=[sm_70,compute_70]\n\n# Tesla K80 \n# ARCH= -gencode arch=compute_37,code=sm_37\n\n# GeForce RTX 2080 Ti, RTX 2080, RTX 2070, Quadro RTX 8000, Quadro RTX 6000, Quadro RTX 5000, Tesla T4, XNOR Tensor Cores\n# ARCH= -gencode arch=compute_75,code=[sm_75,compute_75]\n\n# Jetson XAVIER\n# ARCH= -gencode arch=compute_72,code=[sm_72,compute_72]\n\n# GTX 1080, GTX 1070, GTX 1060, GTX 1050, GTX 1030, Titan Xp, Tesla P40, Tesla P4\n# ARCH= -gencode arch=compute_61,code=sm_61\n\n# GP100\/Tesla P100 - DGX-1\n# ARCH= -gencode arch=compute_60,code=sm_60\n\n# For Jetson TX1, Tegra X1, DRIVE CX, DRIVE PX - uncomment:\n# ARCH= -gencode arch=compute_53,code=[sm_53,compute_53]\n\n# For Jetson Tx2 or Drive-PX2 uncomment:\n# ARCH= -gencode arch=compute_62,code=[sm_62,compute_62]\nimport os\nos.environ['GPU_TYPE'] = str(os.popen('nvidia-smi --query-gpu=name --format=csv,noheader').read())\n\ndef getGPUArch(argument):\n  try:\n    argument = argument.strip()\n    # All Colab GPUs\n    archTypes = {\n        \"Tesla V100-SXM2-16GB\": \"-gencode arch=compute_70,code=[sm_70,compute_70]\",\n        \"Tesla K80\": \"-gencode arch=compute_37,code=sm_37\",\n        \"Tesla T4\": \"-gencode arch=compute_75,code=[sm_75,compute_75]\",\n        \"Tesla P40\": \"-gencode arch=compute_61,code=sm_61\",\n        \"Tesla P4\": \"-gencode arch=compute_61,code=sm_61\",\n        \"Tesla P100-PCIE-16GB\": \"-gencode arch=compute_60,code=sm_60\"\n\n      }\n    return archTypes[argument]\n  except KeyError:\n    return \"GPU must be added to GPU Commands\"\nos.environ['ARCH_VALUE'] = getGPUArch(os.environ['GPU_TYPE'])\n\nprint(\"GPU Type: \" + os.environ['GPU_TYPE'])\nprint(\"ARCH Value: \" + os.environ['ARCH_VALUE'])","46b294dc":"!sed -i 's\/OPENCV=0\/OPENCV=1\/g' Makefile\n!sed -i 's\/GPU=0\/GPU=1\/g' Makefile\n!sed -i 's\/CUDNN=0\/CUDNN=1\/g' Makefile\n!sed -i 's\/CUDNN_HALF=0\/CUDNN_HALF=1\/g' Makefile\n!sed -i 's\/LIBSO=0\/LIBSO=1\/' Makefile\n!sed -i \"s\/ARCH= -gencode arch=compute_60,code=sm_60\/ARCH= ${ARCH_VALUE}\/g\" Makefile\n!make &> compile.log","285bdfb6":"# yolov4 \n!wget https:\/\/github.com\/AlexeyAB\/darknet\/releases\/download\/darknet_yolo_v3_optimal\/yolov4.conv.137\n\n# yolov4 tiny\n!wget https:\/\/github.com\/AlexeyAB\/darknet\/releases\/download\/darknet_yolo_v4_pre\/yolov4-tiny.conv.29\n    \n    \n    \n!touch .\/data\/obj.data\n!touch .\/data\/obj.names","c9bf12f5":"%%writefile .\/data\/obj.names\nPlatelets\nRBC\nWBC","1b018dec":"## Make a folder to store the experiment weights and backups\n!mkdir training\n\n# Store weights for the nth experiment\n!mkdir training\/experiment1","a65fbb1d":"num_classes = 3","32215447":"%%writetemplate .\/data\/obj.data\nclasses = {num_classes}\ntrain = data\/train.txt\nvalid = data\/test.txt \nnames = data\/obj.names \nbackup = \/kaggle\/working\/darknet\/training\/experiment1","61347222":"!cat .\/data\/obj.data","73a19e46":"!ls cfg","03341cae":"## View the default config of yolov4\n\n!cat cfg\/yolov4.cfg","6e5cb1c6":"num_classes = 3\nmax_batches = 2100 #any number of iterations, ideally num_classes * 2000. Due to computation limits, limiting to 2200 iterations\nstep1 = int(0.8*max_batches)\nstep2 = int(0.9*max_batches)\n\nfilters = (num_classes + 5)*3\n\nprint(num_classes)\nprint(max_batches)\nprint(step1)\nprint(step2)","e8a0e26a":"%%writetemplate cfg\/yolov4.cfg\n[net]\nbatch=64\nsubdivisions=16\n# Training\n#width=512\n#height=512\nwidth=608\nheight=608\nchannels=3\nmomentum=0.949\ndecay=0.0005\nangle=0\nsaturation = 1.5\nexposure = 1.5\nhue=.1\n\nlearning_rate=0.0013\nburn_in=1000\nmax_batches = 2200 #{num_classes}*2000\npolicy=steps\nsteps={step1}, {step2}\nscales=.1,.1\n\n#cutmix=1\nmosaic=1\n\n#:104x104 54:52x52 85:26x26 104:13x13 for 416\n\n[convolutional]\nbatch_normalize=1\nfilters=32\nsize=3\nstride=1\npad=1\nactivation=mish\n\n# Downsample\n\n[convolutional]\nbatch_normalize=1\nfilters=64\nsize=3\nstride=2\npad=1\nactivation=mish\n\n[convolutional]\nbatch_normalize=1\nfilters=64\nsize=1\nstride=1\npad=1\nactivation=mish\n\n[route]\nlayers = -2\n\n[convolutional]\nbatch_normalize=1\nfilters=64\nsize=1\nstride=1\npad=1\nactivation=mish\n\n[convolutional]\nbatch_normalize=1\nfilters=32\nsize=1\nstride=1\npad=1\nactivation=mish\n\n[convolutional]\nbatch_normalize=1\nfilters=64\nsize=3\nstride=1\npad=1\nactivation=mish\n\n[shortcut]\nfrom=-3\nactivation=linear\n\n[convolutional]\nbatch_normalize=1\nfilters=64\nsize=1\nstride=1\npad=1\nactivation=mish\n\n[route]\nlayers = -1,-7\n\n[convolutional]\nbatch_normalize=1\nfilters=64\nsize=1\nstride=1\npad=1\nactivation=mish\n\n# Downsample\n\n[convolutional]\nbatch_normalize=1\nfilters=128\nsize=3\nstride=2\npad=1\nactivation=mish\n\n[convolutional]\nbatch_normalize=1\nfilters=64\nsize=1\nstride=1\npad=1\nactivation=mish\n\n[route]\nlayers = -2\n\n[convolutional]\nbatch_normalize=1\nfilters=64\nsize=1\nstride=1\npad=1\nactivation=mish\n\n[convolutional]\nbatch_normalize=1\nfilters=64\nsize=1\nstride=1\npad=1\nactivation=mish\n\n[convolutional]\nbatch_normalize=1\nfilters=64\nsize=3\nstride=1\npad=1\nactivation=mish\n\n[shortcut]\nfrom=-3\nactivation=linear\n\n[convolutional]\nbatch_normalize=1\nfilters=64\nsize=1\nstride=1\npad=1\nactivation=mish\n\n[convolutional]\nbatch_normalize=1\nfilters=64\nsize=3\nstride=1\npad=1\nactivation=mish\n\n[shortcut]\nfrom=-3\nactivation=linear\n\n[convolutional]\nbatch_normalize=1\nfilters=64\nsize=1\nstride=1\npad=1\nactivation=mish\n\n[route]\nlayers = -1,-10\n\n[convolutional]\nbatch_normalize=1\nfilters=128\nsize=1\nstride=1\npad=1\nactivation=mish\n\n# Downsample\n\n[convolutional]\nbatch_normalize=1\nfilters=256\nsize=3\nstride=2\npad=1\nactivation=mish\n\n[convolutional]\nbatch_normalize=1\nfilters=128\nsize=1\nstride=1\npad=1\nactivation=mish\n\n[route]\nlayers = -2\n\n[convolutional]\nbatch_normalize=1\nfilters=128\nsize=1\nstride=1\npad=1\nactivation=mish\n\n[convolutional]\nbatch_normalize=1\nfilters=128\nsize=1\nstride=1\npad=1\nactivation=mish\n\n[convolutional]\nbatch_normalize=1\nfilters=128\nsize=3\nstride=1\npad=1\nactivation=mish\n\n[shortcut]\nfrom=-3\nactivation=linear\n\n[convolutional]\nbatch_normalize=1\nfilters=128\nsize=1\nstride=1\npad=1\nactivation=mish\n\n[convolutional]\nbatch_normalize=1\nfilters=128\nsize=3\nstride=1\npad=1\nactivation=mish\n\n[shortcut]\nfrom=-3\nactivation=linear\n\n[convolutional]\nbatch_normalize=1\nfilters=128\nsize=1\nstride=1\npad=1\nactivation=mish\n\n[convolutional]\nbatch_normalize=1\nfilters=128\nsize=3\nstride=1\npad=1\nactivation=mish\n\n[shortcut]\nfrom=-3\nactivation=linear\n\n[convolutional]\nbatch_normalize=1\nfilters=128\nsize=1\nstride=1\npad=1\nactivation=mish\n\n[convolutional]\nbatch_normalize=1\nfilters=128\nsize=3\nstride=1\npad=1\nactivation=mish\n\n[shortcut]\nfrom=-3\nactivation=linear\n\n\n[convolutional]\nbatch_normalize=1\nfilters=128\nsize=1\nstride=1\npad=1\nactivation=mish\n\n[convolutional]\nbatch_normalize=1\nfilters=128\nsize=3\nstride=1\npad=1\nactivation=mish\n\n[shortcut]\nfrom=-3\nactivation=linear\n\n[convolutional]\nbatch_normalize=1\nfilters=128\nsize=1\nstride=1\npad=1\nactivation=mish\n\n[convolutional]\nbatch_normalize=1\nfilters=128\nsize=3\nstride=1\npad=1\nactivation=mish\n\n[shortcut]\nfrom=-3\nactivation=linear\n\n[convolutional]\nbatch_normalize=1\nfilters=128\nsize=1\nstride=1\npad=1\nactivation=mish\n\n[convolutional]\nbatch_normalize=1\nfilters=128\nsize=3\nstride=1\npad=1\nactivation=mish\n\n[shortcut]\nfrom=-3\nactivation=linear\n\n[convolutional]\nbatch_normalize=1\nfilters=128\nsize=1\nstride=1\npad=1\nactivation=mish\n\n[convolutional]\nbatch_normalize=1\nfilters=128\nsize=3\nstride=1\npad=1\nactivation=mish\n\n[shortcut]\nfrom=-3\nactivation=linear\n\n[convolutional]\nbatch_normalize=1\nfilters=128\nsize=1\nstride=1\npad=1\nactivation=mish\n\n[route]\nlayers = -1,-28\n\n[convolutional]\nbatch_normalize=1\nfilters=256\nsize=1\nstride=1\npad=1\nactivation=mish\n\n# Downsample\n\n[convolutional]\nbatch_normalize=1\nfilters=512\nsize=3\nstride=2\npad=1\nactivation=mish\n\n[convolutional]\nbatch_normalize=1\nfilters=256\nsize=1\nstride=1\npad=1\nactivation=mish\n\n[route]\nlayers = -2\n\n[convolutional]\nbatch_normalize=1\nfilters=256\nsize=1\nstride=1\npad=1\nactivation=mish\n\n[convolutional]\nbatch_normalize=1\nfilters=256\nsize=1\nstride=1\npad=1\nactivation=mish\n\n[convolutional]\nbatch_normalize=1\nfilters=256\nsize=3\nstride=1\npad=1\nactivation=mish\n\n[shortcut]\nfrom=-3\nactivation=linear\n\n\n[convolutional]\nbatch_normalize=1\nfilters=256\nsize=1\nstride=1\npad=1\nactivation=mish\n\n[convolutional]\nbatch_normalize=1\nfilters=256\nsize=3\nstride=1\npad=1\nactivation=mish\n\n[shortcut]\nfrom=-3\nactivation=linear\n\n\n[convolutional]\nbatch_normalize=1\nfilters=256\nsize=1\nstride=1\npad=1\nactivation=mish\n\n[convolutional]\nbatch_normalize=1\nfilters=256\nsize=3\nstride=1\npad=1\nactivation=mish\n\n[shortcut]\nfrom=-3\nactivation=linear\n\n\n[convolutional]\nbatch_normalize=1\nfilters=256\nsize=1\nstride=1\npad=1\nactivation=mish\n\n[convolutional]\nbatch_normalize=1\nfilters=256\nsize=3\nstride=1\npad=1\nactivation=mish\n\n[shortcut]\nfrom=-3\nactivation=linear\n\n\n[convolutional]\nbatch_normalize=1\nfilters=256\nsize=1\nstride=1\npad=1\nactivation=mish\n\n[convolutional]\nbatch_normalize=1\nfilters=256\nsize=3\nstride=1\npad=1\nactivation=mish\n\n[shortcut]\nfrom=-3\nactivation=linear\n\n\n[convolutional]\nbatch_normalize=1\nfilters=256\nsize=1\nstride=1\npad=1\nactivation=mish\n\n[convolutional]\nbatch_normalize=1\nfilters=256\nsize=3\nstride=1\npad=1\nactivation=mish\n\n[shortcut]\nfrom=-3\nactivation=linear\n\n\n[convolutional]\nbatch_normalize=1\nfilters=256\nsize=1\nstride=1\npad=1\nactivation=mish\n\n[convolutional]\nbatch_normalize=1\nfilters=256\nsize=3\nstride=1\npad=1\nactivation=mish\n\n[shortcut]\nfrom=-3\nactivation=linear\n\n[convolutional]\nbatch_normalize=1\nfilters=256\nsize=1\nstride=1\npad=1\nactivation=mish\n\n[convolutional]\nbatch_normalize=1\nfilters=256\nsize=3\nstride=1\npad=1\nactivation=mish\n\n[shortcut]\nfrom=-3\nactivation=linear\n\n[convolutional]\nbatch_normalize=1\nfilters=256\nsize=1\nstride=1\npad=1\nactivation=mish\n\n[route]\nlayers = -1,-28\n\n[convolutional]\nbatch_normalize=1\nfilters=512\nsize=1\nstride=1\npad=1\nactivation=mish\n\n# Downsample\n\n[convolutional]\nbatch_normalize=1\nfilters=1024\nsize=3\nstride=2\npad=1\nactivation=mish\n\n[convolutional]\nbatch_normalize=1\nfilters=512\nsize=1\nstride=1\npad=1\nactivation=mish\n\n[route]\nlayers = -2\n\n[convolutional]\nbatch_normalize=1\nfilters=512\nsize=1\nstride=1\npad=1\nactivation=mish\n\n[convolutional]\nbatch_normalize=1\nfilters=512\nsize=1\nstride=1\npad=1\nactivation=mish\n\n[convolutional]\nbatch_normalize=1\nfilters=512\nsize=3\nstride=1\npad=1\nactivation=mish\n\n[shortcut]\nfrom=-3\nactivation=linear\n\n[convolutional]\nbatch_normalize=1\nfilters=512\nsize=1\nstride=1\npad=1\nactivation=mish\n\n[convolutional]\nbatch_normalize=1\nfilters=512\nsize=3\nstride=1\npad=1\nactivation=mish\n\n[shortcut]\nfrom=-3\nactivation=linear\n\n[convolutional]\nbatch_normalize=1\nfilters=512\nsize=1\nstride=1\npad=1\nactivation=mish\n\n[convolutional]\nbatch_normalize=1\nfilters=512\nsize=3\nstride=1\npad=1\nactivation=mish\n\n[shortcut]\nfrom=-3\nactivation=linear\n\n[convolutional]\nbatch_normalize=1\nfilters=512\nsize=1\nstride=1\npad=1\nactivation=mish\n\n[convolutional]\nbatch_normalize=1\nfilters=512\nsize=3\nstride=1\npad=1\nactivation=mish\n\n[shortcut]\nfrom=-3\nactivation=linear\n\n[convolutional]\nbatch_normalize=1\nfilters=512\nsize=1\nstride=1\npad=1\nactivation=mish\n\n[route]\nlayers = -1,-16\n\n[convolutional]\nbatch_normalize=1\nfilters=1024\nsize=1\nstride=1\npad=1\nactivation=mish\n\n##########################\n\n[convolutional]\nbatch_normalize=1\nfilters=512\nsize=1\nstride=1\npad=1\nactivation=leaky\n\n[convolutional]\nbatch_normalize=1\nsize=3\nstride=1\npad=1\nfilters=1024\nactivation=leaky\n\n[convolutional]\nbatch_normalize=1\nfilters=512\nsize=1\nstride=1\npad=1\nactivation=leaky\n\n### SPP ###\n[maxpool]\nstride=1\nsize=5\n\n[route]\nlayers=-2\n\n[maxpool]\nstride=1\nsize=9\n\n[route]\nlayers=-4\n\n[maxpool]\nstride=1\nsize=13\n\n[route]\nlayers=-1,-3,-5,-6\n### End SPP ###\n\n[convolutional]\nbatch_normalize=1\nfilters=512\nsize=1\nstride=1\npad=1\nactivation=leaky\n\n[convolutional]\nbatch_normalize=1\nsize=3\nstride=1\npad=1\nfilters=1024\nactivation=leaky\n\n[convolutional]\nbatch_normalize=1\nfilters=512\nsize=1\nstride=1\npad=1\nactivation=leaky\n\n[convolutional]\nbatch_normalize=1\nfilters=256\nsize=1\nstride=1\npad=1\nactivation=leaky\n\n[upsample]\nstride=2\n\n[route]\nlayers = 85\n\n[convolutional]\nbatch_normalize=1\nfilters=256\nsize=1\nstride=1\npad=1\nactivation=leaky\n\n[route]\nlayers = -1, -3\n\n[convolutional]\nbatch_normalize=1\nfilters=256\nsize=1\nstride=1\npad=1\nactivation=leaky\n\n[convolutional]\nbatch_normalize=1\nsize=3\nstride=1\npad=1\nfilters=512\nactivation=leaky\n\n[convolutional]\nbatch_normalize=1\nfilters=256\nsize=1\nstride=1\npad=1\nactivation=leaky\n\n[convolutional]\nbatch_normalize=1\nsize=3\nstride=1\npad=1\nfilters=512\nactivation=leaky\n\n[convolutional]\nbatch_normalize=1\nfilters=256\nsize=1\nstride=1\npad=1\nactivation=leaky\n\n[convolutional]\nbatch_normalize=1\nfilters=128\nsize=1\nstride=1\npad=1\nactivation=leaky\n\n[upsample]\nstride=2\n\n[route]\nlayers = 54\n\n[convolutional]\nbatch_normalize=1\nfilters=128\nsize=1\nstride=1\npad=1\nactivation=leaky\n\n[route]\nlayers = -1, -3\n\n[convolutional]\nbatch_normalize=1\nfilters=128\nsize=1\nstride=1\npad=1\nactivation=leaky\n\n[convolutional]\nbatch_normalize=1\nsize=3\nstride=1\npad=1\nfilters=256\nactivation=leaky\n\n[convolutional]\nbatch_normalize=1\nfilters=128\nsize=1\nstride=1\npad=1\nactivation=leaky\n\n[convolutional]\nbatch_normalize=1\nsize=3\nstride=1\npad=1\nfilters=256\nactivation=leaky\n\n[convolutional]\nbatch_normalize=1\nfilters=128\nsize=1\nstride=1\npad=1\nactivation=leaky\n\n##########################\n\n[convolutional]\nbatch_normalize=1\nsize=3\nstride=1\npad=1\nfilters=256\nactivation=leaky\n\n[convolutional]\nsize=1\nstride=1\npad=1\nfilters={filters}\nactivation=linear\n\n\n[yolo]\nmask = 0,1,2\nanchors = 12, 16, 19, 36, 40, 28, 36, 75, 76, 55, 72, 146, 142, 110, 192, 243, 459, 401\nclasses={num_classes}\nnum=9\njitter=.3\nignore_thresh = .7\ntruth_thresh = 1\nscale_x_y = 1.2\niou_thresh=0.213\ncls_normalizer=1.0\niou_normalizer=0.07\niou_loss=ciou\nnms_kind=greedynms\nbeta_nms=0.6\nmax_delta=5\n\n\n[route]\nlayers = -4\n\n[convolutional]\nbatch_normalize=1\nsize=3\nstride=2\npad=1\nfilters=256\nactivation=leaky\n\n[route]\nlayers = -1, -16\n\n[convolutional]\nbatch_normalize=1\nfilters=256\nsize=1\nstride=1\npad=1\nactivation=leaky\n\n[convolutional]\nbatch_normalize=1\nsize=3\nstride=1\npad=1\nfilters=512\nactivation=leaky\n\n[convolutional]\nbatch_normalize=1\nfilters=256\nsize=1\nstride=1\npad=1\nactivation=leaky\n\n[convolutional]\nbatch_normalize=1\nsize=3\nstride=1\npad=1\nfilters=512\nactivation=leaky\n\n[convolutional]\nbatch_normalize=1\nfilters=256\nsize=1\nstride=1\npad=1\nactivation=leaky\n\n[convolutional]\nbatch_normalize=1\nsize=3\nstride=1\npad=1\nfilters=512\nactivation=leaky\n\n[convolutional]\nsize=1\nstride=1\npad=1\nfilters={filters}\nactivation=linear\n\n\n[yolo]\nmask = 3,4,5\nanchors = 12, 16, 19, 36, 40, 28, 36, 75, 76, 55, 72, 146, 142, 110, 192, 243, 459, 401\nclasses={num_classes}\nnum=9\njitter=.3\nignore_thresh = .7\ntruth_thresh = 1\nscale_x_y = 1.1\niou_thresh=0.213\ncls_normalizer=1.0\niou_normalizer=0.07\niou_loss=ciou\nnms_kind=greedynms\nbeta_nms=0.6\nmax_delta=5\n\n\n[route]\nlayers = -4\n\n[convolutional]\nbatch_normalize=1\nsize=3\nstride=2\npad=1\nfilters=512\nactivation=leaky\n\n[route]\nlayers = -1, -37\n\n[convolutional]\nbatch_normalize=1\nfilters=512\nsize=1\nstride=1\npad=1\nactivation=leaky\n\n[convolutional]\nbatch_normalize=1\nsize=3\nstride=1\npad=1\nfilters=1024\nactivation=leaky\n\n[convolutional]\nbatch_normalize=1\nfilters=512\nsize=1\nstride=1\npad=1\nactivation=leaky\n\n[convolutional]\nbatch_normalize=1\nsize=3\nstride=1\npad=1\nfilters=1024\nactivation=leaky\n\n[convolutional]\nbatch_normalize=1\nfilters=512\nsize=1\nstride=1\npad=1\nactivation=leaky\n\n[convolutional]\nbatch_normalize=1\nsize=3\nstride=1\npad=1\nfilters=1024\nactivation=leaky\n\n[convolutional]\nsize=1\nstride=1\npad=1\nfilters={filters}\nactivation=linear\n\n\n[yolo]\nmask = 6,7,8\nanchors = 12, 16, 19, 36, 40, 28, 36, 75, 76, 55, 72, 146, 142, 110, 192, 243, 459, 401\nclasses={num_classes}\nnum=9\njitter=.3\nignore_thresh = .7\ntruth_thresh = 1\nrandom=1\nscale_x_y = 1.05\niou_thresh=0.213\ncls_normalizer=1.0\niou_normalizer=0.07\niou_loss=ciou\nnms_kind=greedynms\nbeta_nms=0.6\nmax_delta=5","c342c291":"!cat cfg\/yolov4.cfg","ac508f48":"!ls .\/data","2ab91c89":"!mkdir .\/data\/obj","878724dc":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nsecret_value_1 = user_secrets.get_secret(\"bloodcell_dataset_darknet\")\n!curl -L -q https:\/\/public.roboflow.com\/ds\/$secret_value_1 > roboflow.zip; unzip roboflow.zip; rm roboflow.zip\n","5bd4d727":"!ls train | wc -l","7275c24a":"!ls test | wc -l ","658aaa8e":"!ls valid | wc -l","3fbe4444":"#copy all under obj folder","aca331d0":"!cp train\/* data\/obj\/\n!cp test\/* data\/obj\/\n!cp valid\/* data\/obj\/\n\n!touch process.py","79df21be":"%%writetemplate process.py\nimport glob, os\n\n# Current directory\ncurrent_dir = os.path.dirname(os.path.abspath(__file__))\n\nprint(current_dir)\n\ncurrent_dir = 'data\/obj'\n\n# Percentage of images to be used for the test set\npercentage_test = 10;\n\n# Create and\/or truncate train.txt and test.txt\nfile_train = open('data\/train.txt', 'w')\nfile_test = open('data\/test.txt', 'w')\n\n# Populate train.txt and test.txt\ncounter = 1\nindex_test = round(100 \/ percentage_test)\nfor pathAndFilename in glob.iglob(os.path.join(current_dir, \"*.jpg\")):\n    title, ext = os.path.splitext(os.path.basename(pathAndFilename))\n\n    if counter == index_test:\n        counter = 1\n        file_test.write(\"data\/obj\" + \"\/\" + title + '.jpg' + \"\\n\")\n    else:\n        file_train.write(\"data\/obj\" + \"\/\" + title + '.jpg' + \"\\n\")\n        counter = counter + 1","506abe60":"!python process.py","15b64a02":"!cat data\/train.txt | wc -l","30edfafe":"!cat data\/test.txt | wc -l","6d17546d":"!.\/darknet.exe detector train data\/obj.data cfg\/yolov4.cfg -dont_show -map","c6cc2837":"#!.\/darknet detector train data\/obj.data cfg\/yolov4-tiny.cfg yolov4-tiny.conv.29 -dont_show -map","cb398360":"!rm \/kaggle\/working\/README.roboflow.txt\n!rm \/kaggle\/working\/README.dataset.txt","27841bb5":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nsecret_value_2 = user_secrets.get_secret(\"bloodcell_dataset_tf_tfrecords\")\n\n#Downloading data from Roboflow\n#UPDATE THIS LINK - get our data from Roboflow\n%cd \/kaggle\/working\n!curl -L https:\/\/public.roboflow.com\/ds\/$secret_value_2 > roboflow.zip; unzip roboflow.zip; rm roboflow.zip","56f2e490":"!mkdir Tensorflow\n%cd Tensorflow","1eec3839":"!git clone https:\/\/github.com\/tensorflow\/models.git\n%cd \/kaggle\/working\/Tensorflow\/models\/research\n!protoc .\/object_detection\/protos\/*.proto --python_out=.\n!cp .\/object_detection\/packages\/tf2\/setup.py .\n!python -m pip install --user .\n\n\n!pip install tensorflow==2.4.1\n!pip install tensorflow-addons\n\n\n!python .\/object_detection\/builders\/model_builder_tf2_test.py\n\n","1654e74d":"# %cd Tensorflow\n# !git clone https:\/\/github.com\/tensorflow\/models","70db7610":"# %cd .\/models\/research\/\n# !protoc .\/object_detection\/protos\/*.proto --python_out=.","25650c9f":"# !cat .\/object_detection\/packages\/tf2\/setup.py","f0973197":"# !protoc object_detection\/protos\/*.proto --python_out=.","cccda0d8":"# !cp .\/object_detection\/packages\/tf2\/setup.py .\n# !python -m pip -q install .","477cea79":"# !pip install scipy==1.5.4\n# !pip install dill==0.3.3\n# !pip install pathos==0.2.7\n# !pip install multiprocess==0.70.11.1\n# !pip install autogluon-core==0.1.0","19040a17":"# !pip install tf_slim\n\n# !pip install pycocotools\n# !pip install lvis\n# !pip install numba ","a4422798":"# !python object_detection\/builders\/model_builder_tf2_test.py","6b5755fb":"import tensorflow as tf\ntf.__version__","80f8b457":"# %cd \/kaggle\/working\n\n# !mkdir Tensorflow","a9c989af":"# %cd \/kaggle\/working\/Tensorflow","3bd20f30":"!pwd","e5d78bba":"%cd \/kaggle\/working\/Tensorflow","9bf6dd68":"!mkdir workspace","fa9feb47":"!ls \/kaggle\/working\/Tensorflow","74a89883":"!mkdir workspace\/training_demo","4c6dac62":"!ls \/kaggle\/working\/Tensorflow\/workspace","c494e140":"!mkdir workspace\/training_demo\/annotations","822373f9":"!ls \/kaggle\/working\/Tensorflow\/","63cdef4c":"!ls \/kaggle\/working\/train\n\n!cp -r \/kaggle\/working\/train\/* \/kaggle\/working\/Tensorflow\/workspace\/training_demo\/annotations\/\n!mv \/kaggle\/working\/Tensorflow\/workspace\/training_demo\/annotations\/cells.tfrecord \/kaggle\/working\/Tensorflow\/workspace\/training_demo\/annotations\/cells_train.tfrecord\n!cp -r \/kaggle\/working\/test\/* \/kaggle\/working\/Tensorflow\/workspace\/training_demo\/annotations\/\n!mv \/kaggle\/working\/Tensorflow\/workspace\/training_demo\/annotations\/cells.tfrecord \/kaggle\/working\/Tensorflow\/workspace\/training_demo\/annotations\/cells_test.tfrecord","61b744c2":"!cat \/kaggle\/working\/train\/cells_label_map.pbtxt","8b65de83":"!pwd","bba39bff":"!mkdir \/kaggle\/working\/Tensorflow\/workspace\/training_demo\/pre-trained-models","a05ca91c":"%cd \/kaggle\/working\/Tensorflow\/workspace\/training_demo\/pre-trained-models","894fb428":"\n\nMODEL = \"faster_rcnn_resnet50_v1_1024x1024_coco17_tpu-8\"\n\n!wget http:\/\/download.tensorflow.org\/models\/object_detection\/tf2\/20200711\/{MODEL}.tar.gz\n!tar -xf {MODEL}.tar.gz\n!rm {MODEL}.tar.gz","2e5136db":"!ls \/kaggle\/working\/Tensorflow\/workspace\/training_demo\/pre-trained-models","a2529cb3":"MODEL2 = 'efficientdet_d1_coco17_tpu-32'\n\n!wget http:\/\/download.tensorflow.org\/models\/object_detection\/tf2\/20200711\/{MODEL2}.tar.gz\n!tar -xf {MODEL2}.tar.gz\n!rm {MODEL2}.tar.gz","876b1804":"!ls \/kaggle\/working\/Tensorflow\/workspace\/training_demo\/pre-trained-models","cfe5e8ae":"MODEL3 = 'ssd_resnet50_v1_fpn_640x640_coco17_tpu-8'\n\n!wget http:\/\/download.tensorflow.org\/models\/object_detection\/tf2\/20200711\/{MODEL3}.tar.gz\n!tar -xf {MODEL3}.tar.gz\n!rm {MODEL3}.tar.gz","01e8c2ca":"!ls \/kaggle\/working\/Tensorflow\/workspace\/training_demo\/pre-trained-models","60f7f1e3":"!mkdir \/kaggle\/working\/Tensorflow\/workspace\/training_demo\/models","78f554ae":"!pwd","4636081a":"!ls \/kaggle\/working\/Tensorflow\/workspace\/training_demo\/","7bbd2316":"!mkdir \/kaggle\/working\/Tensorflow\/workspace\/training_demo\/models\/ssd_resnet50_v1_fpn_640x640_coco17_tpu-8\n!cp ssd_resnet50_v1_fpn_640x640_coco17_tpu-8\/pipeline.config \/kaggle\/working\/Tensorflow\/workspace\/training_demo\/models\/ssd_resnet50_v1_fpn_640x640_coco17_tpu-8","16a83a50":"!ls \/kaggle\/working\/Tensorflow\/workspace\/training_demo\/models\/ssd_resnet50_v1_fpn_640x640_coco17_tpu-8","f1566c9b":"%%writefile \/kaggle\/working\/Tensorflow\/workspace\/training_demo\/models\/ssd_resnet50_v1_fpn_640x640_coco17_tpu-8\/pipeline.config\nmodel {\n  ssd {\n    num_classes: 3 # set this to the number of different label classes\n    image_resizer {\n      fixed_shape_resizer {\n        height: 640\n        width: 640\n      }\n    }\n    feature_extractor {\n      type: \"ssd_resnet50_v1_fpn_keras\"\n      depth_multiplier: 1.0\n      min_depth: 16\n      conv_hyperparams {\n        regularizer {\n          l2_regularizer {\n            weight: 0.00039999998989515007\n          }\n        }\n        initializer {\n          truncated_normal_initializer {\n            mean: 0.0\n            stddev: 0.029999999329447746\n          }\n        }\n        activation: RELU_6\n        batch_norm {\n          decay: 0.996999979019165\n          scale: true\n          epsilon: 0.0010000000474974513\n        }\n      }\n      override_base_feature_extractor_hyperparams: true\n      fpn {\n        min_level: 3\n        max_level: 7\n      }\n    }\n    box_coder {\n      faster_rcnn_box_coder {\n        y_scale: 10.0\n        x_scale: 10.0\n        height_scale: 5.0\n        width_scale: 5.0\n      }\n    }\n    matcher {\n      argmax_matcher {\n        matched_threshold: 0.5\n        unmatched_threshold: 0.5\n        ignore_thresholds: false\n        negatives_lower_than_unmatched: true\n        force_match_for_each_row: true\n        use_matmul_gather: true\n      }\n    }\n    similarity_calculator {\n      iou_similarity {\n      }\n    }\n    box_predictor {\n      weight_shared_convolutional_box_predictor {\n        conv_hyperparams {\n          regularizer {\n            l2_regularizer {\n              weight: 0.00039999998989515007\n            }\n          }\n          initializer {\n            random_normal_initializer {\n              mean: 0.0\n              stddev: 0.009999999776482582\n            }\n          }\n          activation: RELU_6\n          batch_norm {\n            decay: 0.996999979019165\n            scale: true\n            epsilon: 0.0010000000474974513\n          }\n        }\n        depth: 256\n        num_layers_before_predictor: 4\n        kernel_size: 3\n        class_prediction_bias_init: -4.599999904632568\n      }\n    }\n    anchor_generator {\n      multiscale_anchor_generator {\n        min_level: 3\n        max_level: 7\n        anchor_scale: 4.0\n        aspect_ratios: 1.0\n        aspect_ratios: 2.0\n        aspect_ratios: 0.5\n        scales_per_octave: 2\n      }\n    }\n    post_processing {\n      batch_non_max_suppression {\n        score_threshold: 9.99999993922529e-09\n        iou_threshold: 0.6000000238418579\n        max_detections_per_class: 100\n        max_total_detections: 100\n        use_static_shapes: false\n      }\n      score_converter: SIGMOID\n    }\n    normalize_loss_by_num_matches: true\n    loss {\n      localization_loss {\n        weighted_smooth_l1 {\n        }\n      }\n      classification_loss {\n        weighted_sigmoid_focal {\n          gamma: 2.0\n          alpha: 0.25\n        }\n      }\n      classification_weight: 1.0\n      localization_weight: 1.0\n    }\n    encode_background_as_zeros: true\n    normalize_loc_loss_by_codesize: true\n    inplace_batchnorm_update: true\n    freeze_batchnorm: false\n  }\n}\ntrain_config {\n  batch_size: 8   # Increase\/Decrease depending on the available memory\n  data_augmentation_options {\n    random_horizontal_flip {\n    }\n  }\n  data_augmentation_options {\n    random_crop_image {\n      min_object_covered: 0.0\n      min_aspect_ratio: 0.75\n      max_aspect_ratio: 3.0\n      min_area: 0.75\n      max_area: 1.0\n      overlap_thresh: 0.0\n    }\n  }\n  sync_replicas: true\n  optimizer {\n    momentum_optimizer {\n      learning_rate {\n        cosine_decay_learning_rate {\n          learning_rate_base: 0.03999999910593033\n          total_steps: 25000\n          warmup_learning_rate: 0.013333000242710114\n          warmup_steps: 2000\n        }\n      }\n      momentum_optimizer_value: 0.8999999761581421\n    }\n    use_moving_average: false\n  }\n  fine_tune_checkpoint: \"\/kaggle\/working\/Tensorflow\/workspace\/training_demo\/pre-trained-models\/ssd_resnet50_v1_fpn_640x640_coco17_tpu-8\/checkpoint\/ckpt-0\"  # Path to checkpoint of pre-trained models\n  num_steps: 2000\n  startup_delay_steps: 0.0\n  replicas_to_aggregate: 8\n  max_number_of_boxes: 100\n  unpad_groundtruth_tensors: false\n  fine_tune_checkpoint_type: \"detection\" # Set this to detection as we want our full model to train for detection\n  use_bfloat16: false # # Set this to false if you are not training on a TPU\n  fine_tune_checkpoint_version: V2\n}\ntrain_input_reader {\n  label_map_path: \"\/kaggle\/working\/Tensorflow\/workspace\/training_demo\/annotations\/cells_label_map.pbtxt\"  # Path to label map file\n  tf_record_input_reader {\n    input_path: \"\/kaggle\/working\/Tensorflow\/workspace\/training_demo\/annotations\/cells_train.tfrecord\" # Path to training record file\n  }\n}\neval_config {\n  metrics_set: \"coco_detection_metrics\"\n  use_moving_averages: false\n}\neval_input_reader {\n  label_map_path: \"\/kaggle\/working\/Tensorflow\/workspace\/training_demo\/annotations\/cells_label_map.pbtxt\"  #Path to label map file\n  shuffle: false\n  num_epochs: 1\n  tf_record_input_reader {\n    input_path: \"\/kaggle\/working\/Tensorflow\/workspace\/training_demo\/annotations\/cells_test.tfrecord\"  #Path to testing TFRecord\n  }\n}\n\n","5e8a70a0":"!ls \/kaggle\/working\/Tensorflow\/workspace\/training_demo\/annotations","b1d6bf94":"!cp \/kaggle\/working\/Tensorflow\/models\/research\/object_detection\/model_main_tf2.py \/kaggle\/working\/Tensorflow\/workspace\/training_demo\/","6979f757":"%cd \/kaggle\/working\/Tensorflow\/workspace\/training_demo","00f83659":"!python model_main_tf2.py --model_dir=models\/ssd_resnet50_v1_fpn_640x640_coco17_tpu-8 --pipeline_config_path=models\/ssd_resnet50_v1_fpn_640x640_coco17_tpu-8\/pipeline.config","51fbdbe5":"!ls \/kaggle\/working\/Tensorflow\/workspace\/training_demo\/models\/ssd_resnet50_v1_fpn_640x640_coco17_tpu-8","6c88c73b":"#!ls .\/annotations","d8265734":"#%cd models","07c58c43":"# !mkdir {MODEL}\n# !mkdir {MODEL2}","388fdc16":"# !mkdir models","01764917":"# !cp \/kaggle\/working\/models\/research\/object_detection\/pre-trained-models\/{MODEL}\/pipeline.config \\\n#     \/kaggle\/working\/models\/research\/object_detection\/pre-trained-models\/{MODEL}\/pipeline.config.org","f19fea57":"# %cd \/kaggle\/working\/models\/research\/object_detection\n# !cp \/kaggle\/working\/models\/research\/object_detection\/pre-trained-models\/{MODEL}\/pipeline.config \\\n#     \/kaggle\/working\/models\/research\/object_detection\/pre-trained-models\/{MODEL}\/pipeline.config.org","70793bde":"# num_classes = 3\n# batch_size = 4\n# ft_checkpoint = \"ssd_resnet50_v1_fpn_640x640_coco17_tpu-8\/checkpoint\/ckpt-0\"\n# ft_checkpoint_type = \"detection\"\n# tpu = \"false\"\n# labelmap_path = \"labelmap.pbtxt\"\n# training_tfrecord = \"train.record\"\n# testing_tfrecord = \"test.record\"","368d2ed1":"# !python model_main_tf2.py --model_dir=models\/my_ssd_resnet50_v1_fpn --pipeline_config_path=models\/my_ssd_resnet50_v1_fpn\/pipeline.config","33c963f0":"# !pip install tf_slim\n# ## Make sure we have `pycocotools` installed\n# !pip -q install pycocotools\n# !pip -q install lvis\n# !pip -q install numba","1dcb1cc9":"# import os\n# import pathlib\n\n# # Clone the tensorflow models repository if it doesn't already exist\n# if \"models\" in pathlib.Path.cwd().parts:\n#   while \"models\" in pathlib.Path.cwd().parts:\n#     os.chdir('..')\n# elif not pathlib.Path('models').exists():\n#   !git clone --depth 1 https:\/\/github.com\/tensorflow\/models","0c8385dd":"# !pwd","ef38c7a2":"# # Install the Object Detection API\n# %cd models\/research\/\n# !protoc .\/object_detection\/protos\/*.proto --python_out=.\n# # !cp .\/object_detection\/packages\/tf2\/setup.py .\n# # !python -m pip -q install .","2d74478c":"# !cp .\/object_detection\/packages\/tf2\/setup.py .\n# !python -m pip -q install .","ece11815":"# os.environ['PYTHONPATH'] += \":\/kaggle\/working\/models\"\n\n# import sys\n# sys.path.append(\"\/kaggle\/working\/models\")","f6fcecf5":"# import numpy as np\n# import os\n# import sys\n# import tarfile\n# import tensorflow as tf\n# import zipfile\n# import cv2\n# import json\n# import pandas as pd\n# import glob\n# import os.path as osp\n# from path import Path\n# import datetime\n# import random\n# import shutil\n# from io import StringIO, BytesIO\n# from PIL import Image\n# from IPython.display import display\n# import re\n\n# %matplotlib inline\n# import matplotlib.pyplot as plt\n# import seaborn as sns\n# from matplotlib import rcParams\n# sns.set(rc={\"font.size\":9,\"axes.titlesize\":15,\"axes.labelsize\":9,\n#             \"axes.titlepad\":11, \"axes.labelpad\":9, \"legend.fontsize\":7,\n#             \"legend.title_fontsize\":7, 'axes.grid' : False})\n\n# from sklearn.model_selection import train_test_split\n\n# ## Import object detection module\n# from object_detection.utils import ops as utils_ops\n# from object_detection.utils import label_map_util\n# from object_detection.utils import visualization_utils as vis_utils\n# from object_detection.protos.string_int_label_map_pb2 import StringIntLabelMap, StringIntLabelMapItem\n# from object_detection.utils import config_util\n# from object_detection.builders import model_builder\n\n# from google.protobuf import text_format\n\n# import tarfile\n# from numba import cuda ","ea85ab74":"# # patch tf1 into `utils.ops`\n# utils_ops.tf = tf.compat.v1\n\n# # Patch the location of gfile\n# tf.gfile = tf.io.gfile","cfde6488":"# PATH_TO_LABELS = '\/kaggle\/working\/models\/research\/object_detection\/data\/mscoco_label_map.pbtxt'\n# category_index = label_map_util.create_category_index_from_labelmap(PATH_TO_LABELS, use_display_name=True)","c9a36d77":"# pretrained_dir = \"\/kaggle\/working\/training_job\/model\/\"\n\n# if not os.path.exists(pretrained_dir):\n#     os.makedirs(pretrained_dir)\n#     print('Pretrainined Model Directory:', pretrained_dir)","1c50bf18":"MODELS_CONFIG = {\n    'efficientdet-d0': {\n        'model_name': 'efficientdet_d0_coco17_tpu-32',\n        'base_pipeline_file': 'ssd_efficientdet_d0_512x512_coco17_tpu-8.config',\n        'pretrained_checkpoint': 'efficientdet_d0_coco17_tpu-32.tar.gz',\n    },\n    'efficientdet-d1': {\n        'model_name': 'efficientdet_d1_coco17_tpu-32',\n        'base_pipeline_file': 'ssd_efficientdet_d1_640x640_coco17_tpu-8.config',\n        'pretrained_checkpoint': 'efficientdet_d1_coco17_tpu-32.tar.gz',\n    },\n    'efficientdet-d2': {\n        'model_name': 'efficientdet_d2_coco17_tpu-32',\n        'base_pipeline_file': 'ssd_efficientdet_d2_768x768_coco17_tpu-8.config',\n        'pretrained_checkpoint': 'efficientdet_d2_coco17_tpu-32.tar.gz',\n    },\n    'efficientdet-d3': {\n        'model_name': 'efficientdet_d3_coco17_tpu-32',\n        'base_pipeline_file': 'ssd_efficientdet_d3_896x896_coco17_tpu-32.config',\n        'pretrained_checkpoint': 'efficientdet_d3_coco17_tpu-32.tar.gz',\n    },\n    'efficientdet-d4': {\n        'model_name': 'efficientdet_d4_coco17_tpu-32',\n        'base_pipeline_file': 'ssd_efficientdet_d4_896x896_coco17_tpu-32.config',\n        'pretrained_checkpoint': 'efficientdet_d4_coco17_tpu-32.tar.gz',\n    },\n    'efficientdet-d5': {\n        'model_name': 'efficientdet_d5_coco17_tpu-32',\n        'base_pipeline_file': 'ssd_efficientdet_d5_896x896_coco17_tpu-32.config',\n        'pretrained_checkpoint': 'efficientdet_d5_coco17_tpu-32.tar.gz',\n    },\n    'efficientdet-d6': {\n        'model_name': 'efficientdet_d6_coco17_tpu-32',\n        'base_pipeline_file': 'ssd_efficientdet_d6_896x896_coco17_tpu-32.config',\n        'pretrained_checkpoint': 'efficientdet_d6_coco17_tpu-32.tar.gz',\n    },\n    'efficientdet-d7': {\n        'model_name': 'efficientdet_d7_coco17_tpu-32',\n        'base_pipeline_file': 'ssd_efficientdet_d7_896x896_coco17_tpu-32.config',\n        'pretrained_checkpoint': 'efficientdet_d7_coco17_tpu-32.tar.gz',\n    }\n}\n\n## Choosing D1 here for this tutorial\n# chosen_model = 'efficientdet-d1'\n# model_name = MODELS_CONFIG[chosen_model]['model_name']\n# pretrained_checkpoint = MODELS_CONFIG[chosen_model]['pretrained_checkpoint']\n# base_pipeline_file = MODELS_CONFIG[chosen_model]['base_pipeline_file']","0260e4e6":"# !ls \/kaggle\/working\/training_job","b795fb9b":"# model_url = 'http:\/\/download.tensorflow.org\/models\/object_detection\/tf2\/20200711\/'+pretrained_checkpoint\n# # pretrained_dir = \"\/kaggle\/working\/models\/research\/object_detection\/pretrained\"\n\n# !wget {model_url}\n# !tar -xf {pretrained_checkpoint}\n# !mv {model_name}\/ {pretrained_dir}\n# !rm {pretrained_checkpoint}\n\n# model_dir = os.path.join(pretrained_dir, model_name, \"saved_model\")\n# print(\"Pre-trained model directory\", model_dir)\n# model = tf.saved_model.load(str(model_dir))","37d954eb":"# model.signatures['serving_default'].output_dtypes","43a9a395":"# model.signatures['serving_default'].output_shapes","c1cf3dca":"# def run_inference_for_single_image(model, image):\n#     '''\n#     Add a wrapper function to call the model, and cleanup the outputs:\n#     '''\n#     image = np.asarray(image)\n#     # The input needs to be a tensor, convert it using `tf.convert_to_tensor`.\n#     input_tensor = tf.convert_to_tensor(image)\n#     # The model expects a batch of images, so add an axis with `tf.newaxis`.\n#     input_tensor = input_tensor[tf.newaxis,...]\n\n#     # Run inference\n#     model_fn = model.signatures['serving_default']\n#     output_dict = model_fn(input_tensor)\n\n#     # All outputs are batches tensors.\n#     # Convert to numpy arrays, and take index [0] to remove the batch dimension.\n#     # We're only interested in the first num_detections.\n#     num_detections = int(output_dict.pop('num_detections'))\n#     output_dict = {key:value[0, :num_detections].numpy() for key,value in output_dict.items()}\n#     output_dict['num_detections'] = num_detections\n\n#     # detection_classes should be ints.\n#     output_dict['detection_classes'] = output_dict['detection_classes'].astype(np.int64)\n\n#     # Handle models with masks:\n#     if 'detection_masks' in output_dict:\n#         # Reframe the the bbox mask to the image size.\n#         detection_masks_reframed = utils_ops.reframe_box_masks_to_image_masks(\n#                   output_dict['detection_masks'], output_dict['detection_boxes'],\n#                    image.shape[0], image.shape[1])      \n#         detection_masks_reframed = tf.cast(detection_masks_reframed > 0.5,\n#                                            tf.uint8)\n#         output_dict['detection_masks_reframed'] = detection_masks_reframed.numpy()\n\n#     return output_dict\n\n# def show_inference(model, image_path):\n#     # the array based representation of the image will be used later in order to prepare the\n#     # result image with boxes and labels on it.\n#     image_np = np.array(Image.open(image_path))\n#     # Actual detection.\n#     output_dict = run_inference_for_single_image(model, image_np)\n#     # Visualization of the results of a detection.\n#     vis_utils.visualize_boxes_and_labels_on_image_array(\n#       image_np,\n#       output_dict['detection_boxes'],\n#       output_dict['detection_classes'],\n#       output_dict['detection_scores'],\n#       category_index,\n#       instance_masks=output_dict.get('detection_masks_reframed', None),\n#       use_normalized_coordinates=True,\n#       line_thickness=8)\n\n#     return image_np\n    \n# def load_image_into_numpy_array(path):\n#     \"\"\"Load an image from file into a numpy array.\n\n#     Puts image into numpy array to feed into tensorflow graph.\n#     Note that by convention we put it into a numpy array with shape\n#     (height, width, channels), where channels=3 for RGB.\n\n#     Args:\n#     path: a file path.\n\n#     Returns:\n#     uint8 numpy array with shape (img_height, img_width, 3)\n#     \"\"\"\n#     img_data = tf.io.gfile.GFile(path, 'rb').read()\n#     image = Image.open(BytesIO(img_data))\n#     (im_width, im_height) = image.size\n#     return np.array(image.getdata()).reshape(\n#       (im_height, im_width, 3)).astype(np.uint8)\n\n# def plot_detections(image_np, boxes, classes, scores, category_index,\n#                     figsize=(12, 16), image_name=None):\n#     \"\"\"Wrapper function to visualize detections.\n\n#     Args:\n#     image_np: uint8 numpy array with shape (img_height, img_width, 3)\n#     boxes: a numpy array of shape [N, 4]\n#     classes: a numpy array of shape [N]. Note that class indices are 1-based,\n#       and match the keys in the label map.\n#     scores: a numpy array of shape [N] or None.  If scores=None, then\n#       this function assumes that the boxes to be plotted are groundtruth\n#       boxes and plot all boxes as black with no classes or scores.\n#     category_index: a dict containing category dictionaries (each holding\n#       category index `id` and category name `name`) keyed by category indices.\n#     figsize: size for the figure.\n#     image_name: a name for the image file.\n#     \"\"\"\n#     image_np_with_annotations = image_np.copy()\n#     viz_utils.visualize_boxes_and_labels_on_image_array(\n#       image_np_with_annotations,\n#       boxes,\n#       classes,\n#       scores,\n#       category_index,\n#       use_normalized_coordinates=True,\n#       min_score_thresh=0.8)\n#     if image_name:\n#         plt.imsave(image_name, image_np_with_annotations)\n#     else:\n#         plt.imshow(image_np_with_annotations)\n        \n        \n# def plot_img(img, size=(18, 18), is_rgb=True, title=\"\", cmap='gray'):\n#     plt.figure(figsize=size)\n#     plt.imshow(img, cmap=cmap)\n#     plt.suptitle(title)","ecaac475":"# !wget https:\/\/cdn.pixabay.com\/photo\/2015\/03\/26\/09\/43\/city-690158_960_720.jpg\n# !wget https:\/\/cdn.pixabay.com\/photo\/2017\/08\/05\/23\/31\/people-2586656_960_720.jpg\n# !wget https:\/\/upload.wikimedia.org\/wikipedia\/commons\/6\/60\/Naxos_Taverna.jpg\n    \n# !wget https:\/\/cdn.pixabay.com\/photo\/2020\/02\/07\/15\/33\/girl-4827500_960_720.jpg\n# TEST_IMAGE_PATHS = [\n#                     'girl-4827500_960_720.jpg',\n#                     'Naxos_Taverna.jpg',\n#                     'city-690158_960_720.jpg'\n#                    ]","f963abde":"# for image_path in TEST_IMAGE_PATHS:\n#     image = show_inference(model, image_path)\n#     os.remove(image_path)\n#     plot_img(image)\n#     plt.savefig('{}_viz.png'.format(image_path))\n#     plt.show()","9bb9a38f":"# del model\n# device = cuda.get_current_device()\n# device.reset()","38622821":"# tfr_output_dir = \"\/kaggle\/working\/training_job\/tfrecords\/\"\n\n# if not os.path.exists(tfr_output_dir):\n#     os.makedirs(tfr_output_dir)\n#     print('TFRecord Directory:', tfr_output_dir)","2ad4f127":"# import matplotlib\n# import matplotlib.pyplot as plt\n\n# import os\n# import random\n# import io\n# import imageio\n# import glob\n# import scipy.misc\n# import numpy as np\n# from six import BytesIO\n# from PIL import Image, ImageDraw, ImageFont\n# from IPython.display import display, Javascript\n# from IPython.display import Image as IPyImage\n\n# import tensorflow as tf\n\n# from object_detection.utils import label_map_util\n# from object_detection.utils import config_util\n# from object_detection.utils import visualization_utils as viz_utils\n# #from object_detection.utils import colab_utils\n# from object_detection.builders import model_builder\n\n# %matplotlib inline","6445ffd1":"# #run model builder test\n# !python \/kaggle\/working\/models\/research\/object_detection\/builders\/model_builder_tf2_test.py\n","8e11643d":"# def load_image_into_numpy_array(path):\n#   \"\"\"Load an image from file into a numpy array.\n\n#   Puts image into numpy array to feed into tensorflow graph.\n#   Note that by convention we put it into a numpy array with shape\n#   (height, width, channels), where channels=3 for RGB.\n\n#   Args:\n#     path: a file path.\n\n#   Returns:\n#     uint8 numpy array with shape (img_height, img_width, 3)\n#   \"\"\"\n#   img_data = tf.io.gfile.GFile(path, 'rb').read()\n#   image = Image.open(BytesIO(img_data))\n#   (im_width, im_height) = image.size\n#   return np.array(image.getdata()).reshape(\n#       (im_height, im_width, 3)).astype(np.uint8)\n\n# def plot_detections(image_np,\n#                     boxes,\n#                     classes,\n#                     scores,\n#                     category_index,\n#                     figsize=(12, 16),\n#                     image_name=None):\n#   \"\"\"Wrapper function to visualize detections.\n\n#   Args:\n#     image_np: uint8 numpy array with shape (img_height, img_width, 3)\n#     boxes: a numpy array of shape [N, 4]\n#     classes: a numpy array of shape [N]. Note that class indices are 1-based,\n#       and match the keys in the label map.\n#     scores: a numpy array of shape [N] or None.  If scores=None, then\n#       this function assumes that the boxes to be plotted are groundtruth\n#       boxes and plot all boxes as black with no classes or scores.\n#     category_index: a dict containing category dictionaries (each holding\n#       category index `id` and category name `name`) keyed by category indices.\n#     figsize: size for the figure.\n#     image_name: a name for the image file.\n#   \"\"\"\n#   image_np_with_annotations = image_np.copy()\n#   viz_utils.visualize_boxes_and_labels_on_image_array(\n#       image_np_with_annotations,\n#       boxes,\n#       classes,\n#       scores,\n#       category_index,\n#       use_normalized_coordinates=True,\n#       min_score_thresh=0.8)\n#   if image_name:\n#     plt.imsave(image_name, image_np_with_annotations)\n#   else:\n#     plt.imshow(image_np_with_annotations)","f535bb8d":"# from kaggle_secrets import UserSecretsClient\n# user_secrets = UserSecretsClient()\n# secret_value_2 = user_secrets.get_secret(\"bloodcell_dataset_tf_tfrecords\")","4eee8f41":"# #Downloading data from Roboflow\n# #UPDATE THIS LINK - get our data from Roboflow\n# %cd \/kaggle\/working\n# !curl -L https:\/\/public.roboflow.com\/ds\/$secret_value_2 > roboflow.zip; unzip roboflow.zip; rm roboflow.zip","10f970bf":"# !ls train","48cad8be":"# # NOTE: Update these TFRecord names from \"cells\" and \"cells_label_map\" to your files!\n# test_record_fname = '\/kaggle\/working\/valid\/cells.tfrecord'\n# train_record_fname = '\/kaggle\/working\/train\/cells.tfrecord'\n# label_map_pbtxt_fname = '\/kaggle\/working\/train\/cells_label_map.pbtxt'","3dfb09f1":"# ##change chosen model to deploy different models available in the TF2 object detection zoo\n# MODELS_CONFIG = {\n#     'efficientdet-d0': {\n#         'model_name': 'efficientdet_d0_coco17_tpu-32',\n#         'base_pipeline_file': 'ssd_efficientdet_d0_512x512_coco17_tpu-8.config',\n#         'pretrained_checkpoint': 'efficientdet_d0_coco17_tpu-32.tar.gz',\n#         'batch_size': 16\n#     },\n#     'efficientdet-d1': {\n#         'model_name': 'efficientdet_d1_coco17_tpu-32',\n#         'base_pipeline_file': 'ssd_efficientdet_d1_640x640_coco17_tpu-8.config',\n#         'pretrained_checkpoint': 'efficientdet_d1_coco17_tpu-32.tar.gz',\n#         'batch_size': 16\n#     },\n#     'efficientdet-d2': {\n#         'model_name': 'efficientdet_d2_coco17_tpu-32',\n#         'base_pipeline_file': 'ssd_efficientdet_d2_768x768_coco17_tpu-8.config',\n#         'pretrained_checkpoint': 'efficientdet_d2_coco17_tpu-32.tar.gz',\n#         'batch_size': 16\n#     },\n#         'efficientdet-d3': {\n#         'model_name': 'efficientdet_d3_coco17_tpu-32',\n#         'base_pipeline_file': 'ssd_efficientdet_d3_896x896_coco17_tpu-32.config',\n#         'pretrained_checkpoint': 'efficientdet_d3_coco17_tpu-32.tar.gz',\n#         'batch_size': 16\n#     }\n# }\n\n# #in this tutorial we implement the lightweight, smallest state of the art efficientdet model\n# #if you want to scale up tot larger efficientdet models you will likely need more compute!\n# chosen_model = 'efficientdet-d0'\n\n# num_steps = 40000 #The more steps, the longer the training. Increase if your loss function is still decreasing and validation metrics are increasing. \n# num_eval_steps = 500 #Perform evaluation after so many steps\n\n# model_name = MODELS_CONFIG[chosen_model]['model_name']\n# pretrained_checkpoint = MODELS_CONFIG[chosen_model]['pretrained_checkpoint']\n# base_pipeline_file = MODELS_CONFIG[chosen_model]['base_pipeline_file']\n# batch_size = MODELS_CONFIG[chosen_model]['batch_size'] #if you can fit a large batch in memory, it may speed up your training","155a531a":"# #download pretrained weights\n# %mkdir \/kaggle\/working\/models\/research\/deploy\/\n# %cd \/kaggle\/working\/models\/research\/deploy\/\n\n# import tarfile\n# download_tar = 'http:\/\/download.tensorflow.org\/models\/object_detection\/tf2\/20200711\/' + pretrained_checkpoint\n\n# !wget {download_tar}\n# tar = tarfile.open(pretrained_checkpoint)\n# tar.extractall()\n# tar.close()","6a353e82":"# #download base training configuration file\n# %cd \/kaggle\/working\/models\/research\/deploy\n# download_config = 'https:\/\/raw.githubusercontent.com\/tensorflow\/models\/master\/research\/object_detection\/configs\/tf2\/' + base_pipeline_file\n# !wget {download_config}","ac983d4c":"# #prepare\n# pipeline_fname = '\/kaggle\/working\/models\/research\/deploy\/' + base_pipeline_file\n# fine_tune_checkpoint = '\/kaggle\/working\/models\/research\/deploy\/' + model_name + '\/checkpoint\/ckpt-0'\n\n# def get_num_classes(pbtxt_fname):\n#     from object_detection.utils import label_map_util\n#     label_map = label_map_util.load_labelmap(pbtxt_fname)\n#     categories = label_map_util.convert_label_map_to_categories(\n#         label_map, max_num_classes=90, use_display_name=True)\n#     category_index = label_map_util.create_category_index(categories)\n#     return len(category_index.keys())\n# num_classes = get_num_classes(label_map_pbtxt_fname)\n","31101d35":"# #write custom configuration file by slotting our dataset, model checkpoint, and training parameters into the base pipeline file\n\n# import re\n\n# %cd \/kaggle\/working\/models\/research\/deploy\n# print('writing custom configuration file')\n\n# with open(pipeline_fname) as f:\n#     s = f.read()\n# with open('pipeline_file.config', 'w') as f:\n    \n#     # fine_tune_checkpoint\n#     s = re.sub('fine_tune_checkpoint: \".*?\"',\n#                'fine_tune_checkpoint: \"{}\"'.format(fine_tune_checkpoint), s)\n    \n#     # tfrecord files train and test.\n#     s = re.sub(\n#         '(input_path: \".*?)(PATH_TO_BE_CONFIGURED\/train)(.*?\")', 'input_path: \"{}\"'.format(train_record_fname), s)\n#     s = re.sub(\n#         '(input_path: \".*?)(PATH_TO_BE_CONFIGURED\/val)(.*?\")', 'input_path: \"{}\"'.format(test_record_fname), s)\n\n#     # label_map_path\n#     s = re.sub(\n#         'label_map_path: \".*?\"', 'label_map_path: \"{}\"'.format(label_map_pbtxt_fname), s)\n\n#     # Set training batch_size.\n#     s = re.sub('batch_size: [0-9]+',\n#                'batch_size: {}'.format(batch_size), s)\n\n#     # Set training steps, num_steps\n#     s = re.sub('num_steps: [0-9]+',\n#                'num_steps: {}'.format(num_steps), s)\n    \n#     # Set number of classes num_classes.\n#     s = re.sub('num_classes: [0-9]+',\n#                'num_classes: {}'.format(num_classes), s)\n    \n#     #fine-tune checkpoint type\n#     s = re.sub(\n#         'fine_tune_checkpoint_type: \"classification\"', 'fine_tune_checkpoint_type: \"{}\"'.format('detection'), s)\n        \n#     f.write(s)\n\n","e920a059":"# %cat \/kaggle\/working\/models\/research\/deploy\/pipeline_file.config","c824ba08":"# pipeline_file = '\/kaggle\/working\/models\/research\/deploy\/pipeline_file.config'\n# model_dir = '\/kaggle\/working\/training\/'","0cb9ae30":"# !python \/kaggle\/working\/models\/research\/object_detection\/model_main_tf2.py \\\n#     --pipeline_config_path={pipeline_file} \\\n#     --model_dir={model_dir} \\\n#     --alsologtostderr \\\n#     --num_train_steps={num_steps} \\\n#     --sample_1_of_n_eval_examples=1 \\\n#     --num_eval_steps={num_eval_steps}","e6ff9132":"# %cd .\/models\/research","f53d5499":"# !protoc object_detection\/protos\/*.proto --python_out=.\n","ba041b66":"# !cp object_detection\/packages\/tf2\/setup.py . \n# !python -m pip -q install .","7c01ac68":"# !python object_detection\/builders\/model_builder_tf2_test.py\n# %cd .. ","3cb3a71b":"# import matplotlib\n# import matplotlib.pyplot as plt\n\n# import os\n# import random\n# import io\n# import imageio\n# import glob\n# import scipy.misc\n# import numpy as np\n# from six import BytesIO\n# from PIL import Image, ImageDraw, ImageFont\n# from IPython.display import display, Javascript\n# from IPython.display import Image as IPyImage\n\n# import tensorflow as tf\n\n# from object_detection.utils import label_map_util\n# from object_detection.utils import config_util\n# from object_detection.utils import visualization_utils as viz_utils\n# #the following one works with google colab only\n# #from object_detection.utils import colab_utils\n# from object_detection.builders import model_builder\n\n# %matplotlib inline","e406f4e5":"\n# !git clone https:\/\/github.com\/aalpatya\/detect_hands.git\n\n# !cp detect_hands\/egohands_dataset_to_csv.py .\n# !python egohands_dataset_to_csv.py","b0c37b0b":"# !cp detect_hands\/generate_tfrecord.py .\n# # For the train dataset\n# !python generate_tfrecord.py --csv_input=images\/train\/train_labels.csv  --output_path=train.record\n# # For the test dataset\n# !python generate_tfrecord.py --csv_input=images\/test\/test_labels.csv  --output_path=test.record","e5c70c08":"# !wget http:\/\/download.tensorflow.org\/models\/object_detection\/tf2\/20200711\/ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8.tar.gz\n# # Unzip\n# !tar -xzvf ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8.tar.gz\n","b6538c2c":"# !ls \/kaggle\/working\/models\/ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8","b25dc0f1":"# !mv \/kaggle\/working\/models\/ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8 \/kaggle\/working\/models\/research\/object_detection\/test_data\/","83827563":"# def load_image_into_numpy_array(path):\n#     \"\"\"Load an image from file into a numpy array.\n\n#     Puts image into numpy array to feed into tensorflow graph.\n#     Note that by convention we put it into a numpy array with shape\n#     (height, width, channels), where channels=3 for RGB.\n\n#     Args:\n#     path: a file path.\n\n#     Returns:\n#     uint8 numpy array with shape (img_height, img_width, 3)\n#     \"\"\"\n    \n#     img_data = tf.io.gfile.GFile(path, 'rb').read()\n#     image = Image.open(BytesIO(img_data))\n#     (im_width, im_height) = image.size\n    \n#     return np.array(image.getdata()).reshape(\n#         (im_height, im_width, 3)).astype(np.uint8)\n\n\n# def plot_detections(image_np,\n#                     boxes,\n#                     classes,\n#                     scores,\n#                     category_index,\n#                     figsize=(12, 16),\n#                     image_name=None):\n#     \"\"\"Wrapper function to visualize detections.\n\n#     Args:\n#     image_np: uint8 numpy array with shape (img_height, img_width, 3)\n#     boxes: a numpy array of shape [N, 4]\n#     classes: a numpy array of shape [N]. Note that class indices are 1-based,\n#           and match the keys in the label map.\n#     scores: a numpy array of shape [N] or None.  If scores=None, then\n#           this function assumes that the boxes to be plotted are groundtruth\n#           boxes and plot all boxes as black with no classes or scores.\n#     category_index: a dict containing category dictionaries (each holding\n#           category index `id` and category name `name`) keyed by category indices.\n#     figsize: size for the figure.\n#     image_name: a name for the image file.\n#     \"\"\"\n    \n#     image_np_with_annotations = image_np.copy()\n    \n#     viz_utils.visualize_boxes_and_labels_on_image_array(\n#         image_np_with_annotations,\n#         boxes,\n#         classes,\n#         scores,\n#         category_index,\n#         use_normalized_coordinates=True,\n#         min_score_thresh=0.8)\n    \n#     if image_name:\n#         plt.imsave(image_name, image_np_with_annotations)\n    \n#     else:\n#         plt.imshow(image_np_with_annotations)\n","7f48a697":"# # bounding boxes for each of the 5 zombies found in each image. \n# # you can use these instead of drawing the boxes yourself.\n# ref_gt_boxes = [\n#         np.array([[0.27333333, 0.41500586, 0.74333333, 0.57678781]]),\n#         np.array([[0.29833333, 0.45955451, 0.75666667, 0.61078546]]),\n#         np.array([[0.40833333, 0.18288394, 0.945, 0.34818288]]),\n#         np.array([[0.16166667, 0.61899179, 0.8, 0.91910903]]),\n#         np.array([[0.28833333, 0.12543962, 0.835, 0.35052755]]),\n#       ]","3f772307":"# tf.keras.backend.set_learning_phase(True)\n\n# # These parameters can be tuned; since our training set has 5 images\n# # it doesn't make sense to have a much larger batch size, though we could\n# # fit more examples in memory if we wanted to.\n# batch_size = 4\n# learning_rate = 0.01\n# num_batches = 100\n\n# # Select variables in top layers to fine-tune.\n# trainable_variables = detection_model.trainable_variables\n# to_fine_tune = []\n# prefixes_to_train = [\n#   'WeightSharedConvolutionalBoxPredictor\/WeightSharedConvolutionalBoxHead',\n#   'WeightSharedConvolutionalBoxPredictor\/WeightSharedConvolutionalClassHead']\n# for var in trainable_variables:\n#   if any([var.name.startswith(prefix) for prefix in prefixes_to_train]):\n#     to_fine_tune.append(var)\n\n# # Set up forward + backward pass for a single train step.\n# def get_model_train_step_function(model, optimizer, vars_to_fine_tune):\n#   \"\"\"Get a tf.function for training step.\"\"\"\n\n#   # Use tf.function for a bit of speed.\n#   # Comment out the tf.function decorator if you want the inside of the\n#   # function to run eagerly.\n#   @tf.function\n#   def train_step_fn(image_tensors,\n#                     groundtruth_boxes_list,\n#                     groundtruth_classes_list):\n#     \"\"\"A single training iteration.\n\n#     Args:\n#       image_tensors: A list of [1, height, width, 3] Tensor of type tf.float32.\n#         Note that the height and width can vary across images, as they are\n#         reshaped within this function to be 640x640.\n#       groundtruth_boxes_list: A list of Tensors of shape [N_i, 4] with type\n#         tf.float32 representing groundtruth boxes for each image in the batch.\n#       groundtruth_classes_list: A list of Tensors of shape [N_i, num_classes]\n#         with type tf.float32 representing groundtruth boxes for each image in\n#         the batch.\n\n#     Returns:\n#       A scalar tensor representing the total loss for the input batch.\n#     \"\"\"\n#     shapes = tf.constant(batch_size * [[640, 640, 3]], dtype=tf.int32)\n#     model.provide_groundtruth(\n#         groundtruth_boxes_list=groundtruth_boxes_list,\n#         groundtruth_classes_list=groundtruth_classes_list)\n#     with tf.GradientTape() as tape:\n#       preprocessed_images = tf.concat(\n#           [detection_model.preprocess(image_tensor)[0]\n#            for image_tensor in image_tensors], axis=0)\n#       prediction_dict = model.predict(preprocessed_images, shapes)\n#       losses_dict = model.loss(prediction_dict, shapes)\n#       total_loss = losses_dict['Loss\/localization_loss'] + losses_dict['Loss\/classification_loss']\n#       gradients = tape.gradient(total_loss, vars_to_fine_tune)\n#       optimizer.apply_gradients(zip(gradients, vars_to_fine_tune))\n#     return total_loss\n\n#   return train_step_fn\n\n# optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate, momentum=0.9)\n# train_step_fn = get_model_train_step_function(\n#     detection_model, optimizer, to_fine_tune)\n\n# print('Start fine-tuning!', flush=True)\n# for idx in range(num_batches):\n#   # Grab keys for a random subset of examples\n#   all_keys = list(range(len(train_images_np)))\n#   random.shuffle(all_keys)\n#   example_keys = all_keys[:batch_size]\n\n#   # Note that we do not do data augmentation in this demo.  If you want a\n#   # a fun exercise, we recommend experimenting with random horizontal flipping\n#   # and random cropping :)\n#   gt_boxes_list = [gt_box_tensors[key] for key in example_keys]\n#   gt_classes_list = [gt_classes_one_hot_tensors[key] for key in example_keys]\n#   image_tensors = [train_image_tensors[key] for key in example_keys]\n\n#   # Training step (forward pass + backwards pass)\n#   total_loss = train_step_fn(image_tensors, gt_boxes_list, gt_classes_list)\n\n#   if idx % 10 == 0:\n#     print('batch ' + str(idx) + ' of ' + str(num_batches)\n#     + ', loss=' +  str(total_loss.numpy()), flush=True)\n\n# print('Done fine-tuning!')","4b55cb47":"# !cat .\/detect_hands\/model_data\/ssd_mobilenet_v2_fpn_320\/label_map.pbtxt","a7d9800b":"# !cat .\/detect_hands\/model_data\/ssd_mobilenet_v2_fpn_320\/pipeline.config","ce3adaba":"# %%writefile \/kaggle\/working\/detect_hands\/model_data\/ssd_mobilenet_v2_fpn_320\/pipeline.config\n\n# model {\n#   ssd {\n#     num_classes: 1\n#     image_resizer {\n#       fixed_shape_resizer {\n#         height: 320\n#         width: 320\n#       }\n#     }\n#     feature_extractor {\n#       type: \"ssd_mobilenet_v2_fpn_keras\"\n#       depth_multiplier: 1.0\n#       min_depth: 16\n#       conv_hyperparams {\n#         regularizer {\n#           l2_regularizer {\n#             weight: 4e-05\n#           }\n#         }\n#         initializer {\n#           random_normal_initializer {\n#             mean: 0.0\n#             stddev: 0.01\n#           }\n#         }\n#         activation: RELU_6\n#         batch_norm {\n#           decay: 0.997\n#           scale: true\n#           epsilon: 0.001\n#         }\n#       }\n#       use_depthwise: true\n#       override_base_feature_extractor_hyperparams: true\n#       fpn {\n#         min_level: 3\n#         max_level: 7\n#         additional_layer_depth: 128\n#       }\n#     }\n#     box_coder {\n#       faster_rcnn_box_coder {\n#         y_scale: 10.0\n#         x_scale: 10.0\n#         height_scale: 5.0\n#         width_scale: 5.0\n#       }\n#     }\n#     matcher {\n#       argmax_matcher {\n#         matched_threshold: 0.5\n#         unmatched_threshold: 0.5\n#         ignore_thresholds: false\n#         negatives_lower_than_unmatched: true\n#         force_match_for_each_row: true\n#         use_matmul_gather: true\n#       }\n#     }\n#     similarity_calculator {\n#       iou_similarity {\n#       }\n#     }\n#     box_predictor {\n#       weight_shared_convolutional_box_predictor {\n#         conv_hyperparams {\n#           regularizer {\n#             l2_regularizer {\n#               weight: 4e-05\n#             }\n#           }\n#           initializer {\n#             random_normal_initializer {\n#               mean: 0.0\n#               stddev: 0.01\n#             }\n#           }\n#           activation: RELU_6\n#           batch_norm {\n#             decay: 0.997\n#             scale: true\n#             epsilon: 0.001\n#           }\n#         }\n#         depth: 128\n#         num_layers_before_predictor: 4\n#         kernel_size: 3\n#         class_prediction_bias_init: -4.6\n#         share_prediction_tower: true\n#         use_depthwise: true\n#       }\n#     }\n#     anchor_generator {\n#       multiscale_anchor_generator {\n#         min_level: 3\n#         max_level: 7\n#         anchor_scale: 4.0\n#         aspect_ratios: 1.0\n#         aspect_ratios: 2.0\n#         aspect_ratios: 0.5\n#         scales_per_octave: 2\n#       }\n#     }\n#     post_processing {\n#       batch_non_max_suppression {\n#         score_threshold: 1e-08\n#         iou_threshold: 0.6\n#         max_detections_per_class: 100\n#         max_total_detections: 100\n#         use_static_shapes: false\n#       }\n#       score_converter: SIGMOID\n#     }\n#     normalize_loss_by_num_matches: true\n#     loss {\n#       localization_loss {\n#         weighted_smooth_l1 {\n#         }\n#       }\n#       classification_loss {\n#         weighted_sigmoid_focal {\n#           gamma: 2.0\n#           alpha: 0.25\n#         }\n#       }\n#       classification_weight: 1.0\n#       localization_weight: 1.0\n#     }\n#     encode_background_as_zeros: true\n#     normalize_loc_loss_by_codesize: true\n#     inplace_batchnorm_update: true\n#     freeze_batchnorm: false\n#   }\n# }\n# train_config {\n#    fine_tune_checkpoint_version : V2\n#     fine_tune_checkpoint : \"\/kaggle\/working\/ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8\/checkpoint\/ckpt-0\"\n#     fine_tune_checkpoint_type : \"detection\" \n#   batch_size: 4\n#   data_augmentation_options {\n#     random_horizontal_flip {\n#     }\n#   }\n#   data_augmentation_options {\n#     random_crop_image {\n#       min_object_covered: 0.0\n#       min_aspect_ratio: 0.75\n#       max_aspect_ratio: 3.0\n#       min_area: 0.75\n#       max_area: 1.0\n#       overlap_thresh: 0.0\n#     }\n#   }\n#   sync_replicas: true\n#   optimizer {\n#     momentum_optimizer {\n#       learning_rate {\n#         cosine_decay_learning_rate {\n#           learning_rate_base: 0.08\n#           total_steps: 50000\n#           warmup_learning_rate: 0.026666\n#           warmup_steps: 1000\n#         }\n#       }\n#       momentum_optimizer_value: 0.9\n#     }\n#     use_moving_average: false\n#   }\n#   num_steps: 50000\n#   startup_delay_steps: 0.0\n#   replicas_to_aggregate: 8\n#   max_number_of_boxes: 100\n#   unpad_groundtruth_tensors: false\n# }\n# train_input_reader {\n#   label_map_path: \"\/kaggle\/working\/detect_hands\/model_data\/ssd_mobilenet_v2_fpn_320\/label_map.pbtxt\"\n#   tf_record_input_reader {\n#     input_path: \"\/kaggle\/working\/train.record\"\n#   }\n# }\n# eval_config {\n#   metrics_set: \"coco_detection_metrics\"\n#   use_moving_averages: false\n# }\n# eval_input_reader {\n#   label_map_path: \"\/kaggle\/working\/detect_hands\/model_data\/ssd_mobilenet_v2_fpn_320\/label_map.pbtxt\"\n#   shuffle: false\n#   num_epochs: 1\n#   tf_record_input_reader {\n#     input_path: \"\/kaggle\/working\/test.record\"\n#   }\n# }","72285c3a":"# %cd .\/models\/research\/object_detection\/","ada548f3":"# !python model_main_tf2.py \\\n# --pipeline_config_path=\/kaggle\/working\/detect_hands\/model_data\/ssd_mobilenet_v2_fpn_320\/pipeline.config \\\n# --model_dir=\/kaggle\/working\/detect_hands\/output_training --alsologtostderr","ad34085e":"# import tensorflow as tf\n\n# !rm -rf .\/logs\/ \n# !mkdir .\/logs\/","b13df991":"# !wget https:\/\/bin.equinox.io\/c\/4VmDzA7iaHb\/ngrok-stable-linux-amd64.zip\n# !unzip ngrok-stable-linux-amd64.zip\n\n# # Run tensorboard as well as Ngrox (for tunneling as non-blocking processes)\n# import os\n# import multiprocessing\n\n\n# pool = multiprocessing.Pool(processes = 10)\n# results_of_processes = [pool.apply_async(os.system, args=(cmd, ), callback = None )\n#                         for cmd in [\n#                         f\"tensorboard --logdir .\/logs\/ --host 0.0.0.0 --port 6006 &\",\n#                         \".\/ngrok http 6006 &\"\n#                         ]]","222149da":"# ! curl -s http:\/\/localhost:4040\/api\/tunnels | python3 -c \\\n#     \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\"","4fed9cb0":"# mnist = tf.keras.datasets.mnist\n\n# (x_train, y_train),(x_test, y_test) = mnist.load_data()\n# x_train, x_test = x_train \/ 255.0, x_test \/ 255.0\n\n# def create_model():\n#   return tf.keras.models.Sequential([\n#     tf.keras.layers.Flatten(input_shape=(28, 28)),\n#     tf.keras.layers.Dense(512, activation='relu'),\n#     tf.keras.layers.Dropout(0.2),\n#     tf.keras.layers.Dense(10, activation='softmax')\n#   ])","bbc38bc5":"# import datetime\n# model = create_model()\n# model.compile(optimizer='adam',\n#               loss='sparse_categorical_crossentropy',\n#               metrics=['accuracy'])\n\n# log_dir = \"logs\/fit\/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n# tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n\n# model.fit(x=x_train, \n#           y=y_train, \n#           epochs=10, \n#           validation_data=(x_test, y_test), \n#           callbacks=[tensorboard_callback])","48ed2cdf":"# import os\n# import pathlib\n\n# # Clone the tensorflow models repository if it doesn't already exist\n# if \"models\" in pathlib.Path.cwd().parts:\n#   while \"models\" in pathlib.Path.cwd().parts:\n#     os.chdir('..')\n# elif not pathlib.Path('models').exists():\n#   !git clone --depth 1 https:\/\/github.com\/tensorflow\/models","ad9c17c4":"# # Install the Object Detection API\n# !cd models\/research\/\n# !protoc object_detection\/protos\/*.proto --python_out=.\n# !cp object_detection\/packages\/tf2\/setup.py .\n# !python -m pip install .","c8844752":"#https:\/\/public.roboflow.com\/ds\/rcXspzRoxm?key=IdrJ79g98i\n","3282ffb2":"# !pip install -U -q --pre tensorflow==\"2.2.0\"","841a25d2":"# import os\n# import pathlib\n\n# # Clone the tensorflow models repository if it doesn't already exist\n# if \"models\" in pathlib.Path.cwd().parts:\n#   while \"models\" in pathlib.Path.cwd().parts:\n#     os.chdir('..')\n# elif not pathlib.Path('models').exists():\n#   !git clone --depth 1 https:\/\/github.com\/tensorflow\/models","1ecdbd39":"# %cd models\n# %cd research","17fc3ea4":"# %%bash\n# #cd models\/research\n# # Compile protos.\n# protoc object_detection\/protos\/*.proto --python_out=.\n# cp object_detection\/packages\/tf2\/setup.py .\n# python -m pip install .","538484f1":"# import matplotlib\n# import matplotlib.pyplot as plt\n\n# import os\n# import random\n# import io\n# import imageio\n# import glob\n# import scipy.misc\n# import numpy as np\n# from six import BytesIO\n# from PIL import Image, ImageDraw, ImageFont\n# from IPython.display import display, Javascript\n# from IPython.display import Image as IPyImage\n\n# import tensorflow as tf\n\n# from object_detection.utils import label_map_util\n# from object_detection.utils import config_util\n# from object_detection.utils import visualization_utils as viz_utils\n# #from object_detection.utils import colab_utils\n# from object_detection.builders import model_builder\n\n# %matplotlib inline","4f918a1a":"# def load_image_into_numpy_array(path):\n#   \"\"\"Load an image from file into a numpy array.\n\n#   Puts image into numpy array to feed into tensorflow graph.\n#   Note that by convention we put it into a numpy array with shape\n#   (height, width, channels), where channels=3 for RGB.\n\n#   Args:\n#     path: a file path.\n\n#   Returns:\n#     uint8 numpy array with shape (img_height, img_width, 3)\n#   \"\"\"\n#   img_data = tf.io.gfile.GFile(path, 'rb').read()\n#   image = Image.open(BytesIO(img_data))\n#   (im_width, im_height) = image.size\n#   return np.array(image.getdata()).reshape(\n#       (im_height, im_width, 3)).astype(np.uint8)\n\n# def plot_detections(image_np,\n#                     boxes,\n#                     classes,\n#                     scores,\n#                     category_index,\n#                     figsize=(12, 16),\n#                     image_name=None):\n#   \"\"\"Wrapper function to visualize detections.\n\n#   Args:\n#     image_np: uint8 numpy array with shape (img_height, img_width, 3)\n#     boxes: a numpy array of shape [N, 4]\n#     classes: a numpy array of shape [N]. Note that class indices are 1-based,\n#       and match the keys in the label map.\n#     scores: a numpy array of shape [N] or None.  If scores=None, then\n#       this function assumes that the boxes to be plotted are groundtruth\n#       boxes and plot all boxes as black with no classes or scores.\n#     category_index: a dict containing category dictionaries (each holding\n#       category index `id` and category name `name`) keyed by category indices.\n#     figsize: size for the figure.\n#     image_name: a name for the image file.\n#   \"\"\"\n#   image_np_with_annotations = image_np.copy()\n#   viz_utils.visualize_boxes_and_labels_on_image_array(\n#       image_np_with_annotations,\n#       boxes,\n#       classes,\n#       scores,\n#       category_index,\n#       use_normalized_coordinates=True,\n#       min_score_thresh=0.8)\n#   if image_name:\n#     plt.imsave(image_name, image_np_with_annotations)\n#   else:\n#     plt.imshow(image_np_with_annotations)","6fdf2f64":"# train_image_dir = '\/kaggle\/working\/models\/research\/object_detection\/test_images\/ducky\/train\/'\n# train_images_np = []\n# for i in range(1, 6):\n#   image_path = os.path.join(train_image_dir, 'robertducky' + str(i) + '.jpg')\n#   train_images_np.append(load_image_into_numpy_array(image_path))\n\n# plt.rcParams['axes.grid'] = False\n# plt.rcParams['xtick.labelsize'] = False\n# plt.rcParams['ytick.labelsize'] = False\n# plt.rcParams['xtick.top'] = False\n# plt.rcParams['xtick.bottom'] = False\n# plt.rcParams['ytick.left'] = False\n# plt.rcParams['ytick.right'] = False\n# plt.rcParams['figure.figsize'] = [14, 7]\n\n# for idx, train_image_np in enumerate(train_images_np):\n#   plt.subplot(2, 3, idx+1)\n#   plt.imshow(train_image_np)\n# plt.show()\n","305a8eec":"# gt_boxes = [\n#             np.array([[0.436, 0.591, 0.629, 0.712]], dtype=np.float32),\n#             np.array([[0.539, 0.583, 0.73, 0.71]], dtype=np.float32),\n#             np.array([[0.464, 0.414, 0.626, 0.548]], dtype=np.float32),\n#             np.array([[0.313, 0.308, 0.648, 0.526]], dtype=np.float32),\n#             np.array([[0.256, 0.444, 0.484, 0.629]], dtype=np.float32)\n# ]\n","d74fd836":"# duck_class_id = 1\n# num_classes = 1\n\n# category_index = {duck_class_id: {'id': duck_class_id, 'name': 'rubber_ducky'}}\n\n# # Convert class labels to one-hot; convert everything to tensors.\n# # The `label_id_offset` here shifts all classes by a certain number of indices;\n# # we do this here so that the model receives one-hot labels where non-background\n# # classes start counting at the zeroth index.  This is ordinarily just handled\n# # automatically in our training binaries, but we need to reproduce it here.\n# label_id_offset = 1\n# train_image_tensors = []\n# gt_classes_one_hot_tensors = []\n# gt_box_tensors = []\n# for (train_image_np, gt_box_np) in zip(\n#     train_images_np, gt_boxes):\n#   train_image_tensors.append(tf.expand_dims(tf.convert_to_tensor(\n#       train_image_np, dtype=tf.float32), axis=0))\n#   gt_box_tensors.append(tf.convert_to_tensor(gt_box_np, dtype=tf.float32))\n#   zero_indexed_groundtruth_classes = tf.convert_to_tensor(\n#       np.ones(shape=[gt_box_np.shape[0]], dtype=np.int32) - label_id_offset)\n#   gt_classes_one_hot_tensors.append(tf.one_hot(\n#       zero_indexed_groundtruth_classes, num_classes))\n# print('Done prepping data.')","e6908d85":"# dummy_scores = np.array([1.0], dtype=np.float32)  # give boxes a score of 100%\n\n# plt.figure(figsize=(30, 15))\n# for idx in range(5):\n#   plt.subplot(2, 3, idx+1)\n#   plot_detections(\n#       train_images_np[idx],\n#       gt_boxes[idx],\n#       np.ones(shape=[gt_boxes[idx].shape[0]], dtype=np.int32),\n#       dummy_scores, category_index)\n# plt.show()","390d4a27":"# !ls ssd_resnet50_v1_fpn_640x640_coco17_tpu-8","e0940550":"# !wget http:\/\/download.tensorflow.org\/models\/object_detection\/tf2\/20200711\/ssd_resnet50_v1_fpn_640x640_coco17_tpu-8.tar.gz\n# !tar -xf ssd_resnet50_v1_fpn_640x640_coco17_tpu-8.tar.gz\n# !mv ssd_resnet50_v1_fpn_640x640_coco17_tpu-8\/checkpoint \/kaggle\/working\/models\/research\/object_detection\/test_data\/","58732b5b":"# tf.keras.backend.clear_session()\n\n# print('Building model and restoring weights for fine-tuning...', flush=True)\n# num_classes = 1\n# pipeline_config = '\/kaggle\/working\/models\/research\/object_detection\/configs\/tf2\/ssd_resnet50_v1_fpn_640x640_coco17_tpu-8.config'\n# checkpoint_path = '\/kaggle\/working\/models\/research\/object_detection\/test_data\/checkpoint\/ckpt-0'\n\n# # Load pipeline config and build a detection model.\n# #\n# # Since we are working off of a COCO architecture which predicts 90\n# # class slots by default, we override the `num_classes` field here to be just\n# # one (for our new rubber ducky class).\n# configs = config_util.get_configs_from_pipeline_file(pipeline_config)\n# model_config = configs['model']\n# model_config.ssd.num_classes = num_classes\n# model_config.ssd.freeze_batchnorm = True\n# detection_model = model_builder.build(\n#       model_config=model_config, is_training=True)\n\n# # Set up object-based checkpoint restore --- RetinaNet has two prediction\n# # `heads` --- one for classification, the other for box regression.  We will\n# # restore the box regression head but initialize the classification head\n# # from scratch (we show the omission below by commenting out the line that\n# # we would add if we wanted to restore both heads)\n# fake_box_predictor = tf.compat.v2.train.Checkpoint(\n#     _base_tower_layers_for_heads=detection_model._box_predictor._base_tower_layers_for_heads,\n#     # _prediction_heads=detection_model._box_predictor._prediction_heads,\n#     #    (i.e., the classification head that we *will not* restore)\n#     _box_prediction_head=detection_model._box_predictor._box_prediction_head,\n#     )\n# fake_model = tf.compat.v2.train.Checkpoint(\n#           _feature_extractor=detection_model._feature_extractor,\n#           _box_predictor=fake_box_predictor)\n# ckpt = tf.compat.v2.train.Checkpoint(model=fake_model)\n# ckpt.restore(checkpoint_path).expect_partial()\n\n# # Run model through a dummy image so that variables are created\n# image, shapes = detection_model.preprocess(tf.zeros([1, 640, 640, 3]))\n# prediction_dict = detection_model.predict(image, shapes)\n# _ = detection_model.postprocess(prediction_dict, shapes)\n# print('Weights restored!')","917e7e55":"# tf.keras.backend.set_learning_phase(True)\n\n# # These parameters can be tuned; since our training set has 5 images\n# # it doesn't make sense to have a much larger batch size, though we could\n# # fit more examples in memory if we wanted to.\n# batch_size = 4\n# learning_rate = 0.01\n# num_batches = 100\n\n# # Select variables in top layers to fine-tune.\n# trainable_variables = detection_model.trainable_variables\n# to_fine_tune = []\n# prefixes_to_train = [\n#   'WeightSharedConvolutionalBoxPredictor\/WeightSharedConvolutionalBoxHead',\n#   'WeightSharedConvolutionalBoxPredictor\/WeightSharedConvolutionalClassHead']\n# for var in trainable_variables:\n#   if any([var.name.startswith(prefix) for prefix in prefixes_to_train]):\n#     to_fine_tune.append(var)\n\n# # Set up forward + backward pass for a single train step.\n# def get_model_train_step_function(model, optimizer, vars_to_fine_tune):\n#   \"\"\"Get a tf.function for training step.\"\"\"\n\n#   # Use tf.function for a bit of speed.\n#   # Comment out the tf.function decorator if you want the inside of the\n#   # function to run eagerly.\n#   @tf.function\n#   def train_step_fn(image_tensors,\n#                     groundtruth_boxes_list,\n#                     groundtruth_classes_list):\n#     \"\"\"A single training iteration.\n\n#     Args:\n#       image_tensors: A list of [1, height, width, 3] Tensor of type tf.float32.\n#         Note that the height and width can vary across images, as they are\n#         reshaped within this function to be 640x640.\n#       groundtruth_boxes_list: A list of Tensors of shape [N_i, 4] with type\n#         tf.float32 representing groundtruth boxes for each image in the batch.\n#       groundtruth_classes_list: A list of Tensors of shape [N_i, num_classes]\n#         with type tf.float32 representing groundtruth boxes for each image in\n#         the batch.\n\n#     Returns:\n#       A scalar tensor representing the total loss for the input batch.\n#     \"\"\"\n#     shapes = tf.constant(batch_size * [[640, 640, 3]], dtype=tf.int32)\n#     model.provide_groundtruth(\n#         groundtruth_boxes_list=groundtruth_boxes_list,\n#         groundtruth_classes_list=groundtruth_classes_list)\n#     with tf.GradientTape() as tape:\n#       preprocessed_images = tf.concat(\n#           [detection_model.preprocess(image_tensor)[0]\n#            for image_tensor in image_tensors], axis=0)\n#       prediction_dict = model.predict(preprocessed_images, shapes)\n#       losses_dict = model.loss(prediction_dict, shapes)\n#       total_loss = losses_dict['Loss\/localization_loss'] + losses_dict['Loss\/classification_loss']\n#       gradients = tape.gradient(total_loss, vars_to_fine_tune)\n#       optimizer.apply_gradients(zip(gradients, vars_to_fine_tune))\n#     return total_loss\n\n#   return train_step_fn\n\n# optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate, momentum=0.9)\n# train_step_fn = get_model_train_step_function(\n#     detection_model, optimizer, to_fine_tune)\n\n# print('Start fine-tuning!', flush=True)\n# for idx in range(num_batches):\n#   # Grab keys for a random subset of examples\n#   all_keys = list(range(len(train_images_np)))\n#   random.shuffle(all_keys)\n#   example_keys = all_keys[:batch_size]\n\n#   # Note that we do not do data augmentation in this demo.  If you want a\n#   # a fun exercise, we recommend experimenting with random horizontal flipping\n#   # and random cropping :)\n#   gt_boxes_list = [gt_box_tensors[key] for key in example_keys]\n#   gt_classes_list = [gt_classes_one_hot_tensors[key] for key in example_keys]\n#   image_tensors = [train_image_tensors[key] for key in example_keys]\n\n#   # Training step (forward pass + backwards pass)\n#   total_loss = train_step_fn(image_tensors, gt_boxes_list, gt_classes_list)\n\n#   if idx % 10 == 0:\n#     print('batch ' + str(idx) + ' of ' + str(num_batches)\n#     + ', loss=' +  str(total_loss.numpy()), flush=True)\n\n# print('Done fine-tuning!')","b8b19820":"# test_image_dir = '\/kaggle\/working\/models\/research\/object_detection\/test_images\/ducky\/test\/'\n# test_images_np = []\n# for i in range(1, 50):\n# \t\timage_path = os.path.join(test_image_dir, 'out' + str(i) + '.jpg')\n# \t\ttest_images_np.append(np.expand_dims(\n# \t\tload_image_into_numpy_array(image_path), axis=0))\n\n# # Again, uncomment this decorator if you want to run inference eagerly\n# @tf.function\n# def detect(input_tensor):\n# \t\tpreprocessed_image, shapes = detection_model.preprocess(input_tensor);prediction_dict = detection_model.predict(preprocessed_image, shapes)\n# \t\treturn detection_model.postprocess(prediction_dict, shapes)\n\n# # Note that the first frame will trigger tracing of the tf.function, which will\n# # take some time, after which inference should be fast.\n\n# label_id_offset = 1\n# for i in range(len(test_images_np)):\n# \t\tinput_tensor = tf.convert_to_tensor(test_images_np[i], dtype=tf.float32)\n# \t\tdetections = detect(input_tensor)\n\n# \t\tplot_detections(test_images_np[i][0], detections['detection_boxes'][0].numpy(), detections['detection_classes'][0].numpy().astype(np.uint32) + label_id_offset,\tdetections['detection_scores'][0].numpy(),category_index, figsize=(15, 20), image_name=\"gif_frame_\" + ('%02d' % i) + \".jpg\")","4e4ec98b":"Prepare dataset in yolo darknet format","a52f7355":"Here, in this example, I am using ready-made TFRecords to simplify understanding. We'll take on to creating custom TFRecords dataset soo but not now. That would be information overload.","2971f048":"### EfficientDet0","0d4bda91":"##### Brief Overview of the TF2 Object Detection Project Workflow (under making)","866ef597":"Library & Dependency setup complete! ","2eb65d2b":"###### Notebook in making...","bed61617":"labelmap.pbtxt should be under training_demo\/annotations <br>\ntest.record should be under training_demo\/annotations <br>\ntrain.record should be under training_demo\/annotations <br>\n","55411577":"Restoring the weights","9375f662":"### kaggle has permission issues with cuda & cudnn. Follow the procedure on colab or local till a workaround is found for Kaggle Containers. <br><br>","0724de4e":"#### Under pretrained models folder download all the pretrained models you wish to experiment with","370a76ce":"#### yolov4 (Darknet Model)","57fd67c2":"#### setup","011314c5":"##### Designing your Training Workspace","00bf167f":"%%writefile pipeline.config\nmodel {\n  ssd {\n    num_classes: {num_classes} # set this to the number of different label classes\n    image_resizer {\n      fixed_shape_resizer {\n        height: 640\n        width: 640\n      }\n    }\n    feature_extractor {\n      type: \"ssd_resnet50_v1_fpn_keras\"\n      depth_multiplier: 1.0\n      min_depth: 16\n      conv_hyperparams {\n        regularizer {\n          l2_regularizer {\n            weight: 0.00039999998989515007\n          }\n        }\n        initializer {\n          truncated_normal_initializer {\n            mean: 0.0\n            stddev: 0.029999999329447746\n          }\n        }\n        activation: RELU_6\n        batch_norm {\n          decay: 0.996999979019165\n          scale: true\n          epsilon: 0.0010000000474974513\n        }\n      }\n      override_base_feature_extractor_hyperparams: true\n      fpn {\n        min_level: 3\n        max_level: 7\n      }\n    }\n    box_coder {\n      faster_rcnn_box_coder {\n        y_scale: 10.0\n        x_scale: 10.0\n        height_scale: 5.0\n        width_scale: 5.0\n      }\n    }\n    matcher {\n      argmax_matcher {\n        matched_threshold: 0.5\n        unmatched_threshold: 0.5\n        ignore_thresholds: false\n        negatives_lower_than_unmatched: true\n        force_match_for_each_row: true\n        use_matmul_gather: true\n      }\n    }\n    similarity_calculator {\n      iou_similarity {\n      }\n    }\n    box_predictor {\n      weight_shared_convolutional_box_predictor {\n        conv_hyperparams {\n          regularizer {\n            l2_regularizer {\n              weight: 0.00039999998989515007\n            }\n          }\n          initializer {\n            random_normal_initializer {\n              mean: 0.0\n              stddev: 0.009999999776482582\n            }\n          }\n          activation: RELU_6\n          batch_norm {\n            decay: 0.996999979019165\n            scale: true\n            epsilon: 0.0010000000474974513\n          }\n        }\n        depth: 256\n        num_layers_before_predictor: 4\n        kernel_size: 3\n        class_prediction_bias_init: -4.599999904632568\n      }\n    }\n    anchor_generator {\n      multiscale_anchor_generator {\n        min_level: 3\n        max_level: 7\n        anchor_scale: 4.0\n        aspect_ratios: 1.0\n        aspect_ratios: 2.0\n        aspect_ratios: 0.5\n        scales_per_octave: 2\n      }\n    }\n    post_processing {\n      batch_non_max_suppression {\n        score_threshold: 9.99999993922529e-09\n        iou_threshold: 0.6000000238418579\n        max_detections_per_class: 100\n        max_total_detections: 100\n        use_static_shapes: false\n      }\n      score_converter: SIGMOID\n    }\n    normalize_loss_by_num_matches: true\n    loss {\n      localization_loss {\n        weighted_smooth_l1 {\n        }\n      }\n      classification_loss {\n        weighted_sigmoid_focal {\n          gamma: 2.0\n          alpha: 0.25\n        }\n      }\n      classification_weight: 1.0\n      localization_weight: 1.0\n    }\n    encode_background_as_zeros: true\n    normalize_loc_loss_by_codesize: true\n    inplace_batchnorm_update: true\n    freeze_batchnorm: false\n  }\n}\ntrain_config {\n  batch_size: {batch_size}   # Increase\/Decrease depending on the available memory\n  data_augmentation_options {\n    random_horizontal_flip {\n    }\n  }\n  data_augmentation_options {\n    random_crop_image {\n      min_object_covered: 0.0\n      min_aspect_ratio: 0.75\n      max_aspect_ratio: 3.0\n      min_area: 0.75\n      max_area: 1.0\n      overlap_thresh: 0.0\n    }\n  }\n  sync_replicas: true\n  optimizer {\n    momentum_optimizer {\n      learning_rate {\n        cosine_decay_learning_rate {\n          learning_rate_base: 0.03999999910593033\n          total_steps: 25000\n          warmup_learning_rate: 0.013333000242710114\n          warmup_steps: 2000\n        }\n      }\n      momentum_optimizer_value: 0.8999999761581421\n    }\n    use_moving_average: false\n  }\n  fine_tune_checkpoint: {ft_checkpoint}  # Path to checkpoint of pre-trained models\n  num_steps: 2000\n  startup_delay_steps: 0.0\n  replicas_to_aggregate: 8\n  max_number_of_boxes: 100\n  unpad_groundtruth_tensors: false\n  fine_tune_checkpoint_type: {ft_checkpoint_type} # Set this to detection as we want our full model to train for detection\n  use_bfloat16: {tpu} # # Set this to false if you are not training on a TPU\n  fine_tune_checkpoint_version: V2\n}\ntrain_input_reader {\n  label_map_path: {labelmap_path}  # Path to label map file\n  tf_record_input_reader {\n    input_path: {training_tfrecord} # Path to training record file\n  }\n}\neval_config {\n  metrics_set: \"coco_detection_metrics\"\n  use_moving_averages: false\n}\neval_input_reader {\n  label_map_path: {labelmap_path}  #Path to label map file\n  shuffle: false\n  num_epochs: 1\n  tf_record_input_reader {\n    input_path: {testing_tfrecord}  #Path to testing TFRecord\n  }\n}","2615a054":"Utility functions","2f6c465c":"Sample Images","57d2db93":"MASK R-CNN","4e4c8767":"Prepare your dataset","d33f9d8d":"testing on yolov5","47eeb9b0":"Log\n-----------\n\n\n| Version  | Comments  | Status |\n|---|---| ---|\n|  v5 | yolov5 fixed | Done  |\n| v7  | yolov3 added | Done  |\n| v8  | yolov4   | Done |\n| v9  | code fixes  | Done |\n| v10 | ssd_resnet50_v1_fpn_640x640_coco17_tpu-8 | Done|\n| v11 | Fixing the code | Done|\n| v12 | EffDet0 | Due | ","9c7e0bdc":"Activating Tensorboard on Kaggle Notebooks (Latest Docker Images for Kaggle don't support Tensorboard)","bf496bef":"Eager Mode training loop","16414b49":"## And we just ran into an error. ","15e51a1d":"Scratch training 2000 iterations","bc752f79":"### Yolov3","9f3b99dc":"#### Dataset Gathering for Yolov3 & Yolov5","36806ddb":"**the pretrained model should be downloaded under training_demo\/pre-trainined-models**","da09b776":"![](https:\/\/i.redd.it\/qp020ibgi7v41.jpg)","260ad23b":"Pretrained Training 2000 iterations","9fc0545c":"Using Blood Cells Detection dataset","477fd071":"Make a folder models under training_demo","284941c5":"## Tensorflow2 Object Detection","a4bdeb30":"### Data Science community is with you! :)\n\n![](https:\/\/cdn4.eyeem.com\/thumb\/a73ebc988569342622aa9e884a419bf433032f81-1539776693848\/w\/1280)","df44982c":"##### Generating the desired directory structure","720cac06":"Obtain the yolov3 & train","dfc95bc7":"presently working for ssd_resent50_fpn","12053eae":"### EfficientDet4","ac521400":"### Yolov5","099d6459":"training on yolov5","34b6a1c6":"TensorFlow\/<br>\n\u251c\u2500 addons\/ (Optional)<br>\n\u2502  \u2514\u2500 labelImg\/<br>\n\u251c\u2500 models\/ <br>\n\u2502  \u251c\u2500 community\/ <br>\n\u2502  \u251c\u2500 official\/ <br>\n\u2502  \u251c\u2500 orbit\/<br>\n\u2502  \u251c\u2500 research\/<br>\n\u2502  \u2514\u2500 ...<br>\n\u251c\u2500 scripts\/<br>\n\u2502  \u2514\u2500 preprocessing\/<br>\n\u2514\u2500 workspace\/ <br>\n   \u2514\u2500 training_demo\/ <br>","7e08ec03":"Here, models is the tensorflow2 models api git clone <br>\nworkspace is where training and loading of models happen","245b688d":"##### Estimated date of completion -> 13th - June - 2021","f8906b63":"### ssd_resnet50_v1_fpn_640x640_coco17_tpu-8\n","c1ff1fb5":"Setting up TF2 Object Detection Model Configurations","5572b14b":"#### This might seem difficult in the beginning, but don't worry and feel left out. \n\n![](https:\/\/images-wixmp-ed30a86b8c4ca887773594c2.wixmp.com\/f\/b2f14935-b7a6-46bb-8640-77626867290b\/da9zemv-0411af2e-022c-4e00-b434-a865a0ea6de8.jpg\/v1\/fill\/w_1024,h_851,q_75,strp\/diana_the_vampire_rabbit_by_pythos_cheetah_da9zemv-fullview.jpg?token=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJzdWIiOiJ1cm46YXBwOjdlMGQxODg5ODIyNjQzNzNhNWYwZDQxNWVhMGQyNmUwIiwiaXNzIjoidXJuOmFwcDo3ZTBkMTg4OTgyMjY0MzczYTVmMGQ0MTVlYTBkMjZlMCIsIm9iaiI6W1t7ImhlaWdodCI6Ijw9ODUxIiwicGF0aCI6IlwvZlwvYjJmMTQ5MzUtYjdhNi00NmJiLTg2NDAtNzc2MjY4NjcyOTBiXC9kYTl6ZW12LTA0MTFhZjJlLTAyMmMtNGUwMC1iNDM0LWE4NjVhMGVhNmRlOC5qcGciLCJ3aWR0aCI6Ijw9MTAyNCJ9XV0sImF1ZCI6WyJ1cm46c2VydmljZTppbWFnZS5vcGVyYXRpb25zIl19.NcSS0mvUQnDVbDWnTi6dlK49mLVQ0Yx4et4r3E1UbgE)","6d4d2cba":"# Table of Contents\n\n### Let's take a brief overview\n![overview](https:\/\/cdn.abcotvs.com\/dip\/images\/208049_072214-cc-mama-giraffe-img.jpg?w=1280&r=16%3A9)\n\n<br><br><br>\n* Theory\n    * Understanding Gradient Vectors, HOG, SS\n    * CNNs, ResNets ,Overfeats\n    * R-CNNs\n    * Fast Detection Models\n* Implementation\n    * YOLO Models\n        * yolov3\n        * Yolov4\n        * yolov4-tiny\n        * yolov4-scaled (csp)\n        * yolov5\n    * Tensorflow2 Object Detection API Models\n        * EfficientDet0\n        * EfficientDet4\n        * CenterNet50 V2 512x512\n        * CenterNet MobileNetV2 FPN 512x512\n        * Mask RCNN\n        * Fast RCNN\n        * Faster RCNN\n        * MobileNet v1 SSD\n        * MobileNet v2 SSD\n    \n    * Detectron2 ","bb6e1833":"This notebook is a compilation of all object detection models and their training methods using various datasets. The notebook is in making and will be completed by the 13th June 2021","5b4b0b21":"TF2 Object Detection Dependencies","aed0eab6":"Data Preparation\n\nTFRecord dataset","792e30d5":"Faster RCNN","a90f06c5":"Under models, make the corresponding folders for your pretrained models. Each of your pretrained model subfolder in models fodler will have a modified version of the pipeline.config file which needs to be configured as per the use case."}}