{"cell_type":{"888ce52a":"code","d9e4355f":"code","51bef174":"code","c667b045":"code","1af2cb9e":"code","e63a6777":"code","27d70047":"code","80a6fe8c":"code","bed33c41":"code","da6d99fe":"code","c0b3528a":"code","e43050ab":"code","1ba1438a":"code","4b43e3f7":"code","142cb85c":"code","2f8c4f19":"code","c2259720":"code","04490d82":"code","ffa30bd1":"code","c7145d88":"code","62565808":"code","639eaad9":"code","0d45171a":"code","a3f2aaac":"code","6e6f54e0":"code","1e36f5c5":"code","89c276b5":"code","6a97f0f3":"code","b8aef155":"code","6177fef2":"code","c2decabe":"code","f1c2d9e3":"code","02fdefe0":"code","bc380176":"code","fb7e16b2":"code","0fe641bd":"code","bb5f508d":"code","4f7f4847":"code","aeaf7f76":"code","7c5d0cb1":"code","94ffda15":"code","9d80c01f":"code","7cdfaf85":"code","dc31a566":"code","0a391f99":"code","bd00e694":"code","e65ed544":"markdown","2f26a8ad":"markdown","36b38881":"markdown","c070b3a9":"markdown","deb51d69":"markdown","670a27ec":"markdown","d201c1da":"markdown","63f67e5a":"markdown","7e3d3822":"markdown","24bb7f0a":"markdown","754b07c1":"markdown","99fb868e":"markdown","40a6f05e":"markdown","2854e09f":"markdown","aa94007a":"markdown","57365346":"markdown","de2298ce":"markdown","8841b3e7":"markdown","28c3e70a":"markdown","dae42508":"markdown","76ea974b":"markdown","48ed737e":"markdown","dfc21526":"markdown","773a9186":"markdown","ff87a5f2":"markdown","bb8b63dc":"markdown","9ab868d7":"markdown","b6667d01":"markdown","64a7fe99":"markdown","16b6d595":"markdown","ddf74b49":"markdown","2cd7560c":"markdown"},"source":{"888ce52a":"import pandas as pd\n#pandas\nimport numpy as np\n#numpy\nimport matplotlib.pyplot as plt\n#matplotlib\nimport seaborn as sns\n#seaborn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\nfrom sklearn.feature_extraction.text import CountVectorizer\n#sklearn\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers import Activation, Dense, Dropout, Embedding, Flatten, Conv1D, MaxPooling1D, LSTM\n#from tensorflow.keras.layers import Conv2D, MaxPooling2D\nfrom tensorflow.keras.layers import BatchNormalization\nfrom keras import utils\nfrom keras.callbacks import ReduceLROnPlateau, EarlyStopping\nfrom keras.optimizers import Adam\n#keras\nimport tensorflow as tf\n#tensorflow\nimport nltk \nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\nfrom nltk.stem import SnowballStemmer\n#nltk\nimport os\nimport nltk # For NLP\nimport re # For Regex \nimport string # For punctuation\n#other useful stuff\nfrom wordcloud import WordCloud, STOPWORDS\n#wordclouds and cloud stopwords\nimport warnings\nwarnings.filterwarnings(\"ignore\")","d9e4355f":"data = pd.read_csv('\/kaggle\/input\/sentiment140\/training.1600000.processed.noemoticon.csv', encoding = 'latin', header=None)","51bef174":"data.head(10)","c667b045":"data = data.rename(columns={0: 'target', 1: 'id', 2: 'date', 3: 'query', 4: 'username', 5: 'content'})","1af2cb9e":"data.head(10)","e63a6777":"missing_data = data.isna().sum().sort_values(ascending=False)\npercentage_missing = round((data.isnull().sum()\/data.isnull().count()).sort_values(ascending=False)*100,2)\nmissing_info = pd.concat([missing_data,percentage_missing],keys=['Missing values','Percentage'],axis=1)\nmissing_info.style.background_gradient()","27d70047":"pd.set_option('display.max_colwidth', -1)\ndata[data['target']==0]['content'].head()","80a6fe8c":"data[data['target']==4]['content'].head","bed33c41":"data[data['target']==2]['content'].head","da6d99fe":"data['target'] = data['target'].replace([0, 4],['Negative','Positive'])","c0b3528a":"fig = plt.figure(figsize=(8,8))\ntargets = data.groupby('target').size()\ntargets.plot(kind='pie', subplots=True, figsize=(10, 8), autopct = \"%.2f%%\", colors=['red','green'])\nplt.title(\"Pie chart of different classes of tweets\",fontsize=16)\nplt.ylabel(\"\")\nplt.legend()\nplt.show()","e43050ab":"data['target'].value_counts()","1ba1438a":"data['length'] = data.content.str.split().apply(len)","4b43e3f7":"fig = plt.figure(figsize=(14,7))\n\nax1 = fig.add_subplot(122)\nsns.distplot(data[data['target']=='Positive']['length'], ax=ax1,color='green')\ndescribe = data.length[data.target=='Positive'].describe().to_frame().round(2)\n\nax2 = fig.add_subplot(121)\nax2.axis('off')\nfont_size = 14\nbbox = [0, 0, 1, 1]\ntable = ax2.table(cellText = describe.values, rowLabels = describe.index, bbox=bbox, colLabels=describe.columns)\ntable.set_fontsize(font_size)\nfig.suptitle('Distribution of text length for positive sentiment tweets.', fontsize=16)\n\nplt.show()","142cb85c":"fig = plt.figure(figsize=(14,7))\n\nax1 = fig.add_subplot(122)\nsns.distplot(data[data['target']=='Negative']['length'], ax=ax1,color='red')\ndescribe = data.length[data.target=='Negative'].describe().to_frame().round(2)\n\nax2 = fig.add_subplot(121)\nax2.axis('off')\nfont_size = 14\nbbox = [0, 0, 1, 1]\ntable = ax2.table(cellText = describe.values, rowLabels = describe.index, bbox=bbox, colLabels=describe.columns)\ntable.set_fontsize(font_size)\nfig.suptitle('Distribution of text length for negative sentiment tweets.', fontsize=16)\n\nplt.show()","2f8c4f19":"plt.figure(figsize=(14,7))\ncommon_keyword=sns.barplot(x=data[data['target']=='Positive']['username'].value_counts()[:10].index, \\\n                           y=data[data['target']=='Positive']['username'].value_counts()[:10],palette='viridis')\ncommon_keyword.set_xticklabels(common_keyword.get_xticklabels(),rotation=90)\ncommon_keyword.set_ylabel('Positive tweet frequency',fontsize=12)\nplt.title('Top 10 users who publish positive tweets',fontsize=16)\nplt.show()","c2259720":"data[data['username']=='what_bugs_u'].head()","04490d82":"plt.figure(figsize=(14,7))\ncommon_keyword=sns.barplot(x=data[data['target']=='Negative']['username'].value_counts()[:10].index, \\\n                           y=data[data['target']=='Negative']['username'].value_counts()[:10],palette='Spectral')\ncommon_keyword.set_xticklabels(common_keyword.get_xticklabels(),rotation=90)\ncommon_keyword.set_ylabel('Negative tweet frequency',fontsize=12)\nplt.title('Top 10 users who publish negative tweets',fontsize=16)\nplt.show()","ffa30bd1":"data[data['username']=='lost_dog'].head()","c7145d88":"plt.figure(figsize=(14,7))\nword_cloud = WordCloud(stopwords = STOPWORDS, max_words = 200, width=1366, height=768, background_color=\"white\").generate(\" \".join(data[data.target=='Positive'].content))\nplt.imshow(word_cloud,interpolation='bilinear')\nplt.axis('off')\nplt.title('Most common words in positive sentiment tweets.',fontsize=20)\nplt.show()","62565808":"plt.figure(figsize=(14,7))\nword_cloud = WordCloud(stopwords = STOPWORDS, max_words = 200, width=1366, height=768, background_color=\"white\").generate(\" \".join(data[data.target=='Negative'].content))\nplt.imshow(word_cloud,interpolation='bilinear')\nplt.axis('off')\nplt.title('Most common words in negative sentiment tweets.',fontsize=20)\nplt.show()","639eaad9":"data.drop(['id','date','query','username','length'], axis=1, inplace=True)","0d45171a":"data.head()","a3f2aaac":"data.target = data.target.replace({'Positive': 1, 'Negative': 0})","6e6f54e0":"from string import punctuation\nprint(\"DATA CLEANING -- \\n\")\n# emojis defined\nemoji_pattern = re.compile(\"[\"\n         u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n         u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n         u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n         u\"\\U00002702-\\U000027B0\"\n         u\"\\U000024C2-\\U0001F251\"\n         \"]+\", flags=re.UNICODE)\n\ndef replace_emojis(t):\n  '''\n  This function replaces happy unicode emojis with \"happy\" and sad unicode emojis with \"sad.\n  '''\n  emoji_happy = [\"\\U0001F600\", \"\\U0001F601\", \"\\U0001F602\",\"\\U0001F603\",\"\\U0001F604\",\"\\U0001F605\", \"\\U0001F606\", \"\\U0001F607\", \"\\U0001F609\", \n                \"\\U0001F60A\", \"\\U0001F642\",\"\\U0001F643\",\"\\U0001F923\",r\"\\U0001F970\",\"\\U0001F60D\", r\"\\U0001F929\",\"\\U0001F618\",\"\\U0001F617\",\n                r\"\\U000263A\", \"\\U0001F61A\", \"\\U0001F619\", r\"\\U0001F972\", \"\\U0001F60B\", \"\\U0001F61B\", \"\\U0001F61C\", r\"\\U0001F92A\",\n                \"\\U0001F61D\", \"\\U0001F911\", \"\\U0001F917\", r\"\\U0001F92D\", r\"\\U0001F92B\",\"\\U0001F914\",\"\\U0001F910\", r\"\\U0001F928\", \"\\U0001F610\", \"\\U0001F611\",\n                \"\\U0001F636\", \"\\U0001F60F\",\"\\U0001F612\", \"\\U0001F644\",\"\\U0001F62C\",\"\\U0001F925\",\"\\U0001F60C\",\"\\U0001F614\",\"\\U0001F62A\",\n                \"\\U0001F924\",\"\\U0001F634\", \"\\U0001F920\", r\"\\U0001F973\", r\"\\U0001F978\",\"\\U0001F60E\",\"\\U0001F913\", r\"\\U0001F9D0\"]\n\n  emoji_sad = [\"\\U0001F637\",\"\\U0001F912\",\"\\U0001F915\",\"\\U0001F922\", r\"\\U0001F92E\",\"\\U0001F927\", r\"\\U0001F975\", r\"\\U0001F976\", r\"\\U0001F974\",\n                       \"\\U0001F635\", r\"\\U0001F92F\", \"\\U0001F615\",\"\\U0001F61F\",\"\\U0001F641\", r\"\\U0002639\",\"\\U0001F62E\",\"\\U0001F62F\",\"\\U0001F632\",\n                       \"\\U0001F633\", r\"\\U0001F97A\",\"\\U0001F626\",\"\\U0001F627\",\"\\U0001F628\",\"\\U0001F630\",\"\\U0001F625\",\"\\U0001F622\",\"\\U0001F62D\",\n                       \"\\U0001F631\",\"\\U0001F616\",\"\\U0001F623\"\t,\"\\U0001F61E\",\"\\U0001F613\",\"\\U0001F629\",\"\\U0001F62B\", r\"\\U0001F971\",\n                       \"\\U0001F624\",\"\\U0001F621\",\"\\U0001F620\", r\"\\U0001F92C\",\"\\U0001F608\",\"\\U0001F47F\",\"\\U0001F480\", r\"\\U0002620\"]\n\n  words = t.split()\n  reformed = []\n  for w in words:\n    if w in emoji_happy:\n      reformed.append(\"happy\")\n    elif w in emoji_sad:\n      reformed.append(\"sad\") \n    else:\n      reformed.append(w)\n  t = \" \".join(reformed)\n  return t\n\n\ndef replace_smileys(t):\n  '''\n  This function replaces happy smileys with \"happy\" and sad smileys with \"sad.\n  '''\n  emoticons_happy = set([':-)', ':)', ';)', ':o)', ':]', ':3', ':c)', ':>', '=]', '8)', '=)', ':}', ':D',\n    ':^)', ':-D', ':D', '8-D', '8D', 'x-D', 'xD', 'X-D', 'XD', '=-D', '=D',\n    '=-3', '=3', ':-))', \":'-)\", \":')\", ':*', ':^*', '>:P', ':-P', ':P', 'X-P',\n    'x-p', 'xp', 'XP', ':-p', ':p', '=p', ':-b', ':b', '>:)', '>;)', '>:-)', '<3'])\n\n  emoticons_sad = set([':L', ':-\/', '>:\/', ':S', '>:[', ':@', ':-(', ':[', ':-||', '=L', ':<',\n    ':-[', ':-<', '=\\\\', '=\/', '>:(', ':(', '>.<', \":'-(\", \":'(\", ':\\\\', ':-c',\n    ':c', ':{', '>:\\\\', ';('])  \n\n  words = t.split()\n  reformed = []\n  for w in words:\n    if w in emoticons_happy:\n      reformed.append(\"happy\")\n    elif w in emoticons_sad:\n      reformed.append(\"sad\") \n    else:\n      reformed.append(w)\n  t = \" \".join(reformed)\n  return t\n\ndef replace_contractions(t):\n  '''\n  This function replaces english lanuage contractions like \"shouldn't\" with \"should not\"\n  '''\n  cont = {\"aren't\" : 'are not', \"can't\" : 'cannot', \"couln't\": 'could not', \"didn't\": 'did not', \"doesn't\" : 'does not',\n  \"hadn't\": 'had not', \"haven't\": 'have not', \"he's\" : 'he is', \"she's\" : 'she is', \"he'll\" : \"he will\", \n  \"she'll\" : 'she will',\"he'd\": \"he would\", \"she'd\":\"she would\", \"here's\" : \"here is\", \n   \"i'm\" : 'i am', \"i've\"\t: \"i have\", \"i'll\" : \"i will\", \"i'd\" : \"i would\", \"isn't\": \"is not\", \n   \"it's\" : \"it is\", \"it'll\": \"it will\", \"mustn't\" : \"must not\", \"shouldn't\" : \"should not\", \"that's\" : \"that is\", \n   \"there's\" : \"there is\", \"they're\" : \"they are\", \"they've\" : \"they have\", \"they'll\" : \"they will\",\n   \"they'd\" : \"they would\", \"wasn't\" : \"was not\", \"we're\": \"we are\", \"we've\":\"we have\", \"we'll\": \"we will\", \n   \"we'd\" : \"we would\", \"weren't\" : \"were not\", \"what's\" : \"what is\", \"where's\" : \"where is\", \"who's\": \"who is\",\n   \"who'll\" :\"who will\", \"won't\":\"will not\", \"wouldn't\" : \"would not\", \"you're\": \"you are\", \"you've\":\"you have\",\n   \"you'll\" : \"you will\", \"you'd\" : \"you would\", \"mayn't\" : \"may not\"}\n  words = t.split()\n  reformed = []\n  for w in words:\n    if w in cont:\n      reformed.append(cont[w])\n    else:\n      reformed.append(w)\n  t = \" \".join(reformed)\n  return t  \n\ndef remove_single_letter_words(t):\n  '''\n  This function removes words that are single characters\n  '''\n  words = t.split()\n  reformed = []\n  for w in words:\n    if len(w) > 1:\n      reformed.append(w)\n  t = \" \".join(reformed)\n  return t  \n\nprint(\"Cleaning the tweets from the data.\\n\")\nprint(\"Replacing handwritten emojis with their feeling associated.\")\nprint(\"Convert to lowercase.\")\nprint(\"Replace contractions.\")\nprint(\"Replace unicode emojis with their feeling associated.\")\nprint(\"Remove all other unicoded emojis.\")\nprint(\"Remove NON- ASCII characters.\")\nprint(\"Remove numbers.\")\nprint(\"Remove \\\"#\\\". \")\nprint(\"Remove \\\"@\\\". \")\nprint(\"Remove usernames.\")\nprint(\"Remove \\'RT\\'. \")\nprint(\"Replace all URLs and Links with word \\'URL\\'.\")\nprint(\"Remove all punctuations.\")\nprint(\"Removes single letter words.\\n\")\n\ndef dataclean(t):\n  '''\n  This function cleans the tweets.\n  '''\n  t = replace_smileys(t) # replace handwritten emojis with their feeling associated\n  t = t.lower() # convert to lowercase\n  t = replace_contractions(t) # replace short forms used in english  with their actual words\n  t = replace_emojis(t) # replace unicode emojis with their feeling associated\n  t = emoji_pattern.sub(r'', t) # remove emojis other than smiley emojis\n  t = re.sub('\\\\\\\\u[0-9A-Fa-f]{4}','', t) # remove NON- ASCII characters\n  t = re.sub(\"[0-9]\", \"\", t) # remove numbers # re.sub(\"\\d+\", \"\", t)\n  t = re.sub('#', '', t) # remove '#'\n  t = re.sub('@[A-Za-z0\u20139]+', '', t) # remove '@'\n  t = re.sub('@[^\\s]+', '', t) # remove usernames\n  t = re.sub('RT[\\s]+', '', t) # remove retweet 'RT'\n  t = re.sub('((www\\.[^\\s]+)|(https?:\/\/[^\\s]+))', '', t) # remove links (URLs\/ links)\n  t = re.sub('[!\"$%&\\'()*+,-.\/:@;<=>?[\\\\]^_`{|}~]', '', t) # remove punctuations\n  t = t.replace('\\\\\\\\', '')\n  t = t.replace('\\\\', '')\n  t = remove_single_letter_words(t) # removes single letter words\n  \n  return t\n\ndata['content'] = data['content'].apply(dataclean)\nprint(\"Tweets have been cleaned.\")","1e36f5c5":"data.head()","89c276b5":"english_stopwords = stopwords.words('english')\n#base of english stopwords\nstemmer = SnowballStemmer('english')\n#stemming algorithm\nregex = \"@\\S+|https?:\\S+|http?:\\S|[^A-Za-z0-9]+\"\n#regex for mentions and links in tweets","6a97f0f3":"def preprocess(content, stem=False):\n  content = re.sub(regex, ' ', str(content).lower()).strip()\n  tokens = []\n  for token in content.split():\n    if token not in english_stopwords:\n      tokens.append(stemmer.stem(token))\n  return \" \".join(tokens)","b8aef155":"data.content = data.content.apply(lambda x: preprocess(x))","6177fef2":"data.head()","c2decabe":"train, test = train_test_split(data, test_size=0.1, random_state=44)","f1c2d9e3":"print('Train dataset shape: {}'.format(train.shape))\nprint('Test dataset shape: {}'.format(test.shape))","02fdefe0":"tokenizer = Tokenizer()\ntokenizer.fit_on_texts(train.content)  \nvocab_size = len(tokenizer.word_index) + 1 \nmax_length = 50","bc380176":"sequences_train = tokenizer.texts_to_sequences(train.content) \nsequences_test = tokenizer.texts_to_sequences(test.content) \n\nX_train = pad_sequences(sequences_train, maxlen=max_length, padding='post')\nX_test = pad_sequences(sequences_test, maxlen=max_length, padding='post')\n\ny_train = train.target.values\ny_test = test.target.values","fb7e16b2":"embeddings_dictionary = dict()\nembedding_dim = 100\nglove_file = open('\/kaggle\/input\/glove6b100dtxt\/glove.6B.100d.txt')\n\nfor line in glove_file:\n    records = line.split()\n    word = records[0]\n    vector_dimensions = np.asarray(records[1:], dtype='float32')\n    embeddings_dictionary [word] = vector_dimensions\n    \nglove_file.close()\n\nembeddings_matrix = np.zeros((vocab_size, embedding_dim))\nfor word, index in tokenizer.word_index.items():\n    embedding_vector = embeddings_dictionary.get(word)\n    if embedding_vector is not None:\n        embeddings_matrix[index] = embedding_vector","0fe641bd":"embedding_layer = tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length, weights=[embeddings_matrix], trainable=False)","bb5f508d":"num_epochs = 10\nbatch_size = 1000","4f7f4847":"model = Sequential([\n        embedding_layer,\n        tf.keras.layers.Bidirectional(LSTM(128, return_sequences=True)),\n        tf.keras.layers.Dropout(0.4),\n       # tf.keras.layers.BatchNormalization(),\n        tf.keras.layers.Bidirectional(LSTM(128)),\n        tf.keras.layers.Dropout(0.4),\n       # tf.keras.layers.BatchNormalization(),\n        tf.keras.layers.Dense(64, activation='relu'),\n        tf.keras.layers.Dense(1, activation='sigmoid'),\n    ])","aeaf7f76":"model.summary()","7c5d0cb1":"tf.keras.utils.plot_model(model, show_shapes=True)","94ffda15":"model.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=0.001), metrics=['accuracy'])\n#es = EarlyStopping(monitor='val_loss', mode='min', verbose=1)\nhistory = model.fit(X_train, y_train, batch_size = batch_size, epochs=num_epochs, validation_data=(X_test, y_test), verbose=2)# , callbacks=[es])","9d80c01f":"y_pred = model.predict(X_test)\ny_pred = np.where(y_pred>0.5, 1, 0)","7cdfaf85":"print(classification_report(y_test, y_pred))","dc31a566":"#History for accuracy\nplt.figure(figsize=(10,5))\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['Train accuracy', 'Test accuracy'], loc='lower right')\nplt.show()\n# History for loss\nplt.figure(figsize=(10,5))\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['Train loss', 'Test loss'], loc='upper right')\nplt.suptitle('Accuracy and loss for second model')\nplt.show()","0a391f99":"vec = CountVectorizer()\n\nxeval=[\"I hate when I have to call and wake people up \"]\nprint(xeval)\nxeval_numeric = vec.fit_transform(xeval).toarray() \nprediction=model.predict(xeval_numeric)\nprint(prediction)","bd00e694":"for i in range(10):\n    print(X_test[i], y_pred[i])","e65ed544":"# **Exploratory data analysis**","2f26a8ad":"# **Model - Embedding + Stacked LSTM**\n\n\nModel consisted of layers build with lstm cells. With such a large amount of data, the model is computationally complex making the training process take a while. Furthermore, model regularization layers will reduce the possible overfitting which was present in the simpler models tested.","36b38881":"Adding new column to dataset with length of particular tweets.","c070b3a9":"In such a large dataset, tweets belonging to two classes are almost the same lengths. However, the average tweet length for the negative class is about 0.8 words longer.","deb51d69":"Head of data after stemming and removing https.","670a27ec":"### **Most commonly tweeting users**","d201c1da":"By looking at the top part of the dataset we can learn a lot from it. We can indicate which column refers to. Therefore, we can describe them briefly:\n\n\n*   0 - target of sentiment\n*   1 - id of user\n*   2 - date of tweet\n*   3 - unnecessary column, in each row contains 'NO_QUERY'\n*   4 - nickname of author\n*   5 - content of tweet","63f67e5a":"It seems that lost dog bots are rather in a bad mood.","7e3d3822":"# **Model test harness** \n\nThe proposed model architecture will be tested on the following parameters:\n\n*   **loss** = \"binnary_crossentropy\" (due to binary classification problem)\n*   **optimizer** = Adam(learning_rate=0.001) (may be changed after seeing the learning graph)\n*   **metrics** = \"accuracy\" (due to binary classification problem)\n*   **number of epochs** = 10 (due to the large training data set)\n*   **batch size** = 1000 (in order to accelerate learning time)","24bb7f0a":"### **Content cleaning**\n\nStemming - it does refers to the process which goal is to reduce words into thier base form. In case of our problem for classification it is very important ooperation as we need to focus on the meaning of particular word. For instance words: *Running, Runned, Runner* all can reduce to the stem *Run*. Below we have used the base of english stopwords and stemming algorithm from nltk library.","754b07c1":"Fortunately, dataset is free of missing values.","99fb868e":"Now, columns of dataset are much more informative.","40a6f05e":"### **Word embeddings using GloVe**\n\nWord embeddings provide a dense representation of words and their relative meanings. Embedding Matrix is a maxtrix of all words and their corresponding embeddings. Embedding matrix is used in embedding layer in model to embedded a token into it's vector representation, that contains information regarding that token or word.\n\nEmbedding vocabulary is taken from the tokenizer and the corresponding vectors from embedding model, which in this case is GloVe model. GloVe stand for Global Vectors for Word Representation \nand it is an unsupervised learning algorithm for obtaining vector representations for words. Training is performed on aggregated global word-word co-occurrence statistics from a corpus, and the resulting representations showcase interesting linear substructures of the word vector space.\n\nBelow was used pretrained GloVe embeddings from world known Stanford vector files. The smallest available file contains embeddings created for tiny 6 billions of tokens.","2854e09f":"Replacing Positive and Negative labels with 1 and 0 respectively.","aa94007a":"# **EDA**","57365346":"By reading the content of the tweets, we can conclude that they have a rather negative message, so class 0 refers to negative sentiments tweets.","de2298ce":"### **Missing values**\n\n\nMissing data is common occurance in datasets, therefore it is recommended to check if a data set contains missing values before starting any analysis.","8841b3e7":"### **Tokenization**\n\nIt is a particular kind of document segmentation. It does breaks up text into smaller chunks or segments called tokens. A tokenizer breaks unstructured data, natural language text, into chunks of information that can be counted as discrete elements. After this operation these counts of token occurences in particular document can be used as a vector representing given document.","28c3e70a":"### **Renaming column names**\n\n\nBecause of the numerical column names, it will be more convenient to work with a dataset with predefined column names.","dae42508":"By reading the content of the tweets, we can conclude that they have a rather positive message, so class 4 refers to positive sentiments tweets.","76ea974b":"Seems like what_bugs_u is kind of a user who is in good mood really often.","48ed737e":"# **Data preparing**\n### **Dropping unnecessary columns**\n\n\nThere are a lot of unnecessary columns in the following dataset. The task is to classify the semantics of a tweet, so all columns except the target and content columns are unnecessary.","dfc21526":"### **Length of tweet content**\n\nBased on this analysis, we can find out the length of tweets for two particular classes of tweets.","773a9186":"### **Wordclouds**\n\nBy creating word clouds for two classes, we can visualize what words were repeated most often for positive and negative classes. We don't want to show stopwords so i took base of stopwords from nltk library and i passed it to WordCloud function","ff87a5f2":"Based on the analysis of the tweet length it was concluded that the maximum length for tokenization equal to 50 will be sufficient","bb8b63dc":"As we can see dataset is perfectly balanced with the same numbers of occurrences for both classes. It is also worth mentioning that the data is not skewed which will certainly make modeling easier.","9ab868d7":"### **Train test split**\nDue to the rather large size of the dataset 160000 tweets will be enough for testing.","b6667d01":"### **Targets**\n\n\n\nFirstly let's see what the classes of the individual tweets are about.","64a7fe99":"Based on the word cloud, it can be deduced that the most repeated words in tweets with negative sentiment are words such as: quot, lol, today which are the same as for positive sentiment class. However, there are also word occurrences from which negative sentiment of a tweet can be inferred such as: miss, sorry, hate etc.","16b6d595":"Based on the word cloud, it can be deduced that the most repeated words in tweets with positive sentiment are words such as: love, quot, lol, haha, thank, today.","ddf74b49":"Changing labels from 0 and 4 for more informative labels for further analysis.","2cd7560c":"Based on the contet posted by the user, it can be concluded that this is not a regular user but it is just a bot."}}