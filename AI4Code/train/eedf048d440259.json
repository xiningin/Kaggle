{"cell_type":{"22d86aac":"code","5d0fdb3f":"code","d9c2bc93":"code","82d32fdd":"code","13f9e34b":"code","7956d88c":"code","6184b0a1":"code","33750788":"code","9b3c1c7d":"code","a247a240":"code","dff0351c":"code","a42b9a1f":"code","a1256486":"code","b32c83c7":"code","c6701d27":"code","126e6296":"code","f213698e":"code","6eb530b6":"code","01927bb0":"code","260a030e":"code","11ddaba7":"code","9689bdf8":"code","3b996b4b":"code","7801f498":"code","708affc5":"code","09655c96":"code","72308158":"code","d21e71e2":"code","45b37b4c":"code","b0606887":"code","27ba48f5":"code","8621791a":"code","883636f3":"code","6c887b1b":"code","f9d7d0f1":"code","b2030ec4":"markdown","61a4bd41":"markdown","106fc3bf":"markdown","acab8737":"markdown","7128e124":"markdown","b41ae226":"markdown","fdbc1895":"markdown","ad7d9818":"markdown","b8e0f20d":"markdown","e427c67f":"markdown","bb358b59":"markdown","6e959bb6":"markdown","2cae26d5":"markdown","7279df4e":"markdown","2c85a269":"markdown","f0a1230d":"markdown","3c99f9a2":"markdown","78c04539":"markdown","d14e15bf":"markdown"},"source":{"22d86aac":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","5d0fdb3f":"#import neccessary modules\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","d9c2bc93":"# Loading the dataset \ndf = pd.read_csv('..\/input\/iris-flower-dataset\/IRIS.csv')","82d32fdd":"# see the first 5 rows of data \ndf.head()","13f9e34b":"#lets see the shape of dataset\ndf.shape","7956d88c":"#datatypes of features\ndf.dtypes","6184b0a1":"#lets understand target label\ndf['species'].value_counts()","33750788":"#checking if missing values are present\ndf.isna().sum()","9b3c1c7d":"#understand statistical summary of numerical columns\ndf.describe()","a247a240":"#understand the correlation between features\ndf.corr()","dff0351c":"#check variance of the dataset\ndf.var()","a42b9a1f":"sns.swarmplot(x='species', y='petal_length', data=df)\nplt.show()","a1256486":"sns.swarmplot(x='species', y='sepal_width', data=df)\nplt.show()","b32c83c7":"sns.pairplot(df, hue='species', diag_kind='hist')\nplt.show()","c6701d27":"sns.scatterplot(x='petal_length', y='sepal_width', data=df, hue='species')\nplt.show()","126e6296":"#dropping redudant feature\n#as we know dropping petal_length or petal_width would help imporve model performance\n\ndf_1= df.drop('petal_width', axis=1)\ndf_1.head()","f213698e":"#Change Class label dtype to Category\ndf_1['species']= df_1['species'].astype('category')","6eb530b6":"# Seperating the data into dependent and independent variables\nX = df_1.iloc[:, :-1].values\ny = df_1.iloc[:,-1].cat.codes","01927bb0":"#import necessary scikit learn modules\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import confusion_matrix, classification_report, accuracy_score","260a030e":"#split the training, test set\nX_train, X_test, y_train, y_test= train_test_split(X, y, random_state=42)\n\nX_train.shape, X_test.shape, y_train.shape, y_test.shape\n\n#note= statify is not used since the distribution of target labels are even","11ddaba7":"#instantiate KNN model\nknn= KNeighborsClassifier(n_neighbors=6)\n\n#fit KNeighbors Classifiers model to training set\nknn.fit(X_train, y_train)","9689bdf8":"#predict labels for test set\ny_pred= knn.predict(X_test)\nprint(y_pred)","3b996b4b":"#Checking the training and test set accuracy to understand if model is overfit or underfit\n\ntraining_accuracy= knn.score(X_train, y_train)\ntest_accuracy= knn.score(X_test, y_test)\n\nprint('training set accuracy is',training_accuracy)\nprint('test set accuracy is',test_accuracy)","7801f498":"# Accuracy score\nfrom sklearn.metrics import accuracy_score\nprint('accuracy is',accuracy_score(y_pred,y_test))","708affc5":"from sklearn.linear_model import LogisticRegression\n\n#instantiate Logistic Regression model\nlogreg= LogisticRegression(solver='lbfgs')\n\n#Fit Logistic Regression model to training data\nlogreg.fit(X_train, y_train)","09655c96":"#predict labels for test set\ny_pred2= logreg.predict(X_test)\nprint(y_pred2)","72308158":"#Checking the training and test set accuracy to understand if model is overfit or underfit\n\nlogreg_training_accuracy= logreg.score(X_train, y_train)\nlogreg_test_accuracy= logreg.score(X_test, y_test)\n\nprint('training set accuracy is',logreg_training_accuracy)\nprint('test set accuracy is',logreg_test_accuracy)","d21e71e2":"# Accuracy score\nfrom sklearn.metrics import accuracy_score\nprint('accuracy is',accuracy_score(y_pred2,y_test))","45b37b4c":"from sklearn.tree import DecisionTreeClassifier\n\ndt= DecisionTreeClassifier(max_depth=2)\n\n#fit Decision Tree Classifier to training set\ndt.fit(X_train, y_train)","b0606887":"y_pred3= dt.predict(X_test)\ny_pred3","27ba48f5":"#Checking the training and test set accuracy to understand if model is overfit or underfit\n\ndt_training_accuracy= dt.score(X_train, y_train)\ndt_test_accuracy= dt.score(X_test, y_test)\n\nprint('training set accuracy is',dt_training_accuracy)\nprint('test set accuracy is',dt_test_accuracy)","8621791a":"# Accuracy score\nfrom sklearn.metrics import accuracy_score\nprint('accuracy is',accuracy_score(y_pred3,y_test))","883636f3":"from sklearn.ensemble import RandomForestClassifier\n\nrf= RandomForestClassifier(n_estimators=25,)\n\n#Fit the random forest model to the training data\nrf.fit(X_train, y_train)\n","6c887b1b":"y_pred4= dt.predict(X_test)\ny_pred4","f9d7d0f1":"#Checking the training and test set accuracy to understand if model is overfit or underfit\n\nrf_training_accuracy= rf.score(X_train, y_train)\nrf_test_accuracy= rf.score(X_test, y_test)\n\nprint('training set accuracy is',rf_training_accuracy)\nprint('test set accuracy is',rf_test_accuracy)","b2030ec4":"We can interpret anything from histogram it mainly depends on no.of bin. Hence, beeswarm plot is adviseable to show each and every point as data points are shrinked in histograms into bins.","61a4bd41":"**Visual EDA**","106fc3bf":"petal_length and petal_width are highly correlated to eachother(r=0.96).\nso, keeping both of them become redundant features. we may also consider correlation between petal_length and sepal_length. they also have high r value(i.e. 0.87)\n\nFor now, lets consider dropping petal_width alone.","acab8737":"so, we have 3 different types specifies are present. this is multiclass classification problem","7128e124":"**Classification Model 1- KNeighbors Classifier**","b41ae226":"Thanks for your Time!\n\nIf you like the Workbook, Please Upvote.\n\nSuggestions and Comments are always welcome.\n\n\nIf you have any questions regarding any part of the notebook, feel free to post.\n\nThank You.","fdbc1895":"petal_length and petal_width are correlated and petal_length have high variance among features","ad7d9818":"none of the column have missing values. so, we have dataset of all useful information","b8e0f20d":"**Model 4- Random Forest Classifier**","e427c67f":"**Statistical EDA**","bb358b59":"In Logistic regession, model is underfits on testing set","6e959bb6":"Training set and Test set accuracies are almost same. so, this is better model","2cae26d5":"mean and 50% of observations are close, so we dont need to standardize the data","7279df4e":"**Data Preprocessing**","2c85a269":"4 of features are numerical columns and 1 is categorical columns. Species is categorical and it is discrete one.Assume species is our target label. so, our task is to build a model to classify specifies\nWith target label is known, We must build a model for Classification problem","f0a1230d":"Like Logistic regession, Decision Tree model also underfits on testing set","3c99f9a2":"Model 2 - Logistic Regression","78c04539":"we can visually detect redundant features. For example, if any two features are correlated to each other, it is better to keep either one of them to make a case. Seaborn pairplot is used to show correlations between numerical features","d14e15bf":"**Model 3- Decision Tree Classifier**"}}