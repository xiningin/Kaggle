{"cell_type":{"960eeb9b":"code","16e16f5b":"code","fce17c6a":"code","c20bbe33":"code","ecbc6b54":"code","468e8e10":"code","2b2348ec":"code","f3220751":"code","760fbee5":"code","91cb7f88":"code","6a1744d4":"markdown","5a9f5a45":"markdown","e6f16b02":"markdown","721bfc8c":"markdown","cfeaf348":"markdown","a3e6e927":"markdown","86944d5f":"markdown"},"source":{"960eeb9b":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport os\nimport io\nfrom PIL import Image\n\nfrom tqdm.notebook import tqdm\nfrom sklearn.model_selection import train_test_split\nimport tensorflow as tf\nfrom kaggle_datasets import KaggleDatasets","16e16f5b":"NUM_SHARDS = 16\nIMAGE_SIZE = (600, 600)\nSEED=42\n\nIMAGE_DIR = '..\/input\/shopee-product-matching\/train_images'","fce17c6a":"df = pd.read_csv('..\/input\/shopee-product-matching\/train.csv')\ndisplay(df)","c20bbe33":"df.groupby(['label_group'])['posting_id'].nunique().sort_values()","ecbc6b54":"match_map = df.groupby(['label_group'])['posting_id'].unique().to_dict()\ndf['matches'] = df['label_group'].map(match_map)\ndf","468e8e10":"label_mapper = dict(zip(df['label_group'].unique(), np.arange(len(df['label_group'].unique()))))\ndf['label_group'] = df['label_group'].map(label_mapper)\ndisplay(df)","2b2348ec":"def encode_image(filepath, method='bilinear'):\n    image_string = tf.io.read_file(filepath)\n    image = tf.image.decode_jpeg(image_string, channels=3)\n    #Must convert dtype to float32 for most resizing methods to work\n    image = tf.image.convert_image_dtype(image, tf.float32)\n    image = tf.image.resize(image, IMAGE_SIZE, method=method, antialias=True)\n    #Convert dtype to uint8 to be encoded to bytestring for tfrec\n    image = tf.image.convert_image_dtype(image, tf.uint8)\n    image = tf.image.encode_jpeg(image, optimize_size=True)\n    return image\n\ndef featurize(val):\n    if isinstance(val, (bytes, str, tf.Tensor)):\n        if isinstance(val, type(tf.constant(0))):\n            val = val.numpy() \n        elif isinstance(val, str):\n            val = str.encode(val)\n        return tf.train.Feature(bytes_list=tf.train.BytesList(value=[val]))\n    elif isinstance(val, (int, np.integer)):\n        return tf.train.Feature(int64_list=tf.train.Int64List(value=[val]))\n    elif isinstance(val, (float, np.floating)):\n        return tf.train.Feature(float_list=tf.train.FloatList(value=[val]))\n    else:\n        raise Exception(f'Cannot featurize due to type {type(val)}')","f3220751":"def serialize_example(row):    \n    feature = row.to_dict()\n    img_path = os.path.join(IMAGE_DIR, feature['image'])\n    feature['image'] = encode_image(img_path)\n    feature['matches'] = tf.io.serialize_tensor(tf.convert_to_tensor(feature['matches']))\n    for k,v in feature.items():\n        feature[k] = featurize(v)\n        \n    example_proto = tf.train.Example(features=tf.train.Features(feature=feature))\n    return example_proto.SerializeToString()","760fbee5":"def write_tfr(df, filepath, filename, file_index, file_size, image_indexes):       \n    with tf.io.TFRecordWriter(f'{filepath}\/{filename}%.2i.tfrec'%(file_index)) as writer:\n        start = file_size * file_index\n        end = file_size * (file_index + 1)\n        for i in tqdm(image_indexes[start:end]):\n            example = serialize_example(df.loc[i])\n            writer.write(example)\n\ndef to_tfr(df, filepath, filename):\n    if not os.path.exists(filepath):\n        os.makedirs(filepath)\n    image_indexes = df.index.values\n    file_size = len(image_indexes) \/\/ 15\n    file_count = len(image_indexes) \/\/ file_size + int(len(image_indexes) % file_size != 0)\n    for file_index in range(file_count):\n        print('Writing TFRecord %i of %i...'%(file_index, file_count))\n        write_tfr(df, filepath, filename, file_index, file_size, image_indexes)","91cb7f88":"to_tfr(df, '.\/train', 'train')","6a1744d4":"For large datasets, it is good practice to split TFRecords into \"shards\". ","5a9f5a45":"## Basic EDA for confirmation","e6f16b02":"Kaggle allows for competitors to expedite deep learning model training via tensor processing units (TPUs). However, it is necessary to convert the data into a TFRecord format and feed these files through Google Cloud Storage (GCS). [Conversion to TFRecords enables one to fully take advantage of the extra processing power TPUs provide by avoiding data bottlenecks](https:\/\/www.kaggle.com\/docs\/tpu)\n\nThis notebook demonstrates the encoding of the Shopee image data into TFRecords. Based on [mattbast's work in the Google Landmark Retrieval 2020 competition](https:\/\/www.kaggle.com\/mattbast\/google-landmarks-2020-create-a-tfrecord-dataset\/notebook)","721bfc8c":"## Detecting matches\n\nThe following two cells create a list of matches. This is not required, but may be useful for model training.","cfeaf348":"## Convert dataframe to TFRecords","a3e6e927":"# Shopee product matching triplet data conversion to TFRecords","86944d5f":"This confirms that the maximum size of a label group to be 50, as stated in the competition rules."}}