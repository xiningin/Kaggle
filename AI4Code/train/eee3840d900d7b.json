{"cell_type":{"7611d46e":"code","503df04d":"code","92e2222a":"code","1a910b4f":"code","d65500fe":"code","6b973395":"code","af96825c":"code","e90aa5ee":"code","cfe1f3d0":"code","6f603de0":"code","ae445f95":"code","a8971f78":"code","6b1134d3":"code","20b85da3":"code","dc1a9b29":"code","b4cc0685":"code","b9c72f54":"code","3450763c":"code","3be2e5c7":"code","c38e7fa7":"code","55cc21ba":"code","98fec6b9":"code","18e3fea5":"code","aab2858c":"code","8a58a74f":"code","bdadee80":"code","57779e91":"code","d25c216c":"code","56da280f":"code","7db38cfb":"code","aea839df":"code","267d7504":"code","11b037f7":"code","39d6d9cc":"code","e2d83c5c":"code","8476d72d":"code","c5271f93":"code","500540f4":"code","fa3693cd":"markdown","037f2e79":"markdown","067d3f44":"markdown","1d481f80":"markdown","fb6af120":"markdown","612fb6f6":"markdown","d99a772a":"markdown","31378e2a":"markdown","a484d381":"markdown","0560dbe9":"markdown","7f79eb5a":"markdown","f398de9e":"markdown","76935f1a":"markdown","7d48de4a":"markdown","7219ccb7":"markdown","13b29532":"markdown","3fe86902":"markdown","e62afa19":"markdown","1668c4b4":"markdown","482af867":"markdown"},"source":{"7611d46e":"import json\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\nimport time\nimport tensorflow as tf\nimport seaborn as sns\nfrom tensorflow import keras\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import f1_score, classification_report\nfrom collections import defaultdict\nfrom tensorflow.keras import layers\nfrom collections import defaultdict","503df04d":"vocab_size = 10000 # Vocabulary size\nsequence_length = 1024 # Sequence Length\nbatch_size = 128 # Batch size\nunk_token = \"<UNK>\" # Unknownd token\npadding_token = \"<PAD>\"\nnone_class = \"O\"\nvectorizer_path = \"vectorizer.json\"\n# Use output dataset for inference\noutput_dataset_path = \"..\/input\/name-entity-with-transformer-output\/\"\nmodel_path = \"model.h5\"\nembed_size = 128\nhidden_size = 64\nmodes = [\"training\", \"inference\"] # There is training and inference mode\nmode = modes[0]\nepochs = 30","92e2222a":"train = pd.read_csv(\"..\/input\/feedback-prize-2021\/train.csv\")\ntrain.head()","1a910b4f":"submission = pd.read_csv(\"..\/input\/feedback-prize-2021\/sample_submission.csv\")\nsubmission.head()","d65500fe":"train[\"file_path\"] = train[\"id\"].apply(lambda item: \"..\/input\/feedback-prize-2021\/train\/\" + item + \".txt\")\ntrain.head()","6b973395":"submission[\"file_path\"] = submission[\"id\"].apply(lambda item: \"..\/input\/feedback-prize-2021\/test\/\" + item + \".txt\")\nsubmission.head()","af96825c":"train[\"discourse_type\"].value_counts().plot(kind=\"bar\")","e90aa5ee":"discourse_types = list(train[\"discourse_type\"].value_counts().index)","cfe1f3d0":"all_tags = [padding_token]\nfor discourse_type in discourse_types:\n    all_tags.append(\"B-\" + discourse_type)\nfor discourse_type in discourse_types:\n    all_tags.append(\"I-\" + discourse_type)\nall_tags.append(none_class)\npad_index = all_tags.index(padding_token)\nnone_index = all_tags.index(none_class)\ntag_index = dict([(tag, index) for (index, tag) in enumerate(all_tags)])\nindex_tag = dict([(tag_index[tag], tag.replace(\"B-\", \"\").replace(\"I-\", \"\")) for tag in tag_index])\nprint(all_tags)\nprint(tag_index)\nprint(index_tag)","6f603de0":"len(train[\"id\"].unique())","ae445f95":"def tokenize(content):\n    tokens = content.lower().split()\n    return tokens\n    \ndef calc_word_indices(full_text, discourse_start, discourse_end):\n    start_index = len(full_text[:discourse_start].split())\n    token_len = len(full_text[discourse_start:discourse_end].split())\n    output = list(range(start_index, start_index + token_len))\n    if output[-1] >= len(full_text.split()):\n        output = list(range(start_index, start_index + token_len-1))\n    return output\ndef get_range(item):\n    locations = [int(location) for location in item[\"predictionstring\"].split(\" \")]\n    return (locations[0], locations[-1])\ndef add_annotation(all_data, start_index, end_index, discourse_type):\n    for j in range(start_index, end_index): \n        if j == start_index:\n            all_data[-1][1][j] = tag_index[\"B-\" + discourse_type]\n        else:\n            all_data[-1][1][j] = tag_index[\"I-\" + discourse_type]","a8971f78":"%%time\nbegin = time.time()\nlast_id = \"\"\nall_data = []\nids = []\nmatch_count = 0\nstart_index = 0\nfor i in range(len(train)):\n    item = train.iloc[i]\n    identifier = item[\"id\"] \n    discourse_type = item[\"discourse_type\"]\n    if identifier != last_id:\n        last_id = identifier\n        with open(item[\"file_path\"]) as f:\n            content = \"\".join(f.readlines()) \n            tokens = tokenize(content)\n            annotations = [none_index] * len(tokens) \n            all_data.append((tokens, annotations))\n            ids.append(last_id)\n            start_index = 0\n    annotation_range = get_range(item)\n    indices = calc_word_indices(content, int(item[\"discourse_start\"]), int(item[\"discourse_end\"]))\n    if annotation_range[0] == indices[0] and annotation_range[1] == indices[-1]:\n        match_count += 1\n        add_annotation(all_data, annotation_range[0], annotation_range[-1] + 1, discourse_type)\nprint(f\"Match count: {match_count}, Correct Rate: {match_count \/ len(train)}\")\nprint(all_data[0])","6b1134d3":"word_counter = defaultdict(int)\nfor item in all_data:\n    for token in item[0]: \n        word_counter[token] += 1","20b85da3":"word_count = pd.DataFrame({\"key\": word_counter.keys(), \"count\": word_counter.values()})\nword_count.sort_values(by=\"count\", ascending=False, inplace=True)\nword_count.head(30)","dc1a9b29":"plt.figure(figsize=(15, 10))\nsns.barplot(x=\"key\", y=\"count\", data=word_count[:30])","b4cc0685":"word_count.describe()","b9c72f54":"len(word_count)","3450763c":"(word_count[\"count\"] <= 1).sum()","3be2e5c7":"pd.DataFrame({\"Sentence Lengths\": [len(item[0]) for item in all_data]}).hist()","c38e7fa7":"class Vectorizer:\n    \n    def __init__(self, vocab_size = None, sequence_length = None, unk_token = \"<unk>\"):\n        self.vocab_size = vocab_size\n        self.sequence_length = sequence_length\n        self.unk_token = unk_token\n        \n    def fit_transform(self, sentences):\n        word_counter = dict()\n        for tokens in sentences:\n            for token in tokens: \n                if token in word_counter:\n                    word_counter[token] += 1\n                else:\n                    word_counter[token] = 1\n        word_counter = pd.DataFrame({\"key\": word_counter.keys(), \"count\": word_counter.values()})\n        word_counter.sort_values(by=\"count\", ascending=False, inplace=True)\n        vocab = set(word_counter[\"key\"][0:self.vocab_size-2])\n        word_index = dict()\n        begin_index = 1 \n        word_index[self.unk_token] = begin_index\n        begin_index += 1\n        Xs = []\n        for i in range(len(sentences)):\n            X = []\n            for token in sentences[i]:\n                if token not in word_index and token in vocab:\n                    word_index[token] = begin_index\n                    begin_index += 1\n                if token in word_index:\n                    X.append(word_index[token])\n                else:\n                    X.append(word_index[self.unk_token])\n                if len(X) == self.sequence_length:\n                    break\n            Xs.append(X)\n        self.word_index = word_index\n        self.vocab = vocab\n        return Xs\n    \n    def transform(self, sentences):\n        Xs = []\n        for i in range(len(sentences)):\n            X = []\n            for token in sentences[i]:\n                if token in self.word_index:\n                    X.append(self.word_index[token])\n                else:\n                    X.append(self.word_index[self.unk_token])\n                if len(X) == self.sequence_length:\n                    break\n            Xs.append(X)\n        return Xs\n    \n    def load(self, path):\n        with open(path, 'r') as f:\n            dic = json.load(f)\n            self.vocab_size = dic['vocab_size']\n            self.sequence_length = dic['sequence_length']\n            self.unk_token = dic['unk_token']\n            self.word_index = dic['word_index']\n            \n    def save(self, path):\n        with open(path, 'w') as f:\n            data = json.dumps({\n                \"vocab_size\": self.vocab_size, \n                \"sequence_length\": self.sequence_length, \n                \"unk_token\": self.unk_token,\n                \"word_index\": self.word_index\n            })\n            f.write(data)","55cc21ba":"%%time\nvectorizer = Vectorizer(vocab_size = vocab_size, sequence_length = sequence_length, unk_token = unk_token)\ntoken_list = [item[0] for item in all_data]\nif mode == modes[0]:\n    Xs = vectorizer.fit_transform(token_list)\n    vectorizer.save(vectorizer_path)\nelse:\n    vectorizer.load(output_dataset_path + vectorizer_path)\n    Xs = vectorizer.transform(token_list)\nfor i in range(len(all_data)):\n    item = all_data[i]\n    annotation = item[1]\n    if len(annotation) > sequence_length:\n        annotation = annotation[0:sequence_length]\n    all_data[i] = (Xs[i], annotation)\ntrain_data, val_data, train_ids, valid_ids = train_test_split(all_data, ids, test_size = 0.1, random_state=42)","98fec6b9":"def export_to_file(export_file_path, data):\n    with open(export_file_path, \"w+\") as f:\n        for i in range(len(data)):\n            X = data[i][0]\n            y = data[i][1]\n            f.write(\n                str(len(X))\n                + \"\\t\"\n                + \"\\t\".join([str(item) for item in X])\n                + \"\\t\"\n                + \"\\t\".join([str(item) for item in y])\n                + \"\\n\"\n            )","18e3fea5":"export_to_file(\"train.txt\", train_data)\nexport_to_file(\"validation.txt\", val_data)","aab2858c":"def preprocess(record):\n    record = tf.strings.split(record, sep=\"\\t\")\n    length = tf.strings.to_number(record[0], out_type=tf.int32)\n    tokens = record[1 : length + 1]\n    tags = record[length + 1 :]\n    tokens = tf.strings.to_number(tokens, out_type=tf.int64)\n    tags = tf.strings.to_number(tags, out_type=tf.int64)\n    return tokens, tags\ndef make_dataset(file_path, batch_size, mode=\"train\"):\n    ds = tf.data.TextLineDataset(file_path).map(preprocess)\n    if mode == \"train\":\n        ds = ds.shuffle(256)\n    ds = ds.padded_batch(batch_size)\n    \n    ds = ds.cache().prefetch(tf.data.AUTOTUNE)\n    return ds","8a58a74f":"train_ds = make_dataset(\"train.txt\", batch_size, mode=\"train\")\nval_ds = make_dataset(\"validation.txt\", batch_size, mode=\"valid\")","bdadee80":"for X, y in train_ds.take(2):\n    print(X)\n    print(y)","57779e91":"accuracy_metric = keras.metrics.SparseCategoricalAccuracy()\ndef accuracy(y_true, y_pred):\n    acc = accuracy_metric(y_true, y_pred)\n    mask = tf.cast((y_true > 0), dtype=tf.float32)\n    acc = acc * mask\n    return tf.reduce_sum(acc) \/ tf.reduce_sum(mask)","d25c216c":"## Reference https:\/\/keras.io\/examples\/nlp\/ner_transformers\/\nclass CustomNonPaddingTokenLoss(keras.losses.Loss):\n    def __init__(self, name=\"custom_ner_loss\"):\n        super().__init__(name=name)\n\n    def call(self, y_true, y_pred):\n        loss_fn = keras.losses.SparseCategoricalCrossentropy(\n            from_logits=True, reduction=keras.losses.Reduction.NONE\n        )\n        loss = loss_fn(y_true, y_pred)\n        mask = tf.cast((y_true > 0), dtype=tf.float32)\n        loss = loss * mask\n        return tf.reduce_sum(loss) \/ tf.reduce_sum(mask)\n    \nclass TransformerBlock(layers.Layer):\n    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n        super(TransformerBlock, self).__init__()\n        self.att = keras.layers.MultiHeadAttention(\n            num_heads=num_heads, key_dim=embed_dim\n        )\n        self.ffn = keras.Sequential(\n            [\n                keras.layers.Dense(ff_dim, activation=\"relu\"),\n                keras.layers.Dense(embed_dim),\n            ]\n        )\n        self.layernorm1 = keras.layers.LayerNormalization(epsilon=1e-6)\n        self.layernorm2 = keras.layers.LayerNormalization(epsilon=1e-6)\n        self.dropout1 = keras.layers.Dropout(rate)\n        self.dropout2 = keras.layers.Dropout(rate)\n\n    def call(self, inputs, training=False):\n        attn_output = self.att(inputs, inputs)\n        attn_output = self.dropout1(attn_output, training=training)\n        out1 = self.layernorm1(inputs + attn_output)\n        ffn_output = self.ffn(out1)\n        ffn_output = self.dropout2(ffn_output, training=training)\n        return self.layernorm2(out1 + ffn_output)\nclass PositionalEmbedding(layers.Layer):\n    def __init__(self, maxlen, vocab_size, embed_dim):\n        super(PositionalEmbedding, self).__init__()\n        self.token_emb = keras.layers.Embedding(\n            input_dim=vocab_size, output_dim=embed_dim, mask_zero=True\n        )\n        self.pos_emb = keras.layers.Embedding(input_dim=maxlen, output_dim=embed_dim, mask_zero=True)\n\n    def call(self, inputs):\n        maxlen = tf.shape(inputs)[-1]\n        positions = tf.range(start=0, limit=maxlen, delta=1)\n        position_embeddings = self.pos_emb(positions)\n        token_embeddings = self.token_emb(inputs)\n        return token_embeddings + position_embeddings\n    ","56da280f":"model = keras.Sequential([\n    PositionalEmbedding(sequence_length, vocab_size, embed_size),\n    TransformerBlock(embed_size, 4, 32),\n    TransformerBlock(embed_size, 4, 32),\n    layers.Dropout(0.1),\n    layers.Dense(32, activation=\"relu\"),\n    layers.Dropout(0.1),\n    layers.Dense(len(all_tags), activation=\"softmax\")\n])","7db38cfb":"if mode == modes[0]:\n    early_stop = keras.callbacks.EarlyStopping(\n        min_delta=1e-4, \n        monitor=\"val_loss\",\n        patience=10\n    )\n    reduce_lr = keras.callbacks.ReduceLROnPlateau(\n        factor=0.3,\n        patience=4, \n        monitor=\"val_loss\",\n        min_lr=1e-7\n    )\n    optimizer = tf.keras.optimizers.Adam(1e-3)\n    loss = CustomNonPaddingTokenLoss()\n    #loss = keras.losses.SparseCategoricalCrossentropy()\n    model.compile(loss=loss, optimizer=optimizer, metrics=[accuracy])\n    callbacks = [early_stop, reduce_lr]\n    \n    model.fit(train_ds, epochs=epochs, validation_data=val_ds, callbacks=callbacks)\n    model.save_weights(model_path)\nelse:\n    model.build(input_shape=(None, None))\n    model.load_weights(output_dataset_path+model_path)","aea839df":"model.summary()","267d7504":"keras.utils.plot_model(model, show_shapes=True, show_dtype=True)","11b037f7":"def evaluate(model, dataset):\n    all_true_tag_ids, all_predicted_tag_ids = [], []\n    for x, y in dataset:\n        output = model.predict(x)\n        predictions = np.argmax(output, axis=-1)\n        predictions = np.reshape(predictions, [-1])\n\n        true_tag_ids = np.reshape(y, [-1])\n\n        mask = (true_tag_ids != 0) & (true_tag_ids != none_index) & (predictions > 0)\n        true_tag_ids = true_tag_ids[mask]\n        predicted_tag_ids = predictions[mask]\n\n        all_true_tag_ids.append(true_tag_ids)\n        all_predicted_tag_ids.append(predicted_tag_ids)\n\n    all_true_tag_ids = np.concatenate(all_true_tag_ids)\n    all_predicted_tag_ids = np.concatenate(all_predicted_tag_ids)\n    cls_report = classification_report(all_true_tag_ids, all_predicted_tag_ids)\n    print(\"Classifiction report:\")\n    print(cls_report)\n    f1 =  f1_score(all_true_tag_ids, all_predicted_tag_ids, average=\"macro\")\n    print(\"F1 score:\", f1)","39d6d9cc":"def create_prediction_csv_file(model, dataset, origin_ids, file_path):\n    predictions = []\n    classes = []\n    ids = []\n    t = 0\n    for item in dataset:\n        if len(item) == 2:\n            X = item[0]\n        else:\n            X = item\n        y_pred =  np.argmax(model.predict(X), axis=-1)\n        for i in range(y_pred.shape[0]):\n            last_prediction = None\n            indices = []\n            identifier = origin_ids[t]\n            t += 1\n            for j in range(X.shape[1]):\n                if last_prediction != index_tag[y_pred[i, j]]:\n                    if len(indices) > 0:\n                        ids.append(identifier)\n                        predictions.append(indices)\n                        classes.append(last_prediction)\n                        indices = []\n                    last_prediction = index_tag[y_pred[i, j]]\n                if y_pred[i, j] != pad_index and y_pred[i, j] != none_index:\n                    indices.append(j)\n                if j == X.shape[1] - 1:\n                    if len(indices) > 0:\n                        ids.append(identifier)\n                        predictions.append(indices)\n                        classes.append(last_prediction)\n                        indices = []\n                if X[i, j] == pad_index:\n                    break\n    new_ids = []\n    new_classes = []\n    new_preditions = []\n    for i in range(len(ids)):\n        merge = False\n        if ids[i - 1] == ids[i] and i > 0:\n            if len(predictions[i]) <= 3:\n                merge = True\n                j = new_preditions[-1][-1] + 1\n                while j < predictions[i][0]:\n                    new_preditions[-1].append(j)\n                    j += 1\n                new_preditions[-1] = new_preditions[-1] + predictions[i]\n            elif abs(predictions[i][0] - new_preditions[-1][-1]) <= 3 and classes[i] == new_classes[-1]:\n                merge = True\n                j = new_preditions[-1][-1] + 1\n                while j < predictions[i][0]:\n                    new_preditions[-1].append(j)\n                    j += 1\n                new_preditions[-1] = new_preditions[-1] + predictions[i]\n        if not merge:\n            new_ids.append(ids[i])\n            new_classes.append(classes[i])\n            new_preditions.append(predictions[i])\n    df = pd.DataFrame({\"id\": new_ids, \"class\": new_classes, \"predictionstring\": [\" \".join([str(element) for element in item]) for item in new_preditions]})\n    df.to_csv(file_path, index=False)","e2d83c5c":"evaluate(model, val_ds)","8476d72d":"%%time\ntoken_list = []\nfor i in range(len(submission)):\n    item = submission.iloc[i]\n    identifier = item[\"id\"] \n    with open(item[\"file_path\"]) as f:\n        content = \"\".join(f.readlines())\n        tokens = tokenize(content)\n        token_list.append(tokens)","c5271f93":"def preprocess_test(record):\n    record = tf.strings.split(record, sep=\"\\t\")\n    length = tf.strings.to_number(record[0], out_type=tf.int32)\n    tokens = record[1 : length + 1]\n    tokens = tf.strings.to_number(tokens, out_type=tf.int64)\n    return tokens\ndef make_test_dataset(Xs, file_path, batch_size):\n    with open(file_path, \"w+\") as f:\n        for i in range(len(Xs)):\n            X = Xs[i]\n            f.write(\n                str(len(X))\n                + \"\\t\"\n                + \"\\t\".join([str(item) for item in X])\n                + \"\\n\"\n            )\n    ds = tf.data.TextLineDataset(file_path).map(preprocess_test)\n    ds = ds.padded_batch(batch_size).cache().prefetch(tf.data.AUTOTUNE)\n    return ds","500540f4":"%%time\nX_test = vectorizer.transform(token_list)\ntest_ds = make_test_dataset(X_test, \"test.txt\", batch_size)\ncreate_prediction_csv_file(model, test_ds, list(submission[\"id\"]), \"submission.csv\")","fa3693cd":"## Distribution of Labels","037f2e79":"### Tokenization","067d3f44":"### Ditrubtion of Sentences Lengths","1d481f80":"## Common Parameters","fb6af120":"## Utilities","612fb6f6":"### Add File Path to train and submission Files","d99a772a":"## EDA & Preprocessing","31378e2a":"# Name Entity Recognition with Transformer\nIn this notebook, I will build a Name Entity Recognition Model using Transformer to evaluate student writing using dataset for Kaggle Competition [Feedback Prize - Evaluating Student Writing](https:\/\/www.kaggle.com\/c\/feedback-prize-2021). \nI also find a way to train the Model using data with different sequence length by converting this dataset to [TextLineDataset](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/data\/TextLineDataset) and convert with [padded_batch](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/data\/Dataset#padded_batch) function.\n## Import Packages","a484d381":"### Number of Unique files","0560dbe9":"## Import Datasets","7f79eb5a":"## Training","f398de9e":"### Vectorization","76935f1a":"## Create Tensorflow Dataset","7d48de4a":"#### Words appearing only once","7219ccb7":"### Distribution of Word Counts","13b29532":"### Evaluation","3fe86902":"#### Number of words","e62afa19":"## Export to files","1668c4b4":"## Submission","482af867":"## Modeling"}}