{"cell_type":{"967f3660":"code","dd127a2b":"code","d3ab8d9e":"code","47be792a":"code","04a2e186":"code","34f4afd3":"code","89c5fc77":"code","4e27ffea":"code","a054f577":"code","df5d7042":"code","6534e2d5":"code","0d788597":"code","c31a2b5c":"code","f15c31cf":"code","67275019":"code","ff7e12ec":"code","6c728d85":"code","06bc5ddc":"code","aeee17f9":"markdown","1154b017":"markdown","1c52e72f":"markdown","7dcbed65":"markdown","b864363f":"markdown","2dcc2339":"markdown","14b82b0d":"markdown","e12e5b49":"markdown","f7ff1da8":"markdown","d912e887":"markdown","34a4c480":"markdown","d85dc901":"markdown","aae8a8f0":"markdown","e5c8347a":"markdown","1ab44342":"markdown","e23605ea":"markdown","294678c6":"markdown","4a5e6108":"markdown","e4883f5e":"markdown","cf16d140":"markdown","454b00f8":"markdown","4d763baf":"markdown","97bc74d6":"markdown","5f84eb5a":"markdown","fed01a66":"markdown","1ced4552":"markdown","1bf070e0":"markdown","9cd777c6":"markdown","98066550":"markdown","ec27e3a1":"markdown","447165bf":"markdown","2b64e289":"markdown","5bcca403":"markdown","20551e48":"markdown","dabc351f":"markdown"},"source":{"967f3660":"import numpy as np \nimport pandas as pd \nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport random","dd127a2b":"train = pd.read_csv(\"..\/input\/train.csv\")\nrandom.seed(22)\ntext = train.text[random.sample(range(1,50),5)]\n","d3ab8d9e":"train.head()","47be792a":"train.shape","04a2e186":"import spacy\nnlp = spacy.load('en_core_web_sm')\ntext1 = str(text)\ndoc = nlp(text1)","34f4afd3":"df = pd.DataFrame()\n\nfor i, token in enumerate(doc):\n    df.loc[i, 'text'] = token.text\n    df.loc[i, 'pos'] = token.pos_\n    df.loc[i, 'dep'] = token.dep_\n\n    ","89c5fc77":"df.head(15)","4e27ffea":"df1 = pd.DataFrame()\nfor i,token in enumerate(doc):\n    df1.loc[i,'text'] = token.text","a054f577":"print(df1.head(15))","df5d7042":"df3 = pd.DataFrame()\nfor i,token in enumerate(doc):\n    df3.loc[i,'text'] = token.text\n    df3.loc[i,'lemma_'] = token.lemma_\n    df3.loc[i,'pos_'] = token.pos_\n    df3.loc[i,'tag_'] = token.tag_\n    df3.loc[i,'dep_'] = token.dep_\n    df3.loc[i,'shape_'] = token.shape_\n    df3.loc[i,'is_alpha'] = token.is_alpha\n    df3.loc[i,'is_stop'] = token.is_stop","6534e2d5":"df3.head(15)","0d788597":"spacy.displacy.render(doc, style='ent',jupyter=True)","c31a2b5c":"spacy.displacy.render(doc, style='dep',jupyter=True,options = {'compact':60})","f15c31cf":"df2 = pd.DataFrame()\n\nfor i, ent in enumerate(doc):\n    df2.loc[i, 'text'] = ent.text\n    df2.loc[i, 'pos'] = ent.pos_\n    df2.loc[i, 'dep'] = ent.dep_","67275019":"df2.head(15)","ff7e12ec":"df3 = pd.DataFrame()\nfor i,token in enumerate(doc):\n    df3.loc[i,'text'] = token.text\n    df3.loc[i,'has_vector'] = token.has_vector\n    df3.loc[i,'vector_norm'] = token.vector_norm\n    df3.loc[i,'is_oov'] = token.is_oov","6c728d85":"df3.head(15)","06bc5ddc":"for word in doc:\n    lexeme = doc.vocab[word.text]\n    print(lexeme.text, lexeme.orth, lexeme.shape_, lexeme.prefix_, lexeme.suffix_,\n          lexeme.is_alpha, lexeme.is_digit, lexeme.is_title, lexeme.lang_)","aeee17f9":"TOKENIZATION** -** Assigning word types to tokens, like verb or noun.**\nDuring processing, spaCy first tokenizes the text.segments it into words, punctuation .applies rules specific to each language. ","1154b017":"![](http:\/\/)1. Install different models (en_core_web_en,sm,md,lg..etc) using **python -m spacy download modelname**\n2. Load it using **spacy.load(name)** Model to load\nen_core_web_md -- \tEnglish\t-- Vocabulary, syntax, entities, vectors","1c52e72f":"**Different types of Named Entities**\n**PERSON**\tPeople, including fictional.\n**NORP**\tNationalities or religious or political groups.\n**FAC**\tBuildings, airports, highways, bridges, etc.\n**ORG**\tCompanies, agencies, institutions, etc.\n**GPE**\tCountries, cities, states.\n**LOC**\tNon-GPE locations, mountain ranges, bodies of water.\n**PRODUCT**\tObjects, vehicles, foods, etc. (Not services.)\n**EVENT**\tNamed hurricanes, battles, wars, sports events, etc.\n**WORK_OF_ART**\tTitles of books, songs, etc.\n**LAW**\tNamed documents made into laws.\n**LANGUAGE**\tAny named language.\n**DATE**\tAbsolute or relative dates or periods.\n**TIME**\tTimes smaller than a day.\n**PERCENT**\tPercentage, including \"%\".\n**MONEY**\tMonetary values, including unit.\n**QUANTITY**\tMeasurements, as of weight or distance.\n**ORDINAL**\t\"first\", \"second\", etc.\n**CARDINAL**\tNumerals that do not fall under another type.","7dcbed65":"spaCy offers tokenization, sentence boundary detection, POS tagging, NER, syntactic parsing (and chunking, as a subset of this), integrated word vectors, and alignment into the original string.\n\nPlease check below link about the advantages of using  spaCy from the **Matthew Honnibal** (Author of SpaCy)\n\nhttps:\/\/www.quora.com\/What-are-the-advantages-of-Spacy-vs-NLTK","b864363f":"* **Tokenization** Segmenting text into words, punctuations marks etc.\n*** Part-of-speech (POS) Tagging** Assigning word types to tokens, like verb or noun.\n* ** Dependency Parsing Assigning** syntactic dependency labels, describing the relations between individual tokens, like subject or object.\n* **Lemmatization** Assigning the base forms of words. For example, the lemma of \"was\" is \"be\", and the lemma of \"rats\" is \"rat\".\n* **Sentence** **Boundary** **Detection** (SBD) Finding and segmenting individual sentences.\n* **Named** **Entity** **Recognition** (NER) Labelling named \"real-world\" objects, like persons, companies or locations.\n* Similarity Comparing words, text spans and documents and how similar they are to each other.\n* Text Classification Assigning categories or labels to a whole document, or parts of a document.\n* Rule-based Matching Finding sequences of tokens based on their texts and linguistic annotations, similar to regular expressions.\n* 1Training Updating and improving a statistical model's predictions.\n* Serialization Saving objects to files or byte strings.\n\nWill discuss each spacy features in detail.\n","2dcc2339":"<a id=\"3\"><\/a>\n## 3.**Importing Data and  spacy.load()**","14b82b0d":"Tokenizer exceptions strongly depend on the specifics of the individual language","e12e5b49":"- **Labelling named \"real-world\" objects, like persons, companies or locations.**\n\nA named entity is a \"real-world object\" that's assigned a name \u2013 for example, a person, a country, a product or a book title. spaCy can recognise various types of named entities in a document.","f7ff1da8":"<a id=\"4-1\"><\/a>\n### 4.1 **LINGUISTIC ANNOTATIONS**","d912e887":"**SPACY VS NLTK**???","34a4c480":"![](https:\/\/qph.fs.quoracdn.net\/main-qimg-0a2f519557dead772f74a02aa267f431)","d85dc901":"<a id=\"4-4\"><\/a>\n### 4.4**NAMED ENTITIES**","aae8a8f0":"<a id=\"2\"><\/a>\n## 2.**Loading Libraries**","e5c8347a":"![](http:\/\/spacy.io\/assets\/img\/pipeline.svg)","1ab44342":"When you call nlp on a text in **spaCy**, \n1.spaCy first tokenizes the text to produce a Doc object. \n2.The Doc is then processed in several different steps \u2013 this is also referred to as the processing pipeline. The pipeline used by the default models consists of a tagger, a parser and an entity recognizer.\nEach pipeline component returns the processed Doc, which is then passed on to the next component","e23605ea":"The dependency visualizer, dep, shows part-of-speech tags and syntactic dependencies.","294678c6":"**spaCy** is a free, open-source library for advanced Natural Language Processing (NLP) in Python. \n\nspaCy provides very fast and accurate syntactic analysis and also offers named entity recognition and ready access to word vectors.","4a5e6108":"Text is Split into individual words and annotated.\nConfess is a Verb and structure is a Noun\n\n**token.pos_** gives course grained part of speech and **token.dep_**Syntacic dependency relation.\n\nplease find the below table for the different other attributes.\n\n\n**Different Attributes:**\n\ndoc\tDoc\tThe parent document.\n\n\nvocab\tVocab\tThe vocab object of the parent Doc.\ndoc\tDoc\tThe parent document.\nhead\tToken\tThe syntactic parent, or \"governor\", of this token.\n\nleft_edge\tToken\tThe leftmost token of this token's syntactic descendants.\nright_edge\tToken\tThe rightmost token of this token's syntactic descendants.\n\ni\tint\tThe index of the token within the parent document.\nent_type\tint\tNamed entity type.\n\nlemma\tint\tBase form of the token, with no inflectional suffixes.\n\nnorm_\tunicode\tThe token's norm, i.e. a normalised form of the token text. Usually set in the language's tokenizer exceptions or norm exceptions.\n\n**pos_**\tunicode\tCoarse-grained part-of-speech.\n\n\n\n**tag_**\tunicode\tFine-grained part-of-speech.\n\ndep\tint\tSyntactic dependency relation.\n\nlang\tint\tLanguage of the parent document's vocabulary.\n\nprob\tfloat\tSmoothed log probability estimate of token's type.\nidx\tint\tThe character offset of the token within the parent document.\n\n**sentiment**\tfloat\tA scalar value indicating the positivity or negativity of the token.\n","e4883f5e":"<a id=\"4\"><\/a>\n## 4.**spaCy Features**","cf16d140":"<a id=\"4-2\"><\/a>\n### 4.2 **TOKENIZATION**","454b00f8":"<a id=\"4-3\"><\/a>\n### 4.3 PARTS OF SPEECH TAGS AND DEPENDENCIES**","4d763baf":"\nspaCy provides a variety of **linguistic annotations** to give you insights into a text's grammatical structure.\nThis includes the word types, like the parts of speech, and how the words are related to each other. ","97bc74d6":"<a id=\"3-2\"><\/a>\n### 3-2 Install Model and Load using spacy.load()\n","5f84eb5a":"**tokenizer**   Tokenizer                    \tDoc\tSegment text into tokens.\n\n**tagger**\t            Tagger \tDoc[i].tag\tAssign part-of-speech tags.\n\n**parser**\tDependencyParser \tDoc[i].head, Doc[i].dep, Doc.sents, Doc.noun_chunks\tAssign dependency labels\n\n\nner\t**EntityRecognizer** \tDoc.ents, Doc[i].ent_iob, Doc[i].ent_type\tDetect and label named entities.\n\n\ntextcat\t**TextCategorizer** \tDoc.cats\tAssign document labels.\n","fed01a66":"\n[2. Loading Libraries](#2)  \n[3. Importing Data](#3)  \n[4. spaCy's features ](#4)  \n&nbsp; &nbsp;&nbsp; &nbsp; [4.1. LINGUISTIC ANNOTATIONS](#4-1)  \n&nbsp; &nbsp;&nbsp; &nbsp; [4.2TOKENIZATION](#4-2)  \n&nbsp; &nbsp;&nbsp; &nbsp; [4.3PARTS OF SPEECH TAGS AND DEPENDENCIES](#4-3)  \n&nbsp; &nbsp;&nbsp; &nbsp; [4.4NAMED ENTITIES](#4-4)\n\n[5. PipeLines ](#5)  ","1ced4552":"**SPACY FEATURES**","1bf070e0":"The raw text is split on whitespace characters. Then, the tokenizer processes the text from left to right. On each substring, it performs two checks:\n** **Does the substring match a tokenizer exception rule? \n Can a prefix, suffix or infix be split off? For example punctuation like commas, periods, hyphens or quotes.****\n \n \n \n3      confess\n4         that\n5      neither\n6          the\n7    structure\n8           of\n9       langua\n10         ...\n11          \\n\n12          16\n\nplease check below example on how spacy does tokenization","9cd777c6":"Spacy helps us  know more about the text.\nWhat do the **words** mean in context? \nWho is doing what to whom? \nWhat **companies** and products are mentioned? \nWhich **texts** are **similar** to each other?\n","98066550":"<a id=\"5\"><\/a>\n## 5.**Pipelines**","ec27e3a1":" How did spaCy assigned confess as VERB(POS) and I as PRON ?\n \n  **morphology**  is the study of words, how they are formed, and their relationship to other words in the same language.It analyzes the structure of words and parts of words, such as stems, root words, prefixes, and suffixes.\n  \n  spaCy Workflow:\n  \n1. The tokenizer consults a mapping table TOKENIZER_EXCEPTIONS, which allows sequences of characters to be mapped to multiple tokens. \n2.  The part-of-speech tagger then assigns each token an extended **POS** tag.  part-of-speech (e.g. VERB) and if verb is past tense.\n3. Rule-based deterministic **lemmatizer** maps the surface form, to a lemma in light of the previously assigned extended part-of-speech and morphological information.\n\n\n\nspaCy uses the terms head and child to describe the words connected by a single arc in the dependency tree. The term **dep** describes the type of syntactic relation that connects the child to the head.","447165bf":"![](https:\/\/jcharistech.files.wordpress.com\/2018\/05\/nlpwithspacyjcharistech.png?w=840&h=400&crop=1)","2b64e289":"\n![image.png](attachment:image.png)","5bcca403":"** Assigning word types to tokens, like verb or noun.**\nAfter tokenization, spaCy can parse and tag a given Doc.spaCy  makes a **prediction** of which** tag or label **most likely applies in this context by using statistical techniques.\nA model consists of binary data and is produced by showing a system enough examples for it to make predictions that generalise across the language.\n\n[Linguistic annotations] are available as **Token attributes** . spaCy encodes all strings to hash values to reduce memory usage and improve efficiency.","20551e48":"**SPACY**","dabc351f":"**MORE TO COME .STAY TUNED**\n\nReferences: Spacy documentation - https:\/\/spacy.io\/usage\/examples#section-pipeline"}}