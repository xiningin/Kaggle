{"cell_type":{"b094c7f4":"code","67f34674":"code","72bdb3d2":"code","04aa7f89":"code","8949f4c9":"code","d9600a89":"code","edacf488":"code","c28bf296":"code","7793d0c7":"code","e4f195c2":"code","21a7371e":"code","3b18a808":"code","49d4e435":"code","c7a2fd1d":"code","5e2fd2b7":"code","2d4788e6":"code","7524f259":"code","83f3c43a":"code","c1be43b4":"code","da0f7d7f":"code","4e855081":"code","56fa147a":"code","1188ef5b":"code","52e24b89":"code","32745e89":"code","1fedea9a":"code","98f6ea39":"code","8a1399a3":"code","1d3b69ef":"code","016cab67":"markdown","bca4117e":"markdown","e39b94d6":"markdown","954b0b30":"markdown","257177c3":"markdown","25ba68aa":"markdown","aa086184":"markdown","e2dcfe2b":"markdown","09e10b12":"markdown","5e954503":"markdown"},"source":{"b094c7f4":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport os\nfrom copy import deepcopy\n\nprint(os.listdir(\"..\/input\"))\n\nfrom sklearn.linear_model import Ridge\nfrom sklearn.linear_model import Lasso\nfrom sklearn.metrics import mean_squared_error","67f34674":"train = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')\n\n# test doesn't have 'SalePrice'\n# Note that the 'Id' for train range 1-1460 and test range 1461-2919\nprint('Train Shape: {}, Train ID Range {} to {}'.format(train.shape,train.Id.min(), train.Id.max()))\nprint('Test Shape: {}, Test ID Range {} to {}'.format(test.shape,test.Id.min(), test.Id.max()))","72bdb3d2":"train.head(3)","04aa7f89":"all_data = train.append(test,sort = False)\nprint(all_data.shape)\nall_data.head(3)","8949f4c9":"all_data.dtypes.groupby(all_data.dtypes).count()","d9600a89":"numerics = all_data.dtypes[all_data.dtypes != object].index.tolist()\nprint(numerics)\nprint(len(numerics))","edacf488":"skew_calcs = all_data[numerics].skew().sort_values()\n# Note that the skew filter below removes Id from the skewed feature list - we wouldn't have wanted to log-transform Id\nskew_calcs = skew_calcs[skew_calcs > .75]\nprint(skew_calcs)\nskew_feats = skew_calcs.index\n# Also note that SalePrice will be log_transformed","c28bf296":"all_data[skew_feats].hist()\nnp.log1p(all_data[skew_feats]).hist()\nplt.show()\n#plt.tight_layout()","7793d0c7":"all_data[skew_feats] = np.log1p(all_data[skew_feats])","e4f195c2":"#creating list of objects and View the unique strings in columns with object datatype\nobjects = train.dtypes[train.dtypes == object].index.tolist()\nprint(objects)\ntrain[objects].apply(lambda x: set(x))","21a7371e":"# Create Dummy Variables\nall_data = pd.concat([all_data,pd.get_dummies(all_data[objects])],axis = 1)\nprint(all_data.shape)","3b18a808":"# Remove original features now that dummies have been created\nall_data = all_data.drop(objects,axis = 1)\nprint(all_data.shape)","49d4e435":"all_data = all_data.fillna(all_data.mean())\n# could also create dummy variable for nulls\nall_data.head(3)","c7a2fd1d":"train_data = all_data[:1460]\n# drop SalePrice from test_data, was created during .fillna(all_data.mean())\ntest_data = all_data[1460:].drop('SalePrice',axis = 1)","5e2fd2b7":"from sklearn.model_selection import train_test_split","2d4788e6":"X = train_data.drop(['SalePrice'],axis = 1)\nprint(X.shape)\ny = train_data['SalePrice']\nprint(y.shape)","7524f259":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=10)","83f3c43a":"def get_mse(model,alpha):\n    run_model = model(alpha = alpha)\n    run_model.fit(X_train,y_train)\n    return mean_squared_error(y_pred=run_model.predict(X_test),y_true=y_test)","c1be43b4":"alphas = [.01,.05,0.1,0.5,1,2,3,5,10,20,40]\nridge_mses = [get_mse(Ridge,x) for x in alphas]\nplt.plot(alphas,ridge_mses)","da0f7d7f":"lasso_mses = [get_mse(Lasso,x) for x in alphas]\nplt.plot(alphas,lasso_mses)","4e855081":"# alpha = 5 looks best\npd.DataFrame({'alpha':alphas, 'MSE':ridge_mses}).sort_values('MSE',ascending = True)","56fa147a":"ridge_model = Ridge(alpha = 5)\nridge_model.fit(X,y)\ncoefs = ridge_model.coef_","1188ef5b":"labels_and_weights = pd.DataFrame({'field':X.columns,'weight':coefs})","52e24b89":"top_weight = labels_and_weights['weight'].quantile(.97)\nbot_weight = labels_and_weights['weight'].quantile(.03)\nheavy_weights = labels_and_weights[(labels_and_weights['weight']>top_weight) | (labels_and_weights['weight']<bot_weight)].sort_values('weight')\nplt.barh(heavy_weights['field'],heavy_weights['weight'])","32745e89":"predictions = ridge_model.predict(test_data)","1fedea9a":"print(predictions.mean())\n# inverse log-transform\npreds = np.expm1(predictions)\nprint(preds.mean())","98f6ea39":"sub = pd.DataFrame()\nsub['Id'] = test_data['Id']\nsub['SalePrice'] = preds","8a1399a3":"sub.head()","1d3b69ef":"sub.to_csv('submission.csv',index=False)","016cab67":"## **Combining train and test for all preprocessing to be done on one dataframe**","bca4117e":"### **Data type counts**","e39b94d6":"### It looks like we can create dummy variables for all of the objects, since there aren't too many to handle...","954b0b30":"### **Create Train and Test**","257177c3":"### **Quick Visual of Distribution of skewed features before and after log transformation**","25ba68aa":"### Checking object data types","aa086184":"### **Checking out numeric data types**","e2dcfe2b":"## **Log Transforming Skewed features**","09e10b12":"## Some credit here due to https:\/\/www.kaggle.com\/apapiu\/regularized-linear-models\/notebook\n\n## **Import and quick view of the data**","5e954503":"## Looking for skewed data to log transform\n### Will log-transform those with skew > .75"}}