{"cell_type":{"54766855":"code","cb8ddae2":"code","fc80b317":"code","6faf3547":"code","70101f9f":"code","ee8de12e":"code","0d1adb1c":"code","47816295":"code","ba267dbc":"code","15a6c46b":"code","453dee4e":"code","630617fe":"code","4bc7c768":"code","c8d909cb":"code","e493e5c7":"code","babd9563":"code","66e7fb4b":"code","b5d7fd0d":"code","d6080a8b":"code","6f4c7d58":"code","d630e296":"code","e865a8c8":"code","58e50ba5":"code","824673da":"code","6434bba2":"code","5f99194d":"code","788099c9":"code","d3507a60":"code","3cb4f410":"code","7af9960f":"code","871fc17e":"code","620d50c3":"code","c1cdc95b":"code","0fd1d4ee":"code","8e2fa59a":"code","6c62f1e5":"code","e5011bc2":"code","246310cc":"code","0d21dd09":"code","39abbe7d":"code","19686644":"code","195a8838":"code","c39ad4a8":"code","92eb7a72":"code","493f82ab":"code","e7eed56c":"code","401631e6":"code","34b3ae97":"code","6227d51c":"code","86afb079":"code","49bbe380":"code","c6dbb4c7":"code","d3a1129f":"code","5fedbf1b":"code","ae61da57":"code","9c48bcd8":"code","56b4957c":"code","d9f7828d":"code","2c289fdd":"code","6ada069d":"code","7bebd5d1":"code","77d2fa81":"code","1a8112c9":"code","7ef98eff":"code","c923a8d7":"code","373269e4":"code","cee4d470":"code","9704f7af":"code","b755e402":"code","ecac7992":"code","0d31a19a":"code","9789cb7d":"code","ff95530c":"code","8e2caa8e":"code","ef5c35f2":"code","cca9da0c":"code","e3aef939":"code","ac274bb1":"code","488d18c7":"code","4069d6f2":"code","a01db21f":"code","e89321b3":"code","793f8b18":"code","579b3a46":"code","1e707dd4":"code","995dae62":"markdown","135cf9ff":"markdown","5d0005fc":"markdown","ee0992da":"markdown","7f9d6f3c":"markdown","0a726881":"markdown","450c07f9":"markdown","7be9a9fe":"markdown","ea796898":"markdown","4071a84d":"markdown","098e74f3":"markdown","9f3e526f":"markdown","c7b7f856":"markdown","df34c3fb":"markdown","1b061cd1":"markdown","4f467fc6":"markdown","ea23a650":"markdown","0e4708a8":"markdown","425cc4de":"markdown","fb187110":"markdown","806e7729":"markdown","1eac110d":"markdown","a09aa745":"markdown","45d9a127":"markdown","5b5d6ff6":"markdown","9026816b":"markdown","198acb6b":"markdown","25769938":"markdown","11758355":"markdown","5104b7d7":"markdown","67754fff":"markdown","27008073":"markdown","415f0c2a":"markdown","eb47b0bf":"markdown","3708d462":"markdown","cc8b286e":"markdown","9c127684":"markdown","bbb954fc":"markdown","45ec2957":"markdown","cafcbb80":"markdown","53e99241":"markdown","e27705c2":"markdown","8430af1a":"markdown","a0f9476e":"markdown","12634546":"markdown","81cfb31f":"markdown","723c9d7e":"markdown","4371d7f3":"markdown","5acb2465":"markdown","46b6fe9a":"markdown","669ea47a":"markdown","cd5a666a":"markdown","f742bde4":"markdown","0d935e2e":"markdown","152fce7f":"markdown","ede514e6":"markdown","75a6f395":"markdown","014ac59c":"markdown"},"source":{"54766855":"# Setting html stuff for the rest of the notebook\nfrom IPython.core.display import display, HTML, Javascript\nhtml_contents =\"\"\"\n<!DOCTYPE html>\n<html lang=\"en\">\n    <head>\n    <style>\n    \n    .top_section{\n        background-color: #00AF87;\n        color: white;\n        font-family: Copperplate, Papyrus, fantasy;\n        font-weight: 800;\n        font-size: 35px;\n        padding: 20px 14px;\n        margin-bottom: 20px;\n    }\n    \n \n    <\/style>\n    <\/head>\n    \n<\/html>\n\"\"\"\n\nHTML(html_contents)","cb8ddae2":"import pandas as pd\nfrom wordcloud import WordCloud\nimport seaborn as sns\nimport re\nimport string\nfrom collections import Counter, defaultdict\n\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n\nimport plotly.express as px\nfrom plotly.subplots import make_subplots\nimport plotly.graph_objects as go\nfrom plotly.offline import plot\n\nimport matplotlib.gridspec as gridspec\nfrom matplotlib.ticker import MaxNLocator\nimport matplotlib.patches as mpatches\nimport matplotlib.pyplot as plt\n\n","fc80b317":"import warnings\nwarnings.filterwarnings('ignore')","6faf3547":"import nltk\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\nstopWords_nltk = set(stopwords.words('english'))","70101f9f":"import re\nfrom typing import Union, List\n\nclass CleanText():\n    \"\"\" clearing text except digits () . , word character \"\"\" \n\n    def __init__(self, clean_pattern = r\"[^A-Z\u011e\u00dc\u015e\u0130\u00d6\u00c7Ia-z\u011f\u00fc\u0131'\u015f\u00f6\u00e70-9.\\\"',()]\"):\n        self.clean_pattern =clean_pattern\n\n    def __call__(self, text: Union[str, list]) -> List[List[str]]:\n\n        if isinstance(text, str):\n            docs = [[text]]\n\n        if isinstance(text, list):\n            docs = text\n\n        text = [[re.sub(self.clean_pattern, \" \", sent) for sent in sents] for sents in docs]\n\n        return text\n    \ndef remove_emoji(data):\n    emoj = re.compile(\"[\"\n        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n        u\"\\U00002500-\\U00002BEF\"  \n        u\"\\U00002702-\\U000027B0\"\n        u\"\\U00002702-\\U000027B0\"\n        u\"\\U000024C2-\\U0001F251\"\n        u\"\\U0001f926-\\U0001f937\"\n        u\"\\U00010000-\\U0010ffff\"\n        u\"\\u2640-\\u2642\" \n        u\"\\u2600-\\u2B55\"\n        u\"\\u200d\"\n        u\"\\u23cf\"\n        u\"\\u23e9\"\n        u\"\\u231a\"\n        u\"\\ufe0f\"  # dingbats\n        u\"\\u3030\"\n                      \"]+\", re.UNICODE)\n    return re.sub(emoj, '', data)\n\ndef tokenize(text):\n    \"\"\" basic tokenize method with word character, non word character and digits \"\"\"\n    text = re.sub(r\" +\", \" \", str(text))\n    text = re.split(r\"(\\d+|[a-zA-Z\u011f\u00fc\u015f\u0131\u00f6\u00e7\u011e\u00dc\u015e\u0130\u00d6\u00c7]+|\\W)\", text)\n    text = list(filter(lambda x: x != '' and x != ' ', text))\n    sent_tokenized = ' '.join(text)\n    return sent_tokenized\n\nregex = re.compile('[%s]' % re.escape(string.punctuation))\n\ndef remove_punct(text):\n    text = regex.sub(\" \", text)\n    return text\n\nclean = CleanText()\n","ee8de12e":"# label encode\ndef label_encode(x):\n    if x == 1 or x == 2:\n        return 0\n    if x == 3:\n        return 1\n    if x == 5 or x == 4:\n        return 2\n    \n# label to name\ndef label2name(x):\n    if x == 0:\n        return \"Negative\"\n    if x == 1:\n        return \"Neutral\"\n    if x == 2:\n        return \"Positive\"\n    ","0d1adb1c":"df = pd.read_csv(\"..\/input\/trip-advisor-hotel-reviews\/tripadvisor_hotel_reviews.csv\")","47816295":"# show column names\nprint(\"df.columns: \", df.columns) ","ba267dbc":"# head of df\ndf.head()","15a6c46b":"# count of ratings\nfig = px.histogram(df,\n             x = 'Rating',\n             title = 'Histogram of Review Rating',\n             template = 'ggplot2',\n             color = 'Rating',\n             color_discrete_sequence= px.colors.sequential.Blues_r,\n             opacity = 0.8,\n             height = 525,\n             width = 835,\n            )\n\nfig.update_yaxes(title='Count')\nfig.show()","453dee4e":"# basic info \ndf.info()","630617fe":"# encode label and mapping label name\ndf[\"label\"] = df[\"Rating\"].apply(lambda x: label_encode(x))\ndf[\"label_name\"] = df[\"label\"].apply(lambda x: label2name(x))","4bc7c768":"# clean text, lowercase and remove punk\ndf[\"Review\"] = df[\"Review\"].apply(lambda x: remove_punct(clean(remove_emoji(x).lower())[0][0]))","c8d909cb":"df.head()","e493e5c7":"def show_wordcloud(data, title = None):\n    wordcloud = WordCloud(\n        background_color='black',\n        max_words=200,\n        max_font_size=40, \n        scale=1,\n        random_state=1\n).generate(\" \".join(data))\n\n    fig = plt.figure(1, figsize=(15, 15))\n    plt.axis('off')\n    if title: \n        fig.suptitle(title, fontsize=20)\n        fig.subplots_adjust(top=2.3)\n\n    plt.imshow(wordcloud)\n    plt.show()","babd9563":"show_wordcloud(df[\"Review\"].values)","66e7fb4b":"fig = make_subplots(rows=1, cols=2, specs=[[{\"type\": \"pie\"}, {\"type\": \"bar\"}]])\ncolors = ['gold', 'mediumturquoise', 'lightgreen'] # darkorange\nfig.add_trace(go.Pie(labels=df.label_name.value_counts().index,\n                             values=df.label.value_counts().values), 1, 1)\n\nfig.update_traces(hoverinfo='label+percent', textfont_size=20,\n                  marker=dict(colors=colors, line=dict(color='#000000', width=2)))\n\nfig.add_trace(go.Bar(x=df.label_name.value_counts().index, y=df.label.value_counts().values, marker_color = colors), 1,2)\n\nfig.show()\n","b5d7fd0d":"# tokenize data\ndf[\"tokenized_review\"] = df.Review.apply(lambda x: tokenize(x))\n# calculate token count for any sent\ndf[\"sent_token_length\"] = df[\"tokenized_review\"].apply(lambda x: len(x.split()))","d6080a8b":"fig = px.histogram(df, x=\"sent_token_length\", nbins=20, color_discrete_sequence=px.colors.cmocean.algae, barmode='group', histnorm=\"percent\")\nfig.show()","6f4c7d58":"(df.sent_token_length < 512).mean()","d630e296":"from transformers import BertTokenizer\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased', \n                                          do_lower_case=True)","e865a8c8":"# data tokenize with bert tokenizer\ndf[\"sent_bert_token_length\"] = df[\"Review\"].apply(lambda x: len(tokenizer(x, add_special_tokens=False)[\"input_ids\"]))","58e50ba5":"fig = px.histogram(df, x=\"sent_token_length\", nbins=20, color_discrete_sequence=px.colors.cmocean.algae, barmode='group', histnorm=\"percent\")\nfig.show()","824673da":"# Less than 512 covers how many of the data\n(df.sent_bert_token_length < 512).mean()\n","6434bba2":"# valvulate char count for each review\ndf['char_count'] = df['Review'].apply(lambda x: len(str(x)))\n\n\ndef plot_dist3(df, feature, title):\n    fig = plt.figure(constrained_layout=True, figsize=(18, 8))\n    grid = gridspec.GridSpec(ncols=3, nrows=3, figure=fig)\n    ax1 = fig.add_subplot(grid[0, :2])\n    ax1.set_title('Histogram')\n    sns.distplot(df.loc[:, feature],\n                 hist=True,\n                 kde=True,\n                 ax=ax1,\n                 )\n    ax1.set(ylabel='Frequency')\n    ax1.xaxis.set_major_locator(MaxNLocator(nbins=20))\n\n\n    plt.suptitle(f'{title}', fontsize=24)","5f99194d":"plot_dist3(df, 'char_count',\n           'Characters Count in Data')","788099c9":"# Creating a new feature for the visualization.\ndf['Character Count'] = df['Review'].apply(lambda x: len(str(x)))\n\n\ndef plot_dist3(df, feature, title):\n    # Creating a customized chart. and giving in figsize and everything.\n    fig = plt.figure(constrained_layout=True, figsize=(24, 12))\n    # Creating a grid of 3 cols and 3 rows.\n    grid = gridspec.GridSpec(ncols=3, nrows=3, figure=fig)\n\n    # Customizing the histogram grid.\n    ax1 = fig.add_subplot(grid[0, :2])\n    # Set the title.\n    ax1.set_title('Histogram')\n    # plot the histogram.\n    sns.distplot(df.loc[:, feature],\n                 hist=True,\n                 kde=True,\n                 ax=ax1,\n                 color='#e74c3c')\n    ax1.set(ylabel='Frequency')\n    ax1.xaxis.set_major_locator(MaxNLocator(nbins=20))\n\n    # Customizing the ecdf_plot.\n    ax2 = fig.add_subplot(grid[1, :2])\n    # Set the title.\n    ax2.set_title('Empirical CDF')\n    # Plotting the ecdf_Plot.\n    sns.distplot(df.loc[:, feature],\n                 ax=ax2,\n                 kde_kws={'cumulative': True},\n                 hist_kws={'cumulative': True},\n                 color='#e74c3c')\n    ax2.xaxis.set_major_locator(MaxNLocator(nbins=20))\n    ax2.set(ylabel='Cumulative Probability')\n\n    plt.suptitle(f'{title}', fontsize=24)","d3507a60":"plot_dist3(df[df['label'] == 0], 'Character Count',\n           'Characters Count \"Negative\" Rewiev')","3cb4f410":"plot_dist3(df[df['label'] == 2], 'Character Count',\n           'Characters Per \"Positive\" Rewiev')","7af9960f":"plot_dist3(df[df['label'] == 1], 'Character Count',\n           'Characters Per \"Neutral\" Rewiev')","871fc17e":"def plot_word_number_histogram(textno, textye, textz):\n    \n    \"\"\"A function for comparing word counts\"\"\"\n\n    fig, axes = plt.subplots(ncols=1, nrows=3, figsize=(18, 12), sharey=True)\n    sns.distplot(textno.str.split().map(lambda x: len(x)), ax=axes[0], color='#e74c3c')\n    sns.distplot(textye.str.split().map(lambda x: len(x)), ax=axes[1], color='#e74c3c')\n    sns.distplot(textz.str.split().map(lambda x: len(x)), ax=axes[2], color='#e74c3c')\n\n    \n    axes[0].set_xlabel('Word Count')\n    axes[0].set_ylabel('Frequency')\n    axes[0].set_title('negative')\n    axes[1].set_xlabel('Word Count')\n    axes[1].set_title('netrual')\n    axes[2].set_xlabel('Word Count')\n    axes[2].set_title('pozitive')\n    \n    fig.suptitle('Words Per Review', fontsize=24, va='baseline')\n    \n    fig.tight_layout()","620d50c3":"plot_word_number_histogram(df[df['label'] == 0]['Review'],\n                           df[df['label'] == 1]['Review'],\n                           df[df['label'] == 2]['Review'],\n                          )","c1cdc95b":"# remove punk \ndf['tokenized_review'] = df['tokenized_review'].apply(lambda x: remove_punct(x))\n","0fd1d4ee":"texts = df['tokenized_review']\nnew = texts.str.split()\nnew = new.values.tolist()\ncorpus = [word for i in new for word in i]\ncounter = Counter(corpus)\nmost = counter.most_common()\nx, y = [], []\nfor word, count in most[:30]:\n    if word not in stopWords_nltk:\n        x.append(word)\n        y.append(count)\n\nfig = go.Figure(go.Bar(\n            x=y,\n            y=x,\n            orientation='h',  marker=dict(\n        color='rgba(50, 171, 96, 0.6)',\n        line=dict(\n            color='rgba(50, 171, 96, 1.0)',\n            width=1),\n    ),\n    name='Most common Word',))\n\nfig.update_layout( title={\n        'text': \"Most Common Words\",\n        'y':0.9,\n        'x':0.5,\n        'xanchor': 'center',\n        'yanchor': 'top'}, font=dict(\n        family=\"Courier New, monospace\",\n        size=18,\n        color=\"RebeccaPurple\"\n    ))\n\nfig.show()","8e2fa59a":"fig = make_subplots(rows=1, cols=3)\ntitle_ = [\"negative\", \"neutral\", \"positive\"]\n\nfor i in range(3):\n    texts = df[df[\"label\"] == i]['tokenized_review']\n\n    new = texts.str.split()\n    new = new.values.tolist()\n    corpus = [word for i in new for word in i]\n    counter = Counter(corpus)\n    most = counter.most_common()\n    x, y = [], []\n\n    for word, count in most[:30]:\n        if word not in stopWords_nltk:\n            x.append(word)\n            y.append(count)\n\n    fig.add_trace(go.Bar(\n                x=y,\n                y=x,\n                orientation='h', type=\"bar\",\n        name=title_[i], marker=dict(color=colors[i])), 1, i+1)\n    \nfig.update_layout(\n    autosize=False,\n    width=2000,\n    height=600,title=dict(\n        text='<b>Most Common ngrams per Classes<\/b>',\n        x=0.5,\n        y=0.95,\n        font=dict(\n        family=\"Courier New, monospace\",\n        size=24,\n        color=\"RebeccaPurple\"\n        )\n    ),)\n\n\nfig.show()","6c62f1e5":"def _get_top_ngram(corpus, n=None):\n    #getting top ngrams\n    vec = CountVectorizer(ngram_range=(n, n),\n                          max_df=0.9,\n                          ).fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0)\n    words_freq = [(word, sum_words[0, idx])\n                  for word, idx in vec.vocabulary_.items()]\n    words_freq = sorted(words_freq, key=lambda x: x[1], reverse=True)\n    return words_freq[:15]","e5011bc2":"# unigram\nfig = make_subplots(rows=1, cols=3)\n\ntitle_ = [\"negative\", \"neutral\", \"positive\"]\n\nfor i in range(3):\n    texts = df[df[\"label\"] == i]['tokenized_review']\n\n    new = texts.str.split()\n    new = new.values.tolist()\n    corpus = [word for i in new for word in i]\n    top_n_bigrams = _get_top_ngram(texts, 2)[:15]\n    x, y = map(list, zip(*top_n_bigrams))\n\n\n    fig.add_trace(go.Bar(\n                x=y,\n                y=x,\n                orientation='h', type=\"bar\",\n        name=title_[i], marker=dict(color=colors[i])), 1, i+1)\n    \n\nfig.update_layout(\n    autosize=False,\n    width=2000,\n    height=600,title=dict(\n        text='<b>Most Common unigrams per Classes<\/b>',\n        x=0.5,\n        y=0.95,\n        font=dict(\n        family=\"Courier New, monospace\",\n        size=24,\n        color=\"RebeccaPurple\"\n        )\n    ))      \nfig.show()","246310cc":"#trigram\n\nfig = make_subplots(rows=1, cols=3)\ntitle_ = [\"negative\", \"neutral\", \"positive\"]\n\nfor i in range(3):\n    texts = df[df[\"label\"] == i]['tokenized_review']\n\n    new = texts.str.split()\n    new = new.values.tolist()\n    corpus = [word for i in new for word in i]\n\n    top_n_bigrams = _get_top_ngram(texts, 3)[:15]\n    x, y = map(list, zip(*top_n_bigrams))\n\n    fig.add_trace(go.Bar(\n                x=y,\n                y=x,\n                orientation='h', type=\"bar\",\n        name=title_[i], marker=dict(color=colors[i])), 1, i+1),\n\nfig.update_layout(\n    autosize=False,\n    width=2000,\n    height=600,title=dict(\n        text='<b>Most Common trigrams per Classes<\/b>',\n        x=0.5,\n        y=0.95,\n        font=dict(\n        family=\"Courier New, monospace\",\n        size=24,\n        color=\"RebeccaPurple\"\n        )\n    ))\n    \nfig.show()","0d21dd09":"import pandas as pd\nimport numpy as np\nimport os\nimport random\nfrom pathlib import Path\nimport json","39abbe7d":"import torch\nfrom tqdm.notebook import tqdm\n\nfrom transformers import BertTokenizer\nfrom torch.utils.data import TensorDataset\n\nfrom transformers import BertForSequenceClassification","19686644":"class Config():\n    seed_val = 17\n    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n    epochs = 5 \n    batch_size = 6\n    seq_length = 512\n    lr = 2e-5\n    eps = 1e-8\n    pretrained_model = 'bert-base-uncased'\n    test_size=0.15\n    random_state=42\n    add_special_tokens=True \n    return_attention_mask=True \n    pad_to_max_length=True \n    do_lower_case=False\n    return_tensors='pt'\n\nconfig = Config()","195a8838":"# params will be saved after training\nparams = {\"seed_val\": config.seed_val,\n    \"device\":str(config.device),\n    \"epochs\":config.epochs, \n    \"batch_size\":config.batch_size,\n    \"seq_length\":config.seq_length,\n    \"lr\":config.lr,\n    \"eps\":config.eps,\n    \"pretrained_model\": config.pretrained_model,\n    \"test_size\":config.test_size,\n    \"random_state\":config.random_state,\n    \"add_special_tokens\":config.add_special_tokens,\n    \"return_attention_mask\":config.return_attention_mask,\n    \"pad_to_max_length\":config.pad_to_max_length,\n    \"do_lower_case\":config.do_lower_case,\n    \"return_tensors\":config.return_tensors,\n         }","c39ad4a8":"# set random seed and device\nimport random\n\ndevice = config.device\n\nrandom.seed(config.seed_val)\nnp.random.seed(config.seed_val)\ntorch.manual_seed(config.seed_val)\ntorch.cuda.manual_seed_all(config.seed_val)","92eb7a72":"df.head()","493f82ab":"#split train test\nfrom sklearn.model_selection import train_test_split\n\ntrain_df_, val_df = train_test_split(df, \n                                    test_size=0.10, \n                                    random_state=config.random_state, \n                            stratify=df.label.values)","e7eed56c":"train_df_.head()","401631e6":"train_df, test_df = train_test_split(train_df_, \n                                    test_size=0.10, \n                                    random_state=42, \n                            stratify=train_df_.label.values)","34b3ae97":"# count of unique label  control \nprint(len(train_df['label'].unique()))\nprint(train_df.shape)","6227d51c":"# count of unique label  control \nprint(len(val_df['label'].unique()))\nprint(val_df.shape)","86afb079":"print(len(test_df['label'].unique()))\nprint(test_df.shape)","49bbe380":"# create tokenizer\ntokenizer = BertTokenizer.from_pretrained(config.pretrained_model, \n                                          do_lower_case=config.do_lower_case)","c6dbb4c7":"encoded_data_train = tokenizer.batch_encode_plus(\n    train_df.Review.values, \n    add_special_tokens=config.add_special_tokens, \n    return_attention_mask=config.return_attention_mask, \n    pad_to_max_length=config.pad_to_max_length, \n    max_length=config.seq_length, \n    return_tensors=config.return_tensors\n)\nencoded_data_val = tokenizer.batch_encode_plus(\n    val_df.Review.values, \n    add_special_tokens=config.add_special_tokens, \n    return_attention_mask=config.return_attention_mask, \n    pad_to_max_length=config.pad_to_max_length,\n    max_length=config.seq_length, \n    return_tensors=config.return_tensors\n)","d3a1129f":"input_ids_train = encoded_data_train['input_ids']\nattention_masks_train = encoded_data_train['attention_mask']\nlabels_train = torch.tensor(train_df.label.values)\n\ninput_ids_val = encoded_data_val['input_ids']\nattention_masks_val = encoded_data_val['attention_mask']\nlabels_val = torch.tensor(val_df.label.values)","5fedbf1b":"dataset_train = TensorDataset(input_ids_train, attention_masks_train, labels_train)\ndataset_val = TensorDataset(input_ids_val, attention_masks_val, labels_val)","ae61da57":"model = BertForSequenceClassification.from_pretrained(config.pretrained_model,\n                                                      num_labels=3,\n                                                      output_attentions=False,\n                                                      output_hidden_states=False)","9c48bcd8":"from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n\ndataloader_train = DataLoader(dataset_train, \n                              sampler=RandomSampler(dataset_train), \n                              batch_size=config.batch_size)\n\ndataloader_validation = DataLoader(dataset_val, \n                                   sampler=SequentialSampler(dataset_val), \n                                   batch_size=config.batch_size)","56b4957c":"from transformers import AdamW, get_linear_schedule_with_warmup\n\noptimizer = AdamW(model.parameters(),\n                  lr=config.lr, \n                  eps=config.eps)\n                  \n\nscheduler = get_linear_schedule_with_warmup(optimizer, \n                                            num_warmup_steps=0,\n                                            num_training_steps=len(dataloader_train)*config.epochs)","d9f7828d":"from sklearn.metrics import f1_score\n\ndef f1_score_func(preds, labels):\n    preds_flat = np.argmax(preds, axis=1).flatten()\n    labels_flat = labels.flatten()\n    return f1_score(labels_flat, preds_flat, average='weighted')\n\ndef accuracy_per_class(preds, labels, label_dict):\n    label_dict_inverse = {v: k for k, v in label_dict.items()}\n    \n    preds_flat = np.argmax(preds, axis=1).flatten()\n    labels_flat = labels.flatten()\n\n    for label in np.unique(labels_flat):\n        y_preds = preds_flat[labels_flat==label]\n        y_true = labels_flat[labels_flat==label]\n        print(f'Class: {label_dict_inverse[label]}')\n        print(f'Accuracy: {len(y_preds[y_preds==label])}\/{len(y_true)}\\n')","2c289fdd":"def evaluate(dataloader_val):\n\n    model.eval()\n    \n    loss_val_total = 0\n    predictions, true_vals = [], []\n    \n    for batch in dataloader_val:\n        \n        batch = tuple(b.to(config.device) for b in batch)\n        \n        inputs = {'input_ids':      batch[0],\n                  'attention_mask': batch[1],\n                  'labels':         batch[2],\n                 }\n\n        with torch.no_grad():        \n            outputs = model(**inputs)\n            \n        loss = outputs[0]\n        logits = outputs[1]\n        loss_val_total += loss.item()\n\n        logits = logits.detach().cpu().numpy()\n        label_ids = inputs['labels'].cpu().numpy()\n        predictions.append(logits)\n        true_vals.append(label_ids)\n        \n    # calculate avareage val loss\n    loss_val_avg = loss_val_total\/len(dataloader_val) \n    \n    predictions = np.concatenate(predictions, axis=0)\n    true_vals = np.concatenate(true_vals, axis=0)\n            \n    return loss_val_avg, predictions, true_vals","6ada069d":"config.device","7bebd5d1":"model.to(config.device)\n    \nfor epoch in tqdm(range(1, config.epochs+1)):\n    \n    model.train()\n    \n    loss_train_total = 0\n    # allows you to see the progress of the training \n    progress_bar = tqdm(dataloader_train, desc='Epoch {:1d}'.format(epoch), leave=False, disable=False)\n    \n    for batch in progress_bar:\n\n        model.zero_grad()\n        \n        batch = tuple(b.to(config.device) for b in batch)\n        \n        \n        inputs = {'input_ids':      batch[0],\n                  'attention_mask': batch[1],\n                  'labels':         batch[2],\n                 }       \n\n        outputs = model(**inputs)\n        \n        loss = outputs[0]\n        loss_train_total += loss.item()\n        loss.backward()\n\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n\n        optimizer.step()\n        scheduler.step()\n        \n        progress_bar.set_postfix({'training_loss': '{:.3f}'.format(loss.item()\/len(batch))})\n         \n        \n    torch.save(model.state_dict(), f'_BERT_epoch_{epoch}.model')\n        \n    tqdm.write(f'\\nEpoch {epoch}')\n    \n    loss_train_avg = loss_train_total\/len(dataloader_train)            \n    tqdm.write(f'Training loss: {loss_train_avg}')\n    \n    val_loss, predictions, true_vals = evaluate(dataloader_validation)\n    val_f1 = f1_score_func(predictions, true_vals)\n    tqdm.write(f'Validation loss: {val_loss}')\n    \n    tqdm.write(f'F1 Score (Weighted): {val_f1}');\n# save model params and other configs \nwith Path('params.json').open(\"w\") as f:\n    json.dump(params, f, ensure_ascii=False, indent=4)","77d2fa81":"model.load_state_dict(torch.load(f'.\/_BERT_epoch_3.model', map_location=torch.device('cpu')))","1a8112c9":"from sklearn.metrics import classification_report\n\npreds_flat = np.argmax(predictions, axis=1).flatten()\nprint(classification_report(preds_flat, true_vals))","7ef98eff":"# step by step predictions on dataframe\n# We do this to view predictions in the pandas dataframe and easily filter them and perform error analysis.\n\npred_final = []\n\nfor i, row in tqdm(val_df.iterrows(), total=val_df.shape[0]):\n    predictions = []\n\n    review = row[\"Review\"]\n    encoded_data_test_single = tokenizer.batch_encode_plus(\n    [review], \n    add_special_tokens=config.add_special_tokens, \n    return_attention_mask=config.return_attention_mask, \n    pad_to_max_length=config.pad_to_max_length, \n    max_length=config.seq_length,\n    return_tensors=config.return_tensors\n    )\n    input_ids_test = encoded_data_test_single['input_ids']\n    attention_masks_test = encoded_data_test_single['attention_mask']\n\n    \n    inputs = {'input_ids':      input_ids_test.to(device),\n              'attention_mask':attention_masks_test.to(device),\n             }\n\n    with torch.no_grad():        \n        outputs = model(**inputs)\n    \n    logits = outputs[0]\n    logits = logits.detach().cpu().numpy()\n    predictions.append(logits)\n    predictions = np.concatenate(predictions, axis=0)\n    pred_final.append(np.argmax(predictions, axis=1).flatten()[0])","c923a8d7":"# add pred into val_df\nval_df[\"pred\"] = pred_final","373269e4":"#  Add control column for easier wrong and right predictions\ncontrol = val_df.pred.values == val_df.label.values\nval_df[\"control\"] = control","cee4d470":"# filtering false predictions\nval_df = val_df[val_df.control == False]","9704f7af":"# buralar\u0131 d\u00fczenle bbaaaabbaaaaa\n# label to intent mapping\nname2label = {\"Negative\":0,\n              \"Neutral\":1,\n             \"Positive\":2\n             }\nlabel2name = {v: k for k, v in name2label.items()}\n\nval_df[\"pred_name\"] = val_df.pred.apply(lambda x: label2name.get(x)) ","b755e402":"from sklearn.metrics import confusion_matrix\n\n# We create a confusion matrix to better observe the classes that the model confuses.\npred_name_values = val_df.pred_name.values\nlabel_values = val_df.label_name.values\nconfmat = confusion_matrix(label_values, pred_name_values, labels=list(name2label.keys()))","ecac7992":"confmat","0d31a19a":"df_confusion_val = pd.crosstab(label_values, pred_name_values)\ndf_confusion_val","9789cb7d":"# save confissuan matrix df\ndf_confusion_val.to_csv(\"val_df_confusion.csv\")","ff95530c":"test_df.head()","8e2caa8e":"encoded_data_test = tokenizer.batch_encode_plus(\n    test_df.Review.values, \n    add_special_tokens=config.add_special_tokens, \n    return_attention_mask=config.return_attention_mask, \n    pad_to_max_length=config.pad_to_max_length,\n    max_length=config.seq_length, \n    return_tensors=config.return_tensors\n)","ef5c35f2":"input_ids_test = encoded_data_test['input_ids']\nattention_masks_test = encoded_data_test['attention_mask']\nlabels_test = torch.tensor(test_df.label.values)","cca9da0c":"model = BertForSequenceClassification.from_pretrained(config.pretrained_model,\n                                                      num_labels=3,\n                                                      output_attentions=False,\n                                                      output_hidden_states=False)\n\nmodel.to(config.device)\n\nmodel.load_state_dict(torch.load(f'.\/_BERT_epoch_3.model', map_location=torch.device('cpu')))\n\n_, predictions_test, true_vals_test = evaluate(dataloader_validation)\n# accuracy_per_class(predictions, true_vals, intent2label)","e3aef939":"from sklearn.metrics import classification_report\n\npreds_flat_test = np.argmax(predictions_test, axis=1).flatten()\nprint(classification_report(preds_flat_test, true_vals_test))","ac274bb1":"pred_final = []\n\nfor i, row in tqdm(test_df.iterrows(), total=test_df.shape[0]):\n    predictions = []\n\n    review = row[\"Review\"]\n    encoded_data_test_single = tokenizer.batch_encode_plus(\n    [review], \n    add_special_tokens=config.add_special_tokens, \n    return_attention_mask=config.return_attention_mask, \n    pad_to_max_length=config.pad_to_max_length, \n    max_length=config.seq_length,\n    return_tensors=config.return_tensors\n    )\n    input_ids_test = encoded_data_test_single['input_ids']\n    attention_masks_test = encoded_data_test_single['attention_mask']\n\n    \n    inputs = {'input_ids':      input_ids_test.to(device),\n              'attention_mask':attention_masks_test.to(device),\n             }\n\n    with torch.no_grad():        \n        outputs = model(**inputs)\n    \n    logits = outputs[0]\n    logits = logits.detach().cpu().numpy()\n    predictions.append(logits)\n    predictions = np.concatenate(predictions, axis=0)\n    pred_final.append(np.argmax(predictions, axis=1).flatten()[0])","488d18c7":"# add pred into test\ntest_df[\"pred\"] = pred_final","4069d6f2":"#  Add control column for easier wrong and right predictions\ncontrol = test_df.pred.values == test_df.label.values\ntest_df[\"control\"] = control","a01db21f":"# filtering false predictions\ntest_df = test_df[test_df.control == False]","e89321b3":"test_df[\"pred_name\"] = test_df.pred.apply(lambda x: label2name.get(x)) ","793f8b18":"from sklearn.metrics import confusion_matrix\n\n# We create a confusion matrix to better observe the classes that the model confuses.\npred_name_values = test_df.pred_name.values\nlabel_values = test_df.label_name.values\nconfmat = confusion_matrix(label_values, pred_name_values, labels=list(name2label.keys()))","579b3a46":"confmat","1e707dd4":"df_confusion_test = pd.crosstab(label_values, pred_name_values)\ndf_confusion_test","995dae62":"<a id=\"preprocess_for_BERT_Train\"><\/a>\n\n<strong><h1>Preprocess for BERT Train<\/h1><\/strong>","135cf9ff":"In this study, I explained Sentiment Analysis in detail.\n\nI chose a sample dataset for Sentiment Analysis and embodied the subject I explained on a real example.\n\nThen I made a detailed analysis on the dataset and visualized it.\n\nAfter preprocessing the data, I tried to complete the Sentimet Analysis task with state-of-the-art models.\n\nI analyzed the results of this model and interpreted its outputs.\n\nI have indicated the sources I used while doing this study at the end of the notebook.\nThank you to everyone who contributed to this field :).","5d0005fc":"<a id=\"read_data\"><\/a>\n\n<strong><h1>Read Data<\/h1><\/strong>","ee0992da":"<a id=\"token_counts_with_BERT_tokenizer\"><\/a>\n\n## Token Counts with BERT tokenizer\n\nSince we will create a Transformers-based model, the value that BERT tokinezer will give us is very important. With the information here, the value of the `seq_len` parameter that we will use while encoding the data will be decided.","7f9d6f3c":"* `bert-base-uncased` is a smaller pre-trained model.\n* Using `num_labels` to indicate the number of output labels.","0a726881":"<a id=\"Data_Loaders\"><\/a>\n<strong><h2>Data Loaders<\/h2><\/strong>\n\n* `DataLoader` combines a dataset and a sampler, and provides an iterable over the given dataset.\n* We use `RandomSampler` for training and `SequentialSampler` for validation.\n* Given the limited memory in my environment, I set `batch_size=64`.","450c07f9":"<a id=\"word_cloud\"><\/a>\n\n## Word Cloud \n\nWord clouds generators work by breaking the text down into component words and counting how frequently they appear in the body of text. We can quickly obtain preliminary information about the data. We can understand what a dataset we don't know is talking about.","7be9a9fe":"<a id=\"most_common_ngrams\"><\/a>\n\n## Most Common ngrams","ea796898":"<a id=\"information_of_the_data\"><\/a>\n\n<strong><h1> Information of the Data <\/h1><\/strong>\n\nHotels play a crucial role in traveling and with the increased access to information new pathways of selecting the best ones emerged.\nWith this dataset, consisting of 20k reviews crawled from Tripadvisor, you can explore what makes a great hotel and maybe even use this model in your travels!\n\n<b>How to use<\/b>\n* Predict Review Rating\n* Topic Modeling on Reviews\n* Explore key aspects that make hotels good or bad","4071a84d":"<center><strong><h1> <div class=\"top_section\">In depth series 1: SENTIMENT ANALYSIS, why and how, EDA and solutions with Transformers<\/div><\/h1><\/strong><\/center>","098e74f3":"<a id=\"Creating_the_Model\"><\/a>\n\n<strong><h2>Creating the Model<\/h2><\/strong>","9f3e526f":"<a id=\"error_analysis\"><\/a>\n\n<strong><center><h1><div class=\"top_section\">4. ERROR ANALYSIS<\/div><\/h1><\/center><\/strong>\n","c7b7f856":"<a id=\"emotion_detection\"><\/a>\n\n**Emotion Analysis**\n\n<img src=\"https:\/\/raw.githubusercontent.com\/mek12\/detailed_sentiment_analysis_with_transformers\/main\/images\/emotions_boy.jpg\" width=\"800\">\n\n            source = https:\/\/kids.frontiersin.org\/articles\/10.3389\/frym.2018.00015\n\nThe type of emotion analysis in which emotion types(happiness, frustration, anger, and sadness) are classified is called **emotion detection.**\n\nThere are some difficulties with this classification. Users can express their feelings with many different words. They can use a word with a bad meaning for happiness. The most difficult examples of classification models here are; For example, the sentence \"I connect to customer service too late, it's killing me\" is a negative sentence, while the sentence \"you are killing me\" is positive.","df34c3fb":"<a id=\"types_of_sentiment_analysis\"><\/a>\n<strong><h2>Types of Sentiment Analysis<\/h2><\/strong>\n\n![](https:\/\/raw.githubusercontent.com\/mek12\/detailed_sentiment_analysis_with_transformers\/main\/images\/Sentiments-analysis_types.png)\n                \n                source = https:\/\/mobcoder.com\/blog\/sentimental-analysis-how-the-phenomenon-changing-the-dynamics-of-brand-monitoring\/\n\nSentiment analysis is aimed at determining the general emotional state of a text. One of these cases focuses on the polarity of a text (positive, negative, neutral) but it also goes beyond polarity to detect specific feelings and emotions (angry, happy, sad, etc), urgency (urgent, not urgent) and even intentions (interested v. not interested).\n\nLet's explain them in more detail","1b061cd1":"<strong><center><h1><div class=\"top_section\">If you like the notebook, Please don't forget to UPVOTE and comment  :) :)<\/div><\/h1><\/center><\/strong>","4f467fc6":"<a id=\"token_counts_with_simple_tokenizer\"><\/a>\n\n## Token Counts with simple tokenizer\n\nFinding out the number of tokens available for each sample will give us information about the length of our data. The classification algorithm we will use for a long text will not be the same as the algorithm used for a short text.","ea23a650":"**Rule-based Approaches**\n\nUsually, a rule-based system tries to help determine the subjectivity of the sentence, the polarity, or the subject matter of an idea. The most used tool here is \"regex\".\n\nThese rules usually include the following two NLP techniques:\n\n* Stemming, tokenization, part-of-speech tagging and parsing.\n* Lexicons (i.e. lists of words and expressions).\n\n\nThe working mechanism of these systems is briefly as follows;\n\n1. Build a list of polarized words (e.g. bad-good, worst-best, ugly-beautiful etc). You can find them as open source\n\n2. The ratio of positive and positive words in a sentence\n\nRule-based approaches are now obsolete, not used as much as they used to be. Rule-based approaches fail to detect ironies, not exactly how users are feeling. For this reason, automated approaches are gaining more importance now.\n\n\n\n","0e4708a8":"<a id=\"#graded_sentiment_analysis\"><\/a>\n\n**Graded Sentiment Analysis**\n\n![](https:\/\/raw.githubusercontent.com\/mek12\/detailed_sentiment_analysis_with_transformers\/main\/images\/gradient_Sentiment.png)\n            \n            source = https:\/\/i.pinimg.com\/originals\/5b\/7d\/62\/5b7d62fb62b03b8142b402cb85644865.png\n\nIf the precision of the mood is important, the categories can be further elaborated. A broader classification can be made, not just positive and negative:\n\n* Very positive\n* Positive\n* Neutral\n* Negative\n* Very negative\n\n\nThis classification is often used in reviews and reviews where 5 stars are awarded.\n\n* Very Positive = 5 stars\n* Very Negative = 1 star\n","425cc4de":"<a id=\"Optimizer_Scheduler\"><\/a>\n\n<strong><h2>Optimizer & Scheduler<\/h2><\/strong>","fb187110":"<a id=\"why_is_sentiment_analysis_important\"><\/a>\n\n\n**Why Is Sentiment Analysis Important?**\n\n![](https:\/\/brand24.com\/blog\/wp-content\/uploads\/2017\/09\/36896473_m-640x300.jpg)\n\n                    source = https:\/\/brand24.com\/\n\nPeople now share their comments\/emotions on social media, e-commerce sites and many other sites. A lot of data is created on these platforms.\n\nOften brands want to know what they are talking about. Brands\/companies make great efforts to quickly identify their customers' expectations and provide them with the right service.It allows their customers to learn what makes them happy or disappointed so they can tailor products and services to their customers' needs.\nIn addition, brands want to observe the impact of their advertisements on users.\n\nFor these reasons, Sentiment analysis is becoming more important every day.","806e7729":"<a id=\"references\"><\/a>\n\n<strong><center><h1><div class=\"top_section\">6. References<\/div><\/h1><\/center><\/strong>","1eac110d":"<a id=\"intent_analysis\"><\/a>\n\n**Intent Analysis**\n\nIntent analysis focuses on what the user wants to do. Understanding what the user wants to do will allow us to better guide him.\n\nFor example, being able to understand that a customer browsing an e-commerce site has a shopping intention also allows us to offer him the right products.\n\nOne of the most used areas is the smart assistant systems in the applications. It allows us to direct users to the right places within the application in line with their requests and we can offer a better application experience to the user.\n","a09aa745":"<a id=\"word_counts\"><\/a>\n\n## Word Counts\n\nWe see that the situation in the number of characters and the situation in the number of words are the same. We have seen that people use less word count when expressing negative things.","45d9a127":"<strong><h2>Comparison of Transformer Models<\/h2><\/strong>\n\n![](https:\/\/raw.githubusercontent.com\/mek12\/detailed_sentiment_analysis_with_transformers\/main\/images\/transformers_model_compare.png)\n\n            source = https:\/\/towardsdatascience.com\/bert-roberta-distilbert-xlnet-which-one-to-use-3d5ab82ba5f8\n\n\n","5b5d6ff6":"<a id=\"sentiment_analysis\"><\/a>\n\n<center><h1><div class=\"top_section\">1. SENTIMENT ANALYSIS<\/div><\/h1><\/center>\n\n<img src=\"https:\/\/raw.githubusercontent.com\/mek12\/detailed_sentiment_analysis_with_transformers\/main\/images\/sentimentanalysishotelgeneric-2048x803-1.jpg\">\n                \n                    source = https:\/\/d3caycb064h6u1.cloudfront.net\/wp-content\/uploads\/2021\/06\/sentimentanalysishotelgeneric-2048x803-1.jpg\n","9026816b":"<a id=\"automatic_approaches\"><\/a>\n\n**Automatic Approaches**\n\nThese systems don\u2019t rely on manually crafted rules, but on machine learning techniques, such as classification. Classification, which is used for sentiment analysis, is an automatic system that needs to be fed sample text before returning a category, e.g. positive, negative, or neutral.\n\nHere\u2019s how a machine learning classifier can be implemented:\n\n","198acb6b":"<a id=\"helper_functions\"><\/a>\n\n<strong><h1>Helper Functions<\/h1><\/strong>","25769938":"<a id=\"Training_Loop\"><\/a>\n<strong><h2>Training Loop<\/h2><\/strong>","11758355":"<center><h1><div class=\"top_section\">Table of Contents <\/div><\/h1><\/center>\n\n1.  **[SENTIMENT ANALYSIS](#sentiment_analysis)**\n\n    - [Types of Sentiment Analysis](#types_of_sentiment_analysis)\n        - [Emotion Detection](#emotion_detection)\n        - [Multilingual Sentiment Analysis](#multilingual_sentiment_analysis)\n        - [Graded Sentiment Analysis](#graded_sentiment_analysis)\n        - [Aspect-based Sentiment Analysis](#aspect_base_sentiment_analysis)\n        - [Intent Analysis](#intent_analysis)\n        \n    - [Why Is Sentiment Analysis Important?](#why_is_sentiment_analysis_important)\n    \n    - [The overall benefits of sentiment analysis include](#the_overall_benefits_of_sentiment_analysis_include)\n        - [Sorting Data at Scale](#the_overall_benefits_of_sentiment_analysis_include)\n        - [Real-Time Analysis](#the_overall_benefits_of_sentiment_analysis_include)\n        - [Discovering New Marketing Strategies](#the_overall_benefits_of_sentiment_analysis_include)\n      \n    - [How Does Sentiment Analysis Work?](#how_does_sentiment_analysis_work)\n    \n    - [Sentiment analysis Approaches](#sentiment_analysis_Approaches)\n        - [Rule-based Approaches](#rule_based_approaches)\n        - [Automatic Approaches](#automatic_approaches)\n       \n2.  **[EDA](#eda)**\n       - [Information of the DATA](#information_of_the_data)\n       - [Information of the Problem](#information_of_the_problem)\n       - [Imports](#imports)\n       - [Helper Functions](#helper_functions)\n       - [Read Data](#read_data)\n       - [Visualizations](#visualizations)\n           - [Word Cloud](#word_cloud)\n           - [Target Count](#target_count)\n           - [Token Counts with simple tokenizer](#token_counts_with_simple_tokenizer)\n           - [Token Counts with BERT tokenizer](#token_counts_with_BERT_tokenizer)\n           - [Characters Count in the Data](#characters_count_in_the_data)\n           - [Reviews Lengths](#reviews_lengths)\n           - [Word Counts](#word_counts)\n           - [Most Common Words](#most_common_words)\n           - [Most Common ngrams](#most_common_ngrams)\n3.  **[MODELS](#models)**\n    - [A brief information about BERT](#brief_informartion_about_Bert)\n    - [A brief information about XLNET](#brief_informartion_about_XLNET)\n    - [A brief information about RoBERTa](#brief_informartion_about_RoBERTa)\n    - [Comparison of Transformer Models](#comparison_of_Transformer_Models)\n    - [Preprocess for BERT Train](#preprocess_for_BERT_Train)\n    - [Train and Validation Split](#Train_and_Validation_Split)\n    - [BertTokenizer and Encoding the Data](#BertTokenizer_and_Encoding_the1_Data)\n    - [Creating the Model](#Creating_the_Model)\n    - [Data Loaders](#Data_Loaders)\n    - [Optimizer & Scheduler](#Optimizer_Scheduler)\n    - [Performance Metrics](#Performance_Metrics)\n    - [Training Loop](#Training_Loop)\n    - [Test on validation set](#test)\n4. **[ERROR ANALYSIS](#error_analysis)**\n5. **[INFERENCE](#inference)**\n5. **[REFERENCES](#references)**\n\n\n    \n\n\n\n\n    \n\n        \n        \n\n","5104b7d7":"<a id=\"sentiment_analysis_Approaches\"><\/a>\n\n**Sentiment analysis algorithms fall into one of three buckets:**\n\n* <strong>Rule-based:<\/strong> these systems automatically perform sentiment analysis based on a set of manually crafted rules.\n* <strong>Automatic:<\/strong> systems rely on machine learning techniques to learn from data.\n","67754fff":"Sentiment analysis (or opinion mining) is a natural language processing (NLP) technique used to determine whether data is positive, negative or neutral. Sentiment analysis is often performed on textual data to help businesses monitor brand and product sentiment in customer feedback, and understand customer needs.\n\nSentiment analysis helps data analysts within large enterprises gauge public opinion, conduct nuanced market research, monitor brand and product reputation, and understand customer experiences. In addition,  companies often develop sentiment analysis systems for customer experience management, social media monitoring, or workforce analytics platform to about their own customers.","27008073":"<a id=\"imports\"><\/a>\n\n<strong><h1>Imports<\/h1><\/strong>","415f0c2a":"<a id=\"the_overall_benefits_of_sentiment_analysis_include\"><\/a>\n\n**The overall benefits of sentiment analysis include:**\n\n**Sorting Data at Scale**\n\nUsers make a lot of comments about brands, it is almost impossible to process them manually. Sentiment analysis enables businesses to automatically classify large amounts of raw data.\n\n\n**Real-Time Analysis**\n\nCompanies can learn the wishes of their customers by analyzing the social media comments about you in real time. They can identify the angry customer and ensure his satisfaction.\n\n**Discovering New Marketing Strategies**\n\nWith more data and information gathered through sentiment analysis, the organizations could develop an effective marketing strategy.\n\nThe outcome from the strategies can be measured from the customers\u2019 positive or negative key messages.\n\nBy observing the customers\u2019 conversations on their social media and detect the specific key messages related to your brand, specific marketing campaigns can be designed for the target consumers. ","eb47b0bf":"**Classification Algorithms**\n\nThe classification step usually involves a statistical model like Na\u00efve Bayes, Logistic Regression, Support Vector Machines, or Neural Networks:\n\n* <strong>Na\u00efve Bayes:<\/strong> are a family of simple \"probabilistic classifiers\" based on applying Bayes' theorem with strong (na\u00efve) independence assumptions between the features (see Bayes classifier).\n* <strong>Linear Regression:<\/strong> is a linear approach for modelling the relationship between a scalar response and one or more explanatory variables (also known as dependent and independent variables).\n* <strong>Support Vector Machines(SVM):<\/strong> is a supervised machine learning algorithm that can be used for classification or regression problems. However, it is mostly used in classification problems. Support Vector Machine is a boundary that best separates two classes (hyperplane\/line)\n* <strong>Deep Learning:<\/strong> (also known as deep structured learning) is part of a broader family of machine learning methods based on artificial neural networks with representation learning. Learning can be supervised, semi-supervised or unsupervised.","3708d462":"<a id=\"BertTokenizer_and_Encoding_the1_Data\"><\/a>\n\n<strong><h2>BertTokenizer and Encoding the Data<\/h2><\/strong>","cc8b286e":"<a id=\"how_does_sentiment_analysis_work\"><\/a>\n\n\n**How Does Sentiment Analysis Work?**\n\n<img src=\"https:\/\/monkeylearn.com\/static\/30607381159c995d7e967c1f0530e50f\/95a1e\/how-does-sentiment-analysis-work%402x.webp\" width=\"600\">\n                    \n                        source = https:\/\/monkeylearn.com\/sentiment-analysis\/ \n\n\nSentiment analysis works to automatically determine emotional tone thanks to natural language processing (NLP), rule-based methods, and machine learning algorithms.\n\nThere are different ways we can do sentiment analysis, depending on how much data you need to analyze, how accurate your model needs to be, and how many resources you have.\n\nWe will talk about some of them below.\n\n","9c127684":"<a id=\"aspect_base_sentiment_analysis\"><\/a>\n\n**Aspect-based Sentiment Analysis**\n\n![](https:\/\/raw.githubusercontent.com\/mek12\/detailed_sentiment_analysis_with_transformers\/main\/images\/aspectbase.png)\n\n        source = https:\/\/www.surveysensum.com\/wp-content\/uploads\/2020\/02\/SENTIMENT-09-1.png\n\nGenerally, when analyzing the emotions of the texts, the focus is on determining whether the comment\/opinion is positive or negative. But we do not focus on what is positive or negative in this text.\n\nTo put it more clearly, in the expression \"I did not like the product at all, the size is too small\", the user is not satisfied with the product and complains about its dimensions. In a normal sentiment analysis, this sentence is classified as negative, but in **aspect-based sentiment analysis**, the \"the size is too small\" part is also focused on.","bbb954fc":"<a id=\"reviews_lengths\"><\/a>\n\n## Reviews Lengths\n\n\nWhen we look at the number of characters per comment, it can give us very striking information about the data. Here, when we look at the length of the comments made by people according to their feelings, negative comments are shorter than neutral and positive comments. We can come to the notion that people simply express negative things :).","45ec2957":"<a id=\"Test\"><\/a>\n<strong><h2>Test on validation set<\/h2><\/strong>","cafcbb80":"<a id=\"visualizations\"><\/a>\n\n\n<strong><h1>Visualizations<\/h1><\/strong>","53e99241":"<a id=\"inference\"><\/a>\n\n<strong><center><h1><div class=\"top_section\">5. INFERENCE<\/div><\/h1><\/center><\/strong>","e27705c2":"<a id=\"most_common_words\"><\/a>\n## Most Common Words","8430af1a":"<a id=\"multilingual_sentiment_analysis\"><\/a>\n\n\n**Multilingual Sentiment Analysis**\n\n\nIt is the version of Sentiment Analysis systems that provides multi-language support. What is mentioned here is to do sentiment analysis in more than one language.\n\nI usually have two suggestions for this:\n\nMy first suggestion is to detect the language of the text with the language classifier and run a sentimen analysis model suitable for this language. The second method is to develop a Multilingual language model and finetune this model and make the model work in many languages.\n\n","a0f9476e":"**In this table, the models are compared under 5 headings, let's take them all one by one.**\n\n1. When we look at the sizes of the models, BERT, RoBERTa and XLNet have the same values, while the size of the DistillBERT is smaller.\n\n2. The biggest factor that determines Training Times is the size of the models and the data they have. As you can imagine, the time increases as the size increases :). \n\n3. When we look at the performance, BERT considers the model as the base model. RoBERTa offers 2-20% better performance than BERT. A similar performance applies to XLNet. XLNet performs 2-15% better than BERT model. DisltiBERT, despite its small size, is not equally poor in performance. It performs only 3% worse.\n\n4. When we look at its data, the model with the largest corpus is ROBERTa. It is followed by XLNET, then BERT and DistilBERT have the same data. One of the reasons for the higher performance of RoBERTa and XLNet is that the datasets are so high.\n\n\n5. As it is known, there are MLM and NSP tasks in the BERT model. The RoBERTa model is the trained version of the BERT model without the NSP task. DiltilBERT is a reduced number of parameters of BERT, it maintains 97% performance, but uses only half the number of parameters (paper). To enhance the training, XLNet offers permutation language modeling where all tokens are predicted but in random order.\n\nI recommend you to read the articles for more detailed information.\n\n","12634546":"1. [Hugging Face](https:\/\/huggingface.co\/) \n2. [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https:\/\/arxiv.org\/abs\/1810.04805)\n3. [RoBERTa: A Robustly Optimized BERT Pretraining Approach](https:\/\/arxiv.org\/abs\/1907.11692)\n4. [XLNet: Generalized Autoregressive Pretraining for Language Understanding](https:\/\/arxiv.org\/abs\/1906.08237)\n5. [Coursera](https:\/\/www.coursera.org\/projects\/sentiment-analysis-bert)\n6. [Brand24](https:\/\/brand24.com\/)\n7. [MonkeyLearn](https:\/\/monkeylearn.com\/sentiment-analysis\/)","81cfb31f":"<a id=\"brief_informartion_about_XLNET\"><\/a>\n\n<strong><h2>A brief information about XLNET<\/h2><\/strong>\n    \n![](https:\/\/raw.githubusercontent.com\/mek12\/detailed_sentiment_analysis_with_transformers\/main\/images\/xlnet.png)\n\n**XLNet** is a large bidirectional transformer that uses improved training methodology, larger data and more computational power to achieve better than BERT prediction metrics on 20 language tasks.\n\nTo improve the training, XLNet introduces permutation language modeling, where all tokens are predicted but in random order. This is in contrast to BERT\u2019s masked language model where only the masked (15%) tokens are predicted. This is also in contrast to the traditional language models, where all tokens were predicted in sequential order instead of random order. This helps the model to learn bidirectional relationships and therefore better handles dependencies and relations between words. In addition, Transformer XL was used as the base architecture, which showed good performance even in the absence of permutation-based training.\n\nXLNet was trained with over 130 GB of textual data and 512 TPU chips running for 2.5 days, both of which ar e much larger than BERT.\n\n","723c9d7e":"<a id=\"information_of_the_problem\"><\/a>\n\n<strong><h1> Information of the Problem <\/h1><\/strong>\n\nCustomer satisfaction is very important for the service industry. For this reason, it is necessary to determine the emotional state of the customer's thoughts. We need to classify the user's emotion in our hotel reviews data.\n\n","4371d7f3":"<a id=\"eda\"><\/a>\n<strong><center><h1><div class=\"top_section\">2. EDA<\/div><\/h1><\/center><\/strong>\n","5acb2465":"<a id=\"Train_and_Validation_Split\"><\/a>\n<strong><h2>Train and Validation Split<\/h2><\/strong>\n","46b6fe9a":"**We examined and visualized the data, now we can move on to the model building part.**","669ea47a":"<a id=\"target_count\"><\/a>\n\n## Target Count\n\nHow many targets do we have? Learning this information will give us an idea about the model we will build. It will also provide guidance on our methods of analyzing data.\n","cd5a666a":"<a id=\"brief_informartion_about_Bert\"><\/a>\n\n<strong><h2>A brief information about BERT<\/h2><\/strong>\n\n\n![](https:\/\/raw.githubusercontent.com\/mek12\/detailed_sentiment_analysis_with_transformers\/main\/images\/bert_arch.png)\n\n**BERT** makes use of Transformer, an attention mechanism that learns contextual relations between words (or sub-words) in a text. In its vanilla form, Transformer includes two separate mechanisms \u2014 an encoder that reads the text input and a decoder that produces a prediction for the task. Since BERT\u2019s goal is to generate a language model, only the encoder mechanism is necessary. \n\nBERT is a bi-directional transformer for pre-training over a lot of unlabeled textual data to learn a language representation that can be used to fine-tune for specific machine learning tasks. While BERT outperformed the NLP state-of-the-art on several challenging tasks, its performance improvement could be attributed to the bidirectional transformer, novel pre-training tasks of Masked Language Model and Next Structure Prediction along with a lot of data and Google\u2019s compute power.\n\nThe detailed workings of Transformer are described in a paper by Google.","f742bde4":"<a id=\"Performance_Metrics\"><\/a>\n<strong><h2>Performance Metrics<\/h2><\/strong>\n\nWe will use f1 score  as performance metrics.","0d935e2e":"<a id=\"models\"><\/a>\n<strong><center><h1><div class=\"top_section\">3. MODELS<\/div><\/h1><\/center><\/strong>\n","152fce7f":"**Let's look at the frequency of the number of characters. It will give us information about the overall size of our data**","ede514e6":"<a id=\"brief_informartion_about_RoBERTa\"><\/a>\n\n<strong><h2>A brief information about RoBERTa<\/h2><\/strong>\n    \n**RoBERTa**. Introduced at Facebook, Robustly optimized BERT approach RoBERTa, is a retraining of BERT with improved training methodology, 1000% more data and compute power.\n\nTo improve the training procedure, RoBERTa removes the Next Sentence Prediction (NSP) task from BERT\u2019s pre-training and introduces dynamic masking so that the masked token changes during the training epochs. Larger batch-training sizes were also found to be more useful in the training procedure.\n\nImportantly, RoBERTa uses 160 GB of text for pre-training, including 16GB of Books Corpus and English Wikipedia used in BERT. The additional data included CommonCrawl News dataset (63 million articles, 76 GB), Web text corpus (38 GB) and Stories from Common Crawl (31 GB). This coupled with whopping 1024 V100 Tesla GPU\u2019s running for a day, led to pre-training of RoBERTa.","75a6f395":"<a id=\"characters_count_in_the_data\"><\/a>\n\n## Characters Count in the Data","014ac59c":"**We can explain the sentiment analysis in general like this. Now we have determined a data for how we will apply it next, and we will spread visualizations on that data and train models.**"}}