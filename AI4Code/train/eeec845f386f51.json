{"cell_type":{"8c51ba25":"code","a68e0032":"code","ed794a2f":"code","f2de0ffa":"code","b8fb4913":"code","7126fe6f":"code","f0b6b2a3":"markdown","4cd01323":"markdown","cdb1e7b5":"markdown","a8b5da8f":"markdown","76f602d2":"markdown","0debad7b":"markdown","cf437360":"markdown","6e853223":"markdown","35893f32":"markdown","3a6293b1":"markdown","6ca5565b":"markdown"},"source":{"8c51ba25":"from sklearn.ensemble import (\n    RandomForestClassifier,\n    ExtraTreesClassifier,\n    AdaBoostClassifier,\n)\nfrom lightgbm import LGBMClassifier\nfrom mlxtend.classifier import StackingCVClassifier\n# cross validation\nfrom sklearn.model_selection import KFold, cross_val_score\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt # Plot\n%matplotlib inline  \nimport os\nimport seaborn as sns # Plot\nfrom sklearn.model_selection import cross_val_score, train_test_split\nfrom sklearn.preprocessing import StandardScaler,RobustScaler,MinMaxScaler\nfrom sklearn.metrics import mean_absolute_error, accuracy_score\nfrom sklearn.pipeline import make_pipeline\n#Importing Confusion Matrix\nfrom sklearn.metrics import confusion_matrix,classification_report\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom mlxtend.classifier import StackingCVClassifier\n\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \n# Load file\nPath_train_ = \"..\/input\/learn-together\/train.csv\"\nPath_test_ = \"..\/input\/learn-together\/test.csv\"\nPath_train = \"..\/input\/comp-01\/new_train.csv\"\nPath_test = \"..\/input\/comp-01\/new_test.csv\"\n\n#Load dataset\nori_df_train_ = pd.read_csv(Path_train_,index_col='Id')# ->> \"train.csv\"\nori_df_test_ = pd.read_csv(Path_test_,index_col='Id')# ->>\"test.csv\"\ndf_train = pd.read_csv(Path_train)# ->> \"train.csv\"\ndf_test = pd.read_csv(Path_test)# ->>\"test.csv\"\noriginal_data_train = ori_df_train_.copy()\noriginal_data_test = ori_df_test_.copy()\ndata_train = df_train.copy()\ndata_test = df_test.copy()\nprint(\"original Train shape :\",original_data_train.shape)\nprint(\"original Test shape :\",original_data_test.shape)\nprint(\"New Train shape :\",data_train.shape)\nprint(\"New Test shape :\",data_test.shape)\ny = data_train.Cover_Type.values\nx = data_train.copy()\nx.drop('Cover_Type', inplace=True, axis=1)\ntest = data_test.copy()\n# Functions\ndef predict(model, filename, X=x, y=y, test=test):\n    model.fit(X, y)\n    predicts = model.predict(test)\n    \n    output = pd.DataFrame({'Id': test.index,\n                       'Cover_Type': predicts})\n    output.to_csv('best_submission.csv', index=False) \n    return predicts\n\ndef select(importances, edge):\n    c = importances.Importances >= edge\n    cols = importances[c].Features.values\n    return cols\n\ndef feature_importances(clf, x, y, figsize=(30, 30)):\n    clf = clf.fit(x, y)\n    \n    importances = pd.DataFrame({'Features': x.columns, \n                                'Importances': clf.feature_importances_})\n    \n    importances.sort_values(by=['Importances'], axis='index', ascending=False, inplace=True)\n\n    fig = plt.figure(figsize=figsize)\n    sns.barplot(x='Features', y='Importances', data=importances)\n    plt.xticks(rotation='vertical')\n    plt.show()\n    return importances\n\ndef cross_val(models, X=x, y=y):\n    r = dict()\n    for name, model in models.items():\n        cv_results = cross_val_score(model, x, y,\n                             cv=cv, \n                             scoring='accuracy')\n        r[name] = cv_results\n        print(name, 'Accuracy Mean {0:.4f}, Std {1:.4f}'.format(\n              cv_results.mean(), cv_results.std()))\n    return r\n    \ndef choose_best(results):\n    errors = dict()\n\n    for name, arr in results.items():\n        errors[name] = arr.mean()\n\n    best_model =  [m for m, e in errors.items() \n                   if e == max(errors.values())][0]\n    return best_model\n\nseed = 1\n\nmodels = {\n    'LGBM': LGBMClassifier(n_estimators=370,\n                           metric='multi_logloss',\n                           num_leaves=100,\n                           verbosity=0,\n                           random_state=seed,\n                           n_jobs=-1), \n    'Random Forest': RandomForestClassifier(n_estimators=1000,\n                                            n_jobs=-1,\n                                            random_state=seed),\n    'Extra Tree': ExtraTreesClassifier(\n           max_depth=400, \n           n_estimators=450, n_jobs=-1,\n           oob_score=False, random_state=seed, \n           warm_start=True)\n\n}","a68e0032":"clf = models['Random Forest']\nimportances = feature_importances(clf, x, y) \ncol = select(importances, 0.0004)\nx = x[col]\ntest = test[col]\n# model selection functions\ncv = KFold(n_splits=5, shuffle=True, random_state=seed)","ed794a2f":"results = cross_val(models)","f2de0ffa":"best_model_name = choose_best(results)\nmodel = models[best_model_name]\n# Meta Classifier\nmeta_cls = XGBClassifier(learning_rate =0.1, n_estimators=500)\nlist_estimators = [RandomForestClassifier(n_estimators=500,random_state=1, n_jobs=-1),\n                   MLPClassifier(batch_size='auto', \n                                 random_state=1,activation='relu',\n                                 solver='adam',verbose=True,learning_rate='constant',\n                                 alpha=0.004,hidden_layer_sizes=(100,100),max_iter=1000),\n                   ExtraTreesClassifier(max_depth=400,\n                                        criterion='entropy',\n                                        n_estimators=898, \n                                        n_jobs=-1,\n                                        oob_score=False, \n                                        random_state=seed, \n                                        warm_start=True)]\nbase_methods = [\"RandomForestClassifier\", \"MLPClassifier\",\"ExtraTreesClassifier\"]\nstate = 1\nstack = StackingCVClassifier(classifiers=list_estimators,\n                             meta_classifier=meta_cls,\n                             cv=5,\n                             use_probas=True,\n                             verbose=1, \n                             random_state=seed,\n                             n_jobs=-1)","b8fb4913":"stack.fit(x, y)\nprint('Complete!')","7126fe6f":"preds_test = stack.predict(test)\n# Save test predictions to file\noutput = pd.DataFrame({'Id': ori_df_test_.index,\n                       'Cover_Type': preds_test})\noutput.to_csv('submission.csv', index=False)\noutput.head()","f0b6b2a3":"# Feature Importances","4cd01323":"# Resource   \n\nhttps:\/\/www.kaggle.com\/evimarp\/top-6-roosevelt-national-forest-competition\/output#Feautures-importances   \nhttps:\/\/lightgbm.readthedocs.io\/en\/latest\/   \nhttps:\/\/scikit-learn.org\/stable\/modules\/neural_networks_supervised.html   \nhttps:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.RandomForestClassifier.html   \nhttps:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.ExtraTreesClassifier.html\n","cdb1e7b5":"## Create submission","a8b5da8f":"# Conclusion   \n* developing ... \n","76f602d2":"# Stack model","0debad7b":"# Feature score ","cf437360":"seed = 2019   \nLGBM Accuracy Mean 0.8821, Std 0.0033   \nRandom Forest Accuracy Mean 0.8630, Std 0.0036   \nExtra Tree Accuracy Mean 0.8712, Std 0.0020   ","6e853223":"# Import","35893f32":"seed = 2   \nLGBM Accuracy Mean 0.8799, Std 0.0036   \nRandom Forest Accuracy Mean 0.8618, Std 0.0020   \nExtra Tree Accuracy Mean 0.8716, Std 0.0052   ","3a6293b1":"seed = 1   \nLGBM Accuracy Mean 0.8835, Std 0.0037   \nRandom Forest Accuracy Mean 0.8618, Std 0.0044   \nExtra Tree Accuracy Mean 0.8701, Std 0.0035   ","6ca5565b":"seed=0   \nLGBM Accuracy Mean 0.8815, Std 0.0047   \nRandom Forest Accuracy Mean 0.8630, Std 0.0071   \nExtra Tree Accuracy Mean 0.8706, Std 0.0073   "}}