{"cell_type":{"17fa936d":"code","a909f04e":"code","39a53717":"code","561d05e2":"code","d39e2f6e":"code","6d1b9d2f":"code","83f41602":"code","fbd585a7":"code","8ff16861":"code","63fb933c":"code","2970bb12":"code","e0b9ecf9":"code","7bacfca7":"code","94a4fb33":"code","0076909c":"code","aeb5c69b":"code","0817199e":"code","64035fee":"code","464ec804":"code","cf2c057e":"code","5e7357bb":"code","992885a1":"code","380fb42c":"code","794a618e":"code","d7e7b2e1":"code","df9db8b6":"code","d1e5f2ad":"code","8a4cafce":"code","5ab854da":"code","2e5a5e38":"code","e9e4dfed":"code","520f970f":"code","5a0af0f3":"code","4c4041a5":"code","cfc1131b":"code","2d81955e":"code","4f1246c1":"code","531d1d67":"code","8028ae40":"code","a0f67c55":"code","3199b5eb":"code","e24a4025":"code","8a5c690b":"code","bd69f4bf":"code","16f032ad":"code","2b99c448":"code","7dfb7c41":"code","ad0dce72":"code","6ae17899":"code","71b68f3a":"code","3c966238":"code","b60ec651":"code","30dcd77b":"code","d57cdbec":"markdown","c68c1321":"markdown","c230845e":"markdown","140d37ce":"markdown","fe19e093":"markdown","05a51661":"markdown","0ae1ba8c":"markdown","0c995153":"markdown","9354f5c9":"markdown","02cee4fb":"markdown","cb97f71d":"markdown","ab563e3f":"markdown","239d65ec":"markdown","3047133f":"markdown","2dad120e":"markdown","cb3af0b1":"markdown","d95a17cb":"markdown","9d9e2c49":"markdown","d961a6c1":"markdown","dc2a0ccc":"markdown","e533e317":"markdown","8e97b462":"markdown","38e3c780":"markdown","3684508a":"markdown","53aa96ff":"markdown","44583d9b":"markdown","f6f26dee":"markdown","8f6b89e7":"markdown","94fe0ccb":"markdown","7de5b0b7":"markdown","c5c07b1f":"markdown","6943778f":"markdown","9fa45713":"markdown","12ff967b":"markdown","5f45288b":"markdown","bdcca6cb":"markdown","7e713487":"markdown","d60cac63":"markdown","7af25528":"markdown","1814fb4d":"markdown","df7dc049":"markdown","0a7b6738":"markdown","7b7733c1":"markdown","60e6f578":"markdown"},"source":{"17fa936d":"import pandas as pd\nimport numpy as np\nimport json\nfrom pandas.io.json import json_normalize\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression","a909f04e":"with open(\"..\/input\/data.dat\") as json_file:\n    json_data = json.load(json_file)\nprint(type(json_data))","39a53717":"with open(\"..\/input\/data.dat\") as json_file:\n    json_data = json.load(json_file)\njson_data['houses'] \ndf = json_normalize(json_data, 'houses') # parsing the data into dataframe format\ndf.head()","561d05e2":"# get some general insights about the dataset\ndf.info()","d39e2f6e":"# format date for values in column 'date'\ndf['date'] = pd.to_datetime(df['date'], format = \"%Y%m%dT%H%M%S\", errors = 'coerce')\ndf.info()","6d1b9d2f":"# identify row indexes of missing values in column date \ndf[(df.date.isnull() == True)]","83f41602":"# parsing the data again into table \njson_data['houses']\ndf = json_normalize(json_data, 'houses') # parsing the data into dataframe format\n\n# display 2 row indexes that have wrong value format\ndf[4334:4336]","fbd585a7":"df.loc[4334, 'date'] = '20140630T000000'\ndf.loc[4335, 'date'] = '20140523T000000'","8ff16861":"# format date for values in column 'date'\ndf['date'] = pd.to_datetime(df['date'], format = \"%Y%m%dT%H%M%S\", errors = 'ignore')","63fb933c":"# convert data type of column yr_renovated to integer\n# if value is missing, set it to 0\ndf.yr_renovated = df.yr_renovated.apply(lambda x: int(x) if x == x else 0)\n\n# convert data type of column price to integer\n# if value is missing, set it to 0\ndf.price= df.price.apply(lambda x: int(x) if x == x else 0)\n\ndf.head()","2970bb12":"import re\n\n# extract number of bedrooms for each address\/house and store in a list\nbed_rooms = []\nfor bed in df['rooms']:\n    num = re.search(r'bedrooms:\\s+(\\d*\\.\\d+|\\d+)', bed)\n    bed_rooms.append(float(num.group(1)))\n\n# extract number of bathrooms for each address\/house and store in a list\nbath_rooms = []\nfor bath in df['rooms']:\n    num2 = re.search(r'bathrooms:\\s+(\\d*\\.\\d+|\\d+)', bath)\n    bath_rooms.append(float(num2.group(1)))","e0b9ecf9":"# add 2 new columns to the dataframe\ndf['bathrooms'] = bath_rooms\ndf['bedrooms'] = bed_rooms\n\n# df.head()\n# Compare newly added columns with the original one. Numbers are put in correct columns for bathrooms and bedrooms number. ","7bacfca7":"df = df.drop(['rooms'], 1)\ndf.head()","94a4fb33":"# split column 'area' into 3 different columns: 'sqft_basement', 'sqft_above' and 'sqft_living\/sqft_lot'\n# then drop the 'area' column\ndf[['sqft_basement','sqft_above','sqft_living\/sqft_lot']] = pd.DataFrame(df['area'].values.tolist(), columns=['sqft_basement','sqft_above','sqft_living\/sqft_lot'])\ndf = df.drop('area', axis = 1)\n\n# continue to split column 'sqft_living\/sqft_lot' and drop it\ndf[['living\/lot', 'sqft_num']] = df['sqft_living\/sqft_lot'].str.split('=', expand=True)\ndf = df.drop(['sqft_living\/sqft_lot','living\/lot'], 1)\n\n# finally, split 'sqft_num' column and drop it\ndf[['sqft_living', 'sqft_lot']] = df['sqft_num'].str.split(\"\\\\\", expand=True)\ndf[['sqft_living', 'sqft_lot']] = df[['sqft_living', 'sqft_lot']].apply(pd.to_numeric)\ndf = df.drop('sqft_num', 1)\n\ndf.head()","0076909c":"# split column 'address' into 4 columns\ndf[['street','city','statezip','country']] = df['address'].str.split(', ',expand=True)\ndf = df.drop('address', axis=1)\n\n# reorder columns in the dataframe\n# then, check value type of each column for further processing\ndf = df[[\"date\", \"price\", \"bedrooms\", \"bathrooms\", \"sqft_living\", \"sqft_lot\", \"floors\", \"waterfront\", \"view\", \n         \"condition\", \"sqft_above\", \"sqft_basement\", \"yr_built\", \"yr_renovated\", \"street\", \"city\", \"statezip\", \"country\"]]\nprint(len(df.columns))\ndf.dtypes","aeb5c69b":"# checking number of unique values in column 'country'\nprint(len(df['country'].unique()))\nprint(df['country'].unique())","0817199e":"df = df.drop('country', 1)\ndf.head()","64035fee":"df['city'].value_counts()","464ec804":"# sorting the list of unique city names in the dataframe \nprint(sorted(df['city'].unique().tolist()))","cf2c057e":"# Fix name errors in column 'city'\ndf['city'].replace({'Auburnt':'Auburn', 'auburn':'Auburn', \n                 'Belleview':'Bellevue', 'Bellvue':'Bellevue',\n                 'Issaguah':'Issaquah',\n                 'Kirklund':'Kirkland',\n                 'Redmonde':'Redmond', 'Redmund':'Redmond', 'redmond':'Redmond',\n                 'Samamish':'Sammamish', 'sammamish':'Sammamish',\n                 'Seaattle':'Seattle', 'Seatle':'Seattle', 'seattle':'Seattle',\n                 'Snogualmie':'Snoqualmie', 'Snoqualmie Pass':'Snoqualmie',\n                 'Woodenville':'Woodinville'}, inplace=True)\nprint(sorted(df['city'].unique().tolist()))","5e7357bb":"len(df['statezip'].unique())","992885a1":"df[(df['sqft_above'] + df['sqft_basement'] != df['sqft_living'])]","380fb42c":"# replacing values for 2 rows at index 4338 and 4339\ndf.loc[4338, 'sqft_living'] = 2700\ndf.loc[4339, 'sqft_basement'] = 300\ndf.loc[4337:4340]","794a618e":"df[(df['yr_renovated'] != 0) & (df['yr_renovated'] <= df['yr_built'])]","d7e7b2e1":"# swap values of 2 columns yr_built and yr_renovated\ndf.loc[4340, 'yr_built'], df.loc[4340, 'yr_renovated'] = df.loc[4340, 'yr_renovated'], df.loc[4340, 'yr_built']\ndf.loc[4341, 'yr_built'], df.loc[4341, 'yr_renovated'] = df.loc[4341, 'yr_renovated'], df.loc[4341, 'yr_built']\ndf.loc[4342, 'yr_built'], df.loc[4342, 'yr_renovated'] = df.loc[4342, 'yr_renovated'], df.loc[4342, 'yr_built']\ndf.loc[4345, 'yr_built'], df.loc[4345, 'yr_renovated'] = df.loc[4345, 'yr_renovated'], df.loc[4345, 'yr_built']\n\ndf.loc[4340:4345]","df9db8b6":"df[df.duplicated([\"price\", \"street\"], keep=False)]","d1e5f2ad":"# display columns that will be dropped\ndf[df.duplicated([\"price\", \"street\"], keep=\"last\")]","8a4cafce":"# drop the 2 data rows above\ndf.drop_duplicates([\"price\", \"street\"], keep='last', inplace=True)","5ab854da":"df[df.duplicated([\"date\", \"street\"], keep=False)]","2e5a5e38":"df.drop_duplicates([\"date\", \"street\"], keep='first', inplace=True)\n# check the length of dataframe after dropping duplicated rows\nlen(df)","e9e4dfed":"df[(df['bedrooms']==0) | (df['bathrooms']==0)]","520f970f":"%matplotlib inline\ndf['bedrooms'].hist()","5a0af0f3":"df['bathrooms'].hist()","4c4041a5":"# check the information of properties that have price in range from $1,000,000 to $1,500,000\nnew_df = df[(df['price']>=1000000) & (df['price']<=1500000)]\nnew_df","cfc1131b":"# display properties with statezip of WA 98102\nnew_df[(new_df['statezip'] == 'WA 98102')]","2d81955e":"# replace missing values\ndf.loc[2365, 'bedrooms'] = 3\ndf.loc[2365, 'bathrooms'] = 3\n\n# display properties with statezip of WA 98053\nnew_df[(new_df['statezip'] == 'WA 98053')]","4f1246c1":"# replace missing values\ndf.loc[3209, 'bedrooms'] = 3\ndf.loc[3209, 'bathrooms'] = 3.53\n\n# check if missing values of bedrooms and bathrooms still exist in the dataframe\ndf[(df['bedrooms']==0) | (df['bathrooms']==0)]","531d1d67":"# display histogram of price\nplt.figure(figsize=(15, 10))\ndf.price.hist(bins=500)\nplt.xlim(0,2000000)","8028ae40":"len(df[(df.price == 0)])","a0f67c55":"# replace value 0 in column price by NaN \ndf.loc[4353:4600,'price'] = df.loc[4353:4600,'price'].replace(0, np.NaN)\ndf.loc[4353:4600]","3199b5eb":"# create a copy of the initial dataframe\nimpute_df = df.copy()\n\n# drop null values from initial dataframe\ndf.dropna(subset=['price'],axis=0,inplace=True)\ndf['price'].isnull().sum()\n\n# null values still exist in imputed dataframe\nimpute_df['price'].isnull().sum()\n\n# drop column indexes that are inappropriate in the model as they are not in numerical format \nlm_fitting_df = df.drop(['date','street','city','statezip'],axis=1)\n\nlm_for_impute = LinearRegression() #instatiate\nlm_for_impute.fit(lm_fitting_df[[x for x in lm_fitting_df.columns if x != 'price']],lm_fitting_df['price']) #fit\n\nimpute_df[impute_df['price'].isnull()].head()","e24a4025":"lm_for_impute.predict(impute_df.drop(['price','statezip','city','street','date'],axis=1)) \n#this uses the other features to predict 'price' with the model\n\nimpute_df['price'][impute_df['price'].isnull()] = lm_for_impute.predict(impute_df.drop(['price','statezip','city','street','date'],axis=1))\n\n# display boxplot to compare the dropna dataframe and imputed dataframe\nboxplot = pd.DataFrame({'imputed': impute_df['price'], 'dropped': df['price']})\nboxplot.plot(kind='box')","8a5c690b":"# set the y limit a bit smaller\nboxplot = pd.DataFrame({'imputed': impute_df['price'], 'dropped': df['price']})\nboxplot.plot(kind='box')\nplt.ylim(0,2000000)","bd69f4bf":"# convert data type of column price to integer\n# if value is not available, set it to 0\nimpute_df.price = impute_df.price.apply(lambda x: int(x) if x == x else 0)\n\n# display rows that previously had missing value in original dataframe\nimpute_df.loc[4353:4601]","16f032ad":"# display boxplot for yr_built and yr_renovated\nplt.figure(figsize=(10, 7))\ngraph = impute_df[['yr_built','yr_renovated']].boxplot()\nplt.ylim(1890,2020)","2b99c448":"# boxplot number of bedrooms, bathrooms and floors\nplt.figure(figsize=(10, 7))\ngraph = impute_df[['bedrooms','bathrooms','floors']].boxplot()","7dfb7c41":"# display properties which have number of either bedrooms or bathrooms equal or more than 8\nimpute_df[(impute_df['bedrooms'] >= 8) | (impute_df['bathrooms'] >= 8)]","ad0dce72":"# boxplot price\nplt.figure(figsize=(10, 7))\ngraph = impute_df[['price']].boxplot()","6ae17899":"# display sales record that have price more than 5 mil dollar\nimpute_df[(impute_df['price'] >= 5000000)]","71b68f3a":"impute_df['price'].max()","3c966238":"# drop 2 rows that have suspicious values\nimpute_df.drop(4347, axis = 0, inplace=True)\nimpute_df.drop(4351, axis = 0, inplace=True)\nimpute_df = impute_df.reset_index(drop=True)\n\n# display boxplot graph for values of square footage \nplt.figure(figsize=(10, 7))\ngraph = impute_df[['sqft_living','sqft_lot','sqft_above','sqft_basement']].boxplot()","b60ec651":"# display rows which have sqft_lot larger than 600,000\nimpute_df[(impute_df['sqft_lot'] >= 600000)]","30dcd77b":"impute_df.drop(2478, axis = 0, inplace=True)\n\n# reset index and writing cleaned data to csv format file\nimpute_df.reset_index()\nimpute_df.to_csv(\"RealEstateData.csv\", encoding='utf-8')","d57cdbec":"**h. Using regression imputation to fill missing values in price**","c68c1321":"**d. Splitting column 'area'**\n\nThis column is supposed to be splitted into 4 columns which are 'sqft_basement', 'sqft_above', 'sqft_living' and 'sqft_lot'.","c230845e":"There are lexical errors (including typos and inconsistent spelling) that can be found from the list of city names above:\n\n> Auburn vs Auburnt and auburn\n\n> Bellevue vs Belleview and Bellvue\n\n> Issaquah vs Issaguah\n\n> Kirkland vs Kirklund\n\n> Redmond vs Redmonde, Redmund and redmond\n\n> Sammamish vs Samamish and sammamish\n\n> Seattle vs Seaattle, Seatle and seattle\n\n> Snoqualmie vs Snogualmie and Snoqualmie Pass\n\n> Woodinville vs Woodenville\n\nSo we are going to fix these errors by replacing them with correct city name","140d37ce":"We can see that only sqft_lot has outliers","fe19e093":"There are 224 properties in this price range.\n\nWe take a closer look at properties within the same area (statezip).","05a51661":"There is no outlier in yr_built and yr_renovated","0ae1ba8c":"The price at row index 4351 is unrealistic because we can look at other attributes of the property to state that it is not worth up to $26,590,000. It is also the maximum value in the column 'price' and affects significantly to the dataset. So we could reduce the value of price so that it will be more appropriate to other figures in the row. However, This way may generate incorrect value in stead of providing actual number from real life. Thus, the best way is to remove this row from the dataset.\n\nSimilarly, the figures (except the price) at index 4347 can describe this is just an ordinary property with 1 floors, 3 bedrooms and 2.5 bathrooms. Although the square footage of landscape is quite big but it can not be worth $12,899,000. So we should remove this row as well.\n\nThe record at index 2286 has much lower price compared to the other two. It has significantly larger sqft_living and sqft_lot with good view and waterfront. Compared to properties in previous part in term of number bedrooms and bathrooms, we can say it is a bit overpriced, but it is believable. Thus, we keep it in the dataset.","0c995153":"This program tends to wrangle a large dataset of property sales records stored in an unknown format and with unknown data quality issues. After running through all the steps below, it will generate an output file named ReadEstateData.csv.\n\nThis is the description of the data set:\n    - Attribute:  Description\n    - date: Date of the property sold, e.g., 20140502T000000\n    - price: Property sold price\n    - bedrooms: Number of bedrooms\n    - bathrooms: Number of bathrooms, the value of which can be either an integer or a fraction ending with .25, .5, and .75. For example, 0.5 accounts for a room with a toilet but no shower \n    - sqft_living: Square footage of the property's interior living space, it is equal to the sum of basement area (i.e., sqft_basement) and the above living area (i.e., sqft_above) \n    - sqft_lot: Square footage of the land space\n    - floors: Number of floors\n    - waterfront: Whether the property was overlooking the waterfront or not\n    - view: An index from 0 to 4 of how good the view of the property was\n    - condition: An index from 1 to 5 on the condition of the property.\n    - sqft_above: The square footage of the interior living space that is above ground level \n    - sqft_basement: The square footage of the interior living space that is below ground level\n    - yr_built:\tThe year the property was initially built \n    - yr_renovated: The year of the property's last renovation\n    - street: The street address of the property, e.g., \u201c3140 Franklin Ave E\u201d\n    - city: The city where the property is, e.g., \u201cSeattle\u201d\n    - statezip: The zip code area where the property is, which contains state and zip code, separated by a space. For example, \u201cWA 98115\u201d, where WA is the abbreviation of Washington and the number is the zip code.\n    - country: The country where the property is, e.g., \u201cUSA\u201d\n","9354f5c9":"The data is stored in a dictionary, in which the key is \"houses\" and all the records we want to extract are the value of the key. Therefore, we parse the data into datafram format ","02cee4fb":"**f. Missing values in column 'bedrooms' and 'bathrooms'**\n\nIt is not rational if there is a house without bedrooms or bathrooms. So we check in the dataframe if there is any row that have 0 bedrooms or bathrooms. There could be missing values in these two columns of the dataframe.","cb97f71d":"> Row index 122: This is the image of the property from the Google Map. So we can believe the figures of this record and keep it as an outlier in the dataframe. (What a fancy house!) \n\n> Row index 1500: There is nothing strange in the figures of this record. We can inspect the house from the below link. So we should keep this outlier in the dataframe as well. https:\/\/www.google.com.au\/maps\/place\/1210+22nd+Ave+E\/@47.6309693,-122.3025648,49a,35y,270h,45.05t\/data=!3m1!1e3!4m6!3m5!1s0x549014d1189eb96f:0x1b311ea6f2aaca84!4b1!8m2!3d47.630975!4d-122.3031835\n\n> Similar with other 2 properties, the number of bedrooms and bathrooms in these properties could be real. So we keep them all.\n\nRecord at index 241: https:\/\/www.google.com.au\/maps\/place\/8809+Densmore+Ave+N,+Seattle,+WA+98103,+USA\/@47.692608,-122.3376477,42a,35y,270h,39.57t\/data=!3m1!1e3!4m5!3m4!1s0x5490141b39729d5b:0x2089d8e537507155!8m2!3d47.6926869!4d-122.338141\n\nRecord at index 3911: https:\/\/www.google.com.au\/maps\/place\/17512+Corliss+Ave+N,+Shoreline,+WA+98133,+USA\/@47.7564967,-122.3317132,46a,35y,90h,39.54t\/data=!3m1!1e3!4m5!3m4!1s0x549010f9fb069493:0xe372a036f0e82212!8m2!3d47.7564618!4d-122.3311166","ab563e3f":"**e. Duplications**\n\nIf we assume that street and price can uniquely identify a house sale record, we can then use the two values to check whether or not the dataset contains duplicated records.","239d65ec":"Fortunately, there is no null or NA value in this column. Now, we can add two new columns to the dataframe","3047133f":"The output above gives us some hints about problems in the dataframe. Firstly, values in column 'date' are not in the right format, so we need to change it to datetime format. Secondly, there are missing values in column 'yr_renovated' as there are only 229 non-null float64 values which are not in appropriate format for further processing.","2dad120e":"There are 2 rows where the values violate integrity constraint.\n\nIn the first row, sqft_living value is equal to sqft_above value, so we can guess there was a mistake in adding sqft_above and sqft_basement together to generate sqft_living. Thus, it is reasonable to fix this problem in this row by replacing value in sqft_living column by the sum value of sqft_above and sqft_basement. The value of sqft_living would be: 1280 + 1420 = 2700\n\nIn the second column, sqft_living is larger than squft_above while sqft_basement is 0, we can subtract sqft_living to sqft_above in order to get the sqft_basement value. The value of sqft_basement would be: 890 - 590 = 300","cb3af0b1":"One way to fix this error is that we can swap position of values in these 2 columns.","d95a17cb":"**c. Splititing column 'rooms'**\n\nType of values in 'rooms' column is string. Those values indicate number of bathrooms and bedrooms in a house. Therefore, we should use regular expression to extract those numbers and group them into 2 different lists, then we will add those 2 lists into the dataframe.\n","9d9e2c49":"**g. Inspection on column 'price'**","d961a6c1":"In addition, we can also use date and street to uniquely identify a sale record","dc2a0ccc":"We can notice many outliers in 'price'. Those points that lie above 0.5*1e7 (5,000,000 dollar) can be considered as outliers.","e533e317":"We need to parse the data again to see what original data is.","8e97b462":"Here we see that most of properties in the dataset have 3 or 4 bedrooms and 2 or 3 bathrooms (may be 2 as well).\n\nBecause prices of the two properties are both over 1 million dollar so we may want to compare them with other properties in the same category or price range to have a guess on the number of bedrooms and bathrooms.","38e3c780":"For the row index 1077, only sqft_lot seems to be too big compared to other figures in the row. But when we look at the address, this location includes many properties in it, so a large number of square footage of landscape could result from this. The property is a unit in that area. Therefore, there is nothing wrong with this record.\n\nUnlike the previous record, the record at index 2478 shows that there is only one house in that landscape and its price is even larger than the previous one. So we can not trust this number. Thus, we should remove it from dataframe.","3684508a":"There are 248 rows that have missing value in price","53aa96ff":"**e. Splitting column 'address'**","44583d9b":"**c. Inspection on the constraint of data in 3 columns sqft_living, sqft_above and sqft_basement**\n\nSquare footage for the property's interior living space is equal to the sum of basement area (sqft_basement) and the above living area (sqft_above). We implement a code to check if there is any row not complying to this constraint.","f6f26dee":"**b. Syntactical errors in column 'city'**","8f6b89e7":"**1. Task 1: Parsing the property sales data stored in \"data.dat\"**\n\nIn this part, we will need to use some libraries to parse the data in json format and use a \"simple\" imputation method which is LinearRegression to fill in na values","94fe0ccb":"Let's have a check from a real source to find the correct record. The link below contains the information of the sale record: https:\/\/www.redfin.com\/WA\/Auburn\/606-5th-St-SE-98002\/home\/213118\n\nIt seems the first one contains more correct information about the property, so it should be kept in the dataframe. Therefore, we will eliminate the second record from the dataframe.","7de5b0b7":"**a. Fixing datetime format in column 'date'**","c5c07b1f":"**i. Investigating outliers**\n\nWe can group data as following for better visualization:\n\n> price\n\n> bedrooms, bathrooms and floors\n\n> sqft_living, sqft_lot, sqft_above and sqft_basement\n\n> yr_built and yr_renovated","6943778f":"After converting column 'date' to datetime format, date values in 2 rows now turn into null.\n\nSo we need to trace back to the original dataframe to see what is wrong here.","9fa45713":"**b. Formatting column 'yr_renovated' and 'price'**","12ff967b":"**2. Task 2: Auditing and cleansing the loaded data**\n\n**a. Dropping column 'country'**","5f45288b":"**d. yr_built versus yr_renovated**\n\nIn logic, yr_renovated is supposed to be bigger than yr_builts, so we check rows which have value of yr_renovated smaller than value of yr_built.\n","bdcca6cb":"Now, we may want to format the date format of the dataframe again to make everything right.","7e713487":"There are 2 rows missing value of bedrooms and bathrooms.\n\nFirst row indicates a property in Seattle, WA 98102.\n\nThe second one is a property in Redmond, WA 98053.","d60cac63":"The output shows that there are 4 duplicated records. The third and fourth have the same information so we can drop whichever from one of these two.\n\nThe first and second record only have different values in date and sqft_lot. We can see that there is not big differences between those 2 records, so it is reasonable for us to keep the record that have values closer to the mean and drop the other one as it will not affect significantly on the results of output when we have further analysis. In this case, we decide to keep the last duplicate as it has values closer to the mean and drop the first one.","7af25528":"Then, we can drop the original column.","1814fb4d":"Here we see that 'date' value at row index 4334 has wrong value since it indicates that the property was sold on 31 June 2014 but June does not have day 31. The situation here is that it may try to refer date of the sales on the last day of June. Thus, we should change this value to 20140630\n\nThe second row index has a different format from other values in the column as it indicates day-month-year format instead of year-month-day. Therefore, we should edit this value to make it comply with the column format.","df7dc049":"The histogram shows that there are about 250 properties that have price at 0 dollar. This seems illogical. These values could be missing value that we need to fill in or drop off in order to make the dataframe complete.","0a7b6738":"Other properties in the same area have 3 bed rooms. So it is likely to impute the number of bedrooms at index 2365 to 3. For number of bathrooms, we impute the missing value by average number of bathrooms of properties in this area (which is: (2.5+3.5)\/2 = 3)","7b7733c1":"Similar to the previous case, all other properties in the same area with the one at index 3209 have the same number of bedrooms which is 4. So it is reasonable to impute number of bedrooms at this index to 4 and number of bathrooms equal the mean of this group (3.53).","60e6f578":"Since the column country has only one unique value which is 'USA', that means the dataframe includes information of real estate market in United States. Therefore, we can delete this column as it is redundant now."}}