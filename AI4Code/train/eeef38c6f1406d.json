{"cell_type":{"f618a7bb":"code","eb765de7":"code","48d9b476":"code","20d9313b":"code","4649b386":"code","7452fb2c":"code","33e7ac35":"code","b58fa3e9":"code","089a55cb":"code","33b0f4a4":"code","0df77ce0":"code","b14d54c3":"code","a8247496":"code","c9c4fa79":"code","aa2f06dc":"code","1fe96b85":"code","24f3c1bc":"code","2aa2a453":"code","50c94bcb":"code","32aec827":"code","b27bb1b0":"markdown"},"source":{"f618a7bb":"# data analysis and wrangling\nimport pandas as pd\nimport numpy as np\nimport random as rnd\n\n# visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline \n\n# machine learning\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.preprocessing import StandardScaler, normalize","eb765de7":"import warnings\nwarnings.filterwarnings('ignore')","48d9b476":"#Load the training and test dataset\ntrain_df = pd.read_csv('\/kaggle\/input\/widsdatathon2020\/training_v2.csv')\ntest_df = pd.read_csv('\/kaggle\/input\/widsdatathon2020\/unlabeled.csv')","20d9313b":"#Know the features of the dataset\nprint(train_df.info())\nprint(train_df.shape)","4649b386":"# Train missing values (in percent)\ntrain_missing = (train_df.isnull().sum() \/ len(train_df)).sort_values(ascending = False)\ntrain_missing.head()\ntrain_missing = train_missing.index[train_missing > 0.75]\nprint('There are %d columns with more than 75%% missing values' % len(train_missing))\nprint('The missing columns are %s' % train_missing)\ndf_train = train_df.drop(columns = train_missing)\ndf_test = test_df.drop(columns = train_missing)\ndf_train.shape\ndf_test.shape","7452fb2c":"columns_to_drop = train_df[['patient_id', 'hospital_id','icu_id','readmission_status','hospital_death']]\ntarget = train_df['hospital_death']\ndf_train = df_train.drop(columns = columns_to_drop)\ndf_test = df_test.drop(columns = columns_to_drop)","33e7ac35":"print(df_train.shape)\nprint(df_test.shape)","b58fa3e9":"# Remove duplicates from training and test data\ndf_train = df_train.drop_duplicates(subset=None, keep='first', inplace=False).copy()\ndf_test = df_test.drop_duplicates(subset=None, keep='first', inplace=False).copy()\nprint(df_train.shape)\nprint(df_test.shape)","089a55cb":"# Identifying the datatypes of the variables\ncontinous_attrib = df_train.select_dtypes(include=np.number).columns\nbinary_attrib = df_train[['apache_post_operative', 'arf_apache', 'cirrhosis', 'diabetes_mellitus', 'immunosuppression',\n'hepatic_failure', 'leukemia', 'lymphoma', 'solid_tumor_with_metastasis', 'gcs_unable_apache',\n'intubated_apache', 'ventilated_apache']].columns\ncontinous_attrib = continous_attrib.drop(binary_attrib)\ncategorical_attrib =   df_train.select_dtypes(include=['object']).columns\nselected_attributes = list(set(continous_attrib)) + list(set(categorical_attrib)) + list(set(binary_attrib))\nprint(len(selected_attributes))\ndf_train,y_train = df_train[selected_attributes],target\ndf_test= df_test[selected_attributes]\ndf_train.head()\ndf_test.head()\n","33b0f4a4":"#Imputing features in train and test data\nfrom sklearn.impute import SimpleImputer\n# Replacing NAN values in numerical columns with mean\n\nimputer_Num = SimpleImputer(missing_values=np.nan, strategy='mean')\nimputer_Cat_Bin = SimpleImputer(missing_values=np.nan, strategy='most_frequent')\n\ncontinous_attrib = list(set(continous_attrib)) \ncategorical_attrib = list(set(categorical_attrib)) \nbinary_attrib = list(set(binary_attrib))\n\n#One hot coding for train data\ndf_Cat_attrib = pd.DataFrame(df_train[categorical_attrib])\ndf_Cat_OneHotCoded = pd.get_dummies(df_Cat_attrib)\ndf_Cat_OneHotCoded.head()\n\n# Fit and transform to the parameters\ndf_imputed_Num = pd.DataFrame(imputer_Num.fit_transform(df_train[continous_attrib]))\ndf_imputed_Num.columns = continous_attrib\n\ndf_imputed_Cat = pd.DataFrame(imputer_Cat_Bin.fit_transform(df_Cat_OneHotCoded[df_Cat_OneHotCoded.columns]))\ndf_imputed_Cat.columns = df_Cat_OneHotCoded.columns\n\ndf_imputed_binary = pd.DataFrame(imputer_Num.fit_transform(df_train[binary_attrib]))\ndf_imputed_binary.columns = binary_attrib\n\ntrain_imputed_df = pd.concat([df_imputed_Num,df_imputed_Cat,df_imputed_binary], axis=1).dropna()\n\n\n#One hot coding for test data\ndf_Cat_attrib = pd.DataFrame(df_test[categorical_attrib])\ndf_Cat_OneHotCoded = pd.get_dummies(df_Cat_attrib)\ndf_Cat_OneHotCoded.head()\n\n# Fit and transform to the parameters\ndf_imputed_Num = pd.DataFrame(imputer_Num.fit_transform(df_test[continous_attrib]))\ndf_imputed_Num.columns = continous_attrib\n\ndf_imputed_Cat = pd.DataFrame(imputer_Cat_Bin.fit_transform(df_Cat_OneHotCoded[df_Cat_OneHotCoded.columns]))\ndf_imputed_Cat.columns = df_Cat_OneHotCoded.columns\n\ndf_imputed_binary = pd.DataFrame(imputer_Num.fit_transform(df_test[binary_attrib]))\ndf_imputed_binary.columns = binary_attrib\n\ntest_imputed_df = pd.concat([df_imputed_Num,df_imputed_Cat,df_imputed_binary], axis=1).dropna()\n\ntrain_imputed_df.head(5)\ntest_imputed_df.head(5)\n\n","0df77ce0":"# The number of features in the train and test data are different.  Concat and use the get_dummies to even \n# them out such that training and test data have the same number of attributes\ntrain_objs_num = len(train_imputed_df)\ndataset = pd.concat(objs=[train_imputed_df, test_imputed_df], axis=0,sort='False')\ntrain_X_df = dataset[:train_objs_num].copy()\ntest_X_df = dataset[train_objs_num:].copy()\n\ntrain_X_df = train_X_df.round(decimals=2)\ntest_X_df = test_X_df.round(decimals=2)\n\ntrain_X_df.columns.values\ntest_X_df.head(5)","b14d54c3":"#Distribution of train data\nfig = plt.figure(figsize=(20,30))\nfor i in range(int(len(train_X_df.columns)-1)):\n    fig.add_subplot(40,5,i+1)\n    sns.distplot(train_X_df.iloc[:,i+1].dropna())\n    plt.xlabel(train_X_df.columns[i])\nplt.show()","a8247496":"threshold = 0.9\n# Absolute value correlation matrix - train data\ncorr_matrix_train = train_X_df.corr().abs()\ncorr_matrix_train.head()\n\n# test data\ncorr_matrix_test = test_X_df.corr().abs()\ncorr_matrix_test.head()","c9c4fa79":"# Upper triangle of correlations - train data\nupper = corr_matrix_train.where(np.triu(np.ones(corr_matrix_train.shape), k=1).astype(np.bool))\n# Select columns with correlations above threshold\nto_drop = [column for column in upper.columns if any(upper[column] > 0.95)]\nprint(to_drop)\nprint('There are %d columns to remove.' % (len(to_drop)))\n#Drop the columns with high correlations\ntrain_X_df = train_X_df.drop(columns = to_drop)","aa2f06dc":"# Upper triangle of correlations - test data\nupper = corr_matrix_test.where(np.triu(np.ones(corr_matrix_test.shape), k=1).astype(np.bool))\n# Select columns with correlations above threshold\nto_drop = [column for column in upper.columns if any(upper[column] > 0.95)]\nprint(to_drop)\nprint('There are %d columns to remove.' % (len(to_drop)))\n#Drop the columns with high correlations\ntest_X_df = test_X_df.drop(columns = to_drop)\ntrain_X_df = train_X_df.drop(columns = 'd1_hematocrit_max')","1fe96b85":"print(train_X_df.shape)\nprint(test_X_df.shape)\n","24f3c1bc":"from sklearn.model_selection import train_test_split\nX_train, X_eval, Y_train, Y_eval = train_test_split(train_X_df, y_train, test_size=0.15, stratify=y_train)\nX_train.shape, X_eval.shape, Y_train.shape, Y_eval.shape","2aa2a453":"from sklearn.model_selection import  GridSearchCV, RandomizedSearchCV\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import KFold\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import StratifiedKFold, KFold\n\nfrom sklearn.metrics import (confusion_matrix, precision_recall_curve, auc,\n                             roc_curve, recall_score, classification_report, f1_score,\n                             precision_recall_fscore_support, roc_auc_score)\n\ngkf = KFold(n_splits=3, shuffle=True, random_state=42).split(X=X_train, y=Y_train)\nfit_params_of_xgb = {\n    \"early_stopping_rounds\":100, \n    \"eval_metric\" : 'auc', \n    \"eval_set\" : [(X_eval,Y_eval)],\n    # 'callbacks': [lgb.reset_parameter(learning_rate=learning_rate_010_decay_power_099)],\n    'verbose': 500,\n}\nparams = {\n    'booster': [\"gbtree\"],\n    'learning_rate': [0.01],\n    'n_estimators': [3000],#range(1000, 2000, 3000),#range(100, 500, 100)\n    'min_child_weight': [1],#1\n    'gamma': [0],\n    'subsample': [0.4],\n    'colsample_bytree': [0.8],\n    'max_depth': [4],\n    \"scale_pos_weight\": [1],\n    \"reg_alpha\":[1],#0.08\n}\nxgb_estimator = XGBClassifier(\n    objective='binary:logistic',\n    silent=True,\n)\n\ngsearch = GridSearchCV(\n    estimator=xgb_estimator,\n    param_grid=params,\n    scoring='roc_auc',\n    n_jobs=-1,\n    cv=gkf, verbose=3\n)\nxgb_model = gsearch.fit(X=X_train, y=Y_train, **fit_params_of_xgb)\ngsearch.best_params_, gsearch.best_score_","50c94bcb":"fit_params_of_xgb = {\n    \"early_stopping_rounds\":100, \n    \"eval_metric\" : 'auc', \n    \"eval_set\" : [(X_eval,Y_eval)],\n    # 'callbacks': [lgb.reset_parameter(learning_rate=learning_rate_010_decay_power_099)],\n    'verbose': 500\n}\nxgb_estimator = XGBClassifier(\n    objective='binary:logistic',\n    silent=True,    \n    booster= \"gbtree\",\n    learning_rate= 0.01,\n    n_estimators=3000,#range(1000, 2000, 3000),#range(100, 500, 100)\n    min_child_weight= 1,#1\n    gamma= 0,\n    subsample= 0.4,\n    colsample_bytree= 0.8,\n    max_depth= 4,\n    scale_pos_weight=1,\n    reg_alpha=1,#0.08\n)\nxgb_estimator.fit(X=X_train, y=Y_train, **fit_params_of_xgb)\ngsearch.best_params_, gsearch.best_score_","32aec827":"Y_pred = xgb_estimator.predict(test_X_df)\nsubmission = pd.DataFrame({\n        \"encounter_id\": test_df[\"encounter_id\"],\n        \"hospital_death\": Y_pred\n    })\nsubmission.to_csv(\"hospital_death.csv\",index=False)","b27bb1b0":"Acknowledgement :\n\nFeature selection:\n> https:\/\/www.kaggle.com\/usharengaraju\/wids2020-featureselectiontechniques\nFor Hyperparameter tuning\nhttps:\/\/www.kaggle.com\/kuldeep7688\/xgboost-parameter-tuning-baseline\n"}}