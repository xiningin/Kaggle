{"cell_type":{"c4010c2b":"code","e79baff1":"code","ff157a63":"code","e4edc61a":"code","e62c605d":"code","b2dcb1d3":"code","a1abe598":"code","67233ba1":"code","7c879295":"code","c8648162":"code","22e5ae04":"code","490737d2":"code","7e3c0868":"code","55bcff26":"code","1b641f28":"code","bb88178c":"code","55bbe6df":"code","ec2030d0":"code","46fc2019":"code","75405b22":"code","3a57c690":"code","bc32eecf":"code","b136b891":"code","16a2adf3":"code","2147b704":"code","3948d537":"code","0159442b":"code","09064898":"code","a176c98b":"code","2e7f7a96":"code","c7f1e4e8":"code","9822ef8c":"code","7637d03a":"code","39d35991":"code","bb0d9b21":"code","94628d0c":"code","c36cd623":"code","77ae1e7c":"code","7a67652b":"code","2a063d34":"code","f702e886":"code","e41815cc":"code","c6eb3fc9":"code","8e446e93":"code","9c9baf4d":"code","cf3ec520":"code","c1699cee":"code","19879a50":"code","9007844a":"code","eea31bfd":"code","a8be3e18":"code","9a18fe63":"code","dfffee87":"code","269187d9":"code","cce8dca2":"code","b1d252d3":"code","0a685856":"code","6d8f6a0c":"code","b047378d":"code","3938ebf0":"code","37760dc8":"code","e8513034":"code","614443a2":"code","04b1ed4d":"code","40f878b4":"code","69d55532":"code","c022d342":"code","3ab26730":"code","7a33f990":"code","39922e7c":"code","6a916795":"code","b3b28da2":"code","38786d45":"code","1bff37ec":"code","c6b758ce":"code","2d1a2281":"code","3567af47":"code","78013329":"code","c8e2baac":"code","9ae2aa9e":"code","e2a3fd7f":"code","9689901e":"code","7469cf30":"code","4978566d":"code","27e9c86f":"markdown","4514b510":"markdown","bcbcf43f":"markdown","25fb63bc":"markdown","ff73be0b":"markdown","652169d0":"markdown","c266b5c3":"markdown","85ca160c":"markdown","b00c8451":"markdown","c9fe6c31":"markdown","e1acf5b6":"markdown","52dacb85":"markdown","e8e857d9":"markdown","67748f81":"markdown","55e1abb7":"markdown","0a958a0d":"markdown","b7be02c8":"markdown","c1b025ca":"markdown","28e14aec":"markdown","430b7489":"markdown","07653f2a":"markdown","c5dacc70":"markdown","7af737d3":"markdown","8d132b1c":"markdown","1a23ad69":"markdown","12f5cb29":"markdown","618e0a7d":"markdown","8e41317b":"markdown","4223b370":"markdown","891cb366":"markdown","3a5c28c8":"markdown","8003a3b2":"markdown","4774821e":"markdown","4a3c3d25":"markdown","5172c73e":"markdown","5d483327":"markdown","316c9e80":"markdown","9312f7d2":"markdown","79ac6735":"markdown","fbe65f49":"markdown","01ae874a":"markdown","967ac18e":"markdown","7e888835":"markdown","ba2620fc":"markdown","d5865d59":"markdown","de9151d5":"markdown","69efd316":"markdown","f5f30d73":"markdown","2626b776":"markdown","3d474275":"markdown","d2a690d8":"markdown","523483b6":"markdown","08789701":"markdown","ffeae83e":"markdown","41a950fa":"markdown","33c0523c":"markdown","aae279f4":"markdown","3ebded8a":"markdown","4863bedf":"markdown","ad9b3d97":"markdown"},"source":{"c4010c2b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom scipy import stats\nfrom scipy.stats import norm, skew\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","e79baff1":"df = pd.read_csv('\/kaggle\/input\/pima-indians-diabetes-database\/diabetes.csv')\ndf.head()","ff157a63":"df.info()","e4edc61a":"df.describe()","e62c605d":"numericals = ['Pregnancies','Glucose','BloodPressure','SkinThickness','Insulin','BMI','DiabetesPedigreeFunction','Age']\ntarget = ['Outcome']","b2dcb1d3":"plt.figure(figsize=(16,4))\nfor i in range(0, len(numericals)):\n    plt.subplot(1,8, i+1)\n    sns.boxplot(x=df[numericals[i]])\n    plt.tight_layout()","a1abe598":"plt.figure(figsize=(16,10))\nfor i in range(0, len(numericals)):\n    plt.subplot(2,4, i+1)\n    sns.distplot(df[numericals[i]])\n    plt.tight_layout","67233ba1":"count_classes = pd.value_counts(df['Outcome'], sort=True)\ncount_classes.plot(kind='bar', rot=0)\nplt.title('Diabetes Outcome Distribution')\nplt.xlabel('Class')\nplt.ylabel('Frequency')","7c879295":"sns.heatmap(df.corr(), annot=True,  fmt='.2f')\nplt.show()","c8648162":"sns.pairplot(df, diag_kind='kde')","22e5ae04":"sns.pairplot(df, diag_kind='kde', hue='Outcome')","490737d2":"X = df[numericals]\ny = df[target]\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.3, random_state=0) #Using 70-30 Rule\nX_train.shape","7e3c0868":"from sklearn.linear_model import LogisticRegression\n\nlogReg = LogisticRegression(random_state=0, max_iter=400)\nlogReg.fit(X_train, y_train)\ny_predicted = logReg.predict(X_test)\ny_predicted_proba = logReg.predict_proba(X_test)","55bcff26":"from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, recall_score,  roc_auc_score, precision_score\nprint('\\nconfustion matrix') # generate the confusion matrix\nprint(confusion_matrix(y_test, y_predicted))\n\nprint('\\n======================\\nClassification Report:\\n') # generate the confusion matrix\nprint(classification_report(y_test, y_predicted))","1b641f28":"from sklearn.neighbors import KNeighborsClassifier\n\nknn = KNeighborsClassifier(n_neighbors=5) #using default value\nknn.fit(X_train, y_train)\ny_predicted = knn.predict(X_test)","bb88178c":"print('\\nconfustion matrix') # generate the confusion matrix\nprint(confusion_matrix(y_test, y_predicted))\n\n\nprint('\\n======================\\nClassification Report:\\n') # generate the confusion matrix\nprint(classification_report(y_test, y_predicted))","55bbe6df":"from sklearn.tree import DecisionTreeClassifier\n\ndt = DecisionTreeClassifier(random_state=0) #using entropy as calculation\ndt.fit(X_train, y_train)\ny_predicted = dt.predict(X_test)","ec2030d0":"print('\\nconfustion matrix') # generate the confusion matrix\nprint(confusion_matrix(y_test, y_predicted))\n\nprint('\\n======================\\nClassification Report:\\n') # generate the confusion matrix\nprint(classification_report(y_test, y_predicted))","46fc2019":"from scipy.stats import norm\ndf_p1 = df #df_p1 to separate data between RAW and Preprocessed.","75405b22":"ax = sns.distplot(df_p1['Pregnancies'], color=\"y\")\nplt.title('skew: {}'.format(skew(df_p1['Pregnancies'])))","3a57c690":"#Using IQR since pregnancies data is skewed.\nQ1 = df_p1['Pregnancies'].quantile(0.25)\nQ3 = df_p1['Pregnancies'].quantile(0.75)\nIQR = Q3-Q1\nlow_limit = Q1 - (1.5 * IQR)\nhigh_limit = Q3 + (1.5 * IQR)\nfiltered_entries = ((df_p1['Pregnancies'] >= low_limit) & (df_p1['Pregnancies'] <= high_limit))\ndf_p1 = df_p1[filtered_entries]\nprint('Q1=',Q1,'Q3=',Q3,'IQR=',IQR,'low_limit=',low_limit,'high_limit=',high_limit)\n\n#plot the new data after outliers removed.\nax = sns.distplot(df_p1['Pregnancies'], color=\"y\")\nplt.title('skew: {}'.format(skew(df_p1['Pregnancies'])))","bc32eecf":"ax = sns.distplot(df_p1['Glucose'], color=\"y\")\nplt.title('skew: {}'.format(skew(df_p1['Glucose'])))","b136b891":"# using Z-Score as the data distribution is normal\nfrom scipy import stats\nz_scores = np.abs(stats.zscore(df_p1['Glucose']))\nfiltered_entries = (z_scores < 3)\ndf_p1 = df_p1[filtered_entries]\n\nax = sns.distplot(df_p1['Glucose'], color=\"y\")\nplt.title('skew: {}'.format(skew(df_p1['Glucose'])))","16a2adf3":"ax = sns.distplot(df_p1['BloodPressure'], color=\"y\")\nplt.title('skew: {}'.format(skew(df_p1['BloodPressure'])))","2147b704":"# using Z-Score as the data distribution is normal\nfrom scipy import stats\nz_scores = np.abs(stats.zscore(df_p1['BloodPressure']))\nfiltered_entries = (z_scores < 3)\ndf_p1 = df_p1[filtered_entries]\nax = sns.distplot(df_p1['BloodPressure'], color=\"y\")\nplt.title('skew: {}'.format(skew(df_p1['BloodPressure'])))","3948d537":"ax = sns.distplot(df['SkinThickness'], color=\"y\")\nplt.title('skew: {}'.format(skew(df_p1['SkinThickness'])))","0159442b":"# using Z-Score as the data distribution is normal\nfrom scipy import stats\nz_scores = np.abs(stats.zscore(df_p1['SkinThickness']))\nfiltered_entries = (z_scores < 3)\ndf_p1 = df_p1[filtered_entries]\nax = sns.distplot(df_p1['SkinThickness'], color=\"y\")\nplt.title('skew: {}'.format(skew(df_p1['SkinThickness'])))","09064898":"ax = sns.distplot(df_p1['Insulin'], color=\"y\")\nplt.title('skew: {}'.format(skew(df_p1['Insulin'])))","a176c98b":"# Using IQR as data distribution is skewed\nQ1 = df_p1['Insulin'].quantile(0.25)\nQ3 = df_p1['Insulin'].quantile(0.75)\nIQR = Q3-Q1\nlow_limit = Q1 - (1.5 * IQR)\nhigh_limit = Q3 + (1.5 * IQR)\nprint('Q1=',Q1,'Q3=',Q3,'IQR=',IQR,'low_limit=',low_limit,'high_limit=',high_limit)\nfiltered_entries = ((df_p1['Insulin'] >= low_limit) & (df_p1['Insulin'] <= high_limit))\ndf_p1 = df_p1[filtered_entries]\nax = sns.distplot(df_p1['Insulin'], color=\"y\")\nplt.title('skew: {}'.format(skew(df_p1['Insulin'])))","2e7f7a96":"ax = sns.distplot(df_p1['BMI'], color=\"y\")\nplt.title('skew: {}'.format(skew(df_p1['BMI'])))","c7f1e4e8":"# using Z-Score as the data distribution is normal\nfrom scipy import stats\nz_scores = np.abs(stats.zscore(df_p1['BMI']))\nfiltered_entries = (z_scores < 3)\ndf_p1 = df_p1[filtered_entries]\nax = sns.distplot(df_p1['BMI'], color=\"y\")\nplt.title('skew: {}'.format(skew(df_p1['BMI'])))","9822ef8c":"ax = sns.distplot(df_p1['DiabetesPedigreeFunction'], color=\"y\")\nplt.title('skew: {}'.format(skew(df_p1['DiabetesPedigreeFunction'])))","7637d03a":"# Using IQR as data distribution is skewed\nQ1 = df_p1['DiabetesPedigreeFunction'].quantile(0.25)\nQ3 = df_p1['DiabetesPedigreeFunction'].quantile(0.75)\nIQR = Q3-Q1\nlow_limit = Q1 - (1.5 * IQR)\nhigh_limit = Q3 + (1.5 * IQR)\nprint('Q1=',Q1,'Q3=',Q3,'IQR=',IQR,'low_limit=',low_limit,'high_limit=',high_limit)\nfiltered_entries = ((df_p1['DiabetesPedigreeFunction'] >= low_limit) & (df_p1['DiabetesPedigreeFunction'] <= high_limit))\ndf_p1 = df_p1[filtered_entries]\nax = sns.distplot(df_p1['DiabetesPedigreeFunction'], color=\"y\")\nplt.title('skew: {}'.format(skew(df_p1['DiabetesPedigreeFunction'])))","39d35991":"ax = sns.distplot(df_p1['Age'], color=\"y\")\nplt.title('skew: {}'.format(skew(df_p1['Age'])))","bb0d9b21":"# Using IQR as data distribution is skewed\nQ1 = df_p1['Age'].quantile(0.25)\nQ3 = df_p1['Age'].quantile(0.75)\nIQR = Q3-Q1\nlow_limit = Q1 - (1.5 * IQR)\nhigh_limit = Q3 + (1.5 * IQR)\nprint('Q1=',Q1,'Q3=',Q3,'IQR=',IQR,'low_limit=',low_limit,'high_limit=',high_limit)\nfiltered_entries = ((df_p1['Age'] >= low_limit) & (df_p1['Age'] <= high_limit))\ndf_p1 = df_p1[filtered_entries]\nax = sns.distplot(df_p1['Age'], color=\"y\")\nplt.title('skew: {}'.format(skew(df_p1['Age'])))","94628d0c":"plt.figure(figsize=(16,10))\nfor i in range(0, len(numericals)):\n    plt.subplot(2,4, i+1)\n    sns.distplot(df[numericals[i]], color=\"b\")\n    plt.tight_layout\n    plt.title('Before Outliers Removal')","c36cd623":"plt.figure(figsize=(16,10))\nfor i in range(0, len(numericals)):\n    plt.subplot(2,4, i+1)\n    sns.distplot(df_p1[numericals[i]], color=\"y\")\n    plt.tight_layout\n    plt.title('after Outliers Removal')","77ae1e7c":"print(df.shape)\nprint(df_p1.shape)","7a67652b":"df_p2 = df_p1\nprint(df_p2['Outcome'].value_counts())\nx = df_p2[['Pregnancies','Glucose','BloodPressure','SkinThickness','Insulin','BMI','DiabetesPedigreeFunction','Age']]\ny = df_p2['Outcome']","2a063d34":"from imblearn import over_sampling\nx_over, y_over = over_sampling.RandomOverSampler().fit_resample(x,y)\nprint(pd.Series(y_over).value_counts())","f702e886":"df_over = pd.DataFrame(x_over, columns= df[numericals].columns)\ntarget_over = pd.DataFrame(y_over, columns= df[target].columns)\ndf_over.describe()","e41815cc":"count_classes = pd.value_counts(target_over['Outcome'], sort=True)\ncount_classes.plot(kind='bar', rot=0)\nplt.title('Diabetes Outcome Distribution')\nplt.xlabel('Class')\nplt.ylabel('Frequency')","c6eb3fc9":"from sklearn.preprocessing import StandardScaler","8e446e93":"Std = StandardScaler()\ndfX = pd.DataFrame(Std.fit_transform(df_over),\n        columns=['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin',\n       'BMI', 'DiabetesPedigreeFunction', 'Age'])","9c9baf4d":"dfX.describe()","cf3ec520":"X = dfX[['Pregnancies','Glucose','BloodPressure','SkinThickness','Insulin','BMI','DiabetesPedigreeFunction','Age']]\ny = target_over['Outcome']\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.3, random_state=0)\nX_train.shape","c1699cee":"from sklearn.linear_model import LogisticRegression\n\nlogReg = LogisticRegression(random_state=0, max_iter=400)\nlogReg.fit(X_train, y_train)\ny_predicted = logReg.predict(X_test)\ny_predicted_proba = logReg.predict_proba(X_test)","19879a50":"from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, recall_score,  roc_auc_score, precision_score\nprint('\\nconfustion matrix') # generate the confusion matrix\nprint(confusion_matrix(y_test, y_predicted))\n\nprint('\\n======================\\nClassification Report:\\n') # generate the confusion matrix\nprint(classification_report(y_test, y_predicted))","9007844a":"from sklearn.neighbors import KNeighborsClassifier\n\nknn = KNeighborsClassifier(n_neighbors=5) #using default value\nknn.fit(X_train, y_train)\ny_predicted = knn.predict(X_test)\nprint(y_predicted)","eea31bfd":"print('\\nconfustion matrix') # generate the confusion matrix\nprint(confusion_matrix(y_test, y_predicted))\n\nprint('\\n======================\\nClassification Report:\\n') # generate the confusion matrix\nprint(classification_report(y_test, y_predicted))","a8be3e18":"X = df_over[['Pregnancies','Glucose','BloodPressure','SkinThickness','Insulin','BMI','DiabetesPedigreeFunction','Age']]\ny = target_over['Outcome']\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.3, random_state=0)\nX_train.shape","9a18fe63":"from sklearn.tree import DecisionTreeClassifier\n\ndt = DecisionTreeClassifier(random_state=0) #using entropy as calculation\ndt.fit(X_train, y_train)\ny_predicted = dt.predict(X_test)\nprint(y_predicted)","dfffee87":"print('\\nconfustion matrix') # generate the confusion matrix\nprint(confusion_matrix(y_test, y_predicted))\n\nprint('\\n======================\\nClassification Report:\\n') # generate the confusion matrix\nprint(classification_report(y_test, y_predicted))","269187d9":"from sklearn.model_selection import GridSearchCV","cce8dca2":"param_grid = [    \n    {'penalty' : ['l1', 'l2', 'elasticnet', 'none'],\n    'C' : np.logspace(-4, 4, 20),\n    'solver' : ['lbfgs','newton-cg','liblinear','sag','saga'],\n    'max_iter' : [100, 1000,2500, 5000]\n    }\n]","b1d252d3":"grid_search = GridSearchCV(logReg, param_grid, cv=5, verbose=True, n_jobs=-1)\nbest_model = grid_search.fit(X_train,y_train)","0a685856":"best_model.best_params_","6d8f6a0c":"accuracy = best_model.best_score_\naccuracy","b047378d":"y_pred = best_model.predict(X_test)\nprint(classification_report(y_test,y_pred))","3938ebf0":"from sklearn.model_selection import RandomizedSearchCV","37760dc8":"random_search = RandomizedSearchCV(logReg, param_grid, cv=5, verbose=True, n_jobs=-1)","e8513034":"best_model = random_search.fit(X_train,y_train)","614443a2":"best_model.best_params_","04b1ed4d":"accuracy = best_model.best_score_\naccuracy","40f878b4":"y_pred = best_model.predict(X_test)\nprint(classification_report(y_test,y_pred))","69d55532":"param_grid = [    \n    {'n_neighbors':[5,6,7,8,9,10,11],\n     'leaf_size':[1,2,3,5],\n     'weights':['uniform', 'distance'],\n     'algorithm':['auto', 'ball_tree','kd_tree','brute']\n    }\n]","c022d342":"grid_search = GridSearchCV(knn, param_grid, cv=5, verbose=True, n_jobs=-1)\nbest_model = grid_search.fit(X_train,y_train)","3ab26730":"best_model.best_params_","7a33f990":"accuracy = best_model.best_score_\naccuracy","39922e7c":"y_pred = best_model.predict(X_test)\nprint(classification_report(y_test,y_pred))","6a916795":"random_search = RandomizedSearchCV(knn, param_grid, cv=5, verbose=True, n_jobs=-1)","b3b28da2":"best_model = random_search.fit(X_train,y_train)","38786d45":"best_model.best_params_","1bff37ec":"accuracy = best_model.best_score_\naccuracy","c6b758ce":"y_pred = best_model.predict(X_test)\nprint(classification_report(y_test,y_pred))","2d1a2281":"param_grid = [    \n    {'criterion':['gini','entropy'],\n     'splitter':['best','random'],\n     'max_features':['auto','sqrt','log2']\n    }\n]","3567af47":"grid_search = GridSearchCV(dt, param_grid, cv=5, verbose=True, n_jobs=-1)\nbest_model = grid_search.fit(X_train,y_train)","78013329":"best_model.best_params_","c8e2baac":"accuracy = best_model.best_score_\naccuracy","9ae2aa9e":"y_pred = best_model.predict(X_test)\nprint(classification_report(y_test,y_pred))","e2a3fd7f":"random_search = RandomizedSearchCV(dt, param_grid, cv=5, verbose=True, n_jobs=-1)\nbest_model = random_search.fit(X_train,y_train)","9689901e":"best_model.best_params_","7469cf30":"best_model.best_score_","4978566d":"y_pred = best_model.predict(X_test)\nprint(classification_report(y_test,y_pred))","27e9c86f":"## DTree (Gini) \n\nRefitting the X as we don't want to have DTree using standarization feature scaling","4514b510":"### Evaluation","bcbcf43f":"### Fit & Predict","25fb63bc":"## Split Train Test Data","ff73be0b":"### Univariate Analysis Summary\n> - According to boxplot visualization, we have Outliers in every indepdent features where some are lot and some only a few.\n> - According to distribution visualization, we see that there are only 2 features haivng a normal distribution where other features is skewed. This might impact how we will deal with the pre-processing later when we have to remove the Outliers.\n> - According to the bar plot above, our target data is imbalance towards to detected as not diabetes. This might impact on our ML evaluation, therefore during pre-processing we can use oversampling so we can make data more balance for modeling. ","652169d0":"### Fit & Predict","c266b5c3":"### Feature Scaling\n\nAs we have several different measurement unit in the independent features, therefore we're going to scale it using standarization and use it on Logistic and KNN as it does not really affect DTree algorithm.","85ca160c":"### Statistical Summary","b00c8451":"# Evaluation Summary\n---\nSo far from the evaluation, KNN with GridSearch Hyperparameter tuning develop outstanding ML model to detect diabetes wether it is negative or positive diabetes compared to the other 2 in this dataset.\n\nHowever Logistic Regression and Decision Tree is also still a great model for this dataset.\n\nIn the future I might update the overall evaluation of this notebook with ROCAUC and K-Cross Validation.","c9fe6c31":"### Evaluation","e1acf5b6":"## Oversampling for Imbalanced Data\n\nTo deal with majority data in the negative diabetes category, we're going to do oversampling to make distribution more even","52dacb85":"## Logistic Regression Evaluation\n\n> - We're having a good start with 0.78 accuracy in overall in the Logistic Regression. However as described in the introduction, the data is biased toward the negative diabetes making evaluation unfair. \n> - Checking on Recall score, if the patient that actually diabetes, the model only label as positive diabetes at 0.53 rate. However if patient actually negative and labeling it as negative the model perform at 0.90 rate. The model is still very confused to label actual diabetes patient as positive diabetes and as well as make a stronger assumption that the dataset is biased to negative diabetes.\n> - Let's check on the Precision, when we predict negative and actually negative, the model perform good at 0.80 and as well predicting positive and actually positif at 0.71. The precision have a great evaluation here.\n> - at F1-Score we had averaged 0.85 rate at the negative diabetes but still low at 0.60 averaged in positive diabetes. ","e8e857d9":"# Hyperparameter Tuning\n---","67748f81":"### Logistic Regression GridSearch","55e1abb7":"### Fit & Predict","0a958a0d":"### Column, NULL Values, DTypes\nSee if the dataset count did not match with number of row and as well if any of column have mismatch Dtype","b7be02c8":"- - -\n# Modeling w\/o Pre-processing (Raw Dataset)\n\nDevelop the first ML model without any pre-processing. The goal is to compare the model performance raw data and pre-processed data later.","c1b025ca":"## Decision Tree (Gini) Evaluation\n\n> - Pre-Processing make Decision Tree exceptionally good, overall the model have better accuracy at 0.85 from 0.72 and other positive scoring on Recall and Precision is as well bump up hugely.\n> - Recall got bumped up from 0.59 to 0.89 in positive scoring as well as precision went up from 0.56 to 0.84\n> - The F1 Score average at scoring positive diabetes is also improved from 0.58 to 0.85\n> - Overall the Pre-Processing make the Decision Tree model really good at predicting both negative positive diabetes.","28e14aec":"## KNN","430b7489":"### KNN RandomSearch","07653f2a":"## Pairplot + Hue","c5dacc70":"## KNN Evaluation\n> - We're having a good start with evaluation at 0.78 accuracy in overall. However same as before the data is biased toward the negative diabetes making evaluation unfair. \n> - Checking on Recall score, if the patient that actually diabetes, the model only label as positive diabetes at 0.53 rate. However if patient actually negative and labeling it as negative the model perform at 0.85 rate. The model is still very confused to label actual diabetes patient as positive diabetes and as well as make a stronger assumption that the dataset is biased to negative diabetes.\n> - Let's check on the Precision, when we predict negative and actually negative, the model perform good at 0.80 and as well predicting positive and actually positif at 0.63. Comparing with Logistic Regression, KNN have worst evaluation at the precision. \n> - at F1-Score, this KNN model averaged lower than Logistic Regression at scoring positive and negative diabetes.","7af737d3":"## Pairplot","8d132b1c":"> ## Feature Categorization\n> \n> Based on all summary above, all independent features will be selected and will be categorized as numericals.","1a23ad69":"## KNN","12f5cb29":"### KNN GridSearch","618e0a7d":"## Split Train Test Data","8e41317b":"# Multivariate Analysis\n---\n\nLet's analyze how each columns relationship strength to each other by using Correlation and Pairplot!","4223b370":"## Dealing with Outliers\n\nWe're going to use a boxplot in our Univariate analysis to detect outliers visualization","891cb366":"## Descriptive Analysis Summary\n> - According to df.info() above, there are no null data that we have to deal with later in pre-processing.\n> - Dtype on every features is make sense hence no need to change it in the pre-processing.\n> - Based on statistical summary above, there is data issue with people registered with 0 Glucose, 0 Blood Pressure, 0 Skin Thickness. This might be a wrong data input. \n> - There are a great outlier for example a data with 17 pregancies. Other than that we can see several data with Outliers that will be processed in the next step.","3a5c28c8":"### Evaluation","8003a3b2":"## KNN Pre-Processing Evaluation\n\n> - Pre-Processing also make good improvement on KNN, overall the model have better accuracy at 0.77 from 0.72 and other positive scoring on Recall and Precision is as well bump up hugely.\n> - Recall got bumped up from 0.59 to 0.74 in positive scoring as well as precision went up from 0.56 to 0.80\n> - The F1 Score average at scoring positive diabetes is also improved from 0.58 to 0.77\n> - Overall the Pre-Processing make the KNN model more good at predicting positive diabetes.","4774821e":"### Fit & Predict","4a3c3d25":"# Data Import & Collection\n---","5172c73e":"## Descriptive Analysis\n\n**Dataset Information:**\n> - Pregnancies: Number of times pregnant\n> - Glucose: Plasma glucose concentration a 2 hours in an oral glucose tolerance test\n> - Blood Pressure: Diastolic blood pressure (mm Hg)\n> - Skin Thickness: Triceps skin fold thickness (mm)\n> - Insulin: 2-Hour serum insulin (mu U\/ml)\n> - BMI: Body mass index (weight in kg\/(height in m)^2)\n> - Diabetes pedigree function: Self-Explained\n> - Age: Years\n> - Outcome: 1 is True, 0 is False","5d483327":"### Fit & Predict","316c9e80":"## KNN (Raw Data)","9312f7d2":"## DTree (Gini as default) ","79ac6735":"## Multivariate Analysis Summary\n> - According to heatmap visualization, there are no strong relationship between each independent features hence all features will be selected in Modeling.\n> - According to pairplot visualization, in each independent feature there are just a little pattern showing different clusters.","fbe65f49":"## Logistic Regression (Raw Data)","01ae874a":"## Logistic Regression","967ac18e":"## DTree RandomSearch","7e888835":"## Logistic Regression Pre-Processing Evaluation\n\n> - As a result of several Pre-Processing, overall the model have lower accuracy at 0.75 and also other negative score prediction at Precision, Recall and the f-1 score.\n> - However Pre-Processing really bump up the model performance at the positive prediction sector. Especially on the Recall where it bumped from 0.53 to 0.69. On the Precision it went up to from 0.71 to 0.80\n> - The F1 Score average at scoring positive diabetes is also improved from 0.60 to 0.74\n> - Overall the Pre-Processing make the Logistic Regression model more good at predicting positive diabetes.","ba2620fc":"## Univariate Analysis\n\nLet's analyze data distribution of each column individually!","d5865d59":"## Hyperparameter Tuning Evaluation\n\n> - Both GridSearch and RandomSearch greatly boost KNN Model performance with GridSearch at it best.\n> - We don't see any significance boost yet we saw performance get lowered abit at Logistic Regression and Decision Tree with Hypertuning Parameter.","de9151d5":"## Correlation & Heatmap","69efd316":"## DTree GridSearch","f5f30d73":"- - -\n# Pre-Processing","2626b776":"- - -\n# Modeling After Pre-processing\n\nIn here we're going to try to do modeling first with pre-processing.","3d474275":"## Decision Tree (Gini) Evaluation\n> - The model evaluate at 0.72 accuracy in overall which is lower than 2 other models. However same as before the data is still biased toward the negative diabetes making evaluation unfair. \n> - On Recall, the model is still pretty bad at labeling actual positive diabetes as other 2 models and just good at labeling actual negative diabetes.\n> - On the Precision, the model evaluation is also still bad at predicting positive diabetes but good at predicting negative diabetes. \n> - at F1-Score, this DTree model averaged not great at scoring positive diabetes and just good at scoring negative diabetes.","d2a690d8":"### Evaluation","523483b6":"### Logistic Regression RandomSearch","08789701":"# EDA<a id='1'>\n---","ffeae83e":"### Fit & Predict","41a950fa":"## Logistic Regression","33c0523c":"# Introduction\n---\n\nThis notebook is about learning to compare 3 common ML model in classifier (Logistic Regression, KNN, Dtree) including pre-processing and tuning hyperparameter to get the best result.\n\n**In this dataset, the data incline (imbalanced) more to the patients who are diagnosed as negative** and causing the development of the model become bias that the prediction is more likely to result as negative diabetes.\n\nTherefore, the solution of this dataset is to **create a model that has high sensitivity in detecting** whether the patient is positive or negative diabetes in order to overcome the bias. This kind of model will work best rather than a model that has high accuracy in predicting , yet the sensitivity is low that cause the bias in detecting the diagnose to negative only because the accuracy will count the whole data and sensitivity will count only those who are diagnosed positive diabetes.\n\nBelow are the table of the content:\n- <a href='#1'>1. EDA<\/a>\n- <a href='#2'>2. Raw Data ML Modeling & Evaluation for Benchmark<\/a>\n- <a href='#3'>3. Pre-Processing<\/a>\n    - <a href='#3.1'>3.1 Dealing with Outliers<\/a>\n    - <a href='#3.2'>3.2 Dealing with Normal and Skewed Distribution<\/a>\n    - <a href='#3.3'>3.3 Dealing with Different Units Measurement<\/a>\n- <a href='#4'>4. Pre-Processed Data ML Modeling & Evaluation<\/a>\n- <a href='#5'>5. Hypertuning Parameters with Grid & Random Search<\/a>\n- <a href='#6'>6. Evaluation<\/a>\n\n\nDataset originally from https:\/\/www.kaggle.com\/uciml\/pima-indians-diabetes-database","aae279f4":"### Evaluation","3ebded8a":"### Evaluation","4863bedf":"## Outliers Detection Summary\n> Detect and remove about 126 outliers in dataset","ad9b3d97":"In Pre-Processing part, we're going to use few techniques such as find missing data, duplicated data, outliers, standarization\/normalization, feature encoding and Over\/Undersampling."}}