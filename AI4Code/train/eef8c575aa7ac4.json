{"cell_type":{"95363006":"code","f5642e4e":"code","1b2bca96":"code","172072dc":"code","205a6e96":"code","1e3240d7":"code","0c882377":"code","d21a8a18":"code","bcccce89":"code","a6b4f51b":"code","08bfa07a":"code","b3ce963d":"code","83c452f2":"code","c8c52313":"code","39f3e584":"code","4d0add98":"code","9762930d":"code","026612fc":"code","00e3181f":"code","baf0873f":"code","a6ca6bef":"code","5bea2d1e":"code","c428a931":"code","2aa66da4":"code","68ff4383":"code","17f90126":"code","5086000f":"code","b0ff4427":"code","780872ff":"code","638a3078":"code","4853fa28":"code","a2eb5109":"code","ea3341d2":"code","6e0ca202":"code","8a6dda23":"code","f5b557d7":"code","3424b568":"code","8171e05e":"code","759ac24f":"code","62c3eefd":"code","0ea2dfd5":"code","7406a0fa":"code","02650f63":"code","d7af65fc":"code","d9606470":"code","6984a246":"code","b5c145b4":"code","7dde05f3":"code","2eb5fed5":"code","8dcf8ef4":"code","a79ef75e":"code","53681143":"code","da07054e":"code","5f228aa4":"code","f6f8aebe":"code","afe1710d":"code","131b1eca":"code","5f3e7b37":"code","fe7e0324":"code","e44e5ca3":"code","f60f0614":"code","b2a1bd20":"code","c26f5160":"code","96f34592":"code","90df0250":"code","62c78718":"code","a59d2631":"code","f2027d71":"code","c203113a":"code","19069b78":"code","e428fccf":"code","e8796a25":"code","d5a0a85c":"code","0d0ad165":"code","60c70e70":"code","b3830c85":"code","a8d08067":"code","95aeb70f":"code","bc4b7d51":"code","c1a75c73":"code","6a560632":"code","3c9f35d8":"code","8edb68fd":"code","3b27af7b":"code","1a5ec74f":"markdown","cad6d600":"markdown","cba19163":"markdown","fc55131f":"markdown","63496a2b":"markdown","b0646e00":"markdown","32f47b0d":"markdown","b6b38c3d":"markdown","d540c182":"markdown","6ec75051":"markdown","1107acaa":"markdown","f7db8e4f":"markdown","9e432561":"markdown","769b8783":"markdown","a21efed2":"markdown","f0a8d51d":"markdown","62f56b6e":"markdown","064cecc4":"markdown","969d2ab3":"markdown","46d3de71":"markdown","0f02b141":"markdown","77d701d3":"markdown","220fd3e9":"markdown","89378c03":"markdown","a9830155":"markdown","fcd022b8":"markdown","ec49a8b0":"markdown","59454d3b":"markdown","3ed33841":"markdown","d958d630":"markdown","e5a620c0":"markdown","f9cfd021":"markdown","15e9a019":"markdown","5f6f79e4":"markdown","9b0652a1":"markdown","1f7cd072":"markdown","cfcbb262":"markdown","15a473db":"markdown"},"source":{"95363006":"import pandas as pd\nimport numpy as np","f5642e4e":"df = pd.read_csv('..\/input\/coronary-prediction\/coronary_prediction.csv')","1b2bca96":"df","172072dc":"# Check on null values\ndf.info()","205a6e96":"# checking on the relevance of imputation. If correlation is high with other IVs, may be used for imputation\ndef color(val):\n    if val > 0.7 or val < -0.7:\n        color = 'red'\n    else:\n        color = 'black'\n    return 'color : %s' % color\n    \ndf.corr().style.applymap(color)","1e3240d7":"df.isnull().sum()","0c882377":"df_BPMed = df.groupby(['prevalentStroke','prevalentHyp'])\ndf_BPMed['BPMeds'].sum()","d21a8a18":"chol_col = ['totChol','BMI']\ndf_chol = df[chol_col]\ndf_chol[df_chol['BMI'] == 26]","bcccce89":"df[df['BMI'].isna()]","a6b4f51b":"import matplotlib.pyplot as plt\nimport seaborn as sb\n\nsb.scatterplot(x=df['diaBP'],y=df['BMI'],hue=df['currentSmoker'])","08bfa07a":"df_diabetes = df.groupby(['diabetes'])\ndf_diabetes['glucose'].describe()","b3ce963d":"df['education'].mode()","83c452f2":"# imputation of education level\ndf['education'].fillna(1,inplace=True)","c8c52313":"df_smoker = df.groupby(['currentSmoker'])\ndf_smoker['cigsPerDay'].describe()","39f3e584":"# imputation of cigs per day\ndf['cigsPerDay'].fillna(9999,inplace=True)\nindex = []\nfor i,v in enumerate(df['cigsPerDay']):\n    if df['cigsPerDay'].loc[i] == 9999:\n        if df['currentSmoker'].loc[i] == 1:\n            df['cigsPerDay'].loc[i] = 18\n            index.append(i)\n        else:\n            df['cigsPerDay'].loc[i] = 0\n            index.append(i)\n","4d0add98":"df.loc[index]","9762930d":"# imputation of BPMeds\ndf['BPMeds'].fillna(9999,inplace=True)\nindex = []\nfor i,v in enumerate(df['BPMeds']):\n    if v == 9999:\n        if df['prevalentHyp'].loc[i] == 1:\n            df['BPMeds'].loc[i] = 1\n            index.append(i)\n        else:\n            df['BPMeds'].loc[i] = 0\n            index.append(i)","026612fc":"df.loc[index]","00e3181f":"df.totChol.describe()","baf0873f":"# impute total cholestrol with mean\ndf.totChol.fillna(236,inplace=True)","a6ca6bef":"df.BMI.describe()","5bea2d1e":"# impute BMI with mean\ndf.BMI.fillna(25.8,inplace=True)","c428a931":"df.heartRate.describe()","2aa66da4":"#impute heartrate with mean\ndf.heartRate.fillna(75,inplace=True)","68ff4383":"df.info()","17f90126":"df['glucose'].fillna(9999,inplace=True)\nindex = []\nfor i,v in enumerate(df['glucose']):\n    if v == 9999:\n        if df['diabetes'].loc[i] == 1:\n            df['glucose'].loc[i] = 170\n            index.append(i)\n        else:\n            df['glucose'].loc[i] = 79\n            index.append(i)\n","5086000f":"df_dia = df.loc[index]\ndf_dia[df_dia['diabetes'] == 1]","b0ff4427":"df.info()","780872ff":"df['TenYearCHD'].value_counts()\/4238","638a3078":"plt.figure(figsize=(4,4))\nsb.countplot(df['TenYearCHD'])\nplt.title('Ten Year Risk of Coronary Heart Disease')\nplt.xlabel('84% vs 15%')\n","4853fa28":"count_col = ['male','currentSmoker','BPMeds','prevalentStroke','prevalentHyp','diabetes']\n\nfig, axes = plt.subplots(2,3, figsize=(10,6))\nfig.subplots_adjust(hspace=0.4,wspace=0.8)\n\nfig.suptitle('Countplot',fontsize=16)\n\ni=0\nn=0\n\nfor x in count_col:\n    sb.countplot(x=df[x],hue=df['TenYearCHD'],ax=axes[i,n])\n    \n    if n < 2:\n        n+=1\n    else:\n        n=0\n        i+=1\nplt.show()","a2eb5109":"df_CHD = df.groupby(['TenYearCHD'])\ndf_CHD[count_col].sum()","ea3341d2":"sb.countplot(x=df['education'],hue=df['TenYearCHD'])","6e0ca202":"df_CHD['education'].value_counts()","8a6dda23":"df['education'].value_counts()","f5b557d7":"plt.figure(figsize=(16,6))\nsb.countplot(x=df['age'],hue=df['TenYearCHD'])","3424b568":"df.columns","8171e05e":"num_col = ['BMI','totChol','heartRate','TenYearCHD']\nsb.pairplot(df[num_col],corner=True)","759ac24f":"from sklearn.metrics import accuracy_score\nfrom sklearn.utils import shuffle\n\ndata = shuffle(df,random_state=123)\n\ndiv = int(data.shape[0]\/4)\n\ntrain = data.loc[:3*div+1,:]\ntest = data.loc[3*div+1:,:]\n\ntrain.shape, test.shape","62c3eefd":"test['Simple_mode'] = train['TenYearCHD'].mode()[0]\ntest['Simple_mode'].head()","0ea2dfd5":"simple_mode_accuracy = accuracy_score(test['Simple_mode'],test['TenYearCHD'])\nsimple_mode_accuracy","7406a0fa":"from sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import f1_score, roc_auc_score, confusion_matrix, roc_curve, precision_score, recall_score, precision_recall_curve","02650f63":"from sklearn.metrics import classification_report\ndef print_score(clf, X_train, y_train, X_test, y_test, train=True):\n    if train:\n        pred = clf.predict(X_train)\n        clf_report = pd.DataFrame(classification_report(y_train, pred, output_dict=True))\n        print(\"Train Result:\\n================================================\")\n        print(f\"Accuracy Score: {accuracy_score(y_train, pred) * 100:.2f}%\")\n        print(\"_______________________________________________\")\n        print(f\"CLASSIFICATION REPORT:\\n{clf_report}\")\n        print(\"_______________________________________________\")\n        print(f\"Confusion Matrix: \\n {confusion_matrix(y_train, pred)}\\n\")\n        \n    elif train==False:\n        pred = clf.predict(X_test)\n        clf_report = pd.DataFrame(classification_report(y_test, pred, output_dict=True))\n        print(\"Test Result:\\n================================================\")        \n        print(f\"Accuracy Score: {accuracy_score(y_test, pred) * 100:.2f}%\")\n        print(\"_______________________________________________\")\n        print(f\"CLASSIFICATION REPORT:\\n{clf_report}\")\n        print(\"_______________________________________________\")\n        print(f\"Confusion Matrix: \\n {confusion_matrix(y_test, pred)}\\n\")","d7af65fc":"def threshold_adjust_score(clf, X_train, y_train, X_test, y_test, threshold = 0.5):\n    print(f'Threshold value : {threshold}'.center(50,'-'))\n    train_outcome = clf.predict_proba(X_train)\n    train_pred = train_outcome[:,1]\n    for i in range(0,len(train_pred)):\n        if train_pred[i] > threshold:\n            train_pred[i] = 1\n        else:\n            train_pred[i] = 0\n    clf_report = pd.DataFrame(classification_report(y_train, train_pred, output_dict=True))\n    print(\"Train Result:\\n================================================\")\n    print(f\"Accuracy Score: {accuracy_score(y_train, train_pred) * 100:.2f}%\")\n    print(\"_______________________________________________\")\n    print(f\"CLASSIFICATION REPORT:\\n{clf_report}\")\n    print(\"_______________________________________________\")\n    print(f\"Confusion Matrix: \\n {confusion_matrix(y_train, train_pred)}\\n\")\n        \n    test_outcome = clf.predict_proba(X_test)\n    test_pred = test_outcome[:,1]\n    for i in range(0,len(test_pred)):\n        if test_pred[i] > threshold:\n            test_pred[i] = 1\n        else:\n            test_pred[i] = 0\n    clf_report = pd.DataFrame(classification_report(y_test, test_pred, output_dict=True))\n    print(\"Test Result:\\n================================================\")        \n    print(f\"Accuracy Score: {accuracy_score(y_test, test_pred) * 100:.2f}%\")\n    print(\"_______________________________________________\")\n    print(f\"CLASSIFICATION REPORT:\\n{clf_report}\")\n    print(\"_______________________________________________\")\n    print(f\"Confusion Matrix: \\n {confusion_matrix(y_test, test_pred)}\\n\")\n    \n","d9606470":"df.columns","6984a246":"scaled_data = MinMaxScaler().fit_transform(df)","b5c145b4":"scaled_df = pd.DataFrame(scaled_data,columns = df.columns)","7dde05f3":"scaled_df","2eb5fed5":"scaled_df.describe()","8dcf8ef4":"import sklearn, sys\nprint(sklearn.__version__, np.__version__, sys.version_info)","a79ef75e":"X = df.drop(['TenYearCHD'],axis=1)\ny = df['TenYearCHD']\n\nX_train, X_test, y_train, y_test = train_test_split(X,y,train_size=0.8,random_state=12)\n\nlogreg = LogisticRegression(solver='liblinear')\nlogreg.fit(X_train,y_train)\n\nbm_train_predict = logreg.predict(X_train)\nbm_test_predict = logreg.predict(X_test)\n\nprint_score(logreg,X_train,y_train,X_test,y_test,train=True)\nprint_score(logreg,X_train,y_train,X_test,y_test,train=False)","53681143":"scaled_X = scaled_df.drop(['TenYearCHD'],axis=1)\nscaled_y = scaled_df['TenYearCHD']\n\nX_train,X_test,y_train,y_test = train_test_split(scaled_X,scaled_y,train_size=0.8,random_state=123)\n\nbm_model = LogisticRegression()\nbm_model.fit(X_train,y_train)\n\nbm_train_predict = bm_model.predict(X_train)\nbm_test_predict = bm_model.predict(X_test)\n\nprint_score(logreg,X_train,y_train,X_test,y_test,train=True)\nprint_score(logreg,X_train,y_train,X_test,y_test,train=False)","da07054e":"threshold_adjust_score(bm_model,X_train,y_train,X_test,y_test,threshold=0.4)\nthreshold_adjust_score(bm_model,X_train,y_train,X_test,y_test,threshold=0.3)","5f228aa4":"def plot_precision_recall_vs_threshold(precisions, recalls, thresholds):\n    plt.plot(thresholds, precisions[:-1],\"b--\",label='Precision')\n    plt.plot(thresholds, recalls[:-1],\"g--\",label='Recall')\n    plt.xlabel('Threshold')\n    plt.legend(loc='upper left')\n    plt.title('Precision\/Recalls tradeoff')\n    \nprecisions, recalls, thresholds = precision_recall_curve(y_test, bm_test_predict)\n\nplt.figure(figsize=(15,8))\nplt.subplot(2,2,1)\nplot_precision_recall_vs_threshold(precisions,recalls,thresholds)\n\nplt.subplot(2,2,2)\nplt.plot(precisions,recalls)\nplt.xlabel('Precision')\nplt.ylabel('Recall')\nplt.title('PR Curve: precisions\/recalls tradeoff')","f6f8aebe":"logreg_roc_auc = roc_auc_score(y_test,bm_test_predict)\nfpr, tpr, thresholds = roc_curve(y_test, bm_model.predict_proba(X_test)[:,1])\nplt.figure(figsize=(8,6))\nplt.plot(fpr,tpr,label='Logistic Regression (area = %0.2f)' % logreg_roc_auc)\nplt.plot([0,1],[0,1],'r--')\nplt.xlim([0.0,1.0])\nplt.ylim([0.0,1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic')\nplt.legend(loc='lower right')\n","afe1710d":"shuffled_df = scaled_df.sample(frac=1,random_state=47)\n\nCHD_df = shuffled_df.loc[shuffled_df['TenYearCHD'] == 1]\n\nnon_CHD_df = shuffled_df.loc[shuffled_df['TenYearCHD'] == 0].sample(n=650,random_state=47)\n\nUS_df = pd.concat([CHD_df,non_CHD_df])\n\nUS_df","131b1eca":"US_df.TenYearCHD.value_counts()","5f3e7b37":"X = US_df.drop(['TenYearCHD'],axis=1)\ny = US_df['TenYearCHD']\n\nX_train, X_test, y_train, y_test = train_test_split(X,y,random_state=47)\n\nUS_model = LogisticRegression().fit(X,y)\n\nprint_score(US_model,X_train,y_train,X_test,y_test,train=True)\nprint_score(US_model,X_train,y_train,X_test,y_test,train=False)\n","fe7e0324":"US_test_predict = US_model.predict(X_test)\nlogreg_roc_auc = roc_auc_score(y_test,US_test_predict)\nfpr, tpr, thresholds = roc_curve(y_test, US_model.predict_proba(X_test)[:,1])\nplt.figure(figsize=(6,4))\nplt.plot(fpr,tpr,label='Logistic Regression (area = %0.2f)' % logreg_roc_auc)\nplt.plot([0,1],[0,1],'r--')\nplt.xlim([0.0,1.0])\nplt.ylim([0.0,1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic')\nplt.legend(loc='lower right')","e44e5ca3":"shuffled_df = scaled_df.sample(frac=1,random_state=47)\n\nCHD_df = shuffled_df.loc[shuffled_df['TenYearCHD'] == 1]\n\nnon_CHD_df = shuffled_df.loc[shuffled_df['TenYearCHD'] == 0].sample(n=950,random_state=47)\n\nUS70_df = pd.concat([CHD_df,non_CHD_df])\n\nUS70_df","f60f0614":"X = US70_df.drop(['TenYearCHD'],axis=1)\ny = US70_df['TenYearCHD']\n\nX_train, X_test, y_train, y_test = train_test_split(X,y,random_state=47)\n\nUS70_model = LogisticRegression().fit(X,y)\n\nprint_score(US_model,X_train,y_train,X_test,y_test,train=True)\nprint_score(US_model,X_train,y_train,X_test,y_test,train=False)","b2a1bd20":"US_model.coef_","c26f5160":"col_labels = scaled_X.columns.tolist()","96f34592":"plt.figure(figsize=(6, 4), dpi=100, facecolor='w', edgecolor='b')\nx = range(len(X_train.columns))\nc = US_model.coef_.reshape(-1)\nplt.bar( x, c )\nplt.xlabel( \"Variables\")\nplt.ylabel('Coefficients')\nplt.xticks(np.arange(15),col_labels,rotation=60)\nplt.title('Coefficient plot')","90df0250":"Coefficients = pd.DataFrame({\n    'Variable'    : X_train.columns,\n    'coefficient' : abs(c)\n})\nCoefficients = Coefficients.sort_values(by='coefficient',ascending=False)\nCoefficients","62c78718":"sig_var = Coefficients[Coefficients.coefficient > 0.3]\nsig_col = sig_var.Variable.values","a59d2631":"X = US_df[sig_col]\ny = US_df['TenYearCHD']\n\nX_train, X_test, y_train, y_test = train_test_split(X,y,random_state=47)\n\nmodel = LogisticRegression()\nrf1_model = model.fit(X,y)\n\nprint_score(rf1_model,X_train,y_train,X_test,y_test,train=True)\nprint_score(rf1_model,X_train,y_train,X_test,y_test,train=False)","f2027d71":"sig_var = Coefficients[Coefficients.coefficient > 0.5]\nsig_col = sig_var.Variable.values","c203113a":"X = US_df[sig_col]\ny = US_df['TenYearCHD']\n\nX_train, X_test, y_train, y_test = train_test_split(X,y,random_state=47)\n\nmodel = LogisticRegression()\nrf2_model = model.fit(X,y)\n\nprint_score(rf2_model,X_train,y_train,X_test,y_test,train=True)\nprint_score(rf2_model,X_train,y_train,X_test,y_test,train=False)","19069b78":"from sklearn.feature_selection import RFE\n\nX = US_df.drop(['TenYearCHD'],axis=1)\ny = US_df['TenYearCHD']\n\nmodel = LogisticRegression()\nrfe = RFE(estimator=model, n_features_to_select=1,step=1)\nrfe.fit(X,y)","e428fccf":"US_df.columns","e8796a25":"rfe.ranking_","d5a0a85c":"ranking_df = pd.DataFrame()\nranking_df['Feature_name'] = X.columns\nranking_df['Rank'] = rfe.ranking_","0d0ad165":"ranked = ranking_df.sort_values(by=['Rank'])","60c70e70":"ranked","b3830c85":"rfe_col = ranked['Feature_name'][:10].values\n\nX = US_df[rfe_col]\ny = US_df['TenYearCHD']\n\nX_train, X_test, y_train, y_test = train_test_split(X,y,random_state=47)\n\nmodel = LogisticRegression()\nrfe_model = model.fit(X,y)\n\nprint_score(rfe_model,X_train,y_train,X_test,y_test,train=True)\nprint_score(rfe_model,X_train,y_train,X_test,y_test,train=False)","a8d08067":"from sklearn.model_selection import KFold, StratifiedKFold\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\nwarnings.simplefilter(action='ignore', category=UserWarning)\n\ndef cv_scores(ML_model,X,y,rstate = 47, threshold = 0.5, cols=X.columns):\n    i = 1\n    cv_scores = []\n    \n    kf = StratifiedKFold(n_splits=7,random_state=rstate,shuffle=True)\n    for df_index, test_index in kf.split(X,y):\n        print('\\n{} of kfold {}'.format(i,kf.n_splits))\n        X_train,X_val = X.iloc[df_index],X.iloc[test_index]\n        y_train,y_val = y.iloc[df_index],y.iloc[test_index]\n        \n        model = ML_model\n        model.fit(X_train,y_train)\n        pred_probs = model.predict_proba(X_val)\n        pp = []\n        \n        for j in pred_probs[:,1]:\n            if j > threshold:\n                pp.append(1)\n            else:\n                pp.append(0)\n                \n        pred_val = pp\n        roc_score = roc_auc_score(y_val,pred_probs[:,1])\n        recall = recall_score(y_val,pred_val)\n        precision = precision_score(y_val, pred_val)\n        sufix = \"\"\n        msg = \"\"\n        msg += \"ROC AUC score : {}, Recall score : {:.4f}, Precision score: {:.4f} \".format(roc_score,recall,precision)\n        print(\"{}\".format(msg))\n        \n        cv_scores.append(roc_score)\n        i+=1\n    return cv_scores\n","95aeb70f":"scaled_X = scaled_df.drop(['TenYearCHD'],axis=1)\nscaled_y = scaled_df['TenYearCHD']\nprint('Bench mark model scores'.center(60,'*'))\nbenchmark_scores = cv_scores(bm_model,cols=scaled_X.columns,X=scaled_X,y=scaled_y)\n\nUS_X = US_df.drop(['TenYearCHD'],axis=1)\nUS_y = US_df['TenYearCHD']\nprint('\\n')\nprint('Under sampling model scores'.center(60,'*'))\nUS_scores = cv_scores(US_model,cols=US_X.columns,X=US_X,y=US_y)\n\nrfe_X = US_df[rfe_col]\nrfe_y = US_df['TenYearCHD']\nprint('\\n')\nprint('Recursive feature elimination model scores'.center(60,'*'))\nRFE_scores = cv_scores(rfe_model,cols=rfe_X.columns,X=rfe_X,y=rfe_y)\n","bc4b7d51":"print('Bench mark model scores'.center(60,'*'))\nbenchmark_scores = cv_scores(bm_model,cols=scaled_X.columns,X=scaled_X,y=scaled_y,threshold=0.4)\n\nprint('\\n')\nprint('Under sampling model scores'.center(60,'*'))\nUS_scores = cv_scores(US_model,cols=US_X.columns,X=US_X,y=US_y,threshold=0.4)\n\nprint('\\n')\nprint('Recursive feature elimination model scores'.center(60,'*'))\nRFE_scores = cv_scores(rfe_model,cols=rfe_X.columns,X=rfe_X,y=rfe_y,threshold=0.4)","c1a75c73":"results_df = pd.DataFrame({'bench_mark':benchmark_scores, 'under_sampling': US_scores, 'rfe_top_10': RFE_scores})","6a560632":"results_df.plot(y=[\"bench_mark\", \"under_sampling\", \"rfe_top_10\"], kind=\"bar\")\nplt.legend(loc='center')","3c9f35d8":"results_df","8edb68fd":"rfe_X = US_df[rfe_col]\nrfe_y = US_df['TenYearCHD']\nX_train,X_test,y_train,y_test = train_test_split(rfe_X,rfe_y)\nthreshold_adjust_score(rfe_model,X_train,y_train,X_test,y_test,threshold=0.4)","3b27af7b":"threshold_adjust_score(rfe_model,X_train,y_train,X_test,y_test,threshold=0.45)","1a5ec74f":"In our dumb bench mark model, it has performed a 87.7% accuracy. In such case, accuracy score will not be an accurate metric of measurement on how good the model perform in this unbalance data set. May look into other metric of measurement or performing unsampling or smote to test out on a more accurate accuracy score.","cad6d600":"### Data loading","cba19163":"There do not seem to have a distint correlation between the IVs and DV.","fc55131f":"No direct relations between BMI and diaBP readings regardless of smoker or not.","63496a2b":"# Backward Elimination Method","b0646e00":"### Elimination of features using co efficient index","32f47b0d":"Swapping between higher co efficient variables did not improve the model either.","b6b38c3d":"#### Conclusion\nIn the EDA phase, there seem to show some apparent data on risk group across age with pavalent conditions. As such, we should be able to build a model to pick up these attributes and solve the classification problem. We will start with a bench mark logistic regression model.","d540c182":"### Testing with higher threshold for higher recall","6ec75051":"Imputation of data is completed. Data is now full with no missing values.","1107acaa":"Using RFE method have the result as co efficient elimination.","f7db8e4f":"For the target class variable, we have a moderate imbalance class of 84 percent and 15 percent.","9e432561":"# Under Sampling","769b8783":"# Feature selection ","a21efed2":"Judging from the confusion matrix and recall score, a 50-50 balanced target variable seem to have performed better with a higher recall score for our positive class.","f0a8d51d":"### Demographics information about patient\n\n<b>male<\/b> - Gender of patient\n\n<b>age<\/b> - Age of patient\n\n<b>education<\/b> - Education level of patient in ordinal data\n\n### Tobacco usage of patient\n\n<b>currentSmoker<\/b> - information about a patient if he\/she is a smoker\n\n<b>cigsPerDay<\/b> - Amount of consumption if patient is a smoker\n\n### Health information of patient\n\n<b>BPMeds<\/b> - If a patient is taking BP \n\n<b>prevalentStroke<\/b> - Status of stroke occurance\n\n<b>prevalentHyp<\/b> - Status of prevailing hyper tension\n\n<b>diabetes<\/b> - Status of diabetes\n\n<b>totChol<\/b> - Cholestrol level\n\n<b>sysBP<\/b> - Blood pressure level\n\n<b>diaBP<\/b> - Diastolic blood pressure\n\n<b>sysBP<\/b> - Systolic blood pressure\n\n<b>BMI<\/b> - Body mass Index\n\n<b>heartRate<\/b> - Heart rate reading\n\n<b>gluscose<\/b> - Glucose level\n\n<b>TenYearCHD<\/b> - status if will suffer a risk of coronary disease in next ten years\n","62f56b6e":"RFE model is the statistically better performing than the rest base on its recall score and ROC AUC scoring. With a tuning of threshold value at 0.4, it improves recall score but also at the same time reducing much of its precision. \n\nWill try on other models such as decision tree or random forest classifier to see if we are able to achieve better results.","064cecc4":"Scaled data with MinMaxScaler() performed slightly better than original data.","969d2ab3":"# Prediction of Coronary Risk in ten years time","46d3de71":"We have quite a high accuracy score for both models. However, it has performed similarly to the dumb model, in the case of unbalance data set. It achieved a high accuracy by correctly predicting the majority class. Using the confusion matrix and recall as our measurement metrics, we can see the model is getting alot of false negatives and if we decrease the threshold to improve on the recall score, we are moving towards the dumb model. Also from the ROC AUC curve, we do not see a good performing model. We will need to use other techniques to balance the class in the models.","0f02b141":"Next we would like to compare a lower undersampling of our target class, rather than 50-50, we will try a 70-30 model.","77d701d3":"# Conclusion","220fd3e9":"## CHR prediction using logistic regression\n\nUsing the variables available in the dataset, we will build a logistic regression model to predict the ten year CHR.\n\n* Load data\n* Preprocess data, data cleaning and imputation\n* EDA \n* Build a bench mark model\n* Decide on evalution metrics base on prediction objective\n* Build model using all features & compare bench mark model\n* Use feature engineering techniques to improve model\n* do a final comparison on all built models for selection","89378c03":"From the data, it appears\n1. Male patient carry a slightly higher risk\n2. A smoker carry a slightly higher risk\n3. Patient on BP medication carry a much higher risk\n4. Patient with prevalent stroke history carry a much higher risk\n5. There is a higher risk between pavalent hyper tension\n6. Patients with diabetes carry a higher risk","a9830155":"List of question about the data :\n1. What is the poportion of patient at risk of CHD in ten years?\n2. What are the poportion of other pre existing condition with and without risk of CHD?\n3. Is there any relationship with a patient's education level and CHD?\n4. What age would be at risk of CHD?\n5. Will a person with higher BMI, cholestrol level or heartrate carry a higher risk of CHD","fcd022b8":"The objective of this project is to build a model which predicts if a patient is in risk of a coronary disease in the next ten years base on prevailing data available. The variables in the data set into three categories.","ec49a8b0":"# Building Dumb model","59454d3b":"### Evalution Metrics\nFor this classification problem we will look into these evalution metric as a measure of how good our model is going to be.\n1. Recall\n2. ROC curve\n\nWe are looking at recall here because a patient falsely identified with risk would not be as bad as letting a patient who is at risk but not detected hence not looking out for his or her health in the future.\n\nThe ROC AUC is the area under the curve when plotting the (normalized) true positive rate (x-axis) and the false positive rate(y-axis)\n\nOur main metric here would be Recall values. While AUC ROC score would take care of how well the predicted probabilities are able to differentiate between the 2 classes.","3ed33841":"# Preprocessing, Data cleaning and imputation","d958d630":"### Missing values\n1. <b>Education level<\/b> - Can be imputed with most frequent occurances in the data\n2. <b>cigs per day<\/b> - Can be check with smoker status, if non smoker will be zero. else impute with mean\n3. <b>BPMeds<\/b> - Likelihood that a patient with pavalent hyper tension is on BPMed\n4. <b>totChol<\/b> - Will drop or impute with mean or mode\n5. <b>BMI<\/b> - Since there are only a small percentage of missing value, will impute with the mean \n6. <b>heartrate<\/b> - impute with mean\n7. <b>glucose<\/b> - Will take the mean diabetes and non diabetes data","e5a620c0":"# Building a benchmark logistic regression model","f9cfd021":"The risk tend to be higher from the age of 45 onwards.","15e9a019":"Not much of a improvement on the model performance. We try a further reduce using only features of high co efficient.","5f6f79e4":"It seems a prevalent Hyper tension patient are very likely to be on BPMed.","9b0652a1":"There seem to be a slight increase of risk as lower education received.","1f7cd072":"# EDA","cfcbb262":"Cholestrol level are not correlated to any of the IVs. Thus, option is to drop or impute with a mean or mode.","15a473db":"# K-fold cross validation"}}