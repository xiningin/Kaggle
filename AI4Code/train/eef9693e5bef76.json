{"cell_type":{"cb9536b7":"code","411b1117":"code","e0e1504e":"code","95c8317f":"code","b0d11124":"code","5bfd8145":"code","4ef3ba8c":"code","b930d28a":"code","489fc1ad":"code","1c4afc88":"code","c38ac7e9":"code","46cb6f8f":"code","9aeca4b1":"code","0a19ed32":"code","0728becf":"code","7f81afcf":"code","d9401311":"code","9ec91196":"code","03091f91":"code","73daa66b":"code","b6b22356":"code","4a89d38a":"code","d21df70c":"code","a379a70b":"code","200e025f":"code","02630608":"markdown","af5821f9":"markdown","3059ad18":"markdown","def843dc":"markdown","0c595cf5":"markdown","6fd122f1":"markdown","c1cba0f9":"markdown","4b5ca1df":"markdown","6fddd42b":"markdown","fa91c0ec":"markdown"},"source":{"cb9536b7":"%%html\n<style> \n@import url('https:\/\/fonts.googleapis.com\/css?family=Orbitron|Roboto&effect=3d');\nbody {background-color: gainsboro;} \nh3 {color:#818286; font-family:Roboto;}\nspan {color:black; text-shadow:4px 4px 4px #aaa;}\ndiv.output_prompt,div.output_area pre {color:slategray;}\ndiv.input_prompt,div.output_subarea {color:#37c9e1;}      \ndiv.output_stderr pre {background-color:gainsboro;}  \ndiv.output_stderr {background-color:slategrey;}                \n<\/style>","411b1117":"import numpy as np,pandas as pd\nimport pylab as plt,tensorflow as tf\nimport matplotlib.colors as mcolors\nfrom descartes import PolygonPatch\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing \\\nimport RobustScaler,OneHotEncoder\nfrom sklearn.metrics import median_absolute_error,\\\nmean_absolute_error,r2_score,\\\nmean_squared_error,explained_variance_score\nfrom keras.models import Sequential\nimport keras.layers as tkl\nimport keras.callbacks as tkc\ncmap=plt.cm.get_cmap('Spectral',4)\nspectral_cmap=[]\nfor i in range(cmap.N):\n    rgb=cmap(i)[:3]\n    spectral_cmap.append(mcolors.rgb2hex(rgb))\nplt.style.use('seaborn-whitegrid')\npath='..\/input\/data-science-for-good\/'\nfw='weights.passnyc.hdf5'","e0e1504e":"def scores(regressor,y_train,y_valid,y_test,\n           y_train_reg,y_valid_reg,y_test_reg):\n    print(20*\"<=>\"); print(regressor); print(20*\"<=>\")\n    print(\"EV score. Train: \",\n          explained_variance_score(y_train,y_train_reg))\n    print(\"EV score. Valid: \",\n          explained_variance_score(y_valid,y_valid_reg))\n    print(\"EV score. Test: \",\n          explained_variance_score(y_test,y_test_reg))\n    print(20*\"<=>\")\n    print(\"R2 score. Train: \",r2_score(y_train,y_train_reg))\n    print(\"R2 score. Valid: \",r2_score(y_valid,y_valid_reg))\n    print(\"R2 score. Test: \",r2_score(y_test,y_test_reg))\n    print(20*\"<=>\")\n    print(\"MSE score. Train: \",\n          mean_squared_error(y_train,y_train_reg))\n    print(\"MSE score. Valid: \",\n          mean_squared_error(y_valid,y_valid_reg))\n    print(\"MSE score. Test: \",\n          mean_squared_error(y_test,y_test_reg))\n    print(20*\"<=>\")\n    print(\"MAE score. Train: \",\n          mean_absolute_error(y_train,y_train_reg))\n    print(\"MAE score. Valid: \",\n          mean_absolute_error(y_valid,y_valid_reg))\n    print(\"MAE score. Test: \",\n          mean_absolute_error(y_test,y_test_reg))\n    print(20*\"<=>\")\n    print(\"MdAE score. Train: \",\n          median_absolute_error(y_train,y_train_reg))\n    print(\"MdAE score. Valid: \",\n          median_absolute_error(y_valid,y_valid_reg))\n    print(\"MdAE score. Test: \",\n          median_absolute_error(y_test,y_test_reg))\ndef history_plot(fit_history,n):\n    keys=list(fit_history.history.keys())[0:4]\n    plt.figure(figsize=(11,10)); plt.subplot(211)\n    plt.plot(fit_history.history[keys[0]][n:],\n             color='slategray',label='train')\n    plt.plot(fit_history.history[keys[2]][n:],\n             color='#37c9e1',label='valid')\n    plt.xlabel(\"Epochs\"); plt.ylabel(\"Loss\")\n    plt.legend(); plt.title('Loss Function')    \n    plt.subplot(212)\n    plt.plot(fit_history.history[keys[1]][n:],\n             color='slategray',label='train')\n    plt.plot(fit_history.history[keys[3]][n:],\n             color='#37c9e1',label='valid')\n    plt.xlabel(\"Epochs\"); plt.ylabel(\"MAE\"); plt.legend()\n    plt.title('Mean Absolute Error'); plt.show()","95c8317f":"school_explorer=pd.read_csv(path+'2016 School Explorer.csv')\nd5_shsat=pd.read_csv(path+'D5 SHSAT Registrations and Testers.csv')\nschool_explorer.shape,d5_shsat.shape","b0d11124":"drop_list=['Adjusted Grade','New?','Other Location Code in LCGMS']\nschool_explorer=school_explorer.drop(drop_list,axis=1)\nschool_explorer.loc[[427,1023,712,908],'School Name']=\\\n['P.S. 212 D12','P.S. 212 D30','P.S. 253 D21','P.S. 253 D27']\nschool_explorer['School Income Estimate']=\\\nschool_explorer['School Income Estimate'].astype('object') \nfor s in [\",\",\"$\",\" \"]:\n    school_explorer['School Income Estimate']=\\\n    school_explorer['School Income Estimate'].str.replace(s,\"\")\nschool_explorer['School Income Estimate']=\\\nschool_explorer['School Income Estimate'].str.replace(\"nan\",\"0\")\nschool_explorer['School Income Estimate']=\\\nschool_explorer['School Income Estimate'].astype(float)\nschool_explorer['School Income Estimate'].replace(0,np.NaN,inplace=True)\npercent_list=['Percent ELL','Percent Asian','Percent Black',\n              'Percent Hispanic','Percent Black \/ Hispanic',\n              'Percent White','Student Attendance Rate',\n              'Percent of Students Chronically Absent',\n              'Rigorous Instruction %','Collaborative Teachers %',\n              'Supportive Environment %','Effective School Leadership %',\n              'Strong Family-Community Ties %','Trust %']\ntarget_list=['Average ELA Proficiency','Average Math Proficiency']\neconomic_list=['Economic Need Index','School Income Estimate']\nrating_list=['Rigorous Instruction Rating','Collaborative Teachers Rating',\n             'Supportive Environment Rating','Effective School Leadership Rating',\n             'Strong Family-Community Ties Rating','Trust Rating',\n             'Student Achievement Rating']\nfor el in percent_list:\n    school_explorer[el]=school_explorer[el].astype('object')\n    school_explorer[el]=school_explorer[el].str.replace(\"%\",\"\")\n    school_explorer[el]=school_explorer[el].str.replace(\"nan\",\"0\")\n    school_explorer[el]=school_explorer[el].astype(float)\n    school_explorer[el].replace(0,np.NaN,inplace=True)\n    school_explorer[el]=school_explorer[el].interpolate()\nfor el in target_list+economic_list:\n    school_explorer[el]=school_explorer[el].interpolate()\nfor el in rating_list:\n    moda_value=school_explorer[el].value_counts().idxmax()\n    school_explorer[el]=school_explorer[el].fillna(moda_value)    \ncategory_list=['District','Community School?','City','Grades']               \nfor feature in category_list:\n    feature_cat=pd.factorize(school_explorer[feature])\n    school_explorer[feature]=feature_cat[0]    \nfor feature in rating_list:\n    feature_pairs=dict(zip(['Not Meeting Target','Meeting Target', \n                            'Approaching Target','Exceeding Target'],\n                            ['0','2','1','3']))\n    school_explorer[feature].replace(feature_pairs,inplace=True)\n    school_explorer[feature]=school_explorer[feature].astype(int)    \ncategory_list=list(category_list+rating_list)\nnumeric_list=list(school_explorer\\\n.columns[[4,5]+list(range(13,24))+[25,27,29,31,33]+list(range(38,158))])    \nprint('Number of Missing Values: ',sum(school_explorer.isna().sum())) ","5bfd8145":"sat_list=['DBN','Number of students who registered for the SHSAT',\n          'Number of students who took the SHSAT']\nd5_shsat_2016=d5_shsat[sat_list][d5_shsat['Year of SHST']==2016]\\\n.groupby(['DBN'],as_index=False).agg(np.sum)\nd5_shsat_2016['Took SHSAT %']=\\\nd5_shsat_2016['Number of students who took the SHSAT']\\\n\/d5_shsat_2016['Number of students who registered for the SHSAT']\nd5_shsat_2016['Took SHSAT %']=\\\nd5_shsat_2016['Took SHSAT %'].fillna(0).apply(lambda x:round(x,3))\nd5_shsat_2016.rename(columns={'DBN':'Location Code'},inplace=True)\nd5_shsat_2016=\\\npd.merge(school_explorer[['Location Code']+numeric_list+\\\n                         category_list+target_list],\n         d5_shsat_2016,on='Location Code')\nd5_shsat_2016.shape","4ef3ba8c":"features1=school_explorer[numeric_list].values\nfeatures2=school_explorer[numeric_list+category_list]\ntargets1=school_explorer['Average Math Proficiency'].values\ntargets2=school_explorer['Average ELA Proficiency'].values\n#features_enc=features2\n#encode=OneHotEncoder(sparse=False)\n#for column in category_list:\n#    encode.fit(features2[[column]])\n#    transform=encode.transform(features2[[column]])    \n#    transform=\\\n#    pd.DataFrame(transform, \n#                 columns=[(column+\"_\"+str(i)) \n#                          for i in features2[column]\\\n#                          .value_counts().index])\n#    transform=transform.set_index(features2.index.values)    \n#    features_enc=pd.concat([features_enc,transform],axis=1)\n#    features_enc=features_enc.drop(column,1)    \nfeatures2=features2.values\n#features_enc=features_enc.values","b930d28a":"# data = school_explorer\n# features = numeric variables\n# targets = Average Math Proficiency\nX_train1,X_test1,y_train1,y_test1=\\\ntrain_test_split(features1,targets1,\n                 test_size=.2,random_state=1)\nn=int(len(X_test1)\/2)\nX_valid1,y_valid1=X_test1[:n],y_test1[:n]\nX_test1,y_test1=X_test1[n:],y_test1[n:]\n[X_train1.shape,X_test1.shape,X_valid1.shape,\n y_train1.shape,y_test1.shape,y_valid1.shape]","489fc1ad":"# data = school_explorer\n# eatures = numeric variables\n# targets = Average ELA Proficiency\nX_train2,X_test2,y_train2,y_test2=\\\ntrain_test_split(features1,targets2,\n                 test_size=.2,random_state=1)\nn=int(len(X_test2)\/2)\nX_valid2,y_valid2=X_test2[:n],y_test2[:n]\nX_test2,y_test2=X_test2[n:],y_test2[n:]\n[X_train2.shape,X_test2.shape,X_valid2.shape,\n y_train2.shape,y_test2.shape,y_valid2.shape]","1c4afc88":"# data = school_explorer, \n# features = numeric & categorical variables \n# targets = Average Math Proficiency\nX_train3,X_test3,y_train3,y_test3=\\\ntrain_test_split(features2,targets1,\n                 test_size=.2,random_state=1)\nn=int(len(X_test3)\/2)\nX_valid3,y_valid3=X_test3[:n],y_test3[:n]\nX_test3,y_test3=X_test3[n:],y_test3[n:]\n[X_train3.shape,X_test3.shape,X_valid3.shape,\n y_train3.shape,y_test3.shape,y_valid3.shape]","c38ac7e9":"# data = school_explorer\n# features = numeric & categorical variables\n# targets = Average ELA Proficiency\nX_train4,X_test4,y_train4,y_test4=\\\ntrain_test_split(features2,targets2,test_size=.2,random_state=1)\nn=int(len(X_test4)\/2)\nX_valid4,y_valid4=X_test4[:n],y_test4[:n]\nX_test4,y_test4=X_test4[n:],y_test4[n:]\n[X_train4.shape,X_test4.shape,X_valid4.shape,\n y_train4.shape,y_test4.shape,y_valid4.shape]","46cb6f8f":"def mlp_model():\n    model=Sequential()   \n    model.add(tkl.Dense(138,input_dim=138))\n    model.add(tkl.LeakyReLU(alpha=.02))\n    model.add(tkl.Dense(138*4))\n    model.add(tkl.LeakyReLU(alpha=.02))    \n    model.add(tkl.Dense(138*16))\n    model.add(tkl.LeakyReLU(alpha=.02))\n    model.add(tkl.Dense(138*16))\n    model.add(tkl.LeakyReLU(alpha=.02))    \n    model.add(tkl.Dense(1))    \n    model.compile(loss='mse',optimizer='rmsprop',\n                  metrics=['mae'])\n    return model\nmlp_model1=mlp_model()","9aeca4b1":"checkpointer=tkc.ModelCheckpoint(\n    filepath=fw,verbose=2,save_best_only=True)\nlr_reduction=tkc.ReduceLROnPlateau(\n    monitor='val_loss',patience=5,\n    verbose=2,factor=.75)\nhistory=mlp_model1.fit(\n    X_train1,y_train1, \n    epochs=200,batch_size=16,verbose=2,\n    validation_data=(X_valid1,y_valid1),\n    callbacks=[checkpointer,lr_reduction])","0a19ed32":"history_plot(history,100)\nmlp_model1.load_weights(fw)\ny_train_mlp1=mlp_model1.predict(X_train1)\ny_valid_mlp1=mlp_model1.predict(X_valid1)\ny_test_mlp1=mlp_model1.predict(X_test1)\nti='MLP; \\nNumeric Features; \\n'+\\\n   'Average Math Proficiency'\nscores(ti,y_train1,y_valid1,y_test1,\n       y_train_mlp1,y_valid_mlp1,y_test_mlp1)","0728becf":"mlp_model2=mlp_model()\ncheckpointer=tkc.ModelCheckpoint(\n    filepath=fw,verbose=2,save_best_only=True)\nlr_reduction=tkc.ReduceLROnPlateau(\n    monitor='val_loss',patience=5,\n    verbose=2,factor=.75)\nhistory=mlp_model2.fit(\n    X_train2,y_train2, \n    epochs=200,batch_size=16,verbose=2,\n    validation_data=(X_valid2,y_valid2),\n    callbacks=[checkpointer,lr_reduction])","7f81afcf":"history_plot(history,100)\nmlp_model2.load_weights(fw)\ny_train_mlp2=mlp_model2.predict(X_train2)\ny_valid_mlp2=mlp_model2.predict(X_valid2)\ny_test_mlp2=mlp_model2.predict(X_test2)\nti='MLP; \\nNumeric Features; '+\\\n   '\\nAverage ELA Proficiency'\nscores(ti,y_train2,y_valid2,y_test2,\n       y_train_mlp2,y_valid_mlp2,y_test_mlp2)","d9401311":"def cmlp_model():\n    model=Sequential()    \n    model.add(tkl.Dense(149,input_dim=149))\n    model.add(tkl.LeakyReLU(alpha=.02))\n    model.add(tkl.Dense(149*4))\n    model.add(tkl.LeakyReLU(alpha=.02))    \n    model.add(tkl.Dense(149*16))\n    model.add(tkl.LeakyReLU(alpha=.02))\n    model.add(tkl.Dense(1))    \n    model.compile(loss='mse',optimizer='rmsprop',\n                  metrics=['mae'])\n    return model\nmlp_model3=cmlp_model()","9ec91196":"checkpointer=tkc.ModelCheckpoint(\n    filepath=fw,verbose=2,save_best_only=True)\nlr_reduction=tkc.ReduceLROnPlateau(\n    monitor='val_loss',patience=5,\n    verbose=2,factor=.75)\nhistory=mlp_model3.fit(\n    X_train3,y_train3, \n    epochs=200,batch_size=16,verbose=2,\n    validation_data=(X_valid3,y_valid3),\n    callbacks=[checkpointer,lr_reduction])","03091f91":"history_plot(history,100)\nmlp_model3.load_weights(fw)\ny_train_mlp3=mlp_model3.predict(X_train3)\ny_valid_mlp3=mlp_model3.predict(X_valid3)\ny_test_mlp3=mlp_model3.predict(X_test3)\nti='MLP; \\nNumeric & Categorical Features; '+\\\n   '\\nAverage Math Proficiency'\nscores(ti,y_train3,y_valid3,y_test3,\n       y_train_mlp3,y_valid_mlp3,y_test_mlp3)","73daa66b":"mlp_model4=cmlp_model()\ncheckpointer=tkc.ModelCheckpoint(\n    filepath=fw,verbose=2,save_best_only=True)\nlr_reduction=tkc.ReduceLROnPlateau(\n    monitor='val_loss',patience=5,\n    verbose=2,factor=.75)\nhistory=mlp_model4.fit(\n    X_train4,y_train4, \n    epochs=200,batch_size=16,verbose=2,\n    validation_data=(X_valid4,y_valid4),\n    callbacks=[checkpointer,lr_reduction])","b6b22356":"history_plot(history,100)\nmlp_model4.load_weights(fw)\ny_train_mlp4=mlp_model4.predict(X_train4)\ny_valid_mlp4=mlp_model4.predict(X_valid4)\ny_test_mlp4=mlp_model4.predict(X_test4)\nti='MLP; \\nNumeric & Categorical Features; '+\\\n   '\\nAverage ELA Proficiency'\nscores(ti,y_train4,y_valid4,y_test4,\n       y_train_mlp4,y_valid_mlp4,y_test_mlp4)","4a89d38a":"plt.figure(figsize=(12,7))\nplt.plot(y_test1[1:50],'-o',\n         color='#3636ff',label='Real Data')\nplt.plot(y_test_mlp1[1:50],'-o',\n         color='#ff3636',label='MLP')\nti=\"Average Math Proficiency. \"+\\\n   \"NN Test Predictions vs Real Data\"\nplt.legend(); plt.title(ti);","d21df70c":"plt.figure(figsize=(12,7))\nplt.plot(y_test2[1:50],'-o',\n         color='#3636ff',label='Real Data')\nplt.plot(y_test_mlp2[1:50],'-o',\n         color='#ff3636',label='MLP')\nti=\"Average ELA Proficiency. \"+\\\n   \"NN Test Predictions vs Real Data\"\nplt.legend(); plt.title(ti);","a379a70b":"plt.figure(figsize=(12,7))\nplt.plot(y_test3[1:50],'-o',\n         color='#3636ff',label='Real Data')\nplt.plot(y_test_mlp3[1:50],'-o',\n         color='#ff3636',label='MLP')\nti=\"Average Math Proficiency. \"+\\\n   \"NN Test Predictions vs Real Data\"\nplt.legend(); plt.title(ti);","200e025f":"plt.figure(figsize=(12,7))\nplt.plot(y_test4[1:50],'-o',\n         color='#3636ff',label='Real Data')\nplt.plot(y_test_mlp4[1:50],'-o',\n         color='#ff3636',label='MLP')\nti=\"Average ELA Proficiency. \"+\\\n   \"NN Test Predictions vs Real Data\"\nplt.legend(); plt.title(ti);","02630608":"<details><summary style='color:#37c9e1; font-family:Orbitron;'>Useful Links<\/summary><br\/>\n\n&#x1F4E1; &nbsp; [School Quality Reports. Educator Guide](http:\/\/schools.nyc.gov\/NR\/rdonlyres\/967E0EE1-7E5D-4E47-BC21-573FEEE23AE2\/0\/201516EducatorGuideHS9252017.pdf)\n    \n&#x1F4E1; &nbsp; [New York City Department of Education](https:\/\/www.schools.nyc.gov)\n\n&#x1F4E1; &nbsp; [NYC OpenData](https:\/\/opendata.cityofnewyork.us\/)\n\n&#x1F4E1; &nbsp; [Pandas Visualization](https:\/\/pandas.pydata.org\/pandas-docs\/stable\/visualization.html)\n    \n&#x1F4E1; &nbsp; [Pandas Styling](https:\/\/pandas.pydata.org\/pandas-docs\/stable\/style.html)\n\n&#x1F4E1; &nbsp; [ggplot2](https:\/\/ggplot2.tidyverse.org\/)\n\n&#x1F4E1; &nbsp; [R Tutorial](https:\/\/www.tutorialspoint.com\/r\/index.htm)","af5821f9":"### MLP => Numeric & categorical features","3059ad18":"In this case, neural networks were used to predict the indicators of educational outcomes. \n\nIt is difficult to achieve high accuracy here in general since the ultimate goal depends not only on the financing, social environment and professionalism of the staff but also on the personal qualities of the students. \n\nAdding category variables does not improve the accuracy of predictions because these indicators depend not exactly on geolocations but on the well-being of the surrounding area.\n\nI believe that it is possible to reach at least the same accuracy with neural networks as with regression algorithms like `Gradient Boosting` or `Random Forest`.","def843dc":"<h1 class='font-effect-3d' style='color:#37c9e1; font-family:Orbitron;'> &#x1F310; &nbsp; Predictions & Real Data<\/h1>\n\n### Numeric features","0c595cf5":"<h1 class='font-effect-3d' style='color:#37c9e1; font-family:Orbitron;'> &#x1F310; &nbsp; Data Splitting for Neural Networks<\/h1>","6fd122f1":"<h1 class='font-effect-3d' style='color:#37c9e1; font-family:Orbitron;'> &#x1F310; &nbsp; Code Library, Styling, and Links<\/h1>\n<details><summary style='color:#37c9e1; font-family:Orbitron;'>Github<\/summary><br\/>\n\nThe current notebook\n    \n&#x1F4D8; &nbsp; [Python Version](https:\/\/github.com\/OlgaBelitskaya\/kaggle_notebooks\/blob\/master\/passnyc-neural-networks-2.ipynb)\n\nThe previous notebook\n    \n&#x1F4D8; &nbsp; [Python Version](https:\/\/github.com\/OlgaBelitskaya\/kaggle_notebooks\/blob\/master\/passnyc-neural-networks.ipynb)","c1cba0f9":"<h1 class='font-effect-3d' style='color:#37c9e1; font-family:Orbitron;'>&#x1F310; &nbsp; Let's Go Ahead<\/h1>\nIt's time to move to the next step.\n\n&#x1F4D8; &nbsp; [PASSNYC. 32 School Districts and D5. Part 2](https:\/\/www.kaggle.com\/olgabelitskaya\/passnyc-32-school-districts-and-d5-part-2)","4b5ca1df":"<h1 class='font-effect-3d' style='color:#37c9e1; font-family:Orbitron;'> &#x1F310; &nbsp; Data Loading and Preprocessing<\/h1>","6fddd42b":"<h1 class='font-effect-3d' style='color:#37c9e1; font-family:Orbitron;'> &#x1F310; &nbsp; Neural Network Models <\/h1>\n\n### MLP => Numeric features","fa91c0ec":"### Numeric & categorical features"}}