{"cell_type":{"1a27896f":"code","3083386a":"code","797a1e7b":"code","b3e4795f":"code","28007544":"code","1c8e2bbf":"code","888919e8":"code","684a7b4e":"code","b3804e55":"code","05e15c1d":"markdown","d0be6cbd":"markdown","0c146441":"markdown","5a703c1b":"markdown","a6371fd3":"markdown","3c3d05c7":"markdown","b4384002":"markdown","c1f2a139":"markdown"},"source":{"1a27896f":"!pip install pycocotools>=2.0.2 > \/dev\/null\n!pip install timm>=0.3.2 > \/dev\/null\n!pip install omegaconf>=2.0 > \/dev\/null\n!pip install ensemble-boxes > \/dev\/null\n!pip install effdet > \/dev\/null","3083386a":"# Release GPU Memory.\n\"\"\"\nfrom numba import cuda as CU\ntry:\n    device = CU.get_current_device()\n    device.reset()\nexcept Exception as E:\n    print(\"GPU not enabled. Nothing to clear and good to go.\")\n\n\n## Restart session to detect installed libraries\n!pip list | grep effdet\nimport os\nos._exit(00)\n## Check PyTorch Version\n\"\"\"","797a1e7b":"import sys\nimport torch\nimport os\nimport warnings\nimport time\nimport cv2\nimport pandas as pd\nimport numpy as np\nimport gc\nfrom tqdm.auto import tqdm\nfrom datetime import datetime\nfrom collections import Counter\nfrom glob import glob\n\nfrom ensemble_boxes import weighted_boxes_fusion\nimport albumentations as A\nfrom albumentations.pytorch.transforms import ToTensorV2\nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import GroupKFold, train_test_split\nfrom torch.utils.data import Dataset,DataLoader\nfrom torch.utils.data.sampler import SequentialSampler, RandomSampler\nfrom torch.utils.data.dataloader import default_collate\n\nfrom effdet import get_efficientdet_config, EfficientDet, DetBenchTrain\nfrom effdet.efficientdet import HeadNet\nfrom effdet import create_model, unwrap_bench, create_loader, create_dataset, create_evaluator, create_model_from_config\nfrom effdet.data import resolve_input_config, SkipSubset\nfrom effdet.anchors import Anchors, AnchorLabeler\nfrom timm.models import resume_checkpoint, load_checkpoint\n#from timm.utils import *\nfrom timm.optim import create_optimizer\nfrom timm.scheduler import create_scheduler\n\nimport random\n\n# Global variables\nTRAIN = True\nTRAIN_ROOT_PATH = '..\/input\/swimming-pool-512x512\/CANNES_TILES_512x512_PNG\/CANNES_TILES_512x512_PNG'\nSEED = 42\nIMG_SIZE = 512\n\nlabel2color = [[255, 0, 0]]\nviz_labels =  [\"pool\"]\n\n# Load data\ndf_annotations = pd.read_csv('..\/input\/swimmingpool-eda-csv\/swimming_pools_labels_512x512.csv')\ndf_annotations['image_path'] = df_annotations['image_id'].map(lambda x:os.path.join(TRAIN_ROOT_PATH, str(x)))\n\n# Filter wrong annotations (or too small pools)\ndf_annotations = df_annotations.drop(df_annotations[(df_annotations['xmin'] == 0) & (df_annotations['xmax'] == 0)].index)\ndf_annotations = df_annotations.drop(df_annotations[(df_annotations['ymin'] == 0) & (df_annotations['ymax'] == 0)].index)\n\n# Keep only pools\ndf_annotations = df_annotations[df_annotations['class'] == 'pool']\ndf_annotations.reset_index(drop=True, inplace=True)\ndf_annotations['class'] = 1\ndf_annotations['xmin'] = df_annotations['xmin'] - 1\ndf_annotations['ymin'] = df_annotations['ymin'] - 1\ndf_annotations.head(5)","b3e4795f":"image_paths = df_annotations['image_path'].unique()\nprint(\"Number of Images :\",len(image_paths))\nanno_count = df_annotations.shape[0]\nprint(\"Number of Annotations:\", anno_count)","28007544":"# Seed\ndef seed_everything(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\n# Plots\ndef plot_img(img, size=(18, 18), is_rgb=True, title=\"\", cmap=None):\n    plt.figure(figsize=size)\n    plt.imshow(img)\n    plt.suptitle(title)\n    plt.show()\n\ndef plot_imgs(imgs, cols=2, size=10, is_rgb=True, title=\"\", cmap=None, img_size=None):\n    rows = len(imgs)\/\/cols + 1\n    fig = plt.figure(figsize=(cols*size, rows*size))\n    for i, img in enumerate(imgs):\n        if img_size is not None:\n            img = cv2.resize(img, img_size)\n        fig.add_subplot(rows, cols, i+1)\n        plt.imshow(img)\n    plt.suptitle(title)\n    return fig\n\ndef draw_bbox_small(image, box, label, color):   \n    alpha = 0\n    alpha_text = 0.4\n    thickness = 1\n    font_size = 0.4\n    overlay_bbox = image.copy()\n    overlay_text = image.copy()\n    output = image.copy()\n\n    text_width, text_height = cv2.getTextSize(label.upper(), cv2.FONT_HERSHEY_SIMPLEX, font_size, thickness)[0]\n    cv2.rectangle(overlay_bbox, (box[0], box[1]), (box[2], box[3]),\n                color, -1)\n    cv2.addWeighted(overlay_bbox, alpha, output, 1 - alpha, 0, output)\n    cv2.rectangle(overlay_text, (box[0], box[1]-7-text_height), (box[0]+text_width+2, box[1]),\n                (0, 0, 0), -1)\n    cv2.addWeighted(overlay_text, alpha_text, output, 1 - alpha_text, 0, output)\n    cv2.rectangle(output, (box[0], box[1]), (box[2], box[3]),\n                    color, thickness)\n    cv2.putText(output, label.upper(), (box[0], box[1]-5),\n            cv2.FONT_HERSHEY_SIMPLEX, font_size, (255, 255, 255), thickness, cv2.LINE_AA)\n    return output\n\nclass AverageMeter(object):\n    \"\"\"Computes and stores the average and current value\"\"\"\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum \/ self.count\n        \n# Dataset class\nclass DatasetRetriever(Dataset):\n\n    def __init__(self, marking, image_ids, transforms=None, test=False):\n        super().__init__()\n        self.image_ids = image_ids\n        self.marking = marking\n        self.transforms = transforms\n        self.test = test\n        \n    def __getitem__(self, index: int):\n        image_id = self.image_ids[index]\n        \n        if self.test or random.random() > 0.5:\n            image, boxes, labels = self.load_image_and_boxes(index)\n        else:\n            image, boxes, labels = self.load_cutmix_image_and_boxes(index)\n        \n        ## To prevent ValueError: y_max is less than or equal to y_min for bbox from albumentations bbox_utils\n        labels = np.array(labels, dtype=np.int).reshape(len(labels), 1)\n        combined = np.hstack((boxes.astype(np.int), labels))\n        combined = combined[np.logical_and(combined[:,2] > combined[:,0],\n                                                          combined[:,3] > combined[:,1])]\n        boxes = combined[:, :4]\n        labels = combined[:, 4].tolist()\n        \n        target = {}\n        target['boxes'] = boxes\n        target['labels'] = torch.tensor(labels)\n        target['image_id'] = torch.tensor([index])\n        if self.transforms:\n            for i in range(10):\n                sample = self.transforms(**{\n                    'image': image,\n                    'bboxes': target['boxes'],\n                    'labels': labels\n                })\n                if len(sample['bboxes']) > 0:\n                    image = sample['image']\n                    target['boxes'] = torch.stack(tuple(map(torch.tensor, zip(*sample['bboxes'])))).permute(1, 0)\n                    target['boxes'][:,[0,1,2,3]] = target['boxes'][:,[1,0,3,2]]  ## ymin, xmin, ymax, xmax\n                    break\n            \n            ## Handling case where no valid bboxes are present\n            if len(target['boxes'])==0 or i==9:\n                return None\n            else:\n                ## Handling case where augmentation and tensor conversion yields no valid annotations\n                try:\n                    assert torch.is_tensor(image), f\"Invalid image type:{type(image)}\"\n                    assert torch.is_tensor(target['boxes']), f\"Invalid target type:{type(target['boxes'])}\"\n                except Exception as E:\n                    print(\"Image skipped:\", E)\n                    return None      \n\n        return image, target, image_id\n\n    def __len__(self) -> int:\n        return self.image_ids.shape[0]\n    \n    def load_image_and_boxes(self, index):\n        image_id = self.image_ids[index]\n        image = cv2.imread(f'{TRAIN_ROOT_PATH}\/{image_id}', cv2.IMREAD_COLOR).copy()\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n        image \/= 255.0\n        records = self.marking[self.marking['image_id'] == image_id]\n        boxes = records[['xmin', 'ymin', 'xmax', 'ymax']].values\n        labels = records['class'].tolist()\n        resize_transform = A.Compose([A.Resize(height=IMG_SIZE, width=IMG_SIZE, p=1.0)], \n                                    p=1.0, \n                                    bbox_params=A.BboxParams(\n                                        format='pascal_voc',\n                                        min_area=0.1, \n                                        min_visibility=0.1,\n                                        label_fields=['labels'])\n                                    )\n\n        resized = resize_transform(**{\n                'image': image,\n                'bboxes': boxes,\n                'labels': labels\n            })\n\n        resized_bboxes = np.vstack((list(bx) for bx in resized['bboxes']))\n        return resized['image'], resized_bboxes, resized['labels']\n    \n    def load_cutmix_image_and_boxes(self, index, imsize=512):\n        \"\"\" \n        This implementation of cutmix author:  https:\/\/www.kaggle.com\/nvnnghia \n        Refactoring and adaptation: https:\/\/www.kaggle.com\/shonenkov\n        \"\"\"\n        w, h = imsize, imsize\n        s = imsize \/\/ 2\n\n        xc, yc = [int(random.uniform(imsize * 0.25, imsize * 0.75)) for _ in range(2)]  # center x, y\n        indexes = [index] + [random.randint(0, self.image_ids.shape[0] - 1) for _ in range(3)]\n\n        result_image = np.full((imsize, imsize, 3), 1, dtype=np.float32)\n        result_boxes = []\n        result_labels = np.array([], dtype=np.int)\n\n        for i, index in enumerate(indexes):\n            image, boxes, labels = self.load_image_and_boxes(index)\n            if i == 0:\n                x1a, y1a, x2a, y2a = max(xc - w, 0), max(yc - h, 0), xc, yc  # xmin, ymin, xmax, ymax (large image)\n                x1b, y1b, x2b, y2b = w - (x2a - x1a), h - (y2a - y1a), w, h  # xmin, ymin, xmax, ymax (small image)\n            elif i == 1:  # top right\n                x1a, y1a, x2a, y2a = xc, max(yc - h, 0), min(xc + w, s * 2), yc\n                x1b, y1b, x2b, y2b = 0, h - (y2a - y1a), min(w, x2a - x1a), h\n            elif i == 2:  # bottom left\n                x1a, y1a, x2a, y2a = max(xc - w, 0), yc, xc, min(s * 2, yc + h)\n                x1b, y1b, x2b, y2b = w - (x2a - x1a), 0, max(xc, w), min(y2a - y1a, h)\n            elif i == 3:  # bottom right\n                x1a, y1a, x2a, y2a = xc, yc, min(xc + w, s * 2), min(s * 2, yc + h)\n                x1b, y1b, x2b, y2b = 0, 0, min(w, x2a - x1a), min(y2a - y1a, h)\n            result_image[y1a:y2a, x1a:x2a] = image[y1b:y2b, x1b:x2b]\n            padw = x1a - x1b\n            padh = y1a - y1b\n\n            boxes[:, 0] += padw\n            boxes[:, 1] += padh\n            boxes[:, 2] += padw\n            boxes[:, 3] += padh\n\n            result_boxes.append(boxes)\n            result_labels = np.concatenate((result_labels, labels))\n\n        result_boxes = np.concatenate(result_boxes, 0)\n        np.clip(result_boxes[:, 0:], 0, 2 * s, out=result_boxes[:, 0:])\n        result_boxes = result_boxes.astype(np.int32)\n        index_to_use = np.where((result_boxes[:,2]-result_boxes[:,0])*(result_boxes[:,3]-result_boxes[:,1]) > 0)\n        result_boxes = result_boxes[index_to_use]\n        result_labels = result_labels[index_to_use]\n\n        return result_image, result_boxes, result_labels\n    \n#  Image transforms\ndef get_train_transforms():\n    return A.Compose(\n        [\n        A.OneOf([\n            A.HueSaturationValue(hue_shift_limit=0.2, sat_shift_limit= 0.2, \n                                 val_shift_limit=0.2, p=0.9),\n            A.RandomBrightnessContrast(brightness_limit=0.2, \n                                       contrast_limit=0.2, p=0.9),\n        ],p=0.9),\n        A.JpegCompression(quality_lower=85, quality_upper=95, p=0.2),\n        A.OneOf([\n            A.Blur(blur_limit=3, p=1.0),\n            A.MedianBlur(blur_limit=3, p=1.0)\n            ],p=0.1),\n        A.HorizontalFlip(p=0.5),\n        A.VerticalFlip(p=0.5),\n        A.RandomRotate90(p=0.5),\n        A.Transpose(p=0.5),\n        A.Resize(height=IMG_SIZE, width=IMG_SIZE, p=1),\n        A.Cutout(num_holes=8, max_h_size=64, max_w_size=64, fill_value=0, p=0.5),\n        ToTensorV2(p=1.0)\n        ], \n        p=1.0, \n        bbox_params=A.BboxParams(\n            format='pascal_voc',\n            min_area=0, \n            min_visibility=0,\n            label_fields=['labels']\n        )\n    )\n\ndef get_valid_transforms():\n    return A.Compose(\n        [\n            A.Resize(height=IMG_SIZE, width=IMG_SIZE, p=1.0),\n            ToTensorV2(p=1.0),\n        ], \n        p=1.0, \n        bbox_params=A.BboxParams(\n            format='pascal_voc',\n            min_area=0, \n            min_visibility=0,\n            label_fields=['labels']\n        )\n    )\n\n\nclass Fitter:\n    def __init__(self, model, device, config, fold):\n        self.config = config\n        self.epoch = 0\n        self.fold = fold\n\n        self.base_dir = f'.\/{config.folder}'\n        \n        if not os.path.exists(self.base_dir):\n            os.makedirs(self.base_dir)\n            \n        self.log_path = f'{self.base_dir}\/log.txt'\n        self.best_summary_loss = 10**5\n        self.model = model\n        self.device = device\n\n        param_optimizer = list(self.model.named_parameters())\n        no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n        optimizer_grouped_parameters = [\n            {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.001},\n            {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n        ] \n\n        self.optimizer = config.OptimizerClass(self.model.parameters(), lr=config.lr)\n        self.scheduler = config.SchedulerClass(self.optimizer, **config.scheduler_params)\n        self.log(f'Fitter prepared. Device is {self.device}')\n\n    def fit(self, train_loader, validation_loader):\n        history_dict = {}\n        history_dict['epoch'] = []\n        history_dict['train_loss'] = []\n        history_dict['val_loss'] = []\n        history_dict['train_lr'] = []\n        \n        for e in range(self.config.n_epochs):\n            history_dict['epoch'].append(self.epoch)\n            lr = self.optimizer.param_groups[0]['lr']\n            timestamp = datetime.utcnow().isoformat()\n            \n            if self.config.verbose:\n                self.log(f'\\n{timestamp}\\nLR: {lr}')\n\n            t = time.time()\n            summary_loss, loss_trend, lr_trend = self.train_epoch(train_loader)\n            history_dict['train_loss'].append(loss_trend)\n            history_dict['train_lr'].append(lr_trend)\n            self.log(f'[RESULT]: Train. Epoch: {self.epoch}, summary_loss: {summary_loss.avg:.5f}, time: {(time.time() - t):.5f}')\n            self.save(f'{self.base_dir}\/last-checkpoint.bin')\n            \n            t = time.time()\n            summary_loss, loss_trend = self.validation(validation_loader)\n            history_dict['val_loss'].append(loss_trend)\n            self.log(f'[RESULT]: Val. Epoch: {self.epoch}, summary_loss: {summary_loss.avg:.5f}, time: {(time.time() - t):.5f}')\n            \n            if summary_loss.avg < self.best_summary_loss:\n                self.best_summary_loss = summary_loss.avg\n                self.model.eval()\n                self.save(f'{self.base_dir}\/fold-{self.fold}-best-checkpoint-{str(self.epoch).zfill(3)}epoch.bin')\n                \n                try:\n                    os.remove(f)\n                except:pass\n                f = f'{self.base_dir}\/fold-{self.fold}-best-checkpoint-{str(self.epoch).zfill(3)}epoch.bin'\n\n            if self.config.validation_scheduler:\n                self.scheduler.step(metrics=summary_loss.avg)\n\n            self.epoch += 1\n        return history_dict\n\n    def train_epoch(self, train_loader):\n        self.model.train()\n        summary_loss = AverageMeter()\n        t = time.time()\n        loss_trend = []\n        lr_trend = []\n        for step, (images, targets, image_ids) in tqdm(enumerate(train_loader), total=len(train_loader)):\n            if self.config.verbose:\n                if step % self.config.verbose_step == 0:\n                    print(\n                        f'Train Step {step}\/{len(train_loader)}, ' + \\\n                        f'summary_loss: {summary_loss.avg:.5f}, ' + \\\n                        f'time: {(time.time() - t):.5f}', end='\\r'\n                    )            \n            \n            images = torch.stack(images)\n            images = images.to(self.device).float()\n            \n            target_res = {}\n            boxes = [target['boxes'].to(self.device).float() for target in targets]\n            labels = [target['labels'].to(self.device).float() for target in targets]\n            target_res['bbox'] = boxes\n            target_res['cls'] = labels\n            self.optimizer.zero_grad()\n            output = self.model(images, target_res)\n\n            loss = output['loss']\n            loss.backward()\n            summary_loss.update(loss.detach().item(), self.config.batch_size)\n            self.optimizer.step()\n\n            if self.config.step_scheduler:\n                self.scheduler.step()\n\n            lr = self.optimizer.param_groups[0]['lr']\n            loss_trend.append(summary_loss.avg)\n            lr_trend.append(lr)\n        return summary_loss, loss_trend, lr_trend\n    \n    def validation(self, val_loader):\n        self.model.eval()\n        summary_loss = AverageMeter()\n        t = time.time()\n        loss_trend = []\n#         lr_trend = []\n        \n        for step, (images, targets, image_ids) in tqdm(enumerate(val_loader), total=len(val_loader)):\n            if self.config.verbose:\n                if step % self.config.verbose_step == 0:\n                    print(\n                        f'Val Step {step}\/{len(val_loader)}, ' + \\\n                        f'summary_loss: {summary_loss.avg:.5f}, ' + \\\n                        f'time: {(time.time() - t):.5f}', end='\\r'\n                    )\n            with torch.no_grad():\n                images = torch.stack(images)\n                images = images.to(self.device).float()\n                target_res = {}\n                boxes = [target['boxes'].to(self.device).float() for target in targets]\n                labels = [target['labels'].to(self.device).float() for target in targets]\n                target_res['bbox'] = boxes\n                target_res['cls'] = labels \n                target_res[\"img_scale\"] = torch.tensor([1.0] * self.config.batch_size,\n                                                       dtype=torch.float).to(self.device)\n                target_res[\"img_size\"] = torch.tensor([images[0].shape[-2:]] * self.config.batch_size,\n                                                      dtype=torch.float).to(self.device)\n                \n                output = self.model(images, target_res)\n            \n                loss = output['loss']\n                summary_loss.update(loss.detach().item(), self.config.batch_size)\n\n                loss_trend.append(summary_loss.avg)\n        return summary_loss, loss_trend[-1]\n    \n    \n    def save(self, path):\n        self.model.eval()\n        torch.save({\n            'model_state_dict': self.model.model.state_dict(),\n            'optimizer_state_dict': self.optimizer.state_dict(),\n            'scheduler_state_dict': self.scheduler.state_dict(),\n            'best_summary_loss': self.best_summary_loss,\n            'epoch': self.epoch,\n        }, path)\n\n    def load(self, path):\n        checkpoint = torch.load(path)\n        self.model.model.load_state_dict(checkpoint['model_state_dict'])\n        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n        self.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n        self.best_summary_loss = checkpoint['best_summary_loss']\n        self.epoch = checkpoint['epoch'] + 1\n        \n    def log(self, message):\n        if self.config.verbose:\n            print(message)\n        with open(self.log_path, 'a+') as logger:\n            logger.write(f'{message}\\n')\n\ndef collate_fn(batch):\n    batch = list(filter(lambda x: x is not None, batch))\n    \n    return tuple(zip(*batch))\n            \nseed_everything(SEED)","1c8e2bbf":"viz_images = []\n\nfor img_id in df_annotations['image_id'].unique()[10:18]:\n    img_path = df_annotations[df_annotations['image_id'] == img_id]['image_path'].iloc[0]\n    img_array  = cv2.imread(img_path)\n    img_array = cv2.cvtColor(img_array, cv2.COLOR_BGR2RGB)\n\n    boxes = df_annotations[df_annotations['image_id'] == img_id][['xmin', 'ymin', 'xmax', 'ymax']].to_numpy().tolist()\n    labels = df_annotations[df_annotations['image_id'] == img_id][['class']].to_numpy().tolist()\n    \n    ## Visualize Original Bboxes\n    img_before = img_array.copy()\n    for box, label in zip(boxes, labels):\n        x_min, y_min, x_max, y_max = (box[0], box[1], box[2], box[3])\n        color = label2color[0]\n        img_before = draw_bbox_small(img_before, list(np.int_(box)), viz_labels[0], color)\n    viz_images.append(img_before)\n        \nplot_imgs(viz_images)\nplt.show()","888919e8":"viz_ids = df_annotations.sample(6).image_id.tolist()\nviz_dataset = DatasetRetriever(\n                    image_ids=np.array(viz_ids),\n                    marking=df_annotations,\n                    transforms=get_train_transforms(),\n                    test=False,\n                    )\nviz_images = []\nfor idx, im_id in enumerate(viz_ids):\n    image, boxes, labels = viz_dataset.load_cutmix_image_and_boxes(idx)\n    image_viz = image.copy()\n#     print(\"image_viz.shape\", image_viz.shape)\n    for box, label in zip(boxes, labels):\n        color = label2color[0]\n#         image_viz *= 255 \n#         image_viz = image_viz.astype('uint8')\n        image_viz = cv2.normalize(src=image_viz, dst=None, alpha=0, beta=255,\n                                  norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_8U)\n        image_viz = draw_bbox_small(image_viz, list(np.int_(box)), viz_labels[0], color)\n    viz_images.append(image_viz)\n\nfig = plot_imgs(viz_images)\nplt.show()","684a7b4e":"warnings.filterwarnings(\"ignore\")\n\n# Training configuration\nclass TrainGlobalConfig:\n    def __init__(self):\n        self.num_classes = 1\n        self.num_workers = 2\n        self.batch_size = 4\n        self.n_epochs = 40\n        self.lr = 0.001\n        self.model_name = 'tf_efficientdet_d1'\n        self.folder = 'training_job'\n        self.verbose = True\n        self.verbose_step = 1\n        self.step_scheduler = False\n        self.validation_scheduler = True\n        self.n_img_count = len(df_annotations.image_id.unique())\n        self.OptimizerClass = torch.optim.AdamW\n        self.SchedulerClass = torch.optim.lr_scheduler.ReduceLROnPlateau\n        self.scheduler_params = dict(\n            mode='min',\n            factor=0.5,\n            patience=1,\n            verbose=False, \n            threshold=0.0001,\n            threshold_mode='abs',\n            cooldown=0, \n            min_lr=1e-8,\n            eps=1e-08\n        )\n        self.kfold = 5\n    \n    def reset(self):\n        self.OptimizerClass = torch.optim.AdamW\n        self.SchedulerClass = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts\n\n# Training configuration\ntrain_config = TrainGlobalConfig()\n\n## Training will resume if the checkpoint path is specified below\ncheckpoint_path = None\n\ndevice = torch.device('cuda:0' if torch.cuda.is_available() else \"cpu\")\n\n# Run training\nfold_history = []\nval_fold = 0\n\nfor val_fold in range(train_config.kfold):\n\n    print(f'Fold {val_fold+1}\/{train_config.kfold}')\n\n    train_ids = df_annotations[df_annotations['fold'] != val_fold].image_id.unique()\n    val_ids = df_annotations[df_annotations['fold'] == val_fold].image_id.unique()\n\n    # Create dataset\n    train_dataset = DatasetRetriever(\n                        image_ids=train_ids,\n                        marking=df_annotations,\n                        transforms=get_train_transforms(),\n                        test=False,\n                        )\n\n    validation_dataset = DatasetRetriever(\n                            image_ids=val_ids,\n                            marking=df_annotations,\n                            transforms=get_valid_transforms(),\n                            test=True,\n                            )\n\n    # Create dataloader\n    train_loader = torch.utils.data.DataLoader(\n        train_dataset,\n        batch_size=train_config.batch_size,\n        sampler=RandomSampler(train_dataset),\n        pin_memory=False,\n        drop_last=True,\n        num_workers=train_config.num_workers,\n        collate_fn=collate_fn,\n    )\n    val_loader = torch.utils.data.DataLoader(\n        validation_dataset, \n        batch_size=train_config.batch_size,\n        num_workers=train_config.num_workers,\n        shuffle=False,\n        sampler=SequentialSampler(validation_dataset),\n        pin_memory=False,\n        collate_fn=collate_fn,\n    )\n\n    base_config = get_efficientdet_config(train_config.model_name)\n    base_config.image_size = (IMG_SIZE, IMG_SIZE)\n\n    if(checkpoint_path):\n        print(f'Resuming from checkpoint: {checkpoint_path}')        \n        model = create_model_from_config(base_config, bench_task='train', bench_labeler=True,\n                                 num_classes=train_config.num_classes,\n                                 pretrained=False)\n        model.to(device)\n\n        fitter = Fitter(model=model, device=device, config=train_config, fold=val_fold)\n        fitter.load(checkpoint_path)\n\n    else:\n        model = create_model_from_config(base_config, bench_task='train', bench_labeler=True,\n                                     pretrained=True,\n                                     num_classes=train_config.num_classes)\n        model.to(device)\n\n        fitter = Fitter(model=model, device=device, config=train_config, fold=val_fold)  \n\n    model_config = model.config\n    history_dict = fitter.fit(train_loader, val_loader)\n    fold_history.append(history_dict)\n    \n    #break","b3804e55":"if TRAIN:\n    fold_history_c = fold_history.copy()\n\n    train_loss_all = []\n    val_loss_all = []\n\n    n_steps_fold = (train_config.n_img_count\/\/train_config.n_epochs)\/\/train_config.kfold\n\n    for fold, fold_dict in enumerate(fold_history_c):\n        train_losses = [item for sublist in fold_dict['train_loss'] for item in sublist]\n        val_losses = [item for item in fold_dict['val_loss']]\n        train_lrs = [item for sublist in fold_dict['train_lr'] for item in sublist]\n        train_loss_all.append(np.array(train_losses))\n\n        val_losses = np.repeat(val_losses, n_steps_fold).tolist()\n        val_loss_all.append(np.array(val_losses))\n\n        fig = plt.figure(figsize=(22,8))\n        fig.suptitle(f'FOLD{fold+1} - TRAIN LOSS & VAL LOSSES', x=0.125, y=1.00, ha='left',\n                     fontweight=100, fontfamily='Lato', size=36)\n        plt.plot(train_losses, color='red', label='train_loss', linewidth=1)\n        plt.plot(val_losses, color='green', label='val_loss', linewidth=1)\n        plt.legend() \n        plt.savefig(f'fold{fold+1}_loss_trend.png', bbox_inches='tight')\n        plt.show()\n\n        fig = plt.figure(figsize=(22,8))\n        fig.suptitle(f'FOLD{fold+1} - LEARNING RATE TREND', x=0.125, y=1.00, ha='left',\n                     fontweight=100, fontfamily='Lato', size=36)\n        plt.plot(train_lrs, color='blue', label='lr', linewidth=1)\n        plt.legend() \n        plt.savefig(f'fold{fold+1}_lr_trend.png', bbox_inches='tight')\n        plt.show()\n\n    # Save weights\n    !zip -r training_5folds.zip .\/training_job\/*","05e15c1d":"# Visualize images with boxes","d0be6cbd":"<span style=\"color: #0087e4; font-family: Segoe UI; font-size: 2.3em; font-weight: 300;\">Update Environment & Release GPU Memory<\/span>\n\nPlease note that the kernal will stop execution here when using 'Run All'. This is because of the exit command which is used to load refresh and reload the packages installed in the previous steps. So run up till this cell manually then, use 'Run After' with the next cell selected to run the rest in one go.\n\n\nThis applies to fresh environments or if packages in the environment got reset.\n\n<p style='text-align: justify;'><span style=\"color: #001b2e; font-family: Segoe UI; font-size: 1.2em;\">The below cell also helps in clearing GPU memory on crashes and error. When facing OOM error, just run the below cell manually and then run the rest in one go using 'Run After'.<\/span><\/p>\n\n","0c146441":"# Train model","5a703c1b":"# Losses","a6371fd3":"<span style=\"color: #000508; font-family: Segoe UI; font-size: 2.0em; font-weight: 300;\">Import Packages<\/span>","3c3d05c7":"# Visualize augmentations","b4384002":"# Functions","c1f2a139":" <span style=\"color: #000508; font-family: Segoe UI; font-size: 2.0em; font-weight: 300;\">Online installations<\/span>"}}