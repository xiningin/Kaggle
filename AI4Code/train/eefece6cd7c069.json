{"cell_type":{"27c7f34c":"code","744b7d83":"code","6c9f236a":"code","db30512e":"code","c831ffec":"code","42c53ed4":"code","d6530603":"code","223b4b08":"code","eba21e13":"code","4c46b614":"markdown","2ae9b348":"markdown","003274b5":"markdown","1d663372":"markdown","937ebed9":"markdown","c6134ebc":"markdown","796c05d6":"markdown","593444b1":"markdown"},"source":{"27c7f34c":"#Import all the dependencies\nfrom gensim.models.doc2vec import Doc2Vec, TaggedDocument\nfrom nltk.tokenize import sent_tokenize, word_tokenize\n\nfrom nltk.corpus import inaugural\n\ninaugural.fileids()[0:5]","744b7d83":"data2 = [\"I love machine learning. Its awesome.\",\n        \"I love coding in python\",\n        \"I love building chatbots\",\n        \"they chat amagingly well\"]\n\ntagged_data2 = [TaggedDocument(words=word_tokenize(_d.lower()), tags=[str(i)]) for i, _d in enumerate(data2)]\n\nprint(tagged_data2)","6c9f236a":"data = [inaugural.raw('1789-Washington.txt'), \n        inaugural.raw('1793-Washington.txt'),\n        inaugural.raw('2001-Bush.txt'), \n        inaugural.raw('2009-Obama.txt')]\n\nsentences = [] \n\n# iterate through each sentence in the file\nfor para in data:\n    for i in sent_tokenize(para): \n        sentences.append(i)\n\nprint(len(sentences))","db30512e":"tagged_data = [TaggedDocument(words=word_tokenize(_d.lower()), tags=[str(i)]) for i, _d in enumerate(sentences)]","c831ffec":"print(tagged_data[0:4])","42c53ed4":"max_epochs = 100\nvec_size = 20\nalpha = 0.025\n\nmodel = Doc2Vec(size=vec_size,\n                alpha=alpha, \n                min_alpha=0.00025,\n                min_count=1,\n                dm =1)\n\nmodel.build_vocab(tagged_data)","d6530603":"for epoch in range(max_epochs):\n    #print('iteration {0}'.format(epoch))\n    model.train(tagged_data,\n                total_examples=model.corpus_count,\n                epochs=model.iter)\n    \n    #decrease the learning rate\n    model.alpha -= 0.0002\n    \n    #fix the learning rate, no decay\n    model.min_alpha = model.alpha\n\nmodel.save(\"d2v.model\")\nprint(\"Model Saved\")","223b4b08":"from gensim.models.doc2vec import Doc2Vec\n\nmodel= Doc2Vec.load(\"d2v.model\")\n\n#to find the vector of a document which is not in training data\ntest_data = word_tokenize(inaugural.raw('2017-Trump.txt').lower())\n\nv1 = model.infer_vector(test_data)\nprint(\"V1_infer\", v1)","eba21e13":"# to find most similar doc using tags\nsimilar_doc = model.docvecs.most_similar('1')\nprint(similar_doc)","4c46b614":"Official website : https:\/\/radimrehurek.com\/gensim\/models\/doc2vec.html\n\nSome interesting parameters :\n\n(1) **vector_size** (int, optional) \u2013 Dimensionality of the feature vectors.<br>\n(2) **alpha** (float, optional) \u2013 The initial learning rate. <br>\n(3) **min_alpha** (float, optional) \u2013 Learning rate will linearly drop to min_alpha as training progresses. <br>\n(4) **min_count** (int, optional) \u2013 Ignores all words with total frequency lower than this. <br>\n(5) **dm** ({1,0}, optional) \u2013 Defines the training algorithm. If dm=1, \u2018distributed memory\u2019 (PV-DM) is used. Otherwise, distributed bag of words (PV-DBOW) is employed. <br>","2ae9b348":"# **Finding other most similar doc using tags**","003274b5":"So, combining all Four files of Inaugural dataset, We have total 234 different sentences.","1d663372":"# **Doc2Vec related information :**","937ebed9":"**Just for better readability of tagged_data**","c6134ebc":"**Using Four files of Inaugural Dataset to train model**","796c05d6":"**Training Doc2Vec Model**","593444b1":"**dm** defines the training algorithm.\n\n<br>**dm =1** means **\u2018distributed memory\u2019 (PV-DM)** and \n<br>**dm =0** means **\u2018distributed bag of words\u2019 (PV-DBOW)**. \n\nDistributed Memory model preserves the word order in a document whereas Distributed Bag of words just uses the bag of words approach, which doesn\u2019t preserve any word order."}}