{"cell_type":{"2b1460c8":"code","a9649507":"code","c1404618":"code","a5079166":"code","9c083a37":"code","8020225d":"code","e4865e7b":"code","66882abd":"code","a5bcaed4":"code","3ba66705":"code","20b50159":"code","241bf4bb":"code","2bb4496c":"code","35331e8e":"code","8e2d7cf6":"code","5911e321":"code","8a23968b":"code","62cedfb6":"code","feb72aa1":"code","7c90a719":"code","9226fc97":"code","f2a7ca4e":"code","cdbbd727":"code","e50cd2ba":"code","d4c842e7":"code","78686096":"code","266fa76a":"code","40a46284":"code","ee407f82":"code","4ada629c":"code","b1bccb67":"code","33a90889":"code","b075fffc":"code","45867333":"code","f68247dd":"code","b5ef056c":"code","fa7f01fd":"code","63cffafd":"code","687786d3":"code","243a5db4":"code","c91a18fb":"code","6ba23072":"code","7d61c003":"code","d88efcc0":"code","cc69d0cd":"code","b688fc45":"code","e969a3b8":"code","1d41dbfd":"code","3cbd9e16":"code","88ddbdca":"code","79ea2ab9":"code","66c3f3e4":"code","32689647":"code","6563e2d6":"code","bab8ddcf":"code","b4e08bfb":"code","982ae2c5":"code","905d1098":"code","ac456fe8":"code","c2efdc19":"code","7906d024":"code","e05ee723":"code","68fca65c":"code","01d97343":"code","03fc230d":"code","b4c0ce99":"code","3c374cd1":"code","743b5aae":"code","59c08ecf":"code","ef27ed72":"code","8e3b774d":"code","4e4ff137":"code","77e37658":"code","bf166475":"code","1f767b77":"code","52d41fa3":"markdown","ccc73566":"markdown","f647877e":"markdown","19019851":"markdown","ad82f52a":"markdown","b220beb0":"markdown","2df0d72b":"markdown","10353bf4":"markdown","9d9dbb16":"markdown","733f3dae":"markdown","10c21955":"markdown","ef54621e":"markdown","b2d0dc26":"markdown","c6ecea08":"markdown","8c675f3d":"markdown","57a2164e":"markdown","d666dbb5":"markdown","e2219610":"markdown","e07b8cc0":"markdown","5f5c0565":"markdown","2e802756":"markdown","4cfe4a24":"markdown","30adfe60":"markdown","d3ce26a7":"markdown","08bf2fbb":"markdown","3d94ed49":"markdown","e58467f2":"markdown","68cd8a6a":"markdown","e4a515df":"markdown","e71dbd52":"markdown","29d86291":"markdown","270716b1":"markdown","a3411b0a":"markdown","060d3c15":"markdown","e676192a":"markdown","58978d5b":"markdown","a1488e99":"markdown","e9af7210":"markdown","2a997b13":"markdown","e984a3b7":"markdown","5c5da293":"markdown","5119845c":"markdown","901b78af":"markdown","27128b99":"markdown","f13c0ce0":"markdown","df9969d0":"markdown","0a16c1ee":"markdown","f9c22dd4":"markdown","0843bcb7":"markdown","387c7930":"markdown","7bd84bd2":"markdown","5ae93140":"markdown","925f87cb":"markdown","483761ef":"markdown","2a365bb8":"markdown","e9c492e1":"markdown","0ab319ed":"markdown","ae4f9783":"markdown","7e9d8306":"markdown","bd4796a2":"markdown","35f35f0c":"markdown","a4dc2ec3":"markdown","63e3f450":"markdown"},"source":{"2b1460c8":"# Supress warnings\nimport warnings\nwarnings.filterwarnings('ignore')","a9649507":"# Importing the required libraries\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline","c1404618":"# Setting the Data Display Size\n\npd.set_option('display.max_row', 500)\npd.set_option('display.max_columns', 500)\npd.set_option('display.width', 1000)\npd.set_option('display.expand_frame_repr', False)","a5079166":"# Read the csv file using 'read_csv'. \n\nbike = pd.read_csv('..\/input\/boombike-share-assignment\/day.csv')\nbike.head()","9c083a37":"bike.shape","8020225d":"bike.info()","e4865e7b":"# Checking for of missing values in columns\n\n(bike.isnull().sum()\/len(bike)*100).sort_values(ascending = False)","66882abd":"# Checking for of missing values in rows\n\n(bike.isnull().sum(axis=1)\/len(bike)*100).sort_values(ascending = False)","a5bcaed4":"# Duplicate check:\n\nduplicate = bike.copy()\nduplicate.drop_duplicates(subset = None, inplace = True)\nduplicate.shape","3ba66705":"# Checking for unwanted values present to convert and standardised them for better analysis\n\nbike_dummy=bike.iloc[:,1:16]\nfor col in bike_dummy:\n    print(bike_dummy[col].value_counts(ascending=False), '\\n\\n\\n')","20b50159":"# Converting date to Pandas datetime format\n\nbike['dteday'] = pd.to_datetime(bike['dteday'])\nbike.head()","241bf4bb":"# Statistical Understanding of the Numeric columns:\n\nbike[['temp','atemp','hum','windspeed', 'casual', 'registered', 'cnt']].describe()","2bb4496c":"num_cols = ['temp','atemp','hum','windspeed']\nplt.figure(figsize=[18,15])\n\nfor n,col in enumerate(num_cols):\n    plt.subplot(2,2,n+1)\n    sns.distplot(bike[col])","35331e8e":"for n,col in enumerate(num_cols):\n    print(col+\": \"+str(bike[col].skew()))","8e2d7cf6":"num_cols = ['casual', 'registered', 'cnt']\nplt.figure(figsize=[18,15])\n\nfor n,col in enumerate(num_cols):\n    plt.subplot(2,2,n+1)\n    sns.boxplot(bike[col], orient = \"h\")\n    plt.xlabel(\"\")\n    plt.ylabel(\"\")\n    plt.title(col)\n    plt.tight_layout()","5911e321":"for n,col in enumerate(num_cols):\n    print(col+\": \"+str(bike[col].skew()))","8a23968b":"# Looking at the data distribution in both the years 2018 and 2019\n\nplt.figure(figsize=[10,6])\nsns.countplot(bike.yr)\nplt.show()","62cedfb6":"# Looking at the ratio of working days vs holidays.\n\nbike.workingday.value_counts()","feb72aa1":"# Looking at the ratio of data for different weather conditions\n\nbike.weathersit.value_counts(normalize = True)*100","7c90a719":"bike.season.value_counts(normalize = True)*100","9226fc97":"bike.head()","f2a7ca4e":"# Droping holiday as workingday covers the relateable information\n\nbike.drop('holiday',axis=1,inplace=True)","cdbbd727":"# Looking at Categorical variables against the target variable cnt. \n\nplt.figure(figsize=(25, 15))\nplt.subplot(2,3,1)\nsns.barplot(x = 'season', y = 'cnt', data = bike)\nplt.subplot(2,3,2)\nsns.barplot(x = 'yr', y = 'cnt', data = bike)\nplt.subplot(2,3,3)\nsns.barplot(x = 'mnth', y = 'cnt', data = bike)\nplt.subplot(2,3,4)\nsns.barplot(x = 'weekday', y = 'cnt', data = bike)\nplt.subplot(2,3,5)\nsns.barplot(x = 'workingday', y = 'cnt', data = bike)\nplt.subplot(2,3,6)\nsns.barplot(x = 'weathersit', y = 'cnt', data = bike)\nplt.show()","e50cd2ba":"bike.groupby(['workingday'])[['cnt']].sum()","d4c842e7":"# Visualising Numeric Variables\n\nsns.pairplot(bike[['yr','temp', 'atemp', 'hum', 'windspeed', 'casual', 'registered', 'cnt']],\n            vars = ['temp', 'atemp', 'hum', 'windspeed', 'casual', 'registered', 'cnt'], \n            hue = 'yr',  markers=['o', 's'])\nplt.show()","78686096":"# Droping atemp as temp and atemp are highly corelated\n\nbike.drop('atemp',axis = 1, inplace = True)\nbike.head()","266fa76a":"# witnessing change in temp against the categirical variables, thus verifying temp being the strongest predictor: \n\nplt.figure(figsize=(25, 15))\nplt.subplot(2,3,1)\nsns.boxplot(x = 'season', y = 'temp', data = bike)\nplt.subplot(2,3,2)\nsns.boxplot(x = 'yr', y = 'temp', data = bike)\nplt.subplot(2,3,3)\nsns.boxplot(x = 'mnth', y = 'temp', data = bike)\nplt.subplot(2,3,4)\nsns.boxplot(x = 'weekday', y = 'temp', data = bike)\nplt.subplot(2,3,5)\nsns.boxplot(x = 'workingday', y = 'temp', data = bike)\nplt.subplot(2,3,6)\nsns.boxplot(x = 'weathersit', y = 'temp', data = bike)\nplt.show()","40a46284":"bike.head()","ee407f82":"bike_new = bike[['season', 'yr', 'mnth', 'weekday', 'workingday', 'weathersit', 'temp', 'hum', 'windspeed','cnt']]\nbike_new.head()","4ada629c":"bike_new.info()","b1bccb67":"# Before coverting them into our Dummy variables, let's first convert them into categorical datatypes: \n\nbike_new['season'] = bike_new['season'].astype('category')\nbike_new['mnth'] = bike_new['mnth'].astype('category')\nbike_new['workingday'] = bike_new['workingday'].astype('category')\nbike_new['weathersit'] = bike_new['weathersit'].astype('category')\n\nbike_new.info()","33a90889":"# creating dummy variables\n\nbike_new = pd.get_dummies(bike_new, drop_first = True)\nbike_new.head()","b075fffc":"# Also dropping weekday as it has not much as a predictor\n\nbike_new = bike_new.drop('weekday', axis = 1)","45867333":"bike_new.info()","f68247dd":"import sklearn\nfrom sklearn.model_selection import train_test_split","b5ef056c":"bike_new_train, bike_new_test = train_test_split(bike_new, train_size = 0.7, test_size = 0.3, random_state = 100)\nprint(bike_new_train.shape)\nprint(bike_new_test.shape)","fa7f01fd":"from sklearn.preprocessing import MinMaxScaler","63cffafd":"# 1. Initiate and object \n\nscaler = MinMaxScaler()\n\n# 2. Fit on data: applying scaler() to all the numeric variables\n\nnum_vars = ['temp', 'hum', 'windspeed','cnt']\n\nbike_new_train[num_vars] = scaler.fit_transform(bike_new_train[num_vars])\nbike_new_train.head()","687786d3":"# Let's also verify this: \n\nbike_new_train[num_vars].describe()","243a5db4":"# Let's check the correlation coefficients to see which variables are highly correlated\n\nplt.figure(figsize = (16, 10))\nsns.heatmap(bike_new_train.corr(), annot = True, cmap=\"YlGnBu\")\nplt.show()","c91a18fb":"# X_train and y_train\n\nX_train = bike_new_train\ny_train = bike_new_train.pop('cnt')","6ba23072":"X_train.head()","7d61c003":"y_train.head()","d88efcc0":"# Let's Start with the base model \/ Simple Linear Model and import statsmodel library\n\nimport statsmodels.api as sm","cc69d0cd":"# Step-1: Add a constant\nX_train_sm = sm.add_constant(X_train['temp'])\n\n# Step-2: Creating our Model\nlr = sm.OLS(y_train, X_train_sm)\n\n# Step-3: Fitting the Model\nlr_model = lr.fit()\n\n# Step-4: Summary ofthe Model\nlr_model.summary()","b688fc45":"# Step-1: Add a constant\nX_train_sm = X_train[['temp', 'yr']]\nX_train_sm = sm.add_constant(X_train_sm)\n\n# Step-2: Creating our Model\nlr = sm.OLS(y_train, X_train_sm)\n\n# Step-3: Fitting the Model\nlr_model = lr.fit()\n\n# Step-4: Summary ofthe Model\nlr_model.summary()","e969a3b8":"# Step-1: Add a constant\nX_train_sm = X_train[['temp', 'yr', 'season_3']]\nX_train_sm = sm.add_constant(X_train_sm)\n\n# Step-2: Creating our Model\nlr = sm.OLS(y_train, X_train_sm)\n\n# Step-3: Fitting the Model\nlr_model = lr.fit()\n\n# Step-4: Summary ofthe Model\nlr_model.summary()","1d41dbfd":"# Importing RFE and LinearRegression\nfrom sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LinearRegression","3cbd9e16":"X_train.columns","88ddbdca":"# Running RFE with the output number of the variable equal to 10\n\nlm = LinearRegression()\nlm.fit(X_train, y_train)\n\nrfe = RFE(lm, n_features_to_select=12, step=1)        \nrfe = rfe.fit(X_train, y_train)","79ea2ab9":"list(zip(X_train.columns,rfe.support_,rfe.ranking_))","66c3f3e4":"col = X_train.columns[rfe.support_]\ncol","32689647":"X_train.columns[~rfe.support_]","6563e2d6":"# Creating X_train dataframe with RFE selected variables\nX_train_rfe = X_train[col]\n\n# Adding a constant variable \nX_train_new = sm.add_constant(X_train_rfe)\n\n# Running and fitting the linear model\nlm1_model = sm.OLS(y_train,X_train_new).fit()\n\n#Let's see the summary of our linear model\nprint(lm1_model.summary())","bab8ddcf":"X_train_new= X_train_new.drop('season_3',axis=1)","b4e08bfb":"# Adding a constant variable \nX_train_lm2 = sm.add_constant(X_train_new)\n\n# Running and fitting the linear model\nlm2_model = sm.OLS(y_train,X_train_lm2).fit()\n\n#Let's see the summary of our linear model\nprint(lm2_model.summary())","982ae2c5":"X_train_new= X_train_lm2.drop('mnth_10',axis=1)","905d1098":"# Adding a constant variable \nX_train_lm3 = sm.add_constant(X_train_new)\n\n# Running and fitting the linear model\nlm3_model = sm.OLS(y_train,X_train_lm3).fit()\n\n#Let's see the summary of our linear model\nprint(lm3_model.summary())","ac456fe8":"X_train_new= X_train_lm3.drop('const',axis=1)","c2efdc19":"# Calculate the VIFs for the lm1 model\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nvif = pd.DataFrame()\nX = X_train_new\nvif['Features'] = X.columns\nvif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","7906d024":"X_train1 = X_train_new.drop([\"hum\"], axis = 1)\nX_train1.head()","e05ee723":"# Adding a constant variable \nX_train_lm4 = sm.add_constant(X_train1)\n\n# Running and fitting the linear model\nlm4_model = sm.OLS(y_train,X_train_lm4).fit()\n\n#Let's see the summary of our linear model\nprint(lm4_model.summary())","68fca65c":"X_train_new= X_train_lm4.drop('const',axis=1)","01d97343":"# Calculate the VIFs for the lm2 model\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nvif = pd.DataFrame()\nX = X_train_new\nvif['Features'] = X.columns\nvif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","03fc230d":"# Computing the predicted y-values\n\ny_train_pred = lm4_model.predict(X_train_lm4)","b4c0ce99":"# Computing the residuals:\n\nres = y_train - y_train_pred","3c374cd1":"# Plotting the Histogram of the Residuals (Distribution of the Error terms)\n\nplt.figure(figsize = [10,8])\nsns.distplot(res)\nplt.show()","743b5aae":"num_vars = ['temp', 'hum', 'windspeed','cnt']\n\nbike_new_test[num_vars] = scaler.transform(bike_new_test[num_vars])\nbike_new_test.head()","59c08ecf":"bike_new_test[num_vars].describe()","ef27ed72":"X_test = bike_new_test\ny_test = bike_new_test.pop('cnt')","8e3b774d":"X_train_new.columns","4e4ff137":"# Creating X_test_new dataframe by dropping variables from X_test\nX_test_new = X_test[X_train_new.columns]\n\n# Adding a constant variable \nX_test_new = sm.add_constant(X_test_new)","77e37658":"# Making predictions\n\ny_pred = lm4_model.predict(X_test_new)","bf166475":"from sklearn.metrics import r2_score\nr2_score(y_true = y_test, y_pred = y_pred)","1f767b77":"# Plotting y_test and y_pred to understand the spread.\nfig = plt.figure()\nplt.scatter(y_test,y_pred)\nfig.suptitle('y_test vs y_pred', fontsize=20)              # Plot heading \nplt.xlabel('y_test', fontsize=18)                          # X-label\nplt.ylabel('y_pred', fontsize=16)                          # Y-label","52d41fa3":"### <font color = Green > Way Forward: <\/font>\n\n+ We have identified few categories in our categorical columsn that will be a good predictor to our dependent variable but not all. Hence we will convert them into Dummy variables.\n+ We will not be requiring the entire dataset as there are some reductant variables too. Hence converting the original dataset into bike_new. \n+ We will leave behind the casual and registered columns as they are not rquired in model building\n","ccc73566":"#### Our Final Model \n\nlm4_model","f647877e":"### <font color = Green > Inference: <\/font>\n\n1. You have the coef of all the variables as well as constant. \n\n2. Our P-values of all coefficient is less than 0.05; hence we can say our model is significant.\n\n3. Also, our model is strong as the r2 and adjusted r2 is same around 83%\n\n+ But, let's look at our VIF values","19019851":"### <font color = Green > Business Understanding <\/font>\n\n+ A bike-sharing system is a service in which bikes are made available for shared use to individuals on a short term basis for a price or free. \n+ Many bike share systems allow people to borrow a bike from a \"dock\" which is usually computer-controlled wherein the user enters the payment information, and the system unlocks it. This bike can then be returned to another dock belonging to the same system.","ad82f52a":"### <font color = Green > Inference: <\/font>\n\n+ A minimum of 2-3 Degree Celsius upto 35-45 Degree Celsius Temperature has been felt. \n+ Humidity has gone upto 97 units starting from 0. \n+ A standard Gentle and Fresh breeze speed is witnessed between 12 - 38 km\/h speed. Our data interpret the same if the units are in km\/h. \n+ Almost all of their mean and median is almost the same or very close. \n+ The overall count of Casual Users seems less with compare to Registered Ones.\n+ We will plot them below for a better visualisation. ","b220beb0":"### <font color = Green > Inference: <\/font>\n\n+ 63% of the data had captured where it was almost clear sky or partly cloudy.\n+ 34% of the data had captured where it was misty with a bit cloudy sky.\n+ 3% of the data had captured where it was either lighty snowy or lightly raining with some scattered clouds and thunderstorms.\n","2df0d72b":"**Building the Model - II**\n\nThe bottom-up approach was just to get an idea of how the parameters are changing when the number of variables is increasing. More generally, we first build a model using all and then try to improve the model by dropping some of them.\n","10353bf4":"**MLR Model - 4**\n\nRebuilding the model without `hum`","9d9dbb16":"### <font color = Green > Business Goal <\/font>\n\n+ You are required to model the demand for shared bikes with the available independent variables. It will be used by the management to understand how exactly the demands vary with different features. \n+ They can accordingly manipulate the business strategy to meet the demand levels and meet the customer's expectations. Further, the model will be a good way for management to understand the demand dynamics of a new market. ","733f3dae":"### Initial Steps before Model building\n\nBefore model building, you first need to perform the test-train split and scale the features.","10c21955":"### Step - 7:  Final Report:\n\n1. Our top 3 variables that are seen effecting the bike rental service count are: \n\n+ **Temperature (temp) with a coefficient of 0.5211:** This means a unit increase in temp variable increases the bike hire numbers by 0.5221 units.\n+ **Weather3 i.e Light Snowy or Raing with Scatered Clouds and Thunderstorm (weathersit_3) with a coefficient of 0.2786:** This means a unit increase in weather_3 variable increases the bike hire numbers by 0.2786 units. \n+ **Year (yr) with a coefficient of 0.2328:** This means a we can clearly see a YOY increase in demand for the rental bike services. \n\n\n2. Assumptions:\n\n+ After graphing the error terms on the train data with help of the histogram plot, we witness that the error terms are normally distributed. \n+ The accuracy of the train and test set are nearly equal. r2 of the train set is around 83% and r2 of the test set is around 79%.\n+ After Plotting y_test and y_pred on a scatter plot, we can see that there is a Linear Relationship.\n\n","ef54621e":"**`hum` is having high VIF; can be dropped**","b2d0dc26":"### Bi\/Multivariate Analysis","c6ecea08":"**Linear Model: 1**","8c675f3d":"**MLR Model - 3**","57a2164e":"### Task - 1: Reading and Cleaning the dataset ","d666dbb5":"**MLR Model - 1**","e2219610":"### Step 6: Residual Analysis of the train data\n\nSo, now to check if the error terms are also normally distributed (which is infact, one of the major assumptions of linear regression), let us plot the histogram of the error terms and see what it looks like.","e07b8cc0":"From the lm2_model summary, it is evident that all our coefficients are not equal to zerowhich means We REJECT the NULL HYPOTHESIS\n\n**F Statistics\u00b6**\n\nF-Statistics is used for testing the overall significance of the Model: Higher the F-Statistics, more significant the Model is.\n\nF-statistic: 270.8\nProb (F-statistic): 6.57e-186\n\nThe F-Statistics value of 270 (which is greater than 1) and the p-value of '~0.0000' states that the overall model is significant","5f5c0565":"#### Creating Dummy Variables\n    \n+ season\n+ mnth\n+ workingday\n+ weathersit\n\nWe will not keep weekday as it is a week predictor","2e802756":"As we can see there are alot of predictors in the training set which are also correlated to each other and will cause the concern of Multicolinearity henceforth. \n\nLet's use **RFE** to identify the best variables.","4cfe4a24":"### <font color = Green > Inference: <\/font>\n\n1. You have the coef of all the variables as well as constant. \n\n2. `mnth_10` has a P-value of 0.023, which is workable as it should be less than 0.05; but we will dropped to make the model stronger.\n\n3. Again, our model is strong as the r2 and adjusted r2 is same around 83%","30adfe60":"### <font color = Green > Inference: <\/font>\n\n+ As we can clearly see, Casual Users is highly skewed as it is greater than 1 and posses outliers too.\n+ Registered users does not posses oultiers and seems to be near 0 in terms of skewness. \n+ Also, the overall count of Casual users are less as compare to Registered ones.\n+ Count being the total of both, seems to be a perfect shape of boxplot, it's skewness is between -0.5 and 0.5, which denotes it is approximatey symmetric.\n","d3ce26a7":"### Task - 2: Understanding the dataset ","08bf2fbb":"### <font color = Green > Inference: <\/font>\n\n1. You have the coef of all the variables as well as constant. \n\n2. Similarly like above, our P-values of all coefficient is less than 0.05; hence we can say our model is significant.\n\n3. But now, our r2 and adjusted r2 is not same around 83%, but it's fine\n\n+ But, let's look at our VIF values","3d94ed49":"### Model Evaluation","e58467f2":"We can see that the equation of our best fitted line is:\n\n$ cnt = 0.1264 + (yr \\times 0.2328) + (temp \\times 0.5211) \u2212 (windspeed \\times 0.1516) + (season2 \\times 0.1016) + (season4 \\times 0.1374) + (mnth_8 \\times 0.0557) + (mnth_9 \\times 0.1133) \u2212 (weathersit2 \\times 0.0809) \u2212 (weathersit3 \\times 0.2786) $","68cd8a6a":"## MLR model for the prediction of demand for shared bikes","e4a515df":"### <font color = Green > Inference: <\/font>\n\n+ Comparing this with the above categorical variables against cnt (target variable), we can conclude that that the temp will be the strongest predictor.","e71dbd52":"#### Applying the scaling on the test sets","29d86291":"### <font color = Green > Final Inference of the Model: <\/font>\n\n**All the variables have VIF less than or equal to 5 and it's perfect as they are also significant!**\n\n+ We cannot drop Tempearture as it is the most significant and correlated variable to our dependent variable. \n","270716b1":"**Linear Model: 2**","a3411b0a":"### <font color = Green > Inference: <\/font>\n\n+ Since temp has the Highest Coefficient, we will start off with temp variable. \n+ This is followed by, yr, season_3, season_2 and months 6,7,8,9.\n+ We also, some variables higly correlating to each other, and we will check not to consider them.","060d3c15":"### <font color = Green > Inference: <\/font>\n\n1. Significance can be seen by the P-values which should be less than 0.05 which means you can reject the Null and Alternate is true. P-values in our First model are 0, hence the model is significant with atemp.\n2. R-squared is 41%, which means, 41% variance in 'cnt' is explained by atemp (feeling temperature). Which is not very bad but we need to improve the model. Hence, let's add another variable now. ","e676192a":"### Univariate Analysis","58978d5b":"### <font color = Green > Inference: <\/font>\n+ The data is equally distributed in both the years.\n","a1488e99":"+ As verified, more number of bookings were made on the working day and not on non-working days.","e9af7210":"### <font color = Green > Inference: <\/font>\n+ 70% of the days were working.\n+ 30% of the days were holiday.\n+ Out of 730 days, 504 were working days and 226 were not working days.","2a997b13":"### <font color = Green > Inference: <\/font>\n\n1. Just like above the p-values are o, hence the predictors added are significant but we can see that the variable season_3 added now has a negative coefficient. \n2. Moreover, the model has become more stronger as our r2 has increased up 70% and seems to be at our best. But, this is the most tedious way to create a model and hence we will adapt to a new approach followed bellow. \n","e984a3b7":"### <font color = Green > Inference: <\/font>\n\n+ There are no Null values, 4 float values, 11 integer values and 1 object.\n+ There are no duplicate values as the shape of the orginal dataframe and duplicate is same.\n+ dteday is not only the categorical column explicitly. \n+ season, yr, mnth, holiday, weekday, workingday, weathersit are actually categorical columns with labels.","5c5da293":"### <font color = Green > Inference: <\/font>\n\n+ There are no unwanted values present that needs to be converted","5119845c":"+ Turns out, temperature is also negatively skewed but it's skewness is between -0.5 and 0.5, which denotes it is approximatey symmetric.","901b78af":"### <font color = Green > Inference: <\/font>\n\n+ **season**: We can see that the season 3 i.e Fall has made the highest number of bookings, followed by season 2(Summer) and season 4(winter).\n+ **yr**: We can clearly see that the number of bookings made in the subsequent year 2019 is more that the year 2018. That means the demand is increasing. \n+ **mnth**: Higher number of bookings were made in the month (5,6,7,8,9 and 10) in both the years. \n+ **weekday**: This variable seems to have equal amount of bookings made in all days of the week, hence will not be that impactful predictor.\n+ **workingday**: This was the most suprising thing to notice that the higher number of bookings either causal or registered were made on the non-working day, will verify this below. \n+ **weathersit**: We can see that the weather 1 (Clear\/Partly Cloudy Sky) has made the highest number of bookings. This is followed by weather 2 (Misty\/Cloudy sky), could be a good predictor too. \n\n","27128b99":"### <font color = Green > Inference: <\/font>\n\n+ Registered Users is Highly Correlated with the target variable followed by Casual Users which is not suprising as cnt is the total of both the features. \n+ Also, temp is highly correlated to atemp hence dropping atemp.\n+ Whereas temp or you could say atemp are the top correlated variables with the target variable. That means, temperature of the day is the biggest factor in the demand for using shared bike services. \n+ Humidity and Windspeed seems not to be correlated with the target variable at all. ","f13c0ce0":"### <font color = Green > Problem Statement <\/font>\n\n+ A US bike-sharing provider **BoomBikes** has recently suffered considerable dips in their revenues due to the ongoing Corona pandemic. The company is finding it very difficult to sustain in the current market scenario. \n+ So, it has decided to come up with a mindful business plan to be able to accelerate its revenue as soon as the ongoing lockdown comes to an end, and the economy restores to a healthy state. \n+ In such an attempt, BoomBikes aspires to understand the demand for shared bikes among the people after this ongoing quarantine situation ends across the nation due to Covid-19. \n+ They have planned this to prepare themselves to cater to the people's needs once the situation gets better all around and stand out from other service providers and make huge profits.\n+ Specifically, they want to understand the factors affecting the demand for these shared bikes in the American market. The company wants to know:\n    + **Which variables are significant in predicting the demand for shared bikes.**\n    + **How well those variables describe the bike demands**\n+ Based on various meteorological surveys and people's styles, the service provider firm has gathered a large dataset on daily bike demands across the American market based on some factors.","df9969d0":"**MLR Model - 2**","0a16c1ee":"### Task - 5: Training the Model","f9c22dd4":"**Rescaling the Features**","0843bcb7":"**Building the Model - I**\n","387c7930":"### <font color = Green > Steps Followed <\/font>\n\n1. Reading and Cleaning the Data\n2. Understanding the Data\n3. EDA\n4. Preparing the Data for modelling (tarin-test split, dummy and rescalling etc)\n5. Training the model\n6. Residual Analysis\n7. Predictions and Evaluation on the test set\n8. Final Report","7bd84bd2":"**While we Keep on adding the variable, there are two important things to look at after adding one variable**\n\n+ statistical significance of that variable\n+ VIF","5ae93140":"### Task - 4: Preparing the Data for Modelling","925f87cb":"### <font color = Green > Potential Causes for the shared bikes demand could be: <\/font>\n\n1. Time & Season of the year\n2. Special Holidays\n3. Weather conditions\n4. Working day or Non-Working day\n5. Ratio of casual vs registered users\n\n**Target variable is cnt: count of total rental bikes including both casual and registered**","483761ef":"### <font color = Green > Inference: <\/font>\n\n+ We need to witness the temp change in comparision to both the years i.e 2018 and 2019.\n+ Humidity seems to be a little Negatively skewed. Their mean and median is also the same. \n+ Similarly, windspeed seems to be a little Positvely skewed. Their mean and median is also the same. ","2a365bb8":"**Linear Model: 3**","e9c492e1":"### <font color = Green > Inference: <\/font>\n\n1. Similarly, like above the p-values are o, hence the predictors are still significant\n2. Moreover, the model has become more stronger as our r2 has increased up 69%. \n\nLet's add one more variable.\n \n","0ab319ed":"### <font color = Green > Inference: <\/font>\n\n+ Everything seems fine\n","ae4f9783":"**Splitting the Data set**","7e9d8306":"### Step 7: Making Predictions and Evaluation on the test set \n","bd4796a2":"### <font color = Green > Inference: <\/font>\n\n+ Season seems to be equally distributed in the both the years with equal time momentum. \n+ As there are 24 months, each season lasted for 6 months (25%) in 24 months, that means 3 month each in one year.\n","35f35f0c":"#### Dividing into X_test and y_test","a4dc2ec3":"### Task - 3: Exploatory Data Analysis","63e3f450":"### <font color = Green > Inference: <\/font>\n\n**We usually drop a variable by looking at its significance i.e P-value or we can supplement our decision by VIF**\n\n1. You have the coef of all the variables as well as cosntant. \n\n2. `season_3` has a P-value of 0.184, which is not workable and should be less than 0.05; hence we will dropped to make the model stronger. \n\n3. if see, our model is strong as the r2 and adjusted r2 is same around 83%\n"}}