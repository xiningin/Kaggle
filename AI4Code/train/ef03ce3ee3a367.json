{"cell_type":{"1c7106fc":"code","903515a7":"code","3dc03f87":"code","28f83459":"code","d2a62546":"code","ccd53c31":"code","615d43ed":"code","838a5476":"code","31d96d1c":"code","32fa027b":"code","79f53c98":"code","bb1804ed":"code","8aec3a18":"code","4d1467f7":"code","cabfcf6e":"code","09367492":"code","2b4ceeb3":"code","c24ebe5a":"code","491d7ee3":"code","0885ba0d":"code","5cfb2084":"code","ab8d37f1":"code","ee2c4ff4":"code","3e2d3ab2":"code","33312982":"code","3ec25bef":"code","28b938de":"code","50e0fffd":"code","ac10385d":"code","6e9c803e":"code","cfde4f31":"code","879383ed":"code","17fe3143":"code","9c5d3369":"code","8dbd858d":"code","999f9bb9":"code","fd493feb":"code","622c0677":"code","711326f9":"code","fb10ba27":"code","d6b30671":"code","cc9de927":"code","7f6d66a0":"code","51ea22a2":"code","aa6fc3d1":"code","2f3d34de":"code","4e6c82c9":"code","c3000881":"code","3a71875a":"code","69a5c0e9":"code","63f1dcaf":"code","f26048b8":"code","7b85ae75":"code","a08028e0":"code","7bd0a11b":"code","66cbfa7d":"code","c2a6e02d":"code","31ebbc24":"code","6cb7c80e":"code","3975eadc":"code","073a7bb9":"code","53aee88c":"code","95be3886":"code","ecc807dd":"code","41e21c3d":"code","0f7ef250":"code","ab83eaa0":"code","205091e7":"code","297f2f6c":"code","6d2dba17":"code","88ea1cfb":"code","5fa28b1a":"code","86376882":"code","1b896561":"code","7c9b3f40":"code","96d6aff9":"code","4696eae2":"code","7bfd99f8":"code","82e4d602":"code","29ce48b7":"code","c4f980f9":"code","8f9bef9e":"code","448ff67c":"code","3b3aeac0":"code","039cf713":"code","9fcecf9c":"code","164eb365":"code","1cb9dc94":"code","2191bb9f":"code","3968acb9":"code","bbdee91b":"code","1f956296":"code","98ce04d8":"code","625fac55":"code","75b7df65":"code","6178b003":"code","1154686a":"code","f5f33498":"code","35f0e9c7":"code","64d94c59":"code","31da438f":"code","a1aa60d8":"code","65347301":"code","a22ac17a":"code","6e79a1e8":"code","bfda4796":"code","2f34e3cc":"code","42716b70":"code","298a282f":"code","3425ac98":"code","9f3bc9cc":"code","0fb4bdd2":"code","fa824dad":"code","41dbd6ac":"code","db8dc0eb":"code","bab27aeb":"code","77f5e0df":"code","bfa7783a":"code","fae6789d":"code","03a3c9bd":"code","51504534":"code","26a6bd41":"code","657496d1":"code","e0a3fee8":"code","e470bede":"code","f430c367":"code","1f88e3cf":"code","7df5f7d4":"code","9e121481":"code","a44f8a51":"code","3b7476d4":"code","2327b270":"code","2e4a439e":"markdown","e44a8b60":"markdown","aaf4724a":"markdown","e6bfadfb":"markdown","e4d6166d":"markdown","2ddc4148":"markdown","97b97ccf":"markdown","cabb1e9f":"markdown","5d6a7351":"markdown","fcaf13fa":"markdown","3b634531":"markdown","4cc522fb":"markdown","d7785392":"markdown","30b77948":"markdown","7b767212":"markdown","1fb6ab77":"markdown","1a07e71f":"markdown","89847272":"markdown","3f5afa3c":"markdown","b26acc12":"markdown","81b59396":"markdown","88778c62":"markdown","75cc2906":"markdown","f520391e":"markdown","a8ab476a":"markdown","fa4d5c8e":"markdown","7281cf35":"markdown","ff834086":"markdown","8f65071b":"markdown","61624b03":"markdown","efe2caa5":"markdown","3ac51b47":"markdown","fa6ed1be":"markdown","31b7fb73":"markdown","4d633054":"markdown","abb6ec4e":"markdown","7fc20642":"markdown","02ae99cf":"markdown","df565585":"markdown","044aec76":"markdown","597e8a32":"markdown","28e563fa":"markdown","716ac5c4":"markdown","55ab5a8f":"markdown","c6d0583c":"markdown","1215de8e":"markdown","2862320e":"markdown","74264035":"markdown","f982b956":"markdown","6343bf7e":"markdown","8a2a16a9":"markdown","31c24adc":"markdown","c03264aa":"markdown","57c95f95":"markdown","65c9a5d2":"markdown","f2f0c4e5":"markdown","4efead33":"markdown","078ab264":"markdown","662ce1bf":"markdown","15b0df55":"markdown","f01b4247":"markdown","a20fba52":"markdown","c78b6275":"markdown","f5b842ec":"markdown","64e9cc93":"markdown","3c0c142a":"markdown","ea71e57b":"markdown","6770be7a":"markdown","fe3cc454":"markdown","b7da8bbb":"markdown","339f73aa":"markdown","7c841d75":"markdown","d8c26ada":"markdown","5647801a":"markdown","5cd73253":"markdown","00611bc8":"markdown","f0820d09":"markdown","4c4035db":"markdown","e4973adf":"markdown","d365a7c1":"markdown","f1e1f7c0":"markdown","16cdc581":"markdown","2b0aa128":"markdown","6d2e85cf":"markdown","9d5dbf38":"markdown","4f9ede43":"markdown","f675155a":"markdown","20247773":"markdown","9e06acd7":"markdown","d66952e3":"markdown","2102d6fe":"markdown","ab233320":"markdown","ee4f5a56":"markdown","07a796a5":"markdown","11b01b82":"markdown","83abfaab":"markdown","288ce966":"markdown","76586f48":"markdown","403773b6":"markdown","ae701eb3":"markdown","51668c66":"markdown","f4313475":"markdown","bec971a9":"markdown","f6ec05f8":"markdown","f991db3c":"markdown","432104a9":"markdown","5ee2dc61":"markdown","2ec9373c":"markdown","c868486f":"markdown","5f68d822":"markdown"},"source":{"1c7106fc":"from IPython.display import Image\nfrom IPython.core.display import HTML \nImage(url= \"https:\/\/www.esquireme.com\/public\/images\/2019\/11\/03\/airbnb-678x381.jpg\", width=2500, height=500)\n","903515a7":"from warnings import filterwarnings\nfilterwarnings('ignore')\n\nfrom scipy.stats import norm\n# 'Os' module provides functions for interacting with the operating system \nimport os\n\n# 'Pandas' is used for data manipulation and analysis\nimport pandas as pd \n\n# 'Numpy' is used for mathematical operations on large, multi-dimensional arrays and matrices\nimport numpy as np\n\n# 'Matplotlib' is a data visualization library for 2D and 3D plots, built on numpy\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# 'Seaborn' is based on matplotlib; used for plotting statistical graphics\nimport seaborn as sns\n\n# 'Scikit-learn' (sklearn) emphasizes various regression, classification and clustering algorithms\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import ElasticNet\n\n# 'Statsmodels' is used to build and analyze various statistical models\nimport statsmodels\nimport statsmodels.api as sm\nimport statsmodels.stats.api as sms\nfrom statsmodels.tools.eval_measures import rmse\nfrom statsmodels.compat import lzip\nfrom statsmodels.graphics.gofplots import ProbPlot\n\n# 'SciPy' is used to perform scientific computations\nfrom scipy.stats import f_oneway\nfrom scipy.stats import jarque_bera\nfrom scipy import stats","3dc03f87":"df = pd.read_csv('..\/input\/airbnb-new-york\/AB_NYC_2019.csv')","28f83459":"df_copy = df.copy()\ndf.head()","d2a62546":"df.shape","ccd53c31":"# Understanding types\ndf.info()","615d43ed":"df.isnull().sum()","838a5476":"df.describe()","31d96d1c":"# Insight about unique values\ndf.nunique()","32fa027b":"# Making sure if any duplicatd values.\ndf.duplicated().sum()","79f53c98":"df.host_id.value_counts().iloc[:5]","bb1804ed":"df.host_id.value_counts().iloc[:5].plot(kind = 'barh')","8aec3a18":"# we noted that the room_type is only of 3 particular types.\ndf['room_type'].value_counts()","4d1467f7":"fig = plt.figure(figsize=(5,5), dpi=80)\ndf['room_type'].value_counts().plot(kind='pie',  autopct='%1.0f%%', startangle=360, fontsize=13)","cabfcf6e":"# There are 5 particular neighbourhood_group, which means 5 unique locations.\ndf['neighbourhood_group'].value_counts()","09367492":"fig = plt.figure(figsize=(5,5), dpi=80)\ndf['neighbourhood_group'].value_counts().plot(kind='pie',  autopct='%1.0f%%', startangle=360, fontsize=13)","2b4ceeb3":"df['neighbourhood'].value_counts().iloc[:5]","c24ebe5a":"df['neighbourhood'].unique()","491d7ee3":"fig = plt.figure(figsize=(5,5), dpi=80)\ndf['neighbourhood'].value_counts().iloc[:5].plot(kind='pie',  autopct='%1.0f%%', startangle=360, fontsize=13)","0885ba0d":"df.price.value_counts().iloc[:10]","5cfb2084":"df.price.value_counts().iloc[:10].plot(kind = 'bar')","ab8d37f1":"df.price.describe()","ee2c4ff4":"df[df['price'] == 10000.000000]","3e2d3ab2":"df['minimum_nights'].value_counts()","33312982":"for i in range(1,11):\n  print(\"Number of nights: \",i)\n  print(\"Amount of trasactions:\",len(df[df['minimum_nights'] == i]))","3ec25bef":"df['minimum_nights'].value_counts().iloc[:4].plot(kind = 'barh')","28b938de":"df['number_of_reviews'].value_counts()","50e0fffd":"df[df['number_of_reviews'] == 607]","ac10385d":"df['availability_365'].value_counts()","6e9c803e":"df[df['availability_365'] == 365].describe()","cfde4f31":"df_copy[df_copy['reviews_per_month'] > 1].reviews_per_month.value_counts().sum()                 ","879383ed":"df[df['reviews_per_month'] > 1]['reviews_per_month'].value_counts().iloc[:5]            ","17fe3143":"df['reviews_per_month'].max()","9c5d3369":"df_copy[df['reviews_per_month'] == 58.5]","8dbd858d":"df.calculated_host_listings_count.value_counts().iloc[:5]","999f9bb9":"df.calculated_host_listings_count.value_counts().iloc[:5].plot(kind = 'bar')","fd493feb":"df.calculated_host_listings_count.describe()","622c0677":"corr = df.corr().drop([\"price\"], axis = 1)\ncorr = corr.drop([\"price\"], axis = 0)\nplt.figure(figsize=(15,8))\nsns.heatmap(corr, annot=True)","711326f9":"df['neighbourhood_group'].value_counts()","fb10ba27":"plt.figure(figsize=(10,6))\nsns.scatterplot(df.longitude,df.latitude,hue=df.neighbourhood_group)","d6b30671":"df['room_type'].value_counts()","cc9de927":"plt.figure(figsize=(10,6))\nsns.scatterplot(df.longitude,df.latitude,hue=df.room_type)","7f6d66a0":"plt.figure(figsize=(10,6))\nsns.scatterplot(df_copy.longitude,df_copy.latitude,hue=df_copy.availability_365)","51ea22a2":"plt.figure(figsize=(10,6))\nsns.countplot(data = df, x = 'room_type', hue = 'neighbourhood_group')","aa6fc3d1":"plt.figure(figsize=(10,6))\nax = sns.boxplot(data=df_copy, x='neighbourhood_group',y='availability_365',palette='plasma')","2f3d34de":"v2=sns.violinplot(data=df[df.price < 200], x='neighbourhood_group', y='price')\nv2.set_title('Density and distribution of prices for each neighberhood_group')","4e6c82c9":"df['neighbourhood'].value_counts().iloc[:10]","c3000881":"neighb =df.loc[df['neighbourhood'].isin(['Williamsburg','Bedford-Stuyvesant','Harlem','Bushwick','Upper West Side','Hell\\'s Kitchen','East Village','Upper East Side','Crown Heights','Midtown'])]\npl =sns.catplot(x='neighbourhood', hue='neighbourhood_group', col='room_type', data=neighb, kind='count')\npl.set_xticklabels(rotation=90)","3a71875a":"# Rooms with top 100 reviews by neighbourhood\ndfr=df.sort_values(by=['number_of_reviews'],ascending=False).head(100)\ndfr['neighbourhood_group'].value_counts().plot(kind = 'barh')","69a5c0e9":"# Rooms with top 100 expensive by neighbourhood\ndfr=df.sort_values(by=['price'],ascending=False).head(100)\ndfr['neighbourhood_group'].value_counts().plot(kind = 'barh')","63f1dcaf":"# Rooms with top 100 minimum_nights  by neighbourhood\ndfr=df.sort_values(by=['minimum_nights'],ascending=False).head(100)\ndfr['neighbourhood_group'].value_counts().plot()","f26048b8":"plt.figure(figsize=(10,10))\nsns.scatterplot(x=\"room_type\", y=\"price\",\n            hue=\"neighbourhood_group\", size=\"neighbourhood_group\",\n            sizes=(50, 200), palette=\"Dark2\", data=df)\n\nplt.xlabel(\"Room Type\", size=13)\nplt.ylabel(\"Price\", size=13)\nplt.title(\"Room Type vs Price vs Neighbourhood Group\",size=15, weight='bold')","7b85ae75":"## We can remove the unwanted columns. Here id,name , host_name and last_review doesnt help us in anyway in our approch for data analysis.\ndf.drop(['host_id','name','latitude','longitude','id','host_name','last_review'], axis=1, inplace=True)\ndf.head()","a08028e0":"df.isna().sum()","7bd0a11b":"df['reviews_per_month'].describe()","66cbfa7d":"df['reviews_per_month'] = df['reviews_per_month'].fillna(0)\n# Missing value implies there are no reviews for the location.","c2a6e02d":"df.isna().sum()","31ebbc24":"df = pd.read_csv('..\/input\/airbnb-new-york\/AB_NYC_2019.csv')\nplt.rcParams['figure.figsize']=(15,8)\n\n# recheck for outliers\n# column: selects the specifies columns\ndf.boxplot(column=['minimum_nights',\n 'number_of_reviews','reviews_per_month','calculated_host_listings_count','availability_365'])                  \n\n# display only the plot\nplt.show()\nfig=sns.boxplot(df['price'])\nfig","6cb7c80e":"df_numeric_features=df.select_dtypes(include=np.number)\nQ1 = df_numeric_features.drop(['price'], axis=1).quantile(0.25)\n\n# compute the first quartile using quantile(0.75)\n# use .drop() to drop the target variable \n# axis=1: specifies that the labels are dropped from the columns\nQ3 = df_numeric_features.drop(['price'], axis=1).quantile(0.75)\n\n# calculate of interquartile range \nIQR = Q3 - Q1\n\n# print the IQR values for numeric variables\nprint(IQR)","3975eadc":"df = df[~((df < (Q1 - 1.5 * IQR)) | (df > (Q3 + 1.5 * IQR))).any(axis=1)]","073a7bb9":"df.shape","53aee88c":"dfp = df[\"price\"][df[\"price\"] < 250]\ndfp[dfp > 20].hist()\n","95be3886":"df=df[df[\"price\"]<250]\ndf=df[df[\"price\"]>20]\n# df['room_type'].value_counts()\n","ecc807dd":"fig = sns.boxplot(df[\"price\"])\nfig","41e21c3d":"df.fillna({'reviews_per_month':0}, inplace=True)\ndf.drop(['id','host_id','latitude','longitude','host_name','last_review','name'], axis = 1,inplace=True)\ndf = pd.get_dummies(df, columns=['neighbourhood_group',\"room_type\"], prefix = ['ng',\"rt\"],drop_first=True)\ndf.drop([\"neighbourhood\"], axis=1, inplace=True)\ndf.head()","0f7ef250":"plt.figure(figsize=(15,12))\npalette = sns.diverging_palette(20, 220, n=256)\ncorr=df.corr(method='pearson').drop([\"price\"], axis = 1)\ncorr=corr.drop([\"price\"])\nsns.heatmap(corr, annot=True, fmt=\".2f\", cmap=palette)\nplt.title(\"Correlation Matrix\",size=15, weight='bold')","ab83eaa0":"plt.figure(figsize=(5,5))\nsns.distplot(df['price'], fit=norm)\nplt.title(\"Price Distribution Plot\",size=15, weight='bold')","205091e7":"df.head()","297f2f6c":"df1=df.copy()","6d2dba17":"df1['price_log'] = np.log(df.price+1)\ndf1=df1.drop([\"price\"],axis=1)","88ea1cfb":"plt.figure(figsize=(12,10))\nsns.distplot(df1['price_log'], fit=norm)\nplt.title(\"Log-Price Distribution Plot\",size=15, weight='bold')","5fa28b1a":"X = df1.drop(['price_log'],axis=1)\ny = df1[['price_log']]\nX=sm.add_constant(X)\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.3,random_state=1)\nX_train_log,X_test_log,y_train_log,y_test_log=X_train,X_test,y_train,y_test\nMLR=sm.OLS(y_train,X_train).fit()\nMLR.summary()\n","86376882":"def get_train_rmse(model):\n    \n    # For training set:\n    # train_pred: prediction made by the model on the training dataset 'X_train'\n    # y_train: actual values ofthe target variable for the train dataset\n\n    # predict the output of the target variable from the train data \n    train_pred = model.predict(X_train)\n\n    # calculate the MSE using the \"mean_squared_error\" function\n\n    # MSE for the train data\n    mse_train = mean_squared_error(y_train, train_pred)\n\n    # take the square root of the MSE to calculate the RMSE\n    # round the value upto 4 digits using 'round()'\n    rmse_train = round(np.sqrt(mse_train), 4)\n    \n    # return the training RMSE\n    return(rmse_train)","1b896561":"def get_test_rmse(model):\n    \n    # For testing set:\n    # test_pred: prediction made by the model on the test dataset 'X_test'\n    # y_test: actual values of the target variable for the test dataset\n\n    # predict the output of the target variable from the test data\n    test_pred = model.predict(X_test)\n\n    # MSE for the test data\n    mse_test = mean_squared_error(y_test, test_pred)\n\n    # take the square root of the MSE to calculate the RMSE\n    # round the value upto 4 digits using 'round()'\n    rmse_test = round(np.sqrt(mse_test), 4)\n\n    # return the test RMSE\n    return(rmse_test)","7c9b3f40":"print('RMSE on train set: ', get_train_rmse(MLR))\nprint('RMSE on test set: ', get_test_rmse(MLR))\ndifference = abs(get_test_rmse(MLR) - get_train_rmse(MLR))\n","96d6aff9":"MLR_predict =MLR.predict(X_test)\nMLR_predict\n","4696eae2":"actual_price =y_test['price_log']\nactual_price","7bfd99f8":"linreg_full_model_rmse = rmse(actual_price, MLR_predict)\nprint(linreg_full_model_rmse)\nlinreg_full_model_rsquared = MLR.rsquared\nlinreg_full_model_rsquared_adj = MLR.rsquared_adj ","82e4d602":"# create dataframe 'score_card'\n# columns: specifies the columns to be selected\nscore_card = pd.DataFrame(columns=['Model_Name', 'R-Squared', 'Adj. R-Squared', 'RMSE'])\n\n# print the score card\nscore_card","29ce48b7":"linreg_full_model_metrics = pd.Series({\n                     'Model_Name': \"Linreg full model with log of target variable\",\n                     'RMSE':linreg_full_model_rmse,\n                     'R-Squared': linreg_full_model_rsquared,\n                     'Adj. R-Squared': linreg_full_model_rsquared_adj     \n                   })\nscore_card = score_card.append(linreg_full_model_metrics, ignore_index=True)\nscore_card","c4f980f9":"linreg_full_model_pvalues = pd.DataFrame(MLR.pvalues, columns=[\"P-Value\"])\nlinreg_full_model_pvalues","8f9bef9e":"insignificant_variables = linreg_full_model_pvalues[(linreg_full_model_pvalues['P-Value']  > 0.05)]\ninsigni_var = insignificant_variables.index\ninsigni_var = insigni_var.to_list()\ninsigni_var","448ff67c":"X=df.drop(['price'],axis=1)\ny=df[['price']]\nX=sm.add_constant(X)\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.3,random_state=1)\nMLR2=sm.OLS(y_train,X_train).fit()\nMLR2.summary()","3b3aeac0":"print('RMSE on train set: ', get_train_rmse(MLR2))\nprint('RMSE on test set: ', get_test_rmse(MLR2))\n","039cf713":"MLR2_predict =MLR2.predict(X_test)\nMLR2_predict","9fcecf9c":"actual_price = y_test['price']\nactual_price","164eb365":"linreg_full_model_withoutlog_pvalues = pd.DataFrame(MLR2.pvalues, columns=[\"P-Value\"])\nlinreg_full_model_withoutlog_pvalues","1cb9dc94":"insignificant_variables = linreg_full_model_withoutlog_pvalues[linreg_full_model_withoutlog_pvalues['P-Value']  > 0.05]\ninsigni_var = insignificant_variables.index\ninsigni_var = insigni_var.to_list()\ninsigni_var","2191bb9f":"linreg_full_model_withoutlog_rmse = rmse(actual_price, MLR2_predict)\n\nlinreg_full_model_withoutlog_rsquared = MLR2.rsquared\n\nlinreg_full_model_withoutlog_rsquared_adj = MLR2.rsquared_adj ","3968acb9":"print(linreg_full_model_withoutlog_rmse)","bbdee91b":"linreg_full_model_withoutlog_metrics = pd.Series({\n                     'Model_Name': \"Linreg full model without log of target variable\",\n                     'RMSE':linreg_full_model_withoutlog_rmse,\n                     'R-Squared': linreg_full_model_withoutlog_rsquared,\n                     'Adj. R-Squared': linreg_full_model_withoutlog_rsquared_adj     \n                   })\nscore_card = score_card.append(linreg_full_model_withoutlog_metrics, ignore_index=True)\n","1f956296":"score_card","98ce04d8":"df_significant = df1.drop(insigni_var, axis=1)\ndf_significant.head()","625fac55":"df_significant = sm.add_constant(df_significant)\n\nX = df_significant.drop(['price_log'], axis=1)\ny = df_significant[[\"price_log\"]]\n\nX_train_significant, X_test_significant, y_train, y_test = train_test_split(X, y, random_state=1)","75b7df65":"MLR3=sm.OLS(y_train,X_train_significant).fit()\nMLR3.summary()","6178b003":"MLR3_predict =MLR3.predict(X_test_significant)\nMLR3_predict","1154686a":"actual_price = y_test['price_log']\nactual_price","f5f33498":"linreg_model_with_significant_var_rmse = rmse(actual_price, MLR3_predict)\nlinreg_model_with_significant_var_rsquared =MLR3.rsquared\nlinreg_model_with_significant_var_rsquared_adj =MLR3.rsquared_adj ","35f0e9c7":"linreg_model_with_significant_var_metrics = pd.Series({\n                     'Model_Name': \"Linreg full model with significant variables\",\n                     'RMSE': linreg_model_with_significant_var_rmse,\n                     'R-Squared': linreg_model_with_significant_var_rsquared,\n                     'Adj. R-Squared': linreg_model_with_significant_var_rsquared_adj     \n                   })\n\nscore_card = score_card.append(linreg_model_with_significant_var_metrics, ignore_index=True)\nscore_card\n","64d94c59":"name = ['f-value','p-value']           \ntest = sms.het_breuschpagan(MLR3.resid, MLR3.model.exog)\nlzip(name, test) ","31da438f":"fig, ax = plt.subplots(nrows = 2, ncols= 5, figsize=(25, 20))\nfor variable, subplot in zip(X_train_log.columns[1:], ax.flatten()):\n    sns.scatterplot(X_train_log[variable], MLR3.resid , ax=subplot)\nplt.show()","a1aa60d8":"plt.figure(figsize=(7,7))\nstats.probplot(df1['price_log'], plot=plt)\nplt.show()","65347301":"MLR3.resid.mean()","a22ac17a":"df_scaled = df1.apply(lambda rec: (rec - rec.mean()) \/ rec.std())\ndata_scaled =sm.add_constant(df_scaled)","6e79a1e8":"mean_numeric_features = df1.mean()\nstd_numeric_features =df1.std()","bfda4796":"X = data_scaled.drop(['price_log'], axis=1)\ny = data_scaled['price_log']\nX_train_scaled, X_test_scaled, y_train_scaled, y_test_scaled = train_test_split(X, y, random_state=1)","2f34e3cc":"linreg_model_with_significant_scaled_vars = sm.OLS(y_train_scaled,X_train_scaled).fit()\nprint(linreg_model_with_significant_scaled_vars.summary())","42716b70":"predicted_price = linreg_model_with_significant_scaled_vars.predict(X_test_scaled)\nactual_price = y_test['price_log']","298a282f":"linreg_model_with_significant_scaled_vars_rmse = rmse(actual_price, predicted_price)\nlinreg_model_with_significant_scaled_vars_rsquared = linreg_model_with_significant_scaled_vars.rsquared\nlinreg_model_with_significant_scaled_vars_rsquared_adj = linreg_model_with_significant_scaled_vars.rsquared_adj","3425ac98":"linreg_model_with_significant_scaled_vars_metrics = pd.Series({\n                     'Model_Name': \"Linreg with scaled significant variables\",\n                     'RMSE': linreg_model_with_significant_scaled_vars_rmse,\n                     'R-Squared': linreg_model_with_significant_scaled_vars_rsquared,\n                     'Adj. R-Squared': linreg_model_with_significant_scaled_vars_rsquared_adj     \n                   })\nscore_card = score_card.append(linreg_model_with_significant_scaled_vars_metrics, ignore_index = True)\nscore_card","9f3bc9cc":"df","0fb4bdd2":"df_interaction = df1.copy()\ndf_interaction['number_of_reviews*minimum_nights'] = df_interaction['number_of_reviews']*df_interaction['minimum_nights'] \ndf_interaction.head()","fa824dad":"X = df_interaction.drop(['price_log'], axis=1)\ny = df_interaction['price_log']\nX=sm.add_constant(X)\nX_train_interaction, X_test_interaction, y_train, y_test = train_test_split( X, y, random_state=1)","41dbd6ac":"linreg_with_interaction = sm.OLS(y_train, X_train_interaction).fit()\nprint(linreg_with_interaction.summary())","db8dc0eb":"predicted_price = linreg_with_interaction.predict(X_test_interaction)\nactual_price = y_test","bab27aeb":"linreg_with_interaction_rmse = rmse(actual_price, predicted_price)\nlinreg_with_interaction_rsquared = linreg_with_interaction.rsquared\nlinreg_with_interaction_rsquared_adj = linreg_with_interaction.rsquared_adj ","77f5e0df":"linreg_with_interaction_metrics = pd.Series({\n                     'Model_Name': \"linreg_with_interaction\",\n                     'RMSE': linreg_with_interaction_rmse,\n                     'R-Squared': linreg_with_interaction_rsquared,\n                     'Adj. R-Squared': linreg_with_interaction_rsquared_adj     \n                   })\nscore_card = score_card.append(linreg_with_interaction_metrics, ignore_index = True)\nscore_card","bfa7783a":"col = list(X_train_scaled.columns)\ncol.append('ssr')\ncol.append('R squared')\ncol.append('Adj. R squared')\ncol.append('RMSE')","fae6789d":"ridge_regression = sm.OLS(y_train_scaled, X_train_scaled)\nresults_fu = ridge_regression.fit()","03a3c9bd":"frames = []\nfor n in np.arange(0.0001,10.1, 0.1).tolist():\n\n    results_fr = ridge_regression.fit_regularized(L1_wt=0, alpha=n, start_params=results_fu.params)\n    results_fr_fit = sm.regression.linear_model.OLSResults(model=ridge_regression, \n                                             params=results_fr.params, \n                                             normalized_cov_params=ridge_regression.normalized_cov_params)\n   \n    results_fr_fit_predictions = results_fr_fit.predict(X_test_scaled)\n    results_fr_fit_rmse = rmse(y_test, results_fr_fit_predictions)    \n  \n    list_metric = [results_fr_fit.ssr, results_fr_fit.rsquared, results_fr_fit.rsquared_adj, results_fr_fit_rmse]\n    \n   \n    frames.append(np.append(results_fr.params, list_metric))\n    \n\n    df_params = pd.DataFrame(frames, columns= col)\n\n# add column names to the dataframe  \ndf_params.index=np.arange(0.0001, 10.1, 0.1).tolist()\n\n# add the first column name alpha to the data frame. \n# this column will hold the regularization parameter value which can be anything from zero to any positive number\ndf_params.index.name = 'alpha*'\n\n# display the output\ndf_params\n","51504534":"df_params.iloc[[0,1,3,4,5,-2,-1]]","26a6bd41":"alpha = df_params.RMSE[df_params.RMSE == df_params.loc[:,'RMSE'].min()].index.tolist()\nprint('The required alpha is %.4f ' % (alpha[0]))","657496d1":"results_fr = ridge_regression.fit_regularized(L1_wt=0, alpha=0.9001, start_params=results_fu.params)\nridge_regression_best = sm.regression.linear_model.OLSResults(model = ridge_regression, \n                                        params=results_fr.params, \n                                        normalized_cov_params=ridge_regression.normalized_cov_params)\n\nprint (ridge_regression_best.summary())","e0a3fee8":"predicted_price = ridge_regression_best.predict(X_test_scaled)\ny_pred_unscaled = (predicted_price * std_numeric_features.price_log) + mean_numeric_features.price_log\nactual_price = y_test\n","e470bede":"ridge_regression_best_rmse = rmse(actual_price, y_pred_unscaled)\nridge_regression_best_rsquared = ridge_regression_best.rsquared\nridge_regression_best_rsquared_adj = ridge_regression_best.rsquared_adj ","f430c367":"ridge_regression_best_metrics = pd.Series({\n                     'Model_Name': \"Ridge Regression\",\n                     'RMSE': ridge_regression_best_rmse,\n                     'R-Squared': ridge_regression_best_rsquared,\n                     'Adj. R-Squared': ridge_regression_best_rsquared_adj     \n                   })\nscore_card = score_card.append(ridge_regression_best_metrics, ignore_index = True)\nscore_card","1f88e3cf":"X=df.drop(['price'],axis=1)\ny=df[['price']]\nX=sm.add_constant(X)\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.3,random_state=1)","7df5f7d4":"from sklearn.linear_model import SGDRegressor\nlinreg_with_SGD = SGDRegressor()\nlinreg_with_SGD = linreg_with_SGD.fit(X_train, y_train)","9e121481":"linreg_with_SGD_predictions = linreg_with_SGD.predict(X_test)","a44f8a51":"\nlinreg_SGD_mse = mean_squared_error(y_test, linreg_with_SGD_predictions)\n\n# calculate rmse\nlinreg_SGD_rmse = np.sqrt(linreg_SGD_mse)\n\n# calculate R-squared\nlinreg_SGD_r_squared = r2_score(y_test, linreg_with_SGD_predictions)\n\n# calculate Adjusted R-squared\nlinreg_SGD_adjusted_r_squared = 1 - (1-linreg_SGD_r_squared)*(len(y_test)-1)\/(len(y_test)- X_test.shape[1]-1)","3b7476d4":"# compile the required information\nlinreg_full_model_SGD = pd.Series({\n                     'Model_Name': \"Linear Regression SGD\",\n                     'RMSE': linreg_SGD_rmse ,\n                     'R-Squared': linreg_SGD_r_squared,\n                     'Adj. R-Squared': linreg_SGD_adjusted_r_squared   \n                   })\n\nscore_card = score_card.append(linreg_full_model_SGD, ignore_index = True)\nscore_card","2327b270":"score_card = score_card.drop(score_card.index[[6]])\nscore_card","2e4a439e":"#### <font color='darkblue'>Inference:<\/font>\n\nWith help of log transformation, now, price feature have normal distribution.","e44a8b60":"### <font color='darkblue'>Plots<\/font>","aaf4724a":"#### <font color='darkblue'>Inference:<\/font>\n\nUsing this plot, we can infer that the residuals do come from a normal distribution. This is possible since our target variable is normally distributed.","e6bfadfb":"#### <font color='darkblue'>Inference:<\/font>\n\nFrom the plots we see that none of the plots show a specific pattern. Hence, we may conclude that the variables are linearly related to the dependent variable.","e4d6166d":"#### <font color='darkblue'>Inference:<\/font>\n\nOn comparing the above models, it is seen that the RMSE value for the model considering scaled data is higher than the other model.So, for further analysis we use the unscaled data.","2ddc4148":"We performed an Encoding on neighbourhood and room type and removed some unwanted columns.","97b97ccf":"Although Airbnb provides hosts with general guidance, there are no easy to access methods to determine the best price to rent out a space. There is third-party software available, but for a hefty price.\nOne method could be to find a few listings that are similar to the place that will be up for rent, average the listed prices and set our price to this calculated average price. However, with the market being so dynamic, we would probably be looking to update the price regularly and this method can become tedious.\nMoreover, this may not be very accurate, as we are not taking into account other important factors that may give us a comparative advantage over other listings around us. This could be property characteristics such as number of rooms, type of room,location ,neighbourhood of the property and extra services on offer.\n\nThe aim of this project is to propose a data-driven solution, by using machine learning to predict rental price.","cabb1e9f":"#### <font color='darkblue'>Inference:<\/font>\n\nOn comparing the above models, it is seen that the R-squared and the Adjusted R-squared value for the model considering log transformation of the variable 'price' is higher than the other model. And, the RMSE value of the model without considering the log transformation is high. So, we continue with variable 'price' by opting for log transformation.","5d6a7351":"### <font color='darkblue'>Room Type<\/font>","fcaf13fa":"<h1><center> NEW_YORK CITY AIRBNB PRICE PREDICTION<\/center><\/h1>\n\n","3b634531":"### <font color='darkblue'>Reviews per month<\/font>","4cc522fb":"#### Latitude and Longitude with room type","d7785392":"### <font color='darkblue'>Availability<\/font>","30b77948":"#### <font color='darkblue'>Inference:<\/font>\n\n-> From this we can see the mean price to be around 152 dollars. \n\n-> Average availablity of an airbnb around a year is 112 days.\n","7b767212":"<a id='withLog'><\/a>\n## <font color='darkblue'>4.1. LINEAR-REGRESSION(OLS) with log transformed Target<\/font>","1fb6ab77":"#### <font color='darkblue'>Inference:<\/font>\n\nManhattan airbnb's has the highest average price.","1a07e71f":"<a id='RemovingInsignificantVariable_scaleddata'><\/a>\n## <font color='darkblue'>4.3.3. Linear Regression after Removing Insignificant Variable (OLS) - Scaled Data<\/font>","89847272":"#### <font color='darkblue'>Inference:<\/font>\n\nAlmost 2k+ airbnb's has a price of 100 dollars and 150 dollars each respectively.\n\n1.5k airbnb's have around 50 dollars price.","3f5afa3c":"#### <font color='darkblue'>Inference:<\/font>\n\nWe can observe that most of almost 12k people used 1 night stay in airbnb.\n\n11k people choose 2 night stay while 7k choose 3 night stay.\n\nAlmost 3.7k stayed upto a month.","b26acc12":"The good fit indicates that normality is a reasonable approximation.","81b59396":"#### <font color='darkblue'>Inference:<\/font>\n\nWe observe that p-value is less than 0.05 and thus reject the null hypothesis. We conclude that there is heteroskedasticity present in the data. In real life it might not be possible to meet all the assumptions of linear regression.","88778c62":"#### <font color='darkblue'>Inference:<\/font>\n\nThis model explains 49.1% of the variation in dependent variable price.The Durbin-Watson test statistics is 2.006 and indicates that the is no autocorrelation. The Condition Number is 6.82 suggests that there is no multicollinearity.","75cc2906":"#### <font color='darkblue'>Inference:<\/font>\n\nWe can see that Williamsburg is the hottest area of transaction followed by Bedford-Stuyvesant.\n\nThis pie-chart shows the top 5 areas by percentage in the dataset.","f520391e":"#### <font color='darkblue'>Inference:<\/font>\n\nThe average pricing is around 152 dollars.\n\n50% of data has price greater than 106 dollars.\n\nThe costliest airbnb has around 10k dollars as price.","a8ab476a":"<a id='Linearity_of_Residuals'><\/a>\n### <font color='darkblue'>4.3.2.3. Linearity of Residuals<\/font>","fa4d5c8e":"<a id='import_lib'><\/a>\n## 1. Import libraries\nHere we are importing all the libraries required for the case study.","7281cf35":"The null and alternate hypothesis of Breusch-Pagan test is as follows:\n\nH0: The residuals are homoskedastic\n\nH1: The residuals are not homoskedastic","ff834086":"### About the dataset (Adult Income Data)\n\n**id:** listing ID\n\n**name:** name of the listing\n\n**host_id:** host ID\n\n**host_name:** name of the host\n\n**neighbourhood_group:** location\n\n**neighbourhood:** area\n\n**latitude:** latitude coordinateslatitude: latitude coordinates\n\n**longitude:** longitude coordinates\n\n**room_type:** listing space type\n\n**price:** price in dollars\n\n**minimum_nights:** amount of nights minimum\n\n**number_of_reviews:** number of reviews\n\n**last_review:** latest review\n\n**reviews_per_month:** number of reviews per month\n\n**calculated_host_listings_count:** amount of listing per host avail","8f65071b":"#### <font color='darkblue'>Inference:<\/font>\n\nOur last model where we have removed the insignificant variables is performimg better than the other models.Hence we consider Linreg full model with significant variables.","61624b03":"<a id='regularization'><\/a>\n## 5. Regularization (OLS)","efe2caa5":"### <font color='darkblue'>Calculate host listing<\/font>","3ac51b47":"Since we removed 4 columns from the dataframe, we now have to deal with 12 columns.","fa6ed1be":"Airbnb is a paid community platform for renting and booking private accommodation founded in 2008. Airbnb allows individuals to rent all or part of their own home as extra accommodation. The site offers a search and booking platform between the person offering their accommodation and the vacationer who wishes to rent it. It covers more than 1.5 million advertisements in more than 34,000 cities and 191 countries. From creation, inaugust 2008, until June 2012, more than 10 million nights have been booked on Airbnb.","31b7fb73":"#### <font color='darkblue'>Inference:<\/font>\n\nHere, we can see that there are a lot of outliers taking up the data, it could be expensive Airbnbs, but there are a very few of them compared to data and it overall has a huge impact and must be removed.","4d633054":"### <font color='darkblue'>Neighbourhoods:<\/font>","abb6ec4e":"## Table of Contents\n\n1. **[Import Libraries](#import_lib)**\n2. **[Read Data](#Read_Data)**\n3. **[Exploratory Data Analysis](#eda)**\n    - 3.1 - [Understand the Data](#Data_Understanding)\n        - 3.1.1 - [Data Wrangling & Cleaning](#cleaning)\n    - 3.2 - [Data Analysis and Visualisation](#Data_Analysis)\n    - 3.3 - [Encoding Data and Outlier Removal](#Data_Encoding)\n        \n4. **[Linear Regression (OLS)](#LinearRegression)**\n    - 4.1 - [Multiple Linear Regression - Full Model - with Log Transformed Dependent Variable (OLS)](#withLog)\n    - 4.2 - [Multiple Linear Regression - Full Model - without Log Transformed Dependent Variable (OLS)](#withoutLog)\n    - 4.3 - [Fine Tune Linear Regression Model (OLS)](#Finetuning)\n      - 4.3.1 - [Linear Regression after Removing Insignificant Variable (OLS)](#RemovingInsignificantVariable)\n      - 4.3.2 - [Check the Assumptions of Linear Regression](#Assumptions)\n          - 4.3.2.1 - [Detecting Autocorrelation](#Autocorrelation)\n          - 4.3.2.2 - [Detecting Heteroskedasticity](#Heteroskedasticity)\n          - 4.3.2.3 - [Linearity of Residuals](#Linearity_of_Residuals)\n          - 4.3.2.4 - [Normality of Residuals](#Normality_of_Residuals)\n      - 4.3.3 - [Linear Regression after Removing Insignificant Variable (OLS) - Scaled Data](#RemovingInsignificantVariable_scaleddata)\n      - 4.3.4 - [Linear Regression with Interaction (OLS)](#Interaction)\n5. **[Regularization (OLS)](#regularization)**\n    - 5.1 - [Ridge Regression Model (OLS)](#Ridge_Regression)\n    - 5.2 - [Linear Regression with SGD (sklearn)](#LinearRegressionwithStochasticGradientDescent)\n6. **[Conclusion and Interpretation](#rmse_and_r-squared)**    ","7fc20642":"#### <font color='darkblue'>Inference:<\/font>\n\nThe mean of the residuals is very much closer to zero. Therefore, we can say that linearity is present.","02ae99cf":"<a id='Autocorrelation'><\/a>\n### <font color='darkblue'>4.3.2.1. Detecting Autocorrelation<\/font>","df565585":"### <font color='darkblue'>Data Cleaning<\/font>","044aec76":"<a id='eda'><\/a>\n## 3. Exploratory Data Analysis","597e8a32":"#### <font color='darkblue'>Inference:<\/font>\n\nHome service seems to be most used by people and the highest in Manhattan. This is also the highest service used across New York City.\n\nIn Brooklyn, Private rooms were more used.","28e563fa":"#### <font color='darkblue'>Inference:<\/font>\n\nThis model explains 88.9% of the variation in dependent variable claim.The Durbin-Watson test statistics is 2.003 and indicates that the is no autocorrelation. The Condition Number is 3.04e+03 suggests that there is strong multicollinearity. The collinearity is likely to increase because of the interaction effect.","716ac5c4":"#### <font color='darkblue'>Conclusion:<\/font>\n\n#### Findings suggest that the linear regression with interaction algorithm has the highest accuracy(49.1%) with RMSE of 0.35 which is the lowest compared to other models.Finally, it can be concluded that the linear regression with interaction algorithm  can be used by airbnb to predict the price of the airbnb.","55ab5a8f":"<a id='Data_Encoding'><\/a>\n## <font color='darkblue'>3.3. Encoding Data and Outlier removal.<\/font>","c6d0583c":"Manhattan has highest airbnbs with highest minimum nights.","1215de8e":"#### <font color='darkblue'>Inference:<\/font>\n\nNotable that we have 5 locations in dataset and 3 room types.","2862320e":"#### <font color='darkblue'>Inference:<\/font>\n\nGreat Bedroom in Manhattan gets the highest reviews and it costs around 69 dollars.","74264035":"<a id='Data_Analysis'><\/a>\n## <font color='darkblue'>3.2. Data Analysis and Visualization<\/font> ","f982b956":"<a id='Read_Data'><\/a>\n## 2. Read Data","6343bf7e":"Above 1, around 406 airbnbs have 2 reviews per month, 222 with 3 and 130 with 4.\n\n","8a2a16a9":"#### <font color='darkblue'>Inference:<\/font>\n\nThis model explains 49.1% of the variation in dependent variable claim. The Durbin-Watson test statistics is 2.014 which indicates that there is no autocorrelation. The Condition Number is 6.82 suggests that there is no collinearity.","31c24adc":"<a id='Ridge_Regression'><\/a>\n## <font color='darkblue'>5.1. Ridge Regression (OLS)<\/font>","c03264aa":"#### <font color='darkblue'>Inference:<\/font>\n\nWe can actually fill all the rows of column of reviews_per_month as 0 where its null value, this is because the data is null only because no one has reviewd it and hence the number of review is 0 here.","57c95f95":"<a id='LinearRegressionwithStochasticGradientDescent'><\/a>\n## <font color='darkblue'>5.2. Linear Regression with SGD<\/font>","65c9a5d2":"#### <font color='darkblue'>Inference:<\/font>\n\nManhattan is the city where most Airbnb transactions have occured with 44% of entire dataset. The least happend in Staten Island only 1%. Brooklyn consisted on 41% of transactions with 12% Queens and 2 % in Bronx.","f2f0c4e5":"#### <font color='darkblue'>Inference:<\/font>\n\nWe do not have any duplicate rows in our dataset.","4efead33":"#### <font color='darkblue'>Inference:<\/font>\n\nHere we have almost 48.8k rows with 16 columns for each. It is composed of 3 float types, 7 int types and 6 object types.","078ab264":"### <font color='darkblue'>Host ID<\/font>","662ce1bf":"#### Latitude and Longitude with location","15b0df55":"#### <font color='darkblue'>Inference:<\/font>\n\nEnjoy great views in Manhattan has the highest reviews per month. They offer Private room and is worth 100 dollars a night. ","f01b4247":"<a id='Heteroskedasticity'><\/a>\n### <font color='darkblue'>4.3.2.2. Detecting Heteroskedasticity<\/font>","a20fba52":"<a id='rmse_and_r-squared'><\/a>\n## 6. Conclusion and Interpretation","c78b6275":"#### <font color='darkblue'>Inference:<\/font>\n\nIntresting to note that in our dataset, around 25k people (52%) choose to use a house while 22k(46%) for a private room. Only 1k(2%) people choose a shared room. This could mean more people who use airbnb , use it with family maybe for tours,visits,etc... ","f5b842ec":"#### <font color='darkblue'>Inference:<\/font>\n\nThis model explains 46.6% of the variation in dependent variable price.The Durbin-Watson test statistics is 2.003 indicates that there is no autocorrelation. The Condition Number 2.65e+03 suggests that there is strong multicollinearity.","64e9cc93":"#### <font color='darkblue'>Inference:<\/font>\n\nIn the availablity_365 , if we have 365 days availablity then we can directly assgn them as 365 itself and 0 for rest because they isnt availavle for 365 days.","3c0c142a":"#### <font color='darkblue'>Inference:<\/font>\n\nThus, we may say the model obtained by alpha = 0.9001 is performing the best since has the lowest root mean squared error.","ea71e57b":"### <font color='darkblue'>Number of reviews<\/font>","6770be7a":"#### <font color='darkblue'>Inference:<\/font>\n\nAfter experimenting around values the range of (20,250) has a good gaussian wise distribution and hence we can go ahead with data in this range and remove others as outliers.","fe3cc454":"#### <font color='darkblue'>Inference:<\/font>\n\nThe sum of squares of residual, increases with increase in alpha, this reduces model complexity. Compare the coefficients in the first and second row of this table, there is a drastic change in the magnitude of coefficients. Similar change is seen on comparing rows 2 and 3. However, there is not much change in rows 3 and 4. High alpha values can lead to significant underfitting and there is a increase in residual sum of squares.","b7da8bbb":"### <font color='darkblue'>Bivariate Analysis<\/font>","339f73aa":"We can first see the box plot of the price and find some outliers and remove them so that we can do better in further steps.","7c841d75":"<a id='LinearRegression'><\/a>\n## 4. LINEAR-REGRESSION(OLS) ","d8c26ada":"#### <font color='darkblue'>Inference:<\/font>\n\n10k airbnbs dont have any reviews.\n\n5.2k has around 1 review and the maximum number of reviews is 607 which only 1 airbnb has.","5647801a":"### <font color='darkblue'>Neighbourhood Groups - Location<\/font>","5cd73253":"#### <font color='darkblue'>Inference:<\/font>\n\nIf we look at the top 100 airbnb's with number of reviews, Brooklyn has highest reviews followed by Queens and then Manhattan.","00611bc8":"<a id='Interaction'><\/a>\n## <font color='darkblue'>4.3.4. Linear Regression with Interaction (OLS)<\/font>","f0820d09":"#### <font color='darkblue'>Inference:<\/font>\n\nThis is the final price data on which we are performing the rest of operations.","4c4035db":"#### <font color='darkblue'>Inference:<\/font>\n\nThis model explains 49.1% of the variation in dependent variable claim.The Durbin-Watson test statistics is 2.007 indicates that there is no autocorrelation. The Condition Number 1.66e+18 suggests that there is strong multicollinearity.","e4973adf":"<a id='withoutLog'><\/a>\n## <font color='darkblue'>4.2. Multiple Linear Regression - Full Model - without log Transformed Dependent Variable (OLS)<\/font>","d365a7c1":"#### <font color='darkblue'>Inference:<\/font>\n\nThe correlation table shows that there is no strong relationship between price and other features. This indicates no feature needed to be taken out of data. ","f1e1f7c0":"We dont require host_id,name,id,host_name and last_review as these do not any way effect for further analysis or pre-processing. So we can drop them.","16cdc581":"<a id='cleaning'><\/a>\n### <font color='darkblue'>3.1.1. Data Wrangling and Cleaning<\/font> ","2b0aa128":"#### <font color='darkblue'>Inference:<\/font>\n\nSo, we can see there are around 10k null values in the last_review and review_per_month columns and a very few null values in name and host_name.","6d2e85cf":"## Problem Statement","9d5dbf38":"#### <font color='darkblue'>Inference:<\/font>\n\nWe can observe that the highest times transaction done by a customer is 327 in the year 2019.","4f9ede43":"Around 1.3k airbnbs have 365 days availablity and rest doesnt.\n","f675155a":"#### <font color='darkblue'>Inference:<\/font>\n\nThe above distribution graph shows that there is a right-skewed distribution on price. This means there is a positive skewness. Log transformation will be used to make this feature less skewed. ","20247773":"#### <font color='darkblue'>Inference:<\/font>\n\nFrom the summary output from Linreg full model with significant variables,  we see that the Durbin-Watson static is 2.0\nHence we can conclude that there is no autocorrelation.","9e06acd7":"#### Latitude and Longitude with room availability for 365 days","d66952e3":"The null and alternate hypothesis of Durbin-Watson test is as follows:\n\n      H0: There is no autocorrelation in the residuals\n      \n      H1: There is autocorrelation in the residuals","2102d6fe":"#### <font color='darkblue'>Inference:<\/font>\n\nStaten Island has th highest average airbnb availablity. ","ab233320":"<a id='Finetuning'><\/a>\n## <font color='darkblue'>4.3. Fine Tune Linear Regression Model (OLS)<\/font>","ee4f5a56":"<a id='RemovingInsignificantVariable'><\/a>\n### <font color='darkblue'>4.3.1. Linear Regression after Removing Insignificant Variable (OLS)<\/font>","07a796a5":"### <font color='darkblue'>Minimum Nights<\/font>","11b01b82":"#### <font color='darkblue'>Inference:<\/font>\n\nThis shows us the dataset distribution in NewYork city with respect to latitude and longitude.","83abfaab":"#### <font color='darkblue'>Inference:<\/font>\n\nThese are the insignificant variables because their p-value is greater than 0.05.\nThere are redundant variables in calculated_host_listings_count and neighbourhood_group.","288ce966":"The independent variables must have a linear relationship with the dependent variable.","76586f48":"#### <font color='darkblue'>Inference:<\/font>\n\nNotice that R-squared and Adjusted R-squared values are negative and RMSE value is very high. It can be infered that this model does not perform good","403773b6":"#### <font color='darkblue'>Inference:<\/font>\n\nCostliest airbnb with 365 days availablity costs around 10k dollars with average of 250 dollars.","ae701eb3":"<a id='Assumptions'><\/a>\n### <font color='darkblue'>4.3.2. Check the Assumptions of the Linear Regression<\/font>","51668c66":"#### <font color='darkblue'>Inference:<\/font>\n\nWe used 30% of the data for testing and fitting it to the linear model which we generated using the rest off the data.This model explains 49% of the variation in dependent variable price.The Durbin-Watson test statistics is 2.013 indicates that there is no autocorrelation. The Condition Number 2.95e+03 suggests that there is strong multicollinearity.","f4313475":"#### <font color='darkblue'>Inference:<\/font>\n\nIf we take the top 100 airbnbs then almost 70+ comes in Manhattan, followed by 25 in Brooklyn.","bec971a9":"<a id='Data_Understanding'><\/a>\n## <font color='darkblue'>3.1. Understand The Data<\/font> ","f6ec05f8":"#### <font color='darkblue'>Inference:<\/font>\n\nMost hosts used the listings only once that is around 32.3k and 6.6k around 2 times.","f991db3c":"Now we will perform regularization to check whether this technique performs better than our linear regression models without regularization","432104a9":"#### <font color='darkblue'>Inference:<\/font>\n\nOn an average, a host has used listings 7 times.\n\nMaximum times being 327.","5ee2dc61":"### <font color='darkblue'>Price:<\/font>","2ec9373c":"#### <font color='darkblue'>Inference:<\/font>\n\nWe have 3 airbnbs with 10k per night luxury stay, one private room and 2 home stay.","c868486f":"<a id='Normality_of_Residuals'><\/a>\n### <font color='darkblue'>4.3.2.4. Normality of Residuals<\/font>","5f68d822":"15.9k reviews were above 1.\n"}}