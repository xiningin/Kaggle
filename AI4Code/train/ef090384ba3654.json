{"cell_type":{"1e847da7":"code","8ebe0d58":"code","172a2ccb":"code","fa639f4f":"code","d9b4deae":"code","479f1a4d":"code","c4fd7a2c":"code","15d876fa":"code","1fda8ef4":"code","04dbef22":"code","24fe1716":"code","89055931":"code","f5c0f487":"code","d6049193":"code","56fcd855":"markdown","7cdb3926":"markdown","c83dd141":"markdown","c3c88e48":"markdown","392d3943":"markdown","efe78bc9":"markdown","3f6db57d":"markdown","8181533e":"markdown"},"source":{"1e847da7":"import numpy as np\nimport pandas as pd \n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","8ebe0d58":"os.environ[\"SEED\"] = \"420\"\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import BertModel, BertTokenizer, AdamW, get_linear_schedule_with_warmup, BertConfig\nimport re\nfrom tqdm import tqdm","172a2ccb":"df_train = pd.read_csv(\"\/kaggle\/input\/gapvalidation\/gap-test.tsv\", delimiter=\"\\t\")\ndf_val = pd.read_csv(\"\/kaggle\/input\/gapvalidation\/gap-validation.tsv\", delimiter=\"\\t\")\ndf_test = pd.read_csv(\"\/kaggle\/input\/gapvalidation\/gap-development.tsv\", delimiter=\"\\t\")\ntest_2 = pd.read_csv(\"\/kaggle\/input\/gendered-pronoun-resolution\/test_stage_2.tsv\", delimiter=\"\\t\")\n\nPRETRAINED_MODEL_NAME = 'bert-large-uncased'\n\nbert_path = \"..\/input\/bert-base-uncased\/\"\ntokenizer = BertTokenizer.from_pretrained(bert_path)\npad_len = 300","fa639f4f":"def conver_lower(df):\n    df['Text'] = df.apply(lambda row: row['Text'].lower(), axis = 1)\n    df['A'] = df.apply(lambda row: row['A'].lower(), axis = 1)\n    df['B'] = df.apply(lambda row: row['B'].lower(), axis = 1)\n    df['Pronoun'] = df.apply(lambda row: row['Pronoun'].lower(), axis = 1)\n    return df\ndf_train = conver_lower(df_train)\ndf_test = conver_lower(df_test)\ndf_val = conver_lower(df_val)\ntest_2 = conver_lower(test_2)","d9b4deae":"tokenizer.add_tokens(['[A]', '[B]', '[P]'])\ndef insert_tag(row):\n    to_be_inserted = sorted([\n        (row[\"A-offset\"], \" [A] \"),\n        (row[\"B-offset\"], \" [B] \"),\n        (row[\"Pronoun-offset\"], \" [P] \")\n    ], key=lambda x: x[0], reverse=True)\n    text = row[\"Text\"]\n    for offset, tag in to_be_inserted:\n        text = text[:offset] + tag + text[offset:]\n    return text\n\ndef tokenize(text, tokenizer):\n    entries = {}\n    final_tokens = []\n    for token in tokenizer.tokenize(text):\n        if token in (\"[A]\", \"[B]\", \"[P]\"):\n            entries[token] = len(final_tokens)\n            continue\n        final_tokens.append(token)\n    return final_tokens, (entries[\"[A]\"], entries[\"[B]\"], entries[\"[P]\"])\n\ndef target(row):\n    if int(row['A-coref']) == 1:\n        return 0\n    elif int(row['B-coref']) == 1:\n        return 1\n    else:\n        return 2\n\"\"\"\nThe lower part was taken from \n            [PyTorch] BERT + EndpointSpanExtractor + KFold\n\"\"\"\ndef children(m):\n    return m if isinstance(m, (list, tuple)) else list(m.children())\n\ndef set_trainable_attr(m, b):\n    m.trainable = b\n    for p in m.parameters():\n        p.requires_grad = b\n\ndef apply_leaf(m, f):\n    c = children(m)\n    if isinstance(m, nn.Module):\n        f(m)\n    if len(c) > 0:\n        for l in c:\n            apply_leaf(l, f)\n            \ndef set_trainable(l, b):\n    apply_leaf(l, lambda m: set_trainable_attr(m, b))","479f1a4d":"class modified_dataset(torch.utils.data.Dataset):\n    def __init__(self, df, tokenizer):\n        p_text = []\n        offsets = []\n        at_mask = []\n        self.y_lst = df[['A-coref', 'B-coref']].apply(lambda row: target(row), axis = 1)\n        for row in tqdm(range(len(df))):\n            tokens, offset = tokenize(insert_tag(df.iloc[row]), tokenizer)\n            bla = tokenizer.encode_plus(tokens, max_length = pad_len, pad_to_max_length = True, return_token_type_ids = False)\n            p_text.append(bla['input_ids'])\n            at_mask.append(bla['attention_mask'])\n            offsets.append(offset)\n        self.p_text = torch.tensor(p_text)\n        self.offsets = torch.tensor(offsets)\n        self.at_mask = torch.tensor(at_mask)\n        return \n    def __len__(self):\n        return len(self.p_text)\n    def __getitem__(self,item):\n        return self.p_text[item], self.y_lst[item], self.offsets[item], self.at_mask[item]\n\nclass modified_dataset_test(torch.utils.data.Dataset):\n    def __init__(self, df, tokenizer):\n        p_text = []\n        offsets = []\n        at_mask = []\n        for row in tqdm(range(len(df))):\n            tokens, offset = tokenize(insert_tag(df.iloc[row]), tokenizer)\n            bla = tokenizer.encode_plus(tokens, max_length = pad_len, pad_to_max_length = True, return_token_type_ids = False)\n            p_text.append(bla['input_ids'])\n            at_mask.append(bla['attention_mask'])\n            offsets.append(offset)\n        self.p_text = torch.tensor(p_text)\n        self.offsets = torch.tensor(offsets)\n        self.at_mask = torch.tensor(at_mask)\n        return  \n    def __len__(self):\n        return len(self.p_text)\n    def __getitem__(self,item):\n        return self.p_text[item], self.offsets[item], self.at_mask[item]\n \ndef collate_fun(batch):\n    tmp_lst = list(zip(*batch))\n    return torch.stack(tmp_lst[0], axis = 0), torch.tensor(tmp_lst[1]), torch.stack(tmp_lst[2], axis = 0), torch.stack(tmp_lst[3], axis = 0)\n\ndef collate_fun2(batch):\n    tmp_lst = list(zip(*batch))\n    return torch.stack(tmp_lst[0], axis = 0), torch.stack(tmp_lst[1], axis = 0), torch.stack(tmp_lst[2], axis = 0)\n\ntrain_loader = DataLoader(\n        modified_dataset(df_train, tokenizer),\n        batch_size=18,\n        collate_fn=collate_fun,\n        shuffle=True,\n        drop_last=True,\n        num_workers=2)\nval_loader = DataLoader(\n        modified_dataset(df_val, tokenizer),\n        batch_size=30,\n        collate_fn=collate_fun,\n        shuffle=False,\n        num_workers=2)\ntest_loader = DataLoader(\n        modified_dataset(df_test, tokenizer),\n        batch_size=30,\n        collate_fn=collate_fun,\n        shuffle=False,\n        num_workers=2)\ntest_2_loader = DataLoader(\n        modified_dataset_test(test_2, tokenizer),\n        batch_size=30,\n        collate_fn=collate_fun2,\n        shuffle=False,\n        num_workers=2)","c4fd7a2c":"GPU = torch.cuda.is_available()\ntorch.save({'train_loader':train_loader,\n            'test_loader':test_loader,\n            'val_loader':val_loader}, 'dataloader_new.pth')\ntorch.save({'test_dataloader':test_2_loader},'test_loader.pth')\n# train_loader, test_loader, val_loader = torch.load('\/kaggle\/input\/gap-dataloaders\/dataloaders2.pth').values()\n# test_2_loader = torch.load('\/kaggle\/input\/gap-dataloaders\/test_loader(1).pth')['test_dataloader_174']","15d876fa":"class bert(nn.Module):\n    def __init__(self, bert_path):\n        super().__init__()\n        BERT = BertModel.from_pretrained(bert_path, config = BertConfig.from_pretrained(bert_path, output_hidden_states = True))\n        self.BERT = BERT\n        self.fc = nn.Sequential(nn.BatchNorm1d(self.BERT.config.hidden_size * 3),\n                                nn.Dropout(0.4),\n                                nn.Linear(self.BERT.config.hidden_size * 3, 600),\n                                nn.BatchNorm1d(600),\n                                nn.Dropout(0.4),\n                                nn.Linear(600, 600),\n                                nn.BatchNorm1d(600),\n                                nn.Dropout(0.4),\n                                nn.Linear(600,3))\n        \n    def forward(self, token, at_mask, offsets, layer):\n        out = self.BERT(token, attention_mask = at_mask)[2][layer]\n        out_lst = []\n        for j in range(out.shape[0]):\n            out_lst.append(torch.stack([torch.tensor(out[j,offsets[j,0]]),torch.tensor(out[j,offsets[j,1]]),torch.tensor(out[j,offsets[j,2]])] , dim = 0) )\n        out_lst = torch.stack([word_embedding for word_embedding in out_lst], dim = 0)\n        out = out_lst.reshape(out_lst.shape[0], -1)\n        out = self.fc(out)\n        return out\n        \ndef create_model(df_len,epoch_len):        \n    model = bert(bert_path)\n    criteria = nn.CrossEntropyLoss()\n    optimizer = AdamW(model.parameters(), eps = 1e-06, lr = 1e-4)\n    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=df_len*epoch_len)\n    return model, criteria, optimizer, scheduler","1fda8ef4":"epoch_len = 20\nmodel, criteria, optimizer, scheduler = create_model(len(df_train), epoch_len)\nset_trainable(model.BERT, False)\naaa = 0\nfor t in range(epoch_len):\n    tot_loss = 0\n    correct_train = 0\n    val_loss = 0\n    val_correct = 0\n    model = model.train()\n    \n    if GPU:\n        model = model.cuda()\n    \n    for item in tqdm(train_loader):\n        \n        token = item[0]\n        at_mask = item[3]\n        offsets = item[2]\n        target = item[1]\n        if GPU:\n            token = token.cuda()\n            at_mask = at_mask.cuda()\n            target = target.cuda()\n            offsets = offsets.cuda()\n            \n        output = model(token, at_mask, offsets, -2)\n        loss = criteria(output, target)\n        tot_loss += loss.item()\n        correct_train += torch.sum(torch.max(torch.nn.functional.softmax(output, dim = 1), dim = 1)[1] == target)\n        \n        optimizer.zero_grad()\n        loss.backward()\n        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n        optimizer.step()\n        scheduler.step()\n    \n    with torch.no_grad():\n        model = model.eval()\n        \n        if GPU:\n            model = model.cuda()            \n        for item in tqdm(val_loader):\n            token = item[0]\n            at_mask = item[3]\n            offsets = item[2]\n            target = item[1]\n            \n            if GPU:\n                token = token.cuda()\n                at_mask = at_mask.cuda()\n                offsets = offsets.cuda()\n                target = target.cuda()\n                \n            output = model(token, at_mask, offsets, -2)\n            val_correct += torch.sum(torch.max(torch.nn.functional.softmax(output, dim = 1), dim = 1)[1] == target)\n        if val_correct > aaa:\n            bst_model = model\n            aaa = val_correct\n    print(tot_loss, correct_train,\"   \", val_correct,\" out of \", len(val_loader)*30)","04dbef22":"def predict(df, dataloader, model):\n    tmp_array = np.zeros((len(df), 3))\n    with torch.no_grad():\n        model = model.eval()\n        if GPU:\n            model = model.cuda()\n        \n        j = 0\n        for item in tqdm(dataloader):\n            \n            token = item[0]\n            at_mask = item[2]\n            offsets = item[1]\n\n            if GPU:\n                token = token.cuda()\n                at_mask = at_mask.cuda()\n                offsets = offsets.cuda()\n            \n            output = model(token, at_mask, offsets, -2)\n            for zz in output.cpu():\n                tmp_array[j] = zz\n                j+=1\n            \n    return tmp_array","24fe1716":"a = predict(test_2, test_2_loader, bst_model)","89055931":"bla = test_2[['ID']].merge(pd.DataFrame(torch.nn.functional.softmax(torch.tensor(a), dim = 1).numpy()), left_index=True, right_index=True).set_index('ID')\nbla.columns = ['A', 'B', 'NEITHER']\nbla.to_csv('sbmsn2.csv')","f5c0f487":"torch.save({'model':bst_model}, 'model1.pth')","d6049193":"tst_model = torch.load('\/kaggle\/input\/gendered-model\/model1.pth')['model']","56fcd855":"# Functions to extract choices and turning trainable feature off (BERT)","7cdb3926":"# Predicting","c83dd141":"# Training","c3c88e48":"# Loading the csv files","392d3943":"# BERT Model","efe78bc9":"# Loading the PyTorch BERT model as well as the GAP dataset and the required libraries","3f6db57d":"# Saving the dataloaders\n","8181533e":"# Torch Datasets"}}