{"cell_type":{"4176cbf8":"code","6a20b025":"code","65e076a5":"code","5f0eb52d":"code","1cb5c558":"code","d653adf5":"code","9d45a47b":"code","04fd74f3":"code","57d8be08":"code","7367ae54":"code","e33297fc":"code","c3c37e94":"code","686cbe7a":"code","3400849e":"code","2c691359":"code","31abf72b":"code","279f3b16":"code","791cb34e":"code","c2bd8d9f":"code","badb7929":"code","4d449066":"code","77b9fc96":"code","dbc88349":"code","23f2018e":"code","0d8537b9":"markdown","26d6d09f":"markdown","680356ea":"markdown","fe213fb1":"markdown","bdff8ab5":"markdown","f22828b9":"markdown","77dc1dc4":"markdown","859dbf76":"markdown","53952f2b":"markdown","4e8a9601":"markdown"},"source":{"4176cbf8":"import pandas as pd\nimport numpy as np\nimport random           #\u2605\u518d\u73fe\u6027\u306e\u70ba\u306b\u4e71\u6570\u306e\u56fa\u5b9a\u304c\u5fc5\u8981\nimport os               #\u2605\u518d\u73fe\u6027\u306e\u70ba\u306b\u4e71\u6570\u306e\u56fa\u5b9a\u304c\u5fc5\u8981\nimport tensorflow as tf #\u2605\u518d\u73fe\u6027\u306e\u70ba\u306b\u4e71\u6570\u306e\u56fa\u5b9a\u304c\u5fc5\u8981\nfrom tensorflow.keras.models import Sequential #NN\u306e\u5404\u5c64\u3092\u9806\u756a\u306b\u63a5\u7d9a\u3002Functional\u3082\u6709\u308b\u3089\u3057\u3044\u3002\nfrom tensorflow.keras.utils import plot_model\nfrom tensorflow.keras.optimizers import Adam #\u6700\u9069\u5316\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\nfrom tensorflow.keras.layers import Dense, Dropout, Activation #Dense\u306f\u5168\u7d50\u5408\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, Callback, ReduceLROnPlateau #LR=Lerning Rate\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import StratifiedKFold,GroupKFold \nimport matplotlib.pyplot as plt\n%matplotlib inline\n\ndef seed_everything(seed): #\u518d\u73fe\u6027\u306e\u70ba\u306b\u4e71\u6570\u30b7\u30fc\u30c9\u3092\u56fa\u5b9a\u3059\u308b\u70ba\u306e\u95a2\u6570\n    random.seed(seed) #\u4e71\u6570\u306e\u30b7\u30fc\u30c9\n    os.environ['PYTHONHASHSEED'] = str(seed) #os\u306e\u30b7\u30fc\u30c9\n    np.random.seed(seed) #np\u306e\u4e71\u6570\u30b7\u30fc\u30c9\n    tf.random.set_seed(seed) #tf\u306e\u4e71\u6570\u30b7\u30fc\u30c9\n\nseed_everything(2020)    ","6a20b025":"df_tra = pd.read_csv('..\/input\/5th-datarobot-ai-academy-deep-learning\/train.csv')\ndf_tst = pd.read_csv('..\/input\/5th-datarobot-ai-academy-deep-learning\/test.csv')\n\ndisplay(df_tra.head())\ndisplay(df_tra.columns.values)\ndisplay(df_tst.head())\ndisplay(df_tst.columns.values)","65e076a5":"# \u7279\u5fb4\u91cf\nnum_cols = ['bedrooms','bathrooms','zipcode','area']\ntarget = ['price']\n\n# \u6b20\u640d\u5024\u88dc\u586b\ndf_tra[num_cols] = df_tra[num_cols].fillna(0) #\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306f\u6b20\u640d\u88dc\u5b8c\u304c\u51fa\u6765\u306a\u3044\u304b\u3089\u306d\ndf_tst[num_cols] = df_tst[num_cols].fillna(0) #\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306f\u6b20\u640d\u88dc\u5b8c\u304c\u51fa\u6765\u306a\u3044\u304b\u3089\u306d\n\n# \u6b63\u898f\u5316\nscaler = StandardScaler() #NN\u304c\u91cd\u307f\u3065\u3051\u8a08\u7b97\u6642\u306b\u639b\u3051\u7b97\u5272\u308a\u7b97\u3092\u3059\u308b\u306e\u3067\u3001\u5dee\u304c\u5927\u304d\u3044\u3068\u5909\u306a\u91cd\u307f\u3065\u3051\u3092\u884c\u3046\u306e\u3067\ndf_tra[num_cols] = scaler.fit_transform(df_tra[num_cols])\ndf_tst[num_cols] = scaler.fit_transform(df_tst[num_cols])\n","5f0eb52d":"#DNN(Deep Neural Network) = ANN(Artifical Neural Network) = MLP(Multilayer Perceptron)\n#\u30c6\u30fc\u30d6\u30eb\u30c7\u30fc\u30bf\u89e3\u6790\u306b\u9069\u3057\u3066\u3044\u308b\u3002\ndef mlp(num_cols): #\u5165\u529b\u306e\u7279\u5fb4\u91cf\u6570\u3092\u5f15\u6570\u3068\u3059\u308b\u95a2\u6570\n    #\u30d1\u30e9\u30e1\u30fc\u30bf\n    dropout = 0.2\n    \n    \"\"\"\n    \u6f14\u7fd2:Dropout\u3092\u5909\u66f4\u3057\u3066\u307f\u3066\u304f\u3060\u3055\u3044\n    \"\"\"\n    model = Sequential() #\u7a7a\u306e\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u3092\u4f5c\u308b\n\n    #1\u5c64\u76ee\n    model.add(\n        Dense(\n            units=512, #\u96a0\u308c\u5c64\u306e\u96a0\u308c\u30cb\u30e5\u30fc\u30ed\u30f3\u6570\n            input_shape = (len(num_cols),), #\u2605\u3053\u308a\u3083\u306a\u3093\u3060\uff1fimput_dim\u3058\u3083\u306a\u3044\u306e\uff1f\n            kernel_initializer='he_normal', #\u2605\u30d0\u30c3\u30af\u30d7\u30ed\u3071\u3052\u30fc\u30b7\u30e7\u30f3\u306b\u95a2\u4fc2\u3059\u308b\u3089\u3057\u3044\n            activation='relu' #activation = \u6d3b\u6027\u5316\u95a2\u6570\u3002\u305d\u3053\u306brelu\u3068\u3044\u3046\u95a2\u6570\u3092\u4e0e\u3048\u3066\u308b\u307f\u305f\u3044\u3002\n        )\n    )    \n    model.add(Dropout(dropout))\n  \n    #\u2605\u30ab\u30fc\u30cd\u30eb\u30a4\u30cb\u30b7\u30e3\u30e9\u30a4\u30b6\u30fc\u306f\u30d0\u30c3\u30af\u30d7\u30ed\u30d0\u30b2\u30fc\u30b7\u30e7\u30f3\u306b\u95a2\u4fc2\n    #he_normal\u306f\u4f5c\u8005\u306e\u540d\u524d\u306e\u91cd\u307f\u3065\u3051\u521d\u671f\u5024\n    #\u6d3b\u6027\u5316\u95a2\u6570relu\u306fhe_normal\u3068\u306e\u76f8\u6027\u304c\u3044\u3044\n\n    #2\u5c64\u76ee\n    model.add(Dense(units=256,  kernel_initializer='he_normal',activation='relu'))\n    model.add(Dropout(dropout)) #\u4e00\u5b9a\u5272\u5408\u306e\u30ce\u30fc\u30c9\u3092\u4e0d\u6d3b\u6027\u5316\u3055\u305b\u308b\u3002\u3084\u308a\u3059\u304e\u306f\u826f\u304f\u306a\u3044\u306e\u3067\u5fd8\u308c\u308b\u3002\n\n    #3\u5c64\u76ee\n    model.add(Dense(units=32, kernel_initializer='he_normal', activation='relu'))     \n    model.add(Dropout(dropout))\n\n    #4\u5c64\u76ee\n    model.add(Dense(1, activation='linear'))\n\n    \n    model.compile(loss='mape', optimizer='adam', metrics=['mape']) #adam\u306f\u5b9a\u756a\u3067\u6700\u9069\u306a\u6700\u9069\u5316\u3092\u3057\u3066\u304f\u308c\u308b\n    return model","1cb5c558":"def mean_absolute_percentage_error(y_true, y_pred): #\u30e2\u30c7\u30eb\u306e\u8a55\u4fa1\u3092\u884c\u3046\u95a2\u6570\n    y_true, y_pred = np.array(y_true), np.array(y_pred)\n    return np.mean(np.abs((y_true - y_pred) \/ y_true)) * 100","d653adf5":"#\u30d1\u30e9\u30e1\u30bf\u521d\u671f\u5316\ncounter = 0 #\u30d5\u30a9\u30fc\u30eb\u30c9\u306e\u30ab\u30a6\u30f3\u30bf\nskf = StratifiedKFold(n_splits=5, random_state=71, shuffle=True) #StratifiedKFold\u5206\u5272\nfilepath = \"mlp_best_model.hdf5\" #\u91cd\u307f\u3065\u3051\u3092\u51fa\u529b\u3057\u3066\u304f\u308c\u308b\u306e\u3067\u305d\u306e\u30d5\u30a1\u30a4\u30eb\u30d1\u30b9 \nvalid_scores = []\n\n#\u30d5\u30a9\u30fc\u30eb\u30c9\u5206\u30eb\u30fc\u30d7\nfor train_index, valid_index in skf.split(df_tra[num_cols],df_tra[target]): #split\u3057\u306a\u304c\u3089\u30eb\u30fc\u30d7\n    #\u30d5\u30a9\u30fc\u30eb\u30c9\u306e\u30c7\u30fc\u30bf\u5207\u308a\u51fa\u3057\n    counter = counter + 1\n    train = df_tra.iloc[train_index] #\u8a13\u7df4\n    valid = df_tra.iloc[valid_index] #\u691c\u5b9a\n    \n    # \u7279\u5fb4\u91cf\u3068\u30bf\u30fc\u30b2\u30c3\u30c8 df\u304b\u3089np\u306eary\u306b\u5909\u63db\u3059\u308b\n    train_x_num,train_y = train[num_cols].values, train[target].values\n    valid_x_num,valid_y = valid[num_cols].values, valid[target].values\n\n    print('\u25a0\u30d5\u30a9\u30fc\u30eb\u30c9\u5b9f\u884c\u958b\u59cb',counter)\n    #print(train_x_num.shape)\n    #print(valid_x_num.shape)\n\n    print('\u2193\u8abf\u6559\u30d1\u30e9\u30e1\u30bf\u3092\u30bb\u30c3\u30c8\u3057\u3066\u5b66\u7fd2')\n    \"\"\"\n    \u6f14\u7fd2:patience\u3092\u5909\u66f4\u3057\u3066\u307f\u3066\u304f\u3060\u3055\u3044\n    \"\"\"\n    #\u6539\u5584\u3055\u308c\u3066\u3044\u306a\u3044\u5834\u5408\u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u30b8\u30e7\u30d6\u3092\u65e9\u671f\u306b\u505c\u6b62\n    es = EarlyStopping(patience=2, mode='min', verbose=1)  #\u691c\u5b9a\u30c7\u30fc\u30bf\u3067\u306e\u30b9\u30b3\u30a2\u3067\u691c\u8a3c\u3057\u3066\u4e0a\u304c\u3089\u306a\u304f\u306a\u3063\u305f\u3089\u6b62\u3081\u308b\n\n    #\u30ed\u30b9\u6700\u5c0f\u306e\u91cd\u307f\u3092\u4fdd\u5b58\u3059\u308b\n    checkpoint = ModelCheckpoint(monitor='val_loss',filepath=filepath, save_best_only=True, mode='auto') \n\n    #x\u30a8\u30dd\u30c3\u30af\u6539\u5584\u3057\u306a\u304b\u3063\u305f\u3089\u5b66\u7fd2\u7387\u3092\u4e0b\u3052\u308b\n    reduce_lr_loss = ReduceLROnPlateau(monitor='val_loss', patience=2, factor=0.1, verbose=1, mode='min')\n\n    model = mlp(num_cols)\n\n    \"\"\"\n    \u6f14\u7fd2:batch_size,epochs\u3092\u5909\u66f4\u3057\u3066\u307f\u3066\u304f\u3060\u3055\u3044\n    \"\"\"\n    history = model.fit(\n                            train_x_num, #\u8a13\u7df4 \u7279\u5fb4\u91cf\n                            train_y,     #\u8a13\u7df4 \u30bf\u30fc\u30b2\u30c3\u30c8\n                            batch_size=32, \n                            epochs=100, \n                            validation_data=(valid_x_num, #\u691c\u5b9a \u7279\u5fb4\u91cf \n                                             valid_y),    #\u691c\u5b9a \u30bf\u30fc\u30b2\u30c3\u30c8\n                            callbacks=[es, checkpoint, reduce_lr_loss], \n                            verbose=0 #0:\u6a19\u6e96\u51fa\u529b\u7121\u3057\u30011:\u30d7\u30ed\u30b0\u30ec\u30b9\u30d0\u30fc\u30012:\u30a8\u30dd\u30c3\u30af\u3054\u3068\u306b1\u884c\u306e\u30ed\u30b0(\u8b1b\u7fa9\u6642\u306f1)\n                       )\n        \n    print('\u2193\u4eca\u56de\u306e\u30d5\u30a9\u30fc\u30eb\u30c9\u306b\u304a\u3051\u308b\u7cbe\u5ea6\u306e\u8a55\u4fa1')\n    # load best model weights\n    model.load_weights(filepath)\n\n    # predict valid data\n    valid_pred = model.predict(valid_x_num, batch_size=32).reshape((-1,1))\n    valid_score = mean_absolute_percentage_error(valid_y,  valid_pred)\n    valid_scores.append(valid_score)\n    print ('valid mape:',valid_score)","9d45a47b":"#\u2605\u5e73\u5747\u5024\u3092\u8a08\u7b97\u3059\u308b\nnp.array(valid_scores).mean()\nprint ('valid mape mean:',np.array(valid_scores).mean())","04fd74f3":"#valid mape mean: 47.01042599029993 \u2605PL:43.711192","57d8be08":"#model.summary()","7367ae54":"#plot_model(model, to_file='mlp.png')","e33297fc":"\"\"\"\nloss = history.history['loss']\nval_loss = history.history['val_loss']\nepochs = range(len(loss))\nplt.plot(epochs, loss, 'bo' ,label = 'training loss')\nplt.plot(epochs, val_loss, 'b' , label= 'validation loss')\nplt.title('Training and Validation loss')\nplt.legend()\nplt.show()\n\"\"\"","c3c37e94":"#df_tra\u306bprice\u306etbl\u304b\u3089\u306e\u4e88\u6e2c\u5024\u3092\u30bb\u30c3\u30c8\u3059\u308b\ndf_tra['price_pred1'] = model.predict(df_tra[num_cols].values, batch_size=32).astype('int64')\ndf_tst['price_pred1'] = model.predict(df_tst[num_cols].values, batch_size=32).astype('int64')\n\n#\u753b\u50cf\u7528\u306e\u5b66\u7fd2\u30bf\u30fc\u30b2\u30c3\u30c8\u3092\u8ffd\u52a0\ndf_tra[\"price_ratio\"]=(df_tra[\"price\"]\/df_tra[\"price_pred1\"])","686cbe7a":"import cv2\nimport glob","3400849e":"#\u753b\u50cf\u8aad\u307f\u8fbc\u307f\u95a2\u6570\ndef load_images(df,inputPath,size,roomType):\n    images = []\n    for i in df['id']:\n        basePath = os.path.sep.join([inputPath, \"{}_{}*\".format(i,roomType)])\n        housePaths = sorted(list(glob.glob(basePath)))\n\n        for housePath in housePaths:\n            image = cv2.imread(housePath)\n            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n            image = cv2.resize(image, (size, size))\n            \n            center = (int(size\/2), int(size\/2))\n            angle = random.uniform(-15, 15) #\u56de\u8ee2\u89d2\u3092\u6307\u5b9a            \n            scale = 1.0 #\u30b9\u30b1\u30fc\u30eb\u3092\u6307\u5b9a\n            \n            trans = cv2.getRotationMatrix2D(center, angle , scale)\n            image = cv2.warpAffine(image, trans, (size,size))\n        images.append(image)\n    return np.array(images) \/ 255.0\n\n\n\"\"\"\n\ndef concat_tile(im_list_2d):\n    return cv2.vconcat([cv2.hconcat(im_list_h) for im_list_h in im_list_2d])\n\n#\u753b\u50cf\u8aad\u307f\u8fbc\u307f\u95a2\u6570\ndef load_images(df,inputPath,size,roomType):\n    images = []\n    \n    for i in df['id']:\n        basePath = os.path.sep.join([inputPath, \"{}_*\".format(i)])\n        housePaths = sorted(list(glob.glob(basePath)))\n\n        hitomatome = []\n        for housePath in housePaths:\n            \n            image = cv2.imread(housePath)\n            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n            image = cv2.resize(image, (size, size))\n            hitomatome.append(image)\n            if len(hitomatome) == 4:\n                image = concat_tile([\n                                        [hitomatome[0], hitomatome[1]],\n                                        [hitomatome[2], hitomatome[3]]\n                                    ])                \n                image = cv2.resize(image, (size, size))\n                images.append(image)\n                hitomatome = []\n    return np.array(images) \/ 255.0\n\n\"\"\"\n\n","2c691359":"#train\/test\u5171\u901a\u306e\u30a4\u30e1\u30fc\u30b8\u30d1\u30e9\u30e1\u30bf\nsize = 50 \nroomType = 'frontal'\n\n# load train images\ninputPath = '..\/input\/5th-datarobot-ai-academy-deep-learning\/images\/train_images\/'\ntrain_images = load_images(df_tra,inputPath,size,roomType)\n\n# load test images\ninputPath = '..\/input\/5th-datarobot-ai-academy-deep-learning\/images\/test_images\/'\ntest_images = load_images(df_tst,inputPath,size,roomType)\n\n\n#\u3061\u3083\u3093\u3068\u8aad\u3081\u305f\u304b\u78ba\u8a8d\u30b7\u30ea\u30fc\u30ba\n#display(train_images.shape)\n#display(train_images[0][0][0])\n#plt.imshow(train_images[1])","31abf72b":"plt.imshow(train_images[3])","279f3b16":"plt.imshow(test_images[2])","791cb34e":"#\u8a13\u7df4\u3068\u30c6\u30b9\u30c8\nfrom sklearn.model_selection import train_test_split\ntrain_x, valid_x, train_images_x, valid_images_x = train_test_split(df_tra, train_images, test_size=0.2)\ntrain_y = train_x['price_ratio'].values #.reshape(-1,1)\nvalid_y = valid_x['price_ratio'].values #.reshape(-1,1)\n\n#\u3061\u3083\u3093\u3068\u8aad\u3081\u305f\u304b\u78ba\u8a8d\u30b7\u30ea\u30fc\u30ba\n#display(train_images_x.shape)\n#display(valid_images_x.shape)\n#display(train_y.shape)\n#display(valid_y.shape)","c2bd8d9f":"def create_cnn(inputShape):\n    model = Sequential()\n    \"\"\"\n    \u6f14\u7fd2:kernel_size\u3092\u5909\u66f4\u3057\u3066\u307f\u3066\u304f\u3060\u3055\u3044\n    \"\"\"    \n    model.add(Conv2D(filters=32, kernel_size=(5, 5), strides=(1, 1), padding='same',\n                     activation='relu', kernel_initializer='he_normal', input_shape=inputShape))\n    model.add(MaxPooling2D(pool_size=(2, 2)))\n    model.add(BatchNormalization())\n    model.add(Dropout(0.1))\n\n    model.add(Conv2D(filters=64, kernel_size=(5, 5), strides=(1, 1), padding='same', \n                     activation='relu', kernel_initializer='he_normal'))\n    model.add(MaxPooling2D(pool_size=(2, 2)))\n    model.add(BatchNormalization())\n    model.add(Dropout(0.1))\n\n    \n    \"\"\"\n    \u6f14\u7fd2:\u3082\u3046\u4e00\u5c64Conv2D->MaxPooling2D->BatchNormalization->Dropout\u3092\u8ffd\u52a0\u3057\u3066\u304f\u3060\u3055\u3044\n     \n    \u2605\u8db3\u3057\u3066\u307f\u305f\u3088\n    model.add(Conv2D(filters=128, kernel_size=(5, 5), strides=(1, 1), padding='same', \n                     activation='relu', kernel_initializer='he_normal'))\n    model.add(MaxPooling2D(pool_size=(2, 2)))\n    model.add(BatchNormalization())\n    model.add(Dropout(0.1))\n    \"\"\"    \n    \n    \n    model.add(Flatten())\n    \n    model.add(Dense(units=256, activation='relu',kernel_initializer='he_normal'))  \n    model.add(Dense(units=32, activation='relu',kernel_initializer='he_normal'))    \n    model.add(Dense(units=1, activation='linear'))\n    \n    model.compile(loss='mape', optimizer='adam', metrics=['mape']) \n    return model","badb7929":"from tensorflow.keras.layers import Conv2D, MaxPooling2D,Flatten,GlobalAveragePooling2D\nfrom tensorflow.keras.layers import BatchNormalization #,Activation,Dropout,Dense\n\n\n# callback parameter\nfilepath = \"cnn_best_model.hdf5\" \nes = EarlyStopping(patience=5, mode='min', verbose=1) \ncheckpoint = ModelCheckpoint(monitor='val_loss', filepath=filepath, save_best_only=True, mode='auto') \nreduce_lr_loss = ReduceLROnPlateau(monitor='val_loss',  patience=2, verbose=1,  mode='min')\n\n# \u8a13\u7df4\u5b9f\u884c\ninputShape = (size, size, 3)\nmodel = create_cnn(inputShape)\nhistory = model.fit(train_images_x, train_y, validation_data=(valid_images_x, valid_y),epochs=30, batch_size=32,\n    callbacks=[es, checkpoint, reduce_lr_loss])\n","4d449066":"def mean_absolute_percentage_error(y_true, y_pred): \n    y_true, y_pred = np.array(y_true), np.array(y_pred)\n    return np.mean(np.abs((y_true - y_pred) \/ y_true)) * 100\n\n# load best model weights\nmodel.load_weights(filepath)\n\n# \u8a55\u4fa1\nvalid_pred = model.predict(valid_images_x, batch_size=32) #.reshape((-1,1))\nmape_score = mean_absolute_percentage_error(valid_y, valid_pred)\nprint (mape_score)","77b9fc96":"#\u753b\u50cf\u56de\u8ee2\u30eb\u30fc\u30c1\u30f3\u30aa\u30f3 59.68971093345623\n#\u753b\u50cf\u30b5\u30a4\u30ba\u3092 100\u304b\u308960\u306b\u5909\u66f4 55.799141296241906\n#\u753b\u50cf\u30b5\u30a4\u30ba\u3092 60\u304b\u308950\u306b\u5909\u66f4 52.08525775032853\n#\u753b\u50cf\u30b5\u30a4\u30ba50\u306e\u307e\u3093\u307e\u3067\u6700\u521d\u304b\u3089\u5b9f\u884c\u3057\u306a\u304a\u3057 49.50484609736257\n#\u753b\u50cf\u30b5\u30a4\u30ba50\u3067\u30d6\u30ec\u5e45\u3092+-15\u5ea6 46.252672282430204","dbc88349":"df_tst['price_ratio'] = model.predict(test_images, batch_size=32) #.reshape((-1,1))\ndf_tst['price']       = (df_tst[\"price_pred1\"]*df_tst[\"price_ratio\"]).astype('int64')","23f2018e":"#\u30d5\u30a1\u30a4\u30eb\u540d\u3092\u751f\u6210\nfrom datetime import datetime, timedelta, timezone\nimport datetime\nJST = timezone(timedelta(hours=+9), 'JST')\nnow = datetime.datetime.now(JST)\nfilename = 'submission_{0:%Y%m%d_%H%M%S}.csv'.format(now)\n\n#\u30d5\u30a1\u30a4\u30eb\u51fa\u529b\nsubmission = df_tst[['id','price']]\nsubmission.to_csv(filename,index=False)\nsubmission.head()","0d8537b9":"# \u4ea4\u5dee\u691c\u5b9a\u7528\u30c7\u30fc\u30bf\u5206\u5272\u2192\u30e2\u30c7\u30eb\u306e\u8a13\u7df4\u2192\u30e2\u30c7\u30eb\u306e\u8a55\u4fa1","26d6d09f":"#  \u30e2\u30c7\u30eb\u3092\u8a55\u4fa1\u3059\u308b\u95a2\u6570\u3092\u5b9a\u7fa9\u3059\u308b","680356ea":"# \u30e2\u30c7\u30eb\u53ef\u8996\u5316\uff08\u6700\u5f8c\u306e\u30d5\u30a9\u30fc\u30eb\u30c9\u3060\u3051\uff09","fe213fb1":"# \u8a13\u7df4\u5c65\u6b74\u53ef\u8996\u5316\uff08\u6700\u5f8c\u306e\u30d5\u30a9\u30fc\u30eb\u30c9\u3060\u3051\uff09","bdff8ab5":"# \u30e9\u30a4\u30d6\u30e9\u30ea\u306e\u30a4\u30f3\u30dd\u30fc\u30c8 ","f22828b9":"# \u30e2\u30c7\u30eb\u8a55\u4fa1","77dc1dc4":"# \u30c7\u30fc\u30bf\u524d\u51e6\u7406","859dbf76":"# \u2605\u2605\u2605\u4e88\u6e2c\u4fa1\u683c\u304b\u3089\u753b\u50cf\u52a0\u5473\u3057\u3066\u672c\u5f53\u306e\u4fa1\u683c\u3092\u4e88\u6e2c\u3059\u308b\u51e6\u7406","53952f2b":"# MLP Sequential\u30e2\u30c7\u30eb\u3092\u5b9a\u7fa9\u3059\u308b","4e8a9601":" # \u6b20\u640d\u5024\u51e6\u7406\u3068\u6b63\u898f\u5316"}}