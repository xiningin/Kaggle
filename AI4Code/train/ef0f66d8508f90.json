{"cell_type":{"d660f791":"code","b1856231":"code","aea7fae6":"code","c7064a3e":"code","7c4064ee":"code","74caca06":"code","36664ed5":"code","11864cc7":"code","7fbd1c2c":"code","3a2588bf":"code","fc1e298f":"code","cc003b66":"code","5590ccb5":"code","49f20b1e":"code","b3dea81f":"code","608046a5":"code","bc19616f":"code","95847ad1":"code","e0ea364c":"code","9917411b":"code","b0662802":"markdown","00fbdaed":"markdown","8b2d62aa":"markdown","52588c7c":"markdown","90a44ff8":"markdown","84045c5e":"markdown","919ed038":"markdown"},"source":{"d660f791":"import os\nimport sys\nimport numpy as np\nimport keras\nimport shap\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.utils import to_categorical\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Activation, Conv2D, Input, Embedding, Reshape, MaxPool2D, Concatenate, Flatten, Dropout, Dense, Conv1D\nfrom keras.layers import MaxPool1D\nfrom keras.models import Model\nfrom keras.callbacks import ModelCheckpoint\nfrom keras.optimizers import Adam\nimport warnings\nwarnings.filterwarnings('ignore')","b1856231":"# check dataset is added properly \n!ls '..\/input\/20-newsgroup-original\/20_newsgroup\/20_newsgroup\/'","aea7fae6":"TEXT_DATA_DIR = r'..\/input\/20-newsgroup-original\/20_newsgroup\/20_newsgroup\/'\nGLOVE_DIR = r'..\/input\/glove6b\/'\n# make the max word length to be constant\nMAX_WORDS = 10000\nMAX_SEQUENCE_LENGTH = 100\nVALIDATION_SPLIT = 0.20\n# the dimension of vectors to be used\nEMBEDDING_DIM = 100\n# filter sizes of the different conv layers \nfilter_sizes = [3,4,5]\nnum_filters = 512\nembedding_dim = 100\ndrop = 0.5\nbatch_size = 30\nepochs = 2","c7064a3e":"texts = []  # list of text samples\nlabels_index = {}  # dictionary mapping label name to numeric id\nlabels = []  # list of label ids\n\nfor name in sorted(os.listdir(TEXT_DATA_DIR)):\n    path = os.path.join(TEXT_DATA_DIR, name)\n    if os.path.isdir(path):\n        label_id = len(labels_index)\n        labels_index[name] = label_id\n        for fname in sorted(os.listdir(path)):\n            if fname.isdigit():\n                fpath = os.path.join(path, fname)\n                if sys.version_info < (3,):\n                    f = open(fpath)\n                else:\n                    f = open(fpath, encoding='latin-1')\n                t = f.read()\n                i = t.find('\\n\\n')  # skip header\n                if 0 < i:\n                    t = t[i:]\n                texts.append(t)\n                f.close()\n                labels.append(label_id)\n\nprint(labels_index)\nprint('Found %s texts.' % len(texts))","7c4064ee":"tokenizer  = Tokenizer(num_words = MAX_WORDS)\ntokenizer.fit_on_texts(texts)\nsequences =  tokenizer.texts_to_sequences(texts)\nword_index = tokenizer.word_index\nprint(\"Unique words : {}\".format(len(word_index)))\n\ndata = pad_sequences(sequences, maxlen = MAX_SEQUENCE_LENGTH)\n\n# keep original clf value\nlabels_clf = labels\n# transform label to one hot encoding\nlabels = to_categorical(np.asarray(labels))\nprint('Shape of data tensor:', data.shape)\nprint('Shape of label tensor:', labels.shape)\nprint(labels)","74caca06":"# split the data into a training set and a validation set\nindices = np.arange(data.shape[0])\nnp.random.shuffle(indices)\ndata = data[indices]\nlabels = labels[indices]\nnb_validation_samples = int(VALIDATION_SPLIT * data.shape[0])\n\nx_train = data[:-nb_validation_samples]\ny_train = labels[:-nb_validation_samples]\nx_test = data[-nb_validation_samples:]\ny_test = labels[-nb_validation_samples:]","36664ed5":"embeddings_index = {}\nf = open(os.path.join(GLOVE_DIR, 'glove.6B.100d.txt'))\nfor line in f:\n    values = line.split()\n    word = values[0]\n    coefs = np.asarray(values[1:], dtype='float32')\n    embeddings_index[word] = coefs\nf.close()\n\nprint('Found %s word vectors.' % len(embeddings_index))","11864cc7":"embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\nfor word, i in word_index.items():\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        # words not found in embedding index will be all-zeros.\n        embedding_matrix[i] = embedding_vector","7fbd1c2c":"from keras.layers import Embedding\n\nembedding_layer = Embedding(len(word_index) + 1,\n                            EMBEDDING_DIM,\n                            weights = [embedding_matrix],\n                            input_length = MAX_SEQUENCE_LENGTH,\n                            trainable = False)","3a2588bf":"inputs = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\nembedding = embedding_layer(inputs)\n\nprint(embedding.shape)\nreshape = Reshape((MAX_SEQUENCE_LENGTH,EMBEDDING_DIM,1))(embedding)\nprint(reshape.shape)\n\nconv_0 = Conv2D(num_filters, kernel_size=(filter_sizes[0], embedding_dim), padding='valid', kernel_initializer='normal', activation='relu')(reshape)\nconv_1 = Conv2D(num_filters, kernel_size=(filter_sizes[1], embedding_dim), padding='valid', kernel_initializer='normal', activation='relu')(reshape)\nconv_2 = Conv2D(num_filters, kernel_size=(filter_sizes[2], embedding_dim), padding='valid', kernel_initializer='normal', activation='relu')(reshape)\n\nmaxpool_0 = MaxPool2D(pool_size=(MAX_SEQUENCE_LENGTH - filter_sizes[0] + 1, 1), strides=(1,1), padding='valid')(conv_0)\nmaxpool_1 = MaxPool2D(pool_size=(MAX_SEQUENCE_LENGTH - filter_sizes[1] + 1, 1), strides=(1,1), padding='valid')(conv_1)\nmaxpool_2 = MaxPool2D(pool_size=(MAX_SEQUENCE_LENGTH - filter_sizes[2] + 1, 1), strides=(1,1), padding='valid')(conv_2)\n\nconcatenated_tensor = Concatenate(axis=1)([maxpool_0, maxpool_1, maxpool_2])\nflatten = Flatten()(concatenated_tensor)\ndropout = Dropout(drop)(flatten)\noutput = Dense(units=20, activation='softmax')(dropout)\n\n# this creates a model that includes\nmodel = Model(inputs=inputs, outputs=output)\n\ncheckpoint = ModelCheckpoint('weights_cnn_sentece.hdf5', monitor='val_acc', verbose=1, save_best_only=True, mode='auto')\nadam = Adam(lr=1e-4, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n\nmodel.compile(optimizer=adam, loss='binary_crossentropy', metrics=['accuracy'])\nmodel.summary()\n","fc1e298f":"print(\"Traning Model...\")\nmodel.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, callbacks=[checkpoint], validation_data=(x_test, y_test))","cc003b66":"# select a set of samples to take an expectation over\ndistrib_samples = x_train[:100]\nsession = keras.backend.tensorflow_backend.get_session()\n# session had to be manually specified\n# otherwise looked for Keras.._SESSION ct. which doesn't exist!\nexplainer = shap.DeepExplainer(model, distrib_samples, session)\nnum_explanations = 10","5590ccb5":"shap_values = explainer.shap_values(x_test[:num_explanations])","49f20b1e":"num2word = {}\nfor w in word_index.keys():\n    num2word[word_index[w]] = w\nx_test_words = np.stack([np.array(list(map(lambda x: num2word.get(x, \"NONE\"), x_test[i]))) for i in range(10)])\nshap.summary_plot(shap_values, feature_names = list(num2word.values()), class_names = list(labels_index.keys()),)","b3dea81f":"# init the JS visualization code\nshap.initjs()\n# create dict to invert word_idx k,v order\nnum2word = {}\nfor w in word_index.keys():\n    num2word[word_index[w]] = w\nx_test_words = np.stack([np.array(list(map(lambda x: num2word.get(x, \"NONE\"), x_test[i]))) for i in range(10)])\n\n# plot the explanation of a given prediction\nclass_num = 9\ninput_num = 5\nshap.force_plot(explainer.expected_value[class_num], shap_values[class_num][input_num], x_test_words[input_num])","608046a5":"# reverse idx for labels\nnum2label = {}\nfor w in labels_index.keys():\n    num2label[labels_index[w]] = w\nx_test_labels = np.stack([np.array(list(map(lambda x: num2label.get(x, \"NONE\"), x_test[i]))) for i in range(10)])","bc19616f":"# generate 10 predictions\ny_pred = model.predict(x_test[:10])\nsample = 8\ntrue_class = list(y_test[sample]).index(1)\npred_class = list(y_pred[sample]).index(max(y_pred[sample]))\n# one hot encoded result\nprint(f'Predicted vector is {y_pred[sample]} = Class {pred_class} = {num2label[pred_class]}')\n# filter padding words\nprint(f'Input features\/words:')\nprint(x_test_words[sample][np.where(x_test_words[sample] != 'NONE')])\nprint(f'True class is {true_class} = {num2label[true_class]}')\nmax_expected = list(explainer.expected_value).index(max(explainer.expected_value))\nprint(f'Explainer expected value is {explainer.expected_value}, i.e. class {max_expected} is the most common.')","95847ad1":"kernel_explainer = shap.KernelExplainer(model.predict, distrib_samples)\nkernel_shap_values = kernel_explainer.shap_values(x_test[:num_explanations])","e0ea364c":"# plot the explanation of a given prediction\nclass_num = 13\ninput_num = 8\nshap.force_plot(kernel_explainer.expected_value[class_num], kernel_shap_values[class_num][input_num], x_test_words[input_num])","9917411b":"# explanations of the output for the given class \n# y center value is base rate for the given background data\nshap.force_plot(kernel_explainer.expected_value[class_num], kernel_shap_values[class_num], x_test_words[:10])","b0662802":"The next step is to create an **embedding matrix** from the precomputed Glove embeddings.\n\nBecause Glove embeddings are universal features that tend to perform well, we will be freezing the embedding layer and not fine-tuning it during training.","00fbdaed":"**DATASET STRUCTURE**\n\nThe dataset has a hierarchical structure i.e. all files are classified in folders by type and each document\/datapoint is a unique '.txt' file. We will proceed as follows:\n\n1. Go through the entire dataset to build text and label lists. \n2. Tokenize the entire data using Keras' tokenizer utility.\n3. Add padding to the sequences to make them of a uniform length.","8b2d62aa":"**ARCHITECTURE**\n\n<a href=\"https:\/\/imgur.com\/xLrP6IM\"><img src=\"https:\/\/i.imgur.com\/xLrP6IM.png\" title=\"source: imgur.com\" style=\"width:400px;height:600px;\"\/><\/a>","52588c7c":"**DeepExplainer**","90a44ff8":"**INTRODUCTION**\n\nThis notebook contains:\n- A simple a **single channel model with pretrasined Glove embeddings**. \n- A local model explanation using the SHAP DeepExplainer class.\n- A global model explanation using Shapley values and fair feature importance.\n\nThe dataset used is [20_newsgroup dataset](http:\/\/www.cs.cmu.edu\/afs\/cs\/project\/theo-20\/www\/data\/news20.html).\n","84045c5e":"**Kernel explainer**","919ed038":"**EXPLAINABILITY**\n\nCompute Shapley values with SHAP's DeepExplainer class. This allows us to generate multiple model interpretability graphics."}}