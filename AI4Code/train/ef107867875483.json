{"cell_type":{"2a26c2a2":"code","af005a92":"code","45af0322":"code","22333c6c":"code","aab38ffa":"code","795c5dea":"code","c03d87b5":"code","7678c72f":"code","4149aeac":"code","f5e6b319":"code","8fcf5213":"code","3bfb3760":"code","4de05436":"code","c57316ad":"code","7e6f0cd2":"code","8a655451":"code","84913d7b":"code","840402c3":"code","233f355f":"code","83de8166":"code","fbca3482":"code","98ff9df3":"code","dc833a04":"code","e875ca7f":"code","02e6fbc1":"code","c2d9899c":"code","be995147":"code","5db1dee7":"code","dd1a1868":"code","15046bd3":"code","a434f08c":"code","1b85d4e6":"code","74d1969a":"code","522fc350":"code","da4b6a21":"code","45554c15":"code","ae2c66f0":"code","3e1e1391":"code","f7cd1c7e":"code","b34c05e5":"code","dff0b873":"code","115ee034":"code","6d97c989":"code","959021dd":"code","e9abced3":"code","1c104f1c":"code","76ae37f8":"code","c361d253":"code","f1d2095c":"code","2de86854":"code","e7acd2ff":"markdown","b109b64a":"markdown","cd98513c":"markdown","a12f58eb":"markdown","b7511d41":"markdown","5ba48a22":"markdown","f3987359":"markdown","e7c02892":"markdown","7949fd7b":"markdown"},"source":{"2a26c2a2":"# Important libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns","af005a92":"df = pd.read_csv(\"\/kaggle\/input\/amazon-top-50-bestselling-books-2009-2019\/bestsellers with categories.csv\")\ndf.head()","45af0322":"df.info()","22333c6c":"df.isna().sum().max()","aab38ffa":"# No missing data\ndf.describe()","795c5dea":"df.shape","c03d87b5":"# First thing to notice - Name won't really matter here so we will drop this column\ndf.drop('Name', axis=1, inplace=True)\ndf.head()","7678c72f":"df['Author'].nunique()","4149aeac":"# There are 248 authors for 550 books - that's around 2 books for one author which is not a lot\n# That means that there are a lot of authors that probably have one book\ndf['Author'].value_counts().median()","f5e6b319":"# This means that there are a lot of authors that have only one book in the category\n# Now I will calculate how many authors have more than one occurence\nauthors = df['Author'].value_counts()\nauthors_data = pd.DataFrame(authors)\nauthors_data = authors_data.reset_index()\nauthors_data.columns = ['Author', 'Occurrences']\nauthors_data[authors_data['Occurrences'] > 1]","8fcf5213":"# We see that 118 authors have more than 2 books on the list and 130 authors have only one book\n# I will change the author column to reflect that fact\ndef has_more_than_one_book(author, authors_data):\n    if (int(authors_data[authors_data['Author'] == author]['Occurrences']) == 1):\n        return 'One Book'\n    else:\n        return 'More than one book'\n    \ndf['Author'] = df['Author'].apply(lambda x: has_more_than_one_book(x, authors_data))\ndf.head()","3bfb3760":"# Now it's time to see whether this fact affects user rating\ndf.groupby('Author').describe()['User Rating']","4de05436":"sns.boxplot(data=df, x='Author', y='User Rating')","c57316ad":"# We can see that only authors that got more than one book on the list got a review lower than 4.0\ndf['Reviews'].hist(bins=30)","7e6f0cd2":"# Most reviews are within range 0-30000\naverage_reviews = df.groupby('User Rating').mean()\naverage_reviews","8a655451":"plt.figure(figsize=(12, 6))\naverage_reviews.sort_values(by=['Reviews'])['Reviews'].plot(kind='bar')","84913d7b":"# We can see that there are a ton of reviews for 3.8 user rating and a lot for 3.9 and 4.1 user rating\nsns.lineplot(data=df, x='User Rating', y='Reviews')","840402c3":"plt.figure(figsize=(12, 6))\naverage_reviews.sort_values(by=['Price'])['Price'].plot(kind='bar')","233f355f":"# The highest price is for books that have 4.5 rating and lowest for books that have 4.9 rating\n# We can see that there are a ton of reviews for 3.8 user rating and a lot for 3.9 and 4.1 user rating\nsns.lineplot(data=df, x='User Rating', y='Price')","83de8166":"plt.figure(figsize=(12, 6))\naverage_reviews.sort_values(by=['Year'])['Year'].plot(kind='bar', ylim=(2011, 2017))","fbca3482":"# We can see that the oldest books have the rating of 4.2 and the highest around 4.9\nsns.lineplot(data=df, x='User Rating', y='Year')","98ff9df3":"df['Genre'].value_counts()","dc833a04":"# There is a similar amount of genres there\ndf.groupby('Genre').mean()['User Rating']","e875ca7f":"# Fiction novels tend to have a higher rating than non-fiction ones\nsns.boxplot(data=df, x='Genre', y='User Rating')","02e6fbc1":"# Looking at boxplot we can see that fiction novels have overall much better ratings but they also have\n# ratings that are very low compared to non fiction novels","c2d9899c":"# I need to create dummy variables before I decide to test my data\ndata = pd.get_dummies(data=df['Author'], drop_first=True)\ndf = pd.concat([df, data], axis=1)\ndf.drop('Author', axis=1, inplace=True)\n\ndata = pd.get_dummies(data=df['Genre'], drop_first=True)\ndf = pd.concat([df, data], axis=1)\ndf.drop('Genre', axis=1, inplace=True)\n\ndf.head()","be995147":"from sklearn.model_selection import train_test_split","5db1dee7":"X = df.drop('User Rating', axis=1)\ny = df['User Rating']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)","dd1a1868":"# We will use a few different machine learning algorithms and chech which one of them returns the best results","15046bd3":"from sklearn.linear_model import LinearRegression\nfrom sklearn import metrics","a434f08c":"lm = LinearRegression()\nlm.fit(X_train, y_train)\npredictions = lm.predict(X_test)","1b85d4e6":"predictions","74d1969a":"# We can use scatterplot to visualise whether the linear regression was successful in finding user ratings\nax = sns.scatterplot(x=y_test, y=predictions)\nax.set(xlabel='Real values', ylabel='Predictions')\nplt.show()","522fc350":"# We see that linear regression didn't output results that can be easily fitted into a linear function\nRMSE_linear_regression = np.sqrt(metrics.mean_squared_error(y_test, predictions))\nprint('RMSE: ', RMSE_linear_regression)","da4b6a21":"from sklearn.tree import DecisionTreeRegressor","45554c15":"dtree = DecisionTreeRegressor(random_state=42)\ndtree.fit(X_train, y_train)\npredictions = dtree.predict(X_test)","ae2c66f0":"predictions","3e1e1391":"RMSE_decision_tree = np.sqrt(metrics.mean_squared_error(y_test, predictions))\nprint('RMSE: ', RMSE_decision_tree)","f7cd1c7e":"from sklearn.ensemble import RandomForestRegressor","b34c05e5":"rtree = RandomForestRegressor(random_state=42)\nrtree.fit(X_train, y_train)\npredictions = rtree.predict(X_test)","dff0b873":"predictions","115ee034":"RMSE_random_forest = np.sqrt(metrics.mean_squared_error(y_test, predictions))\nprint('RMSE: ', RMSE_random_forest)","6d97c989":"from sklearn.ensemble import GradientBoostingRegressor","959021dd":"gbtree = GradientBoostingRegressor(random_state=42)\ngbtree.fit(X_train, y_train)\npredictions = gbtree.predict(X_test)","e9abced3":"predictions","1c104f1c":"RMSE_boosting_tree = np.sqrt(metrics.mean_squared_error(y_test, predictions))\nprint('RMSE: ', RMSE_boosting_tree)","76ae37f8":"len(predictions)","c361d253":"RMSE_df = pd.DataFrame(data=[['Linear Regression', RMSE_linear_regression], \n                                ['Decision Tree', RMSE_decision_tree], \n                                ['Random Forest', RMSE_random_forest], \n                                ['Gradient Boosting Tree', RMSE_boosting_tree]],\n                         columns=['Model', 'RMSE'])\nRMSE_df","f1d2095c":"plt.figure(figsize=(12, 6))\nsns.barplot(x='Model', y='RMSE', data=RMSE_df, order=RMSE_df.sort_values('RMSE').Model)\nplt.ylim(0.2, 0.3)","2de86854":"RMSE_df = RMSE_df.sort_values('RMSE')\nRMSE_df.reset_index(drop=True, inplace=True)\nRMSE_df","e7acd2ff":"# Exploratory data analysis","b109b64a":"# Summary of a machine learning model","cd98513c":"# Decision Tree","a12f58eb":"# Gradient Boosting Tree","b7511d41":"# Our goal is to predict which rating will the user give to the movie\n## Importing data","5ba48a22":"# Random Forest","f3987359":"# Linear Regression","e7c02892":"# Training Data","7949fd7b":"### We can see that the decision tree was the worst model in predicting reviews\n### Linear regression is the second worst (although it did quite well for a simple machine learning algorithm)\n### Random Forest and Gradient Boosting Tree were the best in predicting the user rating"}}