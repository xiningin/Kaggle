{"cell_type":{"a11e4ab8":"code","069a436a":"code","a20ad38c":"code","b141ff90":"code","7335a045":"code","d0d3fe26":"code","33b9ac61":"code","c9d4dedd":"code","d496a43f":"code","20b5f25a":"code","d94fdad9":"code","04a2664f":"code","b3b0c1ee":"code","d9462a1b":"code","4d320770":"code","2a309965":"code","57dc1769":"code","fe8cc6b7":"code","9bde016d":"code","fffc9c93":"code","2a8acb7f":"code","046285c5":"code","8a4de15a":"code","ddb42c9c":"code","b1133c7d":"code","51f90fd8":"code","629b9e41":"code","d6d774a4":"code","b23bfc37":"code","776ec499":"code","288be4a5":"code","ac98765b":"code","02c23ad1":"code","b68b2534":"code","14e7b79e":"code","f5fbd50b":"code","6f4231a1":"code","75e4ac64":"code","16a7f170":"code","65249f09":"code","0c187ce5":"code","2c2e9355":"code","ad0e7809":"code","69862092":"code","ea62bffb":"code","b7a29f2c":"code","178e6552":"code","cbbd12f4":"code","fec14d66":"code","8e38ada8":"code","26156c19":"code","3f0b3cd3":"code","e0c45be3":"code","01e68acb":"code","0e6defbb":"code","0d5af513":"code","ff22d64a":"code","6d29de02":"code","b62dfced":"code","b7978e79":"code","06fc0afd":"code","b406dff4":"code","0b0e2c03":"code","0151c4ac":"code","4dd89708":"code","6576023e":"code","143cf503":"code","1301fad2":"code","433097aa":"code","2caabea7":"code","f74dee52":"code","d0924050":"code","e13f0229":"code","1f0aad04":"code","a436175e":"code","a64e600e":"code","2a1822b2":"code","8bc6cee5":"code","7f249c5d":"code","4fc592d9":"code","5a9916bc":"code","925be8c2":"code","26a01635":"code","47047e86":"code","4519f533":"code","68e5c5b9":"code","c0c36133":"code","9f248a42":"code","460eaadf":"code","5c2c49c9":"code","61147382":"code","bea0f02c":"code","fba19933":"code","335299af":"code","4a993e89":"code","12de5059":"code","6ac9ac9a":"code","fb656583":"code","78a2fb0d":"code","82ec8869":"code","e7c77caa":"code","d2364218":"code","4e1b15a7":"code","c719b627":"code","cdd53398":"code","193a1037":"code","8b6efc6a":"code","8d0f8645":"code","ac2c5905":"markdown","d8eae08a":"markdown","80b784f9":"markdown","a0b3f536":"markdown","a0f56ca4":"markdown","c0538019":"markdown","7dd60e2e":"markdown","dcd7dd63":"markdown","3108b59b":"markdown","c1471cb9":"markdown","6e100aaa":"markdown","a1491f05":"markdown","f989e211":"markdown","56600f62":"markdown","4a8b955b":"markdown","6435aa8b":"markdown","6e6720e9":"markdown","2e82143a":"markdown"},"source":{"a11e4ab8":"# Regular EDA and plotting libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\n# Scikit-Learn ML models\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\n\n# Model Evaluation tools\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.model_selection import RandomizedSearchCV, GridSearchCV\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.metrics import precision_score, recall_score, f1_score\nfrom sklearn.metrics import plot_roc_curve\n\n# Scaling\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import MinMaxScaler","069a436a":"# Data\ndata_p = pd.read_csv('..\/input\/heart-disease-uci\/heart.csv')","a20ad38c":"data_p.head()","b141ff90":"data_p.info()","7335a045":"# Visualizing the correlation matrix\ncorr_matrix = data_p.corr()\nfig, ax = plt.subplots(figsize=(15, 10))\nax = sns.heatmap(corr_matrix,\n                annot=True,\n                linewidths=0.5,\n                fmt=\".2f\",\n                cmap='YlGnBu');","d0d3fe26":"type(data_p.corr())","33b9ac61":"# Calculate heart disease ratio from true\/false of outcome variable\nn_true = len(data_p.loc[data_p['target'] == True])\nn_false = len(data_p.loc[data_p['target'] == False ])\nprint(\"Number of Positive cases: {0}. Percentage = {1}\".format(n_true, n_true\/(n_true + n_false) * 100))\nprint(\"Number of Negative cases: {0}. Percentage = {1}\".format(n_false, n_false\/(n_true + n_false) * 100))\n\n# Visualization\ndic_1 = {\"Postive\": n_true, \"Negative\": n_false}\nfig, ax = plt.subplots(figsize=(4, 4))\nax.bar(dic_1.keys(), dic_1.values(), width=0.8, color=['salmon', 'lightblue'])\nax.set(title=\"Number of Cases\",\n       ylabel=\"Number of cases\");","c9d4dedd":"# Creating a box plot\nax = sns.boxplot(x='age', y='sex', data=data_p, orient=\"h\");\nax.set(title='Whisker Plot of Age vs. Sex',\n       ylabel=\"M = 1| F = 0\");","d496a43f":"# Ratio of male and female postive & negative cases\npd.crosstab(data_p['sex'], data_p['target'])","20b5f25a":"# Find the categorical variables\ncategorical_variables = []\n\nfor column in data_p:\n    if len(data_p[column].unique()) <= 10:\n        categorical_variables.append(column)\n\n# Remove the target variable\ncategorical_variables.remove('target')\n\nprint(categorical_variables)","d94fdad9":"# Make dummy variables for categorical variables\ndata = pd.get_dummies(data_p, columns=categorical_variables)","04a2664f":"# Split the data\nX = data.drop('target', axis=1)\ny = data['target']","b3b0c1ee":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=7)","d9462a1b":"# Non-scaled data\nX_train_ws = X_train\nX_test_ws = X_test","4d320770":"# Scaled data\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.fit_transform(X_test)","2a309965":"# Models to use\nmodels = {\"Logistic Regression\": LogisticRegression(random_state=7, solver='liblinear'),\n          \"Random Forest\": RandomForestClassifier(random_state=7),\n          \"K-Nearest Neighbors\": KNeighborsClassifier(),\n          \"SVM Kernel\": SVC(kernel=\"rbf\", random_state=7),\n          \"Naive Bayes\": GaussianNB()}\n\n# Function to fit and score models\ndef fit_score(models, X_train, X_test, y_train, y_test):\n    \"\"\"\n    1. models: arguments excepts machine learning models\n    2. X_train, X_test, y_train, y_test: arguement takes the splitted data\n    \"\"\"\n    np.random.seed(7)\n    \n    # Model scores within the dictionary\n    model_scores = {}\n    \n    # Loop through the models\n    for name, model in models.items():\n        # Fit the model to the data\n        model.fit(X_train, y_train)\n        # Evaluate the model\n        model_scores[name] = model.score(X_test, y_test)\n    return model_scores","57dc1769":"# Model accuracies with scaling\nmodel_scores = fit_score(models=models,\n                         X_train=X_train,\n                         X_test=X_test,\n                         y_train=y_train,\n                         y_test=y_test)\nmodel_scores","fe8cc6b7":"# Model accuracies without scaling\nmodel_scores_ws = fit_score(models=models,\n                       X_train=X_train_ws,\n                       X_test=X_test_ws,\n                       y_train=y_train,\n                       y_test=y_test)\nmodel_scores_ws","9bde016d":"fig, ax = plt.subplots(figsize=(10, 5))\nax.bar(model_scores.keys(), model_scores.values())\nax.legend(['Accuracy'])\nax.set(ylabel=\"Accuracy\",\n       title='Model Score Comparision (With Scaling)');","fffc9c93":"fig, ax = plt.subplots(figsize=(10, 5))\nax.bar(model_scores_ws.keys(), model_scores_ws.values())\nax.legend(['Accuracy'])\nax.set(ylabel=\"Accuracy\",\n       title='Model Score Comparision (Without Scaling)');","2a8acb7f":"# Install catboost\n# import sys\n# !conda config --add channels conda-forge\n# !conda install --yes --prefix {sys.prefix} catboost","046285c5":"# Visualize confusion matrix\ndef plot_conf_matrix(con_mat):\n    \n    fig, ax = plt.subplots(figsize=(3,3))\n    ax = sns.heatmap(con_mat,\n                     annot=True,\n                     cbar=False,)\n    ax.set(xlabel=\"True label\",\n           ylabel='Predicted label')","8a4de15a":"# Apply Min-Max scaling For CatBoost\nmms = MinMaxScaler()\nX_train_mms = mms.fit_transform(X_train_ws ,y=None)\nX_test_mms = mms.fit_transform(X_test_ws ,y=None)","ddb42c9c":"from catboost import CatBoostClassifier\n\ncb_clf = CatBoostClassifier(verbose=False)\ncb_clf.fit(X_train_mms, y_train)","b1133c7d":"y_preds_cb = cb_clf.predict(X_test_mms)","51f90fd8":"cat_conf_mat = confusion_matrix(y_test, y_preds_cb)\nplot_conf_matrix(cat_conf_mat)","629b9e41":"cb_clf.score(X_train_mms, y_train)","d6d774a4":"cb_clf.score(X_test_mms, y_test)","b23bfc37":"# Cross validation\ncb_clf_cv = cross_val_score(cb_clf,\n                           X,\n                           y,\n                           cv=5,\n                           scoring='accuracy',\n                           verbose=False)","776ec499":"cb_clf_cv.mean()","288be4a5":"cb_clf_cv.std()","ac98765b":"train_scores = {}\ntest_scores = {}\n\ntrain_scores_l = []\ntest_scores_l = []\n\n# Number of neighbors\nneighbors = range(1, 50)\n\n# Setup KNN instance\nknn = KNeighborsClassifier()\n\n# Loop through the different number of neighbors\nfor i in neighbors:\n    knn.set_params(n_neighbors=i)\n    knn.fit(X_train, y_train)\n    train_scores.update({str(i) :knn.score(X_train, y_train)})\n    test_scores.update({str(i) :knn.score(X_test, y_test)})\n    test_scores_l.append(knn.score(X_test, y_test))\n    train_scores_l.append(knn.score(X_train, y_train))","02c23ad1":"# Maximum train score value\nmax_train_val = max(train_scores.values())\nmax_train_val","b68b2534":"# Calculate the mean of training scores\ndict_vals = train_scores.values()\n\nvals_list = []\n\nfor i in dict_vals:\n    vals_list.append(i)\n    \narr_1 = np.array(vals_list)\narr_1.mean()","14e7b79e":"# Maximum test score value\nmax_test_val = max(test_scores.values())\nmax_test_val","f5fbd50b":"# Calculate the mean of test scores\ndict_vals_2 = test_scores.values()\n\nvals_list_2 = []\n\nfor i in dict_vals_2:\n    vals_list_2.append(i)\n    \narr_2 = np.array(vals_list)\narr_2.mean()","6f4231a1":"plt.plot(neighbors, train_scores_l, label=\"Train score\")\nplt.plot(neighbors, test_scores_l, label=\"Test score\")\nplt.xticks(np.arange(0, 50, 5))\nplt.xlabel(\"Number of neighbors\")\nplt.ylabel('Model score')\nplt.legend()\n\nprint(f\"Maximum KNN score on the test data: {max(test_scores_l)*100:.2f}%\");","75e4ac64":"# Find number of neighbors with the highest accuracy\nn_trees = max_test_val\n\n# Empty list to store the best number of neighbors\nmax_key = []\nfor key, val in test_scores.items(): \n    if val == n_trees:\n        print(key)\n        max_key.append(key)","16a7f170":"# Scale X for cross-validation\nX_scaled = sc.fit_transform(X)","65249f09":"# Take the best number of neighbors\nbest_num = int(max_key[1])","0c187ce5":"# Initiate KNN instance\nknn_cv = KNeighborsClassifier(n_neighbors=best_num)","2c2e9355":"# Cross-validated accuracy\ncv_knn = cross_val_score(knn_cv, \n                         X_scaled, \n                         y, \n                         cv=5, \n                         scoring='accuracy') ","ad0e7809":"cv_knn","69862092":"# Average of cross-validated scores\ncv_knn.mean()","ea62bffb":"# Standard deviation of cross-validated scores\ncv_knn.std()","b7a29f2c":"# Setting up parameters for Randomized Search CV and Grid Search CV\nKNN_params = {'n_neighbors': [3,40],\n             'p': [1, 2, 5]}","178e6552":"# Perform Randomized Search on KNN model\nrs_KNN = RandomizedSearchCV(estimator=KNeighborsClassifier(),\n                     param_distributions=KNN_params,\n                     cv=5,\n                     verbose=True,\n                     n_iter=12)\n\nrs_KNN.fit(X_train, y_train)","cbbd12f4":"rs_KNN.best_params_","fec14d66":"rs_KNN.score(X_train, y_train)","8e38ada8":"rs_KNN.score(X_test, y_test)","26156c19":"# Final model\nclf_knn = KNeighborsClassifier(n_neighbors=best_num)\nclf_knn.fit(X_train, y_train)\n# Model score on test data\nclf_knn.score(X_test, y_test)","3f0b3cd3":"# Model score on train data\nclf_knn.score(X_train, y_train)","e0c45be3":"y_preds_knn = clf_knn.predict(X_test)","01e68acb":"# Confusion matrix\nknn_conf_mat = confusion_matrix(y_test, y_preds_knn)\nplot_conf_matrix(knn_conf_mat)","0e6defbb":"# Classification report\nprint(classification_report(y_test, y_preds_knn))","0d5af513":"## Plot ROC curve and calculate AUC metric\nplot_roc_curve(clf_knn, X_test, y_test);","ff22d64a":"# Logistic Regression grid for hyperparameter tuning\nlog_reg_grid = {\"C\": np.logspace(-4, 4, 30),\n               'solver': ['liblinear']}","6d29de02":"# Perform RandomizedSearchCV on logistic regression\nrs_log_reg = RandomizedSearchCV(LogisticRegression(random_state=7),\n                                param_distributions=log_reg_grid,\n                                cv=5,\n                                n_iter=20,\n                                verbose=True)\n\nrs_log_reg.fit(X_train, y_train)","b62dfced":"rs_log_reg.best_params_","b7978e79":"rs_log_reg.score(X_test, y_test)","06fc0afd":"y_preds_rslg = rs_log_reg.predict(X_test)","b406dff4":"lg_conf_mat = confusion_matrix(y_test, y_preds_rslg)\nplot_conf_matrix(lg_conf_mat)","0b0e2c03":"# Perform GridSearchCV on logistic Regression\ngs_log_reg = GridSearchCV(LogisticRegression(random_state=7),\n                         param_grid=log_reg_grid,\n                         cv=5,\n                         verbose=True)\n\ngs_log_reg.fit(X_train, y_train)","0151c4ac":"gs_log_reg.best_params_","4dd89708":"gs_log_reg.score(X_test, y_test)","6576023e":"cv_log_reg = cross_val_score(LogisticRegression(C=0.1082636733874054,\n                                               solver='liblinear'),\n                            X_scaled,\n                            y,\n                            cv=5,\n                            scoring='accuracy')","143cf503":"cv_log_reg","1301fad2":"cv_log_reg.mean()","433097aa":"cv_log_reg.std() * 100","2caabea7":"# Final model\nclf_log = LogisticRegression(C=0.1082636733874054,\n                            solver='liblinear')\n\nclf_log.fit(X_train, y_train)","f74dee52":"clf_log.score(X_test, y_test)","d0924050":"y_preds_clf_log = clf_log.predict(X_test)","e13f0229":"print(classification_report(y_test, y_preds_clf_log))","1f0aad04":"# Plot ROC Curve\nplot_roc_curve(clf_log, X_test, y_test)","a436175e":"# GridSearch\nsvm_grid = {\"C\": np.logspace(-1, 2, 20),\n            \"gamma\": np.logspace(-4, 2, 20)}","a64e600e":"gs_svm = GridSearchCV(SVC(kernel='rbf', random_state=7),\n                      param_grid=svm_grid,\n                      cv=5,\n                      verbose=True)\n\ngs_svm.fit(X_train, y_train)","2a1822b2":"svm_params = gs_svm.best_params_","8bc6cee5":"c_param = svm_params['C']\ng_param = svm_params['gamma']","7f249c5d":"gs_svm.score(X_test, y_test)","4fc592d9":"svm_cv = cross_val_score(SVC(kernel='rbf', random_state=7, C=c_param, gamma=g_param),\n                         X_scaled,\n                         y,\n                         cv=5,\n                         scoring='accuracy')","5a9916bc":"svm_cv.mean()","925be8c2":"svm_cv.std()","26a01635":"# Final model\nsvm_clf = SVC(kernel='rbf', random_state=7, C=c_param, gamma=g_param)\nsvm_clf.fit(X_train, y_train)","47047e86":"svm_preds = svm_clf.predict(X_test)","4519f533":"svm_conf_mat = confusion_matrix(y_test, svm_preds)\nplot_conf_matrix(svm_conf_mat)","68e5c5b9":"svm_clf.score(X_test, y_test)","c0c36133":"# Dictionary of number of trees with its accuracy score\ntas = {}\n\nfor i in range(1, 2000, 100):\n    rf_clf = RandomForestClassifier(n_estimators=i)\n    rf_clf.fit(X_train_ws, y_train)\n    tas.update({str(i): rf_clf.score(X_test_ws, y_test)})","9f248a42":"max(tas.values())","460eaadf":"# Grid search for other parameters\nrf_grid = {\"max_depth\": [5, 8, 15, 25, 30],\n           \"min_samples_split\": [2, 5, 10, 15, 100],\n           \"min_samples_leaf\": [2, 5, 10],\n           \"n_estimators\": [100, 150, 200]} ","5c2c49c9":"gs_rf = GridSearchCV(RandomForestClassifier(),\n                     param_grid = rf_grid,\n                     cv = 5,\n                     verbose = True)\n\ngs_rf.fit(X_train_ws, y_train)","61147382":"gs_rf.score(X_test_ws, y_test)","bea0f02c":"rf_params = gs_rf.best_params_","fba19933":"rf_params","335299af":"rf_cv = cross_val_score(RandomForestClassifier(random_state=7, max_depth=5, min_samples_leaf=2, min_samples_split=15, n_estimators= 150),\n                        X,\n                        y,\n                        cv=5,\n                        scoring='accuracy')","4a993e89":"rf_cv.mean()","12de5059":"rf_cv.std() * 100","6ac9ac9a":"# Default model cross-validation\nrf_cv2 = cross_val_score(RandomForestClassifier(random_state=7),\n                        X,\n                        y,\n                        cv=5,\n                        scoring='accuracy')","fb656583":"rf_cv2.mean()","78a2fb0d":"rf_cv2.std() * 100","82ec8869":"# Final model\nrf_clf = RandomForestClassifier(random_state=7, n_estimators=100)\nrf_clf.fit(X_train_ws, y_train)","e7c77caa":"rf_clf.score(X_test_ws, y_test)","d2364218":"rf_clf.score(X_train_ws, y_train)","4e1b15a7":"rf_y_preds = rf_clf.predict(X_test_ws)","c719b627":"rf_conf_mat = confusion_matrix(y_test, rf_y_preds)\nplot_conf_matrix(rf_conf_mat)","cdd53398":"plot_roc_curve(rf_clf, X_test, y_test);","193a1037":"print(classification_report(y_test, rf_y_preds))","8b6efc6a":"# Final scores with hyperparameter tuning\nf_scores = {\"Cat Boost:\": cb_clf.score(X_test_mms, y_test),\n            \"KNN:\": rs_KNN.score(X_test, y_test) ,\n            \"Logistic Regression:\": rs_log_reg.score(X_test, y_test),\n            \"SVM Kernel:\": gs_svm.score(X_test, y_test),\n            \"Random Forest:\": rf_clf.score(X_test_ws, y_test)} ","8d0f8645":"for i in f_scores:\n    print(i, f_scores[i])","ac2c5905":"### Model 4: SVM Kernel","d8eae08a":"### EXTRA MODEL: CatBoost","80b784f9":" This whisker plot indicates the different quartiles of our dataset for each sex. The mean age for females in our dataset is higher than that of males. Any female subjects below the age of thirty isn't in our sample. On the other hand, male subjects cover a more vast range than female subjects","a0b3f536":"### Data Source\n\n* Kaggle: https:\/\/www.kaggle.com\/ronitf\/heart-disease-uci\n* UCI Machine Learning repository: https:\/\/archive.ics.uci.edu\/ml\/datasets\/Heart+Disease","a0f56ca4":"### Model 5: Random Forest","c0538019":"## Data","7dd60e2e":"## EDA","dcd7dd63":"#### Model performance on scaled data vs non-scaled data:\n* KNN Model had a drastic change in accuracy score with scaling. \n> **Conclusion:** Euclidean distance based models require standardisation on data.\n\n* Random Forest and Naive Bayes model performed better without scaling. \n> **Conculsion:** Random Forest and Naive Bayes (Gaussian Naive Bayes performs standardization internally) don't require standardisation on data.\n\n* Logistic Regression performed slightly better without scaling\n\n* SVM scored better with scaling. \n> **Conclusion:** Yes, SVM kernel `(Because Support Vector Machine (SVM) optimization occurs by minimizing the decision vector w, the optimal hyperplane is influenced by the scale of the input features and it\u2019s therefore recommended that data be standardized (mean 0, var 1) prior to SVM model training.)` performs better with standardisation on data.\n\nOther resource(s):\nhttps:\/\/www.youtube.com\/watch?v=mnKm3YP56PY","3108b59b":"## Modelling","c1471cb9":"### Model 1: KNN","6e100aaa":"### Model 2: Naive Bayes\n\nP.S: I don't have sufficient knowledge of hyperparameter tuning for this particular model and so, I will skip it for now and implement the model maybe again in a future update.","a1491f05":"## Model Comparision ","f989e211":"# Predicting Heart Disease using Machine Learning ","56600f62":"### Model 3: Logistic Regression\nResources: https:\/\/www.kaggle.com\/joparga3\/2-tuning-parameters-for-logistic-regression\n* Solver (According to Scikit-Learn): For small datasets, \u2018liblinear\u2019 is a good choice, whereas \u2018sag\u2019 and \u2018saga\u2019 are faster for large ones.","4a8b955b":"## Evaluation","6435aa8b":"### Data Dictionary\n\n1. age - age in years\n2. sex - (1 = male; 0 = female)\n3. cp - chest pain type\n        0: Typical angina: chest pain related decrease blood supply to the heart\n        1: Atypical angina: chest pain not related to heart\n        2: Non-anginal pain: typically esophageal spasms (non heart related)\n        3: Asymptomatic: chest pain not showing signs of disease\n4. trestbps - resting blood pressure (in mm Hg on admission to the hospital)\n    - anything above 130-140 is typically cause for concern\n5. chol - serum cholestoral in mg\/dl\n    - serum = LDL + HDL + .2 * triglycerides \n    - above 200 is cause for concern\n6. fbs - (fasting blood sugar > 120 mg\/dl) (1 = true; 0 = false)\n    - '>126' mg\/dL signals diabetes\n7. restecg - resting electrocardiographic results\n        0: Nothing to note\n        1: ST-T Wave abnormality\n            can range from mild symptoms to severe problems\n            signals non-normal heart beat\n        2: Possible or definite left ventricular hypertrophy\n            Enlarged heart's main pumping chamber\n8. thalach - maximum heart rate achieved\n9. exang - exercise induced angina (1 = yes; 0 = no)\n10. oldpeak - ST depression induced by exercise relative to rest\n    - looks at stress of heart during excercise\n    - unhealthy heart will stress more\n11. slope - the slope of the peak exercise ST segment\n        0: Upsloping: better heart rate with excercise (uncommon)\n        1: Flatsloping: minimal change (typical healthy heart)\n        2: Downslopins: signs of unhealthy heart\n12. ca - number of major vessels (0-3) colored by flourosopy\n    - colored vessel means the doctor can see the blood passing through\n    - the more blood movement the better (no clots)\n13. thal - thalium stress result\n        1,3: normal\n        6: fixed defect: used to be defect but ok now\n        7: reversable defect: no proper blood movement when excercising\n14. target - have disease or not (1=yes, 0=no) (= the predicted attribute)","6e6720e9":"Let's look at the following:\n* Hyperparameter tuning\n* Feature importance\n* Confusion matrix\n* Cross-validation\n* Precision\n* Recall\n* F1 score\n* Classification report\n* ROC curve\n* Area under the curve (AUC)","2e82143a":"**Objective:** Given clinical parameters about a patient, build a predictive model that can predict whether a patient has heart disease or not."}}