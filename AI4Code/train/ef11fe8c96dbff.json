{"cell_type":{"e92ad6e2":"code","da8e7b52":"code","3502c16b":"code","6b43d413":"code","ecdb4fe8":"code","30d197dd":"code","5733c2f1":"code","8346161b":"code","9d2a3810":"code","979e1df5":"code","f21b739f":"code","716ee442":"code","40a2e750":"code","eb769e61":"code","f8039529":"code","34a78511":"code","58676326":"code","b4060890":"code","cba21541":"code","8bac665a":"code","2ea8f368":"code","39e899d9":"code","ed4c60c6":"code","51216c55":"code","9e3f2fa7":"code","b6dbb9b4":"code","bfe57d46":"code","1c22ba56":"code","b0a7ad84":"code","bb5ec9a7":"code","13d6e44b":"code","d867f3cc":"code","2f80ffd0":"code","5b2881ee":"code","c1439a23":"code","32615250":"code","e5e40f60":"code","d05ecf3c":"code","5b270dfb":"code","69c5ed26":"code","42d10283":"code","6005c5a4":"code","ade2e308":"code","929972a0":"code","0ddb89bf":"code","809764ae":"markdown","6acd362d":"markdown","c6cb7651":"markdown","ac264ef7":"markdown","98d08181":"markdown","5b87b0c1":"markdown","1b8c5137":"markdown"},"source":{"e92ad6e2":"import os\nprint(os.listdir(\"..\/input\"))","da8e7b52":"#import libraries for data manipulation\nimport numpy as np\nimport pandas as pd\n\n# import libraries for data visualization\nimport matplotlib.pyplot as plt \nimport seaborn as sns\nimport missingno\nplt.style.use('seaborn-whitegrid')\n%matplotlib inline\n#pre-defined style is \u201cggplot\u201d\n\nimport scipy.stats as ss\n#ignore warnings for now \nimport warnings \nwarnings.filterwarnings('ignore')","3502c16b":"#fetch train data and getting its first look\ndata_train = pd.read_csv('..\/input\/train.csv')\nprint('Training data shape: ', data_train.shape)\ndata_train.head()","6b43d413":"#checking names of columns to confirm there is no extra spance or any whitespace with names of column \n#as sometimes it causes errors during data manipulation due to incorrect name. \ndata_train.columns ","ecdb4fe8":"#fetching test data and getting its first look\ndata_test=pd.read_csv('..\/input\/test.csv')\nprint('Testing data shape: ', data_test.shape)\ndata_test.head()","30d197dd":"#Note, Fare feature has missing value in test but not in train\ndata_test.info()","5733c2f1":"#checking column data type and null values in training data\ndata_train.info()\n#python str dataype = pandas object dtype","8346161b":"#statistical summary of training data\ndata_train.describe()\n#Check min and max to know the spread of the data\n#Check 25%, 50%[MEDIAN],75% (combined with min n max) to get a rough idea if it a categorical.eg Pclass and Survived\n#To confrirm the assumption : df['col_name'].unique() and then df.col_name.value_counts() to get frequency  count \n#Compare mean [AVERAGE] and 50%[MEDIAN] values\n#Compare 75% and Max\n#If MEAN <= MEDIAN and 75% << MAX, then there are extreme values-Outliers.\n#38% out of the training-set survived the Titanic.\n","9d2a3810":"#CHECKING MISSING VALUES: sorted percentage of missing data in columns of a DataFrame\ndata = [data_train, data_test]\n\nfor dataset in data:\n    missingValueCount= dataset.isnull().sum().sort_values(ascending=False)\n    missingValuePercent=(100 * dataset.isnull().sum()\/len(dataset)).sort_values(ascending=False)\n    missing_data = pd.concat([missingValueCount, missingValuePercent], axis=1, keys=['missingValueCount', 'missingValuePercent'])\n    print(missing_data.head(5))\n    print(\"-\"*50)\n\n\n#Embarked feature has only 2 missing values, which can easily be filled\n#tricky, to deal with the \u2018Age\u2019 feature, which has 177 missing values\n#Cabin\u2019 feature needs further investigation if we can drop it from the dataset, since 77 % of it are missing.","979e1df5":"#droping all columns with at-least 75% missing values in training data from test and train dataset\n#data_train.dropna(thresh=data_train.shape[0]*.75, how='all', axis='columns',inplace=True)\ncols_to_drop=missing_data[missingValuePercent>75].index.values\nfor dataset in data:\n    for cols in cols_to_drop:\n        dataset.drop([cols],axis=1,inplace=True)","f21b739f":"#FINDING MISSING VALUE FEATURES EXCLUSIVE TO TRAIN OR TEST DATA SET\n#https:\/\/stackoverflow.com\/questions\/37366717\/pandas-print-column-name-with-missing-values\ntrainDatasetMissingValuesFeautres=data_train.columns[data_train.isnull().any()].values\ntestDatasetMissingValuesFeautres=data_test.columns[data_test.isnull().any()].values\nunion=np.union1d(trainDatasetMissingValuesFeautres, testDatasetMissingValuesFeautres)\nintersect=np.intersect1d(trainDatasetMissingValuesFeautres, testDatasetMissingValuesFeautres)\nprint(np.setdiff1d(union,intersect))","716ee442":"data = [data_train, data_test]\n\nfor dataset in data:\n    # filling embarked NaN as it is a categocial data, we use mode.\n    dataset['Embarked'].fillna(dataset['Embarked'].mode().iloc[0],inplace=True)\n    # filling Fare NaN as it is a numerical data, we use mean\n    dataset['Fare'].fillna(dataset['Fare'].mean(),inplace=True)","40a2e750":"#CORRELATION USING HEATMAP #Note: it is valid for continuous-continuous data columns\n#To use linear regression for modelling,its necessary to remove correlated variables to improve your model.\nplt.figure(figsize=(10,5))\nsns.heatmap(data_train.corr(),annot=True,cmap=\"YlGnBu\",linewidths=0.2)\n#Pclass and Fare are correlated.\n#PassengerId has no correlation with target variable, safe to remove in case of linear regression","eb769e61":"#OUTLIER DETECTION using boxplot \nNumericDataFrame=data_train.select_dtypes(include='number')\nl = NumericDataFrame.columns.values\nplt.figure(figsize=(12,5))\nfor i in range(0,len(l)):\n    plt.subplot(1,len(l),i+1)\n    sns.set_style('whitegrid')\n    sns.boxplot(NumericDataFrame[l[i]],color='green',orient='v')\n    plt.tight_layout()","f8039529":"# #correlation between categorical col and target variable(target).\n# def cramers_v(x, y):\n#     confusion_matrix = pd.crosstab(x,y)\n#     chi2 = ss.chi2_contingency(confusion_matrix)[0]\n#     n = confusion_matrix.sum().sum()\n#     phi2 = chi2\/n\n#     r,k = confusion_matrix.shape\n#     phi2corr = max(0, phi2-((k-1)*(r-1))\/(n-1))\n#     rcorr = r-((r-1)**2)\/(n-1)\n#     kcorr = k-((k-1)**2)\/(n-1)\n#     return np.sqrt(phi2corr\/min((kcorr-1),(rcorr-1)))\n\n# #BETTER TO USE GRAPH FOR VISUALIZATION AND FIND USEFUL PATTERNS , IF ANY.\n# print(cramers_v(data_train['Survived'],data_train['Ticket']))\n# print(cramers_v(data_train['Survived'],data_train['Cabin']))\n# print(cramers_v(data_train['Survived'],data_train['Embarked']))","34a78511":"# setting passenger id as index \ndata = [data_train, data_test]\n\nfor dataset in data:\n    dataset.set_index('PassengerId',inplace=True)\n","58676326":"#droping features 'Name','Ticket','Cabin'\n#cabin already dropped as missing vaule count > 75\n#useful info TITLE from Name is extracted.\ndata = [data_train, data_test]\n\nfor dataset in data:\n    dataset.drop(['Ticket'],axis=1, inplace=True)","b4060890":"data = [data_train, data_test]\n\nfor dataset in data:\n    mean = dataset[\"Age\"].mean()\n    std = dataset[\"Age\"].std()\n    no_of_null = dataset[\"Age\"].isnull().sum()\n    # compute random numbers between the mean, std and is_null\n    rand_age = np.random.randint(mean - std, mean + std, size = no_of_null)\n    print(type(rand_age))\n    # fill NaN values in Age column with random values generated\n    dataset['Age'][np.isnan(dataset['Age'])] = rand_age\n    dataset[\"Age\"] = dataset[\"Age\"].astype(int)\n    print(dataset[\"Age\"].isnull().sum())\n    \n","cba21541":"#categorising age feature \ndata = [data_train, data_test]\nfor dataset in data:\n    dataset['Age'] = dataset['Age'].astype(int)\n    dataset.loc[ dataset['Age'] <= 11, 'Age'] = 0\n    dataset.loc[(dataset['Age'] > 11) & (dataset['Age'] <= 18), 'Age'] = 1\n    dataset.loc[(dataset['Age'] > 18) & (dataset['Age'] <= 22), 'Age'] = 2\n    dataset.loc[(dataset['Age'] > 22) & (dataset['Age'] <= 27), 'Age'] = 3\n    dataset.loc[(dataset['Age'] > 27) & (dataset['Age'] <= 33), 'Age'] = 4\n    dataset.loc[(dataset['Age'] > 33) & (dataset['Age'] <= 40), 'Age'] = 5\n    dataset.loc[(dataset['Age'] > 40) & (dataset['Age'] <= 66), 'Age'] = 6\n    dataset.loc[ dataset['Age'] > 66, 'Age'] = 6\n    # let's see how it's distributed \n    print(dataset['Age'].value_counts())\n\n","8bac665a":"#Extracting Name featue from the title to form new features\ndata = [data_train, data_test]\ntitles = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5}\n\nfor dataset in data:\n    # extract titles\n    dataset['Title'] = dataset.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\n    # replace titles with a more common title or as Rare\n    dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess','Capt', 'Col','Don', 'Dr',\\\n                                            'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\n    # convert titles into numbers\n    dataset['Title'] = dataset['Title'].map(titles)\n    # filling NaN with 0, to get safe\n    dataset['Title'] = dataset['Title'].fillna(0)\n    #droping Name freature from data set\n    dataset = dataset.drop(['Name'], axis=1,inplace=True)\n","2ea8f368":"#categorising FARE feature \ndata = [data_train, data_test]\n\nfor dataset in data:\n    dataset.loc[ dataset['Fare'] <= 7.91, 'Fare'] = 0\n    dataset.loc[(dataset['Fare'] > 7.91) & (dataset['Fare'] <= 14.454), 'Fare'] = 1\n    dataset.loc[(dataset['Fare'] > 14.454) & (dataset['Fare'] <= 31), 'Fare']   = 2\n    dataset.loc[(dataset['Fare'] > 31) & (dataset['Fare'] <= 99), 'Fare']   = 3\n    dataset.loc[(dataset['Fare'] > 99) & (dataset['Fare'] <= 250), 'Fare']   = 4\n    dataset.loc[ dataset['Fare'] > 250, 'Fare'] = 5\n    dataset['Fare'] = dataset['Fare'].astype(int)\n    print(dataset.Fare.unique())\n","39e899d9":"#FINDING CATEGORICAL FEATURES from training data \n#categorical data is data which takes on a finite number of possible values.\n#whether or not a feature is categorical depends on its application\n#There is no hard and fast rule for how many values a categorical feature should have.\n\n#HELPER FUNCTION TO CHECK FOR  NUMBER OF UNIQUE VALUES IN DATAFRAME FEATURES\nunique_counts = pd.DataFrame.from_records([(col, data_train[col].nunique()) for col in data_train.columns],\n                          columns=['Column_Name', 'Num_Unique']).sort_values(by=['Num_Unique'])\n\nunique_counts\n#There is a big jump in unique values once we get above 7 unique values.\n#This should be a useful threshold for this data set.","ed4c60c6":"#CONVERTING FEATURES TO 'CATEGORICAL' TYPE OTHER THAN 'DATE' FIELDS\n#DATE fields should not be converted to categorical.\n#date_fretures = []\n#for col in data_train.columns:\n#    if data_train[col].nunique() <= 7 and col not in date_fretures:\n#        data_train[col] = data_train[col].astype('category')\n        \n#data_train.info()\n#NOTE:we have taken down memory usage more than 50% as categorical data types are memory efficient \n#which speed up the code by 10x or more","51216c55":"#New feature is created from Parch and SibSp\ndata = [data_train, data_test]\n\nfor dataset in data:\n    dataset['FamilySize'] = dataset['SibSp'] + dataset['Parch']\n    dataset['Alone'] = 0\n    dataset.loc[dataset.FamilySize==0,'Alone'] = 1\n    dataset.drop(['SibSp','Parch'],axis=1,inplace=True)","9e3f2fa7":"# #New feature is created from Parch and SibSp OR we can go with categorizing features \n# categoircal_features  = ['Pclass','Sex','Embarked']\n\n# data = [data_train, data_test]\n\n# for dataset in data:\n#     for feature in categoircal_features:\n#         dataset = pd.concat([dataset.drop([feature],axis=1), pd.get_dummies(dataset[[feature]], drop_first=True)], axis=1)\n    ","b6dbb9b4":"#Convert \u2018Embarked\u2019 feature into numeric.\nports = {\"S\": 0, \"C\": 1, \"Q\": 2}\ndata = [data_train, data_test]\n\nfor dataset in data:\n    dataset['Embarked'] = dataset['Embarked'].map(ports)","bfe57d46":"#Convert \u2018Sex\u2019 feature into numeric.\ngenders = {\"male\": 0, \"female\": 1}\ndata = [data_train, data_test]\n\nfor dataset in data:\n    dataset['Sex'] = dataset['Sex'].map(genders)","1c22ba56":"# #create new feature for age and Pclass\n# data = [data_train, data_test]\n# for dataset in data:\n#     dataset['Age_Class']= dataset['Age']* dataset['Pclass']","b0a7ad84":"#data_train.head()\ndata_train.info()\nprint('-'*50)\n#data_test.head()\ndata_test.info()","bb5ec9a7":"#importing all the required ML packages\nfrom xgboost import XGBClassifier\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\nfrom sklearn.metrics import accuracy_score, log_loss\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\nfrom sklearn import metrics #accuracy measure\nfrom sklearn.metrics import confusion_matrix #for confusion matrix","13d6e44b":"X,y = data_train.drop(['Survived'],axis=1), data_train['Survived'] #note: df.drop(inplace=Fasle) as it is reqd later.\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n\nprint(X.shape)\nprint(X_train.shape)\nprint(X_test.shape)\nprint(y.shape)\nprint(y_train.shape)\nprint(y_test.shape)","d867f3cc":"# import lightgbm as lgb\n# d_train = lgb.Dataset(X_train, label=y_train)\n# params = {}\n# params['learning_rate'] = 0.003\n# params['boosting_type'] = 'gbdt'\n# params['objective'] = 'binary'\n# params['metric'] = 'binary_logloss'\n# params['sub_feature'] = 0.5\n# params['num_leaves'] = 10\n# params['min_data'] = 50\n# params['max_depth'] = 10\n# clf = lgb.train(params, d_train, 1000)","2f80ffd0":"# #Prediction\n# train_predictions=clf.predict(X_test)\n# #convert into binary values\n# for i in range(0,train_predictions.shape[0]):\n#     if train_predictions[i]>=.5:       # setting threshold to .5\n#        train_predictions[i]=1\n#     else:  \n#        train_predictions[i]=0\n","5b2881ee":"# acc = accuracy_score(y_test, train_predictions)\n# acc","c1439a23":"# survivors = clf.predict(data_test)\n# for i in range(0,survivors.shape[0]):\n#     if survivors[i]>=.5:       # setting threshold to .5\n#        survivors[i]=1\n#     else:  \n#        survivors[i]=0\n# survivors=survivors.astype(int)\n# submission = pd.DataFrame({'PassengerId':data_test.index,'Survived':survivors})\n# submission.head()","32615250":"# classifiers = [\n#     KNeighborsClassifier(3), SVC(probability=True), DecisionTreeClassifier(), RandomForestClassifier(), AdaBoostClassifier(),\n#     GradientBoostingClassifier(), GaussianNB(), LinearDiscriminantAnalysis(), QuadraticDiscriminantAnalysis(), LogisticRegression()\n# ]\n\n# log_cols = [\"Classifier\", \"Accuracy\"]\n# log = pd.DataFrame(columns=log_cols)\n# acc_dict = {}\n\n# for clf in classifiers:\n#     name = clf.__class__.__name__\n#     clf.fit(X_train, y_train)\n#     train_predictions = clf.predict(X_test)\n#     acc = accuracy_score(y_test, train_predictions)\n#     acc_dict[name] = acc\n\n# for clf in acc_dict:\n#     log_entry = pd.DataFrame([[clf, acc_dict[clf]]], columns=log_cols)\n#     log = log.append(log_entry)\n\n\n# plt.figure(figsize=(10,10))\n# plt.xlabel('Accuracy')\n# plt.title('Classifier Accuracy')\n# plt.grid(True)\n# sns.barplot(x='Accuracy', y='Classifier', data=log, color=\"b\");","e5e40f60":"# xgb = XGBClassifier(max_depth = 5,\n#                     n_estimators=1000,\n#                     learning_rate=0.2,\n#                     nthread=3,\n#                     subsample=1.0,\n#                     colsample_bytree=0.5,\n#                     min_child_weight = 3,\n#                     reg_alpha=0.03)\n# xgb.fit(X_train, y_train, early_stopping_rounds=50, eval_metric='auc', eval_set=[(X_train, y_train), (X_test, y_test)])","d05ecf3c":"# survivors = xgb.predict(data_test)\n# submission = pd.DataFrame({'PassengerId':data_test.index,'Survived':survivors})\n# submission.head()","5b270dfb":"from keras.utils import to_categorical\n#one-hot encode target column\ny_train = to_categorical(y_train)\n#vcheck that target column has been converted\ny_train[0:5]","69c5ed26":"from keras.models import Sequential\nfrom keras.layers import Dense\n#create model\nmodel = Sequential()\n#get number of columns in training data\nn_cols = X_train.shape[1]\n#add layers to model\nmodel.add(Dense(250, activation='relu', input_shape=(n_cols,)))\nmodel.add(Dense(250, activation='relu'))\nmodel.add(Dense(250, activation='relu'))\nmodel.add(Dense(2, activation='softmax'))","42d10283":"#compile model using accuracy to measure model performance\nmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])","6005c5a4":"from keras.callbacks import EarlyStopping\n#set early stopping monitor so the model stops training when it won't improve anymore\nearly_stopping_monitor = EarlyStopping(patience=3)\n#train model\nmodel.fit(X_train, y_train, validation_split=0.2, epochs=30, callbacks=[early_stopping_monitor])","ade2e308":"#Prediction\ntrain_predictions=model.predict(X_test)[:,1]\ntrain_predictions\n#convert into binary values\nfor i in range(0,train_predictions.shape[0]):\n    if train_predictions[i]>=.5:       # setting threshold to .7\n       train_predictions[i]=1\n    else:  \n       train_predictions[i]=0\n    \nacc = accuracy_score(y_test, train_predictions)\nacc","929972a0":"survivors = model.predict(data_test)[:,1]\nfor i in range(0,survivors.shape[0]):\n    if survivors[i]>=.5:       # setting threshold to .9\n       survivors[i]=1\n    else:  \n       survivors[i]=0\nsurvivors=survivors.astype(int)\nsubmission = pd.DataFrame({'PassengerId':data_test.index,'Survived':survivors})\nsubmission.head()","0ddb89bf":"submission.to_csv('Titanic_Survivors_PredictionsNN.csv',index=False)","809764ae":"CHECK TRAIN AND TEST DATA BEFORE IMPLEMENTING ML ALGO","6acd362d":"# INITIAL DATA IMPUTATION\nTHAT IS DOING WHAT WE CAN SAFELY AFTER INITIAL\/LIMITED UNDERSTANDING OF DATA","c6cb7651":"# Machine Learning ","ac264ef7":"**Other Classifiers**","98d08181":"BY CATEGORIZING FEATURE, NORMALISATION AND OUTLIER TREATMENT IS DONE.","5b87b0c1":"**LightGBM**","1b8c5137":"**NEURAL NETWORKS**"}}