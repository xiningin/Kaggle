{"cell_type":{"255c047e":"code","db58840a":"code","174f4434":"code","1dc26693":"code","27aeac42":"code","4ee9f5b7":"code","257c7a70":"code","59e5a6d3":"code","af8692da":"code","f186b1ae":"code","59814817":"code","8b5b08a2":"code","b5474a68":"code","ad5d4ccb":"code","8b5632c2":"code","50d6c4b4":"code","b5d53328":"code","e395991d":"code","1484cba0":"code","1c0cc99d":"code","d785532d":"code","b558ccd5":"code","afb1d408":"code","c818368c":"code","5845f191":"code","f1df5a2a":"code","547b592b":"code","eeaabff4":"code","314bf444":"code","3bc278f7":"code","b9233f07":"code","47d7be22":"code","fb07dac3":"markdown","4b3290b2":"markdown","d308af01":"markdown","08e62a4a":"markdown","d0c7d984":"markdown","72c32a79":"markdown","b8daeb28":"markdown","1590e239":"markdown","f14adf45":"markdown","fb20445f":"markdown","fdc97ac0":"markdown","fa8cc99e":"markdown","d6c9cd7b":"markdown","cfbcf5e7":"markdown"},"source":{"255c047e":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom wordcloud import WordCloud\nimport re\n\nimport nltk\nfrom nltk.corpus import stopwords\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import confusion_matrix, f1_score, classification_report, accuracy_score","db58840a":"true = pd.read_csv('..\/input\/fake-and-real-news-dataset\/True.csv')\nfake = pd.read_csv('..\/input\/fake-and-real-news-dataset\/Fake.csv')","174f4434":"true.head(3)","1dc26693":"fake.head(3)","27aeac42":"true['label'] = 0\nfake['label'] = 1\n\n# Concatening the datasets\ndf = pd.concat([true, fake], ignore_index=True)","4ee9f5b7":"df","257c7a70":"df.info()","59e5a6d3":"# Checking if any duplicate records are present\n\nduplicate=df[df.duplicated()] \nduplicate","af8692da":"# Removing duplicate records\n\ndf.drop_duplicates(inplace=True)","f186b1ae":"# Again check if any duplicate records are left\n\nduplicate = df[df.duplicated()] \nduplicate","59814817":"df.describe(include='object')","8b5b08a2":"# Checking for null values\n\ndf.isnull().sum()","b5474a68":"# Visualizing the disribution of true and fake news\n\nsns.countplot(df['label'])","ad5d4ccb":"# Expanding contractions\n\n# Dictionary of English Contractions\ncontractions_dict = { \"ain't\": \"are not\",\"'s\":\" is\",\"aren't\": \"are not\",\n                     \"can't\": \"cannot\",\"can't've\": \"cannot have\",\n                     \"'cause\": \"because\",\"could've\": \"could have\",\"couldn't\": \"could not\",\n                     \"couldn't've\": \"could not have\", \"didn't\": \"did not\",\"doesn't\": \"does not\",\n                     \"don't\": \"do not\",\"hadn't\": \"had not\",\"hadn't've\": \"had not have\",\n                     \"hasn't\": \"has not\",\"haven't\": \"have not\",\"he'd\": \"he would\",\n                     \"he'd've\": \"he would have\",\"he'll\": \"he will\", \"he'll've\": \"he will have\",\n                     \"how'd\": \"how did\",\"how'd'y\": \"how do you\",\"how'll\": \"how will\",\n                     \"I'd\": \"I would\", \"I'd've\": \"I would have\",\"I'll\": \"I will\",\n                     \"I'll've\": \"I will have\",\"I'm\": \"I am\",\"I've\": \"I have\", \"isn't\": \"is not\",\n                     \"it'd\": \"it would\",\"it'd've\": \"it would have\",\"it'll\": \"it will\",\n                     \"it'll've\": \"it will have\", \"let's\": \"let us\",\"ma'am\": \"madam\",\n                     \"mayn't\": \"may not\",\"might've\": \"might have\",\"mightn't\": \"might not\", \n                     \"mightn't've\": \"might not have\",\"must've\": \"must have\",\"mustn't\": \"must not\",\n                     \"mustn't've\": \"must not have\", \"needn't\": \"need not\",\n                     \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\",\"oughtn't\": \"ought not\",\n                     \"oughtn't've\": \"ought not have\",\"shan't\": \"shall not\",\"sha'n't\": \"shall not\",\n                     \"shan't've\": \"shall not have\",\"she'd\": \"she would\",\"she'd've\": \"she would have\",\n                     \"she'll\": \"she will\", \"she'll've\": \"she will have\",\"should've\": \"should have\",\n                     \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\",\"so've\": \"so have\",\n                     \"that'd\": \"that would\",\"that'd've\": \"that would have\", \"there'd\": \"there would\",\n                     \"there'd've\": \"there would have\", \"they'd\": \"they would\",\n                     \"they'd've\": \"they would have\",\"they'll\": \"they will\",\n                     \"they'll've\": \"they will have\", \"they're\": \"they are\",\"they've\": \"they have\",\n                     \"to've\": \"to have\",\"wasn't\": \"was not\",\"we'd\": \"we would\",\n                     \"we'd've\": \"we would have\",\"we'll\": \"we will\",\"we'll've\": \"we will have\",\n                     \"we're\": \"we are\",\"we've\": \"we have\", \"weren't\": \"were not\",\"what'll\": \"what will\",\n                     \"what'll've\": \"what will have\",\"what're\": \"what are\", \"what've\": \"what have\",\n                     \"when've\": \"when have\",\"where'd\": \"where did\", \"where've\": \"where have\",\n                     \"who'll\": \"who will\",\"who'll've\": \"who will have\",\"who've\": \"who have\",\n                     \"why've\": \"why have\",\"will've\": \"will have\",\"won't\": \"will not\",\n                     \"won't've\": \"will not have\", \"would've\": \"would have\",\"wouldn't\": \"would not\",\n                     \"wouldn't've\": \"would not have\",\"y'all\": \"you all\", \"y'all'd\": \"you all would\",\n                     \"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\n                     \"y'all've\": \"you all have\", \"you'd\": \"you would\",\"you'd've\": \"you would have\",\n                     \"you'll\": \"you will\",\"you'll've\": \"you will have\", \"you're\": \"you are\",\n                     \"you've\": \"you have\"}\n\n# Regular expression for finding contractions\ncontractions_re=re.compile('(%s)' % '|'.join(contractions_dict.keys()))\n\n# Function for expanding contractions\ndef expand_contractions(text,contractions_dict=contractions_dict):\n    def replace(match):\n        return contractions_dict[match.group(0)]\n    return contractions_re.sub(replace, text)\n\n# Expanding Contractions in the title, text\ndf['title'] = df['title'].apply(lambda x:expand_contractions(x))\ndf['text'] = df['text'].apply(lambda x:expand_contractions(x))","8b5632c2":"# Converting text to lowercase\n\ndf['title'] = df['title'].apply(lambda x:x.lower())\ndf['text'] = df['text'].apply(lambda x:x.lower())","50d6c4b4":"# Removing digits and words containing digits\n\ndf['title'] = df['title'].apply(lambda x: re.sub('\\w*\\d\\w*','', x))\ndf['text'] = df['text'].apply(lambda x: re.sub('\\w*\\d\\w*','', x))","b5d53328":"# Removing punctuations\n\ndf['title'] = df['title'].apply(lambda x: re.sub(r'[^\\w\\s]', '', x))\ndf['text'] = df['text'].apply(lambda x: re.sub(r'[^\\w\\s]', '', x))","e395991d":"# Removing extra spaces\n\ndf['title'] = df['title'].apply(lambda x: re.sub(' +',' ',x))\ndf['text'] = df['text'].apply(lambda x: re.sub(' +',' ',x))","1484cba0":"# Applying lemmatization\n\nw_tokenizer = nltk.tokenize.WhitespaceTokenizer()\nlemmatizer = nltk.stem.WordNetLemmatizer()\n\ndef lemmatize_text(text):\n    rev = [lemmatizer.lemmatize(w) for w in w_tokenizer.tokenize(text) if w not in stopwords.words('english')]\n    rev = ' '.join(rev)\n    return rev\n\ndf['title'] = df.title.apply(lemmatize_text)\ndf['text'] = df.text.apply(lemmatize_text)","1c0cc99d":"# Displaying title, text after cleaning\n\nprint(\"Title\\n\")\nfor index,text in enumerate(df['title'][0:3]):\n    print('Title %d:\\n'%(index+1), text)\n    \nprint(\"\\nText\\n\")\nfor index,txt in enumerate(df['text'][0:3]):\n    print('Text %d:\\n'%(index+1), txt)","d785532d":"# Wordcloud of title, text in True news\n\n# Cleaned dataframe of True labels\ndf_true = df[df.label == 0]\n\ntitle_true = \" \".join(tit for tit in df_true['title'])\ntext_true = \" \".join(txt for txt in df_true['text'])\n\nplt.figure(figsize=(40, 30))\n\n# Title\ntitle_cloud = WordCloud(collocations=False, background_color='black').generate(title_true)\nplt.subplot(1, 2, 1)\nplt.axis(\"off\")\nplt.title(\"Title\", fontsize=40)\nplt.imshow(title_cloud, interpolation='bilinear')\n\n# Title\ntext_cloud = WordCloud(collocations=False, background_color='black').generate(text_true)\nplt.subplot(1, 2, 2)\nplt.axis(\"off\")\nplt.title(\"Text\", fontsize=40)\nplt.imshow(text_cloud, interpolation='bilinear')","b558ccd5":"# Wordcloud of title, text in Fake news\n\n# Cleaned dataframe of Fake labels\ndf_fake = df[df.label == 1]\n\ntitle_fake = \" \".join(tit for tit in df_fake['title'])\ntext_fake = \" \".join(txt for txt in df_fake['text'])\n\nplt.figure(figsize=(40, 30))\n\n# Title\ntitle_cloud = WordCloud(collocations=False, background_color='black').generate(title_fake)\nplt.subplot(1, 2, 1)\nplt.axis(\"off\")\nplt.title(\"Title\", fontsize=40)\nplt.imshow(title_cloud, interpolation='bilinear')\n\n# Title\ntext_cloud = WordCloud(collocations=False, background_color='black').generate(text_fake)\nplt.subplot(1, 2, 2)\nplt.axis(\"off\")\nplt.title(\"Text\", fontsize=40)\nplt.imshow(text_cloud, interpolation='bilinear')","afb1d408":"# Subject-wise distriution of news \n\nsns.countplot(df['subject'])\nplt.xticks(rotation=90)","c818368c":"tf = TfidfVectorizer(max_features=3000, ngram_range=(1,4))\n\nX = tf.fit_transform(df['text']).toarray()\nX","5845f191":"y = df['label']","f1df5a2a":"# Splitting the dataset into train and test \n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nprint(\"X_train:\", X_train.shape)\nprint(\"X_test:\", X_test.shape)\nprint(\"y_train:\", y_train.shape)\nprint(\"y_test:\", y_test.shape)","547b592b":"# Training the model using Naive Bayes classifier\n\nnb = MultinomialNB().fit(X_train, y_train)","eeaabff4":"print(\"Score of train data:\", nb.score(X_train, y_train))\nprint(\"Score of test data:\", nb.score(X_test, y_test))","314bf444":"y_pred = nb.predict(X_test)\ny_pred","3bc278f7":"# F1 score and accuracy\n\nf1_score = f1_score(y_test, y_pred, average='weighted')\naccuracy = accuracy_score(y_test, y_pred)\n\nprint(\"F1 Score:\", f1_score)\nprint(\"Accuracy Score:\", accuracy)","b9233f07":"# Classification Report\n\nprint(classification_report(y_test, y_pred))","47d7be22":"cm = confusion_matrix(y_test, y_pred)\n\ngroup_names = ['True Neg', 'False Pos', 'False Neg', 'True Pos']\ngroup_counts = [\"{0:0.0f}\".format(value) for value in\n                cm.flatten()]\ngroup_percentages = [\"{0:.2%}\".format(value) for value in\n                     cm.flatten()\/np.sum(cm)]\nlabels = [f\"{v1}\\n{v2}\\n{v3}\" for v1, v2, v3 in\n          zip(group_names,group_counts,group_percentages)]\nlabels = np.asarray(labels).reshape(2,2)\nsns.heatmap(cm, annot=labels, fmt='', cmap='PuRd')","fb07dac3":"<b> The model performs well on train as well as test data. <\/b>","4b3290b2":"## Loading the dataset","d308af01":"## Model","08e62a4a":"<b> The count of fake news is a bit more than true news. <\/b>","d0c7d984":"<b>Four columns are of object datatype and one column is integer.<\/b>","72c32a79":"<b> Common words in title:- <\/b>trump, video, watch, clinton, obama, tweet, president, woman, muslim, democrat, etc.\n\n\n<b> Common words in text:- <\/b>trump, people, said, president, new, obama, state, clinton, time, one, etc.","b8daeb28":"## Text Preprocessing","1590e239":"<b> The dataset has 44,898 records and 5 columns. <\/b>","f14adf45":"<b> There are 209 duplicate records in the dataset. <\/b>","fb20445f":"## Importing libraries","fdc97ac0":"<b>Hence, all duplicate records are removed.<\/b>","fa8cc99e":"<b> Observations:- <\/b>\n\n<ul>\n    <li>Most of the news in the dataset is politicsNews.<\/li>\n    <li>It is followed by worldNews, News and politics.<\/li>\n    <li>Government News, US_News and Middle-east have less than 2000 records.<\/li>\n<\/ul>","d6c9cd7b":"<b> Common words in title:- <\/b>trump, korea, republican, house, russia, say, new, leader, white, senate, etc.\n\n\n<b> Common words in text:- <\/b>trump, state, republican, president, said, reuters, party, official, country, people, etc.","cfbcf5e7":"<b>The dataset doesn't have any missing values.<\/b>"}}