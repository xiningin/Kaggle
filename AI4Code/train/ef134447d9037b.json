{"cell_type":{"be25e7de":"code","0d2d7434":"code","1715550b":"markdown"},"source":{"be25e7de":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","0d2d7434":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport gc\nimport time\nfrom contextlib import contextmanager\nfrom lightgbm import LGBMClassifier\nfrom sklearn.metrics import roc_auc_score, roc_curve\nfrom sklearn.model_selection import KFold, StratifiedKFold\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n\n@contextmanager\ndef timer(title):\n    t0 = time.time()\n    yield\n    print(\"{} - done in {:.0f}s\".format(title, time.time() - t0))\n\n# Preprocess application_train.csv and application_test.csv\ndef application_train_test(num_rows = None, nan_as_category = False):\n    # Read data and merge\n    df = pd.read_csv('\/kaggle\/input\/final-selected-features-for-hc-default-risk\/train__merged_selected_features.csv', nrows= num_rows)\n    test_df = pd.read_csv('\/kaggle\/input\/final-selected-features-for-hc-default-risk\/test__merged_selected_features.csv', nrows= num_rows)\n    print(\"Train samples: {}, test samples: {}\".format(len(df), len(test_df)))\n    df = df.append(test_df).reset_index()\n    \n    del test_df\n    gc.collect()\n    return df\n\n# LightGBM GBDT with KFold or Stratified KFold\n# Parameters from Tilii kernel: https:\/\/www.kaggle.com\/tilii7\/olivier-lightgbm-parameters-by-bayesian-opt\/code\ndef kfold_lightgbm(df, num_folds, stratified = False, debug= False):\n    # Divide in training\/validation and test data\n    train_df = df[df['TARGET'].notnull()]\n    test_df = df[df['TARGET'].isnull()]\n    print(\"Starting LightGBM. Train shape: {}, test shape: {}\".format(train_df.shape, test_df.shape))\n    del df\n    gc.collect()\n    # Cross validation model\n    if stratified:\n        folds = StratifiedKFold(n_splits= num_folds, shuffle=True, random_state=1001)\n    else:\n        folds = KFold(n_splits= num_folds, shuffle=True, random_state=1001)\n    # Create arrays and dataframes to store results\n    oof_preds = np.zeros(train_df.shape[0])\n    sub_preds = np.zeros(test_df.shape[0])\n    feature_importance_df = pd.DataFrame()\n    feats = [f for f in train_df.columns if f not in ['TARGET','SK_ID_CURR','SK_ID_BUREAU','SK_ID_PREV','index']]\n    \n    for n_fold, (train_idx, valid_idx) in enumerate(folds.split(train_df[feats], train_df['TARGET'])):\n        train_x, train_y = train_df[feats].iloc[train_idx], train_df['TARGET'].iloc[train_idx]\n        valid_x, valid_y = train_df[feats].iloc[valid_idx], train_df['TARGET'].iloc[valid_idx]\n\n        # LightGBM parameters found by Bayesian optimization\n        clf = LGBMClassifier(\n            nthread=4,\n            n_estimators=10000,\n            learning_rate=0.02,\n            num_leaves=34,\n            colsample_bytree=0.9497036,\n            subsample=0.8715623,\n            max_depth=8,\n            reg_alpha=0.041545473,\n            reg_lambda=0.0735294,\n            min_split_gain=0.0222415,\n            min_child_weight=39.3259775,\n            silent=-1,\n            verbose=-1, )\n\n        clf.fit(train_x, train_y, eval_set=[(train_x, train_y), (valid_x, valid_y)], \n            eval_metric= 'auc', verbose= 200, early_stopping_rounds= 200)\n\n        oof_preds[valid_idx] = clf.predict_proba(valid_x, num_iteration=clf.best_iteration_)[:, 1]\n        sub_preds += clf.predict_proba(test_df[feats], num_iteration=clf.best_iteration_)[:, 1] \/ folds.n_splits\n\n        fold_importance_df = pd.DataFrame()\n        fold_importance_df[\"feature\"] = feats\n        fold_importance_df[\"importance\"] = clf.feature_importances_\n        fold_importance_df[\"fold\"] = n_fold + 1\n        feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n        print('Fold %2d AUC : %.6f' % (n_fold + 1, roc_auc_score(valid_y, oof_preds[valid_idx])))\n        del clf, train_x, train_y, valid_x, valid_y\n        gc.collect()\n\n    print('Full AUC score %.6f' % roc_auc_score(train_df['TARGET'], oof_preds))\n    # Write submission file and plot feature importance\n    if not debug:\n        test_df['TARGET'] = sub_preds\n        test_df[['SK_ID_CURR', 'TARGET']].to_csv(submission_file_name, index= False)\n    display_importances(feature_importance_df)\n    return feature_importance_df\n\n# Display\/plot feature importance\ndef display_importances(feature_importance_df_):\n    cols = feature_importance_df_[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(by=\"importance\", ascending=False)[:40].index\n    best_features = feature_importance_df_.loc[feature_importance_df_.feature.isin(cols)]\n    plt.figure(figsize=(8, 10))\n    sns.barplot(x=\"importance\", y=\"feature\", data=best_features.sort_values(by=\"importance\", ascending=False))\n    plt.title('LightGBM Features (avg over folds)')\n    plt.tight_layout()\n    plt.savefig('lgbm_importances01.png')\n\ndef main(debug = False):\n    num_rows = 10000 if debug else None\n    df = application_train_test(num_rows)\n    with timer(\"Run LightGBM with kfold\"):\n        feat_importance = kfold_lightgbm(df, num_folds= 5, stratified= False, debug= debug)\n\nif __name__ == \"__main__\":\n    submission_file_name = \"submission_kernel02.csv\"\n    with timer(\"Full model run\"):\n        main()","1715550b":"# 0.Team\n- In this project I have worked with [@ahmettkara](https:\/\/www.kaggle.com\/ahmettkara) and [@farukoz](https:\/\/www.kaggle.com\/farukoz). \n\n# 1. About Home Credit\n- Home Credit (HC) is an international consumer finance company which provides consumers who have insufficient or non-existent credit histories. \n- HC uses a variety of alternative data--including telco and transactional information--to predict their clients' repayment abilities through various statistical and machine learning methods to make these predictions.\n- HC was founded in 1997 in Czechia. HC currently operates in 9 countries (China, India, Russia, Philippines, Czechia, Kazakhstan, Indonesia, Vietnam, Slovakia)\n- HC focuses on lending primarily to people with little or no credit history, particularly underserved borrowers in the blue collar and junior white collar segments who earn regular income from their employment or micro-businesses, but are less likely to access financing from banks and other traditional lenders.\n\n## Business Model\n\n![Imgur](https:\/\/i.imgur.com\/HZwTqv6.jpg)\n\n- Customers typically start with its point-of-sale financing in stores.\n\n### Risk assessment\n- The company accumulates a large volume of borrower behaviour data in order to use to refine risks and cross-selling.\n- HC's assessment includes multiple factors such as:\n    - the risk profile of the individual customer, \n    - the type of product being purchased\n    - characteristics of the loan (such as size, maturity and other factors)\n\n### Products\n- HC offers 3 main products\n- __POS Loan__: POS loans are offered to its customers for their purchases of goods (such as mobile phones, home appliances and personal computers) and services at the point of sale (physical and online) and at the time of purchase.\n    - The credit decision for each customer is determined by reference to a number of factors that include: \n        - the credit record (if any) of the loan applicant,\n        - the product being purchased, \n        - the sales history of loans originated from the relevant point of sale, \n        - the behavioural pattern of the loan applicant at the application stage\n        - the amount of down payment,\n        - data acquired from third parties about the customer. \n    - Depending on the credit analysis, HC;\n        - might accept a loan, \n        - reject a loan or \n        - make the customer a counter-offer as to down payment and\/or term which better fits its assessment of the customer\u2019s risk profile and ability to repay.\n- __Cash Loan__: Cash loans are typically offered to its customers for consumer goods or services without a point of sale connection.\n     - HC cross-sells cash loans to its existing customers who are assessed to be creditworthy as determined by a number of factors, including their credit record with the company. \n    - HC also provides cash loans to new customers, provided that these customers satisfy its underwriting criteria. \n    - The direct acquisition of new cash loan customers is primarily through online sales channels. \n    - Cash loans generally have larger ticket sizes, longer tenor and higher profitability than POS loans.\n- __Revolving Loan__: Revolving loans, including credit cards, are typically offered to its existing customers for their purchases of goods or services up to individual credit limits on a revolving basis.\n* __Note__: The information above is taken from https:\/\/www.homecredit.net\/.\n\n# 2. Purpose of the Project\n\n- The purpose of this project is to recommend a model so that clients capable of repayment are not rejected.\n- The general outlook of the data are as follows:\n\n![Imgur](https:\/\/i.imgur.com\/d3fo2LK.png)\n\n## General information of the data frames\n- __Application_train__\n    - RangeIndex: 307,511 entries, 0 to 307,510\n    - Data columns (total 122 columns)\n    - dtypes: float64(65), int64(41), object(16)\n- __Application_test__\n    - RangeIndex: 48,744 entries, 0 to 48,743\n    - Data columns (total 121 columns)\n    - dtypes: float64(65), int64(40), object(16)\n- __Bureau__\n    - RangeIndex: 1,716,428 entries, 0 to 1,716,427\n    - Data columns (total 17 columns)\n    - dtypes: float64(8), int64(6), object(3)\n- __Bureau_balance__\n    - RangeIndex: 27,299,925 entries, 0 to 27,299,924\n    - Data columns (total 3 columns)\n    - dtypes: int64(2), object(1)\n- __Previous_application__\n    - RangeIndex: 1,670,214 entries, 0 to 1,670,213\n    - Data columns (total 37 columns)\n    - dtypes: float64(15), int64(6), object(16)\n- __POS_CASH_balance__\n    - RangeIndex: 10,001,358 entries, 0 to 10,001,357\n    - Data columns (total 8 columns)\n    - float64(2), int64(5), object(1)\n- __Credit_card_balance__\n    - RangeIndex: 3,840,312 entries, 0 to 3,840,311\n    - Data columns (total 23 columns)\n    - dtypes: float64(15), int64(7), object(1)\n- __Installments_balance__\n    - RangeIndex: 13,605,401 entries, 0 to 13,605,400\n    - Data columns (total 8 columns)\n    - dtypes: float64(5), int64(3)\n\n# 3. Methodology\n\n### Understanding the company and project \n- The basic information regarding the company is taken from the HC website. The project information is taken from the HC Default Risk data presented on concerning kaggle page.\n\n### Understanding the variables\n- The column descriptions presented in the data were examined and discussed. The discussion threads on the HC Default Risk were also read through and analyzed. \n\n### EDA and data manipulation\n- Basic pandas and visualization tools were used to understand the distribution of variables, general statistical information, NaN values, correlations, outliers and linear relations are examined. \n- The EDA of the data for bureau, bureau_balance, previous_application, POS_Cash_balance, credit_card_balance and installments_payments are done in the following [kernel](https:\/\/www.kaggle.com\/brooksrattigan\/home-credit-default-risk-eda).\n\n### Feature engineering and aggregation\n- The kernels of [@willkoehrsen](https:\/\/www.kaggle.com\/willkoehrsen\/introduction-to-manual-feature-engineering) were really helpful and thorough to tackle most of the stages including this stage. Also, kernels of [@jsaguiar](https:\/\/www.kaggle.com\/jsaguiar\/lightgbm-with-simple-features) and [@nlgn](https:\/\/www.kaggle.com\/c\/home-credit-default-risk\/discussion\/64598) have contributed to our newly created features. \n- It was not possible to handle all the stages on kaggle since we faced memory limitation. So, we first handled the tables separately and merged them after aggregation and feature engineering. \n- The notebooks for aggregation and feature engineering are given in the following [github link](https:\/\/github.com\/ahmetinan79\/Home_Credit_Defualt_Risk_Notebooks).\n\n### Feature selection\n- After aggregation we had 1391 features and we used Light GBM to analyze feature importance. We dropped the 531 features having 0 importance. Again, [@willkoehrsen](https:\/\/www.kaggle.com\/willkoehrsen\/introduction-to-feature-selection)'s kernel regarding feature selection was an important guide for our work. We did not prefer dropping further features not to hurt the model and we left the work to the algorithm of Light GBM.\n- The notebooks for feature importance and feature elimination are given in the following [github link](https:\/\/github.com\/ahmetinan79\/Home_Credit_Defualt_Risk_Notebooks).\n\n### Model selection and model tuning\n- We had 860 features for our final model and again we used Light GBM. We examined the Bayesian Optimization approach for model tuning, but we realized that it was really time consuming and we have decided to use the hyperparameters laid out in the kernel of [tilii7](https:\/\/www.kaggle.com\/tilii7\/olivier-lightgbm-parameters-by-bayesian-opt\/code). The same hyperparameters were also used at the feature selection stage. \n- The memory problem on kaggle was challenging, so we worked on data with our personal computers and uploaded our final merged data to kaggle for the final analysis. We benefited from the functions and codes of [@jsaguiar](https:\/\/www.kaggle.com\/jsaguiar\/lightgbm-with-simple-features) to deal with memory issues and to come up with desired outputs of AUC score and feature importance. \n\n# 4. Results\n\n### AUC scores and feature importance\n- We used 5-Fold CV and our AUC score was __0.794620__. Also, our AUC score with 10-Fold CV was __0.795465__. It is not among the top scores but satisfactory for us, since we have considered this project as part of our learning process in data science. We were pleased to see that newly created features were among the top features. This shows the importance feature engineering and feature selection in such Kaggle competition projects.\n\n### LB Scores\n- Our LB scores were __0.79447__ and __0.79998__ for private and public scores respectively.\n- In this framework, we can say that the result of 5-fold CV AUC score is more reliable compared to the AUC score of 10-fold CV. \n- This arises from the imbalanced distribution of target variable.\n\n### Notes\n- We dropped the columns which have higher than 80-85% NaN values and alternatively have tried to impute NaN values with expectation\u2013maximization (EM) algorithm in [ycimpute package](https:\/\/pypi.org\/project\/ycimpute\/) for the rest of the features in bureau, bureau_balance, previous_application, POS_Cash_balance, credit_card_balance and installments_payments data before aggregating them by groupby method. We came up with AUC score of 0.79343 and it was lower than our score without imputation. We could have tried other algorithms of KNN and Iterforest in that package or imputation methods in sklearn but we faced with memory problems in kaggle and personal computers. Thus, imputation may hurt the model and lead to lower AUC scores and it would be wise to use and compare different approaches. \n- We did not drop any features due high correlation and left this to Light GBM algorithm. \n- We also did not drop any outlier observation except one observation due to the income value of 117 Million.\n- We think that issues of NAN values, outliers and correlation are needed to be dealt with precaution and based on research and according to the algorithms we use."}}