{"cell_type":{"498cf4a9":"code","7c3c3714":"code","6d646c7e":"code","299cf512":"code","7a5e65a3":"code","c2dce084":"code","a906f26c":"code","850bf731":"code","f29f3e3b":"code","11665f86":"code","6ac5156c":"code","a0d5b690":"code","bf06beb3":"code","5364a1be":"code","a9e6df62":"code","806681f0":"code","338d7586":"code","c1f0c7fc":"code","2eec7cae":"code","819217fe":"code","71caae9b":"code","1bab175a":"code","64188827":"code","794a4e65":"code","4685ad69":"code","9b794208":"code","d53680d5":"code","078ef73f":"code","59ea14cd":"code","8c87f940":"code","11383a15":"code","86bdca5d":"code","f2595896":"code","7c5f3349":"code","decf19a7":"code","7bff2fcf":"code","f8617fb6":"code","d3d872bb":"code","d6d851b3":"markdown","a2463457":"markdown","f09fc3c0":"markdown","eb49cb9d":"markdown"},"source":{"498cf4a9":"import numpy as np\nimport pandas as pd\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\nfrom sklearn.model_selection import train_test_split\n\nimport glob\n\nimport matplotlib.pyplot as plt\nfrom PIL import Image\n\nimport random\n\nimport cv2\n\nimport torch \nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nimport torch.nn.functional as F\nfrom torch.optim.lr_scheduler import StepLR, MultiplicativeLR\nfrom torchvision import transforms\nimport torch.optim as optim\n\nfrom tqdm import tqdm\n\nfrom time import sleep\n\nfrom collections import OrderedDict, defaultdict \n\nimport warnings\nwarnings.filterwarnings('ignore')","7c3c3714":"GPU = True\ndevice = \"cuda\" if GPU and torch.cuda.is_available() else \"cpu\"\n\nprint(f'Using device {device}')","6d646c7e":"os.listdir('\/kaggle\/input\/lgg-mri-segmentation\/kaggle_3m\/TCGA_DU_6404_19850629\/')[:5]","299cf512":"dirr = \"\/kaggle\/input\/lgg-mri-segmentation\/kaggle_3m\/\"\n\nimage_size = 256\n\ndata = []\n\nfor sub_dir_path in glob.glob(dirr + \"*\"):\n    if os.path.isdir(sub_dir_path):\n        dirname = sub_dir_path.split(\"\/\")[-1]\n        for filename in os.listdir(sub_dir_path):\n            image_path = sub_dir_path + \"\/\" + filename\n            data.extend([dirname, image_path])\n        \n        \ndf = pd.DataFrame(\n    {\n        \"dirname\" : data[::2],\n        \"path\" : data[1::2]\n    }\n)\n\ndf.head()","7a5e65a3":"df.loc[0, 'path']","c2dce084":"df.loc[0, 'path'].split('.')[0].split('_')[-1]","a906f26c":"df.loc[1, 'path']","850bf731":"df.loc[1, 'path'].split('.')[0].split('_')[-2]","f29f3e3b":"df_no_masks = df[~df['path'].str.contains(\"mask\")]\ndf_masks = df[df['path'].str.contains(\"mask\")]\n\nno_masks = sorted(df_no_masks[\"path\"].values, key=lambda string: int(string.split('.')[0].split('_')[-1]))\nmasks = sorted(df_masks[\"path\"].values, key=lambda string: int(string.split('.')[0].split('_')[-2]))","11665f86":"df_no_masks.head()","6ac5156c":"df = pd.DataFrame(\n    {\n        'patient': df_no_masks['dirname'].values,\n        'image_path': no_masks,\n        'mask_path': masks\n    }\n)","a0d5b690":"df.head()","bf06beb3":"df.loc[0, 'image_path']","5364a1be":"df.loc[0, 'mask_path']","a9e6df62":"# \u0435\u0441\u043b\u0438 \u0445\u043e\u0442\u044c \u043e\u0434\u0438\u043d \u043f\u0438\u043a\u0441\u0435\u043b\u044c \u0441\u043e\u0434\u0435\u0440\u0436\u0438\u0442 \u0447\u0442\u043e-\u0442\u043e \u0431\u043e\u043b\u044c\u0448\u0435\u0435 \u0447\u0435\u043c 0\ndiagnos = lambda path: 1 if np.max(cv2.imread(path)) > 0 else 0\n\ndf[\"diagnosis\"] = df[\"mask_path\"].apply(diagnos)","806681f0":"df.head()","338d7586":"df['diagnosis'].value_counts() \/ df.shape[0]","c1f0c7fc":"X_train, X_test = train_test_split(df, stratify=df['diagnosis'], test_size=0.15, random_state=42)\nX_train, X_valid = train_test_split(X_train, stratify=X_train['diagnosis'], test_size=0.15, random_state=42)\n\nX_train = X_train.reset_index(drop=True)\nX_test = X_test.reset_index(drop=True)\nX_valid = X_valid.reset_index(drop=True)\n\nprint(X_train['diagnosis'].value_counts() \/ X_train.shape[0])\nprint(X_valid['diagnosis'].value_counts() \/ X_valid.shape[0])\nprint(X_test['diagnosis'].value_counts() \/ X_test.shape[0])","2eec7cae":"class MRISegmantation(Dataset):\n    def __init__(self, dataframe, transforms):\n        \n        self.dataframe = dataframe\n        self.transforms = transforms\n        \n    def __len__(self):\n        return self.dataframe.shape[0]\n    \n    def __getitem__(self, idx):\n#         image = cv2.imread(self.dataframe.iloc[idx, 1])\n#         mask = cv2.imread(self.dataframe.iloc[idx, 2], 0)\n        \n        image = Image.open(self.dataframe.iloc[idx, 1]).convert('RGB')\n        mask = Image.open(self.dataframe.iloc[idx, 2]).convert('L')\n    \n        seed = np.random.randint(2147483647)\n        random.seed(seed)\n        torch.manual_seed(seed)\n\n        image = self.transforms(image)\n        \n        random.seed(seed)\n        torch.manual_seed(seed)\n        \n        mask = self.transforms(mask)\n\n        return image, mask","819217fe":"# \u043d\u0435 \u0441\u043e\u0432\u0441\u0435\u043c \u043f\u0440\u0430\u0432\u0438\u043b\u044c\u043d\u043e\nmean, std = 0.5, 0.5\nshape = 256\n\ntransform = transforms.Compose([\n    transforms.Resize((shape, shape)),\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomVerticalFlip(),\n    transforms.RandomRotation(90),\n    transforms.RandomAffine(degrees=15, translate=(0.1, 0.1), scale=(0.8, 0.8)),\n#     transforms.ToPILImage(),\n    transforms.ToTensor(),\n#     transforms.Normalize(mean=[mean], std=[std]),\n])","71caae9b":"batch_size = 16\n\ntrain_dataset = MRISegmantation(dataframe=X_train, transforms=transform)\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n\nvalid_dataset = MRISegmantation(dataframe=X_valid, transforms=transform)\nvalid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n\ntest_dataset = MRISegmantation(dataframe=X_test, transforms=transforms.Compose([\n    transforms.Resize((shape, shape)),\n    transforms.ToTensor(),\n]))\ntest_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=0)","1bab175a":"def plot_example(loader, ax, shape):\n    images, target = next(iter(loader))\n    ind = np.random.choice(range(loader.batch_size))\n\n#     try:\n#         ind = random.choice([i for i in range(images.shape[0]) if np.max(target[i].permute(1, 2, 0).cpu().detach().numpy()) > 0])\n#     except IndexError:\n#         images, target = next(iter(loader))\n#         ind = random.choice([i for i in range(images.shape[0]) if np.max(target[i].permute(1, 2, 0).cpu().detach().numpy()) > 0])\n    \n    image = images[ind].permute(1, 2, 0)\n    target = target[ind].permute(1, 2, 0).reshape(shape, shape)\n    target = Image.fromarray(np.asarray(target * 255, dtype=np.uint8)).convert('RGB')\n    target = np.asarray(target)\n\n    ax.imshow(np.hstack([image, target]))\n    ax.set_title('\u0418\u0441\u0445\u043e\u0434\u043d\u043e\u0435 \/ \u0422\u0430\u0440\u0433\u0435\u0442', fontsize=14)","64188827":"fig, axes = plt.subplots(nrows=5, ncols=4, figsize=(20, 15))\n\nfor ax in axes.flatten():\n    plot_example(loader=test_loader, ax=ax, shape=shape)","794a4e65":"class UNet(nn.Module):\n\n    def __init__(self, in_channels=3, out_channels=1, init_features=32):\n        super(UNet, self).__init__()\n\n        features = init_features\n        \n        self.encoder1 = UNet._block(in_channels, features, name=\"enc1\")\n        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.encoder2 = UNet._block(features, features * 2, name=\"enc2\")\n        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.encoder3 = UNet._block(features * 2, features * 4, name=\"enc3\")\n        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.encoder4 = UNet._block(features * 4, features * 8, name=\"enc4\")\n        self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2)\n\n        self.bottleneck = UNet._block(features * 8, features * 16, name=\"bottleneck\")\n\n        self.upconv4 = nn.ConvTranspose2d(\n            features * 16, features * 8, kernel_size=2, stride=2\n        )\n        self.decoder4 = UNet._block((features * 8) * 2, features * 8, name=\"dec4\")\n        self.upconv3 = nn.ConvTranspose2d(\n            features * 8, features * 4, kernel_size=2, stride=2\n        )\n        self.decoder3 = UNet._block((features * 4) * 2, features * 4, name=\"dec3\")\n        self.upconv2 = nn.ConvTranspose2d(\n            features * 4, features * 2, kernel_size=2, stride=2\n        )\n        self.decoder2 = UNet._block((features * 2) * 2, features * 2, name=\"dec2\")\n        self.upconv1 = nn.ConvTranspose2d(\n            features * 2, features, kernel_size=2, stride=2\n        )\n        self.decoder1 = UNet._block(features * 2, features, name=\"dec1\")\n\n        self.conv = nn.Conv2d(\n            in_channels=features, out_channels=out_channels, kernel_size=1\n        )\n        \n    \n    def forward(self, x):\n#         print(1, x.shape)\n        enc1 = self.encoder1(x)\n\n        enc2 = self.encoder2(self.pool1(enc1))\n\n        enc3 = self.encoder3(self.pool2(enc2))\n        enc4 = self.encoder4(self.pool3(enc3))\n\n        bottleneck = self.bottleneck(self.pool4(enc4))\n\n        dec4 = self.upconv4(bottleneck)\n        dec4 = torch.cat((dec4, enc4), dim=1)\n        dec4 = self.decoder4(dec4)\n        dec3 = self.upconv3(dec4)\n        dec3 = torch.cat((dec3, enc3), dim=1)\n        dec3 = self.decoder3(dec3)\n        dec2 = self.upconv2(dec3)\n        dec2 = torch.cat((dec2, enc2), dim=1)\n        dec2 = self.decoder2(dec2)\n        dec1 = self.upconv1(dec2)\n        dec1 = torch.cat((dec1, enc1), dim=1)\n        dec1 = self.decoder1(dec1)\n        #print(dec1.shape)\n        return torch.sigmoid(self.conv(dec1))\n    \n    @staticmethod\n    def _block(in_channels, features, name):\n        return nn.Sequential(\n            OrderedDict(\n                [\n                    (\n                        name + \"conv1\",\n                        nn.Conv2d(\n                            in_channels=in_channels,\n                            out_channels=features,\n                            kernel_size=3,\n                            padding=1,\n                            bias=False,\n                        ),\n                    ),\n                    (name + \"norm1\", nn.BatchNorm2d(num_features=features)),\n                    (name + \"elu1\", nn.ELU(inplace=True)),\n                    (\n                        name + \"conv2\",\n                        nn.Conv2d(\n                            in_channels=features,\n                            out_channels=features,\n                            kernel_size=3,\n                            padding=1,\n                            bias=False,\n                        ),\n                    ),\n                    (name + \"norm2\", nn.BatchNorm2d(num_features=features)),\n                    (name + \"elu2\", nn.ELU(inplace=True)),\n                ]\n            )\n        )","4685ad69":"def dice2(y_pred, y_true, eps=1e-7):\n#     intersect = (y_pred * y_true).sum()\n#     return 1 - 2 * (intersect + eps) \/ (y_pred.sum() + y_true.sum() + eps)\n\n    iflat = y_pred.view(-1)\n    tflat = y_true.view(-1)\n    intersection = (iflat * tflat).sum()\n    \n    return 1 - ((2. * intersection + eps) \/ (iflat.sum() + tflat.sum() + eps))","9b794208":"def bce_dice(y_pred, y_true):\n    dice_loss = dice2(y_pred, y_true)\n    bce_score = nn.BCELoss()\n    bce_loss = bce_score(y_pred, y_true)\n    \n#     return dice_loss + bce_loss\n    return dice_loss","d53680d5":"x = torch.tensor([0., 0., 0., 1., 1., 1., 0., 0.])\ny = torch.tensor([0., 0., 1., 1., 1., 1., 1., 0.])\ndice2(x, x), dice2(x, y)","078ef73f":"bce_dice(x, x), bce_dice(x, y)","59ea14cd":"def make_dirr(name):\n    try:\n        os.makedirs(name)\n    except FileExistsError:\n        pass","8c87f940":"def train_model(model, optimizer, epochs, scheduler, train_data_loader, valid_data_loader, early_stopping=5, checkpoint_dirr=f'.\/checkpoint{1}\/'):\n    # \u043c\u0435\u0442\u0440\u0438\u043a\u0430 loss\n    metrics = defaultdict(list)\n    best_loss = np.inf\n    \n    # \u0434\u043b\u044f \u0440\u0430\u043d\u043d\u0435\u0439 \u043e\u0441\u0442\u0430\u043d\u043e\u0432\u043a\u0438\n    early_count = 0\n    \n    # \u0434\u0438\u0440\u0435\u043a\u0442\u043e\u0440\u0438\u044f\n    make_dirr(checkpoint_dirr)\n    \n    for epoch in range(epochs):\n        model.train()\n        train_loss = 0\n        metric = 0\n        scheduler.step()\n        \n        with tqdm(train_data_loader, unit=\"batch\") as tepoch:\n            tepoch.set_description(f\"Epoch {epoch}\")\n            \n            for i, (images, target) in enumerate(tepoch):\n                # \u043e\u043f\u0442\u0438\u043c\u0438\u0437\u0430\u0446\u0438\u044f \u043f\u043e\u0434 \u0443\u0441\u0442\u0440\u043e\u0439\u0441\u0442\u0432\u043e\n                images = images.to(device)\n                target = target.to(device) \n                \n                # \u0432 64 \u0431\u0438\u0442\u0430\n#                 target = target.long()\n                \n                # \u043e\u0431\u043d\u0443\u043b\u0438\u0442\u044c \u0433\u0440\u0430\u0434\u0438\u0435\u043d\u0442 \u043f\u0435\u0440\u0435\u0434 \u043e\u0431\u0440\u0430\u0442\u043d\u044b\u043c \u0440\u0430\u0441\u043f\u0440\u043e\u0441\u0442\u0440\u0430\u043d\u0435\u043d\u0438\u0435\u043c\n                optimizer.zero_grad()\n                \n                # \u043f\u0440\u043e\u0433\u043d\u043e\u0437\u044b\n                preds = model(images)\n                \n                # \u0444\u0443\u043d\u043a\u0446\u0438\u044f \u043f\u043e\u0442\u0435\u0440\u044c\n                loss = bce_dice(preds, target)\n                \n                # \u0432\u044b\u0447\u0438\u0441\u043b\u0438\u0442\u044c \u0433\u0440\u0430\u0434\u0438\u0435\u043d\u0442 \u043f\u043e\u0442\u0435\u0440\u044c\n                loss.backward()\n                \n                # \u043e\u0431\u043d\u043e\u0432\u0438\u0442\u044c \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u044b\n                optimizer.step()\n                \n                # \u0438\u0437\u043c\u0435\u0440\u0435\u043d\u0438\u0435 \u043c\u0435\u0442\u0440\u0438\u043a\n                train_loss += loss.item()\n                \n                tepoch.set_postfix(loss=train_loss \/ (i + 1), lr=scheduler.get_lr()[0])\n                \n            metrics['loss'].append(train_loss \/ (i + 1))\n            \n        # \u0440\u0435\u0436\u0438\u043c eval\n        model.eval()\n        \n        sleep(0.1)\n\n        with torch.no_grad():\n            valid_loss = 0\n            \n            with tqdm(valid_data_loader, unit=\"batch\") as tepoch:\n                tepoch.set_description(f\"Epoch {epoch}\")\n\n                for i, valid_batch in enumerate(tepoch):\n                    valid_images, valid_target = valid_batch\n                    valid_images = valid_images.to(device)\n                    valid_target = valid_target.to(device)\n                    \n#                     valid_target = valid_target.long()\n\n                    valid_preds = model(valid_images)\n\n                    loss_valid = bce_dice(valid_preds, valid_target)\n\n                    valid_loss += loss_valid.item()\n\n                    tepoch.set_postfix(loss=valid_loss \/ (i + 1))\n\n                metrics['val_loss'].append(valid_loss \/ (i + 1))\n\n        sleep(0.1)\n        \n        if valid_loss < best_loss:\n            # \u043f\u0435\u0440\u0435\u0441\u043e\u0445\u0440\u0430\u043d\u0438\u0442\u044c\n            best_loss = valid_loss\n\n            # \u0437\u0430\u043d\u0443\u043b\u0438\u0442\u044c \u0441\u0447\u0451\u0442\u0447\u0438\u043a\n            early_count = 0\n\n            # \u0437\u0430\u043f\u0438\u0441\u0430\u0442\u044c \n            torch.save(model.state_dict(), f'{checkpoint_dirr}epoch:{epoch}.pt')\n        else:\n            early_count += 1\n\n            if early_count >= early_stopping:\n                print(f\"Loss did not improve over {early_stopping} epochs => early stopping\")\n                break\n\n    return model, metrics, checkpoint_dirr","11383a15":"# \u0441\u0435\u0442\u043a\u0430\nmodel = UNet().to(device)\n\n# \u043e\u043f\u0442\u0438\u043c\u0438\u0437\u0430\u0442\u043e\u0440\noptimizer = optim.Adam(model.parameters(), lr=0.12)\n\n# \u0441\u0442\u0440\u0430\u0442\u0435\u0433\u0438\u044f \u0438\u0437\u043c\u0435\u043d\u0435\u043d\u0438\u044f learning rate\nscheduler = MultiplicativeLR(optimizer, lr_lambda=lambda epoch: 0.9)\n\n# \u043a\u043e\u043b-\u0432\u043e \u044d\u043f\u043e\u0445\nepochs = 35\n\nearly_stopping = 8\n    \nmodel, losses, dirr = train_model(model=model, optimizer=optimizer, epochs=epochs, scheduler=scheduler, early_stopping=early_stopping,\n                                  train_data_loader=train_loader, valid_data_loader=valid_loader)","86bdca5d":"model.load_state_dict(torch.load(f\"{dirr}{[name for name in sorted(os.listdir(dirr)) if str(np.argmax(-np.array(losses['val_loss']))) in name][0]}\"))\nmodel.eval()","f2595896":"def graph_plot(history, typ=False):\n    if typ:\n        for i in history.keys():\n            print(f'{i} = [{min(history[i])}; {max(history[i])}]\\n')\n    \n    epoch = len(history['loss'])\n    # \u043d\u0430 \u043a\u0430\u0436\u0434\u0443\u044e: (train, val) + lr\n    size = len(history.keys()) \/\/ 2 + 1\n\n    ncols = 4\n    nrows = int(np.ceil(size \/ ncols))\n\n    \n    fig = plt.figure(figsize=(27, 4))\n    i = 1\n    for k in list(history.keys()):\n        if 'val' not in k:\n            fig.add_subplot(nrows, ncols, i)\n            plt.plot(history[k], marker='o', markersize=5)\n            if k != 'lr':\n                plt.plot(history['val_' + k], marker='o', markersize=5)\n            plt.title(k, fontsize=10)\n\n            plt.ylabel(k)\n            plt.xlabel('epoch')\n            plt.grid()\n\n            plt.yticks(fontsize=10, rotation=30)\n            plt.xticks(fontsize=10, rotation=30)\n            plt.legend(['train', 'valid'], loc='upper left', fontsize=10, title_fontsize=15)\n            i += 1\n#         plt.show()\n\ngraph_plot(losses)","7c5f3349":"def plot_result(model, loader, ax, shape):\n    images, target = next(iter(loader))\n    ind = np.random.choice(range(loader.batch_size))\n#     try:\n#         ind = random.choice([i for i in range(images.shape[0]) if np.max(target[i].permute(1, 2, 0).cpu().detach().numpy()) > 0])\n#     except IndexError:\n#         images, target = next(iter(loader))\n#         ind = random.choice([i for i in range(images.shape[0]) if np.max(target[i].permute(1, 2, 0).cpu().detach().numpy()) > 0])\n\n    pred = model(images[ind].view(1, 3, shape, shape).to(device)).cpu().detach()[0].permute(1, 2, 0).reshape(shape, shape)\n    loss = dice2(model(images[ind].view(1, 3, shape, shape).to(device)), target[ind].to(device))\n    \n    image = images[ind].permute(1, 2, 0)\n    target = target[ind].permute(1, 2, 0).reshape(shape, shape)\n    target = Image.fromarray(np.asarray(target * 255, dtype=np.uint8)).convert('RGB')\n    pred = Image.fromarray(np.asarray(pred * 255, dtype=np.uint8)).convert('RGB')\n    \n    target = np.asarray(target)\n    pred = np.asarray(pred)\n    \n    ax.imshow(np.hstack([image, target, pred]))\n    ax.set_title(f'\u0418\u0441\u0445\u043e\u0434\u043d\u043e\u0435 \/ \u0422\u0430\u0440\u0433\u0435\u0442 \/ \u041f\u0440\u043e\u0433\u043d\u043e\u0437 ; loss = {round(float(loss), 3)}', fontsize=14)","decf19a7":"fig, axes = plt.subplots(nrows=6, ncols=4, figsize=(30, 20))\n\nfor ax in axes.flatten():\n    plot_result(model=model, loader=test_loader, shape=shape, ax=ax)","7bff2fcf":"loss = 0\n\nfor (images, targets) in test_loader:\n    images = images.to(device)\n    pred = model(images).cpu().detach()\n    \n    for i, (image, target) in enumerate(zip(images, targets)):\n        loss += dice2(pred[i], targets[i])","f8617fb6":"loss","d3d872bb":"loss \/ X_test.shape[0]","d6d851b3":"\u0412 \u043d\u0430\u0437\u0432\u0430\u043d\u0438\u0438 \u0435\u0441\u0442\u044c mask","a2463457":"\u0421\u0440\u0435\u0434\u043d\u044f\u044f \u043e\u0448\u0438\u0431\u043a\u0430","f09fc3c0":"\u0412 \u043d\u0430\u0437\u0432\u0430\u043d\u0438\u0438 \u043d\u0435\u0442 mask","eb49cb9d":"$1 - 2*\\frac{|X ** Y|}{|X| + |Y|}$\n\n0 -> \u043f\u0440\u0430\u0432\u0438\u043b\u044c\u043d\u043e \u0440\u0430\u0441\u043f\u043e\u0437\u043d\u0430\u043d\u043d\u043e\n\n1 -> \u043d\u0435 \u043f\u0440\u0430\u0432\u0438\u043b\u044c\u043d\u043e \u0440\u0430\u0441\u043f\u043e\u0437\u043d\u0430\u043d\u043d\u043e"}}