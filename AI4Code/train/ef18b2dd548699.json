{"cell_type":{"95044da9":"code","8436f610":"code","689418c1":"code","074ecbf9":"code","c9f4a487":"code","0369363c":"code","ba06b7c5":"code","d4e56a33":"markdown","88541406":"markdown","859453a7":"markdown","6dc3d2df":"markdown","3a9c6f28":"markdown","f3d6ff7b":"markdown","438a4ef9":"markdown","cc110798":"markdown","21dca05f":"markdown","1d5c5a4f":"markdown","968639bc":"markdown","5ee8e63a":"markdown"},"source":{"95044da9":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","8436f610":"%matplotlib notebook\nimport pandas as pd\nimport seaborn as sn\nimport numpy as np\nfrom sklearn.neural_network import MLPClassifier   #Multi-Layer Perceptron Classifier\nfrom sklearn.svm import LinearSVC\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score,confusion_matrix\nimport matplotlib.pyplot as plt     \n\ndataset=pd.read_csv('\/kaggle\/input\/heart-disease-uci\/heart.csv')\n\nprint(dataset.head(5))     #prints first 5 values for all columns\n\narray=dataset.values\ndata=array[:,0:13]\nlabels=array[:,13]\n#model1=MLPClassifier(activation='relu',solver='lbfgs',alpha=1e-5,random_state=1,) #doesnot work with neural networks\nmodel2=LinearSVC()    \nmodel3=LogisticRegression()","689418c1":"x_train,x_test,y_train,y_test=train_test_split(data,labels,test_size=0.2,random_state=4)\n\n\nmodel2=LinearSVC(C=0.01, penalty=\"l1\", dual=False)\nmodel2.fit(x_train,y_train)\nk2=model2.predict(x_test)\nprint(\"Accuracy of the SVC model on unseen data is \"+str(accuracy_score(k2,y_test)*100)+\" %\")\n\nmodel3=LogisticRegression(C=0.01)\nmodel3.fit(x_train,y_train)\nk3=model3.predict(x_test)\nprint(\"Accuracy of the Logistic regression model on unseen data is \"+str(accuracy_score(k3,y_test)*100)+\" %\")","074ecbf9":"from sklearn.feature_selection import SelectFromModel\n\nmainModel1=SelectFromModel(model2, prefit=True)\nmainModel2=SelectFromModel(model3, prefit=True)","c9f4a487":"print(\"Original data shape is : \",data.shape)\n\ndata_new1=mainModel1.transform(data)\nprint(\"New data shape after applying SelectFromModel on SVC : \",data_new1.shape)\n\ndata_new2=mainModel2.transform(data)\nprint(\"New data shape after applying SelectFromModel on Logistic Regression : \",data_new2.shape)","0369363c":"data1=data_new1[:,0:6]\nlabels1=data_new1[:,6]\n\nx_train1,x_test1,y_train1,y_test1=train_test_split(data,labels,test_size=0.2,random_state=4)\n\nmodel=LinearSVC()\nmodel.fit(x_train1,y_train1)\n\nk=model.predict(x_test)\n\nprint(\"Accuracy of the model on unseen transformed data : \"+str(accuracy_score(k,y_test)*100)+\" %\")","ba06b7c5":"data2=data_new2[:,0:4]\nlabels2=data_new1[:,4]\n\nx_train2,x_test2,y_train2,y_test2=train_test_split(data,labels,test_size=0.2,random_state=4)\n\nmodel=LinearSVC()\nmodel.fit(x_train1,y_train1)\n\nk=model.predict(x_test)\n\nprint(\"Accuracy of the model on unseen transformed data : \"+str(accuracy_score(k,y_test)*100)+\" %\")","d4e56a33":"## Dependencies and Data","88541406":"## Feature selection using SelectFromModel","859453a7":"As we can see the column value which represents the features has reduced to selecting the best of features","6dc3d2df":"## Transforming the original data ","3a9c6f28":"With SVMs and logistic-regression, the parameter C controls the sparsity: the smaller C the fewer features selected. With Lasso, the higher the alpha parameter, the fewer features selected.","f3d6ff7b":"## Fitting LinearSVC on the Transformed data","438a4ef9":"As we can see the transformed model has better accuracy than the model trained on normal data trained on LinearSVC which gave 78.68852459016394 % accuracy....Well here i'm just lucky ;p..Normally it is not the case, with feature reduction you guarentee the model won't be subjected to overfitting...This i'll just keep aside to preserve my luck...Please understand..","cc110798":"## Splitting Data and Fitting Data","21dca05f":"We will be continuing the same procedures until we used RFE in the last section. Instead of that here we will be using SelectFroModel and for keeping it simple we will use only linear models.\n\nApplying SelectFromModel method working on linear model is after using L1 norm.","1d5c5a4f":"In the Select from model case, we need to first fit the model to some algorithm (preferably SVC or Logistic Regression).\n\nThen select the important features which gives best accuracy when fed to that model.","968639bc":"We are using the sklearn in-built model SelectFromModel.\n\nWe try this on the models SVC and Logistic regression.\n\n    ","5ee8e63a":"Here too we see a rise in accuracy from 81.9672131147541 % to 91.80327868852459 %..Again here i tried a lot to get the accuracy score above the accuracy score without using feature reduction..This is not always the case,Sometimes you may never reach a better accuracy but one thing for sure is that feature reduction saves the model from overfitting."}}