{"cell_type":{"d9e81e9a":"code","46cb0da8":"code","a099c2c4":"code","c904525f":"code","a6958228":"code","d46aa80e":"code","d88ea107":"code","4c2c148b":"code","0d6b2868":"code","5f50c9c0":"code","e163ff9a":"code","1cc35410":"code","a03f76fa":"code","2aeb02c9":"code","ae20110e":"code","36ba419d":"code","3c2b9eea":"code","776b9fa7":"code","909a79d5":"code","21b9be3c":"code","d709af25":"code","06218765":"code","a25c03ae":"code","cdb01cf0":"code","b81527eb":"code","fc116f51":"code","25909e2b":"code","643bf63a":"code","b3a32193":"code","f9f393cc":"code","d05c9108":"code","c090b91a":"code","a144293e":"code","4ff4189b":"markdown","d8fe2964":"markdown","a6bdda66":"markdown","3612b573":"markdown","af5ba45d":"markdown","655fb11f":"markdown","fd405096":"markdown","2718afc5":"markdown","644b1ffd":"markdown","399f6076":"markdown","b7a97be4":"markdown","78172ded":"markdown","a432a207":"markdown","8a8de3e1":"markdown","6217d91f":"markdown","5534c148":"markdown","53aac5a9":"markdown"},"source":{"d9e81e9a":"import os\nimport math\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport imblearn\nfrom imblearn.over_sampling import SMOTE\nfrom collections import Counter\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import make_pipeline\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import  KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nimport xgboost\nfrom xgboost import XGBClassifier\n\nfrom sklearn.metrics import accuracy_score, confusion_matrix, precision_score, plot_confusion_matrix, f1_score, recall_score\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n%matplotlib inline","46cb0da8":"DATA_PATH = \"..\/input\/breast-cancer-wisconsin-data\/data.csv\"","a099c2c4":"data  = pd.read_csv(DATA_PATH)\ndata.head()","c904525f":"data.shape","a6958228":"data.columns","d46aa80e":"data.info()","d88ea107":"len(data.id.unique())","4c2c148b":"data.isnull().sum()","0d6b2868":"data.drop(columns = [\"id\", \"Unnamed: 32\"], inplace = True)\ndata.head()","5f50c9c0":"counts = data[\"diagnosis\"].value_counts()\ndiag_cols = [\"B\", \"M\"]\ndiag_counts = [counts[0], counts[1]]\n\nbenign = (diag_counts[0] \/ sum(diag_counts))*100\nmalignant = (diag_counts[1] \/ sum(diag_counts)) * 100\n\nprint(f\"Benign: {benign}%\")\nprint(f\"Malignant: {malignant}%\")\n\nprint()\n\nplt.figure(figsize = (10, 8))\nsns.barplot(x = diag_cols, y = diag_counts)\nplt.show()","e163ff9a":"diag_map = {\n    \"M\":1,\n    \"B\":0\n}\n\ndata[\"diagnosis\"] = data[\"diagnosis\"].map(diag_map).values.copy()","1cc35410":"all_columns = list(data.columns)","a03f76fa":"plt.figure(figsize = (25,55))\ncols = all_columns[1:]\nfor i in range(30):\n    plt.subplot(10, 3, i+1)\n    sns.distplot(data[cols[i]])\nplt.show()","2aeb02c9":"plt.figure(figsize = (25,55))\ncols = all_columns[1:]\nfor i in range(30):\n    plt.subplot(10, 3, i+1)\n    sns.boxplot(data[cols[i]])\nplt.show()","ae20110e":"plt.figure(figsize = (20,15))\nsns.heatmap(data[all_columns[1:]].corr(), center = 0)\nplt.show()","36ba419d":"corr_matrix = data[all_columns[1:]].corr()\nmask = corr_matrix >= 0.9\nmask","3c2b9eea":"correlated_cols = []\nfor column1 in all_columns[1:]:\n    for column2 in all_columns[1:]:\n        if corr_matrix[column1][column2] >= 0.9 and column1!=column2:\n            pair = (column1, column2)\n            if pair not in correlated_cols and pair[::-1] not in correlated_cols:\n                correlated_cols.append(pair)","776b9fa7":"correlated_cols","909a79d5":"cols = list(set([col for i in correlated_cols for col in i]))\ncols_acc = {}\nfor column in cols:\n    X = data[column]\n    y = data[\"diagnosis\"]\n    model = LogisticRegression()\n    model.fit(X[:250].values.reshape(-1, 1), y[:250])\n    accuracy = accuracy_score(y[250:], model.predict(X[250:].values.reshape(-1, 1)))\n    cols_acc[column] = accuracy\ncols_acc","21b9be3c":"imp_cols = []\nfor col in correlated_cols:\n    scores = [cols_acc[col[0]], cols_acc[col[1]]]\n    req_col = col[scores.index(max(scores))]\n    if req_col not in imp_cols:\n        imp_cols.append(req_col)\nimp_cols","d709af25":"data.shape\ndf = data.copy()\ndf.shape","06218765":"for col in cols:\n    if col not in imp_cols:\n        df.drop(columns=col, inplace = True)\ndf.shape","a25c03ae":"df.head()","cdb01cf0":"X = df[df.columns[1:]]\ny = df[\"diagnosis\"]","b81527eb":"X_train, X_test, y_train, y_test = train_test_split(X.values, y, test_size = 0.2, random_state = 0)\nprint(f\"Train Data: {X_train.shape}, {y_train.shape}\")\nprint(f\"Train Data: {X_test.shape}, {y_test.shape}\")","fc116f51":"counter = Counter(y_train)\ncounter","25909e2b":"upsample = SMOTE()\nX_train, y_train = upsample.fit_resample(X_train, y_train)\ncounter = Counter(y_train)\nprint(counter)","643bf63a":"print(f\"Total Data after Upsampling: {len(X_train)}\")","b3a32193":"print(f\"Train Data: {X_train.shape}, {y_train.shape}\")\nprint(f\"Train Data: {X_test.shape}, {y_test.shape}\")","f9f393cc":"error_rate = []\nfor i in range(1, 50):\n    pipeline = make_pipeline(StandardScaler(), KNeighborsClassifier(n_neighbors = i))\n    pipeline.fit(X_train, y_train)\n    predictions = pipeline.predict(X_test)\n    accuracy = accuracy_score(y_test, predictions)\n    print(f\"Accuracy at k = {i} is {accuracy}\")\n    error_rate.append(np.mean(predictions != y_test))\n\nplt.figure(figsize=(10,6))\nplt.plot(range(1,50),error_rate,color='blue', linestyle='dashed', \n         marker='o',markerfacecolor='red', markersize=10)\nplt.title('Error Rate vs. K Value')\nplt.xlabel('K')\nplt.ylabel('Error Rate')\nprint(\"Minimum error:-\",min(error_rate),\"at K =\",error_rate.index(min(error_rate))+1)","d05c9108":"svm_pipeline = make_pipeline(StandardScaler(), SVC(probability=True))\nsvm_pipeline.fit(X_train, y_train)\n\n# Accuray On Test Data\npredictions = svm_pipeline.predict(X_test)\naccuracy = accuracy_score(y_test, predictions)\nprint(f\"Accuracy on Test Data: {accuracy*100}%\")\nprint(f\"Precision Score: {precision_score(y_test, predictions)}\")\nprint(f\"Recall Score: {recall_score(y_test, predictions)}\")\nprint(f\"F1 Score: {f1_score(y_test, predictions)}\")\nplot_confusion_matrix(svm_pipeline, X_test, y_test)\nplt.title(\"Confusion Matrix for Test Data\")\nplt.show()\n\nprint()\n\n# Accuray On Whole Data\npredictions = svm_pipeline.predict(X.values)\naccuracy = accuracy_score(y, predictions)\nprint(f\"Accuracy on Whole Data: {accuracy*100}%\")\nprint(f\"Precision Score: {precision_score(y, predictions)}\")\nprint(f\"Recall Score: {recall_score(y, predictions)}\")\nprint(f\"F1 Score: {f1_score(y, predictions)}\")\nplot_confusion_matrix(svm_pipeline, X.values, y)\nplt.title(\"Confusion Matrix for Whole Data\")\nplt.show()","c090b91a":"rf_pipeline = make_pipeline(StandardScaler(), RandomForestClassifier(random_state = 18))\nrf_pipeline.fit(X_train, y_train)\n\n# Accuray On Test Data\npredictions = rf_pipeline.predict(X_test)\naccuracy = accuracy_score(y_test, predictions)\nprint(f\"Accuracy on Test Data: {accuracy*100}%\")\nprint(f\"Precision Score: {precision_score(y_test, predictions)}\")\nprint(f\"Recall Score: {recall_score(y_test, predictions)}\")\nprint(f\"F1 Score: {f1_score(y_test, predictions)}\")\nplot_confusion_matrix(rf_pipeline, X_test, y_test)\nplt.title(\"Confusion Matrix for Test Data\")\nplt.show()\n\nprint()\n\n# Accuray On Whole Data\npredictions = rf_pipeline.predict(X.values)\naccuracy = accuracy_score(y, predictions)\nprint(f\"Accuracy on Whole Data: {accuracy*100}%\")\nprint(f\"Precision Score: {precision_score(y, predictions)}\")\nprint(f\"Recall Score: {recall_score(y, predictions)}\")\nprint(f\"F1 Score: {f1_score(y, predictions)}\")\nplot_confusion_matrix(rf_pipeline, X.values, y)\nplt.title(\"Confusion Matrix for Whole Data\")\nplt.show()","a144293e":"xgb_pipeline = make_pipeline(StandardScaler(), XGBClassifier(random_state = 18))\nxgb_pipeline.fit(X_train, y_train)\n\n# Accuray On Test Data\npredictions = xgb_pipeline.predict(X_test)\naccuracy = accuracy_score(y_test, predictions)\nprint(f\"Accuracy on Test Data: {accuracy*100}%\")\nprint(f\"Precision Score: {precision_score(y_test, predictions)}\")\nprint(f\"Recall Score: {recall_score(y_test, predictions)}\")\nprint(f\"F1 Score: {f1_score(y_test, predictions)}\")\nplot_confusion_matrix(xgb_pipeline, X_test, y_test)\nplt.title(\"Confusion Matrix for Test Data\")\nplt.show()\n\nprint()\n\n# Accuray On Whole Data\npredictions = xgb_pipeline.predict(X.values)\naccuracy = accuracy_score(y, predictions)\nprint(f\"Accuracy on Whole Data: {accuracy*100}%\")\nprint(f\"Precision Score: {precision_score(y, predictions)}\")\nprint(f\"Recall Score: {recall_score(y, predictions)}\")\nprint(f\"F1 Score: {f1_score(y, predictions)}\")\nplot_confusion_matrix(xgb_pipeline, X.values, y)\nplt.title(\"Confusion Matrix for Whole Data\")\nplt.show()","4ff4189b":"## Upsampling using SMOTE\n","d8fe2964":"## Univariate Analysis","a6bdda66":"## Bivariate Analysis\n","3612b573":"## XGBoost Classifier\n","af5ba45d":"<div style=\"background-color:lightgreen;color:black;padding:10px;border-radius:10px;\">\n<b>Observation:<\/b> Almost 37% of the total data belong to Malignant Class and 63% belong to Benign class. We can balance this data using Upsampling.\n<\/div>","655fb11f":"<div style=\"background-color:lightgreen;color:black;padding:10px;border-radius:10px;\">\n<b>Observation:<\/b> We can notice that the above mentioned pairs are highly correlated with pearson correlation value >= 0.9. We can eliminate the columns that are highly correlated based on the correlation of the column with the target data. <br\/>\nThe correlation between categorical and continuous data can be measured in the following ways:<br\/>\n1. Point biserial Correlation<br\/>\n2. Logistic Regression<br\/>\n3. Kruskal-Wallis H Test (Or parametric forms such as t-test or ANOVA)\n<\/div>","fd405096":"<div style=\"background-color:lightgreen;color:black;padding:10px;border-radius:10px;\">\n<b>Observation:<\/b> We can notice that all the ids are unique and the number of ids are equal to the number of rows .i.e there is no duplicacy in the data. Let us drop the id's column.\n<\/div>","2718afc5":"\n## Results","644b1ffd":"## Using Logistic Regression for finding correlation between Continuous and Categorical Data\n","399f6076":"<div style=\"background-color:lightgreen;color:black;padding:10px;border-radius:10px;\">\n    \nAfter performing extensive Exploratory Data Analysis, eliminating the problem of imbalance and multicollinearity and experimenting with different machine learning algorithms, XGBoost Classifier outperformed remaining algorithms.<br\/>\n<b>Performance Metrics of the best model i.e. XGBoost Model<\/b><br\/>\n\n<center>\n<div align=\"center\">\n<center>\n<table style=\"background-color:lightgreen;color:black;\">\n    <tr>\n        <th colspan=4>XGB Classifier<\/th>\n    <\/tr>\n    <tr>\n        <th colspan=4>On Test Data<\/th>\n    <\/tr>\n    <tr>\n        <th>Accuracy<\/th>\n        <th>Precision<\/th>\n        <th>Recall<\/th>\n        <th>F1 Score<\/th>\n    <\/tr>\n    <tr>\n        <th>96.49%<\/th>\n        <th>0.93<\/th>\n        <th>0.97<\/th>\n        <th>0.95<\/th>\n    <\/tr>\n    <tr>\n        <th colspan=4>On Whole Data<\/th>\n    <\/tr>\n    <tr>\n        <th>Accuracy<\/th>\n        <th>Precision<\/th>\n        <th>Recall<\/th>        \n        <th>F1 Score<\/th>\n    <\/tr>\n    <tr>\n        <th>99.29%<\/th>\n        <th>0.98<\/th>\n        <th>0.99<\/th>        \n        <th>0.99<\/th>\n    <\/tr>\n<\/table>\n    <\/center>\n    <\/div>\n    <\/center>\n<\/div>","b7a97be4":"<b>Checking for Imbalance in data<\/b>","78172ded":"## KNN Classifier\n","a432a207":"## SVM Classifier\n","8a8de3e1":"## RandomForest Classifier\n","6217d91f":"\n## Exploring the Data","5534c148":"## Dataset Attributes Information\n\n<ul>\n    <li>ID number<\/li>\n    <li>Diagnosis (M = malignant, B = benign) (Target Variable)<\/li>\n    <li>radius (mean of distances from center to points on the perimeter)<\/li>\n    <li>texture (standard deviation of gray-scale values)<\/li>\n    <li>perimeter<\/li>\n    <li>area<\/li>\n    <li>smoothness (local variation in radius lengths)<\/li>\n    <li>compactness (perimeter^2 \/ area - 1.0)<\/li>\n    <li>concavity (severity of concave portions of the contour)<\/li>\n    <li>concave points (number of concave portions of the contour)<\/li>\n    <li>symmetry<\/li>\n    <li>fractal dimension (\"coastline approximation\" - 1)<\/li>\n<\/ul>","53aac5a9":"## Splitting the Data for training and testing\n"}}