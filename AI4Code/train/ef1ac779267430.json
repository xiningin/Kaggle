{"cell_type":{"cb774773":"code","36ab6222":"code","b184fb74":"code","ed16e3fe":"code","af377d09":"code","e458ba67":"code","b39212c1":"code","5e2a2521":"code","a6cdbbb8":"code","3672d712":"code","124ea510":"code","12966472":"code","88a9b861":"code","952a839a":"code","c3540317":"code","fccd1d2b":"code","cbe66bdf":"code","80b665b1":"code","2bda53d1":"code","43ded1ab":"code","8b75b862":"code","a6af6d83":"code","f5f1c57b":"code","91c719d2":"code","1ea3dae5":"code","948f5375":"code","61559a28":"code","16bdc6b8":"code","8da9d564":"code","bdcd01a5":"code","b7b67e9c":"code","0ea5c5bf":"code","aeab397d":"markdown","cb9d61da":"markdown","ed940c86":"markdown","4dbc7767":"markdown","b2a1cca5":"markdown","af36bf38":"markdown","f4a21aa7":"markdown","536f736d":"markdown","685fbf0b":"markdown","050e681d":"markdown","725f6eda":"markdown","b9efa6dc":"markdown","474e0848":"markdown","84e6d074":"markdown","a0cc48af":"markdown","b0ecc226":"markdown","47332534":"markdown","32a2247d":"markdown","36002bf7":"markdown","2704d40f":"markdown","7aedf8c6":"markdown","f3dc9cdb":"markdown","30fa9d9a":"markdown","82141770":"markdown","2ccfe581":"markdown","49aa9d85":"markdown","65d7fbcd":"markdown","ede52424":"markdown"},"source":{"cb774773":"!pip install --upgrade scikit-learn","36ab6222":"# Essentials:\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Clustering algorithm\nfrom sklearn.cluster import AgglomerativeClustering\n\n# Rand Index\nfrom sklearn.metrics.cluster import rand_score\n\n# Encode labels\nfrom sklearn import preprocessing\n\n# Confusion Matrix\nfrom sklearn.metrics import confusion_matrix","b184fb74":"# To make the code reproducable\nnp.random.seed(42)","ed16e3fe":"data_full = pd.read_csv('..\/input\/mushroom-classification\/mushrooms.csv')\ndata_full.head()","af377d09":"target = data_full[['class']]\ndata_no_target = data_full.drop(['class'],axis=1)\ndata_no_target.head()","e458ba67":"data_no_target.info()","b39212c1":"data_no_target.nunique()","5e2a2521":"data_categorical = data_no_target.drop(['veil-type'], axis=1)","a6cdbbb8":"!pip install gower","3672d712":"import gower\n\ndistance_matrix = gower.gower_matrix(data_categorical)\n\ndistance_matrix","124ea510":"model = AgglomerativeClustering(n_clusters=2, \n                                affinity='precomputed')","12966472":"clusters = model.fit_predict(distance_matrix)","88a9b861":"encoder = preprocessing.LabelEncoder()\n\nencoded_target = target.apply(encoder.fit_transform)\n\nprint(f'in this encoding, {encoded_target.iloc[0].values} represents {target.iloc[0].values}')\n\nlabels = pd.DataFrame()\nlabels['target'] = encoded_target.values.reshape(1, -1).tolist()[0]","952a839a":"model_single = AgglomerativeClustering(n_clusters=2, linkage='single', affinity='precomputed')\nclusters_single = model_single.fit_predict(distance_matrix)","c3540317":"labels['single-predictions'] = clusters_single","fccd1d2b":"sri = rand_score(encoded_target.values.reshape(1, -1)[0], clusters_single)\nprint(f'Rand Index: {sri}')","cbe66bdf":"labels[['single-predictions']].value_counts().plot.pie(autopct='%1.0f%%', pctdistance=0.7, labeldistance=1.1)","80b665b1":"model_average = AgglomerativeClustering(n_clusters=2, linkage='average', affinity='precomputed')\nclusters_average = model_average.fit_predict(distance_matrix)","2bda53d1":"labels['average-predictions'] = clusters_average","43ded1ab":"ari = rand_score(encoded_target.values.reshape(1, -1)[0], clusters_average)\nprint(f'Rand Index: {ari}')","8b75b862":"labels[['average-predictions']].value_counts().plot.pie(autopct='%1.0f%%', pctdistance=0.7, labeldistance=1.1)","a6af6d83":"model_complete = AgglomerativeClustering(n_clusters=2, linkage='complete', affinity='precomputed')\nclusters_complete = model_complete.fit_predict(distance_matrix)","f5f1c57b":"labels['complete-predictions'] = clusters_complete","91c719d2":"cri = rand_score(encoded_target.values.reshape(1, -1)[0], clusters_complete)\nprint(f'Rand Index: {cri}')","1ea3dae5":"labels[['complete-predictions']].value_counts().plot.pie(autopct='%1.0f%%', pctdistance=0.7, labeldistance=1.1)","948f5375":"labels.value_counts([\"target\", \"complete-predictions\"])","61559a28":"labels['aligned-clusters'] = labels['complete-predictions'].apply(lambda x: int(not x))","16bdc6b8":"labels.value_counts([\"target\", \"aligned-clusters\"])","8da9d564":"cf_matrix = confusion_matrix(encoded_target.values.reshape(1, -1)[0], labels[[\"aligned-clusters\"]].values.reshape(1, -1)[0])\ncf_labels = ['True Neg','False Pos','False Neg','True Pos']\ncf_labels = np.asarray(cf_labels).reshape(2,2)\nfig, ax = plt.subplots(1, 1)\nsns.heatmap(cf_matrix\/np.sum(cf_matrix), annot=cf_labels, fmt='', cmap='Blues')\nax.set_ylabel('Target Labels')    \nax.set_xlabel('Predicted Labels')","bdcd01a5":"from IPython.display import YouTubeVideo\n\nYouTubeVideo('-ORE0pp9QNk')","b7b67e9c":"True_neg = cf_matrix[0,0]\nFalse_pos = cf_matrix[0,1]\nTrue_pos = cf_matrix[1,1]\nFalse_neg = cf_matrix[1,0]\n\naccuracy = (True_neg + True_pos)\/(True_neg + False_neg + True_pos + False_pos)\nrecall = (True_pos)\/(False_neg+True_pos)\nprecision = (True_pos)\/(False_pos + True_pos)\nF1_score = 2 * ((precision*recall)\/(precision+recall))","0ea5c5bf":"print(f'Accuracy: {accuracy}')\nprint(f'Recall: {recall}')\nprint(f'Precision: {precision}')\nprint(f'F1_score: {F1_score}')","aeab397d":"## Agglomerative Clustering with Complete Linkage","cb9d61da":"one of the columns (`veil-type`) has only 1 unique value. since there are no missing values, this means that in every row, this column has the same repeated value. in other words, this column is useless and we can remove it without any effect on our performance. let's do it:","ed940c86":"<center><img src=\"https:\/\/p1.pxfuel.com\/preview\/700\/259\/602\/mushrooms-fungi-forest-nature.jpg\" alt=\"mushrooms\" width=\"400\"\/>\n    <h1>Clustering Categorical Data using Gower distance<\/h1>\n    <h3>\ud83c\udf44 Mushrooms clustered Hierarchically!<\/h3>\n<\/center>","4dbc7767":"# Everything Looks good!\n\nCongrats! we did it! \ud83c\udf89\n\nWe successfully used Gower Distance to cluster categorical data using Agglomerative clustering and the results were totally acceptable. \n\n<div class=\"alert alert-danger\" role=\"alert\" style=\"text-align:center;\">\n    I hope you enjoyed this tutorial. If you did, please consider subscribing to <b><a href=\"https:\/\/www.youtube.com\/channel\/UC34Gj0-vHuBiTNEYlP7wczg\">my YouTube Channel \u25b6<\/a><\/b>\n<\/div>\n\n<center><h2><span style=\"font-family:cursive;\"> Also, please Upvode! \ud83d\ude1c <\/span><\/h2><\/center>","b2a1cca5":"hmmm... it seems that our clusters have the opposite labels compared with encoded targets. so, let's first align our labels:","af36bf38":"Wow! this is much better! \ud83d\udc4f \n\nLet's compare our clusters with original target classes:","f4a21aa7":"## Examine Data type\n\nLet's see which columns are numerical and which ones are not:","536f736d":"Now that we have encoded target values, we can begin training models!","685fbf0b":"## Confusion Matrix\n\nlet's create a confusion matrix to compare our predicted labels with the actual target:","050e681d":"This is terrible! obviously `single` shouldn't be our choice.","725f6eda":"**That's it! we need no more 'preprocessing' on our data.**","b9efa6dc":"# Gower Distance\n\nGower Distance is a distance measure that can be used to calculate distance between two entity whose attribute has a mixed of categorical and numerical values.\n\nIt is not included in Scikit learn package, but fortunately, there is a nice implementation of it available on [github](https:\/\/github.com\/wwwjk366\/gower).\n\nso, let's install it:","474e0848":"# Preprocessing\n\n## Removing the target value\nSince we want to perform clustering on this dataset, we must remove target values.","84e6d074":"It's even worse than `single` linkage. just one more option is left. let's see if it works! (fingers crossed!!! \ud83e\udd1e)","a0cc48af":"here is how it works: you simply feed the `gower.gower_matrix` your dataset, and it returns a distance matrix; which then can be fed to several scikit learn models like `DBSCAN` and `AgglomerativeClustering`. Let's calculate the distance matrix:","b0ecc226":"Now we can use a confusion matrix to better understand our performance.","47332534":"# Using Gower Distance for Agglomerative Clustering\n\nNow that we have a nice distance matrix, we can cluster our data. some of the scikit learn's clustering models are able to process a distance matrix instead of raw data. for example, `DBSCAN`, `OPTICS` and `AgglomerativeClustering`.\n\nAs I've experienced, parameter tuning for `DBSCAN` and `OPTICS` can be a pain in the A$$, so let's use `AgglomerativeClustering` instead.\n\n<div class=\"alert alert-warning\" role=\"alert\">\n  \u26a0 if you want to learn more about Agglomerative clustering, read <a href=\"https:\/\/github.com\/HalflingWizard\/MachineLearning\/blob\/main\/3-%20Clustering\/Hierarchical%20clustering.md\" class=\"alert-link\">my notes on this method<\/a>.\n<\/div>\n\nIn order to cluster our data with `AgglomerativeClustering` using distance matrix, we should set the `affinity` as `precomputed` and then feed the model with `distance_matrix`. we also want our model make 2 clusters, because the original targets (which we are going to use to evaluate our model's performance) have two classes: `p` (poisonous) and `e` (edible)","32a2247d":"## Agglomerative Clustering with Average Linkage","36002bf7":"## Oops! we made a mistake! \ud83d\ude2c\n\nScikit learn's `AgglomerativeClustering` uses `ward` as its **Linkage** by default. linkage is the measure we use to find distance between clusters. scikit learn provides 4 linkage criterions: `ward`, `average`, `complete` and `single`. you can find a comparison between them [here](https:\/\/scikit-learn.org\/stable\/auto_examples\/cluster\/plot_linkage_comparison.html). \n\n> - single linkage is fast, and can perform well on non-globular data, but it performs poorly in the presence of noise.\n> - average and complete linkage perform well on cleanly separated globular clusters, but have mixed results otherwise.\n> - Ward is the most effective method for noisy data.\n\nas the error massege confirms, **Ward can only work with euclidean distances.** so, we have to choose between `single`, `complete` and `average`. Let's just use three models with different linkages and see which one is better.\n\nIn order to do so, we need a evaluation metric. since we have our target values, I'm going to use _Rand Index_.","2704d40f":"## Load Dataset","7aedf8c6":"using the confusion matrix, we could calculate other evaluation metrics such as **accuracy**, **percision**, **recall** and **F1 Score**\n\n<div class=\"alert alert-danger\" role=\"alert\">\n  \u26a0 If you are not familiar with Confusion matrix and Classification Evaluation and Metrics, I recommend you watch <a href=\"https:\/\/www.youtube.com\/watch?v=-ORE0pp9QNk\" class=\"alert-link\">my video on this subject<\/a>.\n<\/div>","f3dc9cdb":"now let's evaluate the results with rand index and a pie chart indicating the number of data in each cluster. (we expect it to be rather balanced)","30fa9d9a":"since all of the columns have `Dtype = object` we conclude that we are facing a dataset that only consists of categorical data. also, note that there are no _missing values_ in this dataset.","82141770":"## Agglomerative Clustering with Single Linkage","2ccfe581":"As you know, K-Means clustering, DBSCAN, OPTICS and hierarchical clustering all have one thing in common: They are all Distance-based clustering algorithms. since these algorithms all use Euclidean distance function, they are not good for clustering categorical data. so, in order to cluster non-numerical data using these methods, we have to use other distance functions. \n\none of the most famous distance functions that can be aplied on categorical data, is **Gower Distance function**, which we are going to use in this notebook to cluster some data on different mushrooms. ","49aa9d85":"## Investigate categories\n\nhere is how we can see the number of categories in each column:","65d7fbcd":"## Prepare target values for Rand Index\n\nThe Rand Index computes a similarity measure between two clusterings by considering all pairs of samples and counting pairs that are assigned in the same or different clusters in the predicted and true clusterings. Perfect labeling is scored 1.0.\n\nTo do so, I have to encode target labels:","ede52424":"# Getting Started\n\n## Load Libraries\n\nLet's begin by loading some libraries that we are going to use later on:"}}