{"cell_type":{"51354daf":"code","d6240258":"code","54f34118":"code","d887922b":"code","600636c4":"code","7fc1d7d2":"code","16146d85":"code","d3f8d8ca":"code","a5ae10d5":"code","e0db5558":"code","f29eb0e5":"code","f5dc9b6d":"code","d9665516":"code","dd9feab3":"code","e9c91b4f":"code","7d352028":"code","8fa2749a":"code","788889fa":"code","8d90228f":"code","b0139482":"code","7f0cdb39":"code","17f5673a":"code","53128ac1":"code","97760515":"code","caa00516":"code","ea2469de":"code","ce5144e4":"code","d4e5e4b1":"code","ce06a882":"code","154d2eb6":"markdown","6c7a9916":"markdown","fe885fa5":"markdown","6ef9477e":"markdown","2f6df591":"markdown","c7c520d0":"markdown","44d85e75":"markdown","af8125b6":"markdown","1ebb1660":"markdown","abe7f299":"markdown","c06bb5cf":"markdown","1a494c9f":"markdown","43c2c1b4":"markdown","6e72ec11":"markdown","e53ee185":"markdown","fe45f69a":"markdown","d8b75f8a":"markdown","9bf7f767":"markdown","1640c76d":"markdown","b75a1eea":"markdown","1cb492d9":"markdown","fc9f1375":"markdown","cdfd5b3a":"markdown","25c17e4e":"markdown","1f50f5b4":"markdown","e5659bb4":"markdown","68c5c2bd":"markdown","1727dda5":"markdown","26660253":"markdown","df308f5f":"markdown"},"source":{"51354daf":"import os\nimport pandas as pd\nimport numpy as np\nimport json\nimport re\nfrom nltk.tokenize import sent_tokenize \nfrom transformers import BertTokenizer, AutoTokenizer\nfrom torch.utils.data import DataLoader\nimport torch\nimport torch.nn as nn\nimport transformers\nfrom tqdm import tqdm\nimport glob\nfrom sklearn.model_selection import train_test_split\nimport datetime \nimport warnings\nwarnings.filterwarnings('ignore')","d6240258":"platform = 'Kaggle'\nmodel_name = 'model1_bert_base_uncased.bin'\n\nif platform == 'Kaggle':\n    bert_path = '..\/input\/huggingface-bert\/bert-base-uncased\/'\n    train_path = '\/kaggle\/input\/coleridgeinitiative-show-us-the-data\/train\/'\n    test_path = '\/kaggle\/input\/coleridgeinitiative-show-us-the-data\/test\/*'\n    model_path = '..\/input\/coleridgemodels\/'+ model_name\n    \nconfig = {'MAX_LEN':128,\n          'tokenizer': AutoTokenizer.from_pretrained(bert_path , do_lower_case=True),\n          'batch_size':5,\n          'Epoch': 1,\n          'train_path':train_path,\n          'test_path':test_path, \n          'device': 'cuda' if torch.cuda.is_available() else 'cpu',\n          'model_path':model_path,\n          'model_name':model_name\n         }","54f34118":"train = pd.read_csv(\"..\/input\/coleridgeinitiative-show-us-the-data\/train.csv\")\n\ntrain_df = train.groupby(['Id']).agg(label_count = ('cleaned_label', 'count'),\n                                     label = ('cleaned_label', '|'.join)).reset_index()\ntrain_df","d887922b":"def read_all_json(df, path):\n    '''\n    This function reads all the json input files and \n    return a dictionary containing the id as the key and all the \n    contents of the json as values\n    '''\n    text_data = {}\n    for i, rec_id in tqdm(enumerate(df.Id), total = len(df.Id)):\n        location = f'{path}{rec_id}.json'\n\n        with open(location, 'r') as f:\n            text_data[rec_id] = json.load(f)\n        \n    print(\"All files read\")\n    \n    return text_data","600636c4":"train_data_dict = read_all_json(df=train_df, path=config['train_path'])","7fc1d7d2":"def clean_text(txt):\n    '''\n    This is text cleaning function\n    '''\n    return re.sub('[^A-Za-z0-9]+', ' ', str(txt).lower())\n\ndef data_joining(data_dict_id):\n    '''\n    This function is to join all the text data from different \n    sections in the json to a single text file. \n    '''\n    data_length = len(data_dict_id)\n\n    #     temp = [clean_text(data_dict_id[i]['text']) for i in range(data_length)]\n    temp = [data_dict_id[i]['text'] for i in range(data_length)]\n    temp = '. '.join(temp)\n    \n    return temp\n\ndef make_shorter_sentence(sentence):\n    '''\n    This function is to split the long sentences into chunks of shorter sentences upto the \n    maximum length of words specified in config['MAX_LEN']\n    '''\n    sent_tokenized = sent_tokenize(sentence)\n    \n    max_length = config['MAX_LEN']\n    overlap = 20\n    \n    final_sentences = []\n    \n    for tokenized_sent in sent_tokenized:\n        sent_tokenized_clean = clean_text(tokenized_sent)\n        sent_tokenized_clean = sent_tokenized_clean.replace('.','').rstrip() \n        \n        tok_sent = sent_tokenized_clean.split(\" \")\n        \n        if len(tok_sent)<max_length:\n            final_sentences.append(sent_tokenized_clean)\n        else :\n#             print(\"Making shorter sentences\")\n            start = 0\n            end = len(tok_sent)\n            \n            for i in range(start, end, max_length-overlap):\n                temp = tok_sent[i: (i + max_length)]\n                final_sentences.append(\" \".join(i for i in temp))\n\n    return final_sentences","16146d85":"def form_labels(sentence, labels_list):\n    '''\n    This function labels the training data \n    '''\n    matched_kwords = []\n    matched_token = []\n    un_matched_kwords = []\n    label = []\n\n    # Since there are many sentences which are more than 512 words,\n    # Let's make the max length to be 128 words per sentence.\n    tokens = make_shorter_sentence(sentence)\n    \n    for tok in tokens:    \n        tok_split = config['tokenizer'].tokenize(tok)\n        \n        z = np.array(['O'] * len(tok_split)) # Create final label == len(tokens) of each sentence\n        matched_keywords = 0 # Initially no kword matched    \n\n        for kword in labels_list:\n            if kword in tok: #This is to first check if the keyword is in the text and then go ahead\n                kword_split = config['tokenizer'].tokenize(kword)\n                for i in range(len(tok_split)):\n                    if tok_split[i: (i + len(kword_split))] == kword_split:\n                        matched_keywords += 1\n\n                        if (len(kword_split) == 1):\n                            z[i] = 'B'\n                        else:\n                            z[i] = 'B'\n                            z[(i+1) : (i+ len(kword_split))]= 'B'\n\n                        if matched_keywords >1:\n                            label[-1] = (z.tolist())\n                            matched_token[-1] = tok\n                            matched_kwords[-1].append(kword)\n                        else:\n                            label.append(z.tolist())\n                            matched_token.append(tok)\n                            matched_kwords.append([kword])\n                    else:\n                        un_matched_kwords.append(kword)\n                \n    return matched_token, matched_kwords, label, un_matched_kwords","d3f8d8ca":"def labelling(dataset, data_dict):\n    '''\n    This function is to iterate each of the training data and get it labelled \n    from the form_labels() function.\n    '''\n    \n    Id_list_ = []\n    sentences_ = []\n    key_ = []\n    labels_ = []\n    un_mat = []\n    un_matched_reviews = 0\n\n    for i, Id in tqdm(enumerate(dataset.Id), total=len(dataset.Id)):\n\n        sentence = data_joining(data_dict[Id])\n        labels = train_df.label[train_df.Id == Id].tolist()[0].split(\"|\")\n\n        s, k, l, un_matched = form_labels(sentence=sentence, labels_list = labels)\n\n        if len(s) == 0:\n            un_matched_reviews += 1\n            un_mat.append(un_matched)\n        else: \n            sentences_.append(s)\n            key_.append(k)\n            labels_.append(l)\n            Id_list_.append([Id]*len(l))\n\n    print(\"Total unmatched keywords:\", un_matched_reviews)\n    sentences = [item for sublist in sentences_ for item in sublist]\n    final_labels = [item for sublist in labels_ for item in sublist]\n    keywords = [item for sublist in key_ for item in sublist]\n    Id_list = [item for sublist in Id_list_ for item in sublist]\n    \n    return sentences, final_labels, keywords, Id_list\n","a5ae10d5":"train_sentences, train_labels, train_keywords, train_Id_list = labelling(dataset = train_df, \n                                                                         data_dict=train_data_dict)\n\nprint(\"\")\nprint(f\" train sentences: {len(train_sentences)}, train label: {len(train_labels)}, train keywords: {len(train_keywords)}, train_id list: {len(train_Id_list)}\")","e0db5558":"unique_df = pd.DataFrame({'id':train_Id_list, \n                          'train_sentences': train_sentences, \n                          'kword': train_keywords, \n                          'label':train_labels})\nunique_df.label = unique_df.label.astype('str')\nunique_df.kword = unique_df.kword.astype('str')\nunique_df['sent_len'] = unique_df.train_sentences.apply(lambda x : len(x.split(\" \")))\nunique_df = unique_df.drop_duplicates()\nprint(unique_df.shape)\nunique_df","f29eb0e5":"unique_df = unique_df.sample(int(unique_df.shape[0]*0.10)).reset_index(drop=True)\nunique_df.shape","f5dc9b6d":"np.random.seed(100)\ntrain_df, valid_df = train_test_split(unique_df, test_size=0.2)\n\ntrain_df = train_df.reset_index(drop=True)\nvalid_df = valid_df.reset_index(drop=True)\n\nprint(train_df.shape, valid_df.shape)","d9665516":"print(\"Let's look at a simple example\")\nprint(\"Train sentence:\", unique_df.train_sentences[0])\nprint(\"Train label:\", unique_df.kword[0])\nprint(\"Train label:\", unique_df.label[0])","dd9feab3":"tags_2_idx = {'O': 0 , 'B': 1, 'P': 2} # 'P' means padding. \n\ndef dataset_2_list(df):\n    id_list = df.id.values.tolist()\n    sentences_list = df.train_sentences.values.tolist()\n    keywords_list = df.kword.apply(lambda x : eval(x)).values.tolist()\n    \n    labels_list = df.label.apply(lambda x : eval(x)).values.tolist()    \n    labels_list = [list(map(tags_2_idx.get, lab)) for lab in labels_list]\n    \n    return id_list, sentences_list, keywords_list, labels_list\n\nfinal_train_id_list, final_train_sentences, final_train_keywords, final_train_labels = dataset_2_list(df=train_df)\nfinal_valid_id_list, final_valid_sentences, final_valid_keywords, final_valid_labels = dataset_2_list(df=valid_df)","e9c91b4f":"class form_input():\n    def __init__(self, ID, sentence, kword, label, data_type='test'):\n        self.id = ID\n        self.sentence = sentence\n        self.kword = kword\n        self.label = label\n        self.max_length = config['MAX_LEN']\n        self.tokenizer = config['tokenizer']\n        self.data_type = data_type\n    \n    def __len__(self):\n        return len(self.sentence)\n    \n    def __getitem__(self, item):\n        toks = config['tokenizer'].tokenize(self.sentence[item])\n        label = self.label[item]\n\n        if len(toks)>self.max_length:\n            toks = toks[:self.max_length]\n            label = label[:self.max_length]\n        \n        \n        ########################################\n        # Forming the inputs\n        ids = config['tokenizer'].convert_tokens_to_ids(toks)\n        tok_type_id = [0] * len(ids)\n        att_mask = [1] * len(ids)\n        \n        # Padding\n        pad_len = self.max_length - len(ids)        \n        ids = ids + [2] * pad_len\n        tok_type_id = tok_type_id + [0] * pad_len\n        att_mask = att_mask + [0] * pad_len\n        \n        ########################################            \n        # Forming the label\n        if self.data_type !='test':\n            label = label + [2]*pad_len\n        else:\n            label = 1\n            \n        \n        return {'ids': torch.tensor(ids, dtype = torch.long),\n                'tok_type_id': torch.tensor(tok_type_id, dtype = torch.long),\n                'att_mask': torch.tensor(att_mask, dtype = torch.long),\n                'target': torch.tensor(label, dtype = torch.long)\n               }\n            ","7d352028":"train_prod_input = form_input(ID=final_train_id_list, \n                              sentence=final_train_sentences, \n                              kword=final_train_keywords, \n                              label=final_train_labels, \n                              data_type='train')\n\nvalid_prod_input = form_input(ID=final_valid_id_list, \n                              sentence=final_valid_sentences, \n                              kword=final_valid_keywords, \n                              label=final_valid_labels, \n                              data_type='valid')\n\ntrain_prod_input_data_loader = DataLoader(train_prod_input, \n                                          batch_size= config['batch_size'], \n                                          shuffle=True)\n\nvalid_prod_input_data_loader = DataLoader(valid_prod_input, \n                                          batch_size= config['batch_size'], \n                                          shuffle=True)\n","8fa2749a":"# Checking a sample input\nind = 1\nprint(\"Input sentence:\")\nprint(final_train_sentences[ind])\n\nprint(\"\")\nprint(\"Input label:\")\nprint(final_train_keywords[ind])\n\nprint(\"\")\nprint(\"Output:\")\ntrain_prod_input[ind]#, valid_prod_input[ind]","788889fa":"def train_fn(data_loader, model, optimizer):\n    '''\n    Functiont to train the model\n    '''\n    \n    train_loss = 0\n    for index, dataset in enumerate(tqdm(data_loader, total = len(data_loader))):\n        batch_input_ids = dataset['ids'].to(config['device'], dtype = torch.long)\n        batch_att_mask = dataset['att_mask'].to(config['device'], dtype = torch.long)\n        batch_tok_type_id = dataset['tok_type_id'].to(config['device'], dtype = torch.long)\n        batch_target = dataset['target'].to(config['device'], dtype = torch.long)\n                \n        output = model(batch_input_ids, \n                       token_type_ids=None,\n                       attention_mask=batch_att_mask,\n                       labels=batch_target)\n        \n        step_loss = output[0]\n        prediction = output[1]\n        \n        step_loss.sum().backward()\n        optimizer.step()        \n        train_loss += step_loss\n        optimizer.zero_grad()\n        \n    return train_loss.sum()","8d90228f":"def eval_fn(data_loader, model):\n    '''\n    Functiont to evaluate the model on each epoch. \n    We can also use Jaccard metric to see the performance on each epoch.\n    '''\n    \n    model.eval()\n    \n    eval_loss = 0\n    predictions = np.array([], dtype = np.int64).reshape(0, config['MAX_LEN'])\n    true_labels = np.array([], dtype = np.int64).reshape(0, config['MAX_LEN'])\n    \n    with torch.no_grad():\n        for index, dataset in enumerate(tqdm(data_loader, total = len(data_loader))):\n            batch_input_ids = dataset['ids'].to(config['device'], dtype = torch.long)\n            batch_att_mask = dataset['att_mask'].to(config['device'], dtype = torch.long)\n            batch_tok_type_id = dataset['tok_type_id'].to(config['device'], dtype = torch.long)\n            batch_target = dataset['target'].to(config['device'], dtype = torch.long)\n\n            output = model(batch_input_ids, \n                           token_type_ids=None,\n                           attention_mask=batch_att_mask,\n                           labels=batch_target)\n\n            step_loss = output[0]\n            eval_prediction = output[1]\n\n            eval_loss += step_loss\n            \n            eval_prediction = np.argmax(eval_prediction.detach().to('cpu').numpy(), axis = 2)\n            actual = batch_target.to('cpu').numpy()\n            \n            predictions = np.concatenate((predictions, eval_prediction), axis = 0)\n            true_labels = np.concatenate((true_labels, actual), axis = 0)\n            \n    return eval_loss.sum(), predictions, true_labels","b0139482":"def train_engine(epoch, train_data, valid_data):\n    model = transformers.BertForTokenClassification.from_pretrained('bert-base-uncased',  num_labels = len(tags_2_idx))\n    model = nn.DataParallel(model)\n    model = model.to(config['device'])\n    \n    params = model.parameters()\n    optimizer = torch.optim.Adam(params, lr= 3e-5)\n    \n    best_eval_loss = 1000000\n    for i in range(epoch):\n        train_loss = train_fn(data_loader = train_data, \n                              model=model, \n                              optimizer=optimizer)\n        eval_loss, eval_predictions, true_labels = eval_fn(data_loader = valid_data, \n                                                           model=model)\n        \n        print(f\"Epoch {i} , Train loss: {train_loss}, Eval loss: {eval_loss}\")\n\n        if eval_loss < best_eval_loss:\n            best_eval_loss = eval_loss           \n            \n            print(\"Saving the model\")\n            torch.save(model.state_dict(), config['model_name'])\n            \n    return model, eval_predictions, true_labels ","7f0cdb39":"model, val_predictions, val_true_labels = train_engine(epoch=config['Epoch'],\n                                                       train_data=train_prod_input_data_loader, \n                                                       valid_data=valid_prod_input_data_loader)","17f5673a":"def read_test_json(test_data_folder):\n    '''\n    This function reads all the json input files and return a dictionary containing the id as the key\n    and all the contents of the json as values\n    '''\n\n    test_text_data = {}\n    total_files = len(glob.glob(test_data_folder))\n    \n    for i, test_json_loc in enumerate(glob.glob(test_data_folder)):\n        filename = test_json_loc.split(\"\/\")[-1][:-5]\n\n        with open(test_json_loc, 'r') as f:\n            test_text_data[filename] = json.load(f)\n\n        if (i%1000) == 0:\n            print(f\"Completed {i}\/{total_files}\")\n\n    print(\"All files read\")\n    return test_text_data","53128ac1":"test_data_dict = read_test_json(test_data_folder=config['test_path'])","97760515":"# Prediction\ndef prediction_fn(tokenized_sub_sentence):\n\n    tkns = tokenized_sub_sentence\n    indexed_tokens = config['tokenizer'].convert_tokens_to_ids(tkns)\n    segments_ids = [0] * len(indexed_tokens)\n\n    tokens_tensor = torch.tensor([indexed_tokens]).to(config['device'])\n    segments_tensors = torch.tensor([segments_ids]).to(config['device'])\n    \n    model.eval()\n    with torch.no_grad():\n        logit = model(tokens_tensor, \n                      token_type_ids=None,\n                      attention_mask=segments_tensors)\n\n        logit_new = logit[0].argmax(2).detach().cpu().numpy().tolist()\n        prediction = logit_new[0]\n\n        kword = ''\n        kword_list = []\n\n        for k, j in enumerate(prediction):\n            if (len(prediction)>1):\n\n                if (j!=0) & (k==0):\n                    #if it's the first word in the first position\n                    #print('At begin first word')\n                    begin = tkns[k]\n                    kword = begin\n\n                elif (j!=0) & (k>=1) & (prediction[k-1]==0):\n                    #begin word is in the middle of the sentence\n                    begin = tkns[k]\n                    previous = tkns[k-1]\n\n                    if begin.startswith('##'):\n                        kword = previous + begin[2:]\n                    else:\n                        kword = begin\n\n                    if k == (len(prediction) - 1):\n                        #print('begin and end word is the last word of the sentence')\n                        kword_list.append(kword.rstrip().lstrip())\n\n                elif (j!=0) & (k>=1) & (prediction[k-1]!=0):\n                    # intermediate word of the same keyword\n                    inter = tkns[k]\n\n                    if inter.startswith('##'):\n                        kword = kword + \"\" + inter[2:]\n                    else:\n                        kword = kword + \" \" + inter\n\n\n                    if k == (len(prediction) - 1):\n                        #print('begin and end')\n                        kword_list.append(kword.rstrip().lstrip())\n\n                elif (j==0) & (k>=1) & (prediction[k-1] !=0):\n                    # End of a keywords but not end of sentence.\n                    kword_list.append(kword.rstrip().lstrip())\n                    kword = ''\n                    inter = ''\n            else:\n                if (j!=0):\n                    begin = tkns[k]\n                    kword = begin\n                    kword_list.append(kword.rstrip().lstrip())\n\n    return kword_list","caa00516":"def long_sent_split(long_tokens):\n    '''\n    If the token length is >the max length this function splits it into \n    mutiple list of specified smaller max_length\n    '''\n    \n    start = 0\n    end = len(long_tokens)\n    max_length = 64\n\n    final_long_tok_split = []\n    for i in range(start, end, max_length):\n        temp = long_tokens[i: (i + max_length)]\n        final_long_tok_split.append(temp)\n    return final_long_tok_split","ea2469de":"def get_predictions(data_dict):\n    \n    results = {}\n\n    for i, Id in enumerate(data_dict.keys()):\n        current_id_predictions = []\n        \n#         print(Id)\n        sentences = data_joining(data_dict[Id])\n        sentence_tokens = sent_tokenize(sentences)\n        \n        for sub_sentence in sentence_tokens:\n            cleaned_sub_sentence = clean_text(sub_sentence)\n        \n            # Tokenize the sentence\n            tokenized_sub_sentence = config['tokenizer'].tokenize(cleaned_sub_sentence)\n            \n            if len(tokenized_sub_sentence) == 0:\n                # If the tokenized sentence are empty\n                sub_sentence_prediction_kword_list = []\n                \n            elif len(tokenized_sub_sentence) <= 512:\n                # If the tokenized sentence are less than 512\n                sub_sentence_prediction_kword_list = prediction_fn(tokenized_sub_sentence)\n\n            else:\n                # If the tokenized sentence are >512 which is long sentences\n                long_sent_kword_list = []\n                \n                tokenized_sub_sentence_tok_split = long_sent_split(tokenized_sub_sentence)\n                for i, sent_tok in enumerate(tokenized_sub_sentence_tok_split):\n                    if len(sent_tok) != 0:\n                        kword_list = prediction_fn(sent_tok)\n                        long_sent_kword_list.append(kword_list)\n                flat_long_sent_kword = [item for sublist in long_sent_kword_list for item in sublist]\n                sub_sentence_prediction_kword_list = flat_long_sent_kword\n                            \n            if len(sub_sentence_prediction_kword_list) !=0:\n                current_id_predictions = current_id_predictions + sub_sentence_prediction_kword_list\n\n        results[Id] = list(set(current_id_predictions))\n                \n    print(\"All predictions completed\")\n    \n    return results","ce5144e4":"results = get_predictions(data_dict = test_data_dict)","d4e5e4b1":"sub_df = pd.DataFrame({'Id': list(results.keys()),\n                       'PredictionString': list(results.values())})\nsub_df.PredictionString = sub_df.PredictionString.apply(lambda x : \"|\".join(x))\nsub_df","ce06a882":"sub_df.to_csv(\"submission.csv\", index=False)","154d2eb6":"# About the data.\nThe data that we are going to be using for this tutorial is from the recently concluded \n[Coleridge Initiative - Show US the Data](https:\/\/www.kaggle.com\/c\/coleridgeinitiative-show-us-the-data) competition. The summary of the competition is to find the excerpt from the publication which represents the name of a dataset used for that particular analysis.\n\nThe textual information are from different publications, and each of them are stored as a json file with ids as unique key to identify them. The labels from publications are stored in the train.csv which we will use to label the textual documents from the publication. I would recommend you to go through the competition overview to understand in details about the data.","6c7a9916":"# Reading all the json train files","fe885fa5":"As we can see that the input sentence we passed to the function returned a dictionary of \n```ids, tok_type_ids, att_mask, target```.\n\nThe <B>ids<\/B> contains some numbers which is nothing but the index position of the tokens in the Bert vocabulary. \nIf you want to know how the values are obtained in the <B>ids<\/B> I would recommend downloading the vocab.txt from [HuggingFace repo](https:\/\/huggingface.co\/bert-base-uncased\/tree\/main) and checking the index with respect to the token starting from 0. \n\nI have shown the token ids for the first 5 words of an example sentence (<B>\"2 comparsions with the most\"<\/B>) in the screenshot below. The <B>2<\/B> in the ids are nothing but padded value.\n\nThe <B>att_mask<\/B> contains <B>1<\/B> for the all the tokens positions and <B>0<\/B> for all the padded positsion, so the model give attention to the tokens and not the padded positions.\n\n<B>tok_type_id<\/B> is not required for this task so we can skip them.\n\n\n<B>target<\/B> as we saw before are lablled as <B>1<\/B> for the label positions(\"survey of doctorate recipients\"), <B>0<\/B> for the others and <B>2<\/B> for the padded positions. \n\n![image.png](attachment:1bb33830-8139-4f37-bd6f-00398dbd846e.png)","6ef9477e":"# Converting the DataFrame back to list","2f6df591":"# Read the train data and combine the labels together","c7c520d0":"![image.png](attachment:3e3909f4-313a-4a46-ac51-02384c1ddc6f.png)","44d85e75":"### How do we do this?\n* Tokenize the train sentence.\n<br>['control', 'samples', 'were', 'selected', 'from', 'individuals', 'who', 'had', 'participated', 'in', 'genome', '##wide', 'association', 'studies', 'performed', 'by', 'our', 'group', '78', '##7', 'samples', 'from', 'the', 'ne', '##uro', '##gen', '##etic', '##s', 'collection', 'at', 'the', 'co', '##rie', '##ll', 'cell', 'repository', 'and', '72', '##8', 'from', 'the', 'baltimore', 'longitudinal', 'study', 'of', 'aging', 'b', '##ls', '##a']\n\n* Tokenize the labels\n<br>[['genome', '##wide', 'association', 'studies'], ['baltimore', 'longitudinal', 'study', 'of', 'aging']]\n\n* For each tokenized label, loop the tokenized train sentence and wherever you find a match label them as <B>'B'<\/B> and rest of them as <B>'O'<\/B>.\n\n    Since there are two labels we will loop the train sentence twice\n\n    At the end of the 1st loop for ['genome', '##wide', 'association', 'studies'], the label looks like this\n\n    ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'B', 'B', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n\n    At the end of the 2nd loops for ['baltimore', 'longitudinal', 'study', 'of', 'aging'] the final label looks like this\n\n    ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'B', 'B', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'B', 'B', 'B', 'B', 'O', 'O', 'O']\n\nMost tutorials follow a pattern of <B>BIO<\/B> labelling (B-begin, I-interior, O-outside) which is nothing but label the first word as <B>B<\/B>, last word as <B>O<\/B> and all the intermediate words as <B>I<\/B> and all others as some other character say <B>X<\/B>\n\nIf labelled in this format then our labels would look like\n\n['X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'B', 'I', 'I', 'O', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'B', 'I', 'I', 'I', 'O', 'X', 'X', 'X']\n\nHowever I would like to stick to only labeling them as 'B' as I find it to be simple","af8125b6":"# --------- Do upvote if you like :) ---------","1ebb1660":"# Why is it called Token Classification in Bert?\nWe know that in tasks like sequence classification, the entire sequence is classified to belong to 1 or 0 for binary classificationn and any 1 of the N classes for multiclass classfication. \n\nHowever in Token Classification every single token(i.e words) in the sequence is classified as 1 of the N classes. \n\nWhat do I mean by that? Assuming if it's a binary classification problem. If there is a sentence of 10 words, then each word is classified as 0 or 1, so the ouput dimension will be 10 * 2 with 2 being the probability of 0 and 1.\n\nIf it is a multiclass problem then the number of possible classes will be N and there will be N class probablilites for each token.","abe7f299":"# Model initialization and train\n\nWe have initialized the model in the function below as\n\n```\nmodel = transformers.BertForTokenClassification.from_pretrained('bert-base-uncased',  num_labels = len(tags_2_idx))\n```\n\n<U>Note<\/U>: \nDo you know why the label ```num_labels = len(tags_2_idx)```? because we have only 3 classes <B>[O, B, P]<\/B> so each of the token can be classified as any of the 3 i.e.\n```tags_2_idx={'O':0 , 'B':1, 'P':2}```\n\nHowever if we had labelled them in the <B>BIO<\/B> format then there would be 5 classes <B>[X, B, I, O, P]<\/B> then ```tags_2_idx={'X':0 , 'B':1, 'I':2, 'O':3, 'P':5}``` so num_labels would have been ```num_labels = 5```","c06bb5cf":"# Taking a sample of the dataset\nLet's take only a small sample of the data (0.05%) in this tutorial to train a model for experimentation. You can use the entire dataset or whatever sample size you want","1a494c9f":"# Config","43c2c1b4":"<B>Label<\/B>:\n\"genomewide association studies\", \"baltimore longitudinal study of aging\".\nAs you could see there are two labels of interest to us from the example sentence.","6e72ec11":"#### I have written a similar notebook on how we can use Roberta for NER tasks. Do check this [notebook](https:\/\/www.kaggle.com\/thanish\/roberta-token-classification-training-inference) for the tutorial. Consider leaving an upvote if you like and comment for any questions :)\n","e53ee185":"<B>Portion to label<\/B>:\n\"control samples were selected from individuals who had participated in <B>genomewide association studies<\/B> performed by our group 787 samples from the neurogenetics collection at the coriell cell repository and 728 from the <B>baltimore longitudinal study of aging<\/B> blsa\"","fe45f69a":"# Create DataFrame to remove the duplicates","d8b75f8a":"# Name Entity Recognition(NER) using BERT:","9bf7f767":"One added information to note is we have set the Max_length to be 128 words per sentence. \nHowever when we tokenize it, due to Bert's way of splitting non-vocab and compound words into multiple sub-words, the no.of tokens could be more than the no. of words. If the number of tokens is <128 then the extra positions are padded with <B>\"P\"<\/B>, However if they exceed the Max_Length we truncate them to 128 tokens at this portion of the code which is a common practice.\n\n```\nif len(toks)>self.max_length:\n    toks = toks[:self.max_length]\n    label = label[:self.max_length]\n```\n\nDo you feel there could be information loss due to the truncation? Well if you check <B>make_shorter_sentence(sentence)<\/B> function there is a variable <B>overlap=20<\/B>, So the begining of every sentence is from the last 20 words from the previous sentence. So the tokens truncated in the sentence1 could be captured in the sentence 2.","1640c76d":"We have labelled all our train dataset and our labelled positions are denoted as <B>B<\/B>. However when we have to pass it to the model we will have to convert them into numerical representation.\n\nWe will convert them as follows: {<B>'O'<\/B>: 0 , <B>'B'<\/B>: 1, <B>'P'<\/B>: 2}\n\nHere <B>P<\/B> means padding which is appended to our train and label when the total word count in the sentence is not equal to the mentioned <B>MAX_LEN<\/B> in config. We will add the padding in the class <B>form_input()<\/B>","b75a1eea":"There are extensive tutorials available online to understand BERT. However you can check out Jay Alammar's blog on [A Visual Guide to Using BERT for the First Time](https:\/\/jalammar.github.io\/a-visual-guide-to-using-bert-for-the-first-time\/) which is one of the excellent way to learn BERT visually","1cb492d9":"# Tokenization and Labeling the data\n\n### Bert tokenization:\nBefore we move into labelling, let's understand how Bert tokenizer works. Tokenization is nothing but splitting the sentence into words, however Bert tokenizer splits sentences into words that are in the vocabulary. If there are compound words or words that are not part of the vocabulary then Bert splits them into multiple sub-words with each subword except the first subword starting with <B>\"##\"<\/B>. \n\nCheck out this [blog](https:\/\/blog.floydhub.com\/tokenization-nlp\/) on different methods of tokenizer to understand why BERT tokenizes the sentence the way it does\n\n### Labelling\n\nThe function form_labels() is actually where we label the data to be fed to the algorithms later in the notebook. \n\nNote: The label will be an excerpt from the train text.\n\n<B>Example sentence<\/B>:\n\"control samples were selected from individuals who had participated in genomewide association studies performed by our group 787 samples from the neurogenetics collection at the coriell cell repository and 728 from the baltimore longitudinal study of aging blsa\"\n\n","fc9f1375":"# Contents\n* Declare configurations\n* Reading the train dataset and preprocessing the text files\n* Tokenization and Data Labelling for the NER task\n* Define the DataLoader to batch the train dataset for training\n* Model training using BertForTokenClassification\n* Evaluation function to evaluate the model while training\n* Reading the test dataset\n* Prediction function to get the prediction for the test data\n* Result consolidation","cdfd5b3a":"# Training section","25c17e4e":"# Train and validation split","1f50f5b4":"# Reading the test data","e5659bb4":"\nName Entity Recognition in short called NER is method of information extractions from the text data which comes under the NLP space. The most common entities that are extracted from the text data could be name of the person, Country, company, contact information like email-id, phone-no, home address etc.\n\nHowever NER tasks are not just limited to these standard entities, they could also be finetuned\/trained to identify custom entities as per our need from the text data. \n\nThere are function in <B>NLTK<\/B> and <B>Spacy<\/B> that can be used for NER tasks. However in this tutorial we will be going through how can we identify the entities of our interest using <B>BERT<\/B> by using the method <B>BertForTokenClassification<\/B> from HuggingFace Transformer package using Pytorch","68c5c2bd":"# Forming the input and defining the dataloader","1727dda5":"# Prediction function\n\nFor the prediction part we pass the text data sentence.\n\n<U>Example test input:<\/U> \n```\n\"in section 5 the proposed method is illustrated by analyzing a diffusion tensor imaging dti data set from the alzheimer disease neuroimaging initiative adni database\"\n```\n<U>Tokenize the input text<\/U>:\n```\n['in', 'section', '5', 'the', 'proposed', 'method', 'is', 'illustrated', 'by', 'analyzing', 'a', 'diffusion', 'tensor', 'imaging', 'dt', '##i', 'data', 'set', 'from', 'the', 'alzheimer', 'disease', 'ne', '##uro', '##ima', '##ging', 'initiative', 'ad', '##ni', 'database']\n ```\n<U>Get the token ids<\/U>:\n```\n[1999, 2930, 1019, 1996, 3818, 4118, 2003, 7203, 2011, 20253, 1037, 19241, 23435, 12126, 26718, 2072, 2951, 2275, 2013, 1996, 21901, 4295, 11265, 10976, 9581, 4726, 6349, 4748, 3490, 7809]\n```\n\n<U>Prediction output<\/U>: \n\nThe token Ids are passed to the trained model which outputs the predicted probabilites for each token and the argmax is taken to get the class with max probability and it looks like this.\n```\n[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n```\n<U>Final predicted keyword<\/U>:\n\nCompare the prediction and the test input tokenized text. Get all the tokens that are predicted as <B>1<\/B> from the test input Tokenized text which is \n\n<B>[['tensor', 'imaging', 'dt', '##i', 'data', 'set'], ['alzheimer', 'disease', 'ne', '##uro', '##ima', '##ging', 'initiative', 'ad', '##ni', 'database']]<\/B>. \n\nCombine the subwords with <B>\"##\"<\/B>, Separate each element in the list by spaces and return them as the identified entity by the model \n\n<B>\"tensor imaging dti data set\", \"alzheimer disease neuroimaging initiative adni database\"<\/B>\n\n<U>Note<\/U>: If you labelled your dataset in the <B>BIO<\/B> format then you will have to modify the <B>prediction_fn()<\/B> function accordingly","26660253":"# Data cleaning and Joining functions","df308f5f":"![Image](https:\/\/confusedcoders.com\/wp-content\/uploads\/2019\/11\/image2.png)"}}