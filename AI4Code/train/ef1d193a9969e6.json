{"cell_type":{"5a5165a7":"code","532c3660":"code","846dd44b":"code","36f011c9":"code","34d9cd1b":"code","a681e6ba":"code","732e2222":"code","6cbc178c":"code","9ec807d0":"code","ba822da1":"code","c919e5bf":"code","0cd3f670":"code","39f7c331":"code","b97f0460":"code","6167b3d5":"code","72096d8e":"code","38911b10":"code","d99180d4":"code","4f15ec72":"code","c53e2370":"code","3ecf8644":"code","a5a35bcb":"code","764b964d":"code","2f4470d8":"code","5363735e":"code","c02767ed":"code","d0fdd9e2":"code","96b163e1":"code","30699335":"code","49024ad4":"code","ceac01cc":"code","3effb012":"code","eb816c8e":"code","f199639e":"code","40865528":"code","3fa3ce0b":"code","891debb1":"code","fa3659c8":"code","eee877cb":"code","6ebd3e1b":"code","00c6a355":"code","0e20528a":"code","69c2d8f0":"code","5d9ae9e5":"code","3b82155b":"code","8aa475b9":"code","ec756959":"code","ad50be52":"code","b4cf71ca":"code","b8da3234":"code","cc8acb84":"code","7c90a7cf":"code","a082336e":"markdown","3cbae3fa":"markdown","233be8ae":"markdown","5181150e":"markdown","9f374e96":"markdown","57bad82e":"markdown","c193fa22":"markdown","b4d09307":"markdown","630d0d63":"markdown","47b2598d":"markdown","48d0cf83":"markdown","f2caa66c":"markdown","ea4008f5":"markdown","fe486c4e":"markdown","0443ac8e":"markdown","a1dd5932":"markdown","00eb67cc":"markdown","aff34185":"markdown","5fe96579":"markdown","63fda26b":"markdown","767d38c2":"markdown","08bba67e":"markdown"},"source":{"5a5165a7":"import numpy as np\nfrom numpy import expand_dims\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\nfrom tensorflow.keras.preprocessing.image import array_to_img\nfrom tensorflow.keras.utils import to_categorical\nfrom sklearn.metrics import classification_report, confusion_matrix","532c3660":"# Loading the dataset\n(X_trainfull, y_trainfull), (X_test, y_test) = keras.datasets.cifar10.load_data()\n\n#Splitting the trainfull set into train and validation set\nX_train, X_valid = X_trainfull[:-5_000], X_trainfull[-5_000:]\ny_train, y_valid = y_trainfull[:-5_000], y_trainfull[-5_000:]\n\nlabel_names = [\"airplane\", \"automobile\", \"bird\", \"cat\", \"deer\", \"dog\", \"frog\", \"horse\", \"ship\", \"truck\"]","846dd44b":"# Scaling the data\nX_train = X_train \/ 255.\nX_valid = X_valid \/ 255.\nX_test = X_test \/ 255.\n\n# Coverting the target values to 1 dimension\ny_train = y_train.ravel()\ny_valid = y_valid.ravel()\ny_test = y_test.ravel()","36f011c9":"# Converting target values to Categorical\n\ny_train_cat = to_categorical(y_train, 10)\ny_valid_cat = to_categorical(y_valid, 10)\ny_test_cat = to_categorical(y_test, 10)","34d9cd1b":"plt.figure(figsize=(15,12))\nfor i in range (225):\n    plt.subplot(15, 15, i+1)\n    plt.imshow(X_train[i])\n    plt.title(np.array(label_names)[y_train[i]])\n    plt.axis(\"off\")\nplt.tight_layout()\nplt.show()","a681e6ba":"df = pd.DataFrame(np.array(label_names)[y_train])\ndf = pd.DataFrame({\"Count\":df.value_counts()})\ndf = df.reset_index().rename({0:\"Category\"}, axis = 1)\ndf.style.bar(color = \"#f43f1a\", vmin=4450.0)","732e2222":"print(\"y_train Percentage:\")\nround(pd.DataFrame(np.array(label_names)[y_train].ravel()).value_counts(normalize=True)*100,2)","6cbc178c":"print(\"y_valid Percentage:\")\nround(pd.DataFrame(np.array(label_names)[y_valid].ravel()).value_counts(normalize=True)*100,2)","9ec807d0":"print(\"y_test Percentage:\")\nround(pd.DataFrame(np.array(label_names)[y_test].ravel()).value_counts(normalize=True)*100,2)","ba822da1":"from functools import partial\n\n# setting a random seed for reproducibilty of results\nnp.random.seed(42)\ntf.random.set_seed(42)\n\nkeras.backend.clear_session()\n\n# creating a Default convolutional layer\nDefaultConv = partial(keras.layers.Conv2D, strides = 1, kernel_size = 3, padding = 'same', activation = 'relu')\n\nmodel = keras.models.Sequential([\n    DefaultConv(filters = 32, input_shape = [32,32,3]),\n    keras.layers.BatchNormalization(),\n    DefaultConv(filters = 32, kernel_size = 5),\n    keras.layers.BatchNormalization(),\n    keras.layers.MaxPool2D(pool_size=2),\n    keras.layers.Dropout(0.2),\n    \n    DefaultConv(filters = 64),\n    keras.layers.BatchNormalization(),\n    DefaultConv(filters = 64),\n    keras.layers.BatchNormalization(),\n    keras.layers.MaxPool2D(pool_size=2),\n    keras.layers.Dropout(0.3),\n    \n    DefaultConv(filters = 128),\n    keras.layers.BatchNormalization(),\n    DefaultConv(filters = 128),\n    keras.layers.BatchNormalization(),\n    keras.layers.MaxPool2D(pool_size=2),\n    keras.layers.Dropout(0.4),\n    \n    keras.layers.Flatten(),\n    keras.layers.Dense(128, activation = 'relu', kernel_initializer = 'he_normal', use_bias = False),\n    keras.layers.Dropout(0.5),\n    keras.layers.Dense(10, activation = 'softmax'),\n])","c919e5bf":"model.summary()","0cd3f670":"model.compile(loss=\"categorical_crossentropy\", metrics=['accuracy'],\n             optimizer = 'nadam')","39f7c331":"# To save the best model so far during training\nCheckpoint_cb = keras.callbacks.ModelCheckpoint(\"No_Augmentation_model.h5\", save_best_only=True)\n\n# To stop training if our validation loss has not improved after 10 epochs\nEarlystopping_cb = keras.callbacks.EarlyStopping(patience=10)\n\n\ncallback = [Checkpoint_cb, Earlystopping_cb]","b97f0460":"history = model.fit(X_train, y_train_cat, epochs = 100,\n    validation_data = (X_valid, y_valid_cat),\n    callbacks=callback)","6167b3d5":"model = keras.models.load_model(\"No_Augmentation_model.h5\")\n\nmodel.evaluate(X_test, y_test_cat)","72096d8e":"plt.figure(figsize=(16,6))\nplt.plot(history.history['loss'], 'r+-', label = \"Training loss\")\nplt.plot(history.history['val_loss'], 'bo-', label = \"Validation loss\")\nplt.title(\"Training vs Validation Loss (No Data Augmentation)\", fontsize = 16)\nplt.legend(fontsize=14)\nplt.show()","38911b10":"plt.figure(figsize=(16,6))\nplt.plot(history.history['accuracy'], 'r+-', label = \"Training accuracy\")\nplt.plot(history.history['val_accuracy'], 'bo-', label = \"Validation accuracy\")\nplt.yticks(np.linspace(0.3,0.9,7))\nplt.title(\"Training vs Validation Accuracy (No Data Augmentation)\", fontsize = 16)\nplt.legend(fontsize=14, loc = 'lower right')\nplt.show()","d99180d4":"# setting suppress to True to obtain float numbers instead of scientific notation\nnp.set_printoptions(suppress=True)\n\npred_1 = model.predict(X_test)\ny_pred_1 = np.argmax(pred_1, axis = 1)","4f15ec72":"conf_matrix_1 = confusion_matrix(y_test, y_pred_1)\nconf_matrix_1","c53e2370":"# Plotting the confusion matrix using seaborns' heatmap\n\nplt.figure(figsize=(12,8))\nsns.heatmap(conf_matrix_1, cmap = 'PiYG', annot = True, fmt = '.0f', cbar = False,\n            annot_kws = {\n            'weight':'bold'},\n           xticklabels = label_names, yticklabels = label_names)\nplt.title(\"Confusion Matrix (Without Image Augmentation)\", fontsize = 16)\nplt.show()","3ecf8644":"# Printing the classification report based on how well our model performed\n\nprint(classification_report(y_test, y_pred_1, target_names = label_names))","a5a35bcb":"plt.figure(figsize=(15,12))\nfor i in range (100):\n    plt.subplot(10, 10, i+1)\n    plt.imshow(X_test[i])\n    plt.title(f\"Actual: {np.array(label_names)[y_test[i]]}\\nPredicted: {np.array(label_names)[y_pred_1[i]]}\", fontsize = 8)\n    plt.axis(\"off\")\nplt.tight_layout()\nplt.show()","764b964d":"# Creating an image generator\n\ntrain_datagen = ImageDataGenerator(\n    rotation_range = 15,\n    zoom_range = 0.2,\n    width_shift_range = 0.15,\n    height_shift_range = 0.15\n)\n\ntrain_generator = train_datagen.flow(X_train, y_train_cat, batch_size = 32)","2f4470d8":"# Creating a dummy variable\n\nX_dummy = X_train[8]\nX_dummy = X_dummy[np.newaxis, ...]\n\nexample_generator = train_datagen.flow(X_dummy, batch_size = 1)","5363735e":"plt.figure(figsize=(6,6))\nfor i in range(9):\n    plt.subplot(3,3,i+1)\n    batch = example_generator.next()\n    image = batch[0].astype(np.float64)\n    plt.imshow(image)\n    plt.axis('off')\nplt.tight_layout()\nplt.show()","c02767ed":"from functools import partial\n\nnp.random.seed(42)\ntf.random.set_seed(42)\n\nkeras.backend.clear_session()\n\nDefaultConv = partial(keras.layers.Conv2D, strides = 1, kernel_size = 3, padding = 'same', activation = 'relu')\n\nmodel = keras.models.Sequential([\n    DefaultConv(filters = 32, input_shape = [32,32,3]),\n    keras.layers.BatchNormalization(),\n    DefaultConv(filters = 32, kernel_size = 5),\n    keras.layers.BatchNormalization(),\n    keras.layers.MaxPool2D(pool_size=2),\n    keras.layers.Dropout(0.2),\n    \n    DefaultConv(filters = 64),\n    keras.layers.BatchNormalization(),\n    DefaultConv(filters = 64),\n    keras.layers.BatchNormalization(),\n    keras.layers.MaxPool2D(pool_size=2),\n    keras.layers.Dropout(0.3),\n    \n    DefaultConv(filters = 128),\n    keras.layers.BatchNormalization(),\n    DefaultConv(filters = 128),\n    keras.layers.BatchNormalization(),\n    keras.layers.MaxPool2D(pool_size=2),\n    keras.layers.Dropout(0.4),\n    \n    keras.layers.Flatten(),\n    keras.layers.Dense(128, activation = 'relu', kernel_initializer = 'he_normal', use_bias = False),\n    keras.layers.Dropout(0.5),\n    keras.layers.Dense(10, activation = 'softmax'),\n])","d0fdd9e2":"Checkpoint_cb = keras.callbacks.ModelCheckpoint(\"Data_Augmentation_model.h5\", save_best_only=True)\nEarlystopping_cb = keras.callbacks.EarlyStopping(patience=10)\n\ncallback = [Checkpoint_cb, Earlystopping_cb]","96b163e1":"model.compile(loss=\"categorical_crossentropy\", metrics=['accuracy'],\n             optimizer = 'nadam')","30699335":"history = model.fit(train_generator, epochs = 100, steps_per_epoch = len(X_train) \/ 32,\n    validation_data = (X_valid, y_valid_cat),\n    callbacks=callback)","49024ad4":"model = keras.models.load_model('Data_Augmentation_model.h5')\n\nmodel.evaluate(X_test, y_test_cat)","ceac01cc":"plt.figure(figsize=(16,6))\nplt.plot(history.history['loss'], 'r+-', label = \"Training loss\")\nplt.plot(history.history['val_loss'], 'bo-', label = \"Validation loss\")\nplt.title(\"Training vs Validation Loss (With Data Augmentation)\", fontsize = 16)\nplt.legend(fontsize=14)\nplt.show()","3effb012":"plt.figure(figsize=(16,6))\nplt.plot(history.history['accuracy'], 'r+-', label = \"Training accuracy\")\nplt.plot(history.history['val_accuracy'], 'bo-', label = \"Validation accuracy\")\nplt.yticks(np.linspace(0.3,0.9,7))\nplt.title(\"Training vs Validation Accuracy (With Data Augmentation)\", fontsize = 16)\nplt.legend(fontsize=14, loc = 'lower right')\nplt.show()","eb816c8e":"pred_2 = model.predict(X_test)\ny_pred_2 = np.argmax(pred_2, axis = 1)","f199639e":"conf_matrix_2 = confusion_matrix(y_test, y_pred_2)\nconf_matrix_2","40865528":"# Plotting the confusion matrix using seaborns' heatmap\n\nplt.figure(figsize=(12,8))\nsns.heatmap(conf_matrix_2, cmap = 'PiYG', annot = True, fmt = '.0f', cbar = False,\n            annot_kws = {\n            'weight':'bold'},\n           xticklabels = label_names, yticklabels = label_names)\nplt.title(\"Confusion Matrix (With Image Augmentation)\", fontsize = 16)\nplt.show()","3fa3ce0b":"# Printing the classification report based on how well our model performed\n\nprint(classification_report(y_test, y_pred_2, target_names = label_names))","891debb1":"plt.figure(figsize=(15,12))\nfor i in range (100):\n    plt.subplot(10, 10, i+1)\n    plt.imshow(X_test[i])\n    plt.title(f\"Actual: {np.array(label_names)[y_test[i]]}\\nPredicted: {np.array(label_names)[y_pred_2[i]]}\", fontsize = 8)\n    plt.axis(\"off\")\nplt.tight_layout()\nplt.show()","fa3659c8":"# Loading the dataset again because the pretrained network assumes the pixels range from 0 ~ 255. \n(X_trainfull, y_trainfull), (X_test, y_test) = keras.datasets.cifar10.load_data()\n\n#Splitting the trainfull set into train and validation set\nX_train, X_valid = X_trainfull[:-5_000], X_trainfull[-5_000:]\ny_train, y_valid = y_trainfull[:-5_000], y_trainfull[-5_000:]","eee877cb":"def preprocess_ResNet50_input(input_images):\n    input_images = input_images.astype('float32')\n    preprocessed_images = keras.applications.resnet50.preprocess_input(input_images)\n    return preprocessed_images","6ebd3e1b":"# Applying the preprocessing function\n\nX_train_processed = preprocess_ResNet50_input(X_train)\nX_valid_processed = preprocess_ResNet50_input(X_valid)\nX_test_processed = preprocess_ResNet50_input(X_test)","00c6a355":"# Building the model\n\ndef BuildModel():\n    \n    # Our input images have a 32,32,3 dimension\n    inputs = keras.layers.Input(shape=(32,32,3))\n    \n    # Next we need to upsample our 32 x 32 images to 224 x 224 using size (7,7).\n    resized = keras.layers.UpSampling2D(size=(7,7))(inputs)\n    \n    # Next we build our base model from the ReseNet50 pretrained networks and apply the resized input to it\n    base_model = keras.applications.resnet50.ResNet50(input_shape = [224,224,3],\n                                            include_top = False,\n                                            weights = 'imagenet')(resized)\n    \n    \n    # Next, we build our model using the functional API\n    avg = keras.layers.GlobalAveragePooling2D()(base_model)\n    Flatten = keras.layers.Flatten()(avg)\n    dense1 = keras.layers.Dense(1024, activation='relu', kernel_initializer='he_normal')(Flatten)\n    dense2 = keras.layers.Dense(512, activation='relu', kernel_initializer='he_normal')(dense1)\n    output = keras.layers.Dense(10, activation='softmax')(dense2)\n    model = keras.Model(inputs=[inputs], outputs=[output])\n    return model\n","0e20528a":"keras.backend.clear_session()\n\nnp.random.seed(42)\ntf.random.set_seed(42)\n\nmodel = BuildModel()\nmodel.summary()","69c2d8f0":"Checkpoint_cb = keras.callbacks.ModelCheckpoint(\"ResNet50_model.h5\", save_best_only=True)\n\ncallback = [Checkpoint_cb]","5d9ae9e5":"model.compile(loss='sparse_categorical_crossentropy', metrics = ['accuracy'], \n             optimizer = keras.optimizers.SGD())\n\nhistory = model.fit(X_train_processed, y_train, batch_size = 64, \n                    validation_data = (X_valid_processed, y_valid),\n                    epochs = 5, callbacks=callback)","3b82155b":"model = keras.models.load_model(\"ResNet50_model.h5\")\nmodel.evaluate(X_test_processed, y_test)","8aa475b9":"plt.figure(figsize=(16,6))\nplt.plot(history.history['loss'], 'r+-', label = \"Training loss\")\nplt.plot(history.history['val_loss'], 'bo-', label = \"Validation loss\")\nplt.title(\"Training vs Validation Loss (Transfer Learning)\", fontsize = 16)\nplt.legend(fontsize=14)\nplt.show()","ec756959":"plt.figure(figsize=(16,6))\nplt.plot(history.history['accuracy'], 'r+-', label = \"Training accuracy\")\nplt.plot(history.history['val_accuracy'], 'bo-', label = \"Validation accuracy\")\nplt.yticks(np.linspace(0.5,1.1,7))\nplt.title(\"Training vs Validation Accuracy (Transfer Learning)\", fontsize = 16)\nplt.legend(fontsize=14, loc = 'lower right')\nplt.show()","ad50be52":"pred_3 = model.predict(X_test_processed)\ny_pred_3 = np.argmax(pred_3, axis = 1)","b4cf71ca":"conf_matrix_3 = confusion_matrix(y_test, y_pred_3)\nconf_matrix_3","b8da3234":"# Plotting the confusion matrix using seaborns' heatmap\n\nplt.figure(figsize=(12,8))\nsns.heatmap(conf_matrix_3, cmap = 'PiYG', annot = True, fmt = '.0f', cbar = False,\n            annot_kws = {\n            'weight':'bold'},\n           xticklabels = label_names, yticklabels = label_names)\nplt.title(\"Confusion Matrix (Pretrained Layers)\", fontsize = 16)\nplt.show()","cc8acb84":"# Printing the classification report based on how well our model performed\n\nprint(classification_report(y_test, y_pred_3, target_names = label_names))","7c90a7cf":"plt.figure(figsize=(15,12))\nfor i in range (100):\n    plt.subplot(10, 10, i+1)\n    plt.imshow(X_test[i])\n    plt.title(f\"Actual: {np.array(label_names)[y_test[i]]}\\nPredicted: {np.array(label_names)[y_pred_3[i]]}\", fontsize = 8)\n    plt.axis(\"off\")\nplt.tight_layout()\nplt.show()","a082336e":"### 2.0 Image Augmentation\n\nData Augmentation artificially increases the size of the training set by generating variants of an image. This reduces the risk of overfitting by forcing the model to be tolerant to variations in the position, orientaion, and size of the objects in image. This can be done by flipping or shifting the image horizontally or vertically, adding more lighting or contrast, zoming in or out, cropping the image etc. \n\nBy combinning these transformations, we can significantly increase the size of our training set.","3cbae3fa":"#### Testing our Generator on a sample image","233be8ae":"Finding the Percentage of each category in the dataset","5181150e":"# $The$ $End.$ $Please$ $Upvote$","9f374e96":"In this notebook, we trained several models with all having at least 85% accuracy on the test set, which is not bad considering they are vanilla models. We also used a pretrained model (ResNet50) and achieved 95% accuracy on the test set. ","57bad82e":"## Splitting the data","c193fa22":"The CIFAR 10 and MNIST datasets are like the hello world of Deep Learning for people trying to get an understanding of Deep Neural Networks and Covolutional Neural Networks.\n\nThe MNIST dataset is the easier of the two because we have fewer low level features and 1 color channel (i.e white & black) rather than 3 color channels (Red.Green.Blue). For those reasons, it's easy to achieve high accuracy on the MNIST dataset than the CIFAR 10.\n\nIn this notebook, i will build 3 CNN models:\n* model 1: Without Image Augmentation\n* model 2: With Image Augmentation\n* model 3: Pretrained Layers (Transfer Learning)","b4d09307":"# Visualization\n### Loading 225 sample Images\nNext i am going to visualize 225 images from our data together with their respective class.","630d0d63":"#### Visualizing the Count of our target classes in our train set","47b2598d":"## Problem Definition\n\nCan we accurately predict the class an image belongs to?\n\nWe have 60,000 images of shape 32 x 32 with 3 color channels (R.G.B)\nThese images contain 10 classes: \n* Airplane\n* Automobile\n* Bird\n* Cat\n* Deer \n* Dog\n* Frog\n* Horse\n* Ship\n* Truck","48d0cf83":"Next, i'll compile and train my model on the preprocessed input images.\n\n#### NOTE: I tried different configurations by playing the number of Dense layers, neurons per layer, batch size, optimizer before settling for the options below because they gave me the best accuracy. You can try out several configurations, perhaps another pretrained network, and even reach 97% accuracy!!!\n","f2caa66c":"A Classification report is used to measure the quality of predictions from a classification algorithm. How many predictions are True and how many are False.","ea4008f5":"A confusion matrix is a table that is often used to describe the performance of a classification model (or \"classifier\") on a set of test data for which the true values are known.","fe486c4e":"## 3.0 Using a Pretrained model\n\nIn general, it is advisable to use pretrained networks for image classification tasks to save yourself the hassle of tuning paramters. \n\nKeras has a number of pretrained networks that can be available with a single line of code. These networks have been trained on the `imagenet` dataset and have won previous ILSVRC challenges.\n\nIn this section, i will be using the ResNet50 a variation of the ResNet152 that won the 2015 ILSVRC challenge.\n","0443ac8e":"Because we have 10 classes with values 0 ~ 9 representing each class, we have to convert our target values to categorical by using the implementation of One-Hot Encoding in keras.","a1dd5932":"### Scaling the data\n Our data ranges from 0 ~ 255 but Neural Networks work best when they scaled betweeen a range of 0 ~ 1. Therefore i will be rescaling our downloaded data by dividing our instances by a float `255.0`","00eb67cc":"First, i will preprocess the input image because most pretrained models were trained on 224 X 224 pixel images (and the model expects the same dimesion). Every pretrained network has a `preprocess_input() function` that can be used to preprocess images","aff34185":"# Building the Model\n\nBefore we build the model, why do we use use COnvolutional Networks for Image Problems instead of Deep fully connected Neural Networks?\n\nWell, deep neural networks with fully connected layers work well for small image problems like the MNIST dataset because of fewer low level features which means it will have less paramters to tune. For example, a 100 x 100 image has 10,000 pixels and if the first layer contains 500 neurons that means there will be 5,000,000 connections that have to be tweaked during training (especially backpropagation). This makes it incredibly difficult to obtain good accuracy on image classification tasks. \nConvolutional Neural Networks slove this problem by connecting each neuron to only pixels in a layers receptive field with the aid of filters (which pick up low level features which are then combined to form complex patterns in subsequent convolutional layers). CNNs also share weigths for each filter map, meaning there are significantly lesser parameters to tune.\n\nFinally, once a CNN recognizes a pattern\/feature at a location it can locate it in any other location. Whereas, DNNs can only recognize it on that same location.\n\n### 1.0 Without Image Augmentation","5fe96579":"# Introduction","63fda26b":"### Visualizing my model predictions","767d38c2":"#### Next i am test how well my model performs by plotting a confusion matrix based on its predictions and printing out the classification report.","08bba67e":"## Importing the Libraries"}}