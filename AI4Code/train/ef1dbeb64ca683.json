{"cell_type":{"949c3d3c":"code","6ccc7184":"code","d1eb885a":"code","2ed5d4d0":"code","0b32aa9f":"code","5d040c89":"code","7191dd0b":"code","f78ba208":"code","b7370fd4":"code","c6fb7271":"code","e08b6f42":"code","3ac6c8d8":"code","7e024b96":"code","1cc62a5e":"code","81691bba":"code","fd7b5455":"code","d4288051":"code","a8d5d3b0":"code","806f89ce":"code","d813518e":"code","82175c53":"code","643375c5":"code","7d542e06":"code","9e98ec36":"code","5083437b":"code","608d33bb":"code","e8ff3f7c":"markdown","3cecaafc":"markdown","cb5a4ed6":"markdown","26d5ae89":"markdown","b23f96bc":"markdown","2a1a972c":"markdown","cdf6f7a7":"markdown","bfc37987":"markdown","fac95c35":"markdown","01153614":"markdown","5ddf6e7b":"markdown","629d02f1":"markdown","f29ef256":"markdown","3ecfe2e4":"markdown","59540b19":"markdown","f8a29bad":"markdown"},"source":{"949c3d3c":"# To have reproducible results and compare them\nnr_seed = 2019\nimport numpy as np \nnp.random.seed(nr_seed)\nimport tensorflow as tf\ntf.set_random_seed(nr_seed)","6ccc7184":"# import libraries\nimport json\nimport math\nfrom tqdm import tqdm, tqdm_notebook\nimport gc\nimport warnings\nimport os\n\nimport cv2\nfrom PIL import Image\n\nimport pandas as pd\nimport scipy\nimport matplotlib.pyplot as plt\n\nfrom keras import backend as K\nfrom keras import layers\nfrom keras.applications.densenet import DenseNet121\nfrom keras.callbacks import Callback, ModelCheckpoint\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.models import Sequential\nfrom keras.optimizers import Adam\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import cohen_kappa_score, accuracy_score\n\nwarnings.filterwarnings(\"ignore\")\n\n%matplotlib inline","d1eb885a":"# Image size\nim_size = 320\n# Batch size\nBATCH_SIZE = 32","2ed5d4d0":"new_train = pd.read_csv('..\/input\/aptos2019-blindness-detection\/train.csv')\nold_train = pd.read_csv('..\/input\/diabetic-retinopathy-resized\/trainLabels.csv')\nprint(new_train.shape)\nprint(old_train.shape)","0b32aa9f":"old_train = old_train[['image','level']]\nold_train.columns = new_train.columns\nold_train.diagnosis.value_counts()\n\n# path columns\nnew_train['id_code'] = '..\/input\/aptos2019-blindness-detection\/train_images\/' + new_train['id_code'].astype(str) + '.png'\nold_train['id_code'] = '..\/input\/diabetic-retinopathy-resized\/resized_train\/resized_train\/' + old_train['id_code'].astype(str) + '.jpeg'\n\ntrain_df = old_train.copy()\nval_df = new_train.copy()\ntrain_df.head()","5d040c89":"# Not used in version 5\n#train_df, val_df = train_test_split(train_df, shuffle=True, stratify=train_df.diagnosis, test_size=0.1, random_state=2019)","7191dd0b":"# Let's shuffle the datasets\ntrain_df = train_df.sample(frac=1).reset_index(drop=True)\nval_df = val_df.sample(frac=1).reset_index(drop=True)\nprint(train_df.shape)\nprint(val_df.shape)","f78ba208":"def crop_image1(img,tol=7):\n    # img is image data\n    # tol  is tolerance\n        \n    mask = img>tol\n    return img[np.ix_(mask.any(1),mask.any(0))]\n\ndef crop_image_from_gray(img,tol=7):\n    if img.ndim ==2:\n        mask = img>tol\n        return img[np.ix_(mask.any(1),mask.any(0))]\n    elif img.ndim==3:\n        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n        mask = gray_img>tol\n        \n        check_shape = img[:,:,0][np.ix_(mask.any(1),mask.any(0))].shape[0]\n        if (check_shape == 0): # image is too dark so that we crop out everything,\n            return img # return original image\n        else:\n            img1=img[:,:,0][np.ix_(mask.any(1),mask.any(0))]\n            img2=img[:,:,1][np.ix_(mask.any(1),mask.any(0))]\n            img3=img[:,:,2][np.ix_(mask.any(1),mask.any(0))]\n            img = np.stack([img1,img2,img3],axis=-1)\n        return img\n\ndef preprocess_image(image_path, desired_size=224):\n    img = cv2.imread(image_path)\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    img = crop_image_from_gray(img)\n    img = cv2.resize(img, (desired_size,desired_size))\n    img = cv2.addWeighted(img,4,cv2.GaussianBlur(img, (0,0), desired_size\/30) ,-4 ,128)\n    \n    return img\n\ndef preprocess_image_old(image_path, desired_size=224):\n    img = cv2.imread(image_path)\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    #img = crop_image_from_gray(img)\n    img = cv2.resize(img, (desired_size,desired_size))\n    img = cv2.addWeighted(img,4,cv2.GaussianBlur(img, (0,0), desired_size\/40) ,-4 ,128)\n    \n    return img","b7370fd4":"def display_samples(df, columns=4, rows=3):\n    fig=plt.figure(figsize=(5*columns, 4*rows))\n\n    for i in range(columns*rows):\n        image_path = df.loc[i,'id_code']\n        image_id = df.loc[i,'diagnosis']\n        img = cv2.imread(f'{image_path}')\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        #img = crop_image_from_gray(img)\n        img = cv2.resize(img, (im_size,im_size))\n        img = cv2.addWeighted(img,4,cv2.GaussianBlur(img, (0,0), im_size\/40) ,-4 ,128)\n        \n        fig.add_subplot(rows, columns, i+1)\n        plt.title(image_id)\n        plt.imshow(img)\n    \n    plt.tight_layout()\n\ndisplay_samples(train_df)","c6fb7271":"# validation set\nN = val_df.shape[0]\nx_val = np.empty((N, im_size, im_size, 3), dtype=np.uint8)\n\nfor i, image_id in enumerate(tqdm_notebook(val_df['id_code'])):\n    x_val[i, :, :, :] = preprocess_image(\n        f'{image_id}',\n        desired_size = im_size\n    )","e08b6f42":"y_train = pd.get_dummies(train_df['diagnosis']).values\ny_val = pd.get_dummies(val_df['diagnosis']).values\n\nprint(y_train.shape)\nprint(x_val.shape)\nprint(y_val.shape)","3ac6c8d8":"y_train_multi = np.empty(y_train.shape, dtype=y_train.dtype)\ny_train_multi[:, 4] = y_train[:, 4]\n\nfor i in range(3, -1, -1):\n    y_train_multi[:, i] = np.logical_or(y_train[:, i], y_train_multi[:, i+1])\n\ny_val_multi = np.empty(y_val.shape, dtype=y_val.dtype)\ny_val_multi[:, 4] = y_val[:, 4]\n\nfor i in range(3, -1, -1):\n    y_val_multi[:, i] = np.logical_or(y_val[:, i], y_val_multi[:, i+1])\n\nprint(\"Y_train multi: {}\".format(y_train_multi.shape))\nprint(\"Y_val multi: {}\".format(y_val_multi.shape))","7e024b96":"y_train = y_train_multi\ny_val = y_val_multi","1cc62a5e":"# delete the uneeded df\ndel new_train\ndel old_train\ndel val_df\ngc.collect()","81691bba":"class Metrics(Callback):\n\n    def on_epoch_end(self, epoch, logs={}):\n        X_val, y_val = self.validation_data[:2]\n        y_val = y_val.sum(axis=1) - 1\n        \n        y_pred = self.model.predict(X_val) > 0.5\n        y_pred = y_pred.astype(int).sum(axis=1) - 1\n\n        _val_kappa = cohen_kappa_score(\n            y_val,\n            y_pred, \n            weights='quadratic'\n        )\n\n        self.val_kappas.append(_val_kappa)\n\n        print(f\"val_kappa: {_val_kappa:.4f}\")\n        \n        if _val_kappa == max(self.val_kappas):\n            print(\"Validation Kappa has improved. Saving model.\")\n            self.model.save('model.h5')\n\n        return","fd7b5455":"def create_datagen():\n    return ImageDataGenerator(\n        featurewise_std_normalization = True,\n        horizontal_flip = True,\n        vertical_flip = True,\n        rotation_range = 360\n    )","d4288051":"densenet = DenseNet121(\n    weights='..\/input\/densenet-keras\/DenseNet-BC-121-32-no-top.h5',\n    include_top=False,\n    input_shape=(im_size,im_size,3)\n)","a8d5d3b0":"def build_model():\n    model = Sequential()\n    model.add(densenet)\n    model.add(layers.GlobalAveragePooling2D())\n    model.add(layers.Dropout(0.5))\n    model.add(layers.Dense(5, activation='sigmoid'))\n    \n    model.compile(\n        loss='binary_crossentropy',\n        optimizer=Adam(lr=0.0001,decay=1e-6),\n        metrics=['accuracy']\n    )\n    \n    return model","806f89ce":"model = build_model()\nmodel.summary()","d813518e":"#train_df = train_df.reset_index(drop=True)\nbucket_num = 8\ndiv = round(train_df.shape[0]\/bucket_num)","82175c53":"df_init = {\n    'val_loss': [0.0],\n    'val_acc': [0.0],\n    'loss': [0.0], \n    'acc': [0.0],\n    'bucket': [0.0]\n}\nresults = pd.DataFrame(df_init)","643375c5":"# I found that changing the nr. of epochs for each bucket helped in terms of performances\nepochs = [5,5,10,15,15,20,20,30]\nkappa_metrics = Metrics()\nkappa_metrics.val_kappas = []","7d542e06":"for i in range(0,bucket_num):\n    if i != (bucket_num-1):\n        print(\"Bucket Nr: {}\".format(i))\n        \n        N = train_df.iloc[i*div:(1+i)*div].shape[0]\n        x_train = np.empty((N, im_size, im_size, 3), dtype=np.uint8)\n        for j, image_id in enumerate(tqdm_notebook(train_df.iloc[i*div:(1+i)*div,0])):\n            x_train[j, :, :, :] = preprocess_image_old(f'{image_id}', desired_size = im_size)\n\n        data_generator = create_datagen().flow(x_train, y_train[i*div:(1+i)*div,:], batch_size=BATCH_SIZE)\n        history = model.fit_generator(\n                        data_generator,\n                        steps_per_epoch=x_train.shape[0] \/ BATCH_SIZE,\n                        epochs=epochs[i],\n                        validation_data=(x_val, y_val),\n                        callbacks=[kappa_metrics]\n                        )\n        \n        dic = history.history\n        df_model = pd.DataFrame(dic)\n        df_model['bucket'] = i\n    else:\n        print(\"Bucket Nr: {}\".format(i))\n        \n        N = train_df.iloc[i*div:].shape[0]\n        x_train = np.empty((N, im_size, im_size, 3), dtype=np.uint8)\n        for j, image_id in enumerate(tqdm_notebook(train_df.iloc[i*div:,0])):\n            x_train[j, :, :, :] = preprocess_image_old(f'{image_id}', desired_size = im_size)\n        data_generator = create_datagen().flow(x_train, y_train[i*div:,:], batch_size=BATCH_SIZE)\n        \n        history = model.fit_generator(\n                        data_generator,\n                        steps_per_epoch=x_train.shape[0] \/ BATCH_SIZE,\n                        epochs=epochs[i],\n                        validation_data=(x_val, y_val),\n                        callbacks=[kappa_metrics]\n                        )\n        \n        dic = history.history\n        df_model = pd.DataFrame(dic)\n        df_model['bucket'] = i\n\n    results = results.append(df_model)\n    \n    del data_generator\n    del x_train\n    gc.collect()\n    \n    print('-'*40)","9e98ec36":"results = results.iloc[1:]\nresults['kappa'] = kappa_metrics.val_kappas\nresults = results.reset_index()\nresults = results.rename(index=str, columns={\"index\": \"epoch\"})\nresults","5083437b":"results[['loss', 'val_loss']].plot()\nresults[['acc', 'val_acc']].plot()\nresults[['kappa']].plot()\nresults.to_csv('model_results.csv',index=False)","608d33bb":"model.load_weights('model.h5')\ny_val_pred = model.predict(x_val)\n\ndef compute_score_inv(threshold):\n    y1 = y_val_pred > threshold\n    y1 = y1.astype(int).sum(axis=1) - 1\n    y2 = y_val.sum(axis=1) - 1\n    score = cohen_kappa_score(y1, y2, weights='quadratic')\n    \n    return 1 - score\n\nsimplex = scipy.optimize.minimize(\n    compute_score_inv, 0.5, method='nelder-mead'\n)\n\nbest_threshold = simplex['x'][0]\n\ny1 = y_val_pred > best_threshold\ny1 = y1.astype(int).sum(axis=1) - 1\ny2 = y_val.sum(axis=1) - 1\nscore = cohen_kappa_score(y1, y2, weights='quadratic')\nprint('Threshold: {}'.format(best_threshold))\nprint('Validation QWK score with best_threshold: {}'.format(score))\n\ny1 = y_val_pred > .5\ny1 = y1.astype(int).sum(axis=1) - 1\nscore = cohen_kappa_score(y1, y2, weights='quadratic')\nprint('Validation QWK score with .5 threshold: {}'.format(score))","e8ff3f7c":"# Model: DenseNet-121","3cecaafc":"## Train - Valid split","cb5a4ed6":"# Loading & Merging","26d5ae89":"# Creating keras callback for QWK\n\n---\n\nI had to change this function, in order to consider the best kappa score among all the buckets.","b23f96bc":"# Find best threshold","2a1a972c":"# Processing Images","cdf6f7a7":"### Credits\nI started this kernel by forking [APTOS 2019: DenseNet Keras Starter](https:\/\/www.kaggle.com\/xhlulu\/aptos-2019-densenet-keras-starter), by [Xhlulu](https:\/\/www.kaggle.com\/xhlulu).\n\nI also used [previous competition's data](https:\/\/www.kaggle.com\/tanlikesmath\/diabetic-retinopathy-resized) uploaded by [ilovescience](https:\/\/www.kaggle.com\/tanlikesmath).\n\nThank you both guys!","bfc37987":"Crop function: https:\/\/www.kaggle.com\/ratthachat\/aptos-updated-preprocessing-ben-s-cropping ","fac95c35":"### Process Images","01153614":"__UPDATE:__ Here we are reading just the validation set. In order to use 320x320 images, we are going to load one bucket at a time only when needed. This will let our code run without memory-related errors.","5ddf6e7b":"# Creating multilabels\n\nInstead of predicting a single label, we will change our target to be a multilabel problem; i.e., if the target is a certain class, then it encompasses all the classes before it. E.g. encoding a class 4 retinopathy would usually be `[0, 0, 0, 1]`, but in our case we will predict `[1, 1, 1, 1]`. For more details, please check out [Lex's kernel](https:\/\/www.kaggle.com\/lextoumbourou\/blindness-detection-resnet34-ordinal-targets).","629d02f1":"# Training & Evaluation","f29ef256":"## [Inference Kernel](https:\/\/www.kaggle.com\/raimonds1993\/aptos19-densenet-inference-old-new-data\/data?scriptVersionId=17252732)\n\n**Thanks for reading it all! Please let me know if you have any ideas to improve this process.**\n\n**Hope you liked it.**","3ecfe2e4":"# DenseNet Trained with Old and New Data\n\n---\n\nDue to the size of previous competition's data, I faced few memory-related problems while training my model. In this kernel I would like to show the approach I used.\n\nI basically splitted the training set in buckets and trained the model for each bucket.\n\nI'd truly interested to further discuss how it could has been solved. So, if you faced the same issues, please comment with your ideas :)\n\n#### Here you can find the [Inference Kernel](https:\/\/www.kaggle.com\/raimonds1993\/aptos19-densenet-inference-old-new-data\/data?scriptVersionId=17252732)!\n\n---","59540b19":"### Changes\n\n*Version 3:*\n- This is the first completed version to consider. (Still without seed)\n- Inference -> LB: 0.719\n\n*Version 4:*\n- Updated image size (320). In order to process the whole dataset, I load just one bucket at a time and trained the model on that.\n- Added seed to better evaluate the results.\n\n*Version 5:*\n- Changed train - val split: Now let's take previous comp data as train and the new comp data as validation\n\n*Version 9:*\n- Changed preprosessing filter in old (train) data\n- Changed Imagegenerator , more augmentation\n\n*Version 10:*\n- No Cropping in old comp data","f8a29bad":"# Data Generator"}}