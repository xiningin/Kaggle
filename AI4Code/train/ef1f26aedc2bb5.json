{"cell_type":{"8a13e8b3":"code","3db6fb2a":"code","943299d5":"code","c435a5f7":"code","231e0fef":"code","2fe9610f":"code","01d6067c":"code","8603ba9a":"code","c9638382":"code","c34852fc":"code","68eb2961":"code","1cfd0e86":"code","329e1d32":"code","ef603542":"code","428b2185":"code","c7ec92aa":"code","9f99b335":"code","2c8d4075":"code","19917a45":"code","34606ee0":"code","6e74dbbe":"code","9be77eb8":"code","e51e31f7":"code","8a83bcbe":"code","63ea0998":"code","ae306d26":"code","317010cf":"code","43305326":"code","33924b12":"code","d5ee6a4d":"code","32e14224":"code","e7555b9f":"code","df420fa2":"code","411ffbc7":"code","39dac942":"code","8cf5d596":"code","4b3a69a3":"code","2ab754f5":"code","37a40e1d":"code","d0dc9003":"code","9fbeff44":"code","72fc4d44":"code","db489fd0":"code","6573c5e2":"code","da1172a7":"code","8d380112":"code","ef9048a8":"code","e0c0329c":"code","56198024":"code","d36e3b00":"code","b2eaddf5":"code","bd64ee35":"code","f0906852":"code","003bfe82":"code","b0e1d4a2":"code","c8a72a46":"code","13f66a13":"code","2c4e0edb":"code","d40906ea":"code","ac805773":"code","775c81f7":"code","3cb45946":"code","0d32a857":"code","413ddde6":"code","72525f94":"code","6bc54ef2":"code","c7c4cd77":"code","f744078d":"code","a669928f":"code","c7ce7ebb":"code","02b2bf65":"code","763b2513":"code","956042d8":"code","94f94328":"code","6710c603":"code","aa5b86bb":"code","7be91598":"code","c5a83ddd":"code","27ac3eb9":"code","ce8c9525":"code","e974c0b5":"code","cae558f2":"code","7e3b60fc":"code","7eb9e594":"code","876c7867":"markdown","2db99536":"markdown","5e9353e4":"markdown","98c75e2f":"markdown","eb345163":"markdown","9e7458ca":"markdown","38c5581a":"markdown","7ab2b18d":"markdown","eb73c9ab":"markdown","c3d0456c":"markdown","46d285d8":"markdown","2182b19f":"markdown","806337d0":"markdown","97755c34":"markdown","6e44da5e":"markdown","ee98bb42":"markdown","16361ee0":"markdown","63843924":"markdown","f2610bad":"markdown","1ea56a90":"markdown","5d6f516d":"markdown","0adb2cd0":"markdown","67ca7129":"markdown","564ddad2":"markdown","905e65d2":"markdown","cd212c32":"markdown","f44fe66d":"markdown","f8c02d47":"markdown","b09e78fc":"markdown","b692aab3":"markdown","62ccc345":"markdown","35372f19":"markdown","a66a9584":"markdown","385fb835":"markdown","a250e0a1":"markdown","61f15758":"markdown","d00ec0f3":"markdown","1b77921c":"markdown","652dc088":"markdown","fdc45fe7":"markdown","dd5775db":"markdown","aabe4a4e":"markdown","c34d22eb":"markdown","b44de740":"markdown","2530ede1":"markdown","7699a142":"markdown","262ce709":"markdown","fba27538":"markdown","767e5478":"markdown","d9e111ef":"markdown","3485ba75":"markdown","f123b924":"markdown","34454917":"markdown","94c45211":"markdown","dc7ae6f7":"markdown","8ef3c993":"markdown","c41b828b":"markdown","9d76ba54":"markdown","ab2ac4cf":"markdown","36dc52ff":"markdown"},"source":{"8a13e8b3":"# General imports\nimport pandas as pd\nimport seaborn as sns\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport random\n\nimport warnings \nwarnings.filterwarnings('ignore')\n\n# Modules used for assessing the performance of the model\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import  confusion_matrix\n\n\n# Dimensionality reduction modules\nfrom sklearn.decomposition import PCA\nfrom sklearn.manifold import TSNE\n\n# Model Training\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neural_network import MLPClassifier\n\n# Scaling and Sampling the data\nfrom sklearn.preprocessing import StandardScaler\nfrom imblearn.over_sampling import SMOTE\n\n# Graph Visualization\nfrom sklearn.tree import export_graphviz\n\n# Unsupervised learning model\nfrom sklearn.cluster import KMeans\nfrom scipy.stats import mode","3db6fb2a":"# # Mounting the drive to enable the file import\n# from google.colab import drive\n# drive.mount('\/content\/drive')","943299d5":"Data = pd.read_csv('..\/input\/falldeteciton.csv')\nData.head()","c435a5f7":"print(sorted(Data['ACTIVITY'].unique()))\nprint('Here 0 indicates Standing, 1- Walking, 2- Sitting, 3 - Falling, 4 - Cramps, 5 - Running')","231e0fef":"Data.describe()","2fe9610f":"features = Data.iloc[:,1:]\nsns.pairplot(features) \nplt.show()","01d6067c":"single_dimension_pca = PCA(n_components=1)\nsingle_dimention_data = single_dimension_pca.fit_transform(features.T)","8603ba9a":"print(single_dimension_pca.explained_variance_ratio_)\nprint(single_dimension_pca.mean_)","c9638382":"cols = list(features.columns)","c34852fc":"plt.figure(figsize=(10,5))\ny_axis_zeros = np.zeros(len(single_dimention_data))\n\n#plt.scatter(single_dimention_data,y_axis_zeros)\nfor i in range(len(single_dimention_data)):\n  plt.scatter(single_dimention_data[i],y_axis_zeros[i])\n  plt.annotate(cols[i],(single_dimention_data[i],y_axis_zeros[i]),rotation=20)\n\nplt.title('PCA Analysis single dimension')\nplt.show()","68eb2961":"b = cols.index('BP')\nh = cols.index('HR')","1cfd0e86":"single_dimention_data[b],single_dimention_data[h]","329e1d32":"sns.pairplot(Data,x_vars=cols,y_vars='ACTIVITY')\nplt.show()","ef603542":"Data[Data['ACTIVITY']==1].head()","428b2185":"Data[Data['ACTIVITY']==3].head()","c7ec92aa":"for i in cols:\n  sns.boxplot(features[i])\n  plt.show()","9f99b335":"Data.isna().sum()","2c8d4075":"original_data = Data.copy()","19917a45":"initial = len(Data)\nfor i in cols:\n  IQR = Data[i].quantile(0.75) - Data[i].quantile(0.25)\n  Data = Data[(Data[i]< (Data[i].quantile(0.75)+IQR)) & (Data[i] > (Data[i].quantile(0.25)-IQR))]\n  \nfinal = len(Data)\nprint('Number of outliers removed',initial-final)","34606ee0":"# After removing the outliers checking the distribution of each feature in the Dataset\n\nfor i in cols:\n  sns.boxplot(Data[i])\n  plt.show()","6e74dbbe":"Feature_clean_data = Data.iloc[:,1:]\nTarget_label_activity = Data.iloc[:,0]","9be77eb8":"X_train, X_test, y_train, y_test = train_test_split(Feature_clean_data, Target_label_activity, test_size=0.25)","e51e31f7":"log_regress = LogisticRegression(solver='lbfgs',multi_class='multinomial',max_iter=10500)\nlog_regress.fit(X_train,y_train)\ny_pred = log_regress.predict(X_test)\n\n\naccuracy_score(y_test,y_pred)\n","8a83bcbe":"plt.scatter(X_test.iloc[:,0],X_test.iloc[:,1],c=y_pred)\nplt.show()","63ea0998":"# We also need to check if there is a correlation between the features \nData.corr()","ae306d26":"log_regress = LogisticRegression(solver='lbfgs',multi_class='multinomial',max_iter=10500)\nlog_regress.fit(X_train[['CIRCLUATION','EEG']],y_train)\ny_pred = log_regress.predict(X_test[['CIRCLUATION','EEG']])\n\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import recall_score\n\naccuracy_score(y_test,y_pred)","317010cf":"sc = StandardScaler()\nX_train_scaled = sc.fit_transform(X_train)\nX_test_scaled = sc.transform(X_test)\n\nlog_regress = LogisticRegression(solver='lbfgs',multi_class='multinomial',max_iter=10500)\nlog_regress.fit(X_train_scaled,y_train)\ny_pred = log_regress.predict(X_test_scaled)\n\n\nlog_accuracy = round(accuracy_score(y_test,y_pred),3)\nprint(log_accuracy)","43305326":"comparing_models = {}\ncomparing_models['LogisticRegression'] = log_accuracy","33924b12":"model = GaussianNB()\n\n\nmodel.fit(X_train_scaled,y_train)\ny_pred = model.predict(X_test_scaled)\n\n\nprint('Gaussian Naive Bayes')\nprint('Accuracy-score'.ljust(15),'Recall-score'.ljust(15),'F1 score'.ljust(15))\nprint(str(round(accuracy_score(y_test,y_pred),3)).ljust(15),str(round(recall_score(y_test,y_pred,average='weighted'),3)).ljust(15),\n      str(round(f1_score(y_test,y_pred,average='weighted'),3)).ljust(15))\n\nnb_accuracy = round(accuracy_score(y_test,y_pred),3)\n\ncomparing_models['Gaussian Naive Bayes'] = nb_accuracy","d5ee6a4d":"k = range(1,51)\nbest_k = None\nbest_score = -np.inf\nbest_recall = -np.inf","32e14224":"for n in k:\n  knn = KNeighborsClassifier(n)\n  knn.fit(X_train_scaled,y_train)\n  y_pred = knn.predict(X_test_scaled)\n  \n  score = accuracy_score(y_test,y_pred)\n  recall = recall_score(y_test,y_pred,average='weighted')\n  \n  if (score>best_score) | (recall > best_recall):\n    best_score=score\n    best_recall = recall\n    best_k=n\n    \nprint('Best k'.ljust(10),'Accuracy'.ljust(10),'Recall'.ljust(10))\nprint(str(best_k).ljust(10),str(round(best_score,3)).ljust(10),str(round(best_recall,3)).ljust(10))\n\nknn_accuracy = round(best_score,3)\n\nknn=KNeighborsClassifier(best_k)\nknn.fit(X_train_scaled,y_train)\ny_pred = knn.predict(X_test_scaled)\n  \nknn_mat = confusion_matrix(y_test,y_pred)","e7555b9f":"comparing_models['KNN'] = knn_accuracy","df420fa2":"rfc = RandomForestClassifier(random_state=0)\nparam_grid = {'n_estimators':[50,100,150,200],'max_depth':[50,100,150,None]}\ngrid = GridSearchCV(rfc,param_grid=param_grid)","411ffbc7":"grid.fit(X_train_scaled,y_train)","39dac942":"random_forest = grid.best_estimator_\nrandom_forest","8cf5d596":"random_forest.fit(X_train_scaled,y_train)\ny_pred = random_forest.predict(X_test_scaled)\n\nprint('Random Forest Classifier')\nprint('Accuracy-score'.ljust(15),'Recall-score'.ljust(15),'F1 score'.ljust(15))\nprint(str(round(accuracy_score(y_test,y_pred),3)).ljust(15),str(round(recall_score(y_test,y_pred,average='weighted'),3)).ljust(15),\n      str(round(f1_score(y_test,y_pred,average='weighted'),3)).ljust(15))\n\nrf_accuracy = round(accuracy_score(y_test,y_pred),3)","4b3a69a3":"comparing_models['Random Forest Classifier'] = rf_accuracy","2ab754f5":"from sklearn.metrics import  confusion_matrix\nrf_mat = confusion_matrix(y_test,y_pred)\nrf_mat = np.round(rf_mat,1)\nsns.heatmap(rf_mat,annot=True)\nplt.title('Confusion Matrix')\nplt.show()","37a40e1d":"rows  = list(Feature_clean_data.columns)\nplt.bar(rows,random_forest.feature_importances_)\nplt.xlabel('Features')#,color='white')\nplt.ylabel('Importance')#,color='white')\n# plt.xticks(color='white')\n# plt.yticks(color='white')\nplt.title('Feature Importance plot')\nplt.show()","d0dc9003":"mlp_classifier = MLPClassifier()","9fbeff44":"param_grid = {'activation':['relu','logistic','tanh'],'hidden_layer_sizes':[(50,),(100,),(150,)],'learning_rate_init':[0.01,0.1]}","72fc4d44":"grid = GridSearchCV(mlp_classifier,param_grid)\ngrid.fit(X_train_scaled,y_train)","db489fd0":"best_mlp_classifier = grid.best_estimator_","6573c5e2":"best_mlp_classifier","da1172a7":"best_mlp_classifier.fit(X_train_scaled,y_train)\ny_pred = best_mlp_classifier.predict(X_test_scaled)\n\nprint('Multi Layer Perceptron Classifier')\nprint('Accuracy-score'.ljust(15),'Recall-score'.ljust(15),'F1 score'.ljust(15))\nprint(str(round(accuracy_score(y_test,y_pred),3)).ljust(15),str(round(recall_score(y_test,y_pred,average='weighted'),3)).ljust(15),\n      str(round(f1_score(y_test,y_pred,average='weighted'),3)).ljust(15))\n\nmlp_accuracy = round(accuracy_score(y_test,y_pred),3)\nmlp_mat = confusion_matrix(y_test,y_pred)","8d380112":"comparing_models['MLP classifier'] = mlp_accuracy","ef9048a8":"dtc = DecisionTreeClassifier()\ndtc.fit(X_train_scaled,y_train)\ny_pred = dtc.predict(X_test_scaled)\ndt_accuracy = accuracy_score(y_test, y_pred)\n\nprint('Decision Tree Classifier')\nprint('Accuracy-score'.ljust(15),'Recall-score'.ljust(15),'F1 score'.ljust(15))\nprint(str(round(accuracy_score(y_test,y_pred),3)).ljust(15),str(round(recall_score(y_test,y_pred,average='weighted'),3)).ljust(15),\n      str(round(f1_score(y_test,y_pred,average='weighted'),3)).ljust(15))\n\ndt_accuracy = round(accuracy_score(y_test,y_pred),3)\ndecision_tree_mat = confusion_matrix(y_test,y_pred)","e0c0329c":"comparing_models['Decision Tree Classifier'] = dt_accuracy","56198024":"svc = SVC(gamma='auto')\nsvc.fit(X_train_scaled,y_train)\n\ny_pred = svc.predict(X_test_scaled)\n\n\nprint('SVM Classifier')\nprint('Accuracy-score'.ljust(15),'Recall-score'.ljust(15),'F1 score'.ljust(15))\nprint(str(round(accuracy_score(y_test,y_pred),3)).ljust(15),str(round(recall_score(y_test,y_pred,average='weighted'),3)).ljust(15),\n      str(round(f1_score(y_test,y_pred,average='weighted'),3)).ljust(15))\n\nsvm_accuracy = round(accuracy_score(y_test,y_pred),3)","d36e3b00":"comparing_models['SVM classifier'] = svm_accuracy","b2eaddf5":"plt.bar(comparing_models.keys(),comparing_models.values())\nplt.xticks(rotation=75)\nplt.ylabel('Accuracy')\nplt.xlabel('Model')\nplt.title('Comparision across models')\nplt.show()","bd64ee35":"y_train.value_counts() # To check how the data is balanced with the training dataset","f0906852":"sm = SMOTE()\nX_train_fin, y_train_fin = sm.fit_sample(X_train_scaled,y_train.ravel())","003bfe82":"param_grid = {'n_estimators':[50,100,150,200],'max_depth':[50,100,150,None]}\ngrid = GridSearchCV(rfc,param_grid=param_grid)\ngrid.fit(X_train_fin,y_train_fin)\nnew_random_forest = grid.best_estimator_","b0e1d4a2":"new_random_forest.fit(X_train_fin,y_train_fin)\nY_pred = new_random_forest.predict(X_test_scaled)","c8a72a46":"print('Random Forest after SMOTE')\nprint('Accuracy'.ljust(10),'Recall'.ljust(10))\nprint(str(round(accuracy_score(y_test,Y_pred),3)).ljust(10),str(round(recall_score(y_test,Y_pred,average='micro'),3)).ljust(10))","13f66a13":"knn = KNeighborsClassifier(best_k)\nknn.fit(X_train_fin,y_train_fin)\nypred = knn.predict(X_test_scaled)","2c4e0edb":"print('KNN after SMOTE')\nprint('Accuracy'.ljust(10),'Recall'.ljust(10))\nprint(str(round(accuracy_score(y_test,ypred),3)).ljust(10),str(round(recall_score(y_test,ypred,average='micro'),3)).ljust(10))","d40906ea":"print(Data['ACTIVITY'].value_counts())\nx_row = Data['ACTIVITY'].value_counts().index\ny_row = Data['ACTIVITY'].value_counts().values\nplt.bar(x_row,y_row)\nplt.xlabel('Activity')\nplt.ylabel('Amount of Data')\nplt.show()","ac805773":"sm = SMOTE()\nsampled_X, sampled_Y = sm.fit_sample(Feature_clean_data,Target_label_activity)\nsampled_X_train, sampled_X_test, sampled_Y_train, sampled_Y_test = train_test_split(sampled_X, sampled_Y, test_size=0.3, random_state=0)","775c81f7":"random_forest","3cb45946":"rf = RandomForestClassifier(max_depth=50,n_estimators=200) # using the same max_depth and n_estimators as the random_forest classifier before SMOTE analysis","0d32a857":"rf.fit(sampled_X_train,sampled_Y_train)","413ddde6":"Y_pred = rf.predict(sampled_X_test)","72525f94":"print('Random forest after over-sampling the entire dataset')\nprint('Accuracy'.ljust(10),'Recall'.ljust(10),'F1-score'.ljust(10))\nprint(str(round(accuracy_score(sampled_Y_test,Y_pred),3)).ljust(10),str(round(recall_score(sampled_Y_test,Y_pred,average='weighted'),3)).ljust(10),\n     str(round(f1_score(sampled_Y_test,Y_pred,average='weighted'),3)).ljust(10))","6bc54ef2":"from sklearn.metrics import  confusion_matrix\nmat = confusion_matrix(sampled_Y_test,Y_pred)\nmat = np.round(mat,1)\nresult_mat = mat.T\nsns.heatmap(result_mat,annot=True)\nplt.xlabel('True label')\nplt.ylabel('Predicted label')\nplt.show()","c7c4cd77":"rows = list(Feature_clean_data.columns)\nplt.bar(rows,random_forest.feature_importances_)\nplt.xlabel('Features')#,color='white')\nplt.ylabel('Importance')#,color='white')\n# plt.xticks(color='white')\n# plt.yticks(color='white')\nplt.title('Feature Importance plot')\nplt.show()","f744078d":"rf_visualization = RandomForestClassifier(max_depth=5,n_estimators=200)\nrf_visualization.fit(sampled_X_train,sampled_Y_train)\nY_pred = rf_visualization.predict(sampled_X_test)\nprint('Random forest for visualization')\nprint('Accuracy'.ljust(10),'Recall'.ljust(10))\nprint(str(round(accuracy_score(sampled_Y_test,Y_pred),3)).ljust(10),str(round(recall_score(sampled_Y_test,Y_pred,average='weighted'),3)).ljust(10))","a669928f":"r = random.randint(0,len(rf_visualization.estimators_))","c7ce7ebb":"estim = rf_visualization.estimators_[r]\nexport_graphviz(estim,out_file='tree.dot',feature_names=list(Feature_clean_data.columns),class_names=['Standing','Walking','Sitting','Falling','Cramps','Running'],\n               rounded=True,precision=True,filled=True,proportion=False)","02b2bf65":"from subprocess import call\ncall(['dot', '-Tpng', 'tree.dot', '-o', 'tree.png', '-Gdpi=50'])\nfrom IPython.display import Image\nImage(filename = 'tree.png')","763b2513":"tsne = TSNE(n_components=2,random_state=0,perplexity=200,learning_rate=400,angle=0.99)\nprojected = tsne.fit_transform(Feature_clean_data)\n\nnum_clusters = len(Target_label_activity.unique())\n\nkmeans = KMeans(n_clusters=num_clusters)\nclusters = kmeans.fit_predict(projected)","956042d8":"plt.scatter(projected[:,0],projected[:,1],c=kmeans.labels_,cmap=plt.cm.get_cmap('Spectral', 10),alpha=0.5)\nplt.xlabel('component 1')\nplt.ylabel('component 2')\nplt.colorbar();\n\nlabels = np.zeros_like(clusters)\nfor i in range(10):\n    mask = (clusters == i)\n    labels[mask] = mode(Target_label_activity[mask])[0]\n    \naccuracy_score(Target_label_activity, labels)","94f94328":"print(result_mat)","6710c603":"no_false_positives = sum(result_mat[3,:]) - result_mat[3,3]\nno_false_negative = sum(result_mat[:,3]) - result_mat[3,3]\n\nprint('Number of False positives with random forest classifer is',no_false_positives)\nprint('Number of False negatives with random forest classifer is',no_false_negative)\n\nprint('Ratio of false negatives',no_false_negative\/np.sum(result_mat))\nprint('Ratio of false positives',no_false_positives\/np.sum(result_mat))","aa5b86bb":"print('Ratio of False positive to False negatives')","7be91598":"new_labels = [1 if i==3 else 0 for i in Target_label_activity]","c5a83ddd":"sm = SMOTE()\nsampled_X, sampled_Y = sm.fit_sample(Feature_clean_data,new_labels)\nsampled_X_train, sampled_X_test, sampled_Y_train, sampled_Y_test = train_test_split(sampled_X, sampled_Y, test_size=0.3, random_state=0)","27ac3eb9":"threshold = 0.4 \n\nrfc = RandomForestClassifier(random_state=0)\nparam_grid = {'n_estimators':[50,100,150,200],'max_depth':[50,100,150,None]}\ngrid = GridSearchCV(rfc,param_grid=param_grid,scoring='f1')\n\ngrid.fit(sampled_X_train,sampled_Y_train)\n\nrandom_forest = grid.best_estimator_\n\n\nrandom_forest.fit(sampled_X_train,sampled_Y_train)\npredicted_proba = random_forest.predict_proba(sampled_X_test)\n\npredicted = (predicted_proba [:,1] >= threshold).astype('int')\n\n\nprint('Random Forest Classifier')\nprint('Accuracy-score'.ljust(15),'Recall-score'.ljust(15),'F1 score'.ljust(15))\nprint(str(round(accuracy_score(sampled_Y_test,predicted),3)).ljust(15),str(round(recall_score(sampled_Y_test,predicted,average='weighted'),3)).ljust(15),\n      str(round(f1_score(sampled_Y_test,predicted,average='weighted'),3)).ljust(15))\n\nrf_accuracy = round(accuracy_score(sampled_Y_test,predicted),3)","ce8c9525":"random_forest","e974c0b5":"new_matrix = confusion_matrix(sampled_Y_test,predicted)","cae558f2":"sns.heatmap(new_matrix,annot=True)\nplt.xlabel('True labels')\nplt.ylabel('Predicted labels')\nplt.show()\nresult_new_matrix = new_matrix.T\nresult_new_matrix","7e3b60fc":"neg_false = result_new_matrix[0,1]\npos_false = result_new_matrix[1,0]","7eb9e594":"print('Ratio of false negatives',neg_false\/np.sum(new_matrix))\nprint('Ratio of false positives',pos_false\/np.sum(new_matrix))","876c7867":"As you can see, the data in each class is imbalanced, an oversampling of data would probably help us in resolving the issue of imbalancing.","2db99536":"The two features from the dataset i.e. EEG and CIRCLUATION give results almost as good as the entire model. ","5e9353e4":"**Splitting the Data into Train and Test**","98c75e2f":"The accuracy, recall and F1score is comparatively much better","eb345163":"Thus, we would be implementing different models on the scaled dataset.","9e7458ca":"Un-supervised learning is not really a good idea. It might give better results with different hyperparamter combination but that is also the case for all the models we have used above.","38c5581a":"We have not achieved a better accuracy as the data distribution for train and test data has probably changed because we performed a SMOTE (over-sampling technique) on only training data","7ab2b18d":"To have an accurate prediction we will remove the outliers from our dataset.","eb73c9ab":"We can perform SMOTE on the entire data to check if that helps in improving the accuracy.","c3d0456c":"In this case, false positives are those who are wrongly diagnosed to be falling. A business solution would be to then alert emergency service as soon as someone has been classified to fall and thus a false positive would incur extra costs through the response system and loss of trust with future warnings. However, a false negative means that a fallen patient  has been wrongly classified being in another safe state (such as standing) and hence would not alert emergency service and can potentially lead to a fatal injury\/death. Since this is a very important issue and the consequences of wrongly classifying in the case of false negatives is extremely high, weightage of the best model\u2019s performance on false positive and false negative rates will also be taken into account and improved if possible. \n","46d285d8":"We are following the approach recommended by End-to-End Machine Learning Project chapter in the Hands-on Machine Learning book written by Aur\u00e9lien G\u00e9ron. The step \u2018Launch, monitor, and maintain your system\u2019 is not applicable in our case so we have not included it below.","2182b19f":"**SVM Classifier**","806337d0":"# Performace and Fine-tuning the model.\n","97755c34":"Since all the values here are 0, we can conclude that there are no null values. The data is complete.","6e44da5e":"# **Discover and visualize the data to gain insights.**","ee98bb42":"**A pairplot of each feature with the outcome variable to see which can be a good measure of predicting the target variable**","16361ee0":"# SMOTE AND Rearranging the data to improve accuracy","63843924":"**Decision Tree Classifier**","f2610bad":"# Prepare the data for Machine Learning algorithms.","1ea56a90":"Tracking and understanding fall patterns have been really important as they are considered to be a serious public health problem and is possibly a life-threatening situation for the elderly. According to the National Council on Aging (NCOA), one out of every four Americans aged 65 and older falls every year and these are the leading cause of death, injury and hospital admissions among the elderly population. The value of an accurate model for fall detection is immense in terms of its societal impact and the ease in which it can be incorporated into products through companies that already collect health information on a regular basis from Fitbits and smart watches. \nWe have decided to frame the problem in such a way that any businesses that incorporate the solution, can not only make profit but use this as a tool to help the society and illustrate their Corporate Social Responsibility (CSR). ","5d6f516d":"# Detecting Falls of Elderly Patients Through their Medical Information\nDarshan Tina\n\n\n\n","0adb2cd0":"# **Look at the big picture.**","67ca7129":"Let us generate or modify the above data in such a way that we have a binary decision of a person is falling or not\ni.e. we would classify the points belonging to other activities as Not falling.\n\nConverting a multi-class classification problem to binary classification where  Activity 3 ----> 1 (Activity of interest informing about the fall of an individual) and all other activities -----------> 0","564ddad2":"**Generating a temp rf_visualization classifer to visualize a tree within random forest of max_depth 5**","905e65d2":"The accuracy is still poor but the model perfroms comparatively better for scaled datapoints.","cd212c32":"Confusion matrix ","f44fe66d":"We have chosen Random Classifier with parameters like max_depth as 50 and n_estimators as 200 along with a threshold of 0.4 To be the best model not only because of the highest accuracy scores but also because it has the lowest false negative rate. We understand that the false positive rate of 8% is slightly higher than what it could have been but due to the fatal consequences that a high false negative rate has, we have decided that we will accept this solution understanding the tradeoff between the two rates. From a management perspective, the value-add of this solution is immense because we have a low false negative rate of 2% and have also identified the two most important predictors of when a patient can fall.  The cost is comparatively low because the assumption is that companies prevalent already in the healthcare field will adopt this solution. They can integrate this into the existing products like Fitbit, Apple Watches, etc. that already collect or easily can collect in the future most of the healthcare metrics required for this solution. These two captured by the Feature Importance plot are sugar level and the Electroencephalogram level. Although we personally do not have the domain expertise to provide recommendations on how to reduce your EEG level to get back to a normal safe state, we were able to research and find solutions on how patients\u2019 sugar level can be brought back down in a very quick period of time just by eating the right types of food. Recommendations such as \u2018your sugar level is too high, please eat a handful of peanuts as a quick solution\u2019 can be notified to the patient through their Fitbit or alike that the company owns.  \n\n\nWith an active alert system and a responsive recommendation to control the diet, this tool would ensure a reduction in the number of falls not only among the elderly but also the people in differnet age groups and ensure a happy and stay free environment for all.\n","f8c02d47":"We don't see a pattern and  we observe that the accuracy is really low.\n\nNow let's identify the most important features from the correlation matrix and try training on them.","b09e78fc":"# Present your solution.","b692aab3":"**Checking for Null Values**","62ccc345":"# **Get the data.**","35372f19":"**Unsupervised Learning Methods**","a66a9584":"*Better results can be achieved in a real-world setting by having a better understanding of features and tuning our models for a larger hyperparameter setting. *","385fb835":"Logistic Regression seems to give us better results than Naive Bayes Classification.","a250e0a1":"From the training the model above, we observed that Random forest has a better accuracy and a recall score as compared to the other classifiers we tested for. But our problem has a sheer focus of reducing the false negative results as the cost of handling them is really high. (One of the repurcussions of false-negative is death)","61f15758":"**Logistic Regression**","d00ec0f3":"Ratio is low but the cost for 236 false negatives will be high ","1b77921c":"Here we would be preparing and filtering out unrequired instances of the dataset. \n1. We will check for the null values in the dataset, and as per the ratio of number of null values to the overall data we would use appropriate technique to handle them \n\n2. Next, we would be handling the outliers in the dataset. The dataset is pretty huge and there are chances that we might get skewed results mainly because how the outliers are presented in the data. So it is always a safe technique to filter out the outliers and then split the data into train and test data.","652dc088":"Hence, the question is not only about how we can accurately classify the activity state of patients but also how reduce the implementation costs for businesses and increase societal value-add. There is no current solution to this problem because elderly patients depend on people around them at the point of falling to notify emergency response and be taken care of. Our performance measures are f1-score, recall and accuracy. \nIn this case, false positives are those who are wrongly diagnosed to be falling. A business solution would be to then alert emergency service as soon as someone has been classified to fall and thus a false positive would incur extra costs through the response system and loss of trust with future warnings. However, a false negative means that a fallen patient  has been wrongly classified being in another safe state (such as standing) and hence would not alert emergency service and can potentially lead to a fatal injury\/death. Since this is a very important issue and the consequences of wrongly classifying in the case of false negatives is extremely high, weightage of the best model\u2019s performance on false positive and false negative rates will also be taken into account and improved if possible. \n","fdc45fe7":"**Neural Networks**","dd5775db":"**Descriptive Statisitcs**","aabe4a4e":"Thus we achieved a false negative rate of 2% and a false positive rate of 8%","c34d22eb":"**Smote Analysis (to ensure all the data is equally distributed in the training set)**","b44de740":"**Calculating the number of false positives and false negatives**","2530ede1":"Here Sugar Level and EEG seem to be the most important features for training the model.","7699a142":"**Understanding the Activity Label**","262ce709":"We can see that the values are almost the same and in the above scatter plot the points for features \nBlood pressure and Heart Rate overlap each other.\n\nThis shows that 4 features Monitoring Time, Blood pressure, Heart Rate, Blood Circulation are really close to each other.","fba27538":"Based on the results above, we conclude that Random Forest Classifier is the best model for Fall Detection.","767e5478":"This dataset uses medical data from the healthcare domain for elderly patients from a subset of the Chinese population. We found the dataset on Kaggle (https:\/\/www.kaggle.com\/pitasr\/falldata) and it has 16.4K rows and 5 columns. The data was collected using a wearable sensor that collects information such as the blood pressure of the subject wearing the device, the blood circulation in the body and the EEG (electroencephalogram - a non- invasive test that records the electrical patterns within the brain) rate.\n\nThis is a classification problem in which the activity of each user would be differentiated into 6 categories as specified below:\n0 - User\/Subject is Standing             1 - Walking                2 - Sitting\n3 - Falling                     4 - Cramps                 5 - Running     \n\nWe are particularly interested in accurately estimating falling (group 3) based on 6 features (monitoring time, sugar level, EEG monitoring rate, blood pressure, heart beat rate and blood circulation).","d9e111ef":"**Naive Bayes Classification -----------   Baseline Model**","3485ba75":"Well what if we included only BP and HR features for training and testing the model?","f123b924":"**Scaling the Data**","34454917":"False positive - Falsely predicted of activity 3 (type Falling)\n\nFalse negative - It was of type 3 but was predicted as some other type (Not able to identify the activity of falling)","94c45211":"# Select a model and train it. (We have tried training numerous models to determine the best)","dc7ae6f7":"**Box Plot of Features to Understand the Data **","8ef3c993":"**K Nearest Neighbour Classification , KNN**","c41b828b":"Here we have 6 features, and Activity is the target label that we need to estimate.","9d76ba54":"**Random Forest Classifier**","ab2ac4cf":"**Comparision across models**","36dc52ff":"**Correlation between Features**"}}