{"cell_type":{"3d8d6c18":"code","3ebb561d":"code","ff95ccce":"code","397d3bf9":"code","7f02ad71":"code","5917f78d":"code","a271d3ab":"code","bf47a045":"code","13bc3353":"code","cfa50b61":"code","e4c398c1":"code","b7146cee":"code","e287b710":"code","5ab3dd77":"code","8b326042":"code","8336d448":"code","fde79297":"code","72bfdc76":"code","a1ea1962":"code","b898aec7":"code","ba36a93f":"code","5c98a417":"code","6d8720aa":"code","61faae37":"code","626c0222":"code","9cc0f52c":"code","06ad6b63":"code","1a6a300d":"code","327df4d9":"code","c1279aea":"code","6b154b9e":"code","f770ad66":"code","44461b84":"code","dd05e92c":"code","4f35b739":"code","d0ba6402":"code","15e98eaf":"code","31a17f53":"code","0a20bea2":"code","69c771c8":"code","b29a4557":"code","2efdecf9":"code","be3db01d":"code","c5164642":"code","805ba8b0":"code","2e3e3bbb":"code","b8f15c8f":"code","dbd11063":"code","0ddcb186":"code","e636ee68":"code","d2b51ec2":"markdown","25849af8":"markdown","e81d9c92":"markdown","9d73fbae":"markdown","b8029f19":"markdown","b3018604":"markdown","62a695bc":"markdown","c46cadcf":"markdown","16b0f265":"markdown","5d6e8095":"markdown","cd672113":"markdown","e73a7e4e":"markdown","dec68c51":"markdown","e8515eb6":"markdown","fa97c4f0":"markdown","5a522697":"markdown","7c319355":"markdown"},"source":{"3d8d6c18":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","3ebb561d":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns","ff95ccce":"df_company = pd.read_csv('..\/input\/ibm-hr-analytics-attrition-dataset\/WA_Fn-UseC_-HR-Employee-Attrition.csv')","397d3bf9":"df_company.info()\n","7f02ad71":"df_company.isnull().sum()\n\ndf_company['Attrition'] = df_company['Attrition'].apply(lambda x: 0 if x.lower() == 'no' else 1)\ndf_company['Attrition'].value_counts()","5917f78d":"def identify_cat(df):\n    cat =[]\n    con = []\n    for col in df.columns:\n        try:\n            df[col].apply(lambda x: float(x))\n            con.append(col)\n        except:\n            cat.append(col)\n    \n\n    return con,cat","a271d3ab":"con,cat = identify_cat(df_company)","bf47a045":"con","13bc3353":"cat","cfa50b61":"plt.figure(figsize=(30,60))\ni = 1\nfor col in con:\n    plt.subplot(9,3,i)\n    sns.boxplot(data = df_company,x='Attrition',y = col)\n    i +=1 ","e4c398c1":"#df_company['StandardHours'].value_counts()\ndf_company = df_company.drop('StandardHours',axis=1)\ndf_company = df_company.drop('EmployeeCount',axis=1)","b7146cee":"def concatenate(df,cat_cols):\n    for c in cat_cols:\n        temp = pd.get_dummies(df[c],drop_first=True,prefix=c)\n        df = pd.concat([df,temp],axis=1)\n        df = df.drop(c,axis = 1)\n    return df","e287b710":"df_company =  concatenate(df_company,cat)","5ab3dd77":"len(df_company['EmployeeNumber'] ) == len(set(df_company['EmployeeNumber']))","8b326042":"df_company = df_company.drop('EmployeeNumber',axis=1)","8336d448":"from sklearn.model_selection import train_test_split\nimport statsmodels.api as sm\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier,GradientBoostingClassifier\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.decomposition import PCA\nfrom sklearn.feature_selection import RFE","fde79297":"y = df_company.pop('Attrition')","72bfdc76":"X_train,X_test,y_train,y_test = train_test_split(df_company,y,train_size =0.8, random_state=40)","a1ea1962":"sc = MinMaxScaler()\nX_train[X_train.columns] = sc.fit_transform(X_train)\nX_test[X_test.columns] = sc.transform(X_test)","b898aec7":"rfe = RFE(LogisticRegression(),n_features_to_select=20)","ba36a93f":"\nrfe = rfe.fit(X_train,y_train)\n#X_train_sm = sm.add_constant(X_train)","5c98a417":"X_train_sm = X_train.copy()\ncolumns = X_train_sm.columns[rfe.support_]\nX_train_sm = X_train_sm[columns]","6d8720aa":"#X_test_sm = X_test[X_test_sm.columns[rfe.support_]]\n\nX_train_sm = sm.add_constant(X_train_sm)","61faae37":"X_train_sm","626c0222":"lm = sm.GLM(y_train,X_train_sm,family=sm.families.Binomial()).fit()\nlm.summary()","9cc0f52c":"from statsmodels.stats.outliers_influence import variance_inflation_factor\n\ndef check_vif(df):\n    res = pd.DataFrame()\n    res['Columns'] = df.columns\n    res['VIF'] = [variance_inflation_factor(df.values,i) for i in range(len(df.columns))]\n    return res","06ad6b63":"vif = check_vif(X_train_sm)\nvif.sort_values(by='VIF',ascending=False)","1a6a300d":"y_train_pred = lm.predict(X_train_sm)\n\ndef matrix(cuttoffs,predictions,true_val):\n    matrix = np.zeros((10,2,2))\n    for i,c in enumerate(cutoffs):\n        p = predictions.apply(lambda x: 1 if x >= c else 0) \n        matrix[i,:,:] = confusion_matrix(true_val,p)\n    return matrix\n        ","327df4d9":"cutoffs = np.linspace(0.0,0.9,10)\ncf = matrix(cutoffs,y_train_pred,y_train)","c1279aea":"def metrics(cf):\n    for i in range(cf.shape[0]):\n        acc = (cf[i,0,0] + cf[i,1,1])\/(cf[i,0,0] + cf[i,0,1]+ cf[i,1,0] + cf[i,1,1])\n        recall = (cf[i,1,1])\/(cf[i,1,1] + cf[i,1,0])\n        precision = (cf[i,1,1])\/(cf[i,1,1] + cf[i,0,1])\n        specificity = (cf[i,0,0])\/(cf[i,0,0] + cf[i,0,1])\n        print(f\"cutoff = {round(0.1*i,2)} -- acc={acc} -- recall={recall} -- precision={precision} -- specificity={specificity}\\n\\n\")\n        \nmetrics(cf)","6b154b9e":"from imblearn.over_sampling import SMOTE\nsmote = SMOTE(n_jobs=-1,random_state=1)\nX_train_sm,y_train = smote.fit_resample(X_train_sm,y_train)\ny_train_pred = lm.predict(X_train_sm)","f770ad66":"lm = sm.GLM(y_train,X_train_sm,family=sm.families.Binomial()).fit()\ny_train_pred = lm.predict(X_train_sm)\ncf = matrix(cutoffs,y_train_pred,y_train)\ncf","44461b84":"metrics(cf)","dd05e92c":"X_test_sm = X_test[X_test.columns[rfe.support_]]\nX_test_sm = sm.add_constant(X_test_sm)","4f35b739":"y_test_pred = lm.predict(X_test_sm)\ny_test_pred = y_test_pred.apply(lambda x: 1 if x >= 0.5 else 0)","d0ba6402":"cf = confusion_matrix(y_test,y_test_pred)","15e98eaf":"cf\nacc = (cf[0,0] + cf[1,1])\/(cf[0,0] + cf[0,1]+ cf[1,0] + cf[1,1])\nrecall = (cf[1,1])\/(cf[1,1] + cf[1,0])\nprecision = (cf[1,1])\/(cf[1,1] + cf[0,1])\nspecificity = (cf[0,0])\/(cf[0,0] + cf[0,1])\nprint(f\"cutoff = 0.6 -- acc={acc} -- recall={recall} -- precision={precision} -- specificity={specificity}\\n\\n\")","31a17f53":"params = {\n    'max_depth': [5,10,20,50],\n    'min_samples_split': [10,20,50,100],\n    'min_samples_leaf': [10,20,100],\n    #'max_leaf_nodes': [10,50,100,300]\n    \n}\ndt = DecisionTreeClassifier(random_state=100)\n\nfrom sklearn.model_selection import GridSearchCV\n\ndt_grid = GridSearchCV(estimator=dt,param_grid=params,cv=5,verbose=10,n_jobs=-1,scoring='roc_auc')\n","0a20bea2":"X_train,X_test,y_train,y_test = train_test_split(df_company,y,train_size =0.8, random_state=40)\nsm = SMOTE()\nX_train,y_train = sm.fit_resample(X_train,y_train)\ndt_grid = dt_grid.fit(X_train,y_train)","69c771c8":"dt = dt_grid.best_estimator_\ndt = dt.fit(X_train,y_train)","b29a4557":"y_test_pred = dt.predict(X_test)\n\ncf = confusion_matrix(y_test,y_test_pred)","2efdecf9":"from sklearn.metrics import accuracy_score,precision_score,recall_score\nprint(accuracy_score(y_test,y_test_pred), precision_score(y_test,y_test_pred), recall_score(y_test,y_test_pred))","be3db01d":"rf = RandomForestClassifier(random_state=500,oob_score=True)\n\nparams = {\n    \"max_depth\": [5,10,20],\n    'min_samples_split': [10,50,100],\n    'min_samples_leaf': [10,50,100],\n    \"max_features\": [10,20],\n    \"n_estimators\": [100,200]\n    #'max_leaf_nodes': [10,50,100,500]\n    \n}\n\n\n\nrf_grid = GridSearchCV(estimator=rf,param_grid=params,cv=5,scoring='balanced_accuracy',verbose=10,n_jobs=-1)","c5164642":"rf_grid = rf_grid.fit(X_train,y_train)\nrf = rf_grid.best_estimator_","805ba8b0":"y_test_pred = rf.predict(X_test)\nprint(accuracy_score(y_test,y_test_pred),recall_score(y_test,y_test_pred),precision_score(y_test,y_test_pred))\nprint(rf.oob_score_)","2e3e3bbb":"from sklearn.ensemble import AdaBoostClassifier,GradientBoostingClassifier","b8f15c8f":"ad_grid = {\n    \"n_estimators\": [50,100,200],\n    'learning_rate': [0.01,0.05,0.1]\n}\nad = AdaBoostClassifier(random_state=500)\n\nad_grid = GridSearchCV(estimator=ad,param_grid=ad_grid,cv=5,scoring='balanced_accuracy',n_jobs=-1,verbose=10)\n\nad_grid = ad_grid.fit(X_train,y_train)\n\nad = ad_grid.best_estimator_","dbd11063":"y_test_pred = ad.predict(X_test)\nprint(accuracy_score(y_test,y_test_pred),recall_score(y_test,y_test_pred),precision_score(y_test,y_test_pred))","0ddcb186":"gb_grid = {\n    \"n_estimators\": [50,100],\n    'learning_rate': [0.01,0.05,0.1],\n    'max_depth': [3,5,10],\n    \n}\ngb = GradientBoostingClassifier(random_state=500)\n\ngb_grid = GridSearchCV(estimator=gb,param_grid=gb_grid,cv=5,scoring='balanced_accuracy',n_jobs=-1,verbose=10)\n\ngb_grid = gb_grid.fit(X_train,y_train)\n\ngb = gb_grid.best_estimator_","e636ee68":"y_test_pred = gb.predict(X_test)\nprint(accuracy_score(y_test,y_test_pred),recall_score(y_test,y_test_pred),precision_score(y_test,y_test_pred))","d2b51ec2":"## Boosting is given better results as compared to random forest and decision tree","25849af8":"### The data is imbalanced let us try SMOTE","e81d9c92":"### Let us check for null values\n- Let us see the null values\n- We will have the to treat them before training the model","9d73fbae":"### Let us print metrics for different cutoffs","b8029f19":"### The decision tree didnt predict positive flags well. This is due to imbalance classes","b3018604":"## cutoff 0.5 seems to be best by observing metrics","62a695bc":"### We have to drop the Employee Number","c46cadcf":"## Importing Libraries for model building","16b0f265":"## P- values and VIFs are stable we can now train the model\n1. We use the same model \n2. Choose cutoff","5d6e8095":"### Let us try Decision Trees","cd672113":"### Get categorical and Numeric columns from the data Set ","e73a7e4e":"## Performing predictions","dec68c51":"### Observation and Results:\n\n\n- If accuracy is the key then Tree models will work well\n- Here we also want to give importance to both the classes so I would prefer Logistic regression which gave me good recall score\n- We can play with cutoff values I have taken 0.5\n- If focus is more on accuracy the model will predict everything as `0` and predictions are biased\n- Hence I anve used SMOTE() for imbalanced data\n- The tree models(decision tree, random forest, boosting) seem to have been overfitted due to low volume data if the data was larger trees would perform better\n","e8515eb6":"print(\"as\")","fa97c4f0":"### Let us observe the shape of the data set","5a522697":"### Let us check the metrics on decision tree","7c319355":"### RFE on test data"}}