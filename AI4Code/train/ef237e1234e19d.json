{"cell_type":{"1b5ebe44":"code","850a7b23":"code","44b82b9a":"code","72c19b36":"code","a7ea0539":"code","278e11e4":"code","6a9d67d4":"code","01a51fa4":"code","7e8b29cf":"code","1b5a001d":"code","083ca1eb":"code","79f6a4a4":"code","685efaa0":"code","a1ef9996":"code","a2c59004":"code","b27de877":"code","e38b282c":"code","672ff776":"code","e89c61bc":"code","90fd2c9d":"code","ab49a359":"code","eea6261c":"code","6628f9dd":"code","cf9e75c9":"code","9393843d":"code","774b8c32":"code","41980234":"code","a002d7c3":"code","df9c9b45":"code","e1c98d4c":"code","56625c65":"code","7a89785e":"code","e2bce175":"code","b6df3e73":"code","3fa7296b":"code","22cf7315":"code","3fb3f7b1":"code","4b428512":"code","58904998":"code","5be99f93":"code","c4185454":"code","ecc67767":"code","f63c071b":"code","940b652d":"code","28763bb4":"code","2a6fccd4":"code","4823df76":"code","44a18213":"code","c840b9ab":"code","2b1ffbe8":"code","0e70ded0":"code","746ec951":"code","a572fdf6":"code","f52b4e93":"code","ef550996":"code","6848571c":"code","72cf084f":"code","7771c15f":"markdown","433d99dd":"markdown","c4278a8d":"markdown","9f7dd265":"markdown","c53e2e7d":"markdown","8a0ed0be":"markdown","79f9e7d4":"markdown","95957ce3":"markdown","6f495502":"markdown","893b6c10":"markdown","6ec516b0":"markdown","9fe57326":"markdown","7017be47":"markdown","a35bb9a7":"markdown","4c497676":"markdown","e376defa":"markdown","335e49cf":"markdown","b58acebb":"markdown"},"source":{"1b5ebe44":"import numpy as np \nimport pandas as pd\nimport seaborn as sns \nimport matplotlib.pyplot as plt\nimport missingno as msno\nfrom pandas_profiling import ProfileReport\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import StandardScaler","850a7b23":"data=pd.read_csv(\"..\/input\/breast-cancer-classifiction\/breastcancer.csv\")","44b82b9a":"data.shape","72c19b36":"data.head()","a7ea0539":"data.columns","278e11e4":"data.isnull().sum()","6a9d67d4":"data.info()","01a51fa4":"data.drop(\"id\",axis=1,inplace=True)","7e8b29cf":"# Here we have divided the features into sections according to their importance\n# I will use this division in several cases. If we expose one of them to another, we will work with the data completely.\nfeatures_mean= list(data.columns[1:11])\nfeatures_se= list(data.columns[11:21])\nfeatures_worst=list(data.columns[21:31])\nprint(features_mean)\nprint(\"-----------------------------------\")\nprint(features_se)\nprint(\"------------------------------------\")\nprint(features_worst)","1b5a001d":"# Now I will get a report on the data that includes all aspects.\n#profile=ProfileReport(data, title=\"Breast cancer Report\")\n#profile","083ca1eb":"# Here I will convert the data type of a column to the numeric type.\ndata[\"diagnosis\"]=data[\"diagnosis\"].map({\"M\" : 1 , \"B\" : 0})","79f6a4a4":"data.head(10)","685efaa0":"data.shape","a1ef9996":"for i in data.columns:\n  print(i, data[i].unique())","a2c59004":"data_dtype = data.dtypes\ndata_dtype.value_counts()","b27de877":"sns.countplot(data[\"diagnosis\"])\nplt.title(\"Countplot for diagnosis\")\nplt.show();","e38b282c":"# Here I get the number of values in each of our classes.\ndata[\"diagnosis\"].value_counts()","672ff776":"data[\"area_mean\"]","e89c61bc":"plt.scatter(data['area_mean'],data['perimeter_mean'])","90fd2c9d":"# We note that the correlation coefficient between the values is good\ndata.corr()","ab49a359":"fig, ax = plt.subplots(figsize=(20,20))\nsns.heatmap(data.corr(), annot=True, linewidths=.5, ax=ax)\nplt.show()","eea6261c":"# Here we will get the 25 most influential features on the ranking \/ main column\ncorr_1=data.corr()\nmost=corr_1.nlargest(25,\"diagnosis\")\nmost","6628f9dd":"msno.matrix(most)","cf9e75c9":"data.hist(figsize=(18,10))\nplt.show()","9393843d":"most.hist(figsize=(18,10))\nplt.show()","774b8c32":"#Here I compared most of the variables with the target variable.\nf, axes = plt.subplots(7,5 , figsize=(30, 30), sharex=False)\nfor i, feature in enumerate(data):\n    sns.scatterplot(data=most, x = feature, y= \"diagnosis\",ax=axes[i%7, i\/\/7])\n","41980234":"# To see the relationship between the objective feature and the nearest other features to influence it.\nsns.swarmplot(x=data['diagnosis'],\n              y=data['concave points_worst'])\n","a002d7c3":"sns.lmplot(x=\"diagnosis\", y=\"concave points_worst\", data=data)","df9c9b45":"# good\n# From that graph, we notice that the more focus on the data we want, the more details we get, so I will repeat this process several times.\nsns.lmplot(x=\"diagnosis\", y=\"concave points_worst\", data=most)","e1c98d4c":"#Now I will draw the relationship between the second most influential feature in the data on the target feature.\nsns.swarmplot(x=data['diagnosis'],\n              y=data['perimeter_worst'])","56625c65":"sns.lmplot(x=\"diagnosis\", y=\"perimeter_worst\", data=data)","7a89785e":"sns.lmplot(x=\"diagnosis\", y=\"perimeter_worst\", data=most)","e2bce175":"# Finally, the relationship between the 25 most influential feature in the data on the target feature.\nsns.swarmplot(x=data['diagnosis'],\n              y=data['compactness_se'])","b6df3e73":"sns.lmplot(x=\"diagnosis\", y=\"compactness_se\", data=most)","3fa7296b":"data.head(5)","22cf7315":"from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import confusion_matrix","3fb3f7b1":"target=data[\"diagnosis\"]\nfeatures=data.drop([\"diagnosis\"],axis=1)","4b428512":"# Here I do the data transformation.\nscaler = StandardScaler(copy=True, with_mean=True, with_std=True)\nX = scaler.fit_transform(features)","58904998":"# Here I has reduced the amount of data used in the testing process because the data is very small\nx_train,x_test,y_train,y_test=train_test_split(X,target,test_size=0.1,random_state=0)","5be99f93":"print(x_train.shape,x_test.shape,y_train.shape,y_test.shape)","c4185454":"from sklearn.svm import SVC\n\nSVCModel = SVC(kernel= 'rbf',# it can be also linear,poly,sigmoid,precomputed\n               max_iter=100,C=1.0,gamma='auto')\n\nSVCModel.fit(x_train, y_train)","ecc67767":"#Calculating Details\nprint('SVCModel Train Score is : ' , SVCModel.score(x_train, y_train))\nprint('SVCModel Test Score is : ' , SVCModel.score(x_test, y_test))\n#Calculating Prediction\ny_pred = SVCModel.predict(x_test)\nprint('Predicted Value for SVCModel is : ' , y_pred[:20])\nprint('real Values for SVCModel     is : '\"\\n\" , y_test[:20])","f63c071b":"#Calculating Confusion Matrix\nCM = confusion_matrix(y_test, y_pred)\nprint('Confusion Matrix is : \\n', CM)\n\n# drawing confusion matrix\nsns.heatmap(CM, center = True)\nplt.show()\n","940b652d":"from sklearn.ensemble import RandomForestClassifier\nRandomForestClassifierModel=RandomForestClassifier(n_estimators=100, criterion='gini', max_depth=7,\n                                min_samples_split=2, min_samples_leaf=1,min_weight_fraction_leaf=0.0,\n                                max_features='auto',max_leaf_nodes=7,min_impurity_decrease=0.0,\n                                min_impurity_split=None, bootstrap=True,oob_score=False, n_jobs=-1,\n                                random_state=0, verbose=0,warm_start=True)\nRandomForestClassifierModel.fit(x_train, y_train)","28763bb4":"#Calculating Details\nprint('RandomForestClassifierModel Train Score is : ' , RandomForestClassifierModel.score(x_train, y_train))\nprint('RandomForestClassifierModel Test Score is : ' , RandomForestClassifierModel.score(x_test, y_test))\nprint('RandomForestClassifierModel features importances are : ' , RandomForestClassifierModel.feature_importances_)","2a6fccd4":"#Calculating Prediction\ny_pred = RandomForestClassifierModel.predict(x_test)\ny_pred_prob = RandomForestClassifierModel.predict_proba(x_test)\nprint('Predicted Value for RandomForestClassifierModel is : ' , y_pred[:10])\nprint(\"real values of y_test>>>>>>>>>>>>>>>>>>>>>>>>>>>is : \\n\" ,y_test[:10] )\nprint('Prediction Probabilities Value for RandomForestClassifierModel is : ' , y_pred_prob[:10])\n ","4823df76":"#Calculating Confusion Matrix\nCM = confusion_matrix(y_test, y_pred)\nprint('Confusion Matrix is : \\n', CM)\n\n# drawing confusion matrix\nsns.heatmap(CM, center = True)\nplt.show()\n \n","44a18213":"# I will use the continents tree in this classification process.\nfrom sklearn.tree import DecisionTreeClassifier","c840b9ab":"DecisionTreeClassifierModel = DecisionTreeClassifier(criterion='gini',max_depth=3,random_state=33) #criterion can be entropy\nDecisionTreeClassifierModel.fit(x_train, y_train)","2b1ffbe8":"#Calculating Details\n# Here is the success rate of the training sample\nprint('DecisionTreeClassifierModel Train Score is : ' , DecisionTreeClassifierModel.score(x_train, y_train))\n# And now our date with the success rate of the same training.\nprint('DecisionTreeClassifierModel Test Score is : ' , DecisionTreeClassifierModel.score(x_test, y_test))\n# Here to show how many classes we have\nprint('DecisionTreeClassifierModel Classes are : ' , DecisionTreeClassifierModel.classes_)\n# This instruction is very important in knowing which of the features that have the most impact on the model out of the 12 features that we have.\nprint('DecisionTreeClassifierModel feature importances are : ' , DecisionTreeClassifierModel.feature_importances_)\nprint(len( DecisionTreeClassifierModel.feature_importances_))","0e70ded0":"#Calculating Prediction\ny_pred = DecisionTreeClassifierModel.predict(x_test)\ny_pred_prob = DecisionTreeClassifierModel.predict_proba(x_test)\nprint('Predicted Value for DecisionTreeClassifierModel is : ' , y_pred[:10])\nprint(\"The first 10 elements of the test sample so that we know whether they match or not : \" , y_test[:10])\n# This instruction is important as it shows the percentage \/ probability of choosing each of the two projectors.\nprint('Prediction Probabilities Value for DecisionTreeClassifierModel is : ' , y_pred_prob[:10])","746ec951":"#Calculating Confusion Matrix\nCM = confusion_matrix(y_test, y_pred)\nprint('Confusion Matrix is : \\n', CM)\n\n# drawing confusion matrix\nsns.heatmap(CM, center = True)\nplt.show()\n ","a572fdf6":"# Here I will call the libraries that we need.\nfrom keras.wrappers.scikit_learn import KerasClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom keras.models import Sequential # empty neural network\nfrom keras.layers import Dense # layer constitution\n","f52b4e93":"def build_classifier():\n    classifier = Sequential() # initialize neural network architecture\n    classifier.add(Dense(units = 8, kernel_initializer=\"uniform\", activation=\"relu\", input_dim = x_train.shape[1]))\n    classifier.add(Dense(units = 8, kernel_initializer=\"uniform\", activation=\"relu\")) #kernel_initializer: to initialize weights\n    classifier.add(Dense(units = 1, kernel_initializer=\"uniform\", activation=\"sigmoid\")) #output layer\n    classifier.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n    return classifier\n\nclassifier = KerasClassifier(build_fn = build_classifier, epochs=70, batch_size=10)\n# epoch = number of iteration, batch size : efers to the number of training examples utilized in one iteration.\naccurisies = cross_val_score(estimator=classifier, X=x_train, y = y_train, cv = 2)\nmean = accurisies.mean()\nvariance = accurisies.std()\nprint(\"Accuracy mean : \", str(mean))\nprint(\"Accuracy variance : \", str(variance))\n","ef550996":"# Here I start by training the model on validation data.\nhistory = classifier.fit(x_test, y_test, validation_split=0.20, epochs=70, batch_size=10, verbose=1)","6848571c":"# Accurasy\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('Model Accuracy vs Epoch')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.show()","72cf084f":"# Loss\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('Model Loss vs Epoch')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper right')\nplt.show()","7771c15f":"# Reading Data","433d99dd":"**Now let's start with the first algorithm**\n# 1. SVC Algorithm","c4278a8d":"# data splitting","9f7dd265":"> **But the good news is that the 25 most effective data features on the target column have no skew.**","c53e2e7d":"> **The test results we got from this model are very good.**","8a0ed0be":"# 2 . Random Forest Algorithm","79f9e7d4":"# EDA","95957ce3":"> **From the results, we note that in some features, its pattern is linear, whether positive or negative, with the target feature, and some of them are not random patterns.**","6f495502":"> **Now that I have finished exploring and analyzing the data, I will now discuss the division and tuning of data to apply algorithms to it.**","893b6c10":"**The results we obtained are somewhat disturbing, because we are in a trap called overfitting.**","6ec516b0":"# EDA :Breast Cancer and Classification with ML and DL\n\n**On that software page I did a lot of work starting from**\n> **1.Analyze and explore data.**\n\n> **2.Data smoothing and transformation**.\n\n> **3.data splitting.**\n\n>**4.Application of machine learning algorithms.**\n\n>**5.Application of deep learning algorithm using deep network for classification.**\n\n> **6.Show results.**\n\n>**7.Lots of details and analysis of the results.**","9fe57326":"# Initial data recognition","7017be47":"> **From the previous graphs, we notice that a lot of the data have a left skew, that is, a positive skew, with a right tail, and some data also have symmetry.**","a35bb9a7":"> **We can modify the neural network to get rid of the overfitting.\nBut it's okay now.**","4c497676":"**We have now finished putting the algorithms into machine learning and deep learning.**\n> **If you find these notebooks useful for you, let me know.\n> If you have any modifications, let me know.**\n# thank you for your time .","e376defa":"> **From the results that I obtained, we note that this is the best linear relationship that we can get in the data that forms a random relationship from the target feature in the data.**","335e49cf":"# 4 . Deep Learning Algorithm","b58acebb":"# 3 . Decision Tree Algorithm"}}