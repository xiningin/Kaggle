{"cell_type":{"819b3bcc":"code","7c237193":"code","b44afde2":"code","297cac79":"code","2400e2b8":"code","71ea9f97":"code","f5459864":"code","94c488a2":"code","728977f6":"code","dfa976f9":"markdown","a9421c73":"markdown","2564bd87":"markdown","c9437d83":"markdown","748b461d":"markdown","fce77fc6":"markdown","ac2659d3":"markdown","8a30fb39":"markdown"},"source":{"819b3bcc":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","7c237193":"from numpy import array\nfrom numpy import asarray\nfrom numpy import zeros\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import Flatten\nfrom keras.layers import Embedding\n","b44afde2":"# define documents\ndocs = ['Well done!',\n\t\t'Good work',\n\t\t'Great effort',\n\t\t'nice work',\n\t\t'Excellent!',\n\t\t'Weak',\n\t\t'Poor effort!',\n\t\t'not good',\n\t\t'poor work',\n\t\t'Could have done better.']\n# define class labels\nlabels = array([1,1,1,1,1,0,0,0,0,0])\n","297cac79":"# prepare tokenizer\nt = Tokenizer()\nt.fit_on_texts(docs)\nvocab_size = len(t.word_index) + 1\n# integer encode the documents\nencoded_docs = t.texts_to_sequences(docs)\nprint(encoded_docs)\n","2400e2b8":"# pad documents to a max length of 4 words\nmax_length = 4\npadded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\nprint(padded_docs)\n","71ea9f97":"# load the whole embedding into memory\nembeddings_index = dict()\nf = open('..\/input\/glove-global-vectors-for-word-representation\/glove.6B.100d.txt')\nfor line in f:\n\tvalues = line.split()\n\tword = values[0]\n\tcoefs = asarray(values[1:], dtype='float32')\n\tembeddings_index[word] = coefs\nf.close()\nprint('Loaded %s word vectors.' % len(embeddings_index))\n","f5459864":"# create a weight matrix for words in training docs\nembedding_matrix = zeros((vocab_size, 100))\nfor word, i in t.word_index.items():\n\tembedding_vector = embeddings_index.get(word)\n\tif embedding_vector is not None:\n\t\tembedding_matrix[i] = embedding_vector\n","94c488a2":"# define model\nmodel = Sequential()\ne = Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=4, trainable=False)\nmodel.add(e)\nmodel.add(Flatten())\nmodel.add(Dense(1, activation='sigmoid'))\n# compile the model\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n# summarize the model\nprint(model.summary())\n","728977f6":"# fit the model\nmodel.fit(padded_docs, labels, epochs=50, verbose=0)\n# evaluate the model\nloss, accuracy = model.evaluate(padded_docs, labels, verbose=0)\nprint('Accuracy: %f' % (accuracy*100))","dfa976f9":"# Training and Evaluation","a9421c73":"# Dataset","2564bd87":"# Tokenization and Text to Sequences\n\nWe need to be able to map words to integers as well as integers to words.\n\nKeras provides a Tokenizer class that can be fit on the training data, can convert text to sequences consistently by calling the texts_to_sequences() method on the Tokenizer class, and provides access to the dictionary mapping of words to integers in a word_index attribute.","c9437d83":"# Imports","748b461d":"# Creating Fixed Length Vectors Using Padding","fce77fc6":"# Load GloVe Word Embedding","ac2659d3":"# Creating Model Using GloVe Embedding\n\nNow we can define our model, fit, and evaluate it as before.\n\nThe key difference is that the embedding layer can be seeded with the GloVe word embedding weights. We chose the 100-dimensional version, therefore the Embedding layer must be defined with output_dim set to 100. Finally, we do not want to update the learned word weights in this model, therefore we will set the trainable attribute for the model to be False.","8a30fb39":"# Creating Embedding Matrix\n\nThis is pretty slow. It might be better to filter the embedding for the unique words in your training data.\n\nNext, we need to create a matrix of one embedding for each word in the training dataset. We can do that by enumerating all unique words in the Tokenizer.word_index and locating the embedding weight vector from the loaded GloVe embedding.\n\nThe result is a matrix of weights only for words we will see during training."}}