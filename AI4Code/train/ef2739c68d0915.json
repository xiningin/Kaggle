{"cell_type":{"96d3e16a":"code","a124b185":"code","aa16e308":"code","87dca8be":"code","1cfe1dd7":"code","faf0cc60":"code","bf07d37a":"code","9ea8249c":"code","7579c9f4":"code","263ab26e":"code","10cc2de3":"code","a52e6b02":"code","aec1cedb":"code","53b6fe28":"code","665b2ce7":"code","46a34b4e":"code","867a3f81":"code","ccc86874":"code","4430e060":"code","3f96217e":"code","9e07ab6f":"code","17992816":"code","139e7010":"code","fcdac1b3":"code","fde56ea6":"code","14331e7c":"code","d8c17d69":"code","94bbfe77":"code","3d40aa18":"code","db7fe146":"code","e3b7178f":"code","b6012715":"code","216f9ccb":"code","d89fdabe":"code","03c6f31b":"code","6c8fa36b":"code","91272e4f":"code","6335c0fb":"code","9f57035b":"code","6a1cf61a":"code","9690129f":"code","b7dcca1c":"code","0acfb320":"code","d5834ff9":"code","918b8292":"code","54fdedf4":"code","767ba5e6":"code","6b180087":"code","f104fc0a":"code","a16ba28f":"code","17811218":"code","aeddf00d":"code","ec40051b":"code","dbb62c8b":"code","c0ac744b":"code","45ec9aa5":"code","5a5a2394":"code","268d8659":"code","59f478f3":"code","9286480b":"code","92715d6c":"code","ee8aa744":"code","d0c73f7c":"code","577adbbe":"code","9e7d1a42":"code","1f42098b":"code","e2b2cf8a":"code","6c553a5b":"code","7dca986f":"code","9cb4079f":"code","26343ad4":"code","04eba11f":"code","63480471":"code","6bb8c6d0":"code","d5ea38e0":"code","dc786996":"code","5c994a5e":"code","54ab32b7":"code","56afc9dd":"code","813c441d":"code","c48c7581":"code","e271c145":"code","ae892cdd":"code","60166515":"code","2124a648":"code","c8617204":"code","517e2121":"code","604f32b3":"code","7fc859e1":"code","100b903f":"code","a2656f4e":"code","5e2efaa2":"code","49423685":"code","eb26fb80":"code","877f71a3":"code","5c3a5cd3":"code","f5973dd4":"code","94dda13c":"code","bdf03b5c":"code","83351cc5":"code","02167187":"code","3e2cde28":"code","fc83e762":"code","3e710814":"code","6e9077a1":"code","59f0289b":"code","5bc57d0e":"code","2d39c886":"code","e15d1956":"code","a2a3521e":"code","27ad69a1":"code","bf95cbd7":"code","fb71a088":"markdown"},"source":{"96d3e16a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input\/'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","a124b185":"f=open('\/kaggle\/input\/datafilemerek\/1-PK-Pdt-Sus-18-kbl-merek-Pgt.txt', encoding=\"latin-1\")\ncontents =f.read()\nprint(contents)","aa16e308":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input\/datafilemerek'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        f=open(os.path.join(dirname, filename), encoding=\"latin-1\")\n        contents =f.read()\n        print(contents)","87dca8be":"pip install PyPDF2","1cfe1dd7":"import os\nimport PyPDF2\n\nimport pandas as pd\nisi=list()\nisibersih=' '\nisibersih2=' '\npage_content=\"\"\nfor dirname, _, filenames in os.walk('\/kaggle\/input\/datamerekpdf'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        pdfFileObj=open(os.path.join(dirname, filename),'rb')\n        pdfReader=PyPDF2.PdfFileReader(pdfFileObj)\n        page = pdfReader.getPage(1)\n        page_content = page.extractText()\n        print(page_content)","faf0cc60":"pip install pdfminer.six","bf07d37a":"import io\nfrom pdfminer.converter import TextConverter\nfrom pdfminer.pdfinterp import PDFPageInterpreter\nfrom pdfminer.pdfinterp import PDFResourceManager\nfrom pdfminer.pdfpage import PDFPage\ndef extract_text_from_pdf(pdf_path):\n    resource_manager = PDFResourceManager()\n    fake_file_handle = io.StringIO()\n    converter = TextConverter(resource_manager, fake_file_handle)\n    page_interpreter = PDFPageInterpreter(resource_manager, converter)\n    \n    with open(pdf_path, 'rb') as fh:\n        for page in PDFPage.get_pages(fh, \n                                      caching=True,\n                                      check_extractable=True):\n            page_interpreter.process_page(page)\n            \n        text = fake_file_handle.getvalue()\n    \n    # close open handles\n    converter.close()\n    fake_file_handle.close()\n    \n    if text:\n        return text","9ea8249c":"print(extract_text_from_pdf('\/kaggle\/input\/datamerekpdf\/139_K-Pdt.Sus-HKI-2018-ato_revisi.pdf'))","7579c9f4":"import os\nimport PyPDF2\n\nimport pandas as pd\nisi=list()\nisibersih=' '\nisibersih2=' '\nfor dirname, _, filenames in os.walk('\/kaggle\/input\/datamerekpdf'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        pdfFileObj=open(os.path.join(dirname, filename),'rb')\n        pdfReader=PyPDF2.PdfFileReader(pdfFileObj)\n        number_of_pages = pdfReader.getNumPages()\n        page_content=\"\"                # define variable for using in loop.\n        for page_number in range(number_of_pages):\n            page = pdfReader.getPage(page_number)\n            page_content += page.extractText()\n        isibersih=page_content.replace('DisclaimerKepaniteraan Mahkamah Agung Republik Indonesia berusaha untuk selalu mencantumkan informasi paling kini dan akurat sebagai bentuk komitmen Mahkamah Agung untuk pelayanan publik, transparansi dan akuntabilitas pelaksanaan fungsi peradilan. Namun dalam hal-hal tertentu masih dimungkinkan terjadi permasalahan teknis terkait dengan akurasi dan keterkinian informasi yang kami sajikan, hal mana akan terus kami perbaiki dari waktu kewaktu.Dalam hal Anda menemukan inakurasi informasi yang termuat pada situs ini atau informasi yang seharusnya ada, namun belum tersedia, maka harap segera hubungi Kepaniteraan Mahkamah Agung RI melalui :Email : kepaniteraan@mahkamahagung.go.idTelp : 021-384 3348 (ext.318)',' ')\n        isibersih2=isibersih.replace('Mahkamah Agung Republik Indonesia',' ')\n        isi.append(page_content)\n        pdfFileObj.close()\n        ","263ab26e":"len(isi)","10cc2de3":"isi[2]","a52e6b02":"import nltk\n\nsent1 = \"It might help to re-install Python if possible.\"\nsent2 = \"It can help to install Python again if possible.\"\nsent3 = \"It can be so helpful to reinstall C++ if possible.\"\nsent4 = \"help It possible Python to re-install if might.\" # This has the same words as sent1 with a different order.\nsent5 = \"I love Python programming.\"\n\ntokens1 = nltk.word_tokenize(sent1)\ntokens2 = nltk.word_tokenize(sent2)\ntokens3 = nltk.word_tokenize(sent3)\ntokens4 = nltk.word_tokenize(sent4)\ntokens5 = nltk.word_tokenize(sent5)\n\nng1_tokens = set(nltk.ngrams(tokens1, n=3))\nng2_tokens = set(nltk.ngrams(tokens2, n=3))\nng3_tokens = set(nltk.ngrams(tokens3, n=3))\nng4_tokens = set(nltk.ngrams(tokens4, n=3))\nng5_tokens = set(nltk.ngrams(tokens5, n=3))\n\njd_sent_1_2 = nltk.jaccard_distance(ng1_tokens, ng2_tokens)\njd_sent_1_3 = nltk.jaccard_distance(ng1_tokens, ng3_tokens)\njd_sent_1_4 = nltk.jaccard_distance(ng1_tokens, ng4_tokens)\njd_sent_1_5 = nltk.jaccard_distance(ng1_tokens, ng5_tokens)\n\nprint(jd_sent_1_2, \"Jaccard Distance between tokens1 and tokens2 with ngram 3\")\nprint(jd_sent_1_3, \"Jaccard Distance between tokens1 and tokens3 with ngram 3\")\nprint(jd_sent_1_4, \"Jaccard Distance between tokens1 and tokens4 with ngram 3\")\nprint(jd_sent_1_5, \"Jaccard Distance between tokens1 and tokens5 with ngram 3\")","aec1cedb":"import pandas as pd","53b6fe28":"df = pd.read_excel (r'..\/input\/datamerek\/datamerekakhir2.xlsx')\nprint (df)\ndf.to_csv('\/kaggle\/working\/datamerekakhir.csv')","665b2ce7":"dfkorupsi = pd.read_excel (r'..\/input\/data-korupsi\/datakorupsi.xlsx')\nprint (dfkorupsi )\ndfkorupsi.to_csv('\/kaggle\/working\/datakorupsi.csv')","46a34b4e":"documents = pd.read_csv('..\/input\/datamerekcsv\/datamerekakhir.csv', usecols=['text'])\nprint(documents)","867a3f81":"documentsite = pd.read_csv('..\/input\/dataiteutf8\/datasets_DataITE8.csv', usecols=['text'])\nprint(documentsite)","ccc86874":"from gensim import corpora, models, similarities","4430e060":"import re\ndef preprocess_text(document):\n        # Remove all the special characters\n        document = re.sub(r'\\W', ' ', str(document))\n\n        # remove all single characters\n        document = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', document)\n\n        # Remove single characters from the start\n        document = re.sub(r'\\^[a-zA-Z]\\s+', ' ', document)\n\n        # Substituting multiple spaces with single space\n        document = re.sub(r'\\s+', ' ', document, flags=re.I)\n\n        # Removing prefixed 'b'\n        document = re.sub(r'^b\\s+', '', document)\n\n        # Converting to Lowercase\n        document = document.lower()\n\n        \n        return document","3f96217e":"datadocs=[]\nfor document in documents['text']:\n    print(preprocess_text(document))\n    datadocs.append(preprocess_text(document))","9e07ab6f":"datadocsite=[]\nfor documentite in documentsite['text']:\n    print(preprocess_text(documentite))\n    datadocsite.append(preprocess_text(documentite))","17992816":"print(documents[1:3])","139e7010":"print(datadocs[1:3])","fcdac1b3":"pip install Sastrawi","fde56ea6":"from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\ndataakhir=[]\nfactory = StopWordRemoverFactory()\nstopword = factory.create_stop_word_remover()\nfor datadoc in datadocs:\n     print(stopword.remove(datadoc))\n     dataakhir.append(stopword.remove(datadoc)) ","14331e7c":"from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\ndataakhirite=[]\nfactory = StopWordRemoverFactory()\nstopword = factory.create_stop_word_remover()\nfor datadocite in datadocsite:\n     print(stopword.remove(datadocite))\n     dataakhirite.append(stopword.remove(datadocite)) ","d8c17d69":"from sklearn.feature_extraction.text import CountVectorizer\nfrom scipy.spatial import distance\nfrom sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\nimport pandas as pd\n \n# text to vector\nvectorizer = CountVectorizer()","94bbfe77":"from sklearn.feature_extraction.text import TfidfVectorizer\nvectorizer = TfidfVectorizer()\ntfidfvectorizer = vectorizer.fit_transform(dataakhir)\ntfidf_tokens = vectorizer.get_feature_names()\ntfidf_tokens","3d40aa18":"from sklearn.feature_extraction.text import TfidfVectorizer\nvectorizer = TfidfVectorizer()\ntfidfvectorizer = vectorizer.fit_transform(datadocs)\ntfidf_tokens = vectorizer.get_feature_names()\ntfidf_tokens\n","db7fe146":"tfidf_tokens = tfidfvectorizer.get_feature_names()","e3b7178f":"from sklearn.feature_extraction.text import CountVectorizer\nvectorizer = CountVectorizer()\ncorpus = ['This is the first document.',\n          'This is the second second document.',\n          'And the third one.',\n          'Is this the first document?']\n\nX = vectorizer.fit_transform(corpus)\n\nfeatures = vectorizer.get_feature_names()\n\nfeatures","b6012715":"my_matrix.shape","216f9ccb":"my_matrix.vocabulary","d89fdabe":"print(my_matrix)","03c6f31b":"my_matrix[1,2]","6c8fa36b":" all_sentences_to_vector = vectorizer.fit_transform(dataakhir)","91272e4f":" all_sentences_to_vector.shape\n   ","6335c0fb":"from sklearn.decomposition import TruncatedSVD","9f57035b":"svd = TruncatedSVD(n_components=600, random_state=4)\nX_svd = svd.fit_transform(all_sentences_to_vector)\nprint(f\"Total variance explained: {np.sum(svd.explained_variance_ratio_):.2f}\")\n# The variance explained is quite low for real applications. We will investigate it later.","6a1cf61a":"X_svd.shape","9690129f":" text_to_vector_v1 = X_svd.tolist()\n ","b7dcca1c":"# TfidfVectorizer \n# CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\nimport pandas as pd\n# set of documents\ntrain = ['The sky is blue.','The sun is bright.']\ntest = ['The sun in the sky is bright', 'We can see the shining sun, the bright sun.']\n# instantiate the vectorizer object\ncountvectorizer = CountVectorizer(analyzer= 'word', stop_words='english')\ntfidfvectorizer = TfidfVectorizer(analyzer='word',stop_words= 'english')\n# convert th documents into a matrix\ncount_wm = countvectorizer.fit_transform(train)\ntfidf_wm = tfidfvectorizer.fit_transform(train)\n#retrieve the terms found in the corpora\n# if we take same parameters on both Classes(CountVectorizer and TfidfVectorizer) , it will give same output of get_feature_names() methods)\n#count_tokens = tfidfvectorizer.get_feature_names() # no difference\ncount_tokens = countvectorizer.get_feature_names()\ntfidf_tokens = tfidfvectorizer.get_feature_names()\ndf_countvect = pd.DataFrame(data = count_wm.toarray(),index = ['Doc1','Doc2'],columns = count_tokens)\ndf_tfidfvect = pd.DataFrame(data = tfidf_wm.toarray(),index = ['Doc1','Doc2'],columns = tfidf_tokens)\nprint(\"Count Vectorizer\\n\")\nprint(df_countvect)\nprint(\"\\nTD-IDF Vectorizer\\n\")\nprint(df_tfidfvect)","0acfb320":"from wordcloud import WordCloud","d5834ff9":" all_sentences_to_vector[1,2]","918b8292":"print(dataakhir[1:3])","54fdedf4":"def cosine_distance_countvectorizer_method(s1, s2):\n    \n    # sentences to list\n    allsentences = [s1 , s2]\n    \n    # packages\n    from sklearn.feature_extraction.text import CountVectorizer\n    from scipy.spatial import distance\n    \n    # text to vector\n    vectorizer = CountVectorizer()\n    all_sentences_to_vector = vectorizer.fit_transform(allsentences)\n    text_to_vector_v1 = all_sentences_to_vector.toarray()[0].tolist()\n    text_to_vector_v2 = all_sentences_to_vector.toarray()[1].tolist()\n    \n    # distance of similarity\n    cosine = distance.cosine(text_to_vector_v1, text_to_vector_v2)\n    hasil=round((1-cosine),2)\n    print('Similarity of two sentences are equal to ',hasil,'%')\n    return hasil","767ba5e6":"i=0\ncacah=0\ntotal=0\nwhile (i<49):\n    \n    j=i+1\n    while (j<49):\n        \n        print(i)\n        print(j)\n        cacah=cacah+1\n        total=total+cosine_distance_countvectorizer_method(dataakhir[i] , dataakhir[j])\n        j=j+1\n    i=i+1    \nprint(cacah)\nprint(total)\nprint(total\/cacah)","6b180087":"i=0\ncacah=0\ntotal=0\nwhile (i<49):\n    \n    j=i+1\n    while (j<49):\n        \n        print(i)\n        print(j)\n        cacah=cacah+1\n        total=total+cosine_distance_countvectorizer_method(dataakhirite[i] , dataakhirite[j])\n        j=j+1\n    i=i+1    \nprint(cacah)\nprint(total)\nprint(total\/cacah)","f104fc0a":"def Jaccard_Similarity(doc1, doc2): \n    \n    words_doc1 = set(doc1.lower().split()) \n    words_doc2 = set(doc2.lower().split())\n    \n    intersection = words_doc1.intersection(words_doc2)\n    union = words_doc1.union(words_doc2)\n        \n    return float(len(intersection)) \/ len(union)","a16ba28f":"i=0\ncacah=0\ntotal=0\nwhile (i<49):\n    \n    j=i+1\n    while (j<49):\n        \n        print(i)\n        print(j)\n        cacah=cacah+1\n        print(Jaccard_Similarity(dataakhir[i] , dataakhir[j]))\n        total=total+Jaccard_Similarity(dataakhir[i] , dataakhir[j])\n        j=j+1\n    i=i+1    \nprint(cacah)\nprint(total)\nprint(total\/cacah)","17811218":"i=0\ncacah=0\ntotal=0\nwhile (i<49):\n    \n    j=i+1\n    while (j<49):\n        \n        print(i)\n        print(j)\n        cacah=cacah+1\n        print(Jaccard_Similarity(dataakhirite[i] , dataakhirite[j]))\n        total=total+Jaccard_Similarity(dataakhirite[i] , dataakhirite[j])\n        j=j+1\n    i=i+1    \nprint(cacah)\nprint(total)\nprint(total\/cacah)","aeddf00d":"import nltk\ni=0\ncacah=0\ntotal=0\nwhile (i<49):\n    \n    j=i+1\n    while (j<49):\n        tokens1b = nltk.word_tokenize(dataakhir[i])\n        tokens2b = nltk.word_tokenize(dataakhir[j])\n\n        ng1_tokensb = set(nltk.ngrams(tokens1b, n=3))\n        ng2_tokensb = set(nltk.ngrams(tokens2b, n=3))\n        jdsent12 = nltk.jaccard_distance(ng1_tokensb, ng2_tokensb)\n        cacah=cacah+1\n        total=total+jdsent12\n        print( \"Jaccard Distance between \",i,\" and \",j,\" with ngram 3 = \",jdsent12)\n        j=j+1 \n    i=i+1 \nprint(cacah)\nprint(total)\nprint(total\/cacah)    ","ec40051b":"texts = [[word for word in datadoc.lower().split()] for datadoc in dataakhir]\nprint(texts)\ndictionary = corpora.Dictionary(texts)\nprint(dictionary.token2id)","dbb62c8b":"textsite = [[word for word in datadoc.lower().split()] for datadoc in dataakhirite]\nprint(textsite)\ndictionaryite = corpora.Dictionary(textsite)\nprint(dictionaryite.token2id)","c0ac744b":"corpus = [dictionary.doc2bow(text) for text in texts]\ntfidf = models.TfidfModel(corpus)","45ec9aa5":"corpus_tfidf = tfidf[corpus]\nfor doc in corpus_tfidf:\n    print(doc)\nprint(tfidf.dfs)\nprint(tfidf.idfs)","5a5a2394":"lsi = models.LsiModel(corpus_tfidf, id2word=dictionary, num_topics=10)\nlsi.print_topics(10)","268d8659":"corpus_lsi = lsi[corpus_tfidf]\nfor doc in corpus_lsi:\n    print(doc)","59f478f3":"index = similarities.MatrixSimilarity(lsi[corpus])  ","9286480b":"query = \"tergugat mempunyai persamaan merek pada barang dengan Merek milik pihak penggugat yang sudah terdaftar lebih dahulu untuk barang tersebut\"\nquery_bow = dictionary.doc2bow(query.lower().split())\nprint(query_bow)","92715d6c":"i=0\ncacah=0\ntotal=0\nwhile (i<49):\n    print (i)\n    query = dataakhir[i]\n    query_bow = dictionary.doc2bow(query.lower().split())\n    query_lsi = lsi[query_bow]\n    sims = index[query_lsi]\n    sort_sims = sorted(enumerate(sims), key=lambda item: -item[1])\n    print (sort_sims)\n    i=i+1\n\n","ee8aa744":"i=0\ncacah=0\ntotal=0\nwhile (i<49):\n    print (i)\n    query = dataakhirite[i]\n    query_bow = dictionaryite.doc2bow(query.lower().split())\n    query_lsi = lsi[query_bow]\n    sims = index[query_lsi]\n    sort_sims = sorted(enumerate(sims), key=lambda item: -item[1])\n    print (sort_sims)\n    i=i+1","d0c73f7c":"# import StemmerFactory class\nfrom Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n# create stemmer\nfactory = StemmerFactory()\nstemmer = factory.create_stemmer()\ndataakhir2=[]\n\nfor datadoc1 in dataakhir:\n     print(stemmer.stem(datadoc1))\n     dataakhir2.append(stemmer.stem(datadoc1))  ","577adbbe":"\ndfObj = pd.DataFrame(dataakhir2)\ndfObj.to_csv('\/kaggle\/working\/datamerekakhir2.csv')","9e7d1a42":"i=0\ncacah=0\ntotal=0\nwhile (i<49):\n    \n    j=i+1\n    while (j<49):\n        \n        print(i)\n        print(j)\n        cacah=cacah+1\n        total=total+cosine_distance_countvectorizer_method(dataakhir2[i] , dataakhir2[j])\n        j=j+1\n    i=i+1    \nprint(cacah)\nprint(total)\nprint(total\/cacah)","1f42098b":"i=0\ncacah=0\ntotal=0\nwhile (i<49):\n    \n    j=i+1\n    while (j<49):\n        \n        print(i)\n        print(j)\n        cacah=cacah+1\n        print(Jaccard_Similarity(dataakhir2[i] , dataakhir2[j]))\n        total=total+Jaccard_Similarity(dataakhir2[i] , dataakhir2[j])\n        j=j+1\n    i=i+1    \nprint(cacah)\nprint(total)\nprint(total\/cacah)","e2b2cf8a":"import nltk\ni=0\ncacah=0\ntotal=0\nwhile (i<49):\n    \n    j=i+1\n    while (j<49):\n        tokens1b = nltk.word_tokenize(dataakhir2[i])\n        tokens2b = nltk.word_tokenize(dataakhir2[j])\n\n        ng1_tokensb = set(nltk.ngrams(tokens1b, n=3))\n        ng2_tokensb = set(nltk.ngrams(tokens2b, n=3))\n        jdsent12 = nltk.jaccard_distance(ng1_tokensb, ng2_tokensb)\n        cacah=cacah+1\n        total=total+jdsent12\n        print( \"Jaccard Distance between \",i,\" and \",j,\" with ngram 3 = \",jdsent12)\n        j=j+1 \n    i=i+1 \nprint(cacah)\nprint(total)\nprint(total\/cacah)  ","6c553a5b":"texts = [[word for word in datadoc.lower().split()] for datadoc in dataakhir2]\ndictionary = corpora.Dictionary(texts)\ncorpus = [dictionary.doc2bow(text) for text in texts]\ntfidf = models.TfidfModel(corpus)\ncorpus_tfidf = tfidf[corpus]\nlsi = models.LsiModel(corpus_tfidf, id2word=dictionary, num_topics=10)\ncorpus_lsi = lsi[corpus_tfidf]\nindex = similarities.MatrixSimilarity(lsi[corpus]) ","7dca986f":"i=0\ncacah=0\njumlah=0\nwhile (i<49):\n    print (i)\n    query = dataakhir[i]\n    query_bow = dictionary.doc2bow(query.lower().split())\n    query_lsi = lsi[query_bow]\n    sims = index[query_lsi]\n    sort_sims = sorted(enumerate(sims), key=lambda item: -item[1])\n    #print (sort_sims)\n    i=i+1\n    j=1\n   \n    while (j<=48):\n        jumlah=jumlah+sims[j]\n        j=j+1\n        cacah=cacah+1\n    print(jumlah)\n    print(cacah)\nprint(jumlah\/cacah)    ","9cb4079f":"sims = index[query_lsi]\nprint(list(enumerate(sims)))","26343ad4":"print(sims[1])\njumlah=0\ni=1\nwhile (i<=9):\n    jumlah=jumlah+sims[i]\n    i=i+1               \nprint(jumlah)\n                   ","04eba11f":"sort_sims = sorted(enumerate(sims), key=lambda item: -item[1])\n\nprint (sort_sims)","63480471":"from gensim import corpora, models, similarities\ndocuments = ['karena merek bodycology terdaftar No. IDM000289450 atas nama Tergugat secara jelas mempunyai persamaan pada pokoknya dengan Merek Bodycology milik Penggugat Merek Bodycology milik Penggugat sudah terdaftar jauh sebelum merek bodycology No. IDM000289450 atas nama Tergugat terdaftar di Indonesia maksud dan tujuan Tergugat mengajukan pendaftaran merek bodycology terdaftar No. IDM00028450 adalah untuk membonceng dan menjiplak Merek Bodycology milik Penggugat merupakan bukti itikad tidak baik dari Tergugat dalam mendaftarkan merek tersebut',\n         'Bahwa mengingat dalil diatas menyatakan bahwa Merek  Kuntul dan lukisan  dan Kuntul batu   tersebut mempunyai persamaan pada pokoknya dengan Merek  Sin Kian Hin dan cap Kuntul  yang telah terlebih dahulu didaftar oleh dan atas nama Penggugat untuk barang yang sejenis  hal ini ditunjukkan oleh adanya kemiripan yang disebabkan oleh adanya unsur-unsur yang menonjol antara Merek yang satu dan Merek yang lain  yang dapat menimbulkan kesan adanya persamaan  baik mengenai bentuk  cara penempatan  cara penulisan atau kombinasi antara unsur-unsur ataupun persamaan bunyi ucapan yang terdapat dalam merek Penggugat maupun Tergugat dan Kuntul batu   tersebut mempunyai persamaan pada pokoknya dengan Merek  Sin Kian Hin dan cap Kuntul  yang telah terlebih dahulu didaftar oleh dan atas nama Penggugat untuk barang yang sejenis  hal ini ditunjukkan oleh adanya kemiripan yang disebabkan oleh adanya unsur-unsur yang menonjol antara Merek yang satu dan Merek yang lain  yang dapat menimbulkan kesan adanya persamaan  baik mengenai bentuk  cara penempatan  cara penulisan atau kombinasi antara unsur-unsur ataupun persamaan bunyi ucapan yang terdapat dalam merek Penggugat maupun Tergugat dan Kuntul batu   tersebut mempunyai persamaan pada pokoknya dengan Merek  Sin Kian Hin dan cap Kuntul  yang telah terlebih dahulu didaftar oleh dan atas nama Penggugat untuk barang yang sejenis hal ini ditunjukkan oleh adanya kemiripan yang disebabkan oleh adanya unsur-unsur yang menonjol antara Merek yang satu dan Merek yang lain yang dapat menimbulkan kesan adanya persamaan  baik mengenai bentuk cara penempatan cara penulisan atau kombinasi antara unsur-unsur ataupun persamaan bunyi ucapan yang terdapat dalam merek Penggugat maupun tergugat',\n         'Bahwa berdasarkan fakta-fakta yang disampaikan dalam uraian di atas sudah jelaslah bahwa Penggugat adalah Pencipta dan Pemegang Hak Cipta atas Seni Motif Kertas Nomor 020607']","6bb8c6d0":"for document in documents:\n    print(document)","d5ea38e0":"print(documents[1:3])","dc786996":"import re\ndef preprocess_text(document):\n        # Remove all the special characters\n        document = re.sub(r'\\W', ' ', str(document))\n\n        # remove all single characters\n        document = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', document)\n\n        # Remove single characters from the start\n        document = re.sub(r'\\^[a-zA-Z]\\s+', ' ', document)\n\n        # Substituting multiple spaces with single space\n        document = re.sub(r'\\s+', ' ', document, flags=re.I)\n\n        # Removing prefixed 'b'\n        document = re.sub(r'^b\\s+', '', document)\n\n        # Converting to Lowercase\n        document = document.lower()\n\n        \n        return document","5c994a5e":"datadocs=[]\nfor document in documents:\n    print(preprocess_text(document))\n    datadocs.append(preprocess_text(document))","54ab32b7":"from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\ndataakhir=[]\nfactory = StopWordRemoverFactory()\nstopword = factory.create_stop_word_remover()\nfor datadoc in datadocs:\n     print(stopword.remove(datadoc))\n     dataakhir.append(stopword.remove(datadoc))   ","56afc9dd":"def cosine_distance_countvectorizer_method(s1, s2):\n    \n    # sentences to list\n    allsentences = [s1 , s2]\n    \n    # packages\n    from sklearn.feature_extraction.text import CountVectorizer\n    from scipy.spatial import distance\n    \n    # text to vector\n    vectorizer = CountVectorizer()\n    all_sentences_to_vector = vectorizer.fit_transform(allsentences)\n    text_to_vector_v1 = all_sentences_to_vector.toarray()[0].tolist()\n    text_to_vector_v2 = all_sentences_to_vector.toarray()[1].tolist()\n    \n    # distance of similarity\n    cosine = distance.cosine(text_to_vector_v1, text_to_vector_v2)\n    print('Similarity of two sentences are equal to ',round((1-cosine)*100,2),'%')\n    \n    return cosine","813c441d":"cosine_distance_countvectorizer_method(dataakhir[0] , dataakhir[1])\ncosine_distance_countvectorizer_method(dataakhir[0] , dataakhir[2])\ncosine_distance_countvectorizer_method(dataakhir[1] , dataakhir[2])","c48c7581":"def Jaccard_Similarity(doc1, doc2): \n    \n    words_doc1 = set(doc1.lower().split()) \n    words_doc2 = set(doc2.lower().split())\n    \n    intersection = words_doc1.intersection(words_doc2)\n    union = words_doc1.union(words_doc2)\n        \n    return float(len(intersection)) \/ len(union)","e271c145":"print('Jaccard Similarity of two sentences are equal to ',round(Jaccard_Similarity(dataakhir[0] , dataakhir[1])*100,2),'%')","ae892cdd":"import nltk\ntokens1b = nltk.word_tokenize(dataakhir[0])\ntokens2b = nltk.word_tokenize(dataakhir[1])\n\nng1_tokensb = set(nltk.ngrams(tokens1b, n=3))\nng2_tokensb = set(nltk.ngrams(tokens2b, n=3))\njdsent12 = nltk.jaccard_distance(ng1_tokensb, ng2_tokensb)\n\nprint(jdsent12, \"Jaccard Distance between tokens1 and tokens2 with ngram 3\")","60166515":"# import StemmerFactory class\nfrom Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n# create stemmer\nfactory = StemmerFactory()\nstemmer = factory.create_stemmer()\ndataakhir2=[]\n\nfor datadoc1 in dataakhir:\n     print(stemmer.stem(datadoc1))\n     dataakhir2.append(stemmer.stem(datadoc1))   ","2124a648":"import numpy as np\n\ndef to_str(var):\n    if type(var) is list:\n        return str(var)[1:-1] # list\n    if type(var) is np.ndarray:\n        try:\n            return str(list(var[0]))[1:-1] # numpy 1D array\n        except TypeError:\n            return str(list(var))[1:-1] # numpy sequence\n    return str(var) # everything else","c8617204":"f= open(\"cosinesim\",\"w+\")","517e2121":"i=0\ncacah=0\ntotal=0\nj=0\njarak=0\nwhile (i<49):\n    \n    j=i+1\n    while (j<49):\n        \n        print(i)\n        print(j)\n        cacah=cacah+1\n        jarak=cosine_distance_countvectorizer_method(dataakhir2[i] , dataakhir2[j])\n        total=total+jarak\n        f.write(str(i) +\"  \"+to_str(jarak)+\" \" +str(j)+\" \")\n        j=j+1\n    i=i+1    \nprint(cacah)\nprint(total)\nprint(total\/cacah)","604f32b3":"f.close()","7fc859e1":"f= open(\"jaccardsim\",\"w+\")","100b903f":"i=0\ncacah=0\ntotal=0\njarak=0\nwhile (i<49):\n    \n    j=i+1\n    while (j<49):\n        \n        print(i)\n        print(j)\n        cacah=cacah+1\n        jarak=Jaccard_Similarity(dataakhir2[i] , dataakhir2[j])\n        print(jarak)\n        f.write(str(i) +\"  \"+to_str(jarak)+\" \" +str(j)+\" \")\n        total=total+jarak\n        j=j+1\n    i=i+1    \nprint(cacah)\nprint(total)\nprint(total\/cacah)","a2656f4e":"f.close()","5e2efaa2":"cosine_distance_countvectorizer_method(dataakhir2[0] , dataakhir2[1])\ncosine_distance_countvectorizer_method(dataakhir2[0] , dataakhir2[2])\ncosine_distance_countvectorizer_method(dataakhir2[1] , dataakhir2[2])","49423685":"print('Jaccard Similarity of two sentences are equal to ',round(Jaccard_Similarity(dataakhir2[0] , dataakhir2[1])*100,2),'%')","eb26fb80":"import nltk\ntokens1b = nltk.word_tokenize(dataakhir2[0])\ntokens2b = nltk.word_tokenize(dataakhir2[1])\n\nng1_tokensb = set(nltk.ngrams(tokens1b, n=3))\nng2_tokensb = set(nltk.ngrams(tokens2b, n=3))\njdsent12 = nltk.jaccard_distance(ng1_tokensb, ng2_tokensb)\n\nprint(jdsent12, \"Jaccard Distance between tokens1 and tokens2 with ngram 3\")","877f71a3":"texts = [[word for word in datadoc.lower().split()] for datadoc in datadocs]\nprint(texts)\ndictionary = corpora.Dictionary(texts)\nprint(dictionary.token2id)","5c3a5cd3":"texts = [[word for word in datadoc.lower().split()] for datadoc in dataakhir2]\ndictionary = corpora.Dictionary(texts)\ncorpus = [dictionary.doc2bow(text) for text in texts]\ntfidf = models.TfidfModel(corpus)\ncorpus_tfidf = tfidf[corpus]\nlsi = models.LsiModel(corpus_tfidf, id2word=dictionary, num_topics=10)\ncorpus_lsi = lsi[corpus_tfidf]\nindex = similarities.MatrixSimilarity(lsi[corpus]) ","f5973dd4":"import csv\ni=0\ncacah=0\njumlah=0\nwhile (i<49):\n    print (i)\n    query = dataakhir2[i]\n    query_bow = dictionary.doc2bow(query.lower().split())\n    query_lsi = lsi[query_bow]\n    sims = index[query_lsi]\n    sort_sims = sorted(enumerate(sims), key=lambda item: -item[1])\n    with open('tmp_file'+str(i)+'.txt', 'w') as f:\n        csv.writer(f, delimiter=' ').writerows(sort_sims)\n    #print (sort_sims)\n    i=i+1\n    j=1\n   \n    while (j<=48):\n        jumlah=jumlah+sims[j]\n        j=j+1\n        cacah=cacah+1\n    print(jumlah)\n    print(cacah)\nprint(jumlah\/cacah)  ","94dda13c":"corpus3 = [dictionary3.doc2bow(text3) for text3 in texts3]\ntfidf3 = models.TfidfModel(corpus3)","bdf03b5c":"corpus_tfidf3 = tfidf3[corpus3]\nfor doc3 in corpus_tfidf3:\n    print(doc3)\nprint(tfidf3.dfs)\nprint(tfidf3.idfs)","83351cc5":"lsi = models.LsiModel(corpus_tfidf, id2word=dictionary, num_topics=2)\nlsi.print_topics(2)","02167187":"corpus_lsi = lsi[corpus_tfidf]\nfor doc in corpus_lsi:\n    print(doc)","3e2cde28":"index = similarities.MatrixSimilarity(lsi[corpus])  ","fc83e762":"query = \"tergugat mempunyai persamaan merek pada barang dengan Merek milik pihak penggugat yang sudah terdaftar lebih dahulu untuk barang tersebut\"\nquery_bow = dictionary.doc2bow(query.lower().split())\nprint(query_bow)","3e710814":"query_lsi = lsi[query_bow]\nprint (query_lsi)","6e9077a1":"sims = index[query_lsi]\nprint(list(enumerate(sims)))","59f0289b":"sort_sims = sorted(enumerate(sims), key=lambda item: -item[1])\nprint (sort_sims)","5bc57d0e":"lda = models.LdaModel(corpus_tfidf, id2word=dictionary, num_topics=2)\nlda.print_topics(2)","2d39c886":"query2 = \"karena merek bodycology terdaftar No. IDM000289450 atas nama Tergugat secara jelas mempunyai persamaan pada pokoknya dengan Merek Bodycology milik Penggugat Merek Bodycology milik Penggugat sudah terdaftar jauh sebelum merek bodycology No. IDM000289450 atas nama Tergugat terdaftar di Indonesia maksud dan tujuan Tergugat mengajukan pendaftaran merek bodycology terdaftar No. IDM00028450 adalah untuk membonceng dan menjiplak Merek Bodycology milik Penggugat merupakan bukti itikad tidak baik dari Tergugat dalam mendaftarkan merek tersebut\"\nquery_bow2 = dictionary.doc2bow(query2.lower().split())\nprint(query_bow2)","e15d1956":"query_lsi2 = lsi[query_bow2]\nprint (query_lsi2)","a2a3521e":"sims2 = index[query_lsi2]\nprint(list(enumerate(sims2)))","27ad69a1":"sort_sims2 = sorted(enumerate(sims2), key=lambda item: -item[1])\nprint (sort_sims2)","bf95cbd7":"import csv\nimport numpy as np\nimport string\nimport re\nfrom nltk import word_tokenize, pos_tag\nfrom nltk.corpus import wordnet, stopwords\nfrom nltk.stem.wordnet import WordNetLemmatizer\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.stem.porter import PorterStemmer\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer\nfrom sklearn import preprocessing\nfrom sklearn import cross_validation\nfrom sklearn import linear_model\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.svm import SVC\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.ensemble import RandomForestClassifier, VotingClassifier\nfrom sklearn.metrics import precision_score, accuracy_score, recall_score\nfrom sklearn.decomposition import TruncatedSVD\n\n\n#reading the data from the CSV file\ndef create_data(file):\n    with open(file, encoding = 'utf8') as csv_file:\n        reader = csv.reader(csv_file, delimiter = \",\", quotechar = '\"')\n        title = []\n        description = []\n        leafnode = []\n        for row in reader:\n            title.append(row[0])\n            description.append(row[1])\n            leafnode.append(row[2])\n        return title[1:], description[1:], leafnode[1:]\n\n    \n#removing stopwords, punctuations and special characters from the description\n#lemmatizing the words\ndef clean_data(text):\n    wordnet_lemmatizer = WordNetLemmatizer()\n    stops = stopwords.words('english')\n    nonan = re.compile(r'[^a-zA-Z ]')\n    output = []\n    for i in range(len(text)):\n        sentence = nonan.sub('', text[i])\n        words = word_tokenize(sentence.lower())\n        filtered_words = [w for w in words if not w.isdigit() and not w in stops and not w in string.punctuation]\n        tags = pos_tag(filtered_words)\n        cleaned = ''\n        for word, tag in tags:\n          if tag == 'NN' or tag == 'NNS' or tag == 'VBZ' or tag == 'JJ' or tag == 'RB' or tag == 'NNP' or tag == 'NNPS' or tag == 'RBR':\n            cleaned = cleaned + wordnet_lemmatizer.lemmatize(word) + ' '\n        output.append(cleaned.strip())\n    return output\n\n\n#feature extraction - creating a tf-idf matrix\ndef tfidf(data, ma = 0.6, mi = 0.0001):\n    tfidf_vectorize = TfidfVectorizer(max_df = ma, min_df = mi)\n    tfidf_data = tfidf_vectorize.fit_transform(data)\n    return tfidf_data\n\n\n#Naive Bayes classifier\ndef test_NaiveBayes(x_train, x_test, y_train, y_test):\n    MNB = MultinomialNB()\n    NBClassifier = MNB.fit(x_train, y_train)\n    predictions = NBClassifier.predict(x_test)\n\ta = accuracy_score(y_test, predictions)\n    p = precision_score(y_test, predictions, average = 'weighted')\n    r = recall_score(y_test, predictions, average = 'weighted')\n    return p, r\n\n\n#SVM classifier\ndef test_SVM(x_train, x_test, y_train, y_test, label_names):\n    SVM = SVC(kernel = 'linear')\n    SVMClassifier = SVM.fit(x_train, y_train)\n    predictions = SVMClassifier.predict(x_test)\n\ta = accuracy_score(y_test, predictions)\n    p = precision_score(y_test, predictions, average = 'weighted')\n    r = recall_score(y_test, predictions, average = 'weighted')\n    return p, r\n\n\n#Multilayer Perceptron classfier\ndef test_NN(x_train, x_test, y_train, y_test):\n    NN = MLPClassifier(solver = 'lbfgs', alpha = 0.00095, learning_rate = 'adaptive', learning_rate_init = 0.005, max_iter = 300, random_state = 0)\n    Perceptron = NN.fit(x_train, y_train)\n    predictions = Perceptron.predict(x_test)\n\ta = accuracy_score(y_test, predictions)\n    p = precision_score(y_test, predictions, average = 'weighted')\n    r = recall_score(y_test, predictions, average = 'weighted')\n    return p, r\n\n\n#SGD classifier\n#classifier that minimizes the specified loss using stochastic gradient descent\n#hinge loss works pretty well too, the modified huber reports highest precision\ndef test_SGD(x_train, x_test, y_train, y_test):\n    SGD = SGDClassifier(loss = 'modified_huber')\n    SGDC = SGD.fit(x_train1, y_train)\n    predictions = SGDC.predict(x_test1)\n    a = accuracy_score(y_test, predictions)\n    p = precision_score(y_test, predictions, average = 'weighted')\n    r = recall_score(y_test, predictions, average = 'weighted')\n    return p, r\n\n\n#Voting Classifiers\n#SVC and SGD combined with equal weights gave 1% lower precision than SVC\ndef test_voting(x_train, x_test, y_train, y_test):\n    SVM = SVC(kernel = 'linear', probability = True)\n    SGD = SGDClassifier(loss = 'modified_huber')\n    EnsembleClassifier = VotingClassifier(estimators = [('sgd', SGD), ('svc', SVM)], voting = 'soft', weights = [1,1])\n    EnsembleClassifier = EnsembleClassifier.fit(x_train, y_train)\n    predictions = EnsembleClassifier.predict(x_test)\n\ta = accuracy_score(y_test, predictions)\n    p = precision_score(y_test, predictions, average = 'weighted')\n    r = recall_score(y_test, predictions, average = 'weighted')\n    return p, r\n\n\n\t\n\t\n\n#read data from file\ntitle, desc, leaf = create_data(file)\n\n#clean the data\ntitle = clean_data(title)\ndesc = clean_data(desc)\n\n#joining the title and description to create a single text document for each product\ncombined = desc[:]\nfor i in range(len(combined)):\n    combined[i] = title[i] + ' ' + desc[i]\n\n#feature extraction\ntraining = tfidf(combined)\n\n#training and test data splits\nx_train, x_test, y_train, y_test = cross_validation.train_test_split(training, leaf, test_size = 0.25, random_state = 0)\n\n#test a classifier\naccuracy, precision, recall = test_SVM(x_train, x_test, y_train, y_test)","fb71a088":"https:\/\/www.geeksforgeeks.org\/measure-similarity-between-images-using-python-opencv\/?ref=rp"}}