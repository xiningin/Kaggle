{"cell_type":{"43a21544":"code","0b839a70":"code","ca3854b1":"code","aba06b25":"code","26c6d17d":"code","6999ec16":"code","51686546":"code","d0372e35":"code","3370d3f2":"code","4bf2326f":"code","c1aaef31":"code","9daea0a2":"code","2033e3b0":"code","ff5c4ec7":"code","0d47ac0e":"code","4a4f188d":"code","08235840":"code","5d8030ce":"code","5e9379f6":"code","b807b416":"markdown","96813e1c":"markdown","7bce0747":"markdown","af91784f":"markdown","90d075b3":"markdown"},"source":{"43a21544":"# importing necessary libraries\nimport numpy as np\nimport pandas as pd\n\nimport torch\nimport torchtext\nimport torch.nn as nn\nfrom torchtext.data.utils import get_tokenizer\nfrom torchtext.vocab import Vocab\nfrom torch.nn.utils.rnn import pad_sequence\nfrom torch.utils.data import DataLoader, Dataset, TensorDataset\n\nfrom collections import Counter","0b839a70":"# Downloading the text file containing poem sample\n!wget https:\/\/storage.googleapis.com\/laurencemoroney-blog.appspot.com\/irish-lyrics-eof.txt -O sample.txt ","ca3854b1":"data = open('sample.txt').read()\nsent_corpus = data.lower().split(\"\\n\")\n\nsent_corpus[:5] # few sample lines of poem","aba06b25":"def build_vocab(corpus, tokenizer):\n    counter = Counter()\n    for text in corpus:\n        counter.update(tokenizer(text))\n        \n    return Vocab(counter)","26c6d17d":"# building a vocabulary and tokenizer\ntokenizer = get_tokenizer('spacy', language='en')\nvocab = build_vocab(sent_corpus, tokenizer)","6999ec16":"def data_process(corpus, vocab):\n    data = list()\n    for text in corpus:\n        token_list = [vocab[token] for token in tokenizer(text)]\n        for i in range(1, len(token_list)):\n            n_gram_seq = torch.tensor(token_list[:i+1], dtype=torch.long)\n            data.append(n_gram_seq)\n    return data","51686546":"train_data = data_process(sent_corpus, vocab)","d0372e35":"train_data[:10]  # The number below represents the index of the words in the vocab","3370d3f2":"X = [i[:-1] for i in train_data]   # taking all the words except the last in the input set\ny = [i[-1] for i in train_data]    # taking last words in the output set","4bf2326f":"X[3], y[3]","c1aaef31":"X = pad_sequence(X, batch_first=True, padding_value=vocab.stoi['<pad>'])   # padding the text_seq so the training sample are of equal length\ny = torch.from_numpy(np.array(y))","9daea0a2":"train_data = TensorDataset(X, y)","2033e3b0":"BATCH_SIZE = 32\ntrain_dataloader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)","ff5c4ec7":"DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nDEVICE","0d47ac0e":"class Net(nn.Module):\n    def __init__(self, vocab_size, embedding_dim, hidden_size, num_layers, dropout=0.15):\n        super().__init__()\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        self.vocab_size = vocab_size\n        \n        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n        self.lstm = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_size, num_layers=num_layers, dropout=dropout, batch_first=True)\n        self.fc1 = nn.Linear(hidden_size, vocab_size)\n        \n    def forward(self, X, h=None, c=None):\n        if h is None:\n            h, c = self.init_state(X.size(0))\n        out = self.embedding(X)\n        out, (h, c) = self.lstm(out, (h, c))\n        out = out.contiguous().view(-1, self.hidden_size)\n        out = self.fc1(out)\n        out = out.view(-1, X.size(1), self.vocab_size)\n        out = out[:, -1]\n        \n        return out, h, c\n    \n    def init_state(self, batch_size):\n        num_l = self.num_layers\n        hidden = torch.zeros(num_l, batch_size, self.hidden_size).to(DEVICE)\n        cell = torch.zeros(num_l, batch_size, self.hidden_size).to(DEVICE)\n        return hidden, cell","4a4f188d":"VOCAB_SIZE = len(vocab)\nEMBEDDING_DIM = 128\nHIDDEN_SIZE = 128\nNUM_LAYERS = 3","08235840":"model = Net(VOCAB_SIZE, EMBEDDING_DIM, HIDDEN_SIZE, NUM_LAYERS).to(DEVICE)\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\ncriterion = nn.CrossEntropyLoss()","5d8030ce":"EPOCHS = 100\n\nfor epoch in range(EPOCHS):\n    epoch_loss = 0\n    for X, y in train_dataloader:\n        X = X.to(DEVICE)\n        y = y.to(DEVICE)\n        \n        optimizer.zero_grad()\n        output, h, c = model(X)\n        loss = criterion(output, y)\n        epoch_loss += loss\n        loss.backward()\n        nn.utils.clip_grad_norm_(model.parameters(), 5) # Clipping Gradients\n        optimizer.step()\n        \n    print(f\"Epoch: {epoch+1} Loss:{epoch_loss\/len(train_dataloader)}\")","5e9379f6":"seed_text = \"you should chin up\" #Starting of a song\nnext_words = 50\n\nfor i in range(next_words):\n    token_list = np.ones(21, dtype=int)\n    text_token = np.array([vocab[token] for token in tokenizer(seed_text)])\n    if len(text_token)>21:text_token = text_token[-21:]\n    token_list[:len(text_token)] = text_token\n    token_list = torch.from_numpy(token_list).unsqueeze(0).to(DEVICE)\n    \n    \n    out,h,c = model(token_list)\n    \n    idx = torch.argmax(out)\n    seed_text += \" \" + vocab.itos[idx]\n    \nfor i,word in enumerate(seed_text.split()):\n    print(word,end=\" \"),\n    if i!=0 and (i+1)%5==0:\n        print(\"\\n\")","b807b416":"## The function below forms a n_gram sequence\nEg: if your sentence is ***'you are a good person'*** the function generates:\n* [\"you\", \"are\"]\n* [\"you\", \"are\", \"a\"]\n* [\"you\", \"are\", \"a\", \"good\"]\n* [\"you\", \"are\", \"a\", \"good\", \"person\"]\n<\/br>\n<\/br>\nThe above sentences are then conveted to their respective indices in the vocabulary.","96813e1c":"# Training","7bce0747":"# Poem Generator\nThis notebooks describes how we can generate text in pytorch using LSTM. The data used for training purpose is fetched from [here](https:\/\/storage.googleapis.com\/laurencemoroney-blog.appspot.com\/irish-lyrics-eof.txt). ","af91784f":"# Generating the Poem","90d075b3":"# Defining a Model"}}