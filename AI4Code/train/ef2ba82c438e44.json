{"cell_type":{"b82b3c3d":"code","8e1bf922":"code","71618c03":"code","601edf00":"code","240d2981":"code","7fd8a315":"code","0996fd20":"code","050dd207":"code","d7ae29bd":"code","407f5e7a":"code","d12b1ffe":"code","1469d92d":"code","9fce10c5":"code","13437c7f":"code","c906f0b1":"code","db8c77f6":"code","167d9b0c":"code","e3e6f430":"code","410f457c":"code","3304fdaf":"markdown","0dd8992e":"markdown","82d95d99":"markdown","9ab9d1b1":"markdown","4e5954ad":"markdown","b4cb3163":"markdown"},"source":{"b82b3c3d":"import numpy as np\nimport pandas as pd\nimport os,psutil\nfrom sklearn.model_selection import KFold,StratifiedKFold\nfrom lightgbm import LGBMClassifier\nfrom sklearn.metrics import roc_auc_score\nimport gc\nfrom optuna.integration import lightgbm as lgb\nfrom sklearn.feature_selection import SelectKBest, f_classif\nimport matplotlib.pyplot as plt","8e1bf922":"train = pd.read_csv(\"..\/input\/tabular-playground-series-oct-2021\/train.csv\")\ntest = pd.read_csv(\"..\/input\/tabular-playground-series-oct-2021\/test.csv\")\nsample = pd.read_csv(\"..\/input\/tabular-playground-series-oct-2021\/sample_submission.csv\")","71618c03":"train.head()","601edf00":"def cpu_stats():\n    pid = os.getpid()\n    py = psutil.Process(pid)\n    memory_use = py.memory_info()[0] \/ 2. ** 30\n    return 'memory GB:' + str(np.round(memory_use, 2))\n\ndef reduce_memory_usage(df, verbose=True):\n    numerics = [\"int8\", \"int16\", \"int32\", \"int64\", \"float16\", \"float32\", \"float64\"]\n    start_mem = df.memory_usage().sum() \/ 1024 ** 2\n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == \"int\":\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)\n            else:\n                if (\n                    c_min > np.finfo(np.float16).min\n                    and c_max < np.finfo(np.float16).max\n                                    ):\n                    df[col] = df[col].astype(np.float16)\n                elif (\n                    c_min > np.finfo(np.float32).min\n                    and c_max < np.finfo(np.float32).max\n                ):\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n    end_mem = df.memory_usage().sum() \/ 1024 ** 2\n    if verbose:\n        print(\n            \"Mem. usage decreased to {:.2f} Mb ({:.1f}% reduction)\".format(\n                end_mem, 100 * (start_mem - end_mem) \/ start_mem\n            )\n        )\n    return df\n\ntrain = reduce_memory_usage(train, verbose=True)\ntest = reduce_memory_usage(test, verbose=True)\nprint(cpu_stats())\nprint('Memory reduced')","240d2981":"features = []\ncategorical = []\nnumerical = []\nfor feature in train.columns:\n    if feature not in ['id', 'target']:\n        features.append(feature)\n        if train[feature].dtypes=='int8':\n            categorical.append(feature)\n        if train[feature].dtypes=='float16':\n            numerical.append(feature)\nprint(\"Size of train dataframe\",train.shape)\nprint(\"Total number of categorical features is \", len(categorical))\nprint(\"Total number of numerical features is\", len(numerical))","7fd8a315":"train['target'].value_counts()","0996fd20":"train.isnull().sum()","050dd207":"test.isnull().sum()","d7ae29bd":"y = train['target']\ntrain = train.drop(columns = ['target', 'id'])","407f5e7a":"from sklearn.preprocessing import RobustScaler\nscaler = RobustScaler()\ntrain[numerical] = scaler.fit_transform(train[numerical])\ntest[numerical] = scaler.transform(test[numerical])","d12b1ffe":"def SelectKBestFeatures(features, target, threshold):\n    kbest = SelectKBest(score_func = f_classif, k = len(features.columns))\n    X = kbest.fit_transform(features, target.values.ravel())\n    print('Before the SelectKBest =',features.shape)\n    \n    selected_features = []\n    \n    for i in range(len(features.columns)):\n        if kbest.pvalues_[i]<=threshold:\n            selected_features.append(features.columns[i])\n            \n    X_selected =  pd.DataFrame(X)\n    X_selected.columns = features.columns\n    X_selected = X_selected[selected_features]\n    \n    print('After the SelectKBest = ', X_selected.shape)\n    \n    return X_selected, selected_features\n    ","1469d92d":"from sklearn.feature_selection import SelectKBest, f_classif\np_feature = 0.0001\ntrain_numerical, selected_numerical = SelectKBestFeatures(train[numerical], y, p_feature)","9fce10c5":"train_categorical, selected_categorical = SelectKBestFeatures(train[categorical], y, p_feature)","13437c7f":"cols = selected_numerical + selected_categorical\nX = pd.concat([train_numerical,train_categorical], axis=1)\ntest = test[cols]\nY = y\nprint(\"Shape of Final X \", X.shape)\nprint(\"Shape of final Y \", Y.shape)","c906f0b1":"params={'reg_alpha': 8.158768860412389, 'reg_lambda': 8.793022151019823, 'colsample_bytree': 0.2, 'subsample': 0.4, 'learning_rate': 0.02,\n       'max_depth': 100, 'num_leaves': 12, 'min_child_samples': 68, 'cat_smooth': 91,'objective': 'binary',  \n            'random_state': 48,'n_estimators': 20000,'n_jobs': -1}","db8c77f6":"preds = np.zeros(test.shape[0])\n\nkf = StratifiedKFold(n_splits = 5, random_state=20210,shuffle=True)\n\nauc = []\nn = 0\n\nfor train_idx, test_idx in kf.split(X,Y):\n    X_train, X_val = X.iloc[train_idx], X.iloc[test_idx]\n    y_train, y_val = Y.iloc[train_idx], Y.iloc[test_idx]\n    model = LGBMClassifier(**params)\n    model.fit(X_train, y_train, eval_set = [(X_val,y_val)], early_stopping_rounds = 100, eval_metric = \"auc\", verbose = \"False\")\n    preds += model.predict_proba(test)[:,1]\/kf.n_splits\n    auc.append(roc_auc_score(y_val, model.predict_proba(X_val)[:, 1]))\n    gc.collect()\n    print(f\"fold: {n+1}, auc: {auc[n]}\")\n    n+=1      ","167d9b0c":"np.mean(auc)","e3e6f430":"lgb.plot_importance(model, max_num_features=40, figsize=(10,10))\nplt.show()","410f457c":"sample['target']=preds\nsample.to_csv('submission.csv', index=False)","3304fdaf":"RobustScaler - Scale features using statistics that are robust to outliers. This Scaler removes the median and scales the data according to the quantile range (defaults to IQR: Interquartile Range). The IQR is the range between the 1st quartile (25th quantile) and the 3rd quartile (75th quantile).\n\nRobust scaler is used here for non-categorical columns...","0dd8992e":"To make the code efficent we should reduce memory usage...","82d95d99":"We will select some best features and drop others using SelectKBest. ","9ab9d1b1":"Identify the categorical and continuous features of the dataset.","4e5954ad":"Check if there are any missing values...","b4cb3163":"StratifiedKFold - Provides train\/test indices to split data in train\/test sets.\nThis cross-validation object is a variation of KFold that returns stratified folds. The folds are made by preserving the percentage of samples for each class.\n"}}