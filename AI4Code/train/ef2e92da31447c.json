{"cell_type":{"2583c24d":"code","83862e63":"code","567db2ce":"code","44d1176b":"code","5b58e340":"code","fdca5164":"code","9e1ed77f":"code","11a50d1c":"code","9ed02d46":"code","0f0e18da":"code","287710dd":"code","b1c0cda4":"code","d33d6318":"code","8090e6fc":"code","317aa42e":"code","98ed1773":"code","31613e20":"code","838e936a":"code","59a8db34":"code","b9bd0103":"code","dec3cf97":"code","6696fc06":"code","3b312139":"code","d0d73798":"code","96b690e3":"code","9bc5b162":"code","bb80b183":"code","3616d24d":"code","7c97698e":"markdown","ebdf23f8":"markdown","25ddad2c":"markdown","fbef5817":"markdown","e9280661":"markdown","f3df739a":"markdown","051aa572":"markdown","6542209b":"markdown","97e7d8a8":"markdown","c54aa8f1":"markdown","6179d8c0":"markdown","51242566":"markdown","e12491ad":"markdown","4e84d64d":"markdown","d20d01aa":"markdown","d1551292":"markdown","bca41607":"markdown","93073926":"markdown","4c804fca":"markdown"},"source":{"2583c24d":"import pandas as pd\nimport numpy as np\nfrom pathlib import Path \nfrom tqdm import tqdm_notebook\nfrom fastai import *\nfrom fastai.text import *","83862e63":"puncts = [',', '.', '\"', ':', ')', '(', '-', '!', '?', '|', ';', \"'\", '$', '&', '\/', '[', ']', '>', '%', '=', '#', '*', '+', '\\\\', '\u2022',  '~', '@', '\u00a3', \n '\u00b7', '_', '{', '}', '\u00a9', '^', '\u00ae', '`',  '<', '\u2192', '\u00b0', '\u20ac', '\u2122', '\u203a',  '\u2665', '\u2190', '\u00d7', '\u00a7', '\u2033', '\u2032', '\u00c2', '\u2588', '\u00bd', '\u00e0', '\u2026', \n '\u201c', '\u2605', '\u201d', '\u2013', '\u25cf', '\u00e2', '\u25ba', '\u2212', '\u00a2', '\u00b2', '\u00ac', '\u2591', '\u00b6', '\u2191', '\u00b1', '\u00bf', '\u25be', '\u2550', '\u00a6', '\u2551', '\u2015', '\u00a5', '\u2593', '\u2014', '\u2039', '\u2500', \n '\u2592', '\uff1a', '\u00bc', '\u2295', '\u25bc', '\u25aa', '\u2020', '\u25a0', '\u2019', '\u2580', '\u00a8', '\u2584', '\u266b', '\u2606', '\u00e9', '\u00af', '\u2666', '\u00a4', '\u25b2', '\u00e8', '\u00b8', '\u00be', '\u00c3', '\u22c5', '\u2018', '\u221e', \n '\u2219', '\uff09', '\u2193', '\u3001', '\u2502', '\uff08', '\u00bb', '\uff0c', '\u266a', '\u2569', '\u255a', '\u00b3', '\u30fb', '\u2566', '\u2563', '\u2554', '\u2557', '\u25ac', '\u2764', '\u00ef', '\u00d8', '\u00b9', '\u2264', '\u2021', '\u221a', ]\n\ndef clean_text(x):\n    x = str(x)\n    for punct in puncts:\n        x = x.replace(punct, f' {punct} ')\n    return x\n\ndef clean_numbers(x):\n    x = re.sub('[0-9]{5,}', '#####', x)\n    x = re.sub('[0-9]{4}', '####', x)\n    x = re.sub('[0-9]{3}', '###', x)\n    x = re.sub('[0-9]{2}', '##', x)\n    return x\n\nmispell_dict = {\"aren't\" : \"are not\",\n\"can't\" : \"cannot\",\n\"couldn't\" : \"could not\",\n\"didn't\" : \"did not\",\n\"doesn't\" : \"does not\",\n\"don't\" : \"do not\",\n\"hadn't\" : \"had not\",\n\"hasn't\" : \"has not\",\n\"haven't\" : \"have not\",\n\"he'd\" : \"he would\",\n\"he'll\" : \"he will\",\n\"he's\" : \"he is\",\n\"i'd\" : \"I would\",\n\"i'd\" : \"I had\",\n\"i'll\" : \"I will\",\n\"i'm\" : \"I am\",\n\"isn't\" : \"is not\",\n\"it's\" : \"it is\",\n\"it'll\":\"it will\",\n\"i've\" : \"I have\",\n\"let's\" : \"let us\",\n\"mightn't\" : \"might not\",\n\"mustn't\" : \"must not\",\n\"shan't\" : \"shall not\",\n\"she'd\" : \"she would\",\n\"she'll\" : \"she will\",\n\"she's\" : \"she is\",\n\"shouldn't\" : \"should not\",\n\"that's\" : \"that is\",\n\"there's\" : \"there is\",\n\"they'd\" : \"they would\",\n\"they'll\" : \"they will\",\n\"they're\" : \"they are\",\n\"they've\" : \"they have\",\n\"we'd\" : \"we would\",\n\"we're\" : \"we are\",\n\"weren't\" : \"were not\",\n\"we've\" : \"we have\",\n\"what'll\" : \"what will\",\n\"what're\" : \"what are\",\n\"what's\" : \"what is\",\n\"what've\" : \"what have\",\n\"where's\" : \"where is\",\n\"who'd\" : \"who would\",\n\"who'll\" : \"who will\",\n\"who're\" : \"who are\",\n\"who's\" : \"who is\",\n\"who've\" : \"who have\",\n\"won't\" : \"will not\",\n\"wouldn't\" : \"would not\",\n\"you'd\" : \"you would\",\n\"you'll\" : \"you will\",\n\"you're\" : \"you are\",\n\"you've\" : \"you have\",\n\"'re\": \" are\",\n\"wasn't\": \"was not\",\n\"we'll\":\" will\",\n\"didn't\": \"did not\",\n\"tryin'\":\"trying\"}\n\ndef _get_mispell(mispell_dict):\n    mispell_re = re.compile('(%s)' % '|'.join(mispell_dict.keys()))\n    return mispell_dict, mispell_re\n\nmispellings, mispellings_re = _get_mispell(mispell_dict)\ndef replace_typical_misspell(text):\n    def replace(match):\n        return mispellings[match.group(0)]\n    return mispellings_re.sub(replace, text)","567db2ce":"lm = True","44d1176b":"if lm: path = Path('..\/input'); list(path.iterdir())","5b58e340":"train_df = pd.read_csv(path\/'train.csv')\ntrain_df[\"question_text\"] = train_df[\"question_text\"].apply(lambda x: x.lower())\ntrain_df[\"question_text\"] = train_df[\"question_text\"].apply(lambda x: clean_text(x))\ntrain_df[\"question_text\"] = train_df[\"question_text\"].apply(lambda x: clean_numbers(x))\ntrain_df[\"question_text\"] = train_df[\"question_text\"].apply(lambda x: replace_typical_misspell(x))","fdca5164":"if lm:\n    bs = 48\n    data_lm = (TextList.from_df(train_df, path, cols='question_text')\n                .random_split_by_pct(0.1)\n                .label_for_lm()           \n                .databunch(path='.', bs=bs))","9e1ed77f":"if lm:\n    data_lm.save('tmp_lm')\n    data_lm = TextLMDataBunch.load('.', 'tmp_lm', bs=bs)","11a50d1c":"if lm:\n    learn = language_model_learner(data_lm, drop_mult=0.3, emb_sz=300)\n    learn.unfreeze()","9ed02d46":"if lm: learn.fit_one_cycle(1, 1e-2, moms=(0.8,0.7))","0f0e18da":"if lm: learn.save('language_model')","287710dd":"if lm: learn.save_encoder('lm_encoder')","b1c0cda4":"class TextClasDataBunch(TextDataBunch):\n    \"Create a `TextDataBunch` suitable for training an RNN classifier.\"\n    @classmethod\n    def create(cls, train_ds, valid_ds, test_ds=None, path:PathOrStr='.', bs=64, pad_idx=1, pad_first=True,\n               no_check:bool=False, shuffle=[True, True, False], **kwargs) -> DataBunch:\n        \"Function that transform the `datasets` in a `DataBunch` for classification.\"\n        datasets = [train_ds, valid_ds, test_ds]\n        collate_fn = partial(pad_collate, pad_idx=pad_idx, pad_first=pad_first)\n        train_sampler = SortishSampler(datasets[0].x, key=lambda t: len(datasets[0][t][0].data), bs=bs\/\/2)\n        train_dl = DataLoader(datasets[0], batch_size=bs\/\/2, sampler=train_sampler, drop_last=True, **kwargs)\n        dataloaders = [train_dl]\n        dataloaders.append(DataLoader(datasets[1], batch_size=bs, **kwargs))\n        dataloaders.append(DataLoader(datasets[2], batch_size=bs, **kwargs))\n        return cls(*dataloaders, path=path, collate_fn=collate_fn)\n    \nTextList._bunch = TextClasDataBunch","d33d6318":"path = Path('..\/input'); list(path.iterdir())","8090e6fc":"if lm:\n    train_df = pd.read_csv(path\/'train.csv').sample(frac=0.3, random_state=42)\nelse:\n    train_df = pd.read_csv(path\/'train.csv').sample(frac=0.9, random_state=42)\n\ntrain_df[\"question_text\"] = train_df[\"question_text\"].apply(lambda x: x.lower())\ntrain_df[\"question_text\"] = train_df[\"question_text\"].apply(lambda x: clean_text(x))\ntrain_df[\"question_text\"] = train_df[\"question_text\"].apply(lambda x: clean_numbers(x))\ntrain_df[\"question_text\"] = train_df[\"question_text\"].apply(lambda x: replace_typical_misspell(x))\n#train0 = train_df[train_df.target==0].sample(n=100000, random_state=42)\n#train1 = train_df[train_df.target==1].sample(n=100000, random_state=42, replace=True)\n#train = pd.concat((train0, train1)); len(train)\n#train.reset_index(inplace=True, drop=True)","317aa42e":"test_df = pd.read_csv(path\/'test.csv')\ntest_df[\"question_text\"] = test_df[\"question_text\"].apply(lambda x: x.lower())\ntest_df[\"question_text\"] = test_df[\"question_text\"].apply(lambda x: clean_text(x))\ntest_df[\"question_text\"] = test_df[\"question_text\"].apply(lambda x: clean_numbers(x))\ntest_df[\"question_text\"] = test_df[\"question_text\"].apply(lambda x: replace_typical_misspell(x))","98ed1773":"test = TextList.from_df(test_df, path, cols='question_text')","31613e20":"if lm:\n    data = (TextList.from_df(train_df, path, cols='question_text', vocab=data_lm.vocab)\n                    .random_split_by_pct(0.1)\n                    .label_from_df(cols=2)\n                    .add_test(test)\n                    .databunch(path='.')) \nelse:\n    data = (TextList.from_df(train_df, path, cols='question_text')\n                .random_split_by_pct(0.1)\n                .label_from_df(cols=2)\n                .add_test(test)\n                .databunch(path='.')) ","838e936a":"f_score = Fbeta_binary(beta2=1,clas = 1)","59a8db34":"learn = text_classifier_learner(data, drop_mult=0.5, metrics=[accuracy, f_score], emb_sz=300)\nif lm: learn.load_encoder('lm_encoder')\nlearn.freeze()","b9bd0103":"gc.collect();","dec3cf97":"learn.fit_one_cycle(1, 1e-2, moms=(0.8,0.7))","6696fc06":"learn.unfreeze()\nlearn.fit_one_cycle(2, slice(1e-3\/(2.6**4),1e-3), moms=(0.8,0.7))","3b312139":"preds = learn.get_preds(DatasetType.Valid)\nproba = to_np(preds[0][:,1])\nytrue = to_np(preds[1])","d0d73798":"from sklearn.metrics import roc_curve, precision_recall_curve\ndef threshold_search(y_true, y_proba, plot=False):\n    precision, recall, thresholds = precision_recall_curve(y_true, y_proba)\n    thresholds = np.append(thresholds, 1.001) \n    F = 2 \/ (1\/precision + 1\/recall)\n    best_score = np.max(F)\n    best_th = thresholds[np.argmax(F)]\n    if plot:\n        plt.plot(thresholds, F, '-b')\n        plt.plot([best_th], [best_score], '*r')\n        plt.show()\n    search_result = {'threshold': best_th , 'f1': best_score}\n    return search_result ","96b690e3":"thr = threshold_search(ytrue, proba, plot=True); thr","9bc5b162":"preds = learn.get_preds(DatasetType.Test)\nproba = to_np(preds[0][:,1])\npredsC = (proba > thr['threshold']).astype(int)","bb80b183":"sub = pd.read_csv('..\/input\/sample_submission.csv')\nsub.prediction = predsC\nsub.to_csv(\"submission.csv\", index=False)","3616d24d":"sub.head()","7c97698e":"data.show_batch()","ebdf23f8":"data_lm.show_batch()","25ddad2c":"valid = train.sample(frac=0.1, weights=train.weights.values, random_state=42)","fbef5817":"learn.lr_find()\nlearn.recorder.plot()","e9280661":"learn.save('second')","f3df739a":"learn.freeze_to(-3)\nlearn.fit_one_cycle(1, slice(5e-3\/(2.6**4),5e-3), moms=(0.8,0.7))","051aa572":"learn.load('first');","6542209b":"learn.save('final')","97e7d8a8":"# Language Model","c54aa8f1":"learn.load('third');","6179d8c0":"learn.load('second');","51242566":"learn.freeze_to(-2)\nlearn.fit_one_cycle(1, slice(1e-2\/(2.6**4),1e-2), moms=(0.8,0.7))","e12491ad":"learn.lr_find()\nlearn.recorder.plot(skip_end=15)","4e84d64d":"data.save()\ndata = TextDataBunch.load('.')","d20d01aa":"learn.load('final');","d1551292":"learn.save('first')","bca41607":"learn.save('third')","93073926":"# Classifier","4c804fca":"train['weights'] = 0\ntrain.loc[train.target==1, 'weights'] = 0.06187\ntrain.loc[train.target==0, 'weights'] = 1-0.06187"}}