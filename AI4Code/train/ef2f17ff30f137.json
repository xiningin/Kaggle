{"cell_type":{"5bf922eb":"code","1aec8df1":"code","58cfd1cf":"code","c5f7e82d":"code","5e036e7d":"code","a49f68b0":"code","d6f127a7":"code","fd286b46":"code","511b1119":"code","13a0f6f3":"code","c875c832":"code","8a5ed87a":"code","ebe0b06a":"code","efad0628":"code","d19f0be0":"code","f33beda1":"code","67fc0442":"code","95a4ff66":"code","48b3fb93":"code","a6e19302":"code","6ea4c489":"code","7aecb705":"code","95ee0eaf":"code","5b867b51":"code","16d49aac":"code","fd242a5c":"code","576137ec":"code","389497a1":"code","af95ee65":"code","aef46500":"code","8d6ef183":"code","9276e061":"code","19ee55a1":"code","8a0722db":"code","2ec148c6":"code","2befd2f1":"code","c47545ed":"code","62b64d8d":"code","60f018de":"code","ccb96066":"code","6a7b07dd":"code","77897547":"code","8ce0bfff":"code","4c3770b4":"code","9b658701":"code","1cb95acb":"code","8538a615":"code","c777d08e":"code","ab188bf7":"code","ef8299a5":"code","f262d5ac":"code","e7525b98":"code","2539cb19":"code","5b0b5c17":"code","8c311805":"code","d5ee8048":"code","4c468f6b":"code","e63e20c9":"code","0e3793f2":"code","51e6c0fb":"code","abbd106e":"code","b82de10d":"code","9fe1d050":"code","93b82fea":"code","5898fa1f":"code","6910ecf8":"markdown","5c8743c3":"markdown","feb701a4":"markdown","a71cb45d":"markdown","11cd2630":"markdown","63b6b913":"markdown","07e582d1":"markdown","256d2ba5":"markdown","47ff352b":"markdown","eb52fb7f":"markdown","f3e73092":"markdown","e4af4706":"markdown","76ec2f37":"markdown","462c1e43":"markdown","101a467b":"markdown","ae4b6c65":"markdown","a86c87c0":"markdown","de437239":"markdown","d18fb955":"markdown","35ccad5e":"markdown","ecdb3753":"markdown","16905118":"markdown","64bd2849":"markdown"},"source":{"5bf922eb":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","1aec8df1":"# Import all the tools we need\n\n# Regular EDA(exploratory data analysis) plotting libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# We want our plots to appear inside of our notebook\n%matplotlib inline\n\n# Models from scikit-learn\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Model Evaluations\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.model_selection import RandomizedSearchCV, GridSearchCV\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.metrics import precision_score, recall_score, f1_score\nfrom sklearn.metrics import plot_roc_curve","58cfd1cf":"df = pd.read_csv(\"..\/input\/heart-disease-uci\/heart.csv\")\ndf.shape","c5f7e82d":"df.head()","5e036e7d":"df.tail()","a49f68b0":"# Let's find out how many of each class there\ndf[\"target\"].value_counts()","d6f127a7":"df[\"target\"].value_counts().plot(kind=\"bar\", color=[\"salmon\", \"lightblue\"]);","fd286b46":"df.info()","511b1119":"# Check for missing values\ndf.isna().sum()","13a0f6f3":"df.describe()","c875c832":"df.sex.value_counts()","8a5ed87a":"# Compare target columns with sex column\npd.crosstab(df.target, df.sex)","ebe0b06a":"# Create a plot of crosstab\npd.crosstab(df.target, df.sex).plot(kind=\"bar\",\n                                   figsize=(10, 6),\n                                   color=[\"salmon\", \"lightblue\"]);\nplt.title(\"Heart Disease Frequency for Sex\")\nplt.xlabel(\"0 = No Disease, 1 = Disease\")\nplt.ylabel(\"Amount\")\nplt.legend([\"Female\", \"Male\"])\nplt.xticks(rotation = 0);","efad0628":"# Heart Disease Frequency per Chest Pain Type\ndf.cp.value_counts()","d19f0be0":"# Compare target column with chest pain type\npd.crosstab(df.cp, df.target)","f33beda1":"# Creating another plot of crosstab\npd.crosstab(df.cp, df.target).plot(kind=\"bar\",\n                                  figsize=(10, 6),\n                                  color=[\"salmon\", \"lightblue\"]);\nplt.title(\"Heart Disease Frequency for Chest Pain\")\nplt.xlabel(\"Chest Pain Type\")\nplt.ylabel(\"Amount\")\nplt.legend([\"No Disease\", \"Disease\"])\nplt.xticks(rotation = 0);","67fc0442":"# Create another figure\nplt.figure(figsize=(10, 6))\n\n# Scatter with positive examples\nplt.scatter(df.age[df.target==1],\n           df.thalach[df.target==1],\n           c=\"salmon\");\n\n# Scatter with negative examples\nplt.scatter(df.age[df.target==0],\n           df.thalach[df.target==0],\n           c=\"lightblue\");\n\n# Add some helpfull info\nplt.title(\"Heart Disease in function of Age and Max Heart Rate\")\nplt.xlabel(\"Age\")\nplt.ylabel(\"Max Heart Rate\")\nplt.legend([\"Disease\", \"No Disease\"]);","95a4ff66":"# Check the distribution of the age column with a histogram\ndf.age.plot.hist();","48b3fb93":"df.head()","a6e19302":"# Make a correlation matrix\ndf.corr()","6ea4c489":"# Let's make our correlation matrix a little prettier\ncorr_matrix = df.corr()\nfig, ax = plt.subplots(figsize=(15, 10))\nax = sns.heatmap(corr_matrix,\n                annot=True,\n                linewidths=0.5,\n                fmt=\".2f\",\n                cmap=\"YlGnBu\")","7aecb705":"df.head()","95ee0eaf":"# Split data into X and y\nX = df.drop(\"target\", axis = 1)\ny = df[\"target\"]","5b867b51":"X","16d49aac":"y","fd242a5c":"# Split data into train and test sets\nnp.random.seed(12)\n\nX_train, X_test, y_train, y_test = train_test_split(X,\n                                                   y,\n                                                   test_size=0.2)","576137ec":"X_train","389497a1":"y_train, len(y_train)","af95ee65":"# Put models in a dictionary\nmodels = {\"Logistic Regression\": LogisticRegression(),\n        \"KNN\": KNeighborsClassifier(),\n        \"Random Forest\": RandomForestClassifier()}\n\n# Create a function to fit and score models\ndef fit_and_score(models, X_train, X_test, y_train, y_test):\n    \"\"\"\n    Fits and evaluates given machine learninf models.\n    models : a dict of different Scikit-Learn machine learning models\n    X_train : training data (no labels)\n    X_test : testing data (no labels)\n    y_train : training labels\n    y_test : testing labels\n    \"\"\"\n    # Set random seed\n    np.random.seed(12)\n    # Make a dictionary to keep model scores\n    model_scores = {}\n    # Loop through models\n    for name, model in models.items():\n        # Fit the model to the data\n        model.fit(X_train, y_train)\n        # Evaluate the model and append its score to model scores\n        model_scores[name] = model.score(X_test, y_test)\n    return model_scores","aef46500":"model_scores = fit_and_score(models=models,\n                            X_train=X_train,\n                            X_test=X_test,\n                            y_train=y_train,\n                            y_test=y_test)\nmodel_scores","8d6ef183":"model_compare = pd.DataFrame(model_scores, index=[\"accuracy\"])\nmodel_compare.T.plot.bar();","9276e061":"# Let's tune KNN\n\ntrain_scores = []\ntest_scores = []\n\n# Create a list of different values for n_neighbors\nneighbors = range(1, 21)\n\n# Setup KNN instance\nknn = KNeighborsClassifier()\n\n# Loop through diffetent n_neighbors\nfor i in neighbors:\n    knn.set_params(n_neighbors=i)\n    \n    # Fit the algorithm\n    knn.fit(X_train, y_train)\n    \n    # Update the training scores list\n    train_scores.append(knn.score(X_train, y_train))\n    \n    #Update the test scores list\n    test_scores.append(knn.score(X_test, y_test))","19ee55a1":"train_scores","8a0722db":"test_scores","2ec148c6":"plt.plot(neighbors, train_scores, label = \"Train score\")\nplt.plot(neighbors, test_scores, label = \"Test score\")\nplt.xticks(np.arange(1, 21, 1))\nplt.xlabel(\"Number of neighbors\")\nplt.ylabel(\"Model score\")\nplt.legend()\n\nprint(f\"Maximum KNN score on the test data: {max(test_scores)*100:.2f}%\")","2befd2f1":"# Create a hyperparameter grid for LogisticRegression\nlog_reg_grid = {\"C\": np.logspace(-4, 4, 20),\n               \"solver\": [\"liblinear\"]}\n\n# Create a hyperparameter grid for RandomForestClassifier\nrf_grid = {\"n_estimators\": np.arange(10, 1000, 50),\n          \"max_depth\": [None, 3, 5, 10],\n          \"min_samples_split\": np.arange(2, 20, 2),\n          \"min_samples_leaf\": np.arange(1, 20, 2)}","c47545ed":"# Turn LogisticRegression\nnp.random.seed(12)\n\n# Setup random hyperparameters search for LogisticRegression\nrs_log_reg = RandomizedSearchCV(LogisticRegression(),\n                                param_distributions=log_reg_grid,\n                                cv=5,\n                                n_iter=20,\n                                verbose=True)\n\n# Fit random hyperparameter search model for LogisticRegression\nrs_log_reg.fit(X_train, y_train)","62b64d8d":"rs_log_reg.best_params_","60f018de":"rs_log_reg.score(X_test, y_test)","ccb96066":"# Setup random seed\nnp.random.seed(12)\n\n# Setup random hyperparameters search for RandomForestClassifier\nrs_rf = RandomizedSearchCV(RandomForestClassifier(),\n                          param_distributions=rf_grid,\n                          cv=5,\n                          n_iter=100,\n                          verbose=True)\n\n# Fit random hyperparameters search model for RandomForestClassifier()\nrs_rf.fit(X_train, y_train)","6a7b07dd":"rs_rf.best_params_","77897547":"rs_rf.score(X_test, y_test)","8ce0bfff":"# Different hyperparameters for our LogisticRegression model\nlog_reg_grid = {\"C\": np.logspace(-4, 4, 30),\n               \"solver\": [\"liblinear\"]}\n\n# Setup grid hyperparameters search for LogisticRegression\ngs_log_reg = GridSearchCV(LogisticRegression(),\n                         param_grid=log_reg_grid,\n                         cv=5,\n                         verbose=True)\n\n# Fit grid hyperparameters search model\ngs_log_reg.fit(X_train, y_train)","4c3770b4":"gs_log_reg.best_params_","9b658701":"# Evaluate the grid search LogisticRegression Model\ngs_log_reg.score(X_test, y_test)","1cb95acb":"# Make predictions with tuned model\ny_preds = gs_log_reg.predict(X_test)","8538a615":"y_preds","c777d08e":"y_test","ab188bf7":"# Plot ROC curve and calculate AUC metric\nplot_roc_curve(gs_log_reg, X_test, y_test);","ef8299a5":"# Confussion matrix\nprint(confusion_matrix(y_test, y_preds))","f262d5ac":"sns.set(font_scale=1.5)\n\ndef plot_conf_mat(y_test, y_preds):\n    \"\"\"\n    Plots a nice looking cinfusion matrix using Seaborn's heatmap()\n    \"\"\"\n    fig, ax = plt.subplots(figsize=(3, 3))\n    ax = sns.heatmap(confusion_matrix(y_test, y_preds),\n                    annot=True,\n                    cbar=False)\n    plt.xlabel(\"True label\")\n    plt.ylabel(\"Predicted label\")\n    \nplot_conf_mat(y_test, y_preds)","e7525b98":"print(classification_report(y_test, y_preds))","2539cb19":"# Check best hyperparameters\ngs_log_reg.best_params_","5b0b5c17":"# Create a new classifier with best parameters\nclf = LogisticRegression(C=0.1082636733874054,\n                        solver=\"liblinear\")","8c311805":"# Cross-validated accuracy\ncv_acc = cross_val_score(clf,\n                        X,\n                        y,\n                        cv=5,\n                        scoring=\"accuracy\")\ncv_acc=np.mean(cv_acc)\ncv_acc","d5ee8048":"# Cross-validated precision\ncv_prec = cross_val_score(clf,\n                         X,\n                         y,\n                         cv=5,\n                         scoring=\"precision\")\ncv_prec=np.mean(cv_prec)\ncv_prec","4c468f6b":"# Cross-validated recall\ncv_recall = cross_val_score(clf,\n                           X,\n                           y,\n                           cv=5,\n                           scoring=\"recall\")\ncv_recall=np.mean(cv_recall)\ncv_recall","e63e20c9":"# Cross-validated F1-score\ncv_f1 = cross_val_score(clf,\n                       X,\n                       y,\n                       cv=5,\n                       scoring=\"f1\")\ncv_f1=np.mean(cv_f1)\ncv_f1","0e3793f2":"# Visualize cross-validated metrics\ncv_metrics = pd.DataFrame({\"Accuracy\": cv_acc,\n                         \"Precision\": cv_prec,\n                         \"Recall\": cv_recall,\n                         \"F1-score\": cv_f1},\n                         index=[0])\n\ncv_metrics.T.plot.bar(title=\"Cross-validated classification metrics\",\n                     legend=False);","51e6c0fb":"# Fit an instance of LogisticRegression\ngs_log_reg.best_params_\n\nclf = LogisticRegression(C=0.1082636733874054,\n                        solver=\"liblinear\")\n\nclf.fit(X_train, y_train)","abbd106e":"# Check coef_\nclf.coef_","b82de10d":"# Match coef's of features to column\nfeature_dict = dict(zip(df.columns, list(clf.coef_[0])))\nfeature_dict","9fe1d050":"# Visualize feature importance\nfeature_df = pd.DataFrame(feature_dict, index=[0])\nfeature_df.T.plot.bar(title=\"Feature importance\",\n                     legend=False);","93b82fea":"pd.crosstab(df[\"sex\"], df[\"target\"])","5898fa1f":"pd.crosstab(df[\"slope\"], df[\"target\"])","6910ecf8":"## 2. Data\nThe original data came from the Cleavland data from UCI Machine Learning Repository.\n\nhttps:\/\/archive.ics.uci.edu\/ml\/datasets\/Heart+Disease\nThere is also a version of it available in Kaggle.\n\nhttps:\/\/www.kaggle.com\/ronitf\/heart-disease-uci","5c8743c3":"## Predicting heart disease using machine learning\nThis notebook looks into using various Python-based machine learning and data science libraries in an attempt to build a machine learning model capable of predicting whether or not someone has heart disease based on their medical attributes.\n\nWe're going to take the following approach:\n\n1. Problem definition\n2. Data\n3. Evaluation\n4. Features\n5. Modelling\n6. Experimentation","feb701a4":"## 1. Problem Definition\nIn a statement,\n\nGiven clinical parameters about a patient, can we predict whether or not they have heart disease?","a71cb45d":"## Heart Disease Frequency acording to Sex","11cd2630":"## Preparing the tools\nWe're going to use Panda, Matplotlib, NumPy for data analysis and manipulation.","63b6b913":"Now we've tuned LogisticRegression(), let's do the same for RandomForestClassifier()...","07e582d1":"Now we've got a baseline model... and we know a model's first predictions aren't always what we should based our next steps off.\nWhat should do?\n\nLet's look at the following:\n* Hyperparameter tuning\n* Feature importance\n* Confussion matrix\n* Cross-validation\n* Precision\n* Recall\n* F1 score\n* Classification report\n* ROC curve\n* Area under the curve (AUC)\n\n### Hyperparameter tuning","256d2ba5":"Now we've got hyperparameters grids setup for each of our models, lets tune them using RandomizedSearchCV","47ff352b":"### Feature Importance\n\nFeature Importance is another as asking, \"which features contributed dmost to the outcomes of the model and how did thet contribute?\"\n\nFinding featute importance is different for each machine learning model.\nOne way to find feature importance is to search for(\"MODEL NAME\") feature importance.\n\nLet's find the feature importance for our LogisticRegression model...","eb52fb7f":"## 4. Features\nThis is where you'll get different information about each of the features in your data.\n\nCreate data dictionary\n\n1. age - age in years\n2. sex(1 = male; 0 = female)\n3. cp - chest pain type\n4. trestbps - resting blood pressure (in mm Hg on admission to the hospital)\n5. cholserum - cholestoral in mg\/dl\n6. fbs - (fasting blood sugar > 120 mg\/dl) (1 = true; 0 = false)\n7. restecg - resting electrocardiographic results\n8. thalach - maximum heart rate achieved\n9. exang - exercise induced angina (1 = yes; 0 = no)\n10. oldpeak - ST depression induced by exercise relative to rest\n11. slope - the slope of the peak exercise ST segment\n12. ca - number of major vessels (0-3) colored by flourosopy\n13. thal - 3 = normal; 6 = fixed defect; 7 = reversable defect\n14. target - 1 or 0","f3e73092":"## Evaluating our tuned machine learning classifier, beyond accuracy\n\n* ROC curve nad AUC score\n* Confussion matrix\n* Classification report\n* Precision\n* Recall\n* F1-score\n\n...and it would be great if cross-validation was used where possible.\n\nTo make comparison and evaluate our trained model, first we need to make predictions.","e4af4706":"## Hyperparameter tuning with RandomizedSearchCV\n\nWe're going to tune:\n* LigisticRegression()\n* RandomForestClassifier()\n... using RandomizedSearchCV","76ec2f37":"## Hyperparameters Tuning with GreadSearchCV","462c1e43":"# Model Comparison","101a467b":"## 5. Modelling","ae4b6c65":"## Age vs Max Heart Rate for Heart Disease","a86c87c0":"## 3. Evaluation\nIf we can reach 95% accuracy at predicting whether or not a patient has heart disease during the proof of concept, we'll pursue the project","de437239":"### Calculate evaluation metrics using cross-validation\n\nWe're going to calculate accuracy, precision, recall and f1-score of our model using cross-validataion and to do so we'll be using 'cross_val_score'","d18fb955":"## Load data","35ccad5e":"Now we've got our data split into training & test sets, it's timme to build a machine learning model.\n\nWe'll train it (find the patterns) on the training sets.\n\nAnd we'll test it (use the patterns) on the test sets.\n\nWe're going to try 3 different machine learning models:\n\n1. Logistic Regression\n2. K-Nearest Neighbours Classifier\n3. Random Forest Classifier","ecdb3753":"Now we've got a ROC curve, an AUC metric and a confusion matrix, let's get a classification report as well as cross-validated precision, recall abd F1-score.","16905118":"## Data Exploration (EDA)\nThe goal here is to find out more about the data and become a subject matter expert on the dataset you're working with.\n\n1. What questions are you trying to solve?\n2. What kind of data do we have and how do we treat different types?\n3. Whats missing from the data and how do you deal with it?\n4. Where are the outliers and why should you care about them?\n5. How can you add, change or remove features to get more out of your data?","64bd2849":"## 6. Experimentation\n\n* Could i collect more data?\n* Could i try a better model? Like CatBoost or XGBoost?\n* Could i improve the current model? (beyong what we've done so far)"}}