{"cell_type":{"36169495":"code","7012372a":"code","885d912a":"code","16ae310f":"code","f6851a66":"code","dc94edab":"code","5162ee2f":"code","e6705ea3":"code","53ae1dd8":"code","5a0cdfa0":"code","91613644":"code","e46e5f8e":"code","86eea4f0":"code","6ef4f75c":"code","55df0b0c":"code","991ae0b5":"code","63f60236":"code","09a6241b":"code","1df58df6":"code","d1380b5e":"code","fac2cfb4":"code","4535bc39":"code","086d4a8d":"code","160b21a1":"code","23cbda1d":"code","f78e186d":"code","6f53d05e":"code","44ccf91e":"code","5aeaa8fe":"code","7960cb2f":"code","1688194f":"code","8ae97e4c":"code","bc876698":"code","d441dce5":"code","1e771f7f":"code","9fd7f364":"code","6f1c9f79":"code","00f5af26":"code","bf8c5ce8":"code","14d1d536":"code","c628cf58":"code","fa24bb58":"code","d7cbd057":"code","bcf2c0ee":"code","fb7b31c0":"code","b92be9e3":"code","03e4da9d":"code","20fa1d41":"code","36f27eb6":"code","33eae30f":"code","8712ccc0":"code","3385dd43":"code","a41de278":"code","79795ee1":"code","4a9474d2":"markdown","beaf5559":"markdown","96118807":"markdown","20ee876f":"markdown","cd110007":"markdown","498dab17":"markdown","ca506c76":"markdown","d78b4028":"markdown","7bd4bda4":"markdown","f6d9f292":"markdown","1bae3343":"markdown","eafabae1":"markdown","167fc458":"markdown","53f46e39":"markdown","5a714db7":"markdown","10b1d612":"markdown","4c259ab0":"markdown","020b75de":"markdown","f4b0d88f":"markdown","0f3bc162":"markdown","ecf6834f":"markdown","1e678f64":"markdown","34312aff":"markdown","5a0fc135":"markdown","e6416c43":"markdown","6a3820b9":"markdown","22f3d028":"markdown","493285de":"markdown"},"source":{"36169495":"from IPython.display import IFrame\nIFrame('https:\/\/app.powerbi.com\/view?r=eyJrIjoiMzRlZTliMTItZGE0OS00ZTNjLTllM2YtMTlmN2ZhMDRkYWRmIiwidCI6ImRjNTQ3MWFjLWM4NmMtNGNiZS04YTVjLTI0MzUxZGRmMjRjOCIsImMiOjN9', width=800, height=500)","7012372a":"IFrame('https:\/\/app.powerbi.com\/view?r=eyJrIjoiMjc4NDZkYzktZmUyZC00ZThkLTkyYjAtY2EzNzk2ZDliOTk2IiwidCI6ImRjNTQ3MWFjLWM4NmMtNGNiZS04YTVjLTI0MzUxZGRmMjRjOCIsImMiOjN9', width=800, height=500)","885d912a":"# If True, use the SciSpacy package for tokenizing, with dictionary\n# en_core_sci_lg-0.2.4\n# otherwise, use Spacy, with en_core_web_lg\nUseSciSpacy=True\n\n#If True, enrich CORD-19 metadata with cleaned metadata\n# from CoronaWhy datasets\nInsertCiteData=True\n\n#If True, test for sentence negations\nCalculateNegations=True\n\n# If True, re-run the text matching code.\n# Warning, if it takes along time! Intermediate data \n# from this step is provided with the notebook.\nRunMatching=False","16ae310f":"import numpy as np\nimport pylab\nimport pandas as pd\nimport json\nimport os\nimport re\nimport spacy\nimport numpy as np  \nimport pandas as pd \nimport matplotlib\nimport spacy\nimport pickle\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Dense, Input\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom kaggle_datasets import KaggleDatasets\nimport transformers\nfrom transformers import TFAutoModel, AutoTokenizer\nfrom tqdm.notebook import tqdm\nfrom tokenizers import Tokenizer, models, pre_tokenizers, decoders, processors\n\nif(UseSciSpacy):\n   #Instal SciSpacy\n    !pip install -U scispacy\n    !pip install -U https:\/\/s3-us-west-2.amazonaws.com\/ai2-s2-scispacy\/releases\/v0.2.4\/en_core_sci_lg-0.2.4.tar.gz\n    import scispacy\n    nlp=spacy.load(\"\/opt\/conda\/lib\/python3.6\/site-packages\/en_core_sci_lg\/en_core_sci_lg-0.2.4\/\", disable=[\"tagger\"])\nelse:\n    !python -m spacy download en_core_web_lg\n    nlp = spacy.load('en_core_web_lg')","f6851a66":"font = {'family' : 'normal',\n        'weight' : 'normal',\n        'size'   : 6}\n\nmatplotlib.rc('font', **font)","dc94edab":"# Open the RxNorm input file\nfile=open(\"\/kaggle\/input\/rxnorm-inputdata\/RxNorm_full_prescribe_03022020\/rrf\/RXNCONSO.RRF\",'r').readlines()\nnames=[]\nfor line in file:\n    \n    names.append(line.split(\"|\")[14].lower())\nnames=np.unique(names)\n\n# Load up elements\nElements=pd.read_csv(\"\/kaggle\/input\/rxnorm-inputdata\/Elements.csv\")\nElementNames = Elements.Element.str.lower()\n\n# Load up animals, \nAnimalsRaw=open(\"\/kaggle\/input\/rxnorm-inputdata\/animals.txt\",'r').readlines()\nAnimalNames=[]\nfor a in AnimalsRaw:\n    if not \" \" in a:\n        AnimalNames.append(a[:-1].lower())\n\n# Load up fruit and veg\nFruitVegRaw=open(\"\/kaggle\/input\/rxnorm-inputdata\/FruitAndVeg.txt\",'r').readlines()\nFruitVegNames=[]\nfor a in FruitVegRaw:\n    if not \" \" in a:\n        FruitVegNames.append(nlp(a[:-1].lower())[0].lemma_)\n\n# And a few hand-spotted bad ones        \nFakes=['injection','glucose','perform','ethanol','methanol','paraffin','soybean','horseradish','ginger','mouthwash','oregano','formaldehyde','alcohol']\n\n \n# Apply the filter        \nfilterednames=[]\nfor name in names:\n    if (not name  in AnimalNames) and (not name  in FruitVegNames)  and (not name  in ElementNames.values) and (not name in Fakes):\n        filterednames.append(name)\n\n# Now begin more advanced list cleaning step:        \n        \ndnames = filterednames\ndnames = [ x.replace(\"\\n\",\"\") for x in dnames]\n\ndose_terms = open(\"\/kaggle\/input\/rxnorm-inputdata\/dosage terms.txt\").readlines()\ndose_terms = [ x.replace(\"\\n\",\"\") for x in dose_terms]\n\ndelivery_terms = open(\"\/kaggle\/input\/rxnorm-inputdata\/delivery terms.txt\").readlines() \ndelivery_terms = [ x.replace(\"\\n\",\"\") for x in delivery_terms]\n\nsource_terms = open(\"\/kaggle\/input\/rxnorm-inputdata\/source terms.txt\").readlines()   \nsource_terms = [ x.replace(\"\\n\",\"\") for x in source_terms]\n\ndf = pd.DataFrame(dnames, columns=[\"dnames\"])\n\n#uses regex to search for nonwhitespace characters only. Search for unigrams, bigrams, trigrams\ndf[\"unigram\"] = df[\"dnames\"].str.contains(\"^\\\\S*$\")\ndf[\"bigram\"] = df[\"dnames\"].str.contains(\"^\\\\S* \\\\S*$\")\ndf[\"trigram\"] = df[\"dnames\"].str.contains(\"^\\\\S* \\\\S* \\\\S*$\")\n\n# filter df for uni\/bi\/trigram\ndf = df[df[\"unigram\"] | df[\"bigram\"] | df[\"trigram\"]] \n\ndose_regex = re.compile(\"\\\\b(\"+ (\"|\").join(dose_terms) + \")\\\\b\")\ndelivery_regex = re.compile(\"\\\\b(\"+ (\"|\").join(delivery_terms) + \")\\\\b\")\nsource_regex = re.compile(\"\\\\b(\"+ (\"|\").join(source_terms) + \")\\\\b\")\n\ndf = df[df[\"dnames\"].str.contains(dose_regex) == False]\ndf = df[df[\"dnames\"].str.contains(delivery_regex) == False]\n\n# remove chars \/ , % r numbers in a row. punct.\ndf = df[df[\"dnames\"].str.find(\"\/\") == -1]\ndf = df[df[\"dnames\"].str.find(\",\") == -1]\ndf = df[df[\"dnames\"].str.find(\"%\") == -1]\ndf = df[~df[\"dnames\"].str.contains(\"\\d\\d\\d\\d\") ]\ndf = df[~df[\"dnames\"].str.contains(\"^[[:punct:]]\") ]\ndf = df[~df[\"dnames\"].str.contains(\"(\\\\[|\\\\(|\\\\)|\\\\])\") ]\n\n# The source word is replace with nothing; this can introdcue some duplicates which are removed next\ndf.replace(source_regex, value = '', regex = True) \ndf.replace('.', value = '', regex = True) \n\ndf = df.drop_duplicates()\n\n# Recalculate and reinspect\n# By removing words some ngrams are now shorter\ndf[\"unigram\"] = df[\"dnames\"].str.contains(\"^\\\\S*$\") #uses regex to search for nonwhitespace characters only\ndf[\"bigram\"] = df[\"dnames\"].str.contains(\"^\\\\S* \\\\S*$\") #uses regex to search for nonwhitespace characters alternating with space\ndf[\"trigram\"] = df[\"dnames\"].str.contains(\"^\\\\S* \\\\S* \\\\S*$\") #uses regex to search for nonwhitespace characters alternating with space\n\n# save unigrams and multi-word expressions\n\nunigrams = df[df[\"unigram\"] == True][\"dnames\"].tolist()\nmwes = df[df[\"unigram\"] == False][\"dnames\"].tolist()\n\nunigramsdf = pd.DataFrame(unigrams)\nunigramsdf.to_csv( \"unigrams.csv\", index=None, header=False)\n\nmwesdf = pd.DataFrame(mwes)\nmwesdf.to_csv( \"mwes.csv\", index=None, header=False)\n\n\n\nalldrugs=np.concatenate([unigramsdf.values,mwesdf.values])\n\nnp.savetxt(\"\/kaggle\/working\/DrugNames.txt\",alldrugs,fmt=\"%s\")\n","5162ee2f":"excelra_drugs=np.loadtxt(\"\/kaggle\/input\/rxnorm-inputdata\/excelra covid19 repurposing db.csv\",dtype='str',delimiter=',')\nwho_drugs=np.loadtxt(\"\/kaggle\/input\/rxnorm-inputdata\/WHO landscape of therapeutics.csv\",dtype='str',delimiter=',')","e6705ea3":"fulldrugs=np.concatenate([alldrugs[:,0],excelra_drugs[1:,0],who_drugs])\nfulldrugs=np.unique(sorted(fulldrugs[[(len(i)>5) for i in fulldrugs]]))\nngram_drugs=[]\nfor f in fulldrugs:\n    ngram_drugs.append(f.replace(\" \",\"_\"))\nnp.savetxt(\"\/kaggle\/working\/DrugNames.txt\",fulldrugs,fmt=\"%s\")\n\n","53ae1dd8":"# This helper function to deal with multi-word search terms.\n# The basic approach is to switch spaces for underscores, and \n# apply this same filtering to the fulltext via the ngram_map\n# before matching.\n\ndef MakeNGramMapAndList(filename):\n    ngram_map={}\n    names=[]\n    allnames=[]\n\n    file=open(filename).readlines()\n    for line in file:\n        allnames.append(line[:-1])\n\n    for name in allnames:\n        if(len(name)>5):\n            if \" \" in name:\n                if((len(name.split(\" \"))>1) and len(name.split(\" \"))<4):\n                    newname=name.replace(\" \",\"_\")\n                    ngram_map[name]=newname\n                    names.append(newname)\n            else:\n                names.append(name)\n    names=np.unique(names)\n    return names, ngram_map\n    \ndrug_names,  drug_ngram_map          = MakeNGramMapAndList(\"\/kaggle\/working\/DrugNames.txt\")\nstudy_names, study_ngram_map         = MakeNGramMapAndList(\"\/kaggle\/input\/textmatchesvt\/study_words.txt\")\nvirus_names, virus_ngram_map         = MakeNGramMapAndList(\"\/kaggle\/input\/textmatchesvt\/virus_words.txt\")\ntreatment_names, treatment_ngram_map = MakeNGramMapAndList(\"\/kaggle\/input\/textmatchesvt\/treatment_words.txt\")","5a0cdfa0":"# These are helper functions for extracting word matches from the text\n# both lemmatized and non-lemmatized versions are possible.\n\nPaths=[\"\/kaggle\/input\/CORD-19-research-challenge\/noncomm_use_subset\/noncomm_use_subset\/pdf_json\/\",\"\/kaggle\/input\/CORD-19-research-challenge\/comm_use_subset\/comm_use_subset\/pdf_json\/\",\"\/kaggle\/input\/CORD-19-research-challenge\/biorxiv_medrxiv\/biorxiv_medrxiv\/pdf_json\/\",\"\/kaggle\/input\/CORD-19-research-challenge\/custom_license\/custom_license\/pdf_json\/\"]\n\n\n# These functions determine what blocks are pulled from the paper for matching\ndef TitleBlocks(paper):\n    return([{'text':paper['metadata']['title']}])\n\ndef AbstractBlocks(paper):\n    return(paper['abstract'])\n\ndef BodyBlocks(paper):\n    return(paper['body_text'])\n\n\n\n# This function finds matching lemmas and notes positions of\n# occurence in the relevant json block. This function uses\n# the lemmatized text.\ndef PullMentionsLemmatized(Paths, BlockSelector,SecName, Words,replace_dict=None):\n    Positions=[]\n    FoundWords=[]\n    Section=[]\n    BlockID=[]\n    BlockText=[]\n    PaperID=[]\n\n    tokenized_words=[]\n    for w in Words:\n        tokenized_words.append(nlp(w.lower())[0].lemma_)\n    for Path in Paths:\n        print(Path)\n\n        Files=os.listdir(Path)\n        for p in Files:\n\n            readfile=open(Path+p,'r')\n            paper=json.load(readfile)\n            Blocks=BlockSelector(paper)\n\n            for b in range(0,len(Blocks)):\n                text=Blocks[b]['text'].lower()\n                if(not replace_dict==None):\n                    text=RunReplace(text,replace_dict)\n                text=nlp(text)\n                for t in text:\n                    for w in tokenized_words:\n                        if(w == t.lemma_):\n                            Section.append(SecName)\n                            FoundWords.append(w)\n                            Positions.append(t.idx)\n                            BlockText.append(Blocks[b]['text'])\n                            BlockID.append(b)\n                            PaperID.append(p[:-5])\n    return {'sha':PaperID,'blockid':BlockID,'word':FoundWords,'sec':Section,'pos':Positions,'block':BlockText}\n\n\n# This function finds matching words and notes positions of\n# occurence in the relevant json block. This function uses\n# direct text matching (not lemmatized)\ndef PullMentionsDirect(Paths, BlockSelector,SecName, Words, replace_dict=None):\n    Positions=[]\n    FoundWords=[]\n    Section=[]\n    BlockID=[]\n    BlockText=[]\n    PaperID=[]\n    for wi in range(0,len(Words)):\n        Words[wi]=Words[wi].lower()\n    for Path in Paths:\n        print(Path)\n\n        Files=os.listdir(Path)\n        for p in Files:\n\n            readfile=open(Path+p,'r')\n            paper=json.load(readfile)\n            Blocks=BlockSelector(paper)\n\n            for b in range(0,len(Blocks)):\n                text=Blocks[b]['text'].lower()\n                if(not replace_dict==None):\n                    text=RunReplace(text,replace_dict)\n                for w in Words:\n                    if(w in text):\n                        pos=text.find(w)\n                   \n                        #check we're not in the middle of another word\n                        if(text[pos-1]==\" \" and ( (pos+len(w))>=len(text) or not text[pos+len(w)].isalpha())):\n                            Section.append(SecName)\n                            FoundWords.append(w)\n                            Positions.append(text.find(w))\n                            BlockText.append(Blocks[b]['text'])\n                            BlockID.append(b)\n                            PaperID.append(p[:-5])\n    return {'sha':PaperID,'blockid':BlockID,'word':FoundWords,'sec':Section,'pos':Positions,'block':BlockText}\n\n\n# Replace routine for dealing with n-grams\ndef RunReplace(Block, replace_dict):\n    for k in replace_dict.keys():\n        if(k in Block):\n            Block=Block.replace(k,replace_dict[k])\n    return Block\n\n\n# Run to get treatment words\ndef ExtractToCSV(Words,Filename,Lemmatized=True, RunTitle=True, RunAbstract=True, RunBody=False,replace_dict=None):\n\n    if(Lemmatized):\n        PullMentions = PullMentionsLemmatized\n    else:\n        PullMentions = PullMentionsDirect\n    \n    DataDicts=[]\n    if(RunTitle): \n        DataDicts.append(PullMentions(Paths, TitleBlocks,    \"title\",    Words, replace_dict))\n    if(RunAbstract):\n        DataDicts.append(PullMentions(Paths, AbstractBlocks, \"abstract\", Words, replace_dict))\n    if(RunBody):\n        DataDicts.append(PullMentions(Paths, BodyBlocks,     \"body\",     Words, replace_dict))\n\n    SummedDictionary=DataDicts[0]\n    for k in DataDicts[0].keys():\n        for d in DataDicts:\n            SummedDictionary[k]=SummedDictionary[k]+d[k]\n\n    dat=pd.DataFrame(SummedDictionary)\n    dat.to_csv(Filename)","91613644":"\n\n#Switch this off to run over only title and abstract -\n#  go faster for debugging, but less complete info.\nIncludeBodyText=True\n\n# These lines of code will run the extraction\n\nif(RunMatching):\n    ExtractToCSV(virus_words,      \"\/kaggle\/working\/Matches\/TitleAbstractBodyMatches_virusnames.csv\", replace_dict=virus_ngram_map,     Lemmatized=False, RunBody=IncludeBodyText)\n    ExtractToCSV(drug_words,       \"\/kaggle\/working\/Matches\/TitleAbstractBodyMatches_drugs.csv\",      replace_dict=drug_ngram_map,      Lemmatized=False, RunBody=IncludeBodyText)\n    ExtractToCSV(treatment_words,  \"\/kaggle\/working\/Matches\/TitleAbstractBodyMatches_therapies.csv\",  replace_dict=treatment_ngram_map, Lemmatized=True,  RunBody=IncludeBodyText)\n    ExtractToCSV(study_words,      \"\/kaggle\/working\/Matches\/TitleAbstractBodyMatches_esptypes.csv\",   replace_dict=study_ngram_map,     Lemmatized=True,  RunBody=IncludeBodyText)\n","e46e5f8e":"dat_therapies = pd.read_csv(\"\/kaggle\/input\/textmatchesvt\/Matches\/TitleAbstractBodyMatches_therapies.csv\")\ndat_drugs     = pd.read_csv(\"\/kaggle\/input\/textmatchesvt\/Matches\/TitleAbstractBodyMatches_drugs.csv\")\ndat_viruses   = pd.read_csv(\"\/kaggle\/input\/textmatchesvt\/Matches\/TitleAbstractBodyMatches_virusnames.csv\")\ndat_exps      = pd.read_csv(\"\/kaggle\/input\/textmatchesvt\/Matches\/TitleAbstractBodyMatches_studies.csv\")","86eea4f0":"# Drop unnecessary columns\ndat_drugs     = dat_drugs.drop('Unnamed: 0',axis=1).set_index('block')\ndat_therapies = dat_therapies.drop('Unnamed: 0',axis=1).set_index('block')\ndat_viruses   = dat_viruses.drop('Unnamed: 0',axis=1).set_index('block')\ndat_exps      = dat_exps.drop('Unnamed: 0',axis=1).set_index('block')","6ef4f75c":"# We'll use this function later to see if two words are in the same sentence\n#  within the block\n\ndef SameSentenceCheck(block,pos1,pos2):\n    if(pos1<pos2):\n        Interstring=block[int(pos1):int(pos2)]\n    else:\n        Interstring=block[int(pos2):int(pos1)]\n    SentenceEnders=[\".\",\";\",\"?\",\"!\"]\n    for s in SentenceEnders:\n        if s in Interstring:\n            return 0\n    return 1","55df0b0c":"# This function makes the 2D quilt plot for showing co-occurences at block\n#   or sentence level of various classes of search terms\n#\ndef Make2DPlot(dat_joined, factor1, factor2, single_sentence_plots=False):\n    if(single_sentence_plots):\n        grouped = dat_joined[dat_joined.same_sentence==True].groupby(['word_'+factor1,'word_'+factor2])\n    else:\n        grouped = dat_joined.groupby(['word_'+factor1,'word_'+factor2])\n\n    Values    = grouped.count().values[:,0]\n\n    Index=grouped.count().index\n    Index1=[]\n    Index2=[]\n    for i in Index:\n        Index1.append(i[0])\n        Index2.append(i[1])\n\n    Uniq1=np.unique(Index1)\n    Uniq2=np.unique(Index2)\n\n    for i in range(0,len(Index1)):\n        Index1[i]=np.where(Index1[i]==Uniq1)[0][0]\n        Index2[i]=np.where(Index2[i]==Uniq2)[0][0]\n\n    pylab.figure(figsize=(5,5),dpi=200)\n    hist=pylab.hist2d(Index1,Index2, (range(0,len(Uniq1)+1),range(0,len(Uniq2)+1)), weights=Values,cmap='Blues')\n    pylab.xticks(np.arange(0,len(Uniq1))+0.5, Uniq1,rotation=90)\n    pylab.yticks(np.arange(0,len(Uniq2))+0.5, Uniq2)\n    pylab.clim(0,np.max(hist[0])*1.5)\n    for i in range(0,len(Uniq1)):\n        for j in range(0,len(Uniq2)):\n            pylab.text(i+0.5,j+0.5,int(hist[0][i][j]),ha='center',va='center')\n\n    pylab.colorbar()\n    if(single_sentence_plots):\n        pylab.title(factor1+\" and \" +factor2+\" in One Sentence\")\n        pylab.tight_layout()\n        pylab.savefig(\"Overlap\"+factor1+\"_Vs_\"+factor2+\"_2D_sentence.png\",bbox_inches='tight',dpi=200)\n    else:\n        pylab.title(factor1+\" and \" +factor2+\" in One Block\")\n        pylab.tight_layout()\n        pylab.savefig(\"Overlap\"+factor1+\"_Vs_\"+factor2+\"_2D_block.png\",bbox_inches='tight',dpi=200)","991ae0b5":"# Prune and join, and extract overlap counts\ndat_joined_vt=dat_therapies.join(dat_viruses, rsuffix='_virus',lsuffix=\"_therapy\")\ndat_joined_vt=dat_joined_vt[dat_joined_vt.notna().word_therapy & dat_joined_vt.notna().word_virus]\n\n\n#Make single sentence index\ndat_joined_vt=dat_joined_vt.drop([\"sha_therapy\",\"blockid_therapy\",\"sec_therapy\"],axis=1).reset_index().rename(columns={\"sha_virus\":\"sha\",\"blockid_virus\":\"blockid\",\"sec_virus\":\"sec\"})\nSingleSentence=[]\nfor i in dat_joined_vt.index:\n    SingleSentence.append(SameSentenceCheck(dat_joined_vt.block[i],dat_joined_vt.pos_virus[i],dat_joined_vt.pos_therapy[i]))\ndat_joined_vt.insert(len(dat_joined_vt.columns),'same_sentence',SingleSentence)\ndat_joined_vt.to_csv(\"Overlaps_Virus_Therapy.csv\")\n","63f60236":"Make2DPlot(dat_joined_vt,\"virus\",\"therapy\")\nMake2DPlot(dat_joined_vt,\"virus\",\"therapy\",single_sentence_plots=True)","09a6241b":"# Prune and join, and extract overlap counts\ndat_joined_vd=dat_drugs.join(dat_viruses, rsuffix='_virus',lsuffix=\"_drug\")\ndat_joined_vd=dat_joined_vd[dat_joined_vd.notna().word_drug & dat_joined_vd.notna().word_virus]\n\ndat_joined_vd=dat_joined_vd.drop([\"sha_drug\",\"blockid_drug\",\"sec_drug\"],axis=1).reset_index().rename(columns={\"sha_virus\":\"sha\",\"blockid_virus\":\"blockid\",\"sec_virus\":\"sec\"})\nSingleSentence=[]\nfor i in dat_joined_vd.index:\n    SingleSentence.append(SameSentenceCheck(dat_joined_vd.block[i],dat_joined_vd.pos_drug[i],dat_joined_vd.pos_drug[i]))\ndat_joined_vd.insert(len(dat_joined_vd.columns),'same_sentence',SingleSentence)\ndat_joined_vd.to_csv(\"Overlaps_Virus_Drug.csv\")\n","1df58df6":"drugsubset=[\"naproxen\",\"clarithromycin\",\"chloroquine\",\"kaletra\",\"Favipiravir\",\"Avigan\",'hydroxychloroquine','baricitinib']\nMake2DPlot(dat_joined_vd[dat_joined_vd.word_drug.isin(drugsubset)],\"virus\",\"drug\")\nMake2DPlot(dat_joined_vd[dat_joined_vd.word_drug.isin(drugsubset)],\"virus\",\"drug\",single_sentence_plots=True)","d1380b5e":"# Prune and join, and extract overlap counts\ndat_joined_dt=dat_drugs.join(dat_therapies, rsuffix='_therapy',lsuffix=\"_drug\")\ndat_joined_dt=dat_joined_dt[dat_joined_dt.notna().word_drug & dat_joined_dt.notna().word_therapy]\n\ndat_joined_dt=dat_joined_dt.drop([\"sha_drug\",\"blockid_drug\",\"sec_drug\"],axis=1).reset_index().rename(columns={\"sha_therapy\":\"sha\",\"blockid_therapy\":\"blockid\",\"sec_therapy\":\"sec\"})\nSingleSentence=[]\nfor i in dat_joined_dt.index:\n    SingleSentence.append(SameSentenceCheck(dat_joined_dt.block[i],dat_joined_dt.pos_drug[i],dat_joined_dt.pos_therapy[i]))\ndat_joined_dt.insert(len(dat_joined_dt.columns),'same_sentence',SingleSentence)\ndat_joined_dt.to_csv(\"Overlaps_Drug_Therapy.csv\")","fac2cfb4":"Make2DPlot(dat_joined_dt[dat_joined_dt.word_drug.isin(drugsubset)],\"drug\",\"therapy\")\nMake2DPlot(dat_joined_dt[dat_joined_dt.word_drug.isin(drugsubset)],\"drug\",\"therapy\",single_sentence_plots=True)","4535bc39":"# Prune and join, and extract overlap counts\ndat_joined_de=dat_drugs.join(dat_exps, rsuffix='_exp',lsuffix=\"_drug\")\ndat_joined_de=dat_joined_de[dat_joined_de.notna().word_drug & dat_joined_de.notna().word_exp]\n\ndat_joined_de=dat_joined_de.drop([\"sha_drug\",\"blockid_drug\",\"sec_drug\"],axis=1).reset_index().rename(columns={\"sha_exp\":\"sha\",\"blockid_exp\":\"blockid\",\"sec_exp\":\"sec\"})\nSingleSentence=[]\nfor i in dat_joined_de.index:\n    SingleSentence.append(SameSentenceCheck(dat_joined_de.block[i],dat_joined_de.pos_drug[i],dat_joined_de.pos_exp[i]))\ndat_joined_de.insert(len(dat_joined_de.columns),'same_sentence',SingleSentence)\ndat_joined_de.to_csv(\"Overlaps_Drug_Experiment.csv\")","086d4a8d":"Make2DPlot(dat_joined_de[dat_joined_de.word_drug.isin(drugsubset)],\"drug\",\"exp\")\nMake2DPlot(dat_joined_de[dat_joined_de.word_drug.isin(drugsubset)],\"drug\",\"exp\",single_sentence_plots=True)","160b21a1":"dat_joined_vtd=dat_therapies.join(dat_viruses, rsuffix='_virus',lsuffix=\"_therapy\").join(dat_drugs)\ndat_joined_vtd=dat_joined_vtd[dat_joined_vtd.notna().word_therapy & dat_joined_vtd.notna().word_virus & dat_joined_vtd.notna().word]\ngrouped_vtd=dat_joined_vtd.groupby(['word_therapy','word_virus','word'])\ngrouped_vtd.count().sha_therapy","23cbda1d":"dat_joined_vtd=dat_joined_vtd.reset_index().drop(['sha_therapy','blockid_therapy','sec_therapy','sha_virus','blockid_virus','sec_virus'],axis=1).rename(columns={'word':'word_drug','pos':'pos_drug'}).set_index('sha')\ndat_joined_vtd=dat_joined_vtd[[\"block\",\"sec\",\"blockid\",\"word_therapy\",\"pos_therapy\",\"word_virus\", \"pos_virus\",\"word_drug\",\"pos_drug\"]]\ndat_joined_vtd.to_csv(\"Overlaps_Drug_Therapy_Virus.csv\")\n","f78e186d":"dat_viruses.word.value_counts()","6f53d05e":"\nOverlapsVirus=pd.read_csv(\".\/Overlaps_Virus_Drug.csv\")\nOverlapsTherapy=pd.read_csv(\".\/Overlaps_Drug_Therapy.csv\")\n\nVirusWords=['novel_coronavirus','covid-19','2019-ncov','sars-cov-2','sars-cov_2']\n\nPapersWithVirusDrugOverlap=OverlapsVirus[OverlapsVirus.word_virus.isin(VirusWords)].sha.unique()\nPapersWithVirusMention=dat_viruses[dat_viruses.word.isin(VirusWords)].sha.unique()\nOverlapsTherapy=OverlapsTherapy[OverlapsTherapy.same_sentence==1]\n","44ccf91e":"# This two helper function does its best to extract the year from the \n#  inconsistently formatted metadata\n\ndef ConvertDateToYear(datestring):\n    import dateutil.parser as parser\n\n    if(pd.notna(datestring)):\n        try:\n            date=parser.parse(str(datestring),fuzzy=True)\n            return date.year\n        except ValueError:\n            return 0\n    else:\n        return 0\n    \n\n","5aeaa8fe":"# Take the elements we need out of the paper metadata\nmeta=pd.read_csv(\"\/kaggle\/input\/CORD-19-research-challenge\/metadata.csv\")\nyears=meta.publish_time.apply(ConvertDateToYear)\nmeta.insert(len(meta.columns),'year',years)\nmeta_to_use=meta.set_index('sha')[['doi','title','year','abstract','journal','cord_uid']]\n\n# And mix it in\nOverlapsTherapy=OverlapsTherapy.set_index('sha').join(meta_to_use).reset_index()","7960cb2f":"if(InsertCiteData):\n    citefile=pd.read_csv(\"\/kaggle\/input\/coronawhy\/cord19_MicrosoftAcademic_citation_metadata.csv\").set_index('cord_uid')['CitationCount']\n    OverlapsTherapy=OverlapsTherapy.set_index('cord_uid').join(citefile).reset_index()","1688194f":"# Extract the relevant sentences where matches were found\nSentenceEnders=\"\\. |; \\! \\? \"\nExtractedSentences=[]\nfor i in OverlapsTherapy.index:\n    sentences = re.split(SentenceEnders,OverlapsTherapy.block.loc[i])\n    RunningCount=0\n    ExtractedSentences.append(np.NaN)\n\n    for s in range(0,len(sentences)):\n        RunningCount=RunningCount+len(sentences[s])\n        if(OverlapsTherapy.pos_drug.loc[i]<RunningCount):\n            ExtractedSentences[-1]=sentences[s]\n            break\n            \nOverlapsTherapy.insert(len(OverlapsTherapy.columns),'sentence',ExtractedSentences)","8ae97e4c":"# Check for coincidences in block and paper\nCoronaInPaper=OverlapsTherapy.sha.isin(PapersWithVirusMention)\nCoronaInBlock=OverlapsTherapy.sha.isin(PapersWithVirusDrugOverlap)\nOverlapsTherapy.insert(len(OverlapsTherapy.columns),'corona_paper',CoronaInPaper)\nOverlapsTherapy.insert(len(OverlapsTherapy.columns),'corona_block',CoronaInBlock)","bc876698":"#Tidy and trim\nOverlapsTherapy=OverlapsTherapy.rename(columns={'word_drug':'drug','sec':'section','block':'paragraph'}).drop(['blockid','pos_drug','pos_therapy','Unnamed: 0','same_sentence'],axis=1)","d441dce5":"# This is the final file that is used as input to the visualization stage\nOverlapsTherapy=OverlapsTherapy.drop_duplicates()\nOverlapsTherapy.to_csv(\"DrugVisData.csv\")","1e771f7f":"if(CalculateNegations):\n\n    data = OverlapsTherapy\n    output_data = data\n    out_dir = '.\/'\n\n\n    # negated term list (use the human annotated version)\n    neg_list = pd.read_csv('\/kaggle\/input\/negation-words\/neg_list_complete.txt', sep='\\t', header=0)\n    neg = neg_list['ITEM'].values\n    neg_term = [' ' + item + ' ' for item in neg]\n    neg_term.extend(item + ' ' for item in neg)\n\n\n    for i in data.index:\n        if pd.isnull(data.loc[i,'sentence']):\n            output_data.loc[i,'match_quality'] = 2\n        else:\n            # tag negated or affirmed based on string matching --- negation term list\n            # add one space to prevent loss of 'no ', 'not ', ... etc.\n            if any(substring in ' ' + data.loc[i,'sentence'].lower() for substring in neg_term):\n                output_data.loc[i,'match_quality'] = 1\n            else:\n                output_data.loc[i,'match_quality'] = 2\n\n    # save results in a output file\n    output_data.to_csv('DrugVisData.csv',index=False)","9fd7f364":"if(CalculateNegations):\n    negated_drug_mentions = output_data.loc[output_data.match_quality==2,'drug']\\\n                                        .groupby(output_data['drug'])\\\n                                        .value_counts()\\\n                                        .droplevel(level=0)\n    print('Top 20 most negated drugs:\\n')\n    print(negated_drug_mentions.nlargest(20))","6f1c9f79":"if(CalculateNegations):\n    drug_mentions = output_data.groupby([output_data['drug'],output_data.match_quality])\\\n                                .size().to_frame(name = 'size').reset_index()\\\n                                .pivot(index='drug',columns='match_quality',values='size').fillna(0).reset_index()\n\n    drug_mentions['Percentage Negations'] = (drug_mentions[1]*100)\/(drug_mentions[2]+drug_mentions[1])\n\n    drug_mentions.hist(column='Percentage Negations')","00f5af26":"if(CalculateNegations):\n    # Drugs with 100% negation\n    drug_mentions.nlargest(n=1,columns='Percentage Negations',keep='all').plot.bar('drug',[1.0,2.0],figsize=(15,6))","bf8c5ce8":"import tensorflow as tf\nfrom tensorflow.keras.layers import Dense, Input\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom kaggle_datasets import KaggleDatasets\nimport transformers\nfrom transformers import TFAutoModel, AutoTokenizer\nfrom tqdm.notebook import tqdm\nfrom tokenizers import Tokenizer, models, pre_tokenizers, decoders, processors","14d1d536":"!mkdir model_dir\n#!gsutil cp gs:\/\/coronaviruspublicdata\/re_final_best\/new\/config.json model_dir\/config.json\n#!gsutil cp gs:\/\/coronaviruspublicdata\/re_final_best\/new\/sigmoid.pickle model_dir\/sigmoid.pickle\n#!gsutil cp gs:\/\/coronaviruspublicdata\/re_final_best\/new\/tf_model.h5 model_dir\/tf_model.h5\n# Snapshot weights below uncomment if you want to use\n!gsutil cp gs:\/\/coronaviruspublicdata\/re_snapshot\/4_13_2020\/config.json model_dir\/config.json\n!gsutil cp gs:\/\/coronaviruspublicdata\/re_snapshot\/4_13_2020\/sigmoid.pickle model_dir\/sigmoid.pickle\n!gsutil cp gs:\/\/coronaviruspublicdata\/re_snapshot\/4_13_2020\/tf_model.h5 model_dir\/tf_model.h5","c628cf58":"def build_model(transformer, max_len=256):\n    \"\"\"\n    https:\/\/www.kaggle.com\/xhlulu\/jigsaw-tpu-distilbert-with-huggingface-and-keras\n    \"\"\"\n    input_ids = Input(shape=(max_len, ), dtype=tf.int32)\n\n    x = transformer(input_ids)[0]\n    x = x[:, 0, :]\n    x = Dense(1, activation='sigmoid', name='sigmoid')(x)\n\n    # BUILD AND COMPILE MODEL\n    model = Model(inputs=input_ids, outputs=x)\n    model.compile(\n        loss='binary_crossentropy', \n        metrics=['accuracy'], \n        optimizer=Adam(lr=1e-5)\n    )\n    return model\n\ndef save_model(model, transformer_dir='transformer'):\n    \"\"\"\n    Special function to load a keras model that uses a transformer layer\n    \"\"\"\n    transformer = model.layers[1]\n    touch_dir(transformer_dir)\n    transformer.save_pretrained(transformer_dir)\n    sigmoid = model.get_layer('sigmoid').get_weights()\n    pickle.dump(sigmoid, open('sigmoid.pickle', 'wb'))\n\ndef load_model(pickle_path, transformer_dir='transformer', max_len=512):\n    \"\"\"\n    Special function to load a keras model that uses a transformer layer\n    \"\"\"\n    transformer = TFAutoModel.from_pretrained(transformer_dir)\n    model = build_model(transformer, max_len=max_len)\n    sigmoid = pickle.load(open(pickle_path, 'rb'))\n    model.get_layer('sigmoid').set_weights(sigmoid)\n    \n    return model\n\n","fa24bb58":"model = load_model(\"model_dir\/sigmoid.pickle\", \"model_dir\")","d7cbd057":"def regular_encode(texts, tokenizer, maxlen=512):\n    enc_di = tokenizer.batch_encode_plus(\n        texts, \n        return_attention_masks=False, \n        return_token_type_ids=False,\n        pad_to_max_length=True,\n        max_length=maxlen\n    )\n    \n    return np.array(enc_di['input_ids'])","bcf2c0ee":"tokenizer = AutoTokenizer.from_pretrained(\"allenai\/biomed_roberta_base\")","fb7b31c0":"def MagicClassifierFunction(sentence, tokenizer):\n    encode_text = regular_encode(sentence, tokenizer)\n    preds = model.predict(encode_text)\n    BooleanDecision = (preds >= .4) \n    return BooleanDecision ","b92be9e3":"# Due to runtime constraints on kaggle we can only apply the ML classifier to a subset.\n# Given less restriction on submission we could apply to all...\n#SelectionToML=(output_data.corona_paper==True) & (output_data.section.isin(['title','abstract'])&output_data.notna().sentence)\nSelectionToML=(output_data.corona_paper==True) & (output_data.notna().sentence)\nSentencesForML=output_data[SelectionToML].sentence.unique()\n","03e4da9d":"# Run the classifier\n%time preds=MagicClassifierFunction(SentencesForML,tokenizer)\n","20fa1d41":"for i in SentencesForML[preds[:,0]]:\n    output_data.match_quality[i == output_data.sentence]=3","36f27eb6":"#Do not allow claim as covid-19 papers any from before 2019\noutput_data.corona_paper[output_data.year<2019]=False\noutput_data.corona_block[output_data.year<2019]=False","33eae30f":"output_data=output_data.reset_index()\noutput_data=output_data.set_index(\"cord_uid\")\noutput_data.to_csv(\"DrugVisData.csv\")","8712ccc0":"TitleAndAbstractMentions=dat_exps[(dat_exps.sec=='title') | (dat_exps.sec=='abstract')].set_index('sha')\n\nIndicatorMatrix=pd.read_csv(\"\/kaggle\/working\/DrugVisData.csv\")\nIndicatorMatrix=IndicatorMatrix.set_index('sha').join(TitleAndAbstractMentions)\nIndicatorMatrix=IndicatorMatrix[IndicatorMatrix.notna().word].rename({\"word\":\"study_word\"},axis=1)\n\n# Load data we need to make the categorization of study words into \n#  paper category indications\ncatfile=np.loadtxt(\"\/kaggle\/input\/textmatchesvt\/study_classes.csv\",dtype='str', delimiter=',')\ncategories=np.unique([i.lower() for i in catfile[:,1]])\nfor cat in categories:\n    IndicatorMatrix[cat]=False\nfor c in catfile:\n    word=c[0].lower().replace(\" \", \"_\")\n    category=c[1].lower().replace(\" \", \"_\")\n    IndicatorMatrix.loc[(IndicatorMatrix.study_word==word),category]=True\n\n# Build up new data structure with categorization judgements\nIndicatorMatrix=IndicatorMatrix.reset_index()\ngb=IndicatorMatrix.groupby(by=['sha','drug'])\ncat_scores={}\nfor c in categories:\n    cat_scores[c]=[]\nshas=[]\ndrugs=[]\nclinical_score=[]\ngb_groups = gb.groups\nfor g in gb_groups.keys():\n    records=IndicatorMatrix.iloc[gb_groups[g]]\n    shas.append(g[0])\n    drugs.append(g[1])\n    for c in categories:\n        cat_scores[c].append(sum(records[c]))\ndata=[shas,drugs]\ncolnames=['sha','drugs']\nfor c in categories:\n    data.append(cat_scores[c])\n    colnames.append(c)\nstudydata=pd.DataFrame(np.array(data).T,columns=colnames)\nfor c in categories:\n    studydata[c]=studydata[c].apply(pd.to_numeric)\nstudydata.insert(len(studydata.columns),'category',np.NaN)\n   \n    ","3385dd43":"# Papers are categorized by \"popular vote\" by search term.\n\nprint(\"Paper classifications by 'popular vote'\" )\nprint(\"=======================================\")\nprint(\"No Category: \" + str(len(studydata[studydata[categories].max(axis=1)==0].sha.unique())))\nfor cat in categories:\n    print(cat + \": \"+  str(len(studydata[(studydata[categories].max(axis=1)==studydata[cat]) & (studydata[categories].max(axis=1)>0)].sha.unique())))\n    studydata.category[(studydata[categories].max(axis=1)==studydata[cat]) & (studydata[categories].max(axis=1)>0)]=cat\n    \nIndicatorMatrix.drop('study_word',axis=1).drop_duplicates()\nstudydata=studydata[['sha','category']].drop_duplicates().set_index('sha')\nIndicatorMatrix=IndicatorMatrix=IndicatorMatrix.set_index('sha').join(studydata).reset_index().rename({\"category\":\"word\"},axis=1)    \n#IndicatorFields=['sha','paragraph','drug','section','doi','title','year','abstract','sentence','corona_paper','corona_block','word','sec','match_quality']\n","a41de278":"ForGeoMatch=IndicatorMatrix[(IndicatorMatrix.word=='clinical')&(IndicatorMatrix.corona_paper==True)][['sha','cord_uid','drug','year','doi']]\nForGeoMatch=ForGeoMatch.drop_duplicates()\nForGeoMatch.to_csv(\"ClinicalPapers_ForGeo.csv\")","79795ee1":"IndicatorMatrix=IndicatorMatrix.drop(['blockid', 'study_word', 'pos', 'clinical', 'in-silico', 'in-vitro','pre-clinical'],axis=1).drop_duplicates()\nIndicatorMatrix.to_csv(\"IndicatorMatrix.csv\")","4a9474d2":"### Add in additional curated drug terms\n\nWe add curated drug terms WHO data copied from \"Landscape analysis of therapeutics as 21st March 2020\" located at https:\/\/www.who.int\/blueprint\/priority-diseases\/key-action\/Table_of_therapeutics_Appendix_17022020.pdf?ua=1 on April 11, 2020\n\nExcelra data copied from https:\/\/www.excelra.com\/covid-19-drug-repurposing-database\/data on April 11, 2020.\n\nSource descritpion:\nExcelra COVID-19 Drug Repurposing Database\nExcelra\u2019s open-access COVID-19 Drug Repurposing database is a synoptic compilation of small molecules and biologics with known safety and efficacy profiles; which can rapidly enter either Phase 2 or 3 or may even be used directly in clinical settings against COVID-19.\n","beaf5559":"Now, let's look at the distribution of percentage negated mentions for drugs:","96118807":"The below is a basic relation classification model trained at the following link https:\/\/github.com\/CoronaWhy\/task-vt\/pull\/16\/files and inspired by https:\/\/www.kaggle.com\/xhlulu\/jigsaw-tpu-xlm-roberta. The goal of this model is given a sentence with a drug and a treatment classify if the treatment is actually related to the drug.","20ee876f":"Virus \/ Drug co-occurrences","cd110007":"Tripartite co-occurrences","498dab17":"## Table of Content\n1. [Preamble](#preamble)\n2. [Executive Summary](#exec_summary)\n3. [Executable Code](#code)\n4. [List of Contributors](#contrib)\n","ca506c76":"**Virus \/ Therapy co-occurrences **","d78b4028":"This notebook was produced by the Task-VT subgroup of the CoronaWhy research consortium.  We acknowledge significant scientific or technical contributions to this notebook from the following: \n\n\n**Hillary Adragna**: Pharmacology consultant\n\n**Mahmoud Bahaa**: NLP\/ML Developer\n\n**Sam Brewer**: Lead Visualization Developer\n\n**Aradhana Chaturvedi**: NLP\/ML Developer\n\n**Isaac Godfried**: NLP\/ML Developer \n\n**Mike Honey**: Visualization Consultant\n\n**Ben Jones**: Lead Data Analysis Developer\n\n**Andrea Kropp**: NLP \/ Search Term Enrichment\n\n**Siddharta Mitra**: Development support and annotation\n\n**Dan Sosa**: Task-VT Team Coordinator\n\n**Malakiva Suresh**: NLP\/ML Developer\n\n","7bd4bda4":"### Handling n-gram Search Terms\n\nOur text matching approach is to match on single words. However, some of the search terms are multi-word\nentities.  We handle this by replacing spaces in multi-word entities by underscores, and then making this\nsame replacement on the text corpus, implemented via an ngram_map object. The function below constructs the relevant\nword lists and ngram_maps.","f6d9f292":"# 3. Executable Code <a id=\"code\"><\/a>","1bae3343":"# 2. Executive Summary <a id=\"exec_summary\"><\/a>","eafabae1":"Let's take a look at the most negated and most asserted drugs based on number of mentions:","167fc458":"**The primary deliverables from this work are the visualizations embedded in the notebook below.  For optimum viewing experience in full-screen, we recommend visiting these links:**\n\n[COVID-19 Drug Treatment Explorer](https:\/\/app.powerbi.com\/view?r=eyJrIjoiMzRlZTliMTItZGE0OS00ZTNjLTllM2YtMTlmN2ZhMDRkYWRmIiwidCI6ImRjNTQ3MWFjLWM4NmMtNGNiZS04YTVjLTI0MzUxZGRmMjRjOCIsImMiOjN9)\n\n<a href=\"https:\/\/app.powerbi.com\/view?r=eyJrIjoiMzRlZTliMTItZGE0OS00ZTNjLTllM2YtMTlmN2ZhMDRkYWRmIiwidCI6ImRjNTQ3MWFjLWM4NmMtNGNiZS04YTVjLTI0MzUxZGRmMjRjOCIsImMiOjN9\"> <img width=\"300\" src=\"https:\/\/storage.googleapis.com\/coronawhy-vt\/PowerBIData\/Dashboard1.png\"> \n\n\n[COVID-19 Drug Treatment Matrix](https:\/\/app.powerbi.com\/view?r=eyJrIjoiMjc4NDZkYzktZmUyZC00ZThkLTkyYjAtY2EzNzk2ZDliOTk2IiwidCI6ImRjNTQ3MWFjLWM4NmMtNGNiZS04YTVjLTI0MzUxZGRmMjRjOCIsImMiOjN9)\n\n<a href=\"https:\/\/app.powerbi.com\/view?r=eyJrIjoiMjc4NDZkYzktZmUyZC00ZThkLTkyYjAtY2EzNzk2ZDliOTk2IiwidCI6ImRjNTQ3MWFjLWM4NmMtNGNiZS04YTVjLTI0MzUxZGRmMjRjOCIsImMiOjN9\"> <img width=\"300\"  src=\"https:\/\/storage.googleapis.com\/coronawhy-vt\/PowerBIData\/Dashboard2.png\"> <\/a>\n\n","53f46e39":"In the following sections, the code that produces the visualizible output will be\npresented. Running the full notebook will produce the output files used above.","5a714db7":"### Making the matrix for the study-types plot","10b1d612":"Drug \/ Therapy co-occurrences\n","4c259ab0":"### Overlap Extraction\n\nHaving found the search terms, we now seek co-occurences of pairs and triplets in the same sentence or paragraph.  Counts of co-occurences are plotted on 2D matrix plots.","020b75de":"### Text matching in the CORD-19 dataset\n\nAll content extraction in this notebook is based on string matching. We use two methods:\n1.  Lemmatized matching - useful for terms that may be used in different contexts like \"treatment \/ treats \/ treat\", etc. However, it's slower, especially for full-text search\n2.  Direct matching - match string directly when its a proper name, e.g. drug names or coronavirus synonyms\n\nThese are implemented in the two functions below, and then applied to drug names, virus names, treatment words and study types \n","f4b0d88f":"### Drug Search Term List Construction \n\nThis section uses publically source-able data to construct a list of drug terms to search.\n\nThe starting point is the list of all prescribable drugs from the RxNorm database:\n   https:\/\/download.nlm.nih.gov\/rxnorm\/RxNorm_full_prescribe_03022020.zip\n\nFrom this list we then remove the following un-useful names:\n1. names of chemical elements, from this periodic table:\n  https:\/\/gist.github.com\/GoodmanSciences\/c2dd862cd38f21b0ad36b8f96b4bf1ee\n2. names of animals, from this list: \n  https:\/\/gist.github.com\/atduskgreg\/3cf8ef48cb0d29cf151bedad81553a54   \n3. names of fruits and vegetables, from this list: \n  https:\/\/alphabetizer.flap.tv\/lists\/list-of-fruits-and-vegetables.php\n4. Remove dosage terms by regular expression\n5. Remove delivery terms and source terms, using hand-curated lists\n\nWe then remove terms with dosage or delivery information,\nor unusual punctuation within the drug name.\n\nWe restrict to drug names of more than 5 characters and consisting of between 1 and 3 words.\n  \nFor convenience these publically available files have been collected in the kaggle\n directory rxnorm_inputdata.","0f3bc162":"# List of Contributors (alphabetical) <a id=\"contrib\"><\/a>","ecf6834f":"### Preparation for Visualization\nThis section implements some data manipulation and cleaning to prepare for visualization.","1e678f64":"### Negation Detection\n\nA trigger term list (citation1) is used to identify negation for each extracted drug-treatment co-occurence.\n\nPros:\n\n* Quick and easy approach\n\nCons:\n\n* Sentence hierarchical structure is not accounted for. Contrastive conjunctions (but, however, etc.) and long sentences with each part talking about different drugs could cause false positives.\n\nWe follow the methods of \"Clinical Text Summarization with Syntax-Based Negation and Semantic Concept Identification\", Weng, Wei-Hung et al, 2020, arXiv arXiv:2003.00353.","34312aff":"Study type \/ drug co-occurrences","5a0fc135":"# 1. Preamble <a id=\"preamble\"><\/a>\nThis is a notebook created by a collaborative effort of the Task-VT sub-team within the CoronaWhy research consortium. For more information about CoronaWhy:\n- Visit our [website](https:\/\/www.coronawhy.org) to learn more.\n- Read our [story](https:\/\/medium.com\/@arturkiulian\/im-an-ai-researcher-and-here-s-how-i-fight-corona-1e0aa8f3e714).\n- Visit our [main notebook](https:\/\/www.kaggle.com\/arturkiulian\/coronawhy-org-global-collaboration-join-slack) for historical context on how this community started.","e6416c43":"### Relation Extraction Model ","6a3820b9":"### Imports and Installs","22f3d028":"![](https:\/\/storage.googleapis.com\/coronawhy-vt\/PowerBIData\/HeaderForKaggle.png)","493285de":"**Methodology**\n\nThis notebook is targetted at the Task: \"What do we know about vaccines and therapeutics?\", and in particular presents evidence for the sub-task concerning \"Effectiveness of drugs being developed and tried to treat COVID-19 patients\".  \n\nTo address this question we sought to identify all papers in the dataset which indicated that a named drug had been used or considered as a treatment \/ therapy for COVID-19.  We consider every named drug that is prescribable in the United States, as catalogued in the publicly available RxNorm database, strip out problematic terms using word-lists, and combine with drug lists of interest from the WHO and Excelra. \n\nSentences are extracted from each paper indicating what is stated about this drug as a treatment.  These statements are classified by relevance to COVID-19: whether they are mentioned in the same paragraph or paper as the drug, for example.  The statements can also be sorted by drug name and section of paper where it is mentioned (title\/abstract\/elsewhere).  \n\nWe provide the user with the option to keep all sentences with drug-treat co-occurrences (Loose), remove false positive drug treatments using negation detection (Medium) or by using both negation detection and a pre-trained BERT sentence classifier (Tight).\n\nTwo tools for data visualization have been developed. One that allows browsing of statements about drugs in a treatment context providing journal and citation information, dates, and links to articles. Paper titles and abstracts and contexts are browsable in connection with each sentence. A second visualization allows for cross-categorization between drug name and study type in the paper: clinical, pre-clinical, in-silico or in-vitro, assessed by word keyword search in the titles and abstracts.\n\nThe code in later sections of this notebook generates the data that feeds the above visualizations. It uses only the raw CORD-19 data release and some cited publically available data tables, and has a run-time of a few hours.  The above GUI can be used without re-running this notebook to view the results. To facilitate speed of execution we include in this notebook some intermediate files, uploaded to the TextMatchVT Kaggle Dataset, so that the most interesting parts of the code can be executed with more rapid turnaround.\n\nWe hope that this tool may prove to be a valuable resource for anyone who is trying to navigate the wide and rapidly evolving literature on treatments for COVID-19, collecting into one place all statements that have been made about a large array of drug candidates that could be of relevance.\n\nDISCLAIMER: We do NOT claim to be offering any medical advice with this tool. This dashboard is simply intended as a means of facilitating exploration of drug treatment mentions in CORD-19 literature. Proper due diligence, reading the full paper, and understanding the full context of these drug mentions will be required before considering any medical implications. We stress that this tool is meant for EXPLORATION.\n\n\n"}}