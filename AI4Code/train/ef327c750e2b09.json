{"cell_type":{"1ec1cbe2":"code","214b446d":"code","fba20471":"code","117f3887":"code","e57c0f6c":"code","9c8b8d9a":"code","9b2ba77d":"code","28be8fb8":"code","7f9aaa08":"code","86f2f5b6":"code","cdbd45b1":"code","d1d9c684":"code","1533add1":"code","76a50a91":"code","a2c0e26b":"code","e7d428e3":"code","6720ea19":"code","4358c563":"code","cb4d7645":"code","0895ca80":"code","a809d10b":"code","642e66bf":"markdown","9d5e4d58":"markdown"},"source":{"1ec1cbe2":"import numpy as np\nimport pandas as pd \nimport os\nprint(os.listdir(\"..\/input\"))\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport cv2\nimport gc","214b446d":"df = pd.read_csv('..\/input\/openimagevallabel\/validation-annotations-human-imagelabels.csv', usecols=[0,2,3])\ndf = df[df.Confidence == 1]\nclasses = np.array(['\/m\/01g317', '\/m\/09j2d', '\/m\/04yx4', '\/m\/0dzct', '\/m\/07j7r', '\/m\/05s2s', '\/m\/03bt1vf', '\/m\/07yv9', '\/m\/0cgh4', '\/m\/01prls', '\/m\/09j5n', '\/m\/0jbk', '\/m\/0k4j', '\/m\/05r655', '\/m\/02wbm', '\/m\/0c9ph5', '\/m\/083wq', '\/m\/0c_jw', '\/m\/03jm5', '\/m\/0d4v4'])\nli = []\nfor i in classes:\n    li.append(df[df.LabelName == i])\ndf = pd.concat(li).sample(frac=1).reset_index(drop=True)\ndel li\ngc.collect()\ndf.head()","fba20471":"labels = df.LabelName.tolist()\nImageid = df.ImageID.values\nprint(len(df))","117f3887":"from keras.preprocessing.image import load_img, img_to_array\nfrom keras.layers import Dense, Conv2D, PReLU, BatchNormalization, MaxPooling2D, Dropout, Flatten\nimport keras\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\nfrom keras import backend as K\nfrom keras.models import load_model, Sequential\nfrom tqdm import tqdm_notebook\nfrom tqdm import tqdm\nfrom keras.optimizers import Adam","e57c0f6c":"df.head(),gc.collect()","9c8b8d9a":"X_train = [np.array(load_img('..\/input\/open-image-val\/validation\/validation\/{}.jpg'.format(i),target_size=(100,100), grayscale=True))\/255 for i in tqdm(Imageid[10000:20000])]","9b2ba77d":"X_Val = [np.array(load_img('..\/input\/open-image-val\/validation\/validation\/{}.jpg'.format(i),target_size=(100,100), grayscale=True))\/255 for i in tqdm(Imageid[:2000])]","28be8fb8":"#Y_train = labels[10000:20000]\n#Y_Val = labels[:2000]\nclasses = classes.tolist()\nY_train, Y_Val = [], []\nfor i in tqdm(labels[10000:20000]):\n    temp = np.zeros(20)\n    temp[classes.index(i)] = 1\n    Y_train.append(temp)\n    del temp\nfor i in tqdm(labels[:2000]):\n    temp = np.zeros(20)\n    temp[classes.index(i)] = 1\n    Y_Val.append(temp)\n    del temp\nY_train = np.array(Y_train)\nY_Val = np.array(Y_Val)\ngc.collect(), Y_train[0]","7f9aaa08":"nn = Sequential()\nnn.add(BatchNormalization(input_shape=(100, 100, 1)))\nnn.add(Conv2D(4, kernel_size=(2,2), strides=(1,1)))\nnn.add(PReLU())\nnn.add(BatchNormalization())\nnn.add(Dropout(0.25))\nnn.add(Conv2D(8, kernel_size=(2,2), strides=(1,1)))\nnn.add(PReLU())\nnn.add(BatchNormalization())\nnn.add(Dropout(0.25))\nnn.add(Conv2D(16, kernel_size=(2,2), strides=(2,2)))\nnn.add(PReLU())\nnn.add(BatchNormalization())\nnn.add(Dropout(0.25))\nnn.add(Conv2D(32, kernel_size=(2,2), strides=(1,1)))\nnn.add(PReLU())\nnn.add(BatchNormalization())\nnn.add(Dropout(0.25))\nnn.add(Conv2D(32, kernel_size=(2,2), strides=(2,2)))\nnn.add(PReLU())\nnn.add(BatchNormalization())\nnn.add(Dropout(0.25))\nnn.add(Conv2D(32, kernel_size=(2,2), strides=(2,2)))\nnn.add(PReLU())\nnn.add(BatchNormalization())\nnn.add(Dropout(0.25))\nnn.add(Flatten())\nnn.add(Dense(2048))\nnn.add(PReLU())\nnn.add(BatchNormalization())\nnn.add(Dropout(0.25))\nnn.add(Dense(1024))\nnn.add(PReLU())\nnn.add(BatchNormalization())\nnn.add(Dropout(0.25))\nnn.add(Dense(512))\nnn.add(PReLU())\nnn.add(BatchNormalization())\nnn.add(Dropout(0.25))\nnn.add(Dense(128))\nnn.add(PReLU())\nnn.add(BatchNormalization())\nnn.add(Dropout(0.25))\nnn.add(Dense(50))\nnn.add(PReLU())\nnn.add(BatchNormalization())\nnn.add(Dropout(0.25))\nnn.add(Dense(25))\nnn.add(PReLU())\nnn.add(BatchNormalization())\nnn.add(Dropout(0.25))\nnn.add(Dense(20, activation='softmax'))","86f2f5b6":"nn.compile(loss=keras.losses.categorical_crossentropy, metrics=['accuracy'], optimizer='adam')\nnn.summary()","cdbd45b1":"X_train = np.array(X_train).reshape((10000,100,100,1))\nX_Val = np.array(X_Val).reshape((2000,100,100,1))","d1d9c684":"gc.collect()","1533add1":"nn.fit(X_train, Y_train, validation_data=(X_Val,Y_Val), batch_size=100, epochs=50, verbose=2)","76a50a91":"del X_train, Y_train, X_Val, Y_Val, df\ngc.collect()","a2c0e26b":"df = pd.read_csv('..\/input\/inclusive-images-challenge\/stage_1_sample_submission.csv', usecols=[0])\nim = df.image_id.tolist()\ndf.head()","e7d428e3":"X_test = [np.array(load_img('..\/input\/inclusive-images-challenge\/stage_1_test_images\/{}.jpg'.format(i),target_size=(100,100), grayscale=True))\/255 for i in tqdm_notebook(im)]","6720ea19":"X_test = np.array(X_test).reshape((32580,100,100,1))","4358c563":"pre = nn.predict(X_test).argsort(1)[:,:5]\ndel X_test\ngc.collect()","cb4d7645":"p = []\nfor it in tqdm(pre):\n    p.append(' '.join([classes[int(i)] for i in it]))","0895ca80":"df['labels'] = p\ndf.head()","a809d10b":"df.to_csv('sub.csv', index=False)","642e66bf":"# CNN with 20 Classes trained on Open Image Validation Set","9d5e4d58":"## If you find this kernel helpful, please upvote it.\n## If you have any questions or suggestions please let me know."}}