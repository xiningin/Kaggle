{"cell_type":{"7eff91e0":"code","aed1d3c0":"code","003cf948":"code","238facf1":"code","42bc88ca":"code","d1124db8":"code","dbd895f2":"code","1cc3b407":"code","8e5a77d4":"code","8e170f3a":"code","1c6209f7":"markdown","49e8ee55":"markdown","129bd31d":"markdown","812be894":"markdown","7950ef80":"markdown","6af0e1e2":"markdown","d4017648":"markdown","39ba1494":"markdown","85046cad":"markdown","002d4c8e":"markdown"},"source":{"7eff91e0":"!pip install -q git+https:\/\/github.com\/huggingface\/transformers.git","aed1d3c0":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os, gc, warnings\nwarnings.filterwarnings(\"ignore\")\n\n#import transformers\nfrom transformers import pipeline","003cf948":"url = '..\/input\/imdb-dataset-csv-file-with-reviews\/IMDB Dataset.csv'\ndf = pd.read_csv(url,header='infer')\nprint(\"Total Records: \",df.shape[0])","238facf1":"classifier = pipeline(\"zero-shot-classification\")","42bc88ca":"# Example\n\nseq = \"The GDP of many countries have been affected by this pandemic.\"\ncandidate_labels = [\"politics\", \"public health\", \"economics\"]\n\nclassifier(seq, candidate_labels)","d1124db8":"seq = \"The GDP of many countries have been affected by this pandemic.\"\ncandidate_labels = [\"politics\", \"public health\", \"economics\"]\n\nclassifier(seq, candidate_labels, multi_class=True)","dbd895f2":"sequences = [\n    \"Tenet is simply an incredible film with deep complex concepts to unravel well after the credits roll.\",\n    \"The Social Dilemma is densely packed yet lively and entertaining documentary\"\n]\ncandidate_labels = [\"positive\", \"negative\"]\n\nclassifier(sequences, candidate_labels)","1cc3b407":"sequences = [\n    \"Tenet is simply an incredible film with deep complex concepts to unravel well after the credits roll.\",\n    \"The Social Dilemma is densely packed yet lively and entertaining documentary\"\n]\ncandidate_labels = [\"positive\", \"negative\"]\nhypothesis_template = \"The sentiment of this review is {}.\"\n\nclassifier(sequences, candidate_labels,hypothesis_template=hypothesis_template)\n","8e5a77d4":"classifier = pipeline(\"zero-shot-classification\", model='xlm-roberta-large')","8e170f3a":"sequence = \"El dilema social es un documental densamente lleno pero animado y entretenido\" \ncandidate_labels = [\"positiva\", \"negativa\"]\n\nhypothesis_template = 'El sentimiento de esta revisi\u00f3n es {}.'\n\nclassifier(sequence, candidate_labels,hypothesis_template=hypothesis_template)","1c6209f7":"## Initializing Pipeline\n\n#### We can use this pipeline by passing in a sequence and a list of candidate labels. The pipeline assumes by default that only one of the candidate labels is true, returning a list of scores for each label which add up to 1.","49e8ee55":"#### As you can observe, the label **Economics** has high score which means this classifier has categorised this unseen sentence under \"Economics\".\n\nThis classifier can handle **multi-class classification** & to do so, simply pass multi_class=True. In this case, the scores will be independent, but each will fall between 0 and 1.","129bd31d":"#### By providing a more precise hypothesis template, we are able to see a more accurate classification of the second review.","812be894":"# Text Classification using Zero-Shot Learning\n\nIn this age of Social Media, Natural language processing (NLP) is a trending field. From machine-translation to text-sentiment analysis, NLP is an exciting field right now. In recent years, the data analytics community has begun to figure out some pretty effective methods of learning from the enormous amounts of unlabeled textual data available on the internet. The success of transfer learning from unsupervised models has allowed us to surpass virtually all existing benchmarks on downstream supervised learning tasks. One major advantage is that we see a very slow decrease in the reliance on large amounts of annotated data for NLP tasks. \n\n\n## What is zero-shot learning?\n\nTraditionally, zero-shot learning (ZSL) most often referred to a fairly specific type of task: learn a classifier on one set of labels and then evaluate on a different set of labels that the classifier has never seen before. Recently, especially in NLP, it's been used much more broadly to mean get a model to do something that it wasn't explicitly trained to do. A well-known example of this is in the GPT-2 paper where the authors evaluate a language model on downstream tasks like machine translation without fine-tuning on these tasks directly.\n\nThe definition is not all that important, but it is useful to understand that the term is used in various ways and that we should therefore take care to understand the experimental setting when comparing different methods. For example, traditional zero-shot learning requires providing some kind of descriptor (Romera-Paredes et al. 2015) for an unseen class (such as a set of visual attributes or simply the class name) in order for a model to be able to predict that class without training data. Understanding that different zero-shot methods may adopt different rules for what kind of class descriptors are allowed provides relevant context when communicating about these techniques.\n","7950ef80":"## Data","6af0e1e2":"## Zero Shot Classification for Non-English Languages\n\nThe pipeline allows us to perform Zero Shot Classification for non-English languages. This can be achieved simply by passing the model name in the pipeline. \n\nHuggingFace provides us with 2 pre-trained multilingual models:\n\n* bert-base-multilingual (uncased & cased)\n* xlm-roberta-base (also has large version)\n\nFor this example, we'll use the BERT Cased model. You can check out the details [here](https:\/\/github.com\/google-research\/bert\/blob\/master\/multilingual.md)\n","d4017648":"As you can see, even though the score for Public Health class is very low, our classifier has still managed to categorise our random sentence under \"Economics\" & \"Public Health\".\n\n## So how does this method work?\n\nThe underlying model is trained on the task of Natural Language Inference (NLI), which takes in two sequences and determines whether they contradict each other, entail each other, or neither.\n\nThis can be adapted to the task of zero-shot classification by treating the sequence which we want to classify as one NLI sequence (called the premise) and turning a candidate label into the other (the hypothesis). If the model predicts that the constructed premise _entails_ the hypothesis, then we can take that as a prediction that the label applies to the text.\n\nBy default, the pipeline turns labels into hypotheses with the template `This example is {class_name}.`. This works well in many settings, but you can also customize this for your specific setting. \n\n\nLet's add another review to our above sentiment classification example that's a bit more challenging","39ba1494":"We can see that both of the reviews have **Positive** sentiments.\n\nThe second example is a bit harder. Let's see if we can improve the results by using a hypothesis template which is more specific to the setting of review sentiment analysis. Instead of the default, This example is {}., we'll use, The sentiment of this review is {}. (where {} is replaced with the candidate class name)","85046cad":"## Libraries","002d4c8e":"### Hope this was helpful, please do consider it to UPVOTE."}}