{"cell_type":{"d4596f5e":"code","bc5790aa":"code","8e719f25":"code","bda8f53f":"code","abe3d104":"code","0c5477cf":"code","c39e13ad":"code","193e309e":"code","2809926c":"code","6df69356":"code","2d68de47":"code","fcdb55f6":"code","c02f589e":"code","335e21d7":"code","fb569820":"code","290eb0f0":"code","86be1145":"code","652c2fe8":"code","a3fcf5a4":"code","2d19a386":"code","b038d0cb":"code","39cc31fa":"code","12e30c0f":"code","0fbb194c":"code","ad409caf":"code","b67b26be":"code","93a2cdf7":"code","8964150e":"code","aeddaec4":"code","c16577ea":"code","31892580":"code","a30627d9":"code","6664d660":"markdown","509872b7":"markdown","bfce2464":"markdown","a150ecc4":"markdown","3b4f1771":"markdown","51c4af19":"markdown","84d8c0a4":"markdown","cec5de4b":"markdown","13636b10":"markdown","5dbefbb5":"markdown","55963d42":"markdown","8b2b9f41":"markdown"},"source":{"d4596f5e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","bc5790aa":"filename = '..\/input\/credit-card-customers\/BankChurners.csv'\ndata = pd.read_csv(filename)\ndata.head()","8e719f25":"data.shape","bda8f53f":"#check the column-wise distribution of null values\ndata.isnull().values.sum()","abe3d104":"#Drop Unneccessary Columns( LAST 2 COLUMNS ) + ClientNum \n\ndata = data.iloc[:,1:-2]\ndata.head()","0c5477cf":"# Check the dtype \ncategories = dict()\nfor col in data.columns:\n    if data[col].dtype == 'object':\n        categories[col] = data[col].unique()","c39e13ad":"categories.keys()","193e309e":"for col in categories.keys():\n    print(col)\n    print(data[col].value_counts(),\"\\n\")","2809926c":"# Representing the complete data Recod on Customer_id 1\n# For understand the values for specific columns and generalize important keys on real-world understanding\ndata.loc[0]","6df69356":"import seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline","2d68de47":"sns.pairplot(data,hue='Attrition_Flag')","fcdb55f6":"n = round(len(categories.keys())\/2)\ni=1\nfor col in categories.keys():    \n    plt.subplot(n, 2, i)\n    plt.tight_layout()\n    plt.title(col)\n    data[col].value_counts().plot(kind='bar',figsize=(15,10))\n    i+=1    ","c02f589e":"n = round(len(categories.keys())\/2)\ni=1\n\nplt.figure(figsize=(16, 10))\nfor col in categories.keys():    \n    if col != 'Attrition_Flag':\n        plt.subplot(n, 2, i)\n        plt.tight_layout()\n        plt.title(col)\n        sns.countplot(data[col],hue=data['Attrition_Flag'])\n        #data[col].value_counts().plot(kind='bar'column,figsize=(15,10))\n        i+=1   \nplt.subplot(n, 2, i)\ndata['Attrition_Flag'].value_counts().plot(kind=\"pie\",legend=True)","335e21d7":"#handling Categorical Data\ndf_cat = data[categories.keys()]\ndf_cat.head()","fb569820":"df_cat = data[categories.keys()]\n# Chage the Attrition_Flag entries 0 : Existing_Customer  1 : Attrited Customer\ndf_cat['Attrition_Flag'] =[0 if x =='Existing Customer' else 1 for x in data['Attrition_Flag']] #Also df_cat['Attrition_Flag'].replace({'Existing_CustomerF': 0, 'Attrited Customer':1})\n\n# Chage the Gender entries-> 0: Male 1: Female\n\ndf_cat['Gender'] = [ 0 if x =='M' else 1 for x in data['Gender']] #Also df_cat['Gender'].replace({'F':1, 'M':0})\n\ndf_cat['Education_Level'] = df_cat['Education_Level'].replace({'Unknown' : 0, \n                                    'Uneducated': 1, \n                                    'High School' : 2, \n                                    'College' : 3, \n                                    'Graduate' : 4, \n                                    'Post-Graduate': 5, \n                                    'Doctorate' : 6})\n\ndf_cat['Income_Category'] = df_cat['Income_Category'].replace({'Unknown': 0,\n                                                                 'Less than $40K' : 1,\n                                                                 '$40K - $60K'    : 2,\n                                                                 '$80K - $120K':  3,\n                                                                 '$60K - $80K': 4,\n                                                                 '$120K +': 5})\n\ndf_cat = pd.concat([df_cat,pd.get_dummies(df_cat['Card_Category'])],axis=1).drop(columns=['Card_Category','Blue'])\ndf_cat = pd.concat([df_cat,pd.get_dummies(df_cat['Marital_Status'])],axis=1).drop(columns=['Marital_Status','Unknown'])","290eb0f0":"columns = ['Attrition_Flag', 'Gender', 'Education_Level', 'Marital_Status', 'Income_Category', 'Card_Category']\ndata = data.drop(columns,axis=1)\nfinal_data = pd.concat([data,df_cat],axis=1)\nfinal_data.head()","86be1145":"\nplt.figure(figsize=(20,10))\nsns.heatmap(final_data.corr(),annot=True,fmt='.1g')","652c2fe8":"X = final_data.drop(columns=['Attrition_Flag'],axis=1)\ny = final_data['Attrition_Flag']\n\nfrom sklearn.model_selection import train_test_split\n\nx_train,x_test,y_train,y_test = train_test_split(X,y,test_size=0.2, random_state=42)","a3fcf5a4":"print(\"Training Dataset\",x_train.shape)\nprint(\"Testing Dataset\",x_test.shape)","2d19a386":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report,roc_auc_score, plot_confusion_matrix #,confusion_matrix\n\ndef train_model(x_train,y_train,x_test,y_test):\n    \n    model = RandomForestClassifier(n_estimators=10)\n    model.fit(x_train,y_train)\n    \n    predict = model.predict(x_test)\n    \n    print(classification_report(y_test,predict))\n    #table = classification_report(y_test,predict,output_dict=True)\n    #print(pd.DataFrame.from_dict(table))\n\n    print(\"\\nAUC Score\",roc_auc_score(y_test,predict))\n\n    #print(confusion_matrix(y_test,predict))\n    plot_confusion_matrix(model,x_test,y_test)\n","b038d0cb":"train_model(x_train,y_train,x_test,y_test)","39cc31fa":"from sklearn.ensemble import ExtraTreesClassifier\nmodel = ExtraTreesClassifier()\nmodel.fit(X,y)\n\n#feat_importance = pd.Series(model.feature_importances_,index=X.columns)\n#feat_importance.nlargest(5).plot(kind=\"barh\")\n","12e30c0f":"from sklearn.feature_selection import SelectKBest, f_classif\n\nselector = SelectKBest(f_classif, k=5)\nX_selected = selector.fit_transform(x_train, y_train)\n\nselected_features = pd.DataFrame(selector.inverse_transform(X_selected),columns=final_data.columns.drop('Attrition_Flag'))\nselected_columns = selected_features.columns[selected_features.var() != 0]\n# Get the valid dataset with the selected features.\nfinal_data[selected_columns].head()","0fbb194c":"# Apply the top 5 feature_columns to trained the model\ntrain_model(x_train[selected_columns],y_train,x_test[selected_columns],y_test)","ad409caf":"from sklearn.model_selection import cross_val_score\n\nscores = cross_val_score(RandomForestClassifier(), x_train, y_train, cv=5, scoring='accuracy')\nscores.mean()","b67b26be":"model_rf = RandomForestClassifier()\nn_estimators = [int(x) for x in np.linspace(100,1000,10)]\nmax_features = ['auto','sqrt']\nmax_depth = [int(x) for x in np.linspace(5, 30, 6)]\nmin_samples_split = [2, 5, 10, 15, 100]\nmin_samples_leaf = [1, 2, 5, 10]\n\nparams ={\n            'n_estimators': n_estimators,\n            'max_features': max_features,\n            'max_depth': max_depth,\n            'min_samples_split': min_samples_split,\n            'min_samples_leaf': min_samples_leaf,\n            'criterion' : ['gini','entropy']\n        }","93a2cdf7":"from sklearn.model_selection import RandomizedSearchCV\n\nmodel_rscv = RandomizedSearchCV(estimator = model_rf, param_distributions = params,scoring='f1',random_state=42, n_jobs = 1)\nmodel_rscv.fit(x_train,y_train)\n\nprint(\"Best Param: \", model_rscv.best_params_)\nprint(\"Best Score: \",model_rscv.best_score_)","8964150e":"rfc_model = RandomForestClassifier(n_estimators= 700, min_samples_split=10, min_samples_leaf= 1, max_features='auto', max_depth=30, criterion='entropy').fit(x_train,y_train)\n\nrfc_predict = rfc_model.predict(x_test)\n\nprint(\"Classification Report: \\n\",classification_report(y_test,rfc_predict))\nprint(\"AUC Score: \",roc_auc_score(y_test,rfc_predict))\nplot_confusion_matrix(rfc_model,x_test,y_test)","aeddaec4":"from xgboost import XGBClassifier\n\nscores = cross_val_score(XGBClassifier(), x_train, y_train, cv=5, scoring='accuracy')\nscores.mean()","c16577ea":"param_grid = [\n  {'colsample_bytree': [0.7, 0.8, 0.9, 1],\n   'learning_rate': [0.3, 0.4, 0.5, 0.6, 0.7, 0.8],\n   'max_depth': [4, 5, 6, 7, 8],\n   'reg_alpha':[1e-5, 1e-2, 0.1, 1, 100],\n   'reg_lambda': [0.1, 1.0, 5.0, 10.0, 50.0, 100.0]\n  }\n]","31892580":"model_xgb = RandomizedSearchCV(estimator =XGBClassifier(), param_distributions = param_grid,scoring='f1',random_state=42, n_jobs = 1)\nmodel_xgb.fit(x_train,y_train)\n\nprint(\"Best Param: \",model_xgb.best_params_)\nprint(\"Best Score: \",model_xgb.best_score_)\n\n#predict_xgb = model_xgb.predict(x_test)\n#print(classification_report(y_test,predict_rscv))\n#print(\"AUC Score: \",roc_auc_score(y_test,predict_rscv))","a30627d9":"xgb_model = XGBClassifier(reg_lambda=50.0, reg_alpha= 1e-05, max_depth= 7, learning_rate=0.5,colsample_bytree=0.8).fit(x_train, y_train)\n\n#scores = cross_val_score(xgb_model, x_train, y_train, cv=5, scoring='accuracy')\n#scores.mean()\n\nxgb_predict = xgb_model.predict(x_test)\n\nprint(\"Classification Report: \\n\",classification_report(y_test,xgb_predict))\nprint(\"AUC Score: \",roc_auc_score(y_test,xgb_predict))\nplot_confusion_matrix(xgb_model,x_test,y_test)\n","6664d660":"### Accuracy after Feature Selection Method: 0.89 {not good}","509872b7":"## Use Classifcation Model: RandomForestClassifier()","bfce2464":"### Parameter Tuning:RandomForestClassifier()","a150ecc4":"### trained model with rfc_model.best_params_","3b4f1771":"### Parameter Tuning: XGBClassifier()","51c4af19":"### Accuracy of Simple Model Training:  0.94  ","84d8c0a4":"# Accuracy: 0.97 using XGBClassifier()","cec5de4b":"## Trying Features Selection method","13636b10":"# Simply Model Training","5dbefbb5":"### trained model with xgb_model.best_params_","55963d42":"## Applying & Prepare train,test data","8b2b9f41":"## Use Classification Model: XGBClassifier()"}}