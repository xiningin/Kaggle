{"cell_type":{"0a4dcc0d":"code","fcfdca7f":"code","6e3cec41":"code","211e984e":"code","7de8d0fe":"code","315074cb":"code","9ba95853":"code","2bd70f02":"code","602cebba":"code","5807408b":"code","010fba7d":"code","5000d8f5":"code","bc99cdc9":"code","65baa287":"code","b7bd3764":"code","b8687712":"code","a53740d2":"code","ad3e947f":"markdown","bba0c086":"markdown","c396a616":"markdown","a887d75a":"markdown","b5b88506":"markdown","1343bbcc":"markdown","40cf0574":"markdown","45545c95":"markdown","034cdc4b":"markdown","83373b41":"markdown","9dbe0153":"markdown"},"source":{"0a4dcc0d":"import pandas as pd\n\ndf = pd.read_csv(\"..\/input\/train.csv\")\nprint(df.info())","fcfdca7f":"from sklearn.metrics.scorer import make_scorer\n\ndef rmlse(y, y0):\n    assert len(y) == len(y0)\n    return np.sqrt(np.mean(np.power(np.log1p(y)-np.log1p(np.clip(y0, 0, None)), 2)))\n\nrmsle_scorer = make_scorer(rmlse, greater_is_better=False)","6e3cec41":"import pandas as pd\n\ndf = pd.read_csv(\"..\/input\/train.csv\")\nprint(df.info())","211e984e":"test = pd.read_csv(\"..\/input\/test.csv\")\nprint(test.info())","7de8d0fe":"print(df.poster_path[0:10])","315074cb":"extension = [i.split(\".\")[-1] for i in df.poster_path.dropna()]\nprint(set(extension))\nlength = [len(i) for i in df.poster_path.dropna()]\nlabels = [i for i in df.revenue[df.poster_path.notnull()]]\ntemp = pd.DataFrame({\"length\": length, \"labels\": labels})\nprint(temp.groupby(length).agg([\"mean\", \"count\"]))","9ba95853":"df[\"poster_length\"] = 0\ndf.loc[df.poster_path.notnull(), \"poster_length\"] = [len(i) for i in df.poster_path[df.poster_path.notnull()]]\nprint(set(df.poster_length))","2bd70f02":"print(set(df.homepage))","602cebba":"temp = list(df.homepage[df.homepage.dropna()].index)\ntemp_df = pd.DataFrame({\n    \"contains_com\": [\".com\" in i for i in temp],\n    \"contains_uk\": [\".uk\" in i for i in temp],\n    \"contains_fr\": [\".fr\" in i for i in temp],\n    \"contains_de\": [\".de\" in i for i in temp],\n    \"contains_net\": [\".net\" in i for i in temp],\n    \"contains_kr\": [\".kr\" in i for i in temp],\n    \"contains_disney\": [\"disney\" in i for i in temp],\n    \"contains_sony\": [\"sony\" in i for i in temp],\n    \"contains_warnerbros\": [\"warnerbros\" in i for i in temp],\n    \"contains_indexhtml\": [\"index.html\" in i for i in temp],\n    \"contains_movie\": [\"movie\" in i.lower() for i in temp],\n    \"contains_wikipedia\": [\"wikipedia\" in i.lower() for i in temp],\n    \"count_slash\": [len(i.split(\"\/\")) for i in temp]\n})\nprint(np.mean(temp_df.contains_com))\nprint(np.mean(temp_df.contains_uk))\nprint(np.mean(temp_df.contains_fr))\nprint(np.mean(temp_df.contains_de))\nprint(np.mean(temp_df.contains_net))\nprint(np.mean(temp_df.contains_kr))\nprint(np.mean(temp_df.contains_disney))\nprint(np.mean(temp_df.contains_sony))\nprint(np.mean(temp_df.contains_warnerbros))\nprint(np.mean(temp_df.contains_indexhtml))\nprint(np.mean(temp_df.contains_movie))\nprint(np.mean(temp_df.contains_wikipedia))\nprint(set(temp_df.count_slash))","5807408b":"import pandas as pd\nimport numpy as np\n\ndf = pd.read_csv(\"..\/input\/train.csv\")\ntrain = pd.DataFrame(df[[\"budget\", \"popularity\", \"runtime\", \"status\", \"original_language\"]])\ntrain = pd.get_dummies(train)\ntest = pd.read_csv(\"..\/input\/test.csv\")\ndfte = pd.DataFrame(test[[\"budget\", \"popularity\", \"runtime\", \"status\", \"original_language\"]])\ndfte = pd.get_dummies(dfte)\nmissing_columns = set(dfte.columns) - set(train.columns)\nprint(missing_columns)\nfor _ in missing_columns:\n    train[_] = 0\nmissing_columns = set(train.columns) - set(dfte.columns)\nprint(missing_columns)\nfor _ in missing_columns:\n    dfte[_] = 0","010fba7d":"train.loc[1335, \"runtime\"] = 130.0\ntrain.loc[2302, \"runtime\"] = 90.0\ntrain[\"homepage_missing\"] = np.array(df.homepage.isna(), dtype=int)\ntrain[\"belongs_to_collection_missing\"] = np.array(df.belongs_to_collection.isna(), dtype=int)\ntrain[\"release_day\"] = [i.split(\"\/\")[1] for i in df.release_date]\ntrain[\"release_month\"] = [i.split(\"\/\")[0] for i in df.release_date]\ntrain[\"release_year\"] = [i.split(\"\/\")[2] for i in df.release_date]\ntrain[\"release_year\"] = [\"20\"+i if int(i) < 18 else \"19\"+i for i in train.release_year]\n\ntrain[\"poster_length\"] = 0\ntrain.loc[df.poster_path.notnull(), \"poster_length\"] = [len(i) for i in df.poster_path[df.poster_path.notnull()]]\n\nlabel = df[\"revenue\"]","5000d8f5":"train[\"contains_com\"] = 0\ntrain[\"contains_uk\"] = 0\ntrain[\"contains_fr\"] = 0\ntrain[\"contains_de\"] = 0\ntrain[\"contains_net\"] = 0\ntrain[\"contains_kr\"] = 0\ntrain[\"contains_disney\"] = 0\ntrain[\"contains_sony\"] = 0\ntrain[\"contains_warnerbros\"] = 0\ntrain[\"contains_indexhtml\"] = 0\ntrain[\"contains_movie\"] = 0\ntrain[\"contains_wikipedia\"] = 0\ntrain[\"count_slash\"] = 0\n\ntrain.loc[df.homepage.notnull(), \"contains_com\"] = [1 if ((i != \"\") & (\".com\" in i)) else 0 for i in df.homepage[df.homepage.notnull()]]\ntrain.loc[df.homepage.notnull(), \"contains_uk\"] = [1 if ((i != \"\") & (\".uk\" in i)) else 0 for i in df.homepage[df.homepage.notnull()]]\ntrain.loc[df.homepage.notnull(), \"contains_fr\"] = [1 if ((i != \"\") & (\".fr\" in i)) else 0 for i in df.homepage[df.homepage.notnull()]]\ntrain.loc[df.homepage.notnull(), \"contains_de\"] = [1 if ((i != \"\") & (\".de\" in i)) else 0 for i in df.homepage[df.homepage.notnull()]]\ntrain.loc[df.homepage.notnull(), \"contains_net\"] = [1 if ((i != \"\") & (\".net\" in i)) else 0 for i in df.homepage[df.homepage.notnull()]]\ntrain.loc[df.homepage.notnull(), \"contains_kr\"] = [1 if ((i != \"\") & (\".kr\" in i)) else 0 for i in df.homepage[df.homepage.notnull()]]\ntrain.loc[df.homepage.notnull(), \"contains_disney\"] = [1 if ((i != \"\") & (\"disney\" in i)) else 0 for i in df.homepage[df.homepage.notnull()]]\ntrain.loc[df.homepage.notnull(), \"contains_sony\"] = [1 if ((i != \"\") & (\"sony\" in i)) else 0 for i in df.homepage[df.homepage.notnull()]]\ntrain.loc[df.homepage.notnull(), \"contains_warnerbros\"] = [1 if ((i != \"\") & (\"warnerbros\" in i)) else 0 for i in df.homepage[df.homepage.notnull()]]\ntrain.loc[df.homepage.notnull(), \"contains_indexhtml\"] = [1 if ((i != \"\") & (\"index.html\" in i)) else 0 for i in df.homepage[df.homepage.notnull()]]\ntrain.loc[df.homepage.notnull(), \"contains_movie\"] = [1 if ((i != \"\") & (\"movie\" in i.lower())) else 0 for i in df.homepage[df.homepage.notnull()]]\ntrain.loc[df.homepage.notnull(), \"contains_wikipedia\"] = [1 if ((i != \"\") & (\"wikipedia\" in i)) else 0 for i in df.homepage[df.homepage.notnull()]]\ntrain.loc[df.homepage.notnull(), \"count_slash\"] = [len(i.split(\"\/\")) for i in df.homepage[df.homepage.notnull()]]","bc99cdc9":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import cross_val_score\n\nmodel  = RandomForestRegressor(n_estimators=100, random_state=2019)\nscores_randomforest = cross_val_score(model, train, label, cv=10, scoring=rmsle_scorer)\nprint(-np.mean(scores_randomforest), \"+\/-\" ,np.std(scores_randomforest))","65baa287":"model  = RandomForestRegressor(n_estimators=100)\nmodel.fit(train, label)","b7bd3764":"dfte[\"homepage_missing\"] = np.array(test.homepage.isna(), dtype=int)\ndfte[\"belongs_to_collection_missing\"] = np.array(test.belongs_to_collection.isna(), dtype=int)\ndfte.loc[243, \"runtime\"] = 93.0\ndfte.loc[1489, \"runtime\"] = 91.0\ndfte.loc[1632, \"runtime\"] = 100.0\ndfte.loc[3817, \"runtime\"] = 90.0\n\ntest.loc[828, \"release_date\"] = \"03\/30\/2001\"\ndfte[\"release_day\"] = [i.split(\"\/\")[1] for i in test.release_date]\ndfte[\"release_month\"] = [i.split(\"\/\")[0] for i in test.release_date]\ndfte[\"release_year\"] = [i.split(\"\/\")[2] for i in test.release_date]\ndfte[\"release_year\"] = [\"20\"+i if int(i) < 18 else \"19\"+i for i in dfte.release_year]\n\ndfte[\"poster_length\"] = 0\ndfte.loc[test.poster_path.notnull(), \"poster_length\"] = [len(i) for i in test.poster_path[test.poster_path.notnull()]]","b8687712":"dfte[\"contains_com\"] = 0\ndfte[\"contains_uk\"] = 0\ndfte[\"contains_fr\"] = 0\ndfte[\"contains_de\"] = 0\ndfte[\"contains_net\"] = 0\ndfte[\"contains_kr\"] = 0\ndfte[\"contains_disney\"] = 0\ndfte[\"contains_sony\"] = 0\ndfte[\"contains_warnerbros\"] = 0\ndfte[\"contains_indexhtml\"] = 0\ndfte[\"contains_movie\"] = 0\ndfte[\"contains_wikipedia\"] = 0\ndfte[\"count_slash\"] = 0\n\ndfte.loc[test.homepage.notnull(), \"contains_com\"] = [1 if ((i != \"\") & (\".com\" in i)) else 0 for i in test.homepage[test.homepage.notnull()]]\ndfte.loc[test.homepage.notnull(), \"contains_uk\"] = [1 if ((i != \"\") & (\".uk\" in i)) else 0 for i in test.homepage[test.homepage.notnull()]]\ndfte.loc[test.homepage.notnull(), \"contains_fr\"] = [1 if ((i != \"\") & (\".fr\" in i)) else 0 for i in test.homepage[test.homepage.notnull()]]\ndfte.loc[test.homepage.notnull(), \"contains_de\"] = [1 if ((i != \"\") & (\".de\" in i)) else 0 for i in test.homepage[test.homepage.notnull()]]\ndfte.loc[test.homepage.notnull(), \"contains_net\"] = [1 if ((i != \"\") & (\".net\" in i)) else 0 for i in test.homepage[test.homepage.notnull()]]\ndfte.loc[test.homepage.notnull(), \"contains_kr\"] = [1 if ((i != \"\") & (\".kr\" in i)) else 0 for i in test.homepage[test.homepage.notnull()]]\ndfte.loc[test.homepage.notnull(), \"contains_disney\"] = [1 if ((i != \"\") & (\"disney\" in i)) else 0 for i in test.homepage[test.homepage.notnull()]]\ndfte.loc[test.homepage.notnull(), \"contains_sony\"] = [1 if ((i != \"\") & (\"sony\" in i)) else 0 for i in test.homepage[test.homepage.notnull()]]\ndfte.loc[test.homepage.notnull(), \"contains_warnerbros\"] = [1 if ((i != \"\") & (\"warnerbros\" in i)) else 0 for i in test.homepage[test.homepage.notnull()]]\ndfte.loc[test.homepage.notnull(), \"contains_indexhtml\"] = [1 if ((i != \"\") & (\"index.html\" in i)) else 0 for i in test.homepage[test.homepage.notnull()]]\ndfte.loc[test.homepage.notnull(), \"contains_movie\"] = [1 if ((i != \"\") & (\"movie\" in i.lower())) else 0 for i in test.homepage[test.homepage.notnull()]]\ndfte.loc[test.homepage.notnull(), \"contains_wikipedia\"] = [1 if ((i != \"\") & (\"wikipedia\" in i)) else 0 for i in test.homepage[test.homepage.notnull()]]\ndfte.loc[test.homepage.notnull(), \"count_slash\"] = [len(i.split(\"\/\")) for i in test.homepage[test.homepage.notnull()]]","a53740d2":"predictions = model.predict(dfte)\npredictions = np.clip(predictions, 0, None)\nsubmission = pd.DataFrame({\n    \"id\" : test.id,\n    \"revenue\": predictions\n})\nsubmission.to_csv(\"submission.csv\", index=False)","ad3e947f":"The input variables are:\n\n* **id**                      \n* **belongs_to_collection**    \n* **budget**                  \n* **genres**                  \n* **homepage**                 \n* **imdb_id**                  \n* **original_language**        \n* **original_title**           \n* **overview**                 \n* **popularity**              \n* **poster_path**              \n* **production_companies**    \n* **production_countries**     \n* **release_date**             \n* **runtime**                  \n* **spoken_languages**        \n* **status**                   \n* **tagline**                  \n* **title**                   \n* **Keywords**                \n* **cast**                     \n* **crew**                    \n* **revenue**                  \n\nThe metric to be used is **RMLSE**:","bba0c086":"# Adding variables\n\n## Original language\n\nIt is not missing, but contain many distinct values. Currenlty, simple dummy encoding is the approach:","c396a616":"## Homepage\n\nThere is already a variable for missing homepages. Is it possible to get more information from the homepage?","a887d75a":"# Model\n\n## Select train and test","b5b88506":"## Features","1343bbcc":"Prepare the test set:","40cf0574":"## Train the model","45545c95":"## Poster","034cdc4b":"# Importing data","83373b41":"Train on the full train set:","9dbe0153":"## Prepare submission"}}