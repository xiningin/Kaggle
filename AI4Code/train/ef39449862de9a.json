{"cell_type":{"4c7af546":"code","19babeb3":"code","df41be72":"code","c15e218b":"code","587cf835":"code","37e1c3b5":"code","076de32a":"code","cc13cb8d":"code","381c36f1":"code","6b397c17":"code","844ee755":"code","c3634214":"code","85c1b0fe":"code","49b1b828":"code","fafa6b2c":"code","3abaf564":"code","3c99ace9":"code","af5fa37f":"code","289e8aae":"code","e25441cb":"code","ad9c124a":"code","7ef3fe62":"code","5e6afa3d":"code","549fb6dd":"code","7544f16f":"code","4de2055e":"code","a888acf0":"code","1c41423c":"code","ea5909f0":"code","578b38cc":"code","d11ac648":"code","c72d0a2c":"code","661830b3":"code","9bdba4da":"markdown","2897bf71":"markdown","af284a7d":"markdown","9d6caf18":"markdown","311d380b":"markdown","945ae156":"markdown","f24e7960":"markdown","3140d261":"markdown","976105ef":"markdown","bd69ea74":"markdown","d7d3fab1":"markdown","961c2991":"markdown","c822ae86":"markdown","e2d4119f":"markdown"},"source":{"4c7af546":"import re\nimport tqdm\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt","19babeb3":"train_df = pd.read_csv(\"..\/input\/emotions-dataset-for-nlp\/train.txt\", sep=\";\", header= None, names=[\"text\", \"label\"])\ntest_df = pd.read_csv(\"..\/input\/emotions-dataset-for-nlp\/test.txt\", sep=\";\", header= None, names=[\"text\", \"label\"])\nval_df = pd.read_csv(\"..\/input\/emotions-dataset-for-nlp\/val.txt\", sep=\";\", header= None, names=[\"text\", \"label\"])","df41be72":"train_df.head()","c15e218b":"train_df.info()\nprint()\ntest_df.info()\nprint()\nval_df.info()\nprint()","587cf835":"train_df[\"label\"].value_counts()","37e1c3b5":"test_df[\"label\"].value_counts()","076de32a":"sample = train_df[\"text\"][1]\nprint(sample)","cc13cb8d":"import nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\n\nnltk.download('punkt')\nnltk.download('stopwords')\n\nstopwords = stopwords.words('english')","381c36f1":"def lower_text(text):\n    return text.lower()\n\ndef remove_number(text):\n    num = re.compile(r'[-+]?[.\\d]*[\\d]+[:,.\\d]*')\n    return num.sub(r'', text)\n\ndef remove_punct(text):\n    punctuations = '@#!?+&*[]-%.:\/();$=><|{}^' + \"'`\" \n    for p in punctuations:\n        text = text.replace(p, f' {p} ')\n    return text\n    \ndef remove_stopwords(text):\n    text = ' '.join([word for word in text.split() if word not in (stopwords)])\n    return text","6b397c17":"def clean_text(text):\n    text = lower_text(text)\n    text = remove_number(text)\n    text = remove_punct(text)\n    text = remove_stopwords(text)\n    \n    return text","844ee755":"train_df[\"clean_text\"] = train_df[\"text\"].apply(clean_text)\ntest_df[\"clean_text\"] = test_df[\"text\"].apply(clean_text)\nval_df[\"clean_text\"] = val_df[\"text\"].apply(clean_text)","c3634214":"train_df.head()","85c1b0fe":"from sklearn.preprocessing import LabelEncoder\n\nlabel_encoder = LabelEncoder()\ntrain_df[\"encoded_train_label\"] = label_encoder.fit_transform(train_df[\"label\"])\nval_df[\"encoded_val_label\"] = label_encoder.transform(val_df[\"label\"])\ntest_df[\"encoded_test_label\"] = label_encoder.transform(test_df[\"label\"])","49b1b828":"train_df[['label', 'encoded_train_label']].drop_duplicates(keep='first')","fafa6b2c":"x_train, x_test, y_train, y_test = train_df[\"clean_text\"], val_df[\"clean_text\"], train_df[\"encoded_train_label\"], val_df[\"encoded_val_label\"]","3abaf564":"from keras.utils import np_utils\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\n\nimport tensorflow as tf\nfrom keras import layers\nfrom keras.models import Sequential\n\nEPOCHS= 30\nact= \"swish\"\nopt= tf.keras.optimizers.Adam(learning_rate=0.001)","3c99ace9":"tokenizer = Tokenizer()\ntokenizer.fit_on_texts(x_train)\n\nX_train = tokenizer.texts_to_sequences(x_train)\nX_test = tokenizer.texts_to_sequences(x_test)\nTEST = tokenizer.texts_to_sequences(test_df[\"clean_text\"]) \n\nvocab_size = len(tokenizer.word_index) + 1 ","af5fa37f":"lens_train = [len(i) for i in X_train]\nlens_test = [len(i) for i in X_test]\nlens = lens_train + lens_test\n\nmaxlen = np.max(lens)\n\nprint('Max len:', maxlen)","289e8aae":"X_train = pad_sequences(X_train, padding='post', maxlen=maxlen)\nX_test = pad_sequences(X_test, padding='post', maxlen=maxlen)\nTEST = pad_sequences(TEST, padding='post', maxlen=maxlen)","e25441cb":"dummy_y_test = np_utils.to_categorical(y_test)\ndummy_y_train = np_utils.to_categorical(y_train)","ad9c124a":"def get_embedding_vectors(tokenizer, dim=300):\n    embedding_index = {}\n    with open('..\/input\/glove840b300dtxt\/glove.840B.300d.txt','r') as f:\n        for line in tqdm.tqdm(f, \"Reading GloVe\"):\n            values = line.split()\n            word = ''.join(values[:-300])\n            vectors = np.asarray(values[-300:], dtype='float32')\n            embedding_index[word] = vectors\n\n    word_index = tokenizer.word_index\n    embedding_matrix = np.zeros((len(word_index)+1, dim))\n    for word, i in word_index.items():\n        embedding_vector = embedding_index.get(word)\n        if embedding_vector is not None:\n            # words not found will be 0s\n            embedding_matrix[i] = embedding_vector\n          \n    return embedding_matrix","7ef3fe62":"embedding_matrix = get_embedding_vectors(tokenizer)","5e6afa3d":"embedding_dim = 300\n\nmodel = Sequential()\nmodel.add(layers.Embedding(input_dim=vocab_size, output_dim=embedding_dim, weights = [embedding_matrix], input_length=maxlen))\nmodel.add(layers.Dropout(0.3)) \nmodel.add(layers.Conv1D(filters=32, kernel_size=3, activation=act)) \nmodel.add(layers.MaxPool1D(pool_size=3)) \nmodel.add(layers.Dropout(0.3))\nmodel.add(layers.Conv1D(filters=32, kernel_size=3, activation=act)) \nmodel.add(layers.MaxPool1D(pool_size=3)) \nmodel.add(layers.Dropout(0.3))\nmodel.add(layers.Bidirectional(layers.LSTM(256, recurrent_dropout=0.3)))\nmodel.add(layers.Dropout(0.3))\nmodel.add(layers.Dense(256,activation=act)) \nmodel.add(layers.Dropout(0.3)) \nmodel.add(layers.Dense(6, activation=\"softmax\"))\nmodel.compile(optimizer=opt, loss=\"categorical_crossentropy\",metrics=[\"accuracy\"])\nmodel.summary()","549fb6dd":"checkpoint_filepath = '.\/Checkpoint\/checkpoint'\nmodel_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n    filepath=checkpoint_filepath,\n    save_weights_only=True,\n    monitor='val_accuracy',\n    mode='max',\n    save_best_only=True)","7544f16f":"history = model.fit(X_train, dummy_y_train, epochs=EPOCHS, verbose=1, validation_data=(X_test, dummy_y_test), batch_size=128, callbacks=[model_checkpoint_callback]) \nloss, accuracy = model.evaluate(X_train, dummy_y_train, verbose=0)\nprint(\"Training Accuracy: {:.4f}\".format(accuracy))\nloss, accuracy = model.evaluate(X_test, dummy_y_test, verbose=0)\nprint(\"Testing Accuracy:  {:.4f}\".format(accuracy))","4de2055e":"def PlotGraph(history):\n    acc = history.history['accuracy']\n    val_acc = history.history['val_accuracy']\n    loss = history.history['loss']\n    val_loss = history.history['val_loss']\n    x = range(1, len(acc) + 1)\n\n    plt.figure(figsize=(12, 5))\n    plt.subplot(1, 2, 1)\n    plt.plot(x, acc, 'g', label='Training acc')\n    plt.plot(x, val_acc, 'b', label='Validation acc')\n    plt.title('Training and validation accuracy')\n    plt.legend()\n    plt.subplot(1, 2, 2)\n    plt.plot(x, loss, 'g', label='Training loss')\n    plt.plot(x, val_loss, 'b', label='Validation loss')\n    plt.title('Training and validation loss')\n    plt.legend()","a888acf0":"PlotGraph(history)","1c41423c":"print(f\"Best Accuracy Achieved by Model: {max(history.history['val_accuracy'])}\")","ea5909f0":"model.load_weights(checkpoint_filepath)","578b38cc":"predict = model.predict(TEST)\npredict_class = np.argmax(predict, axis=1)\npredict_class = np.array(predict_class)\npredict_class","d11ac648":"from sklearn.metrics import confusion_matrix\n\ncm = confusion_matrix(test_df[\"encoded_test_label\"], predict_class)\ncm","c72d0a2c":"df_cm = pd.DataFrame(cm, index=[\"anger\", \"fear\", \"joy\", \"love\", \"sadness\", \"surprise\"], columns=[\"anger\", \"fear\", \"joy\", \"love\", \"sadness\", \"surprise\"])\nplt.figure(figsize = (10,7))\nsns.heatmap(df_cm,annot=True, fmt =\"d\")","661830b3":"from sklearn.metrics import classification_report\n\nprint(classification_report(test_df[\"encoded_test_label\"], predict_class, target_names =[\"anger\", \"fear\", \"joy\", \"love\", \"sadness\", \"surprise\"]))","9bdba4da":"# Reading GloVe","2897bf71":"# **Reading dataset and separating them as text and label**","af284a7d":"# Defining CNN-BiLSTM model","9d6caf18":"# Calculating maximum length of a vector","311d380b":"# Converting texts to vectors via tokenization","945ae156":"# A sample text from the dataset","f24e7960":"# Performance Matrices","3140d261":"# One Hot Encoding integer labels","976105ef":"# Label Encoding Labels\nThis means replcing String objects as labels with integer values","bd69ea74":"# Distriution of labels in Train and Test data","d7d3fab1":"# Checking for null values in data","961c2991":"# Raw text vs Preprocessed Text","c822ae86":"# Text Preprocessing\nThe preprocessing is done in a few steps:\n* Lowercaseing the text\n* Removing Numbers\n* Removing Punctuations\n* Removing Stopwords","e2d4119f":"# Zero padding every vector so that they are the same size"}}