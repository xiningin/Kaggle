{"cell_type":{"926e44c2":"code","cfcbbc05":"code","32f56604":"code","ad482318":"code","8af5725c":"code","456c61da":"code","0f669582":"code","b10d5cc9":"code","b353e2c9":"code","f3077a94":"code","368d42e6":"markdown","21f72118":"markdown","8b10b697":"markdown","0e5ea0c7":"markdown","eb22580e":"markdown","c44c7aed":"markdown","1a1e2666":"markdown","bec0c749":"markdown","53664374":"markdown","7946105d":"markdown","62133315":"markdown"},"source":{"926e44c2":"#packages\n%matplotlib inline\nimport numpy as np \nimport pandas as pd \nfrom sklearn.model_selection import train_test_split \nimport matplotlib.pyplot as plt \n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","cfcbbc05":"# Loading & Splitting the data\ndf = pd.read_csv('\/kaggle\/input\/programming-assignment-linear-regression\/ex1data1.csv')\nX = df.drop(['food_truck_profit'], axis=1).values\nY = df['food_truck_profit'].values\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, \n                                   test_size = 0.15,\n                                   random_state = 5 ) ","32f56604":"# Model Performance\nclass metrics():\n    def __init__(self, Y_pred, Y_test):\n        self.Y_pred = Y_pred\n        self.Y_test = Y_test\n        self.m = len(Y_test)\n    \n    def MAE(self):\n        self.mae = np.mean(abs(self.Y_test - self.Y_pred))\n        return self.mae\n        \n    def RMSE(self):\n        self.errors = 0\n        for i in range(self.m):\n            self.errors += (self.Y_test[i] - self.Y_pred[i]) ** 2\n        self.rmse = round(np.sqrt(self.errors\/self.m), 2)\n        return self.rmse\n    \n    def R_sq(self):\n        self.y_mean = np.mean(self.Y_test)\n        self.sum_sq = np.sum((self.Y_test - self.y_mean) ** 2)\n        self.sum_res = np.sum((self.Y_test - self.Y_pred) ** 2)\n        self.r_sq = round(1 - (self.sum_res\/self.sum_sq), 4)\n        return self.r_sq","ad482318":"class LinearRegression_OLS():\n    def __init__(self, X, Y):\n        self.X, self.Y = X, Y\n        self.b0, self.b1 = 0, 0 \n        \n    def fit(self):\n        self.m = len(X)\n        self.x_mean = np.mean(X)\n        self.y_mean = np.mean(Y)\n        self.rect_areas, self.sq_areas = 0, 0\n        for i in range(self.m):\n            self.rect_areas += (X[i] - self.x_mean) * (Y[i] - self.y_mean)\n            self.sq_areas += (X[i] - self.x_mean) ** 2\n        self.b1 =  self.rect_areas \/ self.sq_areas\n        self.b0 =  self.y_mean - (self.b1 * self.x_mean)\n        return self\n    \n    def params(self):\n        return np.concatenate([self.b0, self.b1]).flatten()\n    \n    def predict(self, X):\n        return self.b0 + X.dot(self.b1)","8af5725c":"OLS_model = LinearRegression_OLS(X_train, Y_train)\nOLS_model.fit()\nY_pred_OLS = OLS_model.predict(X_test)\nparams_OLS = OLS_model.params()\nOLS_perf = metrics(Y_pred_OLS, Y_test)\nmodels['OLS'] = [OLS_perf.MAE(), OLS_perf.RMSE(), OLS_perf.R_sq()]","456c61da":"class LinearRegression_GD():\n    def __init__(self, alpha, iters):\n        self.alpha = alpha               \n        self.iters = iters  \n\n    def fit(self, X, Y):\n        self.m, self.n = X.shape         \n        \n        self.cost_history = []                          \n        self.W = np.zeros(self.n)   \n        self.X = X\n        self.Y = Y\n        \n        for i in range(self.iters):                \n            self.gradient_descent()\n        return self\n    \n    def gradient_descent(self):\n        Y_pred = self.predict(self.X)\n        error = Y_pred-self.Y\n        \n        cost = 1\/(2*self.m) * error.T.dot(error)\n        grad = (1\/self.m) * self.X.T.dot(error)\n        \n        self.W = self.W - self.alpha * grad\n\n        self.cost_history.append(cost)\n        return self\n    \n    def params(self):\n        return self.W.flatten()\n    \n    def predict(self, X):\n        return X.dot(self.W)\n    \n    def visualize(self):\n        plt.title('Cost Function J')\n        plt.xlabel('No. of iterations')\n        plt.ylabel('Cost')\n        plt.plot(self.cost_history)\n        plt.show()","0f669582":"# Data Splitting & Preprocessing\nX_train_gd = np.c_[np.ones(X_train.shape[0]), X_train]\nX_test_gd = np.c_[np.ones(X_test.shape[0]), X_test]\n\n# Modeling\nGD_model = LinearRegression_GD(alpha=0.01, iters=10000)\nGD_model.fit(X_train_gd, Y_train)\nGD_model.visualize()\n\n# Predicting\nY_pred_GD = GD_model.predict(X_test_gd)\nparams_GD = GD_model.params()\nGD_perf = metrics(Y_pred_GD, Y_test)\nmodels['GD'] = [GD_perf.MAE(), GD_perf.RMSE(), GD_perf.R_sq()]","b10d5cc9":"from sklearn.linear_model import LinearRegression\n\n#modeling & fitting \nreg = LinearRegression(normalize=True)\nreg = reg.fit(X_train, Y_train)\n\n# Prediction\nY_pred_sk = reg.predict(X_test)\n# Model Evaluation\nsk_perf = metrics(Y_pred_sk, Y_test)\nmodels['sklearn'] = [sk_perf.MAE(), sk_perf.RMSE(), sk_perf.R_sq()]","b353e2c9":"### Method 4: Using the statsmodel module\nimport statsmodels.api as sm\n\nX_train = sm.add_constant(X_train)\nmodel_sm = sm.OLS(Y_train, X_train).fit()        #model training\nc = round(model_sm.params[0], 2)\nb = round(model_sm.params[1], 2)\nY_pred_sm = (c + X_test.dot(b)).flatten()\nsm_perf = metrics(Y_pred_sm, Y_test)\nmodels['statsmodels'] = [sm_perf.MAE(), sm_perf.RMSE(), sm_perf.R_sq()]","f3077a94":"models_df = pd.DataFrame.from_dict(models, orient='index',\n                      columns=['MAE', 'RSME', 'R_sq'])\nmodels_df","368d42e6":"# Method 1: Ordinary Least Square","21f72118":"# Method 2: Gradient Descent","8b10b697":"### Explanation \nHere, we are trying to optimizing the performance of our model by minimizing sum of the distance squared of between the line (predicted) and data point (actual). \n\n$\\begin{align}\n    \\hat\\beta_1 &= \\frac{ \\sum{x_iy_i} - \\frac{1}{n}\\sum{x_i}\\sum{y_i} }\n                     { \\sum{x_i^2} - \\frac{1}{n}(\\sum{x_i})^2 } =  \\frac{ \\operatorname{Cov}[x,y] }{ \\operatorname{Var}[x]}  \\\\\n   \\hat\\beta_0 &= \\overline{y} - \\hat\\beta_1\\,\\overline{x}\\ ,\n  \\end{align}$\n![image.png](attachment:image.png)\n","0e5ea0c7":"# 3. Model Building & Performance","eb22580e":"# 1. Data Importing","c44c7aed":"# Method 3: Using the sklearn library","1a1e2666":"### Explanation\nGradient Descent is an optimization algortithm that uses an iterative process to find the global minimum of the loss function (J) modeling the error between our actual and predicted values.\n$J(m,b) =  \\frac{1}{N} \\sum_{i=1}^{n} (y_i - (mx_i + b))^2$ where our parameters m =weight and b= bias.\n\nStep 1: Initialization of values m and b\n\nStep 2: Iteratively Update until converges\n$\\begin{split}f'(m,b) =\n   \\begin{bmatrix}\n     \\frac{df}{dm}\\\\\n     \\frac{df}{db}\\\\\n    \\end{bmatrix}\n=\n   \\begin{bmatrix}\n     \\frac{1}{N} \\sum -2x_i(y_i - (mx_i + b)) \\\\\n     \\frac{1}{N} \\sum -2(y_i - (mx_i + b)) \\\\\n    \\end{bmatrix}\\end{split}$\nNote that we are trying to find the partial derivate of cost using our new m and b and the size of any updates is controleed by the learning rate","bec0c749":"### Implementation 1","53664374":"# 2. Exploratory Data Analysis","7946105d":"# Method 4: Using the statsmodels library","62133315":"### Implementation"}}