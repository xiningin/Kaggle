{"cell_type":{"c1a39470":"code","cb4d1e7e":"code","27b112eb":"code","c159e844":"code","dd8d9043":"code","ec9c429f":"code","72408ac5":"code","598a47b8":"code","63489f24":"code","447c23c0":"code","b3fc3b5f":"code","fb0bb322":"code","b9355cfc":"code","269fdf67":"code","72438a39":"code","4becf847":"code","99cef46a":"code","73a857a1":"code","ba38f334":"code","4d969f95":"code","9475656f":"code","2904e66c":"code","c975aa68":"code","0937b633":"code","68f1f3c1":"code","ad3c27bb":"code","b440d729":"code","03da5633":"code","65df74df":"code","bce1da85":"code","a64c9af2":"code","28fa9ff8":"code","1f4c2f66":"code","36a2130c":"markdown","412fe13e":"markdown","5ccd0d89":"markdown","18371923":"markdown","278b4fab":"markdown","52863afe":"markdown","461fd013":"markdown","ffb602f1":"markdown","c0d3c3f6":"markdown","941c3f7e":"markdown","7db70927":"markdown","b2b07590":"markdown","092a6dc2":"markdown","f6c98dee":"markdown","9f1595eb":"markdown","022247cb":"markdown","57aa3295":"markdown","4ac72aa1":"markdown","d65f7b1c":"markdown","089c3625":"markdown","f7de5535":"markdown"},"source":{"c1a39470":"! pip install gensim","cb4d1e7e":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nimport gensim\nfrom gensim.models import Word2Vec\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\npd.options.display.max_rows = 15\n%matplotlib inline\nsns.set(style=\"whitegrid\", palette=\"colorblind\", font_scale=1, rc={'font.family':'NanumGothic'} )\n\ndef toReadable(v):\n    value = round(v,2) if isinstance(v, float) else v\n\n    if value < 1000:\n        return str(value)\n    elif value<1000000:\n        return str(round(value\/1000,1))+'K'\n    elif value>=1000000:\n        return str(round(value\/1000000,1))+'M'\n    return value","27b112eb":"IDIR = '..\/input\/'","c159e844":"raw_order_ds = pd.read_csv(IDIR + 'orders.csv', dtype={\n        'order_id': np.int32,\n        'user_id': np.int32,\n        'eval_set': 'category',\n        'order_number': np.int16,\n        'order_dow': np.int8,\n        'order_hour_of_day': np.int8,\n        'days_since_prior_order': np.float32})\nprint(\"raw_order_ds shape\",raw_order_ds.shape)\n\norder_product_ds = priors = pd.read_csv(IDIR + 'order_products__prior.csv', dtype={\n            'order_id': np.int32,\n            'product_id': np.uint16,\n            'add_to_cart_order': np.int16,\n            'reordered': np.int8})\nprint(\"order_product_ds shape\",order_product_ds.shape)\n\nproduct_ds = pd.read_csv(IDIR + 'products.csv')\nprint(\"orders shape\",product_ds.shape)\n\norder_product_cnt_ds = order_product_ds.groupby('order_id').count()[['product_id']]\norder_product_cnt_ds.columns = ['product_cnt']\n\n## join product count \norder_ds = raw_order_ds.merge(order_product_cnt_ds, left_on='order_id', right_index=True)","dd8d9043":"total_user = len(order_ds.user_id.unique())\ntotal_order = len(order_ds)\ntotal_ordered_product = len(order_product_ds)\nunique_products = len(order_product_ds.product_id.unique())\n\nprint(\"total user = {}\".format(toReadable(total_user)))\nprint(\"total order = {} ({} orders per a user )\".format(toReadable(total_order), toReadable(total_order\/total_user) ))\nprint(\"total product = \", toReadable(unique_products))\nprint(\"total ordered product  = {} ({} orders per a product )\".format(\n    toReadable(total_ordered_product), toReadable(total_ordered_product\/unique_products) ))","ec9c429f":"merge_order_product_ds = order_product_ds.merge(order_ds, on='order_id' )","72408ac5":"order_product_list = merge_order_product_ds\\\n    .sort_values(['user_id','order_id','add_to_cart_order'])[['order_id','product_id']]\\\n    .values.tolist()\n\nproduct_corpus = []\nsentence = []\nnew_order_id = order_product_list[0][0]\nfor (order_id, product_id) in order_product_list:\n    if new_order_id != order_id:\n        product_corpus.append(sentence)\n        sentence = []\n        new_order_id = order_id\n    sentence.append(str(product_id))","598a47b8":"model = Word2Vec(product_corpus, window=9, size=100, workers=4, min_count=50)\n# model.save('.\/resource\/prod2vec.100d.model')\n# model = Word2Vec.load('.\/resource\/prod2vec.100d.model')","63489f24":"def toProductName(id):\n    return product_ds[product_ds.product_id==id]['product_name'].values.tolist()[0]\ntoProductName(24852)","447c23c0":"def most_similar_readable(model, product_id):\n    similar_list = [(product_id,1.0)]+model.wv.most_similar(str(product_id))\n    \n    return [( toProductName(int(id)), similarity ) for (id,similarity) in similar_list]","b3fc3b5f":"pd.DataFrame(most_similar_readable(model, 24852), columns=['product','similarity'])","fb0bb322":"pd.DataFrame(most_similar_readable(model, 27845), columns=['product','similarity'])","b9355cfc":"pd.DataFrame(most_similar_readable(model, 40939), columns=['product','similarity'])","269fdf67":"pd.DataFrame(most_similar_readable(model, 48697), columns=['product','similarity'])","72438a39":"from __future__ import division\nimport random\nimport numpy as np\nfrom scipy.spatial.distance import cdist  # $scipy\/spatial\/distance.py\n    # http:\/\/docs.scipy.org\/doc\/scipy\/reference\/spatial.html\nfrom scipy.sparse import issparse  # $scipy\/sparse\/csr.py\n\n__date__ = \"2018-09-01\"\n    # X sparse, any cdist metric: real app ?\n    # centres get dense rapidly, metrics in high dim hit distance whiteout\n    # vs unsupervised \/ semi-supervised svm\n#...............................................................................\ndef kmeans( X, centres, delta=.001, maxiter=10, metric=\"euclidean\", p=2, verbose=1 ):\n    \"\"\" centres, Xtocentre, distances = kmeans( X, initial centres ... )\n    in:\n        X N x dim  may be sparse\n        centres k x dim: initial centres, e.g. random.sample( X, k )\n        delta: relative error, iterate until the average distance to centres\n            is within delta of the previous average distance\n        maxiter\n        metric: any of the 20-odd in scipy.spatial.distance\n            \"chebyshev\" = max, \"cityblock\" = L1, \"minkowski\" with p=\n            or a function( Xvec, centrevec ), e.g. Lqmetric below\n        p: for minkowski metric -- local mod cdist for 0 < p < 1 too\n        verbose: 0 silent, 2 prints running distances\n    out:\n        centres, k x dim\n        Xtocentre: each X -> its nearest centre, ints N -> k\n        distances, N\n    see also: kmeanssample below, class Kmeans below.\n    \"\"\"\n    if not issparse(X):\n        X = np.asanyarray(X)  # ?\n    centres = centres.todense() if issparse(centres) \\\n        else centres.copy()\n    N, dim = X.shape\n    k, cdim = centres.shape\n    if dim != cdim:\n        raise ValueError( \"kmeans: X %s and centres %s must have the same number of columns\" % (\n            X.shape, centres.shape ))\n    if verbose:\n        print (\"kmeans: X %s  centres %s  delta=%.2g  maxiter=%d  metric=%s\" % (\n            X.shape, centres.shape, delta, maxiter, metric) )\n    allx = np.arange(N)\n    prevdist = 0\n    for jiter in range( 1, maxiter+1 ):\n        D = cdist_sparse( X, centres, metric=metric, p=p )  # |X| x |centres|\n        xtoc = D.argmin(axis=1)  # X -> nearest centre\n        distances = D[allx,xtoc]\n        avdist = distances.mean()  # median ?\n        if verbose >= 2:\n            print(\"kmeans: av |X - nearest centre| = %.4g\" % avdist)\n        if (1 - delta) * prevdist <= avdist <= prevdist \\\n        or jiter == maxiter:\n            break\n        prevdist = avdist\n        for jc in range(k):  # (1 pass in C)\n            c = np.where( xtoc == jc )[0]\n            if len(c) > 0:\n                centres[jc] = X[c].mean( axis=0 )\n    if verbose:\n        print (\"kmeans: %d iterations  cluster sizes:\" % jiter, np.bincount(xtoc))\n    if verbose >= 2:\n        r50 = np.zeros(k)\n        r90 = np.zeros(k)\n        for j in range(k):\n            dist = distances[ xtoc == j ]\n            if len(dist) > 0:\n                r50[j], r90[j] = np.percentile( dist, (50, 90) )\n        print (\"kmeans: cluster 50 % radius\", r50.astype(int))\n        print (\"kmeans: cluster 90 % radius\", r90.astype(int))\n            # scale L1 \/ dim, L2 \/ sqrt(dim) ?\n    return centres, xtoc, distances\n#...............................................................................\ndef kmeanssample( X, k, nsample=0, **kwargs ):\n    \"\"\" 2-pass kmeans, fast for large N:\n        1) kmeans a random sample of nsample ~ sqrt(N) from X\n        2) full kmeans, starting from those centres\n    \"\"\"\n        # merge w kmeans ? mttiw\n        # v large N: sample N^1\/2, N^1\/2 of that\n        # seed like sklearn ?\n    N, dim = X.shape\n    if nsample == 0:\n        nsample = max( 2*np.sqrt(N), 10*k )\n    Xsample = randomsample( X, int(nsample) )\n    pass1centres = randomsample( X, int(k) )\n    samplecentres = kmeans( Xsample, pass1centres, **kwargs )[0]\n    return kmeans( X, samplecentres, **kwargs )\n\ndef cdist_sparse( X, Y, **kwargs ):\n    \"\"\" -> |X| x |Y| cdist array, any cdist metric\n        X or Y may be sparse -- best csr\n    \"\"\"\n        # todense row at a time, v slow if both v sparse\n    sxy = 2*issparse(X) + issparse(Y)\n    if sxy == 0:\n        return cdist( X, Y, **kwargs )\n    d = np.empty( (X.shape[0], Y.shape[0]), np.float64 )\n    if sxy == 2:\n        for j, x in enumerate(X):\n            d[j] = cdist( x.todense(), Y, **kwargs ) [0]\n    elif sxy == 1:\n        for k, y in enumerate(Y):\n            d[:,k] = cdist( X, y.todense(), **kwargs ) [0]\n    else:\n        for j, x in enumerate(X):\n            for k, y in enumerate(Y):\n                d[j,k] = cdist( x.todense(), y.todense(), **kwargs ) [0]\n    return d\n\ndef randomsample( X, n ):\n    \"\"\" random.sample of the rows of X\n        X may be sparse -- best csr\n    \"\"\"\n    random.seed(100)    \n    sampleix = random.sample( range( X.shape[0] ), int(n) )\n    return X[sampleix]\n\ndef nearestcentres( X, centres, metric=\"euclidean\", p=2 ):\n    \"\"\" each X -> nearest centre, any metric\n            euclidean2 (~ withinss) is more sensitive to outliers,\n            cityblock (manhattan, L1) less sensitive\n    \"\"\"\n    D = cdist( X, centres, metric=metric, p=p )  # |X| x |centres|\n    return D.argmin(axis=1)\n\ndef Lqmetric( x, y=None, q=.5 ):\n    # yes a metric, may increase weight of near matches; see ...\n    return (np.abs(x - y) ** q) .mean() if y is not None \\\n        else (np.abs(x) ** q) .mean()\n\n#...............................................................................\nclass Kmeans:\n    \"\"\" km = Kmeans( X, k= or centres=, ... )\n        in: either initial centres= for kmeans\n            or k= [nsample=] for kmeanssample\n        out: km.centres, km.Xtocentre, km.distances\n        iterator:\n            for jcentre, J in km:\n                clustercentre = centres[jcentre]\n                J indexes e.g. X[J], classes[J]\n    \"\"\"\n    def __init__( self, X, k=0, centres=None, nsample=0, **kwargs ):\n        self.X = X\n        if centres is None:\n            self.centres, self.Xtocentre, self.distances = kmeanssample(\n                X, k=k, nsample=nsample, **kwargs )\n        else:\n            self.centres, self.Xtocentre, self.distances = kmeans(\n                X, centres, **kwargs )\n\n    def __iter__(self):\n        for jc in range(len(self.centres)):\n            yield jc, (self.Xtocentre == jc)\n","4becf847":"def clustering(model, k=500, delta=0.00000001, maxiter=200):\n    movie_vec = model.wv.syn0\n    centres, index2cid, dist = kmeanssample(movie_vec, k, \n                                                   metric = 'cosine', \n                                                   delta = delta, \n                                                   nsample = 0, maxiter = maxiter,)\n    clustered_ds = pd.DataFrame( [ (a, b, c) for a, b, c in zip(model.wv.index2word, index2cid, dist )],\n                 columns=['product_id', 'cid', 'dist'] ).sort_values(['cid','dist'], ascending=True)\n\n    prod2cid = { product_id:cid for product_id,cid in zip(model.wv.index2word, index2cid) }\n\n    return (centres, index2cid, dist, clustered_ds, prod2cid)","99cef46a":"(centres, index2cid, dist, clustered_ds, prod2cid) = clustering(model)","73a857a1":"clustered_ds.product_id = clustered_ds.product_id.apply(pd.to_numeric)","ba38f334":"def idToProductDesc(id):\n    return product_ds[product_ds.product_id==id][['product_name','aisle_id']].values.tolist()[0]\n    \ndef getProductNames(product_id_list):\n    return [ idToProductDesc(int(product_id)) for  product_id in product_id_list ]\n\nimport urllib\ndef printClusterMembers(cluster_id, topn=10):\n    members = getProductNames(clustered_ds[clustered_ds.cid==cluster_id].product_id[:topn].tolist())\n    for member in members:\n        print(\"{aisle} \/ {name} \".format( \n            aisle=member[1], name=member[0], q=urllib.parse.quote_plus(member[0]) ) \n        )","4d969f95":"printClusterMembers(1, topn=10)","9475656f":"printClusterMembers(100, topn=10)","2904e66c":"printClusterMembers(200, topn=10)","c975aa68":"printClusterMembers(300, topn=10)","0937b633":"printClusterMembers(400, topn=10)","68f1f3c1":"printClusterMembers(499, topn=10)","ad3c27bb":"# product_reorder_ds.groupby('aisle_id').agg({'product_name':                                           lambda x: })\nfrom collections import defaultdict\nimport operator\n\ndef popularWords(names, topn=2):\n    wordFrequency = defaultdict(int)\n    def updateWords(words):\n        for word in words :\n            if len(word)>1:\n                wordFrequency[word] += 1\n    names.apply(lambda x: updateWords(x.split()))\n    tops = sorted(wordFrequency.items(), key=operator.itemgetter(1),reverse=True)[:topn]\n    return \" \".join([n[0] for n in tops])","b440d729":"clusterIdToKeywords = { cid: popularWords(sub_ds.product_name,3) for cid, sub_ds in clustered_ds.merge(product_ds, on='product_id').groupby('cid')}","03da5633":"product_hod_ds = merge_order_product_ds.pivot_table(index='product_id', columns='order_hour_of_day', values='order_id', aggfunc=len, fill_value=0)\n\norderByHotHour = clustered_ds.merge(product_hod_ds, left_on='product_id', right_index=True)\\\n    .groupby('cid').sum()[np.arange(0,24)].idxmax(axis=1).sort_values().index","65df74df":"sns.set(style=\"whitegrid\", palette=\"colorblind\", font_scale=1, rc={'font.family':'NanumGothic'} )\n\ndef drawHODCluster(ncols, nrows, startClusterNumber, step):\n    fig, axes = plt.subplots(ncols=ncols, nrows = nrows, figsize=(ncols*2.5,nrows*2), sharex=True, sharey=True)\n\n    for cid, ax  in enumerate(axes.flatten()):\n        cid = startClusterNumber + (cid*step)\n        if cid>=500:\n            break\n        cid = orderByHotHour[cid]\n\n        product_id_list = clustered_ds[clustered_ds.cid==cid].product_id.values\n        tmp_ds = product_hod_ds.loc[product_id_list].T\n        hot_hour = tmp_ds.sum(axis=1).argmax()\n        normalized_ds =(tmp_ds\/tmp_ds.max())\n        title = \"{cid}th {n} products \\n({keyword})\".format(cid=cid, n=normalized_ds.shape[1],  keyword=clusterIdToKeywords[cid][:23])\n        normalized_ds.plot(linewidth=.3, legend=False, alpha=.4, ax=ax, title=title, color='r' if hot_hour<13 else 'k')\n        ax.plot((hot_hour,hot_hour),(1,0), '-.', linewidth=1, color='b')\n        ax.text(hot_hour,0,\"{h}h(hot)\".format(h=hot_hour),color='b')\n\n    fig.tight_layout()","bce1da85":"ncols, nrows=(6,4)\nstep = 3\nfor n in np.arange(0,500,ncols*nrows*step):\n    drawHODCluster(ncols, nrows, n, step)","a64c9af2":"product_dow_ds = merge_order_product_ds.pivot_table(index='product_id', columns='order_dow', values='order_id', aggfunc=len, fill_value=0)\n\norderByHotDay = clustered_ds.merge(product_dow_ds, left_on='product_id', right_index=True)\\\n    .groupby('cid').sum()[np.arange(0,6)].idxmax(axis=1).sort_values().index","28fa9ff8":"def drawDOWCluster(ncols, nrows, startClusterNumber, step):\n    sns.set(style=\"whitegrid\", palette=\"colorblind\", font_scale=1, rc={'font.family':'NanumGothic'} )\n    week_day = \"Sun Mon Tue Wed Thu Fri Sat\".split()\n    fig, axes = plt.subplots(ncols=ncols, nrows = nrows, figsize=(ncols*2.5,nrows*2), sharex=True, sharey=True)\n\n    for cid, ax  in enumerate(axes.flatten()):\n        cid = startClusterNumber + (cid*step)\n        if cid>=500:\n            break\n        cid = orderByHotDay[cid]    \n        product_id_list = clustered_ds[clustered_ds.cid==cid].product_id.values\n        tmp_ds = product_dow_ds.loc[product_id_list].T\n        hot_day = tmp_ds.sum(axis=1).argmax()\n        normalized_ds =(tmp_ds\/tmp_ds.max())\n        normalized_ds.index = week_day\n        title = \"{cid}th \\n({keyword})\".format(cid=cid, h=hot_day,  keyword=clusterIdToKeywords[cid][:23])\n        normalized_ds.plot(kind='bar', linewidth=.1, legend=False, alpha=.4, ax=ax, title=title, color='r' if hot_day in(0,6) else 'k')\n        ax.plot((hot_day,hot_day),(1,0), '-.', linewidth=2, color='b')\n        # ax.text(hot_day+.3,-.5,\"{h}\".format(h=week_day[hot_day]),color='b')\n    \n    fig.tight_layout()","1f4c2f66":"ncols, nrows=(6,4)\nstep = 3\nfor n in np.arange(0,500,ncols*nrows*step):\n    drawDOWCluster(ncols, nrows, n, step)","36a2130c":"* Extract representative keywords from each cluster.\n* Reprosentative keywords : 3 words and max 15 latters\n* Sort by popular order hour ","412fe13e":"#### Hour of Day Trend Per cluster ","5ccd0d89":"### What is the most similar?\n* most similar to banana(24852) is ..","18371923":"* Cluster ID = 0 th","278b4fab":"---\n## Clustering similar product by user's order informaiton\n* Traing product2vec using word2vec \n * word = product_id\n * scentence = user's order = [product_id1, product_id2, ... ]\n* clustering by trained product vector\n* Use only products ordered more than 100 times\n\n* Do not Filter out orders with only 1 item in then in this case ? ","52863afe":"* Cluster ID = 200 th","461fd013":"#### Hour of Day Trend Per cluster ","ffb602f1":"* Cluster ID = 100 th","c0d3c3f6":"* Cluster ID = 499 th","941c3f7e":"----\n### Order time trend of clustered product.","7db70927":"### Dataset Summery\nLet's look at the simple stats of a dataset","b2b07590":"* Cluster ID = 300 th","092a6dc2":"### Product2Vec works well !!! ","f6c98dee":"* most similar to Organic Whole Milk(40939) is .. ","9f1595eb":"* Create 500 clusters as similar products\n* using kmeans ( k=500 )","022247cb":"### It looks goooood!! ","57aa3295":"## Instacart dataset exploratory\n* Forked from : https:\/\/www.kaggle.com\/goodvc\/instacart-product2vec-clustering-using-word2vec \n     * Fixed paths + minor changes\n* Instacart kaggle : https:\/\/www.kaggle.com\/c\/instacart-market-basket-analysis#prizes\n* blog post : https:\/\/tech.instacart.com\/3-million-instacart-orders-open-sourced-d40d29ead6f2\n* data dictionary : https:\/\/gist.github.com\/jeremystan\/c3b39d947d9b88b3ccff3147dbcf6c6b\n* dataset file list \n\n\n","4ac72aa1":"* Cluster ID = 400 th","d65f7b1c":"* most similar to Drinking Water(27845) is ..","089c3625":"### Clustered Result\n### Let's look at clustered product ","f7de5535":"---\n## Load Dataset"}}