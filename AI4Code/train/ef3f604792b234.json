{"cell_type":{"b9069100":"code","9abd06c6":"code","13e22486":"code","6de023ab":"code","141acb68":"code","bf065526":"code","511a587a":"code","8e676e14":"code","346e40e2":"code","c3efb2bc":"code","cfca2f56":"markdown","ec00d308":"markdown","21e3d527":"markdown","7029f047":"markdown","a372a87d":"markdown","ce5538db":"markdown","5d44b052":"markdown"},"source":{"b9069100":"#import libraries\nimport numpy as np                                                      #for fast operations on arrays    \nimport pandas as pd                                                     #for read & manipulate dataset\nimport matplotlib.pyplot as plt                                         #for Data visualization\nimport seaborn as sns                                                   #for Data visualization\nfrom sklearn.cluster import KMeans                                      #clustering algorithm\nfrom PIL import Image                                                   #display image\nimport warnings\nwarnings.filterwarnings('ignore')\n%matplotlib inline","9abd06c6":"#load dataset\ndf = pd.read_csv('..\/input\/irisdataset\/Iris.csv')","13e22486":"df.head()","6de023ab":"#Shape of Dataset\nprint(\"Number of rows : {}\".format(df.shape[0]))","141acb68":"#Check Missing Values\ntotal = df.isnull().sum().sort_values(ascending=False)\npercent = (df.isnull().sum()\/df.isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_data.head()","bf065526":"df.describe()","511a587a":"sns.pairplot(df,size=3)\nplt.show()","8e676e14":"# Finding the optimum number of clusters for k-means classification\n\nx = df.iloc[:, [0, 1, 2, 3]].values\n\nwcss = []\n\nfor i in range(1, 11):\n    kmeans = KMeans(n_clusters = i, init = 'k-means++', \n                    max_iter = 300, n_init = 10, random_state = 0)\n    kmeans.fit(x)\n    wcss.append(kmeans.inertia_)\n    \n# Plotting the results onto a line graph, \n# `allowing us to observe 'The elbow'\nplt.plot(range(1, 11), wcss)\nplt.title('The elbow method')\nplt.xlabel('Number of clusters')\nplt.ylabel('WCSS') # Within cluster sum of squares\nplt.show()","346e40e2":"# Applying kmeans to the dataset \/ Creating the kmeans classifier\nkmeans = KMeans(n_clusters = 3, init = 'k-means++',\n                max_iter = 300, n_init = 10, random_state = 0)\ny_kmeans = kmeans.fit_predict(x)","c3efb2bc":"# Visualising the clusters - On the first two columns\nplt.figure(figsize=(15, 8), dpi=80)\nplt.grid()\nplt.scatter(x[y_kmeans == 0, 0], x[y_kmeans == 0, 1], \n            s = 100, c = 'red', label = 'Cluster1')\nplt.scatter(x[y_kmeans == 1, 0], x[y_kmeans == 1, 1], \n            s = 100, c = 'blue', label = 'Cluster2')\nplt.scatter(x[y_kmeans == 2, 0], x[y_kmeans == 2, 1],\n            s = 100, c = 'green', label = 'Cluster3')\n\n# Plotting the centroids of the clusters\nplt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:,1], \n            s = 100, c = 'yellow', label = 'Centroids')\n\nplt.legend()","cfca2f56":"## The elbow method\n\n#### How do you find the optimum number of clusters ?\n\n**We will use Kmean clustering algorithm but to do this we need to suppose the initial K value**\n\n**How does one determine the value of K?**\n\nwe will use The elbow method","ec00d308":"## Kmean Clustering","21e3d527":"## Observations\n\n**petal_length and petal_width are the most useful features to identify various flower types which we can say there are 3 clusters but it is not accurate way**","7029f047":"You can clearly see why it is called 'The elbow method' from the above graph, the optimum clusters is where the elbow occurs. This is when the within cluster sum of squares (WCSS) doesn't decrease significantly with every iteration.\n\nFrom this we choose the number of clusters as **3**.","a372a87d":"## Exploratory Data Analysis","ce5538db":"### Optimum Number of Clusters\n\n**From the given iris dataset predict the Optimum number of clusters and represent it visually**\n","5d44b052":"We will define the following:-\n\nDistortion: It is calculated as the average of the squared distances from the cluster centers of the respective clusters. Typically, the Euclidean distance metric is used.\n\nInertia: It is the sum of squared distances of samples to their closest cluster center.\n\nWe iterate the values of k from 1 to 10 and calculate the values of distortions for each value of k and calculate the distortion and inertia for each value of k in the given range.\n\nthen Building the clustering model and calculating the values of the Distortion and Inertia\n\nTo determine the optimal number of clusters, we have to select the value of k at the \u201celbow\u201d ie the point after which the distortion\/inertia start decreasing in a linear fashion."}}