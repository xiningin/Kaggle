{"cell_type":{"fa24ee3c":"code","13c37fb8":"code","be70fd8d":"code","240bf2d9":"code","edebeb8a":"code","2113f8d1":"code","81fa5f66":"code","896fb7f3":"code","60f37642":"code","3e25d69c":"code","fd4576ce":"code","ca429564":"code","c3c2d5fa":"code","c6c16940":"markdown","ec2402e7":"markdown","56415547":"markdown","e285a7df":"markdown","ba38f61f":"markdown","d9bb22c4":"markdown","c552ac20":"markdown"},"source":{"fa24ee3c":"import numpy as np\nfrom tqdm import tqdm_notebook\nfrom IPython.display import Image\n\nimport warnings\nwarnings.filterwarnings('ignore')","13c37fb8":"Image(filename='\/kaggle\/input\/imagesforkernel\/classification_kiank.png') ","be70fd8d":"Image(filename='\/kaggle\/input\/imagesforkernel\/grad_summary.png') ","240bf2d9":"class NonLinearFunctions:\n    @staticmethod\n    def sigmoid(z):\n        return 1.0\/(1+np.exp(-z))\n    \n    @staticmethod\n    def relu(z):\n        return max(0,z)\n    \n    @staticmethod\n    def leaky_relu(z,lb=0.01):\n        return max(lb*z,z)\n    \n    @staticmethod\n    def tanh_func(z):\n        return np.tanh(z)","edebeb8a":"class NeuralNet(NonLinearFunctions):\n    '''\n    input: \n        X : input features\n        y : targets\n        h : number of hidden units\n        lr : learning rate (default: 0.01)\n    '''\n    def __init__(self,X,y,h,lr = 1e-2):\n        self.X = X\n        self.y = y\n        self.h = h\n        self.lr = lr\n        \n    def layer_size(self):\n        n_x = self.X.shape[0]\n        n_h = self.h\n        n_y = self.y.shape[0]\n        return (n_x,n_h,n_y)\n    \n    def initialize_parameters(self):\n        (n_x,n_h,n_y) = self.layer_size()\n        W1 = np.random.randn(n_h,n_x)*1\/np.sqrt(n_x ** 1)\n        b1 = np.zeros((n_h,1))\n        W2 = np.random.randn(n_y,n_h)*1\/np.sqrt(n_x ** 1)\n        b2 = np.zeros((n_y,1))\n        \n        parameters = {'W1':W1,\n                     'b1':b1,\n                     'W2':W2,\n                     'b2':b2}\n        return parameters\n    \n    def forward(self,parameter):\n        W1 = parameter['W1']\n        b1 = parameter['b1']\n        W2 = parameter['W2']\n        b2 = parameter['b2']\n        \n        Z1 = np.dot(W1,self.X)+b1\n        A1 = NonLinearFunctions.sigmoid(Z1)\n        Z2 = np.dot(W2,A1)+b2\n        A2 = NonLinearFunctions.sigmoid(Z2)\n        \n        cache = {'Z1':Z1,\n                'A1':A1,\n                'Z2':Z2,\n                'A2':A2}\n        \n        return A2,cache\n    \n    def compute_cost(self,parameters):\n        m = self.y.shape[1]\n        A2,_ = self.forward(parameters)\n        logprob = np.multiply(np.log(A2),self.y)+np.multiply(np.log(1-A2),(1-self.y))\n        cost = -np.sum(logprob)\/m\n        cost = float(np.squeeze(cost))\n        \n        return cost\n    \n    def backward(self,parameter):\n        m = self.X.shape[1]\n        \n        W1 = parameter['W1']\n        b1 = parameter['b1']\n        W2 = parameter['W2']\n        b1 = parameter['b2']\n        \n        _,cache = self.forward(parameter)\n        A1 = cache['A1']\n        A2 = cache['A2']\n        \n        dZ2 = A2-self.y\n        dW2 = 1\/m*(np.dot(dZ2,A1.T))\n        db2 = 1\/m*(np.sum(dZ2,axis=1, keepdims=True))\n        dZ1 = np.multiply(np.dot(W2.T,dZ2),(1-np.power(A1,2)))\n        dW1 = 1\/m*(np.dot(dZ1,self.X.T))\n        db1 = 1\/m*(np.sum(dZ1,axis=1, keepdims=True))\n        \n        grads = {\"dW1\": dW1,\n                 \"db1\": db1,\n                 \"dW2\": dW2,\n                 \"db2\": db2}\n    \n        return grads\n    \n    def update_parameters(self,grads):\n        parameter = self.initialize_parameters()\n        W1 = parameter['W1']\n        b1 = parameter['b1']\n        W2 = parameter['W2']\n        b2 = parameter['b2']\n                \n        dW1 = grads['dW1']\n        db1 = grads['db1']\n        dW2 = grads['dW2']\n        db2 = grads['db2']\n        \n        W1 = W1 - self.lr*dW1\n        b1 = b1 - self.lr*db1\n        W2 = W2 - self.lr*dW2\n        b2 = b2 - self.lr*db2\n        \n        parameters = {\"W1\": W1,\n                      \"b1\": b1,\n                      \"W2\": W2,\n                      \"b2\": b2}\n        \n        return parameters\n    \n    def train_model(self,iteration = 100,print_cost = True,print_interval=10):\n        '''\n        input: \n            iteration : number of iterations\n            print_cost : True if you want to print cost function (default: True)\n            print_interval : interval in which cost will be printed\n        output: \n            cost : list of cost values\n        '''\n        costs = []\n        parameters = self.initialize_parameters()\n        for i in tqdm_notebook(range(1,iteration+1)):\n            A2,cache = self.forward(parameters)\n            cost = self.compute_cost(parameters)\n            grads = self.backward(parameters)\n            parameters = self.update_parameters(grads)\n            costs.append(cost)\n            if print_cost and i%print_interval ==0:\n                print(f'Cost for iteration {i}: {cost}')\n        return costs","2113f8d1":"from sklearn.datasets import fetch_california_housing\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split","81fa5f66":"cal_housing = fetch_california_housing()\nX = pd.DataFrame(cal_housing.data, columns=cal_housing.feature_names)\ny = cal_housing.target","896fb7f3":"y = y.reshape(-1,1)","60f37642":"y -= y.mean()","3e25d69c":"Model = NeuralNet(X,y,128,lr=1e-5)","fd4576ce":"cost = Model.train_model(iteration=500)","ca429564":"X = range(len(cost))\nimport matplotlib.pyplot as plt","c3c2d5fa":"plt.plot(X,cost)\nplt.title('Cost vs number of iteration')\nplt.show()","c6c16940":"- n_x: the size of the input layer\n- n_y: the size of the output layer\n\n- X : Input data\n- h : hidden layer size\n- y : target","ec2402e7":"### Back-Propagation Calculation","56415547":"### Test NeualNetwork","e285a7df":"### Forward Propagation and cost function","ba38f61f":"## In this notebook I will implement simple 2 layer neural network from scratch. \n\n## Please UPVOTE if you like this kernel","d9bb22c4":"### Neural Network ","c552ac20":"**Mathematically**:\n\nFor one example $x^{(i)}$:\n$$z^{[1] (i)} =  W^{[1]} x^{(i)} + b^{[1]}\\tag{1}$$ \n$$a^{[1] (i)} = \\tanh(z^{[1] (i)})\\tag{2}$$\n$$z^{[2] (i)} = W^{[2]} a^{[1] (i)} + b^{[2]}\\tag{3}$$\n$$\\hat{y}^{(i)} = a^{[2] (i)} = \\sigma(z^{ [2] (i)})\\tag{4}$$\n$$y^{(i)}_{prediction} = \\begin{cases} 1 & \\mbox{if } a^{[2](i)} > 0.5 \\\\ 0 & \\mbox{otherwise } \\end{cases}\\tag{5}$$\n\nGiven the predictions on all the examples, you can also compute the cost $J$ as follows: \n$$J = - \\frac{1}{m} \\sum\\limits_{i = 0}^{m} \\large\\left(\\small y^{(i)}\\log\\left(a^{[2] (i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[2] (i)}\\right)  \\large  \\right) \\small \\tag{6}$$"}}