{"cell_type":{"83dcb8c1":"code","0c8d9d6c":"code","ad921ee0":"code","88fe516a":"code","576d1144":"code","d92f06ea":"code","fb983cd6":"code","2b8e0f72":"code","5d1d097b":"code","04bffb63":"code","369cfead":"code","a6422461":"code","b0c96831":"code","c13fb5a3":"code","fe1df5e0":"code","937f80a3":"code","7088109c":"code","6e6f644a":"code","e56bbda3":"code","41fbdee3":"code","92bf60dd":"code","e5f612f7":"code","6f5a7c09":"code","fdec407f":"code","a45206ec":"code","a6dc7d1b":"code","d2772ff4":"code","35c3bb15":"code","ba68216b":"markdown","d8b4c1d7":"markdown","b2d9fef6":"markdown","27623a79":"markdown","c90a8c9c":"markdown","8e44d25c":"markdown","5b51cc21":"markdown","fd610e46":"markdown","9aaf08cd":"markdown","0329060c":"markdown","13985ad1":"markdown","52051f13":"markdown"},"source":{"83dcb8c1":"import numpy as np\nimport pandas as pd\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.metrics.pairwise import euclidean_distances\nimport matplotlib.pyplot as plt","0c8d9d6c":"# Reading train data\ndf_tweet_train = pd.read_csv('..\/input\/nlp-getting-started\/train.csv')","ad921ee0":"# Reading test data\ndf_tweet_test = pd.read_csv('..\/input\/nlp-getting-started\/test.csv')","88fe516a":"#Take a look at the dataset\ndf_tweet_train.head()","576d1144":"#shape of train dataframe\ndf_tweet_train.shape","d92f06ea":"#shape of test dataframe\ndf_tweet_test.shape","fb983cd6":"#gartting number of relevant and irrelevent tweets out of total 7,613 tweets in train dataset\ndf_tweet_train[['text', 'target']].groupby('target').count()","2b8e0f72":"#adding a new column to the dataframe for the length of each tweet\ndf_tweet_train['tweet_len'] = df_tweet_train.text.apply(len)\ndf_tweet_train.head()","5d1d097b":"#Distribution of tweets length based on relevant\/irrelevant fact\nplt.figure(figsize=(10, 6))\n\ndf_tweet_train[df_tweet_train.target== 0].tweet_len.plot(bins=40, kind='hist', color='green', \n                                       label='irrelevant', alpha=0.6)\ndf_tweet_train[df_tweet_train.target==1].tweet_len.plot(bins=40,kind='hist', color='red', \n                                       label='relevant', alpha=0.6)\nplt.legend()\nplt.xlabel(\"text Length\")","04bffb63":"#The following function will remove all stopwords (defined in the list of english stopwords in nltk) and punctuations from the text \nimport string\n\nfrom nltk.corpus import stopwords\n\ndef text_process(text):\n    \"\"\"\n    Takes in a string of text, then performs the following:\n    Vectorization1. Remove all punctuation\n    2. Remove all stopwords\n    3. Returns a list of the cleaned text\n    \"\"\"\n    STOPWORDS = stopwords.words('english')\n    # making a list of characters of the text, excluding punctuations (!\"#$%&'()*+,-.\/:;<=>?@[\\]^_`{|}~)\n    nopunc = [char for char in text if char not in string.punctuation]\n\n    # Join the characters with no space in between to form the text (excluding punctuations) again.\n    nopunc = ''.join(nopunc)\n    \n    # splitting string nopunc with spaces and making all words lowercase then\n    #check if the word exsists in STOPWORDS collection, if not join those words with space in between.\n    return ' '.join([word for word in nopunc.split() if word.lower() not in STOPWORDS])","369cfead":"#making a new column in the dataframe applying the text-preprocessing function to the \"text\" column.\ndf_tweet_train['clean_txt'] = df_tweet_train.text.apply(text_process)\ndf_tweet_test['clean_txt'] = df_tweet_test.text.apply(text_process)\ndf_tweet_train.head()","a6422461":"# defining X (input) and y (label) from the dataframe columns for later use in COUNTVECTORIZER\nX_train = df_tweet_train.clean_txt.values\ny_train = df_tweet_train.target.values\nX_test = df_tweet_test.clean_txt.values\n# y_test = df_tweet_test.target.values\n#shape and dimension of X and y arrays\nprint(X_train.shape)\nprint(y_train.shape)\nprint(X_test.shape)","b0c96831":"from sklearn.feature_extraction.text import CountVectorizer\n# instantiate the vectorizer object (content vectorizer) and training (fitting) that on the train dataset\nvect = CountVectorizer()\nvect.fit(X_train)","c13fb5a3":"#look at the \u201cvocabulary\u201d also called the \u201cdictionary\u201d for the whole representation\nvect.vocabulary_","fe1df5e0":"# learn training data vocabulary, then use it to create a document-term matrix\nX_train_dtm = vect.transform(X_train)\n# examine the document-term matrix\nX_train_dtm","937f80a3":"# transform testing data (using fitted vocabulary) into a document-term matrix\nX_test_dtm = vect.transform(X_test)\nX_test_dtm","7088109c":"# import and instantiate a Multinomial Naive Bayes model\nfrom sklearn.naive_bayes import MultinomialNB\nnb = MultinomialNB()","6e6f644a":"from sklearn import model_selection","e56bbda3":"# Accuracy based on cross validation.\naccuracy = model_selection.cross_val_score(nb, X_train_dtm, df_tweet_train[\"target\"], cv=3, scoring=\"accuracy\")\naccuracy","41fbdee3":"#Average accuracy of nb model in cross validation\nnp.average(accuracy)","92bf60dd":"# prediction based on cross validation.\nfrom sklearn.metrics import confusion_matrix\ny_pred = model_selection.cross_val_predict(nb, X_train_dtm, df_tweet_train[\"target\"], cv=3)\n# confusuin matrix based on cross validation.\nconf_mat = confusion_matrix(y_train, y_pred)\nconf_mat","e5f612f7":"# Fit Naive Bayes classifier according to X, y\nnb.fit(X_train_dtm, y_train)","6f5a7c09":"# make class predictions for X_test_dtm\ny_pred_class = nb.predict(X_test_dtm)\n#The first 10 predictions\ny_pred_class[:10]","fdec407f":"# calculate AUC\nfrom sklearn.metrics import roc_auc_score\nroc_auc_score(y_train, y_pred)","a45206ec":"sample_submission = pd.read_csv(\"..\/input\/nlp-getting-started\/sample_submission.csv\")","a6dc7d1b":"sample_submission[\"target\"] = nb.predict(X_test_dtm)","d2772ff4":"sample_submission.iloc[::5]","35c3bb15":"sample_submission.to_csv(\"submission.csv\", index=False)","ba68216b":"This train dtm matrix contains 7613 train articles samples (rows) and 22,310 vocabs (columns). Its data type is integer, meaning all 0,1,2 (if two of a specific vocab in a text)..\n","d8b4c1d7":"# Vectorization","b2d9fef6":"# Summary\nWe aim to distinguish relavant tweets to a weather disaster. As a result later we will be able to use those tweets to predict a weather disaster, maybe as a weather forcasting agency. \nIn this dataset we initially conducted a comprehensive EDA on the dataset.","27623a79":"#  Text Pre-processing","c90a8c9c":"As we can notice in the above plot relevant tweets are usually longer than irrelevant tweets. ","8e44d25c":"# Modeling","5b51cc21":"As we can see above it is an imbalanced dataset, as number f irrelevant tweets is considerably higher than relevant ones.","fd610e46":"Now we need to convert text documents to a matrix of token counts\n","9aaf08cd":"# Description of Dataset\nUsed dataset is consist of relevant and irrelevant tweets to a weather disaster.","0329060c":"# EDA","13985ad1":"# Reading Data","52051f13":"This test dtm matrix contains 3263 test articles samples (rows) and 22,310 fitted vocabulary (columns). "}}