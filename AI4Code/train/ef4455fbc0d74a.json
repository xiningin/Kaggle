{"cell_type":{"041827d5":"code","0e15c5c5":"code","fc31ab94":"code","c1bb43b7":"code","686019e1":"code","606a34f6":"code","cf12fe5e":"code","31978d74":"code","495be779":"code","fb0682e4":"code","2c1d4039":"code","57d44ce0":"code","1e47d6da":"code","ae027439":"code","a0d0cc8f":"code","84efa8a8":"code","204090d4":"code","267b5acf":"code","45b7fee2":"code","5adb94f5":"code","1d88837b":"code","ad6df825":"code","6a61dc2a":"code","d0504f7f":"code","5dce8cab":"code","f61cdffd":"code","b7a233b0":"code","1277db70":"code","fa6602b2":"code","6367d04d":"code","b92e57e9":"code","e770165d":"code","f530bf26":"code","0f539a27":"code","e98c1a9f":"code","b4e79959":"markdown","cea277f2":"markdown","0aa86463":"markdown"},"source":{"041827d5":"from fastai import *\nfrom fastai.text import *\nimport pandas as pd","0e15c5c5":"train_df= pd.read_csv('..\/input\/nlp-getting-started\/train.csv')\ntest_df= pd.read_csv('..\/input\/nlp-getting-started\/test.csv')\ntrain_df.head()","fc31ab94":"test_df.head()","c1bb43b7":"# Data cleaning steps\n    \n# Remove \"@user from tweets\ndef clean_data(df, text_col, new_col='cleaned_text'):\n    \n    '''It will remove the noise from the text data(@user, characters not able to encode\/decode properly)    \n    Arguments:\n    df : Data Frame\n    col : column name of type string\n    '''\n    tweets_data = df.copy()\n    \n    #tweets_data[new_col] = tweets_data[text_col].apply(lambda x : re.sub(\n    #   f\"@[A-Za-z0-9]+\", '', x))\n    \n    #temp line\n    tweets_data[new_col] = tweets_data[text_col]\n \n\n    # Keeping only few punctuations \n    tweets_data[new_col] = tweets_data[new_col].apply(lambda x : re.sub(\n        '@[A-Za-z0-9]+', '', x))\n\n    #tweets_data[new_col] = tweets_data[new_col].apply(lambda x : re.sub(\n        #f'[^{PUNCTUATION_TO_KEEP}A-Za-z0-9]', '', x))\n\n    # Trimming the sentences\n    tweets_data[new_col] = tweets_data[new_col].str.strip()\n    \n    tweets_data[new_col] = tweets_data[new_col].map(lambda x: re.sub('\\\\n',' ',str(x)))\n    tweets_data[new_col] = tweets_data[new_col].map(lambda x: re.sub(r'\\W',' ',str(x)))\n    tweets_data[new_col] = tweets_data[new_col].map(lambda x: re.sub(r'https\\s+|www.\\s+',r'', str(x)))\n    tweets_data[new_col] = tweets_data[new_col].map(lambda x: re.sub(r'http\\s+|www.\\s+',r'', str(x)))\n    tweets_data[new_col] = tweets_data[new_col].map(lambda x: re.sub(r'\\s+[a-zA-Z]\\s+',' ',str(x)))\n    tweets_data[new_col] = tweets_data[new_col].map(lambda x: re.sub(r'\\^[a-zA-Z]\\s+',' ',str(x)))\n    tweets_data[new_col] = tweets_data[new_col].map(lambda x: re.sub(r'\\s+',' ',str(x)))\n    tweets_data[new_col] = tweets_data[new_col].str.lower()\n    tweets_data[new_col] = tweets_data[new_col].map(lambda x: re.sub(r\"\\\u2019\", \"\\'\", str(x)))\n    tweets_data[new_col] = tweets_data[new_col].map(lambda x: re.sub(r\"won\\'t\", \"will not\", str(x)))\n    tweets_data[new_col] = tweets_data[new_col].map(lambda x: re.sub(r\"can\\'t\", \"can not\", str(x)))\n    tweets_data[new_col] = tweets_data[new_col].map(lambda x: re.sub(r\"don\\'t\", \"do not\", str(x)))\n    tweets_data[new_col] = tweets_data[new_col].map(lambda x: re.sub(r\"dont\", \"do not\", str(x)))\n    tweets_data[new_col] = tweets_data[new_col].map(lambda x: re.sub(r\"n\\\u2019t\", \" not\", str(x)))\n    tweets_data[new_col] = tweets_data[new_col].map(lambda x: re.sub(r\"n\\'t\", \" not\", str(x)))\n    tweets_data[new_col] = tweets_data[new_col].map(lambda x: re.sub(r\"\\'re\", \" are\", str(x)))\n    tweets_data[new_col] = tweets_data[new_col].map(lambda x: re.sub(r\"\\'s\", \" is\", str(x)))\n    tweets_data[new_col] = tweets_data[new_col].map(lambda x: re.sub(r\"\\\u2019d\", \" would\", str(x)))\n    tweets_data[new_col] = tweets_data[new_col].map(lambda x: re.sub(r\"\\d\", \" would\", str(x)))\n    tweets_data[new_col] = tweets_data[new_col].map(lambda x: re.sub(r\"\\'ll\", \" will\", str(x)))\n    tweets_data[new_col] = tweets_data[new_col].map(lambda x: re.sub(r\"\\'t\", \" not\", str(x)))\n    tweets_data[new_col] = tweets_data[new_col].map(lambda x: re.sub(r\"\\'ve\", \" have\", str(x)))\n    tweets_data[new_col] = tweets_data[new_col].map(lambda x: re.sub(r\"\\'m\", \" am\", str(x)))\n    tweets_data[new_col] = tweets_data[new_col].map(lambda x: re.sub(r\"\\n\", \"\", str(x)))\n    tweets_data[new_col] = tweets_data[new_col].map(lambda x: re.sub(r\"\\r\", \"\", str(x)))\n    tweets_data[new_col] = tweets_data[new_col].map(lambda x: re.sub(r\"[0-9]\", \"digit\", str(x)))\n    tweets_data[new_col] = tweets_data[new_col].map(lambda x: re.sub(r\"\\'\", \"\", str(x)))\n    tweets_data[new_col] = tweets_data[new_col].map(lambda x: re.sub(r\"\\\"\", \"\", str(x)))\n    tweets_data[new_col] = tweets_data[new_col].map(lambda x: re.sub(r'[?|!|\\'|\"|#]',r'', str(x)))\n    tweets_data[new_col] = tweets_data[new_col].map(lambda x: re.sub(r'[.|,|)|(|\\|\/]',r' ', str(x)))\n    \n    return tweets_data\n","686019e1":"train_df = clean_data(train_df, \"text\", 'cleaned_text')\nfrom nltk.stem import PorterStemmer\nstemming = PorterStemmer()\nstemming.stem('runs')\ntemp = train_df['cleaned_text'].apply(lambda sentence : [stemming.stem(x) for x in sentence.split(\" \")])\n\nsentences = []\nfor i in temp:\n    sentences.append(\" \".join(i))\nsentences[0]\n\ntrain_df['stemmed_text'] = sentences\n\ndel sentences, temp","606a34f6":"test_df = clean_data(test_df, \"text\", 'cleaned_text')\nfrom nltk.stem import PorterStemmer\nstemming = PorterStemmer()\nstemming.stem('runs')\ntemp = test_df['cleaned_text'].apply(lambda sentence : [stemming.stem(x) for x in sentence.split(\" \")])\n\nsentences = []\nfor i in temp:\n    sentences.append(\" \".join(i))\nsentences[0]\n\ntest_df['stemmed_text'] = sentences\n\ndel sentences, temp","cf12fe5e":"# Combining all the text data \ntweets_data = pd.concat([train_df, test_df], axis=0, sort=False, ignore_index=True)\ntweets_data.head()","31978d74":"path = Path('\/kaggle\/working\/')","495be779":"data_lm = (TextList.from_df(tweets_data, path, cols = \"cleaned_text\").split_by_rand_pct(0.1).label_for_lm().databunch(bs=32))","fb0682e4":"data_lm.show_batch()","2c1d4039":"learn = language_model_learner(data_lm, AWD_LSTM, drop_mult=0.3)","57d44ce0":"learn.lr_find()","1e47d6da":"learn.recorder.plot(skip_end=15, suggestion = True)","ae027439":"learn.fit_one_cycle(5, 1e-2, moms=(0.8,0.7))","a0d0cc8f":"learn.unfreeze()","84efa8a8":"learn.lr_find()\nlearn.recorder.plot(skip_end=15, suggestion = True)","204090d4":"learn.fit_one_cycle(5, 1e-4, moms=(0.8,0.7))","267b5acf":"learn.save_encoder('fine_tuned_enc')","45b7fee2":"path","5adb94f5":"from sklearn.model_selection import train_test_split\nX_train, X_val = train_test_split(train_df, test_size = 0.3)","1d88837b":"data_classifier = (TextDataBunch.from_df('.', X_train, X_val, test_df, text_cols = \"cleaned_text\", label_cols = \"target\", vocab = data_lm.vocab))","ad6df825":"data_classifier.show_batch()","6a61dc2a":"learn = text_classifier_learner(data_classifier, AWD_LSTM, drop_mult=0.7)\nlearn.load_encoder('fine_tuned_enc')","d0504f7f":"learn.lr_find()\nlearn.recorder.plot(suggestion= True)","5dce8cab":"learn.fit_one_cycle(5, 5e-04, moms=(0.8,0.7))","f61cdffd":"learn.freeze_to(-2)\nlearn.fit_one_cycle(5, slice(1e-2\/(2.6**4),1e-2), moms=(0.8,0.7))","b7a233b0":"learn.freeze_to(-3)\nlearn.fit_one_cycle(5, slice(5e-3\/(2.6**4),5e-3), moms=(0.8,0.7))","1277db70":"learn.unfreeze()\nlearn.fit_one_cycle(5, slice(1e-3\/(2.6**4),1e-3), moms=(0.8,0.7))","fa6602b2":"learn.save('final_learner')","6367d04d":"learn.recorder.plot_losses()","b92e57e9":"def get_preds_as_nparray(ds_type) -> np.ndarray:\n    \"\"\"\n    the get_preds method does not yield the elements in order by default\n    we borrow the code from the RNNLearner to resort the elements into their correct order\n    \"\"\"\n    preds = learn.get_preds(ds_type)[0].detach().cpu().numpy()\n    sampler = [i for i in data_classifier.dl(ds_type).sampler]\n    reverse_sampler = np.argsort(sampler)\n    return preds[reverse_sampler, :]","e770165d":"test_preds = get_preds_as_nparray(DatasetType.Test)\npreds = []","f530bf26":"for i in test_preds:\n    preds.append(np.argmax(i))","0f539a27":"sub = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/sample_submission.csv\")\nsub.head(3)","e98c1a9f":"sub['target'] = preds\nsub.to_csv('submission.csv', index=False)\nsub.head(3)","b4e79959":"# Language Model","cea277f2":"# Data Cleaning","0aa86463":"# Classifier"}}