{"cell_type":{"3c4c54ca":"code","a79f9723":"code","68f2ef48":"code","94f98342":"code","68bba947":"code","963a9c49":"code","d06dfb13":"code","71dcfc28":"code","998b860a":"code","e306d94e":"code","bb1adcca":"code","b0102b1a":"code","6589798c":"code","d4dfb069":"code","130c49ee":"code","0e8a18e5":"code","b17e8540":"code","22352f4d":"code","fbd4e53e":"markdown","e5b2eac7":"markdown","f8e46eae":"markdown","4ea606f1":"markdown","67435e7f":"markdown","d75cdc29":"markdown","757eaf6c":"markdown","80d36b9a":"markdown","1c8231c2":"markdown"},"source":{"3c4c54ca":"class Config:\n    name_v1 = \"lgb baseline\"\n    model_params = dict(objective=\"mae\",\n                        n_estimators=10000,\n                        num_leaves=31,\n                        random_state=2021,\n                        importance_type=\"gain\",\n                        colsample_bytree=0.3,\n                        learning_rate=0.5\n                       )\n    fit_params = dict(early_stopping_rounds=100, verbose=100)\n    n_fold = 2\n    seeds = [2021]\n    target_col = \"pressure\"\n    debug = False","a79f9723":"import os\nimport joblib\nimport logging\nimport warnings\nimport datetime\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\n\nfrom sklearn import model_selection\nfrom sklearn.metrics import mean_absolute_error\n\nfrom lightgbm import LGBMModel\nfrom matplotlib_venn import venn2\nfrom tqdm import tqdm","68f2ef48":"class Logger:\n    \"\"\"save log\"\"\"\n    def __init__(self, path):\n        self.general_logger = logging.getLogger(path)\n        stream_handler = logging.StreamHandler()\n        file_general_handler = logging.FileHandler(os.path.join(path, 'Experiment.log'))\n        if len(self.general_logger.handlers) == 0:\n            self.general_logger.addHandler(stream_handler)\n            self.general_logger.addHandler(file_general_handler)\n            self.general_logger.setLevel(logging.INFO)\n\n    def info(self, message):\n        # display time\n        self.general_logger.info('[{}] - {}'.format(self.now_string(), message))\n\n    @staticmethod\n    def now_string():\n        return str(datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S'))\n    \n    \nclass Util:\n    \"\"\"save & load\"\"\"\n    @classmethod\n    def dump(cls, value, path):\n        joblib.dump(value, path, compress=True)\n\n    @classmethod\n    def load(cls, path):\n        return joblib.load(path)\n    \n    \nclass HorizontalDisplay:\n    \"\"\"display dataframe\"\"\"\n    def __init__(self, *args):\n        self.args = args\n\n    def _repr_html_(self):\n        template = '<div style=\"float: left; padding: 10px;\">{0}<\/div>'\n        return \"\\n\".join(template.format(arg._repr_html_())\n                         for arg in self.args)\n    \n    \ndef reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() \/ 1024**2 \n    dfs = []\n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    dfs.append(df[col].astype(np.int8))\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    dfs.append(df[col].astype(np.int16))\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    dfs.append(df[col].astype(np.int32))\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    dfs.append(df[col].astype(np.int64) ) \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    dfs.append(df[col].astype(np.float16))\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    dfs.append(df[col].astype(np.float32))\n                else:\n                    dfs.append(df[col].astype(np.float64))\n        else:\n            dfs.append(df[col])\n    \n    df_out = pd.concat(dfs, axis=1)\n    if verbose:\n        end_mem = df_out.memory_usage().sum() \/ 1024**2\n        num_reduction = str(100 * (start_mem - end_mem) \/ start_mem)\n        print(f'Mem. usage decreased to {str(end_mem)[:3]}Mb:  {num_reduction[:2]}% reduction')\n    return df_out","94f98342":"INPUT = \"..\/input\/ventilator-pressure-prediction\"\nEXP = \".\/\"\nEXP_MODEL = os.path.join(EXP, \"model\")\nEXP_FIG = os.path.join(EXP, \"fig\")\nEXP_PREDS = os.path.join(EXP, \"preds\")\n\n# make dirs\nfor d in [EXP_MODEL, EXP_FIG, EXP_PREDS]:\n    os.makedirs(d, exist_ok=True)\n    \n# utils\nlogger = Logger(EXP)\nwarnings.filterwarnings(\"ignore\")\nsns.set(style='whitegrid')","68bba947":"train = pd.read_csv(os.path.join(INPUT, \"train.csv\"))\ntest = pd.read_csv(os.path.join(INPUT, \"test.csv\"))\nsample_submission = pd.read_csv(os.path.join(INPUT, \"sample_submission.csv\"))\n\nif Config.debug:\n    train = train[train[\"breath_id\"].isin(np.random.choice(train[\"breath_id\"].unique(), 100))].reset_index(drop=True)\n    test = test[test[\"breath_id\"].isin(np.random.choice(test[\"breath_id\"].unique(), 100))].reset_index(drop=True)\n    sample_submission = sample_submission[sample_submission[\"id\"].isin(test[\"id\"].tolist())].reset_index(drop=True)","963a9c49":"HorizontalDisplay(train, test, sample_submission)","d06dfb13":"data = pd.concat([train, test]).reset_index(drop=True)\nHorizontalDisplay(\n    data[\"breath_id\"].value_counts().to_frame(), \n    data[\"R\"].value_counts().to_frame(),\n    data[\"C\"].value_counts().to_frame()\n)","71dcfc28":"def plot_intersection(left, right, column, set_labels, ax=None):\n    left_set = set(left[column])\n    right_set = set(right[column])\n    venn2(subsets=(left_set, right_set), set_labels=set_labels, ax=ax)\n    return ax\n\n\ndef plot_right_left_intersection(train_df, test_df, columns='__all__'):\n    \"\"\"visualize venn(by nyk510)\"\"\"\n    if columns == '__all__':\n        columns = set(train_df.columns) & set(test_df.columns)\n\n    columns = list(columns)\n    nfigs = len(columns)\n    ncols = 5\n    nrows = - (- nfigs \/\/ ncols)\n    fig, axes = plt.subplots(figsize=(3 * ncols, 3 * nrows), ncols=ncols, nrows=nrows)\n    axes = np.ravel(axes)\n    for c, ax in zip(columns, axes):\n        plot_intersection(train_df, test_df, column=c, set_labels=('Train', 'Test'), ax=ax)\n        ax.set_title(c)\n    return fig\n\n\ntrain[\"R+C+u_out\"] = train[\"R\"].astype(str) +\"+\"+ train[\"C\"].astype(str) + \"+\" + train[\"u_out\"].astype(str)\ntest[\"R+C+u_out\"] = test[\"R\"].astype(str) +\"+\"+ test[\"C\"].astype(str) + \"+\" + test[\"u_out\"].astype(str)\nfig = plot_right_left_intersection(train, test, columns=[\"breath_id\", \"R\", \"C\", \"u_out\", \"R+C+u_out\"])\n\ndel train[\"R+C+u_out\"], test[\"R+C+u_out\"]","998b860a":"# pressure distribution (by u out)\nfig, ax = plt.subplots(figsize=(14, 7))\nax = sns.distplot(train.loc[train[\"u_out\"] == 0, \"pressure\"], ax=ax, label=\"u_out=0\", bins=200)\nax = sns.distplot(train.loc[train[\"u_out\"] == 1, \"pressure\"], ax=ax, label=\"u_out=1\", bins=200)\nax.legend(loc='upper right')","e306d94e":"# pressur distribution (by R)\nfig, ax = plt.subplots(figsize=(14, 7))\nax = sns.distplot(train.loc[train[\"R\"] == 5, \"pressure\"], ax=ax, label=\"R=5\", bins=200)\nax = sns.distplot(train.loc[train[\"R\"] == 20, \"pressure\"], ax=ax, label=\"R=20\", bins=200)\nax = sns.distplot(train.loc[train[\"R\"] == 50, \"pressure\"], ax=ax, label=\"R=50\", bins=200)\nax.legend(loc='upper right')","bb1adcca":"# pressure distribution (by C)\nfig, ax = plt.subplots(figsize=(14, 7))\nax = sns.distplot(train.loc[train[\"C\"] == 10, \"pressure\"], ax=ax, label=\"C=10\", bins=200)\nax = sns.distplot(train.loc[train[\"C\"] == 20, \"pressure\"], ax=ax, label=\"C=20\", bins=200)\nax = sns.distplot(train.loc[train[\"C\"] == 50, \"pressure\"], ax=ax, label=\"C=50\", bins=200)\nax.legend(loc='upper right')","b0102b1a":"breath_ids = list(train[\"breath_id\"].sample(12))\nfig, axes = plt.subplots(figsize=(25, 18), ncols=4, nrows=3)\naxes = np.ravel(axes)\nfor b, ax in zip(breath_ids, axes):\n    _df = train[train[\"breath_id\"]==b].copy()\n    (_df\n     .set_index(\"time_step\")[[\"pressure\", \"u_in\", \"u_out\"]]\n     .plot(colormap='Paired',\n           ax=ax,\n           title=f\"breath_id={b}, R={_df['R'].unique()}, C={_df['C'].unique()}\", \n           linewidth=2)\n    )\n    \nfig.subplots_adjust(hspace=0.3)","6589798c":"def aggregation(input_df, group_key, group_values, agg_methods):\n    \"\"\"ref:https:\/\/github.com\/pfnet-research\/xfeat\/blob\/master\/xfeat\/helper.py\"\"\"\n    new_df = []\n    for agg_method in agg_methods:\n        for col in group_values:\n            if callable(agg_method):\n                agg_method_name = agg_method.__name__\n            else:\n                agg_method_name = agg_method\n            new_col = f\"agg_{agg_method_name}_{col}_grpby_{group_key}\"\n            df_agg = (input_df[[col] + [group_key]].groupby(group_key)[[col]].agg(agg_method))\n            df_agg.columns = [new_col]\n            new_df.append(df_agg)\n            \n    _df = pd.concat(new_df, axis=1).reset_index()\n    output_df = pd.merge(input_df[[group_key]], _df, on=group_key, how=\"left\")\n    return output_df.drop(group_key, axis=1)","d4dfb069":"def get_raw_features(input_df):\n    cols = [\n        \"R\",\n        \"C\",\n        \"time_step\",\n        \"u_in\",\n        \"u_out\"\n    ]\n    output_df = input_df[cols].copy()\n    return output_df\n\n\ndef get_cross_features(input_df):\n    output_df = pd.DataFrame()\n    output_df[\"R+C\"] = (input_df[\"R\"].astype(str) + input_df[\"C\"].astype(str)).astype(int)\n    return output_df\n\n\ndef get_shift_grpby_breath_id_features(input_df):\n    # future feats\n    shift_times = [-1, -2, 1, 2]\n    group_key = \"breath_id\"\n    \n    group_values = [\"u_in\"]\n    \n    output_df = pd.DataFrame()\n    for t in shift_times:\n        _df = input_df.groupby(group_key)[group_values].shift(t)\n        _df.columns = [f'shift={t}_{col}_grpby_{group_key}' for col in group_values]\n        output_df = pd.concat([output_df, _df], axis=1)\n    return output_df\n\n\ndef get_diff_grpby_breath_id_features(input_df):\n    # future feats\n    diff_times = [-1, -2, 1, 2]\n    group_key = \"breath_id\"\n    group_values = [\"u_in\"]\n    \n    output_df = pd.DataFrame()\n    for t in diff_times:\n        _df = input_df.groupby(group_key)[group_values].shift(t)\n        _df.columns = [f'diff={t}_{col}_grpby_{group_key}' for col in group_values]\n        output_df = pd.concat([output_df, _df], axis=1)\n    return output_df\n\n\ndef get_cumsum_grpby_breath_id_features(input_df):\n    group_key = \"breath_id\"\n    group_values = [\"time_step\", \"u_in\", \"u_out\"]\n    \n    output_df = pd.DataFrame()\n    for group_val in group_values:\n        col_name = f\"agg_cumsum_{group_val}_grpby_{group_key}\"\n        output_df[col_name] = input_df.groupby(group_key)[group_val].cumsum()\n        \n    return output_df\n\n\ndef get_time_step_cat_features(input_df):\n    output_df = pd.DataFrame()    \n    output_df[\"time_step_cat\"] = input_df[\"time_step\"].copy()\n    output_df.loc[input_df[\"time_step\"] < 1, \"time_step_cat\"] = 0\n    output_df.loc[(1 < input_df[\"time_step\"]) & (input_df[\"time_step\"]< 1.5), \"time_step_cat\"] = 1\n    output_df.loc[1.5 < input_df[\"time_step\"], \"time_step_cat\"] = 2\n    return output_df\n\n\ndef get_breath_id_pivot_features(input_df):\n    _df = input_df[[\"breath_id\", \"time_step\", \"u_in\"]].copy()\n    _df[\"time_step_id\"] =  _df.groupby([\"breath_id\"])[\"time_step\"].rank(ascending=True)\n    _df = pd.pivot_table(_df, columns=\"time_step_id\", index=\"breath_id\", values=\"u_in\")\n    _df.columns = [f\"time_step_id={int(i):02}_u_in\" for i in _df.columns]\n    output_df = pd.merge(input_df[[\"breath_id\"]], _df, left_on=\"breath_id\", right_index=True, how=\"left\")\n    return output_df.drop(\"breath_id\", axis=1)\n\n\ndef get_agg_breath_id_whole_features(whole_df):\n    # do not have to use whole_df\n    group_key = \"breath_id\"\n    group_values = [\"u_in\"]\n    agg_methods = [\"mean\", \"std\", \"median\", \"max\", \"sum\"]\n    \n    output_df = aggregation(whole_df, group_key, group_values, agg_methods)\n    \n    # z-score\n    z_col_name = _get_agg_col_name(group_key, group_values, [\"z-score\"])\n    m_col_name = _get_agg_col_name(group_key, group_values, [\"mean\"])\n    s_col_name = _get_agg_col_name(group_key, group_values, [\"std\"])\n    \n    output_df[z_col_name] = ((whole_df[group_values].values - output_df[m_col_name].values) \n                             \/ (output_df[m_col_name].values + 1e-8))    \n    return output_df\n\n\ndef _get_agg_col_name(group_key, group_values, agg_methods):\n    out_cols = []\n    for group_val in group_values:\n        for agg_method in agg_methods:\n            out_cols.append(f\"agg_{agg_method}_{group_val}_grpby_{group_key}\")\n    return out_cols","130c49ee":"def get_features(input_df):\n    output_df = pd.DataFrame()\n    funcs = [\n        get_raw_features,\n        get_cross_features,\n        get_shift_grpby_breath_id_features,\n        get_diff_grpby_breath_id_features,\n        get_time_step_cat_features,\n        get_breath_id_pivot_features,\n        get_cumsum_grpby_breath_id_features\n    ]\n    for func in funcs:\n        print(func.__name__)\n        _df = func(input_df)\n        _df = reduce_mem_usage(_df)\n        output_df = pd.concat([output_df, _df], axis=1)\n        \n    return output_df\n\n\ndef get_whole_features(train, test):\n    whole_df = pd.concat([train, test]).reset_index(drop=True)\n    output_df = pd.DataFrame()\n    funcs = [\n        get_agg_breath_id_whole_features,\n    ]\n    \n    if not funcs:\n        return pd.DataFrame(), pd.DataFrame()\n    \n    for func in funcs:\n        print(func.__name__)\n        _df = func(whole_df)\n        _df = reduce_mem_usage(_df)\n        output_df = pd.concat([output_df, _df], axis=1)\n        \n    train_x = output_df.iloc[:len(train)]\n    test_x = output_df.iloc[len(train):].reset_index(drop=True)\n    \n    return train_x, test_x\n    \n    \ndef preprocess(train, test):\n    # whole feature\n    train_x, test_x = get_whole_features(train, test)\n    train_x = pd.concat([train_x, get_features(train)], axis=1)\n    test_x = pd.concat([test_x, get_features(test)], axis=1)\n    train_y = train[Config.target_col]\n    return train_x, train_y, test_x","0e8a18e5":"def lgb_metrics(y_true, y_pred, weight):\n    weight = (1 - weight).astype(bool)\n    y_true, y_pred = y_true[weight], y_pred[weight]\n        \n    score = mean_absolute_error(y_true, y_pred)\n    return \"custom_mae\", score, False\n\n\ndef transform_lgb_custom_metric(fit_params, eval_sample_weight):\n    fit_params[\"eval_metric\"] = lgb_metrics\n    fit_params[\"eval_sample_weight\"] = eval_sample_weight\n    \n    return fit_params\n\n\nclass LGBM:\n    \"\"\"MyLGBMModel\"\"\"\n    def __init__(self, model_params={}, fit_params={}):\n        self.model = None\n        self.model_params = model_params\n        self.fit_params = fit_params\n\n    def build(self):\n        self.model = LGBMModel(**self.model_params)\n    \n    def fit(self, tr_x, tr_y, va_x, va_y):\n        self.fit_params = transform_lgb_custom_metric(\n            self.fit_params, \n            eval_sample_weight=[va_x[\"u_out\"].values]\n        )\n        \n        self.model.fit(tr_x, tr_y, eval_set=[(va_x, va_y)], **self.fit_params)\n    \n    def predict(self, x):\n        preds = self.model.predict(x)\n        return preds\n\n    def save(self, filepath):\n        Util.dump(self.model, filepath + \".pkl\")\n    \n    def load(self, filepath):\n        self.model = Util.load(filepath + \".pkl\")","b17e8540":"def metrics(y_true, y_pred, weight=None):\n    \n    if weight is not None:\n        y_true, y_pred = y_true[weight], y_pred[weight]\n        \n    score = mean_absolute_error(y_true, y_pred)\n    return score\n\n\nclass GroupKFold:\n    \"\"\"\n    GroupKFold with random shuffle with a sklearn-like structure (by katsu1110)\n    \"\"\"\n    def __init__(self, n_splits=4, shuffle=True, random_state=42):\n        self.n_splits = n_splits\n        self.shuffle = shuffle\n        self.random_state = random_state\n\n    def get_n_splits(self, X=None, y=None, group=None):\n        return self.n_splits\n\n    def split(self, X=None, y=None, group=None):\n        kf = model_selection.KFold(n_splits=self.n_splits, shuffle=self.shuffle, random_state=self.random_state)\n        unique_ids = group.unique()\n        for tr_group_idx, va_group_idx in kf.split(unique_ids):\n            # split group\n            tr_group, va_group = unique_ids[tr_group_idx], unique_ids[va_group_idx]\n            train_idx = np.where(group.isin(tr_group))[0]\n            val_idx = np.where(group.isin(va_group))[0]\n            yield train_idx, val_idx\n\n            \ndef gkf(X, group, n_splits, random_state):\n    gkf = GroupKFold(n_splits=n_splits, random_state=random_state, shuffle=True)\n    return list(gkf.split(X, group=group))\n\n\ndef train_cv_v1(X, y, model, cv, metrics, name, directory):\n    oof = np.zeros(len(y))\n    for i_fold, (tr_idx, va_idx) in enumerate(cv):\n        filepath = os.path.join(directory, f\"{name}_fold{i_fold+1}\")\n        tr_x, va_x = X.iloc[tr_idx].reset_index(drop=True), X.iloc[va_idx].reset_index(drop=True)\n        tr_y, va_y = y.values[tr_idx], y.values[va_idx]        \n        \n        model.build()\n        model.fit(tr_x, tr_y, va_x, va_y)\n        preds = model.predict(va_x)\n        model.save(filepath)\n        oof[va_idx] = preds\n\n        score = metrics(np.array(va_y), np.array(preds), weight=va_x[\"u_out\"] == 0)\n        logger.info(f\"{name}_fold{i_fold+1} >>> val socre:{score:.4f}\")\n        \n    score = metrics(np.array(y), oof, weight=X[\"u_out\"] == 0)\n    logger.info(f\"{name} >>> val score:{score:.4f}\")\n    return oof\n\n\ndef predict_cv_v1(X, model, name, directory):\n    preds_fold = []\n    for i_fold in range(Config.n_fold):\n        filepath = os.path.join(directory, f\"{name}_fold{i_fold+1}\")\n        logger.info(f\"{name}_fold{i_fold+1} inference\")\n        model.build()\n        model.load(filepath)\n        preds = model.predict(X)\n        preds_fold.append(preds)\n    \n    preds = np.mean(preds_fold, axis=0)\n    return preds\n\n\ndef tree_importance(X, y, model, cv):\n    \"\"\"get importance\"\"\"\n    feature_importance_df = pd.DataFrame()\n    for i, (tr_idx, va_idx) in enumerate(cv):\n        tr_x, va_x = X.iloc[tr_idx], X.iloc[va_idx]\n        tr_y, va_y = y.iloc[tr_idx], y.iloc[va_idx]\n        \n        model.build()\n        model.fit(tr_x, tr_y, va_x, va_y) \n        _df = pd.DataFrame()\n        _df['feature_importance'] = model.model.feature_importances_\n        _df['column'] = X.columns\n        _df['fold'] = i + 1\n        feature_importance_df = pd.concat([feature_importance_df, _df], axis=0, ignore_index=True)\n\n    order = feature_importance_df.groupby('column') \\\n                .sum()[['feature_importance']] \\\n                .sort_values('feature_importance', ascending=False).index[:50]\n    fig, ax = plt.subplots(figsize=(12, max(4, len(order) * .2)))\n    sns.boxenplot(data=feature_importance_df, y='column', x='feature_importance', order=order, ax=ax,\n                  palette='viridis')\n    fig.tight_layout()\n    ax.grid()\n    ax.set_title('feature importance')\n    fig.tight_layout()\n    return fig, feature_importance_df\n\n\ndef plot_regression_result(y, oof, directory):\n    fig, ax = plt.subplots(figsize=(8, 8))\n    sns.distplot(y, label='y', color='cyan', ax=ax)\n    sns.distplot(oof, label='oof', color=\"magenta\", ax=ax)\n    # sns.distplot(preds, label='preds', color=\"yellow\", ax=ax, kde=True)\n    \n    ax.legend()\n    ax.grid()\n    ax.set_title(\"regression_result\")\n    fig.tight_layout()\n    return fig","22352f4d":"# preprocess\nprint(\"# ============= # Preprocess # ============= #\")\ntrain_x, train_y, test_x = preprocess(train, test)\nprint(train_x.shape)\n\n# feature importace \nprint(\"# ============= # Importance # ============= #\")\nfig, _df = tree_importance(\n    X=train_x,\n    y=train_y, \n     model=LGBM(model_params=Config.model_params,\n                fit_params=Config.fit_params),\n    cv=gkf(train, group=train[\"breath_id\"],\n           n_splits=2, random_state=Config.seeds[0])\n)\nfig.savefig(os.path.join(EXP_FIG, \"importance.png\"), dpi=300)\n\n# training\nprint(\"# ============= # Training # ============= #\")\noof_df = pd.DataFrame()\nfor seed in Config.seeds:\n    name = f\"{Config.name_v1}-{seed}\"\n    Config.model_params[\"random_state\"] = seed\n    oof = train_cv_v1(\n        X=train_x,\n        y=train_y, \n        model=LGBM(model_params=Config.model_params, \n                   fit_params=Config.fit_params),\n        cv=gkf(train, \n               group=train[\"breath_id\"],\n               n_splits=Config.n_fold, \n               random_state=seed),\n        metrics=metrics, \n        name=name, \n        directory=EXP_MODEL)\n\n    oof_df[name] = oof\noof_df.to_csv(os.path.join(EXP_PREDS, \"oof.csv\"), index=False)\n\n# get oof score \ny_true = train[Config.target_col]\ny_pred = oof_df.mean(axis=1)\n\noof_score = metrics(y_true, y_pred, weight=train[\"u_out\"] == 0)\nlogger.info(f\"{Config.name_v1} score:{oof_score:.4f}\")\n\nfig = plot_regression_result(y_true, y_pred, directory=EXP_FIG)\nfig.savefig(os.path.join(EXP_FIG, \"regression_result.png\"), dpi=300)\n\n# inference\nprint(\"# ============= # Inference # ============= #\")\npreds_df = pd.DataFrame()\nfor seed in Config.seeds:\n    name = f\"{Config.name_v1}-{seed}\"\n    preds = predict_cv_v1(\n        test_x,\n         model=LGBM(),\n        name=name, \n        directory=EXP_MODEL\n    )\n    preds_df[name] = preds\n\npreds_df.to_csv(os.path.join(EXP_PREDS, \"preds.csv\"), index=False)\ntest_pred = preds_df.mean(axis=1)\n\nsample_submission['pressure'] = test_pred\nsample_submission.to_csv('submission.csv', index=False) ","fbd4e53e":"## Config","e5b2eac7":"## Main","f8e46eae":"## Load Data","4ea606f1":"## Feature Enginnering","67435e7f":"## Model","d75cdc29":"## Funcs","757eaf6c":"## Simple EDA","80d36b9a":"## Library","1c8231c2":"## SetUp"}}