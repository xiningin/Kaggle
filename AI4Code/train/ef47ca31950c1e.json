{"cell_type":{"c5f105e2":"code","a6afd044":"code","58f1a411":"code","cba89e1c":"code","841987d5":"code","cfa4b1eb":"code","4ed96daa":"code","1773a2cc":"code","33db483a":"code","fa4b4735":"code","d43c36cd":"code","6ac40301":"code","e5ee9463":"code","fd895d6c":"code","a286af1c":"code","3ad9dff7":"code","e0203141":"code","31daad9e":"code","48d7c3bd":"code","6d5eee41":"code","8c75bab8":"code","14fbb802":"code","915af970":"code","1322d589":"code","2faff94d":"code","50f27a04":"code","4db98f58":"code","5d317012":"code","1cec3722":"code","ac3f42b8":"code","8c68a0c1":"code","edc3c8c9":"code","553103fb":"code","4b2a01f3":"code","3e974f63":"code","7ae0b4c4":"code","85cc4148":"code","b16d9aab":"code","528a6329":"code","58f86516":"code","d05fad6f":"code","f1106c81":"code","38314324":"code","b95ae898":"code","8e30cb8f":"code","8fabb01b":"code","c485edd4":"code","4ecde1c7":"code","d5bc0f4e":"code","7c954350":"code","8b6d156b":"code","21952f50":"code","f21c382b":"markdown","8a3704be":"markdown","dcc2bac4":"markdown","178fae63":"markdown","13412caf":"markdown","1efdd086":"markdown","1a89a07b":"markdown","751b1647":"markdown","d1eba651":"markdown","006815b0":"markdown","4dcc8c44":"markdown","61501e06":"markdown","1616256e":"markdown","9d40a55a":"markdown","edb8f844":"markdown","a9f63247":"markdown","88dac720":"markdown","a9252e8f":"markdown","131eb9fa":"markdown","9be04be6":"markdown","571b40aa":"markdown","432e68bf":"markdown","8bd1702a":"markdown","3a5d485d":"markdown","ce9ce0cc":"markdown","2c82d57b":"markdown","c658a209":"markdown","9f8bc055":"markdown","fb9b4285":"markdown","2c5e5079":"markdown","2e840457":"markdown","f881983a":"markdown","f8a51399":"markdown"},"source":{"c5f105e2":"import numpy as np\nimport pandas as pd\n\n%matplotlib inline\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom pandas.plotting import scatter_matrix\n\nimport os\nimport urllib.request\nimport matplotlib.image as mpimg\n\nfrom sklearn.impute import SimpleImputer\nfrom sklearn. preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\n\nfrom sklearn.model_selection import GridSearchCV, cross_val_score, StratifiedShuffleSplit\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor, RandomForestClassifier\n\nfrom sklearn.metrics import mean_squared_error\nfrom scipy import stats","a6afd044":"ho = pd.read_csv('..\/input\/california-housing-prices\/housing.csv')\nho.head()","58f1a411":"ho.columns","cba89e1c":"ho['income_cat'] = pd.cut(ho['median_income'], bins =[0., 1.5, 3.0, 4.5, 6., np.inf],\n                          labels = [1,2,3,4,5])\n\nsplit = StratifiedShuffleSplit(n_splits = 1,\n                              test_size = 0.2, \n                              random_state=42)\n\nfor train_index, test_index in split.split(ho,ho['income_cat']):\n    strat_train_set = ho.loc[train_index]\n    strat_test_set = ho.loc[test_index]\n\n# check: let'see the proportions in the test set vs the whole dataframe\nprint('Stratified test set:\\n',strat_test_set['income_cat'].value_counts()\/ len (strat_test_set),'\\n')\nprint('Whole dataframe:\\n', ho['income_cat'].value_counts()\/ len (ho),'\\n')\n# they agree :)\n\n# now we remove the income_cat attribute\nfor set_ in (strat_train_set, strat_test_set): \n    set_.drop('income_cat', axis=1, inplace=True)\n    \nho = strat_train_set.copy()","841987d5":"ho.info() ","cfa4b1eb":"ho.describe()","4ed96daa":"ho[['housing_median_age', 'median_house_value']].hist(bins= 100, figsize = (20,5))\nplt.show()","1773a2cc":"ho.plot(kind='scatter', x='longitude', y='latitude', alpha= 0.1) \nplt.show()","33db483a":"DOWNLOAD_ROOT = \"https:\/\/raw.githubusercontent.com\/ageron\/handson-ml2\/master\/\"\nHOUSING_PATH = os.path.join(\"datasets\", \"housing\")\nHOUSING_URL = DOWNLOAD_ROOT + \"datasets\/housing\/housing.tgz\"\n\nmatplotlib.rc('axes', labelsize=14)\nmatplotlib.rc('xtick', labelsize=12)\nmatplotlib.rc('ytick', labelsize=12)\n\n# Download the California image\nimages_path = os.path.join('.', \"images\", \"end_to_end_project\")\nos.makedirs(images_path, exist_ok=True)\nDOWNLOAD_ROOT = \"https:\/\/raw.githubusercontent.com\/ageron\/handson-ml2\/master\/\"\nfilename = \"california.png\"\nurl = DOWNLOAD_ROOT + \"images\/end_to_end_project\/\" + filename\nurllib.request.urlretrieve(url, os.path.join(images_path, filename))\ncalifornia_img=mpimg.imread(os.path.join(images_path, filename))\n\n# function that plots the attribute attr of the datafreame ho over the California map\ndef California(ho, attr):\n    ax = ho.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", figsize=(10,7),\n                      s=ho['population']\/100, label=\"Population\",\n                      c=attr, cmap=plt.get_cmap(\"jet\"),\n                      colorbar=False, alpha=0.4)\n    plt.imshow(california_img, extent=[-124.55, -113.80, 32.45, 42.05], alpha=0.5,\n               cmap=plt.get_cmap(\"jet\"))\n    plt.ylabel(\"Latitude\", fontsize=14)\n    plt.xlabel(\"Longitude\", fontsize=14)\n\n    prices = ho[attr]\n    tick_values = np.linspace(prices.min(), prices.max(), 11)\n    cbar = plt.colorbar(ticks=tick_values\/prices.max())\n    cbar.ax.set_yticklabels([(round(v\/1000)) for v in tick_values], fontsize=14)\n    cbar.set_label(attr, fontsize=16)\n\n    plt.legend(fontsize=16)\n    plt.show()\n    \nCalifornia(ho, 'median_house_value')","fa4b4735":"x = ho['total_bedrooms']\n\nplt.figure(figsize=(8,5))\nplt.hist(x, bins=50, color='c', edgecolor='k', alpha=0.65)\nplt.axvline(x.mean(), color='k', linestyle='dashed', linewidth=2, label='Mean: {:.2f}'.format(x.mean()))\nplt.axvline(x.median(), color='r', linestyle='dashed', linewidth=2, label='Median: {:.2f}'.format(x.median()))\nplt.xlabel('total_bedrooms')\nplt.ylabel('frequency')\nplt.legend()\nmin_ylim, max_ylim = plt.ylim()","d43c36cd":"corr_matrix= ho.corr()\n\ncorr_matrix['median_house_value'].sort_values(ascending=False)","6ac40301":"ho.plot(kind= 'scatter', x= 'median_income', y='median_house_value', alpha= 0.1)\nplt.show()","e5ee9463":"ho[ho['median_house_value']>= 500001.0].describe()","fd895d6c":"labels =  ['Others','500001 USD']\nexp = (0,0.1)\nfig1, ax1 = plt.subplots()\n#[100-96500\/16512,96500\/16512]\nplt.pie([15547, 965], labels=labels, shadow= True,explode= exp , normalize= True, startangle= 90)\nplt.show()","a286af1c":"expensive_ho = ho[ho['median_house_value']== 500001.0]\nCalifornia(expensive_ho, 'median_house_value')","3ad9dff7":"oc = ho.ocean_proximity.value_counts()\nsns.barplot(data={'Ocean Proximity': oc,\n                  'feature':oc.index},y='feature',x='Ocean Proximity')\nplt.show()","e0203141":"plt.figure(figsize=(10,6))\nsns.boxplot(data=ho, x='ocean_proximity',y='median_house_value',palette='viridis')\nplt.show()","31daad9e":"oc = ho[['ocean_proximity', 'median_house_value']].copy()\n\nocean = {'<1H OCEAN': 0, 'INLAND': 1, 'NEAR OCEAN': 0, 'NEAR BAY': 0, 'ISLAND': 0}\noc['ocean_proximity'] = oc['ocean_proximity'].map(ocean)\noc.corr()['ocean_proximity']['median_house_value']","48d7c3bd":"ho['rooms_per_hausehold'] = ho['total_rooms']\/ho['households']\nho['bedrooms_per_room'] = ho['total_bedrooms']\/ho['total_rooms']\n\nho['population_per_household'] = ho['population']\/ho['households']\nho['household_per_population']=ho['households']\/ho['population']\n\nho['age_households'] = ho['housing_median_age']*ho['households']\n\nho['income_per_population']=ho['median_income']\/(ho['population']-ho['households'])\n\n# select INLAND:\n\n# non inlands to 0\nnon_inland_ind = ho.loc[ho['ocean_proximity'] != 'INLAND'].index\nho['INLAND']= ho['ocean_proximity']\nho.loc[non_inland_ind,'INLAND'] = np.zeros(len(non_inland_ind))\n# inland to 1\ninland_ind = ho.loc[ho['ocean_proximity'] == 'INLAND'].index\nho.loc[inland_ind,'INLAND'] = np.ones(len(inland_ind))\nho['INLAND']= ho['INLAND'].astype(int)\n\n# let's see how they behave\n\ncorr_matrix = ho.corr()\ncorr_matrix['median_house_value'].sort_values(ascending=False)","6d5eee41":"# back to the original dataset\n\nho_train = strat_train_set.drop('median_house_value', axis=1)\nho_labels = strat_train_set['median_house_value'].copy()","8c75bab8":"# let us create a transformer that adds the new attributes, so that we can use a pipeline\n\n# get the column indices\ncol_names = 'total_rooms', 'total_bedrooms', 'population', 'households', 'housing_median_age', 'median_income'\nrooms_ix, bedrooms_ix, population_ix, households_ix, age_ix, income_ix = [ho_train.columns.get_loc(c) for c in col_names] \n\n\nclass CombinedAttributesAdder(BaseEstimator, TransformerMixin):\n    # BaseEstimator gets the extra methods get_params() and set_params()\n    # TransformerMixin gives the fit_transform() method for free\n    \n    def __init__(self, add_bedrooms_per_room=True): \n        self.add_bedrooms_per_room = add_bedrooms_per_room\n        \n    def fit(self, X, y=None):\n        return self  \n    \n    def transform(self, X):\n        rooms_per_household = X[:, rooms_ix] \/ X[:, households_ix]\n        pop_per_household = X[:, population_ix] \/ X[:, households_ix]\n        income_per_pop = X[:, income_ix] \/ (X[:, population_ix] - X[:, households_ix])\n        household_per_population =  X[:, households_ix] \/ X[:, population_ix] \n        age_cat = X[:, age_ix] * X[:, households_ix]\n        if self.add_bedrooms_per_room:\n            bedrooms_per_room = X[:, bedrooms_ix] \/ X[:, rooms_ix]\n            return np.c_[X, rooms_per_household, pop_per_household,\n                         age_cat, bedrooms_per_room, income_per_pop, household_per_population]\n            # np.c_ translates slice objects to concatenation along the second axis.\n        else:\n            return np.c_[X, rooms_per_household, pop_per_household, age_cat, income_per_pop , household_per_population]\n\nattr_adder = CombinedAttributesAdder(add_bedrooms_per_room=True)\nho_extra_attribs = attr_adder.transform(ho_train.values)\n\n# ho_extra_attribs is a NumPy array, we've lost the column names. To recover a DataFrame, run this:\n\nho_extra_attribs = pd.DataFrame(\n    ho_extra_attribs,\n    columns=list(ho_train.columns)+['rooms_per_household', 'pop_per_household', \n                                    'age_households', 'bedrooms_per_room', 'income_per_pop', 'household_per_population'],\n    index=ho_train.index)\n\nho_extra_attribs.head()","14fbb802":"# pipeline for numerical attributes:\n\nho_num = ho_train.drop('ocean_proximity', axis=1)\nho_cat = ho_train[['ocean_proximity']]\n\nnum_pipeline= Pipeline([\n        ('imputer', SimpleImputer(strategy= 'median')),  # fills NaN with median\n        ('attribs_adder', CombinedAttributesAdder()),    # adds new attributes \n        ('scaler', StandardScaler()),                    # scales everything\n    ])\n\nho_num_tra = num_pipeline.fit_transform(ho_num)","915af970":"# full pipeline:\n\nfull_pipeline = ColumnTransformer([\n            ('num', num_pipeline, list(ho_num)), \n            ('cat', OneHotEncoder(), ['ocean_proximity']), # encodes ocean_proximity\n        ])\n  \nho_ready = full_pipeline.fit_transform(ho_train)","1322d589":"# useful functions:\n\ndef display_scores(scores):\n    #print('Scores: ', scores)\n    print('Mean: ', scores.mean())\n    print('Scores std: ', scores.std())\n    \ndef scoring_cv(regr):\n    regr.fit(ho_ready, ho_labels)\n    scores = cross_val_score(regr, ho_ready, ho_labels, scoring ='neg_mean_squared_error', cv = 10) \n    rmse_scores = np.sqrt(-scores)\n    display_scores(rmse_scores)\n    # return rmse_scores\n\ndef GridSearch_best(regr, param_grid):\n    grid_search = GridSearchCV(regr, param_grid, cv = 5, scoring = 'neg_mean_squared_error', \n                            return_train_score=True)\n    grid_search.fit(ho_ready,ho_labels)\n    return grid_search\n    \n# Create the list of columns of the final dataframe ho_ready:\n\nnum_attr = list(ho_num)\nextra_attr = ['rooms_per_household', 'pop_per_household', 'age_cat', 'bedrooms_per_room',\n              'income_per_pop', 'household_per_population']\ncat_encoder= full_pipeline.named_transformers_['cat']\ncat_one_hot_attr = list(cat_encoder.categories_[0])\nattrib = num_attr + extra_attr + cat_one_hot_attr","2faff94d":"lin_reg = LinearRegression()\nscoring_cv(lin_reg)","50f27a04":"# let's have a look at the coefficients and intercept\nlin_reg = LinearRegression()\nlin_reg.fit(ho_ready, ho_labels)\n\nprint('coefficients:\\n')\nfor coef, col in zip(lin_reg.coef_, attrib):\n    print(coef, col)\n\nprint('\\nintercept:\\n', lin_reg.intercept_)","4db98f58":"# plot the regressor line on median_income\n\nx= ho_train['median_income']\ny=ho_labels #['median_house_value']\n\nplt.plot(x, y, 'o', alpha = 0.1)\n\nm, b = np.polyfit(x, y, 1)\n\nm_1 = lin_reg.coef_[7]\nb_1 = lin_reg.intercept_\n\nplt.plot(x, m*x + b)\n\nplt.plot(x, m_1*x + b_1)\n\nplt.xlabel('median_income')\nplt.ylabel('median_house_value')\n\n# the linear regressor looks very much wrong.... \n# the problem is the huge amount of houses with 500k value","5d317012":"# let's try to remove the houses with median_house_value = 500001.0:\n\nfull_pipeline_modif = ColumnTransformer([\n            ('num', num_pipeline, num_attr), \n            ('cat', OneHotEncoder(), ['ocean_proximity']),\n        ])\n\nho_modif = strat_train_set[strat_train_set['median_house_value']!= 500001.0]\nprint('I loose ' , len(strat_train_set)-len(ho_modif), ' train entries out of ', len(ho))\n\nho_labels_modif = ho_modif['median_house_value']\nho_train_modif = ho_modif.drop(['median_house_value'], axis = 1)\n\nho_ready_modif = full_pipeline_modif.fit_transform(ho_train_modif)","1cec3722":"lin_reg_modif = LinearRegression()\nlin_reg_modif.fit(ho_ready_modif, ho_labels_modif)\nho_lin_pred_modif = lin_reg_modif.predict(ho_ready_modif)\nnp.sqrt(mean_squared_error(ho_labels_modif, ho_lin_pred_modif))","ac3f42b8":"forest = RandomForestRegressor()\nscoring_cv(forest)","8c68a0c1":"feat_f = forest.feature_importances_\nplt.figure(figsize=(12,8))\nplt.title('Feature Importance')\n\nsns.barplot(data={'importance': feat_f,\n                  'feature':attrib},y='feature',x='importance')","edc3c8c9":"# ! this cell takes a LOT for running\n\nparam_grid = { \n            'n_estimators': [ 150 , 500, 700 ],\n            'max_features': ['auto', 'sqrt', 5, 8],\n            'bootstrap': [True, False],\n            'max_depth':[50,70,90]\n            }\n\n\nforest_reg = RandomForestRegressor()\n\n# grid_search = GridSearch_best(forest_reg, param_grid)\n# grid_search.best_params_\n\n# OUTPUT: \n# {'bootstrap': False, 'max_depth': 50, 'max_features': 8, 'n_estimators': 700}","553103fb":"# max_depth is the lowest possible, maybe a smaller one fits better?\n# same for max_features\n\nparam_grid = { \n            'n_estimators': [700 ],\n            'max_features': [8, 10],\n            'bootstrap': [False],\n            'max_depth':[30,40,50]\n            }\n\n\nforest_reg_s = RandomForestRegressor()\n\n# grid_search_s = GridSearch_best(forest_reg_s, param_grid)\n# grid_search_s.best_params_\n\n# OUTPUT:\n# {'bootstrap': False, 'max_depth': 40, 'max_features': 10, 'n_estimators': 700}","4b2a01f3":"# scoring_cv(grid_search_s.best_estimator_)\n\n# OUTPUTS: \n# grid_search: 49815.34754567745\n# grid_search_s: 49896.07230279817 \n\n# did not change much, but the (longer) time... ","3e974f63":"forest_reg = RandomForestRegressor(bootstrap= False, max_depth= 40, max_features= 8, n_estimators= 700)\n\nforest_reg.fit(ho_ready, ho_labels)","7ae0b4c4":"feat = forest_reg.feature_importances_\nsorted(zip(feat, attrib), reverse = True)","85cc4148":"least_feat = ['total_rooms','total_bedrooms', 'households','population','<1H OCEAN','NEAR OCEAN', 'NEAR BAY','ISLAND']\n\nplt.figure(figsize=(12,8))\nplt.title('Feature Importance')\nsns.barplot(data={'importance': feat,\n                  'feature':attrib},y='feature',x='importance')","b16d9aab":"# create a new transformer that selects only INLAND, and encodes it too\n\nclass InlandSelector(BaseEstimator, TransformerMixin):\n    # BaseEstimator gets the extra methods get_params() and set_params()\n    # TransformerMixin gives the fit_transform() method for free\n    \n    def __init__(self): # no *args or **kargs\n        print('')\n        \n    def fit(self, X, y=None):\n        return self  # nothing else to do\n    \n    def transform(self, X):\n        # non inlands to 0\n        non_inland_ind = X.loc[X['ocean_proximity'] != 'INLAND'].index\n        X.loc[non_inland_ind,'ocean_proximity'] = np.zeros(len(non_inland_ind))\n        # inland to 1\n        inland_ind = X.loc[X['ocean_proximity'] == 'INLAND'].index\n        X.loc[inland_ind,'ocean_proximity'] = np.ones(len(inland_ind))\n        return X","528a6329":"full_pipeline_for_real = ColumnTransformer([\n            ('num', num_pipeline, num_attr), \n            ('sel',InlandSelector(),['ocean_proximity']), \n        ])\n\nho_train = strat_train_set.drop('median_house_value', axis=1)\nho_labels = strat_train_set['median_house_value'].copy()\n\nho_ready_for_real = full_pipeline_for_real.fit_transform(ho_train)\n\nattr_ready = ['longitude', 'latitude', 'housing_median_age', 'total_rooms', 'total_bedrooms',\n                 'population', 'households','median_income','rooms_per_household', 'pop_per_household', \n            'age_cat', 'bedrooms_per_room', 'income_per_pop', 'household_per_population', 'ocean_proximity']\nho_ready_for_real = pd.DataFrame(\n    ho_ready_for_real,\n    columns= attr_ready, \n    index=ho_train.index)\nho_ready_for_real.drop(['total_rooms','total_bedrooms', 'households','population'], axis = 1, inplace = True)\nho_ready_for_real.head()","58f86516":"final_model = RandomForestRegressor(bootstrap= False, max_depth= 40, max_features= 8, n_estimators= 700)\nfinal_model.fit(ho_ready, ho_labels)\n\nX_test = strat_test_set.drop('median_house_value', axis =1)\ny_test = strat_test_set['median_house_value'].copy()","d05fad6f":"X_test_ready = full_pipeline.transform(X_test)\n\nfinal_mse_train = mean_squared_error(ho_labels, final_model.predict(ho_ready))\nfinal_rmse_train = np.sqrt(final_mse_train)\n\nfinal_predictions= final_model.predict(X_test_ready)\nfinal_mse = mean_squared_error(y_test, final_predictions)\nfinal_rmse = np.sqrt(final_mse)\n\nprint('RMSE train: ', final_mse_train)\nprint('RMSE test: ', final_rmse)\n\n#with feature selection:\n#RMSE train:  0.011411092214658875\n#RMSE test:  48753.10364735771","f1106c81":"# compute a 95% confidence interval for the error:\n\nconfidence = 0.95\n\nsquared_errors= (final_predictions- y_test)**2\nnp.sqrt(stats.t.interval(confidence, len(squared_errors)-1,\n                        loc= squared_errors.mean(),\n                        scale= stats.sem(squared_errors)))","38314324":"ho = pd.read_csv('..\/input\/california-housing-prices\/housing.csv')\n\nho = ho[ho['median_house_value']!= 500001.0].copy()\nho['income_cat'] = pd.cut(ho['median_income'], bins =[0., 1.5, 3.0, 4.5, 6., np.inf],\n                          labels = [1,2,3,4,5])\n\nsplit = StratifiedShuffleSplit(n_splits = 1,\n                              test_size = 0.2, \n                              random_state=42)\n\nfor train_index, test_index in split.split(ho,ho['income_cat']):\n    strat_train_set = ho.iloc[train_index]\n    strat_test_set = ho.iloc[test_index]\n\nho.hist(bins = 100, column = 'median_house_value')","b95ae898":"ho_train = strat_train_set.drop('median_house_value', axis=1)\nho_labels = strat_train_set['median_house_value'].copy()\n\nfull_pipeline_select = ColumnTransformer([\n            ('num', num_pipeline, list(ho_num)), \n            ('cat', OneHotEncoder(), ['ocean_proximity']), \n        ])\n  \nho_ready = full_pipeline_select.fit_transform(ho_train)","8e30cb8f":"model_selected = RandomForestRegressor(bootstrap= False, max_depth= 40, max_features= 8, n_estimators= 700)\nmodel_selected.fit(ho_ready, ho_labels)\n\nX_test = strat_test_set.drop('median_house_value', axis =1)\ny_test = strat_test_set['median_house_value'].copy()","8fabb01b":"X_test_ready = full_pipeline_select.transform(X_test)\nfinal_mse_train = mean_squared_error(ho_labels, model_selected.predict(ho_ready))\nfinal_rmse_train = np.sqrt(final_mse_train)\n\nfinal_predictions= model_selected.predict(X_test_ready)\nfinal_mse = mean_squared_error(y_test, final_predictions)\nfinal_rmse = np.sqrt(final_mse)\n\nprint('RMSE train: ', final_mse_train)\nprint('RMSE test: ', final_rmse)","c485edd4":"squared_errors= (final_predictions- y_test)**2\nnp.sqrt(stats.t.interval(confidence, len(squared_errors)-1,\n                        loc= squared_errors.mean(),\n                        scale= stats.sem(squared_errors)))","4ecde1c7":"ho = pd.read_csv('..\/input\/california-housing-prices\/housing.csv')\n\nho['median_income'] = pd.cut(ho['median_income'], bins =[0., 1.5, 3.0, 4.5, 6., np.inf],\n                          labels = [1,2,3,4,5])\nho['median_house_value'] = pd.qcut(ho['median_house_value'], 3,\n                          labels = [1,2,3]).astype(int)\n\nsplit = StratifiedShuffleSplit(n_splits = 1,\n                              test_size = 0.2, \n                              random_state=42)\n\nfor train_index, test_index in split.split(ho,ho['median_income']):\n    strat_train_set = ho.iloc[train_index]\n    strat_test_set = ho.iloc[test_index]\n","d5bc0f4e":"ho_train = strat_train_set.drop('median_house_value', axis=1)\nho_labels = strat_train_set['median_house_value'].copy()\n\nfull_pipeline_select = ColumnTransformer([\n            ('num', num_pipeline, list(ho_num)), \n            ('cat', OneHotEncoder(), ['ocean_proximity']), \n        ])\n  \nho_ready = full_pipeline_select.fit_transform(ho_train)","7c954350":"model_selected = RandomForestClassifier()\nmodel_selected.fit(ho_ready, ho_labels)\n\nX_test = strat_test_set.drop('median_house_value', axis =1)\ny_test = strat_test_set['median_house_value'].copy()","8b6d156b":"plt.figure(figsize=(12,8))\nplt.title('Feature Importance')\nsns.barplot(data={'importance': model_selected.feature_importances_,\n                  'feature':attrib},y='feature',x='importance')","21952f50":"X_test_ready = full_pipeline_select.transform(X_test)\n\n\nprint('Score train: ', model_selected.score(ho_ready, ho_labels))\nprint('Score test: ', model_selected.score(X_test_ready, y_test))\n","f21c382b":"Hello there! In this notebook we're going to have a look at the dataset about California housing prices. After a quick look at the data, we'll start by cleaning the dataset and find some nice visualizations. Our final goal is implementing a learning machine to predict the median house value from the other columns of the dataset.","8a3704be":"# The data analysis: conclusions\n\nThere are 9 attributes to use to predict the median_house_value:\n\n- **median_house_value**: our target. It has been cut to 500001.0, and this will affect our performance. I don't see good solutions to perform better, other that bin it and predict a price range instead\n\n- **longitude and latitude**: represent California. Clearly we can see houses in San Francisco and the are Los angeles-San DIego are more expensive. Also, there is negative correlation with latitude: if I go north, the houses get cheaper. In a next analysis I'd change the axis to have the California \"rectangle\" vertical.\n\n- **median_income**: the attribute with higher correlation with the house price: 0.687! Indeed we see a good linear dependence, but the LinearRegressor might not perform well because of the cut of median_house_value\n\n- **ocean_proximity**: most of the houses are near to the ocean, but island doesn't look relevant. I noticed that inland is the one that stands out when looking at the median_house_value, and indeed has a very good correlation with our target\n\n- **housing_median_age**: has high variance and has been cut at 52\n\n- **total_rooms**: 0.135 of correlation with the median_house_value, we'll combine it with other attributes to see if we can get better\n\n- **total_bedrooms**: the only attribute with null values. As there are many outliers, I'll fill with the median.\n\n- **population, households**: don't help much alone, but we can consider\n\nFinally, creating new attributes helps:\n\n- **rooms_per_hausehold** is more informative than total_rooms or households\n- **bedrooms_per_room** has a good correlation with median_house_value\n- **inland** looks also promising, as we observed before\n- population_per_household a bit less, but somehow its inverse **household_per_population** is more informative\n\nWe have enough information to feed the machine!","dcc2bac4":"## Alternative #2: bin the labels","178fae63":"## Select and train the model","13412caf":"The data looks quite sparse, I think the median represents it better. We'll fill the null values later in a pipeline.","1efdd086":"Note again that all the prices above 500k have ben set to 500001 USD. This is represented by the horizontal line in the top of the plot.","1a89a07b":"#### Let's look at the geographical data:","751b1647":"The dataframe has 10 columns:\n\n* **longitude** and **latitude**: Measure of the location of the house\n* **housing_median_age**: Median age of a house *within a block*; a lower number is a newer building\n* **total_rooms** and **total_bedrooms**: Total number of rooms\/bedrooms *within a block*\n* **population** and **households**: Total number of people\/households (a group of people residing within a home unit) residing *within a block*\n* **median_income**: Median income for households *within a block* of houses (measured in tens of thousands USD)\n* **median_house_value**: Median house value for households *within a block* (measured in USD)\n* **ocean_proximity**: Location of the house with respect to ocean\/sea\n","d1eba651":"#### How is ocean_proximity relevant?","006815b0":" Here we can see where are these expensive houses: in the San Francisco bay and in the Los Angeles area.","4dcc8c44":"...it looks like California!! :) Let's make a nicer plot:","61501e06":"Nice, but overfits. After some further study (tried Ridge and Lasso, changed parameters..) it still doesn't perform well.","1616256e":"#### Looking for correlations","9d40a55a":"The average median_house_value is 206990 USD. Unfortunately, there is a high variance: the standard deviation is 115703 USD. This will make it harder to make a good prediction. Also, the maximal median_house_value is 500001 USD and many houses belong to the 75 quantile: this means that many houses have a high value, and probably all the houses above 500001 USD have been set to this value.","edb8f844":"Note that there are null values only in the total_bedrooms column \nand ocean_proximity is the only categorical variable.","a9f63247":"First things first: let's create a test set so we're not biased during the data analysis phase!\nTo do so, we make an assumption: suppose that the median income is a very important factor to predict median housing prices. Thus\nwe perform a stratified sampling based on this information: we create a test set which has the same proportions of median incomes as the whole dataframe.","88dac720":"We'll probably drop island because its frequency is irrelevant. Inland is the one that seems to \"detect\" a median house value, but there are many outliers. Let's see if there are correlations with the median_house_prices:","a9252e8f":"### Creating new attributes","131eb9fa":"## A look at the data","9be04be6":"Indeed, by the above histograms we deduce that both housing_median_age and median_house_value have been cut.","571b40aa":"If we look only at the houses with top price we see that they are the 5.8% of the dataset: a small but significant slice of the dataset.\nOf course, they correspond to higher median_income. A solution of this issue could also be to bin the median_house_value and predict a range of price, but it would change the problem.","432e68bf":"#### How can we fill null values of total_bedrooms?","8bd1702a":"#### feature importance","3a5d485d":"## Prepare Data for ML\n\nTo-do list:\n\n1) Fill NaN in total_bedrooms\n\n2) Add the new attributes\n\n3) encode ocean_proximity\n\n4) Scale the num attributes","ce9ce0cc":"## Alternative #1: remove the outliers","2c82d57b":"#### Linear regression","c658a209":"# Time to make a decision! The final model:","9f8bc055":"#### create new pipeline that selects most important features","fb9b4285":"#### Random Forest","2c5e5079":"Above we have the correlations of all the numerical features with median_house_value. Note that there is a negative correlation with latitude, meaning that prices go down if I go north! Of course there is also a high correlation with the median_income, but no surprises here. Let's zoom in the relations of the median_house_value with median_income.","2e840457":"# Final remarks:\n\nThe performances are improved with feature engineering and selecting a good regressor and tuning the parameters.\nThe 95% confidence interval for the error is [45599, 49517].\nStill, on average the prediction is 47598 USD far from the real value (recall that the average house value is 206855 USD).\n\nThe error is medium-high, but the reason for that is the cut of the median_house_value at 500001.\nTo improve the performance there could be two ways:\n\n**Alternative 1**: remove the outliers (the houses with median_house_value = 500001). In this way the RMSE is actually lower (43730 USD), but we removed part of the test set too! Anyway, this means that if we can collect more data the performance would improve.\n\n**Alternative 2**: I discretized the median_house_value variable into 3 equal-sized buckets based on the 3 quantile. The scoring is .7933 on the test set and .99 on the train. It clearly overfits and has a margin of improvement","f881983a":"Correlation of -0.482886 with 'INLAND', pretty high! \n We'll keep an eye on this during ML","f8a51399":"#### Gridsearch for best parameters"}}