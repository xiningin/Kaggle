{"cell_type":{"9d18b748":"code","8b0e6529":"code","2b066c2d":"code","b45e2ed5":"code","48b3436e":"code","e974f2d4":"code","023f32a6":"code","bd7e97c6":"code","7ae6b464":"code","0051547f":"code","9d0d49d0":"code","b1c4fdb4":"code","66d7866f":"code","39c0cf93":"code","d74badba":"code","3fa37686":"code","cf76eb05":"code","1b40d9e8":"code","38c6e40e":"code","b9ca6452":"code","36308ffd":"code","416a193a":"code","9b0a2131":"code","01c7314b":"code","a1150109":"code","826ca38e":"code","8253bfd5":"code","6e3345ca":"code","778da5c2":"code","65f99a97":"code","31c83186":"code","116a5179":"code","4b3c8d94":"code","322c202e":"code","cb9d8e8b":"code","3a8464c7":"code","9e17fbd4":"code","84762a8b":"code","00687b20":"code","1188f57b":"code","51f68767":"code","9590180e":"code","061f8828":"code","23548fd5":"code","7e86729f":"code","178c5115":"code","9feb1829":"code","c4a713cf":"code","50fdee20":"code","d38c25cd":"code","762e3aa2":"code","b5eec536":"code","4f164a21":"code","0a967583":"code","4f66b183":"code","ea9c8e64":"code","077467b6":"code","6292f38c":"code","c138d8d6":"code","4fbdb990":"code","e3a5cbec":"code","c712805f":"code","dbc32a0c":"code","99c98890":"code","d0002adc":"code","63de2724":"code","95c3a540":"code","661376c7":"markdown","35b7a233":"markdown","d259f19b":"markdown","137c9e29":"markdown","aa1b5f60":"markdown","c6c731e3":"markdown","f55f1a4a":"markdown","4d94dadd":"markdown","91bf85fc":"markdown","a27bb7f4":"markdown","e2a686a3":"markdown","be4e3e81":"markdown","60798ab6":"markdown","530c6366":"markdown","c0ae0210":"markdown","4a434535":"markdown","5b916d60":"markdown","e214292c":"markdown","7b80b301":"markdown","51e32e8d":"markdown","b6b8a82b":"markdown","5d9f2a33":"markdown","642d9553":"markdown","b31e3dee":"markdown","518f9105":"markdown","915f43df":"markdown","00eae252":"markdown"},"source":{"9d18b748":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","8b0e6529":"!curl https:\/\/raw.githubusercontent.com\/automl\/auto-sklearn\/master\/requirements.txt | xargs -n 1 -L 1 pip install\n!pip install auto-sklearn","2b066c2d":"!pip install dtreeviz\nfrom dtreeviz.trees import dtreeviz","b45e2ed5":"\n!pip install -U rfpimp\n!pip install -U imbalanced-learn\n%matplotlib inline\n#importing libraries\nimport numpy as np\nimport scipy.stats\nimport scipy.special\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport matplotlib.mlab as mlab\nfrom matplotlib import cm\nimport pandas as pd\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import log_loss\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.pipeline import make_pipeline, make_union, Pipeline\nfrom sklearn.preprocessing import StandardScaler, OrdinalEncoder\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.model_selection import ParameterGrid\nfrom keras.models import Sequential\nfrom keras.models import Model as KerasModel\nfrom keras.layers import Input, Dense, Activation, Reshape\nfrom keras.layers import Concatenate\nfrom keras.layers.embeddings import Embedding\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn import linear_model\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.inspection import permutation_importance\nimport pickle\nimport csv\nimport collections\nfrom rfpimp import *\nfrom rfpimp import plot_corr_heatmap\nfrom datetime import datetime\nfrom sklearn import preprocessing\nfrom keras.callbacks import ModelCheckpoint\nimport xgboost as xgb\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.metrics import classification_report\nimport seaborn as sns\nimport imblearn\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom sklearn.metrics import make_scorer\nfrom sklearn.metrics import plot_confusion_matrix\nimport math\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import roc_curve, auc\nfrom tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\nfrom tensorflow.keras.layers import Input, LSTM, Dense, Concatenate,GRU,Dropout,LeakyReLU\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.io import FixedLenFeature\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras import callbacks\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import StratifiedShuffleSplit\nfrom sklearn.preprocessing import LabelEncoder,OneHotEncoder, normalize\nimport tensorflow as tf","48b3436e":"training=pd.read_csv(\"\/kaggle\/input\/higgs-boson\/training.zip\")\ntraining.head()","e974f2d4":"testing=pd.read_csv(\"\/kaggle\/input\/higgs-boson\/test.zip\")\ntesting.set_index('EventId',inplace=True)\ntesting.head()","023f32a6":"print(training.shape,testing.shape)","bd7e97c6":"train_dataX=training.drop([\"Weight\",\"Label\"],axis=1)\ntrain_dataX.set_index('EventId',inplace=True)\ntrain_dataY=training['Label']\n","7ae6b464":"train_dataY=pd.factorize(train_dataY)[0]","0051547f":"from IPython.display import display, HTML\n\ndef pretty_print(df):\n    return display( HTML( df.to_html().replace(\"\\\\n\",\"<br>\") ) )\ndef tbl_report(tbl, cols=None, card=10):\n    print(\"Table Shape\", tbl.shape)\n    dtypes = tbl.dtypes\n    nulls = []\n    uniques = []\n    numuniques = []\n    vcs = []\n    for col in dtypes.index:\n        n = tbl[col].isnull().sum()\n        nulls.append(n)\n        strdtcol = str(dtypes[col])\n        #if strdtcol == 'object' or strdtcol[0:3] == 'int' or strdtcol[0:3] == 'int':\n        #print(strdtcol)\n        uniqs = tbl[col].unique()\n        uniquenums = uniqs.shape[0]\n        if uniquenums < card: # low cardinality\n            valcounts = pd.value_counts(tbl[col], dropna=False)\n            vc = \"\\n\".join([\"{}:{}\".format(k,v) for k, v in valcounts.items()])\n        else:\n            vc='HC' # high cardinality\n        uniques.append(uniqs)\n        numuniques.append(uniquenums)\n        vcs.append(vc)\n    nullseries = pd.Series(nulls, index=dtypes.index)\n    uniqueseries = pd.Series(uniques, index=dtypes.index)\n    numuniqueseries = pd.Series(numuniques, index=dtypes.index)\n    vcseries = pd.Series(vcs, index=dtypes.index)\n    df = pd.concat([dtypes, nullseries, uniqueseries, numuniqueseries, vcseries], axis=1)\n    df.columns = ['dtype', 'nulls', 'uniques', 'num_uniques', 'value_counts']\n    if cols:\n        return pretty_print(df[cols])\n    return pretty_print(df)","9d0d49d0":"tbl_report(train_dataX)","b1c4fdb4":"cat_vars=['PRI_jet_num']\ncont_vars=np.array(train_dataX.drop('PRI_jet_num',axis=1).columns)","66d7866f":"cont_vars","39c0cf93":"pd.DataFrame(collections.Counter(train_dataY),index=['0','1']).iloc[0]\n\n","d74badba":"fig,axes=plt.subplots(figsize=(10,8))\nprint(training['Label'].value_counts())\nsns.barplot(x = training['Label'].value_counts().index, y = training['Label'].value_counts().values)\nplt.title('Label counts')\nplt.show()\n","3fa37686":"ncols = 2\nnrows = math.ceil(len(cont_vars)\/ncols)\nfig, axen = plt.subplots(nrows, ncols, figsize = (12, nrows*4))\nfor v, ax in zip(cont_vars, axen.ravel()):\n    sns.histplot(train_dataX[v], ax=ax)\n    ax.set_xscale('log')","cf76eb05":"#viz = plot_corr_heatmap(train_dataX, figsize=(11,10))\n#viz.save('corrheatmap.svg')\n#viz","1b40d9e8":"sns.pairplot(data=train_dataX[cont_vars])","38c6e40e":"D = feature_dependence_matrix(train_dataX)\nviz = plot_dependence_heatmap(D, figsize=(11,10))\nviz","b9ca6452":"def cv_optimize(clf, parameters, X, y, n_jobs=1, n_folds=5, score_func=None):\n    if score_func:\n        print(\"SCORE FUNC\", score_func)\n        gs = GridSearchCV(clf, param_grid=parameters, cv=n_folds, n_jobs=n_jobs, scoring=score_func)\n    else:\n        gs = GridSearchCV(clf, param_grid=parameters, n_jobs=n_jobs, cv=n_folds)\n    gs.fit(X, y)\n    print(\"BEST\", gs.best_params_, gs.best_score_)\n    best = gs.best_estimator_\n    return best\ndef do_classify(clf, parameters, indf,y,score_func, n_folds=5, n_jobs=1):\n    X=indf\n    y=y\n    Xtrain,Xtest,ytrain,ytest=train_test_split(X,y,train_size=0.8,random_state=2017)\n    clf = cv_optimize(clf, parameters, Xtrain, ytrain, n_jobs=n_jobs, n_folds=n_folds, score_func=score_func)\n    clf=clf.fit(Xtrain, ytrain)\n    training_accuracy = clf.score(Xtrain, ytrain)\n    test_accuracy = clf.score(Xtest, ytest)\n    print(\"############# based on standard predict ################\")\n    print(\"Accuracy on training data: %0.2f\" % (training_accuracy))\n    print(\"Accuracy on test data:     %0.2f\" % (test_accuracy))\n    print(confusion_matrix(ytest, clf.predict(Xtest)))\n    print(\"########################################################\")\n    plot_confusion_matrix(clf,Xtest,ytest,cmap=\"Blues\")\n    return clf, Xtrain, ytrain, Xtest,ytest","36308ffd":"# set up standardization\nss = StandardScaler()\n# oe hot encoding\noh = OneHotEncoder()\n# continuous variables need to be standardized\ncont_pipe = Pipeline([(\"scale\", ss)])\n# categorical variables need to be one hot encoded\ncat_pipe = Pipeline([('onehot', oh)])\n# combine both into a transformer\ntransformers = [('cont', cont_pipe, cont_vars), ('cat', cat_pipe, cat_vars)]\n# apply transformer to relevant columns. Nothing will be done for the rest\nct = ColumnTransformer(transformers=transformers, remainder=\"passthrough\")\n# create a pipeline so that we are not leaking data from validation to train in the individual folds\npipe = Pipeline(steps=[('ct', ct), ('model', LogisticRegression(max_iter=10000, penalty='l2'))])\n# in paramgrid we dont use C but use model__C corresponding to the name in the pipeline\nparamgrid = dict(model__C=[1000, 100, 10, 1, 0.1, 0.01, 0.001])\n\nlr,Xtrain,ytrain,Xtest,ytest=do_classify(pipe, paramgrid,train_dataX, \n                              train_dataY, \n                              score_func='roc_auc')","416a193a":"print(classification_report(ytest,lr.predict(Xtest)))","9b0a2131":"over = SMOTE(sampling_strategy='auto', k_neighbors=2)\nunder = RandomUnderSampler(sampling_strategy='auto')\nsteps = [('o', over), ('u', under)]\npipeline = imblearn.pipeline.Pipeline(steps=steps)\nX, y = pipeline.fit_resample(train_dataX,train_dataY)\ncounter = collections.Counter(y)\nprint(counter)","01c7314b":"!pip install dtreeviz\nfrom dtreeviz.trees import dtreeviz","a1150109":"clf=DecisionTreeClassifier(random_state=2017)\nparameters={'max_depth':range(1,9),'min_samples_leaf':range(3,5),'criterion':['gini']}\nclf, Xtrain, ytrain, Xtest,ytest=do_classify(clf, parameters,train_dataX,train_dataY,'roc_auc',n_folds=5,n_jobs=-1)","826ca38e":"print(classification_report(ytest,clf.predict(Xtest)))","8253bfd5":"colors = [None,  # 0 classes\n          None,  # 1 class\n          ['#FFF4E5','#D2E3EF'],# 2 classes\n           ]\nvizA = dtreeviz(clf, train_dataX,train_dataY,\n               feature_names = train_dataX.columns,\n               target_name = 'Label', class_names= ['No','Yes']\n              ,orientation = 'TD',\n               colors={'classes':colors},\n               label_fontsize=14,\n               ticks_fontsize=10,\n               )\nvizA","6e3345ca":"vizA.save('tree.svg')","778da5c2":"\ndef p_importance(model, cols, fi, fistd = 0):\n    return pd.DataFrame({'features':cols, 'importance':fi, 'importance_std': fistd}\n                       ).sort_values('importance', ascending=False)\ndimp=permutation_importance(clf,Xtest,ytest)\nddf=p_importance(clf,list(train_dataX.columns),dimp['importances_mean'],dimp['importances_std']).iloc[:10]\nfig,ax=plt.subplots(figsize=(17,10))\nsns.barplot(data=ddf,x='features',y='importance',label='Decision_importances',ax=ax)\nplt.xticks(rotation='45')\nplt.title(\"Bar plot of Importances for Decision Tree Model\");","65f99a97":"D = D['Dependence'].sort_values(ascending=False)\nD","31c83186":"##taking only those features which are important\nfeatures=['DER_mass_MMC','DER_mass_transverse_met_lep','DER_mass_vis','DER_deltar_tau_lep','PRI_tau_pt','DER_met_phi_centrality','DER_pt_h','PRI_met']","116a5179":"clf=DecisionTreeClassifier(random_state=2017)\nparameters={'max_depth':range(1,9),'min_samples_leaf':range(3,5),'criterion':['gini']}\nclf, Xtrain, ytrain, Xtest,ytest=do_classify(clf, parameters,X[features],y,'roc_auc',n_folds=5,n_jobs=-1)","4b3c8d94":"print(classification_report(ytest,clf.predict(Xtest)))","322c202e":"colors = [None,  # 0 classes\n          None,  # 1 class\n          ['#FFF4E5','#D2E3EF'],# 2 classes\n           ]\nvizb = dtreeviz(clf, Xtest[features],ytest,\n               feature_names = X[features].columns,\n               target_name = 'Label', class_names= ['No','Yes']\n              ,orientation = 'TD',\n               colors={'classes':colors},\n               label_fontsize=14,\n               ticks_fontsize=10,\n               )\nvizb","cb9d8e8b":"vizb.save('tree2.svg')","3a8464c7":"clf2=RandomForestClassifier(random_state=2017)\nparameters={\"n_estimators\":[400],'max_features':[3],'max_depth':[5]}\nclf2, Xtrain, ytrain, Xtest, ytest  = do_classify(clf2, \n   parameters,train_dataX[features],train_dataY,\"roc_auc\", n_folds=5, n_jobs=1)","9e17fbd4":"print(classification_report(ytest,clf2.predict(Xtest)))","84762a8b":"\nrimp=permutation_importance(clf2,Xtest,ytest)\nrdf=p_importance(clf2,features,rimp['importances_mean'],rimp['importances_std'])\nfig,ax=plt.subplots(figsize=(17,10))\nsns.barplot(data=rdf,x='features',y='importance',label='RandomForest_importances',ax=ax)\nplt.xticks(rotation='45')\nplt.title(\"Bar plot of Importances for Random Forest Model\");","00687b20":"clf2=RandomForestClassifier(random_state=2017)\nparameters={\"n_estimators\":[400],'max_features':[3],'max_depth':[5]}\nclf2, Xtrain, ytrain, Xtest, ytest  = do_classify(clf2, \n   parameters,X[features],y,\"roc_auc\", n_folds=5, n_jobs=1)\n","1188f57b":"print(classification_report(ytest,clf2.predict(Xtest)))","51f68767":"Xtrain,Xtest,ytrain,ytest=train_test_split(train_dataX[features],train_dataY,train_size=0.8)\nXtrain.shape,ytrain.shape\ndtrainimb=xgb.DMatrix(Xtrain,label=ytrain)\n\nclf3 = xgb.train(xgb_pars, dtrainimb, 500,\n                  maximize=False, verbose_eval=15) \n","9590180e":"dvalid = xgb.DMatrix(Xtest, label=ytest)\ny_predimb=clf3.predict(dvalid)\ny_predimb_=[1 if y>0.5 else 0 for y in y_predimb]\nprint(classification_report(ytest,y_predimb_))","061f8828":"Xtrain,Xtest,ytrain,ytest=train_test_split(X[features],y,train_size=0.8)\nXtrain.shape,ytrain.shape","23548fd5":"dtrain = xgb.DMatrix(Xtrain, label=ytrain)\ndvalid = xgb.DMatrix(Xtest, label=ytest)\n\nxgb_pars = {'min_child_weight': 100, 'eta': 0.04, 'colsample_bytree': 0.8, 'max_depth': 100,\n             'subsample': 0.75, 'lambda': 2, 'nthread': -1, 'booster' : 'gbtree', 'silent': 1, 'gamma' : 0,\n             'eval_metric': 'mae', 'objective': 'reg:linear'}    \n\nclf3 = xgb.train(xgb_pars, dtrain, 500,\n                  maximize=False, verbose_eval=15) ","7e86729f":"y_pred=clf3.predict(dvalid)","178c5115":"y_pred_=[1 if y>0.5 else 0 for y in y_pred]","9feb1829":"print(classification_report(ytest,y_pred_))","c4a713cf":"\ndef make_roc(name, clf, ytest, xtest, ax=None, labe=5,  proba=True, skip=0, initial = False):\n    if not ax:\n        ax=plt.gca()\n    if proba:\n        fpr, tpr, thresholds=roc_curve(ytest, clf.predict_proba(xtest)[:,1])\n    else:\n        fpr, tpr, thresholds=roc_curve(ytest, clf.decision_function(xtest))\n    roc_auc = auc(fpr, tpr)\n    if skip:\n        l=fpr.shape[0]\n        ax.plot(fpr[0:l:skip], tpr[0:l:skip], '.-', lw=2, alpha=0.4, label='ROC curve for %s (area = %0.2f)' % (name, roc_auc))\n    else:\n        ax.plot(fpr, tpr, '.-', lw=2, alpha=0.4, label='ROC curve for %s (area = %0.2f)' % (name, roc_auc))\n    label_kwargs = {}\n    label_kwargs['bbox'] = dict(\n        boxstyle='round,pad=0.3', alpha=0.2,\n    )\n    for k in range(0, fpr.shape[0],labe):\n        #from https:\/\/gist.github.com\/podshumok\/c1d1c9394335d86255b8\n        threshold = str(np.round(thresholds[k], 2))\n        ax.annotate(threshold, (fpr[k], tpr[k]), **label_kwargs)\n    if initial:\n        ax.plot([0, 1], [0, 1], 'k--')\n        ax.set_xlim([0.0, 1.0])\n        ax.set_ylim([0.0, 1.05])\n        ax.set_xlabel('False Positive Rate')\n        ax.set_ylabel('True Positive Rate')\n        ax.set_title('ROC')\n    ax.legend(loc=\"lower right\")\n    return ax","50fdee20":"fig,ax=plt.subplots(figsize=(25,25))\nxtrain,xtest,ytr,yte=train_test_split(train_dataX,train_dataY,train_size=0.8)\nmake_roc('logistic', lr,yte ,xtest, ax=ax, labe=10,  proba=True, skip=16300, initial = False)\n\n","d38c25cd":"fig,ax=plt.subplots(figsize=(25,25))\nmake_roc('random', clf2,ytest ,Xtest, ax=ax, labe=10,  proba=True, skip=16300, initial = False)","762e3aa2":"fig,ax=plt.subplots(figsize=(25,25))\nmake_roc('decisiontree', clf,ytest ,Xtest, ax=ax, labe=10,  proba=True, skip=0, initial = False)","b5eec536":"AUTO = tf.data.experimental.AUTOTUNE","4f164a21":"train_df=training.drop('Weight',axis=1)","0a967583":"enc = LabelEncoder()\n\ntrain_df['Label'] = enc.fit_transform(train_df['Label'])\ntrain_df.head()","4f66b183":"train_df.set_index(['EventId'],inplace = True)\n","ea9c8e64":"testing.shape,train_df.shape","077467b6":"X = train_df.drop(['Label'], axis=1)\ny = train_df['Label'].values\nX = normalize(X)\ntest_df_nor = normalize(testing)","6292f38c":"#split to maintain the imbalance\nsplitter=StratifiedShuffleSplit(n_splits=1,random_state=12)\n\nfor train,test in splitter.split(X,y):     \n    X_train = X[train]\n    y_train = y[train]\n    X_test = X[test]\n    y_test = y[test]","c138d8d6":"X_train = np.reshape(X_train, (X_train.shape[0],X_train.shape[1],1))\nX_test = np.reshape(X_test, (X_test.shape[0],X_test.shape[1],1))","4fbdb990":"import os\nimport csv\nimport math\n\n\ndef create_solution_dictionary(solution):\n    \"\"\" Read solution file, return a dictionary with key EventId and value (weight,label).\n    Solution file headers: EventId, Label, Weight \"\"\"\n    \n    solnDict = {}\n    with open(solution, 'rb') as f:\n        soln = csv.reader(f)\n        soln.next() # header\n        for row in soln:\n            if row[0] not in solnDict:\n                solnDict[row[0]] = (row[1], row[2])\n    return solnDict\ndef check_submission(submission, Nelements):\n    \"\"\" Check that submission RankOrder column is correct:\n        1. All numbers are in [1,NTestSet]\n        2. All numbers are unqiue\n    \"\"\"\n    rankOrderSet = set()    \n    with open(submission, 'rb') as f:\n        sub = csv.reader(f)\n        sub.next() # header\n        for row in sub:\n            rankOrderSet.add(row[1])\n            \n    if len(rankOrderSet) != Nelements:\n        print ('RankOrder column must contain unique values')\n        exit()\n    elif rankOrderSet.isdisjoint(set(xrange(1,Nelements+1))) == False:\n        print ('RankOrder column must contain all numbers from [1..NTestSset]')\n        exit()\n    else:\n        return True\n    def AMS(s, b):\n        \"\"\" Approximate Median Significance defined as:\n            AMS = sqrt(\n                    2 { (s + b + b_r) log[1 + (s\/(b+b_r))] - s}\n                  )        \n        where b_r = 10, b = background, s = signal, log is natural logarithm \"\"\"\n\n        br = 10.0\n        radicand = 2 *( (s+b+br) * math.log (1.0 + s\/(b+br)) -s)\n        if radicand < 0:\n            print ('radicand is negative. Exiting')\n            exit()\n        else:\n            return math.sqrt(radicand)\n    def AMS_metric(solution, submission):\n        \"\"\"  Prints the AMS metric value to screen.\n        Solution File header: EventId, Class, Weight\n        Submission File header: EventId, RankOrder, Class\n        \"\"\"\n\n        numEvents = 550000 # number of events = size of test set\n\n        # solutionDict: key=eventId, value=(label, class)\n        solutionDict = create_solution_dictionary(solution)\n\n        signal = 0.0\n        background = 0.0\n        if check_submission(submission, numEvents):\n            with open(submission, 'rb') as f:\n                sub = csv.reader(f)\n                sub.next() # header row\n                for row in sub:\n                    if row[2] == 's': # only events predicted to be signal are scored\n                        if solutionDict[row[0]][0] == 's':\n                            signal += float(solutionDict[row[0]][1])\n                        elif solutionDict[row[0]][0] == 'b':\n                            background += float(solutionDict[row[0]][1])\n\n            print ('signal = {0}, background = {1}'.format(signal, background))\n            print ('AMS = ' + str(AMS(signal, background)))\n\n\n        if __name__ == \"__main__\":\n\n            # enter path and file names here    \n            path = \"\"\n            solutionFile = \"\"\n            submissionFile = \"\"","e3a5cbec":"def build_rnn_model(train_x,train_y,test_x,test_y):\n    inp = Input(shape=(train_x.shape[1],train_x.shape[2]))\n    rnn_1st_model = LSTM(units=60, return_sequences=True,recurrent_dropout=0.1)(inp)\n    rnn_2nd_model = LSTM(units=60,recurrent_dropout=0.1)(rnn_1st_model)\n    dense_layer = Dense(128)(rnn_2nd_model)\n    drop_out = Dropout(0.2)(dense_layer)\n    output = Dense(1, activation= LeakyReLU(alpha=0.1),name=\"class\")(drop_out)\n    model = Model(inp, output)\n    callbacks = [ReduceLROnPlateau(monitor='val_loss', patience=4, verbose=1, factor=0.6),\n                 EarlyStopping(monitor='val_loss', patience=20),\n                 ModelCheckpoint(filepath='best_model_LSTM.h5', monitor='val_loss', save_best_only=True)]\n    model.summary()\n    model.compile(loss=[tf.keras.losses.MeanSquaredLogarithmicError(),\n                        tf.keras.losses.MeanSquaredLogarithmicError()], optimizer=\"adam\")\n    history = model.fit(train_x, train_y, \n          epochs = 40, \n          batch_size = 100, \n          validation_data=(test_x,  test_y), \n          callbacks=callbacks)\n    return history,model","c712805f":"def plot_Loss(history):\n    plt.plot(history.history['loss'])\n    plt.plot(history.history['val_loss'])\n    plt.title('Loss over epochs')\n    plt.ylabel('Loss')\n    plt.xlabel('Epoch')\n    plt.legend(['Train', 'Validation'], loc='best')\n    plt.show()","dbc32a0c":"def Save_Result_To_Csv(model,model_name,csv_file):\n    test_pre = np.reshape(test_df_nor, (test_df_nor.shape[0],test_df_nor.shape[1],1))\n    model.load_weights(model_name)\n    prediction = model.predict(test_pre)\n    prediction =  np.where(prediction > 0.5, 1, 0)\n    prediction = pd.Series(prediction[:,0])\n    sub = pd.read_csv('..\/input\/higgs-boson\/random_submission.zip')\n    test_predict = pd.DataFrame({\"EventId\":sub['EventId'],\"RankOrder\":sub['RankOrder'],\"Class\":prediction})\n    test_predict['Class'] = test_predict['Class'].replace(1,'s')\n    test_predict['Class'] = test_predict['Class'].replace(0,'b')\n    test_predict.to_csv(csv_file,index=False)","99c98890":"history_RNN,Rnn_model = build_rnn_model(X_train,y_train,X_test,y_test)","d0002adc":"plot_Loss(history_RNN)","63de2724":"Save_Result_To_Csv(Rnn_model,\".\/best_model_LSTM.h5\",\"submission_Rnn.csv\")","95c3a540":"sub=pd.read_csv(\".\/submission_Rnn.csv\")\nsub.head()\n","661376c7":"*We do the same now with the balanced dataset.*","35b7a233":"*Here we have got hold of the multiple dependencies of the different variables. We will eliminate the collinearity of the variables while checking for the fearure importances post modelling.*","d259f19b":"### Recurrent Neural Networks \n\n*In a recurrent neural network we store the output activations from one or more of the layers of the network. Often these are hidden later activations. Then, the next time we feed an input example to the network, we include the previously-stored outputs as additional inputs. You can think of the additional inputs as being concatenated to the end of the \u201cnormal\u201d inputs to the previous layer. For example, if a hidden layer had 10 regular input nodes and 128 hidden nodes in the layer, then it would actually have 138 total inputs (assuming you are feeding the layer\u2019s outputs into itself \u00e0 la Elman) rather than into another layer). Of course, the very first time you try to compute the output of the network you\u2019ll need to fill in those extra 128 inputs with 0s or something.image.png\nRNNs are quite powerful, they suffer from Vanishing gradient problem which hinders them from using long term information, like they are good for storing memory 3-4 instances of past iterations but larger number of instances don't provide good results so we don't just use regular RNNs. Instead, we use a better variation of RNNs: Long Short Term Networks(LSTM).*","137c9e29":"### Boosting without Balancing Dataset:","aa1b5f60":"#### Helper Function:","c6c731e3":"# EDA:","f55f1a4a":"*Without the balancing of the dataset, the model runs poorly and hence the balancing.*","4d94dadd":"### Baseline Model using Decision Tree Classifier:","91bf85fc":"Our dataset has one categorical variable and rest all continuous variable.","a27bb7f4":"### Random Forest Classifier:","e2a686a3":"*As we can see this is an unbalanced dataset, with more number of background events, characterised by 1 and less number of signal events characterised by 0*","be4e3e81":"**We will have a look at our training dataset before we run any EDA,Feature Engineering or Modelling**","60798ab6":"*The imbalance is causing the accuracy issue for predicting the the signal event. Hence, we will balance the dataset by upsampling and downsampling.*","530c6366":"### Boosting:","c0ae0210":"*Here, we have the two datasets of the Higg Boson Challenge of 2014 which had the data for categorising the events as either a signal event or a background event. The accuracy of the classification model is to be calculated using the approximate median significance. The weights of each of the cases are also given.*","4a434535":"# Datasets:","5b916d60":"*This shows that our dataset is highly imbalanced and we will be needing to do certain upsampling and downsampling in our data.*","e214292c":"# Models:","7b80b301":"### Boosting with the Balanced Data:","51e32e8d":"# Comparing the ROC curves:","b6b8a82b":"### Upsampling and DownSampling:","5d9f2a33":"**we see in the dependences and importance plots, the most dependent variables are the least important amongst the top 10 as well.**","642d9553":"### Baseline Model using LogisticRegression:","b31e3dee":"# **Pre-requisites:**","518f9105":"Since, we know this is an imbalanced dataset, we will try out an upsampling downsampling for this purpose and run our model on the balanced dataset, to see how well our model does and if the upsampling helps.","915f43df":"**The ** ","00eae252":"**Although the dataset is imbalanced,the decision tree seems to do a decent job wrt to handling the imbalance**"}}