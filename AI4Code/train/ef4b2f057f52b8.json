{"cell_type":{"f2a90063":"code","5453e227":"code","ffb89e0d":"code","ca942eb6":"code","a7a6f9bd":"code","0eb7a027":"code","330cccbe":"code","1ea5fa10":"code","6a67d278":"code","404f800e":"code","4e870f35":"code","19e24e94":"code","d425ed5e":"code","fdf7ca3c":"code","1e874b4f":"code","a583f59f":"code","f67529f0":"code","e5340bef":"code","c9478ed5":"code","84bc54d3":"code","4740d1cc":"code","95279e53":"code","d5aeeaf2":"code","ca887cf8":"code","bc4c19a3":"code","13699b61":"code","5d6f16cb":"markdown","ba53fa12":"markdown","ef053d63":"markdown","4edc35c2":"markdown","821efe22":"markdown","93c418b5":"markdown"},"source":{"f2a90063":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","5453e227":"train_data = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntest_data = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')","ffb89e0d":"train_data.head()","ca942eb6":"train_data.shape","a7a6f9bd":"test_data.shape","0eb7a027":"train_data.columns","330cccbe":"train_data.info()","1ea5fa10":"train_data.isnull().sum()","6a67d278":"train_data['Survived'].value_counts()","404f800e":"train_data['Pclass'].value_counts()","4e870f35":"train_data['Sex'].value_counts()","19e24e94":"from sklearn.base import BaseEstimator , TransformerMixin\n\nclass DataFrameSelector(BaseEstimator, TransformerMixin):\n    def __init__(self, attribute_names):\n        self.attribute_names = attribute_names\n    def fit(self, X, y=None):\n        return self\n    def transform(self,X):\n        return X[self.attribute_names]","d425ed5e":"#pipeline for numerical attributes\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\n\nnum_pipeline = Pipeline([\n    ('select_numeric', DataFrameSelector(['Age', 'SibSp','Parch','Fare'])),\n    ('imputer',SimpleImputer(strategy='median'))\n])","fdf7ca3c":"num_pipeline.fit_transform(train_data)","1e874b4f":"class MostFrequentImputer(BaseEstimator, TransformerMixin):\n    def fit(self, X, y=None):\n        self.most_frequent_ = pd.Series([X[c].value_counts().index[0] for c in X],\n                                        index=X.columns)\n        return self\n    def transform(self, X, y=None):\n        return X.fillna(self.most_frequent_)","a583f59f":"try:\n    from sklearn.preprocessing import OrdinalEncoder # just to raise an ImportError if Scikit-Learn < 0.20\n    from sklearn.preprocessing import OneHotEncoder\nexcept ImportError:\n    from future_encoders import OneHotEncoder # Scikit-Learn < 0.20","f67529f0":"cat_pipeline = Pipeline([\n    (\"select_cat\", DataFrameSelector([\"Pclass\", \"Sex\", \"Embarked\"])),\n        (\"imputer\", MostFrequentImputer()),\n        (\"cat_encoder\", OneHotEncoder(sparse=False)),\n])","e5340bef":"cat_pipeline.fit_transform(train_data)\n","c9478ed5":"# join teh numerical and categorical variables\nfrom sklearn.pipeline import FeatureUnion\npreprocess_pipeline = FeatureUnion(transformer_list = [\n    ('num_pipeline',num_pipeline),\n    ('cat_pipeline',cat_pipeline)\n])","84bc54d3":"X_train = preprocess_pipeline.fit_transform(train_data)\nX_train","4740d1cc":"y_train = train_data['Survived']\ny_train","95279e53":"#let's train a classifier\n\n# 1. SVC\n\nfrom sklearn.svm import SVC\n\nsvm_clf = SVC(gamma = 'auto')\nsvm_clf.fit(X_train, y_train)","d5aeeaf2":"X_test = preprocess_pipeline.transform(test_data)\ny_pred = svm_clf.predict(X_test)","ca887cf8":"from sklearn.model_selection import cross_val_score\n\nsvm_scores = cross_val_score(svm_clf, X_train, y_train, cv=10)\nsvm_scores.mean()","bc4c19a3":"from sklearn.ensemble import RandomForestClassifier\n\nforest_clf = RandomForestClassifier(n_estimators = 100, random_state=42)\nforest_scores = cross_val_score(forest_clf, X_train, y_train, cv=10)\nforest_scores.mean()","13699b61":"import matplotlib.pyplot as plt\nplt.figure(figsize=(8, 4))\nplt.plot([1]*10, svm_scores, \".\")\nplt.plot([2]*10, forest_scores, \".\")\nplt.boxplot([svm_scores, forest_scores], labels=(\"SVM\",\"Random Forest\"))\nplt.ylabel(\"Accuracy\", fontsize=14)\nplt.show()","5d6f16cb":"The Age, Cabin and Embarked column has missing values.","ba53fa12":"That's much better!\n\nInstead of just looking at the mean accuracy across the 10 cross-validation folds, let's plot all 10 scores for each model, along with a box plot highlighting the lower and upper quartiles, and \"whiskers\" showing the extent of the scores (thanks to Nevin Yilmaz for suggesting this visualization). Note that the boxplot() function detects outliers (called \"fliers\") and does not include them within the whiskers. Specifically, if the lower quartile is  and the upper quartile is , then the interquartile range  (this is the box's height), and any score lower than  is a flier, and so is any score greater than .","ef053d63":"he attributes have the following meaning:\n\nSurvived: that's the target, 0 means the passenger did not survive, while 1 means he\/she survived.<br>\nPclass: passenger class.<br>\nName, Sex, Age: self-explanatory<br>\nSibSp: how many siblings & spouses of the passenger aboard the Titanic.<br>\nParch: how many children & parents of the passenger aboard the Titanic.<br>\nTicket: ticket id<br>\nFare: price paid (in pounds)<br>\nCabin: passenger's cabin number<br>\nEmbarked: where the passenger embarked the Titanic<br>\nLet's get more info to see how much data is missing:<br>","4edc35c2":"And now we could just build a CSV file with these predictions (respecting the format excepted by Kaggle), then upload it and hope for the best. But wait! We can do better than hope. Why don't we use cross-validation to have an idea of how good our model is?","821efe22":"To improve this result further, you could:\n\nCompare many more models and tune hyperparameters using cross validation and grid search,<br>\nDo more feature engineering, for example:<br>\nreplace SibSp and Parch with their sum,<br>\ntry to identify parts of names that correlate well with the Survived attribute (e.g. if the name contains \"Countess\", then survival seems more likely),<br>\ntry to convert numerical attributes to categorical attributes: for example, different age groups had very different survival rates (see below), so it may help to create an age bucket category and use it instead of the age. Similarly, it may be useful to have a special category for people traveling alone since only 30% of them survived","93c418b5":"Okay, over 73% accuracy, clearly better than random chance, but it's not a great score. Looking at the leaderboard for the Titanic competition on Kaggle, you can see that you need to reach above 80% accuracy to be within the top 10% Kagglers. Some reached 100%, but since you can easily find the list of victims of the Titanic, it seems likely that there was little Machine Learning involved in their performance! ;-) So let's try to build a model that reaches 80% accuracy.\n\nLet's try a RandomForestClassifier:"}}