{"cell_type":{"2cbcf0e2":"code","abb80ac8":"code","f47c5617":"code","0ccf9353":"code","9ac3ce87":"code","52ad2862":"code","3d41be36":"code","ebd8ace9":"code","11129913":"code","5d7241a8":"code","3fea963a":"code","af9430c8":"code","0b700eab":"code","37aec56f":"code","f3c41b25":"code","fad34f15":"code","2d754e5b":"code","a63bb8af":"code","2318de9f":"code","e241cfe8":"code","322c66ad":"markdown"},"source":{"2cbcf0e2":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.\ndata=pd.read_csv('..\/input\/mushrooms.csv')","abb80ac8":"#Checking for duplicates\ntot=len(set(data.index))\nlast=data.shape[0]-tot\nlast","f47c5617":"#Checking for null values\ndata.isnull().sum()\n","0ccf9353":"#checking the shape of dataset\ndata.shape","9ac3ce87":"#Lets see how the target variable is balanced\nprint(data['class'].value_counts())\nsns.countplot(x='class', data=data)\nplt.show()","52ad2862":"#Looking for categorical data\ncat=data.select_dtypes(include=['object']).columns\ncat","3d41be36":"#detailed view of each columns\nfor c in cat:\n    print(c)\n    print(\"-\"*50)\n    print(data[c].value_counts())\n    sns.countplot(x=c, data=data)\n    plt.show()\n    print(\"-\"*50)","ebd8ace9":"#we will remove what all we think not important or less contribution to target\ndata['cap-shape']=data[data['cap-shape']!='c']\ndata.dropna(inplace=True)\ndata.shape","11129913":"data['cap-surface']=data[data['cap-surface']!='g']\ndata.dropna(inplace=True)\ndata.shape","5d7241a8":"data.drop('veil-type',axis=1,inplace=True)","3fea963a":"cat=data.select_dtypes(include='object').columns\ncat","af9430c8":"#lets convert categorical data to numerical\nfrom sklearn.preprocessing import LabelEncoder\nle=LabelEncoder()\nfor i in cat:\n    data[i]=le.fit_transform(data[i])\n    ","0b700eab":"f,ax = plt.subplots(figsize=(20, 15))\nsns.heatmap(data.corr(), annot=True, linewidths=0.5,linecolor=\"red\", fmt= '.1f',ax=ax)\nplt.show()","37aec56f":"#lets do some feature engineering for fun\ndata['f-engineer']=((data['gill-size']+5)*(data['population']+5)*(1\/((data['gill-color']+5)*(data['bruises']+5)*(data['ring-type']+5))))","f3c41b25":"f,ax = plt.subplots(figsize=(20, 15))\nsns.heatmap(data.corr(), annot=True, linewidths=0.5,linecolor=\"red\", fmt= '.1f',ax=ax)\nplt.show()","fad34f15":"X = data.iloc[:,1:]\nX = X.values\ny = data['class'].values","2d754e5b":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier","a63bb8af":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=10)","2318de9f":"algo = {'LR': LogisticRegression(), \n        'DT':DecisionTreeClassifier(), \n        'RFC':RandomForestClassifier(n_estimators=100), \n        'SVM':SVC(gamma=0.01),\n        'KNN':KNeighborsClassifier(n_neighbors=10)\n       }\n\nfor k, v in algo.items():\n    model = v\n    model.fit(X_train, y_train)\n    print('Acurracy of ' + k + ' is {0:.2f}'.format(model.score(X_test, y_test)*100)+'%')","e241cfe8":"#yes we have accuracy of 100%","322c66ad":"**we will do this as simple as possible leave a like or comment if it helped you**"}}