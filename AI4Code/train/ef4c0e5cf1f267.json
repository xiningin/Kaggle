{"cell_type":{"38cc2079":"code","dff35337":"code","ddebdaf0":"code","d4934a6c":"code","712c8889":"code","84801a96":"code","ea631401":"code","0ea93978":"code","da14cdab":"code","7fa544dd":"code","2cebe344":"code","16b9f163":"code","d36b56a7":"code","ed3b7f5b":"code","3f99a7d6":"code","6bce2748":"code","bd3080ef":"code","30c6cd04":"code","7df48794":"code","8f395954":"code","2472d622":"code","16445d09":"code","4ff46e17":"code","91b8b1bc":"code","70997960":"code","b857bfab":"code","94d56065":"code","38631250":"code","05980dc4":"code","0607635b":"code","6795340e":"code","a5a3ed72":"code","73de0931":"code","afe5940e":"code","6e793b45":"code","8a6c7d23":"code","b2b6439a":"code","27fb5370":"code","f7d20cf6":"code","d12ec702":"code","4073ea47":"code","cf5df92f":"code","ca211aa2":"code","f285ff00":"code","40d10509":"code","dfc77d7b":"code","1a41a08f":"code","bd69a677":"code","c7e1ab61":"code","146b37a8":"code","332d41a1":"code","32f8a2b5":"code","a1277efb":"code","90769713":"markdown","4244ee72":"markdown","3277f88f":"markdown","8f50b34e":"markdown","377048bb":"markdown","94739ffa":"markdown"},"source":{"38cc2079":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt \nimport seaborn as sns \nfrom scipy.stats import skew, norm \nfrom warnings import filterwarnings as filt \n\nfilt('ignore')\nplt.style.use('fivethirtyeight')\nplt.rcParams['figure.figsize'] = (12,6)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","dff35337":"base_dir = '\/kaggle\/input\/loan-prediction-based-on-customer-behavior\/'\ntraindf = pd.read_csv(f'{base_dir}Training Data.csv')\ntestdf = pd.read_csv(f'{base_dir}Test Data.csv')\ntraindf.shape, testdf.shape","ddebdaf0":"traindf.head()","d4934a6c":"testdf.head()","712c8889":"trainId = traindf.shape[0]\ntarget = traindf.Risk_Flag\ntraindf = traindf.drop(['Risk_Flag','Id'], axis = 1)\ntestdf = testdf.drop(['ID'], axis = 1)\ndf = pd.concat([traindf, testdf]).reset_index(drop = True)\ndf.head()","84801a96":"df.isnull().sum()","ea631401":"uni = pd.DataFrame(df.nunique(), columns = ['unique']).sort_values('unique', ascending = False)\nuni['unique %'] = np.round((uni.unique \/ df.shape[0]) * 100, 2)\nuni","0ea93978":"counts = [c for c in df.columns if df[c].nunique() <= 5]\nfig, ax = plt.subplots(2, 2)\nfig.tight_layout()\nind = 0\nfor r in range(2):\n    for c in range(2):\n        x = df[counts[ind]]\n        sns.countplot(x, ax = ax[r,c])\n        ind += 1","da14cdab":"categorical_feats = df.select_dtypes(include = 'object').columns\nnumerical_feats = df.select_dtypes(exclude = 'object').columns\ndf[categorical_feats].head()","7fa544dd":"df.Profession.unique()","2cebe344":"df['Profession'] = df.Profession.apply(lambda x : '_'.join(x.split()))","16b9f163":"df.CITY.unique().shape","d36b56a7":"import re\ndf['CITY'] = df.CITY.apply(lambda x : '_'.join(re.split(r'-| ',x.split('[')[0])))","ed3b7f5b":" df['STATE'] = df.STATE.apply(lambda x : '_'.join(x.split('[')[0].split()))","3f99a7d6":"df['House_Ownership'] = df.House_Ownership.apply(lambda x : 'broke' if x == 'norent_noown' else x)","6bce2748":"df = df.rename(columns = {'Married\/Single' : 'Single'})\ndf.head()","bd3080ef":"from sklearn.preprocessing import OrdinalEncoder as oe\n\ndummy = pd.get_dummies(df.House_Ownership, prefix = 'House_Ownership')\nhouse_ownership = df.House_Ownership\ndf = df.drop(['House_Ownership'], axis = 1)\ndf = pd.concat([df, dummy], axis = 1)\n\nencoder = oe()\nfeats = ['Single', 'Car_Ownership', 'Profession', 'CITY', 'STATE']\ndf[feats] = encoder.fit_transform(df[feats])\ndf.head()","30c6cd04":"feats_to_plot = [c for c in df.columns if df[c].nunique() > 5]\nfig, ax = plt.subplots(len(feats_to_plot), 2, figsize = (18,13))\nfig.tight_layout()\nfor ind, col in enumerate(feats_to_plot):\n    sns.distplot(df[col], ax = ax[ind, 0])\n    sns.boxplot(df[col], ax = ax[ind, 1])","7df48794":"def train_val(x, y, test_size = 0.2):\n    idx = x.sample(frac = test_size).index\n    train_x, val_x = x.drop(idx), x.loc[idx]\n    train_y, val_y = y.drop(idx), y.loc[idx]\n    return train_x, val_x, train_y, val_y","8f395954":"traindf.shape[0], traindf.shape[0] * 0.05 ","2472d622":"x = df.loc[:trainId - 1]\ntestdf = df.loc[trainId : ].reset_index()\ntrain_x, val_x, train_y, val_y = train_val(x, target, 0.05)\ntrain_x.shape, train_y.shape, val_x.shape, val_y.shape","16445d09":"import eli5\nfrom eli5.sklearn import PermutationImportance\nimport shap\nfrom pdpbox import pdp\nfrom sklearn.ensemble import RandomForestClassifier as rfc\nfrom sklearn.feature_selection import mutual_info_classif\nfrom sklearn.preprocessing import StandardScaler\n\ndef permImp(x, y):\n    model = rfc().fit(x, y)\n    perm = PermutationImportance(model).fit(x, y)\n    return eli5.show_weights(perm , feature_names = x.columns.tolist())\n\ndef isolate(x, y, col):\n    model = rfc().fit(x, y)\n    pdp_dist = pdp.pdp_isolate(model, dataset = x, model_features = x.columns, feature = col)\n    return pdp.pdp_plot(pdp_dist, feature_name = col)\n\ndef interact(x, y, col):\n    model = rfc().fit(x, y)\n    pdp_dist = pdp.pdp_interact(model, dataset = x, model_features = x.columns, features = col)\n    return pdp.pdp_interact_plot(pdp_dist, feature_names = col)\n\ndef forceplot(x, y, n_class = 0):\n    model = rfc().fit(x, y)\n    explainer = shap.TreeExplainer(model)\n    x = x.sample(n = 1)\n    shap_value = explainer.shap_values(x)[n_class]\n    expected_value = explainer.expected_value[n_class]\n    return shap.force_plot(expected_value, shap_value, feature_names = x.columns)\n\ndef plot_mi(score):\n    score = score.sort_values('mi_score', ascending = True)\n    return plt.barh(score.index, score.mi_score)\n\ndef mi_score(x, y):\n    x = pd.DataFrame(StandardScaler().fit_transform(x), columns = x.columns)\n    score = pd.DataFrame(mutual_info_classif(x, y, discrete_features = False), index = x.columns, columns = ['mi_score'])\n    plot_mi(score)\n    return score.sort_values('mi_score', ascending = False)","4ff46e17":"mscore = mi_score(val_x, val_y)","91b8b1bc":"permImp(val_x, val_y)","70997960":"plt.figure(figsize = (12,6))\nisolate(val_x, val_y, 'Income')","b857bfab":"interact(val_x, val_y, ['Income', 'CITY'])","94d56065":"shap.initjs()\nforceplot(val_x, val_y, 1)","38631250":"train_x.head()","05980dc4":"from sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom xgboost import XGBRFClassifier\nimport xgboost as xgb\nfrom lightgbm import LGBMClassifier\nfrom sklearn.linear_model import LogisticRegression\n\nfrom sklearn.model_selection import cross_validate, KFold, train_test_split, GridSearchCV\n\nfrom sklearn.metrics import classification_report, roc_auc_score, confusion_matrix\nfrom sklearn.pipeline import Pipeline\n\nfrom sklearn.preprocessing import StandardScaler , RobustScaler, MinMaxScaler","0607635b":"#         idx = x.sample(frac = frac).index\n#         x = x.loc[idx]\n#         y = y.loc[idx]\n\ndef best_model(x, y, frac = 0):\n    if frac > 0:\n        print(f'Taking the sample size of :===> {x.shape[0] * frac}')\n        bigger_x, x, bigger_y, y = train_test_split(x, y, test_size = frac, stratify = y)\n        print('target variable split %')\n        print(y.value_counts()\/ y.shape[0])\n\n    xgb.set_config(verbosity=0)\n    models = [LogisticRegression(), SVC(), KNeighborsClassifier(), GaussianNB(), rfc(), XGBRFClassifier(), LGBMClassifier()]\n    names = ['logistic regg', 'svm', 'knn', 'naive bayes', 'random forest', 'xgboost', 'lightgb']\n    scores = [[] for _ in range(4)]\n    for model in models:\n        for idx, scaler in enumerate([None, StandardScaler(), RobustScaler(), MinMaxScaler()]):\n            if scaler:\n                model = Pipeline(steps = [('scaler', scaler), ('model', model)])\n            #cv = StratifiedKFold(5, shuffle = True, random_state = 123)\n            cv = KFold(5, shuffle = True, random_state = 123)\n            score = cross_validate(model, X = x, y = y, cv = cv, scoring = 'f1', verbose = 0)['test_score'].mean()\n            scores[idx].append(score)\n    return pd.DataFrame(scores, columns = names, index = ['None', 'std', 'robust', 'minmax']).T\n\ndef clf_report(yt, pred):\n    print()\n    print(classification_report(yt,  pred))\n    print()\n    \ndef get_score(xt, yt, xtest, ytest, model, scaler = None):\n    if scaler:\n        model = Pipeline(steps = [('scaler', scaler), ('model', model)])\n    model.fit(xt, yt)\n    pred = model.predict(xtest)\n    print(' Report '.center(60,'='))\n    print()\n    print(f\"training score  :===>  {model.score(xt, yt)}\")\n    print(f\"testing score   :===>  {model.score(xtest, ytest)}\")\n    clf_report(ytest, pred)\n    sns.heatmap(confusion_matrix(ytest, pred), fmt = '.1f', annot = True)\n    \n    \ndef gridcv(xt, yt, model, params, scaler = None, frac = 0):\n    if frac > 0:\n        print(f'Taking the sample size of :===> {xt.shape[0] * frac}')\n        bigger_xt, xt, bigger_yt, yt = train_test_split(xt, yt, test_size = frac, stratify = yt)\n        print('target variable split %')\n        print(yt.value_counts()\/ yt.shape[0])   \n        \n    if scaler:\n        model = Pipeline(steps = [('scaler', scaler), ('model', model)])\n    cv = KFold(5, shuffle = True, random_state = 123)\n    clf = GridSearchCV(model, param_grid = params, cv = cv, scoring = 'f1_micro', return_train_score = True, verbose = 1)\n    clf.fit(xt, yt)\n    res = pd.DataFrame(clf.cv_results_).sort_values('mean_test_score', ascending = False)\n    return clf.best_estimator_, clf.best_params_, res[['mean_train_score','mean_test_score','params']]\n    ","6795340e":"sns.countplot(train_y)\nplt.legend(train_y.value_counts())","a5a3ed72":"train_y.value_counts()","73de0931":"# since the dataset is large lets try undersampling \n# disadvantage - loss of information\nfrom imblearn.under_sampling import RandomUnderSampler\n\nsmot = RandomUnderSampler()\nus_x, us_y = smot.fit_resample(train_x, train_y)\nus_x.shape, us_y.shape","afe5940e":"sns.countplot(us_y)","6e793b45":"58940 * 0.4","8a6c7d23":"# score = f1\nbest_model(us_x, us_y, frac = 0.4)","b2b6439a":"params = {\n    'n_estimators' : [100,200,300], \n    'max_depth' : [8,12,16,20,None],\n    'criterion' : ['gini', 'entropy'],\n    'bootstrap' : [True, False],\n    'class_weight' : [None, 'balanced']\n}\npipeline_params = {f\"model__{key}\" : value for key, value in params.items()}\n\nclf, best_param, results = gridcv(us_x, us_y, rfc(), params, None, 0.04)","27fb5370":"sns.lineplot(x = np.arange(0,results.shape[0]), y = results.mean_train_score)\nsns.lineplot(x = np.arange(0,results.shape[0]), y = results.mean_test_score)\nplt.title('f1 score comparision for train and test')\nplt.legend(['training score', 'testing score'])","f7d20cf6":"results.head()","d12ec702":"best_param","4073ea47":"new_x_train, new_x_test, new_y_train, new_y_test = train_test_split(us_x, us_y, test_size = 0.2, stratify = us_y)","cf5df92f":"get_score(new_x_train, new_y_train, new_x_test, new_y_test, rfc(max_depth = 16))","ca211aa2":"new_x_train, new_x_test, new_y_train, new_y_test = train_test_split(us_x.drop(['House_Ownership_broke'], axis = 1), us_y, test_size = 0.2, stratify = us_y)","f285ff00":"get_score(new_x_train, new_y_train, new_x_test, new_y_test, rfc(max_depth = 16))","40d10509":"# xt, val_x, yt, val_y = train_val(train_x, train_y)\nxt, val_x, yt, val_y = train_test_split(train_x, train_y, test_size = 0.2, stratify = train_y)","dfc77d7b":"get_score(xt, yt, val_x, val_y, rfc(max_depth = 15, class_weight = 'balanced'))","1a41a08f":"from sklearn.utils import class_weight\n\nclass_weight.compute_class_weight('balanced', yt.unique(), yt)","bd69a677":"weights = np.linspace(0.0,1.0, 20)\nweights","c7e1ab61":"params = {\n    'class_weight' : [{0: x, 1: abs(1 - x)} for x in weights] + ['balanced', 'auto', 'None']\n}\nclf, best_weight, results = gridcv(xt, yt, rfc(max_depth = 15), params, None, 0.04)","146b37a8":"sns.lineplot(x = np.arange(0,results.shape[0]), y = results.mean_train_score)\nsns.lineplot(x = np.arange(0,results.shape[0]), y = results.mean_test_score)\nplt.title('f1 score comparision for train and test')\nplt.legend(['training score', 'testing score'])","332d41a1":"results.head()","32f8a2b5":"best_weight","a1277efb":"get_score(xt, yt, val_x, val_y, clf)","90769713":"### data cleaning","4244ee72":"### original datasets","3277f88f":"there's a greater chance of getting a loan from almost every city if the income is low but if its high the chances are low ","8f50b34e":"### feature engg","377048bb":"judging from the multual info and perm importance income, city and profession are the most important features for the model ","94739ffa":"hmmm according to pd plot, greater the income lower the chance of getting a loan "}}