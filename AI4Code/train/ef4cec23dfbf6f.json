{"cell_type":{"575d5cd4":"code","e4b1861b":"code","088b4fb6":"code","5349ff7f":"code","17144070":"code","c84064e0":"code","81f58a0f":"code","c190fbfe":"code","ce45c5da":"code","d225d87c":"code","7a83109c":"code","84a85b16":"code","4218a946":"code","aa8f0555":"code","f1788f70":"code","6e536530":"code","0e83b850":"code","973a82aa":"code","43682883":"code","d15bed33":"code","8f0f8565":"markdown","6d6e2444":"markdown","1d8d3489":"markdown","01830b77":"markdown","6ead81df":"markdown","a8bca629":"markdown","3ef27b4b":"markdown","18a284a1":"markdown","fe4c61b3":"markdown"},"source":{"575d5cd4":"# Import Time\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.optim import lr_scheduler\nimport torchvision\nfrom torchvision import transforms\nfrom torch.utils.data import DataLoader, Dataset\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nimport os\nfrom tqdm import tqdm_notebook as tqdm\n\n%matplotlib inline\n\nBATCH_SIZE = 256\n\nprint(os.listdir('..\/input\/Kannada-MNIST\/'))\n\n# Gets the GPU if there is one, otherwise the cpu\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(device)","e4b1861b":"df_train = pd.read_csv('..\/input\/Kannada-MNIST\/train.csv')\ntarget = df_train['label']\ndf_train.drop('label', axis=1, inplace=True)\n\nX_test = pd.read_csv('..\/input\/Kannada-MNIST\/test.csv')\nX_test.drop('id', axis=1, inplace=True)\n\nX_test.head()","088b4fb6":"X_train, X_dev, y_train, y_dev = train_test_split(df_train, target, stratify=target, random_state=42, test_size=0.2)\nprint('X_train', len(X_train))\nprint('X_dev', len(X_dev))\nprint('X_test', len(X_test))","5349ff7f":"# Visualization Reference Kernel https:\/\/www.kaggle.com\/josephvm\/kannada-with-pytorch\n# Some quick data visualization \n# First 10 images of each class in the training set\n\nfig, ax = plt.subplots(nrows=10, ncols=10, figsize=(10,10))\n\n# I know these for loops look weird, but this way num_i is only computed once for each class\nfor i in range(10): # Column by column\n    num_i = X_train[y_train == i]\n    ax[0][i].set_title(i)\n    for j in range(10): # Row by row\n        ax[j][i].axis('off')\n        ax[j][i].imshow(num_i.iloc[j, :].to_numpy().astype(np.uint8).reshape(28, 28), cmap='gray')","17144070":"class CharData(Dataset):\n    \"\"\"\n    Infer from standard PyTorch Dataset class\n    Such datasets are often very useful\n    \"\"\"\n    \n    def __init__(self,\n                 images,\n                 labels=None,\n                 transform=None,\n                ):\n        self.X = images\n        self.y = labels\n        \n        self.transform = transform\n    \n    def __len__(self):\n        return len(self.X)\n\n    def __getitem__(self, idx):\n        img = np.array(self.X.iloc[idx, :], dtype='uint8').reshape([28, 28, 1])\n        if self.transform is not None:\n            img = self.transform(img)\n        \n        if self.y is not None:\n            y = np.zeros(10, dtype='float32')\n            y[self.y.iloc[idx]] = 1\n            return img, y\n        else:\n            return img","c84064e0":"# Put some augmentation on training data\ntrain_transform = transforms.Compose([\n    transforms.ToPILImage(),\n    transforms.RandomAffine(degrees=5, translate=(0.1, 0.1), scale=(0.9, 1.1), shear=5),\n    transforms.ToTensor()\n])\n\n# Test data without augmentation\ntest_transform = transforms.Compose([\n    transforms.ToPILImage(),\n    transforms.ToTensor()\n])","81f58a0f":"# Create dataset objects\ntrain_dataset = CharData(X_train, y_train, train_transform)\ndev_dataset = CharData(X_dev, y_dev, test_transform)\ntest_dataset = CharData(X_test, transform=test_transform)","c190fbfe":"# Create data generators - they will produce batches\ntrain_loader = DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\ndev_loader = DataLoader(dataset=dev_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\ntest_loader = DataLoader(dataset=test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)","ce45c5da":"fig, ax = plt.subplots(nrows=1, ncols=16, figsize=(30,4))\n\nfor batch in train_loader:\n    for i in range(16):\n        ax[i].set_title(batch[1][i].data.numpy().argmax())\n        ax[i].imshow(batch[0][i, 0], cmap='gray')\n    break","d225d87c":"DEPTH_MULT = 4\n\nclass ConvLayer(nn.Module):\n    def __init__(self, input_size, output_size, kernel_size=3):\n        super(ConvLayer, self).__init__()\n        self.ops = nn.Sequential(\n            nn.Conv2d(input_size, output_size, kernel_size=kernel_size, stride=1, padding=kernel_size\/\/2),\n            nn.BatchNorm2d(output_size),\n            nn.ReLU(inplace=True)\n        )\n    \n    def forward(self, x):\n        return self.ops(x)\n\n\nclass FCLayer(nn.Module):\n    def __init__(self, input_size, output_size):\n        super(FCLayer, self).__init__()\n        self.ops = nn.Sequential(\n            nn.Linear(input_size, output_size),\n            nn.BatchNorm1d(output_size),\n            nn.ReLU(inplace=True)\n        )\n    \n    def forward(self, x):\n        return self.ops(x)\n\n\ndef mixup(x, shuffle, lam, i, j):\n    if shuffle is not None and lam is not None and i == j:\n        x = lam * x + (1 - lam) * x[shuffle]\n    return x\n\n\nclass Net(nn.Module):\n    def __init__(self, num_classes):\n        super(Net, self).__init__()\n        \n        self.conv1 = ConvLayer(1, DEPTH_MULT * 32)\n        self.conv2 = ConvLayer(DEPTH_MULT * 32, DEPTH_MULT * 32)\n        self.conv3 = ConvLayer(DEPTH_MULT * 32, DEPTH_MULT * 32)\n        self.conv4 = ConvLayer(DEPTH_MULT * 32, DEPTH_MULT * 32)\n        \n        self.conv5 = ConvLayer(DEPTH_MULT * 32, DEPTH_MULT * 64)\n        self.conv6 = ConvLayer(DEPTH_MULT * 64, DEPTH_MULT * 64)\n        self.conv7 = ConvLayer(DEPTH_MULT * 64, DEPTH_MULT * 64)\n        self.conv8 = ConvLayer(DEPTH_MULT * 64, DEPTH_MULT * 64)\n        self.conv9 = ConvLayer(DEPTH_MULT * 64, DEPTH_MULT * 64)\n        self.conv10 = ConvLayer(DEPTH_MULT * 64, DEPTH_MULT * 64)\n        \n        self.mp = nn.MaxPool2d(kernel_size=2, stride=2)\n        \n        self.fc1 = FCLayer(DEPTH_MULT * 64 * 7 * 7, DEPTH_MULT * 512)\n        self.fc2 = FCLayer(DEPTH_MULT * 512, DEPTH_MULT * 512)\n        self.fc3 = FCLayer(DEPTH_MULT * 512, DEPTH_MULT * 512)\n        self.fc4 = FCLayer(DEPTH_MULT * 512, DEPTH_MULT * 512)\n        self.projection = nn.Linear(DEPTH_MULT * 512, 10)\n    \n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.conv2(x)\n        x = self.conv3(x)\n        x = self.conv4(x)\n        x = self.mp(x)\n        \n        x = self.conv5(x)\n        x = self.conv6(x)\n        x = self.conv7(x)\n        x = self.conv8(x)\n        x = self.conv9(x)\n        x = self.conv10(x)\n        x = self.mp(x)\n        \n        x = x.view(x.size(0), -1)\n        x = self.fc1(x)\n        x = self.fc2(x)\n        x = self.fc3(x)\n        x = self.fc4(x)\n        x = self.projection(x)\n        \n        return x","7a83109c":"def criterion(input, target, size_average=True):\n    \"\"\"Categorical cross-entropy with logits input and one-hot target\"\"\"\n    l = -(target * torch.log(F.softmax(input, dim=1) + 1e-10)).sum(1)\n    if size_average:\n        l = l.mean()\n    else:\n        l = l.sum()\n    return l","84a85b16":"model = Net(10)\nmodel = model.to(device)\n\nn_epochs = 80\n\noptimizer = optim.Adam(model.parameters(), lr=0.001)\nexp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=n_epochs \/\/ 4, gamma=0.1)","4218a946":"def train(epoch, history=None):\n    model.train()\n\n    for batch_idx, (data, target) in enumerate(train_loader):\n        data = data.to(device)\n        target = target.to(device)\n                \n        optimizer.zero_grad()\n        output = model(data)\n        loss = criterion(output, target)\n        if history is not None:\n            history.loc[epoch + batch_idx \/ len(train_loader), 'train_loss'] = loss.data.cpu().numpy()\n        \n        loss.backward()\n        optimizer.step()\n        \n        if (batch_idx + 1) % 100 == 0:\n            print('Train Epoch: {} [{}\/{} ({:.0f}%)]\\tLR: {:.6f}\\tLoss: {:.6f}'.format(\n                epoch, (batch_idx + 1) * len(data), len(train_loader.dataset),\n                100. * (batch_idx + 1) \/ len(train_loader),\n                optimizer.state_dict()['param_groups'][0]['lr'],\n                loss.data))\n    exp_lr_scheduler.step()\n\ndef evaluate(epoch, history=None):\n    model.eval()\n    loss = 0\n    correct = 0\n    \n    with torch.no_grad():\n        for data, target in dev_loader:\n            data = data.to(device)\n            target = target.to(device)\n\n            output = model(data)\n\n            loss += criterion(output, target, size_average=False).data\n\n            pred = output.data.max(1, keepdim=True)[1]\n            correct += pred.eq(target.max(1, keepdim=True)[1].data.view_as(pred)).cpu().sum().numpy()\n    \n    loss \/= len(dev_loader.dataset)\n    accuracy = correct \/ len(dev_loader.dataset)\n    \n    if history is not None:\n        history.loc[epoch, 'dev_loss'] = loss.cpu().numpy()\n        history.loc[epoch, 'dev_accuracy'] = accuracy\n    \n    print('Dev loss: {:.4f}, Dev accuracy: {}\/{} ({:.3f}%)\\n'.format(\n        loss, correct, len(dev_loader.dataset),\n        100. * accuracy))","aa8f0555":"%%time\nimport gc\n\nhistory = pd.DataFrame()\n\nfor epoch in range(n_epochs):\n    torch.cuda.empty_cache()\n    gc.collect()\n    train(epoch, history)\n    evaluate(epoch, history)","f1788f70":"history['train_loss'].plot();","6e536530":"history.dropna()['dev_loss'].plot();","0e83b850":"history.dropna()['dev_accuracy'].plot();","973a82aa":"print('max', history.dropna()['dev_accuracy'].max())\nprint('max in last 5', history.dropna()['dev_accuracy'].iloc[-5:].max())\nprint('avg in last 5', history.dropna()['dev_accuracy'].iloc[-5:].mean())","43682883":"model.eval()\npredictions = []\n\nfor data in tqdm(test_loader):\n    data = data.to(device)\n    output = model(data).max(dim=1)[1] # argmax\n    predictions += list(output.data.cpu().numpy())","d15bed33":"submission = pd.read_csv('..\/input\/Kannada-MNIST\/sample_submission.csv')\nsubmission['label'] = predictions\nsubmission.to_csv('submission.csv', index=False)\nsubmission.head()","8f0f8565":"# Load data","6d6e2444":"# I can't overfit...\nIn this kernel I train very wide neural network without dropout and other regularization.  \nIt still works ok and it barely overfits.","1d8d3489":"## Show some generated samples","01830b77":"# Define model","6ead81df":"# PyTorch data generator","a8bca629":"# Plot losses","3ef27b4b":"# Predict","18a284a1":"# Train model","fe4c61b3":"# Visualize"}}