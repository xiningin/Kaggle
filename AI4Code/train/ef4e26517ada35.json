{"cell_type":{"87f1fdd7":"code","31624376":"code","c0b93d70":"code","dccc1b52":"code","f2309519":"code","33126663":"code","529ea138":"code","d3608485":"code","c688ccbf":"code","a3f170fc":"code","1ba11538":"code","6a791e0b":"code","1a41381f":"code","06524bf7":"code","85ab21c0":"code","b991ca4a":"code","3fe37f7b":"code","d6172b9d":"code","e506d44b":"code","50cdc7c9":"code","dd9ad5af":"code","65a0b171":"code","b2695996":"code","a87e738e":"code","f54eb4bd":"code","8dd2598b":"code","4df239ad":"code","d7eb1c24":"code","fd0cf75c":"code","f57e3ce3":"code","116f0101":"code","c8b051e7":"code","3b22c4ca":"code","b89caddb":"code","9879275a":"code","44e14124":"code","1a7b2340":"code","d12f3d73":"markdown","821efae5":"markdown","75eae323":"markdown","10b2d44a":"markdown","09a34ee3":"markdown","8ba04d58":"markdown","a7f5f53f":"markdown","38ebc410":"markdown","96ba1ea1":"markdown","55bd61bf":"markdown","6018f80d":"markdown","bd6fad93":"markdown","36fcb0cb":"markdown","0e4d36d1":"markdown","fb383664":"markdown","c67c65b8":"markdown","a9eca2ab":"markdown"},"source":{"87f1fdd7":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","31624376":"from warnings import filterwarnings\nfilterwarnings('ignore')\n\npd.set_option('display.max_columns',100)\npd.set_option('display.max_rows',100)\n\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nimport matplotlib.pyplot as plt\n\nimport plotly.figure_factory as ff\nfrom plotnine import *\nimport plotnine as pn\n\nimport seaborn as sns","c0b93d70":"# read data\ntrain  = pd.read_csv('\/kaggle\/input\/tabular-playground-series-aug-2021\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/tabular-playground-series-aug-2021\/test.csv')","dccc1b52":"shape_df = pd.DataFrame({'Data':['Train','Test'],\n                       'Shape':[train.shape[0], test.shape[0]]})\ncolors = ['#FFBF00','#40E0D0']\ndata = go.Bar(x =shape_df.Shape[::-1],y=shape_df.Data[::-1], orientation='h', text=shape_df.Shape[::-1], textposition='auto', marker_color=colors)\nlayout = go.Layout(font=dict(family='Arial',size=14),\n                  paper_bgcolor='white',\n                  plot_bgcolor = '#FFFAFA',\n                 showlegend=False,width=800, height=400,title='Train & Test Data Size')\nfig = go.Figure(data=data, layout=layout)\nfig.update_xaxes(showline=True, linewidth=2, linecolor='black')\nfig.update_yaxes(showline=True, linewidth=2, linecolor='black')\nfig.show()","f2309519":"# Data attributes\nprint('Total null values in train data:', train.isna().sum().sum())\nprint('Total null values in test data:', test.isna().sum().sum())","33126663":"# get column dtype counts\nlabels = ['float','int']\nvalues = [95,5]\ndata = go.Pie(labels=labels, values=values,pull=[0.2,0],textinfo='label+value', marker=dict(colors=colors))\nlayout = go.Layout(font=dict(family='Arial',size=14),\n                  paper_bgcolor='white',\n                  plot_bgcolor = '#FFFAFA',\n                 showlegend=False,title='Feature Count by Data Type', height=500, width=500)\nfig = go.Figure(data=data,layout=layout)\nfig.show()","529ea138":"target_counts = train.groupby('loss')['loss'].count()\ntarget_counts_df = pd.DataFrame({'Target Value':target_counts.index, 'Count':target_counts.values})\ntarget_counts_df['Pct'] = target_counts_df['Count']\/train.shape[0]\n\ntarget_counts_df['Target Value'] = target_counts_df['Target Value'].astype('str')\n\nfig = make_subplots(rows=1, cols=2, subplot_titles=(\"Target Value Counts\", \"Percentage Target Values\"),horizontal_spacing=0.05)\n\nfig.add_trace(go.Bar(x = target_counts_df.Count[::-1], y=target_counts_df['Target Value'][::-1], orientation='h',\n                    text = target_counts_df.Count[::-1], textposition='outside', marker_color='#FFBF00'),row=1,col=1)\n\nfig.add_trace(go.Bar(x = target_counts_df.Pct[::-1], y=target_counts_df['Target Value'][::-1], orientation='h',\n                    text = target_counts_df.Pct[::-1], textposition='outside', marker_color='#40E0D0'),row=1,col=2)\n\nfig.update_layout(font=dict(family='Arial'),\n                  paper_bgcolor='white',\n                  plot_bgcolor = '#FFFAFA',\n                 showlegend=False,height=900)\nfig.update_xaxes(showline=True, linewidth=2, linecolor='black')\nfig.update_yaxes(showline=True, linewidth=2, linecolor='black')\n\nfig.show()\nplt.savefig('Target Values.png')","d3608485":"# unique value counts for the integer features\nint_df = train.select_dtypes(include=['int64']).drop(['loss','id'],axis=1)\n\nint_col_list = int_df.columns\n\nunique_value_dict = {}\n\ntrain_feats = []\ntrain_unique_values = []\n\ntest_feats = []\ntest_unique_values = []\n\nfor col in int_col_list:\n    train_feats.append(col)\n    test_feats.append(col)\n    \n    train_unique_values.append(train[col].nunique())\n    test_unique_values.append(test[col].nunique())\n    \ntrain_int_feats_unique_vals = pd.DataFrame({'Feature':train_feats,'Unique Values':train_unique_values})\ntest_int_feats_unique_vals = pd.DataFrame({'Feature':test_feats,'Unique Values':test_unique_values})\n\nfig = make_subplots(rows=1, cols=2, subplot_titles=(\"Train Data\", \"Test Data\"),horizontal_spacing=0.05)\n\nfig.add_trace(go.Bar(x = train_int_feats_unique_vals['Unique Values'][::-1], y=train_int_feats_unique_vals['Feature'][::-1], orientation='h',\n                    text = train_int_feats_unique_vals['Unique Values'][::-1], textposition='auto', marker_color='#FFBF00'),row=1,col=1)\nfig.add_trace(go.Bar(x = test_int_feats_unique_vals['Unique Values'][::-1], y=test_int_feats_unique_vals['Feature'][::-1], orientation='h',\n                    text = test_int_feats_unique_vals['Unique Values'][::-1], textposition='auto', marker_color='#40E0D0'),row=1,col=2)\n\nfig.update_layout(font=dict(family='Arial'),\n                  paper_bgcolor='white',\n                  plot_bgcolor = '#FFFAFA',\n                 showlegend=False,title='Unique Value Count for Integer Features')\n\nfig.update_xaxes(showline=True, linewidth=2, linecolor='black')\nfig.update_yaxes(showline=True, linewidth=2, linecolor='black')\n\nfig.show()\nplt.savefig('integer_features.png')","c688ccbf":"feat_list = train.select_dtypes(include=['float64']).columns\n\nfeat_list_set1 = feat_list[0:20]\n\nfig, axes = plt.subplots(5, 4,figsize=(21, 20))\n\nn = 0\nsns.despine()\nfor row in range(5):\n    for col in range(4):\n        feat = feat_list_set1[n] \n        sns.kdeplot(train[feat],shade=True, color=\"#FFBF00\", alpha=0.1, ax=axes[row,col])\n        sns.kdeplot(test[feat], shade=True, color=\"#40E0D0\", alpha=0.1, ax=axes[row,col])\n        axes[row,col].set(xlabel='', ylabel='')\n        axes[row,col].set_title('Feature: '+str(feat),fontdict= { 'fontsize': 12, 'fontweight':'bold'})\n        n += 1\nplt.tight_layout()","a3f170fc":"feat_list_set2 = feat_list[20:40]\n\nfig, axes = plt.subplots(5, 4,figsize=(20, 20))\n\nn = 0\nsns.despine()\nfor row in range(5):\n    for col in range(4):\n        feat = feat_list_set2[n] \n        sns.kdeplot(train[feat],shade=True, color=\"#FFBF00\", alpha=0.1, ax=axes[row,col])\n        sns.kdeplot(test[feat], shade=True, color=\"#40E0D0\", alpha=0.1, ax=axes[row,col])\n        axes[row,col].set(xlabel='', ylabel='')\n        axes[row,col].set_title('Feature: '+str(feat),fontdict= { 'fontsize': 12, 'fontweight':'bold'})\n        \n        n += 1\nplt.tight_layout()","1ba11538":"feat_list_set3 = feat_list[40:60]\n\nfig, axes = plt.subplots(5, 4,figsize=(20, 20))\n\nn = 0\nsns.despine()\nfor row in range(5):\n    for col in range(4):\n        feat = feat_list_set3[n] \n        sns.kdeplot(train[feat],shade=True, color=\"#FFBF00\", alpha=0.1, ax=axes[row,col])\n        sns.kdeplot(test[feat], shade=True, color=\"#40E0D0\", alpha=0.1, ax=axes[row,col])\n        axes[row,col].set(xlabel='', ylabel='')\n        axes[row,col].set_title('Feature: '+str(feat),fontdict= { 'fontsize': 12, 'fontweight':'bold'})\n        \n        n += 1\nplt.tight_layout()","6a791e0b":"feat_list_set4 = feat_list[60:80]\n\nfig, axes = plt.subplots(5, 4,figsize=(20, 20))\n\nn = 0\nsns.despine()\nfor row in range(5):\n    for col in range(4):\n        feat = feat_list_set4[n] \n        sns.kdeplot(train[feat],shade=True, color=\"#FFBF00\", alpha=0.1, ax=axes[row,col])\n        sns.kdeplot(test[feat], shade=True, color=\"#40E0D0\", alpha=0.1, ax=axes[row,col])\n        axes[row,col].set(xlabel='', ylabel='')\n        axes[row,col].set_title('Feature: '+str(feat),fontdict= { 'fontsize': 12, 'fontweight':'bold'})\n        \n        n += 1\n        \nplt.tight_layout()","1a41381f":"feat_list_set5 = feat_list[80:96]\n\nfig, axes = plt.subplots(5, 3,figsize=(20, 20))\n\nn = 0\nsns.despine()\nfor row in range(5):\n    for col in range(3):\n        feat = feat_list_set5[n] \n        sns.kdeplot(train[feat],shade=True, color=\"#FFBF00\", alpha=0.1, ax=axes[row,col])\n        sns.kdeplot(test[feat], shade=True, color=\"#40E0D0\", alpha=0.1, ax=axes[row,col])\n        axes[row,col].set(xlabel='', ylabel='')\n        axes[row,col].set_title('Feature: '+str(feat),fontdict= { 'fontsize': 12, 'fontweight':'bold'})\n        \n        n += 1\n        \nplt.tight_layout()","06524bf7":"# get data\ntarget_0 = train[train.loss == 0]\ntarget_1 = train[train.loss == 1]\ntarget_2 = train[train.loss == 2]\ntarget_3 = train[train.loss == 3]\ntarget_4 = train[train.loss == 4]","85ab21c0":"corr = train.corr()\n\nf, ax = plt.subplots(figsize=(16, 16))\n\ncmap = sns.diverging_palette(230, 20, as_cmap=True)\n\nmask = np.triu(np.ones_like(corr, dtype=bool))\n\nsns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})","b991ca4a":"# import libraries\nimport optuna \nfrom optuna import Trial, visualization\nfrom optuna.samplers import TPESampler\nimport xgboost as xgb\n\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split","3fe37f7b":"# get data for model\nfeats = train.drop(['id','loss'], axis=1)\ntarget = train['loss']","d6172b9d":"# define function for tuning\ndef objective(trial,data=feats,target=target):\n    \n    train_x, test_x, train_y, test_y = train_test_split(data, target, test_size=0.02,random_state=42)\n    param = {\n        'tweedie_variance_power': trial.suggest_discrete_uniform('tweedie_variance_power', 1.0, 2.0, 0.1),\n        'max_depth': trial.suggest_int('max_depth', 6, 10), # Extremely prone to overfitting!\n        'n_estimators': trial.suggest_int('n_estimators', 400, 4000, 400), # Extremely prone to overfitting!\n        'eta': trial.suggest_float('eta', 0.007, 0.013), \n        'subsample': trial.suggest_discrete_uniform('subsample', 0.2, 0.9, 0.1),\n        'colsample_bytree': trial.suggest_discrete_uniform('colsample_bytree', 0.2, 0.9, 0.1),\n        'colsample_bylevel': trial.suggest_discrete_uniform('colsample_bylevel', 0.2, 0.9, 0.1),\n        'min_child_weight': trial.suggest_loguniform('min_child_weight', 1e-4, 1e4), \n        'reg_lambda': trial.suggest_loguniform('reg_lambda', 1e-4, 1e4), # L2 regularization\n        'reg_alpha': trial.suggest_loguniform('reg_alpha', 1e-4, 1e4), # L1 regularization\n        'gamma': trial.suggest_loguniform('gamma', 1e-4, 1e4),\n        'tree_method':'gpu_hist'\n    } \n    model = xgb.XGBRegressor(**param)  \n    \n    model.fit(train_x,train_y,eval_set=[(test_x,test_y)],early_stopping_rounds=100,verbose=False)\n    \n    preds = model.predict(test_x)\n    \n    rmse = mean_squared_error(test_y, preds,squared=False)\n    \n    return rmse","e506d44b":"# #Optuna tuning - commenting to save time \n# study = optuna.create_study(direction='minimize')\n# study.optimize(objective, n_trials=50)\n# print('Number of finished trials:', len(study.trials))\n# print('Best trial:', study.best_trial.params)","50cdc7c9":"# %%time\n# # train XGB model with best parameters from Optuna\n# xgb_best_params = {'lambda': 0.014133915031135962, 'alpha': 0.1247308992037035, 'colsample_bytree': 0.8, \n#                'subsample': 0.7, 'learning_rate': 0.009, 'n_estimators': 2000, 'max_depth': 11, 'random_state': 48, \n#                'min_child_weight': 114,'tree_method':'gpu_hist'}\n\n# # fit model with Optuna best parameters\n# train_x, val_x, train_y, val_y = train_test_split(feats, target, test_size=0.2,random_state=42)\n\n# xgb_model = xgb.XGBRegressor(**xgb_best_params)\n# xgb_model.fit(train_x, train_y,\n#               eval_set=[(train_x, train_y), (val_x, val_y)],\n#               eval_metric=\"rmse\",\n#               early_stopping_rounds=100,\n#               verbose=False)","dd9ad5af":"# Optuna objective function for Catboost\nfrom catboost import CatBoostRegressor\ndef objective(trial,data=feats,target=target):\n    \n    train_x, test_x, train_y, test_y = train_test_split(data, target, test_size=0.2,random_state=42)\n    param = {'iterations':trial.suggest_int(\"iterations\", 1000, 20000),\n              'od_wait':trial.suggest_int('od_wait', 500, 2000),\n              'task_type':\"GPU\",\n              'eval_metric':'RMSE',\n              'learning_rate' : trial.suggest_uniform('learning_rate', 0.03 , 0.04),\n              'reg_lambda': trial.suggest_loguniform('reg_lambda', 0.32 , 0.33),\n              'subsample': trial.suggest_uniform('subsample',0.9,1.0),\n              'random_strength': trial.suggest_uniform('random_strength',10,50),\n              'depth': trial.suggest_int('depth',1,15),\n              'min_data_in_leaf': trial.suggest_int('min_data_in_leaf',1,30),\n              'leaf_estimation_iterations': trial.suggest_int('leaf_estimation_iterations',1,15),\n             'task_type': 'GPU'\n               }\n    \n    model = CatBoostRegressor(**param)  \n    \n    model.fit(train_x,train_y,eval_set=[(test_x,test_y)],early_stopping_rounds=200,verbose=False)\n    \n    preds = model.predict(test_x)\n    \n    rmse = mean_squared_error(test_y, preds,squared=False)\n    \n    return rmse","65a0b171":"# commenting out since we know the parameters\n# study = optuna.create_study(direction='minimize')\n# study.optimize(objective, n_trials=50)\n# print('Number of finished trials:', len(study.trials))\n# print('Best trial:', study.best_trial.params)","b2695996":"# fit catboost model with best trial\n# best_trial_catboost = {'l2_leaf_reg': 3.8375761158656756, 'max_bin': 345, \n#                        'learning_rate': 0.008643325596038125, 'n_estimators': 10000, 'max_depth': 10, 'random_state': 2020, \n#                        'min_data_in_leaf': 85,'task_type':'GPU'}\n\n# catboost_model = CatBoostRegressor(**best_trial_catboost)\n\n\n# train_x, val_x, train_y, val_y = train_test_split(feats, target, test_size=0.2,random_state=42)\n\n# catboost_model.fit(train_x,train_y,eval_set=[(val_x,val_y)],early_stopping_rounds=200,verbose=False)","a87e738e":"from lightgbm import LGBMRegressor\n\ndef objective(trial,data=feats,target=target):\n    \n    train_x, test_x, train_y, test_y = train_test_split(data, target, test_size=0.2,random_state=42)\n    params = {\n        'reg_alpha' : trial.suggest_loguniform('reg_alpha' , 0.47 , 0.5),\n        'reg_lambda' : trial.suggest_loguniform('reg_lambda' , 0.32 , 0.33),\n        'num_leaves' : trial.suggest_int('num_leaves' , 50 , 70),\n        'learning_rate' : trial.suggest_uniform('learning_rate' , 0.03 , 0.04),\n        'max_depth' : trial.suggest_int('max_depth', 30 , 40),\n        'n_estimators' : trial.suggest_int('n_estimators', 100 , 6100),\n        'min_child_weight' : trial.suggest_loguniform('min_child_weight', 0.015 , 0.02),\n        'subsample' : trial.suggest_uniform('subsample' , 0.9 , 1.0), \n        'colsample_bytree' : trial.suggest_loguniform('colsample_bytree', 0.52 , 1),\n        'min_child_samples' : trial.suggest_int('min_child_samples', 76, 80),\n        'metric' : 'rmse',\n        'device_type' : 'gpu',\n    }\n    model = LGBMRegressor(**params, random_state=2021)  \n    \n    model.fit(train_x, train_y,eval_set=[(test_x,test_y)], early_stopping_rounds=150, verbose=False)\n    \n    preds = model.predict(test_x)\n    \n    rmse = mean_squared_error(test_y, preds,squared=False)\n    \n    return rmse","f54eb4bd":"# commenting out since we know the parameters\n# study = optuna.create_study(direction='minimize')\n# study.optimize(objective, n_trials=50)\n# print('Number of finished trials:', len(study.trials))\n# print('Best trial:', study.best_trial.params)","8dd2598b":"from lightgbm import LGBMRegressor\nfrom xgboost import XGBRegressor\nfrom catboost import CatBoostRegressor","4df239ad":"# best parameters based on Optuna tuning\nxgboost_best_trial = {'max_depth': 10, 'n_estimators': 3200, 'eta': 0.012940730944195646, \n                      'subsample': 0.6000000000000001, 'colsample_bytree': 0.4, 'colsample_bylevel': 0.8, \n                      'min_child_weight': 0.9447548982540029, 'reg_lambda': 0.003408075115131724, 'reg_alpha': 62.732265933421566, \n                      'gamma': 24.458977256371472,'tree_method':'gpu_hist'}\n\ncatBoost_best_trial = {'iterations': 9103, 'od_wait': 1040, 'learning_rate': 0.030857734253353943, 'reg_lambda': 0.3227059153305604, \n                       'subsample': 0.9222810313136256, 'random_strength': 41.37914819936549, 'depth': 11, 'min_data_in_leaf': 8, \n                       'leaf_estimation_iterations': 12,'task_type': 'GPU'}\n\n\nlgbm_best_trial = {'reg_alpha': 0.4865035063664912, 'reg_lambda': 0.32183129255516457, 'num_leaves': 66, 'learning_rate': 0.03383743020793445, 'max_depth': 35, \n                   'n_estimators': 3786, 'min_child_weight': 0.015685452453452698, 'subsample': 0.9384580741116457, 'colsample_bytree': 0.7819836028701871, \n                   'min_child_samples': 77,'device_type' : 'gpu'}\n\n\nxgb_model = XGBRegressor(**xgboost_best_trial)\ncat_model = CatBoostRegressor(**catBoost_best_trial)\nlgb_model = LGBMRegressor(**lgbm_best_trial)","d7eb1c24":"feature_names =  feats.columns.to_list()","fd0cf75c":"# %%time - takes 8 min to run\n# we will use Boruta with three regressors and then select the best features - all models\nfrom boruta import BorutaPy\n\ntrain_x, test_x, train_y, test_y = train_test_split(feats, target, test_size=0.2,random_state=42)\n\nboruta_selector_xgb = BorutaPy(xgb_model, n_estimators = 'auto', random_state = 0)\nboruta_selector_xgb.fit(np.array(train_x),np.array(train_y))","f57e3ce3":"# create a XGB boruta ranking df\nboruta_ranking_xgb = boruta_selector_xgb.ranking_\n\nboruta_ranking_xgb_df = pd.DataFrame(data=boruta_ranking_xgb, index=train_x.columns.values, columns=['values'])\nboruta_ranking_xgb_df['Variable'] = boruta_ranking_xgb_df.index\nboruta_ranking_xgb_df.sort_values(['values'], ascending=True, inplace=True)","116f0101":"# boruta for cat boost\n# boruta_selector_cb = BorutaPy(cat_model, n_estimators = 'auto', random_state = 0)\n# boruta_selector_cb.fit(np.array(train_x),np.array(train_y))","c8b051e7":"boruta_selector_lgb = BorutaPy(lgb_model, n_estimators = 'auto', random_state = 0)\nboruta_selector_lgb.fit(np.array(train_x),np.array(train_y))\n\n# create a LGBM boruta ranking df\nboruta_ranking_lgb = boruta_selector_lgb.ranking_\n\nboruta_ranking_lgb_df = pd.DataFrame(data=boruta_ranking_lgb, index=train_x.columns.values, columns=['values'])\nboruta_ranking_lgb_df['Variable'] = boruta_ranking_lgb_df.index\nboruta_ranking_lgb_df.sort_values(['values'], ascending=True, inplace=True)","3b22c4ca":"# top features - where boruta ranked 1 or 2\nboruta_ranking_xgb_df = boruta_ranking_xgb_df.rename(columns={'values':'Value'})\nboruta_ranking_lgb_df = boruta_ranking_lgb_df.rename(columns={'values':'Value'})\n\nboruta_ranking_xgb_select = boruta_ranking_xgb_df[boruta_ranking_xgb_df.Value <= 2]\nboruta_ranking_lgb_select = boruta_ranking_lgb_df[boruta_ranking_lgb_df.Value <= 2]","b89caddb":"# plot feature importance for XGB and LGB\nfig = make_subplots(rows=1, cols=2, subplot_titles=(\"Feature Importance - XGB\", \"Feature Importance - LGBM\"),horizontal_spacing=0.05)\n\nfig.add_trace(go.Bar(x = boruta_ranking_xgb_select.Value[::-1], y=boruta_ranking_xgb_select['Variable'][::-1], orientation='h',\n                    text = boruta_ranking_xgb_select.Value[::-1], textposition='outside', marker_color='#FFBF00'),row=1,col=1)\n\nfig.add_trace(go.Bar(x = boruta_ranking_lgb_select.Value[::-1], y=boruta_ranking_lgb_select['Variable'][::-1], orientation='h',\n                    text =boruta_ranking_lgb_select.Value[::-1], textposition='outside', marker_color='#40E0D0'),row=1,col=2)\n\nfig.update_layout(font=dict(family='Arial'),\n                  paper_bgcolor='white',\n                  plot_bgcolor = '#FFFAFA',\n                 showlegend=False,height=900)\nfig.update_xaxes(showline=True, linewidth=2, linecolor='black')\nfig.update_yaxes(showline=True, linewidth=2, linecolor='black')\n\nfig.show()\nplt.savefig('Target Values.png')","9879275a":"# predict test data\n# test_model = test[feat_req]\n\n# predictions = catboost_model.predict(test_model)","44e14124":"# make submission file\n# sub = pd.read_csv('\/kaggle\/input\/tabular-playground-series-aug-2021\/sample_submission.csv')\n# sub['loss'] = predictions","1a7b2340":"# save submission file\n# sub.to_csv('submission.csv', index=False)","d12f3d73":"The above plot shows the ranked feature importances based on Boruta. There is a significant difference in between the LGBM and XGB models. As a first step I will use thge LGBM ranked features since I trust the model more based on the baseline results.","821efae5":"### 3.2 Feature Distribution\n\nWe will compare the feature distribution for the train and test data. Of course, we will do this only for the 95 features that are float type. Yellow shows train data and green indicates test data.","75eae323":"This Notebook is my attempt at the Kaggle Monthly challenge for August.","10b2d44a":"### 3.1 Unique Values\nThe unique values for the integer features were evaluated for starters.\n\n**Findings**\n* Features f1 and f86 have the least number of unique values and can be converted to a classification column.\n* The distribution between the train and test data for unique values for the integer features is comparable.","09a34ee3":"## 4.Models\n\nThis section is all about modeling. Since there is not much of data cleaning involved, we can jump right in. There are a few things I would like to try:\n* XGBoost\n* CatBoost\n* LGBM\n* Hyperparameter Tuning with Optuna - since I have never done it before\n\nThis is the plan:\n- Run XGBoost, CatBoost and LGBM with all features\n- Get best parameters using Optuna\n- Based on the best parameters run all three models and pick the top features using Boruta\n- Voting regressor with all the three models based on the best estimator and the best features\n\nAt some point in time I would like to try auto-ML as well.\n\nIf these work then well and good. Otherwise I am not going to sweat it.","8ba04d58":"### 4.3 LightGBM","a7f5f53f":"## 5. Predict & Submit Baseline Model","38ebc410":"### 3.3 Feature Correlation","96ba1ea1":"### 4.2 Catboost","55bd61bf":"### 4.4 Feature Selection with Boruta\n\nBoruta has shown recent success in removing noisy features. And since we have no information on the features, it may not a bad idea to select useful features using the Optuna Optimized parameters.","6018f80d":"### 2. Target (Loss) Variable\n\n**Summary**\n* The target variable has 43 unique values i.e. from 0 - 42. Interesting!\n* Target value '0' constitutes approximately 25% of the data. \n* And values 0,1,2,3,4 make up for 50% of the dataset the datatset. It may be worthwhile to explore the features for these values separately.\n\nIt is very tempting to look at the problem from a classification point of view. But since the evaluation metric is RMSE we have to stick with building regression models.","bd6fad93":"### 3. Features\n\nWe will explore the features to further understand the data.","36fcb0cb":"**Keep Learning and have fun!**","0e4d36d1":"### 4.1 XGBoost with Optuna","fb383664":"Overall, the features are distributed very identically between the train and test data.\n","c67c65b8":"Looks like none of the features are correlated.\n\nOkay then lets start modeling....","a9eca2ab":"### 1. Data Overview\n\nThis section provides a summary of the data.\n\n**Summary**\n* Total number of observations in the train data is 250,000. Observations to be predicted on (test data) is 150,000.\n* Luckily, there are no values in both the train and test data set. We can spend more time on the actual model building.\n* Overall, there are 101 features (named from f0 to f99) that we will be used to predict the target variable (loss). Includes an 'id' column of type integer, which from a cursory look is just a row identifier.\n* Of the 100 features, not including the id column, 95 are float and 5 are integers.\n* The target variable (loss) is an integer variable."}}