{"cell_type":{"3722ed63":"code","2d105006":"code","fa770731":"code","089171ef":"code","461d3554":"markdown","3dbc7efc":"markdown","9dc06019":"markdown","bc1b189d":"markdown","8e44342c":"markdown","439e082a":"markdown"},"source":{"3722ed63":"import pandas as pd\nfrom sklearn.datasets import load_boston\nimport statsmodels.api as sm\nboston=load_boston()\nX = pd.DataFrame(data=boston.data,columns=boston.feature_names)\nY = pd.DataFrame(data=boston.target,columns=['MEDV'])","2d105006":"corr=X.corr()\ncorr.style.background_gradient(cmap='coolwarm')","fa770731":"numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n\nnewdf = X.select_dtypes(include=numerics)\nnewdf.fillna(newdf.mean);","089171ef":"from statsmodels.stats.outliers_influence import variance_inflation_factor \nX=sm.add_constant(X)\n# VIF dataframe \nvif_data = pd.DataFrame() \nvif_data[\"feature\"] = X.columns \n\n# calculating VIF for each feature \nvif_data[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(len(X.columns))] \n\nprint(vif_data)\n","461d3554":"We get a value Infinity when $R^2$ is 1, indicating perfect multicollinearity. \nAs we see RAD, TAX are highly correlated with the other variables.  ","3dbc7efc":"One approach may be the removal of regressors that are correlated.  Another may be principal component analysis or PCA.\n","9dc06019":"# Multicollinearity\nMulticollinearity is the occurrence of high intercorrelations among two or more independent variables in a multiple regression model. \nIn general, multicollinearity can lead to wider confidence intervals that produce less reliable probabilities in terms of the effect of independent variables in a model. That is, the statistical inferences from a model with multicollinearity may not be dependable.\nIt makes it hard for interpretation of model and also creates overfitting problem. When independent variables are highly correlated, change in one variable would cause change to another and so the model results fluctuate significantly. The model results will be unstable and vary a lot given a small change in the data or model.\n","bc1b189d":"#### 1. Variance Inflation Factor (VIF)\nIt is a measure of multi-collinearity in the set of multiple regression variables. The higher the value of VIF the higher correlation between this variable and the rest.\nIn VIF method, we pick each feature and regress it against all of the other features. For each regression, the factor is calculated as :\n$$ VIF=\\frac{1}{1-R^2}$$\nGenerally, a VIF above 5 indicates a high multicollinearity. ","8e44342c":"As we can see the features \"Tax\" and \"Indus\" are highly correlated, \"Age\" and \"Nox\" are highly correlated.","439e082a":"### How to check for Multicollinearity:\n#### 1. Checking Correlation Matrix\n"}}