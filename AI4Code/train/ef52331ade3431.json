{"cell_type":{"9b13eb38":"code","ca997004":"code","243384b0":"code","4046b75e":"code","2101297a":"code","e7274143":"code","0c61695c":"code","357a8a19":"code","36259559":"code","ce4a6ae0":"code","9ae41e3c":"code","dd8d171c":"code","f8b66e31":"code","7823b24c":"code","2cf49da2":"code","a19799bf":"code","3aa685de":"code","934a14e4":"code","c81934f4":"code","2729aa85":"code","0a0285b5":"code","844b0801":"code","8a1d7725":"code","ed2a92ed":"code","9c3d1276":"markdown","4d70dcbb":"markdown","15979277":"markdown","066401cd":"markdown","ef3b7cfa":"markdown","9cf43785":"markdown","48599286":"markdown","f3d9d5bb":"markdown","a7540921":"markdown","62eeff37":"markdown","55594fb6":"markdown"},"source":{"9b13eb38":"#import libraries\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%pylab inline\npylab.rcParams['figure.figsize'] = (10, 6)","ca997004":"# Load the data\nimport os\ntest_df = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')\ntrain_df = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')","243384b0":"#Let's have a look at the data we have\ntrain_df.head()","4046b75e":"# Some columns don't show above so let's have a look at the list of columns\nprint('There are', len(train_df.columns),'columns.')","2101297a":"#Let's look at the number of value per column\nnull_in_train_csv = train_df.isnull().sum()\nnull_in_train_csv = null_in_train_csv[null_in_train_csv > 0]\nnull_in_train_csv.sort_values(inplace=True)\nnull_in_train_csv.plot.bar();","e7274143":"sns.heatmap(train_df.corr(), vmax=1, square=True);","0c61695c":"arr_train_cor = train_df.corr()['SalePrice']\nidx_train_cor_gt0 = arr_train_cor[arr_train_cor > 0].sort_values(ascending=False).index.tolist()\narr_train_cor[idx_train_cor_gt0]","357a8a19":"features = ['SalePrice','OverallQual', 'GrLivArea', 'GarageCars', 'TotalBsmtSF', 'FullBath', 'YearBuilt', 'Fireplaces']\ntrain_features = train_df[features].copy()","36259559":"for feature in features:\n    train_features[train_features[feature].isnull()]=train_features[feature].median()","ce4a6ae0":"sns.pairplot(train_features)","9ae41e3c":"train_no_outliers = train_features.copy()\n# Year Built\ntrain_no_outliers = train_no_outliers.drop(train_features[train_features[\"YearBuilt\"] < 1600].index)\n# TotalBsmtSF\ntrain_no_outliers = train_no_outliers.drop(train_features[train_features[\"TotalBsmtSF\"] > 5000].index)\n#GrLivArea;\nto_remove = train_features[(train_features['GrLivArea'] > 4000) & (train_features['SalePrice'] < 200000)].index.tolist()\ntrain_no_outliers = train_no_outliers.drop(to_remove[0])\ntrain_no_outliers = train_no_outliers.drop(to_remove[1]+1)\n","dd8d171c":"sns.pairplot(train_no_outliers)","f8b66e31":"train_df = train_no_outliers.copy()\ndummies = ['FullBath', 'OverallQual', 'Fireplaces', 'GarageCars']\nfor dummy in dummies:\n    dum = pd.get_dummies(train_no_outliers[dummy], prefix=dummy).iloc[:,1:]\n    train_df = pd.concat([train_df, dum], axis=1)\ntrain_df = train_df.drop(columns=dummies)\ntrain_df['GarageCars_5'] = 0\ntrain_df['Fireplaces_4'] = 0","7823b24c":"from sklearn.model_selection import train_test_split\nrandom_state = 7\n\nyt_train, yt_valid, xt_train, xt_valid = train_test_split(train_no_outliers['SalePrice'], train_no_outliers.drop('SalePrice', axis=1), test_size=.2, random_state=random_state)\nfrom sklearn.utils import shuffle\n\n\nxt_train = np.array(xt_train)\nyt_train = np.array(yt_train)\nxt_valid = np.array(xt_valid)\nyt_valid = np.array(yt_valid)\n\nxt_train, yt_train= shuffle(xt_train, yt_train) ","2cf49da2":"print(\"number of training set: %d\\nnumber of validation set: %d\\ntotal: %d\" % (len(xt_train), len(xt_valid), (len(xt_train)+len(xt_valid))))","a19799bf":"def rmse(arr1, arr2):\n    return np.sqrt(np.mean((arr1-arr2)**2))","3aa685de":"from sklearn.linear_model import LinearRegression\n\nlm = LinearRegression()\nlm.fit(xt_train, yt_train)\nyd_lm = lm.predict(xt_valid)\nrmse_linear = rmse(yt_valid, yd_lm)\nsns.regplot(yt_valid, yd_lm)\nprint(\"RMSE for Linear Regression Model in sklearn: %.2f\" % rmse_linear)","934a14e4":"import xgboost as xgb\nfrom xgboost import plot_importance\nparams = {\n    'booster': 'gbtree',\n    'objective': 'reg:gamma',\n    'gamma': 0.1,\n    'max_depth': 5,\n    'lambda': 3,\n    'subsample': 0.7,\n    'colsample_bytree': 0.7,\n    'min_child_weight': 3,\n    'silent': 1,\n    'eta': 0.1,\n    'seed': 1000,\n    'nthread': 4,\n}\n\nplst = list(params.items())\ndtrain = xgb.DMatrix(xt_train, yt_train)\ndtest = xgb.DMatrix(xt_valid)\nnum_rounds = 500\nxgb_model = xgb.train(plst, dtrain, num_rounds)\nyt_xgb = xgb_model.predict(dtest)\nrmse_xgb = rmse(yt_valid, yt_xgb)\nsns.regplot(yt_valid, yt_xgb)\nprint(\"RMSE for xgboost: %.2f\" % rmse_xgb)","c81934f4":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.layers import Dropout, BatchNormalization\nimport tensorflow as tf\nprint(tf.version.VERSION)\nfrom kerastuner.tuners import Hyperband\n\nfrom kerastuner import HyperModel\n\n\nclass MyHyperModel(HyperModel):\n\n    def __init__(self):\n        return\n        \n    def build(self, hp):\n        model = Sequential()\n        model.add(tf.keras.Input(shape=(7,)))\n        #for i in range(hp.Int('num_layers', 2, 10)):\n        for i in range(hp.Int('num_layers', 2, 4)):\n            model.add(BatchNormalization())\n            model.add(Dense(units=hp.Choice(\n                'units',\n                 values=[32, 64, 128, 256, 512, 1024],\n                 default=128\n            ),\n            #activation=hp.Choice(\n            #    'dense_activation',\n            #     values=['relu', 'tanh', 'sigmoid'],\n            #     default='relu'\n            #)\n            activation='relu'\n                     )\n            )\n            model.add(Dropout(hp.Choice(\n                'drop_out',\n                 values=[0.0,0.2, 0.3,0.5 ],\n                 default=0.5\n            )))\n        model.add(Dense(1))\n        \n        hp_learning_rate = hp.Choice('learning_rate', values = [1e-1, 1e-2, 1e-3, 1e-4])\n        model.compile(optimizer=Adam(learning_rate = hp_learning_rate), loss='mse')\n\n        return model\n    \nhypermodel = MyHyperModel()\n\ntuner = Hyperband(\n    hypermodel,\n    objective='val_loss',\n    overwrite=True,\n    max_epochs=400,)\ntuner.search(xt_train, yt_train,\n             epochs=400,\n             validation_data=(xt_valid, yt_valid))\ntuner.search_space_summary()\nmodel = tuner.get_best_models(num_models=1)[0]\n\nxt_train = np.array(xt_train)\nyt_train = np.array(yt_train)\n    \n\n\nfrom tensorflow.keras.callbacks import EarlyStopping\nearly_stop = EarlyStopping(monitor='val_loss', mode='min', patience=40, restore_best_weights=True)\nmodel.fit(x=xt_train,y= yt_train,\n          verbose=0,\n          batch_size=64,\n          validation_data=(xt_valid,yt_valid),epochs=400, callbacks=[early_stop])\nyt_mlp= model.predict(xt_valid)\nyt_mlp = np.reshape(yt_mlp, (yt_mlp.shape[0]))\nrmse_mlp = rmse(yt_valid, yt_mlp)\nprint(\"RMSE for MLP: %.2f\" % rmse_mlp)\nsns.regplot(yt_valid, yt_mlp)\n","2729aa85":"model.summary()","0a0285b5":"idx_clean_final = features.copy()\nidx_clean_final.remove('SalePrice')\nfinal_clean = test_df[idx_clean_final]\nfinal_clean = final_clean.fillna(0)\nfinal_clean.head(n=5)","844b0801":"yt_final = model.predict(final_clean)\nyt_final =  np.reshape(yt_final, (yt_final.shape[0]))\nsummission = pd.concat([test_df['Id'], pd.DataFrame(yt_final)], axis=1)\nsummission.columns = ['Id', 'SalePrice']\nsummission.head(n=5)","8a1d7725":"sns.distplot(np.array(summission['SalePrice']))","ed2a92ed":"summission.to_csv('summission.csv', encoding='utf-8', index = False)","9c3d1276":"# Feature selection\nIn this part, we'll have a look at the features we have and try to establish which one are relevant in order to obtain the sale price.","4d70dcbb":"We'll now look for the empty cells and fill them with the median value of each column.","15979277":"Based on the correlation with sale price and their low inter-correlation, let's chose the following features :\n* **Overall Quality**\n* **GrLivArea**\n* **Garage Cars**\n* **Total Basement Surface**\n* **Construction Year**\n* **FullBath**\n* **Masonry veneer area**\n* **Fireplaces**","066401cd":"# Training","ef3b7cfa":"We can see there are a few outliers, values that are extreme and might be wrong, for example, we can see a zero in the year construction. Those extreme values can have a negative impact on training, that's why it's better to remove them.","9cf43785":"The higher the value, the stronger the correlation. Thanks to this heatmap, we can easily see which variables are correlated, for example the year of construction of the house's garage is strongly linked to the year of construction of the house itself. Let's have a look at the variables that have a positive correlation to the sale price.","48599286":"We can see that some columns are almost empty. We'll now check the correlation map.","f3d9d5bb":"# Regression For House Price Estimation\n\nIn this notebook, I'll try solving the problem of house price estimation given by the Kaggle challenge *House Prices: Advanced Regression Techniques*.\n\nLet's start by importing some useful librairies.","a7540921":"We also note that some of the graphs display parallel lines of points, that's because the values of their column aren't not continuous. These type of values aren't really suitable for some regression models, that's why we'll divide those columns into categorical columns called dummy variables.","62eeff37":"That's 80 potential features, that's a lot. We'll try to narrow it using to criterias (based on the tutorial cited in the references):\n- Not too many null values\n- Correlated with the sale price","55594fb6":"The score obtained by the submission file is around **0.16**.\n# References\n* https:\/\/www.kaggle.com\/yiidtw\/a-complete-guide-for-regression-problem\n* https:\/\/www.kaggle.com\/pmarcelino\/comprehensive-data-exploration-with-python\n* https:\/\/www.kaggle.com\/serigne\/stacked-regressions-top-4-on-leaderboard\n* https:\/\/www.youtube.com\/watch?v=0s_1IsROgDc\n* https:\/\/www.youtube.com\/watch?v=9yTui_LoSOc&t=242s\n* https:\/\/www.youtube.com\/watch?v=8vwSf0_MYU0&t=179s"}}