{"cell_type":{"04a64864":"code","19b02395":"code","78c9de92":"code","4e6b04e8":"code","90ce9104":"code","cefcc26f":"code","7ff6aac7":"code","6d665794":"code","3f05610b":"code","f63de36b":"code","9e4d2f77":"code","af87cb16":"code","6afe25e1":"code","f242c739":"code","2470392f":"code","82431218":"code","1b796508":"code","8b252b6b":"code","96b44ef5":"code","8ce08cb2":"code","2f87c1b6":"code","d3789815":"code","3cda524d":"code","5fc45daa":"code","6a70d761":"code","0fe21698":"code","d71ef93f":"code","1f7cd1d3":"code","2219c042":"code","2a35aba1":"code","44c6b3bf":"code","00520707":"code","00f30ba5":"code","7b5d2290":"code","56a2ee44":"code","f4eb5631":"code","1db4d472":"code","50a17882":"code","a1164197":"markdown","c782b7cb":"markdown","c6c6e4a7":"markdown","839b4afc":"markdown","3bb13490":"markdown","50fc937f":"markdown","17d7e2ef":"markdown","f0b7911d":"markdown","730a7830":"markdown","a6b86e37":"markdown","157d0476":"markdown","51133411":"markdown","338b411e":"markdown","4eedd806":"markdown","a9d0542c":"markdown","a027414c":"markdown","5f10afb0":"markdown","0dd86f16":"markdown","4e46ee07":"markdown","93f3048f":"markdown","d734a230":"markdown","b1b50c02":"markdown","a680649c":"markdown","a9c771fd":"markdown","9bedc5e1":"markdown","1fd073d9":"markdown","04603c39":"markdown","0a6c945c":"markdown"},"source":{"04a64864":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt # plotting\nimport numpy as np # linear algebra\nimport os # accessing directory structure\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","19b02395":"data=pd.read_csv('..\/input\/mmm111\/kc_house_data.csv')\ndata","78c9de92":"data = data.dropna() \ndata\n","4e6b04e8":"pd.DataFrame(data.isna().sum()).T","90ce9104":"data = data.drop_duplicates()\nprint(data)","cefcc26f":"data","7ff6aac7":"print(data.columns)\n","6d665794":"data.drop(['id','date', 'zipcode'], axis=1, inplace=True)\ndata ","3f05610b":"print(data.columns)","f63de36b":"# generate related variables\nfrom numpy import mean\nfrom numpy import std\nfrom numpy.random import randn\nfrom numpy.random import seed\nfrom matplotlib import pyplot\n\ncorr = data.corr()\nfig = plt.figure()\nax = fig.add_subplot(111)\ncax = ax.matshow(corr,cmap='coolwarm', vmin=-0.8, vmax=0.8)\nfig.colorbar(cax)\nticks = np.arange(0,len(data.columns),1)\nax.set_xticks(ticks)\nplt.xticks(rotation=90)\nax.set_yticks(ticks)\nax.set_xticklabels(data.columns)\nax.set_yticklabels(data.columns)\nplt.show()\n","9e4d2f77":"#Spilt my dataset into training and testing sets :\nX = data\ny = data.price\nfrom sklearn.model_selection import train_test_split\nX_train,X_test,Y_train,Y_test = train_test_split(X,y,test_size=0.30,random_state=42)\nprint(\"test dataset size after spliting is : \",X_test.shape)","af87cb16":"def print_score(classifier,X_train,Y_train,X_test,Y_test,train=True):\n    if train == True:\n        print(\"Training results:\\n\")\n        print('Accuracy Score: {0:.4f}\\n'.format(accuracy_score(Y_train,classifier.predict(X_train))))\n        print('Classification Report:\\n{}\\n'.format(classification_report(Y_train,classifier.predict(X_train))))\n        print('Confusion Matrix:\\n{}\\n'.format(confusion_matrix(Y_train,classifier.predict(X_train))))\n        res = cross_val_score(classifier, X_train, Y_train, cv=10, n_jobs=-1, scoring='accuracy')\n        print('Average Accuracy:\\t{0:.4f}\\n'.format(res.mean()))\n        print('Standard Deviation:\\t{0:.4f}'.format(res.std()))\n    elif train == False:\n        print(\"Test results:\\n\")\n        print('Accuracy Score: {0:.4f}\\n'.format(accuracy_score(Y_test,classifier.predict(X_test))))\n        print('Classification Report:\\n{}\\n'.format(classification_report(Y_test,classifier.predict(X_test))))\n        print('Confusion Matrix:\\n{}\\n'.format(confusion_matrix(Y_test,classifier.predict(X_test))))","6afe25e1":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn import model_selection\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score","f242c739":"from sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier as KNN\n\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import GridSearchCV\n#In case of classifier like knn the parameter to be tuned is n_neighbors\nparam_grid = {'n_neighbors':np.arange(1,50)}\n#Setup arrays to store training and test accuracies\nknn = KNeighborsClassifier()\nknn_cv= GridSearchCV(knn,param_grid,cv=5)\nknn_cv.fit(X,y)","2470392f":"from sklearn.naive_bayes import GaussianNB\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import train_test_split\ndef gnb_clf():\n    model = GaussianNB()\n    scores = cross_val_score(model, X, y, cv=5)\n    return scores.mean()\nprint(gnb_clf())\n\n","82431218":"from sklearn.tree import DecisionTreeClassifier\ndef tree_clf(max_depth):\n    model = DecisionTreeClassifier(max_depth=max_depth)\n    scores = cross_val_score(model, X, y, cv=50)\n    return scores.mean()\nprint(tree_clf(max_depth=50))\n","1b796508":"from sklearn.model_selection import train_test_split\nfrom matplotlib import pyplot as plt\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.linear_model import Ridge\nfrom sklearn.neighbors import KNeighborsClassifier  \nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB    #added myself (https:\/\/machinelearningmastery.com\/compare-machine-learning-algorithms-python-scikit-learn\/)\nfrom sklearn import model_selection\nfrom sklearn.metrics import classification_report, accuracy_score\nfrom sklearn.linear_model import SGDRegressor\nX = data.iloc[:,0:1] # attrbuites\ny = data.iloc[:,1] # Class \nregr = LinearRegression()\nregr.fit(X_train, Y_train)\ny_pred = regr.predict(X_test)\nprint('Coefficients: \\n', regr.coef_)\nprint('intercept: \\n', regr.intercept_)\nprint(\"Liner equation:\", regr.coef_ ,\"X + \", regr.intercept_)\nfrom sklearn.metrics import mean_squared_error\nprint(\"Mean squared error: %.2f\" % mean_squared_error(Y_test, y_pred))\n\n\n","8b252b6b":"import matplotlib.pyplot as plt\nfrom matplotlib import pylab\nline = X+regr.intercept_\nplt.plot(X,y,'o', X, line)\npylab.title('Linear Fit with Matplotlib')\nax = plt.gca()\n#ax.set_axis_bgcolor((0.898, 0.898, 0.898))\nfig = plt.gcf()\n","96b44ef5":"import pandas as pd\nfrom sklearn import  linear_model\n\nX = data.iloc[:,0:13] # attrbuites\ny = data.iloc[:,13] # Class \n#X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.4,random_state=0)\nregr = linear_model.LinearRegression()\nregr.fit(X_train, Y_train)\n# Make predictions using the testing set\ny_pred = regr.predict(X_test)\n","8ce08cb2":"#  The coefficients\nprint('Coefficients: \\n', regr.coef_)\nprint('intercept: \\n', regr.intercept_)\n\n# The mean squared error\nfrom sklearn.metrics import mean_squared_error\nprint(\"Mean squared error: %.2f\" % mean_squared_error(Y_test, y_pred))\n","2f87c1b6":"X = np.asarray(list(range(len(data))))[:, np.newaxis]\ny = data.price\n\nimport matplotlib.pyplot as plt\nplt.style.use('fivethirtyeight')\ndata.plot.line(marker=\"o\", linewidth=0, color='black', title='Brazil Wins or Ties Per World Cup, 1930-2016', figsize=(12, 6))","d3789815":"import numpy as np\nrng = np.random.RandomState(0)\n\nX_p = 5 * rng.rand(100, 1)\ny_p = np.sin(X_p).ravel()\n\ny_p[::5] += 3 * (0.5 - rng.rand(X_p.shape[0] \/\/ 5))\n\nfig, ax = plt.subplots(1, figsize=(12, 6))\nplt.plot(X_p[:, 0], y_p, marker='o', color='black', linewidth=0)\nplt.suptitle(\"Synthetic Polynomial Data\")","3cda524d":"pip install lmfit ","5fc45daa":"\nimport matplotlib.pyplot as plt\n\n  \nX =data\n  \n## You can adjust the slope and intercept to verify the changes in the graph \n  \ny = np.power(X, 2) \ny_noise = 2 *y\nydata = y + y_noise \nplt.plot(X, y,  'bo') \nplt.plot(X, y, 'r')  \nplt.ylabel('Dependent Variable') \nplt.xlabel('Indepdendent Variable') \nplt.show()","6a70d761":"!pip install matplotlib","0fe21698":"from copy import deepcopy\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom matplotlib import pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.cluster import KMeans\nimport pandas as pd\nimport numpy as np\nfrom itertools import cycle, islice\nimport matplotlib.pyplot as plt\nfrom pandas.plotting import parallel_coordinates\nfrom sklearn.datasets import make_blobs\nfrom copy import deepcopy\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy.cluster.hierarchy import dendrogram, ward\nfrom scipy.cluster.hierarchy import fcluster\nfrom sklearn.metrics import silhouette_samples\nfrom matplotlib import cm\n","d71ef93f":"from sklearn.cluster import KMeans\nimport numpy as np\nimport pandas as pd\nfrom sklearn import metrics\ndef purity_score(y_true, y_pred,label):\n    contingency_matrix = metrics.cluster.contingency_matrix(y_true, y_pred)\n    return np.sum(np.amax(contingency_matrix[label], axis=0)) \/ np.sum(contingency_matrix[label]) \n\nX = data\nY=data.price\nK = 3\nkmeans_model = KMeans(n_clusters=K).fit(X)\nprint(\"Labels of points\")\nprint(kmeans_model.labels_)\nprint(\"Center of cluster\")\nprint(kmeans_model.cluster_centers_)\nfor i in range(K):    \n    purity=purity_score(Y, kmeans_model.labels_,i)\n    print(\"purity of label \" + str(i) +\"  is \"+ str(purity) )\n","1f7cd1d3":"numer = set(data.corr()['price'].index)\ncateg = list(set(data.columns) - set(data.corr()['price'].index))","2219c042":"data_targ = data.copy()\nfor i in categ:\n    data_targ[i] = data_targ[i].factorize()[0]\ndata_targ.head(3)","2a35aba1":"from sklearn import preprocessing\nfrom sklearn.preprocessing import Normalizer,MinMaxScaler, RobustScaler\ndata_targ.drop(['price'],axis=1,inplace=True)\nnrm = Normalizer()\nnrm.fit(data_targ)\nnormal_data = nrm.transform(data_targ)","44c6b3bf":"from sklearn.manifold import TSNE\ntsn = TSNE(random_state=20)\nres_tsne = tsn.fit_transform(normal_data)\nplt.figure(figsize=(8,8))\nsns.scatterplot(res_tsne[:,0],res_tsne[:,1]);","00520707":"most_sign = ['OverallQual','GrLivArea','1stFlrSF','FullBath']\nn_row = 2\nn_col = 2\nfig, axes = plt.subplots(nrows=n_row, ncols=n_col, figsize=(15,15))\nsns.set(font_scale=1)\nsns.axes_style(\"whitegrid\")\nfor i in enumerate(most_sign):\n    qq = sns.scatterplot(x=res_tsne[:,0],y=res_tsne[:,1],ax=axes[i[0]\/\/n_row,i[0]%n_col],\\\n                         s=70,palette=\"RdBu\");\nplt.legend().set_title('')\nplt.tight_layout()","00f30ba5":"sns.set()\nn_row = 5\nn_col = 5\nfig, axes = plt.subplots(nrows=n_row, ncols=n_col, figsize=(20,20))\nsns.set(font_scale=2)\nfor i in enumerate(categ):\n    pd.value_counts(data[i[1]]).plot(kind='barh',ax=axes[i[0]\/\/n_row,i[0]%n_col])\n    axes[i[0]\/\/n_row,i[0]%n_col].set_title(i[1]);\nplt.tight_layout()","7b5d2290":"fig = plt.figure(figsize=(20,10))\nsns.set(font_scale=2)\nax1 = fig.add_subplot(2,3,1)\nax1.set_title('Most popular apps')\ndata['price'].hist(bins=20);\nax2 = fig.add_subplot(2,3,2)\nax2.set_title('All apps')\nsns.boxplot(data['price'])\nplt.tight_layout()","56a2ee44":"print(pd.value_counts(data['price']<350000))\ndata_new = data[data['price']<350000]\ntarget_val = data['price'].values","f4eb5631":"cut_value = pd.cut(data_new['price'],5).values\ndata_new['price'] = cut_value\nsns.set(font_scale=2)\nn_row = 4\nn_col = 4\ncateg_targ = set(categ) - set(['waterfront','zipcode','view'])\nfig, axes = plt.subplots(nrows=n_row, ncols=n_col, figsize=(20,20))\nsns.set(font_scale=2)\nsns.axes_style(\"whitegrid\")\n\nfor i in enumerate(categ_targ):\n    qq=sns.countplot(data_new[i[1]],ax=axes[i[0]\/\/n_row,i[0]%n_col],\\\n                     hue=data_new['price'])\n    qq.legend_.remove()\nplt.legend().set_title('')\nplt.tight_layout()","1db4d472":"link = ward(res_tsne)\nvb = fcluster(link,t=300, criterion='distance')\nfig = plt.figure(figsize=(25,25))\nax1 = fig.add_subplot(3,3,1)\npd.value_counts(vb).plot(kind='barh')\nax2 = fig.add_subplot(3,3,2)\naxpl_2 = sns.scatterplot(x=res_tsne[:,0],y=res_tsne[:,1],hue=vb,palette=\"Set1\");\naxpl_2.legend_.remove()","50a17882":"\nsns.set(style='white')\nplt.figure(figsize=(10,7))\n#link = ward(res_tsne)\ndendrogram(link)\nax = plt.gca()\nbounds = ax.get_xbound()\nax.plot(bounds, [300,300],'--', c='k')\nax.plot(bounds,'--', c='k')\nplt.show()\n","a1164197":"# 2- Apply data cleaning based on your dataset needs","c782b7cb":"\nAssignment 2: Dataset Preparation\n\nWrite Python program that achieve the following goals:","c6c6e4a7":"* Missing handling","839b4afc":"No noise in the data set\n","3bb13490":"Compare the behavior of three classifiers, kNN; Na\u00efve Bayes and Decision Tree, on your own dataset.","50fc937f":"70%  and 30% for training and testing sets respectively","17d7e2ef":"[](http:\/\/)* Remove duplicate records","f0b7911d":"# Remove correlated ","730a7830":"# K-Nearest Neighbor(KNN)","a6b86e37":"# Save each of these sets into separated files","157d0476":"Build  four regression models, as flow, using your regression dataset, and compare between these models in term of RMSE metric.\nSimple linear model ( use any attribute)","51133411":"# Assignment 4 - Regression\n","338b411e":"No noise in the data set","4eedd806":"# Assignment 5 _clustering(house_prices)","a9d0542c":"# Naive Bayes","a027414c":"# 3- Spilt your dataset into training and testing sets","5f10afb0":"1- Load your dataset","0dd86f16":"All fields with missing values \u200b\u200bhave been deleted, but in this data set there are no missing values\n![image.png](attachment:image.png)","4e46ee07":"# Simple Non-linear model ","93f3048f":"* Correlation rate greater than or equal 0.8 for positive correlation\n* Correlation rate less than or equal -0.8 for negative correlation\n* Apply discretization on numeric attributes as possible","d734a230":"The columns were reviewed and three unnecessary columns were removed ('id\u2019 , 'date', \u2018 zipcode\u2019 )\n![image.png](attachment:image.png)","b1b50c02":"# 1. Simple Linear Regression","a680649c":"# Decision Tree","a9c771fd":"# Multiple Non-linear model ","9bedc5e1":"# **\n# 1. 1.  Assignment 3: Test and Evaluate Different Classification Models","1fd073d9":"Duplicate records not found in the dataset\n","04603c39":"# Multiple Regression","0a6c945c":"* Remove noise"}}