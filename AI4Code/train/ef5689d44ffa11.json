{"cell_type":{"061a9256":"code","88ccfe71":"code","a9de4096":"code","c955a015":"code","a6c56cf4":"code","dc9cecbe":"code","8b0c9f6e":"code","130517f8":"code","5dbc7bb4":"code","305ea816":"code","68d25527":"code","79e98719":"code","2c45ae02":"code","b2475481":"code","c7dd4e45":"code","c1c0d739":"code","77559061":"code","b732b13c":"code","68f14ca5":"code","23e70979":"code","045ff446":"code","0756593e":"code","0b168457":"code","df1da989":"code","a462448d":"code","3a9ecf6f":"code","630cb1ea":"code","1369d06d":"code","cd3e3221":"code","a1193021":"code","c1c8daf1":"code","2f42994d":"code","b3d7fe5e":"code","a1b10a36":"markdown"},"source":{"061a9256":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom pandas import plotting\n\n#plotly \nimport plotly.offline as py\nimport plotly.graph_objects as go\nfrom plotly.offline import init_notebook_mode, iplot\nfrom plotly import tools\ninit_notebook_mode(connected=True)\nimport plotly.figure_factory as ff\nimport plotly.express as px\n\nfrom sklearn.preprocessing import StandardScaler\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import preprocessing\nfrom sklearn import neighbors\nfrom sklearn.metrics import confusion_matrix,classification_report,precision_score, plot_roc_curve, plot_precision_recall_curve, balanced_accuracy_score\nfrom sklearn.model_selection import train_test_split\n\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\nsns.set(style=\"whitegrid\")\n\nplt.style.use('fivethirtyeight')\n\nimport warnings\n","88ccfe71":"df=pd.read_csv('..\/input\/gender-classification\/Transformed Data Set - Sheet1.csv')","a9de4096":"df.head()","c955a015":"df['Favorite Color'].value_counts()","a6c56cf4":"df['Favorite Music Genre'].value_counts()","dc9cecbe":"df['Favorite Beverage'].value_counts()","8b0c9f6e":"df['Favorite Soft Drink'].value_counts()","130517f8":"df.isnull().sum()","5dbc7bb4":"target_count = df['Gender'].value_counts()\nprint('Class 0:', target_count[0])\nprint('Class 1:', target_count[1])\nprint('Proportion:', round(target_count[0] \/ target_count[1], 2), ': 1')\n\ntarget_count.plot(kind='bar', title='Count (target)');","305ea816":"from sklearn import preprocessing\nlabel_encoder = preprocessing.LabelEncoder()\ndf['Gender']=label_encoder.fit_transform(df['Gender'])\ndf.head()","68d25527":"df.columns","79e98719":"year_feature=pd.get_dummies(df, columns=['Favorite Color', 'Favorite Music Genre', 'Favorite Beverage','Favorite Soft Drink'])\nyear_feature.head()","2c45ae02":"year_feature.columns","b2475481":"X=year_feature.drop('Gender', axis=1)\ny=year_feature['Gender']","c7dd4e45":"from sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import metrics\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay","c1c0d739":"X_train, X_test, y_train, y_test=train_test_split(X, y, test_size=0.3, random_state=10)\n","77559061":"lr=LogisticRegression()\nlr.fit(X, y)","b732b13c":"import statsmodels.api as sm\nlr_model=sm.Logit(y, X)\nresult=lr_model.fit()\nprint(result.summary())","68f14ca5":"print(lr.intercept_)","23e70979":"y_pred=lr.predict(X_train)\n","045ff446":"print(\"Accuracy Score:-\", metrics.accuracy_score(y_train, y_pred))\nprint(\"F1 Score:-\", metrics.f1_score(y_train, y_pred))\nprint(\"Average Precision Score:-\", metrics.average_precision_score(y_train, y_pred))\nprint(\"Log Loss:-\", metrics.log_loss(y_train, y_pred))\nprint(\"Precision Score:-\", metrics.precision_score(y_train, y_pred))\nprint(\"Recall Score:-\", metrics.recall_score(y_train, y_pred))\nprint(\"ROC-AUC Score:-\", metrics.roc_auc_score(y_train, y_pred))","0756593e":"y_test_pred=lr.predict(X_test)\n","0b168457":"lr_acc=metrics.accuracy_score(y_test, y_test_pred)\nprint(\"Accuracy Score:-\",lr_acc)\nprint(\"F1 Score:-\", metrics.f1_score(y_test, y_test_pred))\nprint(\"Average Precision Score:-\", metrics.average_precision_score(y_test, y_test_pred))\nprint(\"Log Loss:-\", metrics.log_loss(y_test, y_test_pred))\nprint(\"Precision Score:-\", metrics.precision_score(y_test, y_test_pred))\nprint(\"Recall Score:-\", metrics.recall_score(y_test, y_test_pred))\nprint(\"ROC-AUC Score:-\", metrics.roc_auc_score(y_test, y_test_pred))","df1da989":"cfm=confusion_matrix(y_test, y_test_pred)\ndisp = ConfusionMatrixDisplay(confusion_matrix=cfm,\n                               display_labels=lr.classes_)\ndisp.plot()","a462448d":"from sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import confusion_matrix,classification_report,precision_score, plot_roc_curve, plot_precision_recall_curve, balanced_accuracy_score\n\ndef clf_scores(clf, y_predicted):\n    # Accuracy\n    acc_train = clf.score(X_train, y_train)*100\n    acc_test = clf.score(X_test, y_test)*100\n    \n    roc = roc_auc_score(y_test, y_predicted)*100 \n    tn, fp, fn, tp = confusion_matrix(y_test, y_predicted).ravel()\n    cm = confusion_matrix(y_test, y_predicted)\n    correct = tp + tn\n    incorrect = fp + fn\n    d=[acc_train, acc_test,  roc, correct, incorrect,  cm]\n    index=[\"acc_train\",'Test Accuracy',\"Roc Score\",\"COrrect\",\"Incorrect\",\"Confusion\"  ]\n    output=pd.DataFrame(data=d, index=index)\n    \n    d=sns.heatmap(cm, annot=True)\n    dd=plot_roc_curve(clf, X_train, y_train)\n    ddd=plot_precision_recall_curve(clf, X_train, y_train)\n\n    return output,d, dd, ddd","3a9ecf6f":"#1. Logistic regression\n\nfrom sklearn.linear_model import LogisticRegression\nclf_lr = LogisticRegression()\nclf_lr.fit(X_train, y_train)\n\nY_pred_lr = clf_lr.predict(X_test)\nprint(clf_scores(clf_lr, Y_pred_lr))","630cb1ea":"# 2 Random Forest\n\nfrom sklearn.ensemble import RandomForestClassifier\nclf_rf = RandomForestClassifier()\nclf_rf.fit(X_train, y_train)\n\nY_pred_rf = clf_rf.predict(X_test)\nprint(clf_scores(clf_rf, Y_pred_rf))","1369d06d":"# 3 XGboost\nfrom sklearn.ensemble import GradientBoostingClassifier\nclf_xg = GradientBoostingClassifier()\nclf_xg.fit(X_train, y_train)\n\nY_pred_xg = clf_xg.predict(X_test)\nprint(clf_scores(clf_xg, Y_pred_xg))","cd3e3221":"from sklearn.model_selection import GridSearchCV\n\nparam_grid = [\n{'n_estimators': [10, 25], 'max_features': [5, 10], \n 'max_depth': [10, 50, None], 'bootstrap': [True, False]}\n]\n\ngrid_search_forest = GridSearchCV(clf_rf, param_grid, cv=10, scoring='neg_mean_squared_error')\ngrid_search_forest.fit(X_train, y_train)","a1193021":"#now let's how the RMSE changes for each parameter configuration\ncvres = grid_search_forest.cv_results_\nfor mean_score, params in zip(cvres[\"mean_test_score\"], cvres[\"params\"]):\n    print(np.sqrt(-mean_score), params)","c1c8daf1":"#find the best model of grid search\ngrid_search_forest.best_estimator_","2f42994d":"# Performance metrics\ngrid_best= grid_search_forest.best_estimator_.predict(X_train)\nerrors = abs(grid_best - y_train)\n# Calculate mean absolute percentage error (MAPE)\nmape = np.mean(100 * (errors \/ y_train))\n# Calculate and display accuracy\naccuracy = 100 - mape    \n#print result\nprint('The best model from grid-search has an accuracy of', round(accuracy, 2),'%')","b3d7fe5e":"# Tuned Random Forest\n\nfrom sklearn.ensemble import RandomForestClassifier\nclf_rf = RandomForestClassifier(max_features=10, n_estimators=25)\nclf_rf.fit(X_train, y_train)\n\nY_pred_rf = clf_rf.predict(X_test)\nprint(clf_scores(clf_rf, Y_pred_rf))","a1b10a36":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session"}}