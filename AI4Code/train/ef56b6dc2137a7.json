{"cell_type":{"322db259":"code","8ab77ede":"code","5f12b573":"code","193a434f":"code","4b34131e":"code","b9a6822a":"code","53ef0376":"code","96597406":"code","139a9a52":"code","ee5e086f":"code","540aae6e":"code","01fd11c8":"code","80874b58":"code","20b410c6":"code","5b84559d":"code","4b6b2750":"code","7dc5c34b":"code","6cf2380f":"code","acb99219":"code","15f4b088":"code","0071b18a":"code","97ceca9c":"code","f10f1aa8":"code","b8e488a4":"code","760fe263":"code","99fe9662":"code","41044732":"code","d09af1b6":"code","c7488e81":"code","66fa0175":"code","a3e220b2":"code","e4028c3d":"code","c0cacdfd":"code","861f2d3c":"code","22d15bb5":"code","7f90e22e":"code","ea71db45":"code","bb6d169d":"code","7cc52043":"code","33f2612d":"markdown","18e4f5fa":"markdown","182216ef":"markdown","a1beca5e":"markdown","937b99e1":"markdown","f8b7d4f0":"markdown","07717e21":"markdown","5ade7692":"markdown","f53831c6":"markdown","58d5bcdd":"markdown"},"source":{"322db259":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","8ab77ede":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport io\nfrom sklearn.preprocessing import LabelEncoder","5f12b573":"data = pd.read_csv('\/kaggle\/input\/weather-dataset-rattle-package\/weatherAUS.csv')","193a434f":"data.head()","4b34131e":"data.describe()\nprint(\"The size of the dataframe is:\",data.shape)","b9a6822a":"# check for null values\ndata_missing = data.isnull().sum()\n\n# calculate the % of missing values\nperc_missing = round(100*(data_missing\/len(data)),2)\nperc_missing","53ef0376":"# dropping columns with large % of missing values \n\ndata_dropped = data.drop(['Evaporation','Sunshine','Cloud9am','Cloud3pm'], axis=1)","96597406":"# And we need to replace NaN values with mean values of each column:\ndata_dropped.fillna(data_dropped.mean(), inplace=True)\n\n# Dropping the NaN values from the data as they can be problematic \ndata_dropped.dropna(inplace=True)\n\ndata_dropped.isna().sum()","139a9a52":"print(\"The new size of the dataframe is:\", data_dropped.shape)\nprint(\"We deleted\",data.shape[0]-data_dropped.shape[0],\"rows and\", data.shape[1]-data_dropped.shape[1],\"columns.\")\ndata_dropped.dtypes","ee5e086f":"# change date type to datetime\n\ndata_dropped['Date'] = pd.to_datetime(data_dropped['Date'])\n# Adding columns Year and Month\n\ndata_dropped['Year'] = pd.to_datetime(data_dropped['Date']).dt.year\ndata_dropped['Month'] = pd.to_datetime(data_dropped['Date']).dt.month\n\n# set Date as index\n\ndata_dropped.set_index('Date', inplace=True)\ndata_dropped.head()","540aae6e":"plt.figure(figsize=(20,5))\ndata_dropped['Rainfall'].plot()\nplt.box(False)\nplt.title ('Rainfall throughout the Years',fontweight=\"bold\", fontsize=15)","01fd11c8":"# plotting Rainfall per Month\nplt.figure(figsize=(8,5))\nsns.barplot(x = 'Month', y='Rainfall', data=data_dropped, color = 'skyblue')\nplt.box(False)\nplt.title ('Rainfall throughout Months', fontweight=\"bold\",fontsize=15)","80874b58":"# plotting average Rainfall by Location\ndata_loc = data_dropped.groupby('Location').agg({'Rainfall':'mean'}).sort_values(by='Rainfall', ascending=False) \n\ndata_loc.plot(kind='bar',figsize=(20,5))\nplt.box(False)\nplt.title ('Average Rainfall by Location', fontsize=15, fontweight=\"bold\")\nplt.show()","20b410c6":"# Plotting Temperature and Rainfall\n\nfig, (ax1,ax2) = plt.subplots(1, 2, figsize=(15, 5))\nsns.despine(left=True)\nsns.scatterplot(x='MinTemp', y='Rainfall', data=data_dropped, ax=ax1)\nax1.set_title(\"Lowest Temperature and Amount of Rainfall\",fontweight=\"bold\")\nsns.scatterplot(x='MaxTemp', y='Rainfall', data=data_dropped, color=\"tomato\", ax=ax2)\nax2.set_title(\"Highest Temperature and Amount of Rainfall\",fontweight=\"bold\")","5b84559d":"# Renaming Dataframe for the Machine Learning Part\ndata_ML = data_dropped","4b6b2750":"# Dropping columns that we do not need for the model building part\ndata_ML = data_ML.drop(['Location','Year'], axis=1)","7dc5c34b":"# Adjusting the Target Variables' values: Yes\/No with 1\/0\ndata_ML = data_ML.replace({'RainTomorrow':'Yes','RainToday':'Yes'},1)\ndata_ML = data_ML.replace({'RainTomorrow':'No','RainToday':'No'},0)","6cf2380f":"# Using labelEncoder to assign numeric values to the string data , according to the label.\nle = LabelEncoder()\ndata_ML['WindGustDir'] = le.fit_transform(data_ML['WindGustDir'])\ndata_ML['WindDir9am'] = le.fit_transform(data_ML['WindDir9am'])\ndata_ML['WindDir3pm'] = le.fit_transform(data_ML['WindDir3pm'])\ndata_ML.head()","acb99219":"# Correlation\n# Create Correlation mask >0.5:\ndata_ML_corr = data_ML.corr()\ncondition = abs(data_ML.corr()) > 0.5\n#data_ML_corr[condition]","15f4b088":"# heatmap\n# correlation plot\nplt.figure(figsize=(20,20))\nsns.heatmap(data_ML.corr(), cmap = 'Wistia')","0071b18a":"# Dropping highly correlated columns\n\ndata_ML = data_ML.drop(['WindGustSpeed','Humidity9am',], axis=1)","97ceca9c":"# Standardize our Data - Feature Scaling 0-1 scale \n\nfrom sklearn.preprocessing import MinMaxScaler\n\nscaler = MinMaxScaler(feature_range=(0,1)) \n\n#assign scaler to column:\ndata_scaled = pd.DataFrame(scaler.fit_transform(data_ML), columns=data_ML.columns)\n\ndata_scaled.head()","f10f1aa8":"# Selection of the most important features using SelectKBest\nfrom sklearn.feature_selection import SelectKBest, chi2\n\nX = data_scaled.loc[:,data_scaled.columns!='RainTomorrow']\ny = data_scaled[['RainTomorrow']]\n\nselector = SelectKBest(chi2, k=5)\nselector.fit(X, y)\n\nX_new = selector.transform(X)\nprint(\"The 5 most important features are:\", X.columns[selector.get_support(indices=True)]) ","b8e488a4":"# Creating a new dataframe with the most important features\n\ndata_new = data_scaled[['Rainfall', 'Humidity3pm', 'Pressure9am', 'Temp3pm', 'RainToday',\n                    'RainTomorrow']]","760fe263":"data_new['RainTomorrow'].value_counts()[0]","99fe9662":"Percentage_No = data_new['RainTomorrow'].value_counts()[0]\/len(data_new['RainTomorrow'])*100\nPercentage_Yes = data_new['RainTomorrow'].value_counts()[1]\/len(data_new['RainTomorrow'])*100","41044732":"# checking the distribution of our target variable \nprint(data_new['RainTomorrow'].value_counts())\n\nprint(\"Percentage Occurences of No Rain on the following day:\", round(Percentage_No,2),\"%\")\nprint(\"Percentage Occurences of Rain on the following day:\", round(Percentage_Yes,2),\"%\")\n\nsns.countplot(data_new['RainTomorrow'])\nplt.title('Balance target',fontsize=15, fontweight='bold')\nplt.box(False)","d09af1b6":"data_new.shape","c7488e81":"from sklearn.model_selection import train_test_split\n\ny = data_new['RainTomorrow']\nX = data_new.drop(['RainTomorrow'], axis = 1)\n\n# Train-Test Split 80-20\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.2,stratify = y)","66fa0175":"data_new.info","a3e220b2":"!pip install tpot","e4028c3d":"import time\nfrom tpot import TPOTClassifier\n\n\n# Construct and fit TPOT classifier\nstart_time = time.time()\ntpot = tpot = TPOTClassifier(generations=5,verbosity=2,population_size=50,scoring='accuracy',\n                             max_eval_time_mins=2,periodic_checkpoint_folder='\/content\/drive\/MyDrive\/Colab Notebooks\/Checkpoint TPOT')\n\ntpot.fit(X_train,y_train) \nend_time = time.time()\n\n# Results\nprint('TPOT classifier finished in %s seconds' % (end_time - start_time)) \nprint('Best pipeline test accuracy: %.3f' % tpot.score(X_test, y_test))","c0cacdfd":"# Results\nprint('Best pipeline test accuracy: %.3f' % tpot.score(X_test, y_test))","861f2d3c":"from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n\ny_pred = tpot.predict(X_test)\n\nprint('MAE',mean_absolute_error(y_pred=y_pred, y_true=y_test))\nprint('MSE',mean_squared_error(y_pred=y_pred, y_true=y_test))\nprint('R2',r2_score(y_pred=y_pred, y_true=y_test))","22d15bb5":"import sklearn.metrics\ny_predictions = tpot.predict(X_test)\nacc= sklearn.metrics.accuracy_score(y_true=y_test,\n                                     y_pred=y_predictions)\nprint(\"Accuracy:\", acc)","7f90e22e":"from sklearn.metrics import classification_report\nprint(classification_report(y_test, y_predictions))","ea71db45":"data_test = pd.read_csv('\/kaggle\/input\/weather-dataset-rattle-package\/weatherAUS.csv')","bb6d169d":"import itertools\n\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=0)\n    plt.yticks(tick_marks, classes)\n\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n        #print(\"Normalized confusion matrix\")\n    else:\n        1#print('Confusion matrix, without normalization')\n\n    #print(cm)\n\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, cm[i, j],\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')","7cc52043":"from sklearn.metrics import confusion_matrix,precision_recall_curve,auc,roc_auc_score,roc_curve,recall_score,classification_report \n\n# Compute confusion matrix\ncnf_matrix = confusion_matrix(y_predictions,y_test)\nnp.set_printoptions(precision=2)\n\nprint(\"Recall metric in the testing dataset: \", cnf_matrix[1,1]\/(cnf_matrix[1,0]+cnf_matrix[1,1]))\n\n# Plot non-normalized confusion matrix\nclass_names = [0,1]\nplt.figure()\nplot_confusion_matrix(cnf_matrix\n                      , classes=class_names\n                      , title='Confusion matrix')\nplt.show()","33f2612d":"## Data Visualisation ","18e4f5fa":"## Train Test Split","182216ef":"# Automated Machine Learning\n\nThis day i wanna learn and try about Automated Machine Learning using TPOT, Tree-based Pipeline Optimization Tool, is a Python library for automated machine learning. TPOT uses a tree-based structure to represent a model pipeline for a predictive modeling problem, including data preparation and modeling algorithms and model hyperparameters.","a1beca5e":"# Data exploration","937b99e1":"## Data Preparation ","f8b7d4f0":"## Checking the Target variables' distribution","07717e21":"## Feature Selection","5ade7692":"## Feature Scaling ","f53831c6":"# TPOT Training","58d5bcdd":"## Result"}}