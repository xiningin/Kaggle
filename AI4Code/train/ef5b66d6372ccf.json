{"cell_type":{"aa43c9e4":"code","c3ae917a":"code","d006759d":"code","f4e83434":"code","057030c7":"code","9e416d9d":"code","eeee819d":"code","5b7ba50b":"code","fae75c59":"code","cb0b98b5":"code","bae14c55":"code","f8b9431c":"code","a7db2e59":"code","b6727637":"code","7b843284":"code","6004d46c":"code","cc8dccbc":"code","64b574ae":"code","3052fafd":"code","baad9099":"code","ffc5ec09":"code","4f9922f7":"code","304f01d3":"code","dc3c351e":"code","dde8e95c":"code","fbd2b4e7":"code","896ae148":"markdown","e3fdbedb":"markdown","11076623":"markdown","1d168e1d":"markdown","95644226":"markdown","8a1f8fbb":"markdown","6ddeffc4":"markdown","01f2dbf4":"markdown","51b4c5e6":"markdown","c95fd25f":"markdown","53d9221b":"markdown","2f9db635":"markdown","3b60236f":"markdown","8eb8f709":"markdown"},"source":{"aa43c9e4":"# !pip install numerapi\n# import numerapi","c3ae917a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np\nimport pandas as pd\nimport os\nimport sys\nimport gc\nimport pathlib\nimport joblib\nimport math\nimport random\nfrom typing import List, NoReturn, Union, Tuple, Optional, Text, Generic, Callable, Dict\nfrom sklearn import preprocessing\nfrom sklearn import model_selection\nfrom sklearn import metrics\nfrom sklearn import decomposition\nfrom scipy import stats\nfrom tqdm.notebook import tqdm\nimport umap\nfrom catboost import CatBoostRegressor, CatBoostClassifier\nimport lightgbm as lgb\nimport xgboost as xgb\nimport operator\n\n# tf keras\nimport tensorflow as tf\nimport tensorflow_addons as tfa\n\n# visualize\nimport matplotlib.pyplot as plt\nimport matplotlib.style as style\nimport seaborn as sns\nfrom matplotlib import pyplot\nfrom matplotlib.ticker import ScalarFormatter\nimport plotly.graph_objects as go\nimport plotly.express as px\npd.options.plotting.backend = \"plotly\"\nsns.set_context(\"talk\")\nstyle.use('fivethirtyeight')\n\nimport warnings\nwarnings.filterwarnings('ignore')","d006759d":"INPUT_TRAIN_DIR = '..\/input\/numerai-train-to-feather-nomi\/'\nINPUT_TEST_DIR = '..\/input\/numerai-test-to-feather-nomi\/'\nOUTPUT_DIR = ''\nSEED = 42\ntarget = 'target'\nprediction = 'prediction'","f4e83434":"def get_int(x):\n    try:\n        return int(x[3:])\n    except:\n        return 1000 # live era!\n    \ndef load_data():\n    # load data\n    train = pd.read_feather(pathlib.Path(INPUT_TRAIN_DIR + 'train.feather'))\n    tournament = pd.read_feather(pathlib.Path(INPUT_TEST_DIR + 'test.feather'))\n    \n    # split valid and test\n    valid = tournament[tournament[\"data_type\"] == \"validation\"].reset_index(drop = True)\n    \n    # era int\n    train[\"era\"] = train[\"era\"].apply(get_int)\n    valid[\"era\"] = valid[\"era\"].apply(get_int)\n    tournament[\"era\"] = tournament[\"era\"].apply(get_int)\n\n    return train, valid, tournament\n\ntrain, valid, tournament = load_data()","057030c7":"print(train.shape)\ntrain.head()","9e416d9d":"print(valid.shape)\nvalid.head()","eeee819d":"print(tournament.shape)\ntournament.head()","5b7ba50b":"# era in training?\ntrain_eras = train['era'].unique().tolist()\nprint('{:,} train eras'.format(len(train_eras)))\nprint(train_eras)","fae75c59":"# era in validation?\nvalid_eras = valid['era'].unique().tolist()\nprint('{:,} valid eras'.format(len(valid_eras)))\nprint(valid_eras)","cb0b98b5":"# data type in tournament?\ntournament['data_type'].unique()","bae14c55":"# live era?\nlive_eras = tournament.loc[tournament['data_type'] == 'live', 'era'].unique().tolist()\nprint('{:,} live eras'.format(len(live_eras)))\nprint(live_eras)","f8b9431c":"train = pd.concat([train, valid, tournament.loc[tournament['data_type'] == 'live', :]], ignore_index=True)\nprint(train.shape)\ntrain.head()","a7db2e59":"# remove unnecessary data to save memory\ndel valid, tournament\ngc.collect()","b6727637":"# check era once again\ntarget = 'era'\nprint('{:,} targets'.format(len(train[target].unique().tolist())))","7b843284":"# features (we use all of them)\nfeatures = train.columns[train.columns.str.startswith('feature')].values.tolist()\nprint('{:,} features'.format(len(features)))","6004d46c":"# train test split\nskf = model_selection.StratifiedKFold(n_splits=5, random_state=None, shuffle=False)\ntrain_idx, val_idx = list(skf.split(train, train['era']))[0]\n\nx_train = train.iloc[train_idx]\nx_val = train.iloc[val_idx]\n\nassert x_train['era'].nunique() == x_val['era'].nunique()","cc8dccbc":"# NN\ndef seed_everything(seed : int) -> NoReturn :    \n    random.seed(seed)\n    np.random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    tf.random.set_seed(seed)\n\nseed_everything(SEED)    \n\n# adapted from https:\/\/github.com\/ghmagazine\/kagglebook\/blob\/master\/ch06\/ch06-03-hopt_nn.py\nparams = {\n    'input_dropout': 0.0,\n    'hidden_layers': 2,\n    'hidden_units': 512,\n    'hidden_activation': 'relu',\n    'dropout': 0.1,\n    'lr': 1e-3,\n    'batch_size': 10000,\n    'epochs': 192\n}\n    \ndef nn_model(num_feats=310, num_outs=149):\n    \"\"\"\n    yield a simple MLP with metric learning\n    \n    \"\"\"\n\n    # NN model architecture\n    n_neuron = params['hidden_units']\n\n    inputs = tf.keras.layers.Input(shape=(num_feats, ))\n    x = tf.keras.layers.Dense(n_neuron, activation=params['hidden_activation'])(inputs)\n    x = tf.keras.layers.BatchNormalization()(x)\n    x = tf.keras.layers.Dropout(params['dropout'])(x)\n\n    # stack hidden layers\n    for i in np.arange(params['hidden_layers'] - 1):\n        x = tf.keras.layers.Dense(n_neuron \/\/ (2 * (i+1)), activation=params['hidden_activation'])(x)\n        x = tf.keras.layers.BatchNormalization()(x)\n        x = tf.keras.layers.Dropout(params['dropout'])(x)\n        \n    x = tf.keras.layers.Lambda(lambda x: tf.math.l2_normalize(x, axis=1), name='l2_norm')(x) # L2 normalize embeddings!\n    out = tf.keras.layers.Dense(num_outs, activation='softmax')(x)\n   \n    # output\n    model = tf.keras.models.Model(inputs=inputs, outputs=out)\n\n    # compile\n    loss = tf.keras.losses.CategoricalCrossentropy()\n    opt = tf.keras.optimizers.Adam(lr=params['lr'])\n    model.compile(loss=loss, optimizer=opt)\n    \n    return model","64b574ae":"# show the model structure\nmodel = nn_model(len(features), 149)\nmodel.summary()","3052fafd":"# visualize the model structure\ntf.keras.utils.plot_model(model, show_shapes=True)","baad9099":"%%time\n\n# NN datasets\ntrain_set = {'X': x_train[features].values, 'y': pd.get_dummies(x_train[target]).values}\nvalid_set = {'X': x_val[features].values, 'y': pd.get_dummies(x_val[target]).values}\n\n# define callbacks\nearly_stop = tf.keras.callbacks.EarlyStopping(patience=4, restore_best_weights=True, monitor='val_loss')\nreduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=8, verbose=1, mode='min')\nmodel_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath=OUTPUT_DIR + 'mybestweight.hdf5', \n                                    save_weights_only=True, verbose=0, monitor='val_loss', save_best_only=True)\n\n# fit\nmodel = nn_model(len(features), train_set['y'].shape[1])\nhistory = model.fit(train_set['X'], train_set['y'], callbacks=[reduce_lr, model_checkpoint_callback, early_stop], \n          verbose=2, epochs=params['epochs'], batch_size=params['batch_size'],\n          validation_data=(valid_set['X'], valid_set['y'])) ","ffc5ec09":"# summarize history for loss\nwith plt.xkcd(): # just for fun\n    plt.plot(history.history['loss'])\n    plt.plot(history.history['val_loss'])\n    plt.title('model loss')\n    plt.ylabel('loss')\n    plt.xlabel('epoch')\n    plt.legend(['train', 'valid'], loc='upper left')\n    plt.show()","4f9922f7":"%%time\n\n# extract embedding features\ndef extract_emb(best_model, x_train, x_val, layer_name='l2_norm', method='umap'):\n\n    # NN model to get representations from a middle layer\n    hidden_model = tf.keras.models.Model(inputs=best_model.input,\n                         outputs=best_model.get_layer(layer_name).output)\n\n    # input train, get embedded outputs\n    hidden_pred_train = hidden_model.predict(x_train)\n    hidden_pred_test = hidden_model.predict(x_val)\n    \n    # dimensionality reduction\n    if method == 'pca':\n        trans = decomposition.PCA(n_components=2, random_state=SEED)\n    elif method == 'umap':\n        trans = umap.UMAP(n_components=2, random_state=SEED)\n    elif method == 'pca+umap':\n        trans = decomposition.PCA(n_components=100, random_state=SEED)\n\n    trans.fit(hidden_pred_train)\n    train_dist = trans.transform(hidden_pred_train)\n    test_dist = trans.transform(hidden_pred_test)\n    \n    if method == 'pca+umap':\n        trans = umap.UMAP(n_components=2, random_state=SEED)\n        trans.fit(train_dist)\n        train_dist = trans.transform(train_dist)\n        test_dist = trans.transform(test_dist)\n    return train_dist, test_dist\n\ntrain_dist, val_dist = extract_emb(model, train_set['X'], valid_set['X'], layer_name='l2_norm', method='pca+umap')","304f01d3":"# identified elsewhere (you might have a different opinion)\nhard_era = [9, 41, 50, 58, 60, 68, 69, 91, 103, 104, 107, 127, 129, 199, 200, 201, 203, 205, 206]","dc3c351e":"# plot \nplot_df = pd.DataFrame()\nplot_df['era'] = np.hstack((x_train[target].values, x_val[target].values))\nplot_df['hard_era'] = plot_df['era'].apply(lambda x : 1 if x in hard_era else 0)\nplot_df['emb1'] = np.hstack((train_dist[:, 0], val_dist[:, 0]))\nplot_df['emb2'] = np.hstack((train_dist[:, 1], val_dist[:, 1]))\n\n# downsampling for train and valid data\nsampled = plot_df.query('era < 1000').groupby('era').apply(lambda x: x.sample(frac=0.1, random_state=SEED))\nlive = plot_df.query('era == 1000')\n\nfig = px.scatter(pd.concat([sampled, live]), x='emb1', y='emb2', opacity=0.4, color='era', symbol='hard_era', \n                 color_continuous_scale='Viridis',)\n\nfig.update_layout(\n    title_text=\"(Fig. 1) metric learning visualization (sampled records)\"\n)\nfig.show()","dde8e95c":"# era-groupby plot \nfig = px.scatter(plot_df.groupby('era').mean().reset_index(), x='emb1', y='emb2', opacity=0.7, color='era', symbol='hard_era', \n                 color_continuous_scale='Jet',  )\n\nfig.update_layout(\n    title_text=\"(Fig. 2) metric learning visualization (group by era)\",\n)\nfig.show()","fbd2b4e7":"# save embedding data for further analysis\nplot_df.to_csv('metric_learning_l2.csv', index=False)\nprint('Embedding data saved!')","896ae148":"# Validation Results","e3fdbedb":"Let's visualize them! \n\nAs for a bonus part, I also include 'hard era' also called 'burning era'. I identified those eras elsewhere by fitting a simple GBDT model using the GroupKFold cross-validation method with 'era' as a group.\n\nFirst we plot downsampled records (for train and valid but not tournament) to see the diversity of records within the era.\n\n\u3055\u3066\u53ef\u8996\u5316\u3067\u3059\u3002\u30dc\u30fc\u30ca\u30b9\u3068\u3057\u3066\u3001\u96e3\u3057\u3044Era\uff08burning era\u3068\u3082\u547c\u3070\u308c\u308b\uff09\u3092\u542b\u3081\u3066\u3044\u307e\u3059\u3002\u81ea\u5206\u306f\u305d\u3046\u3044\u3063\u305fEra\u3092\u3001\u7c21\u5358\u306aGBDT\u30e2\u30c7\u30eb\u3092Era\u3092Group\u3068\u3057\u3066GroupKFold\u3067Cross-validation\u3057\u3066\u898b\u3064\u3051\u3066\u304d\u3066\u3044\u307e\u3059\u3002\n\n\u307e\u305a\u3001Train\u3068Valid\u306f\u30c0\u30a6\u30f3\u30b5\u30f3\u30d7\u30eb\u3057\u305f\u3082\u306e\u3092\u53ef\u8996\u5316\u3057\u307e\u3059\u3002Era\u5185\u3067\u3082\u3070\u3089\u3064\u304d\u304c\u3042\u308b\u305f\u3081\u3001\u305d\u308c\u3092\u78ba\u8a8d\u3059\u308b\u306e\u304c\u72d9\u3044\u3067\u3059\u3002","11076623":"# Load data\nI have already saved the data in the feather format elsewhere such that I can load the data really quickly. \n\u3055\u3063\u3055\u3068\u30c7\u30fc\u30bf\u3092\u30ed\u30fc\u30c9\u3059\u308b\u305f\u3081\u3001\u5225\u306e\u5834\u6240\u306b\u4e88\u3081feather format\u3067\u4fdd\u5b58\u3057\u305f\u30c7\u30fc\u30bf\u3092\u30ed\u30fc\u30c9\u3057\u307e\u3059\u3002","1d168e1d":"# Fit\nLet's try to classify the era!\nEra\u5206\u985e\u3092\u3055\u305b\u307e\u3057\u3087\u3046","95644226":"# Libraries\nWhat we need here...\n\u5fc5\u8981\u306a\u7269\u3092\u5165\u308c\u307e\u3059","8a1f8fbb":"# EDA (Exploratory Data Analysis) for 'era'","6ddeffc4":"# Conclusion\nI use a simple deep metric learning to cluster the era of the Numerai tournament data and found that the live data (for R240) is the closest to the one in the era of 207.\n\u5358\u7d14\u306a\u6df1\u5c64\u8ddd\u96e2\u5b66\u7fd2\u3092\u7528\u3044\u3066\u3001Era\u3092\u30af\u30e9\u30b9\u30bf\u30ea\u30f3\u30b0\u3057\u3001R240\u306eLive data\u306b\u6700\u3082\u8fd1\u3044Era\u306f207\u3067\u3042\u308b\u3053\u3068\u3092\u3064\u304d\u3068\u3081\u305f\u3002\n\n\n# Takeaways\n\n- There is a small cluster in the corner (Fig. 1), where (mostly) early data (era btw. 1 - 40) are located. They do not look related to the 'live' era, so you might want to omit those eras from your model training. \n- In the groupby figure (Fig. 2), the data for each era is aligned in the first embedding dimension almost in a chronological order! This might be expected for a time-series data, but indeed the more new the data are, the closer to the live.\n- The 'live' era is similar to the recent eras such as ones in the validation data. Do use the validation data for your model training!\n- It could be possible that weighting the recent era (by setting 'sample_weight' in your model) might boost your performance.\n\n\n## \u4eca\u56de\u308f\u304b\u3063\u305f\u3053\u3068\n- Fig1\u306e\u89d2\u306b\u5c0f\u3055\u306a\u30af\u30e9\u30b9\u30bf\u30fc\u304c\u3042\u308a\u3001\u305d\u3053\u3067\u306f\u307b\u3068\u3093\u3069\u304c\u521d\u671f\u306eEra\uff081-40\uff09\u306e\u30ec\u30b3\u30fc\u30c9\u304c\u5165\u3063\u3066\u3044\u307e\u3059\u3002\u3053\u308c\u3089\u306fLive\u30c7\u30fc\u30bf\u3068\u5168\u7136\u4f3c\u3066\u3044\u306a\u3044\u306e\u3067\u3001Training\u304b\u3089\u306f\u305a\u3057\u3066\u3082\u3044\u3044\u304b\u3082\u3067\u3059\u3002\n- Fig2\u306b\u304a\u3044\u3066\u3001\u30c7\u30fc\u30bf\u306f\u6f5c\u5728\u5909\u6570\uff11\u306b\u6cbf\u3063\u3066\u3001\u307b\u307c\u6642\u7cfb\u5217\u3067\u4e26\u3093\u3067\u3044\u307e\u3059\uff01\u3059\u306a\u308f\u3061\u3001\u6642\u671f\u7684\u306b\u8fd1\u3044\u30c7\u30fc\u30bf\u307b\u3069\u4f3c\u3066\u3044\u3066\u3001\u9060\u3044\u30c7\u30fc\u30bf\u307b\u3069\u4f3c\u3066\u3044\u306a\u3044\u3068\u3044\u3046\u3001\u6642\u7cfb\u5217\u30c7\u30fc\u30bf\u3067\u306f\u305d\u3046\u306a\u308b\u3060\u308d\u3046\u3068\u3044\u3046\u7d50\u679c\u306b\u306a\u3063\u3066\u3044\u307e\u3059\u3002\n- Live\u30c7\u30fc\u30bf\u306fValidation\u30c7\u30fc\u30bf\u306b\u8fd1\u3044\u3053\u3068\u304c\u308f\u304b\u308a\u307e\u3059\u3002\u5fc5\u305aValidation\u30c7\u30fc\u30bf\u306f\u8a13\u7df4\u306b\u4f7f\u3044\u307e\u3057\u3087\u3046\u3002\n- \u3082\u3057\u304b\u3059\u308b\u3068\u3001\u6642\u671f\u7684\u306b\u8fd1\u3044\u30c7\u30fc\u30bf\u306b\u3088\u308a\u91cd\u70b9\u3092\u7f6e\u304f\uff08'sample_weight'\u3092\u8a2d\u5b9a\u3059\u308b\uff09\u3068\u3001\u30d1\u30d5\u30a9\u30fc\u30de\u30f3\u30b9\u304c\u4e0a\u304c\u308b\u304b\u3082\u3067\u3059\u3002\n\nHope you like this notebook and learn some!\n\u3053\u306eNotebook\u304c\u304a\u5f79\u306b\u7acb\u3066\u308c\u3070\u5b09\u3057\u3044\u3067\u3059\u3002","01f2dbf4":"# NN model for metric learning\nI use a simple MLP (Multi-Layer Perceptron). The only difference from the conventional shallow MLP is that here there is a l2-normalization layer before the final layer.\n\u5358\u7d14\u306aMLP\uff08\u30de\u30eb\u30c1\u30d1\u30fc\u30bb\u30d7\u30c8\u30ed\u30f3\uff09\u3092\u4f7f\u3044\u307e\u3059\u3002\u901a\u5e38\u306e\u3082\u306e\u3068\u306e\u552f\u4e00\u306e\u9055\u3044\u306f\u3001\u6700\u7d42\u30ec\u30a4\u30e4\u30fc\u76f4\u524d\u306bl2-normalization\u304c\u5165\u3063\u3066\u3044\u308b\u3053\u3068\u3067\u3059\u3002","51b4c5e6":"![numerai](https:\/\/numer.ai\/homepage\/img\/Numerai-Logo-Side-Black.png)\n\n\n# Motivation\nHi everyone, how is your Numerai life going?:D\n\nNumerai tournament provides us with train and tournament data, whereby we participants fit a machine learning model to the train data and predict the unknown target in the tournament. \n\nAlthough the tournament data has many 'era' representing each round, which a certain time-window for computing performance, **only what matters for our score is the record with 'data_type = live'**. The difficulty with the Numerai tournament is the validability in model performance dependent on eras: some eras are easy to predict (and win money easily) and other eras are just burning our stake how hard we try. \n\nTherefore, **it would be nice to know if the 'live' era has any similarity to the training data we have at hand**.\n\nIf we could know, for example, that data in 'live' era is close to that in era X, we could focus on our modeling to improve the validation score by putting data in era X into the validation set! Alternatively, we might want to use era X to train our model such that our model performs well in the 'live' era.\n\nThis idea is nothing new --- already posted in the numerai forum: [Adversarial Validation](https:\/\/forum.numer.ai\/t\/adversarial-validation\/407). However, I won't do an adversarial validation here. It's a good technique but hard to do it here, simply because the number of eras (100>) is too large given the amount of the training data. It would be hard to train a model to correctly classify those eras.\n\n\u307f\u306a\u3055\u3093\u3053\u3093\u306b\u3061\u306f\u30fc\u3001Numerai\u751f\u6d3b\u306f\u3044\u304b\u304c\u304a\u904e\u3054\u3057\u3067\u3057\u3087\u3046\u304b\uff1f\u7b11\n\nNumerai\u30c8\u30fc\u30ca\u30e1\u30f3\u30c8\u3067\u306fTrain\u3068Tournament\u3068\u3044\u3046\uff12\u7a2e\u985e\u306e\u30c7\u30fc\u30bf\u3092\u4e0e\u3048\u3089\u308c\u3066\u3001Train\u3067fit\u3057\u305f\u30e2\u30c7\u30eb\u3067Tournament\u306e\u672a\u77e5\u306e\u30e9\u30d9\u30eb\u3092\u4e88\u6e2c\u3057\u307e\u3059\u3002\n\n\u30c8\u30fc\u30ca\u30e1\u30f3\u30c8\u30c7\u30fc\u30bf\u306b\u306f\u591a\u304f\u306e\u300cEra\u300d\uff08\u6642\u671f\uff09\u3068\u3044\u3046Round\u3092\u3042\u3089\u308f\u3059\u3082\u306e\u304c\u591a\u304f\u542b\u307e\u308c\u3066\u304a\u308a\u3001\u305d\u306e\u6642\u671f\u5358\u4f4d\u3067\u30d1\u30d5\u30a9\u30fc\u30de\u30f3\u30b9\u306f\u8a08\u7b97\u3055\u308c\u308b\u308f\u3051\u3067\u3059\u304c\u3001\u6700\u7d42\u7684\u306a\u30b9\u30b3\u30a2\u306b\u5927\u5207\u306a\u306e\u306f\u3001**'data_type = live'**\u306b\u306a\u3063\u3066\u3044\u308b\u30ec\u30b3\u30fc\u30c9\u306e\u307f\u3067\u3059\u3002Numerai\u306e\u96e3\u3057\u3055\u306f\u3001\u30e2\u30c7\u30eb\u6027\u80fd\u306eEra\u306b\u4f9d\u5b58\u3057\u305f\u3070\u3089\u3064\u304d\u306b\u3042\u308a\u307e\u3059\u3002\u3044\u304f\u3064\u304b\u306eEra\u306f\u7c21\u5358\u3067\u3001\u8981\u306f\u7c21\u5358\u306b\u7a3c\u3052\u308b\u308f\u3051\u3067\u3059\u304c\u3001\u4ed6\u306e\u3044\u304f\u3064\u304b\u306eEra\u306f\u4f55\u3092\u3084\u3063\u3066\u3082\u79c1\u305f\u3061\u306eStake\u3092\u71c3\u3084\u3057\u3066\u3044\u308b\u3088\u3046\u306b\u898b\u3048\u307e\u3059\u3002\n\n\u3067\u3059\u306e\u3067\u3001**\u300cLive\u300dEra\u304c\u624b\u5143\u306eTraining\u30c7\u30fc\u30bf\u306e\u3069\u306eEra\u306b\u8fd1\u3044\u306e\u304b\uff1f**\u3068\u3044\u3046\u3053\u3068\u304c\u308f\u304b\u308c\u3070\u3001\u5927\u304d\u306a\u52a9\u3051\u306b\u306a\u308a\u307e\u3059\u3002\n\n\u4f8b\u3048\u3070\u3001Live era\u304cera X\u3068\u8fd1\u3044\u3068\u308f\u304b\u3063\u305f\u3068\u3057\u307e\u3057\u3087\u3046\u3002\u3059\u308b\u3068\u3001\u305d\u306eEra X\u3092validation data\u306b\u3059\u308b\u3053\u3068\u3067\u3001\u30e2\u30c7\u30ea\u30f3\u30b0\u3092\u6539\u5584\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002\u307e\u305f\u3001Era X\u3092Training\u306b\u4f7f\u3046\u3053\u3068\u306b\u3088\u3063\u3066\u3001\u672c\u756a\u3067\u3082\u3044\u3044\u30d1\u30d5\u30a9\u30fc\u30de\u30f3\u30b9\u304c\u51fa\u308b\u3088\u3046\u306b\u306a\u308b\u3067\u3057\u3087\u3046\u3002\n\n\u3053\u306e\u30a2\u30a4\u30c7\u30a2\u81ea\u4f53\u306f\u4f55\u3082\u65b0\u3057\u304f\u3042\u308a\u307e\u305b\u3093\u3001\u3059\u3067\u306b\u904e\u53bb\u306eNumerai forum\u306b\u6295\u7a3f\u3055\u308c\u3066\u3044\u307e\u3059\uff08[Adversarial Validation](https:\/\/forum.numer.ai\/t\/adversarial-validation\/407)\uff09\u3002\u3057\u304b\u3057\u306a\u304c\u3089\u3001\u3053\u306e\u76ee\u7684\u3067Adversarial Validation\u3092\u884c\u3046\u306e\u306f\u3001\u3042\u307e\u308a\u52b9\u679c\u7684\u3067\u306f\u306a\u3044\u3068\u8003\u3048\u3066\u3044\u307e\u3059\u3002\u3053\u308c\u306f\u3044\u3044\u30c6\u30af\u30cb\u30c3\u30af\u3067\u306f\u3042\u308b\u306e\u3067\u3059\u304c\u3001\u5358\u7d14\u306bEra\u306e\u6570\uff08\uff11\uff10\uff10\u4ee5\u4e0a\uff09\u306b\u6bd4\u3079\u3001\u30ec\u30b3\u30fc\u30c9\u6570\u306f\u305d\u308c\u307b\u3069\u591a\u304f\u306a\u304f\u3001Era\u3092\u6b63\u3057\u304f\u5206\u985e\u3067\u304d\u308b\u30e2\u30c7\u30eb\u3092\u4f5c\u308b\u3053\u3068\u306f\u96e3\u3057\u3044\u3067\u3042\u308d\u3046\u304b\u3089\u3067\u3059\u3002\n\n\n# Then what can we do?\nOne naive way to find an era close to the 'live' one is clustering: perform a dimensionality reduction technique (e.g. tSNE, UMAP) and plot all the data in a 2D space to find out which era is close to the 'live' in a high-dimensional space. The advantage of this technique is that we don't have to classify the eras...we just need to know how distant from one another:)\n\nIn this notebook I use a simple **deep metric learning** for that purpose. \n\nThe basic idea of deep metric learning is that **a deep learning model learns the distance between classes by putting together similar data into nearby locations but pushing dissimilar data away in its internal data representation**. So in this case, I train a simple deep learning model to classify the 'era' as the target and later extract the neural network's embedding features and visualize it to see which training era is similar to the 'live' era. Note that although I force the model to classify eras, the accuracy of the model does not matter too much: all we need is a model's internal representation of distance between classes!\n\nAs a method of deep metric learning, I use a simple **L2-constrained softmax loss**.\n\nOK, let's get the ball rolling:D\n\n\u3067\u306f\u3001\u3069\u3046\u3059\u308c\u3070\u3044\u3044\u306e\u304b\uff1f\n\n\u307e\u305a\u601d\u3044\u3064\u304f\u306e\u306f\u3001\u30af\u30e9\u30b9\u30bf\u30ea\u30f3\u30b0\u3067\u3059\u3002tSNE\u3001UMAP\u306a\u3069\u3001\u306a\u3093\u3089\u304b\u306e\u6b21\u5143\u524a\u6e1b\u624b\u6cd5\u3092\u7528\u3044\u3066\u3001\u9ad8\u6b21\u5143\u7a7a\u9593\u3067Live era\u306b\u8fd1\u3044Era\u3092\u898b\u3064\u3051\u307e\u3059\u3002\u3053\u306e\u65b9\u6cd5\u306e\u3044\u3044\u3068\u3053\u308d\u306f\u3001Era\u6642\u4ee3\u3092\u5206\u985e\u3059\u308b\u5fc5\u8981\u304c\u306a\u3044\u3053\u3068\u3067\u3059\u3002\u305f\u3060\u3001Era\u540c\u58eb\u306e\u8ddd\u96e2\u3055\u3048\u308f\u304b\u308c\u3070\u3044\u3044\u308f\u3051\u3067\u3059\u3002\n\n\u3053\u306eNotebook\u3067\u306f\u3001**deep metric learning**\u3068\u3044\u3046\u624b\u6cd5\u3067\u3053\u306e\u8ab2\u984c\u306b\u53d6\u308a\u7d44\u307f\u307e\u3059\u3002\n\n\u3053\u306e\u624b\u6cd5\u306e\u57fa\u672c\u7684\u306a\u8003\u3048\u65b9\u3068\u3057\u3066\u306f\u3001\u6df1\u5c64\u5b66\u7fd2\u30e2\u30c7\u30eb\u304c\u30af\u30e9\u30b9\u9593\u8ddd\u96e2\u3092\u3001\u5185\u90e8\u306b\u6301\u3064\u6f5c\u5728\u7a7a\u9593\u5185\u3067\u3001\u8fd1\u3044\u30c7\u30fc\u30bf\u540c\u58eb\u306e\u30e9\u30d9\u30eb\u306f\u8fd1\u304f\u306b\u3001\u9060\u3044\u30c7\u30fc\u30bf\u540c\u58eb\u306e\u30e9\u30d9\u30eb\u306f\u9060\u304f\u306b\u914d\u7f6e\u3059\u308b\u3053\u3068\u3092\u5b66\u7fd2\u3059\u308b\u3001\u3068\u3044\u3046\u3082\u306e\u3067\u3059\u3002\u3064\u307e\u308a\u4eca\u56de\u306e\u30b1\u30fc\u30b9\u3067\u306f\u3001\u5358\u7d14\u306a\u6df1\u5c64\u5b66\u7fd2\u30e2\u30c7\u30eb\u3092\u7528\u3044\u3066Era\u3092\u5206\u985e\u3057\u3001\u305d\u306e\u904e\u7a0b\u3067\u3067\u304d\u305f\u6df1\u5c64\u5b66\u7fd2\u30e2\u30c7\u30eb\u5185\u306e\u6f5c\u5728\u8868\u73fe\u3092\u62bd\u51fa\u3057\u3001\u53ef\u8996\u5316\u3059\u308b\u3053\u3068\u3067\u3001Live era\u306b\u8fd1\u3044Era\u3092\u898b\u3064\u3051\u3088\u3046\u3001\u3068\u3044\u3046\u3053\u3068\u3092\u3057\u307e\u3059\u3002\u6ce8\u610f\u3057\u3066\u6b32\u3057\u3044\u306e\u306f\u3001\u78ba\u304b\u306bEra\u81ea\u4f53\u3092\u5206\u985e\u3055\u305b\u3088\u3046\u3068\u3057\u3066\u306f\u3044\u308b\u3082\u306e\u306e\u3001\u305d\u306e\u7cbe\u5ea6\u306f\u7279\u306b\u554f\u308f\u305a\u3001\u6df1\u5c64\u5b66\u7fd2\u30e2\u30c7\u30eb\u306b\u306f\u30af\u30e9\u30b9\u9593\u8ddd\u96e2\u3055\u3048\u5b66\u3093\u3067\u3082\u3089\u3048\u308c\u3070\u826f\u3044\u3001\u3068\u3044\u3046\u3053\u3068\u3067\u3059\u3002\n\n\u4eca\u56de\u306f\u5358\u7d14\u306a\u624b\u6cd5\u3092\u8a66\u3059\u306e\u3067\u3001**L2-constrained softmax loss**\u3092\u7528\u3044\u307e\u3059\u3002\n\n\u305d\u308c\u3067\u306f\u3084\u3063\u3066\u3044\u304d\u307e\u3057\u3087\u3046\uff01\n\n## Data\nTournament data here is for R240.\n\u30c8\u30fc\u30ca\u30e1\u30f3\u30c8\u30c7\u30fc\u30bf\u306fR240\u7528\u306e\u3082\u306e\u3092\u4f7f\u7528\u3057\u3066\u3044\u307e\u3059\u3002\n\n## Reference\n- https:\/\/towardsdatascience.com\/deep-metric-learning-76fa0a5a415f\n- [L2-constrained Softmax Loss for Discriminative Face Verification](https:\/\/arxiv.org\/abs\/1703.09507)","c95fd25f":"# Extract Embeddings\nHere it comes. For the visualization, you can use whatever dimensionality reduction technique but here I use **pca + umap**...this combination seems to show a nice cluster in a previous study: [UMAP reveals cryptic population structure and phenotype heterogeneity in large genomic cohorts](https:\/\/journals.plos.org\/plosgenetics\/article?id=10.1371\/journal.pgen.1008432).\n\n\u3055\u3066\u3053\u3053\u304b\u3089\u304c\u672c\u984c\u3067\u3059\u3002\u53ef\u8996\u5316\u3068\u3057\u3066\u3001\u306a\u3093\u306e\u6b21\u5143\u524a\u6e1b\u624b\u6cd5\u3092\u7528\u3044\u3066\u3082\u3044\u3044\u306e\u3067\u3059\u304c\u3001\u3053\u3053\u3067\u306fpca+umap\u306e\u7d44\u307f\u5408\u308f\u305b\u3092\u4f7f\u7528\u3057\u307e\u3059\u3002\u3069\u3046\u3084\u3089\u305d\u306e\u65b9\u304c\u7dba\u9e97\u306b\u5206\u304b\u308c\u308b\u3068\u8a00\u3046\u3053\u3068\u3067...[UMAP reveals cryptic population structure and phenotype heterogeneity in large genomic cohorts](https:\/\/journals.plos.org\/plosgenetics\/article?id=10.1371\/journal.pgen.1008432).","53d9221b":"Cool, you see the yellow dots on the above figure. They are our live data! Although they are distributed quite a lot, most of them are clustered in the corner. In those locations, you can find data from the recent eras, suggesting that our live data are 'closer' to the ones from the recent era.\n\nSo **do use the validation data (which is close to the live timewise) for your model training**!\n\nThe above figure already looks good, but to summarize it better, I show you another figure using the aggregated data by era. I computed the mean by each era...not sure if it is mathematically alright but I hope it does its expected job nonetheless.\n\n\u3044\u3044\u3067\u3059\u306d\u3001\u4e0a\u56f3\u306e\u9ec4\u8272\u3044\u70b9\u304c\u898b\u3048\u308b\u3067\u3057\u3087\u3046\u304b\u3002\u3053\u308c\u3089\u304cLive\u306e\u30c7\u30fc\u30bf\u3067\u3059\uff01\u304b\u306a\u308a\u6563\u3089\u3070\u3063\u3066\u306f\u3044\u307e\u3059\u304c\u3001\u307b\u3068\u3093\u3069\u306f\u89d2\u306e\u4e00\u90e8\u306b\u5b58\u5728\u3059\u308b\u306e\u304c\u308f\u304b\u308b\u3068\u601d\u3044\u307e\u3059\u3002\u3053\u308c\u3089\u306e\u5468\u8fba\u3067\u306f\u3001\u6700\u8fd1\u306eEra\u306e\u30c7\u30fc\u30bf\u304c\u591a\u3044\u3067\u3059\u306d\u3002\u3059\u306a\u308f\u3061\u3001Live\u306e\u30c7\u30fc\u30bf\u306f\u3001\u3088\u308a\u6700\u8fd1\u306eEra\u306e\u30c7\u30fc\u30bf\u3068\u8fd1\u3044\u3001\u3068\u3044\u3046\u3053\u3068\u304c\u8a00\u3048\u307e\u3059\u3002\n\n\u3064\u307e\u308a\u3001**\uff08\u6642\u671f\u306e\u8fd1\u3044\uff09Validation\u30c7\u30fc\u30bf\u306f\u5fc5\u305a\u30e2\u30c7\u30eb\u306e\u8a13\u7df4\u306b\u4f7f\u3044\u307e\u3057\u3087\u3046\uff01**\n\n\u4e0a\u56f3\u3082\u3044\u3044\u3067\u3059\u304c\u3001\u3055\u3089\u306b\u5206\u304b\u308a\u3084\u3059\u304f\u898b\u308b\u305f\u3081\u306b\u3001Era\u3054\u3068\u306b\u96c6\u8a08\u3057\u305f\u56f3\u3082\u304a\u898b\u305b\u3057\u307e\u3059\u3002\u5358\u306bEra\u3054\u3068\u306b\u5e73\u5747\u3092\u3068\u3063\u305f\u3082\u306e\u3067\u3001\u6570\u5b66\u7684\u306b\u6b63\u3057\u3044\u304b\u3069\u3046\u304b\u306f\u3088\u304f\u308f\u304b\u308a\u307e\u305b\u3093\u304c\u3001\u671f\u5f85\u3055\u308c\u308b\u4ed5\u4e8b\u306f\u3057\u3066\u304f\u308c\u308b\u3067\u3057\u3087\u3046\u3068\u3044\u3046\u3053\u3068\u3067\u3084\u3063\u3066\u3044\u307e\u3059\u3002","2f9db635":"For the train-test-split, I use the stratifiedkfold to have all the eras in both train and valid data.\n\u8a13\u7df4\u3068\u691c\u8a3c\u30c7\u30fc\u30bf\u306b\u5206\u3051\u308b\u65b9\u6cd5\u3068\u3057\u3066\u3001StratifiedKFold\u3092\u7528\u3044\u3066\u3001\u3069\u3061\u3089\u306e\u30c7\u30fc\u30bf\u306b\u3082\u5168\u3066\u306eEra\u304c\u5165\u308b\u3088\u3046\u306b\u3057\u307e\u3059\u3002","3b60236f":"Now it is more clear ... the live data (the red dot) is closer to the one from era 207 and maybe 206 or 205. \n\nThe closest one is 207, which was NOT a hard era, suggesting that maybe the live era is (hopefully) not a hard era, either. However, the second closest is 206, which was a hard era, so...you might want to train your model to perform well on the era of 206!\n\n\u3088\u308a\u898b\u3084\u3059\u304f\u306a\u308a\u307e\u3057\u305f\u306d\u3002Live\u30c7\u30fc\u30bf\uff08\u8d64\u3044\u70b9\uff09\u306f\u3001Era 207\u3084\u3001206\u3001205\u3068\u8fd1\u3044\u3053\u3068\u304c\u308f\u304b\u308a\u307e\u3059\u3002\n\u6700\u3082\u8fd1\u3044\u306e\u306f207\u3067\u3001\u3053\u308c\u306f\u96e3\u3057\u3044Era\u3067\u306f\u306a\u304b\u3063\u305f\u3067\u3059\u3002\u3064\u307e\u308a\u3001\uff08\u3082\u3057\u304b\u3057\u305f\u3089\uff09\u3053\u306eLive Era\u306fBurning era\u3067\u306f\u306a\u3044\u304b\u3082\u3057\u308c\u307e\u305b\u3093\u3002\u3057\u304b\u3057\u306a\u304c\u3089\u3001\u305d\u306e\u6b21\u306b\u8fd1\u3044\u306e\u306f206\u3067\u3001\u3053\u308c\u306f\u96e3\u3057\u3044Era\u3060\u3063\u305f\u306e\u3067...\u3054\u81ea\u8eab\u306e\u30e2\u30c7\u30eb\u304cR206\u306b\u5bfe\u3057\u3066\u3044\u3044\u6027\u80fd\u3092\u6301\u3063\u3066\u308b\u304b\u306f\u78ba\u8a8d\u3057\u305f\u65b9\u304c\u3044\u3044\u304b\u3082\u77e5\u308c\u307e\u305b\u3093\u3002","8eb8f709":"# Train test split\nSince the purpose is to classify the 'era', I combine all the necessary data into one big dataframe.\n\u4eca\u56de\u306fEra\u306e\u5206\u985e\u3092\u884c\u3046\u305f\u3081\u3001\u5fc5\u8981\u306a\u30c7\u30fc\u30bf\u30921\u3064\u306e\u30c7\u30fc\u30bf\u30d5\u30ec\u30fc\u30e0\u306b\u307e\u3068\u3081\u307e\u3059\u3002"}}