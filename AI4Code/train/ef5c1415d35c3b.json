{"cell_type":{"07be157b":"code","51341fe6":"code","55a15716":"code","d883a8a9":"code","8dc59a98":"code","3570d434":"code","137fc0ef":"code","3de2902a":"code","1877dc0b":"code","45729190":"code","c4c3ee90":"code","7f66d88a":"code","588d2077":"code","a49bfd2c":"code","643966a3":"code","17f2febb":"code","7359e1eb":"code","2ea29d84":"code","0b9cd2d9":"code","96c9a0b5":"code","d211f91a":"code","fe856e49":"code","fa20ee41":"code","ee267210":"code","6b661f6b":"code","1c63d179":"code","f7a134d8":"code","d969b3d8":"code","2e5c3a46":"code","4253b4de":"code","55412939":"code","8bd11f5f":"code","810a3271":"code","dabd2987":"code","bf6bcf40":"code","ca2a730b":"code","dd98f26c":"code","3d1bffef":"code","e63b05f0":"code","0e4a29f5":"code","0e8e16d5":"code","5bde2485":"code","0670b8b5":"code","0dcc29ce":"markdown","577f0c62":"markdown","414106ee":"markdown","a2ab8097":"markdown","5b6a1cd6":"markdown","1c3bf0d7":"markdown","0528f2ca":"markdown","de172653":"markdown","6f261233":"markdown","e7136ea0":"markdown","145abadd":"markdown","3b10e4df":"markdown","20219f79":"markdown","e88179f2":"markdown","99210728":"markdown","14ed8164":"markdown","ece3cc12":"markdown","5205392e":"markdown"},"source":{"07be157b":"#Lets import the necessary libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns","51341fe6":"Data = pd.read_csv(\"..\/input\/human-resource-datasets\/HR.csv\")\nData.head()","55a15716":"Data.describe().T","d883a8a9":"Data.columns","8dc59a98":"# Get some information on the types of variables in data\nData.info()","3570d434":"Data.isnull().sum()","137fc0ef":"# Print the unique values of the \"salary\" column\nprint(Data.salary.unique())","3de2902a":"print(Data['department'].value_counts())","1877dc0b":"print(Data['salary'].value_counts())","45729190":"table = Data.pivot_table(values=\"satisfaction_level\", \n                         index=\"department\", columns=\"salary\",aggfunc=np.count_nonzero)\ntable\n","c4c3ee90":"f, axes = plt.subplots(2,2, figsize=(10,10), sharex=True)\n\nplt.subplots_adjust(wspace=0.5)# adjust the space between the plots\n\nsns.despine(left=True)\n\n# plot a boxplot of satisfaction_level to see if there is outliers\nsns.boxplot( x= 'satisfaction_level',  data=Data, orient='v',ax=axes[0,0])\n\n# plot a boxplot of last_evaluation to see if there is outliers\nsns.boxplot( x= 'last_evaluation',  data=Data, orient='v',ax=axes[0,1])\n\n# plot a boxplot of number_project to see if there is outliers\nsns.boxplot( x= 'number_project',  data=Data, orient='v',ax=axes[1,0])\n\n# plot a boxplot of average_montly_hours to see if there is outliers\nsns.boxplot( x= 'average_montly_hours',  data=Data, orient='v',ax=axes[1,1]);","7f66d88a":"sns.countplot('left',data=Data )","588d2077":"sns.countplot('salary',data=Data )","a49bfd2c":"sns.factorplot(x=\"salary\", y =\"left\", data=Data, kind=\"bar\", size=3)\nplt.show()","643966a3":"Data.columns","17f2febb":"sns.factorplot(x=\"number_project\", y =\"left\", data=Data, kind=\"bar\", size=3)\nplt.show()","7359e1eb":"sns.factorplot(x=\"time_spend_company\", y =\"left\", data=Data, kind=\"bar\", size=3)\nplt.show()","2ea29d84":"g = sns.FacetGrid(Data, row=\"left\")\ng.map(sns.distplot, \"average_montly_hours\", bins=25)\nplt.show()","0b9cd2d9":"corr= Data.corr()\n\nplt.figure(figsize=(12,10))\nsns.heatmap(corr,annot=True,cbar=True,cmap=\"coolwarm\")\nplt.xticks(rotation=90)\n\n\n\n\nplt.show() ","96c9a0b5":"from sklearn.preprocessing import LabelEncoder # For change categorical variable into int\nfrom sklearn.metrics import accuracy_score \nle=LabelEncoder()\nData['salary']=le.fit_transform(Data['salary']) #thats nice!\n\nData['department']=le.fit_transform(Data['department']) #not Customary decison","d211f91a":"# we can select importance features by using Randomforest Classifier\nfrom sklearn.ensemble import RandomForestClassifier\n\n\nfeature_var = Data.loc[:,Data.columns != \"left\"]\npred_var = Data.loc[:,Data.columns=='left']\n\nmodel= RandomForestClassifier(n_estimators=100)\n\nmodel.fit(feature_var,pred_var)","fe856e49":"featimp = pd.Series(model.feature_importances_,index=feature_var.columns).sort_values(ascending=False)\nprint(featimp)","fa20ee41":"# Importing Machine learning models library used for classification\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier as knn\nfrom sklearn.naive_bayes import GaussianNB as GB\nfrom sklearn.svm import SVC\n","ee267210":"def Classification_model(model,Data,x,y):\n    \n    # here x is the variable which are used for prediction\n    # y is the prediction variable\n    \n    train,test = train_test_split(Data,test_size= 0.33)\n    \n    train_x = Data.loc[train.index,x]   # Data for training only with features\n    train_y = Data.loc[train.index,y]   # Data for training only with predcition variable\n    \n    test_x = Data.loc[test.index,x]     # same as for training \n    test_y = Data.loc[test.index,y]\n    \n    model.fit(train_x,train_y)\n    \n    pred=model.predict(test_x)\n    \n    accuracy=accuracy_score(test_y,pred)\n    \n    return accuracy","6b661f6b":"#for your knowledge\n\nAll_features=['satisfaction_level','number_project','time_spend_company',\n              'average_montly_hours','last_evaluation','department',\n              'salary','Work_accident','promotion_last_5years']\n\nprint(All_features)\n\nImportant_features = ['satisfaction_level','number_project','time_spend_company',\n                      'average_montly_hours','last_evaluation']\n\n\nprint(Important_features)\n\nPred_var = [\"left\"]\nprint(Pred_var)","1c63d179":"# Lets us make a list of models\n\nmodels=[\"RandomForestClassifier\",\"Gaussian Naive Bays\",\"KNN\",\"Logistic_Regression\",\"Support_Vector\"]\n\nClassification_models = [RandomForestClassifier(n_estimators=100),GB(),knn(n_neighbors=7),LogisticRegression(),SVC()]\n\nModel_Accuracy = []\n\nfor model in Classification_models:\n    Accuracy=Classification_model(model,Data,All_features,Pred_var)\n    Model_Accuracy.append(Accuracy)","f7a134d8":"Accuracy_with_all_features = pd.DataFrame(\n    { \"Classification Model\" :models,\n     \"Accuracy with all features\":Model_Accuracy\n     \n    })","d969b3d8":"Accuracy_with_all_features.sort_values(by=\"Accuracy with all features\",ascending=False).reset_index(drop=True)","2e5c3a46":"# Lets try with Important features\n\nModel_Accuracy = []\n\nfor model in Classification_models:\n    Accuracy=Classification_model(model,Data,Important_features,Pred_var) # Just instead of all features give only important features\n    Model_Accuracy.append(Accuracy)","4253b4de":"Accuracy_with_important_features = pd.DataFrame(\n    { \"Classification Model\" :models,\n     \"Accuracy with Important features\":Model_Accuracy\n     \n    })\nAccuracy_with_important_features.sort_values(by=\"Accuracy with Important features\",ascending=False).reset_index(drop=True)","55412939":"from sklearn.model_selection import cross_val_score # This is used for to caculate the score of cross validation by using Kfold\n\ndef Classification_model_CV(model,Data,x,y):\n    \n    # here x is the variable which are used for prediction\n    # y is the prediction variable\n    \n    data_x = Data.loc[:,x]\n    # Here no need of training and test data because in cross validation it splits data into \n    # train and test itself # data_x repersent features\n    \n    data_y = Data.loc[:,y] # data for predication\n\n    \n    scores= cross_val_score(model,data_x,data_y,scoring=\"accuracy\",cv=10)\n    \n    print(scores) # print the scores\n    \n    print('')\n    \n    accuracy=scores.mean()\n    return accuracy","8bd11f5f":"models=[\"RandomForestClassifier\",\"Gaussian Naive Bays\",\"KNN\",\"Logistic_Regression\",\"Support_Vector\"]\n\nClassification_models = [RandomForestClassifier(n_estimators=100),GB(),knn(n_neighbors=7),LogisticRegression(),SVC()]\n\nModel_Accuracy = []\n\nfor model,z in zip(Classification_models,models):\n    \n    print(z) # Print the name of model\n    print('')\n    \n    Accuracy=Classification_model_CV(model,Data,Important_features,Pred_var)\n    \n    Model_Accuracy.append(Accuracy)","810a3271":"Accuracy_with_CV = pd.DataFrame(\n    { \"Classification Model\" :models,\n     \"Accuracy with CV\":Model_Accuracy\n     \n    })\nAccuracy_with_CV.sort_values(by=\"Accuracy with CV\",ascending=False).reset_index(drop=True)","dabd2987":"from sklearn.model_selection import GridSearchCV \ndef Classification_model_GridSearchCV(model,Data,x,y,params):\n    \n    # here params repersent Parameters\n    data_x = Data.loc[:,x]  \n    data_y = Data.loc[:,y] \n    clf = GridSearchCV(model,params,scoring=\"accuracy\",cv=5)\n    \n    clf.fit(data_x,data_y)\n    \n    print(\"best score is :\")\n    print(clf.best_score_)\n    print('')\n    print(\"best estimator is :\")\n    print(clf.best_estimator_)\n\n    return (clf.best_score_)","bf6bcf40":"models=[\"RandomForestClassifier\",\"Gaussian Naive Bays\",\"KNN\",\"Logistic_Regression\",\"Support_Vector\"]\n\nModel_Accuracy=[]\n\nmodel = RandomForestClassifier()\n\nparam_grid = {'n_estimators':(70,80,90,100),'criterion':('gini','entropy'),'max_depth':[25,30]}\n\nAccuracy=Classification_model_GridSearchCV(model,Data,Important_features,Pred_var,param_grid)\n\nModel_Accuracy.append(Accuracy)","ca2a730b":"model = GB()\nparam_grid={}\nAccuracy=Classification_model_GridSearchCV(model,Data,Important_features,Pred_var,param_grid)\nModel_Accuracy.append(Accuracy)","dd98f26c":"model=knn()\nparam_grid={'n_neighbors':[5,15],'weights':('uniform','distance'),'p':[1,5]}\nAccuracy=Classification_model_GridSearchCV(model,Data,Important_features,Pred_var,param_grid)\nModel_Accuracy.append(Accuracy)","3d1bffef":"model=LogisticRegression()\nparam_grid={'C': [0.01,0.1,1,10],'penalty':('l1','l2')}\nAccuracy=Classification_model_GridSearchCV(model,Data,Important_features,Pred_var,param_grid)\nModel_Accuracy.append(Accuracy)","e63b05f0":"model=SVC()\nparam_grid={'C': [1,10,20,100],'gamma':[0.1,1,10]} \nAccuracy=Classification_model_GridSearchCV(model,Data,Important_features,Pred_var,param_grid)\nModel_Accuracy.append(Accuracy)","0e4a29f5":"Accuracy_with_GridSearchCV = pd.DataFrame(\n    { \"Classification Model\" :models,\n     \"Accuracy with GridSearchCV\":Model_Accuracy\n     \n    })\nAccuracy_with_GridSearchCV.sort_values(by=\"Accuracy with GridSearchCV\",ascending=False).reset_index(drop=True)","0e8e16d5":"Comparison=pd.merge(pd.merge(pd.merge(Accuracy_with_all_features,Accuracy_with_important_features,on='Classification Model'),Accuracy_with_CV,on='Classification Model'),Accuracy_with_GridSearchCV,on='Classification Model')","5bde2485":"Comparison1=Comparison.loc[:,[\"Classification Model\",\"Accuracy with all features\",\"Accuracy with Important features\",\"Accuracy with CV\",\"Accuracy with GridSearchCV\"]]","0670b8b5":"Comparison1","0dcc29ce":"time_spend_company, left","577f0c62":"# Box Plot\n","414106ee":"#### Observation \n\n1. The Random Forest is at the top followed by Support_vector and KNN\n\n2. These all are giving accuracy more than 90% for validation data i.e. test data that is not bad\n\n3. In Next we will try same with but only with important features suggested by RandomForest ","a2ab8097":"# This Notebook Contains:\n* Data Underestanding\n* Data Exploration\n* Data Preparation\n* Data Visualization\n* Feature Engineering\n* Build Machine Learning Models\n* Machine Learning Models With Cross Validation\n* Model Evaluation\n* Hyperprameters Tuning\n\n","5b6a1cd6":"Salary & left","1c3bf0d7":"Here no variables are so much correlated so that we can say that all variables are uncorrelated\nso no need to remove any features lets get important features by using Randomforestclassifie","0528f2ca":"Great, we do not have any missing values.","de172653":"# Data Underestanding","6f261233":"# Feature Engineering","e7136ea0":"We can see that the persons with the low salary are more likely to leave the company","145abadd":"1. By using the important features there is a slight increase in accuracy for all classification models\n\n2. The gaussian navie bays and KNN show a increase of 2 % in accuracy","3b10e4df":"### Machine Learning Models","20219f79":"# Welcome to my Notebook Tutorial\n* in this notebook, I want to analyze the Human Resource dataset. I will build different models and compare them with their gained accuracy.\n* The datasets contains the following categories such as:\nsatisfaction_level     \n 1   last_evaluation       \n 2   number_project        \n 3   average_montly_hours   \n 4   time_spend_company     \n 5   Work_accident           \n 6   left                   \n 7   promotion_last_5years  \n 8   department             \n 9   salary \n \n We want to predict if the employee left the company or not. 1 means the employee left the company","e88179f2":"### Machine Learning Models With Cross Validation\n\n1. In this we will do the cross validation with the models to get there mean accuracy\n\n2. From 3.1 we came to know that by using important features there is increase in the accuracy for all models so in this we will go only with important features","99210728":"# Data Visualization","14ed8164":"**Conclusion**\n\n 1. Here we can compare the accuracy obtained by different Classification Models  with different strategy\n 2. For A quick revision\n 3. Accuracy with all features means the all features of data were used for prediction of will employee left or not?  this accuracy is obtained on the test data which was not used in training.\n 4. Accuracy with important features means the same as above but here only 5 most important features were used. The importance of features we got by using Random Forest Classifier.\n 5. Accuracy with CV means the mean of accuracies which were obtained on iteration of one CV. here 10 iterations were used\n 6. Accuracy with GridSearchCV means the best score obtained after tuning the model.  Here for CV only 5 folds were used\n\n 7. Thank you\n","ece3cc12":"Number of project & left","5205392e":"### Machine Learning Models With Parameter tuning\n\n1. In this we will use  Grid SearchCV to find the best parameter for a model \n\n2. in this we will use important features too"}}