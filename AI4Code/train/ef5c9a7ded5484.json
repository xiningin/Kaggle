{"cell_type":{"8c170eff":"code","4137f6b3":"code","28a9d747":"code","c41817ae":"code","9aa7fc66":"code","136cebec":"code","9389b1a7":"code","bec83541":"code","db181c22":"code","1717426e":"code","65e11941":"code","97c5ac14":"code","462f6703":"code","646065cb":"code","713e176a":"code","057ab32f":"code","d7974f65":"code","8805f1cc":"code","088f93d3":"code","670fb125":"code","e9086ac2":"code","fc41c588":"code","26c594f1":"code","23b1fb0f":"code","b199dd7d":"code","44255ff6":"code","de944ccc":"code","3b539c9d":"code","9c2fbaa4":"code","c24a9669":"code","a3dc3a8e":"code","6a4a6be2":"code","77e73aa9":"code","ef6ad5bd":"code","aee7c847":"code","0dd72612":"code","e8a85da7":"code","6d2face6":"code","29497154":"code","a76dd536":"code","b0818b26":"code","060d15e9":"code","c810cdf8":"code","269e289f":"code","83e00f4e":"code","3b2de53a":"code","828671a9":"code","7b7ff27b":"code","5ec0e899":"code","3ce4c3bd":"code","667a4c0f":"code","2a2fddfc":"code","c9bde5dd":"code","4c173213":"code","cb3db5b4":"code","cb229f12":"code","41b9e6af":"code","9d9c6e6a":"code","92cda66d":"code","015aa6a0":"code","5c8f9783":"code","5059268f":"code","bc62597b":"code","296ed779":"code","7a7b8618":"code","74d90d51":"code","57238622":"code","a025a78a":"code","926dff67":"code","c8cf297e":"code","add304b1":"code","1956b2bf":"code","7035b63e":"code","e59e6386":"code","6005278d":"code","98d7f379":"markdown","9bbc82c6":"markdown","e7b5fe5e":"markdown","04ae4627":"markdown","d659e52b":"markdown","34c5bd1b":"markdown","50df57c0":"markdown"},"source":{"8c170eff":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","4137f6b3":"import pandas as pd\ndf = pd.read_csv(\"\/kaggle\/input\/sms-spam-collection-dataset\/spam.csv\",encoding='latin-1')","28a9d747":"df.head()","c41817ae":"df.shape","9aa7fc66":"#1.Data cleaning\n#2.EDA\n#3.Text preprocessing\n#4.Model bulding\n#5.Evaluation\n#6.Improvement","136cebec":"df.info()","9389b1a7":"#drop last 3 columns\ndf.drop(columns=[\"Unnamed: 2\",\"Unnamed: 3\",\"Unnamed: 4\"],inplace=True)","bec83541":"df","db181c22":"#renameing the columns name\ndf.rename(columns={'v1':'target','v2':'text'},inplace=True)","1717426e":"from sklearn.preprocessing import LabelEncoder\nencoder = LabelEncoder()","65e11941":"df['target'] = encoder.fit_transform(df['target'])","97c5ac14":"df.head()","462f6703":"#missing values\ndf.isnull().sum()","646065cb":"#checking for duplicate values\ndf.duplicated().sum()","713e176a":"df=df.drop_duplicates(keep='first')","057ab32f":"df.duplicated().sum()","d7974f65":"df.shape","8805f1cc":"df.head()","088f93d3":"df['target'].value_counts()","670fb125":"import matplotlib.pyplot as plt","e9086ac2":"plt.pie(df[\"target\"].value_counts(),labels=[\"ham\",\"spam\"],autopct=\"%0.2f\")\nplt.show()","fc41c588":"# as we can see the data in imbalance\n\n#lets keep as it is and start working on text data","26c594f1":"import nltk","23b1fb0f":"nltk.download('punkt')","b199dd7d":"#counting number of characters\ndf['num_characters'] = df['text'].apply(len)","44255ff6":"df.head()","de944ccc":"#counting the number of words\ndf['num_words'] = df['text'].apply(lambda x:len(nltk.word_tokenize(x)))","3b539c9d":"df.head()","9c2fbaa4":"#counting the number of sentence \ndf['num_sent'] = df['text'].apply(lambda x:len(nltk.sent_tokenize(x)))","c24a9669":"df.head()","a3dc3a8e":"df[[\"num_characters\",\"num_words\",\"num_sent\"]].describe()","6a4a6be2":"#describe for ham messages\ndf[df['target'] == 0][[\"num_characters\",\"num_words\",\"num_sent\"]].describe()","77e73aa9":"#describe for spam messages\ndf[df['target'] == 1][[\"num_characters\",\"num_words\",\"num_sent\"]].describe()","ef6ad5bd":"import seaborn as sns","aee7c847":"plt.figure(figsize=(12,7))\nsns.histplot(df[df['target'] == 0][\"num_characters\"])\nsns.histplot(df[df['target'] == 1][\"num_characters\"],color='red')","0dd72612":"plt.figure(figsize=(12,7))\nsns.histplot(df[df['target'] == 0][\"num_words\"])\nsns.histplot(df[df['target'] == 1][\"num_words\"],color='red')","e8a85da7":"sns.pairplot(df,hue=\"target\")","6d2face6":"sns.heatmap(df.corr(),annot=True)","29497154":"from nltk.corpus import stopwords\nstopwords.words('english')\nimport string\nstring.punctuation","a76dd536":"from nltk.stem.porter import PorterStemmer\nps = PorterStemmer()","b0818b26":"def transform_text(text):\n    text = text.lower()\n    text = nltk.wordpunct_tokenize(text)\n    #removing special character and appending it into y\n    y = []\n    for i in text:\n        if i.isalnum():\n            y.append(i)\n            \n    text = y[:]\n    y.clear()\n    \n    for i in text:\n        if i not in stopwords.words('english') and i not in string.punctuation:\n            y.append(i)\n            \n            \n    text = y[:]\n    y.clear()\n    \n    for i in text:\n        y.append(ps.stem(i))\n    \n    return \" \".join(y)","060d15e9":"df['transformed_text'] = df['text'].apply(transform_text)","c810cdf8":"df.head()","269e289f":"from wordcloud import WordCloud","83e00f4e":"wc = WordCloud(width=500,min_font_size=10,height=500,background_color='white')","3b2de53a":"#for spam sms\nspam_wc = wc.generate(df[df['target'] == 1]['transformed_text'].str.cat(sep=\" \"))","828671a9":"plt.figure(figsize=(15,8))\nplt.imshow(spam_wc)","7b7ff27b":"#for ham sms\nham_wc = wc.generate(df[df['target'] == 0]['transformed_text'].str.cat(sep=\" \"))","5ec0e899":"plt.figure(figsize=(15,8))\nplt.imshow(ham_wc)","3ce4c3bd":"df.head()","667a4c0f":"#iterating threw all the spam text and spliting them into word in append it into spam_corpus\nspam_corpus = []\nfor msg in df[df['target'] == 1]['transformed_text'].tolist():\n    for word in msg.split():\n        spam_corpus.append(word)","2a2fddfc":"len(spam_corpus)","c9bde5dd":"from collections import Counter","4c173213":"sns.barplot(pd.DataFrame(Counter(spam_corpus).most_common(30))[0],pd.DataFrame(Counter(spam_corpus).most_common(30))[1])\nplt.xticks(rotation='vertical')\nplt.show()","cb3db5b4":"#iterating threw all the ham text and spliting them into word in append it into ham_corpus\nham_corpus = []\nfor msg in df[df['target'] == 0]['transformed_text'].tolist():\n    for word in msg.split():\n        ham_corpus.append(word)","cb229f12":"len(ham_corpus)","41b9e6af":"sns.barplot(pd.DataFrame(Counter(ham_corpus).most_common(30))[0],pd.DataFrame(Counter(ham_corpus).most_common(30))[1])\nplt.xticks(rotation='vertical')\nplt.show()","9d9c6e6a":"from sklearn.feature_extraction.text import CountVectorizer\ncv = CountVectorizer()","92cda66d":"#using count vectorizer\nX =cv.fit_transform(df['transformed_text']).toarray()","015aa6a0":"X.shape","5c8f9783":"y = df['target'].values","5059268f":"y","bc62597b":"from sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=2)","296ed779":"from sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import accuracy_score,confusion_matrix,precision_score","7a7b8618":"mnb = MultinomialNB()","74d90d51":"mnb.fit(X_train,y_train)\ny_pred2 = mnb.predict(X_test)\nprint(accuracy_score(y_test,y_pred2))\nprint(confusion_matrix(y_test,y_pred2))\nprint(precision_score(y_test,y_pred2))","57238622":"from sklearn.feature_extraction.text import TfidfVectorizer","a025a78a":" tfidf = TfidfVectorizer()","926dff67":"X = tfidf.fit_transform(df['transformed_text']).toarray()","c8cf297e":"X.shape","add304b1":"y =df['target'].values\ny.shape","1956b2bf":"from sklearn.linear_model import LogisticRegression","7035b63e":"lrc = LogisticRegression(solver='liblinear', penalty='l1')","e59e6386":"def train_classifier(lrc,X_train,y_train,X_test,y_test):\n    lrc.fit(X_train,y_train)\n    y_pred = lrc.predict(X_test)\n    accuracy = accuracy_score(y_test,y_pred)\n    precision = precision_score(y_test,y_pred)\n    \n    return accuracy,precision","6005278d":"train_classifier(lrc,X_train,y_train,X_test,y_test)","98d7f379":"#### using tf-idf","9bbc82c6":"### spam - top 30 words","e7b5fe5e":"\n# 1. Data Cleaning","04ae4627":"if we have to convert the text data into numarical data \n\nwe can do it by three ways\n\n    1.Bag of words\n    2.tf-Idf\n    3.word2vec","d659e52b":"# 3.Data Preprocessing\n\n    -Lower case\n    -Tokenization = breakdown into words\n    -Removing special charecters\n    -Removing stop words and punctuation\n    -Stemming = converting into base word (eg = dancing,danced = dance)","34c5bd1b":"# 2.EDA","50df57c0":"### Ham-top 30 words"}}