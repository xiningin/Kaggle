{"cell_type":{"d7d21b08":"code","6f5cf115":"code","c81f7a37":"code","c48ce260":"code","554841c0":"code","5ce35bdd":"code","673f305d":"code","0a794997":"code","8c76fbca":"code","5152e37e":"code","8db506e5":"code","aa56e020":"code","c3c9b4f5":"markdown","5ecad590":"markdown","07716f25":"markdown","52e671a6":"markdown","0be5ca59":"markdown","1fec908e":"markdown","9a29dba2":"markdown","99ad2c8a":"markdown"},"source":{"d7d21b08":"import pandas as pd\nimport pandas_profiling as pp\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport warnings\nimport os\nimport plotly.graph_objects as go\nimport plotly.io as pio\nimport pickle\nfrom sklearn.utils import resample\n# Metrics\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix, auc, roc_curve\n\n# Validation\nfrom sklearn.model_selection import train_test_split, cross_val_score, KFold\nfrom sklearn.pipeline import Pipeline, make_pipeline\n\n# Tuning\nfrom sklearn.model_selection import GridSearchCV\n\n# Feature Extraction\nfrom sklearn.feature_selection import RFE\n\n# Preprocessing\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler, Normalizer, Binarizer, LabelEncoder\n\n# Models\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\n\n# Ensembles\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier\n\nwarnings.filterwarnings('ignore')\n\n\nsns.set_style(\"whitegrid\", {'axes.grid' : False})\npio.templates.default = \"plotly_white\"\n\n\n# Analyze Data\ndef explore_data(df):\n    print(\"Number of Instances and Attributes:\", df.shape)\n    print('\\n')\n    print('Dataset columns:',df.columns)\n    print('\\n')\n    print('Data types of each columns: ', df.info())\n    \n# Checking for duplicates\ndef checking_removing_duplicates(df):\n    count_dups = df.duplicated().sum()\n    print(\"Number of Duplicates: \", count_dups)\n    if count_dups >= 1:\n        df.drop_duplicates(inplace=True)\n        print('Duplicate values removed!')\n    else:\n        print('No Duplicate values')\n    \n\n# Split training and validation set\ndef read_in_and_split_data(data, target):\n    X = data.drop(target, axis=1)\n    y = data[target]\n    X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2, random_state=0)\n    return X_train, X_test, y_train, y_test\n    \n# Spot-Check Algorithms\ndef GetModel():\n    Models = []\n    Models.append(('LR'   , LogisticRegression()))\n    Models.append(('LDA'  , LinearDiscriminantAnalysis()))\n    Models.append(('KNN'  , KNeighborsClassifier()))\n    Models.append(('CART' , DecisionTreeClassifier()))\n    Models.append(('NB'   , GaussianNB()))\n    Models.append(('SVM'  , SVC(probability=True)))\n    return Models\n\ndef ensemblemodels():\n    ensembles = []\n    ensembles.append(('AB'   , AdaBoostClassifier()))\n    ensembles.append(('GBM'  , GradientBoostingClassifier()))\n    ensembles.append(('RF'   , RandomForestClassifier()))\n    ensembles.append(( 'Bagging' , BaggingClassifier()))\n    ensembles.append(('ET', ExtraTreesClassifier()))\n    return ensembles\n\n# Spot-Check Normalized Models\ndef NormalizedModel(nameOfScaler):\n    \n    if nameOfScaler == 'standard':\n        scaler = StandardScaler()\n    elif nameOfScaler =='minmax':\n        scaler = MinMaxScaler()\n    elif nameOfScaler == 'normalizer':\n        scaler = Normalizer()\n    elif nameOfScaler == 'binarizer':\n        scaler = Binarizer()\n\n    pipelines = []\n    pipelines.append((nameOfScaler+'LR'  , Pipeline([('Scaler', scaler),('LR'  , LogisticRegression())])))\n    pipelines.append((nameOfScaler+'LDA' , Pipeline([('Scaler', scaler),('LDA' , LinearDiscriminantAnalysis())])))\n    pipelines.append((nameOfScaler+'KNN' , Pipeline([('Scaler', scaler),('KNN' , KNeighborsClassifier())])))\n    pipelines.append((nameOfScaler+'CART', Pipeline([('Scaler', scaler),('CART', DecisionTreeClassifier())])))\n    pipelines.append((nameOfScaler+'NB'  , Pipeline([('Scaler', scaler),('NB'  , GaussianNB())])))\n    pipelines.append((nameOfScaler+'SVM' , Pipeline([('Scaler', scaler),('SVM' , SVC())])))\n    pipelines.append((nameOfScaler+'AB'  , Pipeline([('Scaler', scaler),('AB'  , AdaBoostClassifier())])  ))\n    pipelines.append((nameOfScaler+'GBM' , Pipeline([('Scaler', scaler),('GMB' , GradientBoostingClassifier())])  ))\n    pipelines.append((nameOfScaler+'RF'  , Pipeline([('Scaler', scaler),('RF'  , RandomForestClassifier())])  ))\n    pipelines.append((nameOfScaler+'ET'  , Pipeline([('Scaler', scaler),('ET'  , ExtraTreesClassifier())])  ))\n\n    return pipelines\n\n# Train model\ndef fit_model(X_train, y_train,models):\n    # Test options and evaluation metric\n    num_folds = 10\n    scoring = 'accuracy'\n\n    results = []\n    names = []\n    for name, model in models:\n        kfold = KFold(n_splits=num_folds, shuffle=True, random_state=0)\n        cv_results = cross_val_score(model, X_train, y_train, cv=kfold, scoring=scoring)\n        results.append(cv_results)\n        names.append(name)\n        msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n        print(msg)\n        \n    return names, results\n\n# Save trained model\ndef save_model(model,filename):\n    pickle.dump(model, open(filename, 'wb'))\n\n# Performance Measure\ndef classification_metrics(model, conf_matrix):\n    print(f\"Training Accuracy Score: {model.score(X_train, y_train) * 100:.1f}%\")\n    print(f\"Validation Accuracy Score: {model.score(X_test, y_test) * 100:.1f}%\")\n    fig,ax = plt.subplots(figsize=(8,6))\n    sns.heatmap(pd.DataFrame(conf_matrix), annot = True, cmap = 'YlGnBu',fmt = 'g')\n    ax.xaxis.set_label_position('top')\n    plt.tight_layout()\n    plt.title('Confusion Matrix', fontsize=20, y=1.1)\n    plt.ylabel('Actual label', fontsize=15)\n    plt.xlabel('Predicted label', fontsize=15)\n    plt.show()\n    print(classification_report(y_test, y_pred))\n    \n# ROC_AUC\ndef roc_auc(y_test, y_pred):\n    fpr, tpr, thresholds = roc_curve(y_test, y_pred)\n    plt.figure(figsize=(8,6))\n    print(f\"roc_auc score: {auc(fpr, tpr)*100:.1f}%\")\n    plt.plot(fpr, tpr, color='orange', label='ROC')\n    plt.plot([0, 1], [0, 1], color='darkblue', linestyle='--')\n    plt.xlabel('False Positive Rate',fontsize=12)\n    plt.ylabel('True Positive Rate', fontsize=12)\n    plt.title('Receiver Operating Characteristic (ROC) Curve', fontsize=20)\n    plt.legend()\n    plt.show()\n\n","6f5cf115":"df = pd.read_csv('..\/input\/crop-recommendation-dataset\/Crop_recommendation.csv')\ndf.head()","c81f7a37":"explore_data(df)","c48ce260":"checking_removing_duplicates(df)","554841c0":"df.isna().sum()","5ce35bdd":"# All columns contain outliers except for rice and label you can check outliers by using boxplot\nQ1 = df.quantile(0.25)\nQ3 = df.quantile(0.75)\nIQR = Q3 - Q1\n\ndf_out = df[~((df < (Q1 - 1.5 * IQR)) |(df > (Q3 + 1.5 * IQR))).any(axis=1)]","673f305d":"target ='label'\nX_train, X_test, y_train, y_test = read_in_and_split_data(df, target)\n\nmodels = GetModel()\nnames,results = fit_model(X_train, y_train,models)","0a794997":"ScaledModel = NormalizedModel('minmax')\nname,results = fit_model(X_train, y_train, ScaledModel)","8c76fbca":"ScaledModel = NormalizedModel('standard')\nname,results = fit_model(X_train, y_train, ScaledModel)","5152e37e":"ScaledModel = NormalizedModel('normalizer')\nname,results = fit_model(X_train, y_train, ScaledModel)","8db506e5":"pipeline = make_pipeline(MinMaxScaler(),  GaussianNB())\nmodel = pipeline.fit(X_train, y_train)\ny_pred = model.predict(X_test)\nconf_matrix = confusion_matrix(y_test,y_pred)\nclassification_metrics(pipeline, conf_matrix)\n\n# save model\nsave_model(model, 'model.pkl')","aa56e020":"N = 90\nP = 42\nK = 43\ntemperature = 20.82312\nhumidity = 82.00284\nph = 6.50232\nrainfall = 202.93536\n\n\nsample = [N, P, K, temperature, humidity, ph, rainfall]\nsingle_sample = np.array(sample).reshape(1,-1)\npred = model.predict(single_sample)\npred.item().title()","c3c9b4f5":"# [Train model](http:\/\/)","5ecad590":"# [Checking for Nan Values and duplicates](http:\/\/)","07716f25":"# [Performance Evaluation](http:\/\/)","52e671a6":"# [Predict unseen data](http:\/\/)","0be5ca59":"# [Data Analysis](http:\/\/)","1fec908e":"# [Experiment Different preprocessing techniques](http:\/\/)","9a29dba2":"# [Checking and removing outliers](http:\/\/)\n\n* To check outliers of each columns use boxplot\n\n### These 6 columns have outliers\n\n* P\n* K\n* temperature\n* humidity\n* ph\n* rainfall\n\n\n\n\n\n","99ad2c8a":"# [Crop Recommendation Using Machine Learning](http:\/\/)\n\n![](https:\/\/i.imgur.com\/TnsSPQy.png)\n\n\nThe web app is available at: https:\/\/ai-crop-recommender.herokuapp.com\/\n\nSource Code: https:\/\/github.com\/gabbygab1233\/Crop-Recommendation"}}