{"cell_type":{"8e6a03d9":"code","739799d6":"code","298ab9fa":"code","e2a9d535":"code","7f089f6d":"code","ee34e967":"code","3782f03c":"code","8be8d8dc":"code","e39a6da7":"code","d4bc15a7":"code","c907a2dd":"code","b470713b":"code","08b82573":"code","9769415b":"code","59588c41":"code","fa2470b6":"markdown","d20bc734":"markdown","c830e86a":"markdown","eedfb6bb":"markdown","10e44e9d":"markdown","f1fd0ffa":"markdown","bd203c09":"markdown","3115e791":"markdown"},"source":{"8e6a03d9":"import sys\n!cp ..\/input\/rapids\/rapids.0.15.0 \/opt\/conda\/envs\/rapids.tar.gz\n!cd \/opt\/conda\/envs\/ && tar -xzvf rapids.tar.gz > \/dev\/null\nsys.path = [\"\/opt\/conda\/envs\/rapids\/lib\/python3.7\/site-packages\"] + sys.path\nsys.path = [\"\/opt\/conda\/envs\/rapids\/lib\/python3.7\"] + sys.path\nsys.path = [\"\/opt\/conda\/envs\/rapids\/lib\"] + sys.path \n!cp \/opt\/conda\/envs\/rapids\/lib\/libxgboost.so \/opt\/conda\/lib\/","739799d6":"import cudf\nimport torch\nimport joblib\nimport janestreet\nimport numpy as np\nimport cupy as cp\nfrom time import time\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom contextlib import contextmanager\nfrom sklearn.metrics import roc_auc_score\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import QuantileTransformer\nfrom cupyx.scipy.special import erfinv as cupy_erfinv\n\n\n","298ab9fa":"EPOCHS = 10\nLEARNING_RATE = 1e-3\nWEIGHT_DECAY = 1e-5\nEARLY = 4\nDEVICE = ('cuda' if torch.cuda.is_available() else 'cpu')","e2a9d535":"@contextmanager\ndef timer(name):\n    t0 = time()\n    yield\n    print(f'[{name}] done in {time() - t0:.2f} s')\n","7f089f6d":"with timer('cuDF'):\n    train = cudf.read_csv('..\/input\/jane-street-market-prediction\/train.csv',nrows=1e4)\n    test = cudf.read_csv(\"..\/input\/jane-street-market-prediction\/example_test.csv\")\n","ee34e967":"drop_cols = list(np.setdiff1d(train.columns,test.columns)) + ['ts_id','date']\ntrain.head(3)","3782f03c":"class janeDataset(Dataset):\n    \n    def __init__(self,df,target,mode=\"train\"):\n        \n        self.df = df.values\n        self.mode = mode\n        if self.mode == 'train':\n            self.target = target.values\n        \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self,idx):\n        \n        if self.mode==\"train\":\n            \n            return {'x':torch.FloatTensor(self.df[idx,:]),\n                    'y':torch.FloatTensor(self.target[idx])}\n        else:\n            \n            return {'x':torch.FloatTensor(self.df[idx,:])}\n            \n    \n    ","8be8d8dc":"class JaneModel(nn.Module):\n    \n    def __init__(self):\n        super(JaneModel,self).__init__()\n        \n        self.hidden = [131,64,16]\n        self.batch1 = nn.BatchNorm1d(self.hidden[0])\n        self.dropout1 = nn.Dropout(0.2)\n        self.dense1 = nn.utils.weight_norm(nn.Linear(self.hidden[0],self.hidden[1]))\n        \n        self.batch2 = nn.BatchNorm1d(self.hidden[1])\n        self.dropout2 = nn.Dropout(0.15)\n        self.dense2 = nn.utils.weight_norm(nn.Linear(self.hidden[1],self.hidden[2]))\n        \n        \n        self.batch3 = nn.BatchNorm1d(self.hidden[2])\n        self.dense3 = nn.utils.weight_norm(nn.Linear(self.hidden[2],1))\n        \n        \n    def forward(self,x):\n        \n        x = self.batch1(x)\n        x = self.dropout1(x)\n        x = F.leaky_relu(self.dense1(x))\n        \n        x = self.batch2(x)\n        x = self.dropout2(x)\n        x = F.leaky_relu(self.dense2(x))\n    \n        x = self.batch3(x)\n        x = torch.sigmoid(self.dense3(x))\n        \n        return x\n        \n        \n        \n        ","e39a6da7":"train=train[train['weight']!=0]\ntarget = (train['resp']>0)*1\nprint(train.shape[0])","d4bc15a7":"from sklearn.preprocessing import StandardScaler\n\ndef do_preprocess(train,mode=1):\n    \n    features = [f'feature_{i}' for i in range(1,130)]+['weight']\n    \n    def to_labels(x):\n        if x==1:\n            return 0\n        else:\n            return 1\n    \n    \n    for col in features :\n        \n        train[col].fillna(train[col].mean(),inplace=True)\n        \n    if mode:\n\n            transformer = StandardScaler()\n            matrix = train[features].as_matrix()\n            scaled_data = transformer.fit_transform(matrix)\n            scaled_data  = cudf.DataFrame(scaled_data)\n            scaled_data.columns = features\n            joblib.dump(transformer,f'{col}.pkl')\n        \n    else:\n            transformer = joblib.load(f'{col}.pkl')\n            matrix = train[features].as_matrix()\n            scaled_data = transformer.transform(matrix)\n            scaled_data = cudf.DataFrame(scaled_data)\n            scaled_data.columns = features\n\n            \n\n   \n    train['feature_0'].fillna(-1,inplace=True)\n    scaled_data['feature_0']=train['feature_0'].applymap(to_labels).values\n    \n    \n\n        \n        \n    return train\n\n\n\ntrain = do_preprocess(train)\n","c907a2dd":"def train_fn(model, optimizer, scheduler, loss_fn, dataloader, device):\n    model.train()\n    final_loss = 0\n    final_auc = 0\n    \n    for data in dataloader:\n        optimizer.zero_grad()\n        inputs, targets = data['x'].to(device), data['y'].to(device)\n        outputs = model(inputs).squeeze()\n        loss = loss_fn(outputs, targets)\n        auc = roc_auc_score(targets.detach().cpu().numpy(),outputs.detach().cpu().numpy())\n        loss.backward()\n        optimizer.step()\n        scheduler.step()\n        \n        final_loss += loss.item()\n        final_auc += auc\n        \n    final_loss \/= len(dataloader)\n    final_auc \/= len(dataloader)\n    \n    return final_loss,final_auc\n\ndef valid_fn(model, loss_fn, dataloader, device):\n    model.eval()\n    final_loss = 0\n    final_auc = 0\n    valid_preds = []\n    \n    for data in dataloader:\n        inputs, targets = data['x'].to(device), data['y'].to(device)\n        outputs = model(inputs).squeeze()\n        loss = loss_fn(outputs, targets)\n        auc = roc_auc_score(targets.detach().cpu().numpy(),outputs.detach().cpu().numpy())\n        \n        final_loss += loss.item()\n        final_auc += auc\n        valid_preds.append(outputs.sigmoid().detach().cpu().numpy())\n        \n    final_loss \/= len(dataloader)\n    final_auc \/= len(dataloader)\n    valid_preds = np.concatenate(valid_preds)\n    \n    return final_loss,final_auc,valid_preds\n\ndef inference_fn(model, dataloader, device):\n    model.eval()\n    preds = []\n    \n    for data in dataloader:\n        inputs = data['x'].to(device)\n\n        with torch.no_grad():\n            outputs = model(inputs)\n        \n        preds.append(outputs.sigmoid().detach().cpu().numpy())\n        \n    preds = np.concatenate(preds)\n    \n    return preds","b470713b":"def train_model(train,target):\n    \n    train.fillna(-1,inplace=True)\n    X_train,X_valid,y_train,y_valid  = train_test_split(train.drop(drop_cols,axis=1),target,test_size=0.15)\n    \n    train_data = janeDataset(X_train,y_train)\n    valid_data = janeDataset(X_valid,y_valid)\n    \n    train_data = DataLoader(train_data,batch_size=2**10,shuffle=True)\n    valid_data = DataLoader(valid_data,batch_size=2**10,shuffle=True)\n    \n    model = JaneModel()\n    model.to(DEVICE)\n    \n    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n    scheduler = optim.lr_scheduler.OneCycleLR(optimizer=optimizer, pct_start=0.1, div_factor=1e3, \n                                              max_lr=1e-2, epochs=EPOCHS, steps_per_epoch=len(train_data))\n    loss_fn = nn.BCEWithLogitsLoss()\n    best_loss = np.inf\n    \n    for epoch in range(EPOCHS):\n            \n            train_loss,train_auc = train_fn(model, optimizer, scheduler, loss_fn, train_data, DEVICE)\n            final_loss,valid_auc,valid_pred = valid_fn(model, loss_fn, valid_data, DEVICE)\n            print(f\" Epoch {epoch} train loss {train_loss : .5f} valid loss {final_loss : .5f} train_auc {train_auc: .4f} valid_auc {valid_auc : .4f}\")\n            \n            if final_loss<best_loss:\n                \n                best_loss = final_loss\n                torch.save(model.state_dict(),f'jane_model.pth')\n                early_stop=0\n            if EARLY:\n                early_stop+=1\n                if early_stop>EARLY:\n                    break\n        \n        \n\ntrain_model(train,target)\n    \n    ","08b82573":"model = JaneModel()\nmodel.load_state_dict(torch.load(\"jane_model.pth\"))\nmodel.to(DEVICE)\n","9769415b":"env = janestreet.make_env() \niter_test = env.iter_test()","59588c41":"from tqdm import tqdm\n\nfor (test,sample_pred) in tqdm(iter_test):\n    \n    test = cudf.from_pandas(test)\n    test = test[train.drop(drop_cols,axis=1).columns]\n    test = do_preprocess(test,mode=0)\n    test_ = janeDataset(test,None,mode='test')\n    test_ = DataLoader(test_,batch_size=2**12,shuffle=False)\n    predictions = inference_fn(model,test_,DEVICE)\n    sample_pred.action = np.round(predictions).reshape(1,-1)\n    env.predict(sample_pred)\n","fa2470b6":"## <font size='4' color='blue'><a> Inference <\/a><\/font>","d20bc734":"## <font size='4' color='blue'><a> Dataset <\/a><\/font>","c830e86a":"### <font size='4' ><a> Preprocess <\/a><\/font>","eedfb6bb":"## <font size='4' color='blue'><a> Model <\/a><\/font>","10e44e9d":"## <font size='4' color='blue'><a> Read Data <\/a><\/font>","f1fd0ffa":"## <font size='4' color='blue'><a> Training <\/a><\/font>","bd203c09":"## <font size='4' color='blue'><a> Imports <\/a><\/font>","3115e791":"## <font size='4' color='green'><a> WORK IN PROGRESS !!! DO AN UPVOTE IF YOU LIKED IT <\/a><\/font>"}}