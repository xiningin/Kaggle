{"cell_type":{"b832a6f2":"code","fce4eb83":"code","70f2ee62":"code","bdebf4e8":"code","3f8d8389":"code","6cf575a9":"code","68e6c9ca":"code","25aaa6ae":"code","f5954aa3":"code","13f06eee":"code","70616bfb":"code","d460af78":"code","f541a5b3":"code","da6e6deb":"code","5c87c40e":"code","ca336950":"code","3323d0fc":"code","01fe359d":"code","0bd6c80b":"code","2efe700a":"code","3a766ec3":"markdown","7f5f7b65":"markdown","c7b03948":"markdown","6b382755":"markdown","36a6910a":"markdown","45f31d4c":"markdown","b34a2f6f":"markdown","323fb1a7":"markdown","d5ef4a75":"markdown","de7f44e5":"markdown","2b273c05":"markdown","3c3da015":"markdown","f2504748":"markdown","9011dbb9":"markdown","0f21497a":"markdown","84cd44a3":"markdown","e586bdf0":"markdown","c76867b6":"markdown","220eb91e":"markdown","76d34ccd":"markdown","f6f616b7":"markdown"},"source":{"b832a6f2":"from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D\nfrom keras.models import Sequential\nimport pandas as pd \nimport numpy as np \nimport keras","fce4eb83":"train = pd.read_csv(\"..\/input\/train.csv\")\ntest = pd.read_csv(\"..\/input\/test.csv\")\n\n# include only the rows having label = 0 or 1 (binary classification)\nX = train[train['label'].isin([0, 1])]\n\n# target variable\nY = train[train['label'].isin([0, 1])]['label']\n\n# remove the label from X\nX = X.drop(['label'], axis = 1)","70f2ee62":"# implementing a sigmoid activation function\ndef sigmoid(z):\n    s = 1.0\/ (1 + np.exp(-z))    \n    return s","bdebf4e8":"def network_architecture(X, Y):\n    # nodes in input layer\n    n_x = X.shape[0] \n    # nodes in hidden layer\n    n_h = 10          \n    # nodes in output layer\n    n_y = Y.shape[0] \n    return (n_x, n_h, n_y)","3f8d8389":"def define_network_parameters(n_x, n_h, n_y):\n    W1 = np.random.randn(n_h,n_x) * 0.01 # random initialization\n    b1 = np.zeros((n_h, 1)) # zero initialization\n    W2 = np.random.randn(n_y,n_h) * 0.01 \n    b2 = np.zeros((n_y, 1)) \n    return {\"W1\": W1, \"b1\": b1, \"W2\": W2, \"b2\": b2}    ","6cf575a9":"def forward_propagation(X, params):\n    Z1 = np.dot(params['W1'], X)+params['b1']\n    A1 = sigmoid(Z1)\n\n    Z2 = np.dot(params['W2'], A1)+params['b2']\n    A2 = sigmoid(Z2)\n    return {\"Z1\": Z1, \"A1\": A1, \"Z2\": Z2, \"A2\": A2}    ","68e6c9ca":"def compute_error(Predicted, Actual):\n    logprobs = np.multiply(np.log(Predicted), Actual)+ np.multiply(np.log(1-Predicted), 1-Actual)\n    cost = -np.sum(logprobs) \/ Actual.shape[1] \n    return np.squeeze(cost)","25aaa6ae":"def backward_propagation(params, activations, X, Y):\n    m = X.shape[1]\n    \n    # output layer\n    dZ2 = activations['A2'] - Y # compute the error derivative \n    dW2 = np.dot(dZ2, activations['A1'].T) \/ m # compute the weight derivative \n    db2 = np.sum(dZ2, axis=1, keepdims=True)\/m # compute the bias derivative\n    \n    # hidden layer\n    dZ1 = np.dot(params['W2'].T, dZ2)*(1-np.power(activations['A1'], 2))\n    dW1 = np.dot(dZ1, X.T)\/m\n    db1 = np.sum(dZ1, axis=1,keepdims=True)\/m\n    \n    return {\"dW1\": dW1, \"db1\": db1, \"dW2\": dW2, \"db2\": db2}\n\ndef update_parameters(params, derivatives, alpha = 1.2):\n    # alpha is the model's learning rate \n    \n    params['W1'] = params['W1'] - alpha * derivatives['dW1']\n    params['b1'] = params['b1'] - alpha * derivatives['db1']\n    params['W2'] = params['W2'] - alpha * derivatives['dW2']\n    params['b2'] = params['b2'] - alpha * derivatives['db2']\n    return params","f5954aa3":"def neural_network(X, Y, n_h, num_iterations=100):\n    n_x = network_architecture(X, Y)[0]\n    n_y = network_architecture(X, Y)[2]\n    \n    params = define_network_parameters(n_x, n_h, n_y)\n    for i in range(0, num_iterations):\n        results = forward_propagation(X, params)\n        error = compute_error(results['A2'], Y)\n        derivatives = backward_propagation(params, results, X, Y) \n        params = update_parameters(params, derivatives)    \n    return params","13f06eee":"y = Y.values.reshape(1, Y.size)\nx = X.T.as_matrix()\nmodel = neural_network(x, y, n_h = 10, num_iterations = 10)","70616bfb":"def predict(parameters, X):\n    results = forward_propagation(X, parameters)\n    print (results['A2'][0])\n    predictions = np.around(results['A2'])    \n    return predictions\n\npredictions = predict(model, x)\nprint ('Accuracy: %d' % float((np.dot(y,predictions.T) + np.dot(1-y,1-predictions.T))\/float(y.size)*100) + '%')","d460af78":"from sklearn.model_selection import train_test_split\nfrom sklearn import neural_network\nfrom sklearn import  metrics\n\nY = train['label'][:10000] # use more number of rows for more training \nX = train.drop(['label'], axis = 1)[:10000] # use more number of rows for more training \nx_train, x_val, y_train, y_val = train_test_split(X, Y, test_size=0.20, random_state=42)","f541a5b3":"model = neural_network.MLPClassifier(alpha=1e-5, hidden_layer_sizes=(5,), solver='lbfgs', random_state=18)\nmodel.fit(x_train, y_train)","da6e6deb":"predicted = model.predict(x_val)\nprint(\"Classification Report:\\n %s:\" % (metrics.classification_report(y_val, predicted)))","5c87c40e":"Y = train['label']\nX = train.drop(['label'], axis=1)\n\nx_train, x_val, y_train, y_val = train_test_split(X.as_matrix(), Y.as_matrix(), test_size=0.10, random_state=42)","ca336950":"# network parameters \nbatch_size = 128\nnum_classes = 10\nepochs = 5 # Further Fine Tuning can be done\n\n# input image dimensions\nimg_rows, img_cols = 28, 28","3323d0fc":"# preprocess the train data \nx_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\nx_train = x_train.astype('float32')\nx_train \/= 255\n\n# preprocess the validation data\nx_val = x_val.reshape(x_val.shape[0], img_rows, img_cols, 1)\nx_val = x_val.astype('float32')\nx_val \/= 255\n\ninput_shape = (img_rows, img_cols, 1)\n\n# convert the target variable \ny_train = keras.utils.to_categorical(y_train, num_classes)\ny_val = keras.utils.to_categorical(y_val, num_classes)\n\n# preprocess the test data\nXtest = test.as_matrix()\nXtest = Xtest.reshape(Xtest.shape[0], img_rows, img_cols, 1)","01fe359d":"model = Sequential()\n\n# add first convolutional layer\nmodel.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=input_shape))\n\n# add second convolutional layer\nmodel.add(Conv2D(64, (3, 3), activation='relu'))\n\n# add one max pooling layer \nmodel.add(MaxPooling2D(pool_size=(2, 2)))\n\n# add one dropout layer\nmodel.add(Dropout(0.25))\n\n# add flatten layer\nmodel.add(Flatten())\n\n# add dense layer\nmodel.add(Dense(128, activation='relu'))\n\n# add another dropout layer\nmodel.add(Dropout(0.5))\n\n# add dense layer\nmodel.add(Dense(num_classes, activation='softmax'))\n\n# complile the model and view its architecur\nmodel.compile(loss=keras.losses.categorical_crossentropy,  optimizer=keras.optimizers.Adadelta(), metrics=['accuracy'])\n\nmodel.summary()","0bd6c80b":"model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_data=(x_val, y_val))\naccuracy = model.evaluate(x_val, y_val, verbose=0)\nprint('Test accuracy:', accuracy[1])","2efe700a":"pred = model.predict(Xtest)\ny_classes = pred.argmax(axis=-1)\nres = pd.DataFrame()\nres['ImageId'] = list(range(1,28001))\nres['Label'] = y_classes\nres.to_csv(\"output.csv\", index = False)","3a766ec3":"### 2.9 Predictions ","7f5f7b65":"### 3.2 Train the Model\n\nTrain a neural network model with 10 hidden layers. ","c7b03948":"### 5.5 Train the Model","6b382755":"# <a>Understanding and Implementing Neural Networks from scratch<\/a>\n\n<br>\n\nIn this kernel, I have explained the intution about neural networks and how to implement neural networks from scratch in python. \n\n## Contents  \n<br>\n\n**<a><i> 1. What are Neural Networks<\/i><\/a> **  \n\n**<a><i> 2. Implement a Neural Network - Binary classification<\/i><\/a>**   \n\n**<a><i> 3. Implement a Neural Network - Multiclass classification<\/i><\/a>**  \n\n**<a><i> 4. What are Deep Neural Networks<\/i><\/a> **  \n\n**<a><i> 5. Convolutional Neural Networks Implementation<\/i><\/a>**  \n\n![](https:\/\/www.pangeanic.com\/wp-content\/uploads\/sites\/2\/2017\/07\/neural-network-graph-624x492.jpg)\n\nI would like to thank Andrew NG and deeplearning.ai course for their excellent material\n\n","36a6910a":"### 2.1 Dataset Preparation\n\nFirst step is to load and prepare the dataset","45f31d4c":"### 5.4 Create the CNN Model Architecture\n\nIn this step, create the convolutional neural network architecture with following layers: \n\n1. Convolutional Layer with kernel size = 3*3, 32 convolutional units, and RelU activation function \n2. Convolutional Layer with kernel size = 3*3, 64 convolutional units, and RelU activation function  \n3. Max Pooling Layer with pooling matrix size = 2*2\n4. Dropout Layer : A dropout layer is used for regularization and reducing the overfitting \n5. Flatten Layer : A layer to convert the output in one dimentional array\n6. Dense Layer : A dense layer is a fully connected layer in which every node is connected to every other node in the previous and next layers. In our network, it contains 128 neurons but this number can be changed for further experiments. \n7. Another Dropout Layer for regularization \n8. Final output layer : A dense layer with 10 neurons for generating the output class \n\nIn the simple neural network that we implemented in step 1, the loss function was LogLoss function and the optimizing Algorithm was Gradient Descent, In this neural network, we will use categorical_crossentropy as this is a multi class classification as the loss function and Adadelta as the optimizing function. ","b34a2f6f":"## <a>1. What are Neural Networks <\/a>\n\nNeural networks are a type of machine learning models which are designed to operate similar to biological neurons and human nervous system. These models are used to recognize complex patterns and relationships that exists within a labelled dataset. They have following properties:\n\n1. The core architecture of a Neural Network model is comprised of a large number of simple processing nodes called Neurons which are interconnected and organized in different layers. \n\n2. An individual node in a layer is connected to several other nodes in the previous and the next layer. The inputs form one layer are received and processed to generate the output which is passed to the next layer.\n\n3. The first layer of this architecture is often named as input layer which accepts the inputs, the last layer is named as the output layer which produces the output and every other layer between input and output layer is named is hidden layers. \n\n### Key concepts in a Neural Network \n\n#### A. Neuron:\n\nA Neuron is a single processing unit of a Neural Network which are connected to different other neurons in the network. These connections repersents inputs and ouputs from a neuron. To each of its connections, the neuron assigns a \u201cweight\u201d (W) which signifies the importance the input and adds a bias (b) term. \n\n#### B. Activation Functions \n\nThe activation functions are used to apply non-linear transformation on input to map it to output. The aim of activation functions is to predict the right class of the target variable based on the input combination of variables. Some of the popular activation functions are Relu, Sigmoid, and TanH. \n\n#### C. Forward Propagation \n\nNeural Network model goes through the process called forward propagation in which it passes the computed activation outputs in the forward direction. \n\nZ = W*X + b   \nA = g(Z) \n\n- g is the activation function \n- A is the activation using the input \n- W is the weight associated with the input \n- B is the bias associated with the node \n\n#### D. Error Computation: \n\nThe neural network learns by improving the values of weights and bias. The model computes the error in the predicted output in the final layer which is then used to make small adjustments the weights and bias. The adjustments are made such that the total error is minimized. Loss function measures the error in the final layer and cost function measures the total error of the network. \n\nLoss = Actual_Value - Predicted_Value   \n\nCost = Summation (Loss)   \n\n#### E. Backward Propagation: \n\nNeural Network model undergoes the process called backpropagation in which the error is passed to backward layers so that those layers can also improve the associated values of weights and bias. It uses the algorithm called Gradient Descent in which the error is minimized and optimal values of weights and bias are obtained. This weights and bias adjustment is done by computing the derivative of error, derivative of weights, bias and subtracting them from the original values. \n\n<br>\n\n## <a> 2. Implement a Neural Network - Binary Classification<\/a>  \n\nLets implement a basic neural network in python for binary classification which is used to classify if a given image is 0 or 1.  ","323fb1a7":"### 2.8 Compile and Train the Model\n\nCreate a function which compiles all the key functions and creates a neural network model. ","d5ef4a75":"### 2.3 Define Neural Network Architecture\n\nCreate a model with three layers - Input, Hidden, Output. ","de7f44e5":"<br>\n\n## <a>3. Implement a Neural Network - Multiclass Classification <\/a>\n\nIn the previous step, I discussed about how to implement a NN for binary classification in python from scratch. Python's libraries such as sklearn provides an excellent implementation of efficient neural networks which can be used to directly implement neural networks on a dataset.  In this section, lets implement a multi class neural network to classify the digit shown in an image from 0 to 9\n\n### 3.1 Dataset Preparation \n\nSlice the train dataset into train and validation set","2b273c05":"### 2.4 Define Neural Network Parameters \n\nNeural Network parameters are weights and bias which we need to initialze with zero values. The first layer only contains inputs so there are no weights and bias, but the hidden layer and the output layer have a weight and bias term. (W1, b1 and W2, b2)","3c3da015":"### 5.6 Generate Predictions ","f2504748":"Thanks for exploring this far. Cheers :)","9011dbb9":"<br>\n\n## <a> 4. Deep Neural Networks - Convolutional Neural Networks <\/a>\n\nDeep Neural Networks are composed of complex and many number of hidden layers which tries to extract low level features from the images. Some examples of complex deep neural networks are convolutional neural networks and Recurrent Neural Networks. \n\n###  Convolutional Neural Networks \n\nIn Convolutional Neural Networks, every image input is treated as a a matrix of pixel values which represents the amount of darkness at a given pixel in the image. Unlike, tradational neural networks which treats an image as a one dimentional network, CNNs considers the location of pixels and the neighbours for classification.\n\n<br>\n\n![](http:\/\/www.mdpi.com\/information\/information-07-00061\/article_deploy\/html\/images\/information-07-00061-g001.png)\n\n<br>\n\n### Key components of Convolutional Neural Network. \n\n**A. Convolutional layer: ** In this layer, a kernel (or weight) matrix is used to extract low level features from the images. The kernel with its weights rotates over the image matrix in a sliding window fashion in order to obtained the convolved output. The kernel matrix behaves like a filter in an image extracting particular information from the original image matrix. During the colvolution process, The weights are learnt such that the loss function is minimized.\n\n**B. Stride: ** Stride is defined as the number of steps the kernel or the weight matrix takes while moving across the entire image moving N pixel at a time. If the weight matrix moves N pixel at a time, it is called stride of N.\n\n![](http:\/\/deeplearning.net\/software\/theano\/_images\/numerical_padding_strides.gif) \nImage Credits - www.deeplearning.net\n\n**C. Pooling Layer:**  Pooling layers are used to extract the most informative features from the generated convolved output. \n\n![](https:\/\/upload.wikimedia.org\/wikipedia\/commons\/e\/e9\/Max_pooling.png)\n\n**D. Output Layer:** To generate the final output, a dense or a fully connected layer is applied with the softmax activation function. Softmax function is used to generate the probabilities for each class of the target variable. \n\n<br>\n\n## <a> 5. Implement a Convolution Neural Network <\/a>\n\n### 5.1 Dataset Preparation\n\nIn the first step lets prepare the dataset and slice it into train and validation sets. For the modelling and training purpose, we can use python's library - Keras. ","0f21497a":"### 3.3 Predictions ","84cd44a3":"### 5.2 Define the Network Parameters\n\nNetwork Parameters are : \n\nBatch Size - Number of rows from the input data to use it one iteratation from the training purpose  \nNum Classes - Total number of possible classes in the target variable  \nEpochs - Total number of iterations for which cnn model will run.","e586bdf0":"### 2.2 Implementing a Activation Function \n\nWe will use sigmoid activation function because it outputs the values between 0 and 1 so its a good choice for a binary classification problem","c76867b6":"### 5.3 Preprocess the Inputs\n\nIn the preprocessing step the corresponding image data vectors are reshaped into a 4 dimentional vector : total batch size, width of the image, height of the image, and the channel. In our case, channel = 1 as we will only use single channel instead of three channels (R,G,B). The next step is to normalize the inputs by dividing them by max pixel value ie. 255. ","220eb91e":"### 2.5 Implement Forward Propagation\n\nThe hidden layer and output layer will compute the activations using sigmoid activation function and will pass it in the forward direction. While computing this activation, the input is multiplied with weight and added with bias before passing it to the function. ","76d34ccd":"### 2.6 Compute the Network Error \n\nTo compute the cost, one straight forward approach is to compute the absolute error among prediction and actual value. But a better loss function is the log loss function which is defines as : \n\n  -Summ ( Log (Pred) * Actual + Log (1 - Pred ) * Actual ) \/ m","f6f616b7":"### 2.7 Implement Backward Propagation\n\nIn backward propagation function, the error is passed backward to previous layers and the derivatives of weights and bias are computed. The weights and bias are then updated using the derivatives.  "}}