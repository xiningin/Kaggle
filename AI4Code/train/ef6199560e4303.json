{"cell_type":{"8c5113ca":"code","ec22ff70":"code","5e72b7bc":"code","836ecdf3":"code","5f592f54":"code","4df427ad":"code","afd26f5e":"code","93bb7c25":"code","5af45157":"code","d6b54db5":"code","d4d4bc5e":"code","4fefd123":"code","5fac7ab0":"code","8c33a1ab":"code","b4a57957":"code","d5b13591":"code","7fea23eb":"code","b23d8a97":"code","3acfdbb0":"code","2412277d":"code","764a7680":"code","3f044df4":"code","0435c482":"code","06bc4164":"code","9c8fb5db":"code","c307dda6":"code","3729b876":"code","f018100b":"code","6320126f":"code","866e201c":"code","10669dd6":"code","1ae277d2":"markdown","2288ba31":"markdown","565deedd":"markdown","cad1f10e":"markdown","158f290c":"markdown","dfa8171d":"markdown","442d7a25":"markdown","9c15d844":"markdown","1d4d1e2a":"markdown","cf2facf5":"markdown","ed7d0574":"markdown","c0b6123d":"markdown","5442b4ac":"markdown","82c004be":"markdown","9e92565e":"markdown","c36e08d5":"markdown","9de326fc":"markdown","846bbe37":"markdown","039877e5":"markdown","38bc51ff":"markdown","0733ee1e":"markdown","adda7dcc":"markdown","1ae6ccb3":"markdown","5815726f":"markdown"},"source":{"8c5113ca":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","ec22ff70":"df = pd.read_csv(\"..\/input\/games.csv\")\ndf","5e72b7bc":"df.columns","836ecdf3":"df = df[[\"winner\",\"firstBlood\",\"firstTower\",\"firstInhibitor\",\"firstBaron\",\"firstDragon\",\"firstRiftHerald\",\"t1_towerKills\",\"t1_inhibitorKills\",'t1_baronKills',\n       't1_dragonKills','t2_towerKills','t2_inhibitorKills', 't2_baronKills', 't2_dragonKills'\n       ]]\ndf","5f592f54":"org_df = pd.read_csv(\"..\/input\/games.csv\")\nplt.figure(figsize=(12,6))\np1=sns.kdeplot(org_df['gameDuration'], shade=True, color=\"r\").set_title('Distribution of Duration')","4df427ad":"num_of_missing = []\nfor i in df.columns:\n    missing = sum(df[i].isna())\n    num_of_missing.append(missing)\nnum_of_missing","afd26f5e":"import matplotlib.pyplot as plt\nplt.hist(df[\"t1_towerKills\"])\nplt.show()\nplt.hist(df[\"t2_towerKills\"])\nplt.show()\n(df[\"t1_towerKills\"].mean(),df[\"t2_towerKills\"].mean())","93bb7c25":"# when team 1 get the first blood and win the game\nf1w1 = len(df.loc[(df['winner']==1) & (df['firstBlood']==1)])\/len(df)\n# when team 1 get the first blood and team 2 wins\nf1w2 = len(df.loc[(df['winner']==2) & (df['firstBlood']==1)])\/len(df)\n\nheight1 = [f1w1,f1w2]\nbars1 = (\"team1\",\"team2\")\ny_pos1 = np.arange(len(bars1))\nplt.bar(y_pos1,height1)\nplt.title(\"winning rate when team 1 got first blood\")\nplt.xticks(y_pos1, bars1)\nplt.show()\nprint(abs(f1w1-f1w2))\n\n# when team 2 get the first blood and win the game\nf2w2 = len(df.loc[(df['winner']==2) & (df['firstBlood']==2)])\/len(df)\n# when team 2 get the first blood and team 1 wins\nf2w1 = len(df.loc[(df['winner']==1) & (df['firstBlood']==2)])\/len(df)\n\nheight2 = [f2w1,f2w2]\nbars2 = (\"team1\",\"team2\")\ny_pos2 = np.arange(len(bars2))\nplt.bar(y_pos2,height2)\nplt.title(\"winning rate when team 2 got first blood\")\nplt.xticks(y_pos2, bars2)\nplt.show()\nprint(abs(f2w1-f2w2))","5af45157":"# when team 1 get the first tower and win the game\nt1w1 = len(df.loc[(df['winner']==1) & (df['firstTower']==1)])\/len(df)\n# when team 1 get the first tower and team 2 wins\nt1w2 = len(df.loc[(df['winner']==2) & (df['firstTower']==1)])\/len(df)\n\nheight3 = [t1w1,t1w2]\nbars3 = (\"team1\",\"team2\")\ny_pos3 = np.arange(len(bars3))\nplt.bar(y_pos3,height3)\nplt.title(\"winning rate when team 1 got first tower\")\nplt.xticks(y_pos3, bars3)\nplt.show()\nprint(abs(t1w1-t1w2))\n\n# when team 2 get the first tower and win the game\nt2w2 = len(df.loc[(df['winner']==2) & (df['firstTower']==2)])\/len(df)\n# when team 2 get the first tower and team 1 wins\nt2w1 = len(df.loc[(df['winner']==1) & (df['firstTower']==2)])\/len(df)\n\nheight4 = [t2w1,t2w2]\nbars4 = (\"team1\",\"team2\")\ny_pos4 = np.arange(len(bars4))\nplt.bar(y_pos4,height4)\nplt.title(\"winning rate when team 2 got first tower\")\nplt.xticks(y_pos4, bars4)\nplt.show()\nprint(abs(t2w1-t2w2))","d6b54db5":"# when team 1 get the first baron and win the game\nt4w1 = len(df.loc[(df['winner']==1) & (df['firstBaron']==1)])\/len(df)\n# when team 1 get the first baron and team 2 wins\nt4w2 = len(df.loc[(df['winner']==2) & (df['firstBaron']==1)])\/len(df)\n\nheight3 = [t4w1,t4w2]\nbars3 = (\"team1\",\"team2\")\ny_pos3 = np.arange(len(bars3))\nplt.bar(y_pos3,height3)\nplt.title(\"winning rate when team 1 got first baron\")\nplt.xticks(y_pos3, bars3)\nplt.show()\n\n# when team 2 get the first baron and win the game\nt3w2 = len(df.loc[(df['winner']==2) & (df['firstBaron']==2)])\/len(df)\n# when team 2 get the first baron and team 1 wins\nt3w1 = len(df.loc[(df['winner']==1) & (df['firstBaron']==2)])\/len(df)\n\nheight4 = [t3w1,t3w2]\nbars4 = (\"team1\",\"team2\")\ny_pos4 = np.arange(len(bars4))\nplt.bar(y_pos4,height4)\nplt.title(\"winning rate when team 2 got first baron\")\nplt.xticks(y_pos4, bars4)\nplt.show()","d4d4bc5e":"game = df.copy()","4fefd123":"y = game[\"winner\"].values\nx = game.drop([\"winner\"],axis=1)","5fac7ab0":"from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x,y,test_size = 0.3,random_state=1)","8c33a1ab":"from sklearn.naive_bayes import GaussianNB\nclf1 = GaussianNB()\nclf1.fit(x_train,y_train)\npred = clf1.predict(x_test)\npred","b4a57957":"clf1.score(x_test,y_test)","d5b13591":"from sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import GridSearchCV\ncriterion=[\"gini\",\"entropy\"]\nmax_depth=range(1,20,2)\nsplitter=[\"best\",\"random\"]\ndt=DecisionTreeClassifier()\ngrid_decision_tree=GridSearchCV(estimator=dt,cv=15,param_grid=dict(criterion=criterion,max_depth=max_depth,splitter=splitter))","7fea23eb":"grid_decision_tree.fit(x_train,y_train)\nprint(grid_decision_tree.best_score_)\nprint(grid_decision_tree.best_params_)","b23d8a97":"criterion=[\"gini\",\"entropy\"]\nmax_depth=range(1,20,2)","3acfdbb0":"from sklearn.ensemble import RandomForestClassifier\ngini_score = []\nentropy_score = []\nfor i in max_depth:\n    clf2 = RandomForestClassifier(max_depth=i,criterion=\"gini\")\n    clf2.fit(x_train,y_train)\n    sc = clf2.score(x_test,y_test)\n    gini_score.append(sc)\nfor i in max_depth:\n    clf2 = RandomForestClassifier(max_depth=i,criterion=\"entropy\")\n    clf2.fit(x_train,y_train)\n    sc = clf2.score(x_test,y_test)\n    entropy_score.append(sc)","2412277d":"print(gini_score)\nprint(entropy_score)\n(max(gini_score),max(entropy_score))","764a7680":"max_depth[4]","3f044df4":"from sklearn.neighbors import KNeighborsClassifier\nn_neighbors = [5,7,9,11]\nweights=[\"uniform\",\"distance\"]\nalgorithm = [\"auto\",\"brute\"]\nknn = KNeighborsClassifier()\ngrid_KNN = GridSearchCV(estimator=knn,cv=15,param_grid=dict(n_neighbors=n_neighbors,weights=weights,algorithm=algorithm))","0435c482":"grid_KNN.fit(x_train,y_train)\nprint(grid_KNN.best_score_)\nprint(grid_KNN.best_params_)","06bc4164":"from sklearn.svm import SVC\nkernel = [\"linear\", \"poly\", \"rbf\", \"sigmoid\"]\nsvc = SVC()\ngrid_svc = GridSearchCV(estimator=svc,cv=15,param_grid=dict(kernel=kernel))","9c8fb5db":"grid_svc.fit(x_train,y_train)\nprint(grid_svc.best_score_)\nprint(grid_svc.best_params_)","c307dda6":"final_clf = RandomForestClassifier(max_depth=9,criterion=\"gini\")\nfinal_clf.fit(x_train,y_train)\nprint(\"score:\", final_clf.score(x_test,y_test))","3729b876":"from sklearn.tree import export_graphviz\n# Export as dot file\nexport_graphviz(final_clf.estimators_[0], out_file='tree.dot', \n                feature_names = x.columns,\n                class_names = [\"1\",\"2\"],\n                rounded = True,\n                filled = True)\nfrom subprocess import call\ncall(['dot', '-Tpng', 'tree.dot', '-o', 'tree.png', '-Gdpi=600'])\n\nfrom IPython.display import Image\nImage(filename = 'tree.png')","f018100b":"from sklearn.metrics import confusion_matrix,classification_report\npredicted_values = final_clf.predict(x_test)\ncm=confusion_matrix(y_test,predicted_values)\ncr=classification_report(y_test,predicted_values)\nprint('Classification report : \\n',cr)","6320126f":"import seaborn as sns\ng1 = sns.heatmap(cm,annot=True,fmt=\".1f\",cmap=\"flag\",cbar=False)\ng1.set_ylabel('y_true')\ng1.set_xlabel('y_head')","866e201c":"scenario={\"feature\":[\"first_blood\",\"first_tower\",\"first_inhibitor\",\"first_Baron\",\"first_Dragon\",\"first_RiftHerald\",\n\"t1_tower\",\"t1_inhibitor\",\"t1_baron\",\"t1_dragon\",\"t2_tower\",\"t2_inhibitor\",\"t2_baron\",\"t2_dragon\"],\n         \"value\":[2,0,1,2,2,1,10,2,1,3,5,2,0,1]}\nscen=pd.DataFrame(scenario)\nscen.T","10669dd6":"x1=[[1,1,2,1,1,1,10,2,1,4,7,2,1,1]]\nc=final_clf.predict_proba(x1).reshape(-1,1)\nprint(\"winner is :\" , final_clf.predict(x1) )\nprint(\"first team win probability is % \", list(c[0]*100),\"\\nsecond team win probability is %:\",list(c[1]*100)  )","1ae277d2":"So the best parameter for the random forest model is max_depth=9 and criterion=\"gini\".<br>\nThe accuarcy of this model is 97.38%.","2288ba31":"Select the attributes we are interested and needed for the prediction.","565deedd":"Let's do some EDA to explore the data set.","cad1f10e":"So for this decision tree model, the accuracy is 96.6% and the best parameters are 'criterion': 'entropy', 'max_depth': 7, 'splitter': 'best'.\n","158f290c":"Similar to the previous exploration, we are also interested in how the first tower affect the winning rate of each team. Same as the first blood, first tower also gives players bonus gold in the game.","dfa8171d":"Here are all attributes we have in this dataset.","442d7a25":"From these two graph, we can see that the team who got the first tower is more likely to win. Meanwhile, the difference between the winning rates of the two teams are larger than the difference in the previous comparision using the first blood. So the value of the first tower may larger than the value of the first blood.","9c15d844":"Lets try the K-Nearest Neighbors classification","1d4d1e2a":"So the SVM model gives us a accuracy of 96.88% which is a good one but still not as good as the random forest. The best parameter for the SVM is kernel=\"rbf\".","cf2facf5":"Let's see the distribution of winning when first blood happend.","ed7d0574":"The result shows that we don't have missing values in every column of this dataset.","c0b6123d":"From these two plots we can see that their distribution is almost the same and the means of team1 is slightly larger than the mean of team2 but this difference is not significance. We consider a difference of 1 is significance because there is no such things like 0.5 tower in the game.","5442b4ac":"Try Naive Bayes First:","82c004be":"First, lets compare the distribution of towerkills between team1 and team2.","9e92565e":"The following is a scenario we created as an example to predict the results of the game.","c36e08d5":"The KNN gives us a accuracy of 96.62% which is not a good one for us.<br>\nLet's try SVM next","9de326fc":"As seen in the confusion matrix, our models knew 194 matches which win First team as Second Team. Also it knew 216 matches which win Second Team as First Team. Now let's implement.","846bbe37":"This model gives us a  94.12% accuracy which is not good for us.<br>\nLet's try decision tree with grid search next.","039877e5":"let's plug it into our model to see the result.","38bc51ff":"Assess our model using confusion matrix:","0733ee1e":"In all five models, the random forest model have the highest the accuracy. So we choose the random forest as our final model.<br>\nLet's make a visual example and examine our model using confusion matrix.","adda7dcc":"From these two plots we can see that the team got the first blood in more likely to win in a game.","1ae6ccb3":"visualization of the first tree in the random forest.","5815726f":"Now let's check the missingness in each row of our dataset."}}