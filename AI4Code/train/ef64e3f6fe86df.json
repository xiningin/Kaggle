{"cell_type":{"452227a8":"code","4e67020a":"code","57273542":"code","a7482fdd":"code","482bf6b0":"code","1da533fa":"code","3e0e108f":"code","44f10bf4":"code","5f8a44e0":"code","d1856ce9":"code","752aa202":"code","070c1619":"code","82626e3c":"code","c2b120ce":"code","bb9ffff9":"code","1d628cfe":"code","0985914c":"code","46f5c79a":"code","df479e03":"code","97dd55ee":"code","cfc155e6":"code","8955a179":"code","4ef4b174":"code","c2e60328":"code","a5c8cf69":"code","113a1a4e":"code","483e1730":"code","a816b8d8":"code","1cf7612e":"code","2fa5fa49":"code","08e94fb5":"code","1f4509aa":"code","0ecbf56e":"code","c4ccf0c8":"code","dfde328d":"code","5f46c66b":"code","6f481115":"code","8ac9a41d":"code","6389c8c2":"code","3831ad54":"code","20a6c306":"code","ffd51685":"code","6b3a5798":"code","822ec586":"code","c55618d1":"code","5363d114":"code","79f992f0":"code","d0f54d0f":"code","068e0ee7":"code","56efcfd5":"code","bf40efce":"code","fdf9f13a":"code","9f210756":"code","8c8cc048":"code","731e0425":"code","21e19bc9":"code","9400099f":"code","2f03a33c":"code","f5b661d3":"code","d725c512":"code","d818b373":"code","22886b4b":"code","1ea53625":"code","66978967":"code","0a9ee413":"code","bd3f54d6":"code","407fafaf":"code","26a2100b":"code","38079047":"code","ba214439":"code","1139e0b4":"code","5a4f5ded":"code","b6bf247f":"code","c187ff47":"code","cc3ccc1b":"code","81b67735":"code","078f42c1":"code","cab3ccee":"code","cb5b1f06":"code","d7bc04c5":"code","92ec222f":"code","46d34b12":"code","8fc43f23":"code","8586cec6":"code","e882782b":"code","9c593da5":"code","9d6dbe73":"code","e9b6a1b5":"code","2f832994":"code","19acfcf9":"code","530f72a6":"code","2e39309f":"code","4d75947c":"code","dc5e7813":"code","ce4c5a18":"code","dd4a8ee3":"code","3f8e6497":"code","6ea62cb5":"code","f11310ed":"code","3287c27b":"code","1c91573e":"code","baed210a":"code","1cd219b3":"code","bfd1bd09":"code","04315faa":"code","cb92cce6":"code","5ae84077":"code","96cfe1fb":"code","351802d6":"code","00fc5e6c":"code","61b7fcd2":"code","6ecd7508":"code","12848a34":"code","e21687a2":"code","6f38520a":"code","c863bd39":"code","610b876a":"code","6bf3c93b":"code","08e71976":"code","b61bb5cc":"code","a7e9a9dd":"code","882f1152":"code","1b763889":"code","3edc753d":"code","d33ce987":"code","0908b6bf":"code","d36dd26a":"code","07f6c4ad":"markdown","b552491e":"markdown","a46e9a64":"markdown","c426ea01":"markdown","2b1a4b54":"markdown","64c6e5df":"markdown","0c6857a3":"markdown","c60bd14c":"markdown","1b70dfa7":"markdown","31d5e2e4":"markdown","53d69bf9":"markdown","69fe408c":"markdown","53a274a9":"markdown","b72ec87f":"markdown","1ce8df04":"markdown","f748bd2d":"markdown","d6aa2e36":"markdown","7cc4fc58":"markdown","ebcc0d2a":"markdown","8fc7afcf":"markdown","94d641d1":"markdown","58285cc2":"markdown","e0ff5263":"markdown","829dd50c":"markdown","922ac0d4":"markdown","27761a04":"markdown","4db975e4":"markdown","164fb7e7":"markdown","c31f599d":"markdown","8aa08328":"markdown","afdcdcd2":"markdown","5312d52a":"markdown","f00c5404":"markdown","fad3167c":"markdown","3dc8b12b":"markdown","027ff27c":"markdown","83dffd4b":"markdown","1b8b1e8f":"markdown","976f17a9":"markdown","4889e0bf":"markdown","94de9e67":"markdown","a1cd2492":"markdown","bf8c21de":"markdown","b7520df3":"markdown","229cd5a5":"markdown","d3d65eba":"markdown","dc4a9fd8":"markdown","52d3eb71":"markdown","b5831d67":"markdown","6b9cdfd0":"markdown","7743641a":"markdown","62c98dbc":"markdown","5f3820f1":"markdown","7f8415f3":"markdown","7287419d":"markdown","10fce361":"markdown","9a88d330":"markdown","663eb3d5":"markdown","a77bbca7":"markdown","d6e47d85":"markdown","38fd60b0":"markdown","f9f33f7c":"markdown","42660cc7":"markdown","ceb90d8d":"markdown","e8708315":"markdown","851b7ad9":"markdown","a15562f4":"markdown","f0210081":"markdown","5f943d11":"markdown","ec62e5b9":"markdown","8ece108e":"markdown","9b8ef73e":"markdown","7cfaf4f7":"markdown","9d13f5f8":"markdown","b3e2987a":"markdown","24482777":"markdown","c3b5e9b9":"markdown","393ffbc8":"markdown","55ef4311":"markdown","fc0eef50":"markdown","6b7ab648":"markdown","834e6d80":"markdown","16f423b3":"markdown","7e992f7e":"markdown","e951b15c":"markdown"},"source":{"452227a8":"import pandas as pd\nimport math\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\n\nimport datetime              \nfrom tabulate              import tabulate\nfrom scipy.stats           import chi2_contingency\nfrom IPython.display       import Image\nfrom IPython.core.display  import HTML\n\nfrom sklearn.preprocessing import RobustScaler, MinMaxScaler, LabelEncoder\nfrom boruta                import BorutaPy\nfrom sklearn.ensemble      import RandomForestRegressor\nfrom sklearn.metrics       import mean_absolute_error, mean_squared_error\nfrom sklearn.linear_model  import LinearRegression, Lasso\nfrom sklearn.ensemble      import RandomForestRegressor\nimport xgboost as xgb\n\nimport random\nimport warnings\nwarnings.filterwarnings( 'ignore' )\n\nimport pickle\nfrom flask                 import Flask, request, Response\nimport simplejson as json\nimport requests","4e67020a":"def cross_validation( X_training, kfold, model_name, model, verbose=False ):\n    \n    mae_list = []\n    mape_list = []\n    rmse_list = []\n\n    for k in reversed( range( 1, kfold+1 ) ):\n        if verbose:\n            print( '\\nKFold Number: {}'.format( k ) )\n        # start and end date for validation\n        validation_start_date = X_training['date'].max() - datetime.timedelta( days=k*6*7 )\n        validation_end_date = X_training['date'].max() - datetime.timedelta( days=(k-1)*6*7 )\n\n        # filtering dataset\n        training = X_training[X_training['date'] < validation_start_date]\n        validation = X_training[(X_training['date'] >= validation_start_date) & (X_training['date'] <= validation_end_date)]\n\n        # training and validation dataset\n        # training\n        xtraining = training.drop( ['date', 'sales' ], axis=1 )\n        ytraining = training['sales']\n\n        #validation\n        xvalidation = validation.drop( ['date', 'sales'], axis=1 )\n        yvalidation = validation['sales']\n\n        # model\n        m = model.fit( xtraining, ytraining )\n\n        # prediction\n        yhat = m.predict( xvalidation )\n\n        # performance\n        m_result = ml_error( model_name, np.expm1( yvalidation ), np.expm1( yhat ) )\n\n        # store performance of each kfold interation\n        mae_list.append( m_result['MAE'] )\n        mape_list.append( m_result['MAPE'] )\n        rmse_list.append( m_result['RMSE'] )\n\n    return pd.DataFrame( { 'Model Name': model_name,\n                            'MAE CV': np.round( np.mean( mae_list ), 2 ).astype( str ) + ' +\/- ' + np.round( np.std( mae_list ), 2 ).astype( str ),\n                            'MAPE CV': np.round( np.mean( mape_list ), 2 ).astype( str ) + ' +\/- ' + np.round( np.std( mape_list ), 2 ).astype( str ),\n                            'RMSE CV': np.round( np.mean( rmse_list ), 2 ).astype( str ) + ' +\/- ' + np.round( np.std( rmse_list ), 2 ).astype( str ) }, index=[0] )\n\n\ndef mean_absolute_percentage_error( y, yhat ):\n    return np.mean( np.abs( (y - yhat ) \/ y ) )\n\n\ndef ml_error( model_name, y, yhat ):\n    mae = mean_absolute_error( y, yhat )\n    mape = mean_absolute_percentage_error( y, yhat )\n    rmse = np.sqrt( mean_squared_error( y, yhat ) )\n    \n    return pd.DataFrame( { 'Model Name': model_name,\n                           'MAE': mae,\n                           'MAPE': mape,\n                           'RMSE': rmse }, index=[0] )\n\n\ndef mean_percentage_error( y, yhat ):\n    return np.mean( ( y - yhat ) \/ y )\n\ndef cramer_v( x, y ):\n    cm = pd.crosstab( x, y ).values\n    n = cm.sum()\n    r, k = cm.shape\n    \n    chi2 = chi2_contingency( cm )[0]\n    chi2corr = max( 0, chi2 - (k-1)*(r-1)\/(n-1) )\n    \n    kcorr = k - (k-1)**2\/(n-1)\n    rcorr = r - (r-1)**2\/(n-1)\n\n    return np.sqrt( (chi2corr\/n) \/ ( min( kcorr-1, rcorr-1 ) ) )\n\ndef jupyter_settings():\n    %matplotlib inline\n    %pylab inline\n    \n    plt.style.use( 'bmh' )\n    plt.rcParams['figure.figsize'] = [25, 12]\n    plt.rcParams['font.size'] = 24\n    \n    display( HTML( '<style>.container { width:100% !important; }<\/style>') )\n    pd.options.display.max_columns = None\n    pd.options.display.max_rows = None\n    pd.set_option( 'display.expand_frame_repr', False )\n    \n    sns.set()","57273542":"jupyter_settings()","a7482fdd":"df_sales_raw = pd.read_csv('..\/input\/rossmann-store-sales\/train.csv', low_memory=False)\ndf_store_raw = pd.read_csv('..\/input\/rossmann-store-sales\/store.csv', low_memory=False)\n\ndf_raw = pd.merge( df_sales_raw, df_store_raw, how='left', on='Store')","482bf6b0":"# check if the merge is correct.\ndf_raw.sample()","1da533fa":"# At the beginning of each session make a copy of the dataset to help in case something goes wrong and we need to restart the process.\ndf1 = df_raw.copy()","3e0e108f":"# Rename the columns help us to unsderstand better what kind of info there is in each column.\ndf1.columns","44f10bf4":"# save the old name of the columns for security.\ncols_old = ['Store', 'DayOfWeek', 'Date', 'Sales', 'Customers', 'Open', 'Promo', 'StateHoliday', 'SchoolHoliday', 'StoreType', \n            'Assortment', 'CompetitionDistance', 'CompetitionOpenSinceMonth', 'CompetitionOpenSinceYear', 'Promo2', \n            'Promo2SinceWeek', 'Promo2SinceYear', 'PromoInterval']\n\n\n# rename.\ndf1.columns = ['store', 'day_of_week', 'date', 'sales', 'customers', 'open', 'promo', 'state_holiday', 'school_holiday', 'store_type', \n            'assortment', 'competition_distance', 'competition_open_since_month', 'competition_open_since_year', 'promo2', \n            'promo2_since_week', 'promo2_since_year', 'promo_interval']","5f8a44e0":"# check!\ndf1.columns","d1856ce9":"print( 'Number of Rows: {}'.format( df1.shape[0] ) )\nprint( 'Number of Cols: {}'.format( df1.shape[1] ) )\n# Evaluate the possibilite do use this project in your computer","752aa202":"df1.dtypes\n# Observe date. It has a different data type.","070c1619":"# use \"datetime\" to transform the value into date.\ndf1['date'] = pd.to_datetime( df1['date'] )","82626e3c":"# check if the tranformation was done.\ndf1.dtypes","c2b120ce":"# check if there is some missing value in the dataset.\ndf1.isna().sum()","bb9ffff9":"# Ways to deal with NA\n\n# competition_distance\ndf1['competition_distance'] = df1['competition_distance'].apply(lambda x: 200000.0 if math.isnan( x ) else x )\n\n# competition_open_since_month\ndf1['competition_open_since_month'] = df1.apply(lambda x: x['date'].month if math.isnan(x['competition_open_since_month']) else x['competition_open_since_month'], axis=1)\n\n# competition_open_since_year\ndf1['competition_open_since_year'] = df1.apply(lambda x: x['date'].year if math.isnan(x['competition_open_since_year']) else x['competition_open_since_year'], axis=1)\n\n# promo2_since_week\ndf1['promo2_since_week'] = df1.apply(lambda x: x['date'].week if math.isnan(x['promo2_since_week']) else x['promo2_since_week'], axis=1)\n\n# promo2_since_year\ndf1['promo2_since_year'] = df1.apply(lambda x: x['date'].year if math.isnan(x['promo2_since_year']) else x['promo2_since_year'], axis=1)\n\n# promo_interval\nmonth_map = {1: 'Jan', 2: 'Feb', 3: 'Mar', 4: 'Apr', 5: 'May', 6: 'Jun', 7: 'Jul', 8: 'Aug', 9: 'Sep', 10: 'Oct', 11: 'Nov', 12: 'Dec'}\n\ndf1['promo_interval'].fillna(0, inplace=True)\n\ndf1['month_map'] = df1['date'].dt.month.map( month_map )\n\ndf1['is_promo'] = df1[['promo_interval', 'month_map']].apply( lambda x: 0 if x['promo_interval'] == 0 else 1 if x['month_map'] in x['promo_interval'].split( ',' ) else 0, axis=1 )\n","1d628cfe":"df1.isna().sum()","0985914c":"# Use \"dtypes\" to show what type of data has each column.\ndf1.dtypes","46f5c79a":"# competition\ndf1['competition_open_since_month'] = df1['competition_open_since_month'].astype( 'int64' )\ndf1['competition_open_since_year'] = df1['competition_open_since_year'].astype( 'int64' )\n\n# promo2\ndf1['promo2_since_week'] = df1['promo2_since_week'].astype( 'int64' )\ndf1['promo2_since_year'] = df1['promo2_since_year'].astype( 'int64' )","df479e03":"df1.dtypes","97dd55ee":"num_attributes = df1.select_dtypes( include=['int64', 'float64'] )\ncat_attributes = df1.select_dtypes( exclude=['int64', 'float64', 'datetime64[ns]'] )","cfc155e6":"num_attributes.sample()","8955a179":"cat_attributes.sample()","4ef4b174":"# Central Tendency - mean, median\nct1 = pd.DataFrame( num_attributes.apply( np.mean ) ).T\nct2 = pd.DataFrame( num_attributes.apply( np.median ) ).T\n\n# Dispersion - std, min, max, range, skew, kurtosis\nd1 = pd.DataFrame( num_attributes.apply( np.std ) ).T\nd2 = pd.DataFrame( num_attributes.apply( min ) ).T\nd3 = pd.DataFrame( num_attributes.apply( max ) ).T\nd4 = pd.DataFrame( num_attributes.apply( lambda x: x.max() - x.min() ) ).T\nd5 = pd.DataFrame( num_attributes.apply( lambda x: x.skew() ) ).T\nd6 = pd.DataFrame( num_attributes.apply( lambda x: x.kurtosis() ) ).T\n\n# concatenate\nm = pd.concat( [d2, d3, d4, ct1, ct2, d1, d5, d6] ).T.reset_index()\nm.columns = ( ['attributes', 'min', 'max', 'range', 'mean', 'median', 'std', 'skew', 'kurtosis'])","c2e60328":"m","a5c8cf69":"sns.distplot( df1['sales'] )","113a1a4e":"cat_attributes.apply( lambda x: x.unique().shape[0] )","483e1730":"aux1 = df1[(df1['state_holiday'] != '0' ) & (df1['sales'] > 0)]\n\n\nplt.subplot(1, 3, 1)\nsns.boxplot( x= 'state_holiday', y='sales' , data=aux1 )\n\nplt.subplot(1, 3, 2)\nsns.boxplot( x= 'store_type', y='sales' , data=aux1 )\n\nplt.subplot(1, 3, 3)\nsns.boxplot( x= 'assortment', y='sales' , data=aux1 )","a816b8d8":"df2 = df1.copy()","1cf7612e":"Image( \"..\/input\/hyphoteses-map\/Hyphoteses_Map.png\")","2fa5fa49":"# year\ndf2['year'] = df2['date'].dt.year\n\n# month\ndf2['month'] = df2['date'].dt.month\n\n# day\ndf2['day'] = df2['date'].dt.day\n\n# week of year\ndf2['week_of_year'] = df2['date'].dt.weekofyear\n\n# year week\ndf2['year_week'] = df2['date'].dt.strftime( '%Y-%W' )\n\n\n# competition since\ndf2['competition_since'] = df2.apply( lambda x: datetime.datetime( year=x['competition_open_since_year'], month=x['competition_open_since_month'], day=1 ), axis=1 )\ndf2['competition_time_month'] = ( ( df2['date'] - df2['competition_since'] )\/30 ).apply( lambda x: x.days ).astype( int )\n\n# promo since\ndf2['promo_since'] = df2['promo2_since_year'].astype( str ) + '-' + df2['promo2_since_week'].astype( str )\ndf2['promo_since'] = df2['promo_since'].apply( lambda x: datetime.datetime.strptime( x + '-1', '%Y-%W-%w' ) - datetime.timedelta( days=7 ) )\ndf2['promo_time_week'] = ( ( df2['date'] - df2['promo_since'] )\/7 ).apply( lambda x: x.days ).astype( int )\n\n# assortment\ndf2['assortment'] = df2['assortment'].apply( lambda x: 'basic' if x == 'a' else 'extra' if x == 'b' else 'extended' )\n\n# state holiday\ndf2['state_holiday'] = df2['state_holiday'].apply( lambda x: 'public_holiday' if x == 'a' else 'easter_holiday' if x == 'b' else 'christmas' if x == 'c' else 'regular_day' )","08e94fb5":"df2.head().T","1f4509aa":"df3 = df2.copy()","0ecbf56e":"df3.head()","c4ccf0c8":"df3 = df3[(df3['open']) != 0 & (df3['sales'] > 0)]","dfde328d":"cols_drop = ['customers', 'open', 'promo_interval', 'month_map']\ndf3 = df3.drop( cols_drop, axis = 1)","5f46c66b":"df3.columns","6f481115":"df4 = df3.copy()","8ac9a41d":"sns.distplot( df4['sales'], kde=False )","6389c8c2":"num_attributes.hist(bins=25);","3831ad54":"cat_attributes.head()","20a6c306":"df4['state_holiday'].drop_duplicates()","ffd51685":"df4['store_type'].drop_duplicates()","6b3a5798":"df4['assortment'].drop_duplicates()","822ec586":"# state holiday\n\nplt.subplot( 3, 2, 1)\na = df4[df4['state_holiday'] != 'regular_day']\nsns.countplot( a['state_holiday'] )\n\nplt.subplot( 3, 2, 2)\nsns.kdeplot( df4[df4['state_holiday'] == 'public_holiday']['sales'], label='public_holiday', shade=True )\nsns.kdeplot( df4[df4['state_holiday'] == 'easter_holiday']['sales'], label='easter_holiday', shade=True )\nsns.kdeplot( df4[df4['state_holiday'] == 'christmas_holiday']['sales'], label='christmas_holiday', shade=True )\n\n# store_type\n\nplt.subplot( 3, 2, 3)\nsns.countplot( df4['store_type'] )\n\nplt.subplot( 3, 2, 4)\nsns.kdeplot( df4[df4['store_type'] == 'a']['sales'], label='a', shade=True )\nsns.kdeplot( df4[df4['store_type'] == 'b']['sales'], label='b', shade=True )\nsns.kdeplot( df4[df4['store_type'] == 'c']['sales'], label='c', shade=True )\nsns.kdeplot( df4[df4['store_type'] == 'd']['sales'], label='d', shade=True )\n\n# assortment\n\nplt.subplot( 3, 2, 5)\nsns.countplot( df4['assortment'] )\n\nplt.subplot( 3, 2, 6)\nsns.kdeplot( df4[df4['assortment'] == 'extended']['sales'], label='extended', shade=True )\nsns.kdeplot( df4[df4['assortment'] == 'basic']['sales'], label='basic', shade=True )\nsns.kdeplot( df4[df4['assortment'] == 'extra']['sales'], label='extra', shade=True )","c55618d1":"aux1 = df4[['assortment', 'sales']].groupby( 'assortment' ).sum().reset_index()\nsns.barplot( x='assortment', y='sales', data=aux1);\n\naux2 = df4[['year_week', 'assortment', 'sales']].groupby( ['year_week', 'assortment'] ).sum().reset_index()\naux2.pivot( index='year_week', columns='assortment', values='sales' ).plot()\n\naux3 = aux2[aux2['assortment'] == 'extra']\naux3.pivot( index='year_week', columns='assortment', values='sales' ).plot()","5363d114":"aux1 = df4[['competition_distance', 'sales']].groupby( 'competition_distance' ).sum().reset_index()\nplt.subplot(3,1,1)\nsns.barplot( x='competition_distance', y='sales', data=aux1)","79f992f0":"plt.subplot(1,3,1)\nbins = list( np.arange(0, 20000, 1000) )\naux1['competition_distance_binned'] = pd.cut( aux1['competition_distance'], bins=bins )\naux2 = aux1[['competition_distance_binned', 'sales']].groupby( 'competition_distance_binned' ).sum().reset_index()\nsns.barplot( x='competition_distance_binned', y='sales', data=aux2)\nplt.xticks( rotation=90)\n\nplt.subplot(1,3,2)\nsns.scatterplot( x='competition_distance', y='sales', data=aux1 )\n\nplt.subplot(1,3,3)\nsns.heatmap( aux1.corr( method='pearson' ), annot=True );\n","d0f54d0f":"aux1 = df4[['competition_open_since_month', 'sales']].groupby( 'competition_open_since_month' ).sum().reset_index()\nsns.barplot( x='competition_open_since_month', y='sales', data=aux1);","068e0ee7":"aux1 = df4[['competition_time_month', 'sales']].groupby( 'competition_time_month' ).sum().reset_index()\nsns.barplot( x='competition_time_month', y='sales', data=aux1);","56efcfd5":"plt.subplot( 1, 3, 1)\naux1 = df4[['competition_time_month', 'sales']].groupby( 'competition_time_month' ).sum().reset_index()\naux2 = aux1[( aux1['competition_time_month'] < 120 ) & ( aux1['competition_time_month'] != 0)]\nsns.barplot( x='competition_time_month', y='sales', data=aux2)\nplt.xticks( rotation=90 );\n\nplt.subplot( 1, 3, 2)\nsns.regplot( x='competition_time_month', y='sales', data=aux2)\n\nplt.subplot( 1, 3, 3)\nsns.heatmap( aux1.corr( method='pearson'), annot=True );","bf40efce":"aux1 = df4[['promo_time_week', 'sales']].groupby('promo_time_week').sum().reset_index()\nsns.barplot( x='promo_time_week', y='sales', data=aux1 );","fdf9f13a":"aux1 = df4[['promo_time_week', 'sales']].groupby('promo_time_week').sum().reset_index()\n\ngrid = gridspec.GridSpec( 2, 3 )\n\nplt.subplot(grid[0,0])\naux2 = aux1[aux1['promo_time_week'] > 0] # promo extendido\nsns.barplot( x='promo_time_week', y='sales', data=aux2 );\nplt.xticks( rotation=90 );\n\nplt.subplot(grid[0,1])\nsns.regplot( x='promo_time_week', y='sales', data=aux2 );\n\nplt.subplot(grid[1,0])\naux3 = aux1[aux1['promo_time_week'] < 0] # promo regular\nsns.barplot( x='promo_time_week', y='sales', data=aux3 );\nplt.xticks( rotation=90 );\n\nplt.subplot(grid[1,1])\nsns.regplot( x='promo_time_week', y='sales', data=aux3 );\n\nplt.subplot(grid[:,2])\nsns.heatmap( aux1.corr( method='pearson' ), annot=True );","9f210756":"# Pq riscou?","8c8cc048":"df4[['promo', 'promo2', 'sales']].groupby( ['promo', 'promo2'] ).sum().reset_index()","731e0425":"aux1 = df4[( df4['promo'] == 1 ) & ( df4['promo2'] == 1 )][['year_week', 'sales']].groupby( 'year_week' ).sum().reset_index()\nax = aux1.plot()\n\naux2 = df4[( df4['promo'] == 1 ) & ( df4['promo2'] == 0 )][['year_week', 'sales']].groupby( 'year_week' ).sum().reset_index()\naux2.plot(ax=ax)\n\nax.legend( labels=['Tradicional & Extendida', 'Extendida'] );","21e19bc9":"aux = df4[df4['state_holiday'] != 'regular_day']\n\nplt.subplot( 1, 2, 1)\naux1 = aux[['state_holiday', 'sales']].groupby( 'state_holiday' ).sum().reset_index()\nsns.barplot( x='state_holiday', y='sales', data=aux1);\n\nplt.subplot( 1, 2, 2)\naux2 = aux[['year', 'state_holiday', 'sales']].groupby( ['year', 'state_holiday'] ).sum().reset_index()\nsns.barplot( x='year', y='sales', hue='state_holiday', data=aux2 );","9400099f":"aux1 = df4[['year', 'sales']].groupby( 'year' ).sum().reset_index()\n\nplt.subplot( 1, 3, 1 )\nsns.barplot( x='year', y='sales', data=aux1 );\n\nplt.subplot( 1, 3, 2 )\nsns.regplot( x='year', y='sales', data=aux1 );\n\nplt.subplot( 1, 3, 3 )\nsns.heatmap( aux1.corr( method='pearson' ), annot=True );","2f03a33c":"aux1 = df4[['month', 'sales']].groupby( 'month' ).sum().reset_index()\n\nplt.subplot( 1, 3, 1 )\nsns.barplot( x='month', y='sales', data=aux1 );\n\nplt.subplot( 1, 3, 2 )\nsns.regplot( x='month', y='sales', data=aux1 );\n\nplt.subplot( 1, 3, 3 )\nsns.heatmap( aux1.corr( method='pearson' ), annot=True );","f5b661d3":"aux1 = df4[['day', 'sales']].groupby( 'day' ).sum().reset_index()\n\nplt.subplot( 2, 2, 1 )\nsns.barplot( x='day', y='sales', data=aux1 );\n\nplt.subplot( 2, 2, 2 )\nsns.regplot( x='day', y='sales', data=aux1 );\n\nplt.subplot( 2, 2, 3 )\nsns.heatmap( aux1.corr( method='pearson' ), annot=True );\n\naux1['before_after'] = aux1['day'].apply( lambda x: 'before_after' if x <= 10 else 'after_10_days' )\naux2 = aux1[['before_after', 'sales']].groupby( 'before_after' ).sum().reset_index()\n\nplt.subplot( 2, 2, 4 )\nsns.barplot( x='before_after', y='sales', data=aux2 );","d725c512":"aux1 = df4[['day_of_week', 'sales']].groupby( 'day_of_week' ).sum().reset_index()\n\nplt.subplot( 1, 3, 1 )\nsns.barplot( x='day_of_week', y='sales', data=aux1 );\n\nplt.subplot( 1, 3, 2 )\nsns.regplot( x='day_of_week', y='sales', data=aux1 );\n\nplt.subplot( 1, 3, 3 )\nsns.heatmap( aux1.corr( method='pearson' ), annot=True );","d818b373":"aux1 = df4[['school_holiday', 'sales']].groupby( 'school_holiday' ).sum().reset_index()\nplt.subplot( 2, 1, 1 )\nsns.barplot( x='school_holiday', y='sales', data=aux1 );\n\naux2 = df4[['month', 'school_holiday', 'sales']].groupby( ['month', 'school_holiday'] ).sum().reset_index()\nplt.subplot( 2, 1, 2 )\nsns.barplot( x='month', y='sales', hue='school_holiday', data=aux2 );","22886b4b":"tab = [['Hipoteses', 'Conclusao', 'Relevancia'],\n       ['H1', 'Falsa', 'Baixa'],\n       ['H2', 'Falsa', 'Media'],\n       ['H3', 'Falsa', 'Media'],\n       ['H4', 'Falsa', 'Baixa'],\n       ['H5', '-', '-'],\n       ['H6', 'Falsa', 'Baixa'],\n       ['H7', 'Falsa', 'Media'],\n       ['H8', 'Falsa', 'Alta'],\n       ['H9', 'Falsa', 'Alta'],\n       ['H10', 'Verdadeira', 'Alta'],\n       ['H11', 'Verdadeira', 'Alta'],\n       ['H12', 'Verdadeira', 'Baixa']\n      ]\nprint( tabulate( tab, headers='firstrow' ) )","1ea53625":"correlation = num_attributes.corr( method='pearson' )\nsns.heatmap( correlation, annot=True )","66978967":"# Only categorical data\na1 = cramer_v( a['state_holiday'], a['state_holiday'] )\n\n# Calculate cramer V\na2 = cramer_v( a['state_holiday'], a['store_type'] )\na3 = cramer_v( a['state_holiday'], a['assortment'] )\n\na4 = cramer_v( a['store_type'], a['state_holiday'] )\na5 = cramer_v( a['store_type'], a['store_type'] )\na6 = cramer_v( a['store_type'], a['assortment'] )\n\na7 = cramer_v( a['assortment'], a['state_holiday'] )\na8 = cramer_v( a['assortment'], a['store_type'] )\na9 = cramer_v( a['assortment'], a['assortment'] )\n\n# Final dataset\nd = pd.DataFrame( {'state_holiday': [a1, a2, a3],\n               'store_type': [a4, a5, a6],\n               'assortment': [a7, a8, a9]} )\n\nd = d.set_index( d.columns )\n\nsns.heatmap( d, annot=True )","0a9ee413":"df5 = df4.copy()","bd3f54d6":"# Go to 4.1.2. Numerical variable to see if there is any Gaussian distribution\n\n# We don't have any data distribution like a Gaussian.","407fafaf":"a = df5.select_dtypes( include=['int64', 'float64'] )","26a2100b":"sns.boxplot( df5['competition_distance'] )","38079047":"sns.boxplot( df5['competition_time_month'] )","ba214439":"sns.boxplot( df5['promo_time_week'] )","1139e0b4":"rs = RobustScaler()\n\n# competition distance\ndf5['competition_distance'] = rs.fit_transform( df5[['competition_distance']].values )\n\n# competition time month\ndf5['competition_time_month'] = rs.fit_transform( df5[['competition_time_month']].values )\n\nmms = MinMaxScaler()\n\n# promo time week\ndf5['promo_time_week'] = mms.fit_transform( df5[['promo_time_week']].values )\n\n# year\ndf5['year'] = mms.fit_transform( df5[['year']].values )","5a4f5ded":"sns.distplot( df5['competition_distance'] )","b6bf247f":"# state_holiday - One Hot Encoding\ndf5 = pd.get_dummies( df5, prefix=['state_holiday'], columns=['state_holiday'] )\n\nle = LabelEncoder()\n\n# store_type - Label Encoding\ndf5['store_type'] = le.fit_transform( df5['store_type'] )\n\n# assortment - Ordinal Encoding\nassortment_dict = {'basic': 1, 'extra': 2, 'extended': 3}\ndf5['assortment'] = df5['assortment'].map( assortment_dict )","c187ff47":"df5['sales'] = np.log1p( df5['sales'] )","cc3ccc1b":"sns.distplot( df5['sales'] )","81b67735":"# day of week\ndf5['day_of_week_sin'] = df5['day_of_week'].apply( lambda x: np.sin( x * ( 2. * np.pi\/7 ) ) )\ndf5['day_of_week_cos'] = df5['day_of_week'].apply( lambda x: np.cos( x * ( 2. * np.pi\/7 ) ) )\n\n# month\ndf5['month_sin'] = df5['month'].apply( lambda x: np.sin( x * ( 2. * np.pi\/12 ) ) )\ndf5['month_cos'] = df5['month'].apply( lambda x: np.cos( x * ( 2. * np.pi\/12 ) ) )\n\n# day\ndf5['day_sin'] = df5['day'].apply( lambda x: np.sin( x * ( 2. * np.pi\/30 ) ) )\ndf5['day_cos'] = df5['day'].apply( lambda x: np.cos( x * ( 2. * np.pi\/30 ) ) )\n\n# week of year\ndf5['week_of_year_sin'] = df5['week_of_year'].apply( lambda x: np.sin( x * ( 2. * np.pi\/52 ) ) )\ndf5['week_of_year_cos'] = df5['week_of_year'].apply( lambda x: np.cos( x * ( 2. * np.pi\/52 ) ) )","078f42c1":"df5.head()","cab3ccee":"df6 = df5.copy()","cb5b1f06":"df6.shape","d7bc04c5":"cols_drop = ['week_of_year', 'day', 'month', 'day_of_week', 'promo_since', 'competition_since', 'year_week']\ndf6 = df6.drop( cols_drop, axis=1 )","92ec222f":"df6[['store', 'date']].groupby( 'store' ).max().reset_index()['date'][0] - datetime.timedelta( days=6*7 )","46d34b12":"# training dataset\nX_train = df6[df6['date'] < '2015-06-19']\ny_train = X_train['sales']\n\n# test dataset\nX_test = df6[df6['date'] >= '2015-06-19']\ny_test = X_test['sales']\n\nprint( 'Training Min Date: {}'.format( X_train['date']. min() ) )\nprint( 'Training Max Date: {}'.format( X_train['date']. max() ) )\n\nprint( '\\nTest Min Date: {}'.format( X_test['date']. min() ) )\nprint( 'Test Max Date: {}'.format( X_test['date']. max() ) )","8fc43f23":"# training and test dataset for Boruta\nX_train_n = X_train.drop( ['date', 'sales'], axis=1 ).values\ny_train_n = y_train.values.ravel()\n\n# define RandomForestRegression\nrf = RandomForestRegressor( n_jobs=-1 )\n\n# define Boruta\n###boruta = BorutaPy( rf, n_estimators='auto', verbose=2, random_state=42 ).fit( X_train_n, y_train_n)","8586cec6":"####cols_selected = boruta.support_.tolist()\n\n# best feature\n#######X_train_fs = X_train.drop( ['date', 'sales'], axis=1 )\n###########cols_selected_boruta = X_train_fs.iloc[:, cols_selected].columns.tolist()\n\n# not selected boruta\n#########cols_not_selected_boruta = list( np.setdiff1d( X_train_fs.columns, cols_selected_boruta ) )","e882782b":"############cols_selected_boruta","9c593da5":"############cols_not_selected_boruta","9d6dbe73":"tab = [['Hipoteses', 'Conclusao', 'Relevancia'],\n       ['H1', 'Falsa', 'Baixa'],\n       ['H2', 'Falsa', 'Media'],\n       ['H3', 'Falsa', 'Media'],\n       ['H4', 'Falsa', 'Baixa'],\n       ['H5', '-', '-'],\n       ['H6', 'Falsa', 'Baixa'],\n       ['H7', 'Falsa', 'Media'],\n       ['H8', 'Falsa', 'Alta'],\n       ['H9', 'Falsa', 'Alta'],\n       ['H10', 'Verdadeira', 'Alta'],\n       ['H11', 'Verdadeira', 'Alta'],\n       ['H12', 'Verdadeira', 'Baixa']\n      ]\nprint( tabulate( tab, headers='firstrow' ) )","e9b6a1b5":"cols_selected_boruta = [\n     'store',\n     'promo',\n     'store_type',\n     'assortment',\n     'competition_distance',\n     'competition_open_since_month',\n     'competition_open_since_year',\n     'promo2',\n     'promo2_since_week',\n     'promo2_since_year',\n     'competition_time_month',\n     'promo_time_week',\n     'day_of_week_sin',\n     'day_of_week_cos',\n     'month_sin',\n     'month_cos',\n     'day_sin',\n     'day_cos',\n     'week_of_year_sin',\n     'week_of_year_cos']\n\n# columns to add\nfeat_to_add = ['date', 'sales']\n\n# final features\ncols_selected_boruta_full = cols_selected_boruta.copy()\ncols_selected_boruta_full.extend( feat_to_add )","2f832994":"cols_selected_boruta","19acfcf9":"cols_selected_boruta_full","530f72a6":"x_train = X_train[ cols_selected_boruta ]\nx_test = X_test[ cols_selected_boruta ]\n\n# Time Series Data Preparartion\nX_training = X_train[ cols_selected_boruta_full ]","2e39309f":"aux1 = x_test.copy()\naux1['sales'] = y_test.copy()\n\n# prediction\naux2 = aux1[['store', 'sales']].groupby( 'store' ).mean().reset_index().rename( columns={'sales': 'predictions'} )\naux1 = pd.merge( aux1, aux2, how='left', on='store' )\nyhat_baseline = aux1['predictions']\n\n# performance\nbaseline_result = ml_error( 'Average Model', np.expm1( y_test ), np.expm1( yhat_baseline ) )\nbaseline_result","4d75947c":"# model\nlr = LinearRegression().fit( x_train, y_train )\n\n# prediction\nyhat_lr = lr.predict( x_test )\n\n# performance\nlr_result = ml_error( 'Linear Regression', np.expm1( y_test ), np.expm1( yhat_lr ) )\nlr_result","dc5e7813":"lr_result_cv = cross_validation( X_training, 5, 'Linear Regression', lr, verbose=False )\nlr_result_cv","ce4c5a18":"# model\nlrr = Lasso( alpha=0.00001 ).fit( x_train, y_train )\n\n# prediction\nyhat_lrr = lrr.predict( x_test )\n\n# performance\nlrr_result = ml_error( 'Linear Regression - Lasso', np.expm1( y_test ), np.expm1( yhat_lrr ) )\nlrr_result","dd4a8ee3":"lrr_result_cv = cross_validation( X_training, 5, 'Linear Regression - Lasso', lrr, verbose=False )\nlrr_result_cv","3f8e6497":"# model\nrf = RandomForestRegressor( n_estimators=100, n_jobs=-1, random_state=42 ).fit( x_train, y_train )\n\n# prediction\nyhat_rf = rf.predict( x_test )\n\n# performance\nrf_result = ml_error( 'Random Forest Regressor', np.expm1( y_test ), np.expm1( yhat_rf ) )\nrf_result","6ea62cb5":"rf_result_cv = cross_validation( X_training, 5, 'Random Forest Regressor', rf, verbose=True )\nrf_result_cv","f11310ed":"# model\nmodel_xgb = xgb.XGBRegressor( objective='reg:squarederror',\n                             n_estimators=100, \n                             eta=0.01,\n                             max_depth=10,\n                             subsample=0.7,\n                             colsample_bytree=0.9 ).fit( x_train, y_train )\n\n# prediction\nyhat_xgb = model_xgb.predict( x_test )\n\n# performance\nxgb_result = ml_error( 'XGBoost Regressor', np.expm1( y_test ), np.expm1( yhat_xgb ) )\nxgb_result","3287c27b":"xgb_result_cv = cross_validation( X_training, 5, 'XGBoost Regressor', model_xgb, verbose=True )\nxgb_result_cv","1c91573e":"modelling_result = pd.concat( [baseline_result, lr_result, lrr_result, rf_result, xgb_result] )\nmodelling_result.sort_values( 'RMSE' )","baed210a":"modelling_result_cv = pd.concat( [lr_result_cv, lrr_result_cv, rf_result_cv, xgb_result_cv] )\nmodelling_result_cv","1cd219b3":"param = { \n        'n_estimators': [1500, 1700, 2500, 3000, 3500],\n        'eta': [0.01, 0.03],\n        'max_depth': [3, 5, 9],\n        'subsample': [0.1, 0.5, 0.7],\n        'colsample_bytree': [0.3, 0.7, 0.9],\n        'min_child_weight': [3, 8, 15] \n        }\n\nMAX_EVAL = 5","bfd1bd09":"final_result = pd.DataFrame()\n\nimport random\n\nfor i in range( MAX_EVAL):\n    #chose values for parameters randomly\n    hp = {k: random.sample(v, 1)[0] for k, v in param.items()}\n    print( hp )\n    \n    # model\n    model_xgb = xgb.XGBRegressor( objective='reg:squarederror',\n                                 n_estimators=hp['n_estimators'], \n                                 eta=hp['eta'],\n                                 max_depth=hp['max_depth'],\n                                 subsample=hp['subsample'],\n                                 colsample_bytree=hp['colsample_bytree'],\n                                 min_child_weight=hp['min_child_weight'] )\n\n    # performance\n    result = cross_validation( X_training, 5, 'XGBoost Regressor', model_xgb, verbose=False)\n    final_result = pd.concat( [final_result, result] )\n\nfinal_result","04315faa":"param_tuned = { \n        'n_estimators': 1500,\n        'eta': 0.01,\n        'max_depth': 9,\n        'subsample': 0.5,\n        'colsample_bytree': 0.9,\n        'min_child_weight': 8\n        }","cb92cce6":"# model\nmodel_xgb_tuned = xgb.XGBRegressor( objective='reg:squarederror',\n                            n_estimators=param_tuned['n_estimators'], \n                            eta=param_tuned['eta'],\n                            max_depth=param_tuned['max_depth'],\n                            subsample=param_tuned['subsample'],\n                            colsample_bytree=param_tuned['colsample_bytree'],\n                            min_child_weight=param_tuned['min_child_weight'] ).fit( x_train, y_train )\n\n# prediction\nyhat_xgb_tuned = model_xgb_tuned.predict( x_test )\n\n# performance\nxgb_result_tuned = ml_error( 'XGBoost Regressor', np.expm1( y_test ), np.expm1( yhat_xgb_tuned ) )\nxgb_result_tuned","5ae84077":"mpe = mean_percentage_error( np.expm1( y_test ), np.expm1( yhat_xgb_tuned ) )\nmpe","96cfe1fb":"df9 = X_test[ cols_selected_boruta_full ]\n\n# rescale\ndf9['sales'] = np.expm1( df9['sales'] )\ndf9['predictions'] = np.expm1( yhat_xgb_tuned )","351802d6":"# sum of predictions\ndf91 = df9[['store', 'predictions']].groupby( 'store' ).sum().reset_index()\n\n# MAE e MAPE\ndf9_aux1 = df9[['store', 'sales', 'predictions']].groupby( 'store' ).apply( lambda x: mean_absolute_error( x['sales'], x['predictions'] ) ).reset_index().rename( columns={0:'MAE'} )\ndf9_aux2 = df9[['store', 'sales', 'predictions']].groupby( 'store' ).apply( lambda x: mean_absolute_percentage_error( x['sales'], x['predictions'] ) ).reset_index().rename( columns={0:'MAPE'} )\n\n# Merge\ndf9_aux3 = pd.merge( df9_aux1, df9_aux2, how='inner', on='store' )\ndf92 = pd.merge( df91, df9_aux3, how='inner', on='store' )\n\n# Scenarios\ndf92['worst_scenario'] = df92['predictions'] - df92['MAE']\ndf92['best_scenario'] = df92['predictions'] + df92['MAPE']\n\n# order columns\ndf92 = df92[['store', 'predictions', 'worst_scenario', 'best_scenario', 'MAE', 'MAPE']]","00fc5e6c":"df9_aux1.head()","61b7fcd2":"df92.sample(4)","6ecd7508":"df92.sort_values( 'MAPE', ascending=False ).head()","12848a34":"sns.scatterplot( x='store', y='MAPE', data=df92 )","e21687a2":"df93 = df92[['predictions', 'worst_scenario', 'best_scenario']].apply( lambda x: np.sum( x ), axis=0 ).reset_index().rename( columns={'index': 'Scenario', 0:'Values'} )\ndf93['Values'] = df93['Values'].map( 'R${:,.2f}'.format )\ndf93","6f38520a":"df9['error'] = df9['sales'] - df9['predictions']\ndf9['error_rate'] = df9['predictions'] \/ df9['sales']","c863bd39":"plt.subplot( 2, 2, 1 )\nsns.lineplot( x='date', y='sales', data=df9, label='SALES' )\nsns.lineplot( x='date', y='predictions', data=df9, label='PREDICTIONS' )\n\nplt.subplot( 2, 2, 2 )\nsns.lineplot( x='date', y='error_rate', data=df9 )\nplt.axhline( 1, linestyle='--' )\n\nplt.subplot( 2, 2, 3 )\nsns.distplot( df9['error'] )\n\nplt.subplot( 2, 2, 4 )\nsns.scatterplot( df9['predictions'], df9['error'] )","610b876a":"# Save the Model\n# pickle.dump( model_xgb_tuned, open('\/Users\/favh2\/github_projects\/Data_Science_inProduction\/model\/model_rossmann.pkl', 'wb' ) )","6bf3c93b":"!pip install inflection\n\nimport pickle\nimport inflection\nimport pandas as pd\nimport numpy as np\nimport math\nimport datetime\n\nclass Rossmann( object ):\n    def __init__( self ):\n        self.home_path='\/Users\/favh2\/github_projects\/Data_Science_inProduction\/'\n        self.competition_distance_scaler   = pickle.load( open( self.home_path + 'parameter\/competition_distance_scaler.pkl', 'rb' ) )\n        self.competition_time_month_scaler = pickle.load( open( self.home_path + 'parameter\/competition_time_month_scaler.pkl', 'rb' ) )\n        self.promo_time_week_scaler        = pickle.load( open( self.home_path + 'parameter\/promo_time_week_scaler.pkl', 'rb' ) )\n        self.year_scaler                   = pickle.load( open( self.home_path + 'parameter\/year_scaler.pkl', 'rb' ) )\n        self.store_type_scaler             = pickle.load( open( self.home_path + 'parameter\/store_type_scaler.pkl', 'rb' ) )\n        \n    def data_cleaning( self, df1 ):\n        \n        ## 1.1 Rename olumns\n        cols_old = ['Store', 'DayOfWeek', 'Date', 'Open', 'Promo', 'StateHoliday', 'SchoolHoliday', 'StoreType', \n                    'Assortment', 'CompetitionDistance', 'CompetitionOpenSinceMonth', 'CompetitionOpenSinceYear', 'Promo2', \n                    'Promo2SinceWeek', 'Promo2SinceYear', 'PromoInterval']\n\n\n        snakecase = lambda x: inflection.underscore( x )\n\n        cols_new = list( map( snakecase, cols_old ) )\n\n        # rename\n        df1.columns = cols_new\n\n        ## 1.3. Data Types\n        df1['date'] = pd.to_datetime( df1['date'] )\n\n        ## 1.5. Fillout NA\n        # competition_distance\n        df1['competition_distance'] = df1['competition_distance'].apply(lambda x: 200000.0 if math.isnan( x ) else x )\n\n        # competition_open_since_month\n        df1['competition_open_since_month'] = df1.apply(lambda x: x['date'].month if math.isnan(x['competition_open_since_month']) else x['competition_open_since_month'], axis=1)\n\n        # competition_open_since_year\n        df1['competition_open_since_year'] = df1.apply(lambda x: x['date'].year if math.isnan(x['competition_open_since_year']) else x['competition_open_since_year'], axis=1)\n\n        # promo2_since_week\n        df1['promo2_since_week'] = df1.apply(lambda x: x['date'].week if math.isnan(x['promo2_since_week']) else x['promo2_since_week'], axis=1)\n\n        # promo2_since_year\n        df1['promo2_since_year'] = df1.apply(lambda x: x['date'].year if math.isnan(x['promo2_since_year']) else x['promo2_since_year'], axis=1)\n\n        # promo_interval\n        month_map = {1: 'Jan', 2: 'Feb', 3: 'Mar', 4: 'Apr', 5: 'May', 6: 'Jun', 7: 'Jul', 8: 'Aug', 9: 'Sep', 10: 'Oct', 11: 'Nov', 12: 'Dec'}\n\n        df1['promo_interval'].fillna(0, inplace=True)\n\n        df1['month_map'] = df1['date'].dt.month.map( month_map )\n\n        df1['is_promo'] = df1[['promo_interval', 'month_map']].apply( lambda x: 0 if x['promo_interval'] == 0 else 1 if x['month_map'] in x['promo_interval'].split( ',' ) else 0, axis=1 )\n\n        ## 1.6. Change Types\n        # competition\n        df1['competition_open_since_month'] = df1['competition_open_since_month'].astype( 'int64' )\n        df1['competition_open_since_year'] = df1['competition_open_since_year'].astype( 'int64' )\n\n        # promo2\n        df1['promo2_since_week'] = df1['promo2_since_week'].astype( 'int64' )\n        df1['promo2_since_year'] = df1['promo2_since_year'].astype( 'int64' )\n        \n        return df1\n\n    def feature_engineering( self, df2 ):\n        \n        # year\n        df2['year'] = df2['date'].dt.year\n\n        # month\n        df2['month'] = df2['date'].dt.month\n\n        # day\n        df2['day'] = df2['date'].dt.day\n\n        # week of year\n        df2['week_of_year'] = df2['date'].dt.weekofyear\n\n        # year week\n        df2['year_week'] = df2['date'].dt.strftime( '%Y-%W' )\n\n\n        # competition since\n        df2['competition_since'] = df2.apply( lambda x: datetime.datetime( year=x['competition_open_since_year'], month=x['competition_open_since_month'], day=1 ), axis=1 )\n        df2['competition_time_month'] = ( ( df2['date'] - df2['competition_since'] )\/30 ).apply( lambda x: x.days ).astype( int )\n\n        # promo since\n        df2['promo_since'] = df2['promo2_since_year'].astype( str ) + '-' + df2['promo2_since_week'].astype( str )\n        df2['promo_since'] = df2['promo_since'].apply( lambda x: datetime.datetime.strptime( x + '-1', '%Y-%W-%w' ) - datetime.timedelta( days=7 ) )\n        df2['promo_time_week'] = ( ( df2['date'] - df2['promo_since'] )\/7 ).apply( lambda x: x.days ).astype( int )\n\n        # assortment\n        df2['assortment'] = df2['assortment'].apply( lambda x: 'basic' if x == 'a' else 'extra' if x == 'b' else 'extended' )\n\n        # state holiday\n        df2['state_holiday'] = df2['state_holiday'].apply( lambda x: 'public_holiday' if x == 'a' else 'easter_holiday' if x == 'b' else 'christmas' if x == 'c' else 'regular_day' )\n\n        # 3.0. STEP 03 - VARIABLE FILTERING\n        ## 3.1. Line Filtering\n        df2 = df2[(df2['open']) != 0]\n\n        ## 3.2. Column selection\n        cols_drop = ['open', 'promo_interval', 'month_map']\n        df2 = df2.drop( cols_drop, axis = 1)\n        \n        return df2\n    \n    def data_preparation( self, df5 ):\n        \n        ## 5.2. Rescaling\n        # competition distance\n        df5['competition_distance'] = self.competition_distance_scaler.fit_transform( df5[['competition_distance']].values )\n\n        # competition time month\n        df5['competition_time_month'] = self.competition_time_month_scaler.fit_transform( df5[['competition_time_month']].values )\n        \n        # promo time week\n        df5['promo_time_week'] = self.promo_time_week_scaler.fit_transform( df5[['promo_time_week']].values )\n        \n        # year\n        df5['year'] = self.year_scaler.fit_transform( df5[['year']].values )\n\n        ### 5.3.1 Encoding\n        # state_holiday - One Hot Encoding\n        df5 = pd.get_dummies( df5, prefix=['state_holiday'], columns=['state_holiday'] )\n    \n        # store_type - Label Encoding\n        df5['store_type'] = self.store_type_scaler.fit_transform( df5['store_type'] )\n        \n        # assortment - Ordinal Encoding\n        assortment_dict = {'basic': 1, 'extra': 2, 'extended': 3}\n        df5['assortment'] = df5['assortment'].map( assortment_dict )\n\n        ### 5.3.3. Nature Transformation\n        # day of week\n        df5['day_of_week_sin'] = df5['day_of_week'].apply( lambda x: np.sin( x * ( 2. * np.pi\/7 ) ) )\n        df5['day_of_week_cos'] = df5['day_of_week'].apply( lambda x: np.cos( x * ( 2. * np.pi\/7 ) ) )\n\n        # month\n        df5['month_sin'] = df5['month'].apply( lambda x: np.sin( x * ( 2. * np.pi\/12 ) ) )\n        df5['month_cos'] = df5['month'].apply( lambda x: np.cos( x * ( 2. * np.pi\/12 ) ) )\n\n        # day\n        df5['day_sin'] = df5['day'].apply( lambda x: np.sin( x * ( 2. * np.pi\/30 ) ) )\n        df5['day_cos'] = df5['day'].apply( lambda x: np.cos( x * ( 2. * np.pi\/30 ) ) )\n\n        # week of year\n        df5['week_of_year_sin'] = df5['week_of_year'].apply( lambda x: np.sin( x * ( 2. * np.pi\/52 ) ) )\n        df5['week_of_year_cos'] = df5['week_of_year'].apply( lambda x: np.cos( x * ( 2. * np.pi\/52 ) ) )\n        \n        cols_selected = [ 'store', 'promo', 'store_type', 'assortment', 'competition_distance', 'competition_open_since_month', 'competition_open_since_year', 'promo2', 'promo2_since_week', 'promo2_since_year',\n                                 'competition_time_month', 'promo_time_week', 'day_of_week_sin', 'day_of_week_cos', 'month_sin', 'month_cos', 'day_sin', 'day_cos', 'week_of_year_sin', 'week_of_year_cos' ]\n        \n        return df5[ cols_selected ]\n    \n    def get_prediction( self, model, original_data, test_data ):\n        \n        # prediction\n        pred = model.predict( test_data )\n        \n        # join pred into the original data\n        original_data['prediction'] = np.expm1( pred )\n        \n        return original_data.to_json( orient='records', date_format='iso' )","08e71976":"import pickle\nimport pandas as pd\nfrom flask import Flask, request, Response\nfrom rossmann.Rossmann import Rossmann\n\n# loading model\n# model = pickle.load( open( '\/Users\/favh2\/github_projects\/Data_Science_inProduction\/model\/model_rossmann.pkl', 'rb' ) )\n\n# initialize API\napp = Flask( __name__ )\n\n@app.route( '\/rossmann\/predict', methods=['POST'] )\ndef rossmann_predict():\n    test_json = request.get_json()\n    \n    if test_json: # there is data\n        if isinstance( test_json, dict ): # unique example\n            test_raw = pd.DataFrame( test_json, index=[0] )\n        else: # multiple examples\n            test_raw = pd.DataFrame( test_json, columns=test_json[0].keys() )\n            \n        # Instantiate Rossmann Class\n        pipeline = Rossmann()\n        \n        # data cleaning\n        df1 = pipeline.data_cleaning( test_raw )\n        \n        # feature engineering\n        df2 = pipeline.feature_engineering( df1 )\n        \n        # data preparation\n        df3 = pipeline.data_preparation( df2 )\n        \n        # prediction\n        df_response = pipeline.get_prediction( model, test_raw, df3 )\n        \n        return df_response\n        \n    else:\n        return Response( '{}', status=200, mimetype='application\/json' )\n\nif __name__ == '__main__':\n    app.run( '0.0.0.0' )","b61bb5cc":"# loading test dataset\ndf10 = pd.read_csv( '..\/input\/rossmann-store-sales\/test.csv' )","a7e9a9dd":"# merge test dataset + store\ndf_test = pd.merge( df10, df_store_raw, how='left', on='Store' )\n\n# choose store for prediction\ndf_test = df_test[df_test['Store'].isin([20, 10, 27])]\n\n# remove closed days\ndf_test = df_test[df_test['Open'] != 0]\ndf_test = df_test[~df_test['Open'].isnull()]\ndf_test = df_test.drop( 'Id', axis=1 )","882f1152":"# convert DataFrame to json\ndata = json.dumps( df_test.to_dict( orient='records' ) )","1b763889":"# API call\n#url = 'http:\/\/0.0.0.0:5000\/rossmann\/predict'\n#url = 'https:\/\/rossmann-model-victorpereira.herokuapp.com\/rossmann\/predict'\n#header = { 'Content-type': 'application\/json' }\n#data = data\n\n#r = requests.post( url, data=data, headers=header )\n#print( 'Status Code {}'.format( r.status_code ) )","3edc753d":"#d1 = pd.DataFrame( r.json(), columns=r.json()[0].keys() )","d33ce987":"#d1.sample(10)","0908b6bf":"#d2 = d1[['store', 'prediction']].groupby( 'store' ).sum().reset_index()\n\n#for i in range( len( d2 )):\n#    print( 'Store Number {} will sell R${:,.2f} in the next 6 weeks'.format(\n#            d2.loc[i, 'store'],\n#            d2.loc[i, 'prediction'] ) )","d36dd26a":"#d2","07f6c4ad":"## 10.1. Rossmann Class","b552491e":"## 5.2. Rescaling","a46e9a64":"### H9. Lojas deveriam vender mais no segundo semestre do ano.\n**FALSA** Lojas vendem MENOS no SEGUNDO SEMESTRE DO ANO.","c426ea01":"## 7.6. Compare Model's Performance","2b1a4b54":"### 7.6.2. Real Performance - Cross Validation","64c6e5df":"### 2.1.2. Product Hypotheses","0c6857a3":"## 7.1. Average Model","c60bd14c":"## 10.2. API Handler","1b70dfa7":"**1.** Lojas abertas durante o feriado de Natal deveriam vender mais.\n\n**2.** Lojas deveriam vender mais ao longo dos anos.\n\n**3.** Lojas deveriam vender mais no segundo semestre do ano.\n\n**4.** Lojas deveriam vender mais depois do dia 10 de cada m\u00eas.\n\n**5.** Lojas deveriam vender menos aos finais de semana.\n\n**6.** Lojas deveriam vender menos durante os feriados escolares.","31d5e2e4":"## 7.4. Random Forest Regressor","53d69bf9":"### 4.3.2. Categorical Attributes","69fe408c":"## 7.3. Linear Regression Regularized Model","53a274a9":"### 4.1.2. Numerical Variable","b72ec87f":"# 4.0. STEP 04 - EXPLORATORY DATA ANALYSIS","1ce8df04":"## 6.3. Manual Feature Selection","f748bd2d":"## 1.6. Change Types","d6aa2e36":"## 1.5. Fillout NA","7cc4fc58":"## 2.1. Creation of Hypotheses","ebcc0d2a":"**1.** Lojas com maior n\u00famero de funcion\u00e1rios deveriam vender mais.\n\n**2.** Lojas com maiorcapacidade de estoque deveriam vender mais.\n\n**3.** Lojas com maior porte deveriam vender mais.\n\n**4.** Lojas com maior sortimento deveriam vender mais.\n\n**5.** Lojas com competidores mais pr\u00f3ximos deveriam vender menos.\n\n**6.** Lojas com competidores \u00e0 mais tempo deveriam vender mais.","8fc7afcf":"## 8.2. Final Model","94d641d1":"## 8.1. Random Search","58285cc2":"### 2.1.1. Time Hyphoteses","e0ff5263":"### 7.3.1. Linear Regression - Lasso - Cross Validation","829dd50c":"### 4.3.1. Numerical Attributes","922ac0d4":"## 3.2. Column selection","27761a04":"### 7.6.1. Single Performance","4db975e4":"## 5.1. Normalization","164fb7e7":"## 10.3. Tester","c31f599d":"# 9.0. STEP 09 - TRANSLATION AND INTERPRETATION OF THE ERROR","8aa08328":"# 8.0. STEP 08 - HYPERPARAMETER FINE TUNING","afdcdcd2":"## 4.2. Bivariate analysis","5312d52a":"### 4.1.3. Categorical Variable","f00c5404":"**1.** Lojas que investem mais em Marketing deveriam vender mais.\n\n**2.** Lojas com maior exposi\u00e7\u00e3o de produto deveriam vender mais.\n\n**3.** Lojas produtos com rpe\u00e7o menor deveriam vender mais.\n\n**4.** Lojas com promo\u00e7\u00f5es mais agressivas (descontos maiores), deveriam vender mais.\n\n**5.** Lojas com promo\u00e7\u00f5es ativas por mais tempo deveriam vender mais.\n\n**6.** Lojas com mais dias de promo\u00e7\u00e3o deveriam vener mais.\n\n**7.** Lojas com mais promo\u00e7\u00f5es consecutivas deveriam vender mais.","fad3167c":"### H7. Lojas abertas durante o feriado de Natal deveriam vender mais.\n**FALSA** Lojas ABERTAS DURANTE O FERIADO DO NATAL vendem MENOS.","3dc8b12b":"## 0.2. Loading data","027ff27c":"### 1.7.1. Numerical Attributes","83dffd4b":"# 0.0. IMPORTS","1b8b1e8f":"## 1.1. Rename Columns","976f17a9":"### H11. Lojas deveriam vender menos aos finais de semana.\n**VERDADEIRA** Lojas vendem MENOS NOS FINAIS DE SEMANA.","4889e0bf":"### 5.3.1 Encoding","94de9e67":"### 7.5.1. XGBoost Regressor - Cross Validation","a1cd2492":"## 2.2. Final List of Hyphoteses","bf8c21de":"### 4.1.1. Response Variable","b7520df3":"## 3.1. Line Filtering","229cd5a5":"## 4.3. Multivariate analysis","d3d65eba":"### 7.4.1. Random Forest Regressor - Cross Validation","dc4a9fd8":"## 4.1. Univariate analysis","52d3eb71":"### 5.3.2. Response Variable Transformation","b5831d67":"## 6.1. Split dataframe into training and test dataset","6b9cdfd0":"## 9.3. Machine Learning Performance","7743641a":"### <s>H5. Lojas com mais dias de promo\u00e7\u00e3o deveriam vender mais.<\/s>","62c98dbc":"## 1.7. Descriptive Statistical","5f3820f1":"## 5.3. Transformation","7f8415f3":"### H4. Lojas com promo\u00e7\u00f5es ativas or mais tempo deveriam vender mais.\n**FALSA** Lojas com PROMO\u00c7\u00d5ES ATIVAS POR MAIS TEMPO vendem MENOS, depois de um certo per\u00edodo de promo\u00e7\u00e3o.","7287419d":"## 6.2. Boruta as Feature Selector","10fce361":"### 4.2.1 Resumo das Hip\u00f3teses","9a88d330":"# 10.0. STEP 10 - DEPLOY MODEL TO PRODUCTION","663eb3d5":"## 1.2. Data dimensions","a77bbca7":"### H8. Lojas deveriam vender mais ao longo dos anos.\n**FALSA** Lojas vendem MENOS AO LONGO DOS ANOS.","d6e47d85":"# 2.0. STEP 02 - FEAUTURE ENGINEERING","38fd60b0":"## 9.2. Total Performance","f9f33f7c":"## 7.5. XGBoost Regressor","42660cc7":"### H2. Lojas com competidores mais pr\u00f3ximos deveriam vender menos.\n**FALSA** Lojas com COMPETIDORES MAIS PR\u00d3XIMOS vendem MAIS.","ceb90d8d":"### H6. Lojas com mais promo\u00e7\u00f5es consecutivas deveriam vender mais.\n**FALSA** Lojas com MAIS PROMO\u00c7\u00d5ES CONSECUTIVAS vendem MENOS.","e8708315":"### H10. Lojas deveriam vender mais depois do dia 10 de cada m\u00eas.\n**VERDADEIRA** Lojas vendem MAIS DEPOIS DO DIA 10 DE CADA M\u00caS.","851b7ad9":"### 7.2.1. Linear Regression Model - Cross Validation","a15562f4":"**1.** Lojas com maior sortimento deveriam vender mais.\n\n**2.** Lojas com competidores mais pr\u00f3ximos deveriam vender menos.\n\n**3.** Lojas com competidores \u00e0 mais tempo deveriam vender mais.\n\n**4.** Lojas com promo\u00e7\u00f5es ativas por mais tempo deveriam vender mais.\n\n**5.** Lojas com mais dias de promo\u00e7\u00e3o deveriam vener mais.\n\n**6.** Lojas com mais promo\u00e7\u00f5es consecutivas deveriam vender mais.\n\n**7.** Lojas abertas durante o feriado de Natal deveriam vender mais.\n\n**8.** Lojas deveriam vender mais ao longo dos anos.\n\n**9.** Lojas deveriam vender mais no segundo semestre do ano.\n\n**10.** Lojas deveriam vender mais depois do dia 10 de cada m\u00eas.\n\n**11.** Lojas deveriam vender menos aos finais de semana.\n\n**12.** Lojas deveriam vender menos durante os feriados escolares.","f0210081":"### 2.1.1. Shop Hypotheses","5f943d11":"# 3.0. STEP 03 - VARIABLE FILTERING","ec62e5b9":"### H1. Lojas com maior sortimento deveriam vender mais\n**FALSA** Lojas com MAIOR SORTIMENTO vendem MENOS","8ece108e":"## 1.3. Data Types","9b8ef73e":"## 1.4. Check NA","7cfaf4f7":"## 0.1. Helper function","9d13f5f8":"# 5.0. STEP 05 - DATA PREPARATION","b3e2987a":"# 7.0. STEP 07 - MACHINE LEARNING MODELLING","24482777":"### 1.7.2. Categorical attributes","c3b5e9b9":"# 6.0. STEP 06 - FEATURE SELECTION","393ffbc8":"### H3. Lojas com competidores \u00e0 mais tempo deveriam vender mais.\n**FALSA** Lojas com COMPETIDORES \u00c0 MAIS TEMPO vendem MENOS.","55ef4311":"## 7.2. Linear Regression Model","fc0eef50":"## 9.1. Business Performance","6b7ab648":"### 6.1.1. Best Feature from Boruta","834e6d80":"# 1.0. STEP 01 - DESCRIPTION OF DATA","16f423b3":"### 5.3.3. Nature Transformation","7e992f7e":"### H12. Lojas deveriam vender menos durante os feriados escolares.\n**VERDADEIRA** Lojas VENDEM MENOS DURANTE OS FERIADOS ESCOLARES, except os meses de Julho e Agosto.","e951b15c":"## 2.3. Feature Engineering"}}