{"cell_type":{"b01d25fe":"code","b52e5151":"code","8977088a":"code","90b375a7":"code","50df5fd5":"code","78bcaca0":"code","a266d712":"code","2d8d7744":"code","9ae96537":"code","215edb18":"code","7962ac4c":"code","0034fc45":"code","12503302":"code","11dc6b6d":"code","edc8e7ab":"code","8c50aec2":"code","0b8df7a0":"code","f0caf137":"code","1ff52d8b":"code","47c4c453":"code","c4f8dd57":"code","805873f0":"markdown"},"source":{"b01d25fe":"# Please see the dataset description for additional info \n# current accuracy (balanced): see last run (this is the AraBERT implementation using ktran)","b52e5151":"import time \nprint(f'Started: {time.ctime()}')","8977088a":"!pip install -U pip\n!pip install \"tensorflow_gpu>=2.0.0\"  # cpu: pip3 install \"tensorflo\n!pip install transformers==2.5.1\n!pip install ktrain ","90b375a7":"# restart run time\n#import os\n#def restart_runtime():\n#  os.kill(os.getpid(), 9)\n#restart_runtime()","50df5fd5":"# forked versions of eli5 and stellargraph (required by ktrain)\n!pip install git+https:\/\/github.com\/amaiya\/eli5@tfkeras_0_10_1 \n!pip install git+https:\/\/github.com\/amaiya\/stellargraph@no_tf_dep_082","78bcaca0":"import pandas as pd\npd.set_option('display.max_colwidth',200)\npd.set_option('display.max_rows',50)\ndf = pd.read_excel('..\/input\/tunisian-texts\/tun.xlsx')\ndf['data_labels'].value_counts()","a266d712":"df.dtypes","2d8d7744":"df = df.sample(frac=1).reset_index(drop=True)\ndf.head()","9ae96537":"\"\"\"class_names=['Positive','Negative']\ndf['data_labels'] = df['data_labels'].apply(lambda x: class_names.index(x))\nprint(len(df), '\\n')\ndf.head()\"\"\"","215edb18":"df_test = df.sample(frac=0.15, random_state=42)\ndf_train = df.drop(df_test.index)\nlen(df_train), len(df_test)","7962ac4c":"x_train = df_train['texts'].tolist()\ny_train = df_train['data_labels'].to_numpy()\nx_test = df_test['texts'].tolist()\ny_test = df_test['data_labels'].to_numpy()","0034fc45":"import matplotlib.pyplot as plt \nfig,(ax1,ax2)=plt.subplots(1,2,figsize=(10,5))\ntweet_len=df_train[df_train['data_labels']==1]['texts'].str.split().map(lambda x: len(x))\nax1.hist(tweet_len,color='blue')\nax1.set_title('disaster tweets')\ntweet_len=df_test[df_test['data_labels']==0]['texts'].str.split().map(lambda x: len(x))\nax2.hist(tweet_len,color='CRIMSON')\nax2.set_title('Not disaster tweets')\nfig.suptitle('Words in a tweet')\nplt.show()","12503302":"import ktrain\nfrom ktrain import text \nMODEL_NAME = 'aubmindlab\/bert-base-arabertv01'\nt = text.Transformer(MODEL_NAME, maxlen=90)\n\ntrn = t.preprocess_train(x_train, y_train)\nval = t.preprocess_test(x_test, y_test)\nmodel = t.get_classifier()\nlearner = ktrain.get_learner(model, train_data=trn, val_data=val, batch_size=32)","11dc6b6d":"#takes long time\n#learner.lr_find(show_plot=True, max_epochs=2)","edc8e7ab":"learner.autofit(0.1, 2)","8c50aec2":"learner.validate(class_names=t.get_classes())","0b8df7a0":"print(learner.view_top_losses(n=5, preproc=t)) ","f0caf137":"#n = ?? # from above\n#print(x_test[n])","1ff52d8b":"#test_text = '' # enter a review - ex. for printed above\n#predictor = ktrain.get_predictor(learner.model, preproc=t)\n#predictor.predict(test_text)","47c4c453":"#predictor.explain(test_text)","c4f8dd57":"print(f'Finished: {time.ctime()}')","805873f0":"#https:\/\/medium.com\/analytics-vidhya\/finetuning-bert-using-ktrain-for-disaster-tweets-classification-18f64a50910b"}}