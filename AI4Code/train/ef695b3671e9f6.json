{"cell_type":{"747ed065":"code","fbbe064a":"code","232b7f52":"code","fc3abf5b":"code","9d1fba53":"code","bc5e5922":"code","a83a37e6":"code","31f1e70c":"code","ae69248f":"code","22407674":"code","f22f806b":"code","275ae855":"code","a99def97":"code","c54bf4a4":"code","3d8faf91":"code","7c71c875":"code","f9bb4d3c":"code","5f95abb4":"code","70deae0d":"code","a6dfd812":"code","4b8dc1a7":"code","697e70db":"code","c0fb43b9":"code","5f4141d0":"code","2d07e62d":"code","11f81791":"code","4a780afd":"code","c7867a42":"code","b6b44993":"code","ebca0298":"code","6d9398e5":"code","8488a0bc":"code","ba40dea8":"code","351193b5":"code","1152a495":"code","f1ce87c3":"code","71229f3e":"code","639c2818":"code","aa00c855":"code","0bb324df":"code","220a9f99":"code","2f4ec5c8":"code","b7530718":"code","e3861692":"code","0d3f6129":"markdown","0c8ad4d4":"markdown","1d822118":"markdown","419dae09":"markdown","4a4f5fd4":"markdown","86fb1524":"markdown","72f770c9":"markdown","fdd94285":"markdown","866b981c":"markdown","7ca6e510":"markdown","bc1556b0":"markdown","b13b7dc2":"markdown","6db4faf3":"markdown","85fa10d3":"markdown","34b8f235":"markdown","8ef02f12":"markdown","45ac9a38":"markdown","82ca0157":"markdown","84539bdc":"markdown","9d923094":"markdown","84449e17":"markdown","e2910913":"markdown","3bbe6a5f":"markdown","9cb67c23":"markdown"},"source":{"747ed065":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","fbbe064a":"DATA_FOLDER = '..\/input'\ntransactions    = pd.read_csv(os.path.join(DATA_FOLDER, 'sales_train.csv.gz'))\nitems           = pd.read_csv(os.path.join(DATA_FOLDER, 'items.csv'))\nitem_categories = pd.read_csv(os.path.join(DATA_FOLDER, 'item_categories.csv'))\nshops           = pd.read_csv(os.path.join(DATA_FOLDER, 'shops.csv'))\ntest            = pd.read_csv(os.path.join(DATA_FOLDER, 'test.csv.gz'))","232b7f52":"transactions = pd.merge(transactions, items, on='item_id', how='left')\ntransactions = transactions.drop('item_name', axis=1)\ntransactions.head()","fc3abf5b":"from itertools import product\nindex_cols = ['shop_id', 'item_id', 'date_block_num']","9d1fba53":"grid = []\nfor block_num in transactions['date_block_num'].unique():\n    cur_shops = transactions.loc[transactions['date_block_num'] == block_num, 'shop_id'].unique()\n    cur_items = transactions.loc[transactions['date_block_num'] == block_num, 'item_id'].unique()\n    grid.append(np.array(list(product(*[cur_shops, cur_items, [block_num]])), dtype='int32'))\ngrid = pd.DataFrame(np.vstack(grid), columns = index_cols, dtype=np.int32)","bc5e5922":"grid.head()","a83a37e6":"mean_transactions = transactions.groupby(['date_block_num', 'shop_id', 'item_id']).agg({'item_cnt_day':'sum','item_price':np.mean}).reset_index()","31f1e70c":"mean_transactions = pd.merge(grid,mean_transactions,on=['date_block_num', 'shop_id', 'item_id'],how='left').fillna(0)","ae69248f":"mean_transactions = pd.merge(mean_transactions, items, on='item_id',how='left')","22407674":"mean_transactions.head()","f22f806b":"for type_id in ['item_id', 'shop_id', 'item_category_id']:\n    for column_id, aggregator, aggtype in [('item_price',np.mean,'avg'),('item_cnt_day',np.sum,'sum'),('item_cnt_day',np.mean,'avg')]:\n        \n        mean_df = transactions.groupby([type_id,'date_block_num']).aggregate(aggregator).reset_index()[[column_id,type_id,'date_block_num']]\n        mean_df.columns = [type_id+'_'+aggtype+'_'+column_id,type_id,'date_block_num']\n        mean_transactions = pd.merge(mean_transactions, mean_df, on=['date_block_num',type_id], how='left')","275ae855":"mean_transactions.head(10)","a99def97":"lag_variables  = list(mean_transactions.columns[7:])+['item_cnt_day']\nlags = [1, 2, 3, 6]\nfrom tqdm import tqdm_notebook\nfor lag in tqdm_notebook(lags):\n\n    sales_new_df = mean_transactions.copy()\n    sales_new_df.date_block_num += lag\n    sales_new_df = sales_new_df[['date_block_num','shop_id','item_id']+lag_variables]\n    sales_new_df.columns = ['date_block_num','shop_id','item_id']+ [lag_feat+'_lag_'+str(lag) for lag_feat in lag_variables]\n    mean_transactions = pd.merge(mean_transactions, sales_new_df,on=['date_block_num','shop_id','item_id'] ,how='left')","c54bf4a4":"mean_transactions.head()","3d8faf91":"mean_transactions = mean_transactions[mean_transactions['date_block_num']>12]","7c71c875":"for feat in mean_transactions.columns:\n    if 'item_cnt' in feat:\n        mean_transactions[feat]=mean_transactions[feat].fillna(0)\n    elif 'item_price' in feat:\n        mean_transactions[feat]=mean_transactions[feat].fillna(mean_transactions[feat].median())","f9bb4d3c":"cols_to_drop = lag_variables[:-1] + ['item_price', 'item_name'] # dropping all target variables but not \"item_cnt_day\" cause is target","5f95abb4":"training = mean_transactions.drop(cols_to_drop,axis=1)","70deae0d":"import xgboost as xgb","a6dfd812":"xgbtrain = xgb.DMatrix(training.iloc[:, training.columns != 'item_cnt_day'].values, training.iloc[:, training.columns == 'item_cnt_day'].values)","4b8dc1a7":"param = {'max_depth':10, \n         'subsample':1,\n         'min_child_weight':0.5,\n         'eta':0.3, \n         'num_round':1000, \n         'seed':1,\n         'silent':0,\n         'eval_metric':'rmse'} # random parameters\nbst = xgb.train(param, xgbtrain)","697e70db":"x=xgb.plot_importance(bst)\nx.figure.set_size_inches(10, 30) ","c0fb43b9":"cols = list(training.columns)\ndel cols[cols.index('item_cnt_day')] # eliminate target feature col name","5f4141d0":"[cols[x] for x in [2, 0, 5, 8, 4, 1, 3, 9, 33]]","2d07e62d":"training.columns","11f81791":"test            = pd.read_csv(os.path.join(DATA_FOLDER, 'test.csv.gz'))\ntest.head()","4a780afd":"test['date_block_num'] = 34","c7867a42":"test = pd.merge(test, items, on='item_id', how='left')","b6b44993":"from tqdm import tqdm_notebook\nfor lag in tqdm_notebook(lags):\n\n    sales_new_df = mean_transactions.copy()\n    sales_new_df.date_block_num += lag\n    sales_new_df = sales_new_df[['date_block_num','shop_id','item_id']+lag_variables]\n    sales_new_df.columns = ['date_block_num','shop_id','item_id']+ [lag_feat+'_lag_'+str(lag) for lag_feat in lag_variables]\n    test = pd.merge(test, sales_new_df,on=['date_block_num','shop_id','item_id'] ,how='left')","ebca0298":"_test = set(test.drop(['ID', 'item_name'], axis=1).columns)\n_training = set(training.drop('item_cnt_day',axis=1).columns)\nfor i in _test:\n    assert i in _training\nfor i in _training:\n    assert i in _test","6d9398e5":"assert _training == _test","8488a0bc":"test = test.drop(['ID', 'item_name'], axis=1)","ba40dea8":"for feat in test.columns:\n    if 'item_cnt' in feat:\n        test[feat]=test[feat].fillna(0)\n    elif 'item_price' in feat:\n        test[feat]=test[feat].fillna(test[feat].median())","351193b5":"test[['shop_id','item_id']+['item_cnt_day_lag_'+str(x) for x in [1,2,3]]].head()","1152a495":"print(training[training['shop_id'] == 5][training['item_id'] == 5037][training['date_block_num'] == 33]['item_cnt_day'])\nprint(training[training['shop_id'] == 5][training['item_id'] == 5037][training['date_block_num'] == 32]['item_cnt_day'])\nprint(training[training['shop_id'] == 5][training['item_id'] == 5037][training['date_block_num'] == 31]['item_cnt_day'])","f1ce87c3":"xgbpredict = xgb.DMatrix(test.values)","71229f3e":"pred = bst.predict(xgbpredict)","639c2818":"pd.Series(pred).describe()","aa00c855":"pred = pred.clip(0, 20)","0bb324df":"pred.sum()","220a9f99":"pd.Series(pred).describe()","2f4ec5c8":"sub_df = pd.DataFrame({'ID':test.index,'item_cnt_month': pred })","b7530718":"sub_df.head()","e3861692":"sub_df.to_csv('submission.csv',index=False)","0d3f6129":"the lagged value for (5\t5037) actually correspond, looks like we dont have bugs!","0c8ad4d4":"so we need these columns above without 'item_cnt_day', and we have only the column below","1d822118":"<h2>A)<\/h2>\nSo here we are grouping the transactions and applying the aggreated functions where ** sum of item_cnt_day will be our target variable** .\n\nWhy? This two new columns uses target values so they will be used 1) as a target value 2) to create lagged values in the time serie","419dae09":"the most important features (2 0 5 8 4 1 3 9 33) are:","4a4f5fd4":"this are the transactions we will groupby from, + category_id","86fb1524":"<h2>**8. Predict**","72f770c9":"These above lines add the following 9 features :\n\n'item_id_avg_item_price'\n'item_id_sum_item_cnt_day'\n'item_id_avg_item_cnt_day'\n'shop_id_avg_item_price',\n'shop_id_sum_item_cnt_day'\n'shop_id_avg_item_cnt_day'\n'item_category_id_avg_item_price'\n'item_category_id_sum_item_cnt_day'\n'item_category_id_avg_item_cnt_day'","fdd94285":"<h1>[mlwhiz] Using XGBoost for time series prediction tasks<\/h1>\n\nThis notebook is a basic implementation of the content of the great article [by mlwhiz.com](http:\/\/mlwhiz.com\/blog\/2017\/12\/26\/How_to_win_a_data_science_competition\/) on how he got into the top 10 of 'Final Project' competition. \n\n**What you will find here?**\n1.  A working notebook that you can submit that follows the idea of mlwhiz blog\n2. Copy paste of his data-manipulation\n\n**What you will not find here**\n1.  A replication of his result: namely I just used a basic xgboost to make it work, what I am interested in is the data manipulation part\n\n<h3>Description of the Problem:<\/h3>\n\nIn this competition we were given a challenging time-series dataset consisting of daily sales data, kindly provided by one of the largest Russian software firms - 1C Company.\n\nWe were asked you to predict total sales for every product and store in the next month.\n\nThe evaluation metric was RMSE where True target values are clipped into [0,20] range. This target range will be a lot important in understanding the submissions that I will prepare.\n\nThe main thing that I noticed was that the data preparation aspect of this competition was by far the most important thing. I creted a variety of features. Here are the steps I took and the features I created.\"","866b981c":"After calculating target variable and price_mean we merge it back to our grid, and we add item_category_id","7ca6e510":"<h2>B)<\/h2>\nHere we create **MEAN ENCODING**\n\nNow we create additional encoding with aggregation functions on our data as follow: [('**item_price**',np.mean,'**avg**'),('**item_cnt_day**',np.sum,'**sum**'),('**item_cnt_day**',np.mean,'**avg**')]:\n","bc1556b0":"<h2>6. Now we can finally train the XGB model<\/h2>\nI wrote a really simple xgb process to train a model with the given data model, training the xgb will takes more or less 10 mins","b13b7dc2":"<h2>KEY POINT to understand this passage<\/h2>\nWhat the difference between A and B? (try to answer your self first)\n\n\n**Answer:** is in how we are grouping the data\n\n<p>A) -> transactions.groupby(['date_block_num', 'shop_id', 'item_id'])<\/p>\n<p>B) -> transactions.groupby([type_id,'date_block_num'])<\/p>\n\nIn A) we are grouping our features by BOTH shop_id and item_id , in B) we group individually by shop_id, by item_id and by item_category_id. This follow a suggestion in the course on coursera.","6db4faf3":"lets check that our lag is actually correct","85fa10d3":"What we do now? We created our mean encoding, but we can not using at prediction time (because we don't know the target variable so we can not encode it). What we do we create a lag for all this mean encoding features and we will use this lagged value (mean encoding at t-1, t-2 ..) to predict our values","34b8f235":"<h2>IMPORTANT LINE TO UNDERSTAND HOW TO TRAIN THE MODEL<\/h2>\nin the next line we choose to 'drop' some columns, what are these columns? are the one that we will not be able to have at prediction time. \nWhat column we won't have a prediction time? All the column that are **not lagged**, thats why we drop all lag_variables\n\n[note: for me this was the key to understand everything]","8ef02f12":"Up to now we just crated a combinations of all possible triples (shop_id, item_id, date_block_num)","45ac9a38":"<h2>3. Created Mean Encodings:<\/h2>","82ca0157":"<h2>**1. Created a dataframe of all Date_block_num, Store and Item combinations:**<\/h2>\nThis is important because in the months we don't have a data for an item store combination, the machine learning algorithm needs to be specifically told that the sales is zero.","84539bdc":"As you can see below, now we have the same mean_encoded features but also lagged, the number of columns now is around 50","9d923094":"So we need to manipulate the training set similarly to how we did in the first part of the notebook.\n- add date_block_num = 34\n- add category_id\n- add lagging","84449e17":"<h2>5. Fill NA with zeros:<\/h2>","e2910913":"<h2>4. Create Lag Features:<\/h2>","3bbe6a5f":"**They look the same!**","9cb67c23":"<h2>7. Preparing predictions<\/h2>\nnow we need to embrace the hard task of preparing prediction data without bugs"}}