{"cell_type":{"2d222421":"code","0d121493":"code","aa14268d":"code","624e4462":"code","46aaa961":"code","233f339a":"code","2be77ffc":"code","e19506b3":"code","2e078157":"code","2b66c1d9":"code","ec7a8655":"code","f81be169":"code","eb6751e5":"code","6b447f1f":"code","3b2e1e94":"code","53ee03eb":"code","ec5e0194":"code","2eb1fd7a":"code","db3af430":"code","4bcb59bf":"code","4e62a366":"code","907aa5cb":"code","d3c1d055":"code","f02d2182":"code","0019dc96":"code","69fb59fc":"code","774f95e5":"code","fc9a0991":"code","07ad172c":"code","11b49897":"code","4ee8da79":"code","98cdcd61":"code","0669a7c2":"code","48b9669d":"code","337ef002":"markdown","4a70e75b":"markdown","8b340f63":"markdown","6c2d5c5e":"markdown","a28b3d05":"markdown","97e638b2":"markdown","0987758d":"markdown","646ae9ab":"markdown","0738eace":"markdown","ef4e33d4":"markdown","3ba205d8":"markdown","0df85741":"markdown","f9d57281":"markdown","f37fc386":"markdown","b6100045":"markdown","b50507d8":"markdown","413c11b3":"markdown","2a2003d1":"markdown","a358d55e":"markdown","b2259e0c":"markdown","883db032":"markdown","d950e2f6":"markdown","6580070d":"markdown","e92bce19":"markdown","82990168":"markdown","7b67a9d2":"markdown","c562c62b":"markdown","3d589d55":"markdown","3614078e":"markdown","c6581ac5":"markdown","7f087133":"markdown","f0cb9f8a":"markdown","8c17a745":"markdown","21631fec":"markdown","45859633":"markdown"},"source":{"2d222421":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nimport string\nimport itertools\n\nimport nltk\nfrom nltk.corpus import stopwords\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn import metrics\nfrom sklearn.metrics import roc_curve, auc,accuracy_score\nfrom nltk.stem.porter import PorterStemmer\nwarnings.filterwarnings(\"ignore\")\n#nltk.download('punkt')","0d121493":"Kaggle=1\nif Kaggle==0:\n    reviews =pd.read_csv(\"amazon_alexa.tsv\",sep=\"\\t\")\n    \nelse:\n    reviews = pd.read_csv(\"..\/input\/amazon_alexa.tsv\",sep=\"\\t\")\n    ","aa14268d":"reviews.head()","624e4462":"reviews.describe()","46aaa961":"plt.figure(figsize=(8,8))\nax=sns.countplot(reviews['rating'],palette=sns.color_palette(palette=\"viridis_r\"))\nax.set_title(\"Distribution of the Amazon Alexa Rating\")\nax.set_xlabel(\"Rating\")\nax.set_ylabel(\"Count\")","233f339a":"variant_rating=reviews.groupby('variation')['rating'].mean().reset_index()\nvariant_rating.head()","2be77ffc":"plt.figure(figsize=(8,8))\nax=sns.barplot(x='variation',y='rating',data=variant_rating,palette=sns.color_palette(palette=\"viridis_r\"))\nax.set_title(\"Average Rating based on Alexa Variant\")\nax.set_xticklabels(ax.get_xticklabels(),rotation=90)\nax.set_xlabel(\"Rating\")\nax.set_ylabel(\"Count\")","e19506b3":"rating_review=reviews.sort_values(by='rating',ascending=False)\nrating_review.head()","2e078157":"for i in rating_review['verified_reviews'].iloc[:6]:\n    print(i, '\\n')","2b66c1d9":"for i in rating_review['verified_reviews'].iloc[-6:]:\n    print(i, '\\n')","ec7a8655":"rating_review['date'] = pd.to_datetime(rating_review['date'], errors='coerce')\nmonth_count = rating_review['date'].dt.month.value_counts()\nmonth_count = month_count.sort_index()\nplt.figure(figsize=(9,6))\nsns.barplot(month_count.index, month_count.values,color='green',alpha=0.4)\nplt.xticks(rotation='vertical')\nplt.xlabel('Month', fontsize=12)\nplt.ylabel('Count', fontsize=12)\nplt.title(\"Reviews per Month\")\nplt.show()","f81be169":"weekday_count = rating_review['date'].dt.weekday_name.value_counts()\nweekday_count = weekday_count.sort_index()\nplt.figure(figsize=(9,6))\nsns.barplot(weekday_count.index, weekday_count.values,color='green',alpha=0.4,order=['Sunday','Monday','Tuesday','Wednesday','Thursday','Friday','Saturday'])\nplt.xticks(rotation='vertical')\nplt.xlabel('Weekday', fontsize=12)\nplt.ylabel('Count', fontsize=12)\nplt.title(\"Reviews by Weekday\")\nplt.show()","eb6751e5":"rating_review['weekday']=rating_review['date'].dt.weekday_name\navg_weekday=rating_review.groupby('weekday')['rating'].mean()\nplt.figure(figsize=(9,6))\nsns.barplot(avg_weekday.index, avg_weekday.values,color='green',alpha=0.4,order=['Sunday','Monday','Tuesday','Wednesday','Thursday','Friday','Saturday'])\nplt.xticks(rotation='vertical')\nplt.xlabel('Weekday', fontsize=12)\nplt.ylabel('Avg Rating', fontsize=12)\nplt.title(\"Average Rating over the day\")\nplt.show()","6b447f1f":"def sentiment(x):\n    if x > 3:\n        return 'positive'\n    else:\n        return 'negative'\n        ","3b2e1e94":"rating = rating_review['rating']\nrating=rating.map(sentiment)\nreview=rating_review['verified_reviews']","53ee03eb":"rating.describe()","ec5e0194":"X_train,X_test,y_train,y_test=train_test_split(review,rating,test_size=0.2,stratify=rating,random_state=100)","2eb1fd7a":"print(\"Shape of train is {} and shape of test is {}\".format(X_train.shape,X_test.shape));","db3af430":"### Borrowed from https:\/\/www.kaggle.com\/gpayen\/building-a-prediction-model\n\nstemmer = PorterStemmer()\n\n## Stemming :\n\ndef stem_tokens(tokens, stemmer):\n    stemmed = []\n    for item in tokens:\n        stemmed.append(stemmer.stem(item))\n    return stemmed\n\n## Tokenisation:\n\ndef tokenize(text):\n    tokens = nltk.word_tokenize(text)\n    tokens = [word for word in tokens if word not in nltk.corpus.stopwords.words('english')]\n    stems = stem_tokens(tokens, stemmer)\n    return ' '.join(stems)\n\n## Removing the punctuation:\n\nintab = string.punctuation\nouttab = \"                                \"\ntrantab = str.maketrans(intab, outtab)  ### Remove the punctuations \n\n#--- Training set\n\ncorpus = []\nfor text in X_train:\n    text = text.lower()\n    text = text.translate(trantab)\n    text=tokenize(text)\n    corpus.append(text)","4bcb59bf":"corpus[:5]","4e62a366":"count_vect = CountVectorizer()\nX_train_counts = count_vect.fit_transform(corpus)        \n        \ntfidf_transformer = TfidfTransformer()\nX_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)","907aa5cb":"#--- Test set\n\ntest_set = []\nfor text in X_test:\n    text = text.lower()\n    text = text.translate(trantab)\n    text=tokenize(text)\n    test_set.append(text)\n\nX_new_counts = count_vect.transform(test_set)\nX_test_tfidf = tfidf_transformer.transform(X_new_counts)\n\nprediction = dict()","d3c1d055":"model =MultinomialNB()\nmodel.fit(X_train_tfidf,y_train)\nprediction['Naive Bayes']=model.predict(X_test_tfidf)","f02d2182":"print(accuracy_score(y_test,prediction['Naive Bayes']))","0019dc96":"df_confusion = pd.crosstab(y_test,prediction['Naive Bayes'], rownames=['Actual'], colnames=['Predicted'], margins=True)\ndf_confusion","69fb59fc":"model = LogisticRegression()\nmodel.fit(X_train_tfidf,y_train)\nprediction['Logit']=model.predict(X_test_tfidf)","774f95e5":"print(accuracy_score(y_test,prediction['Logit']))","fc9a0991":"df_confusion = pd.crosstab(y_test,prediction['Logit'], rownames=['Actual'], colnames=['Predicted'], margins=True)\ndf_confusion","07ad172c":"#Borrowed from https:\/\/www.kaggle.com\/gpayen\/building-a-prediction-model\ndef formatt(x):\n    if x == 'negative':\n        return 0\n    return 1\nvfunc = np.vectorize(formatt)\n\ncmp = 0\ncolors = ['b', 'g', 'y', 'm', 'k']\nfor model, predicted in prediction.items():\n    false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test.map(formatt), vfunc(predicted))\n    roc_auc = auc(false_positive_rate, true_positive_rate)\n    plt.plot(false_positive_rate, true_positive_rate, colors[cmp], label='%s: AUC %0.2f'% (model,roc_auc))\n    cmp += 1\n\nplt.title('Classifiers comparaison with ROC')\nplt.legend(loc='lower right')\nplt.plot([0,1],[0,1],'r--')\nplt.xlim([-0.1,1.2])\nplt.ylim([-0.1,1.2])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show()","11b49897":"# Inspired from https:\/\/www.kaggle.com\/rafjaa\/resampling-strategies-for-imbalanced-datasets\n\nfrom imblearn.over_sampling import SMOTE","4ee8da79":"smote=SMOTE(random_state=100)\nX_sm,y_sm=smote.fit_sample(X_train_tfidf,y_train)  ### Oversampling the training dataset.","98cdcd61":"## Applying the Naive Bayes and Logit again on the model:\nmodel =MultinomialNB()\nmodel.fit(X_sm,y_sm)\nprediction['Naive Bayes_SMOTE']=model.predict(X_test_tfidf)\n\n\ndf_confusion_SMOTE = pd.crosstab(y_test,prediction['Naive Bayes_SMOTE'], rownames=['Actual'], colnames=['Predicted'], margins=True)\ndf_confusion_SMOTE\n","0669a7c2":"model = LogisticRegression()\nmodel.fit(X_sm,y_sm)\nprediction['Logit_SMOTE']=model.predict(X_test_tfidf)\n\n\ndf_confusion = pd.crosstab(y_test,prediction['Logit_SMOTE'], rownames=['Actual'], colnames=['Predicted'], margins=True)\ndf_confusion","48b9669d":"def formatt(x):\n    if x == 'negative':\n        return 0\n    return 1\nvfunc = np.vectorize(formatt)\n\ncmp = 0\ncolors = ['b', 'g', 'y', 'm', 'k']\nfor model, predicted in prediction.items():\n    false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test.map(formatt), vfunc(predicted))\n    roc_auc = auc(false_positive_rate, true_positive_rate)\n    plt.plot(false_positive_rate, true_positive_rate, colors[cmp], label='%s: AUC %0.2f'% (model,roc_auc))\n    cmp += 1\n\nplt.title('Classifiers comparaison with ROC')\nplt.legend(loc='lower right')\nplt.plot([0,1],[0,1],'r--')\nplt.xlim([-0.1,1.2])\nplt.ylim([-0.1,1.2])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show()","337ef002":"It is seen that the dataset has more reviews from July . Is there any pattern in the number of reviews posted by weekday ?","4a70e75b":"We have been provided with the rating score for each reviews . Using this I create a sentiment score - either positive or negative . I am interested to create a model that would predict these sentiment given the review text . I split the dataset into training and test . Given the small number of samples , I split the data 80-20 ratio.Before that I create the sentiment type variable using the score.","8b340f63":"### The metric Trap","6c2d5c5e":"### Data Exploration","a28b3d05":"The dataset provided gives us information on various variants of amazon Alexa devices and their reviews . This is a great dataset for natural language processing task like sentiment analysis , word2vec, topic modelling etc . Lets explore in those lines and produce some interesting results.","97e638b2":"To summarise,\n\n* We first did some initial data analysis to understand about various variables present in the dataset.It was seen that the average score for the reviews was 4.46. The number of reviews by weekday and average rating was plotted to understand if there was any significant difference in the reviews by the day . \n\n* Models were created to predict the sentiment of the review given the review statement . Text cleaning steps like removing punctuation , stemming and creating the TF-IDF matrix . The dataset was split into train and test by 80-20 ratio . Given the highly imbalanced dataset , the AUC was around 0.5 for both Naive Bayes and Logit models.\n\n* SMOTE techniques were applied and the model was rebuilt which improved the AUC scores to 0.8 for both the models.\n\nThanks for reading my notebook .Any feedback in the form of comments\/upvotes are appreciated.","0987758d":"### Model Comparison","646ae9ab":"### Predicting the sentiment of the review :","0738eace":"Lets check the top 3 most positive and top 3 most negative reviews.","ef4e33d4":"### Reviews","3ba205d8":"From the AUC curve,it is seen that the AUC of logistic regression is slightly higher .","0df85741":"From the confusion matrix , it is understood that the model is good at predicting positive reviews more strongly than the negative reviews.All the negative reviews are predicted as positive .This is because the dataset is imbalanced with most of the reviews being positive .Sampling methods need to be applied to improve the confusion matrix .","f9d57281":"# Understanding Amazon Alexa Reviews ","f37fc386":"### AUC Curve","b6100045":"From the countplot it is seen that there are more 5 ratings followed by 4 ratings .Lets check the average ratings by variant.","b50507d8":"Though the accuracy rates are good ,it will be too early to conclude about the model since this is an unbalanced dataset with most of the reviews being positive . Therefore using the accuracy score as a metric will always give a high value .Therefore another robust metric like confusion matrix or AUC curve will provide a more realistic picture.Lets check the AUC curve for the two methods.","413c11b3":"### Introduction","2a2003d1":"There are 5 columns:\n\n1.Rating - Provides the rating for each of the variants of the amazon alexa device.\n\n2.Date - Date when the review , rating was posted.\n\n3.Variation - Amazon Alexa variant.\n\n4.Verified Reviews - Detailed review of the device.\n\n5.Feedback - Numeric number 0 or 1 .I am not sure what this is . Going by the report I guess it should be positive and negative feedback based on the rating.\n","a358d55e":"The average rating of the alexa devices based on different variant shows that the average rating is more than 4 for each devices .**Charcoal Fabric**,**Walnut Finish** have highest average ratings.","b2259e0c":"### Conclusion ","883db032":"### Applying Logistic Regression:","d950e2f6":"The accuracy of the model is 86 % . Lets plot the confusion matrix for naive bayes.","6580070d":"From the summary of the data we see that there are no null values and the data is very clean .The average rating is 4.46 which is close to the maximum value.It is seen that most of the users have left positive reviews and are happy with the device . Lets plot the countplot of the ratings. ","e92bce19":"Thus from the AUC graph , it is seen that after applying the SMOTE the score has improved from 0.50 . Both the model's performance is at the same level . ","82990168":"### Dealing with Imbalanced Datasets - SMOTE","7b67a9d2":"We see that there are 2741 positive reviews .Now lets split the dataset into train and test.","c562c62b":"Comparing the confusion matrix before and after over-sampling , it is seen that the model prediction has improved after sampling .Lets plot the AUC metric and compare the scores for all the four models.","3d589d55":"### Applying Naive Bayes Model:","3614078e":"The confusion matrix shows the general trend observed in NB method . The model is good at predicting positive reviews whereas no so good in predicting negative reviews.","c6581ac5":"The number of reviews written are more on Mondays than any other days of the week .Let us see the if the average rating differs by the day of the week.","7f087133":"### Reading the dataset","f0cb9f8a":"The accuracy from the logit is slightly higher - 87 %.","8c17a745":"Inorder to impove the model performance ,lets train the model with balanced dataset . For this we use the **imblearn** library and use SMOTE(Synthetic Minority Oversampling) technique .","21631fec":"From the barplot it is understood that there is no significant difference between the ratings over the day.On all days,the rating has been more than 4 .There has been a slight dip in the ratings provided on thursdays and fridays .","45859633":"### Import necessary libraries"}}