{"cell_type":{"d3f53cc7":"code","e88cda7c":"code","af5b2923":"code","0aeead7f":"code","fee528e2":"code","07011bf8":"code","1f43e636":"code","70ff4ed2":"code","2f9954d6":"code","d4ed810b":"code","44eef034":"code","c954e721":"code","ad948d1d":"code","db0ef62f":"code","57a51a7c":"code","bd5c9394":"code","80eb9a3e":"code","30d4a834":"code","3882e35c":"code","4596c568":"code","bb218dff":"code","c205dbcb":"code","199db6e6":"code","677524cf":"code","5004ae68":"code","f4174193":"code","00aef8e8":"markdown","050e588a":"markdown","7b68a2f4":"markdown","fad11a8b":"markdown","48db56be":"markdown","ee1f059e":"markdown","49c3fdce":"markdown","9ce44c55":"markdown","3c3de885":"markdown","48361f87":"markdown","0d2f6e94":"markdown","2ca51bea":"markdown"},"source":{"d3f53cc7":"%matplotlib inline\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport sqlite3\nimport pandas as pd\nimport numpy as np\nimport nltk\nimport string\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn import metrics\nfrom sklearn.metrics import roc_curve, auc\nfrom nltk.stem.porter import PorterStemmer\n\nimport re\n# Tutorial about Python regular expressions: https:\/\/pymotw.com\/2\/re\/\nimport string\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\nfrom nltk.stem.wordnet import WordNetLemmatizer\n\nfrom gensim.models import Word2Vec\nfrom gensim.models import KeyedVectors\nimport pickle\n\nfrom tqdm import tqdm_notebook as tqdm\n\nfrom tqdm import tqdm\nimport os","e88cda7c":"data = pd.read_csv(\"..\/input\/million-headlines\/abcnews-date-text.csv\")\ndata.head()","af5b2923":"data_sort = data.sort_values(\"publish_date\",axis=0,ascending=True,kind='quicksort',na_position='last')\ndata_sort.head()","0aeead7f":"from bs4 import BeautifulSoup\n#Remove HTML tags and URL from the reviews.\ndef html_tag(phrase):\n    http_remove = re.sub(r\"http\\S+\", \"\",phrase)\n    html_remove = BeautifulSoup(http_remove, 'lxml').get_text()\n    return html_remove","fee528e2":"import re\n#remove words with numbers python: https:\/\/stackoverflow.com\/a\/18082370\/4084039\n#remove spacial character: https:\/\/stackoverflow.com\/a\/5843547\/4084039\ndef decontracted(phrase):\n    # specific\n    phrase = re.sub(r\"won't\", \"will not\", phrase)\n    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n\n    # general\n    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n    return phrase","07011bf8":"# https:\/\/gist.github.com\/sebleier\/554280\n# we are removing the words from the stop words list: 'no', 'nor', 'not'\n# <br \/><br \/> ==> after the above steps, we are getting \"br br\"\n# we are including them into stop words list\n# instead of <br \/> if we have <br\/> these tags would have revmoved in the 1st step\nstopwords= set(['br', 'the', 'i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\",\\\n            \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', \\\n            'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their',\\\n            'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', \\\n            'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', \\\n            'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', \\\n            'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after',\\\n            'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further',\\\n            'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more',\\\n            'most', 'other', 'some', 'such', 'only', 'own', 'same', 'so', 'than', 'too', 'very', \\\n            's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', \\\n            've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn',\\\n            \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn',\\\n            \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", \\\n            'won', \"won't\", 'wouldn', \"wouldn't\"])","1f43e636":"processed_text = []\nfor i in tqdm(data[\"headline_text\"].values):\n    sentance = html_tag(i)\n    sentance = decontracted(sentance)\n    sentance = re.sub(\"\\S*\\d\\S*\", \"\", sentance)\n    sentance = re.sub('[^A-Za-z]+', ' ', sentance)\n    sentance = \" \".join(i.lower() for i in sentance.split() if i.lower() not in stopwords)\n    processed_text.append(sentance)","70ff4ed2":"data[\"Clean_text\"] = processed_text\ndata.head()","2f9954d6":"def word_cloud(cluster_num):\n    sentance = []\n    num = cluster_num\n    sent = final_data[\"Clean_text\"][final_data[\"labels\"]==num]\n    for i in sent:\n        sentance.append(i)\n    sentance = ''.join(sentance)\n    wordcloud = WordCloud(background_color=\"white\").generate(sentance)\n    print(f\"Cluster Number: {num}\")\n    plt.figure(figsize=(9,6))\n    plt.imshow(wordcloud)\n    plt.axis(\"off\")\n    plt.show()","d4ed810b":"from wordcloud import WordCloud\nfrom sklearn.cluster import KMeans\nfinal_data = data[0:100000]\nbow = CountVectorizer(ngram_range=(1,2))\nbow_vector = bow.fit_transform(final_data[\"Clean_text\"])\n\nclusters = [2,3,4,5,6,7,8,9]\ninertia = []\nfor i in tqdm(clusters):\n    k_mean= KMeans(n_clusters=i,n_init=10)\n    k_mean.fit(bow_vector)\n    inertia.append(k_mean.inertia_)","44eef034":"plt.figure(figsize=(9,6))\nsns.set_style(style=\"whitegrid\")\nsns.lineplot(clusters,inertia)\nplt.xlabel(\"No of clusters\",fontsize=12)\nplt.ylabel(\"Loss\",fontsize=12)\nplt.title(\"Error plot for various no of clusters\",fontsize=14)\nplt.show()","c954e721":"k_mean = KMeans(n_clusters=6,n_init=10)\nk_mean.fit(bow_vector)\nfinal_data[\"labels\"] = k_mean.labels_","ad948d1d":"def NWordCloud(n_clusters):\n    print(\"Number of clusters: {}\".format(n_clusters))\n    \n    plt.figure(figsize=(15,15))\n    \n    rows = int((n_clusters\/2)+1) if type(n_clusters\/2)==float else n_clusters\/2\n    for i in range(0,n_clusters):\n        sentance = []\n        num = i\n        sent = final_data[\"Clean_text\"][final_data[\"labels\"]==num]\n        for j in sent:\n            sentance.append(j)\n        sentance = ''.join(sentance)\n        wordcloud = WordCloud(background_color=\"white\").generate(sentance)\n        plt.subplot(rows, 2, i+1)\n        plt.imshow(wordcloud)\n    plt.axis(\"off\")\n    plt.show()","db0ef62f":"NWordCloud(6)","57a51a7c":"tfidf = TfidfVectorizer()\ntfidf_vector = tfidf.fit_transform(final_data[\"Clean_text\"])\n\nclusters = [2,3,4,5,6,7,8,9]\ninertia = []\nfor i in tqdm(clusters):\n    k_mean= KMeans(n_clusters=i,n_init=10)\n    k_mean.fit(tfidf_vector)\n    inertia.append(k_mean.inertia_)","bd5c9394":"plt.figure(figsize=(9,6))\nsns.set_style(style=\"whitegrid\")\nsns.lineplot(clusters,inertia)\nplt.xlabel(\"No of clusters\",fontsize=12)\nplt.ylabel(\"Loss\",fontsize=12)\nplt.title(\"Error plot for various no of clusters\",fontsize=14)\nplt.show()","80eb9a3e":"final_data.drop(columns='labels',axis=1,inplace=True)\nk_mean = KMeans(n_clusters=8,n_init=10)\nk_mean.fit(tfidf_vector)\nfinal_data[\"labels\"] = k_mean.labels_\n\nunique_labels = list(final_data[\"labels\"].unique())\nunique_labels.sort()","30d4a834":"NWordCloud(8)","3882e35c":"list_of_sentance=[]\nfor sentance in tqdm(final_data[\"Clean_text\"]):\n    list_of_sentance.append(sentance.split())\nx = final_data[\"Clean_text\"]\nprint(f\"Shape of X Train : {x.shape}\")\nw2v_model=Word2Vec(list_of_sentance,min_count=5,size=50, workers=4)\nw2v_words = list(w2v_model.wv.vocab)\nw2v_vector = []\nfor sent in tqdm(list_of_sentance):\n    word_count = 0\n    word_vector = np.zeros(50)\n    for words in sent:\n        if words in w2v_words:\n            word_count +=1 \n            each_word_vect = w2v_model.wv[words]\n            word_vector += each_word_vect\n    if word_count != 0: \n        word_vector \/= word_count\n    w2v_vector.append(word_vector)\nprint(f\"Length of w2v_vector: {len(w2v_vector)}\")\n\nclusters = [2,3,4,5,6,7,8,9]\ninertia = []\nfor i in tqdm(clusters):\n    k_mean= KMeans(n_clusters=i,n_init=10)\n    k_mean.fit(w2v_vector)\n    inertia.append(k_mean.inertia_)","4596c568":"plt.figure(figsize=(9,6))\nsns.set_style(style=\"whitegrid\")\nsns.lineplot(clusters,inertia)\nplt.xlabel(\"No of clusters\",fontsize=12)\nplt.ylabel(\"Loss\",fontsize=12)\nplt.title(\"Error plot for various no of clusters\",fontsize=14)\nplt.show()","bb218dff":"final_data.drop(columns='labels',axis=1,inplace=True)\nk_mean = KMeans(n_clusters=5,n_init=10)\nk_mean.fit(w2v_vector)\nfinal_data[\"labels\"] = k_mean.labels_\nunique_labels = list(final_data[\"labels\"].unique())\nunique_labels.sort()","c205dbcb":"NWordCloud(5)","199db6e6":"list_of_sentance=[]\nfor sentance in tqdm(final_data[\"Clean_text\"]):\n    list_of_sentance.append(sentance.split())\nx = final_data[\"Clean_text\"]\nprint(f\"Shape of final_data: {x.shape}\")\n\nmodel = TfidfVectorizer()\nmodel.fit(final_data[\"Clean_text\"])\ndictionary = dict(zip(model.get_feature_names(), list(model.idf_)))\ntfidf_feat = model.get_feature_names() \nw2v_model=Word2Vec(list_of_sentance,min_count=5,size=50, workers=4)\nw2v_words = list(w2v_model.wv.vocab)\n\n\ntfidf_w2v_vector = [];\nfor sent in tqdm(list_of_sentance): \n    sent_vec = np.zeros(50)\n    weight_sum =0;\n    for word in sent:\n        if word in w2v_words and word in tfidf_feat:\n            vec = w2v_model.wv[word]\n            tf_idf = dictionary[word]*(sent.count(word)\/len(sent))\n            sent_vec += (vec * tf_idf)\n            weight_sum += tf_idf\n    if weight_sum != 0:\n        sent_vec \/= weight_sum\n    tfidf_w2v_vector.append(sent_vec)\nprint(f\"Length of tfidf_w2v_vector {len(tfidf_w2v_vector)}\")\n\nclusters = [2,3,4,5,6,7,8,9]\ninertia = []\nfor i in tqdm(clusters):\n    k_mean= KMeans(n_clusters=i,n_init=10)\n    k_mean.fit(tfidf_w2v_vector)\n    inertia.append(k_mean.inertia_)","677524cf":"plt.figure(figsize=(9,6))\nsns.set_style(style=\"whitegrid\")\nsns.lineplot(clusters,inertia)\nplt.xlabel(\"No of clusters\",fontsize=12)\nplt.ylabel(\"Loss\",fontsize=12)\nplt.title(\"Error plot for various no of clusters\",fontsize=14)\nplt.show()","5004ae68":"k_mean = KMeans(n_clusters=5,n_init=10)\nk_mean.fit(tfidf_w2v_vector)\nfinal_data[\"labels\"] = k_mean.labels_\nunique_labels = list(final_data[\"labels\"].unique())\nunique_labels.sort()","f4174193":"NWordCloud(5)","00aef8e8":"# A Million News Headlines","050e588a":"**This** contains data of news headlines published over a period of 15 years.\n\nSourced from the reputable Australian news source ABC (Australian Broadcasting Corp.)\n\nAgency Site: http:\/\/www.abc.net.au\/","7b68a2f4":"## Data Import","fad11a8b":"## Data Description\n- Format: CSV ; Single File\n\n- publish_date: Date of publishing for the article in yyyyMMdd format\n- headline_text: Text of the headline in Ascii , English , lowercase\n- Start Date: 2003-02-19 End Date: 2017-12-31\n\nTotal Records: 1,103,663\n\nFeed-Code: w3-event-abcaus; Si.gh.rank: SND","48db56be":"## Average Word2Vector","ee1f059e":"## TFIDF Weighted V2W","49c3fdce":"## Bag-Of-Word (BOW)","9ce44c55":"In information retrieval, tf\u2013idf or TFIDF, short for term frequency\u2013inverse document frequency, is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus. It is often used as a weighting factor in searches of information retrieval, text mining, and user modeling. The tf-idf value increases proportionally to the number of times a word appears in the document and is offset by the frequency of the word in the corpus, which helps to adjust for the fact that some words appear more frequently in general. Nowadays, tf-idf is one of the most popular term-weighting schemes; 83% of text-based recommender systems in the domain of digital libraries use tf-idf.","3c3de885":"## Import Module","48361f87":"# Word-Cloud","0d2f6e94":"## TFIDF","2ca51bea":"## Data Processing"}}