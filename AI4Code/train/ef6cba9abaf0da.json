{"cell_type":{"e094d379":"code","b7bb51de":"code","2729a730":"code","606fedf4":"code","10211137":"code","5c990427":"code","713b5253":"code","3243cb2f":"code","d1b95738":"code","1f09db6c":"code","f39f5a4b":"code","dd50037e":"code","06cf5e97":"code","1594e7c4":"code","439b5953":"code","2c239122":"code","dd5bbff1":"code","f684fbcf":"code","a90dbde6":"code","5ae3e6af":"code","9a616d8f":"code","5cc3059a":"code","aa2ca0da":"code","18d34171":"code","ef44d037":"code","e8cc2d95":"code","34684fa4":"code","bd77861a":"code","5c059249":"code","620fb3a8":"markdown","637974da":"markdown","356cff19":"markdown","bd23b905":"markdown","b2bdb875":"markdown","b84f193f":"markdown","09f27a86":"markdown","dee5df8d":"markdown","e3459a2e":"markdown","701d449b":"markdown","fe61e72c":"markdown","4fb08ea4":"markdown","355721a3":"markdown","dd5b9eba":"markdown","2d097dba":"markdown","6203a4be":"markdown","78013220":"markdown","fa7ea643":"markdown"},"source":{"e094d379":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","b7bb51de":"import operator\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndf = pd.read_csv(\"\/kaggle\/input\/red-wine-quality-cortez-et-al-2009\/winequality-red.csv\")","2729a730":"df.head()","606fedf4":"df.info()","10211137":"df.describe()","5c990427":"sns.pairplot(df)","713b5253":"sns.distplot(df['pH'])","3243cb2f":"from sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_recall_fscore_support\n\nX = df.iloc[:, 0:11].drop('pH', axis=1).values.reshape(1599, 1, 10)\nX = X[:, :, 0]\ny = df['pH'].values.reshape(df['pH'].size, 1)\n\n# X_train, X_test, y_train, y_test = train_test_split(X , y, random_state = 101, test_size = 0.2)\nX.shape","d1b95738":"degree = 1\n\n\npolf = PolynomialFeatures(degree = degree)\nX_poly = polf.fit_transform(X)\n\nlm = LinearRegression()\nlm.fit(X_poly, y)\npred = lm.predict(X_poly)\n\nplt.figure(figsize=(10, 7))\nplt.scatter(X, y, s=10)\n# sort the values of x before line plot\nsort_axis = operator.itemgetter(0)\nsorted_zip = sorted(zip(X, pred), key=sort_axis)\nX_, pred_ = zip(*sorted_zip)\nplt.plot(X_, pred_, color='m')\nplt.show()","1f09db6c":"degree = 2\n\n\npolf = PolynomialFeatures(degree = degree)\nX_poly = polf.fit_transform(X)\nX_ = X\nlm = LinearRegression()\nlm.fit(X_poly, y)\npred = lm.predict(X_poly)\n\nplt.figure(figsize=(10, 7))\nplt.scatter(X, y, s=10)\n# sort the values of x before line plot\nsort_axis = operator.itemgetter(0)\nsorted_zip = sorted(zip(X_, pred), key=sort_axis)\nX_, pred_ = zip(*sorted_zip)\nplt.plot(X_, pred_, color='m')\nplt.show()","f39f5a4b":"degree = int(input(\"Enter the degree: \"))\n\n\npolf = PolynomialFeatures(degree = degree)\nX_poly = polf.fit_transform(X)\nX_ = X\nlm = LinearRegression()\nlm.fit(X_poly, y)\npred = lm.predict(X_poly)\n\nplt.figure(figsize=(10, 7))\nplt.scatter(X, y, s=10)\n# sort the values of x before line plot\nsort_axis = operator.itemgetter(0)\nsorted_zip = sorted(zip(X_, pred), key=sort_axis)\nX_, pred_ = zip(*sorted_zip)\nplt.plot(X_, pred_, color='m')\nplt.show()","dd50037e":"# Testing the parameters\nprint('MAE:', metrics.mean_absolute_error(y, pred))\nprint('MSE:', metrics.mean_squared_error(y, pred))\nprint('RMSE:', np.sqrt(metrics.mean_squared_error(y, pred)))\nprint('R2:', metrics.r2_score(y, pred))","06cf5e97":"from sklearn import metrics\nfrom prettytable import PrettyTable\nfor i in range(1, 5):\n    polf = PolynomialFeatures(degree = i)\n    X_poly = polf.fit_transform(X)\n    lm = LinearRegression()\n    lm.fit(X_poly, y)\n    pred = lm.predict(X_poly)\n    tabel_parameter = PrettyTable(['Parameter', 'Score'])\n    tabel_parameter.add_row(['Polynomial Degree', i])\n    tabel_parameter.add_row(['MAE', '{:.10}'.format(metrics.mean_absolute_error(y, pred))])\n    tabel_parameter.add_row(['MSE', '{:.10}'.format(metrics.mean_squared_error(y, pred))])\n    tabel_parameter.add_row(['RMSE','{:.10}'.format(np.sqrt(metrics.mean_squared_error(y, pred)))])\n    tabel_parameter.add_row(['R^2', '{:.10}'.format(metrics.r2_score(y, pred))])\n    print(tabel_parameter)","1594e7c4":"from sklearn.cluster import KMeans\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.preprocessing import MinMaxScaler","439b5953":"X = df.iloc[:, 0:11]\ny = df['pH']\ncols = X.columns\n# MinMaxScaler untuk Feature Scaling\nms = MinMaxScaler()\nX = ms.fit_transform(X)\nX = pd.DataFrame(X, columns=[cols])\nX.shape","2c239122":"X.head()","dd5bbff1":"kmeans = KMeans(n_clusters=1)\nkmeans.fit(X)\npred = kmeans.predict(X)","f684fbcf":"kmeans.cluster_centers_","a90dbde6":"kmeans.labels_","5ae3e6af":"print(kmeans.inertia_)","9a616d8f":"cs = []\nfor i in range(1, 11):\n    kmeans = KMeans(n_clusters = i, init = 'k-means++')\n    kmeans.fit(X)\n    cs.append(kmeans.inertia_)\nplt.plot(range(1, 11), cs, color='blue', linestyle='dashed', marker='o',\n         markerfacecolor='red', markersize=10)\nplt.title('The Elbow Method')\nplt.xlabel('Number of clusters')\nplt.ylabel('CS')\nplt.show()","5cc3059a":"from yellowbrick.cluster import silhouette_visualizer\nfrom sklearn.metrics import silhouette_score\nn_clusters = 2\nclusterer = KMeans(n_clusters=n_clusters, random_state=10)\nsilhouette_visualizer(KMeans(n_clusters=n_clusters, random_state=42), X, colors='yellowbrick')\ncluster_labels = clusterer.fit_predict(X)\nsilhouette_avg = silhouette_score(X, cluster_labels)\nprint(\"For n_clusters =\", n_clusters, \"The average silhouette_score is :\", silhouette_avg)","aa2ca0da":"# Input k\nk = int(input(\"Enter the k: \"))\nkmeans = KMeans(n_clusters = k)\nkmeans.fit(X)\ninertia = kmeans.inertia_\nprint(\"Result inertia from k = %d is: %f\" % (k, inertia))\n\n# Optimal k\nk = 2\nkmeans = KMeans(n_clusters=k)\nkmeans.fit(X)\npred = kmeans.predict(X)\ninertia = kmeans.inertia_\nprint(\"Result inertia from k = %d is: %f\" % (k, inertia))","18d34171":"# Input k\nk = int(input(\"Enter the k: \"))\nkmeans = KMeans(n_clusters = k)\nkmeans.fit(X)","ef44d037":"kmeans.labels_","e8cc2d95":"labels = kmeans.labels_\n# check how many of the samples were correctly labeled\ncorrect_labels = sum(y == labels)\n\nprint(\"Result: %d out of %d samples were correctly labeled.\" % (correct_labels, y.size))\nprint('Accuracy score: {0:0.2f}'. format(correct_labels\/float(y.size)))","34684fa4":"# Select Alcohol and pH attribute\nX = df.iloc[:, [10,8]]\ny = df['pH']\n# cols = X.columns\n# MinMaxScaler untuk scaling data X\nms = MinMaxScaler()\nX = ms.fit_transform(X)\nX = pd.DataFrame(X)","bd77861a":"kmeans = KMeans(n_clusters=1)\nkmeans.fit(X)\npred = kmeans.predict(X)\nplt.figure(figsize=(10, 7))\nplt.scatter(X.iloc[:, 0], X.iloc[:, 1], c=pred, s=30, cmap=\"viridis\")\ncenters = kmeans.cluster_centers_ # Coordinates of cluster centers.\nplt.scatter(centers[:, 0], centers[:, 1], c='black', s=200, alpha=0.5)\nplt.show()","5c059249":"kmeans = KMeans(n_clusters=2)\nkmeans.fit(X)\npred = kmeans.predict(X)\nplt.figure(figsize=(10, 7))\nplt.scatter(X.iloc[:, 0], X.iloc[:, 1], c=pred, s=30, cmap='viridis')\ncenters = kmeans.cluster_centers_ # Coordinates of cluster centers.\nplt.scatter(centers[:, 0], centers[:, 1], c='black', s=200, alpha=0.5)\nplt.show()","620fb3a8":"# Evaluation model Clustering\n\nFor checking the k was better and check the accuracy if approaching 1\n\nYou can input and conclude the k you choose was better","637974da":"# Linear Regression vs. Polynomial Regression","356cff19":"# Linear Regression\n","bd23b905":"**Start initialize with k = 1**","b2bdb875":"Tabel Parameter","b84f193f":"# Finding Balance k with Silhouette Method","09f27a86":"Dilihat dari grafik Elbow Curve, titik awal yang membentuk siku di k = 2, tetapi bisa jadi k yang terbaik adalah k = 3","dee5df8d":"This degree start from 2, and see the plot here","e3459a2e":"Feature Scaling","701d449b":"Berdasarkan plot Silhouette, jumlah rata - rata silhouette yang paling tinggi adalah jumlah cluster = 2, yang artinya k = 2 adalah k yang paling optimal","fe61e72c":"# Data Preprocessing","4fb08ea4":"# **Clustering KMeans**\n","355721a3":"# Display a plot Cluster","dd5b9eba":"# Finding k with Elbow Method\n\nThe elbow method runs k-means clustering on the dataset for a range of values for k (say from 1-10) and then for each value of k computes an average score for all clusters.","2d097dba":"# Polynomial Regression","6203a4be":"# Comparing Inertia score from 2 different clusters\n\nYou can input and conclude the k you choose was better","78013220":"How about the degree is more than 2?\n\n**You can input degree here** ","fa7ea643":"# **Evaluation Model Regression parameters**\n\nSee the score from our Regression model"}}