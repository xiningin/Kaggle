{"cell_type":{"264da56b":"code","f9671af7":"code","cff0449a":"code","628e7641":"code","86d89bf6":"code","ac69be0b":"code","9fe6c9a7":"code","7df1b6cc":"code","b9556031":"code","fa11b616":"code","186776cf":"code","ed1ac806":"code","836bca2d":"code","84a42f45":"code","215ea2d8":"code","88a7d8cd":"code","3c0358f4":"code","b30b4ee3":"code","cc1f4071":"code","d032490f":"code","09775ea8":"code","0dc9495d":"code","271083b0":"code","74015573":"code","d77a64e2":"code","7ede3b45":"code","92339413":"code","07cf53d4":"code","29840656":"code","aeec3d92":"code","4a6cce70":"code","b8dff093":"code","80fc7848":"code","c23d8226":"code","6b916e64":"code","15a49bbe":"code","49dd42cc":"code","66a55991":"code","99a35253":"code","7090e326":"code","9d2dc8be":"code","391ac736":"code","1e70bbbf":"code","f56e454e":"code","29b78c02":"code","fbe448bb":"code","7b0b2466":"code","8d991292":"code","42759fbe":"code","1d2e72c3":"code","83f366dc":"code","d1eb0834":"code","5e785480":"code","5e56e3d1":"code","1ef07ca8":"code","9f37f3d3":"markdown","32c3bf49":"markdown","d18659f4":"markdown","9a29b644":"markdown","f293a304":"markdown","e92c922d":"markdown","ffc7f3d5":"markdown","d9776c66":"markdown","b6bb015e":"markdown","ff421fbc":"markdown","0b9bf882":"markdown","53b06cd9":"markdown","5ca052d7":"markdown","d7f42217":"markdown","446444cf":"markdown","5e9cccdb":"markdown","ea9ae0a1":"markdown","ac493272":"markdown","65c46e57":"markdown","93e6ff92":"markdown","80d65622":"markdown","4c5775cf":"markdown","dffc11cc":"markdown","5c55c49d":"markdown","0e895beb":"markdown","c5203b49":"markdown","ab3a7c67":"markdown","9b9843ec":"markdown","130dc431":"markdown","63473434":"markdown","7471ea77":"markdown","e89f3a9b":"markdown","2bc9d5b4":"markdown","e1204b66":"markdown","61ff3290":"markdown","7239ef78":"markdown","2707bb9c":"markdown","50f90e55":"markdown","84ad6dfd":"markdown","31f9101c":"markdown","ee70d11e":"markdown","7c2691a8":"markdown","070d8940":"markdown","a694d759":"markdown"},"source":{"264da56b":"# linear algebra\nimport numpy as np\n# data processing\nimport pandas as pd\n\n# data visualization\nimport seaborn as sns\n%matplotlib inline\nfrom matplotlib import pyplot as plt\nfrom matplotlib import style\n\n# Algortithmic packages\nfrom sklearn import linear_model\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC,LinearSVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\n\nimport os\nimport warnings\nwarnings.filterwarnings('ignore')","f9671af7":"# from google.colab import drive\n# drive.mount('\/content\/drive')","cff0449a":"print(os.listdir(\"..\/input\/titanic\"))","628e7641":"train_df = pd.read_csv('..\/input\/titanic\/train.csv')\ntest_df = pd.read_csv('..\/input\/titanic\/test.csv')\n\n#printing first 5 rows of data\ntrain_df.head()","86d89bf6":"train_df.info()","ac69be0b":"# let's display the columns names\ntrain_df.columns","9fe6c9a7":"train_df.describe()","7df1b6cc":"test_df.iloc[152]","b9556031":"train_df.head()","fa11b616":"# count of null values in each column\ncount  = train_df.isnull().sum().sort_values(ascending = False)\n\n# percentage of Null Values in each column\npercent = train_df.isnull().sum()\/train_df.count()*100\n\n# rounding and arranging the percentage\npercent = round(percent,2).sort_values(ascending = False)\n\n# concatenating count and percentage into one\nmissing_data = pd.concat([count,percent], axis = 1)\n\n# printing top 5 rows\nmissing_data.head()","186776cf":"survived = 'survided'\nnot_survived = 'not_survived'\nfig, axes = plt.subplots(nrows = 1 , ncols = 2 , figsize = (18,8))\nwomen = train_df[train_df['Sex'] == 'female']\nmen = train_df[train_df['Sex'] == 'male']\n\nax = sns.distplot(women[women['Survived']==1].Age.dropna(), bins=8, label = survived , ax=axes[0], kde = False)\nax = sns.distplot(women[women['Survived'] == 0].Age.dropna(), bins = 40, label=not_survived, ax = axes[0], kde =False)\nax.set_title('Female')\n\nax = sns.distplot(men[men['Survived']==1].Age.dropna(), bins=8, label = survived , ax=axes[1], kde = False)\nax = sns.distplot(men[men['Survived'] == 0].Age.dropna(), bins = 40, label=not_survived, ax = axes[1], kde =False)\n_ = ax.set_title('Male')\nax.legend()\nplt.show()","ed1ac806":"facetgrid = sns.FacetGrid(train_df , row = 'Embarked', height = 4.5 , aspect =1.8)\nfacetgrid.map(sns.pointplot, 'Pclass','Survived', 'Sex', order=None, hue_order=None)\nfacetgrid.add_legend()\nplt.show()","836bca2d":"sns.barplot(x = 'Pclass', y = 'Survived' , data = train_df)\nplt.show()","84a42f45":"grid = sns.FacetGrid(train_df, row = 'Pclass', col='Survived', hue_order=None, height = 3, aspect=2)\ngrid.map(plt.hist, 'Age', alpha=0.7, bins = 20)\nplt.show()\n","215ea2d8":"data = [train_df , test_df]\nfor dataset in data:\n  dataset['Relatives']=dataset['Parch']+dataset['SibSp']\n  dataset.loc[dataset['Relatives']>0,'Alone']=0\n  dataset.loc[dataset['Relatives']==0,'Alone']=1\n  dataset['Alone']=dataset['Alone'].astype(int)","88a7d8cd":"train_df.head()","3c0358f4":"train_df.Alone.value_counts()","b30b4ee3":"plt.figure(figsize=(16,7))\nsns.pointplot(x='Relatives', y = 'Survived', data= train_df )\nplt.show()","cc1f4071":"train_df = train_df.drop(['PassengerId'], axis = 1)\ntrain_df.head()","d032490f":"import re\n\ndeck =  {'A':1  , 'B': 2 , 'C': 3, 'D':4 ,'E' : 5 , 'F':6 , 'G':7 , 'U':8}\ndata = [train_df , test_df]\nfor dataset in data:\n  dataset['Cabin']=dataset['Cabin'].fillna('U0')\n  dataset['Deck']=dataset['Cabin'].map(lambda x : re.compile(\"([a-zA-Z]+)\").search(x).group())\n  dataset['Deck']=dataset['Deck'].map(deck)\n  dataset['Deck']=dataset['Deck'].fillna(0)\n  dataset['Deck']=dataset['Deck'].astype(int)","09775ea8":"train_df.Deck.value_counts()","0dc9495d":"# Same code as above, Regular Expression Simplified\n\nimport re\n\ndeck =  {'A':1  , 'B': 2 , 'C': 3, 'D':4 ,'E' : 5 , 'F':6 , 'G':7 , 'U':8}\ndata = [train_df , test_df]\n\nfor dataset in data:\n  dataset['Cabin']=dataset['Cabin'].fillna('U0')\n  dataset['Deck']=dataset['Cabin'].map(lambda x : x[0])\n  dataset['Deck']=dataset['Deck'].map(deck)\n  dataset['Deck']=dataset['Deck'].fillna(0)\n  dataset['Deck']=dataset['Deck'].astype(int)\n\n  \n  \ntrain_df.Deck.value_counts()","271083b0":"data = [train_df,test_df]\nfor dataset in data:\n  dataset=dataset.drop(['Cabin'], axis = 1)","74015573":"train_df=train_df.drop('Cabin', axis = 1)\ntest_df=test_df.drop('Cabin', axis = 1)","d77a64e2":"data = [train_df , test_df]\nmean = train_df['Age'].mean()\nstd  = test_df['Age'].std()\n\n\nfor dataset in data:\n  count_of_null = dataset['Age'].isnull().sum()\n  \n  rand_age = np.random.randint(mean-std,mean+std, size = count_of_null)\n  \n  age_slice = dataset['Age'].copy()\n  age_slice[np.isnan(age_slice)]= rand_age\n  \n  dataset['Age']=age_slice\n  dataset['Age']=dataset['Age'].astype(int)","7ede3b45":"train_df['Embarked'].describe()","92339413":"train_df['Embarked'] = train_df['Embarked'].fillna('S')\ntest_df['Embarked'] = test_df['Embarked'].fillna('S')","07cf53d4":"train_df.info()","29840656":"test_df.info()","aeec3d92":"train_df.Name.head()","4a6cce70":"data = [train_df,test_df]\n\nfor dataset in data:\n  dataset['Title'] = dataset.Name.str.extract('([A-Za-z]+)\\.', expand = False)","b8dff093":"train_df.Title.value_counts()","80fc7848":"data = [train_df,test_df]\n\nfor dataset in data:\n  dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess','Capt', 'Col','Don','Dr',\\\n                                              'Major','Rev','Sir','Jonkheer','Dona'], 'Rare')\n  dataset['Title'] = dataset['Title'].replace('Mlle' , 'Miss')\n  dataset['Title'] = dataset['Title'].replace('Ms' , 'Miss')\n  dataset['Title'] = dataset['Title'].replace('Mme' , 'Mrs')\n  ","c23d8226":"titles={'Mr':1,'Miss':2,'Mrs':3, 'Master':4, 'Rare':5}\n\nfor dataset in data:\n  dataset['Title']=dataset['Title'].map(titles)","6b916e64":"print(train_df.Title.isna().sum())\nprint(test_df.Title.isna().sum())","15a49bbe":"train_df = train_df.drop(['Name'], axis = 1)\ntest_df = test_df.drop(['Name'], axis = 1)","49dd42cc":"train_df.Sex.value_counts()","66a55991":"gender = {'male':0 , 'female':1}\ndata = [train_df,test_df]\n\nfor dataset in data:\n  dataset['Sex']=dataset['Sex'].map(gender)","99a35253":"train_df.Ticket.head()","7090e326":"data = [train_df,test_df]\nfor dataset in data:\n  dataset=dataset.drop('Ticket', axis = 1)","9d2dc8be":"train_df=train_df.drop('Ticket', axis = 1)\ntest_df=test_df.drop('Ticket', axis = 1)","391ac736":"ports = {'S':0,'C':1, 'Q':2}\ndata = [train_df,test_df]\n\nfor dataset in data:\n  dataset['Embarked']=dataset['Embarked'].map(ports)","1e70bbbf":"data = [train_df,test_df]\nfor dataset in data:\n  dataset['Age']=dataset['Age'].astype(int)\n  dataset.loc[dataset['Age']<=11, 'Age']=0\n  dataset.loc[(dataset['Age']>11) & (dataset['Age']<=18), 'Age']=1\n  dataset.loc[(dataset['Age']>18) & (dataset['Age']<=22), 'Age']=2\n  dataset.loc[(dataset['Age']>22) & (dataset['Age']<=27), 'Age']=3\n  dataset.loc[(dataset['Age']>27) & (dataset['Age']<=33), 'Age']=4\n  dataset.loc[(dataset['Age']>33) & (dataset['Age']<=40), 'Age']=5\n  dataset.loc[(dataset['Age']>40) & (dataset['Age']<=66), 'Age']=6\n  dataset.loc[(dataset['Age']>66), 'Age']=6","f56e454e":"train_df.Age.value_counts()","29b78c02":"train_df.head()","fbe448bb":"data = [train_df,test_df]\n\nfor dataset in data:\n  dataset.loc[dataset['Fare']<=7.91, 'Fare']=0\n  dataset.loc[(dataset['Fare']>7.91) & (dataset['Fare']<=14.454), 'Fare']=1\n  dataset.loc[(dataset['Fare']>14.454) & (dataset['Fare']<=31), 'Fare']=2\n  dataset.loc[(dataset['Fare']>31) & (dataset['Fare']<=99), 'Fare']=3\n  dataset.loc[(dataset['Fare']>99) & (dataset['Fare']<=250), 'Fare']=4\n  dataset.loc[(dataset['Fare']>250) , 'Fare']=5\n  dataset['Fare']=dataset['Fare'].fillna(0)\n  dataset['Fare']=dataset['Fare'].astype(int)\n\ntrain_df.Fare.value_counts()","7b0b2466":"# test_df[test_df.Fare.isna()==True]","8d991292":"data = [train_df, test_df]\n\n\nfor dataset in data:\n  dataset['age_class'] = dataset['Age']*dataset['Pclass']\n  \n  \ntrain_df.head()","42759fbe":"# train_df.head()","1d2e72c3":"X_train = train_df.drop('Survived', axis = 1)\ny_train = train_df['Survived']\n\nX_test = test_df.drop('PassengerId' ,  axis = 1)","83f366dc":"# creating model object\nsgd = linear_model.SGDClassifier(max_iter = 5, tol = None)\n\n# Fitting model on Data\nsgd.fit(X_train,y_train)\n\n#using model to predict\ny_pred = sgd.predict(X_test)\n\n# Storing prediction accuracy\nacc_sgd = round(sgd.score(X_train,y_train)*100,2)\nprint(acc_sgd)","d1eb0834":"logreg = LogisticRegression()\nlogreg.fit(X_train,y_train)\n\n\nacc_log=round(logreg.score(X_train,y_train)*100,2)\nprint(acc_log)","5e785480":"dict_model = {'sgd':linear_model.SGDClassifier(max_iter = 5, tol = None), \n             'log_reg':LogisticRegression(),\n             'decision_tree':DecisionTreeClassifier(),\n             'random_forest':RandomForestClassifier(n_estimators = 100),\n             'knn_classifier': KNeighborsClassifier(n_neighbors= 3),\n             'gaussian':GaussianNB(),\n             'perceptron':Perceptron(max_iter=5),\n             'linear_svc':LinearSVC()\n             }","5e56e3d1":"dict_accuracies={}\n\nfor name,classifier in dict_model.items():\n  dict_model[name].fit(X_train,y_train)\n  score = dict_model[name].score(X_train,y_train)\n  dict_accuracies[name]=round(score*100,2)\n  \n","1ef07ca8":"result_df=pd.DataFrame.from_dict(dict_accuracies, orient = 'index',columns = ['Score'])\nresult_df= result_df.sort_values(by = 'Score', ascending = False)\nresult_df","9f37f3d3":"Great 78.23 % accuracy with our first Model.\n\nNow Let's try the classical Logistic Regression Model","32c3bf49":"#### Missing Data in cabin :\n As a reminder, we have to deal with Cabin (687), Embarked (2) and Age (177). First I thought of dropping the 'Cabin' variable but then I found something interesting.\n\n`A cabin number looks like \u2018C123\u2019 and the letter refers to the deck.`\n \nTherefore we\u2019re going to extract these and create a new feature, that contains a persons deck. Afterwords we will convert the feature into a numeric variable. \n\nThe missing values will be converted to zero. In the picture below you can see the actual decks of the titanic, ranging from A to G.","d18659f4":"#### Missing data in Embarked:\nSince the Embarked feature has only 2 missing values, we will just fill these with the most common one.\n\n","9a29b644":"### Cross Validation Evaluation","f293a304":"#### Feature Engineering\n\n\nWe will now create categories within the following features:\n\nAge:\nNow we need to convert the 'age' feature. First we will convert it from float into integer. Then we will create the new 'AgeGroup\" variable, by categorizing every age into a group. Note that it is important to place attention on how you form these groups, since you don't want for example that 80% of your data falls into group 1.","e92c922d":"Let's Perform Fare binning Now:\n","ffc7f3d5":"Let's take a look at description of numerical Values in Dataset","d9776c66":"#### Embarked\nLet's convert Embark into a numerical Feature","b6bb015e":"The training set has 891 examples and 11 features along with the Target variable(survived). Datatypes for 2 of the features are floats, 5 are integers and 5 are objects. Below I have listed the features with a short description:\n\nsurvival:   Survival<br>\nPassengerId: Unique Id of a passenger.<br>\npclass:     Ticket class    <br>\nsex   :     Sex <br>\nAge   :     Age in years    \nsibsp :     # of siblings \/ spouses aboard the Titanic  <br>\nparch :     # of parents \/ children aboard the Titanic  <br>\nticket:     Ticket number   <br>\nfare  :     Passenger fare  <br>\ncabin :     Cabin number    <br>\nembarked:   Port of Embarkation<br>","ff421fbc":"Similarly for the fare , we do same approach, For the 'Fare' feature, we need to do the same as with the 'Age' feature. But it isn't that easy, because if we cut the range of the fare values into a few equally big categories, 80% of the values would fall into the first category. Fortunately, we can use sklearn \"qcut()\" function, that we can use to see, how we can form the categories.\n\nCurrent dataframe looks as follows\n","0b9bf882":"Above we can see that 38% out of the training-set survived the Titanic. We can also see that the passenger ages range from 0.4 to 80. On top of that we can already detect some features, that contain missing values, like the 'Age' feature. Also we can find outliers if any by visual observation of quartiles.\n\nLet's look at head of the data","53b06cd9":"we can now drop the cabin feature now","5ca052d7":"I am going to write a loop which will fit and predict accuracy for different classes of models\n\nand return a dataframe with respective accuracies","d7f42217":"Even the name has some meaning. We cannot afford miss information\n\nDropping name and","446444cf":"From the table above, we can note a few things. First of all, that we need to convert a lot of features into numeric ones later on, so that the machine learning algorithms can process them. Furthermore, we can see that the features have widely different ranges, that we will need to convert into roughly the same scale. We can also spot some more features, that contain missing values (NaN = not a number), that wee need to deal with.\n\nLet's take a more detailed look at what data is actually missing:","5e9cccdb":"#### Name: \nWe will use the Name feature to extract the Titles from the Name, so that we can build a new feature out of that.","ea9ae0a1":"#### 1. Age vs Sex","ac493272":"## Problem Statement\n\n### Introduction\n\nIn this kernel I will go through the whole process of creating a machine learning model on the famous Titanic dataset, which is used by many people all over the world. It provides information on the fate of passengers on the Titanic, summarized according to economic status (class), sex, age and survival. In this challenge, I will predict whether a passenger on the titanic would have been survived or not.\n\n### About Titanic \n\nRMS Titanic was a British passenger liner that sank in the North Atlantic Ocean in the early morning hours of 15 April 1912, after it collided with an iceberg during its maiden voyage from Southampton to New York City. There were an estimated 2,224 passengers and crew aboard the ship, and more than 1,500 died, making it one of the deadliest commercial peacetime maritime disasters in modern history. The RMS Titanic was the largest ship afloat at the time it entered service and was the second of three Olympic-class ocean liners operated by the White Star Line. The Titanic was built by the Harland and Wolff shipyard in Belfast. Thomas Andrews, her architect, died in the disaster.\n\n### About the Data Set\n\nWe have data of all passengers and the problem statement is to predict the survival  of a passenger given his current data inputs. The complete data can be downloaded here - www.kaggle.com\/c\/titanic\n\n\n### Titanic - The sad story\n- Exploratory Data Analysis I\n- Exploratory Data Analysis II\n- This is your last visited course section.Resume Course \n- EDA - Plotting\n- Preprocessing\n- Preprocessing II\n- Feature Engineering\n- Modelling\n- Advanced Modelling\n- Final Model\n- Model Evaluation\n- Feedback\n\n","65c46e57":"Let's Create a new Feature A combination of Age and Class","93e6ff92":"Assuming, Being alone and survival rate has high dependency. let's Create a new binary feature - 'Alone' ","80d65622":"Since the Ticket attribute has 681 unique tickets, it will be a bit tricky to convert them into useful information. So we will drop it from the dataset.","4c5775cf":"### Basic Pre-Processing","dffc11cc":"#### 2. Embarked, Pclass and Sex:","5c55c49d":"The above grouping can also be achieved in single line with Pd.Cut. You can give a try.\n\nNow Let's see the Groups.","0e895beb":"### Hyper Parameter Tuning","c5203b49":"Now let's train our first Model: A Stochastic Gradient Descent Classifier","ab3a7c67":"Let's divide our data into Feature or Preictor Variables i.e X and Label or Predicted variable i.e y","9b9843ec":"Let's check info of the dataset to get an idea of how it's looking","130dc431":"#### 4. No_of_Relatives vs Survived","63473434":"All the missing values in our dataset has been dealt.\nLet's check info of train_df\n","7471ea77":"If you notice, Cabin has the highest percentage. The Embarked feature has only 2 missing values, which can easily be filled. It will be much more tricky, to deal with the 'Age' feature, which has 177 missing values. The 'Cabin' feature needs further investigation, but it looks like that we might want to drop it from the dataset, since 77 % of it are missing.\n\n","e89f3a9b":"### Visualization\n\nTo check the relationship between survival vs age\/sex, let's plot some graphs","2bc9d5b4":"In first step,we will see if any features are irrelevant and drop them. \nAs you can see passenger ID is irrelevant.  I will drop 'PassengerId' from the train set, because it does not contribute to a persons survival probability. \n\nI will not drop it from the test set, since it is required there for the submission","e1204b66":"Let's Load the dataset","61ff3290":"You can see that men have a high probability of survival when they are between 18 and 30 years old, which is also a little bit true for women but not fully. For women the survival chances are higher between 14 and 40.\n\nFor men the probability of survival is very low between the age of 5 and 18, but that isn't true for women. Another thing to note is that infants also have a little bit higher probability of survival.\n\nSince there seem to be certain ages, which have increased odds of survival and because I want every feature to be roughly on the same scale, I will create age groups later on.","7239ef78":"Sex is categorical but we have to convert 'Sex' feature into numeric.","2707bb9c":"### Loading Data and Diagnostics\nLet's import all required packages. Each package will be explained whenever it is used","50f90e55":"#### Ticket & Fare","84ad6dfd":"### Evaluation Metrics","31f9101c":"## Modeling","ee70d11e":"Very interesting to note that survival rate slowly increases and then falls down","7c2691a8":"Here we see clearly, that Pclass is contributing to a persons chance of survival, especially if this person is in class 1.\n\n","070d8940":"#### 3. Let's create another plot below for distribution of P Class:\n","a694d759":"#### Missing data in Age\nNow we can tackle the issue with the age features missing values:  \n\nI will create an array of size n that contains random numbers, which are computed based on the mean age value in regards to the standard deviation where n is equal to number of missing Values."}}