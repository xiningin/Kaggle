{"cell_type":{"6f324855":"code","e34082ac":"code","33308661":"code","ba8342b6":"code","a41d85bc":"code","5c590178":"code","186e09ae":"code","5d94aad7":"code","aac5f514":"code","db8ff540":"code","e113e00b":"code","3dbd2f78":"code","2ba9d417":"code","ab34f247":"code","48dab8c5":"code","f1c0616c":"code","3729dcb8":"code","12d4e508":"code","9b646360":"code","de8c868d":"code","b608a2a2":"code","abbca262":"code","d8df2551":"code","40d5a03f":"code","09374f80":"code","c7004b54":"code","31cc45d8":"code","1d83ab2b":"code","12e142c3":"code","b3898475":"code","6d965f8f":"code","037098b6":"code","ed10a0b0":"code","dbb40810":"code","0d0c796a":"code","5a00ce6f":"code","094c2e9c":"code","adf3dd7e":"code","0be65480":"code","d01a9e9e":"code","5aab472d":"code","44b1948a":"code","65d654dd":"code","0325ddf2":"code","7e4386a0":"code","952455e1":"code","e1ee6afb":"code","d0c20080":"code","5cad1003":"code","95166364":"markdown","92e665e8":"markdown","1764606d":"markdown","33b0ad6c":"markdown","411df08a":"markdown","1b4d5ba8":"markdown","e23a3165":"markdown","42e2ceec":"markdown","f5b26b15":"markdown"},"source":{"6f324855":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","e34082ac":"#loading packages\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')\n","33308661":"#Reading data\ntrain = pd.read_csv(r'..\/input\/jobathon-analytics-vidhya-health-insurance\/Train.csv')\ntest = pd.read_csv(r'..\/input\/jobathon-analytics-vidhya-health-insurance\/Test.csv')","ba8342b6":"train.shape, test.shape","a41d85bc":"train.head()","5c590178":"train['source'] = 'train'\ntest['source'] = 'test'\ndata = pd.concat([train,test],ignore_index = True)","186e09ae":"data.head()","5d94aad7":"data.shape","aac5f514":"data.dtypes","db8ff540":"data.nunique()","e113e00b":"data.isnull().sum()","3dbd2f78":"data['Health Indicator'].value_counts()","2ba9d417":"data['Health Indicator'].replace(to_replace='X1', value='0', regex=True, inplace=True)\ndata['Health Indicator'].replace(to_replace='X2', value='1', regex=True, inplace=True)\ndata['Health Indicator'].replace(to_replace='X3', value='2', regex=True, inplace=True)\ndata['Health Indicator'].replace(to_replace='X4', value='3', regex=True, inplace=True)\ndata['Health Indicator'].replace(to_replace='X5', value='4', regex=True, inplace=True)\ndata['Health Indicator'].replace(to_replace='X6', value='5', regex=True, inplace=True)\ndata['Health Indicator'].replace(to_replace='X7', value='6', regex=True, inplace=True)\ndata['Health Indicator'].replace(to_replace='X8', value='7', regex=True, inplace=True)\ndata['Health Indicator'].replace(to_replace='X9', value='8', regex=True, inplace=True)","ab34f247":"data['Holding_Policy_Duration'].value_counts()","48dab8c5":"data['Holding_Policy_Duration'] = data['Holding_Policy_Duration'].replace('14+','14.0')","f1c0616c":"data['Is_Spouse'].value_counts()","3729dcb8":"data['Is_Spouse'] = data['Is_Spouse'].replace('Yes',1)\ndata['Is_Spouse'] = data['Is_Spouse'].replace('No',0)","12d4e508":"data.head()","9b646360":"def na_randomfill(series):\n    na_mask = pd.isnull(series)   # boolean mask for null values\n    n_null = na_mask.sum()        # number of nulls in the Series\n    \n    if n_null == 0:\n        return series             # if there are no nulls, no need to resample\n    \n    # Randomly sample the non-null values from our series\n    #  only sample this Series as many times as we have nulls \n    fill_values = series[~na_mask].sample(n=n_null, replace=True, random_state=0)\n\n    # This ensures our new values will replace NaNs in the correct locations\n    fill_values.index = series.index[na_mask]\n    \n    return series.fillna(fill_values) ","de8c868d":"data['Health Indicator'] = na_randomfill(data['Health Indicator'])\ndata['Holding_Policy_Duration'] = na_randomfill(data['Holding_Policy_Duration'])\ndata['Holding_Policy_Type'] = na_randomfill(data['Holding_Policy_Type'])","b608a2a2":"data.isnull().sum()","abbca262":"plt.figure(figsize=(20,7))\nplt.subplot(131)\nsns.countplot(hue='Response',x='Accomodation_Type',data=data)\n\nplt.subplot(132)\nsns.countplot(hue='Response',x='Is_Spouse',data=data)\n\nplt.subplot(133)\nsns.countplot(hue='Response',x='Reco_Insurance_Type',data=data)\n\nplt.show()","d8df2551":"sns.countplot(hue='Response',x='Holding_Policy_Type',data=data)\nplt.show()","40d5a03f":"plt.figure(figsize=(14,7))\nplt.subplot(121)\nsns.countplot(x='Is_Spouse',hue='Response',data=data)\n\nplt.subplot(122)\nsns.countplot(x='Reco_Insurance_Type',hue='Response',data=data)\n\nplt.show()","09374f80":"plt.figure(figsize=(14,7))\nplt.subplot(121)\nsns.countplot(x='Accomodation_Type',hue='Response',data=data)\n\nplt.subplot(122)\nsns.countplot(x='Health Indicator',hue='Response',data=data)\n\nplt.show()","c7004b54":"data['Health Indicator'].replace('7','5',inplace=True)\ndata['Health Indicator'].replace('8','5',inplace=True)\ndata['Health Indicator'].replace('6','5',inplace=True)","31cc45d8":"sns.countplot(x='Health Indicator',hue='Holding_Policy_Type',data=data)\nplt.show()","1d83ab2b":"sns.countplot(x='Health Indicator',hue='Is_Spouse',data=data)\nplt.show()","12e142c3":"g = sns.factorplot(\"City_Code\", data=data, aspect=1.5, kind=\"count\", color=\"y\")\ng.set_xticklabels(rotation=75)\nplt.show()","b3898475":"sns.distplot(data['Reco_Policy_Premium'])\nplt.show()","6d965f8f":"data['Reco_Policy_Premium_Log'] = np.log(data['Reco_Policy_Premium'])\nsns.distplot(data['Reco_Policy_Premium_Log'])\nplt.show()","037098b6":"data.dtypes","ed10a0b0":"#Import library:\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nvar_mod = ['City_Code','Health Indicator','Holding_Policy_Duration',]\nfor i in var_mod:\n    data[i] = le.fit_transform(data[i])","dbb40810":"#One Hot InCoding:\ndata = pd.get_dummies(data, columns=['Accomodation_Type','Reco_Insurance_Type'])","0d0c796a":"\ntrain = data.loc[data['source']==\"train\"]\ntest = data.loc[data['source']==\"test\"]\n\n#Drop unnecessary columns:\ntest.drop(['Response','source'],axis=1,inplace=True)\ntrain.drop('source',axis=1,inplace=True)\n","5a00ce6f":"train.shape, test.shape","094c2e9c":"X = train.drop('Response',axis=1)\ny = train['Response']","adf3dd7e":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X,y,stratify = train['Response'],test_size=0.3)","0be65480":"from sklearn.feature_selection import mutual_info_classif\n# determine the mutual information\nmutual_info = mutual_info_classif(X_train, y_train)\nmutual_info","d01a9e9e":"mutual_info = pd.Series(mutual_info)\nmutual_info.index = X_train.columns\nmutual_info.sort_values(ascending=False)","5aab472d":"#let's plot the ordered mutual_info values per feature\nmutual_info.sort_values(ascending=False).plot.bar(figsize=(14, 7))\n","44b1948a":"X_train.drop(['Reco_Insurance_Type_Joint','Holding_Policy_Type','Health Indicator','Is_Spouse','Upper_Age','Region_Code','City_Code','ID'],axis=1)\nX_test.drop(['Reco_Insurance_Type_Joint','Holding_Policy_Type','Health Indicator','Is_Spouse','Upper_Age','Region_Code','City_Code','ID'],axis=1)","65d654dd":"from lightgbm import LGBMClassifier\nlgb = LGBMClassifier(learning_rate = 0.01,\n                      max_depth=8,\n                     n_estimators=900,\n                     num_leaves=8)\nlgb.fit(X_train,y_train)","0325ddf2":"pred = lgb.predict_proba(X_test)[:, 1]","7e4386a0":"from sklearn.metrics import roc_auc_score\nroc_auc_score(y_test,pred)","952455e1":"from catboost import CatBoostClassifier\ncat = CatBoostClassifier(n_estimators=1500,\n                             learning_rate=0.01)\ncat.fit(X_train,y_train)","e1ee6afb":"pred = cat.predict_proba(X_test)[:, 1]","d0c20080":"from sklearn.metrics import roc_auc_score\nroc_auc_score(y_test,pred)","5cad1003":"test_pred = cat.predict_proba(test)[:, 1]\ntest['Response'] = test_pred\nsubmission_1 =test[['ID','Response']]\nsubmission_1.to_csv('cat.csv',index=False)","95166364":"Note that the 'Response' is the target variable and missing values are ones in the test set. So we need not worry about it. But we\u2019ll impute the missing values in 'Health Indicator', 'Holding_Policy_Duration' and 'Holding_Policy_Type' in the data cleaning section.","92e665e8":"Now the distribution looks much closer to normal and effect of extreme values has been significantly subsided. ","1764606d":"## data cleaning","33b0ad6c":"filling null values with randomly same column values","411df08a":"replacing values(6,7,8) with value(5).as you can see 6,7,8 didn't have much impact on response.","1b4d5ba8":"Its generally a good idea to combine both train and test data sets into one, perform feature engineering and then divide them later again. This saves the trouble of performing the same steps twice on test and train. Lets combine them into a dataframe \u2018data\u2019 with a \u2018source\u2019 column specifying where each observation belongs.","e23a3165":"we can see it shifted towards left, i.e.,the distribution is right skewed. so, let's take the log transfomation to make the distribution normal.","42e2ceec":"Let's check the distribution of Reco_Policy_Premium","f5b26b15":"Let's convert data back into train and test data sets. Its generally a good idea to export both of these as modified data sets so that they can be re-used for multiple sessions. This can be achieved using following code:"}}