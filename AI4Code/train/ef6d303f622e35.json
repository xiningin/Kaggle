{"cell_type":{"fbe0aa25":"code","77670a6f":"code","605f440e":"code","acab8ac8":"code","048a2436":"code","b45386ef":"code","41ce8b27":"code","b3b73583":"code","02163133":"code","0a372f62":"code","4e985e72":"code","c6df545f":"code","2f5bdaba":"code","218eccef":"code","dd92353b":"code","253358a2":"code","b3789625":"code","96ee1d42":"code","e977ae4a":"code","31410612":"code","b8f3af24":"code","ad57661c":"code","48fbdb66":"code","1233bc86":"code","f8b113ef":"code","cde90864":"code","a60f0171":"code","92dccf38":"code","8f57dd50":"code","3e5cef59":"code","bfd06754":"code","75057999":"code","21ebb93f":"code","26a1b893":"code","faa735d3":"code","6866e694":"code","6a8691bd":"code","f9ae4973":"code","9050382f":"code","4aec3747":"code","ccc4af65":"code","da819e70":"code","8bd123ba":"code","4101e594":"code","13853187":"code","4ea4b1bf":"code","7305bf50":"code","94418fd4":"code","60093d91":"code","c1914037":"code","28de86b1":"code","65ea74e2":"code","89d7f879":"code","32b19a2a":"code","c8a30b1d":"code","72ab9f22":"code","f0668263":"code","17dd5d46":"code","5c2908c0":"code","937627f8":"code","fa7528c2":"code","39d61d84":"code","fa1671f0":"code","8aef465d":"code","86adcb1b":"code","601c350b":"code","60ab64b0":"code","4804c97b":"code","087f8f02":"code","babc9e9a":"code","a1afbe0d":"code","56b42124":"code","12fd22e6":"code","13b9b4f9":"code","00ac6d4f":"code","0372309d":"code","e61b390f":"code","4c92db81":"code","3042cd59":"code","0d7cb9ed":"code","d5e60483":"code","3e8a5294":"code","59b3089e":"code","0e3224e7":"code","aeda94b2":"code","3fd66ff6":"code","1d6925f1":"code","d78451f2":"code","047822dd":"code","b946e0f2":"code","4e3101b9":"code","10b1fe51":"code","868259eb":"code","6d748159":"code","121587a8":"code","ba151f3a":"code","074d260f":"code","e2a784f9":"code","4b1b0be6":"code","84cb7bb7":"code","80b7d007":"code","07b7cd41":"code","3dddc224":"code","98c20f75":"code","3397e2d5":"code","3aa724c6":"code","01aead05":"code","48d04529":"code","24c0e404":"code","549f17bd":"code","a3413061":"code","f0f27742":"code","59373c4a":"markdown","037e49af":"markdown","54180a1a":"markdown","907ec2c0":"markdown","e84bcf5c":"markdown","a3655a23":"markdown","3f848768":"markdown","4375331d":"markdown","a8e943e6":"markdown","d60ab466":"markdown","4f2e3132":"markdown","4fb2a213":"markdown","97c5c158":"markdown","b505b9db":"markdown","e3389c7f":"markdown","7870740c":"markdown","d3e83372":"markdown","5543c25b":"markdown","d595ae69":"markdown","e49d6146":"markdown","14a31ce2":"markdown","76ae4cd7":"markdown","7bcdc033":"markdown","26a5f2af":"markdown","b0f12dc7":"markdown","7dbce298":"markdown","e572b0e7":"markdown","ffb681f8":"markdown","0cc8d555":"markdown","a68cd6e9":"markdown","3d2b9176":"markdown","8b558df6":"markdown","bcd9adaa":"markdown","7590d07f":"markdown","d248bade":"markdown","30d13e01":"markdown","124d2690":"markdown","e2f11ddf":"markdown","2bdd8e26":"markdown","26a822c4":"markdown","f4754bed":"markdown","0d11ab7e":"markdown","39defd8c":"markdown","df4403c0":"markdown","682c8e07":"markdown","1b8a6228":"markdown","deb91791":"markdown","f813202a":"markdown","979611c4":"markdown","c7e9abb5":"markdown","981cf1e6":"markdown","d795c185":"markdown","87ace136":"markdown","bbbfcb11":"markdown","5e0c1597":"markdown","91d6f9e3":"markdown","6b0f016f":"markdown","34cb754c":"markdown","dc3ca948":"markdown","ebb226e6":"markdown","440ac305":"markdown","7835cf2d":"markdown","51b8c9a8":"markdown","8b968aa4":"markdown","70a6c1ba":"markdown"},"source":{"fbe0aa25":"#Importamos las librerias\n\n# Tratamiento de datos\nimport pandas as pd\npd.set_option('display.max_columns', None)\nfrom pandas_profiling import ProfileReport\nfrom pandas.api.types import is_string_dtype, is_numeric_dtype,is_object_dtype\nimport numpy as np\n\n# Gr\u00e1ficos\nimport matplotlib.pyplot as plt\nimport plotly.express as px\nfrom matplotlib import gridspec\nfrom matplotlib.pyplot import *\nimport seaborn as sns\n\n# Preprocesado y modelado\nfrom sklearn import tree\nfrom sklearn.metrics import accuracy_score,auc,make_scorer,confusion_matrix\nfrom sklearn.metrics import plot_confusion_matrix,f1_score,precision_score,recall_score,roc_curve\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.model_selection import train_test_split,RandomizedSearchCV,GridSearchCV\nfrom sklearn.ensemble import RandomForestClassifier\nimport xgboost as xgb\nfrom xgboost import XGBClassifier\nfrom sklearn.cluster import KMeans\nfrom sklearn.decomposition import PCA\n\n\n#librerias para procesamiento de texto (text mining)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport nltk\nnltk.download('stopwords')\nnltk.download('punkt')\nnltk.download('wordnet')\nnltk.download('averaged_perceptron_tagger')\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom nltk import pos_tag\nimport sys  \n#!{sys.executable} -m pip install contractions\n!pip install spacy\nimport spacy\n!python -m spacy download en_core_web_sm\n\n! pip install textblob \nfrom textblob import TextBlob\n\n#! pip install wordcloud #generador de nube de palabras\nfrom wordcloud import WordCloud, STOPWORDS\n\n#word embeddings\n! pip install -U gensim\nimport gensim\nimport gensim.corpora as corpora\nfrom gensim.corpora import Dictionary\nfrom gensim.models.ldamodel import LdaModel\nfrom gensim.models import CoherenceModel\nfrom gensim.models import Word2Vec\nfrom scipy.spatial.distance import cdist\n\n#vizualizar topic modelling\nimport pyLDAvis\nimport pyLDAvis.gensim\n\nimport pickle # para guardar modelo\n\n# Varios\nimport itertools\nfrom itertools import chain\nfrom collections import Counter\nimport scipy as sp\nfrom tqdm.autonotebook import tqdm #barra de progreso\ntqdm.pandas()\nimport scipy.stats as ss\nimport random\n#Figures inline and set visualization style\n%matplotlib inline\nsns.set()\n\n# Configuraci\u00f3n de advertencias(warnings)\nimport warnings\nwarnings.filterwarnings('ignore')","77670a6f":"#importamos el dataset usando el parametro \"skipinitialspace\" para borrar espacios en blanco en columnas con texto\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\ndataset = pd.read_csv(\"\/kaggle\/input\/515k-hotel-reviews-data-in-europe\/Hotel_Reviews.csv\", skipinitialspace=True)\nnRow, nCol = dataset.shape\nprint(f'El dataset tiene {nRow} filas y {nCol} columnas')","605f440e":"#vemos el dataset\ndataset.head()","acab8ac8":"duplicated = dataset.duplicated().sum()\nprint(f'El dataset tiene {duplicated} filas duplicadas.')","048a2436":"#borramos las filas duplicadas\ndf = dataset.drop_duplicates()\ndf.shape","b45386ef":"# Tipo de cada columna\ndf.info() ","41ce8b27":"#el resumen de estad\u00edsticas de columnas numericas\ndf.describe()","b3b73583":"# N\u00famero de datos ausentes por variable\ndf.isna().sum().sort_values(ascending=False)","02163133":"#veamos el n\u00famero de categorias distintas de cada variable categ\u00f3rica. \nfor column in df:\n    if is_object_dtype(df[column]):\n        print(column,len(df[column].unique()))","0a372f62":"# Gr\u00e1fico de distribuci\u00f3n para cada variable num\u00e9rica\n\nfig, axes = plt.subplots(nrows=3, ncols=3, figsize=(14, 10))\naxes = axes.flat\ncolumnas_numeric = df.select_dtypes(include=['float64', 'int64']).columns\n\nfor i, colum in enumerate(columnas_numeric):\n    sns.histplot(\n        data    = df,\n        x       = colum,\n        stat    = \"count\",\n        kde     = True,\n        color   = (list(plt.rcParams['axes.prop_cycle'])*2)[i][\"color\"],\n        line_kws= {'linewidth': 2},\n        alpha   = 0.3,\n        ax      = axes[i]\n    )\n    axes[i].set_title(colum, fontsize = 9, fontweight = \"bold\")\n    axes[i].tick_params(labelsize = 8)\n    axes[i].set_xlabel(\"\")\n    \n    \nfig.tight_layout()\nplt.subplots_adjust(top = 0.9)\nfig.suptitle('Distribuci\u00f3n variables num\u00e9ricas', fontsize = 14, fontweight = \"bold\");","4e985e72":"columnas = ['Reviewer_Score', 'Average_Score', 'Review_Total_Negative_Word_Counts', 'Total_Number_of_Reviews', \n           'Review_Total_Positive_Word_Counts', 'Total_Number_of_Reviews_Reviewer_Has_Given']\nn = 1\nplt.figure(figsize=(20,15))\n\nfor column in columnas:\n  plt.subplot(4,4,n)\n  n = n+2\n  sns.boxplot(df[column])\n  plt.tight_layout()","c6df545f":"#PROCESADO DE LA VARIABLE Review_Date\"\n\n#transformando la variable \"Review_Date\" en tres variables: a\u00f1o, mes y dia\ndf['month'] = pd.to_datetime(df['Review_Date']).dt.month.apply(str)\ndf['year'] = pd.to_datetime(df['Review_Date']).dt.year.apply(str)\ndf['day'] = pd.to_datetime(df['Review_Date']).dt.day.apply(str)\n\n#cambiando el numero del mes a su nombre\ndf['month'] = pd.to_datetime(df['Review_Date']).dt.month_name() # no se compila con  locale=\"English\"\n\n#creamos una variable que solo tiene mes y a\u00f1o (sin d\u00eda)\ndf['period'] = pd.to_datetime(df['Review_Date']).dt.strftime('%m\/%Y') # => 7\/\ndf['period'] = pd.to_datetime(df['period'])","2f5bdaba":"#TRANSFORMAMOS MISSINGS\n\n# Completando valores faltantes de'Reviewer_Nationality' por valores aleatorios (en total solo 0.1% de missings)\nrandom = np.random.choice(df['Reviewer_Nationality'])\ndf['Reviewer_Nationality'] = df['Reviewer_Nationality'].fillna(random)\n\n# Completando valores faltantes de opiniones \ndf['Negative_Review'] = df['Negative_Review'].fillna('No Negative')\ndf['Positive_Review'] = df['Positive_Review'].fillna('No Positive')\n\n#controlando que no hayan valores faltantes\ndf[['Reviewer_Nationality','Negative_Review','Positive_Review']].isnull().any().any()\n\n#solo nos quedan missings en las variables de longitud\/latitud","218eccef":"#creamos nuevas columnas, contando las palabras de opiniones negativas y positivas\ndf.insert(7,'Negative_Word_Counts',df['Negative_Review'].str.split().str.len())\ndf.insert(11,'Positive_Word_Counts',df['Positive_Review'].str.split().str.len())\n\n#poniendo a 0 el n\u00famero de palabras donde no hay ninguna opini\u00f3n\ndf.loc[df.Positive_Review == 'No Positive', ['Positive_Word_Counts']] = 0\ndf.loc[df.Negative_Review == 'No Negative', ['Negative_Word_Counts']] = 0\n\n#borramos un espacio en blanco al final del texto en \"Reviewer_Nationality\":\ndf.Reviewer_Nationality = df.Reviewer_Nationality.str.rstrip()\n\n# Creamos nueva columna con el pa\u00eds del hotel\ndef country_ident(st):\n    last = st.split()[-1]\n    if last == \"Kingdom\": return \"United Kingdom\"\n    else: \n        return last\n    \ndf[\"Hotel_Country\"] = df[\"Hotel_Address\"].apply(country_ident)","dd92353b":"#borramos columnas \ndf.drop(['Review_Date','Review_Total_Positive_Word_Counts', 'Review_Total_Negative_Word_Counts', 'Additional_Number_of_Scoring'],axis=1, inplace=True)","253358a2":"df.head()","b3789625":"# creamos instancia de labelencoder\nlabelencoder = LabelEncoder()\ndf2 = df\n# Assigning numerical values and storing in another column\ndf2['Reviewer_Nationality_cat'] = labelencoder.fit_transform(df2['Reviewer_Nationality'])\nprint(\"Correlacion entre Nacionalidad y puntuacion:\",df2.Reviewer_Nationality_cat.corr(df2.Reviewer_Score))","96ee1d42":"# obtenemos el numero de huspedes por pais\nnationality_data = pd.DataFrame(df[\"Reviewer_Nationality\"].value_counts())\nnationality_data.rename(columns={\"Reviewer_Nationality\": \"Numero_huespedes\"}, inplace=True)\ntotal_guests = nationality_data[\"Numero_huespedes\"].sum()\nnationality_data[\"Huespedes en %\"] = round(nationality_data[\"Numero_huespedes\"] \/ total_guests * 100, 2)\nnationality_data[\"pais\"] = nationality_data.index\n#nationality_data[\"pais_otros\"] = nationality_data.index\nnationality_data['media_puntaje'] = df.groupby([\"Reviewer_Nationality\"]).Reviewer_Score.mean()\nnationality_data.loc[nationality_data[\"Huespedes en %\"] < 1, \"pais\"] = \"Other\"\n\n# visualizamos\nfig = px.pie(nationality_data,\n             values=\"Numero_huespedes\",\n             names=\"pais\",\n             title=\"Pais de or\u00edgen de hu\u00e9spedes\",\n             template=\"seaborn\")\nfig.update_traces(textposition=\"inside\", textinfo=\"percent+label\")\nfig.show()","e977ae4a":"nationality_data","31410612":"# Dejamos solo las nacionalidades con al menos 3000 opiniones dadas\nreviews_count=nationality_data[nationality_data[\"Numero_huespedes\"]>3000][\"Numero_huespedes\"].sort_values(ascending = False)\n\n# Obtenemos los colores del gr\u00e1fico\ncolors = []\ndim = reviews_count.shape[0]\nfor i in range(dim):\n    r = i * 1\/dim\n    colors.append((0.3,1-r,0.3))\n\n# Mostramos el resultado\nreviews_count.plot.barh(figsize=(10,10), color = colors)\nplt.title(\"Numero de rese\u00f1as por nacionalidad\", fontsize = 18)\nplt.ylabel(\"\")\n\nplt.show()","b8f3af24":"# Conservamos solo las nacionalidades con al menos 1000 rese\u00f1as dadas\nnationality_data2 = nationality_data[nationality_data[\"Numero_huespedes\"] > 1000].sort_values(by = \"media_puntaje\", ascending = False)\n\n# Colores del grafico\ncolors = []\ndim = nationality_data2.shape[0]\nfor i in range(dim):\n    r = i * 1\/dim\n    colors.append((0.3,1-r,0.3))\n\n# Visualizamos el resultado\nnationality_data2[\"media_puntaje\"].plot.barh(figsize = (10,20), color = colors)\nplt.title(\"Quien da peores puntuaciones a los hoteles?\", fontsize = 17)\nplt.axvline(df[\"Reviewer_Score\"].mean(), 0 ,1, color = \"grey\", lw = 3)\nplt.text(8, 55, \"average\", fontsize = 14, c = \"grey\")\nplt.text(8, -2, \"average\", fontsize = 14, c = \"grey\")\nplt.xlabel(\"Media de puntuaciones\", fontsize = 18)\nplt.ylabel(\"\")\nplt.show()","ad57661c":"# 10 hoteles m\u00e1s populares seg\u00fan el 'N\u00famero total de opiniones, la puntuaci\u00f3n media superior a 8,8 y los nombres de los hoteles'\ndf[df.Average_Score >= 8.8][['Hotel_Name','Average_Score','Total_Number_of_Reviews']].drop_duplicates().sort_values(by ='Total_Number_of_Reviews',ascending = False)[:10]\n","48fbdb66":"best_hotels = pd.DataFrame(df.groupby(['Hotel_Name', 'Hotel_Country','Average_Score'])['Average_Score'].mean().sort_values(ascending=False).head(10))\nworst_hotels = pd.DataFrame(df.groupby(['Hotel_Name', 'Hotel_Country', 'Average_Score'])['Average_Score'].mean().sort_values(ascending=False).tail(10))\n\nbest_worst_hotels = best_hotels +worst_hotels\nbest_worst_hotels.drop(best_worst_hotels.columns[len(best_worst_hotels.columns)-1], axis=1, inplace=True)\nbest_worst_hotels.sort_values(by=\"Average_Score\")","1233bc86":"# preparo dataset:\npuntuaciones_hotel = df[[\"Hotel_Name\", \"month\", \"Reviewer_Score\"]].sort_values(\"month\")\nfilter_list = ['Glam Milano', 'The Square Milano Duomo']\npuntuaciones_hotel = puntuaciones_hotel[(puntuaciones_hotel.Hotel_Name.isin(filter_list))] \n\n# orderdeno por meses:\nmeses = [\"January\", \"February\", \"March\", \"April\", \"May\", \"June\", \n          \"July\", \"August\", \"September\", \"October\", \"November\", \"December\"]\npuntuaciones_hotel[\"month\"] = pd.Categorical(puntuaciones_hotel[\"month\"], categories=meses, ordered=True)\n                                         \n# barplot con la deviacion estandar:\nplt.figure(figsize=(12, 8))\nsns.lineplot(x = \"month\", y=\"Reviewer_Score\", hue=\"Hotel_Name\", data=puntuaciones_hotel, \n             hue_order = ['Glam Milano', 'The Square Milano Duomo'],\n             ci=\"sd\", size=\"Hotel_Name\", sizes=(2.5, 2.5), style = \"Hotel_Name\",markers=True)\nplt.title(\"Puntuaciones durante el periodo\", fontsize=16)\nplt.xlabel(\"month\", fontsize=16)\nplt.xticks(rotation=45)\nplt.ylabel(\"Puntuaciones\", fontsize=16)\nplt.show()","f8b113ef":"# preparo dataset:\npuntuaciones_hotel = df[[\"Reviewer_Nationality\", \"period\", \"Reviewer_Score\"]].sort_values(\"period\")\nfilter_list = ['Spain', 'United States of America', 'Russia']\npuntuaciones_hotel = puntuaciones_hotel[(puntuaciones_hotel.Reviewer_Nationality.isin(filter_list))] \n                                         \n# barplot con la deviacion estandar:\nplt.figure(figsize=(12, 8))\nsns.lineplot(x = \"period\", y=\"Reviewer_Score\", hue=\"Reviewer_Nationality\", data=puntuaciones_hotel, \n             hue_order = ['Spain', 'United States of America', 'Russia'],\n             ci=\"sd\", size=\"Reviewer_Nationality\", sizes=(2.5, 2.5), style = \"Reviewer_Nationality\",markers=True)\nplt.title(\"Puntuaciones segun la nacionalidad durante el periodo\", fontsize=16)\nplt.xlabel(\"period\", fontsize=16)\nplt.xticks(rotation=45)\nplt.ylabel(\"Puntuaciones\", fontsize=16)\nplt.show()","cde90864":"x=df['Reviewer_Score'].value_counts()\nx=x.sort_index()\n#plot\nplt.figure(figsize=(12,5))\nax= sns.barplot(x.index, x.values, alpha=0.8, palette=\"icefire\")\nplt.title(\"Distribuci\u00f3n de las puntuaciones\", fontsize=16)\nplt.ylabel('Cantidad de observaciones', fontsize=14)\nplt.xlabel('Puntuaciones', fontsize=14)\nplt.xticks(rotation=45)\n\n#adding the text labels\n#rects = ax.patches\n#labels = x.values\n#for rect, label in zip(rects, labels):\n    #height = rect.get_height()\n    #ax.text(rect.get_x() + rect.get_width()\/2, height + 5, label, ha='center', va='bottom',rotation='vertical')\nplt.show()","a60f0171":"#cuantas rese\u00f1as hace un huesped\n\nx=df[df.Total_Number_of_Reviews_Reviewer_Has_Given <20]['Total_Number_of_Reviews_Reviewer_Has_Given'].value_counts()\nx=x.sort_index()\n\n#plot\nplt.figure(figsize=(12,5))\nax= sns.barplot(x.index, x.values, alpha=0.8, palette=\"icefire\")\nplt.title(\"N\u00famero de rese\u00f1as hechas por un huesped\",  fontsize=16)\nplt.ylabel('N\u00famero de huespedes', fontsize=14)\nplt.xlabel('N\u00famero de rese\u00f1as', fontsize=14)\nplt.xticks(rotation=45)\n\n#agregando las etiquetas \nrects = ax.patches\nlabels = x.values\nfor rect, label in zip(rects, labels):\n    height = rect.get_height()\n    ax.text(rect.get_x() + rect.get_width()\/2, height + 5, label, ha='center', va='bottom')\nplt.show()","92dccf38":"df.groupby([\"Hotel_Name\"]).Reviewer_Score.agg([max,min])","8f57dd50":"df.groupby([\"Hotel_Name\",'Reviewer_Nationality']).Reviewer_Nationality.count()","3e5cef59":"df.groupby([\"Hotel_Name\",'Reviewer_Nationality']).Reviewer_Score.min().head()","bfd06754":"# Visualizando la media de puntuaciones de hoteles \nplt.figure(figsize = (14,6))\nsns.countplot(x = df['Average_Score'],data = df, palette=\"icefire\")\n\n# En el gr\u00e1fico a continuaci\u00f3n, podemos notar que la puntuacion media va de 8 a 9\nplt.title(\"Puntuaci\u00f3n media de los hoteles de lujo\",  fontsize=16)\nplt.ylabel('Cantidad de observaciones', fontsize=14)\nplt.xlabel('puntuaci\u00f3n media', fontsize=14)\nplt.show()","75057999":"# Distribucion de los hoteles en Europa\nhotels_country = df[['Hotel_Country',\"Hotel_Name\"]].drop_duplicates()\n\nplt.figure(figsize = (14,6))\nsns.countplot(y = hotels_country['Hotel_Country'],data = hotels_country, palette=\"icefire\")\n\nplt.title(\"N\u00famero de hoteles por pa\u00eds\",  fontsize=16)\nplt.ylabel('Pa\u00eds', fontsize=14)\nplt.xlabel('N\u00famero de hoteles', fontsize=14)\n      \nplt.show()\n","21ebb93f":"# extraemos la cantidad de noches pasadas en hotel desde la columna \"Tags\"\ndef splitString(string):\n    array = string.split(\" ', ' \")\n    array[0] = array[0][3:]\n    array[-1] = array[-1][:-3]\n    if not 'trip' in array[0]:\n        array.insert(0,None)\n    try:\n        return float(array[3].split()[1])\n    except:\n        return None\n\ndf[\"Nights\"] = df[\"Tags\"].apply(splitString)\n\n#miramos si hay correlaci\u00f3n entre estancia y puntuaciones\nsns.jointplot(data=df,y=\"Reviewer_Score\",x=\"Nights\",kind=\"reg\")\n\n","26a1b893":"#Sacamos la informacion sobre typo de estancia:\ndf['Leisure'] = df['Tags'].map(lambda x: 1 if ' Leisure trip ' in x else 0)\ndf['Business'] = df['Tags'].map(lambda x: 2 if ' Business trip ' in x else 0)\ndf['Trip_type'] = df['Leisure'] + df['Business']","faa735d3":"df.Trip_type.value_counts()","6866e694":"#los mejores hoteles para el viaje de negocio\ndf[df.Business == 2].groupby([\"Hotel_Name\", \"Hotel_Country\"]).Reviewer_Score.mean().sort_values(ascending=False).head(10)","6a8691bd":"#la distribuci\u00f3n de la longitud de las opiniones para cada una de las clases\nfig = plt.figure(figsize=(10,6))\nplt1 = sns.distplot(df[\"Positive_Word_Counts\"], hist=True, label=\"Positive\")\nplt2 = sns.distplot(df[\"Negative_Word_Counts\"], hist=True, label=\"Negative\")\nfig.legend(labels=['Positive','Negative'])\nplt.show()","f9ae4973":"#juntamos en una columna las opiniones positivas y negativas\ndf['all_reviews']=df['Negative_Review']+ ' ' + df['Positive_Review']\n\n#controlando que no haya valores nulos\ndf['all_reviews'].isnull().any().any()","9050382f":"#creamos la columna \"positive\" con etiquetas de las rese\u00f1as.\n# El valor 1 hace referencia a la rese\u00f1a positiva que recibi\u00f3 de 6 a 10 puntos (ambos incluidos)\n# El valor 0 hace referencia a la rese\u00f1a negativa que recibi\u00f3 de 0 a 5.9 puntos \ndf[\"positive\"] = df[\"Reviewer_Score\"].apply(lambda x: 1 if x >= 6 else 0)\ndf_reviews = df[[\"all_reviews\",\"positive\", \"Reviewer_Score\", \"Hotel_Name\"]]\ndf_reviews","4aec3747":"# tras la union borramos'No Negative' or 'No Positive' from text\n\ndf_reviews[\"all_reviews\"] = df_reviews[\"all_reviews\"].apply(lambda x: x.replace(\"No Negative \", \"\").replace(\"No Positive \", \"\"))\n\n#controlando que no haya valores nulos\nprint(df['all_reviews'].isnull().any().any())\ndf_reviews","ccc4af65":"#visualizamos las puntuaciones positivas (color naranja con valor 1) y negativas (color azul con valor 0) segun \n###la cantidad de palabras en rese\u00f1as negativas y positivas.\n#es raro de ver los puntos azules arriba a la izquierda (significa que es una puntuacion negativa con 350 palabras en rese\u00f1a positiva y 0 en negativa)\nplt.figure(figsize=(25,10))\nsns.scatterplot(x=df['Negative_Word_Counts'], y=df['Positive_Word_Counts'],hue=df['positive'])\nplt.show()","da819e70":"#cuantas opiniones positivas y negativas por nacionalidad\na = pd.DataFrame(df.groupby([\"Reviewer_Nationality\",\"positive\"]).positive.count())\na.rename(columns={\"positive\": \"Cantidades\"}, inplace=True)\na","8bd123ba":"#Comprobamos el n\u00famero de observaciones de cada clase de la columna \"positive\" que etiqueta los datos. \ntarget_count = df_reviews.positive.value_counts()\nprint('Class 1 - Positive:', target_count[1])\nprint('Class 0 - Negative:', target_count[0])\nprint('Proportion:', round(target_count[1] \/ target_count[0], 2), ': 1')\n\ntarget_count.plot(kind='bar', title='Count (target)');","4101e594":"# Conteo de clases\nclass_0, class_1 = df_reviews.positive.value_counts()\n\n# Dividimos en clases\ndf_class_0 = df_reviews[df_reviews['positive'] == 0]\ndf_class_1 = df_reviews[df_reviews['positive'] == 1]\n\n# submuestreo aleatorio (random under-sampling)\ndf_class_1_under = df_class_1.sample(class_1,random_state=1)\ndf_sample = pd.concat([df_class_0, df_class_1_under], axis=0)\n\nprint('Submuestreo aleatorio:')\nprint(df_sample.positive.value_counts())\n","13853187":"#Funciones para normalizar el texto:\n\n# texto a minusculas\ndef texto_to_lower(text):\n  return text.lower()\n\n# Tokenizador utilizando nltk \ndef tokenizar(text):\n  tokens = word_tokenize(text)\n  return tokens\n\n# Quitar stop words\nfrom nltk.corpus import stopwords\ndef quitar_stopwords(tokens):\n    stop_words = set(stopwords.words('english')) \n    filtered_sentence = [w for w in tokens if not w in stop_words]\n    return filtered_sentence\n\n# Eliminar signos de puntuaci\u00f3n (nos quedamos s\u00f3lo lo alfabetico en este caso)\ndef quitar_puntuacion(tokens):\n    words=[word for word in tokens if word.isalpha()]\n    return words\n\n#Eliminar palabras de menos 2 letras\ndef quitar_palabras_menos2letras(tokens):\n    words=[word for word in tokens if len(word) >1]\n    return words\n\n# Lematization\nimport en_core_web_sm\nnlp = en_core_web_sm.load(disable=['parser', 'ner'])\ndef lematizar(tokens):\n    sentence = \" \".join(tokens)\n    mytokens = nlp(sentence)\n    # Lematizamos los tokens y los convertimos  a minusculas\n    mytokens = [ word.lemma_ if word.lemma_ != \"-PRON-\" else word.lower_ for word in mytokens ]\n    # Extraemos el text en una string\n    return \" \".join(mytokens)","4ea4b1bf":"#pasar a minuscula\ndf_sample[\"clean_reviews\"] = df_sample[\"all_reviews\"].progress_apply(lambda x: texto_to_lower(x))\n# Tokenizar\ndf_sample[\"clean_reviews\"] = df_sample[\"clean_reviews\"].progress_apply(lambda x: tokenizar(x))\n# Quitar Stopwords\ndf_sample[\"clean_reviews\"] = df_sample[\"clean_reviews\"].progress_apply(lambda x: quitar_stopwords(x))\n# Quitar puntuaci\u00f3n\ndf_sample[\"clean_reviews\"] = df_sample[\"clean_reviews\"].progress_apply(lambda x: quitar_puntuacion(x))\n# Quitar palabras de menos de 2 letras\ndf_sample[\"clean_reviews\"] = df_sample[\"clean_reviews\"].progress_apply(lambda x: quitar_palabras_menos2letras(x))","7305bf50":"#texto sin normalizar\nprint(df_sample[\"all_reviews\"][11])","94418fd4":"#texto limpio sin lematizacion\nprint(df_sample[\"clean_reviews\"][11])","60093d91":"# Lematizar - tarda ~ 8 min visto que son mas de 100 000 opiniones\ndf_sample[\"clean_reviews\"] = df_sample[\"clean_reviews\"].progress_apply(lambda x: lematizar(x)) #tarda ~ 10 min","c1914037":"#tras la lematizacion\ndf_sample[\"clean_reviews\"][11]","28de86b1":"#calculamos la longitud de caracteres de los textos una vez hecha la limpieza anterior:\ndf_sample[\"token_len\"] = df_sample[\"clean_reviews\"].apply(lambda x: len(x))\n\n#calculamos la cantidad de tokens:\ndf_sample['token_quantity'] = df_sample.clean_reviews.apply(lambda x: len(str(x).split(' ')))","65ea74e2":"#verificamos los valores nulos con ayuda de longitud de caracteres \nprint(len(df_sample[df_sample.token_len < 3]))\ndf_sample[df_sample.token_len < 3]","89d7f879":"print(\"Dimensiones antes:\", df_sample.shape)\ndf_sample = df_sample[df_sample.token_len > 2]\nprint(\"Dimensiones despues:\",df_sample.shape)","32b19a2a":"#visualizamos la longitud de tokens de los textos una vez hecha la limpieza anterior:\nfig = plt.figure(figsize=(10,6))\nplt1 = sns.distplot(df_sample[df_sample['positive'] == 0][\"token_len\"], hist=True, label=\"negative\")\nplt2 = sns.distplot(df_sample[df_sample['positive'] == 1][\"token_len\"], hist=True, label=\"positive\")\nfig.legend(labels=['negative','positive'])\nplt.title(\"Longitud de caracteres\", fontsize = 16)\nplt.show()","c8a30b1d":"#visualizamos la cantidad de tokens:\n\nfig = plt.figure(figsize=(10,6))\nplt1 = sns.distplot(df_sample[df_sample['positive'] == 0][\"token_quantity\"], hist=True, label=\"negative\", color=\"g\")\nplt2 = sns.distplot(df_sample[df_sample['positive'] == 1][\"token_quantity\"], hist=True, label=\"positive\", color=\"y\")\nfig.legend(labels=['negative','positive'])\nplt.title(\"Cantidad de palabras\", fontsize = 16)\nplt.show()","72ab9f22":"print(\"Correlacion entre longitud de rese\u00f1as y puntuacion:\",df_sample.token_quantity.corr(df_sample.positive))","f0668263":"#Dividimos el dataset en dos segun la clase: positivo y negativo\npositive = df_sample[\"clean_reviews\"][df_sample.positive == 1]\nnegative = df_sample[\"clean_reviews\"][df_sample.positive == 0]","17dd5d46":"def frecuencia_tokens(lista): \n    # Creamos diccionario vac\u00edo \n    frecuencia = {} \n    for item in lista: \n        if (item in frecuencia): \n            frecuencia[item] += 1\n        else: \n            frecuencia[item] = 1\n    return frecuencia\n\n# Extraemos los tokens de todos los textos y los introducimos en una lista com\u00fan:\nlista_tokens_pos = list()\nfor i in positive:\n  # Tokenizamos cada documento con word_tokenize()\n  tokens_document = word_tokenize(i)\n  # A\u00f1adimos esos tokens como nuevos elementos\n  # Si usamos append se crear\u00eda una lista de listas, de este modo a\u00f1adimos los\n  # Elementos de la segunda lista\n  lista_tokens_pos.extend(tokens_document)\n\n# Calculamos la frecuencia\ndict_freq_pos = frecuencia_tokens(lista_tokens_pos)","5c2908c0":"# Ordenamos el diccionario por la frecuencia de sus palabras\ndict_freq_order = sorted(dict_freq_pos.items(), key=lambda x: x[1], reverse=True)\ntoken_names_pos = list()\ntoken_freqs_pos = list()\nfor i in dict_freq_order:\n  if i[1] > 8000:\n    token_names_pos.append(i[0])\n    token_freqs_pos.append(i[1])\n\n#visualizamos\nplt.rcParams['figure.figsize'] = [10, 5]\nsns_g = sns.barplot(x=token_names_pos, y=token_freqs_pos)\nplt.xticks(rotation=45,fontsize=14)\nplt.yticks(fontsize=12)\nplt.grid(True, color ='k', axis = 'y', linestyle=':')\nplt.title(\"Tokens m\u00e1s utilizados en la clase positiva\", fontsize=16)\nplt.show()","937627f8":"lista_tokens_neg = list()\nfor i in negative:\n  tokens_document = word_tokenize(i)\n  lista_tokens_neg.extend(tokens_document)\n\n# Calculamos la frecuencia\ndict_freq_neg = frecuencia_tokens(lista_tokens_neg)\n\n# Ordenamos el diccionario por la frecuencia de sus palabras\ndict_freq_order_neg = sorted(dict_freq_neg.items(), key=lambda x: x[1], reverse=True)\ntoken_names_neg = list()\ntoken_freqs_neg = list()\nfor i in dict_freq_order_neg:\n  if i[1] > 9000:\n    token_names_neg.append(i[0])\n    token_freqs_neg.append(i[1])\n\nplt.rcParams['figure.figsize'] = [12, 7]\nsns_g = sns.barplot(x=token_names_neg, y=token_freqs_neg)\nplt.xticks(rotation=45,fontsize=14)\nplt.yticks(fontsize=12)\nplt.grid(True, color ='k', axis = 'y', linestyle=':')\nplt.title(\"Tokens m\u00e1s utilizados en la clase negativa\", fontsize=16)\nplt.show()","fa7528c2":"#creando n-gramas\ndef find_ngrams(input_list, n):\n    return list(zip(*[input_list[i:] for i in range(n)]))\n\n#df_sample['bigrams'] = df_sample['clean_reviews'].apply(lambda row: list(nltk.bigrams(row.split(' '))))\ndf_sample['bigrams'] = df_sample['clean_reviews'].map(lambda x: find_ngrams(x.split(\" \"), 2))\ndf_sample['trigrams'] = df_sample['clean_reviews'].map(lambda x: find_ngrams(x.split(\" \"), 3))\ndf_sample.head()","39d61d84":"#Frecuencia de bigramas positivas\nbigrams_positive = df_sample[df_sample.positive == 1]['bigrams'].tolist()\nbigrams_positive = list(chain(*bigrams_positive))\nbigram_positive_counts = Counter(bigrams_positive)\nbigram_positive_counts.most_common(10)\n#Frecuencia de bigramas positivas","fa1671f0":"# Frecuencia de bigramas negativas\nbigrams_negative = df_sample[df_sample.positive == 0]['bigrams'].tolist()\nbigrams_negative = list(chain(*bigrams_negative))\nbigram_negative_counts = Counter(bigrams_negative)\nbigram_negative_counts.most_common(10)\n# Frecuencia de bigramas negativas","8aef465d":"# Frecuencia de trigramas positivas\ntrigrams_positive = df_sample[df_sample.positive == 1]['trigrams'].tolist()\ntrigrams_positive = list(chain(*trigrams_positive))\ntrigram_positive_counts = Counter(trigrams_positive)\ntrigram_positive_counts.most_common(10)\n# Frecuencia de trigramas positivas","86adcb1b":"# Frecuencia de trigramas negativas\ntrigrams_negative = df_sample[df_sample.positive == 0]['trigrams'].tolist()\ntrigrams_negative = list(chain(*trigrams_negative))\ntrigrams_negative_counts = Counter(trigrams_negative)\ntrigrams_negative_counts.most_common(10)\n# Frecuencia de trigramas negativas","601c350b":"# Funci\u00f3n para eliminar cierto tipo de tags\ndef filtra_tags(textos, tags_permitidas=['NOUN', 'ADJ', 'VERB', 'ADV']):\n    textos_out = []\n    for sent in textos:\n        # Juntar los \"bigrams\"\n        doc = nlp(sent) \n        # Filtra por etiqueta\n        textos_out.append([token.text for token in doc if token.pos_ in tags_permitidas])\n    return textos_out\n\n# Calculamos la frecuencia\ndata_pos = filtra_tags(positive, tags_permitidas=['ADJ','ADV'])\n#pasamos la lista de listas en una lista plana\ndata_pos_flat_list = [item for sublist in data_pos for item in sublist]\n\n# Extraemos los tokens de todos los textos y los introducimos en una lista com\u00fan:\nlista_tokens_pos = list()\nfor i in data_pos_flat_list:\n  tokens_document = word_tokenize(i)\n  lista_tokens_pos.extend(tokens_document)\n    \n# Calculamos la frecuencia\ndict_freq_pos = frecuencia_tokens(lista_tokens_pos)\n\n# Ordenamos el diccionario por la frecuencia de sus palabras\ndict_freq_order = sorted(dict_freq_pos.items(), key=lambda x: x[1], reverse=True)\ntoken_names_pos1 = list()\ntoken_freqs_pos1 = list()\nfor i in dict_freq_order:\n  if i[1] > 4000:\n    token_names_pos1.append(i[0])\n    token_freqs_pos1.append(i[1])\n\nplt.rcParams['figure.figsize'] = [10, 5]\nsns_g = sns.barplot(x=token_names_pos1, y=token_freqs_pos1)\nplt.xticks(rotation=45,fontsize=14)\nplt.grid(True, color ='k', axis = 'y', linestyle=':')\nplt.title(\"Tokens(adverbios y adjetivos) m\u00e1s utilizados en la clase positiva\", fontsize=16)\nplt.show()","60ab64b0":"data_neg = filtra_tags(negative, tags_permitidas=['ADJ','ADV'])\ndata_neg_flat_list = [item for sublist in data_neg for item in sublist]\n\n# Extraermos los tokens de todos los textos y los introducimos en una lista com\u00fan:\nlista_tokens_neg = list()\nfor i in data_neg_flat_list:\n  # Tokenizamos cada documento con word_tokenize()\n  tokens_document = word_tokenize(i)\n  # A\u00f1adimos esos tokens como nuevos elementos\n  # Si usamos append se crear\u00eda una lista de listas, de este modo a\u00f1adimos los elementos de la segunda lista\n  lista_tokens_neg.extend(tokens_document)\n\n# Calculamos la frecuencia\ndict_freq_neg = frecuencia_tokens(lista_tokens_neg)\n\n# Ordenamos el diccionario por la frecuencia de sus palabras\ndict_freq_order = sorted(dict_freq_neg.items(), key=lambda x: x[1], reverse=True)\ntoken_names_neg1 = list()\ntoken_freqs_neg1 = list()\nfor i in dict_freq_order:\n  if i[1] > 4000:\n    token_names_neg1.append(i[0])\n    token_freqs_neg1.append(i[1])\n\nplt.rcParams['figure.figsize'] = [10, 5]\nsns_g = sns.barplot(x=token_names_neg1, y=token_freqs_neg1)\nplt.xticks(rotation=45,fontsize=14)\nplt.grid(True, color ='g', axis = 'y', linestyle=':')\nplt.title(\"Tokens(adverbios y adjetivos) m\u00e1s utilizados en la clase negativa\", fontsize=16)\nplt.show()\n","4804c97b":"# Usamos wordcloud para representar visualmente los datos de texto\ndef wordcloud_draw(data, color = 'black'):\n    words = ' '.join(data)\n    wordcloud = WordCloud(background_color=color,\n                      width=2500,\n                      height=2000,\n                      max_words=500, \n                      contour_width=0, \n                      contour_color='steelblue', \n                      scale =3).generate(words)\n    plt.figure(1, figsize = (15, 15))\n    plt.imshow(wordcloud)\n    plt.axis('off')\n    plt.show()\n    \nprint(\"Positive reviews\")\nwordcloud_draw(positive,'white')\nprint(\"Negative reviews\")\nwordcloud_draw(negative)","087f8f02":"df_sample['sentiment_polarity'] = df_sample['all_reviews'].apply(lambda x: TextBlob(x).sentiment.polarity)\ndf_sample['sentiment_subjectivity'] = df_sample['all_reviews'].apply(lambda x: TextBlob(x).sentiment.subjectivity)\ndf_sample.head()","babc9e9a":"print(df_sample['all_reviews'][3043])\nprint(\"sentiment polarity:\"  + str(df_sample['sentiment_polarity'][3043]))\nprint(\"sentiment_subjectivity:\" + str(df_sample['sentiment_subjectivity'][3043]))\nprint(\"Reviewer_Score:\" + str(df_sample['Reviewer_Score'][3043]))\n\nprint('--------------------------------')\nprint(df_sample['all_reviews'][299])\nprint(\"sentiment polarity:\"  + str(df_sample['sentiment_polarity'][299]))\nprint(\"sentiment_subjectivity:\" + str(df_sample['sentiment_subjectivity'][299]))\nprint(\"Reviewer_Score:\" + str(df_sample['Reviewer_Score'][299]))","a1afbe0d":"# Rese\u00f1as de sentimiento m\u00e1s positivas(con m\u00e1s de 5 palabras)\ndf_sample[df_sample[\"token_quantity\"] >= 5].sort_values(\"sentiment_polarity\", ascending = False)[[\"clean_reviews\", \"Reviewer_Score\",\"sentiment_polarity\"]].head(10)","56b42124":"# Rese\u00f1as de sentimiento m\u00e1s negativo(con m\u00e1s de 5 palabras)\ndf_sample[df_sample[\"token_quantity\"] >= 5].sort_values(\"sentiment_polarity\", ascending = False)[[\"clean_reviews\", \"Reviewer_Score\",\"sentiment_polarity\"]].tail(10)","12fd22e6":"# Rese\u00f1as de sentimiento m\u00e1s positivo (con m\u00e1s de 5 palabras)\ndf_sample[df_sample[\"token_quantity\"] >= 5].sort_values(\"sentiment_polarity\", ascending = False)[[\"clean_reviews\", \"Reviewer_Score\",\"sentiment_polarity\"]].head(10)","13b9b4f9":"sns.regplot(y = \"sentiment_polarity\", x = \"sentiment_subjectivity\", data=df_sample)\nplt.title(\"Polaridad y subjetividad de texto\", fontsize = 20)","00ac6d4f":"# BoW Features\nvectorizador = TfidfVectorizer(min_df=0.02, ngram_range=(1,3), max_features=300, lowercase=False)\nvector_data = vectorizador.fit_transform(df_sample[\"clean_reviews\"])\nvector_data","0372309d":"#para ver los tokens \nvector_data_voc = vectorizador.fit(df_sample[\"clean_reviews\"])\nvector_data_voc.vocabulary_","e61b390f":"#A\u00f1adimos otros features de textblob.\nextra_features = df_sample[[\"sentiment_polarity\", \"sentiment_subjectivity\"]]\n\n# Extraemos las etiquetas y las asignamos a la variable y\ny = df_sample[\"positive\"].values.astype(np.float32) \n# Unimos las caracter\u00edsticas TFIDF con las caracter\u00edsticas previamente seleccionadas\n# Extraemos los valores (values) de las extra_features, que es un dataframe  \n# Hay que guardar vector_data (que es una sparse matrix) con extra_features.\n# Elegimos el formato \"csr\", Compressed Sparse Row matrix, que es el formato de\n# resultado de tfidfVectorizer \n# https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.feature_extraction.text.TfidfVectorizer.html#sklearn.feature_extraction.text.TfidfVectorizer.fit_transform\nX = sp.sparse.hstack((vector_data,extra_features.values),format='csr')","4c92db81":"X # tenemos 104781 documentos con 209 caracteristicas ","3042cd59":"#los nombres de las caracteristicas\nX_columns=vectorizador.get_feature_names()+extra_features.columns.tolist()\nX_columns","0d7cb9ed":"X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\nprint(X_train.shape)\nprint(X_test.shape)","d5e60483":"model_log=LogisticRegression() # resultado peor con los parametros ofrecidos por la grid_search: C=0.1, penalty=\"l2\", solver = \"newton-cg\" \nmodel_log.fit(X_train,y_train)\nmodel_log.score(X_test, y_test)","3e8a5294":"naive_bayes = GaussianNB()\nnaive_bayes.fit(X_train.toarray(),y_train) #transforming a sparse matrix to a numpy array (se puede usar tambien X.todense())\nnaive_bayes.score(X_test.toarray(), y_test) #el resultado peor que en regrecion logistica","59b3089e":"tree=DecisionTreeClassifier(random_state=0)\ntree.fit(X_train,y_train)\ntree.score(X_test, y_test)","0e3224e7":"rf = RandomForestClassifier()\nrf.fit(X_train, y_train)\nrf.score(X_test, y_test)","aeda94b2":"#la funcion para sacar metricas del modelo\ndef saca_metricas(y1, y2):\n    print('Matriz de confusi\u00f3n')\n    print(confusion_matrix(y1, y2))\n    print('Accuracy')\n    print(accuracy_score(y1, y2))\n    print('Precision')\n    print(precision_score(y1, y2))\n    print('Recall')\n    print(recall_score(y1, y2))\n    print('F1')\n    print(f1_score(y1, y2))\n\n#usamos el modelo en el muestreo de test\ny_pred = rf.predict(X_test)\nprint(saca_metricas(y_test,y_pred))","3fd66ff6":"#visualizamos la matriz de confusi\u00f3n\nsns.set(font_scale=1.4) # el tama\u00f1o de la etiqueta\nclass_names = ['negative', 'positive']\nplot_confusion_matrix(rf, X_test, y_test, normalize = None,values_format = '.0f', display_labels=class_names)","1d6925f1":"# Curva ROC\ny_pred = [x[1] for x in rf.predict_proba(X_test)]\nfpr, tpr, thresholds = roc_curve(y_test, y_pred, pos_label = 1)\n\nroc_auc = auc(fpr, tpr)\n\nplt.figure(1, figsize = (10, 8))\nlw = 2\nplt.plot(fpr, tpr, color='darkorange',\n         lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)\nplt.plot([0, 1], [0, 1], lw=lw, linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.0])\nplt.xlabel('\u00cdndice de falsos positivos',fontsize = 14)\nplt.ylabel('\u00cdndice de verdaderos positivos', fontsize = 14)\nplt.title('Curva ROC', fontsize = 16)\nplt.legend(loc=\"lower right\")\nplt.show()","d78451f2":"#El AUC es el \u00e1rea bajo la curva ROC\nroc_auc","047822dd":"importance = rf.feature_importances_","b946e0f2":"(pd.Series(importance, index=X_columns).nlargest(20).plot(kind='barh', figsize= (12,8))) \n#tambien se puede hacer con el numpy.argsort","4e3101b9":"txt_data = df_sample[\"all_reviews\"]\n\n#tokenizamos las rese\u00f1as limpias\ndata_words = list()\nfor i in df_sample[\"clean_reviews\"].to_list():\n  data_words.append(tokenizar(i))\n\n#data_words #obtenemos una lista bidimensional ","10b1fe51":"# Construimos los modelos de bigramas y trigramas con gensim\n# No devuelve trigramas o bigramas en si mismos, si no que asocia palabras que aparecen juntas en multitud de ocasiones\nbigram = gensim.models.Phrases(data_words, min_count=2, threshold=100) # higher threshold fewer phrases.\nbigram_tokens = [bigram[data_words[w]] for w in range(len(data_words))]","868259eb":"#modelo Word2Vec\nmodel = Word2Vec(bigram_tokens, vector_size=300, min_count=1, workers=3, window=5, sg=1)","6d748159":"#de Word2Vec a representaci\u00f3n vectorial de tokens\ndef get_feat(x):\n    arr = []\n    for i in x:\n        arr.append(model.wv[i])\n    \n    arr = list(np.mean(arr, axis=0))\n    return arr\n\n#Cada fila en txt_data est\u00e1 representada por 300 caracter\u00edsticas de Word2Vec\nXX = pd.Series(bigram_tokens).apply(lambda x: get_feat(x))\nXX = np.array(list(XX))\nXX = XX\/XX.max(axis=0)\n\n#scaling\nfrom sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\nXX = scaler.fit_transform(XX)","121587a8":"# M\u00e9todo de Elbow para determinar el n\u00famero de clusters.\nfrom plotly.subplots import make_subplots\nimport plotly.graph_objs as go\ndistortions = [] \ninertias = [] \nmapping1 = {} \nmapping2 = {} \nK = range(1,10) \nfor k in K: \n    #Building and fitting the model \n    kmeanModel = KMeans(n_clusters=k).fit(XX) \n    kmeanModel.fit(XX)     \n      \n    distortions.append(sum(np.min(cdist(XX, kmeanModel.cluster_centers_, \n                      'euclidean'),axis=1)) \/ XX.shape[0]) \n    inertias.append(kmeanModel.inertia_) \n  \n    mapping1[k] = sum(np.min(cdist(XX, kmeanModel.cluster_centers_, \n                 'euclidean'),axis=1)) \/ XX.shape[0] \n    mapping2[k] = kmeanModel.inertia_ \n\n\ndf_elbow = pd.DataFrame(list(zip(K, distortions, inertias)), columns=[\"K\",\"Distortions\",\"Inertia\"])\n\n\nfig = make_subplots(rows=1, cols=2)\nfig.add_trace(\n    go.Scatter(x=df_elbow[\"K\"], y=df_elbow[\"Distortions\"], name=\"Distortion\"),\n    row=1, col=1\n)\nfig.add_trace(\n    go.Scatter(x=df_elbow[\"K\"], y=df_elbow[\"Inertia\"], name=\"Inertia\"),\n    row=1, col=2\n)\nfig.update_layout(template=\"plotly\",title=\"Elbow method - Distortions and Inertia\", xaxis_title=\"K\", xaxis2_title=\"K\")","ba151f3a":"#clusterizaci\u00f3n\nkmeans = KMeans(n_clusters=3, random_state=0).fit(XX)\ncenters = kmeans.labels_\ndf_XX  = pd.DataFrame(XX)\ndf_XX[\"clusters\"] = list(centers)\n\n#concat with orginal dataframe\ndf_concat = pd.concat([txt_data.reset_index(),pd.Series(bigram_tokens,name=\"bigram_tokens_clean\"),df_XX],axis=1).set_index('index')\ndf_concat = df_concat.join(df_sample[\"Hotel_Name\"])","074d260f":"# Visualizaci\u00f3n de clusters usando PCA\npca = PCA(n_components=2)\nX_pca = pca.fit_transform(df_concat[np.arange(0,100)])\ndf_concat['PC1'] = X_pca[:,0]\ndf_concat['PC2'] = X_pca[:,1]\n\nfig = px.scatter(df_concat, x=\"PC1\", y=\"PC2\", color=\"clusters\",\n                 size=len('bigram_tokens_clean'), hover_data=['Hotel_Name'])\n\nfig.show()","e2a784f9":"# WordCloud Visualizacion de cada cluster\n \n\ndef clust_words(cluster_num):\n    lst = list(df_concat[df_concat[\"clusters\"]==cluster_num][\"bigram_tokens_clean\"])\n    reviews = \"\"\n    for i in lst:\n        for t in i:\n            if len(t) >= 5:\n                reviews += \" \" + t + \" \"\n\n    reviews = reviews.strip()\n    return reviews\n\n\nlength = len(df_concat[\"clusters\"].unique())\nstopwords = set(STOPWORDS)\nstopwords.add('hotel')\nstopwords.add('nothing')\nstopwords.add('positive')\nwrdcld = WordCloud(width = 200, height =300, \n                background_color ='black', \n                stopwords = stopwords, \n                min_font_size = 4,\n                max_words=2000)\n\nfig, ax = plt.subplots(1,length,figsize = (24,20))\n#plt.figure(figsize = (50, 50), facecolor = None) \n\nfor i in range(length):\n    reviews = clust_words(i)\n    wordcloud = wrdcld.generate(reviews)\n    ax[i].imshow(wordcloud) \n    ax[i].axis(\"off\") \n\nplt.show();","4b1b0be6":"#tokenizamos las rese\u00f1as limpias\ndata_words = list()\nfor i in df_sample[\"clean_reviews\"].to_list():\n  data_words.append(tokenizar(i))\n\n# Construimos los modelos de bigramas y trigramas con gensim\n# No devuelve trigramas o bigtamas en si mismos, si no que asocia palabras\n# que aparecen juntas en multitud de ocasiones\nbigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\n\n# Con esto todo va m\u00e1s r\u00e1pido, pero ya no pueden modificarse lo anterior\n# https:\/\/www.kite.com\/python\/docs\/gensim.models.phrases.Phraser\n# The goal of this class is to cut down memory consumption of `Phrases`, by discarding model state\n# not strictly needed for the bigram detection task.\n# Use this instead of `Phrases` if you do not need to update the bigram statistics with new documents any more.\nbigram_mod = gensim.models.phrases.Phraser(bigram)","84cb7bb7":"bigram_mod[data_words[10]]","80b7d007":"from nltk.corpus import stopwords\nstop_words = stopwords.words('english')\n#stop_words.extend(['day', 'hotel', 'room', 'great', 'night','staff','service'])\n\n# Antes \"entrenamos\" el generador de bigrams, ahora se generan realmente en nuestros textos\ndef make_bigrams(textos):\n    return [bigram_mod[doc] for doc in textos]\n\n# Funci\u00f3n para eliminar cierto tipo de tags\ndef filtra_tags(textos, tags_permitidas=['NOUN', 'ADJ', 'VERB', 'ADV']):\n    textos_out = []\n    for sent in textos:\n        # Juntar los \"bigrams\"\n        doc = nlp(\" \".join(sent)) \n        # Filtra por etiqueta\n        textos_out.append([token.text for token in doc if token.pos_ in tags_permitidas])\n    return textos_out\n\n#Vamos a aplicar esas funciones:\n\n# Remove Stop Words\n#data_words_nostops = remove_stopwords(data_words)\n# Crear Bigrams\ndata_words_bigrams = make_bigrams(data_words)\n\n# Desabilitamos el \"NER\" y el \"Parser\" que no lo vamos a usar\nnlp = spacy.load(\"en_core_web_sm\", disable=['parser', 'ner'])\n#  Filtrar por categor\u00eda gramatical \ndata_pos = filtra_tags(data_words_bigrams, tags_permitidas=['NOUN', 'ADJ', 'ADV'])","07b7cd41":"print(\"Numero tokens antes de filtrar: {} tokens\".format(len(data_words_bigrams[3])))\nprint(\"Numero tokens despues de filtrar: {} tokens\".format(len(data_pos[3])))\nprint(len(data_pos))","3dddc224":"# Creamos diccionario de t\u00e9rminos \nid2word = corpora.Dictionary(data_pos)\nprint(id2word)\n\n# Asignamos a la variable texts nuestro corpus\ntexts = data_pos\n# Transformamos nuestro corpus limpio a Bag of Words. \ncorpus = [id2word.doc2bow(text) for text in texts]\n# View\nprint(corpus[2])","98c20f75":"def calculo_valor_coherencia(corpus, dictionary, k, a, b):\n    lda_model = gensim.models.LdaMulticore(corpus=corpus,\n                                           id2word=dictionary,\n                                           num_topics=k, \n                                           random_state=100,\n                                           chunksize=100,\n                                           passes=10,\n                                           alpha=a,\n                                           eta=b)\n    \n    coherence_model_lda = CoherenceModel(model=lda_model, texts=data_pos, dictionary=dictionary, coherence='c_v')\n    \n    return coherence_model_lda.get_coherence()","3397e2d5":"'''\n#tarda mas de 1 hora\n\nimport tqdm\ngrid = {}\n# Topics range\nmin_topics = 2\nmax_topics = 22\nstep_size = 2\ntopics_range = range(min_topics, max_topics, step_size)\n# Alpha\nalpha = 0.01\n# Beta\nbeta = 0.9\n\n# Validation sets\nnum_of_docs = len(corpus)\ncorpus_sets = corpus\ncorpus_title = '100% Corpus'\nmodel_results = {'Topics': [],\n                 'Alpha': [],\n                 'Beta': [],\n                 'Coherence': []}\n\n# Vamos a esperar mucho....\nif 1 == 1:\n    pbar = tqdm.tqdm()\n    # Itera a lo largo del range de los topics\n    for k in topics_range:\n      # Calculamos coherencia para esos topics\n      cv = calculo_valor_coherencia(corpus=corpus, dictionary=id2word,\n                                    k=k, a=alpha, b=beta)\n      # Guardamos los datos\n      #model_results['Validation_Set'].append(corpus_title[i])\n      model_results['Topics'].append(k)\n      model_results['Alpha'].append(alpha)\n      model_results['Beta'].append(beta)\n      model_results['Coherence'].append(cv)\n\n      pbar.update(1)\n    pbar.close()\n\nmodel_results\n'''","3aa724c6":"'''\n# Mostrar grafico\nplt.plot(topics_range, model_results[\"Coherence\"])\nplt.xlabel(\"Num Topics\")\nplt.ylabel(\"Coherence score\")\nplt.legend((\"coherence_values\"), loc='best')\nplt.show()\n'''","01aead05":"# Entrenamos el modelo con el n\u00famero de topics anteriormente calculado:\nlda_model = gensim.models.LdaMulticore(corpus=corpus,\n                                      id2word=id2word,\n                                      num_topics=4, \n                                      random_state=100,\n                                      chunksize=100,\n                                      passes=10,\n                                      alpha=0.01,\n                                      eta=0.9)","48d04529":"#vemos los topics modelados y tokens m\u00e1s presentes\nprint(lda_model.print_topics())\n","24c0e404":"lda_model.get_document_topics(corpus[8])","549f17bd":"# Obtenemos los temas m\u00e1s relevantes para la palabra dada.\nlda_model.get_term_topics(\"shower\", minimum_probability=0.0001) ","a3413061":"## Top 4 palabras claves, el topic dominante y la probabilid para cada observacion\ntxt_data = df_sample[\"all_reviews\"]\n\narr = []\nfor i, j in enumerate(lda_model[corpus]):\n    if len(j) > 0:\n        max_val = sorted([w[1] for w in j],reverse=True)[0]\n        max_topic = [w[0] for w in j if w[1]==max_val][0]\n        keywords = lda_model.show_topic(max_topic,topn=4)\n        keywords = [k[0] for k in keywords]\n        description = txt_data.iloc[i]\n        arr.append([description,  \",\".join(keywords), max_topic, round(max_val,2),])\n\nlda_distribution = pd.DataFrame(arr, columns=['Description', 'Top Keywords', 'Dominant Topic', 'Probability'])\nlda_distribution.head()","f0f27742":"# Visualizamos los topics\npyLDAvis.enable_notebook()\nvis = pyLDAvis.gensim.prepare(lda_model, corpus, id2word)\nvis","59373c4a":"Vemos que la primera opinion es negativa, pero sobre todo muy subjetiva, que significa que esta basada en el sentimiento personal y no en los hechos reales. La segunda opinion es completamente neutra.","037e49af":"Tras la busqueda de hiperparametros, los resultatos son las siguientes:\nEl resultado de calculo de coherencia de topicos es el siguiente:\n\n{'Topics': [2, 4, 6, 8, 10, 12, 14, 16, 18, 20],\n\n'Alpha': [0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01],\n\n'Beta': [0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9],\n\n'Coherence': [0.5759212122284145, 0.6104550546086185, 0.6005178817668809, 0.5232854245151395, 0.6179901458176291, 0.5590572191080169, 0.5607692771113711, 0.5372332240699184, 0.5474444929027618, 0.5016116513129409]}\n  \n  En el grafico de coherencia vemos el optimo numero de clusters igual a 4 o 10. ","54180a1a":"recibimos 209 caracteristicas. Vamos a visualizarlos:\n","907ec2c0":"## TEXT MINING DE LAS RESE\u00d1AS HOTELERAS\n\n### El an\u00e1lisis forma parte de un proceso de evaluaci\u00f3n del \u201cM\u00e1ster \u2013\"BIG DATA & DATA SCIENCE\" UCM ","e84bcf5c":"## 1.1 EDA (An\u00e1lisis exploratorio)","a3655a23":"## 2.1 Modelo de clasificaci\u00f3n para detectar rese\u00f1as negativas","3f848768":"#### Tokens m\u00e1s utilizados que no sean pronombres y verbos.\nVamos a ver los tokens compuestos por los adjetivos y los adverbios que juegan un gran papel en polaridad del sentimiento.","4375331d":"Vemos que el atributo de polaridad sentimental juega el papel m\u00e1s importante en modelo de clasificaci\u00f3n. La mayor\u00eda de los atributos son adverbios relacionados con la experiencia de una persona.","a8e943e6":"Los modelos LDA necesitaban adem\u00e1s de los vectores de entrada, un n\u00famero de \"topics\" para ser entrenados. Como no sabemos a priori cuantos hay, creamos la funci\u00f3n \"calculo_valor_coherencia\" que a partir del corpus, el diccionario y otros valores calcula un modelo lda, calcula la coherencia entre sus topics y la devuelve.","d60ab466":"Todas las variables categoricas tienen la cardinalidad alta. \n- Hay 1492 hoteles \u00fanicos\n- La variable **Reviewer_Nationality** tiene 227 valores distintos lo que resulta interesante ya que en el mundo existe un total de 194 pa\u00edses soberanos o estados soberanos reconocidos por la ONU (para el a\u00f1o 2021).Por lo tanto, hay un total de 194 nacionalidades. ","4f2e3132":"Vemos que hay m\u00e1s de 50 opiniones que tienen de 0 a 2 caracteres en tokens. Borramos estas opiniones, porque no aportan nada. \n","4fb2a213":"#### WORDCLOUD","97c5c158":"### 2.1.5 Vectorizaci\u00f3n\n\nUna vez hemos limpiado y procesado el texto, vamos a extraer caracter\u00edsticas de cada palabra y cada documento utilizando TFIDFVectorizer (Term Frequency - Inverse Document Frequency) :\n\n- unigramas, bigramas y trigramas\n- con maximum de 300 features\n- Que el sistema no considere los elementos que salgan en menos del 2% de los documentos para reducir el tama\u00f1o de la salida final..\n\nPero, \u00bfpor qu\u00e9 no simplemente contar cu\u00e1ntas veces aparece cada palabra en cada documento? El problema con este m\u00e9todo es que no tiene en cuenta la importancia relativa de las palabras en los textos. Una palabra que aparece en casi todos los textos probablemente no traer\u00e1 informaci\u00f3n \u00fatil para el an\u00e1lisis. Por el contrario, las palabras raras pueden tener muchos m\u00e1s significados.La m\u00e9trica TF-IDF resuelve este problema. TF calcula cuantas veces la palabra aparece en el texto. IDF calcula la importancia relativa de esta palabra, que depende de en cu\u00e1ntos textos se puede encontrar.","b505b9db":"Vemos que tras la limpieza el texto no tiene puntuaciones, ni stopwords(como por ejemplo \"not\", \"n't\", \"did\" etc). El proceso de lematizacion funcion\u00f3 bien: pronombres en plural pasaron a singular, los verbos pasaron en infinitivo etc.","e3389c7f":"Staff are rude Rooms are awful Location is awful Hotel is awful MUCH BETTER HOTELS AROUND THEN THIS Nothing\n- sentiment polarity:-0.5599999999999999\n- sentiment_subjectivity:0.82\n- Reviewer_Score:2.5\n\n--------------------------------\nRenovations taking place which were not communicated foul smell outside our room things left from previous guest Nothing in particular stood out\n- sentiment polarity:0.0\n- sentiment_subjectivity:0.1375\n- Reviewer_Score:4.6","7870740c":"### 2.1.2 Normalizaci\u00f3n (preprocesado) del texto\nVamos a normalizar los datos. Para ello vamos a generar peque\u00f1as funciones que nos permitan:\n\n- Transformar a min\u00fasculas.\n- Tokenizar.\n- Eliminar stopwords.\n- Eliminar palabras de menos 2 letras\n- Eliminar signos de puntuaci\u00f3n.\n- Lematizar tokens.","d3e83372":"El analizador de sentimientos b\u00e1sico basado en el clasificador XGB proporciona 81.5% de precisi\u00f3n y 80.7% en accuracy.","5543c25b":"Si miramos los resultados de acontinuaci\u00f3n, lo que hacen estas librer\u00edas es generar bigramas a partir de la similitud que tienen las palabras en un espacio vectorial (utiliza word2vec por detr\u00e1s).\n\nPor ejemplo, los tokens \"New\" \"York\" son unidos en el proceso anterior a \"New_York\". Algo similar ocurre con \"computer_science\". Basicamente la idea de los brigramas en gensim es la de agrupar palabras que generalmente se usen juntas, y no incorporar m\u00e1s variables al sistema.","d595ae69":"**Modelo Random Forest classification**","e49d6146":"**Cargamos el dataset**","14a31ce2":"Para preprocesar datos de gensim utilizamos las funciones espec\u00edficas con las que cuenta. Podemos utilizar el m\u00e9todo Phrases","76ae4cd7":"Podr\u00edamos ver la contribuci\u00f3n de una palabra espec\u00edfica a cada uno de los topics con get_term_topics:","7bcdc033":"Una vez lo hemos validado, podemos asociar topics a cada uno de los documentos.\n\nUtilizando el m\u00e9todo get_document_topics() sobre el texto del corpus que queramos podemos detectar la composici\u00f3n de temas que tiene cada documento:","26a5f2af":"**Modelo Decision tree classification**","b0f12dc7":"No existe la relacion entre nacionalidad y puntuaciones. Vamos a visualizar que nacionalidades se presentan en las opiniones dadas, que proporciones tienen, que nota suelen dar.","7dbce298":"Vamos a calcular la coherencia suponiendo distintos n\u00fameros de topics. Por ejemplo de 1 a 22 en saltos de 2.","e572b0e7":"El dataset es binario, no balanceado: hay 9 veces m\u00e1s las opiniones positivas que negativas.","ffb681f8":"Dividimos nuestro dataset en Train y Test.","0cc8d555":"Despues de ese procesado tenemos un conjunto de 104774 documentos procesados y preparados para introudcir a un modelo de topic modeling\n\n**Entrenamiento y validaci\u00f3n**\n\nEn primer lugar generamos un diccionario utilizando los objetos de Gensim. Ese diccionario contiene un m\u00e9todo \"doc2bow\" que transforma el texto a vectores comprensibes por el modelo LDA.","a68cd6e9":"### Outliers","3d2b9176":"**Modelo Logistic Regression**","8b558df6":"### Dinamica de puntuaciones y mejores hoteles\n\nMiramos la dinamica de puntuaciones de un hotel durante el tiempo observado","bcd9adaa":"## 2.1.6 Construcci\u00f3n del modelo","7590d07f":"![](https:\/\/raw.githubusercontent.com\/rafjaa\/machine_learning_fecib\/master\/src\/static\/img\/resampling.png)","d248bade":"Visualizamos los primeros 20 caracteristicas m\u00e1s importantes con ayuda de pandas.Series.plot","30d13e01":"## 1.3 VISUALIZACI\u00d3N","124d2690":"# Parte 1: Exploraci\u00f3n, transformaci\u00f3n y visualizaci\u00f3n de dataset  ","e2f11ddf":"## Balancear el dataset - Random under-sampling\n\nUna t\u00e9cnica ampliamente adoptada para tratar con conjuntos de datos altamente desequilibrados se llama remuestreo (resampling). Consiste en eliminar muestras de la clase mayoritaria (undersampling) y \/ o agregar m\u00e1s ejemplos de la clase minoritaria (oversampling).","2bdd8e26":"# Parte 2: Mineria de texto","26a822c4":"### Exploramos la variable  nacionalidad\n\nMiramos si hay una correlacion entre la nacionalidad de tourista y su puntuacion. Para ello convertimos la variable categorica a numerica con LabelEncoder. La funci\u00f3n sklearn.preprocessing.LabelEncoder codifica etiquetas de una caracter\u00edstica categ\u00f3rica en valores num\u00e9ricos entre 0 y el n\u00famero de clases menos 1. Una vez instanciado, el m\u00e9todo fit lo entrena (creando el mapeado entre las etiquetas y los n\u00fameros) y el m\u00e9todo transform transforma las etiquetas que se incluyan como argumento en los n\u00fameros correspondientes. El m\u00e9todo fit_transform realiza ambas acciones simult\u00e1neamente.","f4754bed":"### 2.1.7 Evaluaci\u00f3n del modelo Random Forest. ","0d11ab7e":"**Resumen**:\n\nUsando Word2Vec, hemos convertido observaciones en un vector de 300 caracter\u00edsticas (Palabra -> Vectores; observaci\u00f3n -> Suma de vectores de palabras). Y agrupamos las caracter\u00edsticas para obtener 43 grupos. A continuaci\u00f3n, hemos generado WordCloud de palabras dentro de cada grupo.\n\nLos cl\u00fasteres no est\u00e1n claramente separados, tienen varias palabras que se superponen: las observaciones dentro de cada cl\u00faster tienen descripciones similares. Ser\u00eda conveniente extender los stopwords con las palabras repetidas para internar recibir grupos m\u00e1s separados. \n\nPero a pesar de eso opinamos que el **primer cluster** (wordcloud a la izquierda) representa hoteles con rese\u00f1as mas neutras. El **segundo cluster** (en la mitad) representa los hoteles con rese\u00f1as m\u00e1s negativas (noisy,dirty,small). Y el **tercer cluster** (a la derecha) agrupa los mejores hoteles (great location, amazing, confortable, friendly staff, lovely, clean, helpfull staff, value money etc.)\n","39defd8c":"- Las variables Negative_Review y Positive_Review en el documento inicial tenian  espacios en blanco (la gente a veces pone un espacio cuando no tienen nada que escribir). Al cargar el dataset usamos la caracter\u00edstica \"skipinitialspace=True\" lo que nos ha permitido tener missings en vez de solo espacios vac\u00edos no considerados como missigns.\n\n- Cuando una persona no deja ning\u00fan comentario ni espacio en blanco en la opini\u00f3n negativa, el sistema genera autom\u00e1ticamente las palabras \"No Negative\" en la columna de opini\u00f3n negativa. Entonces, podemos concluir que los missings en las opiniones negativas son iguales a \"No Negative\". Lo mismo pasa con las opiniones positivas.","df4403c0":"En la media los rusos y los espa\u00f1oles dan peores puntuaciones que los americanos. ","682c8e07":"#### M\u00e9tricas de selecci\u00f3n de clusters","1b8a6228":"Hay 9 variables numericas y 8 categoricas. Review_Date y days_since_review no deben ser categoricas. ","deb91791":"**Resumen**\n\nHemos generado 4 topics bastante coherentes. Por supuesto hay palabras que se superponen en cada topic (hotel, room, staff), quiz\u00e1s ser\u00eda conveniente meterles en stopwords para excluir de los topics. Pero podemos distinguir claramente que el **primer topic** se habla de emociones y la experiencia de una persona en un hotel por la cantidad de adverbios que hay (friendly, helpfull, clean, comfortable, perfect, small etc). El **topic 2** nos cuenta sobre el proceso de reserva y llegada al hotel (time, book, check, day, charge, pay, request).\nEl **topic 3** habla m\u00e1s de los ammenities del hotel: room, shower, bed, window, floor, door, light etc). \nEl **topic 4** es sobre el restaurante, minibar, comida (bar, drink, wine, egg, milk, fridge, tea, breakfast).","f813202a":"### 2.1.3 Visualisaci\u00f3n previa al modelado","979611c4":"Calculamos y representamos gr\u00e1ficamente en forma de distribuci\u00f3n las longitudes en caracteres y en tokens (despues del proceso de limpieza) de los documentos del corpus.","c7e9abb5":"C\u00e1lculamos y representamos gr\u00e1ficamente en forma de histograma los 10 tokens m\u00e1s utilizados en cada una de las clases del corpus despues del proceso de limpieza. Dividimos el dataset en dos segun la clase: positivo y negativo:","981cf1e6":"60% de rese\u00f1as estan hechas solo por 4 paises de habla ingles. ","d795c185":"## 2.3 Topic modelling","87ace136":"### 2.1.1 Adquisici\u00f3n de data textual","bbbfcb11":"## 1.2 PRE-PROCESADO","5e0c1597":"- La puntuacion media de los hoteles observada es 8.4\/10. \n- La media de palabras en la opinion negativa es igual a 18.5 palabras (max 408), la positiva a 17.7 (max 395). \n- La media del numero total de opiniones que recibi\u00f3 cada hotel es igual a 2743. \n- La mitad de touristas escribi\u00f3 hasta 3 opiniones. ","91d6f9e3":"La media de puntuaciones m\u00e1s alta del hotel Britannia International Hotel Canary Wharf sera en Enero, y m\u00e1s baja en marzo y septiembre. ","6b0f016f":"A pesar de la ventaja de equilibrar las clases, estas t\u00e9cnicas tambi\u00e9n tienen sus debilidades. La implementaci\u00f3n m\u00e1s sencilla del sobremuestreo es duplicar registros aleatorios de la clase minoritaria, lo que puede provocar un sobreajuste. En el submuestreo, la t\u00e9cnica m\u00e1s simple consiste en eliminar registros aleatorios de la clase mayoritaria, lo que puede provocar la p\u00e9rdida de informaci\u00f3n.","34cb754c":"##  2.2 CLUSTERING DE HOTELES SEG\u00daN SUS RESE\u00d1AS (Word Embeddings)\n\nIntentaremos agrupar los hoteles seg\u00fan las rese\u00f1as.","dc3ca948":"### \u00cdNDICE:\n\n### Parte 1: EXPLORACI\u00d3N, TRANSFORMACI\u00d3N Y VISUALIZACI\u00d3N DE DATASET \n        1.1 EDA (analisis exploratorio)\n        1.2 Preprocesado\n        1.3 Visualizaci\u00f3n\n\n### Parte 2: TEXT MINING\n        2.1 Modelo de clasificaci\u00f3n para detectar rese\u00f1as negativas\n           2.1.1 Adquisici\u00f3n de data textual\n           2.1.2 Normalizaci\u00f3n (preprocesado) del texto\n           2.1.3 Visualisaci\u00f3n previa al modelado\n           2.1.4 Analisis de sentimiento con TextBlob\n           2.1.5 Vectorizaci\u00f3n\n           2.1.6 Construcci\u00f3n del modelo\n           2.1.7 Evaluaci\u00f3n del modelo\n        2.2 Clustering de hoteles seg\u00fan sus rese\u00f1as (Word Embeddings)\n        2.3 Topic Modelling","ebb226e6":"**Modelo Naive Bayes**","440ac305":"Utilizamos la librer\u00eda scipy (funci\u00f3n sparse.hstack) para unir las caracter\u00edsticas TFIDF (contenidas en \u00b4vector_data\u00b4) con las que acabamos de seleccionar (\u00b4extra_features\u00b4). Esta uni\u00f3n nos generar\u00e1 una matriz X que utilizaremos para hacer el train-test split posteriormente:","7835cf2d":"Cuanto m\u00e1s permanezca una tourista en el hotel, menor ser\u00e1 la puntuaci\u00f3n (ligeramente).","51b8c9a8":"### 2.1.4 Analisis de sentimiento con TextBlob\n\nAplicamos la libreria TextBlob a nuestro texto sin procesar para extraer el sentimiento que tiene: la polaridad y la subjetividad.\n\nLa polaridad es el sentimiento mismo, que va de -1 a +1. La subjetividad es una medida del sentimiento siendo objetivo a subjetivo, cuantifica la opini\u00f3n personal contenida en el texto y va de 0 a 1. Una mayor subjetividad significa que el texto contiene mucha m\u00e1s opini\u00f3n personal que una informaci\u00f3n objetiva. ","8b968aa4":"Ahora hay que aplicar los modelos a los datos. Para eso utilizamos las funciones que aparecen m\u00e1s abajo.\n\nAdem\u00e1s, se ha incorporado la funci\u00f3n filtra_tags, que permite coger tokens de una categor\u00eda espec\u00edfica. Muy util apra el topic_modeling","70a6c1ba":"#### Transformacion\n\nPara preprocesar datos de gensim utilizamos las funciones espec\u00edficas con las que cuenta. Podemos utilizar el m\u00e9todo Phrases"}}