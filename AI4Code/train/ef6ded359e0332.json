{"cell_type":{"d7995b52":"code","7459da99":"code","d046134b":"code","c0c4500c":"code","62e0ee7e":"code","6a0bfb80":"markdown"},"source":{"d7995b52":"import pandas as pd\nimport numpy as np\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom xgboost import XGBRegressor","7459da99":"#import the data and shape\ndf = pd.read_csv('..\/input\/train-folds\/train_folds.csv')\ndf_test = pd.read_csv('..\/input\/30-days-of-ml\/test.csv')\nsample_submission = pd.read_csv('..\/input\/30-days-of-ml\/sample_submission.csv')","d046134b":"#features(categorical and numerical datas separate)\nuseful_features = [c for c in df.columns if c not in (\"id\", \"target\", \"kfold\")]\nobject_cols = [col for col in useful_features if col.startswith(\"cat\")]\nnumerical_cols = [col for col in useful_features if 'cont' in col]\ndf_test = df_test[useful_features]\n","c0c4500c":"#Model hyperparameter of XGboostRegressor\nxgb_params = {\n    'random_state': 1, \n    # gpu\n#     'tree_method': 'gpu_hist', \n#     'gpu_id': 0, \n#     'predictor': 'gpu_predictor',\n    # cpu\n    'n_jobs': 4,\n    'booster': 'gbtree',\n    'n_estimators': 10000,\n    # optimized params\n    'learning_rate': 0.03628302216953097,\n    'reg_lambda': 0.0008746338866473539,\n    'reg_alpha': 23.13181079976304,\n    'subsample': 0.7875490025178415,\n    'colsample_bytree': 0.11807135201147481,\n    'max_depth': 3 ,\n    'tree_method':'gpu_hist',\n    'eval_metric':'rmse'\n    \n}\n\nfinal_predictions = []\nscores = []\nfor fold in range(5):\n    xtrain = df[df.kfold != fold].reset_index(drop=True)\n    xvalid = df[df.kfold == fold].reset_index(drop=True)\n    xtest = df_test.copy()\n    \n    ytrain = xtrain.target\n    yvalid = xvalid.target\n    \n    xtrain = xtrain[useful_features]\n    xvalid = xvalid[useful_features]\n    \n    #ordinal encode categorical colums and standardscaler is applied (mean0,sd=1)\n    ordinal_encoder = preprocessing.OrdinalEncoder()\n    xtrain[object_cols] = ordinal_encoder.fit_transform(xtrain[object_cols])\n    xvalid[object_cols] = ordinal_encoder.transform(xvalid[object_cols])\n    xtest[object_cols] = ordinal_encoder.transform(xtest[object_cols])\n    \n    scaler = preprocessing.StandardScaler()\n    xtrain[numerical_cols] = scaler.fit_transform(xtrain[numerical_cols])\n    xvalid[numerical_cols] = scaler.transform(xvalid[numerical_cols])\n    xtest[numerical_cols] = scaler.transform(xtest[numerical_cols])\n    \n    model= XGBRegressor(**xgb_params)\n    model.fit(\n        xtrain, ytrain,\n        early_stopping_rounds=300,\n        eval_set=[(xvalid, yvalid)], \n        verbose=1000\n    )\n    preds_valid = model.predict(xvalid)\n    test_preds = model.predict(xtest)\n    final_predictions.append(test_preds)\n    \n    rmse = mean_squared_error(yvalid, preds_valid, squared=False)\n    scores.append(rmse)\n    print(fold, rmse)\n\nprint(np.mean(scores), np.std(scores))","62e0ee7e":"#prediction of data\npreds = np.mean(np.column_stack(final_predictions), axis=1)\nsample_submission.target = preds\nsample_submission.to_csv(\"submission.csv\", index=False)\nprint(sample_submission)","6a0bfb80":"**Necessary Library**"}}