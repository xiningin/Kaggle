{"cell_type":{"8838dc0d":"code","686d6516":"code","502ebe6b":"code","c6deeec1":"code","efbcb7e2":"code","73101129":"code","4e148445":"code","add6deb5":"code","a75b9fdc":"code","e6a62914":"code","2e62c359":"code","1b3aa59f":"code","76f515e1":"code","01120335":"code","b6ffbebf":"code","9225cf92":"code","d400afb8":"code","244f04b8":"code","5e449e8e":"code","5ba75380":"code","b992b723":"code","d1ed7d6f":"code","7a4633fe":"code","834ae86b":"code","63011b05":"code","7224657e":"code","40bb2428":"code","30206976":"code","3a3b3460":"code","85d61a8e":"code","19f42e23":"code","df9be225":"code","ff08b084":"code","b4d04cf5":"code","df28e341":"code","1d4a3698":"code","a2014864":"code","9a3acb54":"code","912bd083":"code","1d6fb308":"markdown","b6dec3de":"markdown","d168d7d5":"markdown","feb9a049":"markdown","706f9e65":"markdown","643bc7fc":"markdown","19292bcf":"markdown","a92875ba":"markdown","b187e544":"markdown","ba1dc32e":"markdown","2b335b83":"markdown","8e4aa598":"markdown","6599c0dc":"markdown","4cd2fde1":"markdown","dc3530e4":"markdown","923b3e1a":"markdown","9cf1980f":"markdown","9edea011":"markdown","0ae90842":"markdown","f390f176":"markdown","64b545f8":"markdown","ba665dd8":"markdown","dbf61af6":"markdown","7b820d98":"markdown","8ce20d2b":"markdown","8e113e8f":"markdown","8d2c7715":"markdown","155806d3":"markdown"},"source":{"8838dc0d":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport random as rd\n\ndata = pd.read_csv('\/kaggle\/input\/stroke-prediction-dataset\/healthcare-dataset-stroke-data.csv')\nprint(\"Shape:\", data.shape)\nprint(\"Features:\", list(data.columns))\nprint()\nprint(\"Data Describe:\")\nprint(data.iloc[:, 0:5].describe())\nprint(data.iloc[:, 5:12].describe())","686d6516":"# Setting up graphics and color palette\nfrom pylab import rcParams\nrcParams['figure.figsize'] = 9, 7\n\nsns.set_context('notebook')\nsns.set_style('whitegrid')\npal = sns.color_palette('Set2')\nsns.set_palette(pal)\n\nimport warnings  \nwarnings.filterwarnings('ignore')","502ebe6b":"print(data.isnull().sum())","c6deeec1":"sns.histplot(data['stroke'], bins=2)\nplt.show()\nimbalance_ratio = sum(data['stroke']==1)\/sum(data['stroke']==0)\nprint(\"Ratio stroke\/no_stroke:\")\nprint(imbalance_ratio)","efbcb7e2":"rcParams['figure.figsize'] = 20, 7\nfig, axes = plt.subplots(1, 2)\nsns.histplot(data['age'], bins=8, kde= True, ax=axes[0])\nsns.histplot(data['bmi'], bins=10, kde=True, ax=axes[1])\nplt.suptitle('Continuous data distribution')\nplt.show()\nrcParams['figure.figsize'] = 9, 7\nsns.histplot(data['avg_glucose_level'], kde=True, bins=8)\nplt.show()","73101129":"cat_features = ['gender', 'work_type', 'Residence_type', 'smoking_status']\nrcParams['figure.figsize'] = 20, 15\nfig, axes = plt.subplots(2, 2)\nfor i in range(0, len(cat_features)):\n    sns.histplot(data[cat_features[i]], ax=axes[int(i\/2), i%2])\nplt.suptitle('Categorical data distribution')\nplt.show()","4e148445":"bin_features = ['hypertension', 'heart_disease', 'ever_married', 'stroke']\ndata['ever_married'] = [int(b) for b in data['ever_married'] == 'Yes']\nrcParams['figure.figsize'] = 20, 15\nfig, axes = plt.subplots(2, 2)\nfor i in range(0, len(bin_features)):\n    sns.histplot(data[bin_features[i]], bins=2, ax=axes[int(i\/2), i%2])\nplt.suptitle('Binary data distribution')\nplt.show()","add6deb5":"rcParams['figure.figsize'] = 15,11\ncorr_mat = data.drop(['id'], axis=1).corr()\nsns.heatmap(corr_mat, vmin=-1, vmax=1, cmap=sns.diverging_palette(360, 180, as_cmap=True))\nplt.show()","a75b9fdc":"smoke_to_int = {\n    'never smoked': 0,\n    'formerly smoked': 1,\n    'smokes': 2,\n    'Unknown': -1\n}\ndata['smoking_status'] = [smoke_to_int[s] for s in data['smoking_status']]\nprint(data['smoking_status'])","e6a62914":"work_to_int = {\n    'Private': 1,\n    'Self-employed': 2,\n    'Govt_job': 2,\n    'children': 4,\n    'Never_worked': 0\n}\ndata['work_cat'] = [work_to_int[s] for s in data['work_type']]\nprint(data['work_cat'])","2e62c359":"data['gender_female'] = [int(m) for m in data['gender'] == 'Female']\ndata['residence_urban'] = [int(m) for m in data['Residence_type'] == 'Urban']\nprint(data['residence_urban'])","1b3aa59f":"features = [\n    'id', 'gender_female', 'age', 'hypertension', \n    'heart_disease', 'ever_married', 'work_cat',\n    'residence_urban', 'avg_glucose_level', 'bmi', \n    'smoking_status', 'stroke'\n]\n\ndf = data[features]","76f515e1":"from sklearn.neighbors import KNeighborsRegressor\ntrain = df[df['bmi'].isna()==False]\npred = df[df['bmi'].isna()]\n\nbmi_regressor = KNeighborsRegressor(n_neighbors=5)\nX = train.drop(['bmi'], axis=1)\ny = train['bmi']\nbmi_regressor.fit(X, y)\ny_hat = bmi_regressor.predict(pred.drop(['bmi'], axis=1))\npred.loc[:, 'bmi'] = y_hat","01120335":"train.loc[:, 'cat'] = 'Train'\npred.loc[:, 'cat'] = 'Pred'\n\nfig, axes= plt.subplots(2,1, sharex=True)\nsns.histplot(train, x='bmi', stat='probability', hue='cat', bins=15, ax=axes[0], kde=True)\nsns.histplot(pred, x='bmi', stat='probability', hue='cat', bins=15, ax=axes[1], kde=True)\nplt.show()\n\ndf = pd.concat([train, pred])\ndf.drop(['cat'], axis=1, inplace=True)\n","b6ffbebf":"from sklearn.neighbors import KNeighborsClassifier\ntrain = df[df['smoking_status']!=-1]\npred = df[df['smoking_status']==-1]\n\nsmoker_classifier = KNeighborsClassifier(n_neighbors=5)\nX = train.drop(['smoking_status'], axis=1)\ny = train.loc[:, 'smoking_status']\nsmoker_classifier.fit(X, y)\ny_hat = smoker_classifier.predict(pred.drop(['smoking_status'], axis=1))\npred.loc[:, 'smoking_status'] = y_hat","9225cf92":"train.loc[:, 'cat'] = 'Train'\npred.loc[:, 'cat'] = 'Pred'\n\nfig, axes= plt.subplots(2,1, sharex=True)\nsns.histplot(train, x='smoking_status', stat='probability', hue='cat', bins=3, ax=axes[0])\nsns.histplot(pred, x='smoking_status', stat='probability', hue='cat', bins=3, ax=axes[1])\nplt.show()\n\ndf = pd.concat([train, pred])\ndf.drop(['cat'], axis=1, inplace=True)","d400afb8":"df.describe()","244f04b8":"from sklearn.preprocessing import StandardScaler\n\nX = np.array(df.drop(['stroke', 'id'], axis=1))\ny = np.array(df.loc[:, 'stroke'])\nscaler = StandardScaler()\nX = scaler.fit_transform(X)\npd.DataFrame(X).describe()","5e449e8e":"from sklearn.model_selection import train_test_split\n\nprint(\"# of samples: \" + str(y.shape[0]))\n\n# Splitting data into train (80%) CV (10%) test(10%)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .1, stratify = y, random_state = 42)\ny_train = y_train.astype(np.float32).reshape((-1,1))\ny_test = y_test.astype(np.float32).reshape((-1,1))\n\n# #Transposing the data\n# X_train, X_test = [np.array(x).T for x in [X_train, X_test]]\n# y_train, y_test = [np.array(y).reshape(1, -1) for y in [y_train, y_test]]\n\nprint(\"X_train shape: \" + str(X_train.shape) + \"\\t y_train shape:\" + str(y_train.shape))\nprint(\"X_test shape:  \" + str(X_test.shape) + \"\\t y_test shape: \" + str(y_test.shape))\n\nprint(sum(y_train==1))\nprint(sum(y_test==1))","5ba75380":"from sklearn.svm import SVC\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import f1_score\n\nsvc = SVC(C=1000, class_weight = {0: 1, 1: 20}, kernel='poly', degree=5)\nsvc.fit(X_train, y_train)","b992b723":"#Added Parameter Tuning:\niterations = 30\nC_range = [int(10**rd.uniform(0, 4)) for i in range(iterations)] # Weights for Regularization param\nweight_range = [int(10**rd.uniform(1, 2.5)) for i in range(iterations)] # Weights for imbalanced data\ndegree_range = [int(rd.uniform(2, 7)) for i in range(iterations)]\n\ncombos = list(zip(C_range, weight_range, degree_range))\nbest_score = 0\nbest_combination = (1, 10, 2)\n\nfor C, weight, degree in combos:\n    svc = SVC(C=C, class_weight = {0: 1, 1: weight}, kernel='poly', degree=degree)\n    svc.fit(X_train, y_train)\n    y_pred = svc.predict(X_test)\n    current_score = f1_score(y_test, y_pred)\n    print(\"C:\", C, \"\\tWeight:\", weight,\"\\tdegree:\", degree, \"\\tScore:\", current_score)\n    if current_score > best_score:\n        best_score = current_score\n        best_combination = (C, weight, degree)\n\nC, weight, degree = best_combination\nsvc = SVC(C=C, class_weight = {0: 1, 1: weight}, kernel='poly', degree=degree)\nsvc.fit(X_train, y_train)","d1ed7d6f":"y_pred = svc.predict(X_train)\nprint('Score on the training set:')\nprint(classification_report(y_train, y_pred))\nprint('roc_auc score: ', end='')\nprint(roc_auc_score(y_train, y_pred))\nprint('f1 score:', f1_score(y_train, y_pred), end='\\n\\n')\n\ny_pred = svc.predict(X_test)\nprint('Score on the dev set:')\nprint(classification_report(y_test, y_pred))\nprint('roc_auc score: ', end='')\nprint(roc_auc_score(y_test,  y_pred))\nprint('f1 score:', f1_score(y_test,  y_pred), end='\\n\\n')","7a4633fe":"from sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n\nrf_model = RandomForestClassifier(n_estimators=150, criterion='gini',\n                                  class_weight = {0: 1, 1: 100}, min_samples_leaf=6,\n                                  max_features = None)\nrf_model.fit(X_train, np.ravel(y_train))","834ae86b":"#Added Parameter Tuning:\nweight_range = [int(x) for x in np.logspace(1, 2.5, num=5)] # Weights for imbalanced data\nmin_leaf_range = [int(x) for x in np.linspace(8, 20, num=5)] # \nbest_score = 0\nbest_combination = (10, 5)\n\nfor w, leaf_s in [(x, y) for x in weight_range for y in min_leaf_range]:\n    rf_model = RandomForestClassifier(n_estimators=150, criterion='gini',\n                              class_weight = {0: 1, 1: w}, min_samples_leaf=leaf_s,\n                              max_features = None)\n    rf_model.fit(X_train, np.ravel(y_train))\n    y_pred = rf_model.predict_proba(X_test)\n    current_score = f1_score(y_test,np.around(y_pred[:, 1]))\n    print(\"Weight:\", w,\"\\tMin leaf sample:\", leaf_s, \"\\tF1-Score:\", current_score)\n    if current_score > best_score:\n        best_score = current_score\n        best_combination = (w, leaf_s)\n\nw, leaf_s = best_combination\nrf_model = RandomForestClassifier(n_estimators=150, criterion='gini',\n                              class_weight = {0: 1, 1: w}, min_samples_leaf=leaf_s,\n                              max_features = None)\nrf_model.fit(X_train, np.ravel(y_train))","63011b05":"y_pred = rf_model.predict_proba(X_train)\nprint('Score on the training set:')\nprint(classification_report(y_train, np.around(y_pred[:, 1])))\nprint('roc_auc score: ', end='')\nprint(roc_auc_score(y_train, y_pred[:, 1]))\nprint('f1 score:', f1_score(y_train,np.around(y_pred[:, 1])), end='\\n\\n')\n\ny_pred = rf_model.predict_proba(X_test)\nprint('Score on the dev set:')\nprint(classification_report(y_test, np.around(y_pred[:, 1])))\nprint('roc_auc score: ', end='')\nprint(roc_auc_score(y_test, y_pred[:, 1]))\nprint('f1 score:', f1_score(y_test,np.around(y_pred[:, 1])), end='\\n\\n')","7224657e":"def dfify(hist):\n\tdf = pd.DataFrame(hist.history)\n\tdf['epoch'] = df.index\n\tval_cols = [x for x in df.columns if x.startswith('val')]\n\tdf_val = df[val_cols+['epoch']]\n\tdf.drop(columns=val_cols, inplace=True)\n\tdf_val.rename(columns={col: col.split('val_')[-1] for col in df_val.columns}, inplace=True)\n\tdf['phase'] = 'train'\n\tdf_val['phase'] = 'val'\n\treturn pd.concat([df, df_val], ignore_index=True)\n\ndef visu_history(hist):\n    rcParams['figure.figsize'] = 14, 10\n    hist_df = dfify(hist)\n    fig, axes = plt.subplots(2, 2)\n    sns.lineplot(data = hist_df, x='epoch', y='loss', hue='phase', ax=axes[0,0])\n    sns.lineplot(data = hist_df, x='epoch', y='auc', hue='phase', ax=axes[0,1])\n    sns.lineplot(data = hist_df, x='epoch', y='precision', hue='phase', ax=axes[1,0])\n    sns.lineplot(data = hist_df, x='epoch', y='recall', hue='phase', ax=axes[1,1])\n    plt.show()\n","40bb2428":"import tensorflow as tf\nimport tensorflow_addons as tfa\nfrom sklearn.metrics import classification_report\n\ntf.keras.backend.clear_session()\n\n\ndef make_model(optimizer, loss_fn, metrics, output_bias='zeros', dropout=0):\n    model = tf.keras.Sequential([\n        tf.keras.layers.Flatten(input_shape=(X_train.shape[-1],)),\n        tf.keras.layers.Dense(10, activation='relu', kernel_initializer=tf.keras.initializers.HeNormal),\n        tf.keras.layers.Dropout(dropout),\n        tf.keras.layers.Dense(6, activation='relu', kernel_initializer=tf.keras.initializers.HeNormal),\n        tf.keras.layers.Dropout(dropout),\n#         tf.keras.layers.Dense(6, activation='relu', kernel_initializer=tf.keras.initializers.HeNormal),\n#         tf.keras.layers.Dropout(dropout),\n#         tf.keras.layers.Dense(3, activation='tanh', kernel_initializer=tf.keras.initializers.HeNormal),\n#         tf.keras.layers.Dropout(dropout),\n        tf.keras.layers.Dense(1, activation='sigmoid', kernel_initializer=tf.initializers.GlorotUniform, bias_initializer=output_bias)\n    ])\n    \n    model.compile(\n        optimizer=optimizer,\n        loss=loss_fn,\n        metrics=metrics\n    )\n    \n    return model\n\n\nloss_fn = tfa.losses.SigmoidFocalCrossEntropy(from_logits=False)\n# loss_fn = tf.losses.BinaryCrossentropy(from_logits=False)\n\nf1_score_tf = tfa.metrics.F1Score(num_classes=1, average='macro')\npres = tf.keras.metrics.Precision()\nrec = tf.keras.metrics.Recall()\nauc = tf.keras.metrics.AUC()\nmetrics = ['accuracy', pres, rec, f1_score_tf, auc]\n\noptimizer = tf.keras.optimizers.SGD(learning_rate=.1, momentum=1)\n\nmodel = make_model(optimizer, loss_fn, metrics)\nmodel.summary()","30206976":"# Initializing The final layer:\noutput_initializer = tf.keras.initializers.Constant(np.log(imbalance_ratio)) # sigmoid(ln(x))=x\nmodel = make_model(optimizer, loss_fn, metrics, output_initializer)\nmodel.save_weights('initial_weights')","3a3b3460":"result = model.evaluate(X_train, y_train, batch_size=256)","85d61a8e":"model.layers[-1].bias.assign([0])\nresult = model.evaluate(X_train, y_train, batch_size=256)","19f42e23":"# Overfitting a single batch of 10 rows\nmodel.load_weights('initial_weights')\nhistory = model.fit(X_train[97:107], y_train[97:107], epochs=100, batch_size=256, verbose=2)","df9be225":"rcParams['figure.figsize'] = 7, 5\ngrid = sns.lineplot(data = history.history['loss'])\ngrid.set(yscale='log')\nplt.xlabel('epochs')\nplt.ylabel('loss')\nplt.show()","ff08b084":"np.column_stack((np.around(model.predict(X_train[97:107]), 3), y_train[97:107]))","b4d04cf5":"# defining the class_weights\nclass_weight = {0: 1, 1: 1\/imbalance_ratio}\nclass_weight","df28e341":"model = make_model(tf.keras.optimizers.Adam(learning_rate=1e-2), loss_fn, metrics, output_initializer)\nmodel.load_weights('initial_weights')\nhistory = model.fit(X_train, y_train, epochs=300, batch_size=1024, verbose=2, class_weight=class_weight, validation_data=(X_test, y_test))","1d4a3698":"visu_history(history)","a2014864":"callback = tf.keras.callbacks.EarlyStopping(monitor='val_auc', patience=400, mode='max', restore_best_weights=True, verbose=1)\nmodel = make_model(tfa.optimizers.AdamW(learning_rate=1e-2, weight_decay=5e-4), loss_fn, metrics, output_initializer, dropout=.2)\nmodel.load_weights('initial_weights')\nhistory = model.fit(\n    X_train, y_train, epochs=400, batch_size=256, class_weight=class_weight, \n    callbacks=[callback], validation_data=(X_test, y_test), verbose=2\n)","9a3acb54":"visu_history(history)","912bd083":"y_pred = model.predict(X_train)\nprint(classification_report(y_train, np.around(y_pred)))\nprint('roc_auc score: ', end='')\nprint(roc_auc_score(y_train, y_pred))\nprint('f1 score:', f1_score(y_train,np.around(y_pred)), end='\\n\\n')\n\ny_pred = model.predict(X_test)\nprint(classification_report(y_test, np.around(y_pred)))\nprint('roc_auc score: ', end='')\nprint(roc_auc_score(y_test, y_pred))\nprint('f1 score:', f1_score(y_test,np.around(y_pred)), end='\\n\\n')","1d6fb308":"## Initializing the output layer bias:","b6dec3de":"Applying weight decay, dropout of .2, and early stopping:","d168d7d5":"# Conclusion:","feb9a049":"Walkthrough the notebook:\n1. <a href=\"#eda\">Exploratory Data Analysis<\/a>\n2. <a href=\"#preproc\">Preprocessing the data<\/a>\n    * <a href=\"#format\">Formatting the features<\/a>\n    * <a href=\"#fillna\">Filling the missing values<\/a>\n    * <a href=\"#datanorm\">Data Normalization<\/a>\n    * <a href=\"#train_dev_split\">Train dev splits<\/a>\n3. <a href=\"#SVC\">Classification using SVC<\/a>\n4. <a href=\"#randForest\">Classification using Random Forest<\/a>\n5. <a href=\"#ANN\">Classification using ANN<\/a>","706f9e65":"Comments and Critics are welcome.\n\nThank you for your time!","643bc7fc":"### 2) Smoking status:","19292bcf":"## Distribution of the features:","a92875ba":"## Filing in missing values (KNN algorithm):<a id=\"fillna\"><\/a>\n\n### 1) BMI","b187e544":"Clearly the model with the initialized bias has a better initial loss.","ba1dc32e":"## Checking for null values:","2b335b83":"# 3. Classification using SVC <a id=\"SVC\"><\/a>","8e4aa598":"In this notebook we will go through the data of the \"Stroke prediction\" dataset. First we'll explore the data and visualize it, then filling the missing values using KNN. \nLast we'll use ANN to predict the stroke of a given individual.","6599c0dc":"## Overfitting the model on 10 rows of data (test phase):","4cd2fde1":"First we'll define a function to visualize the history of our model (performance):","dc3530e4":"In this notebook we looked at the data of of stroke prediction, understood it doing EDA, preprocessed it and applied different machine learning models (polynomial SVM Classifier, Random Forest, ANN) with a special emphasis on the ANN model. The results of the models were pretty much the same (AUC score of .82), arguabely acceptable given that the data is highly imbalanced.","923b3e1a":"## Regularization:","9cf1980f":"# 1. Exploratory Data Analysis: <a id=\"eda\"><\/a>","9edea011":"# 2. Preprocessing the data: <a id=\"preproc\"><\/a>","0ae90842":"Very important step!","f390f176":"## Overfitting the whole dataset:","64b545f8":"We can further improve the performance of the models by applying these steps:\n* Tuning the hyperparameters for the ann\n* Data agmentation \/ Oversampling \/ Underesampling\n* Ensemble","ba665dd8":"# 4. Cassification using Random Forest: <a id=\"randForest\"><\/a>","dbf61af6":"# Introduction","7b820d98":"## Data Normalization: <a id=\"datanorm\"><\/a>","8ce20d2b":"## Train and Dev Splits: <a id=\"train_dev_split\"><\/a>","8e113e8f":"# 5. Cassification using ANN: <a id=\"ANN\"><\/a>","8d2c7715":"Since there's an important ratio of people with the smoking status 'Unknown', we'll apply KNN in order to fill their smoking status:","155806d3":"## Formatting the features: <a id=\"format\"><\/a>"}}