{"cell_type":{"f1256ab8":"code","1600d36a":"code","202d7790":"code","152a9d39":"code","f1e65617":"code","a85d8a8c":"code","7495e068":"code","0612e0c8":"code","93dfa0f4":"code","26d81b42":"code","22fa2d85":"code","55ed7228":"code","145c8004":"code","4c1d15a7":"code","82e8dab6":"code","379a28b8":"code","c32a866f":"code","f4055edd":"code","cb79aeb5":"code","76a0f2a6":"code","ab9948b2":"code","0ec0966b":"code","c619d1f2":"code","77643b45":"code","0f39bee7":"code","21967f86":"code","0cb8f7ab":"code","7da778a1":"code","7271d25c":"code","b1a441d2":"code","b0c01b0b":"code","73c4a484":"code","e1af3334":"code","a1ba96f8":"code","9e6dc092":"code","bfa3b50f":"code","4191788c":"markdown","6a08b148":"markdown","ed8f5111":"markdown","0674f657":"markdown","299cf828":"markdown","28ddb40f":"markdown","e112455e":"markdown","49321739":"markdown","257dcd90":"markdown","40b5132a":"markdown"},"source":{"f1256ab8":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n#for dirname, _, filenames in os.walk('\/kaggle\/input'):\n    #for filename in filenames:\n        #print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","1600d36a":"from IPython.core.magic import register_cell_magic\n@register_cell_magic\ndef skip(line, cell=None):\n    '''Skips execution of the current line\/cell if line evaluates to True.'''\n    if eval(line):\n        return\n        \n    get_ipython().run_cell(cell)","202d7790":"import pandas as pd\nimport tensorflow as tf\nfrom tensorflow import keras \nfrom tensorflow.keras import backend as K \nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os, cv2\nimport random\nimport dill\nimport gc\nimport time\nfrom kaggle_datasets import KaggleDatasets\nimport tensorflow_datasets.public_api as tfds\nimport math\nfrom tqdm.notebook import tqdm\nimport tensorflow_addons as tfa\nfrom tensorflow.python.distribute import values as value_lib\nfrom mt_utils import CstTokenizer","152a9d39":"seed=123456789\nrandom.seed(seed)\nnp.random.seed(seed)\ntf.random.set_seed(seed)\nos.environ['TF_ENABLE_AUTO_MIXED_PRECISION'] = '1'","f1e65617":"# NEW on TPU in TensorFlow 24: shorter cross-compatible TPU\/GPU\/multi-GPU\/cluster-GPU detection code\ntpu = None\ntry: # detect TPUs\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect() # TPU detection\n    strategy = tf.distribute.TPUStrategy(tpu)\nexcept ValueError: # detect GPUs\n    #strategy = tf.distribute.MirroredStrategy() # for GPU or multi-GPU machines\n    strategy = tf.distribute.get_strategy() # default strategy that works on CPU and single GPU\n    #strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy() # for clusters of multi-GPU machines\n\n    \n#strategy,tpu = tf.distribute.MirroredStrategy(devices=[\"TPU:0\", \"TPU:1\",\"TPU:2\"]),True\n\nprint(\"Number of accelerators: \", strategy.num_replicas_in_sync)\n\n\nAUTO = tf.data.experimental.AUTOTUNE\nREPLICAS = strategy.num_replicas_in_sync\nprint(f'REPLICAS: {REPLICAS}')","a85d8a8c":"GCS_PATH = KaggleDatasets().get_gcs_path('mtcustomvocabimg')\nSUBMIT = True","7495e068":"start = '<start>'\nend = '<end>'\nmax_seq = 393","0612e0c8":"# tokenizer\ntokenizer = CstTokenizer()\nstart_index = tokenizer.word_index[start]\nend_index = tokenizer.word_index[end]\ntokenizer.word_index","93dfa0f4":"class TrainDataset(tfds.core.GeneratorBasedBuilder):\n    VERSION = tfds.core.Version('0.1.0')\n    \n    def _split_generators(self, dl_manager):\n        return [\n            tfds.core.SplitGenerator(\n                    name=f'train',\n                    gen_kwargs={\n                    },\n            )\n        ]\n    \n    def _info(self):\n        return tfds.core.DatasetInfo(\n            builder=self,\n            description=(\"\"),\n            features=tfds.features.FeaturesDict({\n                \"image\": tfds.features.Image(shape=(None,None,1)),\n                \"target\": tfds.features.Tensor(shape=(max_seq,),dtype=tf.int8),\n            }),\n        )\n    \n    def _generate_examples(self,**args):\n        pass\n    \n\nclass TestDataset(tfds.core.GeneratorBasedBuilder):\n    VERSION = tfds.core.Version('0.1.0')\n    \n    def _split_generators(self, dl_manager):\n        return [\n            tfds.core.SplitGenerator(\n                    name=f'test',\n                    gen_kwargs={\n                    },\n            )\n        ]\n    \n    def _info(self):\n        return tfds.core.DatasetInfo(\n            builder=self,\n            description=(\"\"),\n            features=tfds.features.FeaturesDict({\n                \"image\": tfds.features.Image(shape=(None,None,1),),\n                \"image_id\": tfds.features.Text(),\n            }),\n        )\n    \n    def _generate_examples(self,**args):\n        pass","26d81b42":"BATCH_SIZE_PER_REPLICA = 512\nGLOBAL_BATCH_SIZE = BATCH_SIZE_PER_REPLICA * strategy.num_replicas_in_sync\ntrain_steps = 2424186\/\/GLOBAL_BATCH_SIZE\nBUFFER_SIZE = 10000\nTRAIN_IMAGE_MODEL = True\n\nprefetch = 50\nHEIGHT = 320\nWIDTH = 320","22fa2d85":"# Feel free to change these parameters according to your system's configuration\nembedding_dim = 256\nvocab_size = len(tokenizer.index_word)+1\nattention_features_shape = 256\nrnn_units = 512","55ed7228":"def data_augment(image):\n    p_rotation = tf.random.uniform([], 0, 1.0, dtype=tf.float32)\n    p_noise = tf.random.uniform([], 0, 1.0, dtype=tf.float32)\n    p_flip1 = tf.random.uniform([], 0, 1.0, dtype=tf.float32)\n    p_flip2 = tf.random.uniform([], 0, 1.0, dtype=tf.float32)\n            \n    # Rotation\n    if p_rotation > .2:\n        image = rotation(image)\n        \n    # Flip\n    if p_flip1 > .4:\n        image = tf.image.random_flip_left_right(image, seed)\n        \n    # Flip\n    if p_flip2 > .4:\n        image = tf.image.random_flip_up_down(image, seed)\n        \n    # Resize \n    image = tf.image.resize(image,(WIDTH, HEIGHT))\n            \n    # Noise\n    if p_noise >= .4:\n        image = random_noise(image)\n        \n    return image\n\ndef rotation(img, rotation=0.2):\n    rotation = tf.random.uniform([], -1.0, 1.0, dtype=tf.float32)*rotation\n    shape = tf.shape(img)\n    h,w = shape[0],shape[1]\n    # Pad the image with zeros to avoid losing some pixels after rotation. \n    # This will double the image width and height\n    img = tf.image.pad_to_bounding_box(img,h\/\/2, w\/\/2,h*2, w*2)\n    img = tfa.image.rotate(img,rotation,fill_value=0)\n    # Now remove the zero pads\n    return remove_pad(img)\n\n\ndef remove_pad(arr,pad_value = 0.0):\n    arr_masked = tf.reduce_all(arr != pad_value , axis=-1)\n    #x\n    y = tf.argmax(arr_masked, axis=1)\n    y = tf.where(y)\n    y_min,y_max = y[0,0],y[-1,0]+1\n    #y\n    x = tf.argmax(arr_masked, axis=0)\n    x = tf.where(x)\n    x_min,x_max = x[0,0],x[-1,0]+1\n    arr = arr[y_min:y_max,x_min:x_max]\n    return arr\n\ndef random_noise(img,p=0.01):\n    shape = tf.shape(img)\n    choice = tf.random.categorical(tf.math.log([[p, 1-p]]), tf.size(img),dtype=tf.int32)\n    noise = tf.random.categorical(tf.math.log([[1., 1.]]), tf.size(img),dtype=tf.int32)\n    choice = tf.reshape(choice,shape)\n    noise = tf.reshape(noise,shape)\n    noise = tf.abs(choice-1)*noise\n    choice = tf.cast(choice,img.dtype)\n    noise = tf.cast(noise,img.dtype)\n    return (choice*img)+noise","145c8004":"with strategy.scope():\n    load_locally = tf.saved_model.LoadOptions(experimental_io_device='\/job:localhost')\n    #image_model = tf.keras.models.load_model('..\/input\/mt-pretraining\/EfficientNetB0.h5',options=load_locally,compile=False)\n    image_input = tf.keras.layers.Input(shape=(WIDTH,HEIGHT,1))\n    image_model = tf.keras.applications.EfficientNetB1(include_top=False,weights=None,input_shape=(WIDTH,HEIGHT,1),)\n    image_model = image_model(image_input)\n    \n    image_model = tf.keras.layers.Reshape((attention_features_shape,-1))(image_model)\n    image_model = tf.keras.Model(image_input,image_model)\n    image_model.compile()\n    image_model.summary()","4c1d15a7":"def get_dataset(_):\n    builder = TrainDataset(data_dir=GCS_PATH)\n    # The following line download the dataset\n    builder.download_and_prepare()\n    dataset = builder.as_dataset()['train']\n\n    # normalize, shuffle and bacth\n    def preprecoss(x):\n        img,target = x['image'],x['target']\n        # Normalize : There are two pixels 0 and 255\n        img = tf.cast(img == 0,tf.float32)\n        \n        # label\n        target = tf.cast(target, tf.int32 )\n        return data_augment(img),target\n        \n        return data_augment(img),target\n    dataset = dataset.repeat().shuffle(BUFFER_SIZE).map(preprecoss,num_parallel_calls=AUTO)\n    dataset = dataset.batch(BATCH_SIZE_PER_REPLICA).prefetch(prefetch)\n    return dataset\n\nwith strategy.scope():\n    if tpu is None:\n        dataset = get_dataset(0)\n    else:\n        dataset = strategy.experimental_distribute_datasets_from_function(get_dataset)\n    train_iterator = iter(dataset)","82e8dab6":"%%time\ninputs = next(train_iterator)","379a28b8":"class CNN_Encoder(tf.keras.Model):\n    # Since you have already extracted the features and dumped it using pickle\n    # This encoder passes those features through a Fully connected layer\n    def __init__(self, embedding_dim):\n        super(CNN_Encoder, self).__init__()\n        self.lstm_layer = tf.keras.layers.LSTM(rnn_units,\n                                   return_sequences=True,\n                                   return_state=True,\n                                   recurrent_initializer='glorot_uniform')\n\n\n    def call(self, x):\n        output, h, c = self.lstm_layer(x)\n        return output, h, c\n    \n    def get_config(self):\n        return {\n            'lstm_layer': self.lstm_layer,\n        }","c32a866f":"class RNN_Decoder(tf.keras.Model):\n\n    def __init__(self, embedding_dim, vocab_size):\n        super(RNN_Decoder, self).__init__()\n\n        self.dec_units = rnn_units\n        self.attention_type = 'luong'\n\n        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n        \n        # Final Dense layer on which softmax will be applied\n        self.fc = tf.keras.Sequential([\n            tf.keras.layers.Dropout(0.5),\n            tf.keras.layers.Dense(vocab_size)\n        ])\n        \n        # Create attention mechanism with memory = None\n        memory_sequence_length = None# GLOBAL_BATCH_SIZE * [max_seq]\n        self.attention_mechanism = self.build_attention_mechanism(self.dec_units,None, memory_sequence_length,self.attention_type)\n\n        # Define the fundamental cell for decoder recurrent structure\n        self.decoder_rnn_cell = tf.keras.layers.LSTMCell(rnn_units)\n        # Wrap attention mechanism with the fundamental rnn cell of decoder\n        self.rnn_cell = self.build_rnn_cell()\n\n        # Sampler\n        self.sampler = tfa.seq2seq.sampler.TrainingSampler()\n\n        # Define the decoder with respect to fundamental rnn cell\n        self.decoder = tfa.seq2seq.BasicDecoder(self.rnn_cell, sampler=self.sampler, output_layer=self.fc,maximum_iterations=max_seq-1)\n\n\n    def call(self, inputs, initial_state):\n        x = self.embedding(inputs)\n        sequence_length = tf.repeat(max_seq - 1,tf.shape(x)[0])\n        outputs, _, _ = self.decoder(x, initial_state=initial_state,\n                                     sequence_length=sequence_length)\n        return outputs\n\n    def build_rnn_cell(self):\n        rnn_cell = tfa.seq2seq.AttentionWrapper(self.decoder_rnn_cell,\n                                                self.attention_mechanism, attention_layer_size=self.dec_units)\n        return rnn_cell\n\n    def build_attention_mechanism(self, dec_units, memory, memory_sequence_length=None, attention_type='luong'):\n        # ------------- #\n        # typ: Which sort of attention (Bahdanau, Luong)\n        # dec_units: final dimension of attention outputs \n        # memory: encoder hidden states of shape (batch_size, max_length_input, enc_units)\n        # memory_sequence_length: 1d array of shape (batch_size) with every element set to max_length_input (for masking purpose)\n\n        if (attention_type == 'bahdanau'):\n            return tfa.seq2seq.BahdanauAttention(units=dec_units, memory=memory,\n                                                 memory_sequence_length=memory_sequence_length)\n        else:\n            return tfa.seq2seq.LuongAttention(units=dec_units, memory=memory,\n                                              memory_sequence_length=memory_sequence_length)\n\n    def build_initial_state(self, batch_sz,encoder_state):\n        decoder_initial_state = self.rnn_cell.get_initial_state(batch_size=batch_sz, dtype=tf.float32)\n        decoder_initial_state = decoder_initial_state.clone(cell_state=encoder_state)\n        return decoder_initial_state\n\n    def reset_state(self, batch_size):\n        return tf.zeros((batch_size, rnn_units))\n\n    def get_config(self):\n        return {\n            'units': self.units,\n            'embedding': self.embedding,\n            'rnn': self.rnn,\n            'fc': self.fc,\n            'attention': self.attention, }\n","f4055edd":"with strategy.scope():\n    optimizer = tf.keras.optimizers.Adam(0.0025)\n    crossentropy = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction=tf.keras.losses.Reduction.NONE)\n\n    #@tf.function\n    def loss_function(real, pred):\n        mask = tf.math.logical_not(tf.math.equal(real, 0))\n        loss_ = crossentropy(real, pred)\n        \n        mask = tf.cast(mask, dtype=loss_.dtype)\n        loss_ *= mask\n      \n        return tf.nn.compute_average_loss(loss_, global_batch_size=GLOBAL_BATCH_SIZE)\n        #return tf.reduce_mean(loss_)","cb79aeb5":"with strategy.scope():\n    bs = 2\n    encoder = CNN_Encoder(embedding_dim,)\n    sample_output = image_model(tf.zeros((bs,WIDTH,HEIGHT,1),tf.float32))\n    sample_output, sample_h, sample_c = encoder(sample_output)\n    print(\"Encoder Outputs Shape: \", sample_output.shape)","76a0f2a6":"with strategy.scope():\n    decoder = RNN_Decoder(embedding_dim, vocab_size)\n    sample_x = tf.random.uniform((bs, max_seq))\n    decoder.attention_mechanism.setup_memory(sample_output)\n    initial_state = decoder.build_initial_state(bs,[sample_h, sample_c],)\n\n    sample_decoder_outputs = decoder(sample_x, initial_state)\n\n    print(\"Decoder Outputs Shape: \", sample_decoder_outputs.rnn_output.shape)\n","ab9948b2":"checkpoint_name = \".\/model\"\n!cp -r ..\/input\/mt-fast-distributed-training-tpu\/*.dill .\n!cp -r ..\/input\/mt-fast-distributed-training-tpu\/*.h5 .","0ec0966b":"def save_model(name=checkpoint_name):\n    encoder.save_weights(f'{name}_encoder.h5', options=save_locally)\n    decoder.save_weights(f'{name}_decoder.h5', options=save_locally)\n    image_model.save_weights('image_model.h5', options=save_locally)\n    \ndef load_model(name=checkpoint_name):\n    encoder.load_weights(f'{name}_encoder.h5', options=load_locally)\n    decoder.load_weights(f'{name}_decoder.h5', options=load_locally)\n    image_model.load_weights('image_model.h5', options=load_locally)\n    return encoder, decoder,image_model","c619d1f2":"with strategy.scope():\n    save_locally = tf.saved_model.SaveOptions(experimental_io_device='\/job:localhost')\n    load_locally = tf.saved_model.LoadOptions(experimental_io_device='\/job:localhost')\n    encoder, decoder, image_model = load_model()","77643b45":"@tf.function(experimental_relax_shapes=True)\ndef train_step(inputs):\n    img_tensor, target = inputs\n    loss = 0.\n    \n    batch_sz = tf.shape(img_tensor)[0]\n    \n    #image features\n    if not TRAIN_IMAGE_MODEL:\n        img_tensor = image_model(img_tensor,training=False)\n\n    with tf.GradientTape() as tape:\n        \n        if TRAIN_IMAGE_MODEL:\n                img_tensor = image_model(img_tensor,training=True)\n                \n        enc_output, enc_h, enc_c = encoder(img_tensor)\n\n        dec_input = target[:, :-1]  # Ignore <end> token\n        real = target[:, 1:]  # ignore <start> token\n\n        # Set the AttentionMechanism object with encoder_outputs\n        decoder.attention_mechanism.setup_memory(enc_output)\n\n        # Create AttentionWrapperState as initial_state for decoder\n        decoder_initial_state = decoder.build_initial_state(batch_sz, [enc_h, enc_c])\n        pred = decoder(dec_input, decoder_initial_state)\n        logits = pred.rnn_output\n        loss = loss_function(real, logits)\n\n    variables = encoder.trainable_variables + decoder.trainable_variables\n    gradients = tape.gradient(loss, variables)\n    optimizer.apply_gradients(zip(gradients, variables))\n\n    return loss\n\n@tf.function(experimental_relax_shapes=True)\ndef distributed_train_step(inputs,):\n    per_replica_losses = strategy.run(train_step, args=(inputs,))\n    return  strategy.reduce(tf.distribute.ReduceOp.MEAN, per_replica_losses,axis=None)","0f39bee7":"loss_plot = []\nif os.path.exists('losses.dill'):\n    loss_plot = dill.load(open('losses.dill','rb'))","21967f86":"EPOCHS = 10\nbest_loss = 0\nfor epoch in range(EPOCHS):\n    start = time.time()\n    total_loss = 0.0\n    print(f'Epoch {epoch + 1}\/{EPOCHS}')\n    for batch in range(train_steps):\n        inputs = next(train_iterator)\n        loss = distributed_train_step(inputs).numpy()\n        total_loss += loss\n            \n        if batch % 1==0:\n            print(f'{batch+1}\/{train_steps} Total Loss: {total_loss\/(batch+1):.4f}  Batch Loss: {loss:.4f}',end='\\r')\n            \n    # storing the epoch end loss value to plot later\n    loss_plot.append(total_loss \/ train_steps)\n    print()\n    if best_loss >= total_loss:\n        print(\"Saving...\")\n        save_model()\n        best_loss = total_loss\n\n    print(f'Epoch {epoch + 1} Loss {total_loss\/train_steps:.6f}')\n    print(f'Time taken for 1 epoch {time.time() - start} sec\\n')","0cb8f7ab":"save_model()","7da778a1":"plt.plot(loss_plot)\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.title('Loss Plot')\nplt.show()","7271d25c":"dill.dump(loss_plot,open('losses.dill','wb'))","b1a441d2":"# Get test dataset\ndef get_test_dataset(_):\n    builder = TestDataset(data_dir=GCS_PATH)\n    # The following line download the dataset\n    builder.download_and_prepare()\n    dataset = builder.as_dataset()['test']\n\n    # normalize, shuffle and bacth\n    def preprecoss(x):\n        img = x['image']\n        \n        #Normalize\n        img = tf.cast(img == 0,tf.float32)\n        im_size = tf.shape(img)\n        w,h = im_size[0],im_size[1]\n        if h > w:\n            img = tf.image.transpose(img)\n            img = tf.image.flip_up_down(img)\n        \n        img = tf.image.resize(img,(WIDTH, HEIGHT))\n        return img,x['image_id']\n    \n    dataset = dataset.map(preprecoss,num_parallel_calls=AUTO)\n    dataset = dataset.batch(BATCH_SIZE_PER_REPLICA)\n    dataset = dataset.prefetch(prefetch)\n    return dataset\n\nwith strategy.scope():\n    test_dataset = strategy.experimental_distribute_datasets_from_function(get_test_dataset)\n    #test_iterator = iter(test_dataset)","b0c01b0b":"@tf.function(experimental_relax_shapes=True)\ndef greedy_predict(img_tensor):\n    inference_batch_size = img_tensor.shape[0]\n    img_tensor = image_model(img_tensor,training=False)\n    \n    enc_out, enc_h, enc_c = encoder(img_tensor)\n    dec_h = enc_h\n    dec_c = enc_c\n    \n    start_tokens = tf.fill([inference_batch_size], start_index)\n    end_token = end_index\n    \n    greedy_sampler = tfa.seq2seq.GreedyEmbeddingSampler()\n    \n    # Instantiate BasicDecoder object\n    decoder_instance = tfa.seq2seq.BasicDecoder(cell=decoder.rnn_cell, sampler=greedy_sampler, output_layer=decoder.fc,maximum_iterations=max_seq)\n    # Setup Memory in decoder stack\n    decoder.attention_mechanism.setup_memory(enc_out)\n\n    # set decoder_initial_state\n    decoder_initial_state = decoder.build_initial_state(inference_batch_size, [enc_h, enc_c])\n\n    ### Since the BasicDecoder wraps around Decoder's rnn cell only, you have to ensure that the inputs to BasicDecoder \n    ### decoding step is output of embedding layer. tfa.seq2seq.GreedyEmbeddingSampler() takes care of this. \n    ### You only need to get the weights of embedding layer, which can be done by decoder.embedding.variables[0] and pass this callabble to BasicDecoder's call() function\n\n    decoder_embedding_matrix = decoder.embedding.variables[0]\n\n    outputs, _, _ = decoder_instance(decoder_embedding_matrix, start_tokens = start_tokens, end_token= end_token, initial_state=decoder_initial_state)\n    return outputs.sample_id#.numpy()","73c4a484":"@tf.function(experimental_relax_shapes=True)\ndef beam_predict(img_tensor,beam_width=3):\n    inference_batch_size = img_tensor.shape[0]\n    img_tensor = image_model(img_tensor,training=False)\n    \n    enc_out, enc_h, enc_c = encoder(img_tensor)\n\n    dec_h = enc_h\n    dec_c = enc_c\n\n    start_tokens = tf.fill([inference_batch_size], start_index)\n    end_token = end_index\n\n    # From official documentation\n    # NOTE If you are using the BeamSearchDecoder with a cell wrapped in AttentionWrapper, then you must ensure that:\n    # The encoder output has been tiled to beam_width via tfa.seq2seq.tile_batch (NOT tf.tile).\n    # The batch_size argument passed to the get_initial_state method of this wrapper is equal to true_batch_size * beam_width.\n    # The initial state created with get_initial_state above contains a cell_state value containing properly tiled final state from the encoder.\n\n    enc_out = tfa.seq2seq.tile_batch(enc_out, multiplier=beam_width)\n    decoder.attention_mechanism.setup_memory(enc_out)\n    print(\"beam_with * [batch_size, max_length_input, rnn_units] :  3 * [1, 16, 1024]] :\", enc_out.shape)\n\n    # set decoder_inital_state which is an AttentionWrapperState considering beam_width\n    hidden_state = tfa.seq2seq.tile_batch([enc_h, enc_c], multiplier=beam_width)\n    print(beam_width*inference_batch_size)\n    decoder_initial_state = decoder.rnn_cell.get_initial_state(batch_size=beam_width*inference_batch_size,dtype=tf.float32)\n    decoder_initial_state = decoder_initial_state.clone(cell_state=hidden_state)\n\n    # Instantiate BeamSearchDecoder\n    decoder_instance = tfa.seq2seq.BeamSearchDecoder(decoder.rnn_cell,beam_width=beam_width, output_layer=decoder.fc,maximum_iterations=max_seq)\n    decoder_embedding_matrix = decoder.embedding.variables[0]\n\n    # The BeamSearchDecoder object's call() function takes care of everything.\n    outputs, final_state, sequence_lengths = decoder_instance(decoder_embedding_matrix, start_tokens=start_tokens, end_token=end_token, initial_state=decoder_initial_state)\n    # outputs is tfa.seq2seq.FinalBeamSearchDecoderOutput object. \n    # The final beam predictions are stored in outputs.predicted_id\n    # outputs.beam_search_decoder_output is a tfa.seq2seq.BeamSearchDecoderOutput object which keep tracks of beam_scores and parent_ids while performing a beam decoding step\n    # final_state = tfa.seq2seq.BeamSearchDecoderState object.\n    # Sequence Length = [inference_batch_size, beam_width] details the maximum length of the beams that are generated\n\n\n    # outputs.predicted_id.shape = (inference_batch_size, time_step_outputs, beam_width)\n    # outputs.beam_search_decoder_output.scores.shape = (inference_batch_size, time_step_outputs, beam_width)\n    # Convert the shape of outputs and beam_scores to (inference_batch_size, beam_width, time_step_outputs)\n    final_outputs = tf.transpose(outputs.predicted_ids, perm=(0,2,1))\n    beam_scores = tf.transpose(outputs.beam_search_decoder_output.scores, perm=(0,2,1))\n\n    return final_outputs, beam_scores","e1af3334":"@tf.function(experimental_relax_shapes=True)\ndef distributed_greedy_predict(inputs):\n    return strategy.run(greedy_predict, args=(inputs,))\n\n\n@tf.function(experimental_relax_shapes=True)\ndef distributed_beam_predict(inputs):\n    return strategy.run(beam_predict, args=(inputs,))","a1ba96f8":"def process_predict(x):\n    x = x.numpy()\n    \n    res = []\n    for i in range(len(x)):\n        tmp = x[i]\n        idx = np.argwhere(tmp == end_index)\n        # Remove sos, eos and pad \n        if idx.size != 0:\n            idx = idx[0,0]\n            tmp = tmp[:idx]\n            \n        tmp = tmp[tmp!=0]\n        tmp = tmp[tmp!=start_index]\n        tmp = tmp[tmp!=end_index]\n        res.append(tmp)\n    captions = tokenizer.detokenize(res)\n    captions = [f'InChI=1S\/{c}'  for c in captions]\n    return captions","9e6dc092":"%%skip not SUBMIT\nresults = []\nimages = []\n\nfor (image,iids) in tqdm(test_dataset,total=1616107\/\/GLOBAL_BATCH_SIZE):\n    if type(image) == value_lib.PerReplica:\n        stop = False\n        for v in image.values:\n            if v.shape[0] == 0:\n                stop = True\n        if stop:\n            print('Stopping...')\n            break\n        res = distributed_greedy_predict(image)\n        res = tf.concat(strategy.unwrap(res),axis=0)\n        iids = tf.concat(strategy.unwrap(iids),axis=0)\n    else:\n        res = greedy_predict(image)\n    \n    res = process_predict(res)\n    results.extend(res)\n    images.extend([s.decode('utf-8') for s in iids.numpy()])\n\n    \n# Process the last batch\nif type(image) == value_lib.PerReplica:\n    image_values,iids_values =  image.values,iids.values\n\n    for i in range(strategy.num_replicas_in_sync):\n        if image_values[i].shape[0] == 0:\n            continue\n        res = greedy_predict(image_values[i])\n        res = process_predict(res)\n        results.extend(res)\n        images.extend([s.decode('utf-8') for s in iids_values[i].numpy()])","bfa3b50f":"%%skip not SUBMIT\nsubmission_df = pd.read_csv('..\/input\/bms-molecular-translation\/sample_submission.csv',index_col=0)\nsubmission_df.loc[images,'InChI']=results\nsubmission_df.to_csv('submission.csv')","4191788c":"# Dataset","6a08b148":"# Testing","ed8f5111":"## Load the pretrained image model","0674f657":"### Create Encoder","299cf828":"### Create Decoder","28ddb40f":"# Training","e112455e":"# Submit","49321739":"# Checkpoint","257dcd90":"# Model","40b5132a":"# References:\nhttps:\/\/www.tensorflow.org\/tutorials\/distribute\/custom_training\n\nhttps:\/\/www.tensorflow.org\/guide\/tpu\n\nhttps:\/\/www.tensorflow.org\/tutorials\/text\/image_captioning\n\nhttps:\/\/www.tensorflow.org\/addons\/tutorials\/networks_seq2seq_nmt\n\n#### Dataset links : https:\/\/www.kaggle.com\/tchaye59\/mt-tfrecord-custom-vocab & https:\/\/www.kaggle.com\/tchaye59\/mtcustomvocabimg\n#### Pretraining : https:\/\/www.kaggle.com\/tchaye59\/mt-pretraining"}}