{"cell_type":{"54f408b8":"code","cd9e2195":"code","5f60a2ea":"code","36737f6f":"code","e54db653":"code","da8c9536":"code","d776e6fc":"code","c0ea4c85":"code","f72238d9":"code","24352873":"code","1ce3da38":"code","c2c0e789":"markdown","fec4d9d1":"markdown","bbfeb5b3":"markdown","97bf09be":"markdown","5776c963":"markdown","98d44b06":"markdown","5c75cacd":"markdown","23269d31":"markdown","27b137c7":"markdown","8c1f38c2":"markdown","379fa2aa":"markdown","3b617e04":"markdown","e3fceda0":"markdown"},"source":{"54f408b8":"import torch\nimport torchvision\nfrom torch.utils.data import DataLoader\nfrom torchvision.datasets import MNIST\nfrom torchvision import transforms\nfrom torch.optim import SGD\nimport torch.optim as optim\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt","cd9e2195":"# set the random seed\nrandom_seed = 42\ntorch.backends.cudnn.enable = False\ntorch.manual_seed(random_seed)","5f60a2ea":"# set the batch size (could be other size)\nbatch_size = 512\n\n# load original data from Pytorch \ntrain_load = MNIST('.', train=True, download=True, \n                                transform=transforms.Compose([transforms.ToTensor()])) # here we transform To Tensor\ntest_load = MNIST('.', train=False, download=True, \n                                transform=transforms.Compose([transforms.ToTensor()]))\n\n# Use Pytorch loader with batch size \ntrain_ = DataLoader(train_load, batch_size=batch_size, shuffle=True)\ntest_ = DataLoader(test_load, batch_size=batch_size, shuffle=True)\n\n# Set simple dic to call \nsizes = {'train' : train_load.data.shape[0], 'test' : test_load.data.shape[0]}\nloaders = {'train' : train_, 'test': test_}","36737f6f":"train_loader = DataLoader(MNIST('.', train=True, download=True, \n                                transform=transforms.Compose([transforms.ToTensor()])),\n                          batch_size=batch_size, shuffle=True)\n\nexamples = enumerate(train_loader)\ni, (data, target) = next(examples)","e54db653":"%matplotlib inline","da8c9536":"import matplotlib.pyplot as plt\nfigure = plt.figure()\nnum_of_images = 60\nfor index in range(1, num_of_images + 1):\n    plt.subplot(6, 10, index)\n    plt.axis('off')\n    plt.imshow(data[index].numpy().squeeze(), cmap='gray_r')","d776e6fc":"import torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass NeuralNetwork(nn.Module):\n\n    def __init__(self, activation='sigmoid', hidden_size=256):\n        '''Args: activation: {sigmoid, relu, tanh} default sigmoid. \n          Defines which activation function to use.\n        hidden_size: default 256 Defines the size of hidden layer.'''\n        super(NeuralNetwork, self).__init__()\n        \n        self.activation = activation\n        self.hidden_size = hidden_size\n        self.input = nn.Linear(in_features=784, out_features=self.hidden_size)\n        self.hidden = nn.Linear(in_features=self.hidden_size, out_features=self.hidden_size)\n        self.output = nn.Linear(in_features=self.hidden_size, out_features=10)\n    \n    def forward(self, X):\n        x = self.input(X) # z = W*X + b        \n        a = self.activation(x) # a = f(z)\n        z = self.hidden(a) # z1 = W*A + b\n        a = self.activation(z) # a1 = f(z1)\n        output = self.output(a) # output = w*a1 + b\n        probability = F.softmax(output, dim=1) # use softmax \n        \n        return output, probability  ","c0ea4c85":"# params \nepoch = 5\nlearning_rate = 0.1\nactivation = {'sigmoid':F.logsigmoid, 'relu': F.relu, 'lrelu':F.leaky_relu, 'tanh': F.tanh}\n\n# model with activation funct\nmodel = NeuralNetwork(activation=activation['lrelu']) \n\n# instances\noptimizer = SGD(model.parameters(), lr=learning_rate) # which params we want to update\ncriterion = nn.CrossEntropyLoss()\n\n# CUDA (if you have)\ndevice = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\nmodel.to(device)","f72238d9":"# initial lists\ntest_loss = []\ntest_acc = []\ntrain_acc = []\ntrain_loss = []\n\n# start iteration\nfor i in range(epoch):\n    print('Epoch # {}'.format(i))\n    # save acc and loss\n    current_loss = 0\n    current_acc = 0\n    # iterate over train  \/ test\n    for state in ['train', 'test']:\n        # iterate over loaders\n        for data, targets in loaders[state]:\n            # tranfer data to device \n            data = data.to(device) \n            target = targets.to(device) \n            # change data to shape with view\n            data = data.view(-1, 784)\n            # for predict and fit we use diff datasets\n            if state == 'train':\n            # predict \n                output, probs = model(data)\n            else: # we dont need to update  if we predict\n                with torch.no_grad():\n                    output, probs = model(data)\n            # max number of probs\n            _, preds = torch.max(probs, 1)\n            # set gradients to zero (we always do that before new batch)\n            optimizer.zero_grad()\n            # loss calculation\n            loss = criterion(output, targets) #evarage on minibatch\n            # condition of state \n            if state == 'train':\n                # metod pytorch to update \n                loss.backward()\n                # next step by pytorch\n                optimizer.step()\n            # culc loss mult on batch size \n            current_loss += loss.item() * data.size(0)\n            current_acc += torch.sum(preds == targets.data) # compare how our model predict per evelemnt \n        # calc the loss and train\n        epoch_loss_train = current_loss \/ sizes['train']\n        epoch_acc_train = current_acc.double() \/ sizes['train']\n        # for graph data train\n        train_loss.append(epoch_loss_train)\n        train_acc.append(epoch_acc_train)\n        # calc the test\n        epoch_loss_test = current_loss \/ sizes['test']\n        epoch_acc_test = current_acc.double() \/ sizes['test']\n        # for graph test\n        test_loss.append(epoch_loss_train)\n        test_acc.append(epoch_acc_train)\n        # print \n        print('Epoch TRAIN loss {} Epoch Accuracy {}'.format(np.round(epoch_loss_train, 2), np.round(epoch_acc_train, 2)))\n        print('Epoch TEST loss {} Epoch Accuracy {}'.format(np.round(epoch_loss_test, 2), np.round(epoch_acc_test, 2)))\n        print('------------------')","24352873":"plt.plot(range(1, 11), train_acc , label=['train_accouracy'])\nplt.plot(range(1, 11), train_loss, label=['train_loss'])\nplt.legend(['train acc', 'train_loss'])\nplt.xlabel('Epoch')\nplt.title('Train loss and accuracy')\nplt.show();","1ce3da38":"plt.plot(range(1, 11), test_acc , label=['test_accouracy'])\nplt.plot(range(1, 11), test_loss, label=['test_loss'])\nplt.legend(['test acc', 'test_loss'])\nplt.xlabel('Epoch')\nplt.title('test loss and accuracy')\nplt.show();","c2c0e789":"![](https:\/\/raw.githubusercontent.com\/VolodymyrGavrysh\/My_RoadMap_Data_Science\/master\/pictures\/Screenshot%20from%202020-01-21%2022-18-18.png)","fec4d9d1":"**Plan**\n1. Load MNIST dataset from Pytorch\n2. Explore the dataset\n3. Own neural network class.\n4. NN with one hidden layer.\n5. Train it.\n6. Print some training logs\n6. Use different activation functions.","bbfeb5b3":"### Model params","97bf09be":"# Look to the numbers","5776c963":"### Pytorch class","98d44b06":"### Set the random seed","5c75cacd":"Thank you for your time!","23269d31":"Epoch # 0\nEpoch TRAIN loss 1.44 Epoch Accuracy 0.65\nEpoch TEST loss 8.61 Epoch Accuracy 3.89\n------------------\nEpoch TRAIN loss 1.54 Epoch Accuracy 0.78\nEpoch TEST loss 9.24 Epoch Accuracy 4.68\n------------------\nEpoch # 1\nEpoch TRAIN loss 0.45 Epoch Accuracy 0.87\nEpoch TEST loss 2.73 Epoch Accuracy 5.25\n------------------\nEpoch TRAIN loss 0.52 Epoch Accuracy 1.02\nEpoch TEST loss 3.12 Epoch Accuracy 6.14\n------------------\nEpoch # 2\nEpoch TRAIN loss 0.35 Epoch Accuracy 0.9\nEpoch TEST loss 2.11 Epoch Accuracy 5.4\n------------------\nEpoch TRAIN loss 0.41 Epoch Accuracy 1.05\nEpoch TEST loss 2.44 Epoch Accuracy 6.3\n------------------\nEpoch # 3\nEpoch TRAIN loss 0.31 Epoch Accuracy 0.91\nEpoch TEST loss 1.86 Epoch Accuracy 5.47\n------------------\nEpoch TRAIN loss 0.36 Epoch Accuracy 1.06\nEpoch TEST loss 2.15 Epoch Accuracy 6.38\n------------------\nEpoch # 4\nEpoch TRAIN loss 0.28 Epoch Accuracy 0.92\nEpoch TEST loss 1.68 Epoch Accuracy 5.52\n------------------\nEpoch TRAIN loss 0.33 Epoch Accuracy 1.07\nEpoch TEST loss 1.97 Epoch Accuracy 6.43\n------------------\n","27b137c7":"* # Using Pytorch in simple neural network architecture.","8c1f38c2":"## Load data (if it doest work, pls run jupyter notebook)","379fa2aa":"* For learning purpose here is simple class\n* MNIST dataset by Pytorch\n* Direct iteration\n\n#### Less text, more code with some comments )","3b617e04":"### Iteration ","e3fceda0":"![](https:\/\/raw.githubusercontent.com\/VolodymyrGavrysh\/My_RoadMap_Data_Science\/master\/pictures\/Screenshot%20from%202020-01-21%2022-19-54.png)"}}