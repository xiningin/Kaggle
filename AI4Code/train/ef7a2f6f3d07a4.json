{"cell_type":{"aaf75d15":"code","347a832f":"code","c8921a66":"code","5005c8ce":"code","4102b49d":"code","bc80b533":"code","d9bc19ad":"code","5fa74a5e":"code","26d68e65":"code","9d478ac3":"code","424fbb4e":"code","42f239db":"code","7e2ebc17":"code","34fbdaa3":"code","45ad9a80":"code","253c963d":"code","56b85d1e":"code","fc6d66a5":"code","6a038675":"code","df4d6b0f":"code","27f77358":"code","a4381b68":"code","857bedfe":"code","bb7ddf2f":"code","e2bca527":"code","e570fde6":"code","88efb75b":"code","7dae55cb":"code","b0df68aa":"code","c63c907b":"code","cec12ace":"code","e5691d53":"code","8e88cbb0":"code","a887be48":"code","10c12894":"code","8a1c014f":"code","21628142":"code","4411287e":"code","cb29ad49":"code","69bdf39e":"code","3c55a140":"code","cd177868":"code","103cb614":"code","ea9f0782":"code","13222e12":"code","0c450f05":"code","a3f5aefa":"code","a04f3122":"markdown","1ad25830":"markdown","decec77f":"markdown","44c7d35f":"markdown","c35d8676":"markdown","2a52a1f5":"markdown","2fb480cb":"markdown","4f285ac8":"markdown","0c41b04d":"markdown","bc1b0588":"markdown","16baf98b":"markdown","77ebef3d":"markdown","e5678bde":"markdown","f205b9e3":"markdown","0a2eaf93":"markdown","5290781a":"markdown","e3a8dfbd":"markdown","8fa48dc4":"markdown","2a4bf988":"markdown","3e909b13":"markdown","469101fe":"markdown","11481013":"markdown","be1bcf37":"markdown","e394237c":"markdown","3f26990d":"markdown","b3cda255":"markdown","584cf7f0":"markdown","0244ce67":"markdown","f036d7dc":"markdown","c251f8e5":"markdown","7f701e8f":"markdown","a6f4da94":"markdown","c7a7a66d":"markdown","a948f9fb":"markdown","b1f2ea6d":"markdown","6c30b9fe":"markdown","8f342e98":"markdown","833c4206":"markdown","04a3bca3":"markdown"},"source":{"aaf75d15":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn.feature_extraction.text import TfidfVectorizer # converts sentences into vectors\nfrom sklearn.naive_bayes import MultinomialNB # Naive Bayes model for discrete scores\nimport re # regular expression libary\nfrom bs4 import BeautifulSoup # used to interpret websites\nfrom tqdm.auto import tqdm # progress bar\nfrom scipy.sparse import vstack # concat sparse matricies\n\n \n# imbalanced dataset methods\nfrom imblearn.under_sampling import RandomUnderSampler # used to randomly under-sample imbalanced dataset\nfrom imblearn.over_sampling import SMOTE # used to over-sample imbalanced dataset with SMOTE\nfrom imblearn.over_sampling import ADASYN # used to over-sample imbalanced dataset with ADASYN\n# ignore SettingWithCopyWarning\nimport warnings\nfrom pandas.core.common import SettingWithCopyWarning\nwarnings.simplefilter(action=\"ignore\", category=SettingWithCopyWarning)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","347a832f":"# set seed for randomness\nrseed=201","c8921a66":"from imblearn.over_sampling import SMOTE","5005c8ce":"pre_train_df1 = pd.read_csv(\n    \"\/kaggle\/input\/jigsaw-toxic-comment-classification-challenge\/train.csv\")\npre_train_df1['y'] = (\n    pre_train_df1[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']]\n    .sum(axis=1) > 0 ).astype(int)\npre_train_df1 = pre_train_df1[['comment_text', 'y']].rename(\n    columns={'comment_text': 'text'})\npre_train_df1.sample(5)","4102b49d":"pre_train_df2 = pd.read_csv(\n    \"\/kaggle\/input\/ruddit-jigsaw-dataset\/Dataset\/ruddit_with_text.csv\")\n\n# rename the columns containing the text data and score\npre_train_df2 = pre_train_df2[['txt', 'offensiveness_score']].rename(columns={'txt': 'text',\n                                                          'offensiveness_score':'score'})\n\n\n# label all comments with an offensiveness score greater than 0 as 1's, otherwise 0's\npre_train_df2['y'] = (pre_train_df2['score'] > 0 ).astype(int)\n\n# uncomment below to transform the offensiveness score to range 0-1\n#pre_train_df2['score'] = ((pre_train_df2['score'] - pre_train_df2.score.min())\n#                      \/ (pre_train_df2.score.max() - pre_train_df2.score.min()))\n\npre_train_df2.drop(['score'], axis=1, inplace=True)\npre_train_df2.sample(5)","bc80b533":"if 1==0:\n    pre_train_df3 = pd.read_csv(\n        \"\/kaggle\/input\/jigsaw-multilingual-toxic-comment-classification\/jigsaw-unintended-bias-train.csv\")\n    print(len(pre_train_df3))\n    pre_train_df3['y'] = (\n        pre_train_df3[['toxic', 'severe_toxicity', 'obscene', 'threat', 'insult', 'identity_attack']]\n        .sum(axis=1) > 0 ).astype(int)\n    pre_train_df3 = pre_train_df3[['comment_text', 'y']].rename(\n        columns={'comment_text': 'text'})\n    pre_train_df3.sample(3)\n","d9bc19ad":"def text_cleaning(text):\n    '''\n    Cleans text into a basic form for NLP. Operations include the following:-\n    1. Remove special charecters like &, #, etc\n    2. Removes extra spaces\n    3. Removes embedded URL links\n    4. Removes HTML tags\n    5. Removes emojis\n    \n    text - Text piece to be cleaned.\n    '''\n    template = re.compile(r'https?:\/\/\\S+|www\\.\\S+') #Removes website links\n    text = template.sub(r'', text)\n    \n    soup = BeautifulSoup(text, 'lxml') #Removes HTML tags\n    only_text = soup.get_text()\n    text = only_text\n    \n    emoji_pattern = re.compile(\"[\"\n                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                               u\"\\U00002702-\\U000027B0\"\n                               u\"\\U000024C2-\\U0001F251\"\n                               \"]+\", flags=re.UNICODE)\n    text = emoji_pattern.sub(r'', text)\n    \n    text = re.sub(r\"[^a-zA-Z\\d]\", \" \", text) #Remove special Charecters\n    text = re.sub(' +', ' ', text) #Remove Extra Spaces\n    text = text.strip() # remove spaces at the beginning and at the end of string\n\n    return text","5fa74a5e":"import nltk\nfrom nltk.corpus import stopwords\nstop = stopwords.words('english')\n\ndef clean(data, col):\n    \n    data[col] = data[col].str.replace('https?:\/\/\\S+|www\\.\\S+', ' social medium ', regex=True)      \n        \n    data[col] = data[col].str.lower()\n    data[col] = data[col].str.replace(\"4\", \"a\") \n    data[col] = data[col].str.replace(\"2\", \"l\")\n    data[col] = data[col].str.replace(\"5\", \"s\") \n    data[col] = data[col].str.replace(\"1\", \"i\") \n    data[col] = data[col].str.replace(\"!\", \"i\") \n    data[col] = data[col].str.replace(\"|\", \"i\", regex=False) \n    data[col] = data[col].str.replace(\"0\", \"o\") \n    data[col] = data[col].str.replace(\"l3\", \"b\") \n    data[col] = data[col].str.replace(\"7\", \"t\") \n    data[col] = data[col].str.replace(\"7\", \"+\") \n    data[col] = data[col].str.replace(\"8\", \"ate\") \n    data[col] = data[col].str.replace(\"3\", \"e\") \n    data[col] = data[col].str.replace(\"9\", \"g\")\n    data[col] = data[col].str.replace(\"6\", \"g\")\n    data[col] = data[col].str.replace(\"@\", \"a\")\n    data[col] = data[col].str.replace(\"$\", \"s\", regex=False)\n    data[col] = data[col].str.replace(\"#ofc\", \" of fuckin course \")\n    data[col] = data[col].str.replace(\"fggt\", \" faggot \")\n    data[col] = data[col].str.replace(\"your\", \" your \")\n    data[col] = data[col].str.replace(\"self\", \" self \")\n    data[col] = data[col].str.replace(\"cuntbag\", \" cunt bag \")\n    data[col] = data[col].str.replace(\"fartchina\", \" fart china \")    \n    data[col] = data[col].str.replace(\"youi\", \" you i \")\n    data[col] = data[col].str.replace(\"cunti\", \" cunt i \")\n    data[col] = data[col].str.replace(\"sucki\", \" suck i \")\n    data[col] = data[col].str.replace(\"pagedelete\", \" page delete \")\n    data[col] = data[col].str.replace(\"cuntsi\", \" cuntsi \")\n    data[col] = data[col].str.replace(\"i'm\", \" i am \")\n    data[col] = data[col].str.replace(\"offuck\", \" of fuck \")\n    data[col] = data[col].str.replace(\"centraliststupid\", \" central ist stupid \")\n    data[col] = data[col].str.replace(\"hitleri\", \" hitler i \")\n    data[col] = data[col].str.replace(\"i've\", \" i have \")\n    data[col] = data[col].str.replace(\"i'll\", \" sick \")\n    data[col] = data[col].str.replace(\"fuck\", \" fuck \")\n    data[col] = data[col].str.replace(\"f u c k\", \" fuck \")\n    data[col] = data[col].str.replace(\"shit\", \" shit \")\n    data[col] = data[col].str.replace(\"bunksteve\", \" bunk steve \")\n    data[col] = data[col].str.replace('wikipedia', ' social medium ')\n    data[col] = data[col].str.replace(\"faggot\", \" faggot \")\n    data[col] = data[col].str.replace(\"delanoy\", \" delanoy \")\n    data[col] = data[col].str.replace(\"jewish\", \" jewish \")\n    data[col] = data[col].str.replace(\"sexsex\", \" sex \")\n    data[col] = data[col].str.replace(\"allii\", \" all ii \")\n    data[col] = data[col].str.replace(\"i'd\", \" i had \")\n    data[col] = data[col].str.replace(\"'s\", \" is \")\n    data[col] = data[col].str.replace(\"youbollocks\", \" you bollocks \")\n    data[col] = data[col].str.replace(\"dick\", \" dick \")\n    data[col] = data[col].str.replace(\"cuntsi\", \" cuntsi \")\n    data[col] = data[col].str.replace(\"mothjer\", \" mother \")\n    data[col] = data[col].str.replace(\"cuntfranks\", \" cunt \")\n    data[col] = data[col].str.replace(\"ullmann\", \" jewish \")\n    data[col] = data[col].str.replace(\"mr.\", \" mister \", regex=False)\n    data[col] = data[col].str.replace(\"aidsaids\", \" aids \")\n    data[col] = data[col].str.replace(\"njgw\", \" nigger \")\n    data[col] = data[col].str.replace(\"wiki\", \" social medium \")\n    data[col] = data[col].str.replace(\"administrator\", \" admin \")\n    data[col] = data[col].str.replace(\"gamaliel\", \" jewish \")\n    data[col] = data[col].str.replace(\"rvv\", \" vanadalism \")\n    data[col] = data[col].str.replace(\"admins\", \" admin \")\n    data[col] = data[col].str.replace(\"pensnsnniensnsn\", \" penis \")\n    data[col] = data[col].str.replace(\"pneis\", \" penis \")\n    data[col] = data[col].str.replace(\"pennnis\", \" penis \")\n    data[col] = data[col].str.replace(\"pov.\", \" point of view \", regex=False)\n    data[col] = data[col].str.replace(\"vandalising\", \" vandalism \")\n    data[col] = data[col].str.replace(\"cock\", \" dick \")\n    data[col] = data[col].str.replace(\"asshole\", \" asshole \")\n    data[col] = data[col].str.replace(\"youi\", \" you \")\n    data[col] = data[col].str.replace(\"afd\", \" all fucking day \")\n    data[col] = data[col].str.replace(\"sockpuppets\", \" sockpuppetry \")\n    data[col] = data[col].str.replace(\"iiprick\", \" iprick \")\n    data[col] = data[col].str.replace(\"penisi\", \" penis \")\n    data[col] = data[col].str.replace(\"warrior\", \" warrior \")\n    data[col] = data[col].str.replace(\"loil\", \" laughing out insanely loud \")\n    data[col] = data[col].str.replace(\"vandalise\", \" vanadalism \")\n    data[col] = data[col].str.replace(\"helli\", \" helli \")\n    data[col] = data[col].str.replace(\"lunchablesi\", \" lunchablesi \")\n    data[col] = data[col].str.replace(\"special\", \" special \")\n    data[col] = data[col].str.replace(\"ilol\", \" i lol \")\n    data[col] = data[col].str.replace(r'\\b[uU]\\b', 'you', regex=True)\n    data[col] = data[col].str.replace(r\"what's\", \"what is \")\n    data[col] = data[col].str.replace(r\"\\'s\", \" is \", regex=False)\n    data[col] = data[col].str.replace(r\"\\'ve\", \" have \", regex=False)\n    data[col] = data[col].str.replace(r\"can't\", \"cannot \")\n    data[col] = data[col].str.replace(r\"n't\", \" not \")\n    data[col] = data[col].str.replace(r\"i'm\", \"i am \")\n    data[col] = data[col].str.replace(r\"\\'re\", \" are \", regex=False)\n    data[col] = data[col].str.replace(r\"\\'d\", \" would \", regex=False)\n    data[col] = data[col].str.replace(r\"\\'ll\", \" will \", regex=False)\n    data[col] = data[col].str.replace(r\"\\'scuse\", \" excuse \", regex=False)\n    data[col] = data[col].str.replace('\\s+', ' ', regex=True)  # will remove more than one whitespace character\n#     text = re.sub(r'\\b([^\\W\\d_]+)(\\s+\\1)+\\b', r'\\1', re.sub(r'\\W+', ' ', text).strip(), flags=re.I)  # remove repeating words coming immediately one after another\n    data[col] = data[col].str.replace(r'(.)\\1+', r'\\1\\1', regex=True) # 2 or more characters are replaced by 2 characters\n#     text = re.sub(r'((\\b\\w+\\b.{1,2}\\w+\\b)+).+\\1', r'\\1', text, flags = re.I)\n    data[col] = data[col].str.replace(\"[:|\u2663|'|\u00a7|\u2660|*|\/|?|=|%|&|-|#|\u2022|~|^|>|<|\u25ba|_]\", '', regex=True)\n    \n    \n    data[col] = data[col].str.replace(r\"what's\", \"what is \")    \n    data[col] = data[col].str.replace(r\"\\'ve\", \" have \", regex=False)\n    data[col] = data[col].str.replace(r\"can't\", \"cannot \")\n    data[col] = data[col].str.replace(r\"n't\", \" not \", regex=False)\n    data[col] = data[col].str.replace(r\"i'm\", \"i am \", regex=False)\n    data[col] = data[col].str.replace(r\"\\'re\", \" are \", regex=False)\n    data[col] = data[col].str.replace(r\"\\'d\", \" would \", regex=False)\n    data[col] = data[col].str.replace(r\"\\'ll\", \" will \", regex=False)\n    data[col] = data[col].str.replace(r\"\\'scuse\", \" excuse \", regex=False)\n    data[col] = data[col].str.replace(r\"\\'s\", \" \", regex=False)\n\n    # Clean some punctutations\n    data[col] = data[col].str.replace('\\n', ' \\n ')\n    data[col] = data[col].str.replace(r'([a-zA-Z]+)([\/!?.])([a-zA-Z]+)',r'\\1 \\2 \\3', regex=True)\n    # Replace repeating characters more than 3 times to length of 3\n    data[col] = data[col].str.replace(r'([*!?\\'])\\1\\1{2,}',r'\\1\\1\\1', regex=True)    \n    # Add space around repeating characters\n    data[col] = data[col].str.replace(r'([*!?\\']+)',r' \\1 ', regex=True)    \n    # patterns with repeating characters \n    data[col] = data[col].str.replace(r'([a-zA-Z])\\1{2,}\\b',r'\\1\\1', regex=True)\n    data[col] = data[col].str.replace(r'([a-zA-Z])\\1\\1{2,}\\B',r'\\1\\1\\1', regex=True)\n    data[col] = data[col].str.replace(r'[ ]{2,}',' ', regex=True).str.strip()   \n    data[col] = data[col].str.replace(r'[ ]{2,}',' ', regex=True).str.strip()   \n    data[col] = data[col].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n    tqdm.pandas()\n    data[col] = data[col].progress_apply(text_cleaning)\n    return data","26d68e65":"# Test clean function\ntest_clean_df = pd.DataFrame({\"text\":\n                              [\"heyy\\n\\nkkdsfj\",\n                               \"hi   how\/are\/U ???\",\n                               \"hey?????\",\n                               \"noooo!!!!!!!!!   comeone !! \",\n                              \"cooooooooool     brooooooooooo  coool brooo\",\n                              \"naaaahhhhhhh\",\"'re been cool\"]})\ndisplay(test_clean_df)\nclean(test_clean_df,'text')","9d478ac3":"# create TF-IDF object\npre_train_df1 = clean(pre_train_df1,'text')\npre_train_df2 = clean(pre_train_df2,'text')\n\n# fit the TF-IDF object to the CLEAN training comments\npre_train_df = pd.concat([pre_train_df1,pre_train_df2])","424fbb4e":"# generate a model for the words\nvec = TfidfVectorizer(min_df=1e-5)\nvec.fit(pre_train_df['text'])\n\n# print the vocabulary size\nprint(len(vec.vocabulary_))\n\n# transform the text into sparse matrix format\nX_1 = vec.transform(pre_train_df1['text'])\nX_2 = vec.transform(pre_train_df2['text'])\nX_2","42f239db":"print (\"The first training dataset has %i rows.\" % len(pre_train_df1))\nprint (\"The first training dataset has %i toxic comments.\" % (pre_train_df1['y'] == 1).sum())\nprint (\"The first training dataset has %i non-toxic comments.\" % (pre_train_df1['y'] == 0).sum())\n\nprint (\"The second training dataset has %i rows.\" % len(pre_train_df2))\nprint (\"The second training dataset has %i toxic comments.\" % (pre_train_df2['y'] == 1).sum())\nprint (\"The second training dataset has %i non-toxic comments.\" % (pre_train_df2['y'] == 0).sum())\n","7e2ebc17":"if 1==0:\n    rus = RandomUnderSampler(random_state=rseed)\n    rus_pre_train_x1, rus_pretrain_y1 = (\n        rus.fit_resample(X_1, pre_train_df1[\"y\"]))\n    rus_pre_train_x2, rus_pretrain_y2 = (\n        rus.fit_resample(X_2, pre_train_df2[\"y\"]))\n    \n    X_train = vstack([rus_pre_train_x1,rus_pre_train_x2])\n    y_train = pd.concat([rus_pretrain_y1, rus_pretrain_y2])","34fbdaa3":"if 1==1:\n    sm = SMOTE(random_state=rseed)\n    sm_pre_train_x1, sm_pretrain_y1 = (\n        sm.fit_resample(X_1, pre_train_df1[\"y\"]))\n    sm_pre_train_x2, sm_pretrain_y2 = (\n        sm.fit_resample(X_2, pre_train_df2[\"y\"]))\n    \n    X_train = vstack([sm_pre_train_x1,sm_pre_train_x2])\n    y_train = pd.concat([sm_pretrain_y1, sm_pretrain_y2])","45ad9a80":"print (\"The first training dataset has %i rows.\" % len(y_train))\nprint (\"The first training dataset has %i toxic comments.\" % (y_train == 1).sum())\nprint (\"The first training dataset has %i non-toxic comments.\" % (y_train == 0).sum())","253c963d":"model = MultinomialNB() \nmodel.fit(X_train, y_train)","56b85d1e":"val_df = pd.read_csv(\"\/kaggle\/input\/jigsaw-toxic-severity-rating\/validation_data.csv\")\nval_df.head()","fc6d66a5":"val_df = clean(val_df,'less_toxic')\nval_df = clean(val_df,'more_toxic')\nX_less_toxic = vec.transform(val_df['less_toxic'])\nX_more_toxic = vec.transform(val_df['more_toxic'])","6a038675":"p1 = model.predict_proba(X_less_toxic)\np2 = model.predict_proba(X_more_toxic)","df4d6b0f":"# Validation Accuracy\n(p1[:, 1] < p2[:, 1]).mean()","27f77358":"sub_df = pd.read_csv(\"\/kaggle\/input\/jigsaw-toxic-severity-rating\/comments_to_score.csv\")\nsub_df = clean(sub_df,'text')\nX_test = vec.transform(sub_df['text'])\np3 = model.predict_proba(X_test)","a4381b68":"sub_df['score'] = p3[:, 1]","857bedfe":"# uncomment below to generate original submission file\n#sub_df[['comment_id', 'score']].to_csv(\"submission.csv\", index=False)","bb7ddf2f":"less_toxic_score_df=pd.DataFrame()\nless_toxic_score_df[\"text\"] = val_df[\"less_toxic\"].copy()\nless_toxic_score_df[\"score\"] = 0\nmore_toxic_score_df=pd.DataFrame()\nmore_toxic_score_df[\"text\"] = val_df[\"more_toxic\"].copy()\nmore_toxic_score_df[\"score\"] = 1\ntoxic_score_df = pd.concat([less_toxic_score_df, more_toxic_score_df], ignore_index=True)","e2bca527":"# sort the comments (not necessary)\ntoxic_score_df = toxic_score_df.sort_values(by = 'text').copy()\n\n# use groupby function to group the comments\nval_score_df = toxic_score_df.groupby('text')['score'].mean().reset_index()\n\n# set `y` to \"0\" if the average is less than or equal to 0.5 and set `y` to \"1\" otherwise\nval_score_df['y'] = (val_score_df['score'] > .5).astype(int)\n\n# drop the `score` column\n#val_score_df.drop(['score'], axis=1, inplace=True)","e570fde6":"print (\"The training data has %i rows.\" % len(val_score_df))\nprint (\"The training data has %i toxic comments.\" % (val_score_df['y'] == 1).sum())\nprint (\"The training data has %i non-toxic comments.\" % (val_score_df['y'] == 0).sum())","88efb75b":"from sklearn.model_selection import train_test_split\n\n# we chose a random test size, this number may be optimized for improved results\ntrain_df2, val_df2 = train_test_split(val_score_df, test_size=0.70, random_state = rseed)","7dae55cb":"print (\"The training data has %i rows.\" % len(train_df2))\nprint (\"The training data has %i toxic comments.\" % (train_df2['y'] == 1).sum())\nprint (\"The training data has %i non-toxic comments.\" % (train_df2['y'] == 0).sum())","b0df68aa":"vec2 = TfidfVectorizer()\n\n# fit the TF-IDF object to the training comments\ntrain_df2 = clean(train_df2,'text')\nX2 = vec2.fit_transform(train_df2['text'])\nX2","c63c907b":"# create TF-IDF object\n\n\nmodel2 = MultinomialNB()\nmodel2.fit(X2, train_df2['y'])","cec12ace":"val_df2 = clean(val_df2,'text')\nX_predict = vec2.transform(val_df2['text'])\npredictions = model2.predict_proba(X_predict)","e5691d53":"val_df2.loc[:,'pred_score']=predictions[:, 1]\nval_df2.loc[:,'error'] = 1-abs(val_df2['score']-val_df2['pred_score'])","8e88cbb0":"val_df2['error'].describe()","a887be48":"sub_df = sub_df.rename(\n    columns={'score': 'score1'})\nsub_df = clean(sub_df,'text')\nX_test2 = vec2.transform(sub_df['text'])\nsubmission_predictions2 = model2.predict_proba(X_test2)\nsub_df['score2'] = submission_predictions2[:, 1]","10c12894":"sub_df.loc[:,'score'] = sub_df.loc[:,['score1','score2']].astype(float).mean(axis=1)","8a1c014f":"#sub_df[['comment_id', 'score']].to_csv(\"submission.csv\", index=False)","21628142":"pre_train_df = pd.concat([pre_train_df,train_df2.loc[:,[\"text\",\"y\"]]])","4411287e":"# generate a model for the words\nvec3 = TfidfVectorizer(min_df=1e-5)\nvec3.fit(pre_train_df['text'])\n\n# print the vocabulary size\nprint(len(vec3.vocabulary_))\n\n# transform the text into sparse matrix format\nX_1 = vec3.transform(pre_train_df1['text'])\nX_2 = vec3.transform(pre_train_df2['text'])\nX_3 = vec3.transform(train_df2['text'])\nX_3","cb29ad49":"print (\"The first training dataset has %i rows.\" % len(pre_train_df1))\nprint (\"The first training dataset has %i toxic comments.\" % (pre_train_df1['y'] == 1).sum())\nprint (\"The first training dataset has %i non-toxic comments.\" % (pre_train_df1['y'] == 0).sum())\n\nprint (\"The second training dataset has %i rows.\" % len(pre_train_df2))\nprint (\"The second training dataset has %i toxic comments.\" % (pre_train_df2['y'] == 1).sum())\nprint (\"The second training dataset has %i non-toxic comments.\" % (pre_train_df2['y'] == 0).sum())\n\nprint (\"The third training dataset has %i rows.\" % len(train_df2))\nprint (\"The third training dataset has %i toxic comments.\" % (train_df2['y'] == 1).sum())\nprint (\"The third training dataset has %i non-toxic comments.\" % (train_df2['y'] == 0).sum())","69bdf39e":"if 1==1:\n    sm = SMOTE(random_state=rseed)\n    sm_pre_train_x1, sm_pretrain_y1 = (\n        sm.fit_resample(X_1, pre_train_df1[\"y\"]))\n    sm_pre_train_x2, sm_pretrain_y2 = (\n        sm.fit_resample(X_2, pre_train_df2[\"y\"]))\n    sm_pre_train_x3, sm_pretrain_y3 = (\n        sm.fit_resample(X_3, train_df2[\"y\"]))\n    \n    X_train = vstack([sm_pre_train_x1, sm_pre_train_x2, sm_pre_train_x3])\n    y_train = pd.concat([sm_pretrain_y1, sm_pretrain_y2, sm_pretrain_y3])","3c55a140":"X_train = vstack([sm_pre_train_x1, sm_pre_train_x2, sm_pre_train_x3])\ny_train = pd.concat([sm_pretrain_y1, sm_pretrain_y2, sm_pretrain_y3])\n\nmodel3 = MultinomialNB()\nmodel3.fit(X_train, y_train)","cd177868":"val_df2 = clean(val_df2,'text')\nX_predict = vec3.transform(val_df2['text'])\npredictions = model3.predict_proba(X_predict)","103cb614":"val_df2.loc[:,'pred_score']=predictions[:, 1]\nval_df2.loc[:,'error'] = 1-abs(val_df2['score']-val_df2['pred_score'])","ea9f0782":"val_df2['error'].describe()","13222e12":"sub_df = pd.read_csv(\"\/kaggle\/input\/jigsaw-toxic-severity-rating\/comments_to_score.csv\")","0c450f05":"sub_df = clean(sub_df,'text')\nX_test3 = vec3.transform(sub_df['text'])\nsubmission_predictions3 = model3.predict_proba(X_test3)\nsub_df['score'] = submission_predictions3[:, 1]","a3f5aefa":"sub_df[['comment_id', 'score']].to_csv(\"submission.csv\", index=False)","a04f3122":"# Create training data from other jigsaw competitions\nTo train the data we need comments (X) as features and then a \"ground truth\" of the comment's toxicity (y). We use the Jigsaw Toxic Comment Classification challenge training data which labels each comment as either 'toxic', 'severe_toxic', 'obscene', 'threat', 'insult', or 'identity_hate' using 1's and 0's. Since in this competition these categories are all considered to be toxic, we transform this training data to list whether each comment is toxic or not.","1ad25830":"below we generate a clean function from ANDREJ MARINCHENKO","decec77f":"### Fit Naive Bayes\nWe fit the naive bayes model to the comments from this new training data","44c7d35f":"generate submission file","c35d8676":"# Add more training data using current competition\nWe can split the 30108 rows of training data from the current competition for both training and validation. Unlike the comments from the first competition which are binary (1=toxic, 0=not) we can label the comments from the current competition fractions based on whether they were more toxic than the comparing comment more often than not.","2a52a1f5":"generate submission file","2fb480cb":"## Import Ruddit Jigsaw dataset","4f285ac8":"## Validate using the data from the current competition\nTo validate the Naive Bayes model we use the training data for this competition.","0c41b04d":"Group the scores given to each comment_text, get the average of each distinct comment_text, and then set `y` to \"0\" if the average is less than or equal to 0.5 and set `y` to \"1\" otherwise","bc1b0588":"## Import Jigsaw Unintended Bias challenge data","16baf98b":"the code below simply tests the cleaning functions above","77ebef3d":"below we generate a clean function from MANAV","e5678bde":"## Submission","f205b9e3":"add the predictions to the submission data frame","0a2eaf93":"To validate the model we measure whether the \"more_toxic\" comments got higher values than the \"less_toxic\" comments","5290781a":"### Submission with new model","e3a8dfbd":"### Submission with new model","8fa48dc4":"### Use validation data from current competition to create seperate model\nThen validate our results on the new validation data","2a4bf988":"First we generate a table with two columns: 'text' and 'score'. The score will equal *0* if the comment was considered less toxic and *1* if the comment was considered more toxic in the pairwise comparisons.","3e909b13":"This dataset does not look seriously imbalanced so we can continue by splitting the full validation set into a smaller training and validation set.","469101fe":"The first dataset is very unbalanced and the second is a little imbalanced. Below is code to undersample the majority class. Undersampling decreases the chance of false positives. However, we loss a lot of training data this way.","11481013":"# Imports\nImport the following libraries and data from \n* Jigsaw Toxic Comment Classification challenge - predicts whether a comment was 'toxic' 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate' (1,0)\n* Ruddit Jigsaw dataset - scores between -1 (maximally supportive) and 1 (maximally offensive)\n* Jigsaw Unintended Bias in Toxicity Classification - predicts whether a comment was 'toxic' 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate' (1,0)\n* Jigsaw Toxic Severity rating - predicts how toxic a comment is compared to other comments","be1bcf37":"## Imbalanced dataset\nBelow we run code to see if the comments have a relatively equal amount of toxic and non-toxic comments.","e394237c":"The naive bayes model is run on the the comments labeled as \"less_toxic\" and \"more_toxic\" seperately. If a comment is more toxic it should have a higher value.","3f26990d":"## Comments to vectors","b3cda255":"### Comments to vectors","584cf7f0":"Recall we have imbalanced data so we over-sample","0244ce67":"### Use validation data from current competition to create seperate model\nThen validate our results on the new validation data","f036d7dc":"get the average of the two models","c251f8e5":"To prevent tossing training data, I want to try over-sampling using SMOTE. ","7f701e8f":"## Import Jigsaw Toxic Comment Classification challenge data","a6f4da94":"# Background\nThe following code was inspired by the following\n* notebook by JULI\u00c1N PELLER: https:\/\/www.kaggle.com\/julian3833\/jigsaw-incredibly-simple-naive-bayes-0-768\n* notebook by ANDREJ MARINCHENKO: https:\/\/www.kaggle.com\/andrej0marinchenko\/jigsaw-ensemble-0-86\n* notebook by MANAV: https:\/\/www.kaggle.com\/manabendrarout\/pytorch-roberta-ranking-baseline-jrstc-infer\n\nSo far this notebook is split into three submissions:\n1. Using the current competition training datasets as validation data for running a Naive Bayes model\n2. Using the current competition training datasets as a seperate training data to generate two Naive Bayes models\n3. Using the current competition training datasets as additional training data to generate a single Naive Bayes models","c7a7a66d":"The \"predict_proba\" function generated a 2D array where the first dimension lists the probability the comment is not toxic and the second contains the probability that the comment is toxic.","a948f9fb":"### Fit Naive Bayes\nWe fit the naive bayes model to the comments from this new training data","b1f2ea6d":"Generate sparse matrix using all the training data comments","6c30b9fe":"## Fit Naive Bayes\nWe fit the naive bayes model to the comments from the training data based on whether they are labeled as toxic or not.","8f342e98":"## Use validation data from current competition to add to data from the original competition","833c4206":"Now we can embed the comments in each training dataset. Note that we must first fit the embedding on all the comments from both datasets. When building the vocabulary I chose to ignore terms that have a document frequency in less than .001% of the documents.","04a3bca3":"now we can clean up the comments in the training data"}}