{"cell_type":{"d1307d95":"code","e0dcb67c":"code","b4ebdd35":"code","eb8a0f34":"code","7797431d":"code","7b5534d3":"code","f8dbbadd":"code","f23c7713":"code","9404af9f":"code","49850172":"code","79a2e50d":"code","1224b32c":"code","fbbf7c2b":"code","18fb0d3e":"code","4293c885":"code","a2d6801f":"code","289a5f93":"code","d47121f0":"code","3965e33c":"code","adbd70b6":"code","a966bd13":"code","9b8015ca":"code","39f1adef":"code","9afdb097":"code","b8171285":"code","1d4a19b1":"code","6dbc6379":"code","f90e0b10":"code","339b1775":"markdown","fe7f9e84":"markdown","32cc3da5":"markdown","1d92acac":"markdown","00529f19":"markdown","446222f7":"markdown","986647b3":"markdown","e78e0bcf":"markdown","74104d35":"markdown"},"source":{"d1307d95":"# Import of Necessary libraries \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer,TfidfTransformer \nfrom sklearn.linear_model import LogisticRegression, LinearRegression\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import accuracy_score\nfrom sklearn import svm\nfrom sklearn.svm import SVC\nfrom sklearn.svm import LinearSVC\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.model_selection import GridSearchCV\n\nfrom sklearn.pipeline import FeatureUnion\nfrom sklearn.model_selection import train_test_split \nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.decomposition import TruncatedSVD\n\n\nimport os\nprint(os.listdir(\"..\/input\"))\n%matplotlib inline","e0dcb67c":"category= pd.read_csv('..\/input\/category.csv').set_index('category_id')\ncategory.head()","b4ebdd35":"#Reading of data \ndf = pd.read_csv('..\/input\/train.csv')\ntest_set = pd.read_csv('..\/input\/test.csv')\ntrain_data = df.drop(['category_id','item_id'],axis=1) # drop columns not useful for training \ntrain_target = df['category_id']","eb8a0f34":"df.head(5) # Familiarisation with dataset","7797431d":"df.shape, test_set.shape  ","7b5534d3":"test_set.head()","f8dbbadd":"df.isnull().sum() # Check for missing data","f23c7713":"len(df['description'][0]), len(df['title'][0])","9404af9f":"len(df['description'][4]), len(df['title'][4])","49850172":"lens = df.description.str.len()\nprint (lens.mean(), lens.std(), lens.max())\nlens.hist(); ","79a2e50d":"lens1 = df.title.str.len()\nprint(lens1.mean(), lens1.std(), lens1.max())\nlens1.hist(); ","1224b32c":"# Concatenate the text features from the train and test datasets so that  \n#a unique bag of words contains words from both datasets \n\ntrain_text = df['title'] + ' ' + df['description'] \ntest_text = test_set['title'] + ' ' + test_set['description']\nall_text = pd.concat([train_text, test_text,])","fbbf7c2b":"from nltk.corpus import stopwords\nfrom nltk.stem.snowball import RussianStemmer, SnowballStemmer\n\nstemmer = SnowballStemmer(\"russian\", ignore_stopwords=False)   #This converts all words to their stem format  )\n\nstop_words = stopwords.words('russian') # russian stop words from nltk library \n\n\nclass StemmedTfidfVectorizer(TfidfVectorizer):\n    \n    def __init__(self, stemmer, *args, **kwargs):\n        super(StemmedTfidfVectorizer, self).__init__(*args, **kwargs)\n        self.stemmer = stemmer\n        \n    def build_analyzer(self):\n        analyzer = super(StemmedTfidfVectorizer, self).build_analyzer()\n        return lambda doc: (self.stemmer.stem(word) for word in analyzer(doc.replace('\\n', ' ')))\n\n# build the feature matrices\nword_vectorizer = TfidfVectorizer(stop_words= stop_words, \n                                  sublinear_tf= True,\n                                  lowercase= True, \n                                  analyzer= 'word',\n                                  max_features= 150000)","18fb0d3e":"train = word_vectorizer.fit_transform(train_text) #Now we use our vectors to transform our training set\ntest  = word_vectorizer.transform(test_text)","4293c885":"#In order to avoid overfitting, we split the training data into train and test sets\nX_train, X_test,y_train, y_test=train_test_split(train,train_target, \n                                                 test_size= 0.3, random_state=17)","a2d6801f":"import time\n\nsvc = LinearSVC() #.fit(X_train, y_train)\nparam_grid = {'C':(0.01,0.1,1,10)}\na=GridSearchCV(svc,param_grid,cv=5)\nstart = time.time()\na.fit(X_train,y_train)\n# print (a.best_scores_)\nprint (a.best_params_)\nend = time.time()\nprint ('Training classifier takes:', end-start, 'ms')","289a5f93":"print (a.best_params_)","d47121f0":"import time\n#train the classifier\nstart = time.time()\nsvc  = LinearSVC().fit(X_train, y_train)\nclf = LogisticRegression(solver='sag',multi_class= 'multinomial').fit(X_train,y_train)\n# test the classifier\npred  = svc.predict(X_test)\npred1 = clf.predict(X_test)\n# Accuracy scores\nscore  = accuracy_score(y_test,pred)\nscore_a = accuracy_score(y_train,svc.predict(X_train))\nscore1 = accuracy_score(y_test,pred1)\nscore1_a = accuracy_score(y_train,clf.predict(X_train) )\nend = time.time()\nprint ('Training the classifiers takes:', end-start, 'ms')\nprint ('Logistic Regression Score:','Train:',score1_a,'Test:',score1,'\\n',\n       'Support Vector Machine Score:','Train:',score_a,'Test:',score)","3965e33c":"#Check accuracy on each class\ny_pred_=[pred[i] for i in range(len(pred))] \ny_test_=[y_test.iloc[i] for i in range(len(y_test))]\n \nright_predictions=[0 for i in range(54)] \nwrong_predictions=[0 for i in range(54)]\n \nfor i in range(len(y_pred_)): \n    if y_pred_[i] == y_test_[i]: \n        right_predictions[y_test_[i]] += 1 \n    else: \n        wrong_predictions[y_test_[i]] += 1\n#Calculate accuracy for each class\naccur_score = [0 for i in range(54)] \nfor i in range(len(accur_score)): \n    accur_score[i] = right_predictions[i]\/(right_predictions[i]+wrong_predictions[i])\n\nclass_score = pd.DataFrame(columns=['category_id','accuracy'])\nclass_score['category_id'],class_score['accuracy'] = range(54),accur_score\nclass_score.head()","adbd70b6":"#Heirachy Accuracy \ndef add_cat(array):\n    while len(array) !=4:\n        array.append(' '.join(array))\n    return array\n\ncat_df = pd.DataFrame(category['name'].apply(lambda s: s.split('|')).apply(add_cat).tolist())\nheir = {i:cat_df[i].to_dict() for i in range(4)}\n","a966bd13":"def replace(array,mask):\n    return pd.DataFrame(array).replace(mask).values.ravel()\n\nfor level, adapter in heir.items():\n    acc = accuracy_score(replace(y_test,adapter), replace(pred,adapter))\n    \n    print (f'level={level}, accuracy={acc:.3f}')","9b8015ca":"## Implementation using sklearn pipeline\n# Pipeline allows to combine all the transformations before fitting it to our data","39f1adef":"from sklearn.pipeline import Pipeline, FeatureUnion\n\nclass ColumnSelector(BaseEstimator, TransformerMixin):\n    def __init__(self, columns):\n        self.columns = columns\n\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        assert isinstance(X, pd.DataFrame)\n\n        try:\n            return X[self.columns]\n        except KeyError:\n            cols_error = list(set(self.columns) - set(X.columns))\n            raise KeyError(\"The DataFrame does not include the columns: %s\" % cols_error)\n            \nclass TypeSelector(BaseEstimator, TransformerMixin):\n    def __init__(self, dtype):\n        self.dtype = dtype\n    def fit(self, X, y=None):\n        return self\n    def transform(self, X):\n        assert isinstance(X, pd.DataFrame)\n        return X.select_dtypes(include=[self.dtype])","9afdb097":"X_train, X_test,y_train, y_test = train_test_split(train_data,train_target,test_size= 0.2, random_state=17)\n\npipeline = Pipeline([\n         # Use FeatureUnion to combine the features from title and description\n    ('union', FeatureUnion(\n        transformer_list=[\n\n            # Pipeline for pulling features from title\n            ('title', Pipeline([\n                ('selector', ColumnSelector(columns='title')),\n                ('tfidf', TfidfVectorizer(min_df=50,\n                                         stop_words= stop_words, \n                                         sublinear_tf= True,\n                                         lowercase= True, \n                                         analyzer= 'word',\n                                         max_features= 150000)),\n            ])),\n\n            # Pipeline for standard bag-of-words model for description\n            ('description', Pipeline([\n                ('selector', ColumnSelector(columns='description')),\n                ('tfidf', TfidfVectorizer(stop_words= stop_words, \n                                          sublinear_tf= True,\n                                          lowercase= True, \n                                          analyzer= 'word',\n                                          max_features= 150000)),\n                ('best', TruncatedSVD(n_components=50)),\n            ])),\n\n            # Pipeline for pulling price features \n            ('price', Pipeline([\n            ('selector', TypeSelector(np.float)),\n            ('scaler', StandardScaler())\n            ])),\n        ],\n    )),\n    # Use a SVC classifier on the combined features\n    ('svc', LinearSVC()),\n])\n\nstart1 = time.time()\npipeline.fit(X_train, y_train)\npred = pipeline.predict(X_test)\nscore1 = accuracy_score(y_test,pred)\nend1= time.time()\n\nprint ('Training SVC with pipeline  takes:', end1-start1, 'ms')\nprint ('Support Vector Machine Score:',score1)\n","b8171285":"# Result on the provided test set using the trained SVC classifier\nresult = pd.DataFrame()   \nresult['item_id']= test_set['item_id']\nresult['category_id'] =  svc.predict(test)\nresult.to_csv('submission_1.csv', index=False)","1d4a19b1":"pipeline.predict(test_set)\nresult1=  pd.DataFrame()\nresult1['item_id'] = test_set['item_id']\nresult1['category_id']= pipeline.predict(test_set)\nresult1.to_csv('submission_2.csv')","6dbc6379":"result.head(10)","f90e0b10":"result1.head(10)","339b1775":"Our dataset has no missing data which is good ","fe7f9e84":"## Task: Train a classifier that predicts the category of an ad on Avito using its title, description, and price.\n## Metric for evaluating model: Accuracy ","32cc3da5":"We transform the text into vectors using the TF-IDF algorithm from sklearn, before feeding it into the model","1d92acac":"* Price seems to reduce the accuracy of model so I trained the model with the text data(title and description)\n* The pipeline takes too long to train but it provides a short and fast implementation of the classifier\n* SVM classifier gives the best accuracy ","00529f19":"## Training of Model \n\nSVM (support vector machine) is a supervised learning model for text classification, which has shown promising results. It is selected to be our model, with its hyperparameters (C, gamma) tuned in a grid search. We also train a logistic regression model. SVC trained relatively faster compared to Logistic Regression.","446222f7":"## Data pre-processing ","986647b3":"The task of classifying avito ads will be divided into the following steps :\n\n1. Data pre-processing\n2. Training \n3. Paramter optimization (Tuning of hyperparameters)\n4. Model Nesting i.e training model for different hierarchy groups ","e78e0bcf":"The length of our text features varies a lot.","74104d35":"## Dataset "}}