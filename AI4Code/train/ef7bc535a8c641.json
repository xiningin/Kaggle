{"cell_type":{"825aee38":"code","a2f450d1":"code","046ce5ae":"code","65be0e59":"code","622fd2f1":"code","3d41a53f":"code","fabeab67":"code","6e6b3b29":"code","3719f049":"code","08fce274":"code","ebd201a2":"code","84a307c6":"code","a24b5d30":"code","ba35b294":"code","4efb0424":"code","7835af6b":"code","20b59082":"code","97a36f27":"code","3909c82c":"code","6ea3397b":"code","0afc3af4":"code","f1bfad7f":"code","9391ad08":"code","3c22967e":"code","7e2f25cf":"code","47c7d90b":"code","d5b27526":"code","98f92b9d":"code","fa3c11be":"code","34d253de":"code","39a15b19":"code","507c445a":"code","482f3d0c":"code","acac4333":"code","99aed870":"code","0b3e6efb":"code","e8a68907":"code","941d1d5c":"code","f91c715e":"code","d2781da1":"code","b2c0df93":"code","72806748":"code","d13b03c3":"code","03c2d634":"code","3a851613":"code","0feef322":"code","fc73e611":"code","ed71bf17":"code","e7f849b3":"code","b79321bc":"code","aec54634":"code","d05562d9":"code","1f430a81":"code","b390fc93":"code","9042a00f":"code","8cca83b5":"code","e7456220":"code","96246aa6":"code","c6021361":"code","66a497fc":"code","0e4e5136":"code","5604518c":"code","7e37978e":"code","8c9d5793":"code","710df888":"code","f94fc643":"code","36419963":"code","0b08cbea":"code","ba6d8a8f":"code","ef51a686":"markdown","d8b420f4":"markdown","3886b986":"markdown","7013da1e":"markdown","6cfc3e3f":"markdown","7117b2a6":"markdown","bfe3f20a":"markdown","2790ed10":"markdown","f6b3b2ff":"markdown","3c31464b":"markdown","114b185d":"markdown","36dea2ad":"markdown","473671a0":"markdown","d8c2e6bf":"markdown","2d399204":"markdown","4f19f99b":"markdown","2189ec0c":"markdown","f1222bf5":"markdown","728bda4d":"markdown"},"source":{"825aee38":"%%writefile req.txt\n# tensorflow=2.3.0=eigen_py38h71ff20e_0\n# tensorflow-base=2.3.0=eigen_py38hb57a387_0\ntsfresh==0.17.0\n# dtw==1.4.0\necg-plot==0.2.8\n# fastdtw==0.3.4\n# imbalanced-learn==0.8.0\n# librosa==0.8.1\n# scikit-learn==0.24.2\nwfdb==3.4.0\n\n","a2f450d1":"!pip install -r req.txt\n# !conda env update -n base --file env.yaml","046ce5ae":"# !pip install scikit-learn  -U","65be0e59":"%%writefile plot_learning_curve.py\n\"\"\"\n========================\nPlotting Learning Curves\n========================\nIn the first column, first row the learning curve of a naive Bayes classifier\nis shown for the digits dataset. Note that the training score and the\ncross-validation score are both not very good at the end. However, the shape\nof the curve can be found in more complex datasets very often: the training\nscore is very high at the beginning and decreases and the cross-validation\nscore is very low at the beginning and increases. In the second column, first\nrow we see the learning curve of an SVM with RBF kernel. We can see clearly\nthat the training score is still around the maximum and the validation score\ncould be increased with more training samples. The plots in the second row\nshow the times required by the models to train with various sizes of training\ndataset. The plots in the third row show how much time was required to train\nthe models for each training sizes.\n\"\"\"\n# print(__doc__)\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nfrom sklearn.datasets import load_digits\nfrom sklearn.model_selection import learning_curve\nfrom sklearn.model_selection import ShuffleSplit\n\n\ndef plot_learning_curve(estimator, title, X, y, axes=None, ylim=None, cv=None,\n                        n_jobs=None, train_sizes=np.linspace(.1, 1.0, 5)):\n    \"\"\"\n    Generate 3 plots: the test and training learning curve, the training\n    samples vs fit times curve, the fit times vs score curve.\n\n    Parameters\n    ----------\n    estimator : estimator instance\n        An estimator instance implementing `fit` and `predict` methods which\n        will be cloned for each validation.\n\n    title : str\n        Title for the chart.\n\n    X : array-like of shape (n_samples, n_features)\n        Training vector, where ``n_samples`` is the number of samples and\n        ``n_features`` is the number of features.\n\n    y : array-like of shape (n_samples) or (n_samples, n_features)\n        Target relative to ``X`` for classification or regression;\n        None for unsupervised learning.\n\n    axes : array-like of shape (3,), default=None\n        Axes to use for plotting the curves.\n\n    ylim : tuple of shape (2,), default=None\n        Defines minimum and maximum y-values plotted, e.g. (ymin, ymax).\n\n    cv : int, cross-validation generator or an iterable, default=None\n        Determines the cross-validation splitting strategy.\n        Possible inputs for cv are:\n\n          - None, to use the default 5-fold cross-validation,\n          - integer, to specify the number of folds.\n          - :term:`CV splitter`,\n          - An iterable yielding (train, test) splits as arrays of indices.\n\n        For integer\/None inputs, if ``y`` is binary or multiclass,\n        :class:`StratifiedKFold` used. If the estimator is not a classifier\n        or if ``y`` is neither binary nor multiclass, :class:`KFold` is used.\n\n        Refer :ref:`User Guide <cross_validation>` for the various\n        cross-validators that can be used here.\n\n    n_jobs : int or None, default=None\n        Number of jobs to run in parallel.\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n        for more details.\n\n    train_sizes : array-like of shape (n_ticks,)\n        Relative or absolute numbers of training examples that will be used to\n        generate the learning curve. If the ``dtype`` is float, it is regarded\n        as a fraction of the maximum size of the training set (that is\n        determined by the selected validation method), i.e. it has to be within\n        (0, 1]. Otherwise it is interpreted as absolute sizes of the training\n        sets. Note that for classification the number of samples usually have\n        to be big enough to contain at least one sample from each class.\n        (default: np.linspace(0.1, 1.0, 5))\n    \"\"\"\n    if axes is None:\n        _, axes = plt.subplots(1, 3, figsize=(20, 5))\n\n    axes[0].set_title(title)\n    if ylim is not None:\n        axes[0].set_ylim(*ylim)\n    axes[0].set_xlabel(\"Training examples\")\n    axes[0].set_ylabel(\"Score\")\n\n    train_sizes, train_scores, test_scores, fit_times, _ = \\\n        learning_curve(estimator, X, y, cv=cv, n_jobs=n_jobs,\n                       train_sizes=train_sizes,\n                       return_times=True)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    fit_times_mean = np.mean(fit_times, axis=1)\n    fit_times_std = np.std(fit_times, axis=1)\n\n    # Plot learning curve\n    axes[0].grid()\n    axes[0].fill_between(train_sizes, train_scores_mean - train_scores_std,\n                         train_scores_mean + train_scores_std, alpha=0.1,\n                         color=\"r\")\n    axes[0].fill_between(train_sizes, test_scores_mean - test_scores_std,\n                         test_scores_mean + test_scores_std, alpha=0.1,\n                         color=\"g\")\n    axes[0].plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n                 label=\"Training score\")\n    axes[0].plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n                 label=\"Cross-validation score\")\n    axes[0].legend(loc=\"best\")\n\n    # Plot n_samples vs fit_times\n    axes[1].grid()\n    axes[1].plot(train_sizes, fit_times_mean, 'o-')\n    axes[1].fill_between(train_sizes, fit_times_mean - fit_times_std,\n                         fit_times_mean + fit_times_std, alpha=0.1)\n    axes[1].set_xlabel(\"Training examples\")\n    axes[1].set_ylabel(\"fit_times\")\n    axes[1].set_title(\"Scalability of the model\")\n\n    # Plot fit_time vs score\n    axes[2].grid()\n    axes[2].plot(fit_times_mean, test_scores_mean, 'o-')\n    axes[2].fill_between(fit_times_mean, test_scores_mean - test_scores_std,\n                         test_scores_mean + test_scores_std, alpha=0.1)\n    axes[2].set_xlabel(\"fit_times\")\n    axes[2].set_ylabel(\"Score\")\n    axes[2].set_title(\"Performance of the model\")\n\n    return plt\n\nif __name__ == '__main__':\n\n    fig, axes = plt.subplots(3, 2, figsize=(10, 15))\n\n    X, y = load_digits(return_X_y=True)\n\n    title = \"Learning Curves (Naive Bayes)\"\n    # Cross validation with 100 iterations to get smoother mean test and train\n    # score curves, each time with 20% data randomly selected as a validation set.\n    cv = ShuffleSplit(n_splits=100, test_size=0.2, random_state=0)\n\n    estimator = GaussianNB()\n    plot_learning_curve(estimator, title, X, y, axes=axes[:, 0], ylim=(0.7, 1.01),\n                        cv=cv, n_jobs=4)\n\n    title = r\"Learning Curves (SVM, RBF kernel, $\\gamma=0.001$)\"\n    # SVC is more expensive so we do a lower number of CV iterations:\n    cv = ShuffleSplit(n_splits=10, test_size=0.2, random_state=0)\n    estimator = SVC(gamma=0.001)\n    plot_learning_curve(estimator, title, X, y, axes=axes[:, 1], ylim=(0.7, 1.01),\n                        cv=cv, n_jobs=4)\n\n    plt.show()\n","622fd2f1":"from glob import glob\nimport warnings\n\nimport numpy as np\nimport pandas as pd\nimport wfdb\n# import pywt\nimport librosa\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport ecg_plot\n\nfrom tsfresh import extract_features\nfrom statsmodels.tsa import stattools as sts\nfrom sklearn.model_selection import ShuffleSplit, LeaveOneOut\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom plot_learning_curve import plot_learning_curve\n\n%load_ext autoreload\n%autoreload\n%reload_ext autoreload\n\n%matplotlib inline\nplt.rcParams[\"figure.figsize\"] = (20,6)\n\nwarnings.filterwarnings('ignore')","3d41a53f":"import tensorflow.keras as keras","fabeab67":"files = glob(\"..\/input\/ecg-af-termination\/af-termination-challenge-database-1.0.0\/learning-set\/*.dat\")\nfiles[:3]","6e6b3b29":"files_df = pd.DataFrame(files, columns=[\"file_path\"])","3719f049":"def load_data(file_path):\n    _file_path = file_path[:-4]\n    _file_name = file_path.split(\"\/\")[-1]\n    _record = wfdb.rdsamp(_file_path)\n    _class = _file_name[0]\n    return pd.Series({\n        \"class\": _class,\n        \"file_name\": _file_name,\n        \"file_path\": _file_path,\n        \"ecg_record_0\": _record[0][:,0],\n        \"ecg_record_1\": _record[0][:,1],\n        \"ecg_record_info\": _record[1],\n    })\n\ndata_df = files_df[\"file_path\"].apply(load_data)\n# data_df[:3]","08fce274":"data_df = pd.concat([data_df, pd.DataFrame(data_df[\"ecg_record_info\"].tolist())],1).drop([\"ecg_record_info\"],1)\ndata_df[:3]","ebd201a2":"7680\/64","84a307c6":"plt.plot(data_df[\"ecg_record_0\"][0][:120])","a24b5d30":"def split_30(arr):\n    return arr.reshape(64,-1)\n\ndata_df[\"ecg_record_0\"] = data_df[\"ecg_record_0\"].apply(split_30)\ndata_df[\"ecg_record_1\"] = data_df[\"ecg_record_1\"].apply(split_30)","ba35b294":"data_df = data_df.apply(lambda x: x.explode() if x.name in [\"ecg_record_0\",\"ecg_record_1\"] else x)","4efb0424":"data_df[\"file_name\"] = data_df[\"file_name\"] + '_' + pd.Series(range(data_df.shape[0]),index=data_df[\"file_name\"].index).astype(str)","7835af6b":"for col in [\"units\",\"sig_name\",\"comments\"]:\n    data_df[col]=data_df[col].apply(tuple)","20b59082":"unique_counts = data_df[[\"class\", \"file_name\", \"file_path\", \"fs\", \"sig_len\", \"n_sig\", \"base_date\", \"base_time\", \"units\", \"sig_name\", \"comments\"]].nunique()\npd.concat([data_df.dtypes.rename(\"dtypes\"), \n           data_df.isna().sum().rename(\"null\"), \n           unique_counts.rename(\"unique_counts\")],1)","97a36f27":"data_df = pd.concat([data_df, data_df[\"ecg_record_0\"].apply(lambda x: pd.Series({\"rec_min\":np.min(x), \"rec_max\":np.max(x)}))],1)\ndata_df[:3]","3909c82c":"data_df.groupby(\"class\")[[\"rec_min\",\"rec_max\"]].describe()","6ea3397b":"plt.rcParams[\"figure.figsize\"] = (6,2)\nfor idx, row in data_df.sample(4).iterrows():\n#     pd.DataFrame(row[\"ecg_record\"]).plot(subplots=True, title=row[\"file_name\"], ylim=(-2.3,3.2));\n\n    ecg_plot.plot(row[[\"ecg_record_0\",\"ecg_record_1\"]].values.transpose(), sample_rate=40, title=row[\"file_name\"])\n    ecg_plot.show()\n","0afc3af4":"# np.linspace(-0.3,1.1,60)","f1bfad7f":"128*60\/130","9391ad08":"plt.rcParams[\"figure.figsize\"] = (16,5)\nlag = 90\nlag = 120\nlag = 130\nlag = 180\nlag=3\n\nfor idx, row in data_df.sample(4).iterrows():\n#     pd.\n    f,axs=plt.subplots(1,4)\n    axs[0].plot(row[\"ecg_record_0\"][:-lag],row[\"ecg_record_0\"][lag:],\".\")\n    axs[0].set_xlim(-0.5,1.5)\n    axs[0].set_ylim(-0.5,1.5)\n    \n    axs[1].plot(row[\"ecg_record_1\"][:-lag],row[\"ecg_record_1\"][lag:],\".\")\n    axs[1].set_xlim(-0.5,1.5)\n    axs[1].set_ylim(-0.5,1.5)\n    hist = np.histogram2d(row[\"ecg_record_0\"][:-lag],row[\"ecg_record_0\"][lag:],\n                   bins=[np.linspace(-0.3,1.1,60),np.linspace(-0.3,1.1,60)])\n    axs[2].imshow(hist[0],cmap=\"gray\")\n    hist = np.histogram2d(row[\"ecg_record_1\"][:-lag],row[\"ecg_record_1\"][lag:],\n                   bins=[np.linspace(-0.3,1.1,60),np.linspace(-0.3,1.1,60)])\n    axs[3].imshow(hist[0],cmap=\"gray\")\n    plt.title(row[\"file_name\"])\n    plt.show()\n    ","3c22967e":"from scipy.signal import butter, lfilter, freqz\n\ndef butter_lowpass(cutoff, fs, order=5):\n    nyq = 0.5 * fs\n    normal_cutoff = cutoff \/ nyq\n    b, a = butter(order, normal_cutoff, btype='low', analog=False)\n    return b, a\n\ndef butter_lowpass_filter(data, cutoff, fs, order=5):\n    b, a = butter_lowpass(cutoff, fs, order=order)\n    y = lfilter(b, a, data)\n    return y\n\n# Filter settings.\norder = 5\nfs = 128       # sample rate, Hz\ncutoff = 8  # desired cutoff frequency of the filter, Hz\n\ndata = row[\"ecg_record_0\"]\ny = butter_lowpass_filter(data, cutoff, fs, order)","7e2f25cf":"plt.subplots(figsize=(15,6))\nplt.plot(row[\"ecg_record_0\"][:128*4],\"-\")\nplt.plot(y[:128*4],\"-\")","47c7d90b":"# # y = row[\"ecg_record_0\"]#[:128*4]\n# sr=128\n\n# n_fft = 2048\n# S = librosa.stft(y, n_fft=n_fft, hop_length=n_fft\/\/2)\n# # convert to db\n# # (for your CNN you might want to skip this and rather ensure zero mean and unit variance)\n# D = librosa.amplitude_to_db(np.abs(S), ref=np.max)\n# # average over file\n# D_AVG = np.mean(D, axis=1)\n\n# plt.bar(np.arange(D_AVG.shape[0]), D_AVG)\n# x_ticks_positions = [n for n in range(0, n_fft \/\/ 2, n_fft \/\/ 16)]\n# x_ticks_labels = [str(sr \/ 2048 * n) + 'Hz' for n in x_ticks_positions]\n# plt.xticks(x_ticks_positions, x_ticks_labels)\n# plt.xlabel('Frequency')\n# plt.ylabel('dB')\n# plt.show()","d5b27526":"# order = 5\n# fs = 128\n# cutoff = 8\n# data_df[\"ecg_record_0\"]=data_df[\"ecg_record_0\"].apply(lambda x: butter_lowpass_filter(x, cutoff, fs, order))\n# data_df[\"ecg_record_1\"]=data_df[\"ecg_record_1\"].apply(lambda x: butter_lowpass_filter(x, cutoff, fs, order))","98f92b9d":"plt.rcParams[\"figure.figsize\"] = (10,2)\nfor idx, row in data_df.sample(4).iterrows():\n#     pd.DataFrame(row[\"ecg_record\"]).plot(subplots=True, title=row[\"file_name\"], ylim=(-2.3,3.2));\n\n    ecg_plot.plot(row[[\"ecg_record_0\",\"ecg_record_1\"]].values.transpose(), sample_rate=40, title=row[\"file_name\"])\n    ecg_plot.show()\n","fa3c11be":"from scipy.signal import find_peaks\n\ndef peaks_dist_hist(arr):\n    px1, _ = find_peaks(arr, height=0.5, threshold=None, distance=50)\n    px2, _ = find_peaks(arr, height=[0.25,0.75], distance=50)\n    pxh1 = np.histogram(np.diff(px1),bins=np.linspace(0, 250, 12))[0]\n    pxh2 = np.histogram(np.diff(px2),bins=np.linspace(0, 250, 12))[0]\n    return px1, np.concatenate([pxh1,pxh2,[np.argmax(pxh1)],[np.argmax(pxh2)]])","34d253de":"x = data_df[\"ecg_record_0\"][0].iloc[0]","39a15b19":"# x\npx, px_hist = peaks_dist_hist(x)","507c445a":"plt.subplots(figsize=(15,6))\nplt.plot(x,\"-\")\nplt.plot(px,x[px],\"x\")","482f3d0c":"def calc_autocorr(arr):\n    autocorr = sts.acf(arr, nlags=128, fft=True)\n    argmax = np.argmax(autocorr[10:70])\n    ahist = np.histogram2d(np.arange(len(autocorr)),autocorr,bins=[np.linspace(10,70,12),np.linspace(-0.2,0.4,12)])[0]\n    return autocorr, argmax, ahist","acac4333":"acor, amax, ahist = calc_autocorr(y)","99aed870":"plt.rcParams[\"figure.figsize\"] = (5,2)\n\nplt.plot(acor)\nplt.plot(amax+10,acor[amax+10],\"x\")","0b3e6efb":"plt.imshow(ahist.transpose())","e8a68907":"lags = [90,120,130,180]\n\ndef calc_lag_hist(arr):\n    h_list = []\n    for l in lags:\n        hist = np.histogram2d(arr[:-lag],arr[lag:],\n                              bins=[np.linspace(-0.3,1.1,60),np.linspace(-0.3,1.1,60)])\n        h_list.append(hist[0].reshape(-1))\n    return np.concatenate(h_list)","941d1d5c":"def calc_features(arr):\n    \n    px, px_hist = peaks_dist_hist(arr)\n    acor, amax, ahist = calc_autocorr(arr)\n    lhist = calc_lag_hist(arr)\n    \n    return np.concatenate([\n        px_hist, \n#         ahist.reshape(-1),\n#         lhist\n    ])\n\ndata_df[\"features_0\"] = data_df[\"ecg_record_0\"].apply(calc_features)\ndata_df[\"features_1\"] = data_df[\"ecg_record_1\"].apply(calc_features)","f91c715e":"seq_0_df = data_df[[\"file_name\",\"ecg_record_0\"]].explode(\"ecg_record_0\", ignore_index=True).reset_index().rename(columns={\"index\":\"time\", \"file_name\":\"seq_id\", \"ecg_record_0\":\"signal_0\"})\nseq_1_df = data_df[[\"file_name\",\"ecg_record_1\"]].explode(\"ecg_record_1\", ignore_index=True).reset_index().rename(columns={\"index\":\"time\", \"file_name\":\"seq_id\", \"ecg_record_1\":\"signal_1\"})\n\nseq_0_df[:3]","d2781da1":"seq_df = seq_0_df.merge(seq_1_df,how=\"left\",on=[\"time\",\"seq_id\"])\nprint(seq_df.shape)\nseq_df[:3]","b2c0df93":"%%time\n\nfc_parameters = {\n    \"large_standard_deviation\": [{\"r\": 0.05}, {\"r\": 0.1}],\n    \"skewness\":None,\n    \"kurtosis\":None,\n    \"mean\":None,\n    \"median\":None,\n    \"absolute_sum_of_changes\":None,\n    \"maximum\":None,\n    \"minimum\":None,\n    \"mean_abs_change\":None,\n    \"mean_change\":None,\n    \"quantile\":[{\n        \"q\":0.01,\"q\":0.02,\"q\":0.03,\"q\":0.04,\"q\":0.05,\n        \"q\":0.42,\"q\":0.44,\"q\":0.46,\"q\":0.48,\"q\":0.5,}],\n#    \"number_cwt_peaks\":[{\"n\":1},{\"n\":2},{\"n\":3},{\"n\":4},{\"n\":5}],\n    \"autocorrelation\":[{\"lag\":10},{\"lag\":12},{\"lag\":13},{\"lag\":14}]\n}\n\nfeat_df = extract_features(seq_df[:], column_id=\"seq_id\", column_sort='time',\n                 default_fc_parameters=fc_parameters,\n                 kind_to_fc_parameters=None,n_jobs=2)\nfeat_df[:4]","72806748":"final_feat_df = (\n    pd.concat([\n        pd.DataFrame(data_df[\"features_0\"].tolist(),index=data_df[\"file_name\"]), \n        pd.DataFrame(data_df[\"features_1\"].tolist(),index=data_df[\"file_name\"])\n    ],1).merge(\n        feat_df,\n        how=\"left\", right_index=True, left_on=\"file_name\" \n    ))\nfinal_feat_df[:3]","d13b03c3":"X, y = final_feat_df.values, final_feat_df.index.str[0].values","03c2d634":"X.shape","3a851613":"# manual selection\n\n# X = X[:,:-120]\n# X.shape","0feef322":"# selection by variance\n\n# X = X[:,X.var(0)>0.15]\n# X.shape","fc73e611":"from sklearn.feature_selection import f_classif","ed71bf17":"# selection by ANOVA test\n\nf,p = f_classif(X,y)\nmask = p<0.15\nsum(mask)\n# X=X[:,mask]","e7f849b3":"from sklearn.preprocessing import LabelEncoder\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.linear_model import LogisticRegression\n\n# encode class values as integers\nlencoder = LabelEncoder()\nlencoder.fit(y)\nencoded_Y = lencoder.transform(y)\n# convert integers to dummy variables (i.e. one hot encoded)\ndummy_y = keras.utils.to_categorical(encoded_Y)","b79321bc":"import sklearn\nsklearn.__version__","aec54634":"estimator_lr = LogisticRegression(multi_class='auto',solver=\"sag\")\nestimator_ovr = OneVsRestClassifier(estimator_lr)\n# estimator_ovr.fit(X,dummy_y.argmax(1))\n# estimator_ovr.score(X,dummy_y.argmax(1))","d05562d9":"\ncv = ShuffleSplit(n_splits=5, test_size=0.3, random_state=0)\n\nestimator_rfc = RandomForestClassifier(\n    n_estimators=170, max_depth=3,max_features=0.7,\n    min_samples_split=6)\n\nfig, axes = plt.subplots(3, 2, figsize=(10, 15))\n\ntitle = \"Learning Curves (RandomForestClassifier)\"\nplot_learning_curve(estimator_rfc, title, X, y, axes=axes[:, 0], ylim=(0, 1.01),\n                    cv=cv, n_jobs=1)\n\ntitle = \"Learning Curves (LogisticRegression)\"\nplot_learning_curve(estimator_ovr, title, X, dummy_y.argmax(1), axes=axes[:, 1], ylim=(0, 1.01),\n                    cv=cv, n_jobs=1)\n","1f430a81":"from keras.wrappers.scikit_learn import KerasClassifier\nimport tensorflow as tf\nimport tensorflow_addons as tfa\nimport tensorflow.keras as keras\nfrom tensorflow.keras import backend as K\nfrom keras.layers import (\n    LSTM, Dropout,RepeatVector,\n    TimeDistributed,Dense, Input,Conv1D, Multiply, Add)\nimport keras.layers as layers","b390fc93":"7680\/\/64","9042a00f":"class Mish(tf.keras.layers.Layer):\n\n    def __init__(self, **kwargs):\n        super(Mish, self).__init__(**kwargs)\n        self.supports_masking = True\n\n    def call(self, inputs):\n        return inputs * K.tanh(K.softplus(inputs))\n\n    def get_config(self):\n        base_config = super(Mish, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n\n    def compute_output_shape(self, input_shape):\n        return input_shape\ndef mish(x):\n\treturn tf.keras.layers.Lambda(lambda x: x*K.tanh(K.softplus(x)))(x)\n \nfrom tensorflow.keras.utils import get_custom_objects\nfrom tensorflow.keras.layers import Activation\nget_custom_objects().update({'mish': Activation(mish)})","8cca83b5":"LR=0.001","e7456220":"kernel_regularizer = None #keras.regularizers.l1_l2(l1=1e-6, l2=1e-5)\nkernel_initializer_=None\nmetrics = [\n        keras.metrics.AUC(name=\"auc\", curve=\"PR\")]\n\ndef WaveNetResidualConv1D(num_filters, kernel_size, stacked_layer):\n\n    def build_residual_block(l_input):\n        resid_input = l_input\n        for dilation_rate in [2**i for i in range(stacked_layer)]:\n            l_sigmoid_conv1d = Conv1D(\n              num_filters, kernel_size, dilation_rate=dilation_rate,\n              padding='same', activation='sigmoid')(l_input)\n            l_tanh_conv1d = Conv1D(\n             num_filters, kernel_size, dilation_rate=dilation_rate,\n             padding='same', activation='mish')(l_input)\n            l_input = Multiply()([l_sigmoid_conv1d, l_tanh_conv1d])\n            l_input = Conv1D(num_filters, 1, padding='same')(l_input)\n            resid_input = Add()([resid_input ,l_input])\n        return resid_input\n    return build_residual_block\n\ndef Classifier(shape_):\n    num_filters_ = 16\n    kernel_size_ = 3\n    stacked_layers_ = [8, 4, 2, 1]\n    l_input = Input(shape=(shape_))\n    dl_input = layers.Reshape((shape_,1))(l_input)\n    x = Conv1D(num_filters_, 1, padding='same')(dl_input)\n    x = WaveNetResidualConv1D(num_filters_, kernel_size_, stacked_layers_[0])(x)\n    x = Conv1D(num_filters_*2, 1, padding='same')(x)\n    x = layers.MaxPool1D(4)(x)\n    x = WaveNetResidualConv1D(num_filters_*2, kernel_size_, stacked_layers_[1])(x)\n    x = Conv1D(num_filters_*4, 1, padding='same')(x)\n    x = layers.MaxPool1D(4)(x)\n    x = WaveNetResidualConv1D(num_filters_*4, kernel_size_, stacked_layers_[2])(x)\n    x = Conv1D(num_filters_*8, 1, padding='same')(x)\n    x = WaveNetResidualConv1D(num_filters_*8, kernel_size_, stacked_layers_[3])(x)\n    x = layers.MaxPool1D(4)(x)\n#     l_output = Dense(1, activation='relu')(x)\n    l_output = Dense(3, activation='sigmoid')(x)\n    l_output = layers.Reshape((3,))(l_output)\n    \n    model = keras.models.Model(inputs=[l_input], outputs=[l_output])\n    opt = keras.optimizers.Adam(lr=LR)\n    opt = tfa.optimizers.SWA(opt)\n    model.compile(loss=keras.losses.BinaryCrossentropy(), optimizer=opt, metrics=['accuracy'])\n    return model\n\nclf_model = Classifier(120)\nclf_model.summary()","96246aa6":"X_arr = data_df[\"ecg_record_0\"].values\nY = data_df[\"file_name\"].str[0].values\n\nX_arr = np.concatenate([i.reshape(1,-1) for i in X_arr],0)\nX_arr.shape","c6021361":"from sklearn.model_selection import train_test_split","66a497fc":"np.random.seed(1234)\ntr_X, ts_x, tr_y, ts_y = train_test_split(X_arr,dummy_y,test_size=0.2)","0e4e5136":"clf_model = Classifier(120)\n\nes = keras.callbacks.EarlyStopping( monitor='val_loss',\n            min_delta=0.00001,\n            patience=8,\n            verbose=1, restore_best_weights=True)\nhist=clf_model.fit(tr_X,tr_y, \n              verbose=1, epochs=30, validation_split=0.2, \n              callbacks=[es], batch_size=300)","5604518c":"plt.plot(clf_model.history.history[\"loss\"])\nplt.plot(clf_model.history.history[\"val_loss\"])\n# aemodel.history.history","7e37978e":"from sklearn.metrics import classification_report, confusion_matrix","8c9d5793":"lencoder.classes_","710df888":"cr = classification_report(tr_y.argmax(1),clf_model.predict(tr_X).argmax(1))\nprint(cr)","f94fc643":"cr = classification_report(ts_y.argmax(1),clf_model.predict(ts_x).argmax(1))\nprint(cr)","36419963":"cm = confusion_matrix(tr_y.argmax(1),clf_model.predict(tr_X).argmax(1))\n\npd.DataFrame(cm,columns=lencoder.classes_,index=lencoder.classes_).style.background_gradient()","0b08cbea":"ts_y_pred = clf_model.predict(ts_x)\ncm = confusion_matrix(ts_y.argmax(1),ts_y_pred.argmax(1))\n\npd.DataFrame(cm,columns=lencoder.classes_,index=lencoder.classes_).style.background_gradient()","ba6d8a8f":"from sklearn.metrics import roc_curve, auc\nplt.rcParams[\"figure.figsize\"] = (8,6)\n\nn_classes = 3\n# Compute ROC curve and ROC area for each class\nfpr = dict()\ntpr = dict()\nroc_auc = dict()\nfor i in range(n_classes):\n    fpr[i], tpr[i], _ = roc_curve(ts_y[:, i], ts_y_pred[:, i])\n    roc_auc[i] = auc(fpr[i], tpr[i])\n\n# Compute micro-average ROC curve and ROC area\nfpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(ts_y.ravel(), ts_y_pred.ravel())\nroc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n\n# Plot ROC curve\nplt.figure()\nplt.plot(fpr[\"micro\"], tpr[\"micro\"],\n         label='micro-average ROC curve (area = {0:0.2f})'\n               ''.format(roc_auc[\"micro\"]))\nfor i in range(n_classes):\n    plt.plot(fpr[i], tpr[i], label='ROC curve of class {0} (area = {1:0.2f})'\n                                   ''.format(i, roc_auc[i]))\n\nplt.plot([0, 1], [0, 1], 'k--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Some extension of Receiver operating characteristic to multi-class')\nplt.legend(loc=\"lower right\")\nplt.show()","ef51a686":"## Calc Features for all samples ","d8b420f4":"## lag histogram","3886b986":"# automate feature engineering with autoencoder","7013da1e":"# Feature Engineering","6cfc3e3f":"# Performance","7117b2a6":"## lag plot","bfe3f20a":"# About Data:\n- Recordings have two-channel ECG \n- 128 samples per signal per second (128 fs)\n- Classes:\n  - N: non-terminating AF\n  - S: AF that terminates one minute after the end of the record\n  - T: AF that terminates immediately (within one second) after the end of the record","2790ed10":"## Load Data","f6b3b2ff":"## autocorr","3c31464b":"### Check for Data type, Null and Unique Values","114b185d":"## Peak distances","36dea2ad":"## data preparation for tsfresh","473671a0":"## Identify Data Range","d8c2e6bf":"# Feature Selection","2d399204":"# Model Preparation","4f19f99b":"# Preprocessing","2189ec0c":"# Exploration","f1222bf5":"## Visualize Raw Data","728bda4d":"# Objective:\n- Predicting spontaneous termination of atrial fibrillation (AF)"}}