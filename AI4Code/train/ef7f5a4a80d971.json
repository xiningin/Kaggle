{"cell_type":{"30707641":"code","e42f2ea1":"code","fe0c4146":"code","a3cb5b2e":"code","1f6cbdd6":"code","1325c6f3":"code","413fbf0c":"code","0d3a8f8c":"code","6ea8ce60":"code","b9b87287":"code","618e6dd0":"code","7efe10e3":"code","fbca1bf8":"code","d5617b34":"code","d0a77dcf":"code","55573da3":"code","b2d19406":"code","df70171c":"code","bba588b7":"code","3d182d51":"code","b8305ce3":"code","4f43bf1d":"code","bf19bf1b":"code","25b2e8a3":"code","8a5a8d6a":"code","24237554":"code","a2536b4f":"code","2b7f5bee":"code","31da1ccf":"code","c975a55d":"code","a8b6957a":"markdown","6cf02312":"markdown","69bfa4bd":"markdown","46144cfb":"markdown","57c80014":"markdown","ae0c7032":"markdown","423f4617":"markdown","e4ec2b47":"markdown","6a5b4a66":"markdown","7cd2bccb":"markdown","c01e3c68":"markdown","d8cb77c7":"markdown","9fbfb3ba":"markdown","a076e365":"markdown","e91bf0ba":"markdown","4d8e8e58":"markdown","726d91d5":"markdown","cf2a9058":"markdown","1bfde206":"markdown","9b0fa1c5":"markdown","b613c8e4":"markdown","3254786c":"markdown","6688e39a":"markdown","9c97610f":"markdown","074d78c7":"markdown"},"source":{"30707641":"# importing liberaries\nimport torch\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom tqdm.notebook import tqdm\n\n# importing builtin datasets from torchvision and transforms to transform dataset into tensor.\nfrom torchvision import datasets, transforms\n","e42f2ea1":"# downloading MNIST dataset's train and train data.\nmnist_train = datasets.MNIST(root=\".\/datasets\", train=True, transform=transforms.ToTensor(), download=True)\nmnist_test = datasets.MNIST(root=\".\/datasets\", train=False, transform=transforms.ToTensor(), download=True)","fe0c4146":"print(\"Number of MNIST training examples: {}\".format(len(mnist_train)))\nprint(\"Number of MNIST test examples: {}\".format(len(mnist_test)))","a3cb5b2e":"# checking one example image.\nimage, label = mnist_train[3]\n\nprint(\"Default Image shape: {}\".format(image.shape))","1f6cbdd6":"# reshaping the array into 28 X 28 dimensions as it is a image of 28 X 28 pixels and not 1 X 28 X 28 dimensions.\nimage = image.reshape([28,28])\nprint(\"Reshaped Image shape: {}\".format(image.shape))","1325c6f3":"# plotting the image onto graph with grayscale.\nplt.imshow(image, cmap=\"gray\")\nprint(\"Label for this image is: {}\".format(label))","413fbf0c":"# loading our MNIST train and test datasets in form of batches with batch_size=100(Simply dividing our data into 100 elements multi-arrays\/Tensors).\ntrain_loader = torch.utils.data.DataLoader(mnist_train, batch_size=100, shuffle=True)\ntest_loader = torch.utils.data.DataLoader(mnist_test, batch_size=100, shuffle=False)","0d3a8f8c":"# choosing one batch to test.\ndata_train_loader = iter(train_loader)\n# extracting input and label of the batch of 100 images.\nimages, labels = data_train_loader.next()","6ea8ce60":"print(\"Shape of the minibatch of train loader's images: {}\".format(images.shape))\nprint(\"Shape of the minibatch of train loader's labels: {}\".format(labels.shape))","b9b87287":"# reshaping our examples from 2-D to 1-D as we input single dimension array.\nx = images.view(-1, 28*28)\nprint(\"The shape of input x: {}\".format(x.shape))","618e6dd0":"#  creating our model parameters W, b randomly. 784 rows for 784 featuers\/pixels of an example\/image and 10 colums for 10 classes(0-9).\nW = torch.randn(784, 10)\/np.sqrt(784)\n\n# Assigning\/allowing gradients to these parameters as we will be using stochastic gradient descent on these parameters and will be needing gradient w.r.t these.\nW.requires_grad_()\nb = torch.zeros(10, requires_grad=True)","7efe10e3":"# doing dot product of examples with filters to find amount of correlation between two and adding bias.\ny = torch.matmul(x, W) + b","fbca1bf8":"print(y[0,:])","d5617b34":"# Normalizing or making our correlated values positive by using exponents.\npy_eq = torch.exp(y)\/torch.sum(torch.exp(y), dim=1, keepdim=True)\nprint(\"py[0] from equation: {}\".format(py_eq[0]))","d0a77dcf":"# Rather then manually writing equation, we can use inbuilt function of torch.nn.functional .\nimport torch.nn.functional as F\n\n# Using softmax function to normalize our wieghts or correlated values of examples.\npy = F.softmax(y, dim=1)\nprint(\"py[0] with torch.nn.functional.softmax function: {}\".format(py[0]))","55573da3":"# calculating loss function for optimizing manually by writing loss function i.e. adding our predicted value(correlated values of true labels).\ncross_entropy_eq = torch.mean(-torch.log(py_eq)[range(labels.shape[0]),labels])\nprint(\"cross entropy from equation: {}\".format(cross_entropy_eq))","b2d19406":"# Using inbuilt function of torch to calculate cross entropy.\ncross_entropy = F.cross_entropy(y, labels)\nprint(\"cross entropy from torch.nn.functional.cross_entropy function: {}\".format(cross_entropy))","df70171c":"# optimizing paramteres with step size 0.1 and Stochastic Gradient Descent.\noptimizer = torch.optim.SGD([W,b], lr=0.1)\n\n# calculating gradients of allowed parameters w.r.t optimizing condition i.e. loss function.\ncross_entropy.backward()","bba588b7":"# gradient before optimizing.\nb.grad","3d182d51":"# optimzing with step size 0.1\noptimizer.step()\n\n# gradient w.r.t b after optimizing\nprint(b)","b8305ce3":"print(\"b.grad before zero_grad:{}\".format(b.grad))","4f43bf1d":"# Gradients doesn't update but accumulate with every optimizing step. So, have to zero out before updating gradients.\noptimizer.zero_grad()\nprint(\"b.grad after zero_grad:{}\".format(b.grad))","bf19bf1b":"# Filter matrix\nprint(W)","25b2e8a3":"# Doing the same thing for all test batches.\nfor images, labels in tqdm(train_loader):\n    # zero out the gradients.\n    optimizer.zero_grad()\n\n    # preapring data.\n    x = images.view(-1, 28*28)\n    y = torch.matmul(x, W) + b\n    \n    cross_entropy = F.cross_entropy(y, labels)\n    cross_entropy.backward()\n    optimizer.step()","8a5a8d6a":"#  Models's learned parameters\nprint(\"Filter:{}\".format(W))\nprint(\"Bias:{}\".format(b))","24237554":"lin = torch.nn.Linear(784, 10)\noptimizer = torch.optim.SGD(lin.parameters(), lr=0.1)\nfor images, labels in tqdm(train_loader):\n    y = lin(images.view(-1, 784))\n    optimizer.zero_grad()\n    cross_entropy = F.cross_entropy(y, labels)\n    cross_entropy.backward()\n    optimizer.step()\nprint(\"Filter: {}\".format(list(lin.parameters())[0]))\nprint(\"Bias: {}\".format(list(lin.parameters())[1]))","a2536b4f":"# No. of correct values assigned to 0.\ncorrect = 0\ntotal = len(mnist_test)","2b7f5bee":"# Don't have to calculate gradient now. just have to check correct predictions. But as parameters were created with allowd gradient calculation have to cancel that.\nwith torch.no_grad():\n    # iterating on test batches.\n    for images, labels in tqdm(test_loader):\n        # preparing data and calcuting correlated values.\n        x = images.view(-1, 784)\n        y = torch.matmul(x, W) + b\n\n        # Our model prediction is the max correlated value of the given ten classes values.\n        predictions = torch.argmax(y, dim =1)\n\n        # Checking correct values.\n        correct += torch.sum((predictions == labels).float())","31da1ccf":"#  Test Accuracy\nprint('Test Accuracy: {}'.format(correct\/total))\nprint('Test correct: {} of {}'.format(correct, total))","c975a55d":"# Plotting learned filters.\nfig, ax = plt.subplots(1, 10, figsize=(20,2))\nfor digit in range(10):\n    ax[digit].imshow(W[:,digit].detach().view(28,28), cmap='gray')","a8b6957a":"Accuracy: correcttly predicted values\/ total values","6cf02312":"Performing optimization for all batches","69bfa4bd":"Check our filters by plotting them on graphs","46144cfb":"Libraries we will use:\n* Numpy\n* Matplotlib\n* Torch\n* Torchvision\n* tqdm","57c80014":"Checking number of test and training examples","ae0c7032":"Check our input image examples with filter and find correlation betwwen them.","423f4617":"Test our model with test set. ","e4ec2b47":"We have both negative and positive values. Normalize it first with the help of exponents such that values of one example sums upto 1. ","6a5b4a66":"we can optimize paramters with the help of loss function.\n\\begin{align}\nH_{y'}(y)=-\\sum_i y'_i \\text{log}(y_i)\n\\end{align}","7cd2bccb":"So, let begin modelling.\nLet's divide our dataset into batches of batch size of 100.","c01e3c68":"Perform the same thing for all batches. Gradients don't update ut accumulate with every optimization step. Zero out gradients before next optimization step.","d8cb77c7":"**TESTING PERFORMANCE**","9fbfb3ba":"### **Optimizing Parameters**","a076e365":"Checking values for first example of correlated matrix.","e91bf0ba":"### **Preparing data**","4d8e8e58":"Note, Both have different learned parameters. That's because initially we randomly created model's filters. It will produce different results every time we run our model.","726d91d5":"Torch.nn module already has all the function that we need for logistic function. We can train our data with the help of those functions too.","cf2a9058":"Creating parameters of logistic regression model: filter, bias(W, b).","1bfde206":"It is a 3-D tensor(same as a matrix). we will need a 2-D matrix for plotting it as an image through matplotlib.","9b0fa1c5":"# **Multi-Class Logistic Regression Model with Pytorch on MNIST Dataset**","b613c8e4":"The same can be done with functions in torch.nn.functional. Softmax is used for normalizing values.\n\\begin{align}\np(y_i) = \\text{softmax}(y_i) = \\frac{\\text{exp}(y_i)}{\\sum_j\\text{exp}(y_j)}\n\\end{align}","3254786c":"We will download MNIST dataset present in torchvision.datasets","6688e39a":"Modelling a multi-class logistic regression model with pytorch to predict the digit(0-9) in an image of 28X28 pixels.","9c97610f":"Reshaping our matrix into 2-D with view in torch module which act just like reshape of numpy.","074d78c7":"Let's check one example of training set. It contains both input data and corresponding output(label) in form of tuple. Let's extract features:"}}