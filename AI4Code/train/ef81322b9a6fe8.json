{"cell_type":{"fc585bfa":"code","f0a855e3":"code","fb9cb633":"code","c4c97fb5":"code","338828c1":"code","e5b55744":"code","efcac99f":"code","e1769250":"code","2a294321":"code","829e44ff":"code","9a9121f6":"code","67c0da70":"code","cc863c75":"code","b83f6143":"code","1c1ba2c1":"code","c1e45922":"code","139e6ae5":"code","aca7274a":"code","7da19d8a":"code","1c72bde5":"code","6b000cb3":"markdown","d36cf605":"markdown","a34175a4":"markdown","2d31f025":"markdown","92d853fb":"markdown","bdfcd557":"markdown","7f9ee2d2":"markdown","856169aa":"markdown","72f2e99b":"markdown","5ac8c104":"markdown","6f0aa5fe":"markdown","32ea18d3":"markdown","63c8b3d0":"markdown","eb12d148":"markdown","3c9ad167":"markdown","021be97f":"markdown","31ac6f12":"markdown","e5cf98c1":"markdown","1664d515":"markdown","0975ec0f":"markdown","1779517f":"markdown","9464a68e":"markdown"},"source":{"fc585bfa":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nimport os","f0a855e3":"train = pd.read_csv('..\/input\/train.csv')","fb9cb633":"train.head()","c4c97fb5":"sns.heatmap(train.isnull(),yticklabels=False,cbar=False,cmap='viridis')\nplt.title('Train')","338828c1":"age_mean = train['Age'].mean()\ntrain.fillna(age_mean, inplace=True)","e5b55744":"sns.heatmap(train.isnull(),yticklabels=False,cbar=False,cmap='viridis')\nplt.title('Train')","efcac99f":"# Add one-hot encoding for the Sex, as we know that women were more likely to survive than men\nsex_cats_dummies    = pd.get_dummies(train['Sex'], prefix='Sex', drop_first=True) \n\n# Add dummies for Pclass as well, as it is a category, although stored as an int\npclass_cats_dummies = pd.get_dummies(train['Pclass'], prefix='Pclass', drop_first=True)\n\n# Add one-hot-encoded features to the main dataset\ntrain = pd.concat([train, sex_cats_dummies, pclass_cats_dummies], axis=1)\n\n# Drop categorical columns. Feel free to extract more information from those before droping\ntrain.drop(['Sex','Embarked','Name','Ticket', 'Pclass', 'PassengerId', 'Cabin'], axis=1, inplace=True)","e1769250":"train.head()","2a294321":"plt.figure(figsize=(10,6))\nsns.heatmap(train.corr(), annot=True)","829e44ff":"from sklearn.model_selection import train_test_split\n# We use the 80\/20 split here to match the 5-fold cross-validation in the next section\nX_train, X_test, y_train, y_test = train_test_split(train.drop('Survived',axis=1), \n                                                    train['Survived'], test_size=0.20, \n                                                    random_state=314)","9a9121f6":"# Added to suppress warnings from model fiting of type\n# DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. \nimport warnings\nwarnings.filterwarnings('ignore', 'The truth value of an empty array is ambiguous. .*')","67c0da70":"import xgboost as xgb\nimport lightgbm as lgb\n\nfrom sklearn.metrics import accuracy_score, make_scorer\nfrom sklearn.model_selection import cross_validate","cc863c75":"# Let's define several simple randomly-picked LighGBM and XGBoost models.\nmodels = {'XGB_depth3':  xgb.XGBClassifier(max_depth=3, random_state=314, seed=314, silent=True),\n          'XGB_depth5':  xgb.XGBClassifier(max_depth=5, random_state=314, seed=314, silent=True),\n          'XGB_depth7':  xgb.XGBClassifier(max_depth=7, random_state=314, seed=314, silent=True),\n          'XGB_depth7_regularised':  xgb.XGBClassifier(max_depth=7, colsample_bytree= 0.90, reg_lambda= 1, subsample= 0.80, random_state=314, seed=314, silent=True),\n          'LGB_depthINFleaves7': lgb.LGBMClassifier(max_depth=-1, num_leaves=7, random_state=314),\n          'LGB_depthINFleaves20': lgb.LGBMClassifier(max_depth=-1, num_leaves=20, random_state=314),\n          'LGB_depth3':   lgb.LGBMClassifier(max_depth=3, random_state=314),\n          'LGB_depth5':  lgb.LGBMClassifier(max_depth=5, random_state=314)}","b83f6143":"# These will be pandas.DataFrame's to store the final performance values for plotting\nacc_valid_summary = pd.DataFrame(index=sorted(models.keys()), columns=['raw'], dtype=np.float32)\nacc_train_summary = pd.DataFrame(index=sorted(models.keys()), columns=['raw'], dtype=np.float32)\nfit_time_summary  = pd.DataFrame(index=sorted(models.keys()), columns=['raw'], dtype=np.float32)\n\n\ndef benchmarkModels(models_dict, transform_name='raw'):\n    '''\n    The function is used to evaluate performance of each algorithm\n    \n    Parameters\n    ----------\n    models_dict: dictionary \n        A dictionary of models for evaluation. \n        The items are name string and sklearn-like classifier pairs\n    trasform_name: string\n        Not used in this example, but allows one to evaluate performance on different data transformations\n    '''\n    for clf_name, clf in models_dict.items():\n        clf.fit(X_train, y_train)\n        acc_valid_summary.loc[clf_name, transform_name] = accuracy_score(y_test, clf.predict(X_test))\n        acc_train_summary.loc[clf_name, transform_name] = accuracy_score(y_train, clf.predict(X_train))","1c1ba2c1":"def plotPerformance(perf_valid, perf_test, perf_fit_time=None, suff=''):\n    n_plots = 3 if isinstance(perf_fit_time, pd.DataFrame) else 2\n    #create a figure with 2 or subplots\n    fig, ax = plt.subplots(ncols=n_plots, figsize=(12,6))\n    # increase the white space to fil it long Y axis labels\n    fig.subplots_adjust(wspace=1.25)\n    \n    # The comparison of the two tells us about amount of overtraining\n    # performance of the VALIDATION sample \n    sns.heatmap(perf_valid, cmap='Blues', annot=True, vmin=0.75, vmax=0.9, ax=ax[0])\n    ax[0].set_title('Accuracy on VALIDATION sample ' + suff)\n    # performance of the TRAINING sample \n    sns.heatmap(perf_test, cmap='Blues', annot=True, vmin=0.75, vmax=0.9, ax=ax[1])\n    ax[1].set_title('Accuracy on TRAIN sample ' + suff)\n    # Plot also trainign time, if provided\n    if len(ax) > 2:\n        sns.heatmap(perf_fit_time, cmap='Blues', annot=True, ax=ax[2])\n        ax[2].set_title('Training time ' + suff)","c1e45922":"benchmarkModels(models)","139e6ae5":"plotPerformance(acc_valid_summary, acc_train_summary, suff='(Train\/Test split)')","aca7274a":"acc_valid_cv_summary = pd.DataFrame(index=sorted(models.keys()), columns=['raw'], dtype=np.float32)\nacc_train_cv_summary = pd.DataFrame(index=sorted(models.keys()), columns=['raw'], dtype=np.float32)\nfit_time_cv_summary  = pd.DataFrame(index=sorted(models.keys()), columns=['raw'], dtype=np.float32)\n\ndef benchmarkModelsCV(models_dict, transform_name='raw'):\n    for clf_name, clf in models_dict.items():\n        # Let's run cross validation on the classifier\n        # Note, that  we run it on the full train+test sample:\n        # this increases statistics and we do not need the test sample\n        score = cross_validate(clf,\n                               X = train.drop('Survived', axis=1),\n                               y = train['Survived'],\n                               scoring=make_scorer(accuracy_score, greater_is_better=True),\n                               cv=5, \n                               return_train_score=True)\n        # save evaluated performance results\n        acc_valid_cv_summary.loc[clf_name, transform_name] = score['test_score'].mean()\n        acc_train_cv_summary.loc[clf_name, transform_name] = score['train_score'].mean()\n        fit_time_cv_summary.loc[clf_name, transform_name] = score['fit_time'].mean()\n","7da19d8a":"benchmarkModelsCV(models)","1c72bde5":"plotPerformance(acc_valid_cv_summary, acc_train_cv_summary, perf_fit_time=fit_time_cv_summary, suff='(CV mean)')","6b000cb3":"What did we observe?\n* All models lead to a high accuracy score\n* All models are somewhat overtrained (the score on the training sample is higher than the score on the validation)\n* There seems to be some difference in performance.\n\nBut it the comparison stable? Are the differences just fluctuations? To understnad it better, let's run cross-validation!","d36cf605":"So all models seem to show very similar performance, and differences in the simple train\/test split were fluctuations. This makes the case for the cross validation\nAlso note that **LightGBM is faster than XGBoost, while showing the same performance**. The difference in training time as shown on the right plot above is not huge, it it would become much larger, if we would extract more features to train on. ","a34175a4":"Let's look what is the structure of our data","2d31f025":"What happened to the missing values?","92d853fb":"### Load the data and do minimal visualisation","bdfcd557":"Let's import `lighgbm` and `xgboost` packages as well as functions that we will need to evaluate accuracy.","7f9ee2d2":"It  seems that the gender and, `Sex`, the social-economic class, `Pclass`, have the strongest correlation with the survival rate","856169aa":"Fisrt, check for missing values (yellow means that the column entry was `NaN` we want everything to be violet)","72f2e99b":"Now it is time to define several models. Note, that we will use the sklearn interface provided together with each of those tools. This allows one you use them identically to many algorithms implemented in `sklearn` itself. And to profit from the large ecosystem of handy tools inside sklearn.","5ac8c104":"# Preprocessing","6f0aa5fe":"### Import relevant packages ","32ea18d3":"Note, we drop one of columns generated by one-hot encoding, as it is redundant.","63c8b3d0":"`Cabin` column in categorical and hass too many missing values, so we intend to drop it.\nThe `Age` column is not hopeless (and maybe children had higher chance to survive?), thus let's impute missing values.\n\nNote, that **doing it on full training sample is WRONG**, although you will see it being done in many other examples. The problem is that we intend to use a sub-sample of `train` to evaluate performance of model, that we are going to build. And imuting mean or any other data-based quantity leads to a data leakage, thus **biased performance estimate**. The same argument applies if one intends to use cross validation  instead of a single train\/test split. In the case of cross validation one would need to do imutation independently for each split.","eb12d148":"# LightGBM vs XGBoost (sklearn API)\n**Disclaimer:** The purpose of the kernel is to illustrate usage of both XGBoost and LightGBM and to compare their performance on the toy data set. Thus, there will be no exploratory data analysis (EDA) or feature engineering. We will do a basic data cleaning to drop missing values and to impute Age and that is it. You are invited to extend this quick analysis with more data features. One can find many excellent kernels explaining how to do it. \n\n**You have most likely heard many times that XGBoost and LightGBM are trandy buzz words.** Let's get some experience with them and compare their performance!\n\n**Why to bother about this notebook?** It gives you experience with the sklearn interfaces of `lightgbm` and `xgboost` instead of the native `.train()` interface. This allows you to make full use of the sklearn ecosystem and you do not have to convert your data into package-specific format (tools do it themselves under the hood)\n\n### Table of contents\n* [Preprocessing](#Preprocessing)\n* [Which variables have the strongest correlation with the target variable?](#Which-variables-have-the-strongest-correlation-with-the-target-variable?)\n* [Simple Train Test evaluation](#Simple-Train-Test-evaluation)\n* [Advanced evaluation based on cross validation](#Advanced-evaluation-based-on-cross-validation)\n* [Where to go from here](#Where to-go-from-here)","3c9ad167":"### Where to go from here\n* Start adding more features\n* Add hyperparameter optimisation\n* Add early stopping to the GBT training to avoid overtraining\n* Try to add data transformations. Are those expected to help?\n* Anything make a suggestion how to improve this notebook :)","021be97f":"Let's split the data into a simple train\/test split","31ac6f12":"# Simple Train Test evaluation","e5cf98c1":"Let's do some basic data manipulations see other kernels, e.g. [this one](https:\/\/www.kaggle.com\/ldfreeman3\/a-data-science-framework-to-achieve-99-accuracy\/notebook), for examples of how to do it more systematically and clearly.","1664d515":"# Advanced evaluation based on cross validation\nUsage of cross validation requires building a pipeline and redoing any data transforms on the unprocessed features. If one tries it on the stored features, then one sees strange effects, when results for all transformed data are the same. This is due to data leakage- transformers were trained on the train sub-sample and thus they leak info about left-out folds. This topic is not illustrated in the current example, but will be added as a separate kernel in the future","0975ec0f":"Define helper functions that will do fitting and plotting","1779517f":"What is the structure after preprocessing?","9464a68e":"### Which variables have the strongest correlation with the target variable?"}}