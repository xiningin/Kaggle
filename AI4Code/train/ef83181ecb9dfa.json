{"cell_type":{"76e2b516":"code","1da883c6":"code","3e679e24":"code","f9542b43":"code","e8453d72":"code","24c264a3":"code","8143c807":"code","f3ac68f2":"code","9b578015":"code","0394b2c4":"code","e4e7f251":"code","0822ae23":"code","4c333421":"markdown","ddb33728":"markdown","8857ea0e":"markdown","74d741a5":"markdown","f3e5561b":"markdown","dd17478f":"markdown","b682b559":"markdown","8809f1eb":"markdown","48452398":"markdown","296e96bd":"markdown","45a0be96":"markdown","ffae8b66":"markdown"},"source":{"76e2b516":"# Import helpful libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings \n\nfrom sklearn import linear_model\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.ensemble import RandomForestRegressor,GradientBoostingRegressor\nfrom sklearn.feature_selection import mutual_info_regression\nfrom sklearn.model_selection import cross_val_score, GridSearchCV\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\n\nfrom xgboost import XGBRegressor\n\n# Load the data, and separate the target\ntrain_file_path = '..\/input\/train.csv'\ntest_data_path = '..\/input\/test.csv'\n\ntrain_data = pd.read_csv(train_file_path)\ntest_data = pd.read_csv(test_data_path)\nY = train_data.SalePrice\ntrain_data.drop(columns=['SalePrice'],axis=1,inplace=True)\n\nwarnings.simplefilter('ignore')\npd.set_option('display.max_rows',None)","1da883c6":"plt.scatter(train_data.TotalBsmtSF,Y)\noutliers=((train_data['GrLivArea']>4000) & (Y<300000))|((train_data['OverallCond']==2) & (Y>300000))\ntrain_data.drop(train_data[outliers].index,inplace=True)\nY.drop(Y[outliers].index,inplace=True)","3e679e24":"plt.hist(train_data.LotFrontage,bins=50)\nplt.show()\ntrain_data['LotArea']=train_data.LotArea.apply(np.log1p)\ntest_data['LotArea']=test_data.LotArea.apply(np.log1p)\ntrain_data['LotFrontage']=train_data.LotFrontage.apply(np.log1p)\ntest_data['LotFrontage']=test_data.LotFrontage.apply(np.log1p)","f9542b43":"numerical=['LotFrontage', 'BsmtFinSF1', 'WoodDeckSF', 'MSSubClass', '2ndFlrSF', 'TotRmsAbvGrd', 'MasVnrArea', 'YearRemodAdd', 'ScreenPorch', 'OverallCond', 'EnclosedPorch', 'GrLivArea', 'GarageArea', 'LowQualFinSF', 'OverallQual', 'TotalBsmtSF', 'KitchenAbvGr', 'GarageYrBlt', 'BedroomAbvGr', '1stFlrSF', 'YearBuilt', 'OpenPorchSF', 'LotArea', 'BsmtUnfSF']\nmost_frequent_imp_cat=['MSZoning', 'LotShape', 'LandContour', 'LotConfig', 'Neighborhood', 'Condition1', 'Condition2', 'BldgType', 'HouseStyle', 'RoofStyle', 'RoofMatl', 'Exterior1st', 'Exterior2nd', 'MasVnrType', 'ExterQual', 'ExterCond', 'Foundation', 'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', 'Heating', 'HeatingQC', 'CentralAir', 'Electrical', 'KitchenQual', 'Functional', 'FireplaceQu', 'GarageType', 'GarageFinish', 'GarageQual', 'GarageCond', 'PavedDrive', 'SaleType', 'SaleCondition', 'BsmtFullBath', 'BsmtHalfBath', 'FullBath', 'HalfBath', 'YrSold', 'Fireplaces']\nconstant_imp_cat=['PoolQC','Alley','Fence']\ncategorical=most_frequent_imp_cat+constant_imp_cat","e8453d72":"#print(train_data.isnull().sum().sort_values(ascending=False))\n#num_imp=SimpleImputer(strategy='median')\n#cat_imp=SimpleImputer(strategy='most_frequent')\n#train_data[numerical]=num_imp.fit_transform(train_data[numerical])\n#train_data[categorical]=cat_imp.fit_transform(train_data[categorical])","24c264a3":"def get_score(pipeline,cv,X,y):\n    return -1*cross_val_score(pipeline,X,y,cv=cv,scoring='neg_mean_absolute_error').mean()\n\ndef mi_scores(X, y,discrete):\n    if(discrete):\n        mi_scores = mutual_info_regression(X, y, random_state=0,discrete_features=X.dtypes==int)\n    else:\n        mi_scores=mutual_info_regression(X,y,random_state=0)\n    mi_scores = pd.Series(mi_scores, name=\"MI Scores\", index=X.columns)\n    mi_scores = mi_scores.sort_values(ascending=False)\n    return mi_scores\ndef plot_mi_scores(scores,feature_type):\n    scores=scores.sort_values(ascending=True)\n    plt.barh(scores.index,scores)\n    plt.title('Mutual Information Scores '+feature_type+ ' variables')","8143c807":"'''\nX=train_data[categorical].copy()\nfor col in categorical:\n    X[col],_=X[col].factorize()\ncategorical=X.dtypes==int\nscores=mi_scores(train_data[numerical],Y,discrete=False)\nplt.figure(dpi=100, figsize=(8, 5))\nplot_mi_scores(scores,'numerical')\nplt.show()\n\n\nplt.figure(dpi=100,figsize=(8,8))\nscores=mi_scores(X,Y,discrete=True)\nplot_mi_scores(scores,'categorical')\nplt.show()\n'''","f3ac68f2":"X_train=train_data[constant_imp_cat+most_frequent_imp_cat+numerical]\nX_test=test_data[constant_imp_cat+most_frequent_imp_cat+numerical]","9b578015":"\nnumtr=SimpleImputer(strategy='median')\ncattr=Pipeline(steps=[('Imputer',SimpleImputer(strategy='most_frequent')),('OHE',OneHotEncoder(handle_unknown='ignore',sparse=False))])\ncontr=Pipeline(steps=[('Imputer',SimpleImputer(strategy='constant',fill_value='NA')),('OHE',OneHotEncoder(handle_unknown='ignore',sparse=False))])\nprepr=ColumnTransformer(transformers=[('num',numtr,numerical),('cons',contr,constant_imp_cat),('cat',cattr,most_frequent_imp_cat)])\n#xgb=XGBRegressor(n_estimators=6000,learning_rate=0.01,random_state=0)\ngb=GradientBoostingRegressor(n_estimators=6000,learning_rate=0.03,max_depth=2)\nmodel=Pipeline(steps=[('preprocessor',prepr),('model',gb)])\n","0394b2c4":"'''\nnumtr=SimpleImputer(strategy='median')\ncattr=Pipeline(steps=[('Imputer',SimpleImputer(strategy='most_frequent')),('OHE',OneHotEncoder(handle_unknown='ignore',sparse=False))])\nprepr=ColumnTransformer(transformers=[('num',numtr,numerical),('cat',cattr,categorical)])\ngb=GradientBoostingRegressor()\nmodel=Pipeline(steps=[('preprocessor',prepr),('model',gb)])\nparam_grid = {'model__n_estimators': [5000,6000,7000,8000],'model__learning_rate': [0.01,0.02,0.03,0.04,0.05],\n    'model__max_depth':[1,2,3,4]}\ngridsearch=GridSearchCV(model,param_grid,n_jobs=4)\ngridsearch.fit(X_train,Y)\n\n'''","e4e7f251":"#score=get_score(model,5,X_train,Y)\n#print(\"Score:{}\".format(score))\nmodel.fit(X_train,Y)\n#print(gridsearch.best_params_)\n#test_preds=gridsearch.best_estimator_.predict(X_test)\ntest_preds=model.predict(X_test)","0822ae23":"# Run the code to save predictions in the format used for competition scoring\n\noutput = pd.DataFrame({'Id': test_data.Id,\n                       'SalePrice': test_preds})\noutput.to_csv('submission.csv', index=False)","4c333421":"# Deleting Outliers","ddb33728":"# Training the model and Predicting","8857ea0e":"# Generate a submission\n\nRun the code cell below to generate a CSV file with your predictions that you can use to submit to the competition.","74d741a5":"# Creating the Pipelines & Model","f3e5561b":"# Submit to the competition\n\nTo test your results, you'll need to join the competition (if you haven't already).  So open a new window by clicking on **[this link](https:\/\/www.kaggle.com\/c\/home-data-for-ml-course)**.  Then click on the **Join Competition** button.\n\n![join competition image](https:\/\/i.imgur.com\/axBzctl.png)\n\nNext, follow the instructions below:\n1. Begin by clicking on the **Save Version** button in the top right corner of the window.  This will generate a pop-up window.  \n2. Ensure that the **Save and Run All** option is selected, and then click on the **Save** button.\n3. This generates a window in the bottom left corner of the notebook.  After it has finished running, click on the number to the right of the **Save Version** button.  This pulls up a list of versions on the right of the screen.  Click on the ellipsis **(...)** to the right of the most recent version, and select **Open in Viewer**.  This brings you into view mode of the same page. You will need to scroll down to get back to these instructions.\n4. Click on the **Output** tab on the right of the screen.  Then, click on the file you would like to submit, and click on the **Submit** button to submit your results to the leaderboard.\n\nYou have now successfully submitted to the competition!\n\nIf you want to keep working to improve your performance, select the **Edit** button in the top right of the screen. Then you can change your code and repeat the process. There's a lot of room to improve, and you will climb up the leaderboard as you work.\n\n\n# Continue Your Progress\nThere are many ways to improve your model, and **experimenting is a great way to learn at this point.**\n\nThe best way to improve your model is to add features.  To add more features to the data, revisit the first code cell, and change this line of code to include more column names:\n```python\nfeatures = ['LotArea', 'YearBuilt', '1stFlrSF', '2ndFlrSF', 'FullBath', 'BedroomAbvGr', 'TotRmsAbvGrd']\n```\n\nSome features will cause errors because of issues like missing values or non-numeric data types.  Here is a complete list of potential columns that you might like to use, and that won't throw errors:\n- 'MSSubClass'\n- 'LotArea'\n- 'OverallQual' \n- 'OverallCond' \n- 'YearBuilt'\n- 'YearRemodAdd' \n- '1stFlrSF'\n- '2ndFlrSF' \n- 'LowQualFinSF' \n- 'GrLivArea'\n- 'FullBath'\n- 'HalfBath'\n- 'BedroomAbvGr' \n- 'KitchenAbvGr' \n- 'TotRmsAbvGrd' \n- 'Fireplaces' \n- 'WoodDeckSF' \n- 'OpenPorchSF'\n- 'EnclosedPorch' \n- '3SsnPorch' \n- 'ScreenPorch' \n- 'PoolArea' \n- 'MiscVal' \n- 'MoSold' \n- 'YrSold'\n\nLook at the list of columns and think about what might affect home prices.  To learn more about each of these features, take a look at the data description on the **[competition page](https:\/\/www.kaggle.com\/c\/home-data-for-ml-course\/data)**.\n\nAfter updating the code cell above that defines the features, re-run all of the code cells to evaluate the model and generate a new submission file.  \n\n\n# What's next?\n\nAs mentioned above, some of the features will throw an error if you try to use them to train your model.  The **[Intermediate Machine Learning](https:\/\/www.kaggle.com\/learn\/intermediate-machine-learning)** course will teach you how to handle these types of features. You will also learn to use **xgboost**, a technique giving even better accuracy than Random Forest.\n\nThe **[Pandas](https:\/\/kaggle.com\/Learn\/Pandas)** course will give you the data manipulation skills to quickly go from conceptual idea to implementation in your data science projects. \n\nYou are also ready for the **[Deep Learning](https:\/\/kaggle.com\/Learn\/intro-to-Deep-Learning)** course, where you will build models with better-than-human level performance at computer vision tasks.","dd17478f":"**Disclaimer:** I received inspiration from several different notebooks that I found on the leaderboard. Based on that, I did my own research as well and that's how I was able to overcome them. Kaggle courses also helped me in improving my scores a lot.","b682b559":"# Column separation and selection\nBased on above MI plots, ID, PoolArea, MiscVal, 3SsnPorch, BsmtFinSF2, Landslope, Utilities has been dropped. Dropping Condition2 and Street increased the loss, this means they interaction effect with other features even though their MI is zero.","8809f1eb":"# Grid Search","48452398":"# Mutual Information\nGetting Mutual Information Scores and Plotting them to determine feature that does not contribute to SalePrice","296e96bd":"# Defining Helper Functions\nHelper functions to evaluate CV score of the model, getting and plotting MI Scores","45a0be96":"**This notebook is an exercise in the [Introduction to Machine Learning](https:\/\/www.kaggle.com\/learn\/intro-to-machine-learning) course.  You can reference the tutorial at [this link](https:\/\/www.kaggle.com\/alexisbcook\/machine-learning-competitions).**\n\n---\n","ffae8b66":"# Importing libraries and Reading files"}}