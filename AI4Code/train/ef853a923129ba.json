{"cell_type":{"8593d14f":"code","7bb96d60":"code","a3019280":"code","884db23f":"code","f752c16f":"code","58c41aae":"code","8b91eaef":"code","78bc96a1":"code","25545d99":"code","64819c54":"code","43a4806b":"code","78ee6006":"code","542edc33":"code","e9e4fffe":"code","47fc18f5":"code","2384a0b3":"code","ad11d3d1":"code","f11dcda4":"code","34fb1a1b":"markdown","788540a9":"markdown","c573314f":"markdown","03db0e4f":"markdown","a7b19df3":"markdown","d48dbe2d":"markdown"},"source":{"8593d14f":"import os\nimport re\nimport json\nimport time\nimport datetime\nimport random\nimport glob\nimport importlib\n\nimport numpy as np\nimport pandas as pd\n\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nrandom.seed(123)\nnp.random.seed(456)","7bb96d60":"MAX_LENGTH = 64 # max no. words for each sentence.\nOVERLAP = 20 # if a sentence exceeds MAX_LENGTH, we split it to multiple sentences with overlapping\n\nMAX_SAMPLE = None # set a small number for experimentation, set None for production.\n\n# MAX_SAMPLE=5000","a3019280":"train_path = '..\/input\/coleridgeinitiative-show-us-the-data\/train.csv'\npaper_train_folder = '..\/input\/coleridgeinitiative-show-us-the-data\/train'\ntrain = pd.read_csv(train_path)","884db23f":"train.shape","f752c16f":"train = train[:MAX_SAMPLE]\nprint(f'No. raw training rows: {len(train)}')","58c41aae":"all_labels = set()\n\nfor label_1, label_2, label_3 in train[['dataset_title', 'dataset_label', 'cleaned_label']].itertuples(index=False):\n    all_labels.add(str(label_1).lower())\n    all_labels.add(str(label_2).lower())\n    all_labels.add(str(label_3).lower())\n    \nprint(f'No. different labels: {len(all_labels)}')","8b91eaef":"train.head()","78bc96a1":"train = train.groupby('Id').agg({\n    'pub_title': 'first',\n    'dataset_title': '|'.join,\n    'dataset_label': '|'.join,\n    'cleaned_label': '|'.join\n}).reset_index()\n\nprint(f'No. grouped training rows: {len(train)}')","25545d99":"papers = {}\nfor paper_id in train['Id'].unique():\n    with open(f'{paper_train_folder}\/{paper_id}.json', 'r') as f:\n        paper = json.load(f)\n        papers[paper_id] = paper","64819c54":"def clean_training_text(txt):\n    \"\"\"\n    similar to the default clean_text function but without lowercasing.\n    \"\"\"\n    return re.sub('[^A-Za-z0-9]+', ' ', str(txt)).strip()\n\ndef shorten_sentences(sentences):\n    short_sentences = []\n    for sentence in sentences:\n        words = sentence.split()\n        if len(words) > MAX_LENGTH:\n            for p in range(0, len(words), MAX_LENGTH - OVERLAP):\n                short_sentences.append(' '.join(words[p:p+MAX_LENGTH]))\n        else:\n            short_sentences.append(sentence)\n    return short_sentences\n\ndef find_sublist(big_list, small_list):\n    all_positions = []\n    for i in range(len(big_list) - len(small_list) + 1):\n        if small_list == big_list[i:i+len(small_list)]:\n            all_positions.append(i)\n    \n    return all_positions\n\ndef tag_sentence(sentence, labels): # requirement: both sentence and labels are already cleaned\n    sentence_words = sentence.split()\n    \n    if labels is not None and any(re.findall(f'\\\\b{label}\\\\b', sentence)\n                                  for label in labels): # positive sample\n        nes = ['O'] * len(sentence_words)\n        for label in labels:\n            label_words = label.split()\n#             print(label_words)\n            all_pos = find_sublist(sentence_words, label_words)\n#             print(all_pos)\n            for pos in all_pos:\n                nes[pos] = 'B'\n                for i in range(pos+1, pos+len(label_words)):\n                    nes[i] = 'I'\n        \n#         print(nes)\n        return True, [sentence_words, nes]\n#         return True, list(zip(sentence_words, nes))\n        \n    else: # negative sample\n        nes = ['O'] * len(sentence_words)\n        return False, [sentence_words, nes]\n#         return False, list(zip(sentence_words, nes))","43a4806b":"cnt_pos, cnt_neg = 0, 0 # number of sentences that contain\/not contain labels\nner_data = []\ns=False\npbar = tqdm(total=len(train))\nfor i, id, dataset_label in train[['Id', 'dataset_label']].itertuples():\n    # paper\n    paper = papers[id]\n    \n    # labels\n    labels = dataset_label.split('|')\n    labels = [clean_training_text(label) for label in labels]\n    \n    # sentences\n    sentences = set([clean_training_text(sentence) for section in paper \n                 for sentence in section['text'].split('.') \n                ])\n    sentences = shorten_sentences(sentences) # make sentences short\n    sentences = [sentence for sentence in sentences if len(sentence) > 10] # only accept sentences with length > 10 chars\n    \n#     print(sentences)\n    \n    # positive sample\n    for sentence in sentences:\n        is_positive, tags = tag_sentence(sentence, labels)\n        if is_positive:\n#             print(tags)\n\n            cnt_pos += 1\n            ner_data.append(tags)\n#             s=True\n#             break\n        elif any(word in sentence.lower() for word in ['data', 'study']): \n            ner_data.append(tags)\n            cnt_neg += 1\n#     if s==True:\n#         break\n    # process bar\n    pbar.update(1)\n    pbar.set_description(f\"Training data size: {cnt_pos} positives + {cnt_neg} negatives\")\n\n# shuffling\nrandom.shuffle(ner_data)","78ee6006":"ner_dataset=pd.DataFrame(ner_data,columns=['sentence','tag'])","542edc33":"ner_dataset.head()","e9e4fffe":"ner_dataset.to_csv(\"ner_dataset.csv\",index=False)","47fc18f5":"# with open('train_ner.json', 'w') as f:\n#     for row in ner_data:\n#         words, nes = list(zip(*row))\n#         row_json = {'tokens' : words, 'tags' : nes}\n#         json.dump(row_json, f)\n#         f.write('\\n')","2384a0b3":"# import joblib\n# import torch\n# import torch.nn as nn\n# import transformers\n\n# import numpy as np\n# import pandas as pd\n\n# from sklearn import preprocessing\n# from sklearn import model_selection\n\n# from tqdm import tqdm\n# from transformers import AdamW\n# from transformers import get_linear_schedule_with_warmup","ad11d3d1":"# class config:\n#     MAX_LEN = 128\n#     TRAIN_BATCH_SIZE = 32\n#     VALID_BATCH_SIZE = 8\n#     EPOCHS = 3\n#     BASE_MODEL_PATH = \"..\/input\/bert-base-uncased\/\"\n#     MODEL_PATH = \"model.bin\"\n#     TRAINING_FILE = \"..\/input\/entity-annotated-corpus\/ner_dataset.csv\"\n#     TOKENIZER = transformers.BertTokenizer.from_pretrained(\n#         'bert-base-uncased',\n#         do_lower_case=True\n#     )","f11dcda4":"# class EntityDataset:\n#     def __init__(self, texts, pos, tags):\n#         self.texts = texts\n#         self.pos = pos\n#         self.tags = tags\n    \n#     def __len__(self):\n#         return len(self.texts)\n    \n#     def __getitem__(self, item):\n#         text = self.texts[item]\n#         pos = self.pos[item]\n#         tags = self.tags[item]\n\n#         ids = []\n#         target_pos = []\n#         target_tag =[]\n\n#         for i, s in enumerate(text):\n#             inputs = config.TOKENIZER.encode(\n#                 s,\n#                 add_special_tokens=False\n#             )\n#             # abhishek: ab ##hi ##sh ##ek\n#             input_len = len(inputs)\n#             ids.extend(inputs)\n#             target_pos.extend([pos[i]] * input_len)\n#             target_tag.extend([tags[i]] * input_len)\n\n#         ids = ids[:config.MAX_LEN - 2]\n#         target_pos = target_pos[:config.MAX_LEN - 2]\n#         target_tag = target_tag[:config.MAX_LEN - 2]\n\n#         ids = [101] + ids + [102]\n#         target_pos = [0] + target_pos + [0]\n#         target_tag = [0] + target_tag + [0]\n\n#         mask = [1] * len(ids)\n#         token_type_ids = [0] * len(ids)\n\n#         padding_len = config.MAX_LEN - len(ids)\n\n#         ids = ids + ([0] * padding_len)\n#         mask = mask + ([0] * padding_len)\n#         token_type_ids = token_type_ids + ([0] * padding_len)\n#         target_pos = target_pos + ([0] * padding_len)\n#         target_tag = target_tag + ([0] * padding_len)\n\n#         return {\n#             \"ids\": torch.tensor(ids, dtype=torch.long),\n#             \"mask\": torch.tensor(mask, dtype=torch.long),\n#             \"token_type_ids\": torch.tensor(token_type_ids, dtype=torch.long),\n#             \"target_pos\": torch.tensor(target_pos, dtype=torch.long),\n#             \"target_tag\": torch.tensor(target_tag, dtype=torch.long),\n#         }","34fb1a1b":"# Load data","788540a9":"Group by publication, training labels should have the same form as expected output.","c573314f":"# Transform data to NER format","03db0e4f":"# Hyper-parameters","a7b19df3":"## Install packages","d48dbe2d":"# Import"}}