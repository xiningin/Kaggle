{"cell_type":{"8f4d9229":"code","ca2816e2":"code","409d589b":"code","f5812a22":"code","ef660451":"code","25212833":"code","24b363e6":"code","fbf171c3":"code","c27c7249":"code","6c01ac0f":"code","514a1795":"code","057bd896":"code","7f4b8eda":"code","79db34e5":"code","3c59851b":"code","b580139e":"code","ba578740":"markdown","55e0e692":"markdown","3fcfe6c3":"markdown","f08342fe":"markdown","10f66cca":"markdown","91158dea":"markdown","bd0bac24":"markdown","b2edff9f":"markdown","3b309fb9":"markdown","7bab6500":"markdown","f63b2e1e":"markdown","cf27d206":"markdown","e2383c97":"markdown","62430be9":"markdown","0beb60d7":"markdown","554458f3":"markdown","39c1fc8a":"markdown","111bc340":"markdown"},"source":{"8f4d9229":"import pandas as pd \nimport numpy as np\nimport plotly.graph_objects as go\nimport plotly.express as px\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport scipy.stats as stats\nimport warnings\nwarnings.filterwarnings(\"ignore\")","ca2816e2":"hr_train = pd.read_csv('..\/input\/hr-analytics-job-change-of-data-scientists\/aug_train.csv')\nhr_test = pd.read_csv('..\/input\/hr-analytics-job-change-of-data-scientists\/aug_test.csv')\nhr_train.head()","409d589b":"def prepare_hr_data(train, test):\n        \n    features_remove=['enrollee_id','target']\n    y = train['target']\n    X = train.drop(features_remove, axis=1)\n    \n    test_id = test['enrollee_id']\n    test = test.drop(['enrollee_id'],axis=1)\n    \n    print(len(X.columns), len(test.columns))\n    \n    train_len = len(X)\n\n    merged = pd.concat([X, test])\n    \n    categorical_features = []\n    for c in merged.columns:\n        col_type = merged[c].dtype\n        if col_type == \"object\" or col_type.name == \"category\":\n            # an option in case the data(pandas dataframe) isn't passed with the categorical column type\n            # X[c] = X[c].astype('category')\n            categorical_features.append(c)\n            \n            \n    merged = pd.get_dummies(data=merged, columns=categorical_features)\n    #merged = merged.fillna(merged.mean())\n    \n    merged = merged.rename(columns={'experience_<1':'experience_less_than_1',\n'experience_>20':'experience_more_than_20',\n'company_size_<10':'company_size_less_than_20',\n'last_new_job_>4':'last_new_job_more_than_4'})\n    \n    train_x = merged.iloc[:train_len,:]\n    test = merged.iloc[train_len:, :]\n    \n    return train_x, y, test_id, test","f5812a22":"train_X, train_y, test_id, test = prepare_hr_data(hr_train, hr_test)\ntarget = 'target'\nIDcol = \"enrollee_id\"\ntrain = train_X\ntrain[target] = train_y\ntrain.head()","ef660451":"from sklearn.metrics import f1_score,confusion_matrix\nfrom sklearn.metrics import accuracy_score\nimport xgboost as xgb\nfrom xgboost.sklearn import XGBClassifier\nfrom sklearn import metrics   #Additional scklearn functions\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV   #Perforing grid search\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score, train_test_split\nfrom sklearn.utils import shuffle\nfrom bayes_opt import BayesianOptimization as BO\nimport scipy as sp\nimport matplotlib as mp\nfrom matplotlib.pylab import rcParams\n%matplotlib inline","25212833":"def xgb_bo(X, y, categoricals_features = None):\n    \n   # For any optimization problem we have one objective function and a search space.\n    \n   # Define the evaluation function \/ objective function and parameter space.\n   # Here our objective function is AUC of the XGBoost model.\n    \n    def xgb_eval(learning_rate, max_depth, min_child_weight, max_delta_step, \n                 min_split_loss, subsample, colsample_bytree, reg_lambda, reg_alpha, scale_pos_weight):\n        \n        xgb_params = {'objective': 'binary:logistic',\n                      'booster': 'gbtree',\n                      'eval_metric': 'auc',\n                      'learning_rate': learning_rate,\n                      'max_depth': int(max_depth),\n                      'min_child_weight': min_child_weight,\n                      'max_delta_step': max_delta_step,\n                      'min_split_loss': min_split_loss,\n                      'subsample': subsample,\n                      'colsample_bytree': int(colsample_bytree),\n                      'reg_lambda': reg_lambda,  \n                      'reg_alpha': reg_alpha,\n                      'scale_pos_weight' : int(scale_pos_weight)\n                    }\n        \n        dtrain = xgb.DMatrix(X, y)\n        cv_xgb = xgb.cv(xgb_params, dtrain)\n        \n        \n        \n        return cv_xgb['test-auc-mean'].iloc[-1]\n    \n    ### Define the search space\n    \n    xgb_param_space = {\n        'learning_rate': (0.001, 0.1),\n        'max_depth': (3, 80),\n        'min_child_weight': (0.1, 10),\n        'max_delta_step': (0,10),\n        'min_split_loss': (0,1),\n        'subsample': (0.2, 1),\n        'colsample_bytree': (0.2, 1),\n        'reg_lambda': (0, 50),\n        'reg_alpha': (0, 50),\n        'scale_pos_weight': (0.2, 5)\n    }\n    \n    return xgb_eval, xgb_param_space\n\n\ndef run_bayes_opt(eval_func, param_space):\n    \n    \"\"\"\n    This function is to run Bayesian optimization. \n    'init_points' is the number of initializations.\n    'n_iter' is the number of iterations after your random initializations.\n    \"\"\"\n    \n    bo = BO(eval_func, param_space)\n    \n    n_iter = 20\n    init_points = 5\n    \n    with warnings.catch_warnings():\n        warnings.filterwarnings('ignore')\n        \n        bo.maximize(init_points = init_points,\n                   n_iter = n_iter,\n                   acq = 'ucb',\n                   alpha = 1e-6)\n        \n    return bo","24b363e6":"bayes_op = True\nif bayes_op:\n    # run Bayesian optimization\n    xgb_eval, xgb_param_space = xgb_bo(train_X, train_y)\n    xgb_bo = run_bayes_opt(xgb_eval, xgb_param_space)\n    max_bo_params = xgb_bo.max['params']","fbf171c3":"max_bo_params","c27c7249":"xgb_model = XGBClassifier(\n    n_estimators = 1000,\n    objective = 'binary:logistic',\n    booster = 'gbtree',\n    learning_rate = max_bo_params['learning_rate']\/2,\n    max_depth = int(max_bo_params['max_depth']),\n    min_child_weight = max_bo_params['min_child_weight'],\n    max_delta_step = max_bo_params['max_delta_step'],\n    min_split_loss = max_bo_params['min_split_loss'],\n    subsample = max_bo_params['subsample'],\n    colsample_bytree = max_bo_params['colsample_bytree'],\n    reg_lambda = max_bo_params['reg_lambda'],\n    reg_alpha = max_bo_params['reg_alpha'],\n    scale_pos_weight = max_bo_params['scale_pos_weight'], \n    seed=27)","6c01ac0f":"param_test = {  \n    'n_estimators':range(50,1000,50)\n}\nrnd_search = RandomizedSearchCV(xgb_model, param_test, scoring='roc_auc', n_iter=4, cv=4)\nrnd_search.fit(train_X,train_y)","514a1795":"rnd_search.best_params_, rnd_search.best_score_","057bd896":"xgb_final = XGBClassifier(\n    n_estimators = rnd_search.best_params_['n_estimators'],\n    objective = 'binary:logistic',\n    booster = 'gbtree',\n    learning_rate = max_bo_params['learning_rate']\/2,\n    max_depth = int(max_bo_params['max_depth']),\n    min_child_weight = int(max_bo_params['min_child_weight']),\n    max_delta_step = int(max_bo_params['max_delta_step']),\n    min_split_loss = max_bo_params['min_split_loss'],\n    subsample = max_bo_params['subsample'],\n    colsample_bytree = max_bo_params['colsample_bytree'],\n    reg_lambda = max_bo_params['reg_lambda'],\n    reg_alpha = max_bo_params['reg_alpha'],\n    scale_pos_weight = max_bo_params['scale_pos_weight'], \n    seed=27)","7f4b8eda":"def modelfit(alg, dtrain, val, predictors,useTrainCV=True, cv_folds=5, early_stopping_rounds=50, verbose=True):\n    \n    if useTrainCV:\n        xgb_param = alg.get_xgb_params()\n        xgtrain = xgb.DMatrix(dtrain[predictors].values, label=dtrain[target].values)\n        cvresult = xgb.cv(xgb_param, xgtrain, num_boost_round=alg.get_params()['n_estimators'], nfold=cv_folds,\n            metrics=[\"auc\", \"logloss\"], early_stopping_rounds=early_stopping_rounds)\n        alg.set_params(n_estimators=cvresult.shape[0])\n    \n    #Fit the algorithm on the data\n    val_predictors = [x for x in val.columns if x not in [IDcol,target]]\n    eval_s = [(dtrain[predictors], dtrain[target]),(val[predictors], val[target])]\n    alg.fit(dtrain[predictors], dtrain[target],early_stopping_rounds=early_stopping_rounds,eval_metric=[\"auc\", \"logloss\"],eval_set=eval_s,verbose=verbose)\n        \n    #Predict training set:\n    dtrain_predictions = alg.predict(dtrain[predictors])\n    dtrain_predprob = alg.predict_proba(dtrain[predictors])[:,1]\n    val_predictions = alg.predict(val[val_predictors])\n        \n    #Print model report:\n    print (\"\\nModel Report\")\n    print (\"Train Accuracy : %.4g\" % metrics.accuracy_score(dtrain[target].values, dtrain_predictions))\n    print (\"Validation Accuracy : %.4g\" % metrics.accuracy_score(val[target].values, val_predictions))\n    \n    \n    # retrieve performance metrics\n    results = alg.evals_result()\n    epochs = len(results['validation_0']['logloss'])\n    x_axis = range(0, epochs)\n    # plot log loss\n    fig, ax = plt.subplots()\n    ax.plot(x_axis, results['validation_0']['logloss'], label='Train')\n    ax.plot(x_axis, results['validation_1']['logloss'], label='Test')\n    ax.legend()\n    plt.ylabel('Log Loss')\n    plt.title('XGBoost Log Loss')\n    plt.show()\n    #plt.savefig(\"log.png\")\n    fig, ax = plt.subplots()\n    ax.plot(x_axis, results['validation_0']['auc'], label='Train')\n    ax.plot(x_axis, results['validation_1']['auc'], label='Test')\n    ax.legend()\n    plt.ylabel('Area under curve')\n    plt.title('XGBoost AUC value')\n    plt.show()\n    #plt.savefig('error.png')\n    \n    \n    return","79db34e5":"def evaluate_model_performnce(y_test, y_pred):\n    cm = confusion_matrix(y_test, y_pred)\n    \n    # Visualizing model performance\n    ax= plt.subplot()\n    sns.heatmap(cm, annot=True, ax = ax); #annot=True to annotate cells\n\n    # labels, title and ticks\n    ax.set_xlabel('Predicted labels');ax.set_ylabel('True labels'); \n    ax.set_title('Confusion Matrix'); \n\n    tn, fp, fn, tp = cm.ravel()\n    #print(tn, fp, fn, tp)\n    precision = tp\/(tp+fp)\n    recall = tp\/(tp+fn)\n    f1 = 2 * (precision * recall) \/ (precision + recall)\n    accuracy = ((tp+tn)\/(tp+tn+fp+fn))*100\n    print(\"Precision : \",precision)\n    print(\"Recall : \",recall)\n    print(\"F1 Score : \",f1)\n    print(\"Validation Accuracy : \",accuracy)\n    accuracy = metrics.accuracy_score(y_test, y_pred)\n    print(\"Accuracy Score : \", accuracy)\n\n    fpr, tpr, thresholds = metrics.roc_curve(y_test, y_pred)\n    auc = metrics.auc(fpr, tpr)\n    print(\"AUC Value : \", auc)\n    \n    return accuracy, auc, f1","3c59851b":"#xgb_final = XGBClassifier()","b580139e":"# Prepare train and validation set\ndf = train\ntrain, val = train_test_split(df, test_size=0.2)\n# Training the XGBoost Model\npredictors = [x for x in train.columns if x not in [target, IDcol]]\nmodelfit(xgb_final, train, val, predictors, useTrainCV=False, early_stopping_rounds=30)","ba578740":"## Hyper-parameter optimization techniques\n\nHere are some of the major techniques that are majorly used in ML domain\n- Manual Search\n- Grid Search\n- Random Search\n- Bayesian Optimization\n- Gradient Based Optimization","55e0e692":"### Final Model with optimized parameters","3fcfe6c3":"This process is designed to minimize the number of steps required to find a combination of parameters that are close to the optimal combination. To do so, this method uses a proxy optimization problem (finding the maximum of the acquisition function) that, albeit still a hard problem, is cheaper (in the computational sense) and common tools can be employed. Therefore Bayesian Optimization is most adequate for situations where sampling the function to be optimized is a very expensive endeavor.","f08342fe":"The above xgb_model is having parameters optimized by Bayesian Optimization. Once can increase n_iter and init_points to get better results.\n\nThe parameter n_estimator coudn't be optimized using Bayesian optimization. I don't understand why?\n\nSo, for tuning n_estimators we'll use Randomized search. (Currently commented it because it takes a lot of time to run...jusr uncomment if you want to use)","10f66cca":"Now if the rule by which (x,y) are related to lambda is know then one can use calculus or deterministic optimization algorithms to optimize lambda. If the function\/rule is unknown then stochastic algorithms are use. And this process is called hyper-parameter optimization. In the next section we'll look into some of the details of the techniques used for Hype-parameter optimization.\n\n[Image Credit](https:\/\/www.oreilly.com\/library\/view\/statistics-for-machine\/9781788295758\/assets\/ac3f2f5a-9199-4bb7-8ce6-47e4dc307a0e.png)\n\n### Imports","91158dea":"<img src=\"https:\/\/github.com\/fmfn\/BayesianOptimization\/raw\/master\/examples\/bayesian_optimization.gif\">","bd0bac24":"## HR Analytics Dataset","b2edff9f":"### Random Search\n\nIn this method as well we define a search space (grid) for each hyper-parameter that we want to tune but instead of doing exhaustive search it searches the grid space randomly  i.e. it tries randomly selected combinations of parameters. Pseudo code snippet -\n\n> clf = RandomForestClassifier() <br\/>\n> param_grid = {'n_estimators':range(50,100,50)} <br\/>\n> rnd_search = RandomizedSearchCV(clf, param_grid, scoring='roc_auc', n_iter=4, cv=2) <br\/>\n> rnd_search.fit(train_X,train_y) <br\/>\n\n<img src=\"https:\/\/miro.medium.com\/max\/3192\/1*Q3GY243UjUA7r-pLudRFTQ.png\">\n\n<br\/>\nWe can see here that random search does better because of the way the values are picked. In this example, grid search only tested three unique values for each hyperperameter, whereas the random search tested 9 unique values for each.\n\n[Image Credit](https:\/\/miro.medium.com\/max\/3192\/1*Q3GY243UjUA7r-pLudRFTQ.png)","3b309fb9":"## Bayesian Optimization in Action [on XGBoost Model]\n\nWe'll see bayesian optimization in action because it's the most efficient one.","7bab6500":"Optimized parameters from Bayesian Optimization","f63b2e1e":"### Grid Search\n\nIn this method we define a search space (grid) for each hyper-parameter that we want to tune. The model is traned on each combination of the values that we defined in the grid and then performance is evaluated. The v=hyper-parameter values corresponding to best model performance is picked as final.\nPseodo Code Snippet - \n\n> clf = RandomForestClassifier() <br\/>\n> grid_search = {'max_depth': [5,10], 'learning_rate':[0.01, 0.1]} <br\/>\n> model = GridSearchCV(estimator = clf, param_grid = grid_search, cv = 4, verbose= 5, n_jobs = -1)\n> model.fit(train_X,train_y)                            ","cf27d206":"### Manual Search \n\nAs the name suggest we use our knowledge and experience to pick the value of a hyper-parameter then we train the model and evaluate it. Based on the results we change the value of the hyper-parameter and retain the model. The proces continues until satifactory results are obtained. This is rarely used since it takes a lot of time.","e2383c97":"<img src=\"https:\/\/www.oreilly.com\/library\/view\/statistics-for-machine\/9781788295758\/assets\/ac3f2f5a-9199-4bb7-8ce6-47e4dc307a0e.png\">","62430be9":"Disclaimer - This notebook if more focused on how to do hyper-parameter tuning especially using Bayesian Optimization. The model that we'll be using in this notebook is XGBoost.","0beb60d7":"# <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:180%; text-align:center\">Introduction<\/p>\n\n> Developing a model is never enough. It's important that you tune it to get the best out of it.\n\n### What is hyper-parameter?\n\nParameters which define the model architecture are referred to as hyperparameters and thus this process of searching for the ideal model architecture is referred to as hyperparameter optimization\/tuning.\n\nIn more simple terms :\n\n- **Hyper-parameter** are the parameters that are set by the model developer befor starting the training of the model. (i.e. Learning Rate, regularization parameter)\n- **Model Parameters** are the ones' that are learned while model training. (i.e. Weights of Neural Network)\n\n### Hyper-parameter optimization\n\nFor simplicity assume lambda = f(x,y) and the plot of lambda with respect to x and y looks like below. Now we want to find such x and y that the value of lambda is minimized\/maximized. ","554458f3":"### Happy Learning!!","39c1fc8a":"### Bayesian Optimization \n\nThanks to [fernando](https:\/\/github.com\/fmfn) for this beautiful explanation and animation.\n\nBayesian optimization works by constructing a posterior distribution of functions (gaussian process) that best describes the function you want to optimize. As the number of observations grows, the posterior distribution improves, and the algorithm becomes more certain of which regions in parameter space are worth exploring and which are not. As you iterate over and over, the algorithm balances its needs of exploration and exploitation taking into account what it knows about the target function. At each step a Gaussian Process is fitted to the known samples (points previously explored), and the posterior distribution, combined with a exploration strategy (such as UCB (Upper Confidence Bound), or EI (Expected Improvement)), are used to determine the next point that should be explored (see the gif below).\n\n","111bc340":"Running the experiment"}}