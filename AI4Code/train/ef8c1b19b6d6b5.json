{"cell_type":{"ad8477a4":"code","bba468fe":"code","2b2fc7af":"code","1b259cd9":"code","7a1191b8":"code","9aa4b747":"code","264e036d":"code","9c5e4c18":"code","347fc328":"code","9792ff74":"code","d7678d9c":"code","08f9ba37":"code","67da3c12":"code","2cd45e84":"code","9262260a":"code","cd18bf60":"code","ec8b7f46":"code","cfe24b32":"code","2f71cf5f":"code","bbfc556c":"code","4873247d":"code","69d5a0a9":"code","664bc89b":"code","24d74f64":"code","90983e36":"code","c459b497":"code","26395a3c":"code","23bed773":"code","05950af7":"code","8d549aca":"markdown","ac2e9ce4":"markdown","c392565f":"markdown","f6c5ea8b":"markdown","dc93a6c4":"markdown","de15e15a":"markdown","cc28da6b":"markdown","7c66978c":"markdown","628d704c":"markdown","ad45468e":"markdown","8d3b36b4":"markdown","fab4d7e6":"markdown","fa514982":"markdown","a874209d":"markdown","7b3f294b":"markdown","beab7b91":"markdown","14396f78":"markdown","07a1390e":"markdown","53de0a77":"markdown","509c5a4c":"markdown","7eeff003":"markdown","9531412e":"markdown","6a45e91e":"markdown"},"source":{"ad8477a4":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport tensorflow as tf\n\nprint(\"tensorflow version\", tf.__version__)\nprint(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))","bba468fe":"train = pd.read_csv(\"..\/input\/house-price-prediction-challenge\/train.csv\")\ntest = pd.read_csv(\"..\/input\/house-price-prediction-challenge\/test.csv\")\ntrain.head()","2b2fc7af":"train.isnull().sum()","1b259cd9":"train.describe(include=\"all\").transpose()","7a1191b8":"plt.figure(figsize=(10,6))\nsns.distplot(train[\"TARGET(PRICE_IN_LACS)\"], kde=True)\nplt.show()","9aa4b747":"print(f\"The Dataset size before removing outliers : {train.shape}\")\nmax_val = train['TARGET(PRICE_IN_LACS)'].describe()['75%'] + 1.5 * (train['TARGET(PRICE_IN_LACS)'].describe()['75%'] - train['TARGET(PRICE_IN_LACS)'].describe()['25%'])\ntrain = train[train['TARGET(PRICE_IN_LACS)'] < max_val]\nprint(f\"The Dataset size after removing outliers : {train.shape}\")","264e036d":"plt.figure(figsize=(10,6))\nsns.distplot(train[\"TARGET(PRICE_IN_LACS)\"], kde=True)\nplt.show()","9c5e4c18":"print(train[\"POSTED_BY\"].value_counts(), end='\\n\\n')","347fc328":"dic = {}\nfor v, i in zip(train[\"POSTED_BY\"].unique(), range(len(train[\"POSTED_BY\"].unique()))):\n    dic[v] = i\n\ntrain[\"POSTED_BY\"] = train[\"POSTED_BY\"].map(dic)\ntest[\"POSTED_BY\"] = test[\"POSTED_BY\"].map(dic)","9792ff74":"print(train[\"BHK_OR_RK\"].value_counts(), end='\\n\\n')","d7678d9c":"train.drop(labels=[\"BHK_OR_RK\"], axis=1, inplace=True)\ntest.drop(labels=[\"BHK_OR_RK\"], axis=1, inplace=True)","08f9ba37":"plt.figure(figsize=(10,6))\nsns.heatmap(train.corr(), cmap=\"YlGnBu\", annot=True)\nplt.show()","67da3c12":"plt.figure(figsize=(10,6))\nsns.boxplot(x=\"BHK_NO.\", y=\"TARGET(PRICE_IN_LACS)\", data=train)\nplt.show()","2cd45e84":"print(\"House that has 9 rooms in dataset : \", len(train[train[\"BHK_NO.\"]==9]))\nprint(\"House that has 11 rooms in dataset : \", len(train[train[\"BHK_NO.\"]==11]))\nprint(\"House that has 20 rooms in dataset : \", len(train[train[\"BHK_NO.\"]==20]))","9262260a":"train = train[(train[\"BHK_NO.\"]!=9) & (train[\"BHK_NO.\"]!=11) & (train[\"BHK_NO.\"]!=20)]","cd18bf60":"print(len(train[train[\"UNDER_CONSTRUCTION\"]==0]))\nprint(len(train[train[\"UNDER_CONSTRUCTION\"]==1]))\nprint(len(train[train[\"READY_TO_MOVE\"]==0]))\nprint(len(train[train[\"READY_TO_MOVE\"]==1]))","ec8b7f46":"train.drop(labels=[\"UNDER_CONSTRUCTION\"], axis=1, inplace=True)\ntest.drop(labels=[\"UNDER_CONSTRUCTION\"], axis=1, inplace=True)","cfe24b32":"plt.figure(figsize=(12,8))\nsns.scatterplot(x='LONGITUDE', y='LATITUDE', hue='TARGET(PRICE_IN_LACS)', alpha=0.4, palette='RdYlGn', data=train)\nplt.show()","2f71cf5f":"city = train['ADDRESS'].apply(lambda x : x.split(',')[1])\nprint(city.value_counts(), end='\\n\\n')\nprint(\"Total of unique values :\", city.nunique())","bbfc556c":"train.drop(labels=[\"ADDRESS\"], axis=1, inplace=True)\ntest.drop(labels=[\"ADDRESS\"], axis=1, inplace=True)","4873247d":"train.head()","69d5a0a9":"# train dataset\nohe = pd.get_dummies(train['POSTED_BY'], prefix=\"POSTED_BY\", drop_first=True)\ntrain.drop(labels=[\"POSTED_BY\"], axis=1, inplace=True)\ntrain = pd.concat([train, ohe], axis=1)\n\n# test dataset\nohe = pd.get_dummies(test['POSTED_BY'], prefix=\"POSTED_BY\", drop_first=True)\ntest.drop(labels=[\"POSTED_BY\"], axis=1, inplace=True)\ntest = pd.concat([test, ohe], axis=1)","664bc89b":"from sklearn.model_selection import train_test_split\n\nx = train.drop('TARGET(PRICE_IN_LACS)', axis=1).values\ny = train['TARGET(PRICE_IN_LACS)'].values\n\nx_tr, x_ts, y_tr, y_ts = train_test_split(x, y, test_size=0.1, random_state=42)","24d74f64":"from sklearn.preprocessing import MinMaxScaler\n\nscaler = MinMaxScaler().fit(x_tr)\nx_tr_scaled = scaler.transform(x_tr)\nx_ts_scaled = scaler.transform(x_ts)","90983e36":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout\nfrom tensorflow.keras.optimizers import Adam, SGD\nfrom tensorflow.keras.losses import mae\nfrom tensorflow.keras.callbacks import Callback, ModelCheckpoint\nfrom tensorflow.keras.activations import linear, relu\n\nclass stopLearn(Callback):\n    def on_epoch_end(self, epoch, logs={}):\n        if logs.get('val_loss') < 21:\n            print(\"\\nStop training!!\")\n            self.model.stop_training=True\n\ncb = stopLearn()\n# checkpoint_name = 'models\/Weights-{epoch:03d}--{val_loss:.5f}.hdf5' \n# cp = ModelCheckpoint(checkpoint_name, monitor='val_loss', verbose = 1, save_best_only = True, mode ='auto')\n# callbacks_list = [cp, cb]\n\nmodel = Sequential()\nmodel.add(Dense(units=32, kernel_initializer='normal', input_dim=x_tr_scaled.shape[1], activation=relu))\nmodel.add(Dense(units=64, activation=relu))\nmodel.add(Dense(units=64, activation=relu))\nmodel.add(Dense(units=128, activation=relu))\nmodel.add(Dense(units=256, activation=relu))\nmodel.add(Dense(units=1, activation=linear))\n\nmodel.compile(optimizer=Adam(learning_rate=0.001), loss=mae)\n# model.compile(optimizer=SGD(learning_rate=0.001), loss=mae)\n\nmodel.summary()","c459b497":"model.fit(x=x_tr_scaled, y=y_tr, validation_data=(x_ts_scaled, y_ts), batch_size=128, epochs=500, callbacks=[cb])","26395a3c":"plt.figure(figsize=(10,6))\npd.DataFrame(model.history.history).plot()\nplt.show()","23bed773":"y_pred = model.predict(x_ts_scaled).reshape(x_ts_scaled.shape[0],)\npred_df = pd.DataFrame({'Actual value':y_ts, 'Predicted value':y_pred})\n\nprint(pred_df.head())\n\nfrom sklearn.metrics import mean_absolute_error\n\nMAE_val = mean_absolute_error(y_true=pred_df['Actual value'], y_pred=pred_df['Predicted value'])\nprint(\"\\nfrom the MAE result, on average the model about {:.2f} off from true price point in mean of actual values which is not really good and quite bad prediction because it off around 30% based on the mean of the target value.\".format(MAE_val))","05950af7":"'''\nThis score metric is for explain variance regression score function.\nBest possible score is 1.0, lower values are worse.\n'''\nfrom sklearn.metrics import explained_variance_score\n\nexplained_variance_score(y_ts, y_pred)","8d549aca":"### 9. Get the distribution of prices per geographical (latitude & longitude)","ac2e9ce4":"### 8. Analyze features that have high correlations between them\n\nFeatures with high correlation are more linearly dependent and hence have almost the same effect on the dependent variable. So, when two features have high correlation, we can drop one of the two features.","c392565f":"`UNDER_CONSTRUCTION` feature has a perfect negative correlation with `READY_TO_MOVE` feature but their correlations with the target are very low!! `BHK_NO.` has the highest correlation with the target.","f6c5ea8b":"There are some of outliers in this dataset and it may actually make sense to drop those outliers in analysis if they are just a few points that are very extreme because may not be really useful to actually have the model train on those extreme outliers.","dc93a6c4":"Since there is only 1 observation for each room (9, 11, and 20), it'd be better to remove them rather than use them to train the model.","de15e15a":"# Evaluate The Model","cc28da6b":"### 7. Analyze features that have high correlation with the target","7c66978c":"### 3. The distribution of actual target value","628d704c":"### 1. OneHot encoding `POSTED_BY` feature","ad45468e":"---\n# Create The Model","8d3b36b4":"### 5. `BHK_OR_RK` feature","fab4d7e6":"This feature has a very high skewed values, there will be no impact if we drop this feature from the dataset.","fa514982":"This really explains why both of those features are very high correlated, in short that if the house is under construction (value of 1) means `READY_TO_MOVE` will be 0 and vice versa.","a874209d":"### 6. The correlation between target and features","7b3f294b":"# DATA DESCRIPTIONS\n<html>\n<body>\n<table style=\"width:100%\">\n  <tr align=\"left\">\n    <th>Column<\/th>\n    <th>Description<\/th>\n  <\/tr>\n  <tr>\n    <td>POSTED_BY<\/td>\n    <td>Category marking who has listed the property<\/td>\n  <\/tr>\n  <tr>\n    <td>UNDER_CONSTRUCTION<\/td>\n    <td>Under Construction or Not<\/td>\n  <\/tr>\n  <tr>\n    <td>RERA<\/td>\n    <td>Rera approved or Not<\/td>\n  <\/tr>\n  <tr>\n    <td>BHK_NO<\/td>\n    <td>Number of Rooms<\/td>\n  <\/tr>\n  <tr>\n    <td>BHKORRK<\/td>\n    <td>Type of property<\/td>\n  <\/tr>\n  <tr>\n    <td>SQUARE_FT<\/td>\n    <td>Total area of the house in square feet<\/td>\n  <\/tr>\n  <tr>\n    <td>READYTOMOVE<\/td>\n    <td>Category marking Ready to move or Not<\/td>\n  <\/tr>\n  <tr>\n    <td>RESALE<\/td>\n    <td>Category marking Resale or not<\/td>\n  <\/tr>\n  <tr>\n    <td>ADDRESS<\/td>\n    <td>Address of the property<\/td>\n  <\/tr>\n  <tr>\n    <td>LONGITUDE<\/td>\n    <td>Longitude of the property<\/td>\n  <\/tr>\n  <tr>\n    <td>LATITUDE<\/td>\n    <td>Latitude of the property<\/td>\n  <\/tr>\n<\/table>\n\n<\/body>\n<\/html>","beab7b91":"There are 428 unique values in this feature and that won't be possible to do OneHot encoding with it. For now, this feature will be dropped. (FYI, this feature actually can be useful if we know how to group them based on province or maybe higher level. Since I'm not from India this task would be difficult because I don't know administrative area in India.)","14396f78":"Since this feature values in string, all the values will be mapped into number.","07a1390e":"### 4. `POSTED_BY` feature","53de0a77":"---\n# Splitting The Dataset into Train and Test","509c5a4c":"---\n# Feature Engineering","7eeff003":"### 10. `ADDRESS` feature\n\nThere are city information in this feature that maybe can give some information related to the price.","9531412e":"### 2. Statistical information","6a45e91e":"---\n# Exploratory Data Analysis\n\n### 1. Check Null values"}}