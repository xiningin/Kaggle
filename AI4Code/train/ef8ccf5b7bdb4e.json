{"cell_type":{"77194aa7":"code","b5014aa8":"code","7e6f1cf7":"code","070ecfe9":"code","65e82409":"code","5b91a4a4":"code","1d3e2e27":"code","3adf7aeb":"code","c2898e98":"code","635959c9":"code","d3ed12f8":"code","d92b958a":"code","12c328ec":"code","ff7131c0":"code","d84491a4":"code","940f2c5b":"code","52123a31":"code","2b4e778d":"code","b5dd433a":"code","a4e29e19":"code","6bd9724b":"code","eeae634c":"code","8df47e55":"code","51429356":"code","24817173":"code","e020822e":"code","37ac1b17":"code","16028270":"code","3ad6b219":"code","ba4782e4":"code","231e3e48":"code","a52bf03c":"code","c09cadc5":"code","6faef54a":"code","74a96168":"code","f2ddd9c9":"code","08f381db":"code","cf8a8931":"code","e0969341":"code","e607d643":"code","46f0f4ad":"code","9d9ab964":"code","c07b7f0a":"code","5bca1b91":"code","fa19ed86":"code","3f4d1e5d":"code","5955142c":"code","a8012e0e":"code","b7838997":"code","4e420644":"code","916fcc5f":"markdown","5220384f":"markdown","bded3b59":"markdown","a6630992":"markdown","6a8cddf9":"markdown","adcbf5aa":"markdown","57ba1234":"markdown","00e157c5":"markdown","5a0ec939":"markdown","ed75453a":"markdown","efa0520a":"markdown","0ca9d342":"markdown","0d2fabe0":"markdown","8cf8e390":"markdown","513f35ef":"markdown","623f2656":"markdown","38aaf5c9":"markdown","ab6b399b":"markdown","e9a24a3a":"markdown","6d1faf9c":"markdown","fe5142f1":"markdown","bce04f4d":"markdown","46cb1147":"markdown"},"source":{"77194aa7":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","b5014aa8":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn import preprocessing\nfrom statsmodels.graphics.mosaicplot import mosaic\n\n\nfrom mpl_toolkits.basemap import Basemap\nfrom matplotlib import cm\n\nfrom sklearn.cluster import KMeans\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"..\/input\"]).decode(\"utf8\"))\n\n\nimport math\nimport re\nimport datetime","7e6f1cf7":"import pandas as pd\ndf = pd.read_csv(\"..\/input\/aviation-accident-database-synopses\/AviationData.csv\",encoding = \"ISO-8859-1\")\ndf.head()","070ecfe9":"# splitting date field in the components\n\ndf['Year'] = df['Event.Date'].apply(lambda d: datetime.datetime.strptime(d, \"%Y-%m-%d\").year)\ndf['Month'] = df['Event.Date'].apply(lambda d: datetime.datetime.strptime(d, \"%Y-%m-%d\").month)\ndf['Day'] = df['Event.Date'].apply(lambda d: datetime.datetime.strptime(d, \"%Y-%m-%d\").day)\n\ndf_timeseries = df[df['Year'] >= 1982]\n\n# For the time series charts I start sorting data\ndf_timeseries = df_timeseries.sort_values(by=['Year', 'Month', 'Day'], ascending=True)\n\nyears = np.arange(1982, 2017)\n\nsns.set(style=\"darkgrid\")\n\nplt.subplot(211)\n\ng = sns.countplot(x=\"Year\", data=df_timeseries, palette=\"GnBu_d\", order=years)\ng.set_xticklabels(labels=years)\na = plt.setp(g.get_xticklabels(), rotation=90)","65e82409":"fig, axes = plt.subplots(nrows=2, ncols=2,figsize=(15, 10))\nfig.subplots_adjust(hspace=.6)\ncolors = ['#99cc33', '#a333cc', '#333dcc']\ndf['Broad.Phase.of.Flight'].value_counts().plot(ax=axes[0,0], kind='bar', title='Phase of Flight')\ndf['Broad.Phase.of.Flight'].value_counts().plot(ax=axes[0,1], kind='pie', title='Phase of Flight')\ndf['Weather.Condition'].value_counts().plot(ax=axes[1,0], kind='pie', colors=colors, title='Weather Condition')\n# TODO: clean up to add \"other\"\n# ds['cleaned.make'].value_counts().plot(ax=axes[1,1], kind='pie', title='Aircraft Make')","5b91a4a4":"#cleaning the predcitors \ndf['Make'] = df[\"Make\"].str.lower()\ndf['Engine.Type'].fillna('None',inplace = True)\ndf['Weather.Condition'].fillna('unknown',inplace = True)","1d3e2e27":"#cleaning outcome y \ndf.loc[(df['Injury.Severity'] != \"Non-Fatal\") & (df['Injury.Severity'] != \"Incident\"), 'Injury.Severity'] = 'Fatal'\ndf.loc[(df['Injury.Severity'] == \"Incident\"), 'Injury.Severity'] = 'Fatal'\ndf['Injury.Severity'].value_counts()","3adf7aeb":"import graphviz\nfrom sklearn.tree import export_graphviz\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.tree import DecisionTreeClassifier\n\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.tree import export_graphviz","c2898e98":"#splitting the data\npredictors = ['Weather.Condition','Engine.Type','Make']\none_hot_data = pd.get_dummies(df[predictors],drop_first=True)\n# Train Set : 1100 samples\ndf_train = pd.DataFrame(df[:67409])\nhot_predictor_train = pd.DataFrame(one_hot_data[:67409])\ny_train = pd.DataFrame(df_train['Injury.Severity'])\nX_train = hot_predictor_train \n\n# Test Set : 360 samples\ndf_test = pd.DataFrame(df[-16853:])\nhot_predictor_test = pd.DataFrame(one_hot_data[-16853:])\ny_test = pd.DataFrame(df_test['Injury.Severity'])\nX_test = hot_predictor_test","635959c9":"# Decision Tree using Train Data\ndectree = DecisionTreeClassifier(max_depth = 3)  # create the decision tree object\ndectree.fit(X_train, y_train)  ","d3ed12f8":"def dectree_pred(X_train,X_test) :\n    # Predict Response corresponding to Predictors\n    y_train_pred = dectree.predict(X_train)\n    y_test_pred = dectree.predict(X_test)\n\n    # Check the Goodness of Fit (on Train Data)\n    print(\"Goodness of Fit of Model \\tTrain Dataset\")\n    print(\"Classification Accuracy \\t:\", dectree.score(X_train, y_train))\n    print()\n\n    listall_train = confusion_matrix(y_train, y_train_pred)\n    listtop_train = listall_train[0]\n    listbot_train = listall_train[1]\n    fpr_train= listtop_train[1]\/(sum(listtop_train))\n    fnr_train = listbot_train[0]\/(sum(listbot_train))\n    print('The False Positive Rate is \\t:{0:2f}'.format(fpr_train))\n    print('The False Negative Rate is \\t:{0:2f}'.format(fnr_train))\n    print()\n\n    # Check the Goodness of Fit (on Test Data)\n    print(\"Goodness of Fit of Model \\tTest Dataset\")\n    print(\"Classification Accuracy \\t:\", dectree.score(X_test, y_test))\n    print()\n\n    listall_test = confusion_matrix(y_test, y_test_pred)\n    listtop_test = listall_test[0]\n    listbot_test = listall_test[1]\n    fpr_test= listtop_test[1]\/(sum(listtop_test))\n    fnr_test = listbot_test[0]\/(sum(listbot_test))\n    print('The False Positive Rate is \\t:{0:2f}'.format(fpr_test))\n    print('The False Negative Rate is \\t:{0:2f}'.format(fnr_test))\n\n\n    # Plot the Confusion Matrix for Train and Test\n    f, axes = plt.subplots(1, 2, figsize=(12, 4))\n    sns.heatmap(confusion_matrix(y_train, y_train_pred),\n               annot = True, fmt=\".0f\", annot_kws={\"size\": 18}, ax = axes[0])\n    sns.heatmap(confusion_matrix(y_test, y_test_pred), \n               annot = True, fmt=\".0f\", annot_kws={\"size\": 18}, ax = axes[1])\n\n#1 is substantial , 0 is Fatal\n","d92b958a":"dectree_pred(X_train,X_test)","12c328ec":"# Export the Decision Tree as a dot object\ntreedot = export_graphviz(dectree,                                      # the model\n                          feature_names = X_test.columns,          # the features \n                          out_file = None,                              # output file\n                          filled = True,                                # node colors\n                          rounded = True,                               # make pretty\n                          special_characters = True)                    # postscript\n\n# Render using graphviz\nimport graphviz\ngraphviz.Source(treedot)","ff7131c0":"df['Aircraft.Damage'].fillna('unknown',inplace = True)\ndf.loc[(df['Aircraft.Damage'] != \"Destroyed\") , 'Aircraft.Damage'] = 'Substantial'\ndf['Aircraft.Damage'].value_counts()","d84491a4":"y_train = pd.DataFrame(df_train['Aircraft.Damage'])\ny_test = pd.DataFrame(df_test['Aircraft.Damage'])","940f2c5b":"# Decision Tree using Train Data\ndectree = DecisionTreeClassifier(max_depth = 3)  # create the decision tree object\ndectree.fit(X_train, y_train)                    # train the decision tree model","52123a31":"dectree_pred(X_train,X_test)","2b4e778d":"# Export the Decision Tree as a dot object\ntreedot = export_graphviz(dectree,                                      # the model\n                          feature_names = X_test.columns,          # the features \n                          out_file = None,                              # output file\n                          filled = True,                                # node colors\n                          rounded = True,                               # make pretty\n                          special_characters = True)                    # postscript\n\n# Render using graphviz\nimport graphviz\ngraphviz.Source(treedot)","b5dd433a":"from scipy import stats\ndef cramers_corrected_stat(confusion_matrix, correction: bool) -> float:\n    \"\"\"Calculate the Cramer's V corrected stat for two variables.\n\n    Args:\n        confusion_matrix: Crosstab between two variables.\n        correction: Should the correction be applied?\n\n    Returns:\n        The Cramer's V corrected stat for the two variables.\n    \"\"\"\n    chi2 = stats.chi2_contingency(confusion_matrix, correction=correction)[0]\n    n = confusion_matrix.sum().sum()\n    phi2 = chi2 \/ n\n    r, k = confusion_matrix.shape\n\n    # Deal with NaNs later on\n    with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n        phi2corr = max(0.0, phi2 - ((k - 1.0) * (r - 1.0)) \/ (n - 1.0))\n        rcorr = r - ((r - 1.0) ** 2.0) \/ (n - 1.0)\n        kcorr = k - ((k - 1.0) ** 2.0) \/ (n - 1.0)\n        corr = np.sqrt(phi2corr \/ min((kcorr - 1.0), (rcorr - 1.0)))\n    return corr","a4e29e19":"df['Location'] = df[\"Location\"].str.upper() #making all CAPS\ndf['Location'].fillna('unknown',inplace = True) #removing nan\n#removing locations with frequency less than 100 for faster computing \ncol = 'Location'  # 'bar'\nn = 100  # 2\ndf_filtered  = df[df.groupby(col)[col].transform('count').ge(n)]","6bd9724b":"#droping columns that do not hold any true value to prediction or clustering\nlist_to_drop = ['Event.Id','Investigation.Type','Accident.Number','Event.Date','Country','Report.Status','Publication.Date']\ndf_filtered = df_filtered.drop(list_to_drop, axis=1)","eeae634c":"#creating a confusion matrix \ndf_cramer = pd.DataFrame()\ncount = 0 \nfor n in df_filtered:\n    for i in df_filtered:\n        confusion_matrix = pd.crosstab(df_filtered[n], df_filtered[i]).values\n        value = cramers_corrected_stat(confusion_matrix,True)\n        df_cramer.loc[n,i] = value\n        count += 1 ","8df47e55":"f, axes = plt.subplots(figsize=(10, 10))\nax = sns.heatmap(df_cramer, vmin=0, vmax=1, square = True)","51429356":"f, axes = plt.subplots(figsize=(20, 25))\nax  = mosaic(df_filtered, ['Location', 'Injury.Severity'], title='DataFrame as Source', gap = 0.001 ,ax= axes ,horizontal = False)\nplt.show()","24817173":"f, axes = plt.subplots(figsize=(20, 25))\nax  = mosaic(df_filtered, ['Location', 'Aircraft.Damage'], title='DataFrame as Source', gap = 0.001 ,ax= axes , axes_label = True , horizontal = False)\nplt.show()","e020822e":"#Selecting variables for clustering \ndf_cluster = df[['Location','Injury.Severity','Aircraft.Damage','Make','Amateur.Built','Engine.Type']]","37ac1b17":"#removing NAN\ndf_cluster = df_cluster.replace(np.nan, 'Unknown', regex=True)","16028270":"from kmodes.kmodes import KModes","3ad6b219":"km_cao = KModes(n_clusters=2, init = \"Cao\", n_init = 1, verbose=1)\nfitClusters_cao = km_cao.fit_predict(df_cluster)","ba4782e4":"fitClusters_cao","231e3e48":"clusterCentroidsDf = pd.DataFrame(km_cao.cluster_centroids_)\nclusterCentroidsDf.columns = df_cluster.columns\nclusterCentroidsDf","a52bf03c":"km_huang = KModes(n_clusters=2, init = \"Huang\", n_init = 1, verbose=1)\nfitClusters_huang = km_huang.fit_predict(df_cluster)\nfitClusters_huang","c09cadc5":"cost = []\nfor num_clusters in list(range(1,5)):\n    kmode = KModes(n_clusters=num_clusters, init = \"Cao\", n_init = 1, verbose=1)\n    kmode.fit_predict(df_cluster)\n    cost.append(kmode.cost_)","6faef54a":"y = np.array([i for i in range(1,5,1)])\nplt.plot(y,cost)","74a96168":"km_cao = KModes(n_clusters=2, init = \"Cao\", n_init = 1, verbose=1)\nfitClusters_cao = km_cao.fit_predict(df_cluster)\nfitClusters_cao","f2ddd9c9":"clustersDf = pd.DataFrame(fitClusters_cao)\nclustersDf.columns = ['cluster_predicted']\ncombinedDf = pd.concat([df_cluster, clustersDf], axis = 1).reset_index()\ncombinedDf = combinedDf.drop(['index'], axis = 1)","08f381db":"cluster_0 = combinedDf[combinedDf['cluster_predicted'] == 0]\ncluster_1 = combinedDf[combinedDf['cluster_predicted'] == 1]","cf8a8931":"f, axes = plt.subplots(6,2 ,figsize=(20, 10))\n\nsns.countplot(x ='Injury.Severity' , data =cluster_0 , ax = axes[0,0] )\nsns.countplot(x ='Aircraft.Damage' , data =cluster_0 , ax = axes[1,0] )\nsns.countplot(x ='Make' , data =cluster_0 , ax = axes[2,0] ,order = pd.value_counts(cluster_0['Make']).iloc[:5].index)\nsns.countplot(x ='Amateur.Built' , data =cluster_0 , ax = axes[3,0] )\nsns.countplot(x ='Engine.Type' , data =cluster_0 , ax = axes[4,0],order = pd.value_counts(cluster_0['Engine.Type']).iloc[:5].index)\nsns.countplot(x ='Location' , data =cluster_0 , ax = axes[5,0],order = pd.value_counts(cluster_0['Location']).iloc[:5].index)\n\n\nsns.countplot(x ='Injury.Severity' , data =cluster_1 , ax = axes[0,1] )\nsns.countplot(x ='Aircraft.Damage' , data =cluster_1 , ax = axes[1,1] )\nsns.countplot(x ='Make' , data =cluster_1 , ax = axes[2,1] ,order = pd.value_counts(cluster_1['Make']).iloc[:5].index)\nsns.countplot(x ='Amateur.Built' , data =cluster_1 , ax = axes[3,1] )\nsns.countplot(x ='Engine.Type' , data =cluster_1 , ax = axes[4,1],order = pd.value_counts(cluster_1['Engine.Type']).iloc[:5].index)\nsns.countplot(x ='Location' , data =cluster_1 , ax = axes[5,1],order = pd.value_counts(cluster_1['Location']).iloc[:5].index)\n","e0969341":"df['cluster_predicted'] = combinedDf['cluster_predicted']\n\ndf_location_data = df[['Location','Latitude','Longitude','cluster_predicted']]\ndf_location_data = df_location_data.dropna()\n\ncluster0_locs =  df_location_data[df_location_data['cluster_predicted'] == 0]\ncluster1_locs =  df_location_data[df_location_data['cluster_predicted'] == 1]\n","e607d643":"from mpl_toolkits.basemap import Basemap\nfrom matplotlib import cm","46f0f4ad":"centroid_region0 = cluster0_locs.loc[cluster0_locs['Location'] == clusterCentroidsDf.at[0, 'Location']]\ncentroid_region1 = cluster1_locs.loc[cluster1_locs['Location'] == clusterCentroidsDf.at[1, 'Location']]","9d9ab964":"fig = plt.figure()\nplt.figure(figsize=(15,15))\n\nm = Basemap(\n    llcrnrlon=-165,\n    llcrnrlat=20,\n    urcrnrlon=-40,\n    urcrnrlat=70,\n    projection='cyl',\n    resolution='c',\n    area_thresh=None,\n    rsphere=6370997.0,\n    no_rot=False,\n    suppress_ticks=True,\n    satellite_height=35786000,\n    boundinglat=None,\n    fix_aspect=True,\n    anchor='C',\n    celestial=False,\n    round=False,\n    epsg=None,\n    ax=None,\n)\nx, y = m(cluster0_locs['Longitude'].values, cluster0_locs['Latitude'].values)\nm.drawcoastlines()\nm.drawcountries()\nm.hexbin(x, y, gridsize=1000, bins='log', cmap=cm.YlOrRd)\nm.scatter(centroid_region0['Longitude'], centroid_region0['Latitude'], 50, color='g')","c07b7f0a":"fig = plt.figure()\nplt.figure(figsize=(15,15))\n\nm = Basemap(\n    llcrnrlon=-165,\n    llcrnrlat=20,\n    urcrnrlon=-40,\n    urcrnrlat=70,\n    projection='cyl',\n    resolution='c',\n    area_thresh=None,\n    rsphere=6370997.0,\n    no_rot=False,\n    suppress_ticks=True,\n    satellite_height=35786000,\n    boundinglat=None,\n    fix_aspect=True,\n    anchor='C',\n    celestial=False,\n    round=False,\n    epsg=None,\n    ax=None,\n)\nx, y = m(cluster1_locs['Longitude'].values, cluster1_locs['Latitude'].values)\nm.drawcoastlines()\nm.drawcountries()\nm.hexbin(x, y, gridsize=1000, bins='log', cmap=cm.YlOrRd)\nm.scatter(centroid_region1['Longitude'], centroid_region1['Latitude'], 50, color='g')","5bca1b91":"from sklearn.cluster import KMeans","fa19ed86":"latlon = df_location_data[['Longitude', 'Latitude']]\nlatlon.head()","3f4d1e5d":"Sum_of_squared_distances = []\nK = range(1,15)\nfor k in K:\n    km = KMeans(n_clusters=k)\n    km = km.fit(latlon)\n    Sum_of_squared_distances.append(km.inertia_)","5955142c":"plt.plot(K, Sum_of_squared_distances, 'bx-')\nplt.xlabel('k')\nplt.ylabel('Sum_of_squared_distances')\nplt.title('Elbow Method For Optimal k')\nplt.show()","a8012e0e":"kmeans = KMeans(n_clusters=3)\nkmodel = kmeans.fit(latlon)\ncentroids = kmodel.cluster_centers_","b7838997":"centroids\nlons, lats = zip(*centroids)","4e420644":"fig = plt.figure()\nplt.figure(figsize=(15,15))\nnorth, south, east, west = 71.39, 24.52, -66.95, 172.5\nm = Basemap(\n    llcrnrlon=-135,\n    llcrnrlat=-20,\n    urcrnrlon=86,\n    urcrnrlat=60,\n    projection='cyl',\n    resolution='c',\n    area_thresh=None,\n    rsphere=6370997.0,\n    no_rot=False,\n    suppress_ticks=True,\n    satellite_height=35786000,\n    boundinglat=None,\n    fix_aspect=True,\n    anchor='C',\n    celestial=False,\n    round=False,\n    epsg=None,\n    ax=None,\n)\nx, y = m(df_location_data['Longitude'].values, df_location_data['Latitude'].values)\nm.drawcoastlines()\nm.drawcountries()\nm.hexbin(x, y, gridsize=1000, bins='log', cmap=cm.YlOrRd)\ncx, cy = m(lons, lats)\nm.scatter(cx, cy, 50, color='g')","916fcc5f":"Removing Useless variables that do not describe air crash","5220384f":"Time Series Analysis","bded3b59":"Cluster 1 Visualisation","a6630992":"Exploratory Analysis on clusters","6a8cddf9":"Cluster 0 Visualisation","adcbf5aa":"Initialising Kmodes","57ba1234":"Decision Tree for Secondary Outcome of Aircraft Damage ","00e157c5":"Using K-Mode with \"Cao\" initialization","5a0ec939":"Initial Exporatory Analysis","ed75453a":"Creating Confusion Matrix","efa0520a":"Import Decision Tree Libraries","0ca9d342":"Correlation of Variables","0d2fabe0":"Plotting Mosaic Plots to show general relation of locations and injury","8cf8e390":"Importing the CVS file","513f35ef":"Selecting variables for clustering ","623f2656":"Splitting outcome into cluster dataframe","38aaf5c9":"Clustering Based on Location only","ab6b399b":"Cleaning the Data Set","e9a24a3a":"Libraries for Location Visualisation","6d1faf9c":"Finding Optimum Number of Clusters","fe5142f1":"Using K-Mode with \"Huang\" initialization","bce04f4d":"One Hot Encoding the Data","46cb1147":"Cleaning the outcome "}}