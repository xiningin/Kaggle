{"cell_type":{"d20074f8":"code","ec48f297":"code","5304f19a":"code","004cd866":"code","38e5efbe":"code","901ee3bf":"code","57a45af1":"code","ded18202":"code","44801f67":"code","0a720c5d":"code","6656d691":"code","16cec093":"code","d2adeff8":"code","d498d083":"code","56b6d72e":"code","d72f6af0":"code","c1815c20":"code","dd1e5e5e":"code","979599fe":"code","ae4873e6":"code","1f729d55":"code","7622fb56":"code","0280da93":"code","e30275c5":"code","bd1b80cc":"code","ed8e6cc1":"markdown","affbe615":"markdown","67028787":"markdown","7224b6cb":"markdown"},"source":{"d20074f8":"import pandas \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\n\n# Read in the data.\ngames = pandas.read_csv(\"..\/input\/games.csv\")\n# Print the names of the columns in games.\nprint(games.columns)\nprint(games.shape)\n\n# Make a histogram of all the ratings in the average_rating column.\nplt.hist(games[\"average_rating\"])\n\n# Show the plot.\nplt.show()","ec48f297":"import numpy as np","5304f19a":"\ngames.info()","004cd866":"# finding the nulll values \ngames.isnull().sum()","38e5efbe":"# finding the colums that had average _rating=0 it means these games were never played \ndata=games.loc[games['average_rating']==0]\ndata[[\"average_rating\",\"yearpublished\"]]","901ee3bf":"#first we need to remove the nan values b replacing it with -1\ngames=games.replace(np.nan,-1)\ngames.isnull().sum()","57a45af1":"# removing the null values data from datset\ngames=games.loc[games['yearpublished']>=0]\ngames=games.loc[games['minplayers']>=0]\ngames=games.loc[games['maxplayers']>=0]\ngames=games.loc[games['playingtime']>=0]\ngames=games.loc[games['minplaytime']>=0]\ngames=games.loc[games['maxplaytime']>=0]\ngames=games.loc[games['minage']>=0]","ded18202":"# now we need to remove the enteries of year published which is 0 becuase it means game is never published\nprint(games['yearpublished'].unique())\ngames=games.loc[games['yearpublished']>0]\nprint(games['yearpublished'].unique())\n# as you can see the enteries with yearpublished=0 has been removed","44801f67":"# mapping the type of games\nprint(games['type'].unique())\ngames['type']=games['type'].map({\"boardgame\":1,\"boardgameexpansion\":2})\nprint(games['type'].unique())\nprint(games.shape)","0a720c5d":"# playing with various averages and seeing them for any discrepancy\n# lets find out if all the  averages provided=0,maxplayng time=0 and user rated=0 that means game is never played\nd=games.loc[(games['bayes_average_rating']==0.0) & (games['average_rating']==0) & (games['users_rated']==0) & (games['playingtime']==0)]\nprint(d.shape)\nd[['bayes_average_rating','average_rating','users_rated','playingtime','maxplaytime']]\n# so u can clearly see that our dataset has such 7000+ enteries that must be removed to acheive generalisation ","6656d691":"games.describe()\nprint(games.shape)","16cec093":"# removing thee inconsistency of data\ngames=games.loc[((games['average_rating']>0) & (games['playingtime']>0))]\ngames.shape","d2adeff8":"games.describe()","d498d083":"plt.scatter(games['users_rated'],games['average_rating'],color='g')\nplt.plot(games['average_rating'],games['bayes_average_rating'],color='r')\nplt.show()","56b6d72e":"# Make a histogram of all the ratings in the average_rating column to ensure that most of rating with 0 have been removed and \n#only genuine cases have been left\nplt.hist(games[\"average_rating\"])\n\n# Show the plot.\nplt.show()","d72f6af0":"#correlation matrix\ncorrmat = games.corr()\nfig = plt.figure(figsize = (12, 9))\nsns.heatmap(corrmat, square=True);\nplt.show()","c1815c20":"# Get all the columns from the dataframe.\ncolumns = games.columns.tolist()\n# Filter the columns to remove ones we don't want.\ncolumns = [c for c in columns if c not in [\"average_rating\", \"name\", \"id\"]]\n\n# Store the variable we'll be predicting on.\ntarget = \"average_rating\"","dd1e5e5e":"print(columns)","979599fe":"# Import a convenience function to split the sets.\nfrom sklearn.model_selection import train_test_split\n\n# Generate the training set.  Set random_state to be able to replicate results.\ntrain = games.sample(frac=0.8, random_state=1)\n# Select anything not in the training set and put it in the testing set.\ntest = games.loc[~games.index.isin(train.index)]\n# Print the shapes of both sets.\nprint(train.shape)\nprint(test.shape)","ae4873e6":"from sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score","1f729d55":"# Import the linear regression model.\nfrom sklearn.linear_model import LinearRegression\n\n# Initialize the model class.\nmodel = LinearRegression()\n# Fit the model to the training data.\nmodel.fit(train[columns], train[target])\n\n# Import the scikit-learn function to compute error.\nfrom sklearn.metrics import mean_squared_error\n\n# Generate our predictions for the test set.\npredictions = model.predict(test[columns])\n\n# Compute error between our test predictions and the actual values.\nprint(\"Mean square Error : \",mean_squared_error(predictions, test[target]))\n","7622fb56":"# SInce is predicted data is continuous so we cant find the accuracy and classification _report \n# Therefore calculating the mean squared error is best measure","0280da93":"# Import the random forest model.\nfrom sklearn.ensemble import RandomForestRegressor\n\n# Initialize the model with some parameters.\nmodel = RandomForestRegressor(n_estimators=100, min_samples_leaf=10, random_state=1)\n# Fit the model to the data.\nmodel.fit(train[columns], train[target])\n# Make predictions.\npredictions = model.predict(test[columns])\n# Compute the error.\nmean_squared_error(predictions, test[target])","e30275c5":"x=pandas.DataFrame({'Prediction':predictions})\nx","bd1b80cc":"# If u find it helpful please do upvote this kernel","ed8e6cc1":"### 1. Importing Libraries and Loading the Data\n\nAfter the .csv file 'games.csv' has been copied to the current directory, we can import the data as a Pandas DataFrame. As a DataFrame, we will be able to easily explore the type, amount, and distribution of data.  Furthermore, using a correlation matrix, we can explore the relationships between parameters.  This is an important step in determining the type of machine learning algorithm to utilize. ","affbe615":"# Board Game Review Prediction\n\n","67028787":"Data Link\n\ngit clone https:\/\/github.com\/ThaWeatherman\/scrapers.git","7224b6cb":"### 2. Linear Regression\n\nIn the following cells, we will deploy a simple linear regression model to predict the average review of each board game.  We will use the mean squared error as a performance metric.  Furthermore, we will compare and contrast these results with the performance of an ensemble method. "}}