{"cell_type":{"638ba5ee":"code","82710e69":"code","75d89e1d":"code","e7649a66":"code","34622763":"code","a586ca1f":"code","00f697a0":"code","3292c26e":"code","eddfa42a":"code","537a1b89":"code","4e89a4d3":"code","b726af6f":"code","293cd4d1":"code","d1b94f55":"code","95bd1731":"code","83c6d29a":"code","2fae3c16":"code","a8a3c022":"code","6ebd44fb":"code","6613f861":"code","9e62ae5f":"code","a415d454":"code","b2b804a8":"code","c19be6bc":"code","3f21d191":"code","cdad78b3":"code","cbf10376":"code","53d3c01a":"code","19794b78":"code","09afaa06":"code","74b030ac":"code","a2411427":"code","fc9267ea":"code","bfe72c91":"code","b5b0d917":"code","a9725b0d":"code","172d4663":"code","988f65c4":"code","728cf498":"code","1ad56808":"code","6cd23187":"code","1bb8ed57":"code","840069ff":"code","86b61c4e":"code","cae5bd06":"code","3dc611d5":"markdown","19a8e743":"markdown","078b7f3b":"markdown","9b69fe8e":"markdown","63858f3c":"markdown","8143f9ea":"markdown","c0e53299":"markdown","2bb7d1cb":"markdown","2293874b":"markdown","729be13e":"markdown","1ef09ec3":"markdown","bad8866c":"markdown","4f0e0d8f":"markdown","c3a95165":"markdown","e5229623":"markdown","c9356dc2":"markdown","e4c38cef":"markdown","fa4749d9":"markdown","9febe368":"markdown","28b2f315":"markdown","42426c35":"markdown","ce8012f0":"markdown","cd217ab9":"markdown","a435177e":"markdown","3f25d3bd":"markdown","4447f319":"markdown"},"source":{"638ba5ee":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport plotly.graph_objects as go #Plotly for Viz\nimport plotly.express as px # Plotly express\nfrom plotly.subplots import make_subplots\nimport matplotlib.pyplot as plt #Matplotlib for Viz\nimport seaborn as sns #Seaborn for Viz\nfrom scipy import stats #Outlier Analysis & Removal\nfrom sklearn.decomposition import PCA #Dimensionality Reduction\nfrom sklearn.preprocessing import StandardScaler #Scaling variables\nfrom sklearn.model_selection import train_test_split #Splitting data for model training\nfrom sklearn.ensemble import RandomForestClassifier #RF\nfrom sklearn.metrics import log_loss #Evaluation metric for the comp\n\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session\n\ntrain_features=pd.read_csv('..\/input\/lish-moa\/train_features.csv')\ntest_features=pd.read_csv('..\/input\/lish-moa\/test_features.csv')\ntrain_targets_scored=pd.read_csv('..\/input\/lish-moa\/train_targets_scored.csv')","82710e69":"#Combining training and testing datasets\ntest_features['flag']='test'\ntrain_features['flag']='train'\n\nall_data = pd.concat([train_features, test_features], ignore_index=True, sort=False)\nall_data","75d89e1d":"train_features.describe()","e7649a66":"train_targets_scored","34622763":"train_features_scored=pd.merge(train_features,train_targets_scored,how='inner')\n#train_features_scored","a586ca1f":"import random\n#Plotting Histograms for Randomly Selected Gene Expression Variables\nfig = make_subplots(\n    rows=5, cols=4,shared_yaxes=True)\nj=1\nk=1\n\nfor i in range(1,21):\n    rand=random.randint(0, 770)\n    col=\"g-\"+str(rand)\n    fig.add_trace(\n    go.Histogram(x=train_features[col],name=col),\n    row=k, col=j\n    )\n   # print(k,j)\n    j=j+1\n    if(j>4):\n        j=1\n    if(i%4==0):\n        k=k+1\n\n        \nfig.update_layout(title_text=\"Distribution for Randomly Selected Gene Expression Variables\")\nfig.show()","00f697a0":"fig = make_subplots(\n    rows=20, cols=5,shared_yaxes=True)\nj=1\nk=1\n\nfor i in range(1,99):\n    col=\"c-\"+str(i)\n    fig.add_trace(\n    go.Histogram(x=train_features[col],name=col),\n    row=k, col=j\n    )\n   # print(k,j)\n    j=j+1\n    if(j>5):\n        j=1\n    if(i%5==0):\n        k=k+1\n\n        \nfig.update_layout(height=2500,width=800,title_text=\"Distribution for Cell Viability Rate variables\")\nfig.show()","3292c26e":"#Correlation matrix for Variables\n# cell=train_features_scored.loc[:, train_features_scored.columns.str.startswith('g-')]\n# corr = cell.corr(method='pearson')\n# # corr\n# f, ax = plt.subplots(figsize=(25, 25))\n\n# # Generate a custom diverging colormap\n# cmap = sns.diverging_palette(220, 10, as_cmap=True)\n# mask = np.triu(np.ones_like(corr, dtype=np.bool))\n\n# # Draw the heatmap with the mask and correct aspect ratio\n# sns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n#             square=True, linewidths=.8, cbar_kws={\"shrink\": .5})\n\n# ax = sns.heatmap(corr,linewidths=0.8,cmap=cmap)\n","eddfa42a":"#Correlation matrix for Variables\ncell=train_features_scored.loc[:, train_features_scored.columns.str.startswith('c-')]\ncorr = cell.corr(method='pearson')\n# corr\nf, ax = plt.subplots(figsize=(25, 25))\n\n# Generate a custom diverging colormap\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\nmask = np.triu(np.ones_like(corr, dtype=np.bool))\n\n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n            square=True, linewidths=.8, cbar_kws={\"shrink\": .5})\n\nax = sns.heatmap(corr,linewidths=0.8,cmap=cmap)\n","537a1b89":"train_features_scored['sum_actions']=train_features_scored.iloc[:,-206:].sum(axis=1)\ntrain_features_scored\n\nfig = make_subplots(rows=3, cols=1,subplot_titles=('Sum of Drug Actions with different Dosages','Sum of Drug Actions with different Treatment Durations','Sum of Drug Actions with different Dosage Types'))\n#fig = go.Figure()\n\nfig.add_trace(go.Histogram(x=train_features_scored.loc[train_features_scored['cp_dose']=='D1','sum_actions'],name='Drug Dosage - D1'),row=1,col=1)\nfig.add_trace(go.Histogram(x=train_features_scored.loc[train_features_scored['cp_dose']=='D2','sum_actions'],name='Drug Dosage - D2'),row=1,col=1)\n#fig.update_layout(title_text='Sum of Drug Actions with different Dosages',xaxis_title_text='Sum of Drug Actions',yaxis_title_text='Count of Samples')\n#fig.show()\n\n#fig1 = go.Figure()\nfig.add_trace(go.Histogram(x=train_features_scored.loc[train_features_scored['cp_time']==24,'sum_actions'],name='Treatment Duration - 24h'),row=2,col=1)\nfig.add_trace(go.Histogram(x=train_features_scored.loc[train_features_scored['cp_time']==48,'sum_actions'],name='Treatment Duration - 48h'),row=2,col=1)\nfig.add_trace(go.Histogram(x=train_features_scored.loc[train_features_scored['cp_time']==72,'sum_actions'],name='Treatment Duration - 72h'),row=2,col=1)\n#fig.update_layout(title_text='Sum of Drug Actions with different Dosage Times',xaxis_title_text='Sum of Drug Actions',yaxis_title_text='Count of Samples')\n#fig.show()\n\n#fig2 = go.Figure()\nfig.add_trace(go.Histogram(x=train_features_scored.loc[train_features_scored['cp_type']=='trt_cp','sum_actions'],name='Drug Type - trt_cp'),row=3,col=1)\nfig.add_trace(go.Histogram(x=train_features_scored.loc[train_features_scored['cp_type']=='ctl_vehicle','sum_actions'],name='Drug Type - ctl_vehicle'),row=3,col=1)\n#fig2.update_layout(title_text='Distribution ofActions with different Dosage Types',xaxis_title_text='Sum of Drug Actions',yaxis_title_text='Count of Samples')\n#fig2.show()","4e89a4d3":"depsum=train_features_scored.iloc[:,-206:-1].sum(axis=0)\ndepsum=depsum.to_frame()\ndepsum.columns=['sum_actions']\ndepsum['action']=depsum.index\ndepsum=depsum.reset_index(drop=True)\ndepsum=depsum.sort_values(by='sum_actions',ascending=False)\ndepsum_top=depsum.head(10)\ndepsum_tail=depsum.tail(10)\n\nimport plotly.express as px\n# df = px.data.tips()\nfig2 = px.histogram(depsum, x=\"sum_actions\",opacity=0.6, title='Histogram of Sum of Actions across MoAs')\nfig2.show()\n\n\n","b726af6f":"fig=px.bar(depsum_top,y='sum_actions',x='action',title='Most Common MoAs')\n#fig.update_layout(height=350,width=800)\nfig.show()\n\nfig1=px.bar(depsum_tail,y='sum_actions',x='action',title='Least Common MoAs')\n#fig1.update_layout(height=350,width=800)\n\nfig1.show()","293cd4d1":"actions=train_features_scored.iloc[:,-100:]\ncorr = actions.corr(method='pearson')\n# corr\nf, ax = plt.subplots(figsize=(25, 25))\n\n# Generate a custom diverging colormap\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\nmask = np.triu(np.ones_like(corr, dtype=np.bool))\n\n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n            square=True, linewidths=.8, cbar_kws={\"shrink\": .5})\n\nax = sns.heatmap(corr,linewidths=0.8,cmap=cmap)\n","d1b94f55":"#Capping outliers\ndef cap_outliers(col):\n    col[col>3]=3\n    col[col<-3]=-3\n    return col","95bd1731":"#Filtering all the numeric columns\nnumcols=all_data._get_numeric_data().columns\nall_data_num=all_data.loc[:,numcols]\nall_data_num=all_data_num.iloc[:,1:]\n\n#z=np.abs(stats.zscore(all_data_num['g-0']))\n#Calculate Z Scores for all the variables. \nall_data_num=all_data_num.apply(stats.zscore)\n\n#Cap the outliers\nall_data_num=all_data_num.apply(cap_outliers)\n#all_data_num.describe()\n#z\n","83c6d29a":"# Function to implement PCA\ndef pca_application(df,n_components,pattern):\n    df_p=df.loc[:, df.columns.str.startswith(pattern)]\n    x = StandardScaler().fit_transform(df_p)\n    pca = PCA(n_components=n_components)\n    principalComponents = pca.fit_transform(x)\n    principalDf = pd.DataFrame(data = principalComponents)\n    return principalDf,pca","2fae3c16":"#Calculate principal components separately for GE & CV columns\nprincipalDf_g,pca_g=pca_application(all_data,200,'g-')\nprincipalDf_c,pca_c=pca_application(all_data,30,'c-')\n\n#principalDf_g","a8a3c022":"z=np.arange(start=1, stop=len(pca_g.explained_variance_ratio_)+1, step=1)\nz\n\nplt.bar(x=z,height=pca_g.explained_variance_ratio_)\nplt.xlabel('Principal Components')\nplt.ylabel('Explained variance')\nplt.title('Explained Variance for Gene Expression Variable PCAs')\nplt.show()\n\nplt.plot(np.cumsum(pca_g.explained_variance_ratio_))\nplt.xlabel('number of components')\nplt.ylabel('cumulative explained variance')\nplt.title('Cumulative Explained Variance for Gene Expression Variable PCAs')\n","6ebd44fb":"z=np.arange(start=1, stop=len(pca_c.explained_variance_ratio_)+1, step=1)\nz\n\nplt.bar(x=z,height=pca_c.explained_variance_ratio_)\nplt.xlabel('Principal Components')\nplt.ylabel('Explained variance')\nplt.title('Explained Variance for Cell Viability Variable PCAs')\nplt.show()\n\nplt.plot(np.cumsum(pca_c.explained_variance_ratio_))\nplt.xlabel('number of components')\nplt.ylabel('cumulative explained variance')\nplt.title('Cumulative Explained Variance for Cell Viability Variable PCAs')\n","6613f861":"#Extracting the principal components\npca_g=principalDf_g.iloc[:,:100]\nl=[]\nfor i in range(1,101):\n    var='pca_g'+str(i)\n    l.append(var)\n#l\npca_g.columns=l\npca_g\n","9e62ae5f":"#Extracting the principal components\npca_c=principalDf_c.iloc[:,:10]\n#pca_c.columns=['pca_c1','pca_c2','pca_c3','pca_c4','pca_c5']\n#pca_c\nl=[]\nfor i in range(1,11):\n    var='pca_c'+str(i)\n    l.append(var)\n#l\npca_c.columns=l\npca_c\n\n#Merging the principal components dataframes\npca_cg=pd.merge(pca_c, pca_g, left_index=True, right_index=True)\npca_cg","a415d454":"cp_cols=all_data.iloc[:,1:4]\npca_cg_cp=pd.merge(cp_cols, pca_cg, left_index=True, right_index=True)\npca_cg_cp['flag']=all_data['flag']\n","b2b804a8":"#pca_cg_cp=pca_cg_cp.loc[pca_cg_cp['cp_type']!='ctl_vehicle',:]\n#train_features_scored=train_features_scored.loc[train_features_scored['cp_type']!='ctl_vehicle',:]\n\n#Calculating dummies for categorical variables\npca_cg_cp=pd.get_dummies(pca_cg_cp, columns=['cp_type', 'cp_dose'])\npca_cg_cp","c19be6bc":"pca_cg_cp_train=pca_cg_cp.loc[pca_cg_cp['flag']=='train',:]\npca_cg_cp_train\ndel pca_cg_cp_train['flag']","3f21d191":"from sklearn.datasets import make_classification\nfrom sklearn.neighbors import NearestNeighbors\n\nX=pca_cg_cp_train #Selecting feature variables\nY=train_features_scored.iloc[:,-207:-1] #Selecting the output columns\nfeature_list=pca_cg_cp_train.columns\n\n#Split data into train and test datasets\nX_train,X_test,Y_train,Y_test=train_test_split(X, Y,test_size=0.3,random_state=1)\n\n\n#MLSMOTE - https:\/\/www.kaggle.com\/tolgadincer\/upsampling-multilabel-data-with-mlsmote\ndef get_tail_label(df: pd.DataFrame, ql=[0.05, 1.]) -> list:\n    \"\"\"\n    Find the underrepresented targets.\n    Underrepresented targets are those which are observed less than the median occurance.\n    Targets beyond a quantile limit are filtered.\n    \"\"\"\n    irlbl = df.sum(axis=0)\n    irlbl = irlbl[(irlbl > irlbl.quantile(ql[0])) & ((irlbl < irlbl.quantile(ql[1])))]  # Filtering\n    irlbl = irlbl.max() \/ irlbl\n    threshold_irlbl = irlbl.median()\n    tail_label = irlbl[irlbl > threshold_irlbl].index.tolist()\n    return tail_label\n\ndef get_minority_samples(X: pd.DataFrame, y: pd.DataFrame, ql=[0.05, 1.]):\n    \"\"\"\n    return\n    X_sub: pandas.DataFrame, the feature vector minority dataframe\n    y_sub: pandas.DataFrame, the target vector minority dataframe\n    \"\"\"\n    tail_labels = get_tail_label(y, ql=ql)\n    index = y[y[tail_labels].apply(lambda x: (x == 1).any(), axis=1)].index.tolist()\n    \n    X_sub = X[X.index.isin(index)].reset_index(drop = True)\n    y_sub = y[y.index.isin(index)].reset_index(drop = True)\n    return X_sub, y_sub\n\ndef nearest_neighbour(X: pd.DataFrame, neigh) -> list:\n    \"\"\"\n    Give index of 10 nearest neighbor of all the instance\n    \n    args\n    X: np.array, array whose nearest neighbor has to find\n    \n    return\n    indices: list of list, index of 5 NN of each element in X\n    \"\"\"\n    nbs = NearestNeighbors(n_neighbors=neigh, metric='euclidean', algorithm='kd_tree').fit(X)\n    euclidean, indices = nbs.kneighbors(X)\n    return indices\n\n\ndef MLSMOTE(X, y, n_sample, neigh=5):\n    \"\"\"\n    Give the augmented data using MLSMOTE algorithm\n    \n    args\n    X: pandas.DataFrame, input vector DataFrame\n    y: pandas.DataFrame, feature vector dataframe\n    n_sample: int, number of newly generated sample\n    \n    return\n    new_X: pandas.DataFrame, augmented feature vector data\n    target: pandas.DataFrame, augmented target vector data\n    \"\"\"\n    indices2 = nearest_neighbour(X, neigh=5)\n    n = len(indices2)\n    new_X = np.zeros((n_sample, X.shape[1]))\n    target = np.zeros((n_sample, y.shape[1]))\n    for i in range(n_sample):\n        reference = random.randint(0, n-1)\n        neighbor = random.choice(indices2[reference, 1:])\n        all_point = indices2[reference]\n        nn_df = y[y.index.isin(all_point)]\n        ser = nn_df.sum(axis = 0, skipna = True)\n        target[i] = np.array([1 if val > 0 else 0 for val in ser])\n        ratio = random.random()\n        gap = X.loc[reference,:] - X.loc[neighbor,:]\n        new_X[i] = np.array(X.loc[reference,:] + ratio * gap)\n    new_X = pd.DataFrame(new_X, columns=X.columns)\n    target = pd.DataFrame(target, columns=y.columns)\n    return new_X, target\n\n","cdad78b3":"X_sub, y_sub = get_minority_samples(X_train, Y_train)  # Getting minority samples of that datframe\nX_res, y_res = MLSMOTE(X_sub, y_sub, 1000, 5)  # Applying MLSMOTE to augment the dataframe\n\n#print(X_res.shape,y_res.shape)\n","cbf10376":"X_train=X_train.append(X_res,ignore_index=True)\nY_train=Y_train.append(y_res,ignore_index=True)\n\nprint(X_train.shape,Y_train.shape)\n","53d3c01a":"#Y_train.sum(axis=0).sort_values(ascending=False).tail(20)\n","19794b78":"#Y_train1.sum(axis=0).sort_values(ascending=False).tail(20)\n","09afaa06":"from skmultilearn.model_selection import iterative_train_test_split\n\n# X=pca_cg_cp_train #Selecting feature variables\n# Y=train_features_scored.iloc[:,-207:-1] #Selecting the output columns\n# feature_list=pca_cg_cp_train.columns\n\n# #Split data into train and test datasets\n# #X_train,X_test,Y_train,Y_test=train_test_split(X, Y,test_size=0.3,random_state=1)\n# #X_train,Y_train,X_test,Y_test=train_test_split(X, Y,test_size=0.3)\n\nmodel = RandomForestClassifier(n_estimators=200,max_depth=10, random_state=0,min_samples_split=10)\nmodel.fit(X_train,Y_train)#Fitting the model \n\n\n\n#Generating predictions from Random Fores Models\npred_rf=model.predict(X_test)\npred_rf_proba=model.predict_proba(X_test)\n\nfeat_importances = pd.Series(model.feature_importances_, index=feature_list)\nfeat_importances=feat_importances.sort_values()\nfeat_importances.plot(kind='barh',figsize=(16,16))#Plotting feature importance\n\nprint('Model Accuracy')\nprint(model.score(X_test,Y_test))\n","74b030ac":"from skmultilearn.model_selection import IterativeStratification\nfrom sklearn.multioutput import MultiOutputClassifier\nfrom sklearn.linear_model import LogisticRegression\n\nlog_model= MultiOutputClassifier(LogisticRegression(max_iter=10000, tol=0.1, C = 0.5,verbose=0,random_state = 42))\n\n# def fit_model(X,Y,model):\n\n#     k_fold = IterativeStratification(n_splits=3, order=1)\n#     for train, test in k_fold.split(X, Y):\n#         print(X.iloc[train].shape, Y.iloc[test].shape)\n#         model.fit(X.iloc[train], Y.iloc[train])\n#         preds = model.predict_proba(X.iloc[test])\n#         preds=pred_transform(preds)\n#         score = log_loss(np.array(Y.iloc[test]),preds)\n#         print('LogLoss Score:', score)\n\n# fit_model(X,Y,log_model)","a2411427":"#Helper Functions\n\ndef Extract(lst): \n    return [item[:,1] for item in lst] \n\ndef calc_loss_df(pred_proba):\n    out=Extract(pred_proba)\n    arr=np.array(out)\n    arr1=np.transpose(arr)\n    l=[]\n    col=[]\n    testcols=Y_test.columns\n    y=np.array(Y_test)\n    \n    for i in range(0,206):\n        a=arr1[:,i].astype('float')\n        b=y[:,i].astype('int')\n        if np.sum(b)>0:\n            l.append(log_loss(b,a))\n            col.append(testcols[i])\n\n    err=pd.DataFrame(\n        {'cols': col,\n         'log_loss': l\n        })\n    err=err.sort_values(by='log_loss',ascending=False)\n    return err  \n\ndef pred_transform(preds):\n    out=Extract(preds)\n    arr=np.array(out)\n    arr1=np.transpose(arr)\n    return arr1","fc9267ea":"from sklearn.metrics import log_loss\npred_rf_proba_t=pred_transform(pred_rf_proba)\n\nprint(log_loss(np.array(Y_test),np.array(pred_rf_proba_t)))#Compute Log loss\n","bfe72c91":"# Calculating Log Loss by Individual Output Column\n#y=np.array(Y_test)\n\nerr=calc_loss_df(pred_rf_proba)\n\nerr_head=err.head(10)\nerr_tail=err.tail(10)\n\nfig2 = px.histogram(err, x=\"log_loss\",opacity=0.6, title='RF - Distribution of Log Loss values on test dataset')\nfig2.show()\n\nfig=px.bar(err_head,x='cols',y='log_loss',title='RF - Output Variables with Highest Log Loss')\nfig.show()\n\nfig1=px.bar(err_tail,x='cols',y='log_loss',title='RF - Output Variables with Lowest Log Loss')\nfig1.show()","b5b0d917":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.multioutput import MultiOutputClassifier\n\n\nlog_model= MultiOutputClassifier(LogisticRegression(max_iter=10000, tol=0.1, C = 0.5,verbose=0,random_state = 42))\nlog_model.fit(X_train,Y_train)#Fitting the model \n#log_model.fit(X_res,y_res)\n#Generating predictions\npred_log_proba=log_model.predict_proba(X_test)\npred_log_proba_t=pred_transform(pred_log_proba)\n","a9725b0d":"#Compute Log loss for Logistic Regression\nprint(log_loss(np.array(Y_test),pred_log_proba_t))\n","172d4663":"import xgboost\nfrom xgboost import XGBClassifier\n# xgb = MultiOutputClassifier(xgboost.XGBClassifier(n_estimators=100, learning_rate=0.08, gamma=0, subsample=1,\n#                            colsample_bytree=0.75, max_depth=12))\n# xgb.fit(X_train,Y_train)\n                            ","988f65c4":"# Using parameters from https:\/\/www.kaggle.com\/fchmiel\/xgboost-baseline-multilabel-classification\nxgb = MultiOutputClassifier(XGBClassifier(tree_method='gpu_hist'))\n\nparams = {'estimator__colsample_bytree': 0.6522,\n          'estimator__gamma': 3.6975,\n          'estimator__learning_rate': 0.0503,\n          'estimator__max_delta_step': 2.0706,\n          'estimator__max_depth': 10,\n          'estimator__min_child_weight': 31.5800,\n          'estimator__n_estimators': 166,\n          'estimator__subsample': 0.8639\n         }\n\nxgb.set_params(**params)\nxgb.fit(X_train,Y_train)\n","728cf498":"pred_xg_proba = xgb.predict_proba(X_test)\ndf_ll_xg=calc_loss_df(pred_xg_proba)\npred_xg_proba_t=pred_transform(pred_xg_proba)\n\nprint(log_loss(np.array(Y_test),pred_xg_proba_t))\n","1ad56808":"df_ll_log=calc_loss_df(pred_log_proba)\n#print('Log Loss for Logistic Regression',df_ll_log)\n\nfig = go.Figure()\nfig.add_trace(go.Histogram(x=err['log_loss'],name='Random Forest'))\nfig.add_trace(go.Histogram(x=df_ll_log['log_loss'],name='Logistic Regression'))\nfig.add_trace(go.Histogram(x=df_ll_xg['log_loss'],name='XGBoost'))\n\nfig.update_layout(barmode='overlay',title='Comparison of Distribution of Log Loss values for models')\n# Reduce opacity to see both histograms\nfig.update_traces(opacity=0.6)\n\nfig.show()","6cd23187":"loss_actions_r=pd.merge(left=depsum,right=err,how='inner',left_on='action',right_on='cols')\nloss_actions_r.columns=['sum_actions','action','cols','log_loss_rf']\n\nloss_actions_r_x=pd.merge(left=loss_actions_r,right=df_ll_xg,how='inner')\nloss_actions_r_x.columns=['sum_actions','action','cols','log_loss_rf','log_loss_xg']\n#loss_actions_r_x\n\nloss_actions_r_x_l=pd.merge(left=loss_actions_r_x,right=df_ll_log,how='inner')\nloss_actions_r_x_l.columns=['sum_actions','action','cols','log_loss_rf','log_loss_xg','log_loss_logistic']\nloss_actions_r_x_l\n\nfig=go.Figure()\nfig.add_trace(go.Scatter(x=loss_actions_r_x_l['sum_actions'],\n                        y=loss_actions_r_x_l['log_loss_rf'],mode='markers',name='Random Forest'))\nfig.add_trace(go.Scatter(x=loss_actions_r_x_l['sum_actions'],\n                        y=loss_actions_r_x_l['log_loss_xg'],mode='markers',name='XGBoost'))\nfig.add_trace(go.Scatter(x=loss_actions_r_x_l['sum_actions'],\n                        y=loss_actions_r_x_l['log_loss_logistic'],mode='markers',name='Logistic Regression'))\n\nfig.update_layout(title='Model Performance - Sum of MoAs for Dependent Variable in Train dataset vs Log Loss',xaxis_title='Sum of MoAs Output Variable',yaxis_title='Log Loss value for Output Variable')\n\nfig.show()","1bb8ed57":"pca_cg_cp_test=pca_cg_cp.loc[pca_cg_cp['flag']=='test',:]\ndel pca_cg_cp_test['flag']\n#pca_cg_cp_test\n\n#preds=model.predict(pca_cg_cp_test)\n#for i in range(1:207)\n","840069ff":"#Final Predictions \npreds_rf=model.predict_proba(pca_cg_cp_test)\npreds_xg=xgb.predict_proba(pca_cg_cp_test)\npreds_log=log_model.predict_proba(pca_cg_cp_test)\n\ndef pred_transform(preds):\n    out=Extract(preds)\n    arr=np.array(out)\n    arr1=np.transpose(arr)\n    return arr1\n\n#Calculate & Transform predictions for individual models \npreds_rf=pred_transform(preds_rf)\npreds_xg=pred_transform(preds_xg)\npreds_log=pred_transform(preds_log)\n\n","86b61c4e":"comb_preds=np.mean([preds_rf,preds_log,preds_xg],axis=0)","cae5bd06":"df = pd.DataFrame(data=comb_preds)\nsample_submission=pd.read_csv('..\/input\/lish-moa\/sample_submission.csv')\n\ndf.insert(loc=0, column='sig_id', value=test_features['sig_id'])\ndf.columns=sample_submission.columns\n\ndf['cp_type']=test_features['cp_type']\n#Manually setting the predictions as zero for ctl_vehicle records\ndf.iloc[df['cp_type']=='ctl_vehicle',1:207]=0\ndel df['cp_type']\n\n#df.to_csv('chk.csv',index=False)\n#Writing the submission csv file\ndf.to_csv('submission.csv',index=False)\n","3dc611d5":"In our data we have about 775 gene expression variables and 100 cell viability variables and 23000+ records in the training dataset. Building an ensemble learning model on this dataset would take a large amount of time, and also we noticed that a lot of cell viability variables are correlated to each other. Therefore we would be looking at dimensionality reduction to overcome these issues. Here we would be implementing Principal Component Analysis(PCA) to achieve dimensionality reduction.\nAccording to the link [here](https:\/\/builtin.com\/data-science\/step-step-explanation-principal-component-analysis) - \n>   Principal Component Analysis, or PCA, is a dimensionality-reduction method that is often used to reduce the dimensionality of large data sets, by transforming a large set of variables into a smaller one that still contains most of the information in the large set.\n\n#### When to use PCA?\n* When we want to reduce the number of independent variables but still don't want to lose the information available from those variables. \n* When we want to ensure our variables are independent of each other\n* When we are comfortable with making our independent variables less interpretable\n\n#### Steps we will follow to implement PCA - \n* Remove outliers and standardize the variables\n* Covariance Matrix  computation and calculation of Eigen Values. The PCA function in sklearn package takes care of these details and we don't have to worry about them\n* Plot the explained variance by Principal Components and select the number of principal components to include. \n\nPlease watch [this](https:\/\/www.youtube.com\/watch?v=FgakZw6K1QQ) great video on Youtube if you would like to learn in detail about how PCA actually works. ","19a8e743":"The above correlation plot tells us that there is no correlation across MoAs, meaning that these MoAs are independent to each other and triggering of one MoA is unlikely to lead to triggering of other MoA because of zero correlation. ","078b7f3b":"Following things are observable from the plot above - \n* The first principal components for c- and g- variables have the highest feature importance\n* As observed in our EDA - dosage, treatment type and duration of treatment have the lowest feature importance in the dataset. c- and g- PCs solely contribute to the performance of the model. ","9b69fe8e":"Some observations from the plots above - \n* Both the Gene Expression & Cell Viability variables generally tend to follow a normal distrubtion centered around zero\n* Gene Expression variables tend to have long left rail and right tail, indicating presence of outliers in the gene expression variables\n* Cell Viability variables don't have a right skew but generally tend to have a long left tail, which points to negative cell viability rates. Since Cell Viability is the percentage of live cells in an environment, that number cannot be less than zero(you can't have negative live cells in a living organism). One reason for this could be that some sort of transformation has already been applied to the dataset and the mean centered around zero, therefore these values are actually transformed values and do not represent actual cell viability rates. \n\nLet's now look at correlation between Gene Expression & Cell Viability Rate Variables\n","63858f3c":"#### Error Analysis\nWe would analyze the log loss by individual clases to understand the areas our model is performing good and bad. ","8143f9ea":"One thing that stands out is that neither of the three variables we looked at - Drug Dosage, Drug Type & Treatment Duration have any difference in distribution of sum of MoAs for distinct values of the variable. Therefore its unlikely that these features will play an important role in model development\n\nOne thing that was mentioned above and has been verified through the output is that records with ```ctrl_vehicle``` as ```cp_type``` do not have any MoAs. \n\nAnother thing that can be observed is that majority of samples have 0 or 1 MoA associated with them. There are very few samples which have 2 or more MoAs associated with them ","c0e53299":"# Impact of Drug Dosage, Treatment Duration & Drug Type on MoA <a name=\"dosage\"><\/a>","2bb7d1cb":"# Model Building - Ensemble Models <a name=\"model\"><\/a>\n\nIn the steps below we would be fitting a few basic Random Forest, XGBoost & Logistic Regression models on the training dataset and looking at feature importance matrix to understand which variables contribute most to the prediction power of the model. I selected RF because it is one of the models which can be directly used as a Multilabel Classifier from the sklearn package. Here is the [link](https:\/\/scikit-learn.org\/stable\/modules\/multiclass.html) which explains more about Multilabel & Multiclass Classifiers in detail. XGBoost & Logistic Regression models don't support Multilabel Classification out of the box and we would have to wrap them in MultiOutput Classifiers for this problem. We would essentially be training 206 different XGBoost & Logisitc models while using the MultiOutput Classifier wrappers. \n\n\n#### Evaluation Metric - Log Loss\nThe eveluation metric for this competition is Log Loss. The formula for computation of Log Loss metric is below - \n![](https:\/\/media.geeksforgeeks.org\/wp-content\/uploads\/20190620132533\/LogLoss.jpg)\n\n\n* N  : no. of samples.\n* M  : no. of attributes.\n* xij : indicates whether ith sample belongs to jth class or not.\n* pij : indicates probability of ith sample belonging to jth class.\n\nAs the predicted probability of the true class gets closer to zero, the loss increases exponentially:\n\n![](data:image\/png;base64,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)\n\n\nPlease go through this excellent [link](https:\/\/towardsdatascience.com\/understanding-binary-cross-entropy-log-loss-a-visual-explanation-a3ac6025181a) if you want to understand more in detail about the math behind computation of log loss and how it works. ","2293874b":"* The histogram for Log loss values for individual columns has a long right tail. There are about 15 columns having Log loss higher than 0.05\n* The peak for histogram is around the log loss range of 0.005-0.009\n* **Cyclooxygenase Inhibitor** has the highest log loss value of >0.10, which shows that our model is not performing well in predicting this column. ","729be13e":"Heavy presence of red in the plot above suggests that cell viability variables are highly correlated to each other. This information will be helpful for us in dimensionality reduction because we will be able to combine some of these variables since they are highly correlated to each other. We will look at this detail again in PCA section of the notebook. ","1ef09ec3":"#### Selecting ideal number of Principal Components based on Explained Variance Plots","bad8866c":"Here are some of the observations from the plots above - \n* The histogram for sum of actions across different MoAs is left skewed. Majority of MoAs have less than 100 samples where they were triggered. This is quite low given that we have almost 23k samples in our training dataset. \n* nfkb_inhibitor & proteasome_inhibitor are the most common MoAs with counts of around 800+ and 700+ respectively\n* Other popular MoAs for Cyclooxygenase_inhibitor, dopamine_receptor_antagonist, dna_inhibitor etc. have counts of 400+ records each\n* Least common MoAs like steroid, elastase_inhibitor, laxative have less than 10 records each\n* Each MoA has been triggered atleast once in the data\n* Given the low triggers for some of the MoAs, it may be an interesting approach to set their predictions manually to zero and ignore them from the prediction. \n\nNext we would look at correlation between MoAs to understand if a drug sample is likely to trigger multiple MoAs that are correlated to each other. ","4f0e0d8f":"## Next Steps - \n\n* Gain domain expertise to perform intelligent Feature Engineering\n* Improve performance of Dimensionality Reduction technique\n* Test more prediction techniques, use MultiOutput Classifiers, NN etc.","c3a95165":"### Logistic Regression","e5229623":"#### Remove outliers and standardizing the dataset","c9356dc2":"## Overview of Datasets \n\n### train_features\n* sig_id - Refers to a unique sample in the dataset. Primary key for the dataset which links it to train_targets_scored dataset\n* Gene Expression - Contains variables with ```g-``` prefix which store gene expression related information. There are about 775 gene expression variables. These variables generally have a mean around zero and range of -10 to 10\n* Cell Viability - Contains variables with ```c-``` prefix which store cell viability information. There are about 100 cell viability variables. These variables also tend have a mean around zero and range of -10 to 5. The data contains negative values for Cell Viability as well, which does not make sense. We will investigate into this later\n* Drug Dosage - Has distinct values for D1 & D2, signifying low and high dosage\n* CP Time - Has three distinct values of 24, 28 and 72 hours\n* CP Type - Indicates whether a sample has been treated with a compound or control perturbation\n\n### train_targets_scored - \nDataset containing the output variables for training data. There are about 206 binary output variables showing MoAs triggered for a specific sig_id. This dataset is linked to the train_features dataset through ```sig_id``` column \n\n### test_features\nTest data on which our trained models have to make a prediction. \n\nLet's now take a deeper look at distribution of Gene Expression & Cell Viability Variables","e4c38cef":"* Distribution of MoA values for all three models seems to be similar, with RF having the shortest right tail\n* Sum of MoAs seems to have a positive correlation with log loss values. This shows us that class imbalance plays a role here and **our models do not do well for columns which have a high proportion of zeros**.\n\n","fa4749d9":"# Mechanisms of Action (MoA) Prediction Challenge\n\n![](https:\/\/media.cdn.lexipol.com\/article-images\/GettyImages-1153740646.jpg?w=300&format=jpg&quality=87)\n\nThis is a new tabular data competition recently started on Kaggle. This competition asks us to predict the action of drug sample based on gene expression & cell viability data. This competition is interesting because it provided me an opportunity to work on multilabel classification & Principal Component Analysis for the first time. This notebook is my attempt to make sense of the data through exploratory analysis & visualizations and establish a prediction benchmark using Random Forests. Please provide your feedback in the comments.\n\n## Table of Contents\n\n* [Making sense of Domain Specific Terminologies](#terminology)\n* [Importing the raw datasets & basic overview of datasets](#introduction)\n* [Distribution of Gene Expression & Cell Viability Variables](#gecv) \n* [Imapct of Drug Dosage & Interval on MoA](#dosage)\n* [Mechanism of Action Variables](#moa) \n* [Principal Component Analysis for Dimensionality Reduction](#pca) \n* [Random Forest Benchmark model](#model) \n","9febe368":"# Importing the Raw Datasets<a name=\"introduction\"><\/a>\n\nThe Competiton provides the following files - \n\n```train_features.csv```- Features for the training set. Features ```g-``` signify gene expression data, and ```c-``` signify cell viability data. cp_type indicates samples treated with a compound (```cp_vehicle```) or with a control perturbation (```ctrl_vehicle```); **control perturbations have no MoAs**; ```cp_time``` and ```cp_dose``` indicate treatment duration (24, 48, 72 hours) and dose (high or low).\n\n```test_features.csv``` - Features for the test data. We need to predict the probability of each scored MoA for each row in the test data\n\n```train_targets_scored.csv``` - Output dataset. Contains 206 binary output variables, each signifying a specific MoA for each sample in the ```train_features``` dataset\n\n```sample_submission.csv``` - The correct format for submissions. Reference the Evaluation tab for more info.\n\n","28b2f315":"# Dependent Variables - Mechanism of Action <a name=\"moa\"><\/a>\n\nSince this is a multilabel classfication problem, we have 206 binary dependent variables in the data, with individual columns for each MoA. We have to predict whether a particular gene expression & cell viability sample corresponds to a specific MoAs. One single drug sample can have multiple mechanism of actions. \n\nFor the charts below, we summed up the values of individual MoAs to understand which MoAs are most & least commonly triggered in the dataset. ","42426c35":"In the cells above we implemented PCA and plotted the Explained Variance plots. In the plots above we would be looking to identify the points where the curve tends to flatten a bit i.e. adding the number of principal components does not add the cumulative explained variance significantly. \n\nLooking at the plots above we would be selecting 100 PCs for Gene Expression Variables and 10 PCs for Cell Viability Variables.\n\nAlso majority of the variance in the GE & CV columns were expressed by the first few principal components, as is generally the case with PCA. Since the CV columns were highly correlated to each other, we require fewer PCs to explain the variance emanating from those columns. ","ce8012f0":"# Dimensionality Reduction - Principal Component Analysis <a name=\"pca\"><\/a>","cd217ab9":"#### Calculating Principal Components","a435177e":"## Terminologies<a name=\"terminology\"><\/a>\n\nWhen I first started out in this competition I had a hard time making sense of all the terminologies related to Genetics since I don't come from a Biology background. Before diving deep into the data, let's first list out all the domain specific terms that will be mentioned multiple times later on in the notebook - \n\n* **Gene** - A gene is the basic physical and functional unit of heredity. Genes are made up of DNA. Every person has two copies of each gene, one inherited from each parent. Most genes are the same in all people, but a small number of genes (less than 1 percent of the total) are slightly different between people\n\n* **Protein** - Proteins are large, complex molecules that play many critical roles in the body. They do most of the work in cells and are required for the structure, function, and regulation of the body\u2019s tissues and organs.\n\n* **Gene Expression** - Gene expression is the process by which the instructions in our DNA are converted into a functional product, such as a protein.Therefore, the thousands of genes expressed in a particular cell determine what that cell can do. A gene expression determines what kind of a function cell will perform in our body. \n\n* **Cell Viability** - Cell viability is a measure of the proportion of live, healthy cells within a population. Cell viability assays are used to determine the overall health of cells, optimize culture or experimental conditions, **and to measure cell survival following treatment with compounds, such as during a drug screen**.\n\n* **Mechanism of Action** - In medicine, a term used to describe how a drug or other substance produces an effect in the body. For example, a drug\u2019s mechanism of action could be how it affects a specific target in a cell, such as an enzyme, or a cell function, such as cell growth. Knowing the mechanism of action of a drug may help provide information about the safety of the drug and how it affects the body. It may also help identify the right dose of a drug and which patients are most likely to respond to treatment\n\nNow that we have a better understanding of the domain specific terminologies that we will encounter in this dataset, let's import the datasets and dive deep into the data.\n\n","3f25d3bd":"### XGBoost","4447f319":"# Distribution of Gene Expression & Cell Viability Variables <a name=\"gecv\"><\/a>"}}