{"cell_type":{"9ddb04b5":"code","8464a445":"code","68dcd97d":"code","96f72f0a":"code","eed0c346":"code","6995f2ae":"code","8eb182cc":"code","ed981b99":"code","0843d3e7":"code","da5a5931":"code","5e41b70c":"code","b35029c0":"code","45e82fad":"markdown","01fadbab":"markdown","6b4e4205":"markdown","3ee1f3bd":"markdown","d3db29d3":"markdown","2c877c10":"markdown","8b53d1b6":"markdown","0c2c7076":"markdown","a60b56ef":"markdown","0be8ed99":"markdown","b42e0452":"markdown"},"source":{"9ddb04b5":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport pickle\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('..\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","8464a445":"original = \"..\/input\/dataset\/final_project_dataset.pkl\"\n\n\ndestination = \"word_data_unix.pkl\"\n\ncontent = ''\noutsize = 0\nwith open(original, 'rb') as infile:\n    content = infile.read()\nwith open(destination, 'wb') as output:\n    for line in content.splitlines():\n        outsize += len(line) + 1\n        output.write(line + str.encode('\\n'))\n\nwith open('.\/word_data_unix.pkl','rb') as infile :\n    data_dict=pickle.load(infile)\n\ninfile.close()","68dcd97d":"df=pd.DataFrame.from_dict(data_dict)\nprint(df)","96f72f0a":"df = df.replace('NaN', np.nan)\ndf=df.fillna(value=\"0\")\ndf2=df.T","eed0c346":"df2=df2.drop(['email_address','loan_advances','restricted_stock_deferred','director_fees','deferral_payments'], axis = 1)\ndf2[df2.columns]=df2[df2.columns].astype(int)\n\ndf2=df2.replace(\"False\",\"0\").replace(\"True\",\"1\")\ndf2['poi']=df2['poi'].astype(int)\n","6995f2ae":"from sklearn.model_selection import train_test_split\nfeatures=list(df2.columns)\ny=df2['poi']\ndf3=df2\ndf2=df2.drop(['poi'],axis=1)\nX=df2[df2.columns]\nX_train,X_test,y_train,y_test=train_test_split(X,y,random_state=0)\n","8eb182cc":"from sklearn.preprocessing import StandardScaler,MinMaxScaler,PowerTransformer,RobustScaler\nscaler=PowerTransformer(method='yeo-johnson').fit(X_train)\nX_train=scaler.transform(X_train)\nX_test=scaler.transform(X_test)\n","ed981b99":"from sklearn.model_selection import cross_val_score\nfrom sklearn.ensemble import RandomForestClassifier,GradientBoostingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\n\n\nmodel=KNeighborsClassifier()\nmodel1=GradientBoostingClassifier()\n\nmodel3=RandomForestClassifier()\nmodel4=LogisticRegression(solver='liblinear')\nmodel5=SVC()\nmodel6=DecisionTreeClassifier()\nb=[]\nfor i in range(5,11):\n    acc=cross_val_score(model,X,y,cv=i)\n    acc1=cross_val_score(model1,X,y,cv=i)\n    \n    acc3=cross_val_score(model3,X,y,cv=i)\n    acc4=cross_val_score(model4,X,y,cv=i)\n    acc5=cross_val_score(model5,X,y,cv=i)\n    acc6=cross_val_score(model6,X,y,cv=i)\n    b.extend([acc.mean(),acc1.mean(),acc3.mean(),acc4.mean(),acc5.mean(),acc6.mean()])\nb=np.asarray(b).reshape(-1,6)\nprint(\"Best cross validation score:- \"+str(np.amax(b)))\na1= np.where(b==np.amax(b))[1]\na2= np.where(b==np.amax(b))[0]\nif a1==[0]:\n    print(\"K nearest neighbors\")\nelif a1==[1]:\n    print(\"Gradient Boosting Classifier\")\n\nelif a1==[2]:\n    print(\"Random Forest Classifier\")\nelif a1==[3]:\n    print(\"Logistic Regression\")\nelif a1==[4]:\n    print(\"Support Vector Classifier\")\nelse:\n    print(\"Decision Tree Classifier\")\nif a2==[0]:\n    print(\"No. of folds:- 5\")\nelif a2==[1]:\n    print(\"No. of folds:- 6\")\nelif a2==[2]:\n    print(\"No. of folds:- 7\")\nelif a2==[3]:\n    print(\"No. of folds:- 8\")\nelif a2==[4]:\n    print(\"No. of folds:- 9\")\nelif a2==[5]:\n    print(\"No. of folds:- 10\")\nelse:\n    print(\"No. of folds:- 11\")","0843d3e7":"from sklearn.model_selection import GridSearchCV\n\ngrid_values={'n_neighbors':list(range(1,70)),\n              }\ngrid_model=GridSearchCV(model,param_grid=grid_values,scoring='accuracy',cv=8).fit(X_train,y_train)\nprint(\"Best paramters:- \"+str(grid_model.best_params_))","da5a5931":"model=KNeighborsClassifier(n_neighbors=grid_model.best_estimator_.get_params()['n_neighbors'])\nmodel.fit(X_train,y_train)\ny_pred=model.predict_proba(X_test)\nprint(model.predict(X_test))\n","5e41b70c":"from sklearn.metrics import roc_auc_score,roc_curve,classification_report\nprint(\"Accuracy achieved on training set:- \"+str(model.score(X_train,y_train)))\nprint(\"Accuracy achieved on testing set:- \"+str(model.score(X_test,y_test)))\nprint(\"ROC Score:- \"+str(roc_auc_score(y_test,y_pred[:,1])))","b35029c0":"pickle.dump(model,open(\"my_classifier.pkl\",\"wb\"))\npickle.dump(new_dict,open(\"my_dataset.pkl\",\"wb\"))\npickle.dump(features,open(\"my_feature_list\",\"wb\"))","45e82fad":"**Performing Grid Search for hyperparameter tuning**","01fadbab":"**Converting pickle file into dictionary**","6b4e4205":"#data into pkl file","3ee1f3bd":"**Performing cross validation for model selection**","d3db29d3":"**Creating training and testing features**","2c877c10":"**Replacing NaN values with 0**","8b53d1b6":"**Creating Dataframe from dictionary**","0c2c7076":"**Typecasting columns to integer and dropping columns that have large number of null entries**","a60b56ef":"**Preprocessing training and testing features**","0be8ed99":"**Fitting tuned model and predicting probabilities of each class**","b42e0452":"**Model evaluation using different metrics**"}}