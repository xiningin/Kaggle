{"cell_type":{"60a6376b":"code","22e9d227":"code","6a1952a7":"code","05b2e000":"code","02f3bdad":"code","54621c3f":"code","a5f75e55":"code","799cd725":"code","c549bfee":"code","16e730a0":"code","3129118b":"code","b9aad4bc":"code","05350a3d":"code","b1c6569f":"code","a72309fe":"code","7451308a":"code","1db5c81f":"code","a7674de0":"code","c175078a":"code","c5551ff4":"code","f3b112e6":"code","5368ae7e":"code","526797c6":"code","b1f897ef":"code","2e79e1e2":"code","d5ff9b06":"code","e784b052":"code","439b9f83":"code","dbc08043":"code","8b1d2866":"code","1be408e3":"code","5bb9965d":"code","9d787295":"code","5b2ff500":"code","62c75ffd":"code","92d1b77c":"code","e6bb070c":"code","ec7326a7":"code","300fb70f":"code","ad2151ca":"code","7993e8f2":"code","15ea348d":"code","b513ff5c":"code","fdbbb6a5":"code","981b261f":"code","77d14a4b":"code","d32682ff":"code","975befdb":"code","c69a2dc5":"code","2651e0da":"code","2cf73277":"code","02ef8de3":"code","4f43ef65":"code","7a6c6e2b":"code","78a73db5":"markdown","ed3d2682":"markdown","4be9c6fd":"markdown","d06a1317":"markdown","3ba6c0ae":"markdown","b5c1842f":"markdown","db2917d8":"markdown","104b7485":"markdown","d6413057":"markdown","57ab65e7":"markdown","22ee9605":"markdown","fc879104":"markdown","10aa34ea":"markdown","860880f3":"markdown","bb10048b":"markdown","c964a002":"markdown","7875219a":"markdown","598b75a1":"markdown","f3bac833":"markdown","9d91919d":"markdown","74b88690":"markdown","bcc943b2":"markdown","5c9a4158":"markdown","7276b472":"markdown","7e788278":"markdown","1353f296":"markdown","5012ffbb":"markdown","8b782bab":"markdown","f3f7aee6":"markdown","c1ad1ef0":"markdown","83836125":"markdown","cd4a4551":"markdown","8a6be1f0":"markdown","36896ddb":"markdown","7bf1e97b":"markdown","35d9340f":"markdown","4dfa6d21":"markdown","013cc276":"markdown","a5ec9127":"markdown","c1c146e0":"markdown","b4343659":"markdown","1de7a0f9":"markdown","c1293ed2":"markdown","e1b70705":"markdown","02481c64":"markdown","070f1784":"markdown","e44b8677":"markdown","dbdc2115":"markdown","2eb1fa83":"markdown","a6a9cd8c":"markdown","0a71fff2":"markdown","964c26b2":"markdown","a690708b":"markdown","4b59c38a":"markdown","c027b800":"markdown","027dd37f":"markdown","9eeff686":"markdown","4185f780":"markdown","31f37437":"markdown","54b1b04a":"markdown"},"source":{"60a6376b":"#Import necessary libraries\nfrom sklearn.metrics import make_scorer, accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import classification_report\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom scipy.stats import norm, skew\nfrom sklearn import preprocessing\nimport matplotlib.pylab as pylab\nimport matplotlib.pyplot as plt\nfrom pandas import get_dummies\nimport matplotlib as mpl\nfrom scipy import stats\nimport xgboost as xgb\nimport seaborn as sns\nimport pandas as pd\nimport numpy as np\nimport matplotlib\nimport warnings\nimport sklearn\nimport scipy\nimport numpy\nimport json\nimport sys\nimport csv\nimport os","22e9d227":"print('matplotlib: {}'.format(matplotlib.__version__))\nprint('sklearn: {}'.format(sklearn.__version__))\nprint('scipy: {}'.format(scipy.__version__))\nprint('seaborn: {}'.format(sns.__version__))\nprint('pandas: {}'.format(pd.__version__))\nprint('numpy: {}'.format(np.__version__))\nprint('Python: {}'.format(sys.version))","6a1952a7":"sns.set(style='white', context='notebook', palette='deep')\npylab.rcParams['figure.figsize'] = 12,8\nwarnings.filterwarnings('ignore')\nmpl.style.use('ggplot')\nsns.set_style('white')\n%matplotlib inline","05b2e000":"# import the ames housing dataset\n\n#train = pd.read_csv('https:\/\/raw.githubusercontent.com\/hjhuney\/Data\/master\/AmesHousing\/train.csv')\n#test= pd.read_csv('https:\/\/raw.githubusercontent.com\/hjhuney\/Data\/master\/AmesHousing\/test.csv')\n\ntrain = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')","02f3bdad":"#preview the train dataset\ntrain.head()","54621c3f":"#preview the test dataset\ntest.head()","a5f75e55":"# Get the shape of the train and test data sets\ntrain_shape = train.shape\nprint(\"The shape of the train dataset is {}\".format(train_shape))\n\ntest_shape = test.shape\nprint(\"The shape of the test dataset is {}\".format(test_shape))","799cd725":"#Save the \"Id\" columns of the train and test datasets separately\nid_train = train[[\"Id\"]]\nid_test = test[\"Id\"]","c549bfee":"#Save the target feature column \"SalePrice\" separately, drop it from the train dataframe and Concatenate the new train and test data to get the full data in one dataframe\ntarget_feat = train[['SalePrice']]\ntrain_mod = train.drop(['SalePrice'], axis=1, inplace=False)\nfull_data = pd.concat([train_mod, test]).reset_index(drop=True)","16e730a0":"#dfrop the \"Id\" column from the full_data and print the first five rows to confirm that it is in the right state\nfull_data.drop(['Id'], axis=1, inplace=True)\nfull_data.head()","3129118b":"#print the first five of the target feature to confirm that it was well saved\ntarget_feat.head()","b9aad4bc":"#Confirm that the concatenation was well done by printing the shape of the full_data\nprint(\"full_data shape is : {}\".format(full_data.shape))","05350a3d":"#Print the dataset column labels\nfull_data.columns","b1c6569f":"import pandas_profiling as pp\n\n#Explore the data using pandas_profiling\nprofile = pp.ProfileReport(full_data)\nprofile","a72309fe":"#Plot the distribution of the target feature \"SalePrice\" \nsns.distplot(target_feat , fit=norm);\n\n# Get the distribution parameters\n(mu, sigma) = norm.fit(target_feat)\nprint( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\n\n#Plot the distribution\nplt.legend(['Normal Dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n            loc='best')\nplt.ylabel('Frequency')\nplt.title('SalePrice Distribution')","7451308a":"#Plot a heatmap to visualise the correlation among the top 10 most correlated feature \n\nk = 10 #number of variables for heatmap\ncorr_mat = train.corr()\ncols = corr_mat.nlargest(k, 'SalePrice')['SalePrice'].index\ncrm = np.corrcoef(train[cols].values.T)\nsns.set(font_scale=1.25)\n\nsns.heatmap(crm, cbar=True,annot=True,square=True, fmt='.2f', cmap='RdYlGn',linewidths=0.2,annot_kws={'size': 20}, yticklabels=cols.values, xticklabels=cols.values) \nfig=plt.gcf()\nfig.set_size_inches(18,15)\nplt.xticks(fontsize=14)\nplt.yticks(fontsize=14)\nplt.show()","1db5c81f":"#Get the 10 features that are least correlated with \"SalesPrice\"\ntrain_id_less = train.drop(['Id'], axis=1, inplace=False) #Drop the Id column from the train dataset\ncorr_table = train_id_less.corr()[['SalePrice']]\ncorr_table = abs(corr_table)\ncorr_table.sort_values(by=[\"SalePrice\"], ascending=False, inplace=True)\ncorr_table[-10:] \n","a7674de0":"#Generate a table showing the skewness of the features sorted in descending order and select the top 10 skewed features\nnumeric_feats = full_data.dtypes[full_data.dtypes != \"object\"].index\n\n# Check the skew of all numerical features\nskewed_feats = full_data[numeric_feats].apply(lambda x: skew(x.dropna())).sort_values(ascending=False)\nprint(\"\\nSkew in numerical features: \\n\")\nskewness_table = pd.DataFrame({'skewness' :skewed_feats})\nskewness_table.head(10)","c175078a":"#Do a pairplot of the 10 most correlated features\nsns.set()\nsns.pairplot(train[cols], size = 2.5)\nplt.show()\n","c5551ff4":"#Identify the duplicate rows\ndub_full = full_data.duplicated()\ndub_full = dub_full.loc[dub_full == True]\ndub_full","f3b112e6":"# Let us isolate the individual scatter plots of the features with outliers (i.e GrLivArea, TotalBsmtSF and 1stFlrSF) \n#to explore the exact values of the outliers\nout_feat = {\"GrLivArea\", \"TotalBsmtSF\", \"1stFlrSF\"}\nfig.suptitle('Features with Outliers')\nfor feature in (out_feat):\n    fig, ax = plt.subplots(figsize=(7, 4))\n    ax.scatter(x=train[feature],y=train[\"SalePrice\"])#, ax=ax)#(x = train['GrLivArea'], y = train['SalePrice'])\n    plt.ylabel('SalePrice', fontsize=12)\n    plt.xlabel(feature, fontsize=12)\n    plt.show()","5368ae7e":"#Drop the outliers from the features in question\n\n#make a copy of the train dataset\ntrain_copy = train.copy()\nfor ft in out_feat:\n    new_train = train_copy.drop(train_copy[(train_copy[ft]>4000) & (train_copy['SalePrice']<200000)].index)\n    train_copy = new_train\n\n#Plot new scatterplots for the old \"train\" and \"new_train\" datasets to confirm the dropping of the outlier observations\nfor feature in (out_feat):\n    fig, ax = plt.subplots(figsize=(7, 4))\n    ax.scatter(x=new_train[feature],y=new_train[\"SalePrice\"])#, ax=ax)#(x = train['GrLivArea'], y = train['SalePrice'])\n    plt.ylabel('SalePrice', fontsize=12)\n    plt.xlabel(feature, fontsize=12)\n    plt.show()\n","526797c6":"#Drop the \"SalePrice\" column from \"new_train\" dataset and save it as \"new_SalePrice\", concatenate \"test\" dataset to it and assign to \"new_full_data\". \n#Then drop the \"Id\" column from  \"new_full_data\".\n\nnew_SalePrice = new_train[['SalePrice']]\nnew_train_mod = new_train.drop(['SalePrice'], axis=1, inplace=False)\nnew_full_data = pd.concat([new_train_mod, test]).reset_index(drop=True)\nnew_full_data = new_full_data.drop([\"Id\"], axis=1, inplace=False)\nnew_full_data.head()\n","b1f897ef":"#Transform the \"SalePrice\" using np.log1p\nnew_SalePrice = np.log1p(new_SalePrice)\n\n#Plot the new distribution to see if it's normal\nsns.distplot(new_SalePrice, fit=norm)\n# Get the fitted parameters used by the function\n#(mu, sigma) = norm.fit(new_SalePrice)\n#print( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\nplt.legend(['Normal Distn. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],loc='best')\nplt.ylabel('Frequency')\nplt.title('Normalized SalePrice distribution')\nfig = plt.figure()\nplt.show()\n\nprint(\"The transformed SalePrice distribution is obviously much normal now\")","2e79e1e2":"#Now let us separate this list of features with missing values into numerical and categorical features to \n#enable us process them differently.\n\n#First, we make lists of the various features types from the classification we did at the beginning of the Jupyter notebook\nnom_cat = [\"MSSubClass\",\"MSZoning\",\"Street\",\"Alley\",\"LotShape\",\"Neighborhood\",\"RoofStyle\",\"RoofMatl\",\"Exterior1st\",\n                \"Exterior2nd\",\"MasVnrType\",\"Foundation\",\"Heating\",\"CentralAir\",\"GarageType\",\"MiscFeature\",\"MoSold\",\n                \"SaleType\",\"SaleCondition\"]\n\nord_cat = [\"LandContour\",\"Utilities\",\"LotConfig\",\"LandSlope\",\"Condition1\",\"Condition2\",\"BldgType\",\"HouseStyle\",\n            \"OverallQual\",\"OverallCond\",\"ExterQual\",\"ExterCond\",\"BsmtQual\",\"BsmtCond\",\"BsmtExposure\",\"BsmtFinType1\",\n            \"BsmtFinType2\",\"HeatingQC\",\"Electrical\",\"Functional\",\"FireplaceQu\",\"GarageFinish\",\"GarageQual\",\"GarageCond\",\n            \"PavedDrive\",\"PoolQC\",\"Fence\",\"KitchenQual\"]\n\ndiscr_num = [\"BsmtFullBath\",\"BsmtHalfBath\",\"FullBath\",\"HalfBath\",\"BedroomAbvGr\",\"KitchenAbvGr\",\"TotRmsAbvGrd\",\"Fireplaces\",\n            \"GarageYrBlt\",\"GarageCars\",\"YrSold\",\"YearBuilt\",\"YearRemodAdd\"]\n\ncont_num = [\"2ndFlrSF\",\"LowQualFinSF\",\"GrLivArea\",\"GarageArea\",\"WoodDeckSF\",\"OpenPorchSF\",\"EnclosedPorch\",\"3SsnPorch\",\n            \"ScreenPorch\",\"PoolArea\",\"MiscVal\",\"LotFrontage\",\"LotArea\",\"MasVnrArea\",\"BsmtFinSF1\",\n            \"BsmtFinSF2\",\"BsmtUnfSF\",\"TotalBsmtSF\",\"1stFlrSF\"]\n\n#Then we combine the nominal and ordinal categorical features into a single list; \n#and the discrete and continuous numerical features into anoyther list\ncat_feat = nom_cat + ord_cat\nnum_feat = discr_num + cont_num\n","d5ff9b06":"#Handling missing values \n#Create list of features with missing values\nfeat_miss = new_full_data.columns[new_full_data.isnull().any()].tolist() \n\n#Now we separate the list of features with missing values into categorical and numeric features\nmiss_cat = []\nmiss_num = []\nfor fm in feat_miss:\n    if fm in cat_feat:\n        miss_cat.append(fm)\n    else:\n        miss_num.append(fm)\n    \nprint(\"The numerical features with missing values are:{}\".format(miss_num))\nprint()\nprint(\"The categorical features with missing values are:{}\".format(miss_cat))","e784b052":"#Now let's impute \"none\" for the missing values of the categorical features\nfrom sklearn.impute import SimpleImputer\n\nimputer = SimpleImputer(missing_values=np.nan,strategy='constant',fill_value = \"none\")\nimputer = imputer.fit(new_full_data[miss_cat])\nnew_full_data[miss_cat] = imputer.transform(new_full_data[miss_cat])\n\n#Now let's impute 0 for the missing values of the numerical features\nnew_full_data[miss_num] = new_full_data[miss_num].fillna(0)\nnew_full_data.head()","439b9f83":"#Let's get the list of the highly skewed features from the skewness_table generated earlier\nskew_list = list(skewness_table.index.values)[:23]\n \n#Box Cox Transform these skewed explanatory continuous numerical features\nfrom scipy.special import boxcox1p\n\nlam = 0.15\nfor snf in skew_list:\n    new_full_data[snf] = boxcox1p(new_full_data[snf], lam)\nnew_full_data.head()","dbc08043":"#initiallize dataframe of numerical features\nnum_df = new_full_data[num_feat]\n#Normalize the numerical features in the new_full_data\nnew_full_data[num_feat] = (num_df-num_df.mean())\/num_df.std()\nnew_full_data[num_feat].head()","8b1d2866":"#Get the lists of unique values of the ordinal categorical features \n#new_ord_cat = [x for x in ord_cat if x not in [\"Fence\", \"MiscFeature\", \"Utilities\"]]\nuniq_obs = []\n#for od in new_ord_cat:\nfor od in ord_cat:\n    vals = list(sorted(new_full_data[od].unique()))\n    uniq_obs.append(vals)\n    \n#Print the list of the lists of unique values of the ordinal categorical features \nprint(uniq_obs)\nlen(uniq_obs)","1be408e3":"from sklearn.preprocessing import OrdinalEncoder\n#Generate the list of correctly ordered lists of unique ordinal features values in the sequence that they appear in \"ord_cat\" earlier defined\nordered_feat_value = [['Low','Lvl','Bnk', 'HLS'],['ELO','NoSeWa','NoSewr',\"AllPub\"],['Inside','Corner', 'CulDSac', 'FR2', 'FR3'],\n                     ['Sev', 'Mod','Gtl'],['Artery', 'Feedr', 'Norm','RRNn','RRAn', 'PosN','PosA','RRNe','RRAe'],\n                     ['Artery', 'Feedr', 'Norm','RRNn','RRAn', 'PosN','PosA','RRNe','RRAe'],['1Fam','2fmCon','Twnhs','TwnhsE','Duplex'],\n                     ['1Story','1.5Unf','1.5Fin','SFoyer','SLvl','2Story','2.5Unf', '2.5Fin'],[1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n                     [1, 2, 3, 4, 5, 6, 7, 8, 9],['Fa','TA','Gd','Ex'],['Po','Fa','TA','Gd','Ex'],['none','Fa','TA','Gd','Ex'],\n                     ['none','Po','Fa', 'TA','Gd'],['none','No','Mn', 'Av', 'Gd'],['none','Unf','LwQ','Rec', 'BLQ','ALQ','GLQ'],\n                     ['none','Unf','LwQ','Rec', 'BLQ','ALQ','GLQ'],['Po','Fa','TA','Gd','Ex'],['none','Mix','FuseP','FuseF','FuseA','SBrkr'],\n                     ['none','Sev','Maj2','Maj1','Mod','Min2','Min1','Typ'],['none','Po', 'Fa','TA','Gd','Ex'],['none','Unf','RFn','Fin'],\n                     ['none','Po','Fa','TA','Gd','Ex'],['none','Po','Fa','TA','Gd','Ex'],['N', 'P', 'Y'],['none','Fa','Gd','Ex'],\n                     ['none','MnWw','GdWo','MnPrv','GdPrv'],['none','Fa','TA','Gd','Ex']]\n\nfor ftr,ordv in zip(ord_cat,ordered_feat_value):\n    cat = pd.Categorical(new_full_data[ftr],categories = ordv, ordered = True)\n    labels, unique = pd.factorize(cat, sort=True)\n    new_full_data[ftr] = labels","5bb9965d":"#We adopt the pd.get_dummies to transform the nominal categorical features, while inserting the drop_first=True arguement\n#to handle the multicollinearity that may result\n#new_nom_cat = [x for x in nom_cat if x not in [\"Fence\", \"MiscFeature\", \"Utilities\"]]\nencoded_nom = pd.get_dummies(new_full_data[nom_cat],drop_first=True)\n\n#Drop the former norminal categorical features from new_full_data and concatenate the encoded norminal categorical features\nnew_full_data = new_full_data.drop(nom_cat, axis=1, inplace=False)\nnew_full_data = pd.concat([new_full_data, encoded_nom], axis=1)\n","9d787295":"len(new_full_data.columns.values)","5b2ff500":"#Import necesaary libraries and modules\nfrom sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC, LassoCV\nfrom sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split\nfrom sklearn.metrics import mean_squared_error\nimport xgboost as xgb\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import RepeatedKFold , KFold\nfrom sklearn.metrics import make_scorer\n\n#get the fully and finally processed training and test data\ntrain_final = new_full_data[:len(new_train_mod)]\ntest_final = new_full_data[len(new_train_mod):]\n\n#Now here's the model selection function\ndef mod_sel_cv(model):\n    kf = KFold(n_splits=10, shuffle=True, random_state=25).get_n_splits(train_final.values)\n    scorer = np.sqrt(-cross_val_score(model, train_final.values, new_SalePrice.values,scoring=\"neg_mean_squared_error\", cv=kf))\n    return scorer","62c75ffd":"#We use the sklearn's Robustscaler() method to make the model more robust on outliers\nlasso = make_pipeline(RobustScaler(),Lasso(alpha=0.0002,max_iter=501,random_state=1))\nscore1 = mod_sel_cv(lasso)\nprint(\"\\nLasso score: {:.4f} ({:.4f})\\n\".format(score1.mean(), score1.std()))","92d1b77c":"#We use the sklearn's Robustscaler() method to make the model more robust on outliers\nKRR = make_pipeline(RobustScaler(),KernelRidge(alpha=0.1, kernel=\"polynomial\", degree=1, coef0=4.5))\nscore2 = mod_sel_cv(KRR)\nprint(\"Kernel Ridge score: {:.4f} ({:.4f})\\n\".format(score2.mean(), score2.std()))","e6bb070c":"#We use the sklearn's Robustscaler() method to make the model more robust on outliers\nRFR = RandomForestRegressor(max_depth=8, random_state=0,n_estimators=2200)\nscore3 = mod_sel_cv(RFR)\nprint(\"Random Forest Regression score: {:.4f} ({:.4f})\\n\".format(score3.mean(), score3.std()))","ec7326a7":"xg_reg = xgb.XGBRegressor(colsample_bytree=0.5, gamma=0.0468, \n                             learning_rate=0.05, max_depth=3, \n                             min_child_weight=3.5, n_estimators=2200,\n                             reg_alpha=0.2500, reg_lambda=0.8571,\n                             subsample=0.5213, silent=1,\n                             random_state =7, nthread = -1)\n\nscore4 = mod_sel_cv(xg_reg)\nprint(\"XGBoost score: {:.4f} ({:.4f})\\n\".format(score4.mean(), score4.std()))","300fb70f":"#We use the sklearn's Robustscaler() method to make the model more robust on outliers\nENet = make_pipeline(RobustScaler(), ElasticNet(alpha=0.00025, l1_ratio= 0.90, random_state=1))\nscore5 = mod_sel_cv(ENet)\nprint(\"Elastic Net Regression score: {:.4f} ({:.4f})\\n\".format(score5.mean(), score5.std()))","ad2151ca":"#We use the huber loss to make the model more robust to outliers\nGBR = GradientBoostingRegressor(n_estimators=2200, learning_rate=0.01,\n                                   max_depth=4, max_features='sqrt',\n                                   min_samples_leaf=15, min_samples_split=10, \n                                   loss='huber', subsample=0.85, random_state =5)\nscore7 = mod_sel_cv(GBR)\nprint(\"Gradient Boosting Regression score: {:.4f} ({:.4f})\\n\".format(score7.mean(), score7.std()))","7993e8f2":"import lightgbm as lgb\nml_lgb = lgb.LGBMRegressor(objective='regression',num_leaves=8,\n                              learning_rate=0.05, n_estimators=500,\n                              max_bin = 100, bagging_fraction = 0.8,\n                              bagging_freq = 5, feature_fraction = 0.2319,\n                              feature_fraction_seed=9, bagging_seed=9,\n                              min_data_in_leaf =6, min_sum_hessian_in_leaf = 6)\n\nscore_lgb = mod_sel_cv(ml_lgb)\nprint(\"Light Gradient Boosting Regression score: {:.4f} ({:.4f})\\n\".format(score_lgb.mean(), score_lgb.std()))\n","15ea348d":"# Import necessary modules\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nfrom math import sqrt\n\n# Keras specific\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense\n\n\nX = train_final.values #df[predictors].values\ny = new_SalePrice.values \n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=40)\nprint(X_train.shape); print(X_test.shape)\n\n\n# Define model\nkeras_model = Sequential()\nkeras_model.add(Dense(500, input_dim=178, activation= \"relu\"))\nkeras_model.add(Dense(100, activation= \"relu\"))\nkeras_model.add(Dense(100, activation= \"relu\"))\nkeras_model.add(Dense(50, activation= \"relu\"))\nkeras_model.add(Dense(1))\n#model.summary() #Print model Summary\n\nkeras_model.compile(loss= \"mean_squared_error\" , optimizer=\"adam\", metrics=[\"mean_squared_error\"])\nkeras_model.fit(X_train, y_train, epochs=20)\n\npred_train= keras_model.predict(X_train)\nprint(np.sqrt(mean_squared_error(y_train,pred_train)))","b513ff5c":"from keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.wrappers.scikit_learn import KerasRegressor\n\n\ndef baseline_model():\n    # create model\n    model = Sequential()\n    model.add(Dense(500, input_dim=178, kernel_initializer='normal', activation='relu'))\n    model.add(Dense(100, activation= \"relu\"))\n    model.add(Dense(100, activation= \"relu\"))\n    model.add(Dense(100, activation= \"relu\"))\n    model.add(Dense(50, activation= \"relu\"))\n    model.add(Dense(1, kernel_initializer='normal'))\n    # Compile model\n    model.compile(loss='mean_squared_error', optimizer='adam')\n    return model\n\n# fix random seed for reproducibility\nseed = 7\nnumpy.random.seed(seed)\n# evaluate model with standardized dataset\nkeras_estimator = KerasRegressor(build_fn=baseline_model, epochs=100, batch_size=5, verbose=0)\nscore_keras = mod_sel_cv(keras_estimator)\nprint(\"Keras Regression score: {:.4f} ({:.4f})\\n\".format(score_keras.mean(), score_keras.std()))","fdbbb6a5":"from sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin\n\n\nclass AveragingModels(BaseEstimator, RegressorMixin, TransformerMixin):\n    def __init__(self, models):\n        self.models = models\n        \n    # we define clones of the original models to fit the data in\n    def fit(self, X, y):\n        self.models_ = [clone(x) for x in self.models]\n        \n        # Train cloned base models\n        for model in self.models_:\n            model.fit(X, y)\n\n        return self\n    \n    #Now we do the predictions for cloned models and average them\n    def predict(self, X):\n        predictions = np.column_stack([\n            model.predict(X) for model in self.models_\n        ])\n        return np.mean(predictions, axis=1)   \n\n    \naveraged_models = AveragingModels(models = (lasso,ENet,KRR,GBR,ml_lgb,xg_reg))\n\nscore = mod_sel_cv(averaged_models)\nprint(\" Averaged base models score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","981b261f":"class StackingAveragedModels(BaseEstimator, RegressorMixin, TransformerMixin):\n    def __init__(self, base_models, meta_model, n_folds=5):\n        self.base_models = base_models\n        self.meta_model = meta_model\n        self.n_folds = n_folds\n   \n    # We again fit the data on clones of the original models\n    def fit(self, X, y):\n        self.base_models_ = [list() for x in self.base_models]\n        self.meta_model_ = clone(self.meta_model)\n        kfold = KFold(n_splits=self.n_folds, shuffle=True, random_state=156)\n        \n        # Train cloned base models then create out-of-fold predictions\n        # that are needed to train the cloned meta-model\n        out_of_fold_predictions = np.zeros((X.shape[0], len(self.base_models)))\n        for i, model in enumerate(self.base_models):\n            for train_index, holdout_index in kfold.split(X, y):\n                instance = clone(model)\n                self.base_models_[i].append(instance)\n                instance.fit(X[train_index], y[train_index])\n                y_pred = instance.predict(X[holdout_index])\n                #define out_of_fold_predictions[holdout_index, i] as 1D array for it to work in my code\n                oofp = np.zeros(out_of_fold_predictions[holdout_index, i].shape[0]) \n                oofp = y_pred\n\n        # Now train the cloned  meta-model using the out-of-fold predictions as new feature\n        self.meta_model_.fit(out_of_fold_predictions, y)\n        return self\n   \n    #Do the predictions of all base models on the test data and use the averaged predictions as \n    #meta-features for the final prediction which is done by the meta-model\n    def predict(self, X):\n        meta_features = np.column_stack([\n            np.column_stack([model.predict(X) for model in base_models]).mean(axis=1)\n            for base_models in self.base_models_ ])\n        return self.meta_model_.predict(meta_features)\n    \n    \n\n        \nstacked_averaged_models = StackingAveragedModels(base_models = (lasso,ENet,KRR,GBR,ml_lgb),meta_model = xg_reg)\n\nscore_st = mod_sel_cv(stacked_averaged_models)\nprint(\"Stacking Averaged models score: {:.4f} ({:.4f})\".format(score_st.mean(), score_st.std()))","77d14a4b":"from mlxtend.regressor import StackingRegressor\nfrom sklearn.model_selection import cross_validate\n\nstack1 = StackingRegressor(regressors=[ENet,KRR,GBR,lasso,ml_lgb], meta_regressor=xg_reg, verbose=0)\nstack_scores = mod_sel_cv(stack1)#cross_validate(stack1, X, y, cv=10)\nprint(\"StackingRegressor score: {:.4f} ({:.4f})\\n\".format(stack_scores.mean(), stack_scores.std()))","d32682ff":"averaged_models.fit(train_final.values, new_SalePrice.values)\nmodel_train_pred = averaged_models.predict(train_final.values)\nmodel_test_pred = np.expm1(averaged_models.predict(test_final.values))\nprint(np.sqrt(mean_squared_error(new_SalePrice.values, model_train_pred)))","975befdb":"model_lgb = lgb.LGBMRegressor(objective='regression',task = \"predict\",num_leaves=8,\n                              learning_rate=0.05, n_estimators=500,\n                              max_bin = 100, bagging_fraction = 0.8,\n                              bagging_freq = 5, feature_fraction = 0.2319,\n                              feature_fraction_seed=9, bagging_seed=9,\n                              min_data_in_leaf =6, min_sum_hessian_in_leaf = 6)\n\nmodel_lgb.fit(train_final.values, new_SalePrice.values)\nmodel_lgb_train_pred = model_lgb.predict(train_final.values)\nmodel_lgb_test_pred = np.expm1(model_lgb.predict(test_final.values))\nprint(np.sqrt(mean_squared_error(new_SalePrice.values, model_lgb_train_pred)))","c69a2dc5":"GBR.fit(train_final.values, new_SalePrice.values)\nGBR_train_pred = GBR.predict(train_final.values)\nGBR_test_pred = np.expm1(GBR.predict(test_final.values))\nprint(np.sqrt(mean_squared_error(new_SalePrice.values, GBR_train_pred)))","2651e0da":"xg_reg.fit(train_final.values, new_SalePrice.values)\nxg_reg_train_pred = xg_reg.predict(train_final.values)\nxg_reg_test_pred = np.expm1(xg_reg.predict(test_final.values))\nprint(np.sqrt(mean_squared_error(new_SalePrice.values, xg_reg_train_pred)))","2cf73277":"stack1.fit(train_final.values, new_SalePrice.values)\nstack1_train_pred = stack1.predict(train_final.values)\nstack1_test_pred = np.expm1(stack1.predict(test_final.values))\nprint(np.sqrt(mean_squared_error(new_SalePrice.values, stack1_train_pred)))","02ef8de3":"keras_model.fit(train_final.values, new_SalePrice.values)\nkeras_train_pred = keras_model.predict(train_final.values)\nkeras_test_pred = np.expm1(keras_model.predict(test_final.values))\nprint(np.sqrt(mean_squared_error(new_SalePrice.values, keras_train_pred)))","4f43ef65":"ensemble_pred = stack1_test_pred*0.50 + model_test_pred*0.50","7a6c6e2b":"subm = pd.DataFrame()\nsubm['Id'] = id_test\nsubm['SalePrice'] = ensemble_pred * 1.01\nsubm.to_csv('submission.csv',index=False)","78a73db5":"### Initialize, Fit and Score the Base Models","ed3d2682":"#### Keras Regression","4be9c6fd":"#### References\n    1.https:\/\/towardsdatascience.com\/introduction-to-data-preprocessing-in-machine-learning-a9fa83a5dc9d\n    2.https:\/\/www.kaggle.com\/kernels\/scriptcontent\/9252125\/download\n    3.https:\/\/www.kaggle.com\/serigne\/stacked-regressions-top-4-on-leaderboard\n    4.https:\/\/www.datacamp.com\/community\/tutorials\/kaggle-machine-learning-eda\n    5.https:\/\/www.kaggle.com\/kernels\/scriptcontent\/2739619\/download\n    6.https:\/\/medium.com\/datadriveninvestor\/data-preprocessing-for-machine-learning-188e9eef1d2c\n    7.https:\/\/www.datacamp.com\/community\/tutorials\/xgboost-in-python\n    8.https:\/\/www.analyticsvidhya.com\/blog\/2018\/06\/comprehensive-guide-for-ensemble-models\/\n    9.https:\/\/towardsdatascience.com\/automate-stacking-in-python-fc3e7834772e\n    10.https:\/\/www.datacamp.com\/community\/tutorials\/parameter-optimization-machine-learning-models\n    11.https:\/\/machinelearningmastery.com\/implementing-stacking-scratch-python\/\n    12.https:\/\/inclass.kaggle.com\/eikedehling\/trying-out-stacking-approaches\/code","d06a1317":"#### Normalize the Numeric Features","3ba6c0ae":"Investigating the above rows shows that even though they have the same values for many of the features they are not exact identical or duplicates. So we do not drop any row from the dataset.","b5c1842f":"### Lasso","db2917d8":"#### Handling Outliers","104b7485":"## Exploratory Data Analysis (EDA)","d6413057":"#### Averaged Base Models Stacking\nFor this stacked generalization, we will average the six top performing base models (lasso,ENet,KRR,GBR,ml_lgb,xg_reg). ","57ab65e7":"### Final Training and Prediction\u00b6","22ee9605":"Surprisingly, the __Stacking Averaged Models__ returns an extremely low score compared to all the earlier models ","fc879104":"#### Transform the Skewed Target Feature using Log-transformation","10aa34ea":"#### Gradient Boosting Regression","860880f3":"#### Kernel Ridge Regression","bb10048b":"#### Show the highly skewed features table","c964a002":"##### First let us separate list of features into ordinal categorical, nominal categorical, discrete numerical and continuous numerical features for use later on ","7875219a":"### Setup","598b75a1":"#### Handling Highly Skewed Numeric Features","f3bac833":"#### Scanning the Pairplots for Outliers\nA cursory look at the above pairplot shows that there are outliers in the **GrLivArea**, **TotalBsmtSF** and **1stFlrSF** features, which should be dropped at the pre-processing stage","9d91919d":"#### Stacking Averaged Models\nNow let's also see the performance of a stacked model with a meta_model ","74b88690":"### Data Analysis & Visualisation","bcc943b2":"### Versions","5c9a4158":"#### Random Forest Regression","7276b472":"#### Elastic Net Regression ","7e788278":"#### Handling Nominal Categorical Features (One-Hot-Encoding) and Multicollinearity","1353f296":"The __Mlxtend Stacking Regressor__ also returns a score worse than that of the  __Averaged Base Model__. In the overall, the __Averaged Base Model__ obviously stands out as the better model in terms of their scores.","5012ffbb":" #### XGBoost","8b782bab":"#### Ensembled Prediction\nWe do a weighted addition of the predictions of __Averaged Base Model__ and __Mlxtend Stacking Regressor__ ","f3f7aee6":"So far, the __Averaged Base Model__ returns a score better than the other models ","c1ad1ef0":"#### XGBoost","83836125":"#### Mlxtend Stacking Regressor\nWe do an equally weighted addition of the predictions of Averaged Base Model and Mlxtend Stacking Regressor","cd4a4551":"We want to compare the model performance among (i) LASSO (or kernel ridge regression), (ii) random forests regression, (iii) XGBoost, and (iv) stacking (v) Any other method we find effective.","8a6be1f0":"#### Handling Ordinal Categorical Features","36896ddb":"## Modelling","7bf1e97b":"#### Light Gradient Boosting Regression","35d9340f":"### Define a model selection function","4dfa6d21":"#### Handling missing values ","013cc276":"#### Light Gradient Boosting Regression","a5ec9127":"# Advance Prediction of Ames House Prices","c1c146e0":"### Libraries","b4343659":"#### Mlxtend Stacking Regressor","1de7a0f9":"## Problem Description","c1293ed2":"The problem here is to build a machine learning model that accurately predicts the final prices of residential homes in Ames, Iowa given some specific features of these homes.\nThe model is to be trained on 79 explanatory variables that describe (almost) every aspect of residential homes sampled across Ames, Iowa.","e1b70705":"## Python Libraries and Setup","02481c64":"#### Analysing The Heatmap\nThe heatmap of the 10 most correlated features above reported a somewhat high correlation (about 88%) between __GarageArea__ and __GarageCars__. However, I believe we should still keep both features.","070f1784":"### Data Collection","e44b8677":"In all there are 79 explanatory features and one target features - the SalesPrice. The features types cut across categorical variables(nominal and ordinal) and numerical variables(continuous and discrete).\n\nFrom analysis and research the various features can be grouped under the various types of data as follows: \n\n#### Nominal Categorical Variables\t\n\t\"MSSubClass\",\n\t\"MSZoning\",\n\t\"Street\",\n\t\"Alley\",\n\t\"LotShape\",\n\t\"Neighborhood\",\n\t\"RoofStyle\",\n\t\"RoofMatl\",\n\t\"Exterior1st\",\n\t\"Exterior2nd\",\n\t\"MasVnrType\",\n\t\"Foundation\",\n\t\"Heating\",\n\t\"CentralAir\",\n\t\"GarageType\",\n\t\"MiscFeature\",\n\t\"MoSold\",\n\t\"SaleType\",\n\t\"SaleCondition\",\n\t\n#### Ordinal Categorical Variables\t\n\t\"LandContour\",\n\t\"Utilities\",\n\t\"LotConfig\",\n\t\"LandSlope\",\n\t\"Condition1\",\n\t\"Condition2\",\n\t\"BldgType\",\n\t\"HouseStyle\",\n\t\"OverallQual\",\n\t\"OverallCond\",\n\t\"ExterQual\",\n\t\"ExterCond\",\n\t\"BsmtQual\",\n\t\"BsmtCond\",\n\t\"BsmtExposure\",\n\t\"BsmtFinType1\",\n\t\"BsmtFinType2\",\n\t\"HeatingQC\",\n\t\"Electrical\",\n\t\"Functional\",\n\t\"FireplaceQu\",\n\t\"GarageFinish\",\n\t\"GarageQual\",\n\t\"GarageCond\",\n\t\"PavedDrive\",\n\t\"PoolQC\",\n\t\"Fence\",\n\t\"KitchenQual\",\n\t\n#### Discrete Numberical Variables\t\n\t\"BsmtFullBath\",\n\t\"BsmtHalfBath\",\n\t\"FullBath\",\n\t\"HalfBath\",\n\t\"BedroomAbvGr\",\n\t\"KitchenAbvGr\",\n\t\"TotRmsAbvGrd\",\n\t\"Fireplaces\",\n\t\"GarageYrBlt\",\n\t\"GarageCars\",\n\t\"YrSold\",\n\t\"YearBuilt\",\n\t\"YearRemodAdd\",\n\t\n#### Continuous Numberical Variables\t\n\t\"2ndFlrSF\",\n\t\"LowQualFinSF\",\n\t\"GrLivArea\",\n\t\"GarageArea\",\n\t\"WoodDeckSF\",\n\t\"OpenPorchSF\",\n\t\"EnclosedPorch\",\n\t\"3SsnPorch\",\n\t\"ScreenPorch\",\n\t\"PoolArea\",\n\t\"MiscVal\",\n\t\"SalePrice\",\n\t\"LotFrontage\",\n\t\"LotArea\",\n\t\"MasVnrArea\",\n\t\"BsmtFinSF1\",\n\t\"BsmtFinSF2\",\n\t\"BsmtUnfSF\",\n\t\"TotalBsmtSF\",\n\t\"1stFlrSF\",","dbdc2115":"#### Analysing the Categorical and Numeric Features with the pandas_profiling EDA\nExamining the warnings of the pandas_profiling EDA report from the perspective of the dataset description [here,](https:\/\/www.kaggle.com\/c\/house-prices-advanced-regression-techniques\/download\/data_description.txt) reveals the following interesting facts:\n\n    1.The NA's in the categorical features turned out as missing values in the pandas DataFrame. To resolve this we would  fill the missing categorical values with \"None\" in our data pre-processing stage.\n    2.The numerical features that correspond to the \"NA\" observations of the categorical features also turned out as zeros. In this case it is best to leave the zeros as they are, as there were actually nothing to measure\n    3. It is reassuring to note that the percentages of missing values for the various categorical features are rather equal to the percentage of zeros for the various nuimerical features that measure them.\n\n    The EDA also showed a highly skewed __MiscVal__ which needs to be corrected.\n\n    There is also a report of 2 duplicate rows in the dataset which needs to be corrected. ","2eb1fa83":"#### Analysing the Skewness Table\nFrom the computed skewness table we can see that in addition to the __MiscVal__ feature earlier identified, there are several other numerical features with relatively high skewness that need to be transformed.","a6a9cd8c":"### Variables\/Features List","0a71fff2":"#### Searching for Irrelevant Features\nThe \"corr_table\" above showing the 10 features that are least correlated with the target feature is computed in a bid to find out if there are explanatory features that are not at all correlated (zero or very minimal correlation) with the target feature \"SalePrice\".\n\nAs we can see from the table there are no features with zero or extremely minimal correlation with \"SalePrice\". So no feature should be dropped on the basis of that. ","964c26b2":"### Stacking models","a690708b":"#### Benjamin Umeh","4b59c38a":"### Data Pre-processing","c027b800":"#### Handling Duplicated Rows in Dataset","027dd37f":"__Kindly upvote if this submission makes some sense to you__","9eeff686":"#### Gradient Boosting Regression","4185f780":"To correctly transform our ordinal features we must ensure that the orders of the categories are respected. \n\nTo achieve this requires that we pass an ordered list of the unique values for the features to the categories parameter in the sklearn OrdinalEncoder class.\n\nThis means that we must manaually guarantee an ordered list of the unique string values for those ordinal categorical features with string values. Since python does not know the true orders of the string values. \n\nAgain, we can ascertain the correct orders of the string values by referring to the dataset description [here](https:\/\/www.kaggle.com\/c\/house-prices-advanced-regression-techniques\/download\/data_description.txt)","31f37437":"#### Averaged Base Model","54b1b04a":"#### Analysing the Target Feature\nFrom the SalesPrice Distribution plot above we can see that the distribution of the SalePrice is skewed to the right and there need to be normalized."}}