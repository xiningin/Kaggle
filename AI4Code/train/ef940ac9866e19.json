{"cell_type":{"d48314f6":"code","6d91b959":"code","12fbacab":"code","1ec0554c":"code","6b96a10b":"code","c0a0c23f":"code","7346720a":"code","df2c6290":"code","1ed95c3b":"code","299e7dc6":"code","680b6144":"code","dccba35a":"code","47dbeb33":"code","a41c8277":"code","2b9c51f7":"code","4e41c65c":"code","6e1b1399":"code","8cb20497":"code","d45af402":"code","7b433168":"code","a01c080d":"code","d2093484":"code","94612a65":"code","d94cc13f":"code","b20102a1":"code","9dd98584":"code","e716c122":"code","7a48e123":"code","5df1d03e":"code","6236e220":"code","6ad6dd1a":"code","004b8ee7":"code","f8f2645b":"code","b5ae5aff":"code","8493abbc":"code","d9629bf8":"code","769c239b":"code","dbf4b2d2":"code","a99b54c0":"code","02e7047e":"code","abca83d5":"code","9fda6b21":"code","aa88e01e":"code","16570b2b":"code","ea281eac":"code","cd4b4134":"code","811f879d":"code","93ef946c":"code","73a868ab":"code","dc82e206":"code","b3d213d8":"code","5ccd7eac":"code","a51b1618":"code","642800be":"markdown","403e6c45":"markdown","2e5aee07":"markdown","aac74b3e":"markdown","dd0fa148":"markdown","561a8ef9":"markdown","e6fcfd46":"markdown","e0b1a2e4":"markdown","5797a333":"markdown","b9be4fa2":"markdown","8fb6064c":"markdown","8198970f":"markdown","e1ea3158":"markdown","0df5fc08":"markdown","04ff011f":"markdown","f4a7c178":"markdown","8961330a":"markdown","a8d36c27":"markdown","eb3fedbd":"markdown","561fa222":"markdown","ed910488":"markdown","bea366fe":"markdown","09782bc5":"markdown","7b02681f":"markdown","9586ac49":"markdown","e178f8ca":"markdown","57f8ce9a":"markdown","c932c450":"markdown","2c60a339":"markdown","383e120f":"markdown","6a43867c":"markdown","1c22bd4e":"markdown","d2be7470":"markdown","180454ec":"markdown","cff1b4f5":"markdown"},"source":{"d48314f6":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error, r2_score, accuracy_score\nfrom sklearn.preprocessing import LabelEncoder\nimport random\nfrom sklearn import linear_model\nimport keras\nfrom keras.utils import to_categorical\nfrom keras.models import Sequential\nfrom tensorflow.keras import layers, callbacks\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nfrom keras.callbacks import ReduceLROnPlateau, ModelCheckpoint\nfrom tensorflow.keras.models import load_model\nfrom xgboost import XGBClassifier as xgb\nfrom lightgbm import LGBMClassifier as lgb","6d91b959":"test = pd.read_csv('..\/input\/tabular-playground-series-apr-2021\/test.csv')\ntrain = pd.read_csv('..\/input\/tabular-playground-series-apr-2021\/train.csv')","12fbacab":"train.head(10)","1ec0554c":"test.head(10)","6b96a10b":"train.info()","c0a0c23f":"test.info()","7346720a":"print(train.isnull().sum())","df2c6290":"print(test.isnull().sum())","1ed95c3b":"#Aplicamos un codificador a la columna categorica para transformarla en valores del 0 al 2\nencoder = LabelEncoder()\nencoder.fit(train['Embarked'])\ntrain['Embarked'] = encoder.fit_transform(train['Embarked'])\ntest['Embarked'] = encoder.transform(test['Embarked'])\n    \ntrain.head(10)","299e7dc6":"muj = train['Sex'] == 'female'\nhom = train['Sex'] == 'male'\nPorceHom = hom.sum() \/ (hom.sum() + muj.sum()) * 100\nPorceMuj = muj.sum() \/ (hom.sum() + muj.sum()) * 100\ntotal = hom.sum() + muj.sum()\nprint('Total de pasajeros:', total,\n      '\\n\\nTotal de hombres:', hom.sum(),\n     '\\nTotal de mujeres:', muj.sum(),\n     '\\nPorcentaje de hombres: %0.2f' %PorceHom,\n     '\\nPorcentaje de mujeres: %0.2f' %PorceMuj)\n\nSup = (train['Survived'] == 1)\nprint('\\nToltal de supervivientes:',Sup.sum())\n\nSupHom = Sup & hom\nprint('Total de hombres supervivientes:', SupHom.sum(),\n     '\\nTotal de mujeres supervivientes:', Sup.sum() - SupHom.sum())\n\nPorceSup = (Sup.sum() \/ total) * 100\nPorceSupHom = (SupHom.sum() \/ Sup.sum()) * 100\nPorceSupMuj = ((Sup.sum() - SupHom.sum()) \/ Sup.sum()) * 100\nprint('Porcentaje de supervivientes:', PorceSup,\n     '\\nPorcentaje total de hombres supervivientes: %0.2f' %PorceSupHom,\n     '\\nPorcentaje total de mujeres supervivientes: %0.2f' %PorceSupMuj)\n\nPorceRelSupHom = (SupHom.sum() \/ hom.sum()) * 100\nPorceRelSupMuj = ((Sup.sum() - SupHom.sum()) \/ muj.sum()) * 100\nprint('Porcentaje de hombres supervivientes: %0.2f' %PorceRelSupHom,\n     '\\nPorcentaje de mujeres supervivientes: %0.2f' %PorceRelSupMuj)\n\nPrimeraClase = train['Pclass'] == 1\nSegundaClase = train['Pclass'] == 2\nTerceraClase = train['Pclass'] == 3\nprint('\\nTotal de pasajeros de primera clase:', PrimeraClase.sum(),\n     '\\nTotal de pasajeros de segunda clase:', SegundaClase.sum(),\n     '\\nTotal de pasajeros de tercera clase:', TerceraClase.sum())\n\nSupPrimCla = Sup & PrimeraClase\nSupSegCla = Sup & SegundaClase\nSupTerCla = Sup & TerceraClase\nprint('Supervivientes de primera clase:', SupPrimCla.sum(),\n     '\\nSupervivientes de segunda clase:', SupSegCla.sum(),\n     '\\nSupervivientes de tercera clase:', SupTerCla.sum())\nPorceSupPrimCla = (SupPrimCla.sum() \/ Sup.sum()) * 100\nPorceSupSegCla = (SupSegCla.sum() \/ Sup.sum()) * 100\nPorceSupTerCla = (SupTerCla.sum() \/ Sup.sum()) * 100\nprint('Porcentaje de supervivientes total de primera clase: %0.2f' %PorceSupPrimCla,\n     '\\nPorcentaje de supervivientes total de segunda clase: %0.2f' %PorceSupSegCla,\n     '\\nPorcentaje de supervivientes total de tercera clase: %0.2f' %PorceSupTerCla)\n\nPorceRelSupPrimCla = (SupPrimCla.sum() \/ PrimeraClase.sum()) * 100\nPorceRelSupSegCla = (SupSegCla.sum() \/ SegundaClase.sum()) * 100\nPorceRelSupTerCla = (SupTerCla.sum() \/ TerceraClase.sum()) * 100\nprint('Porcentaje de supervivientes de primera clase: %0.2f' %PorceRelSupPrimCla,\n     '\\nPorcentaje de supervivientes de segunda clase: %0.2f' %PorceRelSupSegCla,\n     '\\nPorcentaje de supervivientes de tercera clase: %0.2f' %PorceRelSupTerCla)\n\nCher = train['Embarked'] == 0\nQuee = train['Embarked'] == 1\nSout = train['Embarked'] == 2\nprint('\\nTotal de pasajeros que subieron en Cherbourg:', Cher.sum(),\n     '\\nTotal de pasajeros que subieron en Southampton:', Sout.sum(),\n     '\\nTotal de pasajeros que subieron en Queenstown:', Quee.sum())\n\nSupCher = Sup & Cher\nSupSout = Sup & Sout\nSupQuee = Sup & Quee\nprint('Supervivientes que subieron en Cherbourg:', SupCher.sum(),\n     '\\nSupervivientes que subieron en Southampton:', SupSout.sum(),\n     '\\nSupervivientes que subieron en Queenstown:', SupQuee.sum())\n\nPorceSupCher = (SupCher.sum() \/ Sup.sum()) * 100\nPorceSupSout = (SupSout.sum() \/ Sup.sum()) * 100\nPorceSupQuee = (SupQuee.sum() \/ Sup.sum()) * 100\nprint('Porcentaje total de supervivientes que subieron en Cherbourg: %0.2f' %PorceSupCher,\n     '\\nPorcentaje total de supervivientes que subieron en Southampton: %0.2f' %PorceSupSout,\n     '\\nPorcentaje total de supervivientes que subieron en Queenstown: %0.2f' %PorceSupQuee)\n\nPorceRelSupCher = (SupCher.sum() \/ Cher.sum()) * 100\nPorceRelSupSout = (SupSout.sum() \/ Sout.sum()) * 100\nPorceRelSupQuee = (SupQuee.sum() \/ Quee.sum()) * 100\nprint('Porcentaje de supervivientes que subieron en Cherbourg: %0.2f' %PorceRelSupCher,\n     '\\nPorcentaje de supervivientes que subieron en Southampton: %0.2f' %PorceRelSupSout,\n     '\\nPorcentaje de supervivientes que subieron en Queenstown: %0.2f' %PorceRelSupQuee)","680b6144":"plt.rcParams[\"figure.figsize\"] = (15, 8)\ntrain.drop(['PassengerId', 'Ticket', 'Name', 'Cabin'], axis = 1).hist()\nplt.show()","dccba35a":"plt.rcParams[\"figure.figsize\"] = (15, 15)\nsns.pairplot(train, hue = 'Survived',\n             vars = ['Pclass', 'Age', 'SibSp', 'Parch', 'Fare'],\n            kind = 'reg')","47dbeb33":"fig, ax = plt.subplots(figsize = (10, 8))\nsns.heatmap(train.corr(), annot = True, \n            fmt = \".2f\", \n            cmap = 'coolwarm',\n            cbar_kws = {\"shrink\": .8})","a41c8277":"sns.countplot(x = 'Sex',hue = 'Survived', \n              data = train)","2b9c51f7":"sns.countplot(x = 'Pclass',hue = 'Survived', \n              data = train)","4e41c65c":"fig = px.pie(train, values = 'Survived', \n             names = 'Pclass')\nfig.show()","6e1b1399":"fig = px.sunburst(train, path = ['Sex', 'Pclass'], \n                  values = 'Survived', \n                  color = 'Pclass')\nfig.show()","8cb20497":"fig, ax = plt.subplots(figsize=(15, 8))\nsns.set_theme(style=\"darkgrid\")\nsns.lineplot(x = \"Age\", y = \"Survived\",\n             data = train)","d45af402":"fig = px.histogram(train, \n                   x = 'Survived',\n                   color = 'Survived',\n                   title = 'Histograma de supervivientes')\n\nfig.update_layout(bargap = 0.2)\n\nfig.show()","7b433168":"fig = px.sunburst(train, path = ['Pclass', 'Embarked'], \n                  values = 'Survived', \n                  color = 'Pclass')\nfig.show()","a01c080d":"fig = px.sunburst(train, path = ['Embarked', 'Pclass'], \n                  values = 'Survived', \n                  color = 'Pclass')\nfig.show()","d2093484":"train = train.drop(['Cabin'], axis = 1)\ntest = test.drop(['Cabin'], axis = 1)","94612a65":"train_edad_media = train['Age'].mean()\ntrain_edad_std = train['Age'].std()\nprint('Edad media: %0.2f' %train_edad_media,\n     '\\nDesviacion estandar: %0.2f' %train_edad_std)","d94cc13f":"train['Age'] = train['Age'].fillna(value = random.uniform(20.05, 56.67))\nprint(train['Age'])","b20102a1":"train['Age'].isnull().sum()","9dd98584":"print(train['Age'].mean())","e716c122":"test_edad_media = test['Age'].mean()\ntest_edad_std = test['Age'].std()\nprint('Edad media: %0.2f' %test_edad_media,\n     '\\nDesviacion estandar: %0.2f' %test_edad_std)","7a48e123":"test['Age'] = test['Age'].fillna(value = random.uniform(16.52, 44.62))\nprint(test['Age'])","5df1d03e":"test['Age'].isnull().sum()","6236e220":"print(test['Age'].mean())","6ad6dd1a":"train = train.drop(['Ticket'], axis = 1)\ntest = test.drop(['Ticket', 'Name'], axis = 1)","004b8ee7":"train['Fare'].mean()","f8f2645b":"test['Fare'].mean()","b5ae5aff":"train['Fare'].fillna(0).mean()","8493abbc":"test['Fare'].fillna(0).mean()","d9629bf8":"train['Fare'] = train['Fare'].fillna(0)\ntest['Fare'] = test['Fare'].fillna(0)","769c239b":"#Aplicamos un codificador a la columna categorica para transformarla en valores del 0 al 1\nencoder = LabelEncoder()\nencoder.fit(train['Sex'])\ntrain['Sex'] = encoder.fit_transform(train['Sex'])\ntest['Sex'] = encoder.transform(test['Sex'])\n    \ntrain.head(10)","dbf4b2d2":"y = train['Survived']\nX = train.drop(['Survived', 'Name'], axis = 1)\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size = 0.2, random_state = 10)\nprint('Tama\u00f1o del conjunto de entrenamiento: ',X_train.shape,\n      '\\nTama\u00f1o del conjunto de validacion: ',X_val.shape)","a99b54c0":"X_train.head()","02e7047e":"model = linear_model.LinearRegression(n_jobs = -1)\nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_val)","abca83d5":"# print('error cuadratico medio:', mean_squared_error(y_train, y_pred))","9fda6b21":"model2 = Sequential([\n    keras.layers.BatchNormalization(input_shape = (8,)),\n    \n    keras.layers.Dense(256, activation = 'relu'),\n    \n    keras.layers.Dropout(rate = 0.3),\n    keras.layers.BatchNormalization(),\n    \n    keras.layers.Dense(256, activation = 'relu'),\n    \n    keras.layers.BatchNormalization(),\n    keras.layers.Dropout(rate = 0.3),\n    \n    keras.layers.Dense(256, activation = 'relu'),\n    \n    keras.layers.BatchNormalization(),\n    keras.layers.Dropout(rate = 0.3),\n    \n    keras.layers.Dense(1, activation = 'sigmoid')\n])\nmodel2.summary()","aa88e01e":"model2.compile(loss = 'binary_crossentropy',\n              optimizer = 'adam',\n              metrics = ['binary_accuracy'])","16570b2b":"early_stopping = callbacks.EarlyStopping(\n    min_delta = 0.001,\n    patience = 50,\n    restore_best_weights = True\n    )\n\nmodel_checkpoint = ModelCheckpoint('.\/best_model.hdf5',\n                                   monitor = 'val_loss', \n                                   mode = \"min\", verbose = 0, \n                                   save_best_model = True)","ea281eac":"history = model2.fit(\n    X_train,\n    y_train,\n    validation_data = (X_val, y_val),\n    batch_size = 512,\n    epochs = 500,\n    callbacks = [early_stopping, model_checkpoint],\n    verbose = 0\n    )","cd4b4134":"print(history.history.keys())","811f879d":"accuracy = history.history['binary_accuracy']\nval_accuracy = history.history['val_binary_accuracy']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs = len(accuracy)\n\nplt.figure()\n\nplt.plot(range(epochs), accuracy, 'b', label='Training accuracy')\nplt.plot(range(epochs), val_accuracy, 'r', label='Validation accuracy')\nplt.title('Training and validation accuracy')\n\nplt.figure()\n\nplt.plot(range(epochs), loss, 'b', label='Training Loss')\nplt.plot(range(epochs), val_loss, 'r', label='Training Loss')\nplt.title('Training and validation loss')\n\nplt.show()","93ef946c":"model2 = load_model(\".\/best_model.hdf5\")","73a868ab":"model2.evaluate(X_val, y_val, verbose = 1)","dc82e206":"model3 = lgb(max_depth=123,\n             n_estimators = 95,\n             n_jobs = -1,\n             num_leaves = 65)\n\nmodel3.fit(X_train, y_train)\n\npredict3 = model3.predict(X_val)\n\nprint(accuracy_score(y_val, predict3))","b3d213d8":"model4 = xgb(max_depth = 500,\n             n_stimators = 100,\n             n_jobs = -1)\n\nmodel4.fit(X_train, y_train)\n\npredict4 = model4.predict(X_val)\n\nprint(accuracy_score(y_val, predict4))","5ccd7eac":"test_pred = model2.predict(test)\ntest['Survived'] = test_pred\nsubmission = test[['PassengerId','Survived']]\nsubmission['Survived'] = np.where(test_pred > 0.5, 1, 0)\nsubmission.to_csv('submision.csv', index = False)","a51b1618":"!kaggle competitions submit -c tabular-playground-series-apr-2021 -f submission.csv -m \"Message\"","642800be":"Un mapa de calor nos muestra la correlacion entre las variables que vamos a tratar, si es una correlacion muy alta o muy baja debemos plantearnos el dejarla o no, por que puede indicar dependencia de la misma. En este caso, no encontramos dependencias directas de ninguna variable","403e6c45":"El siguiente valor que vamos a tratar es la edad ('age') de los pasajeros, en este caso nos enfrentamos a casi 3500 valores vacios en ambos conjuntos, como los datos entregados no siguen un orden de edad, creo que lo mejor es rellenar los datos con valores aleatorios que esten en el rango de la media de edad mas menos la desviacion estandar. Para esto se usar\u00e1 la funcion \"uniform(a, b)\", que devuelva valores aleatorios comprendidos entre los valores a y b","2e5aee07":"Grafica comparativa de los supervivientes segun el sexo, 0 no sobrevivio, 1 si lo hizo","aac74b3e":"La forma correcta de leer el grafico siguiente es, en el centro, la clase a la que pertenecian los pasajeros, en el exterior, el puerto de embarque y los supervivientes por puerto (estos se ven al poner el raton encima o haciendo click en una clase)","dd0fa148":"3- Light Gradient Boosting Machine","561a8ef9":"# **TABULAR PLAYGROUND ABRIL 2021**\n\nEn esta edicion del concurso, se nos pide usar una regresi\u00f3n lineal para evaluar, en un conjunto de datos sinteticos, cuantas personas habr\u00edan sobrevivido al hundimiento del titanic, que empez\u00f3 el 14 de abril de 1912 y acabo en la madrugada del dia siguiente.\n\nComo viene siendo costumbre hasta ahora, lo primero que haremos ser\u00e1 dividir el ejercicio en partes, primero,cargamos las librerias que vamos a usar y a\u00f1adiendo las que necesitemos a medida que vamos avanzando en el trabajo, despues cargamos los datos y hacemos una exploracion en ellos, una vez obtenida la informaci\u00f3n que consideramos importante y sacado conclusiones en base a ello, se limpian los datos para que el entrenamiento nos resulte lo mejor posible. Con los dartos limpios, entrenamos los modelos y subimos los datos con el mejor de ellos","e6fcfd46":"Grafico de lineas que nos devuelve el tanto por uno de los supervivientes por edad","e0b1a2e4":"Grafico de tarta interactivo segun la clase y el sexo","5797a333":"Cantidad de valores nulos de los datos","b9be4fa2":"En esta parte he querido mostrar ha sido los datos relativos y absolutos de los supervivientes en funcion de su clase, sexo y puerto de embarque","8fb6064c":"En las siguientes graficas vemos todos los valores numericos representados como histogramas, en ellos vemos que, en el conjunto de entrenamiento,murieron mas personas de las que se salvaron, tambien vemos que la clase mas comun era la tercera y que habia un ligero numero de personas mas en primera clase que en segunda. La terdcera grafica nos muestra la dstribuicion de edad en grupos de 10 a\u00f1os, con un pico en la veintena y un minimo entre la decada de los 70 a los 80. Tambien resalta mucho la cantidad de personas que subieron en Southamton","8198970f":"Aqui vemos una descripcion de los campos con los que estamos tratando","e1ea3158":"4- XGboost","0df5fc08":"El tercer caso que vamos a tratar con valores perdidos es el del 'Ticket', en este caso, aunque no falten muchos casos, el hecho que sea un campo alphanumerico me hace mas simple el desecharlo que el intentar tratarlo","04ff011f":"Estos datos nos muestran que se ten\u00eda mas posibilidades de sobrevivir siendo mujer de primera clase y embarcando en Cherbourg que siendo hombre, independientemente de la clase o del puerto de embarque","f4a7c178":"Por ultimo, nos falta ajustar la tarifa que pag\u00f3 cada pasajero. En cada uno  de los grupos hay poco mas de un 0.13% de datos faltantes, esto no resulta una cantidad lo suficentemente grande como para que nos cambie ucho el proceso, asi que lo primero que vamos a hacer es llenar esas casillas de ceros y ver como afecta a la media, si es muy significativo en cualquiera de los dos casos, habr\u00e1 que hacer algo similar al caso de la edad","8961330a":"Grafica comparativa de los supervivientes segun la clase, 0 no sobrevivio, 1 si lo hizo","a8d36c27":"Este grafico es el mismo que antes, solo se ha cambiado el orden","eb3fedbd":"Ahora, con pairplot, vemos como se realcionan las variables en los conjuntos de train y test mediante regresion lineal con los supervivientes como objetivo. Pairplot nos permite relacionar de dos en dos las distintas columnas y hacer con ello una regresion lineal simple en base al parametro hue ","561fa222":"Informacion del tipo de datos de los conjuntos de datos","ed910488":"Grafico de tarta segun los supervivientes de cada clase","bea366fe":"Antes de empezar a graficar nada es necesario que codifiquemos el puerto de embarque para poder pasarlo sin problema a las graficas","09782bc5":"2- RNA","7b02681f":"Vemos que en ambos grupos hay un campo, Cabin, que tiene una cantidad considerable de datos vacion (hasta un 70% del total), esto lo trataremos mas adelante","9586ac49":"**1- Cargamos las librerias**\n\npandas nos permite leer los datos del dataset\n\nnumpy es una libreria matematica\n\nseaborn, pyplot y graph_objects son librerias graficas, para hacer las graficas\n\ntrain_test_split nos permitir\u00e1 dividir el dataset entre entrenamiento y validaci\u00f3n\n\nmean_square_error y r2_score son metricas de medicion para evaluar el error\n\nLabelEmcoder nos permite convertir letras a valores categoricos\n\nrandom se presenta a si mismo, es para dar numeros aleatorios\n\nlinear_model crea modelos de regresion lineal\n\nkeras y tensorflow nos permiten hacer redes neuronales\n\n...","e178f8ca":"0: Cherbourg\n\n1: Queenstown\n\n2: Southampton","57f8ce9a":"Con esto tenemos una edad media de 38.36 con una maxima de 56.67 y una minima de 20.05","c932c450":"En un primer vistazo vemos que hay valores vacion (NaN), valores categoricos y campos alfanumericos, esto sucede tanto en el conjunto de entrenamiento como en el de prueba","2c60a339":"![image.png](attachment:image.png)![](http:\/\/)","383e120f":"Como se puede ver, el cambio es muy peque\u00f1o","6a43867c":"Ahora, antes de meter los datos en un algoritmo de regresion lineal, debemos tratarlos, limpiarlos y, en defnitiva, manipularlos para que estos nos puedan entregar informacion util sobre las posibilidades de supervivencia en el Titanic.\nSabemos que el campo 'cabin' tiene muchos valores perdidos (mas de un 65% en el conjunto de entrenamiento y un 70% en el de test), por ello es mejor eliminarlo que intentar arreglarlo","1c22bd4e":"**3- Limpieza de datos**","d2be7470":"**4- Comparacion de modelos**\n","180454ec":"**2- Cargamos los datos y los exploramos**","cff1b4f5":"1- Regresion lineal"}}