{"cell_type":{"fe19d5fe":"code","9e1a3a2f":"code","fbe5efb9":"code","16069bc1":"code","cd031bec":"code","092ff2ff":"code","fcaa77de":"code","25f3b69a":"code","f3355efb":"code","40e5d2bd":"code","35a5f9a0":"code","a839ed0b":"code","b8c2c157":"code","f7b8a3c0":"code","13b411e5":"code","c74ea3b9":"code","fd4a744d":"code","ad3aaa49":"code","ae01bfad":"code","61794e08":"code","008948ea":"code","b19c8a78":"code","86c5fed9":"code","baaa9794":"code","2feb4bb5":"code","01dce588":"code","d7adb01a":"code","fa5b3c32":"code","0840e554":"code","e6bdd9d6":"code","1ac5ad31":"code","742c6705":"code","b606618b":"code","389de6fb":"code","cb90bd16":"code","bfd161d3":"code","ccf1d826":"code","fffd3945":"code","731e1840":"code","eb93ac25":"code","dae16c77":"code","dea06489":"code","a2cefbc5":"code","eba000ae":"code","68ff5192":"code","874be38e":"code","b7ab622a":"code","08b097f6":"code","dfbdb774":"code","16930674":"code","cd5c5feb":"code","34497ad7":"code","786de234":"code","79549955":"code","83c1542a":"code","5ed1bb2a":"code","87191144":"code","dc28c53b":"code","055e0a3e":"code","12a4f417":"code","f7e91eaf":"code","38f610ff":"code","9ed2adaa":"code","0121a50e":"code","7d9753fd":"code","ff21631e":"code","e889153d":"markdown","da3ad1b7":"markdown","c02800d5":"markdown","3d1dd073":"markdown","736276cd":"markdown","099f5858":"markdown","f78586d9":"markdown","774ae445":"markdown","d4dae086":"markdown","078c077a":"markdown","c042f8a9":"markdown","64978d69":"markdown"},"source":{"fe19d5fe":"import numpy as np \nimport pandas as pd \nfrom datetime import datetime as dt\nfrom tqdm.notebook import tqdm\nimport itertools\n\nfrom scipy import stats\nimport matplotlib.pyplot as plt\n\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score, f1_score, roc_curve, auc\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom catboost import CatBoostClassifier\nimport lightgbm\nimport xgboost as xgb\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","9e1a3a2f":"train = pd.read_csv('\/kaggle\/input\/itea-goal-prediction\/goal_train.csv')\ntrain","fbe5efb9":"test = pd.read_csv('\/kaggle\/input\/itea-goal-prediction\/goal_test.csv')\ntest","16069bc1":"train.shape, test.shape","cd031bec":"# train target \ny = train['is_goal']\ntrain.drop(columns=['is_goal'], inplace=True)","092ff2ff":"train['is_test'] = 0\ntest['is_test'] = 1\ndf = pd.concat([train, test])\ndf.drop(columns=['matchId', 'firstName', 'middleName', 'lastName', 'currentTeamId', 'shortName', 'city', 'name', 'officialName', 'teamId', 'playerId'], inplace=True)\ndf","fcaa77de":"df['eventSec'].isna().mean(), (df['eventSec']==0).mean()","25f3b69a":"df['matchPeriod'].isna().mean(), (df['matchPeriod']==0).mean()","f3355efb":"df['eventSec'].hist();","40e5d2bd":"stat, p = stats.ttest_rel(df.loc[df['is_test']==0, 'eventSec'], y)\nif p > 0.05:\n    print('Variables eventSec and is_goal are probably independent')\nelse:\n    print('Variables eventSec and is_goal are probably dependent')","35a5f9a0":"# additional time in the first half\ndf.loc[((df['is_test']==0) & (df['matchPeriod']=='1H') & (df['eventSec']>=2700)), 'eventSec'].hist();","a839ed0b":"first_half_duration = df.loc[((df['is_test']==0) & (df['matchPeriod']=='1H') & (df['eventSec']>=2700)), 'eventSec'].median()\nfirst_half_duration","b8c2c157":"df['absEventSec'] = df['eventSec']\ndf.loc[df['matchPeriod']=='2H', 'absEventSec'] += first_half_duration\nstat, p = stats.ttest_rel(df.loc[df['is_test']==0, 'absEventSec'], y)\nif p > 0.05:\n    print('Variables absEventSec and is_goal are probably independent')\nelse:\n    print('Variables absEventSec and is_goal are probably dependent')","f7b8a3c0":"df.loc[(df['is_test']==0), 'matchPeriod'].hist();","13b411e5":"df['matchPeriod'] = (df['matchPeriod'] == '2H').astype(int)\ncontingency_table = pd.concat([df.loc[df['is_test']==0, ['matchPeriod']], y], axis=1).groupby(['matchPeriod', 'is_goal']).size().unstack(level=0)\nstat, p, dof, expected = stats.chi2_contingency(contingency_table)\nif p > 0.05:\n    print('Variables matchPeriod and is_goal are probably independent')\nelse:\n    print('Variables matchPeriod and is_goal are probably dependent')\ncontingency_table","c74ea3b9":"df['x_1'].isna().mean(), df['y_1'].isna().mean()","fd4a744d":"df.plot.scatter(x = 'x_1', y = 'y_1')","ad3aaa49":"df[\"dist\"] = np.sqrt(np.power(df['x_1'] - 100, 2) + np.power(df['y_1'] - 50, 2))\ndf['angle'] = np.degrees(np.arccos(np.subtract(100, df['x_1']) \/ df[\"dist\"]))\ndf['wing'] = (df['y_1'] <= 50).astype(int)\ndf['dist_to_center'] = (df['y_1'] - 50).abs()\ndf","ae01bfad":"for var in ['dist', 'angle', 'y_1', 'x_1', 'dist_to_center']:\n    stat, p = stats.ttest_rel(df.loc[df['is_test']==0, var], y)\n    if p > 0.05:\n        print('Variables', var, 'and is_goal are probably independent')\n    else:\n        print('Variables', var, 'and is_goal are probably dependent')\n","61794e08":"contingency_table = pd.concat([df.loc[df['is_test']==0, ['wing']], y], axis=1).groupby(['wing', 'is_goal']).size().unstack(level=0)\nstat, p, dof, expected = stats.chi2_contingency(contingency_table)\nif p > 0.05:\n    print('Variables wing and is_goal are probably independent')\nelse:\n    print('Variables wing and is_goal are probably dependent')\ncontingency_table","008948ea":"df['is_CA'].isna().mean()","b19c8a78":"contingency_table = pd.concat([df.loc[df['is_test']==0, ['is_CA']], y], axis=1).groupby(['is_CA', 'is_goal']).size().unstack(level=0)\nstat, p, dof, expected = stats.chi2_contingency(contingency_table)\nif p > 0.05:\n    print('Variables is_CA and is_goal are probably independent')\nelse:\n    print('Variables is_CA and is_goal are probably dependent')\ncontingency_table","86c5fed9":"df['body_part'].isna().mean(), df['foot'].isna().mean(), df['wing'].isna().mean()","baaa9794":"df['foot'].value_counts()","2feb4bb5":"df['foot'].fillna('right', inplace=True)\ndf['foot'].value_counts(), df['foot'].isna().mean()","01dce588":"df.loc[df['body_part']=='head\/body', 'body_part'] = 0\ndf.loc[df['body_part']=='left', \"body_part\"] = 1\ndf.loc[df['body_part']=='right', \"body_part\"] = 2\ndf.loc[df['foot']=='both', 'foot'] = 0\ndf.loc[df['foot']=='left', 'foot'] = 1\ndf.loc[df['foot']=='right', 'foot'] = 2","d7adb01a":"contingency_table = pd.concat([df.loc[df['is_test']==0, ['body_part']], y], axis=1).groupby(['body_part', 'is_goal']).size().unstack(level=0)\nif p > 0.05:\n    print('Variables body_part and is_goal are probably independent')\nelse:\n    print('Variables body_part and is_goal are probably dependent')\ncontingency_table","fa5b3c32":"contingency_table = pd.concat([df.loc[df['is_test']==0, ['foot']], y], axis=1).groupby(['foot', 'is_goal']).size().unstack(level=0)\nif p > 0.05:\n    print('Variables foot and is_goal are probably independent')\nelse:\n    print('Variables foot and is_goal are probably dependent')\ncontingency_table","0840e554":"left_wing = df['wing'] == 0\nright_wing = df['wing'] == 1\ndf['wing'] = 0\ndf.loc[(left_wing) & df['body_part']==1, 'wing'] = 1\ndf.loc[(right_wing) & df['body_part']==2, 'wing'] = 1\ndf.loc[df['body_part']==0, 'wing'] = 1\n\ncontingency_table = pd.concat([df.loc[df['is_test']==0, ['wing']], y], axis=1).groupby(['wing', 'is_goal']).size().unstack(level=0)\nstat, p, dof, expected = stats.chi2_contingency(contingency_table)\nif p > 0.05:\n    print('Variables wing and is_goal are probably independent')\nelse:\n    print('Variables wing and is_goal are probably dependent')\ncontingency_table","e6bdd9d6":"df['common_body_part'] = 0\ndf.loc[df['body_part'] == df['foot'], 'common_body_part'] = 1\ndf.loc[df['body_part'] == 0, 'common_body_part'] = 1\ndf.loc[df['foot'] == 0, 'common_body_part'] = 1\n\ncontingency_table = pd.concat([df.loc[df['is_test']==0, ['common_body_part']], y], axis=1).groupby(['common_body_part', 'is_goal']).size().unstack(level=0)\nstat, p, dof, expected = stats.chi2_contingency(contingency_table)\nif p > 0.05:\n    print('Variables common_body_part and is_goal are probably independent')\nelse:\n    print('Variables common_body_part and is_goal are probably dependent')\ncontingency_table","1ac5ad31":"df['league'].isna().mean(), df['passportArea'].isna().mean(), df['birthArea'].isna().mean(), df['area'].isna().mean()","742c6705":"df['passportArea'].fillna('', inplace=True)\ndf['birthArea'].fillna('', inplace=True)","b606618b":"df.loc[df['league']=='IT', \"league\"] = 'Italy'\ndf.loc[df['league']=='SP', \"league\"] = 'Spain'\ndf.loc[df['league']=='GE', \"league\"] = 'Germany'\ndf.loc[df['league']=='FR', \"league\"] = 'France'\ndf.loc[df['league']=='EN', \"league\"] = 'England'\ndf[\"legionnaire\"] = 1\ndf.loc[df['league']==df['birthArea'], 'legionnaire'] = 0\n\ncontingency_table = pd.concat([df.loc[df['is_test']==0, ['legionnaire']], y], axis=1).groupby(['legionnaire', 'is_goal']).size().unstack(level=0)\nstat, p, dof, expected = stats.chi2_contingency(contingency_table)\nif p > 0.05:\n    print('Variables legionnaire and is_goal are probably independent')\nelse:\n    print('Variables legionnaire and is_goal are probably dependent')\ncontingency_table","389de6fb":"contingency_table = pd.concat([df.loc[df['is_test']==0, ['birthArea']], y], axis=1).groupby(['birthArea', 'is_goal']).size().unstack(level=0).fillna(0)\nstat, p, dof, expected = stats.chi2_contingency(contingency_table)\nif p > 0.05:\n    print('Variables birthArea and is_goal are probably independent')\nelse:\n    print('Variables birthArea and is_goal are probably dependent')\ncontingency_table","cb90bd16":"df['emigrant'] = 0\ndf.loc[df['birthArea']!=df['passportArea'], \"emigrant\"] = 1\n\ncontingency_table = pd.concat([df.loc[df['is_test']==0, ['emigrant']], y], axis=1).groupby(['emigrant', 'is_goal']).size().unstack(level=0).fillna(0)\nstat, p, dof, expected = stats.chi2_contingency(contingency_table)\nif p > 0.05:\n    print('Variables emigrant and is_goal are probably independent')\nelse:\n    print('Variables emigrant and is_goal are probably dependent')\ncontingency_table","bfd161d3":"df.drop(columns=['league', 'passportArea', 'area', 'emigrant'], inplace=True)\ndf['birthArea'] = df['birthArea'].astype('category').cat.codes","ccf1d826":"df['weight'].isna().mean(), (df['weight']==0).mean(), df['height'].isna().mean(), (df['height']==0).mean()","fffd3945":"df.loc[df['weight'].isna(), 'weight'] = df['weight'].mean()\ndf.loc[df['height'].isna(), 'height'] = df['height'].mean()\ndf.loc[df['weight']==0, 'weight'] = df['weight'].mean()\ndf.loc[df['height']==0, 'height'] = df['height'].mean()","731e1840":"df['bmi'] = df['weight'] \/ np.power(df['height'] \/ 100, 2)","eb93ac25":"for var in ['weight', 'height', 'bmi']:\n    stat, p = stats.ttest_rel(df.loc[df['is_test']==0, var], y)\n    if p > 0.05:\n        print('Variables', var, 'and is_goal are probably independent')\n    else:\n        print('Variables', var, 'and is_goal are probably dependent')","dae16c77":"df['birthDate'].isna().mean(), (df['birthDate']=='').mean()","dea06489":"df['age'] = (((pd.to_datetime(df['birthDate']) - dt.strptime('2020-09-09', '%Y-%m-%d'))*(-1)).dt.days)\/365.25\ndf.drop(columns=['birthDate'], inplace=True)\ndf['age'].fillna(df['age'].mean(), inplace=True)","a2cefbc5":"stat, p = stats.ttest_rel(df.loc[df['is_test']==0, 'age'], y)\nif p > 0.05:\n    print('Variables age and is_goal are probably independent')\nelse:\n    print('Variables age and is_goal are probably dependent')","eba000ae":"df['role'].isna().mean(), (df['role']=='').mean()","68ff5192":"df['role'].value_counts()","874be38e":"df['role'].fillna(\"Forward\", inplace=True)\ndf.loc[df['role']=='Goalkeeper', 'role'] = 0\ndf.loc[df['role']=='Defender', 'role'] = 1\ndf.loc[df['role']=='Midfielder', 'role'] = 2\ndf.loc[df['role']=='Forward', 'role'] = 3","b7ab622a":"stat, p = stats.spearmanr(df.loc[df['is_test']==0, 'role'], y)\nif p > 0.05:\n    print('Variables role and is_goal are probably independent')\nelse:\n    print('Variables role and is_goal are probably dependent')","08b097f6":"df['currentNationalTeamId'].isna().mean()","dfbdb774":"df['currentNationalTeamId'].fillna(0, inplace=True)","16930674":"contingency_table = pd.concat([df.loc[df['is_test']==0, ['currentNationalTeamId']], y], axis=1).groupby(['currentNationalTeamId', 'is_goal']).size().unstack(level=0).fillna(0)\nstat, p, dof, expected = stats.chi2_contingency(contingency_table)\nif p > 0.05:\n    print('Variables currentNationalTeamId and is_goal are probably independent')\nelse:\n    print('Variables currentNationalTeamId and is_goal are probably dependent')\ncontingency_table","cd5c5feb":"df['body_part'] = df['body_part'].astype('category')\ndf['foot'] = df['foot'].astype('category')\ndf['role'] = df['role'].astype('category')\ndf.info()","34497ad7":"# create masks list for cv with undersampling\ntrain = df.loc[df['is_test']==0].drop(columns=['is_test', 'shot_id'])\ntest = df.loc[df['is_test']==1].drop(columns=['is_test', 'shot_id'])\n\n\nvalidation_size = np.floor(train.shape[0]\/5).astype(int)\n\n#create 5 validation masks\nvalidation_masks = []\nalready_used_indexes = pd.DataFrame(np.zeros(train.shape[0]))\nwhile already_used_indexes.loc[already_used_indexes[0]==0].shape[0] > validation_size:\n    mask = already_used_indexes.loc[already_used_indexes[0]==0].sample(validation_size, random_state=444).index.sort_values()\n    already_used_indexes.loc[mask, 0] = 1\n    validation_masks.append(mask)\n\n    \n# create undersampling masks for each validation\n\nmasks = []\nfor validation_mask in validation_masks:\n    already_used_indexes = pd.DataFrame(np.zeros(train.shape[0]))\n    already_used_indexes.loc[validation_mask] = 1\n    positive_indexes = y[((already_used_indexes == 0)[0]) & (y == 1)].index\n    positive_size = positive_indexes.shape[0]\n    already_used_indexes.loc[positive_indexes, 0] = 1\n    \n    while already_used_indexes.loc[already_used_indexes[0]==0].shape[0] > positive_size:\n        mask = already_used_indexes.loc[already_used_indexes[0]==0].sample(positive_size, random_state=444).index.sort_values()\n        already_used_indexes.loc[mask, 0] = 1\n        curr_mask = {}\n        curr_mask['train'] = positive_indexes | mask\n        curr_mask['validation'] = validation_mask\n        masks.append(curr_mask)\n","786de234":"# Random Forest learning\nmax_depthes = [4, 7, 10, 13, 17, 21]\nmax_featuress = [0.2, 0.4, 0.6, 0.8, 1]\nmin_samples_leafs = [15, 30, 45, 60, 75, 90]\n\nmax_depthes = [5, 6, 7, 8, 9]\nmax_featuress = [0.6]\nmin_samples_leafs = [20, 25, 30, 35, 40]\n\nmax_depthes = [6]\nmax_featuress = [0.6]\nmin_samples_leafs = [21, 23, 25, 27, 29]\n\nmax_depthes = [6]\nmax_featuress = [0.6]\nmin_samples_leafs = [24, 25, 26]\n\nmax_depthes = [6]\nmax_featuress = [0.45, 0.5, 0.55, 0.6, 0.65, 0.7, 0.75]\nmin_samples_leafs = [26]\n\nmax_depthes = [6]\nmax_featuress = [0.56, 0.58, 0.6, 0.62, 0.64]\nmin_samples_leafs = [26]\n\nmax_depthes = [6]\nmax_featuress = [0.6]\nmin_samples_leafs = [26]\n\ncombinations = list(itertools.product(max_depthes, max_featuress, min_samples_leafs))\n\ncombinations\n\n\nbest_score = 0\nfor params in tqdm(combinations):\n    print(params)\n    max_depth, max_features, min_samples_leaf = params\n    scores = []\n    for mask in masks:\n        curr_x_train = train.loc[mask['train']]\n        curr_y_train = y.loc[mask['train']]\n        curr_x_validation = train.loc[mask['validation']]\n        curr_y_validation = y.loc[mask['validation']]\n\n        rf_clf = RandomForestClassifier(random_state=444, criterion=\"gini\", max_depth=max_depth,\n                                        max_features=max_features, min_samples_leaf=min_samples_leaf, n_estimators=1000)\n        rf_clf.fit(curr_x_train, curr_y_train)\n        y_pred_rf = rf_clf.predict_proba(curr_x_validation)[:,1]\n        scores.append(roc_auc_score(curr_y_validation, y_pred_rf))\n    print('Random Forest AUC score ', np.mean(scores))\n    if np.mean(scores) > best_score:\n        best_score = np.mean(scores)\n        best_params = params\nprint('best_score', best_score)\nprint('best_params', best_params)","79549955":"# Catboost learning\n\nscores = []\nfor mask in masks:\n    curr_x_train = train.loc[mask['train']]\n    curr_y_train = y.loc[mask['train']]\n    curr_x_validation = train.loc[mask['validation']]\n    curr_y_validation = y.loc[mask['validation']]\n    cb_clf = CatBoostClassifier(\n        eval_metric='AUC',\n        silent = True,\n        random_seed=444,\n        iterations=1000,\n        cat_features=['body_part', 'foot', 'role']\n    )\n    cb_model = cb_clf.fit(curr_x_train, curr_y_train)\n    y_pred_cb = cb_model.predict_proba(curr_x_validation)[:,1]\n    scores.append(roc_auc_score(curr_y_validation, y_pred_cb))\n\nprint('Catboost AUC score ', np.mean(scores))","83c1542a":"# # LightGBM learning\nnum_leaves = [5, 10, 25, 50, 100]\nlearning_rate = [0.1]\nlambda_l1 = [0]\nlambda_l2 = [0]\nfeature_fraction = [1]\nbagging_fraction = [1]\nbagging_freq = [0]\n\nnum_leaves = [4]\nlearning_rate = [0.01, 0.05, 0.1, 0.25, 0.5, 1]\nlearning_rate = np.arange(0.06, 0.25, 0.01)\nlearning_rate = [0.07]\nlambda_l1 = [0, 0.1, 0.5, 2, 5]\nlambda_l2 = [0, 0.1, 0.5, 2, 5]\nlambda_l1 = np.round(np.arange(0.2, 2, 0.2), 2)\nlambda_l2 = np.round(np.arange(0.6, 5, 0.5), 2)\nlambda_l1 = np.round(np.arange(0.15, 0.4, 0.05), 2)\nlambda_l2 = np.round(np.arange(2.15, 3.1, 0.05), 2)\nlambda_l1 = [0.2]\nlambda_l2 = [2.6]\nfeature_fraction = [1]\nfeature_fraction = np.round(np.arange(0.1, 1.05, 0.05), 2)\nfeature_fraction = [1]\nbagging_fraction = [1]\nbagging_freq = [0]\nbagging_fraction = np.round(np.arange(0.1, 1.05, 0.05), 2)\nbagging_freq = [0, 5, 10, 25, 50]\nbagging_fraction = [1]\nbagging_freq = [0]\n\ncombinations = list(itertools.product(num_leaves, learning_rate, lambda_l1, lambda_l2, feature_fraction, bagging_fraction, bagging_freq))\n\n\nbest_score = 0\nfor params in tqdm(combinations):\n    print(params)\n    num_leaves, learning_rate, lambda_l1, lambda_l2, feature_fraction, bagging_fraction, bagging_freq = params\n\n    parameters = {\n        'objective': 'binary',\n        'metric': 'auc',\n        'is_unbalance': 'true',\n        'boosting': 'gbdt',\n        'verbosity': 1,\n        'num_leaves': num_leaves,\n        'lambda_l1': lambda_l1,\n        'lambda_l2': lambda_l2,\n        'feature_fraction': feature_fraction,\n        'learning_rate': learning_rate,\n        'bagging_fraction': bagging_fraction,\n        \"bagging_freq\": bagging_freq,\n#         \"cat_feature\": ['name:body_part', 'name:foot', 'name:role'],\n        'iterations': 1000\n    }\n\n    scores = []\n    for mask in masks:\n        \n        curr_x_train = train.loc[mask['train']]\n        curr_y_train = y.loc[mask['train']]\n        curr_x_validation = train.loc[mask['validation']]\n        curr_y_validation = y.loc[mask['validation']]\n        \n        \n        curr_train_data = lightgbm.Dataset(curr_x_train, label=curr_y_train)\n        lgb_model = lightgbm.train(parameters,\n                               curr_train_data,\n                               verbose_eval=False\n                            )\n        y_predict_lgb = lgb_model.predict(curr_x_validation)\n        scores.append(roc_auc_score(curr_y_validation, y_predict_lgb))    \n    print('LightGBM AUC score ', np.mean(scores))\n    if np.mean(scores) > best_score:\n        best_score = np.mean(scores)\n        best_params = params\n        \nprint('best_score', best_score)\nprint('best_params', best_params)\n# train","5ed1bb2a":"#xgboost one hot encoding\ntrain_xgboost = pd.get_dummies(train, columns=['body_part', 'foot', 'role'])\ntest_xgboost = pd.get_dummies(test, columns=['body_part', 'foot', 'role'])\ntrain_xgboost","87191144":"# xgboost\n\nmax_depth = [6]\nlearning_rate = [0.3]\nmin_child_weight = [1]\nreg_alpha = [0]\nreg_lambda = [1]\nsubsample = [1]\nn_estimators = [100]\n\n\nmax_depth = [6, 10, 15, 25, 40, 60, 100]\nmax_depth = [2, 4, 6, 8]\nmax_depth = [1, 2, 3]\nmax_depth = [1]\nlearning_rate = [0.3]\nlearning_rate = [0.01, 0.05, 0.1, 0.25, 0.5, 1]\nlearning_rate = np.round(np.arange(0.15, 0.5, 0.05), 2)\nlearning_rate = np.round(np.arange(0.16, 0.25, 0.01), 2)\nlearning_rate = [0.2]\nmin_child_weight = [1]\nmin_child_weight = [0.1, 0.5, 1, 2.5, 5, 10]\nmin_child_weight = np.round(np.arange(3, 10, 0.5), 2)\nmin_child_weight = np.round(np.arange(3, 4, 0.1), 2)\nmin_child_weight = np.round(np.arange(3.41, 3.6, 0.01), 2)\nmin_child_weight = [3.54]\nreg_alpha = [0]\nreg_lambda = [1]\nreg_alpha = [0, 0.1, 0.5, 2, 5]\nreg_lambda = [0, 0.1, 0.5, 2, 5]\nreg_alpha = np.round(np.arange(0.2, 2, 0.2), 2)\nreg_lambda = np.round(np.arange(0.2, 2, 0.2), 2)\nreg_alpha = np.round(np.arange(0.25, 0.6, 0.05), 2)\nreg_lambda = np.round(np.arange(0.25, 0.6, 0.05), 2)\nreg_alpha = np.round(np.arange(0.21, 0.3, 0.01), 2)\nreg_lambda = np.round(np.arange(0.51, 0.6, 0.01), 2)\nreg_alpha = [0.27]\nreg_lambda = [0.52]\nsubsample = [1]\nsubsample = [0.25, 0.5, 0.75, 1]\nsubsample = np.round(np.arange(0.8, 1.05, 0.05), 2)\nsubsample = np.round(np.arange(0.96, 1.005, 0.01), 2)\nsubsample = [1]\nn_estimators = [100]\nn_estimators = [100, 1000]\nn_estimators = [100]\n\n\ncombinations = list(itertools.product(max_depth, learning_rate, min_child_weight, reg_alpha, reg_lambda, subsample, n_estimators))\nbest_score = 0\nfor params in tqdm(combinations):\n    print(params)\n    max_depth, learning_rate, min_child_weight, reg_alpha, reg_lambda, subsample, n_estimators = params\n    xg_class = xgb.XGBClassifier(\n        learning_rate=learning_rate, \n        max_depth=max_depth,\n        min_child_weight=min_child_weight, \n        missing=None, \n        n_estimators=n_estimators,\n        nthread=-1,\n        objective='binary:logistic',\n        reg_alpha=reg_alpha,\n        reg_lambda = reg_lambda,\n        seed=444,\n        verbosity=0, \n        subsample=subsample)\n\n    scores = []\n    for mask in masks:\n        curr_x_train = train_xgboost.loc[mask['train']]\n        curr_y_train = y.loc[mask['train']]\n        curr_x_validation = train_xgboost.loc[mask['validation']]\n        curr_y_validation = y.loc[mask['validation']]\n        \n        xg_fit=xg_class.fit(curr_x_train, curr_y_train)\n        predict = xg_class.predict_proba(curr_x_validation)[:,1]\n        scores.append(roc_auc_score(y_score=predict, y_true=curr_y_validation))\n    print('XGBoost AUC score ', np.mean(scores))\n    if np.mean(scores) > best_score:\n        best_score = np.mean(scores)\n        best_params = params\n        \nprint('best_score', best_score)\nprint('best_params', best_params)\n    \n    \n","dc28c53b":"# Evaluate optimal coeffs for blending and optimal cfs evaluation\n\n# rf_predicts = []\n# cb_predicts = []\n# lgbm_predicts = []\n# xg_predicts = []\n# y_validations = []\n# for mask in tqdm(masks):\n#     curr_x_train = train.loc[mask['train']]\n#     curr_y_train = y.loc[mask['train']]\n#     curr_x_validation = train.loc[mask['validation']]\n#     curr_y_validation = y.loc[mask['validation']]\n    \n#     y_validations.append(curr_y_validation)\n    \n#     #Random Forest\n#     max_depthes = 6\n#     max_featuress = 0.6\n#     min_samples_leafs = 26\n#     rf_clf = RandomForestClassifier(random_state=444, criterion=\"gini\", max_depth=max_depth,\n#                                     max_features=max_features, min_samples_leaf=min_samples_leaf, n_estimators=1000)\n#     rf_clf.fit(curr_x_train, curr_y_train)\n#     y_pred_rf = rf_clf.predict_proba(curr_x_validation)[:,1]\n#     rf_predicts.append(y_pred_rf)\n    \n\n#     #CatBoost\n#     cb_clf = CatBoostClassifier(\n#         eval_metric='AUC',\n#         silent = True,\n#         random_seed=444,\n#         iterations=1000,\n#         cat_features=['body_part', 'foot', 'role']\n#     )\n#     cb_model = cb_clf.fit(curr_x_train, curr_y_train)\n#     y_pred_cb = cb_model.predict_proba(curr_x_validation)[:,1]\n#     cb_predicts.append(y_pred_cb)\n    \n\n#     #lightGBM\n#     num_leaves = 4\n#     learning_rate = 0.07\n#     lambda_l1 = 0.2\n#     lambda_l2 = 2.6\n#     feature_fraction = 1\n#     bagging_fraction = 1\n#     bagging_freq = 0\n       \n#     parameters = {\n#         'objective': 'binary',\n#         'metric': 'auc',\n#         'is_unbalance': 'true',\n#         'boosting': 'gbdt',\n#         'verbosity': 1,\n#         'num_leaves': num_leaves,\n#         'lambda_l1': lambda_l1,\n#         'lambda_l2': lambda_l2,\n#         'feature_fraction': feature_fraction,\n#         'learning_rate': learning_rate,\n#         'bagging_fraction': bagging_fraction,\n#         \"bagging_freq\": bagging_freq,\n#         'iterations': 1000\n#     }\n#     curr_train_data = lightgbm.Dataset(curr_x_train, label=curr_y_train)\n#     lgb_model = lightgbm.train(parameters,\n#                            curr_train_data,\n#                            verbose_eval=False\n#                         )\n#     y_predict_lgb = lgb_model.predict(curr_x_validation)\n#     lgbm_predicts.append(y_predict_lgb) \n    \n    \n#     #XGBoost\n#     max_depth = 1\n#     learning_rate = 0.2\n#     min_child_weight = 3.54\n#     reg_alpha = 0.27\n#     reg_lambda = 0.52\n#     subsample = 1\n#     n_estimators = 100\n    \n#     xg_class = xgb.XGBClassifier(\n#         learning_rate=learning_rate, \n#         max_depth=max_depth,\n#         min_child_weight=min_child_weight, \n#         missing=None, \n#         n_estimators=n_estimators,\n#         nthread=-1,\n#         objective='binary:logistic',\n#         reg_alpha=reg_alpha,\n#         reg_lambda = reg_lambda,\n#         seed=444,\n#         verbosity=0, \n#         subsample=subsample)\n    \n#     curr_x_train = train_xgboost.loc[mask['train']]\n#     curr_x_validation = train_xgboost.loc[mask['validation']]\n#     xg_fit=xg_class.fit(curr_x_train, curr_y_train)\n#     xg_predict = xg_class.predict_proba(curr_x_validation)[:,1]\n#     xg_predicts.append(xg_predict)\n    \n    ","055e0a3e":"# print(rf_predicts, cb_predicts, lgbm_predicts, xg_predicts, y_validations)\n\n\n\n\n# best_score = 0\n# for rf_cf in tqdm(np.arange(0, 1.01, 0.01)):\n#     for cb_cf in np.arange(0, 1.01 - rf_cf, 0.01):\n#         for lgbm_cf in np.arange(0, 1.01 - rf_cf - cb_cf, 0.01):\n#             xgb_cf = 1 - rf_cf - cb_cf - lgbm_cf\n#             scores = []\n#             for i in range(len(rf_predicts)):\n#                 rf_pred = rf_predicts[i]\n#                 cb_pred = cb_predicts[i]\n#                 lgbm_pred = lgbm_predicts[i]\n#                 xg_pred = xg_predicts[i]\n                \n#                 weighted_pred = rf_cf * rf_pred + cb_cf * cb_pred + lgbm_cf * lgbm_pred + xgb_cf * xg_pred #vector of weighted predicts\n#                 scores.append(roc_auc_score(y_validations[i], weighted_pred))\n\n#             if np.mean(scores) > best_score:\n#                 best_score = np.mean(scores)\n#                 best_cfs = rf_cf, cb_cf, lgbm_cf, xgb_cf\n# print('rf_cf', \"cb_cf\", \"lgbm_cf\", 'xgb_cf')\n# print(best_cfs)\n\n\n\n\n# rf_cf  cb_cf  lgbm_cf  xgb_cf\n# 0.0,   0.25,  0.51,    0.24","12a4f417":"# evaluate optimal tresholds\n# Random Forest\n# best_score = 0\n# for treshold in tqdm(np.arange(0.005, 1.001, 0.005)):\n#     scores = []\n#     for i in range(len(rf_predicts)):\n#         scores.append(f1_score(y_validations[i], rf_predicts[i]>treshold))    \n\n#     print('th ', treshold, 'score ', np.mean(scores))\n#     if np.mean(scores) > best_score:\n#         best_score = np.mean(scores)\n#         best_th = treshold\n# print('best_score', best_score)\n# print('best_th', best_th)\n\n# 0.595","f7e91eaf":"# evaluate optimal tresholds\n# CatBoost\n# best_score = 0\n# for treshold in tqdm(np.arange(0.005, 1.004, 0.005)):\n#     scores = []\n#     for i in range(len(cb_predicts)):\n#         scores.append(f1_score(y_validations[i], cb_predicts[i]>treshold))    \n\n#     print('th ', treshold, 'score ', np.mean(scores))\n#     if np.mean(scores) > best_score:\n#         best_score = np.mean(scores)\n#         best_th = treshold\n# print('best_score', best_score)\n# print('best_th', best_th)\n\n# 0.61","38f610ff":"# evaluate optimal tresholds\n# lightGBM\n# best_score = 0\n# for treshold in tqdm(np.arange(0.005, 1.004, 0.005)):\n#     scores = []\n#     for i in range(len(lgbm_predicts)):\n#         scores.append(f1_score(y_validations[i], lgbm_predicts[i]>treshold))    \n\n#     print('th ', treshold, 'score ', np.mean(scores))\n#     if np.mean(scores) > best_score:\n#         best_score = np.mean(scores)\n#         best_th = treshold\n# print('best_score', best_score)\n# print('best_th', best_th)\n\n# 0.605","9ed2adaa":"# evaluate optimal tresholds\n# XGBoost\n# best_score = 0\n# for treshold in tqdm(np.arange(0.005, 1.004, 0.005)):\n#     scores = []\n#     for i in range(len(xg_predicts)):\n#         scores.append(f1_score(y_validations[i], xg_predicts[i]>treshold))    \n\n#     print('th ', treshold, 'score ', np.mean(scores))\n#     if np.mean(scores) > best_score:\n#         best_score = np.mean(scores)\n#         best_th = treshold\n# print('best_score', best_score)\n# print('best_th', best_th)\n# 0.605","0121a50e":"# Final predicts\n\n# rf_predicts = []\ncb_predicts = []\nlgbm_predicts = []\nxg_predicts = []\n\nfor mask in tqdm(masks):\n    curr_x_train = train.loc[mask['train']]\n    curr_y_train = y.loc[mask['train']]\n    \n    #Random Forest\n#     max_depthes = 6\n#     max_featuress = 0.6\n#     min_samples_leafs = 26\n#     rf_clf = RandomForestClassifier(random_state=444, criterion=\"gini\", max_depth=max_depth,\n#                                     max_features=max_features, min_samples_leaf=min_samples_leaf, n_estimators=1000)\n#     rf_clf.fit(curr_x_train, curr_y_train)\n#     y_pred_rf = rf_clf.predict_proba(curr_x_validation)[:,1]\n#     rf_predicts.append(y_pred_rf)\n    \n\n    #CatBoost\n    cb_clf = CatBoostClassifier(\n        eval_metric='AUC',\n        silent = True,\n        random_seed=444,\n        iterations=1000,\n        cat_features=['body_part', 'foot', 'role']\n    )\n    cb_model = cb_clf.fit(curr_x_train, curr_y_train)\n    y_pred_cb = cb_model.predict_proba(test)[:,1]\n    cb_predicts.append(y_pred_cb)\n    \n\n    #lightGBM\n    num_leaves = 4\n    learning_rate = 0.07\n    lambda_l1 = 0.2\n    lambda_l2 = 2.6\n    feature_fraction = 1\n    bagging_fraction = 1\n    bagging_freq = 0\n       \n    parameters = {\n        'objective': 'binary',\n        'metric': 'auc',\n        'is_unbalance': 'true',\n        'boosting': 'gbdt',\n        'verbosity': 1,\n        'num_leaves': num_leaves,\n        'lambda_l1': lambda_l1,\n        'lambda_l2': lambda_l2,\n        'feature_fraction': feature_fraction,\n        'learning_rate': learning_rate,\n        'bagging_fraction': bagging_fraction,\n        \"bagging_freq\": bagging_freq,\n#         \"cat_feature\": ['name:body_part', 'name:foot', 'name:role'],\n        'iterations': 1000\n    }\n    curr_train_data = lightgbm.Dataset(curr_x_train, label=curr_y_train)\n    lgb_model = lightgbm.train(parameters,\n                           curr_train_data,\n                           verbose_eval=False\n                        )\n    y_predict_lgb = lgb_model.predict(test)\n    lgbm_predicts.append(y_predict_lgb) \n    \n    \n    #XGBoost\n    max_depth = 1\n    learning_rate = 0.2\n    min_child_weight = 3.54\n    reg_alpha = 0.27\n    reg_lambda = 0.52\n    subsample = 1\n    n_estimators = 100\n    \n    xg_class = xgb.XGBClassifier(\n        learning_rate=learning_rate, \n        max_depth=max_depth,\n        min_child_weight=min_child_weight, \n        missing=None, \n        n_estimators=n_estimators,\n        nthread=-1,\n        objective='binary:logistic',\n        reg_alpha=reg_alpha,\n        reg_lambda = reg_lambda,\n        seed=444,\n        verbosity=0, \n        subsample=subsample)\n    \n    curr_x_train = train_xgboost.loc[mask['train']]\n    \n    xg_fit=xg_class.fit(curr_x_train, curr_y_train)\n    xg_predict = xg_class.predict_proba(test_xgboost)[:,1]\n    xg_predicts.append(xg_predict)\n","7d9753fd":"lgbm_cf = 0.51\ncb_cf = 0.25\nxgb_cf = 0.24\nlgbm_th = 0.605\ncb_th = 0.61\nxgb_th = 0.605\n\nmean_cb_predict = np.array(cb_predicts).mean(axis=0)\nmean_lgb_predict = np.array(lgbm_predicts).mean(axis=0)\nmean_xg_predict = np.array(xg_predicts).mean(axis=0)\n\nweighted_f1_predict = (mean_cb_predict * cb_cf \/ cb_th + mean_lgb_predict * lgbm_cf \/ lgbm_th + mean_xg_predict * xgb_cf \/ xgb_th) > 1\nres = pd.DataFrame(weighted_f1_predict).reset_index().rename(columns={\"index\" : \"shot_id\", 0: 'is_goal'})\nres['is_goal'] = res['is_goal'].astype(int)\nres['shot_id'] += 1\nres","ff21631e":"res.to_csv('submission.csv', index=False)","e889153d":"* birthDate","da3ad1b7":"# Development, Evaluation, Optimization","c02800d5":"* is_CA","3d1dd073":"# File descriptions\ngoal_train.csv - the training set\ngoal_test.csv - the test set\ngoal_submission.csv - a sample submission file in the correct format\n# Data fields\n* eventId: the identifier of the event's type. Each eventId is associated with an event name (see next point);\n* eventName: tteamIdhe name of the event's type. There are seven types of events: pass, foul, shot, duel, free kick, offside and touch;\n* subEventId: the identifier of the subevent's type. Each subEventId is associated with a subevent name (see next point);\n* subEventName: the name of the subevent's type. Each event type is associated with a different set of subevent types;\n* tags: a list of*  event tags, each one describes additional information about the event (e.g., accurate). Each event type is associated with a different set of tags;\n* eventSec: the time when the event occurs (in seconds since the beginning of the current half of the match);\n* id: a unique identifier of the event;\n* matchId: the identifier of the match the event refers to. The identifier refers to the field \"wyId\" in the match dataset;\n* matchPeriod: the period of the match. It can be \"1H\" (first half of the match), \"2H\" (second half of the match), \"E1\" (first extra time), \"E2\" (second extra time) or \"P\" (penalties time);\n* playerId: the identifier of the player who generated the event. The identifier refers to the field \"wyId\" in a player dataset;\n* positions: the origin and destination positions associated with the event. Each position is a pair of coordinates (x, y). The x and y coordinates are always in the range [0, 100] and indicate the percentage of the field from the perspective of the attacking team. In particular, the value of the x coordinate indicates the event's nearness (in percentage) to the opponent's goal, while the value of the y coordinates indicates the event's nearness (in percentage) to the right side of the field;\n* teamId: the identifier of the player's team. The identifier refers to the field \"wyId\" in the team dataset.\n* city: the city where the team is located. For national teams it is the capital of the country;\n* name: the common name of the team;\n* area: information about the geographic area associated with the team;\n* wyId: the identifier of the team, assigned by Wyscout;\n* officialName: the official name of the team (e.g., Juventus FC);\n* type: the type of the team. It is \"club\" for teams in the competitions for clubs and \"national\" for the teams in international competitions;","736276cd":"* role","099f5858":"* league\n* passportArea\n* birthArea\n* area","f78586d9":"* currentNationalTeamId","774ae445":"* weight\n* height","d4dae086":"# Representation and feature engineering","078c077a":"* x\n* y","c042f8a9":"* **eventSec**\n* **matchPeriod**","64978d69":"* body_part\n* foot\n* wing"}}