{"cell_type":{"ddecaa1f":"code","693935f6":"code","063e7ef1":"code","d79d5cc3":"code","9b4a1df9":"code","d2115142":"code","26567bb6":"code","fb027de3":"code","3c060a27":"code","2dd58980":"code","0e23497c":"code","e6c1cc7e":"code","d324b0ae":"code","5aeaef72":"code","821e8124":"code","1ac3fae7":"code","9c438a83":"code","993587c6":"code","0dc8fb42":"code","794b2c54":"code","a8162388":"code","29976df1":"code","7fa71140":"code","d1e9e870":"code","46d1d2bd":"code","b3727d48":"code","2e75ec02":"code","47680ede":"code","1271f37f":"code","b3aff38b":"code","baa0bf74":"code","b84db3c8":"code","2d44d045":"code","6475c475":"code","fdd0f818":"code","c4c10199":"code","9d3b01ca":"code","db09749f":"code","64a6bd02":"code","03f1b3cd":"code","a1f798bf":"code","eaf8a364":"code","4bace022":"code","f56987d0":"code","a422fe77":"markdown","b1d5a956":"markdown","f0b5a6ee":"markdown","f4f8a8d5":"markdown","f7f263f2":"markdown","dcac29a4":"markdown","95d99c89":"markdown","3264f992":"markdown","76dfb6de":"markdown","3b49d20e":"markdown","fb99f10c":"markdown","cb519946":"markdown","feca342a":"markdown","2fd68035":"markdown","f0aff331":"markdown","b6918ac8":"markdown"},"source":{"ddecaa1f":"# Importing Libraries(Technologies)\nfrom math import sqrt\nfrom pandas_datareader import data \nimport matplotlib.pyplot as plt\nimport scipy\nimport pandas as pd\nimport datetime as dt\nimport urllib.request,json\nimport os\nimport numpy as np\nimport tensorflow as tf\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense,LSTM","693935f6":"dataframe2=pd.read_csv('..\/input\/homesteadus-electricity-consumption\/electric_consumption_till_25jun.csv')","063e7ef1":"dataframe2=dataframe2.drop('Unnamed: 0',axis=1)","d79d5cc3":"fig,ax = plt.subplots()                       # Create a figure and a set of subplots.\nplt.ylabel('Electricity Consumption[in MWh]') # labelling Y-axis\nplt.xlabel('Date')                            # labelling X-axis\nax.plot(dataframe2['Date'],dataframe2['Consumption'])\nplt.show()","9b4a1df9":"DataFrame=dataframe2.copy()","d2115142":"import seaborn as sns\n#get correlations of each features in dataset\ncorrmat = DataFrame.corr()\ntop_corr_features = corrmat.index\nplt.figure(figsize=(20,20))\n#plot heat map\ng=sns.heatmap(DataFrame[top_corr_features].corr(),annot=True,cmap=\"RdYlGn\")","26567bb6":"pearsoncorr = DataFrame.corr(method='pearson')\npearsoncorr","fb027de3":"pearsoncorr_list = pearsoncorr.values.tolist()                     # Dataframe 'pearsoncorr' with pearson correlation values among features converted to list\n\nfeatures = DataFrame.columns\nfeatures = features[1:-1]                                    # features of DataFrame except Date\n\na = list(pearsoncorr_list[-1])                              # Pearson correlation value of features vs consumption\npf = pd.DataFrame(columns=['Features','ratio'])            # Dataframe to visualise list 'a' values relation to features\n\nprint('CONSUMPTION CORRELATIONS (PEARSON) FOR HOMESTEAD')\n\nfor i in range(len(a)-1):\n  d = [features[i],a[i]]\n  ln = len(pf)\n  pf.loc[ln] = d\n\npf","3c060a27":"corr = DataFrame.corr(method='kendall')\ncorr","2dd58980":"DataFrame","0e23497c":"RConsumption = {}\nh = ['Date','Consumption']\nfor col in DataFrame.columns:\n  if( col not in h):\n    slope, intercept, r_value, p_value, std_err = scipy.stats.stats.linregress(DataFrame['Consumption'], DataFrame[col])\n    RConsumption[col] = r_value**2\nprint('DEMAND CORRELATIONS (r^2) FOR HOMESTEAD')\n\ncons_df = pd.DataFrame(RConsumption.items(), columns=['Features', 'r**2'])\ncons_df","e6c1cc7e":"DataFrame.columns","d324b0ae":"DataFrame","5aeaef72":"dataframe2 = DataFrame.copy()\ndataframe2","821e8124":"dataframe2.columns","1ac3fae7":"dataframe2.columns","9c438a83":"dataframe2['Day']= pd.DatetimeIndex(dataframe2['Date']).day\ndataframe2['month']=pd.DatetimeIndex(dataframe2['Date']).month\ndataframe2['Year']=pd.DatetimeIndex(dataframe2['Date']).year","993587c6":"pearsoncorr = dataframe2.corr(method='pearson')\npearsoncorr['Consumption']","0dc8fb42":"f = ['Homestead_cloudcover','Homestead_visibility','Homestead_winddirDegree','Day','Year']","794b2c54":"dataframe2=dataframe2.drop(f,axis = 1)","a8162388":"df.head()","29976df1":"cnsm=dataframe2.pop('Consumption')\ndataframe2['Consumption']=cnsm","7fa71140":"dataframe2","d1e9e870":"print(dataframe2.columns)","46d1d2bd":"dataframe2.shape","b3727d48":"Date = dataframe2.pop('Date')           # Removing column 'Date' from dataframe2, need not to be normalized","2e75ec02":"values = dataframe2.values              #converting to np array","47680ede":"scaler = MinMaxScaler(feature_range=(0,1))\nscaled = scaler.fit_transform(values)           # Fit to data, then transform it.","1271f37f":"scaled.shape","b3aff38b":"dataframe2['Date'] = Date                       #adding date back","baa0bf74":"dataframe2.shape","b84db3c8":"scaled                                         #normalized values","2d44d045":"DataFrame1 = dataframe2.copy(deep = False)\nDataFrame1.head()","6475c475":" # convert series to supervised learning\ndef series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n\tn_vars = 1 if type(data) is list else data.shape[1]\n\tdf = pd.DataFrame(data)\n\tcols, names = list(), list()\n\t# input sequence (t-n, ... t-1)\n\tfor i in range(n_in, 0, -1):\n\t\tcols.append(df.shift(i))\n\t\tnames += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n\t# forecast sequence (t, t+1, ... t+n)\n\tfor i in range(0, n_out):\n\t\tcols.append(df.shift(-i))\n\t\tif i == 0:\n\t\t\tnames += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n\t\telse:\n\t\t\tnames += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n\t# put it all together\n\tagg = pd.concat(cols, axis=1)\n\tagg.columns = names\n\t# drop rows with NaN values\n\tif dropnan:\n\t\tagg.dropna(inplace=True)\n\treturn agg","fdd0f818":"# frame as supervised learning\nreframed = series_to_supervised(scaled, 1, 1)\nreframed.drop(reframed.columns[range(14,27)],axis=1,inplace=True)\nreframed.head()","c4c10199":"# split into train and test sets(68:32 ratio)\nvalues = reframed.values\nn_train_hours = int(len(values)*0.68)\ntrain = values[:n_train_hours, :]\ntest = values[n_train_hours:, :]\n# split into input and outputs\ntrain_X, train_y = train[:, :-1], train[:, -1]\ntest_X, test_y = test[:, :-1], test[:, -1]\n# reshape input to be 3D [samples, timesteps, features]\ntrain_X = train_X.reshape((train_X.shape[0], 1, train_X.shape[1]))\ntest_X = test_X.reshape((test_X.shape[0], 1, test_X.shape[1]))\nprint(train_X.shape, train_y.shape, test_X.shape, test_y.shape)","9d3b01ca":"print(values)","db09749f":"# design network\nmodel = Sequential()\nmodel.add(LSTM(50, input_shape=(train_X.shape[1], train_X.shape[2])))\nmodel.add(Dense(1))\nmodel.compile(loss='mae', optimizer='adam')\n# fit network\nhistory = model.fit(train_X, train_y, epochs=250, batch_size=72, validation_data=(test_X, test_y), verbose=2, shuffle=False)\n# plot history\nplt.plot(history.history['loss'], label='train')\nplt.plot(history.history['val_loss'], label='test')\nplt.legend()\nplt.show()","64a6bd02":"print(test_X.shape)","03f1b3cd":"# make a prediction\nyhat = model.predict(test_X)\nprint(yhat.shape)","a1f798bf":"#reshaping as the shape when scaling was done\ntest_x = test_X.reshape((test_X.shape[0], test_X.shape[2]))\n# invert scaling for forecast\ninv_yhat = np.concatenate((test_x[:,:-1],yhat), axis=1)\nprint(inv_yhat.shape)\ninv_yhat = scaler.inverse_transform(inv_yhat)\ninv_yhat = inv_yhat[:,-1]\n#reshaping as the shape when scaling was done\ntest_y = test_y.reshape((len(test_y), 1))\ninv_y = np.concatenate((test_x[:,:-1],test_y), axis=1)\n# invert scaling for actual\ninv_y = scaler.inverse_transform(inv_y)\ninv_y = inv_y[:,-1]\n# calculate RMSE\nrmse = sqrt(mean_squared_error(inv_y, inv_yhat))\nprint('Test RMSE: %.3f' % rmse)","eaf8a364":" \n from sklearn.metrics import r2_score\n r2_score(inv_y, inv_yhat)","4bace022":"com=pd.DataFrame(list(zip(inv_y,inv_yhat)),columns=['Actual','Predicted'])\ncom.iloc[15:35,:]","f56987d0":"\ny=inv_y[:100]\npredicted=inv_yhat[:100]\nplt.plot(y,label='actual')\nplt.plot(predicted,label='predicted')\nplt.ylabel('Electricity unit')\nplt.title('Actual vs Predicted')\nplt.legend()\nplt.show()","a422fe77":"# **Data Loading**","b1d5a956":"# ***FEATURE SELECTION***\nStatistical tests can be used to select those features that have the strongest relationship with the output variable.","f0b5a6ee":"In scholarly research that focuses on marketing issues, R2 values of *0.75, **0.50**, or 0.25* can, as a rough rule of thumb, be respectively described as substantial, **moderate**, or weak. Sarstedt, M., & Mooi, E. (2014,p.211). \n\nhttps:\/\/people.duke.edu\/~rnau\/rsquared.htm","f4f8a8d5":"Technologies used: Google colab, Python, Json, tf keras  \n\nAPIs used: wwo-hist, api.eia.gov","f7f263f2":"## ***R Squared Correlation***\n\nCorrelation (otherwise known as \u201cR\u201d) is a number between 1 and -1 where a value of +1 implies that an increase in x results in some increase in y, -1 implies that an increase in x results in a decrease in y, and 0 means that there isn\u2019t any relationship between x and y. Like correlation, R\u00b2 tells you how related two things are. However, we tend to use R\u00b2 because it\u2019s easier to interpret. R\u00b2 is the percentage of variation (i.e. varies from 0 to 1) explained by the relationship between two variables.\n\n### ***R\u00b2 = (var(mean) - var(line)) \/ var(mean)***","dcac29a4":"## ***Kendall rank correlation coefficient***\n\nIt is a measure of rank correlation: the similarity of the orderings of the data when ranked by each of the quantities.\n\nThe Kendall correlation between two variables will be high when observations have a similar (or identical for a correlation of 1) rank (i.e. relative position label of the observations within the variable: 1st, 2nd, 3rd, etc.) between the two variables, and low when observations have a dissimilar (or fully different for a correlation of \u22121) rank between the two variables.","95d99c89":"## **Correlation Matrix with Heatmap**\n\nCorrelation states how the features are related to each other or the target variable.\n\nCorrelation can be positive (increase in one value of feature increases the value of the target variable) or negative (increase in one value of feature decreases the value of the target variable)\n\nHeatmap makes it easy to identify which features are most related to the target variable, we will plot heatmap of correlated features using the seaborn library.","3264f992":"We have extracted data from above module and store in .csv file, data until 24th june 2020","76dfb6de":"## ***Pearson Correlation Coefficient***\n\nPearson's Correlation Coefficient helps to find out the relationship between two quantities. It gives the measure of the strength of association between two variables. The value of Pearson's Correlation Coefficient can be between -1 to +1. 1 means that they are highly correlated and 0 means no correlation.","3b49d20e":"Sequential class model is a linear stack of Layers. You can create a Sequential model and define all of the layers in the constructor.\n\nWe have used\n\n1.LSTM Layer\n\n2.Dense Layer\n\nWhy we are using LSTM Layer?\n\nTypical RNN uses information from the previous step to predict the output.But if only the previous step is not enough,that is long term dependency.If we use RNN using all previous steps ,the **explosion\/vanishing gradient problem** is encountered.\n\nLSTM can solve this problem, because it uses gates to control the memorizing process. \n\n**Model Compliation**\n\nModel optimizer is the search technique used to update weights in your model.(uses gradient descent methods to minimize the loss function)\n\nThe loss function, also called the objective function is the evaluation of the model used by the optimizer to navigate the weight space.\n","fb99f10c":"# ***DATA NORMALISATION***","cb519946":"# ***Training and Testing***","feca342a":"## ***Libraries***","2fd68035":"> You can see the periodicity so we can use time features like day,month,year.","f0aff331":"## ***Features Drop-out***","b6918ac8":"## **MinMaxScaler**\n\nStandardizes features by scaling each feature to a given range.\n\nThis estimator scales and translates each feature individually such that it is in the given range on the training set, i.e. between zero and one.\n\nThe standardization is given by:\n```\nX_std = (X - X.min(axis=0)) \/ (X.max(axis=0) - X.min(axis=0))\nX_scaled = X_std * (max - min) + min\n```\n\n\n\nwhere min, max = feature_range."}}