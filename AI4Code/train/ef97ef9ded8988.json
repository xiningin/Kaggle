{"cell_type":{"6bf73e84":"code","e8dece70":"code","7577f2f2":"code","0e9865c1":"code","484bbae2":"code","920c1af5":"code","5950198f":"code","66fc8522":"code","a37f0ba6":"code","948e9197":"code","c88acf4f":"code","4a686c20":"code","35c04e12":"code","36616d3b":"code","329cbcdf":"code","3505195b":"code","d2bada2e":"code","e7f0ba50":"code","4a8d423a":"code","11d1dd89":"code","d857d77a":"code","1626b78d":"code","063cddfe":"code","302af7c9":"code","742df844":"code","d57ae242":"code","310b59cc":"code","83ea5ad2":"markdown","622592c3":"markdown","ddfc0eef":"markdown","9d17eaaa":"markdown","5d7f883f":"markdown","25bba6b2":"markdown","dde3bcaa":"markdown"},"source":{"6bf73e84":"#!pip install pandas, numpy, matplotlib, seaborn, sklearn, xgboost, lightgbm, catboost","e8dece70":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()\n\nfrom sklearn.linear_model     import LogisticRegression\nfrom sklearn.neighbors        import KNeighborsClassifier\nfrom sklearn.svm              import SVC\nfrom sklearn.tree             import DecisionTreeClassifier\nfrom sklearn.ensemble         import RandomForestClassifier\nfrom sklearn.ensemble         import ExtraTreesClassifier\nfrom sklearn.ensemble         import GradientBoostingClassifier\nfrom xgboost                  import XGBClassifier, plot_tree\nimport lightgbm as lgb\n\nfrom sklearn.preprocessing    import LabelEncoder\nfrom sklearn.model_selection  import train_test_split\nfrom sklearn.model_selection  import StratifiedKFold\nfrom sklearn.model_selection  import cross_val_score\nfrom sklearn.model_selection  import GridSearchCV\nfrom sklearn.metrics          import accuracy_score, balanced_accuracy_score\n\nimport catboost\nfrom catboost import *\nfrom catboost import datasets\nfrom catboost import CatBoostClassifier","7577f2f2":"df = pd.read_csv(\"beer_train_clean.csv\")\ndf.head()","0e9865c1":"cat = df.select_dtypes(include=[object]).columns\nprint(\"\\nCategorical features:\\n\", cat.values)","484bbae2":"style_encoder = LabelEncoder()\nstyle_encoder.fit(df['Style'])\n\ndf[cat] = df[cat].apply(LabelEncoder().fit_transform)\n#df['Style'] = df['Style'].apply(style_encoder.fit_transform)\n#df['Style'] = style_encoder.fit_transform(df['Style'])\ndf.head()","920c1af5":"from sklearn.impute import SimpleImputer\n\n# Imputar\nimputed_df = pd.DataFrame(SimpleImputer().fit_transform(df))\n# Restaurar nombres de columnas\nimputed_df.columns = df.columns\ndf = imputed_df\ndf.head()","5950198f":"# Features\nX = df[['Size(L)', 'OG', 'FG', 'ABV', 'IBU', 'Color', 'BoilSize', 'BoilTime',\n        'BoilGravity', 'Efficiency', 'MashThickness', 'PitchRate',\n        'PrimaryTemp', 'SugarScale_Plato', 'SugarScale_Specific Gravity',\n        'BrewMethod_All Grain', 'BrewMethod_BIAB', 'BrewMethod_Partial Mash', 'BrewMethod_extract'\n       ]]\n\n# Label\ny = df[\"Style\"].values\nX[0:5]","66fc8522":"# Train and test split (used only for KNN)\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=4)\nprint ('Train:', X_train.shape,  y_train.shape)\nprint ('Valid:', X_valid.shape,  y_valid.shape)","a37f0ba6":"# KNN\ndef get_best_k(draw_plot):\n  Ks = 20 # max number of Ks\n  k = 1 # best K\n  mean = np.zeros((Ks-1)) # empty array to store the mean accuracy\n  std = np.zeros((Ks-1))  # empty array for standard deviation\n\n  for n in range(1,Ks):\n      knn = KNeighborsClassifier(n_neighbors=n).fit(X_train, y_train) # train model\n      KNN_yhat = knn.predict(X_valid) # get a predicted value from the valid set\n      mean[n-1] = accuracy_score(y_valid, KNN_yhat) # add the accuracy to the mean array\n\n      std[n-1] = np.std(KNN_yhat==y_valid)\/np.sqrt(KNN_yhat.shape[0]) # add the standard deviation to the std array\n  \n  k = mean.argmax()+1\n\n  # Plot\n  if not draw_plot:\n    return k\n  plt.plot(range(1, Ks),mean, 'g') # plot the line\n  plt.fill_between(range(1, Ks), mean - std, mean + std, alpha=0.10) # fill the standard deviation zone\n  plt.legend(('Accuracy ', '+\/- std'))\n  plt.ylabel('Accuracy ')\n  plt.xlabel('Number of Neighbors (K)')\n  plt.tight_layout()\n  plt.show()\n  print(\"Peak accuracy with\", mean.max(), \"k =\", k)\n  \n  return k","948e9197":"models = [\n    ('Logistic Regression', LogisticRegression(n_jobs=-1)),\n    ('SVM',                 SVC()),\n    ('Decision Tree',       DecisionTreeClassifier()),\n    ('KNN',                 KNeighborsClassifier(n_neighbors=get_best_k(False))),\n    ('Extra Trees',         ExtraTreesClassifier(n_jobs=-1)),\n    ('Random Forest',       RandomForestClassifier(n_jobs=-1)),\n    ('Gradient Boosting',   GradientBoostingClassifier()),\n    ('XGBoost',             XGBClassifier(n_estimators=250)),\n    ('CatBoost',            CatBoostClassifier(CatBoostClassifier(n_estimators=350, learning_rate=0.1, early_stopping_rounds=10)))\n]","c88acf4f":"Modelnames = []\noutcome = []\n\nfor name, model in models:\n    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\n    cv_r = cross_val_score(model, X, y, cv=skf, scoring='accuracy')\n    outcome.append(cv_r)\n    Modelnames.append(name)\n    print(\"%s: %.2f%% (%.2f%%)\" % (name, cv_r.mean()*100, cv_r.std()*100))","4a686c20":"models = [None]*2\noutcome = []\nModelnames = [None]*2","35c04e12":"# CatBoost\nmodels[-1] = ('CatBoost',            CatBoostClassifier(iterations=350,learning_rate=0.05, verbose=0))\nskf = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\ncv_r = cross_val_score(models[-1][1], X, y, cv=skf, scoring='accuracy')\noutcome.append(cv_r)\nModelnames[-1] = models[-1][0]\nprint(\"%s: %.2f%% (%.2f%%)\" % (models[-1][0], cv_r.mean()*100, cv_r.std()*100))","36616d3b":"# XGBoost better\nmodels[-2] = ('XGBoost', XGBClassifier(n_estimators=250,max_depth=3,learning_rate=0.1,early_stopping_rounds=10))\nskf = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\ncv_r = cross_val_score(models[-2][1], X, y, cv=skf, scoring='accuracy')\noutcome.append(cv_r)\nModelnames[-2] = models[-2][0]\nprint(\"%s: %.2f%% (%.2f%%)\" % (models[-2][0], cv_r.mean()*100, cv_r.std()*100))","329cbcdf":"# xgboost grid search\nxgb_model = XGBClassifier()\noptimization_dict = {\n        'early_stopping_rounds': [10,15],\n        'n_estimators':[250,300,500],\n        'max_depth': [3,4],\n        'learning_rate': [0.01,0.1]\n        }\njobs = 15\n\nmodel = GridSearchCV(xgb_model, optimization_dict, \n                     scoring='accuracy',pre_dispatch=jobs*2,n_jobs=jobs,verbose=5)\nmodel.fit(X,y)\nprint(model.best_score_)\nprint(model.best_params_)\n\nwith open('best.txt', 'w+') as f:\n    f.write(str(model.best_score_))\n    f.write('\\n')\n    f.write(str(model.best_params_))","3505195b":"# catBoost grid search\ncb_model = CatBoostClassifier() # 0.6423168245605418\noptimization_dict = {\n        'early_stopping_rounds': [10], # 10\n        'n_estimators': [350,500],# 350\n        'learning_rate': [0.1], # 0.1\n        #'l2_leaf_reg': [2, 4, 6],\n        #'one_hot_max_size': [50],\n        #'min_child_weight': [1, 5, 10],\n        #'gamma': [0.5, 1, 2, 5],\n        #'subsample': [0.6, 0.8, 1.0],\n        'max_depth': [3],# 3\n        'random_seed': [0]\n        }\n#0.6399067544096877 ???\n#{'early_stopping_rounds': 10, 'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 400}\n#0.641402582035515\n#{'early_stopping_rounds': 10, 'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 500}\n#0.6402807501598418\n#{'early_stopping_rounds': 10, 'l2_leaf_reg': 6, 'learning_rate': 0.15, 'max_depth': 3, 'n_estimators': 400, 'one_hot_max_size': 50, 'random_seed': 0}\njobs = 20\n\nmodel = GridSearchCV(cb_model, optimization_dict, cv=3,\n                     scoring='accuracy',pre_dispatch=2*jobs,n_jobs=jobs,verbose=0)\nmodel.fit(X,y)\nprint(model.best_score_)\nprint(model.best_params_)\n\nwith open('best.txt', 'w+') as f:\n    f.write(str(model.best_score_))\n    f.write('\\n')\n    f.write(str(model.best_params_))","d2bada2e":"param = {'num_leaves': 31, 'objective': 'multiclassova'}\n#param['metric'] = 'auc'\nnum_round = 10\nbst = lgb.train(param, train_data, num_round, valid_sets=[validation_data])","e7f0ba50":"results = {'Names': Modelnames, 'Results': outcome}\nplt.figure(figsize=(16,6))\nsns.boxplot(x='Names', y='Results', data=results)","4a8d423a":"def encode_and_bind(original_dataframe, feature_to_encode):\n    dummies = pd.get_dummies(original_dataframe[feature_to_encode], prefix=feature_to_encode)\n    res = pd.concat([original_dataframe, dummies], axis=1)\n    res = res.drop([feature_to_encode], axis=1)\n    return(res)","11d1dd89":"# Import test set\ntest_df = pd.read_csv('beer_test.csv')\n# Separate categorical values\ntest_cat = test_df.select_dtypes(include=[object]).columns\n# Encoding\ntest_df[test_cat] = test_df[test_cat].apply(LabelEncoder().fit_transform)\n# One Hot Encoding\nfor c in test_cat.values:\n    test_df = encode_and_bind(test_df, c)\n# Imputing\nimputed_test_df = pd.DataFrame(SimpleImputer().fit_transform(test_df))\nimputed_test_df.columns = test_df.columns\ntest_df = imputed_test_df\n\ntest_df.head()","d857d77a":"# Test Features\n'''\nX_test = test_df[['Size(L)', 'OG', 'FG', 'ABV',\t'IBU', 'Color', 'BoilSize',\n        'BoilTime', 'BoilGravity', 'Efficiency', 'MashThickness',\n        'SugarScale', 'BrewMethod', 'PitchRate', 'PrimaryTemp'\n       ]]\n'''\nX_test = test_df.drop('Id', axis=1)\nX_test[0:5]","1626b78d":"# Predict values from the test set\n#preds = XGBClassifier(n_estimators=300, max_depth=3).fit(X, y).predict(X_test)\n#preds = CatBoostClassifier(n_estimators=350, max_depth=3, learning_rate=0.1, early_stopping_rounds=10).fit(X, y).predict(X_test)\n\npreds = model.predict(X_test)","063cddfe":"from sklearn.inspection      import permutation_importance\n# Compute Permutation Feature Importance\npfi = permutation_importance(model, X_valid, y_valid, n_repeats=10, random_state=0, n_jobs=-1)\n\n# Clean data\nsorted_idx = pfi.importances_mean.argsort()[::-1]\npfi_df = pd.DataFrame(data=pfi.importances[sorted_idx].T, columns=X_valid.columns[sorted_idx])\n\n# Plot (This can be barplot, boxplot, violinplot,...)\nplt.figure(figsize=(12,4))\nsns.barplot(data=pfi_df, orient=\"h\").set_title(\"Permutation Feature Importance (validation set)\",  fontsize=20);","302af7c9":"# Generate solution Dataframe\nfinal_df = pd.DataFrame(preds, columns = ['Style'])\n# Name Index to 'Id'\nfinal_df.index.name = 'Id'\nfinal_df.head()","742df844":"# Decode values\nfinal_df['Style'] = style_encoder.inverse_transform(final_df['Style'].astype('int'))\nfinal_df.head()","d57ae242":"final_df.shape","310b59cc":"# Save to file\nfinal_df.to_csv(\"Desaf\u00edo cervezas.csv\")","83ea5ad2":"# Missings","622592c3":"# #XGBoost","ddfc0eef":"# Encoding","9d17eaaa":"# Accuracy Report","5d7f883f":"# Read CSV","25bba6b2":"# Variables","dde3bcaa":"# Models"}}