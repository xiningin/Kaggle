{"cell_type":{"c1afb80f":"code","e707ec5f":"code","3df7e18a":"code","50dc6cff":"code","83234a0e":"code","ee3b6fb8":"code","50259234":"code","44957f05":"code","7676fe87":"code","893eab37":"code","01ee5a74":"code","2563bba3":"code","26486832":"code","3a34906c":"code","c10f8b89":"code","02ce487a":"code","836e63dd":"code","d903cd22":"code","79b5b9ba":"code","97cd33e5":"code","ffb8543c":"code","644f0fba":"code","3741c404":"code","1554479b":"code","8dac1a64":"code","1a43f228":"code","cb71091b":"code","36899db7":"code","05b1d876":"markdown","4d55be22":"markdown","5420ff53":"markdown","13acbd61":"markdown","8f15a3a6":"markdown","faf2c4d6":"markdown","8636f7ec":"markdown","aae6f16a":"markdown","3bef65ab":"markdown","29ad9312":"markdown","c1fbf82b":"markdown","f0da29af":"markdown","784064c0":"markdown","8d1df819":"markdown","b54211f5":"markdown","7c15c0ff":"markdown","e96e2aa9":"markdown"},"source":{"c1afb80f":"# General imports\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\n\n# Model imports\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import LabelEncoder\nfrom xgboost import XGBRegressor\n\n# Filepaths\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","e707ec5f":"# Function to get train and test cvs, with flexible file names\ndef train_and_test_df_getter():\n    '''Function to read all the filepaths in the \/kaggle\/input folder\n    If filepath contains \"test\" then saved as test file path\n    If filepath contains \"train\" then saved as train file path\n    Then pd.read_csv is used and the test and train dfs are returned\n    \n    Returns:\n        train: pandas dataframe\n        test: pandas dataframe\n    '''\n    # Add all file paths to list\n    pathnames = []\n    for dirname, _, filenames in os.walk('\/kaggle\/input'):\n        for filename in filenames:\n            pathnames.append(os.path.join(dirname, filename))\n    \n    # Look for \"test\" or \"train\" within the filepath strings, and save to relevant variables if found\n    for i in pathnames:\n        if 'train' in i:\n            trainpath = i\n        elif 'test' in i:\n            testpath = i\n        else:\n            pass\n                \n    # Print filepaths (for transparency)\n    print(f'Train filepath: {trainpath}\\nTest filepath: {testpath}')\n    \n    # Return dataframes via pd.read_csv()\n    train = pd.read_csv(trainpath)\n    test = pd.read_csv(testpath)\n    \n    # If there's a column called ID or id or Id then set that as the ID\n    for i in [train,test]:\n        if 'Id' in i.columns:\n            i.set_index('Id',inplace=True)\n        elif 'ID' in i.columns:\n            i.set_index('ID',inplace=True)\n        elif 'id' in i.columns:\n            i.set_index('id',inplace=True)\n        else:\n            pass\n        \n    print('Variables returned!')\n    return train,test","3df7e18a":"train_original,test_original = train_and_test_df_getter()\n\n# Making copies to clean so I keep the originals seperate\ntrain = train_original.copy()\ntest = test_original.copy()\n\n# Adding both to a list to streamline cleaning each df at the same time\ndfs = [train, test]\ndfs_names = ['Train','Test'] # useful for some functions","50dc6cff":"# Save target to variable for future use\ntarget = 'SalePrice'","83234a0e":"def cols_with_na(df,df_name):\n    '''Print out columns with NA values, and save list of cols\n    Also returns list for further use\n    \n    Args:\n        df: dataframe to input\n        df_name: string to be printed, to aid readability\n        \n    Returns:\n        cols_with_na: list containing names of columns with na values\n    '''\n    # Make list of names of columns with na values\n    cols_with_na = df.columns[df.isna().any()].to_list()\n    \n    # Variables for printing:\n    cols_with_na_len = len(cols_with_na)\n    total_cols_count = df.shape[1]\n    percent_cols_with_na = round(cols_with_na_len\/total_cols_count*100,2)\n    \n    # Print, for transparency\n    print('-'*180)\n    print(f'{df_name}:')\n    print(f'Count of columns with NA values: {cols_with_na_len}')\n    print(f'Percent of columns with NA values: {percent_cols_with_na}%')\n    print(f'({total_cols_count} columns total)\\n')\n    print(f'{df_name} columns with NA values: {cols_with_na}\\n')\n    \n    # Return list for further use\n    return(cols_with_na)","ee3b6fb8":"# Running function and saving lists of columns with NA values\ntrain_na_cols = cols_with_na(train,'Train')\ntest_na_cols = cols_with_na(test,'Test')","50259234":"# Of columns with NA values, how many NAs are there?\n# I.e. if not many then I can impute, but if too many then may have to drop column\n\ndef to_impute_or_not_to_impute_getter(df,df_name,impute_thres,verbose=False):\n    '''Print out % of columns with NA values that is made up of NA values\n    Allows determination of if imputation is possible or if column needs to be dropped\n    \n    Args:\n        df: dataframe to input\n        df_name: string to be printed out to aid readability\n        impute_thres: threshold for determining whether to impute: if 25, will print \"ok to impute\" for cols with <= 25% na\n        verbose: if True, will print out explanation for transparency. Default to False as it's a bit clunky\n    \n    Returns:\n        to_impute: list of column names to impute\n        not_to_impute: list of column names not to impute\n    '''\n    # Which columns have NA values?\n    cols_with_na = df.columns[df.isna().any()].to_list()\n\n    # Instantiate list of cols to impute and to not impute\n    to_impute = []\n    not_to_impute = []\n    \n    # For cols with na, print % of col that is na, and print 'ok to impute' if thres < inputted threshold\n    # If % of NA is below threshold, add to \"to_impute\" list\n    # Else, add to \"not_to_impute\" list\n    for i in cols_with_na:\n        count_missing = df[i].isna().sum()\n        percent_missing = round(count_missing\/len(df)*100,1)\n\n        if percent_missing <= impute_thres:\n            if verbose==True:\n                print(f'{i} ok to impute: {percent_missing}% NA, below chosen {impute_thres}% impute threshold')\n            else:\n                pass\n            to_impute.append(i)\n        else:\n            if verbose==True:\n                print(f'**Don\\'t** impute {i} (amount missing = {percent_missing})')\n            else:\n                pass\n            not_to_impute.append(i)\n            \n    # Print what to impute and what not to impute\n    print(f'\\nFor {df_name} df, ok to impute:\\n{to_impute}\\n')\n    print(f'For {df_name} df, don\\'t impute:\\n{not_to_impute}\\n')\n    print('List variables returned for use!')\n    \n    return(to_impute,not_to_impute)","44957f05":"to_impute_train, not_to_impute_train = to_impute_or_not_to_impute_getter(train,'Train',impute_thres=25,verbose=True)\nto_impute_test, not_to_impute_test = to_impute_or_not_to_impute_getter(test,'Test',impute_thres=25)","7676fe87":"def drop_columns(df,cols):\n    '''Input dataframe and list of columns to drop'''\n    for i in cols:\n        try:\n            df.drop(i,axis=1,inplace=True)\n        except:\n            pass\n    return df","893eab37":"train = drop_columns(train,cols=not_to_impute_train)\ntest = drop_columns(test,cols=not_to_impute_test)","01ee5a74":"# How many unique values are there in each 'object' column i.e. what type of encoding to use\ndef cols_to_encode(df,df_name,percent_of_len_unique,verbose=False):\n    '''Return a list with the names of categorical columns with <n number of combinations, for encoding\n    I.e. in the titanic dataset, the number of unique ticket numbers is too high to be useful\n    Whereas the number of classes is 3 and is therefore useful\n    \n    Args:\n        df: dataframe\n        df_name: for printing\n        percent_of_len_unique: i.e. if a categorical variable only has 2 types, and the length of the df is 1000, answer will be 0.2%\n            Point of this is to catch a categorical variable that for example has 500 unique types, as these won't confer useful information\n        verbose: if = True, print out more information\n        \n    Returns:\n        List of categorical columns to impute\n    '''\n    to_encode = []\n    to_drop = []\n    \n    print(f'{df_name}:')\n    df_len = len(df)\n    print('Number of categorical columns: {}'.format(len(df.select_dtypes(include='object').columns)))\n    \n    for col in df.select_dtypes(include='object').columns:\n        # Print information about how many unique (if verbose == True)\n        if verbose == True:\n            print(f'Unique number of {col}: {df[col].nunique()}')\n            print(f'As % of length of {df_name}: {round(df[col].nunique()\/df_len*100,2)}%')\n        else:\n            pass\n        \n        # Add columns to list to encode if they are below the threshold\n        if round(df[col].nunique()\/df_len*100,2) > percent_of_len_unique:\n            print(f'Don\\'t impute {col}')\n            print(f'Unique number of {col}: {df[col].nunique()}')\n            print(f'As % of length of {df_name}: {round(df[col].nunique()\/df_len*100,2)}%')\n            to_drop.append(col)\n        else:\n            to_encode.append(col)\n        \n    return(to_encode,to_drop)","2563bba3":"cols_to_encode_train, cols_to_not_encode_train = cols_to_encode(train,'Train',percent_of_len_unique=5)\ncols_to_encode_test, cols_to_not_encode_train = cols_to_encode(test,'Test',percent_of_len_unique=5)","26486832":"train = drop_columns(train,cols=cols_to_not_encode_train)\ntest = drop_columns(test,cols=cols_to_not_encode_train)","3a34906c":"label_enc = LabelEncoder()\ndef label_encoder(df,cols_to_encode):\n    for i in cols_to_encode:\n        df[i] = df[i].astype(str)\n        df[i] = label_enc.fit_transform(df[i])\n    return df","c10f8b89":"train = label_encoder(train,cols_to_encode_train)\ntest = label_encoder(test,cols_to_encode_test)","02ce487a":"def impute_df(df,strategy):\n    '''Impute the columns previously determined to be ok to impute\n    \n    Args:\n        df: dataframe to impute\n        \n        strategy: string, default='mean'\n            Can be 'mean', 'median', 'most_frequent'\n        '''\n    imputer = SimpleImputer(strategy=strategy)\n    df_imputed = pd.DataFrame(imputer.fit_transform(df))\n    df_imputed.columns = df.columns\n    df = df_imputed.copy() # to get back to normal naming convention\n    return df\n\ntrain = impute_df(train,strategy='mean')\ntest = impute_df(test,strategy='mean')\n\nprint(f'Total NA in train: {train.isna().sum().sum()}')\nprint(f'Total NA in test: {test.isna().sum().sum()}')","836e63dd":"def return_relevant_features(target,df,pos_threshold,neg_threshold):\n    '''Function which makes a correlation matrix of the dataframe and returns list of features to use\n    Based on user input of how correlated they want the columns to be with the target\n    \n    Args:\n        target: string of the target column\n        df: dataframe i.e. train or test\n        pos_threshold: number between 0 and 1 to use as threshold for correlation value\n            i.e. if = 0.3 then features that have correlation coefficient of >=0.3 with target will be returned\n        neg_threshold: number between -1 and 0 to use as threshold for correlation value\n            i.e. if = -0.3 then features that have correlation coefficient of <=-0.3 with target will be returned\n\n    Returns:\n        features: list of columns to be included in model\n    '''\n    # Create correlation matrix\n    corr = df.corr()\n    \n    # Select features that correlate positively or negatively with the target variable based on user threshold\n    pos = corr[target][corr[target]>=pos_threshold]\n    neg = corr[target][corr[target]<=neg_threshold]\n    \n    # Combine into list and return\n    features = pd.concat([pos,neg])\n    features = features.index.to_list()\n    features.remove(target) # Feature appears in the list as it correlates with itself - removing\n    return(features)","d903cd22":"features = return_relevant_features(target='SalePrice',df=train,pos_threshold=0.4,neg_threshold=-0.4)\nfeatures_and_target = features.append(target)","79b5b9ba":"def get_train_test_split(train,target,features):\n    X = train[features].copy()\n    X.drop(target,inplace=True,axis=1)\n    y=train[target].copy()\n    \n    # Train test split\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n    return X_train, X_test, y_train, y_test","97cd33e5":"X_train, X_test, y_train, y_test = get_train_test_split(train,'SalePrice',features)","ffb8543c":"def gridsearch(X,y,model,grid,cv):\n    '''Gridsearch function to perform a gridsearch with different models\n    And then return the best parameters for use\n    \n    Keyword arguments:\n    model -- model to use i.e. XGB\n    grid -- dictionary of parameters to try out\n    cv -- size of cross validation\n    '''\n    CV = GridSearchCV(estimator=model,param_grid=grid,cv=cv)\n    CV.fit(X, y)\n    print(CV.best_params_)\n    print('Best parameters returned for use')\n    return(CV.best_params_)","644f0fba":"xgb = XGBRegressor()\nxgb_grid = {\n    'learning_rate':[0.001,0.002,0.003,0.004,0.005],\n    'n_estimators':[60,70,80,90],\n    'max_depth':[2,3,4],\n    'subsample':[0.1,0.2,0.3,0.4]}\n\n# Run the function\n#xgb_params = gridsearch(X_train,y_train,xgb,xgb_grid,5)","3741c404":"#xgb = XGBRegressor(learning_rate=xgb_params['learning_rate'],max_depth=xgb_params['max_depth'],\n#                    n_estimators=xgb_params['n_estimators'],subsample=xgb_params['subsample'])\n\nxgb = XGBRegressor(learning_rate=0.005,max_depth=4,\n                    n_estimators=90,subsample=0.4)","1554479b":"def cross_val(model_name,model,X,y,cv):\n    '''Cross validate a model and gives scores and average score\n    \n    Keyword arguments:\n    model_name -- string of the name, for printing out\n    model -- model i.e. xgb, forest\n    X -- data to use with no target\n    y -- target\n    cv -- number of cross validations\n    '''\n    scores = cross_val_score(model, X, y, cv=cv)\n    print(f'{model_name} Scores:')\n    for i in scores:\n        print(round(i,2))\n    print(f'Average {model_name} score: {round(scores.mean(),2)}')","8dac1a64":"cross_val('XGB',xgb,X_train,y_train,5)","1a43f228":"test = test[X_train.columns].copy()","cb71091b":"xgb.fit(X_train,y_train)\n\npreds = xgb.predict(test)\n\n#test.Id.astype(int)","36899db7":"output = pd.DataFrame({'Id': test.index.astype(int), 'SalePrice': preds.astype(int)})\noutput.to_csv('my_submission.csv', index=False)\nprint(\"Your submission was successfully saved!\")","05b1d876":"# Encoding & imputing 2 - encoding categorical columns","4d55be22":"---\n\n# <font color=purple>Encoding & imputing<\/font>\n- Fill NA values (imputation)\n- Replace categorical variables with numbers (encoding)\n- Need to encode categorical variables to numbers before I can impute","5420ff53":"## Function to <font color=green>drop columns that aren't suitable for imputation<\/font>\n- Use list of columns not to impute, from to_impute_or_not_to_impute_getter()","13acbd61":"## Function to determine, of columns with NA values, which are <font color=green>suitable*<\/font> for imputation\n- <font color=green>***Suitable**<\/font> is determined by user input into function\n- I.e. only columns that have <=25% na values\n- Returns list of columns to keep and columns to drop","8f15a3a6":"# Encoding & Imputing 1 - drop columns that aren't imputable\n\n## Function to <font color=green>get list of columns with NA values, and print out **report**<\/font>","faf2c4d6":"# EDA\n\n## Function to find columns that correlate with the target","8636f7ec":"## Drop unwanted columns","aae6f16a":"## Function to do simple imputation of columns acceptable for imputation","3bef65ab":"## Making sure that X and the test dataframe are same shape","29ad9312":"# *<font color=purple>**What<\/font> <font color=blue>this<\/font> <font color=red>notebook<\/font> <font color=green>contains**<\/font>*\n\nThis notebook is still a **work in progress**, and at the moment contains:\n\n- Function to **load** train and test **dataframes** <font color=green>**without having to hard-cord in the filepaths**<\/font> \n    - Looks for the word \"test\" or \"train\" in the filepath\n- Function to <font color=green>**print number of columns containing NA values, and % of total columns**<\/font> that contain NA\n- Function to <font color=green>**return list of columns to impute and not to impute, determined by user-inputted choice**<\/font> of threshold for % of NA values that are acceptable (i.e. <=25% NA is ok)\n\n---","c1fbf82b":"---\n\n# <font color=purple>Imports & Loading Data<purple>","f0da29af":"- Currently gives this error when I try and output! :(\n- The functions are still useful though\n\n![Capture.PNG](attachment:Capture.PNG)\n\n- Looks like I may have deleted things from test when I shouldn't have - rows at least","784064c0":"### GridSearch (to find best parameters for the model)","8d1df819":"## Function to <font color=green>get train and test dataframes<\/font>\n- With flexible filepaths, works between different notebooks\/ datasets\n- Loads all filepaths and looks for ones containing word \"train\" or \"test\" to use","b54211f5":"## Cross Validation","7c15c0ff":"# Done cleaning and basic feature selection, now for the model!","e96e2aa9":"## Encoding"}}