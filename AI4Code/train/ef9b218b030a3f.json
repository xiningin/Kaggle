{"cell_type":{"06fc926e":"code","c00e0840":"code","4cdf23d4":"code","a970f4c1":"code","8075da0e":"code","bbe0abe5":"code","82c6823d":"code","1c278f2a":"code","3698102a":"code","1457e189":"markdown","b0f211f0":"markdown","755c60f5":"markdown","29d71bfe":"markdown","6cd506d8":"markdown","b4f522c3":"markdown","ad726a00":"markdown","40928caa":"markdown"},"source":{"06fc926e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np\nimport pandas as pd \nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data.sampler import SubsetRandomSampler\nfrom torchvision import datasets, transforms\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","c00e0840":"test_data=pd.read_csv(\"\/kaggle\/input\/Kannada-MNIST\/test.csv\")\ntrain_data=pd.read_csv(\"\/kaggle\/input\/Kannada-MNIST\/train.csv\")\ntrain_data.head()","4cdf23d4":"labels=train_data.label.values\nimages = train_data.loc[:,train_data.columns != \"label\"].values\nprint(labels.shape)\nprint(images.shape)","a970f4c1":"images=torch.from_numpy(images).type(torch.FloatTensor)\nlabels=torch.from_numpy(labels)\nprint(images.dtype)","8075da0e":"model = nn.Sequential(nn.Linear(784, 128),\n                      nn.ReLU(),\n                      nn.Linear(128, 64),\n                      nn.ReLU(),\n                      nn.Linear(64, 10),\n                      nn.LogSoftmax(dim=1))\nprint(model)\nmodel\ncriterion = nn.NLLLoss()\noptimizer = optim.SGD(model.parameters(), lr=0.01)","bbe0abe5":"train = torch.utils.data.TensorDataset(images,labels,)\ntrain_loader = torch.utils.data.DataLoader(train, batch_size=64)","82c6823d":"epochs = 30\nfor e in range(epochs):\n    running_loss = 0\n    for images, labels in train_loader:\n        # Flatten MNIST images into a 784 long vector\n        images = images.view(images.shape[0],-1)\n        labels=labels\n        optimizer.zero_grad()\n        output = model(images)\n        loss = criterion(output, labels)\n        loss.backward()\n        optimizer.step()\n        running_loss += loss.item()\n    else:\n        print(f\"Training loss : {running_loss\/len(train_loader)}\")","1c278f2a":"import matplotlib.pyplot as plt\nimport numpy as np\nfrom torch import nn, optim\nfrom torch.autograd import Variable\n\n\ndef test_network(net, trainloader):\n\n    criterion = nn.MSELoss()\n    optimizer = optim.Adam(net.parameters(), lr=0.001)\n\n    dataiter = iter(trainloader)\n    images, labels = dataiter.next()\n\n    # Create Variables for the inputs and targets\n    inputs = Variable(images)\n    targets = Variable(images)\n\n    # Clear the gradients from all Variables\n    optimizer.zero_grad()\n\n    # Forward pass, then backward pass, then update weights\n    output = net.forward(inputs)\n    loss = criterion(output, targets)\n    loss.backward()\n    optimizer.step()\n\n    return True\n\n\ndef imshow(image, ax=None, title=None, normalize=True):\n    \"\"\"Imshow for Tensor.\"\"\"\n    if ax is None:\n        fig, ax = plt.subplots()\n    image = image.numpy().transpose((1, 2, 0))\n\n    if normalize:\n        mean = np.array([0.485, 0.456, 0.406])\n        std = np.array([0.229, 0.224, 0.225])\n        image = std * image + mean\n        image = np.clip(image, 0, 1)\n\n    ax.imshow(image)\n    ax.spines['top'].set_visible(False)\n    ax.spines['right'].set_visible(False)\n    ax.spines['left'].set_visible(False)\n    ax.spines['bottom'].set_visible(False)\n    ax.tick_params(axis='both', length=0)\n    ax.set_xticklabels('')\n    ax.set_yticklabels('')\n\n    return ax\n\n\ndef view_recon(img, recon):\n    ''' Function for displaying an image (as a PyTorch Tensor) and its\n        reconstruction also a PyTorch Tensor\n    '''\n\n    fig, axes = plt.subplots(ncols=2, sharex=True, sharey=True)\n    axes[0].imshow(img.numpy().squeeze())\n    axes[1].imshow(recon.data.numpy().squeeze())\n    for ax in axes:\n        ax.axis('off')\n        ax.set_adjustable('box-forced')\n\ndef view_classify(img, ps, version=\"MNIST\"):\n    ''' Function for viewing an image and it's predicted classes.\n    '''\n    ps = ps.data.numpy().squeeze()\n\n    fig, (ax1, ax2) = plt.subplots(figsize=(6,9), ncols=2)\n    ax1.imshow(img.resize_(1, 28, 28).numpy().squeeze())\n    ax1.axis('off')\n    ax2.barh(np.arange(10), ps)\n    ax2.set_aspect(0.1)\n    ax2.set_yticks(np.arange(10))\n    if version == \"MNIST\":\n        ax2.set_yticklabels(np.arange(10))\n   \n    ax2.set_title('Class Probability')\n    ax2.set_xlim(0, 1.1)\n\n    plt.tight_layout()\n    plt.show()\n","3698102a":"images, labels = next(iter(train_loader))\n\n\nimg = images[0].view(1, 784)\n# Turn off gradients to speed up this part\nwith torch.no_grad():\n    logps = model(img).cpu()\n\n# Output of the network are log-probabilities, need to take exponential for probabilities\nps = torch.exp(logps)\nview_classify(img.view(1, 28, 28), ps)","1457e189":"define trainloader","b0f211f0":"split pixels and labels","755c60f5":"start training\n","29d71bfe":"**Load Dataset**","6cd506d8":"Predict output","b4f522c3":"function to viusalisation of output","ad726a00":"Convert Numpy array to tensors","40928caa":"Define model, cross entropy and optimiser"}}