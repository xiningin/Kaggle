{"cell_type":{"2240acc5":"code","542a53b8":"code","67d5e4bb":"code","d71ff15b":"code","a944b7fd":"code","2d11d6f4":"code","d3e48a75":"code","4ef701b5":"code","20188122":"code","1a13872f":"code","e3d24914":"code","d8617287":"code","5aa9e8d1":"code","2d3c43bc":"code","86124f87":"code","1f74b312":"code","e12723d3":"code","64be2828":"code","30fc96b8":"code","0d46c84a":"code","a7dc2638":"code","ae92f8c9":"code","3a54126b":"code","d4af66fa":"code","5dcfb37f":"code","97a50d6b":"markdown","adef70b9":"markdown","41f8076d":"markdown","82adcecc":"markdown","bf4b075e":"markdown","466b5c81":"markdown","5fc56551":"markdown","5ca52d29":"markdown","6db9b0e3":"markdown","5ee2ec56":"markdown","558c057e":"markdown","b866c8f7":"markdown","e9d157cc":"markdown","43d1399e":"markdown","48a60854":"markdown","1c643f35":"markdown","04ddd241":"markdown","4290034e":"markdown","a0d3edb0":"markdown","afa28efe":"markdown","f8c4ac11":"markdown","63e230d3":"markdown","aebdd678":"markdown","2f13e171":"markdown","eca87203":"markdown","a54d7167":"markdown","a232deb1":"markdown","38e1e691":"markdown","dc2933b5":"markdown","6214fcf7":"markdown"},"source":{"2240acc5":"import re\nimport numpy as np\nimport pandas as pd \n\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.model_selection import train_test_split\n\nfrom keras.preprocessing import sequence\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Embedding, LSTM\nfrom keras.datasets import imdb\n\nfrom keras.utils.np_utils import to_categorical\n\nimport warnings\nwarnings.filterwarnings('ignore')\nimport ssl\nssl._create_default_https_context = ssl._create_unverified_context","542a53b8":"max_features = 1000\nmaxlen = 80  # cut texts after this number of words (among top max_features most common words)\nbatch_size = 32\n\n# save np.load\n#np_load_old = np.load\n\n# modify the default parameters of np.load\n#np.load = lambda *a,**k: np_load_old(*a, allow_pickle=True, **k)\n\nprint('Loading data...')\n(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\nprint(len(x_train), 'train sequences')\nprint(len(x_test), 'test sequences')\n\n#np.load = np_load_old\n\nprint('Pad sequences (samples x time)')\nx_train = sequence.pad_sequences(x_train, maxlen=maxlen)\nx_test = sequence.pad_sequences(x_test, maxlen=maxlen)\nprint('x_train shape:', x_train.shape)\nprint('x_test shape:', x_test.shape)","67d5e4bb":"x_train[0]","d71ff15b":"INDEX_FROM=3   # word index offset\n\nword_to_id = imdb.get_word_index()\nword_to_id = {k:(v+INDEX_FROM) for k,v in word_to_id.items()}\nword_to_id[\"<PAD>\"] = 0\nword_to_id[\"<START>\"] = 1\nword_to_id[\"<UNK>\"] = 2\n\nid_to_word = {value:key for key,value in word_to_id.items()}\nprint(' '.join(id_to_word[id] for id in x_train[10] ))","a944b7fd":"print('Build model...')\nmodel = Sequential()\nmodel.add(Embedding(max_features, 8))\nmodel.add(LSTM(16, dropout=0.2, recurrent_dropout=0.2))\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.summary()","2d11d6f4":"# try using different optimizers and different optimizer configs\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n\n# Write the training input and output, batch size, and testing input and output\n\nmodel.fit(x_train, y_train, \n          batch_size=batch_size, \n          epochs=1, \n          validation_data=(x_test, y_test))","d3e48a75":"score, acc = model.evaluate(x_test, y_test, batch_size=batch_size)\nprint('Test score:', score)\nprint('Test accuracy:', acc)","4ef701b5":"prediction = model.predict(x_test[22220:22221])\nprint('Prediction value:',prediction[0])\nprint('Test Label:',y_test[22220:22221])","20188122":"# Credits to Peter Nagy","1a13872f":"!wget https:\/\/notebooks.azure.com\/vipulmishra\/projects\/labgail\/raw\/Senti.csv","e3d24914":"import pandas as pd\ndata = pd.read_csv('Senti.csv')\n# Keeping only the neccessary columns\ndata = data[['text','sentiment']]","d8617287":"data.head(10)","5aa9e8d1":"data = data[data.sentiment != \"Neutral\"]\ndata['text'] = data['text'].apply(lambda x: x.lower())\ndata['text'] = data['text'].apply((lambda x: re.sub('[^a-zA-z0-9\\s]','',x)))\n\nfor idx,row in data.iterrows():\n    row[0] = row[0].replace('rt',' ')\n    \nmax_fatures = 2000\ntokenizer = Tokenizer(nb_words=max_fatures, split=' ')\ntokenizer.fit_on_texts(data['text'].values)\nX = tokenizer.texts_to_sequences(data['text'].values)\nX = pad_sequences(X)","2d3c43bc":"Y = pd.get_dummies(data['sentiment']).values\nX_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.33, random_state = 42)\nprint('Shape of training samples:',X_train.shape,Y_train.shape)\nprint('Shape of testing samples:',X_test.shape,Y_test.shape)","86124f87":"model = Sequential()\nmodel.add(Embedding(max_fatures, 128 ,input_length = X.shape[1], dropout=0.2))\nmodel.add(LSTM(128))\nmodel.add(Dense(2, activation='softmax'))\nmodel.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\nprint(model.summary())","1f74b312":"batch_size = 32\nmodel.fit(X_train, Y_train, epochs = 5, batch_size=batch_size, verbose = 2)","e12723d3":"score,acc = model.evaluate(X_test, Y_test, verbose = 2, batch_size = batch_size)\nprint(\"Score: %.2f\" % (score))\nprint(\"Accuracy: %.2f\" % (acc))","64be2828":"text = 'We are going to Delhi'\ntester = np.array([text])\ntester = pd.DataFrame(tester)\ntester.columns = ['text']\n\ntester['text'] = tester['text'].apply(lambda x: x.lower())\ntester['text'] = tester['text'].apply((lambda x: re.sub('[^a-zA-z0-9\\s]','',x)))\n\nmax_fatures = 2000\ntest = tokenizer.texts_to_sequences(tester['text'].values)\ntest = pad_sequences(test)\n\nif X.shape[1]>test.shape[1]:\n    test = np.pad(test[0], (X.shape[1]-test.shape[1],0), 'constant')\n    \ntest = np.array([test])\n\nprediction = model.predict(test)\nprint('Prediction value:',prediction[0])","30fc96b8":"model = Sequential()\nmodel.add(Embedding(max_features, 8))\nmodel.add(LSTM(8, dropout=0.0, recurrent_dropout=0.0))\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.summary()\n\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\nmodel.fit(x_train, y_train, batch_size=batch_size, epochs=1, validation_data=(x_test, y_test))\n\nscore, acc = model.evaluate(x_test, y_test, batch_size=batch_size)\nprint('Test score:', score)\nprint('Test accuracy:', acc)","0d46c84a":"# Write your code here \n\n# Use the same layer design from the above cell ","a7dc2638":"model = Sequential()\nmodel.add(Embedding(max_features, 4))\nmodel.add(LSTM(16, dropout=0.0, recurrent_dropout=0.0))\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.summary()\n\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\nmodel.fit(x_train, y_train, batch_size=batch_size, epochs=1, validation_data=(x_test, y_test))\n\nscore, acc = model.evaluate(x_test, y_test, batch_size=batch_size)\nprint('Test score:', score)\nprint('Test accuracy:', acc)","ae92f8c9":"model = Sequential()\nmodel.add(Embedding(max_features, 32))\nmodel.add(LSTM(8, dropout=0.5, recurrent_dropout=0.5))\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.summary()\n\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\nmodel.fit(x_train, y_train, batch_size=batch_size, epochs=1, validation_data=(x_test, y_test))\n\nscore, acc = model.evaluate(x_test, y_test, batch_size=batch_size)\nprint('Test score:', score)\nprint('Test accuracy:', acc)","3a54126b":"# Write your code here \n\n# Use the same model design from the above cell ","d4af66fa":"model = Sequential()\nmodel.add(Embedding(max_features, 8))\nmodel.add(LSTM(8, dropout=0.0, recurrent_dropout=0.0, return_sequences=True))\nmodel.add(LSTM(8, dropout=0.0, recurrent_dropout=0.0, return_sequences=True))\nmodel.add(LSTM(8, dropout=0.0, recurrent_dropout=0.0))\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.summary()\n\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\nmodel.fit(x_train, y_train, batch_size=batch_size, epochs=1, validation_data=(x_test, y_test))\n\nscore, acc = model.evaluate(x_test, y_test, batch_size=batch_size)\nprint('Test score:', score)\nprint('Test accuracy:', acc)","5dcfb37f":"# Write your code here \n\n# Use the same node design from the above cell ","97a50d6b":"### Preparing Dataset","adef70b9":"### Other RNN Layers\n\n* keras.layers.RNN(cell, return_sequences=False)\n* keras.layers.SimpleRNN(units, activation='tanh')\n* keras.layers.GRU(units, activation='tanh', recurrent_activation='hard_sigmoid')\n* keras.layers.ConvLSTM2D(filters, kernel_size, strides=(1, 1), padding='valid', )\n* keras.layers.SimpleRNNCell(units, activation='tanh')\n* keras.layers.GRUCell(units, activation='tanh', recurrent_activation='hard_sigmoid')\n* keras.layers.LSTMCell(units, activation='tanh', recurrent_activation='hard_sigmoid')\n* keras.layers.CuDNNGRU(units, kernel_initializer='glorot_uniform')\n* keras.layers.CuDNNLSTM(units, kernel_initializer='glorot_uniform')","41f8076d":"### Training set","82adcecc":"## Influence of Embedding","bf4b075e":"### Format data","466b5c81":"## Multilayered RNNs","5fc56551":"### LSTM with 16 nodes","5ca52d29":"### Visualize data","6db9b0e3":"### RNN with 3 layer LSTM","5ee2ec56":"# Part 3: RNN Design Choices","558c057e":"## Influence of Dropout","b866c8f7":"#  Part 1:  Recurrent Neural Network ","e9d157cc":"### Formatting Test Example","43d1399e":"### RNN with 2 layer LSTM","48a60854":"### Building a Model","1c643f35":"###  Importing packages","04ddd241":"### Dropout with probability 0.5","4290034e":"### Design a model","a0d3edb0":"### Model Training","afa28efe":"### Training ","f8c4ac11":"### LSTM with 8 nodes","63e230d3":"### Prediction","aebdd678":"### Visualize the data","2f13e171":"# Part 2: Recurrent Neural Network with Custom Dataset","eca87203":"### Testing","a54d7167":"### Dropout with probability 0.9","a232deb1":"### Validation","38e1e691":"### Load data","dc2933b5":"## Influence of number of nodes","6214fcf7":"### What are your findings?"}}