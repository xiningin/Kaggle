{"cell_type":{"bde39a0e":"code","fdde666a":"code","cb964a05":"code","e642cdd2":"code","8141acd2":"code","237ae3e2":"code","d4bfa01b":"code","14d03534":"code","d49ac80d":"code","1b1d50b5":"code","153c7e90":"code","e5444b69":"code","50d41793":"code","4fce52db":"code","3e3d072e":"code","ea16131e":"code","97b357fd":"code","60242c75":"code","f1c56648":"code","e6e25961":"code","6df44fe4":"code","1e6f01fc":"code","d9f8964d":"code","501ff7a8":"code","4f1ba847":"code","78d3b6b2":"code","e53922b1":"code","450b1df8":"code","54178318":"code","d83b9f7b":"code","af8596d0":"code","b50abba8":"code","4634ef52":"code","fa4dc0aa":"code","2757a714":"code","2afe27c4":"code","e044be62":"code","26838200":"code","0bbbd09e":"code","e0dbec08":"code","c5b2f0aa":"code","3a8da062":"code","d696239b":"code","31fb5540":"code","ab076d96":"code","d3358d6f":"code","b3dac2f7":"code","1b98f952":"code","6b443a22":"code","4dbebd21":"code","cc2e3b3f":"code","7008f96e":"code","557aac1e":"code","bafad779":"code","00e18edc":"code","647cc505":"code","6799f1ea":"code","68c4c414":"code","80edea21":"code","199ad859":"code","33f2612b":"code","9589ef4c":"code","cbc97a24":"code","ac3d73f7":"code","45569462":"code","ad36f2d0":"code","753d0c9d":"code","4a72a0eb":"code","60418f8d":"code","45395055":"code","479979f5":"code","94040414":"code","97f839ab":"code","59c018c9":"code","b004ee7f":"code","8f0049b7":"code","fc8a55f7":"code","908ecb4b":"code","162c686a":"code","aa3f77f7":"code","d0b5afc6":"code","0f35b9ac":"code","592e1c03":"code","30fb2abd":"code","bca05186":"code","ecef6983":"code","565def2f":"code","c42dd97d":"code","d2ac6e53":"code","31285dad":"code","e722a21d":"code","8594158b":"code","a6bdcc6a":"code","688cdd01":"code","44a8b6a6":"code","33a8b097":"code","aa3e5a35":"code","52e43943":"code","7ed2a7ff":"code","06d34476":"code","cf3065ce":"code","0af71516":"code","3aee9fb1":"code","7534114c":"code","2170e25a":"code","d434d54b":"code","7720cd3e":"code","ae6ab5a5":"code","ed43c6ba":"code","f6defb88":"code","08992650":"code","c7d68f89":"code","80a9947b":"code","cb9d7cf0":"code","2bd486e5":"code","77d4530f":"code","30782757":"code","5b65c8da":"code","4d2dcb23":"code","1b0c5715":"code","b0bee75c":"code","c10ad1ca":"code","01c2398c":"code","8b4fe829":"code","fbd896c9":"code","5ea10a10":"code","1056545d":"code","4f9a9489":"code","e8a4d5eb":"code","254819b3":"code","c07ae4df":"code","926a823f":"code","c654c167":"code","5ac71ae5":"code","9319d01f":"code","f6d49638":"code","3750f233":"code","ac6e8768":"code","b6575b5c":"code","64a5b743":"code","297542fe":"code","9c6c8a45":"code","e74efbd8":"code","a7462449":"code","090efbcb":"code","1faa065f":"code","0cb62cc7":"code","c89d0100":"code","2de5da1f":"code","8840aa59":"code","497e1e6d":"code","49e9f4a6":"code","a1b8a4aa":"code","ee4611bb":"code","f8366367":"code","de9b808d":"code","ff02b89e":"code","ab049983":"code","7068884b":"code","060075a5":"code","f2922b8f":"code","252b95b6":"code","36a48be1":"code","a44c65e6":"code","3adf7dbb":"code","f31844b9":"code","98109f43":"code","13924365":"code","0a5e857d":"code","be727a05":"code","71ebc401":"code","5fe1d3d3":"code","2eb037bb":"code","75866a98":"code","77f6d2fe":"code","d968d4d2":"code","671ed442":"code","f855caf8":"code","5cd75bfb":"code","4f24806d":"code","41d01ff7":"code","8389e9b4":"code","ff6aef85":"code","3bc862b6":"code","a4ac7c91":"markdown","76f8dfd4":"markdown","80eb0f19":"markdown","9aa7f82f":"markdown","aeb53787":"markdown","4c938fea":"markdown","2644f575":"markdown","42301c66":"markdown","485c6bb0":"markdown","c3c27c2d":"markdown","a843d3b1":"markdown","56de1bbb":"markdown","47b581a1":"markdown","19cada8b":"markdown","09d3bece":"markdown","ccbc2340":"markdown","566650f5":"markdown","18ec9cc3":"markdown","80fb8ea0":"markdown","1182f85b":"markdown","15d5960e":"markdown","143458ab":"markdown","7694aab8":"markdown","3ca829ec":"markdown","809814a5":"markdown","caafe8b9":"markdown","f22297ec":"markdown","a4792f81":"markdown","2fdba7eb":"markdown","9f4005ef":"markdown","d6555466":"markdown","fa11dfb3":"markdown","5e02b4fd":"markdown","273a546d":"markdown","eaab3465":"markdown","502a62e2":"markdown","93bb5755":"markdown","570f5899":"markdown","8e28247e":"markdown","20b7f40d":"markdown","118350f1":"markdown","446018a5":"markdown","352ca078":"markdown","916ba622":"markdown","65a7579c":"markdown","a475c55c":"markdown","a034928d":"markdown","1722d198":"markdown","b5681846":"markdown","a37ceee6":"markdown","81b1ea98":"markdown","6285a253":"markdown","941ee004":"markdown","f9e96a22":"markdown","0f8c66e7":"markdown","9f87130f":"markdown","15029cd3":"markdown","c0a1a415":"markdown","fdb6607b":"markdown","e4cc1973":"markdown","c2ee133b":"markdown","3206ba91":"markdown","ff32f0e3":"markdown","29d04b49":"markdown","270aec7b":"markdown","409a2405":"markdown","792ce029":"markdown","5ff23742":"markdown","499fb607":"markdown","0df728c8":"markdown","51adcbb0":"markdown","611554bc":"markdown","b555c124":"markdown","6a6b7c0f":"markdown","1a061bb5":"markdown","a89c6de2":"markdown","5fc1683c":"markdown","f3cfd3ca":"markdown","1f6275a5":"markdown","fd9ac149":"markdown","89eadd65":"markdown","e74a6c79":"markdown","d369b4ed":"markdown","85cf4b8f":"markdown","23c13f78":"markdown","8ea36cf5":"markdown","5b195783":"markdown","a89931a1":"markdown","31e6e4a0":"markdown","b6cd967d":"markdown","ee0520ef":"markdown","820fbd0b":"markdown","05cd5b1b":"markdown","db46fb64":"markdown","30c5d381":"markdown","61876d0c":"markdown","ba52565b":"markdown","e79ea2ed":"markdown","0ec5361a":"markdown","575ba6e9":"markdown","0ed76a2a":"markdown","39748451":"markdown","19a2984f":"markdown","a3b11ea3":"markdown","4cef8f6b":"markdown","bd5a1a17":"markdown","2c158fe8":"markdown","36b1e0d7":"markdown","f35ac8c5":"markdown","7e4b810c":"markdown","bf8f4f54":"markdown","335ab0d7":"markdown","36801585":"markdown","2c13bdfb":"markdown","3c578ece":"markdown","accd5d16":"markdown","78078bfe":"markdown","324753ad":"markdown","e66f3e1d":"markdown","247a253c":"markdown","6d8a847f":"markdown","ac2457e1":"markdown","1ea6ee7f":"markdown","d2d80dec":"markdown","a35624fd":"markdown","875af4d5":"markdown","fa834f70":"markdown","b9c94851":"markdown","1b6ae094":"markdown","665abc1f":"markdown","8d03fb66":"markdown","61739b19":"markdown","d3282e34":"markdown","256b1f4b":"markdown","746592b5":"markdown","2ddb765d":"markdown","03ad331a":"markdown","5dc9b429":"markdown","1108dad6":"markdown","d587aae9":"markdown","98161de1":"markdown","2b917271":"markdown","c746eccc":"markdown","8a6a597b":"markdown","127d3fc4":"markdown","9aac1271":"markdown","daa852fd":"markdown","6769181a":"markdown","3a8ec341":"markdown","5d67c3f0":"markdown","480f1a55":"markdown","9dc37fff":"markdown","2c4577ea":"markdown","33f2a846":"markdown","d0e81369":"markdown","ca497e7b":"markdown","87ace94b":"markdown","3a1ac6a6":"markdown","3e3629a3":"markdown","def49f65":"markdown","53e9d952":"markdown","426acd02":"markdown","a81b1f51":"markdown","8f4c2d53":"markdown","d632dd3e":"markdown","2fbcc957":"markdown","ed146d33":"markdown","95492516":"markdown","3663bbf9":"markdown","1dcd945d":"markdown","939a7674":"markdown","5e6848a1":"markdown","138d2368":"markdown","95144ff6":"markdown","9505bcf6":"markdown","bffb9954":"markdown","b6cfaab3":"markdown","0f3fbe7d":"markdown","dfea83db":"markdown","f7e1c048":"markdown","6cfa921c":"markdown","a2c6887e":"markdown","5c17187b":"markdown","59f3b609":"markdown","06894262":"markdown","f7a91fb2":"markdown","3262e0c5":"markdown","ec4aa877":"markdown","f4116bf6":"markdown"},"source":{"bde39a0e":"!pip install seaborn --upgrade","fdde666a":"\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline \nimport seaborn as sns\nimport scipy.stats as stats\n\nfrom sklearn import preprocessing\nfrom sklearn import metrics\nfrom sklearn import model_selection\nfrom sklearn import feature_selection\nfrom imblearn.over_sampling import SMOTE\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.tree import export_graphviz\n\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import StackingClassifier\nfrom sklearn.ensemble import VotingClassifier\n\nfrom xgboost import XGBClassifier\nfrom xgboost import to_graphviz\n\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nfrom os import system\nfrom IPython.display import Image \nfrom sklearn import tree\n\n\nimport warnings\nwarnings.filterwarnings('ignore')","cb964a05":"sns.__version__","e642cdd2":"missing_value_formats = [\"n.a.\",\"?\",\"NA\",\"n\/a\",\"na\",\"N.A.\",\"nan\",\"NAN\",\"--\",\" \", \"  \",\"@\",\".\",\"-\"]\noriginal_dataset = pd.read_csv('..\/input\/parkinsons-data-set\/parkinsons.data', na_values = missing_value_formats)","8141acd2":"pd.set_option('max_columns', None)","237ae3e2":"original_dataset.shape","d4bfa01b":"original_dataset.info()","14d03534":"original_dataset.head()","d49ac80d":"cols = list(original_dataset.columns.values) \nst=cols.pop(cols.index('status')) \noriginal_dataset = original_dataset[cols+['status']]\noriginal_dataset.head()","1b1d50b5":"original_dataset.tail()","153c7e90":"original_dataset.sample(10)","e5444b69":"original_dataset.describe().T","50d41793":"GroupedByStatus = original_dataset.groupby('status')","4fce52db":"GroupedByStatus.mean()","3e3d072e":"for feature in original_dataset.columns:\n     if((feature != 'name') and (feature != 'status')):\n            print(feature,\"===>\",original_dataset[feature].skew())    ","ea16131e":"original_dataset.isnull().any()","97b357fd":"original_dataset.isna().any()","60242c75":"original_dataset.duplicated().any()","f1c56648":"feature = 'MDVP:Fo(Hz)'\nmean_minus_median = np.round(original_dataset[feature].mean(),4)  -  np.round((original_dataset[feature].median()),4)\nprint(feature,\" Mean - Median = \",np.round(mean_minus_median,4))\nminus3std = np.round(original_dataset[feature].mean(),4)  -  np.round((3*original_dataset[feature].std()),4)\nprint(feature,\" 3 Standard Deviation below mean \",np.round(minus3std,4))\nplus3std = np.round(original_dataset[feature].mean(),4) + np.round((3*original_dataset[feature].std()),4)\nprint(feature,\" 3 Standard Deviation above mean \",np.round(plus3std,4))\nsns.displot(data=original_dataset, x=feature, hue=\"status\", kind=\"hist\",multiple=\"stack\")\nplt.show()\nsns.displot(data=original_dataset, x=feature, hue=\"status\", kind=\"kde\",rug=True)\nplt.show()\n    ","e6e25961":"feature = 'MDVP:Fhi(Hz)'\nmean_minus_median = np.round(original_dataset[feature].mean(),4)  -  np.round((original_dataset[feature].median()),4)\nprint(feature,\" Mean - Median = \",np.round(mean_minus_median,4))\nminus3std = np.round(original_dataset[feature].mean(),4)  -  np.round((3*original_dataset[feature].std()),4)\nprint(feature,\" 3 Standard Deviation below mean \",np.round(minus3std,4))\nplus3std = np.round(original_dataset[feature].mean(),4) + np.round((3*original_dataset[feature].std()),4)\nprint(feature,\" 3 Standard Deviation above mean \",np.round(plus3std,4))\nsns.displot(data=original_dataset, x=feature, hue=\"status\", kind=\"hist\",multiple=\"stack\")\nplt.show()\nsns.displot(data=original_dataset, x=feature, hue=\"status\", kind=\"kde\",rug=True)\nplt.show()\n    ","6df44fe4":"feature = 'MDVP:Flo(Hz)'\nmean_minus_median = np.round(original_dataset[feature].mean(),4)  -  np.round((original_dataset[feature].median()),4)\nprint(feature,\" Mean - Median = \",np.round(mean_minus_median,4))\nminus3std = np.round(original_dataset[feature].mean(),4)  -  np.round((3*original_dataset[feature].std()),4)\nprint(feature,\" 3 Standard Deviation below mean \",np.round(minus3std,4))\nplus3std = np.round(original_dataset[feature].mean(),4) + np.round((3*original_dataset[feature].std()),4)\nprint(feature,\" 3 Standard Deviation above mean \",np.round(plus3std,4))\nsns.displot(data=original_dataset, x=feature, hue=\"status\", kind=\"hist\",multiple=\"stack\")\nplt.show()\nsns.displot(data=original_dataset, x=feature, hue=\"status\", kind=\"kde\",rug=True)\nplt.show()\n    ","1e6f01fc":"feature = 'MDVP:Jitter(%)'\nmean_minus_median = np.round(original_dataset[feature].mean(),4)  -  np.round((original_dataset[feature].median()),4)\nprint(feature,\" Mean - Median = \",np.round(mean_minus_median,4))\nminus3std = np.round(original_dataset[feature].mean(),4)  -  np.round((3*original_dataset[feature].std()),4)\nprint(feature,\" 3 Standard Deviation below mean \",np.round(minus3std,4))\nplus3std = np.round(original_dataset[feature].mean(),4) + np.round((3*original_dataset[feature].std()),4)\nprint(feature,\" 3 Standard Deviation above mean \",np.round(plus3std,4))\nsns.displot(data=original_dataset, x=feature, hue=\"status\", kind=\"hist\",multiple=\"stack\")\nplt.show()\nsns.displot(data=original_dataset, x=feature, hue=\"status\", kind=\"kde\",rug=True)\nplt.show()\n    ","d9f8964d":"feature = 'MDVP:Jitter(Abs)'\nmean_minus_median = np.round(original_dataset[feature].mean(),4)  -  np.round((original_dataset[feature].median()),4)\nprint(feature,\" Mean - Median = \",np.round(mean_minus_median,4))\nminus3std = np.round(original_dataset[feature].mean(),4)  -  np.round((3*original_dataset[feature].std()),4)\nprint(feature,\" 3 Standard Deviation below mean \",np.round(minus3std,4))\nplus3std = np.round(original_dataset[feature].mean(),4) + np.round((3*original_dataset[feature].std()),4)\nprint(feature,\" 3 Standard Deviation above mean \",np.round(plus3std,4))\nsns.displot(data=original_dataset, x=feature, hue=\"status\", kind=\"hist\",multiple=\"stack\")\nplt.show()\nsns.displot(data=original_dataset, x=feature, hue=\"status\", kind=\"kde\",rug=True)\nplt.show()\n    ","501ff7a8":"feature = 'MDVP:RAP'\nmean_minus_median = np.round(original_dataset[feature].mean(),4)  -  np.round((original_dataset[feature].median()),4)\nprint(feature,\" Mean - Median = \",np.round(mean_minus_median,4))\nminus3std = np.round(original_dataset[feature].mean(),4)  -  np.round((3*original_dataset[feature].std()),4)\nprint(feature,\" 3 Standard Deviation below mean \",np.round(minus3std,4))\nplus3std = np.round(original_dataset[feature].mean(),4) + np.round((3*original_dataset[feature].std()),4)\nprint(feature,\" 3 Standard Deviation above mean \",np.round(plus3std,4))\nsns.displot(data=original_dataset, x=feature, hue=\"status\", kind=\"hist\",multiple=\"stack\")\nplt.show()\nsns.displot(data=original_dataset, x=feature, hue=\"status\", kind=\"kde\",rug=True)\nplt.show()\n    ","4f1ba847":"feature = 'MDVP:PPQ'\nmean_minus_median = np.round(original_dataset[feature].mean(),4)  -  np.round((original_dataset[feature].median()),4)\nprint(feature,\" Mean - Median = \",np.round(mean_minus_median,4))\nminus3std = np.round(original_dataset[feature].mean(),4)  -  np.round((3*original_dataset[feature].std()),4)\nprint(feature,\" 3 Standard Deviation below mean \",np.round(minus3std,4))\nplus3std = np.round(original_dataset[feature].mean(),4) + np.round((3*original_dataset[feature].std()),4)\nprint(feature,\" 3 Standard Deviation above mean \",np.round(plus3std,4))\nsns.displot(data=original_dataset, x=feature, hue=\"status\", kind=\"hist\",multiple=\"stack\")\nplt.show()\nsns.displot(data=original_dataset, x=feature, hue=\"status\", kind=\"kde\",rug=True)\nplt.show()\n    ","78d3b6b2":"feature = 'Jitter:DDP'\nmean_minus_median = np.round(original_dataset[feature].mean(),4)  -  np.round((original_dataset[feature].median()),4)\nprint(feature,\" Mean - Median = \",np.round(mean_minus_median,4))\nminus3std = np.round(original_dataset[feature].mean(),4)  -  np.round((3*original_dataset[feature].std()),4)\nprint(feature,\" 3 Standard Deviation below mean \",np.round(minus3std,4))\nplus3std = np.round(original_dataset[feature].mean(),4) + np.round((3*original_dataset[feature].std()),4)\nprint(feature,\" 3 Standard Deviation above mean \",np.round(plus3std,4))\nsns.displot(data=original_dataset, x=feature, hue=\"status\", kind=\"hist\",multiple=\"stack\")\nplt.show()\nsns.displot(data=original_dataset, x=feature, hue=\"status\", kind=\"kde\",rug=True)\nplt.show()\n    ","e53922b1":"feature = 'MDVP:Shimmer'\nmean_minus_median = np.round(original_dataset[feature].mean(),4)  -  np.round((original_dataset[feature].median()),4)\nprint(feature,\" Mean - Median = \",np.round(mean_minus_median,4))\nminus3std = np.round(original_dataset[feature].mean(),4)  -  np.round((3*original_dataset[feature].std()),4)\nprint(feature,\" 3 Standard Deviation below mean \",np.round(minus3std,4))\nplus3std = np.round(original_dataset[feature].mean(),4) + np.round((3*original_dataset[feature].std()),4)\nprint(feature,\" 3 Standard Deviation above mean \",np.round(plus3std,4))\nsns.displot(data=original_dataset, x=feature, hue=\"status\", kind=\"hist\",multiple=\"stack\")\nplt.show()\nsns.displot(data=original_dataset, x=feature, hue=\"status\", kind=\"kde\",rug=True)\nplt.show()\n    ","450b1df8":"feature = 'MDVP:Shimmer(dB)'\nmean_minus_median = np.round(original_dataset[feature].mean(),4)  -  np.round((original_dataset[feature].median()),4)\nprint(feature,\" Mean - Median = \",np.round(mean_minus_median,4))\nminus3std = np.round(original_dataset[feature].mean(),4)  -  np.round((3*original_dataset[feature].std()),4)\nprint(feature,\" 3 Standard Deviation below mean \",np.round(minus3std,4))\nplus3std = np.round(original_dataset[feature].mean(),4) + np.round((3*original_dataset[feature].std()),4)\nprint(feature,\" 3 Standard Deviation above mean \",np.round(plus3std,4))\nsns.displot(data=original_dataset, x=feature, hue=\"status\", kind=\"hist\",multiple=\"stack\")\nplt.show()\nsns.displot(data=original_dataset, x=feature, hue=\"status\", kind=\"kde\",rug=True)\nplt.show()\n    ","54178318":"feature = 'Shimmer:APQ3'\nmean_minus_median = np.round(original_dataset[feature].mean(),4)  -  np.round((original_dataset[feature].median()),4)\nprint(feature,\" Mean - Median = \",np.round(mean_minus_median,4))\nminus3std = np.round(original_dataset[feature].mean(),4)  -  np.round((3*original_dataset[feature].std()),4)\nprint(feature,\" 3 Standard Deviation below mean \",np.round(minus3std,4))\nplus3std = np.round(original_dataset[feature].mean(),4) + np.round((3*original_dataset[feature].std()),4)\nprint(feature,\" 3 Standard Deviation above mean \",np.round(plus3std,4))\nsns.displot(data=original_dataset, x=feature, hue=\"status\", kind=\"hist\",multiple=\"stack\")\nplt.show()\nsns.displot(data=original_dataset, x=feature, hue=\"status\", kind=\"kde\",rug=True)\nplt.show()\n    ","d83b9f7b":"feature = 'Shimmer:APQ5'\nmean_minus_median = np.round(original_dataset[feature].mean(),4)  -  np.round((original_dataset[feature].median()),4)\nprint(feature,\" Mean - Median = \",np.round(mean_minus_median,4))\nminus3std = np.round(original_dataset[feature].mean(),4)  -  np.round((3*original_dataset[feature].std()),4)\nprint(feature,\" 3 Standard Deviation below mean \",np.round(minus3std,4))\nplus3std = np.round(original_dataset[feature].mean(),4) + np.round((3*original_dataset[feature].std()),4)\nprint(feature,\" 3 Standard Deviation above mean \",np.round(plus3std,4))\nsns.displot(data=original_dataset, x=feature, hue=\"status\", kind=\"hist\",multiple=\"stack\")\nplt.show()\nsns.displot(data=original_dataset, x=feature, hue=\"status\", kind=\"kde\",rug=True)\nplt.show()\n    ","af8596d0":"feature = 'MDVP:APQ'\nmean_minus_median = np.round(original_dataset[feature].mean(),4)  -  np.round((original_dataset[feature].median()),4)\nprint(feature,\" Mean - Median = \",np.round(mean_minus_median,4))\nminus3std = np.round(original_dataset[feature].mean(),4)  -  np.round((3*original_dataset[feature].std()),4)\nprint(feature,\" 3 Standard Deviation below mean \",np.round(minus3std,4))\nplus3std = np.round(original_dataset[feature].mean(),4) + np.round((3*original_dataset[feature].std()),4)\nprint(feature,\" 3 Standard Deviation above mean \",np.round(plus3std,4))\nsns.displot(data=original_dataset, x=feature, hue=\"status\", kind=\"hist\",multiple=\"stack\")\nplt.show()\nsns.displot(data=original_dataset, x=feature, hue=\"status\", kind=\"kde\",rug=True)\nplt.show()\n    ","b50abba8":"feature = 'Shimmer:DDA'\nmean_minus_median = np.round(original_dataset[feature].mean(),4)  -  np.round((original_dataset[feature].median()),4)\nprint(feature,\" Mean - Median = \",np.round(mean_minus_median,4))\nminus3std = np.round(original_dataset[feature].mean(),4)  -  np.round((3*original_dataset[feature].std()),4)\nprint(feature,\" 3 Standard Deviation below mean \",np.round(minus3std,4))\nplus3std = np.round(original_dataset[feature].mean(),4) + np.round((3*original_dataset[feature].std()),4)\nprint(feature,\" 3 Standard Deviation above mean \",np.round(plus3std,4))\nsns.displot(data=original_dataset, x=feature, hue=\"status\", kind=\"hist\",multiple=\"stack\")\nplt.show()\nsns.displot(data=original_dataset, x=feature, hue=\"status\", kind=\"kde\",rug=True)\nplt.show()\n    ","4634ef52":"feature = 'NHR'\nmean_minus_median = np.round(original_dataset[feature].mean(),4)  -  np.round((original_dataset[feature].median()),4)\nprint(feature,\" Mean - Median = \",np.round(mean_minus_median,4))\nminus3std = np.round(original_dataset[feature].mean(),4)  -  np.round((3*original_dataset[feature].std()),4)\nprint(feature,\" 3 Standard Deviation below mean \",np.round(minus3std,4))\nplus3std = np.round(original_dataset[feature].mean(),4) + np.round((3*original_dataset[feature].std()),4)\nprint(feature,\" 3 Standard Deviation above mean \",np.round(plus3std,4))\nsns.displot(data=original_dataset, x=feature, hue=\"status\", kind=\"hist\",multiple=\"stack\")\nplt.show()\nsns.displot(data=original_dataset, x=feature, hue=\"status\", kind=\"kde\",rug=True)\nplt.show()\n    ","fa4dc0aa":"feature = 'HNR'\nmean_minus_median = np.round(original_dataset[feature].mean(),4)  -  np.round((original_dataset[feature].median()),4)\nprint(feature,\" Mean - Median = \",np.round(mean_minus_median,4))\nminus3std = np.round(original_dataset[feature].mean(),4)  -  np.round((3*original_dataset[feature].std()),4)\nprint(feature,\" 3 Standard Deviation below mean \",np.round(minus3std,4))\nplus3std = np.round(original_dataset[feature].mean(),4) + np.round((3*original_dataset[feature].std()),4)\nprint(feature,\" 3 Standard Deviation above mean \",np.round(plus3std,4))\nsns.displot(data=original_dataset, x=feature, hue=\"status\", kind=\"hist\",multiple=\"stack\")\nplt.show()\nsns.displot(data=original_dataset, x=feature, hue=\"status\", kind=\"kde\",rug=True)\nplt.show()\n    ","2757a714":"feature = 'RPDE'\nmean_minus_median = np.round(original_dataset[feature].mean(),4)  -  np.round((original_dataset[feature].median()),4)\nprint(feature,\" Mean - Median = \",np.round(mean_minus_median,4))\nminus3std = np.round(original_dataset[feature].mean(),4)  -  np.round((3*original_dataset[feature].std()),4)\nprint(feature,\" 3 Standard Deviation below mean \",np.round(minus3std,4))\nplus3std = np.round(original_dataset[feature].mean(),4) + np.round((3*original_dataset[feature].std()),4)\nprint(feature,\" 3 Standard Deviation above mean \",np.round(plus3std,4))\nsns.displot(data=original_dataset, x=feature, hue=\"status\", kind=\"hist\",multiple=\"stack\")\nplt.show()\nsns.displot(data=original_dataset, x=feature, hue=\"status\", kind=\"kde\",rug=True)\nplt.show()\n    ","2afe27c4":"feature = 'DFA'\nmean_minus_median = np.round(original_dataset[feature].mean(),4)  -  np.round((original_dataset[feature].median()),4)\nprint(feature,\" Mean - Median = \",np.round(mean_minus_median,4))\nminus3std = np.round(original_dataset[feature].mean(),4)  -  np.round((3*original_dataset[feature].std()),4)\nprint(feature,\" 3 Standard Deviation below mean \",np.round(minus3std,4))\nplus3std = np.round(original_dataset[feature].mean(),4) + np.round((3*original_dataset[feature].std()),4)\nprint(feature,\" 3 Standard Deviation above mean \",np.round(plus3std,4))\nsns.displot(data=original_dataset, x=feature, hue=\"status\", kind=\"hist\",multiple=\"stack\")\nplt.show()\nsns.displot(data=original_dataset, x=feature, hue=\"status\", kind=\"kde\",rug=True)\nplt.show()\n    ","e044be62":"feature = 'spread1'\nmean_minus_median = np.round(original_dataset[feature].mean(),4)  -  np.round((original_dataset[feature].median()),4)\nprint(feature,\" Mean - Median = \",np.round(mean_minus_median,4))\nminus3std = np.round(original_dataset[feature].mean(),4)  -  np.round((3*original_dataset[feature].std()),4)\nprint(feature,\" 3 Standard Deviation below mean \",np.round(minus3std,4))\nplus3std = np.round(original_dataset[feature].mean(),4) + np.round((3*original_dataset[feature].std()),4)\nprint(feature,\" 3 Standard Deviation above mean \",np.round(plus3std,4))\nsns.displot(data=original_dataset, x=feature, hue=\"status\", kind=\"hist\",multiple=\"stack\")\nplt.show()\nsns.displot(data=original_dataset, x=feature, hue=\"status\", kind=\"kde\",rug=True)\nplt.show()\n    ","26838200":"feature = 'spread2'\nmean_minus_median = np.round(original_dataset[feature].mean(),4)  -  np.round((original_dataset[feature].median()),4)\nprint(feature,\" Mean - Median = \",np.round(mean_minus_median,4))\nminus3std = np.round(original_dataset[feature].mean(),4)  -  np.round((3*original_dataset[feature].std()),4)\nprint(feature,\" 3 Standard Deviation below mean \",np.round(minus3std,4))\nplus3std = np.round(original_dataset[feature].mean(),4) + np.round((3*original_dataset[feature].std()),4)\nprint(feature,\" 3 Standard Deviation above mean \",np.round(plus3std,4))\nsns.displot(data=original_dataset, x=feature, hue=\"status\", kind=\"hist\",multiple=\"stack\")\nplt.show()\nsns.displot(data=original_dataset, x=feature, hue=\"status\", kind=\"kde\",rug=True)\nplt.show()\n    ","0bbbd09e":"feature = 'D2'\nmean_minus_median = np.round(original_dataset[feature].mean(),4)  -  np.round((original_dataset[feature].median()),4)\nprint(feature,\" Mean - Median = \",np.round(mean_minus_median,4))\nminus3std = np.round(original_dataset[feature].mean(),4)  -  np.round((3*original_dataset[feature].std()),4)\nprint(feature,\" 3 Standard Deviation below mean \",np.round(minus3std,4))\nplus3std = np.round(original_dataset[feature].mean(),4) + np.round((3*original_dataset[feature].std()),4)\nprint(feature,\" 3 Standard Deviation above mean \",np.round(plus3std,4))\nsns.displot(data=original_dataset, x=feature, hue=\"status\", kind=\"hist\",multiple=\"stack\")\nplt.show()\nsns.displot(data=original_dataset, x=feature, hue=\"status\", kind=\"kde\",rug=True)\nplt.show()\n    ","e0dbec08":"feature = 'PPE'\nmean_minus_median = np.round(original_dataset[feature].mean(),4)  -  np.round((original_dataset[feature].median()),4)\nprint(feature,\" Mean - Median = \",np.round(mean_minus_median,4))\nminus3std = np.round(original_dataset[feature].mean(),4)  -  np.round((3*original_dataset[feature].std()),4)\nprint(feature,\" 3 Standard Deviation below mean \",np.round(minus3std,4))\nplus3std = np.round(original_dataset[feature].mean(),4) + np.round((3*original_dataset[feature].std()),4)\nprint(feature,\" 3 Standard Deviation above mean \",np.round(plus3std,4))\nsns.displot(data=original_dataset, x=feature, hue=\"status\", kind=\"hist\",multiple=\"stack\")\nplt.show()\nsns.displot(data=original_dataset, x=feature, hue=\"status\", kind=\"kde\",rug=True)\nplt.show()\n    ","c5b2f0aa":"pd.crosstab(original_dataset['status'],columns='count')","3a8da062":"original_dataset['status'].value_counts()","d696239b":"sns.countplot(x='status',data=original_dataset)\nplt.show()","31fb5540":"for feature in original_dataset.columns:\n    if(feature not in ['name','status']):\n        sns.boxplot(x=feature,hue='status',data=original_dataset)\n        plt.show()","ab076d96":"for feature in original_dataset.columns:\n    if (feature not in ['name','status']):\n        sns.barplot(x='status',y=feature,data=original_dataset)\n        plt.show()","d3358d6f":"plt.figure(figsize=(20,10))\nsns.heatmap(original_dataset.corr(),annot=True)\nplt.show()","b3dac2f7":"sns.pairplot(data=original_dataset,hue='status',corner=True)\nplt.show()","1b98f952":"def binning(column, cut_points, labels=None):\n    minval = column.min()\n    maxval = column.max()\n    break_points = [minval] + cut_points + [maxval]    \n    if not labels:\n        labels = range(len(cut_points)+1)   \n    binned_column = pd.cut(column, bins=break_points, labels=labels, include_lowest=True)\n    return binned_column","6b443a22":"binned_dataset = original_dataset.copy()","4dbebd21":"feature = 'MDVP:Fo(Hz)'\nprint(original_dataset[feature].min())\nprint(original_dataset[feature].max())","cc2e3b3f":"np.linspace(100,250,4)","7008f96e":"cut_points = [100,150,200,250]\nlabels = [\"Low\",\"Medium\",\"High\",\"Very High\",\"Extremely High\"]","557aac1e":"binned_dataset[\"MDVP:Fo(Hz)\"] = binning(original_dataset[\"MDVP:Fo(Hz)\"], cut_points, labels)","bafad779":"feature = 'MDVP:Flo(Hz)'\nprint(original_dataset[feature].min())\nprint(original_dataset[feature].max())","00e18edc":"np.linspace(70,220,4)","647cc505":"cut_points = [70,120,170,220]\nlabels = [\"Low\",\"Medium\",\"High\",\"Very High\",\"Extremely High\"]","6799f1ea":"binned_dataset[\"MDVP:Flo(Hz)\"] = binning(original_dataset[\"MDVP:Flo(Hz)\"], cut_points, labels)","68c4c414":"feature = 'PPE'\nprint(original_dataset[feature].min())\nprint(original_dataset[feature].max())","80edea21":"np.linspace(0.05,0.50,4)","199ad859":"cut_points = [0.05, 0.2 , 0.35, 0.5 ]\nlabels = [\"Low\",\"Medium\",\"High\",\"Very High\",\"Extremely High\"]","33f2612b":"binned_dataset[\"PPE\"] = binning(original_dataset[\"PPE\"], cut_points, labels)","9589ef4c":"plt.figure(figsize=(20,6))\nsns.swarmplot(x=binned_dataset['MDVP:Fo(Hz)'],y=original_dataset['MDVP:Flo(Hz)'],hue=original_dataset['status'] )\nplt.show()","cbc97a24":"plt.figure(figsize=(20,6))\nsns.swarmplot(x=binned_dataset['MDVP:Fo(Hz)'],y=original_dataset['PPE'],hue=original_dataset['status'] )\nplt.show()","ac3d73f7":"plt.figure(figsize=(20,6))\nsns.swarmplot(x=binned_dataset['MDVP:Flo(Hz)'],y=original_dataset['PPE'],hue=original_dataset['status'] )\nplt.show()","45569462":"plt.figure(figsize=(20,6))\nsns.swarmplot(x=binned_dataset['MDVP:Fo(Hz)'],y=original_dataset['spread1'],hue=original_dataset['status'] )\nplt.show()","ad36f2d0":"plt.figure(figsize=(20,6))\nsns.swarmplot(x=binned_dataset['MDVP:Flo(Hz)'],y=original_dataset['spread1'],hue=original_dataset['status'] )\nplt.show()","753d0c9d":"plt.figure(figsize=(20,6))\nsns.swarmplot(x=binned_dataset['PPE'],y=original_dataset['spread1'],hue=original_dataset['status'] )\nplt.show()","4a72a0eb":"plt.figure(figsize=(20,6))\nsns.swarmplot(x=binned_dataset['MDVP:Fo(Hz)'],y=original_dataset['spread2'],hue=original_dataset['status'] )\nplt.show()","60418f8d":"plt.figure(figsize=(20,6))\nsns.swarmplot(x=binned_dataset['MDVP:Flo(Hz)'],y=original_dataset['spread2'],hue=original_dataset['status'] )\nplt.show()","45395055":"plt.figure(figsize=(20,6))\nsns.swarmplot(x=binned_dataset['PPE'],y=original_dataset['spread2'],hue=original_dataset['status'] )\nplt.show()","479979f5":"gb_dataset = original_dataset.copy()","94040414":"gb_dataset.drop('name',axis=1,inplace=True)","97f839ab":"gb_dataset.head()","59c018c9":"GB_X = gb_dataset.iloc[:,:-1]\nGB_y= gb_dataset.iloc[:,-1:]","b004ee7f":"GB_X.head()","8f0049b7":"GB_X_train,GB_X_test,GB_y_train,GB_y_test = model_selection.train_test_split(GB_X,GB_y,test_size=0.3,random_state=1)\ngradientBoost = GradientBoostingClassifier( n_estimators=100,random_state=1)\ngradientBoost.fit(GB_X_train,GB_y_train)\nGB_y_predicted = gradientBoost.predict(GB_X_test)\nGB_y_predicted_proba = gradientBoost.predict_proba(GB_X_test)","fc8a55f7":"conf_matrix  = metrics.confusion_matrix(GB_y_test,GB_y_predicted)\nclass_report = metrics.classification_report(GB_y_test,GB_y_predicted)\ntrain_score  = np.round(gradientBoost.score(GB_X_train,GB_y_train),2)\ntest_score   = np.round(gradientBoost.score(GB_X_test,GB_y_test),2)","908ecb4b":"conf_matrix","162c686a":"print(class_report)","aa3f77f7":"print(\"Train Accuracy \",train_score)\nprint(\"Test Accuracy \",test_score)","d0b5afc6":"selected_feature_dataset = original_dataset.copy()","0f35b9ac":"FS_X=selected_feature_dataset.iloc[:,:-1]\nFS_y=selected_feature_dataset.iloc[:,-1:]","592e1c03":"FS_X.drop(['name'],axis=1,inplace=True)","30fb2abd":"select_k_best = feature_selection.SelectKBest(score_func=feature_selection.f_classif,k=15)\nX_k_best = select_k_best.fit_transform(FS_X,FS_y)","bca05186":"X_k_best.shape","ecef6983":"select_k_best.pvalues_","565def2f":"select_k_best.get_support()","c42dd97d":"FS_X.head()","d2ac6e53":"supportList = select_k_best.get_support().tolist()\np_valuesList = select_k_best.pvalues_.tolist()","31285dad":"toDrop=[]\n\nfor i in np.arange(len(FS_X.columns)):\n    bool = supportList[i]\n    if(bool == False):\n        toDrop.append(FS_X.columns[i])        ","e722a21d":"toDrop","8594158b":"FS_X.drop(toDrop,axis=1,inplace=True)","a6bdcc6a":"FS_X.head()","688cdd01":"x_p_values = pd.DataFrame() \nx_p_values[\"feature\"] = FS_X.columns \nx_p_values[\"P Values\"] = [p_valuesList[i] for i in range(len(FS_X.columns))] \nx_p_values.sort_values(by='P Values')","44a8b6a6":"sns.pairplot(data=FS_X,corner=True)\nplt.show()","33a8b097":"X_VIF = FS_X.columns \n# VIF dataframe \nvif_data = pd.DataFrame() \nvif_data[\"feature\"] = FS_X.columns \n\n# Calculating VIF for each feature \nvif_data[\"VIF\"] = [variance_inflation_factor(FS_X.values, i) for i in range(len(FS_X.columns))] \n  \nvif_data.sort_values(by='VIF')\n","aa3e5a35":"for feature in FS_X.columns:\n    sns.displot(data=original_dataset, x=feature, hue=\"status\", kind=\"kde\",rug=True)\n    plt.show() ","52e43943":"FS_X_train,FS_X_test,FS_y_train,FS_y_test = model_selection.train_test_split(FS_X,FS_y,test_size=0.3,random_state=1)\ngradientBoost = GradientBoostingClassifier( n_estimators=100,random_state=1)\ngradientBoost.fit(FS_X_train,FS_y_train)\nFS_y_predicted = gradientBoost.predict(FS_X_test)\nFS_y_predicted_proba = gradientBoost.predict_proba(FS_X_test)","7ed2a7ff":"conf_matrix  = metrics.confusion_matrix(FS_y_test,FS_y_predicted)\nclass_report = metrics.classification_report(FS_y_test,FS_y_predicted)\ntrain_score  = np.round(gradientBoost.score(FS_X_train,FS_y_train),2)\ntest_score   = np.round(gradientBoost.score(FS_X_test,FS_y_test),2)","06d34476":"conf_matrix","cf3065ce":"print(class_report)","0af71516":"print(\"Train Accuracy \",train_score)\nprint(\"Test Accuracy \",test_score)","3aee9fb1":"smote = SMOTE(random_state=7)\nBalanced_X,Balanced_y = smote.fit_sample(FS_X,FS_y)\nbefore = pd.merge(FS_X,FS_y,right_index=True, left_index=True)\nafter = pd.merge(Balanced_X,Balanced_y,right_index=True, left_index=True)\nb=before['status'].value_counts()\na=after['status'].value_counts()\nprint('Before')\nprint(b)\nprint('After')\nprint(a)","7534114c":"Balanced_X.head()","2170e25a":"Balanced_y.head()","d434d54b":"Bal_X_train,Bal_X_test,Bal_y_train,Bal_y_test = model_selection.train_test_split(Balanced_X,Balanced_y,test_size=0.3,random_state=1)","7720cd3e":"gradientBoost = GradientBoostingClassifier( n_estimators=100,random_state=1)\ngradientBoost.fit(Bal_X_train,Bal_y_train)\nBal_y_predicted = gradientBoost.predict(Bal_X_test)\nBal_y_predicted_proba = gradientBoost.predict_proba(Bal_X_test)","ae6ab5a5":"conf_matrix  = metrics.confusion_matrix(Bal_y_test,Bal_y_predicted)\nclass_report = metrics.classification_report(Bal_y_test,Bal_y_predicted)\ntrain_score  = np.round(gradientBoost.score(Bal_X_train,Bal_y_train),2)\ntest_score   = np.round(gradientBoost.score(Bal_X_test,Bal_y_test),2)","ed43c6ba":"metrics.ConfusionMatrixDisplay(conf_matrix).plot()","f6defb88":"print(class_report)","08992650":"print(\"Train Accuracy \",train_score)\nprint(\"Test Accuracy \",test_score)","c7d68f89":"power = preprocessing.PowerTransformer(method='yeo-johnson', standardize=True)\ndf_transformed = power.fit_transform(Balanced_X)\ntransformedData = pd.DataFrame(df_transformed, columns = Balanced_X.columns)","80a9947b":"for feature in transformedData.columns:\n    if(feature not in ['name','status']):\n        sns.boxplot(x=feature,data=original_dataset)\n        plt.show()\n        sns.boxplot(x=feature,data=transformedData)\n        plt.show()","cb9d7cf0":"for feature in transformedData.columns:\n    sns.displot(data=transformedData, x=feature, kind=\"kde\")\n    plt.show() ","2bd486e5":"power = preprocessing.PowerTransformer(method='yeo-johnson', standardize=True)\ndf_transformed = power.fit_transform(Bal_X_train)\nBal_X_train_ST = pd.DataFrame(df_transformed, columns = Bal_X_train.columns)\n\n\ngradientBoost = GradientBoostingClassifier( n_estimators=100,random_state=1)\ngradientBoost.fit(Bal_X_train_ST,Bal_y_train)\nBal_y_predicted = gradientBoost.predict(Bal_X_test)\nBal_y_predicted_proba = gradientBoost.predict_proba(Bal_X_test)\n\n\nconf_matrix  = metrics.confusion_matrix(Bal_y_test,Bal_y_predicted)\nclass_report = metrics.classification_report(Bal_y_test,Bal_y_predicted)\ntrain_score  = np.round(gradientBoost.score(Bal_X_train_ST,Bal_y_train),2)\ntest_score   = np.round(gradientBoost.score(Bal_X_test,Bal_y_test),2)","77d4530f":"metrics.ConfusionMatrixDisplay(conf_matrix).plot()","30782757":"print(class_report)","5b65c8da":"print(\"Train Accuracy \",train_score)\nprint(\"Test Accuracy \",test_score)","4d2dcb23":"Balanced_X.head()","1b0c5715":"Balanced_y.head()","b0bee75c":"Bal_X_train,Bal_X_test,Bal_y_train,Bal_y_test = model_selection.train_test_split(Balanced_X,Balanced_y,test_size=0.3,random_state=1)","c10ad1ca":"DT = DecisionTreeClassifier(criterion='entropy',random_state = 1)\nDT.fit(Bal_X_train, Bal_y_train)\nDT_y_predicted=DT.predict(Bal_X_test)","01c2398c":"print(\"Decision Tree Train Accuracy\",DT.score(Bal_X_train , Bal_y_train))\n\nprint(\"Decision Tree Test Accuracy\",DT.score(Bal_X_test , Bal_y_test))","8b4fe829":"DT_conf_matrix  = metrics.confusion_matrix(Bal_y_test,DT_y_predicted)\nDT_class_report = metrics.classification_report(Bal_y_test,DT_y_predicted)\nprint(\"Classification Report\")\nprint(\"---------------------\")\nprint(DT_class_report)\nmetrics.ConfusionMatrixDisplay(DT_conf_matrix).plot()\nTP = DT_conf_matrix[0,0]\nTN = DT_conf_matrix[1,1]\nFP = DT_conf_matrix[0,1]\nFN = DT_conf_matrix[1,0]","fbd896c9":"print(TP\/float(TP+FN))\nprint(metrics.recall_score(Bal_y_test,DT_y_predicted))","5ea10a10":"print(TN\/float(TN+FP))","1056545d":"metrics.plot_precision_recall_curve(DT,Bal_X_test , Bal_y_test)\nplt.show()","4f9a9489":"metrics.plot_roc_curve(DT, Bal_X_test , Bal_y_test)\nplt.show()","e8a4d5eb":"feature_Importances_List = DT.feature_importances_.tolist()","254819b3":"x_p_values = pd.DataFrame() \nx_p_values[\"Feature\"] = FS_X.columns \nx_p_values[\"P Values\"] = [p_valuesList[i] for i in range(len(FS_X.columns))] \nx_p_values.sort_values(by='Feature')","c07ae4df":"feature_imp_values = pd.DataFrame() \nfeature_imp_values[\"Feature\"] = FS_X.columns \nfeature_imp_values[\"Importances\"] = [feature_Importances_List[i] for i in range(len(FS_X.columns))] \nfeature_imp_values.sort_values(by='Feature')","926a823f":"train_char_label = ['No', 'Yes']\nexport_graphviz(DT, out_file='Unregularized_tree.dot', feature_names = list(Bal_X_train),class_names = list(train_char_label),   rounded = True, proportion = False, precision = 2, filled = True)","c654c167":"!dot -Tpng Unregularized_tree.dot -o Unregularized_tree.png -Gdpi=600","5ac71ae5":"Image(filename = 'Unregularized_tree.png')","9319d01f":"random_state = np.arange(1,51)\nDT_Random_States_List = []\n\nfor r_state in random_state:\n    DT = DecisionTreeClassifier(criterion='entropy',random_state = r_state)\n    DT_Random_States_List.append(DT)\n\noverall_mean_list = []\n\nfor DecisionTree in DT_Random_States_List:\n    SkFold = model_selection.StratifiedKFold(n_splits=10, random_state=7, shuffle=False)\n    scores = model_selection.cross_val_score(estimator = DecisionTree,X=Balanced_X,y=Balanced_y,scoring='accuracy',cv=SkFold) \n    overall_mean_list.append(np.mean(scores))\n        \n    ","f6d49638":"mean=np.mean(overall_mean_list)\nstandard_deviation=np.std(overall_mean_list)\nprint('mean',mean)\nprint('standard_deviation',standard_deviation)\nn=len(overall_mean_list)\nprint('n',n)\nstandard_error = standard_deviation \/ (np.sqrt(n))\nprint('Standard Error',standard_error)\nMOE = 2.58*standard_error\nprint('MOE',MOE)\nplus =  np.round(mean+MOE,3) \nminus = np.round(mean-MOE,3)\nprint()\nprint(f\"With 99% confidence,we can say that the mean test accuracy is between {plus} and {minus}, based on {n} sample accuracies.\")","3750f233":"DT_Regularized = DecisionTreeClassifier(criterion='entropy',random_state = 1,max_depth=8)\nDT_Regularized.fit(Bal_X_train, Bal_y_train)\nDT_Regularized_y_predicted=DT_Regularized.predict(Bal_X_test)","ac6e8768":"print(\"Decision Tree Train Accuracy\",DT_Regularized.score(Bal_X_train , Bal_y_train))\nprint(\"Decision Tree Test Accuracy\",DT_Regularized.score(Bal_X_test , Bal_y_test))","b6575b5c":"DT_Regularized = DecisionTreeClassifier(criterion='entropy',random_state = 1,max_depth=8,min_samples_leaf=2)\nDT_Regularized.fit(Bal_X_train, Bal_y_train)\nDT_Regularized_y_predicted=DT_Regularized.predict(Bal_X_test)","64a5b743":"print(\"Regularized Decision Tree Train Accuracy\",DT_Regularized.score(Bal_X_train , Bal_y_train))\nprint(\"Regularized Decision Tree Test Accuracy\",DT_Regularized.score(Bal_X_test , Bal_y_test))","297542fe":"DT_Regularized = DecisionTreeClassifier(criterion='entropy',random_state = 1,max_depth=8,min_samples_leaf=3)\nDT_Regularized.fit(Bal_X_train, Bal_y_train)\nDT_Regularized_y_predicted=DT_Regularized.predict(Bal_X_test)","9c6c8a45":"print(\"Decision Tree Train Accuracy\",DT_Regularized.score(Bal_X_train , Bal_y_train))\nprint(\"Decision Tree Test Accuracy\",DT_Regularized.score(Bal_X_test , Bal_y_test))","e74efbd8":"DT_conf_matrix  = metrics.confusion_matrix(Bal_y_test,DT_Regularized_y_predicted)\nDT_class_report = metrics.classification_report(Bal_y_test,DT_Regularized_y_predicted)\nprint(\"Classification Report\")\nprint(\"---------------------\")\nprint(DT_class_report)\nmetrics.ConfusionMatrixDisplay(DT_conf_matrix).plot()\nTP = DT_conf_matrix[0,0]\nTN = DT_conf_matrix[1,1]\nFP = DT_conf_matrix[0,1]\nFN = DT_conf_matrix[1,0]","a7462449":"#Sensitivity\nprint(TP\/float(TP+FN))\nprint(metrics.recall_score(Bal_y_test,DT_Regularized_y_predicted))","090efbcb":"#Specificity\nprint(TN\/float(TN+FP))","1faa065f":"metrics.plot_precision_recall_curve(DT_Regularized,Bal_X_test , Bal_y_test)\nplt.show()","0cb62cc7":"metrics.plot_roc_curve(DT_Regularized, Bal_X_test , Bal_y_test)\nplt.show()","c89d0100":"feature_Importances_List = DT_Regularized.feature_importances_.tolist()","2de5da1f":"feature_imp_values = pd.DataFrame() \nfeature_imp_values[\"Feature\"] = FS_X.columns \nfeature_imp_values[\"Importances\"] = [feature_Importances_List[i] for i in range(len(FS_X.columns))] \nfeature_imp_values.sort_values(by='Feature')","8840aa59":"train_char_label = ['No', 'Yes']\nexport_graphviz(DT_Regularized, out_file='Regularized_tree.dot', feature_names = list(Bal_X_train),class_names = list(train_char_label),   rounded = True, proportion = False, precision = 2, filled = True)\n\n!dot -Tpng Regularized_tree.dot -o Regularized_tree.png -Gdpi=600\n\nImage(filename = 'Regularized_tree.png')","497e1e6d":"random_state = np.arange(1,51)\nDT_Random_States_List = []\n\nfor r_state in random_state:\n    DT_Regularized = DecisionTreeClassifier(criterion='entropy',random_state = r_state,max_depth=8)\n    DT_Random_States_List.append(DT_Regularized)\n\noverall_mean_list = []\n\nfor DecisionTree in DT_Random_States_List:\n    SkFold = model_selection.StratifiedKFold(n_splits=10, random_state=7, shuffle=False)\n    scores = model_selection.cross_val_score(estimator = DecisionTree,X=Balanced_X,y=Balanced_y,scoring='accuracy',cv=SkFold) \n    overall_mean_list.append(np.mean(scores))","49e9f4a6":"mean=np.mean(overall_mean_list)\nstandard_deviation=np.std(overall_mean_list)\nprint('mean',mean)\nprint('standard_deviation',standard_deviation)\nn=len(overall_mean_list)\nprint('n',n)\nstandard_error = standard_deviation \/ (np.sqrt(n))\nprint('Standard Error',standard_error)\nMOE = 2.58*standard_error\nprint('MOE',MOE)\nplus =  np.round(mean+MOE,3) \nminus = np.round(mean-MOE,3)\nprint()\nprint(f\"With 99% confidence,we can say that the mean test accuracy is between {plus} and {minus}, based on {n} sample accuracies.\")","a1b8a4aa":"ran_state = np.arange(1,3)\nneighbours = np.arange(5,7)\n\nk_value_for_plot          = []\ntest_score_for_plot       = []\ntrain_score_for_plot      = []\ntest_accuracy_list        = []\ntrain_accuracy_list       = []\nk_value_list              = []\nrandom_state_list         = []\nconf_matrix_report_list   = []\nclass_report_list         = []\n\nfor r_state in ran_state:\n    \n    # Splitting dataset into training & test set     \n    SkFold = model_selection.StratifiedKFold(n_splits=3, random_state=r_state, shuffle=False)\n    fold = SkFold.split(Balanced_X,Balanced_y)\n   \n    X_trainList = []\n    X_testList = []\n    y_trainList = []\n    y_testList = []\n    \n   \n    \n    for train_index, test_index in fold:\n        X_train = Balanced_X.iloc[train_index]\n        X_test  = Balanced_X.iloc[test_index]\n        y_train = Balanced_y.iloc[train_index]\n        y_test = Balanced_y.iloc[test_index]\n        X_trainList.append(X_train)\n        X_testList.append(X_test)\n        y_trainList.append(y_train)\n        y_testList.append(y_test)\n        \n    for i in np.arange(0,len(X_trainList)):\n        X_trainList[i]\n        X_testList[i]\n        y_trainList[i]\n        y_testList[i] \n        \n        for k_value in neighbours:\n            \n            KNN = KNeighborsClassifier(n_neighbors=k_value,weights='distance',algorithm='auto',metric='minkowski',p=2)\n            KNN.fit(X_trainList[i],y_trainList[i])\n            KNN_y_predicted_proba = KNN.predict_proba(X_testList[i]) \n            KNN_y_predicted_proba = KNN_y_predicted_proba[:,1]\n            KNN_y_predicted = preprocessing.binarize([KNN_y_predicted_proba], 0.3)[0]\n            \n            conf_matrix  = metrics.confusion_matrix(y_testList[i] ,KNN_y_predicted)\n            class_report = metrics.classification_report(y_testList[i],KNN_y_predicted)\n            train_score  = np.round(KNN.score(X_trainList[i],y_trainList[i]),2)\n            test_score   = np.round(KNN.score(X_testList[i],y_testList[i]),2)\n            \n            test_accuracy_list.append(test_score)\n            train_accuracy_list.append(train_score)\n            k_value_list.append(k_value)\n            random_state_list.append(r_state)\n            conf_matrix_report_list.append(conf_matrix)\n            class_report_list.append(class_report)\n            k_value_for_plot.append(k_value)\n            test_score_for_plot.append(test_score)\n            train_score_for_plot.append(train_score)           \n            \n            \n        \n        print('Random State',r_state)\n        print('Stratified K Fold Split No',i)        \n        plt.figure(figsize=(10,5))    \n        plt.title(\"Train Test Accuracy Score\")\n        plt.xlabel('Different Values of K')\n        plt.ylabel('Model score')\n        sns.lineplot(x=k_value_for_plot,y=train_score_for_plot,color ='r', label = \"Training Score\")\n        sns.lineplot(x=k_value_for_plot,y=test_score_for_plot,color  ='b', label = \"Testing Score\")\n        plt.legend()\n        plt.show()\n        k_value_for_plot.clear()\n        test_score_for_plot.clear()\n        train_score_for_plot.clear()\n        \n        test_accuracy_array = np.array(test_accuracy_list)\n        knn_result = np.where(test_accuracy_array>0.80)\n        knn_result = knn_result[0]\n        \n        for r in knn_result:\n            conf = conf_matrix_report_list[r]\n            if((test_accuracy_list[r]>0.80) & (conf[1,0]<10 and conf[0,1]<5)):\n                print('Test Accuracy',test_accuracy_list[r],'Train Accuracy',train_accuracy_list[r],'K Value ' ,k_value_list[r],'Random State ',random_state_list[r])\n                print()\n                print(\"Confusion Matrix \")\n                metrics.ConfusionMatrixDisplay(conf_matrix_report_list[r]).plot()\n                plt.show()\n                print()\n                print(\"Classification Report \")\n                print(class_report_list[r])                                   \n                print(\"--------------------------------------------------------\")  \n                \n        \n        print(\"Precision Recall Curve\")\n        metrics.plot_precision_recall_curve(KNN,X_testList[i],y_testList[i])\n        plt.show()\n        print(\"Receiver Operating Characteristic Curve (ROC)\")\n        metrics.plot_roc_curve(KNN, X_testList[i],y_testList[i])\n        plt.show() \n\n    #Cleaning\n    X_trainList.clear()\n    X_testList.clear()\n    y_trainList.clear()\n    y_testList.clear()\n","ee4611bb":"Bal_X_train,Bal_X_test,Bal_y_train,Bal_y_test = model_selection.train_test_split(Balanced_X,Balanced_y,test_size=0.3,random_state=7)","f8366367":"RF = RandomForestClassifier(n_estimators=10,criterion='entropy',random_state = 1)\nRF.fit(Bal_X_train, Bal_y_train)\nRF_y_predicted_proba = RF.predict_proba(Bal_X_test) \nRF_y_predicted_proba = RF_y_predicted_proba[:,1]\nRF_y_predicted = preprocessing.binarize([RF_y_predicted_proba], 0.4)[0]","de9b808d":"print(\"Random Forest Train Accuracy\",RF.score(Bal_X_train , Bal_y_train))\nprint(\"Random Forest Test Accuracy\",RF.score(Bal_X_test , Bal_y_test))","ff02b89e":"RF_conf_matrix  = metrics.confusion_matrix(Bal_y_test,RF_y_predicted)\nRF_class_report = metrics.classification_report(Bal_y_test,RF_y_predicted)\nprint(\"Classification Report\")\nprint(\"---------------------\")\nprint(RF_class_report)\nmetrics.ConfusionMatrixDisplay(RF_conf_matrix).plot()\nTP = RF_conf_matrix[0,0]\nTN = RF_conf_matrix[1,1]\nFP = RF_conf_matrix[0,1]\nFN = RF_conf_matrix[1,0]","ab049983":"print(TP\/float(TP+FN))\nprint(metrics.recall_score(Bal_y_test,RF_y_predicted))","7068884b":"print(TN\/float(TN+FP))","060075a5":"metrics.plot_precision_recall_curve(RF,Bal_X_test , Bal_y_test)\nplt.show()","f2922b8f":"metrics.plot_roc_curve(RF, Bal_X_test , Bal_y_test)\nplt.show()","252b95b6":"feature_Importances_List = RF.feature_importances_.tolist()","36a48be1":"feature_imp_values = pd.DataFrame() \nfeature_imp_values[\"Feature\"] = FS_X.columns \nfeature_imp_values[\"Importances\"] = [feature_Importances_List[i] for i in range(len(FS_X.columns))] \nfeature_imp_values.sort_values(by='Feature')","a44c65e6":"random_state = np.arange(1,51)\nRF_Random_States_List = []\n\nfor r_state in random_state:\n    RF = RandomForestClassifier(n_estimators=10,criterion='entropy',random_state = r_state)\n    RF_Random_States_List.append(RF)\n","3adf7dbb":"overall_mean_list = []\n\nfor RandomForest in RF_Random_States_List:\n    SkFold = model_selection.StratifiedKFold(n_splits=10, random_state=7, shuffle=False)\n    scores = model_selection.cross_val_score(estimator = RandomForest,X=Balanced_X,y=Balanced_y,scoring='accuracy',cv=SkFold) \n    overall_mean_list.append(np.mean(scores))\n    ","f31844b9":"mean=np.mean(overall_mean_list)\nstandard_deviation=np.std(overall_mean_list)\nprint('mean',mean)\nprint('standard_deviation',standard_deviation)\nn=len(overall_mean_list)\nprint('n',n)\nstandard_error = standard_deviation \/ (np.sqrt(n))\nprint('Standard Error',standard_error)\nMOE = 2.58*standard_error\nprint('MOE',MOE)\nplus =  np.round(mean+MOE,3) \nminus = np.round(mean-MOE,3)\nprint()\nprint(f\"With 99% confidence,we can say that the mean test accuracy is between {plus} and {minus}, based on {n} sample accuracies.\")","98109f43":"SkFold = model_selection.StratifiedKFold(n_splits=50, random_state=7, shuffle=False)\n\nKNN = KNeighborsClassifier(n_neighbors=5,weights='distance',algorithm='auto',metric='minkowski',p=1)\nRF = RandomForestClassifier(n_estimators=10,criterion='entropy',random_state = 1)\nDT = DecisionTreeClassifier(criterion='entropy',random_state = 1,max_depth=8)\nGNB = GaussianNB()\nLR = LogisticRegression(random_state = 1)\n\nestimators_list = [('KNN',KNN),('RF',RF),('DT',DT),('GNB',GNB)]\n\nSCLF = StackingClassifier(estimators = estimators_list,final_estimator = LR,stack_method = 'predict_proba',cv=SkFold,n_jobs = -1)\nscores = model_selection.cross_val_score(estimator = SCLF,X=Balanced_X.values,y=Balanced_y.values,scoring='accuracy',cv=SkFold)\nprint(scores)\n","13924365":"mean=np.mean(scores)\nstandard_deviation=np.std(scores)\nprint('mean',mean)\nprint('standard_deviation',standard_deviation)\nn=len(scores)\nprint('n',n)\nstandard_error = standard_deviation \/ (np.sqrt(n))\nprint('Standard Error',standard_error)\nMOE = 2.58*standard_error\nprint('Margin Of Error',MOE)\nplus =  np.round(mean+MOE,3) \nminus = np.round(mean-MOE,3)\nprint()\nprint(f\"With 99% confidence,we can say that the mean test accuracy is between {plus} and {minus}, based on {n} sample accuracies.\")","0a5e857d":"SkFold = model_selection.StratifiedKFold(n_splits=50, random_state=7, shuffle=False)\n\nKNN = KNeighborsClassifier(n_neighbors=5,weights='distance',algorithm='auto',metric='minkowski',p=1)\nRF = RandomForestClassifier(n_estimators=10,criterion='entropy',random_state = 1)\nDT = DecisionTreeClassifier(criterion='entropy',random_state = 1,max_depth=8)\nGNB = GaussianNB()\nLR = LogisticRegression(random_state = 1)\n\nestimators_list = [('KNN',KNN),('RF',RF),('DT',DT),('GNB',GNB)]\n\nVCLF = VotingClassifier(estimators = estimators_list,voting = 'soft',n_jobs = -1)\nscores = model_selection.cross_val_score(estimator = VCLF,X=Balanced_X.values,y=Balanced_y.values,scoring='accuracy',cv=SkFold)\nprint(scores)\n","be727a05":"SkFold = model_selection.StratifiedKFold(n_splits=50, random_state=7, shuffle=False)\n\n\nDT_Regularized = DecisionTreeClassifier(criterion='entropy',random_state = 1,max_depth=8,min_samples_leaf=2)\n\nBAG = BaggingClassifier(base_estimator=DT_Regularized,n_estimators=10,random_state=1,n_jobs=-1)\n\nscores = model_selection.cross_val_score(estimator = BAG,X=Balanced_X.values,y=Balanced_y.values,scoring='accuracy',cv=SkFold)\nprint(scores)","71ebc401":"mean=np.mean(scores)\nstandard_deviation=np.std(scores)\nprint('mean',mean)\nprint('standard_deviation',standard_deviation)\nn=len(scores)\nprint('n',n)\nstandard_error = standard_deviation \/ (np.sqrt(n))\nprint('Standard Error',standard_error)\nMOE = 2.58*standard_error\nprint('Margin Of Error',MOE)\nplus =  np.round(mean+MOE,3) \nminus = np.round(mean-MOE,3)\nprint()\nprint(f\"With 99% confidence,we can say that the mean test accuracy is between {plus} and {minus}, based on {n} sample accuracies.\")","5fe1d3d3":"SkFold = model_selection.StratifiedKFold(n_splits=50, random_state=7, shuffle=False)\n\nLR = LogisticRegression(random_state=1)\n\nBAG = BaggingClassifier(base_estimator=LR,n_estimators=10,random_state=1,n_jobs=-1)\n\nscores = model_selection.cross_val_score(estimator = BAG,X=Balanced_X.values,y=Balanced_y.values,scoring='accuracy',cv=SkFold)\nprint(scores)","2eb037bb":"mean=np.mean(scores)\nstandard_deviation=np.std(scores)\nprint('mean',mean)\nprint('standard_deviation',standard_deviation)\nn=len(scores)\nprint('n',n)\nstandard_error = standard_deviation \/ (np.sqrt(n))\nprint('Standard Error',standard_error)\nMOE = 2.58*standard_error\nprint('Margin Of Error',MOE)\nplus =  np.round(mean+MOE,3) \nminus = np.round(mean-MOE,3)\nprint()\nprint(f\"With 99% confidence,we can say that the mean test accuracy is between {plus} and {minus}, based on {n} sample accuracies.\")","75866a98":"SkFold = model_selection.StratifiedKFold(n_splits=50, random_state=7, shuffle=False)\n\nDT_Regularized = DecisionTreeClassifier(criterion='entropy',random_state = 1,max_depth=8,min_samples_leaf=2)\n\nADA = AdaBoostClassifier(base_estimator=DT_Regularized,n_estimators=50,random_state=1)\n\nscores = model_selection.cross_val_score(estimator = ADA,X=Balanced_X.values,y=Balanced_y.values,scoring='accuracy',cv=SkFold)\nprint(scores)","77f6d2fe":"mean=np.mean(scores)\nstandard_deviation=np.std(scores)\nprint('mean',mean)\nprint('standard_deviation',standard_deviation)\nn=len(scores)\nprint('n',n)\nstandard_error = standard_deviation \/ (np.sqrt(n))\nprint('Standard Error',standard_error)\nMOE = 2.58*standard_error\nprint('Margin Of Error',MOE)\nplus =  np.round(mean+MOE,3) \nminus = np.round(mean-MOE,3)\nprint()\nprint(f\"With 99% confidence,we can say that the mean test accuracy is between {plus} and {minus}, based on {n} sample accuracies.\")","d968d4d2":"SkFold = model_selection.StratifiedKFold(n_splits=50, random_state=7, shuffle=False)\n\nLR = LogisticRegression(random_state=1)\n\nADA = AdaBoostClassifier(base_estimator=LR,n_estimators=50,random_state=1)\n\nscores = model_selection.cross_val_score(estimator = ADA,X=Balanced_X.values,y=Balanced_y.values,scoring='accuracy',cv=SkFold)\nprint(scores)","671ed442":"mean=np.mean(scores)\nstandard_deviation=np.std(scores)\nprint('mean',mean)\nprint('standard_deviation',standard_deviation)\nn=len(scores)\nprint('n',n)\nstandard_error = standard_deviation \/ (np.sqrt(n))\nprint('Standard Error',standard_error)\nMOE = 2.58*standard_error\nprint('Margin Of Error',MOE)\nplus =  np.round(mean+MOE,3) \nminus = np.round(mean-MOE,3)\nprint()\nprint(f\"With 99% confidence,we can say that the mean test accuracy is between {plus} and {minus}, based on {n} sample accuracies.\")","f855caf8":"SkFold = model_selection.StratifiedKFold(n_splits=50, random_state=7, shuffle=False)\nfold = SkFold.split(Balanced_X,Balanced_y)\n   \nX_trainList = []\nX_testList = []\ny_trainList = []\ny_testList = []\nXGB_accuracy_list = []\n    \n   \n    \nfor train_index, test_index in fold:\n    X_train = Balanced_X.iloc[train_index]\n    X_test  = Balanced_X.iloc[test_index]\n    y_train = Balanced_y.iloc[train_index]\n    y_test = Balanced_y.iloc[test_index]\n    X_trainList.append(X_train)\n    X_testList.append(X_test)\n    y_trainList.append(y_train)\n    y_testList.append(y_test)\n        \nfor i in np.arange(0,len(X_trainList)):\n    X_trainList[i]\n    X_testList[i]\n    y_trainList[i]\n    y_testList[i] \n    \n    XGB = XGBClassifier(n_estimators=10,random_state=1 ,n_jobs=-1)\n    XGB.fit(X_trainList[i],y_trainList[i])\n    XGB_y_pred_proba= XGB.predict_proba(X_testList[i])\n    XGB_y_pred_proba = XGB_y_pred_proba[:,1]\n    XGB_y_prediected = preprocessing.binarize([XGB_y_pred_proba], 0.3)[0]\n    test_accuracy = metrics.accuracy_score(y_testList[i],XGB_y_prediected)\n    XGB_accuracy_list.append(test_accuracy)\n    \n    \n    ","5cd75bfb":"mean=np.mean(XGB_accuracy_list)\nstandard_deviation=np.std(XGB_accuracy_list)\nprint('mean',mean)\nprint('standard_deviation',standard_deviation)\nn=len(XGB_accuracy_list)\nprint('n',n)\nstandard_error = standard_deviation \/ (np.sqrt(n))\nprint('Standard Error',standard_error)\nMOE = 2.58*standard_error\nprint('Margin Of Error',MOE)\nplus =  np.round(mean+MOE,3) \nminus = np.round(mean-MOE,3)\nprint()\nprint(f\"With 99% confidence,we can say that the mean test accuracy is between {plus} and {minus}, based on {n} sample accuracies.\")","4f24806d":"test_accuracy_list        = []\nconf_matrix_report_list   = []\nclass_report_list         = []\n\n\n    \n# Keeping splits = 10 just for sake of avoiding notebook scrolling     \nSkFold = model_selection.StratifiedKFold(n_splits=10, random_state=7, shuffle=False)\nfold = SkFold.split(Balanced_X,Balanced_y)\n   \nX_trainList = []\nX_testList = []\ny_trainList = []\ny_testList = []\nGB_y_predictedList = []\n    \n   \n    \nfor train_index, test_index in fold:\n    X_train = Balanced_X.iloc[train_index]\n    X_test  = Balanced_X.iloc[test_index]\n    y_train = Balanced_y.iloc[train_index]\n    y_test = Balanced_y.iloc[test_index]\n    X_trainList.append(X_train)\n    X_testList.append(X_test)\n    y_trainList.append(y_train)\n    y_testList.append(y_test)\n        \n\nfor i in np.arange(0,len(X_trainList)):\n    X_trainList[i]\n    X_testList[i]\n    y_trainList[i]\n    y_testList[i] \n    \n        \n        \n    gradientBoost = GradientBoostingClassifier( n_estimators=50,random_state=7)\n    gradientBoost.fit(X_trainList[i],y_trainList[i])\n    GB_y_predicted_proba = gradientBoost.predict_proba(X_testList[i]) \n    GB_y_predicted_proba = GB_y_predicted_proba[:,1]\n    GB_y_predicted = preprocessing.binarize([GB_y_predicted_proba], 0.3)[0]\n    \n    conf_matrix  = metrics.confusion_matrix(y_testList[i] ,GB_y_predicted)\n    class_report = metrics.classification_report(y_testList[i],GB_y_predicted)    \n    test_score   = np.round(gradientBoost.score(X=X_testList[i],y=y_testList[i]),2)\n    \n    \n    test_accuracy_list.append(test_score)      \n    conf_matrix_report_list.append(conf_matrix)\n    class_report_list.append(class_report)\n    GB_y_predictedList.append(GB_y_predicted)","41d01ff7":"    test_accuracy_array = np.array(test_accuracy_list)\n    gb_result = np.where(test_accuracy_array>0.90)\n    gb_result = gb_result[0]\n    \n    for r in gb_result:\n        conf = conf_matrix_report_list[r]\n        if((conf[1,0]==0 and conf[0,1]<5)):\n            print('Test Accuracy',test_accuracy_list[r])\n            print()\n            print(\"Confusion Matrix \")\n            metrics.ConfusionMatrixDisplay(conf).plot()\n            plt.show()\n            print()\n            print(\"Classification Report \")\n            print(class_report_list[r])                                   \n            TP = conf[0,0]\n            TN = conf[1,1]\n            FP = conf[0,1]\n            FN = conf[1,0]\n            print(\"Sensitivity\")\n            print(TP\/float(TP+FN))\n            print(metrics.recall_score(y_testList[r],GB_y_predictedList[r]))\n            print(\"Specificity\")\n            print(TN\/float(TN+FP))  \n            print(\"Precision Recall Curve\")\n            metrics.plot_precision_recall_curve(gradientBoost,X_testList[r] , y_testList[r])\n            plt.show()\n            print(\"Receiver Operating Characteristic Curve (ROC)\")\n            metrics.plot_roc_curve(gradientBoost,X_testList[r] , y_testList[r])\n            plt.show()\n            \n            \n            print(\"--------------------------------------------------------\")","8389e9b4":"mean=np.mean(test_accuracy_list)\nstandard_deviation=np.std(test_accuracy_list)\nprint('mean',mean)\nprint('standard_deviation',standard_deviation)\nn=len(test_accuracy_list)\nprint('n',n)\nstandard_error = standard_deviation \/ (np.sqrt(n))\nprint('Standard Error',standard_error)\nMOE = 2.58*standard_error\nprint('MOE',MOE)\nplus =  np.round(mean+MOE,3) \nminus = np.round(mean-MOE,3)\nprint()\nprint(f\"With 99% confidence,we can say that the mean test accuracy is between {plus} and {minus}, based on {n} sample accuracies.\")","ff6aef85":"test_accuracy_list        = []\nconf_matrix_report_list   = []\nclass_report_list         = []\n\n\n    \n     \nSkFold = model_selection.StratifiedKFold(n_splits=50, random_state=7, shuffle=False)\nfold = SkFold.split(Balanced_X,Balanced_y)\n   \nX_trainList = []\nX_testList = []\ny_trainList = []\ny_testList = []\nGB_y_predictedList = []\n    \n   \n    \nfor train_index, test_index in fold:\n    X_train = Balanced_X.iloc[train_index]\n    X_test  = Balanced_X.iloc[test_index]\n    y_train = Balanced_y.iloc[train_index]\n    y_test = Balanced_y.iloc[test_index]\n    X_trainList.append(X_train)\n    X_testList.append(X_test)\n    y_trainList.append(y_train)\n    y_testList.append(y_test)\n        \n\nfor i in np.arange(0,len(X_trainList)):\n    X_trainList[i]\n    X_testList[i]\n    y_trainList[i]\n    y_testList[i] \n    \n        \n        \n    gradientBoost = GradientBoostingClassifier( n_estimators=50,random_state=7)\n    gradientBoost.fit(X_trainList[i],y_trainList[i])\n    GB_y_predicted_proba = gradientBoost.predict_proba(X_testList[i]) \n    GB_y_predicted_proba = GB_y_predicted_proba[:,1]\n    GB_y_predicted = preprocessing.binarize([GB_y_predicted_proba], 0.3)[0]\n    \n    conf_matrix  = metrics.confusion_matrix(y_testList[i] ,GB_y_predicted)\n    class_report = metrics.classification_report(y_testList[i],GB_y_predicted)    \n    test_score   = np.round(gradientBoost.score(X=X_testList[i],y=y_testList[i]),2)\n    \n    \n    test_accuracy_list.append(test_score)      \n    conf_matrix_report_list.append(conf_matrix)\n    class_report_list.append(class_report)\n    GB_y_predictedList.append(GB_y_predicted)","3bc862b6":"mean=np.mean(test_accuracy_list)\nstandard_deviation=np.std(test_accuracy_list)\nprint('mean',mean)\nprint('standard_deviation',standard_deviation)\nn=len(test_accuracy_list)\nprint('n',n)\nstandard_error = standard_deviation \/ (np.sqrt(n))\nprint('Standard Error',standard_error)\nMOE = 2.58*standard_error\nprint('MOE',MOE)\nplus =  np.round(mean+MOE,3) \nminus = np.round(mean-MOE,3)\nprint()\nprint(f\"With 99% confidence,we can say that the mean test accuracy is between {plus} and {minus}, based on {n} sample accuracies.\")","a4ac7c91":"#### <font color=purple>Even though we see significant multicollinearity, I am not dropping any feature for now. Let us create a second baseline model with these 15  features and observe.<\/font>","76f8dfd4":"### <font color=purple>*Training AdaBoost Classifier With Regularized Decision Tree*<\/font>","80eb0f19":"##### <font color=blue>Finding intervals for MDVP:Flo(Hz) as per min & max values<\/font>","9aa7f82f":"#### <font color=blue>Plotting Precision Recall Curve<\/font>","aeb53787":"### <font color=red>Univarite Bivariate\/Multivariate Analysis Completed<\/font>","4c938fea":"#### <font color=blue>Precision Recall Curve<\/font>","2644f575":"#### <font color=blue>Checking distribution<\/font>","42301c66":"##### <font color=blue>*Method 01*<\/font>","485c6bb0":"##### <font color=purple>*It is also one of the measures of variation in fundamental frequency.From 0.00001 to 0.00015 is the range which correlates with PD. Not sure why the Kde graph isn't plotted.It also has outliers.*<\/font>","c3c27c2d":"##### <font color=blue>Binning MDVP:Flo(Hz)<\/font>","a843d3b1":"##### <font color=blue>Considering Confidence Level = 99% so Z* value need to be taken is  2.58<\/font>","56de1bbb":"##### <font color=blue>Dropping name feature only<\/font>","47b581a1":"#### <font color=blue>Feature Importances<\/font>","19cada8b":"##### <font color=purple>*It is a measure of ratio of noise to tonal components in the voice.From 10 to 30 is the range which correlates to persons with PD.This is also a slightly better indicator as there is very little separation.As we can observe in Kde rug, values are densed near around 22 on both sides suggesting need to look more here.It does not have outliers.*<\/font>","09d3bece":"#### <font color=orange>It can be inferred that :<\/font>\n##### <font color=blue>8 instances of low MDVP:Fo(Hz) upto 100Hz with PPE (from 0.19 to 0.31) are related with PD.<\/font>\n##### <font color=blue>several instances of PD are visible for medium & high MDVP:Fo(Hz) with PPE(from 0.1 to 0.45).<\/font>\n##### <font color=blue>few instances of PD are visible for very high MDVP:Fo(Hz) also along with PPE(from around 0.12 to 0.25).<\/font>","ccbc2340":"#### <font color=blue>*Count of each class*<\/font>","566650f5":"#### <font color=purple>*Gradient Boosting Accuracy With 50 Samples*<\/font>","18ec9cc3":"##### <font color=blue>*Many of them have outliers. We might need to work on this before we do modelling.*<\/font>\n##### <font color=blue>There is one more issue here. I don't know whether the outliers are valid or not. If outliers are invalid then we can drop it. But here I don't know their validness as I am not this domain expert, so here considering all of them as valid.<\/font>","80fb8ea0":"#### <font color=orange>It can be inferred that :<\/font>\n##### <font color=blue>combination of low MDVP:Fo(Hz) upto 100Hz & MDVP:Flo(Hz) less than 100Hz are high relation with PD. We can see all orange dots for MDVP:Fo(Hz) = Low & MDVP:Flo(Hz) less than 100 <\/font>\n##### <font color=blue>medium MDVP:Fo(Hz) (between 101 & 150) along with MDVP:Flo(Hz) upto 150Hz also have high relation with PD.<\/font>\n##### <font color=blue>high MDVP:Fo(Hz) (between 151 & 200) along with MDVP:Flo(Hz) upto 175Hz also have high relation with PD.<\/font>\n##### <font color=blue>there are some instances of PD in range of very high MDVP:Fo(Hz) also.<\/font>","1182f85b":"#### <font color=blue>02 Checking influence of MDVP:Fo(Hz) & PPE on status.<\/font>","15d5960e":"#### <font color=orange>It can be inferred that :<\/font>\n##### <font color=blue>8 instances of low MDVP:Fo(Hz) upto 100Hz with spread1 (from around -6 to -4.5) are related with PD.<\/font>\n##### <font color=blue>several instances of PD are visible for medium with spread1(from around -7 to -3).<\/font>\n##### <font color=blue>several instances of PD are visible for high MDVP:Fo(Hz) with spread1(from around -7 to more than -3).<\/font>\n##### <font color=blue>few instances of PD are visible for very high MDVP:Fo(Hz) also along with PPE(from around -6.5 to -5).<\/font>","143458ab":"### <font color=purple>Creating Third & Final Baseline Model Using GradientBoostingClassifier With Features Selected By Feature Selection & Rectifying Class Imbalance Issue.<\/font>\n","7694aab8":"#### <font color=blue>As we can see, all of the outliers have been fixed<\/font>","3ca829ec":"##### <font color=blue>So MDVP:Shimmer,MDVP:Shimmer & Shimmer:APQ3 has the highest VIF values. <\/font>\n##### <font color=blue>Again plotting KDE plots for all these features.<\/font>","809814a5":"#### <font color=orange>It can be inferred that :<\/font>\n##### <font color=blue>11 instances of low MDVP:Flo(Hz) upto 70Hz with spread2 are related with PD.<\/font>\n##### <font color=blue>several instances of PD are visible for medium MDVP:Flo(Hz) with spread2(from around 0.15 to 0.4).<\/font>\n##### <font color=blue>several instances of PD are visible for high MDVP:Flo(Hz) with spread2 across the range.<\/font>\n##### <font color=blue>few instances of PD are visible for very high MDVP:Flo(Hz) also along with spread2.<\/font>","caafe8b9":"### <font color=red>Basic Statistical Analysis<\/font>","f22297ec":"##### <font color=purple>Looking at diagonals: MDVP:Fo(Hz), MDVP:Flo(Hz), spread1,spread2,PPE looks to have some separation. These are the features which looks to be good indicators of our target class<\/font>\n\n##### <font color=purple>Some independent features have linear relation with other independent features:<\/font>\n\n##### <font color=purple> MDVP:Jitter(%), MDVP:Jitter(Abs),MDVP:RAP,MDVP:PPQ,Jitter:DDP have some linear relation among each other. All of these are measures of variation in fundamental frequency so it would be a better approach to keep one of them and drop the remaining features.<\/font>\n\n##### <font color=purple> Similarly, MDVP:Shimmer,MDVP:Shimmer(dB),Shimmer:APQ3,Shimmer:APQ5,MDVP:APQ,Shimmer:DDA also have linear relation among each other. All of these are measures of variation in fundamental amplitude so it would be a better approach to keep one of them and drop the remaining features.<\/font>\n\n##### <font color=purple>Spread1 has linear relation with PPE<\/font>\n\n","a4792f81":"##### <font color=blue>Dropping all other features which are not in supportList<\/font>","2fdba7eb":"##### <font color=blue>Considering Confidence Level = 99% so Z* value need to be taken is  2.58<\/font>","9f4005ef":"#### <font color=purple>*So out of all independent features, only ,<font color=red>**PPE**<\/font>,<font color=red>**spread2**<\/font>,<font color=red>**spread1**<\/font>,<font color=red>**MDVP:Fo(Hz)**<\/font> are the features which looks like good indicators as of now.*<\/font>","d6555466":"#### <font color=blue>Printing Sensitivity & Specificity<\/font>","fa11dfb3":"##### <font color=purple>*It is one of the measures of variation in fundamental amplitude.From 0.02 to 0.08 is the range which correlates to persons with PD. This also is not a good indicator as it too does have slight separation as per kde graph.It also has normal distribution with outliers.Positively skewed.*<\/font>","5e02b4fd":"### <font color=blue>*Loading the dataset*<\/font>","273a546d":"##### <font color=purple>*Maximum vocal fundamental frequency ranges between 100 to around 210 for most of the persons suffering from PD. There are exceptions though.It has normal distribution for persons with PD.There is only a slight separation between two kde graphs indicating that this might not be very good indicator.As we can observe in Kde rug, values are densed near 200Hz on both sides suggesting need to look more here. There are many outliers here also.*<\/font>","eaab3465":"#### <font color=orange>It can be inferred that :<\/font>\n##### <font color=blue>11 instances of low MDVP:Flo(Hz) upto 70Hz with PPE (from around 0.15 to 0.45) are related with PD.<\/font>\n##### <font color=blue>several instances of PD are visible for medium MDVP:Flo(Hz) with PPE(from around 0.1 to 0.45).<\/font>\n##### <font color=blue>again several instances of PD are visible for high MDVP:Flo(Hz) with PPE(from around 0.1 to more than 0.5).<\/font>\n##### <font color=blue>few instances of PD are visible for very high MDVP:Flo(Hz) also along with PPE(from around 0.12 to 0.29).<\/font>","502a62e2":"##### <font color=purple>*It is also one of the measures of variation in fundamental amplitude.From 0.00 to 0.06 is the range which correlates to persons with PD. This isn't a good indicator as it has very little separation as per kde graph.As we can observe in Kde rug, values are densed near 0.02 on both sides suggesting need to look more here.It has near normal distribution with a bump on positive side with outliers. Little bit positively skewed.*<\/font>","93bb5755":"##### <font color=blue>Precision Recall Curve is a metric to evaluate classifier output quality<\/font>\n##### <font color=blue>Precision-Recall is a useful measure of success of prediction when the classes are very imbalanced. Though we had balanced our classes earlier using SMOTE<\/font>\n##### <font color=blue>High area under curve indicates high precision and high recall.<\/font>\n##### <font color=blue>Point at top right corner is the optimum value of both precision and recall<\/font>","570f5899":"##### <font color=purple>*Again it is also one of the measures of variation in fundamental frequency.From 0 to 0.02 is the range which correlates to persons with PD. This also is not a good indicator as it too does not have separation as per kde graph.As we can observe in Kde rug, values are densed near 0.01 on both sides suggesting need to look more here.It also has normal distribution with outliers. Positively skewed*<\/font>","8e28247e":"#### <font color=blue>04 Checking influence of MDVP:Fo(Hz) & spread1 on status.<\/font>","20b7f40d":"### <font color=red>*Training Ensemble Models*<\/font>","118350f1":"##### <font color=blue>One of the best ways to check for multicollinearity is through VIF (variance inflation factor)<\/font>","446018a5":"### <font color=purple>*Training AdaBoost Classifier With Logistic Regression*<\/font>","352ca078":"##### <font color=blue>*Method 02*<\/font>","916ba622":"## <font color=purple>*Training Decision Tree*<\/font>\n#### <font color=purple>*Using criterion='entropy' & keeping everything else to default value.*<\/font>","65a7579c":"### <font color=purple>So, overall this Regularized Decision Tree model is not an overfit model. It has better accuracy than unregularized decision tree model but for random state 1 only and unregularized decision tree has lesser false negatives.And it could not touch the results given by our Gradient Boost third baseline model<\/font>","a475c55c":"##### <font color=blue>Binning MDVP:Fo(Hz)<\/font>","a034928d":"##### <font color=blue>Finding intervals for PPE as per min & max values<\/font>","1722d198":"#### <font color=blue>Printing Confusion Matrix & Classification Report<\/font>","b5681846":"##### <font color=purple>*It is a nonlinear measure of fundamental frequency variation.From -7 to -3 is the range which correlates to persons with PD.This is a good indicator as there is good separation.As we can observe in Kde rug, values are densed between -6 & -7 suggesting need to look more here.It does not have outliers.*<\/font>","a37ceee6":"##### <font color=blue>Binning PPE<\/font>","81b1ea98":"### <font color=purple>K Nearest Neighbours<\/font>","6285a253":"### <font color=red>*Feature Details*<\/font>","941ee004":"**Sensitivity:** ","f9e96a22":"# <font color=blue>Thank You<\/font>","0f8c66e7":"##### <font color=purple>*It is signal fractal scaling exponent.From 0.6 to 0.9 is the range which correlates to persons with PD.This also isn't a good indicator as there is no separation.As we can observe in Kde rug, values are densed between 0.7 & 0.75 suggesting need to look more here.It does not have outliers.*<\/font>","9f87130f":"##### <font color=blue>*There are 195 records & 24 features*<\/font>","15029cd3":"### <font color=purple>*Training Voting Classifier*<\/font>","c0a1a415":"##### <font color=blue>Precision Recall Curve is a metric to evaluate classifier output quality<\/font>\n##### <font color=blue>Precision-Recall is a useful measure of success of prediction when the classes are very imbalanced. Though we had balanced our classes earlier using SMOTE<\/font>\n##### <font color=blue>High area under curve indicates high precision and high recall.<\/font>\n##### <font color=blue>Point at top right corner is the optimum value of both precision and recall<\/font>","fdb6607b":"##### <font color=blue>Finding Min & Max values of PPE<\/font>","e4cc1973":"#### <font color=blue>Now, trying one last thing, power transform<\/font>\n#### <font color=blue>Using Power Transform as it will make the data gaussian.<\/font>","c2ee133b":"Moving our target column \"status\" to the end ","3206ba91":"#### <font color=blue>Creating binning function.<\/font>\n#### <font color=blue>Took inspiration from below mentioned address. :)<\/font>\n#### [How To Create Bins](https:\/\/theprofessionalspoint.blogspot.com\/2019\/04\/how-to-create-bins-for-continuous.html)","ff32f0e3":"#### <font color=orange>It can be inferred that :<\/font>\n##### <font color=blue>same 8 instances of low MDVP:Fo(Hz) upto 100Hz with spread2 are related with PD but with spread2, they are more spread over the range from around 0.09 to 0.4. With spread1, instances were densed in a shorter range.<\/font>\n##### <font color=blue>several instances of PD are visible for medium with spread2(from around 0.1 to 0.35).<\/font>\n##### <font color=blue>several instances of PD are visible for high MDVP:Fo(Hz) with spread2 just like spread1.<\/font>\n##### <font color=blue>few instances of PD are visible for very high MDVP:Fo(Hz) also along with spread2 on higher range of spread2.<\/font>","29d04b49":"##### <font color=blue>Now next we play with min_samples_leaf<\/font>","270aec7b":"#### <font color=blue>Plotting Precision Recall Curve<\/font>","409a2405":"##### <font color=blue>*Looking at the averages*<\/font>","792ce029":"##### <font color=purple>*It is also one of the measures of variation in fundamental frequency.From 0.00 to 0.005 is the range which correlates to persons with PD. As we can observe in Kde rug, values are densed on left side of 0.005 suggesting need to look more here.This also is not a good indicator as it too does not have separation as per kde graph.It also has normal distribution with outliers.*<\/font>","5ff23742":"#### <font color=blue>03 Checking influence of PPE & MDVP:Flo(Hz) on status.<\/font>","499fb607":"##### <font color=blue>Finding Min & Max values of MDVP:Flo(Hz)<\/font>","0df728c8":"### <font color=blue>Regularization help control growth of tree. Other wise trees can easily go overfit and won't generalize. So regularization parameters helps to to keep balance between overfitting and underfitting<\/font>\n### <font color=blue>Regularization, significantly reduces the variance of the model, without substantial increase in its bias.So it helps in controling the impact on bias and variance<\/font>","51adcbb0":"> **Sensitivity:**","611554bc":"##### <font color=blue>By matching True's in select_k_best.get_support() with FS_X.head(), we can easily identify which features are selected but below is a more elegant way to find out that.<\/font>","b555c124":"##### <font color=purple>*Again it is also one of the measures of variation in fundamental amplitude.From 0.00 to 0.04 is the range which correlates to persons with PD. This isn't good indicator as it does not have separation as per kde graph.As we can observe in Kde rug, values are densed near 0.02 on both sides suggesting need to look more here.It also has normal distribution with outliers. Positively skewed.*<\/font>","6a6b7c0f":"##### <font color=blue>Creating a dataframe with  the 15 best features and their corresponding p values<\/font>","1a061bb5":"### <font color=red>Feature Selection<\/font>","a89c6de2":"#### <font color=blue>01 Checking influence of MDVP:Fo(Hz) & MDVP:Flo(Hz) on status.<\/font>","5fc1683c":"##### <font color=blue>Checking for multicollinearity among these 15 best features<\/font>","f3cfd3ca":"##### <font color=purple>*Gradient Boosting Accuracy Is Not Quite Impressive Here*<\/font>","1f6275a5":"**Specificity:** ","fd9ac149":"##### <font color=blue>*There are no duplicate values*<\/font>","89eadd65":"### <font color=purple>*Training Bagging Classifier With Regularized Decision Tree*<\/font>","e74a6c79":"#### <font color=blue>*Analysis of distributions of each of the continuous features*<\/font>","d369b4ed":"#### <font color=blue>*Analysis Of Bar Plots*<\/font>","85cf4b8f":"### <font color=purple>So, overall this Decision Tree model which is an overfit model, could not touch the results given by our Gradient Boost third baseline model<\/font>","23c13f78":"### <font color=purple>*Gradient Boost With Multiple Fold Data*<\/font>","8ea36cf5":"##### <font color=blue>As we know that min_samples_leaf is minimum number of samples a leaf node must have, increasing it increases impurity in nodes.Hence accuracy starts decreasing beyond a certain point as we can see above.<\/font>\n##### <font color=blue>So in this particular case,as we can observe, min_samples_leaf isn't proving useful. Increasing it from 2 to 3 is decreasing accuracy.<\/font>","5b195783":"##### <font color=purple>*Minimum vocal fundamental frequency ranges between 50Hz to 180Hz for persons with PD. This is somewhat good indicator as it has some separation as per kde graph.As we can observe in Kde rug, values are spread near 150Hz on both sides suggesting need to look more here.Again two peaks suggests two different data sources.*<\/font>","a89931a1":"#### <font color=blue>*Analysis Of pairplot*<\/font>","31e6e4a0":"#### <font color=purple>Receiver Operating Characteristic Curve (ROC)<\/font>","b6cd967d":"##### <font color=purple>*I will use Balanced_X & Balanced_y in all of the models.*<\/font>","ee0520ef":"### <font color=purple>Random Forest Second Implementation<\/font>","820fbd0b":"#### <font color=blue>As we can see Gradient Boosting's performance has become worst after transform. So baseline **Model 3** will be our true baseline.<\/font>","05cd5b1b":"## <font color=red>Begin Modelling<\/font>\n### <font color=blue>*Training Independent Models*<\/font>","db46fb64":"##### <font color=blue>ROC Curve is a metric to evaluate classifier output quality<\/font>\n##### <font color=blue>Optimum balance value between FPR & TPR is at top left corner.<\/font>","30c5d381":"##### <font color=blue>Precision Recall Curve is a metric to evaluate classifier output quality<\/font>\n##### <font color=blue>Precision-Recall is a useful measure of success of prediction when the classes are very imbalanced. Though we had balanced our classes earlier using SMOTE<\/font>\n##### <font color=blue>High area under curve indicates high precision and high recall.<\/font>\n##### <font color=blue>Point at top right corner is the optimum value of both precision and recall<\/font>","61876d0c":"##### <font color=blue>ROC Curve is a metric to evaluate classifier output quality<\/font>\n##### <font color=blue>Optimum balance value between FPR & TPR is at top left corner.<\/font>\n##### <font color=blue>And as we can see, it is much better than Decision Tree sd it has just 1 False Positive.<\/font>","ba52565b":"##### <font color=blue>Observing effect of Power Transform<\/font>","e79ea2ed":"##### <font color=blue>*So far looks fine. Let's explore more*<\/font>","0ec5361a":"#### <font color=blue>Creating a copy of original dataset<\/font>","575ba6e9":"##### <font color=blue>max_depth: Is the maximum length of a path from root to leaf (in terms of number of decision points. The leaf node is not split further. It could lead to a tree with leaf node containing many observations on one side of the tree, whereas on the other side, nodes containing much less observations get further split<\/font>\n\n##### <font color=blue>min_samples_leaf: One could run into the problem of having 990 samples in one of them, and 10 on the other.To avoid this, we can set a minimum number of samples allowED on each leaf. So it is the minimum number of samples a leaf node must have. When a leaf contains too few observations, further splitting will result in overfitting.<\/font>","0ed76a2a":"#### <font color=orange>It can be inferred that :<\/font>\n##### <font color=blue>11 instances of low MDVP:Flo(Hz) upto 70Hz with spread1 (from around -6 to -4.5) are related with PD.<\/font>\n##### <font color=blue>several instances of PD are visible for medium MDVP:Flo(Hz) with spread1(from around -7 to -3).<\/font>\n##### <font color=blue>several instances of PD are visible for high MDVP:Flo(Hz) with spread1(from around -7 to more than -3).<\/font>\n##### <font color=blue>few instances of PD are visible for very high MDVP:Flo(Hz) also along with PPE(from around -6.5 to -3).<\/font>","39748451":"#### <font color=orange>Now, we need to analyse how much good indicator features together influence status.<\/font>\n##### <font color=purple>To do that, as all of those features are continuous, we have to make some of them categorical and put that categorical feature on x-axis of swarmplot.<\/font>\n##### <font color=purple>Process of converting continuous features to categorical is called as discretization or binning.<\/font>\n##### <font color=purple>We are going to use pandas' cut function to do that.<\/font>","19a2984f":"##### <font color=blue>max_depth & min_sample_leaf are two of several other hyperparamters available for Decision Tree. Regularizing these parameters can help us achive the optimum accuracy keeping underfitting - overfitting tradeoff in mind.<\/font>","a3b11ea3":"**Specificity:** ","4cef8f6b":"#### <font color=blue>*Analysis Of Correlation HeatMap*<\/font>","bd5a1a17":"#### <font color=blue>06 Checking influence of PPE & spread1 on status.<\/font>","2c158fe8":"##### <font color=blue>*Method 03*<\/font>","36b1e0d7":"#### <font color=blue>As we can see, data has become gaussian, although not fully but upto some extent<\/font>","f35ac8c5":"#### <font color=blue>Printing Train & Test Accuracy<\/font>","7e4b810c":"## <font color=blue>Data Exploration Wrangling Preprocessing Preparation<\/font>","bf8f4f54":"#### <font color=orange>It can be inferred that :<\/font>\n##### <font color=blue>no instances of low PPE with spread1 are related with PD.<\/font>\n##### <font color=blue>several instances of PD are visible for medium PPE with spread1(from around -7 to -6) & 1 case of -5 spread1.<\/font>\n##### <font color=blue>many instances of PD are visible for high PPE with spread1(from around -6.5 to -3.5).Quite densed PD cases in this area.<\/font>\n##### <font color=blue>few instances of PD are visible for very high PPE along with spread1(from around -4.2 to -3).<\/font>","335ab0d7":"#### <font color=blue>05 Checking influence of MDVP:Flo(Hz) & spread1 on status.<\/font>","36801585":"### <font color=purple>*Training Stacking Classifier*<\/font>","2c13bdfb":"##### <font color=blue>Finding Min & Max values of MDVP:Fo(Hz)<\/font>","3c578ece":"#### <font color=blue>AdaBoost With Logistic Regression performance is not good<\/font>","accd5d16":"##### <font color=blue>This dataset is composed of a range of biomedical voice measurements from 31 people, 23 with Parkinson's disease (PD). Each column in the table is a particular voice measure, and each row corresponds one of 195 voice recording from these individuals (\"name\" column). The main aim of the data is to discriminate healthy people from those with PD, according to \"status\" column which is set to 0 for healthy and 1 for PD.<\/font>\n\n##### <font color=blue>The data is in ASCII CSV format. The rows of the CSV file contain an instance corresponding to one voice recording. There are around six recordings per patient, the name of the patient is identified in the first column.<\/font>","78078bfe":"#### <font color=blue>Printing Confusion Matrix & Classification Report<\/font>\n##### <font color=blue>**Keeping max_depth=8,min_samples_leaf=2**<\/font>","324753ad":"### <font color=purple>Creating First Baseline Model Using GradientBoostingClassifier With Raw Features<\/font>","e66f3e1d":"##### <font color=blue>So here, just by trying with various values of max_depth, keeping max_depth = 8, we have got test accuracy of 92.1% from 87.6%<\/font>\n##### <font color=blue>So, by regularizing just one feature we have got significant increase in accuracy.<\/font>","247a253c":"#### <font color=blue>Feature Importances<\/font>","6d8a847f":"#### <font color=purple>Steps Explained<\/font>\n##### <font color=blue>2 random state<\/font>\n##### <font color=blue>3 K fold splits<\/font>\n##### <font color=blue>2 K Values<\/font>\n##### <font color=blue>For each of the 2 random states, balanced dataset is splitted 3 times as train & test set<\/font>\n##### <font color=blue>For each train test split, model is fitted and values are predicted for each of the K values <\/font>\n##### <font color=blue>Kept all values very low as permutaion & combination becomes quite high & then we need to scroll this notebook a lot.<\/font>","ac2457e1":"##### <font color=blue>*Out of total 195, 147 is our target class. Though it may prove good for us in terms of accuracy and precision of our target class but it may have bias towards non target class. We may need to balance classes.*<\/font>","1ea6ee7f":"##### <font color=blue>*So there are 24 features, 1 is categorical, 1 is object (name) & rest of the features are numerical*<\/font>","d2d80dec":"##### <font color=blue>*Here it is observed that features PPE, spread2, NHR, PPE, spread2, spread1, MDVP:Fo(Hz), Shimmer:APQ3, MDVP:Shimmer(dB), MDVP:Shimmer, Jitter:DDP, MDVP:RAP, MDVP:PPQ, MDVP:Jitter(%), MDVP:Jitter(Abs) looks encouraging becuase of the visible significant difference between the two classes.*<\/font>","a35624fd":"#### <font color=blue>08 Checking influence of MDVP:Flo(Hz) & spread2 on status.<\/font>","875af4d5":"##### <font color=blue>ROC Curve is a metric to evaluate classifier output quality<\/font>\n##### <font color=blue>Optimum balance value between FPR & TPR is at top left corner.<\/font>","fa834f70":"##### <font color=purple>*It is a measure of ratio of noise to tonal components in the voice.From 0.00 to 0.05 is the range which correlates to persons with PD.This is not a good indicator as there is no separation.As we can observe in Kde rug, values are densed near around 0.02 on both sides suggesting need to look more here.It also has normal distribution with outliers. Positively skewed.*<\/font>","b9c94851":"##### <font color=blue>As we have class imbalance problem here, we are going to use SMOTE to balance both the classes.<\/font>\n","1b6ae094":"#### <font color=blue>*Checking for null values*<\/font>","665abc1f":"##### <font color=blue>Dropping name feature as it is not useful and it will also create issues,<\/font>","8d03fb66":"#### <font color=blue>Feature Importances<\/font>","61739b19":"#### <font color=blue>Printing Confusion Matrix & Classification Report<\/font>","d3282e34":"### <font color=purple>*There are 24 features in total. Out of these 24, 22 are numerical (looks like continuous) features, 1 is categorical (label encoded) feature which is the target feature & 1 is the name feature which contains name of the patients & their recordings. Right now, it doesn't seems useful. There are 195 records in total. Out of 195, 147 are of status = 1. One one hand, this sounds good becuase we may get higher accuracy & precision towards identification of patients with parkisons but this might lead to bias towards subject having healthy status. The main challenge I feel at this stage is, I can't make sense of any of the attributes and their respective values except status feature. Is zero a valid value or a negative number valid here in any of the features or not? I can't answer these questions.Need domain expert*<\/font> ","256b1f4b":"# <font color=purple>**Parkinsons Disease Dataset Analysis**<\/font>","746592b5":"#### <font color=blue>Receiver Operating Characteristic Curve (ROC)<\/font>","2ddb765d":"##### <font color=blue>*First 5 records*<\/font>","03ad331a":"### <font color=red>Bivariate\/Multivariate Analysis<\/font>","5dc9b429":"### <font color=purple>So, overall this Random Forest model is better than both unregularized and regularized decision tree models. It has much better accuracy than both tree models. Random Forest also has lesser false negatives than both of them.Still, it could not touch the results given by our Gradient Boost third baseline model<\/font>","1108dad6":"##### <font color=purple>*It is also one of the measures of variation in fundamental frequency.From 0.001 to 0.005 is the range which correlates to persons with PD. This also is not a good indicator as it too does not have separation as per kde graph.As we can observe in Kde rug, values are densed on left side of 0.005Hz suggesting need to look more here.It also has normal distribution with outliers. Little bit positively skewed.*<\/font>","d587aae9":"#### <font color=blue>*Checking for duplicates*<\/font>","98161de1":"#### <font color=blue>Printing Train & Test Accuracy<\/font>","2b917271":"##### <font color=purple>*It is a nonlinear measure of fundamental frequency variation.From 0.1 to 0.4 is the range which correlates to persons with PD.This could prove a good indicator as there is some separation.As we can observe in Kde rug, values are densed near 0.2 suggesting need to look more here.It does not have outliers.*<\/font>","c746eccc":"### <font color=purple>Regularized Decision Tree<\/font>","8a6a597b":"#### <font color=blue>Plotting Receiver Operating Characteristic Curve (ROC)<\/font>","127d3fc4":"##### <font color=blue>*Here it is observed that status' has good correlation with features PPE, D2, spread1, spread2, RPDE, MDVP:Shimmer, MDVP:Shimmer(dB), Shimmer:APQ3, Shimmer:APQ5, MDVP:APQ, Shimmer:DDA, MDVP:Jitter(%) & MDVP:Jitter(Abs). Lot of multicolinearity is also observed between various independent features. We may also need to drop few of them.*<\/font>","9aac1271":"### <font color=purple>*Training Bagging Classifier With Logistic Regression*<\/font>","daa852fd":"#### <font color=red>Attribute Information:<\/font> \n##### <font color=blue>name - ASCII subject name and recording number<\/font> \n##### <font color=blue>MDVP:Fo(Hz) - Average vocal fundamental frequency<\/font>\n##### <font color=blue>MDVP:Fhi(Hz) - Maximum vocal fundamental frequency<\/font>\n##### <font color=blue>MDVP:Flo(Hz) - Minimum vocal fundamental frequency<\/font>\n##### <font color=blue>MDVP:Jitter(%),MDVP:Jitter(Abs),MDVP:RAP,MDVP:PPQ,Jitter:DDP - Several measures of variation in fundamental frequency<\/font>\n##### <font color=blue>MDVP:Shimmer,MDVP:Shimmer(dB),Shimmer:APQ3,Shimmer:APQ5,MDVP:APQ,Shimmer:DDA - Several measures of variation in amplitude<\/font>\n##### <font color=blue>NHR,HNR - Two measures of ratio of noise to tonal components in the voice<\/font> \n##### <font color=blue>status - Health status of the subject (one) - Parkinson's, (zero) - healthy<\/font> \n##### <font color=blue>RPDE,D2 - Two nonlinear dynamical complexity measures<\/font> \n##### <font color=blue>DFA - Signal fractal scaling exponent<\/font> \n##### <font color=blue>spread1,spread2,PPE - Three nonlinear measures of fundamental frequency variation<\/font>","6769181a":"#### <font color=orange>*It can be inferred that:*<\/font>\n##### <font color=blue><font color=red>MDVP:Fo(Hz)<\/font> *- Average vocal fundamental frequency of a person suffering by PD is less than by <font color=red>36Hz<\/font> compared to a healthy person.*<\/font>\n##### <font color=blue><font color=red>MDVP:Fhi(Hz)<\/font> *- Maximum vocal fundamental frequency of a person suffering by PD is less than by <font color=red>35Hz<\/font> compared to a healthy person.*<\/font>\n##### <font color=blue><font color=red>MDVP:Flo(Hz)<\/font> *- Minimum vocal fundamental frequency of a person suffering by PD is less than by <font color=red>39Hz<\/font> compared to a healthy person.*<\/font>\n##### <font color=blue><font color=red>MDVP:Jitter(%),MDVP:Jitter(Abs),MDVP:RAP,MDVP:PPQ,Jitter:DDP<\/font> *- Several measures of variation in fundamental frequency have considerable differences between a healthy person & a person suffering from PD.*<\/font>\n##### <font color=blue><font color=red>MDVP:Shimmer,MDVP:Shimmer(dB),Shimmer:APQ3,Shimmer:APQ5,MDVP:APQ,Shimmer:DDA<\/font> *- Several measures of variation in amplitude also have considerable differences between a healthy person & a person suffering from PD.*<\/font>\n##### <font color=blue><font color=red>NHR,HNR<\/font> *- Two measures of ratio of noise to tonal components in the voice also have considerable differences.*<\/font>\n##### <font color=blue>*Similar differences are observed in other features too*<\/font>","3a8ec341":"#### <font color=blue>Printing Sensitivity & Specificity<\/font>","5d67c3f0":"#### <font color=blue>*Checking for skewness*<\/font>","480f1a55":"##### <font color=blue>Multicollinearity is observed between many features.<\/font>\n##### <font color=blue>Basically features MDVP:Shimmer,MDVP:Shimmer(dB),Shimmer:APQ3,Shimmer:APQ5,MDVP:APQ,Shimmer:DDA which are the measures of variation in amplitude has Multicollinearity<\/font>\n##### <font color=blue>Spread1 & PPE are also related<\/font> ","9dc37fff":"##### <font color=purple>*It is a nonlinear measure of fundamental frequency variation.From 0.1 to 0.5 is the range which correlates to persons with PD.This is a good indicator as there is separation.As we can observe in Kde rug, values are densed between 0.2 & 0.3 suggesting need to look more here.It might have outliers.*<\/font>","2c4577ea":"##### <font color=purple>*It is also one of the measures of variation in fundamental amplitude.From 0.00 to 0.08 is the range which correlates to persons with PD. This is not so good indicator there is very little separation as per kde graph.As we can observe in Kde rug, values are densed near 0.025 on both sides suggesting need to look more here.It also has normal distribution with outliers. Little bit positively skewed.*<\/font>","33f2a846":"#### <font color=blue>Now, trying one last thing,trying to observe effects of power transformed data on Gradient Boosting<\/font>","d0e81369":"##### <font color=purple>*It is one of the measures of variation in fundamental frequency.From 0.004 to 0.030 is range which correlates to persons with PD. This is not at all a good indicator as it does not have separation as per kde graph.It has normal distribution with outliers. Little bit positively skewed.*<\/font>","ca497e7b":"##### <font color=blue>*Random sample records*<\/font>","87ace94b":"#### <font color=orange>*It can be inferred that:*<\/font>\n##### <font color=blue>*MDVP:Fo(Hz):-Mean is greater than median indicates positive skewness.There are outliers also considering the range & 99.7% of data lies between 30.0584 to 278.3988.Average vocal fundamental frequency of people does not have much variance.*<\/font>\n##### <font color=green>*MDVP:Fhi(Hz):-Mean is quite greater than median.It is more positively skewed than <font color=blue>**MDVP:Fo(Hz)**<\/font>. 99.7% of data lies between -77.3697 to 471.5795 & max value is 592. There are lot outliers on positive side in this feature.Maximum vocal fundamental frequency of people is quite on higher side than the mean in this dataset.*<\/font>\n##### <font color=blue>*MDVP:Flo(Hz):-Here also mean is greater than median so it is also positively skewed.99.7% of data is in between -14.2396 & 246.8888 & max value is 239.Minimum value is 65. So,there are many outliers on positive side.Minimum vocal fundamental frequency is also higher than average for some people.*<\/font>\n##### <font color=green>*MDVP:Jitter(%):-Mean is almost equal to median indicating normal distribution.99.7% of data is in between -0.0083 & 0.0207.Min value is 0.001680 & max is 0.033160. There are many outliers on positive side.*<\/font>\n##### <font color=blue>*MDVP:Jitter(Abs):-Mean is quite greater than median.Positively skewed.99.7% of data is between -0.0001 & 0.0001. Min value is 0.00007 & max is 0.000260. There are outliers on positive side.*<\/font>\n##### <font color=green>*MDVP:RAP:-Mean is greater than median.Positively skewed. 99.7% of data is between -0.0056 & 0.0122. Min value is 0.000680 & max is 0.021440.There are many outliers on positive side.*<\/font>\n##### <font color=blue>*MDVP:PPQ:-Again Mean is greater than median.Positively skewed.99.7% of data is between -0.0049 & 0.0117. Min value is 0.000920 & max is 0.019580.There are outliers on positive side.*<\/font> \n##### <font color=green>*Jitter:DDP:-Again Mean is greater than median.Positively skewed.99.7% of data is between -0.0168 & 0.0366. Min value is 0.002040 & max is 0.064330.There are outliers on positive side.*<\/font>\n##### <font color=blue>*MDVP:Shimmer:-There is not much difference between mean and median.99.7% of data is between -0.0269 & 0.0863. Min value is 0.009540 & max is 0.119080.There are outliers on positive side.*<\/font>\n##### <font color=green>*MDVP:Shimmer(dB):-Again Mean is greater than median.Positively skewed.99.7% of data is between -0.3023 & 0.8669.Min value is 0.085000 & max is 1.302000.There are many outliers on positive side*<\/font>\n##### <font color=blue>*Shimmer:APQ3:-There is not much difference between mean and median.99.7% of data is between -0.0148 & 0.0462. Min value is 0.004550 & max is 0.056470.There are outliers on positive side.*<\/font>\n##### <font color=green>*Shimmer:APQ5:-There is some difference between mean and median.99.7% of data is between -0.0182 & 0.054. Min value is 0.005700 & max is 0.079400.There are some outliers on positive side.*<\/font> \n##### <font color=blue>*MDVP:APQ:-There is some difference between mean and median.99.7% of data is between -0.0267 & 0.0749. Min value is 0.007190 & max is 0.137780.There might be outliers on positive side.*<\/font>\n##### <font color=green>*Shimmer:DDA:-There is some difference between mean and median.99.7% of data is between -0.0444 & 0.1384. Min value is 0.013640 & max is 0.169420. Outliers are present here also*<\/font>\n##### <font color=blue>*NHR:-Mean is greater than median.Positively skewed.99.7% of data is between -0.0965 & 0.1461. Min value is 0.000650 & max is 0.314820.There are many outliers on positive side.*<\/font>\n##### <font color=green>*HNR:-Mean is less than median, though just a little bit.Just a bit of negative skewness.99.7% of data is between 8.6087 & 35.1633. Min value is 8.44 & max is 33.04.There might be outliers on negative side*<\/font>\n##### <font color=blue>*RPDE:-Mean & median are very close to each other.The difference is of 0.002. 99.7% of data is between 0.1867 & 0.8103.Min value is 0.256570 & max is 0.685151.Looks like we are getting somewhat normal distribution here.*<\/font> \n##### <font color=green>*DFA:-Mean is just a little bit less than median.99.7% of data is between 0.5521 & 0.828771.Min value is 0.8841 & max is 0.825288.Looks like we are getting normal distribution without outliers here.*<\/font>\n##### <font color=blue>*spread1:-There is some difference between mean and median. Little bit of positive skewness.99.7% of data is between -8.955 & -2.4138. Min value is -7.964984 & max is -2.434031. Somewhat normal distribution with just a little bit of positive skewness.*<\/font>\n##### <font color=green>*spread2:-Little difference between mean and median.99.7% of data is between -0.0237 & 0.4767. Min value is 0.006274 & max is 0.450493.Looks like normal distribution*<\/font>\n##### <font color=blue>*D2:- Small difference between mean and median. Mean is just a bit more than median.99.7% of data is between 1.2334 & 3.5302. Seems like it has normal distribution*<\/font>\n##### <font color=green>*PPE:-Some difference between mean and median but we may gt uniform distribution.99.7% of data is between -0.0638 & 0.477.*<\/font> \n##### <font color=blue>*Status:- It is encoded categorical feature, hence mean, median etc. doesn't make any sense. This will be more analyzed in bivariate\/multivariate analysis. This is also our target feature*<\/font>","3a1ac6a6":"##### <font color=blue>*Last 5 records*<\/font>","3e3629a3":"### <font color=purple>*Overall Gradient Boost seems to be winning here but consdering my very limited knowledge in ML, my conclusion can be naive*<\/font>","def49f65":"##### <font color=blue>Finding intervals for MDVP:Fo(Hz) as per min & max values<\/font>","53e9d952":"#### <font color=blue>07 Checking influence of MDVP:Fo(Hz) & spread2 on status.<\/font>","426acd02":"##### <font color=purple>*It is also one of the measures of variation in fundamental amplitude.From 0.00 to 0.14 is the range which correlates to persons with PD. This is not a good indicator as there is no separation as per kde graph.As we can observe in Kde rug, values are densed near 0.05 on both sides suggesting need to look more here.It also has normal distribution with outliers. Little bit positively skewed.*<\/font>","a81b1f51":"##### <font color=purple>*Positively skewed. For some people, average vocal fundamental frequency is higher than the average. Mainly because it is higher for healthy persons as seen above.Frequencies ranging from 100Hz to 200Hz are the ones of persons with PD. Especially 145Hz to around 160Hz frequency ranges are for PD persons only. There is good separation between two kde graphs indicating that this is a good indicator.As we can observe in Kde rug, values are densed near 150Hz on both sides suggesting need to look more here. Also two peaks suggests that the this data might have come from two different sources.There are no outliers here.*<\/font>","8f4c2d53":"##### <font color=blue>So above table shows the 15 best features selected by SelectKBest<\/font>","d632dd3e":"#### <font color=blue>*Checking for outliers*<\/font>","2fbcc957":"#### <font color=purple>_XGBoost's performance is seems just okay here._<\/font>","ed146d33":"#### <font color=orange>It can be inferred that :<\/font>\n##### <font color=blue>relation of PPE with spread2 is quite similar with what it has with spread1.<\/font>\n##### <font color=blue>as value of PPE increasing, cases of PD are increasing and points are getting densed.<\/font>","95492516":"### <font color=purple>Creating Second Baseline Model Using GradientBoostingClassifier With Features Selected By Feature Selection<\/font>","3663bbf9":"#### <font color=purple>*Training Random Forest*<\/font>\n#### <font color=purple>*Using criterion='entropy' & keeping everything else to default value.*<\/font>","1dcd945d":"##### <font color=blue>Creating model with default parameters<\/font>","939a7674":"##### <font color=blue>Considering Confidence Level = 99% so Z* value need to be taken is  2.58<\/font>","5e6848a1":"##### <font color=blue>*We can observe that many of the features are skewed.Mostly positively skewed.*<\/font>","138d2368":"#### <font color=blue>Bagging With Regularized Decision Trees' accuracy variance is fine.<\/font>","95144ff6":"#### <font color=blue>*Analysis of Basic Stats*<\/font>","9505bcf6":"#### <font color=purple>In first baseline we had got 86% test accuracy 86% target class precision 95% target class accuracy, 2 false negatives & 6 false positives.<\/font>\n#### <font color=purple>In second baseline after feature selection we got 92% test accuracy, 91% target class precision, 97% target class accuracy, just 1 false negative & 4 false positives.<\/font>\n### <font color=purple>_In third baseline after feature selection & rectifying class imbalance problem we are getting **100% test accuracy, 100% target class precision, 100% target class accuracy, 0 false negatives & 0 false positives**_.<\/font>\n### <font color=purple>So, we have got amazing results with Gradient Boosting after feature selection & rectifying class imbalance problem.<\/font>\n##### <font color=blue>Keeping random state 7 or 34 in SMOTE is giving above mentioned results.<\/font>","bffb9954":"#### <font color=blue>09 Checking influence of PPE & spread2 on status.<\/font>","b6cfaab3":"### <font color=purple>*Training XGBoost Classifier*<\/font>","0f3fbe7d":"#### <font color=blue>Bagging With Logistic Regressions' accuracy variance is also high & its performace is quite bad<\/font>","dfea83db":"##### <font color=blue>Considering Confidence Level = 99% so Z* value need to be taken is  2.58<\/font>","f7e1c048":"### <font color=red>Univariate Analysis<\/font>","6cfa921c":"##### <font color=blue>We are getting 86% test accuracy 86% target class precision 95% target class accuracy & just 2 false negatives<\/font>\n##### <font color=blue>So, we have got encouraging results with Gradient Boosting even with raw features.<\/font>\n##### <font color=blue>Now we will move ahead and do feature selection<\/font>","a2c6887e":"##### <font color=blue>*There are no null values*<\/font>","5c17187b":"#### <font color=blue>AdaBoost With Regularized Decision Trees' accuracy is high and accuracy variance is low.<\/font>","59f3b609":"##### <font color=blue>In first baseline we had got 86% test accuracy 86% target class precision 95% target class accuracy, 2 false negatives & 6 false positives.<\/font>\n##### <font color=blue>Now after feature selection we are getting 92% test accuracy 91% target class precision 97% target class accuracy, just 1 false negative & 4 false positives.<\/font>\n##### <font color=blue>So, we have got even better results with Gradient Boosting with after feature selection.<\/font>","06894262":"##### <font color=purple>*It is a nonlinear dynamical complexity measure.From 0.3 to 0.7 is the range which correlates to persons with PD.This also isn't a good indicator as there is no separation.As we can observe in Kde rug, values are densed near 0.5 & between 0.6 & 0.7 suggesting need to look more here.It does not have outliers.*<\/font>","f7a91fb2":"##### <font color=purple>*It is a nonlinear dynamical complexity measure.From 1.8 to 3.5 is the range which correlates to persons with PD.This won't be good indicator as there is no separation.As we can observe in Kde rug, values are densed near 2.5 suggesting need to look more here.It does not have outliers.*<\/font>","3262e0c5":"#### <font color=purple>*Gradient Boosting Improved a Little Bit*<\/font>","ec4aa877":"##### <font color=purple>*It is one of the measures of variation in fundamental amplitude.From 0 to 1 is the range which correlates to persons with PD. This also is not a good indicator as it too does have only slight separation as per kde graph.It also has normal distribution with outliers. Little bit positively skewed.*<\/font>","f4116bf6":"#### <font color=blue>Printing Sensitivity & Specificity<\/font>"}}