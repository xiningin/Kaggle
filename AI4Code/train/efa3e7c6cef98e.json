{"cell_type":{"7c0ec198":"code","1b606c87":"code","f773b164":"code","2203a82b":"code","1bd57511":"code","d5aad539":"code","6cadf1e8":"code","8c19aa81":"code","aebe5bee":"code","c275b1de":"code","90c8ecb7":"code","9e1240da":"code","1ded0fc8":"code","7a211021":"code","73782bd7":"code","920a3221":"code","ba2aa6a7":"code","136e660c":"code","4841f530":"code","0dd04005":"code","77433eb9":"code","2cd7002f":"code","e6f33594":"code","302c0809":"code","f3f191f3":"code","17640237":"code","1ea98752":"code","862ee535":"code","c5856a3b":"code","0529649c":"code","2e195e1f":"code","66e0213c":"code","1d80de82":"code","349dd23c":"code","b41d502f":"code","ac87fb0e":"code","8ac41342":"code","fac863b9":"code","e37f0a75":"code","f8d3d394":"code","da599667":"code","6deccbda":"code","915189a6":"code","eb0ee951":"code","11d30547":"code","e33c5d63":"code","4afabeb7":"code","eb869d04":"code","a610e988":"code","d31805b8":"code","f79ae0c0":"code","fd2fa0a6":"code","56158e9e":"code","50e02db9":"code","2aa37ecf":"code","245474f9":"code","934131e3":"code","320538b2":"code","d7894ae5":"code","28c335c8":"code","adbc14b9":"code","4d37d89e":"code","9082b8bb":"code","79a9613c":"code","e1d73655":"code","f720a5a6":"markdown","a9a43484":"markdown","0a36d23f":"markdown","6e5e98c0":"markdown","50add963":"markdown","9c8a89bd":"markdown","504de4d9":"markdown","3d0c102a":"markdown","966cd613":"markdown","a26d4599":"markdown","80ab60f3":"markdown","6049e28a":"markdown","5ec7cae8":"markdown","615099c0":"markdown","c1753e89":"markdown","9a4b0e7e":"markdown"},"source":{"7c0ec198":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as pl\n%matplotlib inline","1b606c87":"train=pd.read_csv('..\/input\/train.csv')\ntest=pd.read_csv('..\/input\/test.csv')","f773b164":"train.head()","2203a82b":"train.info()","1bd57511":"test.info()","d5aad539":"test_passengerid=test.PassengerId","6cadf1e8":"df=train.append(test,ignore_index=True,sort=False)","8c19aa81":"df.info()","aebe5bee":"df.isnull().sum()","c275b1de":"male_avg_age=df[df.Sex=='male'].Age.mean()","90c8ecb7":"female_avg_age=df[df.Sex=='female'].Age.mean()","9e1240da":"df[df.Sex=='male'].Age.hist(bins=[0,10,20,30,40,50,60,70,80,90,100])\npl.xlabel('Male')\npl.text(male_avg_age, 200,male_avg_age)\npl.show()\ndf[df.Sex=='female'].Age.hist(bins=[0,10,20,30,40,50,60,70,80,90,100])\npl.text(female_avg_age, 100,female_avg_age)\npl.xlabel('Female')\npl.show()","1ded0fc8":"df['Name']","7a211021":"df['title']=df.Name.apply(lambda n: n.split(',')[1].split('.')[0].strip())","73782bd7":"df['title'].unique()","920a3221":"titles_cat = {\n    \"Capt\":       \"Officer\",\n    \"Col\":        \"Officer\",\n    \"Major\":      \"Officer\",\n    \"Jonkheer\":   \"Royal\",\n    \"Don\":        \"Royal\",\n    \"Sir\" :       \"Royal\",\n    \"Dr\":         \"Officer\",\n    \"Rev\":        \"Officer\",\n    \"the Countess\":\"Royal\",\n    \"Dona\":       \"Royal\",\n    \"Mme\":        \"Mrs\",\n    \"Mlle\":       \"Miss\",\n    \"Ms\":         \"Mrs\",\n    \"Mr\" :        \"Mr\",\n    \"Mrs\" :       \"Mrs\",\n    \"Miss\" :      \"Miss\",\n    \"Master\" :    \"Master\",\n    \"Lady\" :      \"Royal\"\n}","ba2aa6a7":"df.title=df.title.map(titles_cat)\ndf.title.value_counts()","136e660c":"ag_group=df.groupby(['Sex','Pclass','title'])\nag_group.Age.median()","4841f530":"df.Age=ag_group.Age.apply(lambda a:a.fillna(a.median()))","0dd04005":"df.Cabin.unique()","77433eb9":"df.Cabin=df.Cabin.fillna('U')","2cd7002f":"df.Embarked.value_counts()","e6f33594":"df.Embarked=df.Embarked.fillna(df.Embarked.value_counts().index[0])","302c0809":"df.Fare=df.Fare.fillna(df.Fare.median())","f3f191f3":"df.isnull().sum()","17640237":"df.Cabin=df.Cabin.apply(lambda x:x[0])","1ea98752":"df.head()","862ee535":"df.Fare.hist()","c5856a3b":"from sklearn import preprocessing","0529649c":"mm_scaler=preprocessing.MinMaxScaler()","2e195e1f":"data=df['Fare'].values\nscaled_fare=mm_scaler.fit_transform(pd.DataFrame(data))","66e0213c":"scaled_fare=pd.DataFrame(scaled_fare,columns=['Nm_Fare'])","1d80de82":"prepared_data=pd.concat([df,scaled_fare],axis=1)","349dd23c":"prepared_data.head()","b41d502f":"prepared_data.Sex=prepared_data.Sex.map({'male':0,'female':1})","ac87fb0e":"pclass_dum=pd.get_dummies(prepared_data.Pclass,prefix='Pclass')\ntitle_dum=pd.get_dummies(prepared_data.title,prefix='title')\ncabin_dum=pd.get_dummies(prepared_data.Cabin,prefix='Cabin')\nemb_dum=pd.get_dummies(prepared_data.Embarked,prefix='Embarked')","8ac41342":"prepared_data=pd.concat([prepared_data,pclass_dum,title_dum,cabin_dum,emb_dum],axis=1)","fac863b9":"prepared_data.drop(['Fare','Pclass','title','Cabin','Embarked','Name','Ticket'],axis=1,inplace=True)\nprepared_data.head()","e37f0a75":"train_len=len(train)\ntrain_len","f8d3d394":"train=prepared_data[ :train_len]\ntest=prepared_data[train_len: ]","da599667":"train.Survived.isnull().any()","6deccbda":"test.Survived.notnull().any()","915189a6":"train.Survived=train.Survived.astype(int)","eb0ee951":"X=train.drop('Survived',axis=1).values\ny=train.Survived.values","11d30547":"X_test=test.drop('Survived',axis=1).values","e33c5d63":"from sklearn.model_selection import train_test_split","4afabeb7":"X_train, X_train_test, y_train, y_train_test = train_test_split(X,y,test_size=0.1,random_state=0)","eb869d04":"print(X_train.shape,X_train_test.shape,y_train.shape,y_train_test.shape)","a610e988":"from sklearn.metrics import accuracy_score,fbeta_score\nfrom time import time","d31805b8":"def train_predict(model, sample_size, X_train,y_train, X_test, y_test):\n    result={}\n    start=time()\n    model=model.fit(X_train[:sample_size],y_train[:sample_size])\n    end=time()\n    \n    result['train_time']=end-start\n    \n    start=time()\n    pred_test=model.predict(X_test)\n    pred_train=model.predict(X_train)\n    end=time()\n    \n    result['pred_time']=end-start\n    \n    result['acc_train'] = accuracy_score(y_train, pred_train)\n    result['acc_test'] = accuracy_score(y_test, pred_test)\n    \n    result['f_train'] = fbeta_score(y_train, pred_train, beta = 0.5)\n    result['f_test'] = fbeta_score(y_test, pred_test, beta = 0.5)\n    \n    return(result)","f79ae0c0":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import RandomForestClassifier","fd2fa0a6":"model1=LogisticRegression(random_state=0)\nmodel2=GradientBoostingClassifier(random_state=0)\nmodel3=RandomForestClassifier(random_state=0)","56158e9e":"result={}\nsample_5=int(len(y_train)*0.05)\nsample_25=int(len(y_train)*0.25)\nsample_100=len(y_train)\n\nfor model in [model1, model2, model3]:\n    model_name=model.__class__.__name__\n    result[model_name]={}\n    for i,sample in enumerate([sample_5,sample_25,sample_100]):\n        result[model_name][i]=train_predict(model,sample,X_train, y_train,X_train_test,y_train_test)","50e02db9":"print(result)","2aa37ecf":"import matplotlib.pyplot as pl\nimport matplotlib.patches as mpatches\ndef evaluate(results):\n    \n    # Create figure\n    fig, ax = pl.subplots(2, 3, figsize = (11,7))\n\n    # Constants\n    bar_width = 0.3\n    colors = ['#A00000','#00A0A0','#00A000']\n    \n    # Super loop to plot four panels of data\n    for k, learner in enumerate(results.keys()):\n        for j, metric in enumerate(['train_time', 'acc_train', 'f_train', 'pred_time', 'acc_test', 'f_test']):\n            for i in np.arange(3):\n                \n                # Creative plot code\n                ax[j\/\/3, j%3].bar(i+k*bar_width, results[learner][i][metric], width = bar_width, color = colors[k])\n                ax[j\/\/3, j%3].set_xticks([0.45, 1.45, 2.45])\n                ax[j\/\/3, j%3].set_xticklabels([\"1%\", \"10%\", \"100%\"])\n                #ax[j\/\/3, j%3].set_xlabel(\"Training Set Size\")\n                ax[j\/\/3, j%3].set_xlim((-0.1, 3.0))\n    \n    # Add unique y-labels\n    ax[0, 0].set_ylabel(\"Time (in seconds)\")\n    ax[0, 1].set_ylabel(\"Accuracy Score\")\n    ax[0, 2].set_ylabel(\"F-score\")\n    ax[1, 0].set_ylabel(\"Time (in seconds)\")\n    ax[1, 1].set_ylabel(\"Accuracy Score\")\n    ax[1, 2].set_ylabel(\"F-score\")\n    \n    # Add titles\n    ax[0, 0].set_title(\"Model Training\")\n    ax[0, 1].set_title(\"Accuracy Score on Training Subset\")\n    ax[0, 2].set_title(\"F-score on Training Subset\")\n    ax[1, 0].set_title(\"Model Predicting\")\n    ax[1, 1].set_title(\"Accuracy Score on Testing Set\")\n    ax[1, 2].set_title(\"F-score on Testing Set\")\n    \n    \n    # Set y-limits for score panels\n    ax[0, 1].set_ylim((0, 1))\n    ax[0, 2].set_ylim((0, 1))\n    ax[1, 1].set_ylim((0, 1))\n    ax[1, 2].set_ylim((0, 1))\n\n    # Create patches for the legend\n    patches = []\n    for i, learner in enumerate(results.keys()):\n        patches.append(mpatches.Patch(color = colors[i], label = learner))\n    pl.legend(handles = patches, bbox_to_anchor = (-.80, 2.53), \\\n               loc = 'upper center', borderaxespad = 0., ncol = 3, fontsize = 'x-large')\n    \n    # Aesthetics\n    pl.suptitle(\"Performance Metrics for Three Supervised Learning Models\", fontsize = 16, y = 1.10)\n    pl.tight_layout()\n    pl.show()","245474f9":"evaluate(result)","934131e3":"from sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import make_scorer","320538b2":"mymodel= model1","d7894ae5":"dual=[True,False]\nmax_iter=[100,110,120,130,140]\npenalty=['l2']\nparam_grid=dict(dual=dual,max_iter=max_iter,penalty=penalty)","28c335c8":"grid=GridSearchCV(estimator=mymodel,param_grid=param_grid,cv=3,n_jobs=-1)\ngrid.fit(X,y)","adbc14b9":"best_model=grid.best_estimator_","4d37d89e":"print(best_model)\nprint(grid.best_score_)","9082b8bb":"predictions=best_model.predict(X_test)","79a9613c":"load_kaggle=pd.DataFrame({'PassengerId':test_passengerid,'Survived':predictions})\n\nload_kaggle.to_csv('.\/Titanic_Logistic_Regression.csv',index=False)","e1d73655":"load_kaggle","f720a5a6":"**Now lets see which of the classification model 'Logistic Regression', 'Gradient Boost Classifier' & 'Random Forest Classifier' fits best in this situation. Usually this judgement of model done by the accuracy score between predicted label data and the known test label data. But in this situation we don't have any know test label data, so we just devide the train data in 90-10 train and test & then by checking the score of the known 10% test data we can choose the best classifier algorithm.**","a9a43484":"**As age has quite a large number of null values and it can seriusly effect the accuracy of model as child was most likely to be survived or escaped first than a grown adult., so instead of replacing age with median or mean we can anlyse other coulmns and get better replacement for null values in age.** ","0a36d23f":"**Lets look at null value first!!**","6e5e98c0":"**Lets start with filling age first.**","50add963":"**As the distribution of Fare is quite bad, we can normalize it.**","9c8a89bd":"**Lets divide this titles in some categories.**","504de4d9":"**Lets convert the categorical variables to dummy variable now:**","3d0c102a":"**Ok now lets devide the data back to train and test. And split the train data into features and label.**","966cd613":"**Now we have all the data, lets get the features out ditinctively i.e. Feature Engineering**","a26d4599":"**In below train_predict function we training the model given in the parameter. And also passing sample size, that is lenght of the training size data, with which we want to traun the data. And after training we calculating the accuaracy score and f-beta score to compare in future.**","80ab60f3":"**Here we are taking 3 different sample size 5%,25% and 100% of training data and iterating through all the 3 models with all 3 sample size and storing them in result variable to plot them later see the differnce in training time, predicting time and accuracy scores.**","6049e28a":"**We are setting up the vallues for hyperparameters and then using GridSearch we gonna itterate over all possible combination of those parameters, and then we are gonna choose the model with the best score. As here we setting up quite a few parameters the Gridsearch won't take so much time, but geneally grid search is very time taking process and can be replaced bt baysian technique to get the best fit model.**","5ec7cae8":"**There is no such importance on number in cabin, but the cabin's first letter depicts in which compartment the cabin is, so lets just get out the first char out of the Cabin.**","615099c0":"**By looking at the charts for the test data(2nd axis) we can see that Logistic regression has the maximum accuracy score as compared to other two classifiers. So we are choosing Logisic regression model to predict the result on test data.**","c1753e89":"**Looking at all the features, most probably 'Sex', 'Pclass' & 'title' can be used to fill null values in Age.**","9a4b0e7e":"**Lets check Cabin now**"}}