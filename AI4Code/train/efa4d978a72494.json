{"cell_type":{"39a718d1":"code","cfa0981c":"code","205fa7c7":"code","7bf11e23":"code","cdf2981e":"code","32dd6cf9":"code","0730c6e2":"code","a470a9ff":"code","13c2dc56":"code","d2549a8f":"code","70633776":"code","a62b79ca":"code","6943a436":"code","a8c1e609":"code","af6c7fce":"code","2860c0cb":"code","62ab49bc":"code","940a7bdf":"code","1b55cecb":"code","9be1de2e":"code","fa11eabd":"code","e32da912":"code","321973ea":"code","052e0592":"code","0202d35d":"code","7c94039c":"code","43a8bf5d":"code","eff85558":"code","13f21c74":"code","75abdeb9":"code","0bbe68aa":"code","2d658a25":"code","c6375213":"code","7f34c6e9":"code","3e8df94e":"code","0888fea9":"code","26c622f5":"code","563b4960":"code","93f04592":"code","ebf22ff3":"code","f06362d1":"code","148a8f43":"code","2b11fdcd":"code","b5063056":"code","87c69c1c":"code","9eca6587":"code","b1ea49b7":"code","58c39451":"code","885d500e":"code","dbb29944":"code","705046f8":"code","2e110dee":"code","723e554f":"code","b1fd3167":"code","fcdf3364":"code","fb38bdb5":"code","c0c5fa95":"code","37c620a9":"code","dae0f483":"code","d21d8287":"code","7c1c8468":"code","6e33ed77":"code","cccfb8aa":"code","a5067f4d":"code","363e56e9":"code","9d888415":"code","501fd969":"code","1fbc2cf4":"code","2dfcf11e":"code","8a5f12e6":"code","cdca130a":"code","e17d0f89":"code","0cd49f79":"code","9766e985":"code","accdc001":"code","77944ab0":"code","52730697":"code","85d0e4d8":"code","9dcdac09":"code","6555b6ab":"code","e56ba83a":"code","a9ce852e":"code","c0da87c9":"code","8c634116":"code","9518483e":"code","e2f705bf":"code","c8f14329":"markdown","19a1d19e":"markdown","c1b538df":"markdown","b8904f26":"markdown","899c94ce":"markdown","33884fd1":"markdown","fe26bdb7":"markdown","bb4460e6":"markdown","039ee18e":"markdown","53f4f725":"markdown","6b88927f":"markdown","0e88692c":"markdown","2bcbb077":"markdown","5a068cfd":"markdown","ae664c92":"markdown","31a527b5":"markdown","968bb6da":"markdown","771fe861":"markdown","7668eb96":"markdown","1989af98":"markdown","39e8f2fc":"markdown","c7823b64":"markdown","2699b0f1":"markdown","9a54a724":"markdown","3b6232bf":"markdown","44c4a25d":"markdown","a30ff210":"markdown","bab848d3":"markdown","3f2d0cbe":"markdown","2f31697c":"markdown","f372faef":"markdown","d9edbf48":"markdown","65fd1ace":"markdown","10d0ff52":"markdown","d8acbf52":"markdown","ecc0fc95":"markdown","980c416a":"markdown","4fab2163":"markdown"},"source":{"39a718d1":"# Basic\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Common Tools\nfrom sklearn.preprocessing import LabelEncoder\nfrom collections import Counter\n\n#Algorithms\nfrom sklearn import ensemble, tree, svm, naive_bayes, neighbors, linear_model, gaussian_process, neural_network\nimport xgboost as xgb\nfrom xgboost.sklearn import XGBClassifier\n\n# Model\nfrom sklearn.metrics import accuracy_score, f1_score, auc, roc_curve, roc_auc_score\nfrom sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV, cross_val_score\n#from sklearn.ensemble import VotingClassifier\n\n#Configure Defaults\nimport warnings\nwarnings.filterwarnings('ignore')\n%matplotlib inline","cfa0981c":"pd.__version__","205fa7c7":"np.__version__","7bf11e23":"sns.__version__","cdf2981e":"train = pd.read_csv('..\/input\/train.csv')","32dd6cf9":"train.shape","0730c6e2":"test = pd.read_csv('..\/input\/test.csv')","a470a9ff":"test.shape","13c2dc56":"sns.countplot(x='Survived', data=train)\nprint(\"Survival rate: \", train.Survived.sum()\/train.Survived.count())","d2549a8f":"train.columns","70633776":"train.info()","a62b79ca":"train.head()","6943a436":"train.describe()","a8c1e609":"# Describe categorical features\ntrain.describe(include=['O'])","af6c7fce":"sns.heatmap(train.isnull())","2860c0cb":"sns.pairplot(train, hue=\"Survived\")","62ab49bc":"a = sns.FacetGrid(train, hue = 'Survived', aspect=4 )\na.map(sns.kdeplot, 'Age', shade= True )\na.set(xlim=(0, train['Age'].max()))\na.add_legend()","940a7bdf":"sns.boxplot(x=\"Pclass\", y=\"Fare\",data=train)","1b55cecb":"train[['Pclass', 'Survived']].groupby(['Pclass'], as_index=False).mean().sort_values(by='Survived', ascending=False)","9be1de2e":"train[[\"Sex\", \"Survived\"]].groupby(['Sex'], as_index=False).mean().sort_values(by='Survived', ascending=False)","fa11eabd":"train[[\"SibSp\", \"Survived\"]].groupby(['SibSp'], as_index=False).mean().sort_values(by='Survived', ascending=False)","e32da912":"train[[\"Parch\", \"Survived\"]].groupby(['Parch'], as_index=False).mean().sort_values(by='Survived', ascending=False)","321973ea":"q = train.Fare.quantile(0.99)\nq","052e0592":"train = train[train['Fare'] < q]","0202d35d":"#Save Id for the submission at the very end.\nId = test['PassengerId']","7c94039c":"#Get split marker\nsplit = len(train)","43a8bf5d":"#Merge into one dataset\ndata =  pd.concat(objs=[train, test], axis=0).reset_index(drop=True)","eff85558":"#We don't need the Id anymore now.\ndata.drop('PassengerId', axis=1, inplace=True)","13f21c74":"data.shape","75abdeb9":"sns.distplot(data['Age'].dropna())","0bbe68aa":"median = data[\"Age\"].median()\nstd = data[\"Age\"].std()\nis_null = data[\"Age\"].isnull().sum()\nrand_age = np.random.randint(median - std, median + std, size = is_null)\nage_slice = data[\"Age\"].copy()\nage_slice[np.isnan(age_slice)] = rand_age\ndata[\"Age\"] = age_slice\ndata[\"Age\"] = data[\"Age\"].astype(int)","2d658a25":"#Check\nsns.distplot(data['Age'])","c6375213":"data['Embarked'].fillna(data['Embarked'].mode()[0], inplace = True)","7f34c6e9":"data[\"Embarked\"].isnull().sum()","3e8df94e":"data['Fare'].fillna(data['Fare'].mean(), inplace = True)","0888fea9":"data[\"CabinBool\"] = (data[\"Cabin\"].notnull().astype('int'))","26c622f5":"sns.barplot(x=\"CabinBool\", y=\"Survived\", data=data)","563b4960":"data['Deck'] = data.Cabin.str.extract('([a-zA-Z]+)', expand=False)\ndata[['Cabin', 'Deck']].sample(10)\ndata['Deck'] = data['Deck'].fillna('Z')\ndata = data.drop(['Cabin'], axis=1)","93f04592":"data.groupby(['Embarked'])['Survived'].count()","ebf22ff3":"data['FamilySize'] = data['SibSp'] + data['Parch']","f06362d1":"data['IsAlone'] = 1 #default value","148a8f43":"data['IsAlone'].loc[data['FamilySize'] > 0] = 0","2b11fdcd":"sns.factorplot(x=\"IsAlone\", y=\"Survived\", data=data, kind=\"bar\")","b5063056":"a = sns.FacetGrid(train, hue = 'Survived', aspect=4 )\na.map(sns.kdeplot, 'Age', shade= True )\na.set(xlim=(0, train['Age'].max()))\na.add_legend()","87c69c1c":"# Bucketize\nbins = [-1, 13, 31, 60, 80]\nlabels = ['Child', 'Young Adult', 'Adult', 'Senior']\ndata['AgeBin'] = pd.cut(data[\"Age\"], bins, labels = labels).astype('object')","9eca6587":"#data['AgeBand'] = pd.cut(data['Age'], 5)\n#data[['AgeBand', 'Survived']].groupby(['AgeBand'], as_index=False).mean().sort_values(by='AgeBand', ascending=True)","b1ea49b7":"# Plot\nsns.factorplot(x=\"AgeBin\", y=\"Survived\", data=data, kind=\"bar\")","58c39451":"data['IsBaby'] = 0 #default value","885d500e":"data['IsBaby'].loc[data['Age'] <= 5] = 1","dbb29944":"sns.factorplot(x=\"IsBaby\", y=\"Survived\", data=data, kind=\"bar\")","705046f8":"data['Title'] = data.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)","2e110dee":"data['Title'] = data['Title'].replace(['Lady', 'Capt', 'Col', 'Don', 'Dr', 'Major', 'Rev', 'Jonkheer', 'Dona'], 'Rare')  \ndata['Title'] = data['Title'].replace(['Countess', 'Lady', 'Sir'], 'Royal')\ndata['Title'] = data['Title'].replace('Mlle', 'Miss')\ndata['Title'] = data['Title'].replace('Ms', 'Miss')\ndata['Title'] = data['Title'].replace('Mme', 'Mrs')","723e554f":"data['Title'] = data['Title'].astype('object')","b1fd3167":"data[['Title', 'Survived']].groupby(['Title'], as_index=False).mean()","fcdf3364":"f = sns.FacetGrid(train, hue = 'Survived', aspect=4 )\nf.map(sns.kdeplot, 'Fare', shade= True )\nf.set(xlim=(0, train['Fare'].max()))\nf.add_legend()","fb38bdb5":"# Bucketize\nbins = [-np.inf, 20, 30, 110, np.inf]\nlabels = ['Low', 'Mid', 'High', 'Extreme']\ndata['FareBin'] = pd.cut(data[\"Fare\"], bins, labels = labels).astype('object')","c0c5fa95":"#data['FareBand'] = pd.qcut(data['Fare'], 4)\n#data[['FareBand', 'Survived']].groupby(['FareBand'], as_index=False).mean().sort_values(by='FareBand', ascending=True)","37c620a9":"data.columns","dae0f483":"# To tincker around a bit\ndf = data","d21d8287":"# Drop high cardinality\ndf = df.drop(['Ticket', 'Name', 'Fare'], axis=1)","7c1c8468":"from catboost import Pool, CatBoostClassifier, cv\n\n#Split data\ntrain = df[:split]\ntest = df[split:]\n\n# Get variables for a model\nx = train.drop([\"Survived\"], axis=1)\ny = train[\"Survived\"]\n\n#Do train data splitting\nX_train, X_test, y_train, y_test = train_test_split(x,y, test_size=0.2, random_state=42)\n\n#We will predict this value for a submission\ntest.drop([\"Survived\"], axis = 1, inplace=True)\n\ncat_features = np.where(x.dtypes != float)[0]\n\ncat = CatBoostClassifier(one_hot_max_size=7, iterations=21, random_seed=42, use_best_model=True, eval_metric='Accuracy', loss_function='Logloss')\n\ncat.fit(X_train, y_train, cat_features = cat_features, eval_set=(X_test, y_test))\npred = cat.predict(X_test)\n\npool = Pool(X_train, y_train, cat_features=cat_features)\ncv_scores = cv(pool, cat.get_params(), fold_count=10, plot=True)\nprint('CV score: {:.5f}'.format(cv_scores['test-Accuracy-mean'].values[-1]))\nprint('The test accuracy is :{:.6f}'.format(accuracy_score(y_test, cat.predict(X_test))))","6e33ed77":"def correlation_heatmap(df, method):\n    _ , ax = plt.subplots(figsize =(14, 12))\n    colormap = sns.diverging_palette(220, 10, as_cmap = True)\n    \n    _ = sns.heatmap(\n        df.corr(method=method),\n        cmap = colormap,\n        square=True, \n        annot=True, \n        annot_kws={'fontsize':9 }\n    )\n    \n    plt.title('Correlation Matrix', y=1.05, size=15)","cccfb8aa":"correlation_heatmap(data, 'pearson')","a5067f4d":"# Drop low corrlations \nto_drop = ['Age', 'AgeBin', 'SibSp', 'Parch', 'FamilySize', 'Embarked', 'Title']\ndf = df.drop(to_drop, axis=1, inplace=False)","363e56e9":"#Check\ndf.info()","9d888415":"#Check\ndata.columns","501fd969":"# Categorical boolean mask\ncategorical_feature_mask = df.dtypes==object\n# filter categorical columns using mask and turn it into a list\ncategorical_cols = df.columns[categorical_feature_mask].tolist()\n\n# import labelencoder\nfrom sklearn.preprocessing import LabelEncoder\n# instantiate labelencoder object\nle = LabelEncoder()\n\n# apply le on categorical feature columns\ndf[categorical_cols] = df[categorical_cols].apply(lambda col: le.fit_transform(col))\ndf[categorical_cols].head()","1fbc2cf4":"data.info()","2dfcf11e":"#Split data\ntrain = df[:split]\ntest = df[split:]\n\n# Get variables for a model\nx = train.drop([\"Survived\"], axis=1)\ny = train[\"Survived\"]\n\n#Do train data splitting\nX_train, X_test, y_train, y_test = train_test_split(x,y, test_size=0.2, random_state=42)\n\n#We will predict this value for a submission\ntest.drop([\"Survived\"], axis = 1, inplace=True)","8a5f12e6":"MLA = [\n    ensemble.AdaBoostClassifier(),\n    ensemble.ExtraTreesClassifier(),\n    ensemble.GradientBoostingClassifier(),\n    ensemble.RandomForestClassifier(),\n    gaussian_process.GaussianProcessClassifier(),\n    linear_model.LogisticRegressionCV(),\n    linear_model.RidgeClassifierCV(),\n    linear_model.Perceptron(),\n    naive_bayes.BernoulliNB(),\n    naive_bayes.GaussianNB(),\n    neighbors.KNeighborsClassifier(),\n    svm.SVC(probability=True),\n    svm.NuSVC(probability=True),\n    svm.LinearSVC(),\n    tree.DecisionTreeClassifier(),\n    tree.ExtraTreeClassifier(),\n    xgb.XGBClassifier()\n    ]","cdca130a":"#Do some preperation for the loop\ncol = []\nalgorithms = pd.DataFrame(columns = col)\nidx = 0\n\n#Train and score algorithms\nfor a in MLA:\n    \n    a.fit(X_train, y_train)\n    pred = a.predict(X_test)\n    acc = accuracy_score(y_test, pred) #Other way: a.score(X_test, y_test)\n    f1 = f1_score(y_test, pred)\n    cv = cross_val_score(a, X_test, y_test).mean()\n    \n    Alg = a.__class__.__name__\n    \n    algorithms.loc[idx, 'Algorithm'] = Alg\n    algorithms.loc[idx, 'Accuracy'] = round(acc * 100, 2)\n    algorithms.loc[idx, 'F1 Score'] = round(f1 * 100, 2)\n    algorithms.loc[idx, 'CV Score'] = round(cv * 100, 2)\n\n    idx+=1","e17d0f89":"#Compare invidual models\nalgorithms.sort_values(by = ['CV Score'], ascending = False, inplace = True)    \nalgorithms.head()","0cd49f79":"#Plot them\ng = sns.barplot(\"CV Score\", \"Algorithm\", data = algorithms)\ng.set_xlabel(\"CV score\")\ng = g.set_title(\"Algorithm Scores\")","9766e985":"kfold = StratifiedKFold(n_splits=10) #-> library from sklearn.model_selection import StratifiedKFold","accdc001":"# XGBoost Classifier\nXGB = XGBClassifier()\nxgb_param = {\n    'loss' : [\"deviance\"],\n     'n_estimators' : [100,200,300],\n     'learning_rate': [0.1, 0.05, 0.01],\n     'max_depth': [4, 8],\n     'min_samples_leaf': [100,150],\n     'max_features': [0.3, 0.1] \n    }\n\ngsXGB = GridSearchCV(XGB, param_grid = xgb_param, cv=kfold, scoring=\"accuracy\", n_jobs= 4, verbose = 1)\ngsXGB.fit(X_train,y_train)\nXGB_best = gsXGB.best_estimator_\n\n# Best score\ngsXGB.best_score_","77944ab0":"# SVC Classifier\nSVC = svm.SVC(probability=True)\nsvc_param = {\n    'kernel': ['rbf'], \n    'gamma': [ 0.001, 0.01, 0.1, 1],\n    'C': [1, 10, 50, 100,200,300, 1000]\n    }\n\ngsSVC = GridSearchCV(SVC, param_grid = svc_param, cv=kfold, scoring=\"accuracy\", n_jobs= 4, verbose = 1)\ngsSVC.fit(X_train,y_train)\nSVC_best = gsSVC.best_estimator_\n\n# Best score\ngsSVC.best_score_","52730697":"# Gradient Boosting Classifier\nGB = ensemble.GradientBoostingClassifier()\ngb_param = {\n        'loss' : [\"deviance\"],\n        'n_estimators' : [100,200,300],\n        'learning_rate': [0.1, 0.05, 0.01],\n        'max_depth': [4, 8],\n        'min_samples_leaf': [100,150],\n        'max_features': [0.3, 0.1] \n        }\n\ngsGB = GridSearchCV(GB, param_grid = gb_param, cv=kfold, scoring=\"accuracy\", n_jobs= 4, verbose = 1)\ngsGB.fit(X_train,y_train)\nGB_best = gsGB.best_estimator_\n\n# Best score\ngsGB.best_score_","85d0e4d8":"vc = ensemble.VotingClassifier(\n    estimators = [('xgb', XGB_best), ('gbc',GB_best), ('svc', SVC_best)],\n    voting='soft', n_jobs=4)","9dcdac09":"vc = vc.fit(X_train, y_train)\npred = vc.predict(X_test)\nacc = accuracy_score(y_test, pred) #Other way: vc.score(X_test, y_test)\nf1 = f1_score(y_test, pred)\ncv = cross_val_score(vc, X_test, y_test).mean()\n\nprint(\"Accuracy: \", round(acc*100,2), \"\\nF1-Score: \", round(f1*100,2), \"\\nCV Score: \", round(cv*100,2))","6555b6ab":"ada = ensemble.AdaBoostClassifier()\nada.fit(X_train, y_train)\n\nlg = linear_model.LogisticRegressionCV()\nlg.fit(X_train, y_train)\n\nvc2 = ensemble.VotingClassifier(\n    estimators = [('ada', ada), ('lg',lg), ('VotingClassifier', vc)],\n    voting='soft', n_jobs=4)\nvc2.fit(X_train, y_train)","e56ba83a":"y_scores = vc2.predict_proba(X_test)\ny_scores = y_scores[:,1]","a9ce852e":"false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_scores)","c0da87c9":"def plot_roc_curve(false_positive_rate, true_positive_rate, label=None):\n    plt.plot(false_positive_rate, true_positive_rate, linewidth=2, label=label)\n    plt.plot([0, 1], [0, 1], 'r', linewidth=4)\n    plt.axis([0, 1, 0, 1])\n    plt.xlabel('False Positive Rate (FPR)', fontsize=16)\n    plt.ylabel('True Positive Rate (TPR)', fontsize=16)","8c634116":"plt.figure(figsize=(14, 7))\nplot_roc_curve(false_positive_rate, true_positive_rate)","9518483e":"auroc = roc_auc_score(y_test, y_scores)\nprint(\"ROC-AUC Score:\", auroc)","e2f705bf":"pred = vc2.predict(test).astype(int)\ntarget = pd.Series(pred, name='Survived')\n\noutput = pd.concat({'PassengerId':Id, 'Survived':target}\n                   ,axis='columns')\n\noutput.to_csv('submission.csv', index=False, header=True)","c8f14329":"#### Family","19a1d19e":"### Check for imbalance class problem","c1b538df":"#### Age","b8904f26":"## EDA","899c94ce":"## DATA WRANGLING","33884fd1":"### Train&Score ","fe26bdb7":"##### Deck","bb4460e6":"#### Others","039ee18e":"# Feature encoding","53f4f725":"## LOAD DATA","6b88927f":"## LOAD LIBRARIES","0e88692c":"## Baseline model","2bcbb077":"### Evaluation","5a068cfd":"### Outlier removal","ae664c92":"##### Family Size","31a527b5":"## Correlation","968bb6da":"## Model ","771fe861":"#                                                                            TITANIC: Machine Learning from Disaster","7668eb96":"##### Title","1989af98":"#### Fare ","39e8f2fc":"##### Is Alone","c7823b64":"### Model Assembly ","2699b0f1":"![](http:\/\/blog.ecocentro.es\/wp-content\/uploads\/2015\/07\/5b8be34e82bee5b84719daeec62cd868_large.jpeg)","9a54a724":"#### Cabin","3b6232bf":"### Check versions","44c4a25d":"#### Embarked","a30ff210":"I watched these two short videos to get some \"business understanding\" background:\n*     Sinking of the Titanic: https:\/\/youtu.be\/b0L_2jKEbA4\n*     50 Insane Facts: https: https:\/\/youtu.be\/Rqbsrj6-FgM","bab848d3":"### Parameter Tunning","3f2d0cbe":"### AUROC ","2f31697c":"##### Is Baby","f372faef":"## Submission","d9edbf48":"### Handle Nulls ","65fd1ace":"### **If you like the notebook, please up-vote!**","10d0ff52":"#### Score ","d8acbf52":"### Concat data ","ecc0fc95":"#### Age ","980c416a":"#### Curve ","4fab2163":"**Welcome!** \n\nI've done here all the basic steps you might need to solve a ML problem, such as EDA, feature engineering, encoding, simple model baseline, model assembly, parameter tunning and model evaluation. I hope this will help you with your further classification projects of this kind. If you like this, please *up-vote* and of course, I'll be glad if you drop me a comment in the section below."}}