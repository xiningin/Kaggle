{"cell_type":{"c2fd90e5":"code","52e066ff":"code","7f08f847":"code","e1560025":"code","65ace09a":"code","e33c3af3":"code","906e716b":"code","f2f8bf8d":"code","c56199a3":"code","de83cf9c":"code","3e18777f":"code","e151634c":"code","5208d782":"code","fbf6b7e5":"code","a22dedf7":"code","22d097a6":"code","fdf07e7f":"code","4443da9c":"code","db019813":"code","af595183":"code","09114c06":"code","d92b5700":"code","47c54e3f":"code","24b93a5c":"code","97da10d6":"code","4540d5bf":"code","4a055d33":"code","7c7cee20":"code","28cdc071":"code","4401c9e4":"code","32779724":"code","882e8b22":"code","d6f1b08c":"code","256ff8c5":"code","6943bcab":"code","9ec1e35a":"code","b7425a3a":"code","42fb5600":"code","76bf5bc1":"code","6d4b2c2b":"code","2368338b":"code","fcce2edf":"code","bad0deec":"code","b1d23c65":"code","6d55b4b4":"code","0e2b335f":"code","8a9f2d18":"code","56ebf1ce":"code","5ad6146a":"code","cac5801c":"code","8c47390a":"code","6d796dc6":"markdown","0aadbc14":"markdown","165c5352":"markdown","fa989c32":"markdown","6043bc84":"markdown","1578bbd2":"markdown","eddb74f1":"markdown","7cad65c6":"markdown","3043440b":"markdown","f2fd40f6":"markdown","8c80d617":"markdown","9f2eb5b1":"markdown","6403f688":"markdown","46526bf7":"markdown","2ebaf7a8":"markdown","f33d93df":"markdown","ae44fd80":"markdown","f4f830e5":"markdown","47fdc3e7":"markdown","3541f877":"markdown"},"source":{"c2fd90e5":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport squarify\n# sklearn preprocessing for dealing with categorical variables\nfrom sklearn.preprocessing import LabelEncoder\n\n# File system manangement\nimport os\n\n# Suppress warnings \nimport warnings\nwarnings.filterwarnings('ignore')\n\n# matplotlib and seaborn for plotting\nimport seaborn as sns\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\napp_train = pd.read_csv('\/kaggle\/input\/home-credit-default-risk\/application_train.csv')   \napp_test = pd.read_csv('\/kaggle\/input\/home-credit-default-risk\/application_test.csv')\ninstallment_pay = pd.read_csv('\/kaggle\/input\/home-credit-default-risk\/installments_payments.csv')\nbureau_balance = pd.read_csv('\/kaggle\/input\/home-credit-default-risk\/bureau_balance.csv')\ncredit_card_balance = pd.read_csv('\/kaggle\/input\/home-credit-default-risk\/credit_card_balance.csv')\nprevious_application = pd.read_csv('\/kaggle\/input\/home-credit-default-risk\/previous_application.csv')\nbureau = pd.read_csv('\/kaggle\/input\/home-credit-default-risk\/bureau.csv')\napp_train.shape\n\napp_train.head()\n# Any results you write to the current directory are saved as output.","52e066ff":"app_train.describe()\n","7f08f847":"app_train.shape","e1560025":"app_train['TARGET'].value_counts()\n(app_train.TARGET == 1).sum()\n#(app_train.TARGET == 0).sum()","65ace09a":"#app_train['TARGET'].plot.hist()\nsquarify.plot(sizes=[(app_train.TARGET == 1).sum(),(app_train.TARGET == 0).sum()], label=[\"group 1\", \"group 0\"], color=[\"blue\",\"green\"], alpha=.4 )\nplt.axis('off')\nplt.show()\n","e33c3af3":"app_train['TARGET'].plot.hist()","906e716b":"#Concatenating train and test data\napplication_train_x = app_train[[x for x in app_train.columns if x not in [\"TARGET\"]]]\napplication_train_x[\"type\"] = \"train\"\napp_test[\"type\"]    = \"test\"\ndata = pd.concat([application_train_x,app_test],axis=0)\n\nplt.figure(figsize=(14,7))\nplt.subplot(121)\ndata[data[\"type\"] == \"train\"][\"NAME_CONTRACT_TYPE\"].value_counts().plot.pie(autopct = \"%1.0f%%\",colors = [\"grey\",\"orange\"],startangle = 60,\n                                                                        wedgeprops={\"linewidth\":2,\"edgecolor\":\"white\"},shadow =True)\ncirc = plt.Circle((0,0),.7,color=\"white\")\nplt.gca().add_artist(circ)\nplt.title(\"distribution of contract types in train data\")\n\nplt.subplot(122)\ndata[data[\"type\"] == \"test\"][\"NAME_CONTRACT_TYPE\"].value_counts().plot.pie(autopct = \"%1.0f%%\",colors = [\"grey\",\"orange\"],startangle = 60,\n                                                                        wedgeprops={\"linewidth\":2,\"edgecolor\":\"white\"},shadow =True)\ncirc = plt.Circle((0,0),.7,color=\"white\")\nplt.gca().add_artist(circ)\nplt.ylabel(\"\")\nplt.title(\"distribution of contract types in test data\")\nplt.show()","f2f8bf8d":"def missing_values_table(df):\n        # Total missing values\n        mis_val = df.isnull().sum()\n        \n        # Percentage of missing values\n        mis_val_percent = 100 * df.isnull().sum() \/ len(df)\n        \n        # Make a table with the results\n        mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)\n        \n        # Rename the columns\n        mis_val_table_ren_columns = mis_val_table.rename(\n        columns = {0 : 'Missing Values', 1 : '% of Total Values'})\n        \n        # Sort the table by percentage of missing descending\n        mis_val_table_ren_columns = mis_val_table_ren_columns[\n            mis_val_table_ren_columns.iloc[:,1] != 0].sort_values(\n        '% of Total Values', ascending=False).round(1)\n        \n        # Print some summary information\n        print (\"Your selected dataframe has \" + str(df.shape[1]) + \" columns.\\n\"      \n            \"There are \" + str(mis_val_table_ren_columns.shape[0]) +\n              \" columns that have missing values.\")\n        \n        # Return the dataframe with missing information\n        return mis_val_table_ren_columns","c56199a3":"# Missing values statistics\nmissing_values = missing_values_table(app_train)\nmissing_values.head(50)","de83cf9c":"fig = plt.figure(figsize=(18,6))\nmiss_train = pd.DataFrame((app_train.isnull().sum())*100\/app_train.shape[0]).reset_index()\nmiss_test = pd.DataFrame((app_test.isnull().sum())*100\/app_test.shape[0]).reset_index()\nmiss_train[\"type\"] = \"train\"\nmiss_test[\"type\"]  =  \"test\"\nmissing = pd.concat([miss_train,miss_test],axis=0)\nax = sns.pointplot(\"index\",0,data=missing,hue=\"type\")\nplt.xticks(rotation =90,fontsize =7)\nplt.title(\"Percentage of Missing values in application train and test data\")\nplt.ylabel(\"PERCENTAGE\")\nplt.xlabel(\"COLUMNS\")\nax.set_facecolor(\"k\")\nfig.set_facecolor(\"lightgrey\")","3e18777f":"missing_installment_pay = missing_values_table(installment_pay)\nmissing_installment_pay.head(10)","e151634c":"missing_bureau_balance = missing_values_table(bureau_balance)\nmissing_bureau_balance.head(10)\n\n\n","5208d782":"missing_credit_card_balance = missing_values_table(credit_card_balance)\nmissing_credit_card_balance.head(10)\n","fbf6b7e5":"\nmissing_previous_application = missing_values_table(previous_application)\nmissing_previous_application.head(10)\n\n","a22dedf7":"missing_bureau = missing_values_table(bureau)\nmissing_bureau.head(10)","22d097a6":"plt.figure(figsize=(15,20))\n\nplt.subplot(231)\nsns.heatmap(pd.DataFrame(bureau.isnull().sum()\/bureau.shape[0]*100),annot=True,\n            cmap=sns.color_palette(\"cool\"),linewidth=1,linecolor=\"white\")\nplt.title(\"bureau\")\n\nplt.subplot(232)\nsns.heatmap(pd.DataFrame(bureau_balance.isnull().sum()\/bureau_balance.shape[0]*100),annot=True,\n            cmap=sns.color_palette(\"cool\"),linewidth=1,linecolor=\"white\")\nplt.title(\"bureau_balance\")\n\nplt.subplot(233)\nsns.heatmap(pd.DataFrame(credit_card_balance.isnull().sum()\/credit_card_balance.shape[0]*100),annot=True,\n            cmap=sns.color_palette(\"cool\"),linewidth=1,linecolor=\"white\")\nplt.title(\"credit_card_balance\")\n\nplt.subplot(234)\nsns.heatmap(pd.DataFrame(installment_pay.isnull().sum()\/installment_pay.shape[0]*100),annot=True,\n            cmap=sns.color_palette(\"cool\"),linewidth=1,linecolor=\"white\")\nplt.title(\"installments_payments\")\n\nplt.subplot(236)\nsns.heatmap(pd.DataFrame(previous_application.isnull().sum()\/previous_application.shape[0]*100),annot=True,\n            cmap=sns.color_palette(\"cool\"),linewidth=1,linecolor=\"white\")\nplt.title(\"previous_application\")\n\nplt.subplots_adjust(wspace = 1.6)","fdf07e7f":"fig  = plt.figure(figsize=(13,6))\nplt.subplot(121)\nax = sns.countplot(\"NAME_CONTRACT_TYPE\",hue=\"CODE_GENDER\",data=data[data[\"type\"] == \"train\"],palette=[\"r\",\"b\",\"g\"])\nax.set_facecolor(\"lightgrey\")\nax.set_title(\"Distribution of Contract type by gender -train data\")\n\nplt.subplot(122)\nax1 = sns.countplot(\"NAME_CONTRACT_TYPE\",hue=\"CODE_GENDER\",data=data[data[\"type\"] == \"test\"],palette=[\"b\",\"r\"])\nax1.set_facecolor(\"lightgrey\")\nax1.set_title(\"Distribution of Contract type by gender -test data\")\nplt.show()","4443da9c":"plt.figure(figsize=(13,6))\nplt.subplot(121)\ndata[\"FLAG_OWN_REALTY\"].value_counts().plot.pie(autopct = \"%1.0f%%\",colors = [\"skyblue\",\"gold\"],startangle = 90,\n                                              wedgeprops={\"linewidth\":2,\"edgecolor\":\"k\"},explode=[0.05,0],shadow =True)\nplt.title(\"Distribution of client owns a house or flat\")\n\nplt.subplot(122)\ndata[data[\"FLAG_OWN_REALTY\"] == \"Y\"][\"CODE_GENDER\"].value_counts().plot.pie(autopct = \"%1.0f%%\",colors = [\"orangered\",\"b\"],startangle = 90,\n                                                                        wedgeprops={\"linewidth\":2,\"edgecolor\":\"k\"},explode=[.05,0,0],shadow =True)\nplt.title(\"Distribution of client owning a house or flat by gender\")\nplt.show()","db019813":"def group_by(df,t1='',t2=''):\n    a1=df.groupby([t1,t2])[t2].count()\n    return a1","af595183":"def plot_re(df,t1='',t2=''):\n    f,ax=plt.subplots(1,2,figsize=(10,6))\n    df[[t1,t2]].groupby([t1]).count().plot.bar(ax=ax[0],color='Red')\n    ax[0].set_title('count of customer Based on'+t1)\n    sns.countplot(t1,hue=t2,data=df,ax=ax[1],palette=\"spring\")\n    ax[1].set_title(t1+': Repayer vs Defualter')\n    # Rotate x-labels\n    plt.xticks(rotation=-90)\n    a=plt.show()\n    return a","09114c06":"plot_re(app_train,'NAME_EDUCATION_TYPE','TARGET')","d92b5700":"plot_re(app_train,'OCCUPATION_TYPE','TARGET')","47c54e3f":"plot_re(app_train,'NAME_FAMILY_STATUS','TARGET')","24b93a5c":"plot_re(app_train,'NAME_HOUSING_TYPE','TARGET')","97da10d6":"f, ax = plt.subplots(figsize=(15, 10))\nsns.countplot(y=\"ORGANIZATION_TYPE\", hue='TARGET', \n              data=app_train).set_title('REpayer VS Defaulter based on Organization type')","4540d5bf":"plot_re(app_train,'HOUSETYPE_MODE','TARGET')","4a055d33":"#correlation heatmap of dataset\ndef correlation_heatmap(df):\n    _ , ax = plt.subplots(figsize =(20,15))\n    colormap = sns.diverging_palette(220, 10, as_cmap = True)\n    \n    _ = sns.heatmap(\n        df.corr(), \n        cmap = colormap,\n        square=True, \n        #cbar_kws={'shrink':.9 }, \n        #ax=ax,\n        #annot=True, \n        #linewidths=0.1,vmax=1.0, linecolor='white',\n        #annot_kws={'fontsize':16}\n    )\n    \n    plt.title('Pearson Correlation of Features')\n\ncorrelation_heatmap(app_train)","7c7cee20":"# most correlated features\ncorrmat = app_train.corr()\ntop_corr_features = corrmat.index[abs(corrmat[\"TARGET\"])>=0.03]\nplt.figure(figsize=(20,10))\ng = sns.heatmap(app_train[top_corr_features].corr(),annot=True,cmap=\"Oranges\")","28cdc071":"amt = app_train[[ 'AMT_INCOME_TOTAL','AMT_CREDIT',\n                         'AMT_ANNUITY', 'AMT_GOODS_PRICE',\"TARGET\"]]\namt = amt[(amt[\"AMT_GOODS_PRICE\"].notnull()) & (amt[\"AMT_ANNUITY\"].notnull())]\nsns.pairplot(amt,hue=\"TARGET\",palette=[\"b\",\"r\"])\nplt.show()","4401c9e4":"plt.figure(figsize=(16,8))\nplt.subplot(121)\napp_train[app_train[\"TARGET\"]==0][\"NAME_EDUCATION_TYPE\"].value_counts().plot.pie(fontsize=9,autopct = \"%1.0f%%\",\n                                                                                                 colors = sns.color_palette(\"Set1\"),\n                                              wedgeprops={\"linewidth\":2,\"edgecolor\":\"white\"},shadow =True)\ncirc = plt.Circle((0,0),.7,color=\"white\")\nplt.gca().add_artist(circ)\nplt.title(\"Distribution of Education type for Repayers\",color=\"b\")\n\nplt.subplot(122)\napp_train[app_train[\"TARGET\"]==1][\"NAME_EDUCATION_TYPE\"].value_counts().plot.pie(fontsize=9,autopct = \"%1.0f%%\",\n                                                                                                 colors = sns.color_palette(\"Set1\"),\n                                              wedgeprops={\"linewidth\":2,\"edgecolor\":\"white\"},shadow =True)\ncirc = plt.Circle((0,0),.7,color=\"white\")\nplt.gca().add_artist(circ)\nplt.title(\"Distribution of Education type for Defaulters\",color=\"b\")\nplt.ylabel(\"\")\nplt.show()","32779724":"edu = data.groupby(['NAME_EDUCATION_TYPE','NAME_INCOME_TYPE'])['AMT_INCOME_TOTAL'].mean().reset_index().sort_values(by='AMT_INCOME_TOTAL',ascending=False)\nfig = plt.figure(figsize=(13,7))\nax = sns.barplot('NAME_INCOME_TYPE','AMT_INCOME_TOTAL',data=edu,hue='NAME_EDUCATION_TYPE',palette=\"seismic\")\nax.set_facecolor(\"k\")\nplt.title(\" Average Earnings by different professions and education types\")\nplt.show()","882e8b22":"day = app_train.groupby(\"TARGET\").agg({\"WEEKDAY_APPR_PROCESS_START\":\"value_counts\"})\nday = day.rename(columns={\"WEEKDAY_APPR_PROCESS_START\":\"value_counts\"})\nday = day.reset_index()\nday_0 = day[:7]\nday_1 = day[7:]\nday_0[\"percentage\"] = day_0[\"value_counts\"]*100\/day_0[\"value_counts\"].sum()\nday_1[\"percentage\"] = day_1[\"value_counts\"]*100\/day_1[\"value_counts\"].sum()\ndays = pd.concat([day_0,day_1],axis=0)\ndays[\"TARGET\"] = days.replace({1:\"defaulters\",0:\"repayers\"})\n\nfig = plt.figure(figsize=(13,15))\nplt.subplot(211)\norder = ['SUNDAY', 'MONDAY','TUESDAY', 'WEDNESDAY','THURSDAY', 'FRIDAY', 'SATURDAY']\nax= sns.barplot(\"WEEKDAY_APPR_PROCESS_START\",\"percentage\",data=days,\n                hue=\"TARGET\",order=order,palette=\"prism\")\nax.set_facecolor(\"k\")\nax.set_title(\"Peak days for applying loans (defaulters vs repayers)\")\n\nhr = app_train.groupby(\"TARGET\").agg({\"HOUR_APPR_PROCESS_START\":\"value_counts\"})\nhr = hr.rename(columns={\"HOUR_APPR_PROCESS_START\":\"value_counts\"}).reset_index()\nhr_0 = hr[hr[\"TARGET\"]==0]\nhr_1 = hr[hr[\"TARGET\"]==1]\nhr_0[\"percentage\"] = hr_0[\"value_counts\"]*100\/hr_0[\"value_counts\"].sum()\nhr_1[\"percentage\"] = hr_1[\"value_counts\"]*100\/hr_1[\"value_counts\"].sum()\nhrs = pd.concat([hr_0,hr_1],axis=0)\nhrs[\"TARGET\"] = hrs[\"TARGET\"].replace({1:\"defaulters\",0:\"repayers\"})\nhrs = hrs.sort_values(by=\"HOUR_APPR_PROCESS_START\",ascending=True)\n\nplt.subplot(212)\nax1 = sns.pointplot(\"HOUR_APPR_PROCESS_START\",\"percentage\",\n                    data=hrs,hue=\"TARGET\",palette=\"prism\")\nax1.set_facecolor(\"k\")\nax1.set_title(\"Peak hours for applying loans (defaulters vs repayers)\")\nfig.set_facecolor(\"snow\")","d6f1b08c":"app_train.dtypes.value_counts()","256ff8c5":"missing_installment_pay.dtypes.value_counts()","6943bcab":"app_train.select_dtypes('object').apply(pd.Series.nunique, axis = 0)","9ec1e35a":"app_train[\"type\"] = \"train\"\napp_test[\"type\"]  = \"test\"\n#conactenating train & test data\ndata = pd.concat([app_train,app_test],axis=0)","b7425a3a":"#Removing columns with missing values more than 40%\nmissing_cols = [ 'EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3',\n       'APARTMENTS_AVG', 'BASEMENTAREA_AVG', 'YEARS_BEGINEXPLUATATION_AVG',\n       'YEARS_BUILD_AVG', 'COMMONAREA_AVG', 'ELEVATORS_AVG', 'ENTRANCES_AVG',\n       'FLOORSMAX_AVG', 'FLOORSMIN_AVG', 'LANDAREA_AVG',\n       'LIVINGAPARTMENTS_AVG', 'LIVINGAREA_AVG', 'NONLIVINGAPARTMENTS_AVG',\n       'NONLIVINGAREA_AVG', 'APARTMENTS_MODE', 'BASEMENTAREA_MODE',\n       'YEARS_BEGINEXPLUATATION_MODE', 'YEARS_BUILD_MODE', 'COMMONAREA_MODE',\n       'ELEVATORS_MODE', 'ENTRANCES_MODE', 'FLOORSMAX_MODE', 'FLOORSMIN_MODE',\n       'LANDAREA_MODE', 'LIVINGAPARTMENTS_MODE', 'LIVINGAREA_MODE',\n       'NONLIVINGAPARTMENTS_MODE', 'NONLIVINGAREA_MODE', 'APARTMENTS_MEDI',\n       'BASEMENTAREA_MEDI', 'YEARS_BEGINEXPLUATATION_MEDI', 'YEARS_BUILD_MEDI',\n       'COMMONAREA_MEDI', 'ELEVATORS_MEDI', 'ENTRANCES_MEDI', 'FLOORSMAX_MEDI',\n       'FLOORSMIN_MEDI', 'LANDAREA_MEDI', 'LIVINGAPARTMENTS_MEDI',\n       'LIVINGAREA_MEDI', 'NONLIVINGAPARTMENTS_MEDI', 'NONLIVINGAREA_MEDI',\n       'FONDKAPREMONT_MODE', 'HOUSETYPE_MODE', 'TOTALAREA_MODE',\n       'WALLSMATERIAL_MODE', 'EMERGENCYSTATE_MODE',\"OWN_CAR_AGE\",\"OCCUPATION_TYPE\"]\n\ndata_new  = data[[i for i in data.columns if i not in missing_cols]]","42fb5600":"#Separating numberical and categorical columns\nobj_dtypes = [i for i in data_new.select_dtypes(include=np.object).columns if i not in [\"type\"] ]\nnum_dtypes = [i for i in data_new.select_dtypes(include = np.number).columns if i not in ['SK_ID_CURR'] + [ 'TARGET']]","76bf5bc1":"#MISSING values treatment\namt_cs = [\"AMT_ANNUITY\",\"AMT_GOODS_PRICE\"]\nfor i in amt_cs:\n    data_new[i] = data_new.groupby(\"type\").transform(lambda x:x.fillna(x.mean()))\n    \nenq_cs =['AMT_REQ_CREDIT_BUREAU_DAY', 'AMT_REQ_CREDIT_BUREAU_HOUR',\n       'AMT_REQ_CREDIT_BUREAU_MON', 'AMT_REQ_CREDIT_BUREAU_QRT',\n       'AMT_REQ_CREDIT_BUREAU_WEEK', 'AMT_REQ_CREDIT_BUREAU_YEAR']\nfor i in enq_cs:\n    data_new[i] = data_new[i].fillna(0)\n    \ncols = [\"DEF_30_CNT_SOCIAL_CIRCLE\",\"DEF_60_CNT_SOCIAL_CIRCLE\",\"OBS_30_CNT_SOCIAL_CIRCLE\",\n        \"OBS_60_CNT_SOCIAL_CIRCLE\",\"NAME_TYPE_SUITE\",\"CNT_FAM_MEMBERS\",\n       \"DAYS_LAST_PHONE_CHANGE\",\"DAYS_LAST_PHONE_CHANGE\"]\nfor i in cols :\n    data_new[i]  = data_new[i].fillna(data_new[i].mode()[0])","6d4b2c2b":"#Label encoding\nfrom sklearn.preprocessing import LabelEncoder\n\nle = LabelEncoder()\n\nfor i in obj_dtypes:\n    data_new[i] = le.fit_transform(data_new[i])","2368338b":"#one hot encoding for categorical variables\ndata_new = pd.get_dummies(data=data_new,columns=obj_dtypes)","fcce2edf":"#splitting new train and test data\napplication_train_newdf = data_new[data_new[\"type\"] == \"train\"]\napplication_test_newdf  = data_new[data_new[\"type\"] == \"test\"]","bad0deec":"#splitting application_train_newdf into train and test\nfrom sklearn.model_selection import train_test_split\ntrain,test = train_test_split(application_train_newdf,test_size=.3,random_state = 123)\n\ntrain = train.drop(columns=\"type\",axis=1)\ntest  = test.drop(columns=\"type\",axis=1)\n\n#seperating dependent and independent variables\ntrain_X = train[[i for i in train.columns if i not in ['SK_ID_CURR'] + [ 'TARGET']]]\ntrain_Y = train[[\"TARGET\"]]\n\ntest_X  = test[[i for i in test.columns if i not in ['SK_ID_CURR'] + [ 'TARGET']]]\ntest_Y  = test[[\"TARGET\"]]","b1d23c65":"# Up-sample Minority Class\nfrom sklearn.utils import resample\n\n#separating majority and minority classes\ndf_majority = train[train[\"TARGET\"] == 0]\ndf_minority = train[train[\"TARGET\"] == 1]\n\n#upsample minority data\ndf_minority_upsampled = resample(df_minority,\n                                 replace=True,\n                                 n_samples =197969,\n                                 random_state=123)\n\ndf_upsampled = pd.concat([df_majority,df_minority_upsampled],axis=0)\n\n#splitting dependent and independent variables\ndf_upsampled_X = df_upsampled[[i for i in df_upsampled.columns if i not in ['SK_ID_CURR'] + [ 'TARGET']]]\ndf_upsampled_Y = df_upsampled[[\"TARGET\"]]","6d55b4b4":"# Down-sample Majority Class\nfrom sklearn.utils import resample\n\n#separating majority and minority classes\ndf_majority = train[train[\"TARGET\"] == 0]\ndf_minority = train[train[\"TARGET\"] == 1]\n\ndf_majority_downsampled = resample(df_majority,\n                                   replace=False,\n                                   n_samples=17288,\n                                   random_state=123)\n\ndf_downsampled = pd.concat([df_minority,df_majority_downsampled],axis=0)\n\n#splitting dependent and independent variables\n\ndf_downsampled_X = df_downsampled[[i for i in df_downsampled.columns if i not in ['SK_ID_CURR'] + [ 'TARGET']]]\ndf_downsampled_Y = df_downsampled[[\"TARGET\"]]","0e2b335f":"from sklearn.metrics import confusion_matrix,accuracy_score,recall_score,roc_auc_score,classification_report,roc_auc_score,roc_curve,auc\n\n#Model function\ndef model(algorithm,dtrain_X,dtrain_Y,dtest_X,dtest_Y,cols=None):\n\n    algorithm.fit(dtrain_X[cols],dtrain_Y)\n    predictions = algorithm.predict(dtest_X[cols])\n    print (algorithm)\n    \n    print (\"Accuracy score : \", accuracy_score(predictions,dtest_Y))\n    print (\"Recall score   : \", recall_score(predictions,dtest_Y))\n    print (\"classification report :\\n\",classification_report(predictions,dtest_Y))\n    \n    fig = plt.figure(figsize=(10,8))\n    ax  = fig.add_subplot(111)\n    prediction_probabilities = algorithm.predict_proba(dtest_X[cols])[:,1]\n    fpr , tpr , thresholds   = roc_curve(dtest_Y,prediction_probabilities)\n    ax.plot(fpr,tpr,label   = [\"Area under curve : \",auc(fpr,tpr)],linewidth=2,linestyle=\"dotted\")\n    ax.plot([0,1],[0,1],linewidth=2,linestyle=\"dashed\")\n    plt.legend(loc=\"best\")\n    plt.title(\"ROC-CURVE & AREA UNDER CURVE\")\n    ax.set_facecolor(\"k\")","8a9f2d18":"from sklearn.linear_model import LogisticRegression\nlogit = LogisticRegression()\nmodel(logit,df_downsampled_X,df_downsampled_Y,test_X,test_Y,df_downsampled_X.columns)","56ebf1ce":"from sklearn.ensemble import RandomForestClassifier\nrfc = RandomForestClassifier()\nmodel(rfc,df_downsampled_X,df_downsampled_Y,test_X,test_Y,df_downsampled_X.columns)","5ad6146a":"from sklearn.naive_bayes import GaussianNB\ngnb = GaussianNB()\nmodel(gnb,df_downsampled_X,df_downsampled_Y,test_X,test_Y,df_downsampled_X.columns)","cac5801c":"from xgboost import XGBClassifier\nxgb = XGBClassifier()\nmodel(xgb,df_downsampled_X,df_downsampled_Y,test_X,test_Y,df_downsampled_X.columns)","8c47390a":"from sklearn.ensemble import GradientBoostingClassifier\ngbc = GradientBoostingClassifier()\nmodel(gbc,df_downsampled_X,df_downsampled_Y,test_X,test_Y,df_downsampled_X.columns)","6d796dc6":"For now let us only focus on **Application_train** dataset and do exploratory analysis on this set\n\nLater we will see once we have exploited this ds what all other tables we can use.\n\nFor now we will focus on : gathering missing values in this set and taking care of categorical variable for this model","0aadbc14":"**Distribution of Contract type by gender**","165c5352":"We can see the data is highly imbalanced ","fa989c32":"**NAME_CONTRACT_TYPE** : Identification if loan is cash or revolving In training data the percentage of revolving loans and cash loans are 10% & 90%.\nIn test data the percentage of revolving loans and cash loans are 1% & 99%.","6043bc84":"**Distribution of client owning a house or flat and by gender**\nFLAG_OWN_REALTY - Flag if client owns a house or flat\nSUBPLOT 1 : Distribution of client owning a house or flat . 69% of clients own a flat or house .\nSUBPLOT 1 : Distribution of client owning a house or flat by gender . Out of total clients who own house 67% are female and 33% are male.","1578bbd2":"**LogisticRegression**","eddb74f1":"We have 16 categorical variables - now if those have just 2 values as unique values then we can use label encoder if we have more then we will use One Hot encoding. Also whatever transformation we do in our training data we have to perform the same traansformation in our test data as well.\n\nOne problem which we will face here is one hot encoding might make our training and test data out of sync with the number of variables which we have so we will have to deal with this as well.","7cad65c6":"**Random Forest Classifier**","3043440b":"**Gradient Boosting Classifier**","f2fd40f6":"its clear that by looking at the above given plot and groupby function is clearly customer based on Organization type and the Business type 3 customer has high count where not able to pay loan back compare to all other","8c80d617":"Average Earnings by different professions and education types","9f2eb5b1":"**Gaussian Naive Bayes**","6403f688":"*Check the distribution of the target variable*","46526bf7":"We see a large spread in some of the columns example amt_income_total, annuity etc","2ebaf7a8":"**XGBoost Classifier**","f33d93df":"**Home Credit Default**\nHere we will try to predict if an indvisual will default on his loan or not. The data might be highly skewed and we have a large number of variables to study and identify which of them might be helpful to us.\n\nWe will try to learn the follwing techniques here :\n\n1. **Descriptive Analysis **\n\n    a. Examine the data \u2013 what a field means\n\n    b. Check for missing values\n    \n\n2. **Exploratory Data Analysis**\n\n     a. Examine the missing values\n\n     b. Encoding Categorical Variables - Label Encoding and One-Hot Encoding\n\n     c. Check for outliers\n\n     d. Correlations \u2013 using various visualization techniques and stats tools to keep in mind which variables might be more important to us\n\n     e. Feature engineering \u2013 1. Polynomial features  2. Domain Knowledge Features  \n\n     f. Manual Feature engineering and Automated Feature Engineering\n\n \n\n3. **Applying and fitting the model**\n\n      a. Finding out the best hyperparameters for our models and understanding how to choose them.\n\n \n\n4. **Validation of model**\n\n    a. Cross-Validation\n\n    b. Checking the accuracy\n\n    c. AUC & ROC\n\n    d. Model Tuning\n\n    e. Check how else to evaluate different models\n\n \n\n5. **Check if Deep Learning can be applied and evaluate its performance**\n\n","ae44fd80":"Peak days and hours for applying loans (defaulters vs repayers)\nWEEKDAY_APPR_PROCESS_START - On which day of the week did the client apply for the loan.\nHOUR_APPR_PROCESS_START - Approximately at what hour did the client apply for the loan.\nOn tuesdays , percentage of defaulters applying for loans is greater than that of repayers.\nfrom morning 4'O clock to 9'O clock percentage of defaulters applying for loans is greater than that of repayers.","f4f830e5":"Now let us see how many columns have missing values and try to create a generic function as we have 122 columns checking one at a time will take a long time ","47fdc3e7":"We will figure out what all columns to remove and which ones to keep after imputation \n\nLet us check how many columns which we have are categorical ","3541f877":"BASED on above given plot it clear that the Credit type consumer credit has high count of non payer of loan and credit currency customer with currency 1 has high count of non payer and based credit active customer with in group of closed customer has high count of non payer"}}