{"cell_type":{"b9b20f27":"code","76814916":"code","4f146737":"code","f2e4222c":"code","cf75fe37":"code","ac4423dd":"code","0447774a":"code","e9fff36c":"code","741f9fdc":"code","bf1927a9":"code","13fc0f7b":"code","42d0919f":"code","119d3da2":"code","548ed157":"code","9b40cde5":"code","2c081e62":"code","b8ba8ef4":"code","b511b754":"code","9c64c340":"code","1ac43488":"code","1dc2e523":"code","6ff34d2c":"code","b35715b8":"code","b793bd82":"code","175ef82c":"code","0830a86f":"code","34d1d280":"code","85bd4f07":"code","de50d0f2":"code","c582f72d":"code","4141cae5":"code","71628be6":"code","0832fa71":"code","b8c57990":"code","764e8c2e":"code","3f9bff4e":"code","197acc1e":"code","7e2b218e":"code","fe3d4ede":"code","d4218798":"code","19d2d4b5":"code","1596d18e":"code","b1a3d7a0":"code","81a47a11":"code","6f1e1c5f":"code","28151c70":"code","e24058f0":"code","6ab1ce29":"code","caa69016":"code","6cf4fb46":"code","ce3f7d65":"code","298448f1":"code","fa8f22be":"code","cac0764f":"code","5361891e":"code","d5a54da1":"code","877f11d6":"code","66139ece":"code","c3d915d1":"code","a3a928e8":"code","46838bcc":"code","25ddfae9":"code","82947a96":"code","08ad01dd":"code","b594f951":"code","d56af07b":"markdown","e66637d0":"markdown","b28acd49":"markdown","58417ec5":"markdown","20875b6d":"markdown","beef98a8":"markdown","f0379d1e":"markdown","7b3de73e":"markdown","db9e2df8":"markdown","8f070ea0":"markdown","ad01b4b6":"markdown","d5bafcb6":"markdown","b995b8da":"markdown","086157db":"markdown","b9188b4c":"markdown","2d1743cc":"markdown","85bf0d38":"markdown","1b04467e":"markdown","2295ebf4":"markdown"},"source":{"b9b20f27":"!pip install tweepy\n!pip install preprocessor\n!pip install flair\n!pip install stanza\n!pip install pycorenlp","76814916":"# General dependencies\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nimport re\nimport os\nimport time\nfrom datetime import datetime, date, timedelta\n\n# For Twitter API extraction\nimport tweepy\n\n# Tweet pre-processor\nimport preprocessor as p\n\n# NLTK\nimport nltk\nnltk.download('vader_lexicon')\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\nfrom textblob import TextBlob\n\n# FLairNLP\nfrom flair.models import TextClassifier\nfrom flair.data import Sentence\n\n# Stanza\nimport stanza\nstanza.download('en')\n\n# Stanford CoreNLP\nfrom pycorenlp import StanfordCoreNLP","4f146737":"api_key = 'tsj4P1P8phL90G2QQF1HkQJFh'\napi_key_secret = 'kc8X3dglmH1VKjk14KdhoYGTNe7AabvuJ4hi5pwgNvuwklMWit'\naccess_token = '1367899270444154883-aEisBRiV9QQYQcBBIS7xyloddHbJJT'\naccess_token_secret = 'TpaAiMfFcvPBg765tDeQcxNkFm4HK1TOyrK6E1jwGemrd'\n\nauth = tweepy.OAuthHandler(api_key, api_key_secret)\nauth.set_access_token(access_token, access_token_secret)\napi = tweepy.API(auth)","f2e4222c":"# Generate list of dates (7 days window) based on today's date\nlist_of_dates = []\ntoday = date.today()\nfor i in range(-7,1):\n    target_date = (today + timedelta(days=i)).strftime(\"%Y-%m-%d\")\n    list_of_dates.append(target_date)","cf75fe37":"list_of_dicts = []\nsearch_term = 'VaccinePassports'\nnum_tweets = 16000","ac4423dd":"def get_tweets(search_term = search_term, num_tweets = num_tweets):\n    \n    for end_date in list_of_dates:\n        start_date = (datetime.strptime(end_date, '%Y-%m-%d') - timedelta(days=1)).strftime(\"%Y-%m-%d\") # Create 1-day windows for extraction\n        tweet_count = len(list_of_dicts)\n\n        for tweet in tweepy.Cursor(api.search,\n                                   q=f'{search_term} since:{start_date} until:{end_date}',\n                                   lang = 'en',\n                                   count = num_tweets,\n                                   tweet_mode = 'extended').items(num_tweets):\n            if (not tweet.retweeted) and ('RT @' not in tweet.full_text):\n                if tweet.lang == \"en\":\n                    tweet_dict = {}\n                    tweet_dict['username'] = tweet.user.name\n                    tweet_dict['location'] = tweet.user.location\n                    tweet_dict['text'] = tweet.full_text\n                    #tweet_dict['fav_count'] = tweet.favorite_count  \n                    tweet_dict['hashtags'] = tweet.entities['hashtags']\n                    tweet_dict['tweet_date'] = tweet.created_at\n                    list_of_dicts.append(tweet_dict)\n                    tweet_count +=1\n                    print(f'Extracted tweet count = {tweet_count}')\n                \n        print(f'Completed extraction for {start_date} to {end_date}. Sleep for 15 mins')\n        #time.sleep(900)\n        print('Ready to go again')","0447774a":"# Run tweet extraction function\nget_tweets()","e9fff36c":"# Number of tweets pulled\nlen(list_of_dicts)","741f9fdc":"# Transform list of dictionaries into a Pandas dataframe\ntweets_df = pd.DataFrame(list_of_dicts)\ntweets_df.sort_values(by='tweet_date').reset_index(drop = True)","bf1927a9":"# Setup function to extract hashtags text from the raw hashtag dictionaries\ndef extract_hashtags(hashtag_list):\n    \n    s = \"\" # Create empty string\n    if not hashtag_list: # If list is empty, return empty string\n        return s\n    else:\n        for dictionary in hashtag_list:\n            s+= str(dictionary['text'].lower() + ',') # Create string (lowercase) for each hashtag text\n        s = s[:-1] # Drop last character ','\n        return s","13fc0f7b":"# Extract hashtags\ntweets_df['hashtags_extracted'] = tweets_df['hashtags'].apply(lambda x: extract_hashtags(x))\ntweets_df.drop(columns = 'hashtags', inplace = True)","42d0919f":"tweets_df.head()","119d3da2":"# Keep only tweets that involve the vaccine\ntweets_df_final = tweets_df[(tweets_df['text'].str.contains(\"vacc\")) \n                            | (tweets_df['text'].str.contains(\"Vacc\"))\n                            | (tweets_df['hashtags_extracted'].str.contains(\"vacc\"))\n                            | (tweets_df['hashtags_extracted'].str.contains(\"Vacc\"))]\nlen(tweets_df_final)","548ed157":"# Create timestamp for datetime of extraction\nextract_datetime = datetime.today().strftime('%Y%m%d_%H%M%S')\n\n# Create csv filename\nfilename = '.\/VaccinePassportCampUK' + extract_datetime + '.csv'\n\n# Drop duplicates (if any)\ntweets_df_final.drop_duplicates(inplace = True)\n\n# Export dataframe as csv file with above filename\ntweets_df_final.to_csv(filename, index = False)","9b40cde5":"# Convert all tweet text to lowercase\n# tweets_df['text'] = tweets_df['text'].apply(lambda x: str(x.lower()))\n\n# Note, skipping this step as uppercase reflects sentiments","2c081e62":"tweets_df = tweets_df_final.copy()","b8ba8ef4":"!pip install tweet-preprocessor\n\nimport preprocessor as p","b511b754":"# Clean tweet text with tweet-preprocessor\nfrom preprocessor.api import clean, tokenize, parse\ntweets_df['text_cleaned'] = tweets_df['text'].apply(lambda x: clean(x))","9c64c340":"# Remove duplicate tweets\ntweets_df.drop_duplicates(subset='text_cleaned', keep=\"first\", inplace = True)\nlen(tweets_df)","1ac43488":"# Remove unnecessary characters\n# Note: Need to remove % as Stanford CoreNLP annotation encounters error if text contains some of these characters\npunct =['%','\/',':','\\\\','&amp;','&',';']\n\ndef remove_punctuations(text):\n    for punctuation in punct:\n        text = text.replace(punctuation, '')\n    return text\n\ntweets_df['text_cleaned'] = tweets_df['text_cleaned'].apply(lambda x: remove_punctuations(x))","1dc2e523":"# Drop tweets which have empty text field\ntweets_df['text_cleaned'].replace('', np.nan, inplace=True)\ntweets_df['text_cleaned'].replace(' ', np.nan, inplace=True)\ntweets_df.dropna(subset=['text_cleaned'], inplace=True)\nlen(tweets_df)","6ff34d2c":"tweets_df = tweets_df.reset_index(drop=True)\ntweets_df.sample(5)","b35715b8":"# Define function to get value counts\ndef get_value_counts(col_name, analyzer_name):\n    count = pd.DataFrame(tweets_df[col_name].value_counts())\n    percentage = pd.DataFrame(tweets_df[col_name].value_counts(normalize=True).mul(100))\n    value_counts_df = pd.concat([count, percentage], axis = 1)\n    value_counts_df = value_counts_df.reset_index()\n    value_counts_df.columns = ['sentiment', 'counts', 'percentage']\n    value_counts_df.sort_values('sentiment', inplace = True)\n    value_counts_df['percentage'] = value_counts_df['percentage'].apply(lambda x: round(x,2))\n    value_counts_df = value_counts_df.reset_index(drop = True)\n    value_counts_df['analyzer'] = analyzer_name\n    return value_counts_df","b793bd82":"sia = SentimentIntensityAnalyzer()\n\n# Obtaining NLTK scores\ntweets_df['nltk_scores'] = tweets_df['text_cleaned'].apply(lambda x: sia.polarity_scores(x))\n\n# Obtaining NLTK compound score\ntweets_df['nltk_cmp_score'] = tweets_df['nltk_scores'].apply(lambda score_dict: score_dict['compound'])","175ef82c":"neutral_thresh = 0.05","0830a86f":"# Categorize scores into the sentiments of positive, neutral or negative\ntweets_df['nltk_sentiment'] = tweets_df['nltk_cmp_score'].apply(lambda c: 'Positive' if c >= neutral_thresh else ('Negative' if c <= -(neutral_thresh) else 'Neutral'))\n\n# Neutral score = 0\n# tweets_df['nltk_sentiment'] = tweets_df['nltk_cmp_score'].apply(lambda c: 'Positive' if c > 0 else ('Negative' if c < 0 else 'Neutral'))","34d1d280":"tweets_df['nltk_cmp_score'].describe()","85bd4f07":"nltk_sentiment_df = get_value_counts('nltk_sentiment','NLTK Vader')\nnltk_sentiment_df","de50d0f2":"sns.set_theme(style=\"dark\")\nax = sns.barplot(x=\"sentiment\", y=\"percentage\", data=nltk_sentiment_df)\nax.set_title('NLTK Vader')\n\nfor index, row in nltk_sentiment_df.iterrows():\n    ax.text(row.name,row.percentage, round(row.percentage,1), color='black', ha=\"center\")","c582f72d":"# Obtain polarity scores generated by WordCloud\ntweets_df['wordCloud_score'] = tweets_df['text_cleaned'].apply(lambda x: TextBlob(x).sentiment.polarity)","4141cae5":"neutral_thresh = 0.05","71628be6":"# Convert polarity score into sentiment categories\ntweets_df['wordCloud_sentiment'] = tweets_df['wordCloud_score'].apply(lambda c: 'Positive' if c >= neutral_thresh else ('Negative' if c <= -(neutral_thresh) else 'Neutral'))","0832fa71":"tweets_df['wordCloud_score'].describe()","b8c57990":"from wordcloud import WordCloud\ntextblob_sentiment_df = get_value_counts('wordCloud_sentiment','WordCloud')\ntextblob_sentiment_df","764e8c2e":"sns.set_theme(style=\"dark\")\nax = sns.barplot(x=\"sentiment\", y=\"percentage\", data=textblob_sentiment_df)\nax.set_title('WordCloud')\n\nfrom wordcloud import WordCloud\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk import sent_tokenize, word_tokenize\nfrom wordcloud import WordCloud, STOPWORDS\n\ndef wc(data,bgcolor,title):\n    plt.figure(figsize = (100,100))\n    wc = WordCloud(background_color = bgcolor, max_words = 1000,  max_font_size = 50)\n    wc.generate(' '.join(data))\n    plt.imshow(wc)\n    plt.axis('off')\n    \nfor index, row in textblob_sentiment_df.iterrows():\n    ax.text(row.name,row.percentage, round(row.percentage,1), color='black', ha=\"center\")\n    \nwc(textblob_sentiment_df,'black','VaccinePassport')","3f9bff4e":"nlp = stanza.Pipeline(lang='en', processors='tokenize,sentiment')","197acc1e":"def stanza_analyze(Text):\n    document = nlp(Text)\n    print('Processing')\n    return np.mean([(i.sentiment - 1) for i in document.sentences]) # Minus 1 so as to bring score range of [0,2] to [-1,1]","7e2b218e":"# Obtain sentiment categorical score generated by Stanza\ntweets_df['stanza_score'] = tweets_df['text_cleaned'].apply(lambda x: stanza_analyze(x))","fe3d4ede":"tweets_df['stanza_score'].describe()","d4218798":"neutral_thresh = 0.05","19d2d4b5":"# Convert average Stanza sentiment score into sentiment categories\ntweets_df['stanza_sentiment'] = tweets_df['stanza_score'].apply(lambda c: 'Positive' if c >= neutral_thresh else ('Negative' if c <= -(neutral_thresh) else 'Neutral'))","1596d18e":"stanza_sentiment_df = get_value_counts('stanza_sentiment','Stanza')\nstanza_sentiment_df","b1a3d7a0":"sns.set_theme(style=\"dark\")\nax = sns.barplot(x=\"sentiment\", y=\"percentage\", data=stanza_sentiment_df)\nax.set_title('Stanza')\n\nfor index, row in stanza_sentiment_df.iterrows():\n    ax.text(row.name,row.percentage, round(row.percentage,1), color='black', ha=\"center\")","81a47a11":"flair_clf = TextClassifier.load('en-sentiment')","6f1e1c5f":"# Create function to run flair sentiment analyzer\ndef flair_analyze(Text):\n    sentence = Sentence(Text)\n    flair_clf.predict(sentence)\n    flair_senti = sentence.labels\n    return flair_senti","28151c70":"# Obtain sentiment scores generated by FlairNLP\ntweets_df['flair_output'] = tweets_df['text_cleaned'].apply(lambda x: flair_analyze(x))","e24058f0":"tweets_df['flair_output'] = tweets_df['flair_output'].astype(str)","6ab1ce29":"def extract_flair_senti(output):\n    sentiment = \" \".join(re.findall(\"[a-zA-Z]+\", output))\n    sentiment = sentiment.capitalize()\n    return sentiment\n\ndef extract_flair_score(output):\n    score = \".\".join(re.findall(r\"\\d+\", output)) \n    return score","caa69016":"# Get flair sentiment and polarity score\ntweets_df['flair_sentiment'] = tweets_df['flair_output'].apply(lambda x:extract_flair_senti(x))\ntweets_df['flair_score'] = tweets_df['flair_output'].apply(lambda x:extract_flair_score(x))","6cf4fb46":"flair_sentiment_df = get_value_counts('flair_sentiment','Flair')\nflair_sentiment_df","ce3f7d65":"sns.set_theme(style=\"dark\")\nax = sns.barplot(x=\"sentiment\", y=\"percentage\", data=flair_sentiment_df)\nax.set_title('Flair')\n\nfor index, row in flair_sentiment_df.iterrows():\n    ax.text(row.name,row.percentage, round(row.percentage,1), color='black', ha=\"center\")","298448f1":"df_sentiments = pd.concat([nltk_sentiment_df, \n                           textblob_sentiment_df, \n                           stanza_sentiment_df,\n                           #flair_sentiment_df,\n                           #corenlp_sentiment_df,\n                          ]).reset_index(drop=True)\ndf_sentiments","fa8f22be":"df_sentiments_pivot = df_sentiments.pivot(index='sentiment', columns='analyzer', values='percentage')\ndf_sentiments_pivot","cac0764f":"plt.figure(figsize=(10,6))\nax = sns.barplot(x=\"analyzer\", y=\"percentage\",\n                 hue=\"sentiment\", data=df_sentiments)\n\n# Display annotations\nfor p in ax.patches:\n    ax.annotate(f\"{round(p.get_height(),1)}%\", \n                   (p.get_x() + p.get_width() \/ 2., p.get_height()), \n                   ha = 'center', va = 'center', \n                   size=12,\n                   xytext = (0, -12), \n                   textcoords = 'offset points')","5361891e":"# Make use of sentiments from NLTK Vader, WordCloud and Stanza\ntweets_df['composite_score'] =  (tweets_df['nltk_cmp_score'] \n                                + tweets_df['wordCloud_score']\n                                + tweets_df['stanza_score'])\/3","d5a54da1":"tweets_df['composite_score'].describe()","877f11d6":"# Threshold for neutral sentiment\nneutral_thresh = 0.05","66139ece":"# Convert average sentiment score (from all 3 analyzers) into sentiment categories\ntweets_df['composite_vote_2'] = tweets_df['composite_score'].apply(lambda c: 'Positive' if c >= neutral_thresh else ('Negative' if c <= -(neutral_thresh) else 'Neutral'))","c3d915d1":"composite_sentiment_df_2 = get_value_counts('composite_vote_2','Composite Sentiment')\ncomposite_sentiment_df_2","a3a928e8":"plt.figure(figsize=(10,6))\nax = sns.barplot(x=\"sentiment\", y=\"percentage\",\n                 data=composite_sentiment_df_2)\n\n# Display annotations\nfor p in ax.patches:\n    ax.annotate(f\"{round(p.get_height(),1)}%\", \n                   (p.get_x() + p.get_width() \/ 2., p.get_height()), \n                   ha = 'center', va = 'center', \n                   size=12,\n                   xytext = (0, -12), \n                   textcoords = 'offset points')","46838bcc":"# Make use of sentiments from NLTK Vader, WordCloud and Stanza\ntweets_df['sentiment_votes'] =  tweets_df.apply(lambda x: list([x['nltk_sentiment'], \n                                                                x['wordCloud_sentiment'], \n                                                                x['stanza_sentiment']]),axis=1) ","25ddfae9":"# Create function to get sentiment that appears most often amongst the 3 votes\ndef get_most_voted_senti(List):\n    if len(List) == len(set(List)): # If all elements are different\n        return 'Neutral'\n    else:\n        return max(set(List), key = List.count)","82947a96":"# Get composite sentiment vote\ntweets_df['composite_vote'] = tweets_df['sentiment_votes'].apply(lambda x: get_most_voted_senti(x))","08ad01dd":"composite_sentiment_df = get_value_counts('composite_vote','Composite Sentiment (Max Voting)')\ncomposite_sentiment_df","b594f951":"plt.figure(figsize=(10,6))\nax = sns.barplot(x=\"analyzer\", y=\"percentage\",\n                 hue=\"sentiment\", data=composite_sentiment_df)\n\n# Display annotations\nfor p in ax.patches:\n    ax.annotate(f\"{round(p.get_height(),1)}%\", \n                   (p.get_x() + p.get_width() \/ 2., p.get_height()), \n                   ha = 'center', va = 'center', \n                   size=12,\n                   xytext = (0, -12), \n                   textcoords = 'offset points')","d56af07b":"#### Average Score\n- Take average of the 3 sentiment scores of NLTK Vader, WordCloud and Stanza","e66637d0":"___\nNote: Other libraries (i.e. FlairNLP (Part 3D) and Stanford CoreNLP (Part 3E)) were also explored. FlairNLP was omitted from analysis as it does not give Neutral classification. Stanford CoreNLP was omitted from analysis as it has been replaced by Stanza as Stanford NLP Group's default NLP library.","b28acd49":"___\n<a name=\"sentiment_flair\"><\/a>\n## Part 3D - Sentiment Analysis with FlairNLP\n","58417ec5":"___\n## Part 3 - Sentiment Analysis","20875b6d":"___\n<a name=\"setup\"><\/a>\n## Part 1 - Notebook Setup and Authentication","beef98a8":"#### Twitter API credentials setup","f0379d1e":"Using tweet-preprocessor Python package (https:\/\/pypi.org\/project\/tweet-preprocessor\/)  \n`pip install tweet-preprocessor`\n\nPreprocessor is a preprocessing library for tweet data written in Python. Currently supports cleaning, tokenizing and parsing:\nURLs, Hashtags, Mentions, Reserved words (RT, FAV), Emojis, Smileys","7b3de73e":"# WordCloud and TextBlob","db9e2df8":"___\n<a name=\"insights\"><\/a>\n## Part 4 - Insights from Sentiment Analyses\nWe will focus on the results from NLTK VADER, WordCloud and Stanza because they are:\n- Trained on at least 1 social media dataset\n- Able to give at least 3 classes of sentiments i.e. Positive, Neutral, Negative","8f070ea0":"___\n<a name=\"sentiment_textblob\"><\/a>\n## Part 3B - Sentiment Analysis with WordCloud\n","ad01b4b6":"Score mapping:  \n0: Negative  \n1: Neutral  \n2: Positive","d5bafcb6":"___\n<a name=\"api\"><\/a>\n## Part 2 - Data Extraction with Twitter API and Text Pre-Processing","b995b8da":"___\n### Text Pre-processing","086157db":"___\n<a name=\"sentiment_vader\"><\/a>\n## Part 3A - Sentiment Analysis with NLTK Vader\nNatural Learning Toolkit (NLTK) is a Python package that offers programs supporting natural language processing (NLP). In addition to its text corpus, it also comes with pre-trained models.\u00a0\nIn particular, we will be using the Valence Aware Dictionary and sEntiment Reasoner (VADER) model, which is a lexicon and rule-based sentiment analysis tool specifically aimed at sentiment analysis of social media text. It uses a bag of words approach with simple heuristics (such as increasing sentiment intensity in presence of certain words like \"very\" or \"really\").\n\nAfter installing NLTK with the command `pip install nltk`, we can run sentiment analysis using VADER with these lines of code:","b9188b4c":"___\n<a name=\"sentiment_stanza\"><\/a>\n## Part 3C - Sentiment Analysis with Stanza\nStanza is the default Python NLP library of the Stanford NLP Group, replacing the older Java-based CoreNLP. The modules are built on top of PyTorch, and its pre-built sentiment analyzer is trained on several datasets, including the Stanford Sentiment Treeback and Airline Twitter Sentiment.","2d1743cc":"___\n#### Experiment: Max Voting\n- Get composite sentiment by doing max voting amongst the 3 analyzers NLTK Vader, WordCloud and Stanza","85bf0d38":"___\n<a name=\"ensemble\"><\/a>\n## Part 5 - Composite Sentiment with Ensemble Method","1b04467e":"For nlp output, the sentiment outputs are 'sentiment', 'sentimentValue' and 'sentimentDistribution'. \"sentimentDistribution\" returns an array of the probabilities of each of the 5 sentiments","2295ebf4":"#### Import dependencies"}}