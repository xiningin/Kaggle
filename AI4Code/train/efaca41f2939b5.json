{"cell_type":{"8e49b3c6":"code","127dec80":"code","e8ef1716":"code","cd4209a4":"code","22478fc5":"code","bba02c60":"code","e590249e":"code","c4e1b409":"code","57cf340b":"code","b192049c":"code","99022698":"code","02c1fb9b":"code","37e03434":"code","b44b7137":"code","5503e7b9":"code","656fb4f6":"code","35301fcc":"code","bf898a42":"code","966e1eed":"code","e0d224b5":"code","c7ff2dae":"code","4abbca0d":"code","d3ac42af":"code","8ba84006":"code","4be0aa75":"code","6dd7cae8":"markdown","c6d944a4":"markdown","86eca6ed":"markdown","7b3bb127":"markdown","9d33e9ea":"markdown","5b95729d":"markdown","c606e89a":"markdown","aea68780":"markdown","6f029441":"markdown","805a4596":"markdown","a1835600":"markdown","4ce13f29":"markdown","496a7a04":"markdown","5bf9a821":"markdown","6fd030fe":"markdown","2cc23608":"markdown","cdfe71a6":"markdown","c03b47cf":"markdown","cda7a2fe":"markdown","fc39db57":"markdown","c403d003":"markdown","86eeff4c":"markdown","5dcacc07":"markdown","64f279e4":"markdown"},"source":{"8e49b3c6":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","127dec80":"missing_values = [\"n\/a\", \"na\", \"--\",\"-\"]","e8ef1716":"newsdat=pd.read_csv(\"\/kaggle\/input\/online-news-popularity\/OnlineNewsPopularity.csv\",na_values = missing_values)","cd4209a4":"newsdat.shape","22478fc5":"newsdat.drop_duplicates()","bba02c60":"newsdat.isnull().values.any()","e590249e":"newsdat=newsdat.drop(['url'],axis=1)\n","c4e1b409":"newsdat.head(10)","57cf340b":"newsdat.isnull().sum()","b192049c":"newsdat.describe()","99022698":"plt.figure(figsize=(40,30))\ncor = newsdat.corr(method ='pearson')\nsns.heatmap(cor, cmap=\"RdYlGn\")\nplt.show()","02c1fb9b":"newsdat1=newsdat.drop([' n_non_stop_words',' n_unique_tokens',' kw_avg_min',' kw_avg_avg',' self_reference_avg_sharess'],axis=1)","37e03434":"y=newsdat1[' shares']\nX=newsdat1.drop(' shares',axis=1)","b44b7137":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)","5503e7b9":"from sklearn.ensemble import RandomForestRegressor\nrf = RandomForestRegressor(min_samples_split=2)\nrf.fit(X_train, y_train)","656fb4f6":"predicted_test = rf.predict(X_test)\nfrom sklearn import metrics\nprint('Mean Absolute Error:', metrics.mean_absolute_error(y_test, predicted_test))\nprint('Mean Squared Error:', metrics.mean_squared_error(y_test, predicted_test))\nprint('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, predicted_test)))","35301fcc":"X_train.shape","bf898a42":"def rf_feat_importance(m, df):\n    return pd.DataFrame({'cols':df.columns, 'imp':m.feature_importances_}\n                       ).sort_values('imp', ascending=False)\nfi = rf_feat_importance(rf, X_train); ","966e1eed":"def plot_fi(fi): return fi.plot('cols', 'imp', 'barh', figsize=(12,7), legend=False)\nplot_fi(fi[:45]);","e0d224b5":"x = newsdat1.values #returns a numpy array\nmin_max_scaler = preprocessing.MinMaxScaler()\nx_scaled = min_max_scaler.fit_transform(x)\ndf = pd.DataFrame(x_scaled)","c7ff2dae":"y=df[54]\nX=df.drop(54,axis=1)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\nrf = RandomForestRegressor(min_samples_split=5)\nrf.fit(X_train, y_train)","4abbca0d":"ac=[]\nfor x in range(2,11):\n    rf = RandomForestRegressor(min_samples_split=x)\n    rf.fit(X_train, y_train)\n    predicted_test1 = rf.predict(X_test)\n    ac.append(np.sqrt(metrics.mean_squared_error(y_test, predicted_test1)))","d3ac42af":"b=[2,3,4,5,6,7,8,9,10]\nplt.plot(b,ac)","8ba84006":"rf = RandomForestRegressor(min_samples_split=9)\nrf.fit(X_train, y_train)\npredicted_test = rf.predict(X_test)\n\n","4be0aa75":"print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, predicted_test))\nprint('Mean Squared Error:', metrics.mean_squared_error(y_test, predicted_test))\nprint('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, predicted_test)))","6dd7cae8":"#Selection of Model:","c6d944a4":"#Objective","86eca6ed":"After performing feature engineering we train our model and evaluate our model with different min samples split which has been done by keeping in the loop and recorded the Root mean square error value. The below figure give us at what min_samples_split we get our efficient model that is less RMSE.","7b3bb127":"#Train and test the data","9d33e9ea":"#Random Forest Regression:","5b95729d":"#Feature Engineering:","c606e89a":"By the above Pearson correlation graph we can say that number of unique words and number of non-stop-words and number of non-stop-unique tokens are strongly correlated which implies that they are strongly linearly dependent on each other. Same as the above case Kw-avg- min and kw-max-min are also strongly corelated.","aea68780":"#Data Preprocessing","6f029441":"By the above correlation graph we can clearly say that these features:\n\u2022\tnumber of unique words and number of non-stop-words and number of non-stop-unique tokens\n\u2022\tKw-avg- min and kw-max-min\nThese are strongly correlated and linearly dependent which makes us to assume that these features are so linearly dependent that any one of the strong correlated feature can be used and excluding the other features won\u2019t affect the model and will be indirectly helpful in our model by not allowing to do overfitting\n","805a4596":"As RMSE error rate is too high because of the no proper normalized data. As we see the ranges if data is of different ranges and it effects the regression.So we decided to perform feature engineering. ","a1835600":"#Exploratory Analysis","4ce13f29":"***Tokens Vs Images:***\n![image.jpeg](attachment:image.jpeg)\nFrom the above Analysis we are able to Analyze that for what token lengths the number of images should be more. By observing the above graph we can say that till min of 2000 tokens or words we need to have minimum of 30 images and gradually the increase in tokens we need to increase in images so that people who are reading these articles have more understanding in a visualizing way.\n","496a7a04":"checking for missing values and duplicate values","5bf9a821":"Predicting the number of shares an article can get it","6fd030fe":"#Feature Selection","2cc23608":"As early in Data Preprocessing we analyzed that our data has no missing values and no duplicate values but we have some outliers which has been found out in or Exploratory analysis. But processing the data for our 2 models has been different. For Model1 there is no change in data as we have the whole data in numerical so need not to be having a one hot encoding but as the range of the data in different columns in very variance manner we will perform normalization which we will discuss in our later section of report.\nBut in Second model where we need to classify the articles into different categories we have choosed neural networks and  need to change the every different categories into one category and needed to perform one hot encoded which will finally placed into a vector of one column.\nThen we performed cross-tabulation of every category in our data set and found out the distribution of categories in our data set. The below table shows the number of articles placed in each category:\n\nCategory\tNumber of articles in that category\n    World\t    8427\ntechnology\t    7346\nSocial media\t2323\nEntertainment\t7057\nLifestyle\t    2099\nBusiness\t    6258\n\nAs we can observe the data classification if categories with Social media and life style has less variety of assumptions. We are dropping time delta and Url which is metadata of articles which won\u2019t be any use in our regression model.\n","cdfe71a6":"As we observe the data set we can clearly state that data values in the data set are varied so much and we have some even binary values . To handle this type we have normalized the whole data set so that all column ranges have been normalized to 0-1 range.","c03b47cf":"For this project we have choose Random forest Regression as our Model. Because as we observed in Data Preprocessing we had Low bias and by Descriptive Statistics we had observed High variance of data and above that we have some anomalies which we need to consider and handle it. By considering the above all conditions we have selected Random forest regression as predicting our shares","cda7a2fe":"***Density Distribution of Shares:***\n![image.jpeg](attachment:image.jpeg)","fc39db57":"#Training and Hyper Tuning of Parameters:","c403d003":"#Pearson-Correlational Analysis:","86eeff4c":"The above graph represents the density distribution of shares in our whole dataset. As we can see we have maximum examples of shares between 0-10,000 shares and gradually decreased with the number of shares increasing. We can actually consider that the shares which are above 20,000 as outliers but in our project these are considered as anomalies. As our project says about the maximum of shares of an article if the article has been shared the most of the time then the features of article has been so peculiar that it has reached maximum of shares. So considering anomality condition we are not classifying these peculiar data  as outliers but as anamolies.","5dcacc07":"By above figure it is clear that at Min_samples_split the RMSE error is very less after that it started increasing again implying us that at  9 we get our efficient model . Performing at 9 and test our data set we get the RMSE error as follows.","64f279e4":"***Token length Vs Shares:***\n![image.jpeg](attachment:image.jpeg)\nPlotted this graph is mainly to analyze at what token length or at what length of articles the people are liking the articles and shared most. By observing the above graph we can actually say that people are reading the short articles most and shared a lot and the other one are very length article who has more 2000 tokens has been read and shared a lot. By this we can understood that users are of two types who are concentrating on the content and the other are who are just observing what their need is presented in article in a straight-forward manner."}}