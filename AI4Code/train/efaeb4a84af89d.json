{"cell_type":{"374caa22":"code","2c929bf9":"code","f4bc59a4":"code","4e81ef96":"code","3a29d0cd":"code","30bbfe53":"code","8866eb00":"code","d66531a3":"code","69d5437f":"code","04a77ec5":"code","dc691fb3":"code","66aa5576":"code","4d043b16":"code","b33f16b4":"code","b9f78e01":"code","b6eff119":"code","eb87b02d":"code","7ef879e1":"code","330cbded":"code","003757ed":"code","22eea714":"code","45d02c02":"code","56ef4098":"code","7a3cb167":"code","f950d812":"code","f22f9166":"markdown","6835dd08":"markdown"},"source":{"374caa22":"import pandas as pd\nimport numpy as np\nimport re\nfrom gensim.models import Word2Vec\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.manifold import TSNE\nimport random\nfrom collections import Counter\nimport plotly.express as px\nfrom sklearn.neighbors import NearestNeighbors\nfrom nltk.stem import WordNetLemmatizer\nwordnet_lemmatizer = WordNetLemmatizer()\nfrom sklearn.neighbors import NearestNeighbors\nimport seaborn as sns \nfrom collections import defaultdict\nfrom wordcloud import WordCloud, STOPWORDS\nstopwords = set(STOPWORDS)\nimport cv2\nimport os","2c929bf9":"train = pd.read_csv(\"..\/input\/shopee-product-matching\/train.csv\")\ntest = pd.read_csv(\"..\/input\/shopee-product-matching\/test.csv\")","f4bc59a4":"def len_seq(data): \n    return len(data.split()) \ntrain[\"number_tokens\"] = train[\"title\"].apply(lambda x : len_seq(x)) \nfig = px.histogram(\n    train, \n    x=\"number_tokens\",\n    width=800,\n    height=500,\n    title='Number tokens distribution'\n)\nfig.show()","4e81ef96":"TOKEN_RE = re.compile(r'[\\w]+')\ndef tokenize_text_simple_regex(txt, min_token_size=2):\n    txt = str(txt).lower()\n    all_tokens = TOKEN_RE.findall(txt)\n    return [wordnet_lemmatizer.lemmatize(token, pos=\"v\") for token in all_tokens if len(token) >= min_token_size]","3a29d0cd":"def tokenize_corpus(texts, tokenizer=tokenize_text_simple_regex, **tokenizer_kwargs):\n    return [tokenizer(text, **tokenizer_kwargs) for text in texts]","30bbfe53":"train['title'] = train['image_phash'] + ' ' + train['title'] + ' ' + train['image_phash']\ntest['title'] = test['image_phash'] + ' ' + test['title'] + ' ' + test['image_phash']","8866eb00":"corpus = tokenize_corpus(list(pd.concat([train['title'], test['title']])))","d66531a3":"model = Word2Vec(\n        sentences=corpus,\n        vector_size=100, \n        window=15, \n        min_count=1, \n        sg=1, #skip-gram\n        negative=7, \n        epochs=1000, \n        seed=42,\n        workers=6)","69d5437f":"def plot_vectors(vectors, labels, how='tsne', ax=None):\n    if how == 'tsne':\n        projections = TSNE().fit_transform(vectors)\n    elif how == 'svd':\n        projections = TruncatedSVD().fit_transform(vectors)\n    x = projections[:, 0]\n    y = projections[:, 1]\n    ax.scatter(x, y)\n    for cur_x, cur_y, cur_label in zip(x, y, labels):\n        ax.annotate(cur_label, (cur_x, cur_y))","04a77ec5":"def n_grams(ngram, data):\n    freq_dict = defaultdict(int)\n    for text in data:\n        tokens = [w for w in text.lower().split() if w != \" \" if w not in stopwords]\n        ngrams = zip(*[tokens[i:] for i in range(ngram)])\n        list_grams = [\" \".join(ngram) for ngram in ngrams]\n        for word in list_grams:\n            freq_dict[word] += 1\n    df_ngram =  pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])   \n    df_ngram.columns = [\"word\", \"wordcount\"]\n    return df_ngram ","dc691fb3":"df_3_grams = n_grams(3, train[\"title\"]) \nprint(df_3_grams.head(20))\nprint(df_3_grams.tail())","66aa5576":"test_words = ['jam','tangan','wanita','xiaomi','redmi','note','somebymi','yuja','niacin','mm','3m']\ngensim_words = [w for w in test_words if w in model.wv.index_to_key]\ngensim_vectors = np.stack([model.wv[w] for w in gensim_words])","4d043b16":"fig, ax = plt.subplots()\nfig.set_size_inches((10, 10))\nplot_vectors(gensim_vectors, test_words, how='svd', ax=ax)","b33f16b4":"example_hash = 'a6f319f924ad708c'","b9f78e01":"def draw_img_hash(hash):\n    plot_list = train[train['image_phash'] == example_hash]['image'].tolist()[0]\n    plt.figure(figsize=(5, 5))\n    image = cv2.imread(os.path.join('..\/input\/shopee-product-matching\/train_images\/', plot_list))\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    plt.title(plot_list, fontsize=12)\n    plt.axis(\"off\")\n    plt.imshow(image)\n    plt.show()","b6eff119":"draw_img_hash(example_hash)","eb87b02d":"#similar words\nnn = model.wv.most_similar(example_hash)\nnn","7ef879e1":"draw_img_hash('e69999663199cc93')","330cbded":"draw_img_hash('bce5c11a96393cc6')","003757ed":"import gc\ndel train\ndel test \ndel corpus\ngc.collect()","22eea714":"if len(pd.read_csv(\"..\/input\/shopee-product-matching\/test.csv\")) > 3: \n    df = pd.read_csv(\"..\/input\/shopee-product-matching\/test.csv\")\nelse: \n    df = pd.read_csv(\"..\/input\/shopee-product-matching\/train.csv\")","45d02c02":"embeds = []\nfor phash in df['image_phash'].tolist():\n    try:\n        embeds.append(model.wv[phash].tolist())\n    except KeyError:\n        embeds.append(np.zeros((100), dtype='float32').tolist())","56ef4098":"neighbors_model = NearestNeighbors(n_neighbors = 50, metric='cosine').fit(embeds)\ntext_distances, text_indices = neighbors_model.kneighbors(embeds)","7a3cb167":"predictions = []\nfor k in range(df.shape[0]):\n    idx_text = np.where(text_distances[k,] < 0.17)[0]\n    ids_text = text_indices[k,idx_text]\n    posting_ids = ' '.join(df.iloc[ids_text]['posting_id'].values)\n    predictions.append(posting_ids)","f950d812":"df['matches'] = predictions\ndf[['posting_id', 'matches']].to_csv('submission.csv', index = False)","f22f9166":"W2V is working :)","6835dd08":"Distribution helps to set parameters: window, negative "}}