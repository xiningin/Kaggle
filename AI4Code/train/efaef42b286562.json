{"cell_type":{"3a0d6132":"code","8ca02509":"code","4726fecf":"code","67a202a0":"code","1a4f55d1":"code","7a8ca47b":"code","55d3f136":"code","66d42dd9":"code","14284e1b":"code","9b990256":"code","4b62a031":"code","c6ffede0":"code","ace5c7bb":"code","4cc72f9b":"code","994f397b":"code","628ebc1f":"code","1edcca4b":"code","958275b1":"code","d9c8a9e6":"code","2019b1cc":"code","4cf726df":"code","21814e64":"code","8c37c6ad":"code","c12a252f":"code","1a48889f":"code","021d0bff":"code","13e9064d":"code","b49381a0":"code","4221e7bc":"code","d3d98fa4":"code","2502b4f1":"markdown","ea63e0fc":"markdown","3053f864":"markdown","fd61a2bd":"markdown","a4ebd78d":"markdown","22406799":"markdown","18596175":"markdown","289be48c":"markdown","172896e9":"markdown","e9514740":"markdown","e07421d0":"markdown","c0771363":"markdown","49daf57c":"markdown","b04e3faf":"markdown"},"source":{"3a0d6132":"# Import libraries\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np","8ca02509":"# Load data sets\ndf_train = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv')\ndf_test = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv')\ntest_ID = df_test['Id']","4726fecf":"# Print the first 10 rows\ndf_train.head(10)","67a202a0":"# Get descriptive stats\ndf_train.describe()","1a4f55d1":"# Now drop the  'Id' column since it's unnecessary for  the prediction process.\ndf_train.drop(\"Id\", axis=1, inplace=True)\ndf_test.drop(\"Id\", axis=1, inplace=True)\nprint(df_train.columns)\nprint(df_train['SalePrice'].describe())","7a8ca47b":"# histogram\nsns.distplot(df_train['SalePrice'])","55d3f136":"# scatter plot grlivarea\/saleprice\nvar = 'GrLivArea'\ndata = pd.concat([df_train['SalePrice'], df_train[var]], axis=1)\ndata.plot.scatter(x=var, y='SalePrice', ylim=(0, 800000))","66d42dd9":"# scatter plot totalbsmtsf\/saleprice\nvar = 'TotalBsmtSF'\ndata = pd.concat([df_train['SalePrice'], df_train[var]], axis=1)\ndata.plot.scatter(x=var, y='SalePrice', ylim=(0, 800000))","14284e1b":"# scatter plot LotFrontage\/saleprice\nvar = 'LotFrontage'\ndata = pd.concat([df_train['SalePrice'], df_train[var]], axis=1)\ndata.plot.scatter(x=var, y='SalePrice', ylim=(0, 800000))","9b990256":"# scatter plot 1stFlrSF\/saleprice\nvar = '1stFlrSF'\ndata = pd.concat([df_train['SalePrice'], df_train[var]], axis=1)\ndata.plot.scatter(x=var, y='SalePrice', ylim=(0, 800000))","4b62a031":"# boxplot overallqual\/saleprice\nvar = 'OverallQual'\ndata = pd.concat([df_train['SalePrice'], df_train[var]], axis=1)\nf, ax = plt.subplots(figsize=(8, 6))\nfig = sns.boxplot(x=var, y=\"SalePrice\", data=data, palette=sns.color_palette(\"hls\", 8))\nfig.axis(ymin=0, ymax=800000)","c6ffede0":"# boxplot YearBuilt\/saleprice\nvar = 'YearBuilt'\ndata = pd.concat([df_train['SalePrice'], df_train[var]], axis=1)\nf, ax = plt.subplots(figsize=(16, 8))\nfig = sns.boxplot(x=var, y=\"SalePrice\", data=data, palette=sns.color_palette(\"hls\", 8))\nfig.axis(ymin=0, ymax=800000)\nplt.xticks(rotation=90)","ace5c7bb":"# correlation matrix\ncorrmat = df_train.corr()\nf, ax = plt.subplots(figsize=(12, 9))\nsns.heatmap(corrmat, vmax=.8, square=True)\n\n# saleprice correlation matrix\nk = 10  # number of variables for heatmap\ncols = corrmat.nlargest(k, 'SalePrice')['SalePrice'].index\nprint(cols)\ncm = np.corrcoef(df_train[cols].values.T)\nsns.set(font_scale=1.25)\nhm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 8}, yticklabels=cols.values,\n                 xticklabels=cols.values)","4cc72f9b":"# Combine train and test set for preprocessing\nX_all = pd.concat((df_train, df_test)).reset_index(drop=True)\nX_all.drop(['SalePrice'], axis=1, inplace=True)\nprint(\"X_all size is : {}\".format(X_all.shape))","994f397b":"# Handle missing values (NA, NaN)\n# Check if any missing values\nX_all_na = (X_all.isnull().sum() \/ len(X_all)) * 100\nX_all_na = X_all_na.drop(X_all_na[X_all_na == 0].index).sort_values(ascending=False)[:30]\nmissing_data = pd.DataFrame({'Missing Ratio': X_all_na})\nprint(missing_data.head(10))","628ebc1f":"# Drop features\nX_all = X_all.drop(['PoolQC', \"MiscFeature\", \"Alley\", 'Fence', 'FireplaceQu'], axis=1)","1edcca4b":"# Fill features\nX_all[\"MasVnrType\"] = X_all[\"MasVnrType\"].fillna(\"None\")\nX_all[\"MasVnrArea\"] = X_all[\"MasVnrArea\"].fillna(0)\nX_all['MSZoning'] = X_all['MSZoning'].fillna(X_all['MSZoning'].mode()[0])\nX_all[\"Functional\"] = X_all[\"Functional\"].fillna(\"Typ\")\nX_all['Electrical'] = X_all['Electrical'].fillna(X_all['Electrical'].mode()[0])\nX_all['KitchenQual'] = X_all['KitchenQual'].fillna(X_all['KitchenQual'].mode()[0])\nX_all['Exterior1st'] = X_all['Exterior1st'].fillna(X_all['Exterior1st'].mode()[0])\nX_all['Exterior2nd'] = X_all['Exterior2nd'].fillna(X_all['Exterior2nd'].mode()[0])\nX_all['SaleType'] = X_all['SaleType'].fillna(X_all['SaleType'].mode()[0])\nX_all['MSSubClass'] = X_all['MSSubClass'].fillna(\"None\")\nX_all['Utilities'] = X_all['Utilities'].fillna('AllPub')\nX_all[\"LotFrontage\"] = X_all.groupby(\"Neighborhood\")[\"LotFrontage\"].transform(\n    lambda x: x.fillna(x.median()))\nfor col in ('GarageType', 'GarageFinish', 'GarageQual', 'GarageCond'):\n    X_all[col] = X_all[col].fillna('None')\nfor col in ('GarageYrBlt', 'GarageArea', 'GarageCars'):\n    X_all[col] = X_all[col].fillna(0)\nfor col in ('BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath'):\n    X_all[col] = X_all[col].fillna(0)\nfor col in ('BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2'):\n    X_all[col] = X_all[col].fillna('None')","958275b1":"# Tranform numeric feature to categorical \nX_all['MSSubClass'] = X_all['MSSubClass'].apply(str)","d9c8a9e6":"# Check remaining missing values if any\nX_all_na = (X_all.isnull().sum() \/ len(X_all)) * 100\nX_all_na = X_all_na.drop(X_all_na[X_all_na == 0].index).sort_values(ascending=False)\nmissing_data = pd.DataFrame({'Missing Ratio': X_all_na})\nprint(\"remaining missing value:\", missing_data)","2019b1cc":"# Deal with highly skewed features\nfrom scipy.stats import skew\n\nnumeric_feats = X_all.dtypes[X_all.dtypes != \"object\"].index\nskewed_feats = X_all[numeric_feats].apply(lambda x: skew(x.dropna())).sort_values(ascending=False)\nprint(\"\\nSkew in numerical features: \\n\")\nskewness = pd.DataFrame({'Skew': skewed_feats})\nprint(skewness)","4cf726df":"skewness = skewness[abs(skewness) > 1]\nprint(\"There are {} skewed numerical features to transform\".format(skewness.shape[0]))\n\n","21814e64":"skewed_features = skewness.index\nfor feat in skewed_features:\n    X_all[feat] = np.log1p((X_all[feat]))","8c37c6ad":"# Make categorical features dummies\nX_all = pd.get_dummies(X_all)","c12a252f":"# Create train and test set\nX_train = X_all[:df_train.shape[0]].values\nX_test = X_all[df_train.shape[0]:].values\ny_train = np.log1p(df_train['SalePrice'])\nprint(X_train.shape, y_train.shape)\n","1a48889f":"from sklearn.model_selection import KFold, cross_val_score\nkfolds = KFold(n_splits=10, shuffle=True, random_state=1)\n","021d0bff":"# Define model\nfrom sklearn.ensemble import GradientBoostingRegressor\ngbr = GradientBoostingRegressor(n_estimators=20000, learning_rate=0.02, max_depth=3, max_features='sqrt',\n                                min_samples_leaf=5, min_samples_split=5, loss='ls')","13e9064d":"# Define model hyper parameters for cross validation\ngbr_param_grid = {\n    \"n_estimators\": [40000, 60000, 80000],\n    \"learning_rate\": [0.01, 0.02],\n    \"max_depth\": [3],\n    \"max_features\": [\"sqrt\"],\n    \"min_samples_leaf\": [5],\n    \"min_samples_split\": [5],\n    \"loss\": [\"ls\", 'huber']\n}","b49381a0":"from sklearn.model_selection import  GridSearchCV\ndef grid_search(model, param_grid):\n    grid_search = GridSearchCV(model, param_grid=param_grid, cv=kfolds, scoring='r2', verbose=0, n_jobs=-1)\n    grid_search.fit(X_train, y_train)\n    print(\"Best: {0:.4f} using {1}\".format(grid_search.best_score_, grid_search.best_params_))\n\n    return grid_search","4221e7bc":"grid_search(gbr, gbr_param_grid)","d3d98fa4":"# Prediction\n\n# Use the below parameters grid:\n#              param_grid={'learning_rate': [0.02, 0.01], 'loss': ['ls', 'huber'],\n#                          'max_depth': [3], 'max_features': ['sqrt'],\n#                          'min_samples_leaf': [5], 'min_samples_split': [5],\n#                          'n_estimators': [40000, 60000, 80000]},\n    \n# model_final = grid_search(knr, knr_param_grid)\n# y_pred = np.expm1(np.expm1(model_final.predict(X_test)))\n# y_test = pd.DataFrame()\n# y_test['Id'] = test_id.values\n# y_test['SalePrice'] = y_pred\n# y_test.to_csv('submission.csv',index=False)","2502b4f1":"Obviously, sale prices do not conform to normal distribution; they are skewed.\n\nLet's plot other features paired with sale price to see if they are correlated. ","ea63e0fc":"# 0. Project Introduction\n\nThis is a data analysis project using Python and several regression models.\nThe whole process can be divided into three parts:\n\n1. Data inspection and plotting\n2. Data preprocessing and feature engineering\n3. Modelling and making predictions (including cross validation and hyper parameter tuning)\n","3053f864":"# 1. Data inspection and plotting\n\nFirst, let's load the data and take a look at the data distribution and correlation.\n","fd61a2bd":"The top 5 features have at least half of their data missing. To make our model more robust, we can remove them from our dataset.","a4ebd78d":"We may not get a good grasp of the distribution of Sale Price column. So it is a good idea to plot a histogram.","22406799":"After successfully dealing with missing data, we now spend some time processing the skewed features`","18596175":"We can see that there are multiple features (Alley, PoolArea, etc) containing invalid data that can not serve as model inputs. These NaNs and NAs will be dealed with later. ","289be48c":"# 3. Modelling and making predictions","172896e9":"This log1p transformation can be replaced by Box Cox transforming. However, after some testing on lambda value and comparing these two methods on my local computer, I do not see any apparent discrepancy between them. So, in this notebook, I choose the simpler and more intuitive way of log1p transformation. ","e9514740":"It is common sense that newer houses sell at a premium compared to those with some ages, which is evidenced by the graph. ","e07421d0":"# 2. Data preprocessing and feature engineering","c0771363":"We create a 10x10 correlation matrix which shows the top 10 factors that are positively related to final sale price.All of these features are variables with regard to quality ranking, size of area, and age of the property, which is consistent with our life experience. \n\n# Important findings in Part 1:\n1. Some features in the dataset have missing and\/or invalid data. A feature engineering should be implemented to correct this.\n2. Sale price, the most important feature as well as our prediction target, is negatively skewed (i.e. there are more low prices than high ones). ","49daf57c":"Overall quality ranking has a strong positive correlation with sale price. ","b04e3faf":"After doing cross validation to find the best parameter set with highest accuracy (and lowest CV error on the test set), we can predict the outcome of the test set and save the results to csv file. "}}