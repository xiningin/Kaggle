{"cell_type":{"7dc0c825":"code","b20f968e":"code","9a25865c":"code","87a5d73e":"code","c22ea026":"code","5c172a42":"code","77133617":"code","bd71cafc":"code","35343e93":"code","f6b0e183":"code","ca017dd2":"code","7a431c0c":"code","7345547c":"code","692ca7fe":"code","73ff9cc7":"code","92b13662":"code","621b4339":"code","5b8f28c4":"code","e7175d0f":"code","cba0b3b3":"markdown","fb0eec57":"markdown","ddcea97b":"markdown","be8c137c":"markdown","24d9c29c":"markdown","5194fe57":"markdown","e41070bd":"markdown","f4a69dcc":"markdown","d18683f2":"markdown","ea4a2086":"markdown","669314f5":"markdown","784da190":"markdown","418bf9bb":"markdown","5ec5dcd6":"markdown","337139b2":"markdown"},"source":{"7dc0c825":"import numpy as np\nimport pandas as pd\nimport random\nimport sys\nimport os\n\nos.listdir('..\/input')","b20f968e":"# Read the entire file containing song lyrics\npath = \"..\/input\/songdata.csv\"\ndf = pd.read_csv(path)\ndf.head()","9a25865c":"# See all artist in this dataset\ndf['artist'].unique()","87a5d73e":"DP = df[df['artist']=='Deep Purple']","c22ea026":"DP.head()","5c172a42":"DP_text = DP['text'].str.cat(sep='\\n').lower()\n\nprint(DP_text[:100])\nprint('corpus length:', len(DP_text))","77133617":"chars = sorted(list(set(DP_text)))\nprint(chars)\nprint('total chars:', len(chars))","bd71cafc":"# Create a dictionary of characters, see the index of characters.\nchar_to_int = dict((c, i) for i, c in enumerate(chars))\nint_to_char = dict((i, c) for i, c in enumerate(chars))\n\nprint(char_to_int)","35343e93":"seq_length = 50 # The sentence window size\nstep = 1 # The steps between the windows\nsentences = []\nnext_chars = []\n\n# Create Target and sentences window\nfor i in range(0, len(DP_text) - seq_length, step):\n    sentences.append(DP_text[i: i + seq_length]) # range from current index to sequence length charaters \n    next_chars.append(DP_text[i + seq_length]) # the next character\n    \nsentences = np.array(sentences)\nnext_chars = np.array(next_chars)\n\n#Print Sentence Window and next charaters\nprint('Sentence Window')\nprint (sentences[:5])\nprint('Target charaters')\nprint (next_chars[:5])\nprint('Number of sequences:', len(sentences))","f6b0e183":"def getdata(sentences, next_chars):\n    X = np.zeros((len(sentences),seq_length))\n    y = np.zeros((len(sentences)))\n    length = len(sentences)\n    index = 0\n    for i in range(len(sentences)):\n        sentence = sentences[i]\n        for t, char in enumerate(sentence):\n            X[i, t] = char_to_int[char]\n        y[i] = char_to_int[next_chars[i]]\n    return X, y","ca017dd2":"train_x,train_y = getdata(sentences, next_chars)\nprint('Shape of training_x:', train_x.shape)\nprint('Shape of training_y:', train_y.shape)","7a431c0c":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.autograd import Variable\n\nclass Simple_LSTM(nn.Module):\n    def __init__(self,n_vocab,hidden_dim, embedding_dim,dropout = 0.2):\n        super(Simple_LSTM, self).__init__()\n        \n        self.hidden_dim = hidden_dim\n        self.embedding_dim = embedding_dim\n        self.lstm = nn.LSTM(embedding_dim, hidden_dim,dropout = dropout,num_layers = 2)\n        self.embeddings = nn.Embedding(n_vocab, embedding_dim)\n        self.fc = nn.Linear(hidden_dim, n_vocab)\n    \n    def forward(self, seq_in):\n        # for LSTM, input should be (Sequnce_length,batchsize,hidden_layer), so we need to transpose the input\n        embedded = self.embeddings(seq_in.t()) \n        lstm_out, _ = self.lstm(embedded)\n        # Only need to keep the last character \n        ht=lstm_out[-1] \n        out = self.fc(ht)\n        return out","7345547c":"X_train_tensor = torch.tensor(train_x, dtype=torch.long).cuda()\nY_train_tensor = torch.tensor(train_y, dtype=torch.long).cuda()","692ca7fe":"from torch.utils.data import Dataset, DataLoader\ntrain = torch.utils.data.TensorDataset(X_train_tensor,Y_train_tensor)\ntrain_loader = torch.utils.data.DataLoader(train, batch_size = 128)","73ff9cc7":"model = Simple_LSTM(47,256,256)\nmodel.cuda()\noptimizer = torch.optim.Adam(model.parameters(), lr = 0.002) # Using Adam optimizer","92b13662":"import time # Add time counter\navg_losses_f = []\nn_epochs=20\n\nfor epoch in range(n_epochs):\n    start_time = time.time()\n    model.train()\n    loss_fn = torch.nn.CrossEntropyLoss()\n    avg_loss = 0.\n    for i, (x_batch, y_batch) in enumerate(train_loader):\n        y_pred = model(x_batch)\n        \n        loss = loss_fn(y_pred, y_batch)\n        optimizer.zero_grad()\n        loss.backward()\n        \n        optimizer.step()\n        avg_loss+= loss.item()\/len(train_loader)\n        \n    elapsed_time = time.time() - start_time \n    print('Epoch {}\/{} \\t loss={:.4f} \\t time={:.2f}s'.format(\n        epoch + 1, n_epochs, avg_loss, elapsed_time))\n    \n    avg_losses_f.append(avg_loss)    \n    \nprint('All \\t loss={:.4f} \\t '.format(np.average(avg_losses_f)))","621b4339":"import matplotlib.pyplot as plt\n\nplt.plot(avg_losses_f)\nplt.xlabel('Epoch')\nplt.ylabel('Loss value')\nplt.show()","5b8f28c4":"def sample(preds, temperature=1.0):\n    preds = np.asarray(preds).astype('float64')\n    preds = np.log(preds) \/ temperature\n    exp_preds = np.exp(preds)\n    preds = exp_preds \/ np.sum(exp_preds)\n    probas = np.random.multinomial(1, preds, 1)\n    return np.argmax(probas)","e7175d0f":"# Define the start sentence\nsentence = 'i read in the news\\nthat the average man\\nplease kis'\n\nvariance = 0.25\ngenerated = ''\noriginal = sentence\nwindow = sentence\n\nfor i in range(400):\n    x = np.zeros((1, seq_length))\n    for t, char in enumerate(window):\n        x[0, t] = char_to_int[char] # Change the sentence to index vector shape (1,50)\n        \n    x_in = Variable(torch.LongTensor(x).cuda())\n    pred = model(x_in)\n    pred = np.array(F.softmax(pred, dim=1).data[0].cpu())\n    next_index = sample(pred, variance)\n    next_char = int_to_char[next_index] # index to char\n\n    generated += next_char\n    window = window[1:] + next_char # Update Window for next char predict\n    \nprint(original + generated)","cba0b3b3":"## Load and process data","fb0eec57":"## Start training\n* Hidden_size : 256\n* Embedding_size : 256\n* Use Adam optimizer","ddcea97b":"## Create DataLoader of mini-batch training\n* use GPU to training","be8c137c":"## Let's see training procedure by plot trend of loss value","24d9c29c":"## Tranfer the character to index\n* For pytorch, we don't need to use one-hot-vector","5194fe57":"## Conclusion\n* Some sentence are not too meaningful, but looks words are correct, maybe we can do more process on data, or ensemble more differnt model...\n* I am very curious, DL for CV, have conditional GAN, how about NLP? Can we add condition to LSTM for training?\n* For example, In this dataset, we have differnt artist lyrics, could we give the artist condition, then, we can generate the artist style lyric by condition that we give, That will be very interesting!!","e41070bd":"# Let's use the Deep Purple lysics to train a LSTM simulated Deep Purple write lyrics automatically (Rock!!)","f4a69dcc":"## Creates the sentence window and target characters\n* Create dictionary of characters\n* Create the sentence window\n* Target is the next character of sentence window\n\n","d18683f2":"### Let's focus on Deep Purple lyrics !!","ea4a2086":"## Create the function that can sample an index from a probability array\n* This function is to prevent the most likely chracter always be chosen.","669314f5":"* Count the characters appered in all lyrics","784da190":"## Validate the model\n* Define the 50 start sentence legth\n* Predict next char\n* Total create 400 characters lyrics","418bf9bb":"## This is simle LSTM model for text generator, for beginning of Pytorch.\n\n* Model : Embedding layer (no pretrain) + LSTM (GPU on)\n* Enviroment  : Pytorch \n* This kernel is for beginning of pytorch\n\n## Using Sergey Kuznetsov's dataset - \"55000+ Song Lyrics\" : https:\/\/www.kaggle.com\/mousehead\/songlyrics","5ec5dcd6":"## Process the data\n* Only keep the lyrics , merge all raws.","337139b2":"## Build the model (Pytorch)\n* Embedding layer : transfer index to embedding vector\n* Simple LSTM + dropout : Sequence data to hidden states , dropout for prevent overfitting\n* Fully connection layer : linear tranfer to a n_vocab vector to be output layer.\n\n### P.S we don't need to do softmax here, we will do it when we calculate loss function "}}