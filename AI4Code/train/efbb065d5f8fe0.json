{"cell_type":{"7cd0e048":"code","6effa201":"code","228500f2":"code","292edaf8":"code","dd486326":"code","495668e6":"code","8a5e0e3d":"code","468c8eb6":"code","6a46ca98":"code","ff6ba887":"code","9647b1a7":"code","0c148542":"code","4588c30c":"code","872a029a":"code","d3713f80":"code","944db0d7":"code","d075ebd7":"code","1f0fd669":"code","312b8e4a":"code","1f0b7020":"code","40e74116":"code","d9dbf911":"code","9d7560eb":"code","6d064a72":"code","0f7bb678":"code","b4ffabcf":"code","2dfa0a45":"code","2ff3815c":"code","53395515":"code","ddb10a9c":"code","bb906e08":"code","a110de9d":"code","20077878":"code","152bc7ee":"code","372dfe95":"markdown","ebfbb925":"markdown","520948e7":"markdown","831d7ac7":"markdown","6c4c92a1":"markdown","760814b0":"markdown","07561c50":"markdown","a9cca304":"markdown","cd54d7a7":"markdown","fc4805a0":"markdown","d27cabe8":"markdown","433bb1ba":"markdown","99db47e7":"markdown"},"source":{"7cd0e048":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","6effa201":"train_identity = pd.read_csv('\/kaggle\/input\/ieee-fraud-detection\/train_identity.csv')\ntrain_transaction = pd.read_csv('\/kaggle\/input\/ieee-fraud-detection\/train_transaction.csv')\ntest_identity = pd.read_csv('\/kaggle\/input\/ieee-fraud-detection\/test_identity.csv')\ntest_transaction = pd.read_csv('\/kaggle\/input\/ieee-fraud-detection\/test_transaction.csv')","228500f2":"train_dataset = pd.merge(train_transaction, train_identity, on='TransactionID', how='left')\ntest_dataset = pd.merge(test_transaction, test_identity, on='TransactionID', how='left')","292edaf8":"del train_identity\ndel train_transaction\ndel test_identity\ndel test_transaction","dd486326":"TransactionID = test_dataset['TransactionID']","495668e6":"test_dataset.drop(['TransactionID'],axis=1,inplace=True)","8a5e0e3d":"train_dataset.drop(['TransactionID'],axis=1,inplace=True)","468c8eb6":"y = train_dataset['isFraud']","6a46ca98":"train_dataset.drop(['isFraud'],axis=1,inplace=True)","ff6ba887":"# by https:\/\/www.kaggle.com\/dimartinot\n\ndef clean_inf_nan(df):\n    return df.replace([np.inf, -np.inf], np.nan)   \n\n# Cleaning infinite values to NaN\ntrain_dataset = clean_inf_nan(train_dataset)\ntest_dataset = clean_inf_nan(test_dataset)","9647b1a7":"list_of_num = []\nlist_of_obj = []\nfor i in train_dataset.columns :\n    if train_dataset[i].dtypes == 'object':\n        list_of_obj.append(i)\n    else:\n        list_of_num.append(i)","0c148542":"for i in list_of_num :\n    train_dataset[i] = train_dataset[i].fillna(0)\n    test_dataset[i] = test_dataset[i].fillna(0)","4588c30c":"for i in list_of_obj :\n    temp = str('no_'+i)\n    train_dataset[i] = train_dataset[i].fillna(temp)\n    test_dataset[i] = test_dataset[i].fillna(temp)","872a029a":"def minify_identity_df(df):\n    df['M1']  = df['M1'].map({'T':2, 'F':1, 'no_M1':0})\n    df['M2']  = df['M2'].map({'T':2, 'F':1, 'no_M2':0})\n    df['M3']  = df['M3'].map({'T':2, 'F':1, 'no_M3':0})\n    df['M5']  = df['M5'].map({'T':2, 'F':1, 'no_M5':0})\n    df['M6']  = df['M6'].map({'T':2, 'F':1, 'no_M6':0})\n    df['M7']  = df['M7'].map({'T':2, 'F':1, 'no_M7':0})\n    df['M8']  = df['M8'].map({'T':2, 'F':1, 'no_M8':0})\n    df['M9']  = df['M9'].map({'T':2, 'F':1, 'no_M9':0})\n    df['id_12'] = df['id_12'].map({'Found':1, 'NotFound':2, 'no_id_12':0})\n    df['id_15'] = df['id_15'].map({'New':3, 'Found':2, 'Unknown':1, 'no_id_15':0})\n    df['id_16'] = df['id_16'].map({'Found':2, 'NotFound':1, 'no_id_16':0})\n    df['id_23'] = df['id_23'].map({'IP_PROXY:TRANSPARENT':3, 'no_id_23':0,\n                                   'IP_PROXY:ANONYMOUS':2, 'IP_PROXY:HIDDEN':1})\n    df['id_27'] = df['id_27'].map({'Found':1, 'NotFound':2, 'no_id_27':0})\n    df['id_28'] = df['id_28'].map({'New':2, 'Found':1, 'no_id_28':0})\n    df['id_29'] = df['id_29'].map({'Found':1, 'NotFound':2, 'no_id_29':0})\n    df['id_35'] = df['id_35'].map({'T':1, 'F':2, 'no_id_35':0})\n    df['id_36'] = df['id_36'].map({'T':1, 'F':2, 'no_id_36':0})\n    df['id_37'] = df['id_37'].map({'T':1, 'F':2, 'no_id_37':0})\n    df['id_38'] = df['id_38'].map({'T':1, 'F':2, 'no_id_38':0})\n    df['DeviceType'].map({'desktop':2, 'mobile':1, 'no_DeviceType':0})\n    return df","d3713f80":"#https:\/\/www.kaggle.com\/kyakovlev\/ieee-internal-blend\ndef reduce_mem_usage(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.        \n    \"\"\"\n    start_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        else:\n            df[col] = df[col].astype('category')\n\n    end_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) \/ start_mem))\n    \n    return df","944db0d7":"train_dataset =  minify_identity_df(train_dataset)\ntest_dataset = minify_identity_df(test_dataset)","d075ebd7":"train_dataset =  reduce_mem_usage(train_dataset)\ntest_dataset = reduce_mem_usage(test_dataset)","1f0fd669":"def convert_id31(val):\n    try:\n        val = val.lower()\n        x = val.split()\n        if 'chrome' in x:\n            return 'chrome'\n        elif 'safari' in x:\n            return 'safari'\n        elif 'ie' in x:\n            return 'ie'\n        elif 'edge' in x:\n            return 'edge'\n        elif 'firefox' in x:\n            return 'firefox'\n        elif 'samsung' in x:\n            return 'samsung'\n        else:\n            return 'no_id_31'\n    except:\n        return 'no_id_31'","312b8e4a":"train_dataset['id_31'] = train_dataset['id_31'].apply(convert_id31)\ntest_dataset['id_31'] = test_dataset['id_31'].apply(convert_id31)","1f0b7020":"def convert_id30(val):\n            try:\n                val = val.lower()\n                x = val.split()[0]\n                if x == 'ios':\n                    return 'ios'\n                elif x == 'android':\n                    return 'android'\n                elif x == 'mac':\n                    return 'mac'\n                elif x == 'windows':\n                    return 'windows'\n                elif x == 'linux':\n                    return 'linux'\n                else:\n                    return 'no_id_30'\n            except:\n                return 'no_id_30'","40e74116":"train_dataset['id_30'] = train_dataset['id_30'].apply(convert_id30)\ntest_dataset['id_30'] = test_dataset['id_30'].apply(convert_id30)","d9dbf911":"for X in [train_dataset,test_dataset]:\n    X['device_name'] = X['DeviceInfo'].str.split('\/', expand=True)[0]\n\n    X.loc[X['device_name'].str.contains('SM', na=False), 'device_name'] = 'Samsung'\n    X.loc[X['device_name'].str.contains('SAMSUNG', na=False), 'device_name'] = 'Samsung'\n    X.loc[X['device_name'].str.contains('GT-', na=False), 'device_name'] = 'Samsung'\n    X.loc[X['device_name'].str.contains('Moto G', na=False), 'device_name'] = 'Motorola'\n    X.loc[X['device_name'].str.contains('Moto', na=False), 'device_name'] = 'Motorola'\n    X.loc[X['device_name'].str.contains('moto', na=False), 'device_name'] = 'Motorola'\n    X.loc[X['device_name'].str.contains('LG-', na=False), 'device_name'] = 'LG'\n    X.loc[X['device_name'].str.contains('rv:', na=False), 'device_name'] = 'RV'\n    X.loc[X['device_name'].str.contains('HUAWEI', na=False), 'device_name'] = 'Huawei'\n    X.loc[X['device_name'].str.contains('ALE-', na=False), 'device_name'] = 'Huawei'\n    X.loc[X['device_name'].str.contains('-L', na=False), 'device_name'] = 'Huawei'\n    X.loc[X['device_name'].str.contains('Blade', na=False), 'device_name'] = 'ZTE'\n    X.loc[X['device_name'].str.contains('BLADE', na=False), 'device_name'] = 'ZTE'\n    X.loc[X['device_name'].str.contains('Linux', na=False), 'device_name'] = 'Linux'\n    X.loc[X['device_name'].str.contains('XT', na=False), 'device_name'] = 'Sony'\n    X.loc[X['device_name'].str.contains('HTC', na=False), 'device_name'] = 'HTC'\n    X.loc[X['device_name'].str.contains('ASUS', na=False), 'device_name'] = 'Asus'\n    X.loc[X.device_name.isin(X.device_name.value_counts()[X.device_name.value_counts() < 200].index), 'device_name'] = \"Others\"\n    X.drop(['DeviceInfo'],axis=1,inplace=True)","9d7560eb":"temp = train_dataset['P_emaildomain'].str.split('.', expand=True)\ntrain_dataset['P_emailF'] = temp[0]\ntrain_dataset['P_emailL'] = temp[1]\ntemp = train_dataset['R_emaildomain'].str.split('.', expand=True)\ntrain_dataset['R_emailF'] = temp[0]\ntrain_dataset['R_emailL'] = temp[1]\ntemp = test_dataset['P_emaildomain'].str.split('.', expand=True)\ntest_dataset['P_emailF'] = temp[0]\ntest_dataset['P_emailL'] = temp[1]\ntemp = test_dataset['R_emaildomain'].str.split('.', expand=True)\ntest_dataset['R_emailF'] = temp[0]\ntest_dataset['R_emailL'] = temp[1]\ndel temp\ndel train_dataset['P_emaildomain']\ndel test_dataset['P_emaildomain']\ndel train_dataset['R_emaildomain']\ndel test_dataset['R_emaildomain']","6d064a72":"train_dataset['P_emailF'] = train_dataset['P_emailF'].fillna('no_P_F')\ntrain_dataset['P_emailL'] = train_dataset['P_emailL'].fillna('no_P_L')\ntrain_dataset['R_emailF'] = train_dataset['R_emailF'].fillna('no_R_F')\ntrain_dataset['R_emailL'] = train_dataset['R_emailL'].fillna('no_R_L')\n\ntest_dataset['P_emailF'] = test_dataset['P_emailF'].fillna('no_P_F')\ntest_dataset['P_emailL'] = test_dataset['P_emailL'].fillna('no_P_L')\ntest_dataset['R_emailF'] = test_dataset['R_emailF'].fillna('no_R_F')\ntest_dataset['R_emailL'] = test_dataset['R_emailL'].fillna('no_R_L')","0f7bb678":"from sklearn.preprocessing import LabelEncoder","b4ffabcf":"for i in train_dataset.select_dtypes(include=['category', 'object']).columns.values:\n        encoder = LabelEncoder()\n        encoder.fit(list(train_dataset[i].values) + list(test_dataset[i].values))\n        train_dataset[i] = encoder.transform(list(train_dataset[i].values))\n        test_dataset[i] = encoder.transform(list(test_dataset[i].values))","2dfa0a45":"import datetime\nSTART_DATE = '1800-01-01'\nstartdate = datetime.datetime.strptime(START_DATE, \"%Y-%m-%d\")","2ff3815c":"# code from https:\/\/www.kaggle.com\/kimchiwoong\/simple-eda-ensemble-for-xgboost-and-lgbm\ntrain_dataset[\"Date\"] = train_dataset['TransactionDT'].apply(lambda x: (startdate + datetime.timedelta(seconds=x)))\ntrain_dataset['TransactionDT_Weekdays'] = train_dataset['Date'].dt.dayofweek\ntrain_dataset['TransactionDT_Days'] = train_dataset['Date'].dt.day\ntrain_dataset['TransactionDT_Hours'] = train_dataset['Date'].dt.hour\ntrain_dataset.drop(columns='Date', inplace=True)\n\ntest_dataset[\"Date\"] = test_dataset['TransactionDT'].apply(lambda x: (startdate + datetime.timedelta(seconds=x)))\ntest_dataset['TransactionDT_Weekdays'] = test_dataset['Date'].dt.dayofweek\ntest_dataset['TransactionDT_Days'] = test_dataset['Date'].dt.day\ntest_dataset['TransactionDT_Hours'] = test_dataset['Date'].dt.hour\ntest_dataset.drop(columns='Date', inplace=True)\ntrain_dataset.drop(['TransactionDT'],axis=1,inplace=True)\ntest_dataset.drop(['TransactionDT'],axis=1,inplace=True)","53395515":"def quasi_constant(data):\n    quasi = [col for col in train_dataset.columns if train_dataset[col].value_counts(dropna=False, normalize=True).values[0] > 0.9]\n    return quasi","ddb10a9c":"quasi_features = quasi_constant(train_dataset)","bb906e08":"train_dataset.drop(quasi_features,axis=1,inplace=True)\ntest_dataset.drop(quasi_features,axis=1,inplace=True)","a110de9d":"train_dataset =  reduce_mem_usage(train_dataset)\ntest_dataset = reduce_mem_usage(test_dataset)","20077878":"train_dataset['isFraud'] = y","152bc7ee":"train_dataset.to_pickle('Train.pkl')\ntest_dataset.to_pickle('Test.pkl')","372dfe95":"**Handling column 'id_31'**","ebfbb925":"**Handling column 'id_30'**","520948e7":"**Looking for quasi-constant features**","831d7ac7":"**Removing infinite values**","6c4c92a1":"# Feature Engineering","760814b0":"**Handling columns 'P_emaildomain' and 'R_emaildomain'**","07561c50":"# Data Imports","a9cca304":"**Fixing TransactionDT column**","cd54d7a7":"**Label encoding for  object type column**","fc4805a0":"**Saving our converted data as pickle format, so that we can use it to train model and find best parameters**","d27cabe8":"# Data minify and Reduce mem usage","433bb1ba":"**Handling column 'DeviceInfo'**","99db47e7":"# Filling missing values"}}