{"cell_type":{"6da7a0b2":"code","031c2413":"code","aae48632":"code","79708ba1":"code","12615c87":"code","dc9bcc9b":"code","c046d023":"code","cbadcd48":"code","25a60a51":"code","c55b6f08":"code","0c26155f":"code","7832f78f":"code","85784863":"code","f0ad1a25":"code","237f95b5":"code","11f7a2cd":"code","17a70b98":"code","52b4bd6e":"code","358298e9":"code","dc0c272e":"code","974f708b":"code","23ce5cbb":"code","94fe234c":"code","49b3643a":"code","75622665":"code","c7faf8d4":"code","107e4860":"code","c76679e7":"code","44cb3c8b":"code","c79782c6":"code","3f5a5606":"code","46fac355":"code","c363a980":"code","9fea211f":"markdown","0175a40c":"markdown","08d82e2d":"markdown","85403e91":"markdown","d0f227f5":"markdown","b9c82f3b":"markdown","2cc17a27":"markdown","1a520a78":"markdown","c7605721":"markdown","7259c83b":"markdown","53aa7796":"markdown","660ab7c6":"markdown","e3bb010d":"markdown","fafbe92d":"markdown"},"source":{"6da7a0b2":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom collections import Counter\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import confusion_matrix  \nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score,GridSearchCV\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.\nsns.set(style='white', context='notebook', palette='deep')","031c2413":"train=pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ntest=pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")","aae48632":"import pandas_profiling\ntrain.profile_report()","79708ba1":"import pandas_profiling\ntest.profile_report()","12615c87":"train.describe().plot(kind=\"area\",fontsize=16,figsize=(20,8),grid=True,colormap=\"rainbow\")\nplt.xlabel(\"Statistics\")\nplt.ylabel(\"Values\")\nplt.title(\"General Statistics of Titanic Dataset\")\nplt.show()\nplt.savefig('graph1.png')\ntrain.describe()","dc9bcc9b":"sns.pairplot(train,hue='Survived')\nplt.show()","c046d023":"category1=['Pclass','Sex','SibSp','Parch','Embarked']\n\nfor c in category1:\n    plt.figure(figsize=(10,5))\n    sns.countplot(x=c,hue='Survived',data=train)\n    plt.show()","cbadcd48":"train[['Pclass','Survived']].groupby(['Pclass'],as_index=False).mean().sort_values(by='Survived',ascending=False)","25a60a51":"train[['Sex','Survived']].groupby(['Sex'],as_index=False).mean().sort_values(by='Survived',ascending=False)","c55b6f08":"train[['SibSp','Survived']].groupby(['SibSp'],as_index=False).mean().sort_values(by='Survived',ascending=False)","0c26155f":"sns.factorplot(x='SibSp',y='Survived',data=train,kind='bar',size=6)\nplt.show()\nplt.savefig('graph3.png')","7832f78f":"train[['Parch','Survived']].groupby(['Parch'],as_index=False).mean().sort_values(by='Survived',ascending=False)","85784863":"train[['Embarked','Survived']].groupby(['Embarked'],as_index=False).mean().sort_values(by='Survived',ascending=False)","f0ad1a25":"train.boxplot(column='Fare',by='Embarked')\nplt.show()","237f95b5":"g=sns.FacetGrid(train,col='Survived')\ng.map(sns.distplot,'Age',bins=25)\nplt.show()","11f7a2cd":"g=sns.FacetGrid(train,col='Survived',row='Pclass',size=2)\ng.map(plt.hist,'Age',bins=25)\ng.add_legend()\nplt.show()","17a70b98":"g=sns.FacetGrid(train,row='Embarked',size=5)\ng.map(sns.pointplot,'Pclass','Survived','Sex')\ng.add_legend()\nplt.show()\nplt.savefig('graph4.png')","52b4bd6e":"g=sns.FacetGrid(train,col='Survived',row='Embarked',size=3)\ng.map(sns.barplot,'Sex','Fare')\ng.add_legend()\nplt.show()","358298e9":"# convert Sex into categorical value 0 for male and 1 for female\ntrain[\"Sex\"] = train[\"Sex\"].map({\"male\": 0, \"female\":1})\ntest[\"Sex\"] = test[\"Sex\"].map({\"male\": 0, \"female\":1})\n\n#g = sns.heatmap(train[[\"Survived\",\"SibSp\",\"Parch\",\"Age\",\"Fare\",\"Pclass\"]].corr(),annot=True, fmt = \".2f\", cmap = \"coolwarm\")\nfig=plt.gcf()\nfig.set_size_inches(10,7)\nfig=sns.heatmap(train.corr(),annot=True,cmap='cubehelix',linewidths=1,linecolor='k',square=True,mask=False, vmin=-1, vmax=1,cbar_kws={\"orientation\": \"vertical\"},cbar=True)\nplt.savefig('graph2.png')","dc0c272e":"dataset =  pd.concat(objs=[train, test], axis=0,sort=False).reset_index(drop=True)","974f708b":"# Drop useless variables \n#dataset.drop(labels = [\"PassengerId\"], axis = 1, inplace = True)\n\n# \"S\" is most frequent value of \"Embarked\"\ndataset[\"Embarked\"] = dataset[\"Embarked\"].fillna(\"S\")\n\ndataset['Fare']=dataset['Fare'].fillna(np.mean(dataset[dataset['Pclass']==3]['Fare']))\n\n\n## Fill Age with the median age of similar rows according to Pclass, Parch and SibSp\n# Index of NaN age rows\nindex_NaN_age = list(dataset[\"Age\"][dataset[\"Age\"].isnull()].index)\n\nfor i in index_NaN_age :\n    age_med = dataset[\"Age\"].median()\n    age_pred = dataset[\"Age\"][((dataset['SibSp'] == dataset.iloc[i][\"SibSp\"]) & (dataset['Parch'] == dataset.iloc[i][\"Parch\"]) & (dataset['Pclass'] == dataset.iloc[i][\"Pclass\"]))].median()\n    if not np.isnan(age_pred) :\n        dataset['Age'].iloc[i] = age_pred\n    else :\n        dataset['Age'].iloc[i] = age_med","23ce5cbb":"# convert to indicator values Embarked \n\ndataset = pd.get_dummies(dataset, columns = [\"Embarked\"], prefix=\"Em\",drop_first=True)","94fe234c":"# Replace the Cabin number by the type of cabin 'X' if not\ndataset[\"Cabin\"] = pd.Series([i[0] if not pd.isnull(i) else 'X' for i in dataset['Cabin'] ])\ndataset = pd.get_dummies(dataset, columns = [\"Cabin\"],prefix=\"Cabin\")","49b3643a":"# Create categorical values for Pclass\ndataset[\"Pclass\"] = dataset[\"Pclass\"].astype(\"category\")\ndataset = pd.get_dummies(dataset, columns = [\"Pclass\"],prefix=\"Pc\")","75622665":"# Create a family size descriptor from SibSp and Parch\ndataset[\"Fsize\"] = dataset[\"SibSp\"] + dataset[\"Parch\"] + 1\n","c7faf8d4":"dataset_title = [i.split(\",\")[1].split(\".\")[0].strip() for i in dataset[\"Name\"]]\ndataset[\"Title\"] = pd.Series(dataset_title)\ndataset[\"Title\"] = dataset[\"Title\"].replace(['Lady', 'the Countess','Countess','Capt', 'Col','Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\ndataset[\"Title\"] = dataset[\"Title\"].map({\"Master\":0, \"Miss\":1, \"Ms\" : 1 , \"Mme\":1, \"Mlle\":1, \"Mrs\":1, \"Mr\":2, \"Rare\":3})\ndataset[\"Title\"] = dataset[\"Title\"].astype(int)\ndataset = pd.get_dummies(dataset, columns = [\"Title\"])\ndataset.drop(labels = [\"Name\"], axis = 1, inplace = True)","107e4860":"# Group family size according to factorplot\ndataset['Single'] = dataset['Fsize'].map(lambda s: 1 if s == 1 else 0)\ndataset['SmallF'] = dataset['Fsize'].map(lambda s: 1 if  s == 2  else 0)\ndataset['MedF'] = dataset['Fsize'].map(lambda s: 1 if 3 <= s <= 4 else 0)\ndataset['LargeF'] = dataset['Fsize'].map(lambda s: 1 if s >= 5 else 0)","c76679e7":"# Tickets with same prefixes may have a similar class and survival.\n\nTicket = []\nfor i in list(dataset.Ticket):\n    if not i.isdigit() :\n        Ticket.append(i.replace(\".\",\"\").replace(\"\/\",\"\").strip().split(' ')[0]) #Take prefix\n    else:\n        Ticket.append(\"X\")\n        \ndataset[\"Ticket\"] = Ticket\ndataset = pd.get_dummies(dataset, columns = [\"Ticket\"], prefix=\"T\")","44cb3c8b":"train=dataset[dataset['Survived'].notnull()]\ntest=dataset[dataset['Survived'].isnull()]\ntest=test.drop(labels='Survived',axis=1)","c79782c6":"# Outlier detection \n\ndef detect_outliers(df,n,features):\n    \"\"\"\n    Takes a dataframe df of features and returns a list of the indices\n    corresponding to the observations containing more than n outliers according\n    to the Tukey method.\n    \"\"\"\n    outlier_indices = []\n    \n    # iterate over features(columns)\n    for col in features:\n        # 1st quartile (25%)\n        Q1 = np.percentile(df[col], 25)\n        # 3rd quartile (75%)\n        Q3 = np.percentile(df[col],75)\n        # Interquartile range (IQR)\n        IQR = Q3 - Q1\n        \n        # outlier step\n        outlier_step = 1.5 * IQR\n        \n        # Determine a list of indices of outliers for feature col\n        outlier_list_col = df[(df[col] < Q1 - outlier_step) | (df[col] > Q3 + outlier_step )].index\n        \n        # append the found outlier indices for col to the list of outlier indices \n        outlier_indices.extend(outlier_list_col)\n        \n    # select observations containing more than 2 outliers\n    outlier_indices = Counter(outlier_indices)        \n    multiple_outliers = list( k for k, v in outlier_indices.items() if v > n )\n    \n    return multiple_outliers   \n\n# detect outliers from Age, SibSp , Parch and Fare\nOutliers_to_drop = detect_outliers(train,2,[\"Age\",\"SibSp\",\"Parch\",\"Fare\"])\n\ntrain = train.drop(Outliers_to_drop, axis = 0).reset_index(drop=True)","3f5a5606":"\nx_train=train.drop(labels='Survived',axis=1)\n#x_train=(x_train-x_train.min())\/(x_train.max()-x_train.min())\ny_train=train['Survived'].astype('int')\nx_train.fillna(0,inplace=True)\n\nX_train,X_test,Y_train,Y_test = train_test_split(x_train,y_train,test_size=0.25, random_state=0)\n\nxgb=XGBClassifier(colsample_bytree=1.0,gamma=2.1,max_depth=5,min_child_weight=3,subsample=0.5,n_estimators=160,random_state=42)\nxgb.fit(X_train,Y_train)\nprint('Score: ',xgb.score(X_train,Y_train))\ny_pred=xgb.predict(X_test)\ny_true=Y_test\n\ncm=confusion_matrix(y_true,y_pred.round())\nprint(cm)\nfeature_importances = pd.DataFrame(xgb.feature_importances_,index = x_train.columns,columns=['importance']).sort_values('importance', ascending=False)\nfeature_importances","46fac355":"params = {\n        'min_child_weight': [1, 2, 3],\n        'gamma': [1.9, 2, 2.1, 2.2],\n        'subsample': [0.4,0.5,0.6],\n        'colsample_bytree': [0.6, 0.8, 1.0],\n        'max_depth': [3,4,5]\n        }\ngd_sr = GridSearchCV(estimator=XGBClassifier(),\n                     param_grid=params,\n                     scoring='accuracy',\n                     cv=5,\n                     n_jobs=1\n                     )\ngd_sr.fit(x_train, y_train)\nbest_parameters = gd_sr.best_params_\nbest_result = gd_sr.best_score_\n\nprint(\"Best result:\", best_result)\npd.DataFrame({\"Parameter\":[i for i in best_parameters.keys()],\n              \"Best Values\":[i for i in best_parameters.values()]})","c363a980":"xgb=XGBClassifier(colsample_bytree=0.8,gamma=2,max_depth=3,min_child_weight=3,subsample=0.6)\nxgb.fit(x_train,y_train)\ny_pred=xgb.predict(test)\n\nsubmission=pd.DataFrame(columns=['PassengerId','Survived'])\nsubmission['PassengerId']=test['PassengerId']\nsubmission['Survived']=y_pred.astype(int)\nsubmission.to_csv('submission.csv',header=True,index=False)","9fea211f":"![alt text](https:\/\/vignette.wikia.nocookie.net\/titanic\/images\/f\/f9\/Titanic_side_plan.png\/revision\/latest?cb=20180322183733)\n* On the Boat Deck there were **6** rooms labeled as **T, U, W, X, Y, Z** but only the **T** cabin is present in the dataset\n* **A**, **B** and **C** decks were only for 1st class passengers\n* **D** and **E** decks were for all classes\n* **F** and **G** decks were for both 2nd and 3rd class passengers\n* From going **A** to **G**, distance to the staircase increases which might be a factor of survival","0175a40c":"<a id='1'><\/a><br>\n# 1. General View","08d82e2d":"First seperate train and test","85403e91":"Combine train and test","d0f227f5":"   <font color='red'>\n   Categorical Variables","b9c82f3b":"<a id='5'><\/a><br>\n# 5. Outliers","2cc17a27":"- Only Fare feature seems to have a significative correlation with the survival probability.\n- According to Heatmap Age negatively correlated with Pclass, Parch and SibSp so fill Age with the median age of similar rows according to Pclass, Parch and SibSp.","1a520a78":"<font color='red'>\n    Numerical Variables","c7605721":"<a id='2'><\/a><br>\n# 2. General Explotary Data Analysis\nDescribe is the most important part of statistics. We should show these figures on charts","7259c83b":"<a id='6'><\/a><br>\n# 6. Modelling & Feature Importances","53aa7796":"# INTRODUCTION\n\nAim of this kernel is Titanic:Machine Learning from Disaster one of popular competition among beginners.\nThis is a beginner level kernel which focuses simple but effective analysis & codes. \nFirst of all we must understand the data. If you work on a data without EDA you can't be a succesfull data scientist.\nLook at the picture above and imagine.\n\n<font color='red'>\n      \n1. [General View](#1)  \n1. [Explotary Data Analysis](#2)\n1. [Missing Values & Transformations](#3)\n1. [Feature Engineering](#4)\n1. [Outliers](#5)\n1. [Modelling & Feature Importances](#6)\n\n\n     ","660ab7c6":"<a id='4'><\/a><br>\n# 4. Feature Engineering","e3bb010d":"# TITANIC DISASTER","fafbe92d":"<a id='3'><\/a><br>\n## 3. Missing Values & Transformations"}}