{"cell_type":{"756c8d5b":"code","5f321d06":"code","69497b0a":"code","cbbdfebe":"code","672f01a2":"code","a5577db4":"code","04d70238":"code","7f8be0cc":"code","3f384208":"code","3ff8df3f":"code","8bbb208f":"code","3da81468":"code","8be3a8b4":"code","0e5cf7e9":"code","5ef0e19d":"code","cec6d519":"code","46d09b5e":"code","ba7e4b6b":"code","1d4f8ea3":"code","096cb424":"code","e692dc5e":"code","5405821b":"code","c0fb229d":"code","d7f49d18":"code","afc7bcf0":"code","bc482131":"code","2ff9ba62":"code","7506ad9a":"code","925fd631":"code","53ad99fb":"code","608f091b":"code","b8c33755":"code","87415c09":"code","3df63d7f":"code","4f0a9820":"code","a2334477":"markdown","ef952818":"markdown","98629ac5":"markdown","228e66a8":"markdown","0057682a":"markdown","83336563":"markdown","e995f3b4":"markdown","1fe11b2b":"markdown","1316d4e8":"markdown","9659761f":"markdown","583e845b":"markdown","8e4f49f5":"markdown","15df19c5":"markdown","dd0e5d02":"markdown","01c922a3":"markdown","4d100779":"markdown","a100dfd9":"markdown","6a8ad051":"markdown","7353ef5a":"markdown","a6e0b7fb":"markdown","f77ffbba":"markdown","7297a643":"markdown","4dc296e2":"markdown","614b4ecd":"markdown","f7165d20":"markdown","d5ba5dfe":"markdown"},"source":{"756c8d5b":"#import some libraries\n#in the next step, we may have to import some other libraries for machine learning\n\nimport pandas as pd\nimport numpy as np\nimport warnings\nwarnings.filterwarnings('ignore')\n\n#some libraries that we use for plotting\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport missingno as msno\n\n\n#statistic libraries\nfrom scipy import stats\nfrom scipy.stats import norm, skew ","5f321d06":"train = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/train.csv\")\ntest = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/test.csv\")","69497b0a":"#this are 5 datas from train \ntrain.head()","cbbdfebe":"#this are 5 datas from test \ntest.head()","672f01a2":"#Desciptive Statistic for Sale Price column\ntrain['SalePrice'].describe()","a5577db4":"#Distribution plot\nsns.distplot(train['SalePrice'],fit = norm);\n(mu, sigma) = norm.fit(train['SalePrice'])\nplt.legend(['$\\mu=$ {:.2f} and $\\sigma=$ {:.2f}'.format(mu, sigma)],\n            loc='best')\nplt.title('Sale Price Distribution Plot');\n\n#QQ-plot\nfig = plt.figure()\nres = stats.probplot(train['SalePrice'], plot=plt)\nplt.show()","04d70238":"kurt = train['SalePrice'].kurtosis()\nskew = train['SalePrice'].skew()\nprint('Sale Price Kurtorsis {}'.format(kurt))\nprint('Sale Price Skewness {}'.format(skew))","7f8be0cc":"#transform SalePrice using log function\ntrain['SalePrice'] = np.log(train['SalePrice'])\n\n#Distribution plot\nsns.distplot(train['SalePrice'],fit=norm);\n(mu, sigma) = norm.fit(train['SalePrice'])\nplt.legend(['$\\mu=$ {:.2f} and $\\sigma=$ {:.2f}'.format(mu, sigma)],\n            loc='best')\nplt.title('Log Sale Price Distribution Plot');\n\n#QQ-plot\nfig = plt.figure()\nres = stats.probplot(train['SalePrice'], plot=plt)\nplt.show()","3f384208":"kurt = train['SalePrice'].kurtosis()\nskew = train['SalePrice'].skew()\nprint('Sale Price Kurtorsis {}'.format(kurt))\nprint('Sale Price Skewness {}'.format(skew))","3ff8df3f":"df = pd.concat([train,test],ignore_index=True) \n# joint data train and test so we can work more efficient\n\ncorrmat = train.corr()\nf, ax = plt.subplots(figsize=(6, 6))\nsns.heatmap(corrmat[corrmat>0.8], square=True);\nplt.title('Correlation Heatmap');","8bbb208f":"#Drop GarageArea, GarageYrBlt, 1stFlrSF, TotRmsAbvGrd\ndf.drop(['GarageArea','GarageYrBlt','TotRmsAbvGrd','1stFlrSF'],axis=1, inplace=True) ","3da81468":"#saleprice correlation matrix\nk = 8 #number of variables for heatmap\ncorrmat = train.drop(['GarageArea','GarageYrBlt','TotRmsAbvGrd','1stFlrSF'],axis=1).corr()\ncols = corrmat.nlargest(k, 'SalePrice')['SalePrice'].index\ncm = np.corrcoef(train[cols].values.T)\nsns.set(font_scale=1.25)\nf, ax = plt.subplots(figsize=(6, 6))\nhm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f',\n                 annot_kws={'size': 10}, yticklabels=cols.values, xticklabels=cols.values)\nplt.title('Sale Price Correlation Heatmap');\nplt.show()","8be3a8b4":"#missing data percentage\ntotal = df.isnull().sum().sort_values(ascending=False)\npercent = (df.isnull().sum()\/df.isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_data.head(19)","0e5cf7e9":"#Missing value plot\ncols_with_missing = [col for col in df.columns if df[col].isnull().any()]\ndata_with_miss = df[cols_with_missing]\nmsno.matrix(data_with_miss);","5ef0e19d":"#drop Alley\ndf.drop('Alley',axis =1, inplace=True)\n\n#drop PoolQC\ndf.drop('PoolQC',axis =1, inplace=True)\n\n#drop Fence\ndf.drop('Fence',axis =1, inplace=True)\n\n#drop MiscFeature\ndf.drop('MiscFeature',axis =1, inplace=True)\n\n#drop LotFrontage\ndf.drop('LotFrontage',axis =1, inplace=True)\n\n#drop FireplaceQu\ndf.drop('FireplaceQu',axis =1, inplace=True)\n","cec6d519":"#fill Garage\ndf['GarageType'].fillna('None', inplace=True)\ndf['GarageFinish'].fillna('None', inplace=True)\ndf['GarageQual'].fillna('None', inplace=True)\ndf['GarageCond'].fillna('None', inplace=True)\ndf['GarageCars'].fillna(0,inplace=True)\n\n#fill Basement\ndf['BsmtCond'].fillna(\"None\",inplace=True)\ndf['BsmtQual'].fillna(\"None\",inplace=True)\ndf['BsmtExposure'].fillna(\"No\",inplace=True)\ndf['BsmtFinType1'].fillna(\"None\",inplace=True)\ndf['BsmtFinType2'].fillna(\"None\",inplace=True)\ndf['BsmtFinSF1'].fillna(0,inplace=True)\ndf['BsmtFinSF2'].fillna(0,inplace=True)\ndf['BsmtUnfSF'].fillna(0,inplace=True)\ndf['TotalBsmtSF'].fillna(0,inplace=True)\ndf['BsmtFullBath'].fillna(0,inplace=True)\ndf['BsmtHalfBath'].fillna(0,inplace=True)\n\ndf['MasVnrArea'].fillna(df['MasVnrArea'].mode()[0],inplace=True)\ndf['MasVnrType'].fillna(df['MasVnrType'].mode()[0],inplace=True)\ndf['Electrical'].fillna(df['Electrical'].mode()[0],inplace=True)\ndf['MSZoning'].fillna(df['MSZoning'].mode()[0],inplace=True)\ndf['Exterior1st'].fillna(df['Exterior1st'].mode()[0],inplace=True)\ndf['Exterior2nd'].fillna(df['Exterior2nd'].mode()[0],inplace=True)\ndf['KitchenQual'].fillna(df['KitchenQual'].mode()[0],inplace=True)\ndf['Functional'].fillna(df['Functional'].mode()[0],inplace=True)\ndf['SaleType'].fillna(df['SaleType'].mode()[0],inplace=True)\n","46d09b5e":"df['Utilities'].value_counts()","ba7e4b6b":"df.drop('Utilities',axis=1,inplace=True)\n#isi Utilities cuman AllPubs kecuali 2 data NoSeWa","1d4f8ea3":"sns.scatterplot(data=df, x='GrLivArea', y='SalePrice');\nplt.title('Outliars Searching');","096cb424":"#locating the id, it is 524 and 1299 the id that we looking for\ndf.loc[df['GrLivArea']>4500]","e692dc5e":"df = df.drop(df[df['Id'] == 1299].index)\ndf = df.drop(df[df['Id'] == 524].index)","5405821b":"#Building class are transformed into categorical features.\ndf['MSSubClass'] = df['MSSubClass'].apply(str)\n\n#Year sold are transformed into categorical features.\ndf['YrSold'] = df['YrSold'].astype(str)\n","c0fb229d":"object_col = df.dtypes[df.dtypes== 'O'].index.values\nfrom sklearn import preprocessing\n# #Label Encoding\nle = preprocessing.LabelEncoder()\n\nfor cate in object_col:\n    le.fit(df[cate])\n    df[cate] = le.transform(df[cate])\n\n","d7f49d18":"#Clean train & test data\ntest = df.loc[df['SalePrice'].isnull()]\ntrain= df.loc[df['SalePrice'].notnull()]\ntest.drop('SalePrice',axis=1,inplace=True)\nprint('train shape: {}'.format(train.shape))\nprint('test shape: {}'.format(test.shape))\n#new train and test dimension","afc7bcf0":"from sklearn.model_selection import train_test_split\n\ntrain_train, train_valid = train_test_split(train, test_size=0.2, random_state=42)","bc482131":"#libraries for machine learning\nimport xgboost as xgb\nfrom sklearn.svm import SVR\nfrom catboost import CatBoostRegressor\nfrom lightgbm import LGBMRegressor\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeRegressor\nfrom mlxtend.regressor import StackingRegressor\nfrom sklearn.linear_model import LinearRegression, BayesianRidge\nfrom sklearn.model_selection import RepeatedKFold\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, mean_squared_log_error","2ff9ba62":"def model_check(train_train):\n    X = train_train.drop(columns = ['SalePrice'])\n    y = train_train['SalePrice']\n\n    # Creation of the RMSE metric:\n\n    def rmsle(y, y_pred):\n        return np.sqrt(mean_squared_log_error(y, y_pred))\n\n    def cv_rmse(model):\n        rmse =np.sqrt(-cross_val_score(model, X,y, scoring=\"neg_mean_squared_log_error\", cv=kf))\n        return (rmse)\n\n    # 10 Fold Cross validation\n\n    kf = KFold(n_splits=5, random_state=42, shuffle=True)\n\n    cv_scores = []\n    cv_std = []\n\n\n    baseline_models = ['Bayesian_Ridge_Reg.','LGBM_Reg.','SVR','Dec_Tree_Reg.','Random_Forest_Reg.', 'XGB_Reg.',\n                     'Grad_Boost_Reg.','Cat_Boost_Reg.']\n\n\n    # Bayesian Ridge Regression\n\n    brr = BayesianRidge(compute_score=True)\n    score_brr = cv_rmse(brr)\n    cv_scores.append(score_brr.mean())\n    cv_std.append(score_brr.std())\n\n    # Light Gradient Boost Regressor\n\n    l_gbm = LGBMRegressor(objective='regression')\n    score_l_gbm = cv_rmse(l_gbm)\n    cv_scores.append(score_l_gbm.mean())\n    cv_std.append(score_l_gbm.std())\n\n    # Support Vector Regression\n\n    svr = SVR()\n    score_svr = cv_rmse(svr)\n    cv_scores.append(score_svr.mean())\n    cv_std.append(score_svr.std())\n\n    # Decision Tree Regressor\n\n    dtr = DecisionTreeRegressor()\n    score_dtr = cv_rmse(dtr)\n    cv_scores.append(score_dtr.mean())\n    cv_std.append(score_dtr.std())\n\n    # Random Forest Regressor\n\n    rfr = RandomForestRegressor()\n    score_rfr = cv_rmse(rfr)\n    cv_scores.append(score_rfr.mean())\n    cv_std.append(score_rfr.std())\n\n    # XGB Regressor\n\n    Xgb = xgb.XGBRegressor()\n    score_xgb = cv_rmse(Xgb)\n    cv_scores.append(score_xgb.mean())\n    cv_std.append(score_xgb.std())\n\n    # Gradient Boost Regressor\n\n    gbr = GradientBoostingRegressor(verbose=False)\n    score_gbr = cv_rmse(gbr)\n    cv_scores.append(score_gbr.mean())\n    cv_std.append(score_gbr.std())\n\n    # Cat Boost Regressor\n\n    catb = CatBoostRegressor(verbose=False)\n    score_catb = cv_rmse(catb)\n    cv_scores.append(score_catb.mean())\n    cv_std.append(score_catb.std())\n\n\n    final_cv_score = pd.DataFrame(baseline_models, columns = ['Regressors'])\n    final_cv_score['RMSLE_mean'] = cv_scores\n    final_cv_score['RMSLE_std'] = cv_std\n\n    final_cv_score.sort_values(by='RMSLE_mean', ascending=False)\n\n    return(final_cv_score)","7506ad9a":"score = model_check(train_train)","925fd631":"score.sort_values(by='RMSLE_mean', ascending=True)","53ad99fb":"X = train_train.drop('SalePrice',axis=1)\ny= train_train['SalePrice']\nX_val = train_valid.drop('SalePrice',axis=1)\ny_val = train_valid['SalePrice']\n\n#Model\ncatb = CatBoostRegressor(depth=4,l2_leaf_reg=1e-20,learning_rate=0.01\n                         ,verbose=False,random_seed=40)\ncatb.fit(X,y)\ny_pred = catb.predict(X_val)\nrmsle = np.sqrt(mean_squared_log_error(y_val, y_pred))\nprint('RMLSE: %f' % rmsle)","608f091b":"feature_importance = catb.feature_importances_\ndf_fi = pd.DataFrame({'fiture':X.columns,'weight':feature_importance})\ndf_fi.sort_values(by='weight',ascending=False)","b8c33755":"fi_cols = df_fi.loc[df_fi['weight']>=0.1]['fiture'].to_numpy()\nfi_cols","87415c09":"catb.fit(X[fi_cols],y)\ny_pred = catb.predict(X_val[fi_cols])\nrmsle = np.sqrt(mean_squared_log_error(y_val, y_pred))\nprint('RMLSE: %f' % rmsle)","3df63d7f":"X = train.drop('SalePrice',axis=1)\ny = train['SalePrice']\n\n#Model\ncatb = CatBoostRegressor(depth=4,l2_leaf_reg=1e-20,learning_rate=0.01,\n                         verbose=False,random_seed=40)\ncatb.fit(X[fi_cols],y)\ny_pred_test = catb.predict(test[fi_cols])","4f0a9820":"answer = pd.DataFrame(test['Id'])\nanswer['SalePrice'] = np.exp(y_pred_test)\n#dont forget we have to transform back our Sale Price\n\nanswer.to_csv('answer.csv',index = False)\n","a2334477":"## **Feature Importance**","ef952818":"# **Fill\/Drop NaN**","98629ac5":"After doing GridSearch the best parameter for our model are:\n{'depth': 4, 'l2_leaf_reg': 1e-20, 'learning_rate': 0.01}.","228e66a8":"Utilities variabel is almost AllPub value, so we can drop Utilites variable because it is not helpfull for our modeling","0057682a":"# **Modeling**","83336563":"# **EDA (Exploratory Data Analysis)**\nNext we have to do some reaserch about the data. In this step we tend to find what is the correlation between variabels, is there any redundan variables (having high correlation with other variables), and how our data distributed.\n","e995f3b4":"After we get the best parameter for our model, we want to know wich feature\/variables is the most important for our model. Feature Importance is one of many way to find wich feature weighted the most.","1fe11b2b":"Now we take features that weight equal or more than 0.1","1316d4e8":"RMSLE score is smaller than the model with all the features use, so this is a progress","9659761f":"From the distribution plot and the value of kurtosis and skewness it seems that Sale Price doesn't normaly distributed. I want to have normaly distributed data, so I transform Sale Price using logaritmic function","583e845b":"As you can see above, white row tell us that the data is missing. Alley, GarageCond, PoolQc,Fence, MiscFeature has the most missing value. On Basement and Garage variables you can see that the missing value happend in the same row, so that means there are no Basement or Garage in the house, so we sill fill with 'None' or 0 (if it is numerical). We will delete the data that have >15% are missing. For the other variables we just fill those variabels with their mode.","8e4f49f5":"We use Cat Boost Regressor for our model, because it has the smallest RMSLE","15df19c5":"From the heatmap we can see that the variabels GarageCars & GarageArea, TotalBsmtSF & 1stFlrSF, YearBuilt & GarageYrBlt, TotRmsAbvGrd & GrLivArea strongly correlated. Therefore we just need one of them for our model.","dd0e5d02":"# **Feature Engineering**","01c922a3":"# **Prepare The Data**","4d100779":"This is the end of our journey, hope this code can help you to kick start your own program :)","a100dfd9":"That are 7 variables that strongly correlated with Sale Price.","6a8ad051":"## **Kicking out the outliars**","7353ef5a":"# **Predicting**","a6e0b7fb":"now we want to look how Sale Price columns distributed with distribution plot","f77ffbba":"After reducing some features, lets try on the model and see how it affect the RMSLE","7297a643":"Now we want to see wich 10 variables are strongly correlated with SalePrice","4dc296e2":"GrLivArea point on the right seems to be the outliars, because they are seperated from the group quite far","614b4ecd":"# **Evaluating Model**","f7165d20":"With the logaritmic transformation our Sale Price variables is more likely normaly distributed. Now we want to see the correlation between variabels","d5ba5dfe":"**Transforming Numerical Variables that are rearlly categorical**"}}