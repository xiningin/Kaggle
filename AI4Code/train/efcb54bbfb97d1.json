{"cell_type":{"e1da15d5":"code","96894672":"code","3fe30f5f":"code","c42265d6":"code","f7eb650c":"code","df1d9b85":"code","11a0030e":"code","c783b647":"code","c391fd2f":"code","e6f6f643":"code","99ccc194":"code","4169c6d4":"code","7125c1aa":"code","3608c6b7":"code","e8dece6f":"code","dc3fc0eb":"code","d2156247":"code","22a1828e":"code","3f90bef0":"code","63977f1a":"code","7367ebc7":"code","eafd5a4d":"code","e34cb3b4":"code","973525f2":"code","8796868f":"code","b4564f72":"code","5fd1cec4":"code","8370b819":"code","bc8ea7d9":"code","1db8bf45":"code","26d366f5":"code","5a8ec906":"code","1f51f5c7":"code","bf6a1097":"code","55229cfb":"code","d7e4aa49":"code","6536c70a":"code","e0e0c83e":"code","2f8c1ea1":"code","ab90fe91":"code","5be99cb9":"code","32524d35":"code","f3218b22":"code","cf3c06fd":"code","dd43418c":"code","34ff8cc5":"code","6f8696c8":"code","4b868bd3":"code","06298a7c":"code","384082b2":"code","415f645a":"code","d18d1d9c":"code","fafad2af":"code","10b99749":"code","a6157496":"code","27d213a8":"code","cbe08bf9":"code","db9907c7":"code","285fcf8f":"code","70990966":"code","c82de0b8":"code","2db33b9f":"code","aa32eb55":"code","17fbcf25":"code","9e5f3409":"code","2164a3a0":"code","9c771409":"code","7f82123a":"code","4cc2c71b":"code","37dfd64c":"code","c076acd4":"code","eedca0e6":"code","8385b03a":"code","65c81307":"code","4a85d977":"code","38eeba38":"code","9a1f14ed":"code","f3e0358e":"code","c0993afb":"code","fc6b4439":"code","fd0ffc98":"code","26fa5404":"code","cb7161b9":"code","f464017c":"code","01fb4de7":"code","f5706d6a":"code","b142261e":"code","377aa097":"markdown","14ef82b5":"markdown","b509b2a1":"markdown","9de1f168":"markdown","47b41d7d":"markdown","48bc6d02":"markdown","5847d4b5":"markdown","e866f449":"markdown","ea0f1c34":"markdown","a35ffe15":"markdown","0870b896":"markdown","5e03b2ef":"markdown","7ba2fa2f":"markdown","b4efaa45":"markdown","57a94e6b":"markdown","2d2f6a43":"markdown","669543ff":"markdown","87dafde8":"markdown","ca133184":"markdown","4770ed19":"markdown","8a171372":"markdown","95cb52d3":"markdown","36be6b3e":"markdown","b036662b":"markdown","3b65fb5e":"markdown","6c18da96":"markdown","37f780d9":"markdown","d31fdfb7":"markdown","2c8c5d21":"markdown"},"source":{"e1da15d5":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.experimental import enable_iterative_imputer \nfrom sklearn.impute import IterativeImputer\nfrom sklearn.tree import DecisionTreeRegressor","96894672":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\ndata=pd.read_csv(\"..\/input\/marketing-datacsv\/CC GENERAL.csv\")\n\ndata.head()\n","3fe30f5f":"data=pd.read_csv(\"..\/input\/marketing-datacsv\/CC GENERAL.csv\")\n\ndata.head()","c42265d6":"\nprint(data.shape)#The Data has 8950 rows and 18 columns\nnum_columns=['BALANCE','PURCHASES','ONEOFF_PURCHASES','INSTALLMENTS_PURCHASES',\n             'CASH_ADVANCE','CASH_ADVANCE_TRX','PURCHASE_TRX','CREDIT_LIMIT','PAYMENTS','MINIMUM_PAYMENTS',\n            'PRC_FULL_PAYMENT']#Numerical Columns\ncat_columns=['CUST_ID','BALANCE_FREQUENCY','PURCASES_FREQUENCY','ONEOFF_PURCASES_FREQUENCY','PURCHASE_INSTALLMENTS_FREQUENCY',\n             'CASH_ADVANCE_FREQUENCY','TENURE'] #There are the columns that categorical in nature. \n","f7eb650c":"#Columns like 'BALANCE_FREQUENCY', 'PURCASES_FREQUENCY','ONEOFF_PURCASES_FREQUENCY',\n#'PURCHASE_INSTALLMENTS_FREQUENCY',\n#and 'CASH_ADVANCE_FREQUENCY' have been label encoded, while analyzing we can treat them as numerical.  \n#Hence, we can take only two columns as categorical\n#Revised Num Columns:\nnum_columns=['BALANCE','PURCHASES','ONEOFF_PURCHASES','INSTALLMENTS_PURCHASES',\n             'CASH_ADVANCE','CASH_ADVANCE_TRX','PURCHASES_TRX','CREDIT_LIMIT','PAYMENTS','MINIMUM_PAYMENTS',\n            'PRC_FULL_PAYMENT','BALANCE_FREQUENCY','PURCHASES_FREQUENCY','ONEOFF_PURCHASES_FREQUENCY','PURCHASES_INSTALLMENTS_FREQUENCY',\n             'CASH_ADVANCE_FREQUENCY']\n#Revised Cat Columns:\ncat_columns=['CUST_ID','TENURE']","df1d9b85":"\ndata[num_columns].describe()","11a0030e":"data['CUST_ID'].value_counts(normalize=True)\n#Each row as a unique value which is being used as an index. \n#Hence, we can drop this column. ","c783b647":"data.drop('CUST_ID',axis=1,inplace=True)","c391fd2f":"sns.countplot(data['TENURE'])\nplt.show()\nprint(data['TENURE'].value_counts(normalize=True)*100)\n#The highest count of tenure is 12 and the lowest is 9\n#Unit of tenure is not provvided to us. ","e6f6f643":"plt.figure(figsize=(10,10))\nmasme=np.tril(data.cov())\nsns.heatmap(data.cov())\nplt.show()","99ccc194":"plt.figure(figsize=(10,10))\nmaskme=np.tril(data.corr())\nsns.heatmap(data.corr(),annot=True,mask=maskme)\nplt.show()","4169c6d4":"\nplt.figure(figsize=(25,25))\nsns.pairplot(data)\nplt.show()","7125c1aa":"#Checking for null values\n(data.isnull().sum()\/data.shape[0])*100","3608c6b7":"#Since there are few null values let's impute them using Iterative Imputer and KNNRegressor\n#Let's impute the null values using\nfrom sklearn.experimental import enable_iterative_imputer \nfrom sklearn.impute import IterativeImputer\nfrom sklearn.tree import DecisionTreeRegressor","e8dece6f":"dt=DecisionTreeRegressor()\nit=IterativeImputer(estimator=dt)","dc3fc0eb":"data_impute=it.fit_transform(data)","d2156247":"data_impute=pd.DataFrame(data_impute,columns=data.columns)\ndata_impute.head()","22a1828e":"#Checking for null values_again\n(data_impute.isnull().sum()\/data_impute.shape[0])*100","3f90bef0":"#Alternative Imputation Technique using Simple Imputer\n#sns.kdeplot(data['CREDIT_LIMIT'])\n#plt.show()\n#We will impute the column Credit Limit with Median value since out graph is right skewed","63977f1a":"#sns.kdeplot(data['MINIMUM_PAYMENTS'])\n#plt.show()\n#We will impute the column Minimum_payments with Median value since out graph is right skewed","7367ebc7":"#Filling the null values using median\n#from sklearn.impute import SimpleImputer\n#si=SimpleImputer(strategy='median')\n","eafd5a4d":"#data_impute=si.fit_transform(data)","e34cb3b4":"#data_impute=pd.DataFrame(data_impute,columns=data.columns)\n#data_impute.head()","973525f2":"#Checking for null values_again\n#(data_impute.isnull().sum()\/data_impute.shape[0])*100","8796868f":"from sklearn.preprocessing import MinMaxScaler\nmms=MinMaxScaler()\ndf1=mms.fit_transform(data_impute)\ndata_mms=pd.DataFrame(df1,columns=data.columns)\ndata_mms.head()","b4564f72":"#Applying Log Transformation\ndata_mms=np.log1p(data_mms)\ndata_mms.head()","5fd1cec4":"from sklearn.preprocessing import PowerTransformer\npt=PowerTransformer()\ndf2=pt.fit_transform(data_impute)\ndata_pt=pd.DataFrame(df2,columns=data.columns)\ndata_pt.head()","8370b819":"#We have created two set of transformed data that we can use for applying PCA and then for cluster optimizatioin \n#Techniques:\n#for data_mms: we have applied min-max scaler, then log transformation\n#for data_pt: we have applied Power Tranformer. ","bc8ea7d9":"from sklearn.decomposition import PCA\n#On the basis of correlation matrix, we can say that 6-8 features have high correlation \n#with some of the other features,(have considered one feature having high or negative correlation\n#with two or more features)\n","1db8bf45":"#Applying for Data_MMS\npca=PCA(0.90,svd_solver='full',random_state=3)\npca_mms=pca.fit_transform(data_mms)\npca_mms.shape","26d366f5":"#Applying for Data_PT\npca=PCA(0.90,svd_solver='full',random_state=3)\npca_pt=pca.fit_transform(data_pt)\npca_pt.shape","5a8ec906":"#Applying for PCA_MMS\ndf_pca_mms=pd.DataFrame(pca_mms)\nplt.figure(figsize=(10,10))\nmaskme=np.tril(df_pca_mms.corr())\nsns.heatmap(df_pca_mms.corr(),annot=True,mask=maskme)\nplt.show()","1f51f5c7":"#Applying for PCA_PT\ndf_pca_pt=pd.DataFrame(pca_pt)\nplt.figure(figsize=(10,10))\nmaskme=np.tril(df_pca_pt.corr())\nsns.heatmap(df_pca_pt.corr(),annot=True,mask=maskme)\nplt.show()\n#In both the cases, we can see that multicollinearity has reduced tremendously after applying PCA","bf6a1097":"#Applying for DF_PCA_MMS\nplt.figure(figsize=(10,10))\nsns.pairplot(df_pca_mms)\nplt.show()","55229cfb":"#Applying for DF_PCA_MMS\nplt.figure(figsize=(10,10))\nsns.pairplot(df_pca_pt)\nplt.show()","d7e4aa49":"#Applying for DF_PCA_MMS\nfor i in df_pca_mms.columns:\n    sns.boxplot(df_pca_mms[i])\n    plt.show()","6536c70a":"for i in df_pca_pt.columns:\n    sns.boxplot(df_pca_pt[i])\n    plt.show()","e0e0c83e":"#Currently, we are not removing outliers as we have already applied transformation \n#to the data before applying PCA\n#We will do so if needed by once applying clustering.\n","2f8c1ea1":"from sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_score\nX=df_pca_mms\nwcv=[]\nsil=[]\nfor i in range(2,11):\n    km=KMeans(n_clusters=i)\n    km.fit(X)\n    wcv.append(km.inertia_)\n    sil.append(silhouette_score(X,km.labels_))\n    ","ab90fe91":"plt.plot(range(2,11),wcv)\nplt.title('K-Means')\nplt.xlabel('K-Number of Centroids')\nplt.ylabel('WCV Score')\nplt.show()","5be99cb9":"plt.plot(range(2,11),sil)\nplt.title('K-Means')\nplt.xlabel('K-Number of Centroids')\nplt.ylabel('Silhoutte Score')\nplt.show()","32524d35":"#Firstly Applying for df_pca_mms for AgglomerativeClustering\nfrom sklearn.cluster import AgglomerativeClustering\nimport scipy.cluster.hierarchy as sch\ndendogram= sch.dendrogram(sch.linkage(X,method='ward'))","f3218b22":"from sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_score\nX=df_pca_pt\nwcv=[]\nsil=[]\nfor i in range(2,11):\n    km=KMeans(n_clusters=i)\n    km.fit(X)\n    wcv.append(km.inertia_)\n    sil.append(silhouette_score(X,km.labels_))","cf3c06fd":"plt.plot(range(2,11),wcv)\nplt.title('K-Means')\nplt.xlabel('K-Number of Centroids')\nplt.ylabel('WCV Score')\nplt.show()","dd43418c":"plt.plot(range(2,11),sil)\nplt.title('K-Means')\nplt.xlabel('K-Number of Centroids')\nplt.ylabel('Silhoutte Score')\nplt.show()","34ff8cc5":"from sklearn.cluster import AgglomerativeClustering\nimport scipy.cluster.hierarchy as sch\ndendogram= sch.dendrogram(sch.linkage(X,method='ward'))","6f8696c8":"#For both sets we have worked upon, our optimal value for k is coming out to be 2 or 5\n#How?\n#The silhoutte score is the highest at two, the longest vertical line in the dendrogram is\n#intersecting with a horizontal line at 2 points, hence we choose the optimal value of k as 2\n#In both the cases our WCV Score is hard to interpret, as it is making an elbow at 3-5 k centroids. \n","4b868bd3":"#Conidering the first data that is data_pca_mms:\n#Applying KMeans with 2 clusters\nX=df_pca_mms\nkm=KMeans(n_clusters=2)\nkm.fit(X)\ny_pred=km.predict(X)","06298a7c":"for i in range(2,6):\n    plt.scatter(X.iloc[y_pred==0,1],X.iloc[y_pred==0,i],s=100,c='orange',label='Cluster1',marker='.')\n    plt.scatter(X.iloc[y_pred==1,1],X.iloc[y_pred==1,i],s=100,c='blue',label='Cluster2',marker='.')\n    plt.legend()\n    plt.show()","384082b2":"y_pred=list(y_pred)\n","415f645a":"df4=df_pca_mms","d18d1d9c":"df4['y_pred']=y_pred","fafad2af":"#Checking how our clusters are getting made using  KMeans on df_pca_mms using 2 clusters\nsns.set(style=\"ticks\", color_codes=True)\nsns.pairplot(df4,hue='y_pred')\nplt.show()","10b99749":"#Conidering the first data that is data_pca_mms:\n#Applying KMeans with 5 clusters\nX=df_pca_mms\nkm=KMeans(n_clusters=5)\nkm.fit(X)\ny_pred=km.predict(X)","a6157496":"for i in range(2,6):\n    plt.scatter(X.iloc[y_pred==0,1],X.iloc[y_pred==0,i],s=100,c='orange',label='Cluster1',marker='.')\n    plt.scatter(X.iloc[y_pred==1,1],X.iloc[y_pred==1,i],s=100,c='blue',label='Cluster2',marker='.')\n    plt.scatter(X.iloc[y_pred==2,1],X.iloc[y_pred==2,i],s=100,c='yellow',label='Cluster3',marker='.')\n    plt.scatter(X.iloc[y_pred==3,1],X.iloc[y_pred==3,i],s=100,c='black',label='Cluster4',marker='.')\n    plt.scatter(X.iloc[y_pred==4,1],X.iloc[y_pred==4,i],s=100,c='magenta',label='Cluster5',marker='.')\n    plt.legend()\n    plt.show()","27d213a8":"y_pred=list(y_pred)\ndf4=df_pca_mms\ndf4['y_pred']=y_pred","cbe08bf9":"#Checking how our clusters are getting made using  KMeans on df_pca_mms using 5 clusters\nplt.figure(figsize=(10,10))\nsns.set(style=\"ticks\", color_codes=True)\nsns.pairplot(df4,hue='y_pred')\nplt.show()","db9907c7":"#Trying Agglomerative Clustering on df_pca_mms with 2 clusters \nX=df_pca_mms\nac=AgglomerativeClustering(n_clusters=2,affinity='euclidean',linkage='ward')\ny_pred=ac.fit_predict(X)","285fcf8f":"for i in range(2,6):\n    plt.scatter(X.iloc[y_pred==0,1],X.iloc[y_pred==0,i],s=100,c='orange',label='Cluster1',marker='.')\n    plt.scatter(X.iloc[y_pred==1,1],X.iloc[y_pred==1,i],s=100,c='blue',label='Cluster2',marker='.')\n    plt.legend()\n    plt.show()","70990966":"y_pred=list(y_pred)\ndf4=df_pca_mms\ndf4['y_pred']=y_pred","c82de0b8":"sns.set(style=\"ticks\", color_codes=True)\nsns.pairplot(df4,hue='y_pred')\nplt.show()","2db33b9f":"#Applying Agglomerative Clustering on data df_pca_mms with 5 clusters\nX=df_pca_mms\nac=AgglomerativeClustering(n_clusters=5,affinity='euclidean',linkage='ward')\ny_pred=ac.fit_predict(X)","aa32eb55":"for i in range(2,6):\n    plt.scatter(X.iloc[y_pred==0,1],X.iloc[y_pred==0,i],s=100,c='orange',label='Cluster1',marker='.')\n    plt.scatter(X.iloc[y_pred==1,1],X.iloc[y_pred==1,i],s=100,c='blue',label='Cluster2',marker='.')\n    plt.scatter(X.iloc[y_pred==2,1],X.iloc[y_pred==2,i],s=100,c='yellow',label='Cluster3',marker='.')\n    plt.scatter(X.iloc[y_pred==3,1],X.iloc[y_pred==3,i],s=100,c='black',label='Cluster4',marker='.')\n    plt.scatter(X.iloc[y_pred==4,1],X.iloc[y_pred==4,i],s=100,c='magenta',label='Cluster5',marker='.')\n    plt.legend()\n    plt.show()","17fbcf25":"y_pred=list(y_pred)\ndf4=df_pca_mms\ndf4['y_pred']=y_pred","9e5f3409":"#Checking how our clusters are getting made using  Agglomerative Clustering on df_pca_mms using 5 clusters\nplt.figure(figsize=(10,10))\nsns.set(style=\"ticks\", color_codes=True)\nsns.pairplot(df4,hue='y_pred')\nplt.show()","2164a3a0":"#Trying KMeans on df_pca_pt with 2 clusters \nX=df_pca_pt\nkm=KMeans(n_clusters=2)\nkm.fit(X)\ny_pred=km.predict(X)","9c771409":"for i in range(2,6):\n    plt.scatter(X.iloc[y_pred==0,1],X.iloc[y_pred==0,i],s=100,c='orange',label='Cluster1',marker='.')\n    plt.scatter(X.iloc[y_pred==1,1],X.iloc[y_pred==1,i],s=100,c='blue',label='Cluster2',marker='.')\n    plt.legend()\n    plt.show()","7f82123a":"y_pred=list(y_pred)\ndf5=df_pca_pt\ndf5['y_pred']=y_pred","4cc2c71b":"#Checking how our clusters are getting made using  Agglomerative Clustering on df_pca_pt using 2 clusters\nplt.figure(figsize=(10,10))\nsns.set(style=\"ticks\", color_codes=True)\nsns.pairplot(df5,hue='y_pred')\nplt.show()","37dfd64c":"#Trying KMeans on df_pca_pt with 5 clusters\nX=df_pca_pt\nkm=KMeans(n_clusters=5)\nkm.fit(X)\ny_pred=km.predict(X)","c076acd4":"for i in range(2,6):\n    plt.scatter(X.iloc[y_pred==0,1],X.iloc[y_pred==0,i],s=100,c='orange',label='Cluster1',marker='.')\n    plt.scatter(X.iloc[y_pred==1,1],X.iloc[y_pred==1,i],s=100,c='blue',label='Cluster2',marker='.')\n    plt.scatter(X.iloc[y_pred==2,1],X.iloc[y_pred==2,i],s=100,c='yellow',label='Cluster3',marker='.')\n    plt.scatter(X.iloc[y_pred==3,1],X.iloc[y_pred==3,i],s=100,c='black',label='Cluster4',marker='.')\n    plt.scatter(X.iloc[y_pred==4,1],X.iloc[y_pred==4,i],s=100,c='magenta',label='Cluster5',marker='.')\n    plt.legend()\n    plt.show()","eedca0e6":"y_pred=list(y_pred)\ndf5=df_pca_pt\ndf5['y_pred']=y_pred","8385b03a":"#Checking how our clusters are getting made using  Agglomerative Clustering on df_pca_pt using 5 clusters\nplt.figure(figsize=(10,10))\nsns.set(style=\"ticks\", color_codes=True)\nsns.pairplot(df5,hue='y_pred')\nplt.show()","65c81307":"#Trying AgglomerativeClustering on df_pca_pt with 2 clusters\nX=df_pca_pt\nac=AgglomerativeClustering(n_clusters=2,affinity='euclidean',linkage='ward')\ny_pred=ac.fit_predict(X)","4a85d977":"for i in range(2,6):\n    plt.scatter(X.iloc[y_pred==0,1],X.iloc[y_pred==0,i],s=100,c='orange',label='Cluster1',marker='.')\n    plt.scatter(X.iloc[y_pred==1,1],X.iloc[y_pred==1,i],s=100,c='blue',label='Cluster2',marker='.')\n    plt.legend()\n    plt.show()","38eeba38":"y_pred=list(y_pred)\ndf5=df_pca_pt\ndf5['y_pred']=y_pred","9a1f14ed":"#Checking how our clusters are getting made using  Agglomerative Clustering on df_pca_pt using 2 clusters\nplt.figure(figsize=(10,10))\nsns.set(style=\"ticks\", color_codes=True)\nsns.pairplot(df5,hue='y_pred')\nplt.show()","f3e0358e":"#Trying AgglomerativeClustering on df_pca_pt with 5 clusters\nX=df_pca_pt\nac=AgglomerativeClustering(n_clusters=5,affinity='euclidean',linkage='ward')\ny_pred=ac.fit_predict(X)","c0993afb":"for i in range(2,6):\n    plt.scatter(X.iloc[y_pred==0,1],X.iloc[y_pred==0,i],s=100,c='orange',label='Cluster1',marker='.')\n    plt.scatter(X.iloc[y_pred==1,1],X.iloc[y_pred==1,i],s=100,c='blue',label='Cluster2',marker='.')\n    plt.scatter(X.iloc[y_pred==2,1],X.iloc[y_pred==2,i],s=100,c='yellow',label='Cluster3',marker='.')\n    plt.scatter(X.iloc[y_pred==3,1],X.iloc[y_pred==3,i],s=100,c='black',label='Cluster4',marker='.')\n    plt.scatter(X.iloc[y_pred==4,1],X.iloc[y_pred==4,i],s=100,c='magenta',label='Cluster5',marker='.')\n    plt.legend()\n    plt.show()","fc6b4439":"y_pred=list(y_pred)\ndf5=df_pca_pt\ndf5['y_pred']=y_pred","fd0ffc98":"#Checking how our clusters are getting made using  Agglomerative Clustering on df_pca_pt using 5 clusters\nplt.figure(figsize=(10,10))\nsns.set(style=\"ticks\", color_codes=True)\nsns.pairplot(df5,hue='y_pred')\nplt.show()","26fa5404":"#After careful evaluation of the above graphs, \n#We can see that AgglomerativeClustering fits the best with 5 clusters in data df_pt_pca\n#Also, we see that KMeans with 5 clusters fits the best in df_mms_pca\n#for further analysis, we will be using  data_pt_pca and agglomerative clustering\n#While using 2 clusters, we noticed that our data was at times only showing upper bound and lower bound outliers\nX=df_pca_pt\nac=AgglomerativeClustering(n_clusters=5,affinity='euclidean',linkage='ward')\ny_pred=ac.fit_predict(X)","cb7161b9":"#Inverse transforming the PCA Components\nX_retransformed=pca.inverse_transform(pca_pt)\nX_data=pd.DataFrame(X_retransformed,columns=data_pt.columns)","f464017c":"X_data['y_pred']=list(y_pred)\nX_data.head()","01fb4de7":"plt.figure(figsize=(20,20))\nsns.set(style=\"ticks\", color_codes=True)\nsns.pairplot(X_data,hue='y_pred')\nplt.show()","f5706d6a":"for i in X_data.columns:\n    sns.scatterplot(x='BALANCE',y=i,data=X_data,hue='y_pred',palette=\"deep\")\n    plt.legend(loc='best')\n    plt.show()","b142261e":"for i in X_data.columns:\n    sns.scatterplot(x='PURCHASES',y=i,data=X_data,hue='y_pred',palette=\"deep\")\n    plt.legend(loc='best')\n    plt.show()","377aa097":"**Making Dendrogram for data df_pca_mms for AgglomerativeClustering**","14ef82b5":"# Bank Marketing : Customer Segmentation","b509b2a1":"**4. Pairplot for PCA Components**","9de1f168":"**2. Applyinig PCA (by keeping 90% of variance in the data)**","47b41d7d":"# Part B: Data Preparation ","48bc6d02":"# Part D: Clustering- Using PCA dimensions to cluster the data (through K-means and Agglomerative clustering)**","5847d4b5":"We can see that AgglomerativeClustering fits the best with 5 clusters while using data df_pt_pt\nSince, we are able to diffrentiate between the clusters we consider this model, an effective model.\n\n**Business Interpretation**\n\nBased on the analysis, different offers and schemes can be offered to the differently clustered audience\nEach feature high or low is giving a different aspect or part of that cluster. \nIt helps us understand our consumer better.\n\n**Drawbacks of the interpretation**\n\nWe haven't dealt with the outliers because we thought they will form a seperate cluster\nThere is sometimes over-lap between features\nThe analysis is based on my market understanding, so their maybe a personal bias to it. ","e866f449":"For each record in the dataset it is provided:\n1. CUSTID : Identification of Credit Card holder (Categorical)\n2. BALANCE : Balance amount left in their account to make purchases (\n3. BALANCEFREQUENCY : How frequently the Balance is updated, score between 0 and 1 (1 = frequently updated, 0 = not frequently updated)\n4. PURCHASES : Amount of purchases made from account\n5. ONEOFFPURCHASES : Maximum purchase amount done in one-go\n6. INSTALLMENTSPURCHASES : Amount of purchase done in installment\n7. CASHADVANCE : Cash in advance given by the user\n8. PURCHASESFREQUENCY : How frequently the Purchases are being made, score between 0 and 1 (1 = frequently purchased, 0 = not frequently purchased)\n9. ONEOFFPURCHASESFREQUENCY : How frequently Purchases are happening in one-go (1 = frequently purchased, 0 = not frequently purchased)\n10. PURCHASESINSTALLMENTSFREQUENCY : How frequently purchases in installments are being done (1 = frequently done, 0 = not frequently done)\n11. CASHADVANCEFREQUENCY : How frequently the cash in advance being paid\n12. CASHADVANCETRX : Number of Transactions made with \"Cash in Advanced\"\n13. PURCHASESTRX : Numbe of purchase transactions made\n14. CREDITLIMIT : Limit of Credit Card for user\n15. PAYMENTS : Amount of Payment done by user\n16. MINIMUM_PAYMENTS : Minimum amount of payments made by user\n17. PRCFULLPAYMENT : Percent of full payment paid by user\n18. TENURE : Tenure of credit card service for user\n","ea0f1c34":"**Now Applying for data df_pca_pt for KMeans**\n\n    ","a35ffe15":"**3. Checking for Multi-Collinearity**","0870b896":"**Importing Key Libraries**","5e03b2ef":"# Part A: Data Understanding ","7ba2fa2f":"**5. Checking for Outliers in PCA Data**","b4efaa45":"**1. Applyinig PCA based on Correlation**","57a94e6b":"**Making Dendrogram for data df_pca_pt for AgglomerativeClustering**","2d2f6a43":"**3. Summarize observations for categorical variables**","669543ff":"**4. Covariance Matrix and Correlation Matrix**","87dafde8":"**Firstly Applying for df_pca_mms for KMeans**","ca133184":"### ATTRIBUTES:","4770ed19":"**2. Five Point Summary for Numerical Columns**","8a171372":"**Importing  Dataset**","95cb52d3":"**1. Data Shape**","36be6b3e":"**1. Null Values Treatment**","b036662b":"**5. Pair Plot for all columns**","3b65fb5e":"# Interpretation\n\nCluster1: High Balance,High Purchasing Frequency, Low-Installment-Purchases,Medium-High-Cash Advance Purchases\n\n**Conclusion Cat: Extremely Rich buyer, having high cashflows**(maybe the person is having a business)\n\nCluster2: Low-Medium Balance, Low Purchasing Frequency, Medium OneOff Purchases, Medium Credit Limit\n\n**Conclusion Cat: New Customer\/Not Active Customer**\n\nCluster3: Low Balance, Varied Purchase Frequency(Maybe Daily Buying), Low Oneoff Purchases, Low-cashadvance Purchases, Low Credit Limit\n\n**Conclusion Cat: Regular Buyer**(Middle Class Buyer)\n\nCluster4: High Balance, Medium-High Purchasing Frequency, High-Installment Purchases, High-Cashadvance Purchases,\nHigh_credit_limit\n\n**Conclusion Cat: Rich Buyer, Having low cashflows**(maybe the person is working in a highly paying job)\n\nCluster5: Medium Balance, Medium-High Purchasing Frequency, Varried one-off purchases\n\n**Conclusion Cat: Upper Middle Class Buyers**","6c18da96":"### Dataset Information: \nThis Dataset summarizes the usage behavior of about 9000 active credit card holders during the last 6 months. On this dataset we can perform customer segmentation to define marketing strategy. ","37f780d9":"# Part C: Dimensionality Reduction","d31fdfb7":"# Part E: Making Clusters for Interpretation","2c8c5d21":"**2. Scaling and Transformation**"}}