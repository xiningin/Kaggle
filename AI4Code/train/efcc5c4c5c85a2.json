{"cell_type":{"2cc02d7b":"code","79d061b9":"code","5622916e":"code","f34c958f":"code","6dccaee9":"code","3aa3d67e":"code","74794f5b":"code","594b5ef2":"code","b51a404f":"code","e0fc4c81":"code","3fe8c067":"code","e79554ef":"code","e4c07180":"code","df619807":"code","b7fec76d":"code","d8392a10":"code","f68bef66":"code","eb401dcd":"code","1d613f3f":"code","7ea0590d":"code","662cd368":"code","e852bc87":"code","5f075a2c":"code","05aecf29":"code","e59b4f77":"code","0b4c8c12":"code","7e735436":"code","bc18f6e7":"code","caae9404":"code","6a7c178c":"code","45990fc4":"code","7d8ab572":"code","e9d94b58":"code","c1b17054":"code","206750aa":"code","748e4590":"code","50414a01":"code","01e64aab":"code","cab1fb44":"code","29448131":"code","0bf4ab17":"code","d1b968eb":"code","024ffc7e":"code","d664ae2e":"code","4a1066ec":"code","5cb20e83":"code","8ff08ed7":"code","e97fa6bf":"code","f35dfd4d":"markdown","c48218b1":"markdown","88a36aad":"markdown","77688f2c":"markdown","6f789e5c":"markdown","66938ec3":"markdown","a8d35f39":"markdown","dbc064fc":"markdown","213962f0":"markdown","e1e31711":"markdown","b186c16d":"markdown","f3c4867c":"markdown","07ea813a":"markdown","d9db3a8f":"markdown","ee05d150":"markdown","a8dd2942":"markdown","fbf86867":"markdown","6b0e02a8":"markdown","16b0e23f":"markdown"},"source":{"2cc02d7b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('..\/input\/pricing-data'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","79d061b9":"import matplotlib.pyplot as plt\nimport seaborn as sns\nimport contextily as ctx\nfrom mpl_toolkits.basemap import Basemap","5622916e":"import numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\n\n\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.tools as tls","f34c958f":"df1=pd.read_excel('..\/input\/pricing-data\/AR1979to2019.xlsx')","6dccaee9":"df2=pd.read_excel('..\/input\/pricing-data\/ARRIVAL_REPORT_1979-2020.xlsx')\ndf3=pd.read_excel('..\/input\/pricing-data\/MW_arrivals19-20.xlsx')","3aa3d67e":"df1.head(5)\n","74794f5b":"df2.head(5)\n","594b5ef2":"df3.head(5)\n","b51a404f":"df1.head()\n","e0fc4c81":"df1.describe()","3fe8c067":"df1.corr()","e79554ef":"x_values = list(df1['COMMODITY'])\ny_values = list(df1['TOTAL'])\n\nloc_values = []\n\nfor index in range(0, len(x_values)):\n    temp_value = []\n\n    temp_value.append(x_values[index])\n    temp_value.append(y_values[index])\n    loc_values.append(temp_value)","e4c07180":"apr_values = list(df1['APRIL'])\nmay_values = list(df1['MAY'])\n\njune_values = list(df1['JUNE'])\njuly_values = list(df1['JULY'])\naugust_values = list(df1['AUG'])\n\nseptember_values = list(df1['SEP'])\noctober_values = list(df1['OCT'])\nnov_values = list(df1['NOV'])\n\ndec_values = list(df1['DEC'])\njan_values = list(df1['JAN'])\nfeb_values = list(df1['FEB'])\nmarch_values = list(df1['MAR'])","df619807":"attribute_list = []\n\nfor index in range(0, len(x_values)):\n    temp_list = []\n    \n    temp_list.append(x_values[index])\n    temp_list.append(y_values[index])\n    \n    temp_list.append(apr_values[index])\n    temp_list.append(may_values[index])\n\n    temp_list.append(june_values[index])\n    temp_list.append(july_values[index])\n    \n    temp_list.append(august_values[index])\n\n    temp_list.append(september_values[index])\n    temp_list.append(october_values[index])\n    temp_list.append(nov_values[index])\n    \n    temp_list.append(dec_values[index])\n    temp_list.append(jan_values[index])\n    temp_list.append(feb_values[index])\n    temp_list.append(march_values[index])\n\n    attribute_list.append(temp_list)","b7fec76d":"def count_points(x_points, y_points, scaling_factor):\n    count_array = []\n    \n    for index in range(0, len(x_points)):\n        temp_value = [x_points[index], y_points[index]]\n        count = 0\n        \n        for value in loc_values:\n            if(temp_value == value):\n                count = count + 1\n        count_array.append(count * scaling_factor )\n\n    return count_array","d8392a10":"def histogram_plot(dataset, title):\n    plt.figure(figsize=(12, 8))    \n    \n    ax = plt.subplot()    \n    ax.spines[\"top\"].set_visible(False)    \n    ax.spines[\"bottom\"].set_visible(False)    \n    ax.spines[\"right\"].set_visible(False)    \n    ax.spines[\"left\"].set_visible(False)\n    \n    ax.get_xaxis().tick_bottom()\n    ax.get_yaxis().tick_left() \n    \n    plt.title(title, fontsize = 22)\n    plt.hist(dataset, edgecolor='black', linewidth=1.2)","f68bef66":"plt.figure(figsize=(8, 6))    \n    \nax = plt.subplot()    \nax.spines[\"top\"].set_visible(False)    \nax.spines[\"bottom\"].set_visible(False)    \nax.spines[\"right\"].set_visible(False)    \nax.spines[\"left\"].set_visible(False)\n    \nax.get_xaxis().tick_bottom()\nax.get_yaxis().tick_left() \n    \nplt.title(\"sabji mandi Distribution plot\", fontsize = 22)\nplt.scatter(x_values, y_values, s = count_points(x_values, y_values, 100), alpha = 0.9)\nplt.show()","eb401dcd":"histogram_plot(apr_values, title = \"apr_values_distribution\")\nplt.show()","1d613f3f":"histogram_plot(may_values, title = \"may_values_distribution\")\nplt.show()","7ea0590d":"histogram_plot(june_values, title = \"june_values_distribution\")\nplt.show()","662cd368":"histogram_plot(july_values, title = \"july_values_distribution\")\nplt.show()","e852bc87":"histogram_plot(august_values, title = \"august_values_distribution\")\nplt.show()","5f075a2c":"histogram_plot(september_values, title = \"sep_values_distribution\")\nplt.show()","05aecf29":"histogram_plot(october_values, title = \"oct_values_distribution\")\nplt.show()","e59b4f77":"histogram_plot(nov_values, title = \"nov_values_distribution\")\nplt.show()","0b4c8c12":"histogram_plot(dec_values, title = \"dec_values_distribution\")\nplt.show()","7e735436":"histogram_plot(jan_values, title = \"jan_values_distribution\")\nplt.show()","bc18f6e7":"histogram_plot(feb_values, title = \"feb_values_distribution\")\nplt.show()","caae9404":"histogram_plot(march_values, title = \"march_values_distribution\")\nplt.show()","6a7c178c":"df1.info()\n","45990fc4":"df1.describe(include='O') #capital O","7d8ab572":"# Count of Number of Speicies\nplt.figure(figsize=(10,5))\n# sns.countplot(data['Species'])\nax = sns.countplot(df1['COMMODITY'])\nfor p in ax.patches:\n    h = p.get_height()\n    w = p.get_width()\/2\n    ax.text(p.get_x()+w, h+1,\n            '{:1}'.format(h),\n           ha=\"left\")\nplt.show()","e9d94b58":"a = df1.groupby(['COMMODITY']).count()\na","c1b17054":"sns.barplot(x=a.index, y='APRIL', data=a)","206750aa":"colors = ['cyan',]  *9\ntrace1 = go.Bar(\n    y=df1.COMMODITY,          \n    x=df1.index,\n    marker_color=colors\n   \n)\n\ndf = [trace1]\nlayout = go.Layout(\n    title='Count of Number',\n    font=dict(\n        size=16\n    ),\n    legend=dict(\n        font=dict(\n            size=6\n        )\n    )\n)\nfig = go.Figure(data=df, layout=layout)\npy.iplot(fig, filename='barchart')","748e4590":"plt.figure(figsize=(10,5))\nax = sns.barplot(x=df1.index, y='COMMODITY', data=df1)\nfor p in ax.patches:\n    h = p.get_height()\n    w = p.get_width()\/2\n    ax.text(p.get_x()+w, h+3,\n            '{}'.format(h),\n           ha=\"center\")\nplt.show()","50414a01":"colors = ['cyan','red','green','lightpink','blue','orange'] \ntrace1 = go.Bar(\n    y=df1.COMMODITY,\n    x=df1.index,\n    marker_color=colors,\n   \n)\n\ndf = [trace1]\nlayout = go.Layout(\n    title='Average Weight of Species',\n    font=dict(\n        size=16\n    ),\n    legend=dict(\n        font=dict(\n            size=30\n        )\n    )\n)\nfig = go.Figure(data=df, layout=layout)\npy.iplot(fig, filename='barchart')","01e64aab":"df1['TOTAL'] = (df1['APRIL'] + df1['MAY'] + df1['JUNE'] + df1['JULY']+df1['AUG']+df1['SEP']+df1['OCT']+df1['NOV']+df1['DEC']+df1['JAN']+df1['FEB']+df1['MAR']) \/ 12","cab1fb44":"plt.plot(df1['APRIL'], label='1')\nplt.plot(df1['MAY'], label='2')\nplt.plot(df1['JUNE'], label='3')\nplt.plot(df1['JULY'], label='4')\nplt.plot(df1['AUG'], label='5')\nplt.plot(df1['SEP'], label='6')\nplt.plot(df1['OCT'], label='7')\nplt.plot(df1['NOV'], label='8')\nplt.plot(df1['DEC'], label='9')\nplt.plot(df1['JAN'], label='10')\nplt.plot(df1['FEB'], label='11')\nplt.plot(df1['MAR'], label='12')\nplt.plot(df1['TOTAL'], label='F')\nplt.legend()","29448131":"import seaborn as sns\nplt.figure(figsize=(15,10))\ncor_mat= df1.corr(method='spearman')\nsns.heatmap(cor_mat,annot=True, square=False)","0bf4ab17":"from pandas_profiling import ProfileReport","d1b968eb":"ProfileReport(df1)","024ffc7e":"def box(var):\n    # this function take the variable and return a boxplot for each type of fish\n    sns.boxplot(x=\"COMMODITY\", y=var, data=df1,palette='rainbow')","d664ae2e":"fig, ax = plt.subplots(2, 3,figsize=(20,15))\nplt.subplot(2,3,1)\nbox('APRIL')\nplt.subplot(2,3,2)\nbox('MAY')\nplt.subplot(2,3,3)\nbox('JUNE')\nplt.subplot(2,3,4)\nbox('JULY')\nplt.subplot(2,3,5)\nbox('AUG')\nplt.subplot(2,3,6)\nbox('SEP')\nplt.subplot(2,3,6)\nbox('OCT')\nplt.subplot(2,3,6)\nbox('NOV')\nplt.subplot(2,3,6)\nbox('DEC')\nplt.subplot(2,3,6)\nbox('JAN')\nplt.subplot(2,3,6)\nbox('FEB')\nplt.subplot(2,3,6)\nbox('MAR')","4a1066ec":"sns.pairplot(data=df1)","5cb20e83":"sns.pairplot(data=df1,hue='COMMODITY')\nplt.title('pairwise relationships in a dataset')","8ff08ed7":"dfn =df1.dropna()","e97fa6bf":"dfn.head()\n","f35dfd4d":"df1.describe() function is used to generate the table with mean value, standard deviation with its minimam and maximum value.","c48218b1":"Despite the strange names I gave to the chapters, what we are doing in this kernel is something like:\n\nUnderstand the problem. We'll look at each variable and do a philosophical analysis about their meaning and importance for this problem.\nUnivariable study. We'll just focus on the dependent variable ('SalePrice') and try to know a little bit more about it.\nMultivariate study. We'll try to understand how the dependent variable and independent variables relate.\nBasic cleaning. We'll clean the dataset and handle the missing data, outliers and categorical variables.\nTest assumptions. We'll check if our data meets the assumptions required by most multivariate techniques.","88a36aad":"we are dividing our commodity and total value for using them in ploting for others fields. and both commodity and total is the very co-related fields for our dataset.","77688f2c":"Seaborn is a Python data visualization library based on Matplotlib. It provides a high-level interface for drawing attractive and informative statistical graphics. This article deals with the distribution plots in seaborn which is used for examining univariate and bivariate distributions. In this article we will be discussing 4 types of distribution plots namely:\n\njoinplot\ndistplot\npairplot\nrugplot\nBesides providing different kinds of visualization plots, seaborn also contains some built-in datasets. ","6f789e5c":"we are using pandas to read excel files for our datasets.","66938ec3":"**libraries like numpy, pandas, matplotlib and plotly are the few important and useful libraries that we can use whenever we are generating plots for heavy databases.**","a8d35f39":"Here , we can see the raw look of our dataset and fields of corresponding table.","dbc064fc":"here we are making an attribute list and another temp_list for storing and incrementing the values of given field by their index values. ","213962f0":"making the list of all the months for generating there distribution graph with each entry.","e1e31711":"so, this is the function which is used for plotting histogram distribution plots for every month with their inventory.we have also found that there are most distributed plot for apple and their incoming and outgoing.","b186c16d":"Outliers is also something that we should be aware of. Why? Because outliers can markedly affect our models and can be a valuable source of information, providing us insights about specific behaviours.\n\nOutliers is a complex subject and it deserves more attention. Here, we'll just do a quick analysis through the standard deviation of 'SalePrice' and a set of scatter plots.","f3c4867c":"**here we are importing our files for process their data and generate report.**","07ea813a":"In summary\nStories aside, we can conclude that:\n\n'commodity' and 'Total' seem to be linearly related with 'SalePrice'. Both relationships are positive, which means that as one variable increases, the other also increases. In the case of 'Total', we can see that the slope of the linear relationship is particularly high.\n'OverallQual' and 'YearBuilt' also seem to be related with 'SalePrice'. The relationship seems to be stronger in the case of 'OverallQual', where the box plot shows how sales prices increase with the overall quality.\nWe just analysed four variables, but there are many other that we should analyse. The trick here seems to be the choice of the right features (feature selection) and not the definition of complex relationships between them (feature engineering).\n\nThat said, let's separate the wheat from the chaff.","d9db3a8f":"df1.corr() function returns us the corelation of every row and column with respect to other field of coulmn. the vlaue for correlation deviates from 0 to 1. ","ee05d150":"Although we already know some of the main figures, this mega scatter plot gives us a reasonable idea about variables relationships.\n\nOne of the figures we may find interesting is the one between 'TotalBsmtSF' and 'GrLiveArea'. In this figure we can see the dots drawing a linear line, which almost acts like a border. It totally makes sense that the majority of the dots stay below that line. Basement areas can be equal to the above ground living area, but it is not expected a basement area bigger than the above ground living area (unless you're trying to buy a bunker).\n\nThe plot concerning 'SalePrice' and 'YearBuilt' can also make us think. In the bottom of the 'dots cloud', we see what almost appears to be a shy exponential function (be creative). We can also see this same tendency in the upper limit of the 'dots cloud' (be even more creative). Also, notice how the set of dots regarding the last years tend to stay above this limit (I just wanted to say that prices are increasing faster now).\n\nOk, enough of Rorschach test for now. Let's move forward to what's missing: missing data!\n\n\nImportant questions when thinking about missing data:\n\nHow prevalent is the missing data?\nIs missing data random or does it have a pattern?\nThe answer to these questions is important for practical reasons because missing data can imply a reduction of the sample size. This can prevent us from proceeding with the analysis. Moreover, from a substantive perspective, we need to ensure that the missing data process is not biased and hidding an inconvenient truth.","a8dd2942":"In my opinion, this heatmap is the best way to get a quick overview of our 'plasma soup' and its relationships. (Thank you @seaborn!)\n\nAt first sight, there are two red colored squares that get my attention. The first one refers to the 'TotalBsmtSF' and '1stFlrSF' variables, and the second one refers to the 'GarageX' variables. Both cases show how significant the correlation is between these variables. Actually, this correlation is so strong that it can indicate a situation of multicollinearity. If we think about these variables, we can conclude that they give almost the same information so multicollinearity really occurs. Heatmaps are great to detect this kind of situations and in problems dominated by feature selection, like ours, they are an essential tool.\n\nAnother thing that got my attention was the 'SalePrice' correlations. We can see our well-known 'GrLivArea', 'TotalBsmtSF', and 'OverallQual' saying a big 'Hi!', but we can also see many other variables that should be taken into account. That's what we will do next.\n\n","fbf86867":"Exploratory Data Analysis (EDA) is an approach to analyzing datasets to summarize their main characteristics. It is used to understand data, get some context regarding it, understand the variables and the relationships between them, and formulate hypotheses that could be useful when building predictive models.\n\nAll data analysis must be guided by some key questions or objectives. Before starting any data analysis tasks, you should have a clear goal in mind. When your goal allows you to understand your data and the problem, you will be in a good position to get meaningful results out of your analysis!\n\nIn this tutorial, we will learn how to perform EDA using data visualization. Specifically, we will focus on seaborn, a Python library that is built on top of matplotlib and has support for NumPy and pandas.\n\nseaborn allows us to make attractive and informative statistical graphics. Although matplotlib makes it possible to visualize essentially anything, it is often difficult and tedious to make the plots visually attractive. seaborn is often used to make default matplotlib plots look nicer, and also introduces some additional plot types.\n\nWe will cover how to visually analyze:\n\nNumerical variables with histograms,\nCategorical variables with count plots,\nRelationships between numerical variables with scatter plots, joint plots, and pair plots, and\nRelationships between numerical and categorical variables with box-and-whisker plots and complex conditional plots.\nBy effectively visualizing a dataset\u2019s variables and their relationships, a data analyst or data scientist is able to quickly understand trends, outliers, and patterns. This understanding can then be used to tell a story, drive decisions, and create predictive models.\n","6b0e02a8":"Ok, now we are dealing with the big boss. What do we have here?\n\nSomething that, in general, presents skewness.\nA significant number of observations with value zero (houses without basement).\nA big problem because the value zero doesn't allow us to do log transformations.\nTo apply a log transformation here, we'll create a variable that can get the effect of having or not having basement (binary variable). Then, we'll do a log transformation to all the non-zero observations, ignoring those with value zero. This way we can transform data, without losing the effect of having or not basement.\n\nI'm not sure if this approach is correct. It just seemed right to me. That's what I call 'high risk engineering'.","16b0e23f":"1. So... What can we expect?\nIn order to understand our data, we can look at each variable and try to understand their meaning and relevance to this problem. I know this is time-consuming, but it will give us the flavour of our dataset.\n\nIn order to have some discipline in our analysis, we can create an Excel spreadsheet with the following columns:\n\nVariable - Variable name.\nType - Identification of the variables' type. There are two possible values for this field: 'numerical' or 'categorical'. By 'numerical' we mean variables for which the values are numbers, and by 'categorical' we mean variables for which the values are categories.\nSegment - Identification of the variables' segment. We can define three possible segments: building, space or location. When we say 'building', we mean a variable that relates to the physical characteristics of the building (e.g. 'OverallQual'). When we say 'space', we mean a variable that reports space properties of the house (e.g. 'TotalBsmtSF'). Finally, when we say a 'location', we mean a variable that gives information about the place where the house is located (e.g. 'Neighborhood').\nExpectation - Our expectation about the variable influence in 'SalePrice'. We can use a categorical scale with 'High', 'Medium' and 'Low' as possible values.\nConclusion - Our conclusions about the importance of the variable, after we give a quick look at the data. We can keep with the same categorical scale as in 'Expectation'.\nComments - Any general comments that occured to us.\nWhile 'Type' and 'Segment' is just for possible future reference, the column 'Expectation' is important because it will help us develop a 'sixth sense'. To fill this column, we should read the description of all the variables and, one by one, ask ourselves:\n\nDo we think about this variable when we are buying a house? (e.g. When we think about the house of our dreams, do we care about its 'Masonry veneer type'?).\nIf so, how important would this variable be? (e.g. What is the impact of having 'Excellent' material on the exterior instead of 'Poor'? And of having 'Excellent' instead of 'Good'?).\nIs this information already described in any other variable? (e.g. If 'LandContour' gives the flatness of the property, do we really need to know the 'LandSlope'?).\nAfter this daunting exercise, we can filter the spreadsheet and look carefully to the variables with 'High' 'Expectation'. Then, we can rush into some scatter plots between those variables and 'SalePrice', filling in the 'Conclusion' column which is just the correction of our expectations.\n\nI went through this process and concluded that the following variables can play an important role in this problem:\n\nOverallQual (which is a variable that I don't like because I don't know how it was computed; a funny exercise would be to predict 'OverallQual' using all the other variables available).\nYearBuilt.\nTotalBsmtSF.\nGrLivArea.\nI ended up with two 'building' variables ('OverallQual' and 'YearBuilt') and two 'space' variables ('TotalBsmtSF' and 'GrLivArea'). This might be a little bit unexpected as it goes against the real estate mantra that all that matters is 'location, location and location'. It is possible that this quick data examination process was a bit harsh for categorical variables. For example, I expected the 'Neigborhood' variable to be more relevant, but after the data examination I ended up excluding it. Maybe this is related to the use of scatter plots instead of boxplots, which are more suitable for categorical variables visualization. The way we visualize data often influences our conclusions.\n\nHowever, the main point of this exercise was to think a little about our data and expectactions, so I think we achieved our goal. Now it's time for 'a little less conversation, a little more action please'. Let's shake it!\n\n2. First things first: analysing 'SalePrice'\n'SalePrice' is the reason of our quest. It's like when we're going to a party. We always have a reason to be there. Usually, women are that reason. (disclaimer: adapt it to men, dancing or alcohol, according to your preferences)\n\nUsing the women analogy, let's build a little story, the story of 'How we met 'SalePrice''.\n\nEverything started in our Kaggle party, when we were looking for a dance partner. After a while searching in the dance floor, we saw a girl, near the bar, using dance shoes. That's a sign that she's there to dance. We spend much time doing predictive modelling and participating in analytics competitions, so talking with girls is not one of our super powers. Even so, we gave it a try:\n\n'Hi, I'm Kaggly! And you? 'SalePrice'? What a beautiful name! You know 'SalePrice', could you give me some data about you? I just developed a model to calculate the probability of a successful relationship between two people. I'd like to apply it to us!'"}}