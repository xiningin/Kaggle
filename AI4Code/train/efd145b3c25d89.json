{"cell_type":{"faee591d":"code","4839d0c7":"code","42d9280c":"code","2429c9cc":"code","c68e7909":"code","77e4075c":"code","04045639":"code","59bc17d8":"code","8eb42fed":"code","b974aca7":"code","607c136e":"code","65ae4c1f":"code","396913dc":"code","b9f8b09e":"code","731edfbc":"code","4ad9d481":"code","556ead4d":"code","de8097a2":"code","e2f9175c":"code","855a277d":"code","f5cc7691":"code","3fbaa0e6":"code","209bee6e":"code","96534daa":"code","93c431c0":"code","5bdb4f66":"code","7707cd35":"code","bd6ebe29":"code","07ec8b1e":"code","341f513c":"code","5155982e":"markdown","e52eaf93":"markdown","c4d8918e":"markdown","bd7dbd5e":"markdown","881c3c76":"markdown","9de125f2":"markdown","34b677f6":"markdown","185d58cc":"markdown","6b162e0c":"markdown","357f75c1":"markdown","4924e2d3":"markdown","be532b4a":"markdown","bfa859ff":"markdown","7a583fe2":"markdown","19bce1cc":"markdown","66e69c43":"markdown","d7968946":"markdown"},"source":{"faee591d":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport os\nimport sklearn\n\nimport IPython.display as ipd\nimport librosa\nimport librosa.display\nimport soundfile as sf","4839d0c7":"# De la siguiente manera cargamos el archivo de audio en el Notebook, que ser\u00e1 una serie temporal a modo de 'array'\n# con una tasa de muestreo 'sr' de 22 kHZ mono.\n\naudio_test_1 = \"..\/input\/rfcx-species-audio-detection\/train\/0d25045a9.flac\"\n\n# Seleccion\u00e9 como ejemplo ese audio por tener 1 especie presente\n\nx , sr = librosa.load(audio_test_1)\n\nprint(type(x), type(sr))\nprint(x.shape, sr)","42d9280c":"# Si nos interesa, podemos cambiar la tasa de muestreo a 44.1 kHZ\n\nlibrosa.load(audio_test_1, sr = 44100)","2429c9cc":"# Para poder escuchar el audio aqu\u00ed en el 'Notebook' hacemos lo siguiente:\n\nipd.Audio(audio_test_1)","c68e7909":"# Mostramos el Espectrograma como una gr\u00e1fica de l\u00edneas.\n# Tiempo vs Frecuencia\n\n%matplotlib inline\n\nplt.figure(figsize=(14, 5))\nlibrosa.display.waveplot(x, sr=sr)","77e4075c":"# Mostramos el Espectrograma como un'Heatmap'.\n# Tiempo vs Frecuencia\n\nX = librosa.stft(x)\nXdb = librosa.amplitude_to_db(abs(X))\nplt.figure(figsize=(14, 5))\nlibrosa.display.specshow(Xdb, sr=sr, x_axis='time', y_axis='hz')\nplt.colorbar()","04045639":"# Aplicamos la escala logar\u00edtimica para centrar la atenci\u00f3n el los sonidos a estudiar\n\nlibrosa.display.specshow(Xdb, sr=sr, x_axis='time', y_axis='log')\nplt.colorbar()","59bc17d8":"sr = 22050 # Tiempo de muestreo\nT = 5.0    # Segundos\nt = np.linspace(0, T, int(T*sr), endpoint=False) # Variable tiempo\nx = 0.5*np.sin(2*np.pi*220*t) # Onda senoidal pura a 220 Hz\n\n# Reproduciendo el audio\nipd.Audio(x, rate=sr) # Generando un NumPy 'array'\n\n# Guardando el audio\nsf.write('tono_0d25045a9_220.wav', x, sr)","8eb42fed":"# El 'spectral_centroid' devolver\u00e1 un 'array' con el mismo n\u00famero de \n# columnas que n\u00famero de 'fotogramas' presentes en la muestra de audio\n\nspectral_centroids = librosa.feature.spectral_centroid(x, sr=sr)[0]\nspectral_centroids.shape\n\n# Calculando la variable de tiempo para la visualizaci\u00f3n\n\nplt.figure(figsize=(12, 4))\nframes = range(len(spectral_centroids))\nt = librosa.frames_to_time(frames)\n\n# Normalizando el 'centroide espectral' para la visualizaci\u00f3n\n\ndef normalize(x, axis=0):\n    return sklearn.preprocessing.minmax_scale(x, axis=axis)\n\n# Trazando el 'centroide espectral' a lo largo de la forma de onda\n\nlibrosa.display.waveplot(x, sr=sr, alpha=0.4)\nplt.plot(t, normalize(spectral_centroids), color='b')","b974aca7":"spectral_rolloff = librosa.feature.spectral_rolloff(x+0.01, sr=sr)[0]\nplt.figure(figsize=(12, 4))\nlibrosa.display.waveplot(x, sr=sr, alpha=0.4)\nplt.plot(t, normalize(spectral_rolloff), color='black')","607c136e":"spectral_bandwidth_2 = librosa.feature.spectral_bandwidth(x+0.01, sr=sr)[0]\nspectral_bandwidth_3 = librosa.feature.spectral_bandwidth(x+0.01, sr=sr, p=3)[0]\nspectral_bandwidth_4 = librosa.feature.spectral_bandwidth(x+0.01, sr=sr, p=4)[0]\nplt.figure(figsize=(15, 9))\nlibrosa.display.waveplot(x, sr=sr, alpha=0.4)\nplt.plot(t, normalize(spectral_bandwidth_2), color='r')\nplt.plot(t, normalize(spectral_bandwidth_3), color='g')\nplt.plot(t, normalize(spectral_bandwidth_4), color='y')\nplt.legend(('p = 2', 'p = 3', 'p = 4'))","65ae4c1f":"x.shape","396913dc":"x, sr = librosa.load(audio_test_1)\n\n# Para centrarnos en un momento espec\u00edfico y ver el comportamiento de las frecuencias, podemos\n# mostrar la gr\u00e1fica del audio en cuesti\u00f3n para hacerle un zoom en el momento concreto:\n\nplt.figure(figsize=(14, 5))\nlibrosa.display.waveplot(x, sr=sr)\n\n# Aplicamos el zoom, siendo 9000 y 9100 datos registrados en 'x'.\n\n# Dado que 'x' es un \"array\" de 1.323.000 valores, acotando a los valores \n# mencionados, conseguimos hacer zoom a una zona en concreta del audio.\n\n# Haciendo algunos c\u00e1lculos r\u00e1pidamente, si la duraci\u00f3n total es de 60 segundos\n# y en esos 60 segundos tenemos 1.323.000 valores, el zoom de 9000 a 9100 equivale\n# al periodo de tiempo entre el segundo 0'4081 y el segundo 0'4126.\n\nn0 = 9000\nn1 = 9100\nplt.figure(figsize=(14, 5))\nplt.plot(x[n0:n1])\nplt.grid()","b9f8b09e":"# Librosa tambi\u00e9n nos permite calcular f\u00e1cilmente el n\u00famero de veces\n# que la onda pasa por cero en el periodo de tiempo determinado\n\nzero_crossings = librosa.zero_crossings(x[n0:n1], pad=False)\nprint(sum(zero_crossings))","731edfbc":"# Para obtener una gr\u00e1fica MFCC, se definen los siguientes par\u00e1metros:\n\nfs=10  # Siendo 'fs' un escalar > 0 y que define la tasa de muestreo para el eje \"y\" \nmfccs = librosa.feature.mfcc(x, sr = fs)  # Se define la variable mfccs para poder sacar el gr\u00e1fico, es una matriz\nprint(mfccs.shape)\n(20, 97)\n\n# Utilizando la libreria 'librosa' sacamos el gr\u00e1fico correspondiente\n\nplt.figure(figsize=(15, 7))\nlibrosa.display.specshow(mfccs, sr = sr, x_axis = 'time')  # Siendo 'sr' la tasa de muestreo utilizada para determinar la escala de tiempo en el eje \"x\".","4ad9d481":"# Otra herramienta interesante es 'chromagram', pues permite resaltar las caracter\u00edsticas principales del croma\n\nhop_length=12  # La longitud del 'salto', que tambi\u00e9n se utiliza para determinar la escala de tiempo en el eje \"x\"\nchromagram = librosa.feature.chroma_stft(x, sr = sr, hop_length = hop_length)  # Se define la variable 'chromagram' para poder sacar el gr\u00e1fico, es una matriz\n\n# Y se obtiene el gr\u00e1fico con las variables ya definidas\n\nplt.figure(figsize=(15, 5))\nlibrosa.display.specshow(chromagram,\n                         x_axis = 'time', \n                         y_axis = 'chroma',\n                         hop_length = hop_length, \n                         cmap = 'coolwarm'  #'magma', 'gray_r', 'coolwarm'\n                         )","556ead4d":"# Trabajando en Linux desde mi ordenador, la manera de instalar el paquete 'resnest' fue utilizando:\n\n#pip install resnest\n\n# Pero para poder subir el Notebook a kaggle, la forma de implementar resnest es la siguiente\n\n!pip install resnest > \/dev\/null","de8097a2":"from pathlib import Path\nimport librosa as lb\n\nimport torch\nfrom  torch.utils.data import Dataset, DataLoader\n\nfrom tqdm.notebook import tqdm\n\nfrom resnest.torch import resnest50","e2f9175c":"# Se definen en primer lugar algunas variables y se cargan los datos de la competici\u00f3n para \n# implementarles transformaciones mediante unas funciones que se analizar\u00e1n m\u00e1s adelante\n\nNUM_CLASSES = 24\nSR = 16_000\nDURATION =  60\n\nDATA_ROOT = Path(\"..\/input\/rfcx-species-audio-detection\")\nTRAIN_AUDIO_ROOT = Path(\"..\/input\/rfcx-species-audio-detection\/train\")\nTEST_AUDIO_ROOT = Path(\"..\/input\/rfcx-species-audio-detection\/test\")","855a277d":"# Esta primera funci\u00f3n crea 'MelSpecComputer', para generar 'melspec',\n# a\u00f1adi\u00e9ndole como par\u00e1metros para poder utilizar dicha funci\u00f3n:\n\nclass MelSpecComputer:\n    def __init__(self, sr, n_mels, fmin, fmax):\n        \n        self.sr = sr  # sr: Tasa de muestreo del eje \"y\"\n        self.n_mels = n_mels  # n_mels: N\u00famero de bandas 'Mel' a generar\n        self.fmin = fmin  # fmin: Frecuencia m\u00e1xima\n        self.fmax = fmax  # fmax: Frecuencia m\u00ednima\n\n    def __call__(self, y):\n\n        melspec = lb.feature.melspectrogram(y,\n                                            sr = self.sr,\n                                            n_mels = self.n_mels,\n                                            fmin = self.fmin,\n                                            fmax = self.fmax,\n                                            )\n\n        melspec = lb.power_to_db(melspec).astype(np.float32)\n        \n        return melspec","f5cc7691":"# Esta primera funci\u00f3n se encarga de transformar 'X' en 'V', que contendr\u00e1\n# la informaci\u00f3n necesaria dentro del 'array' para pasar de \"mono\" a \"color\"\n\ndef mono_to_color(X, eps=1e-6, mean=None, std=None):\n    \n    X = np.stack([X, X, X], axis=-1)\n\n    # Estandarizaci\u00f3n\n    mean = mean or X.mean()\n    std = std or X.std()\n    X = (X - mean) \/ (std + eps)\n\n    # Normalizaci\u00f3n a [0, 255]\n    _min, _max = X.min(), X.max()\n\n    if (_max - _min) > eps:\n        V = np.clip(X, _min, _max)\n        V = 255 * (V - _min) \/ (_max - _min)\n        V = V.astype(np.uint8)\n    \n    else:\n        \n        V = np.zeros_like(X, dtype=np.uint8)\n\n    return V\n\n\n# Esta funci\u00f3n se encarga de normalizar la imagen\n\ndef normalize(image, mean=None, std=None):\n    \n    image = image \/ 255.0\n    \n    if mean is not None and std is not None:\n        \n        image = (image - mean) \/ std\n        \n    return np.moveaxis(image, 2, 0).astype(np.float32)\n\n# Y esta determinar\u00e1 la longitud de los audios en funci\u00f3n de los \n# datos dados por la competici\u00f3n (tmax - tmin)\n\ndef crop_or_pad(y, length, sr, is_train=True):\n    \n    if len(y) < length:\n        \n        y = np.concatenate([y, np.zeros(length - len(y))])\n        \n    elif len(y) > length:\n        \n        if not is_train:\n            \n            start = 0\n            \n        else:\n            \n            start = np.random.randint(len(y) - length)\n\n        y = y[start:start + length]\n\n    y = y.astype(np.float32, copy=False)\n\n    return y","3fbaa0e6":"# Una vez definidas las 3 funciones en la celda anterior, el siguiente paso es \n# completar un nuevo dataset 'RFCXDataset' utilizando las funciones anteriores\n# a modo de preparaci\u00f3n de los datos que se recoger\u00e1n en el nuevo dataset\n\nclass RFCXDataset(Dataset):\n\n    def __init__(self, \n                 data, \n                 sr, \n                 n_mels = 128, \n                 fmin = 0, \n                 fmax = None,  \n                 is_train = False,\n                 num_classes = NUM_CLASSES, \n                 root = None, \n                 duration = DURATION) :\n\n        self.data = data\n        \n        self.sr = sr\n        self.n_mels = n_mels\n        self.fmin = fmin\n        self.fmax = fmax or self.sr\/\/2\n\n        self.is_train = is_train\n\n        self.num_classes = num_classes\n        self.duration = duration\n        self.audio_length = self.duration*self.sr\n        \n        self.root =  root or (TRAIN_AUDIO_ROOT if self.is_train else TEST_AUDIO_ROOT)\n\n        self.wav_transfos = get_wav_transforms() if self.is_train else None\n\n        self.mel_spec_computer = MelSpecComputer(sr=self.sr, n_mels=self.n_mels, fmin=self.fmin, fmax=self.fmax)  # Funci\u00f3n definida en la celda anterior\n\n\n    def __len__(self):\n        return len(self.data)\n    \n    def read_index(self, idx, fill_val=1.0, offset=None, use_offset=True):\n        d = self.data.iloc[idx]\n        record, species = d[\"recording_id\"], d[\"species_id\"]\n        try:\n            if use_offset and (self.duration < d[\"duration\"]+1):\n                offset = offset or np.random.uniform(1, int(d[\"duration\"]-self.duration))\n\n            y, _ = lb.load(self.root.joinpath(record).with_suffix(\".flac\").as_posix(),\n                           sr=self.sr, duration=self.duration, offset=offset)\n            \n            if self.wav_transfos is not None:\n                y = self.wav_transfos(y, self.sr)\n            y = crop_or_pad(y, self.audio_length, sr=self.sr)  # Funci\u00f3n definida en la celda anterior\n            t = np.zeros(self.num_classes)\n            t[species] = fill_val\n            \n        except Exception as e:\n#             print(e)\n            raise ValueError()  from  e\n            y = np.zeros(self.audio_length)\n            t = np.zeros(self.num_classes)\n        \n        return y,t\n            \n        \n\n    def __getitem__(self, idx):\n\n        y, t = self.read_index(idx)\n        \n        \n        melspec = self.mel_spec_computer(y)  # Funci\u00f3n definida en la celda anterior\n        image = mono_to_color(melspec)  # Funci\u00f3n definida en la celda anterior\n        image = normalize(image, mean=None, std=None)  # Funci\u00f3n definida en la celda anterior\n\n        return image, t","209bee6e":"# Funci\u00f3n para obtener la duraci\u00f3n de los audios\n\ndef get_duration(audio_name, root=TEST_AUDIO_ROOT):\n    return lb.get_duration(filename=root.joinpath(audio_name).with_suffix(\".flac\"))","96534daa":"# Definici\u00f3n del nuevo dataset, para aplicarle posteriormente la variable definida\n# en las celdas anteriores 'RFCXDataset'\n\ndata = pd.DataFrame({\n    \"recording_id\": [path.stem for path in Path(TEST_AUDIO_ROOT).glob(\"*.flac\")],\n})\ndata[\"species_id\"] = [[] for _ in range(len(data))]\n\nprint(data.shape)\ndata[\"duration\"] = data[\"recording_id\"].apply(get_duration)","93c431c0":"# En este \u00faltimo paso, utilizando el paquete 'resnest' y aplicando 'RFCXDataset' a los datos se obtiene,\n# para cada 'recording_id', la probabilidad de cada una de las 23 especies de aparecer en el audio en\n# concreto, que viene a ser el 'dataset' requerido por la competici\u00f3n\n\n# Se definen los par\u00e1metros necesarios\n\nTEST_BATCH_SIZE = 40\nTEST_NUM_WORKERS = 2","5bdb4f66":"# Aplicamos las funciones al 'dataset' data\n\ntest_data = RFCXDataset(data=data, sr=SR)\n\n# Utilizamos PyTorch para cargar los datos, iterando sobre 'test_data'\n\ntest_loader = DataLoader(test_data, batch_size=TEST_BATCH_SIZE, num_workers=TEST_NUM_WORKERS)","7707cd35":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nnet = resnest50(pretrained=True).to(device)\nn_features = net.fc.in_features\nnet.fc = torch.nn.Linear(n_features, NUM_CLASSES)\nnet = net.to(device)\nnet.load_state_dict(torch.load(\"..\/input\/kkiller-rfcx-species-detection-public-checkpoints\/rfcx_resnest50.pth\", map_location=device))\nnet = net.eval()\nnet","bd6ebe29":"preds = []\nnet.eval()\nwith torch.no_grad():\n    for (xb, yb) in  tqdm(test_loader):\n        xb, yb = xb.to(device), yb.to(device)\n        o = net(xb)\n        o = torch.sigmoid(o) \n        preds.append(o.detach().cpu().numpy())\npreds = np.vstack(preds)\npreds.shape","07ec8b1e":"sub = pd.DataFrame(preds, columns=[f\"s{i}\" for i in range(24)])\nsub[\"recording_id\"] = data[\"recording_id\"].values[:len(sub)]\nsub = sub[[\"recording_id\"] + [f\"s{i}\" for i in range(24)]]\nprint(sub.shape)\nsub.head()","341f513c":"sub.to_csv(\"submission.csv\", index=False)","5155982e":"Short Time Fourier Transform (stft()) convierte los datos en, como su nombre indica, [transformadas de Fourier de tiempo corto](https:\/\/es.wikipedia.org\/wiki\/Transformada_de_Fourier_de_Tiempo_Reducido), usadas para determinar el contenido en frecuencia sinusoidal y de fase en secciones locales de una se\u00f1al as\u00ed como sus cambios con respecto al tiempo.\nSTFT convierte las se\u00f1ales de tal manera que podemos saber la amplitud de la frecuencia dada en un momento dado y adem\u00e1s podemos determinar la amplitud de varias frecuencias que se reproducen en un momento dado de una se\u00f1al de audio.\n\nPor otro lado, specshow() muestra el espectrograma con las siguientes caracter\u00edsticas: El eje vertical muestra las frecuencias (de 0 a 10kHz), y el eje horizontal muestra el tiempo del clip. Como vemos que toda la acci\u00f3n tiene lugar en el fondo del espectro, podemos convertir el eje de frecuencias en uno logar\u00edtmico.","e52eaf93":"# 2. Generaci\u00f3n de Espectrogramas","c4d8918e":"Representa la frecuencia a la que las frecuencias altas descienden a 0. Para obtener dicha frecuencia, hay que calcular la fracci\u00f3n en el espectro en la cual el 85% de la potencia espectral est\u00e1 a bajas frecuencias.\n\nlibrosa.feature.spectral_rolloff() calcula la frecuencia de caida para cada momento de la se\u00f1al","bd7dbd5e":"Finalmente, una vez obtenido \"preds\", definimos el 'dataset' que subiremos a la competici\u00f3n, d\u00e1ndole el formato requerido seg\u00fan las instrucciones de Rainforest Connection, y concluyendo as\u00ed el Notebook!","881c3c76":"El Ancho de Banda del espectro es el intervalo de longitudes de onda en el que una cantidad espectral no es inferior a la mitad de su valor m\u00e1ximo.","9de125f2":"# 6. Inferencia","34b677f6":"Los MFCCs son coe\ufb01cientes para la representaci\u00f3n del habla basados en la percepci\u00f3n auditiva humana. Estos surgen de la necesidad, en el \u00e1rea del reconocimiento de audio autom\u00e1tico, de extraer caracter\u00edsticas de las componentes de una se\u00f1al de audio que sean adecuadas para la identificaci\u00f3n de contenido relevante, as\u00ed como obviar todas aquellas que posean informaci\u00f3n poco valiosa como el ruido de fondo, emociones, volumen, tono, etc.","185d58cc":"# 4. Mel-Frequency Cepstral Coefficients (MFCCs)","6b162e0c":"## Atenuaci\u00f3n del espectro de frecuencias","357f75c1":"# 5. Modelado","4924e2d3":"## Ancho de banda del espectro de frecuencias","be532b4a":"Este notebook es una versi\u00f3n de los notebook de [kkiller](https:\/\/www.kaggle.com\/kneroma\/inference-resnest-rfcx-audio-detection) y de [Tarek Hamdi](https:\/\/www.kaggle.com\/hamditarek\/rainforest-connection-analysis-using-librosa).\n\nMi aportaci\u00f3n ha sido la traducci\u00f3n y la explicaci\u00f3n en espa\u00f1ol de que es lo que se consigue con este c\u00f3digo, utilizando los paquetes de librosa y pytorch principalmente.","bfa859ff":"### Usaremos los siguientes paquetes de an\u00e1lisis de audio:\n## Librosa\nEs un m\u00f3dulo de Python que analiza se\u00f1ales de audio en general. Incluye lo necesario para crear un sistema MIR (Music Information Retrieval), id\u00f3neo para el desarrollo de este an\u00e1lisis. Podemos encontrar la documentaci\u00f3n [aqu\u00ed](https:\/\/librosa.org\/librosa\/), con muchos ejemplos y tutoriales.\n\n## IPython.display.Audio\nQue nos permitir\u00e1 reproducir el audio directamente desde el \"Notebook\" para facilitar la comprensi\u00f3n del c\u00f3digo.","7a583fe2":"Estas 2 celdas de acontinuaci\u00f3n son las que generan los resultados, utilizando el 'dataset' [RFCX Species Detection Public Checkpoints](https:\/\/www.kaggle.com\/kneroma\/kkiller-rfcx-species-detection-public-checkpoints) de Kkiller para evaluar y guardando las predicciones obtenidas en el 'dataset' \"preds\"","19bce1cc":"# 1. Carga del archivo de audio","66e69c43":"Un Espectrograma es una manera de representar el audio de manera VISUAL. A lo largo del tiempo que dura dicho audio, se puede representar gr\u00e1ficamente las frecuencias que se han registrado.\n\nEl Espectograma puede mostrarse de distintas formas, ya sea como una gr\u00e1fica de l\u00edneas o como un 'Heatmap'","d7968946":"# 3. Creaci\u00f3n de la se\u00f1al de Audio"}}