{"cell_type":{"b0778502":"code","e29665ab":"code","8f7c0434":"code","8748eafc":"code","2deddbb2":"code","a1760ec7":"code","e5724208":"code","9ee9701e":"code","6ba1106b":"code","c748215c":"code","7835bc7e":"code","ea064c03":"code","df2fedab":"code","16ad66cb":"code","5e1e0810":"code","913d6f9b":"code","ae277ca6":"code","7d9804f2":"code","1697669e":"code","f03c1e65":"code","88597f0b":"code","1dfe5f68":"code","d98ddd96":"code","5d07e29c":"code","c161c8bc":"code","7dafe607":"code","02c592a6":"code","54a56aad":"code","17c2165c":"code","9ad3a90e":"code","f7908de8":"code","6c288f56":"code","8e94c8de":"code","b6627431":"markdown","f1b57eba":"markdown","3d286af1":"markdown","cdf17458":"markdown"},"source":{"b0778502":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","e29665ab":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns","8f7c0434":"df = pd.read_csv('\/kaggle\/input\/creditcardfraud\/creditcard.csv')","8748eafc":"df.head()","2deddbb2":"df.info()","a1760ec7":"df.describe()","e5724208":"fraud = len(df[df['Class']==1])\nnon_fraud = len(df[df['Class']==0])\nfraud_percent =  (fraud\/(fraud+non_fraud))*100\nprint('Number of Fraud transaction: ', fraud)\nprint('Number of non-Fraud transaction: ', non_fraud)\nprint('Percentage of Fraud transaction: ', np.round(fraud_percent, decimals=3))","9ee9701e":"labels = ['Genuine', 'Fraud']\ncount_classes = df.value_counts(df['Class'], sort=True)\ncount_classes.plot(kind='bar', rot=0, color='g')\nplt.title('Visualization of Labels')\nplt.ylabel('Count')\nplt.xticks(range(2), labels)\nplt.show()","6ba1106b":"from sklearn.preprocessing import RobustScaler\nrs = RobustScaler()\ndf['scaled_amount'] = rs.fit_transform(df['Amount'].values.reshape(-1,1))\ndf['scaled_time'] = rs.fit_transform(df['Time'].values.reshape(-1,1))","c748215c":"df.drop(columns=['Time', 'Amount'], inplace=True)","7835bc7e":"df","ea064c03":"scaled_amount = df['scaled_amount']\nscaled_time = df['scaled_time']\ndf.drop(columns=['scaled_amount', 'scaled_time'], inplace=True)\ndf.insert(0, 'scaled_amount', scaled_amount)\ndf.insert(0, 'scaled_time', scaled_time)","df2fedab":"df.head()","16ad66cb":"X = df.drop(columns=['Class'])\ny = df['Class']","5e1e0810":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","913d6f9b":"print('Shape of the x-train: ', X_train.shape)\nprint('Shape of the x-test: ', X_test.shape)","ae277ca6":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import precision_score, f1_score, accuracy_score, confusion_matrix, recall_score, precision_recall_curve","7d9804f2":"def metrics(actuals, predictions):\n    print('Accuracy: ', np.round(accuracy_score(actuals, predictions), decimals=4))\n    print('Precision: ', np.round(precision_score(actuals, predictions), decimals=4))\n    print('Recall: ', np.round(recall_score(actuals, predictions), decimals=4))    \n    print('F1-score ', np.round(f1_score(actuals, predictions), decimals=4))    ","1697669e":"dt = DecisionTreeClassifier()\ndt.fit(X_train,y_train)\ny_pred = dt.predict(X_test)\ncm = confusion_matrix(y_test, y_pred)\nsns.heatmap(cm, annot=True, fmt='g')\nplt.xlabel('Predicted Label')\nplt.ylabel('Actual Label')","f03c1e65":"from imblearn.over_sampling import SMOTE\nprint(\"Before OverSampling, counts of label '1': \", sum(y_train==1))\nprint(\"Before OverSampling, counts of label '0': \", sum(y_train==0))","88597f0b":"sm = SMOTE(random_state=3)\nX_train_s, y_train_s = sm.fit_resample(X_train, y_train)\n\nprint(\"After OverSampling, shape of the x-train: \", X_train_s.shape)\nprint(\"After OverSampling, shape of the y-train: \", y_train_s.shape)","1dfe5f68":"print(\"After OverSampling, counts of label '1': \", sum(y_train_s==1))\nprint(\"After OverSampling, counts of label '0': \", sum(y_train_s==0))","d98ddd96":"sns.countplot(x=y_train_s, data=df, palette='CMRmap')","5d07e29c":"dt = DecisionTreeClassifier()\ndt.fit(X_train, y_train)\ny_pred = dt.predict(X_test)\ncm = confusion_matrix(y_test, y_pred) \nmetrics(y_test, y_pred)\ndt_f1_score = np.round(f1_score(y_test, y_pred),decimals=4)","c161c8bc":"sns.heatmap(cm, annot=True, fmt='g')\nplt.xlabel('Actual Label')\nplt.ylabel('Predicted Label')","7dafe607":"y_pred_proba = dt.predict_proba(X_test)[:,1]\nprecision, recall, thresholds = precision_recall_curve(y_test, y_pred_proba)\nplt.plot(recall, precision)\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.title('Precision Recall Curve')","02c592a6":"rf = RandomForestClassifier()\nrf.fit(X_train,y_train)\ny_pred = rf.predict(X_test)\ncm = confusion_matrix(y_test, y_pred)\nmetrics(y_test, y_pred)\nrf_f1_score = np.round(f1_score(y_test, y_pred),decimals=4)","54a56aad":"sns.heatmap(cm, annot=True, fmt='g')\nplt.xlabel('Actual Label')\nplt.ylabel('Predicted Label')","17c2165c":"y_pred_proba = rf.predict_proba(X_test)[:,-1]\nprecision, recall, thresholds = precision_recall_curve(y_test, y_pred_proba)\nplt.plot(recall, precision)\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.title('Precision Recall Curve')","9ad3a90e":"logr = LogisticRegression(solver='liblinear')\nlogr.fit(X_train, y_train)\ny_pred = logr.predict(X_test)\ncm = confusion_matrix(y_test, y_pred)\nmetrics(y_test, y_pred)\nlogr_f1_score = np.round(f1_score(y_test, y_pred),decimals=4)","f7908de8":"sns.heatmap(cm, annot=True, fmt='g')\nplt.xlabel('Predicited Label')\nplt.ylabel('Actual Label')","6c288f56":"y_pred_proba = logr.predict_proba(X_test)[:,-1]\nprecision, recall, thresholds = precision_recall_curve(y_test, y_pred_proba)\nplt.plot(recall, precision)\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.title('Precision Recall Curve')","8e94c8de":"models = ['Decision Tree', 'Random Forest', 'Logistic Regression']\nresults = [dt_f1_score, rf_f1_score, logr_f1_score]\nprint(results)\nplt.figure(figsize=(10, 7))\nplt.bar(models, results)\nplt.xlabel('Models',fontsize=15)\nplt.ylabel('Scores',fontsize=15)\nplt.title('Comparision between Models', fontsize=20)\nplt.legend()\nplt.show()","b6627431":"### Now data is balanced. So we are applying classification models.","f1b57eba":"## We found that Random Forest Classifier gave us the better results than other classifier.","3d286af1":"### For Classification problem, we are importing the classifier models.","cdf17458":"### Our dataset is not balance. We try some models with imbalanced data."}}