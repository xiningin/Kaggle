{"cell_type":{"7374d9e5":"code","5aad4cbe":"code","d6467bb7":"code","29243ab5":"code","a81ded3e":"code","7baa2aa0":"code","7cf4418c":"code","28c95315":"code","ba7bce2c":"code","2b385649":"code","b8d24e5c":"code","ed1f25c3":"code","0cc80c1d":"code","0297610b":"code","477033d0":"code","5de2534a":"code","52b6fdaa":"code","1c7a39f8":"code","5ba86657":"code","822b072a":"code","9c5b93f8":"code","6c88fd4d":"code","0cc5f0a8":"code","a2b0cc4d":"code","4c091457":"code","810fe01d":"markdown","00b99269":"markdown","889295ac":"markdown","123ee288":"markdown","d91c0236":"markdown","5adef1bc":"markdown","f45a818c":"markdown","d5fafaa2":"markdown","d62cd996":"markdown","df35045f":"markdown","3e6bea3d":"markdown","2f914b3c":"markdown","3474fbab":"markdown"},"source":{"7374d9e5":"#import cv2\nimport gc\nimport glob\nimport os\nimport json\nimport matplotlib.pyplot as plt\nimport warnings\nimport re\n\nimport numpy as np\nimport pandas as pd\n\n#from PIL import Image\n\n%matplotlib inline\n%config InlineBackend.figure_format = 'svg'\n\nwarnings.filterwarnings(\"ignore\")\n\nplt.rcParams['figure.figsize'] = (12, 9)\nplt.style.use('ggplot')\n\npd.options.display.max_rows = 64\npd.options.display.max_columns = 512","5aad4cbe":"train = pd.read_csv('..\/input\/train\/train.csv')\ntrain['AdoptionSpeed'].astype(np.int32)\ntest = pd.read_csv('..\/input\/test\/test.csv')\ndf = pd.concat([train,test],ignore_index=True)","d6467bb7":"train_sentiment_files = sorted(glob.glob('..\/input\/train_sentiment\/*.json'))\ntest_sentiment_files = sorted(glob.glob('..\/input\/test_sentiment\/*.json'))\nsentimental_analysis = train_sentiment_files + test_sentiment_files","29243ab5":"score=[]\nmagnitude=[]\npetid=[]\nfor filename in sentimental_analysis:\n    with open(filename, 'r') as f:\n        sentiment_file = json.load(f)\n        file_sentiment = sentiment_file['documentSentiment']\n        file_score =  sentiment_file['documentSentiment']['score']\n        file_magnitude = sentiment_file['documentSentiment']['magnitude']\n        score.append(file_score)\n        magnitude.append(file_magnitude)\n        petid.append(filename.replace('.json','').replace('..\/input\/train_sentiment\/', '').replace('..\/input\/test_sentiment\/', ''))","a81ded3e":"score_dict = dict(zip(petid,score))\nmagnitude_dict = dict(zip(petid,magnitude))","7baa2aa0":"df['Score'] = df['PetID'].map(score_dict)\ndf['Score'][df.Score.isnull()] = 0\ndf['Magnitude'] = df['PetID'].map(magnitude_dict)\ndf['Magnitude'][df.Magnitude.isnull()] = 0\ndf.set_index('PetID',inplace=True)","7cf4418c":"def namevaild(name):\n    if name == np.nan:\n        return 0\n    elif len(str(name)) < 3:\n        return 1\n    elif re.match(u'[0-9]', str(name).lower()):\n        return 1\n    elif len(set(str(name).lower().split(' ')+['no','not','yet','male','female','unnamed'])) != len(set(str(name).lower().split(' ')))+6:\n        return 1\n    else:\n        return 2\ndf['Name_state'] = df['Name'].apply(namevaild)","28c95315":"df['Fee_per_pet'] = df.Fee\/df.Quantity\n\ndf['Fee_Bin']=pd.factorize(pd.cut(df.Fee_per_pet,bins=[0,0.01,50,100,200,500,3000],right=False))[0]\nfee_bin_dummies_df = pd.get_dummies(df['Fee_Bin']).rename(columns=lambda x: 'Fee_Bin_' + str(x))\ndf = pd.concat([df, fee_bin_dummies_df], axis=1)","ba7bce2c":"df['Quantity_Bin']=pd.factorize(pd.cut(df.Quantity,bins=[1,2,4,22],right=False))[0]\nquantity_bin_dummies_df = pd.get_dummies(df['Quantity_Bin']).rename(columns=lambda x: 'Quantity_Bin_' + str(x))\ndf = pd.concat([df, quantity_bin_dummies_df], axis=1)","2b385649":"df.VideoAmt = df.VideoAmt.apply(lambda x: 1 if x > 0 else 0)\n\ndf['PhotoAmt_Bin']=pd.factorize(pd.cut(df.PhotoAmt,bins=[0,1,2,4,31],right=False))[0]\nphoto_bin_dummies_df = pd.get_dummies(df['PhotoAmt_Bin']).rename(columns=lambda x: 'PhotoAmt_Bin_' + str(x))\ndf = pd.concat([df, photo_bin_dummies_df], axis=1)","b8d24e5c":"def map_state(state):\n    if state == 41326:\n        return 'Selangor'\n    elif state == 41401:\n        return 'Kuala_Lumpur'\n    else:\n        return 'Other_State'\ndf['State_Bin'] = df.State.apply(map_state)\nstate_bin_dummies_df = pd.get_dummies(df['State_Bin']).rename(columns=lambda x: 'State_' + str(x))\ndf = pd.concat([df, state_bin_dummies_df], axis=1)","ed1f25c3":"rescuer_dict = df.RescuerID.value_counts().to_dict()\ndf['Rescuer_Num'] = df.RescuerID.map(rescuer_dict)\n#df['Rescuer_Bin']=pd.factorize(pd.cut(df.Rescuer_Num,bins=[1,2,5],right=False))[0]\n#df['Rescuer_Bin'].value_counts()\n#rescuer_bin_dummies_df = pd.get_dummies(df['Rescuer_Bin']).rename(columns=lambda x: 'Rescuer_Bin_' + str(x))\n#df = pd.concat([df, rescuer_bin_dummies_df], axis=1)","0cc80c1d":"breeds = pd.read_csv('..\/input\/breed_labels.csv')\nbreeds_dict = {k: v for k, v in zip(breeds['BreedID'], breeds['BreedName'])}\ndf['Breed1_name'] = df['Breed1'].apply(lambda x: '_'.join(breeds_dict[x].split()) if x in breeds_dict else 'NA')\ndf['Breed2_name'] = df['Breed2'].apply(lambda x: '_'.join(breeds_dict[x].split()) if x in breeds_dict else 'NA')","0297610b":"df['Breed'] = df['Breed1_name'] + '--' + df['Breed2_name']\ndef mix_breed(string):\n    breed = string.split('--')\n    if breed[0] in ['Mixed_Breed','NA']:\n        return 1\n    elif breed[1] == 'Mixed_Breed':\n        return 1\n    elif breed[1] == 'NA':\n        return 0\n    elif breed[0] != breed[1]:\n        return 1\n    else:\n        return 0\ndf['Mixed_Breed'] = df.Breed.apply(mix_breed)","477033d0":"'''\ndf.Description[df.Description.isnull()] = ''\ndes_list = df.Description.values.tolist()\n'''","5de2534a":"'''\nimport unicodedata\nimport re\n\nfrom nltk import word_tokenize, sent_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem import LancasterStemmer, WordNetLemmatizer\nfrom nltk.corpus import stopwords\n\ndef remove_non_ascii(words):\n    \"\"\"Remove non-ASCII characters from list of tokenized words\"\"\"\n    new_words = []\n    for word in words:\n        new_word = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n        new_words.append(new_word)\n    return new_words\n\ndef to_lowercase(words):\n    \"\"\"Convert all characters to lowercase from list of tokenized words\"\"\"\n    new_words = []\n    for word in words:\n        new_word = word.lower()\n        new_words.append(new_word)\n    return new_words\n\ndef remove_punctuation(words):\n    \"\"\"Remove punctuation from list of tokenized words\"\"\"\n    new_words = []\n    for word in words:\n        new_word = re.sub(r'[^\\w\\s]', '', word)\n        if new_word != '':\n            new_words.append(new_word)\n    return new_words\n'''\n\n'''\ndef replace_numbers(words):\n    \"\"\"Replace all interger occurrences in list of tokenized words with textual representation\"\"\"\n    p = inflect.engine()\n    new_words = []\n    for word in words:\n        if word.isdigit():\n            new_word = p.number_to_words(word)\n            new_words.append(new_word)\n        else:\n            new_words.append(word)\n    return new_words \n'''\n'''\ndef remove_stopwords(words):\n    \"\"\"Remove stop words from list of tokenized words\"\"\"\n    new_words = []\n    for word in words:\n        if word not in stopwords.words('english'):\n            new_words.append(word)\n    return new_words\n\ndef stem_words(words):\n    \"\"\"Stem words in list of tokenized words\"\"\"\n    stemmer = LancasterStemmer()\n    stems = []\n    for word in words:\n        stem = stemmer.stem(word)\n        stems.append(stem)\n    return stems\n\ndef lemmatize_verbs(words):\n    \"\"\"Lemmatize verbs in list of tokenized words\"\"\"\n    lemmatizer = WordNetLemmatizer()\n    lemmas = []\n    for word in words:\n        lemma = lemmatizer.lemmatize(word, pos='v')\n        lemmas.append(lemma)\n    return lemmas\n\ndef normalize(words):\n    words = remove_non_ascii(words)\n    words = to_lowercase(words)\n    words = remove_punctuation(words)\n    #words = replace_numbers(words)\n    words = remove_stopwords(words)\n    #words = stem_words(words)\n    words = lemmatize_verbs(words)\n    return words\n\n'''\n'''\nword_bag = []\n\nfor i,item in enumerate(des_list):\n    words = word_tokenize(item)\n    words = normalize(words)\n    word_bag.append(words)\ndf['Word_bag'] = word_bag\n\ndef wordjoin(x):\n    return ' '.join(x)\n\ndf['Word_list'] = df['Word_bag'].apply(wordjoin)\n'''\n","52b6fdaa":"'''\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer\n\nvectorizer = CountVectorizer(min_df = 0.02)\ntransformer=TfidfTransformer()\ntfidf = transformer.fit_transform(vectorizer.fit_transform(df.Word_list))\nweight=tfidf.toarray()\n'''","1c7a39f8":"'''\nfrom sklearn.decomposition import PCA\n\nn_components = 25\npca = PCA(n_components=n_components, random_state=42)\npca.fit(weight)\ntext_feature = pca.transform(weight)\n\ncolumns = []\nfor i in range(n_components):\n    columns.append('text_feature_'+str(i+1))\n'''","5ba86657":"'''\ndf = pd.concat([df,pd.DataFrame(text_feature, index = df.index, columns = columns)],axis = 1)\n'''","822b072a":"df.head()","9c5b93f8":"df_copy = df.drop(columns=['Description','Fee','Fee_per_pet','Name','PhotoAmt','Quantity','RescuerID','State','State_Bin','Fee_Bin','Quantity_Bin','PhotoAmt_Bin','Breed','Breed1_name','Breed2_name'])\ntrain = df_copy[df.AdoptionSpeed.notnull()]\ntest  = df_copy[df.AdoptionSpeed.isnull()]\nprint(train.shape, test.shape)","6c88fd4d":"train.head()","0cc5f0a8":"X_train = train.drop(columns=['AdoptionSpeed'])\ny_train = train.AdoptionSpeed\nX_test = test.drop(columns=['AdoptionSpeed'])","a2b0cc4d":"from sklearn.ensemble import RandomForestClassifier\nrf = RandomForestClassifier(n_estimators = 600, max_depth=None, criterion='gini')\nrf.fit(X_train,y_train)\ny_predict = rf.predict(X_test).astype(np.int32)\nsubmission = pd.DataFrame({'PetID': test.index, 'AdoptionSpeed': y_predict})\nsubmission = submission[['PetID','AdoptionSpeed']]\nsubmission.head()","4c091457":"submission.to_csv('submission.csv', index=False)","810fe01d":"#### Mix or Pure\nWe consider a cat\/dog is mixed breed if:\n1. Breed1_name or Breed2_name is Mixed_Breed\n2. Breed1_name is NA\n3. Breed1_name != Breed2_name","00b99269":"### Description\n\nInterestingly, adding this text features actually damages the prediction. I have find any explainations, if you have any ideas, fell free to comment below.","889295ac":"hi everyone, this is a baseline submission that haven't strated considering information from pictures. Fell free to ask me any questions or make suggestions. Thanks.","123ee288":"### Breed","d91c0236":"## Load Data","5adef1bc":"### Quantity\n\nBinning to [1,2,4,22]","f45a818c":"### Name \nCategorize to with meaningful name, with meaningless name and without name.\n\n#### Meaningless Rule\n1. 1 or 2 letters\n2. With the word \"NO\" \"NOT\" \"YET\" \"NAME\"\n3. Start with numbers","d5fafaa2":"### Fee\n\nBinning into 0, (0,50], (50,100], (100,200], (200,500], (500, +inf)","d62cd996":"## Baseline","df35045f":"### State","3e6bea3d":"### VideoAmt & PhotoAmt","2f914b3c":"### Rescuer\nBinning the saving number of animals in total","3474fbab":"## Core features"}}