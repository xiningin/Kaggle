{"cell_type":{"16bd5b8c":"code","6071c204":"code","0a656de1":"code","a3226a5e":"code","afcbd51b":"code","5fef9bf9":"code","aac8c565":"code","b306241e":"code","516d5dac":"code","1fb105b8":"code","b41cf943":"code","76da9456":"code","6f969b0d":"code","d27f9c60":"code","b0b4677f":"code","fe1aefdf":"code","9f08c162":"code","c8f07955":"code","f953812b":"code","3fc0b18f":"code","0eca1d11":"code","db9d20a2":"code","22c7ebc7":"code","2c0841b6":"code","e9f4fc96":"code","7df28ade":"code","0ea53309":"code","9ab72807":"code","cd588519":"code","eb2f409b":"code","c6c0a2ae":"code","05097bc2":"code","063289f6":"code","4db2de57":"code","d05f51e7":"code","b30cddac":"code","3bfd6266":"code","66278a9f":"code","a8431553":"code","166d0926":"code","7416569a":"code","b47ce2a5":"code","7b86ae59":"code","2a1f171b":"code","bfac6648":"code","af88e8ad":"code","5812d404":"markdown","58e1bf87":"markdown","94900665":"markdown","26ca6bfc":"markdown","9d1c4e1b":"markdown","66403201":"markdown","25111d90":"markdown","077854ae":"markdown","77b05c56":"markdown","60ce1066":"markdown","db4637af":"markdown","ee76f9ef":"markdown","762eb3af":"markdown","4c9297a6":"markdown","b2a02b97":"markdown","3fb3b20d":"markdown","92d89081":"markdown","e926f088":"markdown","2d01b372":"markdown"},"source":{"16bd5b8c":"!pip install hmmlearn -q","6071c204":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os, random\n\nimport matplotlib.pyplot as plt\nfrom matplotlib.ticker import MaxNLocator\nimport seaborn as sns\nsns.set_style('whitegrid')\n\nfrom hmmlearn import hmm","0a656de1":"SEED = 2021\nnp.random.seed(SEED)\nrandom.seed(SEED)","a3226a5e":"# load dataset\nset1 = pd.read_csv('..\/input\/nasa-bearing-time-features\/set1_timefeatures.csv')\nset1.columns = ['date'] + list(set1.columns[1:])\nset1['date'] = pd.to_datetime(set1['date'])\n\nset2 = pd.read_csv('..\/input\/nasa-bearing-time-features\/set2_timefeatures.csv')\nset2.columns = ['date'] + list(set2.columns[1:])\nset2['date'] = pd.to_datetime(set2['date'])\n\nset3 = pd.read_csv('..\/input\/nasa-bearing-time-features\/set3_timefeatures.csv')\nset3.columns = ['date'] + list(set3.columns[1:])\nset3['date'] = pd.to_datetime(set3['date'])\n\n# set date as index\nset1 = set1.set_index('date')\nset2 = set2.set_index('date')\nset3 = set3.set_index('date')\n\nset1.head()","afcbd51b":"# merge a and b from bearing 1-4\nset1['B1_mean'] = (set1['B1_a_mean'] + set1['B1_b_mean'])\/2\nset1['B1_std'] = (set1['B1_a_std'] + set1['B1_b_std'])\/2\nset1['B1_skew'] = (set1['B1_a_skew'] + set1['B1_b_skew'])\/2\nset1['B1_kurtosis'] = (set1['B1_a_kurtosis'] + set1['B1_b_kurtosis'])\/2\nset1['B1_entropy'] = (set1['B1_a_entropy'] + set1['B1_b_entropy'])\/2\nset1['B1_rms'] = (set1['B1_a_rms'] + set1['B1_b_rms'])\/2\nset1['B1_max'] = (set1['B1_a_max'] + set1['B1_b_max'])\/2\nset1['B1_p2p'] = (set1['B1_a_p2p'] + set1['B1_b_p2p'])\/2\n\nset1['B2_mean'] = (set1['B2_a_mean'] + set1['B2_b_mean'])\/2\nset1['B2_std'] = (set1['B2_a_std'] + set1['B2_b_std'])\/2\nset1['B2_skew'] = (set1['B2_a_skew'] + set1['B2_b_skew'])\/2\nset1['B2_kurtosis'] = (set1['B2_a_kurtosis'] + set1['B2_b_kurtosis'])\/2\nset1['B2_entropy'] = (set1['B2_a_entropy'] + set1['B2_b_entropy'])\/2\nset1['B2_rms'] = (set1['B2_a_rms'] + set1['B2_b_rms'])\/2\nset1['B2_max'] = (set1['B2_a_max'] + set1['B2_b_max'])\/2\nset1['B2_p2p'] = (set1['B2_a_p2p'] + set1['B2_b_p2p'])\/2\n\nset1['B3_mean'] = (set1['B3_a_mean'] + set1['B3_b_mean'])\/2\nset1['B3_std'] = (set1['B3_a_std'] + set1['B3_b_std'])\/2\nset1['B3_skew'] = (set1['B3_a_skew'] + set1['B3_b_skew'])\/2\nset1['B3_kurtosis'] = (set1['B3_a_kurtosis'] + set1['B3_b_kurtosis'])\/2\nset1['B3_entropy'] = (set1['B3_a_entropy'] + set1['B3_b_entropy'])\/2\nset1['B3_rms'] = (set1['B3_a_rms'] + set1['B3_b_rms'])\/2\nset1['B3_max'] = (set1['B3_a_max'] + set1['B3_b_max'])\/2\nset1['B3_p2p'] = (set1['B3_a_p2p'] + set1['B3_b_p2p'])\/2\n\nset1['B4_mean'] = (set1['B4_a_mean'] + set1['B4_b_mean'])\/2\nset1['B4_std'] = (set1['B4_a_std'] + set1['B4_b_std'])\/2\nset1['B4_skew'] = (set1['B4_a_skew'] + set1['B4_b_skew'])\/2\nset1['B4_kurtosis'] = (set1['B4_a_kurtosis'] + set1['B4_b_kurtosis'])\/2\nset1['B4_entropy'] = (set1['B4_a_entropy'] + set1['B4_b_entropy'])\/2\nset1['B4_rms'] = (set1['B4_a_rms'] + set1['B4_b_rms'])\/2\nset1['B4_max'] = (set1['B4_a_max'] + set1['B4_b_max'])\/2\nset1['B4_p2p'] = (set1['B4_a_p2p'] + set1['B4_b_p2p'])\/2\n\nset1 = set1[['B1_mean','B1_std','B1_skew','B1_kurtosis','B1_entropy','B1_rms','B1_max','B1_p2p',\n             'B2_mean','B2_std','B2_skew','B2_kurtosis','B2_entropy','B2_rms','B2_max','B2_p2p',\n             'B3_mean','B3_std','B3_skew','B3_kurtosis','B3_entropy','B3_rms','B3_max','B3_p2p',\n             'B4_mean','B4_std','B4_skew','B4_kurtosis','B4_entropy','B4_rms','B4_max','B4_p2p']]\nset1.head()","5fef9bf9":"cols = ['B1_mean','B1_rms','B1_skew','B1_kurtosis', \n        'B2_mean','B2_rms','B2_skew','B2_kurtosis',\n        'B3_mean','B3_rms','B3_skew','B3_kurtosis',\n        'B4_mean','B4_rms','B4_skew','B4_kurtosis',]\nset1 = set1[cols]\nset2 = set2[cols]\nset3 = set3[cols]","aac8c565":"# statistics\nset1.describe().T","b306241e":"# statistics\nset2.describe().T","516d5dac":"# statistics\nset3.describe().T","1fb105b8":"def plot_features(df):\n    fig, axes = plt.subplots(4, 1, figsize=(15, 5*4))\n    \n    axes[0].plot(df['B1_mean'])\n    axes[0].plot(df['B2_mean'])   \n    axes[0].plot(df['B3_mean'])\n    axes[0].plot(df['B4_mean'])\n    axes[0].legend(['B1','B2','B3','B4'])\n    axes[0].set_title('Mean')\n    \n    axes[1].plot(df['B1_rms'])\n    axes[1].plot(df['B2_rms'])   \n    axes[1].plot(df['B3_rms'])\n    axes[1].plot(df['B4_rms'])\n    axes[1].legend(['B1','B2','B3','B4'])\n    axes[1].set_title('RMS')\n    \n    axes[2].plot(df['B1_skew'])\n    axes[2].plot(df['B2_skew'])   \n    axes[2].plot(df['B3_skew'])\n    axes[2].plot(df['B4_skew'])\n    axes[2].legend(['B1','B2','B3','B4'])\n    axes[2].set_title('Skewness')\n    \n    axes[3].plot(df['B1_kurtosis'])\n    axes[3].plot(df['B2_kurtosis'])   \n    axes[3].plot(df['B3_kurtosis'])\n    axes[3].plot(df['B4_kurtosis'])\n    axes[3].legend(['B1','B2','B3','B4'])\n    axes[3].set_title('Kurtosis')","b41cf943":"# plot set 1\nplot_features(set1)","76da9456":"# plot set 2\nplot_features(set2)","6f969b0d":"# plot set 3\nplot_features(set3)","d27f9c60":"def slice_columns(columns, target='B1'):\n    if target == 'B1':\n        return columns[0:4]\n    elif target == 'B2':\n        return columns[4:8]\n    elif target == 'B3':\n        return columns[8:12]\n    elif target == 'B4':\n        return columns[12:]","b0b4677f":"# with scikit-learn\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler\n\nset1_scaled = set1.copy()\nset2_scaled = set2.copy()\nset3_scaled = set3.copy()\n\nss_l = []\nminmax_l = []\n\n# scaling set test 1\nfor bear in ['B1','B2','B3','B4']:\n    col_features = slice_columns(set1.columns, target=bear)\n    scaler = StandardScaler()\n#     scaler = MinMaxScaler(feature_range=(-2,2))\n    set1_scaled[col_features] = scaler.fit_transform(set1[col_features])\n    ss_l.append(scaler)\n#     minmax_l.append(scaler)\n    \n# scaling set test 2\nfor bear in ['B1','B2','B3','B4']:\n    col_features = slice_columns(set2.columns, target=bear)\n    scaler = StandardScaler()\n#     scaler = MinMaxScaler(feature_range=(-2,2))\n    set2_scaled[col_features] = scaler.fit_transform(set2[col_features])\n    ss_l.append(scaler)\n#     minmax_l.append(scaler)\n\n# scaling set test 3, except bearing 3\nfor bear in ['B1','B2','B4']:\n    col_features = slice_columns(set3.columns, target=bear)\n    scaler = StandardScaler()\n#     scaler = MinMaxScaler(feature_range=(-2,2))\n    set3_scaled[col_features] = scaler.fit_transform(set3[col_features])#.round(4)\n    ss_l.append(scaler)\n#     minmax_l.append(scaler)","fe1aefdf":"# before and after scaling\nfig, axes = plt.subplots(1, 2, figsize=(14,5), dpi=80)\naxes[0].plot(set1['B1_mean'])\naxes[1].plot(set1_scaled['B1_mean'])\n\naxes[0].set_title('Before scaling (Mean)')\naxes[1].set_title('After scaling (Mean)');","9f08c162":"def flip_transmat(tm, ix_sort):\n    tm_ = tm.copy()\n    for i,ix in enumerate(ix_sort):\n        tm_[i, :] = tm[ix[0], :]\n    tm__ = tm_.copy()\n    for i,ix in enumerate(ix_sort):\n        tm__[:, i] = tm_[:,ix[0]]\n    return tm__","c8f07955":"def random_transitions(n_states) -> np.ndarray:\n    \"\"\"Sets the transition matrix as random (random probability of transitioning\n    to all other possible states from each state) by sampling probabilities\n    from a Dirichlet distribution, according to the topology.\n    Returns\n    -------\n    transitions: :class:`numpy:numpy.ndarray` (float)\n        The random transition matrix of shape `(n_states, n_states)`.\n    \"\"\"\n    transitions = np.zeros((n_states, n_states))\n    for i, row in enumerate(transitions):\n        row[i:] = np.random.dirichlet(np.ones(n_states - i))\n    return transitions\n\ndef create_gmmhmm():\n    startprob = np.array([1., 0., 0.], dtype=np.float64)\n    transmat = np.array([[0.9995, 0.0005,  0.],\n                         [0.,     0.9998,  0.0002],\n                         [0.,     0.,      1.0]], dtype=np.float64)\n#     transmat = np.array([[0.38903512, 0.28715641, 0.32380848],\n#                          [0.        , 0.56796488, 0.43203512],\n#                          [0.        , 0.        , 1.        ]], dtype=np.float64)\n#     transmat = random_transitions(n_states=3)\n    \n    model = hmm.GMMHMM(n_components=3, \n                       n_mix=2, \n                       covariance_type=\"tied\", \n                       n_iter=1000, \n                       tol=1e-6,\n#                        startprob_prior=startprob,\n#                        transmat_prior=transmat,\n#                        init_params='cmw',\n#                        params='stcmw',\n                       random_state=SEED,\n                       verbose=False)\n    model.n_features = 4\n    model.startprob_ = startprob\n    model.transmat_ = transmat\n    \n    return model\n\ndef fit_gmmhmm(model, data):\n    \n    # train model\n    model.fit(data)\n\n    # clasify each observation as state (0, 1, 2)\n    hidden_states = model.predict(data)\n    \n    # get parameters of GMMHMM\n    startprob_ = model.startprob_\n    means_ = model.means_\n    transmat_ = model.transmat_\n    covars_ = model.covars_\n    weights_ = model.weights_\n    \n    # reorganize by mean, so the the order of the states from lower to higher\n    ix_sort = np.argsort(np.array([[np.mean(m)] for m in means_]), axis=0)\n    \n    hidden_states = np.array([ix_sort[st][0] for st in hidden_states])\n    startprob = np.array([startprob_[ix][0] for ix in ix_sort])\n    means = np.array([means_[ix][0] for ix in ix_sort])\n    transmat = flip_transmat(transmat_, ix_sort)\n    covars = np.array([covars_[ix][0] for ix in ix_sort])\n    weights = np.array([weights_[ix][0] for ix in ix_sort])\n    \n    model.startprob_ = startprob\n    model.means_ = means\n    model.transmat_ = transmat\n    model.covars_ = covars\n    model.weights_ = weights\n    \n    # logprob\n    logprob = model.score(data)\n    \n    return ix_sort, logprob, model, hidden_states","f953812b":"def plot_rms_and_state(rms, state):\n    fig, ax1 = plt.subplots(figsize=(12,5))\n    ax2 = ax1.twinx()\n    ax1.plot(rms)\n    ax1.set_ylabel('RMS')\n    ax1.set_xlabel('time (minutes)')\n    ax2.plot(state, color='red')\n\n    ax2.set_yticks(range(0,3))\n    ax2.set_ylabel('state', rotation=270, labelpad=20)\n    plt.show();","3fc0b18f":"# example on bearing 1 in the test 1\n\n# initialize gmmhmm\ngmmhmm = create_gmmhmm()\n\n# setup data\ncol_features = slice_columns(set1_scaled.columns, target='B1')\ndata = set1_scaled[col_features]\n\n# train gmmhmm\nix_sort, logprob, model, hidden_states = fit_gmmhmm(gmmhmm, data)\n\nprint(f'Log probability: {np.around(logprob, decimals=4)}')\nprint(f'Start probability: {np.around(model.startprob_, decimals=4)}')\nprint(f'Means:\\n{np.around(model.means_, decimals=4)}')\nprint(f'Transition Matrix:\\n{np.around(model.transmat_, decimals=3)}')\nprint(f'Covariance matrix:\\n{np.around(model.covars_, decimals=4)}')\nprint(f'Weights:\\n{np.around(model.weights_, decimals=4)}')\nprint(ix_sort)\n\n# plot RMS and state\nplot_rms_and_state(data[data.columns[1]].values, hidden_states)","0eca1d11":"%%time\n# model GMMHMM\nmodel_gmmhmm = []\n\n# train set test 1\nfor bear in ['B1','B2','B3','B4']:\n    gmmhmm = create_gmmhmm()  # create model\n    col_features = slice_columns(set1_scaled.columns, target=bear)\n    ix_sort, logprob, model, hidden_states = fit_gmmhmm(gmmhmm, set1_scaled[col_features])  # train\n    \n    # append model into list\n    model_gmmhmm.append((ix_sort, logprob, model, hidden_states))\n    \n# train set test 2\nfor bear in ['B1','B2','B3','B4']:\n    gmmhmm = create_gmmhmm()  # create model\n    col_features = slice_columns(set2_scaled.columns, target=bear)\n    ix_sort, logprob, model, hidden_states = fit_gmmhmm(gmmhmm, set2_scaled[col_features])  # train\n    \n    # append model into list\n    model_gmmhmm.append((ix_sort, logprob, model, hidden_states))\n    \n# train set test 3, except bearing 3\nfor bear in ['B1','B2','B4']:\n    gmmhmm = create_gmmhmm()  # create model\n    col_features = slice_columns(set3_scaled.columns, target=bear)\n    ix_sort, logprob, model, hidden_states = fit_gmmhmm(gmmhmm, set3_scaled[col_features])  # train\n    \n    # append model into list\n    model_gmmhmm.append((ix_sort, logprob, model, hidden_states))","db9d20a2":"# get rms data for each bearing test, except set test 3 bearing 3 (S3_B3) because test set\nrms_data = []\nfor bear in ['B1','B2','B3','B4']:\n    col_features = slice_columns(set1_scaled.columns, target=bear)\n    data = set1[col_features]\n    rms = data[data.columns[1]]\n    rms_data.append(rms)\nfor bear in ['B1','B2','B3','B4']:\n    col_features = slice_columns(set2_scaled.columns, target=bear)\n    data = set2[col_features]\n    rms = data[data.columns[1]]\n    rms_data.append(rms)\nfor bear in ['B1','B2','B4']:\n    col_features = slice_columns(set3_scaled.columns, target=bear)\n    data = set3[col_features]\n    rms = data[data.columns[1]]\n    rms_data.append(rms)\n\n# sequence data for training\nmodel_data = ['S1_B1','S1_B2','S1_B3','S1_B4',\n              'S2_B1','S2_B2','S2_B3','S2_B4',\n              'S3_B1','S3_B2','S3_B4']\n\nfor (ix_sort,logprob,model,hidden_states),data,rms in zip(model_gmmhmm, model_data, rms_data):\n    print(f'Sequence data: {data}')\n    print(f'Logprob\\n {np.around(logprob, decimals=4)}')\n    print(f'Start Probability {np.around(model.startprob_, decimals=4)}')\n    print(f'Means\\n {np.around(model.means_, decimals=4)}')\n    print(f'Transition Matrix\\n {np.around(model.transmat_, decimals=3)}')\n    print(f'Mixture Weights\\n {np.around(model.weights_, decimals=4)}')\n    print(f'Covariance matrix:\\n{np.around(model.covars_, decimals=4)}')\n\n    # plot RMS and state\n    plot_rms_and_state(rms.values, hidden_states)\n    print()","22c7ebc7":"# logprob and plot decode state on S3_B3\nlogprob_l = []\ni = 0\nfor (ix,logprob,model,hidden_states),data in zip(model_gmmhmm, model_data):\n    # select features\n    col_features = slice_columns(set3.columns, target='B3')\n    \n    # scaling\n    S3_B3 = ss_l[i].transform(set3[col_features])\n    i += 1\n    \n    # calculate logprob on model sample\n    logprob = model.score(S3_B3)\n    pred = model.predict(S3_B3)\n    \n    logprob_l.append(logprob)\n\n    print(f'GMMHMM model from {data} got log probability on S3_B3: {logprob}')\n    rms = set3[col_features]['B3_rms']\n    \n    # plot RMS and decode state\n    plot_rms_and_state(rms.values, pred)\n    print()","2c0841b6":"# example exponential on negative log probability\nnp.exp(-100), np.exp(-150)","e9f4fc96":"# index best model\ni = 4\n\n# select best model\nix, logprob, model, hidden_states = model_gmmhmm[i]\n\n# transform data\nS3_B3 = ss_l[i].transform(set3[col_features])\nmodel.score(S3_B3)","7df28ade":"def calculate_mean(durations):\n    return np.mean(durations)\n\ndef calculate_std(durations):\n    return np.std(durations)","0ea53309":"import math\n\nD_S1 = [0.458, 0.286]\nD_S2 = [0.393, 0.334]\nD_S3 = [0.357, 0.328]\n\nprint(f\"S1 = [mean(S1), std(S1)] = [{math.floor(calculate_mean(D_S1)*10000)}, {math.floor(calculate_std(D_S1)*10000)}]\")\nprint(f\"S2 = [mean(S2), std(S2)] = [{math.floor(calculate_mean(D_S2)*10000)}, {math.floor(calculate_std(D_S2)*10000)}]\")\nprint(f\"S3 = [mean(S3), std(S3)] = [{math.floor(calculate_mean(D_S3)*10000)}, {math.ceil(calculate_std(D_S3)*10000)}]\")","9ab72807":"result = {'S1': {'mean': int(calculate_mean(D_S1)*10000),\n                 'std': int(calculate_std(D_S1)*10000)},\n          'S2': {'mean': int(calculate_mean(D_S2)*10000),\n                 'std': int(calculate_std(D_S2)*10000)},\n          'S3': {'mean': int(calculate_mean(D_S3)*10000),\n                 'std': math.ceil(calculate_std(D_S3)*10000)}\n         }\npath = ['S2','S1','S2','S3','S1','S3']\n\nresult, path","cd588519":"def calculate_rul(path, result, conf=0.95):\n    rul = []\n    \n    cum_rul = 0\n    for p in path:\n        if p == 'S1':\n            rul_upper = result.get('S1').get('mean') + conf * result.get('S1').get('std') + cum_rul\n            rul_mean = result.get('S1').get('mean') + cum_rul\n            rul_lower = result.get('S1').get('mean') - conf * result.get('S1').get('std') + cum_rul\n            \n            cum_rul += result.get('S1').get('mean')\n        elif p == 'S2':\n            rul_upper = result.get('S2').get('mean') + conf * result.get('S2').get('std') + cum_rul\n            rul_mean = result.get('S2').get('mean') + cum_rul\n            rul_lower = result.get('S2').get('mean') - conf * result.get('S2').get('std') + cum_rul\n            \n            cum_rul += result.get('S2').get('mean')\n        elif p == 'S3':\n            rul_upper = result.get('S3').get('mean') + conf * result.get('S3').get('std') + cum_rul\n            rul_mean = result.get('S3').get('mean') + cum_rul\n            rul_lower = result.get('S3').get('mean') - conf * result.get('S3').get('std') + cum_rul\n            \n            cum_rul += result.get('S3').get('mean')\n        \n        rul.append((rul_upper, rul_mean, rul_lower))\n    return rul","eb2f409b":"calculate_rul(path, result)","c6c0a2ae":"range_index = [(0, 3930), (3930, 8510), (8510, 11850), (11850, 15420), (15420, 18280), (18280, 21560)]","05097bc2":"df_visual = pd.DataFrame()\ndf_visual['index'] = [i for i in range(0,21560,10)]\ndf_visual['state'] = [1]*393 + [0]*458 + [1]*334 + [2]*357 + [0]*286 + [2]*328\ndf_visual['rul_upper'] = [3915.25]*393 + [8172.0]*458 + [11270.25]*334 + [14552.75]*357 + [18952.0]*286 + [21697.75]*328\ndf_visual['rul_mean'] = [3635]*393 + [7355]*458 + [10990]*334 + [14415]*357 + [18135]*286 + [21560]*328\ndf_visual['rul_lower'] = [3354.75]*393 + [6538]*458 + [10709.75]*334 + [14277.25]*357 + [17318]*286 + [21422.25]*328\ndf_visual['rul_error_upper'] = (21560 - df_visual['rul_upper'])\/21560 * 100\ndf_visual['rul_error_mean'] = (21560 - df_visual['rul_mean'])\/21560 * 100\ndf_visual['rul_error_lower'] = (21560 - df_visual['rul_lower'])\/21560 * 100\n\ndf_visual","063289f6":"# hidden state\nplt.figure(figsize=(12,5), dpi=80)\nplt.plot(df_visual['index'], df_visual['state'])\nplt.yticks(range(0,3));","4db2de57":"# RUL estimation\nplt.figure(figsize=(12,8), dpi=80)\nplt.plot(df_visual['index'], [21560]*2156, color='r', linestyle='--')\nplt.plot(df_visual['index'], df_visual['rul_upper'], linestyle='--')\nplt.plot(df_visual['index'], df_visual['rul_mean'])\nplt.plot(df_visual['index'], df_visual['rul_lower'], linestyle='--')\n\nplt.legend(['Real','RUL Upper', 'RUL Mean', 'RUL Lower'])\nplt.ylabel('Failure time (min)')\nplt.xlabel('Current time (min)');","d05f51e7":"# RUL error associated\nplt.figure(figsize=(12,8), dpi=80)\nplt.plot(df_visual['index'], df_visual['rul_error_upper'], linestyle='--')\nplt.plot(df_visual['index'], df_visual['rul_error_mean'])\nplt.plot(df_visual['index'], df_visual['rul_error_lower'], linestyle='--')\n\nplt.legend(['RUL Upper', 'RUL Mean', 'RUL Lower'])\nplt.ylabel('Error (%)')\nplt.xlabel('Current time (min)');","b30cddac":"# def rul2(time_state, conf_score):\n#     mean = {0: [], 1: [], 2: []}\n#     std = {0: [], 1: [], 2: []}\n    \n#     for data in time_state.items():\n#         print(data)","3bfd6266":"# # function calculate RUL\n# def rul(time_state, conf):\n#     # calculate mean and standard deviation\n#     mean_std = {0: [], 1: [], 2: []}\n#     for data in time_state.items():\n#         state, time = data[0], data[1]\n        \n#         decrease_time = []\n#         if time:\n#             for t in time:\n#                 decrease_time.append(t[1]-t[0])\n            \n#         mean_state = np.mean(decrease_time)\n#         std_state = np.std(decrease_time)\n        \n#         mean_std[state].append((mean_state, std_state))\n    \n    \n    \n#     # convert nan to zero\n#     mean_std[0][0], mean_std[1][0], mean_std[2][0] = np.nan_to_num(mean_std[0][0]), np.nan_to_num(mean_std[1][0]), np.nan_to_num(mean_std[2][0])\n\n#     # rul upper\n#     rul_upper = (mean_std[0][0][0] + conf * mean_std[0][0][1]) + \\\n#                 (mean_std[1][0][0] + conf * mean_std[1][0][1]) + \\\n#                 (mean_std[2][0][0] + conf * mean_std[2][0][1])\n\n#     # RUL Mean\n#     rul_mean = (mean_std[0][0][0]) + \\\n#                (mean_std[1][0][0]) + \\\n#                (mean_std[2][0][0])\n\n#     # RUL lower\n#     rul_lower = (mean_std[0][0][0] - conf * mean_std[0][0][1]) + \\\n#                 (mean_std[1][0][0] - conf * mean_std[1][0][1]) + \\\n#                 (mean_std[2][0][0] - conf * mean_std[2][0][1])\n#     # print(mean_std)\n#     return rul_upper, rul_mean, rul_lower","66278a9f":"# # model terbaik dari S3_B1\n# col_features = slice_columns(set3.columns, target='B3')\n# S3_B3 = minmax_l[0].transform(set3[col_features])\n\n# # ambil model S3_B1\n# gmmhmm = model_gmmhmm[0][2]\n\n# x = range(len(S3_B3))\n# y = gmmhmm.predict(S3_B3)","a8431553":"# gmmhmm.transmat_","166d0926":"# # slicing only for experiment\n# x_ = range(len(S3_B3))[:150]\n# y_ = gmmhmm.predict(S3_B3)[:150]\n# y_","7416569a":"# # calculate RUL\n# time_state = {0: [], 1: [], 2: []}\n# state_rul = []\n# time_rul = []\n\n# previous = None\n# start_time = 0\n\n# i = 0\n# for time_x, state_y in zip(x, y):\n#     # initiate start time and state\n#     if previous is None:\n#         previous = state_y\n#         start_time = time_x\n    \n#     # if previous state different with next state, calculate time\n#     if previous != state_y:       \n#         time_state[previous].append((start_time, time_x))\n#         start_time = time_x\n#         rul_upper, rul_mean, rul_lower = rul(time_state, 0.95)\n        \n# #         print(time_state)\n# #         print(f'RUL UPPER: {rul_upper}; RUL MEAN: {rul_mean}; RUL LOWER: {rul_lower}')\n# #         print()\n        \n#         state_rul.append((rul_upper, rul_mean, rul_lower))\n#         time_rul.append([time_x, rul_upper, rul_mean, rul_lower])\n    \n#     previous = state_y\n    \n#     # if time_x equals last time in index \n#     if time_x == x[-1]:\n#         time_state[previous].append((start_time, time_x))  \n#         rul_upper, rul_mean, rul_lower = rul(time_state, 0.95)\n#         state_rul.append((rul_upper, rul_mean, rul_lower))\n#         time_rul.append([time_x, rul_upper, rul_mean, rul_lower])\n# # time_rul","b47ce2a5":"# df_rul = pd.DataFrame(data={'RUL UPPER': [], 'RUL MEAN': [], 'RUL LOWER': []})\n# for data in state_rul:\n#     df_rul = df_rul.append({'RUL UPPER': data[0], 'RUL MEAN': data[1], 'RUL LOWER': data[2]}, ignore_index=True)\n# df_rul","7b86ae59":"# df_time_rul = pd.DataFrame(time_rul, columns=['time', 'rul_upper', 'rul_mean', 'rul_lower'])\n# df_time_rul","2a1f171b":"# df_visualize_rul = pd.DataFrame({\"time\": [], \"rul_upper\": [], \"rul_mean\": [], \"rul_lower\": []})\n\n# for index, data in df_time_rul.iterrows():\n#     # if last index\n#     if index == df_time_rul.shape[0]-1:\n#         df_visualize_rul = df_visualize_rul.append({\"time\": data[\"time\"], \"rul_upper\": data[\"rul_upper\"], \"rul_mean\": data[\"rul_mean\"], \"rul_lower\": data[\"rul_lower\"]}, ignore_index=True)\n#     else:\n#         # check time for next index, if difference not 10, do something\n#         current_time = data[\"time\"]\n#         next_time = df_time_rul.iloc[index+1][\"time\"]\n#         if next_time - current_time > 10:\n#             while next_time > current_time:\n#                 df_visualize_rul = df_visualize_rul.append({\"time\": int(current_time), \"rul_upper\": data[\"rul_upper\"], \"rul_mean\": data[\"rul_mean\"], \"rul_lower\": data[\"rul_lower\"]}, ignore_index=True)\n#                 current_time += 10\n#         else:\n#             df_visualize_rul = df_visualize_rul.append({\"time\": int(current_time), \"rul_upper\": data[\"rul_upper\"], \"rul_mean\": data[\"rul_mean\"], \"rul_lower\": data[\"rul_lower\"]}, ignore_index=True)","bfac6648":"# df_visualize_rul","af88e8ad":"# plt.figure(figsize=(12,5), dpi=80)\n# plt.plot(df_visualize_rul[\"time\"], df_visualize_rul[\"rul_upper\"])\n# plt.plot(df_visualize_rul[\"time\"], df_visualize_rul[\"rul_mean\"])\n# plt.plot(df_visualize_rul[\"time\"], df_visualize_rul[\"rul_lower\"])\n\n# plt.title(\"Grafik Remaining Useful Life (RUL)\", fontsize=20)\n# plt.ylabel(\"Remaining Useful Life (RUL)\", fontsize=16)\n# plt.xlabel(\"Waktu (Menit)\", fontsize=18)\n# plt.legend(['RUL UPPER', 'RUL MEAN', 'RUL LOWER'], loc='upper left')\n# plt.show()","5812d404":"**How to interpret log probability?**\n\nFrom reference (1), the more negative value of log probability, the better performance of the model. It happens because if we do exponentials of the log probability on the most negative log prob, the result will more close to zero.\n\nBut, if we see the decoding state of all the models, the result is not as expected from the theory. Theoretically, the GMMHMM model from `S1_B1` with log prob -21625910.26 is the best among others. But, the models from `S1_B4`, `S2_B1`, and `S3_B1` have better results on decoding states despite their log prob respectively got -428826.79, -44063.25, -106197.6555733793.\n\n- (1) https:\/\/www.mathworks.com\/matlabcentral\/answers\/351766-how-to-interpret-log-probability-from-hmmdecode","58e1bf87":"# Scaling\n\nAs mentioned (1), we don't need to perform feature scaling.\n\nBut, after I did some experiments the result is better with feature scaling.\n\nAnd in hmmlearn library (2), they use KMeans to cluster the number of components in data. Also, KMeans is sensitive with the variance of data, so it is better to perform feature scaling.\n\n- (1): https:\/\/stats.stackexchange.com\/questions\/371333\/is-it-important-to-make-a-feature-scaling-before-using-gaussian-mixture-model\n- (2): https:\/\/github.com\/hmmlearn\/hmmlearn\/blob\/master\/lib\/hmmlearn\/hmm.py","94900665":"## Training","26ca6bfc":"**How to make a left-right GMMHMM model?**\n\n![image.png](attachment:d6863120-2ff8-4c07-a139-be1f0c95cb11.png)\n\nBased on references from Sequentia library (1)(2)(3) and hmmlearn library (4), we need to:\n\n1. First initiate `start probability` randomly and `transition matrix` with upper right values.\n2. In GMMHMM model, \n    - define `init_params` equal `cmw`, means only initialize covariance, means, and weights. Since, we will initiate start probability (`s`) and transition matrix (`t`) manually.\n    - define `params` equal `stcmw`, means it will update those parameters when training.\n3. Define `model.startprob_` and `model.transmat_` with its value which has been initialized before.\n\n\n- (1) https:\/\/sequentia.readthedocs.io\/en\/latest\/sections\/classifiers\/gmmhmm.html\n- (2) https:\/\/github.com\/eonu\/sequentia\/blob\/master\/lib\/sequentia\/classifiers\/hmm\/gmmhmm.py\n- (3) https:\/\/github.com\/eonu\/sequentia\/blob\/master\/lib\/sequentia\/classifiers\/hmm\/topologies\/left_right.py\n- (4) https:\/\/hmmlearn.readthedocs.io\/en\/latest\/tutorial.html#building-hmm-and-generating-samples","9d1c4e1b":"**What is the best model chosen?**\n\nComparing the result above, model from `S2_B1` is the best on predicting\/decoding state of test data `S3_B3`.","66403201":"# Description Dataset\nThe data was generated by the NSF I\/UCR Center for Intelligent Maintenance Systems (IMS \u2013 www.imscenter.net) with support from Rexnord Corp. in Milwaukee, WI.\n\n**Test Rig Setup**\n\nFour bearings were installed on a shaft. The rotation speed was kept constant at 2000 RPM by an AC motor coupled to the shaft via rub belts. A radial load of 6000 lbs is applied onto the shaft and bearing by a spring mechanism. All bearings are force lubricated. Rexnord ZA-2115 double row bearings were installed on the shaft as shown in Figure 1. PCB 353B33 High Sensitivity Quartz ICP accelerometers were installed on the bearing housing (two accelerometers for each bearing [x- and y-axes] for data set 1, one accelerometer for each bearing for data sets 2 and 3). Sensor placement is also shown in Figure 1. All failures occurred after exceeding designed life time of the bearing which is more than 100 million revolutions.\n\n![image.png](attachment:51495a34-c91d-4476-9aae-57525c5d357d.png)]\n\nFigure 1 - Bearing Test Rig and sensor placement illustration (Qiu et al., 2006)","25111d90":"After we've found mean and standard deviation for each state, we can calculate RUL from the formula above. \n\nThe path estimation of the example on set test 1 bearing 1 above like this\n\n```\nS2 -> S1 -> S2 -> S3 -> S1 -> S3\n```","077854ae":"# Slice Features (Mean, RMS, Skewness, Kurtosis)\n\nWe only used 4 time features: mean, RMS, skewness, and kurtosis.\n\nReferences: (Tobon, 2010) [A mixture of gaussians hidden markov model for failure diagnostic and prognostic.](https:\/\/hal.archives-ouvertes.fr\/hal-00525073\/documenthttps:\/\/hal.archives-ouvertes.fr\/hal-00525073\/document)","77b05c56":"**Data Structure**\n\nThree (3) data sets are included in the data packet (IMS-Rexnord Bearing Data.zip). Each data set describes a test-to-failure experiment. Each data set consists of individual files that are 1-second vibration signal snapshots recorded at specific intervals. Each file consists of 20,480 points with the sampling rate set at 20 kHz. The file name indicates when the data was collected. Each record (row) in the data file is a data point. Data collection was facilitated by NI DAQ Card 6062E. Larger intervals of time stamps (showed in file names) indicate resumption of the experiment in the next working day.\n\n**Set No. 1**\n\n| Index                    | Description                                                                                                                   |\n|--------------------------|-------------------------------------------------------------------------------------------------------------------------------|\n| Recording Duration:      | October 22, 2003 12:06:24 to November 25, 2003 23:39:56                                                                       |\n| No. of Files:            | 2,156                                                                                                                         |\n| No. of Channels:         | 8                                                                                                                             |\n| Channel Arrangement:     | Bearing 1 \u2013 Ch 1&2; Bearing 2 \u2013 Ch 3&4; Bearing 3 \u2013 Ch 5&6; Bearing 4 \u2013 Ch 7&8.                                               |\n| File Recording Interval: | Every 10 minutes (except the first 43 files were taken every 5 minutes)                                                       |\n| File Format:             | ASCII                                                                                                                         |\n| Description:             | At the end of the test-to-failure experiment, inner race defect occurred in bearing 3 and roller element defect in bearing 4. |\n\n**Set No. 2**\n\n| Index                    | Description                                                                             |\n|--------------------------|-----------------------------------------------------------------------------------------|\n| Recording Duration:      | February 12, 2004 10:32:39 to February 19, 2004 06:22:39                                |\n| No. of Files:            | 984                                                                                     |\n| No. of Channels:         | 4                                                                                       |\n| Channel Arrangement:     | Bearing 1 \u2013 Ch 1; Bearing2 \u2013 Ch 2; Bearing3 \u2013 Ch3; Bearing 4 \u2013 Ch 4.                    |\n| File Recording Interval: | Every 10 minutes                                                                        |\n| File Format:             | ASCII                                                                                   |\n| Description:             | At the end of the test-to-failure experiment, outer race failure occurred in bearing 1. |\n\n**Set No. 3**\n\n| Index                    | Description                                                                             |\n|--------------------------|-----------------------------------------------------------------------------------------|\n| Recording Duration:      | March 4, 2004 09:27:46 to April 4, 2004 19:01:57                                        |\n| No. of Files:            | 4,448                                                                                   |\n| No. of Channels:         | 4                                                                                       |\n| Channel Arrangement:     | Bearing 1 \u2013 Ch 1; Bearing2 \u2013 Ch 2; Bearing3 \u2013 Ch3; Bearing 4 \u2013 Ch 4.                    |\n| File Recording Interval: | Every 10 minutes                                                                        |\n| File Format:             | ASCII                                                                                   |\n| Description:             | At the end of the test-to-failure experiment, outer race failure occurred in bearing 3. |","60ce1066":"The result is the same with the paper, so the code implemented is proven.\n\n![image.png](attachment:a9af1e20-1591-4622-9d72-b66cd1721cd1.png)","db4637af":"To align the formula and the code, I will try to implement figure of decoding state from set test 1 bearing from reference (1) above.\n![image.png](attachment:beb76006-5643-49e1-bd94-16495e369588.png)\n\nAfter did subtitution, we got duration for each state index.\n![e952fec8-bf58-4cbb-b50e-488bd79e9b07.jfif](attachment:faabc8f9-effc-4f65-9e7f-71792a877194.jfif)","ee76f9ef":"# Prepare data\n\n- Testing: Set test 3 bearing 3\n    - 'S3_B3'\n- Learning: 11 sensor data from each set test\n    - 'S1_B1','S1_B2','S1_B3','S1_B4',\n    - 'S2_B1','S2_B2','S2_B3','S2_B4',\n    - 'S3_B1','S3_B2','S3_B4'","762eb3af":"# About\n\nThis notebook implements the concept of calculating Remaining Useful Life (RUL) from the NASA bearing dataset. The paper references for implementing the concept are:\n1. Tobon Mejia, Diego & Medjaher, Kamal & Zerhouni, Noureddine & Tripot, Gerard. (2010). [A Mixture of Gaussians Hidden Markov Model for failure diagnostic and prognostic](https:\/\/www.researchgate.net\/publication\/224177188_A_Mixture_of_Gaussians_Hidden_Markov_Model_for_failure_diagnostic_and_prognostic). 6th Annual IEEE Conference on Automation Science and Engineering, CASE'10.. 338 - 343. 10.1109\/COASE.2010.5584759.\n2. Medjaher, Kamal & Tobon Mejia, Diego & Zerhouni, Noureddine. (2012). [Remaining Useful Life Estimation of Critical Components With Application to Bearings](https:\/\/www.researchgate.net\/publication\/254059871_Remaining_Useful_Life_Estimation_of_Critical_Components_With_Application_to_Bearings). IEEE Transactions on Reliability - TR. 61. 292-302. 10.1109\/TR.2012.2194175.\n3. Tobon Mejia, Diego & Medjaher, Kamal & Zerhouni, Noureddine & Tripot, Gerard. (2012). [A Data-Driven Failure Prognostics Method Based on Mixture of Gaussians Hidden Markov Models](https:\/\/www.researchgate.net\/publication\/254059873_A_Data-Driven_Failure_Prognostics_Method_Based_on_Mixture_of_Gaussians_Hidden_Markov_Models). IEEE Transactions on Reliability - TR. 61. 491-503. 10.1109\/TR.2012.2194177. ","4c9297a6":"## Testing (S3_B3)","b2a02b97":"# Remaining Useful Life (RUL)\n\nBased on reference (1), the formula to calculate mean and standard deviation are:\n\n$$\\mu(D(S_i)) = \\frac{\\sum_{w = 1}^{\\Omega} D(S_{iw})}{\\Omega}$$\n\n$$\\sigma(D(S_i)) = \\frac{\\sum_{w = 1}^{\\Omega} [D(S_{iw})- \\mu(D(S_i))]^2}{\\Omega}$$\n\nwhere,\n- $D(.)$ is visit duration\n- $i$ is the state index\n- $w$ is the visit index\n- $\\Omega$ is the total of visits\n\nThe formula to calculate RUL with $\\eta$ is confidence value:\n\n$$RUL_{upper} = \\sum_{i = current state}^{N} [\\mu(D(S_{i})) + \\eta.\\sigma(D(S_i))]$$\n\n$$RUL_{mean} = \\sum_{i = current state}^{N} \\mu(D(S_{i}))$$\n\n$$RUL_{lower} = \\sum_{i = current state}^{N} [\\mu(D(S_{i})) - \\eta.\\sigma(D(S_i))]$$\n\n- (1) Tobon Mejia, Diego & Medjaher, Kamal & Zerhouni, Noureddine & Tripot, Gerard. (2010). [A Mixture of Gaussians Hidden Markov Model for failure diagnostic and prognostic](https:\/\/www.researchgate.net\/publication\/224177188_A_Mixture_of_Gaussians_Hidden_Markov_Model_for_failure_diagnostic_and_prognostic). 6th Annual IEEE Conference on Automation Science and Engineering, CASE'10.. 338 - 343. 10.1109\/COASE.2010.5584759.","3fb3b20d":"Notes:\n- Still need to modify the model into the left-to-right model. But, somehow when I defined the model as left-to-right, some of the training processes were not successful.","92d89081":"**How to decide the covariance type?**\n\nCovariance matrices for Gaussian Mixture Model has several types such as Full, Tied, Diagonal, and Spherical. You can read more details on this article (1). \n\nIf we look up into the means of the data features (after scaling) for each state in GMM model, they have close value between each other. So, it is decided that covariance type `tied` has potentital to differentiate between each state. (Still need to understand the concept of this theory)\n\n![image.png](attachment:37015e9d-5b95-4acb-9b44-b22fbefbef39.png)\n\n\n- (1) https:\/\/stats.stackexchange.com\/questions\/326671\/different-covariance-types-for-gaussian-mixture-models\/326678#326678","e926f088":"# GMMHMM","2d01b372":"For feature extraction, you can check this notebook https:\/\/www.kaggle.com\/yasirabd\/nasa-bearing-feature-extraction"}}