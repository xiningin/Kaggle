{"cell_type":{"cbcd59c0":"code","3d85a5df":"code","0b09013a":"code","287c921f":"code","ae195c97":"code","c28136b5":"code","3eabb840":"code","f548ed86":"code","8faa5c42":"code","df9ecfef":"code","3fcac9ab":"code","804e9bf7":"code","9dd0e97e":"code","ddb57ee5":"code","b4e47906":"code","e399b2a1":"code","816949fe":"code","cbac3db1":"code","7b2f8a84":"code","0ce366ae":"code","99dd9d45":"code","25324569":"code","197479c9":"markdown","8fac3160":"markdown","f1f355cd":"markdown","9eba1d4a":"markdown","b3c6f1bc":"markdown","64651881":"markdown","fc07a6ac":"markdown","2c5f26d4":"markdown","42fecd1b":"markdown","0140ee67":"markdown","0c3361d6":"markdown","56849658":"markdown","dff9decb":"markdown","2df99c03":"markdown","d2cd1517":"markdown","f883569d":"markdown","5b2a9782":"markdown"},"source":{"cbcd59c0":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","3d85a5df":"train=pd.read_csv(\"..\/input\/titanic\/train.csv\")\ntrain.head()","0b09013a":"train.info()","287c921f":"#Drop the cabin, Passenger Id, Name and Ticket variables\ntrain1=train.drop(columns=[\"Cabin\",\"PassengerId\",\"Name\",\"Ticket\"])\n\n#Replace missing age values by its median\ntrain1['Age'].fillna((train1['Age'].median()), inplace=True)\n\n#Replace missing embarked values by S\ntrain1[\"Embarked\"].fillna(\"S\", inplace = True)","ae195c97":"train1[\"Family\"]=train1[\"SibSp\"]+train1[\"Parch\"]\ntrain2=train1.drop(columns=[\"SibSp\",\"Parch\"])\ntrain2.head()","c28136b5":"train2['Families'] = train2['Family'].apply(lambda x: '0' if x==0 else '1')\ntrain3=train2.drop(columns='Family')\ntrain3.head()","3eabb840":"#Import the libraries first to see the correlation matrix\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom matplotlib.pyplot import figure\n\n#Pclass\nfigure(figsize=(20,20))\nplt.subplot(2,2,1)\np1_data = train3.groupby('Pclass').Survived.mean()\np1 = sns.barplot(y=p1_data.values,\n                 x=p1_data.index, \n                 palette=[\"lightgreen\",\"green\",\"black\"])\np1.set(ylabel='Survival Rate', \n       xlabel=' Pclass');","f548ed86":"#Embarked\nfigure(figsize=(20,20))\nplt.subplot(2,2,2)\np1_data = train3.groupby('Embarked').Survived.mean()\np1 = sns.barplot(y=p1_data.values,\n                 x=p1_data.index, \n                 palette=[\"blue\",\"grey\",\"black\"])\np1.set(ylabel='Survival Rate', \n       xlabel=' Embarked');","8faa5c42":"#Sex\nfigure(figsize=(20,20))\nplt.subplot(2,2,3)\np1_data = train3.groupby('Sex').Survived.mean()\np1 = sns.barplot(y=p1_data.values,\n                 x=p1_data.index, \n                 palette=[\"blue\",\"pink\"])\np1.set(ylabel='Survival Rate', \n       xlabel=' Sex');","df9ecfef":"#Family size\nfigure(figsize=(20,20))\nplt.subplot(2,2,4)\np1_data = train3.groupby('Families').Survived.mean()\np1 = sns.barplot(y=p1_data.values,\n                 x=p1_data.index, \n                 palette=[\"red\",\"orange\"])\np1.set(ylabel='Survival Rate', \n       xlabel=' Family size');","3fcac9ab":"#Fare\ntrain3.boxplot( by='Survived',column=['Fare'],grid= False )","804e9bf7":"#Age\ntrain3.boxplot( by='Survived',column=['Age'],grid= False )","9dd0e97e":"#Import the libraries\nfrom sklearn.preprocessing import LabelEncoder\n\n#Label Encoder\nlabel_encoder = LabelEncoder()\ntrain4 = train3.apply(label_encoder.fit_transform)\ntrain4\n\n#Set the variables\ny=train4[['Survived']]\nx=train4[['Pclass','Embarked','Sex','Families','Fare']]\nX = (x - np.min(x))\/(np.max(x)-np.min(x)).values","ddb57ee5":"#Import librairies\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection import cross_val_score\n\n#Run the model without PCA\ntitanic_model = LogisticRegression()\ntitanic_model.fit(X,y)\nscores=cross_val_score(titanic_model, X, y.values.ravel(),cv=10)\nscores.mean()","b4e47906":"i_range=[0.7,0.75,0.8,0.85,0.9,0.95]\nscores1=[]\n\nfor i in i_range:\n  pca=PCA(i) \n  pca.fit(X) \n  X1=pca.transform(X) \n  titanic_model1 = LogisticRegression()\n  scores1=cross_val_score(titanic_model1, X1, y.values.ravel(),cv=10)\n  print(scores1.mean())","e399b2a1":"import statsmodels.formula.api as smf\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\n\nlm = smf.ols(formula='Survived ~  Sex + Pclass + Embarked ', data=train4).fit()\nprint(lm.summary())\n\nvariables = lm.model.exog\nvif = [variance_inflation_factor(variables, i) for i in range(variables.shape[1])]\nvif \n","816949fe":"from sklearn.neighbors import KNeighborsClassifier\n\nk_range = list(range(1, 30))\nk_scores = []\n\nfor k in k_range:\n    knn = KNeighborsClassifier(n_neighbors=k)\n    scores = cross_val_score(knn, X, y.values.ravel(), cv=10, scoring='accuracy')\n    k_scores.append(scores.mean())\n    \nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nplt.plot(k_range, k_scores)\nplt.xlabel('Value of K for KNN')\nplt.ylabel('Cross-Validated Accuracy')","cbac3db1":"knn = KNeighborsClassifier(n_neighbors=4)\nprint(cross_val_score(knn, X, y.values.ravel(), cv=10, scoring='accuracy').mean())","7b2f8a84":"from sklearn.ensemble import RandomForestClassifier\n\nj_range = [1,5,10,30,50,75,100,200,500]\nj_score = []\n\nfor j in j_range:\n    RF = RandomForestClassifier(n_estimators=j,random_state=2)\n    score = cross_val_score(RF, X, y.values.ravel(), cv=50, scoring='accuracy')\n    j_score.append(score.mean())\n    \nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nplt.plot(j_range, j_score)\nplt.xlabel('Value of J for Random Forest')\nplt.ylabel('Cross-Validated Accuracy')","0ce366ae":"import statistics\n\np_range = list(range(1, 40))\np_score = []\n\nfor p in p_range:\n   RF = RandomForestClassifier(n_estimators=50,random_state=p)\n   score=cross_val_score(RF, X, y.values.ravel(), cv=10, scoring='accuracy').mean()\n   p_score.append(score.mean())\n\nstatistics.mean(p_score)","99dd9d45":"from xgboost import XGBClassifier\n\nxgb_clf = XGBClassifier(n_estimators=300,random_state=1)\nprint(cross_val_score(xgb_clf, X, y.values.ravel(), cv=50, scoring='accuracy').mean())","25324569":"from catboost import CatBoostClassifier\n\ntrain4['Ages'] = train2['Age'].apply(lambda y: '0' if y<16 else '1')\ntrain5=train4.drop(columns='Age')\ntrain5\n\n\n\n\ntest_data=pd.read_csv(\"..\/input\/titanic\/test.csv\")\ntest_data[\"Family\"]=test_data[\"SibSp\"]+test_data[\"Parch\"]\ntest_data1=test_data.drop(columns=[\"SibSp\",\"Parch\"])\ntest_data1['Ages'] = test_data1['Age'].apply(lambda y: '0' if y<16 else '1')\ntest_data2=test_data1.drop(columns='Age')\n\n\n\n\nfeatures = [ \"Sex\", \"Pclass\",\"Ages\",\"Embarked\"]\nX = (train5[features]).apply(pd.to_numeric)\nX_test = test_data2[features]\nX_test1 = (X_test.apply(label_encoder.fit_transform)).apply(pd.to_numeric)\n\nmodel =XGBClassifier(n_estimators=100)\nmodel.fit(X, y)\npredictions = model.predict(X_test1)\n\noutput = pd.DataFrame({'PassengerId': test_data2.PassengerId, 'Survived': predictions})\noutput.to_csv('my_submission16.csv', index=False)\nprint(\"Your submission was successfully saved!\")\n","197479c9":"There seems to be a relationship between the survival rate and the fare. Indeed, the average fare seems higher when the passenger survived.","8fac3160":"# 8-XGBoost\n\nThe accuracy of this model is less than random forest (81.49%).","f1f355cd":"# 7-Run the random forest model\n\nIn this step, I will measure the accuracy using the random forest model. From the graph below, we can see that the best number of estimators seems to be when j=100. I then used that number to run my model, which gives an accuracy of 81.71%, which is better than the KNN model.","9eba1d4a":"There is no more missing values in the dataset now. \n\nWe can now combine the SipSb and the Parch variables by adding them together and deleting them thereafter. This will give the total number of family member that are on the boat for each passenger. This will simplify the dataset.   ","b3c6f1bc":"# 2-Clean the dataset\n\nThe second step is to clean the dataset.\n\nAs I mentioned in step 1, we will first check if there are a lot of missing values for each variable. Fortunately, there are only 3 variables with missing values:\n\n1-The age. Since there are only 177 missing values for this feature, we will then subtitute the missing values by the median.\n\n2-Cabin. I will just drop this variable since there are too many missing values (687). Moreover, the cabin number does not seem to be a good feature to predict if either the passenger died or not. We already have the class for each passenger, which seems to be a good predictor.\n\n3-There are only two missing values, so we will substitute them by S since most of them are represented by this letter.","64651881":"# 9-Conclusion\n\nHere is a summary of the results of this notebook:\n\nAccuracy:\n\n1-Logistic Regression:77.7%\n\n2-KNN:81.4%\n\n3-Random Forest:81.2%\n\n4-XG Boost:81.5%\n\nThe model that is the most accurate seems to be XG Boost since the accuracy seems better than the other models.\n\n******DO NOT FORGET TO UPVOTE MY NOTEBOOK IF YOU LIKE IT******\n******FEEL FREE TO HAVE A LOOK AT MY R VERSION OF THE TITANIC https:\/\/www.kaggle.com\/renaudgendron\/titanic-prediction******\n\n","fc07a6ac":"# 5-Run the logistic regression model with and without PCA\n\nFor this step, we need two libraries: LogisticRegression and PCA. \n\nI decided to run the model with and without PCA because we want to reduce the interaction between each feature in a logistic model. Therefore, it may happen that the accuracy increases if we only choose the most important features only in the model. \n\nMoreover, I decided to use K-Fold cross validation method instead of using the train\/test splitting method since the code is not long to run. The K-Fold cross validation method gives a better accuracy in that case  because train\/test splitting method n times (in that case, n=10).\n\nBy using a fewer number of components, we can see that we obtain a little better accuracy (0.777 vs 0.776 vs 0775). For now, we will then keep the logistic regression model with 70% of the most important components of the model. Note that I could also have chosen to keep 75% and 80% since it gives exactly the same accuracy.","2c5f26d4":"For the family size, to simplify the analysis of this variable, I will divide in only two categories: alone (0) or not alone (1 and over).","42fecd1b":"# 4-Set the variables and the librairies for our ML model\n\nIn this step, we will set the variables for our ML model. I will set y as the response variable (Survived) and x as the features of the model that we determined in the previous step (Pclass, Embarked, Sex, Families and Fare).\n\nMoreover, I will import and use the label encoder library to give a numerical value to each value in the dataset. This will help us thereafter to scale all the values in the dataset to have a better idea of the weight of each feature in the model. For example, in a logistic regression model, beta1=1 will have a bigger impact in the model than beta2=0.01.  ","0140ee67":"# 6-Build the KNN model\n\nThe KNN method is another machine learning model that I chose to test. Before, running this model, it is important to choose the number of neighbors that will give the best accuracy. Therefore, I decided to calculate the accuracy of the model with the K-Fold cross validation method when k=1 to 10, where k is the number of neighbors. As we can notice on the graph below, k=4 is the optimal number of neighbors that we can choose in that case with an accuracy of 81.38%, which is very good. Indeed, it is better than the logistic regression model that I tested before.","0c3361d6":"# 3-Exploratory Data Analysis\n\nFor this part of this project, we will analyze if the correlation is strong between the survived (response) variable and the remaining features left in the dataset. If twe notice the trend on the graph, that means that we will keep the feature in the model. Otherwise, we will delete the feature.\n\nThere seems to be a relationship between the survival rate and the Pclass from the histogram. Indeed, the survival rate seems lower when the passengers are on class 3.","56849658":"There seems to be a relationship between the survival rate and the gender from the histogram. Indeed, the survival rate seems lower when the passenger is a male.","dff9decb":"# 1-Import the dataset\n\nThe first step of this project is to import the dataset. For now, I will just import the testing set as it is the only one for now that will be useful to build our Machine Learning model. The testing set is only there for the competition purpose, and the gender submission file is only an example of how we have to submit our prediction for the competition.  As we can notice in the table below, there are 12 variables:\n\n1-PassengerId: The ID of the passenger. It is a number from 1 to 891.\n\n2-Survived: If the passenger survived or not. \n\n3-Pclass: Ticket Class. There are three classes on the Titanic\n\n4-Name: Name of the passenger.\n\n5-Sex: Gender of the passenger\n\n6-Age: Age of the passenger. We will treat this variable as continuous.\n\n7-SibSp: Number of siblings\/spouses\n\n8-Parch: Number of parents\/children\n\n9-Ticket: Ticket number for each passenger\n\n10-Fare: How much money each passenger has on the Titanic.\n\n11-Cabin: Stands for the cabin number.\n\n12-Embarked: Port of embarkation.\n\nWe can also notice that there are missing values for the Cabin variable. It would then be useful to have a look if there are a lot of missing values for each feature, and deal with them thereafter.","2df99c03":"There seems to be a relationship between the survival rate and the family size from the histogram. Indeed, the survival rate seems lower when the passenger is alone.","d2cd1517":"There does not seem to be a relationship between the survival rate and the age. Indeed, the average age seems similar in both cases.","f883569d":"I will then delete the Cabin variable and replace the missing age and embarked values by the median age.\n\nMoreover, I will drop the passenger ID, the name and the ticket variables since they are, by inspection, irrelevant predictors for the survival rate. ","5b2a9782":"There seems to be a relationship between the survival rate and the port of embarkation from the histogram. Indeed, the survival rate seems lower when the passengers got on port S."}}