{"cell_type":{"b6a8bd52":"code","9c100f78":"code","18191b75":"code","f6a1640e":"code","7844c354":"code","b211f216":"code","bf8b06c8":"code","2a5240ac":"code","704b02d5":"code","bf8ca626":"code","9d94ab11":"code","42fd5a57":"code","7189ee4b":"code","9864246a":"code","6cd02fb6":"code","1b825cbe":"code","86e59e5c":"code","3bc8c576":"code","a3a8c13f":"code","d07d1dfc":"code","2363dac9":"code","e2797237":"code","004c392d":"code","b08f2f08":"code","f001ef49":"code","b8b1d4df":"code","030f0f17":"code","ee6086f7":"code","52cc2012":"code","1b8c1305":"code","d7379bea":"code","c41e07b2":"code","1fa2c790":"code","051dd681":"code","519f4963":"markdown","7f3eafa1":"markdown","eeb6b750":"markdown","6026f9fc":"markdown","10b78714":"markdown","6ac70555":"markdown","3b9153e9":"markdown","305812bc":"markdown","48d6886e":"markdown","82613deb":"markdown","ecc3e9f6":"markdown","1e9972b0":"markdown","62893525":"markdown","eefe1a98":"markdown","9479052e":"markdown","aaa422d0":"markdown","5794ec5c":"markdown","bfe69344":"markdown","53447b66":"markdown","238a4fff":"markdown","f3b12129":"markdown","c16c7415":"markdown"},"source":{"b6a8bd52":"import os\n\nfrom PIL import Image, ImageDraw\nimport cv2\nimport re\nimport pandas as pd\nimport numpy as np\nfrom tqdm import tqdm\n\nfrom matplotlib import pyplot as plt\nimport matplotlib.patches as patches\nimport seaborn as sns\nfrom IPython.display import Video, display","9c100f78":"dataset = {\n    'root_dir': '..\/input\/tensorflow-great-barrier-reef',\n    'train_csv': '..\/input\/tensorflow-great-barrier-reef\/train.csv',\n    'test_csv': '..\/input\/tensorflow-great-barrier-reef\/test.csv',\n    'sample_submission_csv': '..\/input\/tensorflow-great-barrier-reef\/example_sample_submission.csv',\n    'video_img_dir': '..\/input\/tensorflow-great-barrier-reef\/train_images'\n}","18191b75":"train_csv = pd.read_csv(dataset['train_csv'])\ntest_csv = pd.read_csv(dataset['test_csv'])","f6a1640e":"print(\"number of frames:\", len(train_csv))","7844c354":"train_csv.head()","b211f216":"frame_counts = train_csv['video_id'].value_counts().sort_values().to_frame()\nframe_counts.head()","bf8b06c8":"print(\"number of records in video_0 matched: \", frame_counts.loc[0]['video_id'] == len(os.listdir(os.path.join(dataset['video_img_dir'], 'video_0'))))\nprint(\"number of records in video_0 matched: \", frame_counts.loc[1]['video_id'] == len(os.listdir(os.path.join(dataset['video_img_dir'], 'video_1'))))\nprint(\"number of records in video_0 matched: \", frame_counts.loc[2]['video_id'] == len(os.listdir(os.path.join(dataset['video_img_dir'], 'video_2'))))","2a5240ac":"sequence_counts = train_csv['sequence'].value_counts().sort_values().reset_index()\nsequence_counts.columns = [['sequence', 'num_frames']]\nprint(\"number of sequences:\", len(sequence_counts))\nsequence_counts.head()","704b02d5":"num_no_obj_frame = train_csv[train_csv.annotations == '[]']['annotations'].count()\nprint(\"number of frames without objects:\", num_no_obj_frame)","bf8ca626":"num_with_obj_frame = train_csv[train_csv.annotations != '[]']['annotations'].count()\nprint(\"number of frames with objects:\", num_with_obj_frame)","9d94ab11":"train_csv[train_csv.annotations != '[]'].head()","42fd5a57":"print('ratio of frames with objects:', num_with_obj_frame \/ len(train_csv))\n\nfig, axes = plt.subplots(1,1, figsize=(12, 6))\n\nsns.barplot(ax=axes, x=['Number of Frames with Objects', 'Number of Frames with No Objects'], y=[num_with_obj_frame, num_no_obj_frame])\naxes.set_title(\"Distribution of Frames with\/without Objects\")\naxes.set_xlabel(\"Frame Types\")\naxes.set_ylabel(\"Count\")\n\nplt.show()","7189ee4b":"def decode_annotation(annot_line):\n    # annot_line example: [{'x': 540, 'y': 310, 'width': 113, 'height': 105}, {'x': 657, 'y': 501, 'width': 95, 'height': 56}]\n    boxes = []\n    \n    box_pattern = r'\\{\\'\\w\\'\\:\\s\\d+\\,\\s\\'\\w\\'\\:\\s\\d+\\,\\s\\'\\w+\\'\\:\\s\\d+\\,\\s\\'\\w+\\'\\:\\s\\d+\\}'\n    val_pattern = r'\\d+'\n    \n    annotations = re.findall(box_pattern, annot_line)\n    for annot in annotations:\n        x, y, width, height = re.findall(val_pattern, annot)\n        x, y, width, height = float(x), float(y), float(width), float(height)\n        confidence = 1.0\n        \n        box = [x, y, width, height, confidence]\n        boxes.append(box)\n        \n    return boxes\n\ndef count_boxes(annot_line):\n    \n    annot_line  = annot_line[1:-1]\n    box_pattern = r'\\{\\'\\w\\'\\:\\s\\d+\\,\\s\\'\\w\\'\\:\\s\\d+\\,\\s\\'\\w+\\'\\:\\s\\d+\\,\\s\\'\\w+\\'\\:\\s\\d+\\}'\n    val_pattern = r'\\d+'\n    \n    annotations = re.findall(box_pattern, annot_line)\n    \n    return len(annotations)\n\n\ndef test_decode_annotation(annot_line):\n    print(\"sample:\", annot_line)\n    boxes = decode_annotation(annot_line)\n    for i, box in enumerate(boxes):\n        print(f\"box {i}:\", box)\n","9864246a":"test_samples = [\n    \"[{'x': 540, 'y': 310, 'width': 113, 'height': 105}, {'x': 657, 'y': 501, 'width': 95, 'height': 56}, {'x': 257, 'y': 101, 'width': 42, 'height': 59}]\",\n    \"[{'x': 540, 'y': 310, 'width': 113, 'height': 105}, {'x': 657, 'y': 501, 'width': 95, 'height': 59}]\",\n    \"[{'x': 12, 'y': 250, 'width': 143, 'height': 82}]\",\n    \"[]\"\n]\n\nfor i, sample in enumerate(test_samples):\n    num_boxes = count_boxes(sample)\n    print(f\"Test {i+1}:\", f\"found {num_boxes} boxes\")\n    \n    test_decode_annotation(sample)\n    print(\"\")","6cd02fb6":"train_csv['num_boxes'] = train_csv['annotations'].apply(count_boxes)","1b825cbe":"train_csv[train_csv.annotations != '[]'].head()","86e59e5c":"boxes_dist = train_csv[train_csv.annotations != '[]']['num_boxes'].value_counts().sort_values(ascending=False).reset_index()\nboxes_dist.columns = ['num_boxes', 'num_frames']\nboxes_dist","3bc8c576":"fig = plt.figure(figsize=(24, 8))\nsns.barplot(x=boxes_dist.num_boxes, y=boxes_dist.num_frames)\n\nplt.title(\"Box Distribution\")\nplt.xlabel(\"Number of Boxes\")\nplt.ylabel(\"Frame Counts\")\n\nplt.show()","a3a8c13f":"def gen_file_path(image_id):\n    # extract file path by using the image_id in the train file\n    video_id = image_id.split('-')[0]\n    image_id = image_id.split('-')[1]\n    return os.path.join(dataset['video_img_dir'], 'video_' + video_id, image_id + '.jpg')\n\ndef draw_boxes(image_path, annot_line):\n    \n    boxes = decode_annotation(annot_line)\n\n    coords = [] \n    for box in boxes: \n        coord = [] \n        coord.append(box[0]) \n        coord.append(box[1]) \n        coord.append(box[0] + box[2]) \n        coord.append(box[1] + box[3]) \n        coords.append(coord) \n\n    image = Image.open(image_path)\n    imgcp = image.copy()\n    imgcp_draw = ImageDraw.Draw(imgcp)\n\n    for coord in  coords:\n         imgcp_draw.rectangle(coord, fill = None, outline = \"red\", width=5)\n\n    return imgcp","d07d1dfc":"train_csv['file_path'] = train_csv['image_id'].apply(gen_file_path)","2363dac9":"train_csv.head()","e2797237":"# total number of bounding boxes\ntotal_num_boxes = train_csv.num_boxes.sum()\ntotal_num_boxes","004c392d":"# extract the first sample in each group\nsamples = train_csv.groupby('num_boxes').first()\nsamples","b08f2f08":"plt.figure(figsize=(24, 36))\n\nr, c = 7, 3\nfor index, row in samples.iterrows():\n    image_path = row['file_path']\n    annot_line = row['annotations']\n    plt.subplot(r, c, index + 1)\n    dimg = draw_boxes(image_path, annot_line)\n    plt.imshow(dimg)\n    \nplt.tight_layout()\nplt.show()","f001ef49":"all_boxes_xy = []\nall_boxes_wh = []\n\nfor index, row in tqdm(train_csv.iterrows(), total=len(train_csv)):\n    if row['annotations'] != '[]':\n        boxes = decode_annotation(row['annotations'])\n        \n        for box in boxes:\n            all_boxes_xy.append([box[0], box[1]])\n            all_boxes_wh.append([box[2], box[3]])\n            \nall_boxes_xy = np.array(all_boxes_xy)\nall_boxes_wh = np.array(all_boxes_wh)","b8b1d4df":"box_center_df = pd.DataFrame.from_records(all_boxes_xy, columns=['x', 'y'])\n\nbox_shape_df  = pd.DataFrame.from_records(all_boxes_wh, columns=['width', 'height'])\nbox_shape_df['area'] = box_shape_df['width'] * box_shape_df['height']","030f0f17":"box_center_df.describe()","ee6086f7":"box_shape_df.describe()","52cc2012":"plt.figure(figsize=(28, 16))\nplt.scatter(x=all_boxes_xy[:,0], y=all_boxes_xy[:,1], s=0.5, color = 'red')\nplt.title(\"Distribution of Box Center Coordinate on Image\")\nplt.xlabel(\"X value\")\nplt.ylabel(\"Y value\")\nplt.show()","1b8c1305":"# calculate number of boxes in each sequence\n# we can find that sequence 29424, 37114, 44160 do not contain any object\ntrain_csv.groupby('sequence')['num_boxes'].sum().sort_values(ascending=False).to_frame().T","d7379bea":"# sequence length\ntrain_csv.groupby('sequence')['image_id'].count().sort_values(ascending=False).to_frame().T","c41e07b2":"# pick one sequence, and convert it to video and adding the annotations to the video\nsample_seq = train_csv[train_csv.sequence == 22643]\nsample_seq","1fa2c790":"def convert_frames_to_video(files, boxes, save_to,fps):\n    \n    frame_array = []\n    \n    print(\"loading ...\")\n    for filename, annot_line in tqdm(zip(files, boxes), total=len(files)):\n        img = cv2.imread(filename)\n        height, width, layers = img.shape\n        size = (width,height)\n        \n        boxes = decode_annotation(annot_line)\n        \n        coords = [] \n        for box in boxes: \n            coord = [] \n            coord.append(box[0]) \n            coord.append(box[1]) \n            coord.append(box[0] + box[2]) \n            coord.append(box[1] + box[3]) \n            coords.append(coord) \n\n        imgcp = Image.fromarray(img)\n        imgcp_draw = ImageDraw.Draw(imgcp)\n\n        for coord in  coords:\n             imgcp_draw.rectangle(coord, fill = None, outline = \"blue\", width=5)\n        \n        del imgcp_draw\n        \n        frame_array.append(np.array(imgcp))\n        \n    out = cv2.VideoWriter(save_to,cv2.VideoWriter_fourcc(*'DIVX'), fps, size)\n    \n    print(f\"writing to {save_to}\")\n    for i in tqdm(range(len(frame_array))):\n        # writing to a image array\n        out.write(frame_array[i])\n    out.release()","051dd681":"convert_frames_to_video(sample_seq['file_path'].values.tolist(), sample_seq['annotations'].values.tolist(), '.\/sequence.avi', 25)","519f4963":"## Ratio between Frames with Objects and Frames with No Objects","7f3eafa1":"## Summary Statistics: Box Center Coordinates and Box Area","eeb6b750":"at current stage, you might need to download the output sequence...","6026f9fc":"## Distribution of Number of Boxes (Frame Counts)","10b78714":"## Bounding Box Visualization","6ac70555":"<p style=\"text-align: justify;\">Then, let's run a sanity check to make sure the number of images in the data folder match with the number of images in the train file<\/p>","3b9153e9":"# Sequence Preview","305812bc":"# Load Dataset & EDA","48d6886e":"<p style=\"text-align: justify;\">Let's visualize the image and draw the bounding boxes on it, for each case mentioned in the last step, to see some sample data<\/p>","82613deb":"## Number of Boxes for each Frame","ecc3e9f6":"<p style=\"text-align: justify;\">From above, we can see that the number of frames with no objects is almost 3.7 times of the number of frames with objects, and only 21% of the frames in the training data contains objects<\/p>","1e9972b0":"## Distribution of Box Center on Image (all boxes)","62893525":"## Box Location Visualization","eefe1a98":"# Bounding Box Analysis","9479052e":"<p style=\"text-align: justify;\">Box's (width, height), and area summary statistics<\/p>","aaa422d0":"<p style=\"text-align: justify;\">Let's construct a function that can read the annotations and convert it to the format that can be used in object detection algorithm, and we calculate the number of boxes as well to help us get better understanding about the bounding box information<\/p>\n","5794ec5c":"<p style=\"text-align: justify;\">There are 20 distinct sequences in the dataset, and a sequence is a gap-free subset of a given video, and we calculate number of frames in each sequence<\/p>","bfe69344":"<p style=\"text-align: justify;\">We can observe that most of the frames have only one bounding box, and 3 frames contain 18 bounding boxes<\/p>","53447b66":"<p style=\"text-align: justify;\">Box's (x, y) coordinate summary statistics<\/p>","238a4fff":"## Decode Annotations","f3b12129":"<p style=\"text-align: justify;\">Let's first see how many frames are there for each video; also, we need to notice that the frames might not be consecutive (there might be gaps in each video<\/p>","c16c7415":"# TensorFlow - Help Protect the Great Barrier Reef\n\n# Table of Content\n1. Introduction\n2. Load Dataset & EDA\n3. Bounding Box Analysis\n    1. Ratio between Number of Boxes with Objects and Number of Boxes without Objects\n    2. How to decode annotations from the dataframe loaded from the train.csv file\n    3. Summary of Number of Boxes in Each Frame and Distributions of Number of Boxes\n    4. Bounding Box Visualization\n    5. Distribution of Bounding Box Center Coordinates on Image\n4. Sequence Preview\n    1. Generate Annotated Video from Sequences\n\n# Introduction\n\n<p style=\"text-align: justify;\">In this competition, our goal is to predict the presence and position of crown-of-thorns starfish in sequences of underwater images taken at various times and locations around the Great Barrier Reef. Predictions take the form of a bounding box together with a confidence score for each identified starfish. An image may contain zero or more starfish. Our model should evaluate the images in the same order as they were recorded in the video.<\/p>\n\n<img src=\"https:\/\/storage.googleapis.com\/kaggle-media\/competitions\/Google-Tensorflow\/video_thumb_kaggle.png\" style=\"width:720px;height:480px\"><\/img>\n\n[Data Metadata](https:\/\/www.kaggle.com\/c\/tensorflow-great-barrier-reef\/data): Based on the description in the data's metadata, we can summarize the structure of the data as following:\n1. image_id is in a format of: video_id + \"-\" + video_frame\n2. bounding box format is: (xmin, ymin, width, height) in pixels, the annotation is a list of dictionary with structure {x, y, width, height}\n3. there are gaps between frames in videos\n4. sequence is a subset of frames without gaps, but with no ordering\n\n<p style=\"text-align: justify;\">If you have any suggestions or find any issues related to interpretation, welcome to comment and to point those out. Thank you! Hope this notebook can be helpful for getting a better understanding about the data.<\/p>"}}