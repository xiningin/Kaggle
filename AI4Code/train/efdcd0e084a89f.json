{"cell_type":{"5fcd5508":"code","284cf74d":"code","ad7f7295":"code","98539f70":"code","4b39281f":"code","a999ba97":"code","2c234ba4":"code","89057467":"code","bdce7a06":"code","fb1ac76b":"code","5256e7f1":"code","e8de878a":"code","796fde01":"code","6bc64079":"code","6a27d823":"code","07535726":"code","4e18605e":"code","377d13d9":"code","ef0c0cb3":"code","c313df2a":"code","58925b18":"code","34d8eeae":"code","240f5e17":"code","196449c5":"code","79ba1f82":"code","6822fcb2":"markdown","6365cefd":"markdown","4fb183d9":"markdown","3efe07ce":"markdown","d6181ec7":"markdown"},"source":{"5fcd5508":"\n'''\n# import the time package to calculate the execution time of the kernel\nimport time\n#set on start variable the current time\nstart = time.time()\n# run your code and create a new variable with the time\nend = time.time()\n#substract the start time from end time to calculate the execution time\nelapsed = end - start'''\n\nimport time\nstart = time.time()\n\nimport pandas as pd # dataframes\nimport numpy as np # algebra & calculus\n\nimport os\nprint(os.listdir(\"..\/input\"))\nprint(os.listdir(\"..\/input\/instacart-market-basket-analysis\"))\nprint(os.listdir(\"..\/input\/ml-instacart-f1-0-38-part-one-features\"))\n\n\nimport gc #clean-up memory","284cf74d":"uxp = pd.read_pickle('..\/input\/ml-instacart-f1-0-38-part-one-features\/uxp.pkl')\n#uxp = uxp.iloc[0:150000]\nuxp.head()","ad7f7295":"'''\n#### Remove triple quotes to trim your dataset and experiment with your data\n### COMMANDS FOR CODING TESTING - Get 10% of users \nuxp = uxp.loc[uxp.user_id.isin(uxp.user_id.drop_duplicates().sample(frac=0.01, random_state=25))] \nuxp.head()\n'''","98539f70":"orders = pd.read_csv('..\/input\/instacart-market-basket-analysis\/orders.csv' )\norder_products_train = pd.read_csv('..\/input\/instacart-market-basket-analysis\/order_products__train.csv')\n\n#products = pd.read_csv('..\/input\/instacart-market-basket-analysis\/products.csv')","4b39281f":"orders_last = orders[(orders.eval_set=='train') | (orders.eval_set=='test') ]","a999ba97":"uxp = uxp.merge(orders_last, on='user_id', how='left')\nuxp.head(10)","2c234ba4":"uxp_train = uxp[uxp.eval_set=='train']\n\nuxp_train = uxp_train.merge(order_products_train, on=['product_id', 'order_id'], how='left' )\n\nuxp_train = uxp_train.drop(['order_id','eval_set', 'add_to_cart_order'], axis=1)\nuxp_train = uxp_train.fillna(0)\nuxp_train.head(20)","89057467":"uxp_test = uxp[uxp.eval_set=='test']\nuxp_test = uxp_test.drop(['eval_set', 'order_id'], axis=1)\nuxp_test = uxp_test.fillna(0)\nuxp_test.head(20)","bdce7a06":"del uxp\ndel orders_last\ngc.collect()","fb1ac76b":"uxp_train = uxp_train.set_index(['user_id', 'product_id'])\n\n'''#BALANCE REORDERED ROWS\nuxp_train_bal = uxp_train.copy()\nuxp_train_bal = uxp_train_bal[uxp_train_bal.reordered==0].sample(n=uxp_train_bal[uxp_train_bal.reordered==1].shape[0])\nuxp_train_bal = pd.concat([uxp_train_bal, uxp_train[uxp_train.reordered==1]])\nuxp_train_bal = uxp_train_bal.sample(frac=1)\nuxp_train = uxp_train_bal.copy()\nprint(uxp_train.reordered.value_counts())\ndel uxp_train_bal\ngc.collect()'''\n\n\nuxp_test = uxp_test.set_index(['user_id', 'product_id'])","5256e7f1":"import xgboost\nfrom sklearn.model_selection import train_test_split\nuxp_train.loc[:, 'reordered'] = uxp_train.reordered.fillna(0)\n\n\n# subsample\nX_train, X_val, y_train, y_val = train_test_split(uxp_train.drop('reordered', axis=1), uxp_train.reordered,\n                                                    test_size=0.2, random_state=42)\n\n'''del uxp_train'''\ngc.collect()\n\nd_train = xgboost.DMatrix(X_train, y_train)\nparam = {'max_depth':10, \n         'eta':0.02,\n         'colsample_bytree':0.4,\n         'subsample':0.75,\n         'silent':1,\n         'nthread':27,\n         'eval_metric':'logloss',\n         'binary':'logistic',\n         'tree_method':'hist'\n}\n\nwatchlist= [(d_train, \"train\")]\nbst = xgboost.train(params=param, dtrain=d_train, num_boost_round=1000, evals=watchlist, early_stopping_rounds=40, verbose_eval=5)\nxgboost.plot_importance(bst)","e8de878a":"del [X_train, X_val, y_train, y_val]\ngc.collect()","796fde01":"d_test = xgboost.DMatrix(uxp_test)\n\nuxp_test = uxp_test.reset_index()\nuxp_test = uxp_test[['product_id', 'user_id']]\n\nuxp_test[\"reordered\"] = bst.predict(d_test)\n\ndel bst","6bc64079":"orders_test = orders[orders.eval_set=='test']","6a27d823":"uxp_test = uxp_test.merge(orders_test[[\"user_id\", \"order_id\"]], on='user_id', how='left').drop('user_id', axis=1)\nuxp_test.columns = ['product_id', 'prediction', 'order_id']\nuxp_test.product_id = uxp_test.product_id.astype(int)\nuxp_test.order_id = uxp_test.order_id.astype(int)\nuxp_test.head()","07535726":"del orders\ndel orders_test\ngc.collect()","4e18605e":"import numpy as np\nfrom operator import itemgetter\n\nclass F1Optimizer():\n    def __init__(self):\n        pass\n\n    @staticmethod\n    def get_expectations(P, pNone=None):\n        expectations = []\n        P = np.sort(P)[::-1]\n\n        n = np.array(P).shape[0]\n        DP_C = np.zeros((n + 2, n + 1))\n        if pNone is None:\n            pNone = (1.0 - P).prod()\n\n        DP_C[0][0] = 1.0\n        for j in range(1, n):\n            DP_C[0][j] = (1.0 - P[j - 1]) * DP_C[0, j - 1]\n\n        for i in range(1, n + 1):\n            DP_C[i, i] = DP_C[i - 1, i - 1] * P[i - 1]\n            for j in range(i + 1, n + 1):\n                DP_C[i, j] = P[j - 1] * DP_C[i - 1, j - 1] + (1.0 - P[j - 1]) * DP_C[i, j - 1]\n\n        DP_S = np.zeros((2 * n + 1,))\n        DP_SNone = np.zeros((2 * n + 1,))\n        for i in range(1, 2 * n + 1):\n            DP_S[i] = 1. \/ (1. * i)\n            DP_SNone[i] = 1. \/ (1. * i + 1)\n        for k in range(n + 1)[::-1]:\n            f1 = 0\n            f1None = 0\n            for k1 in range(n + 1):\n                f1 += 2 * k1 * DP_C[k1][k] * DP_S[k + k1]\n                f1None += 2 * k1 * DP_C[k1][k] * DP_SNone[k + k1]\n            for i in range(1, 2 * k - 1):\n                DP_S[i] = (1 - P[k - 1]) * DP_S[i] + P[k - 1] * DP_S[i + 1]\n                DP_SNone[i] = (1 - P[k - 1]) * DP_SNone[i] + P[k - 1] * DP_SNone[i + 1]\n            expectations.append([f1None + 2 * pNone \/ (2 + k), f1])\n\n        return np.array(expectations[::-1]).T\n\n    @staticmethod\n    def maximize_expectation(P, pNone=None):\n        expectations = F1Optimizer.get_expectations(P, pNone)\n\n        ix_max = np.unravel_index(expectations.argmax(), expectations.shape)\n        max_f1 = expectations[ix_max]\n\n        predNone = True if ix_max[0] == 0 else False\n        best_k = ix_max[1]\n\n        return best_k, predNone, max_f1\n\n    @staticmethod\n    def _F1(tp, fp, fn):\n        return 2 * tp \/ (2 * tp + fp + fn)\n\n    @staticmethod\n    def _Fbeta(tp, fp, fn, beta=1.0):\n        beta_squared = beta ** 2\n        return (1.0 + beta_squared) * tp \/ ((1.0 + beta_squared) * tp + fp + beta_squared * fn)\n\n\ndef get_best_prediction(items, preds, pNone=None):\n#    print(\"Maximize F1-Expectation\")\n#    print(\"=\" * 23)\n    items_preds = sorted(list(zip(items, preds)), key=itemgetter(1), reverse=True)\n    P = [p for i,p in items_preds]\n    L = [i for i,p in items_preds]\n    \n    opt = F1Optimizer.maximize_expectation(P)\n    best_prediction = []\n    best_prediction += (L[:opt[0]])\n    if best_prediction == []:\n        best_prediction = ['None']\n            \n#    print(\"Prediction {} yields best E[F1] of {}\\n\".format(best_prediction, f1_max))\n    return ' '.join(list(map(str,best_prediction)))","377d13d9":"import pandas as pd\nimport multiprocessing as mp\nimport time\n\n#==============================================================================\n# load\n#==============================================================================\nsub_item = uxp_test.groupby(['order_id','product_id']).prediction.mean().reset_index()\nsub = sub_item.groupby('order_id').product_id.apply(list).to_frame()\nsub['yhat'] = sub_item.groupby('order_id').prediction.apply(list)\nsub.reset_index(inplace=True)\n\ndel uxp_test, sub_item\ngc.collect()\n\ndef multi(i):\n    if i%1000==0:\n        print('{:.3f} min'.format((time.time()-st_time)\/60))\n    items = sub.loc[i,'product_id']\n    preds = sub.loc[i,'yhat']\n    ret = get_best_prediction(items, preds)\n    return ret\n\nst_time = time.time()\npool = mp.Pool(4)\ncallback = pool.map(multi, range(sub.shape[0]))\n\nsub['products'] = callback\nsub.head()","ef0c0cb3":"sub.reset_index(inplace=True)\nsub = sub[['order_id', 'products']]","c313df2a":"'''d = dict()\nfor row in uxp_test.itertuples():\n    if row.reordered == 1:\n        try:\n            d[row.order_id] += ' ' + str(row.product_id)\n        except:\n            d[row.order_id] = str(row.product_id)\n\nfor order in uxp_test.order_id:\n    if order not in d:\n        d[order] = 'None'\n        \ngc.collect()'''","58925b18":"'''sub = pd.DataFrame.from_dict(d, orient='index')\nsub.reset_index(inplace=True)\nsub.columns = ['order_id', 'products']\n'''\n\nprint(sub.shape[0])\nprint(sub.shape[0]==75000)\n\nsub.to_csv('sub.csv', index=False)","34d8eeae":"print(os.listdir(\"..\/working\/\"))","240f5e17":"submission = pd.read_csv(\"..\/working\/sub.csv\")\nsubmission.head()","196449c5":"submission.shape[0]","79ba1f82":"end = time.time()\nelapsed = end - start\nelapsed","6822fcb2":"# 1. Import packages and data\nWe import the **time** package to calculate the execution time of our code. <br>\nWe import the **gc** package to free-up reserved memory by Python.\n\n","6365cefd":"We keep only the train and test orders, excluding all the prior orders (these that we used to create our features)","4fb183d9":"In addition, we load the original .csv files from Instacart that contains the orders and the products that have ben purchased","3efe07ce":"Now we load the pickle file that contains the prd table with several features that we have created in our EDA notebooks","d6181ec7":"# Introduction\nThis kernel has been created by the [Information Systems Lab](http:\/\/islab.uom.gr) at the University of Macedonia, Greece for the needs of the elective course Special Topics of Information Systems I at the [Business Administration](http:\/\/www.uom.gr\/index.php?tmima=2&categorymenu=2) department of the University of Macedonia, Greece.\n\n <br>\n\n# Objective\nThe objective of this Kernel is to predict the future behaviour (which products they will buy) based on the features that we have created in our EDA Notebooks.\n\nBy the time you finish this example, you will be able to:\n\nDescribe the steps of creating a predictive analytics model\nUse Python to manipulate ready features\nUse Python to create, combine, and delete data tables\nUse XGBoost to create a predictive model\nApply the predictive model in order to make a prediction\n\n# Problem Definition\nThe data that Instacart opened up include orders of 200,000 Instacart users with each user having between 4 and 100 orders. Instacart indicates each order in the data as prior, train or test. Prior orders describe the past behaviour of a user while train and test orders regard the future behaviour that we need to predict. As a result, we want to predict which previously purchased products (prior orders) will be in a user\u2019s next order (train and test orders). For the train orders Instacart reveals the results (i.e. the ordered products) while for the test orders we do not have this piece of information. Moreover, the future order of each user can be either train or test meaning that each user will be either a train or a test user. The setting of the Instacart problem is described in the figure below. \n\n<img src=\"https:\/\/i.imgur.com\/S0Miw3m.png\" width=\"350\">\n\nEach user has purchased various products during their prior orders. Moreover, for each user we know the order_id of their future order. The goal is to predict which of these products will be in a user's future order. This is a classification problem because we need to predict whether each pair of user and product is a reorder or not. This is indicated by the value of the reordered variable, i.e. reordered=1 or reordered=0 (see figure below). \n\n<img src=\"https:\/\/i.imgur.com\/SxK2gsR.png\" width=\"350\">\n\nAs a result we need to come up and calculate various predictor variables (X) that will describe the characteristics of a product and the behaviour of a user regarding one or multiple products. We will do so by analysing the prior orders of the dataset. We will then use the train users to create a predictive model and the test users to make our actual prediction. As a result we create a table as the following one and we train an algorithm based on predictor variables (X) and response variable (Y).\n\n<img src=\"https:\/\/i.imgur.com\/Yb1CKAF.png\" width=\"600\">\n\n# Method\nOur method includes the following steps:\n1. <b>Import the ready features from EDA notebooks and reshape data<\/b>: This step includes loading pkl (pickle) files into pandas DataFrames.\n2. <b>Create the test and train DataFrames<\/b>: In this step we create two distinct DataFrames that will be used in the creation and the use of the predictive model.\n4. <b>Create the preditive model<\/b>: In this step we employ XGBoost algorithm to create the predictive model through the train dataset.\n5. <b>Apply the model<\/b>: This step includes applying the model to predict the 'reordered' variable for the test dataset.\n\n# Business Insights\n\n# Python Skills\n\n# Packages \n"}}