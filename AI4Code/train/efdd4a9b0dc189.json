{"cell_type":{"4bd9a5bf":"code","41c51a64":"code","674c63d7":"code","fe0b73e7":"code","4babf19f":"code","469d26dd":"code","00b75de8":"code","04fa7991":"code","3dd45c8b":"code","713fe2dc":"code","f96f221a":"code","e29513bb":"code","472866b0":"code","78edfd58":"code","a10935d2":"code","a38c6b2b":"code","723a77f7":"code","1de2dad7":"code","199c9cc6":"code","50815fba":"code","c058ddc5":"code","938c6bb6":"code","065f59a2":"code","1bc8aba5":"code","8af5b931":"code","4dff646d":"code","1a0f1f8d":"code","8aa9a9dd":"code","b84e90dd":"code","147b540d":"code","30e25987":"code","bdd3fd73":"code","23ed9ebb":"code","10cc4181":"code","1d47a09e":"code","a431efff":"code","7dc18e9f":"code","f8d3c9df":"code","648cfa76":"code","a6f51bdc":"code","4a52c0ea":"code","7fb1f79c":"code","20fb0f29":"code","43a61919":"code","e6e4c643":"code","823c5e6f":"code","30db7c55":"code","cb99892a":"code","3dcfec07":"code","0f4fef1a":"code","7e938cee":"code","1b907017":"code","165f6511":"code","8f0fea62":"code","e28742c6":"markdown","27386bb2":"markdown","546f21ac":"markdown","3985fb2e":"markdown","21dd79bb":"markdown"},"source":{"4bd9a5bf":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","41c51a64":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns","674c63d7":"data = pd.read_csv('\/kaggle\/input\/rossmann-store-sales\/train.csv')\nstore= pd.read_csv('\/kaggle\/input\/rossmann-store-sales\/store.csv')\ntest= pd.read_csv('\/kaggle\/input\/rossmann-store-sales\/test.csv')","fe0b73e7":"print(data.shape)\nprint(store.shape)","4babf19f":"data.head()","469d26dd":"store.head()","00b75de8":"data.info()\n# data.dtypes","04fa7991":"data.describe(include='object')","3dd45c8b":"data.describe()[['Sales','Customers']]","713fe2dc":"data.describe()[['Sales','Customers']].loc['mean']","f96f221a":"data.describe()[['Sales','Customers']].loc['min']","e29513bb":"data.describe()[['Sales','Customers']].loc['max']","472866b0":"data.Store.nunique()","78edfd58":"data.head()\ndata.Store.value_counts().head(50).plot.bar()","a10935d2":"data.Store.value_counts().tail(50).plot.bar()","a38c6b2b":"data.Store.value_counts()","723a77f7":"data.DayOfWeek.value_counts()","1de2dad7":"data.Open.value_counts()","199c9cc6":"data.Promo.value_counts()","50815fba":"data['Date']=pd.to_datetime(data['Date'],format='%Y-%m-%d')\nstore_id= data.Store.unique()[0]\nprint(store_id)\nstore_rows=data[data['Store']==store_id]\nprint(store_rows.shape)\n# store_rows.resample('1D',on='Date')['Sales'].sum().plot.line(figsize=(14,4))","c058ddc5":"# store_rows[store_rows['Sales']==0]","938c6bb6":"test['Date']=pd.to_datetime(test['Date'],format='%Y-%m-%d')\nstore_test_rows = test[test['Store']==store_id]\nstore_test_rows['Date'].min(),store_test_rows['Date'].max()","065f59a2":"store_rows['Sales'].plot.hist()\n# it is slightly skewed.","1bc8aba5":"data['Sales'].plot.hist()\n# it is slightly skewed.","8af5b931":"store.head()","4dff646d":"# store.isna.sum()","1a0f1f8d":"store_id=store[store['Store']==1].T","8aa9a9dd":"store[~store['Promo2SinceYear'].isna()].iloc[0]","b84e90dd":"# Method1\nstore = pd.read_csv('\/kaggle\/input\/rossmann-store-sales\/store.csv')\nstore['Promo2SinceWeek']= store['Promo2SinceWeek'].fillna(0)\nstore['Promo2SinceYear']= store['Promo2SinceYear'].fillna(store['Promo2SinceYear'].mode().iloc[0])\nstore['PromoInterval']= store['PromoInterval'].fillna(store['PromoInterval'].mode().iloc[0]) \n\nstore['CompetitionDistance']=store['CompetitionDistance'].fillna(store['CompetitionDistance'].max())\nstore['CompetitionOpenSinceMonth']= store['CompetitionOpenSinceMonth'].fillna(store['CompetitionOpenSinceMonth'].mode().iloc[0])\nstore['CompetitionOpenSinceYear']= store['CompetitionOpenSinceYear'].fillna(store['CompetitionOpenSinceYear'].mode().iloc[0])\nstore.isna().sum()\n      ","147b540d":"data_merged = data.merge(store, on='Store',how='left')\nprint(data.shape)\nprint(data_merged.shape)\nprint(data_merged.isna().sum().sum()) #to cross check if there are any missing values","30e25987":"# encoding\n# 3 categorical column,1 date column, rest are numerical\n# data_merged.dtypes\ndata_merged['day']=data_merged['Date'].dt.day\ndata_merged['month']=data_merged['Date'].dt.month\ndata_merged['year']=data_merged['Date'].dt.year\n#data_merged['dayofweek']=data_merged['Date'].dt.strftime('%a')\n","bdd3fd73":"# Decision tress - label encoding should be used.\n# regression - one hot encoding must be used.","23ed9ebb":"# data_merged.dtypes\n# StateHoliday,StoreType,Assortment,PromoInterval\ndata_merged['StateHoliday'].unique()\n# for creating dummy variables - label encoding is used\ndata_merged['StateHoliday']=data_merged['StateHoliday'].map({'0':0,0:0,'a':1,'b':2,'c':3})\ndata_merged['StateHoliday']=data_merged['StateHoliday'].astype(int)\ndata_merged","10cc4181":"# encoding assorted\ndata_merged['Assortment']\n# for creating dummy variables - label encoding is used\ndata_merged['Assortment']=data_merged['Assortment'].map({'a':1,'b':2,'c':3})\ndata_merged['Assortment']=data_merged['Assortment'].astype(int)\ndata_merged","1d47a09e":"data_merged['StoreType'].unique()\ndata_merged['StoreType']=data_merged['StoreType'].map({'a':1,'b':2,'c':3,'d':4})\ndata_merged['StoreType']=data_merged['StoreType'].astype(int)\ndata_merged","a431efff":"data_merged['PromoInterval'].unique()\nmap_promo = {'Jan,Apr,Jul,Oct':1,'Feb,May,Aug,Nov':2,'Mar,Jun,Sept,Dec':3}\ndata_merged['PromoInterval']=data_merged['PromoInterval'].map(map_promo)\ndata_merged","7dc18e9f":"# Train and validate Split\nfeatures= data_merged.columns.drop(['Sales','Customers','Date'])\nfrom sklearn.model_selection import train_test_split\ntrain_x,validate_x,train_y,validate_y = train_test_split(data_merged[features],np.log(data_merged['Sales']+1),test_size=0.2,random_state=1)\ntrain_x.shape,validate_x.shape,train_y.shape,validate_y.shape","f8d3c9df":"# from sklearn.tree import DecisionTreeRegressor\n\n# model_dt = DecisionTreeRegressor(max_depth=20,random_state=1).fit(train_x,train_y)\n# validate_y_pred = model_dt.predict(validate_x)\n\nfrom sklearn.tree import DecisionTreeRegressor\nmodel_dt=DecisionTreeRegressor(max_depth=11,random_state=1).fit(train_x,train_y)\nvalidate_y_pred=model_dt.predict(validate_x)","648cfa76":"# !pip install pydotplus","a6f51bdc":"# def draw_tree(model, columns):\n#     import pydotplus\n#     from sklearn.externals.six import StringIO\n#     from IPython.display import Image\n#     import os\n#     from sklearn import tree\n    \n#     graphviz_path = 'C:\\Program Files (x86)\\Graphviz2.38\/bin\/'\n#     os.environ[\"PATH\"] += os.pathsep + graphviz_path\n\n#     dot_data = StringIO()\n#     tree.export_graphviz(model,\n#                          out_file=dot_data,\n#                          feature_names=columns)\n#     graph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \n#     return Image(graph.create_png())","4a52c0ea":"#draw_tree(model_dt,features)","7fb1f79c":"validate_y_pred = model_dt.predict(validate_x)\nfrom sklearn.metrics import mean_squared_error\nvalidate_y_inv = np.exp(validate_y) - 1\nvalidate_y_pred_inv = np.exp(validate_y_pred) - 1\nrmse_val=np.sqrt(mean_squared_error(validate_y_inv , validate_y_pred_inv))\n\ndef ToWeight(y):\n    w = np.zeros(y.shape, dtype=float)\n    ind = y != 0\n    w[ind] = 1.\/(y[ind]**2)\n    return w\n\ndef rmspe(y, yhat):\n    w = ToWeight(y)\n    rmspe = np.sqrt(np.mean( w * (y - yhat)**2 ))\n    return rmspe\n\nvalidate_y_inv = np.exp(validate_y) - 1\nvalidate_y_pred_inv = np.exp(validate_y_pred) - 1\nrmse_val=np.sqrt(mean_squared_error(validate_y_inv , validate_y_pred_inv))\nrmspe_val=rmspe(validate_y_inv , validate_y_pred_inv)\nprint(rmse_val,rmspe_val)","20fb0f29":"# submitting the train on test data set","43a61919":"model_dt.feature_importances_","e6e4c643":"import matplotlib.pyplot as plt\nplt.figure(figsize=(10,5))\nplt.barh(features,model_dt.feature_importances_)\npd.Series(model_dt.feature_importances_,index=features)","823c5e6f":"#Hyperparameter tuning\nfrom sklearn.model_selection import GridSearchCV\n\nparameters={'max_depth':list(range(5,20))}\nbase_model=DecisionTreeRegressor()\ncv_model=GridSearchCV(base_model,param_grid=parameters,cv=5,return_train_score=True).fit(train_x,train_y)\nparameters","30db7c55":"cv_model.best_params_","cb99892a":"stores_avg_cust = data.groupby(['Store'])[['Customers']].mean().reset_index().astype(int)\ntest_1 = test.merge(stores_avg_cust,on='Store',how='left')\ntest.shape,test_1.shape\ntest_merged = test_1.merge(store,on='Store',how='inner')\ntest_merged['Open']=test_merged['Open'].fillna(1)\ntest_merged['Date']=pd.to_datetime(test_merged[\"Date\"],format='%Y-%m-%d')\ntest_merged['day']=test_merged['Date'].dt.day\ntest_merged['month']=test_merged['Date'].dt.month\ntest_merged['year']=test_merged['Date'].dt.year\ntest_merged['StateHoliday']=test_merged['StateHoliday'].map({'0':0,'a':1})\ntest_merged['StateHoliday']=test_merged['StateHoliday'].astype(int)\ntest_merged['Assortment']=test_merged['Assortment'].map({'a':1,'b':2,'c':3})\ntest_merged['Assortment']=test_merged['Assortment'].astype(int)\ntest_merged['StoreType']=test_merged['StoreType'].map({'a':1,'b':2,'c':3,'d':4})\ntest_merged['StoreType']=test_merged['StoreType'].astype(int)\nmap_promo = {'Jan,Apr,Jul,Oct':1,'Feb,May,Aug,Nov':2,'Mar,Jun,Sept,Dec':3}\ntest_merged['PromoInterval']=test_merged['PromoInterval'].map(map_promo)\n","3dcfec07":"df_cv_results=pd.DataFrame(cv_model.cv_results_).sort_values(by='mean_test_score',ascending=False)[['param_max_depth','mean_test_score','mean_train_score']]\nimport matplotlib.pyplot as plt\ndf_cv_results.set_index('param_max_depth')['mean_test_score'].plot.line()\ndf_cv_results.set_index('param_max_depth')['mean_train_score'].plot.line()\nplt.legend(['Test Scores','Train Scores'])","0f4fef1a":"df_cv_results[df_cv_results['param_max_depth']==11].T","7e938cee":"test_merged","1b907017":"test_pred = model_dt.predict(test_merged[features])\ntest_pred_inv = np.exp(test_pred) - 1","165f6511":"submission = pd.read_csv('\/kaggle\/input\/rossmann-store-sales\/sample_submission.csv')\nsubmission_predicted = pd.DataFrame({'Id':test['Id'],'Sales':test_pred_inv})\nsubmission_predicted.to_csv('submission.csv',index=False)\nsubmission_predicted.head()","8f0fea62":"from sklearn.model_selection import GridSearchCV\ndef get_rmspe_score(model,input_values,y_actual):\n    y_predicted=model.predict(input_values)\n    y_actual=np.exp(y_actual)-1\n    y_predicted=np.exp(y_predicted)-1\n    score=rmspe(y_actual,y_predicted)\n    return score\n\nparameters={'max_depth':list(range(5,8))}\nbase_model=DecisionTreeRegressor()\ncv_model=GridSearchCV(base_model,param_grid=parameters,cv=5,return_train_score=True,scoring=get_rmspe_score).fit(train_x,train_y)\npd.DataFrame(cv_model.cv_results_)[['params','mean_test_score','mean_train_score']]","e28742c6":"****Store - It is categorical column hence min and max must not be done. we do value counts to check if every category has same amount of data********","27386bb2":"# steps done in the current ml data\n\n* summary stats\n* understand your data\n* data preprocessing\n --missing value imputation\n --transformation\n --scaling\n --encoding\n* model fitting'\n* model summary\n* Prediction\n* Evaluation","546f21ac":"store_test_rows","3985fb2e":"****Granularity - Here it is the metadata of the branches with respect to promotional events events conducted overall****","21dd79bb":"****Granularity - One big brand has various number of branches and each rows is the data of that branch day wise****"}}