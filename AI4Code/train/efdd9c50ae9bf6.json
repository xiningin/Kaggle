{"cell_type":{"89beb4e7":"code","8932b651":"code","e19595d9":"code","1670b355":"code","39b11d5d":"code","3cb2f097":"code","f9bbb116":"code","6a03a1eb":"code","db23df18":"code","c0b48b55":"code","9d14da8d":"code","32bc4bec":"code","935ca388":"code","567d25fd":"code","ae87537f":"code","00ff1f9b":"code","8a3a7a1c":"code","fa763e24":"code","395fc068":"code","ab8e32e0":"code","37b9fc63":"code","a16a3bf9":"code","823428c3":"code","5477fdb7":"code","e3800c13":"code","bdd042cf":"code","8e5e1c1b":"code","8f73a3c2":"code","20df85ee":"code","d68d297e":"code","28f34f99":"code","29ca0446":"code","1febb89e":"code","4600862f":"code","e455a012":"code","d69241a4":"code","6dc90000":"code","65bf2d17":"code","a904985d":"code","2672d8fa":"code","b67900aa":"code","2972a965":"code","052b7d31":"code","5b10dbca":"code","f03c870b":"code","9fbcca5c":"code","d5fc892e":"code","0a4750d7":"code","de6601ad":"code","1f59ad76":"code","c0bdb15f":"code","dfb71d40":"code","53496d17":"code","9515a57d":"code","dc7a69b4":"code","72171924":"code","fb07b229":"code","04728991":"code","fa844c46":"code","3499913e":"code","97b42a50":"code","85192575":"code","8f56add2":"code","46a6f7e3":"code","b020d8e7":"code","3d5674bc":"code","77c7d35e":"code","5d233c3f":"code","167c4326":"code","a95d40cb":"code","f8f422fc":"code","5505a4ba":"code","e7ee603a":"code","1f04cf17":"code","bab3b177":"code","c53487ff":"code","259da08d":"code","687191bd":"code","0f832445":"code","7b400396":"code","90588117":"code","654ec9c8":"code","19cc0a6a":"code","3cb4b706":"code","83213af8":"code","7ac2f016":"markdown","6235cccd":"markdown","2b12ea2b":"markdown","b8bcba65":"markdown","055bd9b7":"markdown","d7e7ecad":"markdown","9da2dde0":"markdown","9d68abba":"markdown","1d2150ae":"markdown","81b13ba2":"markdown","541bd807":"markdown","153b0091":"markdown","dfdf3962":"markdown","f511f933":"markdown"},"source":{"89beb4e7":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom xgboost import XGBClassifier\n\nimport warnings\nwarnings.filterwarnings('ignore')","8932b651":"train_df = pd.read_csv('..\/input\/loan-prediction-problem-dataset\/train_u6lujuX_CVtuZ9i.csv')\ntest_df = pd.read_csv('..\/input\/loan-prediction-problem-dataset\/test_Y3wMUE5_7gLdaTN.csv')\n\ndf = [train_df, test_df]","e19595d9":"train_df.info()","1670b355":"train_df.head()","39b11d5d":"sns.heatmap(train_df.isnull(), cbar=False, yticklabels=False)","3cb2f097":"# Loan_Status feature --- target variable\ntrain_df['Loan_Status'] = train_df.Loan_Status.map({'Y': 1, 'N': 0}).astype(int)","f9bbb116":"# Gender feature\ntrain_df.Gender.value_counts()","6a03a1eb":"train_df.Gender.isnull().sum()","db23df18":"test_df.Gender.isnull().sum()","c0b48b55":"train_df[['Gender', 'Loan_Status']].groupby('Gender', as_index=False).mean()","9d14da8d":"grid = sns.FacetGrid(train_df, col='Loan_Status')\ngrid.map(plt.hist, 'Gender')","32bc4bec":"for dataset in df:\n    dataset.Gender.fillna('Male', inplace=True)","935ca388":"train_df.Gender.isnull().sum()","567d25fd":"# Changing Gender feature into numeric so that our model works properly, kind of label encoding\nfor dataset in df:\n    dataset['Gender'] = dataset['Gender'].map({'Male': 1, 'Female': 0}).astype(int)","ae87537f":"# Married Feature\ntrain_df.Married.value_counts()","00ff1f9b":"train_df.Married.isnull().sum()","8a3a7a1c":"for dataset in df:\n    dataset['Married'] = dataset.Married.fillna(dataset.Married.mode()[0])","fa763e24":"train_df[['Married', 'Loan_Status']].groupby('Married', as_index=False).mean()","395fc068":"sns.set(style='whitegrid')\ngrid = sns.FacetGrid(train_df, col='Loan_Status')\ngrid.map(plt.hist, 'Married') ","ab8e32e0":"grid = sns.FacetGrid(train_df, row='Education', size=2.8, aspect=1.6)\ngrid.map(sns.barplot, 'Married', 'Loan_Status', 'Gender', ci=None, palette='deep')\ngrid.add_legend()","37b9fc63":"for dataset in df:\n    dataset['Married'] = dataset['Married'].map({'Yes': 1, 'No': 0}).astype(int)","a16a3bf9":"# Dependents feature\ntrain_df.Dependents.value_counts()","823428c3":"train_df.Dependents.isnull().sum()","5477fdb7":"grid = sns.FacetGrid(train_df, row='Gender')\ngrid.map(sns.barplot, 'Dependents', 'Loan_Status', palette='deep', ci=None)\ngrid.add_legend()","e3800c13":"for dataset in df:\n    dataset['Dependents'] = dataset['Dependents'].fillna(train_df.Dependents.mode()[0])\n    dataset['Dependents'] = dataset['Dependents'].replace('3+', '3')\n    dataset['Dependents'] = dataset.Dependents.astype(int)","bdd042cf":"train_df.head()","8e5e1c1b":"# Education, I do this every time to check any error or typos in any categorical feature.\ntrain_df.Education.value_counts()","8f73a3c2":"train_df[['Education', 'Loan_Status']].groupby('Education', as_index=False).mean()","20df85ee":"train_df.Education.isnull().sum()","d68d297e":"test_df.Education.isnull().sum()","28f34f99":"for dataset in df:\n    dataset['Education'] = dataset['Education'].map({'Graduate': 1, 'Not Graduate': 0}).astype(int)","29ca0446":"# Self_Employed\ntrain_df.Self_Employed.value_counts()","1febb89e":"train_df.Self_Employed.isnull().sum()","4600862f":"train_df[['Self_Employed', 'Loan_Status']].groupby('Self_Employed', as_index=False).mean()","e455a012":"for dataset in df:\n    dataset['Self_Employed'] = dataset['Self_Employed'].fillna(dataset['Self_Employed'].mode()[0])\n    dataset['Self_Employed'] = dataset['Self_Employed'].map({'No': 0, 'Yes': 1}).astype(int)","d69241a4":"# Credit_History\ntrain_df.Credit_History.value_counts()","6dc90000":"train_df.Credit_History.isnull().sum()","65bf2d17":"# gender, married, credit history, loan status\n# gender, education, credit history, loan status\n# gender, self employed, credit history, loan status","a904985d":"grid = sns.FacetGrid(train_df, row='Married', aspect=1.5)\ngrid.map(sns.barplot, 'Credit_History', 'Loan_Status', 'Gender', palette='deep', ci=None)\ngrid.add_legend()","2672d8fa":"grid = sns.FacetGrid(train_df, row='Education', aspect=1.5)\ngrid.map(sns.barplot, 'Credit_History', 'Loan_Status', 'Gender', palette='deep', ci=None)\ngrid.add_legend()","b67900aa":"grid = sns.FacetGrid(train_df, row='Self_Employed', aspect=1.5)\ngrid.map(sns.barplot, 'Credit_History', 'Loan_Status', 'Gender', palette='deep', ci=None)\ngrid.add_legend()","2972a965":"for dataset in df:\n    dataset['Credit_History'] = dataset['Credit_History'].fillna(dataset['Credit_History'].mode()[0]).astype(int)","052b7d31":"# Property_Area\ntrain_df.Property_Area.value_counts()","5b10dbca":"train_df.Property_Area.isnull().sum()","f03c870b":"train_df[['Property_Area', 'Loan_Status']].groupby('Property_Area', as_index=False).mean().sort_values(by='Loan_Status', ascending=False)","9fbcca5c":"grid = sns.FacetGrid(train_df, row='Married', aspect=1.5)\ngrid.map(sns.barplot, 'Property_Area', 'Loan_Status', 'Gender', palette='deep', ci=None)\ngrid.add_legend()","d5fc892e":"for dataset in df:\n    dataset['Property_Area'] = dataset['Property_Area'].map({'Rural': 0, 'Urban': 1, 'Semiurban': 2}).astype(int)","0a4750d7":"train_df.head()","de6601ad":"# ApplicantIncome, CoapplicantIncome, LoanAmount, Loan_Amount_Term.","1f59ad76":"train_df.describe()","c0bdb15f":"sns.set(style='darkgrid')\nsns.boxplot(train_df.ApplicantIncome)","dfb71d40":"sns.set(style='darkgrid')\nsns.boxplot(train_df.CoapplicantIncome)","53496d17":"sns.set(style='darkgrid')\nsns.boxplot(train_df.LoanAmount)","9515a57d":"sns.set(style='darkgrid')\nsns.boxplot(train_df.Loan_Amount_Term)","dc7a69b4":"train_df['ApplicantIncome'] = train_df['ApplicantIncome'].astype(int)","72171924":"train_df['ApplicantIncomeBand'] = pd.cut(train_df['ApplicantIncome'], 4)\ntrain_df[['ApplicantIncomeBand', 'Loan_Status']].groupby('ApplicantIncomeBand', as_index=False).mean().sort_values(by='ApplicantIncomeBand', ascending=True)","fb07b229":"train_df['CoapplicantIncome'] = train_df['CoapplicantIncome'].astype(int)","04728991":"train_df['CoapplicantIncomeBand'] = pd.cut(train_df['CoapplicantIncome'], 3)\ntrain_df[['CoapplicantIncomeBand', 'Loan_Status']].groupby('CoapplicantIncomeBand', as_index=False).mean().sort_values(by='CoapplicantIncomeBand', ascending=True)","fa844c46":"for dataset in df:\n    dataset['LoanAmount'] = dataset['LoanAmount'].fillna(dataset['LoanAmount'].mean())","3499913e":"train_df['LoanAmountBand'] = pd.cut(train_df['LoanAmount'], 4)\ntrain_df[['LoanAmountBand', 'Loan_Status']].groupby('LoanAmountBand', as_index=False).mean().sort_values(by='LoanAmountBand', ascending=True)","97b42a50":"for dataset in df:\n    dataset['Loan_Amount_Term'] = dataset['Loan_Amount_Term'].fillna(dataset['Loan_Amount_Term'].mean())","85192575":"train_df['Loan_Amount_TermBand'] = pd.cut(train_df['Loan_Amount_Term'], 3)\ntrain_df[['Loan_Amount_TermBand', 'Loan_Status']].groupby('Loan_Amount_TermBand', as_index=False).mean().sort_values(by='Loan_Amount_TermBand', ascending=True)","8f56add2":"train_df.head()","46a6f7e3":"for dataset in df:\n    dataset.loc[dataset['ApplicantIncome'] <= 20362.5, 'ApplicantIncome'] = 0\n    dataset.loc[(dataset['ApplicantIncome'] > 20362.5) & (dataset['ApplicantIncome'] <= 40575.0), 'ApplicantIncome'] = 1\n    dataset.loc[(dataset['ApplicantIncome'] > 40575.0) & (dataset['ApplicantIncome'] <= 60787.5), 'ApplicantIncome'] = 2\n    dataset.loc[(dataset['ApplicantIncome'] > 60787.5), 'ApplicantIncome'] = 3","b020d8e7":"for dataset in df:\n    dataset.loc[dataset['CoapplicantIncome'] <= 13889.0, 'CoapplicantIncome'] = 0\n    dataset.loc[(dataset['CoapplicantIncome'] > 13889.0) & (dataset['CoapplicantIncome'] <= 27778.0), 'CoapplicantIncome'] = 1\n    dataset.loc[(dataset['CoapplicantIncome'] > 27778.0), 'CoapplicantIncome'] = 2","3d5674bc":"for dataset in df:\n    dataset.loc[dataset['LoanAmount'] <= 181.75, 'LoanAmount'] = 0\n    dataset.loc[(dataset['LoanAmount'] > 181.75) & (dataset['LoanAmount'] <= 354.5), 'LoanAmount'] = 1\n    dataset.loc[(dataset['LoanAmount'] > 354.5) & (dataset['LoanAmount'] <= 527.25), 'LoanAmount'] = 2\n    dataset.loc[(dataset['LoanAmount'] > 527.25), 'LoanAmount'] = 3\n    dataset['LoanAmount'] = dataset['LoanAmount'].astype(int)","77c7d35e":"for dataset in df:\n    dataset.loc[dataset['Loan_Amount_Term'] <= 168.0, 'Loan_Amount_Term'] = 0\n    dataset.loc[(dataset['Loan_Amount_Term'] > 168.0) & (dataset['Loan_Amount_Term'] <= 324.0), 'Loan_Amount_Term'] = 1\n    dataset.loc[(dataset['Loan_Amount_Term'] > 324.0), 'Loan_Amount_Term'] = 2\n    dataset['Loan_Amount_Term'] = dataset['Loan_Amount_Term'].astype(int)","5d233c3f":"train_df.head()","167c4326":"train_df.drop('ApplicantIncomeBand', inplace=True, axis=1)\ntrain_df.drop('CoapplicantIncomeBand', inplace=True, axis=1)\ntrain_df.drop('LoanAmountBand', inplace=True, axis=1)\ntrain_df.drop('Loan_Amount_TermBand', inplace=True, axis=1)","a95d40cb":"for dataset in df:\n    dataset.drop('Loan_ID', axis=1, inplace=True)","f8f422fc":"X = train_df.drop('Loan_Status', axis=1)\ny = train_df['Loan_Status']","5505a4ba":"data_corr = pd.concat([X, y], axis=1)\ncorr = data_corr.corr()\nplt.figure(figsize=(11,7))\nsns.heatmap(corr, annot=True)","e7ee603a":"LogReg_classifier = LogisticRegression()\nLogReg_classifier.fit(X,y)","1f04cf17":"LogReg_acc = cross_val_score(LogReg_classifier, X, y, cv=10, scoring='accuracy').mean()\nLogReg_acc","bab3b177":"SVM_classifier = SVC()\nSVM_classifier.fit(X,y)","c53487ff":"SVM_acc = cross_val_score(SVM_classifier, X, y, cv=10, scoring='accuracy').mean()\nSVM_acc","259da08d":"Knn_classifier = KNeighborsClassifier()\nKnn_classifier.fit(X,y)","687191bd":"Knn_acc = cross_val_score(Knn_classifier, X, y, cv=10, scoring='accuracy').mean()\nKnn_acc","0f832445":"Tree_classifier = DecisionTreeClassifier()\nTree_classifier.fit(X,y)","7b400396":"Tree_acc = cross_val_score(Tree_classifier, X, y, cv=10, scoring='accuracy').mean()\nTree_acc","90588117":"Ran_classifier = RandomForestClassifier(n_estimators=100)\nRan_classifier.fit(X, y)","654ec9c8":"Ran_acc = cross_val_score(Ran_classifier, X, y, cv=10, scoring='accuracy').mean()\nRan_acc","19cc0a6a":"XGB_classifier = XGBClassifier()\nXGB_classifier.fit(X,y)","3cb4b706":"XGB_acc = cross_val_score(XGB_classifier, X, y, cv=10, scoring='accuracy').mean()\nXGB_acc","83213af8":"acc_dict = {'Logistic Regression': round(LogReg_acc, 2), \n           'Support Vectore Classifier': round(SVM_acc, 2), \n           'K-nearest Neighbor': round(Knn_acc, 2), \n           'Decision Tree': round(Tree_acc, 2), \n           'Random Forest': round(Ran_acc, 2),\n            'XGB': round(XGB_acc, 2)\n           }\nprint('Accuracy Scores:-')\nacc_dict","7ac2f016":"## let's have a look on our features:-\n    \n    Categorical features:-\n        numeric:-  \n            Credit_History\n            Dependents\n        non-numeric:-\n            Gender\n            Married\n            Education\n            Self_Employed\n            Property_Area\n            Loan_Status(Target variable)\n    Numeric features:-\n        ApplicantIncome\n        CoapplicantIncome\n        LoanAmount\n        Loan_Amount_Term","6235cccd":"best models are logistic regression and SVM for predicting the output. If you have any suggestions for me please let me know, and if you like my notebook please upvote that.\ud83d\ude0a\ud83d\ude0a","2b12ea2b":"Here Males has high correlation and also mode of our gender feature is 'Male', So I decided to fill the nan values with male category.","b8bcba65":"## Loading the dataset:-\nloading the data of train and test part where we run our model on train part and find the accuracy based on test part that is for testing purpose by which we can find out that how well our model has performed.\n\nHere I created a list of train and test part so that I can make changes in both parts simultaneously, if it is necessary in both.","055bd9b7":"Here if a person is male with so and so Dependents his correlation ranges between (0.6-0.8) but if any female has more then 3 dependents then her correlation suddenly falls down. This feature may doesn't affect direclty to target variable like with increase in dependents correlation neither decreases nor increases strictly. It may affect other feature like Gender in this case.\n\nI can also create another feature from this feature by the name of 'Dependent_3+' which indicates that any person has dependents more than 3 or less, but I'll do that later if accuracy is not good enough.","d7e7ecad":"So if a female is not graduated and has married life she would be more likely to have loan infact if any female is not married she still would be more likely to have a loan then male.(oh! do you remember 0--female, 1--male)","9da2dde0":"There is not that much null values in any feature so I have to fill them up rather then drop any feature well do you know this before if there is lot of null values in any feature comparable to its length then just drop that as it doesn't add any valuable information to our dataset. Also I have to drop Loan_ID as it is just the id and does not impact the target variable.","9d68abba":"if any person is married and has 1 credit history then he\/she would be more likely to have a loan then unmarried one. But overall if any person has 1 credit history then his or her chances are higher to get a loan.","1d2150ae":"Based on matrices created above, mapped accordingly to each feature to create groups.","81b13ba2":"## Feature Engineering\nlet's go feature by feature, generally data scientist makes the steps to solve any problem like **Feature Engineering, Feature Selection, EDA, Model Training etc**. In generall first three processes are used to look into the data, understand that and then make predictions, to achieve good accuracy we do these steps.\n\nBut I personally like to go feature by feature, selecting a feature look over it and do changes if needed. This gives be more command on each feature and able to understand it more.\n\n**General-->** took the data and do all the necessary steps(feature engineering, feature selection, data analysis)\n\n**me-->** I have the data then I took the feature and do all necessary steps on that feature, this all goes on every feature.","541bd807":"## Importing Libraries","153b0091":"Wow! I don't know this before if anyone is married he\/she would be more likely to have a loan.","dfdf3962":"if any person is from semiurban area then that person has higher chances to get a loan.","f511f933":"## Model Training\nTo check accuracy I am using k fold cross validation score, it makes number of train and test parts of the data according to the parameter 'cv' and then mean gave the mean of all outputs. To know more about cross validation score check out this https:\/\/machinelearningmastery.com\/k-fold-cross-validation\/"}}