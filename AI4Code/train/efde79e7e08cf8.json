{"cell_type":{"7430d2cf":"code","43648fcb":"code","3c72b977":"code","b6c17964":"code","98915c99":"code","765aaf5b":"code","4813fdba":"code","d90a2dc3":"code","1c87739d":"code","a8f91a43":"code","c4d71088":"code","b8ffc870":"code","8dfc764b":"code","975010ae":"code","69252bd0":"code","a41d57a3":"code","7bf63b70":"code","ee6c9e8d":"code","47482ada":"code","6d29388e":"code","d33492a7":"code","9d256319":"code","c0678082":"code","d5713591":"code","0221afe3":"code","ce712a4a":"code","be955f9c":"code","4f32b848":"code","29efc0ff":"code","01b916db":"code","171187c5":"code","618b12ec":"code","00cb9647":"code","1864e1df":"code","17ac5685":"code","4295d76b":"code","1dbe4e7f":"code","71f680a8":"code","e39e9248":"code","1acadf38":"code","2cd12dac":"code","3a0dd34b":"code","ce06da98":"code","f79de6cf":"code","13f838c6":"code","d1a71e9b":"code","669e300c":"code","48b726e3":"code","f0087c71":"code","18cf5943":"code","32764f64":"code","8d6e8517":"code","5c74eebf":"code","01be880f":"code","92287442":"code","26b007ee":"code","84eb4a2d":"code","7e56f756":"code","9275f9c9":"code","8419456e":"code","f9e182db":"markdown","62b21145":"markdown","7bc72319":"markdown","47a4c6ef":"markdown","d75ee7a8":"markdown","808140d7":"markdown","94d58eae":"markdown","02632b03":"markdown","69c02434":"markdown","51ec36d7":"markdown","38ea133c":"markdown","9bc0ca3c":"markdown","2f0b9530":"markdown","794b595f":"markdown","920d42c9":"markdown","aeea8afb":"markdown","e17c4652":"markdown","6f60a1a0":"markdown","913d6f8b":"markdown","617acfa5":"markdown","639592c7":"markdown","a6d54963":"markdown","7613ff53":"markdown","fd1b6126":"markdown","a53707f0":"markdown","d2d1a465":"markdown","81aa6bb7":"markdown","40c78aab":"markdown","716e15f0":"markdown","166ec9f2":"markdown"},"source":{"7430d2cf":"import warnings                        # To ignore any warnings\nwarnings.filterwarnings(\"ignore\")\n%matplotlib inline\n%pylab inline\nimport os\nimport pandas as pd\nimport librosa\nimport librosa.display\nimport glob \nimport matplotlib.pyplot as plt\n%config InlineBackend.figure_format = 'retina'\nimport tensorflow as tf; print('tensorflow version: ', tf.__version__)\nimport keras; print('keras version: ',keras.__version__)","43648fcb":"# parent folder of sound files\nINPUT_DIR=\"..\/input\"\n# 16 KHz\nSAMPLE_RATE = 16000\n# seconds\nMAX_SOUND_CLIP_DURATION=12   ","3c72b977":"set_a = pd.read_csv(INPUT_DIR+\"\/set_a.csv\")\nset_a_timing = pd.read_csv(INPUT_DIR+\"\/set_a_timing.csv\")\nset_b = pd.read_csv(INPUT_DIR+\"\/set_b.csv\")","b6c17964":"set_a.head()","98915c99":"set_a_timing.head()","765aaf5b":"set_b.head()","4813fdba":"#merge both set-a and set-b\nframes = [set_a, set_b]\ntrain_ab=pd.concat(frames)\ntrain_ab.describe()","d90a2dc3":"print(set_a.label.unique())","1c87739d":"print(set_b.label.unique())","a8f91a43":"print(\"Number of training examples = \", train_ab.shape[0]) \nprint(\"Number of classes           = \", len(train_ab.label.unique()))\nprint(train_ab.label.unique())","c4d71088":"# visualize data distribution by category\ncategory_group = train_ab.groupby(['label','dataset']).count()\nplot = category_group.unstack().reindex(category_group.unstack().sum(axis=1).sort_values().index)\\\n          .plot(kind='bar', stacked=True, title=\"Number of Audio Samples per Category\", figsize=(16,5))\nplot.set_xlabel(\"Category\")\nplot.set_ylabel(\"Samples Count\");\n\nprint('Min samples per category = ', min(train_ab.label.value_counts()))\nprint('Max samples per category = ', max(train_ab.label.value_counts()))","b8ffc870":"print('Minimum samples per category = ', min(train_ab.label.value_counts()))\nprint('Maximum samples per category = ', max(train_ab.label.value_counts()))","8dfc764b":"normal_file=INPUT_DIR+\"\/set_a\/normal__201106111136.wav\"","975010ae":"# heart it\nimport IPython.display as ipd\nipd.Audio(normal_file) ","69252bd0":"# Load use wave \nimport wave\nwav = wave.open(normal_file)\nprint(\"Sampling (frame) rate = \", wav.getframerate())\nprint(\"Total samples (frames) = \", wav.getnframes())\nprint(\"Duration = \", wav.getnframes()\/wav.getframerate())","a41d57a3":"# Load use scipy\nfrom scipy.io import wavfile\nrate, data = wavfile.read(normal_file)\nprint(\"Sampling (frame) rate = \", rate)\nprint(\"Total samples (frames) = \", data.shape)\nprint(data)","7bf63b70":"# plot wave by audio frames\nplt.figure(figsize=(16, 3))\nplt.plot(data, '-', );","ee6c9e8d":"# Load using Librosa\ny, sr = librosa.load(normal_file, duration=5)   #default sampling rate is 22 HZ\ndur=librosa.get_duration(y)\nprint (\"duration:\", dur)\nprint(y.shape, sr)","47482ada":"# librosa plot\nplt.figure(figsize=(16, 3))\nlibrosa.display.waveplot(y, sr=sr)","6d29388e":"# murmur case\nmurmur_file=INPUT_DIR+\"\/set_a\/murmur__201108222231.wav\"\ny2, sr2 = librosa.load(murmur_file,duration=5)\ndur=librosa.get_duration(y)\nprint (\"duration:\", dur)\nprint(y2.shape,sr2)","d33492a7":"# heart it\nimport IPython.display as ipd\nipd.Audio(murmur_file) ","9d256319":"# show it\nplt.figure(figsize=(16, 3))\nlibrosa.display.waveplot(y2, sr=sr2)","c0678082":"# Extrasystole case\nextrastole_file=INPUT_DIR+\"\/set_b\/extrastole__127_1306764300147_C2.wav\"\ny3, sr3 = librosa.load(extrastole_file, duration=5)\ndur=librosa.get_duration(y)\nprint (\"duration:\", dur)\nprint(y3.shape,sr3)","d5713591":"# heart it\nimport IPython.display as ipd\nipd.Audio(extrastole_file) ","0221afe3":"# show it\nplt.figure(figsize=(16, 3))\nlibrosa.display.waveplot(y3, sr=sr3)","ce712a4a":"# sample file\nartifact_file=INPUT_DIR+\"\/set_a\/artifact__201012172012.wav\"\ny4, sr4 = librosa.load(artifact_file, duration=5)\ndur=librosa.get_duration(y)\nprint (\"duration:\", dur)\nprint(y4.shape,sr4)","be955f9c":"# heart it\nimport IPython.display as ipd\nipd.Audio(artifact_file) ","4f32b848":"# show it\nplt.figure(figsize=(16, 3))\nlibrosa.display.waveplot(y4, sr=sr4)","29efc0ff":"# sample file\nextrahls_file=INPUT_DIR+\"\/set_a\/extrahls__201101070953.wav\"\ny5, sr5 = librosa.load(extrahls_file, duration=5)\ndur=librosa.get_duration(y)\nprint (\"duration:\", dur)\nprint(y5.shape,sr5)","01b916db":"# heart it\nimport IPython.display as ipd\nipd.Audio(extrahls_file) ","171187c5":"# show it\nplt.figure(figsize=(16, 3))\nlibrosa.display.waveplot(y5, sr=sr5)","618b12ec":"# Here's a sample generate mfccs from a wave file\nnormal_file=INPUT_DIR+\"\/set_a\/normal__201106111136.wav\"\n#y, sr = librosa.load(sample_file, offset=7, duration=7)\ny, sr = librosa.load(normal_file)\nmfccs = librosa.feature.mfcc(y=y, sr=sr)\nprint (mfccs)","00cb9647":"# Use a pre-computed log-power Mel spectrogram\nS = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=128,fmax=8000)\nlog_S=librosa.feature.mfcc(S=librosa.power_to_db(S))\nprint (log_S)","1864e1df":"# Get more components\nmfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=40)\n#print (mfccs)","17ac5685":"# Visualize the MFCC series\n# Mel-frequency cepstral coefficients (MFCCs)\nplt.figure(figsize=(12, 3))\nlibrosa.display.specshow(mfccs, x_axis='time')\nplt.colorbar()\nplt.title('Mel-frequency cepstral coefficients (MFCCs)')\nplt.tight_layout()","4295d76b":"# Get onset times from a signal\nonset_frames = librosa.onset.onset_detect(y=y, sr=sr)\nlibrosa.frames_to_time(onset_frames, sr=sr)","1dbe4e7f":"# use a pre-computed onset envelope\no_env = librosa.onset.onset_strength(y, sr=sr)\ntimes = librosa.frames_to_time(np.arange(len(o_env)), sr=sr)\nonset_frames = librosa.onset.onset_detect(onset_envelope=o_env, sr=sr)","71f680a8":"# visualize it\nD = np.abs(librosa.stft(y))\nplt.figure(figsize=(16, 6))\nax1 = plt.subplot(2, 1, 1)\nlibrosa.display.specshow(librosa.amplitude_to_db(D, ref=np.max),x_axis='time', y_axis='log')\nplt.title('Power spectrogram')\nplt.subplot(2, 1, 2, sharex=ax1)\n\nplt.plot(times, o_env, label='Onset strength')\nplt.vlines(times[onset_frames], 0, o_env.max(), color='r', alpha=0.9,linestyle='--', label='Onsets')\nplt.axis('tight')\nplt.legend(frameon=True, framealpha=0.75)\n","e39e9248":"oenv = librosa.onset.onset_strength(y=y, sr=sr)\n# Detect events without backtracking\nonset_raw = librosa.onset.onset_detect(onset_envelope=oenv, backtrack=False)\n# Backtrack the events using the onset envelope\nonset_bt = librosa.onset.onset_backtrack(onset_raw, oenv)\n# Backtrack the events using the RMS values\nrms = librosa.feature.rms(S=np.abs(librosa.stft(y=y)))\nonset_bt_rms = librosa.onset.onset_backtrack(onset_raw, rms[0])","1acadf38":"# Plot the results\nplt.figure(figsize=(16, 6))\nplt.subplot(2,1,1)\nplt.plot(oenv, label='Onset strength')\nplt.vlines(onset_raw, 0, oenv.max(), label='Raw onsets')\nplt.vlines(onset_bt, 0, oenv.max(), label='Backtracked', color='r')\nplt.legend(frameon=True, framealpha=0.75)\nplt.subplot(2,1,2)\nplt.plot(rms[0], label='RMS')\nplt.vlines(onset_bt_rms, 0, rms.max(), label='Backtracked (RMS)', color='r')\nplt.legend(frameon=True, framealpha=0.75)\n","2cd12dac":"D = np.abs(librosa.stft(y))\ntimes = librosa.frames_to_time(np.arange(D.shape[1]))\n\nplt.figure(figsize=(16, 6))\n#ax1 = plt.subplot(2, 1, 1)\n#librosa.display.specshow(librosa.amplitude_to_db(D, ref=np.max),y_axis='log', x_axis='time')\n#plt.title('Power spectrogram')\n\n# Construct a standard onset function\nonset_env = librosa.onset.onset_strength(y=y, sr=sr)\nplt.subplot(2, 1, 1, sharex=ax1)\nplt.plot(times, 2 + onset_env \/ onset_env.max(), alpha=0.8,label='Mean (mel)')\n\n# median\nonset_env = librosa.onset.onset_strength(y=y, sr=sr,aggregate=np.median,fmax=8000, n_mels=256)\nplt.plot(times, 1+ (onset_env\/onset_env.max()), alpha=0.8,label='Median (custom mel)')\n\n# Constant-Q spectrogram instead of Mel\nonset_env = librosa.onset.onset_strength(y=y, sr=sr,feature=librosa.cqt)\nplt.plot(times, onset_env \/ onset_env.max(), alpha=0.8,label='Mean (CQT)')\nplt.legend(frameon=True, framealpha=0.75)\nplt.ylabel('Normalized strength')\nplt.yticks([])\nplt.axis('tight')\nplt.tight_layout()\n\nonset_subbands = librosa.onset.onset_strength_multi(y=y, sr=sr, channels=[0, 32, 64, 96, 128])\n#plt.figure(figsize=(16, 6))\nplt.subplot(2, 1, 2)\nlibrosa.display.specshow(onset_subbands, x_axis='time')\nplt.ylabel('Sub-bands')\nplt.title('Sub-band onset strength')","3a0dd34b":"print(\"Number of training examples=\", train_ab.shape[0], \"  Number of classes=\", len(train_ab.label.unique()))","ce06da98":"def audio_norm(data):\n    max_data = np.max(data)\n    min_data = np.min(data)\n    data = (data-min_data)\/(max_data-min_data+0.0001)\n    return data-0.5\n\n# get audio data without padding highest qualify audio\ndef load_file_data_without_change(folder,file_names, duration=3, sr=16000):\n    input_length=sr*duration\n    # function to load files and extract features\n    # file_names = glob.glob(os.path.join(folder, '*.wav'))\n    data = []\n    for file_name in file_names:\n        try:\n            sound_file=folder+file_name\n            print (\"load file \",sound_file)\n            # use kaiser_fast technique for faster extraction\n            X, sr = librosa.load( sound_file, res_type='kaiser_fast') \n            dur = librosa.get_duration(y=X, sr=sr)\n            # extract normalized mfcc feature from data\n            mfccs = np.mean(librosa.feature.mfcc(y=X, sr=sr, n_mfcc=40).T,axis=0) \n        except Exception as e:\n            print(\"Error encountered while parsing file: \", file)\n        feature = np.array(mfccs).reshape([-1,1])\n        data.append(feature)\n    return data\n\n\n# get audio data with a fix padding may also chop off some file\ndef load_file_data (folder,file_names, duration=12, sr=16000):\n    input_length=sr*duration\n    # function to load files and extract features\n    # file_names = glob.glob(os.path.join(folder, '*.wav'))\n    data = []\n    for file_name in file_names:\n        try:\n            sound_file=folder+file_name\n            print (\"load file \",sound_file)\n            # use kaiser_fast technique for faster extraction\n            X, sr = librosa.load( sound_file, sr=sr, duration=duration,res_type='kaiser_fast') \n            dur = librosa.get_duration(y=X, sr=sr)\n            # pad audio file same duration\n            if (round(dur) < duration):\n                print (\"fixing audio lenght :\", file_name)\n                y = librosa.util.fix_length(X, input_length)                \n            #normalized raw audio \n            # y = audio_norm(y)            \n            # extract normalized mfcc feature from data\n            mfccs = np.mean(librosa.feature.mfcc(y=X, sr=sr, n_mfcc=40).T,axis=0)             \n        except Exception as e:\n            print(\"Error encountered while parsing file: \", file)        \n        feature = np.array(mfccs).reshape([-1,1])\n        data.append(feature)\n    return data","f79de6cf":"# simple encoding of categories, limited to 3 types\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import preprocessing\n\n# Map label text to integer\nCLASSES = ['artifact','murmur','normal']\n# {'artifact': 0, 'murmur': 1, 'normal': 3}\nNB_CLASSES=len(CLASSES)\n\n# Map integer value to text labels\nlabel_to_int = {k:v for v,k in enumerate(CLASSES)}\nprint (label_to_int)\nprint (\" \")\n# map integer to label text\nint_to_label = {v:k for k,v in label_to_int.items()}\nprint(int_to_label)","13f838c6":"# load dataset-a, keep them separate for testing purpose\nimport os, fnmatch\n\nA_folder=INPUT_DIR+'\/set_a\/'\n# set-a\nA_artifact_files = fnmatch.filter(os.listdir(INPUT_DIR+'\/set_a'), 'artifact*.wav')\nA_artifact_sounds = load_file_data(folder=A_folder,file_names=A_artifact_files, duration=MAX_SOUND_CLIP_DURATION)\nA_artifact_labels = [0 for items in A_artifact_files]\n\nA_normal_files = fnmatch.filter(os.listdir(INPUT_DIR+'\/set_a'), 'normal*.wav')\nA_normal_sounds = load_file_data(folder=A_folder,file_names=A_normal_files, duration=MAX_SOUND_CLIP_DURATION)\nA_normal_labels = [2 for items in A_normal_sounds]\n\nA_extrahls_files = fnmatch.filter(os.listdir(INPUT_DIR+'\/set_a'), 'extrahls*.wav')\nA_extrahls_sounds = load_file_data(folder=A_folder,file_names=A_extrahls_files, duration=MAX_SOUND_CLIP_DURATION)\nA_extrahls_labels = [1 for items in A_extrahls_sounds]\n\nA_murmur_files = fnmatch.filter(os.listdir(INPUT_DIR+'\/set_a'), 'murmur*.wav')\nA_murmur_sounds = load_file_data(folder=A_folder,file_names=A_murmur_files, duration=MAX_SOUND_CLIP_DURATION)\nA_murmur_labels = [1 for items in A_murmur_files]\n\n# test files\nA_unlabelledtest_files = fnmatch.filter(os.listdir(INPUT_DIR+'\/set_a'), 'Aunlabelledtest*.wav')\nA_unlabelledtest_sounds = load_file_data(folder=A_folder,file_names=A_unlabelledtest_files, duration=MAX_SOUND_CLIP_DURATION)\nA_unlabelledtest_labels = [-1 for items in A_unlabelledtest_sounds]\n\nprint (\"loaded dataset-a\")","d1a71e9b":"%%time\n# load dataset-b, keep them separate for testing purpose \nB_folder=INPUT_DIR+'\/set_b\/'\n# set-b\nB_normal_files = fnmatch.filter(os.listdir(INPUT_DIR+'\/set_b'), 'normal*.wav')  # include noisy files\nB_normal_sounds = load_file_data(folder=B_folder,file_names=B_normal_files, duration=MAX_SOUND_CLIP_DURATION)\nB_normal_labels = [2 for items in B_normal_sounds]\n\nB_murmur_files = fnmatch.filter(os.listdir(INPUT_DIR+'\/set_b'), 'murmur*.wav')  # include noisy files\nB_murmur_sounds = load_file_data(folder=B_folder,file_names=B_murmur_files, duration=MAX_SOUND_CLIP_DURATION)\nB_murmur_labels = [1 for items in B_murmur_files]\n\nB_extrastole_files = fnmatch.filter(os.listdir(INPUT_DIR+'\/set_b'), 'extrastole*.wav')\nB_extrastole_sounds = load_file_data(folder=B_folder,file_names=B_extrastole_files, duration=MAX_SOUND_CLIP_DURATION)\nB_extrastole_labels = [1 for items in B_extrastole_files]\n\n#test files\nB_unlabelledtest_files = fnmatch.filter(os.listdir(INPUT_DIR+'\/set_b'), 'Bunlabelledtest*.wav')\nB_unlabelledtest_sounds = load_file_data(folder=B_folder,file_names=B_unlabelledtest_files, duration=MAX_SOUND_CLIP_DURATION)\nB_unlabelledtest_labels = [-1 for items in B_unlabelledtest_sounds]\nprint (\"loaded dataset-b\")","669e300c":"#combine set-a and set-b \nx_data = np.concatenate((A_artifact_sounds, A_normal_sounds,A_extrahls_sounds,A_murmur_sounds, \n                         B_normal_sounds,B_murmur_sounds,B_extrastole_sounds))\n\ny_data = np.concatenate((A_artifact_labels, A_normal_labels,A_extrahls_labels,A_murmur_labels,\n                         B_normal_labels,B_murmur_labels,B_extrastole_labels))\n\ntest_x = np.concatenate((A_unlabelledtest_sounds,B_unlabelledtest_sounds))\ntest_y = np.concatenate((A_unlabelledtest_labels,B_unlabelledtest_labels))\n\nprint (\"combined training data record: \",len(y_data), len(test_y))","48b726e3":"# shuffle - whether or not to shuffle the data before splitting. If shuffle=False then stratify must be None.\n# random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random.\n\nseed = 1000\n# split data into Train, Validation and Test\nx_train, x_test, y_train, y_test = train_test_split(x_data, y_data, train_size=0.9, random_state=seed, shuffle=True)\nx_train, x_val, y_train, y_val = train_test_split(x_train, y_train, train_size=0.9, random_state=seed, shuffle=True)\n\n# One-Hot encoding for classes\ny_train = np.array(keras.utils.to_categorical(y_train, len(CLASSES)))\ny_test = np.array(keras.utils.to_categorical(y_test, len(CLASSES)))\ny_val = np.array(keras.utils.to_categorical(y_val, len(CLASSES)))\ntest_y=np.array(keras.utils.to_categorical(test_y, len(CLASSES)))","f0087c71":"print (\"label shape: \", y_data.shape)\nprint (\"data size of the array: : %s\" % y_data.size)\nprint (\"length of one array element in bytes: \", y_data.itemsize)\nprint (\"total bytes consumed by the elements of the array: \", y_data.nbytes)\nprint (y_data[1])\nprint (\"\")\nprint (\"audio data shape: \", x_data.shape)\nprint (\"data size of the array: : %s\" % x_data.size)\nprint (\"length of one array element in bytes: \", x_data.itemsize)\nprint (\"total bytes consumed by the elements of the array: \", x_data.nbytes)\n#print (x_data[1])\nprint (\"\")\nprint (\"training data shape: \", x_train.shape)\nprint (\"training label shape: \", y_train.shape)\nprint (\"\")\nprint (\"validation data shape: \", x_val.shape)\nprint (\"validation label shape: \", y_val.shape)\nprint (\"\")\nprint (\"test data shape: \", x_test.shape)\nprint (\"test label shape: \", y_test.shape)","18cf5943":"import numpy as np\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Activation, Flatten, LSTM\nfrom keras.layers import Convolution2D, MaxPooling2D\nfrom keras.optimizers import Adam\nfrom keras.callbacks import EarlyStopping,ReduceLROnPlateau,ModelCheckpoint,TensorBoard,ProgbarLogger\nfrom keras.utils import np_utils\nfrom sklearn import metrics \nfrom sklearn.metrics import confusion_matrix, classification_report, accuracy_score\nfrom sklearn.preprocessing import LabelEncoder\nimport itertools","32764f64":"print('Build LSTM RNN model ...')\nmodel = Sequential()\nmodel.add(LSTM(units=64, dropout=0.05, recurrent_dropout=0.20, return_sequences=True,input_shape = (40,1)))\nmodel.add(LSTM(units=32, dropout=0.05, recurrent_dropout=0.20, return_sequences=False))\nmodel.add(Dense(len(CLASSES), activation='softmax'))\nmodel.compile(loss='categorical_crossentropy', optimizer='Adamax', metrics=['acc','mse', 'mae', 'mape', 'cosine'])\nmodel.summary()","8d6e8517":"%%time\n# saved model checkpoint file\nbest_model_file=\".\/best_model_trained.hdf5\"\n#train_model_file=file_path+\"\/checkpoints\/weights.best_{epoch:02d}-{loss:.2f}.hdf5\"\nMAX_PATIENT=12\nMAX_EPOCHS=1000\nMAX_BATCH=32\n\n# callbacks\n# removed EarlyStopping(patience=MAX_PATIENT)\ncallback=[ReduceLROnPlateau(patience=MAX_PATIENT, verbose=1),\n          ModelCheckpoint(filepath=best_model_file, monitor='loss', verbose=1, save_best_only=True)]\n\nprint (\"training started..... please wait.\")\n# training\nhistory=model.fit(x_train, y_train, \n                  batch_size=MAX_BATCH, \n                  epochs=MAX_EPOCHS,\n                  verbose=0,\n                  validation_data=(x_val, y_val),\n                  callbacks=callback) \n\nprint (\"training finised!\")","5c74eebf":"# Keras reported accuracy:\nscore = model.evaluate(x_train, y_train, verbose=0) \nprint (\"model train data score       : \",round(score[1]*100) , \"%\")\n\nscore = model.evaluate(x_test, y_test, verbose=0) \nprint (\"model test data score        : \",round(score[1]*100) , \"%\")\n\nscore = model.evaluate(x_val, y_val, verbose=0) \nprint (\"model validation data score  : \", round(score[1]*100), \"%\")\n\nscore = model.evaluate(test_x, test_y, verbose=0) \nprint (\"model unlabeled data score   : \", round(score[1]*100), \"%\")","01be880f":"%%time\n#Plot Keras History\n#Plot loss and accuracy for the training and validation set.\ndef plot_history(history):\n    loss_list = [s for s in history.history.keys() if 'loss' in s and 'val' not in s]\n    val_loss_list = [s for s in history.history.keys() if 'loss' in s and 'val' in s]\n    acc_list = [s for s in history.history.keys() if 'acc' in s and 'val' not in s]\n    val_acc_list = [s for s in history.history.keys() if 'acc' in s and 'val' in s]\n    if len(loss_list) == 0:\n        print('Loss is missing in history')\n        return \n    plt.figure(figsize=(22,10))\n    ## As loss always exists\n    epochs = range(1,len(history.history[loss_list[0]]) + 1)\n    ## Accuracy\n    plt.figure(221, figsize=(20,10))\n    ## Accuracy\n    # plt.figure(2,figsize=(14,5))\n    plt.subplot(221, title='Accuracy')\n    for l in acc_list:\n        plt.plot(epochs, history.history[l], 'b', label='Training accuracy (' + str(format(history.history[l][-1],'.5f'))+')')\n    for l in val_acc_list:    \n        plt.plot(epochs, history.history[l], 'g', label='Validation accuracy (' + str(format(history.history[l][-1],'.5f'))+')')\n    plt.title('Accuracy')\n    plt.xlabel('Epochs')\n    plt.ylabel('Accuracy')\n    plt.legend()\n    ## Loss\n    plt.subplot(222, title='Loss')\n    for l in loss_list:\n        plt.plot(epochs, history.history[l], 'b', label='Training loss (' + str(str(format(history.history[l][-1],'.5f'))+')'))\n    for l in val_loss_list:\n        plt.plot(epochs, history.history[l], 'g', label='Validation loss (' + str(str(format(history.history[l][-1],'.5f'))+')'))    \n    plt.title('Loss')\n    plt.xlabel('Epochs')\n    plt.ylabel('Loss')\n    plt.legend()\n    plt.show()\n\n# plot history\nplot_history(history)","92287442":"def plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n        title='Normalized confusion matrix'\n    else:\n        title='Confusion matrix'\n\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    plt.show()","26b007ee":"# prediction class \ny_pred = model.predict_classes(x_test, batch_size=32)\nprint (\"prediction test return :\",y_pred[1], \"-\", int_to_label[y_pred[1]])","84eb4a2d":"plt.figure(1,figsize=(20,10))\n# plot Classification Metrics: Accuracy \nplt.subplot(221, title='Prediction')\nplt.plot(y_pred)\nplt.show()","7e56f756":"print (best_model_file)","9275f9c9":"### Loading a Check-Pointed Neural Network Model\n# How to load and use weights from a checkpoint\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.callbacks import ModelCheckpoint\nimport matplotlib.pyplot as plt\nimport numpy\n# fix random seed for reproducibility\nseed = 7\nnumpy.random.seed(seed)\n# create model\nprint('Build LSTM RNN model ...')\nmodel = Sequential()\nmodel.add(LSTM(units=64, dropout=0.05, recurrent_dropout=0.35, return_sequences=True,input_shape = (40,1)))\nmodel.add(LSTM(units=32, dropout=0.05, recurrent_dropout=0.35, return_sequences=False))\nmodel.add(Dense(len(CLASSES), activation='softmax'))\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc','mse', 'mae', 'mape', 'cosine'])\nmodel.summary()\n# load weights\nmodel.load_weights(best_model_file)\n# Compile model (required to make predictions)\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\nprint(\"Created model and loaded weights from file\")","8419456e":"# make a prediction\ny_pred = model.predict_classes(x_test, batch_size=32)\n#check scores\nscores = model.evaluate(x_test, y_test, verbose=0)\nprint (\"Model evaluation accuracy: \", round(scores[1]*100),\"%\")","f9e182db":"### LSTM  ","62b21145":"### Build Model","7bc72319":"### Audio Length\nthe lengths of the audio files in the dataset varies from 1 to 30 seconds long. for training purpose we use first 5 seconds of the audio. padd missing lenght for file smaller than 5 seconds. \n","47a4c6ef":"# Feature Extraction","d75ee7a8":"make a prediction\n    x: The input data, as a Numpy array (or list of Numpy arrays if the model has multiple inputs).\n    batch_size: Integer. If unspecified, it will default to 32.\n    steps = Total number of steps (batches of samples) before declaring the prediction round finished. \n    callbacks: List of keras.callbacks.Callback instances. \nreturns\n  Numpy array(s) of predictions.","808140d7":"### Prediction Test","94d58eae":"#### onset strength\nCompute a spectral flux onset strength envelope.\nOnset strength at time t is determined by:\nmean_f max(0, S[f, t] - ref_S[f, t - lag])\nwhere ref_S is S after local max filtering along the frequency axis [1].\nBy default, if a time series y is provided, S will be the log-power Mel spectrogram.","02632b03":"### Model Evaluation","69c02434":"#### 3. Extrasystole \nExtrasystole sounds may appear occasionally and can be identified because there is a heart sound that is out of rhythm involving extra or skipped heartbeats, e.g. a \u201club-lub dub\u201d or a \u201club dub-dub\u201d. (This is not the same as an extra heart sound as the event is not regularly occuring.) An extrasystole may not be a sign of disease. It can happen normally in an adult and can be very common in children. However, in some situations extrasystoles can be caused by heart diseases. If these diseases are detected earlier, then treatment is likely to be more effective. (source: Rita Getz)","51ec36d7":"Loading of the audio data file will be based on content from directory since each filename is associate with the category type. hence, we can use csv file for cross reference check.  Based on directory content approach will be more flexible.","38ea133c":"### Import Datasets","9bc0ca3c":"### Identify Unique Labels","2f0b9530":"## Import Packages","794b595f":"### Load Training Model","920d42c9":"#### 1. Normal case\nIn the Normal category there are normal, healthy heart sounds. These may contain noise in the final second of the recording as the device is removed from the body. They may contain a variety of background noises (from traffic to radios). They may also contain occasional random noise corresponding to breathing, or brushing the microphone against clothing or skin. A normal heart sound has a clear \u201club dub, lub dub\u201d pattern, with the time from \u201club\u201d to \u201cdub\u201d shorter than the time from \u201cdub\u201d to the next \u201club\u201d (when the heart rate is less than 140 beats per minute)(source: Rita Getz)","aeea8afb":"### Merge Datasets","e17c4652":"## Heart sounds analysis and classification with LSTM","6f60a1a0":"### Loading Data","913d6f8b":"### Classes","617acfa5":"### MFCC\n\nMel Frequency Cepstral Coefficient (MFCC) is by far the most successful feature used in the field of Speech Processing. Speech is a non-stationary signal. As such, normal signal processing techniques cannot be directly applied to it. \n\nMel-frequency cepstral coefficients (MFCCs) are coefficients that collectively make up an MFC. They are derived from a type of cepstral representation of the audio clip (a nonlinear \"spectrum-of-a-spectrum\"). The difference between the cepstrum and the mel-frequency cepstrum is that in the MFC, the frequency bands are equally spaced on the mel scale, which approximates the human auditory system's response more closely than the linearly-spaced frequency bands used in the normal cepstrum. This frequency warping can allow for better representation of sound, for example, in audio compression.\n\nMFCCs are commonly derived as follows:\n-Take the Fourier transform of (a windowed excerpt of) a signal.\n-Map the powers of the spectrum obtained above onto the mel scale, using triangular overlapping windows.\n-Take the logs of the powers at each of the mel frequencies.\n-Take the discrete cosine transform of the list of mel log powers, as if it were a signal.\nThe MFCCs are the amplitudes of the resulting spectrum.\n\nIn general, a 39-dimensional feature vector is used which is composed of first 13 MFCCs and their corresponding 13 delta and 13 delta-delta.","639592c7":"### Train Model","a6d54963":"#### onset_backtrack\nBacktrack detected onset events to the nearest preceding local minimum of an energy function.\nThis function can be used to roll back the timing of detected onsets from a detected peak amplitude to the preceding minimum. This is most useful when using onsets to determine slice points for segmentation","7613ff53":"#### 2. Murmur \nHeart murmurs sound as though there is a \u201cwhooshing, roaring, rumbling, or turbulent fluid\u201d noise in one of two temporal locations: (1) between \u201club\u201d and \u201cdub\u201d, or (2) between \u201cdub\u201d and \u201club\u201d. They can be a symptom of many heart disorders, some serious. There will still be a \u201club\u201d and a \u201cdub\u201d. One of the things that confuses non-medically trained people is that murmurs happen between lub and dub or between dub and lub; not on lub and not on dub.(source: Rita Getz)","fd1b6126":"### Test Training Model","a53707f0":"Note: nan label indicate unclassified and unlabel test files","d2d1a465":"### Onset","81aa6bb7":"## Data Preprocessing","40c78aab":"#### onset detector\n\nBasic onset detector. Locate note onset events by picking peaks in an onset strength envelope.\nThe peak_pick parameters were chosen by large-scale hyper-parameter optimization over the dataset provided ","716e15f0":"#### 4. Artifact \nIn the Artifact category there are a wide range of different sounds, including feedback squeals and echoes, speech, music and noise. There are usually no discernable heart sounds, and thus little or no temporal periodicity at frequencies below 195 Hz. This category is the most different from the others. It is important to be able to distinguish this category from the other three categories, so that someone gathering the data can be instructed to try again.(source: Rita Getz)","166ec9f2":"#### 5. Extra Heart Sound \nIn the Artifact category there are a wide range of different sounds, including feedback squeals and echoes, speech, music and noise. There are usually no discernable heart sounds, and thus little or no temporal periodicity at frequencies below 195 Hz. This category is the most different from the others. It is important to be able to distinguish this category from the other three categories, so that someone gathering the data can be instructed to try again.(source: Rita Getz)"}}