{"cell_type":{"dbf617b3":"code","4205f6aa":"code","268f61cc":"code","812f7ff8":"code","396d5f25":"code","052a88f5":"code","c4e18a12":"code","a45dff97":"code","47c8e913":"markdown","84a097b2":"markdown","7395b452":"markdown","7f28acd5":"markdown"},"source":{"dbf617b3":"import numpy as np\nimport pandas as pd\nfrom pathlib import Path\nimport matplotlib.pyplot as plt\n\nimport xgboost as xgb\n\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.dummy import DummyRegressor\nfrom sklearn.metrics import get_scorer, SCORERS\n\nSEED = 123","4205f6aa":"DBPATH = Path('\/kaggle\/input\/tabular-playground-series-aug-2021\/')\ntrain = pd.read_csv(DBPATH \/ 'train.csv')\nsub_test = pd.read_csv(DBPATH \/ 'test.csv')\nFEATURES = [ f\"f{i}\" for i in range(100) ]\nTARGETS = ['loss']\nX, y = train[FEATURES], train[TARGETS]","268f61cc":"# let's train XGBoost using 10% of the data (for speed purpose), parameters I got with optuna hyperoptimization, and 3 Kfold\n# (see kaggle_projects\/tps-aug-2021\/1_xgb_optuna)\n\ntrain_frac_samples = 0.1 # Fraction of samples used for training. TO IMPROVE >> Increase train_frac_samples up to 0.9\nX_train = X.sample(frac=train_frac_samples, random_state=SEED)\ny_train = y.iloc[X_train.index,:]\n# Use all the rest for testing\nX_test = X.drop(X_train.index)\ny_test = y.drop(X_train.index)\n\ntest_preds = np.zeros(X_test.shape[0]) # Test for my own checking \nsub_preds = np.zeros(sub_test.shape[0]) # Submission\n\nkf = KFold(n_splits=5,random_state=SEED,shuffle=True) # TO IMPROVE >> n_splits up to 10\nrmse=[]\nn=0\n\nfor trn_idx, val_idx in kf.split(X_train,y_train):\n    X_trn, X_val = X.iloc[trn_idx], X.iloc[val_idx]\n    y_trn, y_val = y.iloc[trn_idx], y.iloc[val_idx]\n    model = xgb.XGBRegressor(**{\n        'tree_method': 'auto',\n        'n_estimators': 300,\n        'lambda': 0.019333692109917443,\n        'alpha': 8.194847496009798,\n        'colsample_bytree': 0.5471185761751851,\n        'subsample': 0.584376759562932,\n        'learning_rate': 0.031035459267598212,\n        'max_depth': 17,\n        'min_child_weight': 217, \n        'seed': SEED\n    })\n    model.fit(X_trn,y_trn, \n              early_stopping_rounds=20, eval_set=[(X_val,y_val)],\n              verbose=0)\n    test_preds+=model.predict(X_test)\/kf.n_splits # Running average of predictions\n    sub_preds+=model.predict(sub_test[FEATURES])\/kf.n_splits\n\n    rmse.append(mean_squared_error(y_val, model.predict(X_val), squared=False))\n    print(f'Kfold #{n+1}, RMSE (validation): {rmse[n]}')\n    n+=1","812f7ff8":"rmse_test = mean_squared_error(y_test, model.predict(X_test), squared=False)\nprint(f\"RMSE (test): {rmse_test}\")","396d5f25":"# Printing the submission CSV\npd.DataFrame(zip(sub_test['id'], sub_preds), columns=['id', 'loss']).to_csv('subssion.csv', index=False)","052a88f5":"# Let's visualize the predictions on a parity plot\nplt.figure(figsize=[5,5], dpi=100)\nplt.scatter(model.predict(X_test), y_test, c='b', s=0.1, alpha=0.01)\nplt.plot([0,40],[0,40], c='k', ls='--')\nplt.xlabel('Predictions')\nplt.ylabel('Targets')\nplt.grid()\nplt.show()","c4e18a12":"fig, axs = plt.subplots(ncols=2, sharey=True, sharex=True, figsize=[10,4])\naxs[0].hist(y_test, bins=100)\naxs[0].set_title('Targets')\naxs[1].hist(model.predict(X_test), bins=100)\naxs[1].set_title('Predictions')\nplt.show()","a45dff97":"dummy_dict = {'mean': {}, 'median':{}}\nfor k in dummy_dict:\n    dummy_dict[k]['model'] = DummyRegressor(strategy=k)\n    dummy_dict[k]['model'].fit(X_train, y_train)\n    dummy_dict[k]['value'] = dummy_dict[k]['model'].predict([0])[0] # get the mean\/median value\n    for scorer_str in SCORERS.keys(): # get all scorers\n        scorer = get_scorer(scorer_str)\n        try:\n            dummy_dict[k][scorer_str] = scorer(estimator=dummy_dict[k]['model'], X=X_test, y_true=y_test)\n        except:\n            dummy_dict[k][scorer_str] = 'SKIP' # The scorer does not make sense for this problem\npd.DataFrame(dummy_dict) ","47c8e913":"One can notice that almost all the predictions are between 5 and 10, while the predictions span from 0 to 40.\nIt does not matter much to me that we are using regression and therefore continuous value: one can round them later to get discrete integers.\n\nSo how does a dummy model which simply returns the mean or the median value of the training set performs?","84a097b2":"We can note how, just using the mean value of the training set target (6.81184), one can achieve a RMSE score of 7.944.\n\nConsidering that the best score of today is 7.846 and assuming that the score on this test partition is transferrable the hidden test of the competition, the whole change is to achieve just 0.1 RMSE improvement!","7395b452":"Considering that the best RMSE score of today (24 Aug 2021) is 7.846, my resuld does not sound bad!\n\n...but how good are these predictions for a real application?","7f28acd5":"# XGBoost compared with mean baseline\nIn this notebook I'm showing how poor are predictions for this dataset, even after achieving a score among the best for this competition."}}