{"cell_type":{"4979b69d":"code","2fb3b31b":"code","aafbb713":"markdown"},"source":{"4979b69d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","2fb3b31b":"import requests\nfrom bs4 import BeautifulSoup\n\ndef fixForm(S):\n    s2 = S.replace('\\n','')\n    return s2\n\ndef readArt(String): \n    response = requests.get(String)\n    response.encoding = 'utf-8'\n    data = response.text\n    soup = BeautifulSoup(data)\n    soup.find('article').text\n    text = ' '.join(map(lambda p: p.text, soup.find_all('article')))\n    text.encode('ascii', 'ignore')\n    from nltk.tokenize import sent_tokenize,word_tokenize\n    from nltk.corpus import stopwords\n    from string import punctuation\n    sents = sent_tokenize(text)\n    word_sent = word_tokenize(text.lower())\n    _stopwords = set(stopwords.words('english') + list(punctuation))\n    word_sent=[word for word in word_sent if word not in _stopwords]\n    from nltk.probability import FreqDist\n    freq = FreqDist(word_sent)\n    from heapq import nlargest\n    nlargest(10, freq, key=freq.get)\n    # We want to create a signifcant score ordered by highest frequency\n    from collections import defaultdict\n    ranking = defaultdict(int)\n    for i,sent in enumerate(sents):\n        for w in word_tokenize(sent.lower()):\n            if w in freq:\n                ranking[i] += freq[w]\n    # Top 4 Sentences\n    sents_idx = nlargest(4, ranking, key=ranking.get)\n    newStr = str([sents[j] for j in sorted(sents_idx)])\n    s2 = fixForm(newStr)\n    print(newStr.rstrip('\\r\\n').replace('\\n', ' '))\n\n    \n    \nreadArt(\"https:\/\/arstechnica.com\/cars\/2018\/10\/honda-will-use-gms-self-driving-technology-invest-2-75-billion\/\")\n\n#I have no clue why it isnt ripping out all of these new lines i have tried multiple methods that i thought should have worked\n    \n    \n\n    \n                                               \n","aafbb713":"# TL;DR - Automated Gist\n## Find the most important words\n### Word Importance = Word Frequency\n## Compute a significance score for sentences based on words they contain\n### Significant score = Sum(Word Importance)\n## Pick the top most significant sentences\n\n* Retrieve Text\n* Preprocess Text\n* Extract Sentences\n\n#### Source: PluralSight - Natural Langauge Processing"}}