{"cell_type":{"e5c98340":"code","bcfc3822":"code","cd147291":"code","1d5caf6e":"code","a31d1d4b":"code","9b7e65b5":"code","254b4ca0":"code","3ce10bb8":"code","64b37308":"code","a0ccc202":"code","955779f4":"code","cd25161b":"code","12f7c936":"code","3dd3e891":"code","6f660585":"code","2fd450f3":"code","e058f33c":"code","bffa2960":"code","6ad27088":"code","10ea315e":"code","abad8c62":"code","682f924a":"code","72a223fd":"code","b474e3c5":"code","996d745f":"code","a80089d1":"code","0e845deb":"code","8c6a1d7d":"code","9fb17483":"code","cb985916":"code","1db8a609":"code","6619678b":"code","e1752fbd":"code","7c63c932":"code","634ffd4b":"code","9136bbd7":"code","b2ff8807":"code","e2c320e5":"code","6585444d":"code","b296095a":"code","26ddeced":"code","6c1d4ed7":"code","1a466cad":"code","3b675dbd":"code","e1b61323":"code","637fa659":"code","7f178886":"code","68e0898d":"code","1670eec6":"code","efe81c77":"code","2d032c9a":"code","7ef7d28b":"code","b758daab":"code","4986bbdf":"code","7ed329df":"code","0d7bf91f":"code","89899dcc":"code","22ef098e":"code","2da2b702":"code","c70308f2":"code","d842f455":"code","4b0a2e2e":"code","eee28371":"code","a1a568b8":"code","def90519":"code","a1faba52":"code","42d4538d":"code","d013f50d":"code","9ea50251":"code","36a83dc6":"code","c43578ff":"code","09f46736":"code","f23f5ee1":"code","566f6612":"code","2b81117b":"markdown","cecf2c79":"markdown","41057620":"markdown","697038ab":"markdown","674ba72e":"markdown","aee59115":"markdown","222b835a":"markdown","08ae0f27":"markdown","30486db7":"markdown","502f881f":"markdown","f2d1a924":"markdown","1413281f":"markdown","e5d5335a":"markdown","4da279d0":"markdown","fbb302a6":"markdown","50a62c94":"markdown","f54ed1f5":"markdown","ad221c77":"markdown","41e09a0e":"markdown","5812aabc":"markdown","29b16047":"markdown","88c8db03":"markdown","a4eb8284":"markdown","0d3eb8f5":"markdown","3b98f7fd":"markdown","d409a82d":"markdown","8750039e":"markdown","660c50e6":"markdown","d1d1cd41":"markdown","8e6d96f1":"markdown","fa757415":"markdown","37bf0929":"markdown","4033dc04":"markdown","9e2793ee":"markdown","d84f8542":"markdown","19b12a91":"markdown","7617a906":"markdown","6ec88304":"markdown","e4eb357c":"markdown","9cd40cdb":"markdown","8992b829":"markdown","293e3cdc":"markdown"},"source":{"e5c98340":"import pandas as pd\nimport numpy as np\nimport random as rnd\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nsns.set_style(\"white\")\nsurvived = [\"#ffb3ba\", \"#bae1ff\"]\nsns.set_palette(survived)\n#sns.palplot(sns.color_palette())\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","bcfc3822":"train_df = pd.read_csv(\"..\/input\/titanic\/train.csv\")\ntest_df = pd.read_csv(\"..\/input\/titanic\/test.csv\")\n\ntrain_df.head(10)","cd147291":"train_df.info()\nprint('_'*40)\n\ntest_df.info()","1d5caf6e":"sns.heatmap(train_df.isnull(),yticklabels=False,cbar=False, cmap=\"Pastel1\")","a31d1d4b":"survived = train_df[train_df['Survived']==1]\nnot_survived = train_df[train_df['Survived']==0]\n\nprint(\"Survived: %i (%.1f percent)\\nNot Survived: %i (%.1f percent)\\nTotal: %i\"\\\n      %(len(survived), 1.*len(survived)\/len(train_df)*100.0,\\\n        len(not_survived), 1.*len(not_survived)\/len(train_df)*100.0, len(train_df)))","9b7e65b5":"g = sns.FacetGrid(train_df, col='Survived',hue='Survived')\ng.map(plt.hist, \"Age\",bins=20);\nsns.despine(left = True,bottom = True)\n\na = sns.FacetGrid(train_df, hue = 'Survived',aspect=2 )\na.map(sns.kdeplot, 'Age', shade= True)\na.set(xlim=(0 , train_df['Age'].max()))\nsns.despine(left = True,bottom = True)","254b4ca0":"ax = sns.countplot(x = 'Sex', hue = 'Survived', data = train_df)\nax.set(xlabel = 'Sex', ylabel='Total')\nsns.despine(left = True,bottom = True)","3ce10bb8":"sns.barplot(x='Pclass', y='Survived', palette='Pastel1', data=train_df)\nsns.despine(left = True,bottom = True)\n","64b37308":"sns.barplot(x='Embarked', y='Survived', palette='Pastel1', data=train_df)\nsns.despine(left = True,bottom = True)","a0ccc202":"a = sns.FacetGrid(train_df, hue = 'Survived',aspect=2 )\na.map(sns.kdeplot, 'Fare', shade= True)\na.set(xlim=(0 , train_df['Fare'].max()))\nsns.despine(left = True,bottom = True)","955779f4":"a = sns.FacetGrid(train_df, hue = 'Survived',aspect=2 )\na.map(sns.kdeplot, 'Fare', shade= True)\na.set(xlim=(0 , 100))\nsns.despine(left = True,bottom = True)","cd25161b":"f,ax=plt.subplots(1,3,figsize=(20,8))\nsns.distplot(train_df[train_df['Pclass']==1].Fare,ax=ax[0], color=\"#bae1ff\")\nax[0].set_title('Fares in Pclass 1')\nsns.distplot(train_df[train_df['Pclass']==2].Fare,ax=ax[1],color=\"#bae1ff\")\nax[1].set_title('Fares in Pclass 2')\nsns.distplot(train_df[train_df['Pclass']==3].Fare,ax=ax[2],color=\"#bae1ff\")\nax[2].set_title('Fares in Pclass 3')\nsns.despine(left = True,bottom = True)\nplt.show()","12f7c936":"train_test_df = [train_df, test_df]","3dd3e891":"sex_mapping = {'male': 0, 'female': 1}\nfor df in train_test_df:\n    df['Sex'] = df['Sex'].map(sex_mapping)","6f660585":"ax = sns.countplot(x = 'Embarked', hue = 'Pclass', palette='Pastel1', data = train_df)\nax.set(xlabel = 'Embarked', ylabel='Total')\nsns.despine(left = True,bottom = True)","2fd450f3":"train_test_df = [train_df, test_df]\n\nfor df in train_test_df:\n    df['Embarked'] = df['Embarked'].fillna('S')","e058f33c":"emb_mapping = {'S': 0, 'C': 1, 'Q': 2}\nfor df in train_test_df:\n    df['Embarked'] = df['Embarked'].map(emb_mapping)","bffa2960":"train_df['FamilySize'] = train_df['SibSp'] + train_df['Parch'] + 1\ntest_df['FamilySize'] = test_df['SibSp'] + test_df['Parch'] + 1","6ad27088":"train_df[['FamilySize', 'Survived']].groupby(['FamilySize'], as_index=False).mean().sort_values(by='Survived', ascending=False)\naxes = sns.pointplot('FamilySize','Survived', data=train_df)\nsns.despine(left = True,bottom = True)","10ea315e":"facet = sns.FacetGrid(train_df, hue='Survived', aspect=4)\nfacet.map(sns.kdeplot, 'FamilySize', shade=True)\nfacet.set(xlim=(0, train_df['FamilySize'].max()))\nplt.xlim(0)\nsns.despine(left = True,bottom = True)","abad8c62":"train_df = train_df.drop(['Parch','SibSp'], axis=1)\ntest_df = test_df.drop(['Parch','SibSp'], axis=1)","682f924a":"train_test_df = [train_df, test_df]\n\nfor df in train_test_df:\n    df['Title'] = df['Name'].str.extract(' ([A-Za-z]+)\\.', expand=False)","72a223fd":"train_df['Title'].value_counts()","b474e3c5":"test_df['Title'].value_counts()","996d745f":"TitleDict = {\"Capt\": \"Officer\",\"Col\": \"Officer\",\"Major\": \"Officer\",\"Jonkheer\": \"Nobility\", \\\n             \"Don\": \"Nobility\", \"Sir\" : \"Nobility\",\"Dr\": \"Nobility\",\"Rev\": \"Nobility\", \\\n             \"Countess\":\"Nobility\", \"Mme\": \"Mrs\", \"Mlle\": \"Miss\", \"Ms\": \"Mrs\",\"Mr\" : \"Mr\", \\\n             \"Mrs\" : \"Mrs\",\"Miss\" : \"Miss\",\"Master\" : \"Master\",\"Lady\" : \"Nobility\", 'Dona':'Nobility'}\n\nfor df in train_test_df:\n    df['Title'] = df['Title'].map(TitleDict)","a80089d1":"sns.barplot(x='Title', y='Survived', palette='Pastel1', data=train_df)\nsns.despine(left = True, bottom = True)","0e845deb":"title_mapping = {\"Master\": 0, \"Miss\": 1, \"Mr\": 2, \"Mrs\": 3, \"Nobility\": 4, \"Officer\": 5}\n\nfor df in train_test_df:\n    df['Title'] = df['Title'].map(title_mapping)","8c6a1d7d":"train_df.drop('Name', axis = 1, inplace = True)\ntest_df.drop('Name', axis = 1, inplace = True)\n\ntrain_df.head()","9fb17483":"train_df_corr = train_df.corr().abs().unstack().sort_values(kind=\"quicksort\", ascending=False).reset_index()\ntrain_df_corr.rename(columns={\"level_0\": \"Feature 1\", \"level_1\": \"Feature 2\", 0: 'Correlation Coefficient'}, inplace=True)\ntrain_df_corr[train_df_corr['Feature 1'] == 'Age']","cb985916":"train_df.groupby(['Title', 'Pclass','FamilySize'])['Age'].agg(['mean', 'count'])","1db8a609":"for df in train_test_df:\n    df['Age'].fillna(df.groupby(['Title','Pclass','FamilySize'])['Age'].transform('mean'), inplace=True)","6619678b":"for df in train_test_df:\n    df['Age'].fillna(df.groupby(['Title','Pclass'])['Age'].transform('mean'), inplace=True)","e1752fbd":"sns.set_style(\"white\")\nsurvived = [\"#ffb3ba\", \"#bae1ff\"]\nsns.set_palette(survived)\n#sns.palplot(sns.color_palette())\n\ng = sns.FacetGrid(train_df, col='Survived',hue='Survived')\ng.map(plt.hist, \"Age\",bins=20);\nsns.despine(left = True,bottom = True)\n\na = sns.FacetGrid(train_df, hue = 'Survived',aspect=2 )\na.map(sns.kdeplot, 'Age', shade= True)\na.set(xlim=(0 , train_df['Age'].max()))\nsns.despine(left = True,bottom = True)","7c63c932":"facet = sns.FacetGrid(train_df, hue = 'Survived',aspect=2 )\nfacet.map(sns.kdeplot, 'Age', shade= True)\nfacet.set(xlim=(0 , train_df['Age'].max()))\nsns.despine(left = True,bottom = True)\nfacet.set(xticks=np.arange(0,21,1))\nplt.xlim(0, 16)","634ffd4b":"facet = sns.FacetGrid(train_df, hue = 'Survived',aspect=2 )\nfacet.map(sns.kdeplot, 'Age', shade= True)\nfacet.set(xlim=(0 , train_df['Age'].max()))\nsns.despine(left = True,bottom = True)\nfacet.set(xticks=np.arange(17,35,1))\nplt.xlim(17, 34)","9136bbd7":"facet = sns.FacetGrid(train_df, hue = 'Survived',aspect=2 )\nfacet.map(sns.kdeplot, 'Age', shade= True)\nfacet.set(xlim=(0 , train_df['Age'].max()))\nsns.despine(left = True,bottom = True)\nfacet.set(xticks=np.arange(35,61,1))\nplt.xlim(35, 60)","b2ff8807":"facet = sns.FacetGrid(train_df, hue = 'Survived',aspect=2 )\nfacet.map(sns.kdeplot, 'Age', shade= True)\nfacet.set(xlim=(0 , train_df['Age'].max()))\nsns.despine(left = True,bottom = True)\nfacet.set(xticks=np.arange(35,61,1))\nplt.xlim(35, 60)","e2c320e5":"for df in train_test_df:\n    df.loc[df['Age'] <= 16, 'Age'] = 0\n    df.loc[(df['Age'] > 16) & (df['Age'] <= 34), 'Age'] = 1\n    df.loc[(df['Age'] > 34) & (df['Age'] <= 43), 'Age'] = 2\n    df.loc[(df['Age'] > 43) & (df['Age'] <= 60), 'Age'] = 3\n    df.loc[df['Age'] > 60, 'Age'] = 4","6585444d":"test_df['Fare'].fillna(test_df.groupby(['Pclass'])['Fare'].transform('median'), inplace=True)","b296095a":"facet = sns.FacetGrid(train_df, hue = 'Survived',aspect=2 )\nfacet.map(sns.kdeplot, 'Fare', shade= True)\nfacet.set(xlim=(0 , train_df['Fare'].max()))\nsns.despine(left = True,bottom = True)\nfacet.set(xticks=np.arange(0,21,1))\nplt.xlim(0,17)","26ddeced":"facet = sns.FacetGrid(train_df, hue = 'Survived',aspect=2 )\nfacet.map(sns.kdeplot, 'Fare', shade= True)\nfacet.set(xlim=(0 , train_df['Fare'].max()))\nsns.despine(left = True,bottom = True)\nfacet.set(xticks=np.arange(17,31,1))\nplt.xlim(17,30)","6c1d4ed7":"train_test_df = [train_df, test_df]\n\nfor df in train_test_df:\n    df.loc[df['Fare'] <= 17, 'Fare'] = 0\n    df.loc[(df['Fare'] > 17) & (df['Fare'] <= 30), 'Fare'] = 1\n    df.loc[(df['Fare'] > 30) & (df['Age'] <= 100), 'Fare'] = 2\n    df.loc[df['Fare'] > 100, 'Fare'] = 3","1a466cad":"train_df.head()","3b675dbd":"train_df.drop(['PassengerId','Ticket','Cabin'], axis = 1, inplace = True)\ntest_df.drop(['Ticket','Cabin'], axis = 1, inplace = True)","e1b61323":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier","637fa659":"X_train = train_df.drop(\"Survived\", axis=1)\nY_train = train_df[\"Survived\"]\nX_test  = test_df.drop(\"PassengerId\", axis=1).copy()\nX_train.shape, Y_train.shape, X_test.shape","7f178886":"X_test.info()","68e0898d":"logreg = LogisticRegression()\nlogreg.fit(X_train, Y_train)\nY_pred = logreg.predict(X_test)\nacc_log = round(logreg.score(X_train, Y_train) * 100, 2)\nacc_log","1670eec6":"svc = LinearSVC()\nsvc.fit(X_train, Y_train)\nY_pred = svc.predict(X_test)\nacc_svc = round(svc.score(X_train, Y_train) * 100, 2)\nacc_svc","efe81c77":"knn = KNeighborsClassifier(n_neighbors = 3)\nknn.fit(X_train, Y_train)\nY_pred = knn.predict(X_test)\nacc_knn = round(knn.score(X_train, Y_train) * 100, 2)\nacc_knn","2d032c9a":"gaussian = GaussianNB()\ngaussian.fit(X_train, Y_train)\nY_pred = gaussian.predict(X_test)\nacc_gaussian = round(gaussian.score(X_train, Y_train) * 100, 2)\nacc_gaussian","7ef7d28b":"perceptron = Perceptron()\nperceptron.fit(X_train, Y_train)\nY_pred = perceptron.predict(X_test)\nacc_perceptron = round(perceptron.score(X_train, Y_train) * 100, 2)\nacc_perceptron","b758daab":"linear_svc = LinearSVC()\nlinear_svc.fit(X_train, Y_train)\nY_pred = linear_svc.predict(X_test)\nacc_linear_svc = round(linear_svc.score(X_train, Y_train) * 100, 2)\nacc_linear_svc","4986bbdf":"sgd = SGDClassifier()\nsgd.fit(X_train, Y_train)\nY_pred = sgd.predict(X_test)\nacc_sgd = round(sgd.score(X_train, Y_train) * 100, 2)\nacc_sgd","7ed329df":"decision_tree = DecisionTreeClassifier()\ndecision_tree.fit(X_train, Y_train)\nY_pred = decision_tree.predict(X_test)\nacc_decision_tree = round(decision_tree.score(X_train, Y_train) * 100, 2)\nacc_decision_tree","0d7bf91f":"random_forest = RandomForestClassifier(n_estimators=3)\nrandom_forest.fit(X_train, Y_train)\nY_pred = random_forest.predict(X_test)\nrandom_forest.score(X_train, Y_train)\nacc_random_forest = round(random_forest.score(X_train, Y_train) * 100, 2)\nacc_random_forest","89899dcc":"submission = pd.DataFrame({\n        \"PassengerId\": test_df[\"PassengerId\"],\n        \"Survived\": Y_pred\n    })\nsubmission.to_csv('submissionRF.csv', index=False)","22ef098e":"models = pd.DataFrame({\n    'Model': ['Support Vector Machines', 'KNN', 'Logistic Regression', \n              'Random Forest', 'Naive Bayes', 'Perceptron', \n              'Stochastic Gradient Decent', 'Linear SVC', \n              'Decision Tree'],\n    'Score': [acc_svc, acc_knn, acc_log, \n              acc_random_forest, acc_gaussian, acc_perceptron, \n              acc_sgd, acc_linear_svc, acc_decision_tree]})\nmodels.sort_values(by='Score', ascending=False)","2da2b702":"from sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import cross_val_score\n\nk_fold = StratifiedKFold(n_splits = 10, shuffle = True, random_state = 1)","c70308f2":"from sklearn.model_selection import cross_val_score\nrf = RandomForestClassifier(n_estimators=3)\nscores = cross_val_score(rf, X_train, Y_train, cv=10, scoring = \"accuracy\")\nprint(\"Scores:\", scores)\nprint(\"Mean:\", scores.mean())\nprint(\"Standard Deviation:\", scores.std())","d842f455":"importances = pd.DataFrame({'feature':X_train.columns,'Importance':np.round(random_forest.feature_importances_,3)})\nimportances = importances.sort_values('Importance',ascending=False).set_index('feature')\nimportances.head(7)","4b0a2e2e":"from sklearn.model_selection import GridSearchCV","eee28371":"parameters = {'n_estimators': (10,30,50,70,90,100),\n             'criterion': ('gini', 'entropy'),\n             'max_depth': (3,5,7,9,10),\n             'max_features': ('auto', 'sqrt'),\n             'min_samples_split': (2,4,6)\n             }","a1a568b8":"RF_grid = GridSearchCV(RandomForestClassifier(n_jobs = -1, oob_score = False), param_grid = parameters, cv = 3, verbose = True)","def90519":"RF_grid_model = RF_grid.fit(X_train, Y_train)","a1faba52":"RF_grid_model.best_estimator_","42d4538d":"RF_grid_model.best_score_","d013f50d":"from sklearn.model_selection import cross_val_predict\nfrom sklearn.metrics import confusion_matrix\npredictions = cross_val_predict(random_forest, X_train, Y_train, cv=3)\nconfusion_matrix(Y_train, predictions)","9ea50251":"from sklearn.metrics import precision_score, recall_score\n\nprint(\"Precision:\", precision_score(Y_train, predictions))\nprint(\"Recall:\",recall_score(Y_train, predictions))","36a83dc6":"from sklearn.metrics import f1_score\nf1_score(Y_train, predictions)","c43578ff":"from sklearn.metrics import precision_recall_curve\n\n# getting the probabilities of our predictions\ny_scores = random_forest.predict_proba(X_train)\ny_scores = y_scores[:,1]\n\nprecision, recall, threshold = precision_recall_curve(Y_train, y_scores)\ndef plot_precision_and_recall(precision, recall, threshold):\n    plt.plot(threshold, precision[:-1], \"r-\", label=\"precision\", linewidth=5)\n    plt.plot(threshold, recall[:-1], \"b\", label=\"recall\", linewidth=5)\n    plt.xlabel(\"threshold\", fontsize=19)\n    plt.legend(loc=\"upper right\", fontsize=19)\n    plt.ylim([0, 1])\n\nplt.figure(figsize=(14, 7))\nplot_precision_and_recall(precision, recall, threshold)\nplt.show()","09f46736":"def plot_precision_vs_recall(precision, recall):\n    plt.plot(recall, precision, \"g--\", linewidth=2.5)\n    plt.ylabel(\"recall\", fontsize=19)\n    plt.xlabel(\"precision\", fontsize=19)\n    plt.axis([0, 1.5, 0, 1.5])\n\nplt.figure(figsize=(14, 7))\nplot_precision_vs_recall(precision, recall)\nplt.show()","f23f5ee1":"from sklearn.metrics import roc_curve\n# compute true positive rate and false positive rate\nfalse_positive_rate, true_positive_rate, thresholds = roc_curve(Y_train, y_scores)\n# plotting them against each other\ndef plot_roc_curve(false_positive_rate, true_positive_rate, label=None):\n    plt.plot(false_positive_rate, true_positive_rate, linewidth=2, label=label)\n    plt.plot([0, 1], [0, 1], 'r', linewidth=4)\n    plt.axis([0, 1, 0, 1])\n    plt.xlabel('False Positive Rate (FPR)', fontsize=16)\n    plt.ylabel('True Positive Rate (TPR)', fontsize=16)\n\nplt.figure(figsize=(14, 7))\nplot_roc_curve(false_positive_rate, true_positive_rate)\nplt.show()","566f6612":"from sklearn.metrics import roc_auc_score\nr_a_score = roc_auc_score(Y_train, y_scores)\nprint(\"ROC-AUC-Score:\", r_a_score)","2b81117b":"## Age","cecf2c79":"**Analysing Features & Initial Data Presumptions:**\n\n* PassengerId: is a running index (we will drop this feature later on);\n\n* Survived: indication whether the passenger survived (1) or not (0);\n\n* Pclass: Ticket-class: first (1), second (2), and third (3) class tickets. We might assume that first PClass might correlate positively with the Fare feature as a higher class should cost more relative to second or third ticket-classes;\n\n* Name: Name of the passenger. Names contain the family name (indicating possible family relations), titles, such as Mr., Miss., Mrs. (indicating certain age groups or even possibly correlating to Pclass if the name contains a nobility title);\n\n* Sex: indicator whether the passenger was (female) or (male);\n\n* Age: the age in years of the passenger. Listed as a float64, it might be corectly converted to int64 as no decimal age notation was registered. There are 177 NaN values in the train_df and 86 NaN values in the test_df. To estimate some of those missing values, we might use some of the name titles indicating certain age groups;\n\n* SibSp: number of siblings \/ spouses aboard the Titanic associated with the passenger;\n\n* Parch: number of parents \/ children aboard the Titanic associated with the passenger;\n\n* Ticket: alphanumeric variable indicating the ticket number. (If totally random, we might as well drop it too later on);\n\n* Fare: how much each passenger paid for the ticket; There's a single missing-value in the test_df;\n\n* Cabin: cabin number of each passenger. Around 77% and 78% of the values are missing for the train_df and test_df respectively;\n\n* Embarked: shows the port of embarkation as a categorical character value. Either (C), (S) or (Q) (C = Cherbourg; Q = Queenstown; S = Southampton);","41057620":"## Embarked","697038ab":"## Model Evaluation","674ba72e":"Graphical Observations Regarding Sex vs Survival Probability\n\n* 74% of Females Survived vs 19% of Males Survived","aee59115":"## Name Analysis\nIn order to fill most accuratly the missing-values associated with Age and further detail our Exploratory Data Analysis, let's extract titles from passenger names. Not only grouping the final result by Pclass should improve our estimates, most importantly we should group by Parch as most definitely it will present the most discrepencies regarding the mean age (e.g. a \"Miss\" title may translate a group age of either a child, yound-adult or adult, depending whether or not the person is accompanied by other famility members).","222b835a":"# Exploratory Data Analysis (EDA)","08ae0f27":"> Unfortunatly, around 20% of the combined dataframe does not have an 'Age' value.","30486db7":"# The Challenge\n\n\"The sinking of the Titanic is one of the most infamous shipwrecks in history.\n\nOn April 15, 1912, during her maiden voyage, the widely considered \u201cunsinkable\u201d RMS Titanic sank after colliding with an iceberg. Unfortunately, there weren\u2019t enough lifeboats for everyone onboard, resulting in the death of 1502 out of 2224 passengers and crew.\n\nWhile there was some element of luck involved in surviving, it seems some groups of people were more likely to survive than others.\n\nIn this challenge, we ask you to build a predictive model that answers the question: \u201cwhat sorts of people were more likely to survive?\u201d using passenger data (ie name, age, gender, socio-economic class, etc).\"\n\nhttps:\/\/www.kaggle.com\/c\/titanic\/overview\/","502f881f":"## Precision and Recall","f2d1a924":"> The y-axis of a KDE plot represents the Kernel Density Estimate (KDE) of the Probability Density Function (PDF) of a random variable, interpreted as a probability differential. The probability of a value being between two x values is the total area under the curve.\n\n**Graphical Observations Regarding Age vs Survival Probability**\n* Children under 8 years old had high survival rate;\n* Passengers above 40 years old had split odds of survival;\n* Most passengers that did not survive were between 16 and 32 years old;\n\n","1413281f":"## Grid Search","e5d5335a":"## Sex","4da279d0":"## How Many Survived?","fbb302a6":"## Age Analysis","50a62c94":"# Data","f54ed1f5":"## Random Forest","ad221c77":"> Visually, Cabin features most missing values.","41e09a0e":"## Data Types, Describing Data and Missing Values","5812aabc":"## F-Score","29b16047":"## Cross Validation (K-Fold)","88c8db03":"> Converting Age to integer","a4eb8284":"## Fare","0d3eb8f5":"## SibSp and Parch Analysis","3b98f7fd":"## Confusion Matrix","d409a82d":"## SGD","8750039e":"## Modelling","660c50e6":"## Naive Bayes","d1d1cd41":"## Linear SVC","8e6d96f1":"## Decision Tree","fa757415":"In order to have a better predictive algorithm, we must achieve better than 61.6% accuracy. (Same as predicting everyone dies)","37bf0929":"# Perceptron","4033dc04":"## ROC AUC Curve","9e2793ee":"## Sex Analysis","d84f8542":"# Feature Engineering","19b12a91":"## Embarked","7617a906":"## Precision Recall Curve","6ec88304":"## Fare","e4eb357c":"## KNN","9cd40cdb":"## Logistic Regression","8992b829":"## PClass Analysis","293e3cdc":"## SVM"}}