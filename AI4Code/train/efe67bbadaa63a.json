{"cell_type":{"465bbe37":"code","bc6b6bcf":"code","b951fd38":"code","e88243de":"code","c83bcd5c":"code","dd889a32":"code","476203e2":"code","db653cb6":"code","a545e16c":"code","ee1da92d":"code","54826c71":"code","5ba4e2c8":"code","d45370d2":"code","9b132763":"code","0ef5a5af":"code","556a6d14":"code","3fb9b557":"code","38ac5bd5":"code","7bb6cc66":"code","f378d6b0":"code","40e2662e":"code","10581317":"code","dc0b7814":"code","4a92a14d":"code","f9fd92f1":"code","89b7b031":"code","807e73c3":"code","151a8b1a":"code","a77303a1":"code","3b4bbb42":"code","b15b01c7":"code","43a9d0bd":"code","a3941023":"code","d080bee4":"code","b188a3c8":"code","e72e9ff2":"code","a5b086a9":"markdown","3ab6ac7c":"markdown","9c50e0bf":"markdown","ac594647":"markdown","a2964cb8":"markdown","a94d5017":"markdown","d812d86e":"markdown","04836916":"markdown","109a3ef8":"markdown","82e06200":"markdown","07d21537":"markdown","f07dbb8b":"markdown","a16e602c":"markdown","15d7dc1a":"markdown","ddeaa49f":"markdown","feb7dce8":"markdown","0ba9278c":"markdown","f898fddc":"markdown","9ae9a476":"markdown","c97f5a54":"markdown","a0dd7dfb":"markdown","cffaedd4":"markdown","14e4a9dc":"markdown","e2d337f2":"markdown","0472f863":"markdown","53ab144d":"markdown","0fb5e816":"markdown","7b0f0467":"markdown","6fd154d2":"markdown","4b4f3d33":"markdown","5dd88481":"markdown","c1f4ba37":"markdown"},"source":{"465bbe37":"import numpy             as np\nimport pandas            as pd\nimport matplotlib.pyplot as plt\nimport seaborn           as sns\nimport plotly.graph_objs as go\nimport plotly.express    as px \n\n!pip install millify\nfrom millify     import millify\nfrom scipy.stats import norm\nfrom wordcloud   import WordCloud, STOPWORDS\nfrom textblob import TextBlob\nimport re\nfrom collections import Counter\n\nfrom sklearn.metrics import classification_report,accuracy_score,confusion_matrix\nfrom IPython.display import Markdown as md\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","bc6b6bcf":"df=pd.read_csv(\"..\/input\/covid19-tweets\/covid19_tweets.csv\")\ndf.head()","b951fd38":"df.shape","e88243de":"print('There are {} rows and {} columns in the dataset'.format(df.shape[0], df.shape[1]))","c83bcd5c":"#let's get some information about the data types of our dataset by executing the code below\ndf.info()","dd889a32":"#Let's find out about the missing values in the dataset by executing the code below:\ndf.isna().sum()","476203e2":"missing_values = pd.DataFrame()\nmissing_values['column'] = df.columns\n\nmissing_values['percent'] = [round(100* df[col].isnull().sum() \/ len(df), 2) for col in df.columns]\nmissing_values = missing_values.sort_values('percent')\nmissing_values = missing_values[missing_values['percent']>0]","db653cb6":"plt.figure(figsize=(15, 5))\nsns.set(style='whitegrid', color_codes=True)\nsplot=sns.barplot(x='column', y='percent', data=missing_values)\nfor p in splot.patches:\n    splot.annotate(format(p.get_height(), '.2f'), (p.get_x() + p.get_width() \/ 2., p.get_height()), ha = 'center',\n                   va = 'center', xytext = (0, 9), textcoords = 'offset points')\nplt.xlabel(\"Column_Name\", size=14, weight=\"bold\")\nplt.ylabel(\"Percentage\", size=14, weight=\"bold\")\nplt.title(\"Percentage of missing values in column\",fontweight=\"bold\",size=17)\nplt.show()","a545e16c":"df_username_count = df['user_name'].value_counts().reset_index().rename(columns={\n    'user_name':'tweet_count','index':'user_name'})","ee1da92d":"df = pd.merge(df, df_username_count, on='user_name')","54826c71":"data = df.sort_values('user_followers',ascending=False).drop_duplicates(subset='user_name', keep=\"first\")\ndata = data[['user_name', 'user_followers', 'tweet_count']]\ndata.sort_values('user_followers',ascending=False)\n\ndata1 = data.head(50).reset_index().copy()\nfor i in range(50):\n    data1['user_followers'][i] = millify(data1['user_followers'][i],precision=2)\n    \ndata1['user_followers'] = data1['user_followers'].str[:-1].astype(float) # To remove 'M'\n\nplt.figure(figsize=(15, 17))\nsns.barplot(y='user_name',x='user_followers',data=data1.head(50))\ny=data1['user_followers'].head(50)\nfor index, value in enumerate(y):\n    plt.text(value, index, str(value),fontsize=12)\nplt.title('Top50 users by number of followers',weight='bold', size=15)\nplt.ylabel('User_name', size=12, weight='bold')\nplt.xlabel('Followers_count( in Millions )', size=12, weight='bold')\nplt.show()","5ba4e2c8":"def plot_frequency_charts(df, feature, title, pallete):\n    freq_df = pd.DataFrame()\n    freq_df[feature] = df[feature]\n    \n    f, ax = plt.subplots(1,1, figsize=(16,4))\n    total = float(len(df))\n    g = sns.countplot(df[feature], order = df[feature].value_counts().index[:20], palette=pallete)\n    g.set_title(\"Number and percentage of {}\".format(title))\n\n    for p in ax.patches:\n        height = p.get_height()\n        ax.text(p.get_x()+p.get_width()\/2.,\n                height + 3,\n                '{:1.2f}%'.format(100*height\/total),\n                ha=\"center\") \n\n    plt.title('Frequency of {} tweeting about Corona'.format(feature),weight='bold', size=15)\n    plt.ylabel('Frequency', size=12, weight='bold')\n    plt.xlabel(title, size=12, weight='bold')\n    plt.xticks(rotation=90)\n    plt.show()","d45370d2":"plot_frequency_charts(df, 'user_name', 'User Names','inferno')","9b132763":"df = df.fillna('None')\n\ndef wordcloud(string,title,color):\n    wc = WordCloud(background_color=color, width=1200,height=600,mask=None,random_state=1,\n                   max_font_size=200,stopwords=stop_words,collocations=False).generate(string)\n    fig=plt.figure(figsize=(20,8))\n    plt.axis('off')\n    plt.title('--- WordCloud for {} --- '.format(title),weight='bold', size=30)\n    plt.imshow(wc)","0ef5a5af":"stop_words=set(STOPWORDS)\nsource_string = \" \".join(df['source'].astype('str'))\nhastage_string = \" \".join(df['hashtags'].astype('str'))\nlocation_string = \" \".join(df['user_location'].astype('str'))","556a6d14":"wordcloud(source_string,'Source','white')","3fb9b557":"data = pd.read_csv('\/kaggle\/input\/twitterdata\/finalSentimentdata2.csv')\ndata.head()","38ac5bd5":"sns.countplot(data['sentiment'])","7bb6cc66":"#importing the necessary libraries for preprocessing of the dataset\nimport nltk\nimport re\nimport string","f378d6b0":"def clean_text(text):\n    '''Make text lowercase, remove text in square brackets,remove links,remove punctuation\n    and remove words containing numbers.'''\n    text = text.lower()\n    text = re.sub('\\[.*?\\]', '', text)\n    text = re.sub('https?:\/\/\\S+|www\\.\\S+', '', text)\n    text = re.sub('<.*?>+', '', text)\n    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n    text = re.sub('\\n', '', text)\n    text = re.sub('\\w*\\d\\w*', '', text)\n    return text","40e2662e":"data['text'] = data['text'].apply(lambda x: clean_text(x))","10581317":"def remove_emoji(text):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)\n\n","dc0b7814":"data['text']=data['text'].apply(lambda x: remove_emoji(x))","4a92a14d":"data['text'].apply(lambda x:len(str(x).split())).max()","f9fd92f1":"from sklearn.model_selection import train_test_split\n\ntrain,valid = train_test_split(data,test_size = 0.2,random_state=0,stratify = data.sentiment.values) #stratification means that the train_test_split method returns training and test subsets that have the same proportions of class labels as the input dataset.\nprint(\"train shape : \", train.shape)\nprint(\"valid shape : \", valid.shape)","89b7b031":"from sklearn.feature_extraction.text import CountVectorizer\nfrom nltk.corpus import stopwords\nstop = list(stopwords.words('english'))\nvectorizer = CountVectorizer(decode_error = 'replace',stop_words = stop)\n\nX_train = vectorizer.fit_transform(train.text.values)\nX_valid = vectorizer.transform(valid.text.values)\n\ny_train = train.sentiment.values\ny_valid = valid.sentiment.values\n\nprint(\"X_train.shape : \", X_train.shape)\nprint(\"X_train.shape : \", X_valid.shape)\nprint(\"y_train.shape : \", y_train.shape)\nprint(\"y_valid.shape : \", y_valid.shape)","807e73c3":"# example of grid searching key hyperparametres for logistic regression\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import LogisticRegression\n\n\n\n# define models and parameters\nmodel = LogisticRegression()\nsolvers = ['newton-cg', 'lbfgs', 'liblinear']\npenalty = ['l2']\nc_values = [100, 10, 1.0, 0.1, 0.01, 0.001]\n\n# define grid search\ngrid = dict(solver=solvers, penalty=penalty, C=c_values)\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\ngrid_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, cv=cv, \n                           scoring='accuracy',error_score=0, iid=True)\ngrid_result = grid_search.fit(X_train, y_train)\n\n# summarize results\nprint(f\"Best: {grid_result.best_score_:.3f} using {grid_result.best_params_}\")","151a8b1a":"from sklearn.linear_model import RidgeClassifier\n\n# define models and parameters\nmodel = RidgeClassifier()\nalpha = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n\n# define grid search\ngrid = dict(alpha=alpha)\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\ngrid_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, cv=cv, \n                           scoring='accuracy',error_score=0, iid=True)\ngrid_result = grid_search.fit(X_train, y_train)\n\n# summarize results\nprint(f\"Best: {grid_result.best_score_:.3f} using {grid_result.best_params_}\")","a77303a1":"from sklearn.neighbors import KNeighborsClassifier\n\n# define models and parameters\nmodel = KNeighborsClassifier()\nn_neighbors = range(1, 21, 2)\nweights = ['uniform', 'distance']\nmetric = ['euclidean', 'manhattan', 'minkowski']\n\n# define grid search\ngrid = dict(n_neighbors=n_neighbors,weights=weights,metric=metric)\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\ngrid_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, cv=cv, \n                           scoring='accuracy',error_score=0, iid=True)\ngrid_result = grid_search.fit(X_train, y_train)\n\n# summarize results\nprint(f\"Best: {grid_result.best_score_:.3f} using {grid_result.best_params_}\")","3b4bbb42":"from sklearn.naive_bayes import MultinomialNB\n\nnaiveByes_clf = MultinomialNB()\n\nnaiveByes_clf.fit(X_train,y_train)\n\nNB_prediction = naiveByes_clf.predict(X_valid)\nNB_accuracy = accuracy_score(y_valid,NB_prediction)\nprint(\"training accuracy Score    : \",naiveByes_clf.score(X_train,y_train))\nprint(\"Validation accuracy Score : \",NB_accuracy )\nprint(classification_report(NB_prediction,y_valid))","b15b01c7":"from sklearn.linear_model import SGDClassifier\n\nsgd_clf = SGDClassifier(loss = 'hinge', penalty = 'l2', random_state=0)\n\nsgd_clf.fit(X_train,y_train)\n\nsgd_prediction = sgd_clf.predict(X_valid)\nsgd_accuracy = accuracy_score(y_valid,sgd_prediction)\nprint(\"Training accuracy Score    : \",sgd_clf.score(X_train,y_train))\nprint(\"Validation accuracy Score : \",sgd_accuracy )\nprint(classification_report(sgd_prediction,y_valid))","43a9d0bd":"from sklearn.ensemble import RandomForestClassifier\n\nrf_clf = RandomForestClassifier()\n\nrf_clf.fit(X_train,y_train)\n\nrf_prediction = rf_clf.predict(X_valid)\nrf_accuracy = accuracy_score(y_valid,rf_prediction)\nprint(\"Training accuracy Score    : \",rf_clf.score(X_train,y_train))\nprint(\"Validation accuracy Score : \",rf_accuracy )\nprint(classification_report(rf_prediction,y_valid))","a3941023":"#takes huge amount of time to execute\nimport xgboost as xgb\n\nxgboost_clf = xgb.XGBClassifier()\n\nxgboost_clf.fit(X_train, y_train)\n\nxgb_prediction = xgboost_clf.predict(X_valid)\nxgb_accuracy = accuracy_score(y_valid,xgb_prediction)\nprint(\"Training accuracy Score    : \",xgboost_clf.score(X_train,y_train))\nprint(\"Validation accuracy Score : \",xgb_accuracy )\nprint(classification_report(xgb_prediction,y_valid))","d080bee4":"from sklearn.svm import SVC\n\nsvc = SVC()\n\nsvc.fit(X_train, y_train)\n\nsvc_prediction = svc.predict(X_valid)\nsvc_accuracy = accuracy_score(y_valid,svc_prediction)\nprint(\"Training accuracy Score    : \",svc.score(X_train,y_train))\nprint(\"Validation accuracy Score : \",svc_accuracy )\nprint(classification_report(svc_prediction,y_valid))","b188a3c8":"from sklearn.linear_model import LogisticRegression\nlogreg = LogisticRegression()\n\nlogreg.fit(X_train, y_train)\n\nlogreg_prediction = logreg.predict(X_valid)\nlogreg_accuracy = accuracy_score(y_valid,logreg_prediction)\nprint(\"Training accuracy Score    : \",logreg.score(X_train,y_train))\nprint(\"Validation accuracy Score : \",logreg_accuracy )\nprint(classification_report(logreg_prediction,y_valid))","e72e9ff2":"models = pd.DataFrame({\n    'Model': ['Support Vector Machines', 'Logistic Regression', \n              'Random Forest', 'Naive Bayes', \n              'Stochastic Gradient Decent', 'XGBoost'],\n    'Test accuracy': [svc_accuracy, logreg_accuracy, \n              rf_accuracy, NB_accuracy, \n              sgd_accuracy, xgb_accuracy,]})\n\nmodels.sort_values(by='Test accuracy', ascending=False)","a5b086a9":"![coronavirus-social-a-twitter-thumbs-up-for-docs-who-rock-coronavirus-social-a-twitter-thumbs-up-for-docs-who-rock.jpg](attachment:coronavirus-social-a-twitter-thumbs-up-for-docs-who-rock-coronavirus-social-a-twitter-thumbs-up-for-docs-who-rock.jpg)\n\n* Coronavirus disease (COVID-19) is an infectious disease caused by a newly discovered coronavirus. Most people who fall sick with COVID-19 will experience mild to moderate symptoms and recover without special treatment.\n\n* In this notebook, we would perform Sentiment Analysis for the Covid-19 Tweets through passing differnet steps such as:\n* Data underestanding\n* Data Visualization\n* WordCloud\n* Data Preprocessing for the NLP Natural language processing\n* Building different Machine Learning Models\n* Model Evaluation","3ab6ac7c":"# ML model building","9c50e0bf":"# Stochastic Gradient Descent","ac594647":"# Frequency of user_name:","a2964cb8":"* Checking the maximum length of tweet","a94d5017":"# Splitting the dataset","d812d86e":"# Support Vector Machine","04836916":"# Extreme Gradient Boosting","109a3ef8":"# 1. Logistic Regression\n\nLogistic regression does not really have any critical hyperparameters to tune.\n\nSometimes, you can see useful differences in performance or convergence with different solvers (solver).\n- **solver** in `[\u2018newton-cg\u2019, \u2018lbfgs\u2019, \u2018liblinear\u2019, \u2018sag\u2019, \u2018saga\u2019]`\n\nRegularization (penalty) can sometimes be helpful.\n- **penalty** in `[\u2018none\u2019, \u2018l1\u2019, \u2018l2\u2019, \u2018elasticnet\u2019]`\n\n**Note:** not all solvers support all regularization terms.\n\n- The **C** parameter controls the penality strength, which can also be effective. **C** in `[100, 10, 1.0, 0.1, 0.01]`\n\nThe example below demonstrates grid searching the key hyperparameters for LogisticRegression on a synthetic binary classification dataset.","82e06200":"# Removing the Emoji\n![download.jfif](attachment:download.jfif)","07d21537":"# Logistic Regression","f07dbb8b":"# Top50 users by number of followers - Barplot:","a16e602c":"# Data Visualization","15d7dc1a":"# Naive Bayes","ddeaa49f":"# Data Underestanding","feb7dce8":"# Improting the dataset into the kernel","0ba9278c":"# Sentiment Analysis on Covid19 Tweets","f898fddc":"# Let's visualizing the missing values","9ae9a476":"# Data Preprocessing\nNow let us preprocess text using some NLP tchniques like:\n\n* converting to lowercase\n* remove text in square brackets,\n* remove links,\n* remove punctuation\n* remove words containing numbers\n* Removing Punctuation\n* Removing stopwords\n* Stemming\n* Lemmatization","c97f5a54":"* A function to clean data it removes all the punctuation marks, urls etc","a0dd7dfb":"# Vectorizing","cffaedd4":"# Importing the necessary libraries:","14e4a9dc":"# 2. Ridge Classifier\n\nRidge regression is a penalized linear regression model for predicting a numerical value.\n\nNevertheless, it can be very effective when applied to classification.\n\nPerhaps the most important parameter to tune is the regularization strength (alpha). A good starting point might be values in the range `[0.1 to 1.0]`\n\n- alpha in `[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]`","e2d337f2":"### Missing Values","0472f863":"* Count plot of sentiments,we can see that sad and fear are prevailing which is quite obvious","53ab144d":"* To peform sentiment analysis we need labeled dataset.\n* The data can be downloaded from here: https:\/\/www.kaggle.com\/surajkum1198\/twitterdata","0fb5e816":"# WordCloud","7b0f0467":"# Observation:\n* The above barplot shows us that there are only four columns with missing values\n* Percentage of missing values are also shown in the plot","6fd154d2":"* We are going to use CountVectorizer for the following steps","4b4f3d33":"# 3. K-Nearest Neighbors (KNN)\n\nThe most important hyperparameter for KNN is the number of neighbors (n_neighbors).\n\nTest values between at least `1` and `21`, perhaps just the odd numbers.\n\n- n_neighbors in `[1 to 21]`\n\nIt may also be interesting to test different distance metrics (metric) for choosing the composition of the neighborhood.\n\n- metric in `[\u2018euclidean\u2019, \u2018manhattan\u2019, \u2018minkowski\u2019]`\n\nIt may also be interesting to test the contribution of members of the neighborhood via different weightings (weights).\n\n- weights in `[\u2018uniform\u2019, \u2018distance\u2019]`","5dd88481":"## Grid Search\nAll you need to do in GridSearch is tell it which hyperparameters you want it to experiment with, and what values to try out, and it will evaluate all the possible combinations of hyperparameter values, using cross-validation.\n\n## Randomized Search\nthe grid search approach is fine when you are exploring relatevely few combinations, but when the hyperparameter search space is large, it is often preferable to use `RandomizedSearchCV` instead. This technique evaluates a given number of random combinations by selecting a random value for each hyperparameter at every iteration. This approach has two main benefits:\n- If you let the randomized search run for 1000 iterations it will explore 1000 different values for each hyperparameter.\n- You have more control over the computing budget you want to allocate to hyperparameter search, simply by setting the number of iterations.","c1f4ba37":"# Random Forest"}}