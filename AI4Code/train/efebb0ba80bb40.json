{"cell_type":{"13ef3bf7":"code","d050aa6a":"code","c65abcfc":"code","f177cfeb":"code","5fd2aca0":"code","b4a161b6":"code","12ceb02a":"code","d18d6f6c":"code","3370083b":"code","d5cb73f6":"code","22324bae":"code","f0ba4f50":"code","e7df4af6":"code","9ce061a3":"code","8d9edb0e":"code","04b968d9":"code","f7cbf854":"code","d68e0ad6":"code","b6698615":"code","3102483b":"code","1bc8b533":"code","3175a9ee":"code","332cb750":"code","eda7604a":"code","7c909dab":"code","a9e458bb":"code","e947f16f":"code","8a2df728":"code","9c654bc1":"code","b5a16092":"code","52659ca9":"code","298b48a5":"code","ffb88e99":"code","4ec39a2b":"code","1b16668e":"code","b8b155da":"code","17261864":"code","b45660ec":"code","28a5e93f":"code","a39dc8b8":"code","1ac003f6":"code","b852b91c":"code","f4b0ccff":"markdown","efab0cd9":"markdown","e0b12123":"markdown","8e21d83e":"markdown","12957b9f":"markdown","0449462e":"markdown","0066414f":"markdown","0b4bf54f":"markdown","316467bd":"markdown","1c5a9c5e":"markdown","428e5e9c":"markdown","e52deac0":"markdown","61d49187":"markdown","74e7f524":"markdown","173460fb":"markdown","32da2387":"markdown","e688857f":"markdown","6f5a22f4":"markdown","fc0ed78f":"markdown","7f650a30":"markdown","9862b955":"markdown","951b659c":"markdown","335e04b6":"markdown","d2245a7b":"markdown","ecfb8ddd":"markdown","f9f5a0d4":"markdown","88a7b2bc":"markdown","e5d0787d":"markdown","382285d7":"markdown","162cd611":"markdown","6d516cce":"markdown","1f3e6bc5":"markdown","7b05f3ec":"markdown","b0159a1c":"markdown","fdded15b":"markdown","79b777ba":"markdown","d3d6d0a3":"markdown","0144b19f":"markdown","3eae3172":"markdown","e83fcafc":"markdown"},"source":{"13ef3bf7":"# Libraries\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.cluster import KMeans, AgglomerativeClustering\nfrom yellowbrick.cluster import KElbowVisualizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression","d050aa6a":"# Parameters\nsns.set_theme(style=None, palette='pastel')","c65abcfc":"df = pd.read_csv('\/kaggle\/input\/body-performance-data\/bodyPerformance.csv')\n\ndf.head()","f177cfeb":"df.info()","5fd2aca0":"df['class'] = pd.Categorical(df['class'], categories=['A', 'B', 'C', 'D'], ordered=True)","b4a161b6":"df.describe()","12ceb02a":"df.sort_values('sit and bend forward_cm', ascending=False).head(3)","d18d6f6c":"df = df[df['sit and bend forward_cm'] < 50]\ndf.sort_values('sit and bend forward_cm', ascending=False).head(3)","3370083b":"df[df['sit and bend forward_cm'] < 0].head()","d5cb73f6":"df = df[df['sit and bend forward_cm'] >= 0]\ndf[df['sit and bend forward_cm'] < 0].head()","22324bae":"plt.figure (figsize=(16,16))\n\nfor i, column in enumerate(df.columns, 1):\n    plt.subplot(4,3,i)\n    sns.histplot(df[column])","f0ba4f50":"cols_check = ['systolic', 'diastolic', 'gripForce', 'sit-ups counts', 'broad jump_cm']\n\nfor i in cols_check:\n    df = df[df[i] != 0]","e7df4af6":"body_fat = df['body fat_%'] \/ 100\nheight_m = df['height_cm'] \/ 100\nlean_mass = df['weight_kg'] * (1-body_fat)\n\ndf['lean_BMI'] = lean_mass \/ height_m**2","9ce061a3":"# Compute the correlation matrix\ncorr = df.corr()\n\n# Generate a mask for the upper triangle\nmask = np.triu(np.ones_like(corr, dtype=bool))\n\n# Set up the matplotlib figure\nfig, ax = plt.subplots(figsize=(11, 9))\n\n# Generate a custom diverging colormap\ncmap = sns.diverging_palette(230, 20, as_cmap=True)\n\n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\n\nplt.show()","8d9edb0e":"# Create a dataset copy for ML purposes\ndf_ml = df.copy()\n\ndf_ml['class'] = df_ml['class'].astype(str)","04b968d9":"# Get the list of categorical variables\nc = (df_ml.dtypes == 'object')\nobject_cols = (list(c[c].index))\n\nprint('Categorical variables in the dataset:', object_cols)","f7cbf854":"# Label Encoding the object dtypes\nle = LabelEncoder()\nfor i in object_cols:\n    df_ml[i] = df_ml[[i]].apply(le.fit_transform)","d68e0ad6":"# Scaling\nscaler = StandardScaler()\nscaler.fit(df_ml)\nscaled_df = pd.DataFrame(scaler.transform(df_ml),columns=df_ml.columns )","b6698615":"scaled_df.head()","3102483b":"pca = PCA().fit(scaled_df)\n\nimport matplotlib.pyplot as plt\nplt.rcParams[\"figure.figsize\"] = (12,6)\n\nfig, ax = plt.subplots()\nxi = np.arange(1, 14, step=1)\ny = np.cumsum(pca.explained_variance_ratio_)\n\nplt.ylim(0.0,1.1)\nplt.plot(xi, y, marker='o', linestyle='--', color='b')\n\nplt.xlabel('Number of Components')\nplt.xticks(np.arange(0, 14, step=1)) #change from 0-based array index to 1-based human-readable label\nplt.ylabel('Cumulative variance (%)')\nplt.title('The number of components needed to explain variance')\n\nplt.axhline(y=0.95, color='r', linestyle='-')\nplt.text(0.5, 0.85, '95% cut-off threshold', color = 'red', fontsize=16)\n\nax.grid(axis='x')\nplt.show()","1bc8b533":"# Initiating PCA to reduce dimentions to 8\npca = PCA(n_components = 8)\npca.fit(scaled_df)\nPCA_df = pd.DataFrame(pca.transform(scaled_df))","3175a9ee":"Elbow_M = KElbowVisualizer(KMeans(), k=10)\nElbow_M.fit(PCA_df)\nElbow_M.show()","332cb750":"# Agglomerative Clustering model \nAC = AgglomerativeClustering(n_clusters=5)\n# fit model and predict clusters\nyhat_AC = AC.fit_predict(PCA_df)\nPCA_df['clusters'] = yhat_AC\n# Adding the Clusters feature to the orignal dataframe.\ndf['clusters'] = yhat_AC","eda7604a":"df.head()","7c909dab":"# Plot countplot of clusters\nax = sns.countplot(data=df, x='clusters')\nax.set_title('Distribution Of The Clusters')\nplt.show()","a9e458bb":"# Plot scatterplots of height and weight divided by class and cluster type\nax = sns.relplot(data=df, x='height_cm', y='weight_kg', hue='clusters', col='class')\nplt.show()","e947f16f":"# Plot distribution of clusters by score\nax = sns.countplot(data=df, x='class', hue='clusters')\nax.set_title('Distribution of clusters per scoring')\nplt.show()","8a2df728":"tests = ['gripForce', 'sit and bend forward_cm', 'sit-ups counts', 'broad jump_cm']\n\n# Weight comparison plot\nfor i in tests:\n    plt.figure()\n    sns.scatterplot(data=df, x=i, y='weight_kg', hue='clusters')\n    plt.show()","9c654bc1":"# Height comparison plot\nfor i in tests:\n    plt.figure()\n    sns.scatterplot(data=df, x=i, y='height_cm', hue='clusters')\n    plt.show()","b5a16092":"# Age comparison plot\nfor i in tests:\n    plt.figure()\n    sns.scatterplot(data=df, x=i, y='age', hue='clusters')\n    plt.show()","52659ca9":"# Lean_BMI comparison plot\nfor i in tests:\n    plt.figure()\n    sns.scatterplot(data=df, x=i, y='lean_BMI', hue='clusters')\n    plt.show()","298b48a5":"# Plot distribution of gender within the clusters\nax = sns.countplot(data=df, x='clusters', hue='gender')\nax.set_title('Cluster gender distribution')\nplt.show()","ffb88e99":"df.groupby('clusters').mean()","4ec39a2b":"# Define dependent variable\nX = df.drop(['gripForce', 'sit and bend forward_cm', 'sit-ups counts', 'broad jump_cm', 'class', 'clusters'], axis=1)","1b16668e":"# Label encode variables\nle = LabelEncoder()\nX['gender'] = le.fit_transform(X['gender'])","b8b155da":"# Create empty dictionary for coefs\ncoefs = {}\n\n# Initialize linear regression\nlr = LinearRegression()\n\n# Iterate through all tests \nfor i in tests:\n    X_train, X_test, y_train, y_test = train_test_split(X, df[i], test_size=0.3, random_state=42)\n    lr.fit(X_train, y_train)\n    coefs[i] = lr.coef_\n    print('The score for model', i, 'is: ', lr.score(X_test, y_test))","17261864":"# Plot coefficients\nfig, ax = plt.subplots(figsize=(12,8))\n\nnames = X.columns\n\nfor i in tests:\n    ax.plot(range(len(names)), np.reshape(coefs[i], (8)), label=i)\n\nax.set_xticks(range(len(names)), names)\n\nplt.legend()\nplt.show()","b45660ec":"# Divide dataframes by gender\ndf_male = df[df['gender'] == 'M']\ndf_female = df[df['gender'] == 'F']","28a5e93f":"# Define unbiased dependent and independent variables for males\nX_male = df_male.drop(['gripForce', 'sit and bend forward_cm', 'sit-ups counts', 'broad jump_cm', 'class', 'clusters', 'gender'], axis=1)\n# Define unbiased dependent and independent variables for females\nX_female = df_female.drop(['gripForce', 'sit and bend forward_cm', 'sit-ups counts', 'broad jump_cm', 'class', 'clusters', 'gender'], axis=1)","a39dc8b8":"# Initialize LinearRegression\nlr_males = LinearRegression()\n# Create dictionary to append coefficients\ncoefs_males = {}\n# Iterate through all tests \nfor i in tests:\n    X_train, X_test, y_train, y_test = train_test_split(X_male, df_male[i], test_size=0.3, random_state=42)\n    lr_males.fit(X_train, y_train)\n    coefs_males[i] = lr_males.coef_\n    print('The score for model', i, 'on males is: ', lr_males.score(X_test, y_test))","1ac003f6":"# Initialize LinearRegression\nlr_females = LinearRegression()\n# Create dictionary to append coefficients\ncoefs_females = {}\n# Iterate through all tests \nfor i in tests:\n    X_train, X_test, y_train, y_test = train_test_split(X_female, df_female[i], test_size=0.3, random_state=42)\n    lr_females.fit(X_train, y_train)\n    coefs_females[i] = lr_females.coef_\n    print('The score for model', i, 'on females is: ', lr_females.score(X_test, y_test))","b852b91c":"# Plot coefficients\nfig, axs = plt.subplots(1, 2, figsize=(24,8), sharey=True)\n\nnames = X_male.columns\n\nfor i in tests:\n    axs[0].plot(range(len(names)), np.reshape(coefs_males[i], (7)), label=i)\n    axs[1].plot(range(len(names)), np.reshape(coefs_females[i], (7)), label=i)\n\n\naxs[0].set_xticks(range(len(names)), names)\naxs[1].set_xticks(range(len(names)), names)\n\naxs[0].set_title('Males')\naxs[1].set_title('Females')\n\nhandles, labels = axs[0].get_legend_handles_labels()\nfig.legend(handles, labels, loc='upper center')\n\nplt.show()","f4b0ccff":"The chart above doesn't provide much information we didn't know from before. Cluster 4 is related with people with the worst score. However, the other three clusters are distributed evenly between the other three scores. They need to have another meaning.","efab0cd9":"It is impossible to score less than 0 in that test so I'm going to filter the dataframe to remove those values as well.\n","e0b12123":"The data is now clean. Let's plot the **correlation** between the variables in a heatmap,","8e21d83e":"Taking a look at the charts plotted we can say that cluster 4 individuals are heavier, on average, than the other individuals. It is also important to notice that, **the heavier the individual, the better gripForce score**. In the the sit-ups and broad jump tests, it seems to be a positive correlation between weight and better results but it is not as high as the previous one. \n\nLet's see know how the height compares to the score.","12957b9f":"The information regarding the dataset shows us that there aren't any missing values. The datatypes of the features seems to be correct as well. However, before continuing, I will set the correct order for the **class** feature.","0449462e":"After the model is fitted and the cluster data parsed in the original dataset, let's take a look at the new one:","0066414f":"The data looks pretty normally distributed. However, its seems that there are missing values in the  dataset. I will proceed to filter the dataframe to remove all the observations where the value is 0 except from **sit and bend forward_cm** because it's possible to score 0 in this test.","0b4bf54f":"From the above charts, we can also get two important insights:\n\n1. The younger the individual, the better the score.\n2. **Individuals from clusters 2 and 3 are, on average, older than the rest of the dataset**.\n\nLet's compare it now with the lean BMI.","316467bd":"As we can see in the above graph, **the most influencial parameter in all test is the gender**. It looks like male individuals perform better than female in strenght tests while women do better in flexibility ones. The second most important parameter is the **lean BMI** and it makes sense. The higher the lean BMI is, the lower the body fat is, the higher the muscle mass is, the better an individual can perform in physical tests. **Height and weight** are somewhat important and came make a different depending on the test. For example, a heavier guy is probably going to perform worse on the broad jump and better in the grip force than a lighter one. \n\nAt this point, we know that gender plays a huge rol in explaining the test result. Let's remove this bias and try it again.","1c5a9c5e":"The above chart indicates that the optimal number of clusters for this datasets is five. Let's fit the agglomerative clustering model to get the final clusters.","428e5e9c":"Once that's taking care of. Let's analyze the values where the test was below 0.","e52deac0":"As we can see, there are two individuals who scored an incredibly 185 and 213cm. This is impossible so I will filter the dataframe to remove this outliers.","61d49187":"After label enconding the categorical variables and scale the features, the dataframe that I will use for dimensionality reduction is the following:","74e7f524":"_Source: https:\/\/www.mikulskibartosz.name\/pca-how-to-choose-the-number-of-components\/_\n\nThe above chart shows that, to achieve a 95% of variance explained, I need to get at least 8 variables so, **we will select 8 principal components** to carry the dimensionality reduction.\n\n","173460fb":"<a id=\"10\"><\/a>\n## CONCLUSIONS\n\nThe unsupervised clustering provided a good data segmentation. Despite this technique wasn't really needed to carry on the analysis, it showed five different groups with common characteristics.\n\nFrom the data analyzed, we can draw the conclusion that body performance decreases during the years and males and females have different strenghts. In this case, females were more flexible while males did better in strenght tests.\n\nAlso, a higher bodyfat can lead to bad body performance and higher blood pressure. ","32da2387":"The description of the dataset shows us there is an impossible value. In the **sit and bend forward_cm** test, one or more individuals score less than 0. Also, some individual scored 213cm. That is impossible! Let's take care of it.","e688857f":"Both plots show similar results. **Height** influence positively in all results while **weight** does it negatively. This means that the taller an individual is, the better he is going to perform and, the heavier, the worse. **Lean BMI** seems to be the most influencial variable af all and it makes sense. This variable measures the amount of muscle an individual has depending on his height. The higher the lean BMI is, the more athletic he is going to be, therefore, the better he is going to perform in physical tests. ","6f5a22f4":"From the above chart we can see there are some variables that are strongly negative correlated like the age and test results or the bodyfat and test results. This means that the older or fatter an individual is, the worst he's going to perform in the tests.","fc0ed78f":"## WHAT IMPACTS BODY PERFORMANCE?\n\n![Body Performance](https:\/\/images.unsplash.com\/photo-1502904550040-7534597429ae?ixlib=rb-1.2.1&ixid=MnwxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8&auto=format&fit=crop&w=1700&q=80)\n\nIn this notebook, I will try to analyze the different features that makes an individual perform better than other and check possible correlations among other things. The dataset contains more than 13,000 observations from individuals aged from 20 to 64, its physical characteristics and their results in some tests.\n\nThe objective of this notebook is to perform some unsupervised learning to discover possible clusters regarding common characteristics. During the notebook, I will also try to analyze and explore the data visually.","7f650a30":"<a id=\"2\"><\/a>\n## DATA LOADING","9862b955":"Now we can draw some conclusions to determine which type of people form each cluster:\n\n* **Cluster 0**: young tall males with high lean BMI.\n* **Cluster 1**: young small females with low lean BMI.\n* **Cluster 2**: old tall males with high lean BMI.\n* **Cluster 3**: old small females with low lean BMI.\n* **Cluster 4**: overweight males in its majority.","951b659c":"Now, let's check for possible missing outliers by plotting a histogram.","335e04b6":"<a id=\"3\"><\/a>\n## DATA CLEANING\n\nIn this section, I will try to find values that keep the data untidy and dirty. Let's start by looking at the features datatype and the most common statistics.","d2245a7b":"<a id=\"6\"><\/a>\n## CLUSTERING\n\nAfter reducing the attributes to eight dimensions, I will perform clustering using Agglomerative clustering. This type of clustering is a hierarchical clustering method that involves merging examples until the desired number of clusters is achieved.","ecfb8ddd":"<a id=\"8\"><\/a>\n## LINEAR REGRESSION\n\nAt this point of the analysis, I thought it would be good to know what features determine the test results. For example, is a heavier person going to perform better in a strenght test? To answer these kind of questions, I'm going to run a linear regression model.","f9f5a0d4":"Once the order is set up, let's continue by analyzing its most significant statistics.","88a7b2bc":"<a id=\"4\"><\/a>\n## DATA PREPROCESSING\n\nIn this setion, I will preprocess the data to perform clustering operations.\n\nThe following steps are applied to preprocess the data:\n\n* Label encoding for categorical features.\n* Scale the features using the standard scaler.\n* Create a subset dataframe for dimensionality reduction.","e5d0787d":"Once that the predictor variables are defined, let's build the models:","382285d7":"As we can see, the score has dropped down a lot. This is because we have removed the most important variable from the model: the gender. However, we just want to know all the other features perform when the gender is not taken into account.  Let's plot all the coefficients to check out the results!","162cd611":"Now, let's group all observations by cluster type and get the mean of all the features to get more details regarding each cluster.","6d516cce":"<a id=\"8\"><\/a>\n## PROFILING\n\nIn this section, I will try to deduce which individuals are in which clusters. To decide that, I will be plotting some of the features present in the dataset.\n\nLet's first analyze the relation between the weight and the score on different tests.","1f3e6bc5":"As we can see in the charts above, it's pretty clear that theere is a correlation between height, weight and score. **The individuals who weighted more, scored worst in the tests on average.** We can also see that majority of cluster 4 observations are in the class D. Let's check for the other three.","7b05f3ec":"Before proceeding, I\u00b4m going to create a variable to standarize the height, weight and bodyfat. The purpose of this variable is to take all three parameters into account when checking the test results. For example, an individual with high weight but low body fat can perform well in the situps test and a short guy perform worse in the broad jump test.\n\nThe new variable will be called **lean_BMI**. It's calculated by getting the lean body mass and dividing by the squared heihgt. This is like a lean BMI formula.","b0159a1c":"As we can see in the above chart, the clusters are not equally distributed. Let's try to figure out what each one can mean.\n\nFirst thing I'm going to do is to check if height and weight has something to do with the score. I will divide each plot by clusters.","fdded15b":"## TABLE OF CONTENTS\n\n\n1. [LIBRARIES AND PARAMETERS](#1)\n2. [DATA LOADING](#2)\n3. [DATA CLEANING](#3)\n4. [DATA PREPROCESSING](#4)\n5. [DIMENSIONALITY REDUCTION](#5)\n6. [CLUSTERING](#6)\n7. [EVALUATING MODELS](#7)\n8. [PROFILING](#8)\n9. [LINEAR REGRESSION](#9)\n10. [CONCLUSIONS](#10)\n","79b777ba":"<a id=\"5\"><\/a>\n## DIMENSIONALITY REDUCTION\n\nIn this dataset, there are a lot of factors that influence in the final classifications. These factors are features. The higher number of features, the harder it is to work with. As many of these features are correlated, as we saw before, they are redundant. This is why I will be performing dimensionality reduction before putting them through a classifier.\n\nFor this process, I will use the **Principal component analysis(PCA)**. This technique allow us to reduce the dimensionality of the dataset and increase the interpretability without minimizing information loss.","d3d6d0a3":"With this plot we can get some interesting insights! \n\n1. It seems that taller people perform better in broad jump and grip force tests. However, in the other two, height doesn't seem to provide any advantage at all. \n2. **Individuals from cluster 0 are taller than the average while individuals from cluster 3 are shorter**.\n\nNow, let's compare the results with the age.","0144b19f":"From the charts above, we can also get two important insights:\n\n1. Individuals with higher lean BMI performed better in strenght tests (grip force and broadjump).\n2. **Individuals from cluster 0 have the highest lean BMI while individuals from cluster 1 have the lowest**.\n\nBefore trying to get more detailed information to profile each cluster, let's get the gender distribution for each one.","3eae3172":"<a id=\"7\"><\/a>\n## EVALUATING MODELS\n\nSince this is an unsupervised clustering, we don't have a tagged feature to evaluate or score our model. The purpose of this section is to study the patterns in the clusters formed and determine the nature of the clusters' patterns.","e83fcafc":"<a id=\"1\"><\/a>\n## LIBRARIES AND PARAMETERS"}}