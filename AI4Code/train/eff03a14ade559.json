{"cell_type":{"c22d0c96":"code","47abf6c0":"code","78b18e3a":"code","e3c077e2":"code","d59f7dcc":"code","28da41fc":"code","15b4d304":"code","1e16c9be":"code","8977f10c":"code","c0b082a1":"code","3de8e5f8":"code","26b42532":"code","d8c89414":"code","7ac247e3":"code","1da8aeae":"code","3ca86f1b":"code","e192dbbe":"code","2b33a979":"code","a612749d":"code","1dca7c0c":"code","2c443c56":"code","39c52472":"code","180ed5c4":"code","c60d7da8":"code","1f31d270":"code","3a020cc4":"code","a184aa6f":"code","76c6eeda":"code","71a92b05":"code","cee27f1c":"code","68790b29":"code","b65c8562":"code","2d3e0648":"code","09a220d0":"code","7cbb6f53":"markdown","d5e6497d":"markdown","974e42ae":"markdown","3ffe445b":"markdown","c1795ace":"markdown","eb37bfbc":"markdown","cf06ed02":"markdown","3514a873":"markdown","d2195a85":"markdown","6d7cb64e":"markdown","902cd3f9":"markdown","13d1bc18":"markdown","a045cc24":"markdown","b802bef7":"markdown","189850f4":"markdown","ee602339":"markdown","e5e131da":"markdown","29bf1d2b":"markdown","d57f6441":"markdown","f32c14c1":"markdown"},"source":{"c22d0c96":"# manipulation data\n\nimport pandas as pd\nimport numpy as np\n\n\t\t\t\t#visualiation data\n\n# 1) matplotlib & seaborn \n\nimport matplotlib.pyplot as plt\nimport matplotlib\nimport seaborn as sns \n\n\t\t\t\t#default theme\nsns.set(context='notebook', style='darkgrid', palette='colorblind', font='sans-serif', font_scale=1, rc=None)\nmatplotlib.rcParams['figure.figsize'] =[8,8]\nmatplotlib.rcParams.update({'font.size': 15})\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n","47abf6c0":"df = pd.read_csv('..\/input\/breast-cancer-wisconsin-data\/data.csv')\ndf = df.drop('id',axis=1)\ndf.head()","78b18e3a":"print(df.columns.to_list())","e3c077e2":"df.info()","d59f7dcc":"df=df.drop('Unnamed: 32',axis=1)","28da41fc":"print('We have ',df.shape[0],'Rows & ',df.shape[1],' Columns')","15b4d304":"df.describe(include='all')","1e16c9be":"miss = df.isnull().sum()\nmis_perncent = miss \/df.shape[0]*100\n\ndata = {\n    'missing data':miss,\n    'missing data %':mis_perncent,\n    'data type':df.dtypes\n}\n\nmiss_tab = pd.DataFrame(data)\nmiss_tab","8977f10c":"# get duplicate Rows\ndup = df.duplicated()\ndf[dup]","c0b082a1":"df.hist(figsize=(15,20),edgecolor='black',bins=30)\nplt.show()","3de8e5f8":"sns.countplot(x='diagnosis',data=df)\nplt.show()","26b42532":"y = df.diagnosis                          # M or B \nx = df.drop('diagnosis',axis = 1 )\nx.head()\ndata_dia = y\ndata = x\ndata_n_2 = (data - data.mean()) \/ (data.std())              \ndata = pd.concat([y,data_n_2.iloc[:,0:10]],axis=1)\ndata = pd.melt(data,id_vars=\"diagnosis\",\n                    var_name=\"features\",\n                    value_name='value')","d8c89414":"plt.figure(figsize=(10,10))\nsns.violinplot(x=\"features\", y=\"value\", data=data)\nplt.xticks(rotation=90);","7ac247e3":"plt.figure(figsize=(10,10))\nsns.violinplot(x=\"features\", y=\"value\", hue=\"diagnosis\", data=data)\nplt.xticks(rotation=90);","1da8aeae":"plt.figure(figsize=(10,10))\nsns.boxplot(x=\"features\", y=\"value\", data=data)\nplt.xticks(rotation=90);","3ca86f1b":"plt.figure(figsize=(10,10))\nsns.boxplot(x=\"features\", y=\"value\", hue=\"diagnosis\", data=data)\nplt.xticks(rotation=90);","e192dbbe":"# important features\nfrom sklearn.ensemble import ExtraTreesClassifier\n\nmodel = ExtraTreesClassifier()\nmodel.fit(x,y)\nfeat_imp = pd.Series(model.feature_importances_,index=x.columns)\nfeat_imp.nlargest(15).plot(kind='barh')\nplt.title('the most  important deature are')\nplt.show()","2b33a979":"from sklearn.preprocessing import LabelEncoder\ne = LabelEncoder()\ndf.diagnosis =e.fit_transform(df.diagnosis)\ndf.diagnosis","a612749d":"# split data \nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\nfrom sklearn.metrics import accuracy_score , confusion_matrix","1dca7c0c":"df.columns","2c443c56":"x= df.drop('diagnosis',axis=1)\ny=df.diagnosis\nprint(x.shape,y.shape)","39c52472":"x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.3,random_state=0)","180ed5c4":"from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nx_train = sc.fit_transform(x_train)\nx_test = sc.fit_transform(x_test)","c60d7da8":"## logistic regression\n\nfrom sklearn.linear_model import LogisticRegression\n\nmodel = LogisticRegression()\n# fit model \nmodel.fit(x_train,y_train)\ny_pred = model.predict(x_test)\n\n# accuracy score\nacc_log = accuracy_score(y_test,y_pred)\nprint(acc_log)\n","1f31d270":"## KNN \n\nfrom sklearn.neighbors import KNeighborsClassifier\nmodel = KNeighborsClassifier(n_neighbors=9)\n# fit model \nmodel.fit(x_train,y_train)\ny_pred = model.predict(x_test)\n\n# accuracy score\nacc_knn = accuracy_score(y_test,y_pred)\nprint(acc_knn)","3a020cc4":"list1=[]\nfor neighbors in range(2,30):\n    classifier = KNeighborsClassifier(n_neighbors=neighbors, metric='minkowski')\n    classifier.fit(x_train, y_train)\n    y_pred = classifier.predict(x_test)\n    list1.append(accuracy_score(y_test,y_pred))\nplt.plot(list(range(2,30)), list1)\nplt.show()\n","a184aa6f":"# svm\n\nfrom sklearn.svm import SVC\nmodel = SVC(C=0.8)\n# fit model \nmodel.fit(x_train,y_train)\ny_pred = model.predict(x_test)\n\n# accuracy score\nacc_log = accuracy_score(y_test,y_pred)\nprint(acc_log)","76c6eeda":"list1 = []\nfor c in [0.5,0.6,0.7,0.8,0.9,1.0,1.1,1.2,1.3,1.4,1.5]:\n    classifier = SVC(C = c, random_state=0, kernel = 'rbf')\n    classifier.fit(x_train, y_train)\n    y_pred = classifier.predict(x_test)\n    list1.append(accuracy_score(y_test,y_pred))\nplt.plot([0.5,0.6,0.7,0.8,0.9,1.0,1.1,1.2,1.3,1.4,1.5], list1)\nplt.show()\n","71a92b05":"from sklearn.tree import DecisionTreeClassifier\nclassifier = DecisionTreeClassifier(max_leaf_nodes = 6, random_state=0, criterion='entropy')\nclassifier.fit(x_train, y_train)\n\n# Predicting the test set results\n\ny_pred = classifier.predict(x_test)\n\n# Making the confusion matrix and calculating accuracy score\nacc_decisiontree = accuracy_score(y_test, y_pred)\n\nprint(acc_decisiontree)\n\n","cee27f1c":"list1 = []\nfor leaves in range(2,30):\n    classifier = DecisionTreeClassifier(max_leaf_nodes = leaves, random_state=0, criterion='entropy')\n    classifier.fit(x_train, y_train)\n    y_pred = classifier.predict(x_test)\n    list1.append(accuracy_score(y_test,y_pred))\n#print(mylist)\nplt.plot(list(range(2,30)), list1)\nplt.show()\n","68790b29":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import confusion_matrix, accuracy_score\nlist1 = []\nfor estimators in range(20,30):\n    classifier = RandomForestClassifier(n_estimators = estimators, random_state=0, criterion='entropy')\n    classifier.fit(x_train, y_train)\n    y_pred = classifier.predict(x_test)\n    list1.append(accuracy_score(y_test,y_pred))\n#print(mylist)\nplt.plot(list(range(20,30)), list1)\nplt.show()\n","b65c8562":"from sklearn.ensemble import RandomForestClassifier\nclassifier = RandomForestClassifier(n_estimators = 28, criterion='entropy', random_state=0)\nclassifier.fit(x_train,y_train)\n\n# Predicting the test set results\n\ny_pred = classifier.predict(x_test)\n\n\n# Making the confusion matrix and calculating the accuracy score\n\nfrom sklearn.metrics import confusion_matrix, accuracy_score\n\nacc_randomforest = accuracy_score(y_test, y_pred)\n\nprint(acc_randomforest)\n","2d3e0648":"#Finding the optimum number of\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import confusion_matrix, accuracy_score\nlist1 = []\nfor estimators in range(10,30,1):\n    classifier = XGBClassifier(n_estimators = estimators, max_depth=12, subsample=0.7)\n    classifier.fit(x_train, y_train)\n    y_pred = classifier.predict(x_test)\n    list1.append(accuracy_score(y_test,y_pred))\n#print(mylist)\nplt.plot(list(range(10,30,1)), list1)\nplt.show()\n\nfrom xgboost import XGBClassifier\nclassifier = XGBClassifier(n_estimators = 22, max_depth=12, subsample=0.7)\nclassifier.fit(x_train,y_train)\n\ny_pred = classifier.predict(x_test)\nprint(y_pred)\n\n# Making the confusion matrix and calculating the accuracy score\n\nfrom sklearn.metrics import confusion_matrix, accuracy_score\ncm = confusion_matrix(y_test, y_pred)\nac_xgboost = accuracy_score(y_test, y_pred)\nlist1.append(ac_xgboost)\nprint(cm)\nprint(ac_xgboost)\n","09a220d0":"from catboost import CatBoostClassifier\nclassifier = CatBoostClassifier()\nclassifier.fit(x_train, y_train)\n\ny_pred = classifier.predict(x_test)\n\n\n# Making the confusion matrix and calculating the accuracy score\n\nfrom sklearn.metrics import confusion_matrix, accuracy_score\ncm = confusion_matrix(y_test, y_pred)\nac_catboost = accuracy_score(y_test, y_pred)\nlist1.append(ac_catboost)\n\nprint(ac_catboost)\n","7cbb6f53":"# 2.Read data & data analysis","d5e6497d":"### What can we see at first sight?\n1. target values : \n    * B = benign is the most frequent value with 357\n    * M = malignant 569-357 = 212\n       ","974e42ae":"### as we can see there is no missing value ","3ffe445b":"### a. standardization","c1795ace":"# 1.Importing library , packages and themes","eb37bfbc":"#### PS1: first we gonna read our data then delete the id columns because it is usless","cf06ed02":"## data transformation","3514a873":"### c. boxplot","d2195a85":"### b. violinplot","6d7cb64e":"## A. missing values","902cd3f9":"## A. histogram of data","13d1bc18":"## B. boxplot","a045cc24":"# 5. Data visualization","b802bef7":"# 6. features selection ","189850f4":"Attribute Information:\n\n1. Diagnosis (M = malignant, B = benign)\n\n2-31)\n\nTen real-valued features are computed for each cell nucleus:\n\n    a) radius (mean of distances from center to points on the perimeter)\n    b) texture (standard deviation of gray-scale values)\n    c) perimeter\n    d) area\n    e) smoothness (local variation in radius lengths)\n    f) compactness (perimeter^2 \/ area - 1.0)\n    g) concavity (severity of concave portions of the contour)\n    h) concave points (number of concave portions of the contour)\n    i) symmetry\n    j) fractal dimension (\"coastline approximation\" - 1)\n\nPS:\n\nThe mean, standard error and \"worst\" or largest (mean of the three\nlargest values) of these features were computed for each image,\nresulting in 30 features. For instance, field 3 is Mean Radius, field\n13 is Radius SE, field 23 is Worst Radius.\n\nAll feature values are recoded with four significant digits.\n\nMissing attribute values: none\n\nClass distribution: 357 benign, 212 malignant","ee602339":"#### PS2 : as we can see that the last columns is totaly empty so I gonna drop this columns   ","e5e131da":"# 3. data cleaning ","29bf1d2b":"## B. Duplicate values","d57f6441":"####  Feature scaling","f32c14c1":"### there is no duplicate values"}}