{"cell_type":{"29cf7b0b":"code","54c59ba1":"code","49cc5de7":"code","8b4a8927":"code","3250d5ee":"code","53ab2d05":"code","c0374c63":"code","b58db61a":"code","ed97dfe9":"code","f49a3e05":"code","cad5e33e":"code","cb783eea":"code","e3d1efd9":"code","63585f6a":"code","90c4bf02":"code","f13c03bd":"code","bb04a3d0":"code","e8818448":"code","a2391a9f":"code","b68e9068":"code","8d7ee5df":"code","2afacffb":"code","bd07b5e9":"code","73e03c30":"code","5acde9dd":"markdown","bcd5e1c4":"markdown","01534331":"markdown","18425dfa":"markdown","c3de8968":"markdown","5b7b2977":"markdown","b428f85f":"markdown","cb5df0a8":"markdown","a7f2418b":"markdown","38b9f697":"markdown","ca9c51d4":"markdown","9644eb48":"markdown","28815ff8":"markdown","00f04749":"markdown","6f3b6def":"markdown","05b60766":"markdown","7f3f99a8":"markdown","45ef10c1":"markdown","6611c27a":"markdown","12120ecf":"markdown","41c56bdb":"markdown","fdf03fff":"markdown","6e5f5b9a":"markdown","5263da22":"markdown","dfb91e0d":"markdown","474a039f":"markdown","76012322":"markdown","14bb554a":"markdown"},"source":{"29cf7b0b":"%load_ext autoreload\n%autoreload 2","54c59ba1":"import numpy as np\nimport pandas as pd\nfrom sklearn.metrics import confusion_matrix","49cc5de7":"actuals = np.array([4, 4, 3, 4, 4, 4, 1, 1, 2, 1])\npreds   = np.array([0, 2, 1, 0, 0, 0, 1, 1, 2, 1])","8b4a8927":"actuals.shape","3250d5ee":"O = confusion_matrix(actuals, preds); O","53ab2d05":"confusion_matrix(actuals, preds)","c0374c63":"w = np.zeros((5,5)); w","b58db61a":"for i in range(len(w)):\n    for j in range(len(w)):\n        w[i][j] = float(((i-j)**2)\/16) #as per formula, for this competition, N=5","ed97dfe9":"w","f49a3e05":"N=5\nact_hist=np.zeros([N])\nfor item in actuals: \n    act_hist[item]+=1\n    \npred_hist=np.zeros([N])\nfor item in preds: \n    pred_hist[item]+=1","cad5e33e":"print(f'Actuals value counts:{act_hist}, Prediction value counts:{pred_hist}')","cb783eea":"E = np.outer(act_hist, pred_hist); E","e3d1efd9":"E = E\/E.sum(); E.sum()","63585f6a":"O = O\/O.sum(); O.sum()","90c4bf02":"E","f13c03bd":"O","bb04a3d0":"num=0\nden=0\nfor i in range(len(w)):\n    for j in range(len(w)):\n        num+=w[i][j]*O[i][j]\n        den+=w[i][j]*E[i][j]\n \nweighted_kappa = (1 - (num\/den)); weighted_kappa","e8818448":"# The following 3 functions have been taken from Ben Hamner's github repository\n# https:\/\/github.com\/benhamner\/Metrics\ndef Cmatrix(rater_a, rater_b, min_rating=None, max_rating=None):\n    \"\"\"\n    Returns the confusion matrix between rater's ratings\n    \"\"\"\n    assert(len(rater_a) == len(rater_b))\n    if min_rating is None:\n        min_rating = min(rater_a + rater_b)\n    if max_rating is None:\n        max_rating = max(rater_a + rater_b)\n    num_ratings = int(max_rating - min_rating + 1)\n    conf_mat = [[0 for i in range(num_ratings)]\n                for j in range(num_ratings)]\n    for a, b in zip(rater_a, rater_b):\n        conf_mat[a - min_rating][b - min_rating] += 1\n    return conf_mat\n\n\ndef histogram(ratings, min_rating=None, max_rating=None):\n    \"\"\"\n    Returns the counts of each type of rating that a rater made\n    \"\"\"\n    if min_rating is None:\n        min_rating = min(ratings)\n    if max_rating is None:\n        max_rating = max(ratings)\n    num_ratings = int(max_rating - min_rating + 1)\n    hist_ratings = [0 for x in range(num_ratings)]\n    for r in ratings:\n        hist_ratings[r - min_rating] += 1\n    return hist_ratings\n\n\ndef quadratic_weighted_kappa(y, y_pred):\n    \"\"\"\n    Calculates the quadratic weighted kappa\n    axquadratic_weighted_kappa calculates the quadratic weighted kappa\n    value, which is a measure of inter-rater agreement between two raters\n    that provide discrete numeric ratings.  Potential values range from -1\n    (representing complete disagreement) to 1 (representing complete\n    agreement).  A kappa value of 0 is expected if all agreement is due to\n    chance.\n    quadratic_weighted_kappa(rater_a, rater_b), where rater_a and rater_b\n    each correspond to a list of integer ratings.  These lists must have the\n    same length.\n    The ratings should be integers, and it is assumed that they contain\n    the complete range of possible ratings.\n    quadratic_weighted_kappa(X, min_rating, max_rating), where min_rating\n    is the minimum possible rating, and max_rating is the maximum possible\n    rating\n    \"\"\"\n    rater_a = y\n    rater_b = y_pred\n    min_rating=None\n    max_rating=None\n    rater_a = np.array(rater_a, dtype=int)\n    rater_b = np.array(rater_b, dtype=int)\n    assert(len(rater_a) == len(rater_b))\n    if min_rating is None:\n        min_rating = min(min(rater_a), min(rater_b))\n    if max_rating is None:\n        max_rating = max(max(rater_a), max(rater_b))\n    conf_mat = Cmatrix(rater_a, rater_b,\n                                min_rating, max_rating)\n    num_ratings = len(conf_mat)\n    num_scored_items = float(len(rater_a))\n\n    hist_rater_a = histogram(rater_a, min_rating, max_rating)\n    hist_rater_b = histogram(rater_b, min_rating, max_rating)\n\n    numerator = 0.0\n    denominator = 0.0\n\n    for i in range(num_ratings):\n        for j in range(num_ratings):\n            expected_count = (hist_rater_a[i] * hist_rater_b[j]\n                              \/ num_scored_items)\n            d = pow(i - j, 2.0) \/ pow(num_ratings - 1, 2.0)\n            numerator += d * conf_mat[i][j] \/ num_scored_items\n            denominator += d * expected_count \/ num_scored_items\n\n    return (1.0 - numerator \/ denominator)","a2391a9f":"quadratic_weighted_kappa(actuals, preds)","b68e9068":"def quadratic_kappa(actuals, preds, N=5):\n    \"\"\"This function calculates the Quadratic Kappa Metric used for Evaluation in the PetFinder competition\n    at Kaggle. It returns the Quadratic Weighted Kappa metric score between the actual and the predicted values \n    of adoption rating.\"\"\"\n    w = np.zeros((N,N))\n    O = confusion_matrix(actuals, preds)\n    for i in range(len(w)): \n        for j in range(len(w)):\n            w[i][j] = float(((i-j)**2)\/(N-1)**2)\n    \n    act_hist=np.zeros([N])\n    for item in actuals: \n        act_hist[item]+=1\n    \n    pred_hist=np.zeros([N])\n    for item in preds: \n        pred_hist[item]+=1\n                         \n    E = np.outer(act_hist, pred_hist);\n    E = E\/E.sum();\n    O = O\/O.sum();\n    \n    num=0\n    den=0\n    for i in range(len(w)):\n        for j in range(len(w)):\n            num+=w[i][j]*O[i][j]\n            den+=w[i][j]*E[i][j]\n    return (1 - (num\/den))","8d7ee5df":"actuals","2afacffb":"preds","bd07b5e9":"quadratic_kappa(actuals, preds)","73e03c30":"actuals = np.array([4, 4, 3, 4, 4, 4, 1, 1, 2, 0])\npreds   = np.array([4, 4, 3, 4, 4, 4, 1, 1, 2, 0])\nquadratic_kappa(actuals, preds)","5acde9dd":"A weighted Kappa is a metric which is used to calculate the amount of similarity between predictions and actuals. A perfect score of `1.0` is granted when both the predictions and actuals are the same. <br>\nWhereas, the least possible score is `-1` which is given when the predictions are furthest away from actuals. In our case, consider all actuals were 0's and all predictions were 4's. This would lead to a `QWKP` score of `-1`.<br>\nThe aim is to get as close to 1 as possible. Generally a score of 0.6+ is considered to be a really good score. ","bcd5e1c4":"### Interpreting the Quadratic Weighted Kappa Metric ","01534331":"## Rewrite the Quadratic Kappa Metric function","18425dfa":"### Step-4: Expected Value (Outer product of histograms) ","c3de8968":"## Create our own Quadratic Weighted Kappa Metric","5b7b2977":"Submissions are scored based on the quadratic weighted kappa, which measures the agreement between two ratings. This metric typically varies from 0 (random agreement between raters) to 1 (complete agreement between raters). In the event that there is less agreement between the raters than expected by chance, the metric may go below 0. The quadratic weighted kappa is calculated between the scores which are expected\/known and the predicted scores. <br>\n\n\nResults have 5 possible ratings, 0,1,2,3,4.  The quadratic weighted kappa is calculated as follows. First, an N x N histogram matrix O is constructed, such that Oi,j corresponds to the number of adoption records that have a rating of i (actual) and received a predicted rating j. An N-by-N matrix of weights, w, is calculated based on the difference between actual and predicted rating scores.\n\nAn N-by-N histogram matrix of expected ratings, E, is calculated, assuming that there is no correlation between rating scores.  This is calculated as the outer product between the actual rating's histogram vector of ratings and the predicted rating's histogram vector of ratings, normalized such that E and O have the same sum.\n\nFrom these three matrices, the quadratic weighted kappa is calculated.","b428f85f":"The following code to calculate the Weighted Kappa Metric was used by Abhishek in his kernel https:\/\/www.kaggle.com\/abhishek\/maybe-something-interesting-here. ","cb5df0a8":"An N-by-N matrix of weights, w, is calculated based on the difference between actual and predicted rating scores.","a7f2418b":"For the purpose of explaination, we will assume are actual and preds vectors to be the following. ","38b9f697":"## What is Quadratic Weighted Kappa?  ","ca9c51d4":"- First, create a multi class confusion matrix `O` between predicted and actual ratings. \n- Second, construct a weight matrix `w` which calculates the weight between the actual and predicted ratings. \n- Third, calculate `value_counts()` for each rating in preds and actuals. \n- Fourth, calculate `E`, which is the outer product of two value_count vectors \n- Fifth, normalise the `E` and `O` matrix\n- Caclulate, weighted kappa as per formula","9644eb48":"#### Each Step Explained","28815ff8":"### Step-1: Confusion Matrix","00f04749":"Therefore, we have 3 values with adoption rating 1, 1 value with adoption rating 2, 1 value with adoption rating 1 an 5 values with adoption rating of 5 in the actuals. ","6f3b6def":"### Step-5: Normalise E and O matrix","05b60766":"### Compare Result with Existing Metric","7f3f99a8":"### Step-2: Weighted Matrix","45ef10c1":"**Edit:** Quadratic Kappa Metric is the same as cohen kappa metric in Sci-kit learn @ sklearn.metrics.cohen_kappa_score when weights are set to 'Quadratic'. Thanks to Johannes for figuring that out. ","6611c27a":"`E` and `O` are normalized such that E and O have the same sum.","12120ecf":"### Breaking down the formula into parts","41c56bdb":"#### 5 step breakdown for Weighted Kappa Metric","fdf03fff":"Note that all values lying on the diagonal are penalised the least with a penalty of 0, whereas predictions and actuals furthest away from each other are penalised the most.","6e5f5b9a":"Expected matrix is calculated as the outer product between the actual rating's histogram vector of ratings and the predicted rating's histogram vector of ratings","5263da22":"**What if both actuals and predictions match 100%?**","dfb91e0d":"### Step-3: Histogram","474a039f":"### Step-6: Calculate Weighted Kappa","76012322":"Our result matches the existic quadratic weighted kappa metric. ","14bb554a":"**Step-1:** Under Step-1, we shall be calculating a `confusion_matrix` between the Predicted and Actual values. <a href=\"https:\/\/www.dataschool.io\/simple-guide-to-confusion-matrix-terminology\/\">Here<\/a> is a great resource to know more about `confusion_matrix`. <br>\n**Step-2:** Under Step-2, under step-2 each element is weighted. Predictions that are further away from actuals are marked harshly than predictions that are closer to actuals. We will have a less score if our prediction is 5 and actual is 3 as compared to a prediction of 4 in the same case. <br>\n**Step-3:** We create two vectors, one for preds and one for actuals, which tells us how many values of each rating exist in both vectors. <br>\n**Step-4:**`E` is the Expected Matrix which is the outer product of the two vectors calculated in step-3.<br>\n**Step-5:** Normalise both matrices to have same sum. Since, it is easiest to get sum to be '1', we will simply divide each matrix by it's sum to normalise the data. <br>\n**Step-6:** Calculated numerator and denominator of Weighted Kappa and return the Weighted Kappa metric as `1-(num\/den)`"}}