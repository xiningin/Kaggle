{"cell_type":{"ff231205":"code","f1f5724d":"code","ece66e53":"code","b210033a":"code","a0be1422":"code","02afc0d6":"code","96e92eb9":"code","154d140b":"code","70cd4fe4":"code","0d5f4039":"markdown"},"source":{"ff231205":"import torch\nimport torch.nn as nn # All neural network modules, nn.Linear, nn.Conv2d, BatchNorm, Loss functions\nfrom torchvision import datasets, models # dataset from torchvision \nimport matplotlib.pyplot as plt # for visualization\nimport torch.optim as optim # For all optimizer like SGD, Adam etc\nfrom torch.utils.data import DataLoader # Gives easier dataset managment and creates mini batches\nimport torchvision.transforms as transforms  # Transformations we can perform on our dataset\nfrom torchvision.transforms import ToTensor, Compose, Lambda, Normalize\nfrom torch.utils.tensorboard import SummaryWriter  # to print to tensorboard","f1f5724d":"# set device \ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","ece66e53":"# Download training data from open datasets.\ntraining_data = datasets.FashionMNIST(\n    root=\"data\",\n    train=True,\n    download=True,\n    transform=Compose([ToTensor(), Normalize((0.5,), (0.5,))])\n)\n\n# Download test data from open datasets.\ntest_data = datasets.FashionMNIST(\n    root=\"data\",\n    train=False,\n    download=True,\n    transform=Compose([ToTensor(), Normalize((0.5,), (0.5,))])\n)","b210033a":"# check training Data\ntraining_data","a0be1422":"batch_size = 64\n\n# create dataloader \n\ntrain_loader = DataLoader(\n    training_data, batch_size, shuffle=True\n)\n\n# test loader\ntest_loader = DataLoader(\n    test_data, batch_size\n)\n\nfor X, y in test_loader:\n    print(f\"Shape of X [Batch_size, Channel, Hight, Width] >> {X.shape}\")\n    print(f\"Shape of y >> {y.shape}\")\n    break","02afc0d6":"# Model Building\n\nclass NeuralNetwork(nn.Module):\n    def __init__(self):\n        super(NeuralNetwork, self).__init__()\n        self.flatten = nn.Flatten()\n        self.linear_relu_stack = nn.Sequential(\n            nn.Linear(28*28, 512),\n            nn.ReLU(),\n            nn.Linear(512, 512),\n            nn.ReLU(),\n            nn.Linear(512, 10)\n        )\n\n    def forward(self, x):\n        x = self.flatten(x)\n        x = self.linear_relu_stack(x)\n        return x","96e92eb9":"model = NeuralNetwork().to(device)\nmodel","154d140b":"# loss function and optimizer\n\nLR = 1e-3\n\nloss_fn = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=LR)\n\n# Define Scheduler\nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n    optimizer, factor=0.1, patience=5, verbose=True\n)","70cd4fe4":"from tqdm import tqdm\n\n\ndef train_fn(loader, model, optimizer, loss_fn, device):\n    model.train()\n    train_acc, correct_train, losses, target_count, accuracies, running_loss = 0, 0, [], 0, [], 0\n    loop = tqdm(loader)\n    for batch_idx, (data, target) in enumerate(loop):\n        data = data.to(device)\n        target = target.to(device)\n        \n        # forward\n        scores = model(data)\n        loss = loss_fn(scores, target)\n        \n        # backward\n        optimizer.zero_grad()\n        loss.backward()\n        \n        # optimizer step\n        optimizer.step()\n        \n        \n        _, predicted = torch.max(scores.data, 1)\n        target_count += target.size(0)\n        correct_train += (target == predicted).sum().item()\n        train_acc = (correct_train) \/ target_count\n        running_loss =+ loss.item() * data.size(0)\n\n        losses.append(running_loss \/ len(loader))\n        accuracies.append(train_acc)\n        \n        loop.set_postfix(loss=loss.data.item(), accuracy=train_acc)\n        \n        scheduler.step(loss)\n        \n    return losses, accuracies\n\n\ndef test_fn(loader, model, loss_fn):\n    '''\n    model.eval() is a kind of switch for some specific layers\/parts of the model \n    that behave differently during training and inference (evaluating) time. \n    For example, Dropouts Layers, BatchNorm Layers etc. \n    You need to turn off them during model evaluation, and .eval() will do it for you. \n    In addition, the common practice for evaluating\/validation is using torch.no_grad() in pair with model.eval() \n    to turn off gradients computation\n    '''\n    size = len(loader.dataset)\n    num_batches = len(loader)\n    model.eval()\n    num_correct = 0\n    test_loss = 0\n    \n    '''\n    Torch.no_grad() deactivates autograd engine. Eventually it will reduce the memory usage and speed up computations.\n    Use of Torch.no_grad():\n        To perform inference without Gradient Calculation.\n        To make sure there's no leak test data into the model.\n    '''\n    \n    with torch.no_grad():\n        for x, y in loader:\n            x = x.to(device)\n            y = y.to(device)\n            pred = model(x)\n            test_loss += loss_fn(pred, y).item()\n            num_correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n    test_loss \/= num_batches\n    num_correct \/= size\n    \n    print(f\"Test Loss: {test_loss} Test Accuracy : {num_correct}\")\n\n    \n    \nif __name__ == \"__main__\":\n    for epoch in range(10):\n        # train function return losses and accuracies\n        loss, acc = train_fn(train_loader, model, optimizer, loss_fn, device)\n        \n        # test function for testing\n        test_fn(test_loader, model, loss_fn)\n    plt.figure(figsize=(10, 6))\n    plt.plot(loss, \"-r\")\n    plt.plot(acc, \"-b\")\n    plt.show()","0d5f4039":"# Fashion MNIST : Quickstart Pytorch Basics"}}