{"cell_type":{"a624bce2":"code","48d7eec6":"code","acf65cf7":"code","21db9687":"code","557048f5":"code","93306a55":"code","a84be9e3":"code","c8e64a7b":"code","50b089b5":"code","8e1ab22c":"code","a8f5abe1":"code","cf6ee5ba":"code","7fd978e8":"code","16ff1e48":"code","2bde4356":"code","fbe66fc8":"code","1713d116":"code","d2c6e362":"code","e1bc80a5":"code","4c295a26":"code","f57269a5":"code","49aa9c09":"code","f2fabe51":"code","e7892629":"code","7fdce85c":"code","f3bc65b8":"code","3e3f84f1":"code","8bf2201a":"code","cb0c30cb":"code","0280b6ba":"code","200a72d7":"code","582211af":"code","5b64d647":"code","bbc19401":"code","e7b60bf8":"code","9e400fe9":"code","f2b35989":"code","7fdb0beb":"code","36c7f16d":"code","c22b7e17":"code","27b2d183":"code","5210e297":"code","ecae1be9":"code","65782b08":"code","85db500b":"code","909385d1":"code","c56f54e7":"code","932d830a":"code","3772a988":"code","0ffde2ee":"code","380a6c82":"code","b302e0c8":"code","6a26d6be":"code","7e53e8f1":"code","c5f8f67f":"code","b6e401c9":"code","00312176":"code","87fc54ab":"code","4130869a":"code","e3e3c6b5":"code","787c6242":"code","481981b3":"code","f058fad7":"code","e4c26c78":"code","21e7a854":"code","a2dd0967":"code","30a88c86":"code","0364ac37":"code","703274c6":"code","a0e586c6":"code","97ca0153":"code","e54430e8":"code","0de1caa7":"code","22ae6406":"code","2176ff96":"markdown","5d424daf":"markdown","a53d5728":"markdown","cb345f0e":"markdown","94fa814a":"markdown","7d7c027b":"markdown","ae6f892a":"markdown"},"source":{"a624bce2":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","48d7eec6":"\n# from bs4 import BeautifulSoup\n# import csv  \n# import requests  \n# import re\n# import time\n# import pandas as pd\n# import numpy as np\n# import glob\n\n# # \u0444\u0443\u043d\u043a\u0446\u0438\u044f \u0437\u0430\u043f\u0438\u0441\u044b\u0432\u0430\u0435\u0442 \u0432 \u0444\u0430\u0439\u043b url \u0441 \u043f\u0435\u0440\u0432\u044b\u0445 100 \u0441\u0442\u0440\u0430\u043d\u0438\u0446 \u043f\u043e \u0433\u043e\u0434\u0430\u043c \n# def make_car_links(year):\n#     car_links = []\n#     url = f'https:\/\/auto.ru\/cars\/{year}-year\/used\/?page='\n#     f = open(f'car_links_{year}.txt', 'w')\n#     for i in range(1,100):  # pages \n#         url_i = url + str(i)\n#         response = requests.get(url_i)\n#         response.encoding ='utf8'\n#         page = BeautifulSoup(response.text, 'html.parser')\n#         links = page.find_all('a', class_='Link ListingItemTitle-module__link')\n#         for link in links:\n#             f.write(link.get(\"href\")+'\\n')\n#     f.close()\n    \n\n# for i in range(2005,2021):\n#     make_car_links(i)\n\n# # \u0444\u0443\u043d\u043a\u0446\u0438\u044f \u043f\u043e url \u0432\u043e\u0437\u0432\u0440\u0430\u0449\u0430\u0435\u0442 \u0434\u0430\u043d\u043d\u044b\u0435 \u043e\u0431 \u043e\u0431\u044c\u044f\u0432\u043b\u0435\u043d\u0438\u0439     \n# def car_info(url):\n#     response = requests.get(url)    \n#     response.encoding ='utf8' \n#     page = BeautifulSoup(response.text, 'html.parser')\n#     brand = url.split('\/')[6]\n#     model = url.split('\/')[7]\n#     year = page.find('li', class_= 'CardInfoRow CardInfoRow_year').find('a','Link Link_color_black').text\n#     mileage =  page.find('li', class_= 'CardInfoRow CardInfoRow_kmAge').find_all('span','CardInfoRow__cell')[1].text.replace(u'\\xa0', u'')\n#     body_type = page.find('li', class_= 'CardInfoRow CardInfoRow_bodytype').find('a','Link Link_color_black').text\n#     color = page.find('li', class_= 'CardInfoRow CardInfoRow_color').find_all('span','CardInfoRow__cell')[1].text\n#     engine_volume = page.find('li', class_= 'CardInfoRow CardInfoRow_engine').find('div').text.replace(u'\\xa0', u'').split('\/')[0].strip()\n#     horsepower = page.find('li', class_= 'CardInfoRow CardInfoRow_engine').find('div').text.replace(u'\\xa0', u'').split('\/')[1].strip()\n#     flue_type = page.find('li', class_= 'CardInfoRow CardInfoRow_engine').find('div').text.replace(u'\\xa0', u'').split('\/')[2].strip()\n#     tax =  page.find('li', class_= 'CardInfoRow CardInfoRow_transportTax').find_all('span','CardInfoRow__cell')[1].text.replace(u'\\xa0', u'')\n#     transmission =  page.find('li', class_= 'CardInfoRow CardInfoRow_transmission').find_all('span','CardInfoRow__cell')[1].text\n#     drive = page.find('li', class_= 'CardInfoRow CardInfoRow_drive').find_all('span','CardInfoRow__cell')[1].text\n#     whell = page.find('li', class_= 'CardInfoRow CardInfoRow_wheel').find_all('span','CardInfoRow__cell')[1].text\n#     state = page.find('li', class_= 'CardInfoRow CardInfoRow_state').find_all('span','CardInfoRow__cell')[1].text\n#     owners = page.find('li', class_= 'CardInfoRow CardInfoRow_ownersCount').find_all('span','CardInfoRow__cell')[1].text[0]\n#     pts = page.find('li', class_= 'CardInfoRow CardInfoRow_pts').find_all('span','CardInfoRow__cell')[1].text\n#     customs = page.find('li', class_= 'CardInfoRow CardInfoRow_customs').find_all('span','CardInfoRow__cell')[1].text\n#     price = page.find('span', class_='OfferPriceCaption__price').text.replace(u'\\xa0', u'')\n#     return url, brand, model, year, mileage, body_type, color, engine_volume, horsepower, flue_type, tax, transmission, drive, whell, state, owners, pts, customs, price\n\n\n# # \u0434\u043b\u044f \u043a\u0430\u0436\u0434\u043e\u0439 url \u0438\u0437 txt \u0444\u0430\u0439\u043b\u0430 \u0432\u044b\u0437\u044b\u0432\u0430\u0435\u0442\u0441\u044f car_info \u0438 \n# # \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442 \u0435\u0441\u043b\u0438 \u043c\u0430\u0448\u0438\u043d\u0430 \u043d\u0435 \u043f\u0440\u043e\u0434\u0430\u043d\u043d\u0430 \u0437\u0430\u043f\u0438\u0441\u044b\u0432\u0430\u0435\u0442\u0441\u044f \u0432 csv \u0444\u0430\u0439\u043b \n# read_files = glob.glob(\"*.txt\")\n# for file in read_files:\n#     f = open(file)\n#     out_f = open(f'{file[-8:-4]}.csv', 'w')\n#     with out_f:\n#         writer = csv.writer(out_f)\n#         for line in f:\n#             try:\n#                 writer.writerow(car_info(line[:-1]))\n#             except:\n#                 print(line)\n\n# def make_car_links(model):\n#   url = f'https:\/\/auto.ru\/cars\/{model}\/used\/?page='\n#   f = open(f'car_links_{model}.txt', 'w')\n#   for i in range(1,100):  # pages \n#     url_i = url + str(i)\n#     response = requests.get(url_i)\n#     response.encoding ='utf8'\n#     page = BeautifulSoup(response.text, 'html.parser')\n#     links = page.find_all('a', class_='Link ListingItemTitle-module__link')\n#     for link in links:\n#       f.write(link.get(\"href\")+'\\n')\n#   f.close()\n  \n# models = ['AUDI', 'BMW', 'HONDA', 'INFINITI', 'LEXUS', 'MERCEDES', 'MITSUBISHI', 'NISSAN', 'SKODA', 'TOYOTA', 'VOLKSWAGEN', 'VOLVO']\n# for model in models:\n#     make_car_links(str.lower(model))\n\n","acf65cf7":"import pandas as pd\nimport numpy as np\nimport datetime\ntrain  = pd.read_csv('\/kaggle\/input\/train\/train_data.csv')\nsome = pd.read_csv('\/kaggle\/input\/some-data\/some_data.csv')\ntest = pd.read_csv('\/kaggle\/input\/sf-dst-car-price-prediction\/test.csv')\nsample_submission = pd.read_csv('\/kaggle\/input\/sf-dst-car-price-prediction\/sample_submission.csv')\nsome1 = pd.read_csv('\/kaggle\/input\/avtoru20\/data_data.csv')\ndat = pd.read_csv('\/kaggle\/input\/addition-data\/vehicles.csv')\nmoscow = pd.read_csv('\/kaggle\/input\/moscow-1\/train.csv')\nnovember =  pd.read_csv('\/kaggle\/input\/traindataset\/all_auto_ru_14_11_2020.csv')\nVERSION = 15","21db9687":"train = pd.concat([train, some, some1, moscow], ignore_index=True)\ntrain = train.drop_duplicates().dropna()","557048f5":"test","93306a55":"# br=train.brand.unique()\n# model_name = train.model_name.unique()\n# dat= dat[dat['mpgData']=='Y']\n# dat['make'] = dat['make'].str.lower()\n# br_mpg1 = dat.make.unique()\n# for i in range(len(br_mpg1)):   \n#     if br_mpg1[i] not in br:\n#         dat = dat.drop(dat.index[(dat['make'] == br_mpg1[i])]) \n\n# dat['model'] = dat['model'].str.lower()\n# model_mpg1 = dat.model.unique()\n# for i in range(len(model_mpg1)):\n#     if not any(model_mpg1[i]  in s for s in model_name):\n#         dat = dat.drop(dat.index[(dat['model'] == model_mpg1[i])])\n# dat.model.value_counts()\n# col = ['make','model','year','highway08','city08','comb08','displ']\n# dat = dat.reindex(columns=col)\n\n# train['citympg'] = np.full((train.shape[0],1),np.mean(dat['city08']))\n# train['combmpg'] = np.full((train.shape[0],1),np.mean(dat['comb08']))\n# train['highwaympg'] = np.full((train.shape[0],1),np.mean(dat['highway08']))\n# #np.full((1), [21]), np.full((1), [24]), np.full((1), [27])\n# #def citypmg():\n# #pd.set_option('mode.chained_assignment', None)\n# for i in train.model_name.unique():\n#     for j in dat.model.unique():\n#         if j in i:\n#             #train.loc[train.model_name==i][['citympg','combmpg','highmpg']] =  dat.loc[dat.model==j][['citympg','combmpg','highmpg']]\n#             #train.loc[train.model_name==i][('citympg','combmpg','highmpg')] =  dat.loc[dat.model==j][('city08','comb08','highway08')]\n#             val = np.mean( dat.loc[dat.model==j]['city08'])\n#             val1=np.mean( dat.loc[dat.model==j]['comb08'])\n#             val2=np.mean( dat.loc[dat.model==j]['highway08'])\n#             for k in train[train.model_name==i].index:\n#                 train.at[k, 'citympg'] = val \n#                 train.at[k, 'combmpg'] = val1\n#                 train.at[k, 'highwaympg'] = val2\n                \n\n\n# test['citympg'] = np.full((test.shape[0],1),np.mean(dat['city08']))\n# test['combmpg'] = np.full((test.shape[0],1),np.mean(dat['comb08']))\n# test['highwaympg'] = np.full((test.shape[0],1),np.mean(dat['highway08']))\n# for i in test.model_name.unique():\n#     for j in dat.model.unique():\n#         if j in i:\n#             val = np.mean( dat.loc[dat.model==j]['city08'])\n#             val1=np.mean( dat.loc[dat.model==j]['comb08'])\n#             val2=np.mean( dat.loc[dat.model==j]['highway08'])\n#             for k in test[test.model_name==i].index:\n#                 test.at[k, 'citympg'] = val \n#                 test.at[k, 'combmpg'] = val1\n#                 test.at[k, 'highwaympg'] = val2","a84be9e3":"# \u041f\u0440\u0435\u0434\u043e\u0431\u0440\u0430\u0431\u043e\u0442\u043a\u0430 \u043e\u0431\u0443\u0447\u0430\u044e\u0449\u0435\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0438\ndata = train.copy()\ndata.mileage = data.mileage.apply(lambda x: x[:-2])\ndata.bodyType = data.bodyType.apply(lambda x: x+' 4')\ndata.rename(columns={'modelDate': 'productionDate'}, inplace=True)\ndata['numberOfDoors'] = data.bodyType.apply(lambda x: x.split()[1])\ndata.bodyType = data.bodyType.apply(lambda x: x.split()[0])\ndata.engineDisplacement = data.engineDisplacement.apply(lambda x: x.split()[0])\ndata.enginePower = data.enginePower.apply(lambda x: x[:-4])\ndata.fuelType = data.fuelType.apply(lambda x: x.split(',')[0])\ndata.price = data.price.apply(lambda x: x[:-1])\ntrain_y = data['price']\ndata = data.drop('price', axis=1)\n# \u041a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u0434\u0432\u0435\u0440\u0435\u0439\nfor i in data[data.numberOfDoors=='\u043e\u0442\u043a\u0440\u044b\u0442\u044b\u0439'].index:\n    data.at[i, 'numberOfDoors']=3\n\nfor feature in ['mileage', 'enginePower', 'numberOfDoors']:\n    data[feature]=data[feature].astype('int32')","c8e64a7b":"train.info()","50b089b5":"train_y = train_y.astype(str).astype(int)","8e1ab22c":"# \u041f\u0440\u0435\u0434\u043e\u0431\u0440\u0430\u0431\u043e\u0442\u043a\u0430 \u0442\u0435\u0441\u0442\u043e\u0432\u043e\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0438\ntest_x = test.copy()\ntest_y = sample_submission.copy()\n# \u0412\u044b\u043a\u0438\u0434\u044b\u0432\u0430\u0435\u043c \u043b\u0438\u0448\u043d\u0438\u0435 \u0441\u0442\u043e\u043b\u0431\u0446\u044b\ntest_x = test_x.drop('vendor', axis=1)\ntest_x.rename(columns={'\u041f\u0440\u0438\u0432\u043e\u0434':'drivegear', '\u0420\u0443\u043b\u044c':'escl', '\u0421\u043e\u0441\u0442\u043e\u044f\u043d\u0438\u0435':'state', '\u0412\u043b\u0430\u0434\u0435\u043b\u044c\u0446\u044b':'owners', '\u041f\u0422\u0421':'vendor', '\u0422\u0430\u043c\u043e\u0436\u043d\u044f':'customs', '\u0412\u043b\u0430\u0434\u0435\u043d\u0438\u0435':'ownership'}, inplace=True)\ntest_x = test_x.drop(['vehicleConfiguration', 'description',  'ownership', 'car_url', 'image', 'parsing_unixtime', 'complectation_dict', 'sell_id', 'model_info', 'equipment_dict', 'name', 'priceCurrency', 'super_gen', 'state', 'customs','modelDate'], axis=1)\ntest_x.at[10412,'vendor']='\u041e\u0440\u0438\u0433\u0438\u043d\u0430\u043b'\ntest_x.at[16944, 'numberOfDoors']=2\ntest_x.bodyType = test_x.bodyType.apply(lambda x: x.split()[0])\ntest_x.brand = test_x.brand.apply(lambda x: x.lower())\ntest_x.model_name = test_x.model_name.apply(lambda x: x.lower())\ntest_x.engineDisplacement = test_x.engineDisplacement.apply(lambda x: x.split()[0])\ntest_x.enginePower = test_x.enginePower.apply(lambda x: x.split()[0])\ntest_x.owners = test_x.owners.apply(lambda x: x.split()[0])\nfor feature in ['enginePower', 'owners']:\n        test_x[feature]=test_x[feature].astype('int32')\ntrain_and_test = pd.concat([data, test_x])","a8f5abe1":"# data_1 = pd.DataFrame(columns = train.columns)\n# for i in test.brand.apply(lambda x: x.lower()).unique():\n#     data_1 = pd.concat([data_1, train[train.brand==i]])\n    \n# data = pd.concat([train_brand,data_1])\n# data = train.drop_duplicates()","cf6ee5ba":"data.brand.hist(bins = 90,xrot = 90)","7fd978e8":"test.brand.hist(bins= 12,xrot = 90)","16ff1e48":"test.brand.unique()","2bde4356":"data.fuelType.hist(xrot=90)","fbe66fc8":"test.fuelType.hist()","1713d116":"# data.mileage.hist()","d2c6e362":"test.mileage.hist()","e1bc80a5":"train.modelDate.hist(bins =15)","4c295a26":"test.modelDate.hist(bins = 25)","f57269a5":"train.vehicleTransmission.sort_values().hist()","49aa9c09":"test.vehicleTransmission.sort_values().hist()","f2fabe51":"test.color.sort_values().hist(xrot=90)","e7892629":"train.color.sort_values().hist(xrot=90)","7fdce85c":"test[test.brand=='BMW'].model_name.sort_values().hist(xrot=90,xlabelsize = 8)","f3bc65b8":"data[data.brand=='bmw'].model_name.sort_values().hist(xrot=90)","3e3f84f1":"len(np.unique(train[train.brand=='bmw'].model_name))","8bf2201a":"len(np.unique(test[test.brand=='BMW'].model_name))","cb0c30cb":"len(np.unique(train[train.brand=='honda'].model_name))","0280b6ba":"len(np.unique(test[test.brand=='HONDA'].model_name))","200a72d7":"test.bodyType.sort_values().hist(xrot=90)","582211af":"data.bodyType.sort_values().hist(xrot=90)","5b64d647":"data.engineDisplacement.sort_values().hist(xrot=90)","bbc19401":"test.engineDisplacement.sort_values().hist(xrot=90)","e7b60bf8":"# \u041c\u0430\u0440\u043a\u0430\nbrand_to_num = {cl: num for num, cl in enumerate(np.unique(train_and_test['brand']))}\n# \u041d\u0430\u0437\u0432\u0430\u043d\u0438\u0435\nnames_with_freqs = list(train_and_test.model_name.value_counts())\ntop_model_names_count = int(np.percentile(names_with_freqs,95))\nall_models = train_and_test.model_name.value_counts().index\ntop_models = list(all_models)[:top_model_names_count]\nmodels_to_throw_away = list(set(all_models)- set(top_models))\ntrain_and_test.loc[train_and_test['model_name'].isin(models_to_throw_away), 'model_name'] = 'other'\nmodel_name_to_num = {cl: num for num, cl in enumerate(np.unique(train_and_test['model_name']))}\n#\u0422\u0438\u043f \u043a\u0443\u0437\u043e\u0432\u0430\nbodyType_to_num = {cl: num for num, cl in enumerate(np.unique(train_and_test['bodyType']))}\n#\u0426\u0432\u0435\u0442\ncolor_to_num = {cl: num for num, cl in enumerate(np.unique(train_and_test['color']))}\n# \u0422\u0438\u043f \u0442\u0440\u0430\u043d\u0441\u043c\u0438\u0441\u0441\u0438\u0438\nvehicleTransmission_to_num = {cl: num for num, cl in enumerate(np.unique(train_and_test['vehicleTransmission']))}\n# \u0420\u0443\u043b\u044c\nescl_to_num = {cl: num for num, cl in enumerate(np.unique(train_and_test['escl']))}\n# \u041f\u0440\u043e\u0438\u0437\u0432\u043e\u0434\u0441\u0442\u0432\u043e\nvendor_to_num = {cl: num for num, cl in enumerate(np.unique(train_and_test['vendor']))}","9e400fe9":"# \u041c\u0430\u0440\u043a\u0430\ndata.brand = np.array(data['brand'].apply(lambda cl: brand_to_num[cl]))\n# \u041d\u0430\u0437\u0432\u0430\u043d\u0438\u0435\ndata.loc[data['model_name'].isin(models_to_throw_away), 'model_name'] = 'other'\ndata.model_name = np.array(data['model_name'].apply(lambda cl:model_name_to_num[cl]))\n# \u0414\u0430\u0442\u0430 \u043c\u043e\u0434\u0435\u043b\u0438\nyear = datetime.datetime.now().year\ndata.productionDate = data.productionDate.apply(lambda x: round(year-int(x)))\n#\u0422\u0438\u043f \u043a\u0443\u0437\u043e\u0432\u0430\ndata.bodyType = data.bodyType.apply(lambda x: x.split()[0])\ndata.bodyType = np.array(data['bodyType'].apply(lambda cl: bodyType_to_num[cl]))\n#\u0426\u0432\u0435\u0442\ndata.color = np.array(data['color'].apply(lambda cl: color_to_num[cl]))\n#\u0422\u043e\u043f\u043b\u0438\u0432\u043e\ndata = data.merge(pd.get_dummies(data.fuelType), left_index=True, right_index=True)\ndef gibrid_bensin(row):\n    if row.gibrid==1:\n        return 1\n    else:\n        return row.bensin\ndef gibrid_electro(row):\n    if row.gibrid==1:\n        return 1\n    else:\n        return row.electro\ndata.rename(columns={'\u0411\u0435\u043d\u0437\u0438\u043d':'bensin', '\u0413\u0430\u0437':'gas', '\u0413\u0438\u0431\u0440\u0438\u0434':'gibrid', '\u0414\u0438\u0437\u0435\u043b\u044c':'disel'}, inplace=True)\ndata['electro'] = 0\ndata.bensin = data.apply(lambda row:gibrid_bensin(row), axis=1)\ndata.electro = data.apply(lambda row:gibrid_electro(row), axis=1)\ndata = data.drop(['fuelType', 'gibrid'], axis=1)\n# \u0422\u0438\u043f \u0442\u0440\u0430\u043d\u0441\u043c\u0438\u0441\u0441\u0438\u0438\ndata.vehicleTransmission = np.array(data['vehicleTransmission'].apply(lambda cl:vehicleTransmission_to_num[cl]))\n#\u041f\u0440\u0438\u0432\u043e\u0434\ndata = data.merge(pd.get_dummies(data.drivegear), left_index=True, right_index=True)\ndef rear_gear(row):\n    if row.fullgear==1:\n        return 1\n    else:\n        return row.reargear\ndef front_gear(row):\n    if row.fullgear==1:\n        return 1\n    else:\n        return row.frontgear\ndata.rename(columns={'\u043f\u0435\u0440\u0435\u0434\u043d\u0438\u0439':'frontgear', '\u0437\u0430\u0434\u043d\u0438\u0439':'reargear','\u043f\u043e\u043b\u043d\u044b\u0439':'fullgear'}, inplace=True)\ndata.reargear = data.apply(lambda row:rear_gear(row), axis=1)\ndata.frontgear = data.apply(lambda row:front_gear(row), axis=1)\ndata = data.drop(['drivegear', 'fullgear'], axis=1)\n# \u0420\u0443\u043b\u044c\ndata.escl = np.array(data['escl'].apply(lambda cl:escl_to_num[cl]))\n# \u041f\u0440\u043e\u0438\u0437\u0432\u043e\u0434\u0441\u0442\u0432\u043e\ndata.vendor = np.array(data['vendor'].apply(lambda cl:vendor_to_num[cl]))\n# data['km_per_year'] = np.round(data.mileage\/data.modelDate)\n# data.km_per_year = data.km_per_year.astype(int)\nwith pd.option_context('display.max_rows', None, 'display.max_columns', None): display(data.head())\n# \u041c\u0430\u0440\u043a\u0430\ntest_x.brand = np.array(test_x['brand'].apply(lambda cl: brand_to_num[cl]))\n# \u041d\u0430\u0437\u0432\u0430\u043d\u0438\u0435\ntest_x.loc[test_x['model_name'].isin(models_to_throw_away), 'model_name'] = 'other'\ntest_x.model_name = np.array(test_x['model_name'].apply(lambda cl:model_name_to_num[cl]))\n# \u0414\u0430\u0442\u0430 \u043c\u043e\u0434\u0435\u043b\u0438\nyear = datetime.datetime.now().year\ntest_x.productionDate = test_x.productionDate.apply(lambda x: round(int(year)-int(x)))\n#\u0422\u0438\u043f \u043a\u0443\u0437\u043e\u0432\u0430\ntest_x.bodyType = np.array(test_x['bodyType'].apply(lambda cl: bodyType_to_num[cl]))\n#\u0426\u0432\u0435\u0442\ntest_x.color = np.array(test_x['color'].apply(lambda cl: color_to_num[cl]))\n#\u0422\u043e\u043f\u043b\u0438\u0432\u043e\ntest_x = test_x.merge(pd.get_dummies(test_x.fuelType), left_index=True, right_index=True)\ndef gibrid_bensin(row):\n    if row.gibrid==1:\n        return 1\n    else:\n        return row.bensin\ndef gibrid_electro(row):\n    if row.gibrid==1:\n        return 1\n    else:\n        return row.electro\ntest_x.rename(columns={'\u0431\u0435\u043d\u0437\u0438\u043d':'bensin', '\u0433\u0430\u0437':'gas', '\u0433\u0438\u0431\u0440\u0438\u0434':'gibrid', '\u0434\u0438\u0437\u0435\u043b\u044c':'disel', '\u044d\u043b\u0435\u043a\u0442\u0440\u043e':'electro'}, inplace=True)\ntest_x.bensin = test_x.apply(lambda row:gibrid_bensin(row), axis=1)\ntest_x.electro = test_x.apply(lambda row:gibrid_electro(row), axis=1)\ntest_x = test_x.drop(['fuelType', 'gibrid'], axis=1)\n# \u0422\u0438\u043f \u0442\u0440\u0430\u043d\u0441\u043c\u0438\u0441\u0441\u0438\u0438\ntest_x.vehicleTransmission = np.array(test_x['vehicleTransmission'].apply(lambda cl:vehicleTransmission_to_num[cl]))\n#\u041f\u0440\u0438\u0432\u043e\u0434\ntest_x = test_x.merge(pd.get_dummies(test_x.drivegear), left_index=True, right_index=True)\n\ndef rear_gear(row):\n    if row.fullgear==1:\n        return 1\n    else:\n        return row.reargear\ndef front_gear(row):\n    if row.fullgear==1:\n        return 1\n    else:\n        return row.frontgear\n    \ntest_x.rename(columns={'\u043f\u0435\u0440\u0435\u0434\u043d\u0438\u0439':'frontgear', '\u0437\u0430\u0434\u043d\u0438\u0439':'reargear','\u043f\u043e\u043b\u043d\u044b\u0439':'fullgear'}, inplace=True)\ntest_x.reargear = test_x.apply(lambda row:rear_gear(row), axis=1)\ntest_x.frontgear = test_x.apply(lambda row:front_gear(row), axis=1)\ntest_x = test_x.drop(['drivegear', 'fullgear'], axis=1)\n# \u0420\u0443\u043b\u044c\ntest_x.escl = np.array(test_x['escl'].apply(lambda cl:escl_to_num[cl]))\n# \u041f\u0440\u043e\u0438\u0437\u0432\u043e\u0434\u0441\u0442\u0432\u043e\ntest_x.vendor = np.array(test_x['vendor'].apply(lambda cl:vendor_to_num[cl]))\n# test_x['km_per_year'] = np.round(test_x.mileage\/test_x.modelDate)\n# test_x.km_per_year = test_x.km_per_year.astype(int)\nwith pd.option_context('display.max_rows', None, 'display.max_columns', None): display(test_x.head())\n","f2b35989":"train_x = data.drop(['disel', 'engineDisplacement','numberOfDoors'], axis=1)\ntest_x = test_x.drop(['disel', 'engineDisplacement','numberOfDoors'], axis=1)\n# train_x = train_x.drop(['citympg','highwaympg'], axis=1)\n# test_x = test_x.drop(['citympg','highwaympg'], axis=1)","7fdb0beb":"train_x.info()\ntest_x.info()","36c7f16d":"import seaborn as sns\nimport matplotlib.pyplot as plt\n# Compute the correlation matrix\ncorr = train_x.corr()\n\n# Generate a mask for the upper triangle\nmask = np.triu(np.ones_like(corr, dtype=bool))\n\n# Set up the matplotlib figure\nf, ax = plt.subplots(figsize=(20, 15))\n\n# Generate a custom diverging colormap\ncmap = sns.diverging_palette(230, 20, as_cmap=True)\n\n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5},annot=True)","c22b7e17":"X_train,y_train,X_test,y_test = train_x.sort_index(level=1, axis=1), train_y, test_x.sort_index(level=1, axis=1),sample_submission","27b2d183":"def mape(y_true, y_pred):\n    return np.mean(np.abs((y_pred-y_true)\/y_true))","5210e297":"from sklearn.ensemble import RandomForestRegressor\nfrom pprint import pprint\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nX_train,y_train,X_test,y_test = train_x.sort_index(level=1, axis=1), train_y, test_x.sort_index(level=1, axis=1),sample_submission\nrfr = RandomForestRegressor()\nX_train_s, X_val, y_train_s ,y_val = train_test_split(X_train,y_train,test_size=0.25, random_state=42)\ny_train_s = y_train_s.apply(lambda x : np.log(x))\nrfr.fit(X_train_s,y_train_s)\ny_pred = rfr.predict(X_val)\ny_pred = np.exp(y_pred)\npprint(rfr.get_params())\npprint(mape(y_val,y_pred))","ecae1be9":"rfr = RandomForestRegressor(random_state=123\n                      , n_estimators=300\n                      , min_samples_split=2\n                      , min_samples_leaf=1\n                      , max_features='auto'\n                      , max_depth=None\n                      , bootstrap=True)\ny_train = y_train.apply(lambda x : np.log(x))\nrfr.fit(X_train,y_train)\ny_pred = rfr.predict(X_test)\ntest_y.price = np.round(np.exp(y_pred))\ntest_y.to_csv(f'rfr{VERSION}.csv',index=False)","65782b08":"from sklearn.model_selection import RandomizedSearchCV\nimport lightgbm as lgb\nfrom sklearn.model_selection import StratifiedKFold\n# folds = 5\n# param_comb = 5\n# X_train,y_train,X_test,y_test = train_x.sort_index(level=1, axis=1), train_y, test_x.sort_index(level=1, axis=1),sample_submission\n# X_train = X_train.values\n# X_test = X_test.values\n\n# skf = StratifiedKFold(n_splits=folds, shuffle = True, random_state = 123) \n# hyper_params={'num_leaves': range(50,100,5),\n#               'min_child_samples': range(100,500,50),\n#               'min_child_weight': [1e-1,2e-1,3e-1,4e-1,5e-1,6e-1,7e-1,8e-1,9e-1],\n#               'subsample': np.arange(0.9,1,0.01),\n#               'colsample_bytree': np.arange(0.3,0.6,0.1),\n#               'reg_alpha': np.arange(0.5,1.5,0.1),\n#               'reg_lambda': np.arange(0,0.1,0.01)}\n# gbm = lgb.LGBMRegressor(**hyper_params)\n# rand_search = RandomizedSearchCV(gbm, param_distributions=hyper_params,scoring = 'neg_mean_absolute_error', n_jobs=5, cv=skf.split(X_train,y_train), verbose=3)\n# rand_search.fit(X_train, np.log(y_train))\n# print(rand_search.best_params_)","85db500b":"import lightgbm as lgb\nX_train,y_train,X_test,y_test = train_x.sort_index(level=1, axis=1), train_y, test_x.sort_index(level=1, axis=1),sample_submission\nX_train = X_train.values\nX_test = X_test.values\ngbm = lgb.LGBMRegressor()\ngbm.fit(X_train, np.log(y_train))\npreds = gbm.predict(X_test)\ntest_y.price = np.exp(preds)\ntest_y.to_csv(f'gbm{VERSION}.csv',index=False)","909385d1":"# from sklearn.linear_model import ElasticNet\n# hyper_params = {'alpha':[1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 0.0, 1.0, 10.0, 100.0],\n#                 'l1_ratio': np.arange(0, 1, 0.01)}\n# model = ElasticNet(**hyper_params)\n# skf = StratifiedKFold(n_splits=folds, shuffle = True, random_state = 123) \n# rand_search = RandomizedSearchCV(model, param_distributions=hyper_params,scoring = 'neg_mean_absolute_error', n_jobs=-1, cv=skf.split(X_train,y_train), verbose=3)\n# rand_search.fit(X_train, np.log(y_train))\n# print(rand_search.best_params_)","c56f54e7":"import xgboost as xgb\nparams = {'n_estimators' : range(500, 700, 20),\n        'min_child_weight': [1, 3, 5],\n        'gamma': [0, 0.5, 1],\n        'max_depth': [6, 9, 12],\n        'eta' : [0.05, 0.1, 0.2, 0.3],\n         }\nxgb_reg = xgb.XGBRegressor(**params)\nfolds = 5\nparam_comb = 5\nskf = StratifiedKFold(n_splits=folds, shuffle = True, random_state = 123)\n#mapee = make_scorer(mapeee, greater_is_better=True)\nrand_search = RandomizedSearchCV(xgb_reg, param_distributions=params,scoring = 'neg_mean_absolute_error', n_jobs=-1, cv=skf.split(X_train,y_train), verbose=3)\nrand_search.fit(X_train, np.log(y_train))\nprint(rand_search.best_params_)","932d830a":"import xgboost as xgb\nfrom sklearn.model_selection import GridSearchCV\n#XGBoost hyper-parameter tuning\ndef hyperParameterTuning(X_train, y_train):\n    param_tuning = {\n        'learning_rate': [0.01, 0.1],\n        'max_depth': [3, 5, 7, 10],\n        'min_child_weight': [1, 3, 5],\n        'subsample': [0.5, 0.7],\n        'colsample_bytree': [0.5, 0.7],\n        'n_estimators' : [100, 200, 500],\n        'objective': ['reg:squarederror']\n    }\n\n    xgb_model = xgb.XGBRegressor()\n\n    gsearch = GridSearchCV(estimator = xgb_model,\n                           param_grid = param_tuning,                        \n                           scoring = 'neg_mean_absolute_error', #MAE\n                           scoring = 'neg_mean_squared_error',  #MSE\n                           cv = 5,\n                           n_jobs = -1,\n                           verbose = 1)\n\n    gsearch.fit(X_train,y_train)\n\n    return gsearch.best_params_","3772a988":"X_train,y_train,X_test,y_test = train_x.sort_index(level=1, axis=1), train_y, test_x.sort_index(level=1, axis=1),sample_submission\nhyperParameterTuning(X_train,y_train)","0ffde2ee":"from sklearn.model_selection import train_test_split\nimport xgboost as xgb\nX_train,y_train,X_test,y_test = train_x.sort_index(level=1, axis=1), train_y, test_x.sort_index(level=1, axis=1),sample_submission\n# hyper = {'colsample_bytree': 0.7,\n#  'learning_rate': 0.1,\n#  'max_depth': 7,\n#  'min_child_weight': 1,\n#  'n_estimators': 500,\n#  'objective': 'reg:squarederror',\n#  'subsample': 0.5,\n#  'eta': 0.06}\n\nxg_reg = xgb.XGBRegressor(learning_rate=0.1,\n    n_estimators=600, min_child_weight=1,\n    reg_lambda=1,\n    gamma=0, eta=0.06,\n    max_depth=12, colsample_bytree=0.6)\n\n# X_train = X_train.values\n# X_test = X_test.values\n\n# y_train = y_train.apply(lambda x : np.log(x))\nxg_reg.fit(X_train,np.log(y_train))\n\npreds = np.round(np.exp(xg_reg.predict(X_test)))\ntest_y.price = preds\ntest_y.to_csv(f'xgb{VERSION}.csv',index=False)","380a6c82":"from sklearn.model_selection import train_test_split\nfrom pprint import pprint\n# learning_rate=0.081,\n#     n_estimators=600, min_child_weight=1,\n#     reg_lambda=1,\n#     gamma=0, eta=0.09,\n#     max_depth=12, \n#     colsample_bytree=0.6,\n#     num_parallel_tree= 1\nxg_reg = xgb.XGBRegressor()\n\nX_train,y_train,X_test,y_test = train_x.drop(columns=['numberOfDoors','combmpg']).sort_index(level=1, axis=1), train_y, test_x.drop(columns=['numberOfDoors','combmpg']).sort_index(level=1, axis=1),sample_submission\nX_train_s, X_val, y_train_s ,y_val = train_test_split(X_train,y_train,test_size=0.25, random_state=42)\ny_train_s = y_train_s.apply(lambda x : np.log(x))\nxg_reg.fit(X_train_s,y_train_s)\ny_pred = xg_reg.predict(X_val)\ny_pred = np.exp(y_pred)\npprint(xg_reg.get_params())\npprint(mape(y_val,y_pred))","b302e0c8":"xg_reg.get_params()","6a26d6be":"# xg_reg = xgb.XGBRegressor()\n# X_train = X_train.values\n# X_test = X_test.values\n\n# y_train = y_train.apply(lambda x : np.log(x))\nxg_reg.fit(X_train,np.log(y_train))\n\npreds = np.round(np.exp(xg_reg.predict(X_test)),-3)","7e53e8f1":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import cross_val_score, train_test_split\nfrom sklearn.ensemble import BaggingClassifier, RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import StandardScaler","c5f8f67f":"from catboost import CatBoostRegressor\nX_train_s, X_val, y_train_s ,y_val = train_test_split(X_train,y_train,test_size=0.25, random_state=42)\nmodel = CatBoostRegressor(iterations = 5000,\n                          eval_metric='MAPE',\n                          custom_metric=['R2', 'MAE'],\n                          silent=True,\n                         )\ngrid = {'learning_rate': [0.01, 0.1, 0.2, 0.3],\n        'depth': [5,7,9,11],\n        'l2_leaf_reg': [1, 3, 5, 7, 9],\n        'random_strength': [0.01, 0.1, 0.5, 1],\n        'bagging_temperature': [0.1,1,5,25,5000]}\n\nrandomized_search_result = model.randomized_search(grid,\n                                                   X=X_train,\n                                                   y=y_train,\n                                                   plot=True)","b6e401c9":"model.get_params()","00312176":"from catboost import CatBoostRegressor\nmodel = CatBoostRegressor(iterations = 4985,\n                          learning_rate = 0.1,\n                          depth=9,\n                          l2_leaf_reg=9,\n                          random_strength=1,\n                          bagging_temperature=5)\n\nmodel.fit(X_train,np.log(y_train))\n","87fc54ab":"mape(y_val,np.exp(model.predict(X_val)))","4130869a":"test_y.price = np.exp(model.predict(X_test))\ntest_y.to_csv(f'cat{VERSION}.csv',index=False)","e3e3c6b5":"import matplotlib.pyplot as plt\nplt.bar(train_x.sort_index(level=1, axis=1).columns, xg_reg.feature_importances_)\nplt.xticks(rotation=90)\nplt.show()","787c6242":"from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor\nparams = {'n_estimators' : 400,\n        'min_samples_leaf':1,\n        'max_features':'auto',\n        'min_samples_split':2,\n        'max_depth':  12,\n         }\nxtree = ExtraTreesRegressor( **params)\nfolds = 5\nparam_comb = 5\n#skf = StratifiedKFold(n_splits=folds, shuffle = True, random_state = 123)\n#rand_search = RandomizedSearchCV(xtree, param_distributions=params,scoring = 'neg_mean_absolute_error', n_jobs=-1, cv=skf.split(X_train,y_train), verbose=3)\nxtree.fit(X_train_s, np.log(y_train_s))\npreds = np.round(np.exp(xtree.predict(X_val)),-3)\nmape(y_val, preds)","481981b3":"from catboost import CatBoostRegressor\ncat = CatBoostRegressor(iterations = 4985,\n                          learning_rate = 0.1,\n                          depth=9,\n                          l2_leaf_reg=9,\n                          random_strength=1,\n                          bagging_temperature=5)","f058fad7":"xg_reg = xgb.XGBRegressor(n_estimators=600, min_child_weight=1,\n    reg_lambda=1,\n    gamma=0, eta=0.06,\n    max_depth=12, colsample_bytree=0.6, \n)","e4c26c78":"from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor\nxtree = ExtraTreesRegressor()","21e7a854":"rfr = RandomForestRegressor(random_state=123\n                      , n_estimators=300\n                      , min_samples_split=2\n                      , min_samples_leaf=1\n                      , max_features='auto'\n                      , max_depth=None\n                      , bootstrap=True)","a2dd0967":"gbm = lgb.LGBMRegressor(subsample= 1, \n                        reg_lambda= 0, \n                        reg_alpha=1.4, \n                        num_leaves=90, \n                        min_child_weight =0.6, \n                        min_child_samples=100, \n                        colsample_bytree =0.4)","30a88c86":"from sklearn.linear_model import ElasticNet\nenet = ElasticNet(l1_ratio = 0.04, \n                  alpha = 0.0001)","0364ac37":"# from sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\n# from sklearn.model_selection import KFold\n# from sklearn.preprocessing import StandardScaler\n\n# class StackingAveragedModels(BaseEstimator, RegressorMixin, TransformerMixin):\n#     def __init__(self, base_models, meta_model, n_folds=5):\n#         self.base_models = base_models\n#         self.meta_model = meta_model\n#         self.n_folds = n_folds\n   \n#     # We again fit the data on clones of the original models\n#     def fit(self, X, y):\n#         self.base_models_ = [list() for x in self.base_models]\n#         self.meta_model_ = clone(self.meta_model)\n#         kfold = KFold(n_splits=self.n_folds, shuffle=True, random_state=156)\n        \n#         # Train cloned base models then create out-of-fold predictions\n#         # that are needed to train the cloned meta-model\n#         out_of_fold_predictions = np.zeros((X.shape[0], len(self.base_models)))\n#         for i, model in enumerate(self.base_models):\n#             for train_index, holdout_index in kfold.split(X, y):\n#                 instance = clone(model)\n#                 self.base_models_[i].append(instance)\n#                 instance.fit(X[train_index], y[train_index])\n#                 y_pred = instance.predict(X[holdout_index])\n#                 out_of_fold_predictions[holdout_index, i] = y_pred\n                \n#         # Now train the cloned  meta-model using the out-of-fold predictions as new feature\n#         self.meta_model_.fit(out_of_fold_predictions, y)\n#         return self\n   \n#     #Do the predictions of all base models on the test data and use the averaged predictions as \n#     #meta-features for the final prediction which is done by the meta-model\n#     def predict(self, X):\n#         meta_features = np.column_stack([\n#             np.column_stack([model.predict(X) for model in base_models]).mean(axis=1)\n#             for base_models in self.base_models_ ])\n#         return self.meta_model_.predict(meta_features)","703274c6":"# stacked_averaged_models = StackingAveragedModels(base_models = (cat, rfr, gbm, xg_reg, xtree),\n#                                                  meta_model = enet)","a0e586c6":"# stacked_averaged_models.fit(X_train, np.log(y_train))","97ca0153":"# predict=stacked_averaged_models.predict(X_test)","e54430e8":"# test_y.price = np.exp(predict)\n# test_y.to_csv(f'stack{VERSION}.csv',index=False)","0de1caa7":"# compare machine learning models for regression\nfrom numpy import mean\nfrom numpy import std\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import RepeatedKFold\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nimport xgboost as xgb\nfrom matplotlib import pyplot\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import StackingRegressor\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.linear_model import ElasticNet\nfrom catboost import CatBoostRegressor\n# get a stacking ensemble of models\ndef get_stacking():\n\t# define the base models\n\tlevel0 = list()\n\tlevel0.append(('el', ElasticNet()))\n\tlevel0.append(('lr', LinearRegression()))\n\tlevel0.append(('cart', DecisionTreeRegressor()))\n\tlevel0.append(('xgb', xgb.XGBRegressor()))\n\t# define meta learner model\n\tlevel1 = LinearRegression()\n\t# define the stacking ensemble\n\tmodel = StackingRegressor(estimators=level0, final_estimator=level1, cv=5)\n\treturn model\n# get a list of models to evaluate\n\ndef get_models():\n\tmodels = dict()    \n\tmodels['el'] = ElasticNet()    \n\tmodels['lr'] = LinearRegression()\n\tmodels['cart'] = DecisionTreeRegressor()\n\tmodels['xgb'] = xgb.XGBRegressor()\n\tmodels['rfr'] = RandomForestRegressor()\n\tmodels['stacking'] = get_stacking()\n\treturn models\n \n# evaluate a given model using cross-validation\ndef evaluate_model(model, X, y):\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n    model.fit(X_train,np.log(y_train))\n    return np.mean(np.abs((np.exp(model.predict(X_test))-y_test)\/y_test))\n\n    \n# define dataset\nX, y = train_x.sort_index(level=1, axis=1), train_y\n# get the models to evaluate\nmodels = get_models()\n# evaluate the models and store results\nresults, names = list(), list()\nfor name, model in models.items():\n\tscores = evaluate_model(model, X, y)\n\tresults.append(scores)\n\tnames.append(name)\n\tprint('>%s %.3f' % (name, scores))\n","22ae6406":"# define dataset\nX, y = train_x.sort_index(level=1, axis=1), train_y\n# define the base models\nlevel0 = list()\nlevel0.append(('lr', LinearRegression(normalize =True)))\nlevel0.append(('rfr', RandomForestRegressor(random_state=123\n                      , n_estimators=300\n                      , min_samples_split=2\n                      , min_samples_leaf=1\n                      , max_features='auto'\n                      , max_depth=None\n                      , bootstrap=True)))\nlevel0.append(('xgb', xgb.XGBRegressor(learning_rate=0.1,\n    n_estimators=600, min_child_weight=1,\n    reg_lambda=1,\n    gamma=0, eta=0.06,\n    max_depth=12, colsample_bytree=0.6)))\n# define meta learner model\nlevel1 = LinearRegression()\n# define the stacking ensemble\nmodel = StackingRegressor(estimators=level0, final_estimator=level1, cv=5)\n# fit the model on all available data\nmodel.fit(X, np.log(y))\n# make a prediction for one example\nyhat = model.predict(test_x.sort_index(level=1, axis=1))\ntest_y.price = np.exp(yhat)\ntest_y.to_csv(f'stacking{VERSION}.csv',index=False)","2176ff96":"0.07 eta - 0.1217980018807087,0.08 eta - 0.1217980018807087\n0.09 lr - 0.1204\n0.08 lr - 0.1205\n0.11 lr - 0.1217\n","5d424daf":"# Data input","a53d5728":"# **Data Cleaning**","cb345f0e":"# Visualization","94fa814a":"# Models","7d7c027b":"# Feature Impotance","ae6f892a":"# Parser\n"}}