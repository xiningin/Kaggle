{"cell_type":{"ed311013":"code","809f5724":"code","c77df255":"code","2dac5532":"code","02680524":"code","8d0d6db1":"code","736835de":"code","f95b2ee4":"code","b794ae30":"code","583e17f5":"code","7d2e4b37":"code","c1de09ea":"code","9d8e5a22":"code","cda8ff00":"code","4df7ba6c":"code","9d093bb1":"code","494bfcd1":"code","6cfb8735":"code","ecac6f2c":"code","08545b3c":"code","897e43a7":"code","ecb19415":"code","89267f9a":"code","eed987d2":"code","87d229ef":"code","38051a1e":"code","8f784ee4":"code","1bc36bc5":"code","f8f1d834":"code","b578ecef":"code","ffb0fbd1":"code","48e18830":"code","3f8c4304":"code","20986181":"code","abb06e5f":"markdown","353461ec":"markdown","a6fc342e":"markdown","5fba5e99":"markdown","6791ce26":"markdown","0b12db81":"markdown","d369ed63":"markdown","241e01eb":"markdown","a49ef5b0":"markdown","11fe9f86":"markdown","17e08757":"markdown","0fa13c8c":"markdown","35196691":"markdown","a8ae6408":"markdown","a0a32d31":"markdown","f75adfef":"markdown","ac9db93e":"markdown","422fd946":"markdown","3df9e326":"markdown","30da66c6":"markdown","55bc8d90":"markdown","ce021f28":"markdown","62b56138":"markdown","38529aa5":"markdown","e2a474c3":"markdown"},"source":{"ed311013":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport sklearn as skl","809f5724":"df=pd.read_csv(\"..\/input\/loan-prediction-problem-dataset\/train_u6lujuX_CVtuZ9i.csv\")\ndf.head()","c77df255":"df.columns #gives column names in the dataset","2dac5532":"df.shape #shows no of rows and columns in the dataset","02680524":"df.describe()","8d0d6db1":"df.info()","736835de":"#let`s check the missing values with in the dataset\ndf.isnull().sum()","f95b2ee4":"#fill the missing values for numerical terms - mean\nLoanAmountMean = df[\"LoanAmount\"].mean()\nLoanAmountTermMean = df[\"Loan_Amount_Term\"].mean()\ndf[\"LoanAmount\"] = df[\"LoanAmount\"].fillna(LoanAmountMean)\ndf[\"Loan_Amount_Term\"] = df[\"Loan_Amount_Term\"].fillna(LoanAmountTermMean)\n# I have replaced missing values in Credit_History column with most frequent value - 1.0\ndf[\"Credit_History\"] = df[\"Credit_History\"].fillna(1.0)","b794ae30":"#fill the missing values for categorical terms - mode\ndf[\"Gender\"] = df[\"Gender\"].fillna(df[\"Gender\"].mode()[0])\ndf[\"Married\"] = df[\"Married\"].fillna(df[\"Married\"].mode()[0])\ndf[\"Dependents\"] = df[\"Dependents\"].fillna(df[\"Dependents\"].mode()[0])\ndf[\"Self_Employed\"] = df[\"Self_Employed\"].fillna(df[\"Self_Employed\"].mode()[0])","583e17f5":"df.isnull().sum() #now we can see that their are no missing values in the dataset","7d2e4b37":"# ApplicantIncome and CoapplicantIncome can be combined together \n# so we are adding these two columns and making a new column called TotalIncome\n# and we will drop ApplicantIncome and CoapplicantIncome columns\ndf[\"TotalIncome\"] = df[\"ApplicantIncome\"] + df[\"CoapplicantIncome\"]\ncols=[\"ApplicantIncome\",\"CoapplicantIncome\",\"Loan_ID\"]\ndf.drop(cols,axis=1,inplace=True)\n","c1de09ea":"df.head()","9d8e5a22":"#categorical attriburtes visualization\nsns.countplot(df[\"Gender\"])","cda8ff00":"sns.countplot(df[\"Married\"])","4df7ba6c":"sns.countplot(df[\"Dependents\"])","9d093bb1":"sns.countplot(df[\"Education\"])","494bfcd1":"sns.countplot(df[\"Self_Employed\"])","6cfb8735":"# numerical attributes visualization\nsns.distplot(df[\"TotalIncome\"])","ecac6f2c":"# apply log transformation to the attribute\ndf[\"TotalIncome\"]= np.log(df[\"TotalIncome\"])","08545b3c":"sns.distplot(df[\"TotalIncome\"])","897e43a7":"sns.distplot(df[\"LoanAmount\"])","ecb19415":"df[\"LoanAmount\"] = np.log(df[\"LoanAmount\"])\nsns.distplot(df[\"LoanAmount\"])","89267f9a":"sns.distplot(df[\"Loan_Amount_Term\"])","eed987d2":"sns.distplot(df[\"Credit_History\"])","87d229ef":"corr = df.corr()\nplt.figure(figsize=(12,9))\nsns.heatmap(corr, annot = True)","38051a1e":"df.head()","8f784ee4":"from sklearn.preprocessing import LabelEncoder\ncols = [\"Gender\",\"Married\",\"Education\",\"Self_Employed\",\"Property_Area\",\"Loan_Status\",\"Dependents\"]\nle = LabelEncoder()\nfor col in cols:\n    df[col] = le.fit_transform(df[col])","1bc36bc5":"df.head()","f8f1d834":"from sklearn.model_selection import train_test_split\nX = df.drop(\"Loan_Status\",axis=1)\nY = df[\"Loan_Status\"]\nx_train,x_test,y_train,y_test = train_test_split(X,Y,test_size=0.2, random_state=42)","b578ecef":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_val_score\nmodel = LogisticRegression()\nmodel.fit(x_train,y_train)\nprint(\"Accuracy of model is\",model.score(x_test,y_test)*100)\nscore = cross_val_score(model,X,Y,cv=5)\nprint(\"Cross Validation is\",np.mean(score)*100)","ffb0fbd1":"from sklearn.tree import DecisionTreeClassifier\nmodel=DecisionTreeClassifier()\nmodel.fit(x_train,y_train)\nprint(\"Accuracy of model is\",model.score(x_test,y_test)*100)\nscore = cross_val_score(model,X,Y,cv=5)\nprint(\"Cross Validation is\",np.mean(score)*100)","48e18830":"from sklearn.ensemble import RandomForestClassifier\nmodel=RandomForestClassifier()\nmodel.fit(x_train,y_train)\nprint(\"Accuracy of model is\",model.score(x_test,y_test)*100)\nscore = cross_val_score(model,X,Y,cv=5)\nprint(\"Cross Validation is\",np.mean(score)*100)","3f8c4304":"# Let`s fine tune the hyper parameters of RandomForest \nmodel = RandomForestClassifier(n_estimators=100,min_samples_split=25,max_depth=7,max_features=1)\nmodel.fit(x_train,y_train)\nprint(\"Accuracy of model is\",model.score(x_test,y_test)*100)\nscore = cross_val_score(model,X,Y,cv=5)\nprint(\"Cross Validation is\",np.mean(score)*100)","20986181":"from sklearn.metrics import confusion_matrix\ny_predicted = model.predict(x_test)\ncm = confusion_matrix(y_test,y_predicted)\nsns.heatmap(cm,annot=True)\n","abb06e5f":"# Train-Test Split","353461ec":"# Fine Tuning","a6fc342e":"Now we can see all the columns are converted into numerical columns and we can now easily train our model.","5fba5e99":"# Model Training (Logistic Regression, Decision Tree, Random Forest)","6791ce26":"# Data Preprocessing","0b12db81":"# Load and Read the dataset","d369ed63":"Most of the applicants are not self employed.","241e01eb":"so we can see that we have total 13 attributes out of which 12 attributes are Independent variables and 1 attribute (Loan_Status) is dependent variable.\nwe can also see the datatype of each variable. ","a49ef5b0":"## Converting Categorical variables into numerical using label encoder","11fe9f86":"We can see from the above graph that most of the applicants are Graduate.","17e08757":"MAjority of the TotalIncome of applicants is between 0-10,000 , only few are from 20,000 onwards.\ngraph is left skewed i.e most of the applicants are on the left side which is not a good distribution for training the model.so we will apply the log function in the column to normalize the attribute and make a bell curve.\n\nIf you see the graph \"left skewed or right skewed\", you can apply\n1. log transformation\n2. Min-Max Normalization\n3. Standarization\nThese are the common techniques to normalize the distribution in order to train the model better.","0fa13c8c":"# Correlation Matrix","35196691":"so most of the applicants have 0 dependents and very few have 3+ dependents","a8ae6408":"Correlation Matrix is used to see the relationship between variables. if the correlation between two variables is high , drop any one of the variable (This is the best practice). ","a0a32d31":"so the distribution for CoapplicantIncome is also left skewed. we will apply log transformation here as well.","f75adfef":"so majority of the applicants are married","ac9db93e":"# Label Encoding ","422fd946":"# Creating New Features","3df9e326":"so we can analyse from this plot that majority of the data is for Male","30da66c6":"No need to apply transformation here because values are already in the range of 0-1","55bc8d90":"# Confusion Matrix","ce021f28":"so now the distribution of LoanAmount is better than before.","62b56138":"# Import Necessary Libraries","38529aa5":"# Exploratory Data Analysis","e2a474c3":"We have splitted the data like 20% for testing and 80% for training."}}