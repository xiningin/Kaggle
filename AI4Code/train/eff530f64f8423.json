{"cell_type":{"f3c8d23d":"code","fc24a4e9":"code","44f662be":"code","84d744bb":"code","c849c760":"code","67aa473d":"code","ed6fa054":"code","d00d5409":"code","8322b676":"code","3805210f":"code","522b4a31":"code","d2d830d5":"code","79dd4514":"code","22b64f0a":"code","b1931e22":"code","3a955149":"code","a34ae476":"code","15af025c":"code","64975262":"code","ac955104":"code","575e5167":"code","771365a4":"code","250deb16":"code","57166aa5":"code","ea3a24d2":"code","45ad3b77":"code","52cc1208":"code","72aeeeaf":"code","bba0920a":"code","a9834ee9":"code","9316198e":"code","237e16f7":"code","433e15fb":"code","b4c8f7ef":"code","c883281e":"code","bc455381":"markdown","1f456a24":"markdown","68fa5e68":"markdown","fd2c3aca":"markdown","62e4881c":"markdown","8d7054a3":"markdown","36f08dad":"markdown","03b75861":"markdown","6b4aa7ac":"markdown","55c1d45a":"markdown","29e182ac":"markdown","f2abbed2":"markdown","61d40bf6":"markdown","d5fffa7a":"markdown","9ab6aa8a":"markdown","49472b3d":"markdown","15110288":"markdown","4f1baa8c":"markdown","886d3f6f":"markdown","2e1777da":"markdown","b07a74b3":"markdown","978c69e4":"markdown","3e84564e":"markdown","a365a459":"markdown","08513600":"markdown"},"source":{"f3c8d23d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        pass\n        #print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","fc24a4e9":"#import IPython\n#IPython.display.IFrame(<iframe width=\"650\" height=\"400\" frameborder=\"0\" scrolling=\"no\" marginheight=\"0\" marginwidth=\"0\" title=\"2019-nCoV\" src=\"\/gisanddata.maps.arcgis.com\/apps\/Embed\/index.html?webmap=14aa9e5660cf42b5b4b546dec6ceec7c&extent=77.3846,11.535,163.5174,52.8632&zoom=true&previewImage=false&scale=true&disable_scroll=true&theme=light\"><\/iframe>)","44f662be":"from IPython.display import HTML\n\nHTML('<div style=\"position:relative;height:0;padding-bottom:56.25%\"><iframe src=\"https:\/\/www.youtube.com\/embed\/jmHbS8z57yI?ecver=2\" width=\"640\" height=\"360\" frameborder=\"0\" style=\"position:absolute;width:100%;height:100%;left:0\" allowfullscreen><\/iframe><\/div>')","84d744bb":"## install calmap\n#! pip install calmap","c849c760":"# essential libraries\nimport json\nimport random\nfrom urllib.request import urlopen\n\n# storing and anaysis\nimport numpy as np\nimport pandas as pd\n\n# visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\nimport plotly.graph_objs as go\nimport plotly.figure_factory as ff\n#import calmap\nimport folium\nimport plotly.io as pio\npio.templates.default = \"plotly_dark\"\nfrom plotly.subplots import make_subplots\n\n# color pallette\ncnf = '#393e46' # confirmed - grey\ndth = '#ff2e63' # death - red\nrec = '#21bf73' # recovered - cyan\nact = '#fe9801' # active case - yellow\n\n# converter\nfrom pandas.plotting import register_matplotlib_converters\nregister_matplotlib_converters()   \n\n# hide warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# html embedding\nfrom IPython.display import Javascript\nfrom IPython.core.display import display\nfrom IPython.core.display import HTML","67aa473d":"# importing datasets\n\n\nfull_table = pd.read_csv('\/kaggle\/input\/novel-corona-virus-2019-dataset\/covid_19_data.csv',parse_dates = ['ObservationDate'])\n#full_table = pd.read_csv('..\/input\/corona-virus-report\/covid_19_clean_complete.csv', parse_dates=['Date'])\n#train = pd.read_csv('\/kaggle\/input\/covid19-global-forecasting-week-1\/train.csv')\n","ed6fa054":"print ('Last update of this dataset was ' + str(full_table.loc[len(full_table)-1]['Last Update']))\n#print ('Last update of this dataset was ' + str(full_table.loc[len(full_table)-1]['Date']))","d00d5409":"full_table.columns = ['SNo', 'Date', 'Province\/State', 'Country\/Region','Last Update', 'Confirmed', 'Deaths', 'Recovered']","8322b676":"### Cleaning Data\n\n# cases \ncases = ['Confirmed', 'Deaths', 'Recovered', 'Active']\n\n# Active Case = confirmed - deaths - recovered\nfull_table['Active'] = full_table['Confirmed'] - full_table['Deaths'] - full_table['Recovered']\n\n# replacing Mainland china with just China\nfull_table['Country\/Region'] = full_table['Country\/Region'].replace('Mainland China', 'China')\n\n# filling missing values \nfull_table[['Province\/State']] = full_table[['Province\/State']].fillna('')\nfull_table[cases] = full_table[cases].fillna(0)\n# cases in the ships\nship = full_table[full_table['Province\/State'].str.contains('Grand Princess')|full_table['Country\/Region'].str.contains('Cruise Ship')]\n\n# china and the row\nchina = full_table[full_table['Country\/Region']=='China']\nus = full_table[full_table['Country\/Region']=='US']\nskorea  = full_table[full_table['Country\/Region']=='South Korea']\nhot_europe = full_table[full_table['Country\/Region'].isin(\n    ['Italy,Spain','Germany','France','UK','Switzerland','Netherlands','Belgium','Austria','Norway','Sweden','Denmark','Portugal', 'Great Britain']) ]\nitaly = full_table[full_table['Country\/Region']=='Italy']\niran = full_table[full_table['Country\/Region']=='Iran']\nspain = full_table[full_table['Country\/Region']=='Spain']\nfrance = full_table[full_table['Country\/Region']=='France']\nuk = full_table[full_table['Country\/Region']=='UK']\nswitzerland = full_table[full_table['Country\/Region']=='Switzerland']\nsingapore = full_table[full_table['Country\/Region']=='Singapore']\nsweden = full_table[full_table['Country\/Region']=='Sweden']\nindia = full_table[full_table['Country\/Region']=='India']\njapan = full_table[full_table['Country\/Region']=='Japan']\nnz = full_table[full_table['Country\/Region']=='New Zealand']\nportugal = full_table[full_table['Country\/Region']=='Portugal']\nbelgium = full_table[full_table['Country\/Region']=='Belgium']\nnetherlands = full_table[full_table['Country\/Region']=='Netherlands']\ngreece = full_table[full_table['Country\/Region']=='Greece']\nbrazil = full_table[full_table['Country\/Region']=='Brazil']\nrow = full_table[full_table['Country\/Region']!='China']\n#rest of China\nroc = china[china['Province\/State'] != 'Hubei']\ngermany = full_table[full_table['Country\/Region']=='Germany']\nca = us[us['Province\/State'] == 'California']\nny = us[us['Province\/State'] == 'New York']\nwa = us[us['Province\/State'] == 'Washington']\nco = us[us['Province\/State'] == 'Colorado']\nnj = us[us['Province\/State'] == 'New Jersey']\nla = us[us['Province\/State'] == 'Louisiana']\nma = us[us['Province\/State'] == 'Massachusetts']\nfl = us[us['Province\/State'] == 'Florida']\nmi = us[us['Province\/State'] == 'Michigan']\nil = us[us['Province\/State'] == 'Illinois']\nga = us[us['Province\/State'] == 'Georgia']\nak = us[us['Province\/State'] == 'Alaska']\nmn = us[us['Province\/State'] == 'Minnesota']\nmt = us[us['Province\/State'] == 'Montana']\nok = us[us['Province\/State'] == 'Oklahoma']\nsc = us[us['Province\/State'] == 'South Carolina']\ntn = us[us['Province\/State'] == 'Tennessee']\nms = us[us['Province\/State'] == 'Mississippi']\nId = us[us['Province\/State'] == 'Idaho']\nut = us[us['Province\/State'] == 'Utah']\nwy = us[us['Province\/State'] == 'Wyoming']\nsd = us[us['Province\/State'] == 'South Dakota']\nnd = us[us['Province\/State'] == 'North Dakota']\nia = us[us['Province\/State'] == 'Iowa']\ntx = us[us['Province\/State'] == 'Texas']\nal = us[us['Province\/State'] == 'Alabama']\nme = us[us['Province\/State'] == 'Maine']\naz = us[us['Province\/State'] == 'Arizona']\nar = us[us['Province\/State'] == 'Arkansas']\nhi = us[us['Province\/State'] == 'Hawaii']\nIn = us[us['Province\/State'] == 'Indiana']\nks = us[us['Province\/State'] == 'Kansas']\nne = us[us['Province\/State'] == 'Nebraska']\nnv = us[us['Province\/State'] == 'Nevada']\nnc = us[us['Province\/State'] == 'North Carolina']\npa = us[us['Province\/State'] == 'Pennsylvania']\nri = us[us['Province\/State'] == 'Rhode Island'] \n\n    \nnsh = full_table[full_table['Province\/State'].isin(['North Dakota','South Dakota','Iowa','Nebraska','Arkansas',])]\n\n# new countries\n\ndenmark = full_table[full_table['Country\/Region']=='Denmark']\naustria = full_table[full_table['Country\/Region']=='Austria']\nnorway = full_table[full_table['Country\/Region']=='Norway']\n\n# latest\nfull_latest = full_table[full_table['Date'] == max(full_table['Date'])].reset_index()\nchina_latest = full_latest[full_latest['Country\/Region']=='China']\nrow_latest = full_latest[full_latest['Country\/Region']!='China']\n\n# latest condensed\nfull_latest_grouped = full_latest.groupby('Country\/Region')['Confirmed', 'Deaths', 'Recovered', 'Active'].sum().reset_index()\nchina_latest_grouped = china_latest.groupby('Province\/State')['Confirmed', 'Deaths', 'Recovered', 'Active'].sum().reset_index()\nrow_latest_grouped = row_latest.groupby('Country\/Region')['Confirmed', 'Deaths', 'Recovered', 'Active'].sum().reset_index()\ndef country_info (country, dt, reopen = None):\n    \n    by_date = country.groupby (['Date'])[['Confirmed', 'Deaths', 'Recovered', 'Active']].sum()\n    by_date = by_date.reset_index()\n    by_date = by_date[by_date.Date>=dt]\n    #print (len(by_date))\n\n    #print ('Rates for country\/region : ' + pd.unique(country['Country\/Region']))\n\n    #print (by_date)\n    \n    \n    # Add need fields\n    \n    by_date ['prior_confirmed'] = 0\n    by_date ['prior_deaths'] = 0\n    by_date ['prior_recovered'] = 0\n    by_date ['daily_confirmed'] = 0\n    by_date ['daily_deaths'] = 0\n    by_date ['daily_recovered'] = 0\n    p_confirmed = 0\n    p_deaths = 0\n    p_recovered = 0\n   \n    for i, row in by_date.iterrows():\n        #print (by_date.loc[i])\n        by_date.loc[i,'prior_confirmed'] = p_confirmed \n        by_date.loc[i,'prior_deaths'] = p_deaths \n        by_date.loc[i,'prior_recovered'] = p_recovered\n        p_confirmed = by_date.loc[i,'Confirmed']\n        p_deaths = by_date.loc[i,'Deaths']\n        p_recovered = by_date.loc[i,'Recovered']\n        \n        \n    \n    by_date ['delta_confirmed'] = by_date.Confirmed - by_date.prior_confirmed\n    by_date ['delta_deaths'] = by_date.Deaths - by_date.prior_deaths\n    by_date ['delta_recovered'] = by_date.Recovered - by_date.prior_recovered\n    \n    return by_date\n\nus_by_date = country_info(us,'2020-03-04',)\nchina_by_date = country_info(china,'2020-01-01',)\nhot_europe_by_date = country_info(hot_europe,'2020-02-20',) \nitaly_by_date = country_info(italy,'2020-02-20',)\nskorea_by_date = country_info(skorea,'2020-02-17')\niran_by_date = country_info(iran,'2020-02-23')\nspain_by_date = country_info(spain,'2020-02-23')\nrow['Country\/Region'] = 'Rest of World'\nrow_by_date = country_info(row,'2020-02-17')\nroc_by_date = country_info (roc, '2020-01-01')\ngermany_by_date = country_info (germany, '2020-02-23')\nuk_by_date = country_info (uk, '2020-02-23')\nfrance_by_date = country_info (france, '2020-02-23')\nswitzerland_by_date = country_info(switzerland,'2020-02-23')\nsingapore_by_date = country_info(singapore,'2020-01-23')\nsweden_by_date = country_info(sweden,'2020-02-25')\nnorway_by_date = country_info(norway,'2020-02-23')\naustria_by_date = country_info(austria,'2020-02-23')\ndenmark_by_date = country_info(denmark,'2020-02-23')\nindia_by_date = country_info(india,'2020-02-23')\njapan_by_date = country_info(japan,'2020-01-23')\nnz_by_date = country_info(nz,'2020-03-03')\nportugal_by_date = country_info(portugal,'2020-02-23')\nbelgium_by_date = country_info(belgium,'2020-02-23')\nnetherlands_by_date = country_info(netherlands,'2020-02-23')\ngreece_by_date = country_info(greece,'2020-02-23')\nbrazil_by_date = country_info(brazil,'2020-03-01')\n\nca['Country\/Region'] = 'California'\nny['Country\/Region'] = 'New York'\nwa['Country\/Region'] = 'Washington'\nca_by_state = country_info(ca,'2020-03-09')\nny_by_state = country_info(ny,'2020-03-09')\nwa_by_state = country_info(wa,'2020-03-09')\nco_by_state = country_info(co,'2020-03-09')\nnj_by_state = country_info(nj,'2020-03-09')\nla_by_state = country_info(la,'2020-03-09')\nma_by_state = country_info(ma,'2020-03-09')\nfl_by_state = country_info(fl,'2020-03-09')\nmi_by_state = country_info(mi,'2020-03-09')\nil_by_state = country_info(il,'2020-03-09')\nga_by_state = country_info(ga,'2020-03-09')\nms_by_state = country_info(ms,'2020-03-09')\nak_by_state = country_info(ak,'2020-03-16')\nmn_by_state = country_info(mn,'2020-03-09')\nmt_by_state = country_info(mt,'2020-03-11')\nok_by_state = country_info(ok,'2020-03-09')\nsc_by_state = country_info(sc,'2020-03-09')\ntn_by_state = country_info(tn,'2020-03-09')\nId_by_state = country_info(Id,'2020-03-12')\nut_by_state = country_info(ut,'2020-03-12')\nwy_by_state = country_info(wy,'2020-03-12')\nsd_by_state = country_info(sd,'2020-03-12')\nnd_by_state = country_info(nd,'2020-03-12')\nia_by_state = country_info(ia,'2020-03-12')\ntx_by_state = country_info(tx,'2020-03-12')\nal_by_state = country_info(al,'2020-03-12')\nme_by_state = country_info(me,'2020-03-12')\naz_by_state = country_info(az,'2020-03-12') \nar_by_state = country_info(ar,'2020-03-12')\nhi_by_state = country_info(hi,'2020-03-12')\nIn_by_state = country_info(In,'2020-03-12')\nks_by_state = country_info(ks,'2020-03-12')\nne_by_state = country_info(ne,'2020-03-12')\nnv_by_state = country_info(nv,'2020-03-12')\nnc_by_state = country_info(nc,'2020-03-12')\npa_by_state = country_info(pa,'2020-03-12')\nri_by_state = country_info(ri,'2020-03-12')\n\nnsh_by_state = country_info(nsh,'2020-03-09')\n","3805210f":"dict1 = {'United States':us_by_date,\n        'California':ca_by_state,\n        'Washington':wa_by_state,\n        'New York':ny_by_state,\n        'Colorado':co_by_state,\n        'New Jersey': nj_by_state,\n        'Louisiana': la_by_state,\n        'Massachusetts': ma_by_state,\n        'Florida': fl_by_state,\n        'Michigan': mi_by_state,\n        'Illinois': il_by_state,\n        'Georgia': ga_by_state,\n        'China':china_by_date,\n        'Rest of world -w\/o China':row_by_date,\n        'Hot European Countries':hot_europe_by_date,\n        'Italy':italy_by_date,   \n        'Iran':iran_by_date,\n        'South Korea':skorea_by_date,\n        'Spain':spain_by_date,\n        'France':france_by_date,\n        'Germany':germany_by_date,      \n        'United Kingdom':uk_by_date,\n        'Switzerland':switzerland_by_date,\n        'Sweden':sweden_by_date,\n        'Singapore':singapore_by_date,\n        'India':india_by_date,\n        'Japan':japan_by_date,\n\n        'Rest of China w\/o Hubei': roc_by_date,\n        }\n\ndict_reopen = {\n        'Sweden':[sweden_by_date,'2020-03-12',], \n        'Denmark':[denmark_by_date,'2020-04-14',], \n        'Austria':[austria_by_date,'2020-04-14',], \n        'Germany':[germany_by_date, '2020-04-20',], \n        'Norway': [norway_by_date,'2020-04-20',], \n        'Switzerland':[switzerland_by_date,'2020-04-27',], \n        'New Zealand':[nz_by_date,'2020-04-27',],\n        'Portugal':[portugal_by_date,'2020-05-04',],\n        'Belgium':[belgium_by_date,'2020-05-04',],\n        'Italy':[italy_by_date,'2020-05-04',],\n        'Spain':[spain_by_date,'2020-05-04',],\n        'Greece':[greece_by_date,'2020-05-04',],\n        'Brazil':[brazil_by_date,'2020-05-15',],\n        'Netherlands':[netherlands_by_date,'2020-05-11',],\n        'France':[france_by_date,'2020-05-11',],\n        'Hot European':[hot_europe_by_date,'2020-04-27',],\n        'US - All States':[us_by_date,'2020-05-08',],\n        'US - South Carolina':[sc_by_state,'2020-04-20',],\n        'US - Georgia':[ga_by_state,'2020-04-24',], \n        'US - Alaska':[ak_by_state,'2020-04-24',], \n        'US - Minnesota':[mn_by_state,'2020-04-27',],\n        'US - Montana':[mt_by_state,'2020-04-27',], \n        'US - Tennessee':[tn_by_state,'2020-04-27',], \n        'US - Mississippi':[ms_by_state,'2020-04-27',], \n        'US - Colorado':[co_by_state,'2020-05-01',],\n        'US - Oklahoma':[ok_by_state,'2020-05-01',], \n        'US - Idaho':[Id_by_state,'2020-05-01',], \n        'US - Utah':[ut_by_state,'2020-05-01',], \n        'US - Wyoming':[wy_by_state,'2020-05-01',],\n        'US - South Dakota':[sd_by_state,'2020-05-01',],\n        #'US - North Dakota':[nd_by_state,'2020-05-01',],  \n        'US - Iowa':[ia_by_state,'2020-05-01',], \n        'US - Texas':[tx_by_state,'2020-05-01',], \n        'US - Maine':[me_by_state,'2020-05-01',],\n        'US - Arizona':[az_by_state,'2020-05-08',], \n        'US - Arkansas':[ar_by_state,'2020-05-06',], \n        'US - Hawaii':[hi_by_state,'2020-05-06',], \n        'US - Indiana':[In_by_state,'2020-05-03',], \n        'US - Kansas':[ks_by_state,'2020-05-03',], \n        'US - Nebraska':[ne_by_state,'2020-05-03',], \n        'US - Nevada':[nv_by_state,'2020-05-09',], \n        'US - North Carolina':[nc_by_state,'2020-05-08',], \n        'US - Pennsylvania':[pa_by_state,'2020-05-08',],     \n        'US - Rhode Island':[ri_by_state,'2020-05-07',], \n        'US - No Stay Home': [nsh_by_state,'2020-03-15',], \n\n\n\n        }\n\ndict_reopenx = {\n        'Sweden':[sweden_by_date,'2020-02-23',], \n        #'Denmark':[denmark_by_date,'2020-04-14',], \n        #'Austria':[austria_by_date,'2020-04-14',], \n        #'Germany':[germany_by_date, '2020-04-20',], \n        #'Norway': [norway_by_date,'2020-04-20',], \n        #'Switzerland':[switzerland_by_date,'2020-04-27',], \n        'New Zealand':[nz_by_date,'2020-04-27',],\n        #'US - South Carolina':[sc_by_state,'2020-04-20',],\n        #'US - Georgia':[ga_by_state,'2020-04-24',], \n        'US - Alaska':[ak_by_state,'2020-04-24',], \n        #'US - Minnesota':[mn_by_state,'2020-04-27',],\n        #'US - Montana':[mt_by_state,'2020-04-27',], \n        #'US - Tennessee':[tn_by_state,'2020-04-27',], \n        #'US - Mississippi':[ms_by_state,'2020-04-27',], \n        #'US - Colorado':[co_by_state,'2020-05-01',],\n        #'US - Oklahoma':[ok_by_state,'2020-05-01',],     \n        #'US - No Stay Home': [nsh_by_state,'2020-04-01',], \n        #'Minnesota':minnesota_by_state,\n\n        }\n\nskip_R_calc = ['Sweden','New Zealand','Belgium','Spain','France','US - Alasxxka', 'US - Mississippi','US - Montana','US - Hawaii',]\n\ndict_sigmoid = {\n        'China':china_by_date,\n        'South Korea': skorea_by_date,\n        'Rest of China w\/o Hubei': roc_by_date,\n        'Rest of world -w\/o China':row_by_date,\n        'Iran':iran_by_date,\n        'Hot European Countries':hot_europe_by_date,\n        'Italy':italy_by_date,\n        'Switzerland':switzerland_by_date,\n        'Spain':spain_by_date,\n        'Germany':germany_by_date,\n        'France':france_by_date,\n        'United States':us_by_date,\n        'New York':ny_by_state,\n        'Washington':wa_by_state,\n        'California':ca_by_state,\n        'Colorado':co_by_state,\n        'Louisiana': la_by_state,\n        'Massachusetts': ma_by_state,\n        'Florida': fl_by_state,\n\n}\n\ndict_test = {\n        'Sweden':[sweden_by_date,'2020-02-23'],\n        'Denmark': [denmark_by_date,'2020-04-14'],\n        #'Minnesota':minnesota_by_state,\n}","522b4a31":"#import plotly.graph_objects as go\ndef plots_by_country (country, country_name, start_dt):\n\n    temp = country\n\n    # adding two more columns\n    temp['No. of Deaths to 100 Confirmed Cases'] = round(temp['Deaths']\/temp['Confirmed'], 3)*100\n    temp['No. of Recovered to 100 Confirmed Cases'] = round(temp['Recovered']\/temp['Confirmed'], 3)*100\n    # temp['No. of Recovered to 1 Death Case'] = round(temp['Recovered']\/temp['Deaths'], 3)\n    #print (temp)\n\n    \n    #print (temp.iloc[13]['Date'])\n    last_date = temp.iloc[len(temp)-1]['Date']\n    death_rate = temp[temp.Date ==last_date]['No. of Deaths to 100 Confirmed Cases']\n    recovered_rate = temp[temp.Date ==last_date]['No. of Recovered to 100 Confirmed Cases']\n    temp = temp.melt(id_vars='Date', value_vars=['No. of Deaths to 100 Confirmed Cases', 'No. of Recovered to 100 Confirmed Cases'], \n                     var_name='Ratio', value_name='Value')\n\n    #str(full_table.loc[len(full_table)-1]['Date'])\n    #fig = go.Figure()\n    fig = px.line(temp, x=\"Date\", y=\"Value\", color='Ratio', log_y=True, width=1000, height=700,\n                  title=country_name + ' Recovery and Mortality Rate Over Time', color_discrete_sequence=[dth, rec])\n    fig.add_annotation(\n            x=start_dt,\n            y=0,\n            text=\"Reopen\")\n    \n    fig.show()\n    return death_rate, recovered_rate\n        \nrates = []\nfor (key, [value,start_dt]) in dict_reopen.items():\n    print (start_dt)\n    death_rate, recovered_rate  = plots_by_country (value,key, start_dt)\n\n    ","d2d830d5":"def get_smoothed (value):\n    return value.rolling(7,\n        win_type='gaussian',\n        min_periods=1,\n        center=True).mean(std=2).round()","79dd4514":"def plots_of_daily (country, country_name, start_dt, attribute):\n\n    temp = country\n    #print (temp.columns)\n    temp.columns = ['Date', 'Confirmed', 'Deaths', 'Recovered', 'Active', 'prior_confirmed',\n       'prior_deaths', 'prior_recovered', 'daily_confirmed', 'daily_deaths',\n       'daily_recovered', 'New Daily Confirmed', 'delta_deaths', 'delta_recovered',\n       'No. of Deaths to 100 Confirmed Cases',\n       'No. of Recovered to 100 Confirmed Cases']\n    #print (temp.iloc[13]['Date'])\n    last_date = temp.iloc[len(temp)-1]['Date']\n\n    smoothed_daily = temp[attribute].rolling(7,\n        win_type='gaussian',\n        min_periods=1,\n        center=True).mean(std=2).round()\n\n    #str(full_table.loc[len(full_table)-1]['Date'])\n    \n\n    fig = px.line(temp, x=\"Date\", y=attribute, log_y=False, width=800, height=800, \n                  title=country_name + ' ' + attribute ,color_discrete_sequence=[dth, rec])\n    fig.add_scatter(x=temp.Date, y=smoothed_daily, mode='lines')\n    fig.add_annotation(\n            x= start_dt,\n            y=5,\n            text=\"Reopen date\")\n    \n\n\n    fig.update_layout(showlegend=False)\n    fig.show()\n\n        \nrates = []\nfor (key, [value,start_dt]) in dict_reopen.items():\n    \n    plots_of_daily (value,key,start_dt, 'New Daily Confirmed',)\n    plots_of_daily (value,key,start_dt, \"Active\",)","22b64f0a":"import pylab\nimport datetime\nfrom scipy.optimize import curve_fit\n\ndef sigmoid(x, x0, k, ymax):\n     y = ymax \/ (1 + np.exp(-k*(x-x0)))\n     return y\n\ndef exp (x,a,b):\n    y = a* np.exp(x*b)\n    return y\n\ndef gaussian(x, a, x0, sigma):\n    return a*np.exp(-(x-x0)**2\/(2*sigma**2))\n\ndef growth_rate_over_time (f, country, attribute, title):\n    ydata = country[attribute]\n    \n\n    xdata = list(range(len(ydata)))\n\n    rates = []\n    for i, x in enumerate(xdata):\n        if i > 2:\n#            print (xdata[:x+1])\n#            print (ydata[:x+1])\n\n            popt, pcov = curve_fit(f, xdata[:x+1], ydata[:x+1],)\n            if popt[1] < 0:\n                rates.append (0.0)\n            else:    \n                rates.append (popt[1])\n    rates = np.array(rates) \n    pylab.style.use('dark_background')\n    pylab.figure(figsize=(12,8))\n    xdata = np.array(xdata)\n    #pylab.grid(True, linestyle='-', color='0.75')\n    pylab.plot(xdata[3:]+1, 100*rates, 'o', linestyle='solid', label=attribute)\n    #if fit_good:\n    #    pylab.plot(x,y, label='fit')\n    #pylab.ylim(0, ymax*1.05)\n    #pylab.legend(loc='best')\n    pylab.xlabel('Days Since Start')\n    pylab.ylabel('Growth rate percentage ' + attribute)\n    pylab.title(title + attribute, size = 15)\n    pylab.show()\n    \n        \n    \n\ndef plot_curve_fit (f, country, attribute, title, start_dt, normalize = False, curve = 'Exp',):\n    #country = country[10:]\n    first_dt = min(country.Date)\n    fmt = '%Y-%m-%d'\n    #d1 = datetime.datetime.strptime(first_dt,fmt)\n    d2 = datetime.datetime.strptime(start_dt,fmt)\n    d = d2-first_dt\n    print (d.days)\n    fit_good = True\n    ydata = country[attribute]\n    #ydata = np.array(ydata)\n    xdata = range(len(ydata))\n    mu = np.mean(ydata)\n    sigma = np.std(ydata)\n    ymax = np.max(ydata)    \n    if normalize:\n        ydata_norm = ydata\/ymax\n    else:\n        ydata_norm = ydata\n    #f = sigmoid\n    try:\n        if curve == 'Gauss': # pass the mean and stddev\n            popt, pcov = curve_fit(f, xdata, ydata_norm, p0 = [1, mu, sigma])\n        elif curve == 'Sigmoid':\n            popt, pcov = curve_fit(f, xdata, ydata_norm, bounds = ([0,0,0],np.inf),maxfer=1000)\n        else:    \n            popt, pcov = curve_fit(f, xdata, ydata_norm,)\n    except RuntimeError:\n        print ('Exception - RuntimeError - could not fit curve')\n        fit_good = False\n    else:\n\n        fit_good = True\n        \n    if fit_good:\n        if curve == 'Exp':\n            if popt[1] < 0.9: # only print if we have a growth rate\n                \n                print (key + ' -- Coefficients for y = a * e^(x*b)  are ' + str(popt))\n                print ('Growth rate is now ' + str(round(popt[1],2)))\n                print ('...This doubles in ' + str (round(0.72\/popt[1] , 1) ) +' days')\n            else:\n                fit_good = False\n        elif curve == 'Gauss':\n            print (key + ' -- Coefficients are ' + str(popt))\n        else:   # sigmoid \n            print (key + ' -- Coefficients for y = 1\/(1 + e^(-k*(x-x0)))  are ' + str(popt))\n            \n        if fit_good:\n            print ('Mean error for each coefficient: ' + str(np.sqrt(np.diag(pcov))\/popt))\n    else:\n        print (key + ' -- Could not resolve coefficients ---')\n    x = np.linspace(-1, len(ydata), 100)\n    if fit_good:\n        y = f(x, *popt)\n        if normalize:\n            y = y * ymax\n        plt.style.use('dark_background')\n        pylab.figure(figsize=(15,12)) \n        #pylab.grid(True, linestyle='-', color='0.75')\n        pylab.plot(xdata, ydata, 'o', label=attribute)\n        #if fit_good:\n        pylab.plot(x,y, label='fit')\n        pylab.axvline(x=d.days)\n        pylab.ylim(0, ymax*1.05)\n        pylab.legend(loc='best')\n        pylab.xlabel('Days Since Start')\n        pylab.ylabel('Number of ' + attribute)\n        pylab.title(title + attribute, size = 15)\n        pylab.show()\n","b1931e22":"round (72\/35,2)","3a955149":"if False:\n  for (key, [value,start_dt]) in dict_reopen.items():\n\n    if key in [\"China\",'Rest of China w\/o Hubei']:\n        pass\n    else:\n        print (start_dt)\n        plot_curve_fit (exp, value, 'Confirmed', key + ' - Growth Curve for ',start_dt,False,'Exp')\n        plot_curve_fit (exp, value, 'Deaths', key + ' - Growth Curve for ',start_dt,False,'Exp')\n        plot_curve_fit (exp, value, 'Recovered', key + ' - Growth Curve for ',start_dt,False,'Exp')","a34ae476":"for (key, [value,start_dt]) in dict_reopen.items():\n    plot_curve_fit (sigmoid, value, 'Confirmed', key + ' - Logistic Growth Curve for ',start_dt,True,'Logistic')\n    plot_curve_fit (sigmoid, value, 'Deaths', key + ' - Logistic Growth Curve for ',start_dt,True,'Logistic',)\n    #plot_curve_fit (sigmoid, value, 'Recovered', key + ' - Logistic Growth Curve for ',True,'Logistic',start_dt)","15af025c":"\ndef highest_density_interval_slow(pmf, p=.9):\n    # If we pass a DataFrame, just call this recursively on the columns\n    if(isinstance(pmf, pd.DataFrame)):\n        return pd.DataFrame([highest_density_interval(pmf[col], p=p) for col in pmf],\n                            index=pmf.columns)\n    \n    cumsum = np.cumsum(pmf.values)\n    best = None\n    for i, value in enumerate(cumsum):\n        for j, high_value in enumerate(cumsum[i+1:]):\n            if (high_value-value > p) and (not best or j<best[1]-best[0]):\n                best = (i, i+j+1)\n                break\n            \n    low = pmf.index[best[0]]\n    high = pmf.index[best[1]]\n    return pd.Series([low, high], index=[f'Low_{p*100:.0f}', f'High_{p*100:.0f}'])\n\n#hdi = highest_density_interval(posteriors)\n#hdi.tail()","64975262":"def highest_density_interval(pmf, p=.9, debug=False):\n   \n    # If we pass a DataFrame, just call this recursively on the columns\n    if(isinstance(pmf, pd.DataFrame)):\n        return pd.DataFrame([highest_density_interval(pmf[col], p=p) for col in pmf],\n                            index=pmf.columns)\n    cumsum = np.cumsum(pmf.values)\n    \n    # N x N matrix of total probability mass for each low, high\n    total_p = cumsum - cumsum[:, None]\n    \n    # Return all indices with total_p > p\n    lows, highs = (total_p > p).nonzero()\n    \n    # Find the smallest range (highest density)\n    best = (highs - lows).argmin()\n    \n    low = pmf.index[lows[best]]\n    high = pmf.index[highs[best]]\n    \n    return pd.Series([low, high],\n                     index=[f'Low_{p*100:.0f}',\n                            f'High_{p*100:.0f}'])\n\n","ac955104":"from scipy import stats as sps\nfrom scipy.interpolate import interp1d\n# Column vector of k\nk = np.arange(0, 70)[:, None]\n\n# Different values of Lambda\nlambdas = [10, 20, 30, 40]\n\n# Evaluated the Probability Mass Function (remember: poisson is discrete)\ny = sps.poisson.pmf(k, lambdas)\n","575e5167":"fig, ax = plt.subplots(figsize=(10,5))\nplt.style.use('ggplot')\nax.set(title='Poisson Distribution of Cases\\n $p(k|\\lambda)$')\n\nplt.plot(k, y,\n         marker='o',\n         markersize=3,\n         lw=0)\n\nplt.legend(title=\"$\\lambda$\", labels=lambdas);","771365a4":"R_T_MAX = 12\nr_t_range = np.linspace(0, R_T_MAX, R_T_MAX*100+1)\nGAMMA = 1\/7\ndef get_posteriors(sr, sigma=0.15):\n\n    # (1) Calculate Lambda\n    lam = sr[:-1].values * np.exp(GAMMA * (r_t_range[:, None] - 1))\n\n    \n    # (2) Calculate each day's likelihood\n    likelihoods = pd.DataFrame(\n        data = sps.poisson.pmf(sr[1:].values, lam),\n        index = r_t_range,\n        columns = sr.index[1:])\n    \n    # (3) Create the Gaussian Matrix\n    process_matrix = sps.norm(loc=r_t_range,\n                              scale=sigma\n                             ).pdf(r_t_range[:, None]) \n\n    # (3a) Normalize all rows to sum to 1\n    process_matrix \/= process_matrix.sum(axis=0)\n    \n    # (4) Calculate the initial prior\n    #prior0 = sps.gamma(a=4).pdf(r_t_range)\n    prior0 = np.ones_like(r_t_range)\/len(r_t_range)\n    prior0 \/= prior0.sum()\n\n    # Create a DataFrame that will hold our posteriors for each day\n    # Insert our prior as the first posterior.\n    posteriors = pd.DataFrame(\n        index=r_t_range,\n        columns=sr.index,\n        data={sr.index[0]: prior0}\n    )\n    \n    # We said we'd keep track of the sum of the log of the probability\n    # of the data for maximum likelihood calculation.\n    log_likelihood = 0.0\n\n    # (5) Iteratively apply Bayes' rule\n    for previous_day, current_day in zip(sr.index[:-1], sr.index[1:]):\n\n        #(5a) Calculate the new prior\n        current_prior = process_matrix @ posteriors[previous_day]\n        \n        #(5b) Calculate the numerator of Bayes' Rule: P(k|R_t)P(R_t)\n        numerator = likelihoods[current_day] * current_prior\n        \n        #(5c) Calcluate the denominator of Bayes' Rule P(k)\n        denominator = np.sum(numerator)\n        \n        # Execute full Bayes' Rule\n        if denominator == 0:\n            posteriors[current_day] = 0\n        else:    \n            posteriors[current_day] = numerator\/denominator\n        \n        # Add to the running sum of log likelihoods\n        log_likelihood += np.log(denominator)\n    \n    return posteriors, log_likelihood\n","250deb16":"# Note that we're fixing sigma to a value just for the example\n\nsmoothed = get_smoothed(germany_by_date['New Daily Confirmed'])\ndates = germany_by_date[['Date']]\n\n\nposteriors, log_likelihood = get_posteriors(smoothed, sigma=.25)\nposteriors.columns = [dates.Date]\n","57166aa5":"state_name = 'Germany'\n\nif True:\n    plt.style.use('classic')\n\n    ax = posteriors.plot(title=f'{state_name} - Daily Posterior for $R_t$',\n           legend=False, \n           lw=1,\n           c='k',\n           alpha=.3,\n           xlim=(0.4,6))\n\n    ax.set_xlabel('$R_t$');\n","ea3a24d2":"# Note that this takes a while to execute - it's not the most efficient algorithm\nhdis = highest_density_interval(posteriors, p=.9)\n\nmost_likely = posteriors.idxmax().rename('ML')\n\n# Look into why you shift -1\nresult = pd.concat([most_likely, hdis], axis=1)\n\nresult.tail(10)","45ad3b77":"#dict_reopen.get(state_name)[1]","52cc1208":"from matplotlib.dates import date2num, num2date\nfrom matplotlib import dates as mdates\nfrom matplotlib import ticker\nfrom matplotlib.colors import ListedColormap\nfrom matplotlib.patches import Patch\n\ndef plot_rt(result, ax, state_name):\n    \n    ax.set_title(f\"{state_name}\")\n    \n    # Colors\n    ABOVE = [1,0,0]\n    MIDDLE = [1,1,1]\n    BELOW = [0,0,1]\n    cmap = ListedColormap(np.r_[\n        np.linspace(BELOW,MIDDLE,25),\n        np.linspace(MIDDLE,ABOVE,25)\n    ])\n    color_mapped = lambda y: np.clip(y, .5, 1.5)-.5\n    \n    index = result['ML'].index.get_level_values('Date')\n    values = result['ML'].values\n    \n    # Plot dots and line\n    ax.plot(index, values, c='k', zorder=1, alpha=.25)\n    ax.scatter(index,\n               values,\n               s=40,\n               lw=.5,\n               c=cmap(color_mapped(values)),\n               edgecolors='k', zorder=2)\n    \n    # Aesthetically, extrapolate credible interval by 1 day either side\n    lowfn = interp1d(date2num(index),\n                     result['Low_90'].values,\n                     bounds_error=False,\n                     fill_value='extrapolate')\n    \n    highfn = interp1d(date2num(index),\n                      result['High_90'].values,\n                      bounds_error=False,\n                      fill_value='extrapolate')\n    \n    extended = pd.date_range(start=pd.Timestamp('2020-03-01'),\n                             end=index[-1]+pd.Timedelta(days=1))\n    \n    ax.fill_between(extended,\n                    lowfn(date2num(extended)),\n                    highfn(date2num(extended)),\n                    color='k',\n                    alpha=.1,\n                    lw=0,\n                    zorder=3)\n\n    ax.axhline(1.0, c='k', lw=1, label='$R_t=1.0$', alpha=.25);\n    \n    # Formatting\n    ax.xaxis.set_major_locator(mdates.MonthLocator())\n    ax.xaxis.set_major_formatter(mdates.DateFormatter('%b'))\n    ax.xaxis.set_minor_locator(mdates.DayLocator())\n    \n    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n    ax.yaxis.set_major_formatter(ticker.StrMethodFormatter(\"{x:.1f}\"))\n    ax.yaxis.tick_right()\n    ax.spines['left'].set_visible(False)\n    ax.spines['bottom'].set_visible(False)\n    ax.spines['right'].set_visible(False)\n    ax.margins(0)\n    ax.grid(which='major', axis='y', c='k', alpha=.1, zorder=-2)\n    ax.margins(0)\n    ax.set_ylim(0.0, 4.0)\n    ax.set_xlim(pd.Timestamp('2020-03-15'), result.index.get_level_values('Date')[-1]+pd.Timedelta(days=1))\n    ax.set_title(f'Real-time $R_t$ for {state_name}')\n    ax.xaxis.set_major_locator(mdates.WeekdayLocator())\n    ax.xaxis.set_major_formatter(mdates.DateFormatter('%b %d'))\n    ax.axvline(pd.to_datetime(dict_reopen.get(state_name)[1]))\n    fig.set_facecolor('w')\n\n    \nfig, ax = plt.subplots(figsize=(600\/72,400\/72))\n\nplot_rt(result, ax, state_name)\n\n","72aeeeaf":"from IPython.display import clear_output\nsigmas = np.linspace(1\/20, 1, 20)\n\nresults = {}\nfor (key, [value,start_dt]) in dict_reopen.items():\n    smoothed = get_smoothed(value['New Daily Confirmed'])\n    dates = value[['Date']]\n    state_name = key\n    if state_name in skip_R_calc:\n        pass\n    else:\n        print(state_name)\n        result = {}\n\n        # Holds all posteriors with every given value of sigma\n        result['posteriors'] = []\n\n        # Holds the log likelihood across all k for each value of sigma\n        result['log_likelihoods'] = []\n\n        for sigma in sigmas:\n            posteriors, log_likelihood = get_posteriors(smoothed, sigma=sigma)\n            posteriors.fillna(0)\n            posteriors.columns = [dates.Date]\n            result['posteriors'].append(posteriors)\n            result['log_likelihoods'].append(log_likelihood)\n\n        # Store all results keyed off of state name\n        results[state_name] = result\n        clear_output(wait=True)\n\nprint('Done.')","bba0920a":"log_likelihood","a9834ee9":"#results['New Zealand']","9316198e":"# Each index of this array holds the total of the log likelihoods for\n# the corresponding index of the sigmas array.\ntotal_log_likelihoods = np.zeros_like(sigmas)\n\n# Loop through each state's results and add the log likelihoods to the running total.\nfor state_name, result in results.items():\n    total_log_likelihoods += result['log_likelihoods']\n\n# Select the index with the largest log likelihood total\nmax_likelihood_index = total_log_likelihoods.argmax()\n\n# Select the value that has the highest log likelihood\nsigma = sigmas[max_likelihood_index]\n\n# Plot it\nfig, ax = plt.subplots()\nax.set_title(f\"Maximum Likelihood value for $\\sigma$ = {sigma:.2f}\");\nax.plot(sigmas, total_log_likelihoods)\nax.axvline(sigma, color='k', linestyle=\":\");","237e16f7":"spain_by_date.tail(10)","433e15fb":"final_results = None\n\nfor state_name, result in results.items():\n    print(state_name)\n    #print(result)\n    posteriors = result['posteriors'][max_likelihood_index]\n    #print (len(state_name))\n    hdis_90 = highest_density_interval(posteriors, p=.9)\n    hdis_50 = highest_density_interval(posteriors, p=.5)\n    most_likely = posteriors.idxmax().rename('ML')\n    \n    result = pd.concat([most_likely, hdis_90, hdis_50], axis=1)\n    result['State'] = state_name\n    #print (result.columns)\n    if final_results is None:\n        final_results = result\n    else:\n        final_results = pd.concat([final_results, result])\n    clear_output(wait=True)\n\nprint('Done.')","b4c8f7ef":"cols = ['ML', 'Low_90', 'High_90', 'Low_50', 'High_50',]","c883281e":"ncols = 2\nnrows = int(np.ceil(len(results) \/ ncols))\n\nfig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(15, nrows*3))\n\nfor i, (state_name, result) in enumerate(final_results.groupby('State')):\n#for i, (state_name, result) in enumerate(final_results):  \n    #print (result)\n    plot_rt(result[1:][cols], axes.flat[i], state_name)\n\n    fig.tight_layout()\n    fig.set_facecolor('w')","bc455381":"Next, let's review some of the grow curves.\n\n","1f456a24":"# Libraries","68fa5e68":"### Install","fd2c3aca":"## Examining the Growth Curves\n\nThese distributions start off exponentially, but eventually become a logistic curve. We can plot them both ways, and then fit a non-linear regression to the curve to determine the rate.\n\nFirst we look at mortality curves. The trend to what for is an increasing mortality curve. This means that medical treatments are not controlling the virus well. This is true in Italy, which has an older population and seemed to be slow to respond in social distancing efforts. Compare Italy to South Korea, which had an agressive testing and treatment program, we see that Italy has a severe virus growth situation.\n\n### What these curves show\n\nThere are several groups of curves shown. They show:\n\n* Death and recovery rates for each region - these are on a log scale and show rates of death and recovery per confirmed cases \n* Growth rate over time - this shows the daily growth rate for each region \n* Exponential growth for each region - there are separate plots for confirmed cases, deaths, and recovered\n* Logistic growth curves - these are for only the countries that have reached an inflection point\n* Gaussian (Normal) curves - these are an approximation of the derivative of the logistic curve, which is the number of daily new cases over time \n\nThe growth and normal curves also have the coefficents and errors for each coeffients. The second coefficient is the growth rate.\n\nYou may observe several countries\/regions where the daily arrival rates are to the right of the predicted curve. This is a good signal that the growth rate might be reaching an inflection point. Once this point is reached, the infection point, the growth rate will slow down, and the curve will be S-shaped, a sigmoid curve. This is a very good signal!\n\nThe infection point generally indicates that 50 percent of the cummulative cases have been reached.","62e4881c":"### Import Libraries","8d7054a3":"## Most Recent Update","36f08dad":"# Exploration of $R_t$ - Effective Reproduction Rate\n\nThis is a key indicator in letting us know when it is 'safe' to resume somewhat normal activities. As long as the number is above 1, it means the growth rate of the virus is still growing. \n\nKeven Systrom uses a Bayes' rule approach to predict the value for R. The value for $R_t$ is a function of the value of yesterday's value $R_{t-1}$. His work is based on a method in a paper by [Bettencourt & Ribeiro 2008](https:\/\/journals.plos.org\/plosone\/article?id=10.1371\/journal.pone.0002185) to estimate R.\n\nAt this point, I will skip the probability theory behind this estimate and get to the actual algorithm.\n\n\n## A Simple Poisson Arrival Model\n\nWe need a likelihood function which identifies how likely we are to see $k$ new cases, given a value of $R_t$.\n\nStatisticians and operation research people usually model 'arrivals' (in our case, new cases) over some time period of time with [Poisson Distribution](https:\/\/en.wikipedia.org\/wiki\/Poisson_distribution). \n\nGiven an average arrival rate of $\\lambda$ new cases per day, the probability of seeing $k$ new cases is distributed according to the Poisson distribution:\n\n$$P(k|\\lambda) = \\frac{\\lambda^k e^{-\\lambda}}{k!}$$","03b75861":"## Choosing the optimal $\\sigma$\n\nIn the previous section we described choosing an optimal $\\sigma$, but we just assumed a value. But now that we can evaluate each state with any sigma, we have the tools for choosing the optimal $\\sigma$.\n\nAbove we said we'd choose the value of $\\sigma$ that maximizes the likelihood of the data $P(k)$. Since we don't want to overfit on any one state, we choose the sigma that maximizes $P(k)$ over every state. To do this, we add up all the log likelihoods per state for each value of sigma then choose the maximum.\n","6b4aa7ac":"## New Daily Cases\n\nThis graph shows only the new cases on a daily basis. As long as this curve is still rising, we haven't reached an inflection point.","55c1d45a":"The above shows the distributions for each value of lambda. The y axis shows the probability that a new case will arrive over time, given each lambda value.","29e182ac":"# Dataset","f2abbed2":"It is interesting that Germany dropped to below 1 in late March, but then bounces back above several times. Since April 20, the R rate has been rising and now is around 1.3. This is something to be watched. The increase may be due to either a resurgance of the virus or increased testing.","61d40bf6":"https:\/\/www.kaggle.com\/imdevskp\/mers-outbreak-analysis  \nhttps:\/\/www.kaggle.com\/imdevskp\/sars-2003-outbreak-analysis  \nhttps:\/\/www.kaggle.com\/imdevskp\/western-africa-ebola-outbreak-analysis\n","d5fffa7a":"\n## Logistic Growth Curves\n\nHere are logistic growth curves of the reopened regions. The line indicates when the reopening occurred.","9ab6aa8a":"Let's plot these values over time so we can see the evolving $R_t$ values.","49472b3d":"This graph shows that early days have a much higher R value and flatter curve. As time goes on, the distributions become more peaked and closer to 1.\n\nWe also need to calculate a most likely value and low-high density probabilities.\n\n","15110288":"### Growth Rates of Confirmed Cases and Deaths \n\nThere are three graphs in this section which show exponential growth rate of confirmed, deaths, and recovered. The head shows the current growth rate.\n\nYou can use the rule of 72 to find the doubling rate. As of March 20th, the confirmed growth rate for the United States is around 0.35. That means that the number of confirmed cases will double in just 2 days. *( 72\/35 = 2.06 )*","4f1baa8c":"# Plotting Continuous $R_t$ Values for Reopened Countries and States\n\nWe now plot the calculated **R** values for our select re-opened areas. ","886d3f6f":"We also need to find the highest density intervals.","2e1777da":"## COVID-19 - Keeping an Eye on Reopening of Countries and States\n\n\nThis notebook is intended to serve as a watchdog of the various partial economic reopenings occurring around the world. With the arrival rate of new cases dropping in almost every country, many countries are relaxing social distancing practices and allowing the reopening of local businesses. This is an exciting and nervous time - no one wants another hot reignition of the spread of the virus.\n\nIn addition to monitoring growth rates in the re-opened countries and states, a new section of the notebook now calculates the reproduction rate $R_t$, which is a key indicator of the safety of reopening. \n\nThis notebook is based on my prior effort which attempted to identify inflection points in the grow curves:\nhttps:\/\/www.kaggle.com\/wjholst\/covid-19-growth-patterns-in-critical-countries\n\n\n\n\n## Background\n\nI think it is important for everyone to understand the nature of the growth patterns of pandemics. There is an excellent Youtube video from [3Blue1Brown](https:\/\/www.youtube.com\/channel\/UCYO_jab_esuFRV4b17AJtAw) that offers a great explanation.\n\n### Understanding Growth Video Link\n\n![image.png](attachment:image.png)\n\nhttps:\/\/www.youtube.com\/watch?v=Kas0tIxDvrg&t=35s","b07a74b3":"# Preprocessing","978c69e4":"## Smoothing the Curve\n\nWe observe that the curves for some countries are very choppy. This may because of differing reporting mechanisms for each country or state. So it is necessary to smooth this out. A 7 day Gaussian smoothing algorithm in the graphs above removes most of that choppy behavior.\n\n","3e84564e":"# Purpose of This Document\n\nThis document will maintain a dynamic list of countries and states as they reopen local businesses. It will track the daily and logistic growth patterns to identify any flair-ups of the virus.\n\nIn addition to the visual tracking of new cases, active cases, and growth rates, this notebook now incorporates the calculation of the effective reproduction number R. This is a key variable to track in determining when the situation is really safe to reopen public venues. We will track changes in R over time. As R becomes < 1, it is much safer to resume normal activities, because the virus will not spread. \n\nThe addition of the R modeling is inspired by the work from Kevin Systrom, **Estimating COVID-19's $R_t$ in Real-Time** at https:\/\/github.com\/k-sys\/covid-19\/blob\/master\/Realtime%20R0.ipynb\n\n\n## Starting Observations\n\nAs of April 22, 2020, there are about 6 European countries that have some form of reopening in place.\n* Denmark - reopened daycare and primary schools on April 14. April 20 saw reopening of hairdressers, tattooists, and psychologists.\n* Austria - April 14 non-essintial shops with floor space < 400 square meters. May 1, this extended to shopping centers.\n* Norway - April 20 - kindergartens. Primary and some high schools on April 27, along with universities, beauty salons. Domestic travel is allowed but discouraged.\n* Germany - April 20, shops < 800 square meters, along with car showrooms, bookstores, and bicycle shops. Schools on May 4th.\n* Switzerland - April 27 hairdressers, hardware stores, beauty salons and flower shops.  Also non-essential medical care. \n* Sweden - Never closed down so it is included as a reference point.\n* Georgia - First US state to reopen. It includes hair and nail salons, barber shops, massage businesses and gyms.\n* New Zealand - Will partially reopen 4\/27. They had one of the most agressive lock-downs of anywhere and have very low case and death rates. \n* Several US states, Colorado, Tennessee, South Carolina, Alaska, Oklahoma, Montana, Minnesota, Alabama. These are part of an early phase I reopening.\n* Many more European countries opening on 5\/4.\n\nIn the next two weeks, several US states are scheduled to reopen. They will be added as the reopening occurs.\n\n## Observation Log\n\n* 2020-04-26 - European reopening countries are all ok, although Germany and Denmark's R value has moved above 1. Sweden still shows increasing cases; they probably have not reached an inflection point.\n* 2020-04-27 - Alaska, Sweden, and Montana still have errors in R calculations. Germany and Denmark have both dropped below 1. South Carolina has risen above 1 since the reopen, so we should watch it. Sweden's new case rate has turned, so the R calculations may work.\n* 2020-04-29 - Minnesota still has positive > R values. Their growth curve has not peaked, so this is a state to watch. South Carolina has dropped below 1. The R value seems to be a weekly cyclical pattern.\n* 2020-05-01 - Minnesota shows no improvement - R values hover around 2. Sweden and Germany both have a weekly cyclical pattern, hovering around R = 1. \n* 2020-05-05 - Minnesota has now dropped to around 1.\n* 2020-05-08 - Germany, Portugual, and Maine have all shown strong uptick in R values. They should be watched.\n* 2020-05-10 - The 3 regions above have dropped back. South Dakota shows an increase in new cases and R, probably due to a new testing site near the meat packing plant. In general, the R calculation still has some bias due to testing increases. There is a revision in Kevin Systrom's model which softens the testing increase that I will look at using.\n* 2020-05-11 - Germany, Portugal, and Maine are all back under 1. South Dakota is trending down, so the issue there was testing increase. There is no evidence of another meat packing plant outbreak.\n* 2020-05-12 - Arkansas, one on the newly added states, has showing upward movement in R for more than a week. Keep an eye on that state.\n* 2020-05-15 - On https:\/\/rt.live  only 2 states, Wyoming and Minnesota, show R values > 1. Austria and Arizona have shown a steady increase in R, so they should be watched.  \n* 2020-05-18 - Minnesota is the only state which is still above 1, but it may be trending down. Another interesting observation is that the high density interval (HDI) for the collective no-lockdown states is very narrow, which implies that those states now are truely below an R value of 1.\n* 2020-05-22 - Arkansas and Maine both show steady uptrends in R. This many be due to increased testing. We should watch them over the next week.\n* 2020-05-24 - Arkansas, Maine, and Georgia still show increasing R. They should still be monitored.\n* 2020-06-08 - Sweden and several US states all showing increasing R values.\n\n## Change History\n\n* 2020-03-18 - Addressed a problem with some of the curve fitting not converging. Because some of the countries, like the US, had a long period of days with no increases of cases, the tracking start date.\n* 2020-03-18 - Added US \"hot\" states, NY, CA, and WA. Also added Germany, which has shown rapid recent growth.\n* 2020-03-19 - Added Colorado, per friend request. Also added France and 2 high density countries, Monaco and Singapore\n* 2020-03-20 - Removed Monaco, not enough cases\n* 2020-03-21 - Added Switzerland, New Jersey, Louisiana, and 12 'hot' European countries as a group\n* 2020-03-22 - Added United Kingdom and UK to hot European group\n* 2020-03-23 - Changed South Korea extract, due to a data change in source; moved Iran to the logistic curve section;\n* 2020-03-24 - Changed dataset source due to issues with corona-virus-report\/covid_19_clean_complete.csv; United Kingdom is called UK on this dataset\n* 2020-03-27 - Added more US states: Massachusetts, Florida, Michigan, Illinois. Add new cases tracking graph. Removed Iran from logistic graph. \n* 2020-03-30 - Added Sweden to country tracking because they are not enforcing any social distancing rules. Also added India because of population size.\n* 2020-03-31 - Moved Italy, Spain, Hot European, and New York to the logistic plot.\n* 2020-04-02 - Added Washington to logistic plot. Corrected error with negative growth rates.\n* 2020-04-05 - Added Germany, California, Washington to logistic plot. Corrected error with negative growth rates.\n* 2020-04-06 - Added Louisiana, Massachusetts, Florida, and the rest of the world without China to logistic plots.\n* 2020-04-08 - Added United States to logistic plots.\n* 2020-04-22 - Converted this to a reopening tracker\n* 2020-04-22 - Added Rt tracking\n* 2020-04-26 - Added Georgia and New Zealand\n* 2020-04-27 - Added more US states\n* 2020-05-05 - Added more European countries\n* 2020-05-12 - Added more states; added separate date for shelter ends\n* 2020-05-21 - Added composite for US and hot European countries\n* 2020-06-09 - Added Brazil due to alarming increases in case counts\n\n\n## About Coronavirus\n\n* Coronaviruses are **zoonotic** viruses (means transmitted between animals and people).  \n* Symptoms include from fever, cough, respiratory symptoms, and breathing difficulties. \n* In severe cases, it can cause pneumonia, severe acute respiratory syndrome (SARS), kidney failure and even death.\n* Coronaviruses are also asymptomatic, means a person can be a carrier for the infection but experiences no symptoms\n\n## Novel coronavirus (nCoV)\n* A **novel coronavirus (nCoV)** is a new strain that has not been previously identified in humans.\n\n## COVID-19 (Corona Virus Disease 2019)\n* Caused by a **SARS-COV-2** corona virus.  \n* First identified in **Wuhan, Hubei, China**. Earliest reported symptoms reported in **November 2019**. \n* First cases were linked to contact with the Huanan Seafood Wholesale Market, which sold live animals. \n* On 30 January the WHO declared the outbreak to be a Public Health Emergency of International Concern ","a365a459":"# Acknowledgements\n\nThis effort was inspired by an excellent Youtube video from [3Blue1Brown](https:\/\/www.youtube.com\/channel\/UCYO_jab_esuFRV4b17AJtAw)\n\n* Video - https:\/\/www.youtube.com\/watch?v=Kas0tIxDvrg&t=35s \n* Starting kernel - https:\/\/www.kaggle.com\/imdevskp\/covid-19-analysis-viz-prediction-comparisons\n* https:\/\/github.com\/CSSEGISandData\/COVID-19\n* https:\/\/arxiv.org\/ftp\/arxiv\/papers\/2003\/2003.05681.pdf\n* https:\/\/github.com\/k-sys\/covid-19\/blob\/master\/Realtime%20R0.ipynb\n\n","08513600":"## Function for Calculating the Posteriors\n\nThis taken verbatim from Keven Systrom's great notebook. His algorithm is as follows:\n\nTo calculate the posteriors we follow these steps:\n1. Calculate $\\lambda$ - the expected arrival rate for every day's poisson process\n2. Calculate each day's likelihood distribution over all possible values of $R_t$\n3. Calculate the process matrix based on the value of $\\sigma$ we discussed above\n4. Calculate our initial prior because our first day does not have a previous day from which to take the posterior\n  - Based on [info from the cdc](https:\/\/wwwnc.cdc.gov\/eid\/article\/26\/7\/20-0282_article) we will choose a Gamma with mean 7.\n5. Loop from day 1 to the end, doing the following:\n  - Calculate the prior by applying the Gaussian to yesterday's prior.\n  - Apply Bayes' rule by multiplying this prior and the likelihood we calculated in step 2.\n  - Divide by the probability of the data (also Bayes' rule)"}}