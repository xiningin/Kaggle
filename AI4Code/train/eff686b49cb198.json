{"cell_type":{"717e143f":"code","54e4aca1":"code","a0966170":"code","80831b80":"code","d27de889":"code","8e435df8":"code","4983936c":"code","7d5ec0ea":"code","93607c9c":"code","9c06a171":"code","24e87354":"code","12c2fb83":"code","8efbb387":"code","8b16be05":"markdown","2e55df03":"markdown","8370bb4c":"markdown","9534ad1c":"markdown","53384bd9":"markdown","3938783a":"markdown","26e1daa2":"markdown","e843e949":"markdown","d06797ea":"markdown","98b7daac":"markdown","2770855c":"markdown","624cbb9a":"markdown","806ede36":"markdown","fcc74b5b":"markdown","1b5de401":"markdown","fe92d205":"markdown","8e34627d":"markdown","60639bce":"markdown","f998661c":"markdown","3139760d":"markdown","128f94be":"markdown"},"source":{"717e143f":"file = \"oi.tsv\"","54e4aca1":"import unicodedata\nimport re\n\ndef removeURL(text):\n    text = re.sub(\"http\\\\S+\\\\s*\", \"\", text)\n    return text\n\n# 'NFKD' is the code of the normal form for the Unicode input string.\ndef remove_accentuation(text):\n    text = unicodedata.normalize('NFKD', str(text)).encode('ASCII','ignore')\n    return text.decode(\"utf-8\")\n\ndef remove_punctuation(text):\n    # re.sub(replace_expression, replace_string, target)\n    new_text = re.sub(r\"\\.|,|;|!|\\?|\\(|\\)|\\[|\\]|\\$|\\:|\\\\|\\\/\", \"\", text)\n    return new_text\n\ndef remove_numbers(text):\n    # re.sub(replace_expression, replace_string, target)\n    new_text = re.sub(r\"[0-9]+\", \"\", text)\n    return new_text\n\n# Conver a text to lower case\ndef lower_case(text):\n    return text.lower()","a0966170":"# Remove stop words from a text\nfrom nltk.corpus import stopwords\nnltk_stop = set(stopwords.words('portuguese'))\nfor word in [\"2018\",\"claro\",\"oi\",\"tim\",\"vivo\",\"dia\",\"e\",\"pois\",\"r$\"]:\n    nltk_stop.add(word)\n\ndef remove_stop_words(text, stopWords=nltk_stop):\n    for sw in stopWords:\n        text = re.sub(r'\\b%s\\b' % sw, \"\", text)\n        \n    return text","80831b80":"def pre_process(text):\n    new_text = lower_case(text)\n    new_text = removeURL(new_text)\n    new_text = remove_stop_words(new_text)\n    new_text = remove_numbers(new_text)\n    new_text = remove_punctuation(new_text)\n    new_text = remove_accentuation(new_text)\n    return new_text","d27de889":"n_features = 2200 # 10 percent of tokens\nn_components = 3\nn_top_words = 10\nprint(\"{} topics will be modelled for {} file.\".format(n_components, file))","8e435df8":"import pandas as pd\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.decomposition import LatentDirichletAllocation, TruncatedSVD\n\nprint(\"Loading and preprocessing dataset...\")\ndataset = pd.read_csv(\"..\/input\/\"+file, sep=\"\\t\", encoding=\"utf-8\")\ndata_samples = dataset.iloc[:, 4]\npre_processed_samples = data_samples.apply(pre_process)\nn_samples = len(pre_processed_samples)\n\n# Use tf-idf features for LDA.\nprint(\"Extracting tf features and fitting models for LDA...\")\ntf_vectorizer = TfidfVectorizer(max_features=n_features)\ntf = tf_vectorizer.fit_transform(pre_processed_samples)\n\nprint(\"\\nFiting LDA model...\")\nlda = LatentDirichletAllocation(n_components=n_components, max_iter=10,\n                                learning_method='online', learning_offset=50., random_state=0)\nlda.fit(tf)\n\ndef print_top_words(model, feature_names, n_top_words):\n    for topic_idx, topic in enumerate(model.components_):\n        message = \"Topic #%d: \" % topic_idx\n        message += \" \".join([feature_names[i]\n                             for i in topic.argsort()[:-n_top_words - 1:-1]])\n        print(message)\n    print()\n\nprint(\"Topics in LDA model:\")\ntf_feature_names = tf_vectorizer.get_feature_names()\nprint_top_words(lda, tf_feature_names, n_top_words)\n\nprint(\"Fiting LSA model...\")\nlsa = TruncatedSVD(n_components=n_components, n_iter=40, tol=0.01)\nlsa.fit(tf)\n\nprint(\"Topics in LSA model:\")\nprint_top_words(lsa,tf_feature_names,n_top_words)","4983936c":"from wordcloud import WordCloud\nimport matplotlib.pyplot as plt\n\ndef gen_word_cloud(topic,string):\n    # Generate a word cloud image\n    print(\"Wordcloud for {}\".format(topic))\n    wordcloud = WordCloud().generate(string)\n    plt.figure()\n    plt.imshow(wordcloud, interpolation='bilinear')\n    plt.axis(\"off\")\n    plt.show()\n\nstring = ' '.join(pre_processed_samples)\ngen_word_cloud(\"whole dataset\",string)\n    \n    # Loops through Topics\nfor n in range(n_components):\n    indexes = [index for (index,doc) in enumerate(lda.transform(tf)) if doc.argmax()==n]\n    string = ' '.join([pre_processed_samples[i] for i in indexes])\n    topic = \"Topic #\"+str(n)\n    gen_word_cloud(topic,string)","7d5ec0ea":"from collections import OrderedDict\n\nfrom scipy.cluster.hierarchy import ward, dendrogram\nfrom sklearn.metrics.pairwise import cosine_similarity\n\nX = tf\nvectorizer = tf_vectorizer\n\nprint(\"DTM Matrix\")\nfeature_names = vectorizer.get_feature_names()\n# Transform the text collection from our dataset to a Ordered Dictionary, with items as <index, text>.\ntext_collection = OrderedDict([(index, text) for index, text in enumerate(pre_processed_samples.values)])\n#print(text_collection)\ncorpus_index = [n for n in text_collection]\n# Build a DataFrame from the Document-Term Matrix returned by the Vectorizer, to print a nice visualization\ndf = pd.DataFrame(X.todense(), index=corpus_index, columns=feature_names)\n\n\ndist = 1 - cosine_similarity(X.T)\n\nlinkage_matrix = ward(dist) #define the linkage_matrix using ward clustering pre-computed distances\n\nfig, ax = plt.subplots(figsize=(15, 800)) # set size\nax = dendrogram(linkage_matrix, orientation=\"right\", labels=feature_names, leaf_font_size=20);\n\nplt.tick_params(\\\n    axis= 'x',          # changes apply to the x-axis\n    which='both',      # both major and minor ticks are affected\n    bottom='off',      # ticks along the bottom edge are off\n    top='off',         # ticks along the top edge are off\n    labelbottom='off')\n\nplt.tight_layout() #show plot with tight layout\n\n#uncomment below to save figure\n#plt.savefig('ward_clusters.png', dpi=200) #save figure as ward_clust","93607c9c":"!pip install polyglot\n!polyglot download embeddings2.pt ner2.pt\nimport nltk\nnltk.download('rslp')","9c06a171":"from polyglot.text import Text\n\ndef tokenize(string):\n    text = Text(string, hint_language_code='pt')\n    text.language = \"pt\"\n    return text.words","24e87354":"import nltk\n\nstemmer = nltk.stem.RSLPStemmer()\n\ndef stemming(text):\n    words = tokenize(text)\n    stemmer_words = [stemmer.stem(item) for item in words]\n    return ' '.join(stemmer_words)","12c2fb83":"from collections import Counter\n\nstring = ' '.join(pre_processed_samples)\ntext = Text(string)\nprint(\"Identified language \\\"{}\\\" with confidence {}\\n\"\n      .format(text.language.name, text.language.confidence))\n\nprint(\"Performing Tokenization and Stemming tasks...\")\nstemmed_string = stemming(string)\nstemmed_text = Text(stemmed_string)\nmost_commons = Counter(stemmed_text.words).most_common(15)\nprint(\"The 15 most common (stemmed) items found: \\n\")\n\nprint(\"{:<16}{}\".format(\"Stemmed word\", \"Occurrence\")+\"\\n\"+\"-\"*30)\nfor i,j in most_commons:\n    print(\"{:<16}{}\".format(i,j))","8efbb387":"from functools import reduce\n\ndef NER_sentiments(sent):\n    sent.language = \"pt\"\n    if sent.entities:\n        try:\n            positive = list(map(lambda x: x.positive_sentiment, sent.entities))\n            sum_positive = reduce((lambda x, y: x + y), positive)\n            negative = list(map(lambda x: x.negative_sentiment, sent.entities))\n            sum_negative = reduce((lambda x, y: x + y), negative)\n            sum_total = sum_positive - sum_negative    \n            #print(\"{}\\n\".format(sum_total))\n            return sum_total\n        except:\n            return 0\n    else:\n        return 0\n\n#new_data = data_samples.apply(remove_accentuation).values\navaliacoes = dataset.iloc[:,6]\navaliacoes.replace({\"<n\u00e3o h\u00e1 coment\u00e1rios do consumidor>\": \"-\"})\navaliacoes.apply(remove_accentuation)\navaliacoes = avaliacoes.values\n\nfor row_idx, row in enumerate(avaliacoes):\n    #print(\"row {}: {}\".format(row_idx, row))\n    text = Text(row, hint_language_code='pt')\n    sentences = text.sentences\n    if sentences:\n        sentiments = list(map(lambda x: NER_sentiments(x), sentences))\n        sentiment = reduce((lambda x, y: x + y), sentiments)\n        if sentiment < 0:\n            sentiment = \"Negative\"\n        elif sentiment == 0:\n            sentiment = \"Neutral\"\n        else:\n            sentiment = \"Positive\"\n        print(\"Sentiment in doc {}: {}\".format(row_idx, sentiment))\n","8b16be05":"## Preprocessing data\nTo get better results, we must do some transformations in the raw text. Let's define some preprocessing functions:","2e55df03":"## Analysis","8370bb4c":"### Named Entity Recognition and sentiment analysis\nFor NER task, Polyglot provides ```Text.entities```. Also, their ```Entity.positive_sentiment``` and ```Entity.negative_sentiment``` properties will help us.\n\nHere we use the method related in [Minerando Dados](http:\/\/minerandodados.com.br\/index.php\/2018\/05\/15\/analise-de-sentimentos-de-uma-forma-diferente\/) website.\n\nIt consists of collecting lists of positive and negative sentiments of senteces entities, reducing these lists to their sum, and then subtracting negative sum from positive sum. The result is accounted as Negative (<0); Neutral (=0) or Positive (>0).","9534ad1c":"## Crawling data\n\nThe code used for crawling the data can be found at [this Gist](https:\/\/gist.github.com\/JrLima\/53c9fe0c97fa54f7d149060b183bb354).\nThe result was then copied from console and saved in tsv files (in order to preserve text structure and punctuation).\n\nThe dataset contains the following columns:\n1. Empresa\n2. Data\n3. Cidade\n4. Nota\n5. Relato\n6. Resposta\n7. Avaliacao\n\n### Results\nBy running the crawler we got the following amount of registries for each of the companies:\n\n| Company                 | Amount |\n| ------------------------|--------|\n| Claro Celular           | 19932  |\n| Oi Celular              | 12119  |\n| Tim                     | 47088  |\n| Vivo - Telef\u00f4nica (GVT) | 60999  |","53384bd9":"### Output:\n\n\\* **\\[As *lib icu* could not be installed in this Kaggle kernel, here is the output running in a local machine:\\]**\n\nSentiment in doc 0: Neutral\n\nSentiment in doc 1: Neutral\n\nSentiment in doc 2: Neutral\n\nSentiment in doc 3: Neutral\n\nSentiment in doc 4: Neutral\n\nSentiment in doc 5: Positive\n\nSentiment in doc 6: Neutral\n\nSentiment in doc 7: Neutral\n\nSentiment in doc 8: Neutral\n\nSentiment in doc 9: Neutral\n\nSentiment in doc 10: Neutral\n\nSentiment in doc 11: Neutral\n\nSentiment in doc 12: Negative\n\nSentiment in doc 13: Neutral\n\nSentiment in doc 14: Neutral\n\nSentiment in doc 15: Neutral\n\nSentiment in doc 16: Neutral\n\nSentiment in doc 17: Neutral\n\nSentiment in doc 18: Negative\n\nSentiment in doc 19: Neutral\n\n[...]\n\nSentiment in doc 12118: Neutral","3938783a":"## Dependencies\nIn order to follow the steps presented in this work, some Python software and libraries are needed:\n\n- [Python 3.x](https:\/\/www.python.org\/downloads\/)\n- [Jupyter Notebook](http:\/\/jupyter.org\/install)\n- [Pandas](https:\/\/pandas.pydata.org)\n- [NLTK](https:\/\/www.nltk.org\/)\n- [Polyglot](http:\/\/polyglot.readthedocs.io\/en\/latest\/Installation.html)\n- [Scikit-Learn](http:\/\/scikit-learn.org\/stable\/install.html)\n- [Scipy](https:\/\/www.scipy.org\/install.html)\n- [Wordcloud](https:\/\/github.com\/amueller\/word_cloud)\n- [Matplotlib](https:\/\/matplotlib.org\/users\/installing.html)\n","26e1daa2":"### Combining the methods\nWe define our ```pre_process``` function, which uses our predefined functions.","e843e949":"### Tokenization\nThe ```tokenize``` function will identify - and split - each word from the entry. ","d06797ea":"### Identifying language and most common terms\nUsing Polyglot, we can identify some interesting metadata based on the text.","98b7daac":"### Stemming\nWe create here a function to adapt each word to it's lemma.","2770855c":"### Output:\n\n\\* **\\[As *lib icu* could not be installed in this Kaggle kernel, here is the result of the code above running in a local machine\\]**\n\nIdentified language \"portugu\u00eas\" with confidence 99.0\n\nPerforming Tokenization and Stemming tasks...\nThe 15 most common (stemmed) items found: \n\n|Stemmed word  |  Occurrence   |\n|--------------|---------------|\n|val           |  11859        |\n|plan          |  11282        |\n|pag           |  8753         |\n|-             |  8022         |\n|cancel        |  7485         |\n|r             |  7221         |\n|fatur         |  6439         |\n|serv          |  6427         |\n|atend         |  6388         |\n|oper          |  6159         |\n|inform        |  6030         |\n|contrat       |  5670         |\n|e             |  5067         |\n|linh          |  4963         |\n|cont          |  4854         |\n","624cbb9a":"### LDA and LSA\nMosto popular topic modelling algorithm, LDA and it's LSA alternative will be used.\n\n**Sklearn** library provides both topic modelling algorithms.","806ede36":"# Analyzing stories of brazilian telecom companies customers\n\nIn this notebook we present text mining and knowledge extraction tasks based on methods such as LDA; LSA; wordcloud; dendogram etc.\n\nWe used the brazilian platform [consumidor.gov.br](https:\/\/www.consumidor.gov.br) to collect the stories of customers. We will then perform a series of analysis in the collected data.\n\n## Sections\nThe sections in this notebook are as follow:\n\n- [Dependencies](#Dependencies)\n- [Crawling data](#Crawling-data)\n  - [Results](#Results)\n- [Choose data](#Choose-data)\n- [Preprocessing data](#Preprocessing-data)\n  - [Noise removal](#Noise-removal)\n    - [Using stopwords](#Using-stopwords)\n  - [Combining the methods](#Combining-the-methods)\n- [Analysis](#Analysis)\n  - [LDA and LSA](#LDA-and-LSA)\n  - [Data visualization](#Data-visualization)\n    - [WordCloud](#WordCloud)\n    - [Dendrogram](#Dendrogram)\n  - [Using Polyglot](#Using-Polyglot)\n    - [Tokenization](#Tokenization)\n    - [Stemming](#Stemming)\n    - [Identifying language and most common terms](#Identifying-language-and-most-common-terms)\n    - [Named Entity Extraction and sentiment analysis](#Named-Entity-Extraction-and-sentiment-analysis)","fcc74b5b":"## Choose data\nAvailable files:\n1. ```claro.tsv```\n2. ```oi.tsv```\n3. ```tim.tsv```\n4. ```vivo.tsv```","1b5de401":"#### Using stopwords\nSome words aren't relevant in the following text mining activities. We found a great amount of them in ```nltk```'s corpus ```stopwords``` collections. Also, we have added a couple of new stopwords, such as the name of the companies.","fe92d205":"Let's start by defining our setup variables","8e34627d":"### Data visualization","60639bce":"#### Dendrogram\nA clustering method to visualize the data distribution.","f998661c":"## Using Polyglot\nFirst ```pip install polyglot```\n\nTo download the assets, bash ```polyglot download embeddings2.pt ner2.pt```.\n\nAlso, to prepare the stemming setup, type in a Python environment:\n```python\nimport nltk\nnltk.download('rslp')\n```\n\nLet's try it:","3139760d":"### Noise removal\nAs capitalization doesn't make any difference in topic modelling, we will convert all words into lower case, so they won't be treated as different words. Also, in order to simplify our documents collection, we extract some signals as accentuation and punctuation. Regular expressions are also used to remove numbers and URLs.","128f94be":"#### WordCloud\nFirst of all, we get documents per modelled topic using ```lda.transform(tf)``` and then we generate a WordCloud plot from the documents bellonging to each topic."}}