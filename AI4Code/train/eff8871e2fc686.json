{"cell_type":{"0f625db8":"code","e2c609be":"code","e5513319":"code","38e4ce12":"code","48a56a69":"code","6dc862e6":"code","b943b914":"code","3abec661":"code","3b4be17d":"code","dbb82fe3":"code","ffd655a3":"code","ceaf6959":"code","b755d25d":"code","2799e585":"code","03f937ac":"code","fa7aeeba":"code","70f4699e":"code","bc481db0":"code","feff9f71":"code","440c8e12":"code","fc871124":"code","5ded8461":"code","52d84c46":"code","accfd1c2":"code","c7218f89":"code","3abbb350":"markdown"},"source":{"0f625db8":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","e2c609be":"df=pd.read_csv('\/kaggle\/input\/housing-prices-in-metropolitan-areas-of-india\/Mumbai.csv')","e5513319":"df.shape","38e4ce12":"pd.set_option('display.max_columns',None)\ndf.head()","48a56a69":"numeric_data=df.select_dtypes(exclude='object').drop(['Price'],axis=1).copy()\nnumeric_data.head()","6dc862e6":"categorical_data=df.select_dtypes(include='object')\ncategorical_data.head()","b943b914":"#Count plot (categorical, univariate analysis)\nimport matplotlib.pyplot as plt\nimport seaborn as sns \n\ndf1=df.copy()\ndf1['Area'] = pd.cut(df1['Area'], bins=[0, 250, 500, 750, 1000, 1250, 1500, 1750, 2000, 2250, 2500, 2750, 3000, np.inf])\nfig=plt.figure(figsize=(20,30))\nfor i,col in enumerate(numeric_data):\n    fig.add_subplot(10,4,i+1)\n    sns.countplot(df1[col])\n    plt.xlabel(col,size=15)\n    plt.xticks(rotation=90)\nplt.tight_layout(pad=1)\nplt.show()\n ","3abec661":"fig,ax=plt.subplots(figsize=(23,20))\nax.set_title('Houses at each Location',fontsize=20)\nsns.countplot(y='Location',data=df, order=df.Location.value_counts().index[:50])\nax.set_xlabel('Locations',fontsize=20)\nax.set_ylabel('No. of Houses',fontsize=20)\nplt.show()","3b4be17d":"#Count plot (categorical, univariate analysis)\nfig=plt.figure(figsize=(18,20))\nsns.countplot(df1['Area'])\nplt.xlabel('Area',fontsize=15)\nplt.ylabel('No. of Houses',fontsize=15)\nplt.xticks(rotation=40)\nplt.tight_layout(pad=1)\nplt.show()","dbb82fe3":"df2=df.copy().replace(9,np.nan)\ndf2=df2.fillna(method='bfill',axis=0).fillna(0)\ndf2.head()","ffd655a3":"#Correlation\nnum=df2.select_dtypes(exclude='object')\nnumeric_correlation=num.corr()\nplt.figure(figsize=(10,10))\nplt.title('Correlation')\nsns.heatmap(numeric_correlation>0.8, annot=True, square=True)","ceaf6959":"print(numeric_correlation['Price'].sort_values(ascending=False))","b755d25d":"#dropping features due to high correlation\ndf2.drop(['Hospital','AC','Refrigerator','LiftAvailable'],axis=1,inplace=True)","2799e585":"#Missing Values\npd.DataFrame(df2.isnull().sum(), columns=['sum']).sort_values(by=['sum'],ascending=False).head(51)","03f937ac":"plt.figure(figsize=(10,6))\nplt.title(\"Distrubution of SalePrice\")\ndist = sns.distplot(df2['Price'],norm_hist=False)","fa7aeeba":"plt.figure(figsize=(10,6))\nplt.title(\"Distrubution of SalePrice\")\ndist = sns.distplot(np.log(df2['Price']),norm_hist=False)","70f4699e":"from sklearn.metrics import mean_squared_error, mean_absolute_error\nfrom xgboost import XGBRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom lightgbm import LGBMRegressor\nfrom sklearn.model_selection import cross_val_score\nfrom catboost import CatBoostRegressor","bc481db0":"from sklearn.model_selection import train_test_split\n\nx = df2.drop(['Price'], axis=1) \ny = np.log1p(df2['Price'])\nX_train, X_val, y_train, y_val = train_test_split(x, y, test_size=0.2, random_state=1)\n\ncategorical_cols = [cname for cname in x.columns if\n                    x[cname].dtype == \"object\"] \n                \n\n\nnumerical_cols = [cname for cname in x.columns if\n                 x[cname].dtype in ['int64','float64','uint8']]\n\n\nmy_cols = numerical_cols + categorical_cols\nX_train = X_train[my_cols].copy()\nX_val = X_val[my_cols].copy()\nprint(categorical_cols,numerical_cols)","feff9f71":"from sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder\n\nnum_transformer = Pipeline(steps=[\n    ('num_imputer', SimpleImputer(strategy='constant'))\n    ])\n\ncat_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='most_frequent')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n    ])\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num',num_transformer,numerical_cols),       \n        ('cat',cat_transformer,categorical_cols),\n        ])","440c8e12":"# Reversing log-transform on y\ndef inv_y(transformed_y):\n    return np.exp(transformed_y)\n\nn_folds = 10","fc871124":"# XGBoost\nmodel = XGBRegressor(learning_rate=0.01, n_estimators=3460, max_depth=3, min_child_weight=0,gamma=0, subsample=0.7,colsample_bytree=0.7,objective='reg:squarederror', nthread=-1,scale_pos_weight=1, seed=27, reg_alpha=0.00006)\nclf = Pipeline(steps=[('preprocessor', preprocessor),\n                      ('model', model)])\nclf.fit(X_train, y_train)\npredict = clf.predict(X_val)\nprint('XGBoost: ' + str(mean_absolute_error(inv_y(predict), inv_y(y_val))))\n\n\n# Lasso  \nfrom sklearn.linear_model import LassoCV\n\nmodel = LassoCV(max_iter=1e7,  random_state=14, cv=n_folds)\nclf = Pipeline(steps=[('preprocessor', preprocessor),\n                          ('model', model)])\nclf.fit(X_train, y_train)\npredict = clf.predict(X_val)\nprint('Lasso: ' + str(mean_absolute_error(inv_y(predict), inv_y(y_val))))\n\n# GradientBoosting   \nmodel = GradientBoostingRegressor(n_estimators=300, learning_rate=0.05, max_depth=4, random_state=5)\nclf = Pipeline(steps=[('preprocessor', preprocessor),\n                          ('model', model)])\nclf.fit(X_train, y_train)\npredict = clf.predict(X_val)\nprint('Gradient: ' + str(mean_absolute_error(inv_y(predict), inv_y(y_val))))","5ded8461":"df3=df[['Price','Area','No. of Bedrooms','Resale','Location']].copy()\ndf3.head()","52d84c46":"from sklearn.model_selection import train_test_split\n\nx = df3.drop(['Price'], axis=1) \ny = np.log1p(df3['Price'])\nX_train, X_val, y_train, y_val = train_test_split(x, y, test_size=0.2, random_state=1)\n\ncategorical_cols = [cname for cname in x.columns if\n                    x[cname].dtype == \"object\"] \n                \n\n\nnumerical_cols = [cname for cname in x.columns if\n                 x[cname].dtype in ['int64','float64','uint8']]\n\n\nmy_cols = numerical_cols + categorical_cols\nX_train = X_train[my_cols].copy()\nX_val = X_val[my_cols].copy()\nprint(categorical_cols,numerical_cols)","accfd1c2":"from sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder\n\nnum_transformer = Pipeline(steps=[\n    ('num_imputer', SimpleImputer(strategy='constant'))\n    ])\n\ncat_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='most_frequent')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n    ])\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num',num_transformer,numerical_cols),       \n        ('cat',cat_transformer,categorical_cols),\n        ])","c7218f89":"# XGBoost\nmodel = XGBRegressor(learning_rate=0.01, n_estimators=3460, max_depth=3, min_child_weight=0,gamma=0, subsample=0.7,colsample_bytree=0.7,objective='reg:squarederror', nthread=-1,scale_pos_weight=1, seed=27, reg_alpha=0.00006)\nclf = Pipeline(steps=[('preprocessor', preprocessor),\n                      ('model', model)])\nclf.fit(X_train, y_train)\npredict = clf.predict(X_val)\nprint('XGBoost: ' + str(mean_absolute_error(inv_y(predict), inv_y(y_val))))\n\n\n# Lasso  \nfrom sklearn.linear_model import LassoCV\n\nmodel = LassoCV(max_iter=1e7,  random_state=14, cv=n_folds)\nclf = Pipeline(steps=[('preprocessor', preprocessor),\n                          ('model', model)])\nclf.fit(X_train, y_train)\npredict = clf.predict(X_val)\nprint('Lasso: ' + str(mean_absolute_error(inv_y(predict), inv_y(y_val))))\n\n# GradientBoosting   \nmodel = GradientBoostingRegressor(n_estimators=300, learning_rate=0.05, max_depth=4, random_state=5)\nclf = Pipeline(steps=[('preprocessor', preprocessor),\n                          ('model', model)])\nclf.fit(X_train, y_train)\npredict = clf.predict(X_val)\nprint('Gradient: ' + str(mean_absolute_error(inv_y(predict), inv_y(y_val))))","3abbb350":"# Only using columns with no missing (not available) values"}}