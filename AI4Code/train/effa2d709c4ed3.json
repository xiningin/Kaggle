{"cell_type":{"d3510539":"code","46484178":"code","34f7da50":"code","35134670":"code","48d5f12b":"code","59dfe21d":"code","f78af21d":"code","da7dcab7":"code","3eda3dfb":"code","253aaef7":"code","6d40021c":"code","63af8a39":"code","52450800":"code","67b2b285":"code","0e9cd8aa":"code","8cf5967d":"code","2fc57e3c":"code","554417ea":"code","250c19c7":"code","49e28291":"code","f6a8eae6":"code","d9490a9b":"code","2e4abb17":"code","5b098e2c":"code","25ab91b5":"code","6a4ea241":"code","d29d0cb3":"code","01f05584":"code","8e509881":"code","67d78748":"code","eba8b8bd":"code","d0aebaa2":"code","b39a76f9":"code","06ee3dc1":"code","7186b260":"code","e94b4c81":"markdown","f6ed0804":"markdown","8298affc":"markdown","f985c67e":"markdown","033aafea":"markdown","6217bb57":"markdown","ba500d17":"markdown","f5d6e55b":"markdown","f9c94fd6":"markdown","8fa1e5ef":"markdown","6b49c481":"markdown","bad89817":"markdown","aca1c477":"markdown","717fd707":"markdown","75908f86":"markdown","54d2c24b":"markdown","c63e0015":"markdown","733b80ea":"markdown","46c8313f":"markdown"},"source":{"d3510539":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","46484178":"#loading the libraries \nimport matplotlib.pyplot as plt\nfrom mlxtend.data import loadlocal_mnist\n%matplotlib inline\nimport random\n\n\n#for neural network\nimport keras\nfrom keras.layers import Dense\nfrom keras.models import Sequential\nfrom keras.layers import Dropout","34f7da50":"path_training_images = '..\/input\/mnist-dataset\/train-images.idx3-ubyte'\npath_training_labels = '..\/input\/mnist-dataset\/train-labels.idx1-ubyte'\n\npath_test_images = '..\/input\/mnist-dataset\/t10k-images.idx3-ubyte'\npath_test_labels = '..\/input\/mnist-dataset\/t10k-labels.idx1-ubyte'","35134670":"#for loading the files\nX_train, y_train = loadlocal_mnist(path_training_images,path_training_labels)\nX_test, y_test = loadlocal_mnist(path_test_images, path_test_labels)","48d5f12b":"print(type(X_train),type(y_train))\n# we have the data in numpy arrays","59dfe21d":"print('Dim of X train is ',X_train.shape)\nprint('Dim of y train is ',y_train.shape)\nprint('Dim of X test is ',X_test.shape)\nprint('Dim of y test is ',y_test.shape)","f78af21d":"ex_image = X_train[0]\nex_label = y_train[0]\n\nprint('Shape of ex is ',ex_image.shape)\nprint('label of ex is ',ex_label)","da7dcab7":"print(ex_image)\n# pixel values are from 0 to 255\n#0 - black, 255 - white","3eda3dfb":"#but the above one is not that a good visualization..Lets plot the pixels of this example\nplt.imshow(ex_image.reshape(28,28),cmap=plt.cm.gray)\nplt.title('This image represents {}'.format(ex_label))\nplt.xticks([])\nplt.yticks([])","253aaef7":"def show(image, title):\n    index = 1 \n    plt.figure(figsize=(10,5))\n\n    for x in zip(image, title):        \n        image = x[0]        \n        title = x[1]\n        plt.subplot(2, 5, index)        \n        plt.imshow(image.reshape(28,28), cmap=plt.cm.gray)  \n        plt.title(x[1], fontsize = 9)\n        plt.xticks([])\n        plt.yticks([])\n        index += 1","6d40021c":"image = []\ntitle = []\nfor i in range(0, 5):\n    r = random.randint(1, len(X_train))\n    image.append(X_train[r])\n    title.append('training image:' + str(y_train[r]))       \n\nfor i in range(0, 5):\n    r = random.randint(1, len(X_test))\n    image.append(X_test[r])\n    title.append('testing image:' + str(y_test[r]))\n    \nshow(image, title)","63af8a39":"#before model creation, we need to one hot encode labels\nnum_classes = 10 # from 0 to 0\ny_train = keras.utils.to_categorical(y_train,num_classes)\ny_test = keras.utils.to_categorical(y_test,num_classes)","52450800":"print(X_train.shape,y_train.shape)\nprint(y_train[0])","67b2b285":"model = Sequential()\n\nmodel.add(Dense(units=128, activation='relu', input_shape=(784,)))\nmodel.add(Dense(units=32, activation='relu'))\nmodel.add(Dense(units=num_classes, activation='softmax'))\nmodel.summary()","0e9cd8aa":"model.compile(optimizer=\"adam\", loss='categorical_crossentropy', metrics=['accuracy'])\n","8cf5967d":"#fitting the model\nhistory = model.fit(X_train, y_train, batch_size=128, epochs=10, validation_split=.1)\n","2fc57e3c":"plt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['training', 'validation'], loc='best')\nplt.show()","554417ea":"_,test_acc = model.evaluate(X_test, y_test)\nprint('Test accuracy is ',test_acc)","250c19c7":"\n\n#normalize the data \nX_train = X_train \/ 255 \nX_test = X_test \/ 255 \n","49e28291":"print(X_train[0])\n# now the values are in between 0 to 1","f6a8eae6":"\n\nmodel = Sequential()\n\nmodel.add(Dense(units=128, activation='relu', input_shape=(784,)))\nmodel.add(Dense(units=32, activation='relu'))\nmodel.add(Dense(units=num_classes, activation='softmax'))\n\nmodel.compile(optimizer=\"adam\", loss='categorical_crossentropy', metrics=['accuracy'])\n\nhistory = model.fit(X_train, y_train, batch_size=128, epochs=10, validation_split=.1)\n\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['training', 'validation'], loc='best')\nplt.show()\n\n_,test_acc = model.evaluate(X_test, y_test)\nprint('Test accuracy is ',test_acc)","d9490a9b":"model = Sequential()\n\nmodel.add(Dense(units=256, activation='relu', input_shape=(784,)))\nmodel.add(Dense(units=128, activation='relu'))\nmodel.add(Dense(units=32, activation='relu'))\nmodel.add(Dense(units=num_classes, activation='softmax'))\n\nmodel.compile(optimizer=\"adam\", loss='categorical_crossentropy', metrics=['accuracy'])\n\nhistory = model.fit(X_train, y_train, batch_size=128, epochs=10, validation_split=.1)\n\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['training', 'validation'], loc='best')\nplt.show()\n\n_,test_acc = model.evaluate(X_test, y_test)\nprint('Test accuracy is ',test_acc)","2e4abb17":"model = Sequential()\n\nmodel.add(Dense(units=256, activation='relu', input_shape=(784,)))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(units=128, activation='relu'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(units=32, activation='relu'))\nmodel.add(Dense(units=num_classes, activation='softmax'))\n\nmodel.compile(optimizer=\"adam\", loss='categorical_crossentropy', metrics=['accuracy'])\n\nhistory = model.fit(X_train, y_train, batch_size=128, epochs=10, validation_split=.1)\n\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['training', 'validation'], loc='best')\nplt.show()\n\n_,test_acc = model.evaluate(X_test, y_test)\nprint('Test accuracy is ',test_acc)","5b098e2c":"sample = pd.read_csv('..\/input\/digit-recognizer\/sample_submission.csv')\nsample.head()","25ab91b5":"test = pd.read_csv('..\/input\/digit-recognizer\/test.csv')\nprint(test.shape)\ntest.head()","6a4ea241":"test = np.array(test)","d29d0cb3":"test.shape","01f05584":"# normalize the data \ntest = test \/ 255","8e509881":"#predictions \npreds = model.predict(test)","67d78748":"preds[0]","eba8b8bd":"pred_classes = np.argmax(preds,axis=1)\npred_classes.shape","d0aebaa2":"# lets look at some predictions made\nshow(test[:5],pred_classes[:5])","b39a76f9":"submission = pd.DataFrame({'ImageId':np.arange(1,28001),'Label':pred_classes})","06ee3dc1":"submission.head()","7186b260":"submission.to_csv('sub.csv',index = False)","e94b4c81":"# Training an ANN","f6ed0804":"now lets repeat the above procedure once again, but this time we will normalize the data\n","8298affc":"# Exploring the data","f985c67e":"the drawing seems poor, drawn by mouse..","033aafea":"# Making predictions","6217bb57":"This examples contains 784 numbers, which represent the pixel values taken in 28*28 array","ba500d17":"If we remember from above, the first training label was 5, now we have represented the same in one hot.","f5d6e55b":"# Will results improve if we normalize the data?","f9c94fd6":"# Trying a deeper ANN","8fa1e5ef":"# Deep ANN with dropout","6b49c481":"(60000,784) means the training data has 60000 images, where each image is an array of len 784, which can be respresented as a square matrix of (28*28)","bad89817":"So, normalization definately imporves the performance of the model.","aca1c477":"# Getting the data into numpy arrays","717fd707":"Looks good","75908f86":"Lets see how a training examples looks like. <br>\nWe will have a look at the first training examples","54d2c24b":"Deep ANN with dropout gives the best performance","c63e0015":"now lets viszualise more of the images","733b80ea":"# Conclusion","46c8313f":"![Screenshot%20from%202020-09-29%2010-57-19.png](attachment:Screenshot%20from%202020-09-29%2010-57-19.png)"}}