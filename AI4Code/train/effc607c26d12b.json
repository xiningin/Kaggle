{"cell_type":{"7fadaf30":"code","5a5d26da":"code","ee56b169":"code","75a7d2ef":"code","d5ea492a":"code","bb3c2525":"code","d1a22452":"code","3f6efcd3":"code","602acf63":"code","e11ada6f":"code","6094fd27":"code","850f06f7":"code","f99141be":"code","b9cbe51b":"code","254085c3":"code","af8e9ab7":"code","8d4bf7c8":"code","3f213303":"code","bc12f2e5":"code","0f697134":"code","558e8898":"code","4ccf8d10":"code","3bc0dcb2":"code","41660eac":"code","8a040af7":"code","09b67ac1":"code","7099c5b8":"code","a6798f9e":"code","cf1802bf":"code","0fb55e1c":"code","714a6aea":"code","d58052c5":"code","20e4c4b5":"code","fb5a492d":"code","c3150767":"code","1336c6cc":"code","0cb9638b":"code","bf70a95b":"code","c67dbfad":"code","383aad4c":"code","6eb760ff":"code","f9a1a897":"code","3d09a6d5":"code","bfa6c2bf":"code","26220f09":"markdown","04753fdc":"markdown","c492df55":"markdown","66d65092":"markdown","bf72b8cb":"markdown","5c215321":"markdown","adf7c697":"markdown","72ec7d88":"markdown","c1c15df2":"markdown","fcc640b9":"markdown","133d8a7a":"markdown","69006c3f":"markdown","1883494c":"markdown","a4938476":"markdown","deb5b9a2":"markdown","66d154fd":"markdown","a01c88eb":"markdown","4fab9f50":"markdown","655d9be1":"markdown","cf90e67b":"markdown","4c64e1f7":"markdown","8a9816a6":"markdown","5b342ba3":"markdown","7fc85dd9":"markdown","9f9790ec":"markdown","8ad1b30b":"markdown","21acda3e":"markdown","21777340":"markdown","60a587e5":"markdown","d1ce12f6":"markdown","3a9d54fc":"markdown","0cf1914f":"markdown","d0b144e0":"markdown","261d0d20":"markdown","e3a5ee93":"markdown","cfa290c7":"markdown","be1b28f2":"markdown","651e2109":"markdown","281c2c50":"markdown"},"source":{"7fadaf30":"!wget --no-check-certificate \\\n  https:\/\/storage.googleapis.com\/mledu-datasets\/cats_and_dogs_filtered.zip \\\n  -O \/tmp\/cats_and_dogs_filtered.zip","5a5d26da":"import os\nimport zipfile\n\nlocal_zip = '\/tmp\/cats_and_dogs_filtered.zip'\n\nzip_ref = zipfile.ZipFile(local_zip, 'r')\n\nzip_ref.extractall('\/tmp')\nzip_ref.close()","ee56b169":"import os\n\n# base_dir = '..\/input\/dogs-vs-cats'\nbase_dir = '\/tmp\/cats_and_dogs_filtered'\n\ntrain_dir = os.path.join(base_dir, 'train')\ntest_dir = os.path.join(base_dir, 'validation')\n\n# Directory with our training cat\/dog pictures\ntrain_cats_dir = os.path.join(train_dir, 'cats')\ntrain_dogs_dir = os.path.join(train_dir, 'dogs')\n\n# Directory with our validation cat\/dog pictures\ntest_cats_dir = os.path.join(test_dir, 'cats')\ntest_dogs_dir = os.path.join(test_dir, 'dogs')","75a7d2ef":"print('total training cat images :', len(os.listdir(train_cats_dir)))\nprint('total training dog images :', len(os.listdir(train_dogs_dir)))\nprint('total validation cat images :', len(os.listdir(test_cats_dir)))\nprint('total validation dog images :', len(os.listdir(test_dogs_dir)))","d5ea492a":"%matplotlib inline\n\nimport matplotlib.image as mpimg\nimport matplotlib.pyplot as plt\n\n# We want 4 x 4 pics\nnrows = 4\nncols = 4\n\npic_index = 0 # Start from zero\n\ntrain_cats = os.listdir(train_cats_dir)\ntrain_dogs = os.listdir(train_dogs_dir)","bb3c2525":"fig = plt.gcf()\nfig.set_size_inches(ncols*4, nrows*4)\n\npic_index += 8\n\nnext_cat_pix = [os.path.join(train_cats_dir, fname) for fname in train_cats[pic_index-8:pic_index]]\n\nnext_dog_pix = [os.path.join(train_dogs_dir, fname) for fname in train_dogs[ pic_index-8:pic_index]]\n\nfor i, img_path in enumerate(next_cat_pix+next_dog_pix):\n  # create subplot, indices start with 1\n  sp = plt.subplot(nrows, ncols, i + 1)\n  sp.axis('Off') # hide axis\n\n  img = mpimg.imread(img_path)\n  plt.imshow(img)\n\nplt.show()","d1a22452":"!wget -O cat.png https:\/\/live.staticflickr.com\/3689\/8989851909_9b78222fbb.jpg","3f6efcd3":"import PIL.Image\n\nPIL.Image.open('cat.png')","602acf63":"import tensorflow as tf\n\nimage_string=tf.io.read_file(\"cat.png\")\nimage=tf.image.decode_jpeg(image_string,channels=3)","e11ada6f":"def visualize(original, augmented):\n  fig = plt.figure()\n  plt.subplot(1,2,1)\n  plt.title('Original image')\n  plt.imshow(original)\n\n  plt.subplot(1,2,2)\n  plt.title('Augmented image')\n  plt.imshow(augmented)","6094fd27":"flipped = tf.image.flip_left_right(image)\nvisualize(image, flipped)","850f06f7":"rotated = tf.image.rot90(image)\nvisualize(image, rotated)","f99141be":"cropped = tf.image.central_crop(image, central_fraction=0.5)\nvisualize(image,cropped)","b9cbe51b":"from tensorflow.keras.preprocessing.image import ImageDataGenerator\n\ndummy_datagen = ImageDataGenerator(\n      rescale=1.\/255,\n      rotation_range=40)\n\n# here we will use test set\ndummy_ds = dummy_datagen.flow_from_directory(test_dir,\n                                            batch_size=20,\n                                            class_mode='binary',\n                                            target_size=(150, 150))","254085c3":"def plotImages(images_arr):\n    fig, axes = plt.subplots(1, 5, figsize=(20,20))\n    axes = axes.flatten()\n    for img, ax in zip( images_arr, axes):\n        ax.imshow(img)\n        ax.axis('off')\n    plt.tight_layout()\n    plt.show()\n    \n    \naugmented_images = [dummy_ds[0][0][0] for i in range(5)]\nplotImages(augmented_images)","af8e9ab7":"dummy_datagen = ImageDataGenerator(\n      rescale=1.\/255,\n      width_shift_range=0.2,\n      height_shift_range=0.2)\n\n# here we will use test set\ndummy_ds = dummy_datagen.flow_from_directory(test_dir,\n                                            batch_size=20,\n                                            class_mode='binary',\n                                            target_size=(150, 150))\n\n\naugmented_images = [dummy_ds[0][0][0] for i in range(5)]\nplotImages(augmented_images)","8d4bf7c8":"dummy_datagen = ImageDataGenerator(\n      rescale=1.\/255,\n      horizontal_flip=True)\n\n# here we will use test set\ndummy_ds = dummy_datagen.flow_from_directory(test_dir,\n                                            batch_size=20,\n                                            class_mode='binary',\n                                            target_size=(150, 150))\n\n\naugmented_images = [dummy_ds[0][0][0] for i in range(5)]\nplotImages(augmented_images)","3f213303":"# Hyperparameters\nIMAGE_SIZE = 224\nBATCH_SIZE = 128\nLEARNING_RATE = 0.001","bc12f2e5":"model = tf.keras.models.Sequential([\n    tf.keras.layers.Conv2D(16, (3,3), activation='relu', input_shape=(IMAGE_SIZE, IMAGE_SIZE, 3)),\n    tf.keras.layers.MaxPooling2D(2, 2),\n\n    tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\n    tf.keras.layers.MaxPooling2D(2,2),\n\n    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n    tf.keras.layers.MaxPooling2D(2,2),\n\n    tf.keras.layers.Conv2D(128, (3,3), activation='relu'),\n    tf.keras.layers.MaxPooling2D(2,2),\n\n    tf.keras.layers.Dropout(0.5),\n    tf.keras.layers.Flatten(),\n    tf.keras.layers.Dense(128, activation='relu'),\n    tf.keras.layers.Dropout(0.3),\n    tf.keras.layers.Dense(1, activation='sigmoid')\n])","0f697134":"from tensorflow.keras.optimizers import RMSprop\n\nmodel.compile(optimizer='adam',\n              loss='binary_crossentropy',\n              metrics = ['acc'])","558e8898":"model.summary()","4ccf8d10":"from tensorflow.keras.preprocessing.image import ImageDataGenerator\n\n# Train\ntrain_datagen = ImageDataGenerator(\n      rescale=1.\/255.0,\n      rotation_range=40,\n      width_shift_range=0.2,\n      height_shift_range=0.2,\n      horizontal_flip=True,\n      fill_mode='nearest')\n\ntrain_generator = train_datagen.flow_from_directory(\n        train_dir,  # Image source\n        target_size=(IMAGE_SIZE, IMAGE_SIZE),  # All images will be resized to 256*256\n        batch_size=BATCH_SIZE,\n        # Since we use binary_crossentropy loss, we need binary labels\n        class_mode='binary')\n\n\n# Test\ntest_datagen = ImageDataGenerator(rescale=1.\/255.0)\ntest_generator = test_datagen.flow_from_directory(\n        test_dir,\n        target_size=(IMAGE_SIZE, IMAGE_SIZE),\n        batch_size=BATCH_SIZE,\n        class_mode='binary')","3bc0dcb2":"import numpy as np\n\nepochs = 15\n\nhistory = model.fit(\n    train_generator,\n    validation_data=test_generator,\n    epochs=epochs,\n    verbose=1 # we want to see all the logs, to stop log set 0\n)","41660eac":"acc = history.history['acc']\nval_acc = history.history['val_acc']\n\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nplt.figure(figsize=(8, 8))\nplt.subplot(2, 1, 1)\nplt.plot(acc, label='Training Accuracy')\nplt.plot(val_acc, label='Validation Accuracy')\nplt.legend(loc='lower right')\nplt.ylabel('Accuracy')\nplt.ylim([min(plt.ylim()),1])\nplt.title('Training and Validation Accuracy')\n\nplt.subplot(2, 1, 2)\nplt.plot(loss, label='Training Loss')\nplt.plot(val_loss, label='Validation Loss')\nplt.legend(loc='upper right')\nplt.ylabel('Cross Entropy')\nplt.ylim([0,1.0])\nplt.title('Training and Validation Loss')\nplt.xlabel('epoch')\nplt.show()","8a040af7":"print(test_generator.class_indices)\n\nnames = {\n    0:\"Cat\",\n    1: \"Dog\"\n}\n\nprint(names)","09b67ac1":"import numpy as np\nfrom keras.preprocessing import image\n\n\ndef predict(path):\n    img = image.load_img(path, target_size=(IMAGE_SIZE, IMAGE_SIZE))\n    x = image.img_to_array(img)\n    x = np.expand_dims(x, axis=0)\n    x \/= 255.0\n    images = np.vstack([x])\n    classes = model.predict(images, batch_size=10)\n    pred = classes[0][0]\n    if pred > 0.5:\n      print(\"It's a dog\")\n    else:\n      print(\"It's a cat\")\n    print(name)","7099c5b8":"!wget -O cat1.jpg -q n https:\/\/cdn.mos.cms.futurecdn.net\/VSy6kJDNq2pSXsCzb6cvYF.jpg\n!wget -O cat2.jpg -q n https:\/\/static.toiimg.com\/thumb\/msid-72295960,imgsize-545889,width-800,height-600,resizemode-75\/72295960.jpg\n!wget -O dog1.jpg -q n https:\/\/post.medicalnewstoday.com\/wp-content\/uploads\/sites\/3\/2020\/02\/322868_1100-800x825.jpg\n!wget -O dog2.jpg -q n https:\/\/cdn.pixabay.com\/photo\/2016\/12\/13\/05\/15\/puppy-1903313__340.jpg","a6798f9e":"predict('cat1.jpg')","cf1802bf":"predict('dog1.jpg')","0fb55e1c":"import matplotlib.pyplot as plt\nimport numpy as np\nimport random\nfrom tensorflow.keras.preprocessing.image import img_to_array, load_img\n\n# Let's define a new Model that will take an image as input, and will output\n# intermediate representations for all layers in the previous model after\n# the first.\nsuccessive_outputs = [layer.output for layer in model.layers[1:]]\nvisualization_model = tf.keras.models.Model(inputs = model.input, outputs = successive_outputs)\n# Let's prepare a random input image from the training set.\ntrain_cats_files = [os.path.join(train_cats_dir, f) for f in train_cats]\ntrain_dogs_files = [os.path.join(train_dogs_dir, f) for f in train_dogs]\nimg_path = random.choice(train_cats_files + train_dogs_files)\nprint(img_path)\n\nimg = load_img(img_path, target_size=(IMAGE_SIZE, IMAGE_SIZE))  # this is a PIL image\nx = img_to_array(img)  # Numpy array with shape (224, 224, 3)\nx = x.reshape((1,) + x.shape)  # Numpy array with shape (1, 224, 224, 3)\n\n# Rescale by 1\/255\nx \/= 255.0\n\n# Let's run our image through our network, thus obtaining all\n# intermediate representations for this image.\nsuccessive_feature_maps = visualization_model.predict(x)\n\n# These are the names of the layers, so can have them as part of our plot\nlayer_names = [layer.name for layer in model.layers]\n\n# Now let's display our representations\nfor layer_name, feature_map in zip(layer_names, successive_feature_maps):\n  if len(feature_map.shape) == 4:\n    # Just do this for the conv \/ maxpool layers, not the fully-connected layers\n    n_features = feature_map.shape[-1]  # number of features in feature map\n    # The feature map has shape (1, size, size, n_features)\n    size = feature_map.shape[1]\n    # We will tile our images in this matrix\n    display_grid = np.zeros((size, size * n_features))\n    for i in range(n_features):\n      # Postprocess the feature to make it visually palatable\n      x = feature_map[0, :, :, i]\n      x -= x.mean()\n      x \/= x.std()\n      x *= 64\n      x += 128\n      x = np.clip(x, 0, 255).astype('uint8')\n      # We'll tile each filter into this big horizontal grid\n      display_grid[:, i * size : (i + 1) * size] = x\n    # Display the grid\n    scale = 20. \/ n_features\n    plt.figure(figsize=(20, 20))\n    plt.title(layer_name)\n    plt.grid(False)\n    plt.imshow(display_grid, cmap='viridis')","714a6aea":"# Transfer Learning\nbase_model = tf.keras.applications.InceptionV3(input_shape=(IMAGE_SIZE, IMAGE_SIZE, 3),\n                                               include_top=False,\n                                               weights='imagenet')","d58052c5":"base_model.summary()","20e4c4b5":"base_model.trainable = False","fb5a492d":"last_layer = base_model.get_layer('mixed10')\nprint('last layer output shape: ', last_layer.output_shape)\nlast_output = last_layer.output","c3150767":"from tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras import layers\nfrom tensorflow.keras import Model\n\n# Flatten the output layer to 1 dimension\nx = layers.Flatten()(last_output)\n# Add a fully connected layer with 1,024 hidden units and ReLU activation\nx = layers.Dense(256, activation='relu')(x)\n# Add a dropout rate of 0.3\nx = layers.Dropout(0.2)(x)               \n# Add a final sigmoid layer for classification\nx = layers.Dense  (1, activation='sigmoid')(x)           \n\ntransfer_model = Model(base_model.input, x) \n\ntransfer_model.compile(optimizer = Adam(lr=LEARNING_RATE), \n              loss=\"binary_crossentropy\", \n              metrics = ['accuracy'])","1336c6cc":"transfer_model.summary()","0cb9638b":"epochs = 10\n\n# using EarlyStopping\nearly_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=2)\n\nhistory = transfer_model.fit(\n    train_generator,\n    validation_data=test_generator,\n    epochs=epochs,\n    verbose=1, # we want to see all the logs, to stop log set 0\n    callbacks = [early_callback]\n)","bf70a95b":"acc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\n\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nplt.figure(figsize=(8, 8))\nplt.subplot(2, 1, 1)\nplt.plot(acc, label='Training Accuracy')\nplt.plot(val_acc, label='Validation Accuracy')\nplt.legend(loc='lower right')\nplt.ylabel('Accuracy')\nplt.ylim([min(plt.ylim()),1])\nplt.title('Training and Validation Accuracy')\n\nplt.subplot(2, 1, 2)\nplt.plot(loss, label='Training Loss')\nplt.plot(val_loss, label='Validation Loss')\nplt.legend(loc='upper right')\nplt.ylabel('Cross Entropy')\nplt.ylim([0,1.0])\nplt.title('Training and Validation Loss')\nplt.xlabel('epoch')\nplt.show()","c67dbfad":"import numpy as np\nfrom keras.preprocessing import image\n\ndef predict(path):\n    img = image.load_img(path, target_size=(IMAGE_SIZE, IMAGE_SIZE))\n    x = image.img_to_array(img)\n    # rescalse\n    x \/= 255.0\n\n    x = np.expand_dims(x, axis=0)\n\n    images = np.vstack([x])\n    classes = transfer_model.predict(images)\n    pred = classes[0][0]\n\n    if pred > 0.5:\n      print(\"It's a dog\")\n    else:\n      print(\"It's a cat\")","383aad4c":"predict('cat.png')","6eb760ff":"predict('cat1.jpg')","f9a1a897":"predict('cat2.jpg')","3d09a6d5":"predict('dog1.jpg')","bfa6c2bf":"predict('dog2.jpg')","26220f09":"## Center crop Image\nCrop the image from center up to the image part you desire.","04753fdc":"## Rotate image\nRotate an image from 0 to 40 degrees","c492df55":"# Download Datasets\n**Cats and Dogs** dataset also avaiable in [tensorflow dataset](https:\/\/www.tensorflow.org\/datasets\/catalog\/cats_vs_dogs)\nTo load datasets, you can use following code\n```\nimport tensorflow_datasets as tfds\n\ntrain_ds, validation_ds, test_ds = tfds.load(\n    \"cats_vs_dogs\",\n    # Reserve 10% for validation and 10% for test\n    split=[\"train[:40%]\", \"train[40%:50%]\", \"train[50%:60%]\"],\n    as_supervised=True,  # Include labels\n)\n\n```\nTo download file, you can also use \n```\ntf.keras.utils.get_file('fine_name.zip', origin=_URL, extract=True)\n```","66d65092":"## Get the last layers,\nby using this code you can get any layer\n```\nlast_layer = base_model.get_layer('mixed7')\n```","bf72b8cb":"### 2. See how the images look like","5c215321":"## Flip Image\nhere, we are using **horizontal_flip** only.","adf7c697":"Lets see model summary","72ec7d88":"# Transfer Learning\n\nTransfer learning consists of taking features learned on one problem, and leveraging them on a new, similar problem. For instance, features from a model that has learned to identify racoons may be useful to kick-start a model meant to identify tanukis.\n\nTransfer learning is usually done for tasks where your dataset has too little data to train a full-scale model from scratch.\n\nThe most common incarnation of transfer learning in the context of deep learning is the following worfklow:\n\n- Take layers from a previously trained model.\n- Freeze them, so as to avoid destroying any of the information they contain during future training rounds.\n- Add some new, trainable layers on top of the frozen layers. They will learn to turn the old features into predictions on a new dataset.\n- Train the new layers on your dataset\n\n","c1c15df2":"# Visualizing Intermediate Representations","fcc640b9":"We are using Tensorflow v2.\n# Why Tensorflow?\nTensorFlow is an end-to-end open source platform for machine learning. It has a comprehensive, flexible ecosystem of tools, libraries and community resources that lets researchers push the state-of-the-art in ML and developers easily build and deploy ML powered applications","133d8a7a":"## Prediction\nnow send a random image to the model, and see the model prediction","69006c3f":"## Model Summary\nprint model summary, and take a close look on output shape","1883494c":"## Predict","a4938476":"## Flip Image\nFlip left to right, and see how it's look","deb5b9a2":"Now create a helper function to visualize and compare the original and augmented image side by side.","66d154fd":"Here is only, 3 types of argumentation, used from **TF.IMAGE** module\n\nyou can check other available augmentation, check [TF DOC](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/image)\n\nAnd also check TensorFlow official documents about Data Augmentation, [TF DOC](https:\/\/www.tensorflow.org\/tutorials\/images\/data_augmentation)","a01c88eb":"# What Next?\n\n- Play with hyperparameters\n  *   Use different Learning Rate\n  *   Use different optimizer\n  *   Use differenet augmentation\n\n- Play with different datasets\n - It's binnary classification\n - try multiclass classification\n - try multilable classifitcation\n\n- Play with different Model\n  - Here I used **InceptionV3**\n  - Play with other models\n\n\nNote: don't forget to checkout TF DOC\n\n\n  And Thank you very much for reading this Kernel","4fab9f50":"## Data Preprocessing\nNow prepare data, and don't forget to add data argumentation.\n\nyou can try with data argumentation and without data argumentation and see the training changes.\n\nHere we will use data argumentation only with train set","655d9be1":"## Train Model","cf90e67b":"## Width and Height Shifting\nSometimes main object, remain in the side, so by using **width shift** and **height shift** you can move them into center.","4c64e1f7":"To track training progress, you can use callbacks,\n1. Early stopping\nby using this callback you can stop training when a monitored metric has stopped improving.\n```\n    early_callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3)\n```\n2. Custom Callback\nyou can create a custom callback, For instance here\n```\nclass myCallback(tf.keras.callbacks.Callback):\n    def on_epoch_end(self, epoch, logs={}):\n        if(logs.get('acc')>0.9):\n          print(\"\\n 90% accuracy, stop training\")\n          self.model.stop_training = True      \n```\nthis callback will stop training when the accuracy reached into 90%\n\nCheck [TF DOC](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/callbacks)\n\nYou can use all the callbacks in this way -\n```\nmodel.fit(....,\n        callbacks = [early_callback, custom_callback])\n```","8a9816a6":"when you are preparing all the images to fit the model, you can use this **TF.IMAGE**. Here is the tutorial [check this](https:\/\/www.tensorflow.org\/tutorials\/images\/data_augmentation)\n\nThere is another class available, **ImageDataGenerator** you can use it. It's really easy to use, just pass the parameters, and it will do the augmentation on the fly. It under the [tf.keras.preprocessing.image](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/preprocessing\/image\/ImageDataGenerator) module.\n\nLets check it first,\n\n```\n    datagen = ImageDataGenerator()\n```\nThen use **flow_from_directory** method and set image path and other parameters\n```\n    datagen.flow_from_directory(img_path)\n```\n\nBut you need to make sure, your source folder arranged this way\n```\n  train \/\n     cats\/\n        image1.png\n        .......\n     dogs\/\n        image1.png\n        ..........\n```\n\n","5b342ba3":"Let's load a cute cat image from the internet","7fc85dd9":"### Download some pictures","9f9790ec":"## Build new model\nWhen we load our pre-trained model, we load only CNN layers\n```\ninclude_top=False\n```\n\nNow we need to add some Dense layers. By using Keras sequential layers, you can do it easily, But I showing a different way","8ad1b30b":"## Plot History","21acda3e":"# Data augmentation","21777340":"# Data Exploration\nLet's explore the datasets - \n1. Check how much data available\n2. See how the images look like","60a587e5":"### 1. Check how much data available","d1ce12f6":"# Build Model\n\nLets build model -\n\n**** We are using CNN architcture to classify images *****\n\n1. In the first layer you need to specify the input shape\n```\nConv2D(16, (3,3), activation='relu', input_shape=(150, 150, 3)),\n```\n2. Here we need to classify cats and dogs only so it's a binary classification, so output will be O or 1. And here we use '**sigmoid**' as activation function.\n```\ntf.keras.layers.Dense(1, activation='sigmoid')\n```\n3. To create a model in tensorflow, we will define a Sequential layer as before, adding some convolutional layers first. And at the end we will add some Dense Layer\n```\ntf.keras.models.Sequential([\n    tf.keras.layers.Conv2D(16, (3,3), activation='relu', input_shape=(150, 150, 3)),\n    .........\n    tf.keras.layers.Dense(1, activation='sigmoid') \n])\n```\n\nHere we will build a basic model, we can achieve almost ~70% accuracy. We will use transfer learning later and easily can archive almost ~98% accuracy on validation data. ","3a9d54fc":"Convert this image into tensor","0cf1914f":"## Rotate image\nRotate an image by 90 degrees","d0b144e0":"Now we need to compile our model.\n1. Set optimizer -> RMSprop(lr=0.001)\n2. Set loss function -> binary_crossentropy\n\nYou can use other optimizers too, check this document [TF DOC](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/optimizers)","261d0d20":"Notes: in some augmentation, you will notice some pixel is loss, to fix this you can add **fill_mode** agrument\n```\nImageDataGenerator(\n      fill_mode='nearest')\n```\n\nYou have so many other options avaiable, see the complete list of parameters\n\n```\ntf.keras.preprocessing.image.ImageDataGenerator(\n    featurewise_center=False, samplewise_center=False,\n    featurewise_std_normalization=False, samplewise_std_normalization=False,\n    zca_whitening=False, zca_epsilon=1e-06, rotation_range=0, width_shift_range=0.0,\n    height_shift_range=0.0, brightness_range=None, shear_range=0.0, zoom_range=0.0,\n    channel_shift_range=0.0, fill_mode='nearest', cval=0.0, horizontal_flip=False,\n    vertical_flip=False, rescale=None, preprocessing_function=None,\n    data_format=None, validation_split=0.0, dtype=None\n)\n```","e3a5ee93":"## Train Model","cfa290c7":"## Training History","be1b28f2":"In this example, We are using InceptionV3.\nTo load pre-trained model -\n1. You can use **TFHUB**\n```\nhub.KerasLayer(\"https:\/\/tfhub.dev\/google\/imagenet\/inception_v3\/classification\/4\", trainable=True)\n```\n2. You can use **keras.applications** module\n```\ntf.keras.applications.InceptionV3(input_shape=(IMAGE_SIZE, IMAGE_SIZE, 3),\n                                               include_top=False,\n                                               weights='imagenet')\n```","651e2109":"# Cats and Dogs (Basic to end) TF\nIn this notebook, covered topics\n- Data Augmentation\n- Data Preprocessing\n- Build Model from scratch using CNN architecture\n- Train Base Model\n- TF callbacks (prevent overfitting)\n- Visualizing Intermediate Representations \n- Transfer Learning","281c2c50":"Freeze the base model"}}