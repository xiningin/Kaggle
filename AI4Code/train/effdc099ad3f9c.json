{"cell_type":{"76de1b47":"code","e996b8e2":"code","f922eb4c":"code","12963110":"code","f3ecd45d":"code","127bd533":"code","bd47d02d":"code","26223cdf":"code","97306c93":"code","e87909f5":"code","4f94395f":"code","e6844696":"code","cdd6d670":"code","7517d0be":"code","4b30e044":"code","5b9012af":"code","57689513":"code","be9d519f":"code","9594d1a9":"code","00de4d44":"code","90ee2186":"code","a4e54bcb":"code","658676c4":"markdown","f4329c72":"markdown","8cc87929":"markdown","09cb7a7d":"markdown","7cd3b106":"markdown"},"source":{"76de1b47":"import json\nimport re\nimport gc\nimport os\nimport pickle\nimport itertools\nimport pandas as pd\nimport numpy as np\nfrom glob import glob\nfrom datetime import datetime as dt\nfrom pathlib import Path\nfrom tqdm import tqdm\nimport datetime\nts_conv = np.vectorize(datetime.datetime.fromtimestamp) # ut(10 digit) -> date\n\n# pandas settings -----------------------------------------\npd.set_option(\"display.max_colwidth\", 100)\npd.set_option(\"display.max_rows\", None)\npd.set_option(\"display.max_columns\", None)\npd.options.display.float_format = '{:,.5f}'.format\n\n# Graph drawing -------------------------------------------\nimport matplotlib\nfrom matplotlib import font_manager\nimport matplotlib.pyplot as plt\nimport matplotlib.cm as cm\nfrom matplotlib import rc\nfrom matplotlib_venn import venn2, venn2_circles\nfrom matplotlib import animation as ani\nfrom IPython.display import Image\nfrom pylab import imread\nfrom IPython.display import HTML\n\nplt.rcParams[\"patch.force_edgecolor\"] = True\nfrom IPython.display import display # Allows the use of display() for DataFrames\nimport seaborn as sns\nsns.set(style=\"whitegrid\", palette=\"muted\", color_codes=True)\nsns.set_style(\"whitegrid\", {'grid.linestyle': '--'})\nred = sns.xkcd_rgb[\"light red\"]\ngreen = sns.xkcd_rgb[\"medium green\"]\nblue = sns.xkcd_rgb[\"denim blue\"]\n\n%matplotlib inline\n%config InlineBackend.figure_format='retina'\n\n# ML -------------------------------------------\nfrom sklearn.preprocessing import LabelEncoder\n\ndef unpickle(filename):\n    with open(filename, 'rb') as fo:\n        p = pickle.load(fo)\n    return p\n\ndef to_pickle(filename, obj):\n    with open(filename, 'wb') as f:\n        pickle.dump(obj, f, -1)\n\n\n\nclass FeatureStore():\n    \n    # necessayr to re-check\n    floor_convert = {'1F' :  0, '2F' : 1, '3F' : 2, '4F' : 3, '5F' : 4, \n                     '6F' : 5, '7F' : 6, '8F' : 7, '9F' : 8,\n                     'B'  : -1, 'B1' : -1, 'B2' : -2, 'B3' : -3, \n                     'BF' : -1, 'BM' : -1, \n                     'F1' : 0, 'F2' : 1, 'F3' : 2, 'F4' : 3, 'F5' : 4, \n                     'F6' : 5, 'F7' : 6, 'F8' : 7, 'F9' : 8, 'F10': 9,\n                     'L1' : 0, 'L2' : 1, 'L3' : 2, 'L4' : 3, 'L5' : 4, \n                     'L6' : 5, 'L7' : 6, 'L8' : 7, 'L9' : 8, 'L10': 9, \n                     'L11': 10,\n                     'G'  : 0, 'LG1': 0, 'LG2': 1, 'LM' : 0, 'M'  : 0, \n                     'P1' : 0, 'P2' : 1,}\n    \n    df_types = ['accelerometer',\n                'accelerometer_uncalibrated',\n                'beacon',\n                'gyroscope',\n                'gyroscope_uncalibrated',\n                'magnetic_field',\n                'magnetic_field_uncalibrated',\n                'rotation_vector',\n                'waypoint',\n                'wifi']\n    \n    # https:\/\/github.com\/location-competition\/indoor-location-competition-20\n    df_type_cols = {'accelerometer': [\"timestamp\", \"x\", \"y\", \"z\", \"accuracy\"],\n                'accelerometer_uncalibrated': [\"timestamp\", \"x\", \"y\", \"z\", \n                                               \"x2\", \"y2\", \"z2\", \"accuracy\" ],\n                'beacon': [\"timestamp\", \"uuid\", \"major_id\", \"minor_id\", \"tx_power\", \n                           \"rssi\", \"distance\", \"mac_addr\", \"timestamp2\"],\n                'gyroscope': [\"timestamp\", \"x\", \"y\", \"z\", \"accuracy\"],\n                'gyroscope_uncalibrated': [\"timestamp\", \"x\", \"y\", \"z\", \n                                           \"x2\", \"y2\", \"z2\", \"accuracy\" ],\n                'magnetic_field': [\"timestamp\", \"x\", \"y\", \"z\", \"accuracy\"],\n                'magnetic_field_uncalibrated': [\"timestamp\", \"x\", \"y\", \"z\", \n                                                \"x2\", \"y2\", \"z2\", \"accuracy\" ],\n                'rotation_vector': [\"timestamp\", \"x\", \"y\", \"z\", \"accuracy\"],\n                'waypoint': [\"timestamp\", \"x\", \"y\"],\n                'wifi': [\"timestamp\", \"ssid\", \"bssid\",\"rssi\",\"frequency\",\n                         \"last_seen_timestamp\",]}\n\n    dtype_dict = {}\n    dtype_dict[\"accelerometer\"] = {\"timestamp\":int, \"x\":float, \"y\":float, \"z\":float, \n                                   \"accuracy\":int}\n    dtype_dict[\"accelerometer_uncalibrated\"] = {\"timestamp\":int, \"x\":float, \"y\":float, \n                                                \"z\":float, \"x2\":float, \"y2\":float, \n                                                \"z2\":float, \"accuracy\":int}\n    dtype_dict[\"beacon\"] = {\"timestamp\":int, \"uuid\":str, \"major_id\":str, \n                            \"minor_id\":str, \"tx_power\":int,  \"rssi\":int, \n                            \"distance\":float, \"mac_addr\":str, \"timestamp2\":int}\n    dtype_dict[\"gyroscope\"] = {\"timestamp\":int, \"x\":float, \"y\":float, \"z\":float, \n                               \"accuracy\":int}\n    dtype_dict[\"gyroscope_uncalibrated\"] = {\"timestamp\":int, \"x\":float, \"y\":float, \n                                            \"z\":float, \"x2\":float, \"y2\":float, \n                                            \"z2\":float, \"accuracy\":int}\n    dtype_dict[\"magnetic_field\"] = {\"timestamp\":int, \"x\":float, \"y\":float, \n                                    \"z\":float, \"accuracy\":int}\n    dtype_dict[\"magnetic_field_uncalibrated\"] = {\"timestamp\":int, \"x\":float, \n                                                 \"y\":float, \"z\":float, \"x2\":float, \n                                                 \"y2\":float, \"z2\":float, \"accuracy\":int}\n    dtype_dict[\"rotation_vector\"] = {\"timestamp\":int, \"x\":float, \"y\":float, \n                                     \"z\":float, \"accuracy\":int}\n    dtype_dict[\"waypoint\"] = {\"timestamp\":int, \"x\":float, \"y\":float, \"z\":float}\n    dtype_dict[\"wifi\"] = {\"timestamp\":int, \"ssid\":str, \"bssid\":str,\n                          \"rssi\":int,\"frequency\":int, \"last_seen_timestamp\":int}\n\n    def __init__(self, site_id, floor, path_id, \n                 input_path=\"..\/input\/indoor-location-navigation\/\",\n                 save_path=\"..\/mid\"):\n        self.site_id = site_id.strip()\n        self.floor = floor.strip()\n        self.n_floor = self.floor_convert[self.floor]\n        self.path_id = path_id.strip()\n        \n        self.input_path = input_path\n        assert Path(input_path).exists(), f\"input_path do not exist: {input_path}\"\n        \n        self.save_path = save_path\n        Path(save_path).mkdir(parents=True, exist_ok=True)\n        \n        self.site_info = SiteInfo(site_id=self.site_id, floor=self.floor, input_path=self.input_path)\n        \n    def _flatten(self, l):\n        return list(itertools.chain.from_iterable(l))\n    \n    def multi_line_spliter(self, s):\n        matches = re.finditer(\"TYPE_\", s)\n        matches_positions = [match.start() for match in matches]\n        split_idx = [0] + [matches_positions[i]-14 for i in range(1, len(matches_positions))] + [len(s)]\n        return [s[split_idx[i]:split_idx[i+1]] for i in range(len(split_idx)-1)]\n    \n    def load_df(self, ):\n        path = str(Path(self.input_path)\/f\"train\/{self.site_id}\/{self.floor}\/{self.path_id}.txt\")\n        with open(path) as f:\n            data = f.readlines()\n        \n        modified_data = []\n        for s in data:\n            if s.count(\"TYPE_\")>1:\n                lines = self.multi_line_spliter(s)\n                modified_data.extend(lines)\n            else:\n                modified_data.append(s)\n        del data\n        self.meta_info_len = len([d for d in modified_data if d[0]==\"#\"])\n        self.meta_info_df = pd.DataFrame([m.replace(\"\\n\", \"\").split(\":\") \n                                          for m in self._flatten([d.split(\"\\t\") \n                                                                  for d in modified_data if d[0]==\"#\"]) if m!=\"#\"])\n\n        data_df = pd.DataFrame([d.replace(\"\\n\", \"\").split(\"\\t\") for d in modified_data if d[0]!=\"#\"])\n        for dt in self.df_types:\n            # select data type\n            df_s = data_df[data_df[1]==f\"TYPE_{dt.upper()}\"]\n            if len(df_s)==0:\n                setattr(self, dt, pd.DataFrame(columns=self.df_type_cols[dt]))\n            else:\n                # remove empty cols\n                na_info = df_s.isna().sum(axis=0) == len(df_s)\n                df_s = df_s[[i for i in na_info[na_info==False].index if i!=1]].reset_index(drop=True)\n                \n                if len(df_s.columns)!=len(self.df_type_cols[dt]):\n                    df_s.columns = self.df_type_cols[dt][:len(df_s.columns)]\n                else:\n                    df_s.columns = self.df_type_cols[dt]\n            \n                # set dtype          \n                for c in df_s.columns:\n                    df_s[c] = df_s[c].astype(self.dtype_dict[dt][c])\n                                     \n                # set DataFrame to attr\n                setattr(self, dt, df_s)\n    \n    def get_site_info(self, keep_raw=False):\n        self.site_info.get_site_info(keep_raw=keep_raw)\n            \n    def load_all_data(self, keep_raw=False):     \n        self.load_df()\n        self.get_site_info(keep_raw=keep_raw)\n        \n    def __getitem__(self, item):\n        if item in self.df_types:\n            return getattr(self, item)\n        elif item==\"sensors\":\n            try:\n                return getattr(self, \"sensor_df\")\n            except:\n                self.sensor_df = pd.concat([feature[\"magnetic_field\"].set_index(\"timestamp\"), \n                                       feature[\"accelerometer\"].set_index(\"timestamp\"), \n                                       feature[\"gyroscope\"].set_index(\"timestamp\")], axis=1)\n                if self.sensor_df.shape[1]==12:\n                    self.sensor_df.columns = [\"mag_x\", \"mag_y\", \"mag_z\", \"mag_acc\", \n                                              \"acc_x\", \"acc_y\", \"acc_z\", \"acc_acc\",\n                                              \"gyr_x\", \"gyr_y\", \"gyr_z\", \"gyr_acc\", ]\n                else:\n                    self.sensor_df.columns = [\"mag_x\", \"mag_y\", \"mag_z\", \n                                              \"acc_x\", \"acc_y\", \"acc_z\", \n                                              \"gyr_x\", \"gyr_y\", \"gyr_z\",  ]\n                return self.sensor_df\n        else:\n            return None\n    \n    def save(self, ):\n        # to be implemented\n        pass\n    \n    \nclass SiteInfo():\n    def __init__(self, site_id, floor, input_path=\"..\/input\/indoor-location-navigation\/\"):\n        self.site_id = site_id\n        self.floor = floor\n        self.input_path = input_path\n        assert Path(input_path).exists(), f\"input_path do not exist: {input_path}\"\n        \n    def get_site_info(self, keep_raw=False):\n        floor_info_path = f\"{self.input_path}\/metadata\/{self.site_id}\/{self.floor}\/floor_info.json\"\n        with open(floor_info_path, \"r\") as f:\n            self.floor_info = json.loads(f.read())\n            self.site_height = self.floor_info[\"map_info\"][\"height\"]\n            self.site_width = self.floor_info[\"map_info\"][\"width\"]\n            if not keep_raw:\n                del self.floor_info\n            \n        geojson_map_path = f\"{self.input_path}\/metadata\/{self.site_id}\/{self.floor}\/geojson_map.json\"\n        with open(geojson_map_path, \"r\") as f:\n            self.geojson_map = json.loads(f.read())\n            self.map_type = self.geojson_map[\"type\"]\n            self.features = self.geojson_map[\"features\"]\n            \n            self.floor_coordinates = self.features[0][\"geometry\"][\"coordinates\"]\n            self.store_coordinates = [self.features[i][\"geometry\"][\"coordinates\"] \n                                          for i in range(1, len(self.features))]\n                \n            if not keep_raw:\n                del self.geojson_map\n    \n    def show_site_image(self, ax):\n        path = f\"{self.input_path}\/metadata\/{self.site_id}\/{self.floor}\/floor_image.png\"\n        ax.imshow(imread(path), extent=[0, self.site_width, 0, self.site_height])\n\n    def draw_polygon(self, size=8, only_floor=False):\n\n        fig = plt.figure()\n        ax = plt.subplot(111)\n            \n        xmax, xmin, ymax, ymin = self._draw(self.floor_coordinates, ax, calc_minmax=True)\n        if not only_floor:\n            self._draw(self.store_coordinates, ax, fill=True)\n        plt.legend([])\n        \n        xrange = xmax - xmin\n        yrange = ymax - ymin\n        ratio = yrange \/ xrange\n        \n        self.x_size = size\n        self.y_size = size*ratio\n\n        fig.set_figwidth(size)\n        fig.set_figheight(size*ratio)\n        # plt.show()\n        return ax\n        \n    def _draw(self, coordinates, ax, fill=False, calc_minmax=False):\n        xmax, ymax = -np.inf, -np.inf\n        xmin, ymin = np.inf, np.inf\n        for i in range(len(coordinates)):\n            ndim = np.ndim(coordinates[i])\n            if ndim==2:\n                corrd_df = pd.DataFrame(coordinates[i])\n                if fill:\n                    ax.fill(corrd_df[0], corrd_df[1], alpha=0.7)\n                else:\n                    corrd_df.plot.line(x=0, y=1, style=\"-\", ax=ax)\n                        \n                if calc_minmax:\n                    xmax = max(xmax, corrd_df[0].max())\n                    xmin = min(xmin, corrd_df[0].min())\n\n                    ymax = max(ymax, corrd_df[1].max())\n                    ymin = min(ymin, corrd_df[1].min())\n            elif ndim==3:\n                for j in range(len(coordinates[i])):\n                    corrd_df = pd.DataFrame(coordinates[i][j])\n                    if fill:\n                        ax.fill(corrd_df[0], corrd_df[1], alpha=0.6)\n                    else:\n                        corrd_df.plot.line(x=0, y=1, style=\"-\", ax=ax)\n                        \n                    if calc_minmax:\n                        xmax = max(xmax, corrd_df[0].max())\n                        xmin = min(xmin, corrd_df[0].min())\n\n                        ymax = max(ymax, corrd_df[1].max())\n                        ymin = min(ymin, corrd_df[1].min())\n            else:\n                assert False, f\"ndim of coordinates should be 2 or 3: {ndim}\"\n        if calc_minmax:\n            return xmax, xmin, ymax, ymin\n        else:\n            return None\n","e996b8e2":"from joblib import Parallel, delayed\nimport multiprocessing\n\ndataframe_type = ['accelerometer', 'beacon', 'gyroscope', 'magnetic_field', 'wifi', 'waypoint',]\n\ndef get_train_target(used_site_ids):\n    \"\"\"\n    Get list of target site_id, s_floor, path_id\n    :return:\n    \"\"\"\n    target_list = []\n    for site_id in used_site_ids:\n        folders = sorted(glob(os.path.join(base_path, 'train', site_id + '\/*')))\n        for folder in folders:\n            s_floor = folder.split('\/')[-1]\n            files = glob(os.path.join(folder, \"*.txt\"))\n\n            for file in files:\n                path_id = file.split(\"\/\")[-1].replace(\".txt\", \"\")\n                target_list.append([site_id, s_floor, path_id])\n\n    return target_list\n\n\ndef applyParallel(data_list, func, n_jobs=multiprocessing.cpu_count()):\n    ret_list = Parallel(n_jobs=n_jobs)(delayed(func)((data)) for data in data_list)\n    return ret_list\n\n\ndef count_data_length(data):\n    site_id = data[0]\n    s_floor = data[1]\n    path_id = data[2]\n    feature = FeatureStore(site_id=site_id, floor=s_floor, path_id=path_id)\n    feature.load_df()\n    \n    data_len_dict = {}\n    for c in dataframe_type:\n        data_len_dict[c] = len(feature[c])\n        \n    data_len_dict[\"wp_ts_diff_sec\"] = (feature[\"waypoint\"][\"timestamp\"].max() - feature[\"waypoint\"][\"timestamp\"].min()) \/ 1000\n    data_len_dict[\"sensor_ts_diff_sec\"] = (feature[\"accelerometer\"][\"timestamp\"].max() - feature[\"accelerometer\"][\"timestamp\"].min()) \/ 1000\n    \n    data_len_dict[\"site_id\"] = site_id\n    data_len_dict[\"floor\"] = s_floor\n    data_len_dict[\"path_id\"] = path_id\n    return data_len_dict\n\ndef multi_line_spliter(s):\n    matches = re.finditer(\"TYPE_\", s)\n    matches_positions = [match.start() for match in matches]\n    split_idx = [0] + [matches_positions[i] - 14 for i in range(1, len(matches_positions))] + [len(s)]\n    return [s[split_idx[i]:split_idx[i + 1]] for i in range(len(split_idx) - 1)]\n\n\ndef read_txt(file):\n    with open(file) as f:\n        txt = f.readlines()\n\n    modified_data = []\n    for s in txt:\n        if s.count(\"TYPE_\") > 1:\n            lines = multi_line_spliter(s)\n            modified_data.extend(lines)\n        else:\n            modified_data.append(s)\n    return modified_data\n\ndef _count_data_length_test(data):\n    data_df = pd.DataFrame([d.replace(\"\\n\", \"\").split(\"\\t\") for d in data if d[0]!=\"#\"])\n    data_dict = {}\n    for dt in FeatureStore.df_types:\n        # select data type\n        df_s = data_df[data_df[1]==f\"TYPE_{dt.upper()}\"]\n        if len(df_s)==0:\n            data_dict[dt] = 0\n        else:\n            # remove empty cols\n            na_info = df_s.isna().sum(axis=0) == len(df_s)\n            df_s = df_s[[i for i in na_info[na_info==False].index if i!=1]].reset_index(drop=True)\n\n            if len(df_s.columns)!=len(FeatureStore.df_type_cols[dt]):\n                df_s.columns = FeatureStore.df_type_cols[dt][:len(df_s.columns)]\n            else:\n                df_s.columns = FeatureStore.df_type_cols[dt]\n\n            # set dtype          \n            for c in df_s.columns:\n                df_s[c] = df_s[c].astype(FeatureStore.dtype_dict[dt][c])\n            data_dict[dt] = len(df_s)\n    return data_dict\n\ndef count_data_length_test(data):\n    site_id = data[0]\n    path_id = data[1]\n    file = f\"..\/input\/indoor-location-navigation\/test\/{path_id}.txt\"\n    content = read_txt(file)\n    data_len_dict = _count_data_length_test(content)\n    \n    data_len_dict[\"site_id\"] = site_id\n    data_len_dict[\"path_id\"] = path_id\n    return data_len_dict ","f922eb4c":"base_path = '..\/input\/indoor-location-navigation\/'\n\nssubm = pd.read_csv('..\/input\/indoor-location-navigation\/sample_submission.csv')\nssubm_df = ssubm[\"site_path_timestamp\"].apply(lambda x: pd.Series(x.split(\"_\")))\nssubm_df.columns = [\"site_id\", \"path_id\", \"timestamp\"]\nssubm_df[\"site_path_timestamp\"] = ssubm_df.apply(lambda x: f\"{x.site_id}_{x.path_id}_{x.timestamp}\", axis=1)\nused_site_ids = sorted(ssubm_df[\"site_id\"].value_counts().index.tolist())\n\ntarget_list = get_train_target(used_site_ids)\nprint(\"target_list: \", len(target_list))","12963110":"ret_list = applyParallel(target_list, count_data_length)\ncount_df = pd.DataFrame(ret_list)\ncount_df.to_csv(\"count_df.csv\")","f3ecd45d":"count_df.groupby(\"site_id\")[dataframe_type].sum()","127bd533":"count_df.groupby([\"site_id\", \"floor\"])[dataframe_type].sum()","bd47d02d":"target_test = [(r.site_id, r.path_id) for i, r in ssubm_df.drop_duplicates(subset=[\"site_id\", \"path_id\"]).iterrows()]","26223cdf":"ret_list_test = applyParallel(target_test, count_data_length_test)\ncount_test_df = pd.DataFrame(ret_list_test)","97306c93":"test_targets = ssubm_df.groupby([\"path_id\"]).timestamp.count().reset_index().rename({\"timestamp\": \"target_cnt\"}, axis=\"columns\")","e87909f5":"count_test_df_ = count_test_df.merge(test_targets, on=\"path_id\", how=\"left\")\ncount_test_df_.to_csv(\"count_test_df.csv\")","4f94395f":"count_test_df_.head()","e6844696":"count_test_df_.groupby(\"site_id\")[dataframe_type+[\"target_cnt\"]].sum()","cdd6d670":"count_df.wifi.hist(bins=30)","7517d0be":"count_df.accelerometer.hist(bins=30)","4b30e044":"count_df.waypoint.hist(bins=30)","5b9012af":"count_df.plot.scatter(x=\"accelerometer\", y=\"waypoint\", s=5, alpha=0.7)","57689513":"count_df.plot.scatter(x=\"wifi\", y=\"waypoint\", s=5, alpha=0.7)","be9d519f":"count_test_df_.wifi.hist(bins=30)","9594d1a9":"count_test_df_.accelerometer.hist(bins=30)","00de4d44":"count_test_df_.target_cnt.hist(bins=30)","90ee2186":"count_test_df_.plot.scatter(x=\"accelerometer\", y=\"target_cnt\", s=5, alpha=0.7)","a4e54bcb":"count_test_df_.plot.scatter(x=\"wifi\", y=\"target_cnt\", s=5, alpha=0.7)","658676c4":"# Test data","f4329c72":"## test data","8cc87929":"## train data","09cb7a7d":"# Train data","7cd3b106":"# EDA"}}