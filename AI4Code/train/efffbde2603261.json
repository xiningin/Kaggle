{"cell_type":{"3ac0dd2f":"code","69a3caf4":"code","53fe50a3":"code","2ce91607":"code","f7a0d00f":"code","9f8c912e":"code","58826d55":"markdown"},"source":{"3ac0dd2f":"%%writefile agent.py\n\nimport random\n\ndef random_agent(observation, configuration):\n    return random.randrange(configuration.banditCount)","69a3caf4":"%%writefile always_first_agent.py\n\ndef always_first(observation, configuration):\n    return 0","53fe50a3":"%%writefile sample_agent.py\n\nimport math\n\nlast_bandit = -1\ntotal_reward = 0\n\nsums_of_reward = None\nnumbers_of_selections = None\n\ndef ucb_agent(observation, configuration):    \n    global sums_of_reward, numbers_of_selections, last_bandit, total_reward\n\n    if observation.step == 0:\n        numbers_of_selections = [0] * configuration[\"banditCount\"]\n        sums_of_reward = [0] * configuration[\"banditCount\"]\n\n    if last_bandit > -1:\n        reward = observation.reward - total_reward\n        sums_of_reward[last_bandit] += reward\n        total_reward += reward\n\n    bandit = 0\n    max_upper_bound = 0\n    for i in range(0, configuration.banditCount):\n        if (numbers_of_selections[i] > 0):\n            average_reward = sums_of_reward[i] \/ numbers_of_selections[i]\n            delta_i = math.sqrt(2 * math.log(observation.step+1) \/ numbers_of_selections[i])\n            upper_bound = average_reward + delta_i\n        else:\n            upper_bound = 1e400\n        if upper_bound > max_upper_bound and last_bandit != i:\n            max_upper_bound = upper_bound\n            bandit = i\n            last_bandit = bandit\n\n    numbers_of_selections[bandit] += 1\n\n    if bandit is None:\n        bandit = 0\n\n    return bandit","2ce91607":"%%writefile bay_sub.py\n\nimport numpy as np\nfrom scipy.stats import beta\n\nps_a = None\npost_b = None\nbandit = None\ntotal_reward = 0\n\n\ndef agent(observation, configuration):\n    global reward_sums, total_reward, bandit, post_a, post_b\n    \n    n_bandits = configuration.banditCount\n\n    if observation.step == 0:\n        post_a = np.ones(n_bandits)\n        post_b = np.ones(n_bandits)\n    else:\n        r = observation.reward - total_reward\n        total_reward = observation.reward\n\n        post_a[bandit] += r + (1 - observation.step \/ 2000)\n        post_b[bandit] += (1 - r)\n\n    \n    bound = post_a \/ (post_a + post_b).astype(float) + beta.std(post_a, post_b) * 3\n    bandit = int(np.argmax(bound))\n    \n    return bandit","f7a0d00f":"!pip install kaggle-environments --upgrade","9f8c912e":"from kaggle_environments import make\n\nenv = make(\"mab\", debug=True)\n\nenv.run([\"sample_agent.py\", \"bay_sub.py\"])\nenv.render(mode=\"ipython\", width=800, height=800)","58826d55":"Just experimenting with some variables of <a href=\"https:\/\/www.kaggle.com\/xhlulu\/santa-2020-ucb-and-bayesian-ucb-starter\">kernel developed by xhlulu<\/a>. You can also check his kernel."}}