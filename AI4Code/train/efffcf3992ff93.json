{"cell_type":{"c1b4f593":"code","75906f75":"code","0b4abe41":"code","e39157a0":"code","657bc284":"code","d45c9d02":"code","768ff7bd":"code","8d6e064e":"code","01661aaa":"code","72cf048c":"code","f3520f27":"code","baabef26":"code","6b3370fc":"code","43bdff91":"code","4c476b6b":"code","f6f189e1":"code","d0ca8355":"code","c8c62350":"code","1cf81c0a":"code","2cdcca5c":"code","bf18406a":"code","0326550b":"code","a85bea26":"code","64ba3c54":"code","b1d49995":"code","c2808043":"code","4bdd99b6":"code","0e777a5c":"code","e0da2e99":"code","877678b1":"markdown","1b733be3":"markdown","c3ec398a":"markdown","906a3784":"markdown","fd8611eb":"markdown","c2344c0e":"markdown","660fae36":"markdown","39adc9e7":"markdown","25135d9c":"markdown","8e8e4106":"markdown","0818aa54":"markdown","1f956936":"markdown","87bff3ad":"markdown","624fb794":"markdown","4d04590f":"markdown","18437e46":"markdown","e822b8fd":"markdown","d038f614":"markdown","b0edfa83":"markdown","ff8e7596":"markdown","c93f8532":"markdown","9381b98c":"markdown","0893444b":"markdown","4ef19b39":"markdown","79b1700b":"markdown","d48bfc58":"markdown","3383c1ae":"markdown","a6c54f7e":"markdown","85734d0a":"markdown","b5028695":"markdown","a2fca3cf":"markdown"},"source":{"c1b4f593":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport missingno as msno\nimport matplotlib.pyplot as plt\nimport os\n\n%matplotlib inline","75906f75":"df = pd.read_csv('..\/input\/merchants.csv')\n\nprint(\"Size of the dataframe: \", df.shape); display(df.head(5))","0b4abe41":"df.info()","e39157a0":"# Filter onlu nissing values\nnull_columns=df.columns[df.isnull().any()]\nmsno.bar(df[null_columns])","657bc284":"for c in ['avg_sales_lag3', 'avg_sales_lag6', 'avg_sales_lag12']:\n    df[c] = df[c].fillna(df[c].mean())","d45c9d02":"# add other category \ndf['category_2'] = df.category_2.fillna(df.category_2.max()+1)","768ff7bd":"# replace inf to zero\ndf = df.replace([np.inf, -np.inf], np.nan).fillna(0)","8d6e064e":"#merchant_group_id\ncategorical_columns = ['merchant_category_id','subsector_id',\n                       'category_1', 'most_recent_sales_range', 'most_recent_purchases_range',\n                       'category_4', 'city_id', 'state_id', 'category_2']\n\ndf_enc = pd.get_dummies(df, columns=categorical_columns)\nprint(df_enc.shape)\ndf_enc.head()","01661aaa":"from sklearn.preprocessing import MinMaxScaler\n\nscaler    = MinMaxScaler()\ndf_values = df_enc.drop('merchant_id', axis=1)\ndf_norm   = scaler.fit_transform(df_values)","72cf048c":"from keras.layers import Lambda, Input, Dense\nfrom keras.models import Model\nfrom keras.datasets import mnist\nfrom keras.losses import mse, binary_crossentropy\nfrom keras.utils import plot_model\nfrom keras import backend as K\nfrom keras.callbacks import ModelCheckpoint\nfrom keras.layers import Input, Dense, Lambda, Layer, Add, Multiply\nfrom keras.models import Model, Sequential\n\nimport argparse\nimport os","f3520f27":"# network parameters\noriginal_dim= df_enc.shape[1]-1\ninput_shape = (original_dim, )\nintermediate_dim = int(original_dim\/2)\nbatch_size = 128\nlatent_dim = 64\nepochs     = 80\nepsilon_std = 1.0","baabef26":"class KLDivergenceLayer(Layer):\n\n    \"\"\" Identity transform layer that adds KL divergence\n    to the final model loss.\n    \"\"\"\n\n    def __init__(self, *args, **kwargs):\n        self.is_placeholder = True\n        super(KLDivergenceLayer, self).__init__(*args, **kwargs)\n\n    def call(self, inputs):\n\n        mu, log_var = inputs\n\n        kl_batch = - .5 * K.sum(1 + log_var -\n                                K.square(mu) -\n                                K.exp(log_var), axis=-1)\n\n        self.add_loss(K.mean(kl_batch), inputs=inputs)\n\n        return inputs","6b3370fc":"# VAE Architecture\n# * original_dim - Original Input Dimension\n# * intermediate_dim - Hidden Layer Dimension\n# * latent_dim - Latent\/Embedding Dimension\ndef vae_arc(original_dim, intermediate_dim, latent_dim):\n    # Decode\n    decoder = Sequential([\n        Dense(intermediate_dim, input_dim=latent_dim, activation='relu'),\n        Dense(original_dim, activation='sigmoid')\n    ])\n\n    # Encode\n    x = Input(shape=(original_dim,))\n    h = Dense(intermediate_dim, activation='relu')(x)\n\n    z_mu = Dense(latent_dim)(h)\n    z_log_var = Dense(latent_dim)(h)\n\n    z_mu, z_log_var = KLDivergenceLayer()([z_mu, z_log_var])\n    z_sigma = Lambda(lambda t: K.exp(.5*t))(z_log_var)\n\n    eps = Input(tensor=K.random_normal(stddev=epsilon_std,\n                                       shape=(K.shape(x)[0], latent_dim)))\n    z_eps = Multiply()([z_sigma, eps])\n    z = Add()([z_mu, z_eps])\n\n    x_pred = decoder(z)\n    \n    return x, eps, z_mu, x_pred","43bdff91":"def nll(y_true, y_pred):\n    \"\"\" Negative log likelihood (Bernoulli). \"\"\"\n\n    # keras.losses.binary_crossentropy gives the mean\n    # over the last axis. we require the sum\n    return K.sum(K.binary_crossentropy(y_true, y_pred), axis=-1)","4c476b6b":"x, eps, z_mu, x_pred = vae_arc(original_dim, intermediate_dim, latent_dim)\nvae            = Model(inputs=[x, eps], outputs=x_pred)\nvae.compile(optimizer='adam', loss=nll)","f6f189e1":"vae.summary()","d0ca8355":"from sklearn.model_selection import train_test_split\n\n# \nX_train, X_test, y_train, y_test = train_test_split(df_norm, df_norm, \n                                                    test_size=0.33, random_state=42)","c8c62350":"filepath   =\"weights.hdf5\"\ncheckpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\ncallbacks_list = [checkpoint]","1cf81c0a":"# train\nhist = vae.fit(X_train, X_train,\n        epochs=epochs,\n        batch_size=batch_size,\n        callbacks=callbacks_list,\n        validation_data=(X_test, X_test))","2cdcca5c":"def plt_hist(hist):\n    # summarize history for loss\n    plt.plot(hist.history['loss'])\n    plt.plot(hist.history['val_loss'])\n    plt.title('model loss')\n    plt.ylabel('loss')\n    plt.xlabel('epoch')\n    plt.legend(['train', 'test'], loc='upper left')","bf18406a":"plt_hist(hist)","0326550b":"from sklearn.manifold import TSNE\nfrom sklearn.decomposition import PCA\n\ndef plt_reduce(x, color='merchant_category_id'):\n    '''\n    Plot Scatter with color\n    '''\n    plt.figure(figsize=(6, 6))\n    plt.scatter(x[:, 0], x[:, 1], c=df[color],\n            alpha=.4, s=3**2, cmap='viridis')\n    #plt.colorbar()\n    plt.show()","a85bea26":"# Predict Embedding values\nencoder = Model(x, z_mu)\nz_df    = encoder.predict(df_norm, batch_size=batch_size)","64ba3c54":"# Reduce dimmension\npca      = PCA(n_components=2)\nx_reduce = pca.fit_transform(z_df)","b1d49995":"# Plot with merchant_category_id color\nplt_reduce(x_reduce, 'merchant_category_id')","c2808043":"# Plot with subsector_id color\nplt_reduce(x_reduce, 'subsector_id')","4bdd99b6":"# Plot with city_id color\nplt_reduce(x_reduce, 'city_id')","0e777a5c":"df_embedding = pd.DataFrame(z_df)\ndf_embedding['merchant_id'] = df.merchant_id\ndf_embedding.head(5)","e0da2e99":"df_embedding.to_csv('merchant_id_embedding.csv')","877678b1":"**How does a variational autoencoder work?**\n\nFirst, an encoder network turns the input samples x into two parameters in a latent space, which we will note *z_mean* and *z_log_sigma*. Then, we randomly sample similar points z from the latent normal distribution that is assumed to generate the data, via *z = z_mean + exp(z_log_sigma) * epsilon*, where epsilon is a random normal tensor. Finally, a decoder network maps these latent space points back to the original input data.\n\nThe parameters of the model are trained via two loss functions: a reconstruction loss forcing the decoded samples to match the initial inputs (just like in our previous autoencoders), and the KL divergence between the learned latent distribution and the prior distribution, acting as a regularization term. You could actually get rid of this latter term entirely, although it does help in learning well-formed latent spaces and reducing overfitting to the training data.","1b733be3":"## Training Variational autoencoder (VAE)\n","c3ec398a":"#### Fix a missing values","906a3784":"![](https:\/\/www.fast.ai\/images\/instacart.png)\nhttps:\/\/www.fast.ai\/2018\/04\/29\/categorical-embeddings\/","fd8611eb":"An embedding is a relatively low-dimensional space into which you can translate high-dimensional vectors. Embeddings make it easier to do machine learning on large inputs like sparse vectors representing words. Ideally, an embedding captures some of the semantics of the input by placing semantically similar inputs close together in the embedding space. An embedding can be learned and reused across models.","c2344c0e":"### What is Embedding ?","660fae36":"### What is Variational autoencoder (VAE)\n\n* https:\/\/www.jeremyjordan.me\/variational-autoencoders\/\n* https:\/\/blog.keras.io\/building-autoencoders-in-keras.html\n\nA variational autoencoder (VAE) provides a probabilistic manner for describing an observation in latent space. Thus, rather than building an encoder which outputs a single value to describe each latent state attribute, we'll formulate our encoder to describe a probability distribution for each latent attribute.","39adc9e7":"# Variational AutoEncoder to create Embedding of Merchants\n\nIn this notebook, I will use a Variational AutoEncoder (VAE) to create a Merchant Embedding. This information can be used in ML algotithms with higher semantic quality and similarity betweeen Merchants.\n\n* **Introduction**\n    * What is Embedding ?\n    * How to use Merchants Embedding ?\n    * What is Variational autoencoder (VAE)\n* **Data Preparation**\n    * Load Dataset\n    * Data Engineer\n* **Training VAE**\n* **Visualization of latent space**\n","25135d9c":"#### Training VAE","8e8e4106":"#### Build Model\n\nhttps:\/\/tiao.io\/post\/tutorial-on-variational-autoencoders-with-a-concise-keras-implementation\/\n\nKeras is awesome. It is a very well-designed library that clearly abides by its guiding principles of modularity and extensibility, enabling us to easily assemble powerful, complex models from primitive building blocks. This has been demonstrated in numerous blog posts and tutorials, in particular, the excellent tutorial on Building Autoencoders in Keras. As the name suggests, that tutorial provides examples of how to implement various kinds of autoencoders in Keras, including the variational autoencoder (VAE)1.","0818aa54":"#### Data Engineer\n\n* Fix a missing values\n* One hot encoder for categorical columns\n* Normalize MinMax","1f956936":"#### Normalize MinMax","87bff3ad":"### Save Embedding\n\nJoin embedding with merchant_id and save pandas","624fb794":"Note this is a valid definition of a Keras loss, which is required to compile and optimize a model. It is a symbolic function that returns a scalar for each data-point in y_true and y_pred. In our example, y_pred will be the output of our decoder network, which are the predicted probabilities, and y_true will be the true probabilities.","4d04590f":"#### PCA - Principal Component Analysis","18437e46":"and the category column, i will put another category","e822b8fd":"#### How to use Merchants Embedding ?","d038f614":"Split dataset and train\/test","b0edfa83":"This notebook only creates the embeddings of all Merchants for use by ML-Models","ff8e7596":"##### continue....","c93f8532":"The only features that has missing values is the **avg_sales_lag3**, **avg_sales_lag6**, **avg_sales_lag12** and  **category_2**.","9381b98c":"So a natural language modelling technique like Word Embedding is used to map words or phrases from a vocabulary to a corresponding vector of real numbers. As well as being amenable to processing by learning algorithms, this vector representation has two important and advantageous properties:\n\n* **Dimensionality Reduction**\u200a\u2014\u200ait is a more efficient representation\n* **Contextual Similarity**\u200a\u2014\u200ait is a more expressive representation","0893444b":"#### one hot encoder for categorical columns","4ef19b39":"## Introduction","79b1700b":"![](https:\/\/www.jeremyjordan.me\/content\/images\/2018\/03\/Screen-Shot-2018-03-18-at-12.24.19-AM.png)","d48bfc58":"Float columns i will put a average value","3383c1ae":"The \"Sample from distributions\" it's owr Embedding Layer. I will encoder all Merchants and take a Embedding Layer.","a6c54f7e":"![](https:\/\/tiao.io\/post\/tutorial-on-variational-autoencoders-with-a-concise-keras-implementation\/vae_full.svg)","85734d0a":"## Data Preparation\n\nThis session transform the variables from the original dataset, corrects missing values and normalizes the data for training. ","b5028695":"We can use the Embedding as input of the model, containing a reduced dimensionality but with much semantic information.  The previous example shows the use of product, store and customer embedding for a consumer products cecommendation model.","a2fca3cf":"## Visualization of latent space\n\nSince our latent space is not two-dimensional, we will use PCA to reduce dimensionality, so we can use some interesting visualizations that can be made at this point. One is to look at the neighborhoods of different classes in the latent 2D plane:"}}