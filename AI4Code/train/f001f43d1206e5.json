{"cell_type":{"334170b5":"code","f7762bf2":"code","1717d137":"code","aecb8cc2":"code","46810eb1":"code","e1614db5":"code","9b882351":"code","367dab54":"code","36957255":"code","268bf463":"code","774c27dc":"code","c6333ad3":"code","b07409aa":"code","a6ff7587":"code","8fc4f23f":"code","e31ff79d":"code","50efcf2f":"code","7bce4c0d":"code","7a4c6861":"code","90e8c56a":"code","ee0faa10":"code","145695ab":"code","297d3c65":"code","092cb765":"code","725fd47d":"code","93bc9b85":"code","598fce0e":"code","a59edbaf":"code","b8ebb661":"code","a8a770a3":"code","3012f1fa":"code","e2f3c6d5":"markdown","ca470d25":"markdown"},"source":{"334170b5":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport matplotlib.animation as animation\nimport random\nimport time\nimport math\n#PATH PROCESS\nimport os\nimport os.path\nfrom pathlib import Path\nimport glob\n#IMAGE PROCESS\nfrom PIL import Image\nfrom tensorflow.keras.preprocessing import image\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nimport cv2\nfrom tensorflow.keras.applications.vgg16 import preprocess_input, decode_predictions\nimport imageio\nfrom IPython.display import Image\nimport matplotlib.image as mpimg\nfrom skimage.transform import resize\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom matplotlib import cm\nimport zipfile\nfrom skimage import data, io, filters\nfrom io import BytesIO\nfrom nibabel import FileHolder\nfrom nibabel.analyze import AnalyzeImage\nimport PIL\nfrom IPython import display\nfrom skimage.morphology import convex_hull_image, erosion\nfrom skimage.morphology import square\nfrom skimage.feature import hessian_matrix, hessian_matrix_eigvals\nfrom skimage import data, io, filters\nimport skimage\nfrom scipy.ndimage.interpolation import zoom\nfrom scipy.ndimage import gaussian_filter\nfrom IPython.display import HTML\n#SCALER & TRANSFORMATION\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras import regularizers\nfrom sklearn.preprocessing import LabelEncoder\n#ACCURACY CONTROL\nfrom sklearn.metrics import confusion_matrix, accuracy_score, classification_report, roc_auc_score, roc_curve\nfrom sklearn.model_selection import GridSearchCV, cross_val_score\nfrom sklearn.metrics import mean_squared_error, r2_score\n#OPTIMIZER\nfrom tensorflow.keras.optimizers import RMSprop,Adam,Optimizer,Optimizer, SGD\n#MODEL LAYERS\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D, BatchNormalization,MaxPooling2D,BatchNormalization,\\\n                        Permute, TimeDistributed, Bidirectional,GRU, SimpleRNN,\\\nLSTM, GlobalAveragePooling2D, SeparableConv2D, ZeroPadding2D, Convolution2D, ZeroPadding2D,Reshape, Conv2DTranspose, LeakyReLU, ReLU\nfrom tensorflow.keras import models\nfrom tensorflow.keras import layers\nimport tensorflow as tf\nfrom tensorflow.keras.applications import VGG16,VGG19,inception_v3\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras.utils import plot_model\nfrom tensorflow.keras.datasets import mnist\nimport tensorflow.keras\nfrom tensorflow.keras.models import Model\n#IGNORING WARNINGS\nfrom warnings import filterwarnings\nfilterwarnings(\"ignore\",category=DeprecationWarning)\nfilterwarnings(\"ignore\", category=FutureWarning) \nfilterwarnings(\"ignore\", category=UserWarning)","f7762bf2":"Video_Path = \"..\/input\/uchuunew-dark-matter-simulation-naoj\/UCHUU_NEW_SIMULATION_DARK_MATTER\/20210910Ishiyama_time_per.mp4\"","1717d137":"FRAME_LIST = []\n\nVideo_Cap_Func = cv2.VideoCapture(Video_Path)\n\nwhile Video_Cap_Func.isOpened():\n    \n    _,frame = Video_Cap_Func.read()\n    \n    if _ != True:\n        break\n        \n    if Video_Cap_Func.isOpened():\n        \n        Trans_Frame = cv2.cvtColor(frame,cv2.COLOR_BGR2RGB)\n        Resize_Frame = cv2.resize(Trans_Frame,(180,180))\n        \n        Reduce_Frame = Resize_Frame \/ 255.\n        FRAME_LIST.append(Reduce_Frame)\n        \nVideo_Cap_Func.release()","aecb8cc2":"print(np.shape(np.array(FRAME_LIST)))","46810eb1":"figure = plt.figure(figsize=(8,8))\n\nPick_IMG = FRAME_LIST[1]\nplt.xlabel(Pick_IMG.shape)\nplt.ylabel(Pick_IMG.size)\nplt.imshow(Pick_IMG)","e1614db5":"figure = plt.figure(figsize=(8,8))\n\nPick_IMG = FRAME_LIST[100]\nplt.xlabel(Pick_IMG.shape)\nplt.ylabel(Pick_IMG.size)\nplt.imshow(Pick_IMG)","9b882351":"figure = plt.figure(figsize=(8,8))\n\nPick_IMG = FRAME_LIST[1300]\nplt.xlabel(Pick_IMG.shape)\nplt.ylabel(Pick_IMG.size)\nplt.imshow(Pick_IMG)","367dab54":"ARRAY_FRAME = np.array(FRAME_LIST,dtype=\"float32\")","36957255":"print(ARRAY_FRAME.shape)","268bf463":"figure,axis = plt.subplots(4,4,figsize=(15,15))\n\nfor x_number,x_operators in enumerate(axis.flat):\n    \n    EXAMPLE_PICKING = ARRAY_FRAME[x_number * 10]\n    \n    \n    x_operators.axis(\"off\")\n    x_operators.imshow(EXAMPLE_PICKING)\n    \nplt.tight_layout()\nplt.show()","774c27dc":"ITERATIONS = 50\nVECTOR_NOISE_SHAPE = 180\nCOUNT_EXAMPLE= 32\nBATCH_SIZE = 6\nCOUNT_BUFFER = 60000","c6333ad3":"seed = tf.random.normal([COUNT_EXAMPLE,VECTOR_NOISE_SHAPE])","b07409aa":"TRAIN_SET = tf.data.Dataset.from_tensor_slices(ARRAY_FRAME).shuffle(COUNT_BUFFER).batch(BATCH_SIZE)","a6ff7587":"def Generator_Model():\n    \n    \n    Model = Sequential()\n    #\n    Model.add(Dense(90*90*180,use_bias=False,input_shape=(180,)))\n    Model.add(BatchNormalization())\n    Model.add(LeakyReLU())\n    #\n    Model.add(Reshape((90,90,180)))\n    #\n    Model.add(Conv2DTranspose(128,(2,2),padding=\"same\",use_bias=False))\n    Model.add(BatchNormalization())\n    Model.add(LeakyReLU())\n    \n    Model.add(Conv2DTranspose(64, (2,2), strides=(2,2), padding='same', use_bias=False))\n    Model.add(BatchNormalization())\n    Model.add(LeakyReLU())\n\n    #\n    Model.add(Conv2DTranspose(3,(2,2),padding=\"same\",use_bias=False,activation=\"tanh\"))\n    \n    \n    return Model","8fc4f23f":"def Discriminator_Model():\n    \n    Model = Sequential()\n    \n    Model.add(Conv2D(64,(2,2),padding=\"same\",input_shape=[180,180,3]))\n    Model.add(Dropout(0.3))\n    Model.add(LeakyReLU())\n    \n    Model.add(Conv2D(128,(2,2),padding=\"same\"))\n    Model.add(Dropout(0.3))\n    Model.add(LeakyReLU())\n\n\n    Model.add(layers.Flatten())\n    Model.add(layers.Dense(1))\n    \n    return Model","e31ff79d":"Generator = Generator_Model()","50efcf2f":"Discriminator = Discriminator_Model()","7bce4c0d":"Generator_Optimizer = RMSprop(lr=0.0003,clipvalue=1.0,decay=1e-8)\nDiscriminator_Optimizer = RMSprop(lr=0.0003,clipvalue=1.0,decay=1e-8)","7a4c6861":"Loss_Function = tf.keras.losses.BinaryCrossentropy(from_logits=True)","90e8c56a":"def Discriminator_Loss(real_out,fake_out):\n    \n    Real_Loss_Func = Loss_Function(tf.ones_like(real_out),real_out)\n    Fake_Loss_Func = Loss_Function(tf.zeros_like(fake_out),fake_out)\n    Total_Loss = Real_Loss_Func + Fake_Loss_Func\n    \n    return Total_Loss","ee0faa10":"def Generator_Loss(fake_out):\n    \n    return Loss_Function(tf.ones_like(fake_out),fake_out)","145695ab":"def display_and_save_images(model, epoch, test_input):\n    \n    predictions = model(test_input, training=False)\n    fig = plt.figure(figsize=(12, 12))\n    \n    for i in range(predictions.shape[0]):\n        plt.subplot(8, 4, i+1)\n        plt.imshow(predictions[i, :, :, :])\n        plt.axis('off')\n    plt.show()","297d3c65":"os.mkdir(\".\/SINGLE\")","092cb765":"READING_NEW_LIST = []\nplt.style.use(\"dark_background\")\n\ndef SINGLE_save_images(model, epoch, test_input):\n    \n    predictions = model(test_input, training=False)\n    for i in range(predictions.shape[0]):\n        \n        fig = plt.figure(figsize=(10, 10))\n        plt.imshow(predictions[i, :, :, :])\n        plt.axis('off')\n        \n        SAVE_PARAM = random.randint(0,20000)\n        \n        plt.savefig('.\/SINGLE\/single_image{:04d}.png'.format(epoch+i+SAVE_PARAM))\n        plt.show()\n    \n        reading_image = cv2.cvtColor(cv2.imread('.\/SINGLE\/single_image{:04d}.png'.format(epoch+i+SAVE_PARAM)),cv2.COLOR_BGR2RGB)\n\n        READING_NEW_LIST.append(reading_image)","725fd47d":"def Train_Step(images):\n    \n    random_noise_vector = tf.random.normal([BATCH_SIZE,VECTOR_NOISE_SHAPE])\n    \n    with tf.GradientTape() as Generator_Tape, tf.GradientTape() as Discriminator_Tape:\n        \n        Generator_Fake_Image = Generator(random_noise_vector,training=False)\n        \n        real_output = Discriminator(images,training=True)\n        fake_output = Discriminator(Generator_Fake_Image,training=True)\n        \n        Generator_Loss_Output = Generator_Loss(fake_output)\n        Discriminator_Loss_Output = Discriminator_Loss(real_output,fake_output)\n        \n    Generator_Gradients = Generator_Tape.gradient(Generator_Loss_Output,Generator.trainable_variables)\n    Discriminator_Gradients = Discriminator_Tape.gradient(Discriminator_Loss_Output,Discriminator.trainable_variables)\n    \n    Generator_Optimizer.apply_gradients(zip(Generator_Gradients,Generator.trainable_variables))\n    Discriminator_Optimizer.apply_gradients(zip(Discriminator_Gradients,Discriminator.trainable_variables))","93bc9b85":"def Training(dataset,iterations):\n    \n    for epoch in range(iterations):\n        start = time.time()\n        \n        for image_batch in dataset:\n            Train_Step(image_batch)\n            \n        display.clear_output(wait=True)\n        SINGLE_save_images(Generator,epoch+1,seed)","598fce0e":"Training(TRAIN_SET,ITERATIONS)","a59edbaf":"def displaying_sour(source):\n    \n    figure = plt.figure(figsize=(14,14))\n    \n    Image_List = []\n    plt.style.use(\"dark_background\")\n    for counting,indexing in enumerate(source):\n        \n        \n        Read_IMG = plt.imshow(indexing, animated=True)\n        plt.axis('off')\n        Image_List.append([Read_IMG])\n\n    Animation_Func = animation.ArtistAnimation(figure, Image_List, interval=314, repeat_delay=10200)\n    \n    plt.close()\n    \n    return Animation_Func","b8ebb661":"Image_List = []","a8a770a3":"RED_LIST = READING_NEW_LIST[:200]","3012f1fa":"HTML(displaying_sour(RED_LIST).to_html5_video())","e2f3c6d5":"# MODEL PROCESS","ca470d25":"# DATA AND FRAME PROCESS"}}