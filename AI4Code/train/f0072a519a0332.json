{"cell_type":{"4b878522":"code","ec3b1ac0":"code","2ccfb4c2":"code","b864e129":"code","be2c75c3":"code","fc6581ce":"code","d0b092a5":"code","894072ed":"code","80cbbe44":"code","254f149a":"code","88044887":"code","b355de06":"code","fd585a3e":"code","27a508e2":"code","e3454ac5":"code","47ea3f1f":"code","bc0465bc":"code","209bf0a7":"code","5f866977":"code","b26a8834":"code","eb490578":"code","066a512a":"code","2a436aec":"code","70e9c5d3":"code","fd94c9e9":"code","ef35ca89":"code","b7a72948":"code","893df763":"code","31377be6":"code","451214d7":"code","50727a10":"code","153ad5d6":"code","5098ae4b":"code","80f35050":"code","ca5ef90b":"markdown","19f7bfd3":"markdown","d8a72d12":"markdown","834342b4":"markdown","573af834":"markdown","d518decb":"markdown","dbe58153":"markdown","465c293d":"markdown","f9aa8e0e":"markdown","c2ca723a":"markdown","c22aada5":"markdown","fcdbd28b":"markdown"},"source":{"4b878522":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","ec3b1ac0":"import zipfile\n\nsample_submission_zip = \"..\/input\/jigsaw-toxic-comment-classification-challenge\/sample_submission.csv.zip\"\ntest_zip = \"..\/input\/jigsaw-toxic-comment-classification-challenge\/test.csv.zip\"\ntrain_zip = \"..\/input\/jigsaw-toxic-comment-classification-challenge\/train.csv.zip\"\ntest_labels_zip = \"..\/input\/jigsaw-toxic-comment-classification-challenge\/test_labels.csv.zip\"\n\nfor file in [sample_submission_zip, test_zip, train_zip, test_labels_zip]:\n    zip_ref = zipfile.ZipFile(file, 'r')\n    zip_ref.extractall('.\/input\/csv_files\/')\n    zip_ref.close()","2ccfb4c2":"print(os.listdir('.\/input\/csv_files\/'))\n\ntrain = pd.read_csv(\".\/input\/csv_files\/train.csv\")\ntest = pd.read_csv(\".\/input\/csv_files\/test.csv\")\ntest_labels = pd.read_csv(\".\/input\/csv_files\/test_labels.csv\")\nsample_submission = pd.read_csv(\".\/input\/csv_files\/sample_submission.csv\")","b864e129":"train.head(20)","be2c75c3":"train.iloc[2][\"comment_text\"]","fc6581ce":"test.head(5)","d0b092a5":"test_labels.head(5)","894072ed":"sample_submission.head(5)","80cbbe44":"print(train.shape)\nprint(test.shape)","254f149a":"# check for nulls\nprint(train.isnull().any(), \"\\n\")\nprint(test_labels.isnull().any())","88044887":"for col_name in ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']:\n    print(\"For\", col_name, '\\n')\n    print(train[col_name].value_counts())\n    print(\"\\n\")","b355de06":"import sys, os, re, csv\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt","fd585a3e":"from keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\n\nfrom keras.layers import Dense, Dropout, Input, LSTM, Embedding, Activation\nfrom keras.layers import GlobalMaxPool1D, Bidirectional\nfrom keras.models import Model\n\nfrom keras import initializers, optimizers, regularizers, constraints, layers","27a508e2":"import spacy\n# removing some functions from nlp since we have over 1 lakh comments and that would take up a lot of time\nnlp = spacy.load(\"en_core_web_lg\", disable=[\"parser\", \"tagger\", \"ner\"])\n\ndef clean_text(text):\n    \n    # Processing text with the nlp object returns a Doc object that holds all information about the tokens, \n    # their linguistic features and their relationships.\n    doc = nlp(text)\n    \n    # getting tokens, lemmatizing words and eliminating basic english stop-words\n    tokens=[token.lemma_.strip() for token in doc if \n            not token.is_stop and not nlp.vocab[token.lemma_].is_stop\n            ] # Removing only StopWords\n        \n    # Recreation of the text\n    text=\" \".join(tokens)\n\n    # Returning the text\n    return text\n\n\ntrain[\"comment_text\"] = train[\"comment_text\"].apply(lambda x: clean_text(x))\ntest[\"comment_text\"] = test[\"comment_text\"].apply(lambda x: clean_text(x))\n\ntrain.head(20)","e3454ac5":"list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n\nall_train_classes_values = train[list_classes].values\n\nlist_sentences_train = train[\"comment_text\"]\nlist_sentences_test = test[\"comment_text\"]","47ea3f1f":"list_sentences_train","bc0465bc":"from keras.preprocessing.text import Tokenizer\n\n# Intialize the Tokenizer by specifying the maximum number of unique words it can take for forming the dictionary \nmax_features = 20000\n\n# Intialize tokenizer object\ntokenizer = Tokenizer(num_words = max_features)\n\n# Fit tokenizer on the list of train comments\ntokenizer.fit_on_texts(list(list_sentences_train))\n\n# Get tokenized words\nlist_tokenized_train = tokenizer.texts_to_sequences(list_sentences_train)\nlist_tokenized_test = tokenizer.texts_to_sequences(list_sentences_test)","209bf0a7":"count = 0\n\nfor i in list_tokenized_train:\n    if count <= 5:\n        print(i)\n        print(\"\\n\")\n    count += 1 ","5f866977":"# Summary of word counts\nfrom more_itertools import take\n\ntake(10, tokenizer.word_counts.items())","b26a8834":"# Indices associated with words\n\ntake(10,tokenizer.word_index.items())","eb490578":"len(tokenizer.word_counts)","066a512a":"sentenceLength = [len(one_comment) for one_comment in list_tokenized_train]\n\nplt.hist(sentenceLength, bins = np.arange(0, 410, 10))","2a436aec":"# We can take a safe bet on the max length of the sentence by taking the number 3 std dev away from the mean\nnp.mean(sentenceLength) + 3*np.std(sentenceLength)","70e9c5d3":"from keras.preprocessing.sequence import pad_sequences\n\nmax_sen_len = 173\n\nX_train = pad_sequences(list_tokenized_train, maxlen = max_sen_len)\nX_test = pad_sequences(list_tokenized_test, maxlen = max_sen_len)","fd94c9e9":"X_train","ef35ca89":"max_features","b7a72948":"from keras.layers import Dense, Dropout, Input, LSTM, Embedding, Activation\nfrom keras.layers import GlobalMaxPool1D, Bidirectional\nfrom keras.models import Model\nfrom keras.models import Sequential\n\nfrom keras import initializers, optimizers, regularizers, constraints, layers\n\nmodel = Sequential()\n\n\n# Embedding takes input dimension, output dimension, input maximum length\nembedding = Embedding(max_features, # max_features are 20000 in number\n                      128, \n                      input_length = max_sen_len)\nmodel.add(embedding)\n# The embedding layer outputs a 3-D matrix of dimensions None*173*128\n# None indicates that its size is inferred\n# 173 is the sequence length \n# 128 indicates that for each of the 173 words in the sequence we have 128 coordinates associated with each word in the sequence\n\n\n# LSTM layer takes the output of embedding layer and generates a matrix of None*173*60 where 60 is taken as the output dimension\nmodel.add(LSTM(60, return_sequences = True))\n\n\n# Using Max Pooling to reduce dimensions from 3D to 2D\nmodel.add(GlobalMaxPool1D())\n\n\n# Now adding Dense and Dropout layers\nmodel.add(Dropout(0.1))\nmodel.add(Dense(50, activation = \"relu\"))\nmodel.add(Dropout(0.1))\n\n# Adding the final Dense layer with 6 classes and sigmoid activation function\nmodel.add(Dense(6, activation=\"sigmoid\"))\n\n# Since we are tackling binary classification problem we use loss as binary_crossentropy\nmodel.compile(loss=\"binary_crossentropy\",\n             optimizer=\"adam\",\n             metrics=[\"accuracy\"])\n# The default learning rate is 0.001","893df763":"# To check for correct input and output dimensions for each layer\nmodel.summary()","31377be6":"# To view any layer's output\nfrom keras import backend as K\n\nget_3rd_layer_ouput = K.function([model.layers[0].input],\n                                 [model.layers[2].output])\n\n# Passing one comment and checking the output \nlayer_output = get_3rd_layer_output([X_train[:1]])[0]\nlayer_output.shape","451214d7":"# Running the model with 10% validation set\n\n# 32 sentences\/comments will be passed in each batch\nbatch_size = 32 \n\n# 2 iterations\nepochs = 2\n\nmodel.fit(X_train, all_train_classes_values, batch_size = batch_size, epochs = epochs, validation_split = 0.1)","50727a10":"y_pred = model.predict(X_test)\ny_pred","153ad5d6":"sample_submission[list_classes] = y_pred\nsample_submission.head(10)","5098ae4b":"list_classes","80f35050":"# Adding a threshold of 0.5 for each category\ndef threshold_conversion(x):\n    output = 0\n    if x>0.5:\n        output = 1\n    else:\n        ouput = 0\n    return output\n\nsample_submission['toxic'] = sample_submission['toxic'].apply(lambda x: threshold_conversion(x))\nsample_submission['severe_toxic'] = sample_submission['severe_toxic'].apply(lambda x: threshold_conversion(x))\nsample_submission['obscene'] = sample_submission['obscene'].apply(lambda x: threshold_conversion(x))\nsample_submission['threat'] = sample_submission['threat'].apply(lambda x: threshold_conversion(x))\nsample_submission['insult'] = sample_submission['insult'].apply(lambda x: threshold_conversion(x))\nsample_submission['identity_hate'] = sample_submission['identity_hate'].apply(lambda x: threshold_conversion(x))\nsample_submission.head(10)","ca5ef90b":"## References:\n\n1. https:\/\/www.kaggle.com\/muhardianabasandi\/lstm-jigsaw-toxiccomment-classification\n2. https:\/\/www.kaggle.com\/sbongo\/for-beginners-tackling-toxic-using-keras\n3. https:\/\/www.kaggle.com\/jhoward\/improved-lstm-baseline-glove-dropout\n4. https:\/\/www.kaggle.com\/sbongo\/do-pretrained-embeddings-give-you-the-extra-edge\/![image.png](attachment:e8ba7320-c2ca-4c92-8874-3ea79c1ca2c6.png)![image.png](attachment:0afb5a2b-f4f2-4e55-9d91-94be9275618b.png)![image.png](attachment:8074f7c4-aab7-4570-92d5-af35d4766d2f.png)![image.png](attachment:9b845677-3fe1-4b74-8ea4-b2aa33c3b5fe.png)","19f7bfd3":"### This notebook is for supporting Jigsaw:\n\nhttps:\/\/jigsaw.google.com\/\n\nJigsaw is a unit within Google that explores threats to open societies, and builds technology that inspires scalable solutions.\n\n![jigsaw-lead.jpeg](attachment:9a0351e0-c100-4ca0-a6a2-f41afb7cdd55.jpeg)","d8a72d12":"**Accuracy of the training and validation sets are good so we can proceed with prediction**","834342b4":"## Method 1:  Using Keras LSTM","573af834":"As we can observe all sequences\/statements are of different length so we need <b>Padding<\/b> and <b> Shortening <\/b> of very long sentences.\n<br><br> This needs to be done to get all sentences of equal length so that they could be later fed to an LSTM model. Also, we cannot pad to maximum length sentence since that would create a highly sparse matrix and we cannot shorten it to very short lengths because that would lead us lose information and thus affect accuracy.","d518decb":"#### By using stop word removal we reduced the noise in the data from a total 210337 words to 173827\n#### That is the noise reduction was done by 17.35%","dbe58153":"So this is what we are going to do:\n\n<li><b>Tokenization<\/b> - We need to break down the sentence into unique words. For eg, \"I love cats and love dogs\" will become [\"I\",\"love\",\"cats\",\"and\",\"dogs\"]\n    <br><li><b>Indexing<\/b> - We put the words in a dictionary-like structure and give them an index each For eg, {1:\"I\",2:\"love\",3:\"cats\",4:\"and\",5:\"dogs\"}\n        <br><li><b>Index Representation<\/b>- We could represent the sequence of words in the comments in the form of index, and feed this chain of index into our LSTM. For eg, [1,2,3,4,2,5]","465c293d":"### Building the LSTM model","f9aa8e0e":"<b><li> We cannot remove punctuations (like quotes, exclamation mark), special characters(@*%) or digits since those are key in identifying toxic words. \n<br><li> We also cannot lower case words as uppercase might indicate comments written in anger or frustration \n<br><li> So we will just remove the essential english stop-words <\/b>","c2ca723a":"**Keras imports**","c22aada5":"**Pre-processing of words in comments to feed it to the LSTM model**\n\n1. Tokenization\n2. Indexing\n3. Index Representation - Getting sentence of indexed words","fcdbd28b":"## Challenge: Identifying the type of toxicity in the comments for Jigsaw, Google\n\n#### Building a multi-headed model that\u2019s capable of detecting different types of of toxicity like threats, obscenity, insults, and identity-based hate better than Perspective\u2019s current models.\n\n#### The dataset is of comments from Wikipedia\u2019s talk page edits. \n\n#### Improvements to the current model will hopefully help online discussion become more productive and respectful."}}