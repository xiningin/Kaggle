{"cell_type":{"34900228":"code","e3e95b0a":"code","ab4889e1":"code","ebbb495f":"code","ac1119ab":"code","93a8decc":"code","6c53287f":"code","49d7cd45":"code","c184095a":"code","4c04ed3a":"code","e2360514":"code","9975e804":"code","07154f9e":"code","27865b37":"code","81ea1437":"code","e2d856da":"code","db2961bd":"code","d322ac44":"code","058679ab":"code","16751d39":"code","8651bde2":"code","f8f65e06":"code","5f6d5944":"code","864c9203":"code","88db2e68":"code","3c74543d":"code","e55aaeeb":"code","afb9e756":"code","b7843a60":"code","e0a470be":"code","b938d195":"code","9ee449e9":"code","04846fdc":"code","dac3d137":"code","63b0eb82":"code","1a346c45":"code","1f0563f3":"code","3b380ab4":"markdown","3d44cf6d":"markdown","81b27413":"markdown","fb717d6d":"markdown","2261b537":"markdown","f6dfc37d":"markdown","66301d95":"markdown","16664a2f":"markdown","f5729e45":"markdown","aa167dd6":"markdown","a0373737":"markdown","808bd126":"markdown","0b84ff92":"markdown","223f3395":"markdown","b2c2bdfb":"markdown","0517d33e":"markdown","b3f31427":"markdown","6aebe703":"markdown","39bfb12c":"markdown","3b00f4f1":"markdown","e875f107":"markdown","f4db0be7":"markdown","b3562dc5":"markdown","9c5aeef7":"markdown","36f40c1b":"markdown","7da6c019":"markdown","85456626":"markdown","5525eb50":"markdown","c1c806ea":"markdown"},"source":{"34900228":"# Load necessary library\nimport os\nimport pandas as pd\nimport numpy as np\nimport math\nimport datetime\nfrom datetime import timedelta\nimport matplotlib.pyplot as plt\nfrom matplotlib.ticker import StrMethodFormatter\nimport matplotlib.ticker as mtick\nimport seaborn as sns\nimport folium\nimport branca.colormap as cm\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import preprocessing\nfrom sklearn.metrics import confusion_matrix\nimport geopandas as gpd\nimport warnings\nwarnings.filterwarnings('ignore')\nplt.style.use('seaborn')\n%matplotlib inline\n\nplt.rcParams[\"figure.figsize\"] = (15,8)","e3e95b0a":"# Load and preview data \naccident = pd.read_csv(\"..\/input\/us-accidents\/US_Accidents_Dec19.csv\")\naccident.head()","ab4889e1":"# Summary Statistics\naccident.describe()","ebbb495f":"# Check each column for nas\naccident.isnull().sum()","ac1119ab":"# Exclude unnecessary columns\nexclude = [\"TMC\",\"End_Lat\",\"End_Lng\",\"Description\",\"Number\",\"Street\",\"Timezone\",\n           \"Airport_Code\",\"Weather_Timestamp\",\"Civil_Twilight\",\"Nautical_Twilight\",\"Astronomical_Twilight\"]\naccident_clean = accident.drop(exclude,axis=1)\naccident_clean.head()","93a8decc":"# Check nas after excluding unnecessary columns\naccident_clean.isnull().sum()","6c53287f":"# Adding calculation of time difference of start and end time in minutes\naccident_clean.Start_Time = pd.to_datetime(accident_clean.Start_Time)\naccident_clean.End_Time = pd.to_datetime(accident_clean.End_Time)\naccident_clean[\"Time_Diff\"] = (accident_clean.End_Time - accident_clean.Start_Time).astype('timedelta64[m]')\n\naccident_clean[\"Start_Date\"] = accident_clean[\"Start_Time\"].dt.date\naccident_clean[\"End_Date\"] = accident_clean[\"End_Time\"].dt.date\naccident_clean[\"Year\"] = accident_clean[\"Start_Time\"].dt.year\naccident_clean[\"Month\"] = accident_clean[\"Start_Time\"].dt.month\naccident_clean[\"Day\"] = accident_clean[\"Start_Time\"].dt.day\naccident_clean[\"Hour\"] = accident_clean[\"Start_Time\"].dt.hour\n\n# Excluding accidents in 2015 and 2020 where there's not enough data\naccident_clean = accident_clean[(accident_clean[\"Year\"] > 2015) & (accident_clean[\"Year\"] < 2020)]\ngroup = accident_clean.groupby([\"Year\"]).agg(Count = ('ID','count'))\n\n# Verify data\naccident_clean.head()","49d7cd45":"# Examine data\naccident_clean.groupby([\"Year\",\"Severity\"]).size().unstack()","c184095a":"# accident_clean.groupby([\"Start_Date\",\"Severity\"])[\"ID\"].count()\n\n# Group by year and Group by year and severity\ngroup_year = accident_clean.groupby([\"Year\"]).agg(Count = ('ID','count'))\ngroup_year_sev = accident_clean.groupby([\"Year\",\"Severity\"]).size().unstack()\n\n# YoY Total Accident Count\n# fig = plt.figure(figsize=(15,8))\n\n# plt.subplot(1, 2, 1)\nplt.plot(group_year.index, group_year[\"Count\"])\nplt.gca().xaxis.set_major_formatter(StrMethodFormatter('{x:,.0f}'))\nplt.xticks(np.arange(2016, 2020, 1.0))\n\nplt.show","4c04ed3a":"# YoY trend by severity, more in 2, 1 and 4 looks like flat, need to see in a bar plot\n# fig = plt.figure(figsize=(15,8))\ngroup_year_sev2 = accident_clean.groupby([\"Year\",\"Severity\"]).agg(Count = ('ID','count')).reset_index()\nsns.lineplot(x='Year',y='Count',hue=\"Severity\",data=group_year_sev2,palette=\"Set1\")\nplt.gca().xaxis.set_major_formatter(StrMethodFormatter('{x:,.0f}'))\nplt.xticks(np.arange(2016, 2020, 1.0))\nplt.show","e2360514":"# YoY Severity Count\n# group_year_sev = accident_clean.groupby([\"Year\",\"Severity\"]).agg(Count = ('ID','count'))\n# group_year_sev\naccident_clean.groupby([\"Year\",\"Severity\"]).size().unstack().plot(kind='bar',stacked=True)","9975e804":"# Makes more sense to show stacked 100%, a different view\naccident_clean.groupby([\"Year\",\"Severity\"]).size().groupby(level=0).apply(\n    lambda x: 100 * x \/ x.sum()\n).unstack().plot(kind='bar',stacked=True)\n\nplt.gca().yaxis.set_major_formatter(mtick.PercentFormatter())\nplt.legend(loc = 'upper right',title = 'Severity')\nplt.show()","07154f9e":"# Boxplot to show if temperature has impact on the severity of the accident, \n# looks like the more severe accident has lower temperature\nsns.boxplot(x=\"Severity\", y=\"Temperature(F)\", data=accident_clean, palette=\"Set1\")","27865b37":"# Boxplot to show if temperature has impact on the severity of the accident, \n# looks like the more severe accident has lower temperature\nsns.boxplot(x=\"Severity\", y=\"Humidity(%)\", data=accident_clean, palette=\"Set1\")","81ea1437":"# Examine wind chill and accident severity, lower wind chill cause more severe accidents\nsns.boxplot(x=\"Severity\", y=\"Wind_Chill(F)\", data=accident_clean, palette=\"Set1\")","e2d856da":"# Count of Severity by Sunrise_Sunset to see if more severe accidents happened at night\npd.crosstab(accident_clean[\"Severity\"], accident_clean[\"Sunrise_Sunset\"], \n            rownames=['Severity'], colnames=['Sunrise_Sunset'])","db2961bd":"# Severity 1 and two has same % between day and night while 3 and 4 has more accidents % at nights\naccident_clean.groupby([\"Severity\",\"Sunrise_Sunset\"]).size().groupby(level=0).apply(\n    lambda x: 100 * x \/ x.sum()\n).unstack().plot(kind='bar',stacked=True)\n\nplt.gca().yaxis.set_major_formatter(mtick.PercentFormatter())\nplt.legend(loc = 'upper right',title = 'Sunrise\/Sunset')\nplt.show()","d322ac44":"# Most accidents happened on the right side of the road\n# Severity 3 has more on right then the left side of the road\naccident_clean[accident_clean.Side != \" \"].groupby([\"Severity\",\"Side\"]).size().groupby(level=0).apply(\n    lambda x: 100 * x \/ x.sum()\n).unstack().plot(kind='bar',stacked=True)\nplt.legend(loc = 'upper right')\nplt.gca().yaxis.set_major_formatter(mtick.PercentFormatter())\nplt.show()","058679ab":"# Examining Severity by month, most severe accidents (3 and 4) happened in June and July\naccident_clean.groupby([\"Month\",\"Severity\"]).size().groupby(level=0).apply(\n    lambda x: 100 * x \/ x.sum()\n).unstack().plot(kind='bar',stacked=True,figsize = (15,8))\nplt.legend(loc = 'upper right')\nplt.gca().yaxis.set_major_formatter(mtick.PercentFormatter())\nplt.show()","16751d39":"sns.distplot(accident_clean['Distance(mi)'])","8651bde2":"accident_clean.groupby('Severity')['Distance(mi)'].mean()\n# df[(df['year'] > 2012) & (df['reports'] < 30)]\n# sns.boxplot(x=\"Severity\", y=\"Wind_Speed(mph)\", \n#             data=accident_clean[accident_clean[\"Wind_Speed(mph)\"] <= 50], palette=\"Set1\")","f8f65e06":"accident_clean.groupby('Severity')['Time_Diff'].median()\n# sns.boxplot(x=\"Severity\", y=\"Time_Diff\", data=accident_clean, palette=\"Set1\")","5f6d5944":"accident_road = accident[['Severity','Amenity', 'Bump','Crossing','Give_Way',\n                         'Junction','No_Exit','Railway','Roundabout','Station',\n                         'Stop','Traffic_Calming','Traffic_Signal','Turning_Loop']]\naccident_road.head()","864c9203":"accident_road_melt = pd.melt(accident_road,id_vars =['Severity'],value_vars=['Amenity', 'Bump','Crossing','Give_Way',\n                         'Junction','No_Exit','Railway','Roundabout','Station',\n                         'Stop','Traffic_Calming','Traffic_Signal','Turning_Loop'])\ngroup_road = accident_road_melt.groupby([\"Severity\",\"variable\",\"value\"]).agg(Count = ('value','count')).reset_index()\ngroup_road.head()\n# pd.pivot_table(data=accident_road_melt,index='Severity',columns=['value'],aggfunc='count')","88db2e68":"g = sns.catplot(x=\"Severity\", y=\"Count\",\n            hue=\"value\", col=\"variable\",\n            col_wrap=3, data=group_road, kind=\"bar\",\n            height=4, aspect=.7)\ng.fig.set_figwidth(15)\ng.fig.set_figheight(8)","3c74543d":"# Count of True and False of each road condition and group by Severity\n(accident_road.set_index('Severity')\n .groupby(level='Severity')\n# to do the count of columns nj, wd, wpt against the column ptype using \n# groupby + value_counts\n .apply(lambda g: g.apply(pd.value_counts))\n .unstack(level=1)\n .fillna(0))","e55aaeeb":"# More accidents are happening in the second half of the year\ngroup_day = accident_clean.groupby([\"Month\",\"Day\"]).size().unstack()\nax = sns.heatmap(group_day, cmap=\"YlGnBu\",linewidths=0.1)","afb9e756":"# Most accidents happened between 7 and 8, which is the morning rush hour\n# morning rush hour have much more accidents then the afternoon rush hour, which is 4 to 6 in the afternoon\n\ngroup_hour = accident_clean.groupby([\"Day\",\"Hour\"]).size().unstack()\nax = sns.heatmap(group_hour, cmap=\"YlGnBu\",linewidths=0.1)","b7843a60":"from mpl_toolkits.basemap import Basemap\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# A basic map\n# m=Basemap(llcrnrlon=-160, llcrnrlat=-75,urcrnrlon=160,urcrnrlat=80)\nm = Basemap(llcrnrlat=20,urcrnrlat=50,llcrnrlon=-130,urcrnrlon=-60)\n\n# m = Basemap(projection='lcc', resolution='h', lat_0=37.5, lon_0=-119,\n#             width=1E6, height=1.2E6,\n#            llcrnrlat=20,urcrnrlat=50,llcrnrlon=-130,urcrnrlon=-60)\n\nm.shadedrelief()\nm.drawcoastlines(color='gray') \nm.drawcountries(color='gray') \nm.drawstates(color='gray')\n\nlat = accident_clean.Start_Lat.tolist()\nlon = accident_clean.Start_Lng.tolist()\n\nx,y = m(lon,lat)\nm.plot(x,y,'bo',alpha = 0.2)","e0a470be":"# US Shape file from https:\/\/www2.census.gov\/geo\/tiger\/GENZ2018\/shp\/cb_2018_us_state_500k.zip\nshapefile_state = '..\/input\/us-shape-data\/cb_2018_us_state_500k.shp'\n\n\n#Read shapefile using Geopandas\ngdf_state = gpd.read_file(shapefile_state)\ngdf_state.head()","b938d195":"group_state = accident_clean.groupby([\"State\"]).agg(Count = ('ID','count'))\ngroup_state.reset_index(level=0, inplace=True)\ngroup_state[:5]","9ee449e9":"# Merge shape file with accident data\nstate_map = gdf_state.merge(group_state, left_on = 'STUSPS', right_on = 'State')\nstate_map.head()","04846fdc":"# group_state\nm = folium.Map(location=[37, -102], zoom_start=4)\n\nfolium.TileLayer('CartoDB positron',name=\"Light Map\",control=False).add_to(m)\n\n# myscale = (state_map['Count'].quantile((0,0.2,0.4,0.6,0.8,1))).tolist()\n\nm.choropleth(\n    geo_data=state_map,\n    name='Choropleth',\n    data=state_map,\n    columns=['State','Count'],\n    key_on=\"feature.properties.State\",\n    fill_color='YlGnBu',\n#     threshold_scale=myscale,\n    fill_opacity=1,\n    line_opacity=0.2,\n    legend_name='Count of Accidents',\n    smooth_factor=0\n)\n\n\nstyle_function = lambda x: {'fillColor': '#ffffff', \n                            'color':'#000000', \n                            'fillOpacity': 0.1, \n                            'weight': 0.1}\nhighlight_function = lambda x: {'fillColor': '#000000', \n                                'color':'#000000', \n                                'fillOpacity': 0.50, \n                                'weight': 0.1}\n\ntoolkit = folium.features.GeoJson(\n    state_map,\n    style_function=style_function, \n    control=False,\n    highlight_function=highlight_function, \n    tooltip=folium.features.GeoJsonTooltip(\n        fields=['State','Count'],\n        aliases=['State: ','# of Accidents: '],\n        style=(\"background-color: white; color: #333333; font-family: arial; font-size: 12px; padding: 10px;\") \n    )\n)\n\nm.add_child(toolkit)\nm.keep_in_front(toolkit)\nfolium.LayerControl().add_to(m)\n\nm","dac3d137":"variable = [\"Severity\",\"Distance(mi)\",\"Time_Diff\",\"Temperature(F)\",\"Wind_Chill(F)\",\"Humidity(%)\",\n           \"Pressure(in)\",\"Visibility(mi)\",\"Wind_Speed(mph)\",\"Precipitation(in)\"]\naccident_model = accident_clean[variable]\naccident_model = accident_model.dropna()\n# accident_model['Severity'] = np.where(accident_model['Severity']<=2, 0, 1)\naccident_model.head()","63b0eb82":"Y = accident_model.loc[:,'Severity'].values\nX = accident_model.loc[:,'Distance(mi)':'Precipitation(in)'].values\n\nstandardized_X = preprocessing.scale(X)\ntrain_x, test_x, train_y, test_y = train_test_split(standardized_X,Y , test_size=0.3, random_state=0)\n\nmodel = LogisticRegression(solver='lbfgs',multi_class='multinomial',max_iter=1000)\nmodel.fit(train_x, train_y)","1a346c45":"model.score(test_x, test_y)","1f0563f3":"model_y = model.predict(test_x)\n\nmat = confusion_matrix(test_y,model_y)\nsns.heatmap(mat, square=True, annot=True, cbar=False) \nplt.xlabel('predicted value')\nplt.ylabel('true value')","3b380ab4":"Similiarly, the output below summarized the Trues and Falses by each variables grouped by Severity.","3d44cf6d":"As expected, more severe accidents will affect longer distances and last longer time.\n\nThere are another set of variables that describes the condition of the road, wuch as whether there's a bump in the road, or whether there's traffic signal.\n\nThe following analysis will focus on these variables.\n\nFirst select these variables and Severity into a new dataset called `accident road`.","81b27413":"Plot the data to visualize year over year traffic accident trend","fb717d6d":"In order to plot these variables in the same grid, we used `melt()` and then `groupby()` to reshape the dataset.","2261b537":"After wrapping up the data visualization of numeric and categorical variables, we also want to visualiza geo data using a map.\n\nThe static map below are plot using `Basemap` from `mpl_toolkits`. The scatter plot is a good way to start.","f6dfc37d":"The plot below shows that most accidents are happening on the **right** side of the road","66301d95":"The plor below shows by month, the break down of the accidents by severity, more severe accidents (Severity 3 and 4) happened in June and July.","16664a2f":"Using the `heatmap()` function from `seaborn` library, we want to know for each month, which day of the month are more likely to have more accidents.\n\nFrom the heatmap below, we can see that more accidents are happening in the second half of the year and it looked like high accidents days are spead randomly and varied in each month.","f5729e45":"Below is a boxplot of **Wind Chill** and **Severity**, similiarly, we can see that lower Wind Chill might lead to more severe accidents.","aa167dd6":"The dataset also contains some variables regarding the weather condition when the accidents happened. We also want to examine those variables to see if any of the weather related variables have impact on the severity of the accidents.\n\nWe want to use boxplot from `seaborn` library to see how the weather related variable changed in different type of accidents.\n\nBelow is a boxplot of **Temperature** and **Severity**, we can see that there are almost no difference in median temparature in Severity 1,2 and 3, while lower medium temperature in severity 4, which might indicate that lower temperature might result to more severe accidents.","a0373737":"Previewing the data and some summary statistics","808bd126":"The 100% stacked barplot below indicates that more severe accidents are happening during the night then in the day time.","0b84ff92":"# US Accident Anaysis\n\n## Introduction\nThe data from this analysis is from [kaggle US Accidents: A Countrywide Traffic Accident Dataset (2016 - 2019)](https:\/\/www.kaggle.com\/sobhanmoosavi\/us-accidents), which contains accident data are collected from February 2016 to December 2019 in 49 US states, more documentation about this data can be found [here](https:\/\/smoosavi.org\/datasets\/us_accidents).\n\nThe purpose of this analysis is to analyze the data and find out what are the key variables that impact the **severity** of the traffic accidents that happened in US and ultimately predict the severity of the accidents based on given variables through **data visulization** and **regression analysis** using Python.\n\n## Loading data\n\nThe analysis starts with loading data and necessary library.","223f3395":"Checking Columns for nas","b2c2bdfb":"Similiarly, we want to see in what time of the day, more accidents will happen using a heatmap.\n\nAs expected, Most accidents happened between 7 and 8, which is the morning rush hour. Morning rush hour have much more accidents then the afternoon rush hour, which is 4 to 6 in the afternoon. This trend is observed consistently throughout each day of the month.","0517d33e":"Use `score()` to evaluate the model, the model gets 0.72, which is decent for the first try.","b3f31427":"The next step is the plot a Choropleth map using `folium` library.\n\nFirst download the US shape file from [US Census](https:\/\/www.census.gov\/geographies\/mapping-files\/time-series\/geo\/carto-boundary-file.html). The shapefile used in this analysis are [state shape file](https:\/\/www2.census.gov\/geo\/tiger\/GENZ2018\/shp\/cb_2018_us_state_500k.zip) and [zip code shape file](https:\/\/www2.census.gov\/geo\/tiger\/GENZ2018\/shp\/cb_2018_us_zcta510_500k.zip).\n\nWe are using `geopandas` to load the shapefile, then `groupby()` state or zipcode and `merge()` to get the final dataset for map plotting.","6aebe703":"The 100% stacked bar plot shows the % breakdown of each severity by year.","39bfb12c":"Finally use `confusion_matrix` to see how many of the prediction are correct and how many are incorrect.\n\nWe can see that **Severity 2 and 3** are the most accidents. The model did a good job in identifing **Severity 2** but not **Severity 3**.","3b00f4f1":"Following plot shows for each variable break down by accident severity, the count of True and False.\n\nFrom the plot, **Corssing**, **Junction**, **Tracffic Signal** have some impact.","e875f107":"For numeric variables that are severely skewed, boxplot won't be a good interpretation visually, for those variables, we are using `groupby()` to get the mean by **Severity**","f4db0be7":"## Data Visualization\n\nFirst group the data by **year** and **Severity** and use `size()` to count the records in each group, then use `unstack()` to pivot the result.\n\nMost of the accidents falls under **Severity 2 and 3** and the number of accidents are increasing year over year.","b3562dc5":"Below is a boxplot of **Humidity** and **Severity**, similiarly, we can see that higher humidity might lead to more severe accidents.","9c5aeef7":"To prepare the dataset for further analysis, some additional columns are added\n\n- Time_Diff: Time difference between start time and end time of the accident\n- Year: Year of start time\n- Month: Month of start time\n- Day: Day of start time\n- Hour: Hour of start time\n\nDuring this step, also excluded records in 2015 and 2020 where there aren't enough data in these two years. This should give us a relatively clean dataset to start the analysis.","36f40c1b":"Then plot the data by **Severity** and by **Year** to show the trend of accidents by year and by accident severity.\n\nFrom the plot, we can see that:\n\n- **Severity 2** accidents are *increasing* year over year in a rapid speed\n- **Severity 3** accidents have seen a *decrease* in 2019 compared to 2018\n- **Severity 1 and 4** are relatively flat year over year","7da6c019":"Hover over each state will pop a toolkit that show the state name and the count of accidents. Most of the accidents in this dataset are in **California**, followed by **Texas** and **Florida**.\n\n## Regression Analysis\n\nAfter data visualization, we have a brief understanding of the dataset as well as the variables to choose for the regression analysis.\n\nSince the dependent variable is **Severity**, which is a categorical variable. Logestic Regression model is chosen to perform the analysis.\n\nThe independent variables are selected based on the previous analysis:\n\n- Distance(mi)\n- Time_Diff\n- Temperature(F)\n- Wind_Chill(F)\n- Humidity(%)\n- Pressure(in)\n- Visibility(mi)\n- Wind_Speed(mph)\n- Precipitation(in)\n\nUsing `sklearn` library, first separate the dependent variable **y** and independent variable **x** and store them in separate objects. The use `scale()` from `preprocessing` module to properly scale the **x** variable before the regression.\n\nNext step will be using `train_test_split` from `sklearn.model_selection` module to split test and train dataset. Finally fit the logestic regression model.","85456626":"Cleanning the data by removing unnecessary columns that won't be used in the analysis. Then review the cleaned dataset","5525eb50":"The following bar plots are also showing the similiar information then observed previously","c1c806ea":"There are some other categorical variables that might have impact on the severity of the accidents, we are using `crosstab()` function to get a count of the accidents in each group"}}