{"cell_type":{"679fe780":"code","db057ad8":"code","0a7c628b":"code","9fef660e":"code","5028f4ac":"code","c3c1e6c6":"code","6435bac0":"code","f017b54c":"code","4265bea9":"code","d12056f5":"code","d88a594f":"code","ff582961":"code","f855b6e6":"code","376220b2":"code","50e0f016":"code","d164d0e0":"code","c1f49b72":"code","bee4418c":"markdown"},"source":{"679fe780":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nimport csv \nimport tensorflow as tf\nimport tensorflow_addons as tfa\n\nhome = '\/kaggle\/input\/tabular-playground-series-jun-2021'\n\ntrain_dir = os.path.join(home,'train.csv')\ntest_dir = os.path.join(home,'test.csv')\n\n#load test data\ntest_data = pd.read_csv(test_dir, delimiter=',')    \n\n#load train data        \ntrain_data = pd.read_csv(train_dir, delimiter=',')\n\n#concat\nall_df = pd.concat([train_data, test_data]).reset_index(drop=True)\n\n# #onehot encroder\n# train_label = tf.one_hot(train_label['target'], len(np.unique(train_label['target'])))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","db057ad8":"train_data.describe()","0a7c628b":"# !pip install dataprep\n# from dataprep.eda import plot, plot_correlation, create_report, plot_missing","9fef660e":"# #visualize data to inspect skew, mean, std, and any missing nulls\n# plot(train_data.drop(['id'],axis=1))","5028f4ac":"# #visualizing skewness as seen above\n\n# train_data.drop(['id','target'], axis=1).skew().plot(kind='bar', figsize=(18,10))","c3c1e6c6":"# cat_features = [col for col in train_data.columns if 'feature_' in col]","6435bac0":"# #apply boxcox to normalize the skewness\n# train_data = pd.concat([train_data[cat_features].add(1), train_data[['id','target']]],axis=1)\n# test_data = pd.concat([test_data[cat_features].add(1), test_data['id']],axis=1)\n\n\n# from scipy.stats import boxcox\n# for i in train_data.drop(['id','target'], axis=1).columns:\n#     train_data[str(i)], lmbda = boxcox(train_data[str(i)], lmbda = None)\n#     test_data[str(i)], lmbda = boxcox(test_data[str(i)], lmbda = None)","f017b54c":"# #visualizing skewness after boxcox\n\n# train_data.drop(['id','target'], axis=1).skew().plot(kind='bar', figsize=(18,10))","4265bea9":"X_train = train_data.drop(['id', 'target'],axis=1)\nY_train = train_data['target']","d12056f5":"#onehot encroder\nY_train_label = tf.one_hot([int(x.split('_')[1]) for x in Y_train], 9)","d88a594f":"#split train vs dev set\nsplit = int(0.95*len(X_train))\n\nx_train = X_train[:split]\ny_train = Y_train_label[:split]\nx_dev = X_train[split:]\ny_dev = Y_train_label[split:]","ff582961":"#DNN\nepochs = 50\n\nes = tf.keras.callbacks.EarlyStopping(\n    monitor='val_loss', min_delta=1e-05, patience=8, verbose=0,\n    mode='min', baseline=None, restore_best_weights=True)\n\nplateau = tf.keras.callbacks.ReduceLROnPlateau(\n    monitor='val_loss', factor=0.7, patience=2, verbose=0)\n\n\n#regularization term\nregularizer = tf.keras.regularizers.l2(l=0.1)\n\nmodel = tf.keras.models.Sequential([\n    tf.keras.layers.Input(shape=(X_train.shape[1],)),\n    tf.keras.layers.Embedding(360, 18),\n#     tf.keras.layers.GaussianNoise(stddev=0.001, name='gaussian_noise_1'),\n#     tf.keras.layers.Conv1D(64, kernel_size=1, activation='relu'),\n#     tf.keras.layers.Dropout(0.2, name='dropout_1'),\n    tf.keras.layers.Flatten(),\n    tf.keras.layers.Dropout(0.2, name='dropout_1'),\n    tfa.layers.WeightNormalization(tf.keras.layers.Dense(units=32, activation='selu', kernel_initializer=\"lecun_normal\")),\n    tf.keras.layers.Dropout(0.2, name='dropout_2'),\n    tfa.layers.WeightNormalization(tf.keras.layers.Dense(units=32, activation='relu')),\n    tf.keras.layers.Dropout(0.3, name='dropout_3'),\n    tfa.layers.WeightNormalization(tf.keras.layers.Dense(units=32, activation='elu')),\n#     tf.keras.layers.Dense(1024, activation='relu', kernel_regularizer = regularizer),\n#     tf.keras.layers.Dense(512, activation ='relu', kernel_regularizer = regularizer),\n#     tf.keras.layers.Dense(32, activation ='relu', kernel_regularizer = regularizer),\n    tf.keras.layers.Dense(Y_train_label.shape[1], activation='softmax')\n])\n\n\n#hyperparameters for adam \nadam = tf.keras.optimizers.Adam(\n    learning_rate=2e-5, beta_1=0.9, beta_2=0.999, epsilon=1e-07, amsgrad=True\n)\n\n\n\n\nmodel.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n\nhistory = model.fit(x_train, \n                    y_train, \n                    epochs=epochs, \n                    verbose=2, \n                    validation_split = 0.95,\n                    batch_size = 256\n#                     callbacks=[es,plateau]\n                   )","f855b6e6":"from sklearn.metrics import log_loss\npred_row = model.predict(x_dev)\nlog_loss(y_dev, pred_row)","376220b2":"from matplotlib import pyplot as plt\n\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.show()\n\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.show()\n","50e0f016":"test_data.head()","d164d0e0":"y_result= model.predict(test_data.iloc[:,1:].values)\nprint(y_result)","c1f49b72":"#submission \n\nsubmission_result = pd.DataFrame(y_result,columns=['Class_1','Class_2','Class_3','Class_4', 'Class_5', 'Class_6', 'Class_7', 'Class_8', 'Class_9'])\nsubmission_result['id'] = test_data['id']\nsubmission_result.to_csv('submission.csv', index=False)\nsubmission_result.head()\n","bee4418c":"**Preprocess Data**"}}