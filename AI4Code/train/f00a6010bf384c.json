{"cell_type":{"75a5d975":"code","0f695e77":"code","0437c170":"code","31cabfbd":"code","d502e5e7":"code","80b04ca5":"code","e2873ac5":"code","edff6b64":"code","fbfb65f4":"code","0ea4c643":"code","c67a73db":"code","a42c0b81":"code","dd06a107":"code","782bd5db":"code","abbd0d08":"code","7ebb97c3":"code","07cd112f":"code","fbcda958":"code","c78a169e":"code","35d47963":"code","0680d300":"code","af83ce60":"code","ff17c7ca":"code","fe781ef4":"code","cd97ed69":"code","78638b6a":"code","189d0a5d":"code","57d20206":"code","c228db26":"code","266e47e2":"code","ab276d86":"code","9930d600":"code","5953317f":"code","49453277":"code","3fa7a551":"code","419bbb49":"code","ab9a2ee8":"code","0f4c73d4":"code","2cd20905":"code","83655c6d":"code","d549cb16":"code","0b5d3084":"markdown","32d51d96":"markdown","70a0c6a7":"markdown","cf110820":"markdown","e3472a66":"markdown","9d466c50":"markdown","e688cf66":"markdown","43ebb168":"markdown","9775bd37":"markdown","7110a3a7":"markdown","5f7015ab":"markdown","c9bef9f3":"markdown","5afc610a":"markdown","0ff75d7b":"markdown","fbdcf1bd":"markdown","896b0971":"markdown","7259633a":"markdown","e080635b":"markdown","74de9899":"markdown","ced20767":"markdown","fa31e696":"markdown","f8736493":"markdown","51a0d6be":"markdown","9e81b173":"markdown","f16ef784":"markdown","3178bfa4":"markdown","da209f2c":"markdown","846cfc7f":"markdown","4cc6120c":"markdown","fc084b5b":"markdown","96182c36":"markdown","50e1ad81":"markdown","82a3b0a3":"markdown","fa66aabe":"markdown","c099021c":"markdown","55e6a66b":"markdown","9d28d708":"markdown","0fce2804":"markdown","8e38aa08":"markdown","285aa93f":"markdown","6910591c":"markdown","23a247e1":"markdown","2f596dc8":"markdown","7e9b73e3":"markdown","09e1c162":"markdown","c15ab12b":"markdown","e6a485a1":"markdown","50a500d6":"markdown","ff910f97":"markdown","4739bbc6":"markdown","2f072a24":"markdown","a3acc1f2":"markdown","257e4c1a":"markdown","2f14fbe3":"markdown","0e55e9af":"markdown","65a884c3":"markdown","0b98f5c4":"markdown","97750c16":"markdown","2f814b12":"markdown","04ffc6ff":"markdown","e80eba8c":"markdown"},"source":{"75a5d975":"! pip uninstall tensorflow -y","0f695e77":"! pip install tensorflow-gpu==2.6","0437c170":"import tensorflow as tf\ntf.__version__","31cabfbd":"\nimport numpy as np\nimport tensorflow as tf\nimport pandas as pd\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, LSTM, Dropout\nfrom tensorflow.keras.layers import *\nfrom tensorflow.keras.optimizers import Adam, SGD\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras import backend as K\nimport tensorflow_probability as tfp\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\nfrom sklearn.model_selection import train_test_split\nfrom scipy.stats import pearsonr, spearmanr\nimport seaborn as sns\nimport requests\n\n","d502e5e7":"!pip install openpyxl ","80b04ca5":"# Managinig the place dic in Cach memory \n\ncache = dict()\ndef get_article_from_server(url):\n    print(\"Fetching article from server...\")\n    response = requests.get(url)\n    return response.text\ndef get_article(url):\n    print(\"Getting article...\")\n    if url not in cache:\n        cache[url] = get_article_from_server(url)\n    return cache[url]\n","e2873ac5":"x_features = pd.read_excel(\"..\/input\/datas-features\/Features data set.xlsx\") # features \ny_sales = pd.read_excel('..\/input\/datas-features\/sales data-set.xlsx') # labels\nz_stores = pd.read_csv(\"..\/input\/datas-features\/stores data-set.csv\") # stores\n","edff6b64":"print(x_features.head())\nprint(y_sales.head())\nprint(z_stores.head())\nprint('-'*40)\nprint('\\n')","fbfb65f4":"y_sales['Date'] = pd.to_datetime(y_sales['Date'])\n# optimize the sales via store in a unique date with all Deparments to be preper \ny_sales = y_sales.groupby(['Store', 'Date']).sum()\n# we have to repeat its again to fit x_features y_sales \nx_features['Date'] = pd.to_datetime(x_features['Date'])\nx_features = x_features.groupby(['Store', 'Date']).sum()\nprint(x_features.dtypes)\nprint(y_sales.dtypes)\nprint(z_stores.dtypes)\nprint('-'*40)\nprint('\\n')","0ea4c643":"Combined_table = pd.merge(x_features, y_sales['Weekly_Sales'], how='inner', right_index=True, left_index=True)\nCombined_table.isna().sum()\nCombined_table.info\nCombined_table['Weekly_Sales'].describe()\nCombind_graf = Combined_table.copy()\n# Very well... It seems that minimum price is larger than zero. Excellent!","c67a73db":"print(\"Skewness: %f\" % Combined_table['Weekly_Sales'].skew())\nprint(\"Kurtosis: %f\" % Combined_table['Weekly_Sales'].kurt())","a42c0b81":"# let see how Weekly_Sales corellative to other features ?\nsns.displot(data = Combined_table, x = 'Weekly_Sales', kde=True)","dd06a107":"# first convert the store from index into a column\n# Takea a look over each sotre outlayers via \"Weekly_Sales'.\nCombined_table = Combined_table.reset_index(level=0)\nvar = 'Store'\ndata = pd.concat([Combined_table['Weekly_Sales'], Combined_table[var]], axis=1)\nf, ax = plt.subplots(figsize=(8, 6))\nfig = sns.boxplot(x=var, y=\"Weekly_Sales\", data=data)\nfig.axis(ymin = 180000, ymax = 4000000)","782bd5db":"# Let display the relationship between two numerical variables\n# For any combination features. \nsns.set()\ncols = ['Temperature', 'Fuel_Price', 'CPI', 'Unemployment', 'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5', 'Weekly_Sales']\nsns_plot = sns.pairplot(Combind_graf[cols].sample(100), height = 2.5) \nplt.show()","abbd0d08":"# I try to see if it's any corralation between the features\nCombind_graf[['Temperature', 'Fuel_Price', 'CPI', 'Unemployment', 'MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5', 'Weekly_Sales']].plot(subplots=True, figsize=(20,15))\nplt.show()","7ebb97c3":"# In the following Matrix we will see how mach the features are confusing?.\n# As we see not all features are not corelative. \ng = sns.heatmap(Combind_graf[['Temperature', 'Fuel_Price', 'CPI', 'Unemployment', 'MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5', 'Weekly_Sales']].corr(),annot=True, fmt = \".2f\", cmap = \"coolwarm\")\nplt.show()\n#At the end we will see that Pearson accurecy is around 90%","07cd112f":"# Since I have two Prameters with I correlasition, I drop 'MarkDown1', Why ?\n# To prevent  Linkage featurs that are corelatived.\ng = sns.heatmap(Combind_graf[['Temperature', 'Fuel_Price', 'CPI', 'Unemployment', 'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5', 'Weekly_Sales']].corr(),annot=True, fmt = \".2f\", cmap = \"coolwarm\")\nplt.show()\n# Now at the end we will see Pearson accurecy here is around 87.5%\nCombined_table = Combined_table.drop(['MarkDown1'], axis = 1)","fbcda958":"print(\"Dats shape = {}\".format(Combined_table.shape))\nprint()\nprint(\"Lets see some feature:\")\nprint(Combined_table[1:10])","c78a169e":"# Definition of Y (label) and X (inputs)\n# Define label for the new merge table: \"combined_table\"\n# The 'Weekly_Sales' is Indexial so it dosn't take as a label\ny = Combined_table['Weekly_Sales']\n# Define features for the new merge table: \"combined_table\"\nx = Combined_table.drop(['Weekly_Sales'], axis=1)\n\n# Changing Farenhight to Celcius temperature.\nCombined_table['Temperature'] = (Combined_table['Temperature']- 32) * 5.\/9.\n\nprint(Combined_table)\nprint('-'*40)\nprint('\\n')","35d47963":"x.dtypes","0680d300":"scaler = MinMaxScaler()\nx_scaled = scaler.fit_transform(x)\ny.shape\ny = y.values.reshape(6435, 1)\ny_scaled = scaler.fit_transform(y)\nx.head()\nx.tail()","af83ce60":"# we need to take x_scaled after being notmalized\n# In the first step we will split the data in training and remaining dataset\n# random_state = 0 to keep order time as best as I can\nx_train, x_valid, y_train, y_valid = train_test_split(x_scaled, y, train_size=0.80, random_state=42)","ff17c7ca":"# Now since we want the valid and test size to be equal (10% each of overall data). \n# we have to define valid_size=0.5 (that is 50% of remaining data)\ntest_size = 0.5\nx_test, x_valid, y_test, y_valid = train_test_split(x_scaled,y, test_size=0.5)","fe781ef4":"x_train = x_train.reshape(x_train.shape[0],1,x_train.shape[1]) \nx_valid = x_valid.reshape(x_valid.shape[0],1,x_valid.shape[1])\nx_test = x_test.reshape(x_test.shape[0],1,x_test.shape[1]) \n\nTest_Data = (x_test, y_test)\n\nprint('-'*40)\nprint('\\n')\nprint(x_train.shape), print(y_train.shape)\nprint(x_valid.shape), print(y_valid.shape)\nprint(x_test.shape), print(y_test.shape)\nprint('-'*40)\nprint('\\n')","cd97ed69":"# Training basic LSTM model# Initializing the Recurrent Neural Network AS LSTM\ninputs = tf.random.normal([32, 50, 10])\nmodel = Sequential()","78638b6a":"# Adding the first LSTM layer with a sigmoid activation function and some\n# Dropout regularization\n# Units - dimensionality of the output space\nmodel.add(LSTM(units = 32, return_sequences = False, input_shape =(1,x_train.shape[2])))\n# Adding the output layer\nmodel.add(Dense(units = 128))\nmodel.add(Dense(units = 64))\nmodel.add(Dense(units = 1, activation=\"relu\", input_shape=(4,)))\nmodel.summary()","189d0a5d":"opt = tf.keras.optimizers.Adam(learning_rate = 0.1, beta_1 = 0.9, beta_2 = 0.999, epsilon = 1e-08, decay = 0.0)\n# Best learning_rate was found to be 0.1  .\n# Model.compile(loss = 'mean_square_error', for regression \n# Metrics=[soft_acc], optimizer = opt","57d20206":"model.compile(loss = 'mse', optimizer = opt)","c228db26":"# A logger was created for logs the best whieghts of the training to be saved.\nmy_callbacks = [tf.keras.callbacks.ModelCheckpoint(save_best_only = True, filepath = 'model.{epoch:02d}-{val_loss:.2f}.h5'),\n    tf.keras.callbacks.TensorBoard(log_dir = '.\/logs')]\n# Create a TensorBoard logger need to be check (no usefull here since I take\n# other ways like plots)\nlogger = tf.keras.callbacks.TensorBoard(log_dir = 'logs', write_graph = True,\n    histogram_freq = 5)","266e47e2":"history = model.fit(x_train,y_train,epochs = 1000, batch_size = 16, validation_data=(x_valid,y_valid), callbacks=my_callbacks)\np = history.history['loss']\nprint('-'*40)\nprint('\\n')\n# list all data in history\nprint(history.history.keys())\n","ab276d86":"# summarize history for loss\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['x_train', 'x_valid'], loc='upper left')\nplt.legend(['y_train', 'y_valid'], loc='upper left')","9930d600":"preds_val = model.predict(x_valid)\npreds_val = preds_val.squeeze()\nresult_val = y_valid - preds_val","5953317f":"preds_test = model.predict(x_test)\npreds_test = preds_test.squeeze()\nresult_test = y_test - preds_val","49453277":"def correlation_coefficient_var(y_valid, preds_coef_val):\n    pearson_r_val = tfp.stats.correlation(preds_coef_val, y_valid)\n    return(pearson_r_val)\n    print(pearson_r_val)","3fa7a551":"def correlation_coefficient_test(y_test, preds_coef_test):\n    pearson_r_test = tfp.stats.correlation(preds_coef_test, y_test)\n    return(pearson_r_test)\n    print(pearson_r_test)\n    \nprint('-'*40)\nprint('\\n')","419bbb49":"preds_val = preds_val.reshape(-1, 1)\ny_valid = y_valid.astype('float32')\nprint(correlation_coefficient_var(y_valid, preds_val))\n","ab9a2ee8":"preds_test = preds_test.reshape(-1, 1)\ny_test = y_test.astype('float32')\nprint(correlation_coefficient_test(y_test, preds_test))","0f4c73d4":"# Plotting the label prediction of sales.\nCombind_graf = Combined_table.copy()\nCombind_graf[['Weekly_Sales']].plot(subplots=True, figsize=(20,15))\nplt.title('model Weekly sales predict')\nplt.ylabel('Income')\nplt.xlabel('Weekly Time Sales')\nplt.show()","2cd20905":"(print(preds_val))\ncorr, p_val = pearsonr(y_valid.squeeze(), preds_val.squeeze())\nprint('Pearson corr validation')\nprint(corr)\nprint('-'*40)\nprint('\\n')","83655c6d":"(print(preds_test))\ncorr, p_test = pearsonr(y_test.squeeze(), preds_test.squeeze())\nprint('Pearson corr test')\nprint(corr)   \n","d549cb16":"print('Via Pearson on Validation and Test verification after  1000 epochs.')\nprint('WE could declare that the model prediction fits the data training in')\nprint('Approximately around 83% to 92% accuracy even all problems.')\nprint('Via this data we could see that future predict is no big changes')\nprint('Via Weekly Sales, meaning that the predict will be same as every week')\n    \n\n","0b5d3084":"14.2 Checking for the test coefficient.","32d51d96":"Now I check my small data with Pearson a statistic problem.\nForecasting Accuracy = Pearson\nSpearman was checked for correlation, but Spearman is better for this problem.","70a0c6a7":"# 13. Prediction.\n------------------------------------","cf110820":"# 9. Training basic model LSTM model\n------------------------------------------------------------------------------","e3472a66":"# 16. Summery and conclusion.\n--------------------------------------------------------------------","9d466c50":"# 11. Hyperparameters\n--------------------------------------","e688cf66":"# 5. Inquery by Plotting \n------------------------","43ebb168":"# 4. Load and check data\n   --------------------","9775bd37":"# 0.\n# Table of Contents:\n# ------------------------\n     1.  whether small data tabular could be predictable ?\n     2.  Libraries to import.\n     3.  Managinig the place dic in Cach memory.\n     4.  Load and check data.\n     5  Inquery by Plotting.\n     6.  Feature analysis.\n     7.  Feature engineering.\n     8.  Modeling.\n     9.  Training basic LSTM model.\n     10. Addinl LSTM to Dense model deep.\n     11. Hyperparameters.\n     12. Fitting + plotting 'Loss'.\n     13. Prediction.\n     14. Pearson.\n     15. Checking @ comparing all features to be predictable.\n     16. Summery and conclusion.","7110a3a7":"13.1.1 For Validation.","5f7015ab":"# 1.\n# How small data and problems of sequences arrangement\n# could fit Lstm and dense deep model?\n------------------------------------------------","c9bef9f3":"I need to see that all features are numeric as shown above.\nI'm missing the time beacause the data structure rensambeling forced that. ","5afc610a":"13.1 Verification of the prediction.","0ff75d7b":"8.3 splitting the test to test @ val : 50%-50%","fbdcf1bd":"# 6. Feature analysis\n---------------------------------------------------\n\n","896b0971":"4.5.2) Have appreciable positive skewness.","7259633a":"4.1 Load data","e080635b":"8.1 preprocessing normalization values between 0 and 1","74de9899":"14.4 prediction for test.","ced20767":"# To be on the safe side \n# I take correlasition Pearson Metric as another referance\n# Economic vision analization \nI analize only two table : features and sales arreies, to predict sales stores \ngrowing.Union data done with an economic vision and not blind idea.\nThe Data are not so big. In mean while when i Take Pearson correlation at the\nend.The correlation is around 80%-90%\nAs a model of economic predicts, I can't do any augmantation with no meaning - \nSo I lost the order time. Even that is no augmantation Model \nbut Pearson + dense deep models give us an excelent predicts and answers.","fa31e696":"# 10. Addinl LSTM to Dense model deep\n------------------------------------------------------","f8736493":"# 3. Managinig the place dic in Cach memory\n   --------------------------------------","51a0d6be":"==============================================================================\nVia Pearson on Validation and Test verificatuin after  lots of epocs.\nWE could declair that the model prediction fits the Datas training in \nAproximitly around 88% to 92% Pearson accurecy.\nIn both model we couls see same conclusion.\n==============================================================================","9e81b173":"4.3 Groupby working\nThe groupby drop all missing rows","f16ef784":"#  7. Feature engineering\n---------------------------------------------------------","3178bfa4":"12.1 Fitting","da209f2c":"12.3 Mini conclusion","846cfc7f":"5.3) scatterplot - Data Visualization","4cc6120c":"11.2 Compilation","fc084b5b":"6.2) Plotting Confusional Correlation matrix with no features correlative","96182c36":"# 2. Libraries to import.\n   -------------------","50e1ad81":"4.5.1) Have appreciable positive skewness.","82a3b0a3":"6.1) Plotting Confusional Correlation matrix between numerical values","fa66aabe":"5.4) Plotting all features via the label (prediction of sales)","c099021c":"14.3 prediction for validetion.","55e6a66b":"conclusion: Finally The linckage is no so big.\n----------------------\n","9d28d708":"12.2 Plotting 'Loss'","0fce2804":"4.2 Inquering all Data","8e38aa08":"11.1 Optimiztion","285aa93f":"16.1 Print pridict of validetion.","6910591c":"4.5 inquery by: Skewness @ skew","23a247e1":"8.2 splitting the Datas to train @ test : 80%-20%","2f596dc8":"5.1) histogram plot5.1) histogram plot","7e9b73e3":"11.3 Create logger and Tensorboard for analyzing.","09e1c162":"5.2) Inquery of outlier by box plot store \/ Weekly_Sales","c15ab12b":"# 15. Check comparing all features to be predictable.\n-------------------------------------------------------------","e6a485a1":"learning rate need to be learn !!\nAdam was config as Adaptive Learning Rate Methods# learning rate need to be learn !!\nAdam was config as Adaptive Learning Rate Methods","50a500d6":"\"\"\"\nAfter looking the data we have a problem with missing data\nSo to fix it we need apply the dates in both columns to same type date.\nI choose as best as I can the featurs that fit the economic aspects predict.\nFollowing this vision: \n    1) I didn't take data size stores.\n    2) I merge only the full cells data.After looking the data we have a problem with missing data\nSo to fix it we need apply the dates in both columns to same type date.\nI choose as best as I can the featurs that fit the economic aspects predict.\nFollowing this vision: \n    1) I didn't take data size stores.\n    2) I merge only the full cells data.\n\"\"\"","ff910f97":"4.4 Merging all Data\nAs an inner mean - marge just the common Datas.","4739bbc6":"14.1 Checking for the validetion coefficient.","2f072a24":"16.2 Print pridict of test.","a3acc1f2":"# 14. Pearson.","257e4c1a":"6.3) Let see the first 10 variable (for example)","2f14fbe3":"# 12. Fitting + plotting 'Loss'.\n---------------------------------------------------","0e55e9af":"8.5 we need to add an additional dimantion to get numpy arrey for keras shape","65a884c3":"8.4 Finally splitting will be : 80%-10%-10% as train-val-test","0b98f5c4":"===================================================================================\nFinally take a look on the Loss : 'y_train' became fitting with 'y_valid even\n\n* 1) All the faurues arn't correlative each one to others.                           \n* 2) All most all the stores are with outlayers.                                    \n* 3) Even correlation graph are not a simetric Gaussian.                                 \n* 4) Small Data.                                                                    \n* 5) Very big numbers sales that cause big differances and complex calaulation.\n* 6) We lost the order time.\n* 7) No over fitting or under fitting.\n\n\n\n===================================================================================\n++++++++++++++++++++++++++  Conclussiotion  +++++++++++++++++++++++++++++++++++++++\n                Model: Dense Deep Learning work very good  !!!!!!                 \n===================================================================================","97750c16":"13.1.2 for Test","2f814b12":"16.3 Conclusion","04ffc6ff":"# 8. Modeling\n-------------------------------------------------","e80eba8c":"# 1.1\nThe LSTM model is a serial model. Following my small economic data  \nI am trying to produce series because time is important, \nI am looking to preserve the time. Since that sequences arrangement is\nimportant.I note that I have not been able to preserve the time component\nbecause of the data structure and economic definitions aspects that do not \nallow me to complete missing cells just like that, with no meaning. When this\nis the case!! And after all, the problems I check is an economic question - \nSince no time the prediction was done in two ways. Now if each one of them gives me the \nsame answer I know that I overcame the problem of the schedule. \nTo appoint we have 2 problems: \n  1) time schedule and data structure, \n  2) Small data? \nThese are the two main problems are inquering and solving in the economic\ndata structure I have on the hands. And the question is: will we be able to \nget a prediction despite both problems - for those I test it in Pearson as well \nas in deep? And after all I get a model that converges, and accuracy ranges are \nfrom 83% -93%. More of that I check if there is a single variable, a feature \nthat could give me a prediction (I did not find), and further check if all the \nvariables together being in synergy each one to other, they together can give \nme a prediction, and here I see that together, only together I get a moving \nprediction picture that is Between 83% -93%. It is true that when time misorder\nthe value of the LSTM is eroded. I hope it's clearer now. Yet in \nA few approximations that \nwere done to fit problems and for this LSTM approximation economically - \nis right to be done."}}