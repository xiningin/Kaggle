{"cell_type":{"044a8df2":"code","c246b361":"code","f9c47e19":"code","36265d0a":"code","a40932a0":"code","4a250266":"code","ebb86e47":"code","dbf0fb26":"code","e4a8e956":"code","d7beb360":"code","aeca2dc1":"code","a3760874":"code","955389f9":"code","98ab5c78":"code","52565aff":"code","4024ee34":"code","30b85d29":"code","f0aefe19":"code","a91f4308":"code","55fedfad":"code","1b168cb0":"code","570ded4e":"code","8d84ff77":"code","73ec08e8":"code","f81124bd":"code","e0e89c5f":"code","fc9dd07c":"code","59df9921":"code","4c0b1416":"code","4f40292b":"code","05bfa49b":"code","3527783b":"code","e74a81cd":"code","6f018eac":"code","c9568970":"code","af0cd1a7":"code","90707dd0":"code","3f309943":"code","9978e1e1":"code","c818281b":"code","4c0e6555":"code","8c277f9c":"code","a8382056":"code","574b27ca":"code","d80e7f66":"code","a27d36c8":"code","36a87ee4":"code","2c3aa00a":"code","793a71c9":"code","b9af0805":"markdown","7a0cbb42":"markdown","526a3695":"markdown","444af8ff":"markdown","f342ee9b":"markdown","86b1dc83":"markdown"},"source":{"044a8df2":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport tensorflow as tf\nimport re\nimport time\nimport os\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.utils import plot_model\nfrom keras.models import Model\nfrom keras.layers import Input, LSTM, Dense, Embedding, Bidirectional, concatenate\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\nprint(os.listdir(\"..\/input\"))","c246b361":"# Load the data\nlines = open('..\/input\/cornell-moviedialog-corpus\/movie_lines.txt', encoding='utf-8', errors='ignore').read().split('\\n')\nconv_lines = open('..\/input\/cornell-moviedialog-corpus\/movie_conversations.txt', encoding='utf-8', errors='ignore').read().split('\\n')","f9c47e19":"# The sentences that we will be using to train our model.\nlines[:5]","36265d0a":"# The sentences' ids, which will be processed to become our input and target data.\nconv_lines[:5]","a40932a0":"# Create a dictionary to map each line's id with its text\nid2line = {}\nfor line in lines:\n    _line = line.split(' +++$+++ ')\n    if len(_line) == 5:\n        id2line[_line[0]] = _line[4]","4a250266":"# Create a list of all of the conversations' lines' ids.\nconvs = []\nfor line in conv_lines[:-1]:\n    _line = line.split(' +++$+++ ')[-1][1:-1].replace(\"'\",\"\").replace(\" \",\"\")\n    convs.append(_line.split(','))","ebb86e47":"#id and conversation sample\nfor k in convs[300]:\n    print (k, id2line[k])","dbf0fb26":"# Sort the sentences into questions (inputs) and answers (targets)\nquestions = []\nanswers = []\n\nfor conv in convs:\n    for i in range(len(conv)-1):\n        questions.append(id2line[conv[i]])\n        answers.append(id2line[conv[i+1]])","e4a8e956":"# Compare lengths of questions and answers\nprint(len(questions))\nprint(len(answers))","d7beb360":"def clean_text(text):\n    '''Clean text by removing unnecessary characters and altering the format of words.'''\n\n    text = text.lower()\n    \n    text = re.sub(r\"i'm\", \"i am\", text)\n    text = re.sub(r\"he's\", \"he is\", text)\n    text = re.sub(r\"she's\", \"she is\", text)\n    text = re.sub(r\"it's\", \"it is\", text)\n    text = re.sub(r\"that's\", \"that is\", text)\n    text = re.sub(r\"what's\", \"that is\", text)\n    text = re.sub(r\"where's\", \"where is\", text)\n    text = re.sub(r\"how's\", \"how is\", text)\n    text = re.sub(r\"\\'ll\", \" will\", text)\n    text = re.sub(r\"\\'ve\", \" have\", text)\n    text = re.sub(r\"\\'re\", \" are\", text)\n    text = re.sub(r\"\\'d\", \" would\", text)\n    text = re.sub(r\"\\'re\", \" are\", text)\n    text = re.sub(r\"won't\", \"will not\", text)\n    text = re.sub(r\"can't\", \"cannot\", text)\n    text = re.sub(r\"n't\", \" not\", text)\n    text = re.sub(r\"n'\", \"ng\", text)\n    text = re.sub(r\"'bout\", \"about\", text)\n    text = re.sub(r\"'til\", \"until\", text)\n    text = re.sub(r\"[-()\\\"#\/@;:<>{}`+=~|.!?,]\", \"\", text)\n    text = \" \".join(text.split())\n    return text","aeca2dc1":"# Clean the data\nclean_questions = []\nfor question in questions:\n    clean_questions.append(clean_text(question))\n    \nclean_answers = []    \nfor answer in answers:\n    clean_answers.append(clean_text(answer))","a3760874":"r = np.random.randint(1,len(questions))\nprint ('original text......')\nfor i in range(r, r+3):\n    print(questions[i])\n    print(answers[i])\n    print()\nprint ('cleaned text......')\nfor i in range(r, r+3):\n    print(clean_questions[i])\n    print(clean_answers[i])\n    print()","955389f9":"# Find the length of sentences\nlengths = []\nfor question in clean_questions:\n    lengths.append(len(question.split()))\nfor answer in clean_answers:\n    lengths.append(len(answer.split()))\n# Create a dataframe so that the values can be inspected\nlengths = pd.DataFrame(lengths, columns=['counts'])","98ab5c78":"lengths['counts'].describe()","52565aff":"print(np.percentile(lengths, 80))\nprint(np.percentile(lengths, 85))\nprint(np.percentile(lengths, 90))\nprint(np.percentile(lengths, 95))\nprint(np.percentile(lengths, 99))","4024ee34":"# Remove questions and answers that are shorter than 1 word and longer than 20 words.\nmin_line_length = 2\nmax_line_length = 20\n\n# Filter out the questions that are too short\/long\nshort_questions_temp = []\nshort_answers_temp = []\n\nfor i, question in enumerate(clean_questions):\n    if len(question.split()) >= min_line_length and len(question.split()) <= max_line_length:\n        short_questions_temp.append(question)\n        short_answers_temp.append(clean_answers[i])\n\n# Filter out the answers that are too short\/long\nshort_questions = []\nshort_answers = []\n\nfor i, answer in enumerate(short_answers_temp):\n    if len(answer.split()) >= min_line_length and len(answer.split()) <= max_line_length:\n        short_answers.append(answer)\n        short_questions.append(short_questions_temp[i])\n        \nprint(len(short_questions))\nprint(len(short_answers))","30b85d29":"r = np.random.randint(1,len(short_questions))\n\nfor i in range(r, r+3):\n    print(short_questions[i])\n    print(short_answers[i])\n    print()","f0aefe19":"#choosing number of samples\nnum_samples = 60000  # Number of samples to train on.\nshort_questions = short_questions[:num_samples]\nshort_answers = short_answers[:num_samples]","a91f4308":"#append start and end tokens for the answers\nshort_answers2 = []\nfor ans in short_answers:\n    ans = '<SOS> ' + ans + ' <EOS>'\n    short_answers2.append(ans)","55fedfad":"# Create a dictionary for the frequency of the vocabulary\n# Create \nvocab = {}\nfor question in short_questions:\n    for word in question.split():\n        if word not in vocab:\n            vocab[word] = 1\n        else:\n            vocab[word] += 1\n            \nfor answer in short_answers2:\n    for word in answer.split():\n        if word not in vocab:\n            vocab[word] = 1\n        else:\n            vocab[word] += 1","1b168cb0":"# Remove rare words from the vocabulary.\n# We will aim to replace fewer than 5% of words with <UNK>\n# You will see this ratio soon.\nthreshold = 20\ncount = 0\nfor k,v in vocab.items():\n    if v >= threshold:\n        count += 1","570ded4e":"print(\"Size of total vocab:\", len(vocab))\nprint(\"Size of vocab we will use:\", count)","8d84ff77":"#we will create dictionaries to provide a unique integer for each word.\nvocab_to_int = {}\n\nword_num = 0\nfor word, count in vocab.items():\n    if count >= threshold:\n        vocab_to_int[word] = word_num\n        word_num += 1","73ec08e8":"# Add the unique tokens (pad and unknown vocab) to the vocabulary dictionaries.\ncodes = ['<PAD>','<UNK>']\nfor code in codes:\n    code_int = len(vocab_to_int)\n    vocab_to_int[code] = code_int","f81124bd":"#switch <PAD> value to well's value of 0 for padding purposes later\nprint (vocab_to_int['<PAD>'])\nfor i, v in vocab_to_int.items():\n    if v == 0:\n        print (i)","e0e89c5f":"for i, v in vocab_to_int.items():\n    if v == 0:\n        vocab_to_int[i] = vocab_to_int['<PAD>']\nvocab_to_int['<PAD>'] = 0","fc9dd07c":"# Create dictionaries to map the unique integers to their respective words.\n# i.e. an inverse dictionary for vocab_to_int.\nint_to_vocab = {v_i: v for v, v_i in vocab_to_int.items()}","59df9921":"# Check the length of the dictionaries.\nprint(len(vocab_to_int))\nprint(len(int_to_vocab))","4c0b1416":"# Convert the text to integers with paddings\n# Replace any words that are not in the respective vocabulary with <UNK> \nquestions_int = []\nfor question in short_questions:\n    ints = []\n    for word in question.split():\n        if word not in vocab_to_int:\n            ints.append(vocab_to_int['<UNK>'])\n        else:\n            ints.append(vocab_to_int[word])\n    questions_int.append(ints)\n    \nanswers_int = []\nfor answer in short_answers2:\n    ints = []\n    for word in answer.split():\n        if word not in vocab_to_int:\n            ints.append(vocab_to_int['<UNK>'])\n        else:\n            ints.append(vocab_to_int[word])\n    answers_int.append(ints)\n# Check the lengths\nprint(len(questions_int))\nprint(len(answers_int))","4f40292b":"# Calculate what percentage of all words have been replaced with <UNK># Calcul \nword_count = 0\nunk_count = 0\n\nfor question in questions_int:\n    for word in question:\n        if word == vocab_to_int[\"<UNK>\"]:\n            unk_count += 1\n        word_count += 1\n    \nfor answer in answers_int:\n    for word in answer:\n        if word == vocab_to_int[\"<UNK>\"]:\n            unk_count += 1\n        word_count += 1\n    \nunk_ratio = round(unk_count\/word_count,4)*100\n    \nprint(\"Total number of words:\", word_count)\nprint(\"Number of times <UNK> is used:\", unk_count)\nprint(\"Percent of words that are <UNK>: {}%\".format(round(unk_ratio,3)))","05bfa49b":"#include padding\nencoder_input_data = pad_sequences(questions_int, maxlen=max_line_length, value=vocab_to_int['<PAD>'], padding='post') #pad to max_line_length\ndecoder_input_data = pad_sequences(answers_int, maxlen=max_line_length+2, value=vocab_to_int['<PAD>'], padding='post') #pad to max_line_length + start and end tokens","3527783b":"#decoder target is 1 timestep (word) ahead of decoder input, in a 3-d array\ndecoder_target_data = np.zeros(\n    (len(answers_int), max_line_length+2, len(vocab_to_int)), #memory error occurs after 3500\n    dtype='float32')","e74a81cd":"for i, target_seq in enumerate(answers_int):\n    for t, seq in enumerate(target_seq):\n        if t > 0:\n            # decoder_target_data will be ahead by one timestep\n            # and will not include the start character.\n            decoder_target_data[i, t - 1, seq] = 1.","6f018eac":"print (encoder_input_data.shape)\nprint (decoder_input_data.shape)\nprint (decoder_target_data.shape)","c9568970":"#include embedding size\nembedding_size = 200","af0cd1a7":"encoder_inputs = Input(shape=(None,))\nen_x=  Embedding(len(vocab_to_int), embedding_size)(encoder_inputs)\nencoder = Bidirectional(LSTM(100, return_state=True))\nencoder_outputs, state_h_1, state_c_1, state_h_2, state_c_2 = encoder(en_x)","90707dd0":"state_h = concatenate([state_h_1, state_h_2], axis=1)\nstate_c = concatenate([state_c_1, state_c_1], axis=1)\n# We discard `encoder_outputs` and only keep the states.\nencoder_states = [state_h, state_c]","3f309943":"#non-bidirectional approach\n# encoder_inputs = Input(shape=(None,))\n# en_x=  Embedding(len(vocab_to_int), embedding_size)(encoder_inputs)\n# encoder = LSTM(50, return_state=True)\n# encoder_outputs, state_h, state_c = encoder(en_x)\n# # We discard `encoder_outputs` and only keep the states.\n# encoder_states = [state_h, state_c]","9978e1e1":"# Set up the decoder, using `encoder_states` as initial state.\ndecoder_inputs = Input(shape=(None,))\ndex=  Embedding(len(vocab_to_int), embedding_size)\nfinal_dex= dex(decoder_inputs)\ndecoder_lstm = LSTM(200, return_sequences=True, return_state=True)\ndecoder_outputs, _, _ = decoder_lstm(final_dex,\n                                     initial_state=encoder_states)\ndecoder_dense = Dense(len(vocab_to_int), activation='softmax')\ndecoder_outputs = decoder_dense(decoder_outputs)\nmodel = Model([encoder_inputs, decoder_inputs], decoder_outputs)\nmodel.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['acc']) #sparse_categorical_crossentropy as labels in a single integer array","c818281b":"model.summary()","4c0e6555":"#early stopping & saving\n# earlyStopping = EarlyStopping(monitor='val_loss', patience=10, verbose=0, mode='min')\nmcp_save = ModelCheckpoint('Best_weights_movie_word.hdf5', save_best_only=True, monitor='val_loss', mode='min')\n# reduce_lr_loss = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=7, verbose=1, min_delta=1e-4, mode='min')","8c277f9c":"# from keras.models import load_model\n# model = load_model('..\/input\/movie30\/s2s_movie_word_30.h5')","a8382056":"model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n          batch_size=128,\n          epochs=40,\n          validation_split=0.05,\n          callbacks= [mcp_save]\n#           callbacks=[earlyStopping, mcp_save, reduce_lr_loss] #change from 0.1)))\n         )","574b27ca":"model.save('s2s_movie_word_40.h5')","d80e7f66":"encoder_model = Model(encoder_inputs, encoder_states)\nencoder_model.summary()","a27d36c8":"#Create sampling model\ndecoder_state_input_h  = Input(shape=(200,))\ndecoder_state_input_c = Input(shape=(200,))\ndecoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n\nfinal_dex2= dex(decoder_inputs)\n\ndecoder_outputs2, state_h2, state_c2 = decoder_lstm(final_dex2, initial_state=decoder_states_inputs)\ndecoder_states2 = [state_h2, state_c2]\ndecoder_outputs2 = decoder_dense(decoder_outputs2)\ndecoder_model = Model(\n    [decoder_inputs] + decoder_states_inputs,\n    [decoder_outputs2] + decoder_states2)","36a87ee4":"# For non-bidirectional \n# #Create sampling model\n# decoder_state_input_h  = Input(shape=(50,))\n# decoder_state_input_c = Input(shape=(50,))\n# decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n\n# final_dex2= dex(decoder_inputs)\n\n# decoder_outputs2, state_h2, state_c2 = decoder_lstm(final_dex2, initial_state=decoder_states_inputs)\n# decoder_states2 = [state_h2, state_c2]\n# decoder_outputs2 = decoder_dense(decoder_outputs2)\n# decoder_model = Model(\n#     [decoder_inputs] + decoder_states_inputs,\n#     [decoder_outputs2] + decoder_states2)","2c3aa00a":"def decode_sequence(input_seq):\n    # Encode the input as state vectors.\n    states_value = encoder_model.predict(input_seq)\n    # Generate empty target sequence of length 1.\n    target_seq = np.zeros((1,1))\n    # Populate the first character of target sequence with the start character.\n    target_seq[0, 0] = vocab_to_int['<SOS>']\n\n    # Sampling loop for a batch of sequences\n    # (to simplify, here we assume a batch of size 1).\n    stop_condition = False\n    decoded_sentence = ''\n    while not stop_condition:\n        output_tokens, h, c = decoder_model.predict(\n            [target_seq] + states_value)\n\n        # Sample a token\n        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n        sampled_char = int_to_vocab[sampled_token_index]\n        decoded_sentence += ' '+sampled_char\n\n        # Exit condition: either hit max length\n        # or find stop character.\n        if (sampled_char == '<EOS>' or\n           len(decoded_sentence) > 52):\n            stop_condition = True\n\n        # Update the target sequence (of length 1).\n        target_seq = np.zeros((1,1))\n        target_seq[0, 0] = sampled_token_index\n\n        # Update states\n        states_value = [h, c]\n\n    return decoded_sentence","793a71c9":"for i in range(50):\n    seq_index = np.random.randint(1, len(encoder_input_data))\n    # Take one sequence (part of the training set)\n    # for trying out decoding.\n    input_seq = encoder_input_data[seq_index: seq_index + 1]\n    decoded_sentence = decode_sequence(input_seq)\n    print('-')\n    print('Input sentence:', short_answers[seq_index: seq_index + 1])\n    print('Decoded sentence:', decoded_sentence)","b9af0805":"** Creating qns inputs and answer targets**","7a0cbb42":"**Loading the data**\n\n**Matching conv lines to ID**","526a3695":"Cleaning text","444af8ff":"**Selecting qns and answers with appropriate length (<20 words)**","f342ee9b":"Resources:\n\nhttps:\/\/github.com\/Currie32\/Chatbot-from-Movie-Dialogue\n\nhttps:\/\/blog.keras.io\/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html\n\nhttps:\/\/medium.com\/@dev.elect.iitd\/neural-machine-translation-using-word-level-seq2seq-model-47538cba8cd7\nhttps:\/\/github.com\/devm2024\/nmt_keras\/blob\/master\/base.ipynb","86b1dc83":" **Preprocessing for word based model**"}}