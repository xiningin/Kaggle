{"cell_type":{"a763db36":"code","3546b06d":"code","40855321":"code","0ebf7029":"code","385ae78e":"code","6d51a8dd":"code","b34314dd":"code","0d3a0401":"code","326a196b":"code","c70513c7":"code","b1b171e0":"code","cbaa864c":"code","2e061b87":"code","d07b7d1b":"code","8ce3c699":"code","60eec670":"code","eb3ab3e0":"code","ee67ed73":"code","278d9de3":"code","c992cd20":"code","01ac40ac":"code","84b449e2":"code","385df4ed":"code","4460e4e7":"code","d3f9d53b":"code","a02700f5":"code","9b1aacbd":"code","3089f186":"code","1760e005":"code","0eddd93e":"code","94133cc7":"code","5ef924ae":"code","f4e82acf":"code","513d0fa9":"code","89fa3fc9":"code","82a3aabd":"code","865268f1":"code","80dfc3f9":"code","7873f83f":"code","4e5d478b":"code","93eb9cb6":"code","09bcb19f":"code","b6b29cc1":"code","1a5e8a98":"code","0ee27a7a":"markdown","025aea3d":"markdown","6ecb98cf":"markdown","ebb3c97b":"markdown","e23b5e8d":"markdown","12933700":"markdown","44ff7f0d":"markdown"},"source":{"a763db36":"import warnings\n\nwarnings.filterwarnings(\"ignore\")","3546b06d":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt","40855321":"train = pd.read_csv('..\/input\/..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest = pd.read_csv('..\/input\/..\/input\/house-prices-advanced-regression-techniques\/test.csv')","0ebf7029":"train.head()","385ae78e":"test.head()","6d51a8dd":"quantitative = [f for f in train.columns if train.dtypes[f] != 'object']\nquantitative.remove('SalePrice')\nquantitative.remove('Id')\nqualitative = [f for f in train.columns if train.dtypes[f] == 'object']","b34314dd":"quantitative","0d3a0401":"qualitative","326a196b":"sns.set_style(\"whitegrid\")\nmissing = train.isnull().sum()\nmissing = missing[missing>0]\nmissing.sort_values(inplace=True)\nmissing.plot.bar();","c70513c7":"train.head()","b1b171e0":"from scipy import stats\ny = train['SalePrice']\nplt.figure(1);plt.title('Johnson SU')\nsns.distplot(y,kde=False,fit=stats.johnsonsu)\n","cbaa864c":"plt.figure(2); plt.title('Normal')\nsns.distplot(y, kde=False, fit=stats.norm)\nplt.figure(3); plt.title('Log Normal')\nsns.distplot(y, kde=False, fit=stats.lognorm);","2e061b87":"test_normality = lambda x:stats.shapiro(x.fillna(0))[1]<0.01\nnormal = pd.DataFrame(train[quantitative])\nnormal = normal.apply(test_normality)\nprint(not normal.any())","d07b7d1b":"def encode(frame, feature):\n    ordering = pd.DataFrame()\n    ordering['val'] = frame[feature].unique()\n    ordering.index = ordering.val\n    ordering['spmean'] = frame[[feature, 'SalePrice']].groupby(feature).mean()['SalePrice']\n    ordering = ordering.sort_values('spmean')\n    ordering['ordering'] = range(1, ordering.shape[0]+1)\n    ordering = ordering['ordering'].to_dict()\n    \n    for cat, o in ordering.items():\n        frame.loc[frame[feature] == cat, feature+'_E'] = o\n    \nqual_encoded = []\nfor q in qualitative:  \n    encode(train, q)\n    qual_encoded.append(q+'_E')\nprint(qual_encoded)","8ce3c699":"plt.figure(1)\ncorr = train[quantitative+['SalePrice']].corr()\nsns.heatmap(corr)\nplt.figure(2)\ncorr = train[qual_encoded+['SalePrice']].corr()\nsns.heatmap(corr)\nplt.figure(3)\ncorr = pd.DataFrame(np.zeros([len(quantitative)+1, len(qual_encoded)+1]), index=quantitative+['SalePrice'], columns=qual_encoded+['SalePrice'])\nfor q1 in quantitative+['SalePrice']:\n    for q2 in qual_encoded+['SalePrice']:\n        corr.loc[q1, q2] = train[q1].corr(train[q2])\nsns.heatmap(corr);","60eec670":"from sklearn.manifold import TSNE\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA \nfrom sklearn.cluster import KMeans\n\nfeatures = quantitative + qual_encoded\nmodel = TSNE(n_components=2, random_state=0, perplexity=50)\nX = train[features].fillna(0.).values\ntsne = model.fit_transform(X)\n\nstd = StandardScaler()\ns = std.fit_transform(X)\npca = PCA(n_components=30)\npca.fit(s)\npc = pca.transform(s)\nkmeans = KMeans(n_clusters=5)\nkmeans.fit(pc)\n\nfr = pd.DataFrame({'tsne1': tsne[:,0], 'tsne2': tsne[:, 1], 'cluster': kmeans.labels_})\nsns.lmplot(data=fr, x='tsne1', y='tsne2', hue='cluster', fit_reg=False)\nprint(np.sum(pca.explained_variance_ratio_))","eb3ab3e0":"#train.drop(['Id'],axis=1,inplace=True)\ntest.drop(['Id'],axis=1,inplace=True)","ee67ed73":"train = train[train.GrLivArea < 4500]\ntrain.reset_index(drop=True, inplace=True)\ntrain[\"SalePrice\"] = np.log1p(train[\"SalePrice\"])\ny = train['SalePrice'].reset_index(drop=True)","278d9de3":"train_features = train.drop(['SalePrice'],axis=1)\ntest_features = test\nfeatures = pd.concat([train_features,test_features]).reset_index(drop=True)","c992cd20":"features.shape","01ac40ac":"features.head()","84b449e2":"objects = []\nfor i in features.columns:\n    if features[i].dtype == object:\n        objects.append(i)\nfeatures.update(features[objects].fillna('None'))\n\nfeatures['LotFrontage'] = features.groupby('Neighborhood')['LotFrontage'].transform(lambda x: x.fillna(x.median()))\n\nnumeric_dtypes = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\nnumerics = []\nfor i in features.columns:\n    if features[i].dtype in numeric_dtypes:\n        numerics.append(i)\nfeatures.update(features[numerics].fillna(0))","385df4ed":"from scipy.stats import kurtosis\nfrom scipy.stats import skew\nfrom scipy.special import boxcox1p\nfrom scipy.stats import skew,boxcox_normmax\nfrom scipy.special import boxcox, inv_boxcox\nfrom  scipy.stats import yeojohnson_normmax\nfrom scipy.stats import boxcox_llf\nfrom sklearn.preprocessing import power_transform\nfrom sklearn.preprocessing import PowerTransformer\n\n\nnumeric_dtypes = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\nnumerics2 = []\nfor i in features.columns:\n    if features[i].dtype in numeric_dtypes:\n        numerics2.append(i)\nskew_features = features[numerics2].apply(lambda x: skew(x)).sort_values(ascending=False)\n\nhigh_skew = skew_features[skew_features > 0.5]\nskew_index = high_skew.index\n\nfor i in skew_index:\n    features[i] = boxcox1p(features[i], boxcox_normmax(features[i] + 1))","4460e4e7":"features = features.drop(['Utilities', 'Street', 'PoolQC',], axis=1)\n\nfeatures['YrBltAndRemod']=features['YearBuilt']+features['YearRemodAdd']\nfeatures['TotalSF']=features['TotalBsmtSF'] + features['1stFlrSF'] + features['2ndFlrSF']\n\nfeatures['Total_sqr_footage'] = (features['BsmtFinSF1'] + features['BsmtFinSF2'] +\n                                 features['1stFlrSF'] + features['2ndFlrSF'])\n\nfeatures['Total_Bathrooms'] = (features['FullBath'] + (0.5 * features['HalfBath']) +\n                               features['BsmtFullBath'] + (0.5 * features['BsmtHalfBath']))\n\nfeatures['Total_porch_sf'] = (features['OpenPorchSF'] + features['3SsnPorch'] +\n                              features['EnclosedPorch'] + features['ScreenPorch'] +\n                              features['WoodDeckSF'])","d3f9d53b":"features['haspool'] = features['PoolArea'].apply(lambda x: 1 if x > 0 else 0)\nfeatures['has2ndfloor'] = features['2ndFlrSF'].apply(lambda x: 1 if x > 0 else 0)\nfeatures['hasgarage'] = features['GarageArea'].apply(lambda x: 1 if x > 0 else 0)\nfeatures['hasbsmt'] = features['TotalBsmtSF'].apply(lambda x: 1 if x > 0 else 0)\nfeatures['hasfireplace'] = features['Fireplaces'].apply(lambda x: 1 if x > 0 else 0)","a02700f5":"features.shape","9b1aacbd":"final_features = pd.get_dummies(features).reset_index(drop=True)\nfinal_features.shape","3089f186":"X = final_features.iloc[:len(y), :]\nX_sub = final_features.iloc[len(y):, :]\nX.shape, y.shape, X_sub.shape","1760e005":"overfit = []\nfor i in X.columns:\n    counts = X[i].value_counts()\n    zeros = counts.iloc[0]\n    if zeros \/ len(X) * 100 > 99.94:\n        overfit.append(i)\n\noverfit = list(overfit)\nX = X.drop(overfit, axis=1)\nX_sub = X_sub.drop(overfit, axis=1)","0eddd93e":"X.shape, y.shape, X_sub.shape","94133cc7":"from sklearn.model_selection import KFold\n\nkfolds = KFold(n_splits=10, shuffle=True, random_state=42)\n\ndef rmsle(y, y_pred):\n    return np.sqrt(mean_squared_error(y, y_pred))\n\ndef cv_rmse(model, X=X):\n    rmse = np.sqrt(-cross_val_score(model, X, y, scoring=\"neg_mean_squared_error\", cv=kfolds))\n    return (rmse)","5ef924ae":"alphas_alt = [14.5, 14.6, 14.7, 14.8, 14.9, 15, 15.1, 15.2, 15.3, 15.4, 15.5]\nalphas2 = [5e-05, 0.0001, 0.0002, 0.0003, 0.0004, 0.0005, 0.0006, 0.0007, 0.0008]\ne_alphas = [0.0001, 0.0002, 0.0003, 0.0004, 0.0005, 0.0006, 0.0007]\ne_l1ratio = [0.8, 0.85, 0.9, 0.95, 0.99, 1]","f4e82acf":"from sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn import linear_model\nfrom sklearn.linear_model import RidgeCV,LassoCV,ElasticNetCV\nfrom sklearn.svm import LinearSVR\nfrom sklearn.svm import SVR\n\n\nridge = make_pipeline(RobustScaler(), RidgeCV(alphas=alphas_alt, cv=kfolds))\nlasso = make_pipeline(RobustScaler(), LassoCV(max_iter=1e7, alphas=alphas2, random_state=42, cv=kfolds))\nelasticnet = make_pipeline(RobustScaler(), ElasticNetCV(max_iter=1e7, alphas=e_alphas, cv=kfolds, l1_ratio=e_l1ratio))                                \nsvr = make_pipeline(RobustScaler(), SVR(C= 20, epsilon= 0.008, gamma=0.0003,))","513d0fa9":"from lightgbm import LGBMRegressor\n\nlightgbm = LGBMRegressor(objective='regression', \n                                       num_leaves=4,\n                                       learning_rate=0.01, \n                                       n_estimators=5000,\n                                       max_bin=200, \n                                       bagging_fraction=0.75,\n                                       bagging_freq=5, \n                                       bagging_seed=7,\n                                       feature_fraction=0.2,\n                                       feature_fraction_seed=7,\n                                       verbose=-1,\n                                       )\n","89fa3fc9":"from xgboost.sklearn import XGBRegressor\n\n\n\nxgboost = XGBRegressor(learning_rate=0.01,n_estimators=3460,\n                                     max_depth=3, min_child_weight=0,\n                                     gamma=0, subsample=0.7,\n                                     colsample_bytree=0.7,\n                                     objective='reg:linear', nthread=-1,\n                                     scale_pos_weight=1, seed=27,\n                                     reg_alpha=0.00006)","82a3aabd":"from mlxtend.regressor import StackingCVRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor as gbr\nstack_gen = StackingCVRegressor(regressors=(ridge, lasso, elasticnet, gbr, xgboost, lightgbm),\n                                meta_regressor=xgboost,\n                                use_features_in_secondary=True)","865268f1":"from sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import train_test_split\n\n\nscore = cv_rmse(ridge)\nscore = cv_rmse(lasso)\nprint(\"LASSO: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()), datetime.now(), )\n\nscore = cv_rmse(elasticnet)\nprint(\"elastic net: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()), datetime.now(), )\n\nscore = cv_rmse(svr)\nprint(\"SVR: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()), datetime.now(), )\n\nscore = cv_rmse(lightgbm)\nprint(\"lightgbm: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()), datetime.now(), )\n\nscore = cv_rmse(xgboost)\nprint(\"xgboost: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()), datetime.now(), )","80dfc3f9":"print('START Fit')\n\n\n\nprint('elasticnet')\nelastic_model_full_data = elasticnet.fit(X, y)\n\nprint('Lasso')\nlasso_model_full_data = lasso.fit(X, y)\n\nprint('Ridge')\nridge_model_full_data = ridge.fit(X, y)\n\nprint('Svr')\nsvr_model_full_data = svr.fit(X, y)\n\nprint('xgboost')\nxgb_model_full_data = xgboost.fit(X, y)\n\nprint('lightgbm')\nlgb_model_full_data = lightgbm.fit(X, y)","7873f83f":"def blend_models_predict(X):\n    return ((0.1 * elastic_model_full_data.predict(X)) + \\\n            (0.05 * lasso_model_full_data.predict(X)) + \\\n            (0.1 * ridge_model_full_data.predict(X)) + \\\n            (0.1 * svr_model_full_data.predict(X)) + \\\n            (0.15 * xgb_model_full_data.predict(X)) + \\\n            (0.1 * lgb_model_full_data.predict(X)))","4e5d478b":"from sklearn.metrics import mean_squared_error\n\nprint('RMSLE score on train data:')\nprint(rmsle(y, blend_models_predict(X)))","93eb9cb6":"print('Predict submission')\nsubmission = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv\")\nsubmission.iloc[:,1] = np.floor(np.expm1(blend_models_predict(X_sub)))","09bcb19f":"submission.head()","b6b29cc1":"q1 = submission['SalePrice'].quantile(0.005)\nq2 = submission['SalePrice'].quantile(0.995)\nsubmission['SalePrice'] = submission['SalePrice'].apply(lambda x: x if x > q1 else x*0.77)\nsubmission['SalePrice'] = submission['SalePrice'].apply(lambda x: x if x < q2 else x*1.1)\nsubmission.to_csv(\"submission.csv\", index=False)","1a5e8a98":"submission.head()","0ee27a7a":"#### Data Processing","025aea3d":"# EDA\nThere are 1460 instances of training data and 1460 of test data. Total number of attributes equals 81, of which 36 is quantitative, 43 categorical + Id and SalePrice.\n\n#### Quantitative:\n1stFlrSF, 2ndFlrSF, 3SsnPorch, BedroomAbvGr, BsmtFinSF1, BsmtFinSF2, BsmtFullBath, BsmtHalfBath, BsmtUnfSF, EnclosedPorch, Fireplaces, FullBath, GarageArea, GarageCars, GarageYrBlt, GrLivArea, HalfBath, KitchenAbvGr, LotArea, LotFrontage, LowQualFinSF, MSSubClass, MasVnrArea, MiscVal, MoSold, OpenPorchSF, OverallCond, OverallQual, PoolArea, ScreenPorch, TotRmsAbvGrd, TotalBsmtSF, WoodDeckSF, YearBuilt, YearRemodAdd, YrSold\n\n#### Qualitative: \nAlley, BldgType, BsmtCond, BsmtExposure, BsmtFinType1, BsmtFinType2, BsmtQual, CentralAir, Condition1, Condition2, Electrical, ExterCond, ExterQual, Exterior1st, Exterior2nd, Fence, FireplaceQu, Foundation, Functional, GarageCond, GarageFinish, GarageQual, GarageType, Heating, HeatingQC, HouseStyle, KitchenQual, LandContour, LandSlope, LotConfig, LotShape, MSZoning, MasVnrType, MiscFeature, Neighborhood, PavedDrive, PoolQC, RoofMatl, RoofStyle, SaleCondition, SaleType, Street, Utilities,","6ecb98cf":"#### Features","ebb3c97b":"Also none of quantitative variables has normal distribution so these should be transformed as well.\n\n# Spearman correlation\nis better to work with in this case because it picks up relationships between variables even when they are nonlinear. OverallQual is main criterion in establishing house price. Neighborhood has big influence, partially it has some intrisinc value in itself, but also houses in certain regions tend to share same characteristics (confunding) what causes similar valuations.","e23b5e8d":"It is apparent that SalePrice doesn't follow normal distribution, so before performing regression it has to be transformed. While log transformation does pretty good job, best fit is unbounded Johnson distribution.","12933700":"#### Blending Models","44ff7f0d":"# Models"}}