{"cell_type":{"0665aaf3":"code","0a71c825":"code","0f8d618b":"code","a6710e0c":"code","7ff94cbf":"code","8dd0510e":"code","8574dffe":"code","64b08a43":"code","f9fc85db":"code","4ac55078":"code","8558ad3c":"code","5327eea3":"code","1eaa92d5":"code","3fc67d29":"code","956d2947":"code","e5418d9d":"code","1fd87e00":"code","d544a8a8":"code","3d88b6b0":"code","a46fe871":"code","d146bfe7":"code","d62549aa":"code","8a79f0d9":"code","09b58c0d":"code","3f0e9dd6":"code","5568055c":"code","46d4d1ff":"code","ffb67815":"code","6f0f1f56":"code","82b543ce":"code","a743f32f":"code","a32bc725":"code","b6a09bd5":"code","8d36238e":"code","24859bc2":"code","d322e1d5":"code","f95d5574":"code","5c109895":"code","78302468":"code","bb37a23e":"code","72e341ee":"code","cd304c7f":"code","3e3b7803":"code","92aabf97":"code","a91da3e8":"code","681ba5ec":"code","343a313d":"code","740bfa36":"code","7b4efdb4":"code","20043e35":"code","13263fbb":"code","3d2cad11":"code","818d100b":"code","e30a4969":"code","299748ad":"code","ff0e8f29":"markdown","23c6a18e":"markdown","061afef6":"markdown","5a9b2369":"markdown","43e88c47":"markdown","93edf25d":"markdown","f50c83cc":"markdown","9da9c356":"markdown","bceb63e9":"markdown","561c6de4":"markdown","1ed42298":"markdown","787fa40d":"markdown","e7b7f97c":"markdown","5c321d0e":"markdown","67feecc6":"markdown","ff9d760f":"markdown","bec546dd":"markdown","562d4ec3":"markdown","afea584f":"markdown","925ce7c2":"markdown","a2fb64ae":"markdown"},"source":{"0665aaf3":"### Importing Libraries\nimport pandas as pd\nimport numpy as np\n\n### Libraries for machine learning\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import confusion_matrix, recall_score\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import precision_score, f1_score\nfrom sklearn.metrics import precision_recall_fscore_support, classification_report\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.linear_model import LogisticRegressionCV\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.svm import SVC\nfrom xgboost import XGBClassifier\nfrom sklearn import metrics\n\n\n### Libraries for rendering\nimport matplotlib.pyplot as plt\nimport seaborn as sns","0a71c825":"### Loading and visualizing the training dataset\ntitanic_df = pd.read_csv(\"..\/input\/titanic\/train.csv\")\ntitanic_df.head()","0f8d618b":"### Column data type\ntitanic_df.dtypes","a6710e0c":"### Dataset size\ntitanic_df.shape","7ff94cbf":"### Column names\ntitanic_df.columns.values","8dd0510e":"### Number of missing values in each of the columns\n\ntitanic_df.isna().sum()","8574dffe":"### Replace all missing values in the Cabin and Embarked columns with \"U\" (Unknown)\ntitanic_df['Cabin'] = titanic_df['Cabin'].fillna(\"U\")\ntitanic_df['Embarked'] = titanic_df['Embarked'].fillna(\"U\")","64b08a43":"### Find the median age for each gender\ntitanic_df.groupby('Sex')['Age'].median()","f9fc85db":"### Divide the dataframe into two according to the Sex column\nmask = titanic_df['Sex'] == 'female'\ndf_female, df_male = titanic_df[mask], titanic_df[~mask]\n\n### Replace the missing values in ages with the medians for each gender and re-merge the dataframes\ndf_female.fillna(27, inplace=True)\ndf_male.fillna(29, inplace=True)\ndf_titanic = df_female.merge(df_male, how = 'outer')\n\n### Let's set the PassengerId column as an index and sort by it\ndf_titanic = df_titanic.set_index('PassengerId')\ndf_titanic = df_titanic.sort_index()\n","4ac55078":"### \u0421heck, if there are missing values\ndf_titanic.isna().sum()","8558ad3c":"### Let's perform numerical transformations for values\ndf_titanic = df_titanic.replace('male', 0)\ndf_titanic = df_titanic.replace('female', 1)\ndf_titanic = df_titanic.replace('U', 0)\ndf_titanic = df_titanic.replace('S', 1)\ndf_titanic = df_titanic.replace('C', 2)\ndf_titanic = df_titanic.replace('Q', 3)\n\n### Removing Columns\ndf_titanic.drop([\"Name\", \"Ticket\", \"Cabin\"], axis = 1, inplace = True)\n\n### Checking the correctness of the transformations\ndf_titanic.head(20)","5327eea3":"### Displaying general information\ndf_titanic.describe()","1eaa92d5":"### Percentage of survivors for each class\nprint(df_titanic[['Pclass', 'Survived']].groupby(['Pclass']\n                                         ).mean().sort_values(by='Survived'))\ng = sns.catplot(x = 'Pclass', y= 'Survived', data = df_titanic, kind=\"bar\")\ng = g.set_ylabels(\"Survival Probability\")","3fc67d29":"### Percentage of survivors by gender\nprint(df_titanic[[\"Sex\", \"Survived\"]].groupby(['Sex']\n                                     ).mean().sort_values(by='Survived'))\ng = sns.catplot(x = \"Sex\", y= 'Survived', data = df_titanic, kind=\"bar\")\ng = g.set_ylabels(\"Survival Probability\")","956d2947":"### Data for plotting\ndf_age_median = df_titanic[[\"Age\", \"Survived\"]].groupby(['Age'], as_index=False).median()\ndf_age_mean = df_titanic[[\"Age\", \"Survived\"]].groupby(['Age'], as_index=False).mean()\ndf_age_sex = df_titanic[[\"Sex\", \"Age\"]].groupby(['Age'], as_index=False).count().values","e5418d9d":"### Number of passengers by age\nplt.figure(figsize=(20, 10))\nplt.bar(df_age_sex[:,0], df_age_sex[:,1], alpha=0.5)\nplt.xticks(np.arange(0, 81, 1.0))\nplt.show()","1fd87e00":"### Visualizing Median and Mean Survivors by Age\n\nplt.figure(figsize=(20, 10))\nplt.plot(df_age_mean.Age[:40], df_age_mean.Survived[:40], color = \"black\", marker = \"o\", label = \"mean survived\")\nplt.bar(df_age_median.Age[:40], df_age_median.Survived[:40], alpha=0.5, label = \"median survived\")\nplt.xticks(np.arange(0, 31, 1))\nplt.hlines(0.75, 0, 30, color = \"red\", label = \"75 quintile\")\nplt.hlines(0.25, 0, 30, color = \"green\", label = \"25 quintile\")\nplt.legend()\nplt.show()\n\nplt.figure(figsize=(20, 10))\nplt.plot(df_age_mean.Age[40:90], df_age_mean.Survived[40:90], color = \"black\", marker = \"o\"\n         , label = \"mean survived\")\nplt.bar(df_age_median.Age[40:90], df_age_median.Survived[40:90], alpha=0.5, label = \"median survived\")\nplt.xticks(np.arange(31, 81, 1))\nplt.hlines(0.75, 31, 81, color = \"red\", label = \"75 quintile\")\nplt.hlines(0.25, 31, 81, color = \"green\", label = \"25 quintile\")\nplt.legend()\nplt.show()","d544a8a8":"plt.figure(figsize=(20, 10))\nplt.plot(df_age_mean.Age[0:7], df_age_mean.Survived[0:7], color = \"black\", marker = \"o\")\nplt.bar(df_age_median.Age[0:7], df_age_median.Survived[0:7], width=0.1, alpha=0.5)\nplt.xticks(np.arange(0, 2.1, 0.1))\nplt.show()","3d88b6b0":"### Number of people aged 27 on board\nage_27 = df_titanic.loc[df_titanic['Age'] == 27].count()[0]\n\n### Number of survived people aged 27 on board\nage_27_survived = df_titanic.loc[(df_titanic['Age'] == 27) & (df_titanic['Survived'] == 1)].count()[0]\n\n### Number of survived man aged 27 on board\nage_27_man = df_titanic.loc[(df_titanic['Age'] == 27) \n                            & (df_titanic['Survived'] == 1) \n                            & (df_titanic['Sex'] == 0)].count()[0]\n\nprint(f\"Number of people aged 27 on board: {age_27} person or {round(age_27\/891, 3) * 100} %\")\nprint(f\"Survived from them: {age_27_survived} person or {round((age_27_survived)\/(age_27), 4) *100}%\")\nprint(f\"Mans from them: {age_27_man} person or {round((age_27_man)\/(age_27_survived), 3) *100}%\")\n\n### Average for passengers aged 27\ndf_titanic[df_titanic[\"Age\"] == 27].groupby(['Sex']\n                                     ).mean().sort_values(by = 'Survived')","a46fe871":"### Number of people aged 29 on board\nage_29 = df_titanic.loc[df_titanic['Age'] == 29].count()[0]\n\n### Number of survived people aged 29 on board\nage_29_survived = df_titanic.loc[(df_titanic['Age'] == 29) & (df_titanic['Survived']==1)].count()[0]\n\n### Number of survived man aged 29 on board\nage_29_man = df_titanic.loc[(df_titanic['Age'] == 29) \n                            & (df_titanic['Survived'] == 1) \n                            & (df_titanic['Sex'] == 0)].count()[0]\n\nprint(f\"Number of people aged 29 on board: {age_29} person or {round(age_29\/891, 4) * 100} % \u043e\u0442 \u0447\u0438\u0441\u043b\u0430 \u043f\u0430\u0441\u0441\u0430\u0436\u0438\u0440\u043e\u0432\")\nprint(f\"Survived from them: {age_29_survived} person or {round((age_29_survived)\/(age_29), 3) *100}%\")\nprint(f\"Mans from them: {age_29_man} person or {round((age_29_man)\/(age_29_survived), 3) *100}%\")\n\n### Average for passengers aged 29\ndf_titanic[df_titanic[\"Age\"] == 29].groupby(['Sex']\n                                     ).mean().sort_values(by='Survived')","d146bfe7":"### Percentage of survivors for each class by age\n\ngrid = sns.FacetGrid(titanic_df, col='Survived', row='Pclass', height=4.2, aspect=1.6)\ngrid.map(plt.hist, 'Age', bins=40)\ngrid.add_legend()\nplt.show()","d62549aa":"### For convenience, let's denote the training dataset df_train\ndf_train = df_titanic\n\n### Load the test dataset\ndf_test = pd.read_csv(\"..\/input\/titanic\/test.csv\")\ndf_test.head()","8a79f0d9":"### Training dataset size\ndf_test.shape","09b58c0d":"### Let's check the missing values in the training dataset\ndf_test.isna().sum()","3f0e9dd6":"### Let's bring the training dataset to working form\n\n### Set index based on 'PassengerId'\ndf_test = df_test.set_index('PassengerId')\ndf_test = df_test.sort_index()\n\n### Removing uninteresting columns\ndf_test.drop([\"Name\", \"Ticket\", \"Cabin\"], axis = 1, inplace = True)\n\n\n### Converting to a number format\ndf_test = df_test.replace('male', 0)\ndf_test = df_test.replace('female', 1)\ndf_test = df_test.replace('U', 0)\ndf_test = df_test.replace('S', 1)\ndf_test = df_test.replace('C', 2)\ndf_test = df_test.replace('Q', 3)","5568055c":"### Checking missing values\ndf_test.isna().sum()","46d4d1ff":"### Find the median age for each gender\ndf_test.groupby('Sex')['Age'].median()","ffb67815":"### Replace missing values\n\ndf_test['Age'].fillna(27, inplace = True)\ndf_test[\"Fare\"].fillna(12, inplace = True)","6f0f1f56":"full_data = [df_train, df_test]","82b543ce":"# Create new feature FamilySize\nfor dataset in full_data:\n    dataset['FamilySize'] = dataset['SibSp'] + dataset['Parch'] + 1\n# Create new feature IsAlone from FamilySize\nfor dataset in full_data:\n    dataset['IsAlone'] = 0\n    dataset.loc[dataset['FamilySize'] == 1, 'IsAlone'] = 1","a743f32f":"g = sns.catplot(x = \"FamilySize\", y =\"Survived\", data = df_train, kind = 'bar')\ng = g.set_ylabels(\"Survival Probability\")","a32bc725":"sns.jointplot(x='Fare', y='Age', data=df_train, kind='scatter')\nplt.show()","b6a09bd5":"# Mapping Fare\ndf_train.loc[ df_train['Fare'] <= 7.91, 'Fare'] = 0\ndf_train.loc[(df_train['Fare'] > 7.91) & (df_train['Fare'] <= 14.454), 'Fare'] = 1\ndf_train.loc[(df_train['Fare'] > 14.454) & (df_train['Fare'] <= 31), 'Fare']   = 2\ndf_train.loc[ df_train['Fare'] > 31, 'Fare'] = 3\ndf_train['Fare'] = df_train['Fare'].astype(int)\n    \n# Mapping Age\ndf_train.loc[ df_train['Age'] <= 16, 'Age']  = 0\ndf_train.loc[(df_train['Age'] > 16) & (df_train['Age'] < 27), 'Age'] = 1\ndf_train.loc[ df_train['Age'] == 27, 'Age']  = 2\ndf_train.loc[(df_train['Age'] > 27) & (df_train['Age'] < 29), 'Age'] = 3\ndf_train.loc[ df_train['Age'] == 29, 'Age']  = 4\ndf_train.loc[(df_train['Age'] > 29) & (df_train['Age'] < 48), 'Age'] = 5\ndf_train.loc[(df_train['Age'] > 48) & (df_train['Age'] <= 64), 'Age'] = 6\ndf_train.loc[ df_train['Age'] > 64, 'Age'] = 7\ndf_train['Age'] = df_train['Age'].astype(int)","8d36238e":"# Mapping Fare\ndf_test.loc[ df_test['Fare'] <= 7.91, 'Fare'] = 0\ndf_test.loc[(df_test['Fare'] > 7.91) & (df_test['Fare'] <= 14.454), 'Fare'] = 1\ndf_test.loc[(df_test['Fare'] > 14.454) & (df_test['Fare'] <= 31), 'Fare']   = 2\ndf_test.loc[ df_test['Fare'] > 31, 'Fare'] = 3\ndf_test['Fare'] = df_test['Fare'].astype(int)\n    \n# Mapping Age\ndf_test.loc[ df_test['Age'] <= 16, 'Age']  = 0\ndf_test.loc[(df_test['Age'] > 16) & (df_test['Age'] < 27), 'Age'] = 1\ndf_test.loc[ df_test['Age'] == 27, 'Age']  = 2\ndf_test.loc[(df_test['Age'] > 27) & (df_test['Age'] < 29), 'Age'] = 3\ndf_test.loc[ df_test['Age'] == 29, 'Age']  = 4\ndf_test.loc[(df_test['Age'] > 29) & (df_test['Age'] < 48), 'Age'] = 5\ndf_test.loc[(df_test['Age'] > 48) & (df_test['Age'] <= 64), 'Age'] = 6\ndf_test.loc[ df_test['Age'] > 64, 'Age'] = 7\ndf_test['Age'] = df_test['Age'].astype(int)","24859bc2":"df_test.drop([\"SibSp\", \"Parch\"], 1)","d322e1d5":"df_train.drop([\"SibSp\", \"Parch\"], 1)","f95d5574":"df_test.isna().sum()","5c109895":"### Divide the dataset into characteristics and target variable\ntrain = df_train.drop(\"Survived\", 1)\ntarget = df_train[\"Survived\"]\n\nX_train, X_test, y_train, y_test = train_test_split(train, target, test_size=0.20, random_state = 17) ","78302468":"skf = StratifiedKFold(n_splits=10, random_state=17, shuffle=True)","bb37a23e":"### KNN\n\nknn = KNeighborsClassifier(n_neighbors=5)\nknn_model = knn.fit(X_train, y_train)\nknn_prediction = knn.predict(X_test)\nknn_score = cross_val_score(knn, X_train, y_train, cv=skf)\nknn_score.mean()","72e341ee":"# Stochastic Gradient Descent\n\nsgd = SGDClassifier()\nsgd.fit(X_train, y_train)\nsgd_prediction = sgd.predict(X_test)\nsgd_score = cross_val_score(sgd, X_train, y_train, cv=skf)\nsgd_score.mean()","cd304c7f":"# Support Vector Machines\n\nsvc = SVC(probability=True)\nsvc.fit(X_train, y_train)\nsvc_prediction = svc.predict(X_test)\nsvc_score = cross_val_score(svc, X_train, y_train, cv=skf)\nsvc_score.mean()","3e3b7803":"# Logistic Regression\n\nlogreg = LogisticRegression()\nlogreg.fit(X_train, y_train)\nlogreg_prediction = logreg.predict(df_test)\nlogreg_score = cross_val_score(logreg, X_train, y_train, cv=skf)\nlogreg_score.mean()\n","92aabf97":"# Gaussian Naive Bayes\n\ngaussian = GaussianNB()\ngaussian.fit(X_train, y_train)\ngaussian_prediction = gaussian.predict(X_test)\ngaussian_score = cross_val_score(gaussian, X_train, y_train, cv=skf)\ngaussian_score.mean()","a91da3e8":"# Random Forest\n\nrandom_forest = RandomForestClassifier(n_estimators=100)\nrandom_forest.fit(X_train, y_train)\nrf_prediction = random_forest.predict(X_test)\nrandom_forest.score(X_train, y_train)\nrandom_forest_score = cross_val_score(random_forest, X_train, y_train, cv=skf)\nrandom_forest_score.mean()","681ba5ec":"# DecisionTreeClassifier\n\ndecision_tree = DecisionTreeClassifier()\nfrom sklearn.model_selection import GridSearchCV, cross_val_score\n\ntree_params = {'max_depth': [4], \n               'max_features': [7],\n              'random_state' : [15]}\n\ntree_grid = GridSearchCV(decision_tree, tree_params, cv=skf)","343a313d":"tree_grid.fit(X_train, y_train)\nprint(tree_grid.best_params_)\nprint(tree_grid.best_score_)","740bfa36":"dtc_model = tree_grid.fit(X_train, y_train)\ndtc_predictions = tree_grid.predict(df_test)\n\ntree_grid_score = cross_val_score(tree_grid, X_train, y_train, cv=skf)\ntree_grid_score.mean()","7b4efdb4":"### Gradient Boosting\ntree = [1] + list(range(10, 100, 10))\nxgb = XGBClassifier(n_estimators=1000, use_label_encoder=False)","20043e35":"xgb_model = xgb.fit(X_train, y_train)\nxgb_predictions = xgb.predict(X_test)\n\nxgb_model_score = cross_val_score(xgb_model, X_train, y_train, cv=skf)\nxgb_model_score.mean()","13263fbb":"### The best auc_roc models\nplt.figure(figsize=(10,6))\n### KNN\nprobas = knn.fit(X_train, y_train).predict_proba(X_test)\nfpr, tpr, _ = metrics.roc_curve(y_test, probas[:, 1])\nauc = roc_auc_score(y_test, probas[:, 1])\nplt.plot(fpr, tpr, label=\"KNN auc=\" + str(auc))\n### DecisionTreeClassifier\nprobas = decision_tree.fit(X_train, y_train).predict_proba(X_test)\nfpr, tpr, _ = metrics.roc_curve(y_test, probas[:, 1])\nauc = roc_auc_score(y_test, probas[:, 1])\nplt.plot(fpr, tpr, label=\"Decision_tree auc=\" + str(auc))\n# Support Vector Machines\nprobas = svc.fit(X_train, y_train).predict_proba(X_test)\nfpr, tpr, _ = metrics.roc_curve(y_test, probas[:, 1])\nauc = roc_auc_score(y_test, probas[:, 1])\nplt.plot(fpr, tpr, label=\"svc auc=\" + str(auc))\n# Logistic Regression\nprobas = logreg.fit(X_train, y_train).predict_proba(X_test)\nfpr, tpr, _ = metrics.roc_curve(y_test, probas[:, 1])\nauc = roc_auc_score(y_test, probas[:, 1])\nplt.plot(fpr, tpr, label=\"LogReg auc=\" + str(auc))\n# Gaussian Naive Bayes\nprobas = gaussian.fit(X_train, y_train).predict_proba(X_test)\nfpr, tpr, _ = metrics.roc_curve(y_test, probas[:, 1])\nauc = roc_auc_score(y_test, probas[:, 1])\nplt.plot(fpr, tpr, label=\"gaussian auc=\" + str(auc))\n# Random Forest\nprobas = random_forest.fit(X_train, y_train).predict_proba(X_test)\nfpr, tpr, _ = metrics.roc_curve(y_test, probas[:, 1])\nauc = roc_auc_score(y_test, probas[:, 1])\nplt.plot(fpr, tpr, label=\"random forest auc=\" + str(auc))\n#Tree grid\nprobas = tree_grid.fit(X_train, y_train).predict_proba(X_test)\nfpr, tpr, _ = metrics.roc_curve(y_test, probas[:, 1])\nauc = roc_auc_score(y_test, probas[:, 1])\nplt.plot(fpr, tpr, label=\"Tree grid auc=\" + str(auc))\n### Gradient Boosting\nprobas = tree_grid.fit(X_train, y_train).predict_proba(X_test)\nfpr, tpr, _ = metrics.roc_curve(y_test, probas[:, 1])\nauc = roc_auc_score(y_test, probas[:, 1])\nplt.plot(fpr, tpr, label=\"xgb auc=\" + str(auc))\n\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.legend(loc=4)\nplt.show()","3d2cad11":"### Building an over-sampling model based on the SMOTE algorithm\n\nX = df_train.loc[:, df_train.columns != 'Survived']\ny = df_train.loc[:, df_train.columns == 'Survived']\n\nfrom imblearn.over_sampling import SMOTE\n\nos = SMOTE(random_state=17)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=17)\ncolumns = X_train.columns\n\nos_data_X,os_data_y=os.fit_resample(X_train, y_train)\nos_data_X = pd.DataFrame(data=os_data_X,columns=columns )\nos_data_y= pd.DataFrame(data=os_data_y,columns=['Survived'])\n\nprint(\"\u0412\u0435\u043b\u0438\u0447\u0438\u043d\u0430 \u0434\u0430\u0442\u0430\u0441\u0435\u0442\u0430 \",len(os_data_X))\nprint(\"\u041a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u043e\u0441\u0442\u0430\u0432\u0448\u0438\u0445\u0441\u044f \u0430\u0431\u043e\u043d\u0435\u043d\u0442\u043e\u0432\",len(os_data_y[os_data_y['Survived']==0]))\nprint(\"\u041a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u0443\u0448\u0435\u0434\u0448\u0438\u0445 \u0430\u0431\u043e\u043d\u0435\u043d\u0442\u043e\u0432\",len(os_data_y[os_data_y['Survived']==1]))\n\nos_data_X.shape","818d100b":"X = os_data_X\ny = os_data_y\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=17)\n\nrandom_forest = RandomForestClassifier(n_estimators=100, random_state=17)\nrandom_forest.fit(X_train, y_train.values.ravel())\nY_pred_1 = random_forest.predict(df_test)\nrandom_forest_score = cross_val_score(random_forest, X_train, y_train.values.ravel(), cv=skf)\nrandom_forest_score.mean()\n\n","e30a4969":"probas = random_forest.fit(X_train, y_train.values.ravel()).predict_proba(X_test)\nfpr, tpr, _ = metrics.roc_curve(y_test, probas[:, 1])\nauc = roc_auc_score(y_test, probas[:, 1])\nplt.figure(figsize=(10,5))\nplt.plot(fpr, tpr, label=\"auc=\" + str(auc))\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.legend(loc=4)\nplt.show()","299748ad":"# Generate Submission File \nStackingSubmission = pd.DataFrame({ 'PassengerId': df_test.index,\n                            'Survived': dtc_predictions })\nStackingSubmission.to_csv(\"StackingSubmissionBest108.csv\", index=False)","ff0e8f29":"### According to the received data, the DecisionTreeClassifier model shows the best quality","23c6a18e":"### Before analyzing the dataset, we will give some historical information\n\n* Number of people on board at the time of the tragedy: 1,300 passengers and 908 crew members\n* Number of survivors: 712 people or 32.2%\n* The number of deaths: 1496 people or 67.8%\n* Number of decks 8: upper boat and then 7, designated from top to bottom by letters from A to G. The boat deck and deck A occupied only the superstructure, and deck G covered the bow - from the boiler rooms to the bow, and aft - from the engine room to the stern cut ... There were 26 cabins for 106 third class passengers in the bow and 60 two-, four- and six-berth cabins for 186 third class passengers. Decks C, D, E and F stretched along the entire length of the ship.\n* Number of boats - 20. 16 six-oared wooden boats, suspended from davits, and 4 four-oared folding boats with wooden bottom and sailcloth sides. 14 out of 16 wooden boats could take on board 65 people. 2 \"rescue\" boats had a capacity of 40 people. Folding boats were designed for 47 people each. The total capacity of the boats was 1,178 people, or about 53% of the passengers at full design load.\n","061afef6":"### From the data obtained above, the following conclusions can be drawn about the first stage of the dataframe analysis:\n\n* The average value of survivors in the presented dataset is 38.39%, which differs from historical data by 32.2%\n* 50% of people under 29 years old, 75% under 35 years old\n* More than 50% of people traveled in 3rd class\n","5a9b2369":"### The resulting graph shows that the number of people on board at the age of 27 and, in particular, 29 years old significantly exceeds the number of passengers of other ages","43e88c47":"### Percentage of survivors in each group","93edf25d":"#### We see 177 missing values \u200b\u200bfor the age column, 687 for the cabin number column and 2 for the departure point.\n\nObviously, at this stage, for further analysis, we need to perform some transformations:\n* Passenger age is a very important metric for building machine learning models for a given dataset. To fill in the missing values, replace them with medians depending on the gender of the person: i.e. if the gender of the person in the line with the missing value \"male\", replace the missing value of age with the median for all known ages of men, and vice versa.\n* Although the cabin number could be used as a parameter for analysis depending on the deck number, we have 687 missing values \u200b\u200bin this column, so it will have to be excluded from our analysis at the moment.\n* Since not all classifiers can work with categorical features, in this case we will replace them with numeric ones. In the Sex column, replace the values \u200b\u200bwith 0 if the gender is \"male\", and 1 if \"female\". In the Embarked column, replace the parameters \"U\" (two unknown values \u200b\u200bwill be replaced by \"U\" before that), \"S\", \"C\", \"Q\" with 0, 1, 2, 3, respectively.\n* At this stage, we will also delete the Name column, since we will assume that the name does not matter for the purposes of our machine learning","f50c83cc":"### Find the family size of passengers","9da9c356":"### 2) Building models","bceb63e9":"### What conclusions can be drawn from the obtained data on the age of passengers:\n* There are large age groups in which less than 50% of passengers survived:\n- between the ages of 7 and 11\n- from 16 to 26 years old\n- from 37 to 47 years old\n- from 64 to 74 years old\n* The largest number of passengers on board was at the age of 29 - 144 people or 16.16%. At the same time, only 16.7% or 24 people of them survived, with 79.2% or 19 people being men.\n* The second largest group at the age of 27 - 71 people or 8% of the total number of passengers on board. Here we have a completely different situation. 47 people or 66.2% survived, with 12.8 men or 6 people.\n* These two age groups account for over 24%, that is, almost a quarter of our sample. Considering that they also give completely opposite results in terms of gender and number of survivors, this can seriously affect our models. This should be taken into account.\n","561c6de4":"### It is clearly seen that there were many children under 2 years of age on board, we will display this graph separately for greater clarity","1ed42298":"### Family size categories shows us, that small families had more chances to survive than single passenger and large families","787fa40d":"### Preparing a test dataset","e7b7f97c":"#### For the initial analysis, I selected the most popular classification methods:\n\n* KNN\n* Decision Tree\n* Stochastic Gradient Descent\n* Support Vector Machines\n* Logistic Regression\n* Gaussian Naive Bayes\n* Random Forest\n* and also Gradient Boosting for Meta Modeling","5c321d0e":"### Despite the improvement of the cross_val and auc_roc metrics, using SMOTE on the test dataset leads to model overfitting and a drop in quality\n\n### Thus, at the moment, for this dataset, the DecisionTreeClassifier model remains the best, giving cross_val_score = 0.8385 and showing an accuracy of 0.79 on the test sample","67feecc6":"### 1.1) Analysis of training dataset","ff9d760f":"#### The presented dataset contains information about 891 passengers, divided into 12 parameters:\n\n1) PassengerId - passenger id in order (int64)\n\n2) Survived - information about whether the passenger survived the crash (0 - no, 1 - yes) (int64)\n\n3) Pclass - the class number in which the passenger traveled (1, 2 or 3) (int64)\n\n4) Name - passenger's name (object)\n\n5) Sex - passenger gender (male \/ female) (object)\n\n6) Age - passenger's age (float64)\n\n7) SibSp - the number of brothers \/ sisters \/ spouses :)) on board the Titanic (int64)\n\n8) Parch - the number of parents \/ children on board the Titanic (int64)\n\n9) Ticket - ticket number (object)\n\n10) Fare - ticket price (float64)\n\n11) Cabin - cabin number (object)\n\n12) Embarked - port of departure (C = Cherbourg, Q = Queenstown, S = Southampton) (object)","bec546dd":"### In this notebook, we will look at the Titanic dataset from the Kaggle competition and use it to build predictive                                       models to determine the survivability of passengers","562d4ec3":"### From the data obtained, it can be seen that the number of survivors varies greatly depending on both the class in which the passenger traveled and the gender","afea584f":"### Let's combine age and fare by group. Separately, we will highlight the groups of 27 and 29 years old due to previously identified anomalies","925ce7c2":"### Detailed analysis of people aged 27 and 29 ","a2fb64ae":"### 1) Loading and analyzing dataset"}}