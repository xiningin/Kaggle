{"cell_type":{"f9201192":"code","d3e0d89d":"code","53f1f15b":"code","cb11dbf2":"code","3f27c180":"code","9eaf9a90":"code","bc358827":"code","45be6ac8":"code","67b8776f":"code","91ec29a4":"markdown","b37007b0":"markdown","812ce29e":"markdown","ba7f70df":"markdown","6da0e595":"markdown","a1d8d3f7":"markdown","04731e8d":"markdown","bfad882e":"markdown","b453c2ed":"markdown"},"source":{"f9201192":"# import libraries\n\nimport random\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline","d3e0d89d":"np.random.seed(0)","53f1f15b":"from sklearn.datasets.samples_generator import make_blobs \nX, y = make_blobs(n_samples=5000, centers=[[4,4], [-2, -1], [2, -3], [1, 1]], cluster_std=0.9)","cb11dbf2":"plt.scatter(X[:, 0], X[:, 1], marker='.')","3f27c180":"from sklearn.cluster import KMeans \n\nk_means = KMeans(init = 'k-means++',n_clusters = 4, n_init = 12)","9eaf9a90":"k_means.fit(X)","bc358827":"k_means_labels = k_means.labels_\nk_means_labels","45be6ac8":"k_means_cluster_centers = k_means.cluster_centers_\nk_means_cluster_centers","67b8776f":"# Initialize the plot with the specified dimensions.\nfig = plt.figure(figsize=(6, 4))\n\n# Colors uses a color map, which will produce an array of colors based on\n# the number of labels there are. We use set(k_means_labels) to get the\n# unique labels.\ncolors = plt.cm.Spectral(np.linspace(0, 1, len(set(k_means_labels))))\n\n# Create a plot\nax = fig.add_subplot(1, 1, 1)\n\n# For loop that plots the data points and centroids.\n# k will range from 0-3, which will match the possible clusters that each\n# data point is in.\nfor k, col in zip(range(len([[4,4], [-2, -1], [2, -3], [1, 1]])), colors):\n\n    # Create a list of all data points, where the data poitns that are \n    # in the cluster (ex. cluster 0) are labeled as true, else they are\n    # labeled as false.\n    my_members = (k_means_labels == k)\n    \n    # Define the centroid, or cluster center.\n    cluster_center = k_means_cluster_centers[k]\n    \n    # Plots the datapoints with color col.\n    ax.plot(X[my_members, 0], X[my_members, 1], 'w', markerfacecolor=col, marker='.')\n    \n    # Plots the centroids with specified color, but with a darker outline\n    ax.plot(cluster_center[0], cluster_center[1], 'o', markerfacecolor=col,  markeredgecolor='k', markersize=6)\n\n# Title of the plot\nax.set_title('KMeans')\n\n# Remove x-axis ticks\nax.set_xticks(())\n\n# Remove y-axis ticks\nax.set_yticks(())\n\n# Show the plot\nplt.show()\n","91ec29a4":"Now let's fit the KMeans model with the feature matrix we created above, X","b37007b0":"Now let's grab the labels for each point in the model using KMeans' .labels_ attribute and save it as k_means_labels","812ce29e":"Next we will be making random clusters of points by using the **make_blobs** class. The make_blobs class can take in many inputs, but we will be using these specific ones.\n\n **Input**\n\n* **n_samples:** The total number of points equally divided among clusters.\nValue will be: 5000\n\n* **centers:** The number of centers to generate, or the fixed center locations.\nValue will be: [[4, 4], [-2, -1], [2, -3],[1,1]]\n* **cluster_std:** The standard deviation of the clusters.\nValue will be: 0.9\n\n**Output**\n* **X:** Array of shape [n_samples, n_features]. (Feature Matrix)\nThe generated samples.\n* **y:** Array of shape [n_samples]. (Response Vector)\nThe integer labels for cluster membership of each sample.","ba7f70df":"## Creating the Visual Plot\n\nSo now that we have the random data generated and the KMeans model initialized, let's plot them and see what it looks like!\nPlease read through the code and comments to understand how to plot the model.","6da0e595":"Create our own dataset for clustering. First we need to set up a random seed. Use numpy's random.seed() function, where the seed will be set to 0.","a1d8d3f7":"## Now that we have our random data, let's set up our K-Means Clustering.\n\nThe KMeans class has many parameters that can be used, but we will be using these three:\n\n* **init:** Initialization method of the centroids.\nValue will be: \"k-means++\"\n**k-means++:** Selects initial cluster centers for k-mean clustering in a smart way to speed up convergence.\n* **n_clusters:** The number of clusters to form as well as the number of centroids to generate.\nValue will be: 4 (since we have 4 centers)\n* **n_init:** Number of time the k-means algorithm will be run with different centroid seeds. The final results will be the best output of n_init consecutive runs in terms of inertia.\nValue will be: 12\n\nInitialize KMeans with these parameters, where the output parameter is called k_means.","04731e8d":"# K-Means Clustering\n\n## Introduction :\n\nK-means clustering is a method of vector quantization, originally from signal processing, that aims to partition n observations into k clusters in which each observation belongs to the cluster with the nearest mean, serving as a prototype of the cluster. \nThere are many models for clustering out there. In this notebook, I will be presenting the model that is considered one of the simplest models amongst them. Despite its simplicity, the K-means is vastly used for clustering in many data science applications, especially useful if you need to quickly discover insights from unlabeled data.\n\nSome real-world applications of k-means:\n* Customer segmentation\n* Understand what the visitors of a website are trying to accomplish\n* Pattern recognition\n* Machine learning\n* Data compression\n\nIn this notebook we practice k-means clustering with one example:\n\n* k-means on a random generated dataset","bfad882e":"Display the scatter plot of the randomly generated data.","b453c2ed":"We will also get the coordinates of the cluster centers using KMeans' .cluster_centers_ and save it as k_means_cluster_centers"}}