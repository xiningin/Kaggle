{"cell_type":{"47ad7de7":"code","ad6721ab":"code","e3291a66":"code","e4a8b404":"code","c9937d9f":"code","89217526":"code","c96be696":"code","a210baf1":"code","e97f084a":"code","dae60f68":"code","0326d6df":"code","38d3f90b":"code","0bd4e9f3":"markdown","bb96f244":"markdown","9cf52c82":"markdown","1c81a73d":"markdown","2cf14da1":"markdown","dca7acce":"markdown","8404aefe":"markdown","2d0d070a":"markdown","c317c4df":"markdown","1f8cbffb":"markdown","6f2a25a5":"markdown","c2127b1b":"markdown","6341f164":"markdown","9977b40c":"markdown","fc90d362":"markdown","78bd6cc8":"markdown"},"source":{"47ad7de7":"!pip -q install jax jaxlib\n\n%env JAX_ENABLE_X64=1\n%env JAX_PLATFORM_NAME=cpu\n\nimport jax.numpy as np\nfrom jax import grad, jit","ad6721ab":"def J(X, w, b, y):\n    \"\"\"Cost function for a linear regression. A forward pass of our model.\n\n    Args:\n        X: a features matrix.\n        w: weights (a column vector).\n        b: a bias.\n        y: a target vector.\n\n    Returns:\n        scalar: a cost of this solution.    \n    \"\"\"\n    y_hat = X.dot(w) + b # Predict values.\n    return ((y_hat - y)**2).mean() # Return cost.","e3291a66":"# A features matrix.\nX = np.array([\n                 [4., 7.],\n                 [1., 8.],\n                 [-5., -6.],\n                 [3., -1.],\n                 [0., 9.]\n             ])\n\n# A target column vector.\ny = np.array([\n                 [37.],\n                 [24.],\n                 [-34.], \n                 [16.],\n                 [21.]\n             ])\n\nlearning_rate = 0.01","e4a8b404":"w = np.zeros((2, 1))\nb = 0.","c9937d9f":"%timeit grad(J, argnums=1)(X, w, b, y)","89217526":"%timeit grad(J, argnums=2)(X, w, b, y)","c96be696":"for i in range(100):\n    w -= learning_rate * grad(J, argnums=1)(X, w, b, y)\n    b -= learning_rate * grad(J, argnums=2)(X, w, b, y)\n    \n    if i % 10 == 0:\n        print(J(X, w, b, y))","a210baf1":"w = np.zeros((2, 1))\nb = 0.","e97f084a":"grad_X = jit(grad(J, argnums=1))\ngrad_b = jit(grad(J, argnums=2))\n\n# Run once to trigger JIT compilation.\ngrad_X(X, w, b, y)\ngrad_b(X, w, b, y)","dae60f68":"%timeit grad_X(X, w, b, y)","0326d6df":"%timeit grad_b(X, w, b, y)","38d3f90b":"for i in range(100):\n    w -= learning_rate * grad_X(X, w, b, y)\n    b -= learning_rate * grad_b(X, w, b, y)\n    \n    if i % 10 == 0:\n        print(J(X, w, b, y))","0bd4e9f3":"Let's define our model (a linear regression).","bb96f244":"Let's measure how fast gradient functions without JIT compilation.","9cf52c82":"Initialize training data and set a learning rate.","1c81a73d":"## Preparations","2cf14da1":"Cost values are exactly the same, but the speedup is significant.","dca7acce":"# Tutorial: Efficient gradient descent with JAX","8404aefe":"Initialize weights and a bias with zeros.","2d0d070a":"It is incredible, more than a 30x speedup! Let's train a model again with a JIT-compiled gradient functions and compare results.","c317c4df":"Train a model for 100 epochs. Print a cost for every 10th epoch.","1f8cbffb":"If you read my [Tutorial: Automatic derivatives with JAX](https:\/\/www.kaggle.com\/grez911\/tutorial-automatic-derivatives-with-jax) then you you can already automatically calculate derivatives of arbitrary complex functions without any problems. It saves your time and this way is less error prone.\n\nNow we want to train our model. Let's compare how fast it will be trained without vs with [JIT compilation](https:\/\/jax.readthedocs.io\/en\/latest\/notebooks\/quickstart.html#Using-jit-to-speed-up-functions).\n\nWe will use already familiar linear regression just for an example, see my tutorials: [Math of a linear regression](https:\/\/www.kaggle.com\/grez911\/math-of-a-linear-regression) and already mentioned [Tutorial: Automatic derivatives with JAX](https:\/\/www.kaggle.com\/grez911\/tutorial-automatic-derivatives-with-jax).","6f2a25a5":"Measure functions execution time.","c2127b1b":"Initialize weights and a bias with zeros.","6341f164":"Define separate compiled functions for partial derivatives with respect to $X$ and $b$.  ","9977b40c":"Install [JAX](https:\/\/github.com\/google\/jax), enable usage of 64-bit float numbers by default, use CPU for calculations and import modules.","fc90d362":"## Train with JIT","78bd6cc8":"## Train without JIT"}}