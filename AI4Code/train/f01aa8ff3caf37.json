{"cell_type":{"b04ec7cc":"code","b53f17c8":"code","256b0164":"code","21480d80":"code","5e286e3b":"code","dcd07baa":"code","2af486b7":"code","9ca8dd77":"code","4defc30c":"code","e76dd99a":"code","f5d171de":"code","8f65800f":"code","b3b4cf3d":"code","461cad5c":"code","077b1ab6":"code","e0481077":"code","ce3e4785":"code","54b64f16":"code","d3982c2f":"code","d1dd7cfd":"code","0a9f6e70":"code","8bcf62dc":"markdown","8cdb3b2f":"markdown","5a8765c4":"markdown","72d18306":"markdown","8513ae4d":"markdown","3e3cf320":"markdown","83e074e1":"markdown","78fd6bbd":"markdown","87fe5ea0":"markdown","a00fdcab":"markdown","8e16e88a":"markdown","f29c32e9":"markdown"},"source":{"b04ec7cc":"import matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nimport numpy as np\n\n#import data\ntrain_df = pd.read_csv('\/kaggle\/input\/titanic\/train.csv', index_col=0)\ntest_df = pd.read_csv('\/kaggle\/input\/titanic\/test.csv', index_col=0)","b53f17c8":"train_df.head()","256b0164":"train_df.info()","21480d80":"#converting objects data types into categorical types\ntrain_df[\"Pclass\"] = pd.Categorical(train_df[\"Pclass\"], categories=[1,2,3], ordered=True)\ntrain_df[\"Sex\"] = pd.Categorical(train_df[\"Sex\"], categories=[\"male\", \"female\"], ordered=False)\ntrain_df[\"Embarked\"] = pd.Categorical(train_df[\"Embarked\"], categories=[\"C\", \"Q\", \"S\"], ordered=False)\ntest_df[\"Pclass\"] = pd.Categorical(test_df[\"Pclass\"], categories=[1,2,3], ordered=True)\ntest_df[\"Sex\"] = pd.Categorical(test_df[\"Sex\"], categories=[\"male\", \"female\"], ordered=False)\ntest_df[\"Embarked\"] = pd.Categorical(test_df[\"Embarked\"], categories=[\"C\", \"Q\", \"S\"], ordered=False)\ntrain_df.info()","5e286e3b":"train_df.Name.head(50)","dcd07baa":"#extract beginning of string until comma is found\ntrain_df[\"LastName\"] = train_df.Name.str.extract(\"([^,]+)\", expand=True)\ntest_df[\"LastName\"] = test_df.Name.str.extract(\"([^,]+)\", expand=True)\n\n#extract beginning after the comma and ending at the period\ntrain_df[\"Title\"] = train_df.Name.str.extract(r\", (.*?)\\.\", expand=True)\ntrain_df[\"Title\"] = pd.Categorical(train_df[\"Title\"],\n                                    categories=train_df.Title.unique(),\n                                    ordered=False)\ntest_df[\"Title\"] = test_df.Name.str.extract(r\", (.*?)\\.\", expand=True)\ntest_df[\"Title\"] = pd.Categorical(test_df[\"Title\"],\n                                    categories=test_df.Title.unique(),\n                                    ordered=False)\n\n#extract all text following the title word\ntrain_df[\"FirstName\"] = train_df.Name.str.extract(r\"\\. (.*?)$\", expand=True)\ntest_df[\"FirstName\"] = test_df.Name.str.extract(r\"\\. (.*?)$\", expand=True)\n\n#drop the original Name field since its information is now fully duplicated by the new fields.\ntrain_df = train_df.drop(\"Name\", axis=1)\ntest_df = test_df.drop(\"Name\", axis=1)\n\ntrain_df.loc[:,[\"LastName\", \"Title\", \"FirstName\"]].head(50)","2af486b7":"sns.set()\nsns.catplot(x=\"Title\", kind=\"count\", data=train_df, aspect=2)\nplt.xlabel(\"Titles\")\nplt.ylabel(\"Count\")\nplt.title(\"Passenger Name Title Histogram\")\nplt.xticks(rotation=30, ha='right')\nplt.show()","9ca8dd77":"train_df[\"AgeNotExact\"] = np.where((train_df[\"Age\"] > 1) & (round(train_df[\"Age\"]) != train_df[\"Age\"]), True, False)\ntrain_df[\"AgeNotExact\"] = pd.Categorical(train_df[\"AgeNotExact\"], categories=[False, True], ordered=False)\ntest_df[\"AgeNotExact\"] = np.where((test_df[\"Age\"] > 1) & (round(test_df[\"Age\"]) != test_df[\"Age\"]), True, False)\ntest_df[\"AgeNotExact\"] = pd.Categorical(test_df[\"AgeNotExact\"], categories=[False, True], ordered=False)\n\nsns.catplot(x='AgeNotExact', y='Age', data=train_df, kind='swarm', orient='v', aspect=2)\nplt.show()","4defc30c":"train_df = train_df.drop([\"Ticket\", \"Cabin\", \"FirstName\", \"LastName\"], axis=1)\ntest_df = test_df.drop([\"Ticket\", \"Cabin\", \"FirstName\", \"LastName\"], axis=1)\ntrain_df.info()","e76dd99a":"from sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\n\n#create a pipeline for transforming numeric variables\nnum_pipeline = Pipeline([\n    ('imputer', SimpleImputer(strategy='median')),\n    ('std_scaler', StandardScaler())\n])\n\ncat_pipeline = Pipeline([\n    ('imputer', SimpleImputer(strategy='most_frequent')),\n    ('cat_encode', OneHotEncoder())\n])\n\n#create lists which define the numeric and categorical variables\nnum_attr = [\"Age\", \"SibSp\", \"Parch\", \"Fare\"]\ncat_attr = [\"Pclass\", \"Sex\", \"Embarked\", \"AgeNotExact\"]\n\nfull_pipeline = ColumnTransformer([\n    (\"num\", num_pipeline, num_attr),\n    (\"cat\", cat_pipeline, cat_attr)\n])\n\ny_train = train_df[\"Survived\"]\nprepped_X_train = full_pipeline.fit_transform(train_df.drop(\"Survived\", axis=1))\nprepped_X_test = full_pipeline.fit_transform(test_df)","f5d171de":"pclass_agg = train_df.groupby(\"Pclass\")[\"Survived\"].agg(\"mean\")\npclass_agg.plot(x=pclass_agg.index, y=\"Survived\")\nplt.xlabel(\"Passenger Class\")\nplt.ylabel(\"Survival Rate\")\nplt.ylim(bottom=0)\nplt.show()","8f65800f":"title_filter_agg = train_df.groupby(\"Title\")[\"Survived\"].agg(\"count\")\ntitle_filter = title_filter_agg > 5 #acts as a boolean filter to drop groups with small samples\ntitle_agg = train_df.groupby(\"Title\")[\"Survived\"].agg(\"mean\")\ntitle_agg = title_agg[title_filter]\ntitle_agg.plot.bar(x=title_agg.index, y=\"Survived\")\nplt.xlabel(\"Passenger Name Title\")\nplt.xticks(rotation=30, ha='right')\nplt.ylabel(\"Survival Rate\")\nplt.ylim(bottom=0)\nplt.show()","b3b4cf3d":"sex_agg = train_df.groupby(\"Sex\")[\"Survived\"].agg(\"mean\")\nsex_agg.plot.bar(x=sex_agg.index, y=\"Survived\")\nplt.xlabel(\"Passenger Sex\")\nplt.xticks(rotation=0, ha='center')\nplt.ylabel(\"Survival Rate\")\nplt.ylim(bottom=0)\nplt.show()","461cad5c":"age_filter_agg = train_df.groupby(\"Age\")[\"Survived\"].agg(\"count\")\nage_filter = age_filter_agg > 5 #acts as a boolean filter to drop groups with small samples\nage_agg = train_df.groupby(\"Age\")[\"Survived\"].agg(\"mean\")\nage_agg = age_agg[age_filter]\nage_agg.plot(x=age_agg.index, y=\"Survived\")\nplt.xlabel(\"Passenger Age\")\nplt.ylabel(\"Survival Rate\")\nplt.ylim(bottom=0)\nplt.show()","077b1ab6":"sibs_filter_agg = train_df.groupby(\"SibSp\")[\"Survived\"].agg(\"count\")\nsibs_filter = sibs_filter_agg > 5 #acts as a boolean filter to drop groups with small samples\nsibs_agg = train_df.groupby(\"SibSp\")[\"Survived\"].agg(\"mean\")\nsibs_agg = sibs_agg[sibs_filter]\nsibs_agg.plot(x=sibs_agg.index, y=\"Survived\")\nplt.xlabel(\"Passenger Siblings Count\")\nplt.ylabel(\"Survival Rate\")\nplt.ylim(bottom=0)\nplt.show()","e0481077":"parch_filter_agg = train_df.groupby(\"Parch\")[\"Survived\"].agg(\"count\")\nparch_filter = parch_filter_agg > 5 #acts as a boolean filter to drop groups with small samples\nparch_agg = train_df.groupby(\"Parch\")[\"Survived\"].agg(\"mean\")\nparch_agg = parch_agg[parch_filter]\nparch_agg.plot(x=parch_agg.index, y=\"Survived\")\nplt.xlabel(\"Passenger Parent Count\")\nplt.xticks(ticks=[0,1,2])\nplt.ylabel(\"Survival Rate\")\nplt.ylim(bottom=0)\nplt.show()","ce3e4785":"train_df[\"RndFare\"] = round(train_df[\"Fare\"], -1)\nfare_filter_agg = train_df.groupby(\"RndFare\")[\"Survived\"].agg(\"count\")\nfare_filter = fare_filter_agg > 5 #acts as a boolean filter to drop groups with small samples\nfare_agg = train_df.groupby(\"RndFare\")[\"Survived\"].agg(\"mean\")\nfare_agg = fare_agg[fare_filter]\nfare_agg.plot(x=fare_agg.index, y=\"Survived\")\nplt.xlabel(\"Passenger Fare (Aggregated by Rounding to Nearest 10 Unit)\")\nplt.ylabel(\"Survival Rate\")\nplt.ylim(bottom=0)\nplt.show()","54b64f16":"survived_agg = train_df.groupby(\"Survived\").agg(\"count\")\/len(train_df)\nsurvived_agg = survived_agg.iloc[:,1]\nprint(survived_agg.head())","d3982c2f":"from sklearn.tree import DecisionTreeClassifier, export_graphviz\nfrom sklearn.model_selection import cross_validate\nfrom graphviz import Source\nfrom IPython.display import SVG\n\nsurvived_agg = train_df.groupby(\"Survived\").agg(\"count\")\/len(train_df)\nsurvived_agg = survived_agg.iloc[:,1]\nprint(survived_agg.head())\n\ntree_clf = DecisionTreeClassifier(max_depth=3, criterion=\"entropy\", presort=True, random_state=123)\ntree_clf.fit(prepped_X_train, y_train)\n\n#cross validate model\nk = 5\ndt_scores = cross_validate(tree_clf, prepped_X_train, y_train, scoring=[\"accuracy\",\"roc_auc\"], cv=k)\nacc_mean = dt_scores[\"test_accuracy\"].mean()\nacc_rng = dt_scores[\"test_accuracy\"].max() - dt_scores[\"test_accuracy\"].min()\nauc_mean = dt_scores[\"test_roc_auc\"].mean()\nauc_rng = dt_scores[\"test_roc_auc\"].max() - dt_scores[\"test_roc_auc\"].min()\nprint(str().join([str(k), \"-fold Accuracy Score Mean:\"]), round(acc_mean, 5), \"Range:\", round(acc_rng, 5), \"\\n\")\nprint(str().join([str(k), \"-fold ROC AUC Score Mean:\"]), round(auc_mean, 5), \"Range:\", round(auc_rng, 5), \"\\n\")\n\ngraph = Source(export_graphviz(tree_clf, out_file=None))\nSVG(graph.pipe(format='svg'))","d1dd7cfd":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.metrics import roc_auc_score\n\nrf_param_grid = {'n_estimators': [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)],\n    'max_features': ['auto', 'sqrt'],\n    'max_depth': [int(x) for x in np.linspace(10, 110, num = 11)],\n    'min_samples_split': [2, 5, 10],\n    'min_samples_leaf': [1, 2, 4],\n    'bootstrap': [True, False]\n}\n\n#rf_clf = RandomForestClassifier(max_leaf_nodes=16, \n#                n_estimators=500, bootstrap=True, \n#                n_jobs=-1, oob_score=True, random_state=123)\nrf_clf = RandomForestClassifier(random_state=123)\nrf_rand = RandomizedSearchCV(rf_clf, param_distributions=rf_param_grid, n_iter=100, cv = k, verbose=2, random_state=123, n_jobs = -1, scoring=[\"accuracy\",\"roc_auc\"], refit=\"roc_auc\")\n\nrf_rand.fit(prepped_X_train, y_train)\nacc_mean = rf_rand.cv_results_[\"mean_test_accuracy\"].max()\nauc_mean = rf_rand.cv_results_[\"mean_test_roc_auc\"].max()\nprint(str().join([str(k), \"-fold Accuracy Score Mean:\"]), round(acc_mean, 5))\nprint(str().join([str(k), \"-fold ROC AUC Score Mean:\"]), round(auc_mean, 5))\n","0a9f6e70":"y_test_pred_df = pd.DataFrame(rf_rand.predict(prepped_X_test))\ny_test_pred_df.columns = [\"Survived\"]\ntest_pred_df = test_df.reset_index().join(y_test_pred_df)\nsubmit_df = test_pred_df.loc[:,[\"PassengerId\", \"Survived\"]]\nsubmit_df.to_csv('submit.csv', index=False)\n","8bcf62dc":"# Exploratory Data Analysis\n\nThe following code cells will explore data relationships in preparation for building models.  ","8cdb3b2f":"The results above show that the tuned random forest slightly out performed the decision tree classifier, therefore it will be used submission in the contest.","5a8765c4":"Next, attention will be paid to dividing the Name feature into more useful variables.","72d18306":"# Introduction\n\nBy Chris Raper\n\nThis notebook is my first data science project with Python after completing the the Data Science with Python career track on DataCamp.  This work was competed without any reference to Titanic dataset tutorials.  The notebook begins with data cleaning, feature engineering and transforming the data into machine learning format.  Then models are built using decision tree classifiers and random forest ensemble techniques along with hyperparameter tuning via random search.\n\n# Data Wrangling\n\nThe following code cells will examine the training data and wrangle it into a more usable form.  All the operations performed on the training dataset will also be performed on test dataset so the trained models can operate on the test dataset.","8513ae4d":"Studying the Ticket and Cabin features shows they would benefit from cleaning.  However, it isn't obvious how to proceed without researching the meaning of these features.  For now, they will be ignored.\n\nLastly, if Age is greater than 1 and has a fraction of 0.5, then age is an estimate rather than exact.  This feature will be caputred by a new boolean feature AgeIsExact.","3e3cf320":"# Modeling\n\nThe following code cells will proceed to build models which predict survival of passengers using machine learning.  In order to gage the performance of the models, K fold cross validation will be performed, holding 20% of the data set out.  After an acceptable model has been identified, it will be re-trained on the entire dataset to maximize performance.\n\nThe model will ultimately get scored with the accuracy metric in the Kaggle competion.  According to the table below for the training dataset, 61% of passengers will perish.  This means the classification problem has some skew, but not a lot.  It is therefore fair to score training models with accuracy.  However, since the test dataset may not contain the same skew as in the training set, a more sophisticated metric will be used to ultimately measure model performance.  This metric is ROC AUC, which seeks to minimize false psitives and false negatives equally.\n\nInitial runs of the decision tree classifier produced very unbalanced leaf nodes, therefore the Gini impurtiy parameter was replaced with entropy.","83e074e1":"The Pclass, Sex and Embarked features should be converted to Pandas Categorical dtype.  Converting Survived to a category would hinder aggregation calculations during EDA, therefore it will not be converted.","78fd6bbd":"Since the Passenger ID, Ticket and Cabin features are not intended for modeling liklihood of survival, these field will be dropped from the data frame.  Also the FirstName and LastName features will be dropped since language processing techniques will not be applied in the model.","87fe5ea0":"Lastly, the data needs to be transformed for machine learning.  The table printed above shows that only the Age and Embarked features have missing values.  This will be resolved by the common practice of imputing the median Age and the most frequent Embark location wherever missing values are encountered.  Categorical features will be transformed using OneHotEncoder.\n","a00fdcab":"This shows the Name strings follow a pattern: LastName, Title. FirstName.  In addition, some names have another name appended in parantheses, and some names apparently have a nickname appeneded in double quotes.  The first step to wrangle this data will be splitting the name feature into separate Last, Title & First features.","8e16e88a":"A few surpising results were produced above.  First, age has little correlation with survival rate.  Also, having increasingly more than 1 sibling has negative correlation with survival rate.  Perhaps this is an inidcator of class or wealth, or was caused by searching for family members thereby hurting survival rate.","f29c32e9":"The FirstName variable still needs cleaning (dropping the \"nickname\" substrings and splitting the (Associate Name) substring into a separate field.  But this information isn't expected to add value, therefore it will be ignored."}}