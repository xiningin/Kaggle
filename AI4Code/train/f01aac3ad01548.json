{"cell_type":{"ebdea8c3":"code","44bad805":"code","63a66bc3":"code","e94985b4":"code","c8cb40fa":"code","1b98f215":"code","30f6ad3c":"code","734cb874":"code","588ffe38":"code","45e3a67e":"code","dc429e90":"code","96b3dc73":"code","8ef63a7a":"code","bb7ef747":"code","a4c2b56d":"markdown","4ed42075":"markdown","46fadb4e":"markdown","0ae79c3f":"markdown","36c1b0e1":"markdown","bf091c25":"markdown","26afb240":"markdown","bd884c4d":"markdown","2cfb95f1":"markdown","28721319":"markdown","7e216d19":"markdown","ce49acd2":"markdown","70455f34":"markdown","64d8f640":"markdown"},"source":{"ebdea8c3":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import make_pipeline\nfrom imblearn.pipeline import make_pipeline as make_pipeline_imb\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.under_sampling import NearMiss\nfrom collections import Counter\nfrom sklearn import metrics\nfrom sklearn.feature_selection import VarianceThreshold\nimport os\n\n%matplotlib inline\nsns.set_style('whitegrid')","44bad805":"import os\nprint('Data File ==>\\t {}'.format(os.listdir(\"..\/input\")[0]))\ncreditcard = pd.read_csv(\"..\/input\/creditcard.csv\")","63a66bc3":"creditcard.head(3)","e94985b4":"# Missing values\nprint('No of Missing values :\\t{}'.format(creditcard.isnull().sum().max()))","c8cb40fa":"sns.countplot(data=creditcard,x = 'Class')\nplt.title('Class Variables distribution', fontsize=14)\nplt.show()\n\ncreditcard['Class'].value_counts() *100 \/len(creditcard)","1b98f215":"fig,ax = plt.subplots(nrows = 7, ncols=4, figsize=(12,21))\nrow = 0\ncol = 0\nfor i in range(len(creditcard.columns) -3):\n    if col > 3:\n        row += 1\n        col = 0\n    axes = ax[row,col]\n    sns.boxplot(x = creditcard['Class'], y = creditcard[creditcard.columns[i +1]],ax = axes)\n    col += 1\nplt.tight_layout()\nplt.show()","30f6ad3c":"creditcard.drop(['Time','Amount'],axis = 1,inplace=True)","734cb874":"X = creditcard.iloc[:,range(0,28)].values\ny = creditcard['Class'].values","588ffe38":"sel = VarianceThreshold(threshold=(.8 * (1 - .8)))\nX = sel.fit_transform(X)\nprint('Remaining Features Count:\\t{}'.format(X.shape[1]))","45e3a67e":"def print_result(title,actual, prediction,decision):\n    print('****************************************************')\n    print(title)\n    print('****************************************************')\n    print('Accuracy Score :\\t\\t{:.3}'.format(metrics.accuracy_score(actual, prediction)))\n    print('Recall Score :\\t\\t\\t{:.3}'.format(metrics.recall_score(actual, prediction)))\n    print('Average Precision Score :\\t{:.3}'.format(metrics.average_precision_score(actual, decision)))\n    print('ROC AUC Score :\\t\\t\\t{:.3}'.format(metrics.roc_auc_score(actual, decision)))\n    print()","dc429e90":"# split data in training set and test set\nX_train, X_test, y_train, y_test = train_test_split(\\\n    X,y,test_size=0.3, random_state = 0)","96b3dc73":"# Data Distribution \nprint('Normal Data Distribution {}'.format(Counter(creditcard['Class'])))\nC_VALUES = [0.001,0.01,0.1,1]\n# build normal Model\nfor c_value in C_VALUES:\n    pipeline = make_pipeline(LogisticRegression(random_state=42, C = c_value))\n    model = pipeline.fit(X_train,y_train)\n    prediction = model.predict(X_test)\n    decision = model.decision_function(X_test)\n    # print(metrics.confusion_matrix(y_test,prediction))\n    print_result('Normal Data Logistic -> C ={}'.\\\n                 format(c_value), y_test,prediction,decision)","8ef63a7a":"X_SMOTE,y_SMOTE = SMOTE().fit_sample(X,y)\nprint('SMOTE Data Distribution {}'.format(Counter(y_SMOTE)))\n\nC_VALUES = [0.001,0.01,0.1,1]\n# build normal Model\nfor c_value in C_VALUES:\n    smote_pipeline = make_pipeline_imb(SMOTE(random_state=42),\\\n                                       LogisticRegression(random_state=42, C = c_value))\n    smote_model = smote_pipeline.fit(X_train,y_train)\n    smote_prediction = smote_model.predict(X_test)\n    smote_decision = smote_model.decision_function(X_test)\n    # print(metrics.confusion_matrix(y_test,smote_prediction))\n    print_result('SMOTE - Oversampling data(Logistic) -> C ={}'.\\\n                 format(c_value), y_test,smote_prediction,smote_decision)","bb7ef747":"X_NearMiss,y_NearMiss = NearMiss().fit_sample(X,y)\nprint('NearMiss Data Distribution {}'.format(Counter(y_NearMiss)))\n# build moodel with  - undersampling\nC_VALUES = [0.001,0.01,0.1,1]\n# build normal Model\nfor c_value in C_VALUES:\n    nearmiss =  LogisticRegression(random_state=42,C = c_value)\n    nearmiss_model = nearmiss.fit(X_NearMiss,y_NearMiss)\n    nearmiss_decision = nearmiss_model.decision_function(X_test)\n    nearmiss_prediction = nearmiss_model.predict(X_test)\n    print_result('NearMiss - Undersampling data(Logistic) -> C ={}'.\\\n                 format(c_value), y_test,nearmiss_prediction,nearmiss_decision)","a4c2b56d":"**Conclussion**\n\nUndersampling improves recall score but it also reduces accuracy drastically, therefore more normal trasaction are calssified as fraudulent transaction. I, personally, would prefer oversampling over other three models where model predicts approximately 92% of fraudulent transactions with overall accuracy  of 97.7% ","4ed42075":"I am not intrested here to find the outliers, but there are few features those have big difference in mean and median values for fraudent and normal trasaction. I would prefer to remove the low variance variables along with Amount and Time for further analysis.","46fadb4e":"Imbalance class, only 0.172% fraud case data against 99.83% none fraud case data. There are a couple of mathods that cane implemented to handle the imbalanced data.\n\n* Up-sampling\n* Down-sampling\n\nhttps:\/\/en.wikipedia.org\/wiki\/Oversampling_and_undersampling_in_data_analysis","0ae79c3f":"As expected accuracy score is less as compares to normal data. Our purpose is to improve recall scrore. Oversampling with C value of 0.001 is providing recall score of 0.918 with accuracy of 0.977.\n\n**NearMiss Undersampling Model**","36c1b0e1":"As I doubted in the begining, model achieving almost 100% accuracy but recall score is not good enough. Best value for this model C = 1 which will able to catch only 62.6% fraud transaction. I will try upsampling and downsampling to improve the recall Score. ","bf091c25":"**Remove Low variance features**","26afb240":"## Read Dataset","bd884c4d":"## Exploratory Data Analysis","2cfb95f1":"The world of electronics payment enables the card holder to  shop online with virtual money in the pocket. The growth of the online industry also increases the risk of fraudent transaction.  Every credit card provider is working hard in this area to reduce the fraud transaction.\n\nIt is important for thecredit card companies to recognize the fraudulent credit card transactions so that customers are not charged for items that they did not purchase.\n\nProvided dataset is PCA transformation except Time and Amount features. Feature 'Class' is the response variable and it takes value 1 in case of fraud and 0 otherwise.\n\n**Inspiration**\nIdentify fraudulent credit card transactions.\n\nProvided data is imbalanced, Precision-Recall Curve (AUPRC), Accuracy may not work out well. To improving the fraud trasaction detection, recall rate need to improve without much caring on  Precision and F-1 score.","28721319":"**SMOTE Oversampling Model**","7e216d19":"**Logistic Regression without any change in data (Normal)**\n","ce49acd2":"**Remove Amount and Time**","70455f34":"**Training-Test dataset split**\n\nI will use 70% data to train model and rest 30% for validation purpose.","64d8f640":"Since data is imbalance, I would prefer under and oversampling along with normal data.\n\n**Model Metrics Method**"}}