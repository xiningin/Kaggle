{"cell_type":{"3a4e107c":"code","ff6dc865":"code","a89c5067":"code","4e48867e":"code","fa58fd37":"code","ce90c827":"code","ac8555b6":"code","a22c4d98":"code","e7c6b826":"code","d5bd4ba4":"code","a0ce3ed2":"code","a2aedfe5":"code","0eed0a8e":"code","119e5732":"code","fcdd0ee4":"code","f0281b8c":"code","81cc92bc":"code","96ad96a2":"code","3301e1a2":"code","0429f8d4":"code","ab7cd71c":"code","596d4da8":"code","a542c67e":"code","bdcb7858":"code","9071b2f9":"code","d7e7ab07":"code","01f69442":"code","a1845f4f":"code","83174256":"code","abdd28a2":"code","5e880da3":"code","3d11aca0":"code","6a6a138e":"code","fd69d289":"code","26c24460":"code","f6eb8625":"code","d376d098":"code","1ab22f6c":"code","7b22fb68":"code","0b223cb4":"code","8fb118b4":"code","3d7bd5e8":"code","d4e0feb0":"code","9708387f":"code","561d6d1d":"code","b1eba395":"code","7f78b199":"code","530fe315":"code","8b423e7d":"code","8f44bb4d":"code","99ba1173":"code","9a8a7125":"code","aa3a64e9":"code","aa1b7ae1":"markdown","4fbfc77b":"markdown","f6664a5d":"markdown","d452b5a6":"markdown","e1291105":"markdown","a0f86c01":"markdown","75c4f5c6":"markdown","5c10ae25":"markdown","0b710eaf":"markdown","a2095856":"markdown","cf8743b5":"markdown","7fa7f528":"markdown","173243c3":"markdown","ecc80523":"markdown","a21a6ed8":"markdown","a57dd13b":"markdown","e833516c":"markdown","e7693ba1":"markdown","9e678486":"markdown","bce2e02c":"markdown","ca0e076b":"markdown","3a1b20fb":"markdown","b5215e6a":"markdown","2ec5eb00":"markdown","7cba21dd":"markdown","426dfd34":"markdown","35e4ff1e":"markdown","fe3c67a7":"markdown"},"source":{"3a4e107c":"import pandas as pd\nimport numpy as np\nfrom sklearn import datasets\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport plotly.express as px\nimport seaborn as sns\n%matplotlib inline\n\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","ff6dc865":"df = pd.read_csv(\"..\/input\/breast-cancer-wisconsin-data\/data.csv\", index_col = 'id')\ndf.head()","a89c5067":"df.tail()","4e48867e":"df.info()","fa58fd37":"df.shape","ce90c827":"df.dtypes","ac8555b6":"df.diagnosis.unique()","a22c4d98":"df.isna().any()","e7c6b826":"df['Unnamed: 32'].isna().count()","d5bd4ba4":"df.describe()","a0ce3ed2":"df.describe(include=['object'])","a2aedfe5":"df['diagnosis'].value_counts()","0eed0a8e":"df = df.drop('Unnamed: 32', axis=1)","119e5732":"df['diagnosis'] = df['diagnosis'].apply(lambda x: 1 if x == 'M' else 0)","fcdd0ee4":"df.head()","f0281b8c":"sns.countplot(x='diagnosis', data=df, palette='pastel')\nplt.title('Breast Cancer Diagnosis')\nplt.grid(axis='y')","81cc92bc":"for feature in df.columns:\n    fig = px.histogram(df, x = feature)\n    fig.show()","96ad96a2":"corr = df.corr()\nplt.figure(figsize=(30,20));\nsns.heatmap(corr, annot=True, fmt='.2f');","3301e1a2":"CorField = []\nfor i in corr:\n    for j in corr.index[corr[i] > 0.75]:\n        if i != j and j not in CorField and i not in CorField:\n            CorField.append(j)\n            print (i, j, corr[i][corr.index == j].values[0])","0429f8d4":"threshold = 0.75\nfilter_features = np.abs(corr[\"diagnosis\"]) > threshold\ncorr_features = corr.columns[filter_features].tolist()\nsns.heatmap(df[corr_features].corr(),annot=True,fmt=\".2f\");\nplt.title(\"Correlation Between Features w 0.75 Threshold\");\nplt.show();","ab7cd71c":"sns.pairplot(df[corr_features], hue=\"diagnosis\")\nplt.show();","596d4da8":"plt.figure(figsize=(20,35))\nplotnumber =1\nfor column in df.columns[1:]:\n    ax = plt.subplot(10,3,plotnumber)\n    sns.boxplot(data = df, x = column, palette='pastel')\n    plt.xlabel(column)\n    plotnumber+=1\nplt.show()","a542c67e":"mean_list =[]\nse_list =[]\nworst_list =[]\nfor i in df.columns:\n    f_list = i.split('_')\n    if f_list[-1] == 'mean':\n        mean_list.append(i)\n    elif f_list[-1] == 'se':\n        se_list.append(i)\n    elif f_list[-1] == 'worst':\n        worst_list.append(i)    ","bdcb7858":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nfeat_scaled = pd.DataFrame(scaler.fit_transform(df[mean_list]),columns=mean_list, index = df.index)\ndata = pd.concat([df['diagnosis'],feat_scaled],axis=1)\ndf_melt = pd.melt(frame=data, value_vars=mean_list, id_vars=['diagnosis'])\nfig, ax = plt.subplots(1, 1, figsize = (20, 8), dpi=300)\nsns.violinplot(x=\"variable\",y=\"value\",hue = \"diagnosis\",data=df_melt,split = True, inner=\"quart\",palette='pastel')","9071b2f9":"feat_scaled = pd.DataFrame(scaler.fit_transform(df[se_list]),columns=se_list, index = df.index)\ndata = pd.concat([df['diagnosis'],feat_scaled],axis=1)\ndf_melt = pd.melt(frame=data, value_vars=se_list, id_vars=['diagnosis'])\nfig, ax = plt.subplots(1, 1, figsize = (20, 8), dpi=300)\nsns.violinplot(x=\"variable\",y=\"value\",hue = \"diagnosis\",data=df_melt,split = True,  inner=\"quart\", palette='pastel')","d7e7ab07":"feat_scaled = pd.DataFrame(scaler.fit_transform(df[worst_list]),columns=worst_list, index = df.index)\ndata = pd.concat([df['diagnosis'],feat_scaled],axis=1)\ndf_melt = pd.melt(frame=data, value_vars=worst_list, id_vars=['diagnosis'])\nfig, ax = plt.subplots(1, 1, figsize = (20, 8), dpi=300)\nsns.violinplot(x=\"variable\",y=\"value\",hue = \"diagnosis\",data=df_melt,split = True,  inner=\"quart\", palette='pastel')","01f69442":"from sklearn.model_selection import train_test_split","a1845f4f":"X_train, X_test, y_train, y_test = train_test_split(\n    df.drop(['diagnosis'], axis=1), df['diagnosis'], test_size=0.3, random_state=42, stratify=df['diagnosis']\n)","83174256":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\n\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)","abdd28a2":"from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nimport sklearn.metrics as metrics\nfrom sklearn.model_selection import cross_val_score","5e880da3":"knn = KNeighborsClassifier()\nknn.fit(X_train_scaled, y_train)\ny_pred = knn.predict(X_test_scaled)\nprint('Accuracy:', accuracy_score(y_test, y_pred))\nprint('F1 score:', f1_score(y_test, y_pred))\nprint('Recall:', recall_score(y_test, y_pred))\nprint('Precision:', precision_score(y_test, y_pred))","3d11aca0":"cm = confusion_matrix(y_test, y_pred)\nsns.heatmap(cm, annot=True, fmt='g')\nplt.xlabel('Predicted')\nplt.ylabel('True Value')\nplt.show()","6a6a138e":"probs = knn.predict_proba(X_test_scaled)\npreds = probs[:,1]\n\nfpr, tpr, threshold = metrics.roc_curve(y_test, preds)\nroc_auc = metrics.auc(fpr, tpr)\n\nplt.title('Receiver Operating Characteristic')\nplt.plot(fpr, tpr, 'b', label = 'AUC = %0.4f' % roc_auc)\nplt.legend(loc = 'lower right')\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0, 1])\nplt.ylim([0, 1])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show()","fd69d289":"%%time\nfrom sklearn.model_selection import GridSearchCV\n\nparam_grid = {\n        'n_neighbors': range(1, 100),\n        'p': range(1, 10)\n}\n\nknn = KNeighborsClassifier()\n\ngrid_search = GridSearchCV(knn, param_grid, cv=5, scoring='f1')\ngrid_search.fit(X_train_scaled, y_train)\n\nprint(\"Best CV score: {:.3f}, best CV n_neighbors: {}, best CV p: {}\".format(\n    grid_search.best_score_, grid_search.best_estimator_.n_neighbors, grid_search.best_estimator_.p)\n) \n\n\ntest_predictions = grid_search.best_estimator_.predict(X_test_scaled)\nprint(\"Resulting test score: {:.3f}\".format(f1_score(test_predictions, y_test)))","26c24460":"knn = KNeighborsClassifier(n_neighbors=5, p=1)\nknn.fit(X_train_scaled, y_train)\ny_pred = knn.predict(X_test_scaled)\nprint('Accuracy:', accuracy_score(y_test, y_pred))\nprint('F1 score:', f1_score(y_test, y_pred))\nprint('Recall:', recall_score(y_test, y_pred))\nprint('Precision:', precision_score(y_test, y_pred))","f6eb8625":"cm = confusion_matrix(y_test, y_pred)\nsns.heatmap(cm, annot=True, fmt='g')\nplt.xlabel('Predicted')\nplt.ylabel('True Value')\nplt.show()","d376d098":"probs = knn.predict_proba(X_test_scaled)\npreds = probs[:,1]\n\nfpr, tpr, threshold = metrics.roc_curve(y_test, preds)\nroc_auc = metrics.auc(fpr, tpr)\n\nplt.title('Receiver Operating Characteristic')\nplt.plot(fpr, tpr, 'b', label = 'AUC = %0.4f' % roc_auc)\nplt.legend(loc = 'lower right')\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0, 1])\nplt.ylim([0, 1])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show()","1ab22f6c":"from sklearn.linear_model import LogisticRegression","7b22fb68":"param_grid = {\n        'C':range(1, 200),\n}\n\nclf_lr = LogisticRegression()\n\ngrid_search = GridSearchCV(clf_lr, param_grid, cv=5, scoring='f1')\ngrid_search.fit(X_train_scaled, y_train)\n\nprint(\"Best CV score: {:.3f}, best CV C: {}\".format(\n    grid_search.best_score_, grid_search.best_estimator_.C)\n) \n\n\ntest_predictions = grid_search.best_estimator_.predict(X_test_scaled)\nprint(\"Resulting test score: {:.3f}\".format(f1_score(test_predictions, y_test)))","0b223cb4":"clf_lr = LogisticRegression(C=2, random_state=42, max_iter = 1000)\nclf_lr.fit(X_train_scaled, y_train)\n\ny_predicted_lr = clf_lr.predict(X_test_scaled)\nprint(classification_report(y_test, y_predicted_lr, zero_division = 0))","8fb118b4":"cm = confusion_matrix(y_test, y_predicted_lr)\nsns.heatmap(cm, annot=True,fmt='g')\nplt.xlabel('Predicted')\nplt.ylabel('True Value')\nplt.show()","3d7bd5e8":"probs = clf_lr.predict_proba(X_test_scaled)\npreds = probs[:,1]\n\nfpr, tpr, threshold = metrics.roc_curve(y_test, preds)\nroc_auc = metrics.auc(fpr, tpr)\n\nplt.title('Receiver Operating Characteristic')\nplt.plot(fpr, tpr, 'b', label = 'AUC = %0.4f' % roc_auc)\nplt.legend(loc = 'lower right')\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0, 1])\nplt.ylim([0, 1])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show()","d4e0feb0":"corr_features = []\nfor i in corr:\n    for j in corr.index[corr[i] > 0.85]:\n        if i != j and j not in corr_features and i not in corr_features:\n            corr_features.append(j)\ncorr_features","9708387f":"X_train_log = X_train.drop(corr_features, axis=1)\nX_test_log = X_test.drop(corr_features, axis=1)","561d6d1d":"param_grid = {\n        'C':range(1, 200),\n}\n\nclf_lr = LogisticRegression()\n\ngrid_search = GridSearchCV(clf_lr, param_grid, cv=5, scoring='f1')\ngrid_search.fit(X_train_log, y_train)\n\nprint(\"Best CV score: {:.3f}, best CV C: {}\".format(\n    grid_search.best_score_, grid_search.best_estimator_.C)\n) \n\n\ntest_predictions = grid_search.best_estimator_.predict(X_test_log)\nprint(\"Resulting test score: {:.3f}\".format(f1_score(test_predictions, y_test)))","b1eba395":"clf_lr = LogisticRegression(C=159, random_state=42, max_iter = 1000)\nclf_lr.fit(X_train_log, y_train)\n\ny_predicted_lr = clf_lr.predict(X_test_log)\nprint(classification_report(y_test, y_predicted_lr, zero_division = 0))","7f78b199":"cm = confusion_matrix(y_test, y_predicted_lr)\nsns.heatmap(cm, annot=True,fmt='g')\nplt.xlabel('Predicted')\nplt.ylabel('True Value')\nplt.show()","530fe315":"probs = clf_lr.predict_proba(X_test_log)\npreds = probs[:,1]\n\nfpr, tpr, threshold = metrics.roc_curve(y_test, preds)\nroc_auc = metrics.auc(fpr, tpr)\n\nplt.title('Receiver Operating Characteristic')\nplt.plot(fpr, tpr, 'b', label = 'AUC = %0.4f' % roc_auc)\nplt.legend(loc = 'lower right')\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0, 1])\nplt.ylim([0, 1])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show()","8b423e7d":"scaler = StandardScaler()\n\nX_train_log_scaled = scaler.fit_transform(X_train_log)\nX_test_log_scaled = scaler.transform(X_test_log)","8f44bb4d":"param_grid = {\n        'C':range(1, 200),\n}\n\nclf_lr = LogisticRegression()\n\ngrid_search = GridSearchCV(clf_lr, param_grid, cv=5, scoring='f1')\ngrid_search.fit(X_train_log_scaled, y_train)\n\nprint(\"Best CV score: {:.3f}, best CV C: {}\".format(\n    grid_search.best_score_, grid_search.best_estimator_.C)\n) \n\n\ntest_predictions = grid_search.best_estimator_.predict(X_test_log_scaled)\nprint(\"Resulting test score: {:.3f}\".format(f1_score(test_predictions, y_test)))","99ba1173":"clf_lr = LogisticRegression(C=1, random_state=42, max_iter = 1000)\nclf_lr.fit(X_train_log_scaled, y_train)\n\ny_predicted_lr = clf_lr.predict(X_test_log_scaled)\nprint(classification_report(y_test, y_predicted_lr, zero_division = 0))","9a8a7125":"cm = confusion_matrix(y_test, y_predicted_lr)\nsns.heatmap(cm, annot=True,fmt='g')\nplt.xlabel('Predicted')\nplt.ylabel('True Value')\nplt.show()","aa3a64e9":"probs = clf_lr.predict_proba(X_test_log_scaled)\npreds = probs[:,1]\n\nfpr, tpr, threshold = metrics.roc_curve(y_test, preds)\nroc_auc = metrics.auc(fpr, tpr)\n\nplt.title('Receiver Operating Characteristic')\nplt.plot(fpr, tpr, 'b', label = 'AUC = %0.4f' % roc_auc)\nplt.legend(loc = 'lower right')\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0, 1])\nplt.ylim([0, 1])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show()","aa1b7ae1":"Spliting data(mean, standard error and \"worst\")","4fbfc77b":"**Droping highly correlated features**","f6664a5d":"**Features histograms**","d452b5a6":"## Data Preparation","e1291105":"#### KNeighborsClassifier","a0f86c01":"## Conclusion","75c4f5c6":"#### Grid-Search","5c10ae25":"**Without highly correlated features and with Standardization**","0b710eaf":"Find Missing Values (NaNs):","a2095856":"* There is one categorical and 32 numeric columns;\n* Column `diagnosis` is categorical. It's the target column and has class labels M and B (M = malignant, B = benign).","cf8743b5":"Finding highly correlated features","7fa7f528":"## Data Visualization","173243c3":"**Boxplots**","ecc80523":"**Pairplots of highly correlated features**","a21a6ed8":"## Exploratory Data Analysis","a57dd13b":"**Correlation matrix**","e833516c":"## Preparing Data Set","e7693ba1":"**Task**|","9e678486":"## Data Overview","bce2e02c":"Model with the best score is LogisticRegression  \nAchieved roc-auc: 0.99","ca0e076b":"#### LogisticRegression","3a1b20fb":"# Breast Cancer Prediction","b5215e6a":"`Unnamed: 32` column has only null values. So it's useless column. It should be dropped.","2ec5eb00":"Predict `diagnosis` (M = malignant, B = benign)","7cba21dd":"We can see that thees features are linearly dependent","426dfd34":"There are a lot of outliers in our data","35e4ff1e":"## Predictive Model ","fe3c67a7":"**Violinplots**"}}