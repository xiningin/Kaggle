{"cell_type":{"9d7eea35":"code","cb73664c":"code","483667de":"code","c0e6bb07":"code","a7486b9d":"code","9d1949c0":"code","1e39e990":"code","9d41bcdc":"code","366e4962":"code","38379ea3":"code","ebb3304b":"code","9bd2f3cd":"code","d8cbc47b":"code","eecca982":"code","65308343":"code","d47b60cc":"code","4fb5df2f":"code","7221c952":"code","21ff7216":"code","1f90e4fa":"code","9e9fb882":"code","7b780611":"code","2ec7854b":"code","76cba0d5":"code","5881964a":"code","6ef9824d":"code","9cf5da9e":"code","c4099ae7":"code","b5d4ef12":"code","83e62121":"code","fc9df406":"code","d821be6a":"code","05dc96a7":"code","999a0429":"code","01b2d2c4":"code","4bbdfa04":"code","4cd24d59":"code","ef206c90":"markdown","ee16eb70":"markdown","4dc7eaad":"markdown","3ac636da":"markdown","a4821d67":"markdown","5c42ce66":"markdown"},"source":{"9d7eea35":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","cb73664c":"!apt-get install zip","483667de":"train_csv_data = '..\/input\/quora-insincere-questions-classification\/train.csv'\ntest_csv_data = '..\/input\/quora-insincere-questions-classification\/test.csv'","c0e6bb07":"#import the libraries\nimport matplotlib.pyplot as plt\nimport string\nimport nltk\nimport re\nimport seaborn as sns\nfrom unidecode import unidecode\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer, WordNetLemmatizer\nfrom sklearn.utils import resample","a7486b9d":"\nprint(os.getcwd())","9d1949c0":"#use pandas to get data from csv\ndf_train = pd.read_csv(train_csv_data)\n\ndf_train.head(5)\n","1e39e990":"df_test = pd.read_csv(test_csv_data)\ndf_test.head(5)","9d41bcdc":"#drop NA data\ndf_train.dropna(inplace=True)","366e4962":"#get number of labels'values\ndf_train['target'].value_counts()","38379ea3":"#insincere which target is 1 and sincere is 0\ninsincere_data = df_train[df_train['target'] == 1]\nsincere_data = df_train[df_train['target'] == 0]\n","ebb3304b":"#get the percentage of sincere and insincere\n\ny = df_train['target']\ny.value_counts().plot(kind='bar', rot=0)","9bd2f3cd":"from sklearn.utils import resample\n\n#under sampling the data\nsincere = df_train[df_train.target == 0]\ninsincere = df_train[df_train.target == 1]\ndf_train_sampled = pd.concat([resample(sincere, replace = True, n_samples = len(insincere)*4), insincere])\ndf_train_sampled","d8cbc47b":"#data after under sampling\ny = df_train_sampled['target']\ny.value_counts().plot(kind='bar', rot=0)","eecca982":"# get stopwords, punkt, wordnet of english from nltk\nnltk.download('stopwords')\nnltk.download('punkt')\nnltk.download('wordnet')\nstop_words = set(stopwords.words('english'))","65308343":"#define badwords\nbad_words = \"2 girls 1 cup, 2g1c, 4r5e, 5h1t, 5hit, a$$\"\nbad_words = [x.strip() for x in bad_words.split(\",\")]","d47b60cc":"#define acronyms\nACRONYMS = {\n    \"aren't\" : \"are not\", \"can't\" : \"cannot\", \"cant\": \"cannot\", \"couldn't\" : \"could not\", \"didn't\" : \"did not\", \"doesn't\" : \"does not\", \"don't\" : \"do not\", \"hadn't\" : \"had not\", \"hasn't\" : \"has not\", \"haven't\" : \"have not\", \n    \"he'd\" : \"he would\", \"he'll\" : \"he will\", \"he's\" : \"he is\", \n    \"i'd\" : \"I would\", \"i'd\" : \"I had\", \"i'll\" : \"I will\", \"i'm\" : \"I am\", \"isn't\" : \"is not\", \n    \"it's\" : \"it is\", \"it'll\":\"it will\", \"i've\" : \"I have\", \"let's\" : \"let us\", \n    \"mightn't\" : \"might not\", \"mustn't\" : \"must not\", \"shan't\" : \"shall not\", \n    \"she'd\" : \"she would\", \"she'll\" : \"she will\", \"she's\" : \"she is\", \n    \"shouldn't\" : \"should not\", \"that's\" : \"that is\", \"there's\" : \"there is\", \n    \"they'd\" : \"they would\", \"they'll\" : \"they will\", \"they're\" : \"they are\", \"they've\" : \"they have\", \"we'd\" : \"we would\", \"we're\" : \"we are\", \"weren't\" : \"were not\", \"we've\" : \"we have\", \n    \"what'll\" : \"what will\", \"what're\" : \"what are\", \"what's\" : \"what is\", \"what've\" : \"what have\",\n    \"where's\" : \"where is\", \"who'd\" : \"who would\", \"who'll\" : \"who will\", \"who're\" : \"who are\", \"who's\" : \"who is\", \"who've\" : \"who have\",\n    \"won't\" : \"will not\", \"wouldn't\" : \"would not\", \n    \"you'd\" : \"you would\", \"you'll\" : \"you will\", \"you're\" : \"you are\", \"you've\" : \"you have\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\", \"y'all've\": \"you all have\", \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll've\": \"you will have\",\n    \"'re\": \" are\", \"wasn't\": \"was not\", \"we'll\":\" will\", \"didn't\": \"did not\", \"tryin'\":\"trying\"\n}","4fb5df2f":"'''\nTo correct acronym or mispell word on text\n'''\ndef correct_acronym(text):\n    tokens = word_tokenize(text)\n    tokens = [ACRONYMS.get(token) if (ACRONYMS.get(token) != None) else token for token in tokens]\n    text = \" \".join(tokens)\n\n    \n'''\nRemove stopwords which appear from nltk stopwords\n'''\ndef remove_stopword(text):\n    tokens = word_tokenize(text)\n\n    tokens_without_sw = [word for word in tokens if not word in stop_words]\n    text = (' ').join(tokens_without_sw)","7221c952":"'''\npreprocess data with nomalize text, remove url, puntk, emal, sign, number\n'''\ndef preprocess(text):\n    text = unidecode(text).encode(\"ascii\")\n    text = str(text, \"ascii\")\n    \n    #remove bad word\n    for word in bad_words:\n        text = text.replace(word, \"BAD WORDS\")\n        \n    text = text.lower() #normalize\n    text = re.sub('https?:\/\/\\S+|www\\.\\S+', ' ', text) #remove url\n    text = re.sub('<.*?>+', '', text) #remove special character\n    text = re.sub('\\S+@\\S+', ' ', text) #remove email\n    text = re.sub('[%s]' % re.escape(string.punctuation), '', text) #remove sign\n    text = re.sub('(.)\\1+', '\\1', text)\n    text = re.sub('\\d+', ' ', text) #remove number\n    \n    tokens = word_tokenize(text)\n    tokens = [ACRONYMS.get(token) if (ACRONYMS.get(token) != None) else token for token in tokens]\n    text = \" \".join(tokens)\n    \n\n    tokens_without_sw = [word for word in tokens if not word in stop_words]\n    text = (' ').join(tokens_without_sw)\n    return text","21ff7216":"#create new column of question_text column after preprocess\ndf_train_sampled['question_text_preprocess'] = df_train_sampled['question_text'].apply(preprocess)\ndf_train_sampled.head(5)","1f90e4fa":"from wordcloud import WordCloud","9e9fb882":"#using wordcloud to know the words which have most prequence or importance of sincere\nsincere = sincere_wordcloud = WordCloud(width=800, height=600, background_color='white', min_font_size=10).generate(str(df_train_sampled[df_train_sampled[\"target\"] == 0][\"question_text_preprocess\"]))\nplt.figure(figsize=(8, 8))\nplt.imshow(sincere_wordcloud)\nplt.axis(\"off\")\nplt.tight_layout(pad=0)\nplt.show()","7b780611":"#using wordcloud to know the words which have most prequence or importance of insincere\nsincere = sincere_wordcloud = WordCloud(width=800, height=600, background_color='white', min_font_size=10).generate(str(df_train_sampled[df_train_sampled[\"target\"] == 1][\"question_text_preprocess\"]))\nplt.figure(figsize=(8, 8))\nplt.imshow(sincere_wordcloud)\nplt.axis(\"off\")\nplt.tight_layout(pad=0)\nplt.show()","2ec7854b":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom time import time\n\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import precision_score, recall_score, f1_score\n","76cba0d5":"#caculate the weight of words\ncvec = CountVectorizer()\ncvec.fit(df_train_sampled.question_text)\n\n#transform train data to weight matrix\nsin_doc_matrix = cvec.transform(df_train_sampled[df_train_sampled.target == 0].question_text_preprocess)\ninsin_doc_matrix = cvec.transform(df_train_sampled[df_train_sampled.target == 1].question_text_preprocess)\n\nsin_tf = np.sum(sin_doc_matrix,axis=0)\ninsin_tf = np.sum(insin_doc_matrix,axis=0)\n\nsin = np.squeeze(np.asarray(sin_tf))\ninsin = np.squeeze(np.asarray(insin_tf))\n\nterm_freq_df = pd.DataFrame([sin, insin],\n                            columns=cvec.get_feature_names()).transpose()","5881964a":"term_freq_df.columns = ['sincere', 'insincere']\nterm_freq_df['total'] = term_freq_df['sincere'] + term_freq_df['insincere']\n\n#the most words appear in both label\nterm_freq_df.sort_values(by='total', ascending=False).iloc[:10]","6ef9824d":"#show the top 10 insincere\ny_pos = np.arange(10)\nplt.figure(figsize=(20,10))\nplt.bar(y_pos, term_freq_df.sort_values(by='insincere', ascending=False)['insincere'][:10], align='center', alpha=0.5)\nplt.xticks(y_pos, term_freq_df.sort_values(by='sincere', ascending=False)['sincere'][:10].index,rotation='vertical')\nplt.ylabel('Frequency')\nplt.xlabel('Top 10 insincere tokens')\nplt.title('Top 10 tokens in insincere tweets')","9cf5da9e":"#show the top 10 sincere\ny_pos = np.arange(10)\nplt.figure(figsize=(20,10))\nplt.bar(y_pos, term_freq_df.sort_values(by='sincere', ascending=False)['insincere'][:10], align='center', alpha=0.5)\nplt.xticks(y_pos, term_freq_df.sort_values(by='sincere', ascending=False)['insincere'][:10].index,rotation='vertical')\nplt.ylabel('Frequency')\nplt.xlabel('Top 10 sincere tokens')\nplt.title('Top 10 tokens in sincere tweets')","c4099ae7":"#the prequent appearance of both insincere and sincere\nplt.figure(figsize=(8,6))\nax = sns.regplot(x=\"sincere\", y=\"insincere\",fit_reg=False, scatter_kws={'alpha':0.5},data=term_freq_df)\nplt.ylabel('Insincere Frequency')\nplt.xlabel('Sincere Frequency')\nplt.title('Sincere Frequency vs Insincere Frequency')","b5d4ef12":"from sklearn.preprocessing import LabelBinarizer\nlabel_target = LabelBinarizer(sparse_output=True)\ntrain_target= label_target.fit_transform(df_train_sampled['target'])\n\ntrain_target.shape","83e62121":"#devide train data to 3 part: train, validation and test\n\nx = df_train_sampled.question_text_preprocess\ny = df_train_sampled.target\nfrom sklearn.model_selection import train_test_split\nSEED = 2000\nx_train, x_validation_and_test, y_train, y_validation_and_test = train_test_split(x, y, test_size=.02, random_state=SEED)\nx_validation, x_test, y_validation, y_test = train_test_split(x_validation_and_test, y_validation_and_test, test_size=.5, random_state=SEED)\nprint(\"Train set has total {0} entries with \\n {1:.2f}% sincere, {2:.2f}% insincere\".format(\n    len(x_train), \n    (len(x_train[y_train == 0]) \/ (len(x_train)*1.))*100, \n    (len(x_train[y_train == 1]) \/ (len(x_train)*1.))*100))\nprint(\"Validation set has total {0} entries with \\n {1:.2f}% sincere, {2:.2f}% insincere\".format(\n    len(x_validation), \n    (len(x_validation[y_validation == 0]) \/ (len(x_validation)*1.))*100, \n    (len(x_validation[y_validation == 1]) \/ (len(x_validation)*1.))*100))\nprint(\"Test set has total {0} entries with \\n {1:.2f}% sincere, {2:.2f}% insincere\".format(\n    len(x_test),\n    (len(x_test[y_test == 0]) \/ (len(x_test)*1.))*100,\n    (len(x_test[y_test == 1]) \/ (len(x_test)*1.))*100))","fc9df406":"#but in here I just train with train and test with validation and test\ncvec.fit(x_train)\n\nvt_x_train = cvec.transform(x_train)\nvt_x_test = cvec.transform(x_validation_and_test)\n\n","d821be6a":"#train model\ncount_vectorizer = LogisticRegression(n_jobs=10, solver='saga', C=0.1, verbose=1)\n\ncount_vectorizer.fit(vt_x_train, y_train)\n\ny_prediction_count_vectorizer = count_vectorizer.predict(vt_x_test)","05dc96a7":"from sklearn.metrics import classification_report","999a0429":"#the results\nprint(\"results\\n\")\nprint(classification_report(y_validation_and_test, y_prediction_count_vectorizer))","01b2d2c4":"#prediction with test data\ndf_test['clean_questions'] = df_test['question_text'].apply(preprocess)\ntest_vt_x = cvec.transform(df_test['clean_questions'])\npredictions_test_data = count_vectorizer.predict(test_vt_x)","4bbdfa04":"#submission\ndf_test['prediction'] = predictions_test_data\nsubmissions = df_test[['qid', 'prediction']]\nsubmissions","4cd24d59":"submissions.to_csv('submission.csv', index=False)","ef206c90":"# Vectorize data","ee16eb70":"# Train model","4dc7eaad":"# Preprocess data","3ac636da":"# Preparation","a4821d67":"# Results and submission","5c42ce66":"# Prediction"}}