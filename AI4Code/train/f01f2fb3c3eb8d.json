{"cell_type":{"53eb73c0":"code","fe6e6374":"code","c72feb08":"code","2ef1d3fd":"code","b9ed8a83":"code","f0276e75":"code","89ac6f83":"markdown","0bf58291":"markdown","cdf78be8":"markdown","aeab6a1a":"markdown","d26f6f28":"markdown"},"source":{"53eb73c0":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","fe6e6374":"import warnings\nwarnings.filterwarnings(\"ignore\")\n\nfrom sklearn.preprocessing import LabelEncoder\n\ndf = pd.read_csv('\/kaggle\/input\/mushroom-classification\/mushrooms.csv')\ndf.info()\ncolumns = df.columns\n\nfor col in columns:\n    label = LabelEncoder()\n    values = df[col].unique()\n    col_enc = label.fit(values)\n    df[col] = col_enc.transform(df[col])","c72feb08":"from sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, classification_report\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.tree import DecisionTreeClassifier\n\nx_train, x_test, y_train, y_test = train_test_split(df.drop(['class'], axis=1), df['class'], train_size = 0.7, random_state=42)\ndt_fit = DecisionTreeClassifier(criterion = 'gini', max_depth = 5, min_samples_split =2, min_samples_leaf = 1, random_state = 42)\n\nbag_fit = BaggingClassifier(base_estimator = dt_fit, n_estimators = 5000, max_samples = 0.67, max_features = 1,bootstrap = True, bootstrap_features = False, n_jobs = -1, random_state=42)\nbag_fit.fit(x_train, y_train)\n\ncrosstab_train = pd.crosstab(y_train, bag_fit.predict(x_train), rownames = ['Actual'], colnames = ['Predicted'])\naccuracy_train = round(accuracy_score(y_train, bag_fit.predict(x_train)),3)\nclassification_train = classification_report(y_train, bag_fit.predict(x_train))\n\ncrosstab_test = pd.crosstab(y_test, bag_fit.predict(x_test), rownames = ['Actual'], colnames = ['Predicted'])\naccuracy_test = round(accuracy_score(y_test, bag_fit.predict(x_test)),3)\nclassification_test = classification_report(y_test, bag_fit.predict(x_test))\n\nprint('Accuracy Train:', accuracy_train)\nprint('Accuracy Test:', accuracy_test)","2ef1d3fd":"dummy_array = np.empty((6,10))\ndt_wttune = pd.DataFrame(dummy_array)\n\ndt_wttune.columns = ['zero_wght','one_wght','tr_accuracy','tst_accuracy','prec_zero','prec_one','prec_ovll','recl_zero','recl_one','recl_ovll']\nzero_clwghts = [0.01,0.1,0.2,0.3,0.4,0.5]\n\nfor i in range(len(zero_clwghts)):\n    clwght = {0:zero_clwghts[i],1:1.0-zero_clwghts[i]}\n    dt_fit_w = DecisionTreeClassifier(criterion = 'gini', max_depth=5, min_samples_split=2, min_samples_leaf=1, random_state=42, class_weight=clwght)\n    dt_fit_w.fit(x_train, y_train)\n    dt_wttune.loc[i,'zero_wght'] = clwght[0]\n    dt_wttune.loc[i,'one_wght'] = clwght[1]\n    dt_wttune.loc[i,'tr_accuracy'] = round(accuracy_score(y_train, dt_fit_w.predict(x_train)),3)\n    clf_sp = classification_report(y_test, dt_fit_w.predict(x_test)).split()\n    dt_wttune.loc[i,'prec_zero'] = clf_sp[5]\n    dt_wttune.loc[i,'prec_one'] = clf_sp[10]\n    dt_wttune.loc[i,'prec_ovll'] = clf_sp[25]\n    dt_wttune.loc[i,'recl_zero'] = clf_sp[6]\n    dt_wttune.loc[i,'recl_one'] = clf_sp[12]\n    dt_wttune.loc[i,'recl_ovll'] = clf_sp[26]\n    print('\\n Class Weights',clwght, 'Train Accuracy:',round(accuracy_score(y_train, dt_fit_w.predict(x_train),3)), 'Test Accuracy:',round(accuracy_score(y_test, dt_fit_w.predict(x_test))),3)\n    print('Test Confusion Matrix \\n\\n', pd.crosstab(y_test, dt_fit_w.predict(x_test), rownames = ['Actual'], colnames = ['Predicted']))","b9ed8a83":"dt_fit = DecisionTreeClassifier(criterion = 'gini', max_depth=5, min_samples_split=2,min_samples_leaf=1, random_state=42, class_weight = {0: 0.3, 1: 0.7})\ndt_fit.fit(x_train, y_train)\n\ncrosstab_train = pd.crosstab(y_train, dt_fit.predict(x_train), rownames = ['Actual'], colnames = ['Predicted'])\naccuracy_train = round(accuracy_score(y_train, dt_fit.predict(x_train)),3)\nclassification_train = classification_report(y_train, dt_fit.predict(x_train))\n\ncrosstab_test = pd.crosstab(y_test, dt_fit.predict(x_test), rownames = ['Actual'], colnames = ['Predicted'])\naccuracy_test = round(accuracy_score(y_test, dt_fit.predict(x_test)),3)\nclassification_test = classification_report(y_test, dt_fit.predict(x_test))\n\nprint('Accuracy Train:', accuracy_train)\nprint('Classification Train \\n\\n', classification_train)\nprint('Confusion Matrix Train \\n\\n', crosstab_train)\n\nprint('\\n\\n Accuracy Test:', accuracy_test)\nprint('Classification Test \\n\\n', classification_test)\nprint('Confusion Matrix Test \\n\\n', crosstab_test)","f0276e75":"import matplotlib.pyplot as plt\nmodel_ranks = pd.Series(dt_fit.feature_importances_, index=x_train.columns, name='Importance').sort_values(ascending=False, inplace=False)\nmodel_ranks.index.name = 'Variables'\ntop_features = model_ranks.iloc[:5].sort_values(ascending=True, inplace=False)\nplt.figure(figsize=(20,10))\nax = top_features.plot(kind='barh')\nax.set_title('Variable Importance Plot')\nax.set_xlabel('Mean Decrease in Variance')\nax.set_yticklabels(top_features.index, fontsize=13)\n\nprint(top_features.sort_values(ascending=False))","89ac6f83":"Tuning the class weights of the Decision Tree to optimise accuracy shows the class weights of {0: 0.3, 1: 0.7} are optimal.","0bf58291":"Decision Tree Classifier with optimal class weights results in:\n* Training Accuracy: 0.993\n* Testing Accuracy: 0.992\n\nConfusion Matrix and Classification Report can also be viewed below for both training and testing data.","cdf78be8":"For this prediction I chose to use both the DecisionTreeClassifier and BaggingClassifier. I had not yet implemented a Bagging Classifier and so I thought this would be a good opportunity to do so.\n\nThe results for the Bagging Classifier using a base estimator of the DecisionTreeClassifier are:\n\n* Training Accuracy: 0.902\n* Testing Accuracy: 0.895","aeab6a1a":"Preliminary Investigation into the data shows no null values and all columns are discrete which will have to be encoded to integer values.\n\nIn my previous notebook I encoded column by column, however as an improvement this time around I have put it inside a *for* loop to prevent repeating code.","d26f6f28":"The top 3 variables from the Variable Importance Plot are:\n* odor - 0.50\n* gill-color - 0.30\n* population - 0.12"}}