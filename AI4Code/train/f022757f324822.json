{"cell_type":{"4101cd97":"code","2620b4e7":"code","1c21b066":"code","a5548d58":"code","122595e0":"code","abdd548a":"code","d9925c3d":"code","16f9ab36":"code","8b898b80":"code","6d42bf7d":"code","ef8ca8f0":"code","1e9b5b5a":"code","67a8bd0e":"code","b35e29c1":"code","835b70cc":"code","311dbb20":"code","9990f6a1":"code","b7584443":"code","36132684":"code","b7be5740":"code","8ab4edb5":"code","f57e52cb":"code","5ab18ee8":"code","4b9b7616":"code","5c7b2124":"code","53ec2186":"code","cebe6217":"code","ec0aded8":"code","58d4a149":"code","d45af701":"code","cfa730b9":"code","4fdbbd3f":"code","ee14f86f":"code","9f4f79c4":"code","cd31b461":"code","2b52344d":"code","0f0d9868":"code","f066d981":"code","732141d2":"code","bfaa0aa3":"code","27fafb9a":"code","1aad97f9":"code","99024954":"code","d9b7faf1":"code","b1eb4a12":"code","fab5c3dc":"code","39e37349":"code","c1a9c216":"code","6801bc8a":"code","325597e2":"code","420552c6":"code","dd34ffe1":"code","eb7c0413":"code","85e66e12":"code","3aedaf7d":"code","354cccd9":"markdown","7d2f55b0":"markdown","23786cd6":"markdown","ece99ca5":"markdown","2765b3bf":"markdown","c4e7bd2a":"markdown","26a175f1":"markdown","d8f868c3":"markdown","f91eac54":"markdown","f179e7ae":"markdown","13fca78a":"markdown","85e9569f":"markdown","cec8383b":"markdown","b9ee0d7e":"markdown","287277f0":"markdown","a2531466":"markdown","b0797d60":"markdown","89b15849":"markdown","428a4a62":"markdown","96f465f2":"markdown","fea0adc9":"markdown","c660bcae":"markdown","d14d34d4":"markdown","ff3eadcf":"markdown","19e14f2a":"markdown","9ef9556f":"markdown","c27b73ca":"markdown","5a2b0732":"markdown","e4220022":"markdown","fb4e68c5":"markdown","bb764570":"markdown","71a62ecd":"markdown","fb4c1334":"markdown","097796b7":"markdown","6ec81b04":"markdown","a6408777":"markdown","75b8a142":"markdown","994fcf3f":"markdown","f5388d39":"markdown","ef49ac4c":"markdown","29a92bc7":"markdown","282127c1":"markdown","a6d845ad":"markdown","dd6e9b38":"markdown","5b174e58":"markdown","7e88731f":"markdown","d5374d02":"markdown","dde2712e":"markdown","53a08964":"markdown","05b69550":"markdown","516eb1d2":"markdown","b899aa3b":"markdown","c3e134bf":"markdown","d0be931d":"markdown","3c47e136":"markdown","2fb67bbe":"markdown","ed60aa98":"markdown","7f141457":"markdown","c7a6830b":"markdown","0cdf8547":"markdown","bfb6419f":"markdown","7d0a6f48":"markdown","416275d5":"markdown","44e93e65":"markdown","d3152c75":"markdown","bc04eac5":"markdown","f7fea95e":"markdown","46911a48":"markdown","088cebea":"markdown","a06039a5":"markdown","f257bad6":"markdown","ec61b8aa":"markdown","7d338a25":"markdown","f07f3c8b":"markdown","d5333083":"markdown","74473895":"markdown","8645bc1c":"markdown","d85c12dc":"markdown","be71e810":"markdown","74eb56a2":"markdown","608cde0b":"markdown","0974f3f5":"markdown","7d29e009":"markdown"},"source":{"4101cd97":"# Import Relevant Libraries\nimport math\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport scipy.stats as stats\nfrom sklearn import metrics\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import confusion_matrix,f1_score,accuracy_score, classification_report\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split","2620b4e7":"#Read the Data File\ntheraDF = pd.read_csv(\"..\/input\/thera-bank-personal-loan\/Bank_Personal_Loan_Modelling.csv\")\ntheraDF.head()","1c21b066":"# Let us ensure that there are no Null Values\nprint(\"Null Values Check\\n\")\nprint(theraDF.isnull().sum())\nprint(\"\\n\\n NAN Values Check \\n\")\nprint(theraDF.isna().sum())\n","a5548d58":"theraDF.dtypes","122595e0":"theraDF.describe()","abdd548a":"theraDF.describe().T","d9925c3d":"sns.set(color_codes = True)\nsns.distplot(theraDF[\"Age\"])","16f9ab36":"plt.hist(theraDF[\"Age\"],bins=5)","8b898b80":"sns.boxplot(x = 'Age', data=theraDF )","6d42bf7d":"# Take only values >0\nexpDF = theraDF[theraDF[\"Experience\"] > 0]\nprint(\"Number of Experiences <= 0:\", (theraDF.count() - expDF.count())[0])\n# Find mean of all > than zero values\nmeanExp = int(expDF[\"Experience\"].mean())\nprint(\"\\n\\nMean Experience:\",meanExp)\n\n# Replace 0 and negative Experience with the Mean value\ntheraDF.loc[(theraDF.Experience <= 0),'Experience'] = meanExp\n#Print the data description to ensure minimum Experience is fixed\ntheraDF.describe().T\n","ef8ca8f0":"sns.boxplot(x = 'Experience', data=theraDF )","1e9b5b5a":"sns.distplot(theraDF[\"Income\"])","67a8bd0e":"plt.hist(theraDF[\"Income\"], bins=5)","b35e29c1":"sns.boxplot(x = 'Income', data = theraDF)","835b70cc":"print(\"Number of unique Zip Codes:\", theraDF['ZIP Code'].nunique())","311dbb20":"theraDF['ZIP Code'].value_counts()[:10].plot(kind='barh')","9990f6a1":"sns.countplot(theraDF['Family'])","b7584443":"sns.distplot(theraDF[\"CCAvg\"])","36132684":"sns.boxplot(x = 'CCAvg', data = theraDF)","b7be5740":"sns.countplot(theraDF['Education'])","8ab4edb5":"sns.swarmplot(theraDF['Education'],theraDF['Income'])","f57e52cb":"sns.distplot(theraDF[\"Mortgage\"])","5ab18ee8":"numcustNoMortgage = theraDF[theraDF[\"Mortgage\"] == 0].Mortgage.count()\nprint(\"Customers with no Mortgage:\\n\", numcustNoMortgage)\nprint(\"Percentage of customers without Morgage:\\n\", (numcustNoMortgage * 100)\/5000)","4b9b7616":"sns.boxplot(x = 'Mortgage', data = theraDF)","5c7b2124":"prodArr = pd.DataFrame()\n\nproducts = [\"Personal Loan\",\"Securities Account\",\"CD Account\",\"Online\",\"CreditCard\"]\nproductCount = [len(theraDF[theraDF[\"Personal Loan\"] == 1]),len(theraDF[theraDF[\"Securities Account\"] == 1]),\n                len(theraDF[theraDF[\"CD Account\"] == 1]),len(theraDF[theraDF[\"Online\"] == 1]),\n                len(theraDF[theraDF[\"CreditCard\"] == 1])]\n\n# Build cccnt = len(theraDF[theraDF[\"CreditCard\"] == 1])plot\nfig, ax = plt.subplots(figsize=(10, 5))\n\nax.bar(products, productCount, align='center', alpha=0.5)\nax.set_ylabel('Number of customers')\nax.set_xticks(products)\nax.set_xticklabels(products)\nax.set_title('Banking Products')\nax.yaxis.grid(True)\n\n\n# Save the figure and show\nplt.tight_layout()\nplt.savefig('bar_plot.png')\nplt.show()\n\n","53ec2186":"sns.pairplot(theraDF[[\"Age\",\"Income\",\"Experience\",\"Mortgage\"]])","cebe6217":"numcustPersonalLoan = theraDF[theraDF[\"Personal Loan\"] == 1].Mortgage.count()\nprint(\"Customers with Personal Loans:\\n\", numcustPersonalLoan)\nprint(\"Percentage of customers with Personal Loans:\\n\", (numcustPersonalLoan * 100)\/5000)","ec0aded8":"sns.countplot(x='Family',hue='Personal Loan',data=theraDF)","58d4a149":"sns.pairplot(theraDF[['Mortgage', 'Income', 'CCAvg', 'Age', 'Personal Loan']], hue = 'Personal Loan');","d45af701":"sns.swarmplot(theraDF['Personal Loan'],theraDF['Income'])","cfa730b9":"sns.catplot(x='Family', y='Income', hue='Personal Loan', data = theraDF, kind='swarm')","4fdbbd3f":"corr = theraDF.corr()\ncorr","ee14f86f":"plt.figure(figsize=(12,10))\nsns.heatmap(corr,annot=True)","9f4f79c4":"theraDF.drop(columns ='ID',inplace=True)\ntheraDF.drop(columns ='Experience',inplace=True)\ntheraDF.drop(columns ='ZIP Code',inplace=True)\n\ntheraDF.head()","cd31b461":"X = theraDF.drop('Personal Loan', axis=1) # X axis without the Target Variable\ny= theraDF['Personal Loan'] # target variable on y Axis\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=3)","2b52344d":"X_train.head()","0f0d9868":"# Let us scale training data set test data using MinMaxScaler\nscaler = MinMaxScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.fit_transform(X_test)","f066d981":"# Suggested value of K should be sqrt(n)\nK = round(math.sqrt(len(theraDF)))\nprint(len(theraDF))\nprint(K)","732141d2":"error_rate = []\n\nfor i in range(1,100):\n NNH = KNeighborsClassifier(n_neighbors=i)\n NNH.fit(X_train,y_train)\n KNN_predicted_labels = NNH.predict(X_test)\n error_rate.append(np.mean(KNN_predicted_labels != y_test))","bfaa0aa3":"plt.figure(figsize=(10,6))\nplt.plot(range(1,100),error_rate,color= 'blue', linestyle='dashed', marker='o', markerfacecolor='red', markersize=10)\nplt.title('Error Rate vs. K Value')\nplt.xlabel('K')\nplt.ylabel('Error Rate')","27fafb9a":"# From the \nK = 3\n\nNNH = KNeighborsClassifier(n_neighbors= int(K) , weights = 'distance' )\n\n# Call Nearest Neighbour algorithm\nNNH.fit(X_train, y_train)","1aad97f9":"#Predict using the test data and score the result \n\nKNN_predicted_labels = NNH.predict(X_train)\ntrain_acc = metrics.accuracy_score(y_train, KNN_predicted_labels)\nprint(\"Model Accuracy with Training Data: {0:.4f}\".format(metrics.accuracy_score(y_train, KNN_predicted_labels)*100))\nprint()\n\nKNN_predicted_labels = NNH.predict(X_test)\ntest_acc = metrics.accuracy_score(y_test, KNN_predicted_labels)\nprint(\"Model Accuracy with Testing Data: {0:.4f}\".format(metrics.accuracy_score(y_test, KNN_predicted_labels)*100))\nprint()","99024954":"from sklearn.naive_bayes import GaussianNB # using Gaussian algorithm from Naive Bayes\n\n# creatw the model\nGNB = GaussianNB()\n\nGNB.fit(X_train, y_train)","d9b7faf1":"GNB_predicted_labels = GNB.predict(X_train)\n\nprint(\"Model Accuracy with Training Data: {0:.4f}\".format(metrics.accuracy_score(y_train, GNB_predicted_labels)*100))\nprint()\n\nGNB_predicted_labels = GNB.predict(X_test)\n\nprint(\"Model Accuracy with Testing data: {0:.4f}\".format(metrics.accuracy_score(y_test, GNB_predicted_labels)*100))\nprint()","b1eb4a12":"# Fit the model on train\nLR = LogisticRegression(solver=\"liblinear\")\nLR.fit(X_train, y_train)","fab5c3dc":"LR_predicted_labels = LR.predict(X_train)\n\nprint(\"Model Accuracy with Training Data: {0:.4f}\".format(metrics.accuracy_score(y_train, LR_predicted_labels)*100))\nprint()\n\nLR_predicted_labels = LR.predict(X_test)\n\nprint(\"Model Accuracy with Testing data: {0:.4f}\".format(metrics.accuracy_score(y_test, LR_predicted_labels)*100))\nprint()","39e37349":"from sklearn.tree import DecisionTreeClassifier\n\ndTree = DecisionTreeClassifier(criterion = 'gini', random_state=1)\ndTree.fit(X_train, y_train)","c1a9c216":"print(dTree.score(X_train, y_train))\nprint(dTree.score(X_test, y_test))","6801bc8a":"# If graphviz doesn't work, we can use plot_tree method from sklearn.tree\n\nimport matplotlib.pyplot as plt\nfrom sklearn.tree import plot_tree\n\nfn = list(X_train)\ncn = ['No', 'Yes']\nfig, axes = plt.subplots(nrows = 1,ncols = 1,figsize = (4, 4), dpi=600)\nplot_tree(dTree, feature_names = fn, class_names=cn, filled = True)\n\nfig.savefig('tree.png')","325597e2":"dTreeR = DecisionTreeClassifier(criterion = 'gini', max_depth = 3, min_samples_split = 150, min_samples_leaf = 50, random_state=1)\nDT_predicted_labels = dTreeR.fit(X_train, y_train)\nprint(dTreeR.score(X_train, y_train))\nprint(dTreeR.score(X_test, y_test))","420552c6":"#Function to print Confusion Matrix and Classification Report\ndef Print_CM_CR_AUC(y_test,y_predict, algoname):\n    cm = confusion_matrix(y_test,y_predict)\n    sns.heatmap(cm,annot=True, fmt='.2f', xticklabels=[0,1], yticklabels=[0,1])\n    plt.ylabel('observed')\n    plt.xlabel('Predicted')\n    plt.show()\n    # get accuracy of model\n    acc_score = accuracy_score(y_test,y_predict)\n    # get F1-score of model\n    F1_score = f1_score(y_test,y_predict) \n    # get the classification report\n    class_rep = classification_report(y_test,y_predict)\n\n    print(\"Accuracy of \", algoname, \" is {} %\".format(acc_score*100))\n    print(\"F1-score of \", algoname, \" is {} %\".format(F1_score*100))\n    print(\"Classification report for \", algoname, \" is: \\n\",class_rep)\n    \n    #AUC Calculations\n    fpr, tpr, thresholds = metrics.roc_curve(y_test, y_predict, pos_label=1)\n    print(\"AUC for \", algoname, \":\", round(metrics.auc(fpr, tpr)*100,2))\n","dd34ffe1":"#Draw Confusion Matrix and Classification Report for KNN\ny_predictedNNH = NNH.predict(X_test)\nPrint_CM_CR_AUC(y_test,y_predictedNNH, \"KNN\")","eb7c0413":"#Draw Confusion Matrix and Classification Report for Naive Bayes\ny_predictedGNB = GNB.predict(X_test)\nPrint_CM_CR_AUC(y_test,y_predictedGNB, \"Naive Bayes\")","85e66e12":"#Draw Confusion Matrix and Classification Report for Logitic Regression\ny_predictedLR = LR.predict(X_test)\nPrint_CM_CR_AUC(y_test,y_predictedLR,\"Logistic Regression\")","3aedaf7d":"#Draw Confusion Matrix and Classification Report for Decision Tree\ny_predictedDT = dTreeR.predict(X_test)\nPrint_CM_CR_AUC(y_test,y_predictedDT,\"Decision Tree\")","354cccd9":"CLose to 3500 out of 5000 customers are in the less than 100K Income range.","7d2f55b0":"Creating the Training and Testing set","23786cd6":"Process:\nWe will analyse the Data and distribution of all the columns. We will dive deeper into the Personal Loan Column and then use 4 classification algorithms to see which performs the best. We will analyse the Confusion Matrix, the Precision and Recall. We will also analyse the cost of Type I and Type II errors to and use that as one of the cretieria for selecting the right Algorithm.","ece99ca5":"Brefore creating the Taining and Testing set, lets remove some irrelevant columns. <br>\n1. ID Column can be dropped <br>\n2. Expreience is Linearly related to Age. So lets drop Experience to avoid multi-colinearity.<br>\n3. ZIP Code has extremely weak relationship with all the Variables. So we can drop ZIP Code too","2765b3bf":"Since we are dealing with continuous variables, we will use the Gaussian version of the algorithm","c4e7bd2a":"Now we have no Negative Experience","26a175f1":"Since there are lot of columns, lets transpose the describe grid for better readaility","d8f868c3":"Let us see if Family size has an impact on Personal Loans","f91eac54":"#### Decision Tree","f179e7ae":"Since the last 5 parameters are Binary, lets use a hue on the Pairplots to understand their disrtibution across other continuous parameters ","13fca78a":"Most customers are in age range of 32 to 58 (approx). There are no customers below 25 or above 68 (approx).\nThere are no outliers.","85e9569f":"#### Logistic Refression Algorithm","cec8383b":"#### Naive Bayes Algorithm","b9ee0d7e":"To further analyse the Correlation of Personal Loans with other parameters, lets plot a Correlation Martrix ","287277f0":"But we are not sure if 71 is the optimum K value. So lets do an Elbow analysis to find the best K","a2531466":"Logistic Regression seems to have te best Training and Testing accuracy and the difference in accuracies is also very low which means there is less chance of over-fitting.","b0797d60":"Mortgage looked highly skewed in the 5-point summary. It will be intesteting to see the distribution. Mortgage is an important variable to study in relation to other variables too. So it will be interesting to see it's relation to others in the pair-plot.  ","89b15849":"Lets analyse Experience. But before analysing the let us <b>fix the negative values and replace them with a mean value<\/b>.","428a4a62":"KNN Algorithm seems to have Overfit the training Data.<br>\nThe testing accuracy too is not close to the training accuracy ","96f465f2":"A very small number of customers have Personal Loans.","fea0adc9":"Lets create a function to print the Confusion Matrix, Classification Report and the AUC.","c660bcae":"#### Analyze Education","d14d34d4":"The training and testing accuracy of Naive Bayes is very close to each other which means it's good a fit. But the accuracy is low. ","ff3eadcf":"#### Distribution of Income","19e14f2a":"#### Confusion Matrix and Classification Report for KNN ","9ef9556f":"More people are undergraduates than graduates.","c27b73ca":"More than 1400 customers are individuals with no families.And 1200 have a family size of 4.","5a2b0732":"#### Let us analyse the distribution of some key columns","e4220022":"Lets find the the top 10 most populous ZIP Codes to get an idea of where the most customers are coming from","fb4e68c5":"And CCAvg also has a lot of outliers","bb764570":"Majority of the customers are online.\nCredit Card is most popular product.\nOther three products are not very popular.","71a62ecd":"### Study the data distribution in each attribute","fb4c1334":"### Give your reasoning on which is the best model in this case and why it performs better? (5 marks)","097796b7":"#### Analyze ZIP Codes","6ec81b04":"Personal Loans are popular with  family size of 3 and 4 and Income above 100K","a6408777":"Training is over-fitted. Lets try and prun the tree and find the ideal depth","75b8a142":"Some Inetersting observations can be made: <br>\n1. Morgage and Personal Loan KDE plot indicates that the Most people with a Mortgage take a Personal Loan <br>\n2. KDE Plot of Income and Personal Loan indicates that customers with Higher Income are more likely to take Personal Loans <br>\n   <b>Customers with less than 50K Income have not taken Personal Loans <\/b>\n3. Customers with higer credit card balances and higher incomes take Personal Loans. <b> Some relationship is seen bewteen Personal Loans and Income and CCAvg<b>\n","994fcf3f":"Since Personal Loan is a boolean variable, lets put a <b> hue of Personal Loan on the Pairplot <\/b> to see if any patterns emmerge.","f5388d39":"### Use different classification models (Logistic, K-NN and Na\u00efve Bayes) to predict the likelihood of a customer buying personal loans (15 marks)","ef49ac4c":"Intrestingly, Credit Card Average has a very similar distribution pattern to Income. Which means people with low income have higher Credit Card Averages ","29a92bc7":"#### Confusion Matrix and Classification Report for Logistic Regression ","282127c1":"Lets use the the min split and min leaf parameters to prun the tree using 1% and 3% rule or minimum leaf count and minimum split count ","a6d845ad":"Let us now look at Data Types","dd6e9b38":"Income is right skewed with multiple peaks. It has a fat tail. Most people are in the income bracket of < 100K. Let us plot a histogram to better understand the distribution across larger buckets. ","5b174e58":"Let us first run three algorithms and see the accuracy. Post that we will do a Precision and Sensitivity analysis. <br>\n<b>Since the number of 1s and 0s in the Personal Loans column has a big difference, the algorithms will be heavily biased towards 0s.<\/b><br>\nSo we will need to look at the Type I and Type II errors and find out what has more cost. Accordingly pick the right algorithm.","7e88731f":"## Conclusion","d5374d02":"To analyse the Performance of the three algorithms, we will use the following: <br>\n1. Confusion Matrix <br>\n2. Classification Report for Precission, Recall and F1-Score <br>\n3. AUC to get the overall prediction performance for all scenarios <br>","dde2712e":"Clearly, a lot of customers don't have a mortgage. Let us find out the number of people with no Mortgage ","53a08964":"Orange and Red colors on Personal Loan with Income and CCAvg show a strong relationship. <br>\nAlso, there seems to be some relationship with CD Accounts. People who have CD Account seem to be more likely to take a Personal Loan <br>\nAge and Experience have a very strong linear relationship","05b69550":"### Get the target column distribution. Your comments (5 marks)\n#### Since Personal Loan is our target variable for the Classification, let us study the Distribution of Personal Loan against other parameters","516eb1d2":"### Split the data into training and test set in the ratio of 70:30 respectively (5 marks)","b899aa3b":"### Thera Bank - Personal Loan Prediction \nThera Bank wants to improve their Personal Loan conversion rate by predicting the probability of a Customer to opt for a Personal Loan. The data containing the Demographics of the customer and the banking products they use is given.","c3e134bf":"Given that the problem statement is a marketing problem, lets analyse the cost of False-Positives and False-Negatives <br>\n1. False-Positive: Selling a Personal Loan to someone who may not be interested <br>\n2. False-Negative: Someone who might be intereted in a loan but will not be sold to <br>\n\nSince the problem domain is marketing, False-Negative will be more expensive as it will mean missing sales opportunities and hence losing money. The cost of selling to someone who is not interested in a loan and is still sold to is low. <br>\n\nGiven this assumption, we need to find an algorithm that is:\n\n    1. High in Accuracy\n    2. False Negatives needs to be low as we don't want to miss selling opportunities\n    3. Precision and Recall for Personal Loan = 1 (Positive) needs to be high\n    4. To combine the evaluation of precision and recall, we will take the highest F1-Score as one of the decision factor\n    5. Highest Recall\n    5. We will look at AUC for overall performance for all possible outcomes\n\n<b>Testing Accuracy (High is good)<\/b><br>\n    &ensp;&ensp;&ensp;KNN: 92.2% <br>\n    &ensp;&ensp;&ensp;Naive Bayes: 88.5% <br>\n    &ensp;&ensp;&ensp;Logistic Regression: 94.5% <br>\n    &ensp;&ensp;&ensp;<b>Decision Tree: 97.5% <\/b><br>  \n\n<b>False Negatives (Low is good) <\/b><br>\n    &ensp;&ensp;&ensp;KNN: 77 <br>\n    &ensp;&ensp;&ensp;Naive Bayes: 57<br>\n    &ensp;&ensp;&ensp;Logistic Regression: 58 <br>\n    &ensp;&ensp;&ensp;<b>Decision Tree: 53 <\/b><br>\n\n<b>F1-Score (High is good)<\/b><br>\n    &ensp;&ensp;&ensp;KNN: 0.56 <br>\n    &ensp;&ensp;&ensp;Naive Bayes: 0.52 <br>\n    &ensp;&ensp;&ensp;Logistic Regression: 0.69<br>\n    &ensp;&ensp;&ensp;<b>Decision Tree: 0.77 <\/b><br>\n\n<b>Recall (High is good)<\/b><br>\n    &ensp;&ensp;&ensp;KNN: 0.49 <br>\n    &ensp;&ensp;&ensp;Naive Bayes: 0.62 <br>\n    &ensp;&ensp;&ensp;Logistic Regression: 0.62<br>\n    &ensp;&ensp;&ensp;<b>Decision Tree: 0.65 <\/b><br>\n\n\n<b>AUC (High is good)<\/b><br>\n    &ensp;&ensp;&ensp;KNN: 73.02 <br>\n    &ensp;&ensp;&ensp;Naive Bayes: 76.83 <br>\n    &ensp;&ensp;&ensp;Logistic Regression: 79.91<br>\n    &ensp;&ensp;&ensp;<b>Decision Tree: 82.7 <\/b><br>","d0be931d":"On all counts Decision Tree has out-performed. Based on the comparison of other  performance parameters:<br> <b> Decision Tree is the best Algorithm for this problem <\/b>\n\nHowever, one of the challenges with this project is that the target column data is very highly skewed. We need more data to get better Recall numbers. And this might change the decision on the right Algorithm to use.    \n","3c47e136":"### Read the column description and ensure you understand each attribute well","2fb67bbe":"For the product categories like Personal Loan, Securities Account etc., let us take a count of each and make a bar plot to see the distribution of Products across customers.","ed60aa98":"Let us look at Family Sizes. Since Family has only limited values, lets plot a Histogram.","7f141457":"Close to <b>70% of customers have no Mortgage<\/b>","c7a6830b":"#### Confusion Matrix and Classification Report for Naive Bayes","0cdf8547":"The ditribution is normal and wide with a multiple peaks. The variation of frequency ages in the middle are fluctuating a lot. Let us get a histogram with age bins to get a better perspective of the distribution.","bfb6419f":"As expected, Mortgage has a large number of outliers. And it has no minimum whisker which means a lot of people don't have mortgages or have small morgages.","7d0a6f48":"#### Distribution of Age","416275d5":"1. Experieince and Age has a clear and obvious linear relationship. We could consider removing the Experience column to reduce number of variables\n2. Higher Income customers take more Mortgage. Or they qualify for higher mortgage. But the funnel show that some customers take lower mortgage even if Income is high. \n","44e93e65":"We can ignore ID as that is just a number and has no impact on our analysis. <br><br>\n<b>Age:<\/b> 50% mark coincides with the mean. Which means that the Data is Normaly distributed. We will see the distribution plot later to verify this <br><br>\n<b>Experience:<\/b> Experience is also normally distributed. However, <b>there are negative Experiences<\/b> which may be erroneous data. We will fix this later. <br><br>\n<b>Income:<\/b> Mean > Mode. Which means data is slightly right skewed. <br><br>\n<b>Zip Code:<\/b> These being postal codes, we can ignore the numerical analysis as these are not number values and more like representation of areas and addresses.<br><br>\n<b>Family:<\/b> Data looks normally distributed. Maximum family size is 4.<br><br>\n<b>Mortgage:<\/b> Mortgage is heavily right skewed. We will analyse this later.<br><br>\n<b>Education:<\/b> Its a Categorical Variable. Difficult to judge the distribution from the 5-point summary. We will do some visualizations later to understand the Education Distribution <br><br>\n<b>CCAvg:<\/b> mean > mode so the distribution is right skewed. Which means a most people have small Credit Card spends. We will see the shape of the tail later in the visual analysis.<br><br>\n<b>Personal Loan, Securities Account, CD Account, Online and CreditCard <\/b> are boolean variables so 5-Point summary does not make much sense. <br><br>\n\n\n\n","d3152c75":"#### Analyze Mortgage","bc04eac5":"Let us do a more detailed analysis taking the appropriate outcome into account. We need to find the cost of Type I and Type II errors to find the best algorithm. Confusion Matrix, Classification Report and AUC will reveal more.","f7fea95e":"<b>The best value of K is 3<\/b>","46911a48":"A swarm plot of Personal Loan against Income too indicates that people with Income below 50K don't take Personal Loans\nThe rate of Personal Loans goes increasing as Income starts going higher.","088cebea":"Lets plot some pair plots to see relationships","a06039a5":"#### There are no null values or NaN values in the data","f257bad6":"An interesting pattern is seen. Undergraduates have the same income as Graduates and People with Avanced Skills.","ec61b8aa":"CCAvg is the only float value. Understandably, because it's an average.","7d338a25":"#### Distribution of Experience","f07f3c8b":"While a bulk of the customers are in the 48K to 100K range, Income has outliers.","d5333083":"### Distribution and Analysis of Personal Loan","74473895":"It  will also be interesting to see the relationship between Family, Income and Personal Loans ","8645bc1c":"### Print the confusion matrix for all the above models (5 marks)","d85c12dc":"It will be inetersting to find the impact of Education on Income. And since Education is a Categorical Variable, lets do a Swarm plot to understand the relationship","be71e810":"#### K-NN Algorithm","74eb56a2":"Experience is normaly distributed with no outliers","608cde0b":"#### Analyze Family","0974f3f5":"Family size does not seem to have a significant impact on Personal Loans. Though Families with 3 or 4 members are slightly more likely to take a Personal Loan ","7d29e009":"#### Analyze Credit Card Average Balances"}}