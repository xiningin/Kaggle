{"cell_type":{"1b31c970":"code","6f8df04a":"code","2599c9f2":"code","915bf67c":"code","697507e2":"code","13591aac":"code","54012064":"code","488e5fa8":"code","d3eb952c":"code","82bf05e4":"code","51dbacbb":"code","176a91a7":"code","acb62115":"code","43af58b9":"code","acf9500a":"code","ab8421ca":"code","cdd25423":"code","c2f261d5":"code","f802bc88":"code","8cc023e8":"code","ec60e279":"markdown","d895d66c":"markdown","799b30fe":"markdown","5caca606":"markdown","f570007b":"markdown","bbf0690a":"markdown","f898fd76":"markdown","9e6479c1":"markdown","9b3ab4e6":"markdown","fca07849":"markdown","d7ba5450":"markdown","1c3cbd8f":"markdown","b26619be":"markdown","c4e1019e":"markdown","6c5e779f":"markdown","3e0b1b34":"markdown","b70e188d":"markdown","0e4ebfe8":"markdown","80d0566a":"markdown","29bf901a":"markdown","c67129bd":"markdown","857f0199":"markdown","f2aa56bd":"markdown","e8d885a4":"markdown","7123c1b9":"markdown","46ab8732":"markdown","09192f09":"markdown"},"source":{"1b31c970":"import numpy as np\nimport pandas as pd\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","6f8df04a":"train = pd.read_csv(\"\/kaggle\/input\/cap-4611-spring-21-assignment-1\/train.csv\")\ntrain.head()","2599c9f2":"test = pd.read_csv(\"\/kaggle\/input\/cap-4611-spring-21-assignment-1\/test.csv\")\ntest.head()","915bf67c":"train.info()","697507e2":"train.describe()","13591aac":"train[train == 0].info()","54012064":"import matplotlib.pyplot as plt\n\n\n\nplt.boxplot(train.iloc[:,2:12])\nplt.xticks(list(range(1,11)), list(train.columns.values[2:12]), rotation=-30, ha='left')\nplt.show()\n\n#--\n\nplt.boxplot(train.iloc[:,16:22])\nplt.xticks(list(range(1,7)), list(train.columns.values[16:22]), rotation=-30, ha='left')\nplt.show()\n\n#--\n\nplt.boxplot(train.iloc[:,23:30])\nplt.xticks(list(range(1,8)), list(train.columns.values[23:30]), rotation=-30, ha='left')\nplt.show()\n\nplt.boxplot(train.iloc[:,23:30])\nplt.xticks(list(range(1,8)), list(train.columns.values[23:30]), rotation=-30, ha='left')\nplt.show()\n\n#--\n\nplt.boxplot(train.iloc[:,31:35])\nplt.xticks(list(range(1,5)), list(train.columns.values[31:35]), rotation=-30, ha='left')\nplt.show()","488e5fa8":"from sklearn.model_selection import train_test_split\n\nseed = 1\n\n# Split data into training and validation sets.\nX = train.iloc[ : , 2: ]\ny = train[\"Bankrupt\"]\n\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=seed)","d3eb952c":"from sklearn.tree import DecisionTreeClassifier\n\ndt = DecisionTreeClassifier(max_depth = 96, criterion='gini', random_state=seed)\ndt.fit(X_train, y_train)","82bf05e4":"dt_pred_prob = dt.predict_proba(X_valid)[:,1]\ndt_pred = dt.predict(X_valid)","51dbacbb":"from sklearn.metrics import roc_auc_score\n\ndt_roc = roc_auc_score(y_valid, dt_pred_prob)\nprint(dt_roc)","176a91a7":"from sklearn.metrics import f1_score\n\ndt_f1 = f1_score(y_valid, dt_pred)\nprint(dt_f1)","acb62115":"from sklearn.metrics import accuracy_score\n\ndt_acc = accuracy_score(y_valid, dt_pred)\nprint(dt_acc)","43af58b9":"from sklearn.ensemble import RandomForestClassifier\n\nrf = RandomForestClassifier(n_estimators=293, min_samples_split=2, min_samples_leaf=11, max_features='log2', max_depth=11, criterion='gini', bootstrap=False, random_state=seed)\nrf.fit(X_train, y_train)","acf9500a":"from scipy.stats import randint\nfrom sklearn.model_selection import RandomizedSearchCV\n\nparam_dist = {\n    'n_estimators': randint(10,1000),\n    'max_features': ['log2', 'sqrt'],\n    'min_samples_leaf': randint(1,20),\n    'min_samples_split': randint(2,20),\n    'max_depth': list(range(4,50)) + [None],\n    'criterion': ['gini'],\n    'bootstrap': [True, False]\n}\n\n#rf_cv = RandomizedSearchCV(rf, param_dist, cv=5, scoring='roc_auc', n_iter=100, verbose=10, n_jobs=-1)\n\n#rf_cv.fit(X, y)\n\n#print(\"Tuned Logistic Regression Parameters: {}\".format(rf_cv.best_params_)) \n#print(\"Best score is {}\".format(rf_cv.best_score_))","ab8421ca":"rf_pred_prob= rf.predict_proba(X_valid)[:,1]\nrf_pred = rf.predict(X_valid)","cdd25423":"rf_roc = roc_auc_score(y_valid, rf_pred_prob)\nprint(rf_roc)","c2f261d5":"rf_f1 = f1_score(y_valid, rf_pred)\nprint(rf_f1)","f802bc88":"rf_acc = accuracy_score(y_valid, rf_pred)\nprint(rf_acc)","8cc023e8":"X_test = test.iloc[ : , 1: ]\ny_pred = rf.predict_proba(X_test)[:,1]\n\noutput = pd.DataFrame({\"id\": test.id, \"Bankrupt\": y_pred})\noutput.to_csv('my_submission.csv', index=False)\nprint(\"Your submission was successfully saved!\")","ec60e279":"# 11. Select the best model","d895d66c":"I've considered removing data points where data is missing. Specifically in the columns where there are only a couple zero values. However, I think while this could make it easier for decision trees to split certain features, the removal of valid data in other columns would skew the decision tree's data. I think the best approach would be to impute these values using the mean. This would not affect the features where the data points have valid data. Compared to having the data removed, imputing with the mean would not skew the threshold in either direction at a node. I didn't end up doing this because I didn't have to in order to pass the benchmark.","799b30fe":"# 9. Build and train random forest model on training data","5caca606":"# 5. Handle Outliers","f570007b":"## Tune hyperparameters","bbf0690a":"I'm going to leave the cross validation commented out because otherwise it will take forever everytime I hit run all, but the best parameters I got were:\n* bootstrap: False\n* criterion: gini\n* max_depth: 11\n* max_features: log2\n* min_samples_leaf: 11\n* min_samples_split: 2\n* n_estimators: 293\n\nWith 94.1956% accuracy","f898fd76":"In some columns, there are hundreds of zeros, which probably don't indicate missing data. In other columns, there are one or two. If zero values in these columns don't represent missing values then they may still be outliers.","9e6479c1":"# 3. Handle Missing Values\n","9b3ab4e6":"The boxplots show hundreds of points that are considered to be outliers. The performance of decision trees is resilient to outliers because the scale of the data does not matter when calculating information gain. Clamping these values to a normal range would likely have no effect, as the samples would end up on the same side of the chosen thresholds as before. I also don't want to drop these values entirely, as that could skew the thresholds chosen at each node. So I think the best decision is to let the outliers be.","fca07849":"# 7. Build and train a decision tree on training data","d7ba5450":"# 6. Normalization and Standardization","1c3cbd8f":"## Accuracy","b26619be":"## ROC AUC Score","c4e1019e":"There is no data that is explicitly marked as null.","6c5e779f":"I'm choosing the random forest model. Before I started tuning hyperparameters, it was giving me much better results. I also think a random forest model compared to the decision tree model is almost always better. A random forest is just a bunch of decision trees, so it is capable of doing anything a decision tree can in theory. I think the use is also justified by the large number of features in the dataset, and the need for a model which can sort out the useful ones from the insignificant ones.","3e0b1b34":"In this assignment, we are limited to decision tree and random forest classifiers. Normalization will not have any effect on the ability for a decision tree or random forest to classify the data. This is because normalization is just a linear scaling of the data, but decision trees can accomodate data at any scale because the value each node splits at is chosen based on information gain. So after normalizing, a decision tree would simply choose a different value to split each feature at which would have no effect on accuracy.  \n\nThere is also no use for standardization for the same reason.","b70e188d":"## F1 Score","0e4ebfe8":"# 1. Read the CSV data","80d0566a":"## ROC AUC Score","29bf901a":"It looks like 0 is being used to represent missing values across most of the columns.","c67129bd":"Christopher Kerns  \nKaggle: ctkerns","857f0199":"# 2. Check for missing values in training data","f2aa56bd":"# 4. Check for outliers","e8d885a4":"## F1 Score","7123c1b9":"## Acccuracy","46ab8732":"# 10. Random forest accuracy","09192f09":"# 8. Decision tree accuracy"}}