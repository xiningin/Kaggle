{"cell_type":{"b2b99f10":"code","4bb0c9f2":"code","a32e0d77":"code","5ec415b1":"code","743713a6":"code","26383e38":"code","5f199f14":"code","ae532334":"code","985333aa":"code","121cfb73":"code","8d5e5a07":"code","fa140e28":"code","d4e2d726":"code","84b865e3":"code","50806352":"code","8ff695e2":"code","11b2e816":"code","8963a2b5":"code","723f618b":"code","6aed7195":"code","f36b4cda":"code","a67ae3a4":"markdown","c1cd2e63":"markdown","e32f4d84":"markdown","7b786d95":"markdown","4644f6e7":"markdown","96f8976a":"markdown","37b843f0":"markdown","da351a64":"markdown","c49f8823":"markdown","a3766a01":"markdown","0b0b0ccc":"markdown","d0896673":"markdown","62dfc1d7":"markdown","e3c2d431":"markdown","5853365b":"markdown","d7764097":"markdown","4f3eb61b":"markdown","b31f3749":"markdown","09b202c9":"markdown","eb05e704":"markdown","3230c829":"markdown","94d479a8":"markdown"},"source":{"b2b99f10":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","4bb0c9f2":"import matplotlib.pyplot as plt\nimport seaborn as sns \nsns.set()\n%matplotlib notebook\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.metrics import confusion_matrix, accuracy_score\nfrom sklearn.model_selection import train_test_split, RepeatedStratifiedKFold, GridSearchCV\n\nimport nltk\nfrom nltk.stem.porter import PorterStemmer\nfrom nltk import word_tokenize, WordNetLemmatizer\nfrom nltk.corpus import wordnet as wn\nfrom nltk.corpus import stopwords\nfrom collections import Counter \nfrom wordcloud import WordCloud\nimport re\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.ensemble import RandomForestClassifier","a32e0d77":"## loading the dataset\nsentiment_dataset = pd.read_csv(\"..\/input\/stockmarket-sentiment-dataset\/stock_data.csv\")","5ec415b1":"sentiment_dataset.head(5)","743713a6":"sentiment_dataset.info()","26383e38":"## percentage of positive and negative sentiments\n(sentiment_dataset[\"Sentiment\"].value_counts()\/len(sentiment_dataset)) * 100","5f199f14":"## Plot the Sentiment value count \nsns.countplot(x=sentiment_dataset[\"Sentiment\"] )","ae532334":"stop_words=set(stopwords.words(\"english\"))","985333aa":"word_list = list()\nfor i in range(len(sentiment_dataset)):\n    words = sentiment_dataset.Text[i].split()\n    for j in words:\n        word_list.append(j)","121cfb73":"wordCounter = Counter(word_list)\ncountedWordDict = dict(wordCounter)\nsortedWordDict = sorted(countedWordDict.items(),key = lambda x : x[1],reverse=True)\nsortedWordDict[0:20]","8d5e5a07":"stop_word_Cloud = set(stopwords.words(\"english\"))\nwordcloud = WordCloud(stopwords=stop_word_Cloud,max_words=2000,background_color=\"black\",min_font_size=5).generate_from_frequencies(countedWordDict)\nplt.figure(figsize=[10,5])\nplt.axis(\"off\")\nplt.imshow(wordcloud)\nplt.show()","fa140e28":"sentiment_dataset[\"Sentiment\"] = sentiment_dataset[\"Sentiment\"].replace(-1,0)","d4e2d726":"ps = PorterStemmer()\nlemma = WordNetLemmatizer()\nstopwordSet = set(stopwords.words(\"english\"))","84b865e3":"text_reviews = list()\nfor i in range(len(sentiment_dataset)):\n    text = re.sub('[^a-zA-Z]',\" \",sentiment_dataset['Text'][i])\n    text = text.lower()\n    text = word_tokenize(text,language=\"english\")\n    text = [lemma.lemmatize(word) for word in text if(word) not in stopwordSet]\n    text = \" \".join(text)\n    text_reviews.append(text)","50806352":"cv = CountVectorizer(max_features = 1500)\nX = cv.fit_transform(text_reviews).toarray()\nY= sentiment_dataset['Sentiment']\n\n## Split the dataset into Training and Test set\nX_train, X_test, y_train, y_test = train_test_split(X, Y , test_size=0.25, random_state = 1)","8ff695e2":"logreg = LogisticRegression()\nlogreg.fit(X_train, y_train)\nY_pred = logreg.predict(X_test)","11b2e816":"print(\"logistic regression\\n\")\nprint(\"confusion matrix:\\n {}\".format(confusion_matrix(y_test, Y_pred)))\nprint(\"\\n\\naccuracy: {}\".format(accuracy_score(y_test, Y_pred)))","8963a2b5":"logreg = LogisticRegression()\n\n#listing hyperparameters\nsolvers = ['newton-cg', 'lbfgs', 'liblinear']\npenalty = ['l2']\nc_values = [100, 10, 1.0, 0.1, 0.01]\n\n# define grid search\ngrid = dict(solver=solvers,penalty=penalty,C=c_values)\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\ngrid_search = GridSearchCV(estimator=logreg, param_grid=grid, n_jobs=-1, cv=cv, scoring='accuracy',error_score=0)\ngrid_result = grid_search.fit(X_train, y_train)\n\n\n# summarize results\nprint(\"Best accuracy: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n","723f618b":"random_forest = RandomForestClassifier(n_estimators=100)\nrandom_forest.fit(X_train, y_train)\nY_pred = random_forest.predict(X_test)","6aed7195":"print(\"Random Forest\\n\")\nprint(\"confusion matrix:\\n {}\".format(confusion_matrix(y_test, Y_pred)))\nprint(\"\\n\\naccuracy: {}\".format(accuracy_score(y_test, Y_pred)))","f36b4cda":"random_forest = RandomForestClassifier()\n\n#listing hyperparameters\nn_estimators = [10, 100, 1000]\nmax_features = ['sqrt', 'log2']\n\n# defining grid search\ngrid = dict(n_estimators=n_estimators,max_features=max_features)\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\ngrid_search = GridSearchCV(estimator=random_forest, param_grid=grid, cv=cv, scoring='accuracy',error_score=0)\ngrid_result = grid_search.fit(X_train, y_train)\n\n\n# summarizing results\nprint(\"Best accuracy: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n","a67ae3a4":"## getting our arsenal ready!!","c1cd2e63":"## hyperparameter tuning for logistic regression","e32f4d84":"## lets look at the data available","7b786d95":"#### these are the stop words present in the news text","4644f6e7":"#### cleaning the text","96f8976a":"#### NLP Processing","37b843f0":"#                      STOCK MARKET PREDICTION\n\n![stock market](attachment:photo-1585504256855-6718f7a693ae.jpeg)\n\n### Predicting stock prices using the power of Machine Learning has been a rapidly sought after research topics.\n### Owing to the very nature of Stocks. The prediction depends on a **variety of factors**:\n#### Market sentiments are one of such factors on which the price of stock largely depends.\n\n### Market sentiments play a driving role in the pricing of the stock and the smart players are well aware of this boon in disguise. \n\n### In this notebook I have tried to model algorithms and hypertune their hyperparameters for predicting market sentiments which can then be used to price the stock in future with the maximum accuracy.\n\n","da351a64":"## Modelling","c49f8823":"## Data Preprocessing","a3766a01":"#### Replacing the negative one with zero so our model can predict well","0b0b0ccc":"#### Ohho!! we have a fair large number of data points 5791 in total\n#### with a 7:4 taget values (which is good)","d0896673":"#### creating the Bag of Word model","62dfc1d7":"### 2. Random Forest","e3c2d431":"## hyperparameter tuning for random forest classifier","5853365b":"ensemble methods can increase the accuracy by a notable amount. ","d7764097":"#### the above plot shows fair number of news inclined in positive but the imbalance in not that much","4f3eb61b":"#### the model gave an accuracy of 76.79%. Let's see if it can be increased by hypertuning","b31f3749":"## Checking for stopwords","09b202c9":"### 1. Logistic Regression","eb05e704":"#### so the first column contains the news and second has the sentiment\n","3230c829":"## let us load the dataset","94d479a8":"dataset link: [stockmarket-sentiment-dataset](https:\/\/www.kaggle.com\/yash612\/stockmarket-sentiment-dataset)"}}