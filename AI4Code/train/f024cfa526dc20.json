{"cell_type":{"77744ebd":"code","ca77155e":"code","7886db3a":"code","3afff97f":"code","dcdbbeda":"code","6524239f":"code","ddaf5e4c":"code","35920f48":"code","1eee7e80":"code","b8d57de2":"code","fa29409b":"code","70650de5":"code","4bfa97b3":"code","04c246f9":"code","3fa99c19":"code","23ccf17c":"code","19373e36":"code","69f9c6e7":"code","3c515ef9":"code","77a918ad":"code","29dfa229":"code","3eb78fa7":"code","c2400f31":"markdown","99ad7d7c":"markdown","6529e864":"markdown","e2132ea7":"markdown","60bce848":"markdown","7a022459":"markdown","da0a529b":"markdown","839e3732":"markdown","e0eac15c":"markdown","01f1ff69":"markdown","4080bffd":"markdown","bae7e538":"markdown","905d6234":"markdown","e423e975":"markdown","dd0416f9":"markdown","d7b83010":"markdown","cc69ba83":"markdown","a6aa3940":"markdown","d3f81107":"markdown","15670701":"markdown"},"source":{"77744ebd":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt \nfrom pandas import DataFrame \nfrom sklearn.preprocessing import StandardScaler, normalize\nfrom sklearn.decomposition import PCA\nfrom sklearn.mixture import GaussianMixture \nfrom sklearn.metrics import silhouette_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics","ca77155e":"df = pd.read_csv('..\/input\/ccdata\/CC GENERAL.csv')","7886db3a":"df.head()","3afff97f":"df1 = df.drop('CUST_ID', axis = 1) \ndf1.fillna(method ='bfill', inplace = True) ","dcdbbeda":"# Standardize data\nscaler = StandardScaler() \nscaled_df = scaler.fit_transform(df1) ","6524239f":"# Normalizing the Data \nnormalized_df = normalize(df1) ","ddaf5e4c":"# Converting the numpy array into a pandas DataFrame \nnormalized_df = pd.DataFrame(normalized_df) ","35920f48":"# Reducing the dimensions of the data \npca = PCA(n_components = 3) \npcadf = pca.fit_transform(normalized_df) \npcadf = pd.DataFrame(pcadf) \npcadf.columns = ['Principal Component 1', 'Principal Component 2', 'Principal Component 3'] \n  \npcadf.head(10)","1eee7e80":"fig, ax = plt.subplots(figsize=(12, 8))\nscatter = ax.scatter(pcadf['Principal Component 1'],\n                     pcadf['Principal Component 2'], \n                     c = pcadf['Principal Component 3'],\n                     alpha=0.6)\nplt.title('Plotting the 3-Dimensional data after PCA is applied', fontsize = 20)\nplt.xlabel('Principal Component 1', fontsize = 15)\nplt.ylabel('Principal Component 2', fontsize = 15)\nplt.legend(*scatter.legend_elements(), loc=\"best\", title=\"Principal\\nComponent 3\")\nax.plot([])\nax.grid()\nplt.show()","b8d57de2":"pca.explained_variance_ratio_","fa29409b":"import seaborn as sns; sns.set()\nfrom sklearn.cluster import KMeans\n\nsse = []\nK = range(1,10)\nfor k in K:\n    kmeanModel = KMeans(n_clusters=k)\n    kmeanModel.fit(df1)\n    sse.append(kmeanModel.inertia_)\nplt.figure(figsize=(15,6))\nplt.plot(K, sse, 'bx-')\nplt.xlabel('Number of Clusters (k)', fontsize = 15)\nplt.ylabel('Sum of Squared Error', fontsize = 15)\nplt.title('The Elbow Method showing the optimal k \\n(For data without PCA applied)', fontsize = 20)\nplt.show()","70650de5":"import seaborn as sns; sns.set()\nfrom sklearn.cluster import KMeans\n\nsse = []\nK = range(1,10)\nfor k in K:\n    kmeanModel = KMeans(n_clusters=k)\n    kmeanModel.fit(pcadf)\n    sse.append(kmeanModel.inertia_)\nplt.figure(figsize=(15,6))\nplt.plot(K, sse, 'bx-')\nplt.xlabel('Number of Clusters (k)', fontsize = 15)\nplt.ylabel('Sum of Squared Error', fontsize = 15)\nplt.title('The Elbow Method showing the optimal k \\n(For data with PCA applied)', fontsize = 20)\nplt.show()","4bfa97b3":"import matplotlib\nmatplotlib.rc_file_defaults()","04c246f9":"gmm = GaussianMixture(n_components = 3) \ngmm.fit(pcadf)","3fa99c19":"fig = plt.figure(figsize = (7, 7))\nplt.suptitle(\"Plotting the 3 component with colors representing clusters\", fontsize = 15)\n             \nx, s1 = pcadf['Principal Component 1'], \"Principal Component 1\"\ny, s2 = pcadf['Principal Component 2'], \"Principal Component 2\"\nz, s3 = pcadf['Principal Component 3'], \"Principal Component 3\"\n\nc = gmm.fit_predict(pcadf) \n\nax = fig.add_subplot(111, projection = '3d')\nax.scatter(x, y, z, c = c, s=0.5, alpha = 1)\nplt.title('x axis : P1 | y axis : P2 | z axis : P3')\nax.set_xlabel(s1, fontsize = 13)\nax.set_ylabel(s2, fontsize = 13)\nax.set_zlabel(s3, fontsize = 13)\n\nplt.show()","23ccf17c":"gmm1 = GaussianMixture(n_components = 3) \ngmm1.fit(df1)","19373e36":"y_pred = gmm.predict(pcadf)\npred = pd.DataFrame(y_pred)\npred.columns = ['Type']\n\nprediction = pd.concat([pcadf, pred], axis = 1)\n\nclus0 = prediction.loc[prediction.Type == 0]\nclus1 = prediction.loc[prediction.Type == 1]\nclus2 = prediction.loc[prediction.Type == 2]\n\ncluster_list = [clus0.values, clus1.values, clus2.values]","69f9c6e7":"y_pred1 = gmm1.predict(df1)\npred1 = pd.DataFrame(y_pred1)\npred1.columns = ['Type']\n\nprediction1 = pd.concat([df1, pred1], axis = 1)\n\nclus10 = prediction1.loc[prediction1.Type == 0]\nclus11 = prediction1.loc[prediction1.Type == 1]\nclus12 = prediction1.loc[prediction1.Type == 2]\n\ncluster_list1 = [clus10.values, clus11.values, clus12.values]","3c515ef9":"from sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_score\n\nX = pcadf\ny = gmm.fit_predict(pcadf)\nprint(\"Clusters\\tSilhoutte Score\\n\")\nfor n_components in range(2, 11):\n    gmm = GaussianMixture(n_components=n_components).fit(X)\n    sil_coeff = silhouette_score(X, c, metric='euclidean')\n    print(\"k = {} \\t--> \\t{}\".format(n_components, sil_coeff))","77a918ad":"from sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_score\n\nX = df1\ny = gmm1.fit_predict(pcadf)\nprint(\"Clusters\\tSilhoutte Score\\n\")\nfor n_components in range(2, 11):\n    gmm = GaussianMixture(n_components=n_components).fit(X)\n    sil_coeff = silhouette_score(X, c, metric='euclidean')\n    print(\"k = {} \\t--> \\t{}\".format(n_components, sil_coeff))","29dfa229":"from sklearn.metrics import davies_bouldin_score\ndavies_bouldin_score(pcadf, y_pred)","3eb78fa7":"davies_bouldin_score(df1, y_pred1) ","c2400f31":"## Elbow Method ","99ad7d7c":"#### A. For data with PCA applied","6529e864":"Thus, data with PCA applied gives better clustering results","e2132ea7":"**Using Gaussian Clustering and PCA Techniques to make clusters of the Credit Car data**","60bce848":"## Preprocessing the data and applying PCA\n\n1. Dropping Customer ID as it does not provide useful information for clustering\n2. Standardising and Normalising the data. This prevents one attribute from having a greater influence on clustering than another as the data is now uniform. \n3. Creating DataFrame of the uniform  data.\n4. Applying PCA to reduce the data to 2 dimensions. This data is plotted. ","7a022459":"#### A. For data with PCA applied","da0a529b":"#### B. For data without PCA applied","839e3732":"The lower the DB index, the better. Thus, data applied with PCA gives better results","e0eac15c":"## Importing the data and necessary libraries ","01f1ff69":"## Applying the Gaussian Mixture Model to cluster our data (without PCA) into 3 clusters","4080bffd":"#### A. For data with PCA applied","bae7e538":"This notebook explains in detail the implementation of the gaussian mixture model using the expectation maximization for an appropriate data set and validating the clusters properly with the help of various external validation measures.","905d6234":"#### B. For data without PCA applied","e423e975":"### 2. DB Index","dd0416f9":"### 1. Silhoutte Score","d7b83010":"We will choose 3 as k value here","cc69ba83":"## Applying the Gaussian Mixture Model to cluster our data (PCA-applied) into 3 clusters","a6aa3940":"## External Evaluation Metric","d3f81107":"We will choose k as 3 here ","15670701":"#### B. For data without PCA applied"}}