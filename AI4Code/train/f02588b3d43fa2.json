{"cell_type":{"67717eaa":"code","d4358f7f":"code","08c00d7e":"code","852bafca":"code","6a1cd211":"code","5c2be46c":"code","27b740d7":"code","467317c8":"code","d7b85362":"code","500eaa8f":"code","7a816a1f":"code","f510795c":"code","28e47f04":"code","457084b4":"code","9f789271":"code","669d44f8":"code","6c08ef7f":"code","9d466ba9":"markdown","74695bc2":"markdown","9bad801e":"markdown"},"source":{"67717eaa":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","d4358f7f":"%matplotlib inline\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport string\nimport spacy\nimport re\n\nnlp = spacy.load(\"en_core_web_sm\")\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","08c00d7e":"from sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import PassiveAggressiveClassifier","852bafca":"from sklearn.metrics import accuracy_score, zero_one_loss, classification_report","6a1cd211":"data_df = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/train.csv')\ntest_df = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/test.csv')","5c2be46c":"data_df.head()","27b740d7":"print(\"Total rows in train data: \",data_df.shape[0])\nprint(\"Total columns in train data: \", data_df.shape[1])\nprint(\"-\"*30)\nprint(\"Total rows in test data: \", test_df.shape[0])\nprint(\"Total columns in test data: \", test_df.shape[1])","467317c8":"print(data_df.isna().sum())\nprint(\"-\"*30)\nprint(test_df.isna().sum())","d7b85362":"data_df.groupby(data_df.target).count().text","500eaa8f":"sns.countplot(data_df.target, data=data_df)","7a816a1f":"data_df = data_df.groupby('target').apply(lambda x: x.sample(3271)).reset_index(drop=True)\nsns.countplot(data_df.target, data=data_df)","f510795c":"data_df = data_df.sample(frac=1).reset_index()\ntrain_df, valid_df = data_df.iloc[:5233, :], data_df.sample(frac=1).iloc[5234:,:]","28e47f04":"print(\"Total rows and columns in train data is: \", train_df.shape)\nprint(\"-\"*30)\nprint(\"Total rows and columns in validate data is: \",valid_df.shape )","457084b4":"class DisasterTweetModel:\n    def __init__(self):\n        self.vectorizer = TfidfVectorizer(stop_words='english')\n        self.model = PassiveAggressiveClassifier(max_iter=500, tol=1e-3)\n    \n    def get_clean_text(self, df):\n        return df.text.apply(self.clean_tweet)\n\n    def fit(self, x, y):\n        x = np.array([self.clean_tweet(text) for text in x])\n        x_vector = self.vectorizer.fit_transform(x)\n        self.model.fit(x_vector, y)\n        print(\"Training Finished\")\n    \n    def predict(self,ids, x):\n        x = np.array([self.clean_tweet(text) for text in x])\n        x_vector = self.vectorizer.transform(x)\n        y_pred = self.model.predict(x_vector).reshape((x.shape[0], 1))\n        ids = ids.reshape((ids.shape[0], 1))\n        array = np.concatenate((ids, y_pred), axis=1)\n        return pd.DataFrame(array, columns=['id','target'])\n    \n    def clean_tweet(self, t):\n        document = []\n        for token in nlp(t):\n            if not token.is_stop and token.text not in string.punctuation and token.pos_ not in [\"NOUN\", \"VERB\", \"ADJ\", \"ADV\"]:\n                text = re.sub(r'^https?:\\\/\\\/.*[\\r\\n]*', '', token.lemma_, flags=re.MULTILINE)\n                text = text.replace('#', '')\n                document.append(text)\n        return \" \".join(document)\n            \n    \n    def validate(self ,ids, x, y):\n        y_frame = self.predict(ids, x)\n        y_pred = y_frame.target.values\n        accuracy = accuracy_score(y, y_pred)\n        print(\"Accuracy of model is: \", accuracy)\n        print(\"-\"*30)\n        loss = zero_one_loss(y, y_pred)\n        print(\"Loss of model is: \", loss)\n        print(\"-\"*30)\n        cr = classification_report(y, y_pred)\n        print(\"Classification Report of model is: \", cr)\n        return accuracy, loss, cr\n        \n        \n    \n        ","9f789271":"model = DisasterTweetModel()","669d44f8":"model.fit(train_df.text.values, train_df.target.values)","6c08ef7f":"ac, los, cr = model.validate(valid_df.id.values, valid_df.text.values, valid_df.target.values)","9d466ba9":"### **Split Data into Train and Validation Data**","74695bc2":"### **Trim of extra rows with target as '0' to avoid un-fair training**\nThis un-balanced dataset might cause sluggish and biased predictions.\nWe need equal amount of each target-rows for better performance.\n***But loosing 1000 rows is a huge loss, we can use these rows in validation***","9bad801e":"### **Load Data**"}}