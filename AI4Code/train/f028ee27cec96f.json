{"cell_type":{"3897d781":"code","28833c45":"code","e751d547":"code","79db8c6a":"code","07f58078":"code","d7a1a3ea":"code","54547d4a":"code","2b9c28bb":"code","fa26a13c":"code","e35488b8":"code","d9955c16":"code","623be14e":"code","9e392c0d":"markdown"},"source":{"3897d781":"import os\nimport random\nimport warnings\n\nimport numpy as np\nimport pandas as pd\nfrom hyperopt import hp, tpe\nfrom hyperopt.fmin import fmin\nfrom lightgbm import LGBMClassifier\nfrom sklearn.model_selection import cross_val_score, StratifiedKFold\nfrom mlxtend.classifier import StackingCVClassifier\n\nfrom sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import make_scorer\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n\nrandom_state = 1\nrandom.seed(random_state)\nnp.random.seed(random_state)\nos.environ['PYTHONHASHSEED'] = str(random_state)\n\n\nprint('> Loading data')\nX_train = pd.read_csv('\/kaggle\/input\/learn-together\/train.csv', index_col='Id')\nX_test = pd.read_csv('\/kaggle\/input\/learn-together\/test.csv', index_col='Id')\n\ny_train = X_train['Cover_Type'].copy()\nX_train = X_train.drop(['Cover_Type'], axis='columns')\n\n\nprint('> Adding and dropping features')\n\ndef add_features(X_):\n    X = X_.copy()\n\n    X['Hydro_Elevation_diff'] = X[['Elevation',\n                                   'Vertical_Distance_To_Hydrology']\n                                  ].diff(axis='columns').iloc[:, [1]]\n\n    X['Hydro_Euclidean'] = np.sqrt(X['Horizontal_Distance_To_Hydrology']**2 +\n                                   X['Vertical_Distance_To_Hydrology']**2)\n\n    X['Hydro_Fire_sum'] = X[['Horizontal_Distance_To_Hydrology',\n                             'Horizontal_Distance_To_Fire_Points']\n                            ].sum(axis='columns')\n\n    X['Hydro_Fire_diff'] = X[['Horizontal_Distance_To_Hydrology',\n                              'Horizontal_Distance_To_Fire_Points']\n                             ].diff(axis='columns').iloc[:, [1]].abs()\n\n    X['Hydro_Road_sum'] = X[['Horizontal_Distance_To_Hydrology',\n                             'Horizontal_Distance_To_Roadways']\n                            ].sum(axis='columns')\n\n    X['Hydro_Road_diff'] = X[['Horizontal_Distance_To_Hydrology',\n                              'Horizontal_Distance_To_Roadways']\n                             ].diff(axis='columns').iloc[:, [1]].abs()\n\n    X['Road_Fire_sum'] = X[['Horizontal_Distance_To_Roadways',\n                            'Horizontal_Distance_To_Fire_Points']\n                           ].sum(axis='columns')\n\n    X['Road_Fire_diff'] = X[['Horizontal_Distance_To_Roadways',\n                             'Horizontal_Distance_To_Fire_Points']\n                            ].diff(axis='columns').iloc[:, [1]].abs()\n    \n    # Compute Soil_Type number from Soil_Type binary columns\n    X['Stoneyness'] = sum(i * X['Soil_Type{}'.format(i)] for i in range(1, 41))\n    \n    # For all 40 Soil_Types, 1=rubbly, 2=stony, 3=very stony, 4=extremely stony, 0=?\n    stoneyness = [4, 3, 1, 1, 1, 2, 0, 0, 3, 1, \n                  1, 2, 1, 0, 0, 0, 0, 3, 0, 0, \n                  0, 4, 0, 4, 4, 3, 4, 4, 4, 4, \n                  4, 4, 4, 4, 1, 4, 4, 4, 4, 4]\n    \n    # Replace Soil_Type number with \"stoneyness\" value\n    X['Stoneyness'] = X['Stoneyness'].replace(range(1, 41), stoneyness)\n    \n    return X\n\n\ndef drop_features(X_):\n    X = X_.copy()\n    drop_cols = ['Soil_Type7', 'Soil_Type8', 'Soil_Type9', 'Soil_Type14', 'Soil_Type15', \n                 'Soil_Type16', 'Soil_Type18', 'Soil_Type19', 'Soil_Type21', 'Soil_Type25', \n                 'Soil_Type26', 'Soil_Type27', 'Soil_Type28', 'Soil_Type34', 'Soil_Type36', \n                 'Soil_Type37']\n    \n    X = X.drop(drop_cols, axis='columns')\n\n    return X\n\nprint('  -- Processing train data')\nX_train = add_features(X_train)\nX_train = drop_features(X_train)\n\nprint('  -- Processing test data')\nX_test = add_features(X_test)\nX_test = drop_features(X_test)","28833c45":"print('> Adding cluster based feature')\nfrom sklearn.mixture import GaussianMixture\n\ngmix = GaussianMixture(n_components=10)\ngmix.fit(X_test)\n\nX_train['Test_Cluster'] = gmix.predict(X_train)\nX_test['Test_Cluster'] = gmix.predict(X_test)","e751d547":"#gini evaluation metrics\ndef gini(truth, predictions):\n    g = np.asarray(np.c_[truth, predictions, np.arange(len(truth)) ], dtype=np.float)\n    g = g[np.lexsort((g[:,2], -1*g[:,1]))]\n    gs = g[:,0].cumsum().sum() \/ g[:,0].sum()\n    gs -= (len(truth) + 1) \/ 2.\n    return gs \/ len(truth)\n\ndef gini_xgb(predictions, truth):\n    truth = truth.get_label()\n    return 'gini', -1.0 * gini(truth, predictions) \/ gini(truth, truth)\n\ndef gini_lgb(truth, predictions):\n    score = gini(truth, predictions) \/ gini(truth, truth)\n    return 'gini', score, True\n\ndef gini_sklearn(truth, predictions):\n    return gini(truth, predictions) \/ gini(truth, truth)\n\ngini_scorer = make_scorer(gini_sklearn, greater_is_better=True, needs_proba=True)","79db8c6a":"#Tuning Random Forest with hyperopt library\ndef objective(params):\n    params = {'n_estimators': int(params['n_estimators']), 'max_depth': int(params['max_depth'])}\n    clf = RandomForestClassifier(n_jobs=4, class_weight='balanced', **params)\n    score = cross_val_score(clf, X_train, y_train, scoring=gini_scorer, cv=StratifiedKFold()).mean()\n    print(\"Gini {:.3f} params {}\".format(score, params))\n    return score\n\nspace = {\n    'n_estimators': hp.quniform('n_estimators', 25, 500, 25),\n    'max_depth': hp.quniform('max_depth', 1, 10, 1)\n}\n\nbest = fmin(fn=objective,\n            space=space,\n            algo=tpe.suggest,\n            max_evals=10)\n","07f58078":"#optimum parameter for RF\nprint(\"Hyperopt estimated optimum {}\".format(best))","d7a1a3ea":"#Tuning XGBoost\nimport xgboost as xgb\nfrom xgboost.sklearn import XGBClassifier\n#Tree complexity (max_depth)\n#Gamma - Make individual trees conservative, reduce overfitting\n#Column sample per tree - reduce overfitting\ndef objective(params):\n    params = {\n        'max_depth': int(params['max_depth']),\n        'gamma': \"{:.3f}\".format(params['gamma']),\n        'colsample_bytree': '{:.3f}'.format(params['colsample_bytree']),\n    }\n    \n    clf =XGBClassifier(\n        n_estimators=250,\n        learning_rate=0.05,\n        n_jobs=4,\n        **params\n    )\n    \n    score = cross_val_score(clf, X_train, y_train, scoring=gini_scorer, cv=StratifiedKFold()).mean()\n    print(\"Gini {:.3f} params {}\".format(score, params))\n    return score\n\nspace = {\n    'max_depth': hp.quniform('max_depth', 2, 8, 1),\n    'colsample_bytree': hp.uniform('colsample_bytree', 0.3, 1.0),\n    'gamma': hp.uniform('gamma', 0.0, 0.5),\n}\n\nbest = fmin(fn=objective,\n            space=space,\n            algo=tpe.suggest,\n            max_evals=10)","54547d4a":"#optimum parameters for XG\nprint(\"Hyperopt estimated optimum {}\".format(best))","2b9c28bb":"#Tuning LightGBM\nimport lightgbm as lgbm\ndef objective(params):\n    params = {\n        'num_leaves': int(params['num_leaves']),\n        'colsample_bytree': '{:.3f}'.format(params['colsample_bytree']),\n    }\n    \n    clf = lgbm.LGBMClassifier(\n        n_estimators=500,\n        learning_rate=0.01,\n        **params\n    )\n    \n    score = cross_val_score(clf, X_train, y_train, scoring=gini_scorer, cv=StratifiedKFold()).mean()\n    print(\"Gini {:.3f} params {}\".format(score, params))\n    return score\n\nspace = {\n    'num_leaves': hp.quniform('num_leaves', 8, 128, 2),\n    'colsample_bytree': hp.uniform('colsample_bytree', 0.3, 1.0),\n}\n\nbest = fmin(fn=objective,\n            space=space,\n            algo=tpe.suggest,\n            max_evals=10)","fa26a13c":"#optimum for LGBM\nprint(\"Hyperopt estimated optimum {}\".format(best))","e35488b8":"#Hyperopt estimated optimum_RF {'max_depth': 10.0, 'n_estimators': 250.0}\n#Hyperopt estimated optimum_LGBM {'colsample_bytree': 0.911426972757106, 'num_leaves': 88.0}\n#Hyperopt estimated optimum_Xg {'colsample_bytree': 0.8284142875897897, 'gamma': 0.17464593604973394, 'max_depth': 8.0}\nprint('> Setting up classifiers')\nn_jobs = -1\n\n#ab_clf = AdaBoostClassifier(n_estimators=200,\n #                           base_estimator=DecisionTreeClassifier(\n  #                              min_samples_leaf=2,\n  #                              random_state=random_state),\n  #                          random_state=random_state)\n\nlg_clf = LGBMClassifier(n_estimators=400,\n                        num_leaves=88,\n                        verbosity=0,\n                        colsample_bytree=0.91,\n                        random_state=random_state,\n                        n_jobs=n_jobs)\n\nrf_clf = RandomForestClassifier(n_estimators=250,\n                                max_depth=10,\n                                min_samples_leaf=1,\n                                verbose=0,\n                                random_state=random_state,\n                                n_jobs=n_jobs)\n\n\nxgb_clf = xgb.XGBClassifier(\n    n_estimators=250,\n    learning_rate=0.05,\n    n_jobs=4,\n    max_depth=8,\n    colsample_bytree=0.83,\n    gamma=0.17\n)\n\nensemble = [('xgb', xgb_clf),\n            ('lg', lg_clf),\n            ('rf', rf_clf)]\n\n\n\n\n","d9955c16":"stack = StackingCVClassifier(classifiers=[clf for label, clf in ensemble],\n                             meta_classifier=xgb_clf,\n                             cv=5,\n                             use_probas=True,\n                             use_features_in_secondary=True,\n                             verbose=1,\n                             random_state=random_state,\n                             n_jobs=n_jobs)\n\n\nprint('> Fitting & predicting')\nstack = stack.fit(X_train.as_matrix(), y_train.as_matrix())\nprediction = stack.predict(X_test.as_matrix())","623be14e":"print('> Creating submission')\nsubmission = pd.DataFrame({'Id': X_test.index, 'Cover_Type': prediction})\nsubmission.to_csv('submission_tunde.csv', index=False)\n\n\nprint('> Done !')","9e392c0d":"References\nhttps:\/\/www.kaggle.com\/kwabenantim\/forest-cover-stacking-multiple-classifiers\n\nhttps:\/\/www.kaggle.com\/eikedehling\/tune-and-compare-xgb-lightgbm-rf-with-hyperopt\nhttps:\/\/towardsdatascience.com\/automate-stacking-in-python-fc3e7834772e"}}