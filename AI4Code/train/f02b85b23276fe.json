{"cell_type":{"41d8a255":"code","d6887b11":"code","7f5e4d45":"code","c3b170f2":"code","310659da":"code","7da142aa":"code","a5a59672":"code","e11715ec":"code","63244b2e":"code","e1ec99c6":"code","32749eba":"code","6e8e72ab":"code","1deffcd2":"code","57e23d42":"code","69b33a1a":"code","4f32e7d0":"code","16acf547":"code","993e120f":"code","741ff73d":"code","e1131438":"code","e842932d":"code","07433079":"code","8c33fa12":"code","37d64ed3":"code","69d62a28":"code","24745b49":"code","cadb312c":"code","39ba7bb0":"code","99882893":"code","b816f29d":"code","a1ca2ff6":"code","7bc6ea4f":"code","2eea7fdc":"code","9407e53a":"code","fc0f2c9c":"code","38090e9a":"code","70e107b5":"code","4c06e04f":"code","9152cac2":"code","7ce5d6e4":"code","b4beb1cb":"code","d5b03e86":"code","1095bc83":"markdown","98366e7f":"markdown","f6b3c194":"markdown","8f3469d1":"markdown","e11b2cbc":"markdown","a5dfeeed":"markdown","8082c131":"markdown","60915a9b":"markdown","85cccd7e":"markdown","ce912b6a":"markdown","c20648f3":"markdown","665e08b5":"markdown","a2a0abd4":"markdown","1f6a0b51":"markdown","74dd70a2":"markdown","e2ff3ca4":"markdown","10d1c1b7":"markdown","dfa7bd1e":"markdown","6f18e10d":"markdown","00ecab47":"markdown","0d99b57c":"markdown","b2612e81":"markdown","2aaa54f1":"markdown","74ed2f4a":"markdown","863af69d":"markdown","662f4a0e":"markdown","8b476ace":"markdown","77d06896":"markdown","da9e511d":"markdown","8299ec74":"markdown","8c6c2987":"markdown"},"source":{"41d8a255":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# data preprocessing\nimport nltk\nfrom nltk import word_tokenize\nfrom nltk.corpus import stopwords\n\n\n# model building\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n\n# metrics\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n\nimport re\n\n%matplotlib inline\npd.options.display.max_rows = 300\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","d6887b11":"# load the datasets\ntrain = pd.read_csv(\"..\/input\/covid-19-nlp-text-classification\/Corona_NLP_train.csv\", encoding=\"latin-1\")\ntest = pd.read_csv(\"..\/input\/covid-19-nlp-text-classification\/Corona_NLP_test.csv\")","7f5e4d45":"train.head()","c3b170f2":"test.head()","310659da":"print(train.info(), '\\n')\nprint(test.info())","7da142aa":"# drop duplicate entries\ntrain.drop_duplicates(inplace= True)\ntest.drop_duplicates(inplace=True)","a5a59672":"# drop UserName and ScreenName columns\ntrain.drop(['UserName', 'ScreenName'], axis=1, inplace=True)\ntest.drop(['UserName', 'ScreenName'], axis=1, inplace=True)","e11715ec":"# show columns with missing values\nplt.figure(figsize=(14,4))\nfor index, df in enumerate([train, test]):\n    plt.subplot(1,2, index+1)\n    sns.heatmap(df.isnull(), cmap='viridis', yticklabels= False).set_title('train' if index==0 else 'test')\n\nplt.show()","63244b2e":"# check number of missing values\nprint(train.isnull().sum())","e1ec99c6":"# check Location\nprint(train.Location.value_counts(normalize= True, dropna= False)[:30] *100)","32749eba":"train.Location = train.Location.str.split(',').str[0]","6e8e72ab":"print(train.Sentiment.value_counts(normalize=True) * 100)","1deffcd2":"# replace \"extremely positive\/negative\" with \"postive\/negative\"\ntrain[\"Sentiment\"] = train[\"Sentiment\"].str.replace(\"Extremely Negative\", \"Negative\")\ntrain[\"Sentiment\"] = train[\"Sentiment\"].str.replace(\"Extremely Positive\", \"Positive\")\n\ntest['Sentiment'] = test.Sentiment.str.replace('Extremely Positive', 'Positive')\ntest['Sentiment'] = test.Sentiment.str.replace('Extremely Negative', 'Negative')","57e23d42":"# plot of tweet sentiment distribution\nplt.figure(figsize=(6,6))\n\nsentiments = train.Sentiment.value_counts()\n\nsns.set_palette(\"coolwarm\")\nplt.pie(sentiments,\n        labels= sentiments.index,\n        autopct='%1.1f%%', startangle=80, \n        pctdistance=0.82, textprops={\"fontsize\": 14})\n\ncentreCircle = plt.Circle((0,0),0.65,fc='white')\nfig = plt.gcf()\nfig.gca().add_artist(centreCircle)\n\nplt.tight_layout()\nplt.title(\"How much of our tweet data is +ve -ve or neutral?\", x=0.53, fontsize= 16)\n\nplt.show()","69b33a1a":"# plot of top cities\/countries\nplt.style.use(\"fivethirtyeight\")\n\nplt.figure(figsize=(16, 6))\nlocation = sns.countplot(x= 'Location', data= train, hue=\"Sentiment\", order=train.Location.value_counts()[:10].index)\nlocation.set_title(\"Which places tweeted the most about COVID-19?\", y=1.05)\n\ndef axis_labels(ax):\n    ax.set_ylabel(\"Number of tweets\")\n    ax.set_xlabel(\"\")\n\naxis_labels(location)\n\nplt.show()","4f32e7d0":"# Converting the TweetAt column to date time \ntrain['TweetAt'] = pd.to_datetime(train['TweetAt'])\n\n# create day of the week and month columns\ntrain['day'] = train['TweetAt'].dt.dayofweek\ntrain['month'] = train['TweetAt'].dt.month\n\ndays = {0: 'Monday', 1: 'Tuesday', 2:'Wednesday', 3: 'Thursday', 4: 'Friday', 5: 'Saturday', 6: 'Sunday'}\nmonths = {0: 'January ', 1: 'February', 2:'March', 3: 'April', 4: 'May', 5: 'June', 6: 'July',\n      7:'August', 8:'September', 9: 'October', 10: 'November', 11: 'December'  }\n\ntrain[\"day\"] = train[\"day\"].map(days)\ntrain[\"month\"] = train[\"month\"].map(months)","16acf547":"print(f\"First tweet: {train['TweetAt'].dt.date.min()}, Last tweet: {train['TweetAt'].dt.date.max()}\")","993e120f":"plt.figure(figsize=(14, 6))\ndays = sns.countplot(x=\"day\", data=train)\ndays.set_title(\"What days were the most covid-related tweets made in 2020?\", \n                                             y=1.05)\n\ndef add_labels(ax, space):\n    for rect in ax.patches:\n        width = rect.get_width()\n        height = rect.get_height()\n        total = train.shape[0]\n        \n        ax.text(rect.get_x() + width\/2,\n               height + space,\n               '{}%'.format(int(np.round(height\/total*100))),\n                ha=\"center\")\n\nadd_labels(days, 100)\naxis_labels(days)\nplt.show()","741ff73d":"plt.figure(figsize=(14, 6))\nmonths = sns.countplot(train['month'])\nmonths.set_title(\"Which months in 2020 were the most covid-related tweets made?\", \n                                             y=1.05)\n\nadd_labels(months, 300)\naxis_labels(months)\nplt.show()","e1131438":"# check out the first two tweets\ndef tweets(df, n, col_name=\"OriginalTweet\"):\n    for tweet_no, tweet in enumerate(df[col_name][:n]):\n        print(tweet_no+1, tweet, '\\n')\n        print(\"*\" * 60, '\\n')\n        \ntweets(train, 10)","e842932d":"from wordcloud import WordCloud,STOPWORDS,ImageColorGenerator\nfrom PIL import Image","07433079":"def create_wordCloud(pattern):\n    \"\"\"create word cloud visualization\n    \n    arguments:\n        pattern (str): regex pattern to extract certain text from the data\n    \"\"\"\n    data = train[\"OriginalTweet\"].str.extractall(pattern)[0].value_counts()\n\n    data.index = data.index.map(str)                                                       # convert data index to string\n    data_wc = WordCloud(max_words = 500, colormap='Dark2_r', \n                        background_color='white').generate_from_frequencies(data)          # generate word cloud\n\n    # display the cloud\n    fig = plt.figure()\n    fig.set_figwidth(12) # set width\n    fig.set_figheight(12) # set height\n\n    plt.imshow(data_wc, interpolation=\"bilinear\")\n    plt.axis(\"off\")\n    plt.show()\n    \n# create word cloud of the most frequently used hashtags\nhashtag = r\"(#\\w+)\"\ncreate_wordCloud(hashtag)","8c33fa12":"# create word cloud of most frequent mentions\nmentions = r\"(@\\w+)\"\ncreate_wordCloud(mentions)","37d64ed3":"# combine train and test dataframes\ncombined = pd.concat([train, test], ignore_index= True)\n\n# select relevant features: tweet and Sentiments\ncombined = combined.loc[:, [\"OriginalTweet\", \"Sentiment\"]]\n\n# load stop words\nstop_word = stopwords.words('english')\n\ndef clean_tweet(text):\n    text = re.sub(r\"#\\w+\", \" \", text)            # remove hashtags\n    text = re.sub(r\"@\\w+\", \" \",text)             # remove mentions\n    text = re.sub(r\"http\\S+\", \" \", text)         # remove urls\n    text = re.sub(r\"[^a-zA-Z]\", \" \", text)        # remove non-words (digits, punctuations etc)\n    text = text.lower().strip()                  # convert tweet to lowercase and strip\n    \n    text = \" \".join([word for word in text.split() if not word in stop_word])           # remove stop words    \n    \n    text = \" \".join(nltk.word_tokenize(text))           # tokenize text\n      \n    return text\n\n# clean OriginalTweet and assign the data to an new \"tweet\" column\ncombined['tweet'] = combined['OriginalTweet'].apply(lambda x: clean_tweet(x))","69d62a28":"# print first few tweets to confirm the data is rid of non-word characters\ntweets(combined, 7, \"tweet\")","24745b49":"# most common words in our tweet data\ncorpus = \",\".join(word for word in combined.tweet)\nstopwords = set(STOPWORDS)\ntweet_wc = WordCloud(max_words = 500, colormap='Dark2_r', \n                        background_color='white', stopwords=stopwords).generate(corpus)   \n\n# display the cloud\nfig = plt.figure()\nfig.set_figwidth(10) # set width\nfig.set_figheight(10) # set height\n\nplt.imshow(tweet_wc, interpolation=\"bilinear\")\nplt.axis(\"off\")\nplt.show()","cadb312c":"# encode Sentiment label values\nle = LabelEncoder()\ncombined.Sentiment = le.fit_transform(combined.Sentiment)\n\n# split data back into training and validation sets and sets\ntrain = combined[: len(train)]\ntest = combined[len(train):].reset_index(drop=True)\n\n# split test test set\nX_test = test.tweet\ny_test = test.Sentiment\n\n\n# split training set into training and validation set\nX_train, X_val, y_train, y_val = train_test_split(train.tweet,\n                                                    train.Sentiment, test_size=0.2,random_state=42)","39ba7bb0":"# initialize vectorizer\nvectorizer = CountVectorizer(stop_words='english', ngram_range=(1,2), min_df=5).fit(X_train)\n\nX_train = vectorizer.transform(X_train)\nX_val = vectorizer.transform(X_val)\nX_test = vectorizer.transform(X_test)","99882893":"# intialize model and fit it on the training data\nlogmodel = LogisticRegression(max_iter=10000)\nlogmodel.fit(X_train, y_train)\n\n# check training accuracy\ncross_val_score(logmodel, X_train, y_train, cv=5, verbose=1, n_jobs=-1).mean()","b816f29d":"# extract labels from encoder\nlabels = list(le.classes_)","a1ca2ff6":"# make predictions\nval_pred = logmodel.predict(X_val)\ntest_pred = logmodel.predict(X_test)\n\n# print classification report\nprint(classification_report(val_pred, y_val, target_names= labels), '\\n')\nprint(classification_report(test_pred, y_test, target_names= labels))","7bc6ea4f":"# check test accuracy\nprint('accuracy score on validation set: ', accuracy_score(y_val, val_pred))\nprint('accuracy score on test set:', accuracy_score(y_test, test_pred))","2eea7fdc":"max_features = 20000                                            # maximum number of words to take from corpus\ntokenizer = Tokenizer(num_words=max_features, split=' ')            # initialize tokenizer\ntokenizer.fit_on_texts(train['tweet'].values)                   # fit tokenizer on training data\n\n\nmax_len = np.max(train.tweet.apply(lambda x :len(x)))\nvocab_length = len(tokenizer.word_index)","9407e53a":"print(\"Number of unique token:\", vocab_length)\nprint(\"Maximum sequence length:\", max_len)","fc0f2c9c":"# get text sequences from training and test dataframes\ntrain_x = tokenizer.texts_to_sequences(train['tweet'].values)\nX_test = tokenizer.texts_to_sequences(test['tweet'].values)\n\n\n# adding padding of zeros to obtain uniform length for all sequences\ntrain_x = pad_sequences(train_x, maxlen= max_len)\nX_test = pad_sequences(X_test, maxlen= max_len)\n\n# encode sentiment label values\ntrain_y_encoded = pd.get_dummies(train['Sentiment']).values\ny_test_encoded = pd.get_dummies(test['Sentiment']).values\n\n\n# split training data \nX_train, X_val, Y_train, y_val = train_test_split(train_x, train_y_encoded, test_size = 0.33, random_state = 42)","38090e9a":"print(train_x.shape, X_test.shape)\nprint(train_y_encoded.shape, y_test_encoded.shape)","70e107b5":"print(X_train.shape,Y_train.shape)\nprint(X_val.shape, y_val.shape)","4c06e04f":"embed_dim = 16\nlstm_out = 196\n\nmodel = Sequential()\nmodel.add(Embedding(vocab_length, embed_dim, input_length = max_len))\nmodel.add(SpatialDropout1D(0.4))\nmodel.add(LSTM(lstm_out, dropout=0.2, recurrent_dropout=0.2))\nmodel.add(Dense(3,activation='softmax'))\n\nmodel.compile(loss = 'categorical_crossentropy', \n              optimizer='adam',\n              metrics = ['accuracy'])\nprint(model.summary())","9152cac2":"model.fit(X_train, Y_train, \n          validation_data=(X_val, y_val), \n          epochs=5, batch_size= 32, \n          shuffle=True)","7ce5d6e4":"# evaluating model on test dataset\nmodel.evaluate(X_test, y_test_encoded, verbose=0)","b4beb1cb":"predictions = model.predict(X_test)\npredictions = np.argmax(predictions, axis=1)\n\n# predictions = model.predict_classes(X_test)","d5b03e86":"# classification report\nprint(classification_report(y_test, predictions, target_names= labels))","1095bc83":"#### Model Evaluation","98366e7f":"## Introduction\nHello thereee!\n\nIn this project, the goal is to build two models - `Logistic Regression and LSTM` - that can detect and classify the sentiments (`postive, negative or neutral`)  of COVID19-related tweets. We'll also do some exploratory data analysis along the way\n\nThe dataset used can be found [here](https:\/\/www.kaggle.com\/datatattle\/covid-19-nlp-text-classification)","f6b3c194":"#### Most Mentions","8f3469d1":"## Text Preprocessing\nNext step is to clean and prepare our tweet data for modeling. So,we proceed to:\n- Remove all hastages, links and numbers\n- Remove Stopwords (common words like \"the\", \"a\" etc)\n- Tokenize and Vectorize words, i.e, convert tweet words to numbers\n\nBut first, we combine the training and test dataframes, then keep just the features relevant to our model building - `OriginalTweet` and `Sentiment`","e11b2cbc":"#### Most common #hashtags","a5dfeeed":"### Location\nLet's check out the places around the world that tweeted the most about COVID. We'll also check out the mood of these tweets.","8082c131":"    - About 35% of the tweets were made on a Tuesday\/Wednesday, with Sunday having the least engagement","60915a9b":"## Exploratory Data Analysis","85cccd7e":"To make analysis easier, let's rename the \"Extremely Positive\", \"Extremely Negative\" labels to \"Positive\" and \"Negative\" respectively","ce912b6a":"## Import Libraries","c20648f3":"### Logistic Regression","665e08b5":"- About 21% of the Location data is missing\n- The Location values include both cities & countries and do not follow a consistent pattern - which makes it quite chaellnging to clean. However, I'll tidy up the column a bit by replacinig cases where we have for instance, `\"London, England\"` with just `\"London\"`; `\"Los Angeles, CA\"` with `\"Los Angeles\"`","a2a0abd4":"    - Former US president, Donald Trump is unsurprisingly among the most tagged persons. We also see the UK prime minister,   Boris Johnson and Indian Prime Minister, Narendra Modi also gathered a number of mentions\n    - CNN, BBCNews and SkyNews are the most tagged news channels, with Piers Morgan being the most tagged TV personality \n    - Retail companies such as Tesco, Walmart and Morrisons got a lot of mentions too","1f6a0b51":"## Modeling","74dd70a2":"    The model performs about the same on both the validation set and the given test dataset\n    \nNext, we check out how the LSTM model will perform on our data","e2ff3ca4":"    - The tweets are mostly either postive or negative, with just about 20% of the tweet data classified as neutral","10d1c1b7":"    The tweet data looks really unclean (well .. as expected) - but before proceeding to prepare our tweet text for modelling, let's explore the most frequent hashtags and top mentions in our data","dfa7bd1e":"### Tweet At\nAs `TweetAt` contains dates the tweets in our data were made, let's proceed to find out:\n- the period range our tweet data was gathered\n- the most frequent day(s) of the week and month(s) users made covid-related tweets\n\nFor the latter, we would need to create a new day and month column","6f18e10d":"#### Model Building","00ecab47":"### LSTM","0d99b57c":"#### Model Training","b2612e81":"##### Author: Ayomide Aderonmu","2aaa54f1":"    - Most covid-related tweets seem to come from four major countries - the United Kingdom, the USA, Cananda and India.\n    - London and New York lead the way in terms of cities that tweeted the most about covid19\n    - We also observe a pattern: there are more positive tweets than negative in all cities\/countries, except England - well this actually follows the general trend in our data, as we have more postive tweets than negative and more negative ones than neutral","74ed2f4a":"    - As seen, the LTSM algorithm yields a better performance on our data (84% accuracy) than the Logistic Regression (79% accuracy)\n    - While 79-84% is a fairly good score for accuracy, the performance of each model can still be further improved by tuning necessary parameters","863af69d":"- A whooping 64% of the tweets were made in April! \n- This could perhaps be because it was around this period the number of cases and death toll first skyrocketed.  \n- According to the timeline of COVID-19 events stated in this [article](https:\/\/www.thinkglobalhealth.org\/article\/updated-timeline-coronavirus), the Week of March 30\u2013April 4 saw the Worldwide coronavirus cases exceed one million; with millions of Americans filing for unemployment and major sporting events such Wimbledon Tennis Tournament getting canceled for the first time in a very long time. These were very serious and sudden events that shook the world and hence got people talking and tweeting a lot.","662f4a0e":"## Understanding the Data","8b476ace":"    Our tweet data, which contains covid-related tweets made only in 2020, was collected over an 11-month period (January 4, 2020 through to December 4, 2020)","77d06896":"### Sentiment","da9e511d":"### Tweets","8299ec74":"#### Duplicates and Null values","8c6c2987":"    - UserName and ScreenName are randomly generated fields for unique identification purpose only. Their values wouldn't   impact our model, hence, we will be dropping both columns\n    - Location is the only column with missing values\n    - TweetAt, which contains times the tweets were made, has an object datatype - we'll be converting this to a datetime   datatype\n    - Sentiment is the target variable"}}