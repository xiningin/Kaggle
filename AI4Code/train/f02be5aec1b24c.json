{"cell_type":{"6abc5f2a":"code","f740bfa5":"code","5fac2a8e":"code","7560f1b8":"code","44125c19":"code","ae316819":"code","23328002":"code","df2a0255":"code","d7a78cbd":"code","921514cd":"code","62c2f65b":"code","b230b0ac":"code","ce4bb911":"code","82f45477":"code","89203a20":"code","9e44e765":"code","648e81ff":"code","4b457b5a":"code","fa6c5006":"code","b7d0d209":"code","6296e95b":"code","d6b5337d":"code","e2ec29f0":"markdown","831aae9c":"markdown","1f4153ee":"markdown","c40b29e6":"markdown","827a9b09":"markdown","f8774106":"markdown","26ff92f3":"markdown","a4f43369":"markdown","9f2719fc":"markdown","e8417e45":"markdown","1104a792":"markdown","17801c72":"markdown","46709024":"markdown","7228a86c":"markdown","dae04ed9":"markdown","88bf88d5":"markdown","ba894245":"markdown","398cd122":"markdown","128dfbd1":"markdown","263d9126":"markdown","e6c405ea":"markdown","e81e5178":"markdown","7f3f9514":"markdown","2d86cf59":"markdown","c0f854cc":"markdown"},"source":{"6abc5f2a":"%reset","f740bfa5":"import datetime\nimport os\nimport random\nimport numpy as np\nimport pandas as pd\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Input, Dropout, Dense, RepeatVector, TimeDistributed, Flatten, LSTM\nfrom tensorflow.keras import Model\n\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n\n########################################################################\n# global variables\n########################################################################\n# Resampled every signal to this rate for consistency\nBASIC_SRATE = 128 #Hz\nprint('Basic sampling rate(Hz):',BASIC_SRATE)\n\n# >------ parent db directory ------------------------------------------\nDB_NPY = '..\/input\/ecg-lead-2-dataset-physionet-open-access\/db_npy'\nprint('DATASET DIR ::',DB_NPY)\n\n# >------ load standard labels ------------------------------------------\nstd_annotations = os.path.join(DB_NPY,'annotations.txt')\nprint('STD ANN FILE ::',std_annotations)\nstd_ant = np.loadtxt(std_annotations, dtype='str',delimiter=\"\\t\")\n\n#------------------------------------------------------------------------\n# Seperate out stand beat and non-beat annotations using 'annotations.txt'\nbeat_ants = []\nnon_beat_ants = []\nprint('\\nStandard Labels::')\nfor a in std_ant:\n    # a[0] =  # standard label (char)\n    # a[1] =  # type of label\n    # a[2]  = # description (str)\n    print(a[0]+'\\t'+a[1]+'\\t'+a[2])\n    if a[1]=='b': #<<--- is a beat-annotation\n        beat_ants.append(a[0])\n    else:\n        non_beat_ants.append(a[0])\n\nprint('')\nprint('beat-annotation:',len(beat_ants))\nprint('non-beat-annotation:',len(non_beat_ants))\n#-----------------------------------------------------------------------------\ngbreak = '____________________________________________________________________'\n#-----------------------------------------------------------------------------\n# data file identifiers \ng_BEAT = '_BEAT.npy'       #<<--- beat annotations\ng_NBEAT = '_NBEAT.npy'     #<<--- non-beat annotations\ng_SIG2 = '_SIG_II.npy'     #<<--- Signal Data (Removed manual gain)","5fac2a8e":"#<< --- define custom labels. \n_N = '_N'  # N - normal class - Non-Ectopic\n_S = '_S'  # S - SupraVentricular Ectopic Beats (SVEBs)\n_V = '_V'  # V - Ventricular Ectopic Beats (VEBs)\n_F = '_F'  # F - Fusion Beats\n_Q = '_Q'  # Q - Unrecognized\n_X = '_X'  # X - Unmapped - if you don't want to use a lable, map it under this class - it should not be used in classification\n\ncustom_ants = [ _N, _S, _V, _F, _Q ]  # as recomended by AAMI\n\n# define a mapping dictionary #----------------------------------\ncustom_cols = { _N:'tab:green',\n                _S:'tab:red',\n                _V:'tab:blue',\n                _F:'tab:purple',\n                _Q:'yellow',\n                _X:'tab:gray'}\ncustom_mapping = {\n                    #<--- Normal Class\n                    'N': _N, \n                    'L': _N,\n                    'R': _N,\n                    'B': _N,\n    \n                    #<--- SVEB\n                    'A': _S,\n                    'a': _S,\n                    'J': _S,\n                    'S': _S,\n    \n                    #<--- VEB\n                    'V': _V, \n                    'r': _V,\n    \n                    #<--- FUSION\n                    'F': _F,\n    \n                    #<---* Supraventricular escape - aami says its normal but should be mapped to _S\n                    'e': _S,\n                    'j': _S,\n                    'n': _S,\n    \n                    #<---* Ventricular escape - aami says its normal but should be mapped to _V\n                    'E': _V,\n    \n                    #<--- Paced beats are unmapped - dont use record containing paced beats (mitdb - 102,104,107,217)\n                    'f': _X,\n                    '\/': _X,\n    \n                    #<--- Unrecognised or Unclassifiable\n                    'Q': _Q,\n                    '?': _Q,\n                }","7560f1b8":"##############################################################################################################################3\n# CLASS DEFS\n##############################################################################################################################3\n\n#<-------------------------------------------------------------------------------------------------- Represent one ECG database\nclass ecgDatabase:\n    def __init__(self, db_name, db_path):\n        self.db_name = db_name\n        self.db_path = db_path\n        self._prepare()\n        \n    def _prepare(self):\n        # 1. read record list\n        self.record_list = np.loadtxt(os.path.join(self.db_path,'RECORDS'), dtype='str',delimiter=\"\\n\")\n        if len(self.record_list.shape)==0:\n            self.record_list = np.array([self.record_list])\n        \n        # 2. prepare a dictionary of ecgRecord objects\n        self.records = {}\n        for irec in self.record_list:\n            self.records[irec] = ecgRecord(irec,os.path.join(self.db_path),self.db_name) \n            #<--- records are not loaded initially, will be loaded dynamically\n        return\n        \n    def get_record(self, rec_name):\n        if rec_name in self.records.keys():\n            this_rec = self.records[rec_name]\n            if not this_rec.isloaded:\n                this_rec.loadfromdisk()  #<--- Loads records dynamically, on call to this function\n            return this_rec\n        else:\n            return None  #<--- No record with name <rec_name>\n\n#<------------------------------------------------------------------------------------------------- Represent one ECG Record        \nclass ecgRecord:\n    def __init__(self, arg_recname, arg_path, db_name):\n        self.dbname = db_name\n        self.path = arg_path\n        self.name = arg_recname\n        self.isloaded = False\n        \n#<~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ REQUIRED\n    def loadfromdisk(self, verbose=0):\n        if verbose>0:\n            print('\\nLoading Record:', self.name)\n        #<<---------------------------------------------\n        # Load Signal Data\n        #<<---------------------------------------------\n        self.signal_file = os.path.join(self.path,self.name + g_SIG2)\n        self.iSignal = np.load(self.signal_file)\n        if verbose>0:\n            print('Signal Length:',self.iSignal.shape)\n            print('Signal Duration:',len(self.iSignal)\/BASIC_SRATE, 'sec')\n        \n        #<<---------------------------------------------\n        # Load Beat-Annotation (Labels)\n        #<<---------------------------------------------\n        self.beat_file = os.path.join(self.path,self.name + g_BEAT)\n        self.iBeat = np.load(self.beat_file)\n        if verbose>0:\n            print('#Beats:',self.iBeat.shape)\n        # seperate out R-peaks and Labels (note both are in string format)\n        self.iRpeaks = self.iBeat[:,0].astype('int') #<<-- convert sampling locations to integer\n        self.iLabels = self.iBeat[:,1]               #<<-- labels remain as string\n        self.nos_beats = len(self.iRpeaks)\n        \n        #<<---------------------------------------------thats it, Non-Beat-Annotation not required\n        self.isloaded = True\n        return self.isloaded\n    \n#<~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ REQUIRED\n    # for mapping custom labels   ** rec.mLabels = rec.map_beat_labels(custom_mapping)\n    # must be called after loading record to be able to use custom mapping\n    def map_beat_labels(self, c_mapping, c_colors):\n        self.mLabels = np.zeros(len(self.iLabels), dtype='U2')\n        self.mColors = np.zeros(len(self.iLabels), dtype='U10')\n        for i in range(0,len(self.iLabels)):\n            self.mLabels[i]=c_mapping[self.iLabels[i]]\n            self.mColors[i]=c_colors[self.mLabels[i]]\n            \n        return len(self.mLabels)\n    \n#<~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Fixed Duration Signal \n    # Gets Signal Data of fixed duration along with indices of r-peaks in the range\n    # pf and pt are measured in samples not seconds \n    # to convert use, samples =  int(seconds \/ BASIC_SRATE)   ** must be integers\n    # can be negative too\n    \n    def get_true_range(self,pf,pt):\n\n        N = len(self.iSignal)\n        if pt<pf:\n            pt,pf = pf,pt # swap\n            \n        if pf<0:\n            pf=0\n        elif pf>=N:\n            pf=N-1\n            \n        if pt<0:\n            pt=0\n        elif pt>N:\n            pt=N\n        return pf, pt\n            \n    def get_signal_slice_fd(self,pf, pt):\n        signal_slice = self.iSignal[pf:pt]\n        signal_list = np.where((self.iRpeaks>=pf) & (self.iRpeaks<pt))[0]\n        return signal_slice, signal_list, pf, pt\n\n#<~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Variable Length Representation\n    # Gets Signal data of beat at 'beat_index'. (Variable Length)\n    # On the left, take signal data upto R-peak of beat 'minus_i' indices before it and trim by 'signal_trim_left'\n    # On the right, take signal data upto R-peak of beat 'plus_i' indices after it and trim by 'signal_trim_right'\n    # trim params can be negative too\n    def get_signal_slice_biV(self, beat_index, minus_i, plus_i ,signal_trim_left, signal_trim_right ):\n        RpeakL = self.iRpeaks[beat_index-minus_i]+signal_trim_left\n        RpeakC = self.iRpeaks[beat_index]\n        RpeakR = self.iRpeaks[beat_index+plus_i]-signal_trim_right\n        pf,pt = RpeakL,RpeakR\n        signal_slice=self.iSignal[pf:pt]\n        signal_list = np.where((self.iRpeaks>=pf) & (self.iRpeaks<pt))[0]\n        return signal_slice, signal_list, pf, pt\n\n    # a shorter version of above function. use when only the signal data is required\n    def get_signal_slice_biV_(self, beat_index, minus_i, plus_i ,signal_trim_left, signal_trim_right ):\n        return self.iSignal[self.iRpeaks[beat_index-minus_i]+signal_trim_left:self.iRpeaks[beat_index+plus_i]-signal_trim_right]\n\n#<~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Fixed Length Representation\n    # Gets Signal data of beat at 'beat_index'. (Fixed Length)\n    def get_signal_slice_biF(self, beat_index, left_samples, right_samples ):\n        Rpeak = self.iRpeaks[beat_index]\n        pf, pt =  Rpeak-left_samples, Rpeak+right_samples\n        signal_slice = self.iSignal[pf:pt]\n        signal_list = np.where((self.iRpeaks>=pf) & (self.iRpeaks<pt))[0]\n        return signal_slice, signal_list, pf, pt\n    \n    # a shorter version of above function. use when only the signal data is required\n    def get_signal_slice_biF_(self, beat_index, left_samples, right_samples ):\n        return self.iSignal[self.iRpeaks[beat_index]-left_samples:self.iRpeaks[beat_index]+right_samples]\n#-------------------------------------------------------------------------------------------------------------------\n# NOTE: be careful with out of range indices - use rec.get_true_range(pf,pt)\n#-------------------------------------------------------------------------------------------------------------------\n\n# functions for plotting\nyll, yul, ycl = -3, 3.5, -2.8\n\ndef plot_beat(signal_slice, signal_list, pf, pt, ph,\n                show_label_ticks=False, show_custom_labels=True, show_custom_vline=False,\n                w=0.012, h=4, r=0.5):\n    \n    plt.figure(figsize=(w*len(signal_slice),h), constrained_layout=True)\n    plt.ylim(yll, yul)\n    plt.xlim(pf,pt)\n    \n    ilen = len(signal_list)\n    _range = np.arange(pf,pt,BASIC_SRATE)\n    _peaks = rec.iRpeaks[signal_list]\n    _labels = rec.iLabels[signal_list]\n    _colorL = rec.mColors[signal_list]\n    \n    # plot the signal\n    plt.plot(np.arange(pf,pt,1), signal_slice, color='black',linewidth=0.8)\n    plt.vlines(ph,yll,yul,linewidth=0.8,color='black' )\n    \n    # plot ticks\n    if show_label_ticks:\n        plt.xticks(_peaks,_labels) #-pf*BASIC_SRATE\n        plt.grid( axis='x')\n    else:\n        plt.xticks(_range,np.round(_range\/BASIC_SRATE,2))#,rotation=60)\n\n    # plot labels\n    if show_custom_labels:\n        plt.scatter(_peaks ,np.zeros(ilen)+ycl, c=_colorL)\n        if show_custom_vline:\n            for j in range(0,ilen):\n                plt.annotate(signal_list[j],xy=(_peaks[j],ycl+0.3) )\n                plt.vlines(_peaks[j],yll,yul,linewidth=0.4,color=_colorL[j])\n\n    \n    plt.show()\n    return\n#\n\n#===============================================================\n# PERFORMANCE =================================================\n#===============================================================\n#------------------------------------------------------------------\n# Confusion Matrix\n#------------------------------------------------------------------\ndef _print_header(class_labels):\n    g_HSTR=''   # HEADER ROW for printing confusing matrix\n    for i in range(0,len(class_labels)):\n        g_HSTR+='\\t'+str(class_labels[i])\n    return  g_HSTR\ndef _print_rows(cm_row, nos_labels):\n    g_RSTR = ''\n    for j in range(0,nos_labels):\n        g_RSTR += '\\t'+ str(cm_row[j])\n    return g_RSTR\ndef print_conf_matrix(conf_matrix, suffix, class_labels):\n    g_CMSTR=(suffix+'T\\\\P' + _print_header(class_labels)+'\\n')\n    nos_l=len(class_labels)\n    for i in range(0,nos_l):\n        g_CMSTR+=(suffix+str(class_labels[i]) + _print_rows(conf_matrix[i],nos_l )+'\\n')\n    return g_CMSTR\n\n#------------------------------------------------------------------\n# Performance Measures\n#------------------------------------------------------------------\ndef _get_performance(conf_matrix):\n    nos_class = len(conf_matrix[0,:]) # len of 0th row\n    perf_measures_array = np.zeros((0,11),dtype ='float64')\n    for i in range(0,nos_class):\n        \n        CLASS_ACTUAL = np.sum(conf_matrix[i,:]) #<---- ROW SUM = NOS TRUE CLASS\n        CLASS_PRED = np.sum(conf_matrix[:,i])      #<---- COL SUM = NOS PRED CLASS\n        MSUM = np.sum(conf_matrix)  # = TP + FN + FP + TN\n        \n        # compute TP, TN, FP, FN ---------------------------- \n        TP = conf_matrix[i,i]\n        FP =  CLASS_PRED - TP\n        FN = CLASS_ACTUAL - TP\n        TN =  MSUM- FN - FP - TP #<------------ this methods counts more than true negative\n        #TN = np.sum(conf_matrix[np.diag_indices(nos_class)]) - TP..\n\n        #Accuracy #<-= how many samples correctly classifed out of all samples\n        ACC = (TP+TN)   \/   ( MSUM)  \n        \n        #Precision = TP\/CLASS_PRED    #<- = how many samples correctly predicted as true out of all samples predicted as true\n        PRE = (TP)      \/   (TP+FP)         #Presicion\n        \n        #Sensetivity = TP\/CLASS_ACTUAL #<- = how many samples correctly predicted as true out of all actually true samples\n        SEN = (TP)      \/   (TP+FN)         #Sensitivity\/Recall\n        \n        #Specificity #<-= how many samples correctly predicted false out of all samples predicted as false\n        SPF = (TN)      \/   (TN+FP)         \n        \n        # F1-Score #<-= 2*TP \/ (CLASS_ACTUAL + CLASS_PRED) \n        F1S = 2*PRE*SEN \/   (PRE+SEN)       #F1 score #<-= harmonic mean of Precision and Sensetivity\n\n        prefi = np.array([CLASS_ACTUAL , CLASS_PRED, TP, FN, FP, TN, ACC, PRE, SEN, SPF, F1S])\n        perf_measures_array = np.vstack((perf_measures_array,prefi))\n        \n    return perf_measures_array, nos_class\ndef print_performance(conf_matrix, class_labels, do_round=-1):\n    #header_string = 'Class\\tACC\\tPRE\\tSEN\\tSPF\\tF1S'\n    header_string = 'Class\\t#True\\t#Pred\\tTPs\\tFNs\\tFPs\\tTNs\\tACC\\tPRE\\tSEN\\tSPF\\tF1S'\n    perf_measures, nos_class = _get_performance(conf_matrix)\n    if len(class_labels)!=nos_class:\n        print('WARNING:: Class label count mismatch!! Cannot print performance')\n        return -1\n    #nos_class = len(perf_measures[:,0])\n    print('Performance for '+str(nos_class)+' classes')\n    print (header_string)\n    for i in range(0, nos_class):\n        if do_round<0:\n          perf_i = perf_measures [i,:]\n        else:\n          perf_i = np.round(perf_measures [i,:],do_round)\n          \n        print(\n              str(class_labels[i])+'\\t'+\n              str(perf_i[0])+'\\t'+\n              str(perf_i[1])+'\\t'+\n              str(perf_i[2])+'\\t'+\n              str(perf_i[3])+'\\t'+\n              str(perf_i[4])+'\\t'+\n              str(perf_i[5])+'\\t'+\n              str(perf_i[6])+'\\t'+\n              str(perf_i[7])+'\\t'+\n              str(perf_i[8])+'\\t'+\n              str(perf_i[9])+'\\t'+\n              str(perf_i[10])\n              )\n    return nos_class\n\t","44125c19":"sel_db_name = 'mitdb' # [ select from - 'mitdb', 'svdb', 'incartdb']\nsel_db_path = os.path.join(DB_NPY, sel_db_name+'_npy')\nsel_db_files = os.listdir(sel_db_path)\nprint(sel_db_path,',', len(sel_db_files) , 'files')\n\ndbo = ecgDatabase(sel_db_name, sel_db_path)\nprint(dbo.record_list.shape,dbo.record_list)\n","ae316819":"recn = np.random.choice(dbo.record_list,size=1)[0]  #'116'\nrec = dbo.get_record(recn)\nprint(rec.name, '\\t'+str(rec.nos_beats)+ ' beats')\nnos_map = rec.map_beat_labels(custom_mapping, custom_cols)# <<-- custom lables\nprint('Mapped Labels:', nos_map)","23328002":"timestamp_start = datetime.datetime.now()\n# Find Count of all beats\nprint('Recod:',rec.dbname+'\/'+rec.name, '\\t'+str(rec.nos_beats)+ ' beats')\nprint(gbreak)\nprint('Standard Label Count')\nsaDict={}\nsum = 0\nfor sa in  beat_ants:\n    saDict[sa] = np.where(rec.iLabels==sa)[0]\n    isum = len(saDict[sa])\n    sum+=isum\n    print(sa,'\\t\\t',isum)\nprint('__','\\t\\t',sum)\nprint(gbreak)\nprint('\\nCustom Label Count')\ncaDict={}\nsum = 0\nfor ca in  custom_ants:\n    caDict[ca] = np.where(rec.mLabels==ca)[0]\n    isum = len(caDict[ca])\n    sum+=isum\n    print(ca,'\\t\\t',isum)\nprint('__','\\t\\t',sum)\nprint(gbreak)\n# RR-intervals  **NOTE:: SKIPPING FIRST AND LAST BEAT OF THE RECORD\nrp_p, rp_c, rp_n = rec.iRpeaks[0:-2], rec.iRpeaks[1:-1], rec.iRpeaks[2:]\nmLabels_p, mLabels_c, mLabels_n = rec.mLabels[0:-2],rec.mLabels[1:-1],rec.mLabels[2:]\n\nrri_prev = (rp_c-rp_p)                 #<-- previous rr-interval\nrri_next = (rp_n-rp_c)                 #<-- next rr-interval\nrri_mean = (rri_prev + rri_next)\/2       #<-- mean of prev and next\nrri_delta = rri_next - rri_prev        #<-- delta b\/w prev and next\nrri_delta_abs = np.abs(rri_delta)       #...... for a normal rhythm, rri_abs_delta should be very small\n\nABS_DELTA_LIM = int(0.04*BASIC_SRATE)  #-- define a limit on rri_delta_abs for a beat to be considered Normal rhythm = 0.04 sec\n\n\n# Any Normal: All Normal beats\nNN_all = np.where((mLabels_c==_N))[0]+1\n\n# True Normal: a Normal beat surronded by normal beats on both sides\nNN_true = np.where((mLabels_p==_N) & (mLabels_c==_N) & (mLabels_n==_N))[0]+1\n\n# Semi Normal: Normal beat surronded by atleast one abnormal beat on any side\nNN_semi = np.where((mLabels_c==_N) & ((mLabels_p!=_N) | (mLabels_n!=_N)))[0]+1\n\n# Absolute Normal: True Normal and delta_abs_rri < ABS_DELTA_DUR\nNN_abs = np.where((mLabels_p==_N) & (mLabels_c==_N) & (mLabels_n==_N)\n                  & (rri_delta_abs<=ABS_DELTA_LIM))[0] + 1\n\n# Abnormal beats\nAA_all =np.where((mLabels_c!=_N))[0] + 1 \nSS_all =np.where((mLabels_c==_S))[0] + 1\nVV_all =np.where((mLabels_c==_V))[0] + 1\nFF_all =np.where((mLabels_c==_F))[0] + 1\nQQ_all =np.where((mLabels_c==_Q))[0] + 1\n\n\n# Unmapped beats\nXX_all = np.where((mLabels_c==_X))[0] + 1\n\n\nprint(\"\\nNormals\\t\\t\", len(NN_all))\nprint(\" True-type\\t\", len(NN_true))\nprint(\" Semi-type\\t\", len(NN_semi))\nprint(\" Abs-type\\t\", len(NN_abs))\nprint(\"\\nAbnormals\\t\", len(AA_all))\nprint(\" S-type\\t\\t\", len(SS_all))\nprint(\" V-type\\t\\t\", len(VV_all))\nprint(\" F-type\\t\\t\", len(FF_all))\nprint(\" Q-type\\t\\t\", len(QQ_all))\nprint(\"\\nUnmapped\\t\", len(XX_all))\nprint(gbreak)\nprint('\\nElapsed time = ',datetime.datetime.now() - timestamp_start)","df2a0255":"lsamp, rsamp = int(0.4*BASIC_SRATE),int(0.4*BASIC_SRATE)+1\nseqlen=lsamp+rsamp\nprint(lsamp,',',rsamp,',',seqlen)","d7a78cbd":"timestamp_start = datetime.datetime.now()\n# select a bet to train upon\nNNc = NN_abs #<--- learn the Absolute normals\ntrain_on = min(10,len(NNc)) # train on less number of beats\nfor_train_N = np.random.choice(NNc, size=train_on, replace=False )\nlater_recr = np.random.choice(for_train_N, size=1, replace=False ) #<--- will use this beat for reconstruction\n\n#---------------------------------------------Prepare for Training\nsequence_length = seqlen\nnncount = len(for_train_N)\ndata_listN=[]\nprint('Preparing...',nncount )    \n # reshape input into [samples, timesteps, features]\nfor i in range(0,nncount):\n    b_index = for_train_N[i]\n    signal_, _,_,_ = rec.get_signal_slice_biF(b_index,lsamp, rsamp)\n    assert (sequence_length==len(signal_))\n    data_listN.append(signal_)\n    plt.plot(signal_, linewidth=0.5)\nplt.show()\ndata_ = np.array(data_listN).reshape((len(data_listN), sequence_length, 1))\nnp.random.shuffle(data_)\n\nprint(rec.dbname+'\/'+rec.name,'Done.')\nprint('\\nElapsed time = ',datetime.datetime.now() - timestamp_start)","921514cd":"def get_model_LAE(n_in, lstm_cells, Print_Summary=True, do_compile= True, initial_lr = 0.01):\n    # define model - simple model with 1 layer\n    \n    lmodel = Sequential(name='LSTM_AutoEncoder') #samples, timesteps, features\n    #<<-- add batch Norm???- \n    lmodel.add(Input(shape=(n_in,1)))\n    lmodel.add(\n                tf.keras.layers.BatchNormalization(axis=-2,\n                                        momentum=0.99,\n                                        epsilon=0.001,\n                                        center=False,\n                                        scale=True,\n                                        trainable=False,\n                                        virtual_batch_size=None,\n                                        adjustment=None,\n                                        name=\"BatchNorm\")\n                )\n    lmodel.add(LSTM(lstm_cells,  name=\"LEncoder\",activation='elu', input_shape=(n_in,1)))  #<<-- why elu?? - try different things here?\n    lmodel.add(RepeatVector(n_in, name=\"Rep\"))\n    lmodel.add(LSTM(lstm_cells, name=\"LDecoder\", activation='elu', return_sequences=True))\n    lmodel.add(TimeDistributed(Dense(1, name=\"TimeDist\")))\n\n    if do_compile:\n        lmodel.compile(optimizer=tf.optimizers.Adam(learning_rate=initial_lr),loss='mean_absolute_error',) #<--- any other optimizer?\n\n    if Print_Summary:\n        print(lmodel.summary())\n    return lmodel\n\n#========================================================================================\ne_ylim = 2.0\ndef Rec_Error(delta_error): # #<--- Which one to choose as reconstruction error?\n    return np.mean(delta_error) #<--- Mean Absolute Error\n    #return np.sum(delta_error)  #<--- Summation of Absolute Error\n    #return np.mean(delta_error**2) #<--- Mean Squared Error","62c2f65b":"def _deltaLR( epoch, lr):\n    global hist_LR\n    \n    #---------------------------------------------------\n    new_lr = lr      # not controlled\n    #---------------------------------------------------\n    \n    hist_LR[epoch]=new_lr\n    return new_lr\n\ndef _epoch_end( _epoch, _logs):\n    if _epoch%50 == 0:\n        print('epoch:', _epoch)\n    return\n","b230b0ac":"#========================================================================================\n# any callbacks here\ncb_ES_loss = tf.keras.callbacks.EarlyStopping(\n        monitor='loss', \n        min_delta=0.001, \n        patience=50, \n        verbose=1,\n        mode='auto',\n        baseline=None,\n        restore_best_weights=True)\ncb_BASE = tf.keras.callbacks.LambdaCallback(\n        on_train_begin =  None, #lambda logs: _start_train() ,\n        on_epoch_begin =  None, #lambda epoch,logs: _start_epoch(epoch) ,\n        on_batch_begin =  None, #lambda batch,logs: _start_batch(batch),\n        on_batch_end =    None, #lambda batch,logs: _record_batch(logs['accuracy'], logs['loss']) ,\n        on_epoch_end =    lambda epoch,logs: _epoch_end(epoch, logs),\n        on_train_end =    None #lambda logs: _print_results(logs['accuracy'], logs['loss'],logs['val_accuracy'], logs['val_loss'])\n        )\ncb_LRS = tf.keras.callbacks.LearningRateScheduler(_deltaLR, verbose=0)\ncb_list = [cb_ES_loss,cb_LRS, cb_BASE] ","ce4bb911":"nos_cell = int(seqlen*0.75)\nmodel = get_model_LAE(seqlen,nos_cell,initial_lr=0.001) #<-- start with high learn rate?","82f45477":"n_epchoc = 1000\nb_size = int(nncount*0.25) # 25% batch size\nhist_LR = np.zeros(n_epchoc)-1\n\ntimestamp_start = datetime.datetime.now()\nhistory = model.fit(data_, data_, \n                    epochs=n_epchoc,\n                    shuffle=True, \n                    batch_size=b_size, \n                    verbose=0, \n                    callbacks=cb_list)\nprint('Recent Loss:',history.history['loss'][-5:])\nprint('\\nElapsed time = ',datetime.datetime.now() - timestamp_start)","89203a20":"fig, ax = plt.subplots(2, 1, figsize=(16,8),  constrained_layout=True)\nax[0].set_title('Loss')\nax[0].plot(history.history['loss'],color='tab:red', label='train_loss')\n\nhist_LRd = hist_LR[np.where(hist_LR>=0)]\nax[1].set_title('Learning Rate')\nax[1].bar(np.arange(0,len(hist_LRd),1),hist_LRd,color='tab:green')\n\nplt.legend()\nplt.show()","9e44e765":"# predict from training Example\nsignal_, _,_,_ = rec.get_signal_slice_biF(later_recr[0],lsamp, rsamp)\nassert (seqlen == len(signal_))\nsequence = signal_.reshape((1,seqlen, 1))\nypred = model.predict(sequence, verbose=0)\ndE = np.abs(ypred[0,:,0] - sequence[0,:,0])\n\nfig,ax = plt.subplots(2,1,figsize=(12,12))\nax[1].fill_between(np.arange(0,seqlen,1), ypred[0,:,0], sequence[0,:,0], color = 'tab:purple')\nax[1].set_title('Difference')\nax[0].plot(sequence[0,:,0], label='true', color='tab:green', linewidth=0.6)\nax[0].plot(ypred[0,:,0], label='pred', color='tab:red', linewidth=0.8)\nax[0].legend()\nplt.show()\nprint('Error:', Rec_Error(dE))","648e81ff":"# predict other beats # randomly choose one normal and one abnormal\n\nbii_normal = np.random.choice(NN_all)\nbii_abnormal = np.random.choice(AA_all)\n#print('Normal #',bii_normal, ' Label:', rec.mLabels[bii_normal],rec.iLabels[bii_normal])\nfor bii,tx in zip([bii_normal,bii_abnormal],['Normal#','Abnormal#']):\n    print(tx, bii, ' Label:', rec.mLabels[bii],rec.iLabels[bii])\n    signal_, _,_,_ = rec.get_signal_slice_biF(bii,lsamp, rsamp)\n    assert (seqlen == len(signal_))\n    sequence = signal_.reshape((1,seqlen, 1))\n    ypred = model.predict(sequence, verbose=0)\n    dE = np.abs(ypred[0,:,0] - sequence[0,:,0])\n    print('Error:', Rec_Error(dE))\n    \n    fig,ax = plt.subplots(2,1,figsize=(12,12))\n    ax[1].fill_between(np.arange(0,seqlen,1), ypred[0,:,0], sequence[0,:,0], color='tab:purple')\n    ax[1].set_title('Difference')\n    ax[0].plot(sequence[0,:,0], label='true', color='tab:green', linewidth=0.6)\n    ax[0].plot(ypred[0,:,0], label='pred', color='tab:red', linewidth=0.8)\n    ax[0].legend()\n    plt.show()\n    ","4b457b5a":"qN = NN_all #<----- choose from all Normal beats\nqA = AA_all #<----- choose from all AbNormal beats\n\ntakes = min(len(qN),len(qA)) # take which ever is minimum\n\nqN = np.random.choice(qN, size=takes, replace=False)\nqA = np.random.choice(qA, size=takes, replace=False)\n\nq = np.hstack((qA,qN))\nprint(\"...testing on samples:\",len(q),'[', len(qN),'+', len(qA),']')\n\npred_dict = {_X:[]}\nfor cant in custom_ants:\n    pred_dict[cant]= []\n    # initialized dict\ntimestamp_start = datetime.datetime.now()\nfor bindex in q :\n    signal_, _,_,_ = rec.get_signal_slice_biF(bindex,lsamp, rsamp)\n    assert (seqlen == len(signal_))\n    sequence = signal_.reshape((1,seqlen, 1))\n    yhat = model.predict(sequence, verbose=0)\n    dE = np.abs(yhat[0,:,0] - sequence[0,:,0])\n    MAE = Rec_Error(dE)\n    pred_dict[rec.mLabels[bindex]].append(MAE)\nprint('Done!')\n# plot the predictions   \nplt.figure(figsize=(12,4),dpi=150)\nplt.ylim(0,e_ylim)\nfor _key in pred_dict.keys():\n    #print(_key)\n    plt.scatter(np.arange(0,len(pred_dict[_key]),1),pred_dict[_key], color=custom_cols[_key],marker='.',label=_key[1:])\n#plt.hlines(20,0,100)\nplt.ylabel('Recon_Error')\nplt.xlabel('Sample#')\nplt.legend()\nplt.show()\n#-------------------------------------\nprint('done!')\n#print(pred_dict)\nprint('\\nElapsed time = ',datetime.datetime.now() - timestamp_start)","fa6c5006":"\nNpd = np.array(pred_dict[_N])\n#Normalize Npd?\n\nApd = np.hstack((np.array(pred_dict[_S]),\n                         np.array(pred_dict[_V]),\n                         np.array(pred_dict[_F]),\n                         np.array(pred_dict[_Q]),\n                         np.array(pred_dict[_X])))\ndata_ = np.hstack((Npd, Apd))\nprint('Used Samples:', Npd.shape, '+',Apd.shape,'=', data_.shape)\n\npthresh = 0.98 #98% data - good enough?\n\nfig,ax = plt.subplots(1,2,figsize=(8,4),dpi=150 )\n\nn_pbin, n_bin, _ = ax[0].hist( Npd, bins=100,range=(0, np.max(Npd)),  density=True)\nC_pbin, C_bin, _ = ax[1].hist( Npd, bins=100,range=(0, np.max(Npd)),  density=True, cumulative=True)\n\nq_index = np.where(C_pbin>=pthresh)[0][0]\nq_thresh = C_bin[q_index]\nax[1].vlines(q_thresh,0,1,color='black',linestyle='dotted')\n#ax[0].vlines(q_thresh,0,1,color='black',linestyle='dotted')\nax[1].hlines(pthresh,0,q_thresh,color='red',linestyle='dotted')\n#ax[0].hlines(pthresh,0,q_thresh,color='red',linestyle='dotted')\n\nax[0].set_title('PDF')\nax[0].set_xlabel('Error')\nax[1].set_title('CDF')\nax[1].set_xlabel('Error: threshold = '+str(round(q_thresh,2))+ ' ['+str(round(pthresh*100,2))+'%]')\nplt.show()\n#c_pbin, c_bin, _ = plt.hist( train_x, bins=100, range=(0, np.max(Nss)), cumulative=True)\n\nTP_ = len(np.where(Npd<=q_thresh)[0]) # TP\nFN_ = len(np.where(Npd>q_thresh)[0])  # FN\n\nFP_ = len(np.where(Apd<=q_thresh)[0]) # FP\nTN_ = len(np.where(Apd>q_thresh)[0])  # TN\n\nacc_ = (TP_+TN_) \/ len(data_)\nprint('Threshold: %.5f' % (q_thresh))\nprint('Accuracy: %.5f' % (acc_*100))\n\n\nplt.figure(figsize=(12,4),dpi=150)\nplt.ylim(0,e_ylim)\nplt.title('Acc:'+str(acc_))\nplt.scatter(np.arange(0,len(Npd),1),Npd, color='tab:green',marker='.', label='N')\nplt.scatter(np.arange(0,len(Apd),1),Apd, color='tab:red',marker='.', label='A')\n\nplt.hlines(q_thresh,0,takes,color='black')\nplt.legend()\n_=plt.ylabel('Error')\n_=plt.xlabel('Sample#')\n\n#for i in range(0,len(pths)):\n","b7d0d209":"qN = NN_all #<----- choose from all Normal beats\nqA = AA_all #<----- choose from all AbNormal beats\n\n#takes = min(len(qN),len(qA)) # take which ever is minimum\n\n#qN = np.random.choice(qN, size=takes, replace=False)\n#qA = np.random.choice(qA, size=takes, replace=False)\n\nq = np.hstack((qA,qN))\nprint(\"...testing on samples:\",len(q),'[', len(qN),'+', len(qA),']')\n\npred_dict = {_X:[]}\nfor cant in custom_ants:\n    pred_dict[cant]= []\n    # initialized dict\ntimestamp_start = datetime.datetime.now()\nfor bindex in q :\n    signal_, _,_,_ = rec.get_signal_slice_biF(bindex,lsamp, rsamp)\n    assert (seqlen == len(signal_))\n    sequence = signal_.reshape((1,seqlen, 1))\n    yhat = model.predict(sequence, verbose=0)\n    dE = np.abs(yhat[0,:,0] - sequence[0,:,0])\n    MAE = Rec_Error(dE)\n    pred_dict[rec.mLabels[bindex]].append(MAE)\n\n# plot the predictions   \nplt.figure(figsize=(12,4),dpi=150)\nplt.ylim(0,e_ylim)\nllen = 0\nfor _key in pred_dict.keys():\n    slen = len(pred_dict[_key])\n    plt.scatter(np.arange(0,slen,1)+llen,pred_dict[_key], color=custom_cols[_key],marker='.',label=_key[1:])\n    llen+=slen\n#plt.hlines(20,0,100)\nplt.hlines(q_thresh,0,llen,color='black')\nplt.ylabel('Recon_Error')\nplt.xlabel('Sample#')\nplt.legend()\nplt.show()\n#-------------------------------------\nprint('done!')\n#print(pred_dict)\nprint('\\nElapsed time = ',datetime.datetime.now() - timestamp_start)","6296e95b":"Npd = np.array(pred_dict[_N])\n#Normalize Npd?\n\n#(scaler.transform(data))\n\nApd = np.hstack((np.array(pred_dict[_S]),\n                         np.array(pred_dict[_V]),\n                         np.array(pred_dict[_F]),\n                         np.array(pred_dict[_Q]),\n                         np.array(pred_dict[_X])))\ndata_ = np.hstack((Npd, Apd))\nprint('Used Samples:', Npd.shape, '+',Apd.shape,'=', data_.shape)\n\nTP_ = len(np.where(Npd<=q_thresh)[0]) # TP\nFN_ = len(np.where(Npd>q_thresh)[0])  # FN\n\nFP_ = len(np.where(Apd<=q_thresh)[0]) # FP\nTN_ = len(np.where(Apd>q_thresh)[0])  # TN\n\nacc_ = (TP_+TN_) \/ len(data_)\nprint('Threshold: %.5f' % (q_thresh))\nprint('Accuracy: %.5f' % (acc_*100))\n\n\nplt.figure(figsize=(12,4),dpi=150)\nplt.ylim(0,e_ylim\/2)\nplt.title('Acc:'+str(acc_))\nplt.scatter(np.arange(0,len(Npd),1),Npd, color='tab:green',marker='.',label='N')\nplt.scatter(np.arange(0,len(Apd),1)+len(Npd),Apd, color='tab:red',marker='.',label='A')\n\nplt.hlines(q_thresh,0,len(data_),color='black')\nplt.legend()\n_=plt.ylabel('Error')\n_=plt.xlabel('Sample#')\n\n\n#@----------------------------------------->> conf matrix and performance measures\ng_LABELS = ['N','A']\nmatrix = np.array([[TP_,FN_],[FP_,TN_]])\nprint('\\n\\tConfusion Matrix [N\/A]')\nprint(print_conf_matrix( matrix, '', g_LABELS))\nprint_performance( matrix ,g_LABELS, do_round=5)\n\npC = TP_+TN_\npW = FP_+FN_\nprint('')\nprint('Total Predictions\\t',(pC+pW))\nprint('Correct Predictions\\t', pC,'\\t', round((100*pC)\/(pC+pW),2),'%')\nprint('Incorrect Predictions\\t', pW,'\\t', round((100*pW)\/(pC+pW),2),'%')","d6b5337d":"list_sel = AA_all  # NN_all, NN_true, NN_abs, AA_all, SS_all, VV_all, FF_all, QQ_all, XX_all\nprint('Selected list count:',len(list_sel))\nassert (len(list_sel<1)) #<--- assert if zero beats in the list\n\nbii_sel = np.random.choice(list_sel,size=min(10,len(list_sel)),replace=False)\n#bii_sel = list_sel\n\nprint('Predict on Samples:', len(bii_sel))\nprint('\\n')\nfor bii in bii_sel:\n    print('\\n#',bii, ' Label:', rec.mLabels[bii],rec.iLabels[bii])\n    signal_, _,_,_ = rec.get_signal_slice_biF(bii,lsamp, rsamp)\n    assert (seqlen == len(signal_))\n    sequence = signal_.reshape((1,seqlen, 1))\n    ypred = model.predict(sequence, verbose=0)\n    dE = np.abs(ypred[0,:,0] - sequence[0,:,0])\n    RErr = Rec_Error(dE)\n    #print('Error , Threshold:',RErr,',',q_thresh)\n    \n    if RErr>q_thresh:\n        print('Predicted: Abnormal ',round(RErr-q_thresh,4),':: above threshold')\n    else:\n        print('Predicted: Normal ',round(RErr-q_thresh,4),':: below threshold')\n    print('Error =',RErr)\n    #print()\n    fig,ax = plt.subplots(1,2,figsize=(20,6))\n    ax[1].fill_between(np.arange(0,seqlen,1), ypred[0,:,0], sequence[0,:,0], color='tab:purple')\n    ax[1].set_title('Delta')\n    ax[0].set_title('Reconstruction')\n    ax[0].plot(sequence[0,:,0], label='true', color='tab:green', linewidth=0.6)\n    ax[0].plot(ypred[0,:,0], label='pred', color='tab:red', linewidth=0.8)\n    ax[0].legend()\n    plt.show()\n    ","e2ec29f0":"# Automatic Thresholding for 2 classes - Normal v\/s Abnormal\n\nby looking probability distribution","831aae9c":"# Testing\n\n**Note: Choose equal number of beats from the normal and the abnormal class for testing**","1f4153ee":"# View Training Result","c40b29e6":"**The End**","827a9b09":"# Test on all beats of the Record","f8774106":"# Custom Labels","26ff92f3":"# **Select a Database**","a4f43369":"**Concluding Remarks**\n\n> With the choosen beat repsentation, this simple LSTM can seperate out the N and V classes very well but cannot differentiate between N and F types beats based on reconstruction error.\n","9f2719fc":"# Investigate","e8417e45":"# Select a Record","1104a792":"# Use objects to represent ecg databases and records","17801c72":"# LSTM Autoencoder Model and Parameters","46709024":"# Reconstruction of a training beat","7228a86c":"# Evaluate using automatic threshold","dae04ed9":"# Create LSTM Autoencoder Model","88bf88d5":"# Global Section","ba894245":"# [] Train the model []","398cd122":"# Callback Handlers","128dfbd1":"# Test some more beats","263d9126":"# Define Model Parameters and Reconstruction Error","e6c405ea":"# Select Training Beats","e81e5178":"# Reconstruction of a test beat","7f3f9514":"# **Objective - To Detect abnormal beats in ECG waveforms.**\n\nSuppose that we want to detect abnormal beats in an ecg waveform. We first gather information about the normal rhythm of the patient and encode the normal beats using an LSTM Auto-Encoder and then use simple thresholding on reconstruction error for classification. An advantage of this approach is that labeled data from only the Normal class is required for training.\n\n\n\n__________________________________________________________________\n> *For mini-project CS575 (Applied Time Series Analysis), May 2021*","2d86cf59":"> a better method of automated thresholding of reconstruction error should be considered.","c0f854cc":"# Create Callbacks"}}