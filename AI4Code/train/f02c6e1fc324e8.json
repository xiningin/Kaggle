{"cell_type":{"72fe7569":"code","1443aba3":"code","f86e7c1f":"code","24b1b581":"code","4dfa4ce9":"code","ea104050":"code","1f01fb4b":"code","ea484643":"code","53b2324c":"code","f027db89":"code","ee4ac496":"code","f3f908c5":"code","0568140e":"code","37cc9691":"code","4a08e0b0":"code","0f0bfa8a":"code","5103524f":"code","d079c42a":"code","bd0ae2d0":"code","cedb8b73":"code","52bb1908":"code","02430a6b":"code","c6750883":"code","fd2b9e67":"code","a50f3a9c":"code","ad7dac59":"code","eca73960":"code","19315beb":"code","33f1c34a":"code","a078de92":"code","e29acf54":"markdown","70955eb4":"markdown","e49ffdc1":"markdown","ac658c13":"markdown","70fba7d6":"markdown","b4023a67":"markdown","d121e059":"markdown","36ff5ffd":"markdown","f58995fb":"markdown","4a10ae04":"markdown","274f9985":"markdown","aa770fd2":"markdown","99a5f451":"markdown","b6b9c99c":"markdown","1f60f31b":"markdown","04199475":"markdown"},"source":{"72fe7569":"from IPython.display import display\n\nimport itertools\n\nimport numpy as np\nfrom scipy import stats\nimport pandas as pd\n\nimport matplotlib.pyplot as plt; plt.rcParams['figure.dpi'] = 100\nimport matplotlib.gridspec as gridspec\nimport seaborn as sns; sns.set()\n\nimport statsmodels.formula.api as smf","1443aba3":"data = pd.read_csv('..\/input\/factors-affecting-campus-placement\/Placement_Data_Full_Class.csv')\ndata.head()","f86e7c1f":"data.info()","24b1b581":"np.all((data.status == 'Not Placed') == data.salary.isna())","4dfa4ce9":"np.all(data.sl_no == np.arange(1, 216))","ea104050":"data.drop(['sl_no', 'salary'], axis=1, inplace=True)","1f01fb4b":"bin_features = ['gender', 'ssc_b', 'hsc_b', 'workex', 'specialisation']\ncat_features = ['hsc_s', 'degree_t']\nnum_features = ['ssc_p', 'hsc_p', 'degree_p', 'etest_p', 'mba_p']","ea484643":"fig, axes = plt.subplots(2, 4, figsize=(24, 12))\n\nfeature_names = iter(bin_features + cat_features + ['status'])\nfor row in range(2):\n    for col in range(4):\n        feature_name = next(feature_names)\n        sns.countplot(data[feature_name], ax=axes[row, col])\n        axes[row, col].set_title(f'The distribution of {feature_name}')\n\nfig.suptitle('Distributions of the binary and categorical features (+ target feature)')\nfig.subplots_adjust(top=0.92)\nfig.show()","53b2324c":"fig, axes = plt.subplots(3, 2, figsize=(22, 17))\n\nfeature_names = iter(num_features)\nfor row in range(3):\n    for col in range(2):\n        try:\n            feature_name = next(feature_names)\n        except StopIteration:\n            break\n        \n        sns.distplot(data[feature_name], ax=axes[row, col])    \n        axes[row, col].set_title(f'The distribution of {feature_name}')\n        \n\nfig.suptitle('Distributions of the numerical features')\nfig.subplots_adjust(top=0.93)\nfig.show()","f027db89":"fig = plt.figure(figsize=(23, 18))\nouter = gridspec.GridSpec(3, 2, wspace=0.2, hspace=0.2)\n\nfor feature_ind, feature_name in enumerate(bin_features):\n    inner = gridspec.GridSpecFromSubplotSpec(1, 2, subplot_spec=outer[feature_ind], \n                                             wspace=0.3, hspace=0.3)\n    \n    ax = plt.Subplot(fig, outer[feature_ind])\n    ax.set_title(f'The distribution of status for each {feature_name}\\'s class')\n    ax.axis('off')\n    fig.add_subplot(ax)\n    \n    for pie_ind, f_class in enumerate(data[feature_name].unique()):\n        ax = plt.Subplot(fig, inner[pie_ind])\n        f_class_status_vc = data[data[feature_name] == f_class]['status'].value_counts().sort_index()\n        ax.pie(f_class_status_vc.values, labels=f_class_status_vc.index, autopct='%1.1f%%')\n        ax.set_title(f_class)\n        fig.add_subplot(ax)\n\nfig.suptitle('Distributions of status for the binary features')\nfig.subplots_adjust(top=0.93)\nfig.show()","ee4ac496":"fig = plt.figure(figsize=(20, 15))\nouter = gridspec.GridSpec(2, 1, wspace=0.2, hspace=0.1)\n\nfor feature_ind, feature_name in enumerate(cat_features):\n    inner = gridspec.GridSpecFromSubplotSpec(1, 3, subplot_spec=outer[feature_ind], \n                                             wspace=0.2, hspace=0.2)\n    \n    ax = plt.Subplot(fig, outer[feature_ind])\n    ax.set_title(f'The distribution of status for each {feature_name}\\'s class')\n    ax.axis('off')\n    fig.add_subplot(ax)\n    \n    for pie_ind, f_class in enumerate(data[feature_name].unique()):\n        ax = plt.Subplot(fig, inner[pie_ind])\n        f_class_status_vc = data[data[feature_name] == f_class]['status'].value_counts().sort_index()\n        ax.pie(f_class_status_vc.values, labels=f_class_status_vc.index, autopct='%1.1f%%')\n        ax.set_title(f_class)\n        fig.add_subplot(ax)\n\nfig.suptitle('Distributions of status for the categorical features')\nfig.subplots_adjust(top=0.93)\nfig.show()","f3f908c5":"# transforming status into binary feature to calculate proportions\ndata_01_status = data.copy()\ndata_01_status['status'].replace({'Placed': 1, 'Not Placed': 0}, inplace=True)","0568140e":"# proportions of placed people for different binary\/categorical features values combinations\nbin_x_cat_status_props = pd.pivot_table(data_01_status, index=cat_features, columns=bin_features, \n                                        values='status', aggfunc=np.mean)","37cc9691":"plt.figure(figsize=(18, 5))\nsns.heatmap(bin_x_cat_status_props, annot=True).tick_params(labelsize=9)\nplt.title('Proportions of placed people for the different binary\/categorical features values combinations')\nplt.show()","4a08e0b0":"print('Top of the binary features combinations:')\ndisplay(pd.DataFrame((\n    bin_x_cat_status_props.sum(axis=0) \/ (bin_x_cat_status_props.shape[0] - bin_x_cat_status_props.isna().sum(axis=0))\n).sort_values(ascending=False), columns=['Mean proportion of placed people']))\n\nprint('\\n\\nTop of the categorical features combinations:')\ndisplay(pd.DataFrame((\n    bin_x_cat_status_props.sum(axis=1) \/ (bin_x_cat_status_props.shape[1] - bin_x_cat_status_props.isna().sum(axis=1))\n).sort_values(ascending=False), columns=['Mean proportion of placed people']))","0f0bfa8a":"orange = (0.8666666666666667, 0.5176470588235295, 0.3215686274509804)\nblue = (0.2980392156862745, 0.4470588235294118, 0.6901960784313725)\npp = sns.pairplot(data[num_features + ['status']], hue='status', palette={'Placed': orange, 'Not Placed': blue})\npp.fig.suptitle('Distributions and pairplots of the numerical features divided by status', y=1.03)\npp.fig.show()","5103524f":"def matthews_correlation(contingency_table):\n    '''Matthews correlation calculation.'''\n    a, b = contingency_table[0]\n    c, d = contingency_table[1]\n\n    n = np.sum(contingency_table)\n    acabn = (a + c) * (a + b) \/ n\n    accdn = (a + c) * (c + d) \/ n\n    bdabn = (b + d) * (a + b) \/ n\n    bdcdn = (b + d) * (c + d) \/ n\n    if n < 40 or np.any(np.array([acabn, accdn, bdabn, bdcdn]) < 5):\n        raise ValueError('Contingency table isn\\'t suitable for Matthews correlation calculation.')\n\n    p_value = stats.chi2_contingency(contingency_table)[1]\n    corr = (a * d - b * c) \/ np.sqrt((a + b) * (a + c) * (b + d) * (c + d))\n    return corr, p_value\n\n\ndef proportion_confint(sample, alpha=0.05):\n    '''Wilson\\'s \u0441onfidence interval for a proportion.'''\n    p = np.mean(sample)\n    n = len(sample)\n\n    z = stats.norm.ppf(1 - alpha \/ 2)\n    left_boundary = 1 \/ (1 + z ** 2 \/ n) * (p + z ** 2 \/ (2 * n) \\\n                                            - z * np.sqrt(p * (1 - p) \/ n + z ** 2 \/ (4 * n ** 2)))\n    right_boundary = 1 \/ (1 + z ** 2 \/ n) * (p + z ** 2 \/ (2 * n) \\\n                                             + z * np.sqrt(p * (1 - p) \/ n + z ** 2 \/ (4 * n ** 2)))\n\n    return left_boundary, right_boundary\n\n\ndef proportions_diff_confint_ind(sample1, sample2, alpha=0.05):\n    '''Confidence interval for the difference in two independent proportions.'''\n    z = stats.norm.ppf(1 - alpha \/ 2)\n    p1 = np.mean(sample1)\n    p2 = np.mean(sample2)\n    n1 = len(sample1)\n    n2 = len(sample2)\n\n    left_boundary = (p1 - p2) - z * np.sqrt(p1 * (1 - p1) \/ n1 + p2 * (1 - p2) \/ n2)\n    right_boundary = (p1 - p2) + z * np.sqrt(p1 * (1 - p1) \/ n1 + p2 * (1 - p2) \/ n2)\n\n    return left_boundary, right_boundary\n\n\ndef proportions_ztest_ind(sample1, sample2, alternative='two-sided'):\n    '''Z-test for two independent proportions.'''\n    if alternative not in ('two-sided', 'less', 'greater'):\n        raise ValueError('Alternative not recognized, should be \\'two-sided\\', \\'less\\' or \\'greater\\'.')\n\n    p1 = np.mean(sample1)\n    p2 = np.mean(sample2)\n    n1 = len(sample1)\n    n2 = len(sample2)\n\n    P = (p1 * n1 + p2 * n2) \/ (n1 + n2)\n    z_stat = (p1 - p2) \/ np.sqrt(P * (1 - P) * (1 \/ n1 + 1 \/ n2))\n\n    if alternative == 'two-sided':\n        p_value = 2 * (1 - stats.norm.cdf(np.abs(z_stat)))\n\n    if alternative == 'less':\n        p_value = stats.norm.cdf(z_stat)\n\n    if alternative == 'greater':\n        p_value = 1 - stats.norm.cdf(z_stat)\n\n    return z_stat, p_value","d079c42a":"print('BINARY FEATURES\\n'\n      '===============\\n\\n')\nfor feature_name in bin_features:\n    class1, class2 = data_01_status[feature_name].unique()\n    class1_status = data_01_status[data[feature_name] == class1].status\n    class2_status = data_01_status[data[feature_name] == class2].status\n    \n    print(f'Feature: {feature_name}\\n------')\n    \n    print('Contingency table:')\n    contingency_table = pd.crosstab(data['status'], data[feature_name])\n    display(contingency_table)\n    corr, p = matthews_correlation(contingency_table.values)\n    print(f'Correlation between status and {feature_name}: {round(corr, 4)}, p-value: {p}')\n    if p < 0.05:\n        print(f'There is a correlation between {feature_name} and status.')\n    else:\n        print(f'There isn\\'t any correlation between {feature_name} and status.')\n        \n    print()\n    \n    print(f'The proportion of placed people for {class1}: {round(np.mean(class1_status), 4)}')\n    print(f'The proportion of placed people for {class2}: {round(np.mean(class2_status), 4)}')\n    \n    class1_confint = list(map(lambda lim: round(lim, 4), proportion_confint(class1_status)))\n    class2_confint = list(map(lambda lim: round(lim, 4), proportion_confint(class2_status)))\n    print(f'The confidence interval (95%) for {class1}: {class1_confint}')\n    print(f'The confidence interval (95%) for {class2}: {class2_confint}')\n    if class2_confint[0] < class1_confint[0] < class2_confint[1] or \\\n       class2_confint[0] < class1_confint[1] < class2_confint[1]:\n        print('The intervals overlap.')\n    else:\n        print('The intervals don\\'t overlap.')\n    \n    print()\n    \n    if np.mean(class1_status) > np.mean(class2_status):\n        bigger_prop, smaller_prop = class1_status, class2_status\n    else:\n        bigger_prop, smaller_prop = class2_status, class1_status\n    \n    print(f'The difference in the proportions: {round(np.mean(bigger_prop) - np.mean(smaller_prop), 4)}')\n    prop_diff_confint = list(map(lambda lim: round(lim, 4), proportions_diff_confint_ind(bigger_prop, \n                                                                                         smaller_prop)))\n    print(f'The confidence interval (95%) for the difference in the proportions: {prop_diff_confint}')\n    if prop_diff_confint[0] > 0:\n        print(f'The proportions may differ by at least {prop_diff_confint[0]}.')\n    else:\n        print('The difference between the proportions may be 0.')\n    \n    print()\n    \n    p = proportions_ztest_ind(class1_status, class2_status)[1]\n    print(f'Z-test result (two-sided): {p} (p-value)')\n    if p < 0.05:\n        print('The proportions are probably unequal.')\n    else:\n        print('The proportions are probably equal.')\n    \n    print('\\n\\n')","bd0ae2d0":"def cramers_v(contingency_table):\n    '''Cramer\\'s V coefficient.'''\n    n = np.sum(contingency_table)\n    ct_nrows, ct_ncols = contingency_table.shape\n    if n < 40 or np.sum(contingency_table < 5) \/ (ct_nrows * ct_ncols) > 0.2:\n        raise ValueError('Contingency table isn\\'t suitable for Cramers\\'s V coefficient calculation.')\n\n    chi2, p_value = stats.chi2_contingency(contingency_table)[:2]\n    corr = np.sqrt(chi2 \/ (n * (min(ct_nrows, ct_ncols) - 1)))\n    return corr, p_value","cedb8b73":"print('CATEGORICAL FEATURES\\n'\n      '====================\\n\\n')\nfor feature_name in cat_features:\n    print(f'Feature: {feature_name}\\n------')\n    print('Contingency table:')\n    contingency_table = pd.crosstab(data['status'], data[feature_name])\n    display(contingency_table)\n    corr, p = cramers_v(contingency_table.values)\n    print(f'Correlation between status and {feature_name}: {round(corr, 4)}, p-value: {p}')\n    if p < 0.05:\n        print(f'There is a correlation between {feature_name} and status.')\n    else:\n        print(f'There isn\\'t any correlation between {feature_name} and status.')\n    \n    print('\\n\\n')","52bb1908":"norm_features = [feature_name for feature_name in num_features if stats.shapiro(data[feature_name])[1] < 0.05]\nprint(f'Numerical features: {\", \".join(num_features)}')\nprint(f'Features that are probably normally distributed: {\", \".join(norm_features)}')","02430a6b":"for feature_name in num_features:\n    feature_placed = data[data.status == 'Placed'][feature_name]\n    feature_not_placed = data[data.status == 'Not Placed'][feature_name]\n    \n    p_placed = stats.shapiro(feature_placed)[1]\n    p_not_placed = stats.shapiro(feature_not_placed)[1]\n    if p_placed < 0.05 and p_not_placed < 0.05:\n        print(f'The both distributions of {feature_name} for the different status are probably normally distributed.')","c6750883":"norm_num_features = ['degree_p', 'etest_p']","fd2b9e67":"def tconfint(sample, alpha=0.05):\n    '''Confidence interval based on Student t distribution.'''\n    mean = np.mean(sample)\n    S = np.std(sample, ddof=1)\n    n = len(sample)\n\n    t = stats.t.ppf(1 - alpha \/ 2, n - 1)\n    left_boundary = mean - t * S \/ np.sqrt(n)\n    right_boundary = mean + t * S \/ np.sqrt(n)\n\n    return left_boundary, right_boundary\n\n\ndef tconfint_diff(sample1, sample2, alpha=0.05):\n    '''Confidence interval based on Student t distribution for\n    the difference in means of two samples.'''\n    mean1 = np.mean(sample1)\n    mean2 = np.mean(sample2)\n    s1 = np.std(sample1, ddof=1)\n    s2 = np.std(sample2, ddof=1)\n    n1 = len(sample1)\n    n2 = len(sample2)\n    \n    sem1 = np.var(sample1) \/ (n1 - 1)\n    sem2 = np.var(sample2) \/ (n2 - 1)\n    semsum = sem1 + sem2\n    z1 = (sem1 \/ semsum) ** 2 \/ (n1 - 1)\n    z2 = (sem2 \/ semsum) ** 2 \/ (n2 - 1)\n    dof = 1 \/ (z1 + z2)\n    \n    t = np.abs(stats.t.ppf(alpha \/ 2, dof))\n    left_boundary = (mean1 - mean2) - t * np.sqrt((s1 ** 2) \/ n1 + (s2 ** 2) \/ n2)\n    right_boundary = (mean1 - mean2) + t * np.sqrt((s1 ** 2) \/ n1 + (s2 ** 2) \/ n2)\n    \n    return left_boundary, right_boundary\n\n\ndef permutation_test_ind(sample1, sample2, max_permutations=None, alternative='two-sided'):\n    '''Permutation test for two independent samples.'''\n    if alternative not in ('two-sided', 'less', 'greater'):\n        raise ValueError('Alternative not recognized, should be \\'two-sided\\', \\'less\\' or \\'greater\\'.')\n\n    t_stat = np.mean(sample1) - np.mean(sample2)\n\n    joined_sample = np.hstack((sample1, sample2))\n    n1 = len(sample1)\n    n = len(joined_sample)\n\n    if max_permutations:\n        index = list(range(n))\n        indices = set([tuple(index)])\n        for _ in range(max_permutations - 1):\n            np.random.shuffle(index)\n            indices.add(tuple(index))\n\n        indices = [(index[:n1], index[n1:]) for index in indices]\n    else:\n        indices = [(list(index), list(filter(lambda i: i not in index, range(n))))\n                    for index in itertools.combinations(range(n), n1)]\n\n    zero_distr = [joined_sample[list(i[0])].mean() - joined_sample[list(i[1])].mean()\n                  for i in indices]\n\n    if alternative == 'two-sided':\n        p_value = sum([abs(x) >= abs(t_stat) for x in zero_distr]) \/ len(zero_distr)\n\n    if alternative == 'less':\n        p_value = sum([x <= t_stat for x in zero_distr]) \/ len(zero_distr)\n\n    if alternative == 'greater':\n        p_value = sum([x >= t_stat for x in zero_distr]) \/ len(zero_distr)\n\n    return t_stat, p_value","a50f3a9c":"print('NUMERICAL FEATURES\\n'\n      '==================\\n\\n')\nfor feature_name in num_features:\n    feature_placed = data[data.status == 'Placed'][feature_name]\n    feature_not_placed = data[data.status == 'Not Placed'][feature_name]\n    \n    print(f'Feature: {feature_name}\\n------')\n    \n    corr, p = stats.pointbiserialr(data_01_status.status, data[feature_name])\n    print(f'Correlation between status and {feature_name}: {round(corr, 4)}, p-value: {p}')\n    if p < 0.05:\n        print(f'There is a correlation between {feature_name} and status.')\n    else:\n        print(f'There isn\\'t any correlation between {feature_name} and status.')\n        \n    print()\n    \n    print(f'Mean of {feature_name} for placed people:     {round(np.mean(feature_placed), 4)}')\n    print(f'Mean of {feature_name} for not placed people: {round(np.mean(feature_not_placed), 4)}')\n    \n    feature_placed_confint = list(map(lambda lim: round(lim, 4), tconfint(feature_placed)))\n    feature_not_placed_confint = list(map(lambda lim: round(lim, 4), tconfint(feature_not_placed)))\n    \n    print(f'Confidence interval (95%) of mean {feature_name} for placed people:     {feature_placed_confint}')\n    print(f'Confidence interval (95%) of mean {feature_name} for not placed people: {feature_not_placed_confint}')\n    if feature_not_placed_confint[0] < feature_placed_confint[0] < feature_not_placed_confint[1] or \\\n       feature_not_placed_confint[0] < feature_placed_confint[1] < feature_not_placed_confint[1]:\n        print('The intervals overlap.')\n    else:\n        print('The intervals don\\'t overlap.')\n    \n    print()\n    \n    if np.mean(feature_placed) > np.mean(feature_not_placed):\n        bigger_mean, smaller_mean = feature_placed, feature_not_placed\n    else:\n        bigger_mean, smaller_mean = feature_not_placed, feature_placed\n    \n    mean_diff_confint = list(map(lambda lim: round(lim, 4), tconfint_diff(bigger_mean, smaller_mean)))\n    print(f'Difference in means: {round(np.mean(bigger_mean) - np.mean(smaller_mean), 4)}')\n    print(f'Confidence interval (95%) for the difference in means: {mean_diff_confint}')\n    if mean_diff_confint[0] > 0:\n        print(f'The means may differ by at least {mean_diff_confint[0]}.')\n    else:\n        print('The difference in means between the samples may be 0.')\n        \n    print()\n\n    if feature_name in norm_num_features:\n        comparison_subject = 'means'\n        p = stats.ttest_ind(feature_placed, feature_not_placed, equal_var=False)[1]\n        print(f'Student\\'s t-test result (two-sided): {p} (p-value)')\n    else:\n        comparison_subject = 'distributions'\n        p = permutation_test_ind(feature_placed, feature_not_placed, max_permutations=5000)[1]\n        print(f'Permutation test result (two-sided): {p} (p-value)')\n    \n    if p < 0.05:\n        print(f'The {comparison_subject} of the samples are probably unequal.')\n    else:\n        print(f'The {comparison_subject} of the samples are probably equal.')\n    \n    print('\\n\\n')","ad7dac59":"data_01_status[num_features] = data[num_features] \/ 100\ndata_01_status[num_features].describe()","eca73960":"data_01_status.head()","19315beb":"formula = 'status ~ C(' + ') + C('.join(bin_features + cat_features) + ') + ' + ' + '.join(num_features)\nformula","33f1c34a":"model = smf.logit(formula, data=data_01_status)\nfitted = model.fit()\nprint(fitted.summary())","a078de92":"formula = 'status ~ C(degree_t) + C(workex) + ssc_p + hsc_p + degree_p + mba_p'\nmodel = smf.logit(formula, data=data_01_status)\nfitted = model.fit()\nprint(fitted.summary())","e29acf54":"# Campus recruitment analysis\n\nSorry for my English please \/\\\n\n## Data\n\n__Feature list__ (`Variable`: Definition):\n\n- `sl_no`: Serial Number\n- `gender`: Gender - Male='M',Female='F'\n- `ssc_p`: Secondary Education percentage - 10th Grade\n- `ssc_b`: Board of Education - Central\/Others\n- `hsc_p`: Higher Secondary Education percentage - 12th Grade\n- `hsc_b`: Board of Education - Central\/Others\n- `hsc_s`: Specialization in Higher Secondary Education\n- `degree_p`: Degree Percentage\n- `degree_t`: Under Graduation (Degree type) - Field of degree education\n- `workex`: Work Experience\n- `etest_p`: Employability test percentage (conducted by college)\n- `specialisation`: Post Graduation (MBA) - Specialization\n- `mba_p`: MBA percentage\n- `status`: Status of placement - Placed\/Not placed\n- `salary`: Salary offered by corporate to candidates\n\n__Questions:__<br>\n1. Which factor influenced a candidate in getting placed?\n2. Does percentage matters for one to get placed?\n3. Which degree specialization is much demanded by corporate?\n4. Play with the data conducting all statistical tests.","70955eb4":"None of the categorical features has a significant correlation with `status`.\n\n### Numerical features\n\nNot all numerical features are normally distributed:","e49ffdc1":"Seems it hasn't become much worse. \n\nDepending on model's info we can conclude:\n\n- __Question 1.__ (Which factor influenced a candidate in getting placed?) `degree_t` (doubtful) and `workex` are.<br>\n- __Question 2.__ (Does percentage matters for one to get placed?) Yes, `ssc_p`, `hsc_p`, `degree_p` and `mba_p` do.","ac658c13":"Fitting a model and printing statistics:","70fba7d6":"The differences in proportions of placed people for `workex` and `specialization` (__Question 3.__ Which degree specialization is much demanded by corporate?) classes are statistically significant, differences in all the rest ones - aren't. People with work experience are placed at least 15% more often, people of Mkt&Fin specialization - al least 11% more often.\n\n### Categorical features","b4023a67":"There are some na-values in `salary` column but salary is specified only for placed people so it's useless for answering __questions__.","d121e059":"So to compare them we will use Student's t-test for the normally distributed pairs and permutation test for the rest.","36ff5ffd":"__Question 2.__ (Does percentage matters for one to get placed?) The differences in distributions of `ssc_p` and `hsc_p`, and in means of `degree_p` and `etest_p` (doubtful) for placed and not placed people are statistically significant, difference in distributions of `mba_p` - isn't.\n\n## Logistic regression model\n\nLet's train a Logisctic regression model and look at it's weights. But firstly transform percents to proportions (instead of scaling):","f58995fb":"As well as `sl_no` column, it doesn't provide any useful information:","4a10ae04":"__p-value__ < 0.05 for `degree_t`, `workex`, `ssc_p`, `hsc_p`, `degree_p` and `mba_p` weights. Look's like other features aren't too important for the `status` prediction (if a logistic regression is used). Let's fit another model using only them:","274f9985":"## Visualizations\n\nLet's divide our features by type (binary, categorical and numerical) and plot their distributions and distributions of `status` for all of them:","aa770fd2":"__Question 2.__ (Does percentage matters for one to get placed?) Look's like `ssc_p`, `hsc_p` and `degree_p` do if to divide them by `status` and all do if we use them in pairs with each other.\n\n## Statistics\n\n(__Question 4.__)\n\n### Binary features","99a5f451":"And only two of them are normally distributed if we divide them by `status`:","b6b9c99c":"## First look\n\nThe dataset:","1f60f31b":"__Question 3.__ (Which degree specialization is much demanded by corporate?) Look's like that Mkt&Fin is.","04199475":"Dropping these columns and moving on:"}}