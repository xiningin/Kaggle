{"cell_type":{"244523b8":"code","8171921b":"code","39c4d5ba":"code","b3a2403a":"code","aa0a1d9e":"code","8e9404d5":"code","1b2ef836":"code","1a9d129a":"code","df1dfe53":"code","78e46065":"code","61fa3380":"code","60bf332d":"code","04da7bca":"code","0ca08588":"code","a4342db7":"code","8090adbc":"code","fefba3d6":"markdown","b166ddea":"markdown","27733a59":"markdown","ae3a2802":"markdown","98cc389a":"markdown","7fe9804a":"markdown","3cd7d183":"markdown","ed5f06e7":"markdown","3eb99af1":"markdown","d2838aef":"markdown","f8a59a61":"markdown","5942b4a4":"markdown","675b7865":"markdown"},"source":{"244523b8":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","8171921b":"!pip install pyspark","39c4d5ba":"from pyspark.sql import SparkSession\nspark=SparkSession.builder.appName('Dataframe').getOrCreate()\nspark","b3a2403a":"df = spark.read.option('header','true').csv('\/kaggle\/input\/titanic\/train.csv',inferSchema=True)","aa0a1d9e":"df.show()","8e9404d5":"df.head(5)","1b2ef836":"#display only Name and Sex columns\ndf.select(['Name','Sex']).show()","1a9d129a":"#create new column named as 'New Age' whose value is age+2\ndf = df.withColumn( 'New Age', df['Age']+2 )\ndf.head(5)","df1dfe53":"#drop Age column\ndf = df.drop('Age')","78e46065":"df.head(2)","61fa3380":"#rename Name column as Full Name\ndf = df.withColumnRenamed('Name','Full Name')\ndf.show()","60bf332d":"df.na.drop().show()","04da7bca":"#drop rows in which non-null values are less than threshold\ndf.na.drop(thresh = 15).show()\n#thresh = 15 will delete all rows because number of columns = 12 ","0ca08588":"df.na.drop(thresh = 11).show()","a4342db7":"#delete row where Cabin column has null value\ndf.na.drop(subset=['Cabin']).show()","8090adbc":"#replace null values in Cabin column with 0\ndf.na.fill('0',['Cabin']).show()","fefba3d6":"**Display only Top 5 rows**","b166ddea":"**Rename Column**","27733a59":"Reference:\nhttps:\/\/github.com\/krishnaik06\/Pyspark-With-Python\n\n\nhttps:\/\/www.youtube.com\/watch?v=WyZmM6K7ubc&list=PLZoTAELRMXVNjiiawhzZ0afHcPvC8jpcg&ab_channel=KrishNaik","ae3a2802":"**Read the Dataset**","98cc389a":"**Create a new pyspark session**","7fe9804a":"**Adding New Column**","3cd7d183":"**thresh parameter in drop function**","ed5f06e7":"**Drop rows in which specific column contains null value**","3eb99af1":"**Select Specific Columns**","d2838aef":"**Filling Missing Values**","f8a59a61":"**Display dataset**","5942b4a4":"**Drop all the rows in which any null exists**","675b7865":"**Delete Age column**"}}