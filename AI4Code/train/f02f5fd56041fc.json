{"cell_type":{"5e830422":"code","712e320d":"code","0ba36416":"code","421e1dd6":"code","2afe7e40":"code","12dfa182":"code","d2a8fb87":"code","9b3a5389":"code","32a93e90":"code","6ae65847":"code","2eaf6ab1":"code","8419fa3c":"code","3db6c32a":"code","137b6c75":"code","5f2394d1":"code","cd944b93":"code","c4a471cb":"code","e8121222":"code","6b1bda34":"code","42dfe0a7":"code","c06b15d5":"code","09facf5a":"code","05cb62db":"markdown","cc8804b8":"markdown","434c45e5":"markdown","78e6d0a1":"markdown","d5826c2d":"markdown","1be02afd":"markdown","51df99c8":"markdown","c6e865e7":"markdown","3d6c3616":"markdown","22f2430b":"markdown","12a40cd0":"markdown","31f5e1bb":"markdown","cc7e0bb8":"markdown","14bd89df":"markdown"},"source":{"5e830422":"import os\nimport random\nimport pandas as pd\nimport numpy as np\nfrom lightgbm import LGBMRegressor\nimport lightgbm as lgb\nimport gresearch_crypto\nimport datetime\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.metrics import mean_absolute_error as MSE\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.decomposition import PCA\nimport math\nfrom sklearn.cluster import MiniBatchKMeans\nimport warnings\nwarnings.filterwarnings('ignore')\n\nTRAIN_CSV = '\/kaggle\/input\/g-research-crypto-forecasting\/train.csv'\nASSET_DETAILS_CSV = '\/kaggle\/input\/g-research-crypto-forecasting\/asset_details.csv'\n\nSEED = 20\n\nREMOVE_LB_TEST_OVERLAPPING_DATA = True","712e320d":"%%capture\n'''\n!cp ..\/input\/talibinstall\/ta-lib-0.4.0-src.tar.gzh  .\/ta-lib-0.4.0-src.tar.gz\n!tar -xzvf ta-lib-0.4.0-src.tar.gz > null\n!cd ta-lib && .\/configure --prefix=\/usr > null && make  > null && make install > null\n!cp ..\/input\/talibinstall\/TA-Lib-0.4.21.tar.gzh TA-Lib-0.4.21.tar.gz\n!pip install TA-Lib-0.4.21.tar.gz > null\n!pip install ..\/input\/talibinstall\/numpy-1.21.4-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl >null\nimport talib as ta\n'''","0ba36416":"def fix_all_seeds(seed):\n    np.random.seed(seed)\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n\nfix_all_seeds(SEED)","421e1dd6":"#df_train = pd.read_csv(TRAIN_CSV)\n#df_train","2afe7e40":"df_test = pd.read_csv('..\/input\/g-research-crypto-forecasting\/example_test.csv')\ndf_test.head()","12dfa182":"%%capture\n'''\nl =len(df_train)\nl = l\/\/2\ndf_train = df_train[l:].reset_index(drop=True)\ndf_train\n'''","d2a8fb87":"'''\n# Remove the future\nif REMOVE_LB_TEST_OVERLAPPING_DATA:\n    df_train['datetime'] = pd.to_datetime(df_train['timestamp'], unit='s')\n    df_valid = df_train[(df_train['datetime'] > '2021-06-13 00:00:00')].reset_index(drop=True)\n    df_train = df_train[(df_train['datetime'] < '2021-06-13 00:00:00')].reset_index(drop=True)\n    #df_valid = df_train[(df_train['datetime'] > '2021-06-13 00:00:00') | (df_train['datetime'] < '2019-01-01 00:00:00')]\n    #df_train = df_train[(df_train['datetime'] < '2021-06-13 00:00:00') | (df_train['datetime'] > '2019-01-01 00:00:00')]\n    #print('delete data  ','train=',len(df_train),'  valid=',len(df_valid))\n    df_train  = df_train.drop(['datetime'],axis=1)\n    df_valid  = df_valid.drop(['datetime'],axis=1)\nelse:\n    df_train['datetime'] = pd.to_datetime(df_train['timestamp'], unit='s')\n    df_valid = df_train[(df_train['datetime'] > '2021-06-13 00:00:00')].reset_index(drop=True)\n    df_train  = df_train.drop(['datetime'],axis=1)\n    df_valid  = df_valid.drop(['datetime'],axis=1)\n'''","9b3a5389":"#df_train = df_train.dropna(subset=['Target']).reset_index(drop=True)\n#df_valid = df_valid.dropna(subset=['Target']).reset_index(drop=True)\n","32a93e90":"#df_train = df_train.sample(n=len(df_train), random_state=0)\n#df_valid = df_valid.sample(n=len(df_valid), random_state=0)","6ae65847":"df_asset_details = pd.read_csv(ASSET_DETAILS_CSV).sort_values(\"Asset_ID\")\ndf_asset_details","2eaf6ab1":"asset_weight_dict =df_asset_details[['Asset_ID','Weight']].to_dict()","8419fa3c":"from sklearn.model_selection import train_test_split\nimport lightgbm as lgb\nfrom sklearn.metrics import accuracy_score\n#!pip install optuna\nimport optuna \nimport optuna.integration.lightgbm as lgbo","3db6c32a":"def log_return(series, periods=5):\n    return np.log(series).diff(periods=periods)\n\n# Two features from the competition tutorial\n\ndef hlco_ratio(df): \n    return (df['High'] - df['Low'])\/(df['Close']-df['Open'])\ndef upper_shadow(df):\n    return df['High'] - np.maximum(df['Close'], df['Open'])\ndef lower_shadow(df):\n    return np.minimum(df['Close'], df['Open']) - df['Low']\n","137b6c75":"def add_features(df):\n    df['Upper_Shadow'] = upper_shadow(df)\n    df['hlco_ratio'] = hlco_ratio(df)\n    df['Lower_Shadow'] = lower_shadow(df)\n    times = pd.to_datetime(df.index,unit=\"s\",infer_datetime_format=True)\n    \n    df[\"year\"] = times.year\n    df[\"month\"] = times.month\n    \n    df[\"hour\"] = times.hour  \n    df[\"dayofweek\"] = times.dayofweek \n    df[\"day\"] = times.day \n    if 1==1:\n        \n        df['open2close'] = df['Close'] \/ df['Open']\n        df['high2low'] = df['High'] \/ df['Low']\n        mean_price = df[['Open', 'High', 'Low', 'Close']].mean(axis=1)\n        median_price = df[['Open', 'High', 'Low', 'Close']].median(axis=1)\n        df['high2mean'] = df['High'] \/ mean_price\n        df['low2mean'] = df['Low'] \/ mean_price\n        df['high2median'] = df['High'] \/ median_price\n        df['low2median'] = df['Low'] \/ median_price\n        df['volume2count'] = df['Volume'] \/ (df['Count'] + 1)\n    \n        df[\"high_div_low\"] = df[\"High\"] \/ df[\"Low\"]\n        df[\"open_sub_close\"] = df[\"Open\"] - df[\"Close\"]\n        \n        df['time_open'] = df['Open'] * (df['timestamp'] \/1000)\n        df['time_close'] = df['Close']* (df['timestamp'] \/1000)\n        df['time_high'] = df['High']* (df['timestamp'] \/1000)\n        df['time_low'] = df['Low']* (df['timestamp'] \/1000)\n        df['time_vwap'] = df['VWAP']* (df['timestamp'] \/1000)\n        df['time_count'] = df['Count']* (df['timestamp'] \/1000)\n        df['time_volume'] = df['Volume']* (df['timestamp'] \/1000)\n        \n        df['time_open'] = df['Open'].diff().fillna(0)\n        df['time_close'] = df['Close'].diff().fillna(0)\n        df['time_high'] = df['High'].diff().fillna(0)\n        df['time_low'] = df['Low'].diff().fillna(0)\n        df['time_vwap'] = df['VWAP'].diff().fillna(0)\n        df['time_count'] = df['Count'].diff().fillna(0)\n        df['time_volume'] = df['Volume'].diff().fillna(0)\n        \n        #df['time_open_close'] = np.log1p(df['Open'] + df['Close']) * (df['timestamp'] \/1000)\n        \n        ####\n        df['open*2'] = df['Open']**2 * (df['timestamp'] \/1000)#\n        df['close*2'] = df['Close']**2 * (df['timestamp'] \/1000)# \n        df['high*2'] = df['High']**2 * (df['timestamp'] \/1000)#\n        df['low*2'] = df['Low']**2 * (df['timestamp'] \/1000)#\n        df['vwap*2'] = df['VWAP']**2 * (df['timestamp'] \/1000)#\n        df['count*2'] = df['Count']**2 * (df['timestamp'] \/1000)#\n        df['volume*2'] = df['Volume']**2 * (df['timestamp'] \/1000)#\n        df['open_close'] = df['Open']*df['Close']* (df['timestamp'] \/1000)\n        df['open_high'] = df['Open']* df['High']* (df['timestamp'] \/1000)\n        df['open_low'] = df['Open']* df['Low']* (df['timestamp'] \/1000)\n        df['open_vwap'] = df['Open']* df['VWAP']* (df['timestamp'] \/1000)\n        df['open_count'] = df['Open']* df['Count'] * (df['timestamp'] \/1000)#\n        df['open_volume'] = df['Open']* df['Volume']* (df['timestamp'] \/1000) #\n        \n        df['close_high'] = df['Close']* df['High']* (df['timestamp'] \/1000)\n        df['close_low'] = df['Close']* df['Low']* (df['timestamp'] \/1000)\n        df['close_vwap'] = df['Close']* df['VWAP']* (df['timestamp'] \/1000)\n        df['close_count'] = df['Close']* df['Count']* (df['timestamp'] \/1000) #\n        df['high_open'] = df['High']* df['Open']* (df['timestamp'] \/1000) #\n        df['high_close'] = df['High']* df['Close']* (df['timestamp'] \/1000) #\n        df['high_low'] = df['High']* df['Low']* (df['timestamp'] \/1000)\n        df['high_vwap'] = df['High']* df['VWAP']* (df['timestamp'] \/1000)\n        df['high_count'] = df['High']* df['Count']* (df['timestamp'] \/1000) #\n        df['high_volume'] = df['High']* df['Volume']* (df['timestamp'] \/1000) #\n        \n        df['low_open'] = df['Low']* df['Open']* (df['timestamp'] \/1000) #\n        df['low_close'] = df['Low']* df['Close']* (df['timestamp'] \/1000) #\n        df['low_vwap'] = df['Low']* df['VWAP']* (df['timestamp'] \/1000)\n        df['low_count'] = df['Low']* df['Count']* (df['timestamp'] \/1000) #\n        df['low_volume'] = df['Low']* df['Volume']* (df['timestamp'] \/1000) #\n        df['count_open'] = df['Count']* df['Open']* (df['timestamp'] \/1000)\n        df['count_close'] = df['Count']* df['Close']* (df['timestamp'] \/1000) #\n        df['count_low'] = df['Count']* df['Low']* (df['timestamp'] \/1000)\n        df['count_high'] = df['Count']* df['High']* (df['timestamp'] \/1000) #\n        df['count_vwap'] = df['Count']* df['VWAP']* (df['timestamp'] \/1000)\n        df['count_volume'] = df['Count']* df['Volume']* (df['timestamp'] \/1000)\n        \n        df['volume_open'] = df['Volume']* df['Open']* (df['timestamp'] \/1000)\n        df['volume_close'] = df['Volume']* df['Close']* (df['timestamp'] \/1000) #\n        df['volume_low'] = df['Volume']* df['Low']* (df['timestamp'] \/1000)\n        df['volume_high'] = df['Volume']* df['High']* (df['timestamp'] \/1000) #\n        df['volume_vwap'] = df['Volume']* df['VWAP']* (df['timestamp'] \/1000)\n        df['volume_count'] = df['Volume']* df['Count']* (df['timestamp'] \/1000) #\n        ###\n        \n        df['open_sigmoid'] = sigmoid(df['Open'])* (df['timestamp'] \/1000)\n        df['close_sigmoid'] = sigmoid(df['Close'])* (df['timestamp'] \/1000)\n        df['high_sigmoid'] = sigmoid(df['High'])* (df['timestamp'] \/1000)\n        df['low_sigmoid'] = sigmoid(df['Low'])* (df['timestamp'] \/1000)\n        df['vwap_sigmoid'] = sigmoid(df['VWAP'])* (df['timestamp'] \/1000)\n        df['count_sigmoid'] = sigmoid(df['Count'])* (df['timestamp'] \/1000)\n        df['volum_sigmoid'] = sigmoid(df['Volume'])* (df['timestamp'] \/1000)\n        \n        df['open_log'] = np.log1p(df['Open'])* (df['timestamp'] \/1000)\n        df['close_log'] = np.log1p(df['Close'])* (df['timestamp'] \/1000)\n        df['high_log'] = np.log1p(df['High'])* (df['timestamp'] \/1000)\n        df['low_log'] = np.log1p(df['Low'])* (df['timestamp'] \/1000)\n        df['vwap_log'] = np.log1p(df['VWAP'])* (df['timestamp'] \/1000)\n        df['count_log'] = np.log1p(df['Count'])* (df['timestamp'] \/1000)\n        df['volum_log'] = np.log1p(df['Volume'])* (df['timestamp'] \/1000)\n    return df ","5f2394d1":"# Two new features from the competition tutorial\ndef upper_shadow(df):\n    return df['High'] - np.maximum(df['Close'], df['Open'])\n\ndef lower_shadow(df):\n    return np.minimum(df['Close'], df['Open']) - df['Low']\n\n# A utility function to build features from the original df\n# It works for rows to, so we can reutilize it.\ndef get_features(df_feat):\n    #df_feat = df[['Count', 'Open', 'High', 'Low', 'Close', 'Volume', 'VWAP']].copy()\n    df_feat['Upper_Shadow'] = upper_shadow(df_feat)\n    df_feat['Lower_Shadow'] = lower_shadow(df_feat)\n    return df_feat\ndef sigmoid(a):\n    s = 1 \/ (1 + math.e**-a)\n    return s\n\ndef get_Xy_and_model_for_asset(df_train,asset_id):\n    df = df_train[df_train[\"Asset_ID\"] == asset_id]\n    df = df.dropna(subset=['Target'])\n    y = df['Target'] \n    df      = df.drop(['Target','Asset_ID'],axis=1)\n    df_proc = get_features(df)\n    #df_proc = add_features(df_proc)\n    df_proc = df_proc.fillna(-1)\n    X= df_proc #.drop(\"y\", axis=1)\n    \n    model = LGBMRegressor(n_estimators=1500,num_leaves=700,learning_rate=0.1,silent=True)\n    model.fit(X, y)\n    \n    #print( 'step3',datetime.datetime.now().strftime('%Y\u5e74%m\u6708%d\u65e5 %H:%M:%S'))\n    fi =model.feature_importances_\n    fi_df = pd.DataFrame({'feature': list(X.columns),\n         'feature importance': fi[:]}).sort_values('feature importance', ascending = False)\n    if asset_id ==0:\n        display(fi_df)\n    return model","cd944b93":"%%capture\n'''\nXs = {}\nys = {}\nmodels = {}\n\nfor asset_id, asset_name in zip(df_asset_details['Asset_ID'], df_asset_details['Asset_Name']):\n    print(f\"Training model for {asset_name:<16} (ID={asset_id:<2})   \",datetime.datetime.now().strftime('%Y\/%m\/%d %H:%M:%S'))\n    \n    model = get_Xy_and_model_for_asset(df_train, asset_id)    \n    models[asset_id] =  model\n    \n    #validation\n    x = asset_id\n    record = df_valid[df_valid.Asset_ID == x]   \n    target = record.Target \n    record = record.drop(['Target','Asset_ID'],axis=1)\n    model = models[x]\n    x_test = get_features(record)\n    x_test = add_features(x_test) \n    #x_test = x_test.drop(['Asset_ID','Target'],axis=1)\n        \n    x_pred = model.predict(x_test)\n    print('Test score for LR baseline: ', f\"{np.corrcoef(x_pred, target)[0,1]:.5f}\")\n    #print('MSE=',MSE(x_pred.x, target))\n    #print(x_pred.x[:10])\n    #print(target[:10])\n    del record\n    del x_pred\n    del x_test\n    \n    #break\n    '''","c4a471cb":"import pickle\nwith open('..\/input\/greserchmodel\/single_models','rb') as web:\n    models = pickle.load(web)","e8121222":"saved = df_test.copy() ","6b1bda34":"    %%capture\n    '''\n    #############\n    cl = saved.columns\n    df_test = saved.copy()\n    df_test = df_test.loc[:5]\n    \n    if  type(df_test)== pd.core.series.Series:\n        df_pred = pd.DataFrame(index=range(1), columns=['Target'])\n    else:   \n        df_pred = pd.DataFrame(index=range(len(df_test)), columns=['Target'])\n    df_pred['row_id'] = df_test.row_id\n    ################\n    \n    \n    if type(df_test)== pd.core.series.Series or    (type(df_test)!= pd.core.series.Series and len(df_test)<7):\n        if type(df_test)== pd.core.series.Series:\n            test = pd.DataFrame(columns=cl,index=[int(0)])\n            for i in cl:\n                test.iloc[0,i] = df_test[i].astype(float)\n                test[i] = test[i].astype(float)\n            df_test =test.copy()\n        else:\n            test = pd.DataFrame(columns=cl,index=range(len(df_test)))\n            for x in range(len(df_test)):\n                for i in cl:\n                    test.loc[x,i] = df_test.loc[x,i].astype(float)\n                    test[i] = test[i].astype(float)\n            df_test =test.copy()\n                \n                \n        flag=1\n        df_test['PC1'] =0.001\n        df_test['PC2'] =0.001\n        df_test['PC3'] =0.001\n        df_test['PC4'] =0.001\n        df_test['PC5'] =0.001\n        df_test['PC6'] =0.001\n        df_test['PC7'] =0.001\n    else:\n    \n        dfs = df_test.iloc[:, 2:-2].apply(lambda x: (x-x.mean())\/x.std(), axis=0)  ###############-2\n        df_test.iloc[:, 2:-2] = dfs   ################-2\n        pca = PCA()\n        pca.fit(dfs)\n\n        feature = pca.transform(dfs)\n        data = pd.DataFrame(feature, columns=[\"PC{}\".format(x + 1) for x in range(len(dfs.columns))])\n        #del dfs\n        #del feature\n        df_test = pd.concat([df_test,data],axis=1)\n        del data\n        flag=0\n    \n    \n    Asset_id = df_test.Asset_ID\n    lx_test = get_features(df_test)\n    lx_test = add_features(lx_test,flag)\n    lx_test['Asset_ID'] = Asset_id\n    for x in lx_test.Asset_ID.unique():\n        model = models[x]\n        x_test = lx_test[lx_test.Asset_ID == x]\n\n        row_id = x_test.row_id\n        x_test = x_test.drop(['row_id','Asset_ID','group_num'],axis=1)    ###########\n        x_test['pred'] = model.predict(x_test)\n        \n        x_test['row_id'] =row_id\n      \n        for j,row in x_test.iterrows():\n            df_pred.loc[df_pred['row_id'] == row['row_id'], 'Target'] = np.e**(row.pred) -1\n    df_pred[:5]\n    '''","42dfe0a7":"env = gresearch_crypto.make_env()\niter_test = env.iter_test()\n\nfor i, (df_test, df_pred) in enumerate(iter_test):\n    \n    #df_test['Weight'] = df_test['Asset_ID'].map(asset_weight_dict)\n    #dfs = df_test.iloc[:, 2:-1].apply(lambda x: (x-x.mean())\/x.std(), axis=0)\n    #df_test.iloc[:, 2:-1] = dfs \n    rs = RobustScaler().fit(df_test.iloc[:,2:-1])\n    dts = rs.transform(df_test.iloc[:,2:-1])\n    df_test.iloc[:,2:-1] = dts\n        \n    Asset_id = df_test.Asset_ID\n    #lx_test = get_features(df_test)\n    lx_test = add_features(df_test)\n    lx_test['Asset_ID'] = Asset_id\n    for x in lx_test.Asset_ID.unique():\n        #print('Asset_Id=',x)\n        model = models[x]\n        x_test = lx_test[lx_test.Asset_ID == x]\n\n        row_id = x_test.row_id\n        x_test = x_test.drop(['row_id','Asset_ID'],axis=1)    \n        x_test['pred'] = model.predict(x_test)\n        \n        x_test['row_id'] =row_id\n      \n        for j,row in x_test.iterrows():\n            df_pred.loc[df_pred['row_id'] == row['row_id'], 'Target'] = np.e**(row.pred) -1\n        \n    #if x == 0:\n    #    display(df_pred)\n        \n    env.predict(df_pred)","c06b15d5":"df_test","09facf5a":"df_pred","05cb62db":"# In Progress","cc8804b8":"### \u30c7\u30fc\u30bf\u3092\u76f4\u8fd1\u306e\u3082\u306e\u3060\u3051\u306b\u3059\u308b\u3000\u4eca\u306f\u7121\u52b9\u5316\n\n### Make the data only the latest one. Now disabled","434c45e5":"## Feature Function \u3092\u4f5c\u308b\n\n## Make Feature Function","78e6d0a1":"![image.png](attachment:11d7f4d8-3b08-4b4c-a077-6661dcad5152.png)","d5826c2d":"### Target\u304cNan\u306e\u3082\u306e\u3092\u524a\u9664","1be02afd":" # \u884c\u5217\u306e\u6a19\u6e96\u5316","51df99c8":"## \u5b66\u7fd2\u30c7\u30fc\u30bf\u306e\u975e\u9023\u7d9a\u5316","c6e865e7":"# Code Test","3d6c3616":"## Training & Validation","22f2430b":"# model save","12a40cd0":"<pre>\n['timestamp', 'Count', 'Open', 'High', 'Low', 'Close', 'Volume', 'VWAP',\n       'PC1', 'PC2', 'PC3', 'PC4', 'PC5', 'PC6', 'PC7', 'Upper_Shadow',\n       'Lower_Shadow', 'hour', 'dayofweek', 'day', 'open_close', 'open_high',\n       'open_low', 'open_vwap', 'close_high', 'close_low', 'close_vwap',\n       'high_low', 'high_vwap', 'low_vwap', 'count_open', 'count_low',\n       'count_vwap', 'count_volume', 'volume_open', 'volume_low',\n       'volume_vwap', 'Openlog_10', 'Closelog_10', 'Highlog_10', 'Lowlog_10',\n       'VWAPlog_10', 'Openlog_2', 'Closelog_2', 'Highlog_2', 'Lowlog_2',\n       'VWAPlog_2'],","31f5e1bb":"## Leak\u306b\u306a\u308b\u30c7\u30fc\u30bf\u3092\u5916\u3059\u3002valid\u30c7\u30fc\u30bf\u3092\u4f5c\u308b\u3002\n\n## Remove the data that becomes Leak. Create valid data.","cc7e0bb8":"# Predict & submit","14bd89df":"\ud83d\ude3a\ud83d\ude05\u3299\ud83d\udd30\ud83d\uddd1\u2b1b\ud83d\udfe5\ud83d\udfe8\ud83d\udfe9"}}