{"cell_type":{"eefc64d7":"code","c86930f6":"code","ad4dca53":"code","74138832":"code","47829a7f":"code","ec6849c0":"code","77747056":"code","f3ad9256":"code","345c650b":"code","96456b29":"code","8250644a":"code","4f4769e5":"code","a4916007":"code","f8e1dc16":"code","4fa9fda9":"code","6902af5f":"code","9ba13eae":"code","f78e1118":"code","d776b1b0":"code","4393e4bd":"code","a27b014d":"markdown"},"source":{"eefc64d7":"import csv\nimport tensorflow as tf\nimport numpy as np\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences","c86930f6":"## get the data \n!wget --no-check-certificate  https:\/\/storage.googleapis.com\/laurencemoroney-blog.appspot.com\/bbc-text.csv ","ad4dca53":"!ls","74138832":"## setting the hyparameter\nvocab_size = 1000\nembedding_dim = 16\nmax_length = 120\ntrunc_type='post'\npadding_type='post'\noov_tok = \"<OOV>\"\ntraining_portion = .8","47829a7f":"## stop words\n## this word will be removed from the sentences\nstopwords = [ \"a\", \"about\", \"above\", \"after\", \"again\", \"against\", \"all\", \"am\", \"an\", \"and\", \"any\", \"are\", \"as\", \"at\", \"be\", \"because\", \"been\", \"before\", \"being\", \"below\", \"between\", \"both\", \"but\", \"by\", \"could\", \"did\", \"do\", \"does\", \"doing\", \"down\", \"during\", \"each\", \"few\", \"for\", \"from\", \"further\", \"had\", \"has\", \"have\", \"having\", \"he\", \"he'd\", \"he'll\", \"he's\", \"her\", \"here\", \"here's\", \"hers\", \"herself\", \"him\", \"himself\", \"his\", \"how\", \"how's\", \"i\", \"i'd\", \"i'll\", \"i'm\", \"i've\", \"if\", \"in\", \"into\", \"is\", \"it\", \"it's\", \"its\", \"itself\", \"let's\", \"me\", \"more\", \"most\", \"my\", \"myself\", \"nor\", \"of\", \"on\", \"once\", \"only\", \"or\", \"other\", \"ought\", \"our\", \"ours\", \"ourselves\", \"out\", \"over\", \"own\", \"same\", \"she\", \"she'd\", \"she'll\", \"she's\", \"should\", \"so\", \"some\", \"such\", \"than\", \"that\", \"that's\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \"there\", \"there's\", \"these\", \"they\", \"they'd\", \"they'll\", \"they're\", \"they've\", \"this\", \"those\", \"through\", \"to\", \"too\", \"under\", \"until\", \"up\", \"very\", \"was\", \"we\", \"we'd\", \"we'll\", \"we're\", \"we've\", \"were\", \"what\", \"what's\", \"when\", \"when's\", \"where\", \"where's\", \"which\", \"while\", \"who\", \"who's\", \"whom\", \"why\", \"why's\", \"with\", \"would\", \"you\", \"you'd\", \"you'll\", \"you're\", \"you've\", \"your\", \"yours\", \"yourself\", \"yourselves\" ]","ec6849c0":"sentences = []\nlabels = []\nwith open(\"bbc-text.csv\", 'r') as csvfile:\n    reader = csv.reader(csvfile, delimiter=',')\n    next(reader)\n    for row in reader:\n        labels.append(row[0])\n        sentence = row[1]\n        for word in stopwords:\n            token = \" \" + word + \" \"\n            sentence = sentence.replace(token, \" \")\n        sentences.append(sentence)","77747056":"sentences.__len__()","f3ad9256":"labels.__len__()","345c650b":"## train test split\ntrain_size = int(len(sentences) * training_portion)\n\ntrain_sentences = sentences[:train_size]\ntrain_labels = labels[:train_size]\n\nvalidation_sentences = sentences[train_size:]\nvalidation_labels = labels[train_size:]\n","96456b29":"tokenizer = Tokenizer(num_words = vocab_size, oov_token=oov_tok)\ntokenizer.fit_on_texts(train_sentences)\nword_index = tokenizer.word_index\n\ntrain_sequences = tokenizer.texts_to_sequences(train_sentences)\ntrain_padded = pad_sequences(train_sequences, padding=padding_type, maxlen=max_length)","8250644a":"validation_sequences = tokenizer.texts_to_sequences(validation_sentences)\nvalidation_padded = pad_sequences(validation_sequences, padding=padding_type, maxlen=max_length)","4f4769e5":"## we nee dt tokenize ","a4916007":"label_tokenizer = Tokenizer()\nlabel_tokenizer.fit_on_texts(labels)\n\ntraining_label_seq = np.array(label_tokenizer.texts_to_sequences(train_labels))\nvalidation_label_seq = np.array(label_tokenizer.texts_to_sequences(validation_labels))","f8e1dc16":"training_label_seq","4fa9fda9":"model = tf.keras.Sequential([\n    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n    tf.keras.layers.GlobalAveragePooling1D(),\n    tf.keras.layers.Dense(24, activation='relu'),\n    tf.keras.layers.Dense(6, activation='softmax')\n])\nmodel.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\nmodel.summary()","6902af5f":"num_epochs = 30\nhistory = model.fit(train_padded, training_label_seq, epochs=num_epochs, validation_data=(validation_padded, validation_label_seq), verbose=2)","9ba13eae":"import matplotlib.pyplot as plt\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()\n# summarize history for loss\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","f78e1118":"model2 = tf.keras.Sequential([\n    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64,return_sequences=True)),\n    tf.keras.layers.GlobalAveragePooling1D(),\n    tf.keras.layers.Dense(24, activation='relu'),\n    tf.keras.layers.Dense(6, activation='softmax')\n])\nmodel2.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\nmodel2.summary()","d776b1b0":"num_epochs = 30\nhistory = model2.fit(train_padded, training_label_seq, epochs=num_epochs, validation_data=(validation_padded, validation_label_seq), verbose=2)","4393e4bd":"plt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()\n# summarize history for loss\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","a27b014d":"## LSTM layer keep the cell state so it is more and in recurrent nural net the product of one function will be feed to another with the output so it is more preferable for text analysis "}}