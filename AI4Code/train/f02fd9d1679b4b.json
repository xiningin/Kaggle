{"cell_type":{"7e217c19":"code","d519f9a7":"code","010f5797":"code","ed3b012d":"code","8a4c2966":"code","289b921c":"code","6b225166":"code","66ceea5a":"code","b23eaab2":"code","7c3a13a2":"code","914bc222":"code","57da4537":"code","77d4644c":"code","478ec1b0":"code","96b9663c":"code","34d78730":"code","d39139ab":"code","015bafcb":"code","d3fe2646":"code","17bf2c04":"code","1d2a8b87":"code","5864d6b3":"code","0628e6bd":"code","ab501fe7":"code","c39a4739":"code","34143a69":"code","57d0eaf1":"code","67531884":"code","332a74d8":"code","08f4a20f":"code","5caacc8c":"code","923c0195":"code","f6548a7a":"code","8877462f":"code","b19ac860":"code","2a7b4531":"markdown","e6028644":"markdown","3bb31104":"markdown","381274ea":"markdown","0cfe1f75":"markdown","a7901623":"markdown","33e6a687":"markdown","d61e2df9":"markdown","fc176b8e":"markdown","9606a924":"markdown","5f5d9540":"markdown","02c305a8":"markdown","1638c666":"markdown","77b0aa24":"markdown","703a11eb":"markdown","6821df10":"markdown","02d83971":"markdown","f46c4fd0":"markdown","f6c34644":"markdown","90559e21":"markdown","67b71cf8":"markdown","49d8ab72":"markdown","1f265edb":"markdown","ac348ced":"markdown","9465ce3b":"markdown","3b437819":"markdown","940efde6":"markdown","2254b4ac":"markdown","ae1619b2":"markdown","f6c61fcc":"markdown","f700f56a":"markdown","ffba424b":"markdown","bc159519":"markdown","628c0c35":"markdown","cedccb2f":"markdown","098af97d":"markdown","0a367f14":"markdown","91b04210":"markdown","96509831":"markdown"},"source":{"7e217c19":"# load libraries for data manipulation\nimport numpy as np\nimport pandas as pd\nfrom scipy import stats\nfrom scipy.stats import norm\nfrom scipy.stats import skew\nfrom scipy.special import boxcox1p\nfrom scipy.stats import boxcox_normmax\n# load visualization libraries\nimport matplotlib.pyplot as plt\nplt.style.use('classic')\n%matplotlib inline\nimport seaborn as sb\nsb.set()\n# load modelling libraries\nfrom sklearn.linear_model import Ridge,  Lasso, ElasticNet\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.model_selection import  cross_val_score\n# warnings\nimport string\nimport warnings\nwarnings.filterwarnings('ignore')","d519f9a7":"# load the train and test data sets\ntrain = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/train.csv\")\ntest = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/test.csv\")\ntrain_len = len(train)\n# combine train and test data sets\ndata_all = train.append(test, sort=False, ignore_index=True)\nfeatures = data_all.shape[1]\n# check for duplicates\nidUnique = len(set(data_all.Id))\nidTotal = data_all.shape[0]\nidDupli = idTotal - idUnique\nprint(\"There are \" + str(idDupli) + \" duplicate IDs for \" + str(idTotal) + \" total entries\")\nprint(\"There are \" + str(features) + \" variables for \" + str(idTotal) + \" total entries\")","010f5797":"# group variables in quantity(scale) and quality(categorical)\nquantity = [f for f in data_all.columns if data_all.dtypes[f] != 'object']\nquantity.remove('SalePrice')\nquantity.remove('Id')\nquality = [f for f in data_all.columns if data_all.dtypes[f] == 'object']","ed3b012d":"print(quantity)","8a4c2966":"print(quality)","289b921c":"missing_quant = (data_all[quantity].isnull().sum()\/data_all[quantity].isnull().count()).sort_values(ascending=False)\nmissing_quant = missing_quant[missing_quant > 0] * 100\nprint(\"There are {} quantitative features with  missing values :\".format(missing_quant.shape[0]))\nmissing_quant = pd.DataFrame({'Percent' :missing_quant})\nmissing_quant.head()","6b225166":"np.where(pd.isnull(data_all.GarageArea))","66ceea5a":"# fill missing values\ndata_all['LotFrontage'] = data_all.groupby(['Neighborhood'])['LotFrontage'].apply(lambda x: x.fillna(x.median()))\ndata_all.at[2576, 'GarageYrBlt'] = 0\ndata_all['GarageYrBlt'] = data_all.groupby(['Neighborhood'])['GarageYrBlt'].apply(lambda x: x.fillna(x.median()))\nfor col in ['MasVnrArea','BsmtFinSF1','BsmtFinSF2','BsmtUnfSF','TotalBsmtSF','BsmtFullBath','BsmtHalfBath','GarageCars',\n            'GarageArea']:\n    data_all[col] = data_all[col].fillna(0)","b23eaab2":"missing_qual = (data_all[quality].isnull().sum()\/data_all[quality].isnull().count()).sort_values(ascending=False)\nmissing_qual = missing_qual[missing_qual > 0] * 100\nprint(\"There are {} qualitative features with  missing values :\".format(missing_qual.shape[0]))\nmissing_qual = pd.DataFrame({'Percent' :missing_qual})\nmissing_qual.head(10)","7c3a13a2":"# fill missing values\ndata_all['MSZoning'] = data_all.groupby(['Neighborhood', 'MSSubClass'])['MSZoning'].apply(lambda x: x.fillna(x.value_counts().index[0]))\ndata_all['Utilities'] = data_all.groupby(['Neighborhood', 'MSSubClass'])['Utilities'].apply(lambda x: x.fillna(x.value_counts().index[0]))\ndata_all['Exterior1st'] = data_all.groupby(['Neighborhood', 'MSSubClass'])['Exterior1st'].apply(lambda x: x.fillna(x.value_counts().index[0]))\ndata_all['Exterior2nd'] = data_all.groupby(['Neighborhood', 'MSSubClass'])['Exterior2nd'].apply(lambda x: x.fillna(x.value_counts().index[0]))\ndata_all['MasVnrType'] = data_all.groupby(['Neighborhood', 'MSSubClass'])['MasVnrType'].apply(lambda x: x.fillna(x.value_counts().index[0]))\nfor col in ['BsmtQual','BsmtCond','BsmtExposure','BsmtFinType1','BsmtFinType2','FireplaceQu','GarageType','GarageFinish',\n           'GarageQual','GarageCond','Alley','Fence','PoolQC','MiscFeature']:\n    data_all[col] = data_all[col].fillna('None')\ndata_all['Electrical'] = data_all.groupby(['Neighborhood','MSSubClass' ])['Electrical'].apply(lambda x: x.fillna(x.value_counts().index[0]))\ndata_all['KitchenQual'] = data_all.groupby(['Neighborhood','MSSubClass' ])['KitchenQual'].apply(lambda x: x.fillna(x.value_counts().index[0]))\ndata_all['Functional'] = data_all.groupby(['Neighborhood', 'MSSubClass'])['Functional'].apply(lambda x: x.fillna(x.value_counts().index[0]))\ndata_all['SaleType'] = data_all.groupby(['Neighborhood', 'MSSubClass'])['SaleType'].apply(lambda x: x.fillna(x.value_counts().index[0]))","914bc222":"data_all.iloc[np.where(data_all.GrLivArea > 4000)]","57da4537":"scatter = sb.regplot(x='GrLivArea', y='SalePrice', fit_reg =False, data=data_all)","77d4644c":"# dropping the outliers in the train set\ndata_all = data_all.drop(data_all[data_all['Id'] == 524].index)\ndata_all = data_all.drop(data_all[data_all['Id'] == 1299].index)\n# adjusting train len\ntrain_len = train_len - 2\n# fill the outlier in test set with the median\ndata_all.loc[2549,('GrLivArea')]  = data_all['GrLivArea'].median()\ndata_all.loc[2549,('LotArea')]  = data_all['LotArea'].median()","478ec1b0":"#correlation matrix\ncorrmatrix = data_all[:train_len].corr()\nf, ax = plt.subplots(figsize=(30, 24))\nk = 36 \ncols = corrmatrix.nlargest(k, 'SalePrice')['SalePrice'].index\ncm = np.corrcoef(data_all[:train_len][cols].values.T)\nsb.set(font_scale=1.0)\nhm = sb.heatmap(cm, cbar=True, annot=True, square=True, fmt='.1f', annot_kws={'size': 18}, yticklabels=cols.values, xticklabels=cols.values)\nplt.show()","96b9663c":"# scatterplot of dependent vs independent variables\nquantitative = [f for f in data_all.columns if f in cols]\nquantitative.remove('SalePrice')\nvarx = pd.melt(data_all, id_vars=['SalePrice'], value_vars=quantitative)\ngx = sb.FacetGrid(varx, col=\"variable\",  col_wrap=3, sharex=False, sharey=False, height=5)\ngx = gx.map(sb.regplot, \"value\", \"SalePrice\")","34d78730":"# distribution plot of dependent and independent variables\n#quantitative = [f for f in data_all.columns if f in cols]\n#varx = pd.melt(data_all, value_vars=quantitative)\n#gx = sb.FacetGrid(varx, col=\"variable\",  col_wrap=3, sharex=False, sharey=False, height=5)\n#gx = gx.map(sb.distplot, (\"value\"), fit=norm)","d39139ab":"# impute ordinal data with numeric values\ndata_all['KitchenQual'].replace(['Ex','Gd','TA','Fa'],[4,3,2,1],inplace=True)\ndata_all['FireplaceQu'].replace(['Ex','Gd','TA','Fa','Po', 'None'],[6,5,4,3,2,1],inplace=True)\ndata_all['GarageQual'].replace(['Ex','Gd','TA','Fa','Po','None'],[6,5,4,3,2,1],inplace=True)\ndata_all['GarageCond'].replace(['Ex','Gd','TA','Fa','Po','None'],[6,5,4,3,2,1],inplace=True)\ndata_all['PoolQC'].replace(['Ex','Gd','TA','Fa','None'],[5,4,3,2,1],inplace=True)\ndata_all['ExterQual'].replace(['Ex','Gd','TA','Fa'],[4,3,2,1],inplace=True)\ndata_all['ExterCond'].replace(['Ex','Gd','TA','Fa','Po'],[5,4,3,2,1],inplace=True)\ndata_all['BsmtQual'].replace(['Ex','Gd','TA','Fa','Po','None'],[6,5,4,3,2,1],inplace=True)\ndata_all['BsmtCond'].replace(['Ex','Gd','TA','Fa','Po','None'],[6,5,4,3,2,1],inplace=True)\ndata_all['BsmtExposure'].replace(['Gd','Av','Mn','No','None'],[5,4,3,2,1],inplace=True)\ndata_all['HeatingQC'].replace(['Ex','Gd','TA','Fa','Po'],[5,4,3,2,1],inplace=True)\n# transform discrete features to  categorical feature\ndata_all['MSSubClass'] = data_all['MSSubClass'].astype(str)\ndata_all['YrSold'] =    data_all['YrSold'].astype(str)   \ndata_all['MoSold'] =    data_all['MoSold'].astype(str)  ","015bafcb":"# combinations of old features\ndata_all['GarageScale'] = data_all['GarageCars'] * data_all['GarageArea']\ndata_all['GarageOrdinal'] = data_all['GarageQual'] + data_all['GarageCond']\ndata_all['AllPorch'] = data_all['OpenPorchSF'] + data_all['EnclosedPorch'] + data_all['3SsnPorch'] + data_all['ScreenPorch']\ndata_all['ExterOrdinal'] = data_all['ExterQual'] + data_all['ExterCond']\ndata_all['KitchenCombined'] = data_all['KitchenQual'] * data_all['KitchenAbvGr']\ndata_all['FireplaceCombined'] = data_all['FireplaceQu'] * data_all['Fireplaces']\ndata_all['BsmtOrdinal'] = data_all['BsmtQual'] + data_all['BsmtCond']\ndata_all['BsmtFinishedAll'] = data_all['BsmtFinSF1'] + data_all['BsmtFinSF2']\ndata_all['AllFlrSF'] = data_all['1stFlrSF'] + data_all['2ndFlrSF']\ndata_all['OverallCombined'] = data_all['OverallQual'] + data_all['OverallCond']\ndata_all['TotalFullBath'] = data_all['BsmtFullBath'] +  + data_all[\"FullBath\"] \ndata_all['TotalHalfBath'] = data_all[\"HalfBath\"] + data_all['BsmtHalfBath']\ndata_all['TotalSF'] = data_all['AllFlrSF'] + data_all['TotalBsmtSF']\ndata_all['YrBltAndRemod'] = data_all[\"YearRemodAdd\"] + data_all['YearBuilt']","d3fe2646":"cat_features = [f for f in data_all.columns if data_all.dtypes[f] == 'object']\ncategorical = []\nfor i in data_all.columns:\n    if i in cat_features:\n        categorical.append(i)\ndata_cat = data_all[categorical]\ndata_cat = pd.get_dummies(data_cat)","17bf2c04":"# Drop features with zeros > 99.95\noverfit = []\nfor i in data_cat.columns:\n    counts = data_cat[i].value_counts()\n    zeros = counts.iloc[0]\n    if zeros \/ len(data_cat) * 100 > 99.95:\n        overfit.append(i)\ndata_cat = data_cat.drop(overfit, axis=1)\nprint(\"There are {} qualitative features with zeros > 99.95 :\".format(len(overfit)))","1d2a8b87":"numeric = [f for f in data_all.columns if data_all.dtypes[f] != 'object']\nnumeric.remove('Id')\n#calculate spearman corr. coef.\nfeatures = numeric\nspr = pd.DataFrame()\nspr['feature'] = features\nspr['spearman'] = [data_all[f].corr(data_all['SalePrice'], 'spearman') for f in features]\nspr = spr.sort_values('spearman')\nnumeric_cols = spr['feature'].tail(47)# select features with corr. approximately => 0.2\n#plot spearman corr. coef.\nplt.figure(figsize=(6, 0.25*len(features)))\nbar = sb.barplot(data=spr, y='feature', x='spearman', orient='h')","5864d6b3":"# Log transform the target\ndata_all[:train_len].loc[:,('SalePrice')] = np.log1p(data_all[:train_len]['SalePrice'])\n# test SalePrice for normality\nfig = plt.figure()\nres = stats.probplot(data_all[:train_len]['SalePrice'], plot=plt)","0628e6bd":"# gather numerical features\nnumerical = numeric_cols.values.tolist()\nnumerical.remove('SalePrice')\ndata_num = data_all[numerical].astype(float)\n# calculate skewness \nskewness = data_num.apply(lambda x: skew(x)).sort_values(ascending=False)\nskewness = skewness[abs(skewness) > 0.5]\nprint(\"There are {} numerical features with absolute Skew > 0.5 :\".format(skewness.shape[0]))\nskewness = pd.DataFrame({'Skew' :skewness})\nskewness.head(10)","ab501fe7":"# Box-Cox transformation\nfor i in skewness.index.tolist():\n    data_num[i] = boxcox1p(data_num[i], boxcox_normmax(data_num[i] + 1))","c39a4739":"# calculate skewness \nskewness = data_num.apply(lambda x: skew(x)).sort_values(ascending=False)\nskewness = skewness[abs(skewness) > 0.5]\n# transforming negative skewed features\nneg_skew = skewness[skewness < 0]\ndata_num.loc[:,('GarageOrdinal')] = data_num['GarageOrdinal']**1.1\ndata_num.loc[:,('GarageQual')] = data_num['GarageQual']**1.2\ndata_num.loc[:,('GarageCond')] = data_num['GarageCond']**1.3\ndata_num.loc[:,('HeatingQC')] = data_num['HeatingQC']**1.6\n# transforming positive skewed features\npos_skew = skewness[skewness > 0]\npos_skew1 = ['Fireplaces','BsmtFullBath','OpenPorchSF','FireplaceCombined','TotalHalfBath',\n             'WoodDeckSF','2ndFlrSF']\ndata_num.loc[:,pos_skew1] = np.sqrt(data_num[pos_skew1])\ndata_num.loc[:,('HalfBath')] = np.cbrt(data_num['HalfBath'])\ndata_num.loc[:,('MasVnrArea')] = np.log2(data_num['MasVnrArea']+1)\n# check high skewed features\nskewness = data_num.apply(lambda x: skew(x)).sort_values(ascending=False)\nskewness = skewness[abs(skewness) > 1.0]\n# Drop highly skewed features\ndata_num = data_num.drop(skewness.index.tolist(), axis=1)\nprint(\"There are {} numerical features with absolute Skew > 1.0 :\".format(skewness.shape[0]))","34143a69":"scatter = sb.regplot(x='GrLivArea', y='SalePrice', fit_reg =False, data=data_all)","57d0eaf1":"# convert numerical features to standard score\ndata_num = (data_num - data_num.mean())\/data_num.std()\n# separate train and test sets\ndf_all = pd.concat([data_num, data_cat], axis = 1)\nfeatures = (data_cat.columns).append(data_num.columns)\ndf_train = df_all[:1458]\ndf_test = df_all[1458:]\ntarget = 'SalePrice'\nX = df_train[features]\ny = data_all[:1458][target]\nX_test = df_test[features]","67531884":"# Defining RMSE\ndef rmse_cv(model):\n    rmse = np.sqrt(-cross_val_score(model, X, y, scoring = \"neg_mean_squared_error\", cv = 5))\n    return(rmse)","332a74d8":"# Ridge CV and model fitting\nridge = Ridge(alpha = 14)\ncv_ridge = rmse_cv(ridge)\nridge.fit(X , y)\nprint('Ridge CV score min: ' + str(cv_ridge.min()) + ' mean: ' + str(cv_ridge.mean()) \n      + ' max: ' + str(cv_ridge.max()) )","08f4a20f":"# Plot ridge important coeffients\ncoefs = pd.Series(ridge.coef_, index = X.columns)\nimp_coefs = pd.concat([coefs.sort_values().head(10),\n                       coefs.sort_values().tail(10)])\nplt.figure(figsize=(6, 8))\nimp_coefs.plot(kind = \"barh\")\nplt.title(\"10 Top and Bottom Coefficients\")\nplt.show()","5caacc8c":"# Lasso CV and model fitting\nlasso = Lasso(alpha = .00066)\ncv_lasso = rmse_cv(lasso)\nlasso.fit(X, y)\nprint('Lasso CV score min: ' + str(cv_lasso.min()) + ' mean: ' + str(cv_lasso.mean()) \n      + ' max: ' + str(cv_lasso.max()) )","923c0195":"# Lasso features\ncoefs = pd.Series(lasso.coef_, index = X.columns)\nprint(\"Lasso selected \" + str(sum(coefs != 0)) + \" features and discarded \" +  str(sum(coefs == 0)) + \" features\")","f6548a7a":"# GradientBoostng CV and model fitting\ngbrt = GradientBoostingRegressor(n_estimators=1000, learning_rate=.03, max_depth=3, max_features=.04, min_samples_split=4,\n                                 min_samples_leaf=3, loss='huber', subsample=1.0, random_state=0)\ncv_gbrt = rmse_cv(gbrt)\ngbrt.fit(X, y)\nprint('GradientBoosting CV score min: ' + str(cv_gbrt.min()) + ' mean: ' + str(cv_gbrt.mean()) \n      + ' max: ' + str(cv_gbrt.max()) )","8877462f":"from datetime import datetime\n# model blending \ndef blend_models(X):\n    return ((ridge.predict(X)) + (gbrt.predict(X)))\/2\n\nridge_preds =  np.expm1(ridge.predict(X_test))\ngbrt_preds = np.expm1(gbrt.predict(X_test))\n\nridge_gbrt_preds = np.expm1(blend_models(X_test))\nhp_ridge_gbrt = pd.DataFrame({'Id':test.Id, 'SalePrice':ridge_gbrt_preds})\nhp_ridge_gbrt.to_csv('solution.csv', index = False)","b19ac860":"fig = plt.figure(figsize=(6, 4))\naxes = fig.add_subplot(1,1,1)\npredictions = pd.DataFrame({\"ridge\":ridge_preds, \"gbrt\":gbrt_preds})\nplt.plot(gbrt_preds, ridge_preds, '.')\naxes.set_xlabel('gbrt')\naxes.set_ylabel('ridge')","2a7b4531":"### 1.1 Scale Missing Values\nImpute missing values for continous and count variables.","e6028644":"Log transformation of the target variable will reduce deviation from a normal distribution and reduce errors in predicting expensive and cheap houses.","3bb31104":"## 1.0 Data Preparation\nLoad data, identify missing values and outliers, and EDA","381274ea":"#### 2.3.2 Normality and Homogeneity of Variance\nHomogeneity of variance is simply having equal spreads between the predictor variable and the target variable. It ensures the error is same across all values of the predictor variable. This is acheived by reducing a skewed distribution to an approximately symmetrical normal distribution and can impact on linear model performance.","0cfe1f75":"Separate train and test sets for modeling","a7901623":"### 1.4 EDA\nTest independent scale variables for linearity and normality ","33e6a687":"No distribution is normally distributed, skewness and kurtosis enable us to assess deviation from normality. The variables with moderate to high skew will need a transformation to make them approximately normally distributed.","d61e2df9":"The heatmap shows some features have good correlation with the target and also high collinearity between GrLivArea\/TotRmsAbvGrd, GarageCars\/GarageArea, TotalBsmtSF\/1stFlrSF and YearBuilt\/GarageYrBlt. However there are features with low correaltion with the target (i.e. absolute correlation between 0.0 and 3.0)","fc176b8e":"### 2.2 Categorical Encoding\nEncode categoricals with one hot encoding","9606a924":"#### 2.3.1 Linearity with Target\nSpearman correlation coeffient can be used to find correlation of rank data and skewed scale data. It can give more reliable result in comparing the scale and skewed data as it does not need to satisfy the linearity test. Adapted from https:\/\/kaggle.com\/dgawlik\/house-prices-eda","5f5d9540":"There are 2919 observations with 81 columns. Excluding the target variable SalePrice and Id there are 79 independent variables. The train set has 1460 observations while the test set has 1459 observations, the target variable SalePrice is absent in test. The aim of this study is to train a model on the train set and use it to predict the target SalePrice of the test set.","02c305a8":"### 1.3 Outliers\nOutliers can adversely affect the result of data analysis. Dean De Cook author of Ames house dataset recommended the removal of some 5 outliers representing unsual sales in GrLivArea greater than 4000 square feet. Ref: www.amstat.org\/publications\/jse\/v19n3\/decock.pdf","1638c666":"The LotFontage described as the linear feet of street connected to the property was absent in about 16% of the properties. This value may be absent due to measurement difficulty. GarageYearBlt with 5.45% missing values can be inability to collect correct info because less than 1% of the properties have no GarageArea. LotFrontage and GarageYrBlt will be filled with median values of the neighborhood to keep original distribution as much as possible. Other variables with less than 1% missing values will be filled with zero.","77b0aa24":"Beyond feature engineering hyper parameter tuning and hybrid regression extended model performance.","703a11eb":"Graph of GrLivArea and SalePrice showing equal levels of SalePrice dispersion among the values of GrLivArea after both variable satisfaction of an approximately symmetrical normal distribution.","6821df10":"There are 36 scale variables","02d83971":"There are features explicitly stated as NA on a property, those features will be filled with None, while other NA not explicitly assigned in the data description will be filled with the mode in the Neighborhood.","f46c4fd0":"Combime some old features to form new ones.","f6c34644":"#### 3.1.2 Gradient Boosting Regressor","90559e21":"### 3.2 Model Evaluation","67b71cf8":"#### 3.1.1 Linear Regressors","49d8ab72":"3 of the houses are outliers, large houses prized relatively low, while the 2 on top in the scatter are very large houses with commensurate sales . The 2 outliers in the train set will be removed while the other outlier ID 2550 is in the test set .","1f265edb":"Index no. 2576 has no GarageArea and GarageCars and consequently no GarageYrBlt and hence GarageYrBlt will be filled with zero.","ac348ced":" #### 1.4.1 Linearity Test","9465ce3b":"Plotting relationship between ridge and gbrt predictions","3b437819":"The independent variables show weak, moderate and high lineaity with the target, some with increasing linearity with the target and some with decreasing linearity with the target. The following features show decreasing lineraity - OverallCond, BsmtFinSF2, LowQualFinSF, BsmtHalfBath, MiscVal and YrSold.","940efde6":"There are 43 categorical variables","2254b4ac":"#### Conclusion","ae1619b2":"### 2.3 Numeric Feature Optimization\nSelect features with minimal linearity and reduce skews to symetrical normal distribution","f6c61fcc":"Use Box-Cox transformation to reduce skewed features to an approximately normal distribution.","f700f56a":"Features with zero values that can be described as almost 100% can cause overfitting and will be dropped. ","ffba424b":"## INTRODUCTION\nThe regularized linear models ridge, lasso and elasticnet,  and gradient boosting regression tree were used in fitting the data. Simple feature engineering, feature optimization, hyper parameter tuning and hybrid regression extended model performance to <b>LB score 0.11904 Top 5% <\/b>","bc159519":"## 3. Modeling\nCross validation and model evaluation","628c0c35":"### 1.2 Categorical Missing Values\nImpute missing values for nominal and ordinal variables","cedccb2f":" Manual skew reduction will be attempted for moderate and high skewed features. Features that cannot be reduced to < 1.0 will be dropped ","098af97d":"## 2. 0 Data Manipulations\n\nCreate new variables, and transform features for modeling\n","0a367f14":"#### 1.4.2 Normality Test","91b04210":"Probability plot indicates SalePrice deviation from a normal distribution is not significant.","96509831":"### 2.1 Feature Extraction\nCreate new features from existing features. There are 2 types of categorical variables, nominal and ordinal. The ordinal variables show some rank and will be encoded with numeric values."}}