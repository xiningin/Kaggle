{"cell_type":{"a6a0fd91":"code","478f6604":"code","5a6f68b9":"code","caa567ac":"code","41b02d5c":"code","cf11c002":"code","ef9e786b":"code","7d90267d":"code","a3294b18":"code","a47ce80c":"code","946541f1":"code","b42e15d9":"code","c46823ae":"code","622d73fd":"code","be0dc1de":"code","ba4eecc8":"code","91c47252":"code","a413e7da":"code","3761ed23":"code","9c63bf37":"code","3be060d9":"code","b812b5fd":"code","2864cb15":"markdown","0545c319":"markdown"},"source":{"a6a0fd91":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn import datasets\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","478f6604":"iris = datasets.load_iris()","5a6f68b9":"\nprint(iris.target_names)\n\nprint(iris.feature_names)","caa567ac":"print(iris.data[0:5])","41b02d5c":"print(iris.target)","cf11c002":"data=pd.DataFrame({\n    'sepal length':iris.data[:,0],\n    'sepal width':iris.data[:,1],\n    'petal length':iris.data[:,2],\n    'petal width':iris.data[:,3],\n    'species':iris.target\n})\ndata.head()","ef9e786b":"from sklearn.model_selection import train_test_split\n\nX=data[['sepal length', 'sepal width', 'petal length', 'petal width']]  # Features\ny=data['species']  # Labels\n\n# Split dataset into training set and test set\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)","7d90267d":"from sklearn.ensemble import RandomForestClassifier\n\n#Create a Gaussian Classifier\nclf=RandomForestClassifier(n_estimators=100)\n\n#Train the model using the training sets y_pred=clf.predict(X_test)\nclf.fit(X_train,y_train)\n\ny_pred=clf.predict(X_test)","a3294b18":"from sklearn import metrics\n# Model Accuracy, how often is the classifier correct?\nprint(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))","a47ce80c":"import matplotlib.pyplot as plt\n\nplt.plot(y_test)","946541f1":"plt.plot(y_pred)","b42e15d9":"plt.plot(y_test, y_pred)","c46823ae":"clf.predict([[3, 5, 4, 2]])","622d73fd":"from sklearn.ensemble import RandomForestClassifier\n\n#Create a Gaussian Classifier\nclf=RandomForestClassifier(n_estimators=100)\n\n#Train the model using the training sets y_pred=clf.predict(X_test)\nclf.fit(X_train,y_train)","be0dc1de":"RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n            max_depth=None, max_features='auto', max_leaf_nodes=None,\n            min_impurity_decrease=0.0, min_impurity_split=None,\n            min_samples_leaf=1, min_samples_split=2,\n            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=1,\n            oob_score=False, random_state=None, verbose=0,\n            warm_start=False)","ba4eecc8":"feature_imp = pd.Series(clf.feature_importances_,index=iris.feature_names).sort_values(ascending=False)\nfeature_imp","91c47252":"import seaborn as sns\n%matplotlib inline\n# Creating a bar plot\nsns.barplot(x=feature_imp, y=feature_imp.index)\n# Add labels to your graph\nplt.xlabel('Feature Importance Score')\nplt.ylabel('Features')\nplt.title(\"Visualizing Important Features\")\nplt.legend()\nplt.show()","a413e7da":"from sklearn.model_selection import train_test_split\n# Split dataset into features and labels\nX=data[['petal length', 'petal width','sepal length']]  # Removed feature \"sepal length\"\ny=data['species']                                       \n# Split dataset into training set and test set\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.70, random_state=5)","3761ed23":"from sklearn.ensemble import RandomForestClassifier\n\n#Create a Gaussian Classifier\nclf=RandomForestClassifier(n_estimators=100)\n\n#Train the model using the training sets y_pred=clf.predict(X_test)\nclf.fit(X_train,y_train)\n\n# prediction on test set\ny_pred=clf.predict(X_test)\n\n#Import scikit-learn metrics module for accuracy calculation\nfrom sklearn import metrics\n# Model Accuracy, how often is the classifier correct?\nprint(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))","9c63bf37":"plt.plot(y_pred)","3be060d9":"plt.plot(y_pred)","b812b5fd":"plt.plot(y_test, y_pred)","2864cb15":"# RANDOM FOREST","0545c319":"![Decission tree *VS* Random Forest](https:\/\/i2.wp.com\/syncedreview.com\/wp-content\/uploads\/2017\/10\/Screen-Shot-2017-06-10-at-11.11.43-AM.png?fit=1344%2C670&ssl=1)"}}