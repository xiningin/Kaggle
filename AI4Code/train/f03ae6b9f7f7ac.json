{"cell_type":{"cafb1fe3":"code","50bb00b7":"code","ed6931f9":"code","72a71538":"code","7968eac8":"code","d07e2d20":"code","af426948":"code","d1e5e7c1":"code","d2187710":"code","a04e0409":"code","b0593dc3":"code","8be0e5ba":"code","5f744d56":"code","06eb82d8":"code","1f03f8db":"code","7549bd73":"code","60cf183e":"code","4c4509da":"code","0f3bfe77":"code","97c02109":"code","db5aa4d4":"code","f5f88219":"code","334e102f":"code","0e6ccbe8":"code","635d481d":"code","7b5d8c94":"code","a61df407":"code","3db1de0a":"code","25f0cb0e":"code","1781f336":"code","6bcf0b39":"code","98a5e0a3":"code","7b6c57cf":"code","277e6695":"code","ee020963":"code","d17e9bb8":"code","494bbd60":"code","93da1c52":"code","059c1540":"code","e5ca91b1":"code","cd87788b":"code","f3f727d6":"code","83477d26":"code","e2d99348":"code","86ebe966":"code","b6a019a6":"markdown","e3fd1a0c":"markdown","19c5abd2":"markdown","99b96a73":"markdown","af72a058":"markdown","cedd3ebd":"markdown","c5c01719":"markdown","16f66318":"markdown","e5ec4a2d":"markdown","f7e4aa62":"markdown","f5b3c7fe":"markdown","dea19c89":"markdown","98b86edd":"markdown","fc709e13":"markdown","f96133a3":"markdown","102d9371":"markdown"},"source":{"cafb1fe3":"# Let's install Feature-engine\n# this package will allow us to quickly remove \n# non-predictive variables\n\n!pip install feature-engine","50bb00b7":"import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n# to sample the hyperparameter space based on distributions\nfrom scipy import stats\n\n# I use GBM because it usually out-performs other off-the-shelf \n# classifiers\nfrom sklearn.ensemble import GradientBoostingClassifier\n\n# to evaluate features\nfrom sklearn.feature_selection import chi2\n\n# metric to optimize for the competition\nfrom sklearn.metrics import roc_auc_score\n\n# to optimize the hyperparameters we import the randomized search class\nfrom sklearn.model_selection import (\n    RandomizedSearchCV,\n    train_test_split,\n)\n\n# to assemble various procedures in sequence\nfrom sklearn.pipeline import Pipeline\n\n# some methods to work with imbalanced data are based in nearest neighbours\n# and nearest neighbours are sensitive to the magnitude of the features\n# so we need to scale the data\nfrom sklearn.preprocessing import (\n    MinMaxScaler,\n    Binarizer,\n)\n\n# import selection classes from Feature-engine\n# to reduce the number of features\nfrom feature_engine.selection import (\n    DropDuplicateFeatures,\n    DropConstantFeatures,\n)\n\n# to apply sklearn transformers to a subset of features\nfrom feature_engine.wrappers import SklearnTransformerWrapper\n\n# over-sampling techniques for imbalanced data\nfrom imblearn.over_sampling import SMOTENC\n\n# under-sampling techniques for imbalanced data\nfrom imblearn.under_sampling import (\n    InstanceHardnessThreshold,\n)\n\n# special ensemble methods to work with imbalanced data\n# we will use those based on boosting, which tend to work better\nfrom imblearn.ensemble import (\n    RUSBoostClassifier,\n    EasyEnsembleClassifier,\n)\n\n# to put the final model together at the end of the notebook\nfrom imblearn.pipeline import Pipeline as imb_Pipeline","ed6931f9":"# load the Santander Customer Satisfaction dataset\n\ndata = pd.read_csv('\/kaggle\/input\/santander-customer-satisfaction\/train.csv')","72a71538":"# separate dataset into train and test sets\n# I split 20:80 mostly to reduce the size of the train set\n# so that this notebook does not run out of memory :_(\n\nX_train, X_test, y_train, y_test = train_test_split(\n    data.drop(labels=['ID','TARGET'], axis=1),\n    data['TARGET'],\n    test_size=0.8,\n    random_state=0)\n\nX_train.shape, X_test.shape","7968eac8":"# check class imbalance\n\ny_train.value_counts(normalize=True), y_train.value_counts()","d07e2d20":"# check also the test set\ny_test.value_counts(normalize=True)","af426948":"# to remove constant and duplicated features, we use the transformers from Feature-engine\n\npipe = Pipeline([\n    ('constant', DropConstantFeatures(tol=1)), # drops constant features\n    ('duplicated', DropDuplicateFeatures()), # drops duplicates\n])\n\n# find features to remove\npipe.fit(X_train, y_train)","d1e5e7c1":"# how many constant features are there in the dataset?\n\nlen(pipe.named_steps['constant'].features_to_drop_)","d2187710":"# how many duplicated features are there in the dataset?\n\nlen(pipe.named_steps['duplicated'].features_to_drop_)","a04e0409":"print('Number of original variables: ', X_train.shape[1])\n\n# see how with the pipeline we can apply all transformers in sequence\n# with one line of code, for each data set\nX_train = pipe.transform(X_train)\nX_test = pipe.transform(X_test)\n\nprint('Number of variables after selection: ', X_train.shape[1])","b0593dc3":"# we find features with the same value in 98% of \n# the observations\n\nsel_ = DropConstantFeatures(tol=0.98)\n\nsel_.fit(X_train)","8be0e5ba":"# how many quasi-constant features are there in the dataset?\n\nlen(sel_.features_to_drop_)","5f744d56":"# Let's look at 1 feature\n\nsel_.features_to_drop_[0]","06eb82d8":"# we see that this feature has the value 0 in more than 99% of the observations\n\nX_train[sel_.features_to_drop_[0]].value_counts(normalize=True)","1f03f8db":"# let's replace values greater than 0 by 1\n# for this we need the Binarizer from sklearn with threshold 0\n\n# capture quasi cosntant features in a list\nquasi_ = list(sel_.features_to_drop_)\n\n# in order to modify just the quasi-constant features\n# we use the sklearn wrapper from feature engine,\n# which by the way also returns a dataframe\n\nbinarizer_ = SklearnTransformerWrapper(\n    transformer = Binarizer(threshold=0),\n    variables = quasi_,\n)\n\nbinarizer_.fit(X_train)\n\nX_train = binarizer_.transform(X_train)\nX_test = binarizer_.transform(X_test)","7549bd73":"# Now, if we re-evaluate the quasi-constant feature, we should see only 2 values\n\nX_train[quasi_[0]].value_counts(normalize=True)","60cf183e":"# Now let-s evaluate the distribution of the features\n\n# Compute chi-squared stats between each non-negative feature and class.\n\nchi_ = chi2(X_train[quasi_], y_train)","4c4509da":"# join feature names and p-values in a dataframe\n\nfeat = pd.concat([\n    pd.Series(quasi_),\n    pd.Series(chi_[1]),\n], axis=1,\n)\n\nfeat.columns = ['feature', 'p_value']\n\nfeat.head()","0f3bfe77":"feat['p_value'].hist(bins=30)\nplt.ylabel('Number of features')\nplt.xlabel('p value')","97c02109":"print('Number of total quasi-constant features: ', len(feat))\n\nfeat = feat[feat['p_value']<0.4]\n\nprint('Number of non predictive quasi-constant features: ', len(feat))","db5aa4d4":"# let's drop the features\n\nX_train.drop(labels=feat['feature'], axis=1, inplace=True)\nX_test.drop(labels=feat['feature'], axis=1, inplace=True)\n\nX_train.shape","f5f88219":"# Let's find out how many variables we have with 2, or less than 10 or 20 distinct values\n\nfor max_unique in [2, 10, 20]:\n    vars_ = [x for x in X_train.columns if X_train[x].nunique()<= max_unique]\n    vars_ = len(vars_)\n    print(f'{vars_} variables with less than or equal to {max_unique} values')","334e102f":"# set up the gradient boosting classifier with default parameters\ngbm = GradientBoostingClassifier(random_state=0)\n\n# determine the hyperparameter space\n# we use stats to sample from distributions\n\nparam_grid = dict(\n    n_estimators=stats.randint(10, 200),\n    min_samples_split=stats.uniform(0, 1),\n    max_depth=stats.randint(1, 5),\n    loss=('deviance', 'exponential'),\n    )\n\n# set up the search\nsearch = RandomizedSearchCV(\n    gbm, # the model\n    param_grid, # hyperparam space\n    scoring='roc_auc', # metric to optimize\n    cv=2, # I do 2 to speed things up, 5 would be better as the dataset is quite small\n    n_iter = 5, # I do 5 to speed things up, but for randomized search 60 has been shown to find the optimal hyperparameters\n    random_state=5, # reproducibility\n    refit=True, # this fits the model with the best hyperparams to the entire training set after the hyperparam search\n)\n\n# find best hyperparameters\nsearch.fit(X_train, y_train)","0e6ccbe8":"# the best hyperparameters are stored in an attribute:\n\nsearch.best_params_","635d481d":"# Now let's get the benchmark performance on train and test\n\nX_train_preds = search.predict_proba(X_train)[:,1]\nX_test_preds = search.predict_proba(X_test)[:,1]\n\nprint('Train roc_auc: ', roc_auc_score(y_train, X_train_preds))\nprint('Test roc_auc: ', roc_auc_score(y_test, X_test_preds))","7b5d8c94":"# set up instance hardness threshold\n# the instance hardness is determined based on a gradient boosting machine\n# trained on the entire dataset\n\niht = InstanceHardnessThreshold(\n    estimator=gbm, # we pass the model we set up earlier\n    sampling_strategy='auto',  # undersamples only the majority class\n    random_state=1,\n    cv=2,  # cross validation fold, 2 to speed things up.\n)\n\n# resample\nX_resampled, y_resampled = iht.fit_resample(X_train, y_train)\n\n# shape of original data and data after resampling\nX_train.shape, X_resampled.shape","a61df407":"# check the resampled target\ny_resampled.value_counts(normalize=True)","3db1de0a":"# train model while finding best hyperparameters\n\nsearch.fit(X_resampled, y_resampled)","25f0cb0e":"# the best hyperparameters are stored in an attribute:\n\nsearch.best_params_","1781f336":"# Now let's get the performance on train and test\n\nX_train_preds = search.predict_proba(X_resampled)[:,1]\nX_test_preds = search.predict_proba(X_test)[:,1]\n\nprint('Train roc_auc: ', roc_auc_score(y_resampled, X_train_preds))\nprint('Test roc_auc: ', roc_auc_score(y_test, X_test_preds))","6bcf0b39":"# we need to capture the index of the discrete variables\n\n# make list of discrete variables\ncat_vars = [var for var in X_train.columns if X_train[var].nunique() <= 10]\n\n# capture the index in the dataframe columns\ncat_vars_index = [cat_vars.index(x) for x in cat_vars]\n\ncat_vars_index[0:6]","98a5e0a3":"smnc = SMOTENC(\n    sampling_strategy='auto', # samples only the minority class\n    random_state=0,  # for reproducibility\n    k_neighbors=3,\n    categorical_features=cat_vars_index # indeces of the columns of categorical variables\n)  \n\n# because SMOTE uses KNN, and KNN is sensible to variable magnitude, we re-scale the data\nX_resampled, y_resampled = smnc.fit_resample(MinMaxScaler().fit_transform(X_train), y_train)\n\nX_train.shape, X_resampled.shape","7b6c57cf":"# check the distribution of the resampled target\n\ny_resampled.value_counts(normalize=True)","277e6695":"# train the model while finding best hyperparameters\n\nsearch.fit(X_resampled, y_resampled)","ee020963":"# the best hyperparameters are stored in an attribute:\n\nsearch.best_params_","d17e9bb8":"# Now let's get the performance on train and test\n\nX_train_preds = search.predict_proba(X_resampled)[:,1]\nX_test_preds = search.predict_proba(X_test)[:,1]\n\nprint('Train roc_auc: ', roc_auc_score(y_resampled, X_train_preds))\nprint('Test roc_auc: ', roc_auc_score(y_test, X_test_preds))","494bbd60":"# set up the RUSBoost ensemble model\nrusboost = RUSBoostClassifier(\n        base_estimator=None,\n        n_estimators=20,\n        learning_rate=1.0,\n        sampling_strategy='auto',\n        random_state=2909,\n    )\n\n# set up the hyperparameter space\n# the default implementation as 2 hyperparameters to optimize\n\nparam_grid = dict(\n    n_estimators=stats.randint(10, 200),\n    learning_rate=stats.uniform(0.0001, 1),\n    )\n\n# set up the search\nsearch = RandomizedSearchCV(\n    rusboost, # the model\n    param_grid, # hyperparam space\n    scoring='roc_auc', # metric to optimize\n    cv=2, # I do 2 to speed things up, 5 would be better as the dataset is quite small\n    n_iter = 5, # I do 10 to speed things up, but for randomized search 60 has been shown to find the optimal hyperparameters\n    random_state=10, # reproducibility\n    refit=True, # this fits the model with the best hyperparams to the entire training set after the hyperparam search\n)\n\n# find best hyperparameters\n# using the original data (without resampling)\nsearch.fit(X_train, y_train)","93da1c52":"# the best hyperparameters are stored in an attribute:\n\nsearch.best_params_","059c1540":"# Now let's get the performance on train and test\n\nX_train_preds = search.predict_proba(X_train)[:,1]\nX_test_preds = search.predict_proba(X_test)[:,1]\n\nprint('Train roc_auc: ', roc_auc_score(y_train, X_train_preds))\nprint('Test roc_auc: ', roc_auc_score(y_test, X_test_preds))","e5ca91b1":"easy = EasyEnsembleClassifier(\n        n_estimators=20,\n        sampling_strategy='auto',\n        random_state=2909,\n    )\n\n# set up the hyperparameter space\n# the default implementation as 1 hyperparameters to optimize\n\nparam_grid = dict(\n    n_estimators=stats.randint(10, 200),\n    )\n\n# set up the search\nsearch = RandomizedSearchCV(\n    easy, # the model\n    param_grid, # hyperparam space\n    scoring='roc_auc', # metric to optimize\n    cv=2, # I do 2 to speed things up, 5 would be better as the dataset is quite small\n    n_iter = 5, # I do 10 to speed things up, but for randomized search 60 has been shown to find the optimal hyperparameters\n    random_state=10, # reproducibility\n    refit=True, # this fits the model with the best hyperparams to the entire training set after the hyperparam search\n)\n\n# find best hyperparameters\n# using the original data (without resampling)\nsearch.fit(X_train, y_train)","cd87788b":"# the best hyperparameters are stored in an attribute:\n\nsearch.best_params_","f3f727d6":"# Now let's get the performance on train and test\n\nX_train_preds = search.predict_proba(X_train)[:,1]\nX_test_preds = search.predict_proba(X_test)[:,1]\n\nprint('Train roc_auc: ', roc_auc_score(y_train, X_train_preds))\nprint('Test roc_auc: ', roc_auc_score(y_test, X_test_preds))","83477d26":"# set up the gradient boosting classifier with default parameters\ngbm = GradientBoostingClassifier(random_state=0)\n\n# determine the hyperparameter space\n# we use stats to sample from distributions\n\nparam_grid = dict(\n    n_estimators=stats.randint(10, 200),\n    min_samples_split=stats.uniform(0, 1),\n    max_depth=stats.randint(1, 5),\n    loss=('deviance', 'exponential'),\n    )\n\n# set up the search\nsearch = RandomizedSearchCV(\n    gbm, # the model\n    param_grid, # hyperparam space\n    scoring='roc_auc', # metric to optimize\n    cv=2, # I do 2 to speed things up, 5 would be better as the dataset is quite small\n    n_iter = 5, # I do 10 to speed things up, but for randomized search 60 has been shown to find the optimal hyperparameters\n    random_state=10, # reproducibility\n    refit=True, # this fits the model with the best hyperparams to the entire training set after the hyperparam search\n)\n\n# we have an imbalance of 95 to 5, so we use those as weights\nsample_weight = np.where(y_train==1, 95, 5)\n\n# find best hyperparameters\nsearch.fit(X_train, y_train, sample_weight)","e2d99348":"# the best hyperparameters are stored in an attribute:\n\nsearch.best_params_","86ebe966":"# Now let's get the performance on train and test\n\nX_train_preds = search.predict_proba(X_train)[:,1]\nX_test_preds = search.predict_proba(X_test)[:,1]\n\nprint('Train roc_auc: ', roc_auc_score(y_train, X_train_preds))\nprint('Test roc_auc: ', roc_auc_score(y_test, X_test_preds))","b6a019a6":"## Drop quasi-constant features\n\nLet's first find out the number of quasi-constant features.\n\nQuasi-constant features are those that show the same value for the great majority of the observations.","e3fd1a0c":"Let's go ahead and remove them from the datasets.","19c5abd2":"## Cost-sensitive learning\n\nTo finish with, we will implement cost-sensitive learning. That is, we will modify the penalization cost of the minority class. We can do this directly from the sklearn Gradient Boosting Classifier as follows:","99b96a73":"We see that ~ 4% of the customers are not satisfied.","af72a058":"## Drop constant and duplicated features\n\nThis dataset contains constant and duplicated features. I know this from previous analysis so I will quickly remove these features to reduce the data size.\n\nMore insight about feature selection for this dataset here:\nhttps:\/\/www.kaggle.com\/solegalli\/feature-selection-with-feature-engine","cedd3ebd":"To determine if this features are useful, we will:\n\n* replace its values by 0 for the majority value and 1 for the rare values\n* compare the distribution of the transformed variables in satisfied and unsatisfied customers\n\nTo measure these distributions we use chi-squared.","c5c01719":"# Methods for Imbalanced data\n\n## Under-sampling - Instance Hardness Threshold\n\nAmong the under-sampling methods, we can perform random under-sampling, where we extract samples form the majority class at random. We extract normally as many samples as those we have in the minority. \n\nThen we have cleaning methods, but all of them depend on nearest neighbours, so I would argue that are not suitable given that we have a mix of discrete and continuous variables. \n\nWe can use the InstanceHardness treshold which will remove observations from the majority class that are hard to classify correctly. \n\nInstance hardness is a measure of how difficult an observation is to classify correclty, and it is inversely correlated to the probability of its class.\n\nSo to keep things simple, let's just implement the instance hardness treshold to under-sample our data.","16f66318":"The instance hardness seems to cause over-fitting. So it is not making things better in this dataset.\n\n## Over-Sampling - SMOTENC\n\nAmong the over-sampling methods we have, random over-sampling, which bootstraps observations from the minority class to increase their number. This technique in essence duplicates data, so sometimes leads to over-fitting. \n\nInstead, we can create new data based on SMOTE or its variants. There is 1 variant that is suitable for datasets with continuous and discrete variables, which is SMOTE-NC, so we will implement that method.","e5ec4a2d":"Now, we reduced the dataset from 369 features to 231. Let's hope that that helps speed things up!\n\n## Variable exploration\n\nFrom previous analysis we know that this data set does not contain missing values and that all variables are numerical.\n\nWe also know from previous analysis that most variables in this dataset are binary and discrete, with very few continuous variables. In fact, a few variables that had more than 2 values, are now binary after we applied the label binarizer.","f7e4aa62":"## Target\n\nThe target class is imbalanced. The value 1 refers to un-satisfied customers and 0 to satisfied. So most of Santander's customers are satisfied.","f5b3c7fe":"We see that we have 106 binary variables, and a few more that are also discrete.\n\nWhy is this important?\n\n* Some under- and over- sampling methods for imbalanced datasets are based of Nearest neighbours\n* Nearest neighbours depend on distance metrics\n* in theory, distance metrics for continuous variables are not appropriate for discrete variables and vice-versa.\n\nSo, this will guide how I select which under- and over-sampling methods I can apply on my data, as I will discuss later.\n\nFor now, let's train a gradient boosting machine with these variables to determine the benchmark performance.\n\n## Train Gradient Boosting Model\n\nWe know that for classification Gradient Boosting Machines out-perform all other models, so we will implement directly that model.\n\nWe will do so with cross validation and hyperparameter search.\n\nWe know that Random search of hyperparameters out-performs Grid Search, so we will implement that straightaway.\n\n### Hyperparameter optimization\n\nFor hyperparameter optimization we need to define:\n\n- the machine learning model to train\n- the hyperparameter space (the hyperparameter distributions to sample from)\n- the search algorithm\n- the metric to optimize\n\nLet's do that...","dea19c89":"From all the methods tested the GBM trained on the entire dataset, or the last one trained with cost-sensitive learning seem to return the best performing models,","98b86edd":"SMOTENC does not seem to improve model performance either. On the contrary.\n\n## Ensemble methods for imbalanced data\n\nWe will implement RUSBOOSt and BalancedCascade.","fc709e13":"We have a few features that seem to be differently distributed between satisfied and unsatisfied customers. Let's keep those and remove the rest.\n\nI will keep the features with a p_value bigger than 0.4, to try and reduce the data size as much as possible. And remember that this is so I can keep data small not to run out of memory in this notebook.","f96133a3":"## Load Data","102d9371":"# Predicting Customer Satisfaction with Imbalanced Data and Hyperparameter Optimization\n\nIn this notebook, I'll show how to approach developing a model to predict customer satisfaction using an imabalanced datset.\n\nI discuss why I made some decisions on which model to train, on how to optimize hyperparameters and on which techniques to use to improve the performance of the model trained with an imbalanced data set.\n\nNow, to be able to run the code on Kaggle, I need to reduce the size of the dataset quite a bit. Otherwise, the kernel runs out of memory and breaks. So I will do that here, to be able to show methods for hyperparameter optimization and techniques used on imbalanced datasets. But if I was working on a computer with more computational resources, I would probably not reduce the number of features before hand. In fact, I would evaluate how predictve the features are at the back of the techniques applied to work with imbalaced data. And this is because some methods will alter the distribution of the data, thus, features that were not originally predictive, could be so after resampling.\n\nIn any case, I will take the opportunity to discuss the feature selection methods that I use and why.\n\n## In this notebook\n\n* Remove non-predictive features\n* Train a classifier to predict customer satisfaction\n* Optimize hyperparameters\n* Improve performance with different techniques for imbalanced data\n\n## Feature selection\n\nTo begin with, I will remove duplicated and constant features. These are in essence redundant or non-predictive. Next, I will find quasi-constant features, and evaluate the distribution of its values across satisfied and un-satisfied customers, to determine if I can remove them.\n\n* Remove constant features\n* Remove duplicated features\n* Remove quasi-constant features\n\nMore details throughout the notebook.\n\n## The Machine Learning Model\n\nThis is a classification problem. I want to predict if a customer is unsatisfied (1 in the target). From the off-the-shelf algorithms we know that **Gradient Boosting Machines** out-perform all other models. So in this notebook I will train a Gradient Boosting Classifier from Scikit-learn.\n\nOther suitable options would be a Gradient Boosting Classifier from the packages **xgb** or **LightGBM**. In fact, if you have computing power, you could try both along side the GBM from sklearn. Or simply pick the one you like the most, to keep things simple.\n\n## Hyperparameter Optimization\n\nThere are a number of methods to select the best hyperparameters. The basic methods include grid seearch and random search. Those are usually suitable for models with few hyperparameters, like the GBM from sklearn. If we had a lot of hyperparameters and a model that is very costly to train, then we would be better of performing bayesian hyperparameter optimization. But for this problem, that might be an over-kill and a random search should be more than enough.\n\nThe number of hyperparameters in Gradient Boosting Machines is not very big, so we should be able to find the best hyperparameters with a Randomized Search. So in this notebook, I will use this procedure. Randomized Search comes baked into sklearn, so there is no need to use alternative Python packages.\n\n* Randomized hyperparameter search with sklearn\n\n## Imbalanced data\n\nThere are a number of techniques that we can use to try and improve the performance of models trained on imbalanced datasets. We can under- or over-sample the dataset. Within the under-sampling techniques we have cleaning techniques that allow us to remove noisy observations instead of observations just at random. Within the over-sampling techniques we have methods to create new, \"synthetic\", data using existing observations as templates. This way, we do not just \"duplicate\" the data as we would do with over-sampling.\n\nWe can also implement cost-sensitive learning, where we modify the optimization function to account for the cost of miss-classification. Miss-classifying an observation from the minority class tends to be more costly in real situations. And finally we have special ensemble algorithms that were designed specifically to work with imbalanced datasets.\n\nSo in summary, we could try:\n\n* undersampling at random or based on cleaning criteria\n* oversampling at random or create synthethic new data\n* introduce cost sensitive learning\n* train a special algorithm for imbalanced datasets\n\nFor more details on feature selection, hyperparameter optimization or working with imbalanced datasets, visit my [online courses](https:\/\/www.trainindata.com\/)"}}