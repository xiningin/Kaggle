{"cell_type":{"6715528f":"code","b5964936":"code","92335757":"code","6e79e5e9":"code","f0627375":"code","099e182b":"code","9a56d34b":"code","367b45eb":"code","9732f687":"code","f321b216":"code","b3fa6d29":"code","92904492":"code","0d2895a0":"code","5f76eb3d":"code","53a4012c":"code","847cfdd1":"code","712142ce":"code","555f66ab":"code","0b1370b6":"code","4e13a680":"code","cc6fda84":"code","a379a7c3":"code","037686c3":"code","ff05dcbb":"code","b7596e7c":"code","a25b158b":"code","4faa1819":"code","f01d9199":"code","f9128f33":"code","9c438c1f":"code","1c66c4fb":"code","20bc9aaa":"code","b38b54e5":"code","16e79855":"code","bafab925":"code","a03118c7":"code","0371906a":"code","a2190b33":"code","c8c49dbf":"code","2bef2d5c":"code","f62ddd10":"code","17b7a1e0":"code","f02ae257":"code","4a0a14a7":"code","bbd3cbf0":"code","0fafaf08":"code","faa02067":"code","4a233024":"code","bb5eef51":"code","1f510156":"code","44a8d5f6":"code","82b34a6a":"code","eabc238c":"code","fcc537a7":"code","032c323f":"code","4073e5cf":"code","f78107ec":"code","ffdbd175":"code","72960869":"code","67062356":"code","4b7f18cf":"code","1a8b4757":"code","18b27761":"code","accb71e6":"code","c4864d6f":"code","aaefd47c":"code","18c4ffdc":"code","550cf425":"code","71e4347f":"code","0cbbc275":"markdown","10840d5c":"markdown","885c9b19":"markdown","0d28dbf2":"markdown","197ad076":"markdown","f030ec96":"markdown","73cb6641":"markdown","f7101a8e":"markdown","6501e1ef":"markdown","1a089253":"markdown","a749e0f1":"markdown","62bf4a6f":"markdown","c1bee9d1":"markdown","1663655e":"markdown","5c01305e":"markdown","85167dce":"markdown","03b0f03e":"markdown","0c117e78":"markdown","ecbaf518":"markdown","48a123d2":"markdown","f1eab817":"markdown","9be6736c":"markdown","579989c1":"markdown","de71383c":"markdown","4f986bcc":"markdown","aa3a91ae":"markdown"},"source":{"6715528f":"# To check data directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n","b5964936":"#import basic libraries\n\nimport numpy as np                 # For Linear Algebra\nimport pandas as pd                # For data manipulation\n\nimport matplotlib.pyplot as plt    # For data visualization\nimport seaborn as sns              # For data visualization\n\n%matplotlib inline\n\nimport warnings                   #ignore warnings\nwarnings.filterwarnings('ignore')","92335757":"# Read train and test dataset\n\ntrain = pd.read_csv(\"..\/input\/titanic\/train.csv\")\ntest  = pd.read_csv(\"..\/input\/titanic\/test.csv\")","6e79e5e9":"#make copy of train and test datset\n\ntrain_df = train.copy()\ntest_df  = test.copy()\n\n#Combine both datasets for running certain operations together\n\ncombine = [train_df, test_df]","f0627375":"#View sample of top 5 records in train dataset\n\ntrain_df.head()","099e182b":"#Display sample of test dataset\n\ntest_df.head()","9a56d34b":"#display the shape of train and test datset\n\ntrain_df.shape,test_df.shape","367b45eb":"# Satistical summary of numerical variable in the train datset.\n\ntrain_df.describe()","9732f687":"#Statistical summary of categorical variables in the train dataset.\n\ntrain_df.describe(include=['object'])","f321b216":"train_df.info()","b3fa6d29":"train_df['Survived'].value_counts().plot.bar()","92904492":"train_df['Pclass'].value_counts().plot.bar()","0d2895a0":"train_df['SibSp'].value_counts().plot.bar()","5f76eb3d":"train_df['Parch'].value_counts().plot.bar()","53a4012c":"train_df['Sex'].value_counts().plot.bar()","847cfdd1":"train_df['Embarked'].value_counts(normalize=True).plot.bar()","712142ce":"sns.distplot(train_df['Fare'],color=\"m\", ) ","555f66ab":"\n\nplt.subplot(121) \nsns.distplot(train_df['Age'],color=\"m\", ) \n\nplt.subplot(122) \ntrain_df['Age'].plot.box(figsize=(16,5)) \n\nplt.show()","0b1370b6":"#draw a bar plot of survival by sex\n\nsns.barplot(x='Sex',y=\"Survived\",data=train_df)\n\n#print percentages of females vs. males that survive\n\nprint(\"Percentage of females who survived:\", train_df[\"Survived\"][train_df[\"Sex\"] == 'female'].value_counts(normalize = True)[1]*100)\n\nprint(\"Percentage of males who survived:\", train_df[\"Survived\"][train_df[\"Sex\"] == 'male'].value_counts(normalize = True)[1]*100)","4e13a680":"#draw a bar plot of survival by Pclass\nsns.barplot(x=\"Pclass\", y=\"Survived\", data=train_df)\n\n#print percentage of people by Pclass that survived\nprint(\"Percentage of Pclass = 1 who survived:\", train_df[\"Survived\"][train_df[\"Pclass\"] == 1].value_counts(normalize = True)[1]*100)\n\nprint(\"Percentage of Pclass = 2 who survived:\", train_df[\"Survived\"][train_df[\"Pclass\"] == 2].value_counts(normalize = True)[1]*100)\n\nprint(\"Percentage of Pclass = 3 who survived:\", train_df[\"Survived\"][train_df[\"Pclass\"] == 3].value_counts(normalize = True)[1]*100)","cc6fda84":"#draw a bar plot for SibSp vs. survival\nsns.barplot(x=\"SibSp\", y=\"Survived\", data=train_df)\n\n# printing individual percent values for all of these.\nprint(\"Percentage of SibSp = 0 who survived:\", train_df[\"Survived\"][train_df[\"SibSp\"] == 0].value_counts(normalize = True)[1]*100)\n\nprint(\"Percentage of SibSp = 1 who survived:\", train_df[\"Survived\"][train_df[\"SibSp\"] == 1].value_counts(normalize = True)[1]*100)\n\nprint(\"Percentage of SibSp = 2 who survived:\", train_df[\"Survived\"][train_df[\"SibSp\"] == 2].value_counts(normalize = True)[1]*100)","a379a7c3":"#draw a bar plot for Parch vs. survival\nsns.barplot(x=\"Parch\", y=\"Survived\", data=train_df)\nplt.show()","037686c3":"#sort the ages into logical categories\n\ntrain_df[\"Age\"] = train_df[\"Age\"].fillna(-0.5)\ntest_df[\"Age\"] = test_df[\"Age\"].fillna(-0.5)\nbins = [-1, 0, 5, 12, 18, 24, 35, 60, np.inf]\nlabels = ['Unknown', 'Baby', 'Child', 'Teenager', 'Student', 'Young Adult', 'Adult', 'Senior']\ntrain_df['AgeGroup'] = pd.cut(train_df[\"Age\"], bins, labels = labels)\ntest_df['AgeGroup'] = pd.cut(test_df[\"Age\"], bins, labels = labels)\n\n#draw a bar plot of Age vs. survival\nsns.barplot(x=\"AgeGroup\", y=\"Survived\", data=train_df)\nplt.show()","ff05dcbb":"train_df.head()","b7596e7c":"pd.isnull(train_df).sum()","a25b158b":"pd.isnull(test_df).sum()","4faa1819":"train_1 = train_df.drop(['Cabin'], axis = 1)\ntest_1 = test_df.drop(['Cabin'], axis = 1)","f01d9199":"train_1.shape,test_1.shape","f9128f33":"train_2 = train_1.drop(['Ticket'], axis = 1)\ntest_2 = test_1.drop(['Ticket'], axis = 1)","9c438c1f":"train_2.shape,test_2.shape","1c66c4fb":"#now we need to fill in the missing values in the Embarked feature\n\nprint(\"Number of people embarking in Southampton (S):\")\nsouthampton = train_2[train_2[\"Embarked\"] == \"S\"].shape[0]\nprint(southampton)\n\nprint(\"Number of people embarking in Cherbourg (C):\")\ncherbourg = train_2[train_2[\"Embarked\"] == \"C\"].shape[0]\nprint(cherbourg)\n\nprint(\"Number of people embarking in Queenstown (Q):\")\nqueenstown = train_2[train_2[\"Embarked\"] == \"Q\"].shape[0]\nprint(queenstown)","20bc9aaa":"#replacing the missing values in the Embarked feature with S\ntrain_2 = train_2.fillna({\"Embarked\": \"S\"})","b38b54e5":"#create a combined group of both datasets\ncombine = [train_2, test_2]\n\n#extract a title for each Name in the train and test datasets\nfor dataset in combine:\n    dataset['Title'] = dataset.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\n\npd.crosstab(train_2['Title'], train_2['Sex'])","16e79855":"#replace various titles with more common names\n\nfor dataset in combine:\n    dataset['Title'] = dataset['Title'].replace(['Lady', 'Capt', 'Col',\n    'Don', 'Dr', 'Major', 'Rev', 'Jonkheer', 'Dona'], 'Rare')\n    \n    dataset['Title'] = dataset['Title'].replace(['Countess', 'Lady', 'Sir'], 'Royal')\n    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\n\ntrain_2[['Title', 'Survived']].groupby(['Title'], as_index=False).mean()\n","bafab925":"#map each of the title groups to a numerical value\n\ntitle_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Royal\": 5, \"Rare\": 6}\nfor dataset in combine:\n    dataset['Title'] = dataset['Title'].map(title_mapping)\n    dataset['Title'] = dataset['Title'].fillna(0)\n\ntrain_2.head()","a03118c7":"# fill missing age with mode age group for each title\nmr_age = train_2[train_2[\"Title\"] == 1][\"AgeGroup\"].mode() #Young Adult\nmiss_age = train_2[train_2[\"Title\"] == 2][\"AgeGroup\"].mode() #Student\nmrs_age = train_2[train_2[\"Title\"] == 3][\"AgeGroup\"].mode() #Adult\nmaster_age = train_2[train_2[\"Title\"] == 4][\"AgeGroup\"].mode() #Baby\nroyal_age = train_2[train_2[\"Title\"] == 5][\"AgeGroup\"].mode() #Adult\nrare_age = train_2[train_2[\"Title\"] == 6][\"AgeGroup\"].mode() #Adult\n\nage_title_mapping = {1: \"Young Adult\", 2: \"Student\", 3: \"Adult\", 4: \"Baby\", 5: \"Adult\", 6: \"Adult\"}\n\n#I tried to get this code to work with using .map(), but couldn't.\n#I've put down a less elegant, temporary solution for now.\n#train = train.fillna({\"Age\": train[\"Title\"].map(age_title_mapping)})\n#test = test.fillna({\"Age\": test[\"Title\"].map(age_title_mapping)})\n\nfor x in range(len(train_2[\"AgeGroup\"])):\n    if train_2[\"AgeGroup\"][x] == \"Unknown\":\n        train_2[\"AgeGroup\"][x] = age_title_mapping[train_2[\"Title\"][x]]\n        \nfor x in range(len(test_2[\"AgeGroup\"])):\n    if test_2[\"AgeGroup\"][x] == \"Unknown\":\n        test_2[\"AgeGroup\"][x] = age_title_mapping[test_2[\"Title\"][x]]","0371906a":"#map each Age value to a numerical value\n\nage_mapping = {'Baby': 1, 'Child': 2, 'Teenager': 3, 'Student': 4, 'Young Adult': 5, 'Adult': 6, 'Senior': 7}\ntrain_2['AgeGroup'] = train_2['AgeGroup'].map(age_mapping)\ntest_2['AgeGroup'] = test_2['AgeGroup'].map(age_mapping)\n\ntrain_2.head()\n\n#dropping the Age feature for now, might change\ntrain_3 = train_2.drop(['Age'], axis = 1)\ntest_3 = test_2.drop(['Age'], axis = 1)","a2190b33":"train_3.head()","c8c49dbf":"test_3.head()","2bef2d5c":"#drop the name feature since it contains no more useful information.\ntrain_4 = train_3.drop(['Name'], axis = 1)\ntest_4 = test_3.drop(['Name'], axis = 1)","f62ddd10":"#map each Sex value to a numerical value\nsex_mapping = {\"male\": 0, \"female\": 1}\ntrain_4['Sex'] = train_4['Sex'].map(sex_mapping)\ntest_4['Sex'] = test_4['Sex'].map(sex_mapping)\n\ntrain_4.head()","17b7a1e0":"#map each Embarked value to a numerical value\nembarked_mapping = {\"S\": 1, \"C\": 2, \"Q\": 3}\ntrain_4['Embarked'] = train_4['Embarked'].map(embarked_mapping)\ntest_4['Embarked'] = test_4['Embarked'].map(embarked_mapping)\n\ntrain_4.head()","f02ae257":"#fill in missing Fare value in test set based on mean fare for that Pclass \nfor x in range(len(test_4[\"Fare\"])):\n    if pd.isnull(test_4[\"Fare\"][x]):\n        pclass = test_4[\"Pclass\"][x] #Pclass = 3\n        test_4[\"Fare\"][x] = round(train_4[train_4[\"Pclass\"] == pclass][\"Fare\"].mean(), 4)\n        \n#map Fare values into groups of numerical values\ntrain_4['FareBand'] = pd.qcut(train_4['Fare'], 4, labels = [1, 2, 3, 4])\ntest_4['FareBand'] = pd.qcut(test_4['Fare'], 4, labels = [1, 2, 3, 4])\n\n#drop Fare values\ntrain_5 = train_4.drop(['Fare'], axis = 1)\ntest_5 = test_4.drop(['Fare'], axis = 1)\n","4a0a14a7":"train_5.head()","bbd3cbf0":"test_5.head()","0fafaf08":"from sklearn.model_selection import train_test_split\n\npredictors = train_5.drop(['Survived', 'PassengerId','AgeGroup'], axis=1)\ntarget = train_5[\"Survived\"]\nx_train, x_val, y_train, y_val = train_test_split(predictors, target, test_size = 0.22, random_state = 0)","faa02067":"predictors.shape,target.shape","4a233024":"x_train.shape, x_val.shape, y_train.shape, y_val.shape","bb5eef51":"# Gaussian Naive Bayes\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import accuracy_score\n\ngaussian = GaussianNB()\ngaussian.fit(x_train, y_train)\ny_pred = gaussian.predict(x_val)\nacc_gaussian = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_gaussian)","1f510156":"# Logistic Regression\nfrom sklearn.linear_model import LogisticRegression\n\nlogreg = LogisticRegression()\nlogreg.fit(x_train, y_train)\ny_pred = logreg.predict(x_val)\nacc_logreg = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_logreg)","44a8d5f6":"# Support Vector Machines\nfrom sklearn.svm import SVC\n\nsvc = SVC()\nsvc.fit(x_train, y_train)\ny_pred = svc.predict(x_val)\nacc_svc = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_svc)","82b34a6a":"# Linear SVC\nfrom sklearn.svm import LinearSVC\n\nlinear_svc = LinearSVC()\nlinear_svc.fit(x_train, y_train)\ny_pred = linear_svc.predict(x_val)\nacc_linear_svc = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_linear_svc)","eabc238c":"# Perceptron\nfrom sklearn.linear_model import Perceptron\n\nperceptron = Perceptron()\nperceptron.fit(x_train, y_train)\ny_pred = perceptron.predict(x_val)\nacc_perceptron = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_perceptron)","fcc537a7":"#Decision Tree\nfrom sklearn.tree import DecisionTreeClassifier\n\ndecisiontree = DecisionTreeClassifier()\ndecisiontree.fit(x_train, y_train)\ny_pred = decisiontree.predict(x_val)\nacc_decisiontree = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_decisiontree)","032c323f":"# Random Forest\n\nfrom sklearn.ensemble import RandomForestClassifier\n\nrandomforest = RandomForestClassifier()\nrandomforest.fit(x_train, y_train)\ny_pred = randomforest.predict(x_val)\nacc_randomforest = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_randomforest)","4073e5cf":"# KNN or k-Nearest Neighbors\nfrom sklearn.neighbors import KNeighborsClassifier\n\nknn = KNeighborsClassifier()\nknn.fit(x_train, y_train)\ny_pred = knn.predict(x_val)\nacc_knn = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_knn)","f78107ec":"# Stochastic Gradient Descent\nfrom sklearn.linear_model import SGDClassifier\n\nsgd = SGDClassifier()\nsgd.fit(x_train, y_train)\ny_pred = sgd.predict(x_val)\nacc_sgd = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_sgd)","ffdbd175":"# Gradient Boosting Classifier\nfrom sklearn.ensemble import GradientBoostingClassifier\n\ngbk = GradientBoostingClassifier()\ngbk.fit(x_train, y_train)\ny_pred = gbk.predict(x_val)\nacc_gbk = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_gbk)","72960869":"models = pd.DataFrame({\n    'Model': ['Support Vector Machines', 'KNN', 'Logistic Regression', \n              'Random Forest', 'Naive Bayes', 'Perceptron', 'Linear SVC', \n              'Decision Tree', 'Stochastic Gradient Descent', 'Gradient Boosting Classifier'],\n    'Score': [acc_svc, acc_knn, acc_logreg, \n              acc_randomforest, acc_gaussian, acc_perceptron,acc_linear_svc, acc_decisiontree,\n              acc_sgd, acc_gbk]})\nmodels.sort_values(by='Score', ascending=False)","67062356":"#set ids as PassengerId and predict survival \nids = test_5['PassengerId']\npredictions = gbk.predict(test_5.drop(['PassengerId','AgeGroup'], axis=1))\n\n#set the output as a dataframe and convert to csv file named submission.csv\noutput = pd.DataFrame({ 'PassengerId' : ids, 'Survived': predictions })\noutput.to_csv('submission.csv', index=False)","4b7f18cf":"from sklearn.model_selection import StratifiedKFold \nfrom sklearn.model_selection import cross_val_score\n\n\nskfold = StratifiedKFold (n_splits=10,shuffle=True, random_state= 1)\n\nLog_skf = LogisticRegression()\n\nLog_skf1 = cross_val_score(Log_skf, x_train, y_train, cv=skfold)\n\nprint(Log_skf1)\n\nacc_log2 = Log_skf1.mean()*100.0\n\nacc_log2 ","1a8b4757":"skfold = StratifiedKFold (n_splits=5,shuffle=True, random_state= 1)\n\nsvc_sk = SVC(gamma='auto')\n\nsvc_sk1 = cross_val_score(svc_sk, x_train, y_train, cv=skfold)\n\nprint(svc_sk1)\n\nacc_svc2 = svc_sk1.mean()*100.0\n\nacc_svc2 ","18b27761":"skfold = StratifiedKFold (n_splits=10,shuffle=True, random_state= 1)\n\nknn_sk = KNeighborsClassifier(n_neighbors = 3)\n\nknn_sk1 = cross_val_score(knn_sk, x_train, y_train, cv=skfold)\n\nprint(knn_sk1)\n\nacc_knn2 = knn_sk1.mean()*100.0\n\nacc_knn2","accb71e6":"skfold = StratifiedKFold (n_splits=10,shuffle=False, random_state= 1)\n\nrfc_sk = RandomForestClassifier(n_estimators=100,random_state = 1)\n\nrfc_sk1 = cross_val_score(rfc_sk, x_train, y_train, cv=skfold)\n\nprint(rfc_sk1)\n\nacc_rfc2 = rfc_sk1.mean()*100.0\n\nacc_rfc2","c4864d6f":"skfold = StratifiedKFold (n_splits=5,shuffle=False, random_state= 1)\n\ngnb_sk = GaussianNB()\n\ngnb_sk1 = cross_val_score(gnb_sk,x_train, y_train, cv=skfold)\n\nprint(gnb_sk1)\n\nacc_gnb2 = gnb_sk1.mean()*100.0\n\nacc_gnb2","aaefd47c":"skfold = StratifiedKFold (n_splits=5,shuffle=True, random_state= 1)\n\nptn_sk = Perceptron()\n\nptn_sk1 = cross_val_score(ptn_sk, x_train, y_train, cv=skfold)\n\nprint(ptn_sk1)\n\nacc_ptn2 = ptn_sk1.mean()*100.0\n\nacc_ptn2","18c4ffdc":"skfold = StratifiedKFold (n_splits=5,shuffle=True, random_state= None)\n\ndt_sk = DecisionTreeClassifier(random_state=1)\n\ndt_sk1 = cross_val_score(dt_sk, x_train, y_train, cv=skfold)\n\nprint(dt_sk1)\n\nacc_dt2 = dt_sk1.mean()*100.0\n\nacc_dt2","550cf425":"import lightgbm as lgb\n\nskfold = StratifiedKFold (n_splits=10,shuffle=True, random_state= None)\n\nlgb_sk = lgb.LGBMClassifier()\n\nlgb_sk1 = cross_val_score(lgb_sk, x_train, y_train, cv=skfold)\n\nprint(lgb_sk1)\n\nacc_lgb2 = lgb_sk1.mean()*100.0\n\nacc_lgb2","71e4347f":"models = pd.DataFrame({\n    'Model': ['Support Vector Classifier', 'KNN', 'Logistic Regression', \n              'Random Forest', 'Naive Bayes', 'Perceptron',  \n              'Decision Tree','LGBMClassifier'],\n    'Mean Accuracy': [acc_svc2, acc_knn2, acc_log2, \n              acc_rfc2, acc_gnb2, acc_ptn2, \n             acc_dt2,acc_lgb2]})\nmodels.sort_values(by='Mean Accuracy', ascending=False)","0cbbc275":"* The shortest bar indicates that less number of female passengers onboarded in RMS Titanic.","10840d5c":"* After using cross validation our model SVC predicts 83% mean accuracy.This is my first attempt and I believe that still there are ways to improve the model prediction using feature engineering.\n\n* Any suggestions to improve our score are most welcome.","885c9b19":"* Passengers with parents\/children has highest survival rate in number 3 as compared to number 1 and 2. ","0d28dbf2":"# 1.Problem Statement\n\n* Titanic has became one of the most famous ships in the history.\n* One of the unexcepted event is the wreck of Titanic On April 15,1912 after striking an iceberg during her maiden voyage from Southampton to New York City.\n* Unfortunately, there weren\u2019t enough lifeboats for everyone onboard, resulting in the death of 1502 out of 2224 passengers and crew.\n* While there was some element of luck involved in surviving, it seems some groups of people were more likely to survive than others.\n* The challenge is to find the survival status of passengers using passenger data (ie name, age, gender, socio-economic class, etc).  \n\n","197ad076":"# 4.Basic Statistics","f030ec96":"* Passengers travelled in the first class accommodation has highest percentage of survival followed by 2nd and 3rd class passengers. ","73cb6641":"* The Age group seems like Baby category has higher survival rate.\n* The survival rate is same for the category Young Adult,Adult. \n* The second highest survival rate is teenager as compared to child and student.","f7101a8e":"* The bar chart explains less number of passengers survived in the shipwrecks.","6501e1ef":"Observations:\n\n1.Train dataset contains 891 rows and 12 columns.\n\n2.Test dataset contains 418 rows and 11 columns.\n\n3.The objective is to train the independent variable and build statistical model to predict the the target variable.\n\n4.In train dataset the target variable is considered (\"Survived\") which contains categorical variable 0  and 1 .","1a089253":"* The order of survival is higher for passengers with one,two and no siblings\/spouses.\n* Remaining passengers have less survival rate.","a749e0f1":"* The variable Sibsp indicates number of passengers on boarded with their siblings and spouses.\n* The variable Parch recorded number of passengers on boarded with their parents and children.\n* Both variable are following the similar shape of distributions.\n* Larger number of people on boarded as single passengers. ","62bf4a6f":"* The variable Pcclass illustrates largest number of passengers booked 3rd class ticket.\n\n* The second largest number of passengers booked 1st class ticket and remaining passengers booked by 2nd class ticket.\n\n* In addition to that if more number of passengers travelled in 3 rd class are lesser chance for survival.","c1bee9d1":"# 9.Cross Validation","1663655e":"* It is intersting to see that around 74% of female passengers survived.\n* This information conveys a larger number of male passengers were not evacuated because the protocol follws \"children and women\" loaded first on the lifeboats.","5c01305e":"* Around 70% of passengers embarked from S(Southampton) port.\n* Around 20% of passengers embarked from C(Cherbourg) port and remaining passengers embarked from C (Cherbourg) port.","85167dce":"Reference:\n\nThis notebook has been created based on great work done solving the Titanic competition and other sources.\n\n1. Titanic Data Science Solutions","03b0f03e":"# 6.Bivariate Analysis","0c117e78":"* The Age variable follows nomal distribution and the maximum age of passenger was 80 years old.\n","ecbaf518":"**Data types:**\n\n1. Categorical Features\n\n       *  Categorical  : Name,Survived,Sex and Embarked \n       *  Ordinal      : Pclass\n       \n1. Numerical Features \n\n       *  Continous Variable : Age and Fare\n       *  Discrete Variable  : SibSp and Parch \n       \n1. Alpha Numeric  : Ticket and Cabin\n\n       ","48a123d2":"# 10.Model Evaluation","f1eab817":"#  3.Data Preparation","9be6736c":"# 8.Model Building","579989c1":"# 5.Univariate Analysis","de71383c":"# 2.Overview of Dataset\n\nThere are two sets of data available in the data directory\n\n 1.training set (train.csv) use to build machine learning model.\n \n 2.test set (test.csv) use to prdict the built model.\n \n","4f986bcc":"* The distribution of Fare variable is right skewed which means more number of passengers chosen 3 rd class where the ticket price is comparetevely less as compared to 2nd and 1 st class.","aa3a91ae":"# 7.Missing Values Treatment"}}