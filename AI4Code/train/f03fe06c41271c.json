{"cell_type":{"eb7bf7ea":"code","cf56ae1a":"code","3181ac76":"code","2632fca6":"code","08e3f274":"code","86f90fe0":"code","1cd7a4f0":"code","94eafae8":"code","b16e11cc":"code","0d9a6100":"code","cc1117eb":"code","f2ecb2b2":"code","81a02137":"code","96eca4e8":"code","74e5964f":"code","7c8c0235":"code","1d978823":"code","c21b8d30":"code","e5c41c61":"code","7649ca33":"code","a38c9ea9":"code","90c00939":"code","ea32fe1a":"code","84a5ffdf":"code","2b42d2a8":"code","3dc9a831":"code","e89659b5":"code","190bcbc8":"code","741e3528":"code","f00dcaeb":"code","37cc539c":"code","447e7b21":"code","c0a1456c":"code","1bcdbf5d":"code","abd6802a":"code","6f9d84f0":"markdown","380358fe":"markdown","65bb99f1":"markdown","ccbbeee7":"markdown","f3bea651":"markdown","70190767":"markdown","d39afd70":"markdown","ebc890a0":"markdown","21ffbab0":"markdown","070dc355":"markdown","7c0a05be":"markdown","018f6b94":"markdown"},"source":{"eb7bf7ea":"import numpy as np\nimport pandas as pd\nimport os\nimport glob\npd.options.mode.chained_assignment = None","cf56ae1a":"list_of_files = os.listdir('..\/input\/goodreads-book-datasets-10m')\nall_data =[]\nfor each_file in list_of_files:\n    if each_file.startswith('book'):  \n        print (each_file)\n        df = pd.read_csv('..\/input\/goodreads-book-datasets-10m\/'+ each_file, usecols = ['Name', 'Rating', 'PublishYear', 'Authors'])\n        all_data.append(df)\n        \ndata = pd.concat(all_data, axis = 0)        ","3181ac76":"data.head()","2632fca6":"data.isna().any()","08e3f274":"data.sort_values('PublishYear', ascending=False).head()","86f90fe0":"data2 = pd.read_csv('..\/input\/goodreads-book-datasets-10m\/user_rating_0_to_1000.csv', usecols=['Name',\n                                                                                              'Rating'])","1cd7a4f0":"data2.head()","94eafae8":"len(data2)","b16e11cc":"data2.isna().any()","0d9a6100":"data_merge = pd.merge(data, data2, on = 'Name', how = 'right')","cc1117eb":"data_merge.head()","f2ecb2b2":"data_merge.isna().any()","81a02137":"data_merge.dropna(inplace = True)","96eca4e8":"data_merge.isna().any()","74e5964f":"data_merge.head()","7c8c0235":"data_merge.drop_duplicates(subset= ['Name'],inplace = True)","1d978823":"data_merge.head()","c21b8d30":"data_merge.duplicated().any()","e5c41c61":"data_merge.PublishYear.max()","7649ca33":"data_merge.PublishYear.min()","a38c9ea9":"new_data = data_merge[data_merge['PublishYear'] >= 1990]","90c00939":"new_data.head()","ea32fe1a":"new_data.Rating_y.unique()","84a5ffdf":"new_data['Rating_new'] = np.where((new_data['Rating_y'] == 'it was amazing') | (new_data['Rating_y'] == 'really liked it'),\n                                   float(4.5), np.where(new_data['Rating_y'] == 'liked it', float(3.8), \n                                                       np.where(new_data['Rating_y'] == 'it was ok', float(3.5), float(2.0))))","2b42d2a8":"new_data.head()","3dc9a831":"new_data.info()","e89659b5":"new_data['Rating_mean'] = ((new_data['Rating_x'] + new_data['Rating_new'])\/2).round(2).astype(float)","190bcbc8":"new_data.head()","741e3528":"new_data.Rating_mean.max()","f00dcaeb":"new_data.drop(['Rating_x', 'Rating_y', 'Rating_new'], axis = 1, inplace = True)","37cc539c":"new_data = new_data.sort_values(by = ['PublishYear', 'Rating_mean'], ascending = [False, False])","447e7b21":"new_data.head()","c0a1456c":"l = []\nfor year in range(1990, 2021):\n    new_data1 = new_data[new_data.PublishYear == year].iloc[0]\n    #print(new_data1)\n    l.append(new_data1)\n    new_data2 = pd.DataFrame(l, columns = ['Name', 'PublishYear', 'Authors', 'Rating_mean'])","1bcdbf5d":"new_data2 = new_data2.reset_index()\nnew_data2.drop('index', axis = 1, inplace = True)\nnew_data2.PublishYear = new_data2.PublishYear.astype(int)","abd6802a":"new_data2","6f9d84f0":"Calculating the mean by considering both the existing rating and my new rating","380358fe":"Now I'm gonna limit the range of data from Year 1990 to 2020","65bb99f1":"**Below are the best books of every year as per my analysis**","ccbbeee7":"Reading the '**User_rating**' file which contains the texual reviews\/ratings of the books","f3bea651":"Below we can see that there are null values present in the dataset, which we will get rid of.","70190767":"Getting rid of duplicates","d39afd70":"Checking for any null values in the dataset","ebc890a0":"Checking out the various textual reviews in the dataset and based on that, creating my own scoring below","21ffbab0":"# Synopsis:\nThis dataset consists of 2 set of files, one which consists of numerical ratings and the other which consists of reviews\/textual rating. I am gonna amalgamate the two and will create a new set of ratings which will be used to rate the best book of every year.","070dc355":"Merging both the datasets together ","7c0a05be":"Sorted the dataset in descending order of PublishYear and Rating_mean and then applied a loop to get the first value(highest rating) of every year and create a dataframe from it.","018f6b94":"Reading the csv files which start with '**book**' and concatenating them together"}}