{"cell_type":{"cfd5823a":"code","46ea46e1":"code","195a93f6":"code","94896992":"code","12798fef":"code","cad53d56":"code","21592b73":"code","d4aed0ca":"code","d49606a1":"code","2fe00ba6":"code","0578c9cc":"code","0f4ccefd":"code","b176018c":"code","125f4cb8":"code","cedfe420":"code","42a2b00f":"code","ab24e3c9":"code","c2bac654":"code","16d1b796":"code","8e000830":"code","65503262":"code","e5659e12":"code","a23a81a5":"code","c63d1825":"code","f9a6026d":"code","459b8ead":"code","a0683cab":"code","a1aeb482":"code","4eb39293":"code","c885022f":"code","57d30ac8":"code","d841c4d5":"code","db9cfdb3":"code","dc4b1fed":"code","5bddc351":"code","a6fcc1b1":"code","072de6ef":"code","5e0b96c5":"code","9e397916":"code","7a884e64":"code","38b0e1e7":"code","60875387":"code","95781972":"code","52e436f3":"code","a7ef5fed":"code","5df9ac32":"code","53ae1723":"code","d013836f":"code","73b16e53":"code","09b096fc":"code","ce553105":"code","0dcf1456":"code","38dfe564":"code","a4da37c7":"code","769c7c8e":"code","3793e8b0":"code","e6196764":"code","47d7a630":"code","13ca1382":"code","93a3f2a5":"code","fdfe77f0":"code","167d76ee":"code","a8a89bf0":"code","18aef3d8":"code","1a32e03b":"code","3fa1e952":"code","3fa1d5fe":"code","08e8f62c":"code","76980497":"code","a6bb1b51":"code","ee8fa3ef":"code","23b19498":"code","a987c745":"code","62847e64":"code","93c156c0":"code","9f147adf":"code","17217993":"code","7afe9d4a":"code","41f19b9a":"code","7d3d4fe5":"code","96f59845":"code","02d59a6a":"code","32aeb2f7":"code","955ea477":"code","249777a0":"code","8a481755":"code","d8d8939b":"code","2bd87867":"code","7e31a6ea":"code","dbda396f":"markdown","331281b6":"markdown","a940ea64":"markdown","b4ec0e61":"markdown","e403f654":"markdown","12949e29":"markdown","5a869444":"markdown","82519ade":"markdown","a6879c9f":"markdown","6e8e5d3f":"markdown","3cb08e3c":"markdown","fd173c9a":"markdown","ef581fb8":"markdown","bce9ebf4":"markdown","ddcdb77a":"markdown","238f5986":"markdown","8f93ae95":"markdown","bd03a693":"markdown","6b490530":"markdown","2e142e7b":"markdown","708a42b9":"markdown","c0aff6ad":"markdown","644f2052":"markdown","e029ea99":"markdown","b0d21749":"markdown","0f09b31e":"markdown","26e6e775":"markdown","eb779a2d":"markdown","0fe4a1e0":"markdown","65cb07b8":"markdown","4b9c5b19":"markdown","4ee36da6":"markdown","b90ff6bd":"markdown","f98536b9":"markdown","bd1b8f0e":"markdown","9b8078d7":"markdown","96bc9aae":"markdown","6db342cf":"markdown","12bcbdcf":"markdown","c0b18044":"markdown","3bb64a7f":"markdown","01a2b0ef":"markdown","2a8ce155":"markdown","08d1536f":"markdown","d92d10b6":"markdown","f1b76ff7":"markdown","1057f870":"markdown","f052f0b5":"markdown","1ff81cea":"markdown","ee2ee39c":"markdown","01e5fd7f":"markdown","83f0a73c":"markdown","ce66fad8":"markdown","735b50a5":"markdown","bead7ec2":"markdown","5b73944e":"markdown","8b02b36a":"markdown","93ff9df8":"markdown","6d5d0813":"markdown","f64eb9d4":"markdown","fa056c90":"markdown","86037a26":"markdown","d4a89487":"markdown","a47ffb5f":"markdown","8a0e0c43":"markdown","4a849882":"markdown","db673fe4":"markdown","c624003e":"markdown","c828baa1":"markdown","816809ad":"markdown","16e88e4d":"markdown","caca8cb9":"markdown","49072a87":"markdown","85fb7fa5":"markdown","a97d5449":"markdown","26cc761c":"markdown","b04bb971":"markdown","e4163d7d":"markdown","232e730b":"markdown","50148c9d":"markdown","5f00e227":"markdown","e186f52d":"markdown","fbb05bf8":"markdown","cda20ffa":"markdown","8ad6293f":"markdown","9bd69f45":"markdown","8ab9f099":"markdown","14ca42c4":"markdown","a73fea09":"markdown","a60916c7":"markdown","33d47746":"markdown"},"source":{"cfd5823a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt # for data visualization purposes\nimport seaborn as sns # for statistical data visualization\n%matplotlib inline\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","46ea46e1":"import warnings\n\nwarnings.filterwarnings('ignore')","195a93f6":"data = '\/kaggle\/input\/adult-dataset\/adult.csv'\n\ndf = pd.read_csv(data, header=None, sep=',\\s')","94896992":"df","12798fef":"# view dimensions of dataset\n\ndf.shape","cad53d56":"# preview the dataset\n\ndf.head()","21592b73":"col_names = ['age', 'workclass', 'fnlwgt', 'education', 'education_num', 'marital_status', 'occupation', 'relationship',\n             'race', 'sex', 'capital_gain', 'capital_loss', 'hours_per_week', 'native_country', 'income']\n\ndf.columns = col_names\n\ndf.columns","d4aed0ca":"# let's again preview the dataset\n\ndf.head()","d49606a1":"# view summary of dataset\n\ndf.info()","2fe00ba6":"# find categorical variables\n\ncategorical = [var for var in df.columns if df[var].dtype=='O']\n\nprint('There are {} categorical variables\\n'.format(len(categorical)))\n\nprint('The categorical variables are :\\n\\n', categorical)","0578c9cc":"# view the categorical variables\n\ndf[categorical].head()","0f4ccefd":"# check missing values in categorical variables\n\ndf[categorical].isnull().sum()","b176018c":"# view frequency counts of values in categorical variables\n\nfor var in categorical: \n    \n    print(df[var].value_counts())","125f4cb8":"# view frequency distribution of categorical variables\n\nfor var in categorical: \n    \n    print(df[var].value_counts()\/np.float(len(df)))","cedfe420":"# check labels in workclass variable\n\ndf.workclass.unique()","42a2b00f":"# check frequency distribution of values in workclass variable\n\ndf.workclass.value_counts()","ab24e3c9":"# replace '?' values in workclass variable with `NaN`\n\n\ndf['workclass'].replace('?', np.NaN, inplace=True)","c2bac654":"# again check the frequency distribution of values in workclass variable\n\ndf.workclass.value_counts()","16d1b796":"# check labels in occupation variable\n\ndf.occupation.unique()\n","8e000830":"# check frequency distribution of values in occupation variable\n\ndf.occupation.value_counts()","65503262":"# replace '?' values in occupation variable with `NaN`\n\ndf['occupation'].replace('?', np.NaN, inplace=True)\n","e5659e12":"# again check the frequency distribution of values in occupation variable\n\ndf.occupation.value_counts()","a23a81a5":"# check labels in native_country variable\n\ndf.native_country.unique()\n","c63d1825":"# check frequency distribution of values in native_country variable\n\ndf.native_country.value_counts()\n","f9a6026d":"# replace '?' values in native_country variable with `NaN`\n\ndf['native_country'].replace('?', np.NaN, inplace=True)","459b8ead":"# again check the frequency distribution of values in native_country variable\n\ndf.native_country.value_counts()","a0683cab":"df[categorical].isnull().sum()","a1aeb482":"# check for cardinality in categorical variables\n\nfor var in categorical:\n    \n    print(var, ' contains ', len(df[var].unique()), ' labels')","4eb39293":"# find numerical variables\n\nnumerical = [var for var in df.columns if df[var].dtype!='O']\n\nprint('There are {} numerical variables\\n'.format(len(numerical)))\n\nprint('The numerical variables are :', numerical)","c885022f":"# view the numerical variables\n\ndf[numerical].head()","57d30ac8":"# check missing values in numerical variables\n\ndf[numerical].isnull().sum()","d841c4d5":"X = df.drop(['income'], axis=1)\n\ny = df['income']","db9cfdb3":"y","dc4b1fed":"# split X and y into training and testing sets\n\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)\n","5bddc351":"# check the shape of X_train and X_test\n\nX_train.shape, X_test.shape","a6fcc1b1":"y_train","072de6ef":"# check data types in X_train\n\nX_train.dtypes","5e0b96c5":"# display categorical variables\n\ncategorical = [col for col in X_train.columns if X_train[col].dtypes == 'O']\n\ncategorical","9e397916":"# display numerical variables\n\nnumerical = [col for col in X_train.columns if X_train[col].dtypes != 'O']\n\nnumerical","7a884e64":"# print percentage of missing values in the categorical variables in training set\n\nX_train[categorical].isnull().mean()","38b0e1e7":"# print categorical variables with missing data\n\nfor col in categorical:\n    if X_train[col].isnull().mean()>0:\n        print(col, (X_train[col].isnull().mean()))","60875387":"# impute missing categorical variables with most frequent value\n\nfor df2 in [X_train, X_test]:\n    df2['workclass'].fillna(X_train['workclass'].mode()[0], inplace=True)\n    df2['occupation'].fillna(X_train['occupation'].mode()[0], inplace=True)\n    df2['native_country'].fillna(X_train['native_country'].mode()[0], inplace=True)    ","95781972":"# check missing values in categorical variables in X_train\n\nX_train[categorical].isnull().sum()","52e436f3":"# check missing values in categorical variables in X_test\n\nX_test[categorical].isnull().sum()","a7ef5fed":"# check missing values in X_train\n\nX_train.isnull().sum()","5df9ac32":"# check missing values in X_test\n\nX_test.isnull().sum()","53ae1723":"# print categorical variables\n\ncategorical","d013836f":"X_train[categorical].head()","73b16e53":"# import category encoders\n\nimport category_encoders as ce","09b096fc":"# encode remaining variables with one-hot encoding\n\nencoder = ce.OneHotEncoder(cols=['workclass', 'education', 'marital_status', 'occupation', 'relationship', \n                                 'race', 'sex', 'native_country'])\n\nX_train = encoder.fit_transform(X_train)\n\nX_test = encoder.transform(X_test)","ce553105":"X_train.head()","0dcf1456":"X_train.shape","38dfe564":"X_test.head()","a4da37c7":"X_test.shape","769c7c8e":"cols = X_train.columns","3793e8b0":"from sklearn.preprocessing import RobustScaler\n\nscaler = RobustScaler()\n\nX_train = scaler.fit_transform(X_train)\n\nX_test = scaler.transform(X_test)\n","e6196764":"X_train = pd.DataFrame(X_train, columns=[cols])","47d7a630":"X_test = pd.DataFrame(X_test, columns=[cols])","13ca1382":"X_train.head()","93a3f2a5":"y_train","fdfe77f0":"# train a Gaussian Naive Bayes classifier on the training set\nfrom sklearn.naive_bayes import GaussianNB\n\n\n# instantiate the model\ngnb = GaussianNB()\n\n\n# fit the model\ngnb.fit(X_train, y_train)\n","167d76ee":"y_pred = gnb.predict(X_test)\n\ny_pred","a8a89bf0":"from sklearn.metrics import accuracy_score\n\nprint('Model accuracy score: {0:0.4f}'. format(accuracy_score(y_test, y_pred)))","18aef3d8":"y_pred_train = gnb.predict(X_train)\n\ny_pred_train","1a32e03b":"print('Training-set accuracy score: {0:0.4f}'. format(accuracy_score(y_train, y_pred_train)))","3fa1e952":"# print the scores on training and test set\n\nprint('Training set score: {:.4f}'.format(gnb.score(X_train, y_train)))\n\nprint('Test set score: {:.4f}'.format(gnb.score(X_test, y_test)))","3fa1d5fe":"# check class distribution in test set\n\ny_test.value_counts()","08e8f62c":"# check null accuracy score\n\nnull_accuracy = (7407\/(7407+2362))\n\nprint('Null accuracy score: {0:0.4f}'. format(null_accuracy))","76980497":"# Print the Confusion Matrix and slice it into four pieces\n\nfrom sklearn.metrics import confusion_matrix\n\ncm = confusion_matrix(y_test, y_pred)\n\nprint('Confusion matrix\\n\\n', cm)\n\nprint('\\nTrue Positives(TP) = ', cm[0,0])\n\nprint('\\nTrue Negatives(TN) = ', cm[1,1])\n\nprint('\\nFalse Positives(FP) = ', cm[0,1])\n\nprint('\\nFalse Negatives(FN) = ', cm[1,0])","a6bb1b51":"# visualize confusion matrix with seaborn heatmap\n\ncm_matrix = pd.DataFrame(data=cm, columns=['Actual Positive:1', 'Actual Negative:0'], \n                                 index=['Predict Positive:1', 'Predict Negative:0'])\n\nsns.heatmap(cm_matrix, annot=True, fmt='d', cmap='YlGnBu')","ee8fa3ef":"from sklearn.metrics import classification_report\n\nprint(classification_report(y_test, y_pred))","23b19498":"TP = cm[0,0]\nTN = cm[1,1]\nFP = cm[0,1]\nFN = cm[1,0]","a987c745":"# print classification accuracy\n\nclassification_accuracy = (TP + TN) \/ float(TP + TN + FP + FN)\n\nprint('Classification accuracy : {0:0.4f}'.format(classification_accuracy))\n","62847e64":"# print classification error\n\nclassification_error = (FP + FN) \/ float(TP + TN + FP + FN)\n\nprint('Classification error : {0:0.4f}'.format(classification_error))\n","93c156c0":"# print precision score\n\nprecision = TP \/ float(TP + FP)\n\n\nprint('Precision : {0:0.4f}'.format(precision))\n","9f147adf":"recall = TP \/ float(TP + FN)\n\nprint('Recall or Sensitivity : {0:0.4f}'.format(recall))","17217993":"true_positive_rate = TP \/ float(TP + FN)\n\n\nprint('True Positive Rate : {0:0.4f}'.format(true_positive_rate))","7afe9d4a":"false_positive_rate = FP \/ float(FP + TN)\n\n\nprint('False Positive Rate : {0:0.4f}'.format(false_positive_rate))","41f19b9a":"specificity = TN \/ (TN + FP)\n\nprint('Specificity : {0:0.4f}'.format(specificity))","7d3d4fe5":"# print the first 10 predicted probabilities of two classes- 0 and 1\n\ny_pred_prob = gnb.predict_proba(X_test)[0:10]\n\ny_pred_prob","96f59845":"# store the probabilities in dataframe\n\ny_pred_prob_df = pd.DataFrame(data=y_pred_prob, columns=['Prob of - <=50K', 'Prob of - >50K'])\n\ny_pred_prob_df","02d59a6a":"# print the first 10 predicted probabilities for class 1 - Probability of >50K\n\ngnb.predict_proba(X_test)[0:10, 1]","32aeb2f7":"# store the predicted probabilities for class 1 - Probability of >50K\n\ny_pred1 = gnb.predict_proba(X_test)[:, 1]","955ea477":"# plot histogram of predicted probabilities\n\n\n# adjust the font size \nplt.rcParams['font.size'] = 12\n\n\n# plot histogram with 10 bins\nplt.hist(y_pred1, bins = 10)\n\n\n# set the title of predicted probabilities\nplt.title('Histogram of predicted probabilities of salaries >50K')\n\n\n# set the x-axis limit\nplt.xlim(0,1)\n\n\n# set the title\nplt.xlabel('Predicted probabilities of salaries >50K')\nplt.ylabel('Frequency')","249777a0":"# plot ROC Curve\n\nfrom sklearn.metrics import roc_curve\n\nfpr, tpr, thresholds = roc_curve(y_test, y_pred1, pos_label = '>50K')\n\nplt.figure(figsize=(6,4))\n\nplt.plot(fpr, tpr, linewidth=2)\n\nplt.plot([0,1], [0,1], 'k--' )\n\nplt.rcParams['font.size'] = 12\n\nplt.title('ROC curve for Gaussian Naive Bayes Classifier for Predicting Salaries')\n\nplt.xlabel('False Positive Rate (1 - Specificity)')\n\nplt.ylabel('True Positive Rate (Sensitivity)')\n\nplt.show()\n","8a481755":"# compute ROC AUC\n\nfrom sklearn.metrics import roc_auc_score\n\nROC_AUC = roc_auc_score(y_test, y_pred1)\n\nprint('ROC AUC : {:.4f}'.format(ROC_AUC))","d8d8939b":"# calculate cross-validated ROC AUC \n\nfrom sklearn.model_selection import cross_val_score\n\nCross_validated_ROC_AUC = cross_val_score(gnb, X_train, y_train, cv=5, scoring='roc_auc').mean()\n\nprint('Cross validated ROC AUC : {:.4f}'.format(Cross_validated_ROC_AUC))","2bd87867":"# Applying 10-Fold Cross Validation\n\nfrom sklearn.model_selection import cross_val_score\n\nscores = cross_val_score(gnb, X_train, y_train, cv = 10, scoring='accuracy')\n\nprint('Cross-validation scores:{}'.format(scores))","7e31a6ea":"# compute Average cross-validation score\n\nprint('Average cross-validation score: {:.4f}'.format(scores.mean()))","dbda396f":"We can see that there are 1843 values encoded as `?` in `occupation` variable. I will replace these `?` with `NaN`.","331281b6":"So, now we will come to the end of this kernel.\n\nI hope you find this kernel useful and enjoyable.\n\nYour comments and feedback are most welcome.\n\nThank you\n","a940ea64":"As a final check, I will check for missing values in X_train and X_test.","b4ec0e61":"### Observations\n\n\n- In each row, the numbers sum to 1.\n\n\n- There are 2 columns which correspond to 2 classes - `<=50K` and `>50K`.\n\n    - Class 0 => <=50K - Class that a person makes less than equal to 50K.    \n    \n    - Class 1 => >50K  - Class that a person makes more than 50K. \n        \n    \n- Importance of predicted probabilities\n\n    - We can rank the observations by probability of whether a person makes less than or equal to 50K or more than 50K.\n\n\n- predict_proba process\n\n    - Predicts the probabilities    \n    \n    - Choose the class with the highest probability    \n    \n    \n- Classification threshold level\n\n    - There is a classification threshold level of 0.5.    \n    \n    - Class 0 => <=50K - probability of salary less than or equal to 50K is predicted if probability < 0.5.    \n    \n    - Class 1 => >50K - probability of salary more than 50K is predicted if probability > 0.5.    \n    \n","e403f654":"### Rename column names\n\nWe can see that the dataset does not have proper column names. The columns are merely labelled as 0,1,2.... and so on. We should give proper names to the columns. I will do it as follows:-","12949e29":"# **16. Classification metrices** <a class=\"anchor\" id=\"16\"><\/a>\n\n[Table of Contents](#0.1)","5a869444":"### Summary of numerical variables\n\n\n- There are 6 numerical variables. \n\n\n- These are given by `age`, `fnlwgt`, `education_num`, `capital_gain`, `capital_loss` and `hours_per_week`.\n\n\n- All of the numerical variables are of discrete data type.","82519ade":"### Engineering missing values in categorical variables","a6879c9f":"# **2. Naive Bayes algorithm intuition** <a class=\"anchor\" id=\"2\"><\/a>\n\n[Table of Contents](#0.1)\n\n\nNa\u00efve Bayes Classifier uses the Bayes\u2019 theorem to predict membership probabilities for each class such as the probability that given record or data point belongs to a particular class. The class with the highest probability is considered as the most likely class. This is also known as the **Maximum A Posteriori (MAP)**. \n\nThe **MAP for a hypothesis with 2 events A and B is**\n\n**MAP (A)**\n\n= max (P (A | B))\n\n= max (P (B | A) * P (A))\/P (B)\n\n= max (P (B | A) * P (A))\n\n\nHere, P (B) is evidence probability. It is used to normalize the result. It remains the same, So, removing it would not affect the result.\n\n\nNa\u00efve Bayes Classifier assumes that all the features are unrelated to each other. Presence or absence of a feature does not influence the presence or absence of any other feature. \n\n\nIn real world datasets, we test a hypothesis given multiple evidence on features. So, the calculations become quite complicated. To simplify the work, the feature independence approach is used to uncouple multiple evidence and treat each as an independent one.\n","6e8e5d3f":"# **13. Predict the results** <a class=\"anchor\" id=\"13\"><\/a>\n\n[Table of Contents](#0.1)","3cb08e3c":"# **19. k-Fold Cross Validation** <a class=\"anchor\" id=\"19\"><\/a>\n\n[Table of Contents](#0.1)","fd173c9a":"<a class=\"anchor\" id=\"0.1\"><\/a>\n# **Table of Contents**\n\n1.\t[Introduction to Naive Bayes algorithm](#1)\n2.\t[Naive Bayes algorithm intuition](#2)\n3.\t[Types of Naive Bayes algorithm](#3)\n4.\t[Applications of Naive Bayes algorithm](#4)\n5.\t[Import libraries](#5)\n6.\t[Import dataset](#6)\n7.\t[Exploratory data analysis](#7)\n8.\t[Declare feature vector and target variable](#8)\n9.\t[Split data into separate training and test set](#9)\n10.\t[Feature engineering](#10)\n11.\t[Feature scaling](#11)\n12.\t[Model training](#12)\n13.\t[Predict the results](#13)\n14.\t[Check accuracy score](#14)\n15.\t[Confusion matrix](#15)\n16.\t[Classification metrices](#16)\n17.\t[Calculate class probabilities](#17)\n18.\t[ROC - AUC](#18)\n19.\t[k-Fold Cross Validation](#19)\n20.\t[Results and conclusion](#20)\n21. [References](#21)\n","ef581fb8":"We can see that from the initial 14 columns, we now have 113 columns.","bce9ebf4":"### True Positive Rate\n\n\n**True Positive Rate** is synonymous with **Recall**.\n","ddcdb77a":"The confusion matrix shows `5999 + 1897 = 7896 correct predictions` and `1408 + 465 = 1873 incorrect predictions`.\n\n\nIn this case, we have\n\n\n- `True Positives` (Actual Positive:1 and Predict Positive:1) - 5999\n\n\n- `True Negatives` (Actual Negative:0 and Predict Negative:0) - 1897\n\n\n- `False Positives` (Actual Negative:0 but Predict Positive:1) - 1408 `(Type I error)`\n\n\n- `False Negatives` (Actual Positive:1 but Predict Negative:0) - 465 `(Type II error)`","238f5986":"We can see that there are no missing values in the categorical variables. I will confirm this further.","8f93ae95":"We can see that the column names are renamed. Now, the columns have meaningful names.","bd03a693":"# **6. Import dataset** <a class=\"anchor\" id=\"6\"><\/a>\n\n[Table of Contents](#0.1)","6b490530":"### Interpretation\n\n\n- Using the mean cross-validation, we can conclude that we expect the model to be around 80.63% accurate on average.\n\n- If we look at all the 10 scores produced by the 10-fold cross-validation, we can also conclude that there is a relatively small variance in the accuracy between folds, ranging from 81.35% accuracy to 79.64% accuracy. So, we can conclude that the model is independent of the particular folds used for training.\n\n- Our original model accuracy is 0.8083, but the mean cross-validation accuracy is 0.8063. So, the 10-fold cross-validation accuracy does not result in performance improvement for this model.","2e142e7b":"### Classification error","708a42b9":"# **7. Exploratory data analysis** <a class=\"anchor\" id=\"7\"><\/a>\n\n[Table of Contents](#0.1)\n\n\nNow, I will explore the data to gain insights about the data. ","c0aff6ad":"### View summary of dataset","644f2052":"## **Gaussian Na\u00efve Bayes algorithm**\n\n\nWhen we have continuous attribute values, we made an assumption that the values associated with each class are distributed according to Gaussian or Normal distribution. For example, suppose the training data contains a continuous attribute x. We first segment the data by the class, and then compute the mean and variance of x in each class. Let \u00b5i be the mean of the values and let \u03c3i be the variance of the values associated with the ith class. Suppose we have some observation value xi . Then, the probability distribution of xi given a class can be computed by the following equation \u2013\n\n\n![Gaussian Naive Bayes algorithm](https:\/\/encrypted-tbn0.gstatic.com\/images?q=tbn:ANd9GcQEWCcq1XtC1Yw20KWSHn2axYa7eY-a0T1TGtdVn5PvOpv9wW3FeA&s)","e029ea99":"ROC curve help us to choose a threshold level that balances sensitivity and specificity for a particular context.","b0d21749":"### Explore Numerical Variables","0f09b31e":"The training-set accuracy score is 0.8067 while the test-set accuracy to be 0.8083. These two values are quite comparable. So, there is no sign of overfitting. \n","26e6e775":"We can see that there are 32561 instances and 15 attributes in the data set.","eb779a2d":"### Support\n\n\n**Support** is the actual number of occurrences of the class in our dataset.","0fe4a1e0":"### Explore problems within numerical variables\n\n\nNow, I will explore the numerical variables.\n\n\n### Missing values in numerical variables","65cb07b8":"## **Bernoulli Na\u00efve Bayes algorithm**\n\nIn the multivariate Bernoulli event model, features are independent boolean variables (binary variables) describing inputs. Just like the multinomial model, this model is also popular for document classification tasks where binary term occurrence features are used rather than term frequencies.","4b9c5b19":"We can see that the occurences of most frequent class is 7407. So, we can calculate null accuracy by dividing 7407 by total number of occurences.","4ee36da6":"# **8. Declare feature vector and target variable** <a class=\"anchor\" id=\"8\"><\/a>\n\n[Table of Contents](#0.1)","b90ff6bd":"Similarly, I will take a look at the `X_test` set.","f98536b9":"# **10. Feature Engineering** <a class=\"anchor\" id=\"10\"><\/a>\n\n[Table of Contents](#0.1)\n\n\n**Feature Engineering** is the process of transforming raw data into useful features that help us to understand our model better and increase its predictive power. I will carry out feature engineering on different types of variables.\n\n\nFirst, I will display the categorical and numerical variables again separately.","bd1b8f0e":"### Observations\n\n\n- We can see that the above histogram is highly positive skewed.\n\n\n- The first column tell us that there are approximately 5700 observations with probability between 0.0 and 0.1 whose salary \n  is <=50K.\n\n\n- There are relatively small number of observations with probability > 0.5.\n\n\n- So, these small number of observations predict that the salaries will be >50K.\n\n\n- Majority of observations predcit that the salaries will be <=50K.","9b8078d7":"# **20. Results and conclusion** <a class=\"anchor\" id=\"20\"><\/a>\n\n[Table of Contents](#0.1)\n\n\n1.\tIn this project, I build a Gaussian Na\u00efve Bayes Classifier model to predict whether a person makes over 50K a year. The model yields a very good performance as indicated by the model accuracy which was found to be 0.8083.\n2.\tThe training-set accuracy score is 0.8067 while the test-set accuracy to be 0.8083. These two values are quite comparable. So, there is no sign of overfitting.\n3.\tI have compared the model accuracy score which is 0.8083 with null accuracy score which is 0.7582. So, we can conclude that our Gaussian Na\u00efve Bayes classifier model is doing a very good job in predicting the class labels.\n4.\tROC AUC of our model approaches towards 1. So, we can conclude that our classifier does a very good job in predicting whether a person makes over 50K a year.\n5.\tUsing the mean cross-validation, we can conclude that we expect the model to be around 80.63% accurate on average.\n6.\tIf we look at all the 10 scores produced by the 10-fold cross-validation, we can also conclude that there is a relatively small variance in the accuracy between folds, ranging from 81.35% accuracy to 79.64% accuracy. So, we can conclude that the model is independent of the particular folds used for training.\n7.\tOur original model accuracy is 0.8083, but the mean cross-validation accuracy is 0.8063. So, the 10-fold cross-validation accuracy does not result in performance improvement for this model.","96bc9aae":"We can see that there are no missing values in X_train and X_test.","6db342cf":"We can see that `native_country` column contains relatively large number of labels as compared to other columns. I will check for cardinality after train-test split.","12bcbdcf":"### Encode categorical variables","c0b18044":"We now have `X_train` dataset ready to be fed into the Gaussian Naive Bayes classifier. I will do it as follows.","3bb64a7f":"We can see that there are no missing values in the dataset. I will confirm this further.","01a2b0ef":"# **17. Calculate class probabilities** <a class=\"anchor\" id=\"17\"><\/a>\n\n[Table of Contents](#0.1)","2a8ce155":"### Explore problems within categorical variables\n\n\nFirst, I will explore the categorical variables.\n\n\n### Missing values in categorical variables","08d1536f":"### ROC  AUC\n\n\n**ROC AUC** stands for **Receiver Operating Characteristic - Area Under Curve**. It is a technique to compare classifier performance. In this technique, we measure the `area under the curve (AUC)`. A perfect classifier will have a ROC AUC equal to 1, whereas a purely random classifier will have a ROC AUC equal to 0.5. \n\n\nSo, **ROC AUC** is the percentage of the ROC plot that is underneath the curve.","d92d10b6":"Now, we can see that `workclass`, `occupation` and `native_country` variable contains missing values.","f1b76ff7":"# **4. Applications of Naive Bayes algorithm** <a class=\"anchor\" id=\"4\"><\/a>\n\n[Table of Contents](#0.1)\n\n\n\nNa\u00efve Bayes is one of the most straightforward and fast classification algorithm. It is very well suited for large volume of data. It is successfully used in various applications such as :\n\n1. Spam filtering\n2. Text classification\n3. Sentiment analysis\n4. Recommender systems\n\nIt uses Bayes theorem of probability for prediction of unknown class.\n","1057f870":"[Go to Top](#0)","f052f0b5":"### Frequency counts of categorical variables\n\n\nNow, I will check the frequency counts of categorical variables.","1ff81cea":"### Interpretation\n\n\n- ROC AUC is a single number summary of classifier performance. The higher the value, the better the classifier.\n\n- ROC AUC of our model approaches towards 1. So, we can conclude that our classifier does a good job in predicting whether it will rain tomorrow or not.","ee2ee39c":"### Explore categorical variables","01e5fd7f":"Now, based on the above analysis we can conclude that our classification model accuracy is very good. Our model is doing a very good job in terms of predicting the class labels.\n\n\nBut, it does not give the underlying distribution of values. Also, it does not tell anything about the type of errors our classifer is making. \n\n\nWe have another tool called `Confusion matrix` that comes to our rescue.","83f0a73c":"### Specificity","ce66fad8":"Now, we can see that there are several variables like `workclass`, `occupation` and `native_country` which contain missing values. Generally, the missing values are coded as `NaN` and python will detect them with the usual command of `df.isnull().sum()`.\n\nBut, in this case the missing values are coded as `?`. Python fail to detect these as missing values because it do not consider `?` as missing values. So, I have to replace `?` with `NaN` so that Python can detect these missing values.\n\nI will explore these variables and replace `?` with `NaN`.","735b50a5":"# **21. References** <a class=\"anchor\" id=\"21\"><\/a>\n\n[Table of Contents](#0.1)\n\n\n\nThe work done in this project is inspired from following books and websites:-\n\n1. Hands on Machine Learning with Scikit-Learn and Tensorflow by Aure\u0301lie\u0301n Ge\u0301ron\n\n2. Introduction to Machine Learning with Python by Andreas C. Mu\u0308ller and Sarah Guido\n\n3. Udemy course \u2013 Machine Learning \u2013 A Z by Kirill Eremenko and Hadelin de Ponteves\n\n4. https:\/\/en.wikipedia.org\/wiki\/Naive_Bayes_classifier\n\n5. http:\/\/dataaspirant.com\/2017\/02\/06\/naive-bayes-classifier-machine-learning\/\n\n6. https:\/\/www.datacamp.com\/community\/tutorials\/naive-bayes-scikit-learn\n\n7. https:\/\/stackabuse.com\/the-naive-bayes-algorithm-in-python-with-scikit-learn\/\n\n8. https:\/\/jakevdp.github.io\/PythonDataScienceHandbook\/05.05-naive-bayes.html","bead7ec2":"### f1-score\n\n\n**f1-score** is the weighted harmonic mean of precision and recall. The best possible **f1-score** would be 1.0 and the worst \nwould be 0.0.  **f1-score** is the harmonic mean of precision and recall. So, **f1-score** is always lower than accuracy measures as they embed precision and recall into their computation. The weighted average of `f1-score` should be used to \ncompare classifier models, not global accuracy.\n\n\n","5b73944e":"We can see that all the 6 numerical variables do not contain missing values. ","8b02b36a":"# **5. Import libraries** <a class=\"anchor\" id=\"5\"><\/a>\n\n[Table of Contents](#0.1)","93ff9df8":"# **11. Feature Scaling** <a class=\"anchor\" id=\"11\"><\/a>\n\n[Table of Contents](#0.1)","6d5d0813":"We now have training and testing set ready for model building. Before that, we should map all the feature variables onto the same scale. It is called `feature scaling`. I will do it as follows.","f64eb9d4":"### Types of variables\n\n\nIn this section, I segregate the dataset into categorical and numerical variables. There are a mixture of categorical and numerical variables in the dataset. Categorical variables have data type object. Numerical variables have data type int64.\n\n\nFirst of all, I will explore categorical variables.","fa056c90":"<a class=\"anchor\" id=\"0\"><\/a>\n# **Naive Bayes Classifier Tutorial in Python**\n\n\nHello friends,\n\nIn machine learning, Na\u00efve Bayes classification is a straightforward and powerful algorithm for the classification task. In this kernel, I implement Naive Bayes Classification algorithm with Python and Scikit-Learn. I build a Naive Bayes Classifier to predict whether a person makes over 50K a year. \n\nSo, let's get started.","86037a26":"### Precision\n\n\n**Precision** can be defined as the percentage of correctly predicted positive outcomes out of all the predicted positive outcomes. It can be given as the ratio of true positives (TP) to the sum of true and false positives (TP + FP). \n\n\nSo, **Precision** identifies the proportion of correctly predicted positive outcome. It is more concerned with the positive class than the negative class.\n\n\n\nMathematically, precision can be defined as the ratio of `TP to (TP + FP)`.\n\n","d4a89487":"### Explore workclass variable","a47ffb5f":"### Compare the train-set and test-set accuracy\n\n\nNow, I will compare the train-set and test-set accuracy to check for overfitting.","8a0e0c43":"### Explore native_country variable\n","4a849882":"We can see that there are 1836 values encoded as `?` in workclass variable. I will replace these `?` with `NaN`.","db673fe4":"### Check missing values in categorical variables again","c624003e":"### Recall\n\n\nRecall can be defined as the percentage of correctly predicted positive outcomes out of all the actual positive outcomes.\nIt can be given as the ratio of true positives (TP) to the sum of true positives and false negatives (TP + FN). **Recall** is also called **Sensitivity**.\n\n\n**Recall** identifies the proportion of correctly predicted actual positives.\n\n\nMathematically, recall can be given as the ratio of `TP to (TP + FN)`.\n\n","c828baa1":"# **15. Confusion matrix** <a class=\"anchor\" id=\"15\"><\/a>\n\n[Table of Contents](#0.1)\n\n\nA confusion matrix is a tool for summarizing the performance of a classification algorithm. A confusion matrix will give us a clear picture of classification model performance and the types of errors produced by the model. It gives us a summary of correct and incorrect predictions broken down by each category. The summary is represented in a tabular form.\n\n\nFour types of outcomes are possible while evaluating a classification model performance. These four outcomes are described below:-\n\n\n**True Positives (TP)** \u2013 True Positives occur when we predict an observation belongs to a certain class and the observation actually belongs to that class.\n\n\n**True Negatives (TN)** \u2013 True Negatives occur when we predict an observation does not belong to a certain class and the observation actually does not belong to that class.\n\n\n**False Positives (FP)** \u2013 False Positives occur when we predict an observation belongs to a    certain class but the observation actually does not belong to that class. This type of error is called **Type I error.**\n\n\n\n**False Negatives (FN)** \u2013 False Negatives occur when we predict an observation does not belong to a certain class but the observation actually belongs to that class. This is a very serious error and it is called **Type II error.**\n\n\n\nThese four outcomes are summarized in a confusion matrix given below.\n","816809ad":"We can summarize the cross-validation accuracy by calculating its mean.","16e88e4d":"# **9. Split data into separate training and test set** <a class=\"anchor\" id=\"9\"><\/a>\n\n[Table of Contents](#0.1)","caca8cb9":"# **14. Check accuracy score** <a class=\"anchor\" id=\"14\"><\/a>\n\n[Table of Contents](#0.1)","49072a87":"## **Multinomial Na\u00efve Bayes algorithm**\n\nWith a Multinomial Na\u00efve Bayes model, samples (feature vectors) represent the frequencies with which certain events have been generated by a multinomial (p1, . . . ,pn) where pi is the probability that event i occurs. Multinomial Na\u00efve Bayes algorithm is preferred to use on data that is multinomially distributed. It is one of the standard algorithms which is used in text categorization classification.","85fb7fa5":"### Explore occupation variable","a97d5449":"### Summary of categorical variables\n\n\n- There are 9 categorical variables. \n\n\n- The categorical variables are given by `workclass`, `education`, `marital_status`, `occupation`, `relationship`, `race`, `sex`, `native_country` and `income`.\n\n\n- `income` is the target variable.","26cc761c":"Now, we can see that there are no values encoded as `?` in the `workclass` variable.\n\nI will adopt similar approach with `occupation` and `native_country` column.","b04bb971":"### Classification Report\n\n\n**Classification report** is another way to evaluate the classification model performance. It displays the  **precision**, **recall**, **f1** and **support** scores for the model. I have described these terms in later.\n\nWe can print a classification report as follows:-","e4163d7d":"# **12. Model training** <a class=\"anchor\" id=\"12\"><\/a>\n\n[Table of Contents](#0.1)","232e730b":"### Compare model accuracy with null accuracy\n\n\nSo, the model accuracy is 0.8083. But, we cannot say that our model is very good based on the above accuracy. We must compare it with the **null accuracy**. Null accuracy is the accuracy that could be achieved by always predicting the most frequent class.\n\nSo, we should first check the class distribution in the test set. ","50148c9d":"### False Positive Rate","5f00e227":"# **18. ROC - AUC** <a class=\"anchor\" id=\"18\"><\/a>\n\n[Table of Contents](#0.1)\n\n\n\n### ROC Curve\n\n\nAnother tool to measure the classification model performance visually is **ROC Curve**. ROC Curve stands for **Receiver Operating Characteristic Curve**. An **ROC Curve** is a plot which shows the performance of a classification model at various \nclassification threshold levels. \n\n\n\nThe **ROC Curve** plots the **True Positive Rate (TPR)** against the **False Positive Rate (FPR)** at various threshold levels.\n\n\n\n**True Positive Rate (TPR)** is also called **Recall**. It is defined as the ratio of `TP to (TP + FN)`.\n\n\n\n\n**False Positive Rate (FPR)** is defined as the ratio of `FP to (FP + TN)`.\n\n\n\nIn the ROC Curve, we will focus on the TPR (True Positive Rate) and FPR (False Positive Rate) of a single point. This will give us the general performance of the ROC curve which consists of the TPR and FPR at various threshold levels. So, an ROC Curve plots TPR vs FPR at different classification threshold levels. If we lower the threshold levels, it may result in more items being classified as positve. It will increase both True Positives (TP) and False Positives (FP).\n\n","e186f52d":"### Classification accuracy","fbb05bf8":"We can see that our model accuracy score is 0.8083 but null accuracy score is 0.7582. So, we can conclude that our Gaussian Naive Bayes Classification model is doing a very good job in predicting the class labels.","cda20ffa":"# **3. Types of Naive Bayes algorithm** <a class=\"anchor\" id=\"3\"><\/a>\n\n[Table of Contents](#0.1)\n\n\nThere are 3 types of Na\u00efve Bayes algorithm. The 3 types are listed below:-\n\n  1. Gaussian Na\u00efve Bayes\n\n  2. Multinomial Na\u00efve Bayes\n\n  3. Bernoulli Na\u00efve Bayes\n\nThese 3 types of algorithm are explained below.\n","8ad6293f":"# **1. Introduction to Naive Bayes algorithm** <a class=\"anchor\" id=\"1\"><\/a>\n\n[Table of Contents](#0.1)\n\n\nIn machine learning, Na\u00efve Bayes classification is a straightforward and powerful algorithm for the classification task. Na\u00efve Bayes classification is based on applying Bayes\u2019 theorem with strong independence assumption between the features.  Na\u00efve Bayes classification produces good results when we use it for textual data analysis such as Natural Language Processing.\n\n\nNa\u00efve Bayes models are also known as `simple Bayes` or `independent Bayes`. All these names refer to the application of Bayes\u2019 theorem in the classifier\u2019s decision rule. Na\u00efve Bayes classifier applies the Bayes\u2019 theorem in practice. This classifier brings the power of Bayes\u2019 theorem to machine learning.\n","9bd69f45":"Here, **y_test** are the true class labels and **y_pred** are the predicted class labels in the test-set.","8ab9f099":"We can see that there are 583 values encoded as `?` in `native_country` variable. I will replace these `?` with `NaN`.\n","14ca42c4":"### Check for overfitting and underfitting","a73fea09":"### View top 5 rows of dataset","a60916c7":"**As always, I hope you find this kernel useful and your <font color=\"red\"><b>UPVOTES<\/b><\/font> would be highly appreciated**.\n","33d47746":"### Number of labels: cardinality\n\n\nThe number of labels within a categorical variable is known as **cardinality**. A high number of labels within a variable is known as **high cardinality**. High cardinality may pose some serious problems in the machine learning model. So, I will check for high cardinality."}}