{"cell_type":{"25425b07":"code","7423ad6e":"code","1ac71fb0":"code","ca29ed08":"code","05f1107c":"code","6cfed830":"code","1368236a":"code","8bbf16bf":"code","ad75a8ae":"code","bad27af8":"code","a0455cbd":"code","ee1dc9c5":"code","a6ab0741":"code","09ce2ace":"code","e5d10d00":"code","064c5497":"code","4dd8ce86":"code","b1c23d70":"code","9c587a99":"code","cd6410c2":"code","f155a001":"code","905d7539":"code","8225828a":"code","4ccce686":"code","b340fb6a":"code","874dcbed":"code","ce929dd3":"code","39c9d954":"code","dcd48997":"code","57df2b33":"code","283a6371":"code","75966036":"code","5d8e506c":"code","2fbea214":"code","010f6bc8":"code","8ab5a5f6":"code","b0107ff9":"code","26ce8e99":"code","050ada43":"code","bf42485d":"code","d6be2979":"code","d5c6515c":"code","703baeee":"markdown","7b15b2a0":"markdown","e9efc039":"markdown","7d6421e2":"markdown","565dd3a3":"markdown","299c66cd":"markdown","147f2c76":"markdown","84bbc0dc":"markdown","1ba65691":"markdown","072851ab":"markdown"},"source":{"25425b07":"import numpy as np","7423ad6e":"import pandas as pd\nfrom sklearn import linear_model\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\n","1ac71fb0":"df = pd.read_csv(r\"\/kaggle\/input\/titanic\/train.csv\")","ca29ed08":"df.head()","05f1107c":"test_data = pd.read_csv(r\"\/kaggle\/input\/titanic\/test.csv\")","6cfed830":"test_data.head()","1368236a":"total = df.isnull().sum().sort_values(ascending= False)","8bbf16bf":"percent1 = df.isnull().sum()\/df.isnull().count()*100","ad75a8ae":"percent2 = (round(percent1,1)).sort_values(ascending= False)","bad27af8":"missing_data = pd.concat([total, percent2], axis=1, keys= ['Total','%'])","a0455cbd":"missing_data.head(5)","ee1dc9c5":"df= df.drop(\"Cabin\",axis =1)","a6ab0741":"data = [df,test_data]","09ce2ace":"for dataset in data:\n    mean = df[\"Age\"].mean()\n    std = test_data[\"Age\"].std()\n    is_null = dataset[\"Age\"].isnull().sum()\n    rand_age=  np.random.randint(mean-std,mean+std, size= is_null)\n    age_slice = dataset[\"Age\"].copy()\n    age_slice[np.isnan(age_slice)] = rand_age\n    dataset[\"Age\"] = age_slice\n    dataset[\"Age\"] = df[\"Age\"].astype(int)\n    \n    ","e5d10d00":"df[\"Age\"].isnull().sum()","064c5497":"common_value = 'S'","4dd8ce86":"data = [df,test_data]","b1c23d70":"for dataset in data:\n    dataset[\"Embarked\"] = dataset[\"Embarked\"].fillna(common_value)","9c587a99":"df.info()","cd6410c2":"genders= {\"male\":0, \"female\":1}\ndata = [df, test_data]","f155a001":"for dataset in data:\n    dataset[\"Sex\"] = dataset[\"Sex\"].map(genders)","905d7539":"data = [df, test_data]\n\nfor dataset in data:\n    dataset['Fare'] = dataset['Fare'].fillna(0)\n    dataset['Fare'] = dataset['Fare'].astype(int)","8225828a":"ports = {\"S\":0, \"C\":1,\"Q\":2}\ndata = [df, test_data]","4ccce686":"for dataset in data:\n    dataset[\"Embarked\"] = dataset[\"Embarked\"].map(ports)","b340fb6a":"df.info()","874dcbed":"data = [df, test_data]\nfor dataset in data:\n    dataset['Age'] = dataset['Age'].astype(int)\n    dataset.loc[ dataset['Age'] <= 11, 'Age'] = 0\n    dataset.loc[(dataset['Age'] > 11) & (dataset['Age'] <= 18), 'Age'] = 1\n    dataset.loc[(dataset['Age'] > 18) & (dataset['Age'] <= 22), 'Age'] = 2\n    dataset.loc[(dataset['Age'] > 22) & (dataset['Age'] <= 27), 'Age'] = 3\n    dataset.loc[(dataset['Age'] > 27) & (dataset['Age'] <= 33), 'Age'] = 4\n    dataset.loc[(dataset['Age'] > 33) & (dataset['Age'] <= 40), 'Age'] = 5\n    dataset.loc[(dataset['Age'] > 40) & (dataset['Age'] <= 66), 'Age'] = 6\n    dataset.loc[ dataset['Age'] > 66, 'Age'] = 6","ce929dd3":"df.head()","39c9d954":"Xtrain = df.drop([\"PassengerId\",\"Survived\",\"Name\",\"Ticket\"], axis =1)","dcd48997":"Ytrain = df[\"Survived\"]","57df2b33":"Xtest = test_data.drop([\"PassengerId\",\"Name\",\"Ticket\"], axis =1)","283a6371":"random_forest = RandomForestClassifier(n_estimators = 100)","75966036":"Xtest = Xtest.drop(\"Cabin\" , axis=1)","5d8e506c":"random_forest.fit(Xtrain,Ytrain)","2fbea214":"Xtest.head()","010f6bc8":"Xtrain.head()","8ab5a5f6":"Y_prediction = random_forest.predict(Xtest)","b0107ff9":"random_forest.score(Xtrain, Ytrain)","26ce8e99":"acc_random_forest = round(random_forest.score(Xtrain, Ytrain) * 100, 2)","050ada43":"print(acc_random_forest)","bf42485d":"output = pd.DataFrame({\"PassengerId\": test_data.PassengerId,\"Survived\": Y_prediction})","d6be2979":"output.to_csv(\"my_submssion.csv\", index = False)","d5c6515c":"print(\"Your submission was successufly saved!\")","703baeee":"Converting the different data types into a single specific like int datatype here for efficient working of our ML models.","7b15b2a0":"Will  be trying feature engineering more significantly in the next version to improve our models accuracy.\nHope it was a interesting read!!!","e9efc039":"### **Building Model**","7d6421e2":"### ****Predicting Titanic Survival****\n\n  This is a simple and elegant ML Problem for a beginner. This problem has input with 12 features and output of 0\/1 like   a binary classification problem. There are mnay binary classifications algorithms available to classify this problem     at first.   But the predictions accuracy can been improved  further with the following techniques:-\n  \n  1. Data Analysis.\n  \n     a) Data Visualising to find the relevance of the 12 features, their relation to the prediction and finding the data         discripencies.\n     \n     b) Data Cleaning  to remove the undesired data, handeling the missing data and making necessary data type                   convesions before training the model.\n     \n  2. Feature Engineering .\n  \n  3. Comparing the various ML models.\n  \n  This is true for all the ML model but  as beginner following this procedure step by step allows one to have a complete   good hold of our ML model and helps us in keeping our foundations(data) strong. ","565dd3a3":"Now we have cleaned our data, and hence can now feed to a ML alorithm. Will be using Feature Engineering in the next version of this notebook, for now we will proceed with the first step i.e data cleaning and  the model implementation. ","299c66cd":"### **Categorising Age**","147f2c76":"For the 'Embarked' column too, have replaced the missing values with the most common values for this column.","84bbc0dc":"### **Converting value types**","1ba65691":"Getting the number of null values for all the features. Such that it can be furthur used for cleaning the data using different techniques depeding upon the its number for each feature. ","072851ab":"The above table illustrates the percentage of null values for the different features int the dataset. The column 'Cabin' has about 77% null values,  we can drop the column from our dataset to be used for prediction. And we can use a different approach for the second most null values feature i.e the 'Age' column. Have replaced the null age values with the mean age value. "}}