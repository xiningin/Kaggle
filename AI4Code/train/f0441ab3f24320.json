{"cell_type":{"5a7071ff":"code","a445075b":"code","f1fcebc5":"code","3aed556f":"code","75ca950a":"code","be7d2ec3":"code","3bd3c0b6":"code","90e344c5":"code","20e3eb0a":"code","f2f2d4a0":"code","1b6c0239":"code","caac98fb":"code","665d26bd":"code","7315305b":"code","cdd7a8bf":"code","ae254292":"code","de300c83":"code","d0fbc331":"code","b7c06301":"code","63fb8192":"markdown","c5ea7d7c":"markdown","9a84aa86":"markdown","98fdba0d":"markdown","f7209079":"markdown","698c27cb":"markdown","460a5187":"markdown","35cedf00":"markdown","d18daaa7":"markdown","ff0c7e15":"markdown","46b6740e":"markdown","ab99e4c8":"markdown","20bb04b5":"markdown"},"source":{"5a7071ff":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","a445075b":"from sklearn.feature_extraction.text import TfidfVectorizer\nimport nltk\nfrom nltk.corpus import stopwords\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression","f1fcebc5":"TRAIN_PATH = '..\/input\/nlp-getting-started\/train.csv'\nTEST_PATH = '..\/input\/nlp-getting-started\/test.csv'\ntrain = pd.read_csv(TRAIN_PATH)\ntest = pd.read_csv(TEST_PATH)","3aed556f":"for i in range(len(train)):\n    train.text[i] = train.text[i].lower()\n\nfor i in range(len(test)):\n    test.text[i] = test.text[i].lower()","75ca950a":"tokenizer = nltk.tokenize.TreebankWordTokenizer()","be7d2ec3":"stop_words = stopwords.words('english')\n\nfor i in range(len(train)):\n    tokens = tokenizer.tokenize(train.text[i])\n    review = [i for i in tokens if not i in stop_words]\n    train.text[i] = \" \".join(review)\n    \nfor i in range(len(test)):\n    tokens = tokenizer.tokenize(test.text[i])\n    review = [i for i in tokens if not i in stop_words]\n    test.text[i] = \" \".join(review)","3bd3c0b6":"lemmatizer = nltk.stem.WordNetLemmatizer()","90e344c5":"for i in range(len(train)):\n    tokens = tokenizer.tokenize(train.text[i])\n    train.text[i] = \" \".join(lemmatizer.lemmatize(token) for token in tokens)","20e3eb0a":"for i in range(len(test)):\n    tokens = tokenizer.tokenize(test.text[i])\n    test.text[i] = \" \".join(lemmatizer.lemmatize(token) for token in tokens)","f2f2d4a0":"train.head()","1b6c0239":"train = train.drop(labels=['keyword', 'location'], axis=1)\ntest = test.drop(labels=['keyword', 'location'], axis=1)","caac98fb":"tfidf = TfidfVectorizer(min_df=5, max_df=0.5, ngram_range=(1, 2))","665d26bd":"fit_tfidf = tfidf.fit_transform(train.text).toarray()\ntrain_data = pd.DataFrame(fit_tfidf, columns=tfidf.get_feature_names())","7315305b":"test_data = pd.DataFrame(tfidf.transform(test.text).toarray(), \n                         columns=tfidf.get_feature_names())","cdd7a8bf":"x_train, x_val, y_train, y_val = train_test_split(train_data, \n                                  train.target, \n                                  test_size=0.2,\n                                  random_state=47)","ae254292":"logreg = LogisticRegression()","de300c83":"logreg.fit(x_train, y_train)","d0fbc331":"val_pred = logreg.predict(x_val)\ny_pred = logreg.predict(test_data)","b7c06301":"logreg.score(x_val, y_val)","63fb8192":"2. Remove Stopwords","c5ea7d7c":"Create the Logistic Regression Object","9a84aa86":"Make predictions on the validation and test data","98fdba0d":"#### This is merely my second notebook and I have just started making them. Please do comment or give any suggestions that you can to help me learn and improve!","f7209079":"## Import the Libraries","698c27cb":"We probably don't need the other 2 columns for this task, so I will drop the columns","460a5187":"Let's specify the data paths!","35cedf00":"Fit the object on the training data","d18daaa7":"Now, we initialize the TFIDF vectorizer for creating the bag of n grams","ff0c7e15":"### Text Preprocessing\n\n1. Tweets to lowercase","46b6740e":"Let's check the score on validation data","ab99e4c8":"We got an 80% probability score on the validation data, which is a decent score","20bb04b5":"3. I will be doing lemmatization for token normalization with TreeBank Tokenizer and WordNet Lemmatizer. I have tried various combinations of stemmer and lemmatizer but the best result that I have got is with only doing lemmatization"}}