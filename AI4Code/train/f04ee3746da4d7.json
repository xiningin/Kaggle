{"cell_type":{"55c5d065":"code","2ec54ae5":"code","98284dc7":"code","8fb0d1bb":"code","29b914cb":"code","4ecabdd3":"code","5240f768":"code","55c61b53":"code","412cbaaf":"code","cb0b91cb":"code","6376aefc":"code","01683c84":"code","b77f17b8":"code","c0d9f655":"code","923f38c2":"code","7d0accc9":"code","6c2abdec":"code","ebd7b840":"code","f6f44367":"code","2465da4a":"code","82ab1e95":"code","43b36fb4":"code","3287b0ed":"code","7a5cd3fd":"code","7c4a2fcf":"code","f532c349":"code","ce25511f":"code","36070aed":"code","4260a6d1":"markdown","15afbebc":"markdown","38d2fcdd":"markdown","395017fa":"markdown","f7d3a1d5":"markdown","69069c4b":"markdown","642414be":"markdown","6d93d2c6":"markdown","131c4362":"markdown","1efc933d":"markdown","feb0ee3f":"markdown","c2530c9c":"markdown","08bdf20a":"markdown","5576e8d8":"markdown","d6d97752":"markdown","0a5d1927":"markdown","5a82664e":"markdown"},"source":{"55c5d065":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n%matplotlib inline\nsns.set_style(\"whitegrid\")","2ec54ae5":"data = pd.read_csv(\"\/kaggle\/input\/creditcardfraud\/creditcard.csv\")\ndata.head()","98284dc7":"data.info()","8fb0d1bb":"pd.set_option(\"display.float\", \"{:.2f}\".format)\ndata.describe()","29b914cb":"data.isnull().sum().sum()","4ecabdd3":"data.columns","5240f768":"LABELS = [\"Normal\", \"Fraud\"]\n\ncount_classes = pd.value_counts(data['Class'], sort = True)\ncount_classes.plot(kind = 'bar', rot=0)\nplt.title(\"Transaction Class Distribution\")\nplt.xticks(range(2), LABELS)\nplt.xlabel(\"Class\")\nplt.ylabel(\"Frequency\");","55c61b53":"data.Class.value_counts()","412cbaaf":"fraud = data[data['Class']==1]\nnormal = data[data['Class']==0]\n\nprint(f\"Shape of Fraudulant transactions: {fraud.shape}\")\nprint(f\"Shape of Non-Fraudulant transactions: {normal.shape}\")","cb0b91cb":"pd.concat([fraud.Amount.describe(), normal.Amount.describe()], axis=1)","6376aefc":"pd.concat([fraud.Time.describe(), normal.Time.describe()], axis=1)","01683c84":"# plot the time feature\nplt.figure(figsize=(14,10))\n\nplt.subplot(2, 2, 1)\nplt.title('Time Distribution (Seconds)')\n\nsns.distplot(data['Time'], color='blue');\n\n#plot the amount feature\nplt.subplot(2, 2, 2)\nplt.title('Distribution of Amount')\nsns.distplot(data['Amount'],color='blue');","b77f17b8":"# data[data.Class == 0].Time.hist(bins=35, color='blue', alpha=0.6)\nplt.figure(figsize=(14, 12))\n\nplt.subplot(2, 2, 1)\ndata[data.Class == 1].Time.hist(bins=35, color='blue', alpha=0.6, label=\"Fraudulant Transaction\")\nplt.legend()\n\nplt.subplot(2, 2, 2)\ndata[data.Class == 0].Time.hist(bins=35, color='blue', alpha=0.6, label=\"Non Fraudulant Transaction\")\nplt.legend()","c0d9f655":"# heatmap to find any high correlations\n\nplt.figure(figsize=(10,10))\nsns.heatmap(data=data.corr(), cmap=\"seismic\")\nplt.show();","923f38c2":"from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\nscalar = StandardScaler()\n\nX = data.drop('Class', axis=1)\ny = data.Class\n\nX_train_v, X_test, y_train_v, y_test = train_test_split(X, y, \n                                                    test_size=0.3, random_state=42)\nX_train, X_validate, y_train, y_validate = train_test_split(X_train_v, y_train_v, \n                                                            test_size=0.2, random_state=42)\n\nX_train = scalar.fit_transform(X_train)\nX_validate = scalar.transform(X_validate)\nX_test = scalar.transform(X_test)\n\nw_p = y_train.value_counts()[0] \/ len(y_train)\nw_n = y_train.value_counts()[1] \/ len(y_train)\n\nprint(f\"Fraudulant transaction weight: {w_n}\")\nprint(f\"Non-Fraudulant transaction weight: {w_p}\")","7d0accc9":"print(f\"TRAINING: X_train: {X_train.shape}, y_train: {y_train.shape}\\n{'_'*55}\")\nprint(f\"VALIDATION: X_validate: {X_validate.shape}, y_validate: {y_validate.shape}\\n{'_'*50}\")\nprint(f\"TESTING: X_test: {X_test.shape}, y_test: {y_test.shape}\")","6c2abdec":"from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, f1_score\n\ndef print_score(label, prediction, train=True):\n    if train:\n        clf_report = pd.DataFrame(classification_report(label, prediction, output_dict=True))\n        print(\"Train Result:\\n================================================\")\n        print(f\"Accuracy Score: {accuracy_score(label, prediction) * 100:.2f}%\")\n        print(\"_______________________________________________\")\n        print(f\"Classification Report:\\n{clf_report}\")\n        print(\"_______________________________________________\")\n        print(f\"Confusion Matrix: \\n {confusion_matrix(y_train, prediction)}\\n\")\n        \n    elif train==False:\n        clf_report = pd.DataFrame(classification_report(label, prediction, output_dict=True))\n        print(\"Test Result:\\n================================================\")        \n        print(f\"Accuracy Score: {accuracy_score(label, prediction) * 100:.2f}%\")\n        print(\"_______________________________________________\")\n        print(f\"Classification Report:\\n{clf_report}\")\n        print(\"_______________________________________________\")\n        print(f\"Confusion Matrix: \\n {confusion_matrix(label, prediction)}\\n\") ","ebd7b840":"from tensorflow import keras\n\nmodel = keras.Sequential([\n    keras.layers.Dense(256, activation='relu', input_shape=(X_train.shape[-1],)),\n    keras.layers.BatchNormalization(),\n    keras.layers.Dropout(0.3),\n    keras.layers.Dense(256, activation='relu'),\n    keras.layers.BatchNormalization(),\n    keras.layers.Dropout(0.3),\n    keras.layers.Dense(256, activation='relu'),\n    keras.layers.BatchNormalization(),\n    keras.layers.Dropout(0.3),\n    keras.layers.Dense(1, activation='sigmoid'),\n])\n\nmodel.summary()","f6f44367":"METRICS = [\n#     keras.metrics.Accuracy(name='accuracy'),\n    keras.metrics.FalseNegatives(name='fn'),\n    keras.metrics.FalsePositives(name='fp'),\n    keras.metrics.TrueNegatives(name='tn'),\n    keras.metrics.TruePositives(name='tp'),\n    keras.metrics.Precision(name='precision'),\n    keras.metrics.Recall(name='recall')\n]\n\nmodel.compile(optimizer=keras.optimizers.Adam(1e-4), loss='binary_crossentropy', metrics=METRICS)\n\ncallbacks = [keras.callbacks.ModelCheckpoint('fraud_model_at_epoch_{epoch}.h5')]\nclass_weight = {0:w_p, 1:w_n}\n\nr = model.fit(\n    X_train, y_train, \n    validation_data=(X_validate, y_validate),\n    batch_size=2048, \n    epochs=300, \n#     class_weight=class_weight,\n    callbacks=callbacks,\n)","2465da4a":"score = model.evaluate(X_test, y_test)\nprint(score)","82ab1e95":"plt.figure(figsize=(12, 16))\n\nplt.subplot(4, 2, 1)\nplt.plot(r.history['loss'], label='Loss')\nplt.plot(r.history['val_loss'], label='val_Loss')\nplt.title('Loss Function evolution during training')\nplt.legend()\n\nplt.subplot(4, 2, 2)\nplt.plot(r.history['fn'], label='fn')\nplt.plot(r.history['val_fn'], label='val_fn')\nplt.title('Accuracy evolution during training')\nplt.legend()\n\nplt.subplot(4, 2, 3)\nplt.plot(r.history['precision'], label='precision')\nplt.plot(r.history['val_precision'], label='val_precision')\nplt.title('Precision evolution during training')\nplt.legend()\n\nplt.subplot(4, 2, 4)\nplt.plot(r.history['recall'], label='recall')\nplt.plot(r.history['val_recall'], label='val_recall')\nplt.title('Recall evolution during training')\nplt.legend()","43b36fb4":"y_train_pred = model.predict(X_train)\ny_test_pred = model.predict(X_test)\n\nprint_score(y_train, y_train_pred.round(), train=True)\nprint_score(y_test, y_test_pred.round(), train=False)\n\nscores_dict = {\n    'ANNs': {\n        'Train': f1_score(y_train, y_train_pred.round()),\n        'Test': f1_score(y_test, y_test_pred.round()),\n    },\n}","3287b0ed":"from xgboost import XGBClassifier\n\nxgb_clf = XGBClassifier()\nxgb_clf.fit(X_train, y_train, eval_metric='aucpr')\n\ny_train_pred = xgb_clf.predict(X_train)\ny_test_pred = xgb_clf.predict(X_test)\n\nprint_score(y_train, y_train_pred, train=True)\nprint_score(y_test, y_test_pred, train=False)\n\nscores_dict['XGBoost'] = {\n        'Train': f1_score(y_train,y_train_pred),\n        'Test': f1_score(y_test, y_test_pred),\n}","7a5cd3fd":"from sklearn.ensemble import RandomForestClassifier\n\nrf_clf = RandomForestClassifier(n_estimators=100, oob_score=False)\nrf_clf.fit(X_train, y_train)\n\ny_train_pred = rf_clf.predict(X_train)\ny_test_pred = rf_clf.predict(X_test)\n\nprint_score(y_train, y_train_pred, train=True)\nprint_score(y_test, y_test_pred, train=False)\n\nscores_dict['Random Forest'] = {\n        'Train': f1_score(y_train,y_train_pred),\n        'Test': f1_score(y_test, y_test_pred),\n}","7c4a2fcf":"from catboost import CatBoostClassifier\n\ncb_clf = CatBoostClassifier()\ncb_clf.fit(X_train, y_train)","f532c349":"y_train_pred = cb_clf.predict(X_train)\ny_test_pred = cb_clf.predict(X_test)\n\nprint_score(y_train, y_train_pred, train=True)\nprint_score(y_test, y_test_pred, train=False)\n\nscores_dict['CatBoost'] = {\n        'Train': f1_score(y_train,y_train_pred),\n        'Test': f1_score(y_test, y_test_pred),\n}","ce25511f":"from lightgbm import LGBMClassifier\n\nlgbm_clf = LGBMClassifier()\nlgbm_clf.fit(X_train, y_train, eval_metric='aucpr')\n\ny_train_pred = lgbm_clf.predict(X_train)\ny_test_pred = lgbm_clf.predict(X_test)\n\nprint_score(y_train, y_train_pred, train=True)\nprint_score(y_test, y_test_pred, train=False)\n\nscores_dict['LigthGBM'] = {\n        'Train': f1_score(y_train,y_train_pred),\n        'Test': f1_score(y_test, y_test_pred),\n}","36070aed":"scores_df = pd.DataFrame(scores_dict)\n\nscores_df.plot(kind='barh', figsize=(15, 8))","4260a6d1":"# 4. 3. Random Forest","15afbebc":"# \ud83d\udd0d Exploratory Data Analysis\n","38d2fcdd":"How different are the amount of money used in different transaction classes?","395017fa":"### Highest correlations come from:\n    - Time & V3 (-0.42)\n    - Amount & V2 (-0.53)\n    - Amount & V4 (0.4)\n\n- While these correlations are high, I don't expect it to run the risk of multicollinearity.\n\n- The correlation matrix shows also that none of the V1 to V28 PCA components have any correlation to each other however if we observe Class has some form positive and negative correlations with the V components but has no correlation with Time and Amount.","f7d3a1d5":"Let us now check the missing values in the dataset","69069c4b":"Do fraudulent transactions occur more often during certain time frame ?","642414be":"### The only non-transformed variables to work with are:\n- `Time`\n- `Amount`\n- `Class` (1: fraud, 0: not_fraud)","6d93d2c6":"# 4. Model Building\n# 4. 1. Artificial Neural Network (ANNs)","131c4362":"# 4. 5. LigthGBM","1efc933d":"# 5. Model Comparaison","feb0ee3f":"Notice how imbalanced is our original dataset! Most of the transactions are non-fraud. If we use this dataframe as the base for our predictive models and analysis we might get a lot of errors and our algorithms will probably overfit since it will \"assume\" that most transactions are not fraud. But we don't want our model to assume, we want our model to detect patterns that give signs of fraud!","c2530c9c":"# 4. 2. XGBoost","08bdf20a":"By seeing the distributions we can have an idea how skewed are these features, we can also see further distributions of the other features. There are techniques that can help the distributions be less skewed which will be implemented in this notebook in the future.\n\nDoesn't seem like the time of transaction really matters here as per above observation.\nNow let us take a sample of the dataset for out modelling and prediction","5576e8d8":"# 4. 4. CatBoost","d6d97752":"# \ud83d\udcb3 Credit Card Fraud Detection Intuitions\n\n## What is Credit Card Fraud?\nCredit card fraud is when someone uses another person's credit card or account information to make unauthorized purchases or access funds through cash advances. Credit card fraud doesn\u2019t just happen online; it happens in brick-and-mortar stores, too. As a business owner, you can avoid serious headaches \u2013 and unwanted publicity \u2013 by recognizing potentially fraudulent use of credit cards in your payment environment.\n\n## Three challenges surrounding credit card fraud\n\n1. It's not always easy to agree on ground truth for what \"fraud\" means.\n2. Regardless of how you define ground truth, the vast majority of charges are not fraudulent.\n3. Most merchants aren't experts at evaluating the business impact of fraud.\n\n## Problem Statement:\n\nThe Credit Card Fraud Detection Problem includes modeling past credit card transactions with the knowledge of the ones that turned out to be a fraud. This model is then used to identify whether a new transaction is fraudulent or not. Our aim here is to detect 100% of the fraudulent transactions while minimizing the incorrect fraud classifications.\n\n\n## Observations\n- Very few transactions are actually fraudulent (less than 1%). The data set is highly skewed, consisting of `492` frauds in a total of `284,807` observations. This resulted in only `0.172%` fraud cases. This skewed set is justified by the low number of fraudulent transactions.\n- The dataset consists of numerical values from the `28` \u2018Principal Component Analysis (PCA)\u2019 transformed features, namely V1 to V28. Furthermore, there is no metadata about the original features provided, so pre-analysis or feature study could not be done.\n- The \u2018Time\u2019 and \u2018Amount\u2019 features are not transformed data.\n- There is no missing value in the dataset.\n\n## Why does class imbalanced affect model performance?\n\n- In general, we want to maximize the recall while capping FPR (False Positive Rate), but you can classify a lot of charges wrong and still maintain a low FPR because you have a large number of true negatives.\n- This is conducive to picking a relatively low threshold, which results in the high recall but extremely low precision.\n\n## What is the catch?\n- Training a model on a balanced dataset optimizes performance on validation data.\n- However, the goal is to optimize performance on the imbalanced production dataset. You ultimately need to find a balance that works best in production.\n- One solution to this problem is: Use all fraudulent transactions, but subsample non-fraudulent transactions as needed to hit our target rate.\n\n## Business questions to brainstorm:\nSince all features are anonymous, we will focus our analysis on non-anonymized features: `Time`, `Amount`\n1. How different is the amount of money used in different transaction classes?\n2. Do fraudulent transactions occur more often during a certain frames?\n","0a5d1927":"Determine the number of fraud and valid transactions in the entire dataset.","5a82664e":"# 3. Data Pre-processing\n\n`Time` and `Amount` should be scaled as the other columns."}}