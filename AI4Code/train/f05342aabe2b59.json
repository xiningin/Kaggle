{"cell_type":{"b787ec65":"code","910a16e7":"code","05ab71bd":"code","0ae10344":"code","60f1545f":"code","4b50326f":"code","f1b36381":"code","36977a4c":"code","05fc6ad6":"code","c0315996":"code","ce5513dd":"code","7b4e30ea":"code","3c9d4611":"code","14a4c937":"code","3321d65a":"code","8f47ded7":"code","561e5fe1":"code","23e44f9b":"code","0668e2d1":"code","d558d51c":"code","6a6293df":"code","91ea1a9d":"code","8b081372":"code","26314912":"code","0f360acd":"code","a444b4bf":"code","ee9cb0a0":"code","0896ec53":"code","5c0f85a2":"code","28070491":"code","054ba8e4":"markdown","91b5f904":"markdown","73eb42c8":"markdown","0d0949e0":"markdown","f9ae3e5b":"markdown","3a02c795":"markdown","621ae305":"markdown","718af6ae":"markdown"},"source":{"b787ec65":"import numpy as np\nimport random\nimport time\n\nfrom matplotlib import pyplot as plt\n\nfrom scipy.stats import beta\nfrom scipy import integrate\n\nimport pandas as pd\n","910a16e7":"# Simulate very brute-forcely the optimal selecting of bandits\n# to see what expected optimal rewards is, and how low a probability is taken.\n# Set up 100 bandits with probs from 0.01 to 1.0:\nbandit_probs = (np.array(range(100)) + 1.0)\/100.0\n\n# Or, can use the actual values from a simulation further down:\n# After running the simulation and getting the initial probs\n# return here and uncomment this line:\n##bandit_probs = np.sort(np.array(initial_probs)\/100.0)\n# Optimal value and min probability look very similar to idealized case.\n\n# Keep track of things...\nbandit_counts = np.zeros(100)\ncurrent_probs = 1.0 * bandit_probs\npulls = []\npulled_probs = []\nscore = [0]\nfor ip in range(4000):\n    pulls = pulls + [ip]\n    # choose the highest prob one\n    ihigh = np.argmax(current_probs)\n    # add it to probs chosen, count bandit and adjust prob\n    score = score + [score[-1] + current_probs[ihigh]]\n    pulled_probs = pulled_probs + [current_probs[ihigh]]\n    bandit_counts[ihigh] += 1\n    current_probs[ihigh] *= 0.97\nscore = np.array(score[1:])\npulls = np.array(pulls)\nbandit_counts = np.array(bandit_counts)\n\n# Some results:\nave_prob_pulled = sum(pulled_probs)\/len(pulled_probs)\nprint(\"Expected Total rewards: {:.2f}\".format(4000*ave_prob_pulled))\nprint(\"Average probability of the pulls = {:.4f}\".format(ave_prob_pulled))\nprint(\"Minimum probability pulled: {:.4f}\".format(min(pulled_probs)))\nprint(\"\\nNumber of times each bandit was pulled (combined player pulls):\")\nprint(bandit_counts)\n\n# These plots are for one of the two optimal players\n# Make a Fraz plot: probability of selected bandit vs step\nplt.plot(pulls\/2, pulled_probs)\nplt.xlabel(\"Game step (each step = 2 pulls)\")\nplt.ylabel(\"Bandit probability when selected\")\nplt.ylim(0.,1.0)\nplt.show()\n\n# Expected score vs step\nplt.plot(pulls\/2, score\/2)\nplt.xlabel(\"Game step (each step = 2 pulls)\")\nplt.ylabel(\"Expected score (one player)\")\nplt.show()\n\n# Show the counts vs bin\nplt.scatter(bandit_probs,bandit_counts\/2,s=2)\nplt.xlabel(\"Bandit Initial Probability\")\nplt.ylabel(\"Number of Bandit's Pulls (per player)\")\nplt.show()\n\n","05ab71bd":"# Copy and play with Xin Cui's numerical calculation of the posterior\n# mean and std of p_i - it includes the \"discount factor\" (0.97**m).\n# https:\/\/www.kaggle.com\/xincuimath\/weighted-expected-mean-and-std-estimation \n# Added optional plotting of curves.\n\ndef get_expected_mean_std(weights, rewards, plotit=False):\n\n    def pdf(p, weights, rewards, normalization=1):\n        s = 1\n        for weight, reward in zip(weights, rewards):\n            if reward == 1:\n                s *= (weight * p)\n            else:\n                s *= (1 - weight * p)\n        return s \/ normalization\n\n    normalization = integrate.quad(\n        lambda x: pdf(x, weights, rewards), 0, 1)[0]\n    first_order = integrate.quad(\n        lambda x: x * pdf(x, weights, rewards, normalization), 0, 1)[0]\n    second_order = integrate.quad(\n        lambda x: x * x * pdf(x, weights, rewards, normalization), 0, 1)[0]\n    \n    # Show a plot of the \"Bay-XC\" pdf along with the naive beta?\n    a = sum(rewards)\n    b = len(rewards) - a\n    \n    if plotit:\n        # for use in plotting:\n        mean = first_order\n        std = (second_order - first_order ** 2) ** 0.5\n        prob_values = np.linspace(0, 1, 1000)\n        fig, ax = plt.subplots(figsize = (10,6))\n        # plot the beta pdf\n        ax.plot(prob_values, beta.pdf(prob_values, a+1, b+1),c='gray',\n                label=f'Beta: a={a}, b={b}')\n        # calc and plot the Bay-XC pdf:\n        bayxc_pdf_values = pdf(prob_values, weights, rewards, normalization)\n        ax.plot(prob_values, bayxc_pdf_values, c='blue',\n                label=f'BayXC: p_intrinsic')\n        # show the mean\n        peak_height = max(bayxc_pdf_values)\n        ax.plot([mean,mean],[0.0,0.95*peak_height],\"--\",c='lightblue')\n        # show +\/-1 std\n        ax.plot([mean+std,mean+std],[0.0,0.2*peak_height],\"--\",c='lightblue')\n        ax.plot([mean-std,mean-std],[0.0,0.2*peak_height],\"--\",c='lightblue')\n        # show + UCB_delta, p=5%\n        ucb_delta = np.sqrt(-1.0*np.log(0.05)\/(2*len(rewards)))\n        ax.plot([mean+ucb_delta,mean+ucb_delta],[0.0,0.2*peak_height],\"--\",\n                c='lightgreen')\n        # plot the Bay-XC modified by the minimum weight (aka \"discount factor\")\n        ax.plot(min(weights)*prob_values, bayxc_pdf_values\/min(weights),\n                c='orange', label=f'BayXC(p_i) * DF={int(1000*min(weights))\/1000}')\n        plt.xlabel('p_intrinsic',size=20)\n        plt.ylabel('probability density value',size=16)\n        plt.title(\"Comparing the 'BayXC' distribution (includes DFs) with Beta\"+\n                  \", \"+str(len(rewards))+\" trials\", size=16)\n        ax.legend(prop={'size': 14})\n        plt.show()\n    \n    return first_order, (second_order - first_order ** 2) ** 0.5\n","0ae10344":"# Demonstrate how it works compared with usual beta distribution\ntrials = 17\np_one = 0.5\n\n# No discounting\ndiscountings = [1 for _ in range(trials)]\nrewards = [int(np.random.choice([0, 1], p=(1-p_one, p_one)))\n                                       for _ in range(trials)]\n\n# Use the with-weightings formula\n(mean, std) = get_expected_mean_std(discountings, rewards, plotit=True)\nprint(\"Xin Cui's function gives  mean, std:\")\nprint((mean,std))\n\n# Do the equivalent calculations using beta function:\nprint(\"The beta function gives  mean, std:\")\nprint((beta.mean(1 + sum(rewards), trials+1 - sum(rewards)),\n        beta.std(1 + sum(rewards), trials+1 - sum(rewards))))\n\nprint(\"--> They are the same when there is no 'discount factor'.\")","60f1545f":"# Now re-make the observations and include the discounting in the \"data\"\n# Put new observed values in the existing variables:\n\nfor it in range(trials):\n    discountings[it] = 0.97**it\n    p_k = p_one * discountings[it]\n    rewards[it] = int(np.random.choice([0, 1], p=(1-p_k, p_k)))\n    \n# Use the with-weightings formula\n(mean, std) = get_expected_mean_std(discountings, rewards, plotit=True)\nprint(\"Xin Cui's function gives  mean, std:\")\nprint((mean,std))\n\n# Do the equivalent calculations using beta function:\nprint(\"The beta function gives  mean, std:\")\nprint((beta.mean(1 + sum(rewards), trials+1 - sum(rewards)),\n        beta.std(1 + sum(rewards), trials+1 - sum(rewards))))\n\nprint(\"--> The BayXC (usually) gives a closer estimate\" +\n              \" of the true p_i = {}\".format(p_one))","4b50326f":"! pip install kaggle-environments --upgrade -q   # -q for quiet ?\n\nfrom kaggle_environments import make, evaluate\n\nenv = make(\"mab\", debug=True)\n\n# List of available default agents\n##print(list(env.agents))","f1b36381":"# For reference, here are the values passed into the agent:\n# observation:\n#   {'remainingOverageTime': 60, 'step': 0, 'agentIndex': 0, \n#    'reward': 0, 'lastActions': []}\n# configuration:\n#   {'episodeSteps': 2000, 'actTimeout': 0.25, 'runTimeout': 1200, \n#    'banditCount': 100, 'decayRate': 0.97, 'sampleResolution': 100}","36977a4c":"%%writefile random_agent.py\n\nimport random\n\ndef random_agent(observation, configuration):\n    return random.randrange(configuration.banditCount)","05fc6ad6":"%%writefile constant_agent.py\n\n# Always \"pull\" the same bandit; the reward prob reduces to 0.01 (not 0).\ndef random_agent(observation, configuration):\n    return int(42)","c0315996":"%%writefile sequential_agent.py\n\n# Pull each bandit in sequence... (reverse order for fun)\ndef random_agent(observation, configuration):\n    return int(configuration.banditCount -\n               1 - (observation.step % configuration.banditCount))","ce5513dd":"# There is a submission.py provided by the Santa 2020 contest\n# copy that to this dir:\n!cp ..\/input\/santa-2020\/submission.py ucb_santa2020.py","7b4e30ea":"%%writefile ucb_half5percent.py\n\n# This is the Santa 2020 UCB agent modified for a constant p_small = 0.05\n# and using 1\/2 of the delta value.\n# Also randomize the starting order of bandits.\nimport math\nimport numpy as np\n\nlast_bandit = -1\ntotal_reward = 0\n\nsums_of_reward = None\nnumbers_of_selections = None\n\ndef ucb_agent(observation, configuration):    \n    global sums_of_reward, numbers_of_selections, last_bandit, total_reward\n\n    if observation.step == 0:\n        numbers_of_selections = [0] * configuration[\"banditCount\"]\n        sums_of_reward = [0] * configuration[\"banditCount\"]\n\n    if last_bandit > -1:\n        reward = observation.reward - total_reward\n        sums_of_reward[last_bandit] += reward\n        total_reward += reward\n\n    bandit = 0\n    max_upper_bound = 0\n    for i in range(0, configuration.banditCount):\n        if (numbers_of_selections[i] > 0):\n            average_reward = sums_of_reward[i] \/ numbers_of_selections[i]\n            ##delta_i = (math.sqrt(2 * math.log(observation.step+1) \/ \n            ##            numbers_of_selections[i]))\n            # Use a fixed 5% to get the upper_bound delta:\n            delta_i = math.sqrt( -1.0*math.log(0.05) \/ \n                                        (2*numbers_of_selections[i]))\n            # Use 1\/2 of the delta value\n            upper_bound = average_reward + 0.5*delta_i\n        else:\n            # haven't pulled this bandit yet - want to pull it\n            upper_bound = 1000.0 + np.random.rand()\n        if upper_bound > max_upper_bound and last_bandit != i:\n            max_upper_bound = upper_bound\n            bandit = i\n            last_bandit = bandit\n\n    numbers_of_selections[bandit] += 1\n\n    if bandit is None:\n        bandit = 0\n\n    return bandit","3c9d4611":"%%writefile vegas_slots.py\n\n# From https:\/\/www.kaggle.com\/sirishks\/pull-vegas-slot-machines\nimport numpy as np\nimport pandas as pd\nimport random, os, datetime\n\ntotal_reward = 0\nbandit_dict = {}\n\ndef set_seed(my_seed=42):\n    os.environ['PYTHONHASHSEED'] = str(my_seed)\n    random.seed(my_seed)\n    np.random.seed(my_seed)\n\ndef get_next_bandit():\n    best_bandit = 0\n    best_bandit_expected = 0\n    for bnd in bandit_dict:\n        expect = (bandit_dict[bnd]['win'] - bandit_dict[bnd]['loss'] + bandit_dict[bnd]['opp'] - (bandit_dict[bnd]['opp']>0)*1.5) \\\n                 \/ (bandit_dict[bnd]['win'] + bandit_dict[bnd]['loss'] + bandit_dict[bnd]['opp'])\n        if expect > best_bandit_expected:\n            best_bandit_expected = expect\n            best_bandit = bnd\n    return best_bandit\n\ndef multi_armed_probabilities(observation, configuration):\n    global total_reward, bandit_dict\n\n    my_pull = random.randrange(configuration['banditCount'])\n    if 0 == observation['step']:\n        set_seed()\n        total_reward = 0\n        bandit_dict = {}\n        for i in range(configuration['banditCount']):\n            bandit_dict[i] = {'win': 1, 'loss': 0, 'opp': 0}\n    else:\n        last_reward = observation['reward'] - total_reward\n        total_reward = observation['reward']\n        \n        my_idx = observation['agentIndex']\n        if 0 < last_reward:\n            bandit_dict[observation['lastActions'][my_idx]]['win'] = bandit_dict[observation['lastActions'][my_idx]]['win'] +1\n        else:\n            bandit_dict[observation['lastActions'][my_idx]]['loss'] = bandit_dict[observation['lastActions'][my_idx]]['loss'] +1\n        bandit_dict[observation['lastActions'][1-my_idx]]['opp'] = bandit_dict[observation['lastActions'][1-my_idx]]['opp'] +1\n        my_pull = get_next_bandit()\n    \n    return my_pull","14a4c937":"# Can play with the BayXC distribution here\n# to see effect of getting certain sequences of rewards.\n\nrewards = [0,0,0] #[1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1]\nweights = [0.97**iw for iw in range(len(rewards))]\nmean, std = get_expected_mean_std(weights, rewards, plotit=True)\ndf_factor = 0.97**len(rewards)\nprint(\"{:.3f} ({:.3f})  <--  mean, std\".format(mean, std))\nprint(\"{:.3f} ({:.3f})  <--  dfmean, dfstd\".format(df_factor*mean, df_factor*std))\n","3321d65a":"# Summary of some mean, std from above:\n# dfmean  (dfstd)     rewards\n#    possible values during all-1s scan (with variation from opponent)\n#  0.323 (0.229)   [0]        --0-->  0.239 (0.185)\n#  0.484 (0.216)   [1,0]      --0-->  0.382 (0.191)\n#  0.575 (0.189)   [1,1,0]    --0-->  0.476 (0.179)  --0-->  0.402 (0.164)\n#  0.628 (0.163)   [1,1,1,0]  --0-->  0.540 (0.163)  --0-->  0.468 (0.155)\n#                             --1-->  0.651 (0.140)\n#  0.738 (0.125)   [1,1,1,1]  --0-->  0.658 (0.140)  --0-->  0.583 (0.145)","8f47ded7":"%%writefile dfmeansV16_agent.py\n\nimport numpy as np\nimport random   # python module, not same as np.random\nimport time\nfrom scipy.stats import beta\nfrom scipy import integrate\nfrom matplotlib import pyplot as plt\nimport pandas as pd\n\n# Routine to calculate discount factor corrected distribution\ndef get_expected_mean_std(weights, rewards, plotit=False):\n\n    def pdf(p, weights, rewards, normalization=1):\n        s = 1\n        for weight, reward in zip(weights, rewards):\n            if reward == 1:\n                s *= (weight * p)\n            else:\n                s *= (1 - weight * p)\n        return s \/ normalization\n\n    normalization = integrate.quad(\n        lambda x: pdf(x, weights, rewards), 0, 1)[0]\n    first_order = integrate.quad(\n        lambda x: x * pdf(x, weights, rewards, normalization), 0, 1)[0]\n    second_order = integrate.quad(\n        lambda x: x * x * pdf(x, weights, rewards, normalization), 0, 1)[0]\n    \n    # Show a plot of the \"Bay-XC\" pdf along with the naive beta?\n    # REMOVED PLOT CODE\n    return first_order, (second_order - first_order ** 2) ** 0.5\n\ndef prob_histogram(observation, play_state):\n    # Display a histogram of the estimated probabilities during the run\n    # and write snapshot values to a csv file.\n    global band_means, band_stds, band_dfmeans, band_dfstds\n    global band_rewards\n    counts, binvals = np.histogram(band_means,range=(0.0,1.0),bins=20)\n    fig, ax = plt.subplots(figsize = (7,3))\n    ax.hist(band_means,bins=binvals,histtype='step',\n             color='lightblue', linewidth=4)\n    ax.hist(band_dfmeans,bins=binvals,histtype='step',\n             color='orange', linewidth=2)\n    title_str = (\"Play_state {} completed\".format(play_state) + \n                      \" at step {}\".format(observation.step) +\n                      \" with reward {}\".format(observation.reward))\n    ax.set_title(title_str)\n    plt.show()\n    # Write the values to a dataframe csv\n    frame = pd.DataFrame(band_dfmeans, columns=['dfmean'])\n    frame['dfstd'] = band_dfstds\n    frame['step'] = observation.step\n    frame['mean'] = band_means\n    frame['std'] = band_stds\n    # count the number of times * I * pulled the bandit\n    npulls = np.zeros(len(band_rewards))\n    for iband in range(len(band_rewards)):\n        npulls[iband] = len(band_rewards[iband])\n    frame['npulls'] = npulls\n    \n    print(\"Saving data frame: play_state_{}.csv\".format(play_state))\n    print(frame.head(5),\"\\n\\n\\n\")\n    frame.to_csv(\"play_state_{}.csv\".format(play_state),\n                index_label='bandit')\n\ndef dfmeans_agent(observation, configuration):\n    # A 7-state scheme is used:\n    # 1,2) rand pull all 1s, mix w\/never pulled\n    # 3) dfmean, mix w\/(1\/3) 1 zeros pull\n    # 4) dfmean+0.5dfstd, mix w\/(1\/4) 2 zeros or 30-50,7\n    # 5) dfmean+0.5*dfstd, mix w\/(1\/6) dfstd > 0.06\n    # 6) dfmean, require mean>0.20\n    # 7) dfmean, require mean>0.25\n    # The dfmean\/dfstd are calculated using the BayXC function of Xin Cui.\n    \n    global play_state, last_reward, my_last, other_last, other_lastlast\n    # Keep track of the bandit's actions, etc.\n    global band_pulls, band_rewards, band_rewdfs\n    global band_means, band_stds, band_dfmeans, band_dfstds\n    # \"figure of merits\" - this is used for selection in different ways\n    global band_foms\n\n    df = 0.97   # the discount factor\n    nband = configuration.banditCount\n    \n    # - - - - -  Setup - - - - -\n    # Set things up if this is the start:\n    if observation.step == 0:\n        play_state = 1\n        last_reward = observation.reward\n        got_reward = 0\n        # current and previous bandit I pulled\n        my_action = -10\n        my_last = -20\n        # current and last two bandits pulled by the other player\n        other_action = -1\n        other_last = -2\n        other_lastlast = -3\n        # Setup values related to the bandits:\n        band_pulls = np.zeros(nband)\n        # Initially have no rewards info on the bandits,\n        # setup the list of lists with flag values.\n        band_rewards = [[-1.0]] * nband\n        band_rewdfs = [[1.0]] * nband\n        # Initial values of the means and stds\n        band_means = np.zeros(nband)\n        band_stds = np.zeros(nband)\n        band_dfmeans = np.zeros(nband)\n        band_dfstds = np.zeros(nband)\n        band_foms = np.zeros(nband)\n    else:\n        # - - - - - Digest new information from the pull - - - - -\n        # Here we'll get the results from doing the last step's pull\n        # and do basic updating and processing\n        my_index = observation.agentIndex\n        my_action = observation.lastActions[my_index]\n        got_reward = int(observation.reward - last_reward)\n        last_reward = observation.reward  # save for next time\n        # the other player\n        other_action = observation.lastActions[1 - my_index]\n        # Update the bandit reward info\n        if band_rewards[my_action][0] > -0.5:\n            # just add on this reward\n            band_rewards[my_action] = band_rewards[my_action] + [got_reward]\n            band_rewdfs[my_action] = (band_rewdfs[my_action] +\n                                  [df**band_pulls[my_action]])\n        else:\n            # This is the first reward I've gotten for this bandit -\n            # set the first rewards and rewdfs appropriately:\n            band_rewards[my_action] = [got_reward]\n            band_rewdfs[my_action] = [df**band_pulls[my_action]]\n                \n        # Update the pulls\n        band_pulls[my_action] += 1\n        band_pulls[other_action] += 1\n        \n        # Update the df-corrected mean and std for my selected bandit\n        mean, std = get_expected_mean_std(band_rewdfs[my_action],\n                            band_rewards[my_action], plotit=False)\n        # We're interested in the high-side std: since the dist can be skewed,\n        # clip the std to be at most 0.5*(1.0 - mean)\n        std = min([std, 0.5*(1.0 - mean)])\n        # save the estimated intrinsic(initial) mean and std:\n        band_means[my_action] = mean\n        band_stds[my_action] = std\n        # set the new effective dfmeans, dfstds value for this bandit\n        band_dfmeans[my_action] = mean*df**band_pulls[my_action]\n        band_dfstds[my_action] = std*df**band_pulls[my_action]\n        # and adjust the other's selected bandit (if not same as mine)\n        if other_action != my_action:\n            band_dfmeans[other_action] *= df\n            band_dfstds[other_action] *= df\n            \n        # Assume\/include an opponent reward if\n        # play_state 1,2,3 and opponent pulled same one again and pulls <= 10\n        if( (play_state <= 3) and (other_action == other_last) and\n                  (other_action != my_action) and (band_pulls[other_action] <= 10)):\n            # Same code as above but for other_action; could be the first reward too.\n            if band_rewards[other_action][0] > -0.5:\n                band_rewards[other_action] = band_rewards[other_action] + [1]\n                band_rewdfs[other_action] = (band_rewdfs[other_action] +\n                                  [df**(band_pulls[other_action]-2)])\n            else:\n                band_rewards[other_action] = [1]\n                band_rewdfs[other_action] = [df**(band_pulls[other_action]-2)]\n            mean, std = get_expected_mean_std(band_rewdfs[other_action],\n                            band_rewards[other_action], plotit=False)\n            std = min([std, 0.5*(1.0 - mean)])\n            band_means[other_action] = mean\n            band_stds[other_action] = std\n            band_dfmeans[other_action] = mean*df**band_pulls[other_action]\n            band_dfstds[other_action] = std*df**band_pulls[other_action]\n            # Could report this... but it's not a pull decision so keep it in background.\n            ##print(\"Step\",observation.step,\": assume\/add opponent's reward for\", \n            ##      other_action, \", dfmean =\",band_dfmeans[other_action])\n        \n    # - - - - - Decide on the next pull - - - - -\n    # OK, now with the new information digested,\n    # decide on a value for the next pull_bandit\n    \n    # Look for possible poaching opportunities\n    poach_it = False\n    if ((other_action == other_last) and\n                (other_last == other_lastlast)):\n        # play_state 1 poaching\n        if play_state == 1:\n            # Poach it if I have not gotten a 0 reward from this bandit\n            if (min(band_rewards[other_action]) > 0.5):\n                poach_it = True\n        # later play_state poaching\n        if play_state == 2:\n            # Only poach in play state 2 if the mean is high\n            pass  # not using state 2.\n        if play_state == 3:\n            # play state 3: require a minimal dfmean value\n            if (band_dfmeans[other_action] > 0.35):\n                poach_it = True\n        if play_state >= 4:\n            # play state 4 or later: require a minimal dfmean value\n            if (band_dfmeans[other_action] > 0.25):\n                poach_it = True\n                \n    # Do the apropriate state's action\n    if poach_it:\n        pull_bandit = int(other_action)\n        print(\"Step\", observation.step, \": poaching\",other_action,\n                 \"w\/ dfmean =\",band_dfmeans[other_action])\n        poach_it = False\n    #\n    elif play_state == 1:  # 1,2) rand pull all 1s, mix w\/never pulled\n        # --- DO THIS STATE'S EVAL AND PULLS:\n        # Recalculate what's available each time, so don't need to use global band_foms\n        # Find all bandits not yet pulled: they have reward as [-1].\n        not_pulled = np.zeros(nband) + 0.01*np.random.rand(nband)\n        for iband in range(nband):\n            if (band_rewards[iband][0] < -0.5):\n                not_pulled[iband] = 1.0 + 0.01*np.random.rand()\n        # Find all bandits that currently have all 1s\n        have_all1s = np.zeros(nband) + 0.01*np.random.rand(nband)\n        for iband in range(nband):\n            if min(band_rewards[iband]) > 0.5:\n                have_all1s[iband] = 1.0 + 0.01*np.random.rand()\n        #\n        # There are 4 cases for the two types of selected bandits\n        # depending on which are available.\n        # When exploiting, if the opponent's last pull was one I want\n        # to exploit, then do it now.\n        #   Explore is only option\n        if (max(not_pulled) > 0.5) and (max(have_all1s) < 0.5):\n            pull_bandit = int(np.argmax(not_pulled))  # explore\n            print(\"Step\",observation.step, \": Explore\",pull_bandit)\n        #   Exploit is only option\n        elif (max(not_pulled) < 0.5) and (max(have_all1s) > 0.5):\n            if have_all1s[other_action] > 0.5:\n                pull_bandit = int(other_action)  # exploit opponent\n                print(\"Step\", observation.step, \": +exploit other's choice\",\n                      other_action)\n            else:\n                pull_bandit = int(np.argmax(have_all1s))  # exploit\n                print(\"Step\",observation.step, \": +exploit\",pull_bandit)\n        #   Both available: Exploit, but Explore every nth time\n        elif (max(not_pulled) > 0.5) and (max(have_all1s) > 0.5):\n            if (observation.step % 3) == 0:\n                pull_bandit = int(np.argmax(not_pulled))  # explore\n                print(\"Step\",observation.step, \": Explore\",pull_bandit)\n            else:   \n                if have_all1s[other_action] > 0.5:\n                    pull_bandit = int(other_action)  # exploit opponent\n                    print(\"Step\", observation.step, \": +exploit other's choice\",\n                          other_action)\n                else:\n                    pull_bandit = int(np.argmax(have_all1s))  # exploit\n                    print(\"Step\",observation.step, \": +exploit\",pull_bandit)\n        #   Neither available...\n        else:\n            # --- GO TO NEXT STATE?:\n            # Here when there are none to explore and none to exploit,\n            # time to go to the next state. Make a choice based on dfmeans:\n            pull_bandit = int(np.argmax(band_dfmeans))\n            prob_histogram(observation, play_state)\n            play_state = 3            \n    #\n    elif play_state == 3:  # 3) dfmean, mix w\/(1\/3) 1 zeros pull\n        # --- DO THIS STATE'S EVAL AND PULLS:\n        # find ones with 1 or less 0 rewards\n        band_foms = np.zeros(nband) + 0.01*np.random.rand(nband)\n        for iband in range(nband):\n            # pull if one or less 0 rewards\n            if ( (len(band_rewards[iband]) - sum(band_rewards[iband])) < 2):\n                    band_foms[iband] = 1.0 + 0.01*np.random.rand()\n        # alternate with pulling the highest dfmean\n        if (observation.step % 3 == 0) and (max(band_foms) > 0.5):\n            pull_bandit = int(np.argmax(band_foms))\n        else:\n            pull_bandit = int(np.argmax(band_dfmeans))\n        # --- GO TO NEXT STATE?:\n        if max(band_foms) < 0.5:\n            prob_histogram(observation, play_state)\n            play_state = 4\n    #\n    elif play_state == 4:  # 4) dfmean+0.5dfstd, mix w\/(1\/4) 2 zeros or 30-50,7\n        # --- DO THIS STATE'S EVAL AND PULLS:\n        # find ones with 2 or less 0 rewards\n        #  OR\n        # mean between 0.30, 0.50 with low number of pulls\n        band_foms = np.zeros(nband) + 0.01*np.random.rand(nband)\n        for iband in range(nband):\n            if ( \n                # pull if two or less 0 rewards\n                ( ((len(band_rewards[iband]) - sum(band_rewards[iband])) < 3) and\n                       (band_means[iband] < 0.60) )\n                or\n                # mean between 0.30, 0.50 with low number of pulls\n                ( (band_means[iband] > 0.30) and (band_means[iband] < 0.50)\n                        and (len(band_rewards[iband]) <= 7) ) \n                ):\n                band_foms[iband] = 1.0 + 0.01*np.random.rand()\n        # alternate with pulling the highest dfmean\n        if observation.step % 4 == 0:\n            pull_bandit = int(np.argmax(band_foms))\n        else:\n            pull_bandit = int(np.argmax(band_dfmeans + 0.5*band_dfstds))\n        # --- GO TO NEXT STATE?:\n        if max(band_foms) < 0.5:\n            prob_histogram(observation, play_state)\n            play_state = 5\n    #\n    elif play_state == 5:  # 5) dfmean+0.5*dfstd, mix w\/(1\/6) dfstd > 0.06\n        # --- DO THIS STATE'S EVAL AND PULLS:\n        if (observation.step % 6 == 0) and (max(band_dfstds) > 0.06):\n            # do a bandit that has dfstd > 0.06\n            pull_bandit = int(np.argmax(band_dfmeans * \n                                       (band_dfstds > 0.06)))\n        else:\n            pull_bandit = int(np.argmax(band_dfmeans + 0.5*band_dfstds +\n                                   0.02*np.random.rand(nband)))\n        # --- GO TO NEXT STATE?:\n        if observation.step > 1500:\n            prob_histogram(observation, play_state)\n            play_state = 6\n    #\n    elif play_state == 6:  # 6) dfmean requiring mean>0.20\n        # --- DO THIS STATE'S EVAL AND PULLS:\n        pull_bandit = int(np.argmax(band_dfmeans * (band_means > 0.20)))\n        # --- GO TO NEXT STATE?:\n        # Add a state at 1800 to get the histogram\n        if observation.step > 1750:\n            prob_histogram(observation, play_state)\n            play_state = 7\n    #\n    elif play_state == 7:  # 7) dfmean requiring mean>0.25\n        # --- DO THIS STATE'S EVAL AND PULLS:\n        pull_bandit = int(np.argmax(band_dfmeans * (band_means > 0.25)))\n        # --- GO TO NEXT STATE?:\n        if observation.step == 1998:\n            prob_histogram(observation, play_state)\n            # no next state ;-)\n    #\n    else:\n        print(\"Shouldn't get to this else.\")\n    \n    # Update past actions\n    my_last = 0 + my_action\n    other_lastlast = 0 + other_last\n    other_last = 0 + other_action\n    \n    return pull_bandit\n","561e5fe1":"# Show all the .py (agent) files in the current dir:\n!ls *.py","23e44f9b":"# Choose two agents to run\n# Detailed information of the run is saved...\nname1 = \"dfmeansV16_agent\"\nname2 = \"vegas_slots\"  #\"ucb_half5percent\"\nenvrun = env.run([name1+\".py\", name2+\".py\"])","0668e2d1":"# Histogram of the original probabilities\ninitial_probs = (np.array(envrun[0][0].observation.thresholds))\/100.0\ncounts, binvals = np.histogram(initial_probs,range=(0.0,1.0),bins=20)\nfig, ax = plt.subplots(figsize = (7,3))\nax.hist(initial_probs,bins=binvals,histtype='step',color='lightblue',linewidth=4)\nplt.show()","d558d51c":"# Fancy output...\n##env.render(mode=\"ipython\", width=600, height=250)","6a6293df":"# The final scores:\nmy_score = envrun[1999][0].reward\nother_score = envrun[1999][1].reward\nprint(name1,\":\",my_score, \"  \", name2,\":\", other_score, \"   Sum :\",\n      my_score+other_score,\n      \"\\nPercent greater: \",100.0*(my_score-other_score)\/other_score)","91ea1a9d":"# Plot the player choices (actions) and rewards\np1_actions = []\np2_actions = []\np1_rewards = []\np2_rewards = []\nfor istep in range(1,2000):\n    envrun_istep = envrun[istep]\n    p1_actions = p1_actions + [envrun_istep[0].action]\n    p2_actions = p2_actions + [envrun_istep[1].action]\n    p1_rewards = p1_rewards + [envrun_istep[0].reward]\n    p2_rewards = p2_rewards + [envrun_istep[1].reward]\n    \nfig, ax = plt.subplots(figsize=(7,4))\nplt.plot(p1_actions,'o',markersize=1)\nplt.plot(np.array(p1_rewards)\/6,'o',markersize=1)\nplt.xlabel(\"Step\")\nplt.ylabel(\"Selected Bandit  and  Reward\/6\")\nplt.title(name1)\nplt.show()\n\nfig, ax = plt.subplots(figsize=(7,4))\nplt.plot(p2_actions,'o',markersize=1)\nplt.plot(np.array(p2_rewards)\/6,'o',markersize=1)\nplt.xlabel(\"Step\")\nplt.ylabel(\"Selected Bandit  and  Reward\/6\")\nplt.title(name2)\nplt.show()\n\nfig, ax = plt.subplots(figsize=(7,4))\nplt.plot(np.array(p1_rewards) - np.array(p2_rewards),'o',markersize=1)\nplt.xlabel(\"Step\")\nplt.ylabel(\"Player 1 Score -  Player 2 Score\")\nplt.title(name1+\"  -  \"+name2)\nplt.show()","8b081372":"# The cute plot from Aatif Fraz\n# Select the player to make the plot for\niplayer = 0   # 0, 1  = player1 player2\n# I like the xkcd but is hurting my brainif I look at it too much ;-)\n##with plt.xkcd():\nif True:\n    fig, ax = plt.subplots(figsize=(11,8))\n    # Will put the actual played values on the top of the plot, at end.\n    # Pulled_probs for an optimum agent playing against a random player\n    # grid lines at every 100 steps\n    for istep in range(0,2100,100):\n        ax.plot([istep,istep],[0.0,100.0],c='lightgray') \n    # percentile curves\n    ax.plot([x for x in range(2000)], \n            [max(envrun[i][0].observation.thresholds) for i in range(2000)],\n            'lime', label='Max Chance of getting a candy at the step')\n    #\n    ax.plot([x for x in range(2000)], \n            [(np.sort(envrun[i][0].observation.thresholds))[-10]\n                                                 for i in range(2000)],\n            'C1', label='90-th %tile Chance of getting a candy')\n    #\n    ax.plot([x for x in range(2000)], \n            [(np.sort(envrun[i][0].observation.thresholds))[-40]\n                                                 for i in range(2000)],\n            'C1', label='60-th %tile Chance of getting a candy')\n    #\n    ax.plot([x for x in range(2000)], \n            [(np.sort(envrun[i][0].observation.thresholds))[-70]\n                                                 for i in range(2000)],\n            'C1', label='30-th %tile Chance of getting a candy')\n    #\n    # The actual choices\n    ax.scatter([x for x in range(2000)], \n               [envrun[i][0].observation.thresholds[envrun[i][iplayer].action] \n                for i in range(2000)], s=8,\n               label=\"Chance of getting a \\n candy based on the Agent's selection\")\n    #\n    ax.set_title(\"Chances of getting a candy:  \"+[name1,name2][iplayer]+\n                \"  vs  \"+[name1,name2][1-iplayer])\n    ax.legend()\n    plt.xlabel(\"Game Step\")\n    plt.ylabel(\"Threshold of pulled bandit\")\n    plt.ylim(-1.0,101.0)\n    plt.savefig('fraz_plot.png')","26314912":"# Snapshots saved -- Only saved by the dfmeansVNN_agent !\n!ls *.csv","0f360acd":"# Show the snapshot frame columns\n# * Note: the npulls is the number of times that I pulled the bandit, \n#         i.e., the number of rewards used to determine its mean, etc.\n#         (It's not band_pulls[] which counts mine and opponents pullings.)\nsnap_name = \"play_state_5\"\nframe = pd.read_csv(snap_name + \".csv\")\nprint(frame.head(5))","a444b4bf":"# The initial probabilites for this run\ninitial_probs = (np.array(envrun[0][0].observation.thresholds))\/100.0\n\n# Based on the ideal simulation at beginning of notebook,\n# find the sequence of best-pulls for two optimal players.\ncurrent_probs = 1.0 * initial_probs\npulled_probs = []\nfor ip in range(4000):\n    # choose the highest prob one\n    ihigh = np.argmax(current_probs)\n    pulled_probs = pulled_probs + [current_probs[ihigh]]\n    current_probs[ihigh] *= 0.97\n    \nprint(\"Expected rewards per player = {:.1f}\".format(sum(pulled_probs)\/2.0))","ee9cb0a0":"# Plot all the snapshots\nfor isnap in range(1,8):\n    snap_name = \"play_state_{}\".format(isnap)\n    # This allows me to leave out some of the play states...\n    try:\n        frame = pd.read_csv(snap_name + \".csv\")    \n        this_step = frame.step[0]\n        frame_probs = (np.array(envrun[this_step][0].observation.thresholds))\/100.0\n\n        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12,6))\n        fig.suptitle(\"Snapshot after \"+snap_name+\n             \" - through step {}\".format(this_step),size=16)\n\n        max_xy = max(frame_probs)\n        ax1.plot([0.,max_xy],[0.,max_xy],'lightblue')\n        ax1.plot([0.,0.7*max_xy],[0.,max_xy],'--',c='lightblue')\n        ax1.plot([0.,max_xy],[0.,0.7*max_xy],'--',c='lightblue') \n        # show the probability level we'd ideally be pulling (see previous cell)\n        ax1.plot([0.1*max_xy, 0.90*max_xy],\n                [pulled_probs[2*this_step],pulled_probs[2*this_step]],'pink',alpha=0.4)\n        # show the 30, 60, 90, and 100 %-tile of actual pull possibilities\n        for xptile in [30, 60, 90, 100]:\n            valtile = (np.sort(frame_probs))[xptile-1]\n            clrtile = 'orange'\n            if xptile == 100:\n                clrtile = 'lightgreen'\n            ax1.plot([valtile,valtile],[0.1*max_xy,0.85*max_xy],\n                 clrtile,alpha=0.4,linewidth=2)\n        ax1.scatter(frame_probs,frame['dfmean'],s=10)\n        for iband in range(len(frame)):\n            flag_len = 0.25*frame['dfstd'][iband]; flag_name = \"std (0.25*dfstd)\"\n            ##flag_len = 0.005*np.sqrt(frame['npulls'][iband]); flag_name = \"pulls\"\n            ax1.plot([frame_probs[iband],frame_probs[iband]],\n                [frame['dfmean'][iband],\n                 frame['dfmean'][iband]+flag_len],\n                   c='gray')\n        ax1.set_title(\"Flag represents {}\".format(flag_name))\n        ax1.set_ylim(-0.01,)\n        ax1.set_xlabel(\"Actual Probability of Bandit this step\")\n        ax1.set_ylabel(\"Estimated Probability of Bandit this step\")\n\n        ax2.scatter(initial_probs,frame['mean'],s=10)\n        ax2.plot([0.,1.],[0.,1.],'lightblue')\n        ax2.plot([0.,1.0],[0.,0.7],'--',c='lightblue')\n        ax2.plot([0.,0.7],[0.,1.0],'--',c='lightblue')\n        for iband in range(len(frame)):\n            # std flags\n            ##flag_len = 0.5*frame['std'][iband]; flag_name = \"std\"\n            # number of pulls\n            flag_len = 0.01*np.sqrt(frame['npulls'][iband]); flag_name = \"pulls (0.01*sqrt(pulls))\"\n            ax2.plot([initial_probs[iband],initial_probs[iband]],\n                [frame['mean'][iband],\n                 frame['mean'][iband]+flag_len],\n                   c='gray')\n        ax2.set_title(\"Flag represents {}\".format(flag_name))\n        ax2.set_ylim(-0.01,1.01)\n        ax2.set_xlabel(\"Actual Starting Threshold of Bandit\")\n        ax2.set_ylabel(\"Estimated Starting Threshold of Bandit\")\n\n        plt.show()\n    except:\n        pass","0896ec53":"# How much variation in the score is expected from the random threshold selection?\n\n# Randomly select a set of thresholds and see what the expected optimal-play score is.\n# How the threshold values are setup in the environment, from:\n# https:\/\/github.com\/Kaggle\/kaggle-environments\/blob\/b043a9588ef14018816a47b93b841cbcbd9ba906\/kaggle_environments\/envs\/mab\/mab.py#L97\n\n# Using the normal random module\ndef sample():\n    \"\"\"Obtain a value between 0 and sampleResolution to check against a bandit threshold.\"\"\"\n    return random.randint(0, 100)\n\nngames = 100\ngame_scores = []\nfor igame in range(ngames):\n    start_thresholds = [sample() for _ in range(100)]\n    initial_probs = (np.array(start_thresholds))\/100.0\n    current_probs = 1.0 * initial_probs\n    pulled_probs = []\n    for ip in range(4000):\n        # choose the highest prob one\n        ihigh = np.argmax(current_probs)\n        pulled_probs = pulled_probs + [current_probs[ihigh]]\n        current_probs[ihigh] *= 0.97\n    this_score = sum(pulled_probs)\/2.0\n    game_scores = game_scores + [this_score]\n    ##print(\"Expected rewards per player = {:.1f}\".format(this_score)\n","5c0f85a2":"# Show a histogram of the game scores\n# fyi, I ran for 5000 games and the distribution looked normal with mean ~ 640, std ~ 40\nif True:\n    counts, binvals = np.histogram(game_scores,range=(500,800),bins=30)\n    fig, ax = plt.subplots(figsize = (7,3))\n    ax.hist(game_scores,bins=binvals,histtype='step',\n             color='lightblue', linewidth=4)\n    ax.set_xlabel(\"Player's score\")\n    ax.set_ylabel(\"Number of games\")\n    title_str = (\"Distribution of Player's Score, {} Games\".format(ngames))\n    ax.set_title(title_str)\n    plt.show()","28070491":"# Structure of the env.run output - keep this at the end for reference.\n# envrun[istep][iplayer]\n#   istep = 1, 1999  (0 doesn't count?)\n#   iplayer:  0 or 1\n# At each of these there is a dictionary:\n#\n# iplayer=0:\n#  {'action': 98, 'reward': 100, 'info': {},\n#   'observation':\n#       {'remainingOverageTime': 60,\n#        'step': 199,\n#        'agentIndex': 0,\n#        'reward': 100,\n#        'lastActions': [98, 90],\n#        'thresholds': [ . . . ]},\n#   'status': 'ACTIVE'}\n#\n# iplayer=1:\n#  {'action': 90, 'reward': 102, 'info': {},\n#   'observation':\n#       {'remainingOverageTime': 60, 'agentIndex': 1, 'reward': 102},\n#   'status': 'ACTIVE'}","054ba8e4":"# More Candy !  -  Santa 2020","91b5f904":"## Run the simulation","73eb42c8":"### My submission(s) and a random agent for comparison","0d0949e0":"### Kludgy maximum candies calculation","f9ae3e5b":"### Dairy\n\n**(v1)**: 13-Dec-2020 Read through discussion posts, [Lilian Weng's MAB post](https:\/\/lilianweng.github.io\/lil-log\/2018\/01\/23\/the-multi-armed-bandit-problem-and-its-solutions.html), [Wikipedia](https:\/\/en.wikipedia.org\/wiki\/Multi-armed_bandit), etc. The basic situation is not so tricky but the \"Simulations Platform\" is a bit mysterious... Try different code demos related to the RPS... <br>\n14,15-Dec Copy\/play-with Xin Cui's [\"Weighted expected mean and std\"](https:\/\/www.kaggle.com\/xincuimath\/weighted-expected-mean-and-std-estimation), (\"BayXC\") vs Beta distribution. Make simple \"max candies\" model\/prediction, 1292 combined scores.\nLook through [Simple MAB](https:\/\/www.kaggle.com\/ilialar\/simple-multi-armed-bandit) notebook (nice beta plot - add a plot to BayXC stuff). <br>\nMade a simple \"scan, repeat\" agent written into submission.py. Is that what I need to submit? Yes.<br>\n**(v2)** 15-Dec Well, my scan-repeat starts with 0, 1, 2, etc. *and so do lots of other agents*. So subtract a small random amount from the starting all 1s so that argmax will not see them as equal (and go in order) and instead they will be in a random (fixed) scan order. <br>\nThe skill rating is kind of strange... For reference, submit the random agent from v2. I expect this to rate lower than my other ones:<br>\n**(v3)** Setup global variables to keep track of bandit properties (pulls, rewards and their dfs, predicted mean prob with discount factor included, i.e., the prob we expect to have if it is pulled. Set all bandits to having on reward and one non-reward observation giving a mean of 0.5. Add some noise to those values to reduce sequential selection of bandits. Pull the bandit with the largest dfmean value.<br>\n**(v4)** Noticed that there is a Santa 2020 submission.py that implements ucb_agent(). Compared to that my \"select largest dfmean value\" (v3) agent zooms ahead for the first half-ish of the pulls, then falls behind... THe bandit-pulled vs step number plot shows a clear change in behavior around 700 steps, my problem is in the \"end game\"? Modified my agent to use only observed rewards to calculate dfmean (removed starting rewards of [1,0]), after first pass select bandit with the highest mean+std.<br>\n**(v5)** Re-structure the code slightly adding a \"state\" value to help implement a strategy. Found\/fixed some errors in how the reward history was filled in. In the first \"scan\" state continue pulling a bandit if a reward is obtained up to a max of 4 rewards. After that chose the bandit with the highest df-ed mean + std. <br>\n**(v6)** Nice discussion posts in [Some thoughts about the rating system](https:\/\/www.kaggle.com\/c\/santa-2020\/discussion\/204741) and Aatif Fraz' notebook with [Chances of getting a candy](https:\/\/www.kaggle.com\/aatiffraz\/a-beginner-s-approach-to-performance-analysis) plot - add that below, along with 90th, 50th percentiles.  Clean up the structure to be more clearly state-based. The V6 agent has 3 states: i) bandits are scanned initially in random order and each bandits is pulled until they produce a 0, or 4 rewards in a row; ii) bandits with dfmean > 0.5 are pulled until there are none > 0.5; finally iii) the bandit with maximum dfmean+dfstd is pulled.<br>\n**(v7)** Don't like that opponent might spy on my repeat pulls, so try to arrange things so that each pull is randomly selected from bandits with some criteria.\nGo to a 5-state scheme: i) each pulled once; ii) all-1s are pulled at random until no all-1s; iii) dfmean > 0.5 pulled; iv) [0],[1,0],[0,1],etc pulled alternately w\/dfmean; and v) max dfmeans+dfstds pulled.<br>\n**(v8)** Use foms as flag instead of dfmeans in state 1; pull <= 2 zero rewards in state iv (more explore); add a 6-th \"endgame\" state starting at step 1200 (require mean>0.25); poach opponents bandit if played 3 times in a row.<br>\n**(v9)** Checkout MAB in chapter 2 of [Reinforcement Learning](http:\/\/incompleteideas.net\/book\/RLbook2020.pdf). Cleaned up the states code a bit: keep all the state's action in one place (no more 'setup for next state').\nPlot histograms of means and dfmeans after each state - showed where exploration was needed. Some changes for version 9: added one repeat and possible poaching in state 1; in state 3, 2\/3 time do best dfmean and mix in 1\/3-time pull bandits that have only 1 zero reward; state 4 slight adjusts; state 5 dfmean+dfstd but mix in 1\/10-time explore bandits with mean < 0.25; endgame state 6: pull highest dfmean but only among ones with mean>0.20.<br>\n**(v10)** Clean up some more, nicer histogram plots, add a 7th state to get hist plot at step 1600.  Added a \"ucb_half5percent\" agent file to do UCB \"correctly\", though the delta value is very large compared with BayXC std, so use 0.5 times delta. <br>\nSome adjustment of the poaching criteria, but otherwise no intentional, significant change to the submitted agent's operation.<br>\n**(v11)** Cleaning up things. Slight changes to scheme: state 2 copy opponent if I was going to do that one anyway; state 4 is 0.30 to 0.50 and <= 7; state 5 < 0.35; and state 7 >0.25. Added plots of estimated and actual thresholds for all the snapshots. <br>\n**(v12)** Little stuff: added 30, 60, 90, 100 %-tile lines to snapshot plots. Added the [\"Vegas Slots\"](https:\/\/www.kaggle.com\/sirishks\/pull-vegas-slot-machines) agent as one to compare with mine. Considering [adding bandit information based on opponent's pulls](https:\/\/www.kaggle.com\/c\/santa-2020\/discussion\/203391)... added it only in my play states 2 and 3 -- still high probabilities present so OK to assign reward = 1 based on an opponent's repeat pull, might slightly over-estimate some bandit's thresholds. <br>\n**(v13)** Slight changes, mostly in state 5: dfmean+0.5\\*dfstd, mix w\/(1\/10) dfstd > 0.10 <br>\n**Commit following versions running against the \"vegas slots\" agent.** <br>\n**(v14)** Another tweak to state 5: dfmean+0.5\\*dfstd, w\/(1\/6) dfstd > 0.06. <br>\n**(v15)** Made a histogram of simulated player's score to show variation due to threshold randomness (at end of nb.) Combined states 1,2 and slight changes to the state 1,2 poaching and the other-reward including. Expect about the same performance. <br>\n**(v16)** Had left out the \"pull other's choice if it's among ones I want to exploit\" logic that was in v14 state 2; added it to the combined states 1,2. States 1,2 prints out what happened at each of its steps; changed the Explore-every-nth value from 2 to 3.\n\n### Submissions\nSubmitted agents, from newest to oldest; ratings as of most recent submission.<br>\n(v16) Submission 1xxxxxxx **???.?** as v15, state 1,2 includes \"other pull\" as was in v14. <br>\n(v15) Submission 1xxxxxxx **???.?** as previous v14, combined states 1 and 2.<br>\n(v14) Submission 18887673 **709.5** 7 state as v13-v11 but state 5: dfmean+0.5\\*dfstd, w\/6 dfstd > 0.06 <br>\n(v13) Submission 18876229 **678.1** 7 state as v12,v11 but state 5: dfmean+0.5\\*dfstd, w\/10 dfstd > 0.10 <br>\n(v12) Submission 18860127 **700.0** 7 state as v11, with adding opponent assumed rewards.<br>\n(v11) Submission 18846489 **667.9** 7 state: rand pull each (1 repeat); rand pull first 0; dfmean w\/1-zero; dfmean+0.5dfstd, w\/ 2-zeros or 30-50,7; dfmean+dfstd, w\/mean<0.35; dfmean w\/mean>0.20, dfmean w\/mean>0.25.<br>\n(v10) Submission 18843273 **658.3** 6(7) state: rand pull each (1 repeat); rand pull first 0; dfmean w\/1-zero; dfmean+0.5dfstd, w\/ 2-zeros or 35-55; dfmean+dfstd, w\/mean<0.25; dfmean w\/mean>0.20.<br>\n(v10) Submission 18843269 **599.5** UCB half-delta(5%) - modified Santa 2020 UCB.<br>\n(v9) Submission 18820215 **665.1** 6 state: rand pull each (1 repeat); rand pull first 0; dfmean w\/1-zero; dfmean+0.5dfstd, w\/2-zeros; dfmean+dfstd, w\/mean<0.25; dfmean w\/mean>0.20.<br>\n(v8) Submission 18787619 **665.4** 6 state: pull each, pull all-1s, dfmean>0.5, pull <= 2 zeros mix w\/dfmean, dfmean+dfstd, dfmean w\/mean >0.25<br>\n(v7) Submission 18772801 **583.9** 5 state: pull each, pull all-1s, dfmean>0.5, pull [0],[1,0],[0,1], dfmean+dfstd<br>\n(v6) Submission 18758336 **631.2** 3 state: scan\/pull up-to-4, dfmean>0.5, dfmean+dfstd<br>\n(v5) Submission 18705910 **629.0** scan\/pull up-to-4 times, dfmean+dfstd<br>\n(v4) Submission 18689974 **592.4** scan, true mean, select mean+std<br>\n(v3) Submission 18675001 **502.2** select largest dfmean value<br>\n(v2) Submission 18668271 **302.5** random agent\" for reference<br>\n(v2) Submission 18667842 **396.6** scan, repeat, shuffled order<br>\n(v1) Submission 18667244 **394.2** scan, repeat, 0,1,2,3...\"<br>","3a02c795":"## The MAB environment etc.","621ae305":"### Look at saved snapshots","718af6ae":"### Estimating p_i from observations"}}