{"cell_type":{"2aa05d3c":"code","8dcf5a54":"code","f1b8f88c":"code","57f839c1":"code","3f2fb807":"code","8a13d00f":"code","69911003":"code","551306ff":"code","b8185901":"code","8c0fbf99":"code","03a4745f":"code","78b420a2":"code","b1d10a9c":"code","2329460b":"code","b5a388e7":"code","2555c953":"code","4adf131d":"code","3efd14b6":"code","069e6c71":"code","e61bb83a":"code","a7573de7":"code","7c08ca36":"code","3772857b":"code","181d77a1":"code","da7065df":"code","c5f336f8":"code","c380d2ea":"code","247ec956":"code","9b120b7d":"code","be7e2269":"code","9c538485":"code","c8b0b14d":"code","2e3a1fba":"code","f28d292d":"code","8c558dcc":"code","815bf6ae":"code","61d60cfe":"code","3993ddc5":"code","70d463cf":"code","e19caff2":"code","2c2b4a18":"code","b4a22b42":"code","193260a7":"code","bd394057":"code","fe81caff":"code","7faff28d":"code","2861ee18":"code","6b77ab52":"code","fcc2a5c5":"code","3aaa8bd9":"code","64aae00e":"code","78d15a86":"code","7cac67c8":"code","4917e596":"code","a010db8b":"code","65e9e48d":"code","fb0c2100":"code","a55eb233":"code","d103d455":"code","89c102a4":"code","93b99c8b":"code","c9f849ef":"code","a11eb4c5":"code","fe112059":"code","aedf7467":"code","e224cee7":"code","fc2bd7f4":"code","1b2cdc9a":"code","b33e9327":"code","7e0ab918":"code","90849634":"code","0785d4cc":"code","34f29262":"code","a9511915":"markdown","0f0aadb3":"markdown","9ef8f2d8":"markdown","49ea4bef":"markdown","e7e1bec7":"markdown","3ac042fc":"markdown","263d82fb":"markdown","ec674de9":"markdown","207cced9":"markdown","141c6270":"markdown","8deefbbb":"markdown","7af644db":"markdown","0f340dd8":"markdown","d52f24bd":"markdown","13984410":"markdown","c348c8cb":"markdown","df846d59":"markdown","57cdd20e":"markdown","371a4437":"markdown","bf8fa155":"markdown","c423d1b9":"markdown","3850d88c":"markdown","e3400f10":"markdown","3d359fa5":"markdown","9ddec64c":"markdown","18f132f7":"markdown","9a64c233":"markdown","0137f153":"markdown","a674d703":"markdown","a968141e":"markdown","af62044a":"markdown","b5b99757":"markdown","9ebb0ff4":"markdown","344bdf7a":"markdown","fe694eb0":"markdown","0528d2a1":"markdown","475c77e1":"markdown","56a0fee7":"markdown","b00627ed":"markdown","c3457650":"markdown","bfeb8075":"markdown","04a1b6db":"markdown","03e02aba":"markdown","b25fcbbd":"markdown","5f245dd3":"markdown","58710f8f":"markdown","393438e8":"markdown","2e1bc0b5":"markdown","a339d332":"markdown","61c303de":"markdown","184e8441":"markdown","324dd68e":"markdown","0fe54068":"markdown","6d54f163":"markdown","b6585a45":"markdown","712b83ad":"markdown","3be62253":"markdown","bf591e06":"markdown","76a6adf6":"markdown","8997afea":"markdown","84c6bfe2":"markdown","ba32d4a3":"markdown","fafa75c8":"markdown","9dbda15f":"markdown","c23fa5d8":"markdown","c305a7a4":"markdown","c911e6a8":"markdown","7477c2b5":"markdown","1a8e1585":"markdown","2eb14a3c":"markdown","b01d9770":"markdown","b46224d9":"markdown","3e1473a5":"markdown","aa99e13d":"markdown","dcd5dd7d":"markdown","c23a7090":"markdown","34281613":"markdown","a1e15848":"markdown","d99e83a0":"markdown","5709e97c":"markdown","50f5f8fa":"markdown","ac7387dc":"markdown","d74b3332":"markdown","5429ed0b":"markdown","81bcef99":"markdown","9f1e5d29":"markdown","d6ec1a3f":"markdown","c20c2760":"markdown"},"source":{"2aa05d3c":"# Third party\nfrom matplotlib import pyplot as mpl\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.metrics import accuracy_score, confusion_matrix, roc_auc_score, auc\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import Pipeline, make_pipeline\nfrom sklearn.preprocessing import KBinsDiscretizer, FunctionTransformer, Normalizer\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.feature_selection import SelectKBest, chi2, RFE\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.dummy import DummyClassifier\nimport plotly.figure_factory as ff\nimport plotly.offline as py\nimport plotly.graph_objs as go\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn.preprocessing import StandardScaler\nimport pandas as panda\nfrom imblearn.over_sampling import SMOTE\n# Local application\nimport miner_a_de_datos_an_lisis_exploratorio_utilidad as utils\nseed = 27912","8dcf5a54":"filepathC = \"..\/input\/breast-cancer-wisconsin-data\/data.csv\"\n\nindexC = \"id\"\ntargetC = \"diagnosis\"\n\n\ndataC = utils.load_data(filepathC, indexC, targetC)","f1b8f88c":"dataC.head(5)","57f839c1":"dataC = dataC.drop(dataC.columns[31], axis='columns')\n","3f2fb807":"dataC.head(5)","8a13d00f":"(X, y) = utils.divide_dataset(dataC, target=targetC)","69911003":"X.sample(5, random_state=seed)","551306ff":"y.sample(5, random_state=seed)","b8185901":"train_size = 0.7\n\n(X_train, X_test, y_train, y_test) = train_test_split(X, y,\n                                                      stratify=y,\n                                                      random_state=seed,\n                                                      train_size=train_size)","8c0fbf99":"X_train.sample(5, random_state=seed)\n","03a4745f":"X_test.sample(5, random_state=seed)\n","78b420a2":"y_train.sample(5, random_state=seed)\n","b1d10a9c":"y_test.sample(5, random_state=seed)","2329460b":"dataC_train = utils.join_dataset(X_train, y_train)\ndataC_test = utils.join_dataset(X_test, y_test)","b5a388e7":"dataC_train.shape","2555c953":"dataC_train.info(memory_usage=False)","4adf131d":"utils.plot_histogram(dataC_train)","3efd14b6":"utils.plot_barplot(dataC_train)","069e6c71":"utils.plot_pairplot(dataC_train, target=targetC)","e61bb83a":"dataC_trainC,dataC_trainO=(dataC_train.drop(\"diagnosis\",axis=1),dataC_train[\"diagnosis\"])","a7573de7":"columnas=[]\nfor i in dataC.drop(['diagnosis'],axis='columns'):\n    maximo=max(dataC[i])\n    dataC[i]=dataC[i]\/maximo\n    columnas.append(i)\nfor i in dataC_train.drop(['diagnosis'],axis='columns'):\n    maximo=max(dataC_train[i])\n    dataC_train[i]=dataC_train[i]\/maximo\n","7c08ca36":"f,ax=mpl.subplots(figsize=(31,31))\ncorrMatrix = dataC_trainC.corr()\n\nsns.heatmap(corrMatrix,annot=True,linewidths=.5,fmt=\".1f\",ax=ax)\n","3772857b":"categorias=dataC_trainC.columns\nx=0\naux=x\neliminado=[]\nfor i in categorias:\n    aux=x+1\n    no_encontrado=True\n    while (aux<len(categorias) and no_encontrado and not(i in eliminado)  ):\n        #print(eliminado)\n        if(corrMatrix[i][categorias[aux]]>0.8):\n            #print([corrMatrix[i][categorias[aux]]])\n            no_encontrado=False\n            eliminado.append(i)\n            \n        aux=aux+1\n    x=x+1\nfor i in eliminado:\n    dataC_trainC=dataC_trainC.drop(i,axis=1)\n    dataC_train=dataC_train.drop(i,axis=1)\nprint(eliminado)","181d77a1":"f,ax=mpl.subplots(figsize=(len(dataC_trainC.columns),len(dataC_trainC.columns)))\ncorrMatrix = dataC_trainC.corr()\n\nsns.heatmap(corrMatrix,annot=True,linewidths=.5,fmt=\".1f\",ax=ax)\n\n","da7065df":"utils.plot_barplot(dataC_train)","c5f336f8":"utils.plot_pairplot(dataC_train, target=\"diagnosis\")","c380d2ea":"utils.plot_histogram(dataC_trainC)","247ec956":"dataC_testC,dataC_testO=(dataC_test.drop(\"diagnosis\",axis=1),dataC_test[\"diagnosis\"])","9b120b7d":"best_features = SelectKBest(chi2, 4)\nl1_normalizer = Normalizer(\"l1\")\n","be7e2269":"zero_r_model = DummyClassifier(strategy=\"most_frequent\")\nutils.evaluate(zero_r_model,\n               dataC_trainC, dataC_testC,\n               dataC_trainO, dataC_testO)","9c538485":"tree_model = DecisionTreeClassifier(random_state=seed)\n","c8b0b14d":"dataC_testC_red=dataC_testC\nfor i in eliminado:\n    dataC_testC_red=dataC_testC_red.drop(i,axis=1)","2e3a1fba":"utils.evaluate(tree_model,\n               dataC_trainC, dataC_testC_red,\n               dataC_trainO, dataC_testO)","f28d292d":"discretizer = KBinsDiscretizer(n_bins=2, strategy=\"uniform\")\ndiscretize_tree_model = make_pipeline(discretizer, tree_model)\nutils.evaluate(discretize_tree_model,\n               dataC_trainC, dataC_testC_red,\n               dataC_trainO, dataC_testO)","8c558dcc":"seed = 27912","815bf6ae":"filepath = \"..\/input\/pima-indians-diabetes-database\/diabetes.csv\"\n\nindex = None\ntarget = \"Outcome\"\n\ndata = utils.load_data(filepath, index, target)","61d60cfe":"data.head(5)","3993ddc5":"data.sample(5, random_state=seed)","70d463cf":"(X, y) = utils.divide_dataset(data, target=\"Outcome\")","e19caff2":"y.sample(5, random_state=seed)","2c2b4a18":"train_size = 0.7\n\n(X_train, X_test, y_train, y_test) = train_test_split(X, y,\n                                                      stratify=y,\n                                                      random_state=seed,\n                                                      train_size=train_size)","b4a22b42":"X_train.sample(5, random_state=seed)","193260a7":"X_test.sample(5, random_state=seed)","bd394057":"y_train.sample(5, random_state=seed)","fe81caff":"y_test.sample(5, random_state=seed)","7faff28d":"y.cat.categories","2861ee18":"data.shape","6b77ab52":"utils.plot_histogram(data)","fcc2a5c5":"utils.plot_barplot(data)","3aaa8bd9":"f,ax=mpl.subplots(figsize=(31,31))\ncorrMatrix = X_train.corr()\n\nsns.heatmap(corrMatrix,annot=True,linewidths=.5,fmt=\".1f\",ax=ax)","64aae00e":"utils.plot_pairplot(data, target=\"Outcome\")","78d15a86":"data.describe(include=\"number\")","7cac67c8":"data.describe(include=\"category\")","4917e596":"def medina(variable):\n    sumt1=[]\n    sumt2=[]\n    n1=0\n    n2=0\n    t=0\n    for i in data[variable]:\n        j=data['Outcome'][t]\n        t=t+1\n        if (j==1 and i>0):\n            n1=n1+1\n            sumt1.append(i)\n        elif (j ==0 and i>0):\n            n2=n2+1\n            sumt2.append(i)\n    return (np.median(sumt1),np.median(sumt2))","a010db8b":"medina('Glucose')\nmedina('BloodPressure')\nmedina('BMI')\nmedina('SkinThickness')\nmedina('Insulin')","65e9e48d":"col=['Glucose','BloodPressure','Insulin','BMI','SkinThickness']\nfor i in col:\n    data[i].replace(0, np.nan, inplace= True)","fb0c2100":"data.loc[(data['Outcome'] == 0 ) & (data['Insulin'].isnull()), 'Insulin'] = 102.5\ndata.loc[(data['Outcome'] == 1 ) & (data['Insulin'].isnull()), 'Insulin'] = 169.5\ndata.loc[(data['Outcome'] == 0 ) & (data['Glucose'].isnull()), 'Glucose'] = 107\ndata.loc[(data['Outcome'] == 1 ) & (data['Glucose'].isnull()), 'Glucose'] = 140\ndata.loc[(data['Outcome'] == 0 ) & (data['SkinThickness'].isnull()), 'SkinThickness'] = 27\ndata.loc[(data['Outcome'] == 1 ) & (data['SkinThickness'].isnull()), 'SkinThickness'] = 32\ndata.loc[(data['Outcome'] == 0 ) & (data['BloodPressure'].isnull()), 'BloodPressure'] = 70\ndata.loc[(data['Outcome'] == 1 ) & (data['BloodPressure'].isnull()), 'BloodPressure'] = 74.5\ndata.loc[(data['Outcome'] == 0 ) & (data['BMI'].isnull()), 'BMI'] = 30.1\ndata.loc[(data['Outcome'] == 1 ) & (data['BMI'].isnull()), 'BMI'] = 34.3","a55eb233":"utils.plot_histogram(data)","d103d455":"yOut = data.Outcome\nXOut = data.drop('Outcome', axis = 1)\ncolumns = XOut.columns\nscaler = StandardScaler()\nXOut = scaler.fit_transform(XOut)\ndataOut_x = panda.DataFrame(XOut, columns = columns)","89c102a4":"X_train, X_test, y_train, y_test = train_test_split(dataOut_x, yOut,\n                                                                    stratify=yOut, random_state = seed, train_size = train_size)","93b99c8b":"discretizerQuantile = KBinsDiscretizer(n_bins=2, strategy='quantile')","c9f849ef":"discretizerUniform = KBinsDiscretizer(n_bins=2, strategy=\"uniform\")","a11eb4c5":"discretizerKmeans = KBinsDiscretizer(n_bins=2, strategy=\"kmeans\")","fe112059":"zero_r_model = DummyClassifier(strategy=\"most_frequent\")","aedf7467":"tree_model = DecisionTreeClassifier(random_state=seed)","e224cee7":"discretize_tree_model_Quantile = make_pipeline(discretizerQuantile, tree_model)","fc2bd7f4":"discretize_tree_model_Uniform = make_pipeline(discretizerUniform, tree_model)","1b2cdc9a":"discretize_tree_model_Kmeans = make_pipeline(discretizerKmeans, tree_model)","b33e9327":"utils.evaluate(zero_r_model,\n               X_train, X_test,\n               y_train, y_test)","7e0ab918":"utils.evaluate(tree_model,\n               X_train, X_test,\n               y_train, y_test)","90849634":"utils.evaluate(discretize_tree_model_Quantile,\n               X_train, X_test,\n               y_train, y_test)","0785d4cc":"utils.evaluate(discretize_tree_model_Uniform,\n               X_train, X_test,\n               y_train, y_test)","34f29262":"utils.evaluate(discretize_tree_model_Kmeans,\n               X_train, X_test,\n               y_train, y_test)","a9511915":"Comenzamos con las variables predictoras del conjunto de datos de training:","0f0aadb3":"Esta base de datos nos cuenta basicamente si una persona tiene un cancer benigno o maligno(este atributo sera el objetivo de nuestro sistema)por otro lado tenemos otros atributos que nos indican la media,el error y la desviacion de algunos datos sobre las celulas del paciente.\nLo primero que haremos sera dividir los diferentes **casos** entre test y training,mas tarde estos casos se dividiran por sus **variables**(separaremos el objetivo de los demas).Antes de esto tendriamos que eliminar una **variable**  en todos los casos,la ultima que es un null y no aporta nada al estudio.Luego tendriamos que buscar aquellas **variables** que tampoco aportan mucho al estudio para ello buscaremos su correlacion con las demas ,pero antes lo normalizaremso dividiendo cada **variable** entre el valor maximo de todos los casos\n","9ef8f2d8":"Ahora podemos ver que no hay ninguna correlacion mayor que 0.9","49ea4bef":"Ahora para poder constrastar datos debemos de separar diagnosis de los demas","e7e1bec7":"A continuacion creamos dos matrices para analizar la correlaci\u00f3n entre pares de variables, en el primer caso el numero de cada hueco representa la coincidencia o solopamiento entre variables , que en caso de ser demasiado alta nos podria indicar la redundancia de ciertas variables predictoras","3ac042fc":"# 4.Prepocesamiento de datos","263d82fb":"* ### CART con discretizacion de igual frecuencia (Quantile)","ec674de9":"Pero para conseguir una muestra menos sesgada hacemos una aleatoria, porque los data-set suelen estar ordenanos en funci\u00f3n de sus variables.","207cced9":"# 5.Algoritmos de classificacion y evaluacion","141c6270":"# 1.Analisis de los datos de CW","8deefbbb":"Para tratar los datos anomalos usaremos StandarScaler que \"escala\" la propiedad restando por la media y diviendo por la desviaci\u00f3n est\u00e1ndar, con esto conseguiremos que los datos anomalos sean menos sesgados que le resto en comparacion con el resto.","7af644db":"data.info(memory_usage=False)","0f340dd8":"Viendo el resultado ninguna de las variables predictoras es redundante","d52f24bd":"Empezamos Algoritmo Zero-R","13984410":"Como podemos observar varias variables tiene tienen datos ruidosos, todos los que tienen una cantidad 0, alejados de la mayor\u00eda de valores de esas variables, este ser\u00eda el caso de`Glucose` , `BloodPressure`, `SkinThickness`,`insulin` y `BMI`. Tambein podemos ver que `SkinThickness`, `Age` y `Insulin` tienen una disposici\u00f3n central. No encuentro valores anomalos en este analisis multivariado.","c348c8cb":"Tambien creamos una pipeline para cada una de las discretizaciones y usando el algoritmo CART","df846d59":"Procedemos a juntar los atributos con los target","57cdd20e":"Al haber tantas **variables** no podemos verlo claramente y no solo eso al no estar normalizado tenemos **variables** que van a lo mejor de 0 a 100 y otros que van hasta mas de 4000 por lo que para ver todos los datos juntos debemos de normalizarlo entre el maximo de cada uno para que todo vaya de 0 a 1(esto lo haremos mas adelante)\n ","371a4437":"Comprobamos cuantos **casos** hay ","bf8fa155":"* ### CART sin discretizaci\u00f3n.","c423d1b9":"Para hacer un proceso de Holdout creamos la muestra de Test y la muestra de training con la siguiente proporci\u00f3n:\n\n* Muestra training (70%)\n* Muestra test (t\u00edpicamente, 30%)\n\n\nPara realizar un *holdout* podemos utilizar el m\u00e9todo `train_test_split` de `scikit-learn`:\n","3850d88c":"# 3. An\u00e1lisis exploratorio de datos","e3400f10":"Ahora probaremos con el modelo tree_model","3d359fa5":"Cargamos el data-set","9ddec64c":"Hay 398 casos y 31 variables","18f132f7":"#\u00a0Pr\u00e1ctica 1: An\u00e1lisis exploratorio de datos, preprocesamiento y validaci\u00f3n de modelos de clasificaci\u00f3n\\*\n\n###\u00a0Miner\u00eda de Datos: Curso acad\u00e9mico 2020-2021\n\n### Pima diabetes","9a64c233":"Comprobamos que los valores con 0 de las variables han sido eliminados","0137f153":"Ahora veremos de nuevo los datos y se veran mejor ","a674d703":"Comprobamos que los conjuntos de las variables predictoras y el de la variable objetivo se han sepeparado correctamente","a968141e":"Ahora tenemos que hacer varias tareas:\n* Limpieza de datos\n* Integracion de datos\n* Transformacion de datos\n* Reduccion de datos\nLas dos ultimas ya se han realizado con anterioridad,y sobre la limpieza ya hemos visto que no es necesario ya que no hay datos anomalos o nuloss ","af62044a":"Genreamos modelos de ZeroR y de CART. Para ello usamos los siguientes clasificadores de la librer\u00eda  scikit-learn","b5b99757":"Lo primero que haremos para tratar los datos crudos sera una imputaci\u00f3n de valores perdidos,hay varias variables , 'Glucose','BloodPressure','Insulin','BMI','SkinThickness', que por el contexto no deber\u00edan tener valores a 0, as\u00ed que lo que haremos ser\u00e1 sustituir esos valores por la mediana de cada uno , separando tambi\u00e9n segun la variable objetivo.","9ebb0ff4":"Vemos que a\u00f1ade una tabla que sobra,por lo que eliminamos","344bdf7a":"### Descripci\u00f3n del conjunto de datos","fe694eb0":"# 4. Preprocesamiento de datos","0528d2a1":"#\u00a05. Algoritmos de clasificaci\u00f3n","475c77e1":"Una vez cargado hacemos una prueba para comprobar que efectivamente se ha cargado la BBDD,haciendo una tirada de las 5 primeras","56a0fee7":"Comprobamos","b00627ed":"Usamos el metodo info para saber de que tipo son las variables","c3457650":"Agregamos los datos","bfeb8075":"# Discretizacion","04a1b6db":"Hacemos la muestra con el conjunto de training","03e02aba":"* ### ZERO-R","b25fcbbd":"Para buscar que otro atributos no nos son utiles,dividimos cada columna por su maximo","5f245dd3":"En el caso de Pima Diabetes no disponemos de variable \u00edndice, asi que index lo ponemos a \"none\"\n Usamos la funci\u00f3n head para obtener las n primeras instancias del conjunto de datos:","58710f8f":"Y test:","393438e8":"En el caso de nuestra \u00fanica variable categ\u00f3rica podemos ver que la muestra no esta balanceada, de nuestros 768 pacientes la mayor\u00eda dio negativo en tener diabetes como nos dice este analisis univariado.","2e1bc0b5":"En nuestro caso todas las variables predictoras son varibales continuas, y solo la variable obejetivo es una variable discreta,","a339d332":"Y test:","61c303de":"Ahora tenemos que observar los datos:","184e8441":"Con shape podemos saber la cantidad de casos que hay en nuestra base de datos frente a la cantidad que variables que disponemos, 768 pacientes estudiados frente a las nueve variables que disponemos, que son las diferentes pruebas que se han hecho a estos pacientes.","324dd68e":"Calculamos la mediana de esas variables para usarlas a modo de marcador","0fe54068":"El conjunto de datos que vamos a emplear es Pima diabetes. Tiene dos posibles resultados.\n\n0 Cuando el paciente no tiene diabetes\n1 Cuando el paciente tiene diabetes\nQue conforman los valores de la variable a predecir, la variable objetivo (Outcome). Se tienene en cuenta las siguientes variables predictoras:\n\nAge: Edad del paciente.\nPregnancies: N\u00famero de embarazos de la paciente.\nGlucose: Concentraci\u00f3n de glucosa en plasma sangu\u00edneo.\nBloodPressure: Presi\u00f3n diast\u00f3lica arterial (mm Hg).\nSkinThickness: Grosor de la piel en el triceps (mm).\ninsulin: Cantidad de insulina en sangre (mu Insulina\/ml).\nBMI: \u00cdndice de Masa Corporal (kg\/m^2).\nDiabetesPedigreeFunction: Funci\u00f3n para el historial de diabetes en la familia del paciente.","6d54f163":"* ### CART con discretizacion de igual anchura (Uniform)","b6585a45":"Hacemos una comprobacion de que se ha cargado","712b83ad":"Ahora remplazamos los NaN por las medianas calculadas","3be62253":"Como era de esperar no nos da mucha confianza ya que basicamente coge el valor que mas se repite en el train y se lo aplica a todos los test","bf591e06":"A partir de esto podemos ver que hay algunos datos que por asi decirlo sobran ya que tienen mucha correlacion,asi que lo que haremos sera una funcion para que localice aquellos que tienen una correlacion de 0.9 o mas y luego eliminarlo","76a6adf6":"# 5. Evaluaci\u00f3n de modelos","8997afea":"Y por \u00faltimo discretizaremos,teniendo en cuenta lo visto en el diagrama de puntos probaremos con las tres discretizaciones , las tres con dos bins ya que nuestra variable objetivo tiene dos valores","84c6bfe2":"Vamos a comenzar visualizando las variables num\u00e9ricas del conjunto de datos usando histogramas para las variables num\u00e9ricas y diagramas de barras para las variables categ\u00f3ricas","ba32d4a3":"Nuestra variable objetivo tiene dos posibles valores, 0 y 1","fafa75c8":"omo podemos ver ninguna de las tres discretizaciones resultan en una mejora del algoritmo CART, todo lo contrario pues la precisi\u00f3n del algortimo es sensible superior sin ninguna discretizaci\u00f3n, aun as\u00ed podemos observar que la que da mejor resultado de las tres es la discretizaci\u00f3n de igual anchura.","9dbda15f":"Por ultimo debemos de separar los datos del test como ya hicimos anteriormente","c23fa5d8":"Comprobamos tambi\u00e9n con la variable objetivo:\n","c305a7a4":"# 1.Preliminares","c911e6a8":"El algoritmo zero_r no nos dara mucha precisi\u00f3n pero nos servira como baseline, como referencia para el resto. Para que salga la clase m\u00e1s frecuente del subconjunto train usamos el hiperpar\u00e1metro strategy=\"most_frequent\"","7477c2b5":"#\u00a02. Acceso y almacenamiento de datos","1a8e1585":"Determinados que son los atributos y cuales los target","2eb14a3c":"* ### CART con discretizacion basada en k-medias (Kmeans)","b01d9770":"Con las medianas calculadas pasamos a remplazar los 0 por NaN","b46224d9":"* ### Visualizaci\u00f3n de las variables","3e1473a5":"Volvemos a comprobar que esta vez la tabla este bien y que ya no aparece la ultima columna","aa99e13d":"En esta caso a diferencia de la de iris no podemos averiguar claramente el poder discriminatorio de cada variable predictora pues hay muchos datos ruidosos.","dcd5dd7d":"X.sample(5, random_state=seed)","c23a7090":"Iniciamos las distintas bibliotecas y script necesarios para ","34281613":"Creamos nuevos subconjuntos sin los datos anomalos","a1e15848":"Una vez que comprobamos que se realiza correctamente determinamos que cantidad de datos usamos para el entrenamiento(un 70%).Tambien definimos las X e Y de test","d99e83a0":"Como podemos ver no podemos ver casi nada,por lo que tenemos que restringir los datos.\nTambien ya podemos eliminar algunos datos como por ejemplo perimetro y area ya que son dependientes del radio,pero en vez de eso haremos una tabla de coorelacion\n","5709e97c":"## 2.Analizamos la base de datos\n","50f5f8fa":"Creamos conjunto de datos separado dos subconjuntos, uno con las variables predictoras (X) y otro con la variable objetivo (y)","ac7387dc":"#\u00a01. Preliminares","d74b3332":"Ahora para ver las propiedades de nuestro conjunto de datos y poder sacar conclusiones de cara al preprocesamiento usaremos gr\u00e1ficos y estadisticos.","5429ed0b":"El zero-r nos da la precisi\u00f3n m\u00e1s baja de los algoritmos testeados.","81bcef99":"#\u00a0Pr\u00e1ctica 1: An\u00e1lisis exploratorio de datos, preprocesamiento y validaci\u00f3n de modelos de clasificaci\u00f3n\\*\n\n###\u00a0Miner\u00eda de Datos: Curso acad\u00e9mico 2020-2021\n\n### Compa\u00f1eros:Juan Iba\u00f1ez y Raul Milla","9f1e5d29":"Para este caso debemos de reducir el test ya que tiene mas atributos que el train","d6ec1a3f":"Tras hacer las diferentes pruebas vemos que el mejor algoritmo que tenemos es el modelo de arbol pero sin descretizar con un error de menos de un 8 por cierto y vemos que el peor obviamente ha sido el de Zero ya que ese nos da un error muy alto(casi el 40%).Por otro lado no podria decir cual es mejor y cual se recomieda usar por que por un lado el algortimo de arbo descretizado solo da un benigno mal asi que dependiendo de que es mas valioso si o bien que se equivoque en falsos positivos o en positivos falsos o en los dos.","c20c2760":"Asi vemos que no hay ningun dato da\u00f1ado,es decir que todos los casos tienes sus variables"}}