{"cell_type":{"7adaa449":"code","57f21bad":"code","f1208020":"code","a5ce6174":"code","f4f075e4":"code","df29a3d7":"code","b4ba2004":"code","e74075c3":"code","453c49b6":"code","52a5c43d":"code","21fcd96d":"code","e321f3bb":"code","ca707476":"code","4471a1d3":"code","05556d58":"code","3915b079":"code","8e759017":"code","67e605f8":"markdown","50821431":"markdown","d92e862e":"markdown","09822ce5":"markdown","d3280821":"markdown","ec5928e9":"markdown","f8b3083d":"markdown","134bcf75":"markdown","a7c25fc7":"markdown","db14176d":"markdown","fd25a6f8":"markdown","043833ab":"markdown","753d9b9c":"markdown","d5f9eb41":"markdown","51df0dbd":"markdown","ac36234d":"markdown"},"source":{"7adaa449":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt # visualization\nimport seaborn as sns; sns.set()\n\ndata=pd.read_csv(\"..\/input\/StudentsPerformance.csv\")\n\nprint(\"data shape is:\", data.shape,\"\\n\") #There are 1000 records with 8 features each\nprint(\"features are:\", data.columns.tolist()) #To see the columns names\n","57f21bad":"print(data.isnull().sum(),\"\\n\")\nprint(data.describe().round(2))","f1208020":"fig, axes = plt.subplots(1,3, sharey=True, figsize=(18,5))\n\nax1, ax2, ax3 = axes.flatten()\nax1.hist(data['math score'], bins=10, color=\"red\")\nax2.hist(data['reading score'], bins=10, color=\"blue\")\nax3.hist(data['writing score'], bins=10, color=\"orange\")\nax1.set_xlabel('MATH', fontsize=\"large\")\nax1.set_ylabel(\"SCORE\", fontsize=\"large\")\nax2.set_xlabel('READING', fontsize=\"large\")\nax3.set_xlabel('WRITING', fontsize=\"large\")\n\nplt.suptitle('Score Comparison', ha='center', fontsize='x-large')\nplt.show()","a5ce6174":"import warnings\nwarnings.filterwarnings('ignore') # to ignore some warnings\n\nsns.kdeplot(data['math score'], shade=True, color=\"red\", alpha=0.9)\nsns.kdeplot(data['reading score'], shade=True, color=\"blue\", alpha=0.6)\nsns.kdeplot(data['writing score'], shade=True, color=\"orange\", alpha=0.4)\nplt.show()","f4f075e4":"col=[\"gender\", \"race\/ethnicity\",\"parental level of education\", \"lunch\", \"test preparation course\"]\nfor item in col:\n    print(item.upper(),\":\")\n    print(data[item].value_counts(),\"\\n\") ","df29a3d7":"fig, axs = plt.subplots(3, 2)\nfig.set_figheight(15)\nfig.set_figwidth(10)\nax1, ax2, ax3, ax4,ax5 = axs[0, 0], axs[0, 1], axs[1, 0], axs[1, 1], axs[2, 0]\naxs[-1,-1].axis('off')\ncolors = [\"grey\", \"pink\", \"yellowgreen\", \"orange\", \"violet\", \"yellow\"]\nax1.pie(data['gender'].value_counts(), labels=list(data['gender'].unique()), colors=colors, autopct='%1.1f%%', startangle=90)\nax2.pie(data['race\/ethnicity'].value_counts(), labels=list(data['race\/ethnicity'].unique()), colors=colors, autopct='%1.1f%%', startangle=90)\nax3.pie(data['lunch'].value_counts(), labels=list(data['lunch'].unique()), colors=colors, autopct='%1.1f%%', startangle=90)\nax4.pie(data['test preparation course'].value_counts(), labels=list(data['test preparation course'].unique()), colors=colors, autopct='%1.1f%%')\nax5.pie(data['parental level of education'].value_counts(), labels=list(data['parental level of education'].unique()), colors=colors, autopct='%1.1f%%', startangle=60)\nplt.suptitle('PERCENTAGE DISTRIBUTION', ha='center', fontsize='xx-large',fontweight='bold')\nax1.set_title(\"GENDER\",fontsize='x-large',fontweight='bold' )\nax2.set_title(\"RACE\/ETHNICITY\",fontsize='x-large',fontweight='bold')\nax3.set_title(\"LUNCH\",fontsize='x-large',fontweight='bold')\nax4.set_title(\"TEST PREP. COURSE\",fontsize='x-large',fontweight='bold')\nax5.set_title(\"PARENT EDUCATION\",fontsize='x-large',fontweight='bold')\nplt.show()","b4ba2004":"data[\"average score\"]=np.mean(data[['math score', 'reading score', 'writing score']], axis=1).round(1)\ndata['admitted\/rejected']=np.where(data['average score']>70,1,0)\ndata_old=data.copy()\ndata_old.head()","e74075c3":"sns.set(style=\"ticks\")\ng = sns.catplot(data=data, x=\"parental level of education\", y=\"average score\", hue=\"gender\")\ng.set_xticklabels(rotation=90)\nplt.show()","453c49b6":"g=sns.FacetGrid(data, col='admitted\/rejected', hue=\"gender\", height=3.5)\ng.map(sns.kdeplot, 'math score')\nplt.legend()\nplt.show()","52a5c43d":"g=sns.FacetGrid(data, col='admitted\/rejected', hue=\"gender\", height=3.5)\ng.map(sns.kdeplot, 'reading score')\nplt.legend()\nplt.show()","21fcd96d":"g=sns.FacetGrid(data, col='admitted\/rejected', hue=\"gender\", height=3.5)\ng.map(sns.kdeplot, 'writing score')\nplt.legend()\nplt.show()","e321f3bb":"g = sns.pairplot(data.iloc[:, [0,5,6,7]], hue=\"gender\", diag_kind=\"kde\", height=2.5)\nplt.show()","ca707476":"from sklearn.preprocessing import LabelEncoder\nlbl=LabelEncoder()\n\ndata[\"gender_code\"]=lbl.fit_transform(data[[\"gender\"]])\ndata[\"race\/ethnicity_code\"]=lbl.fit_transform(data[[\"race\/ethnicity\"]])\ndata[\"parental level of education_code\"]=lbl.fit_transform(data[[\"parental level of education\"]])\ndata[\"lunch_code\"]=lbl.fit_transform(data[[\"lunch\"]])\ndata[\"test preparation course_code\"]=lbl.fit_transform(data[[\"test preparation course\"]])\ngood_cols=['reading score','writing score','math score','gender_code','race\/ethnicity_code','parental level of education_code','lunch_code','test preparation course_code']\ntarget=data['admitted\/rejected']\ndata[good_cols].head()","4471a1d3":"fig, ax = plt.subplots(figsize=(3,3))\ncol=['reading score','writing score', 'math score']\ncorr_matrix=data[col].corr(method=\"spearman\")\nax=sns.heatmap(corr_matrix, center=0, vmax=1, vmin=-1, annot=True, square=True)","05556d58":"from collections import Counter\nimport math\nimport scipy.stats as ss\n\ndef conditional_entropy(x,y):\n    # entropy of x given y\n    y_counter = Counter(y)\n    xy_counter = Counter(list(zip(x,y)))\n    total_occurrences = sum(y_counter.values())\n    entropy = 0\n    for xy in xy_counter.keys():\n        p_xy = xy_counter[xy] \/ total_occurrences\n        p_y = y_counter[xy[1]] \/ total_occurrences\n        entropy += p_xy * math.log(p_y\/p_xy)\n    return entropy\n\ndef theil_u(x,y):\n    s_xy = conditional_entropy(x,y)\n    x_counter = Counter(x)\n    total_occurrences = sum(x_counter.values())\n    p_x = list(map(lambda n: n\/total_occurrences, x_counter.values()))\n    s_x = ss.entropy(p_x)\n    if s_x == 0:\n        return 1\n    else:\n        return (s_x - s_xy) \/ s_x\n    \ncolumn=['gender_code','race\/ethnicity_code','parental level of education_code','lunch_code','test preparation course_code']\ndata=data[column]\ndata[\"admitted\/rejected\"]=target\ncolumns=data.columns\ntheilu = pd.DataFrame(index=['admitted\/rejected'], columns=data.drop(\"admitted\/rejected\", axis=1).columns)\n\n\nfor j in range(0,len(column)):\n    u = theil_u(data['admitted\/rejected'].tolist(),data[columns[j]].tolist())\n    theilu.loc[:,columns[j]] = u\ntheilu.fillna(value=np.nan,inplace=True)\nplt.figure(figsize=(15,1))\nsns.heatmap(theilu,annot=True,fmt='.3f')\nplt.show()","3915b079":"from sklearn.metrics import confusion_matrix, accuracy_score\n\nfrom sklearn.ensemble import RandomForestClassifier\nRF=RandomForestClassifier(max_depth=2, random_state=0)\n\nfrom sklearn.linear_model import LogisticRegression\nLR=LogisticRegression(random_state=0)\n\nfrom sklearn.svm import SVC\nsvc=SVC(kernel=\"sigmoid\", random_state=0)\n\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test=train_test_split(data_old.iloc[:,5:8], target, test_size=0.2, random_state=0)\nmodels=[RF, LR, svc]\nfor model in models:\n    model.fit(x_train, y_train)\n    y_pred=model.predict(x_test)\n    print(model)\n    print('confusion matrix:',\"\\n\",confusion_matrix(y_test, y_pred))\n    print('accuracy score:',accuracy_score(y_test, y_pred),\"\\n\\n\") ","8e759017":"from sklearn.preprocessing import StandardScaler\nsc=StandardScaler()\nx_train=sc.fit_transform(x_train)\nx_test=sc.transform(x_test)\n\nmodels=[RF, LR, svc]\nfor model in models:\n    model.fit(x_train, y_train)\n    y_pred=model.predict(x_test)\n    print(model)\n    print('confusion matrix:',\"\\n\",confusion_matrix(y_test, y_pred))\n    print('accuracy score:',accuracy_score(y_test, y_pred),\"\\n\\n\") ","67e605f8":"So the best result gives LR.","50821431":"**STUDENT PERFORMANCE IN EXAMS**\n\nIn this kernel I am going to present analysis for Student Performance in Exams. After some basec analysis we will make an assumption to split the student into two groups (Admitted orRejected). Later, we will use classification algorithms to check their performances.","d92e862e":"Let's see if there are some missing values, and check main statistics...","09822ce5":"Now let's see the difference between the two genders per subject.","d3280821":"Below one can notice that the higher the degree of parrent education the higher the minimum average score is.  ","ec5928e9":"One can notice that  in both admitted and rejected cases Male students succeeded more in Math.\n\nThis relationship is also seen in the next Pairplot.","f8b3083d":"**CLASSIFICATION**\n\nNow let's test several classification algorithms and  evaluate their performances.","134bcf75":"We can see that there are no misrecords in the grades (negatives or above 100) and no missing data.\n\nNow let's plot each subject based on the scores.","a7c25fc7":"As the outcome (admitted\/rejected) was totally based on our assumption (average score of above 80%), there is no need to express the correlation between the outcome and the test scores.\n\nBut it is interesting to find the correlation of the outcome with the rest of the features.\n\nHaving nominal variables for the rest of the features, let's use Theil's U (also known as Uncertainty Coefficient), which is based on the conditional entropy. This will help us find out the association between the outcome and the features. In other words, marked as U(x|y), this coefficient provides a value in the range of [0,1], where 0 means that feature y provides no information about feature x, and 1 means that feature y provides full information about  x's value.\n\nThus, we have the following association:","db14176d":"***CORRELATION***\n\nNow let's see the correlation between the features and the result (whether the student is admitted or rejected).\n\nFirst let's use LabelEncoder to convert categorical variables into numbers.","fd25a6f8":"Now let's assume that the students with 80% average score can be admitted.\n\nThen let's split the data into features and target, where the target will be binary output Admitted\/Rejected.","043833ab":"The correlation between the numeric features is as follows:","753d9b9c":"Hope you found this kernel informative.\n\nAs this is my first public kernel, please let me know if someting could be improved.\n\nYour comments are always welcome.\n\nThanks!","d5f9eb41":"Combined distribution is as follows:","51df0dbd":"After applying StandardScaler we have improvement in model performance.","ac36234d":"Next we will see the numbers of categories in the given features, and visualization of them will follow:"}}