{"cell_type":{"06aebe67":"code","19950946":"code","405d3b7f":"code","86ca9430":"code","3db1f3fc":"code","85dcbc85":"code","35282e0e":"code","642e8427":"code","e662c854":"code","b32dc4f4":"code","89f84ff7":"code","4637d5b3":"code","7bbd3d9b":"code","610c3a7a":"code","85ce55ac":"code","ac28eb17":"code","fe14dea1":"code","9cbcaeab":"code","f5e22c33":"code","883a462d":"code","d11c58e2":"code","0c20fa85":"code","2e494b9e":"code","e75bb6dc":"code","5e592de0":"code","4f7b95ba":"code","db187e3e":"code","5cd73f9a":"code","7c06ca6c":"code","95ca1db9":"markdown","fc89ec9c":"markdown","38ebf623":"markdown","712707cb":"markdown","ee1558b4":"markdown","ffe31eb9":"markdown","24e6f770":"markdown","2936a6e6":"markdown","dd03f291":"markdown","0f8e2922":"markdown","065666c0":"markdown","7b958f95":"markdown","82c30b30":"markdown","d390718c":"markdown","dfe46f5f":"markdown","c4f2ded6":"markdown","b1ee871e":"markdown","fd5e67dc":"markdown","398633c2":"markdown","a86d9be2":"markdown","8bce921c":"markdown","c8bd3f81":"markdown","5bb268a3":"markdown","ecd4610b":"markdown","7e2a4f46":"markdown","869a9cb8":"markdown","e2d8ffff":"markdown"},"source":{"06aebe67":"import pandas as pd\npd.set_option('display.max_columns', None)\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.style as style\nstyle.use('fivethirtyeight')\nimport matplotlib.pylab as plt\nimport calendar\nimport warnings\nwarnings.filterwarnings(\"ignore\")","19950946":"%%time\ntrain = pd.read_csv('..\/input\/data-science-bowl-2019\/train.csv')\ntrain_labels = pd.read_csv('..\/input\/data-science-bowl-2019\/train_labels.csv')\ntest = pd.read_csv('..\/input\/data-science-bowl-2019\/test.csv')\nspecs = pd.read_csv('..\/input\/data-science-bowl-2019\/specs.csv')\nsample_sub = pd.read_csv('..\/input\/data-science-bowl-2019\/sample_submission.csv')","405d3b7f":"train.head()","86ca9430":"train.shape","3db1f3fc":"keep_id = train[train.type == \"Assessment\"][['installation_id']].drop_duplicates()\ntrain = pd.merge(train, keep_id, on=\"installation_id\", how=\"inner\")","85dcbc85":"train.shape","35282e0e":"keep_id.shape","642e8427":"plt.rcParams.update({'font.size': 16})\n\nfig = plt.figure(figsize=(12,10))\nax1 = fig.add_subplot(211)\nax1 = sns.countplot(y=\"type\", data=train, color=\"blue\", order = train.type.value_counts().index)\nplt.title(\"number of events by type\")\n\nax2 = fig.add_subplot(212)\nax2 = sns.countplot(y=\"world\", data=train, color=\"blue\", order = train.world.value_counts().index)\nplt.title(\"number of events by world\")\n\nplt.tight_layout(pad=0)\nplt.show()","e662c854":"def get_time(df):\n    df['timestamp'] = pd.to_datetime(df['timestamp'])\n    df['date'] = df['timestamp'].dt.date\n    df['month'] = df['timestamp'].dt.month\n    df['hour'] = df['timestamp'].dt.hour\n    df['dayofweek'] = df['timestamp'].dt.dayofweek\n    return df\n    \ntrain = get_time(train)\n\n#list(set(train['title'].unique()).union(set(test['title'].unique())))","b32dc4f4":"fig = plt.figure(figsize=(12,10))\nse = train.groupby('date')['date'].count()\nse.plot()\nplt.title(\"Event counts by date\")\nplt.xticks(rotation=90)\nplt.show()","89f84ff7":"fig = plt.figure(figsize=(12,10))\nse = train.groupby('dayofweek')['dayofweek'].count()\nse.index = list(calendar.day_abbr)\nse.plot.bar()\nplt.title(\"Event counts by day of week\")\nplt.xticks(rotation=0)\nplt.show()","4637d5b3":"fig = plt.figure(figsize=(12,10))\nse = train.groupby('hour')['hour'].count()\n#se.index = list(calendar.day_abbr)\nse.plot.bar()\nplt.title(\"Event counts by hour of day\")\nplt.xticks(rotation=0)\nplt.show()","7bbd3d9b":"test.head()","610c3a7a":"test.shape","85ce55ac":"test.installation_id.nunique()","ac28eb17":"sample_sub.shape","fe14dea1":"plt.rcParams.update({'font.size': 22})\n\nplt.figure(figsize=(12,6))\nsns.countplot(y=\"title\", data=train_labels, color=\"blue\", order = train_labels.title.value_counts().index)\nplt.title(\"Counts of titles\")\nplt.show()","9cbcaeab":"plt.rcParams.update({'font.size': 16})\n\nplt.figure(figsize=(8,8))\nsns.countplot(x=\"accuracy_group\", data=train_labels, color=\"blue\")\nplt.title(\"Counts of accuracy group\")\nplt.show()","f5e22c33":"train_labels[train_labels.installation_id == \"0006a69f\"]","883a462d":"train[(train.event_code == 4100) & (train.installation_id == \"0006a69f\") & (train.title == \"Bird Measurer (Assessment)\")]","d11c58e2":"#credits for this code chuck go to Andrew Lukyanenko\ntrain['attempt'] = 0\ntrain.loc[(train['title'] == 'Bird Measurer (Assessment)') & (train['event_code'] == 4110),\\\n       'attempt'] = 1\ntrain.loc[(train['type'] == 'Assessment') &\\\n       (train['title'] != 'Bird Measurer (Assessment)')\\\n       & (train['event_code'] == 4100),\\\n          'attempt'] = 1\n\ntrain['correct'] = None\ntrain.loc[(train['attempt'] == 1) & (train['event_data'].str.contains('\"correct\":true')), 'correct'] = True\ntrain.loc[(train['attempt'] == 1) & (train['event_data'].str.contains('\"correct\":false')), 'correct'] = False","0c20fa85":"train[(train.installation_id == \"0006a69f\") & (train.attempt == 1)]","2e494b9e":"train[~train.installation_id.isin(train_labels.installation_id.unique())].installation_id.nunique()","e75bb6dc":"train = train[train.installation_id.isin(train_labels.installation_id.unique())]\ntrain.shape","5e592de0":"print(f'Number of rows in train_labels: {train_labels.shape[0]}')\nprint(f'Number of unique game_sessions in train_labels: {train_labels.game_session.nunique()}')","4f7b95ba":"count_combi = train.groupby(['game_session', 'world']).size()\nprint(f'Number of unique game_session in train: {train.game_session.nunique()}')\nprint(f'Number of unique combi of game_session and world in train: {count_combi.shape[0]}')","db187e3e":"train_labels = pd.merge(train_labels, train[['game_session', \"world\"]].drop_duplicates(), on= \"game_session\", how=\"left\")","5cd73f9a":"train_labels.shape","7c06ca6c":"train_labels.head()","95ca1db9":"As you can see, we have now lost about 3 million rows.","fc89ec9c":"When looking at the day of the week, we see no major difference. Of course, we are talking about kids who don't have to go to work ;-)","38ebf623":"I will first visualize some of the existing columns.","712707cb":"To add in upcoming versions: things like start of session (timestamp first event in session), number of events and sessions by installation before the assessment. Number of events by installation_id by event type (game, activity, assessment, clip) before the assessment, game_time before assessment.","ee1558b4":"So we have 1.1 million rows on a thousand unique installation_ids in the test set. Below, you can see that we have this same amount of rows in the sample submission. This means that there are no installation_ids without assessment in the test set indeed.","ffe31eb9":"**To be continued**","24e6f770":"# 3. Understanding and visualizing the train labels","2936a6e6":"From Kaggle: The file train_labels.csv has been provided to show how these groups would be computed on the assessments in the training set. Assessment attempts are captured in event_code 4100 for all assessments except for Bird Measurer, which uses event_code 4110. If the attempt was correct, it contains \"correct\":true.\n\nHowever, in the first version I already noticed that I had one attempt too many for this installation_id when mapping the rows with the train_labels for. It turns out that there are in fact also assessment attemps for Bird Measurer with event_code 4100, which should not count (see below). In this case that also makes sense as this installation_id already had a pass on the first attempt","dd03f291":"# Table of contents\n\n* [1. Understanding the train data](#1.-Understanding-the-train-data)\n* [2. Understanding the test set](#2.-Understanding-the-test-set)\n* [3. Understanding and visualizing the train labels](#3.-Understanding-and-visualizing-the train-labels)\n* [4. Feature engineering and visualizations](#4.-Feature-engineering-and-visualizations)","0f8e2922":"So we have 11 million rows and just 11 columns. However, Kaggle provided the following note: Note that the training set contains many installation_ids which never took assessments, whereas every installation_id in the test set made an attempt on at least one assessment.\n\nAs there is no point in keeping training data that cannot be used for training anyway, I am getting rid of the installation_ids that never took an assessment\n    ","065666c0":"As this check works out, I can safely add world to train_labels.","7b958f95":"Below you can see that row 2615 is not in the assessment results of this installation_id anymore, and the results add up nicely to the num_correct and num_incorrect in the train_labels.","82c30b30":"# 2. Understanding the test set\n\nFrom Kaggle: For each installation_id represented in the test set, you must predict the accuracy_group of the last assessment for that installation_id.","d390718c":"After having read Andrew Lukyanenko's notebook, I decided to also create an attempt variable. In addition, I have also borrowed his code to extract the correct variable (thanks Andrew!).","dfe46f5f":"Now the question arises: Could there be installation_id's who did assessments (we have already taken out the ones who never took one), but without results in the train_labels? As you can see below, yes there are 628 of those.","c4f2ded6":"Ok, now that we have that confirmed, I am looking for values that are always the same for a game_session in the train dataframe. It turns out that the only one is world. I also checked if some of the datetime variables were unique, but this is not always the case (events within a session may cross midnight).","b1ee871e":"When looking at the numbers by hour of the day, I find the distribution a little bit strange. Kids seem up late at night and don't do much early in the morning. Has this something to do with time zones perhaps?","fd5e67dc":"Below your see the counts by date. By the way, I have wasted a lot of time on trying to fix the weird ticks on the x-axis, but this seems a bug: https:\/\/github.com\/matplotlib\/matplotlib\/issues\/13183","398633c2":"# Data Science Bowl 2019\n\n# Introduction\n\nPBS KIDS, a trusted name in early childhood education for decades, aims to gain insights into how media can help children learn important skills for success in school and life. In this challenge, you\u2019ll use anonymous gameplay data, including knowledge of videos watched and games played, from the PBS KIDS Measure Up! app, a game-based learning tool developed as a part of the CPB-PBS Ready To Learn Initiative with funding from the U.S. Department of Education. Competitors will be challenged to predict scores on in-game assessments and create an algorithm that will lead to better-designed games and improved learning outcomes. Your solutions will aid in discovering important relationships between engagement with high-quality educational media and learning processes.\n\n**Where does the data for the competition come from?**\nThe data used in this competition is anonymous, tabular data of interactions with the PBS KIDS Measure Up! app. Select data, such as a user\u2019s in-app assessment score or their path through the game, is collected by the PBS KIDS Measure Up! app, a game-based learning tool.\n\n**What is the PBS KIDS Measure Up! app?**\nIn the PBS KIDS Measure Up! app, children ages 3 to 5 learn early STEM concepts focused on length, width, capacity, and weight while going on an adventure through Treetop City, Magma Peak, and Crystal Caves. Joined by their favorite PBS KIDS characters, children can also collect rewards and unlock digital toys as they play. \n\nBesides the info provided above by Kaggle, I found the following additional info on the website of the app:\n\nSpecific features of Measure Up! include:\n\n* 19 unique measuring games.\n* 10 measurement-focused video clips.\n* Sticker books featuring favorite PBS KIDS characters.\n* Rewards for completion of tasks.\n* Embedded challenges and reports to help parents and caregivers monitor kids\u2019 progress.\n* Ability to track your child's progress using the PBS KIDS Super Vision companion app.\n\n**Evaluation**\nSubmissions are scored based on the quadratic weighted kappa, which measures the agreement between two outcomes. This metric typically varies from 0 (random agreement) to 1 (complete agreement). In the event that there is less agreement than expected by chance, the metric may go below 0.\n\nThe outcomes in this competition are grouped into 4 groups (labeled accuracy_group in the data):\n\n3: the assessment was solved on the first attempt\n\n2: the assessment was solved on the second attempt\n\n1: the assessment was solved after 3 or more attempts\n\n0: the assessment was never solved\n\n\n\nFor each installation_id represented in the test set, you must predict the accuracy_group of the last assessment for that installation_id.\n\nNote that the training set contains many installation_ids which never took assessments, whereas every installation_id in the test set made an attempt on at least one assessment.\n\nThe file train_labels.csv has been provided to show how these groups would be computed on the assessments in the training set. Assessment attempts are captured in event_code 4100 for all assessments except for Bird Measurer, which uses event_code 4110. If the attempt was correct, it contains \"correct\":true.","a86d9be2":"# 4. Feature engineering; adding features to train_labels","8bce921c":"# 1. Understanding the train data","c8bd3f81":"The outcomes in this competition are grouped into 4 groups (labeled accuracy_group in the data):\n\n3: the assessment was solved on the first attempt\n\n2: the assessment was solved on the second attempt\n\n1: the assessment was solved after 3 or more attempts\n\n0: the assessment was never solved\n\n\nI started by visualizing some of these columns","5bb268a3":"As we can not train on those installation_id's anyway, I am taking them out of the train set. This reduces our train set further from 8.3 million rows to 7.7 million.","ecd4610b":"As the match between the train dataframe and the train_labels dataframe is not straightforward, it tried to figure out how these dataframes are to be matched by focussing on just one particular installation_id.","7e2a4f46":"Basically what we need to do is add features to the train_labels dataframe to get our training dataset. Before I get started, I am quickly checking if game_session alone is the unique identifier in train labels.","869a9cb8":"I will now add some new columns based on the timestamp, and visualize these.","e2d8ffff":"The number of unique installations in our \"smaller\" train set is now 4242."}}