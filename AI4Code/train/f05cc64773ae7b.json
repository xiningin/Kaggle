{"cell_type":{"5bf01c18":"code","4baa6455":"code","5b2a2d74":"code","278678f6":"code","f0cc6370":"code","47df2f94":"code","4748b10a":"code","fa862c10":"code","5b04d883":"code","ffff5f8c":"code","f273408b":"code","d7aa62f1":"code","1f71096e":"code","66e5918e":"code","fbe00431":"code","fef5e18e":"code","257d5996":"code","f19e8091":"code","037c5681":"code","61677275":"code","f7aa6eda":"code","eb750f70":"code","782a77ca":"code","f09578b2":"code","f3a43908":"code","9fd2ecc0":"code","afaddd9a":"code","6fe8fc45":"code","baa758d7":"markdown","bcc2b2ad":"markdown","036d3bca":"markdown","4ae02c36":"markdown","b407350e":"markdown","c2da4190":"markdown","62ce61ec":"markdown","d9fb247a":"markdown","4fa81116":"markdown","ca775de7":"markdown","24bc3d92":"markdown","318b27d3":"markdown","862dd63e":"markdown","cccc1fec":"markdown","76b7d41c":"markdown","1392bb37":"markdown","43ca53f7":"markdown"},"source":{"5bf01c18":"import pandas as pd\nimport numpy as np\nimport os\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\n\nimport torchvision\nimport torchvision.transforms as transforms\n\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms, utils\n\nimport ast\nimport cv2\n\nfrom sklearn import model_selection\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils import shuffle\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# Don't Show Warning Messages\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Note: Pytorch uses a channels-first format:\n# [batch_size, num_channels, height, width]\n\nprint(torch.__version__)\nprint(torchvision.__version__)","4baa6455":"# Set the seed values\n\nimport random\n\nseed_val = 101\n\nos.environ['PYTHONHASHSEED'] = str(seed_val)\nrandom.seed(seed_val)\nnp.random.seed(seed_val)\ntorch.manual_seed(seed_val)\ntorch.cuda.manual_seed_all(seed_val)\ntorch.backends.cudnn.deterministic = True","5b2a2d74":"os.listdir('..\/input\/v2-balloon-detection-dataset')","278678f6":"base_path = '..\/input\/v2-balloon-detection-dataset\/'","f0cc6370":"NUM_CORES = os.cpu_count()\nNUM_CORES","47df2f94":"device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\nprint(device)\n\nif torch.cuda.is_available():\n    print('Num GPUs:', torch.cuda.device_count())\n    print('GPU Type:', torch.cuda.get_device_name(0))","4748b10a":"path = base_path + 'balloon-data.csv'\n\ndf_data = pd.read_csv(path)\n\n# Convert bbox column entries from strings to lists\n# \"[........]\" to [......]\ndf_data['bbox'] = df_data['bbox'].apply(ast.literal_eval)\n\nprint(df_data.shape)\n\ndf_data.head()","fa862c10":"# https:\/\/pythonprogramming.net\/drawing-writing-python-opencv-tutorial\/\n# https:\/\/codeyarns.com\/tech\/2015-03-11-fonts-in-opencv.html\n# https:\/\/stackoverflow.com\/questions\/60674501\/how-to-make-black-background-in-cv2-puttext-with-python-opencv\n# https:\/\/www.geeksforgeeks.org\/python-opencv-cv2-puttext-method\/\n# https:\/\/pysource.com\/2018\/01\/22\/drawing-and-writing-on-images-opencv-3-4-with-python-3-tutorial-3\/\n\n\ndef draw_bbox(image, xmin, ymin, xmax, ymax, text=None):\n    \n    \"\"\"\n    This functions draws one bounding box on an image.\n    \n    Input: Image (numpy array)\n    Output: Image with the bounding box drawn in. (numpy array)\n    \n    If there are multiple bounding boxes to draw then simply\n    run this function multiple times on the same image.\n    \n    Set text=None to only draw a bbox without\n    any text or text background.\n    E.g. set text='Balloon' to write a \n    title above the bbox.\n    \n    xmin, ymin --> coords of the top left corner.\n    xmax, ymax --> coords of the bottom right corner.\n    \n    \"\"\"\n\n\n    w = xmax - xmin\n    h = ymax - ymin\n\n    # Draw the bounding box\n    # ......................\n    \n    start_point = (xmin, ymin) \n    end_point = (xmax, ymax) \n    bbox_color = (255, 0, 0) \n    bbox_thickness = 15\n\n    image = cv2.rectangle(image, start_point, end_point, bbox_color, bbox_thickness) \n    \n    \n    \n    # Draw the tbackground behind the text and the text\n    # .................................................\n    \n    # Only do this if text is not None.\n    if text:\n        \n        # Draw the background behind the text\n        text_bground_color = (0,0,0) # black\n        cv2.rectangle(image, (xmin, ymin-150), (xmin+w, ymin), text_bground_color, -1)\n\n        # Draw the text\n        text_color = (255, 255, 255) # white\n        font = cv2.FONT_HERSHEY_DUPLEX\n        origin = (xmin, ymin-30)\n        fontScale = 3\n        thickness = 10\n\n        image = cv2.putText(image, text, origin, font, \n                           fontScale, text_color, thickness, cv2.LINE_AA)\n\n\n\n    return image","5b04d883":"def display_images(df):\n\n    # set up the canvas for the subplots\n    plt.figure(figsize=(20,70))\n\n\n    for i in range(1,13):\n\n        index = i\n\n        # Load an image\n        path = base_path + 'images\/' + df.loc[index, 'fname']\n        image = plt.imread(path)\n        #image = cv2.resize(image, (IMAGE_SIZE, IMAGE_SIZE))\n\n        plt.subplot(10,3,i)\n\n        plt.imshow(image)\n        plt.axis('off')","ffff5f8c":"display_images(df_data)","f273408b":"# set the figsize so the image is larger\nplt.figure(figsize=(8,8))\n\n# Choose an index.\n# Change this number to see different images.\ni = 4   \n\n# Load an image\nfname = df_data.loc[i, 'fname']\n\npath = base_path + 'images\/' + fname\nimage = plt.imread(path)\n\nbbox_list = df_data.loc[i, 'bbox']\n\n# Draw the bboxes on the image\nfor coord_dict in bbox_list:\n    \n    xmin = int(coord_dict['xmin'])\n    ymin = int(coord_dict['ymin'])\n    xmax = int(coord_dict['xmax'])\n    ymax = int(coord_dict['ymax'])\n    \n    image = draw_bbox(image, xmin, ymin, xmax, ymax, text=None)\n\nprint(image.dtype)\nprint(image.min())\nprint(image.max())\nprint(image.shape)\n\nplt.imshow(image)\nplt.axis('off')\nplt.show()\n","d7aa62f1":"df_train, df_val = train_test_split(df_data, test_size=0.2, random_state=101)\n\nprint(df_train.shape)\nprint(df_val.shape)","1f71096e":"# Get the torch version\nimport torch\nprint(torch.__version__)","66e5918e":"# Get the CUDA version\n# The GPU needs to be enabled for this to work.\n\n!nvcc --version","fbe00431":"# Get the CUDA version\n# The GPU needs to be enabled for this to work.\n# The CUDA version is in the top right corner.\n\n! nvidia-smi","fef5e18e":"# Install Dectectron2\n\n# Examples:\n# If the CUDA version is 10.2 the we enter cu102 below.\n# If the CUDA version is 10.0 we enter cu100 below.\n\n# You can add the -q flag and the output will not displayed.\n# !pip install detectron2 -f https:\/\/dl.fbaipublicfiles.com\/detectron2\/wheels\/cu110\/torch1.7\/index.html -q\n\n# Using torch version 1.7 (torch1.7) and cuda version 11.0 (cu110)\n!pip install detectron2 -f https:\/\/dl.fbaipublicfiles.com\/detectron2\/wheels\/cu110\/torch1.7\/index.html","257d5996":"import detectron2\nfrom detectron2.utils.logger import setup_logger\nsetup_logger()\n\nfrom detectron2 import model_zoo\nfrom detectron2.engine import DefaultPredictor\nfrom detectron2.config import get_cfg\n\nfrom detectron2.engine import DefaultTrainer\nfrom detectron2.engine import DefaultPredictor\nfrom detectron2.data import DatasetCatalog\nfrom detectron2.data import MetadataCatalog\n\nfrom detectron2.utils.visualizer import Visualizer\nfrom detectron2.structures import BoxMode\nfrom detectron2.utils.visualizer import ColorMode\n\nimport matplotlib.pyplot as plt\nimport cv2\nimport random","f19e8091":"# Step 1 - Convert train and val data into a list of dictionaries\n# ................................................................\n\n# In order to register the data we will need to write \n# functions that return a list of dictionaries:\n\n#  def train_dataset_function():\n#    ...\n#\n#    return [image_dict_1, image_dict2, ...]\n\n\n#  def val_dataset_function():\n#    ...\n#\n#    return [image_dict_1, image_dict2, ...]\n\n\n\n# What is the format of image_dict and what does it contain?\n# ...........................................................\n\n# Each image_dict in the above list corresponds to one image in the train or val dataset.\n\n\n# This is the format for each image_dict:\n\n# image_dict_1 = {\n#     'file_name': 'The full path to the image file. ',\n#     'image_id': 'A unique id that identifies this image. Can be the index. (str or int)',\n#     'height': 'The height of the image - on which the bbox coords are based (int)',\n#     'width': 'The width of the image - on which the bbox coords are based (int)',\n#    'annotations': [bbox_dict_1, bbox_dict2, bbox_dict3] # for 3 bboxes on the image\n# }\n\n# An image can have multiple bboxes and these bboxes can have different classes.\n# Each bbox dict corresponds to one bounding box.\n\n# Each bbox_dict is a dictionary. The bbox dict format is as follows:\n\n# bbox_dict_1 = {\n#     'bbox': [xmin, ymin, xmax, ymax],\n#     'bbox_mode': BoxMode.XYXY_ABS,\n#     'category_id': 'The label (class) of the bbox. (int)'\n# }\n\n# BoxMode.XYXY_ABS means that the bbox format is [xmin, ymin, xmax, ymax] and\n# the list items are absolute values.\n# It's also possible to use BoxMode.XYWH_ABS. Then the\n# format is [xmin, ymin, width, height].\n\n\n\n\n# Step 2 - Register the train and val data\n# .........................................\n\n\n# The functions, defined at the top, will then be used to register the data.\n# This is how the train and val data are registered:\n\n\n# thing_classes = ['dog', 'cat']\n# thing_colors = [(255, 0, 0), (0, 0, 255)]\n\n# rgb colours:\n# red is (255, 0, 0)\n# blue is (0, 0, 255)\n\n# from detectron2.data import DatasetCatalog\n# DatasetCatalog.register(\"train_dataset\", train_dataset_function)\n# DatasetCatalog.register(\"val_dataset\", val_dataset_function)\n\n# from detectron2.data import MetadataCatalog\n# MetadataCatalog.get(\"train_dataset\").thing_classes=thing_classes\n# MetadataCatalog.get(\"val_dataset\").thing_classes=thing_classes\n\n# If we don't assign a color to each class then the model will\n# use random colors when displaying a prediction. Now, for example, all dogs will\n# appear red and all cats will appear blue.\n\n# MetadataCatalog.get(\"train_dataset\").thing_colors=thing_colors\n# MetadataCatalog.get(\"val_dataset\").thing_colors=thing_colors","037c5681":"def train_dataset_function():\n    \n    df = df_train\n    \n    image_dict_list = []\n    \n    for i in range(0, len(df)):\n    \n        # Load an image\n        fname = df.loc[i, 'fname']\n        \n        bbox_list =  df.loc[i, 'bbox']\n        height = df.loc[i, 'height']\n        width = df.loc[i, 'width']\n        \n\n        path = base_path + 'images\/' + fname\n        image_id = i\n        \n        target = 0\n        \n        annotations_list = []\n        \n        for box in bbox_list:\n            \n            \n            xmin = int(round(box['xmin']))\n            ymin = int(round(box['ymin']))\n            xmax = int(round(box['xmax']))\n            ymax = int(round(box['ymax']))\n            \n            target = 0 # there's only one class.\n            \n            bbox_dict = {\n            'bbox': [xmin, ymin, xmax, ymax],\n            'bbox_mode': BoxMode.XYXY_ABS,\n            'category_id': target\n            }\n            \n            annotations_list.append(bbox_dict)\n            \n            \n        image_dict = {\n            'file_name': path,\n            'image_id': image_id,\n            'height': height,\n            'width': width,\n            'annotations': annotations_list\n        }\n        \n        image_dict_list.append(image_dict)\n        \n        \n    return image_dict_list\n        \n        \ndef val_dataset_function():\n    \n    df = df_val\n    \n    image_dict_list = []\n    \n    for i in range(0, len(df)):\n    \n        # Load an image\n        fname = df.loc[i, 'fname']\n        \n        bbox_list =  df.loc[i, 'bbox']\n        height = df.loc[i, 'height']\n        width = df.loc[i, 'width']\n        \n\n        path = base_path + 'images\/' + fname\n        image_id = i\n        \n        \n        annotations_list = []\n        \n        for box in bbox_list:\n            \n            \n            xmin = int(round(box['xmin']))\n            ymin = int(round(box['ymin']))\n            xmax = int(round(box['xmax']))\n            ymax = int(round(box['ymax']))\n            \n            target = 0 # there's only one class.\n            \n            bbox_dict = {\n            'bbox': [xmin, ymin, xmax, ymax],\n            'bbox_mode': BoxMode.XYXY_ABS,\n            'category_id': target\n            }\n            \n            annotations_list.append(bbox_dict)\n            \n            \n        image_dict = {\n            'file_name': path,\n            'image_id': image_id,\n            'height': height,\n            'width': width,\n            'annotations': annotations_list\n        }\n        \n        image_dict_list.append(image_dict)\n        \n        \n    return image_dict_list","61677275":"# Test the functions\n\ndf_train = df_train.reset_index(drop=True)\ndf_val = df_val.reset_index(drop=True)\n\ntrain_image_dict_list = train_dataset_function()\nval_image_dict_list = val_dataset_function()\n\nprint(len(train_image_dict_list))\nprint(len(val_image_dict_list))","f7aa6eda":"# Register the train and val data\n\n# add all the classes to a list\nthing_classes = ['Balloon']\n\nDatasetCatalog.register(\"train_dataset\", train_dataset_function)\nDatasetCatalog.register(\"val_dataset\", val_dataset_function)\n\nMetadataCatalog.get(\"train_dataset\").thing_classes=thing_classes\nMetadataCatalog.get(\"val_dataset\").thing_classes=thing_classes","eb750f70":"# Check that the data registration process worked\n\ntrain_dataset_dicts = train_dataset_function()\ntrain_metadata = MetadataCatalog.get(\"train_dataset\")\n\ni = 4\nd = train_dataset_dicts[i]\n\nfname = d['file_name'].split('\/')[1]\nprint(fname)\n\nimage = cv2.imread(d['file_name'])\n\nvisualizer = Visualizer(image[:, :, ::-1], metadata=train_metadata, scale=0.5)\nout = visualizer.draw_dataset_dict(d)\nplt.imshow(out.get_image()[:, :, ::-1])","782a77ca":"# How to choose a model\n# ----------------------\n\n# 1- Go to the Detectron2 \"configs\" page on Github.\n# https:\/\/github.com\/facebookresearch\/detectron2\/tree\/master\/configs\n\n# 2- Click on the task folder e.g. if you want to do object detection then\n# click on the \"COCO-Detection\" folder. You will see a list of all models\n# that have been trained on the COCO dataset.\n\n# 3- Choose a model and take note of its yaml file name.\n\n\n# This is how to load a model.\n# .............................\n# Say you chose faster_rcnn_R_101_FPN_3x.yaml:\n# Add the name of the model to the config\n# (COCO-Detection is the Github folder where the model is stored.)\n\n# from detectron2.config import get_cfg\n# cfg = get_cfg()\n\n# cfg.merge_from_file(model_zoo.get_config_file(\"COCO-Detection\/faster_rcnn_R_101_FPN_3x.yaml\"))\n# cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-Detection\/faster_rcnn_R_101_FPN_3x.yaml\")\n\n\n# This is another way to load a model but I don't know\n# in what context this method would be used.\n# Shown here: https:\/\/paperswithcode.com\/lib\/detectron2\/faster-r-cnn\n\n# from detectron2 import model_zoo\n# model = model_zoo.get(\"COCO-Detection\/faster_rcnn_R_50_C4_1x.yaml\", trained=True)","f09578b2":"# Create a config object.\ncfg = get_cfg()\n\n# Change the config\ncfg.merge_from_file(model_zoo.get_config_file(\"COCO-Detection\/faster_rcnn_R_101_FPN_3x.yaml\"))\ncfg.DATASETS.TRAIN = (\"train_dataset\",)\ncfg.DATASETS.TEST = ()   # Not using the validation data during training.\ncfg.DATALOADER.NUM_WORKERS = NUM_CORES\ncfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-Detection\/faster_rcnn_R_101_FPN_3x.yaml\")\ncfg.SOLVER.IMS_PER_BATCH = 2\ncfg.SOLVER.BASE_LR = 0.00025\ncfg.SOLVER.MAX_ITER = 500\ncfg.SOLVER.STEPS = []        \ncfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 128   \ncfg.MODEL.ROI_HEADS.NUM_CLASSES = 1  # only has one class (ballon)\n\n# Uncomment this line to see all fields in the config.\n# print(cfg)","f3a43908":"# Train the model\nos.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\ntrainer = DefaultTrainer(cfg) \ntrainer.resume_or_load(resume=False)\ntrainer.train()","9fd2ecc0":"# Change the config\ncfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, \"model_final.pth\")\ncfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.7   # set the testing threshold for this model\ncfg.DATASETS.TEST = (\"val_dataset\", )\n\n# Create a predictor\npredictor = DefaultPredictor(cfg)","afaddd9a":"# Get the metadata\nval_dataset_dicts = val_dataset_function()\nval_metadata = MetadataCatalog.get(\"val_dataset\")\n\n# Get an image to predict on.\n# Change this number to select a different image from the val set.\ni = 0\n\nd = val_dataset_dicts[i] \nim = cv2.imread(d[\"file_name\"])\n\n# make a prediction\noutputs = predictor(im)\n\noutputs","6fe8fc45":"# Visulaize the predicted bounding boxes\n\n# Keep scale=1 in order to associate the predicted coords with the image below.\n# If the scale is different the predicted coords won't match the bbox shown on\n# the image below.\nv = Visualizer(im[:, :, ::-1], metadata=val_metadata, scale=1) \nv = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n\n\nplt.figure(figsize = (14, 10))\nplt.imshow(cv2.cvtColor(v.get_image()[:, :, ::-1], cv2.COLOR_BGR2RGB))\nplt.show()","baa758d7":"## Choose a model\n\nFor example, there are several Faster R-CNN models available in Detectron2, with different backbones and learning schedules.<br>\nhttps:\/\/paperswithcode.com\/lib\/detectron2\/faster-r-cnn\n\nClick this link and then use the selector at the top left to choose a variant of faster-rcnn. You can see info on all the different variants. Also click the \"SHOW MORE' link.<br>\nhttps:\/\/paperswithcode.com\/lib\/detectron2\/faster-r-cnn","bcc2b2ad":"Thank you for reading.","036d3bca":"## Make a prediction","4ae02c36":"## References and Resources\n\n- Balloon training code by Gilbert Tanner<br>\n(Includes code to parse the original Balloon data json files.)<br>\nhttps:\/\/github.com\/TannerGilbert\/Object-Detection-and-Image-Segmentation-with-Detectron2\/blob\/master\/Detectron2_train_on_a_custom_dataset.ipynb\n\n- VinBigData detectron2 train notebook by corochann<br>\n(Includes info on data augmentation and on evaluation during training.)\nhttps:\/\/www.kaggle.com\/corochann\/vinbigdata-detectron2-train\n\n- Video: Detectron2 - Object Detection with PyTorch by Gilbert Tanner<br>\nhttps:\/\/www.youtube.com\/watch?v=CrEW8AfVlKQ\n\n- Video: Using Machine Learning with Detectron2 by Facebook Open Source<br>\n(Building an \u2018I spy\u2019 colour app.)<br>\nhttps:\/\/www.youtube.com\/watch?v=eUSgtfK4ivk\n\n- Detectron2 github repo<br>\nhttps:\/\/github.com\/facebookresearch\/detectron2\n\n- Detecron2 docs<br>\nhttps:\/\/detectron2.readthedocs.io\/en\/latest\/tutorials\/getting_started.html\n\n\n\n","b407350e":"## Train the model","c2da4190":"## Display a few images","62ce61ec":"## Create train and val data","d9fb247a":"## Install Dectectron2\u00b6\nIn order to install Detectron2 we need to know:\n\n- the torch version that's installed and,\n- the CUDA version that's installed.","4fa81116":"## Load the balloon data","ca775de7":"## Display one image with bounding boxes","24bc3d92":"### Basics of Detectron2 - Balloon detection\n\nby @vbookshelf<br>\n10 June 2021","318b27d3":"## Define the device","862dd63e":"- Model Zoo<br>\nhttps:\/\/github.com\/facebookresearch\/detectron2\/blob\/master\/MODEL_ZOO.md\n- Model selection page (click the folder for a specific model):<br>\nhttps:\/\/github.com\/facebookresearch\/detectron2\/tree\/master\/configs\n- For example, these are object detection models trained on Coco - fast_rcnn, retinanet etc.<br>\nhttps:\/\/github.com\/facebookresearch\/detectron2\/tree\/master\/configs\/COCO-Detection","cccc1fec":"## Conclusion\n\nOn the plus side Detectron2 is easy to use and one can try different models. However, I don't feel comfortable using it as a black box. I prefer a more flexible and transparent training process where I understand what's going on - like in Pytorch. For example, how does one get the model to print out a validation loss during training?\n\nAccording to the Dectectron2 docs it is in fact possible to control the entire training loop, but it's not yet clear to me how to set this up. If you find an example, please link to it in the comments below.","76b7d41c":"## Register the image data\nRef: https:\/\/detectron2.readthedocs.io\/en\/latest\/tutorials\/datasets.html\n\n**Quote from the docs:**<br>\nIf \"annotations\" is an empty list, it means the image is labeled to have no objects. Such images will by default be removed from training, but can be included using DATALOADER.FILTER_EMPTY_ANNOTATIONS.","1392bb37":"## Introduction\n\nRecently I've started exploring Detectron2. In this notebook I will apply what I've learned to create a model that detects balloons on images.\n\nThese are some of the questions that this notebook will answer:\n\n- What does the step by step workflow look like?\n- How does one feed data into the model?\n- What format does the input data need to have?\n- What is the format of the model's output?\n\nLet's get started.\n","43ca53f7":"## Helper functions"}}