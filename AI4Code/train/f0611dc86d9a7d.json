{"cell_type":{"7995bcaf":"code","89e2f494":"code","fb46c448":"code","382847ec":"code","38c436bc":"code","393009ae":"code","0876c577":"code","ec9dd1d4":"code","5291b633":"code","558bdde2":"code","670abe38":"code","6f3b3492":"code","489cbc92":"code","29499aef":"code","09f23892":"code","72ada8da":"code","d7c07c62":"markdown","162b1ba7":"markdown"},"source":{"7995bcaf":"!pip install --upgrade scikit-learn","89e2f494":"import numpy as np \nimport pandas as pd \nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import roc_auc_score\nfrom skimage.filters import threshold_otsu\nimport lightgbm as lgb\nfrom lightgbm import LGBMClassifier\nimport gc\nfrom tqdm import tqdm\n\nSEED = 0","fb46c448":"train = pd.read_csv(\"\/kaggle\/input\/tabular-playground-series-nov-2021\/train.csv\", index_col='id')\ntest = pd.read_csv(\"\/kaggle\/input\/tabular-playground-series-nov-2021\/test.csv\", index_col='id')","382847ec":"pointy = [0,2,4,9,12,16,19,20,21,23,24,27,28,30,31,32,33,35,39,42,44,46,48,49,51,52,53,56,58,59,60,61,62,63,64,68,69,72,73,75,76,78,79,81,83,84,87,88,89,90,92,93,94,95,98,99]\nbimodal = [x for x in range(0,100) if x not in pointy]\n\npointy = list(map(lambda x: 'f'+str(x), pointy))\nbimodal = list(map(lambda x: 'f'+str(x), bimodal))\n\nfeatures = [x for x in train.columns.values if x[0]==\"f\"]","38c436bc":"def create_features(df, cols, prefix='new_'):\n    df[prefix+'abs_sum'] = df[cols].abs().sum(axis=1)\n    df[prefix+'sem'] = df[cols].sem(axis=1)\n    df[prefix+'std'] = df[cols].std(axis=1)\n    df[prefix+'avg'] = df[cols].mean(axis=1)\n    df[prefix+'max'] = df[cols].max(axis=1)\n    df[prefix+'min'] = df[cols].min(axis=1)\n    \n    return df","393009ae":"train = create_features(train, pointy, 'point_')\ntrain = create_features(train, bimodal, 'bimodal_')\ntest = create_features(test, pointy, 'point_')\ntest = create_features(test, bimodal, 'bimodal_')","0876c577":"def check_peak(df, test_df, cols, suffix='_peak'):\n    for col in cols:\n        peak = threshold_otsu(df[col])\n        df[str(col)+suffix] = df[col] > peak\n        test_df[str(col)+suffix] = test_df[col] > peak","ec9dd1d4":"check_peak(train, test, bimodal)","5291b633":"test.head()","558bdde2":"X = train.drop([\"target\"], axis=1)\nX_test = test\ny = train[\"target\"]","670abe38":"scaler = RobustScaler()\nX = scaler.fit_transform(X)\nX_test = scaler.transform(X_test)","6f3b3492":"del test, train, scaler\ngc.collect()","489cbc92":"from sklearn.calibration import CalibratedClassifierCV\nfrom sklearn.svm import LinearSVC","29499aef":"%%time\nmodel = CalibratedClassifierCV(LinearSVC(dual=False, verbose=1, random_state=SEED), cv=5, n_jobs=-1)\nmodel.fit(X, y)\n\ngc.collect()","09f23892":"preds = model.predict_proba(X_test)[:,1]","72ada8da":"submission = pd.read_csv('..\/input\/tabular-playground-series-nov-2021\/sample_submission.csv', index_col='id')\nsubmission['target'] = preds\nsubmission.to_csv('submission.csv')","d7c07c62":"## contribution 1\nI pretty much looked at whether the distribution was a unimodal point, or a bimodal distribution\nThen split the features according to which distribution it was under and then apply feature engineering to each","162b1ba7":"## contribution 2\nThe bimodal distributions clearly influence the target. We can create a boolean comparison as to which peak it sits under\n\nSee: www.kaggle.com\/realtimshady\/eda-feature-exploration"}}