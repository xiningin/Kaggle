{"cell_type":{"721172a5":"code","125bcf83":"code","6af79708":"code","1aa4696b":"code","2e37e1ce":"code","834a28ed":"code","bf352805":"code","fe85808b":"code","eacc8536":"code","ef99a6d2":"code","6d445e73":"code","8e61a64b":"code","fc0aad36":"code","62a8823e":"code","80aa0946":"code","23b561d6":"code","f5b17c4f":"code","7cc1166c":"code","b1854794":"code","10d4cd59":"code","38cb9dfd":"code","92c06124":"code","8ec02e7d":"code","803944b6":"code","06a9bcb9":"code","6164f534":"code","2fb2eaa1":"code","19ed5904":"code","89f262b6":"code","5486ae79":"code","1abcbd90":"code","4149be6d":"code","e5cb8ece":"code","dd63ccb6":"code","32c2b5ae":"code","f41e610a":"code","4f8ffdf9":"code","736fc79f":"code","e706e5bc":"code","6db90c55":"code","3b8d9e1d":"code","deb6d3f3":"code","3c240e32":"code","08e0f9bf":"code","1f642c8e":"code","f02938d9":"code","c473e8ff":"code","3b7654bf":"code","15535b4c":"code","fe9fa05a":"code","a88d7a7f":"code","170821f5":"code","51ca765e":"code","3a9b7333":"code","8c508faa":"code","62a63a42":"code","15b39e99":"code","470fc99e":"code","72eadcb1":"code","66dcb165":"code","60c447e6":"code","76126812":"code","d9d84a65":"code","0aff80db":"code","e35f631e":"code","8ed0622a":"code","2e646dc1":"code","bbba1fd0":"code","804c5fe3":"code","c86b89d3":"code","e2800f05":"code","8d3d961c":"code","e06f47f5":"code","a93d11b4":"code","3a188bf9":"code","b4dde955":"code","d0032a57":"code","949b3a35":"code","cd6fae8f":"markdown","1878b249":"markdown","cbabc591":"markdown","71b27b6b":"markdown"},"source":{"721172a5":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","125bcf83":"import seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline","6af79708":"train=pd.read_csv(\"..\/input\/train.csv\")\ntest=pd.read_csv(\"..\/input\/test.csv\")","1aa4696b":"train.shape,test.shape","2e37e1ce":"train.head()","834a28ed":"train.describe(include='all')","bf352805":"train.isnull().sum(), test.isnull().sum()\n#full_data=[train,test], full_data[1].shape,full_data[0].shape","fe85808b":"train.columns","eacc8536":"train['Sex'].unique() #pd.unique(train['Sex']) , np.unique(train['Sex'])","ef99a6d2":"np.unique(train['Sex'])","6d445e73":"from sklearn.preprocessing import OneHotEncoder, LabelEncoder","8e61a64b":"label=LabelEncoder()\ntrain['Sex_code']=label.fit_transform(train['Sex'])","fc0aad36":"train['Sex_code'].unique()","62a8823e":"train.groupby('Pclass').size()","80aa0946":"train.groupby('Pclass').count()","23b561d6":"print(train[['Pclass','Survived']].groupby(['Pclass'],as_index=True).size())","f5b17c4f":"print(train[train['Survived']==1].groupby(['Pclass'],as_index=False).size())","7cc1166c":"print(train[['Pclass','Survived']].groupby(['Pclass'],as_index=False).mean())","b1854794":"print(train[['Sex','Survived']].groupby(['Sex'],as_index=False).count())","10d4cd59":"print(train[train['Survived']==1].groupby(['Sex'],as_index=False).size())","38cb9dfd":"print(train[['SibSp','Survived']].groupby(['SibSp'],as_index=False).count())","92c06124":"print(train[train['Survived']==1].groupby(['SibSp'],as_index=False).size())","8ec02e7d":"print(train[['Embarked','Survived']].groupby(['Embarked'],as_index=False).size())","803944b6":"print(train[train['Survived']==1].groupby(['Embarked'],as_index=False).size())","06a9bcb9":"sns.factorplot('Pclass','Survived', data=train,size=4,aspect=3)","6164f534":"g=sns.FacetGrid(train,col='Survived')\ng.map(plt.hist,'Age',bins=20,color='m')","2fb2eaa1":"g=sns.FacetGrid(train,col='Survived')\ng.map(plt.scatter,'Fare','Age',edgecolor='w')","19ed5904":"grid=sns.FacetGrid(train,col='Survived',row='Pclass',size=3,aspect=1.2)\ngrid.map(plt.hist,'Age',alpha=.7,bins=20).add_legend()","89f262b6":"g1=sns.FacetGrid(train,row='Embarked')\ng1.map(sns.pointplot,'Pclass','Survived','Sex',palette='deep').add_legend()","5486ae79":"g2=sns.FacetGrid(train,row='Embarked',col='Survived')\ng2.map(sns.barplot,'Sex','Fare').add_legend()","1abcbd90":"train['Sex'] = train['Sex'].map( {'female': 1, 'male': 0} ).astype(int)\ntest['Sex'] = test['Sex'].map( {'female': 1, 'male': 0} ).astype(int)","4149be6d":"train['Embarked'] = train['Embarked'].fillna('S')\ntrain['Embarked'] = train['Embarked'].map( {'S': 0, 'C': 1, 'Q':2} ).astype(int)","e5cb8ece":"test['Embarked'] = test['Embarked'].fillna('S')\ntest['Embarked'] = test['Embarked'].map( {'S': 0, 'C': 1, 'Q':2} ).astype(int)","dd63ccb6":"grid=sns.FacetGrid(train,col='Pclass',row='Sex',size=3,aspect=1.2)\ngrid.map(plt.hist,'Age',alpha=.7,bins=20).add_legend()","32c2b5ae":"avg_age=train['Age'].mean()\nstd_age=train['Age'].std()\nage_null_count=train['Age'].isnull().sum()\nage_null_random_list = np.random.randint(avg_age - std_age, avg_age + std_age, size=age_null_count)\ntrain['Age'][np.isnan(train['Age'])] = age_null_random_list\ntrain['Age']=train['Age'].astype(int)\ntrain['cat_age']=pd.cut(train['Age'],5)","f41e610a":"avg_age,std_age,age_null_count\n#age_null_random_list","4f8ffdf9":"avg_age=test['Age'].mean()\nstd_age=test['Age'].std()\nage_null_count=test['Age'].isnull().sum()\nage_null_random_list = np.random.randint(avg_age - std_age, avg_age + std_age, size=age_null_count)\ntest['Age'][np.isnan(test['Age'])] = age_null_random_list\ntest['Age']=test['Age'].astype(int)\ntest['cat_age']=pd.cut(test['Age'],5)","736fc79f":"test.loc[ test['Age'] <= 16, 'Age'] = 0\ntest.loc[(test['Age'] > 16) & (test['Age'] <= 32), 'Age'] = 1\ntest.loc[(test['Age'] > 32) & (test['Age'] <= 48), 'Age'] = 2\ntest.loc[(test['Age'] > 48) & (test['Age'] <= 64), 'Age'] = 3\ntest.loc[ test['Age'] > 64, 'Age'] = 4","e706e5bc":"test['Sex'].unique()","6db90c55":"train.groupby('Pclass').count()","3b8d9e1d":"test.groupby('Pclass').count()","deb6d3f3":"train","3c240e32":"train.loc[ train['Age'] <= 16, 'Age'] = 0\ntrain.loc[(train['Age'] > 16) & (train['Age'] <= 32), 'Age'] = 1\ntrain.loc[(train['Age'] > 32) & (train['Age'] <= 48), 'Age'] = 2\ntrain.loc[(train['Age'] > 48) & (train['Age'] <= 64), 'Age'] = 3\ntrain.loc[ train['Age'] > 64, 'Age'] = 4","08e0f9bf":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier","1f642c8e":"rf=RandomForestClassifier()","f02938d9":"y1=train['Survived']#.ravel()\nx2=train.drop(['Survived','Name','Ticket','Cabin','cat_age'],axis=1)\n#x1=x2.values","c473e8ff":"y1.shape,x2.shape,x2.columns","3b7654bf":"#def feature_importances(self,x1,y1):\n#       print(self.clf.fit(x1,y1).feature_importances_)","15535b4c":"def feature_selection(x2,y1):\n    clf = ExtraTreesClassifier(n_estimators=10)\n    #clf = clf.fit(X_train.iloc[0:780269,:], np.ravel(y_train.iloc[0:780269,:]))\n    clf = clf.fit(x2,np.ravel(y1))\n    feat_importances = pd.Series(clf.feature_importances_, index=x2.columns)\n    dict={'A':clf.feature_importances_,'B':x2.columns}\n    df_new=pd.DataFrame(dict)\n    #df_new.to_csv('..\/working\/fet.csv')\n    print(df_new)","fe9fa05a":"feature_selection(x2,y1)","a88d7a7f":"#rf.fit(x1,y1)\n#rf.feature_importances(x1,y1)\n#print (zip(map(lambda x2: round(x2, 4), rf.feature_importances_), x2.columns) )             ","170821f5":"#X_train = train.drop(\"Survived\",axis=1)\n#Y_train = train[\"Survived\"]\n#X_test  = test.drop(\"PassengerId\",axis=1).copy()","51ca765e":"train.shape,test.shape","3a9b7333":"X_train = train.drop(['Ticket','Cabin','cat_age','Name','PassengerId','Survived','Sex_code'],axis=1)\nY_train = train['Survived']\nX_test  = test.drop(['Ticket','Cabin','Name','PassengerId','cat_age'],axis=1)","8c508faa":"X_train.shape,X_test.shape,Y_train.shape,test.shape,train.shape","62a63a42":"X_train.columns,X_test.columns","15b39e99":"from sklearn.model_selection import train_test_split","470fc99e":"x_train,x_test,y_train,y_test=train_test_split(X_train,Y_train,test_size=0.20,random_state=42)","72eadcb1":"x_train.shape,x_test.shape,y_train.shape,y_test.shape,X_test.shape","66dcb165":"import tensorflow as tf\nfrom keras.utils import to_categorical ","60c447e6":"classes=2\ny_train=to_categorical(y_train,num_classes = classes)\ny_test=to_categorical(y_test,num_classes = classes)","76126812":"y_train[0]","d9d84a65":"epochs=2\nbatch_size=32\ndisplay_progress=40\nwt_init=tf.contrib.layers.xavier_initializer()","0aff80db":"n_input=7\nn_dense_1=32\nn_dense_2=32\nn_classes=2","e35f631e":"x=tf.placeholder(tf.float32,[None,n_input])\ny=tf.placeholder(tf.float32,[None,n_classes])","8ed0622a":"def dense(x,W,b):\n    z=tf.add(tf.matmul(x,W),b)\n    a=tf.nn.relu(z)\n    return a","2e646dc1":"def network(x,weights,biases):\n    dense_1=dense(x,weights['W1'],biases['b1'])\n    dense_2=dense(dense_1,weights['W2'],biases['b2'])\n    output_layer_z=tf.add(tf.matmul(dense_2,weights['W_out']),biases['b_out'])\n    return output_layer_z","bbba1fd0":"bias_dict={\n    'b1':tf.Variable(tf.zeros([n_dense_1])),\n    'b2':tf.Variable(tf.zeros([n_dense_2])),\n    'b_out':tf.Variable(tf.zeros([n_classes]))\n}\n\nweight_dict={\n    'W1':tf.get_variable('W1',[n_input,n_dense_1],initializer=wt_init),\n    'W2':tf.get_variable('W2',[n_dense_1,n_dense_2],initializer=wt_init),\n    'W_out':tf.get_variable('W_out',[n_dense_2,n_classes],initializer=wt_init)\n}","804c5fe3":"predictions=network(x,weights=weight_dict ,biases=bias_dict)","c86b89d3":"print(predictions.shape),print(y.shape)","e2800f05":"cost=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=predictions ,labels=y))\noptimizer=tf.train.AdamOptimizer().minimize(cost)","8d3d961c":"correct_prediction=tf.equal(tf.argmax(predictions,1),tf.argmax(y,1))\naccuracy_pct=tf.reduce_mean(tf.cast(correct_prediction,tf.float32))*100","e06f47f5":"init=tf.global_variables_initializer()","a93d11b4":"predict=tf.argmax(predictions,1)","3a188bf9":"with tf.Session() as session:\n    session.run(init)\n    \n    print(\"Training for\", epochs, \"epochs.\")\n    \n    # loop over epochs: \n    for epoch in range(epochs):\n        \n        avg_cost = 0.0 # track cost to monitor performance during training\n        avg_accuracy_pct = 0.0\n        \n        # loop over all batches of the epoch:\n        n_batches = int(x_train.shape[0] \/ batch_size)\n        #batchnumber=0\n        for i in range(n_batches):\n            \n            # batch_x, batch_y = mnist.train.next_batch(batch_size)\n            #batchnumber= batchnumber+1\n            batch_start_idx = (i * batch_size) % (x_train.shape[0] - batch_size)\n            batch_end_idx = batch_start_idx + batch_size\n            batch_X = x_train[batch_start_idx:batch_end_idx]\n            batch_Y = y_train[batch_start_idx:batch_end_idx]\n            \n            # feed batch data to run optimization and fetching cost and accuracy: \n            _, batch_cost, batch_acc, Predict = session.run([optimizer, cost, accuracy_pct, predictions], \n                                                   feed_dict={x: batch_X, y: batch_Y})\n            \n            # accumulate mean loss and accuracy over epoch: \n            avg_cost += batch_cost \/ n_batches\n            avg_accuracy_pct += batch_acc \/ n_batches\n            \n        # output logs at end of each epoch of training:\n        print(\"Epoch \", '%03d' % (epoch+1), \n              \": cost = \", '{:.3f}'.format(avg_cost), \n              \", accuracy = \", '{:.2f}'.format(avg_accuracy_pct), \"%\", \n              sep='')\n    \n    print(\"Training Complete. Testing Model.\\n\")\n    \n    test_cost = cost.eval({x: x_test, y: y_test})\n    test_accuracy_pct = accuracy_pct.eval({x: x_test, y: y_test})\n    \n    print(\"Test Cost:\", '{:.3f}'.format(test_cost))\n    print(\"Test Accuracy: \", '{:.2f}'.format(test_accuracy_pct), \"%\", sep='')\n    \n    predicted_lables = predict.eval({x: X_test})\n    print(len(predicted_lables))\n    #predicted_lables = np.zeros(X_test.shape[0])\n    #for i in range(0,X_test.shape[0]\/\/batch_size):\n        #predicted_lables[i*batch_size : (i+1)*batch_size] = predict.eval({x: X_test[i*batch_size : (i+1)*batch_size], \n                                       ","b4dde955":"X_test.shape[0]","d0032a57":"predicted_lables[0]","949b3a35":"submission = pd.DataFrame({\n        \"PassengerId\": test[\"PassengerId\"],\n        \"Survived\": predicted_lables\n    })\nsubmission.to_csv('titanic.csv', index=False)","cd6fae8f":"Get the count of Null values in each column.","1878b249":"Alpha is the brightness 0.1 darkest and 0.9 brightest.\nsize size of the graph 1 is smaller 10 is bigger. bins-number of division along x axis","cbabc591":" Difference between count and size, count() applies the function to each column, returning the number of not null records within each.","71b27b6b":"To get the unique records from the column"}}