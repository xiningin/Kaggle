{"cell_type":{"b9e3c8be":"code","acba615f":"code","f3f4f60a":"code","071a70bd":"code","670e1a76":"code","1789b155":"code","81cb4099":"code","e85ccb91":"code","5bc00195":"code","cdd1705c":"code","8e85a344":"code","7cfedee6":"code","ba819a13":"code","daa7b4d0":"code","f8fc19e2":"code","e97c63af":"code","0e813721":"code","c19d6dc1":"code","9f94dfac":"code","c251951b":"code","bccbe821":"code","4bebc355":"code","532f8479":"code","b02bf2de":"code","159ee5b9":"code","288d50ea":"code","bf54267b":"code","c228e12c":"code","2bd2a6dc":"code","e0cb3dcc":"code","ebb47d21":"code","a6d2bec5":"code","b5466ca6":"code","ddbf5463":"code","e283b6c1":"code","454447c3":"code","c540e1a6":"code","d5084f82":"code","faf2c907":"code","1c4cb6d5":"code","e17ff3c4":"code","c9b898e9":"code","145bd436":"code","709c030e":"code","d752ef48":"code","6901f4af":"code","5143f687":"code","8bd051b2":"code","e23448ce":"code","61535094":"code","e627396b":"code","004dddcf":"code","8f4592c4":"code","865858a2":"code","4735ac86":"code","44e46537":"code","d57a3b28":"code","a4e7515a":"code","a557f3e1":"code","211f3c42":"code","27a147a9":"code","18443b63":"code","b5a0cb37":"code","fea92be0":"code","835a0f5a":"code","ebec974d":"code","1c7185b4":"markdown","33a18ee1":"markdown","e9209ba5":"markdown","c19c2539":"markdown","3e1c7b7b":"markdown","7c29be56":"markdown"},"source":{"b9e3c8be":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","acba615f":"import seaborn as sns\nimport matplotlib.pyplot as plt","f3f4f60a":"df = pd.read_csv('\/kaggle\/input\/shark-attacks\/attacks.csv')","071a70bd":"df.head()\n# right off, it seems the Case Number and Year columns are redundant... we'll drop them","670e1a76":"df.drop(['Case Number', 'Year'], axis =1, inplace= True)","1789b155":"df.info()","81cb4099":"# similarly, the columns : 'Investigator' 'pdf', 'href formula', 'href', 'Case Number.1', 'Case Number.2'\n# and 'original order' contain either redundant or irrelevant information for prediction \n# purposes. The columns: 'Investigator' 'pdf', 'href formula', 'href',\n# would be helpful to dig into the dataset more\n# For exploratory purposes, we drop these columns.\ndf.columns","e85ccb91":"df.drop(['Name','Investigator or Source','pdf', 'href formula', 'href',\n       'Case Number.1', 'Case Number.2', 'original order'], axis = 1, inplace = True)","5bc00195":"# this dataset is full of null values. We cannot impute as the data is text based. \ndf.isnull().sum()\/len(df)\n#Let's see where the null values lie in the data","cdd1705c":"sns.heatmap(df.isnull(), yticklabels = False,cbar = False, cmap = 'viridis')\n# Yellow is NaN here. A huge swath of this data is missing. Let's first just remove all rows\n # that only contain the enrty Nan","8e85a344":"df.dropna(axis = 0, how = 'all', inplace = True)","7cfedee6":"sns.heatmap(df.isnull(), yticklabels = False,cbar = False, cmap = 'viridis')\n# Now it is clear that the 'Age', 'Time' and 'Species' columns are missing a lot information","ba819a13":"# There are now at least two approaches. The first approach is to delete the 'Age','Time' and\n#'Species' column, then delete remaining rows with NaN and proceed. \n\n# The species and time columns are interesting features...is there a shark that attacks \n#humans more frequently? What time do these attacks occur?\n\n# For now, we'll take the first approach for sanity sake!","daa7b4d0":"# copies of dataframe up until now\n# data is dropped according to discussion above\ndf1 = df.copy(deep = True)\ndf1.drop(['Age', 'Time', 'Species '], axis = 1, inplace = True)\ndf1.dropna(axis = 0, how = 'any', inplace = True)","f8fc19e2":"#df1 now has no missing data (it is not clean though!)\nsns.heatmap(df1.isnull(), yticklabels= False, cbar = False, cmap = 'viridis')","e97c63af":"df1.info()\n# 4601 entries by first removing 'Age', 'Time' and 'Species' columns","0e813721":"# fatal data column is not neatly classified\nsns.countplot(x = df1['Fatal (Y\/N)'], data = df1)","c19d6dc1":"# appears that six non-fatal encounters are classified as ' N'. we need to group \n# these with the 'N' category and get rid of the 'UNKNOWN' and '2017' entrys.\ndf1['Fatal (Y\/N)'].value_counts()","9f94dfac":"df1[df1['Fatal (Y\/N)'] == ' N']['Fatal (Y\/N)'].iloc[0:5]","c251951b":"# replacing ' N' values with 'N' in fatal column\nif df1[df1['Fatal (Y\/N)'] == ' N']['Fatal (Y\/N)'].iloc[0]:\n    df1.replace(to_replace = df1[df1['Fatal (Y\/N)'] == ' N']['Fatal (Y\/N)'].iloc[0],\\\n    value = 'N',inplace = True)\n    \ndf1['Fatal (Y\/N)'].value_counts()","bccbe821":"df1[df1['Fatal (Y\/N)'] == 'UNKNOWN']['Fatal (Y\/N)'].index.values","4bebc355":"# As we do not know what the 'UNKNOWN' and '2017' labels correspond to, we'll have to\n# drop these rows\ndf1.drop(index = df1[df1['Fatal (Y\/N)'] == 'UNKNOWN']['Fatal (Y\/N)'].index.values,\\\n        inplace = True)\n\ndf1.drop(index = df1[df1['Fatal (Y\/N)'] == '2017']['Fatal (Y\/N)'].index.values,\\\n        inplace = True)\n\ndf1['Fatal (Y\/N)'].value_counts()","532f8479":"sns.countplot(x = 'Fatal (Y\/N)', data = df1)","b02bf2de":"# dummy variables for Fatal (Y\/N) column\nfatal = pd.get_dummies(df1['Fatal (Y\/N)'], drop_first = True)\ndf1.drop('Fatal (Y\/N)', axis = 1, inplace = True)\ndf1 = pd.concat([df1, fatal], axis = 1)","159ee5b9":"# We can use the same approach for the 'Type', 'Activity', 'Sex' and 'Area' columns:\n\n#we'll work on the 'Sex column first'\nsns.countplot(x = 'Sex ', data = df1)","288d50ea":"df1['Sex '].value_counts()","bf54267b":"if df1[df1['Sex '] == 'M ']['Sex '].iloc[0]:\n    df1.replace(to_replace = df1[df1['Sex '] == 'M ']['Sex '].iloc[0], value = 'M',\\\n               inplace = True)\n    \ndf1['Sex '].value_counts()","c228e12c":"#dropping remaing ambiguous 'Sex' labels\ndf1.drop(df1[df1['Sex '] == 'lli']['Sex '].index.values, inplace = True)\ndf1.drop(df1[df1['Sex '] == '.']['Sex '].index.values, inplace = True)\ndf1.drop(df1[df1['Sex '] == 'N']['Sex '].index.values, inplace = True)\ndf1['Sex '].value_counts()","2bd2a6dc":"sns.countplot(x = 'Sex ', data = df1, hue = 'Y')","e0cb3dcc":"sex = pd.get_dummies(df1['Sex '], drop_first = True)\ndf1.drop('Sex ', axis = 1, inplace = True)\ndf1 = pd.concat([df1, sex], axis = 1)","ebb47d21":"# moving on to the 'Type' column\ndf1['Type'].value_counts()\n# 'Boat' and 'Boating' columns can be merged","a6d2bec5":"#merging Boat and Boating columns\nif df1[df1['Type'] == 'Boat']['Type'].iloc[0]:\n    df1.replace(to_replace = df1[df1['Type'] == 'Boat']['Type'].iloc[0], value = 'Boating',\\\n               inplace = True)\ndf1['Type'].value_counts()\n#we'll keep the invalid column as we lack additional information to change it","b5466ca6":"sns.countplot(x = 'Type', data = df1, hue = 'Y')\n# sea diaster entries are most fatal. Possibly due to time in the water\/presence of blood\n# number of people...","ddbf5463":"type = pd.get_dummies(df1['Type'], drop_first= True)\ndf1.drop('Type', axis = 1, inplace = True)\ndf1 = pd.concat([df1, type], axis = 1)","e283b6c1":"# Now the 'Activity column'\n# some simple typos to correct and then some larger explainations...\n\n# first we make everything lowercase\nfor i in range(len(df1['Activity'])):\n    df1['Activity'].iloc[i] = df1['Activity'].iloc[i].lower()","454447c3":"# capturing and classifiying as many 'activities' as possible\n\n# some activities are lumped together for processing \nfor i in range(len(df1['Activity'])):\n    \n    if 'surfing' in df1['Activity'].iloc[i].split():\n        df1.replace(to_replace = df1['Activity'].iloc[i],value = 'surfing', inplace = True)\n    elif 'surfing,' in df1['Activity'].iloc[i].split():\n        df1.replace(to_replace = df1['Activity'].iloc[i],value = 'surfing', inplace = True)\n    elif 'surfboard' in df1['Activity'].iloc[i].split():\n        df1.replace(to_replace = df1['Activity'].iloc[i],value = 'surfing', inplace = True)\n    elif 'swimming' in df1['Activity'].iloc[i].split():\n        df1.replace(to_replace = df1['Activity'].iloc[i],value = 'swimming', inplace = True)\n    elif 'swimming,' in df1['Activity'].iloc[i].split():\n        df1.replace(to_replace = df1['Activity'].iloc[i],value = 'swimming', inplace = True)\n    elif 'bathing' in df1['Activity'].iloc[i].split():\n        df1.replace(to_replace = df1['Activity'].iloc[i],value = 'swimming', inplace = True)\n    elif 'floating' in df1['Activity'].iloc[i].split():\n        df1.replace(to_replace = df1['Activity'].iloc[i],value = 'swimming', inplace = True)\n    elif 'water' in df1['Activity'].iloc[i].split():\n        df1.replace(to_replace = df1['Activity'].iloc[i],value = 'swimming', inplace = True)\n    elif 'fishing' in df1['Activity'].iloc[i].split():\n        df1.replace(to_replace = df1['Activity'].iloc[i],value = 'fishing', inplace = True)\n    elif 'fishing,' in df1['Activity'].iloc[i].split():\n        df1.replace(to_replace = df1['Activity'].iloc[i],value = 'fishing', inplace = True)\n    elif 'wading' in df1['Activity'].iloc[i].split():\n        df1.replace(to_replace = df1['Activity'].iloc[i],value = 'wading', inplace = True)\n    elif 'standing' in df1['Activity'].iloc[i].split():\n        df1.replace(to_replace = df1['Activity'].iloc[i],value = 'wading', inplace = True)\n    elif 'boogie' in df1['Activity'].iloc[i].split():\n        df1.replace(to_replace = df1['Activity'].iloc[i],value = 'body boarding', inplace = True)\n    elif 'body-boarding' in df1['Activity'].iloc[i].split():\n        df1.replace(to_replace = df1['Activity'].iloc[i],value = 'body boarding', inplace = True)\n    elif 'bodyboarding' in df1['Activity'].iloc[i].split():\n        df1.replace(to_replace = df1['Activity'].iloc[i],value = 'body boarding', inplace = True)\n    elif 'spearfishing' in df1['Activity'].iloc[i].split():\n        df1.replace(to_replace = df1['Activity'].iloc[i],value = 'spearfishing', inplace = True)\n    elif 'spearfishing,' in df1['Activity'].iloc[i].split():\n        df1.replace(to_replace = df1['Activity'].iloc[i],value = 'spearfishing', inplace = True)\n    elif 'diving' in df1['Activity'].iloc[i].split():\n        df1.replace(to_replace = df1['Activity'].iloc[i],value = 'diving', inplace = True)\n    elif 'freediving' in df1['Activity'].iloc[i].split():\n        df1.replace(to_replace = df1['Activity'].iloc[i],value = 'diving', inplace = True)\n    elif 'skindiving' in df1['Activity'].iloc[i].split():\n        df1.replace(to_replace = df1['Activity'].iloc[i],value = 'diving', inplace = True)\n    elif 'snorkeling' in df1['Activity'].iloc[i].split():\n        df1.replace(to_replace = df1['Activity'].iloc[i],value = 'snorkeling', inplace = True)\n    elif 'surf-skiing' in df1['Activity'].iloc[i].split():\n        df1.replace(to_replace = df1['Activity'].iloc[i],value = 'surf skiing', inplace = True)\n    elif 'skiing' in df1['Activity'].iloc[i].split():\n        df1.replace(to_replace = df1['Activity'].iloc[i],value = 'surf skiing', inplace = True)\n    elif 'canoeing' in df1['Activity'].iloc[i].split():\n        df1.replace(to_replace = df1['Activity'].iloc[i],value = 'kayaking', inplace = True)\n","c540e1a6":"pd.set_option('max_rows', None)\nprint(df1['Activity'].value_counts()[:13].sum())\nprint('\\n')\ndf1['Activity'].value_counts()[:13].sum()\/df1['Activity'].value_counts().sum()\n#we'll drop all activities that have less than ten entrys. \n#We're keeping 88% with this cut off","d5084f82":"act_list = df1['Activity'].value_counts()[:13].index.values","faf2c907":"drop_act = []\nfor i in range(len(df1)):\n    if df1['Activity'].iloc[i] not in act_list: \n        drop_act = np.append(drop_act,  int(df1['Activity'].index[i]))\n\ndrop_act = list(map(int, drop_act))","1c4cb6d5":"df1.drop(index = drop_act, inplace = True)","e17ff3c4":"plt.figure(figsize = (15,5))\nsns.countplot(x = 'Activity', data = df1)","c9b898e9":"plt.figure(figsize = (15,5))\nsns.countplot(x = 'Activity', data = df1, hue = 'Y')\n# swimming (and the activities that are lumped there) have the greatest fatality rate","145bd436":"act_dum = pd.get_dummies(df1['Activity'], drop_first=True)\ndf1.drop('Activity', axis =1, inplace = True)\ndf1 = pd.concat([df1, act_dum], axis=1)","709c030e":"#let's remove the injury as it does not help with predictive properties. Also, we'll drop \n# 'Country' and 'Location' Columns and focus on the 'Area'\ndf1.drop(['Injury', 'Country', 'Location'], axis = 1, inplace = True)","d752ef48":"# Again, collecting 'Area' entrys with value counts greater than 9\narea_list = df1['Area'].value_counts()[:32].index.values","6901f4af":"drop_area = []\nfor i in range(len(df1)):\n    if df1['Area'].iloc[i] not in area_list: \n        drop_area = np.append(drop_area,  int(df1['Area'].index[i]))\n\ndrop_area = list(map(int, drop_area))","5143f687":"df1.drop(index = drop_area, inplace = True)","8bd051b2":"plt.figure(figsize = (50,5))\nsns.countplot(x = 'Area', data = df1)","e23448ce":"plt.figure(figsize = (50,5))\nsns.countplot(x = 'Area', data = df1, hue = 'Y')\n# New South Wales has the greatest fatality rate per shark attack.","61535094":"area_dum = pd.get_dummies(df1['Area'], drop_first=True)\ndf1.drop('Area', axis = 1, inplace = True)\ndf1 = pd.concat([df1, area_dum], axis = 1)","e627396b":"# We'll keep only the year\n\n# year column\ndf1['Year'] = df1['Date'].apply(lambda x:x.split('-')[0])","004dddcf":"#correcting the nonyear values\n\nwrong_year_val = []\nwrong_year_ind = []\nfor i in range(len(df1['Year'])):\n    if len(df1['Year'].iloc[i]) > 4:\n        wrong_year_val = np.append(wrong_year_val, df1['Date'].iloc[i])\n        wrong_year_ind = np.append(wrong_year_ind, df1['Date'].index[i])\n    elif len(df1['Year'].iloc[i]) < 4:\n        wrong_year_val = np.append(wrong_year_val, df1['Date'].iloc[i])\n        wrong_year_ind = np.append(wrong_year_ind, df1['Date'].index[i])\n\nwrong_year_ind = list(map(int, wrong_year_ind))","8f4592c4":"#replacing nonyear values\n\ndrop_wrongdf = pd.DataFrame(wrong_year_val)\nsplit_year = drop_wrongdf[0].apply(lambda x:x.split('-'))\n\nfor i in range(len(split_year)):\n    df1['Year'].at[wrong_year_ind[i]] = split_year[i][-1]","865858a2":"#final cleaning and dropping remaining erroneous values\ndf1['Year'] = df1['Year'].apply(lambda x:x.split()[-1])\ndf1.drop(df1[df1['Year'].map(len)!=4].index, inplace = True)\ndf1.drop(df1[df1['Year']> '2021'].index, inplace = True)\ndf1['Year'] = list(map(int, df1['Year']))\nlen(df1)","4735ac86":"# reported shark attacks have certainly increased...what happened in 1905?\nplt.figure(figsize=(100,5))\nyear_order = sorted(df1['Year'].unique(), reverse = True)\nsns.countplot(x = 'Year',order = year_order, data = df1)","44e46537":"#removing the 'Date' column\ndf1.drop('Date', axis =1, inplace = True)","d57a3b28":"df1.columns","a4e7515a":"# we are now ready to train a classification model! \ndf1.select_dtypes(['object']).columns","a557f3e1":"# We'll attempt to predict whether or not an attack will be fatal\nfrom sklearn.model_selection import train_test_split\nX = df1.drop('Y', axis = 1).values\ny = df1['Y'].values","211f3c42":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=101)","27a147a9":"# We'll train three classification models: Logistic Regression, Decision tree classifier \n# and random forest classifer.\n\nfrom sklearn.linear_model import LogisticRegression\nlr = LogisticRegression(solver ='liblinear')\nlr.fit(X_train, y_train)\nlr_pred = lr.predict(X_test)","18443b63":"from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\nprint(classification_report(y_test, lr_pred))\nprint('\\n')\nprint(confusion_matrix(y_test, lr_pred))\nprint('\\n')\nprint(accuracy_score(y_test, lr_pred))","b5a0cb37":"from sklearn.tree import DecisionTreeClassifier\ndtc = DecisionTreeClassifier()\ndtc.fit(X_train, y_train)\ndtc_pred = dtc.predict(X_test)","fea92be0":"print(classification_report(y_test, dtc_pred))\nprint('\\n')\nprint(confusion_matrix(y_test, dtc_pred))\nprint('\\n')\nprint(accuracy_score(y_test, dtc_pred))","835a0f5a":"from sklearn.ensemble import RandomForestClassifier\nrfc = RandomForestClassifier()\nrfc.fit(X_train, y_train)\nrfc_pred = rfc.predict(X_test)","ebec974d":"print(classification_report(y_test, rfc_pred))\nprint('\\n')\nprint(confusion_matrix(y_test, rfc_pred))\nprint('\\n')\nprint(accuracy_score(y_test, rfc_pred))","1c7185b4":"# Now for the Date Column...","33a18ee1":"# Training Models","e9209ba5":"# Cleaning up the data features columns...","c19c2539":"# Logistic Regression","3e1c7b7b":"# Random Forest Classifer","7c29be56":"# Decision Tree Classifier"}}