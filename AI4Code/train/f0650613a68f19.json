{"cell_type":{"57bab0f5":"code","74caf99e":"code","804511be":"code","3a25c104":"code","15a7ae35":"code","af2c0327":"code","3ce1d7ab":"code","c1299c73":"code","29919fc2":"code","298395cf":"code","f1afd39f":"code","4935514e":"code","a5f66166":"code","8681bb4e":"code","ffe5af5b":"code","ac6876fc":"code","fc9c2e75":"code","fcc9555f":"code","d63ee358":"code","f12173df":"code","983311ef":"code","359c5d6c":"code","17be78e7":"code","e5d346d6":"code","029f6cfb":"code","a3fc7430":"code","30b0a325":"markdown","41e78569":"markdown","0811fc0a":"markdown","c9db2443":"markdown"},"source":{"57bab0f5":"import numpy as np \nimport pandas as pd\nimport pandas_profiling as pp\nimport seaborn as sns \nimport matplotlib.pyplot as plt \nfrom imblearn.over_sampling import RandomOverSampler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import confusion_matrix, classification_report, accuracy_score \n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n","74caf99e":"df = pd.read_csv(\"\/kaggle\/input\/stroke-prediction-dataset\/healthcare-dataset-stroke-data.csv\")","804511be":"df[\"stroke\"].hist()","3a25c104":"pp.ProfileReport(df)","15a7ae35":"df.fillna(value=df[\"bmi\"].median(), inplace=True)\ndf.isnull().sum()","af2c0327":"df.drop(\"id\", axis=1, inplace=True)\ndf.drop(df[df[\"gender\"] == \"Other\"].index, inplace=True)","3ce1d7ab":"df.head(2)","c1299c73":"sns.pairplot(df)","29919fc2":"g = sns.FacetGrid(df, col=\"gender\", hue=\"ever_married\")\ng.map(sns.scatterplot, \"age\", \"bmi\", alpha=.7)\nplt.figure(figsize=(20,15))\ng.add_legend()","298395cf":"fig,ax = plt.subplots(2,2, figsize=(15 ,10))  \nsns.distplot(df['age'], ax = ax[0,0], color = 'b') \nsns.distplot(df['avg_glucose_level'], ax = ax[0,1], color = 'b')\nsns.distplot(df['bmi'], ax = ax[1,0], color = 'b')\nplt.show()","f1afd39f":"df.head()","4935514e":"# Encoding Columns\n# df = pd.get_dummies(df, columns=[\"ever_married\"], drop_first=True)\n# df = pd.get_dummies(df, columns= [\"work_type\"])\n# df = pd.get_dummies(df, columns=[\"Residence_type\"], drop_first=True)\n# df[\"smoking_status\"] = df[\"smoking_status\"].replace({\"never smoked\": 0, \"Unknown\": 1, \"formerly smoked\": 2, \"smokes\": 3})\ndf[\"gender\"] = df[\"gender\"].replace({\"Male\": 1, \"Female\": 0})\ndf","a5f66166":"# Splitting data into train and test data. \nX = df.drop(\"stroke\", axis=1)\ny = df[\"stroke\"]","8681bb4e":"os = RandomOverSampler(sampling_strategy=1)\nX_ros, y_ros = os.fit_resample(X, y)","ffe5af5b":"X_train, X_test, y_train, y_test = train_test_split(X_ros, y_ros, test_size=0.25, random_state=1 ,shuffle=True)","ac6876fc":"data_scaler = StandardScaler()\nX_train = pd.DataFrame(data=data_scaler.fit_transform(X_train), columns=X_train.columns, index=X_train.index)\nX_test = pd.DataFrame(data=data_scaler.fit_transform(X_test), columns=X_test.columns, index=X_test.index)","fc9c2e75":"# Applying Logistic Regression model\nLR_model = LogisticRegression().fit(X_train, y_train)\npredictions = LR_model.predict(X_test)","fcc9555f":"df1 = pd.DataFrame(columns = [\"Not Stroke\", \"Stroke\"], index = [\"Not Stroke\", \"Stroke\"], data = confusion_matrix(y_test, predictions))\nprint(df1)\nprint(\"\\n\")\nprint(classification_report(y_test, predictions))","d63ee358":"# Applying K-NEAREST NEIGHBORS\naccuracy = []\n\nfor k in range(2, 10):\n    model = KNeighborsClassifier(n_neighbors=k).fit(X_train, y_train)\n    predictions = model.predict(X_test)\n    print(f'Accuracy at {k}: {accuracy_score(y_test, predictions)}')\n\n","f12173df":"# Highest Accucary achieved when n_neighbors coefficient was 2. So, we proceed with that. \nmodel = KNeighborsClassifier(n_neighbors=2).fit(X_train, y_train)\npredictions = model.predict(X_test)","983311ef":"df1 = pd.DataFrame(columns = [\"Not Stroke\", \"Stroke\"], index = [\"Not Stroke\", \"Stroke\"], data = confusion_matrix(y_test, predictions))\nprint(df1)\nprint(\"\\n\")\nprint(classification_report(y_test, predictions))","359c5d6c":"# Applying Decision Tree Classifier\nmodel = DecisionTreeClassifier().fit(X_train, y_train)\npredictions = model.predict(X_test)","17be78e7":"df1 = pd.DataFrame(columns = [\"Not Stroke\", \"Stroke\"], index = [\"Not Stroke\", \"Stroke\"], data = confusion_matrix(y_test, predictions))\nprint(df1)\nprint(\"\\n\")\nprint(classification_report(y_test, predictions))","e5d346d6":"# Applying RANDOM FOREST CLASSIFIER \ntree_numbers = [100,150,200,250,300,350,400,450,500]\n\nfor i in tree_numbers:\n    model = RandomForestClassifier(n_estimators=i).fit(X_train, y_train)\n    predictions = model.predict(X_test)\n    print(f'Accuracy at {i}: {accuracy_score(y_test, predictions)}')","029f6cfb":"model = RandomForestClassifier(n_estimators=350).fit(X_train, y_train)\npredictions = model.predict(X_test)","a3fc7430":"df1 = pd.DataFrame(columns = [\"Not Stroke\", \"Stroke\"], index = [\"Not Stroke\", \"Stroke\"], data = confusion_matrix(y_test, predictions))\nprint(df1)\nprint(\"\\n\")\nprint(classification_report(y_test, predictions))","30b0a325":"<h2>Seaborn Visualizations<\/h2>","41e78569":"<h2>Treating missing values<\/h2>","0811fc0a":"<h2>Feature Engineering<\/h2>","c9db2443":"<h2>Machine Learning Models<\/h2>"}}