{"cell_type":{"fe4e63e5":"code","72513618":"code","7f38dd12":"code","ddcc841b":"code","6a5ad638":"code","50abb967":"code","2dbd5a70":"code","138b9ffa":"code","10eafa9d":"code","035e1d06":"code","c63adb6f":"code","f50fb203":"code","36ed49d6":"code","db586822":"code","3a9ee2b9":"code","da9d98d1":"code","13661e4f":"code","73096e3d":"code","0b8deeba":"code","69ecfada":"code","b3366a82":"markdown","bc0189b2":"markdown","8ec6a97d":"markdown","d12cf0e7":"markdown","db634562":"markdown","a888d3fe":"markdown","dc93c9b8":"markdown","45413dfb":"markdown","44343af2":"markdown","783aab83":"markdown","e9a67536":"markdown","bc88ea7e":"markdown","d11f3e25":"markdown","142d2fd7":"markdown","21a68ddc":"markdown","88d0e603":"markdown","37b416e7":"markdown","2ad552da":"markdown","35239220":"markdown","f2a2540f":"markdown","24496417":"markdown","22fe6988":"markdown","5e835464":"markdown","c4eb011c":"markdown","48838fe2":"markdown","3742b7fa":"markdown","5993f0a0":"markdown","05c3f818":"markdown","765277a5":"markdown","6cf8af2f":"markdown","a89e0a58":"markdown","9c6ff8b1":"markdown","efae1cea":"markdown","d833e273":"markdown","b6031d6a":"markdown","369fa61c":"markdown","dc88b120":"markdown","87cb6a51":"markdown","2debd586":"markdown","5575bd6d":"markdown","12d178e2":"markdown","a69b5f8f":"markdown"},"source":{"fe4e63e5":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport sqlite3\nimport datetime\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nplt.style.use('seaborn')\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import r2_score\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import KFold\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.manifold import TSNE\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","72513618":"X = pd.read_csv('\/kaggle\/input\/predicting-real-time-delay-status-part-1\/X.csv', index_col=0)\ny = pd.read_csv('\/kaggle\/input\/predicting-real-time-delay-status-part-1\/y.csv', index_col=0)","7f38dd12":"col_to_keep = ['last_status','avg_station_same', 'avg_station_opp', 'avg_sys']\npair_plot_df = X[col_to_keep]\nsns.pairplot(pair_plot_df)","ddcc841b":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=200)","6a5ad638":"X = []\ny = []","50abb967":"standardize = StandardScaler()\nX_train = standardize.fit_transform(X_train)","2dbd5a70":"X_test = standardize.transform(X_test)","138b9ffa":"pca = PCA(n_components=51)\npca.fit(X_train)\npc_vs_variance = np.cumsum(pca.explained_variance_ratio_)\n\nfig, ax = plt.subplots(figsize=[8, 8])\nplt.plot(pc_vs_variance)\nax.set_xlabel('Num of components')\nax.set_ylabel('Cummulative variance')\nplt.show()","10eafa9d":"print(pc_vs_variance[46])\nprint('According to the plot, n = 47 should be enough to capture 100% of variance')","035e1d06":"pca = PCA(n_components=47)\nX_train = pca.fit_transform(X_train)\nX_test = pca.transform(X_test)","c63adb6f":"lr = LinearRegression()\nlr.fit(X_train, y_train)\ny_pred = lr.predict(X_test)\nprint('mean squared error on train sets:', mean_squared_error(y_train, lr.predict(X_train)))\nprint('mean squared error on test sets:', mean_squared_error(y_test, y_pred))\nprint('r2 on train sets:', r2_score(y_train, lr.predict(X_train)))\nprint('r2 on test sets:', r2_score(y_test, y_pred))","f50fb203":"kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\ny_train_np = np.array(y_train)","36ed49d6":"cvscores = []\n\nfor train, test in kfold.split(X_train, y_train_np):\n  model = Sequential()\n  model.add(Dense(40, input_dim=47, kernel_initializer='normal', activation='relu'))\n  model.add(Dense(40, kernel_initializer='normal', activation='relu'))\n  model.add(Dense(40, kernel_initializer='normal', activation='relu'))\n  model.add(Dense(1, kernel_initializer='normal'))    \n  # Compile model\n  model.compile(loss='mse', optimizer='adam')\n  print('-------------')\n  # Fit the model\n  model.fit(X_train[train], y_train_np[train], epochs=10, batch_size=128)\n  print('-------------')\n  # evaluate the model\n  scores = model.evaluate(X_train[test], y_train_np[test])\n  cvscores.append(scores)","db586822":"print('loss: %.4f' % np.mean(cvscores), '(+\/-%.3f)' % np.std(cvscores))","3a9ee2b9":"cvscores2 = []\n\nfor train, test in kfold.split(X_train, y_train_np):\n  model = Sequential()\n  model.add(Dense(40, input_dim=47, kernel_initializer='normal', activation='relu'))\n  model.add(Dense(40, kernel_initializer='normal', activation='relu'))\n  model.add(Dense(1, kernel_initializer='normal'))    \n  # Compile model\n  model.compile(loss='mse', optimizer='adam')\n  # Fit the model\n  print('-------------')\n  model.fit(X_train[train], y_train_np[train], epochs=10, batch_size=128)\n\t# evaluate the model\n  print('-------------')\n  scores = model.evaluate(X_train[test], y_train_np[test])\n  cvscores2.append(scores)","da9d98d1":"print('loss: %.4f' % np.mean(cvscores2), '(+\/-%.3f)' % np.std(cvscores2))","13661e4f":"cvscores3 = []\n\nfor train, test in kfold.split(X_train, y_train_np):\n  model = Sequential()\n  model.add(Dense(30, input_dim=47, kernel_initializer='normal', activation='relu'))\n  model.add(Dense(30, kernel_initializer='normal', activation='relu'))\n  model.add(Dense(1, kernel_initializer='normal'))    \n  # Compile model\n  model.compile(loss='mse', optimizer='adam')\n  # Fit the model\n  print('-------------')\n  model.fit(X_train[train], y_train_np[train], epochs=10, batch_size=128)\n\t# evaluate the model\n  print('-------------')\n  scores = model.evaluate(X_train[test], y_train_np[test])\n  cvscores3.append(scores)","73096e3d":"print('loss: %.4f' % np.mean(cvscores3), '(+\/-%.3f)' % np.std(cvscores3))","0b8deeba":"model = Sequential()\nmodel.add(Dense(40, input_dim=47, kernel_initializer='normal', activation='relu'))\nmodel.add(Dense(40, kernel_initializer='normal', activation='relu'))\nmodel.add(Dense(1, kernel_initializer='normal'))    \n# Compile model\nmodel.compile(loss='mse', optimizer='adam')\n# Fit the model\nmodel.fit(X_train, y_train_np, epochs=10, batch_size=128)","69ecfada":"y_pred = model.predict(X_test)\ny_pred_train = model.predict(X_train)\n\nprint('mean squared error on train sets:', mean_squared_error(y_train, y_pred_train))\nprint('mean squared error on test sets:', mean_squared_error(y_test, y_pred))\nprint('r2 on train sets:', r2_score(y_train, y_pred_train))\nprint('r2 on test sets:', r2_score(y_test, y_pred))","b3366a82":"## 7.1 Model comparison","bc0189b2":"## 6.2 Neural Network Model","8ec6a97d":"Based on the the cross-validation results, I chose the model 2 for the final model of neural network.","d12cf0e7":"Use K-fold validation to validate different models. First initialize K-fold validation.","db634562":"Print out validation score.","a888d3fe":"Print out validation score.","dc93c9b8":"## 5.2 Split train and test sets","45413dfb":"Test second model:\n* Number of hidden layers: 2 \n* **Number of nodes each layer: 30 (only difference between 1 & 2)**\n* Activation function type: relu\n* Loss function: Mean squared error\n* Epochs: 10\n* Mini-batch size: 128","44343af2":"Predict based on test set.","783aab83":"### 6.2.2 Neural Network Model","e9a67536":"# Section 6: ML models","bc88ea7e":"# Section 7: Conclusion and Discussion","d11f3e25":"Test second model:\n* **Number of hidden layers: 2 (only difference between 1 & 2)**\n* Number of nodes each layer: 40\n* Activation function type: relu\n* Loss function: Mean squared error\n* Epochs: 10\n* Mini-batch size: 128","142d2fd7":"Set n_compnents to 47 and re-apply PCA to training set. Then, use the same projection to transform test set.","21a68ddc":"Load feature and target matrices from part 1","88d0e603":"The results show that Model 2 is better than Model 3.","37b416e7":"Apply PCA and first set number of compenents to original dimension. Plot cummulative variance over number of components.","2ad552da":"### 6.2.1 K-fold validation","35239220":"Use the same settings to standardize test set.","f2a2540f":"Come up with an initial model:\n* Number of hidden layers: 3\n* Number of nodes each layer: 40\n* Activation function type: relu\n* Loss function: Mean squared error\n* Epochs: 10\n* Mini-batch size: 128","24496417":"Release the RAM used by X and y.","22fe6988":"As for future work, the first thing I want to mention is that all the models here can be further improved if more time is given to tune hyperparameters and test more models with cross-validation.","5e835464":"The second challenge is the time. At the very beginning, I used a lot of apply function to process the data, but noticed that it occupied a lot of time. I re-wrote the program by replacing apply functions with built-in pandas methods, and it worked pretty well.\n\nHowever, there are still some time-consuming process which are inevitable like GridSearchCV (10 hours + in Google Colab), training of Random Forest Regressor (30 minutes + in Google Colab), training of Nerual Network (20 minutes + in Google Colab). So, if I have more time, I could further improve my work.","c4eb011c":"## 5.1 Partial pairplot of feature matrix","48838fe2":"Based on the mean squared error and r2 scores, the Neural Network model is better than Linear regression results. I also tried random forest regressor in Google colab, which has much better hardware than my laptop, and it shows slightly worse result compared with the neural network but still better than the linear regression.","3742b7fa":"Standardize training feature matrix.","5993f0a0":"Re-build model 2 and train with the whole training set.","05c3f818":"# Section 5: Pre-pocessing feature and target matrix","765277a5":"## 5.3 Standardize feature matrix","6cf8af2f":"Print out validation score.","a89e0a58":"## 5.4 Apply PCA","9c6ff8b1":"#### Model 3: Further simplify the model. Decrease the neurons of each hidden layer to 30.","efae1cea":"## 7.2 Description of challenges\/ Obstacles faced","d833e273":"Apply linear regression and print out mean squared error and r2 on both training and test sets.","b6031d6a":"#### Model 1: Intial model","369fa61c":"And I am pretty satisfied with the current performance of models described in this notebook, since the mean squared error is around 1.4 in minutes. So I think it could be potentially applied to the SEPTA system to predict delay time.\n\nIn order to do so, the models need to ba adjusted to receive real-time data, re-run machine learning program, and give out real-time prediction. This would require stream processing.","dc88b120":"Split train and test sets with test size = 20%.","87cb6a51":"## 6.1 Linear regression","2debd586":"The results show that Model 2 is better than Model 1.","5575bd6d":"### 7.3 Potential Next Steps\/ Future Direction","12d178e2":"#### Model 2: w\/ less hidden layer","a69b5f8f":"The first challenge I came across is from the limit of kaggle notebook. The RAM limit is 14 GB, which is clearly not enough for such a huge dataset.\n\nTo test in a safer way, I am personally more likely to use the function .copy() to make sure that I will not ruin my original dataframe. However, it would directly double the usage of RAM. I have used the RAM up several times before going into model section. And eventually, it ended up with spliting into two separat notebooks.\n\nAfter making sure all the processing is correct, I removed the .copy() function and re-run program to release RAM. However, in the model section, the fit process also takes a lot of memory. My notebook crashed once with Random Forest Regressor (even in Google colab with 25-G RAM) and crashed more than 5 times with neural network."}}