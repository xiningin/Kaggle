{"cell_type":{"f5357cbe":"code","ba7a0558":"code","1b602f27":"code","e66ab49a":"code","5ce4237d":"code","6cc9457e":"code","1dd60cda":"code","00a0ea31":"code","6e4fdea7":"markdown","370d7438":"markdown","c1e95a61":"markdown","6d866623":"markdown","6b5fc255":"markdown","4d47eeb5":"markdown","c5eaa964":"markdown","444b2c0a":"markdown","71ff1ade":"markdown","2a166808":"markdown","838e60b4":"markdown"},"source":{"f5357cbe":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport matplotlib.pyplot as plt\nimport statsmodels.tsa.seasonal as smt\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport random\nimport datetime as dt\nfrom sklearn import linear_model \nfrom sklearn.metrics import mean_absolute_error\nimport plotly\n\n# import the relevant Keras modules\nfrom keras.models import Sequential\nfrom keras.layers import Activation, Dense\nfrom keras.layers import LSTM\nfrom keras.layers import Dropout\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"..\/input\"]).decode(\"utf8\"))\nimport os\nos.chdir('..\/input\/Data\/Stocks\/')","ba7a0558":"#read data\n# kernels let us navigate through the zipfile as if it were a directory\n\n# trying to read a file of size zero will throw an error, so skip them\n# filenames = [x for x in os.listdir() if x.endswith('.txt') and os.path.getsize(x) > 0]\n# filenames = random.sample(filenames,1)\nfilenames = ['prk.us.txt', 'bgr.us.txt', 'jci.us.txt', 'aa.us.txt', 'fr.us.txt', 'star.us.txt', 'sons.us.txt', 'ipl_d.us.txt', 'sna.us.txt', 'utg.us.txt']\nfilenames = [filenames[1]]\nprint(filenames)\n\ndata = []\nfor filename in filenames:\n    df = pd.read_csv(filename, sep=',')\n\n    label, _, _ = filename.split(sep='.')\n    df['Label'] = filename\n    df['Date'] = pd.to_datetime(df['Date'])\n    data.append(df)","1b602f27":"r = lambda: random.randint(0,255)\ntraces = []\n\nfor df in data:\n    clr = str(r()) + str(r()) + str(r())\n#     df = df.sample(n=100, replace=True)\n    df = df.sort_values('Date')\n#     print(df['Label'])\n    label = df['Label'].iloc[0]\n\n    trace = plotly.graph_objs.Scattergl(\n        x=df['Date'],\n        y=df['Close'],\n        mode='line',\n        line=dict(\n            color = clr\n        )\n    )\n    traces.append(trace)\n    \nlayout = plotly.graph_objs.Layout(\n    title='Plot',\n)\nfig = plotly.graph_objs.Figure(data=traces, layout=layout)\n\nplotly.offline.init_notebook_mode(connected=True)\nplotly.offline.iplot(fig, filename='dataplot')","e66ab49a":"df = data[0]\nwindow_len = 10\n\n#Create a data point (i.e. a date) which splits the training and testing set\nsplit_date = list(data[0][\"Date\"][-(2*window_len+1):])[0]\n\n#Split the training and test set\ntraining_set, test_set = df[df['Date'] < split_date], df[df['Date'] >= split_date]\ntraining_set = training_set.drop(['Date','Label', 'OpenInt'], 1)\ntest_set = test_set.drop(['Date','Label','OpenInt'], 1)\n\n#Create windows for training\nLSTM_training_inputs = []\nfor i in range(len(training_set)-window_len):\n    temp_set = training_set[i:(i+window_len)].copy()\n    \n    for col in list(temp_set):\n        temp_set[col] = temp_set[col]\/temp_set[col].iloc[0] - 1\n    \n    LSTM_training_inputs.append(temp_set)\nLSTM_training_outputs = (training_set['Close'][window_len:].values\/training_set['Close'][:-window_len].values)-1\n\nLSTM_training_inputs = [np.array(LSTM_training_input) for LSTM_training_input in LSTM_training_inputs]\nLSTM_training_inputs = np.array(LSTM_training_inputs)\n\n#Create windows for testing\nLSTM_test_inputs = []\nfor i in range(len(test_set)-window_len):\n    temp_set = test_set[i:(i+window_len)].copy()\n    \n    for col in list(temp_set):\n        temp_set[col] = temp_set[col]\/temp_set[col].iloc[0] - 1\n    \n    LSTM_test_inputs.append(temp_set)\nLSTM_test_outputs = (test_set['Close'][window_len:].values\/test_set['Close'][:-window_len].values)-1\n\nLSTM_test_inputs = [np.array(LSTM_test_inputs) for LSTM_test_inputs in LSTM_test_inputs]\nLSTM_test_inputs = np.array(LSTM_test_inputs)","5ce4237d":"def build_model(inputs, output_size, neurons, activ_func=\"linear\",\n                dropout=0.10, loss=\"mae\", optimizer=\"adam\"):\n    \n    model = Sequential()\n\n    model.add(LSTM(neurons, input_shape=(inputs.shape[1], inputs.shape[2])))\n    model.add(Dropout(dropout))\n    model.add(Dense(units=output_size))\n    model.add(Activation(activ_func))\n\n    model.compile(loss=loss, optimizer=optimizer)\n    return model","6cc9457e":"# initialise model architecture\nnn_model = build_model(LSTM_training_inputs, output_size=1, neurons = 32)\n# model output is next price normalised to 10th previous closing price\n# train model on data\n# note: eth_history contains information on the training error per epoch\nnn_history = nn_model.fit(LSTM_training_inputs, LSTM_training_outputs, \n                            epochs=5, batch_size=1, verbose=2, shuffle=True)","1dd60cda":"plt.plot(LSTM_test_outputs, label = \"actual\")\nplt.plot(nn_model.predict(LSTM_test_inputs), label = \"predicted\")\nplt.legend()\nplt.show()\nMAE = mean_absolute_error(LSTM_test_outputs, nn_model.predict(LSTM_test_inputs))\nprint('The Mean Absolute Error is: {}'.format(MAE))","00a0ea31":"#https:\/\/github.com\/llSourcell\/How-to-Predict-Stock-Prices-Easily-Demo\/blob\/master\/lstm.py\ndef predict_sequence_full(model, data, window_size):\n    #Shift the window by 1 new prediction each time, re-run predictions on new window\n    curr_frame = data[0]\n    predicted = []\n    for i in range(len(data)):\n        predicted.append(model.predict(curr_frame[np.newaxis,:,:])[0,0])\n        curr_frame = curr_frame[1:]\n        curr_frame = np.insert(curr_frame, [window_size-1], predicted[-1], axis=0)\n    return predicted\n\npredictions = predict_sequence_full(nn_model, LSTM_test_inputs, 10)\n\nplt.plot(LSTM_test_outputs, label=\"actual\")\nplt.plot(predictions, label=\"predicted\")\nplt.legend()\nplt.show()\nMAE = mean_absolute_error(LSTM_test_outputs, predictions)\nprint('The Mean Absolute Error is: {}'.format(MAE))","6e4fdea7":"### Plot of prediction of one data point ahead\nAs can be seen in the plot, one step prediction is not bad. The scale is a bit of, because the data is normalized. ","370d7438":"### Prediction of one window (10 time steps) ahead\nAs can be seen in the plot below, the performance quickly degrades when predicting multiple time points ahead. However compered to something like linear regression the performance is better. ","c1e95a61":"### Plotting the data","6d866623":"### Loading in the data","6b5fc255":"# LSTM model of StockData\nIn this notebook we will go through a basic Long Short Term Memory (LSTM) model for time series. The notebooks does the following things:\n* First load in the data. The preproccessing only consist of normalization and the creation of windows.\n* Creation of the LSTM model\n* Training the LSTM model\n* Testing the LSTM model with 1 time step and with 1 window","4d47eeb5":"### Creating windows and normalizing the data","c5eaa964":"## LSTM model definition","444b2c0a":"## Training of the LSTM model","71ff1ade":"### Import libraries","2a166808":"## Conclusion\nLSTM's do not solve time series prediction. The prediction on one time step is not much better then the lag model. If we increase the number of time steps predicted the performance does not degrade as fast as other, more traditional, methods. However in this case we have an increase of around a factor of 4.5 in error. This grows super-linear with the number of time steps we try to predict.","838e60b4":"## Importing libraries and loading in the data"}}