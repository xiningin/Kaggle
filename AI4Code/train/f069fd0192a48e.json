{"cell_type":{"093ece89":"code","2dd3d0b1":"code","759de6ec":"code","46653dd1":"code","c9699861":"code","42dea0ec":"code","84703ac7":"code","9469486e":"code","77a1f44a":"code","ce451fef":"code","5b09b4c6":"code","3e7fc4c4":"code","b2504a42":"code","7e92e504":"code","1be3bb2a":"code","ccf2ec64":"code","d9dce2e8":"code","893f5cea":"code","e45bc325":"code","bf0b8394":"code","f8e2b0b6":"code","d77c3f56":"code","b1880777":"code","135ee48e":"code","e129267f":"code","2fedf4c8":"code","006d6450":"code","c66ffe09":"code","04408a8a":"code","33119b7d":"code","424182b4":"code","c8beebb8":"code","7486dbc1":"code","692f1640":"code","61a3d80e":"code","c1d9e107":"code","06053f8d":"code","c455e863":"code","8558db6c":"code","6946e7a4":"code","bc080674":"code","a3685e30":"code","21f5f28c":"code","375090a4":"code","7538ba3e":"code","c1127201":"code","ddc3afdc":"code","606ac776":"code","da92e7e9":"code","65133e31":"code","31570b5c":"code","1f771349":"code","ecfa6821":"code","a6789832":"code","19429d89":"code","8072310f":"code","1a1b836d":"code","c1108f4f":"code","561c1348":"code","6e4701ff":"code","1bc8c1ba":"code","a8ade91a":"code","772f27d3":"code","3a7119ac":"code","2f95d307":"markdown","0aa98263":"markdown","72c3d6b5":"markdown","e31dc8d4":"markdown","9382e0d7":"markdown","271d67c3":"markdown","530f9de6":"markdown","83e86589":"markdown","c7a0870e":"markdown","df009b21":"markdown","09e78aef":"markdown","2f0d03f0":"markdown","03b865f7":"markdown","52495241":"markdown","ea0de77b":"markdown","fb6e469a":"markdown","eb749520":"markdown","8f3b29b4":"markdown","1ada61c1":"markdown"},"source":{"093ece89":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport random\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.model_selection import StratifiedKFold, KFold, RepeatedKFold\nfrom catboost import CatBoostRegressor\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import  RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\nlbl = LabelEncoder()\nimport os\ncolor = sns.color_palette()\nsns.set_style('darkgrid')\n%matplotlib inline\nprint(os.listdir(\"..\/input\"))\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 100)","2dd3d0b1":"path = \"..\/input\/\"\ntrain_identity = pd.read_csv(path+'train_identity.csv')\ntrain_transaction = pd.read_csv(path+'train_transaction.csv')\ntest_identity = pd.read_csv(path+'test_identity.csv')\ntest_transaction = pd.read_csv(path+'test_transaction.csv')","759de6ec":"# It will be comment before final submission\ntrain_transaction = train_transaction.sample(frac=0.05, random_state=10)","46653dd1":"train_identity.head()","c9699861":"train_transaction.head()","42dea0ec":"print(\"Train transaction are cols {} and rows {}\".format(train_transaction.shape[0], train_transaction.shape[1]))\nprint(\"Train identity are cols {} and rows {}\".format(train_identity.shape[0], train_identity.shape[1]))\nprint(\"Train transaction are cols {} and rows {}\".format(test_transaction.shape[0], test_transaction.shape[1]))\nprint(\"Train identity are cols {} and rows {}\".format(test_identity.shape[0], test_identity.shape[1]))","84703ac7":"train_df = pd.merge(train_transaction, train_identity, how='left', left_on=['TransactionID'], right_on=['TransactionID'], right_index=False)\ntest_df = pd.merge(test_transaction, test_identity, how='left', left_on=['TransactionID'], right_on=['TransactionID'], right_index=False)","9469486e":"print(\"Train dataframe are cols {} and rows {}\".format(train_df.shape[0], train_df.shape[1]))\nprint(\"test dataframe are cols {} and rows {}\".format(test_df.shape[0], test_df.shape[1]))","77a1f44a":"train_df.head(5)","ce451fef":"train_df.describe()","5b09b4c6":"train_df.describe(include=['O'])","3e7fc4c4":"train_df[[\"ProductCD\", \"isFraud\"]].groupby(['ProductCD'], as_index=False).mean().sort_values(by='isFraud', ascending=False)","b2504a42":"sns.countplot(test_df['card6'])\nplt.show()","7e92e504":"train_df[[\"card6\", \"isFraud\"]].groupby(['card6'], as_index=False).mean().sort_values(by='isFraud', ascending=False)","1be3bb2a":"sns.countplot(test_df['card4'])\nplt.show()","ccf2ec64":"train_df[[\"card4\", \"isFraud\"]].groupby(['card4'], as_index=False).mean().sort_values(by='isFraud', ascending=False)","d9dce2e8":"sns.countplot(train_df['ProductCD'])\nplt.show()","893f5cea":"total_fraud = train_df.loc[(train_df['isFraud'] == 1),].shape[0]","e45bc325":"fraud_percentage = (total_fraud*100)\/train_df.shape[0]\nprint(\"Total fraud percenate \",format(fraud_percentage,'.2f'),\"%\")","bf0b8394":"round(train_df['TransactionAmt'].sum(), 2)","f8e2b0b6":"round(train_df['TransactionAmt'].max(),2 )","d77c3f56":"train_df.loc[(train_df['isFraud'] == 1),'TransactionAmt'].max()","b1880777":"train_df.loc[(train_df['isFraud'] == 1),'TransactionAmt'].min()","135ee48e":"format(train_df.loc[(train_df['isFraud'] == 1),'TransactionAmt'].sum(), '.2f')","e129267f":"format((train_df.loc[(train_df['isFraud'] == 1),'TransactionAmt'].sum()*100)\/train_df['TransactionAmt'].sum(), '.2f')","2fedf4c8":"train_df['TransactionAmt'].apply(np.log).plot(kind='hist', bins=50) \nplt.show()","006d6450":"#for columns in train_df.columns:\n#    print(columns)","c66ffe09":"print(train_df['card1'].min())\nprint(train_df['card2'].min())\nprint(train_df['card3'].min())\nprint(train_df['card5'].min())","04408a8a":"print(train_df['card1'].max())\nprint(train_df['card2'].max())\nprint(train_df['card3'].max())\nprint(train_df['card5'].max())","33119b7d":"train_df['card_avg'] = round((train_df['card1']+train_df['card2']+train_df['card3']+train_df['card5'])\/4, 2)\ntest_df['card_avg'] = round((test_df['card1']+test_df['card2']+test_df['card3']+test_df['card5'])\/4, 2)","424182b4":"pd.isnull(train_df['card_avg']).sum()","c8beebb8":"card_avg = round(train_df['card_avg'].mean(), 2)\ntrain_df['card_avg']= train_df['card_avg'].fillna(card_avg)\ntest_df['card_avg']= test_df['card_avg'].fillna(card_avg)","7486dbc1":"train_df.head()","692f1640":"test_df.shape","61a3d80e":"#pd.isnull(train_df).sum()","c1d9e107":"selected_col = ['TransactionAmt','C1','C2','C3','C4','C5','C6','C7','C8','C9','C10','C11','C12','C13','C14','D1','V95','V96','V97','V98','V99','V100','V101','V102','V103','V104','V105','V106','V107','V108','V109','V110','V111','V112','V113','V114','V115','V116','V117','V118','V119','V120','V121','V122','V123','V124','V125','V126','V127','V128','V129','V130','V131','V132','V133','V134','V135','V136','V137','V279','V280','V281','V282','V283','V284','V285','V286','V287','V288','V289','V290','V291','V292','V293','V294','V295','V296','V297','V298','V299','V300','V301','V302','V303','V304','V305','V306','V307','V308','V309','V310','V311','V312','V313','V314','V315','V316','V317','V318','V319','V320','V321','card_avg']","06053f8d":"len(selected_col)","c455e863":"train_df[selected_col].columns[pd.isnull(train_df[selected_col]).sum() != 0]","8558db6c":"import random\n#random.choice([3, 4, 99, 29, 49])","6946e7a4":"#random.choice(train_df['V106'][~train_df['V106'].isnull()])","bc080674":"#train_df['V106'] = train_df['V106'].fillna(random.choice(train_df['V106'][~train_df['V106'].isnull()]))","a3685e30":"na_cols = []\nna_cols = ['D1', 'V95', 'V96', 'V97', 'V98', 'V99', 'V100', 'V101', 'V102', 'V103',\n       'V104', 'V105', 'V107', 'V108', 'V109', 'V110', 'V111', 'V112', 'V113',\n       'V114', 'V115', 'V116', 'V117', 'V118', 'V119', 'V120', 'V121', 'V122',\n       'V123', 'V124', 'V125', 'V126', 'V127', 'V128', 'V129', 'V130', 'V131',\n       'V132', 'V133', 'V134', 'V135', 'V136', 'V137', 'V281', 'V282', 'V283',\n       'V288', 'V289', 'V296', 'V300', 'V301', 'V313', 'V314', 'V315']","21f5f28c":"col =''\nfor col in na_cols:\n    train_df[col] = train_df[col].fillna(random.choice(train_df[col][~train_df[col].isnull()]))\n    test_df[col] = test_df[col].fillna(random.choice(test_df[col][~test_df[col].isnull()]))","375090a4":"test_df[selected_col].columns[pd.isnull(test_df[selected_col]).sum() != 0]","7538ba3e":"test_na_cols = ['C1', 'C2', 'C3', 'C4', 'C5', 'C6', 'C7', 'C8', 'C9', 'C10', 'C11',\n       'C12', 'C13', 'C14', 'V279', 'V280', 'V284', 'V285', 'V286', 'V287',\n       'V290', 'V291', 'V292', 'V293', 'V294', 'V295', 'V297', 'V298', 'V299',\n       'V302', 'V303', 'V304', 'V305', 'V306', 'V307', 'V308', 'V309', 'V310',\n       'V311', 'V312', 'V316', 'V317', 'V318', 'V319', 'V320', 'V321']\n\ncol =''\nfor col in test_na_cols:\n    test_df[col] = test_df[col].fillna(random.choice(test_df[col][~test_df[col].isnull()]))","c1127201":"train_df.loc[:,train_df.dtypes =='object'].columns","ddc3afdc":"train_df['ProductCD']= lbl.fit_transform(train_df['ProductCD']) \ntest_df['ProductCD']= lbl.fit_transform(test_df['ProductCD']) ","606ac776":"from catboost import CatBoostRegressor\nfrom catboost import CatBoostClassifier","da92e7e9":"catboost = CatBoostClassifier(iterations=1000)\ncatboost.fit(train_df[selected_col],\n          train_df['isFraud'],\n          verbose = False)","65133e31":"catboost.score(train_df[selected_col],train_df['isFraud'])","31570b5c":"#catboost.predict(test_df[selected_col])","1f771349":"pd.isnull(train_df[selected_col]).sum()","ecfa6821":"train_df['V106'] = train_df['V106'].fillna(random.choice(train_df['V106'][~train_df['V106'].isnull()]))\ntest_df['V106'] = test_df['V106'].fillna(random.choice(test_df['V106'][~test_df['V106'].isnull()]))","a6789832":"model_name = []\nmodel_score = []","19429d89":"kneighbors = KNeighborsClassifier()\nkneighbors.fit(train_df[selected_col],train_df['isFraud'])\nkneighbors_score = round(kneighbors.score(train_df[selected_col],train_df['isFraud'])*100, 2)\nmodel_name.append('KNeighborsClassifier')\nmodel_score.append(kneighbors_score)\nkneighbors_score","8072310f":"linsvc = LinearSVC()\nlinsvc.fit(train_df[selected_col],train_df['isFraud'])\nlinsvc_score = round(linsvc.score(train_df[selected_col],train_df['isFraud'])*100, 2)\nmodel_name.append('LinearSVC')\nmodel_score.append(linsvc_score)\nlinsvc_score","1a1b836d":"randomforest = RandomForestClassifier(n_estimators=8, max_depth=10, min_samples_split=0.8, random_state=58)\nrandomforest.fit(train_df[selected_col],train_df['isFraud'])\nrandomforest_score = round(randomforest.score(train_df[selected_col],train_df['isFraud'])*100, 2)\nmodel_name.append('RandomForestClassifier')\nmodel_score.append(randomforest_score)\nrandomforest_score","c1108f4f":"#x_train, x_test, y_train, y_test = train_test_split(train_df[selected_col], train_df['isFraud'], test_size=0.05)","561c1348":"gradient = GradientBoostingClassifier()\ngradient.fit(train_df[selected_col],train_df['isFraud'])\ngradient_score = round(gradient.score(train_df[selected_col],train_df['isFraud'])*100, 2)\nmodel_name.append('GradientBoostingClassifier')\nmodel_score.append(gradient_score)\ngradient_score","6e4701ff":"all_score = pd.DataFrame({'model_name':model_name, 'model_score':model_score})\nall_score","1bc8c1ba":"#pd.isnull(test_df[selected_col]).sum()","a8ade91a":"predicted_fraud_detection = kneighbors.predict(test_df[selected_col])","772f27d3":"my_submission = pd.DataFrame({'TransactionID':test_transaction['TransactionID'], 'isFraud':predicted_fraud_detection})","3a7119ac":"my_submission.to_csv('my_submission.csv', index=False)","2f95d307":"Using catboost model","0aa98263":"I got to join this both the datashet then prepare features after that apply this feature in many models. The column name TransactionID is the same in both tables.","72c3d6b5":"Fill the missing colums values by the random value.","e31dc8d4":"#### Total required 506691 cols in final submition.","9382e0d7":"### Conclusion\n<p>Our target value is binary so it\u2019s a binary classification problem. Regression will not work here.<\/p>\n**I am still working in this kernel** <br\/>\nAny suggestions to improve our score are most welcome. Upvote would be appreciated - That will keep me motivated :) ","271d67c3":"I seen <code>train_transaction.csv<\/code> file that lots of missing value. when lots of missing value in the dataset definitely that effect in over result.\nI am only taking a 20% dataset data and I know when taking sample size small that result is overfitted. \n\n\n<ul>\n    <li> Data Exploration <\/li>\n    <li> Data visualization <\/li>\n    <li> Create feature <\/li>\n    <li> Make pipline <\/li>\n    <li> Predict result <\/li>\n    <li> Submit result <\/li>\n    <li> Conclusion <\/li>\n<\/ul>","530f9de6":"Total transaction amount","83e86589":"Explore datashet.","c7a0870e":"Find object in selected columns","df009b21":"Applying models","09e78aef":"Max transaction amount","2f0d03f0":"Fill the missing colums values by the random value.","03b865f7":"Load train data","52495241":"Max fraud amount","ea0de77b":"Percentage of fraud amount","fb6e469a":"Total fraud amount","eb749520":"Select only nan columns in selected columns.","8f3b29b4":"After join dataset check numbers of rows and columns.","1ada61c1":"Min fraud amount"}}