{"cell_type":{"26ffc4a8":"code","6e47e963":"code","d82ac21a":"code","9741a8e3":"code","1f573ebb":"code","406a6300":"code","d98b24e8":"code","b358ab79":"code","bf7f22bb":"code","d8f8058e":"code","9af61862":"code","8f23c806":"code","04516bcf":"code","45b335b4":"code","ea385736":"code","e16d10fa":"code","9e5f7dde":"code","e909a81c":"code","de8ad766":"code","ecb36884":"code","7123db83":"markdown","0c85915d":"markdown","5887bd2e":"markdown","b12fd295":"markdown","dcfb9dfe":"markdown"},"source":{"26ffc4a8":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","6e47e963":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nimport seaborn as sns\n\n# Import things I need","d82ac21a":"data = pd.read_csv(\"..\/input\/heart-failure-clinical-data\/heart_failure_clinical_records_dataset.csv\")\ndisplay(data)\ndisplay(data.describe())","9741a8e3":"null_value = data.isnull().sum()\nprint(null_value)\n\n# We check whether there are null_values and there are no null_values so we can pass this process","1f573ebb":"val_check = [\"age\",\"creatinine_phosphokinase\",\"ejection_fraction\",\"platelets\",\"serum_creatinine\",\"serum_sodium\",\"time\"]\n\nplt.figure(figsize=(30,15))\nn1 = 0\nfor i in range(1,8):\n    plt.subplot(2,4,i)\n    plt.hist(data[val_check[n1]])\n    plt.title(val_check[n1])\n    n1 +=1\n    \n# We should also check whether data follow normal distribuion. We can find that \"creatinine_phosphokinase\" and \"serum_creatinine\" columns skewed a lot.","406a6300":"def logarithm(x):\n    return np.log(x+1)","d98b24e8":"df_2 = data.copy()\n\nlog1 = data.creatinine_phosphokinase.apply(lambda x : logarithm(x)).to_frame()\nlog2 = data.serum_creatinine.apply(lambda x : logarithm(x)).to_frame()\n\ndf_2.drop([\"creatinine_phosphokinase\",\"serum_creatinine\"],axis=1,inplace=True)\n\n\ndisplay(df_2)\n\n# Normalize data through logarithm function.","b358ab79":"df_3 = pd.concat([df_2,log1,log2],axis=1)\ndisplay(df_3)\n\nresult_val = [\"creatinine_phosphokinase\",\"serum_creatinine\"]\n\nplt.figure(figsize=(10,10))\nfor j in range(0,2):\n    plt.subplot(1,2,j+1)\n    plt.hist(df_3[result_val[j]])\n    plt.title(result_val[j])","bf7f22bb":"from sklearn.preprocessing import MinMaxScaler\n\n# As I mentioned earlier, we should stasndardilize each variables so that one variable can not affect to the result more than other variables. So I use MinMaxScaler ","d8f8058e":"scaler = MinMaxScaler()\ndf_4 = pd.DataFrame(scaler.fit_transform(df_3),columns=df_3.columns)\ndisplay(df_4)","9af61862":"corr = df_4.corr()\nplt.figure(figsize=(16,16))\ncmap = sns.cubehelix_palette(as_cmap=True)\nsns.heatmap(corr,fmt=\".2f\",annot=True,cmap=cmap,vmin=0.2)\n\n# When we see \"Death_event\", \"age\" and \"serum_creatinine\" affect most","8f23c806":"from sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import fbeta_score\n\n# For classification I choose RandomForestClassifier, AdaBoostClassifier and Support Vector machine.","04516bcf":"X = df_4.loc[:,df_4.columns != \"DEATH_EVENT\"]\ny = df_4.loc[:,\"DEATH_EVENT\"]\n\nX_train,X_test,y_train,y_test = train_test_split(X,y,random_state=62)\n\n# Split data and define X_train, X_test, y_train, y_test to train model. I define y variable as \"Death event\" which indicate whether patient deceased.\n# I import Fbeta_score and I will weight more to recall rather than precision because data that I classified should reflect well about real data. ","45b335b4":"model1 = RandomForestClassifier()\nmodel1.fit(X_train,y_train)\nmodel1_preds = model1.predict(X_test)\naccuracy1 = accuracy_score(y_test,model1_preds)\nfbeta_1 = fbeta_score(y_test,model1_preds,beta=1.5)\nprint(\"Accuracy of RandomForestClassifier : {}  fbeta score : {}\".format(accuracy1,fbeta_1))\n\n# First I train RandomForestClassifier and test it. Accuracy_score is 0.84 and f1_score is 0.74. Not bad","ea385736":"model2 = AdaBoostClassifier()\nmodel2.fit(X_train,y_train)\nmodel2_preds = model2.predict(X_test)\nAccuracy2 = accuracy_score(y_test,model2_preds)\nfbeta_2 = fbeta_score(y_test,model2_preds,beta=1.5)\nprint(\"Accuracy of AdaBoostClassifier : {} fbeta score : {}\".format(Accuracy2,fbeta_2))","e16d10fa":"model3 = SVC()\nmodel3.fit(X_train,y_train)\nmodel3_preds = model3.predict(X_test)\nAccuracy3 = accuracy_score(y_test,model3_preds)\nfbeta_3 = fbeta_score(y_test,model3_preds,beta=1.5)\nprint(\"Accuracy of SVC : {} fbeta score : {}\".format(Accuracy3,fbeta_3))","9e5f7dde":"from sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import make_scorer\nimport time\n\n# Import things that I need. Also I import make_scorer for scoring. In this case I also use fbeta_score that has beta = 1.5. \n# Also I import time so that I can measure time for searching best estimator.","e909a81c":"scorer = make_scorer(fbeta_score,beta=1.5)\nparameters = {\n    \"n_estimators\" : [100,150,200,250,300],\n    \"min_samples_split\" : [2,4,6],\n    \"min_samples_leaf\" : [4,6,8],\n    \"max_depth\" : [80,100,150,200]\n    \n}\n\nstart = time.time()\ngrid = GridSearchCV(estimator=model1,param_grid=parameters,scoring=scorer,n_jobs=-1,cv=2)\ngrid.fit(X_train,y_train)\nend = time.time()\n\nprint(\"Search Time : {} seconds\".format(end-start))\n\ngrid.best_params_\n\n# Check RandomForestClassifier parameters here \"https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.RandomForestClassifier.html\"","de8ad766":"model4 = grid.best_estimator_\nmodel4_preds = model4.predict(X_test)\nAccuracy4 = accuracy_score(y_test,model4_preds)\nfbeta_4 = fbeta_score(y_test,model4_preds,beta=1.5)\nprint(\"GridSearch accuracy : {} fbeta score : {}\".format(Accuracy4,fbeta_4))\n\n# Through GridSearch I can improve my model. Accuracy : 0.84 \u2192 0.86 Fbeta : 0.74 \u2192 0.78","ecb36884":"Importance = np.sort(np.round(model4.feature_importances_*100,3))\ndf_feature = pd.DataFrame({\n    \"importance\" : Importance\n},index=X_train.columns)\ndisplay(df_feature)\n\nplt.figure(figsize=(16,16))\nplt.barh(df_feature.index.to_list(),df_feature.importance)\nplt.title(\"Feature importance\")\n\n# Through RandomForestClassifier, we can check feature importance. Through using numpy, sort values and make dataframe.","7123db83":"### We finish seeing and preprocessing dataset. After this, we will train model and evaluate model through accuracy score and f1 score","0c85915d":"### This is end of my code. Thank you for seeing my code!","5887bd2e":" ### Display dataset. We can know that all data type is num. So we don't need process that encode char to num. All we need is normalize, standandarlize dataset.\n \n### When we see data, data range are different. For example \"platelets\" columns show higher figure than other columns. If we don't standardlize columns, accuracy of model can be decreased.\n\n### First we start with EDA and preprocessing dataset","b12fd295":"# Heart Failure Prediction\n\n## Prerequisite\n\nPandas, Matplotlib, Seaborn, Sklearn\n\n## What we do?\n\nFirst I will do some EDA and visualization data. After that I will train model and find best model\n\n","dcfb9dfe":"### As a result, RandomForestClassifier shows highest Accuracy score and fbeta score so I will choose model1. And through this model I will do gridsearch so that I can improve my model."}}