{"cell_type":{"0b670668":"code","5d1abafa":"code","d61b59e0":"code","d3e72ab2":"code","ea9d54bc":"code","ccceb157":"code","33ea0f2d":"code","3d30e611":"code","6bbf9d28":"code","2619113a":"code","50edff4e":"code","e8b05207":"code","1313f7b9":"code","2dc5bb95":"code","c739e55a":"code","60b0caa5":"code","735d05eb":"code","1c4ee638":"code","a05589c5":"code","0b25c753":"code","c2da363e":"code","ed01f69a":"code","1924c919":"code","1e191347":"code","71241790":"code","dbb81973":"code","fae2b0a6":"code","a703d90c":"code","60841353":"code","15c44a78":"code","288811bf":"code","65c37100":"code","7134cf14":"code","60e75d13":"code","59d4450f":"code","86206163":"markdown","dc21b095":"markdown","d539768a":"markdown","5fcce731":"markdown","a4ea95ed":"markdown","c248631c":"markdown","6d1651d1":"markdown","960a35c5":"markdown","3f61136c":"markdown","3741de12":"markdown","90e79797":"markdown","7a8a9f95":"markdown","fbd93ac9":"markdown","634ca9d9":"markdown","a326fc07":"markdown","8b038802":"markdown","7c6a5ca9":"markdown","a0e3094e":"markdown","60294dcf":"markdown","b59dd8e0":"markdown","f2a3309d":"markdown"},"source":{"0b670668":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nsns.set_style('whitegrid')","5d1abafa":"df = pd.read_csv('..\/input\/customer-segmentation-tutorial-in-python\/Mall_Customers.csv')","d61b59e0":"df.head(2)","d3e72ab2":"df.shape","ea9d54bc":"df.info()","ccceb157":"df.describe()","33ea0f2d":"100*df.isnull().sum()\/df.shape[0]","3d30e611":"sns.countplot(df['Gender'])\nplt.xlabel('Gender')\nplt.ylabel('Number of Customers')\nplt.title('Customer Distribution by Gender')","6bbf9d28":"sns.distplot(df['Age'],kde=False,bins=30)\nplt.ylabel('Frequency')\nplt.title('Customer Distribution by Age')","2619113a":"sns.distplot(df[df['Gender'] == 'Male']['Age'],kde=False,bins=30,color='blue',label='Male')\nsns.distplot(df[df['Gender'] == 'Female']['Age'],kde=False,bins=30,color='red',label='Female')\nplt.title('Customer Distribution by Age and Gender')\nplt.ylabel('Number of Customers')\nplt.legend()","50edff4e":"sns.boxplot(df['Age'])","e8b05207":"sns.distplot(df['Annual Income (k$)'],kde=False,bins=25)\nplt.title('Customer Distribution by Annual Income (k$)')\nplt.ylabel('Frequency')","1313f7b9":"sns.distplot(df[df['Gender'] == 'Male']['Annual Income (k$)'],kde=False,bins=30,color='blue',label='Male')\nsns.distplot(df[df['Gender'] == 'Female']['Annual Income (k$)'],kde=False,bins=30,color='red',label='Female')\nplt.title('Customer Distribution by Annual Income (k$) and Gender')\nplt.ylabel('Number of Customers')\nplt.legend()","2dc5bb95":"sns.boxplot(df['Annual Income (k$)'])","c739e55a":"sns.distplot(df['Spending Score (1-100)'])","60b0caa5":"sns.distplot(df[df['Gender'] == 'Male']['Spending Score (1-100)'],color='blue',kde=False,label='Male')\nsns.distplot(df[df['Gender'] == 'Female']['Spending Score (1-100)'],color='red',kde=False,label='Female')\nplt.legend()","735d05eb":"sns.boxplot(df['Spending Score (1-100)'])","1c4ee638":"sns.pairplot(data=df,hue='Gender',palette='coolwarm')","a05589c5":"plt.figure(figsize=(12,6))\nsns.heatmap(df.corr(),cmap='viridis',annot=True)","0b25c753":"sns.lmplot(x='Annual Income (k$)',y='CustomerID',data=df)\nplt.title('CustomerID vs. Annual Income (k$)')","c2da363e":"df.dtypes","ed01f69a":"df['Gender'].unique()","1924c919":"df.Gender.value_counts()","1e191347":"dmap = {'Male':1,'Female':0}\ndf['Gender'] = df['Gender'].map(dmap)","71241790":"df.head(5)","dbb81973":"from sklearn.preprocessing import MinMaxScaler","fae2b0a6":"scaler = MinMaxScaler()","a703d90c":"data_scaled = scaler.fit_transform(df)","60841353":"from sklearn.cluster import KMeans\nwcss = []\nfor i in range(1, 11):\n    kmeans = KMeans(n_clusters = i, init = 'k-means++', random_state = 42)\n    kmeans.fit(data_scaled)\n    wcss.append(kmeans.inertia_)\nplt.plot(range(1, 11), wcss,marker='*')\nplt.title('The Elbow Method')\nplt.xlabel('Number of clusters')\nplt.ylabel('WCSS')\nplt.show()","15c44a78":"kmeans = KMeans(n_clusters=5,init='k-means++',max_iter=300,tol=0.0001).fit(data_scaled)\nkmeans","288811bf":"kmeans.labels_","65c37100":"kmeans.cluster_centers_","7134cf14":"clusters_data = pd.DataFrame(data_scaled,columns=['CustomerID', 'Genre', 'Age', 'Annual Income (k$)',\n       'Spending Score (1-100)'])\nclusters_data['Cluster'] = kmeans.labels_\nclusters_data.head()","60e75d13":"clusters_data.groupby('Cluster').count()['Spending Score (1-100)']","59d4450f":"from sklearn.metrics import confusion_matrix,classification_report\nprint(confusion_matrix(clusters_data['Cluster'],kmeans.labels_))\nprint(classification_report(clusters_data['Cluster'],kmeans.labels_))","86206163":"Check the data types of all the columns of the dataframe.","dc21b095":"k-means clustering is a method of vector quantization, that aims to partition ***n*** observations into ***k*** cluster in which each observation belongs to the cluster  with the nearest mean (cluster centers or cluster centroid), serving as a prototype of the cluster. The less the variation we have within clusters, the more homogenous (similar) the data points are within the same cluster.\n\nThe k-means algorithm can be summarised as follows:\n\n1. Specify the number of clusters to be created.\n2. Initialize centroids by first shuffling the dataset and then randomly selecting K data points for the centroids without replacement.\n3. Keep iterating until there is no change to the centroids. i.e assignment of data points to clusters isn\u2019t changing.\n-Compute the sum of the squared distance between data points and all centroids.\n\n-Assign each data point to the closest cluster (centroid).\n\n-Compute the centroids for the clusters by taking the average of the all data points that belong to each cluster.","d539768a":"**Customer Gender Visualization**\n\nCreate a count plot of the gender distribution across the mall customer dataset.","5fcce731":"# **Mall Customers Segmentation Project**\n\nCustomer Relationship Management (CRM) seeks to build relations with the most profitable clients by performing customer segmentation and designing appropriate marketing tools. This is particularly important within the competitive environment that combines sociodemographic characteristics of retail consumers and specialisation of sellers and buyers which forces companies to adopt a dynamic management of clients to achieve higher profits and to gain a higer share of the market than its competitors.\n\n","a4ea95ed":"**Annual Income Analysis**","c248631c":"There seems to a strong positive correlation between CustomerID and Annual Income (k$). Let us investigate this further by plotting an lmplot.","6d1651d1":"**Create a pairplot of the Mall Customer Data.**","960a35c5":"# **2. Load Data**","3f61136c":"**Elbow Method**","3741de12":"# **4. Feature Engineering**","90e79797":"**Customer Distribution by Age**\n\nCreate a distribution plot of the ages by frequencies","7a8a9f95":"**Create a heatmap of the Mall Customer Data.**","fbd93ac9":"# **6. Evaluation**","634ca9d9":"# **1. Import Libraries**","a326fc07":"From the above plot, we could conclude that the optimal number of cluster is 5 since it occurs at the bend in the elbow plot.","8b038802":"**Determining Optimal Clusters**\n\nDetermining the optimal number of clusters in a data set is fundamental in partitioning into clusters, which requires the user to specify the number of clusters ***k*** to be generated.\n\nThe methods employed in determining the optimal number of cluster can be categorised into*** Direct Methods*** and ***Statistical Testing Methods***.\n\n1. Direct Methods consist of optimising a criterion, that is, within cluster sum of squares. *Elbow* and *silhouette methods* are in this category.\n2. Statistical Methods compare the evidence against the null hypothesis. *Gap statistic *is one of the examples.\n\n\n\n* **Elbow method**\n\nThe well-known elbow method is to identify the number of clusters based on the assumption that the optimal number of clusters must produce small inertia, or total intra-cluster variation. As such, there will be a trade-off between the inertia and the number of clusters.\n\n* **Silhouette method**\n\nSilhouette score measures how well an observation is clustered and it estimates the average distance between clusters. It wants to find the optimal number of clusters that produce a subdivision of the dataset to dense blocks that are well separated from each other. \n\nThe value will be between -1 and 1, whereas a value near 0 indicates overlapping clusters. Negative values generally indicate that an observation has been assigned to the wrong cluster.\n\n\n\n* **Gap Statistic Method**\n\nThe gap statistic compares the total within intra-cluster variation for different values of k with their expected values under the null reference distribution of the data. Hence, the optimal choice of k is the value that maximizes the gap (meaning that the clustering structure is far away from a random uniform distribution of points).\n\n\n\n.","7c6a5ca9":"Check the number of null values.","a0e3094e":"# **5. Customer Segmentation via K-Means Clustering**","60294dcf":"**Training the K-Means model on the dataset**","b59dd8e0":"**Normalizing the Data**\n","f2a3309d":"# **3. Exploratory Data Analysis**"}}