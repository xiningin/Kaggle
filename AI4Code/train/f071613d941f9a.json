{"cell_type":{"916efb02":"code","fafe9c78":"code","5d1f540a":"code","9abb123d":"code","83037000":"code","b8e3453e":"code","be8c7ffe":"code","8b6b2637":"code","75cb94da":"code","023463c9":"code","02477f7e":"code","7eaf6afb":"code","105ebb0c":"code","a03ae1fa":"code","14f8d343":"markdown","c5b77fa7":"markdown","01efeea4":"markdown","35f5c762":"markdown"},"source":{"916efb02":"import numpy as np # linear algebra\nimport pandas as pd\nfrom nltk import word_tokenize\nfrom nltk.corpus import stopwords\nfrom sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\nfrom sklearn.model_selection import train_test_split, cross_val_score\nimport os\nprint(os.listdir(\"..\/input\"))","fafe9c78":"train_df=pd.read_csv('..\/input\/train.csv')[:200000]\ntrain_df.shape","5d1f540a":"vectorizer = TfidfVectorizer(min_df=5,strip_accents='unicode',lowercase =True, analyzer='word',use_idf=True, smooth_idf=True, sublinear_tf=True, \n                        stop_words = 'english',tokenizer=word_tokenize)","9abb123d":"#train_vectorized = vectorizer.transform(train_df.question_text.values)\n\n#train1_tfidf=\nvectorizer.fit_transform(train_df[train_df.target==1].question_text.values)\nwoord1=vectorizer.get_feature_names()\n#train0_tfidf=\nvectorizer.fit_transform(train_df[train_df.target==0].question_text.values)\nwoord0=vectorizer.get_feature_names()\n\nwoord01=[x for x in set(woord1) if x in set(woord0)]\nprint(len(woord01),len(woord1),len(woord0))\nwordsimpf=[x for x in set(woord1) if x not in set(woord01)]\nlen(wordsimpf)","83037000":"EMBEDDING_FILE = '..\/input\/embeddings\/glove.840B.300d\/glove.840B.300d.txt'\ndef get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\nembeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE))\n\nall_embs = np.stack(embeddings_index.values())\nemb_mean,emb_std = all_embs.mean(), all_embs.std()\nembed_size = all_embs.shape[1]","b8e3453e":"\ndef embedword(word_index,word_pos,word_neg):\n    nb_words = min(60000, len(word_index))\n    embedding_matrix_1 =pd.DataFrame([])\n    i=0\n    for word in word_index:\n        embedding_vector = embeddings_index.get(word)\n        embedding_matrix_1=embedding_matrix_1.append(pd.DataFrame(embedding_vector,columns=[i]).T)\n        i=i+1\n\n\n    #el embeddings_index\n    embedding_matrix_1['woord']=word_index\n    embedding_matrix_1['target']=0\n    for w in word_pos:\n        pos=embedding_matrix_1.loc[embedding_matrix_1['woord'] ==w].index\n        if pos.size>0:\n            embedding_matrix_1.iat[pos[0],301]=1\n\n    for w in word_neg:\n        pos=embedding_matrix_1.loc[embedding_matrix_1['woord'] ==w].index\n        if pos.size>0:\n            embedding_matrix_1.iat[pos[0],301]=2\n    embedding_matrix_1.plot.scatter(x=0,y=1,c='target',colormap='viridis')\n    return embedding_matrix_1.fillna(0)","be8c7ffe":"wh1=embedword(woord1,woord0,wordsimpf)\nwh0=embedword(woord0,woord1,wordsimpf)","8b6b2637":"del embeddings_index,EMBEDDING_FILE","75cb94da":"from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import SGDClassifier\nfrom xgboost import XGBClassifier\n\nclf=KNeighborsClassifier(3)\nclf = SGDClassifier(max_iter=1000)\nclf =XGBClassifier(max_depth=5, base_score=0.005)\nwht=wh0.append(wh1)\nclf.fit(wht.drop(labels=['woord','target'],axis=1).fillna(0),wht['target'])","023463c9":"wht['pred']=clf.predict(wht.drop(labels=['woord','target'],axis=1).fillna(0))  #==wht['target']).mean()","02477f7e":"wb=wht[wht[\"pred\"]>0].woord\n\nvectorizer2 = TfidfVectorizer(min_df=5,strip_accents='unicode',lowercase =True, analyzer='word',use_idf=True, smooth_idf=True, sublinear_tf=True, \n                        stop_words = 'english',vocabulary=np.unique(wb),tokenizer=word_tokenize)\n#vectorizer2 = CountVectorizer(min_df=5,strip_accents='unicode',lowercase =True, analyzer='word',stop_words = 'english',vocabulary=np.unique(wb),tokenizer=word_tokenize)\ntrain_tfidf=vectorizer2.fit_transform([train_df.question_text.sum()] )\n","7eaf6afb":"len( vectorizer2.get_feature_names() )","105ebb0c":"wordcount=pd.DataFrame(vectorizer2.get_feature_names(),columns=['word'])\nwordcount['idf']=1\/train_tfidf.T.sum(axis=1)\ntemp=wht[wht[\"pred\"]>0].groupby(\"woord\").max()\nwordcount['target']=temp.target.values\nwordcount['pred']=temp.pred.values\nwordcount[  wordcount.target>1].sort_values(by=['idf','target','word'])","a03ae1fa":"wordcount[  wordcount.pred>1].sort_values(by=['idf','target','word'])","14f8d343":"# take glove vh matrix","c5b77fa7":"# classify words with the bad words","01efeea4":"# shitty words","35f5c762":"# split in words"}}