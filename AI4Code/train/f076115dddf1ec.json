{"cell_type":{"76508da6":"code","6655e99b":"code","108c1af0":"code","cfb4b0e2":"code","2fb2962b":"code","da245e9d":"code","9509e6ff":"code","16f725fe":"code","5b02b178":"code","7a427031":"code","bb8cbc80":"code","be5f56a9":"code","424081a6":"code","8736e9bb":"code","6ccc550f":"code","3be62b60":"code","47a23201":"code","11b7be4f":"code","0f80deda":"code","9b547cbd":"code","b199781f":"code","ef37b878":"code","9f89fce6":"code","f665a95d":"code","4d0a19fb":"code","4c201401":"code","c523bafb":"code","91d6a565":"code","66283649":"code","2b1dfd68":"code","30398b9b":"code","442d59c6":"markdown","d375680a":"markdown","11fda6cb":"markdown","f4a64fdf":"markdown","2beceb4e":"markdown","07a17e4d":"markdown","7626399c":"markdown","c88ecbc3":"markdown","4c7daba6":"markdown","17c2d737":"markdown","10fde130":"markdown","bc1c6f39":"markdown","ff0b8da6":"markdown"},"source":{"76508da6":"from operator import itemgetter    \nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')\nget_ipython().magic(u'matplotlib inline')\nplt.style.use('ggplot')\n\nfrom pandas.tools.plotting import scatter_matrix\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\nfrom sklearn.preprocessing import RobustScaler, StandardScaler, LabelEncoder, MinMaxScaler, OneHotEncoder, LabelBinarizer\nfrom sklearn.metrics import mean_squared_error, accuracy_score\nfrom sklearn.pipeline import Pipeline, make_pipeline, FeatureUnion\nfrom scipy.stats import skew\nfrom sklearn.decomposition import PCA, KernelPCA\nfrom sklearn.preprocessing import Imputer\nfrom sklearn.base import clone, BaseEstimator, TransformerMixin\nfrom sklearn.cross_validation import KFold, cross_val_score\nfrom sklearn.model_selection import cross_val_score, GridSearchCV, RandomizedSearchCV, KFold, cross_val_predict, StratifiedKFold, train_test_split, learning_curve, ShuffleSplit\nfrom sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet, SGDRegressor, BayesianRidge\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, ExtraTreesRegressor, BaggingRegressor\nfrom sklearn.svm import SVR, LinearSVR\nfrom sklearn.kernel_ridge import KernelRidge\nfrom xgboost import XGBRegressor\nimport lightgbm as lgb\n\nprint(\"Modules imported \\n\")\n\nprint(\"Files in current directory:\")\nimport os\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"..\/input\"]).decode(\"utf8\")) #check the files available in the directory\n# Any results you write to the current directory are saved as output.","6655e99b":"# Load raw data\ntrain = pd.read_csv('..\/input\/train.csv') \ntest = pd.read_csv('..\/input\/test.csv') \n\n# Locally \n#train = pd.read_csv('\/Users\/Alex\/Desktop\/HousingLinReg\/train.csv') \n#test = pd.read_csv('\/Users\/Alex\/Desktop\/HousingLinReg\/test.csv') \nprint(\"train \", train.shape)\nprint(\"test \", test.shape)","108c1af0":"# Histograms\ntrain.hist(bins=50, figsize=(20,15))\nplt.show()","cfb4b0e2":"# Check contents of ONE column\nprint(train[\"MSSubClass\"].value_counts())\n","2fb2962b":"#Deleting outliers\n#plt.figure(figsize=(12,6))\n#plt.scatter(x=train.GrLivArea, y=train.SalePrice)\n#plt.xlabel(\"GrLivArea\", fontsize=13)\n#plt.ylabel(\"SalePrice\", fontsize=13)\n#plt.ylim(0,800000)\n\n#train = train.drop(train[(train['GrLivArea']>4000) & (train['SalePrice']<300000)].index)\n\n#plt.figure(figsize=(12,6))\n#plt.scatter(x=train.GrLivArea, y=train.SalePrice)\n#plt.xlabel(\"GrLivArea\", fontsize=13)\n#plt.ylabel(\"SalePrice\", fontsize=13)\n#plt.ylim(0,800000)\n\n#print(\"train without outliers \", train.shape)","da245e9d":"# Pearson Correlation Coefficient\ncorr_matrix = train.corr()\ncorr_matrix[\"SalePrice\"].sort_values(ascending=False)","9509e6ff":"# Scatter_matrix\nattributes = [\"OverallQual\", \"YrSold\", \"YearBuilt\",\"SalePrice\"]\nscatter_matrix(train[attributes], figsize=(12, 8))","16f725fe":"# Zoom in on one plot\ntrain.plot(kind=\"scatter\", x=\"OverallQual\", y=\"SalePrice\",alpha=0.2)","5b02b178":"# Some transformation should be done on train + test\n# If not - there's a difference between the columns in train and test after get_dummies as there are different options in train vs test\n# Scaler - is important to be done separately as not to influence the mean and std of train with those of test, this leads to snooping on the test and overfitting\n\ntrainWprice = pd.DataFrame(train)\ntrainNoPrice = trainWprice.drop(\"SalePrice\", axis=1)\n\nfull=pd.concat([trainNoPrice,test], ignore_index=True)\nfull.drop(['Id'],axis=1, inplace=True)\n\nprint(\"train \", train.shape)\nprint(\"test \", test.shape)\nprint(\"full without Id and no SalePrice \", full.shape)","7a427031":"# ### Missing Data\nColsMissingValues = full.isnull().sum()\nprint(\"There are \", len(ColsMissingValues[ColsMissingValues>0]), \" features with missing values\")\n#print(\"_\"*80)\nall_data_na = (full.isnull().sum() \/ len(full)) * 100\nall_data_na = all_data_na.sort_values(ascending=False)\nmissing_data = pd.DataFrame({'Missing Ratio' :all_data_na})\nprint(missing_data.head(len(ColsMissingValues[ColsMissingValues>0])))","bb8cbc80":"class feat_eng(BaseEstimator, TransformerMixin):\n    def __init__(self, fill_missvals = True):\n        self.fill_missvals = fill_missvals\n        pass\n    \n    def fit(self,X,y=None):\n        return self\n    \n    def transform(self,X):\n        if self.fill_missvals:\n            X[\"PoolQC\"] = X[\"PoolQC\"].fillna(\"None\")\n            X[\"MiscFeature\"] = X[\"MiscFeature\"].fillna(\"None\")\n            X[\"Alley\"] = X[\"Alley\"].fillna(\"None\")\n            X[\"Fence\"] = X[\"Fence\"].fillna(\"None\")\n            X[\"FireplaceQu\"] = X[\"FireplaceQu\"].fillna(\"None\")\n            X[\"LotFrontage\"] = X.groupby(\"Neighborhood\")[\"LotFrontage\"].transform(lambda x: x.fillna(x.median()))\n            X['MSZoning'] = X['MSZoning'].fillna(X['MSZoning'].mode()[0])            \n            X[\"Functional\"] = X[\"Functional\"].fillna(\"Typ\")\n            X['Electrical'] = X['Electrical'].fillna(X['Electrical'].mode()[0])\n            X['KitchenQual'] = X['KitchenQual'].fillna(X['KitchenQual'].mode()[0])\n            X['Exterior1st'] = X['Exterior1st'].fillna(X['Exterior1st'].mode()[0])\n            X['Exterior2nd'] = X['Exterior2nd'].fillna(X['Exterior2nd'].mode()[0])\n            X['SaleType'] = X['SaleType'].fillna(X['SaleType'].mode()[0])\n            X['MSSubClass'] = X['MSSubClass'].fillna(\"None\")\n\n            for col in ('GarageType', 'GarageFinish', 'GarageQual', \n                        'GarageCond', 'BsmtQual', 'BsmtCond', 'BsmtExposure', \n                        'BsmtFinType1', 'BsmtFinType2','MasVnrType'):\n                X[col] = X[col].fillna('None')\n                \n            for col in ('GarageYrBlt', 'GarageArea', 'GarageCars', 'MasVnrArea'):\n                X[col] = X[col].fillna(0)\n                \n            for col in ('BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF','TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath'):\n                X[col] = X[col].fillna(0)\n                \n            X['MSSubClass'] = X['MSSubClass'].apply(str) \n            X['OverallCond'] = X['OverallCond'].astype(str)\n            X['YrSold'] = X['YrSold'].astype(str)\n            X['MoSold'] = X['MoSold'].astype(str)\n            \n            X = X.drop(['Utilities'], axis=1)\n            \n        return X","be5f56a9":"class add_feature(BaseEstimator, TransformerMixin):\n    def __init__(self,additional=1):\n        self.additional = additional\n    \n    def fit(self,X,y=None):\n        return self\n    \n    def transform(self,X):\n        if self.additional==1:\n            X[\"TotalHouse\"] = X[\"TotalBsmtSF\"] + X[\"1stFlrSF\"] + X[\"2ndFlrSF\"]   \n            X[\"TotalArea\"] = X[\"TotalBsmtSF\"] + X[\"1stFlrSF\"] + X[\"2ndFlrSF\"] + X[\"GarageArea\"]\n            #X[\"Age\"] = X[\"YrSold\"] - X[\"YearBuilt\"]\n        else:\n            X[\"TotalHouse\"] = X[\"TotalBsmtSF\"] + X[\"1stFlrSF\"] + X[\"2ndFlrSF\"]   \n            X[\"TotalArea\"] = X[\"TotalBsmtSF\"] + X[\"1stFlrSF\"] + X[\"2ndFlrSF\"] + X[\"GarageArea\"]\n            #X[\"Age\"] = X[\"YrSold\"] - X[\"YearBuilt\"]\n\n        return X","424081a6":"class skew_dummies(BaseEstimator, TransformerMixin):\n    def __init__(self,skew=0.75):\n        self.skew = skew\n    \n    def fit(self,X,y=None):\n        return self\n    \n    def transform(self,X):\n        X_numeric=X.select_dtypes(exclude=[\"object\"])\n        skewness = X_numeric.apply(lambda x: skew(x))\n        skewness_features = skewness[abs(skewness) >= self.skew].index\n        X[skewness_features] = np.log1p(X[skewness_features])\n        X = pd.get_dummies(X)\n        \n        return X","8736e9bb":"class labelenc(BaseEstimator, TransformerMixin):\n    def __init__(self):\n        pass\n    \n    def fit(self,X,y=None):\n        return self\n    \n    def transform(self,X):      \n        lab = LabelEncoder()\n        \n        cols = ('FireplaceQu', 'BsmtQual', 'BsmtCond', 'GarageQual', 'GarageCond', \n        'ExterQual', 'ExterCond','HeatingQC', 'PoolQC', 'KitchenQual', 'BsmtFinType1', \n        'BsmtFinType2', 'Functional', 'Fence', 'BsmtExposure', 'GarageFinish', 'LandSlope',\n        'LotShape', 'PavedDrive', 'Street', 'Alley', 'CentralAir', 'MSSubClass', 'OverallCond', \n        'YrSold', 'MoSold')\n\n        for c in cols:\n            X[c] = lab.fit_transform(X[c])\n        \n        return X","6ccc550f":"class grid():\n    def __init__(self,model):\n        self.model = model\n    \n    def grid_get(self,X,y,param_grid):\n        grid_search = GridSearchCV(self.model,param_grid,cv=5, scoring=\"neg_mean_squared_error\")\n        grid_search.fit(X,y)\n        print(grid_search.best_params_, np.sqrt(-grid_search.best_score_))\n        grid_search.cv_results_['mean_test_score'] = np.sqrt(-grid_search.cv_results_['mean_test_score'])\n        print(pd.DataFrame(grid_search.cv_results_)[['params','mean_test_score','std_test_score']])","3be62b60":"def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,\n                        n_jobs=1, train_sizes=np.linspace(.1, 1.0, 5)):\n    plt.figure()\n    plt.title(title)\n    if ylim is not None:\n        plt.ylim(*ylim)\n    plt.xlabel(\"Training examples\")\n    plt.ylabel(\"Error\")\n    train_sizes, train_scores, test_scores = learning_curve(\n        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n    train_scores_mean = 1-np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = 1-np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    plt.grid()\n\n    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.1,\n                     color=\"r\")\n    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n             label=\"Training score\")\n    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n             label=\"Cross-validation score\")\n\n    plt.legend(loc=\"best\")\n    return plt\n","47a23201":"class drop_cols(BaseEstimator, TransformerMixin):\n    def __init__(self, remove_cols = True):\n        self.remove_cols = remove_cols\n        pass\n    \n    def fit(self,X,y=None):\n        return self\n    \n    def transform(self,X):\n        if self.remove_cols:\n            del X['PoolQC']\n            #del X['YrSold']\n            #del X['YearBuilt']\n            del X['BsmtFinType1']\n            del X['LowQualFinSF']\n            del X['MoSold']\n            del X['Electrical']\n            del X['BldgType']\n            #del X['HouseStyle']\n            #del X['BsmtUnfSF']\n            #del X['BsmtFinType2']\n            #del X['BsmtFullBath']\n            #del X['BsmtExposure']\n            #del X['BsmtQual']\n            #del X['BsmtCond']\n            del X['SaleType']\n            del X['BsmtFinSF2']\n            #del X['HeatingQC']\n            #del X['Heating']\n            del X['Exterior2nd']\n            #del X['Foundation']\n            del X['ExterCond']\n            #del X['BedroomAbvGr']\n            del X['2ndFlrSF']\n            #del X['KitchenAbvGr']\n            #del X['RoofMatl']\n            del X['3SsnPorch']\n            del X['Exterior1st']\n            del X['MasVnrType']\n            #del X['ExterQual']\n            #del X['PavedDrive']\n            #del X['FireplaceQu']\n            #del X['Functional']\n            #del X['GarageType']\n            del X['GarageFinish']\n            #del X['MSSubClass']\n            del X['Alley']\n            #del X['LotShape']\n            del X['PoolArea']\n            #del X['LandContour']\n            #del X['Condition1']         \n            #del X['SaleCondition']\n            #del X['Condition2']\n            del X['RoofStyle']\n            del X['MiscFeature']\n            del X['Fence']\n            #del X['GarageCond']\n            #del X['GarageQual']\n            #del X['MSZoning']\n            #del X['Neighborhood']\n            del X['BsmtHalfBath']\n            del X['Street']\n            #del X['KitchenQual']\n            del X['LotConfig']\n            #del X['OverallQual']\n            #del X['FullBath']\n            #del X['TotRmsAbvGrd']\n            #del X['GarageArea']\n            #del X['LandSlope']\n            del X['TotalBsmtSF']\n            del X['GarageYrBlt']\n            #del X['LotFrontage']\n          \n        return X","11b7be4f":"class AveragingModels(BaseEstimator, RegressorMixin, TransformerMixin):\n    def __init__(self, models):\n        self.models = models\n        \n    # we define clones of the original models to fit the data in\n    def fit(self, X, y):\n        self.models_ = [clone(x) for x in self.models]\n        \n        # Train cloned base models\n        for model in self.models_:\n            model.fit(X, y)\n\n        return self\n    \n    #Now we do the predictions for cloned models and average them\n    def predict(self, X):\n        predictions = np.column_stack([\n            model.predict(X) for model in self.models_\n        ])\n        return np.mean(predictions, axis=1)   ","0f80deda":"# Prepare the data, pipeline for models evaluation\n\n# PIPELINE\npipe = Pipeline([\n    ('feat_eng', feat_eng()),\n    ('add_feature', add_feature(additional=2)),\n    ('lab_enc', labelenc()), \n    ('drop_cols', drop_cols()), \n    ('skew_dummies', skew_dummies(skew=1)), \n    \n    ])\n###                   ** skew_dummies is taking care of the labelencoder AND onehotencoder with get_dummies() !  **\n\ntrain = pd.read_csv('..\/input\/train.csv') \ntest = pd.read_csv('..\/input\/test.csv') \n\ntrainWprice = pd.DataFrame(train)\n#trainWprice = trainWprice.drop(trainWprice[(trainWprice['GrLivArea']>4000) & (trainWprice['SalePrice']<300000)].index)\ntrainNoPrice = trainWprice.drop(\"SalePrice\", axis=1)\n\nfull=pd.concat([trainNoPrice,test], ignore_index=True)\nfull.drop(['Id'],axis=1, inplace=True)\nprint(\"full without Id and no price \", full.shape)\n\nFullDataPipe = pipe.fit_transform(full)\nprint(\"FullDataPipe \", FullDataPipe.shape)\n\nn_train=train.shape[0]\ntrainFinal = pd.DataFrame(FullDataPipe[:n_train])\ntestFinal = pd.DataFrame(FullDataPipe[n_train:])\ny= train.SalePrice\nyFinal = np.log(train.SalePrice)\n\n# Scaler should be run separately on train and test to prevent overfitting\nscaler = RobustScaler()\ntrainFinal = scaler.fit_transform(trainFinal)\ntestFinal = scaler.fit_transform(testFinal)\n\n# PCA should be run separately on train and test \n#pca = PCA(n_components = 0.999) \n# Check the number of feats that will keep 99.9% variance. Note the number may be different for train vs test\n#pca = PCA(n_components = 200) \n#trainFinal = pca.fit_transform(trainFinal)\n#testFinal = pca.fit_transform(testFinal)\n\nprint(\"trainFinal\", trainFinal.shape)\nprint(\"testFinal\", testFinal.shape)\nprint(\"yFinal\", yFinal.shape)","9b547cbd":"# FEATURE IMPORTANCE - Needs its own SEPARATE pipeline without Scaler or PCA in order to see the features' NAMES and not their numbers\n# Useful even AFTER PCA - check the relevance of features for prediction\n\ntrainFinalFI = pd.DataFrame(trainFinal)\nyFinalFI = yFinal\n\nlasso=Lasso(alpha=0.001, copy_X=True, fit_intercept=True, max_iter=1000,\n   normalize=False, positive=False, precompute=False, random_state=None,\n   selection='cyclic', tol=0.0001, warm_start=False)\nlasso.fit(trainFinalFI,yFinalFI)\n\nFI_lasso = pd.DataFrame({\"Feature Importance\":lasso.coef_}, index=trainFinalFI.columns)\n\n# Focus on those with 0 importance\n#print(FI_lasso.sort_values(\"Feature Importance\",ascending=False).to_string())\n#print(\"_\"*80)\nFI_lasso[FI_lasso[\"Feature Importance\"]!=0].sort_values(\"Feature Importance\").plot(kind=\"barh\",figsize=(15,25))\nplt.xticks(rotation=90)\nplt.show()","b199781f":"# CHECK if any Missing Data\ntrainFinal = pd.DataFrame(trainFinal)\n\nColsMissingValues = trainFinal.isnull().sum()\nprint(\"There are \", len(ColsMissingValues[ColsMissingValues>0]), \" features with missing values\")\n#print(\"_\"*80)\nall_data_na = (trainFinal.isnull().sum() \/ len(trainFinal)) * 100\nall_data_na = all_data_na.sort_values(ascending=False)\nmissing_data = pd.DataFrame({'Missing Ratio' :all_data_na})\nprint(missing_data.head(len(ColsMissingValues[ColsMissingValues>0])))\nprint(\"_\"*80)\nprint(\"trainFinal \", trainFinal.shape)\nprint(\"yFinal \", yFinal.shape)","ef37b878":"# define cross validation \ndef rmse_cv(model,X,y):\n    rmse = np.sqrt(-cross_val_score(model, X, y, scoring=\"neg_mean_squared_error\", cv=5))\n    return rmse","9f89fce6":"# Lin reg ALL 14 models HYPERPARAMS NOT optimized\n#models = [LinearRegression(),Ridge(),Lasso(),RandomForestRegressor(),GradientBoostingRegressor(),SVR(),LinearSVR(),\n#          ElasticNet(),SGDRegressor(),BayesianRidge(),KernelRidge(),ExtraTreesRegressor(),XGBRegressor(),lgb.LGBMRegressor()]\n#names = [\"LR\", \"Ridge\", \"Lasso\", \"RF\", \"GBR\", \"SVR\", \"LinSVR\", \"Ela\",\"SGD\",\"Bay\",\"Ker\",\"Extra\",\"Xgb\", \"LightGBM\"]","f665a95d":"# Some initial models with Hyper params optimized\nmodels = [\n    Lasso(alpha =0.0005, random_state=1),\n    ElasticNet(alpha=0.0005, l1_ratio=.9, random_state=3),\n    KernelRidge(alpha=2.2, coef0=2.5, degree=3, gamma=None, kernel='polynomial',kernel_params=None),\n    SVR(C=2, cache_size=200, coef0=0.1, degree=3, epsilon=0.005, gamma=0.005,\n  kernel='rbf', max_iter=-1, shrinking=True, tol=0.01, verbose=False),\n    GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n             learning_rate=0.05, loss='huber', max_depth=5,\n             max_features='sqrt', max_leaf_nodes=None,\n             min_impurity_decrease=0.0, min_impurity_split=None,\n             min_samples_leaf=10, min_samples_split=10,\n             min_weight_fraction_leaf=0.0, n_estimators=2000,\n             n_iter_no_change=None, presort='auto', random_state=None,\n             subsample=1.0, tol=0.0001, validation_fraction=0.1, verbose=0,\n             warm_start=False),\n    #XGBRegressor(colsample_bytree=0.4603, gamma=0.0468, \n    #                         learning_rate=0.05, max_depth=3, \n    #                         min_child_weight=1.7817, n_estimators=2200,\n    #                         reg_alpha=0.4640, reg_lambda=0.8571,\n    #                         subsample=0.5213, silent=1,\n    #                         random_state =7, nthread = -1),\n    #lgb.LGBMRegressor(objective='regression',num_leaves=5,\n    #                          learning_rate=0.05, n_estimators=720,\n    #                          max_bin = 55, bagging_fraction = 0.8,\n    #                          bagging_freq = 5, feature_fraction = 0.2319,\n    #                          feature_fraction_seed=9, bagging_seed=9,\n    #                          min_data_in_leaf =6, min_sum_hessian_in_leaf = 11),\n    \n         ]\n\nnames = [\"LASSO\", \"ELA\",\"KER\", \"SVR \", \"GBR\" ]","4d0a19fb":"# Run the models and compare\nModScores = {}\n\nfor name, model in zip(names, models):\n    score = rmse_cv(model, trainFinal, yFinal)\n    ModScores[name] = score.mean()\n    print(\"{}: {:.6f}\".format(name,score.mean()))\n\nprint(\"trainFinal \", trainFinal.shape)\nprint(\"_\"*80)\nfor key, value in sorted(ModScores.items(), key = itemgetter(1), reverse = False):\n    print(key, value)","4c201401":"# SEARCH GRID FOR HYPERPARAMS OPTIMIZATION\n\n# SVR\nparam_grid = [{'kernel': [\"rbf\"], 'degree': [3]},\n              {'gamma': [0.005],'coef0': [0.1,0.05],'tol': [0.01],\n              'C': [2.5,2.2],'epsilon': [0.005],},]","c523bafb":"# Grid Search Optimization for models\nmodel4cv = SVR()\n\ngrid_search = GridSearchCV(model4cv, param_grid)\ngrid_search.fit(trainFinal, yFinal)\nprint(grid_search.best_params_)\nprint(grid_search.best_estimator_)\nprint(\"_\"*80)\nprint(grid(grid_search.best_estimator_).grid_get(trainFinal, yFinal,{}))","91d6a565":"# Optimized hyper params for models\nlasso = Lasso(alpha =0.0005, random_state=1)\n\nker = KernelRidge(alpha=2.2, coef0=2.5, degree=3, gamma=None, kernel='polynomial',\n      kernel_params=None)\n\nsvr = SVR(C=2, cache_size=200, coef0=0.1, degree=3, epsilon=0.005, gamma=0.005,\n  kernel='rbf', max_iter=-1, shrinking=True, tol=0.01, verbose=False)\n\ngbr = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05,\n                                   max_depth=4, max_features='sqrt',\n                                   min_samples_leaf=15, min_samples_split=10, \n                                   loss='huber', random_state =5)\n\nxgb = XGBRegressor(colsample_bytree=0.4603, gamma=0.0468, \n                             learning_rate=0.05, max_depth=3, \n                             min_child_weight=1.7817, n_estimators=2200,\n                             reg_alpha=0.4640, reg_lambda=0.8571,\n                             subsample=0.5213, silent=1,\n                             random_state =7, nthread = -1)\n\nlgb = lgb.LGBMRegressor(objective='regression',num_leaves=5,\n                              learning_rate=0.05, n_estimators=720,\n                              max_bin = 55, bagging_fraction = 0.8,\n                              bagging_freq = 5, feature_fraction = 0.2319,\n                              feature_fraction_seed=9, bagging_seed=9,\n                              min_data_in_leaf =6, min_sum_hessian_in_leaf = 11)\n\nela = ElasticNet(alpha=0.01, copy_X=True, fit_intercept=True, l1_ratio=0.05,\n      max_iter=5000, normalize=False, positive=False, precompute=False,\n      random_state=None, selection='cyclic', tol=0.0001, warm_start=False)\n","66283649":"# Final model fit, evaluation & prediction\n\nmodel = SVR(C=2, cache_size=200, coef0=0.1, degree=3, epsilon=0.005, gamma=0.005,\n  kernel='rbf', max_iter=-1, shrinking=True, tol=0.01, verbose=False)\n#model = AveragingModels(models = (svr, ker))\n#model = AveragingModels(models = (ela, svr, ker, gbr, lasso, xgb, lgb))\n\nmodel.fit(trainFinal, yFinal)\nscore = rmse_cv(model, trainFinal, yFinal)\nprint(\" model score: {:.5f} ({:.4f})\\n\".format(score.mean(), score.std()))\n\npred = np.exp(model.predict(testFinal))\npred = np.around(pred, decimals=4, out=None)\nprint(pred)","2b1dfd68":"# LEARNING CURVE\nX, y = trainFinal, yFinal\nestimator = model\n\ntitle = \"Learning Curves (SVR)\"\ncv = ShuffleSplit(n_splits=10, test_size=0.2, random_state=0)\nplot_learning_curve(estimator, title, X, y, ylim=(0.01, 0.17), cv=cv, n_jobs=4)","30398b9b":"# SUBMISSION\nresult=pd.DataFrame({'Id':test.Id, 'SalePrice':pred})\nresult.to_csv(\"submission.csv\",index=False)\nprint(result)","442d59c6":"**Load modules**\n\n","d375680a":"**Load raw data**","11fda6cb":"**MODELS**","f4a64fdf":"**HOUSE PRICES - Advanced Linear Regression**\n\n* In this notebook I've tried to keep ALL the imputing, transformations, scaling, etc - to be done via PIPELINE (using classes).\n* This allowed me to experiment quickly with different options, parameters, etc. \n* Found for example that manually eliminating features, using the feature importance tool with a class that drops columns - is better than PCA.\n* I cannot overestimate the importance of the Learning \/ Validation curves - it is the ONLY way to identify UNDER or OVERfitting.\n* With some tweaking you can achieve 12.24 on Kaggle with this notebook\n* But hey, play with it and let me know your thoughts and ideas on this notebook.\n______________________________________________________________________________________________________________________________________________________________________________\n* Many thanks to:\n* Serigne https:\/\/www.kaggle.com\/serigne\/stacked-regressions-top-4-on-leaderboard\n* MassQuantity https:\/\/www.kaggle.com\/massquantity\/all-you-need-is-pca-lb-0-11421-top-4\n* Their notebooks helped me a LOT ! Hopefully this notebook will help others as well.","2beceb4e":"**Visualization**","07a17e4d":"**Learning \/ Validation Curves - Jtrain vs Jcv to help identify under or overfitting**","7626399c":"**PIPELINE**","c88ecbc3":"**SUBMISSION**","4c7daba6":"**Final model fit, evaluation & prediction**","17c2d737":"**Classes to be used by the pipeline**  with feature engineering: Imputing missing data, Adding \/ Removing features, Skew, PCA, Scaling","10fde130":"**Cross Validation**","bc1c6f39":"**Check for missing data**","ff0b8da6":"**Hyper params optimization of the  models with Grid Search**"}}