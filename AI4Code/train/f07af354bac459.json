{"cell_type":{"56761acf":"code","3ca06481":"code","1b088f99":"code","232373de":"code","d8ccb75c":"code","740a5cf5":"code","0f5c150c":"code","ef13f140":"code","0622a139":"code","5b5fc286":"code","e34b1d47":"code","3300730f":"code","6894d17c":"code","f17ccc24":"code","54727b8b":"code","847a37eb":"code","8473bd1c":"code","11c2e876":"code","d0ef1951":"code","60cace9f":"code","d8f06536":"code","d48051cf":"code","82a0cbf5":"code","0a71b132":"code","29ce6925":"code","5a8f5c6a":"code","6b986975":"code","30f6b9e3":"code","42e6c1ae":"code","5920448f":"code","8f6af226":"code","736b77ba":"code","a9793b3f":"code","a6183720":"code","476cb1e5":"code","f1bab841":"code","c86ec49c":"code","3227d8a3":"code","b70b1a99":"code","22a32cb1":"code","ea22e174":"code","6ddd598a":"code","cc186a29":"code","74a38783":"code","130fe739":"code","1a655c4a":"code","96e12a66":"code","3e9094b5":"code","25e37b1c":"code","4dd5e138":"code","ab078889":"code","722ae44a":"code","f30af74f":"code","aa34318b":"code","9dde751b":"code","343c0cbe":"code","089f743d":"code","066959bc":"code","784e5dbf":"code","032da7c3":"code","49c6a820":"code","842425a6":"code","46d241db":"code","91b43de0":"code","ad2df42b":"code","a338d6bd":"code","f0d8bd7a":"code","8f30d08d":"code","7ae48a3d":"code","cbdb3d1e":"code","8526409d":"code","a078d410":"code","f4dd4ac2":"code","4f3797ff":"code","5dad4eb3":"code","d7ae0085":"code","412a6e2f":"code","03c1d38b":"markdown","050dbe3c":"markdown","d992a6bd":"markdown","9973f72e":"markdown","c21ead7c":"markdown","6cc42c3b":"markdown","4492b944":"markdown","27b25a6f":"markdown","56c074da":"markdown","e5245766":"markdown","5105f23a":"markdown","632d0a66":"markdown","22737578":"markdown","d800723b":"markdown","1fa113c3":"markdown","1cddd825":"markdown","59a43315":"markdown","c44114f5":"markdown","5568841d":"markdown","b0400122":"markdown","8eda78c6":"markdown","1e70117b":"markdown","48a3b166":"markdown","5dd31138":"markdown","7ecd1310":"markdown","59d8d968":"markdown","3b461821":"markdown","2633de68":"markdown","96dc3da5":"markdown","a9be8e20":"markdown","b1b62f28":"markdown","3eb51f9f":"markdown","03665057":"markdown","0a618cae":"markdown","fd309685":"markdown","622af3b0":"markdown","2960e4a9":"markdown","4261c514":"markdown","09b34a18":"markdown","b47f6af0":"markdown","3e7aaea1":"markdown","bb427ad3":"markdown","c7931cc9":"markdown","8e96208c":"markdown","13463af8":"markdown","cfe3a8ee":"markdown","652de219":"markdown","41b329ae":"markdown"},"source":{"56761acf":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os # To check filepath","3ca06481":"print(os.listdir(\"..\/input\")) #to check the name of the directory inside which we have our files","1b088f99":"#a particular folder\nfrom glob import glob# To find all the pathnames matching a specified pattern \nfiles = glob('..\/input\/breast-histopathology-images\/**\/*', recursive=True) \nfiles[0]","232373de":"extention=list() #will store end 3 letters of all the file names (extentions)\nfor image in files:\n    ext=image[-3:]\n    if ext not in extention:\n        extention.append(ext)\nalpha_ext=list()\nfor ex in extention: #any valid image will have extention in alphabets \n    if ex.isalpha() == True: #this line checks for such alphabet extentions\n        alpha_ext.append(ex)\nprint(alpha_ext)","d8ccb75c":"Data = glob('..\/input\/breast-histopathology-images\/**\/*.png', recursive=True)  #we extract only png files","740a5cf5":"print(Data[0]) ","0f5c150c":"len(Data)","ef13f140":"from os import listdir\nbase_path = \"..\/input\/breast-histopathology-images\/IDC_regular_ps50_idx5\/\"\nfolder = listdir(base_path)\nlen(folder)","0622a139":"from PIL import Image #adds support for opening, manipulating, and saving many different image file formats\nfrom tqdm import tqdm #adds progress bar for the loops\ndimentions=list()\nx=1\nfor images in (Data):\n    dim = Image.open(images)\n    size= dim.size\n    if size not in dimentions:\n        dimentions.append(size)\n        x+=1\n    if(x>8): #going through all the images will take up lot of memory, so therefore we will check until we get three different dimentions.\n        break\nprint(dimentions)","5b5fc286":"import cv2 #used for computer vision tasks such as reading image from file, changing color channels etc\nimport matplotlib.pyplot as plt #for plotting various graph, images etc.\ndef view_images(image): #function to view an image\n    image_cv = cv2.imread(image) #reads an image\n    plt.imshow(cv2.cvtColor(image_cv, cv2.COLOR_BGR2RGB)); #displays an image\nview_images(Data[12596])","e34b1d47":"def plot_images(photos) : #to plot multiple image\n    x=0\n    for image in photos:\n        image_cv = cv2.imread(image)\n        plt.subplot(5, 5, x+1)\n        plt.imshow(cv2.cvtColor(image_cv, cv2.COLOR_BGR2RGB));\n        plt.axis('off');\n        x+=1\nplot_images(Data[:25])","3300730f":"total_images= 277524\ndata = pd.DataFrame(index=np.arange(0, total_images), columns=[\"patient_id\", \"path\", \"target\"])\n\nk = 0\nfor n in range(len(folder)):\n    patient_id = folder[n]\n    patient_path = base_path + patient_id \n    for c in [0,1]:\n        class_path = patient_path + \"\/\" + str(c) + \"\/\"\n        subfiles = listdir(class_path)\n        for m in range(len(subfiles)):\n            image_path = subfiles[m]\n            data.iloc[k][\"path\"] = class_path + image_path\n            data.iloc[k][\"target\"] = c\n            data.iloc[k][\"patient_id\"] = patient_id\n            k += 1  ","6894d17c":"data.head()","f17ccc24":"data.shape","54727b8b":"data.info()","847a37eb":"from tqdm import tqdm\nimport csv #to open and write csv files\nData_output=list()\nData_output.append([\"target\"])\nfor file_name in tqdm(Data):\n    Data_output.append([file_name[-10:-4]])\nwith open(\"output.csv\", \"w\") as f:\n    writer = csv.writer(f)\n    for val in Data_output:\n        writer.writerows([val])","8473bd1c":"import pandas as pd # Allows the use of display() for DataFrames\ndata_output = pd.read_csv(\"output.csv\")\n#print(data_output.head(),\"\\n\",\"\\n\",\"\\n\",data_output.tail())\n","11c2e876":"def class_output(images,x,i):  #to display image along with their labels\n    fig = plt.figure()\n    ax = plt.subplot(2, 2,i)\n    ax.set_title(data_output.loc[x].item())\n    view_images(images)\n    i+=1\n    return\nk=0 #we have to show only one image of class0 therefore this variable is to check that\nl=0 #we have to show only one image of class1 therefore this variable to check that\ni=0 #for subplot position\nfor x in range(1,len(Data)):\n    if(data_output.loc[x].item()==\"class0\" and k!=1):\n        k+=1\n        i+=1\n        class_output(Data[x],x,i)\n    elif(data_output.loc[x].item()==\"class1\" and l!=1):\n        l+=1\n        i+=1\n        class_output(Data[x],x,i)\n    elif(k==0 or l==0):\n        continue\n    else:\n        break","d0ef1951":"def get_cancer_dataframe(patient_id, cancer_id):\n    path = base_path + patient_id + \"\/\" + cancer_id\n    files = listdir(path)\n    dataframe = pd.DataFrame(files, columns=[\"filename\"])\n    path_names = path + \"\/\" + dataframe.filename.values\n    dataframe = dataframe.filename.str.rsplit(\"_\", n=4, expand=True)\n    dataframe.loc[:, \"target\"] = np.int(cancer_id)\n    dataframe.loc[:, \"path\"] = path_names\n    dataframe = dataframe.drop([0, 1, 4], axis=1)\n    dataframe = dataframe.rename({2: \"x\", 3: \"y\"}, axis=1)\n    dataframe.loc[:, \"x\"] = dataframe.loc[:,\"x\"].str.replace(\"x\", \"\", case=False).astype(np.int)\n    dataframe.loc[:, \"y\"] = dataframe.loc[:,\"y\"].str.replace(\"y\", \"\", case=False).astype(np.int)\n    return dataframe\n\ndef get_patient_dataframe(patient_id):\n    df_0 = get_cancer_dataframe(patient_id, \"0\")\n    df_1 = get_cancer_dataframe(patient_id, \"1\")\n    patient_df = df_0.append(df_1)\n    return patient_df","60cace9f":"example = get_patient_dataframe(data.patient_id.values[0])\nexample.head()","d8f06536":"fig, ax = plt.subplots(6,3,figsize=(20, 25))\n\npatient_ids = data.patient_id.unique()\n\nfor n in range(6):\n    for m in range(3):\n        patient_id = patient_ids[m + 3*n]\n        example = get_patient_dataframe(patient_id)\n        ax[n,m].scatter(example.x.values, example.y.values, c=example.target.values, cmap=\"coolwarm\", s=20);\n        ax[n,m].set_title(\"patient \" + patient_id)","d48051cf":"class1 = data_output[(data_output[\"target\"]==\"class1\" )].shape[0]\nclass0 = data_output[(data_output[\"target\"]==\"class0\" )].shape[0]\nobjects=[\"class1\",\"class0\"]\ny_pos = np.arange(len(objects))\ncount=[class1,class0]\nplt.bar(y_pos, count, align='center', alpha=0.5)\nplt.xticks(y_pos, objects)\nplt.ylabel('Number of images')\nplt.title('Target distribution')\n \nplt.show()","82a0cbf5":"percent_class1=class1\/len(Data)\npercent_class0=class0\/len(Data)\nprint(\"Total Class1 images :\",class1)\nprint(\"Total Class0 images :\",class0)\nprint(\"Percent of class 0 images : \", percent_class0*100)\nprint(\"Percent of class 1 images : \", percent_class1*100)","0a71b132":"from sklearn.utils import shuffle #to shuffle the data\nData,data_output= shuffle(Data,data_output)","29ce6925":"from tqdm import tqdm\ndata=list()\nfor img in tqdm(Data):\n    image_ar = cv2.imread(img)\n    data.append(cv2.resize(image_ar,(50,50),interpolation=cv2.INTER_CUBIC))","5a8f5c6a":"data_output=data_output.replace(to_replace=\"class0\",value=0)\ndata_output=data_output.replace(to_replace=\"class1\",value=1)","6b986975":"from keras.utils import to_categorical #to hot encode the output labels\ndata_output_encoded =to_categorical(data_output, num_classes=2)\nprint(data_output_encoded.shape)","30f6b9e3":"from sklearn.model_selection import train_test_split\ndata=np.array(data)\nX_train, X_test, Y_train, Y_test = train_test_split(data, data_output_encoded, test_size=0.3)\nprint(\"Number of train files\",len(X_train))\nprint(\"Number of test files\",len(X_test))\nprint(\"Number of train_target files\",len(Y_train))\nprint(\"Number of  test_target  files\",len(Y_test))","42e6c1ae":"X_train=X_train[0:70000]\nY_train=Y_train[0:70000]\nX_test=X_test[0:30000]\nY_test=Y_test[0:30000]","5920448f":"from keras.utils import to_categorical #to hot encode the data\nfrom imblearn.under_sampling import RandomUnderSampler #For performing undersampling\n\nX_train_shape = X_train.shape[1]*X_train.shape[2]*X_train.shape[3]\nX_test_shape = X_test.shape[1]*X_test.shape[2]*X_test.shape[3]\nX_train_Flat = X_train.reshape(X_train.shape[0], X_train_shape)\nX_test_Flat = X_test.reshape(X_test.shape[0], X_test_shape)\n\nrandom_US = RandomUnderSampler(ratio='auto') #Constructor of the class to perform undersampling\nX_train_RUS, Y_train_RUS = random_US.fit_sample(X_train_Flat, Y_train) #resamples the dataset\nX_test_RUS, Y_test_RUS = random_US.fit_sample(X_test_Flat, Y_test) #resamples the dataset\ndel(X_train_Flat,X_test_Flat)\n\nclass1=1\nclass0=0\n\nfor i in range(0,len(Y_train_RUS)): \n    if(Y_train_RUS[i]==1):\n        class1+=1\nfor i in range(0,len(Y_train_RUS)): \n    if(Y_train_RUS[i]==0):\n        class0+=1\n#For Plotting the distribution of classes\nclasses=[\"class1\",\"class0\"]\ny_pos = np.arange(len(classes))\ncount=[class1,class0]\nplt.bar(y_pos, count, align='center', alpha=0.5)\nplt.xticks(y_pos, objects)\nplt.ylabel('Number of images')\nplt.title('Class distribution')\n \nplt.show()\n\n\n#hot encoding them\nY_train_encoded = to_categorical(Y_train_RUS, num_classes = 2)\nY_test_encoded = to_categorical(Y_test_RUS, num_classes = 2)\n\ndel(Y_train_RUS,Y_test_RUS)\n\nfor i in range(len(X_train_RUS)):\n    X_train_RUS_Reshaped = X_train_RUS.reshape(len(X_train_RUS),50,50,3)\ndel(X_train_RUS)\n\nfor i in range(len(X_test_RUS)):\n    X_test_RUS_Reshaped = X_test_RUS.reshape(len(X_test_RUS),50,50,3)\ndel(X_test_RUS)\n","8f6af226":"X_test, X_valid, Y_test, Y_valid = train_test_split(X_test_RUS_Reshaped, Y_test_encoded, test_size=0.2,shuffle=True)","736b77ba":"print(\"Number of train files\",len(X_train_RUS_Reshaped))\nprint(\"Number of valid files\",len(X_valid))\nprint(\"Number of train_target files\",len(Y_train_encoded))\nprint(\"Number of  valid_target  files\",len(Y_valid))\nprint(\"Number of test files\",len(X_test))\nprint(\"Number of  test_target  files\",len(Y_test))","a9793b3f":"from sklearn.utils import shuffle\nX_train,Y_train= shuffle(X_train_RUS_Reshaped,Y_train_encoded)","a6183720":"print(Y_train_encoded.shape)\nprint(Y_test.shape)\nprint(Y_valid.shape)","476cb1e5":"print(\"Training Data Shape:\", X_train.shape)\nprint(\"Validation Data Shape:\", X_valid.shape)\nprint(\"Testing Data Shape:\", X_test.shape)\nprint(\"Training Label Data Shape:\", Y_train.shape)\nprint(\"Validation Label Data Shape:\", Y_valid.shape)\nprint(\"Testing Label Data Shape:\", Y_test.shape)","f1bab841":"import itertools #create iterators for effective looping\n#Plotting the confusion matrix for checking the accuracy of the model\ndef plot_confusion_matrix(cm, classes,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    print(cm)\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f'\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    plt.tight_layout()","c86ec49c":"from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D #Import layers for the model\nfrom keras.layers import Dropout, Flatten, Dense \nfrom keras.models import Sequential #Our model will be Sequential\n\nmodel = Sequential()\nmodel.add(Conv2D(filters=32,kernel_size=(3,3),strides=2,padding='same',activation='relu',input_shape=(50,50,3)))\nmodel.add(Flatten()) #Flattens the matrix into a vector\nmodel.add(Dense(2, activation='softmax')) \nmodel.summary()","3227d8a3":"model.compile(optimizer= 'adam', loss='categorical_crossentropy', metrics=['accuracy']) #Compiling the model","b70b1a99":"from keras.callbacks import ModelCheckpoint  #Checkpoint to save the best weights of the model.\ncheckpointer = ModelCheckpoint(filepath='weights.best.cnn.hdf5', \n                               verbose=1, save_best_only=True) \nmodel.fit(X_train, Y_train, \n          validation_data=(X_valid, Y_valid),\n          epochs=6, batch_size=256, callbacks=[checkpointer], verbose=2,shuffle=True)","22a32cb1":"model.load_weights('weights.best.cnn.hdf5') #Load the saved weights from file.","ea22e174":"predictions = [np.argmax(model.predict(np.expand_dims(feature, axis=0))) for feature in tqdm(X_test)]","6ddd598a":"from sklearn.metrics import confusion_matrix #to plot confusion matrix\nclass_names=['IDC(-)','IDC(+)']\ncnf_matrix_bench=confusion_matrix(np.argmax(Y_test, axis=1), np.array(predictions))\nplot_confusion_matrix(cnf_matrix_bench, classes=class_names,\n                      title='Confusion matrix')","cc186a29":"from keras.preprocessing.image import ImageDataGenerator  #For Image argumentaton\ndatagen = ImageDataGenerator(\n        shear_range=0.2,\n        rotation_range=40,\n        width_shift_range=0.2,\n        height_shift_range=0.2,\n        zoom_range=0.2,\n        rescale=1\/255.0,\n        horizontal_flip=True,\n        vertical_flip=True)","74a38783":"X_valid_e=X_valid\/255.0 #rescaling X_valid\nX_test_e=X_test\/255.0 #rescaling X_Test","130fe739":"from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D\nfrom keras.layers import Dropout, Flatten, Dense\nfrom keras.models import Sequential\n\nargum_model = Sequential()\nargum_model.add(Conv2D(filters=32,kernel_size=(3,3),strides=2,padding='same',activation='relu',input_shape=X_train.shape[1:]))\nargum_model.add(Dropout(0.15))\nargum_model.add(MaxPooling2D(pool_size=2,strides=2))\nargum_model.add(Conv2D(filters=64,kernel_size=(3,3),strides=2,padding='same',activation='relu'))\nargum_model.add(Dropout(0.25))\nargum_model.add(Conv2D(filters=128,kernel_size=(3,3),strides=2,padding='same',activation='relu'))\nargum_model.add(Dropout(0.35))\nargum_model.add(Conv2D(filters=512,kernel_size=(3,3),strides=2,padding='same',activation='relu'))\nargum_model.add(Dropout(0.45))\nargum_model.add(Flatten())\nargum_model.add(Dense(2, activation='softmax'))\nargum_model.summary()","1a655c4a":"argum_model.compile(loss='categorical_crossentropy', optimizer='AdaDelta', metrics=['accuracy'])","96e12a66":"from keras.callbacks import ModelCheckpoint\ncheckpointer = ModelCheckpoint(filepath='weights.bestarg.hdf5', verbose=1, save_best_only=True)","3e9094b5":"batch_size=32\nepochs=13\nargum_model.fit_generator(datagen.flow(X_train, Y_train, batch_size), \n          validation_data=(X_valid_e, Y_valid), steps_per_epoch=500,\n          epochs=epochs,callbacks=[checkpointer], verbose=0)","25e37b1c":"argum_model.load_weights('weights.bestarg.hdf5')","4dd5e138":"predictions_arg = [np.argmax(argum_model.predict(np.expand_dims(feature, axis=0))) for feature in tqdm(X_test_e)]","ab078889":"from sklearn.metrics import confusion_matrix\nclass_names=['IDC(-)','IDC(+)']\ncnf_matrix_Arg=confusion_matrix(np.argmax(Y_test, axis=1), np.array(predictions_arg))\nplot_confusion_matrix(cnf_matrix_Arg, classes=class_names,\n                      title='Confusion matrix')","722ae44a":"from keras.applications.vgg16 import VGG16 #downloading model for transfer learning\narg_model = VGG16(include_top=False,weights='imagenet', input_tensor=None, input_shape=None, pooling=None,)","f30af74f":"from keras.applications.vgg19 import preprocess_input #preprocessing the input so that it could work with the downloaded model\nbottleneck_train=arg_model.predict(preprocess_input(X_train),batch_size=50,verbose=1) #calculating bottleneck features, this inshure that we hold the weights of bottom layers","aa34318b":"from keras.applications.vgg19 import preprocess_input\nbottleneck_valid=arg_model.predict(preprocess_input(X_valid),batch_size=50,verbose=1)","9dde751b":"from keras.applications.vgg19 import preprocess_input\nbottleneck_test=arg_model.predict(preprocess_input(X_test),batch_size=50,verbose=1)","343c0cbe":"\nfrom keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D\nfrom keras.layers import Dropout, Flatten, Dense\nfrom keras.models import Sequential\n\nmodel_transfer = Sequential()\nmodel_transfer.add(GlobalAveragePooling2D(input_shape=bottleneck_train.shape[1:]))\nmodel_transfer.add(Dense(32,activation='relu'))\nmodel_transfer.add(Dropout(0.15))\nmodel_transfer.add(Dense(64,activation='relu'))\nmodel_transfer.add(Dropout(0.20))\nmodel_transfer.add(Dense(128,activation='relu'))\nmodel_transfer.add(Dropout(0.25))\nmodel_transfer.add(Dense(256,activation='relu'))\nmodel_transfer.add(Dropout(0.35))\nmodel_transfer.add(Dense(512,activation='relu'))\nmodel_transfer.add(Dropout(0.45))\n\nmodel_transfer.add(Dense(2, activation='softmax'))\n\nmodel_transfer.summary()","089f743d":"model_transfer.compile(loss='categorical_crossentropy', optimizer='AdaDelta', metrics=['accuracy'])","066959bc":"from keras.callbacks import ModelCheckpoint\ncheckpointer = ModelCheckpoint(filepath='weights.bestarg.tranfer.hdf5', verbose=1, save_best_only=True)","784e5dbf":"batch_size=64\nepochs=20\nmodel_transfer.fit(bottleneck_train, Y_train, batch_size,\n          validation_data=(bottleneck_valid, Y_valid),\n          epochs=epochs,callbacks=[checkpointer], verbose=1)","032da7c3":"model_transfer.load_weights('weights.bestarg.tranfer.hdf5')","49c6a820":"predictions_transfer = [np.argmax(model_transfer.predict(np.expand_dims(feature, axis=0))) for feature in bottleneck_test]","842425a6":"from sklearn.metrics import confusion_matrix\nclass_names=['IDC(-)','IDC(+)']\ncnf_matrix_transfer_vgg16=confusion_matrix(np.argmax(Y_test, axis=1), np.array(predictions_transfer))\nplot_confusion_matrix(cnf_matrix_transfer_vgg16, classes=class_names,\n                      title='Confusion matrix')","46d241db":"from keras.applications.vgg19 import VGG19 #downloading model for transfer learning\narg_model = VGG19(include_top=False,weights='imagenet', input_tensor=None, input_shape=None, pooling=None,)","91b43de0":"from keras.applications.vgg19 import preprocess_input #preprocessing the input so that it could work with the downloaded model\nbottleneck_train=arg_model.predict(preprocess_input(X_train),batch_size=50,verbose=1) #calculating bottleneck features, this inshure that we hold the weights of bottom layers","ad2df42b":"from keras.applications.vgg19 import preprocess_input\nbottleneck_valid=arg_model.predict(preprocess_input(X_valid),batch_size=50,verbose=1)","a338d6bd":"from keras.applications.vgg19 import preprocess_input\nbottleneck_test=arg_model.predict(preprocess_input(X_test),batch_size=50,verbose=1)","f0d8bd7a":"\nfrom keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D\nfrom keras.layers import Dropout, Flatten, Dense\nfrom keras.models import Sequential\n\nmodel_transfer_vgg19 = Sequential()\nmodel_transfer_vgg19.add(GlobalAveragePooling2D(input_shape=bottleneck_train.shape[1:]))\nmodel_transfer_vgg19.add(Dense(32,activation='relu'))\nmodel_transfer_vgg19.add(Dropout(0.15))\nmodel_transfer_vgg19.add(Dense(64,activation='relu'))\nmodel_transfer_vgg19.add(Dropout(0.20))\nmodel_transfer_vgg19.add(Dense(128,activation='relu'))\nmodel_transfer_vgg19.add(Dropout(0.25))\nmodel_transfer_vgg19.add(Dense(256,activation='relu'))\nmodel_transfer_vgg19.add(Dropout(0.35))\nmodel_transfer_vgg19.add(Dense(512,activation='relu'))\nmodel_transfer_vgg19.add(Dropout(0.45))\n\nmodel_transfer_vgg19.add(Dense(2, activation='softmax'))\n\nmodel_transfer_vgg19.summary()","8f30d08d":"model_transfer_vgg19.compile(loss='categorical_crossentropy', optimizer='AdaDelta', metrics=['accuracy'])","7ae48a3d":"from keras.callbacks import ModelCheckpoint\ncheckpointer = ModelCheckpoint(filepath='weights.bestarg.tranfer.hdf5', verbose=1, save_best_only=True)","cbdb3d1e":"batch_size=64\nepochs=20\nmodel_transfer_vgg19.fit(bottleneck_train, Y_train, batch_size,\n          validation_data=(bottleneck_valid, Y_valid),\n          epochs=epochs,callbacks=[checkpointer], verbose=1)","8526409d":"model_transfer_vgg19.load_weights('weights.bestarg.tranfer.hdf5')","a078d410":"predictions_transfer = [np.argmax(model_transfer.predict(np.expand_dims(feature, axis=0))) for feature in bottleneck_test]","f4dd4ac2":"from sklearn.metrics import confusion_matrix\nclass_names=['IDC(-)','IDC(+)']\ncnf_matrix_transfer_vgg19=confusion_matrix(np.argmax(Y_test, axis=1), np.array(predictions_transfer))\nplot_confusion_matrix(cnf_matrix_transfer_vgg19, classes=class_names,\n                      title='Confusion matrix')","4f3797ff":"#Bar chart to compare different models\ntp=0\nfor i in range(0,len(Y_test)): #Number of positive cases\n    if(np.argmax(Y_test[i])==1):\n        tp+=1\n#Senstivity of models\nconfusion_bench_s=cnf_matrix_bench[1][1]\/tp *100 \nconfusion_Arg_s=cnf_matrix_Arg[1][1]\/tp *100\nconfusion_transfer_s_vgg16=cnf_matrix_transfer_vgg16[1][1]\/tp *100\nconfusion_transfer_s_vgg19=cnf_matrix_transfer_vgg19[1][1]\/tp *100\n\nclasses=[\"benchmark\",\"data argum\",\"TL Vgg16\",\"TL Vgg19\"]\nobjects=[\"benchmark\",\"data argum\",\"TL Vgg16\",\"TL Vgg19\"]\ny_pos = np.arange(len(classes))\ncount=[confusion_bench_s,confusion_Arg_s,confusion_transfer_s_vgg16,confusion_transfer_s_vgg19]\nplt.bar(y_pos, count, align='center', alpha=0.5)\nplt.xticks(y_pos, objects)\nplt.ylabel('Percentage')\nplt.title('Sensitivity')\n\nplt.show()","5dad4eb3":"tp=0\ntn=0\nfor i in range(0,len(Y_test)):  #Number of postive cases\n    if(np.argmax(Y_test[i])==1): \n        tp+=1\nfor i in range(0,len(Y_test)): #number of negative cases\n    if(np.argmax(Y_test[i])==0):\n        tn+=1\nconfusion_bench=cnf_matrix_bench[0][0]\/tn *100\nconfusion_Arg=cnf_matrix_Arg[0][0]\/tn *100\nconfusion_transfer_vgg16=cnf_matrix_transfer_vgg16[0][0]\/tn *100\nconfusion_transfer_vgg19=cnf_matrix_transfer_vgg19[0][0]\/tn *100\nclasses=[\"benchmark\",\"data argum\",\"TL Vgg16\",\"TL Vgg19\"]\nobjects=[\"benchmark\",\"data argum\",\"TL Vgg16\",\"TL Vgg19\"]\ny_pos = np.arange(len(classes))\ncount=[confusion_bench_s,confusion_Arg_s,confusion_transfer_s_vgg16,confusion_transfer_s_vgg19]\nplt.bar(y_pos, count, align='center', alpha=0.5)\nplt.xticks(y_pos, objects)\nplt.ylabel('Percentage')\nplt.title('Specificity')\n\nplt.show()","d7ae0085":"col=['Models','Senstivity','Specificity']\nresults=pd.DataFrame(columns=col) #dataframe to store the results\nresults.loc[0]=['Bench',confusion_bench_s,confusion_bench]\nresults.loc[1]=['Image Arg model',confusion_Arg_s,confusion_Arg]\nresults.loc[2]=['TL Vgg16',confusion_transfer_s_vgg16,confusion_transfer_vgg16]\nresults.loc[3]=['TL Vgg16',confusion_transfer_s_vgg19,confusion_transfer_vgg19]","412a6e2f":"display(results)","03c1d38b":"We will now add image argumentation to our data, so that it may be set for wider range of domain","050dbe3c":"**Local Directory**","d992a6bd":"print first filepath","9973f72e":"We can see that we have an **unbalanced class** and which is a common problem when we have medical data.","c21ead7c":"**How many patients do we have?**","6cc42c3b":"### ***Data Extraction and Visualization***","4492b944":"We also need a validation set inorder to check overfitting. We can do two things either split test set further into valid set or split train se into valid set.","27b25a6f":"### Import Libraries","56c074da":"**Predicting IDC in Breast Cancer Histology Images**\n\nBreast cancer is the most common form of cancer in women, and **invasive ductal carcinoma (IDC)** is the most common form of breast cancer. Accurately identifying and categorizing breast cancer subtypes is an important clinical task, and automated methods can be used to save time and reduce error.\n![](https:\/\/blogs.nvidia.com\/wp-content\/uploads\/2018\/01\/AI_Mammographie.jpg)","e5245766":"### Binary cancer visualisation\nBefore we will take a look at the whole tissue let's keep it a bit simpler by looking at the target structure in the x-y-space for a handful of patients:","5105f23a":"### Vgg19","632d0a66":"We have a large dataset almost (280k image) and we will work with neural networks, for better debugging we will use only a part of data.","22737578":"### Visualising the breast tissue ","d800723b":"Result : We can see that tranfer learning and image argumentation both are doing a great job, they both have senstivity and specificity o about 0.8.","1fa113c3":" We can see that the dimentions of images are **not equal**, So we would make it all equal  to work bettter with our network.","1cddd825":"We have to extract all coordinates of image patches that are stored in the image names","59a43315":"### Data Processing  ","c44114f5":"Depending on the result the Image Arg mode is the best.","5568841d":"From the above images we can conclude that **brighter** region is **more** than the **darken** region in **Class0** and the opposite is true.","b0400122":"Now we have our three sets of train, valid and test. We will now create our benchmark model.","8eda78c6":"Now we will split our data into training set and testing set.","1e70117b":"#### To improve model we will add arguments to images..","48a3b166":"Compare through by showing only one image frome class0 and class1(IDC- and IDC+)","5dd31138":"That's mean I dont have any null value it is goog","7ecd1310":"**How many patches do we have in total?**","59d8d968":"### Vgg16 Confuse matrix","3b461821":"In the next we will OneHot encode our data to better work with neural networks.","2633de68":"### Resize images to do better in CNN","96dc3da5":"Almost **280** patients we have.","a9be8e20":"We would encode our output data which is present as Class1 and Class0 to 1 and 0(Class1=>1,Class0=>0).","b1b62f28":" ***Now we will plot the confusion matrix :***","3eb51f9f":"### General Information","03665057":"#### Simple CNN","0a618cae":"Load png image","fd309685":"#### Vgg16","622af3b0":"We will also rescale our image pixels, from range of 0-255.0 to 0-1.","2960e4a9":" We have total of **277524** image files","4261c514":"### Transfer Learning\n\nWe will now add transfer learning from various models","09b34a18":"we need to extract the columns names in which each files belong from its file names. ","b47f6af0":"We need to now preprocess our image file. We change pixels range from 0-255 to 0-1.","3e7aaea1":"#### confusion matrix ","bb427ad3":"We will first shuffle are images to remove any patterns if present and then load them.","c7931cc9":"we will check whether the dimentions of all the images are same or not","8e96208c":"Now we can  read the data from output.csv and  write it.","13463af8":"Comparision is made between three algorithms with repect to false positives and false negatives. As considering dealing with cancer , the algorithm must give less false negative, at its a matter of death and life.","cfe3a8ee":"**Now we will plot the confusion matrix **","652de219":"### Genaral Overviwe","41b329ae":"#### Check the image extentions"}}