{"cell_type":{"06824cfa":"code","a4ad4e3d":"code","83e47b93":"code","ba52f303":"code","fab67694":"code","dbebc854":"code","c7305eaf":"markdown","a01836eb":"markdown","eb3d333d":"markdown","ed013d72":"markdown","6be4b1a2":"markdown","89d97f15":"markdown","5c2f480d":"markdown","b830e539":"markdown","a2aa1805":"markdown"},"source":{"06824cfa":"import numpy as np\nimport matplotlib.pyplot as plt","a4ad4e3d":"def cross_entropy(yreal, ypred):\n    with np.errstate(divide='ignore', invalid='ignore'):\n        return -1*(yreal*np.log(ypred) + (1-yreal)*np.log(1-ypred))","83e47b93":"yreal = np.ones(50)\nypred = np.linspace(0., 1., 50)\n\nplt.figure(figsize=(15,5))\nplt.plot(ypred, cross_entropy(yreal, ypred), \"o--\")\nplt.xlabel(\"prediction\", fontsize=23)\nplt.ylabel(\"loss\", fontsize=23)\nplt.grid()\nplt.show()","ba52f303":"yreal = np.zeros(50)\nypred = np.linspace(0., 1., 50)\n\nplt.figure(figsize=(15,5))\nplt.plot(ypred, cross_entropy(yreal, ypred), \"o--\")\nplt.xlabel(\"prediction\", fontsize=23)\nplt.ylabel(\"loss\", fontsize=23)\nplt.grid()\nplt.show()","fab67694":"for x in np.arange(0, 1.01, 0.1):\n    yreal = x*np.ones(50)\n    ypred = np.linspace(0., 1., 50)\n\n    plt.figure(figsize=(15,5))\n    plt.plot(ypred, cross_entropy(yreal, ypred), \"o--\")\n    plt.title(f\"true target: {x}\", fontsize=19)\n    plt.xlabel(\"prediction\", fontsize=23)\n    plt.ylabel(\"loss\", fontsize=23)\n    plt.grid()\n    plt.show()","dbebc854":"true_target = np.linspace(0.001, 0.999, 50)\nmin_loss = [cross_entropy(x, x) for x in true_target]\n\nplt.figure(figsize=(15,5))\nplt.plot(true_target, min_loss, \"o--\")\nplt.xlabel(\"True target\", fontsize=23)\nplt.ylabel(\"Minimal loss\", fontsize=23)\nplt.grid()\nplt.show()","c7305eaf":"I first define a function to compute $l(y_i, p(x_i))$ in a pairwise way:","a01836eb":"and the values of $l(y_i, p(x_i))$ when all the samples are of class 0","eb3d333d":"For these two reasons, I think that binary cross-entropy is **not an appropriate loss function for this problem**. \n\nWhat do you think?\n***","ed013d72":"I have seen that most of the public kernels in this competition are using the **binary cross-entropy** as the loss function. Thus, I think it is very important that we fully understand how this loss works.\n\nThis is a loss function designed for **binary classification** problems, where the classifier can return the predicted probabilities of the positive class. The binary cross-entropy is defined as:\n$$\n\\sum_{i=1}^{N} l(y_i, p(x_i)) = \\sum_{i=1}^{N} - \\left( y_i \\cdot \\log(p(x_i)) + (1-y_i) \\cdot \\log(1-p(x_i))  \\right)\n$$\nwhere $y_i$ is the target variable (can only be $0$ or $1$), $x_i$ are the variables\/features, and **$p$ the probability model of being of class $1$**. \n\nYou can see that with a sample of the class 1:\n$$\nl(y_i=1, p(x_i)) = - \\log(p(x_i))\n$$\nand thus the loss is $0$ when $p(x_i) = 1$ (the sample is of class 1, and the model outputs a probability 1 of being of class 1), and the loss is $\\infty$ when $p(x_i) = 0$ (the sample is of class 1, and the model outputs a probability 0 of being of class 1). \n\nOn the other hand, when the sample is of the class 0:\n$$\nl(y_i=0, p(x_i)) = - \\log(1-p(x_i))\n$$\nand thus the loss is $0$ when $p(x_i) = 0$ (the sample is of class 0, and the model outputs a probability 0 of being of class 1), and the loss is $\\infty$ when $p(x_i) = 1$ (the sample is of class 0, and the model outputs a probability 1 of being of class 1).","6be4b1a2":"***\n### Let's see it graphically","89d97f15":"# Understanding the binary cross entropy","5c2f480d":"***\n### What does it happen when we use binary cross entropy and $y$ is not binary? \n\nIn this competition we are using binary cross-entropy, however, the target variable can take any value between 0 and 1. Let's see what happen with this loss function as we move the true target value between 0 and 1: ","b830e539":"It seems ok, right? The loss function takes its minimum when the prediction is equal to the true target. But there are two \"little\" problems:\n1. **The loss is not symmetric!**. For example, when the true target is 0.4, a sample with prediction 0 will get a higher penalization than a sample with prediction 0.8, but both are equidistant to the true target.\n2. And even more important, **the minimum value of the loss is not 0 when the true target is not 0 or 1** (see carefully the y-axis). \n\nFurthermore, the minimum value of the loss **depends on the true target value!**. If you do the math, you will see that $l(y_i, p(x_i))$ reaches its minimum when $p(x_i) = y_i$. \n\nThe figure in the cell below shows the minimal loss as a function of the true target value:","a2aa1805":"Below you can see the values of $l(y_i, p(x_i))$ when all the samples are of class 1"}}