{"cell_type":{"f22a03d8":"code","d7fa8182":"code","c3358c9d":"code","ffa96877":"code","86b77230":"code","e05d96af":"code","6f99db0a":"code","358d09a5":"code","131ec0f3":"code","bd289ba9":"code","ac1030c1":"code","c62db302":"code","c93559b9":"code","c4ce33ad":"code","3876e350":"code","01d059f6":"code","ddb1cd8a":"code","ffbad891":"markdown","7f91e142":"markdown","575b9969":"markdown","8fffd32b":"markdown","db55efbe":"markdown","49b792e7":"markdown","d7fadb21":"markdown"},"source":{"f22a03d8":"import gc\nimport os\nimport random\n\nimport lightgbm as lgb\nimport numpy as np\nimport pandas as pd\nimport time\nfrom tqdm import tqdm\nfrom sklearn.model_selection import GroupKFold\n\nimport xgboost as xgb\nfrom xgboost import plot_importance, plot_tree\nimport matplotlib.pyplot as plt\n\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import LabelEncoder\nimport pickle\n\npath_data = \"\/kaggle\/input\/ashrae-energy-prediction\/\"\npath_train = path_data + \"train.csv\"\npath_test = path_data + \"test.csv\"\npath_building = path_data + \"building_metadata.csv\"\npath_weather_train = path_data + \"weather_train.csv\"\npath_weather_test = path_data + \"weather_test.csv\"\n\n\nseed = 2019\nrandom.seed(seed)\nplt.style.use('fivethirtyeight')","d7fa8182":"## Memory optimization\n\n# Original code from https:\/\/www.kaggle.com\/gemartin\/load-data-reduce-memory-usage by @gemartin\n# Modified to support timestamp type, categorical type\n# Modified to add option to use float16\n\nfrom pandas.api.types import is_datetime64_any_dtype as is_datetime\nfrom pandas.api.types import is_categorical_dtype\n\ndef reduce_mem_usage(df, use_float16=False):\n    \"\"\"\n    Iterate through all the columns of a dataframe and modify the data type to reduce memory usage.        \n    \"\"\"\n    \n    start_mem = df.memory_usage().sum() \/ 1024**2\n    print(\"Memory usage of dataframe is {:.2f} MB\".format(start_mem))\n    \n    for col in df.columns:\n        if is_datetime(df[col]) or is_categorical_dtype(df[col]):\n            continue\n        col_type = df[col].dtype\n        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == \"int\":\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if use_float16 and c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        else:\n            df[col] = df[col].astype(\"category\")\n\n    end_mem = df.memory_usage().sum() \/ 1024**2\n    print(\"Memory usage after optimization is: {:.2f} MB\".format(end_mem))\n    print(\"Decreased by {:.1f}%\".format(100 * (start_mem - end_mem) \/ start_mem))\n    \n    return df","c3358c9d":"def prepare_data(X, building_data, weather_data, test=False):\n    \"\"\"\n    Preparing final dataset with all features.\n    \"\"\"\n    \n    X = X.merge(building_data, on=\"building_id\", how=\"left\")\n    X = X.merge(weather_data, on=[\"site_id\", \"timestamp\"], how=\"left\")\n    \n    #X.sort_values(\"timestamp\")\n    #X.reset_index(drop=True)\n    \n    gc.collect()\n    \n    holidays = [\"2016-01-01\", \"2016-01-18\", \"2016-02-15\", \"2016-05-30\", \"2016-07-04\",\n                \"2016-09-05\", \"2016-10-10\", \"2016-11-11\", \"2016-11-24\", \"2016-12-26\",\n                \"2017-01-02\", \"2017-01-16\", \"2017-02-20\", \"2017-05-29\", \"2017-07-04\",\n                \"2017-09-04\", \"2017-10-09\", \"2017-11-10\", \"2017-11-23\", \"2017-12-25\",\n                \"2018-01-01\", \"2018-01-15\", \"2018-02-19\", \"2018-05-28\", \"2018-07-04\",\n                \"2018-09-03\", \"2018-10-08\", \"2018-11-12\", \"2018-11-22\", \"2018-12-25\",\n                \"2019-01-01\"]\n    \n    X.timestamp = pd.to_datetime(X.timestamp, format=\"%Y-%m-%d %H:%M:%S\")\n    X.square_feet = np.log1p(X.square_feet)\n    \n    X[\"hour\"] = X.timestamp.dt.hour\n    X[\"month\"]=X.timestamp.dt.month\n    X[\"weekday\"] = X.timestamp.dt.weekday\n    X[\"is_holiday\"] = (X.timestamp.isin(holidays)).astype(int)\n    \n    drop_features = [\"timestamp\", \"sea_level_pressure\", \"wind_direction\", \"wind_speed\"]\n\n    X.drop(drop_features, axis=1, inplace=True)\n\n    if test:\n        row_ids = X.row_id\n        X.drop(\"row_id\", axis=1, inplace=True)\n        return X, row_ids\n    else:\n        y = np.log1p(X.meter_reading)\n        X.drop(\"meter_reading\", axis=1, inplace=True)\n        return X, y","ffa96877":"#TRAIN MAKER\n\ndef TRAINMAKER():\n    #get csv\n    df_train = pd.read_csv(path_train)\n    building = pd.read_csv(path_building)\n    #labelencode it\n    le = LabelEncoder()\n    building.primary_use = le.fit_transform(building.primary_use)\n    weather_train = pd.read_csv(path_weather_train)\n    #reduce memory\n    df_train = reduce_mem_usage(df_train, use_float16=True)\n    building = reduce_mem_usage(building, use_float16=True)\n    weather_train = reduce_mem_usage(weather_train, use_float16=True)\n    #make train set\n    X_train, y_train = prepare_data(df_train, building, weather_train)\n    \n    print('helo')\n    del df_train, weather_train,building\n    gc.collect()\n    \n    return X_train,y_train","86b77230":"#TEST MAKER\ndef TESTMAKER():\n    df_test = pd.read_csv(path_test)\n    df_test = reduce_mem_usage(df_test)\n\n    weather_test = pd.read_csv(path_weather_test)\n    weather_test = reduce_mem_usage(weather_test)\n    \n    building = pd.read_csv(path_building)\n    building = reduce_mem_usage(building, use_float16=True)\n    \n    le = LabelEncoder()\n    building.primary_use = le.fit_transform(building.primary_use)\n    \n    X_test, row_ids = prepare_data(df_test, building, weather_test, test=True)\n    del df_test, building, weather_test\n    gc.collect()\n    return X_test,row_ids","e05d96af":"X_train,y_train=TRAINMAKER()","6f99db0a":"X_train.shape","358d09a5":"rows_to_drop = pd.read_csv(\"..\/input\/ashrae-simple-data-cleanup-lb-1-08-no-leaks-v3\/rows_to_drop.csv\")\n\nbad_rows = rows_to_drop.values.tolist() # This is a list of lists\n# Flatten the bad_rows list\nflattened_bad_rows = [val for sublist in bad_rows for val in sublist]\nX_train.drop(flattened_bad_rows, axis=0, inplace=True)\n\n# Do the same to the target variable\ny_train = y_train.reindex_like(X_train)\n\nX_train.shape, y_train.shape","131ec0f3":"cols=list(X_train.columns)\nmodels = []\nskf = GroupKFold(n_splits=6)\na=0","bd289ba9":"    for i, (idxT, idxV) in enumerate( skf.split(X_train, y_train, groups=X_train['month']) ):\n        month = X_train.iloc[idxV]['month'].iloc[0]\n        print('Fold',i,'withholding month',month)\n        print(' rows of train =',len(idxT),'rows of holdout =',len(idxV))\n        oof = []\n        reg =  xgb.XGBRegressor(\n                      n_estimators=6000,\n                      max_depth=12,\n                      num_boost_round=500,\n                      learning_rate=0.03,\n                      subsample=0.8,\n                      colsample_bytree=0.4,\n                      missing=np.nan,\n                      objective ='reg:squarederror',\n                      tree_method='gpu_hist'\n                      )\n        h = reg.fit(X_train[cols].iloc[idxT], y_train.iloc[idxT], \n                eval_set=[(X_train[cols].iloc[idxV],y_train.iloc[idxV])],\n                verbose=1000, early_stopping_rounds=500)\n    \n        oof = reg.predict(X_train[cols].iloc[idxV])\n        #preds += reg.predict_proba(X_test[cols])[:,1]\/skf.n_splits\n        print('#'*20)\n        print ('OOF CV=',mean_squared_error(y_train.iloc[idxV],oof))\n        print('#'*20)\n       # models.append(reg)\n        pickle.dump(reg, open(\"thunder{}.pickle.dat\".format(a), \"wb\"))\n        a=a+1\n        del h, reg, oof\n        x=gc.collect()\n   ","ac1030c1":"del X_train,y_train\ngc.collect()","c62db302":"X_test,row_ids=TESTMAKER()","c93559b9":"i=0\na=0\nres=[]\nmodels=[]\nfolds=6\nstep_size = 50000\nfor a in range(0,6):\n    models.append(pickle.load(open(\"thunder{}.pickle.dat\".format(a), \"rb\")))","c4ce33ad":"for a in models:\n    plot_importance(a)","3876e350":"for j in tqdm(range(int(np.ceil(X_test.shape[0]\/50000)))):\n    res.append(np.expm1(sum([model.predict(X_test.iloc[i:i+step_size]) for model in models])\/folds))\n    i+=step_size","01d059f6":"res = np.concatenate(res)","ddb1cd8a":"submission = pd.read_csv('\/kaggle\/input\/ashrae-energy-prediction\/sample_submission.csv')\nsubmission['meter_reading'] = res\nsubmission.loc[submission['meter_reading']<0, 'meter_reading'] = 0\nsubmission.to_csv('submission.csv', index=False)\nsubmission.sample(10)","ffbad891":"<p style=\"color:orange;\">UPVOTE if you like<p> ","7f91e142":"## <p style=\"color:MediumSeaGreen;\">Training XGB With GroupKFold (Month) :<\/p>","575b9969":"## <p style=\"color:MediumSeaGreen;\">XGBOOST Regressor<\/p>\n\n\n\n\n<p style=\"color:rgb(60, 60, 60);\">In this kernel XGB Regressor is implemented.<br>\nDue to memory reason rather than appending model i exported them with pickle and imported to predict.<br>\n\n\n","8fffd32b":"## <p style=\"color:MediumSeaGreen;\">Feature Importance<\/p>","db55efbe":"## <p style=\"color:MediumSeaGreen;\">Prepare Data :}<\/p>\nThere are two files with features that need to be merged with the data. One is building metadata that has information on the buildings and the other is weather data that has information on the weather.   \n\nNote that the only features created are hour, weekday and is_holiday!<br>\n\nFE code ref:-https:\/\/www.kaggle.com\/rohanrao\/ashrae-half-and-half<br><\/p>","49b792e7":"## <p style=\"color:MediumSeaGreen;\">Memory reducer function<\/p>\n","d7fadb21":"## <p style=\"color:MediumSeaGreen;font_size:100px\">Predicting Chunks<\/p>\nprediction code ref:-https:\/\/www.kaggle.com\/corochann\/ashrae-training-lgbm-by-meter-type<br>"}}