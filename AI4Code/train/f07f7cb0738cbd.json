{"cell_type":{"90772731":"code","654f3d97":"code","44200091":"code","1ca9ab79":"code","8f89eedd":"code","3e284ffa":"code","ad70b555":"code","81065fdd":"code","3282787b":"code","a75c5219":"code","b4bc5835":"code","23305eb5":"code","eb479a39":"markdown","a0c49780":"markdown","e4f6b2de":"markdown","4866ff4f":"markdown","cbc42355":"markdown","1af5a85f":"markdown"},"source":{"90772731":"!pip install torchsummary","654f3d97":"import torch\nimport torchvision\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchsummary import summary\nimport os\nfrom glob import glob\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport cv2\nfrom PIL import Image\nfrom sklearn.model_selection import train_test_split\nfrom torch import optim\nfrom torchvision import datasets, transforms, models\nfrom torchvision.datasets import ImageFolder\nfrom torchvision.models import resnet18","44200091":"device = 'cuda' if torch.cuda.is_available() else 'cpu'","1ca9ab79":"train_dir = '..\/input\/face-mask-12k-images-dataset\/Face Mask Dataset\/Train\/'\nval_dir = '..\/input\/face-mask-12k-images-dataset\/Face Mask Dataset\/Validation\/'\ntest_dir = '..\/input\/face-mask-12k-images-dataset\/Face Mask Dataset\/Test\/'","8f89eedd":"train_transforms = transforms.Compose([\n#                                        transforms.RandomRotation(30),\n                                       transforms.Resize((224, 224)),\n#                                        transforms.RandomHorizontalFlip(),\n#                                        transforms.Grayscale(3),\n                                       transforms.ToTensor(),\n                                       transforms.Normalize([0.485, 0.456, 0.406],\n                                                            [0.229, 0.224, 0.225])])\n\ntest_transforms = transforms.Compose([\n                                      transforms.Resize((224, 224)),\n#                                       transforms.Grayscale(3),\n                                      transforms.ToTensor(),\n                                      transforms.Normalize([0.485, 0.456, 0.406],\n                                                           [0.229, 0.224, 0.225])])\nval_transforms = transforms.Compose([\n                                      transforms.Resize((224, 224)),\n#                                       transforms.Grayscale(3),\n                                      transforms.ToTensor(),\n                                      transforms.Normalize([0.485, 0.456, 0.406],\n                                                           [0.229, 0.224, 0.225])])\ntrain_data = datasets.ImageFolder(train_dir, transform=train_transforms)\ntest_data = datasets.ImageFolder(test_dir, transform=test_transforms)\nval_data = datasets.ImageFolder(val_dir, transform=val_transforms)\ntrain_loader = DataLoader(dataset=train_data, batch_size=32, shuffle=True)\ntest_loader = DataLoader(dataset=test_data, batch_size=32)\nval_loader = DataLoader(dataset=val_data, batch_size=32)","3e284ffa":"def imshow(inp, title=None):\n    \"\"\"Imshow for Tensor.\"\"\"\n    inp = inp.numpy().transpose((1, 2, 0))\n    mean = np.array([0.485, 0.456, 0.406])\n    std = np.array([0.229, 0.224, 0.225])\n    inp = std * inp + mean\n    inp = np.clip(inp, 0, 1)\n    plt.imshow(inp)\n    if title is not None:\n        plt.title(title)\n    plt.pause(0.001)","ad70b555":"for (images, labels) in train_data:\n    out = torchvision.utils.make_grid(images)\n    imshow(out, title=[labels])\n    break","81065fdd":"model = models.resnet18(pretrained=True)\nprint(model)","3282787b":"for param in model.parameters():\n    param.requires_grad = False\nmodel.avgpool = nn.AdaptiveAvgPool2d(1)\nmodel.fc = nn.Sequential(nn.Flatten(), \n    nn.Linear(512, 128),\n    nn.ReLU(),\n    nn.Dropout(0.2),\n    nn.Linear(128, 1),\n    nn.Sigmoid())\ncriterion = nn.BCELoss()\noptimizer = torch.optim.Adam(model.parameters(), lr= 0.001)","a75c5219":"model.to(device)\nsummary(model, (3,224,224))","b4bc5835":"num_epochs = 10\nn_total_steps = len(train_loader)\nmodel.train()\nvalid_loss_min = np.Inf\nfor epoch in range(num_epochs):\n    valid_loss = 0.0\n    for i, (images, labels) in enumerate(train_loader):\n        images = images.float().to(device)\n        labels = torch.unsqueeze(labels, 1)\n        labels = labels.float().to(device)\n        \n        # Forward pass\n        outputs = model(images)\n        loss = criterion(outputs, labels)\n\n        # Backward and optimize\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        if (i + 1) % 10 == 0:\n            print(f'Epoch [{epoch + 1}\/{num_epochs}], Step [{i + 1}\/{n_total_steps}], Loss: {loss.item():.4f}')\n        valid_loss = loss.item()\n        if valid_loss <= valid_loss_min:\n            print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(valid_loss_min,valid_loss))\n            torch.save(model.state_dict(), 'model.pt')\n            valid_loss_min = valid_loss","23305eb5":"model.load_state_dict(torch.load('model.pt'))\nmodel.eval()\nwith torch.no_grad():\n    n_correct = 0\n    n_samples = 0\n    for images, labels in test_loader:\n        images = images.to(device)\n        labels = torch.unsqueeze(labels, 1)\n        labels = labels.to(device)\n        predictions = model(images)\n        predictions = (predictions >= 0.5)\n        \n        n_samples += labels.shape[0]\n        n_correct += (predictions == labels).sum().item()\n#         print(n_correct, n_samples)\n    acc = 100.0 * n_correct \/ n_samples\n    print(f'Accuracy of the network on the test images: {acc} %')","eb479a39":"# Test model","a0c49780":"# Model","e4f6b2de":"# Let's train","4866ff4f":"# Customize some layers","cbc42355":"# Prepare data","1af5a85f":"num_epochs = 10\nbatch_size = 32"}}