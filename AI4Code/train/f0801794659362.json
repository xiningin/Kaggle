{"cell_type":{"0ff8662a":"code","998c8e9c":"code","402284b4":"code","4f705806":"code","d49ba917":"code","20cf1ee1":"code","240d32cb":"code","b21cd251":"code","d17d9f66":"code","78c6f562":"code","7c320887":"code","784d4184":"code","8ea4b5e8":"code","3d69c6d0":"code","e4062a59":"code","d229ae02":"code","c4e2d0c8":"code","901888a5":"code","2f9552ef":"code","ab4b2dcc":"code","7c137b6e":"code","f8f04daf":"code","d0a223f8":"code","bf233c2e":"code","066cc110":"code","c5563b59":"code","905a3fc7":"code","d8cbc384":"code","29fa3a8a":"code","68e39594":"code","4aa077c6":"code","c23dc14a":"code","4836fb58":"code","81015dc6":"code","d7de434a":"code","0d158005":"code","6198504d":"code","0e0ed9a1":"code","eae85327":"code","8694242b":"code","b7e13c91":"code","b1aed76f":"code","1b218378":"code","0174b196":"code","d6c4423a":"code","b209ee17":"code","0d1ff7ee":"code","041c1b8f":"code","428c00f4":"code","4c246450":"code","707f2a40":"code","a2a536ca":"code","a4344fc0":"code","e4388d9a":"markdown","dd85ccf4":"markdown","8cc6b315":"markdown","65d957b3":"markdown","ca34695b":"markdown","4f583a61":"markdown","f771b52c":"markdown","d41f6df8":"markdown","12e80a2e":"markdown","772d6a2e":"markdown","35cb0358":"markdown","0d1cdd70":"markdown","72efa683":"markdown","63db3504":"markdown","e56437fc":"markdown","4c3aef43":"markdown","baf928c9":"markdown","6cde09b5":"markdown","471e4b4f":"markdown","6de5262f":"markdown","b24dc699":"markdown","ac7aa316":"markdown","cf92346f":"markdown","a19345bb":"markdown","a9c534d6":"markdown","716ae3b2":"markdown","bd0ab9f6":"markdown","e288a538":"markdown","2d8ba8f6":"markdown","7eaba359":"markdown","40fdfb7e":"markdown","97c59033":"markdown","811643c3":"markdown","885a1df3":"markdown","4d212b91":"markdown","c842e2b7":"markdown","4eb759ef":"markdown","cb3beac7":"markdown","24d354a3":"markdown","300986b0":"markdown","72fc6335":"markdown","0785b11d":"markdown","1d28a0ad":"markdown","fd7c0aea":"markdown","6ab06f05":"markdown","31fefbc0":"markdown","aa0e9aad":"markdown","8ef7f133":"markdown","793c0355":"markdown","d5e1ba41":"markdown","0d0c210d":"markdown","b065ad5f":"markdown","f2f0cfb7":"markdown","b3f9c6f3":"markdown","e5c399e9":"markdown","8848f2e3":"markdown","5a6b7803":"markdown","ec3c3171":"markdown","aab3f5c9":"markdown","9fa2c84b":"markdown","a3fa3e17":"markdown","94b0c67e":"markdown","f41a0b75":"markdown","5077d559":"markdown","818a4515":"markdown","910c3015":"markdown"},"source":{"0ff8662a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","998c8e9c":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings(\"ignore\")","402284b4":"dataset = pd.read_csv('\/kaggle\/input\/pima-indians-diabetes-database\/diabetes.csv')","4f705806":"dataset","d49ba917":"dataset.info()","20cf1ee1":"dataset.isnull().sum()","240d32cb":"dataset.describe()","b21cd251":"plt.figure(figsize=(10,10))\nsns.heatmap(dataset.corr(), cmap=\"YlGnBu\", annot= True,)\nplt.show()","d17d9f66":"sns.set(style=\"whitegrid\")\nlabels = ['Healthy', 'Diabetic']\nsizes = dataset['Outcome'].value_counts(sort = True)\n\ncolors = [\"lightblue\",\"red\"]\nexplode = (0.05,0) \n \nplt.figure(figsize=(7,7))\nplt.pie(sizes, explode=explode, labels=labels, colors=colors, autopct='%1.1f%%', shadow=True, startangle=90,)\n\nplt.title('Number of diabetes in the dataset')\nplt.show()","78c6f562":"plt.style.use(\"classic\")\nplt.figure(figsize=(10,10))\n\nsns.distplot(dataset[dataset['Outcome'] == 0][\"Pregnancies\"], color='green') # Healthy - green\nsns.distplot(dataset[dataset['Outcome'] == 1][\"Pregnancies\"], color='red') # Diabetic - Red\n\nplt.title('Healthy vs Diabetic by Pregnancy', fontsize=15)\nplt.xlim([-5,20])\nplt.grid(linewidth = 0.7)\nplt.show()","7c320887":"plt.style.use(\"classic\")\nplt.figure(figsize=(10,10))\n\nsns.distplot(dataset[dataset['Outcome'] == 0][\"Glucose\"], color='green') # Healthy - green\nsns.distplot(dataset[dataset['Outcome'] == 1][\"Glucose\"], color='red') # Diabetic - Red\n\nplt.title('Healthy vs Diabetic by Glucose', fontsize=15)\nplt.xlim([-5,250])\nplt.grid(linewidth = 0.7)\nplt.show()","784d4184":"plt.style.use(\"classic\")\nplt.figure(figsize=(10,10))\n\nsns.distplot(dataset[dataset['Outcome'] == 0][\"BloodPressure\"], color='green') # Healthy - green\nsns.distplot(dataset[dataset['Outcome'] == 1][\"BloodPressure\"], color='red') # Diabetic - Red\n\nplt.title('Healthy vs Diabetic by Blood Pressure', fontsize=15)\nplt.xlim([-5,150])\nplt.grid(linewidth = 0.7)\nplt.show()","8ea4b5e8":"plt.style.use(\"classic\")\nplt.figure(figsize=(10,10))\n\nsns.distplot(dataset[dataset['Outcome'] == 0][\"SkinThickness\"], color='green') # Healthy - green\nsns.distplot(dataset[dataset['Outcome'] == 1][\"SkinThickness\"], color='red') # Diabetic - Red\n\nplt.title('Healthy vs Diabetic by Skin Thickness', fontsize=15)\nplt.xlim([-5,120])\nplt.grid(linewidth = 0.7)\nplt.show()","3d69c6d0":"plt.style.use(\"classic\")\nplt.figure(figsize=(10,10))\n\nsns.distplot(dataset[dataset['Outcome'] == 0][\"Insulin\"], color='green') # Healthy - green\nsns.distplot(dataset[dataset['Outcome'] == 1][\"Insulin\"], color='red') # Diabetic - Red\n\nplt.title('Healthy vs Diabetic by Insulin', fontsize=15)\nplt.xlim([-10,900])\nplt.grid(linewidth = 0.7)\nplt.show()","e4062a59":"plt.style.use(\"classic\")\nplt.figure(figsize=(10,10))\n\nsns.distplot(dataset[dataset['Outcome'] == 0][\"BMI\"], color='green') # Healthy - green\nsns.distplot(dataset[dataset['Outcome'] == 1][\"BMI\"], color='red') # Diabetic - Red\n\nplt.title('Healthy vs Diabetic by BMI', fontsize=15)\nplt.xlim([-10,75])\nplt.grid(linewidth = 0.7)\nplt.show()","d229ae02":"plt.style.use(\"classic\")\nplt.figure(figsize=(10,10))\n\nsns.distplot(dataset[dataset['Outcome'] == 0][\"DiabetesPedigreeFunction\"], color='green') # Healthy - green\nsns.distplot(dataset[dataset['Outcome'] == 1][\"DiabetesPedigreeFunction\"], color='red') # Diabetic - Red\n\nplt.title('Healthy vs Diabetic by Diabetes Pedigree Function', fontsize=15)\nplt.xlim([-1,3])\nplt.grid(linewidth = 0.7)\nplt.show()","c4e2d0c8":"plt.style.use(\"classic\")\nplt.figure(figsize=(10,10))\n\nsns.distplot(dataset[dataset['Outcome'] == 0][\"Age\"], color='green') # Healthy - green\nsns.distplot(dataset[dataset['Outcome'] == 1][\"Age\"], color='red') # Diabetic - Red\n\nplt.title('Healthy vs Diabetic by Age', fontsize=15)\nplt.xlim([0,100])\nplt.grid(linewidth = 0.7)\nplt.show()","901888a5":"sns.set_theme(style=\"darkgrid\")\n\nsns.boxplot(x= dataset.Pregnancies)\nplt.xlim([-1,20])\nplt.title(\"Box Plot before Median Imputation\")\nplt.show()\n\nq1 = dataset.Pregnancies.quantile(0.25)\nq3 = dataset.Pregnancies.quantile(0.75)\niqr = q3-q1\nLower_tail = q1 - 1.5 * iqr\nUpper_tail = q3 + 1.5 * iqr\nmed = np.median(dataset.Pregnancies)\nfor i in dataset.Pregnancies:\n    if i > Upper_tail or i < Lower_tail:\n            dataset.Pregnancies = dataset.Pregnancies.replace(i, med)\nsns.boxplot(x= dataset.Pregnancies)\nplt.xlim([-1,15])\nplt.title(\"Box Plot after Median Imputation\")\nplt.show()   ","2f9552ef":"sns.set_theme(style=\"darkgrid\")\n\nsns.boxplot(x= dataset.Glucose)\nplt.xlim([-2,210])\nplt.title(\"Box Plot before Median Imputation\")\nplt.show()\n\nq1 = dataset.Glucose.quantile(0.25)\nq3 = dataset.Glucose.quantile(0.75)\niqr = q3-q1\nLower_tail = q1 - 1.5 * iqr\nUpper_tail = q3 + 1.5 * iqr\nmed = np.median(dataset.Glucose)\nfor i in dataset.Glucose:\n    if i > Upper_tail or i < Lower_tail:\n            dataset.Glucose = dataset.Glucose.replace(i, med)\nsns.boxplot(x= dataset.Glucose)\nplt.xlim([-2,210])\nplt.title(\"Box Plot after Median Imputation\")\nplt.show()   ","ab4b2dcc":"sns.set_theme(style=\"darkgrid\")\n\nsns.boxplot(x= dataset.BloodPressure)\nplt.xlim([-1,140])\nplt.title(\"Box Plot before Median Imputation\")\nplt.show()\n\nq1 = dataset.BloodPressure.quantile(0.25)\nq3 = dataset.BloodPressure.quantile(0.75)\niqr = q3-q1\nLower_tail = q1 - 1.5 * iqr\nUpper_tail = q3 + 1.5 * iqr\nmed = np.median(dataset.BloodPressure)\nfor i in dataset.BloodPressure:\n    if i > Upper_tail or i < Lower_tail:\n            dataset.BloodPressure = dataset.BloodPressure.replace(i, med)\nsns.boxplot(x= dataset.BloodPressure)\nplt.xlim([-1,140])\nplt.title(\"Box Plot after Median Imputation\")\nplt.show()   ","7c137b6e":"sns.set_theme(style=\"darkgrid\")\n\nsns.boxplot(x= dataset.SkinThickness)\nplt.xlim([-3,105])\nplt.title(\"Box Plot before Median Imputation\")\nplt.show()\n\nq1 = dataset.SkinThickness.quantile(0.25)\nq3 = dataset.SkinThickness.quantile(0.75)\niqr = q3-q1\nLower_tail = q1 - 1.5 * iqr\nUpper_tail = q3 + 1.5 * iqr\nmed = np.median(dataset.SkinThickness)\nfor i in dataset.SkinThickness:\n    if i > Upper_tail or i < Lower_tail:\n            dataset.SkinThickness = dataset.SkinThickness.replace(i, med)\nsns.boxplot(x= dataset.SkinThickness)\nplt.xlim([-3,105])\nplt.title(\"Box Plot after Median Imputation\")\nplt.show()   ","f8f04daf":"sns.set_theme(style=\"darkgrid\")\n\nsns.boxplot(x= dataset.Insulin)\nplt.xlim([-10,905])\nplt.title(\"Box Plot before Median Imputation\")\nplt.show()\n\nq1 = dataset.Insulin.quantile(0.25)\nq3 = dataset.Insulin.quantile(0.75)\niqr = q3-q1\nLower_tail = q1 - 1.5 * iqr\nUpper_tail = q3 + 1.5 * iqr\nmed = np.median(dataset.Insulin)\nfor i in dataset.Insulin:\n    if i > Upper_tail or i < Lower_tail:\n            dataset.Insulin = dataset.Insulin.replace(i, med)\nsns.boxplot(x= dataset.Insulin)\nplt.xlim([-10,905])\nplt.title(\"Box Plot after Median Imputation\")\nplt.show()   ","d0a223f8":"sns.set_theme(style=\"darkgrid\")\n\nsns.boxplot(x= dataset.BMI)\nplt.xlim([-3,75])\nplt.title(\"Box Plot before Median Imputation\")\nplt.show()\n\nq1 = dataset.BMI.quantile(0.25)\nq3 = dataset.BMI.quantile(0.75)\niqr = q3-q1\nLower_tail = q1 - 1.5 * iqr\nUpper_tail = q3 + 1.5 * iqr\nmed = np.median(dataset.BMI)\nfor i in dataset.BMI:\n    if i > Upper_tail or i < Lower_tail:\n            dataset.BMI = dataset.BMI.replace(i, med)\nsns.boxplot(x= dataset.BMI)\nplt.xlim([-3,75])\nplt.title(\"Box Plot after Median Imputation\")\nplt.show()   ","bf233c2e":"sns.set_theme(style=\"darkgrid\")\n\nsns.boxplot(x= dataset.DiabetesPedigreeFunction)\nplt.title(\"Box Plot before Median Imputation\")\nplt.show()\n\nq1 = dataset.DiabetesPedigreeFunction.quantile(0.25)\nq3 = dataset.DiabetesPedigreeFunction.quantile(0.75)\niqr = q3-q1\nLower_tail = q1 - 1.5 * iqr\nUpper_tail = q3 + 1.5 * iqr\nmed = np.median(dataset.DiabetesPedigreeFunction)\nfor i in dataset.DiabetesPedigreeFunction:\n    if i > Upper_tail or i < Lower_tail:\n            dataset.DiabetesPedigreeFunction = dataset.DiabetesPedigreeFunction.replace(i, med)\nsns.boxplot(x= dataset.DiabetesPedigreeFunction)\nplt.xlim([0,1.5])\nplt.title(\"Box Plot after Median Imputation\")\nplt.show()   ","066cc110":"sns.set_theme(style=\"darkgrid\")\n\nsns.boxplot(x= dataset.Age)\nplt.title(\"Box Plot before Median Imputation\")\nplt.show()\n\nq1 = dataset.Age.quantile(0.25)\nq3 = dataset.Age.quantile(0.75)\niqr = q3-q1\nLower_tail = q1 - 1.5 * iqr\nUpper_tail = q3 + 1.5 * iqr\nmed = np.median(dataset.Age)\nfor i in dataset.Age:\n    if i > Upper_tail or i < Lower_tail:\n            dataset.Age = dataset.Age.replace(i, med)\nsns.boxplot(x= dataset.Age)\nplt.title(\"Box Plot after Median Imputation\")\nplt.show()   ","c5563b59":"sns.pairplot(data=dataset,hue='Outcome',diag_kind='kde')\nplt.show()","905a3fc7":"x = dataset.iloc[:, :-1].values\ny = dataset.iloc[:, -1].values","d8cbc384":"x","29fa3a8a":"y","68e39594":"from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size= 0.2, random_state= 0)","4aa077c6":"print(\"Number transactions x_train dataset: \", x_train.shape)\nprint(\"Number transactions y_train dataset: \", y_train.shape)\nprint(\"Number transactions x_test dataset: \", x_test.shape)\nprint(\"Number transactions y_test dataset: \", y_test.shape)","c23dc14a":"from sklearn.preprocessing import StandardScaler \nsc = StandardScaler()\nx_train = sc.fit_transform(x_train)\nx_test = sc.transform(x_test)","4836fb58":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.naive_bayes import BernoulliNB\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, AdaBoostClassifier, GradientBoostingClassifier\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier","81015dc6":"from sklearn.metrics import accuracy_score, confusion_matrix, roc_auc_score, ConfusionMatrixDisplay, precision_score, recall_score, f1_score, classification_report, roc_curve, plot_roc_curve, auc, precision_recall_curve, plot_precision_recall_curve, average_precision_score\nfrom sklearn.model_selection import cross_val_score","d7de434a":"models = []\nmodels.append(['Logistic Regreesion', LogisticRegression(random_state=0)])\nmodels.append(['SVM', SVC(random_state=0)])\nmodels.append(['KNeighbors', KNeighborsClassifier()])\nmodels.append(['GaussianNB', GaussianNB()])\nmodels.append(['BernoulliNB', BernoulliNB()])\nmodels.append(['Decision Tree', DecisionTreeClassifier(random_state=0)])\nmodels.append(['Random Forest', RandomForestClassifier(random_state=0)])\nmodels.append(['Extra Tree', ExtraTreesClassifier(random_state=0)])\nmodels.append(['AdaBoost', AdaBoostClassifier(random_state=0)])\nmodels.append(['Gradiesnt Boost', GradientBoostingClassifier(random_state=0)])\nmodels.append(['Light GBM', LGBMClassifier(random_state=0)])\nmodels.append(['XGBoost', XGBClassifier(eval_metric= 'error')])\n\nlst_1= []\n\nfor m in range(len(models)):\n    lst_2= []\n    model = models[m][1]\n    model.fit(x_train, y_train)\n    y_pred = model.predict(x_test)\n    cm = confusion_matrix(y_test, y_pred)  #Confusion Matrix\n    accuracies = cross_val_score(estimator = model, X = x_train, y = y_train, cv = 10)   #K-Fold Validation\n    roc = roc_auc_score(y_test, y_pred)  #ROC AUC Score\n    precision = precision_score(y_test, y_pred)  #Precision Score\n    recall = recall_score(y_test, y_pred)  #Recall Score\n    f1 = f1_score(y_test, y_pred)  #F1 Score\n    print(models[m][0],':')\n    print(cm)\n    print('Accuracy Score: ',accuracy_score(y_test, y_pred))\n    print('')\n    print(\"K-Fold Validation Mean Accuracy: {:.2f} %\".format(accuracies.mean()*100))\n    print('')\n    print(\"Standard Deviation: {:.2f} %\".format(accuracies.std()*100))\n    print('')\n    print('ROC AUC Score: {:.2f}'.format(roc))\n    print('')\n    print('Precision: {:.2f}'.format(precision))\n    print('')\n    print('Recall: {:.2f}'.format(recall))\n    print('')\n    print('F1: {:.2f}'.format(f1))\n    print('-----------------------------------')\n    print('')\n    lst_2.append(models[m][0])\n    lst_2.append((accuracy_score(y_test, y_pred))*100) \n    lst_2.append(accuracies.mean()*100)\n    lst_2.append(accuracies.std()*100)\n    lst_2.append(roc)\n    lst_2.append(precision)\n    lst_2.append(recall)\n    lst_2.append(f1)\n    lst_1.append(lst_2)","0d158005":"df = pd.DataFrame(lst_1, columns= ['Model', 'Accuracy', 'K-Fold Mean Accuracy', 'Std. Deviation', 'ROC AUC', 'Precision', 'Recall', 'F1'])","6198504d":"df.sort_values(by= ['Accuracy', 'K-Fold Mean Accuracy'], inplace= True, ascending= False)","0e0ed9a1":"df","eae85327":"from sklearn.model_selection import GridSearchCV","8694242b":"grid_models = [(LogisticRegression(),[{'C':[0.25,0.5,0.75,1],'random_state':[0]}]), \n               (KNeighborsClassifier(),[{'n_neighbors':[5,7,8,10], 'metric': ['euclidean', 'manhattan', 'chebyshev', 'minkowski']}]), \n               (SVC(),[{'C':[0.25,0.5,0.75,1],'kernel':['linear', 'rbf'],'random_state':[0]}]), \n               (GaussianNB(),[{'var_smoothing': [1e-09]}]), \n               (BernoulliNB(), [{'alpha': [0.25, 0.5, 1]}]), \n               (DecisionTreeClassifier(),[{'criterion':['gini','entropy'],'random_state':[0]}]), \n               (RandomForestClassifier(),[{'n_estimators':[100,150,200],'criterion':['gini','entropy'],'random_state':[0]}]),\n               (ExtraTreesClassifier(),[{'n_estimators':[100,150,200],'criterion':['gini','entropy'],'random_state':[0]}]),\n               (AdaBoostClassifier(),[{'n_estimators':[100,150,200],'learning_rate':[0.1, 0.5, 0.8, 1],'algorithm':['SAMME', 'SAMME.R'], 'random_state':[0]}]),\n               (GradientBoostingClassifier(),[{'n_estimators':[100,150,200],'criterion':['friedman_mse','mse'],'loss':['deviance','exponential'],'learning_rate':[0.1, 0.5, 0.8, 1],'random_state':[0]}]),\n               (LGBMClassifier(),[{'n_estimators':[100,150,200],'learning_rate':[0.1, 0.5, 0.8, 1],'random_state':[0]}]),\n               (XGBClassifier(), [{'learning_rate': [0.01, 0.05, 0.1], 'eval_metric': ['error']}])]","b7e13c91":"for i,j in grid_models:\n    grid = GridSearchCV(estimator=i,param_grid = j, scoring = 'accuracy',cv = 10)\n    grid.fit(x_train, y_train)\n    best_accuracy = grid.best_score_\n    best_param = grid.best_params_\n    print('{}:\\nBest Accuracy : {:.2f}%'.format(i,best_accuracy*100))\n    print('Best Parameters : ',best_param)\n    print('')\n    print('----------------')\n    print('')","b1aed76f":"#Fitting Logistic Regression Model\nclassifier = LogisticRegression(C= 1, random_state= 0)\nclassifier.fit(x_train, y_train)\ny_pred = classifier.predict(x_test)\ny_prob = classifier.predict_proba(x_test)[:,1]\ncm = confusion_matrix(y_test, y_pred)\n\nprint(classification_report(y_test, y_pred))\nprint(f'ROC AUC score: {roc_auc_score(y_test, y_prob)}')\nprint('Accuracy Score: ',accuracy_score(y_test, y_pred))\n\n# Visualizing Confusion Matrix\nplt.figure(figsize = (6, 6))\nsns.heatmap(cm, cmap = 'Blues', annot = True, fmt = 'd', linewidths = 5, cbar = False, annot_kws = {'fontsize': 15}, \n            yticklabels = ['Healthy', 'Diabetic'], xticklabels = ['Predicted Healthy', 'Predicted Diabetic'])\nplt.yticks(rotation = 0)\nplt.show()\n\n# Roc AUC Curve\nfalse_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_prob)\nroc_auc = auc(false_positive_rate, true_positive_rate)\n\nsns.set_theme(style = 'white')\nplt.figure(figsize = (6, 6))\nplt.plot(false_positive_rate,true_positive_rate, color = '#b01717', label = 'AUC = %0.3f' % roc_auc)\nplt.legend(loc = 'lower right')\nplt.plot([0, 1], [0, 1], linestyle = '--', color = '#174ab0')\nplt.axis('tight')\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.title('ROC AUC Curve')\nplt.legend()\nplt.show()\n\n#Precision Recall Curve\nplt.figure(figsize = (6, 6))\naverage_precision = average_precision_score(y_test, y_prob)\ndisp = plot_precision_recall_curve(classifier, x_test, y_test)\nplt.title('Precision-Recall Curve')\nplt.show()","1b218378":"#Fitting KNeighborsClassifier Model\nclassifier = KNeighborsClassifier(metric= 'manhattan', n_neighbors= 8)\nclassifier.fit(x_train, y_train)\ny_pred = classifier.predict(x_test)\ny_prob = classifier.predict_proba(x_test)[:,1]\ncm = confusion_matrix(y_test, y_pred)\n\nprint(classification_report(y_test, y_pred))\nprint(f'ROC AUC score: {roc_auc_score(y_test, y_prob)}')\nprint('Accuracy Score: ',accuracy_score(y_test, y_pred))\n\n# Visualizing Confusion Matrix\nplt.figure(figsize = (6, 6))\nsns.heatmap(cm, cmap = 'Blues', annot = True, fmt = 'd', linewidths = 5, cbar = False, annot_kws = {'fontsize': 15}, \n            yticklabels = ['Healthy', 'Diabetic'], xticklabels = ['Predicted Healthy', 'Predicted Diabetic'])\nplt.yticks(rotation = 0)\nplt.show()\n\n# Roc AUC Curve\nfalse_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_prob)\nroc_auc = auc(false_positive_rate, true_positive_rate)\n\nsns.set_theme(style = 'white')\nplt.figure(figsize = (6, 6))\nplt.plot(false_positive_rate,true_positive_rate, color = '#b01717', label = 'AUC = %0.3f' % roc_auc)\nplt.legend(loc = 'lower right')\nplt.plot([0, 1], [0, 1], linestyle = '--', color = '#174ab0')\nplt.axis('tight')\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.title('ROC AUC Curve')\nplt.legend()\nplt.show()\n\n#Precision Recall Curve\naverage_precision = average_precision_score(y_test, y_prob)\ndisp = plot_precision_recall_curve(classifier, x_test, y_test)\nplt.title('Precision-Recall Curve')\nplt.show()","0174b196":"#Fitting SVC Model\nclassifier = SVC(C= 0.5, kernel= 'linear', random_state= 0, probability=True)\nclassifier.fit(x_train, y_train)\ny_pred = classifier.predict(x_test)\ny_prob = classifier.predict_proba(x_test)[:,1]\ncm = confusion_matrix(y_test, y_pred)\n\nprint(classification_report(y_test, y_pred))\nprint(f'ROC AUC score: {roc_auc_score(y_test, y_prob)}')\nprint('Accuracy Score: ',accuracy_score(y_test, y_pred))\n\n# Visualizing Confusion Matrix\nplt.figure(figsize = (6, 6))\nsns.heatmap(cm, cmap = 'Blues', annot = True, fmt = 'd', linewidths = 5, cbar = False, annot_kws = {'fontsize': 15}, \n            yticklabels = ['Healthy', 'Diabetic'], xticklabels = ['Predicted Healthy', 'Predicted Diabetic'])\nplt.yticks(rotation = 0)\nplt.show()\n\n# Roc AUC Curve\nfalse_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_prob)\nroc_auc = auc(false_positive_rate, true_positive_rate)\n\nsns.set_theme(style = 'white')\nplt.figure(figsize = (6, 6))\nplt.plot(false_positive_rate,true_positive_rate, color = '#b01717', label = 'AUC = %0.3f' % roc_auc)\nplt.legend(loc = 'lower right')\nplt.plot([0, 1], [0, 1], linestyle = '--', color = '#174ab0')\nplt.axis('tight')\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.title('ROC AUC Curve')\nplt.legend()\nplt.show()\n\n#Precision Recall Curve\naverage_precision = average_precision_score(y_test, y_prob)\ndisp = plot_precision_recall_curve(classifier, x_test, y_test)\nplt.title('Precision-Recall Curve')\nplt.show()","d6c4423a":"#Fitting GaussianNB Model\nclassifier = GaussianNB(var_smoothing= 1e-09)\nclassifier.fit(x_train, y_train)\ny_pred = classifier.predict(x_test)\ny_prob = classifier.predict_proba(x_test)[:,1]\ncm = confusion_matrix(y_test, y_pred)\n\nprint(classification_report(y_test, y_pred))\nprint(f'ROC AUC score: {roc_auc_score(y_test, y_prob)}')\nprint('Accuracy Score: ',accuracy_score(y_test, y_pred))\n\n# Visualizing Confusion Matrix\nplt.figure(figsize = (6, 6))\nsns.heatmap(cm, cmap = 'Blues', annot = True, fmt = 'd', linewidths = 5, cbar = False, annot_kws = {'fontsize': 15}, \n            yticklabels = ['Healthy', 'Diabetic'], xticklabels = ['Predicted Healthy', 'Predicted Diabetic'])\nplt.yticks(rotation = 0)\nplt.show()\n\n# Roc AUC Curve\nfalse_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_prob)\nroc_auc = auc(false_positive_rate, true_positive_rate)\n\nsns.set_theme(style = 'white')\nplt.figure(figsize = (6, 6))\nplt.plot(false_positive_rate,true_positive_rate, color = '#b01717', label = 'AUC = %0.3f' % roc_auc)\nplt.legend(loc = 'lower right')\nplt.plot([0, 1], [0, 1], linestyle = '--', color = '#174ab0')\nplt.axis('tight')\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.title('ROC AUC Curve')\nplt.legend()\nplt.show()\n\n#Precision Recall Curve\naverage_precision = average_precision_score(y_test, y_prob)\ndisp = plot_precision_recall_curve(classifier, x_test, y_test)\nplt.title('Precision-Recall Curve')\nplt.show()","b209ee17":"#Fitting BernoulliNB Model\nclassifier = BernoulliNB(alpha= 0.25)\nclassifier.fit(x_train, y_train)\ny_pred = classifier.predict(x_test)\ny_prob = classifier.predict_proba(x_test)[:,1]\ncm = confusion_matrix(y_test, y_pred)\n\nprint(classification_report(y_test, y_pred))\nprint(f'ROC AUC score: {roc_auc_score(y_test, y_prob)}')\nprint('Accuracy Score: ',accuracy_score(y_test, y_pred))\n\n# Visualizing Confusion Matrix\nplt.figure(figsize = (6, 6))\nsns.heatmap(cm, cmap = 'Blues', annot = True, fmt = 'd', linewidths = 5, cbar = False, annot_kws = {'fontsize': 15}, \n            yticklabels = ['Healthy', 'Diabetic'], xticklabels = ['Predicted Healthy', 'Predicted Diabetic'])\nplt.yticks(rotation = 0)\nplt.show()\n\n# Roc AUC Curve\nfalse_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_prob)\nroc_auc = auc(false_positive_rate, true_positive_rate)\n\nsns.set_theme(style = 'white')\nplt.figure(figsize = (6, 6))\nplt.plot(false_positive_rate,true_positive_rate, color = '#b01717', label = 'AUC = %0.3f' % roc_auc)\nplt.legend(loc = 'lower right')\nplt.plot([0, 1], [0, 1], linestyle = '--', color = '#174ab0')\nplt.axis('tight')\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.title('ROC AUC Curve')\nplt.legend()\nplt.show()\n\n#Precision Recall Curve\naverage_precision = average_precision_score(y_test, y_prob)\ndisp = plot_precision_recall_curve(classifier, x_test, y_test)\nplt.title('Precision-Recall Curve')\nplt.show()","0d1ff7ee":"#Fitting DecisionTreeClassifier Model\nclassifier = DecisionTreeClassifier(criterion= 'gini', random_state= 0)\nclassifier.fit(x_train, y_train)\ny_pred = classifier.predict(x_test)\ny_prob = classifier.predict_proba(x_test)[:,1]\ncm = confusion_matrix(y_test, y_pred)\n\nprint(classification_report(y_test, y_pred))\nprint(f'ROC AUC score: {roc_auc_score(y_test, y_prob)}')\nprint('Accuracy Score: ',accuracy_score(y_test, y_pred))\n\n# Visualizing Confusion Matrix\nplt.figure(figsize = (6, 6))\nsns.heatmap(cm, cmap = 'Blues', annot = True, fmt = 'd', linewidths = 5, cbar = False, annot_kws = {'fontsize': 15}, \n            yticklabels = ['Healthy', 'Diabetic'], xticklabels = ['Predicted Healthy', 'Predicted Diabetic'])\nplt.yticks(rotation = 0)\nplt.show()\n\n# Roc AUC Curve\nfalse_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_prob)\nroc_auc = auc(false_positive_rate, true_positive_rate)\n\nsns.set_theme(style = 'white')\nplt.figure(figsize = (6, 6))\nplt.plot(false_positive_rate,true_positive_rate, color = '#b01717', label = 'AUC = %0.3f' % roc_auc)\nplt.legend(loc = 'lower right')\nplt.plot([0, 1], [0, 1], linestyle = '--', color = '#174ab0')\nplt.axis('tight')\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.title('ROC AUC Curve')\nplt.legend()\nplt.show()\n\n#Precision Recall Curve\naverage_precision = average_precision_score(y_test, y_prob)\ndisp = plot_precision_recall_curve(classifier, x_test, y_test)\nplt.title('Precision-Recall Curve')\nplt.show()","041c1b8f":"#Fitting RandomForestClassifier Model\nclassifier = RandomForestClassifier(criterion= 'entropy', n_estimators= 200, random_state= 0)\nclassifier.fit(x_train, y_train)\ny_pred = classifier.predict(x_test)\ny_prob = classifier.predict_proba(x_test)[:,1]\ncm = confusion_matrix(y_test, y_pred)\n\nprint(classification_report(y_test, y_pred))\nprint(f'ROC AUC score: {roc_auc_score(y_test, y_prob)}')\nprint('Accuracy Score: ',accuracy_score(y_test, y_pred))\n\n# Visualizing Confusion Matrix\nplt.figure(figsize = (6, 6))\nsns.heatmap(cm, cmap = 'Blues', annot = True, fmt = 'd', linewidths = 5, cbar = False, annot_kws = {'fontsize': 15}, \n            yticklabels = ['Healthy', 'Diabetic'], xticklabels = ['Predicted Healthy', 'Predicted Diabetic'])\nplt.yticks(rotation = 0)\nplt.show()\n\n# Roc AUC Curve\nfalse_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_prob)\nroc_auc = auc(false_positive_rate, true_positive_rate)\n\nsns.set_theme(style = 'white')\nplt.figure(figsize = (6, 6))\nplt.plot(false_positive_rate,true_positive_rate, color = '#b01717', label = 'AUC = %0.3f' % roc_auc)\nplt.legend(loc = 'lower right')\nplt.plot([0, 1], [0, 1], linestyle = '--', color = '#174ab0')\nplt.axis('tight')\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.title('ROC AUC Curve')\nplt.legend()\nplt.show()\n\n#Precision Recall Curve\naverage_precision = average_precision_score(y_test, y_prob)\ndisp = plot_precision_recall_curve(classifier, x_test, y_test)\nplt.title('Precision-Recall Curve')\nplt.show()","428c00f4":"#Fitting ExtraTreesClassifier Model\nclassifier = ExtraTreesClassifier(criterion= 'gini', n_estimators= 100, random_state= 0)\nclassifier.fit(x_train, y_train)\ny_pred = classifier.predict(x_test)\ny_prob = classifier.predict_proba(x_test)[:,1]\ncm = confusion_matrix(y_test, y_pred)\n\nprint(classification_report(y_test, y_pred))\nprint(f'ROC AUC score: {roc_auc_score(y_test, y_prob)}')\nprint('Accuracy Score: ',accuracy_score(y_test, y_pred))\n\n# Visualizing Confusion Matrix\nplt.figure(figsize = (6, 6))\nsns.heatmap(cm, cmap = 'Blues', annot = True, fmt = 'd', linewidths = 5, cbar = False, annot_kws = {'fontsize': 15}, \n            yticklabels = ['Healthy', 'Diabetic'], xticklabels = ['Predicted Healthy', 'Predicted Diabetic'])\nplt.yticks(rotation = 0)\nplt.show()\n\n# Roc AUC Curve\nfalse_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_prob)\nroc_auc = auc(false_positive_rate, true_positive_rate)\n\nsns.set_theme(style = 'white')\nplt.figure(figsize = (6, 6))\nplt.plot(false_positive_rate,true_positive_rate, color = '#b01717', label = 'AUC = %0.3f' % roc_auc)\nplt.legend(loc = 'lower right')\nplt.plot([0, 1], [0, 1], linestyle = '--', color = '#174ab0')\nplt.axis('tight')\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.title('ROC AUC Curve')\nplt.legend()\nplt.show()\n\n#Precision Recall Curve\naverage_precision = average_precision_score(y_test, y_prob)\ndisp = plot_precision_recall_curve(classifier, x_test, y_test)\nplt.title('Precision-Recall Curve')\nplt.show()","4c246450":"#Fitting AdaBoostClassifier Model\nclassifier = AdaBoostClassifier(algorithm= 'SAMME', learning_rate= 0.1, n_estimators= 100, random_state= 0)\nclassifier.fit(x_train, y_train)\ny_pred = classifier.predict(x_test)\ny_prob = classifier.predict_proba(x_test)[:,1]\ncm = confusion_matrix(y_test, y_pred)\n\nprint(classification_report(y_test, y_pred))\nprint(f'ROC AUC score: {roc_auc_score(y_test, y_prob)}')\nprint('Accuracy Score: ',accuracy_score(y_test, y_pred))\n\n# Visualizing Confusion Matrix\nplt.figure(figsize = (6, 6))\nsns.heatmap(cm, cmap = 'Blues', annot = True, fmt = 'd', linewidths = 5, cbar = False, annot_kws = {'fontsize': 15}, \n            yticklabels = ['Healthy', 'Diabetic'], xticklabels = ['Predicted Healthy', 'Predicted Diabetic'])\nplt.yticks(rotation = 0)\nplt.show()\n\n# Roc AUC Curve\nfalse_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_prob)\nroc_auc = auc(false_positive_rate, true_positive_rate)\n\nsns.set_theme(style = 'white')\nplt.figure(figsize = (6, 6))\nplt.plot(false_positive_rate,true_positive_rate, color = '#b01717', label = 'AUC = %0.3f' % roc_auc)\nplt.legend(loc = 'lower right')\nplt.plot([0, 1], [0, 1], linestyle = '--', color = '#174ab0')\nplt.axis('tight')\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.title('ROC AUC Curve')\nplt.legend()\nplt.show()\n\n#Precision Recall Curve\naverage_precision = average_precision_score(y_test, y_prob)\ndisp = plot_precision_recall_curve(classifier, x_test, y_test)\nplt.title('Precision-Recall Curve')\nplt.show()","707f2a40":"#Fitting GradientBoostingClassifier Model\nclassifier = GradientBoostingClassifier(criterion= 'mse', learning_rate= 0.1, loss= 'exponential', n_estimators= 100, random_state= 0)\nclassifier.fit(x_train, y_train)\ny_pred = classifier.predict(x_test)\ny_prob = classifier.predict_proba(x_test)[:,1]\ncm = confusion_matrix(y_test, y_pred)\n\nprint(classification_report(y_test, y_pred))\nprint(f'ROC AUC score: {roc_auc_score(y_test, y_prob)}')\nprint('Accuracy Score: ',accuracy_score(y_test, y_pred))\n\n# Visualizing Confusion Matrix\nplt.figure(figsize = (6, 6))\nsns.heatmap(cm, cmap = 'Blues', annot = True, fmt = 'd', linewidths = 5, cbar = False, annot_kws = {'fontsize': 15}, \n            yticklabels = ['Healthy', 'Diabetic'], xticklabels = ['Predicted Healthy', 'Predicted Diabetic'])\nplt.yticks(rotation = 0)\nplt.show()\n\n# Roc AUC Curve\nfalse_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_prob)\nroc_auc = auc(false_positive_rate, true_positive_rate)\n\nsns.set_theme(style = 'white')\nplt.figure(figsize = (6, 6))\nplt.plot(false_positive_rate,true_positive_rate, color = '#b01717', label = 'AUC = %0.3f' % roc_auc)\nplt.legend(loc = 'lower right')\nplt.plot([0, 1], [0, 1], linestyle = '--', color = '#174ab0')\nplt.axis('tight')\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.title('ROC AUC Curve')\nplt.legend()\nplt.show()\n\n#Precision Recall Curve\naverage_precision = average_precision_score(y_test, y_prob)\ndisp = plot_precision_recall_curve(classifier, x_test, y_test)\nplt.title('Precision-Recall Curve')\nplt.show()","a2a536ca":"#Fitting LGBMClassifier Model\nclassifier = LGBMClassifier(learning_rate= 0.1, n_estimators= 100, random_state= 0)\nclassifier.fit(x_train, y_train)\ny_pred = classifier.predict(x_test)\ny_prob = classifier.predict_proba(x_test)[:,1]\ncm = confusion_matrix(y_test, y_pred)\n\nprint(classification_report(y_test, y_pred))\nprint(f'ROC AUC score: {roc_auc_score(y_test, y_prob)}')\nprint('Accuracy Score: ',accuracy_score(y_test, y_pred))\n\n# Visualizing Confusion Matrix\nplt.figure(figsize = (6, 6))\nsns.heatmap(cm, cmap = 'Blues', annot = True, fmt = 'd', linewidths = 5, cbar = False, annot_kws = {'fontsize': 15}, \n            yticklabels = ['Healthy', 'Diabetic'], xticklabels = ['Predicted Healthy', 'Predicted Diabetic'])\nplt.yticks(rotation = 0)\nplt.show()\n\n# Roc AUC Curve\nfalse_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_prob)\nroc_auc = auc(false_positive_rate, true_positive_rate)\n\nsns.set_theme(style = 'white')\nplt.figure(figsize = (6, 6))\nplt.plot(false_positive_rate,true_positive_rate, color = '#b01717', label = 'AUC = %0.3f' % roc_auc)\nplt.legend(loc = 'lower right')\nplt.plot([0, 1], [0, 1], linestyle = '--', color = '#174ab0')\nplt.axis('tight')\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.title('ROC AUC Curve')\nplt.legend()\nplt.show()\n\n#Precision Recall Curve\naverage_precision = average_precision_score(y_test, y_prob)\ndisp = plot_precision_recall_curve(classifier, x_test, y_test)\nplt.title('Precision-Recall Curve')\nplt.show()","a4344fc0":"#Fitting XGBClassifier Model\nclassifier = XGBClassifier(eval_metric= 'error', learning_rate= 0.1)\nclassifier.fit(x_train, y_train)\ny_pred = classifier.predict(x_test)\ny_prob = classifier.predict_proba(x_test)[:,1]\ncm = confusion_matrix(y_test, y_pred)\n\nprint(classification_report(y_test, y_pred))\nprint(f'ROC AUC score: {roc_auc_score(y_test, y_prob)}')\nprint('Accuracy Score: ',accuracy_score(y_test, y_pred))\n\n# Visualizing Confusion Matrix\nplt.figure(figsize = (6, 6))\nsns.heatmap(cm, cmap = 'Blues', annot = True, fmt = 'd', linewidths = 5, cbar = False, annot_kws = {'fontsize': 15}, \n            yticklabels = ['Healthy', 'Diabetic'], xticklabels = ['Predicted Healthy', 'Predicted Diabetic'])\nplt.yticks(rotation = 0)\nplt.show()\n\n# Roc AUC Curve\nfalse_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_prob)\nroc_auc = auc(false_positive_rate, true_positive_rate)\n\nsns.set_theme(style = 'white')\nplt.figure(figsize = (6, 6))\nplt.plot(false_positive_rate,true_positive_rate, color = '#b01717', label = 'AUC = %0.3f' % roc_auc)\nplt.legend(loc = 'lower right')\nplt.plot([0, 1], [0, 1], linestyle = '--', color = '#174ab0')\nplt.axis('tight')\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.title('ROC AUC Curve')\nplt.legend()\nplt.show()\n\n#Precision Recall Curve\naverage_precision = average_precision_score(y_test, y_prob)\ndisp = plot_precision_recall_curve(classifier, x_test, y_test)\nplt.title('Precision-Recall Curve')\nplt.show()","e4388d9a":"### BMI <a id='3.4.6'><\/a>","dd85ccf4":"<div style=\"color:white;\n           display:fill;\n           border-radius:5px;\n           background-color:LightSlateGray;\n           font-size:150%;\n           text-align:center;\n            letter-spacing:0.5px\"> <a id='1'><\/a>\n    \n    Importing Libraries \n<\/div>","8cc6b315":"\ud83d\udccc An outlier is an observation that lies an abnormal distance from other values in a random sample from a population. <br>\n\n\ud83d\udccc In this notebook, we are using ***Box Plot*** to detect the outliers of each features in our dataset, where any point above or below the whiskers represent an outlier. This is also known as \u201c***Univariate method***\u201d as here we are using one variable outlier analysis. <br>\n\n\ud83d\udccc It is represented by the formula ***IQR = Q3 \u2212 Q1***. The lines of code below calculate and print the interquartile range for each of the variables in the dataset. The above output prints the IQR scores, which can be used to detect outliers. <br>\n\n\ud83d\udccc After detecting, we are using ***Median Imputation*** to take care of outliers. In this technique, we replace the extreme values with median values. It is advised to not use mean values as they are affected by outliers. ","65d957b3":"<div style=\"color:white;\n           display:fill;\n           border-radius:5px;\n           background-color:LightSlateGray;\n           font-size:150%;\n           text-align:center;\n           letter-spacing:0.5px\"> <a id = '9'><\/a>\n\n    Models after Tuning Hyperparameters \n<\/div>","ca34695b":"## **AdaBoost** <a id='9.9'><\/a>","4f583a61":"### Pregnancy <a id='3.4.1'><\/a>","f771b52c":"### Healthy vs Diabetic by Glucose <a id='3.3.2'><\/a>","d41f6df8":"### Healthy vs Diabetic by Insulin <a id='3.3.5'><\/a>","12e80a2e":"## **Outliers - Detecting and Removing** <a id='3.4'><\/a>","772d6a2e":"\ud83d\udccc Check if there are any NULL values.","35cb0358":"\ud83d\udccc The ***GridSearchCV*** is a library function that is a member of sklearn's model_selection package. It helps to loop through predefined hyperparameters and fit your estimator (model) on your training set. So, in the end, you can select the best parameters from the listed hyperparameters.","0d1cdd70":"\ud83d\udccc Being overweight (BMI of 25-29.9), or affected by obesity (BMI of 30-39.9) or morbid obesity (BMI of 40 or greater), greatly increases your risk of developing type 2 diabetes. The more excess weight you have, the more resistant your muscle and tissue cells become to your own insulin hormone. <br>\n\n\ud83d\udccc From above graph we can determine that, as the BMI increases the person likely being healthy decreases and being diabetic increases. ","72efa683":"<div style=\"color:white;\n           display:fill;\n           border-radius:5px;\n           background-color:LightSlateGray;\n           font-size:150%;\n           text-align:center;\n           letter-spacing:0.5px\"> <a id = '5'><\/a>\n    \n    Splitting Data into Train and Test Set \n<\/div>","63db3504":"\ud83d\udccc Diabetes Pedigree Function is a function which scores likelihood of diabetes based on family history. It provided some data on diabetes mellitus history in relatives and the genetic relationship of those relatives to the patient. <br>\n\n\ud83d\udccc From above graph, as thefunction increase the diabetic people increases, showing that the diabetes could be hereditary for that individual.","e56437fc":"## **KNeighbors** <a id='9.2'><\/a>","4c3aef43":"### Healthy vs Diabetic by Pregnancy <a id='3.3.1'><\/a>","baf928c9":"### **Table of Contents:**\n1. [Importing Libraries](#1)<a href='1' ><\/a> <br>\n2. [Importing Dataset](#2)<a href='2' ><\/a> <br>\n3. [Expolatory Data Analysis](#3)<a href='3' ><\/a> <br>\n    3.1. [Heat Map Correlation](#3.1)<a href='3.1' ><\/a> <br>\n    3.2. [Pie Chart](#3.2)<a href='3.2' ><\/a> <br>\n    3.3. [Distribution Plot](#3.3)<a href='3.3' ><\/a> <br>\n    &nbsp;&nbsp;&nbsp;&nbsp; a. [Healthy vs Diabetic by Pregnancy](#3.3.1)<a href='3.3.1' ><\/a> <br>\n    &nbsp;&nbsp;&nbsp;&nbsp; b. [Healthy vs Diabetic by Glucose](#3.3.2)<a href='3.3.2' ><\/a> <br>\n    &nbsp;&nbsp;&nbsp;&nbsp; c. [Healthy vs Diabetic by Blood Pressure](#3.3.3)<a href='3.3.3' ><\/a> <br>\n    &nbsp;&nbsp;&nbsp;&nbsp; d. [Healthy vs Diabetic by Skin Thickness](#3.3.4)<a href='3.3.4' ><\/a> <br>\n    &nbsp;&nbsp;&nbsp;&nbsp; e. [Healthy vs Diabetic by Insulin](#3.3.5)<a href='3.3.5' ><\/a><br>\n    &nbsp;&nbsp;&nbsp;&nbsp; f. &nbsp;[Healthy vs Diabetic by BMI](#3.3.6)<a href='3.3.6' ><\/a> <br>\n    &nbsp;&nbsp;&nbsp;&nbsp; g. [Healthy vs Diabetic by Diabetes Pedigree Function](#3.3.7)<a href='3.3.7' ><\/a> <br>\n    &nbsp;&nbsp;&nbsp;&nbsp; h. [Healthy vs Diabetic by Age](#3.3.8)<a href='3.3.8' ><\/a> <br>\n    3.4. [Outliers - Detecting and Removing](#3.4)<a href='3.4' ><\/a> <br>\n    &nbsp;&nbsp;&nbsp;&nbsp; a. [Pregnancy](#3.4.1)<a href='3.4.1' ><\/a> <br>\n    &nbsp;&nbsp;&nbsp;&nbsp; b. [Glucose](#3.4.2)<a href='3.4.2' ><\/a> <br>\n    &nbsp;&nbsp;&nbsp;&nbsp; c. [Blood Pressure](#3.4.3)<a href='3.4.3' ><\/a> <br>\n    &nbsp;&nbsp;&nbsp;&nbsp; d. [Skin Thickness](#3.4.4)<a href='3.4.4' ><\/a> <br>\n    &nbsp;&nbsp;&nbsp;&nbsp; e. [Insulin](#3.4.5)<a href='3.4.5' ><\/a> <br>\n    &nbsp;&nbsp;&nbsp;&nbsp; f. &nbsp;[BMI](#3.4.6)<a href='3.4.6' ><\/a> <br>\n    &nbsp;&nbsp;&nbsp;&nbsp; g. [Diabetes Pedigree Function](#3.4.7)<a href='3.4.7' ><\/a> <br>\n    &nbsp;&nbsp;&nbsp;&nbsp; h. [Age](#3.4.8)<a href='3.4.8' ><\/a> <br>\n    3.4. [Pair Plot](#3.5)<a href='3.5' ><\/a> <br>\n4. [Data Preprocessing](#4)<a href='4' ><\/a> <br> \n5. [Splitting Data into Train and Test Set](#5)<a href='5' ><\/a> <br>\n6. [Feature Scaling](#6)<a href='6' ><\/a> <br>\n7. [Model Selection](#7)<a href='7' ><\/a> <br>\n8. [Tuning the Models](#8)<a href='8' ><\/a> <br>\n9. [Models after Tuning Hyperparameters](#9)<a href='9' ><\/a> <br>\n    9.1. [Logistic Regression](#9.1)<a href='9.1' ><\/a> <br>\n    9.2. [Kneighbors](#9.2)<a href='9.2' ><\/a> <br>\n    9.3. [SVC](#9.3)<a href='9.3' ><\/a> <br> \n    9.4. [GaussianNB](#9.4)<a href='9.4' ><\/a> <br>\n    9.5. [BernoulliNB](#9.5)<a href='9.5' ><\/a> <br>\n    9.6. [Decision Tree](#9.6)<a href='9.6' ><\/a> <br>\n    9.7. [Random Forest](#9.7)<a href='9.7' ><\/a> <br>\n    9.8. [Extra Trees](#9.8)<a href='9.8' ><\/a> <br>\n    9.9. [AdaBoost](#9.9)<a href='9.9' ><\/a> <br>\n    9.10. [Gradient Boost](#9.10)<a href='9.10' ><\/a> <br>\n    9.11. [LightGBM](#9.11)<a href='9.11' ><\/a> <br>\n    9.12. [XGBoost](#9.12)<a href='9.12' ><\/a> <br>\n10. [Conclusion](#10)<a href='10' ><\/a> <br>    ","6cde09b5":"### Healthy vs Diabetic by Diabetes Pedigree Function <a id='3.3.7'><\/a>","471e4b4f":"### Diabetes Pedigree Function <a id='3.4.7'><\/a>","6de5262f":"<div style=\"color:white;\n           display:fill;\n           border-radius:5px;\n           background-color:LightSlateGray;\n           font-size:150%;\n           text-align:center;\n           letter-spacing:0.5px\"> <a id = '7'><\/a>\n\n    Model Selection \n<\/div>","b24dc699":"\ud83d\udccc As the person ages, they are at high risk for the development of type 2 diabetes due to the combined effects of increasing insulin resistance and impaired pancreatic islet function with aging. <br>\n\n\ud83d\udccc From above graph, we can see that there are more healthy people around 20-25 age but as the age gradually increases so does the people being diabetic, this shows that age and diabetes go hand in hand.","ac7aa316":"### Glucose <a id='3.4.2'><\/a>","cf92346f":"<div style=\"color:black;\n           display:fill;\n           border-radius:5px;\n           border:2px solid DodgerBlue;\n           background-color:white;\n           font-size:150%;\n           text-align:center;\n           letter-spacing:0.5px\">\n    \n    Thank You!\n<\/div>","a19345bb":"## **Decision Tree** <a id='9.6'><\/a>","a9c534d6":"<div style=\"color:white;\n           display:fill;\n           border-radius:5px;\n           background-color:LightSlateGray;\n           font-size:150%;\n           text-align:center;\n           letter-spacing:0.5px\"> <a id = '8'><\/a>\n\n    Tuning the Models \n<\/div>","716ae3b2":"<div style=\"color:white;\n           display:fill;\n           border-radius:5px;\n           background-color:LightSlateGray;\n           font-size:150%;\n           text-align:center;\n           letter-spacing:0.5px\"> <a id = '4'><\/a>\n    \n    Data Preprocessing \n<\/div>","bd0ab9f6":"\ud83d\udccc Diabetes is a disease that occurs when your blood glucose, also called blood sugar, is too high. Blood glucose is your main source of energy and comes from the food you eat. <br>\n\n\ud83d\udccc The Glucose level for a Normal Adult is around 120-130mg\/dl anything above it means that the person is likely suffering from pre-diabetes and diabetes. <br>\n\n\ud83d\udccc From above graph, we can see the the Healthy person are more around 120mg\/dl but it then gradually drops, and for diabetic person it is vice versa.","e288a538":"\ud83d\udccc After extensive data analysis and I tried different classification models to see how it performs on the dataset. I got pretty good results with accuracy, roc, precision and recall score. <br>\n\n\ud83d\udccc But, I didn't stop there, after that I tuned the hyperparamters with the help of Grid Search and saw the classification report with ROC AUC and Precision-Recall curve of different models. <br>\n\n\ud83d\udccc With that, I came to conclusion that ***RandomForest***, ***ExtraTrees*** and ***SVC*** are models which are best fit for our dataset. <br>","2d8ba8f6":"### Healthy vs Diabetic by Age <a id='3.3.8'><\/a>","7eaba359":"### Blood Pressure <a id='3.4.3'><\/a>","40fdfb7e":"## **Distribution Plot** <a id='3.3'><\/a>","97c59033":"\ud83d\udccc After Grid Search, we got best parameters for all the models. Now, we going to tune hyperparameters see how to it perform. <br>\n\n\ud83d\udccc ***True Positives (TP)*** - These are the correctly predicted positive values which means that the value of actual class is yes and the value of predicted class is also yes. <br>\n\n\ud83d\udccc ***True Negatives (TN)*** - These are the correctly predicted negative values which means that the value of actual class is no and value of predicted class is also no. <br>\n\n\ud83d\udccc ***False Positives (FP)*** \u2013 When actual class is no and predicted class is yes. <br>\n\n\ud83d\udccc ***False Negatives (FN)*** \u2013 When actual class is yes but predicted class in no. <br>\n\n\ud83d\udccc ***Accuracy*** - Accuracy is the most intuitive performance measure and it is simply a ratio of correctly predicted observation to the total observations. <br>\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**Accuracy = TP+TN\/TP+FP+FN+TN** <br>\n\n\ud83d\udccc ***Precision*** - Precision is the ratio of correctly predicted positive observations to the total predicted positive observations. <br>\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**Precision = TP\/TP+FP** <br>\n\n\ud83d\udccc ***Recall (Sensitivity)*** - Recall is the ratio of correctly predicted positive observations to the all observations in actual class - yes. <br>\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**Recall = TP\/TP+FN** <br>\n\n\ud83d\udccc ***F1 score*** - F1 Score is the weighted average of Precision and Recall. Therefore, this score takes both false positives and false negatives into account. <br>\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**F1 Score = 2*(Recall * Precision) \/ (Recall + Precision)** <br>\n\n\ud83d\udccc ***Support*** - Support is the number of actual occurrences of the class in the specified dataset. Support doesn\u2019t change between models but instead diagnoses the evaluation process. ","811643c3":"## **SVC** <a id='9.3'><\/a>","885a1df3":"<div style=\"color:white;\n           display:fill;\n           border-radius:5px;\n           background-color:LightSlateGray;\n           font-size:150%;\n           text-align:center;\n           letter-spacing:0.5px\"> <a id = '6'><\/a>\n    \n    Feature Scaling \n<\/div>","4d212b91":"\ud83d\udccc Changes to the blood vessels because of diabetes can cause a skin condition called diabetic dermopathy. Dermopathy appears as scaly patches that are light brown or red, often on the front of the legs. The patches do not hurt, blister, or itch, and treatment generally is not necessary. <br>\n\n\ud83d\udccc From above graph, the distribution between healthy and diabetic people are around same for skin thickness.","c842e2b7":"### Healthy vs Diabetic by Blood Pressure <a id='3.3.3'><\/a>","4eb759ef":"\ud83d\udccc From above graph, we can say that the Pregnancy isn't likely cause for diabetes as the distribution between the Healthy and Diabetic is almost same. ","cb3beac7":"<div style=\"color:white;\n           display:fill;\n           border-radius:5px;\n           background-color:LightSlateGray;\n           font-size:150%;\n           text-align:center;\n           letter-spacing:0.5px\"> <a id = '2'><\/a>\n    \n    Importing Dataset \n<\/div>","24d354a3":"## **BernoulliNB** <a id='9.5'><\/a>","300986b0":"\ud83d\udccc From above all the graphs, I took care of all the outliers present in the dataset. ","72fc6335":"### Insulin <a id='3.4.5'><\/a>","0785b11d":"\ud83d\udccc Insulin is a hormone that your pancreas makes to allow cells to use glucose. When your body isn't making or using insulin correctly, you can take man-made insulin to help control your blood sugar. Many types can be used to treat diabetes. <br>\n\n\ud83d\udccc Insulin helps control blood glucose levels by signaling the liver and muscle and fat cells to take in glucose from the blood. Insulin therefore helps cells to take in glucose to be used for energy. If the body has sufficient energy, insulin signals the liver to take up glucose and store it as glycogen. <br>\n\n\ud83d\udccc From above graph, we can see that there are diabetic people increase as the levels of insulin gradually increases. There are more healthy people around insulin levels 0-100.","1d28a0ad":"## **LightGBM** <a id='9.11'><\/a>","fd7c0aea":"## **Heat Map Correlation** <a id='3.1' ><\/a>","6ab06f05":"\ud83d\udccc StandardScaler standardizes a feature by subtracting the mean and then scaling to unit variance. Unit variance means dividing all the values by the standard deviation. StandardScaler results in a distribution with a standard deviation equal to 1.","31fefbc0":"<div style=\"color:white;\n           display:fill;\n           border-radius:5px;\n           background-color:LightSlateGray;\n           font-size:150%;\n           text-align:center;\n           letter-spacing:0.5px\">\n    \n     Welcome\n<\/div>","aa0e9aad":"## **GaussianNB** <a id='9.4'><\/a>","8ef7f133":"### Healthy vs Diabetic by Skin Thickness <a id='3.3.4'><\/a>","793c0355":"## **Logistic Regression** <a id='9.1'><\/a>","d5e1ba41":"## **Random Forest** <a id='9.7'><\/a>","0d0c210d":"\ud83d\udccc From the above graphs, we can see that the outliers in our dataset have been taken care of. ","b065ad5f":"\ud83d\udccc High blood pressure (also known as \u201chypertension\u201d) is very common in people with diabetes. In fact, the two conditions often go hand-in-hand because they can both result from the same lifestyle factors. <br>\n\n\ud83d\udccc Diabetes damages arteries and makes them targets for hardening, called atherosclerosis. That can cause high blood pressure, which if not treated, can lead to trouble including blood vessel damage, heart attack, and kidney failure. <br>\n\n\ud83d\udccc For a Normal person BP should be at or below 120\/80 mm Hg, the person with hypertension can be above 139\/89 mm Hg. <br>\n\n\ud83d\udccc From above graph, we can say that, diabetic and healthy people are evenly distributed with low and normal BP but, there are less healthy people who have high BP. ","f2f0cfb7":"<div style=\"color:white;\n           display:fill;\n           border-radius:5px;\n           background-color:LightSlateGray;\n           font-size:150%;\n           text-align:center;\n           letter-spacing:0.5px\"> <a id = '10'><\/a>\n\n    Conclusion\n<\/div>","b3f9c6f3":"\ud83d\udccc We are using different classification models to determine Accuracy, K-Fold Validation, ROC AUC, Precision, Recall and F1 score.","e5c399e9":"## **Extra Trees** <a id='9.8'><\/a>","8848f2e3":"<div style=\"color:black;\n           display:fill;\n           border-radius:5px;\n           background-color:Beige;\n           font-size:110%;\n           letter-spacing:0.5px\">\n\n<p style=\"padding: 10px;\n              color:black;\">\n    \nHello Kagglers, <br>\n\nIn this notebook, I am going to predict whether an individual is healthy or diabetic. But, first I am going to do exploratory data analysis on our dataset and learn what is the common trait of people who has diabetes. Then, I am going to use multiple classification models on our dataset and select the best performing one. \n<\/p>\n<\/div>   ","5a6b7803":"### Healthy vs Diabetic by BMI <a id='3.3.6'><\/a>","ec3c3171":"\ud83d\udccc From above we can determine, RandomForest seems to have high accuracy but the standard deviation is high and its ROC AUC score also is high. <br>\n\n\ud83d\udccc But, we will also tune the model with Grid Search to determine best parameters for different models and to also increase its overall scores. ","aab3f5c9":"<div style=\"color:white;\n           display:fill;\n           border-radius:5px;\n           background-color:LightSlateGray;\n           font-size:150%;\n           text-align:center;\n           letter-spacing:0.5px\"> <a id = '3'><\/a>\n\n    Exploratory Data Analysis \n<\/div>","9fa2c84b":"## **Pie Chart** <a id='3.2' ><\/a>","a3fa3e17":"### **Pair Plot** <a id='3.5'><\/a>","94b0c67e":"\ud83d\udccc From above pie chart, we can say that around 65% of the people are Healthy and 35% are Diabetic ","f41a0b75":"## **Gradient Boost** <a id='9.10'><\/a>","5077d559":"### Age <a id='3.4.8'><\/a>","818a4515":"## **XGBoost** <a id='9.12'><\/a>","910c3015":"### Skin Thickness <a id='3.4.4'><\/a>"}}