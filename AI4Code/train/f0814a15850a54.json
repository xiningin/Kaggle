{"cell_type":{"d3872ba5":"code","f5cb3be1":"code","a7ef3f53":"code","63a498b8":"code","522d8173":"code","c6957f2c":"code","ab04a0cb":"code","8a4361cb":"code","c0d30035":"code","761ab7dd":"code","e6161eef":"code","1e21a8f8":"code","e50a36a4":"code","c19a9286":"code","4288c922":"code","6a28aebc":"code","71e490b5":"code","5c2325a1":"code","98403acd":"code","bae9375a":"code","3770f456":"markdown","f123bbb2":"markdown","05158cfb":"markdown","c28ea92b":"markdown","f80b651d":"markdown","e8dad098":"markdown","0f24f0ae":"markdown","3d57c268":"markdown","2c02733a":"markdown","dfa18d6e":"markdown","4d7638eb":"markdown","3de56662":"markdown","120007be":"markdown","5a886d2e":"markdown","d37f3092":"markdown","fc0b27b2":"markdown","8c5d6603":"markdown","79808806":"markdown","9a27a148":"markdown"},"source":{"d3872ba5":"##      \/\\___\/\\\n##     \/       \\\n##    l  u   u  l\n##  --l----*----l--\n##     \\   w   \/     - Meow!\n##       ======\n##     \/        \\ __\n##     l        l\\ \\\n##     l        l\/ \/ \n##     l  l l   l \/\n##     \\ ml lm \/_\/","f5cb3be1":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n#booster\nimport catboost as cb\n\n#preprocess\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import GridSearchCV\n\n# metrics\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import mean_squared_error","a7ef3f53":"train_df = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/train.csv\")\ntest_df = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/test.csv\")","63a498b8":"# select only num_features\nnumeric_features=[\n    'SalePrice',\n    'OverallQual', # I use this as numeric\n    'OverallCond', # numeric\n    'YearBuilt',\n    'MasVnrArea',\n    'TotalBsmtSF',\n    '1stFlrSF',\n    '2ndFlrSF',\n    'GrLivArea',\n    'GarageArea',\n    'GarageCars',\n    'GarageYrBlt',\n    'PoolArea',\n    'MiscVal',\n    'LotArea'\n]\n    \n    \n#looking on correlation with Price\ncorr_mx = train_df[numeric_features].corr()\nfig,ax=plt.subplots(figsize=(18,10))\nax=sns.heatmap(corr_mx, annot=True)\nplt.show()","522d8173":"numeric_train_features = [\n    'OverallQual',\n    'YearBuilt',\n    'MasVnrArea',\n    'TotalBsmtSF',\n    'GrLivArea',\n    'GarageCars',\n    '1stFlrSF',\n    '2ndFlrSF',\n    'LotArea',\n    'YearRemodAdd',\n    'LotFrontage'\n]\n\ncategorical_features = [\n    'OverallCond'\n]","c6957f2c":"bad_features=[\n    'Id', # no effect on price\n    'MSSubClass',\n    'Alley',\n    'LotShape',\n    'LandContour',\n    'LandSlope',\n    'Neighborhood', #no, bad feature!\n    'RoofStyle',\n    'Exterior2nd',\n    'MasVnrType', # will use area instead\n    'BsmtFinType2',\n    'BsmtFinSF2',\n    'BsmtFinSF1', # bad correlation\n    'LowQualFinSF',\n    'BsmtFullBath',\n    'BsmtHalfBath',\n    'Fireplaces',\n    'FireplaceQu',\n    'GarageFinish',\n    'GarageCond',\n    'PavedDrive',\n    'WoodDeckSF',\n    'OpenPorchSF',\n    'EnclosedPorch',\n    '3SsnPorch',\n    'ScreenPorch',\n    'PoolQC',\n    'Fence',\n    'MoSold', # no corr\n    'YrSold',\n    'Street',\n    'HouseStyle',\n    'ExterCond',\n    'BsmtUnfSF',\n    'PoolArea', # bad cor\n    'MiscVal',\n    'GarageArea',\n    'GarageYrBlt'\n]\nlen(bad_features)","ab04a0cb":"# exclude target array\ntarget = train_df['SalePrice']\n\n#will drop bad features\ntrain_data=train_df.drop(bad_features, axis=1)\ntrain_data = train_data.drop('SalePrice', axis=1)\n\ntest_data = test_df.drop(bad_features, axis=1)\n\n\n# lets make categorical features list now\ncategorical_features = []\nfor feat in train_data.columns:\n    if feat not in numeric_train_features:\n        categorical_features.append(feat)","8a4361cb":"print('Numeric features:', len(numeric_train_features))\nprint('Categorical features:', len(categorical_features))","c0d30035":"# look at those LOT of NaNs!\ntest_data.isna().sum()","761ab7dd":"# we will fill both train and test dataframes with average values: median for numeric, average value for categories\nfor df in [train_data, test_data]:\n    df['LotFrontage'].fillna(df['LotFrontage'].median(), inplace=True)\n    df['MasVnrArea'].fillna(df['MasVnrArea'].median(), inplace=True)\n    df['BsmtQual'].fillna('TA', inplace=True)\n    df['BsmtCond'].fillna('TA', inplace=True)\n    df['BsmtExposure'].fillna('Av', inplace=True)\n    df['BsmtFinType1'].fillna('ALQ', inplace=True)\n    df['Electrical'].fillna('Sbrkr', inplace=True)\n    df['GarageType'].fillna('NA', inplace=True)\n    df['GarageQual'].fillna('NA', inplace=True)\n    df['MiscFeature'].fillna('NA', inplace=True)\n    df['MSZoning'].fillna('C', inplace=True)\n    df['Utilities'].fillna('NoSeWa', inplace=True)\n    df['KitchenQual'].fillna('TA', inplace=True)\n    df['Functional'].fillna('Mod', inplace=True)\n    df['TotalBsmtSF'].fillna(test_df['TotalBsmtSF'].median(), inplace=True)\n    df['SaleType'].fillna('CWD', inplace=True)\n    df['Exterior1st'].fillna('Other', inplace=True)\n    df['GarageCars'].fillna(1,inplace=True)","e6161eef":"print('NaNs in train:', train_data.isna().sum().sum(axis=0))\n\nprint('NaNs in test:', test_data.isna().sum().sum(axis=0))","1e21a8f8":"scaler = StandardScaler()\nscaler.fit(train_data[numeric_train_features])\nprint('Before scaler:')\nprint(train_data[numeric_train_features[:5]].head())\n\ntrain_data[numeric_train_features] = scaler.transform(train_data[numeric_train_features])\ntest_data[numeric_train_features] = scaler.transform(test_data[numeric_train_features])\n\nprint('After scaler:')\nprint(train_data[numeric_train_features[:5]].head())","e50a36a4":"print('Features train:', train_data.shape)\nprint('Target train:', target.shape)\nprint('Features test:', test_data.shape)","c19a9286":"print(train_df['SalePrice'].describe())\n\nfig, ax = plt.subplots(figsize=(12,8))\nax=sns.distplot(target)\nax.set_title('Sale Price distribution')\nplt.show()","4288c922":"pred_median = np.full(target.shape, target.median())\nconst_rmse = (mean_squared_error(target, pred_median))**0.5\nprint('Constant model RMSE score:', const_rmse)","6a28aebc":"# converting to cb.Pools\ntrain_pool = cb.Pool(train_data,\n                     target,\n                     cat_features = categorical_features)\n\ntest_pool = cb.Pool(test_data, cat_features=categorical_features)","71e490b5":"def CB_cv_grid():\n    cb_m = cb.CatBoostRegressor(random_state=112211)\n\n    param_grid = {\n            \"iterations\"        : [1000, 2000],\n            \"learning_rate\"     : [0.01,0.005],\n            \"depth\" : [4,6],\n            \"loss_function\": ['RMSE'],\n            \"bootstrap_type\": ['No', 'Bayesian'],\n            \"verbose\":[False],\n            \n            }\n\n    cb_grid = GridSearchCV(cb_m, param_grid, n_jobs=-1, cv=5, scoring='neg_root_mean_squared_error')\n    cb_grid.fit(train_data, target, cat_features=categorical_features, verbose=False)\n\n    print('...Search completed...')\n    print('Best score:', cb_grid.best_score_)\n    print('Best params:', cb_grid.best_params_)\n    return cb_grid.best_params_","5c2325a1":"# run the grid-search\n\n#cb_params = CB_cv_grid()\ncb_params = {'bootstrap_type': 'No', 'depth': 6, 'iterations': 2000, 'learning_rate': 0.01, 'loss_function': 'RMSE', 'verbose':False}\ncb_params","98403acd":"# fit model with best parameters\ncb_model = cb.CatBoostRegressor(**cb_params)\ncb_model.fit(train_pool)","bae9375a":"# make a prediction and return submission file\npred=cb_model.predict(test_pool)\npd.DataFrame(data = {'Id': test_df.Id,\n                     'SalePrice': pred}).to_csv('submission_flat.csv', index = False)","3770f456":"# \"Light\" as a cat step (CatBoost)","f123bbb2":"No more NaNs! Awesome!","05158cfb":"## 2.2. CatBoost","c28ea92b":"## 1.3 Dealing with NaNs","f80b651d":"The main idea is to not loose any little single row. So we will fill NaN in each row with average values.<br\/>\n* For numeric features: median value\n* For categorical: the average category OR N\/A category","e8dad098":"## 1.4 Scaling\n\nFor better fit, let's scale our numeric features to standard range values. Will use a StandardScaler. ","0f24f0ae":"Some thoughts:\n* OverallCond seems to be a bad numeric, so let it be categorical\n* Useless num. features:\n    * MiscVal\n    * PoolArea\n* GarageCars correlates better then GarageArea. Lets leave GarageCars only to avoid linear dependencies","3d57c268":"Awesome!\nLet's see the shapes of data before training.","2c02733a":"# 2. Modeling","dfa18d6e":"## 1.5 Target feature\nOur target is SalePrice. Lets have a look on that.","4d7638eb":"Load data.","3de56662":"CatBoost is really good for boosting large categorical feature's DFs. <br\/>\nWhat even better, we doesn't need to convert cat.features anykind (OHE for example) - CatBoost will do encoding by himself. \n\nCatBoost uses class cb.Pool as the fastest way for processing data. And it is really raises time-rates for for models.<br\/>\n[Docs for cb.Pool](https:\/\/catboost.ai\/docs\/concepts\/python-reference_pool.html)","120007be":"# 1. Preprocess","5a886d2e":"Thats it. Final score is rather above average. For best scores, will try better fitting in next version.","d37f3092":"Will use a GridSearchCV function to find the best hyper params. Write a function for this, so we can not execute the search any time RunAllCells.","fc0b27b2":"## 2.1. Constant model evaluation\n\nTo make sure in adequatness of model, lets see the RMSE score for median predictions.","8c5d6603":"## 1.1 Numeric features analysis\n\nWe have regression model, so lets start with numbers!","79808806":"A quick view on a dataset makes cry every Machine Model, cause of large number of features, and low number of examples. [:poor_robot:] <br\/>\nEven worse, if we understand how many categories inside them. <br\/>\nTo get a good result from the start, lets try to use a \"light\" methods:\n* \"Light\" features analysis and engineering\n* \"Light\" gradient boosting using CatBoost library\n\nWhy CatBoost? Well, it seems, it is best to work with large number of categorical features. So let's try to use this on House_prices competition.\n\n[Catboost website](https:\/\/catboost.ai\/)","9a27a148":"## 1.2 Features filtering\n\nLets select features, that LOGICALLY will make no (or minimum) influense on Price. <br\/>\nLogically means as a hypothesis:) Ask yourself: 'Will this feature REALLY affect on your price evaluations?' If answer is NO - remove this."}}