{"cell_type":{"9fbf2e70":"code","b0f5c15c":"code","5213c533":"code","0bd29d8b":"code","b63b64ec":"code","2cfe504d":"code","fa461c20":"code","237385bd":"code","543efa8a":"code","2a7a5aa3":"code","f1787dad":"code","5e82e874":"code","1260d4c3":"code","3adb2f9e":"code","d0322d96":"code","1d2c0706":"code","0c0a8a81":"code","37fed44e":"code","7fc74047":"code","fd1491cb":"code","013c586e":"code","829a45af":"markdown","a6b510d9":"markdown","e601c919":"markdown","0efc6ac6":"markdown","a3c35a51":"markdown","0a50bec8":"markdown","06180975":"markdown","c413dfe5":"markdown"},"source":{"9fbf2e70":"from datetime import date, datetime, timedelta\nimport numpy as np\nimport pandas as pd\n\nconfirmed = pd.read_csv(\"..\/input\/jhucovid19\/csse_covid_19_data\/csse_covid_19_time_series\/time_series_covid19_confirmed_global.csv\")\ndeaths   = pd.read_csv(\"..\/input\/jhucovid19\/csse_covid_19_data\/csse_covid_19_time_series\/time_series_covid19_deaths_global.csv\")","b0f5c15c":"launch_date = date(2020, 4, 2)\nlatest_train_date = date(2020, 4, 1)\n\npublic_leaderboard_start_date = launch_date - timedelta(7)\nclose_date = launch_date + timedelta(7)\nfinal_evaluation_start_date = launch_date + timedelta(8)","5213c533":"confirmed.columns = list(confirmed.columns[:4]) + [datetime.strptime(d, \"%m\/%d\/%y\").date().strftime(\"%Y-%m-%d\") for d in confirmed.columns[4:]]\ndeaths.columns    = list(deaths.columns[:4])    + [datetime.strptime(d, \"%m\/%d\/%y\").date().strftime(\"%Y-%m-%d\") for d in deaths.columns[4:]]","0bd29d8b":"# Filter out problematic data points (The West Bank and Gaza had a negative value, cruise ships were associated with Canada, etc.)\nremoved_states = \"Recovered|Grand Princess|Diamond Princess\"\nremoved_countries = \"US|The West Bank and Gaza\"\n\nconfirmed.rename(columns={\"Province\/State\": \"Province_State\", \"Country\/Region\": \"Country_Region\"}, inplace=True)\ndeaths.rename(columns={\"Province\/State\": \"Province_State\", \"Country\/Region\": \"Country_Region\"}, inplace=True)\nconfirmed = confirmed[~confirmed[\"Province_State\"].replace(np.nan, \"nan\").str.match(removed_states)]\ndeaths    = deaths[~deaths[\"Province_State\"].replace(np.nan, \"nan\").str.match(removed_states)]\nconfirmed = confirmed[~confirmed[\"Country_Region\"].replace(np.nan, \"nan\").str.match(removed_countries)]\ndeaths    = deaths[~deaths[\"Country_Region\"].replace(np.nan, \"nan\").str.match(removed_countries)]\n\nconfirmed.drop(columns=[\"Lat\", \"Long\"], inplace=True)\ndeaths.drop(columns=[\"Lat\", \"Long\"], inplace=True)","b63b64ec":"confirmed","2cfe504d":"deaths","fa461c20":"us_keys = pd.read_csv(\"..\/input\/jhucovid19\/csse_covid_19_data\/csse_covid_19_daily_reports\/04-01-2020.csv\")\nus_keys = us_keys[us_keys[\"Country_Region\"]==\"US\"]\nus_keys = us_keys.groupby([\"Province_State\", \"Country_Region\"])[[\"Confirmed\", \"Deaths\"]].sum().reset_index()\n\nus_keys = us_keys[~us_keys.Province_State.str.match(\"Diamond Princess|Grand Princess|Recovered|Northern Mariana Islands|American Samoa\")].reset_index(drop=True)\nus_keys","237385bd":"confirmed = confirmed.append(us_keys[[\"Province_State\", \"Country_Region\"]], sort=False).reset_index(drop=True)\ndeaths = deaths.append(us_keys[[\"Province_State\", \"Country_Region\"]], sort=False).reset_index(drop=True)","543efa8a":"for col in confirmed.columns[2:]:\n    confirmed[col].fillna(0, inplace=True)\n    deaths[col].fillna(0, inplace=True)","2a7a5aa3":"confirmed","f1787dad":"us_start_date = date(2020, 3, 10)\nday_date = us_start_date\n\nwhile day_date <= latest_train_date:\n    day = pd.read_csv(\"..\/input\/jhucovid19\/csse_covid_19_data\/csse_covid_19_daily_reports\/%s.csv\" % day_date.strftime(\"%m-%d-%Y\"))\n    \n    if \"Country\/Region\" in day.columns:\n        day.rename(columns={\"Country\/Region\": \"Country_Region\", \"Province\/State\": \"Province_State\"}, inplace=True)\n    \n    us = day[day[\"Country_Region\"]==\"US\"]\n    us = us.groupby([\"Province_State\", \"Country_Region\"])[[\"Confirmed\", \"Deaths\"]].sum().reset_index()\n    \n    unused_data = []\n    untouched_states = set(confirmed[confirmed[\"Country_Region\"]==\"US\"][\"Province_State\"])\n    \n    for (i, row) in us.iterrows():\n        if confirmed[(confirmed[\"Country_Region\"]==\"US\") & (confirmed[\"Province_State\"]==row[\"Province_State\"])].shape[0]==1:\n            confirmed.loc[(confirmed[\"Country_Region\"]==\"US\") & (confirmed[\"Province_State\"]==row[\"Province_State\"]), day_date.strftime(\"%Y-%m-%d\")] = row[\"Confirmed\"]\n            deaths.loc[(deaths[\"Country_Region\"]==\"US\") & (deaths[\"Province_State\"]==row[\"Province_State\"]), day_date.strftime(\"%Y-%m-%d\")] = row[\"Deaths\"]\n            untouched_states.remove(row[\"Province_State\"])\n        else:\n            unused_data.append(row[\"Province_State\"])\n            \n    print(day_date, \"Untouched\", untouched_states)\n    print(day_date, \"Unused\", unused_data)\n\n    day_date = day_date + timedelta(1)","5e82e874":"confirmed","1260d4c3":"deaths","3adb2f9e":"dates_on_after_launch = [col for col in confirmed.columns[4:] if col>=launch_date.strftime(\"%Y-%m-%d\")]\nprint(\"Removing %d columns: %s\" % (len(dates_on_after_launch), str(dates_on_after_launch)))\n\ncols_to_keep = [col for col in confirmed.columns if col not in dates_on_after_launch]\n\nconfirmed = confirmed[cols_to_keep]\ndeaths = deaths[cols_to_keep]","d0322d96":"for i in range(36):\n    this_date = (launch_date + timedelta(i)).strftime(\"%Y-%m-%d\")\n    confirmed.insert(len(confirmed.columns), this_date, np.NaN)\n    deaths.insert(len(deaths.columns), this_date, np.NaN)","1d2c0706":"confirmed_melted = confirmed.melt(confirmed.columns[:2], confirmed.columns[2:], \"Date\", \"ConfirmedCases\")\n#confirmed_melted.insert(5, \"Type\", \"Confirmed\")\ndeaths_melted = deaths.melt(deaths.columns[:2], deaths.columns[2:], \"Date\", \"Fatalities\")\n#deaths_melted.insert(5, \"Type\", \"Deaths\")\n\nconfirmed_melted.sort_values(by=[\"Country_Region\", \"Province_State\", \"Date\"], inplace=True)\ndeaths_melted.sort_values(by=[\"Country_Region\", \"Province_State\", \"Date\"], inplace=True)\n\nassert confirmed_melted.shape==deaths_melted.shape\nassert list(confirmed_melted[\"Province_State\"])==list(deaths_melted[\"Province_State\"])\nassert list(confirmed_melted[\"Country_Region\"])==list(deaths_melted[\"Country_Region\"])\nassert list(confirmed_melted[\"Date\"])==list(deaths_melted[\"Date\"])\n\ncases = confirmed_melted.merge(deaths_melted, on=[\"Province_State\", \"Country_Region\", \"Date\"], how=\"inner\")\ncases = cases[[\"Country_Region\", \"Province_State\", \"Date\", \"ConfirmedCases\", \"Fatalities\"]]\n\ncases.sort_values(by=[\"Country_Region\", \"Province_State\", \"Date\"], inplace=True)\ncases.insert(0, \"Id\", range(1, cases.shape[0]+1))\ncases","0c0a8a81":"forecast = cases.loc[cases[\"Date\"]>=public_leaderboard_start_date.strftime(\"%Y-%m-%d\")]\nforecast.drop(columns=\"Id\", inplace=True)\nforecast.insert(0, \"ForecastId\", range(1, forecast.shape[0]+1))\nforecast.insert(6, \"Usage\", \"Ignored\")\nforecast.loc[forecast[\"Date\"]<launch_date.strftime(\"%Y-%m-%d\"),\"Usage\"]=\"Public\"\nforecast.loc[forecast[\"Date\"]>=final_evaluation_start_date.strftime(\"%Y-%m-%d\"),\"Usage\"]=\"Private\"\nforecast","37fed44e":"train = cases[cases[\"Date\"]<launch_date.strftime(\"%Y-%m-%d\")]\ntrain.to_csv(\"train.csv\", index=False)\ntrain","7fc74047":"test = forecast[forecast.columns[:-3]]\ntest.to_csv(\"test.csv\", index=False)\ntest","fd1491cb":"solution = forecast[[\"ForecastId\", \"ConfirmedCases\", \"Fatalities\", \"Usage\"]].copy()\nsolution[\"ConfirmedCases\"].fillna(1, inplace=True)\nsolution[\"Fatalities\"].fillna(1, inplace=True)\nsolution.to_csv(\"solution.csv\", index=False)\nsolution","013c586e":"submission = forecast[[\"ForecastId\", \"ConfirmedCases\", \"Fatalities\"]].copy()\nsubmission[\"ConfirmedCases\"] = 1\nsubmission[\"Fatalities\"] = 1\nsubmission.to_csv(\"submission.csv\", index=False)\n\nsubmission","829a45af":"Starting to pull in US state data, since this was saved separately","a6b510d9":"Filtering out any data on or after the launch date for the competition","e601c919":"## Global competition data","0efc6ac6":"Adding the rows to be forecast","a3c35a51":"Adding in daily US state data","0a50bec8":"Move to ISO 8601 dates","06180975":"### COVID-19 Forecasting Challenge (Week 3) Data Prep\n\nThis notebook prepared the data in Kaggle's [COVID-19 Global Forecasting Competition (Week 2)](https:\/\/www.kaggle.com\/c\/covid19-global-forecasting-week-2) that was used to launch the competition. The source data comes from [JHU CSSE's COVID-19 data repository on GitHub](https:\/\/github.com\/CSSEGISandData\/COVID-19).\n\nI re-ran this notebook on updated data to add descriptive comments, so it won't output precisely the same as the original launch data. I saved the original launch data [to this dataset](https:\/\/www.kaggle.com\/benhamner\/covid19-forecasting-week-two-launch-data).\n\nThe data for the submission period for the forecasting challenges is also updated every day, alongside leaderboard rescores. I use [this notebook](https:\/\/www.kaggle.com\/benhamner\/covid-19-forecasting-ongoing-data-updates\/) to run the ongoing data updates.","c413dfe5":"Melting the data to a version that will be friendlier to Kaggle's evaluation system."}}