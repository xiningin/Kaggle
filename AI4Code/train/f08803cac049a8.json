{"cell_type":{"517f07f7":"code","872ef8d0":"code","80e147dc":"code","0ed73517":"code","4a35c42b":"code","9a9dccbf":"code","51c3e200":"code","58e810eb":"code","f22e2d3f":"code","e9801a51":"code","f215b484":"code","1d97d103":"code","9766a950":"code","1db2823d":"code","e5dad46e":"code","dd7e0b76":"code","cfd10b90":"markdown","c1148146":"markdown","11483ddb":"markdown","55b6d84f":"markdown","acfa9a6a":"markdown","df59f320":"markdown","f459ec5b":"markdown","16db36e4":"markdown","3915b2ec":"markdown","651a5401":"markdown","212cfcfa":"markdown","f193c04d":"markdown","fcab17e1":"markdown"},"source":{"517f07f7":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O\nimport os\nimport datetime\nimport gc","872ef8d0":"path = '..\/input\/ashrae-energy-prediction'\n\nfor dirname, _, filenames in os.walk(path):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","80e147dc":"def reduce_mem(df):\n    result = df.copy()\n    for col in result.columns:\n        col_data = result[col]\n        dn = col_data.dtype.name\n        if not dn.startswith(\"datetime\"):\n            if dn == \"object\":  # only object feature has low cardinality\n                result[col] = pd.to_numeric(col_data.astype(\"category\").cat.codes, downcast=\"unsigned\")\n            elif dn.startswith(\"int\") | dn.startswith(\"uint\"):\n                if col_data.min() >= 0:\n                    result[col] = pd.to_numeric(col_data, downcast=\"unsigned\")\n                else:\n                    result[col] = pd.to_numeric(col_data, downcast='integer')\n            else:\n                result[col] = pd.to_numeric(col_data, downcast='float')\n    return result","0ed73517":"def add_lag_features(weather_df, window=3):\n    group_df = weather_df.groupby('site_id')\n    cols = ['air_temperature', 'cloud_coverage', 'dew_temperature', 'precip_depth_1_hr']\n    rolled = group_df[cols].rolling(window=window, min_periods=0)\n    lag_mean = rolled.mean().reset_index().astype(np.float16)\n    lag_max = rolled.max().reset_index().astype(np.float16)\n    lag_min = rolled.min().reset_index().astype(np.float16)\n    for col in cols:\n        weather_df[f'{col}_min_lag{window}'] = lag_min[col]\n        weather_df[f'{col}_mean_lag{window}'] = lag_mean[col]\n        weather_df[f'{col}_max_lag{window}'] = lag_max[col]\n    return weather_df","4a35c42b":"def load_data(source='train'):\n    assert source in ['train','test']\n    df = pd.read_csv(f'{path}\/{source}.csv', parse_dates=['timestamp'])\n    return reduce_mem(df)\n\ndef load_building():\n    df = pd.read_csv(f'{path}\/building_metadata.csv').fillna(-1)\n    return reduce_mem(df)\n\ndef load_weather(source='train', fix_timezone=True, impute=True, add_lag=True):\n    assert source in ['train','test']\n    df = pd.read_csv(f'{path}\/weather_{source}.csv', parse_dates=['timestamp'])\n    if fix_timezone:\n        offsets = [5,0,9,6,8,0,6,6,5,7,8,6,0,7,6,6]\n        offset_map = {site: offset for site, offset in enumerate(offsets)}\n        df.timestamp = df.timestamp - pd.to_timedelta(df.site_id.map(offset_map), unit='h')\n    if impute:\n        site_dfs = []\n        for site in df.site_id.unique():\n            if source == 'train':\n                new_idx = pd.date_range(start='2016-1-1', end='2016-12-31-23', freq='H')\n            else:\n                new_idx = pd.date_range(start='2017-1-1', end='2018-12-31-23', freq='H')\n            site_df = df[df.site_id == site].set_index('timestamp').reindex(new_idx)\n            site_df.site_id = site\n            for col in [c for c in site_df.columns if c != 'site_id']:\n                site_df[col] = site_df[col].interpolate(limit_direction='both', method='linear')\n                site_df[col] = site_df[col].fillna(df[col].median())\n            site_dfs.append(site_df)\n        df = pd.concat(site_dfs)\n        df['timestamp'] = df.index\n        df = df.reset_index(drop=True)\n        \n    if add_lag:\n        df = add_lag_features(df, window=3)\n    \n    return reduce_mem(df)\n\ndef merged_dfs(source='train', fix_timezone=True, impute=True, add_lag=True):\n    df = load_data(source=source).merge(load_building(), on='building_id', how='left')\n    df = df.merge(load_weather(source=source, fix_timezone=fix_timezone, impute=impute, add_lag=add_lag),\n                 on=['site_id','timestamp'], how='left')\n    if source == 'train':\n        X = df.drop('meter_reading', axis=1)  \n        y = np.log1p(df.meter_reading)  # log-transform of target\n        return X, y\n    elif source == 'test':\n        return df","9a9dccbf":"%%time\nX_train, y_train = merged_dfs(add_lag=False)\nX_train.head()","51c3e200":"def _delete_bad_sitezero(X, y):\n    cond = (X.timestamp > '2016-05-20') | (X.site_id != 0) | (X.meter != 0)\n    X = X[cond]\n    y = y.reindex_like(X)\n    return X.reset_index(drop=True), y.reset_index(drop=True)\n\ndef _extract_temporal(X):\n    X['hour'] = X.timestamp.dt.hour\n    X['weekday'] = X.timestamp.dt.weekday\n    # month and year cause overfit, could try other (holiday, business, etc.)\n    return reduce_mem(X)","58e810eb":"# preprocessing\nX_train, y_train = _delete_bad_sitezero(X_train, y_train)\nX_train = _extract_temporal(X_train)\n\n# remove timestamp and other unimportant features\nto_drop = ['timestamp','sea_level_pressure','wind_direction','wind_speed']\nX_train.drop(to_drop, axis=1, inplace=True)\n\ngc.collect()","f22e2d3f":"X_train.info()","e9801a51":"y_train = y_train.to_frame()","f215b484":"X_train.to_feather('X_train.feather')\ny_train.to_feather('y_train.feather')","1d97d103":"import pickle\nfrom fastai.tabular.all import *\n\n# Subclass the Unpickler to load cuda model on CPU\nclass CPU_Unpickler(pickle.Unpickler):\n    def find_class(self, module, name):\n        if module == 'torch.storage' and name == '_load_from_bytes':\n            return lambda b: torch.load(io.BytesIO(b), map_location='cpu')\n        else: return super().find_class(module, name)\n\nwith open('..\/input\/ashrae-with-fast-ai-part-2\/tabular_nn.pickle', mode='rb') as f:\n    #learn = pickle.load(f) becomes...\n    learn = CPU_Unpickler(f).load()","9766a950":"cat_features = ['meter','site_id','primary_use','hour','weekday']\n\ndef add_embeds(learn, x):\n    x = x.copy()\n    for i, cat in enumerate(cat_features):\n        emb = learn.embeds[i]\n        vec = tensor(x[cat], dtype=torch.int64) # this is on cpu\n        emb_data = emb(vec)\n        emb_names = [f'{cat}_{j}' for j in range(emb_data.shape[1])]\n        \n        emb_df = pd.DataFrame(emb_data, index=x.index, columns=emb_names)\n        x = x.drop(columns=cat)\n        x = x.join(emb_df)\n    return x","1db2823d":"X_train = add_embeds(learn, X_train)\ngc.collect()","e5dad46e":"X_train.info()","dd7e0b76":"X_train.to_feather('X_embeds.feather')","cfd10b90":"# Load data","c1148146":"# Concatenate embedding vectors","11483ddb":"Let us remove the first 141 days of electrical meter readings at site 0, which are mostly zero or contain anomalous spikes. This is the type of outlier which causes the most trouble and is comparatively easier to remove. We also extract some basic temporal features.","55b6d84f":"# Utilities","acfa9a6a":"# Random forest with Entity Embeddings: Training set","df59f320":"Now save the DataFrame with the embeddings.","f459ec5b":"Now let us load the categorical embeddings learned with a NN [here](https:\/\/www.kaggle.com\/michelezoccali\/ashrae-with-fast-ai-part-2). \n\nThis step is more easily performed on the GPU, where the original model was trained. It is sometimes necessary to perform this step on the CPU, however, so let us see here the simples changes required.","16db36e4":"# Outlier removal and basic FE","3915b2ec":"Now on to modeling. Take a look at [Part 2](https:\/\/www.kaggle.com\/michelezoccali\/lgbm-with-entity-embeddings-part-2).","651a5401":"Save target and training set without embeddings.","212cfcfa":"This kernel prepares the ASHRAE Energy Prediction dataset for training a Random Forest. It performs the same preprocessing as in [this kernel](https:\/\/www.kaggle.com\/michelezoccali\/ashrae-energy-prediction-single-lgbm), while substituting categorical features with the corresponding embedding vectors previously learned by a NN in https:\/\/www.kaggle.com\/michelezoccali\/ashrae-with-fast-ai-part-2.","f193c04d":"Memory reduction adapted from [this kernel.](https:\/\/www.kaggle.com\/purist1024\/ashrae-simple-data-cleanup-lb-1-08-no-leaks\/notebook)","fcab17e1":"Routine to add lag features to weather dataset, adapted from [this kernel](https:\/\/www.kaggle.com\/corochann\/ashrae-training-lgbm-by-meter-type\/notebook)."}}