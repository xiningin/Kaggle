{"cell_type":{"93f72fc4":"code","ced33fac":"code","444d9432":"code","4233fe42":"code","d28e33a7":"code","83e2d805":"code","ce514ef9":"code","efc861fa":"code","5db50c00":"code","486bbed6":"code","8ae843d5":"code","ed1dcf8a":"code","78fcf6da":"code","fd897d81":"code","79a8701e":"code","e2ad93ae":"code","fad11de2":"code","50fe44aa":"code","29597848":"code","fc4a721a":"code","c00b25f9":"code","d2fb0e4a":"code","1ca56590":"code","cc762ba8":"code","8e116567":"code","b7fd7791":"code","07ce909d":"code","89e906b3":"code","25a6e77c":"code","29b4e188":"code","ba683eb7":"code","6068b869":"code","f502ce9e":"code","9508ca9f":"code","b2e8c36f":"code","11a27844":"code","74f34df0":"code","b145b3e6":"code","f8acc3b7":"code","7b450db7":"code","764b5e4c":"code","7b3c7251":"code","621108d2":"code","1917baaa":"code","5f935a3a":"code","5a5282f1":"code","3ea51441":"code","ba4e0883":"code","57c041fb":"code","de247f90":"code","675f2b06":"code","d457bea1":"code","9fd991f8":"code","f9383b07":"code","17aa7901":"markdown","14daf183":"markdown","a770fd7c":"markdown"},"source":{"93f72fc4":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","ced33fac":"import seaborn as sns \nimport matplotlib.pyplot as plt\n\nfrom sklearn.metrics import roc_auc_score # this is the metric used to score the competition\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler, QuantileTransformer # scaling will be necessary for most models\nfrom sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, RandomizedSearchCV # using a small validation + cv set may help\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.feature_selection import SelectKBest, chi2, f_classif\nfrom sklearn.inspection import permutation_importance\nfrom sklearn.decomposition import KernelPCA, PCA\nfrom sklearn.linear_model import LogisticRegression, SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\nfrom sklearn.svm import LinearSVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom xgboost import XGBClassifier\nfrom skopt import BayesSearchCV # works similar to GridSearchCV but it doesn't try all param combinations, takes structure of search space into acc.","444d9432":"RAND_STATE = 13","4233fe42":"def prep_test_data(test_data):\n    \"\"\"Prepares Test Data for predictions, by applying all non-pipeline preproccessing steps.\n        - Downcasting float and integer columns to save memory\n        - Dropping 'id' feature\n    \n    Args:\n        test_data(pd.DataFrame): DataFrame containing all columns of training data except id.\n    Returns:\n        test_data_prep(pd.DataFrame): DataFrame with features equal to training data. \n    \"\"\"\n    for col in test_data.columns:\n        if test_data.loc[:,col].dtype == 'float64':\n            test_data.loc[:,col] = pd.to_numeric(test_data.loc[:,col], downcast='float')   \n        if test_data.loc[:,col].dtype == 'int64':\n            test_data.loc[:,col] = pd.to_numeric(test_data.loc[:,col], downcast='integer')\n\n    test_data_prep = test_data.drop('id', axis = 1)  \n    return test_data_prep","d28e33a7":"df_train = pd.read_csv('\/kaggle\/input\/tabular-playground-series-nov-2021\/train.csv')\ndf_test = pd.read_csv('\/kaggle\/input\/tabular-playground-series-nov-2021\/test.csv')","83e2d805":"df_train.shape","ce514ef9":"for col in df_train.columns:    \n    if df_train.loc[:,col].dtype == 'float64':\n        df_train.loc[:,col] = pd.to_numeric(df_train.loc[:,col], downcast='float')   \n    if  df_train.loc[:,col].dtype == 'int64':\n         df_train.loc[:,col] = pd.to_numeric(df_train.loc[:,col], downcast='integer')","efc861fa":"df_train.head()","5db50c00":"df_train.isna().any().sum()","486bbed6":"fig, ax = plt.subplots(figsize = (7,7))\nsns.countplot(x = df_train.loc[:,'target'], ax = ax)\nsns.despine()","8ae843d5":"df_train.describe().T.sample(20)","ed1dcf8a":"# fig, axes = plt.subplots(nrows = 10, ncols = 10, figsize = (20,20))\n# axes = axes.flatten()\n\n# df_plot = df_train.sample(frac = 0.33, random_state = RAND_STATE) #only use a third of the data to visualize\n# for idx, axis in enumerate(axes):\n#     sns.histplot(data = df_plot, x = f'f{idx}',\n#                 ax = axis, hue = 'target', legend = False)\n#     axis.set_ylabel('')    \n#     axis.set_xlabel('')","78fcf6da":"fig, ax = plt.subplots(figsize = (15,13))\nsns.heatmap(df_train.corr(), ax = ax)","fd897d81":"df_train.drop('id', axis = 1, inplace = True) ","79a8701e":"features = df_train.drop('target', axis = 1)\ntarget = df_train.loc[:,'target']\n\nfeatures_train, features_val, target_train,target_val = train_test_split(features, \n                                                                         target, \n                                                                         test_size = 0.1, \n                                                                         random_state = RAND_STATE)","e2ad93ae":"features_train.shape, features_val.shape","fad11de2":"target_train.shape, target_val.shape","50fe44aa":"# base_dt = DecisionTreeClassifier()\n# cross_val_score(base_dt, features_train, target_train, scoring = 'roc_auc', n_jobs = -1)","29597848":"nb_clf = GaussianNB()\npipe_standard = Pipeline([('standard_scaler', StandardScaler()), ('nb_model', nb_clf)])\npipe_minmax = Pipeline([('minmax_scaler', MinMaxScaler()), ('nb_model', nb_clf)])\npipe_robust = Pipeline([('robust_scaler', RobustScaler()), ('nb_model', nb_clf)])\npipe_quantile = Pipeline([('quantile_scaler', QuantileTransformer()), ('nb_model', nb_clf)])\npipe_quantile_norm = Pipeline([('quantile_scaler', QuantileTransformer(output_distribution = 'normal')),\n                          ('nb_model', nb_clf)])","fc4a721a":"pipes = {'Standard': pipe_standard,\n         'MinMax': pipe_minmax,\n         'Robust': pipe_robust,\n         'Quantile': pipe_quantile,\n         'Quantile Normal Dist': pipe_quantile_norm}\n\nfor key,pipe in pipes.items():\n    print(key)\n    cv_scores = cross_val_score(pipe,features_train, target_train, \n                               scoring = 'roc_auc', n_jobs = -1)\n    print(f\"Mean ROC AUC: {np.mean(cv_scores)}\")\n    ","c00b25f9":"pipe_quantile_norm.fit(features_train, target_train)\nquant_nb_preds = pipe_quantile_norm.predict_proba(features_val)","d2fb0e4a":"roc_auc_score(target_val, quant_nb_preds[:,1])","1ca56590":"# pipe_quantile_norm.fit(features_train, target_train)\n# features_test = prep_test_data(df_test)\n# quant_nb_preds_sub = pipe_quantile_norm.predict_proba(features_test)\n# df_sub_quant_nb = pd.DataFrame({'id': df_test.loc[:,'id'], 'target':quant_nb_preds_sub[:,1]})\n# df_sub_quant_nb\n#df_sub_quant_nb.to_csv('submission.csv', index = None)","cc762ba8":"log_clf = LogisticRegression(n_jobs = -1)\nquant_norm = QuantileTransformer(output_distribution = 'normal')\npipe_robust_log = Pipeline([('robust_scaler', RobustScaler()), ('LogReg',log_clf)])\npipe_log = Pipeline([('Quantile Transformer', quant_norm),('LogReg',log_clf)])","8e116567":"# rf_clf = RandomForestClassifier(n_jobs = -1, random_state = RAND_STATE)\n# pipe_rf = Pipeline([('Quantile Transformer', quant_norm),('RandomForest',rf_clf)])","b7fd7791":"# ada_clf = AdaBoostClassifier(random_state = RAND_STATE)\n# pipe_ada = Pipeline([('Quantile Transformer', quant_norm),('AdaBoost',ada_clf)])","07ce909d":"svc_clf = LinearSVC(dual = False)\npipe_robust_svc = Pipeline([('robust_scaler', RobustScaler()), ('LinearSVC',svc_clf)])\npipe_svc = Pipeline([('Quantile Transformer', quant_norm),('LinearSVC',svc_clf)])","89e906b3":"svcsgd_clf = SGDClassifier(loss = 'hinge', n_jobs = -1)\npipe_robust_svcsgd = Pipeline([('robust_scaler', RobustScaler()), ('LinearSGDSVC',svcsgd_clf)])\npipe_svcsgd = Pipeline([('Quantile Transformer', quant_norm),('LinearSGDSVC',svcsgd_clf)])\n","25a6e77c":"# XGB_clf = XGBClassifier(max_depth = 5,\n#                                  learning_rate = 0.007,\n#                                  n_estimators = 7000,\n#                                  objective = 'binary:logistic',\n#                                  booster = 'gbtree',\n#                                  gamma = 1.5,\n#                                  max_delta_step = 3,\n#                                  min_child_weight = 10,\n#                                  subsample = 0.6,\n#                                  colsample_bytree = 0.8,\n#                                  n_jobs = -1\n#                                  )\n\n# quant_scaler = QuantileTransformer()\n# features_train_xgb = pd.DataFrame(quant_scaler.fit_transform(features_train))\n# features_val_xgb = pd.DataFrame(quant_scaler.transform(features_val))\n\n# xgb = XGB_clf.fit(features_train_xgb.values,\n#                        target_train.values.ravel(),\n#                        eval_set = [(features_train_xgb.values, target_train), (features_val_xgb.values, target_val)], \n#                        eval_metric = 'auc',\n#                        early_stopping_rounds = 25,\n#                        verbose = True)","29b4e188":"# features_test = prep_test_data(df_test)\n# features_test = pd.DataFrame(quant_scaler.transform(features_test))\n# quant_xgb_preds_sub = xgb.predict_proba(features_test)\n# df_sub_quant_xgb = pd.DataFrame({'id': df_test.loc[:,'id'], 'target':quant_xgb_preds_sub[:,1]})\n# print(df_sub_quant_xgb.head())\n# df_sub_quant_xgb.to_csv('df_sub_quant_xgb.csv', index = None)","ba683eb7":"#df_sub_quant_xgb.to_csv('submission_quant_xgb.csv', index = None)","6068b869":"pipes = {'Quant LogReg': pipe_log,\n         'Robust LogReg': pipe_robust_log,\n         #'RandomForest':pipe_rf,\n        #'AdaBoost':pipe_ada,\n         'Quant LinearSVC':pipe_svc,\n         'Robust LinearSVC': pipe_robust_svc,\n        'Quant LinearSGDSVC': pipe_svcsgd,\n        'Robust LinearSGDSVC': pipe_robust_svcsgd,}\n\nfor key,pipe in pipes.items():\n    print(key)\n    cv_scores = cross_val_score(pipe,features_train, target_train, \n                               scoring = 'roc_auc', n_jobs = -1)\n    print(f\"Mean ROC AUC: {np.mean(cv_scores)}\")","f502ce9e":"pipe_robust_log.fit(features_train, target_train)\nfeatures_test = prep_test_data(df_test)\nquant_log_preds_sub = pipe_robust_log.predict_proba(features_test)\ndf_sub_quant_log = pd.DataFrame({'id': df_test.loc[:,'id'], 'target':quant_log_preds_sub[:,1]})\ndf_sub_quant_log.to_csv('submission_robust_log.csv', index = None)","9508ca9f":"# pipe_robust_svc.fit(features_train, target_train)\n# features_test = prep_test_data(df_test)\n# quant_svc_preds_sub = pipe_robust_svc.predict(features_test)\n# df_sub_quant_svc = pd.DataFrame({'id': df_test.loc[:,'id'], 'target':quant_svc_preds_sub})\n# df_sub_quant_svc.to_csv('submission_robust_svc.csv', index = None)","b2e8c36f":"perm_feat_imp_logreg = permutation_importance(pipe_robust_log, \n                                               features, target, \n                                               scoring = 'roc_auc',\n                                               n_repeats = 3,\n                                               n_jobs = -1, \n                                               random_state = RAND_STATE)","11a27844":"perm_feat_imp_logreg_series = pd.Series(perm_feat_imp_logreg.get('importances_mean'),index = features.columns,)\nperm_feat_imp_logreg_series = perm_feat_imp_logreg_series.sort_values(ascending = False)\n\nfig,ax = plt.subplots(figsize = (15,20))\nsns.barplot(y = perm_feat_imp_logreg_series.index,\n            x = perm_feat_imp_logreg_series.values,\n            ax = ax)\nsns.despine()","74f34df0":"features_reduced = features_train.loc[:,perm_feat_imp_logreg_series[:40].index]","b145b3e6":"cv_scores = cross_val_score(pipe_robust_log,features_reduced, target_train, \n                               scoring = 'roc_auc', n_jobs = -1)\nprint(f\"Mean ROC AUC: {np.mean(cv_scores)}\")","f8acc3b7":"# features_reduced = features.loc[:,perm_feat_imp_logreg_series[:30].index]\n# pipe_robust_log.fit(features_reduced, target)\n# features_test = prep_test_data(df_test).loc[:,perm_feat_imp_logreg_series[:30].index]\n# quant_log_preds_sub = pipe_robust_log.predict_proba(features_test)\n# df_sub_quant_log = pd.DataFrame({'id': df_test.loc[:,'id'], 'target':quant_log_preds_sub[:,1]})\n# df_sub_quant_log.to_csv('submission_robust_log.csv', index = None)\n","7b450db7":"#df_sub_quant_log.to_csv('submission.csv', index = None)","764b5e4c":"selector_f = SelectKBest(score_func = f_classif, k = 40)\nselector_f.fit(features_train, target_train)\n","7b3c7251":"col_index = selector_f.get_support(indices=True)\ncol_names = features_train.iloc[:,col_index].columns\npd.Series(selector_f.scores_[:40], index = col_names)","621108d2":"features_train_kbest = features_train.iloc[:,col_index]\n\npipes = {'Quantile Uni NB': pipe_quantile,\n         'Quantile Normal NB': pipe_quantile_norm,\n         'Quant LogReg': pipe_log,\n         'Robust LogReg': pipe_robust_log,\n         'Quant LinearSVC':pipe_svc,\n         'Robust LinearSVC': pipe_robust_svc,\n        'Quant LinearSGDSVC': pipe_svcsgd,\n        'Robust LinearSGDSVC': pipe_robust_svcsgd,}\n\nfor key,pipe in pipes.items():\n    print(key)\n    cv_scores = cross_val_score(pipe,features_train_kbest, target_train, \n                               scoring = 'roc_auc', n_jobs = -1)\n    print(f\"Mean ROC AUC: {np.mean(cv_scores)}\")","1917baaa":"selector_f70 = SelectKBest(score_func = f_classif, k = 70)\nselector_f70.fit(features_train, target_train)\ncol_index = selector_f70.get_support(indices=True)\n\nfeatures_train_kbest = features_train.iloc[:,col_index]\n\npipes = {'Quantile Uni NB': pipe_quantile,\n         'Quantile Normal NB': pipe_quantile_norm,\n         'Quant LogReg': pipe_log,\n         'Robust LogReg': pipe_robust_log,\n         'Quant LinearSVC':pipe_svc,\n         'Robust LinearSVC': pipe_robust_svc,\n        'Quant LinearSGDSVC': pipe_svcsgd,\n        'Robust LinearSGDSVC': pipe_robust_svcsgd,}\n\nfor key,pipe in pipes.items():\n    print(key)\n    cv_scores = cross_val_score(pipe,features_train_kbest, target_train, \n                               scoring = 'roc_auc', n_jobs = -1)\n    print(f\"Mean ROC AUC: {np.mean(cv_scores)}\")","5f935a3a":"features_reduced = features_train_kbest = features_train.iloc[:,col_index]\npipe_robust_log.fit(features_reduced, target_train)\npipe_robust_svc.fit(features_reduced, target_train)\nfeatures_test = prep_test_data(df_test).iloc[:,col_index]\nrobust_log_70_preds_sub = pipe_robust_log.predict_proba(features_test)\nrobust_svc_70_preds_sub = pipe_robust_svc.predict(features_test)\ndf_sub_robust_log_70 = pd.DataFrame({'id': df_test.loc[:,'id'], 'target':robust_log_70_preds_sub[:,1]})\ndf_sub_robust_svc_70 = pd.DataFrame({'id': df_test.loc[:,'id'], 'target':robust_svc_70_preds_sub})\ndf_sub_robust_log_70.to_csv('submission_robust_log_70best.csv', index = None)\ndf_sub_robust_svc_70.to_csv('submission_robust_svc_70best.csv', index = None)","5a5282f1":"pca = PCA(n_components = 0.95)\n\npipe_quant_pca_logreg = Pipeline([('QuantileTransformer', QuantileTransformer()),\n                                  ('PCA',pca),\n                                  ('LogReg',log_clf)])\npipe_robust_pca_logreg = Pipeline([('Scaler', RobustScaler()),\n                                  ('PCA',pca),\n                                  ('LogReg',log_clf)])\npipe_quant_pca_svc = Pipeline([('QuantileTransformer', QuantileTransformer()),\n                                  ('PCA',pca),\n                                  ('LinearSVC',svc_clf)])\npipe_robust_pca_svc = Pipeline([('Scaler', RobustScaler()),\n                                  ('PCA',pca),\n                                  ('LinearSVC',svc_clf)])","3ea51441":"pipes = {'Quant LogReg': pipe_quant_pca_logreg,\n         'Robust LogReg': pipe_robust_pca_logreg,\n         'Quant LinearSVC':pipe_quant_pca_svc,\n         'Robust LinearSVC': pipe_robust_pca_svc}\n\nfor key,pipe in pipes.items():\n    print(key)\n    cv_scores = cross_val_score(pipe,features_train, target_train, \n                               scoring = 'roc_auc', n_jobs = -1)\n    print(f\"Mean ROC AUC: {np.mean(cv_scores)}\")","ba4e0883":"def tune_hyperparams(pipeline, param_grid, n_iter = 50, iid = True):\n    '''ADD DOCSTRING'''\n    bayes_search = BayesSearchCV(pipeline,\n                                 param_grid,\n                                 n_iter = n_iter,\n                                 scoring = 'roc_auc',\n                                 cv = 3,\n                                 random_state = RAND_STATE,\n                                 verbose = 1,\n                                 n_jobs = 2)\n    bayes_search.fit(features, target)\n    best_estimator = bayes_search.best_estimator_\n    print(f'Best CV ROC-AUC {bayes_search.best_score_}\\n')\n    #print(pd.DataFrame(bayes_search.cv_results_))\n    print(bayes_search.best_estimator_)\n    return best_estimator","57c041fb":"# logreg_clf = LogisticRegression(solver = 'saga', \n#                                 random_state = RAND_STATE,\n#                                 n_jobs = -1,\n#                                 max_iter = 500,\n#                                 penalty = 'elasticnet')\n\n# pipe_robust_logreg = Pipeline([('scaler', RobustScaler()),\n#                                ('logreg',logreg_clf)])\n\n# robust_logreg_params = {\n#                         'logreg__l1_ratio': np.arange(0, 1.1, 0.1),\n#                         'logreg__C': np.geomspace(0.001, 100, 10)\n#                        }","de247f90":"# best_robust_logreg = tune_hyperparams(pipe_robust_logreg,\n#                                      robust_logreg_params, n_iter = 50)\n","675f2b06":"# best_robust_logreg.fit(features,target)\n# features_test = prep_test_data(df_test)\n# robust_log_tuned_preds_sub = best_robust_logreg.predict_proba(features_test)\n# df_sub_robust_log_tuned = pd.DataFrame({'id': df_test.loc[:,'id'], 'target':robust_log_tuned_preds_sub[:,1]})\n# df_sub_robust_log_tuned.to_csv('submission.csv', index = None)","d457bea1":"# logreg_clf = LogisticRegression(solver = 'saga', \n#                                 random_state = RAND_STATE,\n#                                 n_jobs = -1,\n#                                 max_iter = 500,\n#                                 penalty = 'elasticnet')\n\n# pipe_quant_logreg = Pipeline([('scaler', QuantileTransformer()),\n#                                ('logreg',logreg_clf)])\n\n# quant_logreg_params = {'scaler__output_distribution':['normal','uniform'],\n#                         'logreg__l1_ratio': np.arange(0, 1.1, 0.1),\n#                         'logreg__C': np.geomspace(0.001, 100, 10)\n#                        }\n\n# best_quant_logreg = tune_hyperparams(pipe_quant_logreg,\n#                                      quant_logreg_params)","9fd991f8":"best_quant_logreg = Pipeline(steps=[('scaler', QuantileTransformer(output_distribution='normal')),\n                ('logreg',\n                 LogisticRegression(C=0.001, l1_ratio=0.7000000000000001,\n                                    max_iter=500, n_jobs=-1,\n                                    penalty='elasticnet', random_state=13,\n                                    solver='saga'))])","f9383b07":"best_quant_logreg.fit(features,target)\nfeatures_test = prep_test_data(df_test)\nquant_log_tuned_preds_sub = best_quant_logreg.predict_proba(features_test)\ndf_sub_quant_log_tuned = pd.DataFrame({'id': df_test.loc[:,'id'], 'target':quant_log_tuned_preds_sub[:,1]})\ndf_sub_quant_log_tuned.to_csv('submission_quant.csv', index = None)\n","17aa7901":"#### What are you trying to do in this notebook?\nI'm predicting a categorical target based on a number of feature columns given in the data. The data is synthetically generated by a GAN that was trained on the data from the Forest Cover Type Prediction. This dataset is (a) much larger, and (b) may or may not have the same relationship to the target as the original data.\n\n#### Why are you trying it?\nPractice my ML skills on this approachable dataset.\n","14daf183":"#### The dataset is used for this competition is synthetic, but based on a real dataset and generated using a CTGAN. This dataset is based off of the original Forest Cover Type Prediction competition.","a770fd7c":"### PLEASE UPVOTE if you like this notebook. It will keep me motivated to update my notebook."}}