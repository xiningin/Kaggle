{"cell_type":{"6f059bf2":"code","20829396":"code","c9487aee":"code","c7a0e722":"code","19e4c4bd":"code","4270f438":"code","b0274208":"code","282a868d":"markdown","346319f9":"markdown","1436615e":"markdown","e0348cbc":"markdown","c7149758":"markdown","b45d77cf":"markdown","bd3ef0fc":"markdown","80f30d1d":"markdown"},"source":{"6f059bf2":"import numpy as np\nimport pandas as pd\nimport warnings\nwarnings.filterwarnings('ignore')\n\ndf_train = pd.read_csv('..\/input\/artificial-data-leaks\/train.csv')\ndf_test = pd.read_csv('..\/input\/artificial-data-leaks\/test.csv')\n\ndf = pd.DataFrame({'Value': df_train['col8'].value_counts().index,\n                  'Count in train': df_train['col8'].value_counts()})\ndf['Count in test'] = df_test['col8'].value_counts()\ndf = df.fillna(0).astype(int)\ndf","20829396":"import lightgbm as lgb\nimport matplotlib.pyplot as plt\nfrom sklearn import metrics\n\nRS = 0\nROUNDS = 500\nTARGET = 'target'\n\nparams = {\n    'objective': 'binary',\n    'metric': 'auc',\n    'boosting': 'gbdt',\n    'learning_rate': 0.1,\n    'verbose': 0,\n    'num_leaves': 64,\n    'bagging_fraction': 0.8,\n    'bagging_seed': RS,\n    'feature_fraction': 0.9,\n    'feature_fraction_seed': RS,\n    'max_bin': 100,\n    'max_depth': 5\n}\n\nx_train = lgb.Dataset(df_train.drop(TARGET, axis=1), df_train[TARGET])\nmodel = lgb.train(params, x_train, num_boost_round=ROUNDS)\n\nlgb.plot_tree(model, tree_index=0, figsize=(800, 48), show_info=['split_gain'])\nplt.show()","c9487aee":"df['Mean target in train'] = df_train.groupby('col8')['target'].mean()\ndf_mean = df[df['Value'].isin(range(90,110))]\ndf_mean","c7a0e722":"plt.bar(df_mean.index, df_mean['Mean target in train'])\npass","19e4c4bd":"df_train['Leak1'] = df_train['col8'].apply(lambda x: 1 if x % 6 < 3 else 0)\ndf_mean['Leak1 mean'] = df_train.groupby('col8')['Leak1'].mean()\nplt.bar(df_mean.index, df_mean['Leak1 mean'])\npass","4270f438":"df_train = pd.read_csv('..\/input\/artificial-data-leaks\/train.csv')\ndf_test = pd.read_csv('..\/input\/artificial-data-leaks\/test.csv')\n\ndf = pd.concat([df_train, df_test])\n\ndf['Leak1'] = df['col8'].apply(lambda x: 1 if x % 6 < 3 else 0)\n\ndf_train = df[:df_train.shape[0]]\ndf_test = df[df_train.shape[0]:]\n\nx_train = lgb.Dataset(df_train.drop(TARGET, axis=1), df_train[TARGET])\nmodel = lgb.train(params, x_train, num_boost_round=ROUNDS)\npreds = model.predict(df_test.drop(TARGET, axis=1))\n\nscore = metrics.roc_auc_score(df_test[TARGET], preds)\nprint('Test AUC score:',score)\n\nfig, axs = plt.subplots(ncols=2, figsize=(15,6))\nlgb.plot_importance(model, importance_type='split', ax=axs[0], title='Feature importance (split)')\nlgb.plot_importance(model, importance_type='gain', ax=axs[1], title='Feature importance (gain)')\npass\n\n#Baseline not-tuned model, raw features:  AUC 0.74632\n#First leak found, same model parameters: AUC 0.74675\n#All leaks found, same model parameters:  AUC 0.93927","b0274208":"df_train = pd.read_csv('..\/input\/artificial-data-leaks\/train.csv')\ndf_test = pd.read_csv('..\/input\/artificial-data-leaks\/test.csv')\n\ndf = pd.concat([df_train, df_test])\n\ndf['Leak1'] = df['col8'].apply(lambda x: 1 if x % 6 < 3 else 0)\ndf.drop(['col8'], axis=1, inplace=True)\n\ndf_train = df[:df_train.shape[0]]\ndf_test = df[df_train.shape[0]:]\n\nx_train = lgb.Dataset(df_train.drop(TARGET, axis=1), df_train[TARGET])\nmodel = lgb.train(params, x_train, num_boost_round=ROUNDS)\npreds = model.predict(df_test.drop(TARGET, axis=1))\n\nscore = metrics.roc_auc_score(df_test[TARGET], preds)\nprint('Test AUC score:',score)\n\nfig, axs = plt.subplots(ncols=2, figsize=(15,6))\nlgb.plot_importance(model, importance_type='split', ax=axs[0], title='Feature importance (split)')\nlgb.plot_importance(model, importance_type='gain', ax=axs[1], title='Feature importance (gain)')\npass\n\n#Baseline not-tuned model, raw features:  AUC 0.74632\n#First leak found:                        AUC 0.74675\n#First leak found, original col8 dropped: AUC 0.74715\n#All leaks found, same model parameters:  AUC 0.93927","282a868d":"We can clearly see, that mean target values differs - there's a group of values having mean target around 0.4 and another group with mean target 0.6. Let's put the data in a plot for better view.","346319f9":"# First leak found\n\nThis notebook reveals first of 10 leaks hidden in **Artificial data leaks** dataset. This probably is the easiest leak, as it was found first also in a workshop using this data.\n\nSo, where to start looking for some hidden information? We can see from feature importance plot in [\"First look at data and baseline model\"](https:\/\/www.kaggle.com\/alijs1\/first-look-at-data-and-baseline-model) kernel, that the biggest gain for model is obtained from **col8** feature. Let's start with this one and see if we can find something interesting.\n\nFrom [\"First look at data and baseline model\"](https:\/\/www.kaggle.com\/alijs1\/first-look-at-data-and-baseline-model) kernel we also already know that this feature has integer values and it's distribution visually looks very similar to Normal distribution with the mean at about 100.\n\nLet's check the most frequent values of this feature:","1436615e":"What we can observe is that our new **Leak1** feature is very low on Splits graph, but in the top of Gains graph - which means that our model is able to capture all the information with very few splits. That's exactly what was our purpose when building this feature. Also we can see slight increase in score - from 0.74632 to 0.74675. Increase is not that big for this leak, as model was able to capture it almost fully even without our help. What we did was just consolidated information according to our found insights for easier use by our model.\n\nWe can observe that **col8** is still used by the model quite actively even if we captured the main signal in our new **Leak1** feature. There could be several reasons for this, like:\n* there exists some additional information (like interactions with other features) we didn't capture fully;\n* these are consequences of using *feature_fraction* parameter for our model;\n* model is simply overfitting on **col8** duplicated feature.\n\nSo it's worth to check if dropping the original **col8** feature helps improving our score further.","e0348cbc":"Looks good! Now we can add it to our baseline model and re-run it to if this new feature helps.","c7149758":"So here it is! Now it becomes very obvious why model is making splits on values with step 3 - there's a clear pattern.\n\nModel is able to catch pattern like this (as we can see it is very correctly making necessary splits), but it requires a lot of splits for tree-based model to capture it fully. So let's help our model with some feature designed to capture this.\n\nWhat we need is a feature having values 1 and 0 depending on our observed pattern with step 3. Let's create it and plot to check that it corresponds to values we observed.","b45d77cf":"Great, we managed to improve the score little bit more! It looks that we have found and fully captured first leak in the data. 9 more leaks left to be found...","bd3ef0fc":"We can find that splits are done in a quite interesting pattern. Values on which splits by **col8**\nare made looks as folows:\n* 92.5\n* 95.5\n* 98.5\n* 101.5\n* 104.5\n* 107.5\n\nSo what's so special about them? First, they are all around 100 (mean value of this feature) - which is kinda expected, as making splits in the ends of long tails woldn't give big gain. But the most interesting thing is that split point values are growing by exactly step 3.\n\nLet's check mean target value for each value of **col8**. We will filter 20 **col8** values close to the mean value (which have more samples).","80f30d1d":"Not surprisingly the most frequent values are all close to the mean value of 100.\n\nAs this feature is considered very important by our model, it would be interesting to find out, what exactly model sees in it. One thing which could help in this is to plot some actual trees from our model to see, what splits are made on our **col8** feature. We'll use the same baseline model as in [\"First look at data and baseline model\"](https:\/\/www.kaggle.com\/alijs1\/first-look-at-data-and-baseline-model) kernel."}}