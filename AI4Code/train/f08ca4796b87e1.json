{"cell_type":{"751d35a6":"code","ab88794f":"code","90d15e7c":"code","8b25237d":"code","85de0932":"code","59342f40":"code","3e24d97b":"code","f702d07e":"code","149c67e5":"code","13d61b47":"code","a8443fbe":"code","09185094":"code","1945fa77":"code","5ae0bbde":"code","0efec0d9":"code","8dc811c6":"code","c8eca2b7":"code","3a7f0171":"code","a1bf108c":"code","f44968ec":"code","b8eb7e5b":"code","caaa7931":"code","a65a8d8b":"code","44215049":"code","399f7b01":"code","24c1973e":"code","e215990b":"code","e74f4e68":"code","e7a8e3dd":"code","54fc8aca":"code","c4da700b":"code","fb5906fd":"code","d8d87830":"code","8d62ba6c":"code","10403ace":"code","b29b7480":"code","cc9c08d9":"code","f0a6bc6d":"code","8d68ffc9":"code","e42495ae":"code","8b6a52e9":"code","a5807590":"code","251ba7d4":"code","0095a97d":"code","1326b11f":"code","c3e72be3":"code","b77fc0d4":"code","52249031":"code","063d74fa":"code","ca122e00":"code","d5a8f52c":"code","141e38b6":"code","7f137257":"code","58419da3":"code","cd65fe43":"code","a9761a75":"code","ea4ccd59":"code","cda001c7":"code","9e53694b":"code","ac17f7fc":"code","9e88c997":"code","2eabceb6":"code","4894127a":"code","e4cc18ba":"code","193298fe":"code","e20847d0":"code","e4129489":"code","26e667b4":"code","9a7b3086":"code","d8865d33":"code","3a5dc779":"code","72655471":"code","5fd3217c":"code","3fd3c829":"code","851b0a00":"code","32320ecd":"code","91fc148c":"code","ad2556e6":"code","00090126":"code","38262219":"code","8b2be331":"code","40b628d1":"code","afad1ef6":"code","f80b7221":"code","c7e2024b":"code","93003e95":"code","cc72ee6c":"code","ac9697f9":"code","0bcdd201":"code","0c6594b6":"code","eb63eaf9":"code","3fc21286":"code","3172d0c3":"code","da93dd2e":"code","29c11336":"code","77f0d3cb":"code","1e8e4ff4":"code","5de732d2":"code","962827b8":"markdown","c752f1d3":"markdown","fc50ab6a":"markdown","51a11c42":"markdown","eb118f57":"markdown","78b949ac":"markdown","3d4b1bf8":"markdown","414fbc2d":"markdown","50a3f788":"markdown","2dd5f7b4":"markdown","1aa65150":"markdown","e393dda4":"markdown","1de3a118":"markdown","292d9f80":"markdown","20f23da3":"markdown","fc17ac19":"markdown","cb6db1f9":"markdown","29d31e9c":"markdown","e2fe9cd1":"markdown","03c34b46":"markdown","8a35d190":"markdown","57a752fd":"markdown","8884a7aa":"markdown","cf3258d7":"markdown","3dc78370":"markdown","f5602176":"markdown","92f7ff52":"markdown","64fa273b":"markdown","9f21346c":"markdown","00b080b2":"markdown","b1dbd761":"markdown","68bfce2d":"markdown","b9825bbb":"markdown","efed684b":"markdown","0cd5cce5":"markdown","9136ac2b":"markdown","e4123982":"markdown","dafe60ce":"markdown","3eb1ed4d":"markdown","a3d1bc24":"markdown","9e7f2342":"markdown","5c1f29b9":"markdown","7d4ce845":"markdown","085b4019":"markdown","0573e0af":"markdown","04ae2c78":"markdown","2e95e37a":"markdown","c5177027":"markdown","f14cc02c":"markdown","464ad063":"markdown","901f70bb":"markdown","29e9c6c4":"markdown","a45d5f64":"markdown","ff3f2b1e":"markdown","19b798c4":"markdown","5be3b170":"markdown","27e3edf8":"markdown"},"source":{"751d35a6":"\n#Imporing necessary Libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nimport imblearn","ab88794f":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","90d15e7c":"#reading source file\ndata=pd.read_csv(\"\/kaggle\/input\/bank-full.csv\")","8b25237d":"#to check the head of the data-frame\ndata.head(10)","85de0932":"#checking the dtypes of the data\ndata.dtypes","59342f40":"#Checking the information of the data set\ndata.info()","3e24d97b":"#Checking the shape of the data-set and the target column\nprint(data.shape)\ndata['Target'].value_counts()","f702d07e":"#To check if there are any null values present\nnulllvalues=data.isnull().sum()\nprint(nulllvalues)\n","149c67e5":"#To check if there are any NaN values present\nNaNvalues=data.isna().sum()\nprint(NaNvalues)","13d61b47":"#Changing Target to numerical representation to use in EDA\nTarget_dict={'yes':1,'no':0}\n\ndata['Target']=data.Target.map(Target_dict)\n\ndata.head()","a8443fbe":"#To describe the data- Five point summary\ndata.describe().T","09185094":"#Distribution of continous data\n\nplt.figure(figsize=(30,6))\n\n#Subplot 1\nplt.subplot(1,3,1)\nplt.title('Age')\nsns.distplot(data['age'],color='red')\n\n#Subplot 2\nplt.subplot(1,3,2)\nplt.title('Balance')\nsns.distplot(data['balance'],color='blue')\n\n#Subplot 3\nplt.subplot(1,3,3)\nplt.title('Duration')\nsns.distplot(data['duration'],color='green')\n\n\n\nplt.figure(figsize=(30,6))\n\n#Subplot 1- Boxplot\nplt.subplot(1,3,1)\nplt.title('Age')\nsns.boxplot(data['age'],orient='horizondal',color='red')\n\n#Subplot 2\nplt.subplot(1,3,2)\nplt.title('Balance')\nsns.boxplot(data['balance'],orient='horizondal',color='blue')\n\n#Subplot 3\nplt.subplot(1,3,3)\nplt.title('Duration')\nsns.boxplot(data['duration'],orient='horizondal',color='green')\n","1945fa77":"# Distribution of Categorical data\n\nplt.figure(figsize=(30,6))\n\n#Subplot 1\nplt.subplot(1,3,1)\nplt.title('Contact')\nsns.countplot(data['contact'],color='cyan')\n\n#Subplot 2\nplt.subplot(1,3,2)\nplt.title('Education')\nsns.countplot(data['education'],color='violet')\n\n#Subplot 3\nplt.subplot(1,3,3)\nplt.title('Marital')\nsns.countplot(data['marital'],color='green')\n\nplt.figure(figsize=(30,6))\n\n#Subplot 4\nplt.subplot(1,3,1)\nplt.title('Default')\nsns.countplot(data['default'],color='red')\n\n#Subplot 5\nplt.subplot(1,3,2)\nplt.title('Housing')\nsns.countplot(data['housing'],color='blue')\n\n#Subplot 6\nplt.subplot(1,3,3)\nplt.title('Loan')\nsns.countplot(data['loan'],color='orange')","5ae0bbde":"# Distribution of Categorical data\n\nplt.figure(figsize=(30,6))\n\n#Subplot 1\nplt.subplot(1,3,1)\nplt.title('Day')\nsns.countplot(data['day'],color='cyan')\n\n#Subplot 2\nplt.subplot(1,3,2)\nplt.title('Month')\nsns.countplot(data['month'],color='violet')\n\n#Subplot 3\nplt.subplot(1,3,3)\nplt.title('Poutcome')\nsns.countplot(data['poutcome'],color='green')","0efec0d9":"data['job'].value_counts().head(30).plot(kind='bar')","8dc811c6":"# Distribution of Target column\nsns.countplot(data['Target'])","c8eca2b7":"sns.catplot(x='Target',y='age', data=data)","3a7f0171":"sns.catplot(x='Target',y='balance', data=data)","a1bf108c":"sns.catplot(x='Target',y='duration', data=data)","f44968ec":"sns.catplot(x='Target',y='campaign', data=data)","b8eb7e5b":"sns.catplot(x='Target',y='pdays', data=data)","caaa7931":"sns.catplot(x='Target',y='previous', data=data)","a65a8d8b":"sns.countplot(x='education',hue='Target', data=data)","44215049":"sns.violinplot(x=\"Target\", y=\"duration\", data=data,palette='rainbow')\n","399f7b01":"sns.catplot(x='marital',hue='Target',data=data,kind='count',height=4)","24c1973e":"sns.pairplot(data, palette=\"Set2\")","e215990b":"#To find the correlation between the continous variables\ncorrelation=data.corr()\ncorrelation.style.background_gradient(cmap='coolwarm')","e74f4e68":"sns.heatmap(correlation)","e7a8e3dd":"data.head()\ndata['Target']=data['Target'].astype('object')\ndata.head()","54fc8aca":"integers = data.columns[data.dtypes == 'int64']\n\nfor col in integers:\n    col_z = col + '-z'\n    data[col_z] = (data[col] - data[col].mean())\/data[col].std(ddof=0) \n\ndata.drop(['age','balance','day','duration','campaign','pdays','previous'],axis=1,inplace=True)","c4da700b":"data.head()","fb5906fd":"#Checking the dtypes after obtaining z-score\ndata.dtypes","d8d87830":"cleanup_nums = {\n               \"education\":     {\"primary\": 1, \"secondary\": 2,\"tertiary\":3,\"unknown\":-1},\n               \"housing\":     {\"yes\": 1, \"no\": 0},\n               \"loan\":        {\"yes\": 1, \"no\": 0},\n               \"default\":        {\"yes\": 1, \"no\": 0},\n               \"marital\":     {\"single\": 1, \"married\": 2,\"divorced\":3},\n               \"poutcome\":     {\"success\": 3, \"other\": 2,\"unknown\":-1,\"failure\":0},\n               \"contact\":{\"cellular\": 1, \"telephone\": 2, \"unknown\": -1},\n               \"Target\":{\"1\":1,\"0\":0}\n                \n                }\n                \ndata.replace(cleanup_nums, inplace=True)\n\nfor categories in data.columns[data.columns=='object']:\n    data[categories]=data[categories].astype(\"int32\")\n\ndata.dtypes","8d62ba6c":"data.head()","10403ace":"floats = data.columns[data.dtypes == 'float64']\n\nfor x in floats:\n    indexNames_larger = data[ data[x]>3].index\n    indexNames_lesser = data[ data[x]<-3].index\n    # Delete these row indexes from dataFrame\n    data.drop(indexNames_larger , inplace=True)\n    data.drop(indexNames_lesser , inplace=True)\ndata.shape\ndata.head()","b29b7480":"categoricals=['month','job']\n\nfor cols in categoricals:\n    data=pd.concat([data,pd.get_dummies(data[cols],prefix=cols)],axis=1)\n    data.drop(cols,axis=1,inplace=True)","cc9c08d9":"data['Target']=data['Target'].astype('category')\n\ndata.dtypes","f0a6bc6d":"import imblearn\nX=data.drop(['Target','duration-z'],axis=1)\nTarget_Variable=data['Target']\nY=Target_Variable\nX.head()\n","8d68ffc9":"Y=Y.astype(\"int32\")","e42495ae":"Y.head()\nY.value_counts()","8b6a52e9":"#Importing necessary libraries\nfrom sklearn.model_selection import train_test_split\n\nXtrain,Xtest,Ytrain,Ytest=train_test_split(X,Y,test_size=0.3,random_state=22)\nprint(Ytrain.value_counts())\nprint(Ytest.value_counts())","a5807590":"from imblearn.over_sampling import SMOTE","251ba7d4":"print(\"Before OverSampling, counts of label '1': {}\".format(sum(Ytrain==1)))\nprint(\"Before OverSampling, counts of label '0': {} \\n\".format(sum(Ytrain==0)))\n\nsm = SMOTE(random_state=2)\nXtrain_res, Ytrain_res = sm.fit_sample(Xtrain, Ytrain)\n\nprint('After OverSampling, the shape of train_X: {}'.format(Xtrain_res.shape))\nprint('After OverSampling, the shape of train_y: {} \\n'.format(Ytrain_res.shape))\n\nprint(\"After OverSampling, counts of label '1': {}\".format(sum(Ytrain_res==1)))\nprint(\"After OverSampling, counts of label '0': {}\".format(sum(Ytrain_res==0)))\n","0095a97d":"log_cols = [\"Classifier\", \"Accuracy\",\"Precision Score\",\"Recall Score\",\"F1-Score\",\"roc-auc_Score\"]\nlog = pd.DataFrame(columns=log_cols)","1326b11f":"#importing necessary libraries\nfrom sklearn.linear_model import LogisticRegression","c3e72be3":"model_log_regression=LogisticRegression(solver=\"liblinear\")","b77fc0d4":"model_log_regression.fit(Xtrain_res,Ytrain_res)\ncoef_df = pd.DataFrame(model_log_regression.coef_)\ncoef_df['intercept'] = model_log_regression.intercept_\nprint(coef_df)","52249031":"#Checking the score for logistic regression\nlogistic_regression_Trainscore=model_log_regression.score(Xtrain_res,Ytrain_res)\nprint(\"The score for Logistic regression-Training Data is {0:.2f}%\".format(logistic_regression_Trainscore*100))\nlogistic_regression_Testscore=model_log_regression.score(Xtest,Ytest)\nprint(\"The score for Logistic regression-Test Data is {0:.2f}%\".format(logistic_regression_Testscore*100))","063d74fa":"#Predicting the Y values\nYpred=model_log_regression.predict(Xtest)\n\n#Misclassification error\nLR_MSE=1-logistic_regression_Testscore\nprint(\"Misclassification error of Logistical Regression model is {0:.1f}%\".format(LR_MSE*100))","ca122e00":"#Confusion Matrix\nfrom sklearn import metrics\ncm=metrics.confusion_matrix(Ytest, Ypred, labels=[1, 0])\n\ndf_cm = pd.DataFrame(cm, index = [i for i in [\"1\",\"0\"]],\n                  columns = [i for i in [\"Predict 1\",\"Predict 0\"]])\nplt.figure(figsize = (7,5))\nsns.heatmap(df_cm, annot=True)\nprint(metrics.classification_report(Ytest, Ypred, digits=3))","d5a8f52c":"accuracy_score=metrics.accuracy_score(Ytest,Ypred)\npercision_score=metrics.precision_score(Ytest,Ypred)\nrecall_score=metrics.recall_score(Ytest,Ypred)\nf1_score=metrics.f1_score(Ytest,Ypred)\nprint(\"The Accuracy of this model is {0:.2f}%\".format(accuracy_score*100))\nprint(\"The Percission of this model is {0:.2f}%\".format(percision_score*100))\nprint(\"The Recall score of this model is {0:.2f}%\".format(recall_score*100))\nprint(\"The F1 score of this model is {0:.2f}%\".format(f1_score*100))","141e38b6":"#AUC ROC curve\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import roc_curve\n\nlogit_roc_auc = roc_auc_score(Ytest, model_log_regression.predict(Xtest))\nfpr, tpr, thresholds = roc_curve(Ytest, model_log_regression.predict_proba(Xtest)[:,1])\nplt.figure()\nplt.plot(fpr, tpr, label='Logistic Regression (area = %0.2f)' % logit_roc_auc)\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.legend(loc=\"lower right\")\nplt.savefig('Log_ROC')\nplt.show()","7f137257":"auc_score = metrics.roc_auc_score(Ytest, model_log_regression.predict_proba(Xtest)[:,1])\nprint(\"The AUC score is {0:.2f}\".format(auc_score))","58419da3":"log_entry = pd.DataFrame([[\"Logistic Regression\",accuracy_score,percision_score,recall_score,f1_score,auc_score]], columns=log_cols)\nlog = log.append(log_entry)\nlog","cd65fe43":"#Importing necessary libraries\nfrom sklearn.tree import DecisionTreeClassifier\n","a9761a75":"#Going with Decision Tree classifier with gini criteria, max_depth is kept at 10 to avoid overfitting of data\ndtc=DecisionTreeClassifier(criterion='gini',random_state = 22,max_depth=10, min_samples_leaf=3,max_leaf_nodes=None)\n","ea4ccd59":"#Fitting the data\ndtc.fit(Xtrain_res,Ytrain_res)","cda001c7":"#Predicting the data\nYpred=dtc.predict(Xtest)","9e53694b":"from sklearn import metrics","ac17f7fc":"#Checking the score for Decision Tree Classifier\nDecision_Tree_Trainscore=dtc.score(Xtrain_res,Ytrain_res)\nprint(\"The score for Decision Tree-Training Data is {0:.2f}%\".format(Decision_Tree_Trainscore*100))\nDecision_Tree_Testscore=dtc.score(Xtest,Ytest)\nprint(\"The score for Decision Tree-Test Data is {0:.2f}%\".format(Decision_Tree_Testscore*100))","9e88c997":"#Misclassification error\nDTC_MSE=1-Decision_Tree_Testscore\nprint(\"Misclassification error of Decision Tree Classification model is {0:.1f}%\".format(DTC_MSE*100))","2eabceb6":"accuracy_score=metrics.accuracy_score(Ytest,Ypred)\npercision_score=metrics.precision_score(Ytest,Ypred)\nrecall_score=metrics.recall_score(Ytest,Ypred)\nf1_score=metrics.f1_score(Ytest,Ypred)\nprint(\"The Accuracy of this model is {0:.2f}%\".format(accuracy_score*100))\nprint(\"The Percission of this model is {0:.2f}%\".format(percision_score*100))\nprint(\"The Recall score of this model is {0:.2f}%\".format(recall_score*100))\nprint(\"The F1 score of this model is {0:.2f}%\".format(f1_score*100))","4894127a":"#Confusion Matrix\ncm=metrics.confusion_matrix(Ytest, Ypred, labels=[1, 0])\n\ndf_cm = pd.DataFrame(cm, index = [i for i in [\"1\",\"0\"]],\n                  columns = [i for i in [\"Predict 1\",\"Predict 0\"]])\nplt.figure(figsize = (7,5))\nsns.heatmap(df_cm, annot=True, cmap=\"YlGnBu\")\nprint(metrics.classification_report(Ytest, Ypred, digits=3))","e4cc18ba":"#AUC ROC curve\n\ndtc_auc = roc_auc_score(Ytest, dtc.predict(Xtest))\nfpr, tpr, thresholds = roc_curve(Ytest, dtc.predict_proba(Xtest)[:,1])\nplt.figure()\nplt.plot(fpr, tpr, label='Decision Tree Classifier (area = %0.2f)' % dtc_auc)\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.legend(loc=\"lower right\")\nplt.savefig('dtc_ROC')\nplt.show()","193298fe":"## Calculating feature importance\nfeat_importance = dtc.tree_.compute_feature_importances(normalize=False)\n\nfeat_imp_dict = dict(zip(X.columns, dtc.feature_importances_))\nfeat_imp = pd.DataFrame.from_dict(feat_imp_dict, orient='index')\nfeat_imp.sort_values(by=0, ascending=False)","e20847d0":"auc_score = metrics.roc_auc_score(Ytest, dtc.predict_proba(Xtest)[:,1])\nprint(\"The AUC score is {0:.2f}\".format(auc_score))","e4129489":"log_entry = pd.DataFrame([[\"Decision Tree Classifier\",accuracy_score,percision_score,recall_score,f1_score,auc_score]], columns=log_cols)\nlog = log.append(log_entry)\nlog","26e667b4":"# Importing libraries\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn import model_selection\n\n# Random Forest Classifier with gini critireon and max_depth of 150 to increase overfitting\nkfold = model_selection.KFold(n_splits=10, random_state=22,shuffle=True)\nrf = RandomForestClassifier(n_estimators = 100,criterion = 'gini', max_depth = 150, min_samples_leaf=1,class_weight='balanced')\nrf = rf.fit(Xtrain_res, Ytrain_res)\nresults = model_selection.cross_val_score(rf, Xtrain_res, Ytrain_res, cv=kfold)\nprint(results)\nYpred = rf.predict(Xtest)\n","9a7b3086":"Random_Forest_Trainscore=rf.score(Xtrain_res,Ytrain_res)\nprint(\"The score for Random Forest-Training Data is {0:.2f}%\".format(Random_Forest_Trainscore*100))\nRandom_Forest_Testscore=rf.score(Xtest,Ytest)\nprint(\"The score for Random Forest-Test Data is {0:.2f}%\".format(Random_Forest_Testscore*100))","d8865d33":"#Misclassification error\nRF_MSE=1-Random_Forest_Testscore\nprint(\"Misclassification error of Random Forest Classification model is {0:.1f}%\".format(RF_MSE*100))","3a5dc779":"accuracy_score=metrics.accuracy_score(Ytest,Ypred)\npercision_score=metrics.precision_score(Ytest,Ypred)\nrecall_score=metrics.recall_score(Ytest,Ypred)\nf1_score=metrics.f1_score(Ytest,Ypred)\nprint(\"The Accuracy of this model is {0:.2f}%\".format(accuracy_score*100))\nprint(\"The Percission of this model is {0:.2f}%\".format(percision_score*100))\nprint(\"The Recall score of this model is {0:.2f}%\".format(recall_score*100))\nprint(\"The F1 score of this model is {0:.2f}%\".format(f1_score*100))\nprint(metrics.classification_report(Ytest,Ypred))","72655471":"#Confusion Matrix\ncm=metrics.confusion_matrix(Ytest, Ypred, labels=[1, 0])\n\ndf_cm = pd.DataFrame(cm, index = [i for i in [\"1\",\"0\"]],\n                  columns = [i for i in [\"Predict 1\",\"Predict 0\"]])\nplt.figure(figsize = (7,5))\nsns.heatmap(df_cm, annot=True, cmap=\"BuPu\")","5fd3217c":"#AUC ROC curve\n\nrf_auc = roc_auc_score(Ytest, rf.predict(Xtest))\nfpr, tpr, thresholds = roc_curve(Ytest, rf.predict_proba(Xtest)[:,1])\nplt.figure()\nplt.plot(fpr, tpr, label='Random Forest Classifier (area = %0.2f)' % rf_auc)\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.legend(loc=\"lower right\")\nplt.savefig('rf_ROC')\nplt.show()\nauc_score = metrics.roc_auc_score(Ytest, rf.predict_proba(Xtest)[:,1])\nprint(\"The AUC score is {0:.2f}\".format(auc_score))","3fd3c829":"log_entry = pd.DataFrame([[\"Random Forest Classifier\",accuracy_score,percision_score,recall_score,f1_score,auc_score]], columns=log_cols)\nlog = log.append(log_entry)\nlog","851b0a00":"# Importing libraries\nfrom sklearn.ensemble import BaggingClassifier\n\nbg = BaggingClassifier(n_estimators=100, max_samples= .9, bootstrap=True, oob_score=True, random_state=22)\nbg = bg.fit(Xtrain_res, Ytrain_res)\nYpred = bg.predict(Xtest)\n","32320ecd":"Bagging_Trainscore=bg.score(Xtrain_res, Ytrain_res)\nprint(\"The score for Bagging-Training Data is {0:.2f}%\".format(Bagging_Trainscore*100))\nBagging_Testscore=bg.score(Xtest,Ytest)\nprint(\"The score for Bagging-Test Data is {0:.2f}%\".format(Bagging_Testscore*100))","91fc148c":"#Misclassification error\nBG_MSE=1-Bagging_Testscore\nprint(\"Misclassification error of Bagging Classification model is {0:.1f}%\".format(BG_MSE*100))","ad2556e6":"accuracy_score=metrics.accuracy_score(Ytest,Ypred)\npercision_score=metrics.precision_score(Ytest,Ypred)\nrecall_score=metrics.recall_score(Ytest,Ypred)\nf1_score=metrics.f1_score(Ytest,Ypred)\nprint(\"The Accuracy of this model is {0:.2f}%\".format(accuracy_score*100))\nprint(\"The Percission of this model is {0:.2f}%\".format(percision_score*100))\nprint(\"The Recall score of this model is {0:.2f}%\".format(recall_score*100))\nprint(\"The F1 score of this model is {0:.2f}%\".format(f1_score*100))\nprint(metrics.classification_report(Ytest,Ypred))","00090126":"#Confusion Matrix\ncm=metrics.confusion_matrix(Ytest, Ypred, labels=[1, 0])\n\ndf_cm = pd.DataFrame(cm, index = [i for i in [\"1\",\"0\"]],\n                  columns = [i for i in [\"Predict 1\",\"Predict 0\"]])\nplt.figure(figsize = (7,5))\nsns.heatmap(df_cm, annot=True, cmap=\"Greens\")","38262219":"#AUC ROC curve\n\nbg_auc = roc_auc_score(Ytest, bg.predict(Xtest))\nfpr, tpr, thresholds = roc_curve(Ytest, bg.predict_proba(Xtest)[:,1])\nplt.figure()\nplt.plot(fpr, tpr, label='Bagging Classifier (area = %0.2f)' % bg_auc)\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.legend(loc=\"lower right\")\nplt.savefig('bg_ROC')\nplt.show()\nauc_score = metrics.roc_auc_score(Ytest, bg.predict_proba(Xtest)[:,1])\nprint(\"The AUC score is {0:.2f}\".format(auc_score))","8b2be331":"log_entry = pd.DataFrame([[\"Bagging Classifier\",accuracy_score,percision_score,recall_score,f1_score,auc_score]], columns=log_cols)\nlog = log.append(log_entry)\nlog","40b628d1":"#Importing necessary libraries\nfrom sklearn.ensemble import AdaBoostClassifier\nab = AdaBoostClassifier(n_estimators= 100, learning_rate=0.5, random_state=22)\nab = ab.fit(Xtrain_res, Ytrain_res)","afad1ef6":"Ypred=ab.predict(Xtest)","f80b7221":"Adaboosting_Trainscore=ab.score(Xtrain_res,Ytrain_res)\nprint(\"The score for Adaboosting-Training Data is {0:.2f}%\".format(Adaboosting_Trainscore*100))\nAdaboosting_Testscore=ab.score(Xtest,Ytest)\nprint(\"The score for Adaboosting-Test Data is {0:.2f}%\".format(Adaboosting_Testscore*100))","c7e2024b":"#Misclassification error\nAB_MSE=1-Adaboosting_Testscore\nprint(\"Misclassification error of Bagging Classification model is {0:.1f}%\".format(AB_MSE*100))","93003e95":"accuracy_score=metrics.accuracy_score(Ytest,Ypred)\npercision_score=metrics.precision_score(Ytest,Ypred)\nrecall_score=metrics.recall_score(Ytest,Ypred)\nf1_score=metrics.f1_score(Ytest,Ypred)\nprint(\"The Accuracy of this model is {0:.2f}%\".format(accuracy_score*100))\nprint(\"The Percission of this model is {0:.2f}%\".format(percision_score*100))\nprint(\"The Recall score of this model is {0:.2f}%\".format(recall_score*100))\nprint(\"The F1 score of this model is {0:.2f}%\".format(f1_score*100))\nprint(metrics.classification_report(Ytest,Ypred))","cc72ee6c":"#Confusion Matrix\ncm=metrics.confusion_matrix(Ytest, Ypred, labels=[1, 0])\n\ndf_cm = pd.DataFrame(cm, index = [i for i in [\"1\",\"0\"]],\n                  columns = [i for i in [\"Predict 1\",\"Predict 0\"]])\nplt.figure(figsize = (7,5))\nsns.heatmap(df_cm, annot=True, cmap=\"Reds\")","ac9697f9":"#AUC ROC curve\n\nab_auc = roc_auc_score(Ytest, ab.predict(Xtest))\nfpr, tpr, thresholds = roc_curve(Ytest, ab.predict_proba(Xtest)[:,1])\nplt.figure()\nplt.plot(fpr, tpr, label='Bagging Classifier (area = %0.2f)' % ab_auc)\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.legend(loc=\"lower right\")\nplt.savefig('ab_ROC')\nplt.show()\nauc_score = metrics.roc_auc_score(Ytest, ab.predict_proba(Xtest)[:,1])\nprint(\"The AUC score is {0:.2f}\".format(auc_score))","0bcdd201":"log_entry = pd.DataFrame([[\"Adaptive Boosting Classifier\",accuracy_score,percision_score,recall_score,f1_score,auc_score]], columns=log_cols)\nlog = log.append(log_entry)\nlog","0c6594b6":"from sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.ensemble import GradientBoostingClassifier\nnum_estimators = [100,200]\nlearn_rates = [0.2,0.3]\n\nscoreFunction = {\"recall\": \"recall\", \"precision\": \"precision\"}\n\nparam_grid = {'n_estimators': num_estimators,\n              'learning_rate': learn_rates,\n}\n\nrandom_search =RandomizedSearchCV(GradientBoostingClassifier(loss='deviance'), param_grid, scoring = scoreFunction,               \n                                       refit = \"precision\", random_state=1, n_iter=100, cv=5, verbose=0, n_jobs=-1)\n\nrandom_search.fit(Xtrain_res, Ytrain_res)","eb63eaf9":"random_search.best_params_","3fc21286":"# Importing necessary libraries and fitting the data\n\nfrom sklearn.ensemble import GradientBoostingClassifier\ngb = GradientBoostingClassifier(n_estimators = 200, learning_rate = 0.3, random_state=22)\ngb = gb.fit(Xtrain_res, Ytrain_res)\nYpred = gb.predict(Xtest)","3172d0c3":"Gradient_Booosting_Trainscore=gb.score(Xtrain_res,Ytrain_res)\nprint(\"The score for Gradient_Booosting-Training Data is {0:.2f}%\".format(Gradient_Booosting_Trainscore*100))\nGradient_Booosting_Testscore=gb.score(Xtest,Ytest)\nprint(\"The score for Gradient_Booostinge-Test Data is {0:.2f}%\".format(Gradient_Booosting_Testscore*100))","da93dd2e":"#Misclassification error\nGB_MSE=1-Gradient_Booosting_Testscore\nprint(\"Misclassification error of Gradient Boosting Classification model is {0:.1f}%\".format(GB_MSE*100))","29c11336":"accuracy_score=metrics.accuracy_score(Ytest,Ypred)\npercision_score=metrics.precision_score(Ytest,Ypred)\nrecall_score=metrics.recall_score(Ytest,Ypred)\nf1_score=metrics.f1_score(Ytest,Ypred)\nprint(\"The Accuracy of this model is {0:.2f}%\".format(accuracy_score*100))\nprint(\"The Percission of this model is {0:.2f}%\".format(percision_score*100))\nprint(\"The Recall score of this model is {0:.2f}%\".format(recall_score*100))\nprint(\"The F1 score of this model is {0:.2f}%\".format(f1_score*100))\nprint(metrics.classification_report(Ytest,Ypred))","77f0d3cb":"#Confusion Matrix\ncm=metrics.confusion_matrix(Ytest, Ypred, labels=[1, 0])\n\ndf_cm = pd.DataFrame(cm, index = [i for i in [\"1\",\"0\"]],\n                  columns = [i for i in [\"Predict 1\",\"Predict 0\"]])\nplt.figure(figsize = (7,5))\nsns.heatmap(df_cm, annot=True, cmap=\"Blues\")\n","1e8e4ff4":"#AUC ROC curve\n\ngb_auc = roc_auc_score(Ytest, gb.predict(Xtest))\nfpr, tpr, thresholds = roc_curve(Ytest, gb.predict_proba(Xtest)[:,1])\nplt.figure()\nplt.plot(fpr, tpr, label='Gradient Boosting Classifier (area = %0.2f)' % gb_auc)\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.legend(loc=\"lower right\")\nplt.savefig('gb_ROC')\nplt.show()\nauc_score = metrics.roc_auc_score(Ytest, gb.predict_proba(Xtest)[:,1])\nprint(\"The AUC score is {0:.2f}\".format(auc_score))","5de732d2":"log_entry = pd.DataFrame([[\"Gradient Boosting Classifier\",accuracy_score,percision_score,recall_score,f1_score,auc_score]], columns=log_cols)\nlog = log.append(log_entry)\nlog","962827b8":"# The confusion matrix\n\nTrue Positives (TP): we correctly predicted that they have taken Term Deposit is 490\n\nTrue Negatives (TN): we correctly predicted that they have not taken Term Deposit is 10000\n\nFalse Positives (FP): we incorrectly predicted that have taken Term Deposit (a \"Type I error\") 550 Falsely predict positive Type I error\n\nFalse Negatives (FN): we incorrectly predicted that they have not taken Term Deposit  (a \"Type II error\") 780 Falsely predict negative Type II error","c752f1d3":"## Ensemble Technique- Bagging Classifier","fc50ab6a":"#### Target column is highly imbalanced with only close the 5000 people have taken term deposits","51a11c42":"#### pdays doesnot seem to influence the target","eb118f57":"#### previous doesnot seem to influence the target","78b949ac":"### Bagging Classifier Results:\n\n#### The Accuracy of this model is 88.62%\n#### The Percission of this model is 44.88%\n#### The Recall score of this model is 35.51%\n#### The F1 score of this model is 39.65%\n#### The AUC score of this model is 76%","3d4b1bf8":"### One Hot encoding is performed on Month and Job columns","414fbc2d":"## Final Insights\n#### 1. The aim of the data-set is to predict customers who would subscribe for a term deposit\n#### 2. Data had 16 independent variable and 1 target variable\n#### 3. Outliers were handled by converting numerical variables to z-score and removing rows greater than +\/- 3\n#### 4. Label encoding and one-hot encoding was employed for Categorical columns\n#### 5. Since the target variable was highly imbalanced SMOTE oversampling on the training data was employed(accuracy was better without oversampling but f1 and AUC score were low)\n#### 6. Out of all models tried on, Gradient Boosing is considered the best as it has good Accuracy, f1 and AUC scores\n#### 7. Hyper-parameterization tuning was done on Gradient Boosting to find the best parameters('n_estimators': 200, 'learning_rate': 0.3). RamdomSearchCV was employed.\n#### 8. All Ensemble Techniques were better than the base models(Standard Classification Algorithm) Logistic Regression and Decision Tree Classifier","50a3f788":"#### Around 5000 people have taken term deposits and 40000 have not taken term deposits. That is around 12.5% success rate.","2dd5f7b4":"##### Mean of Campaign, Previous, Balance,  Duration,  pdays is much more than the median which infers that they have outliers","1aa65150":"## Base Model- Decision Tree Classifier","e393dda4":"##### The confusion matrix\n\nTrue Positives (TP): we correctly predicted that they have taken Term Deposit is 570\n\nTrue Negatives (TN): we correctly predicted that they have not taken Term Deposit is 9800\n\nFalse Positives (FP): we incorrectly predicted that have taken Term Deposit (a \"Type I error\") 1000 Falsely predict positive Type I error\n\nFalse Negatives (FN): we incorrectly predicted that they have not taken Term Deposit  (a \"Type II error\") 700 Falsely predict negative Type II error","1de3a118":"##### Blue collar and Management jobs are the highest followed by Technician","292d9f80":"## Exploratory Data Analytics","20f23da3":"## Multi-Variate Analysis","fc17ac19":"### Changing the categorical variables to numerical representation\n","cb6db1f9":"## Ensemble Technique- Gradient Boosting","29d31e9c":"### Adaboosting Classifier Results:\n\n#### The Accuracy of this model is 84.57%\n#### The Percission of this model is 34.85%\n#### The Recall score of this model is 53.54%\n#### The F1 score of this model is 42.22%\n#### The AUC score is 77%","e2fe9cd1":"#### Age doesnot seem to influence the target","03c34b46":"##### People with matital status as \"single\" invest in term deposit by their total percentage\n","8a35d190":"## Strategy 1: Removing the outliers from numerical columns\n### Identifying the z-score for numerical columns","57a752fd":"### Hyper Parameterization- Tuning the model with RandomSearch CV","8884a7aa":"#### Duration does have good corelation with the Target","cf3258d7":"\n\n\n# The confusion matrix\n\nTrue Positives (TP): we correctly predicted that they have taken Term Deposit is 450\n\nTrue Negatives (TN): we correctly predicted that they have not taken Term Deposit is 10000\n\nFalse Positives (FP): we incorrectly predicted that have taken Term Deposit (a \"Type I error\") 550 Falsely predict positive Type I error\n\nFalse Negatives (FN): we incorrectly predicted that they have not taken Term Deposit  (a \"Type II error\") 820 Falsely predict negative Type II error","3dc78370":"#### May was the most last contacted month\n#### Most of the outcome of the previous capaign was others","f5602176":"## Strategy 2: Oversampling the training data to balance the Target column.","92f7ff52":"### Decision Tree Results:\n#### The Accuracy of this model is 85.92%\n#### The Percission of this model is 36.35%\n#### The Recall score of this model is 44.88%\n#### The F1 score of this model is 40.17%\n#### The AUC score of this model is 72%","64fa273b":"##### we can infer that people with secondary  and tertiary education  opt for term deposit comparitively","9f21346c":"## Ensemble Technique- Random Forest Classifier","00b080b2":"#### More the number of campaigns, lesser the customers who have subscribed to Term deposits","b1dbd761":"\n##### There are object data-types in the data-set which would need conversion at the latter stage of our analysis","68bfce2d":"##### Correlation between pdays and previous column is better where as all other independent columns has very less correlation\n##### There are no strong linear relationships between any two variables except Target and duration\n","b9825bbb":"### Removing all columns with z-score greater and lesser than 3 and -3 respectivley as the values are outliers\n","efed684b":"##### The confusion matrix\n\nTrue Positives (TP): we correctly predicted that they have taken Term Deposit is 500\n\nTrue Negatives (TN): we correctly predicted that they have not taken Term Deposit is 10000\n\nFalse Positives (FP): we incorrectly predicted that have taken Term Deposit (a \"Type I error\") 480 Falsely predict positive Type I error\n\nFalse Negatives (FN): we incorrectly predicted that they have not taken Term Deposit  (a \"Type II error\") 770 Falsely predict negative Type II error","0cd5cce5":"## Preparing the independent and target variables as X and Y\n### Dropping duration column to get a realistic model\n","9136ac2b":"#### From the feature importance dataframe we can infer that campaign,housing are the variables that impact term depositors","e4123982":"\n## Attribute information\n### Input variables:\n\n#### 1 - age (numeric)\n#### 2 - job : type of job (categorical)\n#### 3 - marital : marital status (categorical)\n#### 4 - education (categorical)\n#### 5 - default: has credit in default? (categorical)\n#### 6 - balance(Numeric)\n#### 7 - housing: has housing loan? (categorical)\n#### 8 - loan: has personal loan? (categorical)\n#### 9 - contact: contact communication type (categorical) \n#### 10 - day: last contact day of the month (numberical)\n#### 11 - month: last contact month of year (categorical)\n#### 12 - duration: last contact duration, in seconds (numeric). \n\n##### Important note: this attribute highly affects the output target (e.g., if duration=0 then y='no'). Yet, the duration is not known before a call is performed. Also, after the end of the call y is obviously known. Thus, this input should only be included for benchmark purposes and should be discarded if the intention is to have a realistic predictive model.\n\n#### 13 - campaign: number of contacts performed during this campaign and for this client (numeric)\n#### 14 - pdays: number of days that passed by after the client was last contacted from a previous campaign (numeric; -1 means client was not previously contacted)\n#### 15 - previous: number of contacts performed before this campaign and for this client (numeric)\n#### 16 - poutcome: outcome of the previous marketing campaign (categorical)\n\n### Output variable (desired target):\n#### 21 - Target - has the client subscribed a term deposit? (binary: 'yes','no')\n","dafe60ce":"#### Cellular way of contact is higher than other methods\n#### Most of the customers have Secondary education followed by Tertiary education\n#### Most of the customers in this data set are married\n#### Most of the customers have not defaulted on their credit\n#### People have housing loan is more\n#### People not having personal loan is higher than people having personal loan","3eb1ed4d":"##### There are no NaN values present in the data-set","a3d1bc24":"##### There are no null values present in the data-set","9e7f2342":"###### The confusion matrix\n\nTrue Positives (TP): we correctly predicted that they have taken Term Deposit is 680\n\nTrue Negatives (TN): we correctly predicted that they have not taken Term Deposit is 9500\n\nFalse Positives (FP): we incorrectly predicted that have taken Term Deposit (a \"Type I error\") 1300 Falsely predict positive Type I error\n\nFalse Negatives (FN): we incorrectly predicted that they have not taken Term Deposit  (a \"Type II error\") 590 Falsely predict negative Type II error","5c1f29b9":"#### After removing outliers we have 36155 records who have not purchased term deposit and 4054 records which have purchased term deposits","7d4ce845":"## Bi-variate Aanalysis","085b4019":"# Preparing the data for analytics","0573e0af":"### Split the data into training and test set in the ratio of 70:30 (Training:Test) based on dependent and independent variables.","04ae2c78":"##### Average age is between 30 and 50 Years and there are some outliers\n##### Average duration is between 0 and 800. Huge number of outliers and are right skewed\n#####  Balance  is right skewed and have huge number of outliers ","2e95e37a":"\n## Reading data file and cheking data","c5177027":"### Logistic Regression Results:\n#### The Accuracy of this model is 83.87%\n#### The Percission of this model is 32.45%\n#### The Recall score of this model is 49.21%\n#### The F1 score of this model is 39.11%\n#### The AUC Score of this model is 75%","f14cc02c":"# The confusion matrix\n\nTrue Positives (TP): we correctly predicted that they have taken Term Deposit is 620\n\nTrue Negatives (TN): we correctly predicted that they have not taken Term Deposit is 9500\n\nFalse Positives (FP): we incorrectly predicted that have taken Term Deposit (a \"Type I error\") 1300 Falsely predict positive Type I error\n\nFalse Negatives (FN): we incorrectly predicted that they have not taken Term Deposit  (a \"Type II error\") 640 Falsely predict negative Type II error","464ad063":"##### Average campaigns are around 2 with good number of outliers\n#### pdays and previous data have huge number of outliers","901f70bb":"### Univariate Analysis","29e9c6c4":"#Distribution of continous data\n\nplt.figure(figsize=(30,6))\n\n#Subplot 1\nplt.subplot(1,3,1)\nplt.title('Campaign')\nsns.distplot(data['campaign'],color='red')\n\n#Subplot 2\nplt.subplot(1,3,2)\nplt.title('P-days')\n#sns.distplot(data['pdays'],color='blue')\n\n#Subplot 3\nplt.subplot(1,3,3)\nplt.title('Previous')\nsns.distplot(data['previous'],color='green')\n\n\n\nplt.figure(figsize=(30,6))\n\n#Subplot 1- Boxplot\nplt.subplot(1,3,1)\nplt.title('Campaign')\nsns.boxplot(data['campaign'],orient='horizondal',color='red')\n\n#Subplot 2\nplt.subplot(1,3,2)\nplt.title('P-days')\n#sns.boxplot(data['pdays'],orient='horizondal',color='blue')\n\n#Subplot 3\nplt.subplot(1,3,3)\nplt.title('Previous')\nsns.boxplot(data['previous'],orient='horizondal',color='green')\n","a45d5f64":"##### Increase in duration of last call shows the variation in target output column","ff3f2b1e":"## Ensemble Technique- AdaBoost Classifier","19b798c4":"## Base Model- Logistic Regression","5be3b170":"#### Balance has a slight influence on the target","27e3edf8":"##### We can infer that the data-set has 45211 records with 16 independent variables and 1 target variable where as 39922 people have not subscribed to term deposit whereas 5289 people have subscribed to term deposit"}}