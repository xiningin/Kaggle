{"cell_type":{"1d80a637":"code","8c340ab9":"code","fd735883":"code","a3047169":"code","d8fa9120":"code","029d7228":"code","3e01a244":"code","91a8a6ba":"code","0b37b07d":"code","df2a661f":"code","771203f0":"code","930d7f54":"code","aa38394f":"code","b3334c99":"code","4952c91b":"code","3862b72b":"code","fcdbb755":"code","5a69d673":"code","181b9550":"code","d91ebe5c":"code","37ec9e85":"code","43681456":"code","982079d7":"code","59bf5f89":"code","3da8e318":"code","b60e57fa":"code","c982cb1f":"code","dfbcf246":"code","580698dd":"code","4a289ab9":"code","2d7d8c5a":"code","f3a3e503":"code","b2bb9fd4":"code","57c65fb2":"code","660cf59c":"code","621cc4f0":"code","d50c4fb3":"code","a3b3bbf7":"code","d62fc370":"code","930f1d1c":"code","ad5d3410":"code","6cc966f7":"code","d3a73eb3":"code","fd4db675":"code","8084484c":"code","cf85b2f1":"code","eef068dc":"code","a4591072":"code","2efe9e9e":"code","1bf63272":"code","c39d002b":"code","51bf49e7":"code","4188ece5":"code","dbb9c57e":"code","6e9fec7e":"code","db51c9a1":"code","31d95e7a":"code","71a843e7":"code","3016652c":"code","7812c625":"code","0db95970":"code","9e8fb9a7":"code","9dad2491":"code","5a3605b7":"code","8fb4c783":"code","7f8d549e":"code","8dd997f7":"code","051c6d11":"code","9d2b65d9":"code","c42190f4":"markdown","caedb8b6":"markdown","86292017":"markdown","2f41f634":"markdown","229dbf8e":"markdown","9486fbd5":"markdown","946cdee8":"markdown","2449025a":"markdown","0387e8ff":"markdown","254fd7c4":"markdown","28dc590b":"markdown","b83bf0c1":"markdown","2bfd7852":"markdown","be133549":"markdown","15fb75f2":"markdown","b6a75e04":"markdown","4cc679fe":"markdown","6f4c50f9":"markdown","4861ef8d":"markdown","b8d8504a":"markdown","5dc20cf8":"markdown","2b07258e":"markdown","f794fdf9":"markdown","ecf78de6":"markdown","6f3c571c":"markdown","4f587b3c":"markdown","540f7560":"markdown","c843ea44":"markdown","f047e775":"markdown","ab2751e7":"markdown","5bdd86fe":"markdown","9b708e00":"markdown","3e92e9d4":"markdown","bcfe3aa7":"markdown","193f0ccf":"markdown","3fe69e30":"markdown"},"source":{"1d80a637":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport math\n\nfrom datetime import datetime\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import PowerTransformer\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.pipeline import Pipeline\n\nfrom numpy import unique\nfrom numpy import where\n\n#Machine Learning models\n# k-means clustering\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_score\nfrom kneed import KneeLocator\n\nfrom sklearn.metrics import accuracy_score\nfrom xgboost import XGBRegressor\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split\n\n\nimport warnings\nwarnings.filterwarnings('ignore')","8c340ab9":"#Load the dataset considering the datetime for the Dt_Customer column\ndateparse = lambda dates: datetime.strptime(dates, '%d-%m-%Y')\n\ndata = pd.read_csv('\/kaggle\/input\/customer-personality-analysis\/marketing_campaign.csv', sep='\\t', parse_dates = ['Dt_Customer'], date_parser = dateparse)\ndata.head()","fd735883":"#Show missing values percentages and the type\ntype_and_missing = pd.concat([data.isna().sum().sort_values(ascending = False), data.dtypes], axis=1, keys=['Total', 'Type'])\ntype_and_missing[type_and_missing['Total'] > 0]","a3047169":"data.Income = data.Income.fillna(data.Income.mean())","d8fa9120":"#Describe 2 features with same value for all entries, that is, feartues that tell nothing and we can drop than.\nprint('Z_Revenue number of unique Values: ',(data.Z_Revenue.nunique()), ', That is: ', (data.Z_Revenue.unique()))\nprint('Z_CostContact number of unique Values: ',data.Z_CostContact.nunique(), ', That is: ', (data.Z_CostContact.unique()))\n\n#drop those uninformative features\ndata = data.drop('Z_CostContact', axis=1).drop('Z_Revenue', axis=1)","029d7228":"#Correlation heat Map\n\nfig,ax=plt.subplots(1,1,figsize=(20,15))\nsns.heatmap(data.corr(), annot=True, label='Spearman Correlation Heat Map')","3e01a244":"plt.figure(figsize=(15,8))\nplt.title('How Income increase by Education?')\nsns.barplot(x='Education', y='Income', data=data, order=['Basic', '2n Cycle', 'Graduation', 'Master', 'PhD'])","91a8a6ba":"data.groupby('Education')['Income'].mean()","0b37b07d":"#Income Analysis\nplt.figure(figsize=(15,5))\nsns.boxplot(x=data['Income'], color='red')","df2a661f":"sns.histplot(x=data.Income, kde = True)","771203f0":"data.Income.describe()","930d7f54":"#Detect the outliers using IQR technique\n\nq1 = data['Income'].quantile(0.25)\nq3 = data['Income'].quantile(0.75)\niqr = q3 - q1\n \nprint(\"Old Shape: \", data.shape)\n \n#Upper and Lower Limits\nupper = q3 + 1.5 * iqr\nlower = q1 - 1.5 * iqr\n\nprint(\"Lower bound:\", lower)\nprint(\"Upper bound:\", upper)\n \n# new_df = data[data['Income'] > upper]\n# new_df = data[data['Income']  < lower]\n\n# Capping (above or below certain limit, all Incomes will be the same)\n\ndata['Income'] = np.where(data['Income'] > upper, upper, \n                 np.where(data['Income'] < lower, lower,\n                  data['Income']))\n\nprint(\"New Shape: \", data.shape)\n\n#distribution of Income without the greater outliers\nplt.figure(figsize=(15,3))\nsns.boxplot(x=data['Income'], color='red')\n","aa38394f":"#distribution of Income without the greater outliers\nincome = data.loc[data['Income'] < 300000]['Income']\nsns.histplot(x=income, kde = True)","b3334c99":"data['Total_Spent'] = data['MntWines'] + data['MntFruits'] + data['MntMeatProducts'] + data['MntFishProducts'] + data['MntSweetProducts'] + data['MntGoldProds']\ndata['Total_Purchases'] = data['NumDealsPurchases'] + data['NumWebPurchases'] + data['NumCatalogPurchases'] + data['NumStorePurchases']","4952c91b":"#Total Spent and Total Purchases Vs Income\nfig, axes = plt.subplots(1, 2, figsize=(15,5))\nfig.suptitle('Total Spent and Total Purchases Vs Income')\n\nsns.regplot(x='Total_Spent', y='Income', data=data, ax=axes[0], line_kws={\"color\": \"red\"})\n\nsns.regplot(x='Total_Purchases', y='Income', data=data, ax=axes[1], line_kws={\"color\": \"red\"})","3862b72b":"#Total Spent and Total Purchases Vs Recency and Response\nfig, axes = plt.subplots(1, 2, figsize=(15,5))\nfig.suptitle('Total Spent Vs Recency and Response')\n\nsns.regplot(x='Total_Spent', y='Recency', data=data, ax=axes[0], line_kws={\"color\": \"red\"})\n\nsns.regplot(x='Total_Spent', y='Response', data=data, ax=axes[1], line_kws={\"color\": \"red\"})","fcdbb755":"# Analyse the acceptance of each level of the campaing\nacceptedConcat = data[['AcceptedCmp1', 'AcceptedCmp2', 'AcceptedCmp3', 'AcceptedCmp4', 'AcceptedCmp5']]\nacceptedConcat = acceptedConcat.apply(pd.DataFrame.sum)\n\n\nplt.figure(figsize=(15,8))\nplt.title('How many accepted the campaings in the first, second, third...attempt')\nsns.barplot(x=acceptedConcat.index, y=acceptedConcat)","5a69d673":"#Acceptance correlations\nfig, axes = plt.subplots(3, 2, sharex=True, figsize=(10,5))\nfig.suptitle('Acceptance correlations with the Total Spent')\n\nsns.regplot(x='AcceptedCmp1', y='Total_Spent', data=data, ax=axes[0,0], line_kws={\"color\": \"red\"})\n\nsns.regplot(x='AcceptedCmp2', y='Total_Spent', data=data, ax=axes[0,1], line_kws={\"color\": \"red\"})\n\nsns.regplot(x='AcceptedCmp3', y='Total_Spent', data=data, ax=axes[1,0], line_kws={\"color\": \"red\"})\n\nsns.regplot(x='AcceptedCmp4', y='Total_Spent', data=data, ax=axes[1,1], line_kws={\"color\": \"red\"})\n\nsns.regplot(x='AcceptedCmp5', y='Total_Spent', data=data, ax=axes[2,0], line_kws={\"color\": \"red\"})","181b9550":"data.Year_Birth.describe()","d91ebe5c":"#Remove outliers\n#Detect the outliers using  Z-score treatment\n\nlowerYear = math.floor(data['Year_Birth'].mean() - 3 * data['Year_Birth'].std())\nupperYear = round(data['Year_Birth'].mean() + 3 * data['Year_Birth'].std())\n\nprint(\"Upper Ok Year: \", upperYear)\nprint(\"Lower Ok Year: \", lowerYear) \nprint(\"Old Shape: \", data.shape)\n\n\n#find the outliers on the dataset\nprint('Number of record above or below our outliers indexes: ', len(data[(data['Year_Birth'] > upperYear) | (data['Year_Birth'] < lowerYear)]))\n \n#Removing outliers (trimming)\ndata = data[(data['Year_Birth'] > lowerYear) & (data['Year_Birth'] < upperYear)]\nprint(\"New Shape: \", data.shape)\n\nsns.boxplot(x=data['Year_Birth'], color='red')","37ec9e85":"data.Year_Birth.describe()","43681456":"#distplot for Year of Birth\n\n# plt.figure(figsize=(10,5))\nfig, axes = plt.subplots(1, 2, sharex=False, figsize=(15,5))\nsns.histplot(x=data['Year_Birth'], ax=axes[0], kde = True)\n\nsns.regplot(y='Income', x='Year_Birth', data=data, ax=axes[1], line_kws={\"color\": \"red\"})\n","982079d7":"currentYear = datetime.now().year\ncurrentYear","59bf5f89":"data['Age'] = (currentYear - data['Year_Birth']).astype('int')\ndata.Age.describe()","3da8e318":"#Create discretization of Age feature\n#Group (cut) severity by classes - Apply Binning\nlabels = [0,1,2,3]\n\ndata['Age_score'] = pd.cut(data['Age'], bins=4, labels=labels, right=False)\ndata.head()","b60e57fa":"#Age versus webPurchases\nfig, axes = plt.subplots(2, 2, sharex=True, figsize=(10,5))\nfig.suptitle('Age Vs Number of Purchases by Channel')\n\nsns.regplot(x='NumWebPurchases', y='Age', data=data, ax=axes[0,0], line_kws={\"color\": \"red\"})\n\nsns.regplot(x='NumDealsPurchases', y='Age', data=data, ax=axes[0,1], line_kws={\"color\": \"red\"})\n\nsns.regplot(x='NumCatalogPurchases', y='Age', data=data, ax=axes[1,0], line_kws={\"color\": \"red\"})\n\nsns.regplot(x='NumStorePurchases', y='Age', data=data, ax=axes[1,1], line_kws={\"color\": \"red\"})","c982cb1f":"#Kidshome versus Products\nfig, axes = plt.subplots(3, 2, sharex=True, figsize=(10,5))\nfig.suptitle('Age Vs Amount Spent on Products by Category')\n\nsns.regplot(y='MntWines', x='Age', data=data, ax=axes[0,0], line_kws={\"color\": \"red\"})\n\nsns.regplot(y='MntFruits', x='Age', data=data, ax=axes[0,1])\n\nsns.regplot(y='MntMeatProducts', x='Age', data=data, ax=axes[1,0])\n\nsns.regplot(y='MntFishProducts', x='Age', data=data, ax=axes[1,1])\n\nsns.regplot(y='MntSweetProducts', x='Age', data=data, ax=axes[2,0])\n\nsns.regplot(y='MntGoldProds', x='Age', data=data, ax=axes[2,1])\n","dfbcf246":"# People with kids at home\ngrouped = data.Kidhome.value_counts(normalize=True)*100\ngrouped.plot.bar(title='Quantity of children at home')","580698dd":"#Kidshome versus webPurchases\nfig, axes = plt.subplots(2, 2, sharex=True, figsize=(10,5))\nfig.suptitle('Kids Home Vs Amount Spent on Purchases by Channel')\n\nsns.regplot(y='NumWebPurchases', x='Kidhome', data=data, ax=axes[0,0])\n\nsns.regplot(y='NumDealsPurchases', x='Kidhome', data=data, ax=axes[0,1])\n\nsns.regplot(y='NumCatalogPurchases', x='Kidhome', data=data, ax=axes[1,0])\n\nsns.regplot(y='NumStorePurchases', x='Kidhome', data=data, ax=axes[1,1])","4a289ab9":"#Kidshome versus Products\nfig, axes = plt.subplots(3, 2, sharex=True, figsize=(10,5))\nfig.suptitle('Kids Home Vs Amount Spent on Products by Category')\n\nsns.regplot(y='MntWines', x='Kidhome', data=data, ax=axes[0,0])\n\nsns.regplot(y='MntFruits', x='Kidhome', data=data, ax=axes[0,1])\n\nsns.regplot(y='MntMeatProducts', x='Kidhome', data=data, ax=axes[1,0])\n\nsns.regplot(y='MntFishProducts', x='Kidhome', data=data, ax=axes[1,1])\n\nsns.regplot(y='MntSweetProducts', x='Kidhome', data=data, ax=axes[2,0])\n\nsns.regplot(y='MntGoldProds', x='Kidhome', data=data, ax=axes[2,1])\n","2d7d8c5a":"#Kidshome versus Totals\nfig, axes = plt.subplots(2, 2, sharex=True, figsize=(10,5))\nfig.suptitle('Kids Home Vs Amount Spent on Products by Category')\n\nsns.regplot(y='Total_Spent', x='Kidhome', data=data, ax=axes[0,0])\n\nsns.regplot(y='Total_Spent', x='Kidhome', data=data, ax=axes[0,1])\n\nsns.regplot(y='Total_Purchases', x='Kidhome', data=data, ax=axes[1,0])\n\nsns.regplot(y='Total_Purchases', x='Kidhome', data=data, ax=axes[1,1])\n","f3a3e503":"fig, axes = plt.subplots(3, 2, sharex=True, figsize=(10,5))\nfig.suptitle('Marital Status Vs Amount Spent on Products by Category')\n\nsns.barplot(y='MntWines', x='Marital_Status', data=data, ax=axes[0,0])\n\nsns.barplot(y='MntFruits', x='Marital_Status', data=data, ax=axes[0,1])\n\nsns.barplot(y='MntMeatProducts', x='Marital_Status', data=data, ax=axes[1,0])\n\nsns.barplot(y='MntFishProducts', x='Marital_Status', data=data, ax=axes[1,1])\n\nsns.barplot(y='MntSweetProducts', x='Marital_Status', data=data, ax=axes[2,0])\n\nsns.barplot(y='MntGoldProds', x='Marital_Status', data=data, ax=axes[2,1])\n","b2bb9fd4":"#Show values for Absurd Matiral status that seem strange\ndata[data['Marital_Status'] == 'Absurd']","57c65fb2":"#Drop rows with Absurd as Marital Status\ndata = data.drop(data.loc[data['Marital_Status'] == 'Absurd'].index).drop(data.loc[data['Marital_Status'] == 'YOLO'].index)","660cf59c":"#Show Plots after excluding \"Absurd\" from Marital Status\nfig, axes = plt.subplots(3, 2, sharex=True, figsize=(10,5))\nfig.suptitle('Marital Status Vs Amount Spent on Products by Category')\n\nsns.barplot(y='MntWines', x='Marital_Status', data=data, ax=axes[0,0])\n\nsns.barplot(y='MntFruits', x='Marital_Status', data=data, ax=axes[0,1])\n\nsns.barplot(y='MntMeatProducts', x='Marital_Status', data=data, ax=axes[1,0])\n\nsns.barplot(y='MntFishProducts', x='Marital_Status', data=data, ax=axes[1,1])\n\nsns.barplot(y='MntSweetProducts', x='Marital_Status', data=data, ax=axes[2,0])\n\nsns.barplot(y='MntGoldProds', x='Marital_Status', data=data, ax=axes[2,1])","621cc4f0":"plt.title('Recency Vs Acceptance of an offer')\nsns.lineplot(x='Recency', y='Response', data=data, )","d50c4fb3":"#Age versus webPurchases\nfig, axes = plt.subplots(1, 2, figsize=(15,5))\nfig.suptitle('Number of Web Visits Vs Teenagers or Kids at home')\n\nsns.regplot(x='NumWebVisitsMonth', y='Teenhome', data=data, ax=axes[0], line_kws={\"color\": \"red\"})\n\nsns.regplot(x='NumWebVisitsMonth', y='Kidhome', data=data, ax=axes[1], line_kws={\"color\": \"red\"})","a3b3bbf7":"groupedDate = data.set_index('Dt_Customer')\n\ngroupedDate = groupedDate.resample('m').count()\n\nplt.figure(figsize=(15,3))\nplt.title('Customer register by mounths')\ngroupedDate.ID.plot(kind='line')\n","d62fc370":"groupedDate = data.set_index('Dt_Customer')\n\ngroupedDate = groupedDate.resample('M').sum()[['Total_Spent', 'Total_Purchases']]\n\nplt.figure(figsize=(15,3))\nplt.title('Total Spent and Total Purchases Over Time')\n\nsns.lineplot(data=groupedDate, linewidth=2)\n\nplt.show()","930f1d1c":"#Years of enrollment Feature creation\ndata['TimeClient'] = (2014 - data['Dt_Customer'].dt.year).astype('int')","ad5d3410":"print(data.groupby('TimeClient')['Total_Spent'].mean()) #mean of spenses for time as client\n\nplt.title('Total Spent by Time as Client')\nsns.barplot(x='TimeClient', y='Total_Spent', data=data)","6cc966f7":"fig, axes = plt.subplots(3, 2, sharex=True, figsize=(10,5))\nfig.suptitle('Education Vs Amount Spent on Products by Category')\n\norder= ['Basic', '2n Cycle', 'Graduation', 'Master', 'PhD']\n\nsns.barplot(y='MntWines', x='Education', data=data, ax=axes[0,0], order=order)\n\nsns.barplot(y='MntFruits', x='Education', data=data, ax=axes[0,1], order=order)\n\nsns.barplot(y='MntMeatProducts', x='Education', data=data, ax=axes[1,0], order=order)\n\nsns.barplot(y='MntFishProducts', x='Education', data=data, ax=axes[1,1], order=order)\n\nsns.barplot(y='MntSweetProducts', x='Education', data=data, ax=axes[2,0], order=order)\n\nsns.barplot(y='MntGoldProds', x='Education', data=data, ax=axes[2,1], order=order)","d3a73eb3":"#Encoding for the Marital Status\n\n#Map mathod to control the order of the encodings\n\ndata['Marital_Status'] = data['Marital_Status'].map({'Alone':'0',\n                                                     'Single':'1',\n                                                     'Together':'2',\n                                                     'Married':'3',\n                                                     'Divorced':'4',\n                                                     'Widow':'5'\n                                                     })\n\ndata['Education'] = data['Education'].map({'Basic':'0',\n                                           '2n Cycle':'1',\n                                           'Graduation':'2',\n                                           'Master':'3',\n                                           'PhD':'4'\n                                            })","fd4db675":"#Display the encoded columns\ndata[['Marital_Status', 'Education']]","8084484c":"# Standardizations\n\nnumCols = (data.dtypes == 'float64') #select column to Standardize (Income in this case)\ncolumnsToStandardize = list(numCols[numCols].index)\n\ntoStandardizeData = data[columnsToStandardize]\n\nstandardScaler = StandardScaler()\ndata[columnsToStandardize] = standardScaler.fit_transform(toStandardizeData.values)\n","cf85b2f1":"#normalize columns with quantities\n\ncolumnstoNormalize = ['MntWines', 'MntFruits',\n                      'MntMeatProducts', 'MntFishProducts', 'MntSweetProducts',\n                      'MntGoldProds', 'NumDealsPurchases', 'NumWebPurchases',\n                      'NumCatalogPurchases', 'NumStorePurchases', 'NumWebVisitsMonth',\n                      'Total_Spent', 'Total_Purchases', 'Income']\n\n\ntoNormalizeData = data[columnstoNormalize]\n\nminMaxScaler = MinMaxScaler()\n# powerTrans = PowerTransformer(method='yeo-johnson')\n\n# pipeline = Pipeline(steps=[('minMaxScaler', minMaxScaler),\n#                            ('powerTransform', powerTrans)\n#                            ])\n\ndata[columnstoNormalize] = minMaxScaler.fit_transform(toNormalizeData.values)\n","eef068dc":"data","a4591072":"#Drop columns we will not use now on\ncolumnToDrop = ['Year_Birth','Age','Dt_Customer']\n\ndata = data.drop(list(columnToDrop), axis = 1)","2efe9e9e":"columns = ['Income', 'Total_Purchases', 'Total_Spent']","1bf63272":"#K-means algorithm\nX = data.copy()\nX = X[columns]\n\n# define the model \nkmeans = KMeans(n_clusters=3)\n\n# assign a cluster to each\nX['class'] = kmeans.fit_predict(X)\n\n# retrieve unique clusters\nclusters = unique(X['class']) \n\n#Plot results\nfig, axes = plt.subplots(1, 3, sharex=False, figsize=(15,5))\nfig.suptitle('Results of clustering')\n\nsns.scatterplot(x=X.Income, y=X.index, ax=axes[0], hue=X['class'], s=30)\n\nsns.scatterplot(x=X.Total_Purchases, y=X.index, ax=axes[1], hue=X['class'], s=30)\n\nsns.scatterplot(x=X.Total_Spent, y=X.index, ax=axes[2], hue=X['class'], s=30)","c39d002b":"#Function to process individual variables, calculate the elbow and Silhouette Scores and cluster by the feature.\n\ndef cluster_individual(data, column):\n    data = data\n    columnName = str(column)\n    className = \"class_\" + columnName\n    X = data[column]\n    X = pd.DataFrame(X)\n    \n\n    print('Results for clustering the feature:', columnName)\n    \n    #Define the model using Elbow method to show best K value \n    kmeans_kwargs = {\n        \"init\": \"random\",\n        \"n_init\": 10,\n        \"max_iter\": 300,\n        \"random_state\": 1,\n    }\n\n\n#--------> Sort Clusters by yasirroni\n#--------> https:\/\/github.com\/yasirroni\/sorted_cluster\n#--------> Copyright (c) 2020 Muhammad Yasirroni\n\n    def sorted_cluster(x, model=None):\n        if model == None:\n            model = KMeans()\n        model = sorted_cluster_centers_(model, x)\n        model = sorted_labels_(model, x)\n        return model\n\n    def sorted_cluster_centers_(model, x):\n        model.fit(x)\n        new_centroids = []\n        magnitude = []\n        for center in model.cluster_centers_:\n            magnitude.append(np.sqrt(center.dot(center)))\n        idx_argsort = np.argsort(magnitude)\n        model.cluster_centers_ = model.cluster_centers_[idx_argsort]\n        return model\n\n    def sorted_labels_(sorted_model, x):\n        sorted_model.labels_ = sorted_model.predict(x)\n        return sorted_model\n\n#--------> Sort Clusters by yasirroni\n\n\n    # A list holds the SSE values for each k\n    sse = []\n    for k in range(1, 11):\n        kmeans = KMeans(n_clusters=k, **kmeans_kwargs)\n        kmeans.fit(X)\n        sse.append(kmeans.inertia_)\n\n    #---Plot the elbow result\n    plt.plot(range(1, 11), sse)\n    plt.xticks(range(1, 11))\n    plt.xlabel(\"Number of Clusters\")\n    plt.ylabel(\"SSE\")\n    plt.show()\n\n    kl = KneeLocator(range(1, 11), sse, curve=\"convex\", direction=\"decreasing\")\n    print(columnName, 'best Knee mehtod \"K\" is:', kl.elbow)\n\n    # A list holds the silhouette coefficients for each k\n    silhouette_coefficients = []\n\n    #Start at 2 clusters for silhouette coefficient\n    for k in range(2, 11):\n        kmeans = KMeans(n_clusters=k, **kmeans_kwargs)\n        kmeans.fit(X)\n        score = silhouette_score(X, kmeans.labels_)\n        silhouette_coefficients.append(score)\n\n\n\n    #-----Plot Results\n    plt.plot(range(2, 11), silhouette_coefficients)\n\n    plt.xticks(range(2, 11))\n    plt.xlabel(\"Number of Clusters\")\n    plt.ylabel(\"Silhouette Coefficient\")\n    plt.show()\n\n    # Final model \n    km = KMeans(n_clusters=kl.elbow, **kmeans_kwargs).fit(X)\n\n    # assign a cluster to each\n    # X[className] = km.fit_predict(X)\n    cluster = sorted_cluster(X, km)\n    X[className] = cluster.predict(X)\n\n\n\n    #------Plot results of the clustering using the above K\n    plt.figure(figsize=(5,2))\n    plt.title('Results of clustering')\n\n    sns.scatterplot(x=X[columnName], y=X.index, hue=X[className], s=30)\n    \n    #Append the clustering to the original DataSet\n    data[className] = X[className]\n\n    # return data","51bf49e7":"cluster_individual(data, 'Income')","4188ece5":"cluster_individual(data, 'Total_Spent')","dbb9c57e":"cluster_individual(data, 'Total_Purchases')","6e9fec7e":"data.head()","db51c9a1":"#Potencial customer (Income x Total Spent)\ndata['Potencial_Income_X_Expenses'] = data['class_Income'] + data['class_Total_Spent']\ndata['Potencial_Income_X_Expenses'] = data['Potencial_Income_X_Expenses'].map({0:'Very Low',\n                                                                               1:'Low',\n                                                                               2:'Medium', \n                                                                               3:'High', \n                                                                               4:'Very High'})\ndata","31d95e7a":"data.Potencial_Income_X_Expenses.value_counts().loc[['Very Low', 'Low', 'Medium', 'High', 'Very High']].plot()","71a843e7":"#Potencial customer (Time x Total Spent x Income)\n#new Customers with high income and high expenses \n\ndata['Potencial_Over_Time'] = data['class_Income'] + data['class_Total_Spent'] - data['TimeClient']\n\ndata['Potencial_Over_Time'] = data['Potencial_Over_Time'].map({0:'Very Low',\n                                                               1:'Low',\n                                                               2:'Medium', \n                                                               3:'High', \n                                                               4:'Very High'})\n\n#if the result is less than 0, we will have a NaN. To solve this fill NaNs with Very Low class.\ndata['Potencial_Over_Time'] = data['Potencial_Over_Time'].fillna('Very Low')\n\n","3016652c":"data.Potencial_Over_Time.value_counts().loc[['Very Low', 'Low', 'Medium', 'High', 'Very High']].plot()","7812c625":"#Customers with expenses above the .75 quantile, considering the time as a client\n#Calculate the quantile grouped by time as client\navgExp = data.groupby('TimeClient', as_index=False)['Total_Spent'].quantile(0.75)\n\n#Append the results in the dataset\ndata['Above_Average'] = np.where((data['TimeClient'] == avgExp['TimeClient'][0]) & (data['Total_Spent'] > avgExp['Total_Spent'][0]), 'Yes', \n                        np.where((data['TimeClient'] == avgExp['TimeClient'][1]) & (data['Total_Spent'] > avgExp['Total_Spent'][1]), 'Yes', \n                        np.where((data['TimeClient'] == avgExp['TimeClient'][2]) & (data['Total_Spent'] > avgExp['Total_Spent'][2]), 'Yes',            \n                        'No')))","0db95970":"data.Above_Average.value_counts().plot(kind='bar')","9e8fb9a7":"data.Above_Average.value_counts(normalize=True)*100","9dad2491":"#Human-readeble for the Income segmantation obtained by the K-Means\ndata['class_Income'] = data['class_Income'].map({0:'Low-Income',\n                                                 1:'Medium-Income',\n                                                 2:'High-Income'})","5a3605b7":"data.class_Income.value_counts().loc[['Low-Income', 'Medium-Income', 'High-Income']].plot()","8fb4c783":"columnsToSave = ['ID','class_Income','Potencial_Income_X_Expenses', 'Potencial_Over_Time', 'Above_Average']\nfinalData = data[columnsToSave]\nfinalData\n\n# #Save final data to disk\n# finalData.to_csv('data\/customer_segmantation_sh1.csv', index=False)","7f8d549e":"columnsML = ['Education', 'Marital_Status', 'Kidhome', 'Teenhome', 'Age_score', 'class_Income', 'class_Total_Spent']\nX = data[columnsML]\n\n# Revert the proccess of Human-readeble for the Income segmantation obtained by the K-Means\nX['class_Income'] = X['class_Income'].map({'Low-Income' : 0,\n                                            'Medium-Income':1,\n                                            'High-Income':2})\n\nX","8dd997f7":"# Load the categorical and object columns as int for ML\nX[['Education', 'Marital_Status', 'Age_score']] = X[['Education', 'Marital_Status', 'Age_score']].astype('int64')\nX.dtypes","051c6d11":"# Divide data in training and test splits\ny = X.pop('class_Total_Spent')\n\nX_train, X_test, y_train, y_test = train_test_split(X,\n                                                    y,\n                                                    test_size = 0.20,\n                                                    random_state = 1)","9d2b65d9":"# Machine LEarning model\nXGBoost_model = XGBClassifier(n_estimators=10, learning_rate=0.1, objective = 'multi:softprob',)\n\nXGBoost_model.fit(X_train, y_train, \n                  early_stopping_rounds=5,\n                  eval_set=[(X_test, y_test)], \n                  verbose=False)\n\n# Proccess the predictions for the training of the model\npredictions_XGBoost = XGBoost_model.predict(X_test)\n\nprint(\"Accuracy Score was: \", round(accuracy_score(predictions_XGBoost, y_test),2)*100, '%')","c42190f4":"### Find the best K for Kmeans and Proccess clusterings","caedb8b6":"### Handle Nans\nFirst, we display total of missing values to determine how is our dataset","86292017":"Poeople with more children tends to buy less, in general.","2f41f634":"# References\n\n----\nOutliers:\n\nhttps:\/\/www.analyticsvidhya.com\/blog\/2021\/05\/feature-engineering-how-to-detect-and-remove-outliers-with-python-code\/\n\n----\nStandardization:\n\nhttps:\/\/machinelearningmastery.com\/standardscaler-and-minmaxscaler-transforms-in-python\/\n\n----\nClassification Algorithms: \n\nhttps:\/\/machinelearningmastery.com\/clustering-algorithms-with-python\/\n\nhttps:\/\/realpython.com\/k-means-clustering-python\/#partitional-clustering\n\n----\nSorting Clusters:\n\nhttps:\/\/github.com\/yasirroni\/sorted_cluster","229dbf8e":"Here we consider the potential expenses versus the income versus the length of relationship with the company. The objective is to give a higher score to customers who have a good income score, or who spend a lot in less relationship time. As a result, customers who take longer to spend have a lower score. \n\nWe will consider 5 groups, that is: Very Low, Low, Medium, High and Very High Potencial.","9486fbd5":"#### The Income feature\n\nThe income increases faster from Basic education to 2n Cycle but, incomes from the 4 last categories are similar.\n\nAnd we have to handle outliers here","946cdee8":"We have just 24 missing values on the Income variable, that we will be able to fill with the mean.","2449025a":"#### KidHome feature\n\nPeople with more children in home tend to buy less, specially at stores but tend to leverage more on deals.","0387e8ff":"#### The Education Feature\n\nPeople with Basic Education tend to spend less in every category, probably because of the reduced income. Tough, the greater difference is in Meat and Wine products.","254fd7c4":"Age has just a slight and negative relationship with the Income feature, meaning younger people tends to have a lower Income.\n\n----\n\nWe might create a new feature, named \"age\", illustrating better the age of each record.","28dc590b":"### Correlation Map for the features\n---","b83bf0c1":"# Conclusion\n\nWith the dataset we had the opportunity to deeply analyze the behavior of each customer group with the company. We were also able to segment customers into 3 different categories, each considering different variables that can be used in different ways by the marketing team and support decision-making. We've also developed a machine learning system that can predict with 76% accuracy in which total spend category a new customer would be and thus support the marketing team in decision-making once the customer registers and fills in initial data, so that over time, that customer's behavior would be collected and attributed to the previous clusterings. ","2bfd7852":"# 2nd Part - Machine Learning Models to Predict the Potential of Recent Customers \n\n----\n### Are we able to predict how much a customer will spend considering the features we should collect on first contact?\n\nThe variables that we will have access to after the first contact with the client are related to income, age, number of children and teenagers at home, marital status and education. So we'll just use them and try to forecast the 'Total Spent' class","be133549":"#### Time of register Feature\n\nWe will create a new variable, considering the time that each customer is enrolled with the company. ","15fb75f2":"#### Year_Birth feature and Age feature Creation\n\nWe also have to treat the outliers for the Year_Birth, as we have just 3 records with great discrepancy.","b6a75e04":"#### Acceptance of campaings feature\n\nPeople tend to not accept the 2nd campaing. Maybe our marketing team has to review the 2nd campaing strategy.","4cc679fe":"We will need to remove the income Outliers","6f4c50f9":"#### Total Spent and Total Purchases - Feature Creation\n\nMore Income is equal more purchases and more amount spent in general.\n\nThe recency has no relation with the total amount spent but the response variable has a relation with the total spent by a customer.","4861ef8d":"### Conclusion of Machine Learning part\n\n\nUsing the XGBoost model, we were able to predict with 77% correct answers the class of expenses in which the new client would fit. It's not a very high rate, but it's already possible to use this forecast for marketing strategies in the early relationship stage, as we collect new data and process it according to the clusters achieved with more data.","b8d8504a":"## Prediciting the class_Total_Spent","5dc20cf8":"## Models Experiments\n\n----\nKMeans and GaussianMixture had the best results, so we will chooce GaussianMixture to cluster the dataset.","2b07258e":"### Customer segmentation base on the clustering\n\nNow that we have ordered clusters by Income, Total Spent and Total Purchases we are able to calculate and segment them using simple combinations. For example, we can sum Total Spent class with Income class and get scores for clients with good income and good expenses, or very high expenses even with not so good income, showing potencial. In the same way we can determine customers that have low budgets and low expenses, so we can put less effort on them. \n\nWe will consider 5 groups, that is: Very Low, Low, Medium, High and Very High Potencial.","f794fdf9":"#### Marital Status\n\n\"Absurd\" on Marital Status is the class with the most expenses in each category, in overwall. But \"Absurd\" is not an Marital Status at all, and it is about only two records. So, we will desconsider it from now on. Same as YOLO, that seems to be a duplicated of a unique record.","ecf78de6":"As the age increases, the amount spent increases too, even in digital channels. Though, the categories of products are not so influenced by the age, except for the eines category that has a light increase on expenses from older ones. ","6f3c571c":"### Simple Data Cleaning\nLooking for uninformative features we found that 2 of our columns are of this type, so we drop them","4f587b3c":"# Machine Learning Models for segmantation","540f7560":"## Vizualize Feautures\nBased on the automatic report generated, we can make deeper analyses","c843ea44":"#### Response and Recency Variables\n\nIs there a relationship between the response and recency variables with the total purchases and expenses? ","f047e775":"## Data Transformation","ab2751e7":"#### Recency Feature\n\nThe recency plot shows a trend of accepting an offer decreases as the recency increase.","5bdd86fe":"## Export results to a new dataset to be used by other company stakeholders\n\nNow that we have customer segments formed by different grouping techniques, we can export a new dataset for other parts of the company to use. ","9b708e00":"#### Chosen model\n\nKmeans had the best overall result, like GaussianMixture, so we will use it for machine learning classifications.","3e92e9d4":"We'll also describe the grouping performed by the Income class by Kmeans, making it more human-readable. ","bcfe3aa7":"#### Date Feature\n\nThere is no big variation on registers by month, what might reveal a consistent campaing strategy over the year.","193f0ccf":"We are going to consider clients who spend much more than the average, considering the quantile of .75 as a reference, also taking into account the length of relationship with the company. Thus, we separate them into 2 groups, those that are above average and those that are not.","3fe69e30":"#### NumWebVisitsMonth Feature\n\nYoungers tend to visit the website less times. \nThe number of visits increases highly on the groups with more kids in home, probably searching for more offers."}}