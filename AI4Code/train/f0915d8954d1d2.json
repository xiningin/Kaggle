{"cell_type":{"8dfb19da":"code","29bd192f":"code","74b4b950":"code","35fa2fda":"code","1b9387a1":"code","cc18b6cc":"code","9b114370":"code","bbae86bd":"code","a4389dda":"code","f306e5f9":"code","79728ce2":"code","67bd3594":"code","49696784":"code","2b220605":"code","821bd0c2":"code","12accf14":"code","02f1db17":"code","62163ffc":"code","9a4ae01f":"code","af1d5b78":"code","e9d83fa9":"code","26d76899":"code","9f6b080c":"code","44cf70e5":"code","e05e0373":"code","cb059f3b":"code","2108c9e7":"code","67c28aa6":"code","d978a8c8":"code","b277c03f":"code","045ed5dc":"code","35b1c256":"code","ff0d89b5":"code","36d9ed05":"code","dbe0c645":"code","ada266ab":"code","a885ff6e":"code","de5fa1a2":"code","12a0e676":"code","0b54b34a":"code","8b720bce":"code","60218eee":"code","56ec9767":"markdown","fc162a06":"markdown","b82541d1":"markdown","ce039909":"markdown","2d5dd575":"markdown","e4c16a69":"markdown","8e122ee1":"markdown","665c7d24":"markdown","010b1488":"markdown","9c1e8894":"markdown","59566d9b":"markdown","c6d6b3ae":"markdown","59e72ec3":"markdown","eed1baed":"markdown","1ef15557":"markdown","f10917cc":"markdown","4c901f38":"markdown","bf9220d6":"markdown","7ec6ec79":"markdown","39d75339":"markdown","52c680bc":"markdown","1cf25ecf":"markdown","24e279c1":"markdown","8a3d21c3":"markdown","94982bd4":"markdown","d8857a28":"markdown","99a77cb6":"markdown","e08db891":"markdown","87d7fc9a":"markdown","65355168":"markdown","76530e1e":"markdown","a3438dec":"markdown","949b504b":"markdown"},"source":{"8dfb19da":"\"\"\"Importing libraries\"\"\"\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nplt.style.use('fivethirtyeight')\n\nfrom tqdm import tqdm\nimport time\nimport re\nimport string\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\n\nfrom sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold,KFold, cross_val_score\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import MultinomialNB\nimport xgboost as xgb\nfrom sklearn import preprocessing, model_selection, pipeline\nfrom sklearn.metrics import f1_score, roc_auc_score\n\nfrom keras.models import Sequential\nfrom keras.layers.recurrent import LSTM, GRU\nfrom keras.layers.core import Dense, Activation, Dropout\nfrom keras.layers.embeddings import Embedding\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.utils import np_utils\nfrom keras.layers import GlobalMaxPooling1D, Conv1D, MaxPooling1D, Flatten, Bidirectional, SpatialDropout1D\nfrom keras.preprocessing import sequence, text\nfrom keras.callbacks import EarlyStopping","29bd192f":"df = pd.read_csv(\"..\/input\/real-or-fake-fake-jobposting-prediction\/fake_job_postings.csv\")\ndf.head()","74b4b950":"\"\"\"Missing values\"\"\"\ndf.isnull().sum()","35fa2fda":"\"\"\"Extracting Text Featurs\"\"\"\n\ntext_df = df[[\"title\", \"company_profile\", \"description\", \"requirements\", \"benefits\",\"fraudulent\"]]\ntext_df = text_df.fillna(' ')\n\ntext_df.head()","1b9387a1":"\"\"\"Catagorical Feature\"\"\"\ncat_df = df[[\"telecommuting\", \"has_company_logo\", \"has_questions\", \"employment_type\", \"required_experience\", \"required_education\", \"industry\", \"function\",\"fraudulent\"]]\ncat_df = cat_df.fillna(\"None\")\n\ncat_df.head()","cc18b6cc":"fig, axes = plt.subplots(ncols=2, figsize=(17, 5), dpi=100)\nplt.tight_layout()\n\ndf[\"fraudulent\"].value_counts().plot(kind='pie', ax=axes[0], labels=['Real Post (95%)', 'Fake Post (5%)'])\ntemp = df[\"fraudulent\"].value_counts()\nsns.barplot(temp.index, temp, ax=axes[1])\n\naxes[0].set_ylabel(' ')\naxes[1].set_ylabel(' ')\naxes[1].set_xticklabels([\"Real Post (17014) [0's]\", \"Fake Post (866) [1's]\"])\n\naxes[0].set_title('Target Distribution in Dataset', fontsize=13)\naxes[1].set_title('Target Count in Dataset', fontsize=13)\n\nplt.show()","9b114370":"cat_cols = [\"telecommuting\", \"has_company_logo\", \"has_questions\", \"employment_type\", \"required_experience\", \"required_education\",]\n# visualizating catagorical variable by target\nimport matplotlib.gridspec as gridspec # to do the grid of plots\ngrid = gridspec.GridSpec(3, 3, wspace=0.5, hspace=0.5) # The grid of chart\nplt.figure(figsize=(15,25)) # size of figure\n\n# loop to get column and the count of plots\nfor n, col in enumerate(cat_df[cat_cols]): \n    ax = plt.subplot(grid[n]) # feeding the figure of grid\n    sns.countplot(x=col, data=cat_df, hue='fraudulent', palette='Set2') \n    ax.set_ylabel('Count', fontsize=12) # y axis label\n    ax.set_title(f'{col} Distribution by Target', fontsize=15) # title label\n    ax.set_xlabel(f'{col} values', fontsize=12) # x axis label\n    xlabels = ax.get_xticklabels() \n    ylabels = ax.get_yticklabels() \n    ax.set_xticklabels(xlabels,  fontsize=10)\n    ax.set_yticklabels(ylabels,  fontsize=10)\n    plt.legend(fontsize=8)\n    plt.xticks(rotation=90) \n    total = len(cat_df)\n    sizes=[] # Get highest values in y\n    for p in ax.patches: # loop to all objects\n        height = p.get_height()\n        sizes.append(height)\n        ax.text(p.get_x()+p.get_width()\/2.,\n                height + 3,\n                '{:1.2f}%'.format(height\/total*100),\n                ha=\"center\", fontsize=10) \n    ax.set_ylim(0, max(sizes) * 1.15) #set y limit based on highest heights\n\n\nplt.show()\n","bbae86bd":"fig,(ax1,ax2)= plt.subplots(ncols=2, figsize=(17, 5), dpi=100)\nlength=text_df[text_df[\"fraudulent\"]==1]['description'].str.len()\nax1.hist(length,bins = 20,color='orangered')\nax1.set_title('Fake Post')\nlength=text_df[text_df[\"fraudulent\"]==0]['description'].str.len()\nax2.hist(length, bins = 20)\nax2.set_title('Real Post')\nfig.suptitle('Characters in description')\nplt.show()","a4389dda":"fig,(ax1,ax2)= plt.subplots(ncols=2, figsize=(17, 5), dpi=100)\nlength=text_df[text_df[\"fraudulent\"]==1]['requirements'].str.len()\nax1.hist(length,bins = 20,color='orangered')\nax1.set_title('Fake Post')\nlength=text_df[text_df[\"fraudulent\"]==0]['requirements'].str.len()\nax2.hist(length,bins = 20)\nax2.set_title('Real Post')\nfig.suptitle('Characters in requirements')\nplt.show()","f306e5f9":"fig,(ax1,ax2)= plt.subplots(ncols=2, figsize=(17, 5), dpi=100)\nlength=text_df[text_df[\"fraudulent\"]==1]['benefits'].str.len()\nax1.hist(length,bins = 20,color='orangered')\nax1.set_title('Fake Post')\nlength=text_df[text_df[\"fraudulent\"]==0]['benefits'].str.len()\nax2.hist(length,bins = 20)\nax2.set_title('Real Post')\nfig.suptitle('Characters in benefits')\nplt.show()","79728ce2":"fig,(ax1,ax2)= plt.subplots(ncols=2, figsize=(17, 5), dpi=100)\nnum=text_df[text_df[\"fraudulent\"]==1]['company_profile'].str.split().map(lambda x: len(x))\nax1.hist(num,bins = 20,color='orangered')\nax1.set_title('Fake Post')\nnum=text_df[text_df[\"fraudulent\"]==0]['company_profile'].str.split().map(lambda x: len(x))\nax2.hist(num, bins = 20)\nax2.set_title('Real Post')\nfig.suptitle('Words in company profile')\nplt.show()","67bd3594":"fig,(ax1,ax2)= plt.subplots(ncols=2, figsize=(17, 5), dpi=100)\nnum=text_df[text_df[\"fraudulent\"]==1]['description'].str.split().map(lambda x: len(x))\nax1.hist(num,bins = 20,color='orangered')\nax1.set_title('Fake Post')\nnum=text_df[text_df[\"fraudulent\"]==0]['description'].str.split().map(lambda x: len(x))\nax2.hist(num, bins = 20)\nax2.set_title('Real Post')\nfig.suptitle('Words in description')\nplt.show()","49696784":"fig,(ax1,ax2)= plt.subplots(ncols=2, figsize=(17, 5), dpi=100)\nnum=text_df[text_df[\"fraudulent\"]==1]['requirements'].str.split().map(lambda x: len(x))\nax1.hist(num,bins = 20,color='orangered')\nax1.set_title('Fake Post')\nnum=text_df[text_df[\"fraudulent\"]==0]['requirements'].str.split().map(lambda x: len(x))\nax2.hist(num,bins = 20)\nax2.set_title('Real Post')\nfig.suptitle('Words in requirements')\nplt.show()","2b220605":"fig,(ax1,ax2)= plt.subplots(ncols=2, figsize=(17, 5), dpi=100)\nnum=text_df[text_df[\"fraudulent\"]==1]['benefits'].str.split().map(lambda x: len(x))\nax1.hist(num,bins = 20,color='orangered')\nax1.set_title('Fake Post')\nnum=text_df[text_df[\"fraudulent\"]==0]['benefits'].str.split().map(lambda x: len(x))\nax2.hist(num, bins = 20)\nax2.set_title('Real Post')\nfig.suptitle('Words in benefits')\nplt.show()","821bd0c2":"\"\"\"Concate the text data for preprocessing and modeling\"\"\"\ntext = text_df[text_df.columns[0:-1]].apply(lambda x: ','.join(x.dropna().astype(str)),axis=1)\ntarget = df['fraudulent']\n\nprint(len(text))\nprint(len(target))","12accf14":"def get_top_tweet_unigrams(corpus, n=None):\n    vec = CountVectorizer(ngram_range=(1, 1)).fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:n]\n\ndef get_top_tweet_bigrams(corpus, n=None):\n    vec = CountVectorizer(ngram_range=(2, 2)).fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:n]\n\nfig, axes = plt.subplots(ncols=2, figsize=(18, 30), dpi=100)\nplt.tight_layout()\n\ntop_unigrams=get_top_tweet_unigrams(text)[:50]\nx,y=map(list,zip(*top_unigrams))\nsns.barplot(x=y,y=x, ax=axes[0], color='teal')\n\n\ntop_bigrams=get_top_tweet_bigrams(text)[:50]\nx,y=map(list,zip(*top_bigrams))\nsns.barplot(x=y,y=x, ax=axes[1], color='crimson')\n\n\naxes[0].set_ylabel(' ')\naxes[1].set_ylabel(' ')\n\naxes[0].set_title('Top 50 most common unigrams in text', fontsize=15)\naxes[1].set_title('Top 50 most common bigrams in text', fontsize=15)\n\nplt.show()","02f1db17":"%%time\ndef clean_text(text):\n    '''Make text lowercase, remove text in square brackets,remove links,remove punctuation\n    and remove words containing numbers.'''\n    text = text.lower()\n    text = re.sub('\\[.*?\\]', '', text)\n    text = re.sub('https?:\/\/\\S+|www\\.\\S+', '', text)\n    text = re.sub('<.*?>+', '', text)\n    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n    text = re.sub('\\n', '', text)\n    text = re.sub('\\w*\\d\\w*', '', text)\n    return text\n\n\n# Applying the cleaning function to both test and training datasets\ntext = text.apply(lambda x: clean_text(x))\ntext.head(3)","62163ffc":"%%time\ntokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n\n# appling tokenizer5\ntext = text.apply(lambda x: tokenizer.tokenize(x))\ntext.head(3)","9a4ae01f":"%%time\nstop_words = stopwords.words('english')\ndef remove_stopwords(text):\n    \"\"\"\n    Removing stopwords belonging to english language\n    \n    \"\"\"\n    words = [w for w in text if w not in stop_words]\n    return words\n\n\ntext = text.apply(lambda x : remove_stopwords(x))","af1d5b78":"%%time\ndef combine_text(list_of_text):\n    combined_text = ' '.join(list_of_text)\n    return combined_text\n\ntext = text.apply(lambda x : combine_text(x))\ntext.head(3)","e9d83fa9":"kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=2)\nauc_buf = []   \ncnt = 0\npredictions = 0\n# enumerate the splits and summarize the distributions\nfor train_ix, test_ix in kfold.split(text, target):\n    print('Fold {}'.format(cnt + 1))\n    train_X, test_X = text[train_ix], text[test_ix]\n    train_y, test_y = target[train_ix], target[test_ix]\n\n    # Appling Count Vectorizer\n    count_vectorizer = CountVectorizer()\n    train_X_vec = count_vectorizer.fit_transform(train_X)\n    test_X_vec = count_vectorizer.transform(test_X)    \n    \n    lr = LogisticRegression(C=0.1, solver='lbfgs', max_iter=1000, verbose=0, n_jobs=-1)\n    lr.fit(train_X_vec, train_y)\n    preds = lr.predict(test_X_vec)\n    \n    auc = roc_auc_score(test_y, preds)\n    print('{} AUC: {}'.format(cnt, auc))\n    auc_buf.append(auc)\n    cnt += 1\n\nprint('AUC mean score = {:.6f}'.format(np.mean(auc_buf)))\nprint('AUC std score = {:.6f}'.format(np.std(auc_buf)))","26d76899":"# spliting tthe data for glove\nX_train, X_test, y_train, y_test = train_test_split(text, target, test_size=0.2, random_state=4, stratify=target)","9f6b080c":"\"\"\"Load the Glove vectors in a dictionay\"\"\"\nembeddings_index={}\nwith open('..\/input\/glove-global-vectors-for-word-representation\/glove.6B.200d.txt','r') as f:\n    for line in f:\n        values=line.split()\n        word=values[0]\n        vectors=np.asarray(values[1:],'float32')\n        embeddings_index[word]=vectors\nf.close()\n\nprint('Found %s word vectors.' % len(embeddings_index))","44cf70e5":"\"\"\" Function Creates a normalized vector for the whole sentence\"\"\"\ndef sent2vec(s):\n    words = str(s).lower()\n    words = word_tokenize(words)\n    words = [w for w in words if not w in stop_words]\n    words = [w for w in words if w.isalpha()]\n    M = []\n    for w in words:\n        try:\n            M.append(embeddings_index[w])\n        except:\n            continue\n    M = np.array(M)\n    v = M.sum(axis=0)\n    if type(v) != np.ndarray:\n        return np.zeros(200)\n    return v \/ np.sqrt((v ** 2).sum())","e05e0373":"# create glove features\nxtrain_glove = np.array([sent2vec(x) for x in tqdm(X_train)])\nxtest_glove = np.array([sent2vec(x) for x in tqdm(X_test)])","cb059f3b":"\"\"\"scale the data before any neural net\"\"\"\nscl = preprocessing.StandardScaler()\nxtrain_glove_scl = scl.fit_transform(xtrain_glove)\nxtest_glove_scl = scl.transform(xtest_glove)","2108c9e7":"\"\"\"create a simple 2 layer sequential neural net\"\"\"\nmodel = Sequential()\n\nmodel.add(Dense(200, input_dim=200, activation='relu'))\nmodel.add(Dropout(0.2))\nmodel.add(BatchNormalization())\n\nmodel.add(Dense(100, activation='relu'))\nmodel.add(Dropout(0.2))\nmodel.add(BatchNormalization())\n\nmodel.add(Dense(100, activation='relu'))\nmodel.add(Dropout(0.2))\nmodel.add(BatchNormalization())\n\nmodel.add(Dense(1))\nmodel.add(Activation('sigmoid'))\n\n# compile the model\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])","67c28aa6":"model.fit(xtrain_glove_scl, y=y_train, batch_size=64, \n          epochs=10, verbose=1, \n          validation_data=(xtest_glove_scl, y_test))","d978a8c8":"predictions = model.predict(xtest_glove_scl)\npredictions = np.round(predictions).astype(int)\nprint('2 layer sequential neural net on GloVe Feature')\nprint (\"AUC score :\", np.round(roc_auc_score(y_test, predictions),5))","b277c03f":"new_text = text_df[text_df.columns[0:-1]].apply(lambda x: ','.join(x.dropna().astype(str)),axis=1)\ntarget = df['fraudulent']","045ed5dc":"%%time\ndef clean_text(text):\n    '''Make text lowercase, remove text in square brackets,remove links,remove punctuation\n    and remove words containing numbers.'''\n    text = text.lower()\n    text = re.sub('\\[.*?\\]', '', text)\n    text = re.sub('https?:\/\/\\S+|www\\.\\S+', '', text)\n    text = re.sub('<.*?>+', '', text)\n    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n    text = re.sub('\\n', '', text)\n    text = re.sub('\\w*\\d\\w*', '', text)\n    return text\n\n\n# Applying the cleaning function to both test and training datasets\nnew_text = new_text.apply(lambda x: clean_text(x))\nnew_text.head(3)","35b1c256":"# Trying First 2000 sample\nbatch_1 = new_text[:2000]\ntarget_1 = target[:2000]\ntarget_1.value_counts()","ff0d89b5":"# importing the tools \nimport torch\nimport transformers as ppb\nimport warnings\nwarnings.filterwarnings('ignore')","36d9ed05":"# DistilBERT\nmodel_class, tokenizer_class, pretrained_weights = (ppb.DistilBertModel, ppb.DistilBertTokenizer, 'distilbert-base-uncased')\n\n# Load pretrained model\/tokenizer\ntokenizer = tokenizer_class.from_pretrained(pretrained_weights)\nmodel = model_class.from_pretrained(pretrained_weights)","dbe0c645":"%%time\n# Tokenization\ntokenized = batch_1.apply((lambda x: tokenizer.encode(x, max_length = 60, add_special_tokens=True)))\ntokenized.shape","ada266ab":"# Padding ==> convert 1D array to 2D array\nmax_len = 0\nfor i in tokenized.values:\n    if len(i) > max_len:\n        max_len = len(i)\n\npadded = np.array([i + [0]*(max_len-len(i)) for i in tokenized.values])\nnp.array(padded).shape","a885ff6e":"# Masking ==>  ignore (mask) the padding we've added\nattention_mask = np.where(padded != 0, 1, 0)\nattention_mask.shape","de5fa1a2":"# Deep Learning\ninput_ids = torch.tensor(padded)  \nattention_mask = torch.tensor(attention_mask)\n\nwith torch.no_grad():\n    last_hidden_states = model(input_ids, attention_mask=attention_mask)\n\nlast_hidden_states[0].shape","12a0e676":"features = last_hidden_states[0][:,0,:].numpy()\nlabels = target_1","0b54b34a":"train_features, test_features, train_labels, test_labels = train_test_split(features, labels)","8b720bce":"# train default para\nlr_clf = LogisticRegression()\nlr_clf.fit(train_features, train_labels)","60218eee":"predictions = lr_clf.predict(test_features)\npredictions = np.round(predictions).astype(int)\nprint (\"AUC score :\", np.round(roc_auc_score(test_labels, predictions),5))","56ec9767":"The distribution of charaters in requirements of the fake and real post are similar.","fc162a06":"The distribution of words in requirements of the fake and real post are similar.","b82541d1":"## Text cleaning","ce039909":"It's seem simple Logistic Regression model perform well. **AUC mean score of 0.85** demonstrating the good fitting of chosen model. Let's apply Glove for vectorization with deep learning.","2d5dd575":"## <font color=\"red\">Give me your feedback and if you find my kernel helpful please UPVOTE will be appreciated.<\/font>","e4c16a69":"**It can safely be said that LR ,in general, is better at discriminating between positives and negatives than LR. Also LR(~ 84%) auc score (which is the area under the roc curve) is greater than equential neural net(~ 74%).We can improve the result by appling diffrent approachs.**","8e122ee1":"## Tokenizer","665c7d24":"### Model ","010b1488":"# Data preprocessing","9c1e8894":"Hmm!! Both the post has similar distribution of words in description.","59566d9b":"Pattern of words in company profile is same as character in company profile. **fake post has less words** in the company profile while **real post has more words.**","c6d6b3ae":"## Number of words\nLet's compare the number of words in the fake post and real post and try to distinguish pattern in the fake and real post based on number of words used in the post.\n\n#### Company Profile","59e72ec3":"## Number of characters\nLet's compare the number of character in the fake post and real post and try to distinguish pattern in the fake and real post based on number of charater used in the post.\n\n#### Company profile\nWe can see that **fake post has less characters** in the company profile while **real post has more charaters.**","eed1baed":"The above AUC score is based on the only 2000 sample. we can improve our score by using full dataset and train the bert model on deep learing lebraries. ","1ef15557":"The distribution of words in benefits of the fake and real post are also similar.","f10917cc":"# Target","4c901f38":"#### Requirements","bf9220d6":"Hmm!! Class distributions are **95% for 0 (Real Post)** and **5% for 1 (Fake Post).** Target distribution is highly imbalanced. Accuracy metric is not useful here it will mislead the result. So, we've to look into Precision, Recall, F1 Score for model evalution.","7ec6ec79":"#### Requirements","39d75339":"The distribution of charaters in requirements of the fake and real post is same around 1500 to 1800.","52c680bc":"The distribution of charaters in description of the fake and real post are similar but some fake post reach to 6000 to 6500 characters. ","1cf25ecf":"# Exploratory Data Analysis of tweets","24e279c1":"#### Description","8a3d21c3":"## Ngrams Analysis\n\nMost common unigrams exist in **both classes** are mostly punctuations, stop words or numbers. It is better to clean them before modelling since they don't give much information about target.","94982bd4":"# Simple Bert Implementation","d8857a28":"#### Benefits","99a77cb6":"# Deep Learning \n## Sequential Neural Net ","e08db891":"# GloVe \u00b6\nHere we will use GloVe pretrained corpus model to represent our words.It is available in 3 varieties :50D ,100D and 200 Dimentional.We will try 200 D here.","87d7fc9a":"#### Benefits","65355168":"\n## References\n1. https:\/\/www.kaggle.com\/vikassingh1996\/different-approaches-to-nlp-problems\n2. https:\/\/www.kaggle.com\/shahules\/basic-eda-cleaning-and-glove\n3. http:\/\/jalammar.github.io\/a-visual-guide-to-using-bert-for-the-first-time\/","76530e1e":"#### Description","a3438dec":"# [Real or Fake] : Fake Job Description Prediction\n<img src=\"https:\/\/encrypted-tbn0.gstatic.com\/images?q=tbn%3AANd9GcRCSJEp0uveDRpbG9W1oGoPVsDHGC2b2Z-qnyFrAn8wcwqJmf9w\" align=\"left\" height=\"200\" width=\"200\">\nThis dataset contains 18K job descriptions out of which about 800 are fake. The data consists of both textual information and meta-information about the jobs. The dataset can be used to create classification models which can learn the job descriptions which are fraudulent.","949b504b":"# Baseline Model"}}