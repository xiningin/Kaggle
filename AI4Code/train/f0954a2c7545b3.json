{"cell_type":{"5928dacd":"code","dbc01d24":"code","057e3c96":"code","219106fc":"code","5a4ba6cd":"code","3fb87049":"code","d8505e08":"code","398bd873":"code","2b860fe3":"code","4cbfb8e4":"code","b08b8ece":"code","634d4f48":"code","179fae07":"code","736f3427":"code","e99975bb":"code","67acea56":"code","e22fc7cd":"code","78ad480e":"code","78d76ea3":"code","eee027b5":"code","c21d5e7e":"code","ea956e50":"code","f56ea340":"code","7fd2e01b":"code","947d8a98":"code","c17c85ae":"code","ed8d9237":"code","a1d81808":"code","cf535b15":"code","a7684c25":"code","0c147ea7":"code","54ecabd4":"code","52e02e7e":"code","959e6beb":"code","f709b6ed":"code","c5177b75":"code","0b345d86":"code","2595c8f1":"code","98bb026e":"markdown","4ab590c2":"markdown","944fcac0":"markdown","7042f3da":"markdown","8dd6e481":"markdown","22ccbaec":"markdown","2915c1aa":"markdown","1d70a4cf":"markdown","aa84dc25":"markdown","6b2e0efb":"markdown","87972773":"markdown","c93e888b":"markdown","e24813fe":"markdown","def81583":"markdown","4d5b3805":"markdown","a712398f":"markdown","07cfd767":"markdown","4a80ba3c":"markdown"},"source":{"5928dacd":"import numpy as np\nimport seaborn as sns\nimport pandas as pd\nimport matplotlib.pyplot as plt\nplt.style.use('fivethirtyeight')","dbc01d24":"height = np.array([150, 160, 155, 173, 180, 169]) \nweight = np.array([50, 63, 60, 67, 72, 66])\n# # Interpretation:\n# # for height with 150 cm, the weight of the person is 48 kg","057e3c96":"sns.scatterplot(height, weight)\nplt.xlabel('Height (cm)')\nplt.ylabel('Weight (kg)')","219106fc":"def system(val):\n    return val\/3","5a4ba6cd":"new_weight = system(height)","3fb87049":"new_weight","d8505e08":"weight","398bd873":"def mean_squared_error(actual, pred):\n    return np.mean((actual - pred)**2)\n    ","2b860fe3":"mean_squared_error(actual=weight, pred=new_weight)","4cbfb8e4":"error_vals = []\nerror_to_slope = {}\n# We can try substituting the slope from 0 to 1\nfor temp_m in np.linspace(0, 1, num=100):\n    # For every slope , We can try substituting the slope from 0 to 1\n    for temp_c in np.linspace(0, 1, num=100):\n        new_weight = temp_m*height + temp_c\n        _error = mean_squared_error(actual=weight, pred=new_weight)\n        error_to_slope[_error] = [temp_m, temp_c]\n        error_vals.append(_error)\n    \n    ","b08b8ece":"len(error_vals)","634d4f48":"min(error_vals)","179fae07":"slope, intercept = error_to_slope[min(error_vals)]","736f3427":"print(slope, intercept)","e99975bb":"new_weight = slope*height + intercept\nprint(new_weight)","67acea56":"weight","e22fc7cd":"_error, _slopes = [], []\nfor key, value in error_to_slope.items():\n    _error.append(key)\n    _slopes.append(value[0])\n\nerror_df = pd.DataFrame({\"error\":_error, \"slope\":_slopes})","78ad480e":"error_df","78d76ea3":"error_df.error.plot()","eee027b5":"# Initially Setting some values for slope and intercept\nslope = 0.1\nintercept = 0.1","c21d5e7e":"# Forward propagation (basically multiplication)\nnew_weight = slope*height + intercept \n\n_error = mean_squared_error(actual=weight, pred=new_weight)\n\ntotal_items = len(weight) # Lenght of samples\n\n# Find the Gradients \n# Since derivative is slope, we are finding the change in slope and updating the slope\n\nde_dm = (-2\/total_items) * np.sum(height * (weight - new_weight)) # Derivative of error function wrt slope_val\nde_dc = (-2\/total_items) * np.sum(weight - new_weight) # Derivative of error function wrt c\n\n# Back propagation (update the weights)\nslope = slope - de_dm \nintercept = intercept - de_dc ","ea956e50":"slope, intercept, de_dm, de_dc","f56ea340":"slope = 0.1\nintercept = 0.1\n# Forward propagation (basically multiplication)\nnew_weight = slope*height + intercept \n_learning_rate = 0.001\n_error = mean_squared_error(actual=weight, pred=new_weight)\n\ntotal_items = len(weight) # Lenght of samples\n\n# Find the Gradients \n# Since derivative is slope, we are finding the change in slope and updating the slope\n\nde_dm = (-2\/total_items) * np.sum(height * (weight - new_weight)) # Derivative of error function wrt slope_val\nde_dc = (-2\/total_items) * np.sum(weight - new_weight) # Derivative of error function wrt c\n\n# Back propagation (update the weights)\nslope = slope - (de_dm * _learning_rate)\nintercept = intercept - (de_dc * _learning_rate)\n\nprint(slope, intercept)","7fd2e01b":"slope = 0.1\nintercept = 0.1\n_learning_rate = 0.001\n# we can run this for some iterations i.e epoch \nepochs = 3000\nfor itr in range(epochs):\n    total_items = len(weight) # Lenght of samples\n    \n    new_weight = slope*height + intercept\n    # Find the Gradients \n    # Since derivative is slope, we are finding the change in slope and updating the slope\n\n    de_dm = (-2\/total_items) * np.sum(height * (weight - new_weight)) # Derivative of error function wrt slope_val\n    de_dc = (-2\/total_items) * np.sum(weight - new_weight) # Derivative of error function wrt c\n\n    # Back propagation (update the weights)\n    slope = slope - (de_dm * _learning_rate)\n    intercept = intercept - (de_dc * _learning_rate)","947d8a98":"slope, intercept","c17c85ae":"def normalize(x):\n    x = (x - min(x))\/(max(x)-min(x))\n    return x","ed8d9237":"_weight = normalize(weight)\n_height = normalize(height)\n# By Normalization we can bring the values between 0 to 1 ","a1d81808":"_weight,_height","cf535b15":"slope = 0.1\nintercept = 0.1\n_learning_rate = 0.01\n# we can run this for some iterations i.e epoch \nepochs = 3000\n\n_weight = normalize(weight)\n_height = normalize(height)\n\ntotal_items = len(_weight) # Lenght of samples\n\nfor itr in range(epochs):\n    new_weight = slope*_height + intercept\n#     print(slope, intercept, mean_squared_error(_weight, new_weight))\n    # Find the Gradients \n    # Since derivative is slope, we are finding the change in slope and updating the slope\n\n    de_dm = (-2\/total_items) * np.sum(_height * (_weight - new_weight)) # Derivative of error function wrt slope_val\n    de_dc = (-2\/total_items) * np.sum(_weight - new_weight) # Derivative of error function wrt c\n\n    # Back propagation (update the weights)\n    slope = slope - (de_dm * _learning_rate)\n    intercept = intercept - (de_dc * _learning_rate)","a7684c25":"print(slope, intercept)","0c147ea7":"new_weight = slope*_height + intercept","54ecabd4":"sns.scatterplot(_height, _weight)\nplt.plot(_height, new_weight)\nplt.xlabel('Height (cm)')\nplt.ylabel('Weight (kg)')","52e02e7e":"x1 = np.array([150, 160, 155, 173, 180, 169, 140, 178], dtype=float) # x1\nx2 = np.array([48, 63, 60, 67, 80, 66, 45, 76], dtype=float) # x2\ny = np.array([ 714.6,  844.6,  811.1,  905.1, 1013.6,  888.1,  668.6,  980.6])\n","959e6beb":"# Model Weights\nm1 = 0.1\nm2 = 0.1\nc = 0.1\n\n# Hyperparameters\nlearning_rate = 0.001 # The learning Rate 0.1, 0.001, 0.0001 # adaptive lr\nepochs = 20000 # The number of iterations to perform gradient descent\n\nn = len(x1)\n\nx1 = normalize(x1)\nx2 = normalize(x2)\ny = normalize(y)\n\n# Performing Gradient Descent\nfor i in range(epochs):\n    # Forward propagation (basically multiplication)\n    y_hat = m1*x1 + m2*x2 + c # The current predicted value of Y\n\n    # Find the Gradients\n    de_dm1 = (-2\/n) * np.sum(x1 * (y - y_hat)) # Derivative of error function wrt m\n    de_dm2 = (-2\/n) * np.sum(x2 * (y - y_hat))\n\n    de_dc = (-2\/n) * np.sum(y - y_hat) # Derivative of error function wrt c\n\n\n    # Back propagation (update the weights)\n    m1 = m1 - (de_dm1 * learning_rate) \n    m2 = m2 - (de_dm2 * learning_rate)\n    c = c - (de_dc * learning_rate)\n#     print(m1, m2, c)","f709b6ed":"x1,x2,y","c5177b75":"y","0b345d86":"pred_y = m1*x1 + m2*x2 + c\nprint(pred_y)","2595c8f1":"mean_squared_error(actual=y, pred=pred_y)","98bb026e":"For most of the values the error is low from index 1. This is a good progress, but we have considered the range of slope values from 0 to 1 only. There is a possibility that in a different range we might get a low error as well ","4ab590c2":"# Error Function : Mean Squared Error\n","944fcac0":"Error is high, we can try to bring to zero. But this time instead of using the val\/3 system, we can give a try to \nline equation, by giving some values to slope and intercept \n\n## Y = M*X + C","7042f3da":"height = np.array([150, 160, 155, 173, 180, 169]) \nweight = np.array([48, 63, 60, 67, 72, 66])\n\nBy investigating the data, it seems if we are dividing the height by 3, we are getting an answer near to weight. \nWith that logic we can define a system","8dd6e481":"# Introducing Learning Rate","22ccbaec":"# Gradient Descent","2915c1aa":"## Now we have got a slope, intercept value lesser than our previous method, Now we can run it for some iteration (i.e.,) Epoch","1d70a4cf":"### Since this is a linear model we can represent this with  a line equation\n### weight = slope*height + Intercept (y = mx+c)\n","aa84dc25":"With the above arrays we can have the relation as \n### y = m1*x1 + m2*x2 + c","6b2e0efb":"But still we can see some error is there. We can find the error value using MSE method (There are many error finding mechanisms are there like MAE, RMSE and more)","87972773":"# Multivariate","c93e888b":"# Considering a dataset sample of height and weight\n","e24813fe":"## Here we are getting nan - Not An Number. Its occuring due to our values of weight and height are larger ","def81583":"# Introducing Normalization","4d5b3805":"These slope and intercept values are higher, it because its jumping with high values (i.e, de_dm, de_dc)\nSo we can introduce a learning rate","a712398f":"The error is reduced very much","07cfd767":"This is the best fit line ","4a80ba3c":"### Here we can see the error is minimum near, 4000. So we need to find the slope at that point.  \n### for finding the slope at the global minima, we can use the Gradient Descent Method"}}