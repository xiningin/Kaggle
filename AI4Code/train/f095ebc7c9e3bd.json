{"cell_type":{"b2698509":"code","5028910c":"code","888f94dc":"code","1018613d":"code","53a9769d":"code","08d73315":"code","bdf5877f":"code","9ad31f5d":"code","703b843e":"code","b1c2c541":"code","044e98dc":"code","642b5b8d":"code","7a834825":"code","c4fc3d15":"code","07221b2c":"code","a48a5720":"code","3916977a":"code","d73ee78c":"code","da6f769d":"code","0c4aca6f":"code","70098b59":"code","9b99584a":"code","40ac5c1d":"code","eb1fb597":"code","95d18254":"code","0fcb9619":"code","910d55cc":"code","7a679878":"code","916dff8e":"code","fca8622d":"code","864e9b2f":"code","be74ca5c":"code","11ad50b5":"code","4f2ceea8":"code","fc076968":"code","2cf7307e":"code","1c8c3979":"code","6240bdc0":"code","00f101ce":"code","1f345738":"code","ac8c6642":"code","e71ac248":"markdown","4b7336aa":"markdown","f1c80112":"markdown","feb39188":"markdown","184ea0c2":"markdown","51e16f30":"markdown","4311a959":"markdown","e9b62f41":"markdown","0ac1abe9":"markdown","40a25c81":"markdown","1e2e822c":"markdown","470fb718":"markdown","ead33d32":"markdown","84f15c24":"markdown","44ec2c34":"markdown","27528c81":"markdown","bf2bd945":"markdown"},"source":{"b2698509":"# Import and reading libraries\nimport pandas as pd\nimport numpy as np\nimport random\nimport matplotlib.pyplot as plt\nimport datetime as dt\n%matplotlib inline\n\n# Setting the seed \nrandom.seed(42)\n\n# Start tracking running time\nstart = dt.datetime.now()","5028910c":"# Reading the data file\ndf = pd.read_csv('..\/input\/analyze-ab-test-results-notebook\/ab_data.csv')","888f94dc":"# Reading first few raws of the file\ndf.head()","1018613d":"# identify the shape of the file (raws, columns) count\ndf.shape ","53a9769d":"# number of unique users in the dataset \nprint('There is a total of', df.nunique()[0], 'unique users in the dataset, out of', df.shape[0], 'transcations recorded')\nprint('Total users with more than one transcation is ', (df.user_id.value_counts() > 1).sum())","08d73315":"# Proportion of conversion in the dataset\nprint(f'Mean conversion: {df.converted.mean():.4f}')","bdf5877f":"# Proportion of unique users converted \nusers_converted = df.query('converted == 1').user_id.nunique() \/ df.user_id.nunique()\nprint(f'Proportion of unique users converted: {users_converted:.4f}')","9ad31f5d":"# Number of times when the \"group\" is treatment but \"landing_page\" is not a new_page.\ntreatment_newpage = df.query('group == \"treatment\" & landing_page != \"new_page\"')\nprint(f'Number of times when the \"group\" is treatment but \"landing_page\" is not a new_page is {treatment_newpage.shape[0]}')\n\n# Number of times when the \"group\" is control but \"landing_page\" is new_page.\ncontrol_oldpage = df.query('group != \"treatment\" & landing_page == \"new_page\"')\nprint(f'Number of times when the \"group\" is control but \"landing_page\" is new_page is {control_oldpage.shape[0]}')","703b843e":"# Remove the inaccurate rows, and store the result in a new dataframe df2\ninaccurate = df.query('group == \"treatment\" & landing_page != \"new_page\" | group != \"treatment\" & landing_page == \"new_page\"')\ndf2 = df.drop(inaccurate.index, axis=0)","b1c2c541":"# Double Check all of the incorrect rows were removed from df2 - \ndf2.query('group == \"treatment\" & landing_page != \"new_page\" | group != \"treatment\" & landing_page == \"new_page\"').shape[0]","044e98dc":"# Checking for Null values\ndf.isna().sum()","642b5b8d":"# How many unique user_ids are in the dataset?\ndf2.user_id.nunique(), df2.shape[0]","7a834825":"# Find duplicate user_id\ndf2[df2.duplicated(subset='user_id')]","c4fc3d15":"# Duplicated records\ndf2[df2.user_id == 773192]","07221b2c":"# Remove one of the rows with a duplicate user_id..\ndf2.drop(2893, inplace=True)\n\n# Double check if the row with a duplicate user_id is deleted or not\ndf2[df2.duplicated(subset='user_id')].shape[0]","a48a5720":"# What is the probability of an individual converting regardless of the page they receive?\np_population = df2.query('converted == 1').user_id.nunique() \/ df2.user_id.nunique()\np_population","3916977a":"# What is the probability of an individual converting given he is in the control group?\nconversion_control = df2.query('group == \"control\"').converted.mean()\nconversion_control","d73ee78c":"# What is the probability of an individual converting given he is in the treatment group?\nconversion_treatment = df2.query('group != \"control\"').converted.mean()\nconversion_treatment","da6f769d":"# What is the actual difference (obs_diff) between the conversion rates for the two groups.\nobs_diff = conversion_control - conversion_treatment\nobs_diff","0c4aca6f":"# What is the probability that an individual received the new page?\ndf2.query('landing_page == \"new_page\"').user_id.nunique() \/ df2.user_id.nunique()","70098b59":"# simualting conversion rate on all dataset - Assuming null hypothesis p-new == p-population\nnew_sample_null = np.random.choice(df2['converted'], df2.shape[0])\nnew_sample_null.mean()","9b99584a":"# simualting conversion rate on all dataset - Assuming null hypothesis p-old == p-population\nold_sample_null = np.random.choice(df2['converted'], df2.shape[0])\nold_sample_null.mean()","40ac5c1d":"# number of individuals per group \nindividuals_group = df2.groupby('landing_page')['user_id'].nunique()\n\n# number of individuals in treatment group \nindividuals_new = individuals_group[0]\nprint(f'Number of individuals in the treatment group is {individuals_new}')\n\n# number of individuals in control group \nindividuals_old = individuals_group[1]\nprint(f'Number of individuals in the control group is {individuals_old}')","eb1fb597":"# Simulate a sample for the treatment Group, and calculatig conversion rate\nnew_page_converted = np.random.choice(df2['converted'], individuals_new)\nnew_page_converted.mean()","95d18254":"# Simulate a sample for the control Group, and calculatig conversion rate\nold_page_converted = np.random.choice(df2['converted'], individuals_old)\nold_page_converted.mean()","0fcb9619":"# Simualted difference in conversion rate between new page and old page\nconverted_diff = new_page_converted.mean() - old_page_converted.mean()\nconverted_diff","910d55cc":"%%time\n\n#Sampling distribution using numpy built-in fuctions\n\np_new = np.random.binomial(individuals_new, p=conversion_treatment, size=10000) \/ individuals_new\np_old = np.random.binomial(individuals_old, p=conversion_control, size=10000) \/ individuals_old\np_diffs = p_old - p_new \n\n# Visualizing the sampling distributions\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=[15, 5]);\nax1.hist(p_new, alpha =0.5);\nax1.hist(p_old, alpha =0.5);\n\nax1.title.set_text('Simulating matching between p_new and p_old');\nax1.set_xlabel('Distribution');\nax1.set_ylabel('Counts');\n\nax2.hist(p_diffs);\nax2.axvline(obs_diff, color='g', label='obs_diff', ls='--');\nax2.axvline(np.percentile(p_diffs, 2.5), label='Lower limit', c='yellow', ls='--');\nax2.axvline(np.percentile(p_diffs, 97.5), label='Upper limit', c='red', ls='--');\nax2.title.set_text('Simulating p_diffs');\nax2.set_xlabel('p_diffs');\nax2.set_ylabel('Counts');\n\nplt.legend();","7a679878":"#proportion of the p_diffs are greater than the actual difference observed in the df2\nprobability_diffs = (p_diffs > obs_diff).sum() \/ 10000\nprobability_diffs","916dff8e":"%%time\n\n#Sampling distribution using for loops\np_diffs = []\np_news = []\np_olds = []\n\nfor i in range(10000):\n    p_new = np.random.choice(df2['converted'], individuals_new)\n    p_news.append(p_new.mean())\n    p_old = np.random.choice(df2['converted'], individuals_old)\n    p_olds.append(p_old.mean())\n    p_diff = p_new.mean() - p_old.mean()\n    p_diffs.append(p_diff)","fca8622d":"# Visualizing the sampling distributions\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=[15, 5])\nax1.hist(p_news, alpha =0.5);\nax1.hist(p_olds, alpha =0.5);\n\nax1.title.set_text('Simulating matching between p_new and p_old');\nax1.set_xlabel('Distribution');\nax1.set_ylabel('Counts');\n\n\nax2.hist(p_diffs);\nax2.axvline(obs_diff, color='g', label='obs_diff', ls='--');\nax2.axvline(np.percentile(p_diffs, 2.5), label='Lower limit', c='yellow', ls='--');\nax2.axvline(np.percentile(p_diffs, 97.5), label='Upper limit', c='red', ls='--');\n\nax2.title.set_text('Simulating p_diffs');\nax2.set_xlabel('p_diffs');\nax2.set_ylabel('Counts');\n\nplt.legend();","864e9b2f":"#proportion of the p_diffs are greater than the actual difference observed in the df2\nprobability_diffs = (np.array(p_diffs) > obs_diff).sum() \/ 10000\nprobability_diffs","be74ca5c":"import statsmodels.api as sm\n\n# number of conversions with the old_page\nconvert_old = df2.query('landing_page == \"old_page\"')['converted'].sum()\n\n# number of conversions with the new_page\nconvert_new =df2.query('landing_page != \"old_page\"')['converted'].sum()\n\n# number of individuals who were shown the old_page\nn_old = df2.query('landing_page == \"old_page\"')['user_id'].nunique()\n\n# number of individuals who received new_page\nn_new = df2.query('landing_page != \"old_page\"')['user_id'].nunique()\n\nconvert_old, convert_new, n_old, n_new","11ad50b5":"# applying sm.stats.proportions_ztest() method \nz_score, p_value = sm.stats.proportions_ztest([convert_old, convert_new], [n_old, n_new], alternative = 'larger')\nprint(z_score, p_value)","4f2ceea8":"# Adding an intercept\ndf2['intercept'] = 1\n\n# Transform group column to using get_dummies and drop control column\ndf2['ab_page'] = pd.get_dummies(df2['group'])['treatment']","fc076968":"# Instantiate and fitting the regression model (statsmodels) on the two columns\nlog = sm.Logit(df2['converted'], df2[['intercept', 'ab_page']])\nresult = log.fit()\n\n# Summary statistics of model\nresult.summary2()","2cf7307e":"# Read the countries file\ncountries = pd.read_csv('..\/input\/countries\/countries.csv')\ncountries.head()","1c8c3979":"# Join with the df2 dataframe\ndf_merged = df2.merge(countries, on='user_id')\ndf_merged.head()","6240bdc0":"# Create the necessary dummy variables\ndf_merged[['UK', 'US']] = pd.get_dummies(df_merged['country']).drop('CA', axis=1)","00f101ce":"# Create new variables to check interaction between page and country\ndf_merged['ab_UK'] = df_merged['ab_page'] * df_merged['UK']\ndf_merged['ab_US'] = df_merged['ab_page'] * df_merged['US']\ndf_merged.head()","1f345738":"# Instantiate, fitting the model and summarizing the results\nlog2 = sm.Logit(df_merged['converted'], df_merged[['intercept', 'ab_page','UK', 'US', 'ab_UK', 'ab_US']])\nresult2 = log2.fit()\nresult2.summary2()","ac8c6642":"finish = dt.datetime.now()\nduration =  finish - start\nprint(f'Total running time is {str(duration)[2:4]} minute, {str(duration)[5:7]} seconds')","e71ac248":"#### Using Stats model - Ztest for Hypothesis Testing","4b7336aa":"# Analyze A\/B Test Results \n\nThe Project is organized into the following sections: \n\n- [Introduction](#intro)\n- [Part I - Exploring & Probabilities](#probability)\n- [Part II - A\/B Test](#ab_test)\n- [Part III - Regression](#regression)\n- [Final Conclusion](#finalConclusion)\n\n \n\n<a id='intro'><\/a>\n## Introduction\n\nA\/B tests are very commonly performed by data analysts and data scientists. For this project, we will be working to understand the results of an A\/B test run by an e-commerce website.  The goal is to work through this notebook to help the company understand if they should:\n- Implement the new webpage, \n- Keep the old webpage, or \n- Perhaps run the experiment longer to make their decision.\n\n\n\n\n<a id='probability'><\/a>\n## Part I - Exploring & Probabilities\n\n","f1c80112":"### Description of dataset parameters\n\n","feb39188":"<a id='regression'><\/a>\n### Part III - A regression approach\n\n\nWe can use **statsmodels** library to fit the regression model to see if there is a significant difference in conversion based on the page-type a customer receives","184ea0c2":"<a id='ab_test'><\/a>\n## Part II - A\/B Test\n \n\nSince just calculated that the \"converted\" probability (or rate) for the old page is *slightly* higher than that of the new page, we will assume that the old page is better unless the new page proves to be definitely better at alpha (Type I error rate): 5%\n<br><br>\n**Hypothesis Test:**\n><center>\n$H_{0}: $   $p_{old}$ > $p_{new}$ <br><br>\n$H_{1}: $   $p_{old}$ $\\leq$ $p_{new}$\n<\/center>","51e16f30":">**There is one `user_id` repeated in the dataset.  What is it?**","4311a959":"#### Sampling 10,000 distributions using for loops","e9b62f41":"**Hypothesis test result - Regression** <br> \n\n>The p-value of ab_page is 0.1899 which indicates that the variable is not significant in predicating conversion at an alpha rate of 0.05, so we reject the null hypothesis","0ac1abe9":"### Adding countries\nIn an effort to refine the model performance and trying to get a better assesment to our statistical test, we will add more factors to the regression model to making a better predeication as we are increasing the training data by merging the countries file and adding the effect based on which country a user lives in\n\n","40a25c81":"### Observation:\n   >1. The probability of an individual received the new page or old page is almost 50%\n   >2. The conversion rate for the control group(old page) is slightly higher than the conversion rate for the treatment group\n   \n**We suspect that the new page is not leading to more conversion**\n","1e2e822c":"**Hypothesis test result** <br>\n>1. The P-value of for difference in mean between old and new page using Numpy built-in functions is greater than the Alpha value\n>2. The P-value of for difference in mean between old and new page using for loops is also greater than the Alpha value\n\nSince the P-value in both cases is greater than the alpha of 0.05, **we can reject the null hypothesis**","470fb718":"|Data columns|Purpose|Valid values|\n| ------------- |:-------------| -----:|\n|user_id|Unique ID|Int64 values|\n|timestamp|Time stamp when the user visited the webpage|-|\n|group|In the current A\/B experiment, the users are categorized into two broad groups. <br>The `control` group users are expected to be served with `old_page`; and `treatment` group users are matched with the `new_page`. <br>However, **some inaccurate rows** are present in the initial data, such as a `control` group user is matched with a `new_page`. |`['control', 'treatment']`|\n|landing_page|It denotes whether the user visited the old or new webpage.|`['old_page', 'new_page']`|\n|converted|It denotes whether the user decided to pay for the company's product. Here, `1` means yes, the user bought the product.|`[0, 1]`|","ead33d32":">Considering that the treatment group users are shown the new page, and the control group are shown the old page, **What is number of individuals in each group?**","84f15c24":"**Hypothesis test result - Z-test** <br> \n\n>As the observed p value from the z test is higher than the test alpha value of 0.05, we reject the null hypothesis","44ec2c34":"### Data Cleaning for mismatched data\nIn a particular row, the **group** and **landing_page** columns should have either of the following acceptable values:\n\n|user_id| timestamp|group|landing_page|converted|\n|---|---|---|---|---|\n|XXXX|XXXX|`control`| `old_page`|X |\n|XXXX|XXXX|`treatment`|`new_page`|X |\n\n\nIt means, the `control` group users should match with `old_page`; and `treatment` group users should matched with the `new_page`. \n\nHowever, for the rows where `treatment` does not match with `new_page` or `control` does not match with `old_page`, we cannot be sure if such rows truly received the new or old wepage.  \n","27528c81":"<a id='finalConclusion'><\/a>\n# Final conclusions<br> \n>  **<center>The p-values are above the alpha value of 0.05, we reject the null hypothesis<\/center>**  \n>- Considering the facts stated from all the previous statistical tests we can denote the below:<br><br>\n    1. There is no apparent statistical difference in performance between old page and the new page in terms of conversions rate, even with taking the the location of the customer in consideration<br><br>\n    2. Deciding on whether to continue with the new page or the old one is not yet conclusive, if we look at the subject from a practical perspective will require including other features about the customers and the products, also cost is a huge contributor in the decision making process","bf2bd945":"#### Sampling 10,000 distributions using numpy built-in fuctions"}}