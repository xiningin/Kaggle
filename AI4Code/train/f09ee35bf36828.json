{"cell_type":{"d04ea47e":"code","0816385c":"code","60d50895":"code","0452beca":"code","0de2f35e":"code","a6634385":"code","c63bc66c":"code","e3690e01":"markdown","a1e5fd1e":"markdown","72e63a40":"markdown","24d45ad2":"markdown","3baf9c1b":"markdown","cdb0f1f3":"markdown","e22d66d8":"markdown","6ed447e0":"markdown","9af8bd17":"markdown","99b79815":"markdown"},"source":{"d04ea47e":"!pip install imutils\n\nimport tensorflow as tf\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport albumentations as A\nimport imutils\n\nimport cv2\n\ndef show_sample (data, title=None, ncol=10,  path=\"..\/input\/dcaimnistroman\/data\"):\n    \n    nrow =  len(data) \/\/ ncol + min ( len(data) % ncol, 1 )\n    nrow = max ( nrow, 1)\n    files = [ f\"{path}\/{name}\" for name in  data[\"name\"].values ]\n    labels = data[\"label\"].values\n\n    fig, axes = plt.subplots(nrow, ncol, figsize=(10,  nrow), squeeze=False)\n    axes = axes.flatten()\n    \n    for i,ax in enumerate(axes):\n        \n        if i < len(files):\n            file = files[i]\n            label = labels[i]\n            image = cv2.imread(file, cv2.IMREAD_GRAYSCALE)\n            ax.imshow(image, cmap='Greys')\n            ax.set_title(f\"{label}\")\n        ax.axis(\"off\")\n    if title:\n        plt.suptitle(title)\n    plt.tight_layout()    \n    plt.show()\n\nlabels = [\"i\",\"ii\",\"iii\", \"iv\", \"v\", \"vi\", \"vii\", \"viii\", \"ix\", \"x\"]\n    \ndef show_label_disributions (data):\n    def absolute_value(val):\n        a = np.round(val\/100.*y.sum(), 0)\n        return int(a)\n\n    d = data.groupby([\"label\"]).agg({\"name\":\"count\"})[\"name\"].to_dict()\n    y = np.array([d[label] for label in labels[::-1]])\n    \n    plt.figure(figsize=(6,6))\n    plt.pie(y, labels=labels[::-1], autopct=absolute_value)\n    plt.title(\"class distributions\")\n    \n    plt.show() \n    \n\ndef show_mapping (data):\n    plt.figure(figsize=(8,8))\n    sns.scatterplot(x=\"variability\", y=\"confidence\", hue=\"correctness\", data = data)\n    plt.text(0.01, 0.1, \"hard-to-learn\", horizontalalignment='left', size='medium', color='red', weight='semibold')\n    plt.text(0.1, 0.95, \"easy-to-learn\", horizontalalignment='left', size='medium', color='green', weight='semibold')\n    plt.text(0.35, 0.5, \"ambiguous\", horizontalalignment='left', size='medium', color='blue', weight='semibold')\n\n    plt.show()    \n\ndef show_annots (data, ncol=10, path=\"..\/input\/dcaimnistroman\/data\", title=None):\n    \n    nrow =  len(data) \/\/ ncol + min ( len(data) % ncol, 1 )\n    nrow = max ( nrow, 1)\n    files = [ f\"{path}\/{name}\" for name in  data[\"name\"].values ]\n    actions = data[\"annot_action\"].values\n    annot_labels = data[\"annot_label\"].values\n    labels = data[\"label\"].values\n\n    names = data[\"name\"].values\n\n\n    fig, axes = plt.subplots(nrow, ncol, figsize=(5,  nrow), squeeze=False)\n    axes = axes.flatten()\n    \n    for i,ax in enumerate(axes):\n        \n        if i < len(files):\n            file = files[i]\n            action = actions[i]\n            annot_label = annot_labels[i]\n            label = labels[i]\n            name = names[i].split(\"-\")[0]\n            image = cv2.imread(file, cv2.IMREAD_GRAYSCALE)\n            ax.imshow(image, cmap='Greys')\n            if action == \"relabel\":\n                text = f\"{label}->{annot_label}\"\n            else:\n                text = f\"{label}\"\n            ax.set_title(text)\n        ax.axis(\"off\")\n    \n    if title:\n        plt.suptitle(title)\n    \n    plt.tight_layout()    \n\n    plt.show()\n\ndef show_augmentation(df, aug_dict, ncol=6, path=\"..\/input\/dcaimnistroman\/data\"):\n    nrow = len(aug_dict)\n\n    files = df[\"name\"].values\n    labels = df[\"label\"].values\n    \n    fig, axes = plt.subplots(nrow, ncol, figsize=(10, 2*nrow), squeeze=False)\n    for i, (key, aug) in enumerate(aug_dict.items()):\n        for j in range(ncol):\n            ax = axes[i, j]\n            if j == 0:\n                ax.text(0.5, 0.5, key, horizontalalignment='center', verticalalignment='center', fontsize=15)\n                ax.get_xaxis().set_visible(False)\n                ax.get_yaxis().set_visible(False)\n                \n            else:\n                \n                file = path + \"\/\" + files[j-1]\n                image = cv2.imread(file, cv2.IMREAD_GRAYSCALE)\n\n                label = labels [j-1]\n                \n                \n                \n                if aug is not None:\n                    # quantize\n                    image = (image \/\/ 43) * 43\n                    image[image > 43] = 255\n\n                    image = aug(image=image)['image']\n                \n                ax.imshow(image, cmap='Greys')\n                ax.set_title(f'label: {label}')\n            ax.axis('off')\n    plt.tight_layout()\n    plt.show()\n    plt.close()    \n\ndef get_image(name, path, contour):\n    \n    image = cv2.imread(path + name, cv2.IMREAD_GRAYSCALE)\n    \n    (x, y, w, h) = cv2.boundingRect(contour)\n    cv2.rectangle(image, (x, y), (x+w, y+h), (255, 255, 255), -1)\n    # quantize\n    image = (image \/\/ 43) * 43\n    image[image > 43] = 255\n    \n    return image\n\ndef sort_contours(cnts, method=\"left-to-right\"):\n    reverse = False\n    i = 0\n    if method == \"right-to-left\" or method == \"bottom-to-top\":\n        reverse = True\n    if method == \"top-to-bottom\" or method == \"bottom-to-top\":\n        i = 1\n    boundingBoxes = [cv2.boundingRect(c) for c in cnts]\n    (cnts, boundingBoxes) = zip(*sorted(zip(cnts, boundingBoxes),\n    key=lambda b:b[1][i], reverse=reverse))\n    # return the list of sorted contours and bounding boxes\n    return (cnts, boundingBoxes)\n\ndef get_segmentation(img, area_threshold=40):\n    image = cv2.imread(img)\n    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n    ret,thresh1 = cv2.threshold(gray ,127,255,cv2.THRESH_BINARY_INV)\n    dilated = cv2.dilate(thresh1, None, iterations=2)\n\n    cnts = cv2.findContours(dilated.copy(), cv2.RETR_EXTERNAL,cv2.CHAIN_APPROX_SIMPLE)\n    cnts = imutils.grab_contours(cnts)\n    cnts = sort_contours(cnts, method=\"left-to-right\")[0]\n    cnts = [c for c in cnts if cv2.contourArea(c)>area_threshold]\n    \n    # loop over the contours\n    for c in cnts:\n        (x, y, w, h) = cv2.boundingRect(c)\n        cv2.rectangle(image, (x, y), (x + w, y + h), (0, 255, 0), 2)\n    return cnts, image\n\ndef show_segmentation (data, input_path = \"..\/input\/dcaimnistroman\/data\/\"):\n   \n    for i, row in data.iterrows():\n        name = row[\"name\"]\n        label = row[\"annot_label\"]\n        contours, image = get_segmentation (input_path  + name, area_threshold = 40)\n        if len(contours)>10:\n            contours, image = get_segmentation (input_path  + name, area_threshold = 200)\n\n        if len(contours)>1:\n\n            nrow = 1\n            ncol = len(contours)+1\n\n\n            fig, axes = plt.subplots(nrow, ncol, figsize=(ncol, nrow), squeeze=False)\n            axes = axes.flatten()\n\n            ax = axes[0]\n            ax.imshow(image)\n            ax.set_title(f\"{label}:{len(contours)}\")\n            ax.axis(\"off\")\n\n            for i,contour in enumerate(contours):\n                ax = axes[i+1]\n                img = get_image(name, input_path, contour)\n                ax.imshow(img, cmap='Greys')\n                ax.set_title(f\"segm:{i}\")\n                ax.axis(\"off\")\n\n            plt.show()\n","0816385c":"base_model = tf.keras.applications.ResNet50(\n        input_shape=(32, 32, 3),\n        include_top=False,\n        weights=None,\n    )\n\nbase_model = tf.keras.Model(\n    base_model.inputs, outputs=[base_model.get_layer(\"conv2_block3_out\").output]\n)\n\ninputs = tf.keras.Input(shape=(32, 32, 3))\nx = tf.keras.applications.resnet.preprocess_input(inputs)\nx = base_model(x)\nx = tf.keras.layers.GlobalAveragePooling2D()(x)\nx = tf.keras.layers.Dense(10)(x)\nmodel = tf.keras.Model(inputs, x)\n\nmodel.compile(\n    optimizer=tf.keras.optimizers.Adam(lr=0.0001),\n    loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n    metrics=[\"accuracy\"],\n)\nmodel.summary()","60d50895":"data = pd.read_csv (\"..\/input\/dcaimnistroman\/data.csv\")\nshow_label_disributions (data)\nshow_sample ( data=data.sample(n=50, random_state=123), title=\"base dataset sample\" )\n","0452beca":"data = pd.read_csv (\"..\/input\/dcaimnistromanannotations\/annot_data.csv\")\nshow_mapping (data)\n\nshow_sample ( data=data.query(\"correctness > 0.8 and variability < 0.1 and confidence > 0.8\").sample(n=50, random_state=123), title=\"easy-to-learn sample\" )\nshow_sample ( data=data.query(\"variability > 0.1 and confidence > 0.4 and confidence < 0.6 and correctness < 0.6\").sample(n=50, random_state=123), title=\"ambiguous sample\" )\nshow_sample ( data=data.query(\"correctness < 0.6 and variability < 0.1 and confidence < 0.2\").sample(n=50, random_state=123), title=\"hard-to-learn\" )","0de2f35e":"data = pd.read_csv (\"..\/input\/dcaimnistromanannotations\/annot_data.csv\")\ndata = data.sample(frac=1.0, random_state=123)\n\nshow_annots (data.query(\"annot_action == 'none'\")[:15], ncol=5, title=\"Action:none\")\nshow_annots (data.query(\"annot_action == 'relabel'\")[:15], ncol=5, title=\"Action:relabel\")\nshow_annots (data.query(\"annot_action == 'remove'\")[:15], ncol=5, title=\"Action:remove\")\n","a6634385":"data = pd.read_csv (\"..\/input\/dcaimnistromanannotations\/annot_data.csv\")\ndata = data.sample(frac=1.0, random_state=123)\n\n    \nncol = 6\nshow_augmentation(data.query(\"annot_augmented == 1\").sample(n=ncol), {'Original': None,\n             'PiecewiseAffine': A.PiecewiseAffine (p=1.0, \n                                                  scale= (0.03, 0.1),\n                                                  nb_rows= 3,\n                                                  nb_cols= 3),\n             'ShiftScaleRotate': A.ShiftScaleRotate(\n                    shift_limit=0.2,\n                    scale_limit=0.1,\n                    rotate_limit=10,\n                    border_mode=cv2.BORDER_REPLICATE,\n                    p=1.0)\n            },\n            ncol=ncol)","c63bc66c":"data = pd.read_csv (\"..\/input\/dcaimnistromanannotations\/annot_data.csv\")\nshow_segmentation(data.sample(frac=1.0, random_state=123)[:10])","e3690e01":"## What is the Data-Centric AI Competition:\n\nAndrew Ng and Deepelearnig.ai created this new amazing challenge format in efforts to promote the data-centric AI movement -  that shift the focus of AI pratictioners from model\/algorithm development to the quality of the data they use to train models. \n\nIn most machine learning competitions, you are asked to build a high-performance model given a fixed dataset. \nHowever, machine learning has matured to the point that high-performance model architectures are widely available, while approaches to engineering datasets have lagged. \n\nThe Data-Centric AI Competition inverts the traditional format and instead asks you to improve a dataset given a fixed model. \n\nSo in this kind of competition you must iterate over data and not over model.\n\nThe objective of the competition is to produce a trainset and a validation set of pictures used by a fixed model to recognize hand-written roman numerals, starting from a base dataset of 2900++ images\n\nThe fixed model is a cut of a ResNet50 with batch 8 trained for 100 epochs and a maximum of 10_000 (32x32 resized) images can be used splitted between train and validation set.\n\n[more details on Data-Centric AI Competition](https:\/\/https-deeplearning-ai.github.io\/data-centric-comp\/)\n","a1e5fd1e":"At this point we are able to split folds through a multilabel stratification using [label, confidence, variability, correctness, annot_action] and then train and evaluate the model for each fold.","72e63a40":"## The base dataset","24d45ad2":"*stay tuned ! we have heard that upcoming data centric competition maybe will be on Kaggle!*","3baf9c1b":"### How to Augment data ?\n\nWe used two kinds of augmentation:\n1. Classic augmentation by \"distortion\" of existing items\n\nWe are not \"true\" computer vision experts and so we went through some handwritten recognition literature and we found that very common parameters are the following:\n\n    aug_1 = A.ShiftScaleRotate(\n                    shift_limit=0.2,\n                    scale_limit=0.1,\n                    rotate_limit=10,\n                    border_mode=cv2.BORDER_REPLICATE,\n                    p=1.0)\n\n    aug_0 = A.PiecewiseAffine (p=1.0,scale= (0.03, 0.1),nb_rows= 3, nb_cols= 3)\n    \n\n\nMaking some experiments we realized that it was better to apply this type of augmentation not to all items and therefore\nwe defined \"item to be augmented\" as item that is clearly classifiable by human eye and which is under-represented.\n\n","cdb0f1f3":"### The fixed model","e22d66d8":"At this point we have two datasets\n- the original one, divided by kfold for which the \"denoised\" label and the feature \"augmented\" is defined for each item\n- the augmented one with the data generated by the segmentation (which does not contain iv, viii and ix) \n\n\n\nThe iterations we did during the competition had the goal of redefining  annotations features of dataset and defining the various percentages of parts (original items, augmented items, items of the segmented dataset) \nfor each  classes to maximize average fold accuracy and minimize standard deviation","6ed447e0":"2. Augmentation via segmentation\n\nThe Roman numbering system is an additive \/ subtractive numbering system: the number represented is given by the sum or difference of the values of each symbol that composes it.\nIn this sense, not all the symbols of the first 10 cardinal numbers are unique but some are the composition of the symbols (\"i\", \"v\", \"x\"). \n\nWe have generated  - via segmentation script - all the images obtained by eliminating a single segment from \"ii\", \"iii\", \"iv\", \"vi\", \"vii\", \"viii\", \"ix\" items.\nIn practice from \"ii\" we tried to generate \"i\" and \"i\", from vi we tried to generate \"v\" and \"i\" etc .... : we have been careful to create only images that have a different label from the original. \n\nA manual observation made it possible to distinguish the generated images that could be used as items of Roman numeral. ","9af8bd17":"## How we (accademia_del_cimento) reach 85% accuracy  ##\n\nOur main idea is to use a k-fold cross-validation to find a k-fold partition that maximize average fold's accuracy and minimize standard deviation of the fold accuracy and then pick up one of fold to generate the train\/validation for submission\n\nDuring the k-fold cross-validation process, data is split in folds with (almost) the same length:  out-of-fold predictions are made on each (validation) fold using data in other folds to train the model.\n\nMean and standard deviation of out-of-fold predictions are most commonly used to estimate the performance of a model when making predictions on unseen data.\n\nFrom data-centric point of view, we think that finding a balanced partition could be a good way to estimate the goodness of the data.\n\nSince it is extremely important that each fold is balanced not only in terms of label distribution  but also in terms of training difficulty and noise, we used a method called [\"Training Dynamics\"](https:\/\/arxiv.org\/abs\/2009.10795), which consists in training all the data for a fixed number of epochs to associate features to each sample that define its \"difficulty to learn level\".\n\nThe features genetated in this way for each items are:\n* **correctness**: the fraction of times the model correctly labels across epochs\n* **confidence**: the mean of probality of the true label across epochs\n* **variability**: the standard deviation of the probality of the true label across epochs\n\nIn this way it is possible to classify the elements that are **\"easy-to-learn\"** (high correctness\/confience and low variability), **\"hard-to-learn\"** (low corretcness, confidence and variability)  and **ambiguous**. ","99b79815":"Because **\"hard-to-learn\"** items could be both edge cases and noisy items we manually annotated each element with an action to take to \"correct\" the noisy items.\n\nAnnotated action could be:\n* relabeling (the label is modified because it is wrong) \n* remove  (item is removed because too noisy)\n* none (no action is needed)\n"}}