{"cell_type":{"be292f6f":"code","6c861696":"code","1261f4f0":"code","7ccd1794":"code","8a6d6ff0":"code","8e87b3b8":"code","b17e89c8":"code","918bb1b9":"code","28c5fe01":"code","99522a72":"code","142b3e29":"code","f2e87fa9":"code","dc35da36":"code","6d72e812":"code","ffe799b9":"code","aa61cfde":"code","7d7fc176":"code","17b3ecaa":"code","3d2552aa":"code","5c94aeb8":"code","58a9bbd2":"code","5c642072":"code","13d6906c":"code","cae995c0":"code","4708db2a":"code","ec72ff9f":"code","0ded4616":"code","933d7308":"code","d2ffa74c":"code","7e47b7f8":"code","488e8919":"code","e30415b4":"code","714b6158":"code","f70114fb":"code","f786149f":"code","826c5ca0":"code","4820678c":"code","2cbfad14":"code","74bd538c":"code","59cfe167":"code","864555a4":"code","52a0625a":"code","984037aa":"code","6bcf982c":"code","15188e61":"code","f62301ec":"code","68dcb1e2":"code","1aa852bb":"code","ae4a158e":"code","1084b647":"code","b9f93159":"code","996b66de":"code","c695bc11":"code","471f5a91":"code","d8b17060":"code","331be37c":"code","f0325a0e":"code","70cc9608":"code","48f926bd":"code","5a11a9bf":"code","5ebc72ae":"code","0395f7de":"code","d383d38f":"markdown","35418c94":"markdown","ab438246":"markdown","910668d7":"markdown","cf672888":"markdown","c40936e0":"markdown","9222ff2e":"markdown","bc501310":"markdown","28dfbbcc":"markdown","c6aace17":"markdown","9dc73747":"markdown","0ba5a8c7":"markdown","d59a7847":"markdown","50201a93":"markdown"},"source":{"be292f6f":"##########################################################################################################\n# IMPORT NECESSARY LIBRARIES AND MODULES\n# Making necessary adjustments for the representation of the dataset\n##########################################################################################################\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\n\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import RobustScaler, MinMaxScaler, LabelEncoder\nfrom sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\nfrom sklearn.svm import SVR\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, VotingRegressor\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV, cross_val_score\n\nimport warnings\nwarnings.simplefilter(action='ignore', category=Warning)\n\npd.set_option('display.max_columns', None)\npd.set_option('display.float_format', lambda x: '%.3f' % x)\npd.set_option('display.width', 500)\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","6c861696":"##########################################################################################################\n# HELPER FUNCTIONS \n##########################################################################################################\n# 1. Data Pre-processing\ndef outlier_thresholds(dataframe, col_name, q1=0.10, q3=0.90):\n    quartile1 = dataframe[col_name].quantile(q1)\n    quartile3 = dataframe[col_name].quantile(q3)\n    interquantile_range = quartile3 - quartile1\n    up_limit = quartile3 + 1.5 * interquantile_range\n    low_limit = quartile1 - 1.5 * interquantile_range\n    return low_limit, up_limit\n\ndef check_outlier(dataframe, col_name):\n    low_limit, up_limit = outlier_thresholds(dataframe, col_name)\n    if dataframe[(dataframe[col_name] > up_limit) | (dataframe[col_name] < low_limit)].any(axis=None):\n        return True\n    else:\n        return False\n\ndef grab_outliers(dataframe, col_name, index=False):\n    low, up = outlier_thresholds(dataframe, col_name)\n    if dataframe[((dataframe[col_name] < low) | (dataframe[col_name] > up))].shape[0] > 10:\n        print(dataframe[((dataframe[col_name] < low) | (dataframe[col_name] > up))].head())\n    else:\n        print(dataframe[((dataframe[col_name] < low) | (dataframe[col_name] > up))])\n\n    if index:\n        outlier_index = dataframe[((dataframe[col_name] < low) | (dataframe[col_name] > up))].index\n        return outlier_index\n\n\ndef remove_outlier(dataframe, col_name):\n    low_limit, up_limit = outlier_thresholds(dataframe, col_name)\n    df_without_outliers = dataframe[~((dataframe[col_name] < low_limit) | (dataframe[col_name] > up_limit))]\n    return df_without_outliers\n\ndef replace_with_thresholds(dataframe, variable):\n    low_limit, up_limit = outlier_thresholds(dataframe, variable)\n    dataframe.loc[(dataframe[variable] < low_limit), variable] = low_limit\n    dataframe.loc[(dataframe[variable] > up_limit), variable] = up_limit\n\ndef missing_values_table(dataframe, na_name=False):\n    na_columns = [col for col in dataframe.columns if dataframe[col].isnull().sum() > 0]\n    n_miss = dataframe[na_columns].isnull().sum().sort_values(ascending=False)\n    ratio = (dataframe[na_columns].isnull().sum() \/ dataframe.shape[0] * 100).sort_values(ascending=False)\n    missing_df = pd.concat([n_miss, np.round(ratio, 2)], axis=1, keys=['n_miss', 'ratio'])\n    print(missing_df, end=\"\\n\")\n    if na_name:\n        return na_columns\n\ndef missing_vs_target(dataframe, target, na_columns):\n    temp_df = dataframe.copy()\n    for col in na_columns:\n        temp_df[col + '_NA_FLAG'] = np.where(temp_df[col].isnull(), 1, 0)\n    na_flags = temp_df.loc[:, temp_df.columns.str.contains(\"_NA_\")].columns\n    for col in na_flags:\n        print(pd.DataFrame({\"TARGET_MEAN\": temp_df.groupby(col)[target].mean(),\n                            \"Count\": temp_df.groupby(col)[target].count()}), end=\"\\n\\n\\n\")\n\ndef rare_analyser(dataframe, target, cat_cols):\n    for col in cat_cols:\n        print(col, \":\", len(dataframe[col].value_counts()))\n        print(pd.DataFrame({\"COUNT\": dataframe[col].value_counts(),\n                            \"RATIO\": dataframe[col].value_counts() \/ len(dataframe),\n                            \"TARGET_MEAN\": dataframe.groupby(col)[target].mean()}), end=\"\\n\\n\\n\")\n\ndef label_encoder(dataframe, binary_col):\n    labelencoder = LabelEncoder()\n    dataframe[binary_col] = labelencoder.fit_transform(dataframe[binary_col])\n    return dataframe\n        \n# 2. Exploratory Data Analysis \ndef check_df(dataframe, head=5, tail=5, quan=False):\n    print(\"##################### Shape #####################\")\n    print(dataframe.shape)\n    print(\"##################### Types #####################\")\n    print(dataframe.dtypes)\n    print(\"##################### Head #####################\")\n    print(dataframe.head(head))\n    print(\"##################### Tail #####################\")\n    print(dataframe.tail(tail))\n    print(\"##################### NA #####################\")\n    print(dataframe.isnull().sum())\n\n    if quan:\n        print(\"##################### Quantiles #####################\")\n        print(dataframe.quantile([0, 0.05, 0.50, 0.95, 0.99, 1]).T)\n\ndef grab_col_names(dataframe, cat_th=10, car_th=20):\n    # cat_cols, cat_but_car\n    cat_cols = [col for col in dataframe.columns if dataframe[col].dtypes == \"O\"]\n\n    num_but_cat = [col for col in dataframe.columns if dataframe[col].nunique() < cat_th and\n                   dataframe[col].dtypes != \"O\"]\n\n    cat_but_car = [col for col in dataframe.columns if dataframe[col].nunique() > car_th and\n                   dataframe[col].dtypes == \"O\"]\n\n    cat_cols = cat_cols + num_but_cat\n    cat_cols = [col for col in cat_cols if col not in cat_but_car]\n\n    # num_cols\n    num_cols = [col for col in dataframe.columns if dataframe[col].dtypes != \"O\"]\n    num_cols = [col for col in num_cols if col not in num_but_cat]\n\n    # binary_cols\n    binary_cols = [col for col in dataframe.columns if df[col].dtype not in [int, float]\n                   and dataframe[col].nunique() == 2]\n\n    print(f\"Observations: {dataframe.shape[0]}\")\n    print(f\"Variables: {dataframe.shape[1]}\")\n    print(f'cat_cols: {len(cat_cols)}')\n    print(f'num_cols: {len(num_cols)}')\n    print(f'cat_but_car: {len(cat_but_car)}')\n    print(f'num_but_cat: {len(num_but_cat)}')\n    print(f'binary_cols: {len(binary_cols)}')\n\n    return cat_cols, num_cols, cat_but_car, num_but_cat, binary_cols\n\ndef cat_summary(dataframe, col_name, plot=False):\n    print(pd.DataFrame({col_name: dataframe[col_name].value_counts(),\n                        \"Ratio\": 100 * dataframe[col_name].value_counts() \/ len(dataframe)}))\n    print(\"##########################################\")\n\n    if plot:\n        sns.countplot(x=dataframe[col_name], data=dataframe)\n        plt.show()","1261f4f0":"# Read the dataset\ndf_ = pd.read_csv(\"..\/input\/hitters\/hitters.csv\")\ndf = df_.copy()","7ccd1794":"check_df(df)","8a6d6ff0":"# We have 59 NA values in \"Salary\"\n# Observe NaN values\ndf[df.isnull().any(axis=1)].head()","8e87b3b8":"missing_values_table(df)","b17e89c8":"df.describe().T","918bb1b9":"# Distplot of target variable\nsns.distplot(df.Salary)","28c5fe01":"cat_cols, num_cols, cat_but_car, num_but_cat, binary_cols = grab_col_names(df)","99522a72":"# Correlation matrix\nplt.figure(figsize=(15, 10))\nmask = np.triu(np.ones_like(df.corr().round(2)))\ndataplot = sns.heatmap(df.corr(), cmap=\"YlGnBu\", annot=True, mask=mask)\nplt.show()","142b3e29":"# We have 59 NaN values in \"Salary\". Their ratio was > 18\n# I will extract NaN values of \"Salary\"\ndf2 = df[~(df.isnull().any(axis=1))]\ndf2.shape","f2e87fa9":"# Implement grab_col_names fo df2\ncat_cols2, num_cols2, cat_but_car2, num_but_cat2, binary_cols2 = grab_col_names(df2)","dc35da36":"# Summary of categorical columns\nfor col in cat_cols2:\n    cat_summary(df2, col, plot=False)","6d72e812":"# Outlier Analysis\nfor col in num_cols:\n    print(col, check_outlier(df2, col))","ffe799b9":"# Keep in mind min=0 values while feature engineering\nmin_zero_cols = [col for col in df2 if df2[col].min() == 0]\nmin_zero_cols  ","aa61cfde":"# Years\nsns.displot(df2.Years)","7d7fc176":"# I choose the bins considering the distplot of \"Years\"\ndf2[\"New_Years_Cat\"] = pd.cut(x=df2['Years'],\n                              bins=[0, 3, 6, 10, 15, 19, 24],\n                              labels=[\"(0-3]\", \"(3, 6]\", \"(6, 10]\", \"(10, 15]\", \"(15, 19]\", \"(19, 24]\"])","17b3ecaa":"df2.groupby(['League','Division', 'New_Years_Cat']).agg({'Salary':'mean'})","3d2552aa":"df2.groupby(['New_Years_Cat']).agg({'Salary':'mean'})","5c94aeb8":"df2.groupby(['Division']).agg({'Salary':'mean'})","58a9bbd2":"df2.groupby(['League']).agg({'Salary':'mean'})","5c642072":"df2.groupby(['NewLeague']).agg({'Salary':'mean'})","13d6906c":"# CREATING NEW FEATURES\n\n# BATTING AVERAGE\n# 1986-1987 season\ndf2[\"New_Batting_Avg\"] = df2[\"Hits\"]\/df2[\"AtBat\"]\n# Whole career\ndf2[\"New_C_Batting_Avg\"] = df2[\"CHits\"]\/df2[\"CAtBat\"]\n\n# PERCENTAGE OF HOME RUNS\n# 1986-1987 Season\ndf2.loc[df2[\"Runs\"] != 0, \"New_Percentage_HmRun\"] = df2[\"HmRun\"] \/ df2[\"Runs\"] # divide by 0 correction\ndf2.loc[df2[\"Runs\"] == 0, \"New_Percentage_HmRun\"] = 0\n# Whole career\ndf2[\"New_C_Percentage_HmRun\"] = df2[\"CHmRun\"]\/df2[\"CRuns\"]\n\n# AT BATS PER HOME RUN\n# 1986-1987 Season\ndf2.loc[df2[\"HmRun\"] != 0, \"New_AtBat_Per_HmRun\"] = df2[\"AtBat\"] \/ df2[\"HmRun\"] # divide by 0 correction\ndf2.loc[df2[\"HmRun\"] == 0, \"New_AtBat_Per_HmRun\"] = 0\n# Whole career\ndf2.loc[df2[\"CHmRun\"] != 0, \"New_C_AtBat_Per_HmRun\"] = df2[\"CAtBat\"] \/ df2[\"CHmRun\"] # divide by 0 correction\ndf2.loc[df2[\"CHmRun\"] == 0, \"New_C_AtBat_Per_HmRun\"] = 0\n\n# HOME RUN PER HITS\n# 1986-1987 Season\ndf2[\"New_HmRun_Per_Hits\"] = df2[\"HmRun\"]\/df2[\"Hits\"]\n# Whole career\ndf2[\"New_C_HmRun_Per_Hits\"] = df2[\"CHmRun\"]\/df2[\"CHits\"]\n\n# FIELDING PERCENTAGE (FPCT)\n# FPCT = (Putouts + Assists) \/ (Total Chances)\n# Total Chances = Putouts + Assists + Errors\n# 1986-1987 Season\ndf2.loc[(df2[\"PutOuts\"]+df2[\"Assists\"]+df2[\"Errors\"]) != 0, \"New_FPCT\"] = (df2[\"PutOuts\"]+df2[\"Assists\"]) \/ (df2[\"PutOuts\"]+df2[\"Assists\"]+df2[\"Errors\"]) # divide by 0 correction\ndf2.loc[(df2[\"PutOuts\"]+df2[\"Assists\"]+df2[\"Errors\"]) == 0, \"New_FPCT\"] = 0\n\n# PERFORMANCES OF 1986-1987 SEASON\ndf2[\"New_Perf_Hits\"] = df2[\"Hits\"]\/df2[\"CHits\"]\ndf2[\"New_Perf_AtBat\"] = df2[\"AtBat\"]\/df2[\"CAtBat\"]\ndf2.loc[df2[\"CHmRun\"] != 0, \"New_Perf_HmRun\"] = df2[\"HmRun\"] \/ df2[\"CHmRun\"] # divide by 0 correction\ndf2.loc[df2[\"CHmRun\"] == 0, \"New_Perf_HmRun\"] = 0\ndf2[\"New_Perf_Runs\"] = df2[\"Runs\"]\/df2[\"CRuns\"]\ndf2[\"New_Perf_RBI\"] = df2[\"RBI\"]\/df2[\"CRBI\"]\ndf2[\"New_Perf_Walks\"] = df2[\"Walks\"]\/df2[\"CWalks\"]\n\n# AVERAGE STATISTICS OF 1986-1987 SEASON\ndf2[\"New_Avg_Hits\"] = df2[\"CHits\"]\/df2[\"Years\"]\ndf2[\"New_Avg_AtBat\"] = df2[\"CAtBat\"]\/df2[\"Years\"]\ndf2[\"New_Avg_HmRun\"] = df2[\"CHmRun\"]\/df2[\"Years\"]\ndf2[\"New_Avg_Runs\"] = df2[\"CRuns\"]\/df2[\"Years\"]\ndf2[\"New_Avg_RBI\"] = df2[\"CRBI\"]\/df2[\"Years\"]\ndf2[\"New_Avg_Walks\"] = df2[\"CWalks\"]\/df2[\"Years\"]","cae995c0":"# HOME RUN ANALYSIS\nsns.distplot(df2.HmRun)","4708db2a":"df2[\"New_HmRun_Cat\"] = pd.cut(x=df2['HmRun'],\n                              bins=[0, 10, 20, 30, 40],\n                              labels=[\"(0-10]\", \"(10-20]\", \"(20-30]\", \"(30-40]\"])","ec72ff9f":"df2.groupby('New_HmRun_Cat').agg({'Salary':'mean'})","0ded4616":"# Let's see our dataset after creating new features\ndf3 = df2.copy()\ndf3.describe().T","933d7308":"# Checking null values\ndf3.isnull().sum()","d2ffa74c":"# There are 10 NaN values at \"New_HmRun_Cat\"\n# Observe NaN values\ndf3[df3.isnull().any(axis=1)].head(10)","7e47b7f8":"# These NaN values are resulting of HmRun = 0, so we can fill this value with zero (\"0-10\" for this categorical feature).\ndf3[\"New_HmRun_Cat\"].fillna(\"(0-10]\", inplace=True)","488e8919":"# As we have a category type of variables (\"New_Years_Cat\" and \"New_HmRun_Cat\"), we need to find cat_cols, etc. again:\ncat_cols3 = [col for col in df3.columns if df3[col].dtypes not in [\"int\", \"int64\",\"float64\"]]\n\ncat_but_car3 = [col for col in df3.columns if df3[col].nunique() > 20 and df3[col].dtypes == \"O\"] \n\n# num_cols\nnum_cols3 = [col for col in df3.columns if df3[col].dtypes in [\"int\", \"int64\",\"float64\"]]\nlen(num_cols3)      \n\n# binary_cols\nbinary_cols3 = [col for col in df3.columns if df3[col].dtype not in [int, float] and\n                df3[col].nunique() == 2]\n\nprint(f\"Observations: {df3.shape[0]}\")\nprint(f\"Variables: {df3.shape[1]}\")\nprint(f'cat_cols: {len(cat_cols3)}')\nprint(f'num_cols: {len(num_cols3)}')\nprint(f'cat_but_car: {len(cat_but_car3)}')\nprint(f'binary_cols: {len(binary_cols3)}')","e30415b4":"# We will not use \"Salary\" column while encoding\nnum_cols3 = [col for col in num_cols3 if \"Salary\" not in col]","714b6158":"# LABEL ENCODING\n# Label encoding for binary columns and categorical columns:\nfor col in cat_cols3:\n    df3 = label_encoder(df3, col)","f70114fb":"# ROBUST SCALER\nrs = RobustScaler()\ndf3[num_cols3] = rs.fit_transform(df3[num_cols3])","f786149f":"# Let's see the scaled end encoded version of the dataset:\ndf3.describe().T","826c5ca0":"# RARE ANALYSIS\nrare_analyser(df3, \"Salary\", cat_cols3)","4820678c":"# Selecting dependent and independent variables\ny = df3[[\"Salary\"]]\nX = df3.drop(\"Salary\", axis=1)","2cbfad14":"######################################################\n# Base Models\n######################################################\n# First check base models and select among them\nmodels = [('LR', LinearRegression()),\n          (\"Ridge\", Ridge()),\n          (\"Lasso\", Lasso()),\n          (\"ElasticNet\", ElasticNet()),\n          ('KNN', KNeighborsRegressor()),\n          ('CART', DecisionTreeRegressor()),\n          ('RF', RandomForestRegressor()),\n          ('SVR', SVR()),\n          ('GBM', GradientBoostingRegressor()),\n          (\"XGBoost\", XGBRegressor(objective='reg:squarederror')),\n          (\"LightGBM\", LGBMRegressor())]\n\nfor name, regressor in models:\n    rmse = np.mean(np.sqrt(-cross_val_score(regressor, X, y, cv=10, scoring=\"neg_mean_squared_error\")))\n    print(f\"RMSE: {round(rmse, 4)} ({name}) \")","74bd538c":"################################################\n# Random Forests\n################################################\nrf_model = RandomForestRegressor(random_state=1)\n\nrmse = np.mean(np.sqrt(-cross_val_score(rf_model, X, y, cv=10, scoring=\"neg_mean_squared_error\")))\nprint(f\"RMSE: {round(rmse, 4)} \")       \n\nrf_params = {\"max_depth\": [2, 5, 8, 15, None],\n             \"max_features\": [2, 3, 5, 7, \"auto\"],\n             \"min_samples_split\": [2, 5, 8, 15, 20],\n             \"n_estimators\": [200, 500, 700, 1000]}\n\nrf_best_grid = GridSearchCV(rf_model, rf_params, cv=3, n_jobs=-1, verbose=False).fit(X, y.values.ravel())\n\nrf_final = rf_model.set_params(**rf_best_grid.best_params_,\n                               random_state=1).fit(X, y)\n\nrmse = np.mean(np.sqrt(-cross_val_score(rf_final, X, y, cv=10, scoring=\"neg_mean_squared_error\")))\nprint(f\"RMSE (After): {round(rmse, 4)} \")       \nprint(f\"RF best params: {rf_best_grid.best_params_}\", end=\"\\n\\n\")\n","59cfe167":"################################################\n# GBM Model\n################################################\ngbm_model = GradientBoostingRegressor(random_state=1)\n\nrmse = np.mean(np.sqrt(-cross_val_score(gbm_model, X, y, cv=10, scoring=\"neg_mean_squared_error\")))\nprint(f\"RMSE: {round(rmse, 4)} \")       \n\ngbm_params = {\"learning_rate\": [0.01, 0.03, 0.05, 0.1, 0.2],\n              \"max_depth\": [2, 3, 4, 5, 8],\n              \"n_estimators\": [100, 200, 500, 700],\n              \"subsample\": [1, 0.3, 0.5, 0.7]}\n\ngbm_best_grid = GridSearchCV(gbm_model, gbm_params, cv=3, n_jobs=-1, verbose=False).fit(X, y.values.ravel())\n\ngbm_final = gbm_model.set_params(**gbm_best_grid.best_params_,\n                               random_state=1).fit(X, y)\n\nrmse = np.mean(np.sqrt(-cross_val_score(gbm_final, X, y, cv=10, scoring=\"neg_mean_squared_error\")))\nprint(f\"RMSE (After): {round(rmse, 4)} \")       \nprint(f\"GBM best params: {gbm_best_grid.best_params_}\", end=\"\\n\\n\")","864555a4":"################################################\n# LightGBM Model\n################################################\nlgbm_model = LGBMRegressor(random_state=1)\n\nrmse = np.mean(np.sqrt(-cross_val_score(lgbm_model, X, y, cv=10, scoring=\"neg_mean_squared_error\")))\nprint(f\"RMSE: {round(rmse, 4)} \")       \n\nlightgbm_params = {\"learning_rate\": [0.01, 0.03, 0.05, 0.1, 0.2, 0.3],\n                   \"n_estimators\": [200, 300, 400, 500, 700, 1000],\n                   \"colsample_bytree\": [0.2, 0.3, 0.4, 0.5, 0.7]}\n\nlgbm_best_grid = GridSearchCV(lgbm_model, lightgbm_params, cv=3, n_jobs=-1, verbose=False).fit(X, y.values.ravel())\n\nlgbm_final = lgbm_model.set_params(**lgbm_best_grid.best_params_,\n                               random_state=1).fit(X, y)\n\nrmse = np.mean(np.sqrt(-cross_val_score(lgbm_final, X, y, cv=10, scoring=\"neg_mean_squared_error\")))\nprint(f\"RMSE (After): {round(rmse, 4)} \")       \nprint(f\"LightGBM best params: {lgbm_best_grid.best_params_}\", end=\"\\n\\n\")","52a0625a":"# FEATURE IMPORTANCE\ndef plot_importance(model, features, num=len(X), save=False):\n    feature_imp = pd.DataFrame({'Value': model.feature_importances_, 'Feature': features.columns})\n    plt.figure(figsize=(10, 10))\n    sns.set(font_scale=1)\n    sns.barplot(x=\"Value\", y=\"Feature\", data=feature_imp.sort_values(by=\"Value\",\n                                                                     ascending=False)[0:num])\n    plt.title('Features')\n    plt.title(f\"Features for {type(model).__name__}\")\n    plt.tight_layout()\n    plt.show()\n    if save:\n        plt.savefig('importances.png')","984037aa":"plot_importance(rf_final, X, num=20)","6bcf982c":"plot_importance(gbm_final, X, num=20)","15188e61":"plot_importance(lgbm_final, X, num=20)","f62301ec":"# MODEL BASED FEATURE SELECTION\n# Get the importances of each final model\nfeature_imp_rf = pd.DataFrame({'Value': rf_final.feature_importances_, 'Feature': X.columns})\nfeature_imp_gbm = pd.DataFrame({'Value': gbm_final.feature_importances_, 'Feature': X.columns})\nfeature_imp_lgbm = pd.DataFrame({'Value': lgbm_final.feature_importances_, 'Feature': X.columns})","68dcb1e2":"# Gather them in one dataframe (First merge RF and GBM)\nfeature_df = pd.merge(feature_imp_rf, feature_imp_gbm, on=\"Feature\")\n\n# Add LightGBM to feature_df\nfeature_df2 = pd.merge(feature_df, feature_imp_lgbm, on=\"Feature\")\n\nfeature_df2.head()","1aa852bb":"# Rename the columns\nfeature_df = feature_df2.rename(columns = {'Value_x': 'RF_score',\n                                           'Value_y': 'GBM_score',\n                                           'Value': 'LGBM_score'})\n\nfeature_df = feature_df[['Feature', 'RF_score', 'GBM_score', 'LGBM_score']]\n\nfeature_df.head(3)","ae4a158e":"# Our dataframe needs to be standardized (Check the scores!) \n# Standardization with MinMaxScaler\nnum_cols_imp = [col for col in feature_df.columns if feature_df[col].dtypes in ['int32', 'float32', 'float64']]\nscaler = MinMaxScaler()\nfeature_df[num_cols_imp] = scaler.fit_transform(feature_df[num_cols_imp])\nfeature_df.head()","1084b647":"# Calculate and sort weighted scores\nfeature_df[\"Weighted_Score\"] = (feature_df[\"RF_score\"] + feature_df[\"GBM_score\"] + feature_df[\"LGBM_score\"])\/3\n\ndff = feature_df[['Feature', 'Weighted_Score']]\n\ndff.sort_values(by= 'Weighted_Score', ascending=False).head(10)","b9f93159":"######################################################\n# MODEL ACCORDING TO NEW FEATURE SELECTION\n######################################################\n# I can drop least important features and build models again and check the RMSE scores.\n# I will drop features whose scores < 0.3\n\nto_drop = dff.loc[dff['Weighted_Score'] < 0.3]\nfeatures_to_drop = to_drop['Feature']\n\n# Before dropping above features, I will drop high correlated features first.\n# Remembering the correlation matrix, we can drop CRuns and CAtBat.\n# Because correlation between CAtBat and CRuns = 0.98\n# Correlation between CHits and CRuns = 0.98\n# Correlation between CHits and CAtBat = 1\n\ndf4 = df3.drop('CRuns', axis=1)\ndf4 = df4.drop('CAtBat', axis=1)","996b66de":"# This is the dataframe that we will build our models\ndf4.head(3)","c695bc11":"# Modeling again\ny2 = df4[[\"Salary\"]]\nX2 = df4.drop(\"Salary\", axis=1)\n\nmodels = [('RF', RandomForestRegressor(random_state=1)),\n          ('GBM', GradientBoostingRegressor(random_state=1)),\n          (\"LightGBM\", LGBMRegressor(random_state=1))]\n\nfor name, regressor in models:\n    rmse = np.mean(np.sqrt(-cross_val_score(regressor, X2, y2, cv=10, scoring=\"neg_mean_squared_error\")))\n    print(f\"RMSE: {round(rmse, 4)} ({name}) \")","471f5a91":"# I will drop the least important features found above.\ndf5= df4.drop(features_to_drop, axis=1)\n\ny3 = df5[[\"Salary\"]]\nX3 = df5.drop(\"Salary\", axis=1)\n\n# Check RMSE values:\nfor name, regressor in models:\n    rmse = np.mean(np.sqrt(-cross_val_score(regressor, X3, y3, cv=10, scoring=\"neg_mean_squared_error\")))\n    print(f\"RMSE: {round(rmse, 4)} ({name}) \")","d8b17060":"# For Random Forest: \nrf_model = RandomForestRegressor(random_state=1)\n\nrf_random_params = {\"max_depth\": np.random.randint(5, 50, 10),\n                    \"max_features\": [2, 3, 5, 7, \"auto\", \"sqrt\"],\n                    \"min_samples_split\": np.random.randint(2, 50, 20),\n                    \"n_estimators\": [int(x) for x in np.linspace(start=200, stop=1500, num=10)]}\n\nrf_random = RandomizedSearchCV(estimator=rf_model,\n                               param_distributions=rf_random_params,\n                               n_iter=100,  \n                               cv=3,\n                               verbose=True,\n                               random_state=1,\n                               n_jobs=-1)\n\nrf_random.fit(X3, y3.values.ravel())","331be37c":"# Best parameter values\nrf_random.best_params_","f0325a0e":"# Final Model\nrf_random_final = rf_model.set_params(**rf_random.best_params_,\n                                      random_state=1).fit(X3, y3)","70cc9608":"# Let's see RMSE now:\nrmse = np.mean(np.sqrt(-cross_val_score(rf_random_final, X3, y3, cv=10, scoring=\"neg_mean_squared_error\")))\nprint(f\"RMSE (After): {round(rmse, 4)} \")       \nprint(f\"RF random final best params: {rf_random.best_params_}\", end=\"\\n\\n\")","48f926bd":"# Let's implement automated hyperparameter optimisation to the best parameters found above and see the results\n\nrf_params = {\"max_depth\": [2, 3, 24, None],\n             \"max_features\": [2, 3, 5],\n             \"min_samples_split\": [2, 3, 4],\n             \"n_estimators\": [200, 633, 1000]}\n\ngbm_params = {\"learning_rate\": [0.05, 0.1, 0.2],\n              \"max_depth\": [2, 3, 5],\n              \"n_estimators\": [100, 300, 500],\n              \"subsample\": [0.3, 0.5, 1]}\n\nlightgbm_params = {\"learning_rate\": [0.03, 0.1, 0.2],\n                   \"n_estimators\": [500, 700, 1000],\n                   \"colsample_bytree\": [0.01, 0.2, 0.3]}\n\nregressors = [(\"RF\", RandomForestRegressor(random_state=1), rf_params),\n              (\"GBM\", GradientBoostingRegressor(random_state=1), gbm_params),\n              ('LightGBM', LGBMRegressor(random_state=1), lightgbm_params)]\n\nbest_models = {}\n\nfor name, regressor, params in regressors:\n    print(f\"########## {name} ##########\")\n    rmse = np.mean(np.sqrt(-cross_val_score(regressor, X3, y3, cv=10, scoring=\"neg_mean_squared_error\")))\n    print(f\"RMSE: {round(rmse, 4)} ({name}) \")\n\n    gs_best = GridSearchCV(regressor, params, cv=3, n_jobs=-1, verbose=False).fit(X3, y3.values.ravel())\n\n    final_model = regressor.set_params(**gs_best.best_params_)\n    rmse = np.mean(np.sqrt(-cross_val_score(final_model, X3, y3, cv=10, scoring=\"neg_mean_squared_error\")))\n    print(f\"RMSE (After): {round(rmse, 4)} ({name}) \")\n\n    print(f\"{name} best params: {gs_best.best_params_}\", end=\"\\n\\n\")\n\n    best_models[name] = final_model","5a11a9bf":"voting_reg = VotingRegressor(estimators=[('RF', best_models[\"RF\"]),\n                                         ('GBM', best_models[\"GBM\"]),\n                                         ('LightGBM', best_models[\"LightGBM\"])])\n\nvoting_reg.fit(X3, y3)\n\nrmse = np.mean(np.sqrt(-cross_val_score(voting_reg, X3, y3, cv=10, scoring=\"neg_mean_squared_error\")))\nprint(f\"RMSE: {round(rmse, 4)} ({name}) \")\n","5ebc72ae":"# Selection of random user and make prediction\nrandom_user = X3.sample(1, random_state=42)\nvoting_reg.predict(random_user)[0] ","0395f7de":"# Let's see the original salary (I check the index number from PyCharm)\ndf.iloc[148]['Salary']    ","d383d38f":"#### I choose RF, GBM and LightGBM as they have less RMSE than other models. ","35418c94":"#### **DATA SET STORY:**\n\n- This dataset was originally taken from the StatLib library at Carnegie Mellon University.\n- This is part of the data that was used in the 1988 ASA Graphics Section Poster Session.\n- The salary data were originally from Sports Illustrated, April 20, 1987.\n- The 1986 and career statistics were obtained from The 1987 Baseball Encyclopedia Update published by Collier Books, Macmillan Publishing Company, New York.\n\n#### **ATTRIBUTES:**\n\n#### A data frame with 322 observations of major league players on the following 20 variables.\n\n- AtBat: Number of times at bat in 1986-1987 season\n- Hits: Number of hits in 1986-1987 season\n- HmRun: Number of home runs in 1986-1987 season\n- Runs: Number of runs in 1986-1987 season\n- RBI: Number of runs batted in 1986-1987 season\n- Walks: Number of walks in 1986-1987 season\n- Years: Number of years in the major leagues\n- CAtBat: Number of times at bat during his career\n- CHits: Number of hits during his career\n- CHmRun: Number of home runs during his career\n- CRuns: Number of runs during his career\n- CRBI: Number of runs batted in during his career\n- CWalks: Number of walks during his career\n- League: A factor with levels A and N indicating player's league at the end of 1986\n- Division: A factor with levels E and W indicating player's division at the end of 1986\n- PutOuts: Number of put outs in 1986-1987 season\n- Assists: Number of assists in 1986-1987 season\n- Errors: Number of errors in 1986-1987 season\n- Salary: 1996-1987 annual salary on opening day in thousands of dollars\n- NewLeague: A factor with levels A and N indicating player's league at the beginning of 1987","ab438246":"<a id = \"6\"><\/a><br>\n#### **6. ENSEMBLE LEARNING**","910668d7":"#### There are rare values under New_Years_Cat (2 and 3). Dropping these values can be considered. I will continue with the existing dataframe for now. ","cf672888":"<a id = \"4\"><\/a><br>\n#### **4. MODEL BASED FEATURE SELECTION**","c40936e0":"<a id = \"8\"><\/a><br>\n#### **8. REFERENCES**","9222ff2e":"- Data Science and Machine Learning Bootcamp, 2021, https:\/\/www.veribilimiokulu.com\/\n- https:\/\/www.kaggle.com\/mathchi\/four-different-models-for-hitters\n- https:\/\/www.mlb.com\/glossary","bc501310":"<a id = \"2\"><\/a><br>\n#### **2. DATA PRE-PROCESSING & FEATURE ENGINEERING**","28dfbbcc":"<a id = \"5\"><\/a><br>\n#### **5. HYPERPARAMETER OPTIMISATION WITH RandomizedSearchCV**","c6aace17":"<a id = \"1\"><\/a><br>\n#### **1. EXPLORATORY DATA ANALYSIS**","9dc73747":"## **END TO END MACHINE LEARNING PROJECT ON HITTERS DATASET**\n\n#### **Business Problem:** Can a machine learning project be implemented to estimate the salaries of baseball players whose salary information and career statistics for 1986 are shared?\n\n<font color = 'blue'>\nContent: \n\n1. [Exploratory Data Analysis](#1)\n1. [Data Preprocessing & Feature Engineering](#2)\n1. [Modelling & Hyperparameter Optimization](#3)\n1. [Model Based Feature Selection](#4)\n1. [Hyperparameter Optimization with RandomizedSearchCV](#5)\n1. [Ensemble Learning](#6)\n1. [Prediction](#7)\n1. [References](#8)","0ba5a8c7":"##### **NOTE:** I will continue with the dataset including outliers.","d59a7847":"<a id = \"7\"><\/a><br>\n#### **7. PREDICTION** ","50201a93":"<a id = \"3\"><\/a><br>\n#### **3. MODELING & HYPERPARAMETER OPTIMISATION**"}}