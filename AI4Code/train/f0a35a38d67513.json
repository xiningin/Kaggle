{"cell_type":{"08e421e8":"code","cace25f8":"code","227b8a26":"code","0d7711d4":"code","88b8af67":"code","5f627a70":"code","49a286ab":"code","99cb230b":"code","fdefc9b7":"code","92402531":"code","28ef587c":"code","30448f59":"code","3011b8c4":"code","954175a6":"markdown","e03270bc":"markdown","f497da23":"markdown","25183af3":"markdown","f1479405":"markdown","f78e9be0":"markdown","14713947":"markdown","6b783eb9":"markdown","26dfab69":"markdown","5aaae1e0":"markdown","36f918f9":"markdown","64b05eb2":"markdown","7d0f5cbe":"markdown"},"source":{"08e421e8":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","cace25f8":"#we load the dataset\ndf = pd.read_csv('\/kaggle\/input\/pima-indians-diabetes-database\/diabetes.csv')","227b8a26":"df.shape","0d7711d4":"df.columns","88b8af67":"df.describe()","5f627a70":"df.isna().sum()","49a286ab":"X = df.drop(['Outcome'], axis = 1)\ny = df.Outcome","99cb230b":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 1)","fdefc9b7":"from sklearn.metrics import accuracy_score, log_loss\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.linear_model import LogisticRegression\n\nclassifiers = [\n    SVC(),\n    DecisionTreeClassifier(),\n    RandomForestClassifier(),\n    GradientBoostingClassifier(),\n    KNeighborsClassifier(),\n    LogisticRegression()\n    ]\n\n\nfor clf in classifiers:\n    clf.fit(X_train, y_train)\n    name = clf.__class__.__name__\n    \n    print(\"=\"*30)\n    print(name)\n    \n    print('****Results****')\n    train_predictions = clf.predict(X_test)\n    acc = accuracy_score(y_test, train_predictions)\n    print(\"Accuracy: {:.4%}\".format(acc))\n    \nprint(\"=\"*30)","92402531":"from sklearn.dummy import DummyClassifier","28ef587c":"dc_strat = DummyClassifier(strategy = \"stratified\")\ndc_strat.fit(X_train, y_train)\n\ndc_freq = DummyClassifier(strategy = \"most_frequent\")\ndc_freq.fit(X_train, y_train)\n\nprint(\"DC Stratified accuracy : \", dc_strat.score(X_test, y_test))\nprint(\"DC Most Frequent accuracy : \", dc_freq.score(X_test, y_test))","30448f59":"from sklearn.model_selection import GridSearchCV","3011b8c4":"# Set the parameters by cross-validation\nparameters = {'kernel': ['rbf'],  'C': [1, 10, 100, 1000]}\n\n\nclf = GridSearchCV(SVC(), parameters)\nclf.fit(X_train, y_train)\n\nprint(\"Best parameters set found on development set:\")\nprint()\nprint(clf.best_params_)\nprint()\nprint(\"Best score:\")\nprint()\nprint(clf.best_score_)","954175a6":"## Probl\u00e8me score GridSearchCV\n\n<span style=\"color:red\"> La pr\u00e9cision du mod\u00e8le avec hyperparam\u00e8tres optimis\u00e9s par GridSearch CV est moins bonne que la pr\u00e9cision du mod\u00e8le \u00e0 vide... cf question pos\u00e9e en cours, je ne vois pas d'o\u00f9 peut venir le probl\u00e8me. Dans la documentation du SVC, la valeur par d\u00e9faut de C est 1, et la valeur de kernel est 'rbf', que j'ai bien inclus dans la liste des hyperparam\u00e8tres \u00e0 tester.","e03270bc":"### Baselines\n\nTo conclude on the accuracy of our models, we need to compare them with different baselines.\n\nThese baselines will be given by the Dummy Classifier.","f497da23":"## Pima Indians Diabetes\n\nOur objective is to predict whether a patient is suffering or not form diabetes. \n\nOur input values include \"number of pregnancies the patient has had, their BMI, insulin level, age\" and others.\nOur ouput value will be a boolean, i.e 1 if he has diabetes and 0 if not.","25183af3":"### Loading of packages and dataset path","f1479405":"### N\/A Values","f78e9be0":"### Train\/Test Split","14713947":"We have 768 rows in our dataset with 9 columns (8 predictor variables and 1 outcome variable)","6b783eb9":"## Machine Learning","26dfab69":"### Testing different models","5aaae1e0":"We don't have any N\/A value","36f918f9":"## Data Pre-Processing","64b05eb2":"### X\/Y definition","7d0f5cbe":"### Parameters tuning : SVC\n\nWe want to improve the accuracy we have with our classifiers. To do so, we can tune their parameters.\n\nLet's pick our SVC model. We will tune its paramters thanks to the Cross-Validation tool of Grid Search."}}