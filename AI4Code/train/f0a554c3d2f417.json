{"cell_type":{"97dfa94f":"code","fdd85a12":"code","733b76d4":"code","13840a27":"code","e113cc7a":"code","6b8093fa":"code","3e1dc48b":"code","4d3483a0":"code","a327c878":"code","aa3b5a5a":"code","932e8374":"code","d7ff83a4":"code","3825d3f0":"code","af7eb9b8":"code","fbe9ddc7":"code","a9044550":"code","260c9c6d":"code","79736165":"markdown","32c15a52":"markdown","4209b813":"markdown","5715c181":"markdown","22dd317b":"markdown","4054104c":"markdown","62cd16e0":"markdown","430018d6":"markdown","66f5bd5f":"markdown","95532b95":"markdown","17784e5e":"markdown","6b044c43":"markdown","63b3d9a5":"markdown","3374930d":"markdown","b4a572f9":"markdown"},"source":{"97dfa94f":"import numpy as np \nimport pandas as pd \nfrom sklearn import preprocessing\n\nimport matplotlib.pyplot as plt\nfrom sklearn import tree\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import GridSearchCV, cross_val_score\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn import preprocessing\nfrom sklearn.tree import DecisionTreeRegressor\n\nfrom sklearn.metrics import classification_report\n\nfrom sklearn.ensemble import RandomForestClassifier\n\n\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n","fdd85a12":"db = pd.read_csv('\/kaggle\/input\/glass\/glass.csv')\ndb.head()","733b76d4":"db['Type'].value_counts()","13840a27":"db.info()","e113cc7a":"elements = []\n\nfor col in db.columns[0:8]:\n    \n    elements.append(col)\n    \n    \nx  = db['Type']\nfor elem in elements:\n    \n    y = db[elem]\n    print(plt.bar(x, y, align='center', alpha=0.5))\n    plt.title(elem)\n\n    plt.show()","6b8093fa":"import matplotlib.pyplot as plt\n%matplotlib inline\ndb.hist(bins=50,figsize=(20,15))\nplt.show()","3e1dc48b":"\nfrom sklearn.model_selection import train_test_split as tts\nx_train = db.iloc[:,0:8]\ny_train = db['Type']\nx_train,x_test, y_train, y_test = tts(x_train,y_train, test_size = 0.2, random_state = 18,stratify=db[\"Type\"])\n","4d3483a0":"from sklearn.linear_model import LogisticRegression\ny_train = np.array(y_train)\n\nlog_reg = LogisticRegression(C = 1, max_iter = 50)\nlog_reg.fit(x_train,y_train.ravel())\n","a327c878":"y_predict = log_reg.predict(x_test)\n\nfrom sklearn.metrics import confusion_matrix\nconfusion_matrix(y_test, y_predict)\n\nfrom sklearn.metrics import classification_report\nclassification_report(y_test, y_predict)\ntarget_names = ['1', '2','3','5','6','7']\nprint(classification_report(y_test, y_predict, target_names=target_names))","aa3b5a5a":"sc = preprocessing.StandardScaler()\ndtreeClf = tree.DecisionTreeClassifier()\n\n\npipe = Pipeline(steps=[('sc', sc),('dtreeClf', dtreeClf)])\n\ncriterion = ['gini', 'entropy']\nmax_depth = [4,5,6,7,8,10]\n\n\nparameters = dict(dtreeClf__criterion=criterion, dtreeClf__max_depth=max_depth)\nclf = GridSearchCV(pipe, parameters)\nbest = clf.fit(x_train, y_train)\n","932e8374":"y_predict = clf.predict(x_test)","d7ff83a4":"from sklearn.metrics import classification_report #67 accuracy w. grid search\nclassification_report(y_test, y_predict)\ntarget_names = ['1', '2','3','5','6','7']\nprint(classification_report(y_test, y_predict, target_names=target_names)) ","3825d3f0":"print(clf.best_params_)","af7eb9b8":"sc = preprocessing.StandardScaler()\n\nrandomforestClf = RandomForestClassifier()\n\n\npipe = Pipeline(steps=[('sc', sc),('randomforestClf', randomforestClf)])\n\nn_estimators = [500, 600, 550, 300, 200,100]\ncriterion = ['gini', 'entropy']\nmax_depth = [5, None]\nmin_samples_split = [0.005, 0.01]\nmax_features = [0.05 , 0.1]\n\n#parameters = dict(pca__n_components=n_components,dtreeClf__criterion=criterion, dtreeClf__max_depth=max_depth)\nparameters = dict(randomforestClf__criterion=criterion, randomforestClf__max_depth=max_depth,\n                 randomforestClf__n_estimators = n_estimators,randomforestClf__min_samples_split =min_samples_split,\n                 randomforestClf__max_features = max_features)\nclf = GridSearchCV(pipe, parameters)\nbest = clf.fit(x_train, y_train)\n","fbe9ddc7":"y_predict = clf.predict(x_test)","a9044550":"from sklearn.metrics import classification_report \nclassification_report(y_test, y_predict)\ntarget_names = ['1', '2','3','5','6','7']\nprint(classification_report(y_test, y_predict, target_names=target_names)) ","260c9c6d":"print(clf.best_params_)","79736165":"So the idea is to try and determine the type by using all these components; let's see how many types there are:","32c15a52":"So the accuracy is very low, at 0.6, so I will continue testing the Decision Trees algorithm to see what I'll get.\nFor this, I'l be creating a pipeline that will allow to find faster which parameters are the best,I'm playing on the max_depth and the criterion parameters.","4209b813":"In this notebook I made a clasification of the types of glass depending on its components by using a Logistic Regression; however, as the prediction results were not amazing on the test sample, I continued to make the classification by using Decision Trees and Random Forests, which gave, unsurprisingly, better results.  ","5715c181":"Now on to check the performance of the Logistic Regression on the test sample by using the classification report:","22dd317b":"# 3. Random Forests","4054104c":"So now the accuracy is better, at 0.65, but not amazing, let's see what parameters it chose:","62cd16e0":"So at 0.77 accuracy it's the best result until now, based on this it's recommended to use Random Forests for this database.","430018d6":"Now let's the how the Random Forest performed:","66f5bd5f":"# 1. Logistic Regression","95532b95":"Checking also the variables' distribution:","17784e5e":"So there are six types but very unequal distribution in each one, so it's important to make the train and test databases split have the same structure as the original one; also, let's have a closer look at all the elements:","6b044c43":"First steps, importing the packages that will be used and the database:","63b3d9a5":"Last but not least, let's test also Random Forests:","3374930d":"The data distribution looks very far from normal for most of the variables, but after some research it seems it's preferred not to normalize it, so I will leave it as it is and go on to the train - test split and running the regression:","b4a572f9":"# 2. Decision Trees"}}