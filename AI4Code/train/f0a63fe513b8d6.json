{"cell_type":{"5e2afc4a":"code","20c16516":"code","acbe9e36":"code","3c8a4443":"code","ac689940":"code","594ce519":"code","9f152554":"code","7d8f899a":"code","19cc4852":"code","872f4ff0":"code","7df2ecd2":"code","3f7aaf43":"code","19e3d6a9":"code","e31f7e40":"code","420b941e":"code","91b5e074":"code","61000594":"code","c6abb42a":"code","607b4b46":"code","6a12af44":"code","0adbb555":"code","e806dddf":"code","688df0d8":"code","cafb2d55":"code","4e4b2b3f":"code","6b82385e":"code","d3303a4d":"code","64bfb6ec":"code","c5db3b5c":"code","02edc027":"code","f02ef648":"code","cfc9406c":"code","3819f450":"code","4e349355":"code","88826194":"code","ea3ba002":"code","aa155fc8":"code","225f3601":"code","cb6936b9":"code","f4c9ea6a":"markdown","0e267001":"markdown","7e2c2f78":"markdown"},"source":{"5e2afc4a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","20c16516":"import pandas as pd\nimport numpy as np\nimport holoviews as hv\nfrom pprint import pprint\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport holoviews as hv\nhv.extension('bokeh', 'matplotlib', logo=False)\nimport matplotlib.pyplot as plt\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import classification_report\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.utils import resample\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.ensemble import ExtraTreesClassifier, RandomForestClassifier\nfrom sklearn.metrics import accuracy_score, confusion_matrix, f1_score\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom xgboost import XGBClassifier\nimport  warnings\nwarnings.filterwarnings(\"ignore\")\n\n\n","acbe9e36":"#Carregando os dados\ndf = pd.read_csv('\/kaggle\/input\/hmeq-data\/hmeq.csv')\ndf.head()","3c8a4443":"# Verificando os tipos de dados\ndf.dtypes","ac689940":"#Verificando a quantidade de linhas e colunas\ndf.shape","594ce519":"#An\u00e1lisando a base\ndf.info()","9f152554":"# Mostrando as colunas \/\/ ACHO QUE N\u00c3O PRECISA COLOCA\ndf.columns","7d8f899a":"# Estatistica Descritiva. Vamos incluir TUDO....\ndf.describe(include='all')","19cc4852":"numeric_feats = [c for c in df.columns if df[c].dtype != 'object' and c not in ['BAD']]\ndf_numeric_feats = df[numeric_feats]","872f4ff0":"# avaliando as vari\u00e1veis num\u00e9ricas\nsns.pairplot(df_numeric_feats)","7df2ecd2":"df_numeric_feats.hist(figsize=(20,8), bins=30)","3f7aaf43":"# Analise Explorat\u00f3ria\ndf[\"BAD\"].value_counts().plot.bar(title='BAD')","19e3d6a9":"#Visualizando a vari\u00e1vel categorica REASON\nREASON_count= df[\"REASON\"].value_counts().rename_axis('REASON').reset_index(name='Total Count')\ndf[\"REASON\"].value_counts().plot.bar(title='REASON')","e31f7e40":"#visualizando  a vari\u00e1vel categ\u00f3rica JOB\nJOB_count= df[\"JOB\"].value_counts().rename_axis('JOB').reset_index(name='Total Count')\ndf[\"JOB\"].value_counts().plot.bar(title='JOB')","420b941e":"#Rela\u00e7\u00e3o JOB vs BAD\nJOB=pd.crosstab(df['JOB'],df['BAD'])\nJOB.div(JOB.sum(1).astype(float), axis=0).plot(kind=\"bar\", stacked=True, title='JOB vs BAD', figsize=(4,4))","91b5e074":"#df = t.copy()","61000594":"# Tratando colunas categ\u00f3ricas\n#for col in df.select_dtypes(include='object').columns:\n#    if df[col].isna().sum() > 0:\n#         df[col].fillna(df[col].mode()[0], inplace=True)   ","c6abb42a":"# Tratando colunas num\u00e9ricas\n#for col in df.select_dtypes(exclude='object').columns:\n#    if df[col].isna().sum() > 0:\n#        df[col].fillna(-1, inplace=True)      \n#df.info()","607b4b46":"def showBalance(df, col):\n    for c in col:\n        print('Distribui\u00e7\u00e3o da Coluna: ', c,'\\n',df[c].value_counts(normalize=True),'\\n')\n    else:\n       pass\n        \nshowBalance(df, col=['REASON','JOB','BAD'])","6a12af44":"\n# Finalmente, o n\u00famero de linha de cr\u00e9dito aberta (CLNO) parece estatisticamente consistente em ambos os casos,\n# sugerindo que essa vari\u00e1vel n\u00e3o possui poder de discrimina\u00e7\u00e3o significativo.\n# Este m\u00e9todo \u00e9 primariamente baseado nas labels da colunas, por\u00e9m podemos utilizar com um array booleano tamb\u00e9m. (Usando o loc)\n# Uma informa\u00e7\u00e3o importante sobre loc \u00e9: quando nenhum item \u00e9 encontrado ele retorna um KeyError.\ndf.loc[df.BAD == 1, 'STATUS'] = 'DEFAULT'\ndf.loc[df.BAD == 0, 'STATUS'] = 'PAID'","0adbb555":"# rela\u00e7\u00e3o do empr\u00e9stimo pagos. Pelo que mostra 81% dos empr\u00e9stimos foram pagos.\n#A discrep\u00e2ncia de 4% observada n\u00e3o \u00e9 estatisticamente significativa, dado o montante de empr\u00e9stimos no conjunto de dados.\ng = df.groupby('REASON')\ng['STATUS'].value_counts(normalize=True).to_frame().style.format(\"{:.1%}\")","e806dddf":"# Matrix de correla\u00e7\u00e3o\n\ncorr = df.corr()\n#Plot figsize\nfig, ax = plt.subplots(figsize=(10,8))\n#Generate Color Map\ncolormap = sns.diverging_palette(220, 10, as_cmap=True)\n#Generate Heat Map, allow annotations and place floats in map\nsns.heatmap(corr, cmap=colormap, annot=True, fmt=\".2f\")\n#Apply xticks\nplt.xticks(range(len(corr.columns)), corr.columns);\n#Apply yticks\nplt.yticks(range(len(corr.columns)), corr.columns)\n#show plot\nplt.show()","688df0d8":"#Verificando os missing \ndf.isnull().sum()","cafb2d55":"# Criando uma c\u00f3pia de um novo dataframe e vamos eliminar os NA, \ndf2 = df.copy()\ndf2.dropna(axis=0,how='any',inplace= True)\ndf2.info(), df2.isna().any() \ndf2.shape","4e4b2b3f":"# Analisando como ficou\ndf2.head()","6b82385e":"# tranformando as colunas de object em categoria com codigos #\nfor col in df2.columns:\n    if df2[col].dtype == 'object':\n        df2[col]= df2[col].astype('category').cat.codes\ndf2.info()","d3303a4d":"# analisando REASON (MOTIVO)\ndf2['REASON'].value_counts()","64bfb6ec":"#  Separando em Treino e Teste\n\ntreino, teste = train_test_split(df2, random_state=42)\n\n#  Separando o treino e validacao, para refinar o modelo\n\n#treino, validacao = train_test_split(treino, random_state=42)\n\ntreino.shape, teste.shape, #validacao.shape","c5db3b5c":"# separar as colunas para usar no treino\n\nusadas_treino = [c for c in treino.columns if c not in ['BAD','REASON','JOB','STATUS']]","02edc027":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import f1_score\n\nrf = RandomForestClassifier(n_estimators=900, random_state=42)\n\nrf.fit(treino[usadas_treino],treino['BAD'])\n#Gerando as predi\u00e7\u00f5es do modelo\nrf_pred = rf.predict(teste[usadas_treino])\n\naccuracy_score(teste['BAD'], rf_pred), f1_score(teste['BAD'],rf_pred)","f02ef648":"#Olhando os valores da SITUACAO - TREINO\n\ntreino['BAD'].value_counts(normalize=True)","cfc9406c":"# Trabalhando com RandomForest\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\n\nrf2 = RandomForestClassifier(n_estimators=900, min_samples_split=5, max_depth=4, random_state=42)\nrf2.fit(treino[usadas_treino],treino['BAD'])\n\n#Gerando as predi\u00e7\u00f5es do modelo\nrf_pred2 = rf.predict(teste[usadas_treino])\n\naccuracy_score(teste['BAD'], rf_pred2), f1_score(teste['BAD'],rf_pred2)","3819f450":"# Trabalhando com GBM\nfrom sklearn.ensemble import GradientBoostingClassifier\ngbm = GradientBoostingClassifier(n_estimators=200, learning_rate=1.0, max_depth=1, random_state=42)\ngbm.fit(treino[usadas_treino], treino['BAD'])\n\naccuracy_score(validacao['BAD'], gbm.predict(validacao[usadas_treino]))","4e349355":"# Trabalhando com XGBoost\nfrom xgboost import XGBClassifier\nxgb = XGBClassifier(n_estimators=200, learning_rate=0.09, random_state=42)\nxgb.fit(treino[usadas_treino], treino['BAD'])\n\n#Gerando as predi\u00e7\u00f5es do modelo\naccuracy_score(teste['BAD'], xgb.predict(teste[usadas_treino]))","88826194":"# Verificando e avaliando a importancia de cada coluna para o modelo RF\n\npd.Series(rf.feature_importances_, index=usadas_treino).sort_values().plot.barh()","ea3ba002":"# O modelo GBM em cada coluna...\n\npd.Series(gbm.feature_importances_, index=usadas_treino).sort_values().plot.barh()\n","aa155fc8":"# importando a bilbioteca para plotar o gr\u00e1fico de Matriz de Confus\u00e3o\nimport scikitplot as skplt\n\n# Matriz de Confus\u00e3o - Dados de Valida\u00e7\u00e3o\nskplt.metrics.plot_confusion_matrix(teste['BAD'], rf_pred)","225f3601":"#Verificando o desbalanceio da vari\u00e1vel dependente\ndf['BAD'].value_counts()","cb6936b9":"# Dividindo o DataFrame\nfrom sklearn.model_selection import train_test_split\n\n# Treino e teste\ntreino, test = train_test_split(df, test_size=0.15, random_state=42)\n\n# Veificando o tanho dos DataFrames\ntreino.shape, test.shape","f4c9ea6a":"Amostra de cada vari\u00e1vel.","0e267001":"**** Dicion\u00e1rio de Dados\n# * O dicion\u00e1rio de dados das colunas dispon\u00edveis abaixo:*****\n\nBAD: 1 = client defaulted on loan 0 = loan repaid\n\nLOAN: Amount of the loan request\n\nMORTDUE: Amount due on existing mortgage\n\nVALUE: Value of current property\n\nREASON: DebtCon = debt consolidation ; HomeImp = home improvement\n\nJOB: Six occupational categories\n\nYOJ: Years at present job\n\nDEROG: Number of major derogatory reports\n\nDELINQ: Number of delinquent credit lines\n\nCLAGE: Age of oldest trade line in months\n\nNINQ: Number of recent credit lines\n\nCLNO: Number of credit lines****","7e2c2f78":"VAMOS TREINAR E ANALISAR O MODELO"}}