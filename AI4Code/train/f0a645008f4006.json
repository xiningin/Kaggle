{"cell_type":{"f865385d":"code","993d2813":"code","827401e4":"code","6419dbcb":"code","9966abc4":"code","16cddaa1":"code","a2384e00":"code","823b7522":"code","e0c3ba21":"code","93ae8308":"code","cf27e8c8":"code","7e37e4cb":"code","abe7201a":"code","514141a0":"code","9eba6119":"code","f960865f":"code","3e4554fb":"code","9fad11d7":"code","f0f2592e":"code","81c119c1":"code","48b2081f":"code","cb360341":"code","c6dff6f0":"code","5d3a964f":"code","199f6b04":"code","5688d8b2":"code","1a409039":"code","18b83292":"code","293498aa":"code","6e90e272":"code","3d44fc08":"code","fd658f28":"code","ff90ceae":"code","228bc69a":"code","fad4b884":"code","5ad7f607":"code","f55d27a7":"code","cd6f12f4":"code","c412dc1d":"code","b28e3ee1":"code","3ad40b79":"code","0d21ac71":"code","f9eadec1":"code","a761bf3a":"code","863c5046":"code","4abe4ab3":"code","4955a029":"code","1ed2ff42":"code","0fd18b8c":"markdown","c8d255a5":"markdown","2adb80e5":"markdown","c6d168b9":"markdown","699b2c52":"markdown","45ec71cd":"markdown","36cc3268":"markdown","fe9cec6b":"markdown","6b958880":"markdown","0cd59534":"markdown","421962a5":"markdown","49c30546":"markdown","f1b37893":"markdown","958fb5d2":"markdown","cc21a441":"markdown","caa33674":"markdown","03a884a6":"markdown","661d80a1":"markdown","6f3b828c":"markdown","418f1d0e":"markdown","f7444307":"markdown","f540a499":"markdown","4ac860c2":"markdown","da744972":"markdown","135f379a":"markdown","45c16fcb":"markdown","fea43e18":"markdown","5ef34385":"markdown","55a814a3":"markdown","6e8cf715":"markdown","006b81b2":"markdown","9a8456c6":"markdown","04f0a4e9":"markdown"},"source":{"f865385d":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\n\npd.options.display.latex.repr=True","993d2813":"store = pd.HDFStore('..\/input\/instacart-feature-engineering\/io.h5', 'r')\nstore.open()\nstore.keys()","827401e4":"dsets = ['train', 'test', 'kaggle']\n\nX = dict.fromkeys(dsets)\ny = dict.fromkeys(dsets)\n\nfor ds in dsets:\n    X[ds] = store['\/X\/' + str(ds)]\n    y[ds] = store['\/y\/' + str(ds)]","6419dbcb":"store.close()\nstore.is_open","9966abc4":"# Import necessary modules\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\nimport datetime\nfrom scipy.stats import randint\n\nfrom sklearn import metrics\nfrom sklearn.ensemble import RandomForestClassifier","16cddaa1":"# Initialize and fit classifier\nrfc = RandomForestClassifier(n_estimators=600,\n                             max_features='sqrt',\n                             min_impurity_decrease=3e-7,\n                             min_samples_leaf=24,\n                             n_jobs=-1,\n                             random_state=20190603,\n                             oob_score=True,\n                             warm_start=True)\n\nprint(datetime.datetime.now())\n\nrfc.fit(X['train'], y['train'].values.ravel())\n\nprint(datetime.datetime.now())\n\noob_error = 1 - rfc.oob_score_","a2384e00":"# tree_.node_count estimators\n\nnode_counts = [rfc.estimators_[i].tree_.node_count for i in range(len(rfc.estimators_))]\n\nplt.hist(node_counts)\nplt.show()","823b7522":"plt.figure(figsize=(10,12))\nsns.barplot(data=(pd.DataFrame(data=rfc.feature_importances_,\n           index=X['train'].columns)\n                  .reset_index()\n                  .sort_values(by=0,ascending=False)\n                  .rename(columns = {'index': 'feature',\n                                     0: 'feature_importance'}\n                         )\n                 ),\n            x='feature_importance',\n            y='feature'\n           )\nplt.title('RandomForestClassifier feature_importances_')\nplt.show()","e0c3ba21":"# Create a dictionary of metrics to compute multiple scores\n\nfrom imblearn.metrics import geometric_mean_score\n\nmetrics_dict = {}\n\nmetrics_dict['auc_roc'] = {'fcn' : metrics.roc_auc_score,\n                        'name': 'AUC-ROC',\n                        'thr' : False}\n\nmetrics_dict['auc_pr'] = {'fcn' : metrics.average_precision_score,\n                        'name': 'AUC-PR',\n                        'thr' : False}\n\nmetrics_dict['log_loss'] = {'fcn' : metrics.log_loss,\n                        'name': 'Log Loss',\n                        'thr' : False}\n\nmetrics_dict['prec'] = {'fcn' : metrics.precision_score,\n                        'name': 'Precision',\n                        'thr' : True}\n\nmetrics_dict['rec'] = {'fcn' : metrics.recall_score,\n                        'name': 'Recall',\n                        'thr' : True}\n\nmetrics_dict['f1'] = {'fcn' : metrics.f1_score,\n                        'name': 'F1 Score',\n                        'thr' : True}\n\nmetrics_dict['bal_acc'] = {'fcn' : metrics.balanced_accuracy_score,\n                        'name': 'Balanced Accuracy',\n                        'thr' : True}\n\nmetrics_dict['g_mean'] = {'fcn' : geometric_mean_score,\n                        'name': 'Geometric Mean',\n                        'thr' : True}\n\nmetrics_dict['kappa'] = {'fcn' : metrics.cohen_kappa_score,\n                        'name': 'Cohen\\'s Kappa',\n                        'thr' : True}","93ae8308":"# oob scores\ny_score = rfc.oob_decision_function_\n\n# predictions\ny_predict_binary = dict.fromkeys(dsets)\ny_predict_proba = dict.fromkeys(dsets)\ny_predict_proba_df = dict.fromkeys(dsets)\n\nfor ds in dsets:\n    # binary predictions\n    y_predict_binary[ds] = rfc.predict(X[ds])\n\n    # probability of True\n    y_predict_proba[ds] = rfc.predict_proba(X[ds])[:, 1]\n\n    # True probabilities as Series\n    y_predict_proba_df[ds] = pd.Series(data=y_predict_proba[ds],\n                                         index=X[ds].index)","cf27e8c8":"fpr = dict.fromkeys(dsets)\ntpr = dict.fromkeys(dsets)\nroc_auc = dict.fromkeys(dsets)\n\nfor ds in dsets[:2]:\n    fpr[ds], tpr[ds], _ = metrics.roc_curve(y[ds], y_predict_proba[ds])\n    roc_auc[ds] = metrics.roc_auc_score(y[ds], y_predict_proba[ds])\n\nplt.figure(figsize=(14,10))\nlw = 2\nplt.plot(fpr['train'], tpr['train'], color='blue',\n         lw=lw, label='train (AUC = %0.2f)' % roc_auc['train'])\nplt.plot(fpr['test'], tpr['test'], color='darkorange',\n         lw=lw, label='test (AUC = %0.2f)' % roc_auc['test'])\nplt.plot([0, 1], [0, 1], color='black', lw=lw, linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic')\nplt.legend(loc=\"lower right\")\nplt.show()","7e37e4cb":"# adapted from sklearn docs\nfrom sklearn.utils.fixes import signature\n\nprecision, recall, _ = metrics.precision_recall_curve(y['test'], y_predict_proba['test'])\n\naverage_precision = metrics.average_precision_score(y['test'], y_predict_proba['test'])\n\n# # Iso-F1\n# for i in np.linspace(0.2, 0.9, 7, endpoint=False):\n#     plt.plot(xs, xs * i \/ (2 * xs - i), color='navy')\n\nplt.figure(figsize=(14,10))\n\n# In matplotlib < 1.5, plt.fill_between does not have a 'step' argument\nstep_kwargs = ({'step': 'post'}\n               if 'step' in signature(plt.fill_between).parameters\n               else {})\nplt.step(recall, precision, color='b', alpha=0.2,\n         where='post')\nplt.fill_between(recall, precision, alpha=0.2, color='r', **step_kwargs)\n\n\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.ylim([0.0, 1.0])\nplt.xlim([0.0, 1.0])\nplt.title('2-class Precision-Recall curve: AUC-PR={0:0.2f}'.format(\n          average_precision))\nplt.show()","abe7201a":"skew_train = ((y['train'].count() - y['train'].sum())\/y['train'].sum())\n\nprint('The skew of the training data set is skew_train = %.2f.' % skew_train)","514141a0":"variant_group = 'N_threshold'","9eba6119":"# Compute N\n\nN_05 = dict.fromkeys(dsets)\nN_skew = dict.fromkeys(dsets)\nN_basket = dict.fromkeys(dsets)\nN_basket_reorder = dict.fromkeys(dsets)\n\nfor ds in dsets:\n    N_05[ds] = y_predict_binary[ds].sum()\n    \n    N_skew[ds] = int(len(y_predict_proba_df[ds]) \/ (1 + skew_train))    \n    \n    N_basket[ds] = int(X[ds].U_order_size_mean\n                        .groupby('user_id')\n                        .max()\n                        .astype('float64')\n                        .sum())\n    \n    N_basket_reorder[ds] = int(X[ds].U_reorder_size_mean\n                        .groupby('user_id')\n                        .max()\n                        .astype('float64')\n                        .sum())","f960865f":"# Compute binary prediction vectors\n\ny_topN_05 = dict.fromkeys(dsets)\ny_topN_skew = dict.fromkeys(dsets)\ny_topN_basket = dict.fromkeys(dsets)\ny_topN_basket_reorder = dict.fromkeys(dsets)\n\nfor ds in dsets:\n    y_topN_05[ds] = pd.Series(data=y_predict_binary[ds],\n                              index=X[ds].index)\n    \n    y_topN_skew[ds] = pd.Series(data=True,\n                         index=y_predict_proba_df[ds]\n                         .nlargest(N_skew[ds])\n                         .index).reindex_like(X[ds]).fillna(False)  \n        \n    y_topN_basket[ds] = pd.Series(data=True,\n                         index=y_predict_proba_df[ds]\n                         .nlargest(N_basket[ds])\n                         .index).reindex_like(X[ds]).fillna(False)\n\n    y_topN_basket_reorder[ds] = pd.Series(data=True,\n                         index=y_predict_proba_df[ds]\n                         .nlargest(N_basket_reorder[ds])\n                         .index).reindex_like(X[ds]).fillna(False)","3e4554fb":"# proba thresholds (test)\n\np_05 = dict.fromkeys(dsets)\np_skew = dict.fromkeys(dsets)\np_basket = dict.fromkeys(dsets)\np_basket_reorder = dict.fromkeys(dsets)\n\nfor ds in dsets:\n    p_05[ds] = 0.5\n    p_skew[ds] = float(y_predict_proba_df[ds]\n                       .nlargest(N_skew[ds])\n                       .tail(1))\n    p_basket[ds] = float(y_predict_proba_df[ds]\n                         .nlargest(N_basket[ds])\n                         .tail(1))\n    p_basket_reorder[ds] = float(y_predict_proba_df[ds]\n                                 .nlargest(N_basket_reorder[ds])\n                                 .tail(1))    ","9fad11d7":"# Create a dictionary of the predictions of various variants\n\nfrom collections import defaultdict\n\nvariants_dict = defaultdict(dict)\n\nvariants_dict[variant_group]['N_05'] = {\n    'N' : N_05,\n    'y' : y_topN_05,\n    'p' : p_05\n}\n\nvariants_dict[variant_group]['N_skew'] = {\n    'N' : N_skew,\n    'y' : y_topN_skew,\n    'p' : p_skew\n}\n\nvariants_dict[variant_group]['N_basket'] = {\n    'N' : N_basket,\n    'y' : y_topN_basket,\n    'p' : p_basket\n}\n\nvariants_dict[variant_group]['N_basket_reorder'] = {\n    'N' : N_basket_reorder,\n    'y' : y_topN_basket_reorder,\n    'p' : p_basket_reorder\n}","f0f2592e":"pd.concat(\n    {\n        ds: pd.DataFrame({\n            variant: {\n                col: variants_dict[variant_group][variant][col][ds]\n                for col in variants_dict[variant_group][variant].keys()\n            }\n            for variant in variants_dict[variant_group].keys()\n        }).transpose()[['N', 'p']].assign(\n            N_frac=lambda x: x['N'] \/ len(y[ds])).sort_index(axis=1)\n        for ds in dsets[:2]\n    },\n    axis=1)","81c119c1":"pd.concat([\n    y_topN_05['train'].groupby('user_id').sum()\n    .astype('int').rename('top_N_05_pred'),\n    y_topN_skew['train'].groupby('user_id').sum()\n    .astype('int').rename('top_N_skew_pred'),\n    y_topN_basket['train'].groupby('user_id').sum()\n    .astype('int').rename('top_N_basket_pred'),\n    y_topN_basket_reorder['train'].groupby('user_id').sum()\n    .astype('int').rename('top_N_basket_reorder_pred'),\n    X['train'].U_order_size_mean.groupby('user_id').max(),\n    X['train'].U_reorder_size_mean.groupby('user_id').max(),\n],\naxis=1).head(15)","48b2081f":"# Make a dataframe with one row of threshold metrics to score y_pred from\n# a dictionary of metrics of the form metrics_dict\n\ndef scores_dataframe(y_true_dict, y_pred_dict, metrics_dict, col_name=0):\n    \n    cols = []\n    scores = []\n\n    for ds in dsets[:2]:\n        for key, metric in metrics_dict.items():\n            if metric['thr'] == True:\n                cols.append((ds, metric['name']))\n                scores.append(metric['fcn'](y_true_dict[ds].values.ravel(),\n                                            y_pred_dict[ds].values.ravel()))\n\n    return pd.DataFrame(data=[scores],\n                        columns=pd.MultiIndex.from_tuples(cols, names=['dset', 'metric']),\n                        index=[col_name])","cb360341":"plt.figure(figsize=(14,8))\nplt.title('Scores for ' + variant_group + ' variants')\nsns.heatmap(\npd.concat(\n    [scores_dataframe(y, val['y'], metrics_dict, key)['test']\n    for key, val in variants_dict[variant_group].items()]),\n    annot=True,\n    fmt='.3f',\n    cmap=\"OrRd\"\n)\nplt.show()","c6dff6f0":"# Confusion matrix in a standard format\n\ndef make_confusion_df(y_test, y_pred):\n    cm = metrics.confusion_matrix(\n        y_test, y_pred\n    )\n\n    return (pd.DataFrame(data=cm)\n            .iloc[::-1,::-1]\n            .rename_axis('Predicted Label')\n            .rename_axis('True Label', axis=1))","5d3a964f":"# Make a normed confusion matrix (recall version)\n# for variant in the keys of variant_group\n# and 'test' set and y_pred_dict of form variants_dict\n\ndef make_norm_confusion_df(y_test, y_pred_dict, variant_group, variant):\n    cm = metrics.confusion_matrix(\n        y_test,\n        y_pred_dict[variant_group][variant]['y']['test']\n    )\n    \n    long_form_matrix_df = (pd.DataFrame(\n        data=(cm \/ (cm.sum(axis=1)[:, np.newaxis])))\n            .iloc[::-1,::-1]\n            .rename_axis('Predicted Label')\n            .rename_axis('True Label', axis=1)\n    .reset_index()\n    .melt(id_vars=['Predicted Label'])\n    .assign(variant=variant))\n    \n    cols = long_form_matrix_df.columns.tolist()\n    cols = cols[-1:] + cols[:-1]\n    long_form_matrix_df = long_form_matrix_df[cols]\n    \n    return long_form_matrix_df","199f6b04":"# Concatenate long-form confusion matrices in single df\n\ndef combine_norm_confusion_df(y_test, y_pred_dict, variant_group):\n    return pd.concat(\n    [make_norm_confusion_df(y_test, y_pred_dict, variant_group, variant)\n    for variant in variants_dict[variant_group].keys()]\n    )","5688d8b2":"# Plot function for FacetGrid use:\n# From long format df (kw-packed), pivot and plot heatmap\n\ndef draw_heatmap(index, columns, values, **kwargs):\n    data = kwargs.pop('data')\n    d = data.pivot(index=index,\n                   columns=columns,\n                   values=values).iloc[::-1,::-1]\n    sns.heatmap(d, **kwargs)","1a409039":"# Create long ('tidy') DataFrame of confusion matrices\n\nconfusion_matrices_long = combine_norm_confusion_df(y['test'], variants_dict, variant_group)","18b83292":"# Plot normalized confusion heatmaps on FacetGrid\n\nfg = sns.FacetGrid(confusion_matrices_long,\n                   col='variant',\n                   col_wrap=2,\n                   height=5)\n\nfg.map_dataframe(draw_heatmap,\n                 index='Predicted Label', \n                 columns='True Label',\n                 values='value',\n                 square=True,\n                 annot=True,\n                 fmt='0.3f',\n                 cmap='Blues')\n\nfg.set_axis_labels('True Label', 'Predicted Label')\n\nplt.show()","293498aa":"variant_group = 'N_u'","6e90e272":"# Compute N_u\n\nN_u = dict.fromkeys(dsets)\nN_u_reorder = dict.fromkeys(dsets)\n\nfor ds in dsets:\n    N_u[ds] = (X[ds].U_order_size_mean\n                        .groupby('user_id')\n                        .max()\n                        .apply(np.ceil)\n                        .astype('uint8'))\n\n    N_u_reorder[ds] = (X[ds].U_reorder_size_mean\n                        .groupby('user_id')\n                        .max()\n                        .apply(np.ceil)\n                        .astype('uint8'))","3d44fc08":"# Compute binary prediction vectors\n\ny_top_N_u = dict.fromkeys(dsets)\ny_top_N_u_reorder = dict.fromkeys(dsets)\n\nfor ds in dsets:\n    y_top_N_u[ds] = (pd.Series(data=True,\n                          index=y_predict_proba_df[ds].groupby('user_id')\n                          .apply(lambda gp: gp.nlargest(N_u[ds].loc[gp.name]))\n                          .reset_index(level=1, drop=True).index)\n                 .reindex_like(X[ds])\n                 .fillna(False))\n    y_top_N_u_reorder[ds] = (pd.Series(data=True,\n                      index=y_predict_proba_df[ds].groupby('user_id')\n                      .apply(lambda gp: gp.nlargest(N_u_reorder[ds].loc[gp.name]))\n                      .reset_index(level=1, drop=True).index)\n             .reindex_like(X[ds])\n             .fillna(False))","fd658f28":"# Note this is not a threshold variant so there are no 'p' keys\n\nvariants_dict[variant_group]['N_u'] = {\n    'N' : N_u,\n    'y' : y_top_N_u\n}\n\nvariants_dict[variant_group]['N_u_reorder'] = {\n    'N' : N_u_reorder,\n    'y' : y_top_N_u_reorder\n}","ff90ceae":"pd.concat([y_top_N_u['train'].groupby('user_id').sum().rename('top_N_u_pred'),\n           y_top_N_u_reorder['train'].groupby('user_id').sum().rename('top_N_reorder_u_pred'),\n           X['train'].U_order_size_mean.groupby('user_id').max(),\n           X['train'].U_reorder_size_mean.groupby('user_id').max()\n          ],\n          axis=1).head(15)","228bc69a":"plt.figure(figsize=(14,4))\nplt.title('Scores for ' + variant_group + ' variants')\nsns.heatmap(\npd.concat(\n    [scores_dataframe(y, val['y'], metrics_dict, key)['test']\n    for key, val in variants_dict[variant_group].items()]),\n    annot=True,\n    fmt='.3f',\n    cmap=\"OrRd\"\n)\nplt.show()","fad4b884":"# Create long ('tidy') DataFrame of confusion matrices\n\nconfusion_matrices_long = combine_norm_confusion_df(y['test'], variants_dict, variant_group)","5ad7f607":"# Plot normalized confusion heatmaps on FacetGrid\n\nfg = sns.FacetGrid(confusion_matrices_long,\n                   col='variant',\n                   col_wrap=2,\n                   height=5)\n\nfg.map_dataframe(draw_heatmap,\n                 index='Predicted Label', \n                 columns='True Label',\n                 values='value',\n                 square=True,\n                 annot=True,\n                 fmt='0.3f',\n                 cmap='Blues')\n\nfg.set_axis_labels('True Label', 'Predicted Label')\n\nplt.show()","f55d27a7":"variant_group = 'top_N'","cd6f12f4":"# Define Ns\n\nNs = list(range(4,25,4))\n\ny_top = []\nfor N in Ns:\n    y_top.append(dict.fromkeys(dsets))","c412dc1d":"# Compute top-N predictions and add to variants_dict\n\nfor N in Ns:\n    for ds in dsets:\n        y_top[Ns.index(N)][ds] = (pd.Series(data=True,\n                          index=y_predict_proba_df[ds].groupby('user_id')\n                          .nlargest(N)\n                          .reset_index(level=1, drop=True).index)\n                 .reindex_like(X[ds])\n                 .fillna(False))\n        \n        variants_dict[variant_group][N] = {\n            'N' : N,\n            'y' : y_top[Ns.index(N)]\n        }","b28e3ee1":"plt.figure(figsize=(14,12))\nplt.title('Scores for ' + variant_group + ' variants')\nsns.heatmap(\npd.concat(\n    [scores_dataframe(y, val['y'], metrics_dict, key)['test']\n    for key, val in variants_dict[variant_group].items()]),\n    annot=True,\n    fmt='.3f',\n    cmap=\"OrRd\"\n)\nplt.show()","3ad40b79":"# Create long ('tidy') DataFrame of confusion matrices\n\nconfusion_matrices_long = combine_norm_confusion_df(y['test'], variants_dict, variant_group)","0d21ac71":"# Plot normalized confusion heatmaps on FacetGrid\n\nfg = sns.FacetGrid(confusion_matrices_long,\n                   col='variant',\n                   col_wrap=3,\n                   height=4)\n\nfg.map_dataframe(draw_heatmap,\n                 index='Predicted Label', \n                 columns='True Label',\n                 values='value',\n                 square=True,\n                 annot=True,\n                 fmt='0.3f',\n                 cmap='Blues')\n\nfg.set_axis_labels('True Label', 'Predicted Label')\n\nplt.show()","f9eadec1":"variant_group = 'p_kaggle'","a761bf3a":"# Compute kaggle threshold predictions and add to variants_dict\n\np_0s = list(np.round(np.linspace(0.192, 0.198, num=4), 3))\n\ny_kaggle = []\n\nfor p_0 in p_0s:\n    for ds in dsets:\n        y_kaggle.append(dict.fromkeys(dsets))\n        y_kaggle[p_0s.index(p_0)][ds] = pd.Series((y_predict_proba_df[ds] >= p_0))\n        \n        variants_dict[variant_group][p_0] = {\n            'y' : y_kaggle[p_0s.index(p_0)]\n        }","863c5046":"plt.figure(figsize=(14,10))\nplt.title('Scores for ' + variant_group + ' variants')\nsns.heatmap(\npd.concat(\n    [scores_dataframe(y, val['y'], metrics_dict, key)['test']\n    for key, val in variants_dict[variant_group].items()])\n    .sort_index(),\n    annot=True,\n    fmt='.3f',\n    cmap=\"OrRd\"\n)\nplt.show()","4abe4ab3":"# Create long ('tidy') DataFrame of confusion matrices\n\nconfusion_matrices_long = combine_norm_confusion_df(y['test'], variants_dict, variant_group)","4955a029":"# Plot normalized confusion heatmaps on FacetGrid\n\nfg = sns.FacetGrid(confusion_matrices_long,\n                   col='variant',\n                   col_wrap=2,\n                   height=5)\n\nfg.map_dataframe(draw_heatmap,\n                 index='Predicted Label', \n                 columns='True Label',\n                 values='value',\n                 square=True,\n                 annot=True,\n                 fmt='0.3f',\n                 cmap='Blues')\n\nfg.set_axis_labels('True Label', 'Predicted Label')\n\nplt.show()","1ed2ff42":"# Output all predictions to .csv\n\nfor var_gp_name, var_gp in variants_dict.items():\n    for var_name, var in var_gp.items():\n        for ds in dsets:\n            pd.DataFrame(data=var['y'][ds],\n                         index=X[ds].index\n                        ).to_csv(str(var_gp_name) + '-' + str(var_name) + '-' + str(ds) + '.csv')","0fd18b8c":"### Scores","c8d255a5":"### Load Data","2adb80e5":"## Top-$N(u)$ Variant\nIf we want to avoid the high variation of basket sizes in the top-$N_\\text{total}$ variant, we can define ultimate basket sizes by user. Let\n$$N(u) = \\left\\lceil \\overline{b(u)} \\right\\rceil$$\n$$N_\\text{reorder}(u) = \\left\\lceil \\overline{r(u)} \\right\\rceil$$\nfor all $u \\in U$, that is, the ceiling of the mean (re)orders per basket for user $u$. Using the ceiling rather than round or floor offers the advantage of recommending at least one item to users in a straight-forward way. The mean reorders per basket is biased even as a predictor of ultimate *re*orders since user's initial orders have zero reorders. On the other hand, since we are predicting fewer positives, the precision of $N_\\text{reorder}(u)$ is better.\n\nThese kinds of variants, or those which offer control of ultimate basket size while offering more flexibility in that control, may be useful for product applications like auto-populating user carts with items we most expect users to reorder. For such an application we would prefer models with higher precision.","c6d168b9":"### Confusion Matrices","699b2c52":"Note the heatmap colors are only comparable *within columns*.\n\nSince $N_\\text{basket}$ is greater than $N_\\text{basket reorder}$, the latter choice has greater precision, while $N_\\text{skew}$ balances precision and recall. We may not necessarily care about high precision scores. Avoiding false negatives is not an issue if we want to be more liberal in recommendations.","45ec71cd":"## Top-$N_\\text{threshold}$ Variants\n\nIf we choose the $N_\\text{threshold}$ user-product pairs $(u,p)$ with the greatest $P((u,p))$, then this top-$N$ variant is a reparamaterization of the classification threshold, $p_0$, via\n$$N_\\text{threshold} =  \\left\\{ (u,p) \\ | \\ P((u,p)) > p_0 \\right\\}.$$\n\nAn advantage of this variant is that it recommends the items users are, in aggregate, most likely to purchase. Therefore, we can, overall, make the 'best' recommendations. A disadvantage is that there is a lot variation in the number of recommended products.\n\nA few potentially principled choices for $N$ follow:","36cc3268":"### $N_\\text{basket}$\n\nA couple other values we may consider to be principled are\n$$ N_\\text{basket} = \\sum_{u \\in U} \\overline{ b(u) }$$\n$$ N_\\text{basket reorder} = \\sum_{u \\in U} \\overline{ r(u) }$$\nwhere $\\overline{b(u)}$ is the mean basket size for user $u$ and $\\overline{ r(u)}$ is the mean reordered items per basket for user $u$.","fe9cec6b":"#### Inspect Basket Sizes ('train'):","6b958880":"The `feature_importances_` attribute shows that one of the most fruitful avenues to pursue to improve classifier performance is to focus on manually creating additional user-product features which capture more complex user-product interactions.","0cd59534":"### Confusion Matrices","421962a5":"#### Inspect $N$ and $p$ values","49c30546":"### Confusion Matrices","f1b37893":"### Scores","958fb5d2":"Of course now there is no difference between mean basket sizes and predictions. Although we have not (yet) created a variant which offers a compromise between the high variation in differences of mean basket size and prediction of `N_threshold` and the (near) zero variation in differences of mean basket size and prediction of `N_u`, there are many such variants which would be straight-forward to construct.","cc21a441":"## Random Forest Classifier\nInitialize and fit the random forest classifier","caa33674":"### Prediction Vectors","03a884a6":"### Scores","661d80a1":"### $N_\\text{skew}$\n\nIt may instead be most principled to choose $N$ such that the skew of predictions equals the `train` skew (negative classes divided by positive classes). Let $y_\\oplus$ be the count of positive classes (`train`) and $y_\\ominus$ be the count of negative classes (`train`). We compute `skew_train` as $\\mathrm{skew} = y_\\ominus \/ y_\\oplus$. For $s \\in \\mathrm{DSets}$, let $n_s = |I_s|$, the length of the multiindex of user-product pairs. Define\n$$N_{\\text{skew}_s} := n_s * P(y_\\oplus) = \\frac{n_s}{1+\\mathrm{skew}},$$\nwhere $$P(y_\\oplus) = \\frac{y_\\oplus}{y_\\oplus + y_\\ominus}$$\nis the ratio of positive classes in `train` to the length of `train`.","6f3b828c":"# Top-$N$ Random Forest Model\n\nIn this notebook we construct a random forest classifier to compute probabilities of ultimate purchase for all previously purchased user-product pairs. Next, we select top probability scores to provide recommendations using top-$N$ variants for different use cases.","418f1d0e":"#### Inspect Basket Sizes\n\nNote this variant group is defined by *setting* basket sizes to user mean basket sizes:","f7444307":"There is a good deal of variation in basket size prediction and sometimes a considerable mismatch between predictions and mean basket sizes. This may be a reason for the threshold variant to be of less practical relevance from a product standpoint.","f540a499":"[SO on normalized confusion](https:\/\/stackoverflow.com\/a\/20934655)","4ac860c2":"## Top-$N$\n\nWhile the top-$N_u$ model gives better predictions, a top-$N$ model with a fixed $N$ for all users may be useful, for example, in displaying previously purchased product recommendations on a web page of fixed size. Below are scores for $N \\in \\left\\{4, 8, 12, 16, 20, 24\\right\\}$. An application like this one would prefer higher recall models.","da744972":"### Precision-Recall Curve","135f379a":"## Random Forest Classifier Metrics\n\nThe data has an imbalance on the order of $\\text{Skew} = 10$. Since any given basket typically has far fewer items than the total previous products a user has purchased, there are many more actual negatives than actual positives. The ease of finding negatives inflates the AUC score; the precision-recall curve shows the classifier has predictive power only for larger classification probability threshold values. [Reference](https:\/\/machinelearningmastery.com\/roc-curves-and-precision-recall-curves-for-classification-in-python\/)","45c16fcb":"Additional threshold choices for Kaggle submission:","fea43e18":"### Confusion Matrices","5ef34385":"## Kaggle Output (sanity check)","55a814a3":"Let's take a look at the node sizes of the trees.","6e8cf715":"### ROC Curve","006b81b2":"### $N_{0.5}$\n\nA direct interpretation of `y_predict_proba` in terms of probabilities would make a threshold of $p_0 = 0.5$ the most principled choice. On the other hand, choices made in the model definition and implementation could make this interpretation dubious. Nonetheless, we can define $$N_{0.5} = \\left\\{ (u,p) \\ | \\ P((u,p)) > 0.5 \\right\\}$$ to take a look at scores.","9a8456c6":"This [top-$N(u)$ `groupby()` reference](https:\/\/stackoverflow.com\/questions\/37482733\/selecting-top-n-elements-from-each-group-in-pandas-groupby) was helpful.","04f0a4e9":"### Scores"}}