{"cell_type":{"72c1a79c":"code","af95a30c":"code","22a0ad7d":"code","01e62549":"code","9817322d":"code","e64fe972":"code","42df5738":"code","38dd38b1":"code","028f295a":"code","6f983828":"code","59ac54ff":"code","bf5f0d3d":"code","fdcc1cf4":"code","e124af5a":"code","2298ce4c":"code","c8b39008":"code","d4480fc0":"code","03edf505":"code","c086d881":"code","20d9d9d5":"code","f614673c":"code","ba58480a":"code","165615b7":"code","5b046d2f":"code","b55eb8df":"code","600ca744":"code","25da9a4c":"code","b4f15c7a":"code","2ba78687":"code","785c303e":"code","098c0d71":"code","7ad3b56b":"code","80185966":"code","16879083":"code","52a44bcc":"code","d7e1a882":"code","8021037c":"code","89da9f8c":"code","ef02a658":"code","72edeb52":"code","0d92538f":"code","8aa40c06":"code","93593548":"code","7b0ca26e":"code","207bb3d2":"code","b22926b8":"code","7bd8d750":"code","e86dea57":"code","fc5e03bc":"code","a1d3d678":"code","04395d0b":"code","73253c9b":"code","e8c8f58e":"code","f498e94d":"code","1a1262f5":"code","caec3d06":"code","03e1dcd7":"code","34142d28":"code","d47a4f85":"code","b144a07a":"code","0d8b7f43":"code","b02a0f16":"code","ff35b830":"code","fb791853":"code","1437f5f4":"markdown","9ef75c13":"markdown","e6493d61":"markdown","3ab61027":"markdown","132673c8":"markdown","215566e5":"markdown","7cea1720":"markdown","9da9fe5e":"markdown","a9c5d9ad":"markdown","fde00be8":"markdown","2e393506":"markdown","b3ca0e0b":"markdown","427354cc":"markdown","420d0070":"markdown","caf47404":"markdown","5c4ea91c":"markdown","234cb329":"markdown","c1113a1d":"markdown","4f1bbb28":"markdown","88f76108":"markdown","dd06694b":"markdown"},"source":{"72c1a79c":"# Import all tools needed\n\n# Regular EDA (exploratory data analysis) and plotting libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n%matplotlib inline\n\n# Models from Scikit-learn\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Model Evaluations\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.model_selection import RandomizedSearchCV, GridSearchCV\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.metrics import precision_score, recall_score, f1_score\nfrom sklearn.metrics import plot_roc_curve\n","af95a30c":"df = pd.read_csv(\"data\/heart-disease.csv\")\ndf.shape #( rows, columns)","22a0ad7d":"df.head()","01e62549":"df.tail()","9817322d":"# Determine how many of each class there are in data set\ndf[\"target\"].value_counts()","e64fe972":"df[\"target\"].value_counts().plot(kind=\"bar\", color=[\"red\", \"blue\"])","42df5738":"df.info()","38dd38b1":"# Checking whether there are any missing values\ndf.isna().sum()","028f295a":"df.describe()","6f983828":"df.sex.value_counts()","59ac54ff":"# Compare target column with sex column\npd.crosstab(df.target, df.sex)","bf5f0d3d":"# Create a plot of crosstab\npd.crosstab(df.target, df.sex).plot(kind=\"bar\",\n                                   figsize=(10,6),\n                                   color=[\"red\",\"blue\"])\nplt.title(\"Heart Disease frequency for sex\")\nplt.xlabel(\"0: No heart disease, 1: heart disease\")\nplt.ylabel(\"Number of individuals\")\nplt.legend([\"Female\", \"Male\"]);\nplt.xticks(rotation=0);","fdcc1cf4":"df.cp.value_counts()","e124af5a":"# Compare target column with chest pain column\npd.crosstab(df.target, df.cp)","2298ce4c":"# Create a plot of crosstab\npd.crosstab(df.target, df.cp).plot(kind=\"bar\",\n                                   figsize=(10,6),\n                                   color=[\"red\",\"blue\",\"green\",\"orange\"])\nplt.title(\"Heart Disease frequency for type of chest pain\")\nplt.xlabel(\"0: No heart disease, 1: Heart disease\")\nplt.ylabel(\"Number of individuals\")\nplt.legend([\"typical angina\", \"atypical angina\", \"non-anginal pain\", \"asymptomatic\"]);\nplt.xticks(rotation=0);","c8b39008":"df[\"thalach\"].value_counts()","d4480fc0":"# Create another figure\nplt.figure(figsize=(10,6))\n\n# Scatter with positive examples\nplt.scatter(df.age[df.target==1],\n           df.thalach[df.target==1])\n\n# Scatter with negative examples\nplt.scatter(df.age[df.target==0],\n           df.thalach[df.target==0])\n\n# Add info\nplt.title(\"Heart Disease as function of Age and Max Heart Rate\")\nplt.xlabel(\"Age\")\nplt.ylabel(\"Max Heart Rate\")\nplt.legend([\"Disease\", \"No Disease\"])","03edf505":"# Check distribution of age column with histogram\ndf.age.plot.hist()","c086d881":"df.head()","20d9d9d5":"# Make a correlation matrix\ndf.corr()","f614673c":"# Visualizing correlation","ba58480a":"corr_matrix = df.corr()\nfix, ax = plt.subplots(figsize=(15,10))\nax = sns.heatmap(corr_matrix,\n                annot=True,\n                linewidths=0.5,\n                fmt=\".2f\",\n                cmap=\"YlGnBu\")\nbottom, top = ax.get_ylim()\nax.set_ylim(bottom + 0.5, top - 0.5)","165615b7":"df.head()","5b046d2f":"# Split data into X and y\nX = df.drop(\"target\", axis=1)\n\ny = df[\"target\"]","b55eb8df":"X","600ca744":"y","25da9a4c":"# Split data into train and test sets\nnp.random.seed(18)\n\n# Split into train and test set\nX_train, X_test, y_train, y_test = train_test_split(X,\n                                                   y,\n                                                   test_size=0.2)","b4f15c7a":"X_train.head()","2ba78687":"y_train.head()","785c303e":"# Put models in a dictionary\nmodels = {\"Logistic Regression\": LogisticRegression(),\n         \"KNN\": KNeighborsClassifier(),\n         \"Random Forest\": RandomForestClassifier()}\n\n# Create a function to fit and score models\ndef fit_and_score(models, X_train, XP_test, y_train, y_test):\n   \n    \"\"\"\n   Fits and evaluates given machine learning models.\n   models: a dict of different Scikit_Learn machine learning models\n   X_train: training data (no labels)\n   X_test: testing data (no lables\n   y_train: training labels\n   y_test: test labels)\n   \"\"\" \n    # Set random seed\n    np.random.seed(18)\n    # Make a dictionary to keep model scores\n    model_scores = {}\n    # Loop through models\n    for name, model in models.items():\n        # Fit model to data\n        model.fit(X_train, y_train)\n        # Evaluate model and append its score to model_scores\n        model_scores[name] = model.score(X_test, y_test)\n\n    return model_scores\n    \n    ","098c0d71":"model_scores = fit_and_score(models,\n                            X_train,\n                             X_test,\n                             y_train,\n                             y_test)\n\nmodel_scores","7ad3b56b":"model_compare = pd.DataFrame(model_scores, index=[\"accuracy\"])\nmodel_compare.T.plot.bar();","80185966":"# Tuning KNN\n\ntrain_scores = []\ntest_scores = []\n\n# Create a list of different values for KNN\nneighbors = range(1, 31)\n\n# Setup KNN instance\nknn = KNeighborsClassifier()\n\n# Loop through different n_neighbors\nfor i in neighbors:\n    knn.set_params(n_neighbors=i)\n    \n    # Fit algorithm\n    knn.fit(X_train, y_train)\n    \n    # Update training scores list\n    train_scores.append(knn.score(X_train, y_train))\n    \n    # Update test scores list\n    test_scores.append(knn.score(X_test, y_test))\n","16879083":"train_scores","52a44bcc":"test_scores","d7e1a882":"plt.plot(neighbors, train_scores, label=\"Train score\")\nplt.plot(neighbors, test_scores, label=\"Test score\")\nplt.xticks(np.arange(1,31,1))\nplt.xlabel(\"Number of neighbors\")\nplt.ylabel(\"Model score\")\nplt.legend()\n\n\nprint(f'Max KNN score on test data: {max(test_scores)*100:.2f}% accuracy')","8021037c":"# Create a hyperparameter grid for LogisticRegression\nlog_reg_grid = {\"C\":np.logspace(-25,25,100),\n               \"solver\": [\"liblinear\"]}\n# Create a hyperparameter grid for RandomForestClassifier\nrf_grid = {\"n_estimators\": np.arange(10,1000,50),\n           \"max_depth\": [None, 3,5,10],\n          \"min_samples_split\": np.arange(2,20,2),\n          \"min_samples_leaf\": np.arange(1,20,2)}","89da9f8c":"# Tune Logistic Regression\n\nnp.random.seed(18)\n\n# Setup random hyperparameter search for LogisticRegression\nrs_log_reg = RandomizedSearchCV(LogisticRegression(),\n                                param_distributions=log_reg_grid,\n                                cv=5,\n                                n_iter=20,\n                                verbose=True\n                               )\n\n# Fit random hyperparameter search model for LogisticRegression\nrs_log_reg.fit(X_train,y_train)","ef02a658":"# Find best hyperparamaters\nrs_log_reg.best_params_","72edeb52":"# Find best score\nrs_log_reg.score(X_test, y_test)","0d92538f":"# Setup random seed\nnp.random.seed(18)\n\n# Setup random hyperparameter search for RandomForestClassifier\nrs_rf = RandomizedSearchCV(RandomForestClassifier(),\n                          param_distributions=rf_grid,\n                          cv=5,\n                          n_iter=25,\n                          verbose=True)\n\n# Fit random hyperparameter search model for RandomForestClassifier\nrs_rf.fit(X_train, y_train)","8aa40c06":"# Find best hyperparamaters\nrs_rf.best_params_","93593548":"rs_rf.score(X_test,y_test)","7b0ca26e":"# Evaluate the randomized search RandomForestClassifier model\nrs_rf.score(X_test, y_test)","207bb3d2":"model_scores","b22926b8":"# Different hyperparameters for LogisticRegression model\nlog_reg_grid = {\"C\": np.logspace(-4,4,200),\n               \"solver\": [\"liblinear\"]}\n\n# Setup grid hyperparameter search for LogisticRegression\ngs_log_reg = GridSearchCV(LogisticRegression(),\n                         param_grid=log_reg_grid,\n                         cv=5,\n                         verbose=True)\n\n# Fit grid hyperparameter seach model\ngs_log_reg.fit(X_train, y_train)","7bd8d750":"# Check best hyperparams\ngs_log_reg.best_params_","e86dea57":"# Evaluate grid search LogisticRegression model\ngs_log_reg.score(X_test, y_test)","fc5e03bc":"# Make predictions with tuned model\ny_preds = rs_rf.predict(X_test)\ny_preds","a1d3d678":"# Import ROC curve function from sklearn.metrics\n# Plot ROC curve and calculate AUC metric for RandomForest\nplot_roc_curve(rs_rf, X_test, y_test)","04395d0b":"# Plot ROC curve and calculate AUC metric for Logistic Regression\nplot_roc_curve(gs_log_reg, X_test, y_test)\n","73253c9b":"# Confusion matrix\nprint(confusion_matrix(y_test, y_preds))","e8c8f58e":"# Confusion matrix\nimport seaborn as sns\nsns.set(font_scale=1.5) # Increase font size\n\ndef plot_conf_mat(y_test, y_preds):\n    \"\"\"\n    Plots a confusion matrix using Seaborn's heatmap().\n    \"\"\"\n    fig, ax = plt.subplots(figsize=(3, 3))\n    ax = sns.heatmap(confusion_matrix(y_test, y_preds),\n                     annot=True, # Annotate the boxes\n                     cbar=False)\n    plt.xlabel(\"Predicted label\") # predictions go on the x-axis\n    plt.ylabel(\"True label\") # true labels go on the y-axis \n\nplot_conf_mat(y_test, y_preds)","f498e94d":"print(classification_report(y_test, y_preds))","1a1262f5":"# Check best hyperparameters\nrs_rf.best_params_","caec3d06":"# Create new classifier with best parameters\nclf = RandomForestClassifier(n_estimators=610,\n                             min_samples_split=16,\n                             min_samples_leaf=5,\n                             max_depth=3)","03e1dcd7":"# Metrics to be cross-validated\nmetrics = [\"accuracy\", \"precision\", \"recall\", \"f1\" ]\n\n# Cross-validation function\ndef cv_eval(metrics,X,y,cv):\n    score_dict = {}\n    for metric in metrics:\n        print(metric)\n        cv_score = cross_val_score(clf,\n                        X,\n                        y,\n                        cv=cv,\n                        scoring=str(metric))\n        score_dict[metric] = np.mean(cv_score)\n    return score_dict        ","34142d28":"score_dict = cv_eval(metrics,X,y,5)","d47a4f85":"# Visualize cross-validated metrics\ncv_metrics = pd.DataFrame(score_dict,index=[0])\n\ncv_metrics.T.plot.bar(title=\"Cross-validated classification metrics for Random Forest Classifier\",\n                     legend=False)","b144a07a":"# Fit an instance of LogisticRegression\nclf = LogisticRegression(C=0.19791668678535573,\n                        solver=\"liblinear\")\nclf.fit(X_train, y_train)","0d8b7f43":"# Check coef_\nclf.coef_","b02a0f16":"df.head()","ff35b830":"# Mach coef_'s of features to columns\nfeature_dict = dict(zip(df.columns, list(clf.coef_[0])))\nfeature_dict","fb791853":"# Visualize feature importance\nfeature_df = pd.DataFrame(feature_dict, index=[0])\nfeature_df.T.plot.bar(title=\"Feature Importance\", legend=False)","1437f5f4":"Tuning models using hyperparameter grid...","9ef75c13":"### Feature Importance\n\nWhich features contributed most to the outcomes of the model and how did they contribute?\n\nFinding feature importance is different for each machine learning model. One way to find feature importance is search the feature importance for that specific model on the internet.\n\nFor LogisticRegression...                         ","e6493d61":"## Preparing tools\nPandas, Matplotlib and NumPy will be used for data analysis and manipulation","3ab61027":"### Heart disease frequency relation to sex","132673c8":"### Hyperparameter tuning with RandomizedSearchCV","215566e5":"## 6. Experimentation\n\nWe haven't hit our evaluation metric of 90%...\n\nWe may:\n\n* Collect more data\n* Improve on current models\n* Try another model such as CatBoost or XGBoost\n","7cea1720":"### Tuning Model\n\nWe will look at:\n* Hyperparameter tuning\n* Cross-validation\n* Feature importance\n* Confusion matrix\n* Precision\n* Recall\n* F1 score\n* Classification report\n* ROC curve\n* Area under curve (AUC)\n\n### Hyperparameter Tuning\n1. by hand\n2. RandomizedSearchCV\n3. GridSearchCV","9da9fe5e":"## 5. Modelling","a9c5d9ad":"## Evaluating tuned machine learning classifier\n\n* ROC curve and AUC score\n* Confusion matrix\n* Classification report\n* Precision\n* Recall\n* F1-score\n\nTo make comparisons and evaluate our trained model, we need to make predictions.","fde00be8":"We will tune:\n* `LogisticRegression()`\n* `RandomForestClassifier()`\n","2e393506":"Tuning RandomForestClassifier()...","b3ca0e0b":"### Age. vs. max heart rate for heart disease","427354cc":"* age: age in years\n* sex: sex (1 = male; 0 = female)\n* cp: chest pain type\n    - Value 1: typical angina\n    - Value 2: atypical angina\n    - Value 3: non-anginal pain\n    - Value 4: asymptomatic \n* trestbps: resting blood pressure (in mm Hg on admission to the hospital)\n* chol: serum cholestoral in mg\/dl\n* fbs: fasting blood sugar > 120 mg\/dl\n* restecg: resting electrocardiographic results (values 0,1,2)\n* thalach: maximum heart rate achieved\n* exang: exercise induced angina\n* oldpeak: ST depression induced by exercise relative to rest\n* slope: the slope of the peak exercise ST segment\n* ca: number of major vessels (0-3) colored by flourosopy\n* thal: 3 = normal; 6 = fixed defect; 7 = reversable defect","420d0070":"## Exploratory data analysis or EDA\n\nThe goal is to become a subject matter expert on the dataset being worked with.\n\n1. What question(s) are we trying to solve?\n2. What kind of data do we have and how do we treat different types?\n3. What's missing from the data and how do we deal with it?\n4. Where are the outliers and why should we care about them?\n5. How can we add, change or remove features from the dataset?","caf47404":"Classification report, cross-validated precision, recall and f1-score","5c4ea91c":"### Heart disease frequency relation to chest pain","234cb329":"### Model Comparison","c1113a1d":"After splitting data into training and test sets, we will build a machine model and **train** it on the **training set**, and **test** it on the **test set**.\n\nWe will experiment with three different machine learning models:\n1. Logistic Regression\n2. K-Nearest Neighbors\n3. Random Forest Classifier","4f1bbb28":"### Calculate evaluation metrics using cross-validation\n\nCalculate precision, recall and f1-score of our model using `cross_val_score()`","88f76108":"**Name:** Heart Health Classifier\n\n**Author:** Sharome Burton\n\n**Date:** 07\/16\/2021\n\n**Description:** Machine learning model used to classify whether a patient has heart disease based on structured data. This notebook looks into using various Python-based machine learning model capable of predicting whether or not someone has heart disease based on their medical attributes.\n\nApproach:\n1. Problem definition\n2. Data (source: https:\/\/archive.ics.uci.edu\/ml\/datasets\/Heart+Disease)\n3. Evaluation\n4. Features\n5. Modelling\n6. Experimentation\n\n## 1. Problem Definition\n\n> Given clinical parameters about a patient, are we able to predict whether they have heart disease?\n\n## 2. Data\n\nThe original data came from Cleveland data from UCI Machine Learning Repository: https:\/\/archive.ics.uci.edu\/ml\/datasets\/Heart+Disease\n\nData is also available on Kaggle: https:\/\/www.kaggle.com\/ronitf\/heart-disease-uci\n\n## 3. Evaluation\n\n> Goal: Reach 90% accuracy at predicting whether a patient has heart disease during proof of concept\n\n## 4. Features\n\nInformation about each feature in the dataset:\n\n* age: age in years\n* sex: sex (1 = male; 0 = female)\n* cp: chest pain type\n    - Value 1: typical angina\n    - Value 2: atypical angina\n    - Value 3: non-anginal pain\n    - Value 4: asymptomatic \n* trestbps: resting blood pressure (in mm Hg on admission to the hospital)\n* chol: serum cholestoral in mg\/dl\n* fbs: fasting blood sugar > 120 mg\/dl\n* restecg: resting electrocardiographic results (values 0,1,2)\n* thalach: maximum heart rate achieved\n* exang: exercise induced angina\n* oldpeak: ST depression induced by exercise relative to rest\n* slope: the slope of the peak exercise ST segment\n* ca: number of major vessels (0-3) colored by flourosopy\n* thal: 3 = normal; 6 = fixed defect; 7 = reversable defect\n\n","dd06694b":"## Hyperparameter Tuning using GridSearchCV\n\n"}}