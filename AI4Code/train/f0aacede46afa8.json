{"cell_type":{"4845634b":"code","b3100a75":"code","d1e1ac21":"code","8903c378":"code","d0387274":"code","da5d3b76":"code","5da36d4f":"code","304b2a93":"code","9bf988a0":"code","a0200efb":"code","f8596dd1":"markdown","e0899021":"markdown","cf2bbca6":"markdown","b76cf464":"markdown","f0923471":"markdown","80ec1e10":"markdown","abe09bbb":"markdown","f6fecfd1":"markdown","50c56ad6":"markdown","fb00d546":"markdown"},"source":{"4845634b":"import pandas as pd\nimport numpy as np\nimport os\nimport torch.nn as nn\nimport torch \nimport matplotlib.pyplot as plt\nfrom torch.utils.data import Dataset,DataLoader","b3100a75":"def plot_data(data_set, model = None, n = 1, color = False):\n    X = data_set[:][0]\n    Y = data_set[:][1]\n    plt.plot(X[Y == 0, 0].numpy(), Y[Y == 0].numpy(), 'bo', label = 'y = 0')\n    plt.plot(X[Y == 1, 0].numpy(), 0 * Y[Y == 1].numpy(), 'ro', label = 'y = 1')\n    plt.plot(X[Y == 2, 0].numpy(), 0 * Y[Y == 2].numpy(), 'go', label = 'y = 2')\n    plt.ylim((-0.1, 3))\n    plt.legend()\n    if model != None:\n        w = list(model.parameters())[0][0].detach()\n        b = list(model.parameters())[1][0].detach()\n        y_label = ['yhat=0', 'yhat=1', 'yhat=2']\n        y_color = ['b', 'r', 'g']\n        Y = []\n        for w, b, y_l, y_c in zip(model.state_dict()['0.weight'], model.state_dict()['0.bias'], y_label, y_color):\n            Y.append((w * X + b).numpy())\n            plt.plot(X.numpy(), (w * X + b).numpy(), y_c, label = y_l)\n        if color == True:\n            x = X.numpy()\n            x = x.reshape(-1)\n            top = np.ones(x.shape)\n            y0 = Y[0].reshape(-1)\n            y1 = Y[1].reshape(-1)\n            y2 = Y[2].reshape(-1)\n            plt.fill_between(x, y0, where = y1 > y1, interpolate = True, color = 'blue')\n            plt.fill_between(x, y0, where = y1 > y2, interpolate = True, color = 'blue')\n            plt.fill_between(x, y1, where = y1 > y0, interpolate = True, color = 'red')\n            plt.fill_between(x, y1, where = ((y1 > y2) * (y1 > y0)),interpolate = True, color = 'red')\n            plt.fill_between(x, y2, where = (y2 > y0) * (y0 > 0),interpolate = True, color = 'green')\n            plt.fill_between(x, y2, where = (y2 > y1), interpolate = True, color = 'green')\n    plt.legend()\n    plt.show()","d1e1ac21":"torch.manual_seed(0)","8903c378":"class Data(Dataset):\n    \n    def __init__(self):\n        self.x = torch.arange(-2, 2, 0.1).view(-1, 1)\n        self.y = torch.zeros(self.x.shape[0])\n        self.y[(self.x > -1.0)[:, 0] * (self.x < 1)[:, 0]] = 1\n        self.y[(self.x >= 1)[:, 0]] = 2\n        self.y = self.y.type(torch.LongTensor)\n        self.len = self.x.shape[0]\n    \n    def __getitem__(self, index):\n        return self.x[index], self.y[index]\n    \n    def __len__(self):\n        return self.len","d0387274":"data_set = Data()\ndata_set.x\nplot_data(data_set)","da5d3b76":"model = nn.Sequential(nn.Linear(1, 3))\nmodel.state_dict()","5da36d4f":"criterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.SGD(model.parameters(), lr = 0.05)\ntrain_loader = DataLoader(dataset = data_set, batch_size = 5)","304b2a93":"LOSS = []\ndef train_model(epochs):\n    for epoch in range(epochs):\n        if epoch % 50 == 0:\n            pass\n            plot_data(data_set, model)\n        for x, y in train_loader:\n            optimizer.zero_grad()\n            ypred = model(x)\n            loss = criterion(ypred, y)\n            LOSS.append(loss)\n            loss.backward()\n            optimizer.step()\ntrain_model(300)","9bf988a0":"z = model(data_set.x)\n_,ypred = z.max(1)\nypred","a0200efb":"corr = (data_set.y == ypred).sum().item()\naccuracy = corr\/len(data_set)\nprint('Accuracy: ', accuracy)","f8596dd1":"* **Cross-Entropy loss:**\n    \n    **To start, our loss function should minimize the negative log likelihood of the correct class (our logarithm here is actually base \"e\" (natural logarithm) since we are taking the inverse of the exponentiation over \"e\" earlier), the negative log yields our cross-entropy loss (computing it over an entire dataset is done by taking the average of the loss for every example).** \n    \n* **Stochastic Gradient Descent Optimizer (SGD):**\n    \n    **Stochastic gradient descent is an iterative method for optimizing an objective function with suitable smoothness properties which I will not cover in this notebook.**","e0899021":"* **In conclusion classification is fun.    Please give this notebook an upvote if you liked it and consider it educative.**","cf2bbca6":"* **Creating the dataset object and plotting it.**","b76cf464":"# What is classification? \n  **Classification is a task that requires the use of machine learning algorithms that learn how to assign a class label to examples from the problem domain. There are many different types of classification tasks that you may encounter in machine learning and specialized approaches to modeling that may be used for each, but in this notebook I will cover the SoftMax Classifier in order to get an intuition of it's actual inner workings.**\n \n **The SoftMax classifier is just a generalization of the binary form of Logistic Regression, our mapping function \"f\" is defined such that it takes an input set of data \"x\" and maps them to the output class labels via a simple (linear) dot product of the data \"x\" and weight matrix \"W\".**\n\n\n\n\n","f0923471":"* **Lets take a look at the predictions.**","80ec1e10":"* **This training process is similar to the one that a baby goes through when he starts to take the first steps. He falls and gets up until he learns to walk properly.  Our classifier has the exact same behaviour he learns and evolves depending on the amount of data it is exposed to.**","abe09bbb":"* **Defining our linear model using torch's \"nn\" module and viewing the trainable parameters.**","f6fecfd1":"* **This function helps us plot the data in order to see how our linear classifier will evolve and learn.**","50c56ad6":"* **As you can see it has correctly classified all the examples.**","fb00d546":"* **Creating a linearly separable dataset for our classifier to train on.**"}}