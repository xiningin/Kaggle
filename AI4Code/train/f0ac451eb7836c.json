{"cell_type":{"b8cc11e3":"code","ba2a36f8":"code","892076d6":"code","7352cd5a":"code","8edc33f4":"code","dc29f160":"code","b62086fc":"code","03726efa":"code","ce10a3fe":"code","3ce580b6":"code","dc9ef786":"code","1c69bcbc":"code","8101b4c3":"code","77a8349d":"code","bf139524":"code","95f196e5":"code","8ceeba5c":"code","d3ad7f84":"code","b09d585a":"code","90127d98":"code","b5c19508":"code","095845a6":"code","95e8ecba":"code","dc9496c3":"code","5cdcec44":"code","a59ed7c3":"code","08fc47dc":"code","4fad6940":"code","45e85c54":"code","8c48da65":"code","f1855d39":"code","cb17d5e5":"code","e9594bf7":"code","656f6d53":"markdown","a43e96f0":"markdown","6168ebed":"markdown","f8e4acb7":"markdown","a12df536":"markdown","85aa6b47":"markdown","bee3e0fc":"markdown"},"source":{"b8cc11e3":"import numpy as np\nimport pandas as pd\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n%matplotlib inline","ba2a36f8":"#importing train and test data\ntrain_data = pd.read_csv(filepath_or_buffer = \"\/kaggle\/input\/titanic\/train.csv\")\ntest_data = pd.read_csv(filepath_or_buffer = \"\/kaggle\/input\/titanic\/test.csv\")","892076d6":"#Let's investigate the training data a little bit\ntrain_data.head(10)","7352cd5a":"#Dropping Name, Ticket, and Cabin columns, since they don't describe much information\ntrain_data.drop(labels = ['Name','Ticket', 'Cabin'], axis = 1, inplace = True)\ntrain_data.describe()","8edc33f4":"#Knowing NaNs in the Dataset is very important\nprint(\"NaN values in the DataFrame:\")\ntrain_data.isna().sum()","dc29f160":"#2 people with NaN in Embarked column can be easily dropped\ntrain_data.dropna(subset = ['Embarked'], inplace = True, axis = 0)","b62086fc":"#Checking whether the deletion was successful\nprint(\"NaN in the DataFrame:\")\ntrain_data.isna().sum()","03726efa":"#Checking whether Age is correlated to any of the columns\naxes = sns.heatmap(train_data.corr(), vmin=-1,vmax=1,center=0, square = True)\naxes.set_xticklabels(axes.get_xticklabels(), rotation=45, horizontalalignment='right')","ce10a3fe":"#Survived people's age\nsns.histplot(x = train_data[train_data['Survived']==1]['Age'], hue = train_data['Pclass'])","3ce580b6":"#Age of people who didn't survive\nsns.histplot(x = train_data[train_data['Survived']==0]['Age'], hue = train_data['Pclass'])","dc9ef786":"#Since Age is somewhat negatively correlated to Pclass,\n# it is a good idea to fill Age's NaN values according to Pclass and its survivors\n# to achieve even more accuracy.\nplots = sns.FacetGrid(train_data, col = 'Pclass', row = 'Survived')\nplots.map(sns.histplot, 'Age', color = '#C4D7F2', bins = 20)","1c69bcbc":"#It seems reasonable to fill all Age's NaN values with\n# means of different Pclasses' Age values, dividing them into survivors and people who didn't survive\nP_train_sur_avg = []\nP_train_die_avg = []\nfor i in [1,2,3]:\n    P_train_sur_avg.append(train_data[(train_data['Survived']==1) & (train_data['Pclass'] == i)]['Age'].mean())\n    P_train_die_avg.append(train_data[(train_data['Survived']==0) & (train_data['Pclass'] == i)]['Age'].mean())","8101b4c3":"#As we can see, older people tend to die in each of classes\nfor i in [1,2,3]:\n    print('Avg age of people survived in ', i, ' class: ', P_train_sur_avg[i-1])\nfor i in [1,2,3]:\n    print('Avg age of people died in ', i, ' class: ', P_train_die_avg[i-1])","77a8349d":"for index in train_data.index:\n    if np.isnan(train_data.loc[index, 'Age']):\n        if train_data.loc[index, 'Survived']==1:\n            train_data.loc[index, 'Age'] = P_train_sur_avg[train_data.iloc[index]['Pclass']-1]\n        else: \n            train_data.loc[index, 'Age'] = P_train_die_avg[train_data.iloc[index]['Pclass']-1]","bf139524":"train_data.isna().sum()","95f196e5":"#Encoding all the categorical variables\ntrain_data = pd.get_dummies(train_data, columns = ['Sex', 'Embarked'])\ntrain_data.drop(labels = ['Sex_female', 'Embarked_C'], axis = 1, inplace = True)\ntrain_data.head(5)","8ceeba5c":"#Having a glance at the test data and cleaning it as well\ntest_data.head(10)","d3ad7f84":"print(test_data.isna().sum())\ntest_data.describe()","b09d585a":"#Dropping columns such as Name, Ticket and Cabin would be a reasonable move\ntest_data.drop(labels = ['Ticket', 'Name', 'Cabin'], axis = 1, inplace = True)","90127d98":"#Also, there is a need in encoding categoricals as in the train dataset\ntest_data = pd.get_dummies(test_data, columns = ['Sex', 'Embarked'])\ntest_data.drop(labels = ['Sex_female', 'Embarked_C'], axis = 1, inplace = True)\ntest_data.head(5)","b5c19508":"#Firstly, it is more convenient to deal with Fare's NaN\ntest_data[test_data['Fare'].isnull()]","095845a6":"#As it turned out, we have exactly \n# Class, Sex and Age to fill the missing value\nFare_missing_index = test_data[test_data['Fare'].isnull()].index\ntest_wo_NaN = test_data.dropna(axis = 0)\n\n#Tuning age precision it is possible to find\n# people with the same Pclass and Sex,\n# and take mean of their Fare\nSex_criteria = (test_wo_NaN['Sex_male'] == test_data.loc[Fare_missing_index, 'Sex_male'].values[0])\nPclass_criteria = (test_wo_NaN['Pclass'] == test_data.loc[Fare_missing_index, 'Pclass'].values[0])\nAge_precision = 20\nAge_criteria = (test_wo_NaN['Age'] >= test_data.loc[Fare_missing_index, 'Age'].values[0] - Age_precision) & \\\n            (test_wo_NaN['Age'] <= test_data.loc[Fare_missing_index, 'Age'].values[0] + Age_precision)\n\ntest_wo_NaN[Sex_criteria & Pclass_criteria & Age_criteria]","95e8ecba":"test_data.loc[Fare_missing_index, 'Fare'] = test_wo_NaN[Sex_criteria & Pclass_criteria & Age_criteria]['Fare'].mean()\nprint(test_data.loc[Fare_missing_index, 'Fare'])","dc9496c3":"#Now, when we do not have survived column, it would be\n# reasonable to create a light model to fill Age column in Test Data\n\n#After certain tries between SVR and simple Linear Regression,\n# the latter has shown better results, so it will be used.\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error as MSE\n\nLR_age = LinearRegression()\n\n#lightm stands for \"light model\"\n#lightm_data is combined data(train + test) to achieve\n# more accuracy while predicting Age\nlight1 = train_data.drop(labels = ['Survived'], axis = 1)\nlight2 = test_data.dropna(axis = 0)\n\nlightm_data = light1.append(light2, sort=False)\nlightm_train, lightm_test = train_test_split(lightm_data, test_size = 0.2, random_state = 42)\n\nLR_age.fit(lightm_train.drop(labels = ['Age', 'PassengerId'], axis = 1), lightm_train['Age'])\ny_true = lightm_test['Age']\ny_pred = LR_age.predict(lightm_test.drop(labels = ['Age', 'PassengerId'], axis = 1))\nprint(MSE(y_true, y_pred, squared=False))","5cdcec44":"#The result is good enough for predictions, so \n# now it is time to fill all test_data Age's NaNs\npred = LR_age.predict(test_data[test_data['Age'].isnull()].drop(labels = ['PassengerId', 'Age'], axis = 1))\nfor (index, i) in zip(test_data[test_data['Age'].isnull()].index, range(len(pred))):\n    test_data.loc[index, 'Age'] =  pred[i]","a59ed7c3":"#Making sure the data is clean and ready for predictions\nprint(' Train data:')\nprint(train_data.isna().sum())\nprint('\\n Test data:')\nprint(test_data.isna().sum())","08fc47dc":"#scaling Age and Fare columns\nfrom sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\ntrain_data[['Age', 'Fare']] = scaler.fit_transform(train_data[['Age', 'Fare']])\ntest_data[['Age', 'Fare']] = scaler.transform(test_data[['Age', 'Fare']])","4fad6940":"#Essentials - importing metrics and splitting the training data\nfrom sklearn.metrics import confusion_matrix, roc_curve, roc_auc_score, RocCurveDisplay\ntrain_data_tr, train_data_te = train_test_split(train_data, test_size = 0.2, random_state = 42)","45e85c54":"#Now it is time to try out Different Classification models\nfrom sklearn.linear_model import LogisticRegression\nLRm = LogisticRegression()\nLRm.fit(train_data_tr.drop(labels = ['PassengerId', 'Survived'], axis = 1), train_data_tr['Survived'])\n\ny_true = train_data_te['Survived']\ny_pred = LRm.predict(train_data_te.drop(labels = ['PassengerId', 'Survived'], axis = 1))\n\nprint('Confusion matrix:')\nprint(confusion_matrix(y_true, y_pred))\n\ny_scores = LRm.predict_proba(train_data_te.drop(labels = ['PassengerId', 'Survived'], axis = 1))\nfpr, tpr, thresholds = roc_curve(y_true, y_scores[:, 1])\nroc_auc = roc_auc_score(y_true, y_scores[:, 1])\ndisplay = RocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=roc_auc, estimator_name='Logistic Regression')\ndisplay.plot()","8c48da65":"from sklearn.svm import SVC\nSVCm = SVC(kernel = 'linear', probability = True)\nSVCm.fit(train_data_tr.drop(labels = ['PassengerId', 'Survived'], axis = 1), train_data_tr['Survived'])\n\ny_true = train_data_te['Survived']\ny_pred = SVCm.predict(train_data_te.drop(labels = ['PassengerId', 'Survived'], axis = 1))\n\nprint('Confusion matrix:')\nprint(confusion_matrix(y_true, y_pred))\n\ny_scores = SVCm.predict_proba(train_data_te.drop(labels = ['PassengerId', 'Survived'], axis = 1))\nfpr, tpr, thresholds = roc_curve(y_true, y_scores[:, 1])\nroc_auc = roc_auc_score(y_true, y_scores[:, 1])\ndisplay = RocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=roc_auc, estimator_name='Support Vector Classifier')\ndisplay.plot()","f1855d39":"from sklearn.ensemble import RandomForestClassifier\nRFCm = RandomForestClassifier(n_estimators=120)\nRFCm.fit(train_data_tr.drop(labels = ['PassengerId', 'Survived'], axis = 1), train_data_tr['Survived'])\n\ny_true = train_data_te['Survived']\ny_pred = RFCm.predict(train_data_te.drop(labels = ['PassengerId', 'Survived'], axis = 1))\n\nprint('Confusion matrix:')\nprint(confusion_matrix(y_true, y_pred))\n\ny_scores = RFCm.predict_proba(train_data_te.drop(labels = ['PassengerId', 'Survived'], axis = 1))\nfpr, tpr, thresholds = roc_curve(y_true, y_scores[:, 1])\nroc_auc = roc_auc_score(y_true, y_scores[:, 1])\ndisplay = RocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=roc_auc, estimator_name='Random Forest Classifier')\ndisplay.plot()","cb17d5e5":"#All the models show almost the same result, \n# however, Logistic Regression is the most efficient\n# and the most stable out of all.\nSurvived_pred = LRm.predict(test_data.drop('PassengerId', axis = 1))\nprediction = pd.DataFrame(data = Survived_pred, columns = ['Survived'])\nprediction.insert(0, 'PassengerId', test_data['PassengerId'])\nprediction.head(5)","e9594bf7":"#Creating a submission\nprediction.to_csv(path_or_buf = 'submission.csv', sep = ',', index = False, header = True)","656f6d53":"# EDA & data cleaning","a43e96f0":"# Training models","6168ebed":"## Train dataset","f8e4acb7":"## Test dataset","a12df536":"## Scaling datasets","85aa6b47":"# Importing libraries and data","bee3e0fc":"# Creating a submission"}}