{"cell_type":{"80a6fb24":"code","7a6c1575":"code","af6552fe":"code","a07631e3":"code","b2057cc2":"code","b4cf6a97":"code","33b4880f":"markdown","3c321e8d":"markdown","1f23c6c2":"markdown","60729d17":"markdown","6801db50":"markdown","010654c0":"markdown","23649bfa":"markdown","42b78b78":"markdown","cdcade55":"markdown","07b9bd51":"markdown","7bd44331":"markdown","2e51e2fa":"markdown"},"source":{"80a6fb24":"import gym\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport random","7a6c1575":"env = gym.make(\"Taxi-v3\").env\n\n# Q table 500 sample(observation space = 5*5*5*4 = 500) - 6 action (left,right, up, down, pickup, dropout)\nq_table = np.zeros([env.observation_space.n,env.action_space.n])","af6552fe":"\nalpha = 0.1\ngamma = 0.9 \nepsilon = 0.1\n# plotting metric\nreward_list = []\ndropout_list = []","a07631e3":"episode_number = 10000\n\nfor i in range(1,episode_number):\n    \n    # init enviroment\n    state = env.reset()\n    \n    reward_count = 0\n    dropouts = 0\n    \n    while True:\n        \n        # exploit vs explore to find action epsilon 0.1 => %10 explore %90 explotit\n        if random.uniform(0,1) < epsilon:\n            action = env.action_space.sample()\n        else:\n            action = np.argmax(q_table[state])\n            \n        # action process and take reward \/ take observation\n        next_state, reward, done, _ = env.step(action)\n        \n        # q learning funct\n        \n        old_value = q_table[state, action]  #old value\n        next_max = np.max(q_table[next_state]) #next max\n        \n        next_value = (1-alpha)*old_value + alpha*(reward + gamma*next_max)\n        # q table update\n        q_table[state,action] = next_value\n        \n        # update state\n        state = next_state\n        \n        # find wrong dropout \n        if reward == -10:\n            dropouts += 1\n            \n        if done:\n            break\n        \n        reward_count  += reward\n    if i%10 == 0:\n        \n        dropout_list.append(dropouts)\n        reward_list.append(reward_count)\n        print(\"Episode: {}, reward {}, wrong dropout {}\".format(i, reward_count,dropouts))","b2057cc2":"fig, axs = plt.subplots(1,2)\n\naxs[0].plot(reward_list)\naxs[0].set_xlabel(\"episode\")\naxs[0].set_ylabel(\"reward\")\n\naxs[1].plot(dropout_list)\naxs[1].set_xlabel(\"episode\")\naxs[1].set_ylabel(\"wrong dropout\")\n\naxs[0].grid(True)\naxs[1].grid(True)\nplt.show()","b4cf6a97":"env.s = env.encode(0,0,3,4)\nenv.render()\n","33b4880f":"**As you see, algorithm make correct desicion as \"dropoff\" customer.**","3c321e8d":"## Taxi enviroment \n **There are 4 locations (labeled by different letters) and your job is to pick up the passenger at one location and drop him off in another. You receive +20 points for a successful dropoff, and lose 1 point for every timestep it takes. There is also a 10 point penalty for illegal pick-up and drop-off actions.**","1f23c6c2":"![0_9UeNpmTbV3NZjWUk.png](attachment:0_9UeNpmTbV3NZjWUk.png)","60729d17":"**First of all, import necessary libraries. We are going to use \"numpy\" to use math operations, \"gym\" library to reach taxi enviroment, \"matplotlib\" to make graphics and visualize, \"random\" to create random numbers.**","6801db50":"![1_foZSvhano3gHO5476pYV6Q.png](attachment:1_foZSvhano3gHO5476pYV6Q.png)","010654c0":"**I choose 10000 as episode number. It's like epoch in Machine learning. Every episode is a wrong dropout or dropof which means a completed test. As you see, as long as episode number (test number) increasing, the wrong dropouts decreases and it limits at 0. The prizes are negative just because of time penalty in each timestep.**","23649bfa":"**In this section, we are going to visualize the reward and wrong dropouts. As you see, the wrong dropouts decreases and converges at zero. Reward also converges ar -1 and zero which is expected result just because of time penalty.**","42b78b78":"You can check & learn q-learning algorithm [here](https:\/\/towardsdatascience.com\/simple-reinforcement-learning-q-learning-fcddc4b6fe56)","cdcade55":"# Classical Reinforcement Learning Example with Q-Learning Algorithm & Taxi Enviroment (GYM TOOLKIT)","07b9bd51":"**Let's try an example. We will use \"encode\" method and see action that algorithm make.\nenv.encode(taxi row, taxi col, passenger index, destination)**","7bd44331":"**epsilon is explore rate**\n\n**alpha = learning rate**\n\n**gamma = discount facto**\n\n** reward list is defined to keep rewards.**\n** droput list is defined to keep wrong dropouts to see how algorithm works**","2e51e2fa":"**In this section, we define the enviroment and q table. The rows of q table are observation spaces and the columns of q table are actions.\nThere are 5x5 grids, 5 different customer, 4 different pickup-dropoff locations. So obs space is 500.**"}}