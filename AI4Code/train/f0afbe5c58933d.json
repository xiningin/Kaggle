{"cell_type":{"4597b0cb":"code","69d958f8":"code","3478fa62":"code","5707989f":"code","183839d1":"code","7b84e11f":"code","99bb75ff":"code","9bdddcf2":"code","4902f4ce":"code","4c7ae40a":"code","80dc52ab":"code","ffe5ec5f":"code","ef173c8c":"code","e06b6950":"code","52d4e1a5":"code","46bb6f27":"code","d6b7c186":"code","87b4aef8":"code","f73d2276":"code","8cf1641e":"code","798d69e6":"code","71d044a1":"code","3c30dc86":"markdown","78e45e68":"markdown","7a64f397":"markdown","049486a5":"markdown","ff812d4a":"markdown","a6a749b9":"markdown","e29ddd3d":"markdown","721e971a":"markdown","ff1fec35":"markdown","0436aa6f":"markdown","abd25912":"markdown","1cced563":"markdown","93b4df9e":"markdown","4b1f80e8":"markdown","1e4432ee":"markdown","ff4b7366":"markdown","b96ad9e6":"markdown","e22c6896":"markdown","a4f44341":"markdown","8f7709b1":"markdown","53260fe3":"markdown","8019da46":"markdown","348b600c":"markdown","0150e263":"markdown","29ed876b":"markdown"},"source":{"4597b0cb":"import h2o\nimport time\nimport seaborn\nimport itertools\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom h2o.estimators.glm import H2OGeneralizedLinearEstimator\nfrom h2o.estimators.gbm import H2OGradientBoostingEstimator\nfrom h2o.estimators.random_forest import H2ORandomForestEstimator\n\n%matplotlib inline\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","69d958f8":"h2o.init()","3478fa62":"heart_df = h2o.import_file(\"..\/input\/heart.csv\", destination_frame=\"heart_df\")","5707989f":"heart_df.head()","183839d1":"heart_df.describe()","7b84e11f":"for col in heart_df.columns:\n    heart_df[col].hist()","99bb75ff":"plt.figure(figsize=(10,10))\ncorr = heart_df.cor().as_data_frame()\ncorr.index = heart_df.columns\nsns.heatmap(corr, annot = True, cmap='RdYlGn', vmin=-1, vmax=1)\nplt.title(\"Correlation Heatmap\", fontsize=16)\nplt.show()","9bdddcf2":"train, valid, test = heart_df.split_frame(ratios=[0.6,0.1], seed=1234)\nresponse = \"target\"\ntrain[response] = train[response].asfactor()\nvalid[response] = valid[response].asfactor()\ntest[response] = test[response].asfactor()\nprint(\"Number of rows in train, valid and test set : \", train.shape[0], valid.shape[0], test.shape[0])","4902f4ce":"predictors = heart_df.columns[:-1]\ngbm = H2OGradientBoostingEstimator()\ngbm.train(x=predictors, y=response, training_frame=train)","4c7ae40a":"print(gbm)","80dc52ab":"perf = gbm.model_performance(valid)\nprint(perf)","ffe5ec5f":"gbm_tune = H2OGradientBoostingEstimator(\n    ntrees = 1000,\n    learn_rate = 0.01,\n    stopping_rounds = 20,\n    stopping_metric = \"AUC\",\n    col_sample_rate = 0.7,\n    sample_rate = 0.7,\n    seed = 1234\n)      \ngbm_tune.train(x=predictors, y=response, training_frame=train, validation_frame=valid)","ef173c8c":"gbm_tune.model_performance(valid).auc()","e06b6950":"from h2o.grid.grid_search import H2OGridSearch\n\ngbm_grid = H2OGradientBoostingEstimator(\n    ntrees = 1000,\n    learn_rate = 0.01,\n    stopping_rounds = 20,\n    stopping_metric = \"AUC\",\n    col_sample_rate = 0.7,\n    sample_rate = 0.7,\n    seed = 1234\n) \n\nhyper_params = {'max_depth':[4,6,8,10,12]}\ngrid = H2OGridSearch(gbm_grid, hyper_params,\n                         grid_id='depth_grid',\n                         search_criteria={'strategy': \"Cartesian\"})\n#Train grid search\ngrid.train(x=predictors, \n           y=response,\n           training_frame=train,\n           validation_frame=valid)","52d4e1a5":"print(grid)","46bb6f27":"sorted_grid = grid.get_grid(sort_by='auc',decreasing=True)\nprint(sorted_grid)","d6b7c186":"cv_gbm = H2OGradientBoostingEstimator(\n    ntrees = 3000,\n    learn_rate = 0.05,\n    stopping_rounds = 20,\n    stopping_metric = \"AUC\",\n    nfolds=4, \n    seed=2018)\ncv_gbm.train(x = predictors, y = response, training_frame = train, validation_frame=valid)\ncv_summary = cv_gbm.cross_validation_metrics_summary().as_data_frame()\ncv_summary","87b4aef8":"cv_gbm.model_performance(valid).auc()","f73d2276":"from h2o.estimators import H2OXGBoostEstimator\n\ncv_xgb = H2OXGBoostEstimator(\n    ntrees = 1000,\n    learn_rate = 0.05,\n    stopping_rounds = 20,\n    stopping_metric = \"AUC\",\n    nfolds=4, \n    seed=2018)\ncv_xgb.train(x = predictors, y = response, training_frame = train, validation_frame=valid)\ncv_xgb.model_performance(valid).auc()","8cf1641e":"cv_xgb.varimp_plot()","798d69e6":"from h2o.automl import H2OAutoML\n\naml = H2OAutoML(max_models = 10, max_runtime_secs=100, seed = 1)\naml.train(x=predictors, y=response, training_frame=train, validation_frame=valid)","71d044a1":"lb = aml.leaderboard\nlb","3c30dc86":"Let us now split the data into three parts - train, valid and test datasets - at a ratio of 60%, 10% and 20% respectively. We could use split_frame() function for the same.","78e45e68":"As we can see this has printed the log loss performance at various depths. If we want to look at the validation AUC, then we can use the following.","7a64f397":" # H20\nThis is an introductory notebook for people wanting to get started with H2O (the open source machine learning package by H2O.ai)\nH2O is the world\u2019s number one machine learning platform. \n\nIt is an open-source software, and the H2O-3 GitHub repository is available for anyone to start hacking. This hands-on guide aims to explain the basic principles behind H2O and get you as a data scientist started as quickly as possible in the most simple way. The rest is just machine learning.\n\nAfter reading this guide, you\u2019ll be able to:\n\n- Understand which basic problems H2O solves and why,\n- play with H2O \u2014 explore data and create and tune models,\n- see beyond the horizon. Understand where H2O can take you.","049486a5":"# Correlation Heatmap","ff812d4a":"!! Great we have achieved AUC of 0.91 which is satisfactory","a6a749b9":"Once the module in imported, the first step is to initialize the h2o module.\n\nThe h2o.init() command is pretty smart and does a lot of things. First, an attempt is made to search for an existing H2O instance being started already, before starting a new one. When none is found automatically or specified manually with argument available, a new instance of H2O is started.\n\nDuring startup, H2O is going to print some useful information. Version of the Python it is running on, H2O\u2019s version, how to connect to H2O\u2019s Flow interface or where error logs reside, just to name a few.","e29ddd3d":"Now that the initialization is done, let us first import the dataset. The command is very similar to pandas.read_csv and the data is stored in memory as H2OFrame\n\nH2O supports various file formats and data sources.","721e971a":"Now that is quite a bit of information. We can look at them individually.\n\n1. First, we get the name of the model and a key to acces the model ( key is not much useful for us I guess )\n2. Error metrics on the train data like log-loss, mean per class error, AUC, Gini, MSE, RMSE\n3. Confusion matrix for max F1 threshold\n4. Threshold values for different metrics\n5. Gains \/ Lift table\n6. Scoring history - information on how the metrics changed in each of the epochs\n7. Feature importance\nOkay. I heard you. How can we use the metrics of train set (as we actually trained on this dataset). We need to evaluate them from the valid set. We can use the model_performance() function for the same. We can then print the metrics.","ff1fec35":"So using our baseline model, we are getting about 0.87 auc in valid set and 0.1 auc in train set. Similarly, log loss is 0.466 in valid set and 0.093 in train set.\n\nNow we can use the validation set to tune our parameters. We can use the early stopping to find the number of iterations to train similar to other GBM implementations. We can set some random values for the parameters to start with. \n\nPlease note that, we have added a new validation_frame parameter in this one compared to the previous one while training.","0436aa6f":"# Model Tuning","abd25912":"At max_depth of 4 maximum auc is achieved which is 0.917968 ","1cced563":"Interestingly, there is not much change in the AUC for the top two results. Since we train on a very small sample, we might be getting this.\n\nAlso please note that, we just searched for the max_depth parameter. Please do a more comprehensive search for better results. Please refer to this [notebook](https:\/\/github.com\/h2oai\/h2o-3\/blob\/master\/h2o-docs\/src\/product\/tutorials\/gbm\/gbmTuning.ipynb) for more comprehensive details on finetuning.","93b4df9e":"# Train Test validation split","4b1f80e8":"Now, Lets check out the performance of tuned model","1e4432ee":"# feature importance\nfeature importance is inbuilt with xgboost model to see what are the contribution features in heart disease prediction with XGboost.","ff4b7366":"# Histograms for all the features ","b96ad9e6":"now lets look out the auto ml leader board","e22c6896":"# Data Exploration\nNow, lets look out the dataset with h20 dataframe","a4f44341":"# AutoML : Automatic Machine Learning:\n\n\n\nH2O\u2019s AutoML can be used for automating the machine learning workflow, which includes automatic training and tuning of many models within a user-specified time-limit. Stacked Ensembles will be automatically trained on collections of individual models to produce highly predictive ensemble models which, in most cases, will be the top performing models in the AutoML Leaderboard.\n\nSo let us use the H2OAutoML function to do automatic machine learning. We can specify the max_models parameter which indicates the number of individual (or \"base\") models, and does not include the two ensemble models that are trained at the end.","8f7709b1":"## K-Fold cross validation:\n\nMost of the times, we will just do K-fold cross valdiation. So now let us do the same using H2O. Just setting the nfolds parameter in the model will do the k-fold cross validation.","53260fe3":"# Modeling Building\n\nNow, let us build a baseline model using these splits. There are multiple algorithms available in the H2O module. \nFirst Starting with one of my favourite algo Gradient Boosting Machines","8019da46":"# Grid Search","348b600c":"As we can see XGboost AutoML is the top contributor.\n\nPlease hit **Upvote** if you like the introductory kernel of H20.Please share your valuable feedback.","0150e263":"There is a improvement of 2 percent in comparison of GBM that's great","29ed876b":"# H20 with XGBoost"}}