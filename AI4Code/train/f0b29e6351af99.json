{"cell_type":{"b35e289a":"code","6e9bc6b1":"code","873fb978":"code","db102bf4":"code","4efe89a1":"code","53a2687f":"code","a5b3b55b":"code","7f7593ee":"code","80b2e585":"code","d2e68535":"code","733ce608":"markdown","712c5225":"markdown","7400b334":"markdown","42e8fdde":"markdown","2fa64f19":"markdown","d73bf3be":"markdown","76aa2a32":"markdown","870e94e2":"markdown"},"source":{"b35e289a":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import ListedColormap\nimport seaborn as sns\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.datasets import make_moons\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import PolynomialFeatures, StandardScaler\nimport xgboost as xgb\n\nsns.set()","6e9bc6b1":"X, y = make_moons(2000, random_state=0, noise=0.22)\nX = StandardScaler().fit_transform(X) # Useful for polynomial features\nxvar, yvar, label = 'height', 'ear_length', 'survived'\ndf = pd.DataFrame({xvar: X[:, 0], yvar: X[:, 1], label: y})","873fb978":"plt.figure(figsize=(15, 8))\nsns.scatterplot(data=df, x=xvar, y=yvar, hue=label, palette={0.0: 'red', 1.0: 'green'});","db102bf4":"X_train, X_test, y_train, y_test = train_test_split(df[[xvar, yvar]], df[label], random_state=0)","4efe89a1":"plt.figure(figsize=(15, 8))\nsns.scatterplot(x=X_train[xvar], y=X_train[yvar], hue=y_train, palette={0.0: (1.0, 0.7, 0.7), 1.0: (0.7, 1.0, 0.7)}, legend=False)\nsns.scatterplot(x=X_test[xvar], y=X_test[yvar], hue=y_test, palette={0.0: 'red', 1.0: 'green'}, legend=False)\nplt.title('Train-test split');","53a2687f":"def plot_decision_boundary(model, df):\n    xmin, xmax = -2.5, 2.5\n    ymin, ymax = -2.5, 2.5\n    xstep = 0.01\n    ystep = 0.01\n    \n    xx, yy = np.meshgrid(np.arange(xmin, xmax+xstep, xstep), np.arange(ymin, ymax+ystep, ystep))\n    meshdf = pd.DataFrame({xvar: xx.ravel(), yvar: yy.ravel()})\n    Z = model.predict(meshdf).reshape(xx.shape)\n\n    plt.figure(figsize=(16, 8))\n    plt.pcolormesh(xx, yy, Z, cmap=ListedColormap([(1.0, 0.7, 0.7), (0.7, 1.0, 0.7)]))\n    sns.scatterplot(data=df, x=xvar, y=yvar, hue=label, palette={0.0: 'red', 1.0: 'green'});","a5b3b55b":"model = LogisticRegression()\nmodel.fit(X_train, y_train)\nprint('Train accuracy: ', model.score(X_train, y_train))\nprint('Test  accuracy: ', model.score(X_test, y_test))\nplot_decision_boundary(model, df)","7f7593ee":"model = Pipeline([\n    ('poly', PolynomialFeatures(3, include_bias=False)),\n    ('model', LogisticRegression(max_iter=100000))\n])\nmodel.fit(X_train, y_train)\nprint('Train accuracy: ', model.score(X_train, y_train))\nprint('Test  accuracy: ', model.score(X_test, y_test))\nplot_decision_boundary(model, df)","80b2e585":"model = Pipeline([\n    ('poly', PolynomialFeatures(15)),\n    ('model', LogisticRegression(max_iter=100000, solver='newton-cg', penalty='none'))\n])\nmodel.fit(X_train, y_train)\nprint('Train accuracy: ', model.score(X_train, y_train))\nprint('Test  accuracy: ', model.score(X_test, y_test))\nplot_decision_boundary(model, df)","d2e68535":"model = xgb.XGBClassifier()\nmodel.fit(X_train, y_train)\nprint('Train accuracy: ', model.score(X_train, y_train))\nprint('Test  accuracy: ', model.score(X_test, y_test))\nplot_decision_boundary(model, df)","733ce608":"# Underfitting, overfitting and model complexity\n\nWelcome to this kernel on these omni-present machine learning concepts! This notebook is intended to support [my blog bost](https:\/\/anarthal.github.io\/kernel\/posts\/underfitting-overfitting\/) on this same topic. The concepts are illustrated using a binary classification example with two features and a logistic regression model. As always, feedback is welcome. Please upvote if you found it useful!\n\nThis kernel features a made-up dataset, which employs sklearn's [`make_moons`](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.datasets.make_moons.html) to create a non-linearly separable dataset with just two features. This makes visualizations easier to understand. In practice, underfitting and overfitting are much more likely to happen in multi-dimensional spaces (but these are more difficult to visualize!).\n\nAs calling my features `x_0` and `x_1` is extremely boring, let's also make a story up from our data. We all know that elves are rare creatures living in our forest without anyone noticing it. But do all elves reach adult age? We will try to predict a binary variable `survived` given two numeric input features describing physical attributes of the creatures: `height` and `ear_length`.","712c5225":"## Underfitting\n\nLet's start with a simple logistic regression. The model is too simple, so it will underfit our data:","7400b334":"It overfits the data too much. XGBoost is really a good model but a complex one, and usually produces better results in more comples problems and when you have more data. Tuning regularization hyperparameters for XGBoost could likely yield much better results (but I won't do that in this notebook).\n\nThis ends our discussion on underfitting and overfitting. If the notebook doesn't seem very explanative, be sure to check [the blog bost](https:\/\/anarthal.github.io\/kernel\/posts\/underfitting-overfitting\/).\n\nI hope you liked the post! As always, feedback is welcome. Please upvote if you found it useful!","42e8fdde":"## The train-test split\n\nAs mentioned [in the blog post](https:\/\/anarthal.github.io\/kernel\/posts\/underfitting-overfitting\/#train-and-test-set), we split the training set into two.","2fa64f19":"## Visualizing the dataset\n\nLet's see what our dataset looks like:","d73bf3be":"## Overfitting\n\nIf we increase the degree of our polynomial features too much, we end up overfitting.\n\nNote that `LogisticRegression` employs regularization by default, which decreases the overfitting effect a lot. I've turned it off by setting `penality='none'` to make the overfitting effect more evident. The `max_iter` and `solver` args are there to guarantee convergence during training.","76aa2a32":"## Adding polynomial features\n\nTo prevent underfitting, we add polynomial features to the model.\n\nThe approach presented here uses pipelines, which is convenient when plotting the decision boundary (it saves me a call to `PolynomialFeatures().fit_transform()` when sampling the plane to plot the decision boundary). With the pipeline approach, the polynomial feature creation is integrated into the `model` object (called automatically when invoking `fit()` or `predict()`). The functionality is equivalent to the lines presented in the post. ","870e94e2":"## XGBoost\n\nOut of curiosity, I wanted to check what the decision boundary of an `xgboost` model looks like. "}}