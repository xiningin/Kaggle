{"cell_type":{"03d41db4":"code","f7e946a1":"code","770bff21":"code","d349c561":"code","9e9f9d20":"code","0c735552":"code","15aa6897":"code","0a929b0d":"code","466e24d5":"code","e3d59002":"code","cca2b990":"markdown","9f49bb13":"markdown","a7550d26":"markdown","18b3e370":"markdown","8b725d16":"markdown","88bc3655":"markdown","02262e54":"markdown","650dde59":"markdown"},"source":{"03d41db4":"import nltk","f7e946a1":"import numpy as np\nimport os\nfrom nltk.corpus import machado\nimport unicodedata\n\nos.environ['NLTK_DATA'] = '..\/input\/machado\/'\n\n# Remove acentos e coloca palavras em min\u00fasculas\ndef strip_accents_and_lower(s):\n    return ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn').lower()\n\nmachado_sents = map(lambda sent: list(map(strip_accents_and_lower, sent)), machado.sents())\n\n# 'Executa' o mapeamento da lista\n%time machado_sents = list(machado_sents)","770bff21":"import gensim\n\n# Tamanho do 'embedding'\nN = 200\n\n# N\u00famero de palavras anteriores a serem consideradas\nC = 7\n\n%time model = gensim.models.Word2Vec(machado_sents, sg=0, size=N, window=C, min_count=5, hs=0, negative=14)","d349c561":"# Fun\u00e7\u00f5es auxiliares\n\n# Embedding de uma palavra\ndef word_embedding(word):\n    return model[word]\n\n# Pega apenas as palavras a partir do resultado da fun\u00e7\u00e3o 'most_similar'\ndef strip_score(result):\n    return [w for w, s in result]\n\n# Lista as palavras mais pr\u00f3ximas\ndef closest_words(word, num=5):\n    word_score_pair = model.most_similar(word, topn=num)\n    return strip_score(word_score_pair)","9e9f9d20":"# Exemplo de um embedding\nemb = word_embedding('homem')\n\nprint(emb.shape)\nprint(emb)","0c735552":"# Exibe algumas palavras pr\u00f3ximas daquelas contidas nesta lista\ntest_words = ['seja', 'foi', 'amou', 'aquele', 'foram', 'homem', 'rua', 'marcela']\n\nfor w in test_words:\n    print(w, closest_words(w))","15aa6897":"from sklearn.manifold import TSNE\nfrom sklearn.decomposition import PCA\nfrom sklearn.random_projection import GaussianRandomProjection\n\nimport matplotlib.pyplot as plt\n\n%matplotlib inline\n\nseed_words = set([\n    # Personagens:\n    'quincas', 'cubas',\n        \n    # Verbos:\n    'estar', 'encontrar',\n    \n    # Objetos\n    'bolsa', 'relogio'\n])\n\n# Vamos montar uma lista com as palavras raiz + 3 palavras pr\u00f3ximas\nall_words = []\nfor w in seed_words:\n    all_words.append(w)\n    all_words.extend(closest_words(w, 3))\n    \nall_words = set(all_words)\n\n# Converte cada palavra para sua representa\u00e7\u00e3o (embedding)\nhigh_dim_embs = np.array(list(map(word_embedding, all_words)))\n\n# Cria os gr\u00e1ficos (3 linas e 1 coluna)\nfig, axes = plt.subplots(3, 1, figsize=(16, 16))\n\n# Fun\u00e7\u00e3o que ir\u00e1 desenhar as palavras\ndef plot_labels(ax, high_dim_embs, words, dim_reduction):\n    # Faz a redu\u00e7\u00e3o para 2 dimens\u00f5es (espera-se que o par\u00e2metro `dim_reduction`\n    # esteja corretamente configurado)\n    low_dim_embs = dim_reduction.fit_transform(high_dim_embs)\n    \n    ax.set_title(dim_reduction.__class__.__name__)\n    \n    # Agora vamos \"desenhar\" cada palavra em sua posi\u00e7\u00e3o x, y\n    for (x, y), w in zip(low_dim_embs, words):\n        ax.scatter(x, y, c='red' if w in seed_words else 'blue')\n        ax.annotate(w,\n                     xy=(x, y),\n                     xytext=(5, 2),\n                     textcoords='offset points',\n                     ha='right',\n                     va='bottom')\n        \n# Chama a rotina de desenho, modificando o algoritmo de redu\u00e7\u00e3o de dimens\u00e3o\n\n## PCA:\nplot_labels(axes[0], high_dim_embs, all_words, PCA(2))\n\n## TSNE:\nplot_labels(axes[1], high_dim_embs, all_words, TSNE(2, perplexity=10, method='exact'))\n\n## GaussianRandomProjection:\nplot_labels(axes[2], high_dim_embs, all_words, GaussianRandomProjection(2))","0a929b0d":"# Faz a conjunga\u00e7\u00e3o do verbo no passado, a partir do exemplo do verbo 'amar' -> 'amou'\n\ndef past(verb):\n    result = model.most_similar(positive=['amou', verb], negative=['amar'], topn=5)\n    return strip_score(result)\n\nverbs = ['amar', 'explicar', 'contar', 'falar']\n\nfor verb in verbs:\n    print(verb, past(verb))","466e24d5":"# Agora vamos calcular o passado de um verbo, s\u00f3 que utilizando v\u00e1rios exemplos\n# A id\u00e9ia \u00e9 calcular um vetor m\u00e9dio que leve da regi\u00e3o do 'infinitivo' para o 'passado'\npast_table = [\n    ('andar', 'andou'),\n    ('chegar', 'chegou'),\n    ('sair', 'saiu'),\n    ('sentir', 'sentiu'),\n    ('perder', 'perdeu'),\n    ('lembrar', 'lembrou')\n]\n\npositives = list(map(lambda v: v[1], past_table))\nnegatives = list(map(lambda v: v[0], past_table))\n\ndef past2(verb):\n    # Aqui iremos incluir todos os exemplos positivos (relacionados ao 'passado'), os negativos\n    # (relacionados ao 'infinitivo'), e o verbo que se quer 'calcular o passado'.\n    # Note que temos que multiplicar o verbo pelo n\u00famero de exemplos que temos a fim de se \n    # manter a propor\u00e7\u00e3o\n    result = model.most_similar(positive=positives + [verb] * len(past_table), negative=negatives, topn=5)\n    return strip_score(result)\n\nfor verb in verbs:\n    print(verb, past2(verb))","e3d59002":"past2('levantar')","cca2b990":"## Parte 2 - Treino do _embedding_\n\nVamos treinar um _embedding_ com tamanho 200 e utilizando uma janela de 7 palavras.\n\nOutros par\u00e2metros s\u00e3o:\n - Modelo: CBOW\n - Tamanho do embedding: 200\n - Janela: 7 palavras\n - N\u00famero m\u00ednimo de vezes que a palavra deve aparecer: 5\n - N\u00famero de palavras \"negativas\" utilizadas: 14\n \nA documenta\u00e7\u00e3o completa dos par\u00e2metros pode ser encontrada neste link: https:\/\/radimrehurek.com\/gensim\/models\/word2vec.html#gensim.models.word2vec.Word2Vec","9f49bb13":"Podemos verificar que a fun\u00e7\u00e3o n\u00e3o \u00e9 perfeita, mas temos como melhor\u00e1-la.\n\nUma forma de realizar isso \u00e9 fazendo opera\u00e7\u00f5es em outros pares de palavras que seguem a mesma dire\u00e7\u00e3o. Em seguida, combinamos estes vetores.","a7550d26":"### Exemplo de um embedding\n\nChamando-se a fun\u00e7\u00e3o `word_embedding(word)` podemos inspecionar o vetor descritivo daquela palavra","18b3e370":"# Laborato\u0301rio 11 - Word Embeddings\n\nNeste laborat\u00f3rio, vamos utilizar a obra completa de Machado de Assis para treinar um _word embedding_ em portugu\u00eas.\n\nO acesso aos dados ser\u00e1 feito atrav\u00e9s da biblioteca <a href=\"http:\/\/www.nltk.org\/\">NLTK<\/a>.\n\nNote que esta biblioteca \u00e9 bastante completa no \u00e2mbito lingu\u00edstico, contendo fun\u00e7\u00f5es como _stemming_ (redu\u00e7\u00e3o ao radical da palavra) e remo\u00e7\u00e3o de _stop-words_ (palavras comuns, como artigos e pronomes).","8b725d16":"## Parte 3 - Palavras semelhantes\n\nPodemos fazer uma busca a partir de uma palavra e encontrar aquelas mais pr\u00f3ximas.","88bc3655":"## Parte 1 - Carregando os dados\n\nVamos carregar cada frase da obra de Machado de Assis utilizando a fun\u00e7\u00e3o `machado.sents()`.\n\nEm seguida, utilizamos a fun\u00e7\u00e3o `strip_accents_and_lower` para remover acentos e converter para letras min\u00fasculas.","02262e54":"## Parte 4 -  Conjugando um verbo no passado\n\nUm das formas mais interessantes de se trabalhar com _word embedding_ \u00e9 que certas dire\u00e7\u00f5es, dentro do espa\u00e7o vetorial, capturam certas rela\u00e7\u00f5es sem\u00e2nticas entre as palavras.\n\nComo exemplo disto, temos as rela\u00e7\u00f5es masculino-feminino, tempos verbais e pa\u00eds-capital.\n\n![title](..\/input\/linear-relationships.png)\n\nNesta se\u00e7\u00e3o, vamos criar uma fun\u00e7\u00e3o chamada `past` que transforma um verbo no infitinivo em sua conjuga\u00e7\u00e3o passada.\n\nEla n\u00e3o funciona de forma perfeita, mas nos casos dos verbos mais comuns costuma realizar a conjuga\u00e7\u00e3o corretamente.","650dde59":"### Desenhando palavras pr\u00f3ximas\n\nNeste se\u00e7\u00e3o vamos escolher algumas palavras 'sementes' e encontrar suas 3 semelhantes."}}