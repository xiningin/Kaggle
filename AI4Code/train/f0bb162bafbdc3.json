{"cell_type":{"78f45470":"code","9a837503":"code","f9c7d5d6":"code","203118f5":"code","08c5eac6":"code","6abbb8c3":"code","1e91adfe":"code","a2e9fca0":"code","d2cf181f":"code","e23ff5e9":"code","b056285d":"code","28fb9a3e":"code","06190141":"code","a8a06e15":"code","1498c41b":"code","8a5ff500":"code","0e5441ac":"code","c92f19f8":"code","e9193000":"code","10e9be41":"code","2b8fa4c8":"code","dc8d2e08":"code","52cc75c2":"code","59390aee":"markdown","05cd108f":"markdown","853119b2":"markdown","00f934c5":"markdown","02be37f1":"markdown","29d55804":"markdown","872a61e4":"markdown","c205bc27":"markdown","e44abec6":"markdown","c1419afb":"markdown","270e70d4":"markdown","f56689fe":"markdown","0173a12e":"markdown"},"source":{"78f45470":"import numpy as np\nimport pandas as pd\nimport re\nimport torch\nimport torchvision\nimport torch.nn as nn\nfrom glob import glob\nfrom PIL import Image\nfrom torchvision import transforms\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.model_selection import train_test_split","9a837503":"device = 'cuda' if torch.cuda.is_available() else 'cpu'","f9c7d5d6":"file_paths = glob(\"\/kaggle\/input\/multiclass-weather-dataset\/Multi-class Weather Dataset\/*\/*\")","203118f5":"labels = list()\nfor path in file_paths:\n    file_name = path[path.rfind(\"\/\")+1:]\n    labels.append(file_name[:re.search(r\"\\d\", file_name).start()])","08c5eac6":"df = pd.DataFrame({'path': file_paths, 'class': labels})","6abbb8c3":"df.head()","1e91adfe":"def label_column(df):\n    classes = list(df['class'].unique())\n    class_to_num = dict(zip(classes, range(len(classes))))\n    df['label'] = df['class'].apply(lambda x: class_to_num[x])","a2e9fca0":"label_column(df)","d2cf181f":"df.head()","e23ff5e9":"train, test = train_test_split(df, test_size=0.2, shuffle=True, random_state=42)\ntest, val = train_test_split(test, test_size=0.5, shuffle=True, random_state=42)","b056285d":"len(val)","28fb9a3e":"transform = transforms.Compose([\n                transforms.ToPILImage(),\n                transforms.CenterCrop((100, 100)),\n                transforms.RandomCrop((80, 80)),\n                transforms.RandomHorizontalFlip(p=0.5),\n                transforms.RandomRotation(degrees=(-90, 90)),\n                transforms.RandomVerticalFlip(p=0.5),\n                transforms.Resize((156, 156)),\n                transforms.ToTensor(),\n                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n            ])","06190141":"class CustomDataset(Dataset):\n    def __init__(self, df, transform=None):\n        self.X = df['path']\n        self.y = df['label']\n        self.transform = transform\n        \n    def __len__(self):\n        return len(self.y)\n        \n    def __getitem__(self, index):\n        label = self.y[index]\n        img = Image.open(self.X[index]).convert('RGB')\n        img = img.resize((156, 156))\n        img = np.array(img)\n        if self.transform: # augmentation\n            img = self.transform(img)\n        else:\n            img = transforms.ToTensor()(img)\n        return {'X': img.to(device), 'y': label} # output should always be tensors","a8a06e15":"trainD = CustomDataset(train.reset_index(drop=True), transform=None)\ntestD = CustomDataset(test.reset_index(drop=True))\nvalD = CustomDataset(val.reset_index(drop=True))","1498c41b":"trainDL = DataLoader(trainD, batch_size=16, shuffle=True)\ntestDL = DataLoader(testD, batch_size=16)\nvalDL = DataLoader(valD, batch_size=16)","8a5ff500":"for data in valDL:\n    print(data['X'].shape)\n    print(data['y'])","0e5441ac":"class CNN(nn.Module):\n    def __init__(self):\n        super(CNN, self).__init__()\n        self.conv1 = nn.Conv2d(3, 6, 5)\n        self.conv2 = nn.Conv2d(6, 10, 5)\n        self.conv3 = nn.Conv2d(10, 14, 5)\n        self.conv4 = nn.Conv2d(14, 18, 5)\n        self.maxpool = nn.MaxPool2d(2, 2)\n        self.linear1 = nn.Linear(6*6*18, 120)\n        self.linear2 = nn.Linear(120, 30)\n        self.output = nn.Linear(30, 4)\n        self.relu = nn.ReLU()\n        self.sigmoid = nn.Sigmoid()\n        \n    def forward(self, x):\n        x = self.maxpool(self.relu(self.conv1(x))) # input 156x156, output: 76x76\n        x = self.maxpool(self.relu(self.conv2(x))) # input 76x76, output: 36x36\n        x = self.maxpool(self.relu(self.conv3(x))) # input 36x36, output: 16x16\n        x = self.maxpool(self.relu(self.conv4(x))) # input 16x16, output: 6x6\n        x = x.view(-1, 6*6*18) # flatten\n        x = self.sigmoid(self.linear1(x))\n        x = self.sigmoid(self.linear2(x))\n        x = self.output(x)\n        return x","c92f19f8":"model = CNN()\nmodel = model.to(device)","e9193000":"loss_function = nn.CrossEntropyLoss()\noptimizer = torch.optim.AdamW(model.parameters())","10e9be41":"def trainer(epochs, trainDL, valDL, model, loss_function, optimizer):\n    for epoch in range(epochs):\n        for i, data in enumerate(trainDL):\n            model.train()\n            output = model(data['X'])\n            t_loss = loss_function(output, data['y'])\n            optimizer.zero_grad()\n            t_loss.backward()\n            optimizer.step()\n            \n            with torch.no_grad():\n                v_loss = 0\n                model.eval()\n                for j, data in enumerate(valDL):\n                    loss = loss_function(model(data['X']), data['y'])\n                    v_loss += loss.item()\n            print(f\"Epoch: {epoch+1}, Batch: {i+1}, Training loss: {str(round(t_loss.item(), 2))}, Validation loss: {str(round(v_loss\/j, 2))}\")","2b8fa4c8":"trainer(4, trainDL, valDL, model, loss_function, optimizer)","dc8d2e08":"def tester(testDL, model):\n    model.eval()\n    total = 0\n    correct = 0\n    for i, data in enumerate(testDL):\n        output = model(data['X'])\n        values, indices = torch.max(output.data, 1)\n        total += data['y'].size(0)\n        correct += (data['y'] == indices).sum().item()\n    print(f\"Accuracy: {str(round(correct\/total, 2))}\")","52cc75c2":"tester(testDL, model)","59390aee":"### Convolutional Neural Network","05cd108f":"### Prepare Dataset","853119b2":"### Test","00f934c5":"### DataLoaders","02be37f1":"### Train, Test, Val split","29d55804":"### Create Dataframe and labelling","872a61e4":"* Without Augmentation = 60%\n* With Augmentation = 61%\n* To Improve Accuracy: Hyperparameter Tuning (network architecture, learning rate etc.), Augmentation\n* NOTE: Augmentation doesn't gurantee better performance, it depends on the Model too.","c205bc27":"### Imports","e44abec6":"### Loss function and Optimizer","c1419afb":"### Train","270e70d4":"### Prepare labels","f56689fe":"### Augmentation","0173a12e":"### Get file paths"}}