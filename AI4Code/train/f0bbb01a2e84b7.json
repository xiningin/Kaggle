{"cell_type":{"c36357d8":"code","18fd59cc":"code","818e9907":"code","a9cc7395":"code","7c7f4f50":"code","a56d178a":"code","e7ebf95d":"code","b27ddba4":"code","29ddf752":"code","1c8fcd50":"code","5141bf54":"code","e4ddbc45":"code","12a41f6a":"code","cbe7d1ec":"code","ba4bd504":"code","8ea3673d":"code","cd2084dd":"markdown","566e2514":"markdown"},"source":{"c36357d8":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","18fd59cc":"cd '\/kaggle\/input\/dont-overfit-ii\/'","818e9907":"df = pd.read_csv('train.csv')\ny = df['target']\nx = df.drop(['target','id'],axis=1)\ndf.head(5)","a9cc7395":"from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.1)\nprint(x_train.shape,x_test.shape)","7c7f4f50":"# Let's first try with a very simple linear model.\nfrom keras import models, layers\nnet_input = layers.Input((300,))\noutput = layers.Dense(1,activation='sigmoid')(net_input)\nmodel = models.Model(net_input, output)\nmodel.compile(loss='binary_crossentropy',optimizer='adam',metrics=['acc'])\nhistory = model.fit(x_train, y_train, epochs=10,validation_data=(x_test,y_test))","a56d178a":"# Alright, it seems to work. However, it's overfitting extremely heavily. Let's do some feature analysis to see which\n# features affect the output the most.\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import f_regression\n\nbestfeatures = SelectKBest(score_func=f_regression, k=10)\nfit = bestfeatures.fit(x.values,y)\ndfscores = pd.DataFrame(fit.scores_)\ndfcolumns = pd.DataFrame(x.columns)\nfeatureScores = pd.concat([dfcolumns,dfscores],axis=1)\nfeatureScores.columns = ['Feature','Score']  #naming the dataframe columns\nprint(featureScores.nlargest(10,'Score'))  #print 10 best features","e7ebf95d":"import matplotlib.pyplot as plt\ntop10 = featureScores.nlargest(10,'Score')\nplt.bar(top10['Feature'],top10['Score'])","b27ddba4":"# Seems as if features 33 and 65 are the most important ones. Let's single those out:\nvalues = (33,65)\nx_train, x_test, y_train, y_test = train_test_split(x.values[:,tuple(values)],y,test_size=0.1)\nprint(x_train.shape,x_test.shape)\nlosses = []\nfor i in range(5):\n    net_input = layers.Input((len(values),))\n    output = layers.Dense(1,activation='sigmoid')(net_input)\n    model = models.Model(net_input, output)\n    model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['acc'])\n    history = model.fit(x_train, y_train, epochs=10,validation_data=(x_test,y_test),verbose=0)\n    losses.extend(history.history['val_loss'][-1:])\n    print(\"Attempt\",i+1,':',history.history['val_loss'][-1:])\nprint(\"Average loss: \",sum(losses)\/len(losses))\n\n# Try playing around with the values a bit - see which combination gives you the lowest loss.","29ddf752":"# It's pretty clear that only the five most important features help us.\n# Let's see if we can enhance our model.\nfrom keras import optimizers\nfrom keras import callbacks\nvalues = (33,65,217,117,91)\n\nx_train, y_train = x.values[:,tuple(values)],y\nopt = optimizers.Adam(lr=0.03)\nnet_input = layers.Input((len(values),))\noutput = layers.Dense(1,activation='sigmoid')(net_input)\nmodel = models.Model(net_input, output)\ncallback = callbacks.EarlyStopping(monitor='val_loss',\n                              min_delta=0,\n                              patience=2,\n                              verbose=0, mode='auto',\n                              restore_best_weights=True)\nmodel.compile(loss='binary_crossentropy',optimizer=opt,metrics=['acc'])\nhistory = model.fit(x_train, y_train, epochs=20,validation_split=0.1,callbacks=[callback])","1c8fcd50":"from keras import optimizers\nx_train, y_train = x.values[:,tuple(values)],y\nopt = optimizers.Adam(lr=0.03)\nnet_input = layers.Input((len(values),))\noutput = layers.Dense(1,activation='sigmoid')(net_input)\nmodel = models.Model(net_input, output)\ncallback = callbacks.EarlyStopping(monitor='val_loss',\n                              min_delta=0,\n                              patience=2,\n                              verbose=0, mode='auto',\n                              restore_best_weights=True)\nmodel.compile(loss='binary_crossentropy',optimizer=opt,metrics=['acc'])\nhistory = model.fit(x_train, y_train, epochs=8)","5141bf54":"x_val = pd.read_csv('test.csv')\nx_val.head()","e4ddbc45":"values = (33,65,217,117,91)\nx = x_val.drop(['id'],axis=1)\nids = x_val['id']\npredictions = model.predict(x.values[:,tuple(values)])","12a41f6a":"submission = ['id,target\\n']\nfor index,prediction in enumerate(np.around(predictions)[:,0].astype(np.int)):\n    txt = str(ids[index])+','+str(prediction)\n    if ids[index] != 19999: txt += '\\n'\n    submission.append(txt)","cbe7d1ec":"cd ..\/..\/working","ba4bd504":"with open('submission.csv','w+') as writer:\n    writer.writelines(submission)","8ea3673d":"submission = pd.read_csv('submission.csv')\nsubmission.head(10)","cd2084dd":"After some testing, it seems like 7-8 epochs is the optimal number.\nLet's see what results we can get with this model - time to train with all the data!","566e2514":"Aaaaand we're done!"}}