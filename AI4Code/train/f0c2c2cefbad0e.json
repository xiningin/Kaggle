{"cell_type":{"cba61f98":"code","a9a2b521":"code","180d5389":"code","124e4d14":"code","e9db3e17":"code","cb370892":"code","49a71687":"code","b9e8c7e9":"code","378aa802":"code","088d297f":"code","e9b5dedc":"code","fc1ef584":"code","9fb0f445":"code","1ff10c0a":"code","1b6170dc":"code","31801cfc":"code","826d4246":"code","4fba05be":"code","dc7aa8da":"code","51fbe4a1":"code","e8e622dd":"code","df0ed1bf":"code","192752cd":"code","cdf35eec":"code","3920b41e":"code","bf9123be":"code","6ee06391":"code","f7d8da7f":"code","b986d45c":"code","a88cc4b2":"code","9a035a7f":"code","ed790761":"code","08ae49b0":"code","c2e25285":"code","cadd1671":"code","21183230":"code","faf59462":"code","2d86f64b":"markdown","2b2156b6":"markdown","2310f9d2":"markdown","788413fc":"markdown","6c26a0ec":"markdown","53c24ea7":"markdown","8ea0e5e2":"markdown","98582a9c":"markdown","faa3f556":"markdown","37061915":"markdown","d64262e3":"markdown","a5e8f724":"markdown","9395f07e":"markdown","729a2a48":"markdown","4d641ef1":"markdown","f239fb5e":"markdown","6b6b7709":"markdown","b5f33c81":"markdown","2ba5fae2":"markdown"},"source":{"cba61f98":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","a9a2b521":"dataset = pd.read_csv('\/kaggle\/input\/stroke-prediction-dataset\/healthcare-dataset-stroke-data.csv')","180d5389":"dataset.head()","124e4d14":"dataset.shape","e9db3e17":"dataset.describe()\n\ndataset = dataset.drop(['id'],axis=1)","cb370892":"features_with_na = [features for features in dataset.columns if dataset[features].isnull().sum()>1]\n\nfor feature in features_with_na:\n  print(f\"{feature} has {100*(np.round(dataset[feature].isnull().mean(),4))} % missing values\")","49a71687":"correlation_with_features_with_na = dataset.corr()\nprint(correlation_with_features_with_na)","b9e8c7e9":"numerical_features = [features for features in dataset.columns if dataset[features].dtype != 'O']\n\nprint(f\"Numerical features {len(numerical_features)}\")\ndataset[numerical_features].head()","378aa802":"discrete_features = [features for features in numerical_features if dataset[features].dtype == 'int']\n\nprint(f'There are {len(discrete_features)} discrete variables present ')","088d297f":"continous_features = [features for features in numerical_features if dataset[features].dtype == 'float' ]\n\nprint(continous_features)\nprint(f'There are {len(continous_features)} features present')","e9b5dedc":"for features in continous_features:\n  data = dataset.copy()\n  data[features].hist(bins=25)\n  plt.xlabel(features)\n  plt.ylabel('Stroke')\n  plt.show()","fc1ef584":"for features in ['bmi','avg_glucose_level']:\n  data = dataset.copy()\n  data[features]=np.log(data[features])\n  data[features].hist(bins=25)\n  plt.xlabel(features)\n  plt.ylabel('Stroke')\n  plt.show()","9fb0f445":"for feature in continous_features:\n  data = dataset.copy()\n  if 0 in data[features].unique():\n    pass\n  else:\n    data[feature] = np.log(data[feature])\n    data.boxplot(column=feature)\n    plt.ylabel(feature)\n    plt.title(feature)\n    plt.show()\n","1ff10c0a":"categorical_features = [features for features in dataset.columns if data[features].dtypes == 'O']\nprint(categorical_features)","1b6170dc":"for features in categorical_features:\n  print(f\"Feature {features} number of categories {len(dataset[features].unique())}\")","31801cfc":"for features in categorical_features:\n  data = dataset.copy()\n  data.groupby(features)['stroke'].count().plot.bar()\n  plt.xlabel(features)\n  plt.ylabel('Stroke')\n  plt.title(features)\n  plt.show()\n\n","826d4246":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(dataset,dataset['stroke'],test_size = 0.1, random_state=0)","4fba05be":"features_nan = [feature for feature in dataset.columns if dataset[feature].isnull().sum()>1]\n\nprint(features_nan)","dc7aa8da":"for features in ['bmi']:\n  median_value = dataset[features].median()\n\n  #dataset[features+'nan']=np.where(dataset[features].isnull(),1,0)\n  dataset[features].fillna(median_value,inplace=True)\n\ndataset['bmi'].isnull().sum()","51fbe4a1":"median_value_train = X_train['bmi'].median()\nmedian_value_test = X_test['bmi'].median()\n\nX_train['bmi'].fillna(median_value_train, inplace=True)\nX_test['bmi'].fillna(median_value_test, inplace=True)\n","e8e622dd":"X_train.head()","df0ed1bf":"X_train = X_train.drop(['stroke'],axis=1)\nX_test = X_test.drop(['stroke'],axis=1)","192752cd":"for features in ['bmi','avg_glucose_level']:\n  X_train[features] = np.log(X_train[features])\n  X_test[features] = np.log(X_test[features])","cdf35eec":"X_train.shape","3920b41e":"categorical_variables_to = [feature for feature in dataset.columns if dataset[feature].dtype == 'O']","bf9123be":"label_X_train = X_train.copy()\nlabel_X_test = X_test.copy()","6ee06391":"from sklearn.preprocessing import LabelEncoder\n\nlabel_encoder= LabelEncoder()","f7d8da7f":"for cols in categorical_variables_to:\n  label_X_train[cols] = label_encoder.fit_transform(X_train[cols])\n  label_X_test[cols] = label_encoder.transform(X_test[cols])","b986d45c":"from sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder","a88cc4b2":"ct = ColumnTransformer(transformers = [('encoder', OneHotEncoder(), [0,4,5,6,9])], remainder='passthrough')\noh2_x_train = pd.DataFrame(ct.fit_transform(X_train))\noh2_x_test = pd.DataFrame(ct.transform(X_test))","9a035a7f":"oh2_x_train.head()","ed790761":"oh2_x_train.index = X_train.index\noh2_x_test.index = X_test.index","08ae49b0":"from sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report","c2e25285":"accur = []\npred_as_1 =[]\nfor i in range(2,51):\n    model = DecisionTreeClassifier(max_depth= i).fit(oh2_x_train, y_train)\n    pred = model.predict(oh2_x_test)\n    accur.append(accuracy_score(y_test, pred))\n    pred_as_1.append(confusion_matrix(y_test, pred)[1][1])","cadd1671":"fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(15,4))\nx=list(range(2,51))\naxes[0].plot(x,accur,'r')\naxes[1].plot(x,pred_as_1,'b')\nplt.show()\n","21183230":"model = DecisionTreeClassifier(max_depth= 30).fit(oh2_x_train, y_train)\npred = model.predict(oh2_x_test)\n\nconfusion_matrix(y_test, pred)","faf59462":"print(classification_report(y_test,pred))","2d86f64b":"Cardinality of categorical variables","2b2156b6":"### Missing values","2310f9d2":"# Exploratory Data Analysis","788413fc":"## One Hot encoder second","6c26a0ec":"### Handling skewed data","53c24ea7":"Finding outliers in the data","8ea0e5e2":"As we can see the correlation between Stroke and Bmi is very less. I can not use it but we'll see further.","98582a9c":"### Numerical Values","faa3f556":"We can one hot encode them all but before that we should find the relation between them and stroke","37061915":"### Continous variables","d64262e3":"Initial observations from the data and ToDo list:\n\n* 5110 Items 12 labes (Stroke-Dependent Variable)\n* We need to handle categorical variables\n* Check for imbalance data\n* Handle Null values\n* Scaling (Maybe-Maybe not)\n* Feature selection\n\n","a5e8f724":"# Feature Engineering","9395f07e":"### Categorical features","729a2a48":"### Creating a train test split before any preprocessing to prevent any data leakage","4d641ef1":"### Handling missing values","f239fb5e":"Performing Log transformation on bmi and avg_gucose_level","6b6b7709":"## Label encoder","b5f33c81":"# Modeling Decision tree","2ba5fae2":"Replacing missing values with median"}}