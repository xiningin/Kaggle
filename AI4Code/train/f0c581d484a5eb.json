{"cell_type":{"b93d6c0c":"code","d0d53cd0":"code","1f5f6f31":"code","fdb7f904":"code","61619f41":"code","144ede60":"code","242de224":"code","fe44fd69":"code","cc7b4eee":"code","4d323d83":"code","3892a84b":"code","59ec82ef":"code","b253bc36":"code","6653c65d":"code","77d8413a":"code","54f8d38e":"code","205c5f15":"code","ef441e56":"code","a6743d23":"code","13812bd9":"code","db22b655":"code","681efbe4":"code","ff6d918f":"code","4064187e":"code","8372a2fa":"code","edfc0bd2":"code","63faaa36":"code","116bbe56":"code","50d6281b":"code","9b74fe87":"markdown","358f9e4e":"markdown","246aed84":"markdown","c2276183":"markdown","2d552494":"markdown","0c5daf48":"markdown","431b430d":"markdown","d6483949":"markdown","bfb1b5ba":"markdown","05e8c6ec":"markdown","b04df429":"markdown","493989ce":"markdown","1658ad1e":"markdown","e2dfc59c":"markdown","6a976751":"markdown","0aeabbcc":"markdown","37e2c239":"markdown","a31a2b5e":"markdown","f0f804fa":"markdown","7f914d03":"markdown","c5cbd0a0":"markdown","80aef6a5":"markdown","05753e3f":"markdown","949b3e37":"markdown","be84c8dc":"markdown","4997d2ba":"markdown"},"source":{"b93d6c0c":"import numpy as np\nfrom scipy import stats\nimport itertools\nimport pandas as pd\npd.set_option(\"display.max_rows\", 10000)\n\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\nfrom matplotlib.colors import Normalize\nimport matplotlib.cm as cm\nimport seaborn as sns\n\nimport os\nimport fnmatch as fn\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom pprint import pprint","d0d53cd0":"file_path = '..\/input\/smhi-malm-hourly-temperatures-19312019'\nfiles = os.listdir(file_path)","1f5f6f31":"pprint(files)","fdb7f904":"a_file = os.path.join(file_path, files[0])\nb_file = os.path.join(file_path, files[1])\nc_file = os.path.join(file_path, files[2])\nd_file = os.path.join(file_path, files[3])","61619f41":"a_df = pd.read_csv(a_file, delimiter=';', usecols=[0,1,2,3], skiprows=9, parse_dates=True) #some rows above the actual headers must be skipped, as well as some columns\na_df.head()","144ede60":"b_df = pd.read_csv(b_file, delimiter=';', usecols=[0,1,2,3], skiprows=10, parse_dates=True)\nb_df.head()","242de224":"c_df = pd.read_csv(c_file, delimiter=';', usecols=[0,1,2,3], skiprows=9, parse_dates=True)\nc_df.head()","fe44fd69":"d_df = pd.read_csv(d_file, delimiter=';', usecols=[0,1,2,3], skiprows=10, parse_dates=True)\nd_df.head()","cc7b4eee":"#Function to rename column names in dataframe\ndef rename_columns_map(df): \n    col_name_map = dict(zip(df.columns.values, ['Date', 'Time (UTC)', 'Air temperature', 'Quality']))\n    return col_name_map\n\n#function to add date related fields (pandas.dt attributes)\ndef add_date_attributes(df, datefield):\n    '''Requires that datefield is a datetime object'''\n    fields_to_add = ['year', 'month_name', 'month', 'weekofyear']\n    obj = df[datefield].dt\n    for attr in dir(df[datefield].dt):\n        if attr in fields_to_add:\n            if not callable(getattr(obj, attr)):\n                df[str.capitalize(attr)] = getattr(obj, attr)\n            else:\n                df[str.capitalize(attr)] = getattr(obj, attr)()\n    return df.copy()","4d323d83":"df_list = [a_df, b_df, c_df, d_df]","3892a84b":"#Renaming column names\nfor df in df_list:\n    col_name_map = rename_columns_map(df)\n    if set(df.columns.values) != set(col_name_map.values()): #since inplace is used, first checks whether already renamed\n        df.rename(columns=col_name_map, inplace=True)","59ec82ef":"a_df.columns, b_df.columns, c_df.columns, d_df.columns","b253bc36":"#Adding 'Datetime' from 'Date' and 'Time'\nfor df in df_list:\n    if 'Datetime' not in df.columns.values: \n        df['Datetime'] = df['Date'] + ' ' + df['Time (UTC)']\n        df.drop(columns='Time (UTC)', inplace=True)\n        df['Datetime'] = pd.to_datetime(df['Datetime']) \n        df['Date'] = pd.to_datetime(df['Date'])","6653c65d":"#Column dtype check\nfor df in df_list:\n    print(df.dtypes, '\\n--')","77d8413a":"#Final concatenated dataframe\ncomplete_df_total = pd.concat(df_list, ignore_index=True)    \ncomplete_df = complete_df_total.groupby(['Date', 'Datetime']).mean()\ncomplete_df = complete_df.reset_index()\n\n#Checking if we have unique samples\nprint(f'Unique rows: {complete_df.Datetime.nunique()}; Total rows: {complete_df.shape[0]}')","54f8d38e":"#Adding date related files\ncomplete_df = add_date_attributes(complete_df, 'Datetime')\ncomplete_df.tail(15)","205c5f15":"def time_plot_per_year(df, aggregation_type='average', rolling_window=5):\n    \n    ''' Plot both the actual aggregated and a smoothed rolling average of the values in [df], \n        with window according to [rolling_window] given as an integer; by default 5. \n        [aggregation_type] is related to the aggretation function applied to [df] \n        and affects the titles of the plot; by default 'average'. '''\n    \n    max_val = df.max()\n    min_val = df.min()\n    max_year = df.idxmax()\n    min_year = df.idxmin()\n\n    min_max_dict = {max_year: max_val, min_year: min_val}\n\n    fig, axes = plt.subplots(2,1, figsize=(12,10), dpi=200, sharex=True);\n    fig.suptitle(f'{aggregation_type.lower().capitalize()} Malm\u00f6 temperatures per year', fontsize=19, y=0.95);\n    fig.subplots_adjust(hspace=0.125);\n\n    for ax in axes:\n        ax.set_xticks(np.arange(df.index[0]-1, df.index[-1]+2, 5));\n        ax.set_ylabel('Air temperature [$^{\\circ}$C]');\n\n    #First plot\n    df.plot(ax=axes[0], title=f'Actual {aggregation_type.lower()}', linewidth=2);\n    axes[0].axhline(max_val, linestyle= '--', color='r', lw=0.8);\n    axes[0].axhline(min_val, linestyle= '--', color='r', lw=0.8);\n    axes[0].xaxis.set_ticks_position('none');\n\n    for i, (year, value) in enumerate(min_max_dict.items()):\n        if i == 0:\n            xy, xytext, position = (1,1), (5, -10), 'Max'\n        else:\n            xy, xytext, position = (1,0), (5, 15), 'Min'   \n        axes[0].annotate(xy=xy, text=f'{position}: {year}; {round(value,2)}' + '$^{\\circ}$C', \n                         xytext=xytext, va='top', xycoords='axes fraction', \n                         textcoords='offset points', alpha=0.8,\n                         bbox=dict(boxstyle=\"round,pad=0.1\", fc=\"white\", ec=\"k\", lw=0.8));\n\n    #Second plot\n    avg_per_year_rolling = df.rolling(rolling_window).mean()\n\n    avg_per_year_rolling.plot(ax=axes[1], title=f'Rolling {df.index.name.lower()} average: {rolling_window}', linewidth=2);\n    axes[1].grid(True, axis='y', linestyle='--');","ef441e56":"avg_per_year = complete_df.groupby('Year')['Air temperature'].mean()\ntime_plot_per_year(avg_per_year)","a6743d23":"max_per_year = complete_df.groupby('Year')['Air temperature'].max()\ntime_plot_per_year(max_per_year, aggregation_type='maximum')","13812bd9":"min_per_year = complete_df.groupby('Year')['Air temperature'].min()\ntime_plot_per_year(min_per_year, aggregation_type='minimum')","db22b655":"def plot_top10_highest_and_lowest(df_highest, df_lowest, df_average):\n    \n    ''' Plot ordered and stylized bar charts of aggregated dataframes [df_highest], [df_lowest], [df_average]. '''\n\n    fig, axes = plt.subplots(2,2, figsize=(15, 12), dpi=200)\n    fig.suptitle('Highest and lowest temperatures', fontsize=19, y=0.95);\n    fig.subplots_adjust(hspace=0.25, wspace=0.25);\n\n    top_10_highest = df_highest.sort_values(ascending=False)[:10].sort_values()\n    top_10_lowest = df_lowest.sort_values(ascending=True)[:10]\n    top_10_highest_avg = df_average.sort_values(ascending=False)[:10].sort_values()\n    top_10_lowest_avg = df_average.sort_values(ascending=True)[:10]\n    \n    frames = [top_10_highest, top_10_lowest, top_10_highest_avg, top_10_lowest_avg]\n    cmaps = [cm.get_cmap('YlOrBr'), cm.get_cmap('Blues_r'), cm.get_cmap('pink_r'), cm.get_cmap('bone')]\n    titles = ['Top 10 highest recorded', 'Top 10 lowest recorded', 'Top 10 highest monthly averages', 'Top 10 lowest monthly averages']\n    \n    for i,j in enumerate(itertools.product([0,1],[0,1])): # (0,0) (0,1) (1,0) (1,1)\n        x, y = j[0], j[1] #subplotgrid indices\n        ax = axes[x][y]\n        ax.set_xlabel('Air temperature [$^{\\circ}$C]');\n        ax.set_title(titles[i])\n        \n        #creating customized color gradients for bar charts\n        norm = Normalize(vmin=frames[i].min(), vmax=frames[i].max()+0.1)\n        cmap = cmaps[i]\n        gradient_color = cmap(norm(frames[i].values))\n        \n        frames[i].plot(kind='barh', ax=ax, color=gradient_color, alpha=0.75, linewidth=0.5, edgecolor='gray');\n        \n        #annotating bars with values and adding some style\n        for index, value in enumerate(frames[i].values):\n            if y == 1: #right side plots\n                text_color = (0, 0, 0) if index != 0 else (1, 1, 1)\n                text_style = 'oblique' if index == 0 else 'normal'\n            else: #left side plots\n                text_color = (0, 0, 0) if index != 9 else (1, 1, 1)\n                text_style = 'oblique' if index == 9 else 'normal'\n            shift = -value*0.1\n            ax.annotate(xy=(value, index), text=str(round(value,2)), xytext=(value+shift,index-0.1), color=text_color, fontstyle=text_style)\n\n            ","681efbe4":"max_per_year_month = complete_df.groupby(['Year', 'Month'])['Air temperature'].max()\nmin_per_year_month = complete_df.groupby(['Year', 'Month'])['Air temperature'].min()\navg_per_year_month = complete_df.groupby(['Year', 'Month'])['Air temperature'].mean()\nplot_top10_highest_and_lowest(max_per_year_month, min_per_year_month, avg_per_year_month)","ff6d918f":"avg_per_month = complete_df.groupby('Month_name')['Air temperature'].mean().sort_values()\nstd_per_month = complete_df.groupby('Month_name')['Air temperature'].std()\n\nfig, ax = plt.subplots(1,1, figsize=(8, 6), dpi=100)\n\nnorm = Normalize(vmin=avg_per_month.min(), vmax=avg_per_month.max()+0.1)\ncmap = cm.get_cmap('Spectral_r')\ngradient_color = cmap(norm(avg_per_month.values))\n        \navg_per_month.plot(kind='barh', ax=ax,  title='Average temperatures per month', color=gradient_color, alpha=0.75,\n                  linewidth=0.5, edgecolor='black', xerr=std_per_month, capsize=3);\n\nax.set_xlabel('Air temperature [$^{\\circ}$C]');\nax.set_xticks(range(-4, 24, 1), minor=True);\nfor a, m in dict(zip([0.25, 0.7], ['minor', 'major'])).items():\n    ax.grid(True, axis='x', linestyle=':', alpha=a, which=m);\n","4064187e":"def plot_hist_prob(frames: [], hist_titles: [], main_title: str, main_title_yloc=0.95):\n    \n    ''' Plot histograms and norm.dist quantile plots of aggregated dataframes [frames]. Histogram titles are given in [hist_titles],\n        whereas the main title of the figure in [main_title]. Optionally adjust the vertical distance of the main title\n        with [main_title_yloc].'''\n    \n    fig, axes = plt.subplots(len(frames),2, figsize=(8, len(frames)*4), dpi=150);\n    fig.suptitle(main_title, fontsize=19, y=main_title_yloc);\n    fig.subplots_adjust(hspace=0.3, wspace=0.25);\n    \n    for i, frame in enumerate(frames):\n        mean_val = round(frame.mean(), 2)\n        std_val = round(frame.std(), 2)\n        \n        #Histogram\n        sns.distplot(frame, ax=axes[i][0]);\n        \n        axes[i][0].set_title(hist_titles[i]);\n        \n        min_xtick, max_ytick = min(axes[i][0].get_xticks()), max(axes[i][0].get_yticks())\n        x_shift = 1.1 if min_xtick > 15 else 1.35 if min_xtick > 0 else 0.75\n        y_shift = 0.85 \n        \n        axes[i][0].annotate('$\\mu$: ' + str(mean_val), xy=(0, 1), xytext=(12, -12), va='top',\n             xycoords='axes fraction', textcoords='offset points');\n        axes[i][0].annotate('$\\sigma$: ' + str(std_val), xy=(0, 1), xytext=(12, -22), va='top',\n             xycoords='axes fraction', textcoords='offset points')\n        axes[i][0].set_xlabel('Air temperature [$^{\\circ}$C]');\n\n        #Quantile plot\n        stats.probplot(frame, sparams=(mean_val, std_val), plot=axes[i][1], rvalue=True);","8372a2fa":"hist_titles = ['Maximum yearly temperatures', 'Minimum yearly temperatures', 'Average yearly temperatures']\nmain_title='Temperature distributions: 1931-2019'\nplot_hist_prob([max_per_year, min_per_year, avg_per_year], hist_titles, main_title)","edfc0bd2":"previous = complete_df[complete_df.Year < 1990]\nlast_three_decades = complete_df[(complete_df.Year >= 1990) & (complete_df.Year < 2019)]\nprint(previous.shape, last_three_decades.shape, complete_df.shape)","63faaa36":"previous_max_per_year = previous.groupby('Year')['Air temperature'].max()\nlatest_max_per_year = last_three_decades.groupby('Year')['Air temperature'].max()\n\nhist_titles = ['Maximum yearly temperatures \\'90-\\'18', 'Maximum yearly temperatures \\'31-\\'89']\nmain_title = 'Temperature distributions: 1941-1989 vs 1990-2018'\n\nplot_hist_prob([latest_max_per_year, previous_max_per_year], hist_titles, main_title, 0.97)","116bbe56":"alpha=0.1\n\nbartlett_stat, p_value_b = stats.bartlett(latest_max_per_year, previous_max_per_year)\nlevene_stat, p_value_l = stats.levene(latest_max_per_year, previous_max_per_year)\n\nprint(f'Bartlett statistic: {bartlett_stat}; Bartlet p: {p_value_b};\\nLevene stat: {levene_stat}; p: {p_value_l}; \\nConfidence level: {alpha}')","50d6281b":"#stats.ttest_ind performs a two-sided test, but this can be intepreted as a one tailed 'greater-than test', when p\/2 < alpha and t > 0\nt_stat, p_value = stats.ttest_ind(a=latest_max_per_year, b=previous_max_per_year, equal_var=False)\nprint(f'Test statistic: {t_stat};\\np\/2: {p_value\/2};\\nConfidence level: {alpha}')","9b74fe87":"* When looking at the \"actuals\", the pattern is quite volatile, but with the smoothed plots we can sense more easily the trends. That is especially the case for the maximum records, where it seems to have stayed on a relatively stable level since around 1990-1995. If we were to smooth these values even further, this would become even clearer (although we'd lose the initial years). ","358f9e4e":"## Final words","246aed84":"### Importing data","c2276183":"## Trend over time","2d552494":"* The goodness-of-fit is now not as high as before. We have some leaps and outliers in the years prior to 1990. Still, we can say that these distributions are fairly normal. Let us therefore continue and put some hypotheses to trial.","0c5daf48":"Both tests show that the null hypothesis of equal variances can be confidently rejected. Therefore we will use the Welch's version of the t-test (not assuming equal population variances).","431b430d":"If you made it to this point, I thank you for the patience. As this is my first publicated notebook, I encourage you to provide feedback on the structure, the code, the visualizations, the analytical approach, what could be improved on the existing content and so on. If you have any ideas on how to make more of the analysis, either diving deeper into the historical data and continue the descriptive approach, or formulating a predictive approach perhaps incorporating machine learning - I'm all ears.","d6483949":"Finally, let's carry out some simple statistical analysis on the data. First step here is to assess the temperature distributions. We will ultimately do a statistical test on maximum temperatures recorded during the last 30 years compared to the years before that. This analysis could naturally be continued to consider both average and minimum temperatures.","bfb1b5ba":"## Introduction\nGlobal warming has for a long time been a very, excuse the pun, hot debate. June of 2019 was the [warmest June ever recorded on a global scale](https:\/\/www.nytimes.com\/2019\/07\/03\/climate\/hottest-june-on-record.html), a fact that was destined to add fuel to the discussion and, maybe more importantly, an increased interest in the topic among all people. In this vein, although from a more local perspective, this notebook will be looking at air temperature data collected in Malm\u00f6, Sweden from 1931 until 2019. We visualize trends, look at records, and make a simple statistical test on differences in temperatures between the last three decades and the time prior to that. \n\nMeteorological data for Malm\u00f6 and other cities in Sweden is openly available for download via [www.smhi.se\/data\/meteorologi](http:\/\/www.smhi.se\/data\/meteorologi\/ladda-ner-meteorologiska-observationer#param=airtemperatureInstant,stations=all,stationid=53360).","05e8c6ec":"**We have four files with hourly temperature data generated by three SMHI stations (depicted above) in the city of Malm\u00f6**:\n- Malm\u00f6 2: From 1931-01-01 to 1964-04-30\n- Malm\u00f6: From 1941-01-01 to 1989-12-31\n- Malm\u00f6 A: From 1990-01-01 to 2019-03-01\n- Malm\u00f6 A: From 2019-02-19 to 2019-06-29 (date of download)\n\n*Note: as will be seen below, all rows do not include data for every hour of the day.*","b04df429":"We can see that out of the 10 warmest records, 7 come from the last 30 years. Inversely, the 10 coldest records  to a larger extent belong to previous years and decades. From the above charts we also get an idea of which months in *general* are the hottest and coldest respectively, but let's confirm it nonetheless. Here we will skip the creating-a-function part, since the code is relatively short.","493989ce":"Fun fact: I was born in the coldest month of the year. Other (not always so fun) fun fact is that temperatures in Malm\u00f6 do generally not get very high.","1658ad1e":"Now we'll study the highest and lowest temperature recorded and when (year and month) these where recorded. We will also see which periods have had the highest and lowest *average*.","e2dfc59c":"**Another function to help us out:**","6a976751":"Having prepared our data to a workable form, let's have a look at how the temperatures have developed over time. We'll consider both the maximum, minimum and average values per time unit.","0aeabbcc":"**Defining some functions to help us out:**","37e2c239":"Our test suggests that the average maximum temperatures in Malm\u00f6 have increased during the last three decades, compared to the previous years.","a31a2b5e":"The files have Swedish descriptions, which we'll of course change. We will also concatenate the three datasets into one and then add some relevant date related fields (the latter could be done either prior or after concatenation). \n\nOne thing to note here is that we have an overlap in timestamps. We could either remove duplicates after concatenation, or take the average temperature of two identical timestamps. Some rows are flagged with \"Y\" (`Kvalitet`), meaning that the data may not be exact (dubious) or may be a result of underlying aggregations. Because of this we will not remove duplicates, but take the average of values grouped by timestamps, as precision is to some degree already lost beforehand.","f0f804fa":"## Temperature distribution","7f914d03":"![](https:\/\/storage.googleapis.com\/kagglesdsdata\/datasets\/254523\/534759\/SHMI_stations.jpg?GoogleAccessId=web-data@kaggle-161607.iam.gserviceaccount.com&Expires=1562511712&Signature=czPhcF6b9gaH2VvmVKtLsbZ5MG%2Bx4eA78GlqdjimIo1fCjcnnQYb1ZC3IK5IQGAEfJeXKfMeOofDK3ha4gVZfUpujRlzrWY3LsRihkXwv9ZHDeRd%2BmgIbwmSydsGCE%2Bo%2ByIEWIwGEk%2BLGYSu7bpfI%2BskXIhLDgl0yhReFz2bEZLl%2BUrs5H0iUMtUJuGyrHc13nJdOIBXw5NC2t96aIsMX736X0znsB%2BCS2TkpIByIlkdYSo9D0NyYKgirTVJtW0gP4s9PeAGGnvCysaHB85yD%2Fr%2BRTO%2B2Px1TE4HzzMoRhA4oXi7UDFU9%2FnIK9ny24xakeJnkV7WyrO7s%2ByLZCoG5w%3D%3D)","c5cbd0a0":"## Highest and lowest temperatures recorded","80aef6a5":"To no big surprise, we have more rows on later dates, because for these dates more frequent collections of data have been made.","05753e3f":"If we look at the complete dataset, the goodness-of-fit, determined by [$R^2$](https:\/\/en.wikipedia.org\/wiki\/Coefficient_of_determination), is fairly high, for all three aggregation functions. This is visually confirmed by the histogram and the superimposed probability distribution function. Let us now look at the last three decades compared to everything we have prior to that. Since 2019 is not a full year, we will truncate it for this part.","949b3e37":"### Data cleaning and transformation","be84c8dc":"**Is the maximum air temperature since 1990 significantly higher than the previous period?**\n\n$H_0$: ($\\mu_{max, last}$) $\\leq$ ($\\mu_{max, prev}$) \n\n$H_1$: ($\\mu_{max, last}$) $\\gt$ ($\\mu_{max, prev}$) \n\n$\\alpha$ = 0.10\n\nThe alpha value here is chosen slightly higher than the \"standard\" 0.05, as per the recommendations of H.C.S Thom and D. Thom (*Tests of Significance for Temperature and Precipitation Normals*, [Monthly Weather Review, June 1972, Vol. 100, No. 6](https:\/\/journals.ametsoc.org\/toc\/mwre\/100\/6)).\n\nSince our samples show *fairly* good fit to the normal distribution, it is reasonable to carry out an independent t-test. We can also do an F-test, a [Bartlett](https:\/\/en.wikipedia.org\/wiki\/Bartlett%27s_test) or a [Levene](https:\/\/en.wikipedia.org\/wiki\/Levene%27s_test) test, to get an idea of whether we can assume equal population variances or not. Again, normality is a *reasonable* assumption here, which means we could use the F-test with *some* confidence, but we will go with the more insensitive Bartlett and Levene test (the F-test is [notoriously sensitive](https:\/\/en.wikipedia.org\/wiki\/F-test_of_equality_of_variances) to the normality assumption). We'll do both the latter, just to be certain.","4997d2ba":"# Malm\u00f6 temperatures from 1931 to 2019\n\n![](https:\/\/storage.googleapis.com\/kagglesdsdata\/datasets\/254523\/534759\/malmo_city.jpg?GoogleAccessId=web-data@kaggle-161607.iam.gserviceaccount.com&Expires=1562511635&Signature=CNkfokd4bcqvDoesB4dqpZKTLw11FJyXvUsGO2KLPku2iMo2dJhTZaMXmFl2nUFk8MaTUnyJIDLqxF%2FiNU7%2FkTo20z7ujm%2FezpdW3FO0xFzjqiJsi%2FUFnQeBO6THYg6FfbJoQ3HY4JM3lTcCKn1%2BbVbSMa92KcluT0Ssn5LCJXTlIbo8WdtRuAwFBu0PcgF9MvUlpundm6wiLwC47n6IccYV804SkTbUthQt3DPEMsMQf03CVtlxxAYwxh5kod9AgKcXY0Cb5yLVeZ%2BwsffjTpXxcHVY6973NKWdaRe5tpU7AEbi9%2FaHyGBxPvaTh43DfPsDuKeH%2F%2B2HlfBkLEHF7A%3D%3D)<img width=\"500\" align=\"left\">"}}