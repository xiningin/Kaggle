{"cell_type":{"bbda296e":"code","1b92b64e":"code","880b6b98":"code","6736f13a":"code","b5f94231":"code","3a3ff515":"code","06cf28bb":"code","5475404a":"code","22e4a54a":"code","77faad98":"code","aebdc6d4":"markdown","5c1ccd6b":"markdown","4404f1bc":"markdown"},"source":{"bbda296e":"import matplotlib.pylab as plt\nimport numpy as np\nimport pandas as pd\nfrom keras import models, layers, optimizers\nfrom keras import applications\nfrom keras.preprocessing.image import ImageDataGenerator #\u56fe\u50cf\u8f6c\u6210\u6570\u636e\nimport os\n\nmonkey_species = os.listdir('\/kaggle\/input\/training\/training')\nprint(\"Number of Categories:\", len(monkey_species))\nprint(\"Categories: \", monkey_species)\n\nimg_width, img_height = 224, 224 \n\ntrain_data_dir = '\/kaggle\/input\/training\/training'\nvalidation_data_dir = '\/kaggle\/input\/validation\/validation'\nbatch_size = 32\n\ntrain_datagen = ImageDataGenerator(\n    rotation_range = 30, \n    rescale=1. \/ 255, #\u89c4\u8303\u5316\n    shear_range=0.2,\n    zoom_range=0.2,\n    horizontal_flip=True)\n\ntest_datagen = ImageDataGenerator(rescale=1. \/ 255)\n\ntrain_generator = train_datagen.flow_from_directory(\n    train_data_dir,\n    target_size=(img_height, img_width), #\u56fe\u7247\u6309\u6bd4\u4f8b\u538b\u7f29\u81f3\u60f3\u8981\u7684size\n    batch_size=32, #\u4e00\u4e2abatch 32\u5f20\u7167\u7247 \u9ed8\u8ba4\u968f\u673a\n    class_mode='categorical') #onehot \n\nvalidation_generator = test_datagen.flow_from_directory(\n    validation_data_dir,\n    target_size=(img_height, img_width),\n    batch_size=32,\n    class_mode='categorical')\n","1b92b64e":"x_train=train_generator.next()[0]\nx_train.shape","880b6b98":"y_train=train_generator.next()[1]\ny_train.shape","6736f13a":"import pandas as pd\nfrom plotly.offline import init_notebook_mode, iplot\nimport plotly.graph_objs as graph\ninit_notebook_mode(connected=True)","b5f94231":"training_data = pd.DataFrame(train_generator.classes, columns=['classes'])\nvalidation_data = pd.DataFrame(validation_generator.classes, columns=['classes'])","3a3ff515":"def create_stack_bar_data(col, df):  #\u628apadas\u7684DataFrame\u53d8\u6210\u77e9\u9635\n    aggregated = df[col].value_counts().sort_index()\n    x_values = aggregated.index.tolist()\n    y_values = aggregated.values.tolist()\n    return x_values, y_values\nx1, y1 = create_stack_bar_data('classes', training_data)","06cf28bb":"x1 = list(train_generator.class_indices.keys())","5475404a":"trace1 = graph.Bar(x=x1, y=y1, opacity=0.75, name=\"Class Count\")\nlayout = dict(height=400, width=1200, title='Class Distribution in Training Data', legend=dict(orientation=\"h\"), \n                yaxis = dict(title = 'Class Count'))\nfig = graph.Figure(data=[trace1], layout=layout);\niplot(fig);","22e4a54a":"x1, y1 = create_stack_bar_data('classes', validation_data)\nx1 = list(validation_generator.class_indices.keys())\n\ntrace1 = graph.Bar(x=x1, y=y1, opacity=0.75, name=\"Class Count\")\nlayout = dict(height=400, width=1100, title='Class Distribution in Validation Data', legend=dict(orientation=\"h\"), \n                yaxis = dict(title = 'Class Count'))\nfig = graph.Figure(data=[trace1], layout=layout);\niplot(fig);","77faad98":"import tensorflow as tf  \nimport pandas as pd   \nimport numpy as np\n\nfrom keras.models import Sequential,load_model,Model\nfrom keras.layers import Dense,Activation,Conv2D,MaxPooling2D,Flatten,Dropout,ZeroPadding2D,BatchNormalization,GlobalMaxPooling2D\n\nfrom keras.utils import np_utils\nimport keras\n\nn_filters = 64    #\u5377\u79ef\u6ee4\u955c\u4e2a\u6570\npool_size = (2,2) #\u8bbe\u7f6e\u6c60\u5316\u6838\u5927\u5c0f\n\ncnn_net = Sequential()\n#\u8f93\u5165\u5c42\u548c\u7b2c\u4e00\u5c42\u9690\u5c42\ncnn_net.add(Conv2D(n_filters,kernel_size=(3,3),strides=(1,1),input_shape=(None,None,3))) \t#\u5377\u79ef\ncnn_net.add(Activation(\"relu\")) \t#\u6fc0\u6d3b\ncnn_net.add(BatchNormalization()) \t\t#\u89c4\u8303\ncnn_net.add(MaxPooling2D(pool_size=pool_size))          #\u6c60\u5316\n#\u7b2c\u4e8c\u9690\u5c42\t\ncnn_net.add(ZeroPadding2D((1,1))) #ZeroPadding\ncnn_net.add(Conv2D(64,kernel_size=(3,3))) #\u5377\u79ef\ncnn_net.add(Activation(\"relu\")) #\u6fc0\u6d3b\ncnn_net.add(BatchNormalization()) #\u89c4\u8303\ncnn_net.add(MaxPooling2D(pool_size=pool_size))       #\u6c60\u5316\n\ncnn_net.add(ZeroPadding2D((1,1))) #ZeroPadding\ncnn_net.add(Conv2D(32,kernel_size=(3,3))) #\u5377\u79ef\ncnn_net.add(Activation(\"relu\")) #\u6fc0\u6d3b\ncnn_net.add(BatchNormalization()) #\u89c4\u8303\ncnn_net.add(MaxPooling2D(pool_size=pool_size))       #\u6c60\u5316\n\ncnn_net.add(ZeroPadding2D((1,1))) #ZeroPadding\ncnn_net.add(Conv2D(32,kernel_size=(3,3))) #\u5377\u79ef\ncnn_net.add(Activation(\"relu\")) #\u6fc0\u6d3b\ncnn_net.add(BatchNormalization()) #\u89c4\u8303\ncnn_net.add(MaxPooling2D(pool_size=pool_size))       #\u6c60\u5316\n\n\n#\u7b2cn\u9690\u5c42\t\ncnn_net.add(Conv2D(64,kernel_size=(3,3)))\t#\u5377\u79ef\ncnn_net.add(Activation(\"relu\")) \t#\u6fc0\u6d3b\ncnn_net.add(BatchNormalization()) #\u89c4\u8303\ncnn_net.add(GlobalMaxPooling2D())       #\u6c60\u5316\n    #\u82e5\u4e0a\u5c42\u6539\u4e3acnn_net.add(MaxPooling2D(pool_size=pool_size)) \u5219\u62a5\u9519\uff0c\u9700\u8981Global,\n    #\u82e5\u56fe\u7247\u662f\u5355chanel\u5219\u6ca1\u95ee\u9898\ncnn_net.add(Dropout(0.25))\n\ncnn_net.add(Dense(3168))\ncnn_net.add(Activation(\"relu\"))\ncnn_net.add(Dense(512))\ncnn_net.add(Activation(\"relu\"))\n\ncnn_net.add(Dense(10))\ncnn_net.add(Activation(\"softmax\"))\n\ncnn_net.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n\n#cnn_net.summary()\n\n\ntrained = cnn_net.fit_generator(\n        train_generator,\n        steps_per_epoch=1097\/\/32, #\u4e00\u4e2aepoch34\u6b21\u8fed\u4ee3 batch_size=32\n        epochs=100,\n        validation_data=validation_generator,\n        validation_steps=272 \/\/ 32\n)","aebdc6d4":"3 \u5efa\u7acb\u6a21\u578b","5c1ccd6b":"1 \u5bfc\u5165\u6570\u636e","4404f1bc":"2 \u6570\u636e\u53ef\u89c6\u5316"}}