{"cell_type":{"2f2200f8":"code","80442376":"code","e3d78073":"code","1515af80":"code","e94490cb":"code","d8e33d93":"code","ff930778":"code","4ae616a2":"code","5316d0b4":"code","3d1ab6f3":"code","5ebc7e86":"code","7ef3070a":"code","a45a1eef":"code","fe06dc1d":"code","6f9ec497":"markdown","4b08b7a3":"markdown","808c847c":"markdown","75cffa6d":"markdown","7102c1d3":"markdown","0fd6ab03":"markdown","1c961259":"markdown","8c2a7c9c":"markdown","f9a50a1b":"markdown","3351fa57":"markdown","76fa2e1d":"markdown","df9c3bb6":"markdown","fc609e39":"markdown","5ce2b34b":"markdown","bbd33fd2":"markdown"},"source":{"2f2200f8":"import pandas as pd\nimport numpy as np\ndata = pd.read_csv('..\/input\/pokemon\/Pokemon.csv')\ndata.columns\ndata.describe()","80442376":"data.corr()","e3d78073":"y = data.HP\nX_full = data\nX_sample = X_full.drop(['HP'], axis=1)\n# X_sample['Type 2'].fillna(value='None',inplace=True) #enable if no combine\nX_sample.head()","1515af80":"### Legendary\nX_sample.Legendary = X_sample.Legendary.astype(int)\n\n### Pokemon Type\n\n# Get all unique types from column \"type 1\"\ntypes = np.unique(data['Type 1'].values)\n\n# Prepare columns for one-hot (combine)\n# for t in types:\n#     X_sample.insert(12, f'Type_{t}', 0)\n    \n# # One hot type 1 & type 2\n# for i, r in data.iterrows():\n#     type_1 = r['Type 1']\n#     X_sample.at[i, f'Type_{type_1}'] = 1\n    \n#     type_2 = r['Type 2']\n       \n#     if not type_2 == 'nan':\n#         X_sample.at[i, f'Type_{type_2}'] = 1\n        \nX_sample.columns","e94490cb":"# X_sample = X_sample.drop(['Name', '#', 'Generation', 'Total', 'Type 1', 'Type 2', 'Type_nan'], axis=1)\nX_sample = X_sample.drop(['Name', '#', 'Generation', 'Total'], axis=1)\nX_sample.head()","d8e33d93":"from sklearn.model_selection import train_test_split\n\n# One Hot Using pd.get_dummies()\nX_sample = pd.get_dummies(X_sample)\nprint(X_sample.columns)\n\nX_train, X_valid, y_train, y_valid = train_test_split(X_sample, y, train_size=0.8, test_size=0.2, random_state=0)\nX_full_train, X_full_valid, y_full_train, y_full_valid = train_test_split(X_full, y, train_size=0.8, test_size=0.2, random_state=0)","ff930778":"# import math\n# boolean_cols = [cname for cname in X_train.columns if X_train[cname].dtype == 'bool']\nnumerical_cols = [cname for cname in X_train.columns if X_train[cname].dtype in ['int64', 'float64'] \n                  and 'Type' not in cname ]\nonehot_cols = [cname for cname in X_train.columns if 'Type' in cname ]\ncategorical_cols = [cname for cname in X_train.columns if X_train[cname].dtype == 'object']\ncols_with_missing = [cols for cols in numerical_cols if X_train[cols].isnull().any()]\n\n# print(\"Boolean columns: \", boolean_cols)\nprint(\"Numerical columns: \", numerical_cols)\nprint(\"OneHot columns: \", onehot_cols)\nprint(\"Categorical columns: \", categorical_cols)\nprint(\"Columns with missing values: \", cols_with_missing)","4ae616a2":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.compose import ColumnTransformer\n# from sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import mean_absolute_error\n\n# Preprocessor\noh_transformer = SimpleImputer(strategy='constant', fill_value=0)\nnum_transformer = SimpleImputer(strategy='mean')\ncat_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='constant', fill_value=None)),\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n])\npreprocessor = ColumnTransformer(\ntransformers=[\n    ('oh', oh_transformer, onehot_cols),\n    ('num', num_transformer, numerical_cols),\n    ('cat', cat_transformer, categorical_cols)\n])\n\n# Model\nrf_model = RandomForestRegressor(n_estimators=250, criterion='mae', random_state=0, n_jobs=-1)\n\n# Pipeline\n# rf_pipeline = Pipeline(steps=[('model', rf_model)])\nrf_pipeline = Pipeline(steps=[('preprocessor', preprocessor), ('model', rf_model)])\n\n# Fit model\nrf_pipeline.fit(X_train, y_train)\n\n# Predict\nrf_preds = rf_pipeline.predict(X_valid)","5316d0b4":"# Result\nprint(\"Model#1: RandomForest - Performance Summary\\n----------\")\n\nrf_score = mean_absolute_error(y_valid, rf_preds)\nprint('MAE score (80\/20 train-test split) :', rf_score)\n\nfrom sklearn.model_selection import cross_val_score\nrf_scores = -1 * cross_val_score(rf_pipeline, X_sample, y, cv=5, scoring='neg_mean_absolute_error')\n# print(\"MAE scores (5CVs): \", rf_scores)\nprint(\"MAE score (Cross Validation)       :\", rf_scores.mean())\nprint(\"Overall MAE =\", round(rf_scores.mean(), 2))","3d1ab6f3":"rf_importances = pd.DataFrame({'feature': X_valid.columns, 'rf_importance': np.round(rf_model.feature_importances_,3)})\nrf_importances = rf_importances.sort_values('rf_importance', ascending=False).set_index('feature')\nrf_importances[:10]","5ebc7e86":"from xgboost import XGBRegressor\n# from sklearn.impute import SimpleImputer\n# from sklearn.compose import ColumnTransformer\n# from sklearn.preprocessing import OneHotEncoder\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import mean_absolute_error\n\n# Temporarily remove preprocessor because it throws error otherwise\n\n# Model\nxgb_model = XGBRegressor(\n            n_estimators=100,\n            learning_rate=0.05, n_jobs=2, verbosity=0, random_state=0)\n\nxgb_pipeline = Pipeline(steps=[('model', xgb_model)])\n# xgb_pipeline = Pipeline(steps=[('preprocessor', preprocessor), ('model', xgb_model)])\n\n# Fit model\nxgb_pipeline.fit(X_train, y_train, \n                 model__early_stopping_rounds=3,\n                 model__eval_set=[(X_valid, y_valid)],\n                 model__verbose=0)\n\n# Predict\nxgb_preds = xgb_pipeline.predict(X_valid)","7ef3070a":"# Result\nprint(\"Model#2: XGBoost - Performance Summary\\n----------\")\nxgb_score = mean_absolute_error(y_valid, xgb_preds)\nprint('MAE score (80\/20 train-test split) :', xgb_score)\n\nfrom sklearn.model_selection import cross_val_score\nxgb_scores = -1 * cross_val_score(xgb_pipeline, X_sample, y, cv=5, scoring='neg_mean_absolute_error', verbose=0, error_score=0)\n# print(\"MAE scores (5CVs): \", rf_scores)\nprint(\"MAE score (Cross Validation)       :\", xgb_scores.mean())\nprint(\"Overall MAE =\", round(xgb_scores.mean(), 2))","a45a1eef":"rf_importances = pd.DataFrame({'feature': X_valid.columns, 'xgb_importance': np.round(xgb_model.feature_importances_,3)})\nrf_importances = rf_importances.sort_values('xgb_importance', ascending=False).set_index('feature')\nrf_importances[:10]","fe06dc1d":"# Display first 5 predictions & actual result\nprint(\"First 5 Predictions (Random Forest): \", rf_preds.tolist()[:5])\nprint(\"First 5 Predictions (XGBoost)      : \", xgb_preds.tolist()[:5])\nprint(\"First 5 Actual result              : \", y_valid.tolist()[:5])\nX_full_valid.head()","6f9ec497":"## Cleaning & Dropping Features\n1. Unused values: **Index(#)**, **Name**\n\n2. Very low correlation: **Generation** - Does not correlate to any base stats\n\n3. Data leakage: **Total** - The sum of all base stats including HP. So it will be not available when model use for prediction.\n\n4. Leftovers after one-hot","4b08b7a3":"## Model 2 - XG Boost\n#### pipeline(model), train, predict, cross-validation","808c847c":"OneHotEncoder: 14.02 || pd.get_dummies: 14.04\n\n### Feature Importance (Random Forest)","75cffa6d":"## Model 1 - Random Forest\n#### pipeline(preprocessor + model), train, predict, cross-validation","7102c1d3":"## Choosing Target - HP (Health Power)","0fd6ab03":"## Choosing Features\n#### 1. Base stats (Atk, Sp.Atk, Def, Sp.Def, Speed) \n(Numerical) Use value as is. Impute missing values with MEAN value. \n\n#### 2. Is it Legendary? (Legendary) \n(Bool) Convert boolean to Integer 0\/1. Use value as is.\n\n#### 3. Pokemon Types (Type 1 & Type 2)\n(Categorical) One-hot encode. Missing values = 0.","1c961259":"## Identifying Features to Impute\/Encode\n#### 1. Numerical columns\nimpute missing values, some use MEAN, some use ZERO\n\n#### 2. Categorical columns\none hot encoding","8c2a7c9c":"## Study Correlations to Identify Noise\nFeature vs Feature : High = redundant = noise. (Action = Drop)<br>\nFeature vs Target  : Very low = no contribution to prediction = noise.","f9a50a1b":"Type 2 missing simply means the Pokemon does not have a secondary type","3351fa57":"OneHotEncoder: error || pd.get_dummies: 13.83\n\n### Feature Importance (XG Boost)","76fa2e1d":"## Conclusion\n* XG Boost slightly perform better than Random Forest\n* Both models are not accurate \n    * Insufficient model tuning? Wrong params?\n    * Not enough features? -> explore other datasets","df9c3bb6":"## Understand Dataset\n**.columns** & **.describe()** - Min, Max, Average, etc","fc609e39":"Need to decide how to impute categorical column \"type 2\". Currently I impute with empty string.","5ce2b34b":"## Split Train\/Test Data","bbd33fd2":"### POKEMON MACHINE LEARNING - Predict Pokemon HP stats\n\n#### regression models: 1) Random Forest 2) XGBoost \n\nSteps:\n- understand datasets, choose features and target\n- ensure no data leakage in any features\n- study correlations between X & Y, if the relationship is weak, it is unlikely to contribute to the prediction\n\n- data prep, casting, removing un-helpful columns\n- split train\/test datasets\n- decide how to impute\/encode numerical OR categorical data\n\n- create pipelines for preprocessor and model\n- random forest model: experiment different parameters, compare MAE, use CV to get overall MAE, list importance\n- XGBoost model: experiment different parameters, compare MAE, use CV to get overall MAE, list importance\n"}}