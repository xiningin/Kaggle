{"cell_type":{"132734e8":"code","b8b562f3":"code","4d18c88c":"code","169ed1eb":"code","c8eb3d90":"code","2e56e0c6":"code","ef70123a":"code","7dc90cf5":"code","b831868c":"code","86b2d08b":"code","6f3c0af4":"code","5b222bec":"code","9c80afdb":"code","dd1db3d0":"code","c72ea13b":"code","f52ff00f":"code","4f4c030f":"code","2c078059":"code","caeb3ca1":"code","14da33e8":"code","4e488857":"code","12b60f62":"code","a14ee886":"code","ed812922":"code","d5cd11a2":"code","32daa733":"code","c3e8f44b":"code","33812ea5":"code","35463911":"code","eae98730":"code","bdbc443c":"code","21a8887f":"code","0888bb44":"code","83470770":"code","24b5641d":"code","4bdd1db9":"code","775b04d0":"code","ae42c45c":"code","89f06c0c":"code","84d210df":"code","a9dbc28d":"code","7cc910f1":"code","04c1eced":"code","eaccbd63":"code","dc8f1721":"code","da47355a":"code","9fc21366":"code","3df2d222":"code","676f2356":"code","de8c5ad0":"code","5bfafa33":"code","8905396b":"code","336c56f5":"code","064795cd":"code","0d888537":"code","824628c8":"code","5209bdfd":"code","ef26899d":"code","0b247e2c":"code","d2b7ad33":"markdown","7ee9f4fb":"markdown","9effb2a7":"markdown","35401bfb":"markdown","a99b10a1":"markdown","1878250d":"markdown","84589a7c":"markdown","8725cb13":"markdown","e23fc2d1":"markdown","98127776":"markdown","fb4b4c6f":"markdown","8377bd37":"markdown","8079678e":"markdown","c072987b":"markdown","34bc4e87":"markdown","86f8afe0":"markdown","dcce245e":"markdown","18372140":"markdown","2557352f":"markdown","da44dfd6":"markdown","8a964d38":"markdown","f1f5b645":"markdown","967d1161":"markdown","8361eebe":"markdown","22ffad97":"markdown","ea83ba26":"markdown","3d22f965":"markdown","04fab96e":"markdown","2f29371a":"markdown","666acfe7":"markdown","1884c7a8":"markdown","669de224":"markdown","e434e946":"markdown","7385f8e5":"markdown","11b1615c":"markdown","f0e2aa92":"markdown","43d43c33":"markdown","233e1cec":"markdown","72795320":"markdown","1a0d84cd":"markdown","a6915592":"markdown","434ef34f":"markdown","6a266ebd":"markdown","1aa17f83":"markdown","35cb161a":"markdown","8d30b91a":"markdown","5d2dafba":"markdown","7161f541":"markdown","ede8c17a":"markdown","2becb8ca":"markdown","2490df41":"markdown","d48560a2":"markdown","ca9eae7b":"markdown","765d090d":"markdown","b1ac3667":"markdown","54755478":"markdown","cb9f875c":"markdown","d28ec9e8":"markdown"},"source":{"132734e8":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nplt.style.use(\"fivethirtyeight\")\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","b8b562f3":"df = pd.read_csv('\/kaggle\/input\/pima-indians-diabetes-database\/diabetes.csv')","4d18c88c":"df.head()","169ed1eb":"print(\"Number of rows present in the dataset are: \", df.shape)","c8eb3d90":"df.info()","2e56e0c6":"df.describe().T","ef70123a":"import seaborn as sns\nfrom itertools import cycle\ncolor_cycle = cycle(plt.rcParams['axes.prop_cycle'].by_key()['color'])\n\nsns.countplot(df['Outcome'])\nplt.show()","7dc90cf5":"df['Outcome'].value_counts()","b831868c":"fig, ax = plt.subplots()\n\nlabels = ['Diabetic', \n         'Non-Diabetic']\npercentages = [34.89, 65.10]\nexplode=(0.1,0)\nax.pie(percentages, explode=explode, labels=labels, autopct='%1.0f%%', \n       shadow=False, startangle=0,   \n       pctdistance=1.2,labeldistance=1.4)\nax.legend(frameon=False, bbox_to_anchor=(1.5,0.8))\nplt.show()","86b2d08b":"for col in df.columns:\n    print(\"The minimum value fore the columns {} is {}\".format(col, df[col].min()))","6f3c0af4":"def msv_1(data, thresh = 20, color = 'black', edgecolor = 'black', height = 3, width = 15):\n    \n    plt.figure(figsize = (width, height))\n    percentage = (data.isnull().mean()) * 100\n    percentage.sort_values(ascending = False).plot.bar(color = color, edgecolor = edgecolor)\n    plt.axhline(y = thresh, color = 'r', linestyle = '-')\n    \n    plt.title('Missing values percentage per column', fontsize=20, weight='bold' )\n    \n    plt.text(len(data.isnull().sum()\/len(data))\/1.7, thresh+2.5, f'Columns with more than {thresh}% missing values', fontsize=12, color='crimson',\n         ha='left' ,va='top')\n    plt.text(len(data.isnull().sum()\/len(data))\/1.7, thresh - 0.5, f'Columns with less than {thresh}% missing values', fontsize=12, color='green',\n         ha='left' ,va='top')\n    plt.xlabel('Columns', size=15, weight='bold')\n    plt.ylabel('Missing values percentage')\n    plt.yticks(weight ='bold')\n    \n    return plt.show()\nmsv_1(df, 20, color=sns.color_palette('Reds',15))","5b222bec":"df[['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']] = df[['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']].replace(0, np.nan)","9c80afdb":"msv_1(df, 20, color=sns.color_palette('Reds',15))","dd1db3d0":"fig, axes = plt.subplots(4, 2, figsize=(15, 10))\naxes = axes.flatten()\nax_idx = 0\ncolumns = df.drop('Outcome', axis = 1).columns\nfor col in columns:\n    df[col].plot(kind = 'hist', ax = axes[ax_idx], title = col, color = next(color_cycle))\n    ax_idx += 1\n\nplt.suptitle('Sales Trend according to Departments')\nplt.tight_layout()\nplt.show()","c72ea13b":"from scipy.stats import skew\nfor col in df.drop('Outcome', axis = 1).columns:\n    print(\"Skewness for the column {} is {}\".format(col, df[col].skew()))","f52ff00f":"df['Insulin'] = df['Insulin'].fillna(df['Insulin'].median()) # Filling null values with the median.\n\nfor col in ['Glucose', 'BloodPressure', 'SkinThickness', 'BMI']:\n    df[col] = df[col].fillna(df[col].mean())","4f4c030f":"msv_1(df, 10, color=sns.color_palette('Greens',15))","2c078059":"df.isnull().sum()","caeb3ca1":"def mean_target(var):\n    \"\"\"\n    A function that will return the mean values for 'var' column depending on whether the person\n    is diabetic or not\n    \"\"\"\n    return pd.DataFrame(df.groupby('Outcome').mean()[var])","14da33e8":"def distplot(col_name):\n    \"\"\"\n    A function that will plot the distribution of column 'col_name' for diabetic and non-diabetic people separately\n    \"\"\"\n    plt.figure()\n    ax = sns.distplot(df[col_name][df.Outcome == 1], color =\"red\", rug = True)\n    sns.distplot(df[col_name][df.Outcome == 0], color =\"lightblue\",rug = True)\n    plt.legend(['Diabetes', 'No Diabetes'])","4e488857":"distplot('Pregnancies')","12b60f62":"mean_target('Pregnancies')","a14ee886":"distplot('Insulin')","ed812922":"mean_target('Insulin')","d5cd11a2":"distplot('BloodPressure')","32daa733":"mean_target('BloodPressure')","c3e8f44b":"distplot('Glucose')","33812ea5":"mean_target('Glucose')","35463911":"sns.boxplot(x = 'Outcome', y = 'Age', data = df)\nplt.title('Age vs Outcome')\nplt.show()","eae98730":"sns.boxplot(x = 'Outcome', y = 'BloodPressure', data = df, palette = 'Blues')\nplt.title('BP vs Outcome')\nplt.show()","bdbc443c":"sns.jointplot(x='Age',y='BloodPressure', data=df, kind = 'reg', color = 'green')","21a8887f":"my_pal = {0: \"lightgreen\", 1: \"lightblue\"}\nsns.boxplot(x = 'Outcome', y = 'DiabetesPedigreeFunction', data = df, palette = my_pal)\nplt.title('DPF vs Outcome')\nplt.show()","0888bb44":"my_pal = {0: \"lightgrey\", 1: \"lightyellow\"}\nsns.boxplot(x = 'Outcome', y = 'Glucose', data = df, palette = my_pal)\nplt.title('Glucose vs Outcome')\nplt.show()","83470770":"sns.jointplot(x='Insulin',y='Glucose', data=df, kind = 'reg', color = 'red')\nplt.show()","24b5641d":"sns.boxplot(x = 'Outcome', y = 'Insulin', data = df)\nplt.title('Insulin vs Outcome')\nplt.show()","4bdd1db9":"my_pal = {0: \"lightyellow\", 1: \"lightpink\"}\nsns.boxplot(x = 'Outcome', y = 'BMI', data = df, palette = my_pal)\nplt.title('BMI vs Outcome')\nplt.show()","775b04d0":"corr = df.corr()\n\n# Generate a mask for the upper triangle\nmask = np.triu(np.ones_like(corr, dtype=bool))\n\n# Set up the matplotlib figure\nf, ax = plt.subplots(figsize=(11, 9))\n\n# Generate a custom diverging colormap\ncmap = sns.diverging_palette(230, 20, as_cmap=True)\n\n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(corr, mask=mask, cmap=cmap, vmax=1.0, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5}, annot = True)","ae42c45c":"from sklearn.model_selection import train_test_split\n\nX = df.drop('Outcome', axis = 1)\ny = df['Outcome']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 42, stratify = y)","89f06c0c":"from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train =  pd.DataFrame(sc.fit_transform(X_train),\n        columns=['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin','BMI', 'DiabetesPedigreeFunction', 'Age'])\nX_test = pd.DataFrame(sc.fit_transform(X_test),\n        columns=['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin','BMI', 'DiabetesPedigreeFunction', 'Age'])","84d210df":"from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n\ndef evaluation(model, x_train_std, y_train, x_test, y_test, train = True):\n    \"\"\"\n    A function that returns the score of every evaluation metrics\n    \"\"\"\n    if train == True:\n        pred = model.predict(x_train_std)\n        classifier_report = pd.DataFrame(classification_report(y_train, pred, output_dict = True))\n        print(\"Train Result:\\n================================================\")\n        print(f\"Accuracy Score: {accuracy_score(y_train, pred) * 100:.2f}%\")\n        print(\"_______________________________________________\")\n        print(f\"F1 Score: {round(f1_score(y_train, pred), 2)}\")\n        print(\"_______________________________________________\")\n        print(f\"CLASSIFICATION REPORT:\\n{classifier_report}\")\n        print(\"_______________________________________________\")\n        print(f\"Confusion Matrix: \\n {confusion_matrix(y_train, pred)}\\n\")\n        \n    if train == False:\n        pred = model.predict(x_test)\n        classifier_report = pd.DataFrame(classification_report(y_test, pred, output_dict = True))\n        print(\"Test Result:\\n================================================\")\n        print(f\"Accuracy Score: {accuracy_score(y_test, pred) * 100:.2f}%\")\n        print(\"_______________________________________________\")\n        print(f\"F1 Score: {round(f1_score(y_test, pred), 2)}\")\n        print(\"_______________________________________________\")\n        print(f\"CLASSIFICATION REPORT:\\n{classifier_report}\")\n        print(\"_______________________________________________\")\n        print(f\"Confusion Matrix: \\n {confusion_matrix(y_test, pred)}\\n\")","a9dbc28d":"from sklearn.linear_model import LogisticRegression\n\nlr = LogisticRegression(solver = 'liblinear')\nlr.fit(X_train, y_train)\n\nevaluation(lr, X_train, y_train, X_test, y_test, True)\nprint()\nevaluation(lr, X_train, y_train, X_test, y_test, False)","7cc910f1":"train_score_lr = round(accuracy_score(y_train, lr.predict(X_train)) * 100, 2)\ntest_score_lr = round(accuracy_score(y_test, lr.predict(X_test)) * 100, 2)","04c1eced":"from sklearn.tree import DecisionTreeClassifier\n\ndtc = DecisionTreeClassifier()\ndtc.fit(X_train, y_train)\n\nevaluation(dtc, X_train, y_train, X_test, y_test, True)\nprint()\nevaluation(dtc, X_train, y_train, X_test, y_test, False)","eaccbd63":"train_score_dtc = round(accuracy_score(y_train, dtc.predict(X_train)) * 100, 2)\ntest_score_dtc = round(accuracy_score(y_test, dtc.predict(X_test)) * 100, 2)","dc8f1721":"from sklearn.ensemble import RandomForestClassifier\n## Finding out the right number of estimators\naccuracy_scores = []\nfor i in range(1, 1000, 100):\n    rfc = RandomForestClassifier(n_estimators = i, random_state = 0)\n    rfc.fit(X_train, y_train)\n    accuracy_scores.append(accuracy_score(y_test, rfc.predict(X_test)))\nplt.plot(accuracy_scores)","da47355a":"rfc = RandomForestClassifier(n_estimators = 500)\nrfc.fit(X_train, y_train)\n\nevaluation(rfc, X_train, y_train, X_test, y_test, True)\nprint()\nevaluation(rfc, X_train, y_train, X_test, y_test, False)","9fc21366":"train_score_rfc = round(accuracy_score(y_train, rfc.predict(X_train)) * 100, 2)\ntest_score_rfc = round(accuracy_score(y_test, rfc.predict(X_test)) * 100, 2)","3df2d222":"from sklearn.neighbors import KNeighborsClassifier\n\naccuracy_scores = []\n\nfor i in range(1, 10):\n    knn = KNeighborsClassifier(n_neighbors = i)\n    knn.fit(X_train, y_train)\n    accuracy_scores.append(accuracy_score(y_test, knn.predict(X_test)))\n    \nplt.plot(accuracy_scores)","676f2356":"knn = KNeighborsClassifier(n_neighbors = 9)\nknn.fit(X_train, y_train)\n\nevaluation(knn, X_train, y_train, X_test, y_test, True)\nevaluation(knn, X_train, y_train, X_test, y_test, False)","de8c5ad0":"train_score_knn = round(accuracy_score(y_train, knn.predict(X_train)) * 100, 2)\ntest_score_knn = round(accuracy_score(y_test, knn.predict(X_test)) * 100, 2)","5bfafa33":"from xgboost import XGBClassifier\n\nxgb = XGBClassifier(n_estimators = 500, learning_rate = 0.15)\nxgb.fit(X_train, y_train)\n\nevaluation(xgb, X_train, y_train, X_test, y_test, True)\nevaluation(xgb, X_train, y_train, X_test, y_test, False)","8905396b":"train_score_xgb = round(accuracy_score(y_train, xgb.predict(X_train)) * 100, 2)\ntest_score_xgb = round(accuracy_score(y_test, xgb.predict(X_test)) * 100, 2)","336c56f5":"models = {\n           'Train Accuracy': [train_score_lr, train_score_dtc, train_score_rfc, train_score_knn, train_score_xgb],\n          'Test Accuracy' : [test_score_lr, test_score_dtc, test_score_rfc, test_score_knn, test_score_xgb]\n         }\n\nmodels = pd.DataFrame(models, index = ['Logistic Regression', 'Decision Tree Classifier', 'Random Forest Classifier', 'K-Nearest Neighbor', 'XGBoost'])\nmodels.head()","064795cd":"from sklearn.model_selection import cross_val_score, StratifiedKFold\nfrom sklearn.ensemble import AdaBoostClassifier\n\nkfold = StratifiedKFold(n_splits=10)\n\nrandom_state = 0\nclassifiers = []\nclassifiers.append(LogisticRegression(random_state = random_state))\nclassifiers.append(DecisionTreeClassifier(random_state=random_state))\nclassifiers.append(RandomForestClassifier(random_state=random_state, n_estimators = 500))\nclassifiers.append(KNeighborsClassifier())\nclassifiers.append(XGBClassifier(random_state=random_state))\n\ncv_results = []\nfor classifier in classifiers :\n    cv_results.append(cross_val_score(classifier, X_train, y = y_train, scoring = \"accuracy\", cv = kfold, n_jobs=-1))\n\ncv_means = []\ncv_std = []\nfor cv_result in cv_results:\n    cv_means.append(cv_result.mean())\n    cv_std.append(cv_result.std())\n\ncv_res = pd.DataFrame({\"CrossValMeans\":cv_means,\"CrossValerrors\": cv_std,\"Algorithm\":[\"Logistic Regression\",\"Decision Tree\",\n\"Random Forest\", \"KNeighboors\", \"XGBoosting\"]})\n\ng = sns.barplot(\"CrossValMeans\",\"Algorithm\",data = cv_res, palette=\"Set3\",orient = \"h\",**{'xerr':cv_std})\ng.set_xlabel(\"Mean Accuracy\")\ng = g.set_title(\"Cross validation scores\")","0d888537":"cv_res.head()","824628c8":"from sklearn.model_selection import GridSearchCV\n\nRFC = RandomForestClassifier()\n\n\n## Search grid for optimal parameters\nrf_param_grid = {\"max_depth\": [None],\n              \"max_features\": [1, 3, 10],\n              \"min_samples_split\": [2, 3, 10],\n              \"min_samples_leaf\": [1, 3, 10],\n              \"bootstrap\": [False],\n              \"n_estimators\" :[100,300],\n              \"criterion\": [\"gini\"]}\n\n\ngsRFC = GridSearchCV(RFC,param_grid = rf_param_grid, cv=kfold, scoring=\"accuracy\", n_jobs= 4, verbose = 1)\n\ngsRFC.fit(X_train,y_train)\n\nRFC_best = gsRFC.best_estimator_\n\n# Best score\ngsRFC.best_score_","5209bdfd":"print(RFC_best)","ef26899d":"# XGBoosting Classifier tunning\n\nXGB = XGBClassifier(verbosity = 0)\ngb_param_grid = {'loss' : [\"deviance\"],\n              'n_estimators' : [100,200,300],\n              'learning_rate': [0.1, 0.05, 0.01],\n              'max_depth': [4, 8],\n              'min_samples_leaf': [100,150],\n              'max_features': [0.3, 0.1] \n              }\n\ngsXGB = GridSearchCV(XGB, param_grid = gb_param_grid, cv=kfold, scoring=\"accuracy\")\n\ngsXGB.fit(X_train,y_train)\n\nXGB_best = gsXGB.best_estimator_\n# Best score\ngsXGB.best_score_","0b247e2c":"XGB_best","d2b7ad33":"**All null values are taken care of now**","7ee9f4fb":"**Yes, we were right, the median of the age of diabetic people is greater than that of non-diabetic people.**\n\n**Let's also check the effect of Blood Pressure on the Outcome.**","9effb2a7":"**We can observe that Insulin column has close to 50% zero or null values, followed by SkinThickness that has close to 30% missing values. We will be filling these values later.**","35401bfb":"## Insulin","a99b10a1":"# Dataset Splitting and Features Scaling","1878250d":"**Diabetic People tend to have more Insulin level.**","84589a7c":"# Null Values","8725cb13":"# Exploratory Data Analysis and Building ML Model for Diabetes Prediction","e23fc2d1":"**By Hyperparameters tuning, we get the best accuracy as 77% and the hyperparameters that give the best result are shown below.**","98127776":"## BloodPressure","fb4b4c6f":"**You might be wondering that there are no null values in the dataset, but are you sure? Remember what we had discussed in the previous section where certain columns were having zero as their minima eventhough they aren't supposed to. Those values will be considered as null values. Let's replace the zeros present in the Glucose, BloodPressure, SkinThickness, Insulin, and BMI columns with null.**","8377bd37":"**Now out of the above columns having zero as their minima, only Pregnancie Column can take the values as zero, so what should do we do with those columns that have zero as their minimum even if they aren't supposed to?**","8079678e":"## Glucose","c072987b":"**From the above heatmap, we can observe that all the features are weakly correlated, so that removes multicollinearity out of equation. Multicollinearity (also collinearity) is a phenomenon in which one predictor variable in a multiple regression model can be linearly predicted from the others with a substantial degree of accuracy. Models like Logistic Regression assumes the presence of non-collinearity among the features, if multicollinearity is present it can lead to the bad performance of such models.**","34bc4e87":"**Maximum test accuracy is achieved using 500 trees**","86f8afe0":"**Quite a proportion of people having high DPF does not end up having Diabetes.  But usually the diabetic people have DPF value close to 0.5 (50th Percentile)**","dcce245e":"**We can see that the number of pregnancies is high for the diabetic people**","18372140":"## KNN","2557352f":"**The median of the BloodPressure of diabetic people lies close to the 75th Percentile of non-diabetic people.**\n\n**The next thing a common man would check is the relationship between age and BP**","da44dfd6":"### Random Forest Classifier","8a964d38":"**Indeed, the Median BMI of the Diabetic People is greater than the Median BMI of the Non-Diabetic people.**","f1f5b645":"## Decision Tree Classifier","967d1161":"## Pregnancies","8361eebe":"**The distribution of the dependent variable is not skewed or imbalanced. We can move ahead with the same data without having to apply SMOTE or undersampling or oversampling techniques. But we do need to make that we distribution of the classes remain same when we split our data to train and test set.**\n\n**Before we move ahead, we need to check what are the minimum values for each column, certain columns like Glucose or Insulin can not have values as 0. Therefore, we need to take care of such values.**","22ffad97":"The different columns present in the dataset are:\n\n* Pregnancies -> Number of times Pregnant\n\n* Glucose -> Plasma glucose concentration\n\n* BloodPressure -> Diastolic blood pressure (mm Hg)\n\n* SkinThickness -> Triceps skin fold thickness (mm)\n\n* Insulin -> 2-Hour serum insulin (mu U\/ml)\n\n* BMI -> Body Mass Index\n\n* DiabetesPedigreeFunction -> Diabetes pedigree function\n\n* Age -> Age in years\n\n* Outcome -> Whether the lady is diabetic or not, 0 represents the person is not diabetic and 1 represents that the person is diabetic.\n\n","ea83ba26":"## Logistic Regression","3d22f965":"**Hmm, as the age increases, generally the Blood Pressure also increases**\n\n**One would also want to know the chances of getting diabetes, if it is common in the family. We can check that with the Diabetes Pedigree Function.**","04fab96e":"**Hope you liked the notebook, any suggestions would be highly appreciated.**\n\n**I will continue experimenting in future versions of the notebook.**\n\n**Please upvote if you liked it.**","2f29371a":"# Comman Man Analysis","666acfe7":"**Wow! the median of the Glucose level of Diabetic People is greater than the 75th Percentile of the glucose level of non-diabetic people. Therefore having a high glucose level does increase the chances of having diabetes.**","1884c7a8":"![diabetes.png](attachment:c62368f4-3543-4895-a2ae-c27aba0ffb49.png)","669de224":"**Let's think like a common man, and analyze the data.**\n\n**First, we would know what is the effect of Age on the Outcome because we have heard that as the age increases, the chances of diabetes also commonly increases.**","e434e946":"**Let's check the skewness of each of the columns.**\n\n**Skewness refers to the amount of asymmetry in the given feature or in other words amount of distortions from the normal distribution. The peak of the histogram represents the mode.**","7385f8e5":"**Diabetes is a disease that occurs when your blood glucose, also called blood sugar, is too high. The objective of the dataset is to diagnostically predict whether or not a patient has diabetes, based on certain diagnostic measurements included in the dataset. Several constraints were placed on the selection of these instances from a larger database. In particular, all patients here are females at least 21 years old of Pima Indian heritage.**\n\n**We will explore this dataset and find out factors that contribute the most for diabetes causation. We will also build Machine Learning Models that can help to predict whether a person is diabetic or not and try to improve the model by performing Cross Validation and hyperparameter tuning.**\n\n**Do Upvote the notebook if you liked it!**","11b1615c":"# Hyperparameter Tuning","f0e2aa92":"# Baseline Models","43d43c33":"# Exploratory Data Analysis","233e1cec":"**From the above table, we can observe that we get the best results for Logistic Regression Algorithm, the accuracy in the case is 78.3%. The Results for Random Forest and KNeighbors Classifer is also not bad. Let's try out Hyperparameter Tuning using GridSearchCv for Random Forest Classifier and XGBoost Classifier**","72795320":"## Body Mass Index","1a0d84cd":"**Let's first check whether there is any relation between glucose and insulin level.**","a6915592":"# Correlation Matrix","434ef34f":"**Diabetic People tend to have much higher Glucose level**","6a266ebd":"**Columns like Pregnancies, Glucose, BloodPressure, SkinThickness and BMI are not that much skewed. We can fill null values with the mean for these columns, but for columns like Insulin and DiabetesPedigreeFunction, we will have to replace them will median due to the effect of skewness.**","1aa17f83":"**We can see that as the insulin level increases, the Glucose level also increases.**","35cb161a":"**Standardizing a dataset involves rescaling the distribution of values so that the mean of observed values is 0 and the standard deviation is 1.**\n\n**This can be thought of as subtracting the mean value or centering the data. Scaling the features is of utmost importance because different features are in different scales. Let's say the Age has values in double digits, whereas the DPF is of the kind float, the effect of the Age feature will be more as compared to the DPF**\n\n**Best practice is to use only the training set to figure out how to scale \/ normalize, then blindly apply the same transform to the test set.**\n\n**For example, say you're going to normalize the data by removing the mean and dividing out the variance. If you use the whole dataset to figure out the feature mean and variance, you're using knowledge about the distribution of the test set to set the scale of the training set - 'leaking' information.**\n\n**The right way to do this is to use only the training set to calculate the mean and variance, normalize the training set, and then at test time, use that same (training) mean and variance to normalize the test set.**","8d30b91a":"# Cross Validation","5d2dafba":"**The F1 score we got using Logistic Regression is very less, and in the Confusion Matrix it is visible that 53 wrong predictions has been made. We need to improve it.**","7161f541":"**The mean of the blood pressure is greater for diabetic people as compared to the non-diabetic people**","ede8c17a":"## Gluscose Level","2becb8ca":"## Random Forest Classifier","2490df41":"**We know that Decision Tree Classifier is prone to Overfitting, and that's what we have got here. We can see that the train accuracy is 100% but the test accuracy is only 67.71%. We also know that Random Forest Classifier helps to overcome this drawback of Decision Tree Classifer. Let's check that model out.**","d48560a2":"**Body mass index (BMI) is a measure of body fat based on height and weight that applies to adult men and women. Does having a higher BMI leads to more chances of being diabetic? Let's check that out!**","ca9eae7b":"## Insulin","765d090d":"### XGBoost Classifier","b1ac3667":"**Here also the model has overfitted, but the test accuracy has increased to 74%. And 50 wrong predictions are being made. We can also observe that the precision, recall and F1 score is less for the diabetic people. This means that there is a lack of training instances for the outcome '1'. We will be using Cross Validation later in the notebook**","54755478":"**In this section, we will be doing some basic Exploratory Data Analysis to get the \"feel\" of the data, we will be checking the distributions, the correlations etc of the different columns and try to remove the null values present.**","cb9f875c":"**We can observe from the above dataframe that Decision Tree Classifier, Random Forest Classifier, and XGBoost have all overfitted the given dataset. Let's explore the results when we use Cross Validation.**","d28ec9e8":"## XGBoost"}}