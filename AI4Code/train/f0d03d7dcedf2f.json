{"cell_type":{"94ddaa36":"code","ba51535e":"code","9223695d":"code","e224372f":"code","a867998b":"code","7667218f":"code","e7384b6b":"code","41a921bc":"code","8f7db967":"code","a0a6b7a3":"code","b553e5c2":"code","d97ba0fe":"code","0a6d1b88":"code","b2177fb4":"code","cd4f6eb2":"code","842b109f":"markdown","1d0ef1ff":"markdown","25d99008":"markdown","28fc3fda":"markdown"},"source":{"94ddaa36":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\n\nfrom keras import models\nfrom keras import layers\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","ba51535e":"column_names = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV']\ndata = pd.read_csv(\"..\/input\/boston-house-prices\/housing.csv\", header=None, delimiter=r\"\\s+\", names=column_names)","9223695d":"data.head()","e224372f":"y = data[\"MEDV\"]\nX = data.drop(labels = [\"MEDV\"],axis = 1) \ntrain_data, test_data, train_targets, test_targets = train_test_split(X, y, test_size = 0.2, random_state=0)","a867998b":"#train_data.shape\n#test_data.shape\ntrain_targets","7667218f":"#Normalizing\nmean = train_data.mean(axis=0)\ntrain_data -= mean\nstd = train_data.std(axis=0)\ntrain_data \/= std\ntest_data -= mean\ntest_data \/= std","e7384b6b":"def build_model():\n    model = models.Sequential()\n    model.add(layers.Dense(64, activation='relu', input_shape=(train_data.shape[1],)))\n    model.add(layers.Dense(64, activation='relu'))\n    model.add(layers.Dense(1))\n    model.compile(optimizer='rmsprop', loss='mse', metrics=['mae'])\n    return model","41a921bc":"k = 4\nnum_val_samples = len(train_data) \/\/ k\nnum_epochs = 100\nall_scores = []\n\nfor i in range(k):\n    print('processing fold #', i)\n    val_data = train_data[i * num_val_samples: (i + 1) * num_val_samples]\n    val_targets = train_targets[i * num_val_samples: (i + 1) * num_val_samples]\n    partial_train_data = np.concatenate(\n        [train_data[:i * num_val_samples],\n        train_data[(i + 1) * num_val_samples:]],\n        axis=0)\n    partial_train_targets = np.concatenate(\n        [train_targets[:i * num_val_samples],\n        train_targets[(i + 1) * num_val_samples:]],\n        axis=0)\n    model = build_model()\n    model.fit(partial_train_data, partial_train_targets,\n        epochs=num_epochs, batch_size=1, verbose=0)\n    val_mse, val_mae = model.evaluate(val_data, val_targets, verbose=0)\n    all_scores.append(val_mae)\n","8f7db967":"#all_scores\nnp.mean(all_scores)","a0a6b7a3":"# 500 epochs with log\nnum_epochs = 500\nall_mae_histories = []\nfor i in range(k):\n    print('processing fold #', i)\n    val_data = train_data[i * num_val_samples: (i + 1) * num_val_samples]\n    val_targets = train_targets[i * num_val_samples: (i + 1) * num_val_samples]\n    partial_train_data = np.concatenate(\n        [train_data[:i * num_val_samples],\n        train_data[(i + 1) * num_val_samples:]],\n        axis=0)\n    partial_train_targets = np.concatenate(\n        [train_targets[:i * num_val_samples],\n        train_targets[(i + 1) * num_val_samples:]],\n        axis=0)\n    model = build_model()\n    history = model.fit(partial_train_data, partial_train_targets,\n                        validation_data=(val_data, val_targets),\n                        epochs=num_epochs, batch_size=1, verbose=0)\n    mae_history = history.history['val_mae']\n    all_mae_histories.append(mae_history)","b553e5c2":"average_mae_history = [\nnp.mean([x[i] for x in all_mae_histories]) for i in range(num_epochs)]","d97ba0fe":"plt.plot(range(1, len(average_mae_history) + 1), average_mae_history)\nplt.xlabel('Epochs')\nplt.ylabel('Validation MAE')\nplt.show()","0a6d1b88":"def smooth_curve(points, factor=0.9):\n    smoothed_points = []\n    for point in points:\n        if smoothed_points:\n            previous = smoothed_points[-1]\n            smoothed_points.append(previous * factor + point * (1 - factor))\n        else:\n            smoothed_points.append(point)\n    return smoothed_points\nsmooth_mae_history = smooth_curve(average_mae_history[10:])\nplt.plot(range(1, len(smooth_mae_history) + 1), smooth_mae_history)\nplt.xlabel('Epochs')\nplt.ylabel('Validation MAE')\nplt.show()","b2177fb4":"model = build_model()\nmodel.fit(train_data, train_targets,\nepochs=80, batch_size=16, verbose=0)\ntest_mse_score, test_mae_score = model.evaluate(test_data, test_targets)","cd4f6eb2":"test_mae_score","842b109f":"## Evaluate on test set","1d0ef1ff":"## Model","25d99008":"## K-fold validation","28fc3fda":"## Preprocessing"}}