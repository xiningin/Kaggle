{"cell_type":{"1dee38c7":"code","5fe24c10":"code","74469f47":"code","dea8525e":"code","070e879a":"code","25037c89":"code","a6ac4b9a":"code","e23e689e":"code","59ace187":"code","2feb3c4e":"code","11cc5084":"code","99f27d29":"code","771fa9c0":"code","16c918c6":"code","db4ae4f2":"code","f7418ef1":"code","a622d846":"code","07b2a470":"code","9f911ce4":"code","82ad4413":"code","0ade4007":"code","ad0ba845":"code","60cb1bed":"code","b7ad4784":"code","8e7445ce":"code","d65a5dff":"code","0b80235e":"code","9f20a61c":"code","ff263b11":"code","b1eb061a":"code","00da30af":"code","91482a63":"code","de0ac394":"code","76c14d86":"code","5821c412":"code","70506962":"code","6de50fc1":"code","6ae7550b":"code","94ab0036":"code","7319daaf":"code","a9c845d4":"code","4913b1b5":"code","71671936":"code","09382ea0":"code","770bd1b3":"code","7ab405d2":"code","3f3a95e1":"code","965fdc7b":"code","cf4fb86c":"code","a460c770":"code","9ef724c1":"code","216d09c9":"markdown","f0b44d67":"markdown","f4ba4cd1":"markdown","8a7b436c":"markdown","7c0ad369":"markdown","4e14889b":"markdown","8dfb43e2":"markdown","cc0627ed":"markdown","91de41e9":"markdown","5033b825":"markdown","edae71e1":"markdown","8eff56f9":"markdown","14b2f373":"markdown","8d501411":"markdown","6458a6ac":"markdown","0b26c707":"markdown","ac52c35d":"markdown","e3b3f0da":"markdown","ba98f0b8":"markdown","a64e7797":"markdown","62bbac76":"markdown","a52b42a3":"markdown","3e1be941":"markdown","71e00863":"markdown","0fed6523":"markdown","ab614280":"markdown","de091219":"markdown","6a62215d":"markdown","4130085f":"markdown","704351de":"markdown","9c8a6da1":"markdown","ed445a6b":"markdown","8d4b5d7c":"markdown","eddceda9":"markdown","33360557":"markdown","4f8bb2e5":"markdown","4c347299":"markdown","ed7f4873":"markdown","a3b4fa94":"markdown","ffd797c7":"markdown","22df4495":"markdown","f819e9c2":"markdown","2142b7a3":"markdown","9502e420":"markdown","c6dc1b56":"markdown","4f135c52":"markdown","1b47e91d":"markdown","357a5af1":"markdown","15d91367":"markdown","54e4ea07":"markdown","a08fe033":"markdown","bed6510a":"markdown","5724af5b":"markdown","0a85b57d":"markdown","e120862a":"markdown","b74c6a02":"markdown","ea00646d":"markdown"},"source":{"1dee38c7":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","5fe24c10":"#matplotlib and seaborn are imported for visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\n#splitting the dataset into train & test data\nfrom sklearn.model_selection import train_test_split\n\n#GridSearchCV is used for hyperparameter tuning in Lasso & Ridge\nfrom sklearn.model_selection import GridSearchCV\n\n#three linear models used in the project\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Ridge\nfrom sklearn.linear_model import Lasso\n\n#StandardScaler for preprocessing the dataset\nfrom sklearn.preprocessing import StandardScaler\n\n#metrics to evaluate the linear regression\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import r2_score\nfrom sklearn.metrics import mean_absolute_error\n\n#import warnings to ignore any warnings during execution\nimport warnings\nwarnings.filterwarnings('ignore')","74469f47":"data=pd.read_csv(\"..\/input\/car-price-dataset\/CarPrice.csv\")\n","dea8525e":"data.head()","070e879a":"data.shape","25037c89":"data.isnull().sum()","a6ac4b9a":"data.info()","e23e689e":"data.describe(include=\"all\")","59ace187":"#data.drop(axis=0) by default so its important to specify the axis=1 else you can specify \n#columns=[\"car_ID\",\"CarName\"]\ndf_car=data.drop([\"car_ID\",\"CarName\"],\n         axis=1\n         )","2feb3c4e":"df_car.columns","11cc5084":"df_car.head()","99f27d29":"X=df_car.drop(columns=[\"price\"])\ny=df_car[\"price\"]","771fa9c0":"cat_col=[]#will store categorical features\nnum_col=[]#will store numerical features\n\n#iterating thourgh all columns in X\nfor col in X:\n    #append the features whose datatype is object in cat_col\n    if df_car[col].dtype==\"O\":\n        cat_col.append(col)\n    #append those features whose datatype is other than object in num_col    \n    else:\n        num_col.append(col)","16c918c6":"#dataframe to store the categorical features\ndf_cat=pd.DataFrame(\n    data=df_car,\n    #we will use the column names from the cat_col list\n    columns=cat_col\n)\n\n#dataframe to store the categorical features\ndf_num=pd.DataFrame(\n    data=df_car,\n    #we will use the column names from the num_col list\n    columns=num_col\n)","db4ae4f2":"df_num.head()","f7418ef1":"df_cat.head()","a622d846":"for cols in df_cat:\n    print(cols,\" contains :\",df_cat[cols].nunique(),\" labels\")","07b2a470":"df_cat=pd.get_dummies(\n    data=df_cat,\n    drop_first=True\n)","9f911ce4":"df_cat.head()","82ad4413":"car_final=pd.concat(\n    [df_num,df_cat,y],\n    axis=1\n)","0ade4007":"X=car_final.drop(\"price\",\n                axis=1)\ny=car_final[\"price\"]","ad0ba845":"X_train,X_test,y_train,y_test=train_test_split(\n    X,\n    y,\n    random_state=42,\n    test_size=0.2\n)","60cb1bed":"scaler=StandardScaler()\n\nX_train_scaled=scaler.fit_transform(X_train)\nX_test_scaled=scaler.transform(X_test)","b7ad4784":"#the function takes model, train & test split as an argument\ndef fit_model_getScores(model,X_train,y_train,X_test,y_test):\n    #fit the model with training dataset\n    model.fit(X_train,y_train)\n    \n    #score the training data\n    train_score=model.score(X_train,y_train)\n    #score the test data\n    test_score=model.score(X_test,y_test)\n    \n    #Display the scores\n    print(\"Scores of {}\".format(model),\"\\n\")\n    print(\"Training Score:{:.2f}\".format(train_score))\n    print(\"Testing Score:{:.2f}\".format(test_score))","8e7445ce":"#function takes model,and test data split as an argument \ndef get_metrics(model,X_test,y_test):\n    #calculate the predicted value of y \n    y_pred=model.predict(X_test)\n    mse=mean_squared_error(y_test,y_pred)#mse\n    r2__score=r2_score(y_test,y_pred)#r2_score\n    mae=mean_absolute_error(y_test,y_pred)#mae\n    rmse=mean_squared_error(y_test,y_pred,squared=False)#rmse\n    \n    #print the metrics \n    print(\"The Metrics for {}:\".format(model))\n    print(\"----------------------------\")\n    print(\"Mean Squared Error:{:.2f}\".format(mse))\n    print(\"Root Mean Squared Error:{:.2f}\".format(rmse))\n    print(\"Mean Absolute Error:{:.2f}\".format(mae))\n    print(\"r2_score:{:.2f}\".format(r2__score))\n","d65a5dff":"#the function takes model and independent dataframe as an argument\ndef return_coef_series(model,X):\n    #it will give the coefficeint pertaining to a specific linear model\n    coef=model.coef_\n    \n    #make a series out of coefficient with columns of X as an index \n    coef_series=pd.Series(\n        data=coef,\n        index=X.columns\n    )\n    \n    #return the series\n    return coef_series\n    ","0b80235e":"#takes coefficient of linear_model as an argument\ndef plot_coef(model_coef):\n    fig=plt.figure(figsize=(12,8))\n    model_coef.plot(\n        kind=\"bar\"\n    )\n    plt.xticks(rotation=90)","9f20a61c":"#takes model, user-defined hyper-parameters, train & test data splits as argument \ndef gridSearch(model,params,X_train,y_train):\n    grid=GridSearchCV(\n        estimator=model,\n        param_grid=params,\n        cv=5\n    )\n    grid.fit(X_train,y_train)\n   \n    return grid.best_params_","ff263b11":"#initializing the model\nlinear_model=LinearRegression()","b1eb061a":"fit_model_getScores(linear_model,\n                    X_train_scaled,y_train,\n                    X_test_scaled,y_test\n                   )","00da30af":"get_metrics(linear_model,\n            X_test_scaled,y_test\n           )","91482a63":"linear_coef=return_coef_series(linear_model,X).sort_values()\nlinear_coef","de0ac394":"plot_coef(linear_coef)","76c14d86":"ridge_model=Ridge()","5821c412":"params={\n    \"alpha\":[1e-9,1e-6,1e-3,1,100,1000,10000],\n    \"max_iter\":[1e3,1e4,1e5,1e6]#maximum number of iterations to run\n}\n\nridge_best_params=gridSearch(ridge_model,params,X_train_scaled,y_train)\nridge_best_params","70506962":"ridge1_model=Ridge(**ridge_best_params)","6de50fc1":"fit_model_getScores(ridge1_model,\n                    X_train_scaled,y_train,\n                    X_test_scaled,y_test\n                   )","6ae7550b":"get_metrics(ridge1_model,\n            X_test_scaled,y_test\n           )","94ab0036":"ridge_coef=return_coef_series(ridge1_model,X).sort_values()\nridge_coef","7319daaf":"plot_coef(ridge_coef)","a9c845d4":"lasso_model=Lasso()","4913b1b5":"params={\n    \"alpha\":[1e-9,1e-6,1e-3,1,100,1000,10000],\n    \"max_iter\":[1e3,1e4,1e5,1e6]#maximum number of iterations to run\n}\n\nlasso_best_params=gridSearch(lasso_model,params,X_train_scaled,y_train)\nlasso_best_params","71671936":"lasso1_model=Lasso(**lasso_best_params)","09382ea0":"fit_model_getScores(lasso1_model,\n                    X_train_scaled,y_train,\n                    X_test_scaled,y_test\n                   )","770bd1b3":"get_metrics(lasso1_model,\n            X_test_scaled,y_test\n           )","7ab405d2":"lasso_coef=return_coef_series(lasso1_model,X).sort_values()\nlasso_coef","3f3a95e1":"lasso_coef_df=pd.DataFrame(\n    data=lasso_coef,\n    columns=[\"Coefficient\"]\n)\nlasso_coef_df","965fdc7b":"features_used=lasso_coef_df[lasso_coef_df[\"Coefficient\"]==0]\nfeatures_used","cf4fb86c":"print(\"Total Features:{}\".format(X.shape[1]))\nprint(\"Features Neglected:{}\".format(features_used.shape[0]))\nprint(\"Features Used:{}\".format(X.shape[1]-features_used.shape[0]))","a460c770":"plot_coef(lasso_coef)","9ef724c1":"#specify the figure & size\nfig=plt.figure(figsize=(12,7))\n\n#plot the coefficient of individual linear models\nplt.plot(linear_model.coef_,'s',label=\"Linear Regression\")\nplt.plot(ridge1_model.coef_,'^',label=\"Ridge\")\nplt.plot(lasso1_model.coef_,'v',label=\"Lasso\")\n\n#specify columns\/features as the xticks\nplt.xticks(range(X.shape[1]), X.columns,rotation=90)\n\n#the length of horizontal line equals to the length of features\nplt.hlines(0,0,X.shape[1])\n\n#specify the x & y labels\nplt.xlabel(\"Features\")\nplt.ylabel(\"Coefficient magnitude\")\n\nplt.legend()\nplt.show()","216d09c9":"The function given below will help return the **metrics** used for **evaluating linear models** & that includes **mse,mae,rmse,r2_score**","f0b44d67":"The **plot_coef**() will help **visualize** the **coefficient** of a particular **linear model**","f4ba4cd1":"Display the head of **one-hot encoded** dataframe","8a7b436c":"Display head of Dataframe with numerical features","7c0ad369":"**describe()** describes the features of the dataframe by & default it will show the description of only int and float features but by specifying **include=\"all\"** we will be able to get the description of all features irrespective of its datatypes","4e14889b":"load the csv file ","8dfb43e2":"Lets see the magnitude of coefficients returned by the lasso model.\n\nThis is where it gets very intresting we can see that there are many features whose cofficients are reduced to zero. It means that the Lasso model have completely ignored those features with coefficients equals to zero while fitting the model.","cc0627ed":"I have dropped two features **car_ID** & **CarName** because they does not effect the price of the car in the dataset.","91de41e9":"Import necessary libraries for the project","5033b825":"Get the coefficeint series of ridge model\n\nFrom the values returned in series, we can see that the **coefficients** have been **reduced** to the fractions of its original value.\n\n**L2 regularization** technique in Ridge reduces the coefficient the features **as close to zero.**\n","edae71e1":"Fit ridge model with best parameters","8eff56f9":"Lets perform Hyperparameter tuning and fit the ridge model with the best parameters generated by GridSeachCV hypertuning method","14b2f373":"The below given function is used to perform h**yper-parameter tuning** for **Ridge()** and **Lasso()** regression.\n\n**GridSearchCV** is used for **hypertuning** and return the **best parameters fitting the linear model.**","8d501411":"Lets visualize the coefficient of LinearRegression() model","6458a6ac":"**2-Ridge():** It is a linear model which uses **L2 regularization** technique.\n\n**L2 Regularization:** Regularization techniques explicitly restricts a model to aviod overfitting.\n\n**LinearRegression()** does not allow us to control its complexity so its very likely that it will **overfit** the models when the dataset is **relatively small**.\n\n**l2 regularization** reduces the cofficient of the independent features to small magnitude as possible i.e all entries of **w should be close to zero**\n\n**Ridge** have **alpha parameter** which makes a trade-off between the simplicity of the model and its perfomance on training set & hence tuning it will yeild different model performance.","0b26c707":"Lets us see how many features have been used in the model and how many have been neglected by the Lasso model","ac52c35d":"Since the dataset does not contain any null values hence the df_num & df_cat will also have no null values, however we are intrested in df_cat whose datatype is object.\n\nTo feed our data to the Machine Learning Models the data values must be converted into numerical values.\n\n","e3b3f0da":"Print the **labels** of each columns in **df_cat**","ba98f0b8":"**Comparison between the three Linear Models:**\n\n* From the plot below comparing the coefficient of independent features, its clear that **LinearRegression()** model have most coefficients nonzero and are of large magnitude and most of its values are out of y-lim, Which are represented by blue square blocks.\n\n* The **Ridge()** model however have the coefficents whose magnitude are smaller and are close to zero, which are represented with orange **'^'**.\n\n* Comming to **Lasso()** model most of its values are lying either on the horizotal line and few which are very close to horizontal line, owing to its smaller magnitude represented by green **'v'**","a64e7797":"We got **alpha=100** & **max_iter=1000** for the lasso model \n\n","62bbac76":"Perform preprocessing on the **X_train** and **X_test** using **StandardScaler()**\n\nIt will scale the data values in such a way that the **mean is zero** and a **variance of one**","a52b42a3":"Get the **metrics** to evaluate LinearRegression","3e1be941":"The dataframes below shows the features as an indexes whose coefficient has been reduced to zero and are completely neglected by the Lasso model","71e00863":"Since I am comparing three linear models namely **LinearRegression() Ridge() & Lasso()**, I will have to write the same codes again and again for individual linear models so I have used **functions** that would perform the same job for all three regression models to **ease** and **shorten** my work","0fed6523":"The features in the dataset does not contain any null values","ab614280":"The function is used to fit the models in given linear model and return **training** and **testing** **scores**\n\n","de091219":"Now we have converted categorical features into **numerical** values by perfoming **one-hot** encoding & now we have all the features on both **df_num** & **df_cat** in numerical form so we **concatenate** them to get the final desired dataframe.","6a62215d":"Display the first five entries of dataframe","4130085f":"This function will calculate the **coefficient** of a given **linear model** and will return the **series of coefficient** with **independent features(columns) as an index**.\n\n**Note**: In a linear model the **numbers(count) of coefficient** is always **equal** to **the number of independent features** present in the dataset ","704351de":"info() will return the informations of columns(features),count of non-null values and datatype of individual columns","9c8a6da1":"Create dataFrames **df_cat** & **df_num** to store the features with datatypes **object** and **numerical** respectively","ed445a6b":"Perform **one-hot encoding** to the categorical features, using **pd.get_dummies()**\n\n","8d4b5d7c":"Lets try fitting the Lasso model using the parameters that have been returned from Hypertuning","eddceda9":"Print the metrics of lasso model","33360557":"We are now able to see the remaining columns in the dataset after dropping **car_ID** and **CarName**","4f8bb2e5":"display the first five entries of data","4c347299":"**1-LinearRegression() Model**\n\n**LinearRegression(aka ordinary least squares):** Simplest & most classic linear method for regression. It finds the parameters w & b that minimize the mean squard error between predicted value and true value.\n\ny=wx + b\n\nw->Weights associated with individual independent features(Slope of a line)\n\nb->y intercept\n\n","ed7f4873":"Fit the ridge model and return the **test** and **train** scores\n\nFitting the model we get scores equivalent to the LinearRegression()","a3b4fa94":"The dataset contains 205 rows with 26 features","ffd797c7":"The function **return_coef_series** will return the series of **coefficient** along with **features** as its **index**.","22df4495":"The scores are **93% for training set** and **86% for testing set** which is better generalized model than the above two models i.e LinearRegression() & Ridge()","f819e9c2":"Lets fit the LinearRegression and fetch **training** and **testing** scores","2142b7a3":"Visualize the coefficient series of ridge model\n\nWe can see that the upper & lower x-limmits have been reduced.","9502e420":"Lets create Dataframe that stores the features with its corresponding coefficient values","c6dc1b56":"Its is very clear from the plot that many of the features are neglected and its bar are being reduced to 0 magnitude, and hence lasso is moslty used for automatic feature selection.","4f135c52":"**3-Lasso():** It is a linear model which uses **L1 regularization** technique.\n\n**l1 regularization** also reduces the coefficient magnitude however unlike Ridge it **reduces magnitude of some of the features to zero**. Hence it **neglects** some of the features completely.\n\nHence it is also used for **automatic feature selection** as it ignores some of the features.\n\n**Lasso** also have **alpha parameter** which makes a trade-off between the simplicity of the model and its perfomance on training set & hence tuning it will yeild different model performance.","1b47e91d":"Lasso model have used 26 features out of 43 and have neglected 17 features","357a5af1":"Specify two empty lists **cat_col** & **num_col** to store **categorical** and **numerical** columns respectively","15d91367":"Split the data into **training** and **testing** data, with **test data** of size of **20%** of total dataset.","54e4ea07":"GridSeachCV gave us **alpha=1** and **max_iter=1000** as the best parameters for the model","a08fe033":"**BUILDING LINEAR MODELS**","bed6510a":"**EXPLORATORY DATA ANALYSIS(EDA)**","5724af5b":"Get the metrics to evaluate ridge model","0a85b57d":"By dividing the dataframe into **numerical** & **categorical** features seperately, it will allow an ease handling of numerical and categorical features in their respective dataframes.","e120862a":"Split the data into **dependent(y)** and **independent(X)** variables","b74c6a02":"Split the dataset into **dependent(y)** & **independent(X)** variables,\n\nDependent variable is also called the target variable which is **price** of the car in our case","ea00646d":"Display head of Dataframe with categorical features"}}