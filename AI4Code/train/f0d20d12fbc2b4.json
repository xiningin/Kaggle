{"cell_type":{"e15c7d42":"code","70ede76f":"code","12dc8aa7":"code","94439a23":"code","98d0f163":"code","820b93ed":"code","73c53a34":"code","3d87d4e2":"code","71250bc2":"code","3f06aa08":"code","b0016f8b":"code","842eb1aa":"code","b51f2a9f":"code","5a894168":"code","b405a2de":"code","fdfcd6aa":"code","4a0280c8":"code","e729279a":"code","a380164c":"code","cc9e16db":"code","7f6b43b1":"code","aa23bbf1":"code","f43cfd7e":"code","f9855330":"code","cf20c158":"code","c77541fb":"code","15d2a9c8":"code","38c5fe52":"code","f5218571":"code","e2447606":"code","d068c2fa":"code","5ea8dba1":"code","400cb026":"code","fd5e17f8":"code","a11da3b9":"code","59e7b4e8":"code","b43ad30e":"code","3fc740d4":"code","6e9d5334":"code","d6da8701":"code","c525755c":"code","7fe0f701":"code","7c297ca3":"code","0a44758d":"code","b60cda7b":"code","032f9379":"code","01b92ba3":"code","3fe5d44d":"code","912e480c":"code","fed17f5a":"code","ed1e1bf9":"code","4904252a":"markdown","18cc3e62":"markdown","4a0d4206":"markdown","6edd0919":"markdown","35d205d8":"markdown","3adbdf3f":"markdown","82d52c29":"markdown","e837878e":"markdown","cf600ffa":"markdown","1b190b6c":"markdown","3b590730":"markdown","c1195af1":"markdown","9cc6d78d":"markdown","5ba60c2c":"markdown","d4f81b6a":"markdown","030899b4":"markdown","e02e0eb8":"markdown","04caf29d":"markdown","7ac78123":"markdown","1bd78a41":"markdown","f896e814":"markdown","1ada68d1":"markdown","62ca9b6a":"markdown","717a50c5":"markdown","e7c02954":"markdown","622249f4":"markdown","d652527f":"markdown","4f1a4352":"markdown","0d8a7d44":"markdown","d235363c":"markdown","c4bbadee":"markdown","6fb58403":"markdown","1ac3b5ef":"markdown","ec06e04e":"markdown","a2ce395e":"markdown","591e5ac0":"markdown","4e35bada":"markdown","494755d0":"markdown","a74a85bc":"markdown","257ffadb":"markdown","1dea56e2":"markdown","34563d35":"markdown","07c9dc18":"markdown","d97672d8":"markdown","25bc398d":"markdown","0104a266":"markdown","beea0ed0":"markdown"},"source":{"e15c7d42":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import confusion_matrix, classification_report, roc_auc_score, roc_curve, precision_recall_curve, f1_score, accuracy_score, recall_score, precision_score\nfrom sklearn.manifold import TSNE","70ede76f":"dataset = pd.read_csv(\"..\/input\/creditcardfraud\/creditcard.csv\")\ndataset.head()","12dc8aa7":"dataset.Class.value_counts()","94439a23":"dataset.shape","98d0f163":"dataset.info()","820b93ed":"dataset.describe()","73c53a34":"dataset.isna().sum()","3d87d4e2":"# Plot Fraud vs Non-fraud cases\nplt.figure(figsize=(10,5))\nax = dataset.Class.value_counts().plot(kind = 'bar')\nplt.xlabel(\"Fraud vs Non-fraud cases\")\nplt.ylabel(\"Count\")\nplt.title(\"Fraud vs Non-fraud cases Count\")","71250bc2":"fig, ax = plt.subplots(1, 2, figsize=(20,5))\n\nsns.distplot(dataset['Amount'].values, ax=ax[0], color='g')\nax[0].set_title('Amount Distribution')\nax[0].set_xlim([min(dataset['Amount'].values), max(dataset['Amount'].values)])\n\nsns.distplot(dataset['Time'].values, ax=ax[1], color='b')\nax[1].set_title('Time Distribution')\nax[1].set_xlim([min(dataset['Time'].values), max(dataset['Time'].values)])\n\nplt.show()","3f06aa08":"# Heatmap to find any high correlations\nplt.figure(figsize=(20,10))\nsns.heatmap(data=dataset.corr(), cmap=\"seismic\")\nplt.show()","b0016f8b":"X = dataset.drop([\"Class\"], axis = 1)\ny = dataset[\"Class\"]","842eb1aa":"tsne = TSNE(n_components=2, random_state=0)\nX_tsne = tsne.fit_transform(X.values)","b51f2a9f":"import matplotlib.patches as mpatches\nblue_patch = mpatches.Patch(color='#0A0AFF', label='No Fraud')\nred_patch = mpatches.Patch(color='#AF0000', label='Fraud')\nplt.figure(figsize=(20,10))\nplt.scatter(X_tsne[:,0], X_tsne[:,1], c=(y == 0), cmap='coolwarm', label='No Fraud', linewidths=2)\nplt.scatter(X_tsne[:,0], X_tsne[:,1], c=(y == 1), cmap='coolwarm', label='Fraud', linewidths=2)\nplt.title('t-SNE')\nplt.legend(handles=[blue_patch, red_patch])","5a894168":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)","b405a2de":"X_trainA = X_train.drop([\"Time\"], axis = 1)\nX_testA = X_test.drop([\"Time\"], axis = 1)","fdfcd6aa":"from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train = sc.fit_transform(X_trainA)\nX_test = sc.transform(X_testA)","4a0280c8":"print(y_train[y_train.values == 0].shape[0])\nprint(y_train[y_train.values == 1].shape[0])\nprint(y_test[y_test.values == 0].shape[0])\nprint(y_test[y_test.values == 1].shape[0])","e729279a":"def fit_and_predict(classifier, X_train, X_test, y_train, y_test):\n    classifier.fit(X_train, y_train)\n    ypred = classifier.predict(X_test)\n    \n    print(\"Accuracy Score:\", accuracy_score(y_test, ypred))\n    print(\"Recall Score:\", recall_score(y_test, ypred))\n    print(\"Precision Score:\", precision_score(y_test, ypred))\n    \n    print(\"\\n*********Confusion Matrix*********\")\n    cm = confusion_matrix(y_test, ypred)\n    print(cm)\n    fig= plt.figure(figsize=(10,5))\n    sns.heatmap(cm,cmap=\"coolwarm_r\",annot=True,linewidths=0.5)\n    plt.title(\"Confusion_matrix\")\n    plt.xlabel(\"Predicted_class\")\n    plt.ylabel(\"Real class\")\n    plt.show()\n    print(\"\\n*********Classification Report*********\")\n    print(classification_report(y_test, ypred))\n    \n    test_df = pd.DataFrame(X_test, columns = X.columns[1:])\n    test_df['Actual'] = y_test.values\n    test_df['Predicted'] = ypred\n    test_df.head()\n    tp = test_df[(test_df['Actual'] == 1) & (test_df['Predicted'] == 1)].shape[0]\n    actual_positive = test_df[(test_df['Actual'] == 1)].shape[0]\n    print(\"True Positives: \", tp)\n    print(\"Accuracy for fraud cases: \", (tp \/ actual_positive))\n    print(\"ROC AUC Score: \", roc_auc_score(y_test, ypred))\n    return (y_test, ypred)\n#     return roc_curve(y_test, ypred)","a380164c":"from sklearn.linear_model import LogisticRegression\nlr = LogisticRegression()\ny_test, ypred = fit_and_predict(lr, X_train, X_test, y_train, y_test)\nlr_fp, lr_tp, lr_threshold = roc_curve(y_test, ypred)\nlr_precision, lr_recall, _ = precision_recall_curve(y_test, ypred)","cc9e16db":"from sklearn.tree import DecisionTreeClassifier\ndtree = DecisionTreeClassifier()\ny_test, ypred = fit_and_predict(dtree, X_train, X_test, y_train, y_test)\ndtree_fp, dtree_tp, dtree_threshold = roc_curve(y_test, ypred)\ndtree_precision, dtree_recall, _ = precision_recall_curve(y_test, ypred)\n# print(\"Threshold:\", dtree_threshold)","7f6b43b1":"from sklearn.ensemble import RandomForestClassifier\nrf = RandomForestClassifier(n_estimators=100)\ny_test, ypred = fit_and_predict(rf, X_train, X_test, y_train, y_test)\nrf_fp, rf_tp, rf_threshold = roc_curve(y_test, ypred)\nrf_precision, rf_recall, _ = precision_recall_curve(y_test, ypred)\n# print(\"Threshold:\", rf_threshold)","aa23bbf1":"from sklearn.ensemble import AdaBoostClassifier\nadb = AdaBoostClassifier(n_estimators = 100, random_state = 0)\ny_test, ypred = fit_and_predict(adb, X_train, X_test, y_train, y_test)\nadb_fp, adb_tp, adb_threshold = roc_curve(y_test, ypred)\nadb_precision, adb_recall, _ = precision_recall_curve(y_test, ypred)","f43cfd7e":"from xgboost import XGBClassifier\nxgb = XGBClassifier(random_state = 0)\ny_test, ypred = fit_and_predict(xgb, X_train, X_test, y_train, y_test)\nxgb_fp, xgb_tp, xgb_threshold = roc_curve(y_test, ypred)\nxgb_precision, xgb_recall, _ = precision_recall_curve(y_test, ypred)","f9855330":"plt.figure(figsize=(20,10))\nplt.plot([0, 1], [0, 1], linestyle = \"--\")\nplt.plot(lr_fp, lr_tp, color=\"red\", label =\"Logistic Regression\")\nplt.plot(dtree_fp, dtree_tp, color=\"green\", label = \"Decision Tree\")\nplt.plot(rf_fp, rf_tp, color=\"blue\", label = \"Random Forest\")\nplt.plot(adb_fp, adb_tp, color=\"orange\", label = \"AdaBoost\")\nplt.plot(xgb_fp, xgb_tp, color=\"cyan\", label = \"XGBoost\")\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.title(\"ROC Curve\")\nplt.legend()","cf20c158":"fig = plt.figure(figsize=(20,10))\nplt.step(lr_recall, lr_precision, marker='.', color='red', label =\"Logistic Regression\")\nplt.step(dtree_recall, dtree_precision, marker='.', color='green', label = \"Decision Tree\")\nplt.step(rf_recall, rf_precision, marker='.', color='blue', label = \"Random Forest\")\nplt.step(adb_recall, adb_precision, marker='.', color='orange', label = \"AdaBoost\")\nplt.step(xgb_recall, xgb_precision, marker='.', color='cyan', label = \"XGBoost\")\n\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.title('Precision-Recall curve')\nplt.legend()","c77541fb":"print(X_trainA.shape, y_train.shape)\nprint(X_testA.shape, y_test.shape)","15d2a9c8":"X_train1 = X_trainA\nX_train1['Class'] = y_train\nX_train1.head()","38c5fe52":"X_train_0_class, X_train_1_class = X_train1.Class.value_counts()\nprint(X_train_0_class, X_train_1_class)","f5218571":"X_train1_0_df = X_train1[X_train1['Class']==0]\nX_train1_1_df = X_train1[X_train1['Class']==1]\nprint(X_train1_0_df.shape, X_train1_1_df.shape)","e2447606":"X_train1_1_df = X_train1_1_df.sample(X_train_0_class, replace=True, random_state=0)\nprint(X_train1_0_df.shape, X_train1_1_df.shape)","d068c2fa":"X_train1 = pd.concat([X_train1_0_df, X_train1_1_df])\nX_train1.shape","5ea8dba1":"print(X_train1[X_train1['Class']==0].shape)\nprint(X_train1[X_train1['Class']==1].shape)","400cb026":"X_trainB = X_train1.drop(\"Class\", axis =1)\ny_trainB = X_train1[\"Class\"]","fd5e17f8":"print(X_trainB.shape, y_trainB.shape)\nprint(X_testA.shape, y_test.shape)","a11da3b9":"X_trainB = sc.fit_transform(X_trainB)\nX_testA = sc.transform(X_testA)","59e7b4e8":"from sklearn.linear_model import LogisticRegression\nlr = LogisticRegression()\ny_test, ypred = fit_and_predict(lr, X_trainB, X_testA, y_trainB, y_test)\nlr_fp, lr_tp, lr_threshold = roc_curve(y_test, ypred)\nlr_precision, lr_recall, _ = precision_recall_curve(y_test, ypred)","b43ad30e":"from sklearn.tree import DecisionTreeClassifier\ndtree = DecisionTreeClassifier()\ny_test, ypred = fit_and_predict(dtree, X_trainB, X_testA, y_trainB, y_test)\ndtree_fp, dtree_tp, dtree_threshold = roc_curve(y_test, ypred)\ndtree_precision, dtree_recall, _ = precision_recall_curve(y_test, ypred)","3fc740d4":"from sklearn.ensemble import RandomForestClassifier\nrf = RandomForestClassifier(n_estimators=100)\ny_test, ypred = fit_and_predict(rf, X_trainB, X_testA, y_trainB, y_test)\nrf_fp, rf_tp, rf_threshold = roc_curve(y_test, ypred)\nrf_precision, rf_recall, _ = precision_recall_curve(y_test, ypred)","6e9d5334":"from sklearn.ensemble import AdaBoostClassifier\nadb = AdaBoostClassifier(n_estimators = 100, random_state = 0)\ny_test, ypred = fit_and_predict(adb, X_trainB, X_testA, y_trainB, y_test)\nadb_fp, adb_tp, adb_threshold = roc_curve(y_test, ypred)\nadb_precision, adb_recall, _ = precision_recall_curve(y_test, ypred)","d6da8701":"from xgboost import XGBClassifier\nxgb = XGBClassifier(random_state = 0)\ny_test, ypred = fit_and_predict(xgb, X_trainB, X_testA, y_trainB, y_test)\nxgb_fp, xgb_tp, xgb_threshold = roc_curve(y_test, ypred)\nxgb_precision, xgb_recall, _ = precision_recall_curve(y_test, ypred)","c525755c":"import keras\nfrom keras.layers import Dense\nfrom keras.models import Sequential","7fe0f701":"classifier = Sequential()\nclassifier.add(Dense(units=16, activation=\"relu\", input_dim=29))\nclassifier.add(Dense(units=2, activation=\"softmax\"))\nclassifier.summary()","7c297ca3":"classifier.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])","0a44758d":"classifier.fit(X_trainB, y_trainB, batch_size=10, epochs=20)","b60cda7b":"ypred_nn = classifier.predict_classes(X_testA)\nprint(confusion_matrix(y_test, ypred_nn))\nprint(classification_report(y_test, ypred_nn))\nprint(\"Accuracy Score:\", accuracy_score(y_test, ypred_nn))\n\ntest_df = pd.DataFrame(X_test, columns = X.columns[1:])\ntest_df['Actual'] = y_test.values\ntest_df['Predicted'] = ypred_nn\ntest_df.head()\ntp = test_df[(test_df['Actual'] == 1) & (test_df['Predicted'] == 1)].shape[0]\nactual_positive = test_df[(test_df['Actual'] == 1)].shape[0]\nprint(\"True Positives: \", tp)\nprint(\"Accuracy for fraud cases: \", (tp \/ actual_positive))\nprint(\"ROC AUC Score: \", roc_auc_score(y_test, ypred_nn))\nnn_fp, nn_tp, nn_threshold = roc_curve(y_test, ypred_nn)\nnn_precision, nn_recall, _ = precision_recall_curve(y_test, ypred_nn)","032f9379":"classifier.evaluate(X_testA, y_test)","01b92ba3":"plt.figure(figsize=(20,10))\nplt.plot([0, 1], [0, 1], linestyle = \"--\")\nplt.plot(lr_fp, lr_tp, color=\"red\", label =\"Logistic Regression\")\nplt.plot(dtree_fp, dtree_tp, color=\"green\", label = \"Decision Tree\")\nplt.plot(rf_fp, rf_tp, color=\"blue\", label = \"Random Forest\")\nplt.plot(adb_fp, adb_tp, color=\"orange\", label = \"AdaBoost\")\nplt.plot(xgb_fp, xgb_tp, color=\"cyan\", label = \"XGBoost\")\nplt.plot(nn_fp, nn_tp, color=\"purple\", label = \"Neural Networks\")\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.title(\"ROC Curve\")\nplt.legend()","3fe5d44d":"fig = plt.figure(figsize=(20,10))\nplt.step(lr_recall, lr_precision, marker='.', color='red', label =\"Logistic Regression\")\nplt.step(dtree_recall, dtree_precision, marker='.', color='green', label = \"Decision Tree\")\nplt.step(rf_recall, rf_precision, marker='.', color='blue', label = \"Random Forest\")\nplt.step(adb_recall, adb_precision, marker='.', color='orange', label = \"AdaBoost\")\nplt.step(xgb_recall, xgb_precision, marker='.', color='cyan', label = \"XGBoost\")\nplt.step(nn_recall, nn_precision, marker='.', color=\"purple\", label = \"Neural Networks\")\n\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.title('Precision-Recall curve')\nplt.legend()","912e480c":"X_trainA.columns[:-1]","fed17f5a":"feature_importance_df = pd.DataFrame(X_trainA.columns[:-1], columns=[\"Feature\"])\nfeature_importance_df[\"Importance\"] = rf.feature_importances_\nfeature_importance_df.sort_values('Importance', ascending=False, inplace=True)\nfeature_importance_df = feature_importance_df.head(20)\nfeature_importance_df","ed1e1bf9":"plt.figure(figsize=(15,5))\nax = feature_importance_df['Feature']\nplt.bar(range(feature_importance_df.shape[0]), feature_importance_df['Importance']*100)\nplt.xticks(range(feature_importance_df.shape[0]), feature_importance_df['Feature'], rotation = 20)\nplt.xlabel(\"Features\")\nplt.ylabel(\"Importance\")\nplt.title(\"Plot Feature Importances\")","4904252a":"With imbalanced dataset we are still getting satisfactory results.\nAnd it shows that the Decison Tree and Random Forest both are giving almost same results.\nBut lets see how it reacts when we balance the dataset.","18cc3e62":"**Adaboost**\n<a id = \"balanced-adaboost\"><\/a>","4a0d4206":"**Logistic Regression**\n<a id = \"balanced-lr\"><\/a>","6edd0919":"# Table of Contents\n\n1. [Import Libraries](#import-libraries)\n2. [Read Data](#read-data)\n3. [Understand the data](#understand-data)\n    * [Check missing values](#check-missing-values)\n4. [Exploratory Data Analysis](#eda)\n    * [Observations from Data](#data-observations)\n5. [Label Data](#label-data)\n6. [Cluster data using Dimensionality reduction](#cluster-data)\n7. [Split into train and test sets](#split-data)\n8. [Scaling](#scaling)\n9. [Predictive Analysis on unbalanced data](#unbalanced-predictive-analysis)\n    * [Logistic regression](#unbalanced-lr)\n    * [Decision Tree](#unbalanced-dt)\n    * [Random Forest](#unbalanced-rf)\n    * [Adaboost](#unbalanced-adaboost)\n    * [XGBoost](#unbalanced-xgboost)\n10. [Validate Unbalanced Data](#validate-unbalanced)\n    * [ROC Curve](#unbalanced-roc)\n    * [Precision-Recall Curve](#unbalanced-pr)\n11. [Balance Data using oversampling method](#balance-data)\n    * [Understand Balanced Data](#understand-balanced-data)\n12. [Predictive Analysis on Balanced Data](#balanced-predictive-analysis)\n    * [Logistic Regression](#balanced-lr)\n    * [Decision Tree](#balanced-dt)\n    * [Random Forest](#balanced-rf)\n    * [Adaboost](#balanced-adaboost)\n    * [XGBoost](#balanced-xgboost)\n    * [Neural Networks](#balanced-nn)\n13. [Validate Balanced Data](#validate-balanced)\n    * [ROC Curve](#balanced-roc)\n    * [Precision-Recall Curve](#balanced-pr)\n14. [Feature Importance](#feature-importance)\n15. [Conclusion](#conclusion)\n","35d205d8":"# Fraud Detection\n\nFraud Detection is a technique used to identify unusual patterns that are different from the rest of the population and not behaving as expected. These unusual patterns are also called as outliers. \n\nThe fraud detection involves in-depth data analysis\/data-mining to recognize the unusual patterns.\nIn this dataset, most of the data analysis part is already done and most of the features are scaled. The names of the features are not shown due to privacy reasons.\n\nHence our main focus will be to balance the data and perform predective analysis.","3adbdf3f":"**Understand Balanced Data**\n<a id =\"understand-balanced-data\"><\/a>","82d52c29":"# Validate Unbalanced Data\n<a id = \"validate-unbalanced\"><\/a>","e837878e":"# Split into train and test sets\n<a id = \"split-data\"><\/a>","cf600ffa":"Distributed plot above for Amount and Time shows how skewed these features are. So we have to sclae them as well. We will do scaling further in the notebook.","1b190b6c":"# Goals\n\nGoal here is to identify as much fraudulent credit card transactions as possible. And as mentioned in the dataset insperation, I will calculate the accuracy using the Area Under the Precision-Recall Curve (AUPRC). Confusion matrix accuracy is not meaningful for unbalanced classification.\n","3b590730":"**XGBoost**\n<a id = \"unbalanced-xgboost\"><\/a>","c1195af1":"# Read Data\n<a id = \"read-data\"><\/a>\nDataSet : The dataset that is used for Credit Card Fraud Detection is derived from the Kaggle URL :\n\nhttps:\/\/www.kaggle.com\/mlg-ulb\/creditcardfraud","9cc6d78d":"**ROC Curve**\n<a id = \"balanced-roc\"><\/a>","5ba60c2c":"The count plot above shows how unbalanced the data set is where most of the transactions are non-fraud. If we train model with these transactions, then algorithm will also assume that most of the transactions are non-fraud. But we don't want this, instead we want algorithm to find patterns for both fraud and non-fraud cases. Hence we have to balance fraud and non-fraud cases equally. We will do this step further in the notebook.","d4f81b6a":"# Exploratory Data Analysis\n<a id = \"eda\"><\/a>","030899b4":"**ROC Curve**\n<a id = \"unbalanced-roc\"><\/a>","e02e0eb8":"# Predictive Analysis on Balanced Data\n<a id = \"balanced-predictive-analysis\"><\/a>","04caf29d":"# Scaling\n<a id = \"scaling\"><\/a>","7ac78123":"<div align='center'><font size=\"4\" color=\"grey\"><b>Credit Card Fraud<\/b><\/font><br>\nEvery year, millions of people fall victim to fraud that costs the global economy billions of dollars. If you're a victim, it can wreak havoc on your personal finances. Luckily, due to some modern fraud detection techniques many financial institutions have measures in place to help protect you from credit fraud.<br><br>\n\n![credit-card-fraud.jpg](attachment:credit-card-fraud.jpg)","1bd78a41":"**Precision-Recall Curve**\n<a id = \"balanced-pr\"><\/a>","f896e814":"# Problem Statement\n\nThe Credit Card Fraud Detection dataset contains transactions made by credit cards in September 2013 by european cardholders.\nThis dataset presents transactions that occurred in two days, where we have 492 frauds out of 284,807 transactions. The dataset is highly unbalanced, the positive class (frauds) account for 0.172% of all transactions.\n","1ada68d1":"\nIt is very imbalanced data set, but first check how it is performing.","62ca9b6a":"**Adaboost**\n<a id = \"unbalanced-adaboost\"><\/a>","717a50c5":"# Understand the data\n<a id = \"understand-data\"><\/a>","e7c02954":"**Decision Tree**\n<a id = \"balanced-dt\"><\/a>","622249f4":"**Precision-Recall Curve**\n<a id = \"unbalanced-pr\"><\/a>","d652527f":"# Conclusion\n<a id = \"conclusion\"><\/a>","4f1a4352":"# Label Data\n<a id = \"label-data\"><\/a>","0d8a7d44":"After investigating and visualizing through the data, we found that the data is unbalanced. So we balanced the data. <br>\nAfter balancing training data, we trainied it and these were the results-<br>\n\n<b>Logistic Regression<\/b> is giving 98% accuracy, 91% recall and 6% precision. Accuracy for fraud cases is 91% and ROC AUC Score is 94%<br>\n<b>Decision Tree<\/b> is giving 99% accuracy, 69% recall and 76% precision. Accuracy for fraud cases is 69% and ROC AUC Score is 85%<br>\n<b>Random Forest<\/b> is giving 99% accuracy, 78% recall and 95% precision. Accuracy for fraud cases is 78% and ROC AUC Score is 89%<br>\n<b>AdaBoost<\/b> is giving 99% accuracy, 86 % recall and 18% precision. Accuracy for fraud cases is 86% and ROC AUC Score is 93%<br>\n<b>XGBoost<\/b> is giving 99% accuracy, 83 % recall and 92% precision. Accuracy for fraud cases is 83% and ROC AUC Score is 91%<br>\n<b>Neural Networks<\/b> is giving 99% accuracy, 82 % recall and 44% precision. Accuracy for fraud cases is 82% and ROC AUC Score is 91%\n\nWith Neural Networks on balanced dataset, its giving 86% accuracy on Fraud Cases.\nBut still for Fraud cases the winner is Logistic Regression with 91% accuracy on Fraud Cases. But logistic regression is giving more False Positives and the precision is less. Boosting model - Adaboost is next consideration with accuracy of 93%.\nIf we want to give precision the preference, then XGBoost can be considered, and accuracy can be improved twiking some parameters.<br><br>\n**Hence it depemds on the accuracy parameter we want to consider, but for the unbalanced data most of the time consideration is given to ROC AUC score so Logistic Regression is winner here, as its giving most True Positives. Specially, in cases like fraud transactions False Positives can be fine, but we can't compromise with the True Positives and leave the fraudent transactions happen.**","d235363c":"**Random Forest**\n<a id = \"unbalanced-rf\"><\/a>","c4bbadee":"**Check missing values**\n<a id = \"check-missing-values\"><\/a>","6fb58403":"**Don't forget to upvote if you like the kernel. Its free :)**","1ac3b5ef":"# Import Libraries\n<a id = \"import-libraries\"><\/a>","ec06e04e":"# Balance Data using oversampling method\n<a id = \"balance-data\"><\/a>","a2ce395e":"**Random Forest**\n<a id = \"balanced-rf\"><\/a>","591e5ac0":"**XGBoost**\n<a id = \"balanced-xgboost\"><\/a>","4e35bada":"Get most important features and their contribution in model","494755d0":"**Decision Tree**\n<a id = \"unbalanced-dt\"><\/a>","a74a85bc":"**Neural Networks**\n<a id = \"balanced-nn\"><\/a>","257ffadb":"<hr>","1dea56e2":"**Observations from Data-**\n<a id = \"data-observations\"><\/a>\n\nThe data set is very imbalanced with only 492 Fraud records and 284315 Non-fraud records.\nThe dataset consists of numerical values from V1 to V28, which looks transformed from real data.\nBut 'Time' and 'Amount' features are not transformed. So we need scaling of dataset.\nThere is no missing value\/NaNs in the dataset.\nUsing this data as it is with balancing it might overfit, but for comparison,we will first start with imbalanced data and see the results. Then we will balance the data and check the results again.","34563d35":"# Validate Balanced Data\n<a id = \"validate-balanced\"><\/a>","07c9dc18":"# Predictive Analysis on unbalanced data\n<a id =\"unbalanced-predictive-analysis\"><\/a>","d97672d8":"# Cluster data using Dimensionality reduction\n<a id = \"cluster-data\"><\/a>","25bc398d":"We will use **t-SNE** algorithm, which will accurately cluster the fraud vs non-fraud cases in the dataset.","0104a266":"# Feature Importance\n<a id = \"feature-importance\"><\/a>","beea0ed0":"**Logistic regression**\n<a id = \"unbalanced-lr\"><\/a>"}}