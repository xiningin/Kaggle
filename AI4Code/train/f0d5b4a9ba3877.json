{"cell_type":{"ebb30ab8":"code","76fd71ad":"code","5ec98bfe":"code","e64952cf":"code","907c3674":"code","53bca4a6":"code","54765d83":"code","abaa2c92":"code","b396e405":"code","ca0a3bc2":"code","bc3f4a36":"code","9be5ea89":"code","272675ec":"code","3762dcbf":"code","dbfd82d8":"code","ef0f7222":"code","fe12e57e":"code","0c87910a":"code","1b81a4bc":"code","dd43325c":"code","9e2807b8":"code","7b214422":"code","c3a82210":"code","e9e7581d":"code","483d265e":"code","2a1e56d6":"code","2772ac9c":"code","0a8d9af8":"code","a18265b9":"code","4c72b4e2":"code","cdd7fccc":"code","3e84f95d":"code","0dc37faf":"code","79119511":"code","7cdba98e":"code","38b28aeb":"code","14c6d438":"code","6ba5a507":"code","8b341042":"code","34a5d4a6":"markdown","80ab4a64":"markdown","31700aaf":"markdown","ebc90513":"markdown","a8dcf0b1":"markdown","da951334":"markdown","b6539bbd":"markdown","fe99a1b5":"markdown","71847c1f":"markdown","04dffdbe":"markdown","b76255d1":"markdown","1a4ba27f":"markdown","6e0b03bb":"markdown","2522beeb":"markdown","b01cf756":"markdown","d7e16a07":"markdown","ebdff51d":"markdown","d98a44e1":"markdown","d96a0b9c":"markdown","81c78588":"markdown","37b6279b":"markdown","077fafdd":"markdown","a8c83be0":"markdown","e5225ca7":"markdown","a66ef195":"markdown","a68d5c3a":"markdown","b9b1b6ab":"markdown","6e64e900":"markdown","d25d5fd1":"markdown","4ace0e83":"markdown","e508e385":"markdown","2caf15c2":"markdown","fc778abf":"markdown","31e625a9":"markdown","b01d5e3f":"markdown","6c23b4c3":"markdown","fe01a749":"markdown","cde326bd":"markdown","7b794628":"markdown","5b4b31af":"markdown","40635caf":"markdown","1faf6e73":"markdown"},"source":{"ebb30ab8":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns","76fd71ad":"original_data = pd.read_csv('\/kaggle\/input\/mytitanic\/train.csv')\n\noriginal_data.head(10)","5ec98bfe":"original_data.isnull().sum()","e64952cf":"from sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\n\n\ndef run_and_validate(numeric_data, labels):\n    X_train, X_test, y_train, y_test = train_test_split(numeric_data, labels, test_size=0.33, random_state=42)\n\n    rfc = RandomForestClassifier(n_estimators=200, max_depth=8, min_samples_split=2, min_samples_leaf=1, \n                                        min_weight_fraction_leaf=0.0, random_state=20)\n    rfc.fit(X_train, y_train)\n    X_train_preds_rfc = rfc.predict(X_train)\n    X_test_preds_rfc = rfc.predict(X_test)\n\n    print(\"Train acc: %0.3f\" % accuracy_score(y_train, X_train_preds_rfc))\n    print(\"Test acc: %0.3f\" % accuracy_score(y_test, X_test_preds_rfc))","907c3674":"g = sns.catplot(x=\"SibSp\", y=\"Survived\", data=original_data, kind=\"bar\", palette=\"muted\")\ng = g.set_ylabels(\"survival probability\")","53bca4a6":"g  = sns.catplot(x=\"Parch\", y=\"Survived\", data=original_data, kind=\"bar\", palette=\"muted\")\ng = g.set_ylabels(\"survival probability\")","54765d83":"g  = sns.catplot(x=\"Pclass\", y=\"Survived\", data=original_data, kind=\"bar\", palette=\"muted\")\ng = g.set_ylabels(\"survival probability\")","abaa2c92":"data = original_data[['SibSp', 'Parch', 'Pclass']].copy()\nlabels = original_data['Survived'].copy()\n\nprint(data.head(5), '\\n')\nrun_and_validate(data, labels)","b396e405":"fig, axes = plt.subplots(1, 3, sharex=True, figsize=(15,5))\naxes[0].set_title('All')\nsns.histplot(original_data['Age'], kde=True, ax=axes[0], color='cornflowerblue')\n\naxes[1].set_title('Survived')\nsns.histplot(original_data[original_data['Survived'] == 1]['Age'], kde=True, ax=axes[1], color='g')\n\naxes[2].set_title('Not survived')\nsns.histplot(original_data[original_data['Survived'] == 0]['Age'], kde=True, ax=axes[2], color='r')","ca0a3bc2":"data['Age'] = original_data['Age'].fillna(original_data['Age'].median())\n\nprint(data.head(5), '\\n')\nrun_and_validate(data, labels)","bc3f4a36":"original_data[\"Fare\"]","9be5ea89":"# Your code here\n\n# End\n\nprint(data.head(5), '\\n')\nrun_and_validate(data, labels)","272675ec":"g = sns.histplot(data['Fare'], kde=True, color=\"m\", label=\"Skewness : %.2f\"%(data[\"Fare\"].skew()))\ng = g.legend(loc=\"best\")","3762dcbf":"# Your code here\n\n# End\n\nprint(data.head(5), '\\n')\nrun_and_validate(data, labels)","dbfd82d8":"g = sns.histplot(data['Fare'], kde=True, color=\"m\", label=\"Skewness : %.2f\"%(data[\"Fare\"].skew()))\ng = g.legend(loc=\"best\")","ef0f7222":"f,ax = plt.subplots(figsize=(15, 12))\nsns.heatmap(original_data.corr(), annot=True, linewidths=0.5, fmt='.2f',ax=ax)","fe12e57e":"g = sns.barplot(x=\"Sex\",y=\"Survived\",data=original_data)\ng = g.set_ylabel(\"Survival Probability\")","0c87910a":"original_data[[\"Sex\",\"Survived\"]].groupby('Sex').mean()","1b81a4bc":"from sklearn.preprocessing import LabelEncoder\n\n# Your code here\n\n# End\n\nprint(data.head(5), '\\n')\nrun_and_validate(data, labels)","dd43325c":"g = sns.catplot(x=\"Embarked\", y=\"Survived\", data=original_data, kind=\"bar\", palette=\"muted\")\ng = g.set_ylabels(\"survival probability\")","9e2807b8":"print(original_data[\"Embarked\"].value_counts(), '\\n')\n\nprint('Sum of null values:', original_data[\"Embarked\"].isnull().sum())","7b214422":"# Your code here\n\n# End\n\nprint(data.head(5), '\\n')\nrun_and_validate(data, labels)","c3a82210":"original_data[\"Name\"].head(10)","e9e7581d":"data[\"Title\"] = [i.split(\",\")[1].split(\".\")[0].strip() for i in original_data[\"Name\"]]\ndata[\"Title\"].head(10)","483d265e":"g = sns.countplot(x=\"Title\",data=data)\ng = plt.setp(g.get_xticklabels(), rotation=45)","2a1e56d6":"# Your code here\n\n# End\n\ndata[\"Title\"] = data[\"Title\"].astype(int)\n\ng = sns.countplot(x=data[\"Title\"])\ng = g.set_xticklabels([\"Master\",\"Miss\/Ms\/Mme\/Mlle\/Mrs\",\"Mr\",\"Rare\"])","2772ac9c":"tmp_data = data.copy()\ntmp_data[\"Survived\"] = labels\ng = sns.catplot(x=\"Title\", y=\"Survived\", data=tmp_data, kind=\"bar\", palette=\"muted\")\ng = g.set_ylabels(\"survival probability\")","0a8d9af8":"print(data.head(5), '\\n')\nrun_and_validate(data, labels)","a18265b9":"original_data[\"Cabin\"].isnull().sum()","4c72b4e2":"original_data[\"Cabin\"]","cdd7fccc":"# Your code here\n\n# End","3e84f95d":"g = sns.countplot(x=data[\"Cabin\"],order=['A','B','C','D','E','F','G','T','X'])","0dc37faf":"tmp_data = data.copy()\ntmp_data[\"Survived\"] = labels\n\ng = sns.catplot(y=\"Survived\", x=\"Cabin\", data=tmp_data, kind=\"bar\",order=['A','B','C','D','E','F','G','T','X'])\ng = g.set_ylabels(\"Survival Probability\")","79119511":"# Your code here\n\n# End\n\nprint(data.head(5), '\\n')\nrun_and_validate(data, labels)","7cdba98e":"# Your code here\n\n# End\n\nprint(data.head(5), '\\n')\nrun_and_validate(data, labels)","38b28aeb":"# Your code here\n\n# End\n\nprint(data.head(5), '\\n')\nrun_and_validate(data, labels)","14c6d438":"from sklearn.decomposition import PCA\n\n# Your code here\n\n# End\n\nrun_and_validate(data_after_pca, labels)","6ba5a507":"# Your code here\n\n# End","8b341042":"# Your code here\n\n# End","34a5d4a6":"**1. Add `Fare` to `data`.**","80ab4a64":"Explore Fare distribution.","31700aaf":"We can see that this feature is a very strong determinant of decision making.","ebc90513":"### Embarked\n\nExplore Embarked feature vs Survived.","a8dcf0b1":"If one-hot encoding makes your result worse, then [this](https:\/\/towardsdatascience.com\/one-hot-encoding-is-making-your-tree-based-ensembles-worse-heres-why-d64b282b5769) article can explain the reason for you.","da951334":"### Action\nWe're gonna work in the following scheme:\n- visualize some feature,\n- derive and add a feature to data,\n- check how it affects the model.\n\nPlease try to understand the attached visualizations.","b6539bbd":"Obviously it helps a lot. Notice, however, that despite the men are much less likely to survive, young children are very likely. How can you fill NaNs from `Age` column using that information?","fe99a1b5":"### Parch\n\nExplore Parch feature vs Survived.","71847c1f":"Check null values in the `Cabin` feature.","04dffdbe":"**!!! Remember that the data is described [here](https:\/\/www.kaggle.com\/c\/titanic\/data). !!!**","b76255d1":"Do you see here something interesting?\n\nLet's get the mean survival probability of both genders.","1a4ba27f":"**11. Plot features variance by using `explained_variance_ratio_` argument from PCA object.**","6e0b03bb":"Let's add age to `data` with filled NaN values.","2522beeb":"<a id=\"3\"><\/a>\n## III. Correlation\n\nCorrelation shows us how individual features depend on each other (linearly).\n\nSo, let's take a look.","b01cf756":"**8. Encode `Title` and `Embarked` columns.**","d7e16a07":"**6. Replace the Cabin number by the type of cabin (first letter of a string). Add `X` if value `isnull()`.**","ebdff51d":"Explore Title feature vs Survived.","d98a44e1":"**4. `Embarked` column has 2 empty values. Fill it with some value (which one will be the best?), encode and add to `data`.**","d96a0b9c":"### Pclass\n\nExplore Pclass feature vs Survived.","81c78588":"**2. Fare distribution is very skewed, as you can see. Instead of using raw values, use `np.log()` of it.**\n\nSkewed data might have very bad impact on some models based on, for example, regression.\n\nRandomForestClassifier should be immune to the effect of skewed data, but it's still a good practice to get rid of it.","37b6279b":"### What is and why use feature engineering?\n\nWiki: **Feature engineering is the process of using domain knowledge to extract features (characteristics, properties, attributes) from raw data.**\n\nNot all the dependencies between the data features can be found just by learning algorithms itself. It helps a lot to feed them with specialized, derived, meaningful features.","077fafdd":"## Class 3: Features engineering\n\n## Content:\n* [I. Start](#1)\n* [II. Numerical variables](#2)\n* [III. Correlation](#3)\n* [IV. Categorical variables](#4)\n* [V. Dummy encoding](#5)\n* [VI. PCA (Principal Component Analysis)](#6)\n* [VII. (Optional) Still have some time?](#7)","a8c83be0":"### Adding a title\n\nAdding a title may be a little bit harder. You should group it somehow to give useful information for the model.","e5225ca7":"**5. Use `.replace()` method to replace titles. Then encode the titles.**\n\nReplace all from the list:\n\n`['Lady', 'the Countess','Countess', 'Capt', 'Col','Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona']`\n\nwith \n\n`'Rare'`.\n\nAnd then encode 4 groups:\n- Master,\n- Miss\/Ms\/Mme\/Mlle\/Mrs,\n- Mr,\n- Rare.\n\nYou can use `LabelEncoder()` or `.map()` method.","a66ef195":"Note the spike of low-age people on the \"Survived\" histogram.","a68d5c3a":"<a id=\"7\"><\/a>\n## VII. (Optional) Still have some time?\n1. Fare column has some outliers. Analyze them and get rid of them\n2. Age can be preprocessed better. You can clearly see that age it correlated with survival but is also class-dependent.\n\nThis notebook is based on https:\/\/www.kaggle.com\/yassineghouzam\/titanic-top-4-with-ensemble-modeling\n","b9b1b6ab":"### Fare","6e64e900":"**3. Encode `Sex` column.**\n\nUse `LabelEncoder` or `.map()`.","d25d5fd1":"<a id=\"2\"><\/a>\n## II. Numerical variables\n\n### SibSp\n\nExplore SibSp feature vs Survived.","4ace0e83":"Check the score.","e508e385":"<a id=\"4\"><\/a>\n## IV. Categorical variables\n\n### Sex\n\nExplore Sex feature vs Survived.","2caf15c2":"<a id=\"6\"><\/a>\n## VI. PCA (Principal Component Analysis)\n\n**10. Use `PCA` object from `sklearn` to make new features with maximum variance (leave 3 principal components) of our data.**","fc778abf":"### Age","31e625a9":"Explore Fare distribution again.","b01d5e3f":"<a id=\"5\"><\/a>\n## V. Dummy encoding\n\n**7. Use function `get_dummies()` from `pandas` to encode `Cabin`.**\n\nWhy don't we use `LabelEncoder` or `map()`?\nYou should understand the difference between:\n- Categorical (nominal) variable,\n- Full dummy encodeing (called one-hot encoding sometimes),\n- Dummy encoding.\n\nhttps:\/\/en.wikiversity.org\/wiki\/Dummy_variable_(statistics) ","6c23b4c3":"<a id=\"1\"><\/a>\n## I. Start\n\nHello and welcome to the third lesson.\n\nToday we will get acquainted with:\n- features engineering,\n- dummy encoding,\n- principal component analysis (PCA).\n\nGood luck!","fe01a749":"### Get first data\nWe start with 3 numerical features with all data present.","cde326bd":"### New features validation\n\nI prepared a function that trains and evaluate some fixed model on our data.\n\nEvery time you add new feature, you can use this function to see if it improves model results.\n\nFunction contains:\n- train\/test spliting with 67\/33 ratio,\n- random forest model,\n- model training,\n- train\/test set prediction and validation.","7b794628":"### Cabin","5b4b31af":"As you can see, in this case, 3 main components can describe the data pretty good. The processing of fewer features is much faster.","40635caf":"### Family size\n\n**9. Create `Fsize` column with family size. It is SibSp + Parch + 1.**","1faf6e73":"### Let's recall the data \n\nLet's import our favorite libraries and dataset from the last class."}}