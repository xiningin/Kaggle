{"cell_type":{"87605de6":"code","a40623e6":"code","432a0167":"code","6700023e":"code","1a05eb38":"code","29f1267e":"code","8dd0213c":"code","0f71545e":"code","1c83be21":"code","dc62009b":"code","53e7777d":"code","d37fbd22":"code","ba574c5e":"code","c1c6e668":"code","780b6741":"code","9ef10bb1":"code","85a604d9":"code","1569a979":"code","e544ce79":"code","bc612208":"code","179b162a":"code","f7d89e6b":"code","01e072af":"code","a5245ff7":"code","78970350":"code","c31b37fc":"code","6fc5e384":"code","1b9a9370":"code","32df652a":"code","06c3f8d8":"code","0701a628":"code","18d47f08":"code","7a4799ef":"code","f5d59590":"code","46c1ded1":"code","f3f6dd36":"code","6ea5580e":"code","62b65ef6":"code","b9fad438":"code","966ce270":"code","e20f845f":"code","e3345e56":"code","e3d7832d":"code","a743fcd5":"code","3cc93381":"code","b2f72569":"code","fc2b59f9":"code","645e1ed9":"code","875a91e0":"code","115a4f8e":"code","f81feb69":"code","ad1d030f":"code","8598a311":"markdown","a2a350b1":"markdown","47e41c32":"markdown","8244d8ef":"markdown","ae58f8e5":"markdown","f68e4999":"markdown","dca9902e":"markdown","f38c5234":"markdown","33b58167":"markdown","cf7f3b23":"markdown","e72e3ecc":"markdown","515a762f":"markdown","0ac5036d":"markdown","7cd7fa92":"markdown","f8beb189":"markdown","773f8d55":"markdown","a6007f91":"markdown","a347cee4":"markdown","8d66ef8b":"markdown","acc6be5b":"markdown","01b66168":"markdown","1d2c9be5":"markdown","f37046c5":"markdown","3cb7f8f6":"markdown","dba714a2":"markdown","47dae88c":"markdown","4c6e9612":"markdown","7c5b965c":"markdown","36984914":"markdown","f626fb18":"markdown","ee95c770":"markdown","cd2266e8":"markdown","baa9829b":"markdown","ce29a04d":"markdown","ad98bddb":"markdown","cd9bfce6":"markdown","f644d289":"markdown","0eaa37cf":"markdown","4ec41c3d":"markdown","2a3c15a1":"markdown","be2c5207":"markdown","d8b56392":"markdown","5312200d":"markdown","5bab9935":"markdown","7432a16c":"markdown"},"source":{"87605de6":"import os","a40623e6":"for dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","432a0167":"import pandas as pd\nimport numpy as np","6700023e":"train_df = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntest_df = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\nsubmission = pd.read_csv('\/kaggle\/input\/titanic\/gender_submission.csv')","1a05eb38":"train_df.head()","29f1267e":"# train_df.describe()  \n# test_df.head()\n# train_df.tail()\n# test_df.tail()\n# test_df.describe()\n# test_df.info()\n# submission.describe()\n# submission.head()\n# train_df.info()","8dd0213c":"import seaborn as sns\n\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport plotly.io as pio\npio.templates\nimport matplotlib.pyplot as plt\n%matplotlib inline\nplt.style.use('seaborn-notebook')\nfrom matplotlib.ticker import StrMethodFormatter\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelBinarizer","0f71545e":"import io\nimport requests\nimport re\nimport warnings","1c83be21":"women = train_df.loc[train_df.Sex == 'female'][\"Survived\"]\nrate_women = sum(women)\/len(women)\nmen = train_df.loc[train_df.Sex == 'male'][\"Survived\"]\nrate_men = sum(men)\/len(men)\n\nprint(\"% of women who survived:\", rate_women)\nprint(\"% of men who survived:\", rate_men)","dc62009b":"train_df.info()","53e7777d":"test_df.info()","d37fbd22":"for template in [\"plotly\"]:\n    fig = px.scatter(train_df,\n                     x=\"PassengerId\", y=\"Age\", color=\"Survived\",\n                     log_x=True, size_max=20,\n                     template=template, title=\"Which Age Survived?\")\n    fig.show()","ba574c5e":"embarked_mode = train_df['Embarked'].mode()\ndata = [train_df, test_df]\nfor dataset in data:\n    dataset['Embarked'] = dataset['Embarked'].fillna(embarked_mode)","c1c6e668":"grid = sns.FacetGrid(train_df, col='Survived', row='Pclass', height=2.2, aspect=1.6)\ngrid.map(plt.hist, 'Age', alpha=.5, bins=20)\ngrid.add_legend();","780b6741":"grid = sns.FacetGrid(train_df, row='Embarked', height=2.2, aspect=1.6)\ngrid.map(sns.pointplot, 'Pclass', 'Survived', 'Sex', palette='deep')\ngrid.add_legend()","9ef10bb1":"grid = sns.FacetGrid(train_df, row='Embarked', col='Survived', height=2.2, aspect=1.6)\ngrid.map(sns.barplot, 'Sex', 'Fare', alpha=.5, ci=None)\ngrid.add_legend()","85a604d9":"combine = [train_df, test_df]\nprint(\"Before\", train_df.shape, test_df.shape, combine[0].shape, combine[1].shape)\ntrain_df = train_df.drop(['Ticket', 'Cabin'], axis=1)\ntest_df = test_df.drop(['Ticket', 'Cabin'], axis=1)\ncombine = [train_df, test_df]\nprint(\"After\", train_df.shape, test_df.shape, combine[0].shape, combine[1].shape)\n","1569a979":"for dataset in combine:\n    dataset['Title'] = dataset.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\n\npd.crosstab(train_df['Title'], train_df['Sex'])","e544ce79":"for dataset in combine:\n    dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess','Capt', 'Col',\\\n \t'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n\n    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\n    \ntrain_df[['Title', 'Survived']].groupby(['Title'], as_index=False).mean()","bc612208":"title_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5}\nfor dataset in combine:\n    dataset['Title'] = dataset['Title'].map(title_mapping)\n    dataset['Title'] = dataset['Title'].fillna(0)\n\ntrain_df.head()\n","179b162a":"for dataset in combine:\n    dataset['Sex'] = dataset['Sex'].map( {'female': 1, 'male': 0} ).astype(int)\n\ntrain_df.head()","f7d89e6b":"train_df = train_df.drop(['Name', 'PassengerId'], axis=1)\ntest_df = test_df.drop(['Name'], axis=1)\ncombine = [train_df, test_df]\ntrain_df.shape, test_df.shape","01e072af":"train_df.head()","a5245ff7":"test_df.head()","78970350":"guess_ages = np.zeros((2,3))\nfor dataset in combine:\n    for i in range(0, 2):\n        for j in range(0, 3):\n            guess_df = dataset[(dataset['Sex'] == i) & \\\n                                  (dataset['Pclass'] == j+1)]['Age'].dropna()\n\n\n            age_guess = guess_df.median()\n\n            # Convert random age float to nearest .5 age\n            guess_ages[i,j] = int( age_guess\/0.5 + 0.5 ) * 0.5\n            \n    for i in range(0, 2):\n        for j in range(0, 3):\n            dataset.loc[ (dataset.Age.isnull()) & (dataset.Sex == i) & (dataset.Pclass == j+1),\\\n                    'Age'] = guess_ages[i,j]\n\n    dataset['Age'] = dataset['Age'].astype(int)\n\ntrain_df.head()","c31b37fc":"train_df['AgeBand'] = pd.cut(train_df['Age'], 5)\ntrain_df[['AgeBand', 'Survived']].groupby(['AgeBand'], as_index=False).mean().sort_values(by='AgeBand', ascending=True)\n","6fc5e384":"for dataset in combine:    \n    dataset.loc[ dataset['Age'] <= 16, 'Age'] = 0\n    dataset.loc[(dataset['Age'] > 16) & (dataset['Age'] <= 32), 'Age'] = 1\n    dataset.loc[(dataset['Age'] > 32) & (dataset['Age'] <= 48), 'Age'] = 2\n    dataset.loc[(dataset['Age'] > 48) & (dataset['Age'] <= 64), 'Age'] = 3\n    dataset.loc[ dataset['Age'] > 64, 'Age']\ntrain_df.head()\ntrain_df = train_df.drop(['AgeBand'], axis=1)\ncombine = [train_df, test_df]\ntrain_df.head()","1b9a9370":"for dataset in combine:\n    dataset['FamilySize'] = dataset['SibSp'] + dataset['Parch'] + 1\n\ntrain_df[['FamilySize', 'Survived']].groupby(['FamilySize'], as_index=False).mean().sort_values(by='Survived', ascending=False)","32df652a":"for dataset in combine:\n    dataset['IsAlone'] = 0\n    dataset.loc[dataset['FamilySize'] == 1, 'IsAlone'] = 1\n\ntrain_df[['IsAlone', 'Survived']].groupby(['IsAlone'], as_index=False).mean()","06c3f8d8":"train_df = train_df.drop(['Parch', 'SibSp', 'FamilySize'], axis=1)\ntest_df = test_df.drop(['Parch', 'SibSp', 'FamilySize'], axis=1)\ncombine = [train_df, test_df]\n\ntrain_df.head()","0701a628":"train_df[train_df['Embarked'].isna()]","18d47f08":"freq_port = train_df.Embarked.dropna().mode()[0]\nfreq_port","7a4799ef":"for dataset in combine:\n    dataset['Embarked'] = dataset['Embarked'].fillna(freq_port)\n    \ntrain_df[['Embarked', 'Survived']].groupby(['Embarked'], as_index=False).mean().sort_values(by='Survived', ascending=False)","f5d59590":"for dataset in combine:\n    dataset['Embarked'] = dataset['Embarked'].map( {'S': 0, 'C': 1, 'Q': 2} ).astype(int)\n\ntrain_df.head()","46c1ded1":"train_df[train_df['Fare'].isna()]","f3f6dd36":"test_df[test_df['Fare'].isna()]","6ea5580e":"test_df['Fare'].fillna(test_df['Fare'].dropna().median(), inplace=True)\ntest_df.head()","62b65ef6":"train_df['FareBand'] = pd.qcut(train_df['Fare'], 4)\ntrain_df[['FareBand', 'Survived']].groupby(['FareBand'], as_index=False).mean().sort_values(by='FareBand', ascending=True)","b9fad438":"for dataset in combine:\n    dataset.loc[ dataset['Fare'] <= 7.91, 'Fare'] = 0\n    dataset.loc[(dataset['Fare'] > 7.91) & (dataset['Fare'] <= 14.454), 'Fare'] = 1\n    dataset.loc[(dataset['Fare'] > 14.454) & (dataset['Fare'] <= 31), 'Fare']   = 2\n    dataset.loc[ dataset['Fare'] > 31, 'Fare'] = 3\n    dataset['Fare'] = dataset['Fare'].astype(int)\n\ntrain_df = train_df.drop(['FareBand'], axis=1)\ncombine = [train_df, test_df]\n    \ntrain_df.head(10)","966ce270":"# machine learning\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier","e20f845f":"X_train = train_df.drop(\"Survived\", axis=1)\nY_train = train_df[\"Survived\"]\nX_test  = test_df.drop(\"PassengerId\", axis=1).copy()\nX_train.shape, Y_train.shape, X_test.shape","e3345e56":"logreg = LogisticRegression()\nlogreg.fit(X_train, Y_train)\nY_pred = logreg.predict(X_test)\nacc_log = round(logreg.score(X_train, Y_train) * 100, 2)\nacc_log\n\n","e3d7832d":"coeff_df = pd.DataFrame(train_df.columns.delete(0))\ncoeff_df.columns = ['Feature']\ncoeff_df[\"Correlation\"] = pd.Series(logreg.coef_[0])\n\ncoeff_df.sort_values(by='Correlation', ascending=False)","a743fcd5":"knn = KNeighborsClassifier(n_neighbors = 3)\nknn.fit(X_train, Y_train)\nY_pred = knn.predict(X_test)\nacc_knn_train = round(knn.score(X_train, Y_train) * 100, 2)\nacc_knn","3cc93381":"svc = SVC()\nsvc.fit(X_train, Y_train)\nY_pred = svc.predict(X_test)\nacc_svc = round(svc.score(X_train, Y_train) * 100, 2)\nacc_svc","b2f72569":"gaussian = GaussianNB()\ngaussian.fit(X_train, Y_train)\nY_pred = gaussian.predict(X_test)\nacc_gaussian = round(gaussian.score(X_train, Y_train) * 100, 2)\nacc_gaussian","fc2b59f9":"decision_tree = DecisionTreeClassifier()\ndecision_tree.fit(X_train, Y_train)\nY_pred = decision_tree.predict(X_test)\nacc_decision_tree = round(decision_tree.score(X_train, Y_train) * 100, 2)\nacc_decision_tree","645e1ed9":"random_forest = RandomForestClassifier(n_estimators=100)\nrandom_forest.fit(X_train, Y_train)\nY_pred = random_forest.predict(X_test)\nrandom_forest.score(X_train, Y_train)\nacc_random_forest = round(random_forest.score(X_train, Y_train) * 100, 2)\nacc_random_forest","875a91e0":"perceptron = Perceptron()\nperceptron.fit(X_train, Y_train)\nY_pred = perceptron.predict(X_test)\nacc_perceptron = round(perceptron.score(X_train, Y_train) * 100, 2)\nacc_perceptron","115a4f8e":"linear_svc = LinearSVC(max_iter=12000)\nlinear_svc.fit(X_train, Y_train)\nY_pred = linear_svc.predict(X_test)\nacc_linear_svc = round(linear_svc.score(X_train, Y_train) * 100, 2)\nacc_linear_svc","f81feb69":"sgd = SGDClassifier()\nsgd.fit(X_train, Y_train)\nY_pred = sgd.predict(X_test)\nacc_sgd = round(sgd.score(X_train, Y_train) * 100, 2)\nacc_sgd","ad1d030f":"models = pd.DataFrame({\n    'Model': ['Support Vector Machines', 'KNN', 'Logistic Regression', \n              'Random Forest', 'Naive Bayes', 'Perceptron', \n              'Stochastic Gradient Decent', 'Linear SVC', \n              'Decision Tree'],\n    'Score': [acc_svc, acc_knn, acc_log, \n              acc_random_forest, acc_gaussian, acc_perceptron, \n              acc_sgd, acc_linear_svc, acc_decision_tree]})\nmodels.sort_values(by='Score', ascending=False)","8598a311":"## Examining the Fare column and imputing the missing values","a2a350b1":"## Uncomment and execute the following commands to see what they can tell you.","47e41c32":"# Exploring the Data","8244d8ef":"## Convert the Fare into bands\n","ae58f8e5":"### Read and store in a data frame\nLet us read the three csv files we saw in the directory into three different data frames and then name them train_df, test_df, gender_df so that we can examine them","f68e4999":"#  ML Models\nWe will look at the following algorithms to build different models using the same data\n* KNN or k-Nearest Neighbors\n* Support Vector Machines\n* Naive Bayes classifier\n* Decision Tree\n* Random Forrest\n* Perceptron\n* Linear SVC\n* Stochastic Gradient Descent\n\n\n# KNN - K-Nearest Neighbors","dca9902e":"# The Basics\n## A picture of the titanic\n![Titanic](https:\/\/www.encyclopedia-titanica.org\/files\/1\/figure-one-side-view.gif)\n\nWhen we look at the picture and read some of the wikipedia archives, and examine the statistics, it looks like the number of people who survived can be guessed based on\n* the class they were traveling on, \n* their age, \n* their gender \n* whether they were crew or the passenger.\n\nHow did we come to this conclusion, well we looked at the statistics and developed an intuition for making these types of guesses.\nModeling can be considered to be a form of guessing or rather using probability to make an informed judgement or guess. \nUsing this information, we now can make a guess of whether a passenger in a given group of passengers survived or not. But how can we build an AI\/ML model which can make that guess by learning these patterns.\n\nThat is what we are going to do today.","f38c5234":"## Converting the Categorical Values to  Numerical Labels (Title \/ Sex)","33b58167":"Observations.\n\nWhen we plot Title, Age, and Survived, we note the following observations.\n\n* Most titles band Age groups accurately. For example: Master title has Age mean of 5 years.\n* Survival among Title Age bands varies slightly.\n* Certain titles mostly survived (Mme, Lady, Sir) or did not (Don, Rev, Jonkheer).\n\n**Decision**\n\nWe can retain the new Title feature for model training.","cf7f3b23":"# Support Vector Machines\n","e72e3ecc":"## Importing Machine learning packages","515a762f":"# Perceptron","0ac5036d":"## Examining how each of these features impact the Survivability\nWe will look at how each of these features correlate with the Survivability. We will do that by using a simple Logistic Regression.\n","7cd7fa92":"Let us convert the Embarked Columns from strings to numbers","f8beb189":"# Create a New Feature\nWe will create a new feature column for the family size by using the two columns SibSp and Parch columns.\nWe can now analyze to see if people who were part of a family more reluctant to leave their families behind as opposed to those who traveled alone","773f8d55":"## Importing the visualization libraries","a6007f91":"# Comparing and Ranking the Models","a347cee4":"# Stochastic Gradient Descent","8d66ef8b":"## Imputing Values using the most frequent value for Embarked","acc6be5b":"## Understanding Survivorship based on the Embarked Port","01b66168":"### Convert the age band into numeric labels\nWe will convert the ageband into numeric labels and get rid of the age band","1d2c9be5":"Let us identify the most frequent embarked port","f37046c5":"### Drop the columns Parch, SibSp and FamilySize\nSince we have created the feature IsAlone we can get rid of the above columns","3cb7f8f6":"## Converting age into a Age Band\nWe will use a cut function and break it into 5 bands. We can look at the survival rate for each band after creating the band","dba714a2":"## Examining the Embarked Column and filling in the missing values\n","47dae88c":"Port of embarkation correlates with survival rates. Confirms correlating (#1) and completing (#2).s","4c6e9612":"## Starting with the data\nWe now will look at how we can get the data that we want to use. First let us import a library that will help us check on the data that we will use. We are used to writing commands at the command line to find what files are there in a particular directory. We however are in a cloud environment and for python to access directory we need to import a library called os.","7c5b965c":"Higher fare paying passengers had better survival. Confirms our assumption for creating (#4) fare ranges.\n\n**Decision**\n* Consider banding the Fare feature","36984914":"### Questions to Ask\n**Which features are categorical?**\n\nThese values classify the samples into sets of similar samples. Within categorical features are the values nominal, ordinal, ratio, or interval based? Among other things this helps us select the appropriate plots for visualization.\n\nCategorical: Survived, Sex, and Embarked. \nOrdinal: Pclass.\n\n\n**Which features are numerical? **\n\nThese values change from sample to sample. Within numerical features are the values discrete, continuous, or timeseries based? Among other things this helps us select the appropriate plots for visualization.\nContinous: Age, Fare. \nDiscrete: SibSp, Parch.\n\n**Which features are mixed data types?**\n\nNumerical, alphanumeric data within same feature. These are candidates for correcting goal.\n\nTicket is a mix of numeric and alphanumeric data types. \nCabin is alphanumeric.\n\n**Which features may contain errors or typos? **\n\nThis is harder to review for a large dataset, however reviewing a few samples from a smaller dataset may just tell us outright, which features may require correcting.\nName feature may contain errors or typos as there are several ways used to describe a name including titles, round brackets, and quotes used for alternative or short names.\n\n\n**Which features contain blank, null or empty values?**\n\nThese will require correcting.\n\nCabin > Age > Embarked features contain a number of null values in that order for the training dataset.\nCabin > Age are incomplete in case of test dataset.\n\n**What are the data types for various features?**\n\nKnowing this is important and will tell us what we data types we need to convert.\n\n* Seven features are integer or floats. \n* Six in case of test dataset.\n* Five features are strings (object).\n\n\n**What is the distribution of numerical feature values across the samples?**\n\nThis helps us determine, among other early insights, how representative is the training dataset of the actual problem domain.\n\n* Total samples are 891 or 40% of the actual number of passengers on board the Titanic (2,224).\n* Survived is a categorical feature with 0 or 1 values.\n* Around 38% samples survived representative of the actual survival rate at 32%.\n* Most passengers (> 75%) did not travel with parents or children.\n* Nearly 30% of the passengers had siblings and\/or spouse aboard.\n* Fares varied significantly with few passengers (<1%) paying as high as $512.\n* Few elderly passengers (<1%) within age range 65-80.\n\n\n**What is the distribution of categorical features?**\n\n* Names are unique across the dataset (count=unique=891)\n* Sex variable as two possible values with 65% male (top=male, freq=577\/count=891).\n* Cabin values have several dupicates across samples. Alternatively several passengers shared a cabin.\n* Embarked takes three possible values. S port used by most passengers (top=S)\n* Ticket feature has high ratio (22%) of duplicate values (unique=681)","f626fb18":"## Survivability Correlations\nWe know that Women were more likely to survive and since women were coded as 1 and Men as zero we know that survivability increased when the Sex feature changed from 0 to 1\nSimilary with Passenger Class if the person was in first class they were more likely to survive as opposed to third class. So increase in the feature meant decrease in survivability so negative correlation","ee95c770":"# Random Forest","cd2266e8":"# Naive Bayes Classifier","baa9829b":"# Decision Tree","ce29a04d":"## Imputing the age where not available","ad98bddb":"## Exploring the data\nWe see that there are three data sets (apart from the titanic.gif picture). The first one is a training data set. We will give this training data set to the model to learn the patterns of what would be a good reason to guess if the person survived or not. The model will read the data, know the outcome and try to guess the outcomes using some formula, by the way we also call it a model. We make changes to this formula until we it is able to guess a large number of answers correctly on our practice or training data.\n\nWe then use the formula on the test data to make a guess on what the outcome might be. If the formula is good then our guesses are going to turn out to be good.\nWe will ignore the submission file for now.\n\nTo explore the data we now need to import a few libraries. We will import the common libraries pandas and numpy\n\n### Data Dictionary\n|Variable | Definition| Key|\n|--------|------------|----|\n|survival | Survival | 0 = No, 1 = Yes|\n|pclass  | Ticket class  | 1 = 1st, 2 = 2nd, 3 = 3rd|\n|sex  | Sex |\n|Age | Age in years | \n|sibsp  | # of siblings \/ spouses aboard the Titanic |**Sibling**= brother, sister, stepbrother, stepsister;; **Spouse** = husband, wife (mistresses and fianc\u00e9s were ignored)|\n| parch  | # of parents \/ children aboard the Titanic | \n|ticket | Ticket number | \n| fare  | Passenger fare | \n| cabin | Cabin number | \n| embarked | Port of Embarkation | C = Cherbourg, Q = Queenstown, S = Southampton\n\n\nparch: The dataset defines family relations in this way...\n* Parent = mother, father\n* Child = daughter, son, stepdaughter, stepson\n* Some children travelled only with a nanny, therefore parch=0 for them.","cd9bfce6":"### Converting the Age to integers\nWe will iterate over Sex (0 or 1) and Pclass (1, 2, 3) to calculate guessed values of Age for the six combinations.\nTo begin with we will create a matrix of zeros for the six combinations\nWe will impute the age if not available using the median value. We can use other means as well like imputing witha uniform distribution with a calculated mean and standard deviation\n            # age_mean = guess_df.mean()\n            # age_std = guess_df.std()\n            # age_guess = rnd.uniform(age_mean - age_std, age_mean + age_std)\n\n\n","f644d289":"# Linear SVC","0eaa37cf":"## Survivorship by Gender","4ec41c3d":"# Converting the data","2a3c15a1":"## Checking Survival by Passenger Class","be2c5207":"**Observations.**\n\n* Female passengers had much better survival rate than males. Confirms classifying (#1).\n* Exception in Embarked=C where males had higher survival rate. This could be a correlation between Pclass and Embarked and in turn Pclass and Survived, not necessarily direct correlation between Embarked and Survived.\n* Males had better survival rate in Pclass=3 when compared with Pclass=2 for C and Q ports. Completing (#2).\n* Ports of embarkation have varying survival rates for Pclass=3 and among male passengers. Correlating (#1).\n\n**Decisions**\n* Add Sex feature to model training.\n* Complete and add Embarked feature to model training.","d8b56392":"## Review the data in train and test data frames","5312200d":"## Data analysis\n\n#### Pivoting the Features\n\nWe want to know how well each feature\/column correlates with Survival. \n\nCompleting.\nWe may want to complete Age feature as it is definitely correlated to survival.\nWe may want to complete the Embarked feature as it may also correlate with survival or another important feature.\nCorrecting.\n\nTicket feature may be dropped from our analysis as it contains high ratio of duplicates (22%) and there may not be a correlation between Ticket and survival.\nCabin feature may be dropped as it is highly incomplete or contains many null values both in training and test dataset.\nPassengerId may be dropped from training dataset as it does not contribute to survival.\nName feature is relatively non-standard, may not contribute directly to survival, so maybe dropped.\nCreating.\n\nWe may want to create a new feature called Family based on Parch and SibSp to get total count of family members on board.\nWe may want to engineer the Name feature to extract Title as a new feature.\nWe may want to create new feature for Age bands. This turns a continous numerical feature into an ordinal categorical feature.\nWe may also want to create a Fare range feature if it helps our analysis.\nClassifying.\n\nWe may also add to our assumptions based on the problem description noted earlier.\n\nWomen (Sex=female) were more likely to have survived.\nChildren (Age<?) were more likely to have survived.\nThe upper-class passengers (Pclass=1) were more likely to have survived.\nAnalyze by pivoting features\nTo confirm some of our observations and assumptions, we can quickly analyze our feature correlations by pivoting features against each other. We can only do so at this stage for features which do not have any empty values. It also makes sense doing so only for features which are categorical (Sex), ordinal (Pclass) or discrete (SibSp, Parch) type.\n\nPclass We observe significant correlation (>0.5) among Pclass=1 and Survived (classifying #3). We decide to include this feature in our model.\nSex We confirm the observation during problem definition that Sex=female had very high survival rate at 74% (classifying #1).\nSibSp and Parch These features have zero correlation for certain values. It may be best to derive a feature or a set of features from these individual features (creating #1).","5bab9935":"## Steps to load the data\nNow that we have the library to access os level file system\n1. Click on the file menu and select add or upload data\n2. Click on the competition data and then search for Titanic\n3. Select the add button to add the data set to your input directory.\n\nNow let us execute the cell below to see if the data is visible to this notebook.","7432a16c":"Creating new feature extracting from existing\nWe want to analyze if Name feature can be engineered to extract titles and test correlation between titles and survival, before dropping Name and PassengerId features.\n\nIn the following code we extract Title feature using regular expressions. \n\nThe RegEx pattern (\\w+\\.) matches the first word which ends with a dot character within Name feature. \nThe expand=False flag returns a DataFrame.\n\n"}}