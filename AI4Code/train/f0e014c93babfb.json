{"cell_type":{"763fc390":"code","bcc20e15":"code","c53adec6":"code","ecd9af8c":"code","86eff169":"code","04af2081":"code","a11e388f":"code","edf87d6e":"code","9d91201e":"code","f0ab0683":"code","110b3fef":"code","e941600b":"code","10683305":"markdown","72e18fee":"markdown","a0f43c15":"markdown","063113c3":"markdown","dc4893d1":"markdown","1f28b18f":"markdown","06130af8":"markdown","2072390e":"markdown","5809c87c":"markdown","2de26fa0":"markdown","f13cedee":"markdown","53b90144":"markdown","8e0aefff":"markdown","003b7ae7":"markdown","df1e9dc0":"markdown","3989cfea":"markdown"},"source":{"763fc390":"import pandas as pd\nimport numpy as np\ndf = pd.read_csv(\"..\/input\/rintro-chapter7.csv\")\ndf.head()","bcc20e15":"df.describe()","c53adec6":"import seaborn as sns\nsns.set(style=\"white\")\ng = sns.PairGrid(df, diag_sharey=False)\ng.map_lower(sns.kdeplot)\n#g.map_upper(sns.scatterplot)\ng.map_upper(sns.regplot)\ng.map_diag(sns.kdeplot, lw=3)","ecd9af8c":"df['logdistance'] = np.log(df['distance'])\ndf.head()","86eff169":"sns.set(style=\"white\")\ng = sns.PairGrid(df, diag_sharey=False)\ng.map_lower(sns.kdeplot)\n#g.map_upper(sns.scatterplot)\ng.map_upper(sns.regplot)\ng.map_diag(sns.kdeplot, lw=3)","04af2081":"df.columns\n\nX = df.drop(columns= ['overall','distance', 'num.child' ])\nX = pd.get_dummies(data=X, drop_first=True, columns = ['weekend'])\nX.head()","a11e388f":"Y = df[['overall']]\nY.head()","edf87d6e":"import statsmodels.api as sm\nX = sm.add_constant(X)\nmodel = sm.OLS(Y,X)\nresults = model.fit()\nresults.summary()","9d91201e":"# predicted  values\nmodel_fitted_y  = results.fittedvalues\n# model residuals\nmodel_residuals = results.resid\n# normalized residuals\nmodel_norm_residuals = results.get_influence().resid_studentized_internal\n# absolute squared normalized residuals\nmodel_norm_residuals_abs_sqrt = np.sqrt(np.abs(model_norm_residuals))\n# absolute residuals\nmodel_abs_resid = np.abs(model_residuals)\n# leverage, from statsmodels internals\nmodel_leverage = results.get_influence().hat_matrix_diag\n# cook's distance, from statsmodels internals\nmodel_cooks = results.get_influence().cooks_distance[0]\n","f0ab0683":"import matplotlib.pyplot as plt\nplot_lm_1 = plt.figure()\nplot_lm_1.axes[0] = sns.residplot(model_fitted_y, df.columns[-1], data=df,\n                          lowess=True,\n                          scatter_kws={'alpha': 0.5},\n                          line_kws={'color': 'red', 'lw': 1, 'alpha': 0.8})\n\nplot_lm_1.axes[0].set_title('Residuals vs Fitted')\nplot_lm_1.axes[0].set_xlabel('Fitted values')\nplot_lm_1.axes[0].set_ylabel('Residuals');","110b3fef":"from statsmodels.graphics.gofplots import ProbPlot\nQQ = ProbPlot(model_norm_residuals)\nplot_lm_2 = QQ.qqplot(line='45', alpha=0.5, color='#4C72B0', lw=1)\nplot_lm_2.axes[0].set_title('Normal Q-Q')\nplot_lm_2.axes[0].set_xlabel('Theoretical Quantiles')\nplot_lm_2.axes[0].set_ylabel('Standardized Residuals');\n# annotations\nabs_norm_resid = np.flip(np.argsort(np.abs(model_norm_residuals)), 0)\nabs_norm_resid_top_3 = abs_norm_resid[:3]\nfor r, i in enumerate(abs_norm_resid_top_3):\n    plot_lm_2.axes[0].annotate(i,\n                               xy=(np.flip(QQ.theoretical_quantiles, 0)[r],\n                                   model_norm_residuals[i]));","e941600b":"plot_lm_4 = plt.figure();\nplt.scatter(model_leverage, model_norm_residuals, alpha=0.5);\nsns.regplot(model_leverage, model_norm_residuals,\n              scatter=False,\n              ci=False,\n              lowess=True,\n              line_kws={'color': 'red', 'lw': 1, 'alpha': 0.8});\nplot_lm_4.axes[0].set_xlim(0, max(model_leverage)+0.01)\nplot_lm_4.axes[0].set_ylim(-3, 5)\nplot_lm_4.axes[0].set_title('Residuals vs Leverage')\nplot_lm_4.axes[0].set_xlabel('Leverage')\nplot_lm_4.axes[0].set_ylabel('Standardized Residuals');\n\n  # annotations\nleverage_top_3 = np.flip(np.argsort(model_cooks), 0)[:3]\nfor i in leverage_top_3:\n    plot_lm_4.axes[0].annotate(i,\n                                 xy=(model_leverage[i],\n                                     model_norm_residuals[i]));","10683305":"- we only need to find leverage points that have a distance greater than 0.5. \n- In this plot, we do not have any leverage points that meet this criteria.","72e18fee":"# Multiple Linear Regression","a0f43c15":"#### Can see no apparent patterns in the scatterplot and the red-line is horizontal which suggests that the relationship between the X and Y is indeed linear","063113c3":"## 2. Normal Q-Q Plot","dc4893d1":"* F-statistic = significant - hence, at least one X is related to Y, i.e. at least one useful predictor\n* Adj. R2 = 0.558 - high, signfies a good fit, 58% of the variation in Y is explained by the variation in X\n* All features are significant except for weekend_yes\n","1f28b18f":"- Will test that if errors are normally distributed with mean zero\n- This plot shows if the residuals are normally distributed. \n- A good normal QQ plot has all of the residuals lying on or close to the red line.","06130af8":"# Skewed distribution - Distance\n\nThe above plot reveals that distance has a highly skewed distribution and should be transformed before modeling. let's take the log of distance that will make the variable more normally distributed.\n","2072390e":"# Diagnostic Tests","5809c87c":"- Linear modeling with a satisfaction drivers analysis using survey data for customers who have visited an amusement park. \n- In the survey, respondents report their levels of satisfaction with different aspects of their experience, and their overall satisfaction.\n- Marketers frequently use this type of data to figure out what aspects of the experience drive overall satisfaction, asking questions such as, \u201cAre people who are more satisfied with the rides also more satisfied with their experience overall?\u201d \n- If the answer to this question is \u201cno,\u201d then the company will know to invest in improving other aspects of the experience.","2de26fa0":"## 3. Scale-location\n\nThis plot is a way to check if the residuals suffer from non-constant variance, aka heteroscedasticity.\n","f13cedee":"- Looking at the graph above, there are not many points that fall far away from the red line. \n- This is indicative of the errors being normally distributed, which would mean that our model does NOT suffers from \u201cheavy tails\u201d.","53b90144":"**These 4 plots examine a few different assumptions about the model and the data:**\n\n1) The data can be fit by a line (this includes any transformations made to the predictors, e.g., x^2 or \u221ax)\n\n2) Errors are normally distributed with mean zero\n\n3) Errors have constant variance, i.e., homoscedasticity\n\n4) There are no high leverage points","8e0aefff":"### One-hot encoding needs to be done to the 'weekend' variable since it is categorical ","003b7ae7":"# Correlation Analysis","df1e9dc0":"* A common issue with marketing data and especially satisfaction surveys is that variables may be highly correlated with one another. \n* Although we as marketers care about individual elements of customers\u2019 experiences such as their amusement park experience with rides and games, when completing a survey, the respondents might not give independent ratings to each of those items.\n* They may instead form an overall halo rating and rate individual elements of the experience in light of that overall feeling.","3989cfea":"## 1. First up is the Residuals vs Fitted plot. \n\n- This graph shows if there are any nonlinear patterns in the residuals, and thus in the data as well. \n- One of the mathematical assumptions in building an OLS model is that the data can be fit by a line. \n- If this assumption holds and our data can be fit by a linear model, then we should see a relatively flat line when looking at the residuals vs fitted."}}