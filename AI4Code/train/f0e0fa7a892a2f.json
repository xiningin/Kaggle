{"cell_type":{"8370a82d":"code","f66821cb":"code","84b8c540":"code","a02fcfd5":"code","dc082f08":"code","85636b25":"code","64730350":"code","0edec3fc":"code","5acb63b7":"code","7132efe8":"code","891300b0":"code","077cf145":"code","528098ea":"code","bbad6714":"code","e3c355d8":"code","37c676cf":"code","bc9a3cc9":"code","1034b7a9":"code","f2b22512":"code","fab5190d":"markdown","da97d7ef":"markdown","24ebc79a":"markdown","c9723339":"markdown","14cace7e":"markdown","b4c97a92":"markdown","62174e68":"markdown","00f80d86":"markdown","5037276e":"markdown","5bf0375e":"markdown","cd9bb287":"markdown","46e9680c":"markdown","4ffd6a36":"markdown","42929765":"markdown","b93a51aa":"markdown","87ea626a":"markdown","b7ded923":"markdown","c2fec546":"markdown","75bef029":"markdown","a4490f3e":"markdown","51f6ee2b":"markdown","90cf2aa1":"markdown","eeb4e1d6":"markdown","80d234ba":"markdown","7ddd9c40":"markdown","a2800619":"markdown","aeadcb3e":"markdown","003ea36e":"markdown","bc27d91b":"markdown","1d32fa6f":"markdown","1f004b47":"markdown","c71147c6":"markdown","702fec59":"markdown","bea2d579":"markdown","7ecc421c":"markdown","ed4f44d9":"markdown","f5959485":"markdown","93662399":"markdown","49819c2f":"markdown","c5e8eb2f":"markdown","a19517f1":"markdown","1e302c80":"markdown","42aa3c48":"markdown","2ecee4d5":"markdown","0d8c5988":"markdown","8cf4571a":"markdown","9046a4f0":"markdown","9c2e4b86":"markdown","341b8182":"markdown","3b17e244":"markdown","f6d561ef":"markdown","74a51dc3":"markdown","5a3b06e7":"markdown","88c38df8":"markdown","b48c3f7d":"markdown","940e9446":"markdown","52cc3e1c":"markdown","0fe1690a":"markdown","84efb44b":"markdown","e7593e0e":"markdown","6f8b169e":"markdown","33583b0a":"markdown","f8a53372":"markdown","00041a83":"markdown","364cb8da":"markdown","2988d4be":"markdown","e3aabbc6":"markdown"},"source":{"8370a82d":"import numpy as np\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm","f66821cb":"k = 10\nnum_problems = 2000\n\nq_star = np.random.normal(0, 1, (num_problems,k))\narms = [0] * k\n\nfor i in range(10):\n    arms[i] = np.random.normal(q_star[0, i], 1, 2000) # first problem as a sample","84b8c540":"plt.figure(figsize=(12,8))\nplt.ylabel('Rewards distribution')\nplt.xlabel('Actions')\nplt.xticks(range(1,11))\nplt.yticks(np.arange(-5,5,0.5))\n\nplt.violinplot(arms, positions=range(1,11), showmedians=True)\nplt.show()","a02fcfd5":"def bandit(action, problem):\n    return np.random.normal(q_star[problem, action], 1)","dc082f08":"def simple_max(Q, N, t):\n#     return np.argmax(Q)\n    return np.random.choice(np.flatnonzero(Q == Q.max())) # breaking ties randomly","85636b25":"def simple_bandit(k, epsilon, steps, initial_Q, alpha=0, argmax_func=simple_max):\n    rewards = np.zeros(steps)\n    actions = np.zeros(steps)\n    \n    for i in tqdm(range(num_problems)):\n        Q = np.ones(k) * initial_Q # initial Q\n        N = np.zeros(k)  # initalize number of rewards given\n        best_action = np.argmax(q_star[i])\n        for t in range(steps):\n            if np.random.rand() < epsilon: # explore\n                a = np.random.randint(k)\n            else: # exploit\n                a = argmax_func(Q, N, t)\n\n            reward = bandit(a, i)\n\n            N[a] += 1\n            if alpha > 0:\n                Q[a] = Q[a] + (reward - Q[a]) * alpha\n            else:\n                Q[a] = Q[a] + (reward - Q[a]) \/ N[a]\n\n            rewards[t] += reward\n            \n            if a == best_action:\n                actions[t] += 1\n    \n    return np.divide(rewards,num_problems), np.divide(actions,num_problems)","64730350":"ep_0, ac_0 = simple_bandit(k=10, epsilon=0, steps=1000, initial_Q=0)\nep_01, ac_01 = simple_bandit(k=10, epsilon=0.01, steps=1000, initial_Q=0)\nep_1, ac_1 = simple_bandit(k=10, epsilon=0.1, steps=1000, initial_Q=0)","0edec3fc":"plt.figure(figsize=(12,6))\nplt.plot(ep_0, 'g', label='epsilon = 0')\nplt.plot(ep_01, 'r', label='epsilon = 0.01')\nplt.plot(ep_1, 'b', label='epsilon = 0.1')\nplt.legend() \nplt.show()","5acb63b7":"plt.figure(figsize=(12,6))\nplt.yticks(np.arange(0,1,0.1))\nplt.plot(ac_0, 'g', label='epsilon = 0')\nplt.plot(ac_01, 'r', label='epsilon = 0.01')\nplt.plot(ac_1, 'b', label='epsilon = 0.1')\nplt.legend() \nplt.show()","7132efe8":"opt_0, ac_opt_0 = simple_bandit(k=10, epsilon=0, steps=1000, initial_Q=5, alpha=0.2)","891300b0":"plt.figure(figsize=(12,6))\nplt.yticks(np.arange(0,3,0.2))\nplt.plot(ac_1, 'r', label='Realistic')\nplt.plot(ac_opt_0, 'b', label='Optimistic')\nplt.legend() \nplt.show()","077cf145":"def ucb(Q, N, t):\n    c = 2\n    if N.min() == 0:\n        return np.random.choice(np.flatnonzero(N == N.min()))\n    \n    M = Q + c * np.sqrt(np.divide(np.log(t),N))\n    return np.argmax(M) # breaking ties randomly","528098ea":"ucb_2, ac_ucb_2 = simple_bandit(k=10, epsilon=0, steps=1000, initial_Q=0, argmax_func=ucb)","bbad6714":"plt.figure(figsize=(12,6))\nplt.plot(ep_1, 'g', label='e-greedy e=0.1')\nplt.plot(ucb_2, 'b', label='ucb c=2')\nplt.legend() \nplt.show()","e3c355d8":"def softmax(x):\n    e_x = np.exp(x - np.max(x)) \n    M = e_x \/ e_x.sum()\n    return np.argmax(M), M","37c676cf":"def gradient_bandit(k, steps, alpha, initial_Q, is_baseline=True):\n    rewards = np.zeros(steps)\n    actions = np.zeros(steps)\n    \n    for i in tqdm(range(num_problems)):\n        Q = np.ones(k) * initial_Q # initial Q\n        N = np.zeros(k) # initalize number of rewards given\n        R = np.zeros(k)\n        H = np.zeros(k) # initalize preferences\n        pi = np.zeros(k)\n        best_action = np.argmax(q_star[i]) # best action of i'th problem\n        \n        for t in range(steps):\n            a, pi = softmax(H)\n\n            reward = bandit(a, i)\n\n            N[a] += 1\n            Q[a] = Q[a] + (reward - Q[a]) \/ N[a]\n            \n            for action_i in range(k):\n                if action_i == a :\n                    H[a] = H[a] + alpha * (reward - R[a]) * (1 - pi[a])\n                else:\n                    H[action_i] = H[action_i] - alpha * (reward - R[action_i]) * pi[action_i]\n\n            if is_baseline == True:\n                R[a] = Q[a]\n\n            rewards[t] += reward\n            if a == best_action:\n                actions[t] += 1\n    \n    return np.divide(rewards,num_problems), np.divide(actions,num_problems)\n    ","bc9a3cc9":"sft_4, ac_sft_4 = gradient_bandit(k=10, steps=1000, alpha=0.4, initial_Q=0, is_baseline=False)\nsft_4_baseline, ac_sft_4_baseline = gradient_bandit(k=10, steps=1000, alpha=0.4, initial_Q=0, is_baseline=True)","1034b7a9":"sft_1, ac_sft_1 = gradient_bandit(k=10, steps=1000, alpha=0.1, initial_Q=0, is_baseline=False)\nsft_1_baseline, ac_sft_1_baseline = gradient_bandit(k=10, steps=1000, alpha=0.1, initial_Q=0, is_baseline=True)","f2b22512":"plt.figure(figsize=(10,6))\nplt.ylim([0,1])\nplt.plot(ac_sft_4_baseline, 'b', label='alpha=0.1')\nplt.plot(ac_sft_4, 'lightskyblue', label='alpha=0.1 without baseline')\nplt.plot(ac_sft_1_baseline, 'r', label='alpha=0.4')\nplt.plot(ac_sft_1, 'lightcoral', label='alpha=0.4 without baseline')\nplt.legend() \nplt.show()","fab5190d":"where $\\mathbb{1}_{predicate}$ denotes the random variable that is 1 if predicate is true and 0 if it is not.","da97d7ef":"# Multi-armed Bandits","24ebc79a":"we often encounter reinforcement learning problems that are effectively nonstationary. In such cases it makes sense to give more weight to recent rewards than to long-past rewards. One of the most popular ways of doing this is to use a constant step-size parameter.","c9723339":"Suppose that instead of setting the initial action values to zero, as we did in the 10-armed testbed, we set them all to $+5$. An initial estimate of $+5$ is wildly optimistic and this optimism encourages action-value methods to explore. ","14cace7e":"The idea of this ***upper confidence bound*** (UCB) action selection is that the square-root term is a measure of the uncertainty or variance in the estimate of $a$\u2019s value. The quantity being max\u2019ed over is thus a sort of upper bound on the possible true value of action a, with c determining the confidence level. ","b4c97a92":"## Associative Search (Contextual Bandits)","62174e68":"### $\\varepsilon$-Greedy Action Selection","00f80d86":"$$ q^*(a) = \\mathbb{E}[R_t|A_t = a] $$","5037276e":"We consider learning a numerical _preference_ for each action $a$, which we denote $H_t(a) \\in {\\rm I\\!R}$.","5bf0375e":"Associative search tasks are intermediate between the k-armed bandit problem and the full reinforcement learning problem. They are like the full reinforcement learning problem in that they involve learning a policy, but they are also like our version of the k-armed bandit problem in that each action affects only the immediate reward. If actions are allowed to affect the next situation as well as the reward, then we have the full reinforcement learning problem.","cd9bb287":"The first condition is required to guarantee that the steps are large enough to eventually overcome any initial conditions or random fluctuations.","46e9680c":"$$ Pr\\{ A_t = a\\} \\space \\dot{=} \\space \\frac{e^{H_t(a)}}{\\sum_{b=1}^{k}e^{H_t(b)}} \\space \\dot{=} \\space  \\pi_t(a)$$","4ffd6a36":"If you knew the value of each action, then it would be trivial to solve the k-armed bandit problem.","42929765":"$$ \\text{(1)}\\space \\sum_{n=1}^{\\infty}\\alpha_n(a) = \\infty $$","b93a51aa":"## Optimistic Initial Values","87ea626a":"### Step-Size Parameter","b7ded923":"Another analogy is that of a doctor choosing between experimental treatments for a series of seriously ill patients. Each action is the selection of a treatment, and each reward is the survival or well-being of the patient.","c2fec546":"Consider the following learning problem. You are faced repeatedly with a choice among k different options, or actions. After each choice you receive a numerical reward chosen from a stationary probability distribution that depends on the action you selected.\n\nYour objective is to maximize the expected total reward over some time period, for example, over 1000 action selections, or time steps.","75bef029":"***Associative search task*** involves both trial-and-error learning to search for the best actions, and association of these actions with the situations in which they are best. Associative search tasks are often called _contextual bandits_.","a4490f3e":"The true value $q^*(a)$ of each of the ten actions was selected according to a normal distribution with mean zero and unit variance, and then the actual rewards were selected according to a mean $q^*(a)$, unit-variance normal distribution, as suggested by these blue distributions.","51f6ee2b":"Initially, the optimistic method performs worse because it explores more, but eventually it performs better because its exploration decreases with time. We call this technique for encouraging exploration optimistic initial values. We regard it as a simple trick that can be quite effective on stationary problems, but it is far from being a generally useful approach to encouraging exploration.","90cf2aa1":"We denote the action selected on time step t as $A_t$, and the corresponding reward as $R_t$. The value then of an arbitrary action a, denoted $q^*(a)$, is the expected reward given that a is selected:","eeb4e1d6":"Sometimes it is convenient to vary the step-size parameter from step to step. Let $\\alpha_n(a)$ denote the step-size parameter used to process the reward received after the $n$th selection of action $a$.","80d234ba":"One natural way to estimate the true value of an action is by **averaging** the rewards actually received:","7ddd9c40":"$$ A_t \\space \\dot{=} \\space \\underset{a}{\\mathrm{argmax}} \\space Q_t(a) $$","a2800619":"The second condition guarantees that eventually the steps become small enough to assure convergence.","aeadcb3e":"$$ Q_{n+1} \\space \\dot{=} \\space Q_n + \\alpha \\space [R_n - Q_n] $$","003ea36e":"Code for a complete bandit algorithm using incrementally computed sample averages and $\\varepsilon$-greedy action selection is shown in the box below. The function **bandit(a)** is assumed to take an action and return a corresponding reward.","bc27d91b":"This results in $Q_{n+1}$ being a weighted average of past rewards and the initial estimate $Q_1$:","1d32fa6f":"Exploration is needed because there is always uncertainty about the accuracy of the action-value estimates. $\\varepsilon$-greedy action selection forces the non-greedy actions to be tried, but indiscriminately, with no preference for those that are nearly greedy or particularly uncertain. \nIt would be better to select among the non-greedy actions according to their potential for actually being optimal, taking into account both how close their estimates are to being maximal and the uncertainties in those estimates.","1f004b47":"## Gradient Bandit Algorithms","c71147c6":"$$ Q_{n+1} = Q_n + \\frac{1}{n} [R_n - Q_n] $$","702fec59":"## References\n\n+ Reinforcement Learning: An Introduction, Richard S. Sutton and Andrew G. Barto\n+ Reinforcement Learning, Phil Winder\n+ [Multi-Armed Bandits](http:\/\/ethen8181.github.io\/machine-learning\/bandits\/multi_armed_bandits.html#Algorithm-1---Epsilon-Greedy)\n+ [Multi-Armed Bandits and Reinforcement Learning](https:\/\/towardsdatascience.com\/multi-armed-bandits-and-reinforcement-learning-dc9001dcb8da)","bea2d579":"## Incremental Implementation","7ecc421c":"One effective way of doing this is to select actions according to","ed4f44d9":"It is easy to devise incremental formulas for updating averages with small, constant computation required to process each new reward. Given $Q_n$ and the $n$th reward, $R_n$, the new average of all n rewards can be computed by","f5959485":"There is a natural learning algorithm for soft-max action preferences based on the idea of stochastic gradient ascent. On each step, after selecting action $A_t$ and receiving the reward $R_t$, the action preferences are updated by:","93662399":"Initial action values can also be used as a simple way to encourage exploration.","49819c2f":"This is a set of 2000 randomly generated k-armed bandit problems with k = 10. For each bandit problem, the action values, $q^*(a)$, $a = 1, ... , 10$, are selected according to a normal (Gaussian) distribution with mean 0 and variance 1.","c5e8eb2f":"We call this a weighted average because the sum of the weights is $ (1-\\alpha)^n + \\sum_{i=1}^{n} \\alpha (1-\\alpha)^{n-i} = 1 $.","a19517f1":"This is the original form of the k-armed bandit problem, so named by analogy to a slot machine, or \u201cone-armed bandit,\u201d except that it has k levers instead of one.","1e302c80":"As the denominator goes to infinity, by the law of large numbers, $Q_t(a)$ converges to $q^*(a)$. We call this the ***sample-average*** method for estimating action values because each estimate is an average of the sample of relevant rewards.","42aa3c48":"$$ Q_{n+1} = (1-\\alpha)^n Q_1 + \\sum_{i=1}^{n} \\alpha (1-\\alpha)^{n-i} R_i $$","2ecee4d5":"We denote the estimated value of action a at time step $t$ as $Q_t(a)$. We would like $Q_t(a)$ to be close to $q^*(a)$.","0d8c5988":"$$ H_{t+1}(A_t) = H_t(A_t) + \\alpha\\space(R_t - \\bar{R_t})(1 - \\pi_t(A_t)) $$ and,\n$$ H_{t+1}(a) = H_t(a) + \\alpha\\space(R_t - \\bar{R_t})\\pi_t(a) $$ for all $a \\neq A_t$","8cf4571a":"A well-known result in stochastic approximation theory gives us the conditions required to assure convergence with probability 1:","9046a4f0":"The simplest action selection rule is to select one of the actions with the highest estimated value.","9c2e4b86":"Action probabilities are determined according to a soft-max distribution (i.e., Gibbs or Boltzmann distribution) as follows:","341b8182":"When you select one of these actions, we say that you are ***exploiting*** your current knowledge of the values of the actions. If instead you select one of the nongreedy actions, then we say you are ***exploring***, because this enables you to improve your estimate of the nongreedy action\u2019s value.","3b17e244":"$$ A_t \\space \\dot{=} \\space \\underset{a}{\\mathrm{argmax}} \\space \\left[Q_t(a) + c \\space \\sqrt{\\frac{\\ln t}{N_t(a)}} \\right] $$ ","f6d561ef":"The weight decays exponentially according to the exponent on $1-\\alpha$. Accordingly, this is sometimes called an ***exponential recency-weighted average***.","74a51dc3":"## Nonstationary Problem","5a3b06e7":"Greedy action selection always exploits current knowledge to maximize immediate reward; it spends no time at all sampling apparently inferior actions to see if they might really be better. A simple alternative is to behave *greedily* most of the time, but every once in a while, say with small probability $\\varepsilon$, instead select randomlyfrom among all the actions with equal probability, independently of the action-value estimates. We call methods using this near-greedy action selection rule ***$\\varepsilon$-greedy*** methods.","88c38df8":"$$ \\text{(2)}\\space \\sum_{n=1}^{\\infty}\\alpha_n(a)^2 < \\infty $$","b48c3f7d":"## The 10-armed Testbed","940e9446":"If $N_t(a) = 0$, then a is considered to be a maximizing action.","52cc3e1c":"Note that both convergence conditions are met for the sample-average case, $\\alpha_n(a) = \\frac{1}{n}$ , but not for the case of constant step-size parameter, $\\alpha_n(a) = \\alpha$.","0fe1690a":"$$ \nQ_t(a) \\space \\dot{=} \n\\space \\frac{\\text{sum of rewards when $a$ taken prior to $t$}}{\\text{number of times $a$ taken prior to $t$}}\n = \\frac{\\sum_{t=1}^{t-1} R_i . \\mathbb{1}_{A_i = a}}{\\sum_{t=1}^{t-1} \\mathbb{1}_{A_i = a}}\n$$","84efb44b":"where $\\alpha > 0$ is a step-size parameter, and $\\bar{R_t} \\in {\\rm I\\!R}$ is the average of the rewards up to but not including time $t$. ","e7593e0e":"## Action-value Methods","6f8b169e":"In the latter case, the second condition is not met, indicating that the estimates never completely converge but continue to vary in response to the most recently received rewards. As we mentioned above, this is actually desirable in a nonstationary environment, and problems that are e\u21b5ectively nonstationary are the most common in reinforcement learning.","33583b0a":"The $\\bar{R_t}$ term serves as a baseline with which the reward is compared. If the reward is higher than the baseline, then the probability of taking At in the future is increased, and if the reward is below baseline, then the probability is decreased. The non-selected actions move in the opposite direction.","f8a53372":"This implementation requires memory only for Qn and n, and only the small computation for each new reward.","00041a83":"The larger the preference, the more often that action is taken, but the preference has no interpretation in terms of reward. Only the relative preference of one action over another is important.","364cb8da":"In **nonassociative tasks**, there is no need to associate different actions with different situations. In these tasks the learner either tries to find a single best action when the task is stationary, or tries to track the best action as it changes over time when the task is nonstationary.","2988d4be":"## Upper-Confidence-Bound Action Selection","e3aabbc6":"UCB often performs well, as shown here, but is more difficult than $\\varepsilon$-greedy to extend beyond bandits to the more general reinforcement learning settings.\nOne difficulty is in dealing with nonstationary problems, another difficulty is dealing with large state spaces."}}