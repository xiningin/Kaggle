{"cell_type":{"1d1af673":"code","473c0606":"code","324ecada":"code","65b1a88b":"code","0ea965f5":"code","d40aac5c":"code","f3d76f7a":"code","33116544":"code","7c060dc7":"code","8664c102":"code","5a0a089a":"code","ed463463":"code","6706a3cb":"code","6deeea10":"code","2c8f10ab":"code","855c1ece":"code","865cac92":"code","da3a8842":"code","2d139e4f":"code","a7213626":"markdown","e92c9d18":"markdown","bbbaacad":"markdown","f987e5b0":"markdown","6340a4c2":"markdown","ed64912b":"markdown","ff7a5fd0":"markdown","04af9cbe":"markdown","e625d6da":"markdown","59723c66":"markdown","fcc451fe":"markdown","5619c575":"markdown","ac222aea":"markdown"},"source":{"1d1af673":"import numpy as np\nimport pandas as pd \nfrom  datetime import datetime, timedelta\nimport lightgbm as lgb\nimport gc\nimport os\nimport matplotlib.pyplot as plt\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","473c0606":"from IPython.display import IFrame, YouTubeVideo\nYouTubeVideo('e8Yw4alG16Q',width=600, height=400)","324ecada":"BASE_PATH = '..\/input\/m5-forecasting-accuracy'\nMAX_LAGS = 70\nTR_LAST = 1913\nSTART_DATE = datetime(2016,4, 25)  # we are using data points where date > START_DATE to avoid memory errors\nSTART_DATE\n\n\n\ncalendar_df = pd.read_csv(f'{BASE_PATH}\/calendar.csv')\nstv_df = pd.read_csv(f'{BASE_PATH}\/sales_train_validation.csv')\nsp_df = pd.read_csv(f'{BASE_PATH}\/sell_prices.csv')\nsample_df = pd.read_csv(f'{BASE_PATH}\/sample_submission.csv')","65b1a88b":"def reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() \/ 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics: \n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() \/ 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) \/ start_mem))\n    return df","0ea965f5":"stv_df.head()","d40aac5c":"stv_df.shape","f3d76f7a":"# melting sales_train_validation dataframe to create demand column\nid_vars = ['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id']\nstv_df = pd.melt(stv_df, id_vars = id_vars, var_name = 'd', value_name = 'demand')","33116544":"print(f'DF shape after melting is {stv_df.shape}')\n\n# since we have 30490 rows prior melting, we melted 1913 different columns ['d_1', 'd_2', 'd_3', ..., 'd_1911', 'd_1912', 'd_1913']\n# 30490 * 1913 = 58327370","7c060dc7":"stv_df.head()","8664c102":"stv_df.store_id.unique()","5a0a089a":"stv_df.dept_id.unique()","ed463463":"stv_df = stv_df.loc[stv_df.store_id == 'CA_1']\nstv_df = stv_df.loc[stv_df.dept_id == 'HOBBIES_1']\n\n# Now we have nearly 6 years of sales data for sales of dept_id 'HOBBIES_1' in store_id 'CA_1' in state 'CA'\nstv_df.shape","6706a3cb":"cal_imp_cols = ['d', 'date', 'day', 'wday','week', 'month','quarter', 'year','is_weekend', 'event_name_1', 'event_type_1', ]\n\ndef add_date_features(df):\n    # date time features\n    df['date'] = pd.to_datetime(df['date'])\n    attrs = [\"year\", \"quarter\", \"month\", \"week\", \"day\", \"dayofweek\", \"is_year_end\", \"is_year_start\", \"is_quarter_end\", \\\n        \"is_quarter_start\", \"is_month_end\",\"is_month_start\",\n    ]\n\n    for attr in attrs:\n        dtype = np.int16 if attr == \"year\" else np.int8\n        df[attr] = getattr(df['date'].dt, attr).astype(dtype)\n    df[\"is_weekend\"] = df[\"dayofweek\"].isin([5, 6]).astype(np.int8)\n    return df\n\n\ncalendar_df = add_date_features(calendar_df)\ncalendar_df = calendar_df[cal_imp_cols]\ncalendar_df = reduce_mem_usage(calendar_df)","6deeea10":"calendar_df.head()","2c8f10ab":"import statsmodels.api as sm\n\n\nfor i in range(0, 5):\n    sm.graphics.tsa.plot_acf(list(stv_df.groupby(['id'])['demand'])[i][1], lags=40)\n    sm.graphics.tsa.plot_pacf(list(stv_df.groupby(['id'])['demand'])[i][1], lags=40)\n    \n# below graph shows that window 1, 7, 14, 21, 28 are better choices.","855c1ece":"# So, we choose window 1, 7 and 28 to calculate our lag features.\n\n\ndef create_lag_features(df):\n    \n    # shift and rolling demand features\n    shifts = [1, 7, 28]\n    for shift_window in shifts:\n        df[f\"shift_t{shift_window}\"] = df.groupby([\"id\"])[\"demand\"].transform(lambda x: x.shift(shift_window))\n            \n    # rolling mean\n    windows = [7, 28]\n    for val in windows:\n        for col in [f'shift_t{win}' for win in windows]:\n            df[f'roll_mean_{col}_{val}'] = df[['id', col]].groupby(['id'])[col].transform(lambda x: x.rolling(val).mean())\n    \n    # rolling standard deviation    \n    for val in windows:\n        for col in [f'shift_t{win}' for win in windows]:\n            df[f'roll_std_{col}_{val}'] = df[['id', col]].groupby(['id'])[col].transform(lambda x: x.rolling(val).std())\n    \n    return df\n    ","865cac92":"stv_df = create_lag_features(stv_df)","da3a8842":"stv_df.groupby(['id'])['shift_t1'].head(20)","2d139e4f":"stv_df.to_csv('features.csv')","a7213626":"## 1. Introduction \n\nIn this notebook, i will show how to do feature engineering for timeseries forecasting problems. I will try to make it make simple and begineer friendly and at the same time will try cover as much depth as possible.\n\nVideo explaining what is timeseries and how to do timeseries analysis<\/br>","e92c9d18":"## 2. Exploring Dataset\n\nFor better EDA and data exploration please visit [this](https:\/\/www.kaggle.com\/headsortails\/back-to-predict-the-future-interactive-m5-eda) great kernel.","bbbaacad":"> for illustration purpose and to make computation faster as well as avoiding memory errors i am taking data for only 1 store (CA_1)","f987e5b0":"\nmelting sales_train_validation dataframe to create demand column\n\nTo know more about what pd.melt does please visit [this](https:\/\/pandas.pydata.org\/pandas-docs\/version\/0.23.4\/generated\/pandas.melt.html) link.[](http:\/\/)","6340a4c2":"### How to find which lags to choose.\n\n> The lag value we choose will depend on the correlation of individual values with its past values.\n\n* There is more than one way of determining the lag at which the correlation is significant. For instance, we can use the `ACF (Autocorrelation Function)` and `PACF (Partial Autocorrelation Function)` plots.\n\n* `ACF`: The ACF plot is a measure of the correlation between the time series and the lagged version of itself\n* `PACF`: The PACF plot is a measure of the correlation between the time series with a lagged version of itself but after eliminating the variations already explained by the intervening comparisons\n* For our particular example, here are the ACF and PACF plots:","ed64912b":"## 3. Saving Features","ff7a5fd0":"### 2.2 Creating Lag Features\n\nPlease read [this](https:\/\/machinelearningmastery.com\/basic-feature-engineering-time-series-data-python\/) and [this](https:\/\/www.analyticsvidhya.com\/blog\/2019\/12\/6-powerful-feature-engineering-techniques-time-series\/) blog to know more about shift and rolling shift features.\n\n\nConsider this \u2013 you are predicting the demand for an item. So, the previous day\u2019s demand is important to make a prediction, right? In other words, the value at time t is greatly affected by the value at time t-1. The past values are known as lags, so `t-1 is lag 1`, `t-7 is lag 7`,`t-28 is lag 28`  and so on.\n\n![](https:\/\/cdn.analyticsvidhya.com\/wp-content\/uploads\/2019\/11\/3hotmk.gif)","04af9cbe":"### 3.1 Date features\n\n\n![Imgur](https:\/\/i.imgur.com\/0UORwwz.png)\n\n> Source: https:\/\/pandas.pydata.org\/pandas-docs\/version\/0.23\/api.html#datetimelike-properties\n","e625d6da":"#### Util function to reduce memory","59723c66":"## 3. Create Features","fcc451fe":"**We can calculate all sort of dataframe  statistics on rolling features**\n \n *  [sum, cumsum, min, max, mean, median, var, skew, kurt]","5619c575":"What about Version 3: \n\nIn version 3 of this notebook, i will explore following features:\n\n* Calculate item price related features\n* %age change in price over rolling window of [1, 7, 28] days.\n* More timeseries related features.\n* How to select top k important features\n\n#### > See you soon.","ac222aea":"* Help taken from these kernels\n* https:\/\/www.kaggle.com\/rohitsingh9990\/m5-lgbm-fe\n\n> Note: If you like my work, please, upvote \u263a"}}