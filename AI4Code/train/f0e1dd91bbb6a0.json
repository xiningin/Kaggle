{"cell_type":{"10aac886":"code","90e24e5d":"code","8ed08170":"code","efd13f41":"code","ee644738":"code","d120167e":"code","0a449cb2":"code","b070083b":"code","786bc73a":"code","5abb1c62":"code","1e0ebf5a":"code","7e243d76":"code","2e475914":"code","6f76bc07":"code","d2dfc60d":"code","08e324a3":"code","53ee9828":"code","8b85f65d":"code","09ee80ab":"markdown","93723195":"markdown","99658aa0":"markdown","12c32c12":"markdown","8bca662b":"markdown","bc45ca94":"markdown","df64699b":"markdown","d0951857":"markdown","ccdad916":"markdown","e9cd67f6":"markdown","b8b0d53b":"markdown","74b872e0":"markdown","5dda49f3":"markdown","c202470b":"markdown","b0b412e3":"markdown","390a653b":"markdown","cf5d5b27":"markdown","cb41ce4a":"markdown","a1fb6b2a":"markdown","4c6fdb9c":"markdown","17bd2264":"markdown","8a682fed":"markdown","922b21b5":"markdown","f4004c87":"markdown","5c95bd12":"markdown","8080c194":"markdown","48a9ae6a":"markdown","e456ddcf":"markdown","7c6638d7":"markdown","f67eaaa8":"markdown","da712871":"markdown","eed463fc":"markdown"},"source":{"10aac886":"import os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nimport seaborn as sns\n\nfrom sklearn.model_selection import train_test_split\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n\nimg_folder = '\/kaggle\/input\/captcha-version-2-images\/samples\/samples\/'","90e24e5d":"img_2g7nm = mpimg.imread(img_folder + '2g7nm.png')\nimg_34pcn = mpimg.imread(img_folder + '34pcn.png')\nimg_bny23 = mpimg.imread(img_folder + 'bny23.png')\nimg_c4mcm = mpimg.imread(img_folder + 'c4mcm.png')\nimg_3c7de = mpimg.imread(img_folder + '3c7de.jpg')\nimg_nxf2c = mpimg.imread(img_folder + 'nxf2c.jpg')\nimg_pcmcc = mpimg.imread(img_folder + 'pcmcc.jpg')\nimg_yge7c = mpimg.imread(img_folder + 'yge7c.jpg')\nsamples = {'2g7nm.png':img_2g7nm, '34pcn.png':img_34pcn, 'bny23.png':img_bny23, 'c4mcm.png':img_c4mcm, \n           '3c7de.jpg':img_3c7de, 'nxf2c.jpg':img_nxf2c, 'pcmcc.jpg':img_pcmcc, 'yge7c.jpg':img_yge7c}\n\nfig=plt.figure(figsize=(20, 5))\npos = 1\nfor filename, img in samples.items():\n    fig.add_subplot(2, 4, pos)\n    pos = pos+1\n    plt.imshow(img)\n    plt.title('filename='+filename+' shape='+str(img.shape))\nplt.show()","8ed08170":"df = pd.DataFrame(columns=['filename','extension','label','labelsize','char1','char2','char3','char4','char5'])\ni = 0\nfor _, _, files in os.walk(img_folder):\n    for f in files:\n        df.loc[i,'filename'] = f\n        df.loc[i,'extension'] = f.split('.')[1]\n        df.loc[i,'label'] = f.split('.')[0]\n        df.loc[i,'labelsize'] = len(f.split('.')[0])\n        df.loc[i,'char1'] = f.split('.')[0][0]\n        df.loc[i,'char2'] = f.split('.')[0][1]\n        df.loc[i,'char3'] = f.split('.')[0][2]\n        df.loc[i,'char4'] = f.split('.')[0][3]\n        df.loc[i,'char5'] = f.split('.')[0][4]\n        i = i+1\n        \n#df.head()\n\ndata = pd.DataFrame(df['char1'].value_counts()+df['char2'].value_counts()+df['char3'].value_counts()+df['char4'].value_counts()+df['char5'].value_counts()).reset_index()\ndata.columns = ['character','count']\n\nsns.barplot(data=data, x='character', y='count')\nplt.show()","efd13f41":"def compute_perf_metric(predictions, groundtruth):\n    if predictions.shape == groundtruth.shape:\n        return np.sum(predictions == groundtruth)\/(predictions.shape[0]*predictions.shape[1])\n    else:\n        raise Exception('Error : the size of the arrays do not match. Cannot compute the performance metric')","ee644738":"# Dictionaries that will be used to convert characters to integers and vice-versa\nvocabulary = {'2','3','4','5','6','7','8','b','c','d','e','f','g','m','n','p','w','x','y'}\nchar_to_num = {'2':0,'3':1,'4':2,'5':3,'6':4,'7':5,'8':6,'b':7,'c':8,'d':9,'e':10,'f':11,'g':12,'m':13,'n':14,'p':15,'w':16,'x':17,'y':18}\n\n##############################################################################################################################\n# This function encodes a single sample. \n# Inputs :\n# - img_path : the string representing the image path e.g. '\/kaggle\/input\/captcha-version-2-images\/samples\/samples\/6n6gg.jpg'\n# - label : the string representing the label e.g. '6n6gg'\n# - crop : boolean, if True the image is cropped around the characters and resized to the original size.\n# Outputs :\n# - a multi-dimensional array reprensenting the image. Its shape is (50, 200, 1)\n# - an array of integers representing the label after encoding the characters to integer. E.g [6,16,6,14,14] for '6n6gg' \n##############################################################################################################################\ndef encode_single_sample(img_path, label, crop):\n    # Read image file and returns a tensor with dtype=string\n    img = tf.io.read_file(img_path)\n    # Decode and convert to grayscale (this conversion does not cause any information lost and reduces the size of the tensor)\n    # This decode function returns a tensor with dtype=uint8\n    img = tf.io.decode_png(img, channels=1) \n    # Scales and returns a tensor with dtype=float32\n    img = tf.image.convert_image_dtype(img, tf.float32)\n    # Crop and resize to the original size : \n    # top-left corner = offset_height, offset_width in image = 0, 25 \n    # lower-right corner is at offset_height + target_height, offset_width + target_width = 50, 150\n    if(crop==True):\n        img = tf.image.crop_to_bounding_box(img, offset_height=0, offset_width=25, target_height=50, target_width=125)\n        img = tf.image.resize(img,size=[50,200],method='bilinear', preserve_aspect_ratio=False,antialias=False, name=None)\n    # Transpose the image because we want the time dimension to correspond to the width of the image.\n    img = tf.transpose(img, perm=[1, 0, 2])\n    # Converts the string label into an array with 5 integers. E.g. '6n6gg' is converted into [6,16,6,14,14]\n    label = list(map(lambda x:char_to_num[x], label))\n    return img.numpy(), label\n\ndef create_train_and_validation_datasets(crop=False):\n    # Loop on all the files to create X whose shape is (1040, 50, 200, 1) and y whose shape is (1040, 5)\n    X, y = [],[]\n\n    for _, _, files in os.walk(img_folder):\n        for f in files:\n            # To start, let's ignore the jpg images\n            label = f.split('.')[0]\n            extension = f.split('.')[1]\n            if extension=='png':\n                img, label = encode_single_sample(img_folder+f, label,crop)\n                X.append(img)\n                y.append(label)\n\n    X = np.array(X)\n    y = np.array(y)\n\n    # Split X, y to get X_train, y_train, X_val, y_val \n    X_train, X_val, y_train, y_val = train_test_split(X.reshape(1040, 10000), y, test_size=0.1, shuffle=True, random_state=42)\n    X_train, X_val = X_train.reshape(936,200,50,1), X_val.reshape(104,200,50,1)\n    return X_train, X_val, y_train, y_val","d120167e":"X_train, X_val, y_train, y_val = create_train_and_validation_datasets(crop=True)\nX_train_, X_val_, y_train_, y_val_ = create_train_and_validation_datasets(crop=False)\n\nfig=plt.figure(figsize=(20, 10))\nfig.add_subplot(2, 4, 1)\nplt.imshow(X_train[0], cmap='gray')\n#plt.imshow(X_train[0].transpose((1,0,2)), cmap='gray')\nplt.title('Image from X_train with label '+ str(y_train[0]))\nplt.axis('off')\nfig.add_subplot(2, 4, 2)\nplt.imshow(X_train[935], cmap='gray')\n#plt.imshow(X_train[935].transpose((1,0,2)), cmap='gray')\nplt.title('Image from X_train with label '+ str(y_train[935]))\nplt.axis('off')\nfig.add_subplot(2, 4, 3)\nplt.imshow(X_val[0], cmap='gray')\n#plt.imshow(X_val[0].transpose((1,0,2)), cmap='gray')\nplt.title('Image from X_val with label '+ str(y_val[0]))\nplt.axis('off')\nfig.add_subplot(2, 4, 4)\nplt.imshow(X_val[103], cmap='gray')\n#plt.imshow(X_val[103].transpose((1,0,2)), cmap='gray')\nplt.title('Image from X_val with label '+ str(y_val[103]))\nplt.axis('off')\nfig.add_subplot(2, 4, 5)\nplt.imshow(X_train_[0], cmap='gray')\nplt.title('Image from X_train with label '+ str(y_train_[0]))\nplt.axis('off')\nfig.add_subplot(2, 4, 6)\nplt.imshow(X_train_[935], cmap='gray')\nplt.title('Image from X_train with label '+ str(y_train_[935]))\nplt.axis('off')\nfig.add_subplot(2, 4, 7)\nplt.imshow(X_val_[0], cmap='gray')\nplt.title('Image from X_val with label '+ str(y_val_[0]))\nplt.axis('off')\nfig.add_subplot(2, 4, 8)\nplt.imshow(X_val_[103], cmap='gray')\nplt.title('Image from X_val with label '+ str(y_val_[103]))\nplt.axis('off')\nplt.show()","0a449cb2":"def build_model():\n    \n    # Inputs to the model\n    input_img = layers.Input(shape=(200,50,1), name=\"image\", dtype=\"float32\") \n\n    # First conv block\n    x = layers.Conv2D(32,(3, 3),activation=\"relu\",kernel_initializer=\"he_normal\",padding=\"same\",name=\"Conv1\")(input_img)\n    x = layers.MaxPooling2D((2, 2), name=\"pool1\")(x)\n\n    # Second conv block\n    x = layers.Conv2D(64,(3, 3),activation=\"relu\",kernel_initializer=\"he_normal\",padding=\"same\",name=\"Conv2\")(x)\n    x = layers.MaxPooling2D((2, 2), name=\"pool2\")(x)\n\n    # We have used two max pool with pool size and strides 2.\n    # Hence, downsampled feature maps are 4x smaller. The number of\n    # filters in the last layer is 64 --> output volume shape = (50,12,64) \n    # Reshape to \"split\" the volume in 5 time-steps\n    x = layers.Reshape(target_shape=(5, 7680), name=\"reshape\")(x)\n\n    # FC layers\n    x = layers.Dense(256, activation=\"relu\", name=\"dense1\")(x)\n    x = layers.Dense(64, activation=\"relu\", name=\"dense2\")(x)\n   \n    # Output layer\n    output = layers.Dense(19, activation=\"softmax\", name=\"dense3\")(x) \n    \n    # Define the model\n    model = keras.models.Model(inputs=input_img, outputs=output, name=\"ocr_classifier_based_model\")\n    \n    # Compile the model and return\n    model.compile(optimizer=keras.optimizers.Adam(), loss=\"sparse_categorical_crossentropy\", metrics=\"accuracy\")\n    return model\n\n\n# Get the model\nmodel = build_model()\nmodel.summary()","b070083b":"X_train, X_val, y_train, y_val = create_train_and_validation_datasets(crop=True)\nhistory = model.fit(x=X_train, y=y_train, validation_data=(X_val, y_val), epochs=30)","786bc73a":"fig=plt.figure(figsize=(20, 5))\n# summarize history for accuracy\nfig.add_subplot(1, 2, 1)\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('Model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper left')\n\n# summarize history for loss\nfig.add_subplot(1, 2, 2)\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('Model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper left')\nplt.show()","5abb1c62":"y_pred = model.predict(X_val) # y_pred shape = (104,50,19)\ny_pred = np.argmax(y_pred, axis=2)\nnum_to_char = {'-1':'UKN','0':'2','1':'3','2':'4','3':'5','4':'6','5':'7','6':'8','7':'b','8':'c','9':'d','10':'e','11':'f','12':'g','13':'m','14':'n','15':'p','16':'w','17':'x','18':'y'}\nnrow = 1\nfig=plt.figure(figsize=(20, 5))\nfor i in range(0,10):\n    if i>4: nrow = 2\n    fig.add_subplot(nrow, 5, i+1)\n    plt.imshow(X_val[i].transpose((1,0,2)),cmap='gray')\n    plt.title('Prediction : ' + str(list(map(lambda x:num_to_char[str(x)], y_pred[i]))))\n    plt.axis('off')\nplt.show() ","1e0ebf5a":"compute_perf_metric(y_pred, y_val)","7e243d76":"# Let's create a new CTCLayer by subclassing\nclass CTCLayer(layers.Layer):\n    def __init__(self, name=None):\n        super().__init__(name=name)\n        self.loss_fn = keras.backend.ctc_batch_cost\n\n    def call(self, y_true, y_pred):\n        # Compute the training-time loss value and add it to the layer using `self.add_loss()`.\n        batch_len = tf.cast(tf.shape(y_true)[0], dtype=\"int64\")\n        input_length = tf.cast(tf.shape(y_pred)[1], dtype=\"int64\")\n        label_length = tf.cast(tf.shape(y_true)[1], dtype=\"int64\")\n\n        input_length = input_length * tf.ones(shape=(batch_len, 1), dtype=\"int64\")\n        label_length = label_length * tf.ones(shape=(batch_len, 1), dtype=\"int64\")\n\n        loss = self.loss_fn(y_true, y_pred, input_length, label_length)\n        self.add_loss(loss)\n\n        # At test time, just return the computed predictions\n        return y_pred\n\ndef build_model():\n    \n    # Inputs to the model\n    input_img = layers.Input(shape=(200,50,1), name=\"image\", dtype=\"float32\") \n    labels = layers.Input(name=\"label\", shape=(None,), dtype=\"float32\")\n\n    # First conv block\n    x = layers.Conv2D(32,(3, 3),activation=\"relu\",kernel_initializer=\"he_normal\",padding=\"same\",name=\"Conv1\")(input_img)\n    x = layers.MaxPooling2D((2, 2), name=\"pool1\")(x)\n\n    # Second conv block\n    x = layers.Conv2D(64,(3, 3),activation=\"relu\",kernel_initializer=\"he_normal\",padding=\"same\",name=\"Conv2\")(x)\n    x = layers.MaxPooling2D((2, 2), name=\"pool2\")(x)\n\n    # We have used two max pool with pool size and strides 2.\n    # Hence, downsampled feature maps are 4x smaller. The number of\n    # filters in the last layer is 64. Reshape accordingly before\n    # passing the output to the RNN part of the model \n    x = layers.Reshape(target_shape=(50, 768), name=\"reshape\")(x)\n    x = layers.Dense(64, activation=\"relu\", name=\"dense1\")(x)\n    x = layers.Dropout(0.2)(x)\n\n    # RNNs\n    x = layers.Bidirectional(layers.LSTM(128, return_sequences=True, dropout=0.25))(x)\n    x = layers.Bidirectional(layers.LSTM(64, return_sequences=True, dropout=0.25))(x)\n\n    # Output layer\n    x = layers.Dense(20, activation=\"softmax\", name=\"dense2\")(x) # 20 = 19 characters + UKN\n\n    # Add CTC layer for calculating CTC loss at each step\n    output = CTCLayer(name=\"ctc_loss\")(labels, x)\n\n    # Define the model\n    model = keras.models.Model(inputs=[input_img, labels], outputs=output, name=\"ocr_cnn_lstm_model\")\n    \n    # Compile the model and return\n    model.compile(optimizer=keras.optimizers.Adam())\n    return model\n\n\n# Get the model\nmodel = build_model()\nmodel.summary()","2e475914":"X_train, X_val, y_train, y_val = create_train_and_validation_datasets(crop=False)\n\n# Add early stopping\nearly_stopping = keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=10, restore_best_weights=True)\n\n# Train the model\nhistory = model.fit([X_train, y_train], validation_data=[X_val, y_val], epochs=100, callbacks=[early_stopping],)","6f76bc07":"plt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('CTC loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'])\nplt.show()","d2dfc60d":"prediction_model = keras.models.Model(\n    model.get_layer(name=\"image\").input, model.get_layer(name=\"dense2\").output\n)\nprediction_model.summary()","08e324a3":"y_pred = prediction_model.predict(X_val) # y_pred shape = (104,50,20)\ny_pred = keras.backend.ctc_decode(y_pred, input_length=np.ones(104)*50, greedy=True) # decoding -> y_pred[0].shape = (104,5)\n#y_pred[0][0][X][0:5] corresponds to the prediction of one image (with X in [0,...,103])\n#it is a tensor whose corresponding numpy array is for example [15,  7, 15, 12,  8]\ny_pred = y_pred[0][0][0:104,0:5].numpy() # y_pred.shape=(104,5)\n#array([[ 2,  2,  8,  0,  0],\n#       [15,  7, 15, 12,  8],\n#       ...\n#       [13, 17, 13, 16,  2],\n#       [10,  3, 14,  4,  4]])","53ee9828":"num_to_char = {'-1':'UKN','0':'2','1':'3','2':'4','3':'5','4':'6','5':'7','6':'8','7':'b','8':'c','9':'d','10':'e','11':'f','12':'g','13':'m','14':'n','15':'p','16':'w','17':'x','18':'y'}\nnrow = 1\nfig=plt.figure(figsize=(20, 5))\nfor i in range(0,10):\n    if i>4: nrow = 2\n    fig.add_subplot(nrow, 5, i+1)\n    plt.imshow(X_val[i].transpose((1,0,2)),cmap='gray')\n    plt.title('Prediction : ' + str(list(map(lambda x:num_to_char[str(x)], y_pred[i]))))\n    plt.axis('off')\nplt.show()    \n","8b85f65d":"compute_perf_metric(y_pred, y_val)","09ee80ab":"<div style=\"display:fill; background-color:#000000;border-radius:5px;\">\n    <p style=\"font-size:300%; color:white;text-align:center\";>The CNN LSTM model<\/p>\n<\/div>","93723195":"https:\/\/machinelearningmastery.com\/cnn-long-short-term-memory-networks\/  \nhttps:\/\/keras.io\/examples\/vision\/captcha_ocr\/   \nhttps:\/\/www.tensorflow.org\/guide\/keras\/rnn  \nhttps:\/\/towardsdatascience.com\/intuitively-understanding-connectionist-temporal-classification-3797e43a86c","99658aa0":"![2en7g_big.png](attachment:9cb4594c-f5cb-434c-becb-cc4f833f4cb9.png)","12c32c12":"<div style=\"display:fill; background-color:#000000;border-radius:5px;\">\n    <p style=\"font-size:300%; color:white;text-align:center\";>Creation of the training and validation datasets<\/p>\n<\/div>","8bca662b":"## Build Model","bc45ca94":"Now we have a trained model that performs well on both the training and validation sets from a CTC loss perspective. Let's shrink it to take only the layers that will be used to make predictions :","df64699b":"This section generates the training set that will be used to train the neural network and the validation set that will be used to evaluate the model performance.\n\nThe training set will have 90% of the data :\n- X_train with 936 images. X_train shape will be (936,50,200,1) \n- y_train with 936 labels. y_train shape will be (936,5)  \n\nThe validation set will have 10% of the data \n- X_val with 104 images. X_val shape will be (104,50,200,1) \n- y_val with 104 labels. y_val shape will be (104,5)\n\nThe label corresponding to each image is a string corresponding to the filename minus the extension. As the neural net deals only with numerical values, we will have to map each character in the string to an integer.","d0951857":"<div style=\"display:fill; background-color:#000000;border-radius:5px;\">\n    <p style=\"font-size:300%; color:white;text-align:center\";>Quick EDA<\/p>\n<\/div>","ccdad916":"## Train the model","e9cd67f6":"Each image has 50x200 pixels. That said, the shape of the array differs between jpg and png images indicating that jpg images are RGB and png are RGBA. Since images are black & white, R, G and B are all similar. Also for png images, there is no differecence in transparency (alpha is always 1). ","b8b0d53b":"We will assess the performance of the different models on the validation dataset that contains 10% of the complete dataset. \n\nFor each image in the validation set, we will compare the true label with the prediction. The true label and the prediction are (5,1) vectors. Therefore we will compare the vectors element-wise and count the number of good predictions element-wise. For each image, the score will be (nb of good predictions element-wise)\/5. \n\nE.g. :   \n- the true label corresponding to the image 368y5.png is [1 4 6 18 3] (after transforming the characters to numerical values)\n- if the predicted label is [1 4 6 18 3], then the score for this image is 1\n- if the predicted label is [2 6 4 10 1], then the score for this image is 0\n- if the predicted label is [1 6 6 10 1], then the score for this image is 0.4 (2 characters correctly predicted)\n\n**The performance of the model will be the average of the scores for each image in the validation set. This number corresponds to the percentage of characters that are correctly predicted.**","74b872e0":"**This solution solution combines a CNN and a FC Classifier and achieves pretty good performance (>94%)**. It :\n- crops the image around the characters and flips the cropped image (as we want the time dimension to correspond to the width of the image).  \n- uses a CNN to extract features from the cropped image.\n- reshapes the features to \"split\" them into 5 time-steps. \n- uses a FC Classifier to predict 5 characters : for each time-step, the output is the probability distribution of a character being at this step.  \n\n![architecture_CNN_FC classifier.PNG](attachment:ad2d6e95-a576-44fa-902c-4c4a8383216e.PNG)","5dda49f3":"Let's display 10 images and their predicted labels. This model is really making great:","c202470b":"<div style=\"display:fill; background-color:#000000;border-radius:5px;\">\n    <p style=\"font-size:300%; color:white;text-align:center\";>References<\/p>\n<\/div>","b0b412e3":"<div style=\"display:fill; background-color:#000000;border-radius:5px;\">\n    <p style=\"font-size:300%; color:white;text-align:center\";>The classifier-like model<\/p>\n<\/div>","390a653b":"<div style=\"display:fill; background-color:#000000;border-radius:5px;\">\n    <p style=\"font-size:300%; color:white;text-align:center\";>Abstract<\/p>\n<\/div>","cf5d5b27":"Let's use this model to predict the labels for the 104 images of the validation set :","cb41ce4a":"**This solution implements a CNN LSTM architecture with CTC loss and achieves great performance (>99%). All the credits for this solution go to [A_K_Nain](https:\/\/keras.io\/examples\/vision\/captcha_ocr\/)**. It:\n- flips the input image (as we want the \"time dimension\" to correspond to the width of the image).\n- uses a CNN to extract features from the image.\n- uses a CNN encoder (which consists of a single Fully Connected layer) to encode the features into 50 time-steps\n- uses 2 BRNNs to process a sequence of 50 steps : for each step, the output is the probability distribution of a character being at this step.\n- uses a CTC decoder to predict 5 characters.\n\n![architecture_CNN_LSTM.PNG](attachment:86692cd7-913b-4e30-b046-eaa78f692558.PNG)\n","a1fb6b2a":"A quick overview of the dataset provided by @fournierp https:\/\/www.kaggle.com\/fournierp\/captcha-version-2-images :\n- The dataset contains 1070 files. \n- Each file is an image representing a CAPTCHA image. \n- The image is either in png format (1040 png files) or in jpg format (30 jpg files). \n- The file name is composed of the 5 characters contained in the CAPTCHA image followed by the image format (e.g. 2g7nm.png) ","4c6fdb9c":"Let's check a few examples :   \n- images on the 1st row are transposed, cropped around the characters and resized as expected by the \"classifier-like\" model\n- images on the 2nd row are transposed only as expected by the CNN LSTM model","17bd2264":"## Predictions","8a682fed":"Let's use the function compute_perf_metric to calculate the performance metric of this model. Reminder : this metric corresponds to the percentage of characters that are correctly predicted (and this % is calculated using the 104 images hence 520 characters of the validation set).","922b21b5":"## Build Model","f4004c87":"<div style=\"display:fill; background-color:#000000;border-radius:5px;\">\n    <p style=\"font-size:300%; color:white;text-align:center\";>Definition of the evaluation metric<\/p>\n<\/div>","5c95bd12":"## Model Performance","8080c194":"Now, looking at the characters in the CAPTCHA images, one can notice that:\n- only 19 characters are used 2,3,4,5,6,7,8 and b,c,d,e,f,g,m,n,p,w,x,y\n- the frequency of each character is roughly the same with one exception : n is used twice often than other characters","48a9ae6a":"## Predictions","e456ddcf":"In this notebook, I used the [CAPTCHA dataset](https:\/\/www.kaggle.com\/fournierp\/captcha-version-2-images) provided by @fournierp and I tested 2 solutions to read CAPTCHA images. \n\n**The 1st solution solution combines a CNN and a FC Classifier and achieves pretty good performance (>94%)**. It :\n- crops the image around the characters and flips the cropped image (as we want the time dimension to correspond to the width of the image).  \n- uses a CNN to extract features from the cropped image.\n- reshapes the features to \"split\" them into 5 time-steps. \n- uses a FC Classifier to predict 5 characters : for each time-step, the output is the probability distribution of a character being at this step.  \n\n**The 2nd solution implements a CNN LSTM architecture with CTC operation and achieves great performance (>99%). All the credits for this solution go to [A_K_Nain](https:\/\/keras.io\/examples\/vision\/captcha_ocr\/)**. It:\n- flips the input image (as we want the \"time dimension\" to correspond to the width of the image).\n- uses a CNN to extract features from the image.\n- uses a CNN encoder (which consists of a single Fully Connected layer) to encode the features into 50 time-steps\n- uses 2 BRNNs to process a sequence of 50 steps : for each time-step, the output is the probability distribution of a character being at this step.\n- uses a CTC decoder to predict 5 characters.  ","7c6638d7":"Let's use this model to predict the labels for the 104 images of the validation set :","f67eaaa8":"## Train the model","da712871":"## Model performance","eed463fc":"Let's use the function compute_perf_metric to calculate the performance metric of this model. Reminder : this metric corresponds to the percentage of characters that are correctly predicted (and this % is calculated using the 104 images hence 520 characters of the validation set)."}}