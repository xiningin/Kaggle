{"cell_type":{"cbe41136":"code","7b0e38e9":"code","89681ccd":"code","f3d7d35d":"code","325d281c":"code","2594526c":"code","516d3ed7":"code","2e324ab3":"code","bfe94109":"code","d40ba5ab":"code","461628dd":"code","cc35ed9d":"code","05bca75e":"code","fcd52472":"code","8ae92314":"code","a2f7f417":"code","0f354036":"code","0ba57769":"code","464de230":"code","af3da225":"code","2b54e15e":"code","964efb64":"code","7112af94":"code","75b4e7a3":"code","3fa76918":"code","b39317c0":"code","3a1b50f6":"code","2820a5fb":"code","eb18236d":"code","f538dc6c":"code","e21ad22f":"code","61f789ac":"code","66a23162":"code","2d9fe4cb":"code","1298fa43":"code","c604ae43":"code","fca68959":"code","dbd44269":"code","93222dcc":"code","46b72e27":"code","698d3ab4":"code","b53ac8db":"code","934f766a":"code","ebd3a1eb":"code","7f06e840":"code","ecc59f2f":"code","19fb3258":"code","5ff77a67":"code","e3034dd7":"markdown","373407be":"markdown","ec527872":"markdown","1d56aa14":"markdown","60a6692a":"markdown","56bddacc":"markdown","c2667800":"markdown","904ed1e2":"markdown","3c9c60c9":"markdown","e1301efa":"markdown","4ebf765d":"markdown","53115cc3":"markdown","34200ffd":"markdown","c9c46fe5":"markdown","1af07e8e":"markdown","cbbdffd5":"markdown","8d4158bc":"markdown","d9e4a538":"markdown","d7f449c8":"markdown","316aed6b":"markdown","c94c131e":"markdown","1a1efad4":"markdown","90f688a5":"markdown","30a6c0d2":"markdown","d364f259":"markdown","1dfe8017":"markdown","763babac":"markdown","da3a7a3a":"markdown","68dcb5ff":"markdown"},"source":{"cbe41136":"import pandas as pd\npd.set_option('display.max_columns', None)\nimport matplotlib.pyplot as plt \nimport seaborn as sns\n\n%matplotlib inline","7b0e38e9":"## for exploration we'll use only training data\ntrain = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\nprint(train.shape)\nprint(test.shape)","89681ccd":"train.info()","f3d7d35d":"train.describe()","325d281c":"## divide quantitive and categorical variables\ndf_num = train[['Age','SibSp','Parch','Fare']]\ndf_cat = train[['Survived','Pclass','Sex','Ticket','Cabin','Embarked']]","2594526c":"#distributions for all quantitive variables \nfor i in df_num.columns:\n    plt.hist(df_num[i], bins=10)\n    plt.title(i)\n    plt.show();","516d3ed7":"corr = df_num.corr()\nprint(corr)\nsns.heatmap(corr, cmap='YlGnBu');","2e324ab3":"sns.pairplot(df_num);","bfe94109":"sns.violinplot(x=train.Survived, y=train.Age, data=train);","d40ba5ab":"sns.violinplot(x=train.Survived, y=train.Fare, data=train);","461628dd":"sns.violinplot(x=train.Survived, y=train.SibSp, data=train);","cc35ed9d":"sns.violinplot(x=train.Survived, y=train.Parch, data=train);","05bca75e":"df_num = train[['Age','SibSp','Parch','Fare', 'Survived']]\n\npd.pivot_table(df_num, index='Survived', aggfunc=['mean', 'median'])","fcd52472":"for i in df_cat.columns:\n    sns.barplot(df_cat[i].value_counts().index,df_cat[i].value_counts()).set_title(i)\n    plt.show()","8ae92314":"for i in df_cat.columns:\n    if i in ['Survived', 'Cabin', 'Ticket']:\n        pass\n    else:\n        ct = pd.crosstab(train[i], train.Survived).apply(lambda x: x\/x.sum(), axis=1)\n        ct.plot(kind='bar', stacked=True)\n","a2f7f417":"for i in df_cat.columns:\n    if i in ['Survived', 'Cabin', 'Ticket']:\n        pass\n    else:\n        ct = pd.crosstab(train.Survived, train[i]).apply(lambda x: x\/x.sum(), axis=1)\n        ct.plot(kind='bar', stacked=True)","0f354036":"## making age groups\ntrain['Age'] = train.Age.fillna(train.Age.median())\ntrain['agegroup'] = pd.cut(train.Age, [0, 10, 20, 30, 40, 50, 60, 70, 80], include_lowest=False)\ntrain.agegroup.value_counts()","0ba57769":"## see how agegroup relate to Survived\npd.crosstab(train.Survived, train.agegroup).apply(lambda x: x\/x.sum(), axis=0)","464de230":"## getting person's title\ntrain['name_title'] = train.Name.apply(lambda x: x.split(',')[1].split('.')[0].strip())\ntrain.name_title.value_counts()","af3da225":"## see how name title relate to Survived\npd.crosstab(train.Survived, train.name_title).apply(lambda x: x\/x.sum(), axis=0)","2b54e15e":"## create new categorical in test\ntest['Age'] = test.Age.fillna(test.Age.median())\ntest['agegroup'] = pd.cut(test.Age, [0, 10, 20, 30, 40, 50, 60, 70, 80], include_lowest=False)\ntest['name_title'] = test.Name.apply(lambda x: x.split(',')[1].split('.')[0].strip())\n\nprint(train.columns)\nprint(test.columns)","964efb64":"train.isna().value_counts().unstack()","7112af94":"test.isna().value_counts().unstack()","75b4e7a3":"train.dropna(subset=['Embarked'], inplace=True)\ntest['Fare'] = test.Fare.fillna(test.Fare.median())","3fa76918":"train.isna().value_counts().unstack()","b39317c0":"test.isna().value_counts().unstack()","3a1b50f6":"import numpy as np\n\n## normalize fare\ntrain['Fare'] = np.log(train.Fare+1)\ntest['Fare'] = np.log(test.Fare+1)\n\nprint(train.columns)\nprint(test.columns)","2820a5fb":"## Select features and target\nfeatures = ['Pclass', 'Sex', 'Embarked', 'agegroup', 'SibSp', 'Parch', 'Fare', 'name_title']\n\ny = train['Survived']\nX_train = train[features]\nX_test = test[features]\n\nprint(X_train.columns)\nprint(X_test.columns)","eb18236d":"## get dummies\nX_train = pd.get_dummies(X_train)\nX_test = pd.get_dummies(X_test)","f538dc6c":"# Scale numeric data data\nfrom sklearn.preprocessing import StandardScaler\nX_train[['SibSp','Parch','Fare']] = StandardScaler().fit_transform(X_train[['SibSp','Parch','Fare']])\nX_test[['SibSp','Parch','Fare']] = StandardScaler().fit_transform(X_test[['SibSp','Parch','Fare']])","e21ad22f":"print(X_train.shape)\nprint(X_test.shape)","61f789ac":"X_train.drop(X_train.columns.difference(list(X_test.columns)), axis=1, inplace=True) \nX_test.drop(X_test.columns.difference(list(X_train.columns)), axis=1, inplace=True)\nprint(X_train.shape)\nprint(X_test.shape)","66a23162":"print(list(X_test.columns))\nprint(list(X_train.columns))","2d9fe4cb":"## import cross_val for the scores\nfrom sklearn.model_selection import cross_val_score","1298fa43":"from sklearn.linear_model import LogisticRegression\n\nlr = LogisticRegression()\ncv = cross_val_score(lr, X_train, y, cv=5)\nprint(cv)\nprint(cv.mean())","c604ae43":"from sklearn.naive_bayes import GaussianNB\n\ngnb = GaussianNB()\ncv = cross_val_score(gnb, X_train, y, cv=5)\nprint(cv)\nprint(cv.mean())","fca68959":"from sklearn.svm import SVC\n\n# The default SVC kernel is radial basis function (RBF)\nsvc_rbf = SVC(probability=True)\ncv = cross_val_score(svc_rbf, X_train, y, cv=5)\nprint('RBF kernel')\nprint(cv)\nprint(cv.mean())\nprint()\n\n# kernel poly\nsvc_poly = SVC(kernel='poly', probability=True)\ncv = cross_val_score(svc_poly, X_train, y, cv=5)\nprint('Poly kernel')\nprint(cv)\nprint(cv.mean())","dbd44269":"from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import cross_val_predict\n\nerror = []\nfor k in range(1,51):\n    knn = KNeighborsClassifier(n_neighbors=k)\n    y_pred = cross_val_predict(knn, X_train, y, cv=5)\n    error.append(mean_squared_error(y,y_pred))\n    \nplt.plot(range(1,51), error);","93222dcc":"## look only between 1 and 10\n\nerror = []\nfor k in range(1,11):\n    knn = KNeighborsClassifier(n_neighbors=k)\n    y_pred = cross_val_predict(knn, X_train, y, cv=5)\n    error.append(mean_squared_error(y,y_pred))\n    \nplt.plot(range(1,11), error);","46b72e27":"knn = KNeighborsClassifier(n_neighbors=7)\ncv = cross_val_score(knn, X_train, y, cv=5)\nprint(cv)\nprint(cv.mean())","698d3ab4":"from sklearn.ensemble import RandomForestClassifier\n\nrf = RandomForestClassifier(n_estimators=100)\ncv = cross_val_score(rf, X_train, y, cv=5)\nprint(cv)\nprint(cv.mean())","b53ac8db":"from sklearn.ensemble import GradientBoostingClassifier\n\ngbc = GradientBoostingClassifier(n_estimators=150, random_state=0)\ncv = cross_val_score(gbc, X_train, y, cv=5)\nprint(cv)\nprint(cv.mean())","934f766a":"from sklearn.ensemble import VotingClassifier\nvoting_clf_soft = VotingClassifier(estimators = [('lr',lr),('gnb',gnb),('svc_rbf',svc_rbf),('svc_poly',svc_poly), \n                                                 ('knn',knn),('rf',rf),('gbc',gbc)], \n                                   voting = 'soft')\nvoting_clf_hard = VotingClassifier(estimators = [('lr',lr),('gnb',gnb),('svc_rbf',svc_rbf),('svc_poly',svc_poly),\n                                                 ('knn',knn),('rf',rf),('gbc',gbc)], \n                                   voting = 'hard')","ebd3a1eb":"cv = cross_val_score(voting_clf_soft,X_train,y,cv=5)\nprint('soft')\nprint(cv)\nprint(cv.mean())\nprint()\n\ncv = cross_val_score(voting_clf_hard,X_train,y,cv=5)\nprint('hard')\nprint(cv)\nprint(cv.mean())","7f06e840":"voting_clf_soft.fit(X_train,y)\npredictions = voting_clf_soft.predict(X_test)","ecc59f2f":"## make the file to submit\n\ndf_submition = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\n\ndf_submition['Survived'] = predictions\n\ndf_submition.drop(df_submition.columns.difference(['PassengerId', 'Survived']), axis=1, inplace=True) # Selecting only needed columns\n\ndf_submition.head(5) ","19fb3258":"## file is expected to have 418 rows\n\ndf_submition.count()","5ff77a67":"df_submition.to_csv('my_submission.csv', index=False)\nprint('File created')","e3034dd7":"## Understanding the data\n\nUnderstanding the data you're working with is a very important thing to do before any model fitting. For this we'll use histograms -to look for skewness or kurtosis- and a scatter-matrix -to see the correlacion- in the quantitive variables. For categorical variables bar-charts and pivot-tables will be useful. Since we're trying to predict whether a passenger survived or not it'll be good to compare this variable with each of the remaining.","373407be":"In the train set there are null values in Cabin and Embarked. Cabin values will be ignore given that we won't include them in the model. For Embarked null values we'll just drop them -there are only two-.\n\nIn the test set there's one null value in Fare, let's fill it with the median fare.","ec527872":"## Model Fitting and Evaluation\n\nTo get the best results is important to compare various model. Here we're going to use logistic regression, naive bayes, support vector classifier, k nearest neighbors, random forest and gradient boosted classifier. I used 5 fold cross validation to get a baseline for scores. \n\n__Note:__ scores from here aren't the actual scores we'll get in the test.","1d56aa14":"We can see some non-normal distrubution; that's something to keep in mind maybe we should try to normalize them later.","60a6692a":"Logistic Regression ","56bddacc":"Now use some cross-tables to see the relation between these variables and our target, Survived. We'll look at proportions; we're also going to ignore Cabin and Ticket given that they have to many values and the output can be messy.","c2667800":"# Submission","904ed1e2":"Support Vector Classifier","3c9c60c9":"There's a big concentration of registers, for both survived and not survived, between the ages 10 an 40 approximately.","e1301efa":"First import the libraries for the exploration and load the data. ","4ebf765d":"X train and X test ended up with different number of columns due to the person's title and predictions can't be made like this so we drop the diference.","53115cc3":"## Overview\n\n1) Understanding the data\n\n2) Feature Engineering\n\n3) Data Preprocessing for Model\n\n4) Model Fitting and Evaluation\n\n5) Submission","34200ffd":"The smallest error seems to be around 7 so I used that as the number of neighbors","c9c46fe5":"from here we can highlight that 74% of women survived against only 18% of men and that higher classes have bigger proportion of survivors. ","1af07e8e":"A correlation matrix to get an idea of the correlation within the numeric variables.","cbbdffd5":"The biggest difference for survivors and not-survivors is in the Fare variable. This could be an important feature for the model.","8d4158bc":"## Feature Engineering\n\nIn this part we'll do the next:\n\n* Convert Age into a categorical variable creating groups of ten years and assing the passenger into these groups and see survival rates within each group\n\n* Obtain person's title from the Name variable and see his relation to survival rates","d9e4a538":"Naive Bayes","d7f449c8":"Let's also calculate the mean and the median in our numeric variables for both survivors and not-survivors.","316aed6b":"Let's continue the exploration, this time with the categorical variables. Start with some bar-plots to see the frequency of the different values in the variables.","c94c131e":"A nice way to see the shape of the distribution and the correlation within variables in just one plot is through a scatter-matrix. A scatter-matrix combines histograms and scatterplots.","1a1efad4":"k Nearest Neighbors\n\nIt's important to keep in mind that the number of neighbors we choose can change the score we get, that's  why I decided to test a range of values and plot their errors to get an idea of which number was a good choice.  ","90f688a5":"Let's use our voting classifier for the submission. Both soft and hard voting have pretty similar scores, but soft did slightly better so i decided to used that one.","30a6c0d2":"For a more robust model we're going to use a Voting Classifier. A Voting Classifier trains on an ensemble of numerous models. It simply aggregates the findings of each classifier passed into Voting Classifier and predicts the output class based on the highest majority of voting. Voting Classifier supports two types of votings:\n* Hard Voting: For a \"hard\" voting classifier each classifier gets 1 vote \"yes\" or \"no\" and the result is just a popular vote.\n\n* Soft Voting: In soft voting, the output class is the prediction based on the average of probability given to that class.","d364f259":"Now let's see how the Survived variable relates with our numeric data. In this case the target value is a categorical variable. An easy way to see the relationship between this one and the other quantitive variables in the dataset is using violinplots.","1dfe8017":"## Data Preprocessing for Model\n\nin this part we'll do the next:\n\n* Create categorical variables that we did above for both training and test sets\n\n* Select features for the model\n\n* Deal with null values\n\n* Normalize fare using logarithm\n\n* Get dummies\n\n* Scale data with standard scaler","763babac":"Gradient Boosted Classifier","da3a7a3a":"Random Forest","68dcb5ff":"# Titanic Project Example\n\nThe goal for this project is to predict whether a passenger survived or not based on various variables like age and fare. I also intend to use this notebook to as a way to put my knowledge into practice in order to reinforce it given that I'm relatively new to the field of data science and machine learning. My main intention is not to make a tutorial for the competition but I hope you find this notebook useful."}}