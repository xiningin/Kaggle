{"cell_type":{"936e2eda":"code","566c6b90":"code","3a83d685":"code","00e22d82":"code","1e2f405f":"code","91b7bb4f":"code","ab8d1f2f":"code","28941e5b":"code","7ae5dec8":"code","19c92877":"code","61151692":"code","d0cad960":"markdown","70ca7445":"markdown","8097815b":"markdown","44cbcda0":"markdown","853c37ed":"markdown","75fb3762":"markdown","2796a70d":"markdown","142236bd":"markdown","d3728a9c":"markdown","ec7b6a88":"markdown"},"source":{"936e2eda":"import torch\nprint(torch.__version__)","566c6b90":"from plotly.subplots import make_subplots\nimport plotly.graph_objects as go\nimport numpy as np\nnp.random.seed(42)\n\nnum_data = 2400 # train 1600, validation 400, test 400\nx1 = np.random.rand(num_data) * 10\nx2 = np.random.rand(num_data) * 10\ne = np.random.normal(0, 0.5, num_data)\nX = np.c_[x1, x2]\ny = 2*np.sin(x1) + np.log(0.5*x2**2) + e\n\ntrain_x, train_y = X[:1600], y[:1600]\nvalid_x, valid_y = X[1600:2000], y[1600:2000]\ntest_x, test_y = X[2000:], y[2000:]\n\n# subplots\nfig = make_subplots(\n    rows=1, cols=3,\n    specs=[[{\"type\": \"scene\"}, {\"type\": \"scene\"}, {\"type\": \"scene\"}]],\n    subplot_titles=(\"train set\",\"validation set\", \"test set\"),\n    column_widths=[0.5, 0.25, 0.25] # ratio\n)\n\n# configurations for 3dplot\nmarker = dict(\n    size=3,\n    color=train_y, # set color to an array\/list of desired values\n    colorscale='Viridis',   # choose a colorscale\n    opacity=0.8\n)\n\n# 3dplot\nplot_train = go.Scatter3d(x=train_x[:, 0], y=train_x[:, 1], z=train_y, mode='markers', marker=marker)\nplot_valid = go.Scatter3d(x=valid_x[:, 0], y=valid_x[:, 1], z=valid_y, mode='markers', marker=marker)\nplot_test = go.Scatter3d(x=test_x[:, 0], y=test_x[:, 1], z=test_y, mode='markers', marker=marker)\n\nfig.add_trace(plot_train, row=1, col=1)\nfig.add_trace(plot_valid, row=1, col=2)\nfig.add_trace(plot_test, row=1, col=3)\n\nfig.update_layout(height=400, title_text=\"Distributions\", showlegend=False)\nfig.show()","3a83d685":"import torch\nimport torch.nn as nn\n\nclass LinearModel(nn.Module):\n    def __init_(self):\n        super(LinearModel, self).__init__()\n        self.linear == nn.Linear(in_feature=2, out_features=1, bias=True)\n\n    def forward(self, x):\n        return self.linear(x)\n\nclass MLPModel(nn.Module):\n    def __init__(self): \n        super(MLPModel, self).__init__()\n        self.linear1 = nn.Linear(in_features=2, out_features=200)\n        self.linear2 = nn.Linear(in_features=200, out_features=1)\n        self.relu = nn.ReLU()\n    \n    def forward(self, x):\n        x = self.linear1(x)\n        x = self.relu(x)\n        x = self.linear2(x)\n        return x        ","00e22d82":"reg_loss = nn.MSELoss()\n\n# test\ntest_pred_y = torch.Tensor([0,0,0,0])\ntest_true_y = torch.Tensor([0,1,0,1])\n\nprint(reg_loss(test_pred_y, test_true_y))\nprint(reg_loss(test_true_y, test_true_y))","1e2f405f":"import torch.optim as optim\nfrom sklearn.metrics import mean_absolute_error\n\n# model\nmodel = MLPModel()\nnum_param = sum(p.numel() for p in model.parameters())\nprint(f\"the number of parameter is {num_param}\")\n\n# optimizer\nlr = 0.005\noptimizer = optim.SGD(model.parameters(), lr=lr)\n\nlst_epoch = []\nlst_train_loss = []\nlst_val_loss = []\nlst_mae = []\nlst_mae_epoch = []\n\n# training\nepoch = 4000\ntest_cycle = 1000\nfig = make_subplots(\n    rows=int(epoch\/test_cycle), cols=3,\n    specs=[[{\"type\": \"scene\"}, {\"type\": \"scene\"}, {\"type\": \"scene\"}]]*int(epoch\/test_cycle)\n)\n\nrow_num = 0\n\nfor i in range(epoch):\n\n    ## training\n    model.train()         # train \ubaa8\ub4dc\ub85c \uc138\ud305\n    optimizer.zero_grad() # \uadf8\ub77c\ub514\uc5b8\ud2b8\ub97c 0\uc73c\ub85c \ucd08\uae30\ud654\n\n    input_x = torch.Tensor(train_x)\n    true_y = torch.Tensor(train_y)\n    train_pred_y = model(input_x)\n\n    loss = reg_loss(train_pred_y.squeeze(), true_y) # 1\ucc28\uc6d0 \ubc31\ud130\ub85c \ubcc0\uacbd \ud544\uc694\n    loss.backward() # \uadf8\ub77c\ub514\uc5b8\ud2b8 \uacc4\uc0b0\n    optimizer.step()\n    lst_epoch.append(i)\n    lst_train_loss.append(loss.detach().numpy())\n\n    ## validation\n    model.eval()\n    optimizer.zero_grad()\n    input_x = torch.Tensor(valid_x)\n    true_y = torch.Tensor(valid_y)\n    pred_y = model(input_x)\n    loss = reg_loss(pred_y.squeeze(), true_y)\n    lst_val_loss.append(loss.detach().numpy())\n\n    ## test\n    if i % test_cycle == 0:\n        row_num += 1\n        ### MAE\n        model.eval()\n        optimizer.zero_grad()\n        input_x = torch.Tensor(test_x)\n        true_y = torch.Tensor(test_y)\n        pred_y = model(input_x).detach().numpy().reshape(400) \n        mae = mean_absolute_error(true_y, pred_y) # sklearn \ucabd \ud568\uc218\ub4e4\uc740 true_y \uac00 \uba3c\uc800, pred_y\uac00 \ub098\uc911\uc5d0 \uc778\uc790\ub85c \ub4e4\uc5b4\uac00\ub294 \uac83\uc5d0 \uc8fc\uc758\ud569\uc2dc\ub2e4\n        lst_mae.append(mae)\n        lst_mae_epoch.append(i)\n\n        plot_test = go.Scatter3d(x=test_x[:, 0], y=test_x[:, 1], z=true_y, mode='markers', marker=marker)\n        plot_valid = go.Scatter3d(x=test_x[:, 0], y=test_x[:, 1], z=pred_y, mode='markers', marker=marker)\n        plot_train = go.Scatter3d(x=train_x[:, 0], y=train_x[:, 1], z=train_pred_y.detach().numpy().reshape(1600), mode='markers', marker=marker) # 1\ucc28\uc6d0 \ubc31\ud130\ub85c \ubcc0\uacbd \ud544\uc694\n        fig.add_trace(plot_test, row=row_num, col=1)\n        fig.add_trace(plot_valid, row=row_num, col=2)\n        fig.add_trace(plot_train, row=row_num, col=3)\n\nfig.update_layout(height=1200, showlegend=False)\nfig.show()","91b7bb4f":"import matplotlib.pyplot as plt\nfig, ax = plt.subplots(1, 2, figsize=(15, 5))\n\nax[0].plot(lst_epoch, lst_train_loss, label=\"train_loss\")\nax[0].plot(lst_epoch, lst_val_loss, \"--\", label=\"val_loss\")\nax[0].set_xlabel('epoch')\nax[0].set_ylabel('loss')\nax[0].grid()\nax[0].legend()\nax[0].set_ylim(0, 5)\n\nax[1].plot(lst_mae_epoch, lst_mae, label=\"mae\", marker='x')\nax[1].set_xlabel('epoch')\nax[1].set_ylabel('mae')\nax[1].grid()\nax[1].legend()\nax[1].set_ylim(0, 5)","ab8d1f2f":"%matplotlib inline\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Generating Dataset\nr = np.random.rand(10000)*3\ntheta = np.random.rand(10000)*2*np.pi\ny = r.astype(int)\nr = r * (np.cos(theta) + 1)\nx1 = r * np.cos(theta)\nx2 = r * np.sin(theta)\nX = np.array([x1, x2]).T\n\n# Split Dataset into Train, Validation, Test\ntrain_X, train_y = X[:8000, :], y[:8000]\nval_X, val_y = X[8000:9000, :], y[8000:9000]\ntest_X, test_y = X[9000:, :], y[9000:]\n\n# Visualize Each Dataset\nfig = plt.figure(figsize=(15,5))\nax1 = fig.add_subplot(1, 3, 1)\nax1.scatter(train_X[:, 0], train_X[:, 1], c=train_y, s=0.7)\nax1.set_xlabel('x1')\nax1.set_ylabel('x2')\nax1.set_title('Train Set Distribution')\n\nax2 = fig.add_subplot(1, 3, 2)\nax2.scatter(val_X[:, 0], val_X[:, 1], c=val_y)\nax2.set_xlabel('x1')\nax2.set_ylabel('x2')\nax2.set_title('Validation Set Distribution')\n\nax3 = fig.add_subplot(1, 3, 3)\nax3.scatter(test_X[:, 0], test_X[:, 1], c=test_y)\nax3.set_xlabel('x1')\nax3.set_ylabel('x2')\nax3.set_title('Test Set Distribution')\n\nplt.show()","28941e5b":"import torch\nimport torch.nn as nn\n\n\nclass LinearModel(nn.Module):\n    def __init__(self): \n        super(LinearModel, self).__init__()\n        self.linear = nn.Linear(in_features=2, out_features=3, bias=True)\n    \n    def forward(self, x):\n        x = self.linear(x)\n        return x\n\n    \nclass MLPModel(nn.Module):\n    def __init__(self): \n        super(MLPModel, self).__init__()\n        self.linear1 = nn.Linear(in_features=2, out_features=200)\n        self.linear2 = nn.Linear(in_features=200, out_features=3)\n        self.relu = nn.ReLU()\n    \n    def forward(self, x):\n        x = self.linear1(x)\n        x = self.relu(x)\n        x = self.linear2(x)\n        return x","7ae5dec8":"cls_loss = nn.CrossEntropyLoss()\n\n# Cross Entropy Test\ntest_pred_y = torch.Tensor([[2,0.1],[0,1]]) # \ud569\uc774 1\uc774 \uc544\ub2c8\uc5b4\ub3c4 \ub3d9\uc791\uc740 \ud568\ntest_true_y1 = torch.Tensor([1,0]).long()   # \ub808\uc774\ube14\uc740 int\ud615\ntest_true_y2 = torch.Tensor([0,1]).long()\n\nprint(cls_loss(test_pred_y, test_true_y1))\nprint(cls_loss(test_pred_y, test_true_y2))","19c92877":"import torch.optim as optim\nfrom sklearn.metrics import accuracy_score\n\n# Model\nmodel = MLPModel() \nprint('{} parameters'.format(sum(p.numel() for p in model.parameters() if p.requires_grad))) # \ubcf5\uc7a1\ud574\ubcf4\uc774\uc9c0\ub9cc \uac04\ub2e8\ud788 \ubaa8\ub378 \ub0b4\uc5d0 \ud559\uc2b5\uc744 \ub2f9\ud560 \ud30c\ub77c\ubbf8\ud130 \uc218\ub97c \uce74\uc6b4\ud305\ud558\ub294 \ucf54\ub4dc\uc785\ub2c8\ub2e4.\n\n# Optimizer\nlr = 0.005 \noptimizer = optim.SGD(model.parameters(), lr=lr)\n\n# Lists for saving the training results\nlist_epoch = [] \nlist_train_loss = []\nlist_val_loss = []\nlist_acc = []\nlist_acc_epoch = []\n\n\nepoch = 4000 \nfor i in range(epoch):\n    \n    ## Train\n    model.train()\n    optimizer.zero_grad()\n    \n    input_x = torch.Tensor(train_X)\n    true_y = torch.Tensor(train_y).long()\n    pred_y = model(input_x)\n    #print(input_x.shape, true_y.shape, pred_y.shape) # check input and output dimension\n    \n    loss = cls_loss(pred_y.squeeze(), true_y)\n    loss.backward()\n    optimizer.step()\n    list_epoch.append(i)\n    list_train_loss.append(loss.detach().numpy())\n    \n    ## Validation\n    model.eval()\n    optimizer.zero_grad()\n    input_x = torch.Tensor(val_X)\n    true_y = torch.Tensor(val_y).long()\n    pred_y = model(input_x)   \n    loss = cls_loss(pred_y.squeeze(), true_y)\n    list_val_loss.append(loss.detach().numpy())\n    \n    ## Evaluation\n    if i % 500 == 0:\n        \n        model.eval()\n        optimizer.zero_grad()\n        input_x = torch.Tensor(test_X)\n        true_y = torch.Tensor(test_y)\n        pred_y = model(input_x).detach().max(dim=1)[1].numpy() \n        acc = accuracy_score(true_y, pred_y) # sklearn \ud568\uc218\ub4e4\uc740 true_y \uac00 \uba3c\uc800, pred_y\uac00 \ub098\uc911\n        list_acc.append(acc)\n        list_acc_epoch.append(i)\n\n        fig = plt.figure(figsize=(15,5))\n        \n        ### True Y Scattering\n        ax1 = fig.add_subplot(1, 3, 1)\n        ax1.scatter(test_X[:, 0], test_X[:, 1], c=test_y)\n        ax1.set_xlabel('x1')\n        ax1.set_ylabel('x2')\n        ax1.set_title('True test y')\n        \n        ### Predicted Y Scattering\n        ax2 = fig.add_subplot(1, 3, 2)\n        ax2.scatter(test_X[:, 0], test_X[:, 1], c=pred_y)\n        ax2.set_xlabel('x1')\n        ax2.set_ylabel('x2')\n        ax2.set_title('Predicted test y')\n        \n        ### Just for Visualizaing with High Resolution\n        input_x = torch.Tensor(train_X)\n        pred_y = model(input_x).detach().max(dim=1)[1].numpy() \n        \n        ax3 = fig.add_subplot(1, 3, 3)\n        ax3.scatter(train_X[:, 0], train_X[:, 1], c=pred_y)\n        ax3.set_xlabel('x1')\n        ax3.set_ylabel('x2')\n        ax3.set_title('Prediction on train set')\n\n        plt.show()\n        print('Epoch: ', i,  'Accuracy: ', acc*100, '%')","61151692":"## Report Experiment\n\nfig = plt.figure(figsize=(15,5))\n\n# ====== Loss Fluctuation ====== #\nax1 = fig.add_subplot(1, 2, 1)\nax1.plot(list_epoch, list_train_loss, label='train_loss')\nax1.plot(list_epoch, list_val_loss, '--', label='val_loss')\nax1.set_xlabel('epoch')\nax1.set_ylabel('loss')\nax1.grid()\nax1.legend()\nax1.set_title('epoch vs loss')\n\n# ====== Metric Fluctuation ====== #\nax2 = fig.add_subplot(1, 2, 2)\nax2.plot(list_acc_epoch, list_acc, marker='x', label='Accuracy metric')\nax2.set_xlabel('epoch')\nax2.set_ylabel('Acc')\nax2.grid()\nax2.legend()\nax2.set_title('epoch vs Accuracy')\n\nplt.show()","d0cad960":"## Hypothesis Define\n\nLinear Model\n$$H = \\ XW + b \\ \\ ( W \\in \\mathcal{R}^{2 \\times 1}, b \\in \\mathcal{R}^{1}, H \\in \\mathcal{R}^{N \\times 1})$$\n\nMLP Model$$Let \\ relu(X) = \\ max(X, 0)$$\n\n$$h = \\ relu(X W_1 + b_1) \\ \\  ( W_1 \\in \\mathcal{R}^{2 \\times 200}, b_1 \\in \\mathcal{R}^{200}, h \\in \\mathcal{R}^{N \\times 200})$$\n$$H = \\ h W_2 + b_2  \\ \\  ( W_2 \\in \\mathcal{R}^{200 \\times 1}, b_2 \\in \\mathcal{R}^{1}, H \\in \\mathcal{R}^{N  \\times 1})$$","70ca7445":"# Lab3. Pytorch Classification\n \n## Data generation\n\nData Set\n$$X_{train} \\in \\mathcal{R}^{8000 \\times 2}, Y_{train} \\in \\mathcal{Z}^{8000}$$\n$$X_{val} \\in \\mathcal{R}^{1000 \\times 2}, Y_{val} \\in \\mathcal{Z}^{1000}$$\n$$X_{test} \\in \\mathcal{R}^{1000 \\times 2}, Y_{test} \\in \\mathcal{Z}^{1000}$$","8097815b":"## Cost Function\n\nloss functions: https:\/\/pytorch.org\/docs\/stable\/_modules\/torch\/nn\/modules\/loss.html","44cbcda0":"## Train & Evaluation\n\nmetric: $$MAE(Y_{true}, Y_{predict}) = \\sum_{i} | \\ y_{true}^{(i)} - y_{predict}^{(i)} \\ | $$","853c37ed":"Multi-Label Logistic Model\n$$z = \\ XW + b \\ \\ ( W \\in \\mathcal{R}^{2 \\times 3}, b \\in \\mathcal{R}^{3}, z \\in \\mathcal{R}^{N \\times 3} ) $$\n$$H = \\ softmax(z) \\ \\ (  H \\in \\mathcal{R}^{N \\times 3})$$\n\nMLP Model\n$$Let \\ relu(X) = \\ max(X, 0)$$\n$$h = \\ relu(X W_1 + b_1) \\ \\  ( W_1 \\in \\mathcal{R}^{2 \\times 200}, b_1 \\in \\mathcal{R}^{200}, h \\in \\mathcal{R}^{N \\times 200}$$\n$$z = \\ h W_2 + b_2  \\ \\  ( W_2 \\in \\mathcal{R}^{200 \\times 3}, b_2 \\in \\mathcal{R}^{3}, z \\in \\mathcal{R}^{N  \\times 3})$$\n$$H = \\ softmax(z) \\ \\ ( H \\in \\mathcal{R}^{N \\times 3})$$","75fb3762":"# Lab2. Pytorch Regression","2796a70d":"## Data Generation\n\n$$ y = \\ 2 sin(x_1) + log({1 \\over 2}x_2^2) + e $$\n$$ e \\sim \\mathcal{N} (0, 0.5) $$\n\nData Set\n$$X_{train} \\in \\mathcal{R}^{1600 \\times 2}, Y_{train} \\in \\mathcal{R}^{1600}$$\n$$X_{val} \\in \\mathcal{R}^{400 \\times 2}, Y_{val} \\in \\mathcal{R}^{400}$$\n$$X_{test} \\in \\mathcal{R}^{400 \\times 2}, Y_{test} \\in \\mathcal{R}^{400}$$","142236bd":"## Train & Evaluation","d3728a9c":"## Cost Function Define\nCrossEntropy Documentation : https:\/\/pytorch.org\/docs\/stable\/nn.html#crossentropyloss","ec7b6a88":"## Report Experiment\n\n* Check train loss and validation loss"}}