{"cell_type":{"3677fb23":"code","85934701":"code","70be75d5":"code","1aca0c69":"code","8c0781ea":"code","42af7166":"code","2d15a69c":"code","f335eb22":"code","a347515b":"code","8ac8a7cf":"code","a7c75291":"code","5468a5fd":"code","e7575bd6":"code","0be92901":"code","c8852422":"code","6eb5e78d":"code","f625dbc1":"markdown","e4fa4583":"markdown","e75d4962":"markdown","5a12c50e":"markdown","55a652c1":"markdown","88a58c01":"markdown","0d1af943":"markdown","5365150f":"markdown","ff9c26ad":"markdown","a576881a":"markdown"},"source":{"3677fb23":"#Installing the required packages \n! pip install googletrans \n! pip install transformers\n! pip install preprocessor\n! pip install tweepy","85934701":"import pandas as pd # For Data manipulation\nimport tweepy # For extracting tweets from twitter\nfrom tweepy import OAuthHandler # For extracting tweets from twitter\nfrom tweepy import API # For extracting tweets from twitter\nfrom googletrans import Translator # For identifying teh language of the hashtag\nimport datetime # For datetime manipulation\nimport copy # For data processing\nimport string # For data processing\nimport re # For data processing\nimport preprocessor as p # For data processing\nimport matplotlib as mpl # For data visualization\nimport matplotlib.pyplot as plt # For data visualization\nfrom subprocess import check_output \nfrom wordcloud import WordCloud, STOPWORDS # For data visualization\nimport torch # For Modelling\nimport json # For data processing\nfrom transformers import T5Tokenizer, T5ForConditionalGeneration, T5Config, AutoModelWithLMHead, AutoTokenizer # For Modelling\nimport os # For dir handling\nimport textwrap # For text manipulation","70be75d5":"model = T5ForConditionalGeneration.from_pretrained('t5-small') # Downloading 't5-small' model \ntokenizer = T5Tokenizer.from_pretrained('t5-small') # Downloading 't5-small' model tokenizer\ndevice = torch.device('cuda') # Setting for GPU support","1aca0c69":"#Paste the keys extracted from the Twitter App\nconsumer_key = 'Paste credentials from Twitter App Developer profile'\nconsumer_secret = 'Paste credentials from Twitter App Developer profile'\naccess_token = 'Paste credentials from Twitter App Developer profile'\naccess_token_secret = 'Paste credentials from Twitter App Developer profile'","8c0781ea":"# Consumer key authentication(consumer_key,consumer_secret can be collected from our twitter developer profile)\nauth = OAuthHandler(consumer_key, consumer_secret)\n# Access key authentication(access_token,access_token_secret can be collected from our twitter developer profile)\nauth.set_access_token(access_token, access_token_secret)\n# Set up the API with the authentication handler\napi = API(auth)","42af7166":"def get_location_trends(locations, auth,n_hashtags,lang = 'en'):\n  '''\n  This function extracts the top trends for the selected location\n  \n  Function parameters \n  \n  locations :   ID for the lcoation, which can be extarcted from https:\/\/nations24.com\/world-wide\n  auth      :   Authentication for Twitter API\n  n_hashtags:   Integer Value, that defines number of top trending hashtags we are looking at\n  lang      :   Language we are looking at for the tweets, by default its english\n  '''\n  api = API(auth)\n  trends = api.trends_place(locations)\n  data = trends[0]\n  trends_data = data['trends']\n  global tred_data\n  tred_data = []\n  for info in trends_data:\n    tred_data.append([info['name'],info['tweet_volume'],lan_find.detect(info['name']).lang ] )\n  tred_data = pd.DataFrame(tred_data, columns = list(['Hashtag',\n                                                    'Tweet_Volume', 'Language'])).sort_values(by = ['Tweet_Volume'],ascending = False)\n  #select english language trends\n  tred_data = tred_data[tred_data.Language == lang]\n  #select top 3 trends\n  tred_data = tred_data.nlargest(n_hashtags,columns=['Tweet_Volume'])\n  return tred_data","2d15a69c":"WOE_ID = 2459115 # Where on Earth id can be extracted from https:\/\/nations24.com\/world-wide\nlan_find = Translator() # for finding the language of the hashtags","f335eb22":"df_trending = get_location_trends(WOE_ID,auth,3,'en')\ndf_trending # Shows Top three hashtags & its corresponding Volume of Tweets","a347515b":"hashtag_list = list(df_trending['Hashtag']) #Gets list of Top n hashtags\n#hashtag_list = list(['#philosophy']) #Uncomment this & input any hashtag if you would like to look at any particular hashtag","8ac8a7cf":"def extract_tweets_for_htags(no_of_tweets,hashtag_list):\n  '''\n  This function extracts teh tweets for the Hashtags\n  Parameters:\n  no_of_tweets  : Integer, number of tweets to extract keep it between 100 to 500 for faster execution\n  hashtag_list  : List of top trending Hashtags\n  '''\n  dict_of_df = {} \n  for htag in hashtag_list:\n    today = datetime.datetime.now().strftime(\"%Y-%m-%d\")\n    tweets = tweepy.Cursor(api.search,q=htag,lang=\"en\",since=today,wait_on_rate_limit = True,wait_on_rate_limit_notify = True ).items(no_of_tweets) #Extracting tweets for the htag\n    tweets= [tweet.text for tweet in tweets] # Saving the tweets as list\n    \n    \n    key_name = 'df_htag_'+str(htag)\n    dict_of_df[key_name] = copy.deepcopy(tweets)\n  return dict_of_df","a7c75291":"dict_of_df = extract_tweets_for_htags(500,hashtag_list) #Dictionary that has data for all top three hashtags\nhashtag_df_list = dict_of_df.keys() #List contains individual the key names for accessing the data inside the dictionary","5468a5fd":"def remove_punct(text):\n    '''\n    Remove punctuation of the string, and returns the string\n\n    Parameters:\n    text = String Value\n  \n    ''' \n  \n    clean_text = []\n    for tweet in text:\n      tweet = re.sub(r\"http\\S+\", \"\", tweet)\n      text  = \"\".join([char for char in tweet if char not in string.punctuation])\n      text = ''.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\\/\\\/\\S+)\",\"\",text))\n      text = text.replace('RT','').strip()\n      clean_text.append(text)\n    return clean_text\n\n\ndef wrap(line):\n    broken = textwrap.wrap(line, 50, break_long_words=False)\n    return '\\n'.join(broken)","e7575bd6":"def clean_tweet():\n  '''\n  Function that returns cleaned string\n  '''\n  processed_data_dict = {}\n  for htag in hashtag_df_list:\n    df = dict_of_df[htag]\n    df = remove_punct(df)\n    key_name = str(htag)\n    processed_data_dict[key_name] = copy.deepcopy(df)\n  \n  return processed_data_dict","0be92901":"clean_data = clean_tweet()\ndf_clean_data = pd.DataFrame.from_dict(clean_data) # Dataframe taht contains string data for each hasgtag represented in each column\nhtags = df_clean_data.columns # Dataframe's Column list to iterate through data for each hashtag","c8852422":"def show_me_summary(htags,df):\n  for htag in htags:\n    temp =  df[htag].str.cat(sep=', ') # Select a column insert i here\n    \n    #Word Cloud\n    htag = htag.split('_')[2]\n    print(f'Word Cloud for the trending hashtag: {htag} \\n') \n    stopwords = set(STOPWORDS)\n    wordcloud = WordCloud(background_color='white',stopwords=stopwords,max_words=200,max_font_size=40, random_state=42).generate(str(temp))\n    fig = plt.figure(1)\n    plt.imshow(wordcloud)\n    plt.axis('off')\n    plt.show() # Shows teh Word Cloud for the Hashtag\n    \n    #Text Summary Model\n    preprocess_text = temp.strip().replace(\"\\n\",\"\") # Preprocessing the data\n    #t5_prepared_Text = \"summarize: \"+preprocess_text\n    t5_prepared_Text =preprocess_text\n    # print (\"Original text preprocessed: \\n\", preprocess_text) # Uncomment this to see the Original Text\n    \n    #MODEL 1\n    tokenized_text = tokenizer.encode(t5_prepared_Text, return_tensors=\"pt\",  \n                                      truncation=True,\n                                      #padding=True, \n                                      add_special_tokens= True)\n    # Converting teh Tokens into ids for the Model\n    summary_ids = model.generate(tokenized_text,\n                                 do_sample=False,\n                                 min_length=50 ,\n                                 max_length=160, \n                                 top_k=90, \n                                 top_p=0.90,\n                                 num_return_sequences=1,\n                                 num_beams=300,\n                                 no_repeat_ngram_size=2,\n                                 temperature=0.8,\n                                 \n                                 #early_stopping=True\n       )\n    \n    \n    output = tokenizer.decode(summary_ids[0], skip_special_tokens=True) # Decoding the Encoded ids into Words\n    output = ('\\n'.join(textwrap.wrap(output, 90, break_long_words=False)))\n    print(f\"\\n\\nSummarized text for {htag}: \\n\\n\",output)\n    print('*' *100)\n    print('\\n')","6eb5e78d":"show_me_summary(htags=htags, df =df_clean_data )","f625dbc1":"### Top Trends","e4fa4583":"#### Basic Setting for the Model","e75d4962":"#### Data Processing functions","5a12c50e":"### Extract Tweets for the top three trending hashtags","55a652c1":"## Importing Packages","88a58c01":"### Final Summary of the Tweets for Top three Hashtags along with simple Worl Cloud viz","0d1af943":"**Reference: **\n\n\n***Tokenizer***\n\nTokenization is a way of separating a piece of text into smaller units called tokens. Here, tokens can be either words, characters, or subwords. Hence, tokenization can be broadly classified into 3 types \u2013 word, character, and subword (n-gram characters) tokenization.\n\n\n[Tonenizer](https:\/\/www.analyticsvidhya.com\/blog\/2020\/05\/what-is-tokenization-nlp\/#:~:text=Tokenization%20is%20a%20common%20task%20in%20Natural%20Language%20Processing%20(NLP).&text=Tokenization%20is%20a%20way%20of,words%2C%20characters%2C%20or%20subwords.)\n\n\n***Transformer Model***\n\n[Transformer Explained](http:\/\/jalammar.github.io\/illustrated-transformer\/)\n\n[logo]: http:\/\/jalammar.github.io\/images\/t\/The_transformer_encoders_decoders.png\n\n\n![alt text][logo]","5365150f":"### Extract top trends in Twitter for user defined location, and get its corresponding hashtags","ff9c26ad":"# Twitter-Summary-mini-Project\n\n[Github Link](https:\/\/github.com\/balajisriraj\/Twitter-Location-based-Trend-summarizer)\n\n### **This project is to identify top trends in twitter and generates the summary of those trending hashtags in about 50 to 160 words using Text-to-Text transformer model.**\n\n\n### Requirments: \n- \n  To install packages use \n  ```\n      !pip install <package name>\n  ```\n- Pre-trainined transformer model to be downloaded\n- Twitter credentials like API Key, Access Tokens,etc\n  Steps for getting Twitter credentials:\n    - Create a twitter account incase if you do not have already\n    - Go to [https:\/\/apps.twitter.com\/](https:\/\/apps.twitter.com\/) \n    - Click 'Create an app', this will help to create a developer account\n    - Once you fill all the mandatory details you will be provided with required keys, please save those keys locally in a notepad for later use \n    \n\n## Flow of the Project run:\n\n### Step 1:\nIdentify the top trends in twitter for the current day with respect to the user defined location.\n### Step 2:\nSelect the top n number of trends in the user defined language based on the volume of tweets available at the point of extraction.\n### Step 3:\nExtract the tweet data for those selected trends\n### Step 4:\nDo bit of data cleaning & processing to cater well for the model\n### Step 5:\nPut all the individual tweets together as a string chunk\n### Step 6:\nBy leveraging pre- trained model\u2019s ability, generate text summary of the tweets in about 50 to 160 words along with Word cloud graph for better understanding of the context.\n\nFor more information on the NLP model:\nRefer : [Model Blog] (https:\/\/ai.googleblog.com\/2020\/02\/exploring-transfer-learning-with-t5.html)\n\n![alt text][logo]\n\n[logo]: https:\/\/1.bp.blogspot.com\/-o4oiOExxq1s\/Xk26XPC3haI\/AAAAAAAAFU8\/NBlvOWB84L0PTYy9TzZBaLf6fwPGJTR0QCLcBGAsYHQ\/s1600\/image3.gif \"T5 - TEXT-to-TEXT\"\n#### Future Scope:\n<font> <font colour = 'green'> \n              Able to work with multi language tweets using google translate API, comprensive web app for the ease of intraction, \n              model tuning better for results. \n<\/font>\n","a576881a":"### Credentials"}}