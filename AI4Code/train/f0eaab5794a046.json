{"cell_type":{"bfae2b9c":"code","eb321981":"code","3d6cc4d4":"code","a077e1c7":"code","4a2d1c86":"code","423dbfc9":"code","46c1c96c":"code","bd97c616":"code","9f7620db":"code","ecc5d6a7":"code","5e97f5ea":"code","b339ac0a":"code","c72bd4db":"code","d17e5247":"code","57d0611d":"code","60320bc3":"code","c2a75d4f":"code","e9567f0a":"code","ef3bb9b3":"code","f096df0a":"code","5a83ecfe":"code","98abe656":"code","d66c1a74":"code","51421d1a":"code","f7ea3c50":"code","83bda561":"code","d966de90":"code","fc97cf3c":"code","b2c8c5ab":"code","4589adc2":"code","b5fd1cc4":"code","531394c0":"code","fef06895":"code","02d12c07":"code","7ecef6d6":"code","6fcb762c":"code","f2e19e9b":"code","8402f586":"code","6207547e":"code","18ae5901":"code","37dd1b16":"code","7ab0a831":"code","019c10d5":"code","c70b77cb":"code","f3969c45":"code","5e6c7a00":"code","24943269":"code","1369e609":"code","4d6376a7":"code","726954ca":"code","02af7a8e":"code","37454f82":"code","71278733":"code","f9e269f4":"code","4dad50cb":"code","6f7b9049":"markdown","e8706440":"markdown","1c8a33cd":"markdown","c37c2238":"markdown","7a473a9c":"markdown","57efe4e2":"markdown","007a830b":"markdown","4dad0570":"markdown","45290d7f":"markdown","51d6374e":"markdown","d4ea2e8a":"markdown","e0b24ed7":"markdown","2e8f3bfc":"markdown","ae4de586":"markdown","3a10a520":"markdown","b2753ba7":"markdown","041b3b08":"markdown","4114de31":"markdown","f286b2eb":"markdown","74dcafa7":"markdown"},"source":{"bfae2b9c":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom collections import Counter\nimport seaborn as sns\n","eb321981":"train_data = pd.read_csv('..\/input\/train.csv')\ntest_data = pd.read_csv('..\/input\/test.csv')","3d6cc4d4":"train_data.shape","a077e1c7":"train_data.info()","4a2d1c86":"train_data.head()","423dbfc9":"train_data.dtypes","46c1c96c":"test_data.shape","bd97c616":"test_data.info()","9f7620db":"test_data.head()","ecc5d6a7":"test_data.dtypes","5e97f5ea":"# check constant columns\nremove_cols = []\nfor col in train_data.columns:\n    if train_data[col].std() == 0: \n        remove_cols.append(col)\nprint remove_cols\n\n","b339ac0a":"remove_cols1 = []\nfor col in test_data.columns:\n    if test_data[col].std() == 0: \n        remove_cols1.append(col)\nprint(remove_cols1)","c72bd4db":"train_data.isnull().any().values","d17e5247":"test_data.isnull().any().values","57d0611d":"'''\nCreate a new column 'Wilderness_Area' \n\n1 for Wilderness_Area1\n2 for Wilderness_Area2\n3 for Wilderness_Area3\n4 for Wilderness_Area4\n\n'''\n\nfor i in train_data.index:\n    if train_data['Wilderness_Area1'][i] == 1:\n        train_data.set_value(i,'Wilderness_Area',1)\n    elif train_data['Wilderness_Area2'][i] == 1:\n        train_data.set_value(i,'Wilderness_Area',2)\n    elif train_data['Wilderness_Area3'][i] == 1:\n        train_data.set_value(i,'Wilderness_Area',3)\n    elif train_data['Wilderness_Area4'][i] == 1:\n        train_data.set_value(i,'Wilderness_Area',4)","60320bc3":"for i in test_data.index:\n    if test_data['Wilderness_Area1'][i] == 1:\n        test_data.set_value(i,'Wilderness_Area',1)\n    elif test_data['Wilderness_Area2'][i] == 1:\n        test_data.set_value(i,'Wilderness_Area',2)\n    elif test_data['Wilderness_Area3'][i] == 1:\n        test_data.set_value(i,'Wilderness_Area',3)\n    elif test_data['Wilderness_Area4'][i] == 1:\n        test_data.set_value(i,'Wilderness_Area',4)","c2a75d4f":"#Wilderness_Area \nplt.figure(figsize=(10,6))\nsns.countplot(x=\"Wilderness_Area\", data=train_data)\nplt.ylabel('Count', fontsize=10)\nplt.xlabel('Wilderness_Area', fontsize=10)\nplt.xticks(rotation='vertical')\nplt.title(\"Frequency of Wilderness_Area\", fontsize=14)\nplt.show()\n","e9567f0a":"#Wilderness_Area for test_data\nplt.figure(figsize=(10,6))\nsns.countplot(x=\"Wilderness_Area\", data=test_data)\nplt.ylabel('Count', fontsize=10)\nplt.xlabel('Wilderness_Area', fontsize=10)\nplt.xticks(rotation='vertical')\nplt.title(\"Frequency of Wilderness_Area\", fontsize=14)\nplt.show()","ef3bb9b3":"all_columns = train_data.columns\nsoil_type = []\nfor each in all_columns:\n    if each.startswith((\"Soil_Type\")):\n        soil_type.append(each)\nprint(soil_type)","f096df0a":"for i in train_data.index:\n    for ind,each in enumerate(soil_type,1):\n        if train_data[each][i] == 1:\n            train_data.set_value(i,'Soil_Type',ind)","5a83ecfe":"len(train_data.Soil_Type)","98abe656":"#Soil_Type \nplt.figure(figsize=(10,6))\nsns.countplot(x=\"Soil_Type\", data=train_data)\nplt.ylabel('Count', fontsize=10)\nplt.xlabel('Soil_Type', fontsize=10)\nplt.xticks(rotation='vertical')\nplt.title(\"Frequency of Soil_Type\", fontsize=14)\nplt.show()","d66c1a74":"for i in test_data.index:\n    for ind,each in enumerate(soil_type,1):\n        if test_data[each][i] == 1:\n            test_data.set_value(i,'Soil_Type',ind)","51421d1a":"Counter(test_data.Soil_Type)","f7ea3c50":"#Soil_Type \nplt.figure(figsize=(10,6))\nsns.countplot(x=\"Soil_Type\", data=test_data)\nplt.ylabel('Count', fontsize=10)\nplt.xlabel('Soil_Type', fontsize=10)\nplt.xticks(rotation='vertical')\nplt.title(\"Frequency of Soil_Type\", fontsize=14)\nplt.show()","83bda561":"train_data.drop(soil_type,axis=1,inplace=True)","d966de90":"train_data.drop(['Wilderness_Area1','Wilderness_Area2','Wilderness_Area3','Wilderness_Area4'],axis=1,inplace=True)","fc97cf3c":"test_data.drop(soil_type,axis=1,inplace=True)","b2c8c5ab":"test_data.drop(['Wilderness_Area1','Wilderness_Area2','Wilderness_Area3','Wilderness_Area4'],axis=1,inplace=True)","4589adc2":"train_data_without_id = train_data.copy()\ntrain_data_without_id.drop(['Id'],axis=1,inplace=True)","b5fd1cc4":"test_data_without_id = test_data.copy()\ntest_data_without_id.drop(['Id'],axis=1,inplace=True)","531394c0":"#finding Correlation\nimport seaborn as sns\ncorr = train_data.corr()\nplt.figure(figsize=(15,10))\nsns.heatmap(corr, annot=True)","fef06895":"plt.figure(figsize=(8,6))\nsns.countplot(x=\"Cover_Type\", data=train_data)\nplt.ylabel('Count', fontsize=12)\nplt.xlabel('Cover_Type', fontsize=12)\nplt.xticks(rotation='vertical')\nplt.title(\"Class Imbalance Checking\", fontsize=15)\nplt.show()","02d12c07":"X = train_data_without_id.drop(['Cover_Type'],axis=1)","7ecef6d6":"X.columns","6fcb762c":"y = train_data_without_id['Cover_Type']","f2e19e9b":"# Splitting the dataset into the Training set and Test set\nfrom sklearn.cross_validation import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0)","8402f586":"# Feature Scaling\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)","6207547e":"print(X_train.shape)\nprint(X_test.shape)","18ae5901":"print(y_train.shape)\nprint(y_test.shape)","37dd1b16":"from sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import LogisticRegression\n\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import GaussianNB\n","7ab0a831":"kfold = KFold(n_splits=10, random_state=0)\nlogestic = LogisticRegression()\nlogestic.fit(X_train,y_train)\nscoring = 'accuracy'\nresults = cross_val_score(logestic,X_train,y_train, cv=kfold, scoring=scoring)\nacc_log = results.mean()\nlog_std = results.std()\nacc_log","019c10d5":"kfold = KFold(n_splits=10, random_state=0)\nknn = KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski', p = 2)\nknn.fit(X_train,y_train)\nscoring = 'accuracy'\nresults = cross_val_score(knn,X_train,y_train, cv=kfold, scoring=scoring)\nacc_knn = results.mean()\nknn_std = results.std()\nacc_knn","c70b77cb":"kfold = KFold(n_splits=10, random_state=0)\nsvc = SVC(kernel = 'rbf')\nsvc.fit(X_train,y_train)\nscoring = 'accuracy'\nresults = cross_val_score(svc,X_train,y_train, cv=kfold, scoring=scoring)\nacc_svc = results.mean()\nsvc_std = results.std()","f3969c45":"kfold = KFold(n_splits=10, random_state=0)\ndTree = DecisionTreeClassifier(criterion = 'entropy', random_state = 0)\ndTree.fit(X_train,y_train)\nscoring = 'accuracy'\nresults = cross_val_score(dTree,X_train,y_train, cv=kfold, scoring=scoring)\nacc_dt = results.mean()\ndt_std = results.std()\nacc_dt","5e6c7a00":"kfold = KFold(n_splits=10, random_state=0)\nrandomForest = RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = 0)\nrandomForest.fit(X_train,y_train)\nscoring = 'accuracy'\nresults = cross_val_score(randomForest,X_train,y_train, cv=kfold, scoring=scoring)\nacc_rf = results.mean()\nrf_std = results.std()\nacc_rf","24943269":"nb = GaussianNB()\nnb.fit(X_train,y_train)\n\nscoring = 'accuracy'\nresults = cross_val_score(nb,X_train,y_train, cv=kfold, scoring=scoring)\nacc_nb = results.mean()\nnb_std = results.std()\nacc_nb","1369e609":"result_df = pd.DataFrame({\n                            'Model': ['LogisticRegression','SVC','KNN', 'Decision Tree','Random Forest','Naive_Bayes'],\n                            'Score': [acc_log, acc_svc, acc_knn, acc_dt, acc_rf,acc_nb]\n                         })\nresult_df.sort_values(by='Score', ascending=False)","4d6376a7":"ypred = randomForest.predict(X_test)","726954ca":"accuracy_score(y_test, ypred)","02af7a8e":"test_scaled = sc.transform(test_data_without_id)","37454f82":"y_pred_final = randomForest.predict(test_scaled)","71278733":"Counter(y_pred_final)","f9e269f4":"submission_file = pd.DataFrame({\"Id\":test_data.Id,\"Target\":y_pred_final})","4dad50cb":"submission_file.to_csv(\"Forest_coverType_prediction_submission.csv\",index=False)","6f7b9049":"## Forest Cover Type Prediction","e8706440":"### Soil Type","1c8a33cd":"### Read input files","c37c2238":"### Decision Tree","7a473a9c":"##### There is no class imbalance","57efe4e2":"### Is there any constant features?","007a830b":"### Train_Test_Split","4dad0570":"### -----------------------------------------------------------------------------------------------------------------------------------------------------------","45290d7f":"### Naive_Bayes","51d6374e":"### Class imbalance checking","d4ea2e8a":"##### There is no highly correlated columns","e0b24ed7":"### RandomForest","2e8f3bfc":"#### It shows that, train_data have some constant features, but test_data have not. So we shouldn't remove those features","ae4de586":"### Wilderness_Area","3a10a520":"### Import necessary packages","b2753ba7":"### About the data","041b3b08":"### KNN","4114de31":"### Checking Null Values ","f286b2eb":"### Logistic Regression","74dcafa7":"### SVC"}}