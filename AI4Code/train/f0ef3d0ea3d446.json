{"cell_type":{"4d51b14e":"code","b7f05a8a":"code","3b4a7233":"code","9db11caf":"code","10bc26fb":"code","940449b8":"code","4ed7f3dd":"code","9bee1738":"code","d572d411":"code","c25780fa":"code","9ec5e9cd":"code","40159f43":"code","a2e4010b":"code","381dba2b":"code","d252f275":"code","34d698ed":"code","99bbe227":"code","d3ba265c":"code","947b6f7c":"code","5d0c80bb":"code","0eddf7d2":"code","9f61fe03":"code","4c3fc2e1":"markdown","cd8f14cc":"markdown","0d766acc":"markdown","e7e678f5":"markdown","2c6c7ba6":"markdown","6f402ad0":"markdown","d5819d66":"markdown","03166a75":"markdown","c6c3c37d":"markdown"},"source":{"4d51b14e":"import torch\nimport pandas as pd\n\n# nlp library of Pytorch\nfrom torchtext import data\n\nimport warnings as wrn\nwrn.filterwarnings('ignore')","b7f05a8a":"SEED = 2021\n\ntorch.manual_seed(SEED)\ntorch.backends.cuda.deterministic = True","3b4a7233":"data_ = pd.read_csv('..\/input\/email-spam-ham-prediction\/sms_spam.csv')\ndata_.head()","9db11caf":"data_.info()","10bc26fb":"# Field is a normal column \n# LabelField is the label column.\n\nTEXT = data.Field(tokenize='spacy',batch_first=True,include_lengths=True)\nLABEL = data.LabelField(dtype = torch.float,batch_first=True)","940449b8":"fields = [(\"type\",LABEL),('text',TEXT)]","4ed7f3dd":"training_data = data.TabularDataset(path=\"..\/input\/email-spam-ham-prediction\/sms_spam.csv\",\n                                    format=\"csv\",\n                                    fields=fields,\n                                    skip_header=True\n                                   )\n\nprint(vars(training_data.examples[0]))","9bee1738":"import random\n# train and validation splitting\ntrain_data,valid_data = training_data.split(split_ratio=0.75,\n                                            random_state=random.seed(SEED))\n","d572d411":"# Building vocabularies => (Token to integer)\nTEXT.build_vocab(train_data,\n                 min_freq=5)\n\nLABEL.build_vocab(train_data)","c25780fa":"print(\"Size of text vocab:\",len(TEXT.vocab))","9ec5e9cd":"print(\"Size of label vocab:\",len(LABEL.vocab))","40159f43":"TEXT.vocab.freqs.most_common(10)","a2e4010b":"# Creating GPU variable\ndevice = torch.device(\"cuda\")\n\nBATCH_SIZE = 64\n\n# We'll create iterators to get batches of data when we want to use them\n\"\"\"\nThis BucketIterator batches the similar length of samples and reduces the need of \npadding tokens. This makes our future model more stable\n\n\"\"\"\ntrain_iterator,validation_iterator = data.BucketIterator.splits(\n    (train_data,valid_data),\n    batch_size = BATCH_SIZE,\n    # Sort key is how to sort the samples\n    sort_key = lambda x:len(x.text),\n    sort_within_batch = True,\n    device = device\n)","381dba2b":"# Pytorch's nn module has lots of useful feature\nimport torch.nn as nn\n\nclass LSTMNet(nn.Module):\n    \n    def __init__(self,vocab_size,embedding_dim,hidden_dim,output_dim,n_layers,bidirectional,dropout):\n        \n        super(LSTMNet,self).__init__()\n        \n        # Embedding layer converts integer sequences to vector sequences\n        self.embedding = nn.Embedding(vocab_size,embedding_dim)\n        \n        # LSTM layer process the vector sequences \n        self.lstm = nn.LSTM(embedding_dim,\n                            hidden_dim,\n                            num_layers = n_layers,\n                            bidirectional = bidirectional,\n                            dropout = dropout,\n                            batch_first = True\n                           )\n        \n        # Dense layer to predict \n        self.fc = nn.Linear(hidden_dim * 2,output_dim)\n        # Prediction activation function\n        self.sigmoid = nn.Sigmoid()\n        \n    \n    def forward(self,text,text_lengths):\n        embedded = self.embedding(text)\n        \n        # Thanks to packing, LSTM don't see padding tokens \n        # and this makes our model better\n        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths.cpu(),batch_first=True)\n        \n        packed_output,(hidden_state,cell_state) = self.lstm(packed_embedded)\n        \n        # Concatenating the final forward and backward hidden states\n        hidden = torch.cat((hidden_state[-2,:,:], hidden_state[-1,:,:]), dim = 1)\n        \n        dense_outputs=self.fc(hidden)\n\n        #Final activation function\n        outputs=self.sigmoid(dense_outputs)\n        \n        return outputs\n    ","d252f275":"SIZE_OF_VOCAB = len(TEXT.vocab)\nEMBEDDING_DIM = 100\nNUM_HIDDEN_NODES = 64\nNUM_OUTPUT_NODES = 1\nNUM_LAYERS = 2\nBIDIRECTION = True\nDROPOUT = 0.2","34d698ed":"model = LSTMNet(SIZE_OF_VOCAB,\n                EMBEDDING_DIM,\n                NUM_HIDDEN_NODES,\n                NUM_OUTPUT_NODES,\n                NUM_LAYERS,\n                BIDIRECTION,\n                DROPOUT\n               )","99bbe227":"import torch.optim as optim\nmodel = model.to(device)\noptimizer = optim.Adam(model.parameters(),lr=1e-4)\ncriterion = nn.BCELoss()\ncriterion = criterion.to(device)","d3ba265c":"model","947b6f7c":"# We'll use this helper to compute accuracy\ndef binary_accuracy(preds, y):\n    #round predictions to the closest integer\n    rounded_preds = torch.round(preds)\n    \n    correct = (rounded_preds == y).float() \n    acc = correct.sum() \/ len(correct)\n    return acc","5d0c80bb":"def train(model,iterator,optimizer,criterion):\n    \n    epoch_loss = 0.0\n    epoch_acc = 0.0\n    \n    model.train()\n    \n    for batch in iterator:\n        \n        # cleaning the cache of optimizer\n        optimizer.zero_grad()\n        \n        text,text_lengths = batch.text\n        \n        # forward propagation and squeezing\n        predictions = model(text,text_lengths).squeeze()\n        \n        # computing loss \/ backward propagation\n        loss = criterion(predictions,batch.type)\n        loss.backward()\n        \n        # accuracy\n        acc = binary_accuracy(predictions,batch.type)\n        \n        # updating params\n        optimizer.step()\n        \n        epoch_loss += loss.item()\n        epoch_acc += acc.item()\n        \n    # It'll return the means of loss and accuracy\n    return epoch_loss \/ len(iterator), epoch_acc \/ len(iterator)\n        ","0eddf7d2":"def evaluate(model,iterator,criterion):\n    \n    epoch_loss = 0.0\n    epoch_acc = 0.0\n    \n    # deactivate the dropouts\n    model.eval()\n    \n    # Sets require_grad flat False\n    with torch.no_grad():\n        for batch in iterator:\n            text,text_lengths = batch.text\n            \n            predictions = model(text,text_lengths).squeeze()\n              \n            #compute loss and accuracy\n            loss = criterion(predictions, batch.type)\n            acc = binary_accuracy(predictions, batch.type)\n            \n            #keep track of loss and accuracy\n            epoch_loss += loss.item()\n            epoch_acc += acc.item()\n        \n    return epoch_loss \/ len(iterator), epoch_acc \/ len(iterator)","9f61fe03":"EPOCH_NUMBER = 15\nfor epoch in range(1,EPOCH_NUMBER+1):\n    \n    train_loss,train_acc = train(model,train_iterator,optimizer,criterion)\n    \n    valid_loss,valid_acc = evaluate(model,validation_iterator,criterion)\n    \n    # Showing statistics\n    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')\n    print()","4c3fc2e1":"* I am writing this kernel in order to learn how to build an LSTM using Pytorch.\n* So I've chosen this dataset because it's small and easy to use.\n* I'll add some comment lines to make code more understandable and readable.\n","cd8f14cc":"# Conclusion\nIt's real fun to work with Pytorch. I dont't know why but, using and learning Pytorch after Keras API, using a lower-level API and seeing how the things work in deep learning is awesome.\n\nThanks for your attention. I'm waiting for your upvotes&questions.\n","0d766acc":"* Let's train the model","e7e678f5":"# Data Preprocessing\nPytorch offers a good way of preprocessing text data: **torchtext**. Altough it seems like not stable and hard-to-use for newbies, it has nice features and it's easy to use.\n\n","2c6c7ba6":"# RNN Network\nNow we'll use Pytorch to build an LSTM network in order to classify sms messages spam or not.","6f402ad0":"* Our model class is ready, let's declare hyperparameters","d5819d66":"# Training\nNow let's create our model instance, optimizer and loss function","03166a75":"* Also we need a function to evaluate model","c6c3c37d":"**Thanks for everything:\nhttps:\/\/www.analyticsvidhya.com\/blog\/2020\/01\/first-text-classification-in-pytorch\/**"}}