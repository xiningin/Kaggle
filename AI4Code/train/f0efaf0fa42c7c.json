{"cell_type":{"f116c0a9":"code","0f312408":"code","578d6c74":"code","b88dd8c7":"code","d5a92f71":"code","acc85b99":"code","91f015ad":"code","8cb85eea":"code","79d121e2":"code","243e3851":"code","88e320eb":"code","40d25036":"code","3e2e19cf":"code","81c65648":"code","1966d102":"code","a8b21722":"code","05ff38c0":"code","51da92d6":"code","ba43e79e":"code","1dd7d6b5":"code","f3879fbb":"code","a94468bb":"code","97e62c08":"code","d6905ada":"code","6c892867":"code","2c4f242b":"code","4637bb9b":"code","3cf02a42":"code","16ce812b":"code","43aafe0e":"code","411474d7":"code","41b8fc77":"code","3c047396":"code","2e75b9dd":"code","4b3da0bf":"code","f45a85fd":"code","fbcfb1c8":"code","d53cdafe":"code","38249578":"code","19bc3df8":"code","037913ee":"code","afd99238":"code","3635e13f":"code","6268e556":"code","da887ee1":"code","50446187":"code","339d2221":"code","8a7e38dc":"code","6ae4ad73":"code","71d1259a":"code","63dc840f":"code","3a76aeec":"code","653fe147":"code","fb1fd4af":"code","439ecb68":"code","e471d269":"code","856b449a":"code","498331ec":"code","bbafdc69":"code","ec9b4447":"code","489d4fb7":"code","239a4099":"code","470f0329":"code","1562ca22":"code","2999e83d":"code","18781d43":"code","7b305cb6":"code","eff21144":"code","39a232be":"code","9e36300d":"code","c9f8877e":"code","1302bec0":"code","0662795f":"code","9e5566d2":"code","d0dcfb08":"code","52baf929":"code","5e3a1970":"code","cd4b964a":"code","c481f1d4":"code","ee06be6c":"code","b9db266d":"code","b16a0e55":"code","e2a290d5":"code","2f68a253":"code","7de35443":"code","5916671d":"code","093bcc86":"code","4990e22b":"code","8a1f8a33":"code","3b170655":"code","f6e8d487":"code","c221c64a":"code","39c05835":"code","f87097a9":"code","f350b83d":"code","b05d775d":"code","efe42ddd":"code","de59038c":"code","ac7ebb7b":"code","845fc0be":"code","410069b6":"code","b7a935ad":"code","61159ee0":"code","f50c7d67":"code","ca8cd241":"code","d5452799":"code","130caa77":"code","7111a761":"code","07dfca41":"code","b97b96b7":"code","eccaefee":"code","a1506887":"code","eee1cc14":"code","0682f83b":"code","f6f18da7":"markdown","340a79fb":"markdown","c35dc198":"markdown","3a39c847":"markdown","321e71ce":"markdown","e5be5037":"markdown","01be8943":"markdown","cee2001e":"markdown","3ea80f0a":"markdown","642fd18f":"markdown","8a748a1d":"markdown","204ab2d8":"markdown","81699b47":"markdown","d8830aea":"markdown","4ae559cc":"markdown","a741b343":"markdown","98bc2c8f":"markdown","ae3d8f7b":"markdown","2396cb63":"markdown","ac274361":"markdown","bef11744":"markdown","8f4a3c74":"markdown","85b9f9ab":"markdown","949ee9b1":"markdown","c9f15251":"markdown","53b0ce18":"markdown","c61ba6d8":"markdown","2ef00cee":"markdown","3321fc2d":"markdown","76444b3d":"markdown","457c7de4":"markdown","cccf7983":"markdown"},"source":{"f116c0a9":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","0f312408":"train_df = pd.read_csv('..\/input\/train.csv')\ntrain_df.head()","578d6c74":"train_df.info()","b88dd8c7":"# Check for missing values\n\ntrain_df.isnull().sum()","d5a92f71":"# Percentage of missing values\n\nround(train_df.isnull().sum() * 100 \/ len(train_df.index), 2)","acc85b99":"train_df = train_df.drop('Cabin', axis=1)","91f015ad":"train_df.info()","8cb85eea":"train_df = train_df.drop(columns=['Name', 'Ticket'], axis=1)\ntrain_df.info()","79d121e2":"embarked_counts = train_df['Embarked'].astype('category').value_counts()\nembarked_counts","243e3851":"count_sex = train_df['Sex'].astype('category').value_counts()\ncount_sex","88e320eb":"count_parch = train_df['Parch'].astype('category').value_counts()\ncount_parch","40d25036":"train_df['Age'].describe()","3e2e19cf":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\nsns.boxplot(train_df['Age'])\nplt.show()","81c65648":"train_df.loc[train_df['Age'].isnull(), ['Age']] = train_df['Age'].median()\ntrain_df['Age'].isnull().sum()","1966d102":"sns.boxplot(train_df['Age'])\nplt.show()","a8b21722":"train_df.head()","05ff38c0":"def binary_func(x):\n    if x == 'male':\n        x = 1\n    elif x == 'female':\n        x = 0\n    return x\n\ny = list(map(binary_func, train_df['Sex']))\ny = np.array(y)\ny = pd.Series(y)\ntrain_df['Sex'] = y\ntrain_df.head()","51da92d6":"train_df.info()","ba43e79e":"embarked = pd.get_dummies(train_df['Embarked'])\nembarked.head()","1dd7d6b5":"train_df = pd.concat([train_df, embarked], axis=1)\ntrain_df.head()","f3879fbb":"train_df = train_df.drop(columns=['Embarked'])\ntrain_df.head()","a94468bb":"corr = train_df.corr()\ncorr","97e62c08":"plt.figure(figsize=(10, 6))\nsns.heatmap(corr, annot=True, cmap='YlGnBu')\nplt.show()","d6905ada":"train_df.isnull().sum()","6c892867":"# Check for outliers\ntrain_df.describe(percentiles=[0.25, 0.50, 0.75, 0.95, 0.99])","2c4f242b":"sns.countplot(train_df['Survived'])\nplt.show()","4637bb9b":"sns.countplot(train_df['Survived'], hue=train_df['Sex'])\nplt.show()","3cf02a42":"sns.countplot(train_df['Survived'], hue=train_df['Pclass'])\nplt.show()","16ce812b":"sns.countplot(train_df['S'], hue=train_df['Survived'])\nplt.show()","43aafe0e":"sns.countplot(train_df['C'], hue=train_df['Survived'])\nplt.show()","411474d7":"sns.countplot(train_df['Q'], hue=train_df['Survived'])\nplt.show()","41b8fc77":"sns.countplot(train_df['Survived'], hue=train_df['Parch'])\nplt.show()","3c047396":"sns.countplot(train_df['Survived'], hue=train_df['SibSp'])\nplt.show()","2e75b9dd":"plt.hist(train_df['Pclass'])\nplt.show()","4b3da0bf":"plt.hist(train_df['Sex'])\nplt.show()","f45a85fd":"plt.hist(train_df['SibSp'])\nplt.show()","fbcfb1c8":"plt.hist(train_df['Age'])\nplt.show()","d53cdafe":"plt.hist(train_df['Fare'])\nplt.show()","38249578":"X_train = train_df.drop(columns=['PassengerId', 'Survived'], axis=1)\nX_train.head()","19bc3df8":"y_train = train_df['Survived']\ny_train.head()","037913ee":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()","afd99238":"X_train[['Age', 'Fare']] = scaler.fit_transform(X_train[['Age', 'Fare']])\nX_train.head()","3635e13f":"plt.figure(figsize=(12, 6))\nsns.heatmap(X_train.corr(), annot=True, cmap='YlGnBu')\nplt.show()","6268e556":"import statsmodels.api as sm","da887ee1":"# Logistic regression model\nlog_reg_m1 = sm.GLM(y_train, (sm.add_constant(X_train)), family=sm.families.Binomial())\nlog_reg_m1_summary = log_reg_m1.fit().summary()\nlog_reg_m1_summary","50446187":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.feature_selection import RFE","339d2221":"rfe = RFE(LogisticRegression(), 5) # running RFE with 5 variables as output\nrfe = rfe.fit(X_train, y_train)\nrfe.support_","8a7e38dc":"list(zip(X_train.columns, rfe.support_, rfe.ranking_))","6ae4ad73":"col = X_train.columns[rfe.support_]\ncol","71d1259a":"X_train.columns[~rfe.support_]","63dc840f":"X_train_sm = sm.add_constant(X_train[col])\nlog_reg_m2 = sm.GLM(y_train, X_train_sm, family = sm.families.Binomial())\nres = log_reg_m2.fit()\nres.summary()","3a76aeec":"# Check for the VIF values of the feature variables. \nfrom statsmodels.stats.outliers_influence import variance_inflation_factor","653fe147":"# Create a dataframe that will contain the names of all the feature variables and their respective VIFs\nvif = pd.DataFrame()\nvif['Features'] = X_train[col].columns\nvif['VIF'] = [variance_inflation_factor(X_train[col].values, i) for i in range(X_train[col].shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","fb1fd4af":"col = col.drop('S', 1)\ncol","439ecb68":"# Let's re-run the model using the selected variables\nX_train_sm = sm.add_constant(X_train[col])\nlog_reg_m3 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())\nres = log_reg_m3.fit()\nres.summary()","e471d269":"col = col.drop('Q', 1)\ncol","856b449a":"# Let's re-run the model using the selected variables\nX_train_sm = sm.add_constant(X_train[col])\nlog_reg_m4 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())\nres = log_reg_m4.fit()\nres.summary()","498331ec":"# Recalculating the VIFs\nvif = pd.DataFrame()\nvif['Features'] = X_train[col].columns\nvif['VIF'] = [variance_inflation_factor(X_train[col].values, i) for i in range(X_train[col].shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","bbafdc69":"y_train_pred = res.predict(X_train_sm).values.reshape(-1)\ny_train_pred[:10]","ec9b4447":"y_train_pred_final = pd.DataFrame({'Survived': y_train.values, 'Survival_Prob':y_train_pred})\ny_train_pred_final['PassengerId'] = y_train.index\ny_train_pred_final.head()","489d4fb7":"y_train_pred_final['Predicted'] = y_train_pred_final.Survival_Prob.map(lambda x: 1 if x > 0.5 else 0)\ny_train_pred_final.head()","239a4099":"from sklearn import metrics","470f0329":"# Confusion matrix \nconfusion = metrics.confusion_matrix(y_train_pred_final.Survived, y_train_pred_final.Predicted )\nprint(confusion)","1562ca22":"# Predicted     not_survived    survived\n# Actual\n# not_survived        443      106\n# survived            92       250  ","2999e83d":"# Let's check the overall accuracy.\nprint(metrics.accuracy_score(y_train_pred_final.Survived, y_train_pred_final.Predicted))","18781d43":"TP = confusion[1,1] # true positive \nTN = confusion[0,0] # true negatives\nFP = confusion[0,1] # false positives\nFN = confusion[1,0] # false negatives","7b305cb6":"# Checking the sensitivity of the logistic regression model\nTP \/ float(TP+FN)","eff21144":"# Checking specificity\nTN \/ float(TN+FP)","39a232be":"# Calculating false postive rate - predicting survival when a passenger has died.\nprint(FP\/ float(TN+FP))","9e36300d":"# positive predictive value \nprint (TP \/ float(TP+FP))","c9f8877e":"# Negative predictive value\nprint (TN \/ float(TN+ FN))","1302bec0":"def draw_roc(actual, probs):\n    fpr, tpr, thresholds = metrics.roc_curve(actual, probs, drop_intermediate = False)\n    auc_score = metrics.roc_auc_score(actual, probs)\n    plt.figure(figsize=(5, 5))\n    plt.plot( fpr, tpr, label='ROC Curve (area = %0.2f)' % auc_score )\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate or [1 - True Negative Rate]')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver Operating Characteristic')\n    plt.legend(loc=\"lower right\")\n    plt.show()\n\n    return None","0662795f":"fpr, tpr, thresholds = metrics.roc_curve(y_train_pred_final.Survived, y_train_pred_final.Survival_Prob, drop_intermediate = False)","9e5566d2":"draw_roc(y_train_pred_final.Survived, y_train_pred_final.Survival_Prob)","d0dcfb08":"# Create columns with different probability cutoff values\nnumbers = [float(x)\/10 for x in range(10)]\nfor i in numbers:\n    y_train_pred_final[i]= y_train_pred_final.Survival_Prob.map(lambda x: 1 if x > i else 0)\ny_train_pred_final.head()","52baf929":"# Calculate accuracy, sensitivity and specificity for various probability cutoffs.\ncutoff_df = pd.DataFrame(columns = ['prob','accuracy','sensi','speci'])\nfrom sklearn.metrics import confusion_matrix\n\n# TP = confusion[1,1] # true positive \n# TN = confusion[0,0] # true negatives\n# FP = confusion[0,1] # false positives\n# FN = confusion[1,0] # false negatives\n\nnum = [0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]\nfor i in num:\n    conf_mat_1 = metrics.confusion_matrix(y_train_pred_final.Survived, y_train_pred_final[i] )\n    total1=sum(sum(conf_mat_1))\n    accuracy = (conf_mat_1[0,0]+conf_mat_1[1,1])\/total1\n    \n    speci = conf_mat_1[0,0]\/(conf_mat_1[0,0]+conf_mat_1[0,1])\n    sensi = conf_mat_1[1,1]\/(conf_mat_1[1,0]+conf_mat_1[1,1])\n    cutoff_df.loc[i] =[i, accuracy,sensi,speci]\nprint(cutoff_df)","5e3a1970":"# Plotting accuracy, sensitivity and specificity for various probabilities.\nplt.figure(figsize=(12, 8))\ncutoff_df.plot.line(x='prob', y=['accuracy','sensi','speci'])\nplt.show()","cd4b964a":"y_train_pred_final['final_predicted'] = y_train_pred_final.Survival_Prob.map(lambda x: 1 if x > 0.38 else 0)\ny_train_pred_final.head()","c481f1d4":"# Check the overall accuracy.\nmetrics.accuracy_score(y_train_pred_final.Survived, y_train_pred_final.final_predicted)","ee06be6c":"confusion2 = metrics.confusion_matrix(y_train_pred_final.Survived, y_train_pred_final.final_predicted )\nconfusion2","b9db266d":"TP = confusion2[1,1] # true positive \nTN = confusion2[0,0] # true negatives\nFP = confusion2[0,1] # false positives\nFN = confusion2[1,0] # false negatives","b16a0e55":"# Recalculating the sensitivity of our logistic regression model\nTP \/ float(TP+FN)","e2a290d5":"# Specificity\nTN \/ float(TN+FP)","2f68a253":"# False postive rate - predicting survival when a passenge is alive\nprint(FP\/ float(TN+FP))","7de35443":"# Positive predictive value \nprint(TP \/ float(TP+FP))","5916671d":"# Negative predictive value\nprint(TN \/ float(TN+ FN))","093bcc86":"confusion = metrics.confusion_matrix(y_train_pred_final.Survived, y_train_pred_final.final_predicted)\nconfusion","4990e22b":"confusion[1,1]\/(confusion[0,1]+confusion[1,1])","8a1f8a33":"confusion[1,1]\/(confusion[1,0]+confusion[1,1])","3b170655":"from sklearn.metrics import precision_score, recall_score","f6e8d487":"precision_score(y_train_pred_final.Survived, y_train_pred_final.final_predicted)","c221c64a":"recall_score(y_train_pred_final.Survived, y_train_pred_final.final_predicted)","39c05835":"from sklearn.metrics import precision_recall_curve","f87097a9":"y_train_pred_final.Survived, y_train_pred_final.final_predicted","f350b83d":"p, r, thresholds = precision_recall_curve(y_train_pred_final.Survived, y_train_pred_final.Survival_Prob)","b05d775d":"plt.plot(thresholds, p[:-1], \"g-\")\nplt.plot(thresholds, r[:-1], \"r-\")\nplt.show()","efe42ddd":"test_df = pd.read_csv('..\/input\/test.csv')\ntest_df.head()","de59038c":"test_df.info()","ac7ebb7b":"test_df.describe()","845fc0be":"test_df.shape","410069b6":"# Checking for missing value\ntest_df.isnull().sum()","b7a935ad":"z = list(map(binary_func, test_df['Sex']))\nz = np.array(z)\nz = pd.Series(z)\ntest_df['Sex'] = z\ntest_df.head()","61159ee0":"test_embarked = pd.get_dummies(test_df['Embarked'])\ntest_embarked.head()","f50c7d67":"test_df = pd.concat([test_df, test_embarked], axis=1)\ntest_df.head()","ca8cd241":"test_df = test_df.drop(columns=['Embarked'])\ntest_df.head()","d5452799":"test_df = test_df[['PassengerId', 'Pclass', 'Sex', 'C']]\ntest_df.head()","130caa77":"X_test = test_df[['Pclass', 'Sex', 'C']]\nX_test.head()","7111a761":"X_test_sm = sm.add_constant(X_test)","07dfca41":"y_test_pred = res.predict(X_test_sm)\ny_test_pred[:10]","b97b96b7":"y_test_pred_final = pd.DataFrame(y_test_pred)\ny_test_pred_final['Survival_Prob'] = pd.DataFrame(y_test_pred)","eccaefee":"y_test_pred_final['Survived'] = y_test_pred_final.Survival_Prob.map(lambda x: 1 if x > 0.38 else 0)\ny_test_pred_final = y_test_pred_final[['Survival_Prob', 'Survived']]\ny_test_pred_final['PassengerId'] = test_df.PassengerId\ny_test_pred_final.head()","a1506887":"df_for_submission = y_test_pred_final[['PassengerId', 'Survived']]\ndf_for_submission.head()","eee1cc14":"df_for_submission.shape","0682f83b":"df_for_submission.to_csv('titanic-second-submission.csv')","f6f18da7":"### Model Building","340a79fb":"### Precision and recall tradeoff","c35dc198":"Based on our Logistic Regression model, we are concnered with the ```Pclass```, ```Sex``` and ```C```. These features are not missing from the test database, so we don't have to treat the missing values. However, we have to binary encode the ```Sex``` values and extract the ```C``` values from the test dataframe.","3a39c847":"### Feature Select Using RFE","321e71ce":"#### Assessing the model with StatsModels","e5be5037":"**### Compute Optimal Survival Probablity Cutoff Value","01be8943":"##### Precision\nTP \/ TP + FP","cee2001e":"Additionally, `Name` and `Ticket` columns are not going to help us in making prediction. So, let's drop them as well.","3ea80f0a":"No need to scale the values as they are comparable.","642fd18f":"All the variables have a VIF value less than 5 which is great.","8a748a1d":"#### Creating new column 'Predicted' with 1 if Churn_Prob > 0.5 else 0","204ab2d8":"Based on the quartiles, mean and standard deviation, there seem to be no outliers.","81699b47":"#### Using sklearn utilities for the same metrics","d8830aea":"So, the accuracy is almost 78%. This is quite good. Let's check for other metrics as well.","4ae559cc":"### Feature Scaling","a741b343":"#### From the curve above, 0.38 is the optimum point to consider it as a cutoff probability.","98bc2c8f":"Let's impute the missing values in the `Age` column with the median values.","ae3d8f7b":"### Making predictions on the test set","2396cb63":"### Checking VIFs","ac274361":"This `Cabin` column has lots of missing values. So, lets' drop it from the dataframe.","bef11744":"All the variables are statistically significant.","8f4a3c74":"Columns not supported by RFE.","85b9f9ab":"#### Precision and Recall\n\nLet's go over the confusion matrix again.****","949ee9b1":"## Metrics beyond simply accuracy","c9f15251":"So, the accuracy value gets reduced to 75% from ~78%.","53b0ce18":"The `Q` column is still statistically insignificant. Hence, dropping it and rebuilding the model.","c61ba6d8":"Making predictions on the test set","2ef00cee":"##### Recall\nTP \/ TP + FN","3321fc2d":"We need `Pclass` because according to p-Value, it is statistically significant. So, let's drop `S` column and rebuild the model.","76444b3d":"### Making Predictions on the Train Data","457c7de4":"Columns supported by RFE","cccf7983":"### ROC Curve"}}